{
  "metadata": {
    "timestamp": 1736567159522,
    "page": 753,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "FeatureBaseDB/featurebase",
      "stars": 2527,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.8740234375,
          "content": "default.etcd/\n*.test\nvendor\n.protoc-gen-gofast\n.DS_Store\nbuild\n*~\nrelease-pilosa-fsck.*.*.tar.gz\n/log.*\n/tourna.log.*\npilosa\n/featurebase\n*.dot\n.idea/\n.*.swp\n.terraform/\n*.tfstate\nlaunch.json\n.terraform.lock.hcl\n__pycache__/\nreport.xml\noutputs.json\nbuilds/\n*.tfstate.backup\n.vscode\n\nbatch/testdata/batch*.out\n\nidk/testdata/idk*.out\nidk/testenv/certs/*\n\n# copy of .gitignore from archived idk repo\n# Compiled Object files, Static and Dynamic libs (Shared Objects)\n*.o\n*.a\n*.so\n\n# Folders\n_obj\n_test\n\n# Architecture specific extensions/prefixes\n*.[568vq]\n[568vq].out\n\n*.cgo1.go\n*.cgo2.c\n_cgo_defun.c\n_cgo_gotypes.go\n_cgo_export.*\n\n_testmain.go\n\n*.exe\n*.test\n*.prof\n\nvendor\n\n.terraform\nterraform.tfstate*\n\nbin\nbuild\ntestenv\n.pulled\n\npilosa-sec-data-idk\n\n.idea/\ntags.dot\n*.log\n*.swp\n*__debug_bin\n\n# SQL3\n/sql3/sql3.html\n\nstaticcheck.conf\n\n\n.quick\ndax/dax-data\n\ncoverage-from-docker\n*.client_id.txt\n\n"
        },
        {
          "name": ".gitlab",
          "type": "tree",
          "content": null
        },
        {
          "name": ".golangci.yml",
          "type": "blob",
          "size": 2.5166015625,
          "content": "run:\n  deadline: 5m\n  timeout: 5m\n  skip-dirs-use-default: true\n  #skip the protobuf generated files\n  skip-dirs:\n    - pb\n    - proto\n  skip-files:\n    - pql/pql.peg.go\nlinters:\n  enable:\n    # Recommended to be enabled by default (https://golangci-lint.run).\n    # - errcheck (lots to fix)\n    - gosimple\n    - govet\n    - ineffassign\n    - staticcheck\n    - typecheck\n    # - unused (about 20 to fix)\n\n    # Additional linters we choose to enable.\n    # - bodyclose (lots to fix, but we should)\n    - errchkjson\n    - errname\n    - gofmt\n    # - misspell (lots to fix, but we should)\n    - prealloc\n    # - predeclared (20 to fix)\n    # - stylecheck (quite a lot to fix, but we should definitely work on this)\n    - stylecheck\n    # - unconvert (not at all critical, but makes for cleaner code)\n  enable-all: false\n  disable-all: true\n\noutput:\n  # colored-line-number|line-number|json|tab|checkstyle|code-climate, default is \"colored-line-number\"\n  format: tab\n  # print lines of code with issue, default is true\n  print-issued-lines: true\n  # print linter name in the end of issue text, default is true\n  print-linter-name: true\n\nlinters-settings:\n  gofmt:\n    simplify: true\n  govet:\n    # report about shadowed variables\n    check-shadowing: true\n\n\n    # settings per analyzer\n    settings:\n      printf: # analyzer name, run `go tool vet help` to see all analyzers\n        funcs: # run `go tool vet help printf` to see available settings for `printf` analyzer\n          - (github.com/golangci/golangci-lint/pkg/logutils.Log).Infof\n          - (github.com/golangci/golangci-lint/pkg/logutils.Log).Warnf\n          - (github.com/golangci/golangci-lint/pkg/logutils.Log).Errorf\n          - (github.com/golangci/golangci-lint/pkg/logutils.Log).Fatalf\n\n    # enable or disable analyzers by name\n    # run `go tool vet help` to see all analyzers\n    enable:\n      - atomicalign\n    enable-all: false\n    disable:\n      - shadow\n    disable-all: false\n\n  stylecheck:\n    # ST1000: at least one file in a package should have a package comment\n    # ST1003: golang naming standards\n    # ST1016: methods on the same type should have the same receiver name\n    # ST1020: comment on exported function\n    checks: [\"all\", \"-ST1000\", \"-ST1003\", \"-ST1016\", \"-ST1020\"]\n\nissues:\n  exclude-use-default: false\n  max-issues-per-linter: 0\n  max-same-issues: 0\n  exclude:\n    - 'declaration of \"(err|ctx)\" shadows declaration at'\n    - 'Error return value of .(.*\\.Help|.*\\.MarkFlagRequired|(os\\.)?std(out|err)\\..*|.*Close|.*Flush|os\\.Remove(All)?|.*printf?|os\\.(Un)?Setenv). is not checked'\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 5.3623046875,
          "content": "\n# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, caste, color, religion, or sexual\nidentity and orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n* Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n* Focusing on what is best not just for us as individuals, but for the overall\n  community\n\nExamples of unacceptable behavior include:\n\n* The use of sexualized language or imagery, and sexual attention or advances of\n  any kind\n* Trolling, insulting or derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or email address,\n  without their explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\ncommunity@featurebase.com.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series of\nactions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or permanent\nban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior, harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within the\ncommunity.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.1, available at\n[https://www.contributor-covenant.org/version/2/1/code_of_conduct.html][v2.1].\n\nCommunity Impact Guidelines were inspired by\n[Mozilla's code of conduct enforcement ladder][Mozilla CoC].\n\nFor answers to common questions about this code of conduct, see the FAQ at\n[https://www.contributor-covenant.org/faq][FAQ]. Translations are available at\n[https://www.contributor-covenant.org/translations][translations].\n\n[homepage]: https://www.contributor-covenant.org\n[v2.1]: https://www.contributor-covenant.org/version/2/1/code_of_conduct.html\n[Mozilla CoC]: https://github.com/mozilla/diversity\n[FAQ]: https://www.contributor-covenant.org/faq\n[translations]: https://www.contributor-covenant.org/translations\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.0908203125,
          "content": "ARG GO_VERSION=latest\n\n#######################\n### Lattice builder ###\n#######################\n\nFROM ghcr.io/featurebasedb/nodejs:0.0.1 as lattice-builder\nWORKDIR /lattice\n\nCOPY lattice/package.json ./\nCOPY lattice/yarn.lock ./\nRUN yarn install\n\nCOPY lattice ./\nRUN yarn build\n\n######################\n### Pilosa builder ###\n######################\n\nFROM golang:${GO_VERSION} as pilosa-builder\nARG MAKE_FLAGS\nARG SOURCE_DATE_EPOCH\n\nWORKDIR /pilosa\n\nRUN go install github.com/rakyll/statik@v0.1.7\n\nCOPY . ./\nCOPY --from=lattice-builder /lattice/build /lattice\nRUN /go/bin/statik -src=/lattice -dest=/pilosa\n\nENV SOURCE_DATE_EPOCH=${SOURCE_DATE_EPOCH}\nRUN make build FLAGS=\"-o build/featurebase\" ${MAKE_FLAGS}\n\n#####################\n### Pilosa runner ###\n#####################\n\nFROM alpine:3.13.2 as runner\n\nLABEL maintainer \"dev@molecula.com\"\n\nRUN apk add --no-cache curl jq tree\n\nCOPY --from=pilosa-builder /pilosa/build/featurebase /\n\nCOPY NOTICE /NOTICE\n\nEXPOSE 10101\nVOLUME /data\n\nENV PILOSA_DATA_DIR /data\nENV PILOSA_BIND 0.0.0.0:10101\nENV PILOSA_BIND_GRPC 0.0.0.0:20101\n\nENTRYPOINT [\"/featurebase\"]\nCMD [\"server\"]\n"
        },
        {
          "name": "Dockerfile-clustertests",
          "type": "blob",
          "size": 1.236328125,
          "content": "# This Dockerfile is used for cluster testing - it produces a much larger image\n# and includes all of Go as well as some utilities.\n\nFROM golang:1.19\n\nLABEL maintainer \"dev@pilosa.com\"\n\nCOPY . /go/src/github.com/featurebasedb/featurebase/\n\n\n# download pumba for fault injection\nADD https://github.com/alexei-led/pumba/releases/download/0.6.0/pumba_linux_amd64 /pumba\nRUN chmod +x /pumba\n\n# add docker client to pause/unpause nodes\nRUN apt update\nRUN apt install -y docker.io\n\n# add docker-compose so tests can use it for stuff\nADD https://github.com/docker/compose/releases/latest/download/docker-compose-Linux-x86_64 /usr/local/bin/docker-compose\nRUN chmod +x /usr/local/bin/docker-compose\n\nWORKDIR /go/src/github.com/featurebasedb/featurebase/cmd/featurebase\n\n# generate an instrumented binary to allow for calculating code coverage for clustertests\n# the entrypoint for the binary is TestRunMain, which is wrapper for main\nRUN go test -covermode=atomic -coverpkg=../../... -c -tags testrunmain -o featurebase\nRUN cp /go/src/github.com/featurebasedb/featurebase/cmd/featurebase/featurebase /featurebase\n\nCOPY NOTICE /NOTICE\n\nEXPOSE 10101\nVOLUME /data\n\n\n# use e.g. \"-test.coverprofile=/results/coverage.out\"\nCMD [\"/featurebase\", \"-test.run=TestRunMain\", \"server\"]\n\n"
        },
        {
          "name": "Dockerfile-clustertests-client",
          "type": "blob",
          "size": 1.267578125,
          "content": "# This Dockerfile is used for cluster testing - it produces a much larger image\n# and includes all of Go as well as some utilities.\n\nFROM golang:1.19\n\nLABEL maintainer \"dev@pilosa.com\"\n\nCOPY . /go/src/github.com/featurebasedb/featurebase/\n\n# download pumba for fault injection\nADD https://github.com/alexei-led/pumba/releases/download/0.6.0/pumba_linux_amd64 /pumba\nRUN chmod +x /pumba\n\n# add docker client to pause/unpause nodes\nRUN apt update\nRUN apt install -y docker.io\n\n# add docker-compose so tests can use it for stuff\nADD https://github.com/docker/compose/releases/latest/download/docker-compose-Linux-x86_64 /usr/local/bin/docker-compose\nRUN chmod +x /usr/local/bin/docker-compose\n\nWORKDIR /go/src/github.com/featurebasedb/featurebase/cmd/featurebase\n\nRUN go test -covermode=atomic -coverpkg=../../... -c -tags testrunmain -o featurebase\nRUN cp /go/src/github.com/featurebasedb/featurebase/cmd/featurebase/featurebase /featurebase\n\n\nCOPY NOTICE /NOTICE\n\nCOPY ./internal/clustertests /go/src/github.com/featurebasedb/featurebase/internal/clustertests\n\nEXPOSE 10101\nVOLUME /data\n\nWORKDIR /go/src/github.com/featurebasedb/featurebase\n\nCMD [\"/featurebase\", \"-test.run=TestRunMain\", \"-test.coverprofile=/results/coverage.out\", \"server\", \"--data-dir\", \"/data\", \"--bind\", \"http://0.0.0.0:10101\"]\n"
        },
        {
          "name": "Dockerfile-datagen",
          "type": "blob",
          "size": 0.9375,
          "content": "# syntax=docker/dockerfile:1\n\n##########################\n### datagen builder    ###\n##########################\n\nFROM golang:alpine as builder\n\nWORKDIR /featurebase\n\nCOPY . ./\n\nRUN apk add --no-cache build-base bash git make librdkafka pkgconfig\n\n# install librdkafka\nRUN git clone https://github.com/edenhill/librdkafka.git\nRUN cd librdkafka && ./configure --prefix /usr && make && make install\n\nENV PKG_CONFIG_PATH=/usr/lib/pkgconfig/\n\nRUN cd idk && make build-datagen\n\n# ENTRYPOINT [\"tail\", \"-f\", \"/dev/null\"]\n\n#########################\n### datagen runner    ###\n#########################\n\nFROM alpine:3.15.3 as runner\n\nWORKDIR /\n\nLABEL maintainer \"dev@molecula.com\"\n\nRUN apk add --no-cache curl jq\n\nCOPY --from=builder /featurebase/idk/build/datagen /bin/\nCOPY --from=builder /usr/lib/librdkafka* /usr/lib/\nCOPY idk/datagen/testdata/* /testdata/\n\nEXPOSE 8080\n\n# VOLUME /data\n# ENV ADDR 0.0.0.0:8080\n\n#ENTRYPOINT [\"sleep\", \"infinity\"]\nENTRYPOINT [\"datagen\"]\n"
        },
        {
          "name": "Dockerfile-dax",
          "type": "blob",
          "size": 0.5615234375,
          "content": "ARG GO_VERSION=latest\n\n\n###########################\n### FeatureBase Builder ###\n###########################\n\nFROM golang:${GO_VERSION} as featurebase-builder\nARG MAKE_FLAGS\nWORKDIR /fb\n\nCOPY . ./\nRUN make build FLAGS=\"-o build/featurebase\" ${MAKE_FLAGS}\n\n##########################\n### FeatureBase runner ###\n##########################\n\nFROM golang:alpine as runner\n\nLABEL maintainer \"dev@featurebase.com\"\n\nRUN apk add --no-cache curl jq tree\n\nCOPY --from=featurebase-builder /fb/build/featurebase /\n\nCOPY NOTICE /NOTICE\n\nEXPOSE 8080\n\nENTRYPOINT [\"/featurebase\"]\nCMD [\"dax\"]\n"
        },
        {
          "name": "Dockerfile-dax-quick",
          "type": "blob",
          "size": 0.2880859375,
          "content": "ARG GO_VERSION=latest\n\n##########################\n### FeatureBase runner ###\n##########################\n\nFROM alpine:3.13.2 as runner\n\nLABEL maintainer \"dev@featurebase.com\"\n\nRUN apk add --no-cache curl jq tree\n\nCOPY ./fb_linux /featurebase\n\nEXPOSE 8080\n\nENTRYPOINT [\"/featurebase\"]\nCMD [\"dax\"]\n"
        },
        {
          "name": "Dockerfile-fbsql",
          "type": "blob",
          "size": 0.9931640625,
          "content": "ARG GO_VERSION=1.19\n\nFROM golang:1.19-buster as builder\n\nWORKDIR /\nRUN apt-get update -y -qq && apt-get install -y -qq \\\n  build-essential \\\n  git \\\n  musl-tools \\\n  netcat \\\n  unixodbc \\\n  unixodbc-dev \\\n  && rm -rf /var/lib/apt/lists/*\nRUN [\"git\", \"clone\", \"https://github.com/edenhill/librdkafka.git\"]\nWORKDIR /librdkafka\nRUN ./configure --prefix /usr && \\\n  make && \\\n  make install\n\nWORKDIR /featurebase\n\nCOPY . .\n\nARG MAKE_FLAGS\nARG GO_BUILD_FLAGS\nARG SOURCE_DATE_EPOCH\n\nWORKDIR /featurebase/\n\nENV SOURCE_DATE_EPOCH=${SOURCE_DATE_EPOCH}\nRUN make build-fbsql GO_BUILD_FLAGS=\"-mod=vendor ${GO_BUILD_FLAGS}\" ${MAKE_FLAGS}\n\nFROM ubuntu:20.04 as runner\n\nRUN apt-get update -y -qq && apt-get install -y -qq \\\n  ca-certificates \\\n  musl-tools \\\n  netcat \\\n  unixodbc-dev \\\n  && rm -rf /var/lib/apt/lists/*\n\nCOPY --from=builder /featurebase/fbsql /usr/local/bin/\n\n# Verify that the linker can find everything.\nFROM runner as linkcheck\nRUN if [ -e /usr/local/bin/fbsql ] ; then ldd /usr/local/bin/fbsql; fi\n\nFROM runner\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0986328125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2023 Molecula Corp. All rights reserved.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "LICENSE-2.0.txt",
          "type": "blob",
          "size": 11.091796875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 15.53515625,
          "content": ".PHONY: build clean build-lattice cover cover-viz default docker docker-build docker-build-fbsql docker-tag-push generate generate-protoc generate-pql generate-statik generate-stringer install install-protoc-gen-gofast install-protoc install-statik install-peg test docker-login\n\nSHELL := /bin/bash\nVERSION := $(shell git describe --tags 2> /dev/null || echo unknown)\nVARIANT = Molecula\nGO=go\nGOOS=$(shell $(GO) env GOOS)\nGOARCH=$(shell $(GO) env GOARCH)\nVERSION_ID=$(if $(TRIAL_DEADLINE),trial-$(TRIAL_DEADLINE)-,)$(VERSION)-$(GOOS)-$(GOARCH)\nDATE_FMT=\"+%FT%T%z\"\n# set SOURCE_DATE_EPOCH like this to use the last git commit timestamp\n# export SOURCE_DATE_EPOCH=$(git log -1 --pretty=%ct) instead of the current time from running `date`\nifdef SOURCE_DATE_EPOCH\n\tBUILD_TIME ?= $(shell date -u -d \"@$(SOURCE_DATE_EPOCH)\" \"$(DATE_FMT)\" 2>/dev/null || date -u -r \"$(SOURCE_DATE_EPOCH)\" \"$(DATE_FMT)\" 2>/dev/null || date -u \"$(DATE_FMT)\")\nelse\n\tBUILD_TIME ?= $(shell date -u \"$(DATE_FMT)\")\nendif\nSHARD_WIDTH = 20\nCOMMIT := $(shell git describe --exact-match >/dev/null 2>&1 || git rev-parse --short HEAD)\nLDFLAGS=\"-X github.com/featurebasedb/featurebase/v3.Version=$(VERSION) -X github.com/featurebasedb/featurebase/v3.BuildTime=$(BUILD_TIME) -X github.com/featurebasedb/featurebase/v3.Variant=$(VARIANT) -X github.com/featurebasedb/featurebase/v3.Commit=$(COMMIT) -X github.com/featurebasedb/featurebase/v3.TrialDeadline=$(TRIAL_DEADLINE)\"\nGO_VERSION=1.19\nGO_BUILD_FLAGS=\nDOCKER_BUILD= # set to 1 to use `docker-build` instead of `build` when creating a release\nBUILD_TAGS += \nTEST_TAGS = roaringparanoia\nTEST_TIMEOUT=10m\nRACE_TEST_TIMEOUT=10m\n# size in GB to use for ramdisk, ?= so you can override it with env\n# 4GB is not enough for `make test`, 8GB usually is.\nRAMDISK_SIZE ?= 8\n\nexport GO111MODULE=on\nexport GOPRIVATE=github.com/molecula\nexport CGO_ENABLED=0\nAWS_ACCOUNTID ?= undefined\n\n# Run tests and compile Pilosa\ndefault: test build\n\n# Remove build directories\nclean:\n\trm -rf vendor build\n\trm -f *.rpm *.deb\n\n# Set up vendor directory using `go mod vendor`\nvendor: go.mod\n\t$(GO) mod vendor\n\nversion:\n\t@echo $(VERSION)\n\n# We build a list of packages that omits the IDK and batch packages because\n# those packages require fancy environment setup.\nGOPACKAGES := $(shell $(GO) list ./... | grep -v \"/v3/idk\" | grep -v \"/v3/batch\")\n\n# Run test suite\ntest:\n\t$(GO) test $(GOPACKAGES) -tags='$(BUILD_TAGS) $(TEST_TAGS)' $(TESTFLAGS) -v -timeout $(TEST_TIMEOUT) -count=1\n\n# Run test suite with race flag\ntest-race:\n\tCGO_ENABLED=1 $(GO) test $(GOPACKAGES) -tags='$(BUILD_TAGS) $(TEST_TAGS)' $(TESTFLAGS) -race -timeout $(RACE_TEST_TIMEOUT) -v\n\ntestv: testvsub\n\ntestv-race: testvsub-race\n\n# testvsub: run go test -v in sub-directories in \"local mode\" with incremental output,\n#            avoiding go -test ./... \"package list mode\" which doesn't give output\n#            until the test run finishes. Package list mode makes it hard to\n#            find which test is hung/deadlocked.\n#\ntestvsub:\n\t@set -e; for pkg in $(GOPACKAGES); do \\\n\t\t\tif [ $${pkg:0:38} == \"github.com/featurebasedb/featurebase/v3/idk\" ]; then \\\n\t\t\t\techo; echo \"___ skipping subpkg $$pkg\"; \\\n\t\t\t\tcontinue; \\\n\t\t\tfi; \\\n\t\t\techo; echo \"___ testing subpkg $$pkg\"; \\\n\t\t\t$(GO) test -tags='$(BUILD_TAGS) $(TEST_TAGS)' $(TESTFLAGS) -v -timeout $(RACE_TEST_TIMEOUT) $$pkg || break; \\\n\t\t\techo; echo \"999 done testing subpkg $$pkg\"; \\\n\t\tdone\n\n# make a $(RAMDISK_SIZE)GB RAMDisk. Speed up tests by running\n# them with TMPDIR=/mnt/ramdisk.\nramdisk-linux:\n\tmount -o size=$(RAMDISK__SIZE)G -t tmpfs none /mnt/ramdisk\n\n# make a $(RAMDISK_SIZE)GB RAMDisk. Speed up tests by running\n# them with TMPDIR=/Volumes/RAMDisk. This is more important on\n# OS X than it is on Linux, because there's performance issues\n# with fsync on OS X that can make the SSD slow down to moving-platters\n# drive speeds. Oops.\nramdisk-osx:\n\tdiskutil erasevolume HFS+ 'RAMDisk' $$(hdiutil attach -nobrowse -nomount ram://$$(expr 2097152 \\* $(RAMDISK_SIZE)))\n\ndetach-ramdisk-osx:\n\thdiutil detach /Volumes/RAMDisk\n\ntestvsub-race:\n\t@set -e; for pkg in $(GOPACKAGES); do \\\n           echo; echo \"___ testing subpkg $$pkg\"; \\\n           CGO_ENABLED=1 $(GO) test -tags='$(BUILD_TAGS) $(TEST_TAGS)' $(TESTFLAGS) -v -race -timeout $(RACE_TEST_TIMEOUT) $$pkg || break; \\\n           echo; echo \"999 done testing subpkg $$pkg\"; \\\n        done\n\n\nbench:\n\t$(GO) test $(GOPACKAGES) -bench=. -run=NoneZ -timeout=127m $(TESTFLAGS)\n\n# Run test suite with coverage enabled\ncover:\n\tmkdir -p build\n\t$(MAKE) test TESTFLAGS=\"-coverprofile=build/coverage.out\"\n\n# Run test suite with coverage enabled and view coverage results in browser\ncover-viz: cover\n\t$(GO) tool cover -html=build/coverage.out\n\n# Build featurebase\nbuild:\n\t$(GO) build -tags='$(BUILD_TAGS)' -ldflags $(LDFLAGS) $(FLAGS) ./cmd/featurebase\n\npackage:\n\tGOOS=$(GOOS) GOARCH=$(GOARCH) $(MAKE) build\n\tGOOS=$(GOOS) GOARCH=$(GOARCH) $(MAKE) build-fbsql\n\tGOARCH=$(GOARCH) VERSION=$(VERSION) nfpm package --packager deb --target featurebase.$(VERSION).$(GOARCH).deb\n\tGOARCH=$(GOARCH) VERSION=$(VERSION) nfpm package --packager rpm --target featurebase.$(VERSION).$(GOARCH).rpm\n\n# We allow setting a custom docker-compose \"project\". Multiple of the\n# same docker-compose environment can exist simultaneously as long as\n# they use different projects (the project name is prepended to\n# container names and such). This is useful in a CI environment where\n# we might be running multiple instances of the tests concurrently.\nPROJECT ?= clustertests\nDOCKER_COMPOSE = docker-compose -p $(PROJECT)\n\n# Run cluster integration tests using docker. Requires docker daemon to be\n# running and docker-compose to be installed.\nclustertests: vendor\n\t$(DOCKER_COMPOSE) -f internal/clustertests/docker-compose.yml down\n\t$(DOCKER_COMPOSE) -f internal/clustertests/docker-compose.yml build\n\t$(DOCKER_COMPOSE) -f internal/clustertests/docker-compose.yml up -d pilosa1 pilosa2 pilosa3\n\tPROJECT=$(PROJECT) $(DOCKER_COMPOSE) -f internal/clustertests/docker-compose.yml run client1\n\t$(DOCKER_COMPOSE) -f internal/clustertests/docker-compose.yml down\n\n# Run the cluster tests with authentication enabled\nAUTH_ARGS=\"-c /go/src/github.com/featurebasedb/featurebase/internal/clustertests/testdata/featurebase.conf\"\nauthclustertests: vendor\n\tCLUSTERTESTS_FB_ARGS=$(AUTH_ARGS) $(DOCKER_COMPOSE) -f internal/clustertests/docker-compose.yml down\n\tCLUSTERTESTS_FB_ARGS=$(AUTH_ARGS) $(DOCKER_COMPOSE) -f internal/clustertests/docker-compose.yml build\n\tCLUSTERTESTS_FB_ARGS=$(AUTH_ARGS) $(DOCKER_COMPOSE) -f internal/clustertests/docker-compose.yml up -d pilosa1 pilosa2 pilosa3\n\tPROJECT=$(PROJECT) ENABLE_AUTH=1 $(DOCKER_COMPOSE) -f internal/clustertests/docker-compose.yml run client1\n\tCLUSTERTESTS_FB_ARGS=$(AUTH_ARGS) $(DOCKER_COMPOSE) -f internal/clustertests/docker-compose.yml down\n\n# Install FeatureBase and IDK\ninstall: install-featurebase install-idk install-fbsql\n\ninstall-featurebase:\n\t$(GO) install -tags='$(BUILD_TAGS)' -ldflags $(LDFLAGS) $(FLAGS) ./cmd/featurebase\n\ninstall-idk:\n\t$(MAKE) -C ./idk install\n\ninstall-fbsql:\n\tCGO_ENABLED=1 $(GO) install ./cmd/fbsql\n\n# Build the lattice assets\nbuild-lattice:\n\tdocker build -t lattice:build ./lattice\n\texport LATTICE=`docker create lattice:build`; docker cp $$LATTICE:/lattice/. ./lattice/build && docker rm $$LATTICE\n\n# `go generate` protocol buffers\ngenerate-protoc: require-protoc require-protoc-gen-gofast\n\t$(GO) generate github.com/featurebasedb/featurebase/v3/pb\n\n# `go generate` statik assets (lattice UI)\ngenerate-statik: build-lattice require-statik\n\t$(GO) generate github.com/featurebasedb/featurebase/v3/statik\n\n# `go generate` statik assets (lattice UI) in Docker\ngenerate-statik-docker: build-lattice\n\tdocker run --rm -t -v $(PWD):/pilosa golang:1.15.8 sh -c \"go get github.com/rakyll/statik && /go/bin/statik -src=/pilosa/lattice/build -dest=/pilosa -f\"\n\n# `go generate` stringers\ngenerate-stringer:\n\t$(GO) generate github.com/featurebasedb/featurebase/v3\n\ngenerate-pql: require-peg\n\tcd pql && peg -inline pql.peg && cd ..\n\ngenerate-proto-grpc: require-protoc require-protoc-gen-go\n\tprotoc -I proto proto/pilosa.proto --go_out=plugins=grpc:proto\n#\taddress re-generation here only if we need to\t\n#\tprotoc -I proto proto/vdsm.proto --go_out=plugins=grpc:proto\n\n# `go generate` all needed packages\ngenerate: generate-protoc generate-statik generate-stringer generate-pql\n\n# Create release using Docker\ndocker-release:\n\t$(MAKE) docker-build GOOS=linux GOARCH=amd64\n\t$(MAKE) docker-build GOOS=linux GOARCH=arm64\n\t$(MAKE) docker-build GOOS=darwin GOARCH=amd64\n\t$(MAKE) docker-build GOOS=darwin GOARCH=arm64\n\n# Build a release in Docker\ndocker-build: vendor\n\tdocker build \\\n\t    --build-arg GO_VERSION=$(GO_VERSION) \\\n\t    --build-arg MAKE_FLAGS=\"TRIAL_DEADLINE=$(TRIAL_DEADLINE) GOOS=$(GOOS) GOARCH=$(GOARCH)\" \\\n\t    --build-arg SOURCE_DATE_EPOCH=$(SOURCE_DATE_EPOCH) \\\n\t    --target pilosa-builder \\\n\t    --tag featurebase:build .\n\tdocker create --name featurebase-build featurebase:build\n\tmkdir -p build/featurebase-$(VERSION_ID)\n\tdocker cp featurebase-build:/pilosa/build/. ./build/featurebase-$(VERSION_ID)\n\tcp NOTICE install/featurebase.conf install/featurebase*.service ./build/featurebase-$(VERSION_ID)\n\tdocker rm featurebase-build\n\ttar -cvz -C build -f build/featurebase-$(VERSION_ID).tar.gz featurebase-$(VERSION_ID)/\n\n# Create Docker image from Dockerfile\ndocker-image: vendor\n\tdocker build \\\n\t    --build-arg GO_VERSION=$(GO_VERSION) \\\n\t    --build-arg MAKE_FLAGS=\"TRIAL_DEADLINE=$(TRIAL_DEADLINE)\" \\\n\t    --tag featurebase:$(VERSION) .\n\t@echo Created docker image: featurebase:$(VERSION)\n\ndocker-image-featurebase: vendor\n\tdocker build \\\n\t    --build-arg GO_VERSION=$(GO_VERSION) \\\n\t\t--file Dockerfile-dax \\\n\t\t--tag dax/featurebase .\n\ndocker-image-featurebase-linux-amd64: vendor\n\tdocker build \\\n\t    --build-arg GO_VERSION=$(GO_VERSION) \\\n\t    --platform linux/amd64 \\\n\t    --file Dockerfile-dax \\\n\t    --tag dax/featurebase .\n\ndocker-image-featurebase-test: vendor\n\tdocker build \\\n\t    --build-arg GO_VERSION=$(GO_VERSION) \\\n\t\t--file Dockerfile-clustertests \\\n\t\t--tag dax/featurebase-test .\n\n\n# build-for-quick builds a linux featurebase binary outside of docker\n# (which is much faster for some reason), and places it in the .quick\n# subdirectory.\nbuild-for-quick:\n\tGOOS=linux $(MAKE) build FLAGS=\"-o .quick/fb_linux\"\n\n# docker-image-featurebase-quick uses a pre-built featurebase binary\n# to quickly create a fresh docker image without needing to send the\n# context of the featurebase top level directory.\ndocker-image-featurebase-quick: build-for-quick\n\tdocker build \\\n\t    --build-arg GO_VERSION=$(GO_VERSION) \\\n\t    --file Dockerfile-dax-quick \\\n\t    --tag dax/featurebase ./.quick/\n\n\ndocker-image-datagen: vendor\n\tdocker build --tag dax/datagen --file Dockerfile-datagen .\n\nget-account-id:\n\t$(eval AWS_ACCOUNTID := $(shell aws sts get-caller-identity --output=json | jq -r .Account))\n\n\necr-push-featurebase: docker-login\n\techo \"Pushing to account $(AWS_ACCOUNTID), profile $(AWS_PROFILE)\"\n\tdocker tag dax/featurebase:latest $(AWS_ACCOUNTID).dkr.ecr.us-east-2.amazonaws.com/dax/featurebase:latest\n\tdocker push $(AWS_ACCOUNTID).dkr.ecr.us-east-2.amazonaws.com/dax/featurebase:latest\n\necr-push-datagen: docker-login\n\tdocker tag dax/datagen:latest $(AWS_ACCOUNTID).dkr.ecr.us-east-2.amazonaws.com/dax/datagen:latest\n\tdocker push $(AWS_ACCOUNTID).dkr.ecr.us-east-2.amazonaws.com/dax/datagen:latest\n\ndocker-login: get-account-id\n\taws ecr get-login-password --region us-east-2 | docker login --username AWS --password-stdin $(AWS_ACCOUNTID).dkr.ecr.us-east-2.amazonaws.com\n\n# Create docker image (alias)\ndocker: docker-image # alias\n\n# Tag and push a Docker image\ndocker-tag-push: vendor\n\tdocker tag \"featurebase:$(VERSION)\" $(DOCKER_TARGET)\n\tdocker push $(DOCKER_TARGET)\n\t@echo Pushed docker image: $(DOCKER_TARGET)\n\n# These commands (docker-idk and docker-idk-tag-push)\n# are designed to be used in CI.\n# docker-idk builds idk docker images and tags them - intended for use in CI.\ndocker-idk: vendor\n\tdocker build \\\n\t\t-f idk/Dockerfile \\\n\t\t--build-arg GO_VERSION=$(GO_VERSION) \\\n\t\t--build-arg MAKE_FLAGS=\"GOOS=$(GOOS) GOARCH=$(GOARCH) BUILD_CGO=$(BUILD_CGO)\" \\\n\t\t--tag registry.gitlab.com/molecula/featurebase/idk:$(VERSION_ID) .\n\t@echo Created docker image: registry.gitlab.com/molecula/featurebase/idk:$(VERSION_ID)\n# docker-idk-tag-push pushes tagged docker images to the GitLab container\n# registry - intended for use in CI.\ndocker-idk-tag-push:\n\tdocker push registry.gitlab.com/molecula/featurebase/idk:$(VERSION_ID)\n\t@echo Pushed docker image: registry.gitlab.com/molecula/featurebase/idk:$(VERSION_ID)\n\n# Run golangci-lint\ngolangci-lint: require-golangci-lint\n\tgolangci-lint run --timeout 3m --skip-files '.*\\.peg\\.go'\n\n# Alias\nlinter: golangci-lint\n\n# Better alias\nocd: golangci-lint\n\n######################\n# Build dependencies #\n######################\n\n# Verifies that needed build dependency is installed. Errors out if not installed.\nrequire-%:\n\t$(if $(shell command -v $* 2>/dev/null),\\\n\t\t$(info Verified build dependency \"$*\" is installed.),\\\n\t\t$(error Build dependency \"$*\" not installed. To install, try `make install-$*`))\n\ninstall-build-deps: install-protoc-gen-gofast install-protoc install-statik install-peg\n\ninstall-statik:\n\tgo install github.com/rakyll/statik@latest\n\ninstall-protoc-gen-gofast:\n\tGO111MODULE=off $(GO) get -u github.com/gogo/protobuf/protoc-gen-gofast\n\ninstall-protoc:\n\t@echo This tool cannot automatically install protoc. Please download and install protoc from https://google.github.io/proto-lens/installing-protoc.html\n\t@echo On mac, brew install protobuf seems to work.\n\t@echo As of the commit that added this line, protoc-gen-gofast was at 226206f39bd7, and the protoc version in use was:\n\t@echo $$ protoc --version\n\t@echo libprotoc 3.19.4\n\ninstall-peg:\n\tGO111MODULE=off $(GO) get github.com/pointlander/peg\n\ninstall-golangci-lint:\n\tGO111MODULE=off $(GO) get github.com/golangci/golangci-lint/cmd/golangci-lint\n\ntest-external-lookup:\n\t$(GO) test . -tags='$(BUILD_TAGS) $(TEST_TAGS)' $(TESTFLAGS) -run ^TestExternalLookup$$ -externalLookupDSN $(EXTERNAL_LOOKUP_DSN)\n\nbnf:\n\tebnf2railroad --no-overview-diagram  --no-optimizations ./sql3/sql3.ebnf\n\n#################################\n# fbsql builds in docker\n#################################\n\n# This allows multiple concurrent builds to happen in CI without\n# creating container name conflicts and such. (different BUILD_NAMEs\n# are passed in from gitlab-ci.yml)\nBUILD_NAME ?= fbsql-build\n\nLDFLAGS_STATIC=\"-linkmode external -extldflags \\\"-static\\\" -X 'github.com/featurebasedb/featurebase/v3/fbsql.Version=$(VERSION)' -X 'github.com/featurebasedb/featurebase/v3/fbsql.BuildTime=$(BUILD_TIME)' \"\n\nUNAME_P := $(shell uname -p)\nBUILD_CGO ?= 0\n\n# Build fbsql\nbuild-fbsql:\n\t@echo GOOS=$(GOOS) GOARCH=$(GOARCH) uname -p=$(UNAME_P) build_cgo=$(BUILD_CGO)\nifeq ($(BUILD_CGO), 0)\n\tmake build-fbsql-non-cgo\nendif\nifeq ($(BUILD_CGO), 1)\n\tmake build-fbsql-cgo\nendif\n\nbuild-fbsql-non-cgo:\n\tCGO_ENABLED=0 $(GO) build -ldflags $(LDFLAGS) $(GO_BUILD_FLAGS) -o fbsql ./cmd/fbsql\n\nbuild-fbsql-cgo:\nifeq ($(GOARCH), arm64)\n\tCGO_ENABLED=1 $(GO) build -tags dynamic $(GO_BUILD_FLAGS) -o fbsql ./cmd/fbsql\nendif\nifeq ($(GOARCH), amd64)\n\tCC=/usr/bin/musl-gcc CGO_ENABLED=1 $(GO) build -tags \"musl static\" -ldflags $(LDFLAGS_STATIC) $(GO_BUILD_FLAGS) -o fbsql ./cmd/fbsql\nendif\n\ndocker-build-fbsql: vendor\n\tDOCKER_BUILDKIT=0 docker build \\\n\t\t--file Dockerfile-fbsql \\\n\t    --build-arg GO_VERSION=$(GO_VERSION) \\\n\t    --build-arg MAKE_FLAGS=\"GOOS=$(GOOS) GOARCH=$(GOARCH) BUILD_CGO=$(BUILD_CGO)\" \\\n\t    --build-arg GO_BUILD_FLAGS=$(GO_BUILD_FLAGS) \\\n\t\t--build-arg SOURCE_DATE_EPOCH=$(SOURCE_DATE_EPOCH) \\\n\t    --target builder \\\n\t    --tag fbsql:$(BUILD_NAME) .\n\tmkdir -p build\n\tdocker create --name $(BUILD_NAME) fbsql:$(BUILD_NAME)\n\tdocker cp $(BUILD_NAME):/featurebase/fbsql ./build/fbsql_$(GOOS)_$(GOARCH)\n\tdocker rm $(BUILD_NAME)\n\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 5.478515625,
          "content": "Software license\n================\n\nCopyright (C) 2017-2021 Molecula Corp. All rights reserved.\n\nThird-party software licenses\n=============================\n\nThe file /lru/lru.go contains a redistribution of lru\n(github.com/golang/groupcache/lru); the license follows:\n\n    Copyright 2013 Google Inc.\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n         http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\nThe file /roaring/btree.go contains a modified redistribution of b\n(https://github.com/cznic/b); the license follows:\n\n    Copyright (c) 2014 The b Authors. All rights reserved.\n\n    Redistribution and use in source and binary forms, with or without\n    modification, are permitted provided that the following conditions are\n    met:\n\n       * Redistributions of source code must retain the above copyright\n    notice, this list of conditions and the following disclaimer.\n       * Redistributions in binary form must reproduce the above\n    copyright notice, this list of conditions and the following disclaimer\n    in the documentation and/or other materials provided with the\n    distribution.\n       * Neither the names of the authors nor the names of the\n    contributors may be used to endorse or promote products derived from\n    this software without specific prior written permission.\n\n    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n    \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n    LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n    A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n    OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n    SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n    LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n    DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n    THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n    (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n    OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nThe file /server/tlsconfig.go contains a modified redistribution of bridge \n(https://github.com/robustirc/bridge); the license follows:\n  \n    Copyright  2014-2015 The RobustIRC Authors. All rights reserved.\n\n    Redistribution and use in source and binary forms, with or without\n    modification, are permitted provided that the following conditions are met:\n\n        * Redistributions of source code must retain the above copyright\n          notice, this list of conditions and the following disclaimer.\n\n        * Redistributions in binary form must reproduce the above copyright\n          notice, this list of conditions and the following disclaimer in the\n          documentation and/or other materials provided with the distribution.\n\n        * Neither the name of RobustIRC nor the names of contributors may be used\n          to endorse or promote products derived from this software without\n          specific prior written permission.\n\n    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n    ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n    WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n    DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE\n    FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n    DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n    SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n    CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n    OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n    OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nThe files /logger/filewriter.go and /logger/filewriter_test.go contain a modified redistribution of reopen (github.com/client9/reopen); the license follows:\n\n    The MIT License (MIT)\n\n    Copyright (c) 2015 Nick Galbreath\n\n    Permission is hereby granted, free of charge, to any person obtaining a copy\n    of this software and associated documentation files (the \"Software\"), to deal\n    in the Software without restriction, including without limitation the rights\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n    copies of the Software, and to permit persons to whom the Software is\n    furnished to do so, subject to the following conditions:\n\n    The above copyright notice and this permission notice shall be included in all\n    copies or substantial portions of the Software.\n\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n    SOFTWARE.\n"
        },
        {
          "name": "OPENSOURCE.md",
          "type": "blob",
          "size": 3.2666015625,
          "content": "## User Contribution Guidelines for FeatureBase\n\nThank you for your interest in contributing to FeatureBase! We appreciate your support in making this open-source project even better. Here are some guidelines to help you get started with contributing to FeatureBase:\n\n1. Familiarize Yourself with the Project:\n   - Visit the FeatureBase website at www.featurebase.com to understand the project's goals, capabilities, and features.\n   - Read the documentation available on the website, including the installation guide, configuration options, and data modeling concepts.\n   - Explore the codebase by cloning the repository and reviewing the source code.\n\n2. Join the Community:\n   - Visit the FeatureBase community page at https://www.featurebase.com/community to learn more about the project's community and how to get involved.\n   - Join the Discord server at https://discord.gg/FBn2vEp7Na to chat with other contributors and users, ask questions, and share your ideas.\n\n3. Set Up Your Development Environment:\n   - Ensure you have Go installed on your machine. Make sure your shell's search path includes the go/bin directory.\n   - Clone the FeatureBase repository or download it as a zip file from the repository's page.\n   - Follow the \"Build FeatureBase Server from source\" instructions in the README file to compile the server binary and the ingester binaries.\n\n4. Choose a Contribution Area:\n   - Identify the area you'd like to contribute to, such as bug fixes, new features, performance improvements, documentation updates, or community support.\n   - Check the issue tracker on the repository or the FeatureBase community for open issues or feature requests that align with your interests and skills. Alternatively, propose your own idea by creating a new issue.\n\n5. Create a New Branch:\n   - Before making any changes, create a new branch in the repository's Git repository. This branch will contain your contributions.\n   - Give your branch a descriptive name that reflects the nature of your contribution.\n\n6. Make Your Changes:\n   - Follow the coding style and conventions used in the existing codebase.\n   - Write clear and concise commit messages for each logical change.\n   - If you're introducing new features or modifying existing behavior, make sure to update the documentation to reflect the changes.\n\n7. Test Your Changes:\n   - Run the existing test suite to ensure that your modifications do not introduce any regressions.\n   - If applicable, write additional tests to cover the changes you made.\n   - Document any new testing procedures required for your contribution.\n\n8. Submitting Your Contribution:\n   - Push your branch to the main repository or create a fork and submit a pull request to the main repository.\n   - Provide a detailed description of your changes, including the problem you solved and the approach you took.\n   - Be responsive to any feedback or suggestions provided by the project maintainers or other contributors.\n   - Once your contribution is approved, it will be reviewed and merged into the main codebase.\n\nPlease note that by contributing to FeatureBase, you agree that your contributions will be licensed under the Apache 2.0 license, which governs the project.\n\nThank you for considering contributing to FeatureBase! Your contributions are valuable and help improve the project for everyone.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.12109375,
          "content": "# FeatureBase Community\n\nFeatureBase Community is now archived and no longer maintained.\n\n* [FeatureBase Community Help](https://github.com/FeatureBaseDB/FB-community-help)\n\n\n\n## Pilosa is now FeatureBase\n\nAs of September 7, 2022, the Pilosa project is now FeatureBase. The core of the project remains the same: FeatureBase is the first real-time distributed database built entirely on bitmaps. (More information about updated capabilities and improvements below.)\n\nFeatureBase delivers low-latency query results, regardless of throughput or query volumes, on fresh data with extreme efficiency. It works because bitmaps are faster, simpler, and far more I/O efficient than traditional column-oriented data formats. With FeatureBase, you can ingest data from batch data sources (e.g. S3, CSV, Snowflake, BigQuery, etc.) and/or streaming data sources (e.g. Kafka/Confluent, Kinesis, Pulsar).\n\nFor more information about FeatureBase, please visit [www.featurebase.com][HomePage].\n\n## Getting Started\n\n* [Learn how to install FeatureBase Community](https://github.com/FeatureBaseDB/FB-community-help/blob/main/docs/community/com-getstart/com-getstart-home.md)\n\n### Build FeatureBase Server from source\n\n0. Install go. Ensure that your shell's search path includes the go/bin directory.\n1. Clone the FeatureBase repository (or download as zip).\n2. In the featurebase directory, run `make install` to compile the FeatureBase server binary. By default, it will be installed in the go/bin directory.\n3. In the idk directory, run `make install` to compile the ingester binaries. By default, they will be installed in the go/bin directory.\n4. Run `featurebase server --handler.allowed-origins=http://localhost:3000` to run FeatureBase server with default settings (learn more about configuring FeatureBase at the link below). The `--handler.allowed-origins` parameter allows the standalone web UI to talk to the server; this can be omitted if the web UI is not needed.\n5. Run `curl localhost:10101/status` to verify the server is running and accessible.\n\n### Data Model\n\nBecause FeatureBase is built on bitmaps, there is bit of a learning curve to grasp how your data is represented. \n\n* [Learn about Data Modeling](https://github.com/FeatureBaseDB/FB-community-help/blob/main/docs/concepts/concepts-home.md)\n\n\n### Ingest Data and Query\n\n* [Learn how to ingest data from multiple data sources](https://github.com/FeatureBaseDB/FB-community-help/blob/main/docs/community/com-ingest/com-ingest-manage.md)\n\n## Community\n\nYou can email us at community@featurebase.com and [learn more about contributing](https://github.com/FeatureBaseDB/featurebase/blob/master/OPENSOURCE.md).\n\nChat with us: [https://discord.gg/FBn2vEp7Na][Discord]\n\n## What's Changed Since the Pilosa Days? \n\nA lot has changed since the days of Pilosa. This list highlights some new capabilites included in FeatureBase. We have also made signficant improvements to the performance, scalability, and stability of the FeatureBase product. \n\n* Query Languages: FeatureBase supports Pilosa Query Language (PQL), as well as SQL\n* Stream and Batch Ingest: Combine real-time data streams with batch historical data and act on it within milliseconds.\n* Mutable: Perform inserts, updates, and deletes at scale, in real time and on-the-fly. This is key for meeting data compliance requirements, and for reflecting the constantly-changing nature of high-volume data.\n* Multi-Valued Set Fields: Store multiple comma-delimited values within a single field while *increasing* query performance of counts, TopKs, etc.\n* Time Quantums: Setting a time quantum on a field creates extra views which allow ranged Row queries down to the time interval specified. For example, if the time quantum is set to YMD, ranged Row queries down to the granularity of a day are supported.\n* RBF storage backend: this is a new compressed bitmap format which improves performance in a number of ways: ACID support on a per shard basis, prevents issues with the number of open files, reduces memory allocation and lock contention for reads, provides more consistent garbage collection, and allows backups to run concurrently with writes. However, because of this change, Pilosa backup files cannot be restored into FeatureBase.\n\n## License\n\nFeatureBase is licensed under the [Apache License, Version 2.0][License]\n\n[Community]: https://github.com/FeatureBaseDB/FB-community-help/tree/main\n[Install]:https://github.com/FeatureBaseDB/FB-community-help/blob/main/docs/community/com-getstart/com-getstart-home.md\n[Config]: https://github.com/FeatureBaseDB/FB-community-help/tree/main/docs/community/com-config\n[DataModel]: https://github.com/FeatureBaseDB/FB-community-help/blob/main/docs/concepts/concepts-home.md\n[Discord]: https://discord.gg/FBn2vEp7Na\n[HomePage]: http://featurebase.com?utm_campaign=Open%20Source&utm_source=GitHub\n[Ingest]: https://github.com/FeatureBaseDB/FB-community-help/blob/main/docs/community/com-ingest/com-ingest-manage.md\n \n[License]: http://www.apache.org/licenses/LICENSE-2.0\n[PQL]: https://docs.featurebase.com/docs/pql-guide/pql-home/?utm_campaign=Open%20Source&utm_source=GitHub\n[SQL]: https://docs.featurebase.com/docs/sql-guide/sql-guide-home/?utm_campaign=Open%20Source&utm_source=GitHub\n"
        },
        {
          "name": "api.go",
          "type": "blob",
          "size": 105.7119140625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\n//go:generate stringer -type=apiMethod\n\npackage pilosa\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/binary\"\n\t\"encoding/csv\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"net/url\"\n\t\"os\"\n\t\"runtime\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\tfbcontext \"github.com/featurebasedb/featurebase/v3/context\"\n\t\"github.com/featurebasedb/featurebase/v3/dax\"\n\t\"github.com/featurebasedb/featurebase/v3/dax/computer\"\n\t\"github.com/featurebasedb/featurebase/v3/dax/storage\"\n\t\"github.com/featurebasedb/featurebase/v3/disco\"\n\t\"github.com/featurebasedb/featurebase/v3/logger\"\n\t\"github.com/featurebasedb/featurebase/v3/rbf\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\n\t//\"github.com/featurebasedb/featurebase/v3/pg\"\n\t\"github.com/featurebasedb/featurebase/v3/pql\"\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\tplanner_types \"github.com/featurebasedb/featurebase/v3/sql3/planner/types\"\n\t\"github.com/featurebasedb/featurebase/v3/tracing\"\n\t\"github.com/pkg/errors\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\n// API provides the top level programmatic interface to Pilosa. It is usually\n// wrapped by a handler which provides an external interface (e.g. HTTP).\ntype API struct {\n\tmu     sync.Mutex\n\tclosed bool // protected by mu\n\n\tholder  *Holder\n\tcluster *cluster\n\tserver  *Server\n\ttracker *queryTracker\n\n\timportWorkersWG      sync.WaitGroup\n\timportWorkerPoolSize int\n\timportWork           chan importJob\n\n\tSerializer Serializer\n\n\tserverlessStorage *storage.ResourceManager\n\n\tdirectiveWorkerPoolSize int\n\n\t// isComputeNode is set to true if this node is running as a DAX compute\n\t// node.\n\tisComputeNode bool\n}\n\nfunc (api *API) Holder() *Holder {\n\treturn api.holder\n}\n\nfunc (api *API) logger() logger.Logger {\n\treturn api.server.logger\n}\n\n// apiOption is a functional option type for pilosa.API\ntype apiOption func(*API) error\n\nfunc OptAPIServer(s *Server) apiOption {\n\treturn func(a *API) error {\n\t\ta.server = s\n\t\ta.holder = s.holder\n\t\ta.cluster = s.cluster\n\t\ta.Serializer = s.serializer\n\t\treturn nil\n\t}\n}\n\nfunc OptAPIServerlessStorage(mm *storage.ResourceManager) apiOption {\n\treturn func(a *API) error {\n\t\ta.serverlessStorage = mm\n\t\treturn nil\n\t}\n}\n\nfunc OptAPIImportWorkerPoolSize(size int) apiOption {\n\treturn func(a *API) error {\n\t\ta.importWorkerPoolSize = size\n\t\treturn nil\n\t}\n}\n\nfunc OptAPIDirectiveWorkerPoolSize(size int) apiOption {\n\treturn func(a *API) error {\n\t\ta.directiveWorkerPoolSize = size\n\t\treturn nil\n\t}\n}\n\nfunc OptAPIIsComputeNode(is bool) apiOption {\n\treturn func(a *API) error {\n\t\ta.isComputeNode = is\n\t\treturn nil\n\t}\n}\n\n// NewAPI returns a new API instance.\nfunc NewAPI(opts ...apiOption) (*API, error) {\n\tapi := &API{\n\t\timportWorkerPoolSize: 2,\n\n\t\tdirectiveWorkerPoolSize: 2,\n\t}\n\n\tfor _, opt := range opts {\n\t\terr := opt(api)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"applying option\")\n\t\t}\n\t}\n\n\tapi.importWork = make(chan importJob, api.importWorkerPoolSize)\n\tfor i := 0; i < api.importWorkerPoolSize; i++ {\n\t\tapi.importWorkersWG.Add(1)\n\t\tgo func() {\n\t\t\timportWorker(api.importWork)\n\t\t\tdefer api.importWorkersWG.Done()\n\t\t}()\n\t}\n\n\tapi.tracker = newQueryTracker(api.server.queryHistoryLength)\n\n\treturn api, nil\n}\n\n// SetAPIOptions applies the given functional options to the API.\nfunc (api *API) SetAPIOptions(opts ...apiOption) error {\n\tfor _, opt := range opts {\n\t\terr := opt(api)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"setting API option\")\n\t\t}\n\t}\n\treturn nil\n}\n\n// validAPIMethods specifies the api methods that are valid for each\n// cluster state.\nvar validAPIMethods = map[disco.ClusterState]map[apiMethod]struct{}{\n\tdisco.ClusterStateNormal:   appendMap(methodsCommon, methodsNormal),\n\tdisco.ClusterStateDegraded: appendMap(methodsCommon, methodsDegraded),\n\tdisco.ClusterStateDown:     methodsCommon,\n}\n\nfunc appendMap(a, b map[apiMethod]struct{}) map[apiMethod]struct{} {\n\tr := make(map[apiMethod]struct{})\n\tfor k, v := range a {\n\t\tr[k] = v\n\t}\n\tfor k, v := range b {\n\t\tr[k] = v\n\t}\n\treturn r\n}\n\nfunc (api *API) validate(f apiMethod) error {\n\tstate, err := api.cluster.State()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting cluster state\")\n\t}\n\tif _, ok := validAPIMethods[state][f]; ok {\n\t\treturn nil\n\t}\n\treturn newAPIMethodNotAllowedError(errors.Errorf(\"api method %s not allowed in state %s\", f, state))\n}\n\n// Close closes the api and waits for it to shutdown.\nfunc (api *API) Close() error {\n\t// only close once\n\tapi.mu.Lock()\n\tdefer api.mu.Unlock()\n\tif api.closed {\n\t\treturn nil\n\t}\n\tapi.closed = true\n\n\tclose(api.importWork)\n\tapi.importWorkersWG.Wait()\n\tapi.tracker.Stop()\n\treturn nil\n}\n\nfunc (api *API) Txf() *TxFactory {\n\treturn api.holder.Txf()\n}\n\n// Query parses a PQL query out of the request and executes it.\nfunc (api *API) Query(ctx context.Context, req *QueryRequest) (QueryResponse, error) {\n\tstart := time.Now()\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"API.Query\")\n\tdefer span.Finish()\n\n\tif err := api.validate(apiQuery); err != nil {\n\t\treturn QueryResponse{}, errors.Wrap(err, \"validating api method\")\n\t}\n\n\tif !req.Remote {\n\t\tdefer api.tracker.Finish(api.tracker.Start(req.Query, req.SQLQuery, api.server.nodeID, req.Index, start))\n\t}\n\n\treturn api.query(ctx, req)\n}\n\n// query provides query functionality for internal use, without tracing, validation, or tracking\nfunc (api *API) query(ctx context.Context, req *QueryRequest) (QueryResponse, error) {\n\tq, err := pql.NewParser(strings.NewReader(req.Query)).Parse()\n\tif err != nil {\n\t\treturn QueryResponse{}, errors.Wrap(err, \"parsing\")\n\t}\n\n\t// TODO can we get rid of exec options and pass the QueryRequest directly to executor?\n\texecOpts := &ExecOptions{\n\t\tRemote:        req.Remote,\n\t\tProfile:       req.Profile,\n\t\tPreTranslated: req.PreTranslated,\n\t\tEmbeddedData:  req.EmbeddedData, // precomputed values that needed to be passed with the request\n\t\tMaxMemory:     req.MaxMemory,\n\t}\n\tresp, err := api.server.executor.Execute(ctx, dax.StringTableKeyer(req.Index), q, req.Shards, execOpts)\n\tif err != nil {\n\t\treturn QueryResponse{}, errors.Wrap(err, \"executing\")\n\t}\n\n\t// Check for an error embedded in the response.\n\tif resp.Err != nil {\n\t\terr = errors.Wrap(resp.Err, \"executing\")\n\t}\n\n\treturn resp, err\n}\n\n// CreateIndex makes a new Pilosa index.\nfunc (api *API) CreateIndex(ctx context.Context, indexName string, options IndexOptions) (*Index, error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.CreateIndex\")\n\tdefer span.Finish()\n\n\t// get the requestUserID from the context -- assumes the http handler has populated this from\n\t// authN/Z info\n\trequestUserID, _ := fbcontext.UserID(ctx) // requestUserID is \"\" if not in ctx\n\n\tif err := api.validate(apiCreateIndex); err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating api method\")\n\t}\n\n\t// Populate the create index message.\n\tts := timestamp()\n\tcim := &CreateIndexMessage{\n\t\tIndex:     indexName,\n\t\tCreatedAt: ts,\n\t\tOwner:     requestUserID,\n\t\tMeta:      options,\n\t}\n\n\t// Create index.\n\tindex, err := api.holder.CreateIndexAndBroadcast(ctx, cim)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating index\")\n\t}\n\n\tCounterCreateIndex.Inc()\n\treturn index, nil\n}\n\n// Index retrieves the named index.\nfunc (api *API) Index(ctx context.Context, indexName string) (*Index, error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.Index\")\n\tdefer span.Finish()\n\n\tif err := api.validate(apiIndex); err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating api method\")\n\t}\n\n\tindex := api.holder.Index(indexName)\n\tif index == nil {\n\t\treturn nil, newNotFoundError(ErrIndexNotFound, indexName)\n\t}\n\treturn index, nil\n}\n\nfunc (api *API) DeleteDataframe(ctx context.Context, indexName string) error {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.DeleteDataframe\")\n\tdefer span.Finish()\n\n\tif err := api.validate(apiDeleteDataframe); err != nil {\n\t\treturn errors.Wrap(err, \"validating api method\")\n\t}\n\t// Delete index from the holder.\n\terr := api.holder.DeleteDataframe(indexName)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"deleting index\")\n\t}\n\t// Send the delete index message to all nodes.\n\terr = api.server.SendSync(\n\t\t&DeleteDataframeMessage{\n\t\t\tIndex: indexName,\n\t\t})\n\tif err != nil {\n\t\tapi.server.logger.Errorf(\"problem sending DeleteIndex message: %s\", err)\n\t\treturn errors.Wrap(err, \"sending DeleteIndex message\")\n\t}\n\tCounterDeleteDataframe.Inc()\n\treturn nil\n}\n\n// DeleteIndex removes the named index. If the index is not found it does\n// nothing and returns no error.\nfunc (api *API) DeleteIndex(ctx context.Context, indexName string) error {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.DeleteIndex\")\n\tdefer span.Finish()\n\n\tif err := api.validate(apiDeleteIndex); err != nil {\n\t\treturn errors.Wrap(err, \"validating api method\")\n\t}\n\n\t// Delete index from the holder.\n\terr := api.holder.DeleteIndex(indexName)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"deleting index\")\n\t}\n\n\t// Remove from writelogger/snapshotter if serverless.\n\tif api.isComputeNode {\n\t\tif err := api.serverlessStorage.RemoveTable(dax.TableKey(indexName).QualifiedTableID()); err != nil {\n\t\t\treturn errors.Wrapf(err, \"removing table from serverless storage: %s\", indexName)\n\t\t}\n\t}\n\n\t// Send the delete index message to all nodes.\n\terr = api.server.SendSync(\n\t\t&DeleteIndexMessage{\n\t\t\tIndex: indexName,\n\t\t})\n\tif err != nil {\n\t\tapi.server.logger.Errorf(\"problem sending DeleteIndex message: %s\", err)\n\t\treturn errors.Wrap(err, \"sending DeleteIndex message\")\n\t}\n\t// Delete ids allocated for index if any present\n\tsnap := api.cluster.NewSnapshot()\n\tif snap.IsPrimaryFieldTranslationNode(api.NodeID()) {\n\t\tif err := api.holder.ida.reset(indexName); err != nil {\n\t\t\treturn errors.Wrap(err, \"deleting id allocation for index\")\n\t\t}\n\t}\n\tCounterDeleteIndex.Inc()\n\treturn nil\n}\n\n// CreateField makes the named field in the named index with the given options.\n//\n// The resulting field will always have TrackExistence set.\nfunc (api *API) CreateField(ctx context.Context, indexName string, fieldName string, opts ...FieldOption) (*Field, error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.CreateField\")\n\tdefer span.Finish()\n\n\tif err := api.validate(apiCreateField); err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating api method\")\n\t}\n\n\t// get the requestUserID from the context -- assumes the http handler has populated this from\n\t// authN/Z info\n\trequestUserID, _ := fbcontext.UserID(ctx) // requestUserID is \"\" if not in ctx\n\n\t// newFieldOptions is also used in the path through the index creating\n\t// a field from an update from DAX, so it can't assume it can always\n\t// override this. But we're the call path for creating new fields, and\n\t// new fields should always have TrackExistence on.\n\topts = append(opts, OptFieldTrackExistence())\n\t// Apply and validate functional options.\n\tfo, err := newFieldOptions(opts...)\n\tif err != nil {\n\t\treturn nil, NewBadRequestError(errors.Wrap(err, \"applying option\"))\n\t}\n\n\t// Find index.\n\tindex := api.holder.Index(indexName)\n\tif index == nil {\n\t\treturn nil, newNotFoundError(ErrIndexNotFound, indexName)\n\t}\n\n\t// Populate the create field message.\n\tcfm := &CreateFieldMessage{\n\t\tIndex:     indexName,\n\t\tField:     fieldName,\n\t\tCreatedAt: timestamp(),\n\t\tOwner:     requestUserID,\n\t\tMeta:      fo,\n\t}\n\n\t// Create field.\n\tfield, err := index.CreateField(fieldName, requestUserID, opts...)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating field\")\n\t}\n\n\t// Send the create field message to all nodes. We do this *outside* the\n\t// CreateField logic so we're not blocking on it.\n\tif err := api.holder.sendOrSpool(cfm); err != nil {\n\t\treturn nil, errors.Wrap(err, \"sending CreateField message\")\n\t}\n\n\tCounterCreateField.With(prometheus.Labels{\"index\": indexName})\n\treturn field, nil\n}\n\n// FieldUpdate represents a change to a field. The thinking is to only\n// support changing one field option at a time to keep the\n// implementation sane. At time of writing, only TTL is supported.\ntype FieldUpdate struct {\n\tOption string `json:\"option\"`\n\tValue  string `json:\"value\"`\n}\n\nfunc (api *API) UpdateField(ctx context.Context, indexName, fieldName string, update FieldUpdate) error {\n\t// Find index.\n\tindex := api.holder.Index(indexName)\n\tif index == nil {\n\t\treturn newNotFoundError(ErrIndexNotFound, indexName)\n\t}\n\n\t// get the requestUserID from the context -- assumes the http handler has populated this from\n\t// authN/Z info\n\trequestUserID, _ := fbcontext.UserID(ctx)\n\n\tcfm, err := index.UpdateField(ctx, fieldName, requestUserID, update)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"updating field\")\n\t}\n\n\tif err := index.UpdateFieldLocal(cfm, update); err != nil {\n\t\treturn errors.Wrap(err, \"updating field locally\")\n\t}\n\n\t// broadcast field update\n\terr = api.holder.sendOrSpool(&UpdateFieldMessage{\n\t\tCreateFieldMessage: *cfm,\n\t\tUpdate:             update,\n\t})\n\treturn errors.Wrap(err, \"sending UpdateField message\")\n}\n\n// Field retrieves the named field.\nfunc (api *API) Field(ctx context.Context, indexName, fieldName string) (*Field, error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.Field\")\n\tdefer span.Finish()\n\n\tif err := api.validate(apiField); err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating api method\")\n\t}\n\n\tfield := api.holder.Field(indexName, fieldName)\n\tif field == nil {\n\t\treturn nil, newNotFoundError(ErrFieldNotFound, fieldName)\n\t}\n\treturn field, nil\n}\n\nfunc setUpImportOptions(opts ...ImportOption) (*ImportOptions, error) {\n\toptions := &ImportOptions{}\n\tfor _, opt := range opts {\n\t\terr := opt(options)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"applying option\")\n\t\t}\n\t}\n\treturn options, nil\n}\n\ntype importJob struct {\n\tctx     context.Context\n\tqcx     *Qcx\n\treq     *ImportRoaringRequest\n\tshard   uint64\n\tfield   *Field\n\terrChan chan error\n}\n\nfunc importWorker(importWork chan importJob) {\n\tfor j := range importWork {\n\t\terr := func() (err0 error) {\n\t\t\tfor viewName, viewData := range j.req.Views {\n\t\t\t\tviewName, err0 = j.field.cleanupViewName(viewName)\n\t\t\t\tif err0 != nil {\n\t\t\t\t\treturn err0\n\t\t\t\t}\n\t\t\t\tif len(viewData) == 0 {\n\t\t\t\t\treturn fmt.Errorf(\"no data to import for view: %s\", viewName)\n\t\t\t\t}\n\n\t\t\t\t// TODO: deprecate ImportRoaringRequest.Clear, but\n\t\t\t\t// until we do, we need to check its value to provide\n\t\t\t\t// backward compatibility.\n\t\t\t\tdoAction := j.req.Action\n\t\t\t\tif doAction == \"\" {\n\t\t\t\t\tif j.req.Clear {\n\t\t\t\t\t\tdoAction = RequestActionClear\n\t\t\t\t\t} else {\n\t\t\t\t\t\tdoAction = RequestActionSet\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif err := func() (err1 error) {\n\t\t\t\t\ttx, finisher, err := j.qcx.GetTx(Txo{Write: writable, Index: j.field.idx, Shard: j.shard})\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\t\tdefer finisher(&err1)\n\n\t\t\t\t\tvar doClear bool\n\t\t\t\t\tswitch doAction {\n\t\t\t\t\tcase RequestActionOverwrite:\n\t\t\t\t\t\terr := j.field.importRoaringOverwrite(j.ctx, tx, viewData, j.shard, viewName, j.req.Block)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn errors.Wrap(err, \"importing roaring as overwrite\")\n\t\t\t\t\t\t}\n\t\t\t\t\tcase RequestActionClear:\n\t\t\t\t\t\tdoClear = true\n\t\t\t\t\t\tfallthrough\n\t\t\t\t\tcase RequestActionSet:\n\t\t\t\t\t\tfileMagic := uint32(binary.LittleEndian.Uint16(viewData[0:2]))\n\t\t\t\t\t\tdata := viewData\n\t\t\t\t\t\tif fileMagic != roaring.MagicNumber {\n\t\t\t\t\t\t\t// if the view data arrives is in the \"standard\" roaring format, we must\n\t\t\t\t\t\t\t// make a copy of data in order allow for the conversion to the pilosa roaring run format\n\t\t\t\t\t\t\t// in field.importRoaring\n\t\t\t\t\t\t\tdata = make([]byte, len(viewData))\n\t\t\t\t\t\t\tcopy(data, viewData)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif j.req.UpdateExistence {\n\t\t\t\t\t\t\tif ef := j.field.idx.existenceField(); ef != nil {\n\t\t\t\t\t\t\t\texistence, err := combineForExistence(data)\n\t\t\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\t\t\treturn errors.Wrap(err, \"merging existence on roaring import\")\n\t\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t\terr = ef.importRoaring(j.ctx, tx, existence, j.shard, \"standard\", false)\n\t\t\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\t\t\treturn errors.Wrap(err, \"updating existence on roaring import\")\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\terr := j.field.importRoaring(j.ctx, tx, data, j.shard, viewName, doClear)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn errors.Wrap(err, \"importing standard roaring\")\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\treturn nil\n\t\t\t\t}(); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\t}()\n\n\t\tselect {\n\t\tcase <-j.ctx.Done():\n\t\tcase j.errChan <- err:\n\t\t}\n\t}\n}\n\n// combineForExistence unions all rows in the fragment to be imported into a single row to update the existence field. TODO: It would probably be more efficient to only unmarshal the input data once, and use the calculated existence Bitmap directly rather than returning it to bytes, but most of our ingest paths update existence separately, so it's more important that this just be obviously correct at the moment.\nfunc combineForExistence(inputRoaringData []byte) ([]byte, error) {\n\trowSize := uint64(1 << shardVsContainerExponent)\n\trit, err := roaring.NewRoaringIterator(inputRoaringData)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbm := roaring.NewBitmap()\n\terr = bm.MergeRoaringRawIteratorIntoExists(rit, rowSize)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbuf := new(bytes.Buffer)\n\n\t_, err = bm.WriteTo(buf)\n\treturn buf.Bytes(), err\n}\n\n// ImportRoaring is a low level interface for importing data to Pilosa when\n// extremely high throughput is desired. The data must be encoded in a\n// particular way which may be unintuitive (discussed below). The data is merged\n// with existing data.\n//\n// It takes as input a roaring bitmap which it uses as the data for the\n// indicated index, field, and shard. The bitmap may be encoded according to the\n// official roaring spec (https://github.com/RoaringBitmap/RoaringFormatSpec),\n// or to the pilosa roaring spec which supports 64 bit integers\n// (https://www.pilosa.com/docs/latest/architecture/#roaring-bitmap-storage-format).\n//\n// The data should be encoded the same way that Pilosa stores fragments\n// internally. A bit \"i\" being set in the input bitmap indicates that the bit is\n// set in Pilosa row \"i/ShardWidth\", and in column\n// (shard*ShardWidth)+(i%ShardWidth). That is to say that \"data\" represents all\n// of the rows in this shard of this field concatenated together in one long\n// bitmap.\nfunc (api *API) ImportRoaring(ctx context.Context, indexName, fieldName string, shard uint64, remote bool, req *ImportRoaringRequest) (err0 error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"API.ImportRoaring\")\n\tspan.LogKV(\"index\", indexName, \"field\", fieldName)\n\tdefer span.Finish()\n\n\tif err := api.validate(apiField); err != nil {\n\t\treturn errors.Wrap(err, \"validating api method\")\n\t}\n\n\tapi.server.logger.Debugf(\"ImportRoaring: %v %v %v\", indexName, fieldName, shard)\n\tindex, field, err := api.indexField(indexName, fieldName, shard)\n\tif index == nil || field == nil {\n\t\treturn err\n\t}\n\n\t// This node only handles the shard(s) that it owns.\n\tif api.isComputeNode {\n\t\tdirective := api.holder.Directive()\n\t\tif !shardInShards(dax.ShardNum(shard), directive.ComputeShards(dax.TableKey(index.Name()))) {\n\t\t\treturn errors.Errorf(\"import request shard is not supported (roaring): %d\", shard)\n\t\t}\n\t}\n\n\tif err = req.ValidateWithTimestamp(index.CreatedAt(), field.CreatedAt()); err != nil {\n\t\treturn newPreconditionFailedError(err)\n\t}\n\n\tqcx := api.Txf().NewQcx()\n\tdefer qcx.Abort()\n\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := api.cluster.NewSnapshot()\n\n\tnodes := snap.ShardNodes(indexName, shard)\n\terrCh := make(chan error, len(nodes))\n\tfor _, node := range nodes {\n\t\tnode := node\n\t\tif node.ID == api.server.nodeID {\n\t\t\tapi.importWork <- importJob{\n\t\t\t\tctx:     ctx,\n\t\t\t\tqcx:     qcx,\n\t\t\t\treq:     req,\n\t\t\t\tshard:   shard,\n\t\t\t\tfield:   field,\n\t\t\t\terrChan: errCh,\n\t\t\t}\n\t\t} else if !remote { // if remote == true we don't forward to other nodes\n\t\t\t// forward it on\n\t\t\tgo func() {\n\t\t\t\terrCh <- api.server.defaultClient.ImportRoaring(ctx, &node.URI, indexName, fieldName, shard, true, req)\n\t\t\t}()\n\t\t} else {\n\t\t\terrCh <- nil\n\t\t}\n\t}\n\n\tvar maxNode int\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\t// defered tx.Rollback() happens automatically here.\n\t\t\treturn ctx.Err()\n\t\tcase nodeErr := <-errCh:\n\t\t\tif nodeErr != nil {\n\t\t\t\t// defered tx.Rollback() happens automatically here.\n\t\t\t\treturn nodeErr\n\t\t\t}\n\t\t\tmaxNode++\n\t\t}\n\n\t\t// Exit once all nodes are processed.\n\t\tif maxNode == len(nodes) {\n\t\t\tif api.isComputeNode && !req.SuppressLog {\n\t\t\t\t// Write the request to the write logger.\n\t\t\t\tpartition := disco.ShardToShardPartition(indexName, shard, disco.DefaultPartitionN)\n\t\t\t\tmsg := &computer.ImportRoaringMessage{\n\t\t\t\t\tTable:           indexName,\n\t\t\t\t\tField:           fieldName,\n\t\t\t\t\tPartition:       partition,\n\t\t\t\t\tShard:           shard,\n\t\t\t\t\tClear:           req.Clear,\n\t\t\t\t\tAction:          req.Action,\n\t\t\t\t\tBlock:           req.Block,\n\t\t\t\t\tUpdateExistence: req.UpdateExistence,\n\t\t\t\t\tViews:           req.Views,\n\t\t\t\t}\n\n\t\t\t\ttkey := dax.TableKey(indexName)\n\t\t\t\tqtid := tkey.QualifiedTableID()\n\t\t\t\tpartitionNum := dax.PartitionNum(partition)\n\t\t\t\tshardNum := dax.ShardNum(shard)\n\n\t\t\t\tb, err := computer.MarshalLogMessage(msg, computer.EncodeTypeJSON)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errors.Wrap(err, \"marshalling log message\")\n\t\t\t\t}\n\n\t\t\t\tresource := api.serverlessStorage.GetShardResource(qtid, partitionNum, shardNum)\n\t\t\t\terr = resource.Append(b)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errors.Wrap(err, \"appending shard data\") // TODO do we need to set err0 or something?\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn qcx.Finish()\n\t\t}\n\t}\n}\n\n// DeleteField removes the named field from the named index. If the index is not\n// found, an error is returned. If the field is not found, it is ignored and no\n// action is taken.\nfunc (api *API) DeleteField(ctx context.Context, indexName string, fieldName string) error {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.DeleteField\")\n\tdefer span.Finish()\n\n\tif err := api.validate(apiDeleteField); err != nil {\n\t\treturn errors.Wrap(err, \"validating api method\")\n\t}\n\n\t// Find index.\n\tindex := api.holder.Index(indexName)\n\tif index == nil {\n\t\treturn newNotFoundError(ErrIndexNotFound, indexName)\n\t}\n\n\t// Delete field from the index.\n\tif err := index.DeleteField(fieldName); err != nil {\n\t\treturn errors.Wrap(err, \"deleting field\")\n\t}\n\n\t// Send the delete field message to all nodes.\n\terr := api.server.SendSync(\n\t\t&DeleteFieldMessage{\n\t\t\tIndex: indexName,\n\t\t\tField: fieldName,\n\t\t})\n\tif err != nil {\n\t\tapi.server.logger.Errorf(\"problem sending DeleteField message: %s\", err)\n\t\treturn errors.Wrap(err, \"sending DeleteField message\")\n\t}\n\tCounterDeleteField.With(prometheus.Labels{\"index\": indexName})\n\treturn nil\n}\n\n// DeleteAvailableShard a shard ID from the available shard set cache.\nfunc (api *API) DeleteAvailableShard(_ context.Context, indexName, fieldName string, shardID uint64) error {\n\tif err := api.validate(apiDeleteAvailableShard); err != nil {\n\t\treturn errors.Wrap(err, \"validating api method\")\n\t}\n\n\t// Find field.\n\tfield := api.holder.Field(indexName, fieldName)\n\tif field == nil {\n\t\treturn newNotFoundError(ErrFieldNotFound, fieldName)\n\t}\n\n\t// Delete shard from the cache.\n\tif err := field.RemoveAvailableShard(shardID); err != nil {\n\t\treturn errors.Wrap(err, \"deleting available shard\")\n\t}\n\n\t// Send the delete shard message to all nodes.\n\terr := api.server.SendSync(\n\t\t&DeleteAvailableShardMessage{\n\t\t\tIndex:   indexName,\n\t\t\tField:   fieldName,\n\t\t\tShardID: shardID,\n\t\t})\n\tif err != nil {\n\t\tapi.server.logger.Errorf(\"problem sending DeleteAvailableShard message: %s\", err)\n\t\treturn errors.Wrap(err, \"sending DeleteAvailableShard message\")\n\t}\n\tCounterDeleteAvailableShard.With(prometheus.Labels{\"index\": indexName}).Inc()\n\treturn nil\n}\n\n// ExportCSV encodes the fragment designated by the index,field,shard as\n// CSV of the form <row>,<col>\nfunc (api *API) ExportCSV(ctx context.Context, indexName string, fieldName string, shard uint64, w io.Writer) error {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.ExportCSV\")\n\tdefer span.Finish()\n\n\tif err := api.validate(apiExportCSV); err != nil {\n\t\treturn errors.Wrap(err, \"validating api method\")\n\t}\n\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := api.cluster.NewSnapshot()\n\n\t// Validate that this handler owns the shard.\n\tif !snap.OwnsShard(api.NodeID(), indexName, shard) {\n\t\tapi.server.logger.Errorf(\"node %s does not own shard %d of index %s\", api.NodeID(), shard, indexName)\n\t\treturn ErrClusterDoesNotOwnShard\n\t}\n\n\t// Find index.\n\tindex := api.holder.Index(indexName)\n\tif index == nil {\n\t\treturn newNotFoundError(ErrIndexNotFound, indexName)\n\t}\n\n\t// Find field from the index.\n\tfield := index.Field(fieldName)\n\tif field == nil {\n\t\treturn newNotFoundError(ErrFieldNotFound, fieldName)\n\t}\n\n\t// Find the fragment.\n\tf := api.holder.fragment(indexName, fieldName, viewStandard, shard)\n\tif f == nil {\n\t\treturn ErrFragmentNotFound\n\t}\n\n\t// Obtain transaction\n\ttx := index.holder.txf.NewTx(Txo{Write: !writable, Index: index, Shard: shard})\n\tdefer tx.Rollback()\n\n\t// Wrap writer with a CSV writer.\n\tcw := csv.NewWriter(w)\n\n\t// Define the function to write each bit as a string,\n\t// translating to keys where necessary.\n\tvar n int\n\tfn := func(rowID, columnID uint64) error {\n\t\tvar rowStr string\n\t\tvar colStr string\n\t\tvar err error\n\n\t\tif field.Keys() {\n\t\t\t// TODO: handle case: field.ForeignIndex\n\t\t\tif rowStr, err = field.TranslateStore().TranslateID(rowID); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"translating row\")\n\t\t\t}\n\t\t} else {\n\t\t\trowStr = strconv.FormatUint(rowID, 10)\n\t\t}\n\n\t\tif index.Keys() {\n\t\t\tif store := index.TranslateStore(snap.IDToShardPartition(indexName, columnID)); store == nil {\n\t\t\t\treturn errors.Wrap(err, \"partition does not exist\")\n\t\t\t} else if colStr, err = store.TranslateID(columnID); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"translating column\")\n\t\t\t}\n\t\t} else {\n\t\t\tcolStr = strconv.FormatUint(columnID, 10)\n\t\t}\n\n\t\tn++\n\t\treturn cw.Write([]string{rowStr, colStr})\n\t}\n\n\tciter, _, err := tx.ContainerIterator(indexName, fieldName, viewStandard, shard, 0)\n\tif err != nil {\n\t\treturn err\n\t}\n\tvar row, hi uint64\n\tvar failed error\n\tprocess := func(u uint16) {\n\t\tif err := fn(row, hi|uint64(u)); err != nil {\n\t\t\tfailed = err\n\t\t}\n\t}\n\tfor citer.Next() {\n\t\tkey, c := citer.Value()\n\t\thi = key << 16\n\t\trow, hi = (hi / ShardWidth), (shard*ShardWidth)+(hi%ShardWidth)\n\t\troaring.ContainerCallback(c, process)\n\t\tif failed != nil {\n\t\t\treturn errors.Wrap(err, \"writing CSV\")\n\t\t}\n\t}\n\n\t// Ensure data is flushed.\n\tcw.Flush()\n\tspan.LogKV(\"n\", n)\n\ttx.Rollback()\n\treturn nil\n}\n\n// ShardNodes returns the node and all replicas which should contain a shard's data.\nfunc (api *API) ShardNodes(ctx context.Context, indexName string, shard uint64) ([]*disco.Node, error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.ShardNodes\")\n\tdefer span.Finish()\n\n\tif err := api.validate(apiShardNodes); err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating api method\")\n\t}\n\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := api.cluster.NewSnapshot()\n\n\treturn snap.ShardNodes(indexName, shard), nil\n}\n\n// PartitionNodes returns the node and all replicas which should contain a partition key data.\nfunc (api *API) PartitionNodes(ctx context.Context, partitionID int) ([]*disco.Node, error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.PartitionNodes\")\n\tdefer span.Finish()\n\n\tif err := api.validate(apiPartitionNodes); err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating api method\")\n\t}\n\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := api.cluster.NewSnapshot()\n\n\treturn snap.PartitionNodes(partitionID), nil\n}\n\n// FragmentData returns all data in the specified fragment.\nfunc (api *API) FragmentData(ctx context.Context, indexName, fieldName, viewName string, shard uint64) (io.WriterTo, error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.FragmentData\")\n\tdefer span.Finish()\n\n\tif err := api.validate(apiFragmentData); err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating api method\")\n\t}\n\n\t// Retrieve fragment from holder.\n\tf := api.holder.fragment(indexName, fieldName, viewName, shard)\n\tif f == nil {\n\t\treturn nil, ErrFragmentNotFound\n\t}\n\treturn f, nil\n}\n\ntype RedirectError struct {\n\tHostPort string\n\terror    string\n}\n\nfunc (r RedirectError) Error() string {\n\treturn r.error\n}\n\n// TranslateData returns all translation data in the specified partition.\nfunc (api *API) TranslateData(ctx context.Context, indexName string, partition int) (TranslateStore, error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.TranslateData\")\n\tdefer span.Finish()\n\n\tif err := api.validate(apiTranslateData); err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating api method\")\n\t}\n\n\t// Retrieve index from holder.\n\tidx := api.holder.Index(indexName)\n\tif idx == nil {\n\t\treturn nil, newNotFoundError(ErrIndexNotFound, indexName)\n\t}\n\n\t// Find the node that can service the request.\n\tsnap := api.cluster.NewSnapshot()\n\tnodes := snap.PartitionNodes(partition)\n\tvar upNode *disco.Node\n\tfor _, node := range nodes {\n\t\t// we all UNKNOWN state here because we often mistakenly think\n\t\t// a node is not up under heavy load, but prefer STARTED if we\n\t\t// find one.\n\t\tif node.State == disco.NodeStateStarted {\n\t\t\tupNode = node\n\t\t\tbreak\n\t\t} else if node.State == disco.NodeStateUnknown {\n\t\t\tif upNode != nil {\n\t\t\t\tupNode = node\n\t\t\t}\n\t\t}\n\t}\n\n\t// If there is no upNode, then we can't service the request.\n\tif upNode == nil {\n\t\treturn nil, fmt.Errorf(\"can't get translate data, no nodes available for partition %d\", partition)\n\t}\n\n\t// If we're not the upNode, we need to redirect to it.\n\tif upNode.ID != api.server.NodeID() {\n\t\treturn nil, RedirectError{\n\t\t\tHostPort: upNode.URI.HostPort(),\n\t\t\terror:    fmt.Sprintf(\"can't translate data, this node(%s) does not partition %d\", api.server.uri, partition),\n\t\t}\n\t}\n\n\t// We are the upNode!\n\tstore := idx.TranslateStore(partition)\n\tif store == nil {\n\t\treturn nil, ErrTranslateStoreNotFound\n\t}\n\n\treturn store, nil\n}\n\n// FieldTranslateData returns all translation data in the specified field.\nfunc (api *API) FieldTranslateData(ctx context.Context, indexName, fieldName string) (TranslateStore, error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.FieldTranslateData\")\n\tdefer span.Finish()\n\tif err := api.validate(apiFieldTranslateData); err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating api method\")\n\t}\n\n\t// Retrieve index from holder.\n\tidx := api.holder.Index(indexName)\n\tif idx == nil {\n\t\treturn nil, newNotFoundError(ErrIndexNotFound, indexName)\n\t}\n\n\t// Retrieve field from index.\n\tfield := idx.Field(fieldName)\n\tif field == nil {\n\t\treturn nil, newNotFoundError(ErrFieldNotFound, fieldName)\n\t}\n\n\t// Retrieve translatestore from holder.\n\tstore := field.TranslateStore()\n\tif store == nil {\n\t\treturn nil, ErrTranslateStoreNotFound\n\t}\n\treturn store, nil\n}\n\n// Hosts returns a list of the hosts in the cluster including their ID,\n// URL, and which is the primary.\nfunc (api *API) Hosts(ctx context.Context) []*disco.Node {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.Hosts\")\n\tdefer span.Finish()\n\treturn api.cluster.Nodes()\n}\n\n// Node gets the ID, URI and primary status for this particular node.\nfunc (api *API) Node() *disco.Node {\n\treturn api.server.node()\n}\n\n// NodeID gets the ID alone, so it doesn't have to do a complete lookup\n// of the node, searching by its ID, to return the ID it searched for.\nfunc (api *API) NodeID() string {\n\treturn api.server.nodeID\n}\n\n// PrimaryNode returns the primary node for the cluster.\nfunc (api *API) PrimaryNode() *disco.Node {\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := api.cluster.NewSnapshot()\n\treturn snap.PrimaryFieldTranslationNode()\n}\n\n// RecalculateCaches forces all TopN caches to be updated.\n// This is done internally within a TopN query, but a user may want to do it ahead of time?\nfunc (api *API) RecalculateCaches(ctx context.Context) error {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.RecalculateCaches\")\n\tdefer span.Finish()\n\n\tif err := api.validate(apiRecalculateCaches); err != nil {\n\t\treturn errors.Wrap(err, \"validating api method\")\n\t}\n\n\terr := api.server.SendSync(&RecalculateCaches{})\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"broacasting message\")\n\t}\n\tapi.holder.recalculateCaches()\n\treturn nil\n}\n\n// ClusterMessage is for internal use. It decodes a protobuf message out of\n// the body and forwards it to the BroadcastHandler.\nfunc (api *API) ClusterMessage(ctx context.Context, reqBody io.Reader) error {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.ClusterMessage\")\n\tdefer span.Finish()\n\n\tif err := api.validate(apiClusterMessage); err != nil {\n\t\treturn errors.Wrap(err, \"validating api method\")\n\t}\n\n\t// Read entire body.\n\tbody, err := io.ReadAll(reqBody)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"reading body\")\n\t}\n\n\ttyp := body[0]\n\tmsg := getMessage(typ)\n\terr = api.server.serializer.Unmarshal(body[1:], msg)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"deserializing cluster message\")\n\t}\n\n\t// Forward the message.\n\tif err := api.server.receiveMessage(msg); err != nil {\n\t\treturn MessageProcessingError{err}\n\t}\n\treturn nil\n}\n\n// MessageProcessingError is an error indicating that a cluster message could not be processed.\ntype MessageProcessingError struct {\n\tErr error\n}\n\nfunc (err MessageProcessingError) Error() string {\n\treturn \"processing message: \" + err.Err.Error()\n}\n\n// Cause allows the error to be unwrapped.\nfunc (err MessageProcessingError) Cause() error {\n\treturn err.Err\n}\n\n// Unwrap allows the error to be unwrapped.\nfunc (err MessageProcessingError) Unwrap() error {\n\treturn err.Err\n}\n\n// Schema returns information about each index in Pilosa including which fields\n// they contain.\nfunc (api *API) Schema(ctx context.Context, withViews bool) ([]*IndexInfo, error) {\n\tif err := api.validate(apiSchema); err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating api method\")\n\t}\n\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.Schema\")\n\tdefer span.Finish()\n\n\tif withViews {\n\t\treturn api.holder.Schema()\n\t}\n\n\treturn api.holder.limitedSchema()\n}\n\n// IndexInfo returns the same information as Schema(), but only for a single\n// index.\nfunc (api *API) IndexInfo(ctx context.Context, name string) (*IndexInfo, error) {\n\tschema, err := api.Schema(ctx, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor _, idx := range schema {\n\t\tif idx.Name == name {\n\t\t\treturn idx, nil\n\t\t}\n\t}\n\n\treturn nil, ErrIndexNotFound\n}\n\n// FieldInfo returns the same information as Schema(), but only for a single\n// field.\nfunc (api *API) FieldInfo(ctx context.Context, indexName, fieldName string) (*FieldInfo, error) {\n\tidx, err := api.IndexInfo(ctx, indexName)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfld := idx.Field(fieldName)\n\tif fld == nil {\n\t\treturn nil, ErrFieldNotFound\n\t}\n\n\treturn fld, nil\n}\n\n// ApplySchema takes the given schema and applies it across the\n// cluster (if remote is false), or just to this node (if remote is\n// true). This is designed for the use case of replicating a schema\n// from one Pilosa cluster to another which is initially empty. It is\n// not officially supported in other scenarios and may produce\n// surprising results.\nfunc (api *API) ApplySchema(ctx context.Context, s *Schema, remote bool) error {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.ApplySchema\")\n\tdefer span.Finish()\n\n\tif err := api.validate(apiApplySchema); err != nil {\n\t\treturn errors.Wrap(err, \"validating api method\")\n\t}\n\n\terr := api.holder.applySchema(s)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"applying schema\")\n\t}\n\n\treturn nil\n}\n\n// Views returns the views in the given field.\nfunc (api *API) Views(ctx context.Context, indexName string, fieldName string) ([]*view, error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.Views\")\n\tdefer span.Finish()\n\n\tif err := api.validate(apiViews); err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating api method\")\n\t}\n\n\t// Retrieve views.\n\tf := api.holder.Field(indexName, fieldName)\n\tif f == nil {\n\t\treturn nil, newNotFoundError(ErrFieldNotFound, fieldName)\n\t}\n\n\t// Fetch views.\n\tviews := f.views()\n\treturn views, nil\n}\n\n// DeleteView removes the given view.\nfunc (api *API) DeleteView(ctx context.Context, indexName string, fieldName string, viewName string) error {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.DeleteView\")\n\tdefer span.Finish()\n\n\tif err := api.validate(apiDeleteView); err != nil {\n\t\treturn errors.Wrap(err, \"validating api method\")\n\t}\n\n\t// Retrieve field.\n\tf := api.holder.Field(indexName, fieldName)\n\tif f == nil {\n\t\treturn newNotFoundError(ErrFieldNotFound, fieldName)\n\t}\n\n\t// Delete the view.\n\tif err := f.deleteView(viewName); err != nil {\n\t\t// Ignore this error because views do not exist on all nodes due to shard distribution.\n\t\tif err != ErrInvalidView {\n\t\t\treturn errors.Wrap(err, \"deleting view\")\n\t\t}\n\t}\n\n\t// Send the delete view message to all nodes.\n\terr := api.server.SendSync(\n\t\t&DeleteViewMessage{\n\t\t\tIndex: indexName,\n\t\t\tField: fieldName,\n\t\t\tView:  viewName,\n\t\t})\n\tif err != nil {\n\t\tapi.server.logger.Errorf(\"problem sending DeleteView message: %s\", err)\n\t}\n\n\treturn errors.Wrap(err, \"sending DeleteView message\")\n}\n\n// IndexShardSnapshot returns a reader that contains the contents of\n// an RBF snapshot for an index/shard. When snapshotting for\n// serverless, we need to be able to transactionally move the write\n// log to the new version, so we expose writeTx to allow the caller to\n// request a write transaction for the snapshot even though we'll just\n// be reading inside RBF.\nfunc (api *API) IndexShardSnapshot(ctx context.Context, indexName string, shard uint64, writeTx bool) (io.ReadCloser, error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.IndexShardSnapshot\")\n\tdefer span.Finish()\n\n\t// Find index.\n\tindex := api.holder.Index(indexName)\n\tif index == nil {\n\t\treturn nil, newNotFoundError(ErrIndexNotFound, indexName)\n\t}\n\n\t// Start transaction.\n\ttx := index.holder.txf.NewTx(Txo{Index: index, Shard: shard, Write: writeTx})\n\n\t// Ensure transaction is an RBF transaction.\n\trtx, ok := tx.(*RBFTx)\n\tif !ok {\n\t\ttx.Rollback()\n\t\treturn nil, fmt.Errorf(\"snapshot not available for %q storage\", tx.Type())\n\t}\n\n\tr, err := rtx.SnapshotReader()\n\tif err != nil {\n\t\ttx.Rollback()\n\t\treturn nil, err\n\t}\n\treturn &txReadCloser{tx: tx, Reader: r}, nil\n}\n\nvar _ io.ReadCloser = (*txReadCloser)(nil)\n\n// txReadCloser wraps a reader to close a tx on close.\ntype txReadCloser struct {\n\tio.Reader\n\ttx Tx\n}\n\nfunc (r *txReadCloser) Close() error {\n\tr.tx.Rollback()\n\treturn nil\n}\n\n// ImportOptions holds the options for the API.Import\n// method.\n//\n// TODO(2.0) we have entirely missed the point of functional options\n// by exporting this structure. If it needs to be exported for some\n// reason, we should consider not using functional options here which\n// just adds complexity.\ntype ImportOptions struct {\n\tClear          bool\n\tIgnoreKeyCheck bool\n\tPresorted      bool\n\tsuppressLog    bool\n\n\t// test Tx atomicity if > 0\n\tSimPowerLossAfter int\n}\n\n// ImportOption is a functional option type for API.Import.\ntype ImportOption func(*ImportOptions) error\n\n// OptImportOptionsClear is a functional option on ImportOption\n// used to specify whether the import is a set or clear operation.\nfunc OptImportOptionsClear(c bool) ImportOption {\n\treturn func(o *ImportOptions) error {\n\t\to.Clear = c\n\t\treturn nil\n\t}\n}\n\n// OptImportOptionsIgnoreKeyCheck is a functional option on ImportOption\n// used to specify whether key check should be ignored.\nfunc OptImportOptionsIgnoreKeyCheck(b bool) ImportOption {\n\treturn func(o *ImportOptions) error {\n\t\to.IgnoreKeyCheck = b\n\t\treturn nil\n\t}\n}\n\nfunc OptImportOptionsPresorted(b bool) ImportOption {\n\treturn func(o *ImportOptions) error {\n\t\to.Presorted = b\n\t\treturn nil\n\t}\n}\n\nfunc OptImportOptionsSuppressLog(b bool) ImportOption {\n\treturn func(o *ImportOptions) error {\n\t\to.suppressLog = b\n\t\treturn nil\n\t}\n}\n\nvar ErrAborted = fmt.Errorf(\"error: update was aborted\")\n\nfunc (api *API) ImportAtomicRecord(ctx context.Context, qcx *Qcx, req *AtomicRecord, opts ...ImportOption) error {\n\tsimPowerLoss := false\n\tlossAfter := -1\n\tvar opt ImportOptions\n\tfor _, setter := range opts {\n\t\tif setter != nil {\n\t\t\terr := setter(&opt)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Wrap(err, \"ImportAtomicRecord ImportOptions\")\n\t\t\t}\n\t\t}\n\t}\n\tif opt.SimPowerLossAfter > 0 {\n\t\tsimPowerLoss = true\n\t\tlossAfter = opt.SimPowerLossAfter\n\t}\n\n\tidx, err := api.Index(ctx, req.Index)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting index\")\n\t}\n\n\t// the whole point is to run this part of the import atomically.\n\t// Begin that Tx now!\n\tqcx.StartAtomicWriteTx(Txo{Write: writable, Index: idx, Shard: req.Shard})\n\ttot := 0\n\n\toptions, err := setUpImportOptions(opts...)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"setting up import options\")\n\t}\n\n\t// BSIs (Values)\n\tfor _, ivr := range req.Ivr {\n\t\ttot++\n\t\tif simPowerLoss && tot > lossAfter {\n\t\t\treturn ErrAborted\n\t\t}\n\t\tsubOpts := *options\n\t\tsubOpts.Clear = ivr.Clear\n\t\terr = api.ImportValueWithTx(ctx, qcx, ivr, &subOpts)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"ImportAtomicRecord ImportValueWithTx\")\n\t\t}\n\t}\n\n\t// other bits, non-BSI\n\tfor _, ir := range req.Ir {\n\t\ttot++\n\t\tif simPowerLoss && tot > lossAfter {\n\t\t\treturn ErrAborted\n\t\t}\n\t\tsubOpts := *options\n\t\tsubOpts.Clear = ir.Clear\n\t\terr := api.ImportWithTx(ctx, qcx, ir, &subOpts)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"ImportAtomicRecord ImportWithTx\")\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc addClearToImportOptions(opts []ImportOption) []ImportOption {\n\tvar opt ImportOptions\n\tfor _, o := range opts {\n\t\t// check for side-effect of setting io.Clear; that is\n\t\t// how we know it is present.\n\t\t_ = o(&opt)\n\t\tif opt.Clear {\n\t\t\t// we already have the clear flag set, so nothing more to do.\n\t\t\treturn opts\n\t\t}\n\t}\n\t// no clear flag being set, add that option now.\n\treturn append(opts, OptImportOptionsClear(true))\n}\n\n// Import does the top-level importing.\nfunc (api *API) Import(ctx context.Context, qcx *Qcx, req *ImportRequest, opts ...ImportOption) (err error) {\n\tif req.Clear {\n\t\topts = addClearToImportOptions(opts)\n\t}\n\t// Set up import options.\n\toptions, err := setUpImportOptions(opts...)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"setting up import options\")\n\t}\n\n\t/////////////////////////////////////////////////////////////////////////////\n\t// We build the ImportMessage here BEFORE the call to api.ImportWithTx(),\n\t// because something in that method is modifying the values of req, so if we\n\t// build ImportMessage after the call to api.ImportWithTx(), then the values\n\t// that get logged are incorrect. An example I saw were RowIDS going from:\n\t// shard 0 [1, 2, 3]\n\t// shard 4 [0]\n\t//\n\t// to:\n\t// shard 0 [1048577, 2097154, 3145731]\n\t// shard 4 [1]\n\t//\n\t// which seem to be the offset in the field shard bitmap.\n\tvar partition int\n\tvar msg *computer.ImportMessage\n\tif api.isComputeNode && !options.suppressLog {\n\t\tpartition = disco.ShardToShardPartition(req.Index, req.Shard, disco.DefaultPartitionN)\n\t\tmsg = &computer.ImportMessage{\n\t\t\tTable:      req.Index,\n\t\t\tField:      req.Field,\n\t\t\tPartition:  partition,\n\t\t\tShard:      req.Shard,\n\t\t\tRowIDs:     make([]uint64, len(req.RowIDs)),\n\t\t\tColumnIDs:  make([]uint64, len(req.ColumnIDs)),\n\t\t\tRowKeys:    make([]string, len(req.RowKeys)),\n\t\t\tColumnKeys: make([]string, len(req.ColumnKeys)),\n\t\t\tTimestamps: make([]int64, len(req.Timestamps)),\n\t\t\tClear:      req.Clear,\n\n\t\t\tIgnoreKeyCheck: options.IgnoreKeyCheck,\n\t\t\tPresorted:      options.Presorted,\n\t\t}\n\t\tcopy(msg.RowIDs, req.RowIDs)\n\t\tcopy(msg.ColumnIDs, req.ColumnIDs)\n\t\tcopy(msg.RowKeys, req.RowKeys)\n\t\tcopy(msg.ColumnKeys, req.ColumnKeys)\n\t\tcopy(msg.Timestamps, req.Timestamps)\n\t}\n\t/////////////////////////////////////////////////////////////////////////////\n\n\terr = api.ImportWithTx(ctx, qcx, req, options)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif api.isComputeNode && !options.suppressLog {\n\t\ttkey := dax.TableKey(req.Index)\n\t\tqtid := tkey.QualifiedTableID()\n\t\tpartitionNum := dax.PartitionNum(partition)\n\t\tshardNum := dax.ShardNum(req.Shard)\n\n\t\tb, err := computer.MarshalLogMessage(msg, computer.EncodeTypeJSON)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"marshalling log message\")\n\t\t}\n\n\t\tresource := api.serverlessStorage.GetShardResource(qtid, partitionNum, shardNum)\n\t\terr = resource.Append(b)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"appending shard data\") // TODO do we need to set err0 or something?\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// ImportWithTx bulk imports data into a particular index,field,shard.\nfunc (api *API) ImportWithTx(ctx context.Context, qcx *Qcx, req *ImportRequest, options *ImportOptions) error {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.Import\")\n\tdefer span.Finish()\n\n\tif err := api.validate(apiImport); err != nil {\n\t\treturn errors.Wrap(err, \"validating api method\")\n\t}\n\n\tidx, field, err := api.indexField(req.Index, req.Field, req.Shard)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting index and field\")\n\t}\n\n\t// This node only handles the shard(s) that it owns.\n\tif api.isComputeNode {\n\t\tdirective := api.holder.Directive()\n\t\tif !shardInShards(dax.ShardNum(req.Shard), directive.ComputeShards(dax.TableKey(idx.Name()))) {\n\t\t\treturn errors.Errorf(\"import request shard is not supported (with tx): %d\", req.Shard)\n\t\t}\n\t}\n\n\tif err := req.ValidateWithTimestamp(idx.CreatedAt(), field.CreatedAt()); err != nil {\n\t\treturn errors.Wrap(err, \"validating import value request\")\n\t}\n\n\tspan.LogKV(\n\t\t\"index\", req.Index,\n\t\t\"field\", req.Field)\n\n\t// Unless explicitly ignoring key validation (meaning keys have been\n\t// translated to ids in a previous step at the primary node), then\n\t// check to see if keys need translation.\n\tif !options.IgnoreKeyCheck {\n\t\t// Translate row keys.\n\t\tif field.Keys() {\n\t\t\tspan.LogKV(\"rowKeys\", true)\n\t\t\tif len(req.RowIDs) != 0 {\n\t\t\t\treturn errors.New(\"row ids cannot be used because field uses string keys\")\n\t\t\t}\n\t\t\tif req.RowIDs, err = api.cluster.translateFieldKeys(ctx, field, req.RowKeys, true); err != nil {\n\t\t\t\treturn errors.Wrapf(err, \"translating field keys\")\n\t\t\t}\n\t\t} else if len(req.RowKeys) != 0 {\n\t\t\treturn errors.New(\"value keys cannot be used because field uses integer IDs\")\n\t\t}\n\n\t\t// Translate column keys.\n\t\tif idx.Keys() {\n\t\t\tspan.LogKV(\"columnKeys\", true)\n\t\t\tif len(req.ColumnIDs) != 0 {\n\t\t\t\treturn errors.New(\"column ids cannot be used because index uses string keys\")\n\t\t\t}\n\t\t\tif req.ColumnIDs, err = api.cluster.translateIndexKeys(ctx, req.Index, req.ColumnKeys, true); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"translating columns\")\n\t\t\t}\n\t\t\t// mark this request as having an unknown shard, meaning it will\n\t\t\t// be sorted and served out to multiple nodes.\n\t\t\treq.Shard = ^uint64(0)\n\t\t} else if len(req.ColumnKeys) != 0 {\n\t\t\treturn errors.New(\"record keys cannot be used because field uses integer IDs\")\n\t\t}\n\t}\n\n\t// if you specify a shard of ^0, we try to split this out. If we did any\n\t// key translation, we set it to ^0 already above.\n\tif req.Shard == ^uint64(0) {\n\t\treqs := req.SortToShards()\n\n\t\t// Signal to the receiving nodes to ignore checking for key translation.\n\t\toptions.IgnoreKeyCheck = true\n\n\t\tvar eg errgroup.Group\n\t\tguard := make(chan struct{}, runtime.NumCPU()) // only run as many goroutines as CPUs available\n\t\tfor _, subReq := range reqs {\n\t\t\t// TODO: if local node owns this shard we don't need to go through the client\n\t\t\tguard <- struct{}{} // would block if guard channel is already filled\n\t\t\tsubReq := subReq\n\t\t\teg.Go(func() error {\n\t\t\t\terr := api.server.defaultClient.Import(ctx, qcx, subReq, options)\n\t\t\t\t<-guard\n\t\t\t\treturn err\n\t\t\t})\n\t\t}\n\t\treturn eg.Wait()\n\t}\n\n\t// otherwise, this has to be a shard that we have, and everything has\n\t// to be for that shard.\n\n\t// Validate shard ownership.\n\tif err := api.validateShardOwnership(req.Index, req.Shard); err != nil {\n\t\treturn errors.Wrap(err, \"validating shard ownership\")\n\t}\n\n\tvar timestamps []int64\n\tfor _, v := range req.Timestamps {\n\t\tif v != 0 {\n\t\t\ttimestamps = req.Timestamps\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// Import columnIDs into existence field.\n\tif !options.Clear {\n\t\tif err := importExistenceColumns(qcx, idx, req.ColumnIDs, req.Shard); err != nil {\n\t\t\tapi.server.logger.Errorf(\"import existence error: index=%s, field=%s, shard=%d, columns=%d, err=%s\", req.Index, req.Field, req.Shard, len(req.ColumnIDs), err)\n\t\t\treturn err\n\t\t}\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"importing existence columns\")\n\t\t}\n\t}\n\n\t// Import into fragment.\n\terr = field.Import(qcx, req.RowIDs, req.ColumnIDs, timestamps, req.Shard, options)\n\tif err != nil {\n\t\tapi.server.logger.Errorf(\"import error: index=%s, field=%s, shard=%d, columns=%d, err=%s\", req.Index, req.Field, req.Shard, len(req.ColumnIDs), err)\n\t\treturn errors.Wrap(err, \"importing\")\n\t}\n\treturn errors.Wrap(err, \"committing\")\n}\n\n// ImportRoaringShard transactionally imports roaring-encoded data\n// across many fields in a single shard. It can both set and clear\n// bits and updates caches/bitDepth as appropriate, although only the\n// bitmap parts happen truly transactionally.\n//\n// This function does not attempt to do existence tracking, because\n// it can't; there's no way to distinguish empty sets from not setting\n// bits. As a result, users of this endpoint are responsible for\n// providing corrected existence views for fields with existence\n// tracking. Our batch API does that.\nfunc (api *API) ImportRoaringShard(ctx context.Context, indexName string, shard uint64, req *ImportRoaringShardRequest) error {\n\tindex, err := api.Index(ctx, indexName)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting index\")\n\t}\n\n\t// we really only need a Tx, but getting a Qcx so that there's only one path for getting a Tx\n\tqcx := api.Txf().NewQcx()\n\tqcx.write = true\n\ttx, finisher, err := qcx.GetTx(Txo{Write: true, Index: index, Shard: shard})\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting Tx\")\n\t}\n\tdefer qcx.Finish()\n\tvar err1 error\n\tdefer finisher(&err1)\n\n\tif !req.Remote {\n\t\terr1 = errors.New(\"forwarding unimplemented on this endpoint\")\n\t\treturn err1\n\t}\n\n\tfor _, viewUpdate := range req.Views {\n\t\tfield := index.Field(viewUpdate.Field)\n\t\tif field == nil {\n\t\t\terr1 = errors.Errorf(\"no field named '%s' found.\", viewUpdate.Field)\n\t\t\treturn err1\n\t\t}\n\n\t\tfieldType := field.Options().Type\n\t\tif viewUpdate.View, err1 = field.cleanupViewName(viewUpdate.View); err1 != nil {\n\t\t\treturn err1\n\t\t}\n\n\t\tview, err := field.createViewIfNotExists(viewUpdate.View)\n\t\tif err != nil {\n\t\t\terr1 = errors.Wrap(err, \"getting view\")\n\t\t\treturn err1\n\t\t}\n\n\t\tfrag, err := view.CreateFragmentIfNotExists(shard)\n\t\tif err != nil {\n\t\t\terr1 = errors.Wrap(err, \"getting fragment\")\n\t\t\treturn err1\n\t\t}\n\n\t\tswitch fieldType {\n\t\tcase FieldTypeSet, FieldTypeTime:\n\t\t\tif !viewUpdate.ClearRecords {\n\t\t\t\terr1 = frag.ImportRoaringClearAndSet(ctx, tx, viewUpdate.Clear, viewUpdate.Set)\n\t\t\t} else {\n\t\t\t\terr1 = frag.ImportRoaringSingleValued(ctx, tx, viewUpdate.Clear, viewUpdate.Set)\n\t\t\t}\n\t\tcase FieldTypeInt, FieldTypeTimestamp, FieldTypeDecimal:\n\t\t\terr1 = frag.ImportRoaringBSI(ctx, tx, viewUpdate.Clear, viewUpdate.Set)\n\t\tcase FieldTypeMutex, FieldTypeBool:\n\t\t\terr1 = frag.ImportRoaringSingleValued(ctx, tx, viewUpdate.Clear, viewUpdate.Set)\n\t\tdefault:\n\t\t\terr1 = errors.Errorf(\"field type %s is not supported\", fieldType)\n\t\t}\n\t\tif err1 != nil {\n\t\t\treturn err1\n\t\t}\n\n\t\t// need to update field/bsiGroup bitDepth value if this is an int-like field.\n\t\t//\n\t\t// TODO get rid of cached bitDepth entirely because the fact\n\t\t// that we have to do this is weird and since this state isn't\n\t\t// in RBF might have transactional issues.\n\t\tif len(field.bsiGroups) > 0 {\n\t\t\tmaxRowID, _, err := frag.maxRow(tx, nil)\n\t\t\tif err != nil {\n\t\t\t\terr1 = errors.Wrapf(err, \"getting fragment max row id\")\n\t\t\t\treturn err1\n\t\t\t}\n\t\t\tvar bd uint64\n\t\t\tif maxRowID+1 > bsiOffsetBit {\n\t\t\t\tbd = maxRowID + 1 - bsiOffsetBit\n\t\t\t}\n\t\t\tfield.cacheBitDepth(bd) // updating bitDepth shouldn't harm anything even if we roll back... only might make some ops slightly more inefficient\n\t\t}\n\t}\n\n\tif api.isComputeNode && !req.SuppressLog {\n\t\tpartition := disco.ShardToShardPartition(indexName, shard, disco.DefaultPartitionN)\n\t\tmsg := &computer.ImportRoaringShardMessage{\n\t\t\tTable:     indexName,\n\t\t\tPartition: partition,\n\t\t\tShard:     shard,\n\t\t\tViews:     make([]computer.RoaringUpdate, len(req.Views)),\n\t\t}\n\t\tfor i, view := range req.Views {\n\t\t\tmsg.Views[i] = computer.RoaringUpdate{\n\t\t\t\tField:        view.Field,\n\t\t\t\tView:         view.View,\n\t\t\t\tClear:        view.Clear,\n\t\t\t\tSet:          view.Set,\n\t\t\t\tClearRecords: view.ClearRecords,\n\t\t\t}\n\t\t}\n\n\t\ttkey := dax.TableKey(indexName)\n\t\tqtid := tkey.QualifiedTableID()\n\t\tpartitionNum := dax.PartitionNum(partition)\n\t\tshardNum := dax.ShardNum(shard)\n\n\t\tb, err := computer.MarshalLogMessage(msg, computer.EncodeTypeJSON)\n\t\tif err != nil {\n\t\t\terr1 = errors.Wrap(err, \"marshalling log message\")\n\t\t\treturn err1\n\t\t}\n\n\t\tresource := api.serverlessStorage.GetShardResource(qtid, partitionNum, shardNum)\n\t\terr1 = errors.Wrap(resource.Append(b), \"appending shard data\")\n\t\tif err1 != nil {\n\t\t\treturn err1\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// ImportValue is a wrapper around the common code in ImportValueWithTx, which\n// currently just translates req.Clear into a clear ImportOption.\nfunc (api *API) ImportValue(ctx context.Context, qcx *Qcx, req *ImportValueRequest, opts ...ImportOption) error {\n\tif req.Clear {\n\t\topts = addClearToImportOptions(opts)\n\t}\n\t// Set up import options.\n\toptions, err := setUpImportOptions(opts...)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"setting up import options\")\n\t}\n\n\t/////////////////////////////////////////////////////////////////////////////\n\t// We build the ImportValueMessage here BEFORE the call to\n\t// api.ImportValueWithTx() because we don't trust that req doesn't get\n\t// changed out from under us. See the similar comment in the API.Import()\n\t// method above.\n\tvar partition int\n\tvar msg *computer.ImportValueMessage\n\tif api.isComputeNode && !options.suppressLog {\n\t\tpartition = disco.ShardToShardPartition(req.Index, req.Shard, disco.DefaultPartitionN)\n\t\tmsg = &computer.ImportValueMessage{\n\t\t\tTable:           req.Index,\n\t\t\tField:           req.Field,\n\t\t\tPartition:       partition,\n\t\t\tShard:           req.Shard,\n\t\t\tColumnIDs:       make([]uint64, len(req.ColumnIDs)),\n\t\t\tColumnKeys:      make([]string, len(req.ColumnKeys)),\n\t\t\tValues:          make([]int64, len(req.Values)),\n\t\t\tFloatValues:     make([]float64, len(req.FloatValues)),\n\t\t\tTimestampValues: make([]time.Time, len(req.TimestampValues)),\n\t\t\tStringValues:    make([]string, len(req.StringValues)),\n\t\t\tClear:           req.Clear,\n\n\t\t\tIgnoreKeyCheck: options.IgnoreKeyCheck,\n\t\t\tPresorted:      options.Presorted,\n\t\t}\n\t\tcopy(msg.ColumnIDs, req.ColumnIDs)\n\t\tcopy(msg.ColumnKeys, req.ColumnKeys)\n\t\tcopy(msg.Values, req.Values)\n\t\tcopy(msg.FloatValues, req.FloatValues)\n\t\tcopy(msg.TimestampValues, req.TimestampValues)\n\t\tcopy(msg.StringValues, req.StringValues)\n\t}\n\t/////////////////////////////////////////////////////////////////////////////\n\n\tif err := api.ImportValueWithTx(ctx, qcx, req, options); err != nil {\n\t\treturn errors.Wrap(err, \"importing value with tx\")\n\t}\n\n\tif api.isComputeNode && !options.suppressLog {\n\t\t// Get the current version for shard.\n\t\ttkey := dax.TableKey(req.Index)\n\t\tqtid := tkey.QualifiedTableID()\n\t\tpartitionNum := dax.PartitionNum(partition)\n\t\tshardNum := dax.ShardNum(req.Shard)\n\t\tb, err := computer.MarshalLogMessage(msg, computer.EncodeTypeJSON)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"marshalling log message\")\n\t\t}\n\n\t\tresource := api.serverlessStorage.GetShardResource(qtid, partitionNum, shardNum)\n\t\terr = resource.Append(b)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"appending shard data\") // TODO do we need to set err0 or something?\n\t\t}\n\n\t}\n\n\treturn nil\n}\n\n// ImportValueWithTx bulk imports values into a particular field.\nfunc (api *API) ImportValueWithTx(ctx context.Context, qcx *Qcx, req *ImportValueRequest, options *ImportOptions) (err0 error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.ImportValue\")\n\tdefer span.Finish()\n\n\tif err := api.validate(apiImportValue); err != nil {\n\t\treturn errors.Wrap(err, \"validating api method\")\n\t}\n\n\tnumCols := len(req.ColumnIDs) + len(req.ColumnKeys)\n\tnumVals := len(req.Values) + len(req.FloatValues) + len(req.TimestampValues) + len(req.StringValues)\n\tif numCols != numVals {\n\t\treturn errors.New(fmt.Sprintf(\"number of columns (%v) and number of values (%v) do not match\", numCols, numVals))\n\t}\n\tif numCols == 0 {\n\t\treturn nil\n\t}\n\n\tapi.server.logger.Debugf(\"ImportValueWithTx: %v %v %v\", req.Index, req.Field, req.Shard)\n\tidx, field, err := api.indexField(req.Index, req.Field, req.Shard)\n\tif err != nil {\n\t\treturn errors.Wrap(err, fmt.Sprintf(\"getting index '%v' and field '%v'; shard=%v\", req.Index, req.Field, req.Shard))\n\t}\n\n\t// This node only handles the shard(s) that it owns.\n\tif api.isComputeNode {\n\t\tdirective := api.holder.Directive()\n\t\tif !shardInShards(dax.ShardNum(req.Shard), directive.ComputeShards(dax.TableKey(idx.Name()))) {\n\t\t\treturn errors.Errorf(\"import request shard is not supported (value with tx): %d\", req.Shard)\n\t\t}\n\t}\n\n\tif err := req.ValidateWithTimestamp(idx.CreatedAt(), field.CreatedAt()); err != nil {\n\t\treturn errors.Wrap(err, \"validating import value request\")\n\t}\n\n\tspan.LogKV(\n\t\t\"index\", req.Index,\n\t\t\"field\", req.Field)\n\t// Unless explicitly ignoring key validation (meaning keys have been\n\t// translate to ids in a previous step at the primary node), then\n\t// check to see if keys need translation.\n\tif !options.IgnoreKeyCheck {\n\t\t// Translate column keys.\n\t\tif idx.Keys() {\n\t\t\tspan.LogKV(\"columnKeys\", true)\n\t\t\tif len(req.ColumnIDs) != 0 {\n\t\t\t\treturn errors.New(\"column ids cannot be used because index uses string keys\")\n\t\t\t}\n\t\t\tif req.ColumnIDs, err = api.cluster.translateIndexKeys(ctx, req.Index, req.ColumnKeys, true); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"translating columns\")\n\t\t\t}\n\t\t\treq.Shard = math.MaxUint64\n\t\t}\n\n\t\t// Translate values when the field uses keys (for example, when\n\t\t// the field has a ForeignIndex with keys).\n\t\tif field.Keys() {\n\t\t\t// Perform translation.\n\t\t\tspan.LogKV(\"rowKeys\", true)\n\t\t\tuints, err := api.cluster.translateIndexKeys(ctx, field.ForeignIndex(), req.StringValues, true)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t// Because the BSI field supports negative values, we have to\n\t\t\t// convert the uint64 keys to a slice of int64.\n\t\t\tints := make([]int64, len(uints))\n\t\t\tfor i := range uints {\n\t\t\t\tints[i] = int64(uints[i])\n\t\t\t}\n\t\t\treq.Values = ints\n\t\t}\n\t}\n\n\tif !options.Presorted {\n\t\t// horrible hackery: we implement a secondary key so we can\n\t\t// get a stable sort without using sort.Stable\n\t\treq.scratch = make([]int, len(req.ColumnIDs))\n\t\tfor i := range req.scratch {\n\t\t\treq.scratch[i] = i\n\t\t}\n\t\tsort.Sort(req)\n\t\t// don't keep that list around since we don't need it anymore\n\t\treq.scratch = nil\n\t}\n\n\t// if we're importing into a specific shard\n\tif req.Shard != math.MaxUint64 {\n\t\t// Check that column IDs match the stated shard.\n\t\tshard := req.ColumnIDs[0] / ShardWidth\n\t\tif s2 := req.ColumnIDs[len(req.ColumnIDs)-1] / ShardWidth; (shard != s2) || (shard != req.Shard) {\n\t\t\treturn errors.Errorf(\"shard %d specified, but import spans shards %d to %d\", req.Shard, shard, s2)\n\t\t}\n\t\t// Validate shard ownership. TODO - we should forward to the\n\t\t// correct node rather than barfing here.\n\t\tif err := api.validateShardOwnership(req.Index, req.Shard); err != nil {\n\t\t\treturn errors.Wrap(err, \"validating shard ownership\")\n\t\t}\n\t\t// Import columnIDs into existence field.\n\t\tif !options.Clear {\n\t\t\tif err := importExistenceColumns(qcx, idx, req.ColumnIDs, shard); err != nil {\n\t\t\t\tapi.server.logger.Errorf(\"import existence error: index=%s, field=%s, shard=%d, columns=%d, err=%s\", req.Index, req.Field, req.Shard, len(req.ColumnIDs), err)\n\t\t\t\treturn errors.Wrap(err, \"importing existence columns\")\n\t\t\t}\n\t\t}\n\n\t\t// Import into fragment.\n\t\tif len(req.Values) > 0 {\n\t\t\terr = field.importValue(qcx, req.ColumnIDs, req.Values, shard, options)\n\t\t\tif err != nil {\n\t\t\t\tapi.server.logger.Errorf(\"import error: index=%s, field=%s, shard=%d, columns=%d, err=%s\", req.Index, req.Field, req.Shard, len(req.ColumnIDs), err)\n\t\t\t}\n\t\t} else if len(req.TimestampValues) > 0 {\n\t\t\terr = field.importTimestampValue(qcx, req.ColumnIDs, req.TimestampValues, shard, options)\n\t\t\tif err != nil {\n\t\t\t\tapi.server.logger.Errorf(\"import error: index=%s, field=%s, shard=%d, columns=%d, err=%s\", req.Index, req.Field, req.Shard, len(req.ColumnIDs), err)\n\t\t\t}\n\t\t} else if len(req.FloatValues) > 0 {\n\t\t\terr = field.importFloatValue(qcx, req.ColumnIDs, req.FloatValues, shard, options)\n\t\t\tif err != nil {\n\t\t\t\tapi.server.logger.Errorf(\"import error: index=%s, field=%s, shard=%d, columns=%d, err=%s\", req.Index, req.Field, req.Shard, len(req.ColumnIDs), err)\n\t\t\t}\n\t\t}\n\t\treturn errors.Wrap(err, \"importing value\")\n\n\t} // end if req.Shard != math.MaxUint64\n\toptions.IgnoreKeyCheck = true\n\tstart := 0\n\tshard := req.ColumnIDs[0] / ShardWidth\n\tvar eg errgroup.Group\n\tguard := make(chan struct{}, runtime.NumCPU()) // only run as many goroutines as CPUs available\n\tfor i, colID := range req.ColumnIDs {\n\t\tif colID/ShardWidth != shard {\n\t\t\tsubreq := &ImportValueRequest{\n\t\t\t\tIndex:     req.Index,\n\t\t\t\tField:     req.Field,\n\t\t\t\tShard:     shard,\n\t\t\t\tColumnIDs: req.ColumnIDs[start:i],\n\t\t\t}\n\t\t\tif req.Values != nil {\n\t\t\t\tsubreq.Values = req.Values[start:i]\n\t\t\t} else if req.FloatValues != nil {\n\t\t\t\tsubreq.FloatValues = req.FloatValues[start:i]\n\t\t\t} else if req.TimestampValues != nil {\n\t\t\t\tsubreq.TimestampValues = req.TimestampValues[start:i]\n\t\t\t}\n\t\t\tguard <- struct{}{} // would block if guard channel is already filled\n\t\t\teg.Go(func() error {\n\t\t\t\terr := api.server.defaultClient.ImportValue(ctx, qcx, subreq, options)\n\t\t\t\t<-guard\n\t\t\t\treturn err\n\t\t\t})\n\t\t\tstart = i\n\t\t\tshard = colID / ShardWidth\n\t\t}\n\t}\n\tsubreq := &ImportValueRequest{\n\t\tIndex:     req.Index,\n\t\tField:     req.Field,\n\t\tShard:     shard,\n\t\tColumnIDs: req.ColumnIDs[start:],\n\t}\n\tif req.Values != nil {\n\t\tsubreq.Values = req.Values[start:]\n\t} else if req.FloatValues != nil {\n\t\tsubreq.FloatValues = req.FloatValues[start:]\n\t}\n\teg.Go(func() error {\n\t\t// TODO we should elevate the logic for figuring out which\n\t\t// node(s) to send to into API instead of having those details\n\t\t// in the client implementation.\n\t\treturn api.server.defaultClient.ImportValue(ctx, qcx, subreq, options)\n\t})\n\terr = eg.Wait()\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc importExistenceColumns(qcx *Qcx, index *Index, columnIDs []uint64, shard uint64) (err0 error) {\n\tef := index.existenceField()\n\tif ef == nil {\n\t\treturn nil\n\t}\n\ttx, finisher, err := qcx.GetTx(Txo{Write: true, Index: index, Shard: shard})\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer finisher(&err0)\n\t// markExistingInView is simpler/faster than Import, but unusually, we use the\n\t// standard view of the existence field, instead of the existence view of\n\t// a specific field, when doing the index-wide update.\n\treturn ef.markExistingInView(tx, columnIDs, viewStandard, shard)\n}\n\n// ShardDistribution returns an object representing the distribution of shards\n// across nodes for each index, distinguishing between primary and replica.\n// The structure of this information is [indexName][nodeID][primaryOrReplica][]uint64.\n// This function supports a view in the UI.\nfunc (api *API) ShardDistribution(ctx context.Context) map[string]interface{} {\n\tdistByIndex := make(map[string]interface{})\n\n\tfor idx := range api.holder.indexes {\n\t\tdist := api.cluster.shardDistributionByIndex(idx)\n\t\tdistByIndex[idx] = dist\n\t}\n\n\treturn distByIndex\n}\n\n// MaxShards returns the maximum shard number for each index in a map.\n// TODO (2.0): This method has been deprecated. Instead, use\n// AvailableShardsByIndex.\nfunc (api *API) MaxShards(ctx context.Context) map[string]uint64 {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.MaxShards\")\n\tdefer span.Finish()\n\n\tm := make(map[string]uint64)\n\tfor k, v := range api.holder.availableShardsByIndex() {\n\t\tm[k] = v.Max()\n\t}\n\treturn m\n}\n\n// AvailableShardsByIndex returns bitmaps of shards with available by index name.\nfunc (api *API) AvailableShardsByIndex(ctx context.Context) map[string]*roaring.Bitmap {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.AvailableShardsByIndex\")\n\tdefer span.Finish()\n\treturn api.holder.availableShardsByIndex()\n}\n\n// AvailableShards returns bitmap of available shards for a single index.\nfunc (api *API) AvailableShards(ctx context.Context, indexName string) (*roaring.Bitmap, error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"API.AvailableShards\")\n\tdefer span.Finish()\n\n\t// Find the index.\n\tindex := api.holder.Index(indexName)\n\tif index == nil {\n\t\treturn nil, newNotFoundError(ErrIndexNotFound, indexName)\n\t}\n\treturn index.AvailableShards(false), nil\n}\n\n// LongQueryTime returns the configured threshold for logging/statting\n// long running queries.\nfunc (api *API) LongQueryTime() time.Duration {\n\treturn api.server.longQueryTime\n}\n\nfunc (api *API) validateShardOwnership(indexName string, shard uint64) error {\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := api.cluster.NewSnapshot()\n\t// Validate that this handler owns the shard.\n\tif !snap.OwnsShard(api.NodeID(), indexName, shard) {\n\t\tapi.server.logger.Errorf(\"node %s does not own shard %d of index %s\", api.NodeID(), shard, indexName)\n\t\treturn ErrClusterDoesNotOwnShard\n\t}\n\treturn nil\n}\n\nfunc (api *API) indexField(indexName string, fieldName string, shard uint64) (*Index, *Field, error) {\n\t// Find the Index.\n\tindex := api.holder.Index(indexName)\n\tif index == nil {\n\t\tapi.server.logger.Errorf(\"fragment error: index=%s, field=%s, shard=%d, err=%s\", indexName, fieldName, shard, ErrIndexNotFound.Error())\n\t\treturn nil, nil, newNotFoundError(ErrIndexNotFound, indexName)\n\t}\n\n\t// Retrieve field.\n\tfield := index.Field(fieldName)\n\tif field == nil {\n\t\tapi.server.logger.Errorf(\"field error: index=%s, field=%s, shard=%d, err=%s\", indexName, fieldName, shard, ErrFieldNotFound.Error())\n\t\treturn nil, nil, newNotFoundError(ErrFieldNotFound, fieldName)\n\t}\n\treturn index, field, nil\n}\n\n// State returns the cluster state which is usually \"NORMAL\", but could be\n// \"STARTING\", or potentially others. See disco.go for more\n// details.\nfunc (api *API) State() (disco.ClusterState, error) {\n\tif err := api.validate(apiState); err != nil {\n\t\treturn \"\", errors.Wrap(err, \"validating api method\")\n\t}\n\n\treturn api.cluster.State()\n}\n\n// ClusterName returns the cluster name.\nfunc (api *API) ClusterName() string {\n\tif api.cluster.Name == \"\" {\n\t\treturn api.cluster.id\n\t}\n\treturn api.cluster.Name\n}\n\n// Version returns the Pilosa version.\nfunc (api *API) Version() string {\n\treturn strings.TrimPrefix(Version, \"v\")\n}\n\n// Info returns information about this server instance.\nfunc (api *API) Info() serverInfo {\n\tsi := api.server.systemInfo\n\t// we don't report errors on failures to get this information\n\tphysicalCores, logicalCores, _ := si.CPUCores()\n\tmhz, _ := si.CPUMHz()\n\tmem, _ := si.MemTotal()\n\treturn serverInfo{\n\t\tShardWidth:       ShardWidth,\n\t\tCPUPhysicalCores: physicalCores,\n\t\tCPULogicalCores:  logicalCores,\n\t\tCPUMHz:           mhz,\n\t\tCPUType:          si.CPUModel(),\n\t\tMemory:           mem,\n\t\tStorageBackend:   api.holder.txf.TxType(),\n\t\tReplicaN:         api.cluster.ReplicaN,\n\t\tShardHash:        api.cluster.Hasher.Name(),\n\t\tKeyHash:          api.cluster.Hasher.Name(),\n\t}\n}\n\n// GetTranslateEntryReader provides an entry reader for key translation logs starting at offset.\nfunc (api *API) GetTranslateEntryReader(ctx context.Context, offsets TranslateOffsetMap) (_ TranslateEntryReader, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"API.GetTranslateEntryReader\")\n\tdefer span.Finish()\n\n\t// Ensure all readers are cleaned up if any error.\n\tvar a []TranslateEntryReader\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tfor i := range a {\n\t\t\t\ta[i].Close() // nolint: errcheck\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Fetch all index partition readers.\n\tfor indexName, indexMap := range offsets {\n\t\tindex := api.holder.Index(indexName)\n\t\tif index == nil {\n\t\t\treturn nil, newNotFoundError(ErrIndexNotFound, indexName)\n\t\t}\n\n\t\tfor partitionID, offset := range indexMap.Partitions {\n\t\t\tstore := index.TranslateStore(partitionID)\n\t\t\tif store == nil {\n\t\t\t\treturn nil, ErrTranslateStoreNotFound\n\t\t\t}\n\n\t\t\tr, err := store.EntryReader(ctx, uint64(offset))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"index partition translate reader\")\n\t\t\t}\n\t\t\ta = append(a, r)\n\t\t}\n\t}\n\n\t// Fetch all field readers.\n\tfor indexName, indexMap := range offsets {\n\t\tindex := api.holder.Index(indexName)\n\t\tif index == nil {\n\t\t\treturn nil, newNotFoundError(ErrIndexNotFound, indexName)\n\t\t}\n\n\t\tfor fieldName, offset := range indexMap.Fields {\n\t\t\tfield := index.Field(fieldName)\n\t\t\tif field == nil {\n\t\t\t\treturn nil, newNotFoundError(ErrFieldNotFound, fieldName)\n\t\t\t}\n\t\t\tstore := field.TranslateStore()\n\t\t\tif store == nil {\n\t\t\t\treturn nil, ErrTranslateStoreNotFound\n\t\t\t}\n\t\t\tr, err := field.TranslateStore().EntryReader(ctx, uint64(offset))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"field translate reader\")\n\t\t\t}\n\t\t\ta = append(a, r)\n\t\t}\n\t}\n\n\treturn NewMultiTranslateEntryReader(ctx, a), nil\n}\n\nfunc (api *API) TranslateIndexKey(ctx context.Context, indexName string, key string, writable bool) (uint64, error) {\n\treturn api.cluster.translateIndexKey(ctx, indexName, key, writable)\n}\n\nfunc (api *API) TranslateIndexIDs(ctx context.Context, indexName string, ids []uint64) ([]string, error) {\n\treturn api.cluster.translateIndexIDs(ctx, indexName, ids)\n}\n\n// TranslateKeys handles a TranslateKeyRequest.\n// ErrTranslatingKeyNotFound error will be swallowed here, so the empty response will be returned.\nfunc (api *API) TranslateKeys(ctx context.Context, r io.Reader) (_ []byte, err error) {\n\tvar req TranslateKeysRequest\n\tbuf, err := io.ReadAll(r)\n\tif err != nil {\n\t\treturn nil, NewBadRequestError(errors.Wrap(err, \"read translate keys request error\"))\n\t} else if err := api.Serializer.Unmarshal(buf, &req); err != nil {\n\t\treturn nil, NewBadRequestError(errors.Wrap(err, \"unmarshal translate keys request error\"))\n\t}\n\n\t// Lookup store for either index or field and translate keys.\n\tvar ids []uint64\n\tif req.Field == \"\" {\n\t\tids, err = api.cluster.translateIndexKeys(ctx, req.Index, req.Keys, !req.NotWritable)\n\t} else {\n\t\tfield := api.holder.Field(req.Index, req.Field)\n\t\tif field == nil {\n\t\t\treturn nil, newNotFoundError(ErrFieldNotFound, req.Field)\n\t\t}\n\n\t\tif fi := field.ForeignIndex(); fi != \"\" {\n\t\t\tids, err = api.cluster.translateIndexKeys(ctx, fi, req.Keys, !req.NotWritable)\n\t\t} else {\n\t\t\tids, err = api.cluster.translateFieldKeys(ctx, field, req.Keys, !req.NotWritable)\n\t\t}\n\t}\n\tif err != nil && errors.Cause(err) != ErrTranslatingKeyNotFound {\n\t\treturn nil, errors.WithMessage(err, \"translating keys\")\n\t}\n\n\t// Encode response.\n\tif buf, err = api.Serializer.Marshal(&TranslateKeysResponse{IDs: ids}); err != nil {\n\t\treturn nil, errors.Wrap(err, \"translate keys response encoding error\")\n\t}\n\treturn buf, nil\n}\n\n// TranslateIDs handles a TranslateIDRequest.\nfunc (api *API) TranslateIDs(ctx context.Context, r io.Reader) (_ []byte, err error) {\n\tvar req TranslateIDsRequest\n\tif buf, err := io.ReadAll(r); err != nil {\n\t\treturn nil, NewBadRequestError(errors.Wrap(err, \"read translate ids request error\"))\n\t} else if err := api.Serializer.Unmarshal(buf, &req); err != nil {\n\t\treturn nil, NewBadRequestError(errors.Wrap(err, \"unmarshal translate ids request error\"))\n\t}\n\n\t// Lookup store for either index or field and translate ids.\n\tvar keys []string\n\tif req.Field == \"\" {\n\t\tif keys, err = api.cluster.translateIndexIDs(ctx, req.Index, req.IDs); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else {\n\t\tif field := api.holder.Field(req.Index, req.Field); field == nil {\n\t\t\treturn nil, newNotFoundError(ErrFieldNotFound, req.Field)\n\t\t} else if fi := field.ForeignIndex(); fi != \"\" {\n\t\t\tkeys, err = api.cluster.translateIndexIDs(ctx, fi, req.IDs)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t} else if keys, err = api.cluster.translateFieldListIDs(ctx, field, req.IDs); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// Encode response.\n\tbuf, err := api.Serializer.Marshal(&TranslateIDsResponse{Keys: keys})\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"translate ids response encoding error\")\n\t}\n\treturn buf, nil\n}\n\n// FindIndexKeys looks up column keys in the index, mapping them to IDs.\n// If a key does not exist, it will be absent from the resulting map.\nfunc (api *API) FindIndexKeys(ctx context.Context, index string, keys ...string) (map[string]uint64, error) {\n\treturn api.cluster.findIndexKeys(ctx, index, keys...)\n}\n\n// FindFieldKeys looks up keys in a field, mapping them to IDs.\n// If a key does not exist, it will be absent from the resulting map.\nfunc (api *API) FindFieldKeys(ctx context.Context, index, field string, keys ...string) (map[string]uint64, error) {\n\tf := api.holder.Field(index, field)\n\tif f == nil {\n\t\treturn nil, newNotFoundError(ErrFieldNotFound, field)\n\t}\n\treturn api.cluster.findFieldKeys(ctx, f, keys...)\n}\n\n// CreateIndexKeys looks up column keys in the index, mapping them to IDs.\n// If a key does not exist, it will be created.\nfunc (api *API) CreateIndexKeys(ctx context.Context, index string, keys ...string) (map[string]uint64, error) {\n\treturn api.cluster.createIndexKeys(ctx, index, keys...)\n}\n\n// CreateFieldKeys looks up keys in a field, mapping them to IDs.\n// If a key does not exist, it will be created.\nfunc (api *API) CreateFieldKeys(ctx context.Context, index, field string, keys ...string) (map[string]uint64, error) {\n\tf := api.holder.Field(index, field)\n\tif f == nil {\n\t\treturn nil, newNotFoundError(ErrFieldNotFound, field)\n\t}\n\treturn api.cluster.createFieldKeys(ctx, f, keys...)\n}\n\n// MatchField finds the IDs of all field keys matching a filter.\nfunc (api *API) MatchField(ctx context.Context, index, field string, like string) ([]uint64, error) {\n\tf := api.holder.Field(index, field)\n\tif f == nil {\n\t\treturn nil, newNotFoundError(ErrFieldNotFound, field)\n\t}\n\treturn api.cluster.matchField(ctx, f, like)\n}\n\n// PrimaryReplicaNodeURL returns the URL of the cluster's primary replica.\nfunc (api *API) PrimaryReplicaNodeURL() url.URL {\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := api.cluster.NewSnapshot()\n\n\tnode := snap.PrimaryReplicaNode(api.NodeID())\n\tif node == nil {\n\t\treturn url.URL{}\n\t}\n\treturn node.URI.URL()\n}\n\nfunc (api *API) StartTransaction(ctx context.Context, id string, timeout time.Duration, exclusive bool, remote bool) (*Transaction, error) {\n\tif err := api.validate(apiStartTransaction); err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating api method\")\n\t}\n\tt, err := api.server.StartTransaction(ctx, id, timeout, exclusive, remote)\n\n\tswitch err {\n\tcase nil:\n\t\tif exclusive {\n\t\t\tCounterExclusiveTransactionRequest.Inc()\n\t\t} else {\n\t\t\tCounterTransactionStart.Inc()\n\t\t}\n\tcase ErrTransactionExclusive:\n\t\tif exclusive {\n\t\t\tCounterExclusiveTransactionBlocked.Inc()\n\t\t} else {\n\t\t\tCounterTransactionBlocked.Inc()\n\t\t}\n\t}\n\tif exclusive && t != nil && t.Active {\n\t\tCounterExclusiveTransactionActive.Inc()\n\t}\n\treturn t, err\n}\n\nfunc (api *API) FinishTransaction(ctx context.Context, id string, remote bool) (*Transaction, error) {\n\tif err := api.validate(apiFinishTransaction); err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating api method\")\n\t}\n\tt, err := api.server.FinishTransaction(ctx, id, remote)\n\tif err == nil {\n\t\tif t.Exclusive {\n\t\t\tCounterExclusiveTransactionEnd.Inc()\n\t\t} else {\n\t\t\tCounterTransactionEnd.Inc()\n\t\t}\n\t}\n\treturn t, err\n}\n\nfunc (api *API) Transactions(ctx context.Context) (map[string]*Transaction, error) {\n\tif err := api.validate(apiTransactions); err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating api method\")\n\t}\n\treturn api.server.Transactions(ctx)\n}\n\nfunc (api *API) GetTransaction(ctx context.Context, id string, remote bool) (*Transaction, error) {\n\tif err := api.validate(apiGetTransaction); err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating api method\")\n\t}\n\tt, err := api.server.GetTransaction(ctx, id, remote)\n\tif err == nil {\n\t\tif t.Exclusive && t.Active {\n\t\t\tCounterExclusiveTransactionActive.Inc()\n\t\t}\n\t}\n\treturn t, err\n}\n\nfunc (api *API) ActiveQueries(ctx context.Context) ([]ActiveQueryStatus, error) {\n\tif err := api.validate(apiActiveQueries); err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating api method\")\n\t}\n\treturn api.tracker.ActiveQueries(), nil\n}\n\nfunc (api *API) PastQueries(ctx context.Context, remote bool) ([]PastQueryStatus, error) {\n\tif err := api.validate(apiPastQueries); err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating api method\")\n\t}\n\n\tclusterQueries := api.tracker.PastQueries()\n\n\tif !remote {\n\t\tnodes := api.cluster.Nodes()\n\t\tfor _, node := range nodes {\n\t\t\tif node.ID == api.server.nodeID {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tnodeQueries, err := api.server.defaultClient.GetPastQueries(ctx, &node.URI)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrapf(err, \"collecting query history from %s\", node.URI)\n\t\t\t}\n\t\t\tclusterQueries = append(clusterQueries, nodeQueries...)\n\t\t}\n\t}\n\n\tsort.Slice(clusterQueries, func(i, j int) bool {\n\t\treturn clusterQueries[i].Start.After(clusterQueries[j].Start)\n\t})\n\n\treturn clusterQueries, nil\n}\n\nfunc (api *API) ReserveIDs(key IDAllocKey, session [32]byte, offset uint64, count uint64) ([]IDRange, error) {\n\tif err := api.validate(apiIDReserve); err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating api method\")\n\t}\n\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := api.cluster.NewSnapshot()\n\n\tif !snap.IsPrimaryFieldTranslationNode(api.NodeID()) {\n\t\treturn nil, errors.New(\"cannot reserve IDs on a non-primary node\")\n\t}\n\n\treturn api.holder.ida.reserve(key, session, offset, count)\n}\n\nfunc (api *API) CommitIDs(key IDAllocKey, session [32]byte, count uint64) error {\n\tif err := api.validate(apiIDCommit); err != nil {\n\t\treturn errors.Wrap(err, \"validating api method\")\n\t}\n\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := api.cluster.NewSnapshot()\n\n\tif !snap.IsPrimaryFieldTranslationNode(api.NodeID()) {\n\t\treturn errors.New(\"cannot commit IDs on a non-primary node\")\n\t}\n\n\treturn api.holder.ida.commit(key, session, count)\n}\n\nfunc (api *API) ResetIDAlloc(index string) error {\n\tif err := api.validate(apiIDReset); err != nil {\n\t\treturn errors.Wrap(err, \"validating api method\")\n\t}\n\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := api.cluster.NewSnapshot()\n\n\tif !snap.IsPrimaryFieldTranslationNode(api.NodeID()) {\n\t\treturn errors.New(\"cannot reset IDs on a non-primary node\")\n\t}\n\n\treturn api.holder.ida.reset(index)\n}\n\nfunc (api *API) WriteIDAllocDataTo(w io.Writer) error {\n\t_, err := api.holder.ida.WriteTo(w)\n\treturn err\n}\n\nfunc (api *API) RestoreIDAlloc(r io.Reader) error {\n\treturn api.holder.ida.Replace(r)\n}\n\n// TranslateIndexDB is an internal function to load the index keys database\n// rd is a boltdb file.\nfunc (api *API) TranslateIndexDB(ctx context.Context, indexName string, partitionID int, rd io.Reader) error {\n\tidx := api.holder.Index(indexName)\n\tif idx == nil {\n\t\treturn fmt.Errorf(\"index %q not found\", indexName)\n\t}\n\tstore := idx.TranslateStore(partitionID)\n\tif store == nil {\n\t\treturn fmt.Errorf(\"index %q has no translate store\", indexName)\n\t}\n\t_, err := store.ReadFrom(rd)\n\treturn err\n}\n\n// TranslateFieldDB is an internal function to load the field keys database\nfunc (api *API) TranslateFieldDB(ctx context.Context, indexName, fieldName string, rd io.Reader) error {\n\tidx := api.holder.Index(indexName)\n\tif idx == nil {\n\t\treturn fmt.Errorf(\"index %q not found\", indexName)\n\t}\n\tfield := idx.Field(fieldName)\n\tif field == nil {\n\t\t// Older versions used to accidentally provide an empty translation\n\t\t// data file for a nonexistent field called \"_keys\". To make migration\n\t\t// easier, we politely ignore that.\n\t\tif fieldName == \"_keys\" {\n\t\t\treturn nil\n\t\t}\n\t\treturn fmt.Errorf(\"field %q/%q not found\", indexName, fieldName)\n\t}\n\tstore := field.TranslateStore()\n\tif store == nil {\n\t\treturn fmt.Errorf(\"field %q/%q has no translate store\", indexName, fieldName)\n\t}\n\t_, err := store.ReadFrom(rd)\n\treturn err\n}\n\n// RestoreShard is used by the restore tool to restore previously backed up data. This call is specific to RBF data for a shard.\nfunc (api *API) RestoreShard(ctx context.Context, indexName string, shard uint64, rd io.Reader) error {\n\tsnap := api.cluster.NewSnapshot()\n\tif !snap.OwnsShard(api.server.nodeID, indexName, shard) {\n\t\treturn ErrClusterDoesNotOwnShard // TODO (twg)really just node doesn't own shard but leave for now\n\t}\n\n\tidx := api.holder.Index(indexName)\n\t// need to get a dbShard\n\tdbs, err := api.holder.Txf().dbPerShard.GetDBShard(indexName, shard, idx)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdb := dbs.W\n\tfinalPath := db.Path() + \"/data\"\n\ttempPath := finalPath + \".tmp\"\n\to, err := os.OpenFile(tempPath, os.O_CREATE|os.O_WRONLY|os.O_TRUNC, 0o600)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer o.Close()\n\n\tbw := bufio.NewWriter(o)\n\tif _, err = io.Copy(bw, rd); err != nil {\n\t\treturn err\n\t} else if err := bw.Flush(); err != nil {\n\t\treturn err\n\t} else if err := o.Sync(); err != nil {\n\t\treturn err\n\t} else if err := o.Close(); err != nil {\n\t\treturn err\n\t}\n\n\tif err != nil {\n\t\t_ = os.Remove(tempPath)\n\t\treturn err\n\t}\n\terr = db.CloseDB()\n\tif err != nil {\n\t\treturn err\n\t}\n\terr = os.Rename(tempPath, finalPath)\n\tif err != nil {\n\t\t_ = os.Remove(tempPath)\n\t\treturn err\n\t}\n\terr = db.OpenDB()\n\tif err != nil {\n\t\treturn err\n\t}\n\ttx, err := db.NewTx(false, idx.name, Txo{})\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer tx.Rollback()\n\t// arguments idx,shard do not matter for rbf they\n\t// are ignored\n\tflvs, err := tx.GetSortedFieldViewList(idx, shard)\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\tfor _, flv := range flvs {\n\t\tfld := idx.field(flv.Field)\n\t\tview := fld.view(flv.View)\n\t\tif view == nil {\n\t\t\tview, err = fld.createViewIfNotExists(flv.View)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tfrag, err := view.CreateFragmentIfNotExists(shard)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\terr = frag.RebuildRankCache(ctx)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tbd, err := view.bitDepth([]uint64{shard})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\terr = fld.cacheBitDepth(bd)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (api *API) mutexCheckThisNode(ctx context.Context, qcx *Qcx, indexName string, fieldName string, details bool, limit int) (map[uint64]map[uint64][]uint64, error) {\n\tindex := api.holder.Index(indexName)\n\tif index == nil {\n\t\treturn nil, newNotFoundError(ErrIndexNotFound, indexName)\n\t}\n\tfield := index.Field(fieldName)\n\tif field == nil {\n\t\treturn nil, newNotFoundError(ErrFieldNotFound, fieldName)\n\t}\n\tresults, err := field.MutexCheck(ctx, qcx, details, limit)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif limit != 0 && len(results) > limit {\n\t\ttoDel := len(results) - limit\n\t\t// yes, Go allows you to delete keys you've already seen while\n\t\t// iterating a map. The spec says that if a value not-yet-reached\n\t\t// is deleted during iteration, it may or may not appear; this\n\t\t// carries the implication that deleting things during map iteration\n\t\t// is safe.\n\t\tfor k := range results {\n\t\t\tdelete(results, k)\n\t\t\ttoDel--\n\t\t\tif toDel == 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\treturn results, err\n}\n\n// mergeIDLists merges a list of numeric IDs into another list, removing\n// duplicates.\nfunc mergeIDLists(dst []uint64, src []uint64) []uint64 {\n\tdst = append(dst, src...)\n\tsort.Slice(dst, func(i, j int) bool {\n\t\treturn dst[i] < dst[j]\n\t})\n\t// dedup.\n\tn := 1\n\tprev := dst[0]\n\tfor i := 1; i < len(dst); i++ {\n\t\tif dst[i] != prev {\n\t\t\tdst[n] = dst[i]\n\t\t\tn++\n\t\t}\n\t\tprev = dst[i]\n\t}\n\treturn dst[:n]\n}\n\n// mergeKeyLists merges a list of string IDs into another list, removing\n// duplicates.\nfunc mergeKeyLists(dst []string, src []string) []string {\n\tdst = append(dst, src...)\n\tsort.Slice(dst, func(i, j int) bool {\n\t\treturn dst[i] < dst[j]\n\t})\n\t// dedup.\n\tn := 1\n\tprev := dst[0]\n\tfor i := 1; i < len(dst); i++ {\n\t\tif dst[i] != prev {\n\t\t\tdst[n] = dst[i]\n\t\t\tn++\n\t\t}\n\t\tprev = dst[i]\n\t}\n\treturn dst[:n]\n}\n\n// MutexCheckNode checks for collisions in a given mutex field. The response is\n// a map[shard]map[column]values, not translated.\nfunc (api *API) MutexCheckNode(ctx context.Context, qcx *Qcx, indexName string, fieldName string, details bool, limit int) (map[uint64]map[uint64][]uint64, error) {\n\tif err := api.validate(apiMutexCheck); err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating api method\")\n\t}\n\treturn api.mutexCheckThisNode(ctx, qcx, indexName, fieldName, details, limit)\n}\n\n// MutexCheck checks a named field for mutex violations, returning a\n// map of record IDs to values for records that have multiple values in the\n// field. The return will be one of:\n//\n//\tdetails true:\n//\tmap[uint64][]uint64 // unkeyed index, unkeyed field\n//\tmap[uint64][]string // unkeyed index, keyed field\n//\tmap[string][]uint64 // keyed index, unkeyed field\n//\tmap[string][]string // keyed index, keyed field\n//\tdetails false:\n//\t[]uint64            // unkeyed index\n//\t[]string            // keyed index\nfunc (api *API) MutexCheck(ctx context.Context, qcx *Qcx, indexName string, fieldName string, details bool, limit int) (result interface{}, err error) {\n\tif err = api.validate(apiMutexCheck); err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating api method\")\n\t}\n\tindex, err := api.Index(ctx, indexName)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfield, err := api.Field(ctx, indexName, fieldName)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif field.Type() != FieldTypeMutex {\n\t\treturn nil, errors.New(\"can only check mutex state for mutex fields\")\n\t}\n\t// request data from other nodes as well\n\tsnap := api.cluster.NewSnapshot()\n\teg, _ := errgroup.WithContext(ctx)\n\tmyID := api.NodeID()\n\tresults := make([]map[uint64]map[uint64][]uint64, len(snap.Nodes))\n\tfor i, node := range snap.Nodes {\n\t\ti := i // loop variable shadowing is a war crime\n\t\tif node.ID != myID {\n\t\t\tnode := node // loop variable shadowing again\n\t\t\teg.Go(func() (err error) {\n\t\t\t\tresults[i], err = api.server.defaultClient.MutexCheck(ctx, &node.URI, indexName, fieldName, details, limit)\n\t\t\t\treturn err\n\t\t\t})\n\t\t} else {\n\t\t\teg.Go(func() (err error) {\n\t\t\t\tresults[i], err = api.mutexCheckThisNode(ctx, qcx, indexName, fieldName, details, limit)\n\t\t\t\treturn err\n\t\t\t})\n\t\t}\n\t}\n\terr = eg.Wait()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// Set this arbitrarily large so we don't have to be hand-checking for 0\n\t// throughout.\n\tif limit == 0 {\n\t\tlimit = math.MaxInt32\n\t}\n\t// We now have a series of maps from shards to maps of record IDs to\n\t// values. But wait! Either the field, or the index, might be using keys,\n\t// and want those translated. So we have to translate those. We'll create\n\t// some tables.\n\tuseIndexKeys := index.Keys()\n\t// If we're not doing details, we won't translate field keys even if we could.\n\tuseFieldKeys := field.Keys() && details\n\tindexKeys := map[uint64]string{}\n\tfieldKeys := map[uint64]string{}\n\tvar indexIDs []uint64\n\tvar fieldIDs []uint64\n\t// We'll use the string \"untranslated\" as our default value and overwrite\n\t// it with translations. We do check for missing translation values in\n\t// our returns, but just in case, you know?\n\tuntranslated := \"untranslated\"\n\t// We don't know which of four map types we want to be working with,\n\t// but what we can do is make a function which works with that map type\n\t// given the raw integer values, and is a closure with an already-created\n\t// map which has already been stashed in `result`. Because maps are\n\t// reference-y, this should actually work. This function returns true if\n\t// it's hit the limit for length of results.\n\tvar process func(uint64, []uint64) bool\n\tif useIndexKeys || useFieldKeys {\n\t\tfor _, nodeResults := range results {\n\t\t\tfor _, shardResults := range nodeResults {\n\t\t\t\tfor record, values := range shardResults {\n\t\t\t\t\tif useIndexKeys {\n\t\t\t\t\t\tif _, ok := indexKeys[record]; !ok {\n\t\t\t\t\t\t\tindexKeys[record] = untranslated\n\t\t\t\t\t\t\tindexIDs = append(indexIDs, record)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tif useFieldKeys {\n\t\t\t\t\t\tfor _, value := range values {\n\t\t\t\t\t\t\tif _, ok := fieldKeys[value]; !ok {\n\t\t\t\t\t\t\t\tfieldKeys[value] = untranslated\n\t\t\t\t\t\t\t\tfieldIDs = append(fieldIDs, value)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// if context is done, return early.\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tuntranslatedKeys := 0\n\t\t// Obtain translation tables for the keys.\n\t\tif useIndexKeys {\n\t\t\tindexKeyList, err := api.cluster.translateIndexIDs(ctx, indexName, indexIDs)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"translating index keys\")\n\t\t\t}\n\t\t\tif len(indexKeyList) != len(indexIDs) {\n\t\t\t\treturn nil, fmt.Errorf(\"translating %d record IDs, got %d keys\", len(indexIDs), len(indexKeyList))\n\t\t\t}\n\t\t\tfor i := range indexIDs {\n\t\t\t\tif indexKeyList[i] != \"\" {\n\t\t\t\t\tindexKeys[indexIDs[i]] = indexKeyList[i]\n\t\t\t\t} else {\n\t\t\t\t\tuntranslatedKeys++\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// if context is done, return early.\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif useFieldKeys {\n\t\t\tfieldKeyList, err := api.cluster.translateFieldListIDs(ctx, field, fieldIDs)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"translating index keys\")\n\t\t\t}\n\t\t\tif len(fieldKeyList) != len(fieldIDs) {\n\t\t\t\treturn nil, fmt.Errorf(\"translating %d IDs, got %d keys\", len(indexIDs), len(fieldKeyList))\n\t\t\t}\n\t\t\tfor i := range fieldIDs {\n\t\t\t\tif fieldKeyList[i] != \"\" {\n\t\t\t\t\tfieldKeys[fieldIDs[i]] = fieldKeyList[i]\n\t\t\t\t} else {\n\t\t\t\t\tuntranslatedKeys++\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif untranslatedKeys > 0 {\n\t\t\tapi.server.logger.Warnf(\"translating mutex check results: %d key(s) untranslated\", untranslatedKeys)\n\t\t}\n\t}\n\n\t// if context is done, return early.\n\tif err := ctx.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// define the process functions. separated from above code just to make\n\t// it easier to follow/compare them.\n\tif useIndexKeys {\n\t\tif !details {\n\t\t\toutMap := make(map[uint64]struct{})\n\t\t\toutStrings := []string{}\n\t\t\t// unlike a map, the slice won't get updated-in-place, so we have\n\t\t\t// to assign to result after we're done\n\t\t\tdefer func() {\n\t\t\t\tif err == nil {\n\t\t\t\t\tresult = outStrings\n\t\t\t\t}\n\t\t\t}()\n\t\t\tprocess = func(recordID uint64, valueIDs []uint64) bool {\n\t\t\t\tif _, ok := outMap[recordID]; ok {\n\t\t\t\t\treturn len(outMap) >= limit\n\t\t\t\t}\n\t\t\t\toutMap[recordID] = struct{}{}\n\t\t\t\toutStrings = append(outStrings, indexKeys[recordID])\n\t\t\t\treturn len(outMap) >= limit\n\t\t\t}\n\t\t} else if useFieldKeys {\n\t\t\toutMap := make(map[string][]string)\n\t\t\tvar valueKeys []string\n\t\t\tresult = outMap\n\t\t\tprocess = func(recordID uint64, valueIDs []uint64) bool {\n\t\t\t\tvalueKeys = valueKeys[:0]\n\t\t\t\tfor _, id := range valueIDs {\n\t\t\t\t\tvalueKeys = append(valueKeys, fieldKeys[id])\n\t\t\t\t}\n\t\t\t\trecord := indexKeys[recordID]\n\t\t\t\tif existing, ok := outMap[record]; ok {\n\t\t\t\t\toutMap[record] = mergeKeyLists(existing, valueKeys)\n\t\t\t\t} else if len(outMap) < limit {\n\t\t\t\t\t// The append is so we can reuse this buffer safely,\n\t\t\t\t\t// which matters if there's replication, because many\n\t\t\t\t\t// cases won't need to copy the buffer, they'll just\n\t\t\t\t\t// copy individual things from it.\n\t\t\t\t\toutMap[record] = append([]string{}, valueKeys...)\n\t\t\t\t}\n\t\t\t\treturn len(outMap) >= limit\n\t\t\t}\n\t\t} else {\n\t\t\toutMap := make(map[string][]uint64)\n\t\t\tresult = outMap\n\t\t\tprocess = func(recordID uint64, values []uint64) bool {\n\t\t\t\trecord := indexKeys[recordID]\n\t\t\t\tif existing, ok := outMap[record]; ok {\n\t\t\t\t\toutMap[record] = mergeIDLists(existing, values)\n\t\t\t\t} else if len(outMap) < limit {\n\t\t\t\t\toutMap[record] = values\n\t\t\t\t}\n\t\t\t\treturn len(outMap) >= limit\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif !details {\n\t\t\toutMap := make(map[uint64]struct{})\n\t\t\toutIDs := []uint64{}\n\t\t\t// unlike a map, the slice won't get updated-in-place, so we have\n\t\t\t// to assign to result after we're done\n\t\t\tdefer func() {\n\t\t\t\tif err == nil {\n\t\t\t\t\tresult = outIDs\n\t\t\t\t}\n\t\t\t}()\n\t\t\tprocess = func(recordID uint64, valueIDs []uint64) bool {\n\t\t\t\tif _, ok := outMap[recordID]; ok {\n\t\t\t\t\treturn len(outMap) >= limit\n\t\t\t\t}\n\t\t\t\toutMap[recordID] = struct{}{}\n\t\t\t\toutIDs = append(outIDs, recordID)\n\t\t\t\treturn len(outMap) >= limit\n\t\t\t}\n\t\t} else if useFieldKeys {\n\t\t\toutMap := make(map[uint64][]string)\n\t\t\tvar valueKeys []string\n\t\t\tresult = outMap\n\t\t\tprocess = func(record uint64, valueIDs []uint64) bool {\n\t\t\t\tvalueKeys = valueKeys[:0]\n\t\t\t\tfor _, id := range valueIDs {\n\t\t\t\t\tvalueKeys = append(valueKeys, fieldKeys[id])\n\t\t\t\t}\n\t\t\t\tif existing, ok := outMap[record]; ok {\n\t\t\t\t\toutMap[record] = mergeKeyLists(existing, valueKeys)\n\t\t\t\t} else {\n\t\t\t\t\t// The append is so we can reuse this buffer safely,\n\t\t\t\t\t// which matters if there's replication, because many\n\t\t\t\t\t// cases won't need to copy the buffer, they'll just\n\t\t\t\t\t// copy individual things from it.\n\t\t\t\t\toutMap[record] = append([]string{}, valueKeys...)\n\t\t\t\t}\n\t\t\t\treturn len(outMap) >= limit\n\t\t\t}\n\t\t} else {\n\t\t\toutMap := make(map[uint64][]uint64)\n\t\t\tresult = outMap\n\t\t\tprocess = func(record uint64, values []uint64) bool {\n\t\t\t\tif existing, ok := outMap[record]; ok {\n\t\t\t\t\toutMap[record] = mergeIDLists(existing, values)\n\t\t\t\t} else {\n\t\t\t\t\toutMap[record] = values\n\t\t\t\t}\n\t\t\t\treturn len(outMap) >= limit\n\t\t\t}\n\t\t}\n\t}\n\n\t// if you specify a limit, and you have *different* errors on different\n\t// nodes, we will not check all of the nodes. otherwise there's no practical\n\t// way to get the primary benefit of specifying a limit.\nprocessing:\n\tfor _, nodeResults := range results {\n\t\tif len(nodeResults) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\t// if context is done, return early.\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfor _, v := range nodeResults {\n\t\t\tif len(v) == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcounter := 0\n\t\t\tfor record, values := range v {\n\t\t\t\tcounter++\n\t\t\t\tif process(record, values) {\n\t\t\t\t\tbreak processing\n\t\t\t\t}\n\t\t\t\t// every 65k items or so, check the context for done-ness\n\t\t\t\tif counter%(1<<16) == 0 {\n\t\t\t\t\tif err := ctx.Err(); err != nil {\n\t\t\t\t\t\treturn nil, err\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn result, ctx.Err()\n}\n\n// CompilePlan takes a sql string and returns a PlanOperator. Note that this is\n// different from the internal CompilePlan() method on the CompilePlanner\n// interface, which takes a parser statement and returns a PlanOperator. In\n// other words, this CompilePlan() both parses and plans the provided sql\n// string; it's the equivalent of the CompileExecutionPlan() method on Server.\n// TODO: consider renaming this to something with less conflict.\nfunc (api *API) CompilePlan(ctx context.Context, q string) (planner_types.PlanOperator, error) {\n\treturn api.server.CompileExecutionPlan(ctx, q)\n}\n\nfunc (api *API) RehydratePlanOperator(ctx context.Context, reader io.Reader) (planner_types.PlanOperator, error) {\n\treturn api.server.RehydratePlanOperator(ctx, reader)\n}\n\nfunc (api *API) RBFDebugInfo() map[string]*rbf.DebugInfo {\n\tinfos := make(map[string]*rbf.DebugInfo)\n\n\tfor key, dbShard := range api.holder.Txf().dbPerShard.Flatmap {\n\t\twrapper, ok := dbShard.W.(*RbfDBWrapper)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tskey := fmt.Sprintf(\"%s/%d\", key.index, key.shard)\n\t\tinfos[skey] = wrapper.db.DebugInfo()\n\t}\n\treturn infos\n}\n\n// Directive applies the provided Directive to the local computer.\nfunc (api *API) Directive(ctx context.Context, d *dax.Directive) error {\n\treturn api.ApplyDirective(ctx, d)\n}\n\n// DirectiveApplied returns true if the computer's current Directive has been\n// applied and is ready to be queried. This is temporary (primarily for tests)\n// and needs to be refactored as we improve the logic around\n// controller-to-computer communication.\nfunc (api *API) DirectiveApplied(ctx context.Context) (bool, error) {\n\treturn api.holder.DirectiveApplied(), nil\n}\n\n// SnapshotShardData triggers the node to perform a shard snapshot based on the\n// provided SnapshotShardDataRequest.\nfunc (api *API) SnapshotShardData(ctx context.Context, req *dax.SnapshotShardDataRequest) error {\n\tif !api.holder.DirectiveApplied() {\n\t\treturn errors.New(\"don't have directive yet, can't snapshot shard\")\n\t}\n\t// TODO(jaffee) confirm this node is actually responsible for the given\n\t// shard? Not sure we need to given that this request comes from\n\t// the Controller, but might be a belt&suspenders situation.\n\n\tqtid := req.TableKey.QualifiedTableID()\n\n\tpartition := disco.ShardToShardPartition(string(req.TableKey), uint64(req.ShardNum), disco.DefaultPartitionN)\n\tpartitionNum := dax.PartitionNum(partition)\n\n\t// Open a write Tx snapshotting current version.\n\trc, err := api.IndexShardSnapshot(ctx, string(req.TableKey), uint64(req.ShardNum), true)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting index/shard readcloser\")\n\t}\n\tdefer rc.Close()\n\n\tresource := api.serverlessStorage.GetShardResource(qtid, partitionNum, req.ShardNum)\n\t// Bump writelog version while write Tx is held.\n\tif ok, err := resource.IncrementWLVersion(); err != nil {\n\t\treturn errors.Wrap(err, \"incrementing write log version\")\n\t} else if !ok {\n\t\treturn nil\n\t}\n\t// TODO(jaffee) look into downgrading Tx on RBF to read lock here now that WL version is incremented.\n\terr = resource.Snapshot(rc)\n\treturn errors.Wrap(err, \"snapshotting shard data\")\n}\n\n// SnapshotTableKeys triggers the node to perform a table keys snapshot based on\n// the provided SnapshotTableKeysRequest.\nfunc (api *API) SnapshotTableKeys(ctx context.Context, req *dax.SnapshotTableKeysRequest) error {\n\tif !api.holder.DirectiveApplied() {\n\t\treturn errors.New(\"don't have directive yet, can't snapshot table keys\")\n\t}\n\t// If the index is not keyed, no-op on snapshotting its keys.\n\tif idx, err := api.Index(ctx, string(req.TableKey)); err != nil {\n\t\treturn newNotFoundError(ErrIndexNotFound, string(req.TableKey))\n\t} else if !idx.Keys() {\n\t\treturn nil\n\t}\n\n\tqtid := req.TableKey.QualifiedTableID()\n\n\t// Create the snapshot for the current version.\n\ttrans, err := api.TranslateData(ctx, string(req.TableKey), int(req.PartitionNum))\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"getting index/partition translate store: %s/%d\", req.TableKey, req.PartitionNum)\n\t}\n\t// get a write tx to ensure no other writes while incrementing WL version.\n\twrTo, err := trans.Begin(true)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"beginning table translate write tx\")\n\t}\n\tdefer wrTo.Rollback()\n\n\tresource := api.serverlessStorage.GetTableKeyResource(qtid, req.PartitionNum)\n\tif ok, err := resource.IncrementWLVersion(); err != nil {\n\t\treturn errors.Wrap(err, \"incrementing write log version\")\n\t} else if !ok {\n\t\t// no need to snapshot, no writes\n\t\treturn nil\n\t}\n\t// TODO(jaffee) downgrade write tx to read-only\n\terr = resource.SnapshotTo(wrTo)\n\treturn errors.Wrap(err, \"snapshotting table keys\")\n}\n\n// SnapshotFieldKeys triggers the node to perform a field keys snapshot based on\n// the provided SnapshotFieldKeysRequest.\nfunc (api *API) SnapshotFieldKeys(ctx context.Context, req *dax.SnapshotFieldKeysRequest) error {\n\tif !api.holder.DirectiveApplied() {\n\t\treturn errors.New(\"don't have directive yet, can't snapshot field keys\")\n\t}\n\tqtid := req.TableKey.QualifiedTableID()\n\n\t// Create the snapshot for the current version.\n\ttrans, err := api.FieldTranslateData(ctx, string(req.TableKey), string(req.Field))\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting index/field translator\")\n\t}\n\t// get a write tx to ensure no other writes while incrementing WL version.\n\twrTo, err := trans.Begin(true)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"beginning field translate write tx\")\n\t}\n\tdefer wrTo.Rollback()\n\n\tresource := api.serverlessStorage.GetFieldKeyResource(qtid, req.Field)\n\tif ok, err := resource.IncrementWLVersion(); err != nil {\n\t\treturn errors.Wrap(err, \"incrementing writelog version\")\n\t} else if !ok {\n\t\t// no need to snapshot, no writes\n\t\treturn nil\n\t}\n\t// TODO(jaffee) downgrade to read tx\n\terr = resource.SnapshotTo(wrTo)\n\treturn errors.Wrap(err, \"snapshotTo in FieldKeys\")\n}\n\ntype serverInfo struct {\n\tShardWidth       uint64 `json:\"shardWidth\"`\n\tReplicaN         int    `json:\"replicaN\"`\n\tShardHash        string `json:\"shardHash\"`\n\tKeyHash          string `json:\"keyHash\"`\n\tMemory           uint64 `json:\"memory\"`\n\tCPUType          string `json:\"cpuType\"`\n\tCPUPhysicalCores int    `json:\"cpuPhysicalCores\"`\n\tCPULogicalCores  int    `json:\"cpuLogicalCores\"`\n\tCPUMHz           int    `json:\"cpuMHz\"`\n\tStorageBackend   string `json:\"storageBackend\"`\n}\n\ntype apiMethod int\n\n// API validation constants.\nconst (\n\tapiClusterMessage apiMethod = iota\n\tapiCreateField\n\tapiCreateIndex\n\tapiDeleteField\n\tapiDeleteAvailableShard\n\tapiDeleteIndex\n\tapiDeleteView\n\tapiExportCSV\n\tapiFragmentBlockData\n\tapiFragmentBlocks\n\tapiFragmentData\n\tapiTranslateData\n\tapiFieldTranslateData\n\tapiField\n\t// apiHosts // not implemented\n\tapiImport\n\tapiImportValue\n\tapiIndex\n\t// apiLocalID // not implemented\n\t// apiLongQueryTime // not implemented\n\t// apiMaxShards // not implemented\n\tapiQuery\n\tapiRecalculateCaches\n\tapiSchema\n\tapiShardNodes\n\tapiState\n\t// apiStatsWithTags // not implemented\n\t// apiVersion // not implemented\n\tapiViews\n\tapiApplySchema\n\tapiStartTransaction\n\tapiFinishTransaction\n\tapiTransactions\n\tapiGetTransaction\n\tapiActiveQueries\n\tapiPastQueries\n\tapiIDReserve\n\tapiIDCommit\n\tapiIDReset\n\tapiPartitionNodes\n\tapiMutexCheck\n\tapiApplyChangeset\n\tapiDeleteDataframe\n)\n\nvar methodsCommon = map[apiMethod]struct{}{\n\tapiClusterMessage: {},\n\tapiState:          {},\n}\n\nvar methodsDegraded = map[apiMethod]struct{}{\n\tapiExportCSV:         {},\n\tapiFragmentBlockData: {},\n\tapiFragmentBlocks:    {},\n\tapiField:             {},\n\tapiIndex:             {},\n\tapiQuery:             {},\n\tapiRecalculateCaches: {},\n\tapiShardNodes:        {},\n\tapiSchema:            {},\n\tapiViews:             {},\n\tapiStartTransaction:  {},\n\tapiFinishTransaction: {},\n\tapiTransactions:      {},\n\tapiGetTransaction:    {},\n\tapiActiveQueries:     {},\n\tapiPastQueries:       {},\n\tapiPartitionNodes:    {},\n}\n\nvar methodsNormal = map[apiMethod]struct{}{\n\tapiCreateField:          {},\n\tapiCreateIndex:          {},\n\tapiDeleteField:          {},\n\tapiDeleteAvailableShard: {},\n\tapiDeleteIndex:          {},\n\tapiDeleteView:           {},\n\tapiExportCSV:            {},\n\tapiFragmentBlockData:    {},\n\tapiFragmentBlocks:       {},\n\tapiField:                {},\n\tapiFieldTranslateData:   {},\n\tapiImport:               {},\n\tapiImportValue:          {},\n\tapiIndex:                {},\n\tapiQuery:                {},\n\tapiRecalculateCaches:    {},\n\tapiShardNodes:           {},\n\tapiSchema:               {},\n\tapiViews:                {},\n\tapiApplySchema:          {},\n\tapiStartTransaction:     {},\n\tapiFinishTransaction:    {},\n\tapiTransactions:         {},\n\tapiTranslateData:        {},\n\tapiGetTransaction:       {},\n\tapiActiveQueries:        {},\n\tapiPastQueries:          {},\n\tapiIDReserve:            {},\n\tapiIDCommit:             {},\n\tapiIDReset:              {},\n\tapiPartitionNodes:       {},\n\tapiMutexCheck:           {},\n\tapiApplyChangeset:       {},\n\tapiDeleteDataframe:      {},\n}\n\nfunc shardInShards(i dax.ShardNum, s dax.ShardNums) bool {\n\tfor _, o := range s {\n\t\tif i == o {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\ntype SchemaAPI interface {\n\tCreateDatabase(context.Context, *dax.Database) error\n\tDropDatabase(context.Context, dax.DatabaseID) error\n\n\tDatabaseByName(ctx context.Context, dbname dax.DatabaseName) (*dax.Database, error)\n\tDatabaseByID(ctx context.Context, dbid dax.DatabaseID) (*dax.Database, error)\n\tSetDatabaseOption(ctx context.Context, dbid dax.DatabaseID, option string, value string) error\n\tDatabases(context.Context, ...dax.DatabaseID) ([]*dax.Database, error)\n\n\tTableByName(ctx context.Context, tname dax.TableName) (*dax.Table, error)\n\tTableByID(ctx context.Context, tid dax.TableID) (*dax.Table, error)\n\tTables(ctx context.Context) ([]*dax.Table, error)\n\n\tCreateTable(ctx context.Context, tbl *dax.Table) error\n\tCreateField(ctx context.Context, tname dax.TableName, fld *dax.Field) error\n\n\tDeleteTable(ctx context.Context, tname dax.TableName) error\n\tDeleteField(ctx context.Context, tname dax.TableName, fname dax.FieldName) error\n}\n\n// Ensure type implements interface.\nvar _ SchemaAPI = (*NopSchemaAPI)(nil)\n\n// NopSchemaAPI is a no-op implementation of the SchemaAPI.\ntype NopSchemaAPI struct{}\n\nfunc (n *NopSchemaAPI) ClusterName() string {\n\treturn \"\"\n}\nfunc (n *NopSchemaAPI) CreateDatabase(context.Context, *dax.Database) error { return nil }\nfunc (n *NopSchemaAPI) DropDatabase(context.Context, dax.DatabaseID) error  { return nil }\nfunc (n *NopSchemaAPI) DatabaseByName(ctx context.Context, dbname dax.DatabaseName) (*dax.Database, error) {\n\treturn nil, nil\n}\nfunc (n *NopSchemaAPI) DatabaseByID(ctx context.Context, dbid dax.DatabaseID) (*dax.Database, error) {\n\treturn nil, nil\n}\nfunc (n *NopSchemaAPI) SetDatabaseOption(ctx context.Context, dbid dax.DatabaseID, option string, value string) error {\n\treturn nil\n}\nfunc (n *NopSchemaAPI) Databases(context.Context, ...dax.DatabaseID) ([]*dax.Database, error) {\n\treturn nil, nil\n}\nfunc (n *NopSchemaAPI) TableByName(ctx context.Context, tname dax.TableName) (*dax.Table, error) {\n\treturn nil, nil\n}\nfunc (n *NopSchemaAPI) TableByID(ctx context.Context, tid dax.TableID) (*dax.Table, error) {\n\treturn nil, nil\n}\nfunc (n *NopSchemaAPI) Tables(ctx context.Context) ([]*dax.Table, error) { return nil, nil }\n\nfunc (n *NopSchemaAPI) CreateTable(ctx context.Context, tbl *dax.Table) error { return nil }\nfunc (n *NopSchemaAPI) CreateField(ctx context.Context, tname dax.TableName, fld *dax.Field) error {\n\treturn nil\n}\nfunc (n *NopSchemaAPI) DeleteTable(ctx context.Context, tname dax.TableName) error { return nil }\nfunc (n *NopSchemaAPI) DeleteField(ctx context.Context, tname dax.TableName, fname dax.FieldName) error {\n\treturn nil\n}\n\ntype ClusterNode struct {\n\tID        string\n\tType      string\n\tState     string\n\tURI       string\n\tGRPCURI   string\n\tIsPrimary bool\n}\n\ntype SystemAPI interface {\n\tClusterName() string\n\tVersion() string\n\tPlatformDescription() string\n\tPlatformVersion() string\n\tClusterNodeCount() int\n\tClusterReplicaCount() int\n\tShardWidth() int\n\tClusterState() string\n\tDataDir() string\n\n\tNodeID() string\n\tClusterNodes() []ClusterNode\n}\n\n// CreateFieldObj is used to encapsulate the information required for creating a\n// field in the SchemaAPI.CreateIndexAndFields interface method.\ntype CreateFieldObj struct {\n\tName    string\n\tOptions []FieldOption\n}\n\n// QueryAPI is a subset of the API methods which have to do with query.\ntype QueryAPI interface {\n\tQuery(ctx context.Context, req *QueryRequest) (QueryResponse, error)\n}\n\n// Ensure type implements interface.\nvar _ SystemAPI = (*FeatureBaseSystemAPI)(nil)\n\n// FeatureBaseSystemAPI is a wrapper around pilosa.API. It implements the\n// SystemAPI interface\ntype FeatureBaseSystemAPI struct {\n\t*API\n}\n\nfunc (fsapi *FeatureBaseSystemAPI) ClusterName() string {\n\treturn fsapi.API.ClusterName()\n}\n\nfunc (fsapi *FeatureBaseSystemAPI) Version() string {\n\treturn fsapi.API.Version()\n}\n\nfunc (fsapi *FeatureBaseSystemAPI) PlatformDescription() string {\n\tsi := fsapi.server.systemInfo\n\tplatform, err := si.Platform()\n\tif err != nil {\n\t\treturn \"unknown\"\n\t}\n\treturn platform\n}\n\nfunc (fsapi *FeatureBaseSystemAPI) PlatformVersion() string {\n\tsi := fsapi.server.systemInfo\n\tplatformVersion, err := si.OSVersion()\n\tif err != nil {\n\t\treturn \"unknown\"\n\t}\n\treturn platformVersion\n}\n\nfunc (fsapi *FeatureBaseSystemAPI) ClusterNodeCount() int {\n\treturn len(fsapi.cluster.noder.Nodes())\n}\n\nfunc (fsapi *FeatureBaseSystemAPI) ClusterReplicaCount() int {\n\treturn fsapi.cluster.ReplicaN\n}\n\nfunc (fsapi *FeatureBaseSystemAPI) ShardWidth() int {\n\treturn ShardWidth\n}\n\nfunc (fsapi *FeatureBaseSystemAPI) ClusterState() string {\n\tstate, err := fsapi.State()\n\tif err != nil {\n\t\treturn \"UNKNOWN\"\n\t}\n\treturn string(state)\n}\n\nfunc (fsapi *FeatureBaseSystemAPI) DataDir() string {\n\treturn fsapi.server.dataDir\n}\n\nfunc (fsapi *FeatureBaseSystemAPI) NodeID() string {\n\treturn fsapi.cluster.Node.ID\n}\n\nfunc (fsapi *FeatureBaseSystemAPI) ClusterNodes() []ClusterNode {\n\tresult := make([]ClusterNode, 0)\n\n\tnodes := fsapi.Hosts(context.Background())\n\n\tfor _, n := range nodes {\n\t\tscn := ClusterNode{\n\t\t\tID:        n.ID,\n\t\t\tState:     string(n.State),\n\t\t\tURI:       n.URI.String(),\n\t\t\tGRPCURI:   n.GRPCURI.String(),\n\t\t\tIsPrimary: n.IsPrimary,\n\t\t}\n\t\tresult = append(result, scn)\n\t}\n\n\treturn result\n}\n\n// Ensure type implements interface.\nvar _ SystemAPI = (*NopSystemAPI)(nil)\n\n// NopSystemAPI is a no-op implementation of the SystemAPI.\ntype NopSystemAPI struct{}\n\nfunc (napi *NopSystemAPI) ClusterName() string {\n\treturn \"\"\n}\n\nfunc (napi *NopSystemAPI) Version() string {\n\treturn \"\"\n}\n\nfunc (napi *NopSystemAPI) PlatformDescription() string {\n\treturn \"\"\n}\n\nfunc (napi *NopSystemAPI) PlatformVersion() string {\n\treturn \"\"\n}\n\nfunc (napi *NopSystemAPI) ClusterNodeCount() int {\n\treturn 0\n}\n\nfunc (napi *NopSystemAPI) ClusterReplicaCount() int {\n\treturn 0\n}\n\nfunc (napi *NopSystemAPI) ShardWidth() int {\n\treturn 0\n}\n\nfunc (napi *NopSystemAPI) ClusterState() string {\n\treturn \"\"\n}\n\nfunc (napi *NopSystemAPI) DataDir() string {\n\treturn \"\"\n}\n\nfunc (napi *NopSystemAPI) NodeID() string {\n\treturn \"\"\n}\n\nfunc (napi *NopSystemAPI) ClusterNodes() []ClusterNode {\n\tresult := make([]ClusterNode, 0)\n\treturn result\n}\n"
        },
        {
          "name": "api",
          "type": "tree",
          "content": null
        },
        {
          "name": "api_directive.go",
          "type": "blob",
          "size": 29.71484375,
          "content": "// Copyright 2021 Molecula Corp. All rights reserved.\npackage pilosa\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"log\"\n\t\"sync\"\n\n\t\"github.com/featurebasedb/featurebase/v3/dax\"\n\t\"github.com/featurebasedb/featurebase/v3/dax/computer\"\n\t\"github.com/featurebasedb/featurebase/v3/dax/storage\"\n\t\"github.com/featurebasedb/featurebase/v3/disco\"\n\t\"github.com/pkg/errors\"\n)\n\n// ApplyDirective applies a Directive received, from the Controller, at the\n// /directive endpoint.\nfunc (api *API) ApplyDirective(ctx context.Context, d *dax.Directive) error {\n\t// Get the current directive for comparison.\n\tpreviousDirective := api.holder.Directive()\n\n\t// Check that incoming version is newer.\n\t// Note: 0 is an invalid Directive version. This decision was made because\n\t// previousDirective is not a pointer to a directive, but a concrete\n\t// Directive. Which means we can't check for nil, and by default it has a\n\t// version of 0. So in order to ensure the version has increased, we need to\n\t// require that incoming directive versions are greater than 0.\n\tif d.Version == 0 {\n\t\treturn errors.Errorf(\"directive version cannot be 0\")\n\t} else if previousDirective.Version >= d.Version {\n\t\treturn errors.Errorf(\"directive version mismatch, got %d, but already have %d\", d.Version, previousDirective.Version)\n\t}\n\n\t// Handle the operations based on the directive method.\n\tswitch d.Method {\n\tcase dax.DirectiveMethodDiff:\n\t\t// In order to prevent adding too much code specific to handling a diff\n\t\t// directive (e.g. adding something like an `enactDirectiveDiff()`\n\t\t// method), we are instead going to build a full Directive based on the\n\t\t// diff, and then proceed normally as if we had received a full\n\t\t// Directive. We do that by copying the previous Directive and then\n\t\t// applying the diffs to the copy.\n\t\tnewD := previousDirective.Copy()\n\n\t\t// Apply the diffs from the incoming Directive to the new, copied\n\t\t// Directive.\n\t\tnewD.ApplyDiff(d)\n\n\t\t// Now proceed with the new diff as if we had received it as a full diff.\n\t\td = newD\n\n\tcase dax.DirectiveMethodFull:\n\t\t// pass: normal operation\n\n\tcase dax.DirectiveMethodReset:\n\t\t// Delete all tables.\n\t\tif err := api.deleteAllIndexes(ctx); err != nil {\n\t\t\treturn errors.Wrap(err, \"deleting all indexes\")\n\t\t}\n\t\t// Set previousDirective to empty so the diff handles everything as new.\n\t\tpreviousDirective = dax.Directive{}\n\n\tcase dax.DirectiveMethodSnapshot:\n\t\t// TODO(tlt): this was the existing logic, but we should really diff the\n\t\t// directive and ensure that overwriting the value in the cache doesn't\n\t\t// have a negative effect.\n\t\tapi.holder.SetDirective(d)\n\t\treturn nil\n\n\tdefault:\n\t\treturn errors.Errorf(\"invalid directive method: %s\", d.Method)\n\t}\n\n\t// Cache this directive as the latest applied. There is functionality within\n\t// the \"enactDirective\" stage of ApplyDirective which validates against this\n\t// cached Directive, so it's important that it be set before calling\n\t// enactDirective(). An example: when loading partition data from the\n\t// Writelogger, there are validations to ensure that the partition being\n\t// loaded is meant to be handled by this node; that validation is done\n\t// against the cached Directive.\n\t// TODO(tlt): despite what this comment says, this logic is not sound; we\n\t// shouldn't be setting the directive until enactiveDirective() succeeds.\n\tapi.holder.SetDirective(d)\n\tdefer api.holder.SetDirectiveApplied(true)\n\n\treturn api.enactDirective(ctx, &previousDirective, d)\n}\n\n// deleteAllIndexes deletes all indexes handled by this node.\nfunc (api *API) deleteAllIndexes(ctx context.Context) error {\n\tindexes, err := api.Schema(ctx, false)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting schema\")\n\t}\n\n\tfor i := range indexes {\n\t\tif err := api.DeleteIndex(ctx, indexes[i].Name); err != nil {\n\t\t\treturn errors.Wrapf(err, \"deleting index: %s\", indexes[i].Name)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// directiveJobType allows us to switch on jobType in the directiveWorker in\n// order to use a single worker pool for all job types (as opposed to having a\n// separate worker pool for each job type).\ntype directiveJobType interface {\n\t// We have this method just to prevent *any* struct from implementing this\n\t// interface automatically. But, interestingly enough, we don't actually\n\t// have to have this method on the implementation because we embed the\n\t// interface.\n\tisJobType() bool\n}\n\ntype directiveJobTableKeys struct {\n\tdirectiveJobType\n\tidx       *Index\n\ttkey      dax.TableKey\n\tpartition dax.PartitionNum\n}\n\ntype directiveJobFieldKeys struct {\n\tdirectiveJobType\n\ttkey  dax.TableKey\n\tfield dax.FieldName\n}\n\ntype directiveJobShards struct {\n\tdirectiveJobType\n\ttkey  dax.TableKey\n\tshard dax.ShardNum\n}\n\n// directiveWorker is a worker in a worker pool which handles portions of a\n// directive. Multiple instances of directiveWorker run in goroutines in order\n// to load data from snapshotter and writelogger concurrently. Note: unlike the\n// api.ingestWorkerPool, of which one pool is always running, the\n// directiveWorker pool is only running during the life of the\n// api.ApplyDirective call. Technically, this means that multiple\n// directiveWorker pools could be active at the same time, but we should never\n// be running more than once instance of ApplyDirective concurrently.\nfunc (api *API) directiveWorker(ctx context.Context, jobs <-chan directiveJobType, errs chan<- error) {\n\tfor j := range jobs {\n\t\tswitch job := j.(type) {\n\t\tcase directiveJobTableKeys:\n\t\t\tif err := api.loadTableKeys(ctx, job.idx, job.tkey, job.partition); err != nil {\n\t\t\t\terrs <- errors.Wrapf(err, \"loading table keys: %s, %s\", job.tkey, job.partition)\n\t\t\t}\n\t\tcase directiveJobFieldKeys:\n\t\t\tif err := api.loadFieldKeys(ctx, job.tkey, job.field); err != nil {\n\t\t\t\terrs <- errors.Wrapf(err, \"loading field keys: %s, %s\", job.tkey, job.field)\n\t\t\t}\n\t\tcase directiveJobShards:\n\t\t\tif err := api.loadShard(ctx, job.tkey, job.shard); err != nil {\n\t\t\t\terrs <- errors.Wrapf(err, \"loading shard: %s, %s\", job.tkey, job.shard)\n\t\t\t}\n\t\tdefault:\n\t\t\terrs <- errors.Errorf(\"unsupported job type: %T %[1]v\", job)\n\t\t}\n\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tdefault:\n\t\t\t// continue pulling jobs off the channel\n\t\t}\n\t}\n}\n\nfunc (api *API) enactDirective(ctx context.Context, fromD, toD *dax.Directive) error {\n\t// enactTables is called before the jobs that run in the worker pool because\n\t// it probably makes sense to apply the schema before trying to load data\n\t// concurrently.\n\tif err := api.enactTables(ctx, fromD, toD); err != nil {\n\t\treturn errors.Wrap(err, \"enactTables\")\n\t}\n\n\t// The following types use a shared pool of workers to run each\n\t// directiveJobType.\n\n\tvar wg sync.WaitGroup\n\n\t// open job channel\n\tjobs := make(chan directiveJobType, api.directiveWorkerPoolSize)\n\terrs := make(chan error)\n\tdone := make(chan struct{})\n\n\t// Spin up n workers in goroutines that pull jobs from the jobs channel.\n\tfor i := 0; i < api.directiveWorkerPoolSize; i++ {\n\t\twg.Add(1)\n\t\tgo func() {\n\t\t\tapi.directiveWorker(ctx, jobs, errs)\n\t\t\tdefer wg.Done()\n\t\t}()\n\t}\n\n\t// Wait for the WaitGroup counter to reach 0. When it has, indicate that\n\t// we're done processing all jobs by closing the done channel.\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(done)\n\t}()\n\n\t// Run through all the \"enact\" methods. These push jobs onto the jobs\n\t// channel. Once all the jobs have been queued to the channel, we close the\n\t// jobs channel. This allows the directiveWorkers to exit out of the\n\t// function, which will then decrement the WaitGroup counter.\n\tgo func() {\n\t\tapi.pushJobsTableKeys(ctx, jobs, fromD, toD)\n\t\tapi.pushJobsFieldKeys(ctx, jobs, fromD, toD)\n\t\tapi.pushJobsShards(ctx, jobs, fromD, toD)\n\t\tclose(jobs)\n\t}()\n\n\t// Keep running until we get an error or until the done channel is closed.\n\t// Note: the code is written such that only non-nil errors are pushed to the\n\t// errs channel.\n\tfor {\n\t\tselect {\n\t\tcase err := <-errs:\n\t\t\treturn err\n\t\tcase <-done:\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\nfunc (api *API) enactTables(ctx context.Context, fromD, toD *dax.Directive) error {\n\tcurrentIndexes := api.holder.Indexes()\n\n\t// Make a list of indexes that currently exist (from).\n\tfrom := make(dax.TableKeys, 0, len(currentIndexes))\n\tfor _, idx := range currentIndexes {\n\t\tqtid, err := dax.QualifiedTableIDFromKey(idx.Name())\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"converting index name to qualified table id\")\n\t\t}\n\t\tfrom = append(from, qtid.Key())\n\t}\n\n\t// TODO sanity check holder against fromD. We're getting existing\n\t// indexes from holder, but in theory fromD should be\n\t// identical. If we have an error in our directive-caching logic\n\t// (it has happened before (just now, in fact!) and we'd be\n\t// foolish to think it won't happen again), or we have schema\n\t// mutations that are not going through the directive path, we\n\t// could potentially catch them here.\n\n\t// Make a list of tables that are in the directive (to) along with a map of\n\t// tableKey to table (m).\n\tm := make(map[dax.TableKey]*dax.QualifiedTable, len(toD.Tables))\n\tto := make(dax.TableKeys, 0, len(toD.Tables))\n\tfor _, t := range toD.Tables {\n\t\tm[t.Key()] = t\n\t\tto = append(to, t.Key())\n\t}\n\n\tsc := newSliceComparer(from, to)\n\n\t// Remove all indexes that are no longer part of the directive.\n\tfor _, tkey := range sc.removed() {\n\t\tidx := string(tkey)\n\t\tif err := api.DeleteIndex(ctx, idx); err != nil {\n\t\t\treturn errors.Wrapf(err, \"deleting index: %s\", tkey)\n\t\t}\n\t}\n\n\t// Put partitions into a map by table.\n\tpartitionMap := toD.TranslatePartitionsMap()\n\n\t// Add all indexes that weren't previously (but now are) a part of the\n\t// directive.\n\tfor _, tkey := range sc.added() {\n\t\tif qtbl, found := m[tkey]; !found {\n\t\t\treturn errors.Errorf(\"table '%s' was not in map\", tkey)\n\t\t} else if err := api.createTableAndFields(qtbl, partitionMap[tkey]); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Check fields on all indexes present in both from and to.\n\tfor _, tkey := range sc.same() {\n\t\tif err := api.enactFieldsForTable(ctx, tkey, fromD, toD); err != nil {\n\t\t\treturn errors.Wrapf(err, \"enacting fields for table: '%s'\", tkey)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (api *API) enactFieldsForTable(ctx context.Context, tkey dax.TableKey, fromD, toD *dax.Directive) error {\n\tqtid := tkey.QualifiedTableID()\n\n\tfromT, err := fromD.Table(qtid)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting from table\")\n\t}\n\ttoT, err := toD.Table(qtid)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting to table\")\n\t}\n\n\t// Get the index for tkey.\n\tidx := api.holder.Index(string(tkey))\n\tif idx == nil {\n\t\treturn errors.Errorf(\"index not found: %s\", tkey)\n\t}\n\n\tsc := newSliceComparer(fromT.FieldNames(), toT.FieldNames())\n\n\t// Add fields new to toT.\n\tfor _, fldName := range sc.added() {\n\t\tif field, found := toT.Field(fldName); !found {\n\t\t\treturn dax.NewErrFieldDoesNotExist(fldName)\n\t\t} else if err := createField(idx, field); err != nil {\n\t\t\treturn errors.Wrapf(err, \"creating field: %s/%s\", tkey, fldName)\n\t\t}\n\t}\n\n\t// Remove fields which don't exist in toT.\n\tfor _, fldName := range sc.removed() {\n\t\tif err := api.DeleteField(ctx, string(tkey), string(fldName)); err != nil {\n\t\t\treturn errors.Wrapf(err, \"deleting field: %s/%s\", tkey, fldName)\n\t\t}\n\t}\n\n\t// // Update any field options which have changed for existing fields.\n\t// for _, fldName := range sc.same() {\n\t// \t// handle changed field options??\n\t// }\n\n\treturn nil\n}\n\nfunc (api *API) pushJobsTableKeys(ctx context.Context, jobs chan<- directiveJobType, fromD, toD *dax.Directive) {\n\ttoPartitionsMap := toD.TranslatePartitionsMap()\n\n\t// Get the diff between from/to directive.partitions.\n\tpartComp := newPartitionsComparer(fromD.TranslatePartitionsMap(), toPartitionsMap)\n\n\t// Remove any partitions which are no longer assigned to this worker.\n\t// TODO(tlt): currently, this is just removing the file lock on the\n\t// resource; it's not actually removing the resource from the local\n\t// computer. We should do that.\n\tfor tkey, partitions := range partComp.removed() {\n\t\tqtid := tkey.QualifiedTableID()\n\t\tfor _, partition := range partitions {\n\t\t\tapi.serverlessStorage.RemoveTableKeyResource(qtid, partition)\n\t\t}\n\t}\n\n\t// Loop over the partition map and load from Writelogger.\n\tfor tkey, partitions := range partComp.added() {\n\t\t// Get index in order to find the translate stores (by partition) for\n\t\t// the table.\n\t\tidx := api.holder.Index(string(tkey))\n\t\tif idx == nil {\n\t\t\tlog.Printf(\"index not found in holder: %s\", tkey)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Update the cached version of translate partitions that we keep on the\n\t\t// Index.\n\t\tidx.SetTranslatePartitions(toPartitionsMap[tkey])\n\n\t\tfor _, partition := range partitions {\n\t\t\tjobs <- directiveJobTableKeys{\n\t\t\t\tidx:       idx,\n\t\t\t\ttkey:      tkey,\n\t\t\t\tpartition: partition,\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (api *API) loadTableKeys(ctx context.Context, idx *Index, tkey dax.TableKey, partition dax.PartitionNum) error {\n\tqtid := tkey.QualifiedTableID()\n\n\tresource := api.serverlessStorage.GetTableKeyResource(qtid, partition)\n\tif resource.IsLocked() {\n\t\tapi.logger().Warnf(\"skipping loadTableKeys (already held) %s %d\", tkey, partition)\n\t\treturn nil\n\t}\n\n\t// load latest snapshot\n\tif rc, err := resource.LoadLatestSnapshot(); err != nil {\n\t\treturn errors.Wrap(err, \"loading table key snapshot\")\n\t} else if rc != nil {\n\t\tdefer rc.Close()\n\t\tif err := api.TranslateIndexDB(ctx, string(tkey), int(partition), rc); err != nil {\n\t\t\treturn errors.Wrap(err, \"restoring table keys\")\n\t\t}\n\t}\n\n\t// define write log loading in a function since we have to do it\n\t// before and after locking\n\tloadWriteLog := func() error {\n\t\twritelog, err := resource.LoadWriteLog()\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"getting write log reader for table keys\")\n\t\t}\n\t\tif writelog == nil {\n\t\t\treturn nil\n\t\t}\n\t\treader := storage.NewTableKeyReader(qtid, partition, writelog)\n\t\tdefer reader.Close()\n\t\tstore := idx.TranslateStore(int(partition))\n\t\tfor msg, err := reader.Read(); err != io.EOF; msg, err = reader.Read() {\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Wrap(err, \"reading from log reader\")\n\t\t\t}\n\t\t\tfor key, id := range msg.StringToID {\n\t\t\t\tif err := store.ForceSet(id, key); err != nil {\n\t\t\t\t\treturn errors.Wrapf(err, \"forcing set id, key: %d, %s\", id, key)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\t// 1st write log load\n\tif err := loadWriteLog(); err != nil {\n\t\treturn err\n\t}\n\n\t// acquire lock on this partition's keys\n\tif err := resource.Lock(); err != nil {\n\t\treturn errors.Wrap(err, \"locking table key partition\")\n\t}\n\n\t// reload writelog in case of changes between last load and\n\t// lock. The resource object takes care of only loading new data.\n\treturn loadWriteLog()\n}\n\nfunc (api *API) pushJobsFieldKeys(ctx context.Context, jobs chan<- directiveJobType, fromD, toD *dax.Directive) {\n\t// Get the diff between from/to directive.fields.\n\tfieldComp := newFieldsComparer(fromD.TranslateFieldsMap(), toD.TranslateFieldsMap())\n\n\t// Remove any field keys which are no longer assigned to this worker.\n\t// TODO(tlt): currently, this is just removing the file lock on the\n\t// resource; it's not actually removing the resource from the local\n\t// computer. We should do that.\n\tfor tkey, fields := range fieldComp.removed() {\n\t\tqtid := tkey.QualifiedTableID()\n\t\tfor _, field := range fields {\n\t\t\tapi.serverlessStorage.RemoveFieldKeyResource(qtid, field)\n\t\t}\n\t}\n\n\t// Loop over the field map and load from Writelogger.\n\tfor tkey, fields := range fieldComp.added() {\n\t\tfor _, field := range fields {\n\t\t\tjobs <- directiveJobFieldKeys{\n\t\t\t\ttkey:  tkey,\n\t\t\t\tfield: field,\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (api *API) loadFieldKeys(ctx context.Context, tkey dax.TableKey, field dax.FieldName) error {\n\tqtid := tkey.QualifiedTableID()\n\n\tresource := api.serverlessStorage.GetFieldKeyResource(qtid, field)\n\tif resource.IsLocked() {\n\t\tapi.logger().Warnf(\"skipping loadFieldKeys (already held) %s %s\", tkey, field)\n\t\treturn nil\n\t}\n\n\t// load latest snapshot\n\tif rc, err := resource.LoadLatestSnapshot(); err != nil {\n\t\treturn errors.Wrap(err, \"loading field key snapshot\")\n\t} else if rc != nil {\n\t\tdefer rc.Close()\n\t\tif err := api.TranslateFieldDB(ctx, string(tkey), string(field), rc); err != nil {\n\t\t\treturn errors.Wrap(err, \"restoring field keys\")\n\t\t}\n\t}\n\n\t// define write log loading in a function since we have to do it\n\t// before and after locking\n\tloadWriteLog := func() error {\n\t\twritelog, err := resource.LoadWriteLog()\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"getting write log reader for field keys\")\n\t\t}\n\t\tif writelog == nil {\n\t\t\treturn nil\n\t\t}\n\t\treader := storage.NewFieldKeyReader(qtid, field, writelog)\n\t\tdefer reader.Close()\n\t\t// Get field in order to find the translate store.\n\t\tfld := api.holder.Field(string(tkey), string(field))\n\t\tif fld == nil {\n\t\t\tlog.Printf(\"field not found in holder: %s\", field)\n\t\t\treturn nil\n\t\t}\n\t\tstore := fld.TranslateStore()\n\n\t\tfor msg, err := reader.Read(); err != io.EOF; msg, err = reader.Read() {\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Wrap(err, \"reading from log reader\")\n\t\t\t}\n\t\t\tfor key, id := range msg.StringToID {\n\t\t\t\tif err := store.ForceSet(id, key); err != nil {\n\t\t\t\t\treturn errors.Wrapf(err, \"forcing set id, key: %d, %s\", id, key)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\t// 1st write log load\n\tif err := loadWriteLog(); err != nil {\n\t\treturn err\n\t}\n\n\t// acquire lock on this partition's keys\n\tif err := resource.Lock(); err != nil {\n\t\treturn errors.Wrap(err, \"locking field key partition\")\n\t}\n\n\t// reload writelog in case of changes between last load and\n\t// lock. The resource object takes care of only loading new data.\n\treturn loadWriteLog()\n}\n\nfunc (api *API) pushJobsShards(ctx context.Context, jobs chan<- directiveJobType, fromD, toD *dax.Directive) {\n\t// Put shards into a map by table.\n\tshardMap := toD.ComputeShardsMap()\n\n\t// Get the diff between from/to directive shards.\n\tshardComp := newShardsComparer(fromD.ComputeShardsMap(), shardMap)\n\n\t// Remove any shards which are no longer assigned to this worker.\n\t// TODO(tlt): currently, this is just removing the file lock on the\n\t// resource; it's not actually removing the resource from the local\n\t// computer. We should do that.\n\tfor tkey, shards := range shardComp.removed() {\n\t\tqtid := tkey.QualifiedTableID()\n\t\tfor _, shard := range shards {\n\t\t\tpartition := dax.PartitionNum(disco.ShardToShardPartition(string(tkey), uint64(shard), disco.DefaultPartitionN))\n\t\t\tapi.serverlessStorage.RemoveShardResource(qtid, partition, shard)\n\t\t}\n\t}\n\n\t// Loop over the shard map and load from Writelogger.\n\tfor tkey, shards := range shardComp.added() {\n\t\tfor _, shard := range shards {\n\t\t\tjobs <- directiveJobShards{\n\t\t\t\ttkey:  tkey,\n\t\t\t\tshard: shard,\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (api *API) loadShard(ctx context.Context, tkey dax.TableKey, shard dax.ShardNum) error {\n\tqtid := tkey.QualifiedTableID()\n\n\tpartition := dax.PartitionNum(disco.ShardToShardPartition(string(tkey), uint64(shard), disco.DefaultPartitionN))\n\n\tresource := api.serverlessStorage.GetShardResource(qtid, partition, shard)\n\tif resource.IsLocked() {\n\t\tapi.logger().Warnf(\"skipping loadShard (already held) %s %d\", tkey, shard)\n\t\treturn nil\n\t}\n\n\tif rc, err := resource.LoadLatestSnapshot(); err != nil {\n\t\treturn errors.Wrap(err, \"reading latest snapshot for shard\")\n\t} else if rc != nil {\n\t\tdefer rc.Close()\n\t\tif err := api.RestoreShard(ctx, string(tkey), uint64(shard), rc); err != nil {\n\t\t\treturn errors.Wrap(err, \"restoring shard data\")\n\t\t}\n\t}\n\n\t// define write log loading in a func because we do it twice.\n\tloadWriteLog := func() error {\n\t\twritelog, err := resource.LoadWriteLog()\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"\")\n\t\t}\n\t\tif writelog == nil {\n\t\t\treturn nil\n\t\t}\n\n\t\treader := storage.NewShardReader(qtid, partition, shard, writelog)\n\t\tdefer reader.Close()\n\t\tfor logMsg, err := reader.Read(); err != io.EOF; logMsg, err = reader.Read() {\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Wrap(err, \"reading from log reader\")\n\t\t\t}\n\n\t\t\tswitch msg := logMsg.(type) {\n\t\t\tcase *computer.ImportRoaringMessage:\n\t\t\t\treq := &ImportRoaringRequest{\n\t\t\t\t\tClear:           msg.Clear,\n\t\t\t\t\tAction:          msg.Action,\n\t\t\t\t\tBlock:           msg.Block,\n\t\t\t\t\tViews:           msg.Views,\n\t\t\t\t\tUpdateExistence: msg.UpdateExistence,\n\t\t\t\t\tSuppressLog:     true,\n\t\t\t\t}\n\t\t\t\tif err := api.ImportRoaring(ctx, msg.Table, msg.Field, msg.Shard, true, req); err != nil {\n\t\t\t\t\treturn errors.Wrapf(err, \"import roaring, table: %s, field: %s, shard: %d\", msg.Table, msg.Field, msg.Shard)\n\t\t\t\t}\n\n\t\t\tcase *computer.ImportMessage:\n\t\t\t\treq := &ImportRequest{\n\t\t\t\t\tIndex:      msg.Table,\n\t\t\t\t\tField:      msg.Field,\n\t\t\t\t\tShard:      msg.Shard,\n\t\t\t\t\tRowIDs:     msg.RowIDs,\n\t\t\t\t\tColumnIDs:  msg.ColumnIDs,\n\t\t\t\t\tRowKeys:    msg.RowKeys,\n\t\t\t\t\tColumnKeys: msg.ColumnKeys,\n\t\t\t\t\tTimestamps: msg.Timestamps,\n\t\t\t\t\tClear:      msg.Clear,\n\t\t\t\t}\n\n\t\t\t\tqcx := api.Txf().NewQcx()\n\t\t\t\tdefer qcx.Abort()\n\n\t\t\t\topts := []ImportOption{\n\t\t\t\t\tOptImportOptionsClear(msg.Clear),\n\t\t\t\t\tOptImportOptionsIgnoreKeyCheck(msg.IgnoreKeyCheck),\n\t\t\t\t\tOptImportOptionsPresorted(msg.Presorted),\n\t\t\t\t\tOptImportOptionsSuppressLog(true),\n\t\t\t\t}\n\t\t\t\tif err := api.Import(ctx, qcx, req, opts...); err != nil {\n\t\t\t\t\treturn errors.Wrapf(err, \"import, table: %s, field: %s, shard: %d\", msg.Table, msg.Field, msg.Shard)\n\t\t\t\t}\n\n\t\t\tcase *computer.ImportValueMessage:\n\t\t\t\treq := &ImportValueRequest{\n\t\t\t\t\tIndex:           msg.Table,\n\t\t\t\t\tField:           msg.Field,\n\t\t\t\t\tShard:           msg.Shard,\n\t\t\t\t\tColumnIDs:       msg.ColumnIDs,\n\t\t\t\t\tColumnKeys:      msg.ColumnKeys,\n\t\t\t\t\tValues:          msg.Values,\n\t\t\t\t\tFloatValues:     msg.FloatValues,\n\t\t\t\t\tTimestampValues: msg.TimestampValues,\n\t\t\t\t\tStringValues:    msg.StringValues,\n\t\t\t\t\tClear:           msg.Clear,\n\t\t\t\t}\n\n\t\t\t\tqcx := api.Txf().NewQcx()\n\t\t\t\tdefer qcx.Abort()\n\n\t\t\t\topts := []ImportOption{\n\t\t\t\t\tOptImportOptionsClear(msg.Clear),\n\t\t\t\t\tOptImportOptionsIgnoreKeyCheck(msg.IgnoreKeyCheck),\n\t\t\t\t\tOptImportOptionsPresorted(msg.Presorted),\n\t\t\t\t\tOptImportOptionsSuppressLog(true),\n\t\t\t\t}\n\t\t\t\tif err := api.ImportValue(ctx, qcx, req, opts...); err != nil {\n\t\t\t\t\treturn errors.Wrapf(err, \"import value, table: %s, field: %s, shard: %d\", msg.Table, msg.Field, msg.Shard)\n\t\t\t\t}\n\t\t\tcase *computer.ImportRoaringShardMessage:\n\t\t\t\treq := &ImportRoaringShardRequest{\n\t\t\t\t\tRemote:      true,\n\t\t\t\t\tViews:       make([]RoaringUpdate, len(msg.Views)),\n\t\t\t\t\tSuppressLog: true,\n\t\t\t\t}\n\t\t\t\tfor i, view := range msg.Views {\n\t\t\t\t\treq.Views[i] = RoaringUpdate{\n\t\t\t\t\t\tField:        view.Field,\n\t\t\t\t\t\tView:         view.View,\n\t\t\t\t\t\tClear:        view.Clear,\n\t\t\t\t\t\tSet:          view.Set,\n\t\t\t\t\t\tClearRecords: view.ClearRecords,\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif err := api.ImportRoaringShard(ctx, msg.Table, msg.Shard, req); err != nil {\n\t\t\t\t\treturn errors.Wrapf(err, \"import roaring shard table: %s, shard: %d\", msg.Table, msg.Shard)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\t// 1st write log load\n\tif err := loadWriteLog(); err != nil {\n\t\treturn err\n\t}\n\n\t// acquire lock on this partition's keys\n\tif err := resource.Lock(); err != nil {\n\t\treturn errors.Wrap(err, \"locking field key partition\")\n\t}\n\n\t// reload writelog in case of changes between last load and\n\t// lock. The resource object takes care of only loading new data.\n\treturn loadWriteLog()\n}\n\n//////////////////////////////////////////////////////////////\n\n// sliceComparer is used to compare the differences between two slices of comparables.\ntype sliceComparer[K comparable] struct {\n\tfrom []K\n\tto   []K\n}\n\nfunc newSliceComparer[K comparable](from []K, to []K) *sliceComparer[K] {\n\treturn &sliceComparer[K]{\n\t\tfrom: from,\n\t\tto:   to,\n\t}\n}\n\n// added returns the items which are present in `to` but not in `from`.\nfunc (s *sliceComparer[K]) added() []K {\n\treturn thingsAdded(s.from, s.to)\n}\n\n// removed returns the items which are present in `from` but not in `to`.\nfunc (s *sliceComparer[K]) removed() []K {\n\treturn thingsAdded(s.to, s.from)\n}\n\n// same returns the items which are in both `to` and `from`.\nfunc (s *sliceComparer[K]) same() []K {\n\tvar same []K\n\tfor _, fromThing := range s.from {\n\t\tfor _, toThing := range s.to {\n\t\t\tif fromThing == toThing {\n\t\t\t\tsame = append(same, fromThing)\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\treturn same\n}\n\n// thingsAdded returns the comparable things which are present in `to` but not\n// in `from`.\nfunc thingsAdded[K comparable](from []K, to []K) []K {\n\tvar added []K\n\tfor i := range to {\n\t\tvar found bool\n\t\tfor j := range from {\n\t\t\tif from[j] == to[i] {\n\t\t\t\tfound = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !found {\n\t\t\tadded = append(added, to[i])\n\t\t}\n\t}\n\treturn added\n}\n\n// partitionsComparer is used to compare the differences between two maps of\n// table:[]partition.\ntype partitionsComparer struct {\n\tfrom map[dax.TableKey]dax.PartitionNums\n\tto   map[dax.TableKey]dax.PartitionNums\n}\n\nfunc newPartitionsComparer(from map[dax.TableKey]dax.PartitionNums, to map[dax.TableKey]dax.PartitionNums) *partitionsComparer {\n\treturn &partitionsComparer{\n\t\tfrom: from,\n\t\tto:   to,\n\t}\n}\n\n// added returns the partitions which are present in `to` but not in `from`. The\n// results remain in the format of a map of table:[]partition.\nfunc (p *partitionsComparer) added() map[dax.TableKey]dax.PartitionNums {\n\treturn partitionsAdded(p.from, p.to)\n}\n\n// removed returns the partitions which are present in `from` but not in `to`.\n// The results remain in the format of a map of table:[]partition.\nfunc (p *partitionsComparer) removed() map[dax.TableKey]dax.PartitionNums {\n\treturn partitionsAdded(p.to, p.from)\n}\n\n// partitionsAdded returns the partitions which are present in `to` but not in `from`.\nfunc partitionsAdded(from map[dax.TableKey]dax.PartitionNums, to map[dax.TableKey]dax.PartitionNums) map[dax.TableKey]dax.PartitionNums {\n\tif from == nil {\n\t\treturn to\n\t}\n\n\tadded := make(map[dax.TableKey]dax.PartitionNums)\n\tfor tt, tps := range to {\n\t\tfps, found := from[tt]\n\t\tif !found {\n\t\t\tadded[tt] = tps\n\t\t\tcontinue\n\t\t}\n\n\t\taddedPartitions := dax.PartitionNums{}\n\t\tfor i := range tps {\n\t\t\tvar found bool\n\t\t\tfor j := range fps {\n\t\t\t\tif fps[j] == tps[i] {\n\t\t\t\t\tfound = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !found {\n\t\t\t\taddedPartitions = append(addedPartitions, tps[i])\n\t\t\t}\n\t\t}\n\n\t\tif len(addedPartitions) > 0 {\n\t\t\tadded[tt] = addedPartitions\n\t\t}\n\t}\n\treturn added\n}\n\n// fieldsComparer is used to compare the differences between two maps of\n// table:[]fieldVersion.\ntype fieldsComparer struct {\n\tfrom map[dax.TableKey][]dax.FieldName\n\tto   map[dax.TableKey][]dax.FieldName\n}\n\nfunc newFieldsComparer(from map[dax.TableKey][]dax.FieldName, to map[dax.TableKey][]dax.FieldName) *fieldsComparer {\n\treturn &fieldsComparer{\n\t\tfrom: from,\n\t\tto:   to,\n\t}\n}\n\n// added returns the fields which are present in `to` but not in `from`. The\n// results remain in the format of a map of table:[]field.\nfunc (f *fieldsComparer) added() map[dax.TableKey][]dax.FieldName {\n\treturn fieldsAdded(f.from, f.to)\n}\n\n// removed returns the fields which are present in `from` but not in `to`.\n// The results remain in the format of a map of table:[]field.\nfunc (f *fieldsComparer) removed() map[dax.TableKey][]dax.FieldName {\n\treturn fieldsAdded(f.to, f.from)\n}\n\n// fieldsAdded returns the fields which are present in `to` but not in `from`.\nfunc fieldsAdded(from map[dax.TableKey][]dax.FieldName, to map[dax.TableKey][]dax.FieldName) map[dax.TableKey][]dax.FieldName {\n\tif from == nil {\n\t\treturn to\n\t}\n\n\tadded := make(map[dax.TableKey][]dax.FieldName)\n\tfor tt, tps := range to {\n\t\tfps, found := from[tt]\n\t\tif !found {\n\t\t\tadded[tt] = tps\n\t\t\tcontinue\n\t\t}\n\n\t\taddedFieldVersions := []dax.FieldName{}\n\t\tfor i := range tps {\n\t\t\tvar found bool\n\t\t\tfor j := range fps {\n\t\t\t\tif fps[j] == tps[i] {\n\t\t\t\t\tfound = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !found {\n\t\t\t\taddedFieldVersions = append(addedFieldVersions, tps[i])\n\t\t\t}\n\t\t}\n\n\t\tif len(addedFieldVersions) > 0 {\n\t\t\tadded[tt] = addedFieldVersions\n\t\t}\n\t}\n\treturn added\n}\n\n// shardsComparer is used to compare the differences between two maps of\n// table:[]shardV.\ntype shardsComparer struct {\n\tfrom map[dax.TableKey]dax.ShardNums\n\tto   map[dax.TableKey]dax.ShardNums\n}\n\nfunc newShardsComparer(from map[dax.TableKey]dax.ShardNums, to map[dax.TableKey]dax.ShardNums) *shardsComparer {\n\treturn &shardsComparer{\n\t\tfrom: from,\n\t\tto:   to,\n\t}\n}\n\n// added returns the shards which are present in `to` but not in `from`. The\n// results remain in the format of a map of table:[]shard.\nfunc (s *shardsComparer) added() map[dax.TableKey]dax.ShardNums {\n\treturn shardsAdded(s.from, s.to)\n}\n\n// removed returns the shards which are present in `from` but not in `to`. The\n// results remain in the format of a map of table:[]shard.\nfunc (s *shardsComparer) removed() map[dax.TableKey]dax.ShardNums {\n\treturn shardsAdded(s.to, s.from)\n}\n\n// shardsAdded returns the shards which are present in `to` but not in `from`.\nfunc shardsAdded(from map[dax.TableKey]dax.ShardNums, to map[dax.TableKey]dax.ShardNums) map[dax.TableKey]dax.ShardNums {\n\tif from == nil {\n\t\treturn to\n\t}\n\n\tadded := make(map[dax.TableKey]dax.ShardNums)\n\tfor tt, tss := range to {\n\t\tfss, found := from[tt]\n\t\tif !found {\n\t\t\tadded[tt] = tss\n\t\t\tcontinue\n\t\t}\n\n\t\taddedShards := dax.ShardNums{}\n\t\tfor i := range tss {\n\t\t\tvar found bool\n\t\t\tfor j := range fss {\n\t\t\t\tif fss[j] == tss[i] {\n\t\t\t\t\tfound = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !found {\n\t\t\t\taddedShards = append(addedShards, tss[i])\n\t\t\t}\n\t\t}\n\n\t\tif len(addedShards) > 0 {\n\t\t\tadded[tt] = addedShards\n\t\t}\n\t}\n\treturn added\n}\n\n// createTableAndFields creates the FeatureBase Tables and Fields provided in\n// the dax.Directive format.\nfunc (api *API) createTableAndFields(tbl *dax.QualifiedTable, partitions dax.PartitionNums) error {\n\tcim := &CreateIndexMessage{\n\t\tIndex:     string(tbl.Key()),\n\t\tCreatedAt: 0,\n\t\tMeta: IndexOptions{\n\t\t\tKeys:           tbl.StringKeys(),\n\t\t\tTrackExistence: true,\n\t\t},\n\t}\n\n\t// Create the index in etcd as the system of record.\n\tif err := api.holder.persistIndex(context.Background(), cim); err != nil {\n\t\treturn errors.Wrap(err, \"persisting index\")\n\t}\n\n\tidx, err := api.holder.createIndexWithPartitions(cim, partitions)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"adding index: %s\", tbl.Name)\n\t}\n\n\t// Add the fields\n\tfor _, fld := range tbl.Fields {\n\t\tif fld.IsPrimaryKey() {\n\t\t\tcontinue\n\t\t}\n\t\tif err := createField(idx, fld); err != nil {\n\t\t\treturn errors.Wrapf(err, \"creating field: %s\", fld.Name)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// createField creates a FeatureBase Field in the provided FeatureBase Index\n// based on the provided field's type.\nfunc createField(idx *Index, fld *dax.Field) error {\n\topts, err := FieldOptionsFromField(fld)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"creating field options from field: %s\", fld.Name)\n\t}\n\n\tif _, err := idx.createNullableField(string(fld.Name), \"\", opts...); err != nil {\n\t\treturn errors.Wrapf(err, \"creating field on index: %s\", fld.Name)\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "api_directive_internal_test.go",
          "type": "blob",
          "size": 0.451171875,
          "content": "package pilosa\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestThingsAddedGeneric(t *testing.T) {\n\tfrom := []string{\"a\", \"b\", \"c\"}\n\tto := []string{\"b\", \"c\", \"d\"}\n\n\tadded := thingsAdded(from, to)\n\tassert.Equal(t, added, []string{\"d\"})\n}\n\nfunc TestSliceComparer(t *testing.T) {\n\tfrom := []string{\"a\", \"b\", \"c\"}\n\tto := []string{\"b\", \"c\", \"d\"}\n\n\tsc := newSliceComparer(from, to)\n\n\tadded := sc.added()\n\tassert.Equal(t, added, []string{\"d\"})\n}\n"
        },
        {
          "name": "api_directive_test.go",
          "type": "blob",
          "size": 2.4404296875,
          "content": "package pilosa_test\n\nimport (\n\t\"context\"\n\t\"testing\"\n\n\tpilosa \"github.com/featurebasedb/featurebase/v3\"\n\t\"github.com/featurebasedb/featurebase/v3/dax\"\n\tdaxtest \"github.com/featurebasedb/featurebase/v3/dax/test\"\n\t\"github.com/featurebasedb/featurebase/v3/test\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\n// Ensure holder can handle an incoming directive.\nfunc TestAPI_Directive(t *testing.T) {\n\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\n\tapi := c.GetPrimary().API\n\tctx := context.Background()\n\n\tqdbid := dax.NewQualifiedDatabaseID(\"acme\", \"db1\")\n\ttbl1 := daxtest.TestQualifiedTableWithID(t, qdbid, \"1\", \"tbl1\", 12, false)\n\ttbl2 := daxtest.TestQualifiedTableWithID(t, qdbid, \"2\", \"tbl2\", 12, false)\n\ttbl3 := daxtest.TestQualifiedTableWithID(t, qdbid, \"3\", \"tbl3\", 12, false)\n\n\tt.Run(\"Schema\", func(t *testing.T) {\n\n\t\t// Empty directive (and empty holder).\n\t\t{\n\t\t\td := &dax.Directive{\n\t\t\t\tMethod:  dax.DirectiveMethodFull,\n\t\t\t\tVersion: 1,\n\t\t\t}\n\t\t\terr := api.ApplyDirective(ctx, d)\n\t\t\tassert.NoError(t, err)\n\t\t\tassertTablesMatch(t, []string{}, api.Holder().Indexes())\n\t\t}\n\n\t\t// Add a new table.\n\t\t{\n\t\t\td := &dax.Directive{\n\t\t\t\tMethod: dax.DirectiveMethodFull,\n\t\t\t\tTables: []*dax.QualifiedTable{\n\t\t\t\t\ttbl1,\n\t\t\t\t},\n\t\t\t\tVersion: 2,\n\t\t\t}\n\t\t\terr := api.ApplyDirective(ctx, d)\n\t\t\tassert.NoError(t, err)\n\t\t\tassertTablesMatch(t, []string{\"tbl__acme__db1__1\"}, api.Holder().Indexes())\n\t\t}\n\n\t\t// Add a new table, and keep the existing table.\n\t\t{\n\t\t\td := &dax.Directive{\n\t\t\t\tMethod: dax.DirectiveMethodFull,\n\t\t\t\tTables: []*dax.QualifiedTable{\n\t\t\t\t\ttbl1,\n\t\t\t\t\ttbl2,\n\t\t\t\t},\n\t\t\t\tVersion: 3,\n\t\t\t}\n\t\t\terr := api.ApplyDirective(ctx, d)\n\t\t\tassert.NoError(t, err)\n\t\t\tassertTablesMatch(t, []string{\"tbl__acme__db1__1\", \"tbl__acme__db1__2\"}, api.Holder().Indexes())\n\t\t}\n\n\t\t// Add a new table and remove one of the existing tables.\n\t\t{\n\t\t\td := &dax.Directive{\n\t\t\t\tMethod: dax.DirectiveMethodFull,\n\t\t\t\tTables: []*dax.QualifiedTable{\n\t\t\t\t\ttbl2,\n\t\t\t\t\ttbl3,\n\t\t\t\t},\n\t\t\t\tVersion: 4,\n\t\t\t}\n\t\t\terr := api.ApplyDirective(ctx, d)\n\t\t\tassert.NoError(t, err)\n\t\t\tassertTablesMatch(t, []string{\"tbl__acme__db1__2\", \"tbl__acme__db1__3\"}, api.Holder().Indexes())\n\t\t}\n\t})\n}\n\n// assertTablesMatch is a helper function which asserts that the list of index\n// names in `actual` match those provided in `expected`.\nfunc assertTablesMatch(t *testing.T, expected []string, actual []*pilosa.Index) {\n\tt.Helper()\n\n\tact := make([]string, len(actual))\n\tfor i := range actual {\n\t\tact[i] = actual[i].Name()\n\t}\n\tassert.ElementsMatch(t, expected, act)\n}\n"
        },
        {
          "name": "api_test.go",
          "type": "blob",
          "size": 46.7548828125,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa_test\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/hex\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"math/rand\"\n\t\"net/http\"\n\t\"net/http/httptest\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"sort\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\tpilosa \"github.com/featurebasedb/featurebase/v3\"\n\t\"github.com/featurebasedb/featurebase/v3/authn\"\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\t\"github.com/featurebasedb/featurebase/v3/server\"\n\t\"github.com/featurebasedb/featurebase/v3/shardwidth\"\n\t\"github.com/featurebasedb/featurebase/v3/test\"\n\t. \"github.com/featurebasedb/featurebase/v3/vprint\" // nolint:staticcheck\n\t\"github.com/golang-jwt/jwt\"\n\n\t\"golang.org/x/sync/errgroup\"\n)\n\nfunc TestAPI_Import(t *testing.T) {\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\n\tm0 := c.GetNode(0)\n\tm1 := c.GetNode(1)\n\n\tindexNames := map[bool]string{false: c.Idx(\"u\"), true: c.Idx(\"k\")}\n\tfieldNames := map[bool]string{false: \"f\", true: \"kf\"}\n\n\tctx := context.Background()\n\tfor ik, indexName := range indexNames {\n\t\t_, err := m0.API.CreateIndex(ctx, indexName, pilosa.IndexOptions{Keys: ik, TrackExistence: true})\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating index: %v\", err)\n\t\t}\n\t\tfor fk, fieldName := range fieldNames {\n\t\t\tif fk {\n\t\t\t\t_, err = m0.API.CreateField(ctx, indexName, fieldName, pilosa.OptFieldTypeSet(pilosa.DefaultCacheType, 100), pilosa.OptFieldKeys())\n\t\t\t} else {\n\t\t\t\t_, err = m0.API.CreateField(ctx, indexName, fieldName, pilosa.OptFieldTypeSet(pilosa.DefaultCacheType, 100))\n\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"creating field: %v\", err)\n\t\t\t}\n\t\t}\n\t}\n\n\tN := 10\n\n\t// Keys are sharded so ordering is not guaranteed.\n\tcolKeys := make([]string, N)\n\trowKeys := make([]string, N)\n\trowIDs := make([]uint64, N)\n\tcolIDs := make([]uint64, N)\n\tfor i := range colKeys {\n\t\tcolKeys[i] = fmt.Sprintf(\"col%d\", i)\n\t\trowKeys[i] = fmt.Sprintf(\"row%d\", i)\n\t\tcolIDs[i] = (uint64(i) + 1) * 3\n\t\trowIDs[i] = 1\n\t}\n\tsort.Strings(colKeys)\n\tsort.Strings(rowKeys)\n\n\tt.Run(\"RowIDColumnKey\", func(t *testing.T) {\n\t\t// Import data with keys to the primary and verify that it gets\n\t\t// translated and forwarded to the owner of shard 0\n\t\treq := &pilosa.ImportRequest{\n\t\t\tIndex:      indexNames[true],\n\t\t\tField:      fieldNames[false],\n\t\t\tShard:      0, // inaccurate, but keys override it\n\t\t\tRowIDs:     rowIDs,\n\t\t\tColumnKeys: colKeys,\n\t\t}\n\n\t\tqcx := m0.API.Txf().NewQcx()\n\n\t\tif err := m0.API.Import(ctx, qcx, req); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tPanicOn(qcx.Finish())\n\n\t\tpql := fmt.Sprintf(\"Row(%s=%d)\", fieldNames[false], rowIDs[0])\n\n\t\t// Query node0.\n\t\tvar keys []string\n\t\tif res, err := m0.API.Query(ctx, &pilosa.QueryRequest{Index: indexNames[true], Query: pql}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else {\n\t\t\tkeys = res.Results[0].(*pilosa.Row).Keys\n\t\t}\n\t\tsort.Strings(keys)\n\t\tif !reflect.DeepEqual(keys, colKeys) {\n\t\t\tt.Fatalf(\"expected colKeys='%#v'; observed column keys: %#v\", colKeys, keys)\n\t\t}\n\n\t\t// Query node1.\n\t\tif err := test.RetryUntil(5*time.Second, func() error {\n\t\t\tif res, err := m1.API.Query(ctx, &pilosa.QueryRequest{Index: indexNames[true], Query: pql}); err != nil {\n\t\t\t\treturn err\n\t\t\t} else {\n\t\t\t\tkeys = res.Results[0].(*pilosa.Row).Keys\n\n\t\t\t}\n\t\t\tsort.Strings(keys)\n\t\t\tif !reflect.DeepEqual(keys, colKeys) {\n\t\t\t\treturn fmt.Errorf(\"unexpected column keys: %#v\", keys)\n\t\t\t}\n\t\t\treturn nil\n\t\t}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t})\n\tt.Run(\"ExpectedErrors\", func(t *testing.T) {\n\t\tt.Skip(\"partitioning strategy changed, test not supported\") // skipping due to change partitioning strategy\n\t\tctx := context.Background()\n\t\tfor ik, indexName := range indexNames {\n\t\t\tfor fk, fieldName := range fieldNames {\n\t\t\t\treq := pilosa.ImportRequest{\n\t\t\t\t\tIndex: indexName,\n\t\t\t\t\tField: fieldName,\n\t\t\t\t\tShard: 0,\n\t\t\t\t}\n\t\t\t\tfor rik := range indexNames {\n\t\t\t\t\tif rik {\n\t\t\t\t\t\treq.ColumnKeys = colKeys\n\t\t\t\t\t\treq.ColumnIDs = nil\n\t\t\t\t\t} else {\n\t\t\t\t\t\treq.ColumnKeys = nil\n\t\t\t\t\t\treq.ColumnIDs = colIDs\n\t\t\t\t\t}\n\t\t\t\t\tfor rfk := range fieldNames {\n\t\t\t\t\t\tif rfk {\n\t\t\t\t\t\t\treq.RowKeys = rowKeys\n\t\t\t\t\t\t\treq.RowIDs = nil\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\treq.RowKeys = nil\n\t\t\t\t\t\t\treq.RowIDs = rowIDs\n\t\t\t\t\t\t}\n\t\t\t\t\t\terr := func() error {\n\t\t\t\t\t\t\tqcx := m0.API.Txf().NewQcx()\n\t\t\t\t\t\t\tdefer qcx.Abort()\n\t\t\t\t\t\t\terr := m0.API.Import(ctx, qcx, req.Clone())\n\t\t\t\t\t\t\te2 := qcx.Finish()\n\t\t\t\t\t\t\tif e2 != nil {\n\t\t\t\t\t\t\t\tt.Fatalf(\"unexpected error committing: %v\", e2)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\treturn err\n\t\t\t\t\t\t}()\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\tif rfk == fk && rik == ik {\n\t\t\t\t\t\t\t\tt.Errorf(\"unexpected error: schema keys %t/%t, req keys %t/%t: %v\",\n\t\t\t\t\t\t\t\t\tik, fk, rik, rfk, err)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tif rfk != fk || rik != ik {\n\t\t\t\t\t\t\t\tt.Errorf(\"unexpected no error: schema keys %t/%t, req keys %t/%t, req %#v\",\n\t\t\t\t\t\t\t\t\tik, fk, rik, rfk, req)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t})\n\n\t// Relies on the previous test creating an index with TrackExistence and\n\t// adding some data.\n\tt.Run(\"SchemaHasNoExists\", func(t *testing.T) {\n\t\tschema, err := m1.API.Schema(context.Background(), false)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tfor _, f := range schema[0].Fields {\n\t\t\tif f.Name == \"_exists\" {\n\t\t\t\tt.Fatalf(\"found _exists field in schema\")\n\t\t\t}\n\t\t\tif strings.HasPrefix(f.Name, \"_\") {\n\t\t\t\tt.Fatalf(\"found internal field '%s' in schema output\", f.Name)\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc TestAPI_ImportValue(t *testing.T) {\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\n\tcoord := c.GetPrimary()\n\tm0 := c.GetNode(0)\n\tm1 := c.GetNode(1)\n\tm2 := c.GetNode(2)\n\n\tt.Run(\"ValColumnKey\", func(t *testing.T) {\n\t\tctx := context.Background()\n\t\tindex := c.Idx(\"valck\")\n\t\tfield := \"f\"\n\n\t\t_, err := coord.API.CreateIndex(ctx, index, pilosa.IndexOptions{Keys: true})\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating index: %v\", err)\n\t\t}\n\t\t_, err = coord.API.CreateField(ctx, index, field, pilosa.OptFieldTypeInt(math.MinInt64, math.MaxInt64))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating field: %v\", err)\n\t\t}\n\n\t\t// Generate some keyed records.\n\t\tvalues := []int64{}\n\t\tfor i := 1; i <= 10; i++ {\n\t\t\tvalues = append(values, int64(i))\n\t\t}\n\n\t\t// Column keys are sharded so their order is not guaranteed.\n\t\tcolKeys := []string{\"col10\", \"col8\", \"col9\", \"col6\", \"col7\", \"col4\", \"col5\", \"col2\", \"col3\", \"col1\"}\n\n\t\t// Import data with keys to the primary and verify that it gets\n\t\t// translated and forwarded to the owner of shard 0\n\t\treq := &pilosa.ImportValueRequest{\n\t\t\tIndex:      index,\n\t\t\tField:      field,\n\t\t\tColumnKeys: colKeys,\n\t\t\tValues:     values,\n\t\t\tShard:      0, // inaccurate but keys override it\n\t\t}\n\n\t\tqcx := coord.API.Txf().NewQcx()\n\t\tif err := coord.API.ImportValue(ctx, qcx, req); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tPanicOn(qcx.Finish())\n\n\t\tpql := fmt.Sprintf(\"Row(%s>0)\", field)\n\n\t\t// Query node0.\n\t\tres, err := m0.API.Query(ctx, &pilosa.QueryRequest{Index: index, Query: pql})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tkeys := res.Results[0].(*pilosa.Row).Keys\n\t\tif !sameStringSlice(keys, colKeys) {\n\t\t\tt.Fatalf(\"unexpected column keys: %+v\", keys)\n\t\t}\n\n\t\t// Query node1.\n\t\tif err := test.RetryUntil(5*time.Second, func() error {\n\t\t\tres, err := m1.API.Query(ctx, &pilosa.QueryRequest{Index: index, Query: pql})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tkeys := res.Results[0].(*pilosa.Row).Keys\n\t\t\tif !sameStringSlice(keys, colKeys) {\n\t\t\t\tt.Fatalf(\"unexpected column keys: %+v\", keys)\n\t\t\t}\n\t\t\treturn nil\n\t\t}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t})\n\n\tt.Run(\"ValIntEmpty\", func(t *testing.T) {\n\t\tctx := context.Background()\n\t\tindex := c.Idx(\"valintempty\")\n\t\tfield := \"fld\"\n\t\tcreateIndexForTest(index, coord, t)\n\t\tcreateFieldForTest(index, field, coord, t)\n\n\t\t// Column keys are sharded so their order is not guaranteed.\n\t\tcolKeys := []string{\"col2\", \"col1\", \"col3\"}\n\t\tvalues := []int64{1, 2, 3, 4}\n\n\t\t// Import without data, verify that it succeeds\n\t\treq := &pilosa.ImportValueRequest{\n\t\t\tIndex: index,\n\t\t\tField: field,\n\t\t}\n\t\tqcx1 := coord.API.Txf().NewQcx()\n\t\tdefer qcx1.Abort()\n\n\t\t// Import with empty request, should succeed\n\t\tif err := coord.API.ImportValue(ctx, qcx1, req); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tPanicOn(qcx1.Finish())\n\n\t\t// Import without data but with columnkeys, verify that it errors\n\t\treq.ColumnKeys = colKeys\n\t\tqcx2 := coord.API.Txf().NewQcx()\n\t\tdefer qcx2.Abort()\n\t\tif err := coord.API.ImportValue(ctx, qcx2, req); err == nil {\n\t\t\tt.Fatal(\"expected error but succeeded\")\n\t\t}\n\t\tPanicOn(qcx2.Finish())\n\n\t\t// Import with mismatch column and value lengths\n\t\treq.Values = values\n\t\tqcx3 := coord.API.Txf().NewQcx()\n\t\tdefer qcx3.Abort()\n\t\tif err := coord.API.ImportValue(ctx, qcx3, req); err == nil {\n\t\t\tt.Fatal(\"expected error but succeeded\")\n\t\t}\n\t\tPanicOn(qcx3.Finish())\n\n\t\t// Import with data but no columns\n\t\treq.ColumnKeys = make([]string, 0)\n\t\tqcx4 := coord.API.Txf().NewQcx()\n\t\tdefer qcx4.Abort()\n\t\tif err := coord.API.ImportValue(ctx, qcx4, req); err == nil {\n\t\t\tt.Fatal(\"expected error but succeeded\")\n\t\t}\n\t\tPanicOn(qcx4.Finish())\n\n\t})\n\n\tt.Run(\"ValDecimalField\", func(t *testing.T) {\n\t\tt.Skip() // skipping due to change partitioning strategy\n\t\tctx := context.Background()\n\t\tindex := \"valdec\"\n\t\tfield := \"fdec\"\n\t\t_, err := m2.API.CreateIndex(ctx, index, pilosa.IndexOptions{})\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating index: %v\", err)\n\t\t}\n\t\t_, err = m2.API.CreateField(ctx, index, field, pilosa.OptFieldTypeDecimal(1))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating field: %v\", err)\n\t\t}\n\t\t// Generate some records.\n\t\tvalues := []float64{}\n\t\tcolIDs := []uint64{}\n\t\tfor i := 0; i < 10; i++ {\n\t\t\tvalues = append(values, float64(i)+0.1)\n\t\t\tcolIDs = append(colIDs, uint64(i))\n\t\t}\n\t\t// Import data with keys to node1 and verify that it gets translated and\n\t\t// forwarded to the owner of shard 0\n\t\treq := &pilosa.ImportValueRequest{\n\t\t\tIndex:       index,\n\t\t\tField:       field,\n\t\t\tColumnIDs:   colIDs,\n\t\t\tFloatValues: values,\n\t\t}\n\t\tqcx := m0.API.Txf().NewQcx()\n\t\tif err := m0.API.ImportValue(ctx, qcx, req); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tPanicOn(qcx.Finish())\n\t\tquery := fmt.Sprintf(\"Row(%s>6)\", field)\n\t\t// Query node0.\n\t\tif res, err := m0.API.Query(ctx, &pilosa.QueryRequest{Index: index, Query: query}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if ids := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(ids, colIDs[6:]) {\n\t\t\tt.Fatalf(\"unexpected column keys: observerd %+v;  expected '%+v'\", ids, colIDs[6:])\n\t\t}\n\t})\n\n\tt.Run(\"ValDecimalFieldNegativeScale\", func(t *testing.T) {\n\t\tctx := context.Background()\n\t\tindex := c.Idx(\"valdecneg\")\n\t\tfield := \"fdecneg\"\n\n\t\t_, err := m0.API.CreateIndex(ctx, index, pilosa.IndexOptions{})\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating index: %v\", err)\n\t\t}\n\t\t_, err = m0.API.CreateField(ctx, index, field, pilosa.OptFieldTypeDecimal(-1))\n\t\tif err == nil {\n\t\t\tt.Fatal(\"expected error creating field\")\n\t\t}\n\t})\n\n\tt.Run(\"ValTimestampField\", func(t *testing.T) {\n\t\tt.Skip(\"partition strategy change invalidated\") // skipping due to change partitioning strategy\n\t\tctx := context.Background()\n\t\tindex := c.Idx(\"valts\")\n\t\tfield := \"fts\"\n\n\t\t_, err := m1.API.CreateIndex(ctx, index, pilosa.IndexOptions{})\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating index: %v\", err)\n\t\t}\n\t\t_, err = m1.API.CreateField(ctx, index, field, pilosa.OptFieldTypeTimestamp(pilosa.DefaultEpoch, pilosa.TimeUnitSeconds))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating field: %v\", err)\n\t\t}\n\n\t\t// Generate some records.\n\t\tvalues := []time.Time{}\n\t\tcolIDs := []uint64{}\n\t\tfor i := 0; i < 10; i++ {\n\t\t\tvalues = append(values, pilosa.MinTimestamp.Add(time.Duration(i)*time.Second))\n\t\t\tcolIDs = append(colIDs, uint64(i))\n\t\t}\n\n\t\t// Import data with keys to node1 and verify that it gets translated and\n\t\t// forwarded to the owner of shard 0\n\t\treq := &pilosa.ImportValueRequest{\n\t\t\tIndex:           index,\n\t\t\tField:           field,\n\t\t\tColumnIDs:       colIDs,\n\t\t\tTimestampValues: values,\n\t\t}\n\n\t\tqcx := m2.API.Txf().NewQcx()\n\t\tif err := m2.API.ImportValue(ctx, qcx, req); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tPanicOn(qcx.Finish())\n\n\t\tquery := fmt.Sprintf(\"Row(%s>='1833-11-24T17:31:50Z')\", field) // 6s after MinTimestamp\n\n\t\t// Query node0.\n\t\tif res, err := m0.API.Query(ctx, &pilosa.QueryRequest{Index: index, Query: query}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if ids := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(ids, colIDs[6:]) {\n\t\t\tt.Fatalf(\"unexpected column keys: observerd %+v;  expected '%+v'\", ids, colIDs[6:])\n\t\t}\n\t})\n\n\tt.Run(\"ValStringField\", func(t *testing.T) {\n\t\tt.Skip(\"partition strategy change invalidated\") // skipping due to change partitioning strategy\n\t\tctx := context.Background()\n\t\tindex := c.Idx(\"valstr\")\n\t\tfield := \"fstr\"\n\n\t\tfgnIndex := \"fgnvalstr\"\n\n\t\t_, err := coord.API.CreateIndex(ctx, index, pilosa.IndexOptions{})\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating index: %v\", err)\n\t\t}\n\n\t\t_, err = coord.API.CreateIndex(ctx, fgnIndex, pilosa.IndexOptions{Keys: true})\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating foreign index: %v\", err)\n\t\t}\n\t\t_, err = coord.API.CreateField(ctx, index, field,\n\t\t\tpilosa.OptFieldTypeInt(0, math.MaxInt64),\n\t\t\tpilosa.OptFieldForeignIndex(fgnIndex),\n\t\t)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating field: %v\", err)\n\t\t}\n\n\t\t// Generate some keyed records.\n\t\tvalues := []string{}\n\t\tcolIDs := []uint64{}\n\t\tfor i := 0; i < 10; i++ {\n\t\t\tvalue := fmt.Sprintf(\"strval-%d\", (i)*100+10)\n\t\t\tvalues = append(values, value)\n\t\t\tcolIDs = append(colIDs, uint64(i))\n\t\t}\n\n\t\t// Import data with keys to the node0 and verify that it gets translated\n\t\t// and forwarded to the owner of shard 0\n\t\treq := &pilosa.ImportValueRequest{\n\t\t\tIndex:        index,\n\t\t\tField:        field,\n\t\t\tColumnIDs:    colIDs,\n\t\t\tStringValues: values,\n\t\t}\n\t\tqcx := m0.API.Txf().NewQcx()\n\t\tif err := m0.API.ImportValue(ctx, qcx, req); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tPanicOn(qcx.Finish())\n\n\t\tpql := fmt.Sprintf(`Row(%s==\"strval-110\")`, field)\n\n\t\t// Query node1.\n\t\tif res, err := m1.API.Query(ctx, &pilosa.QueryRequest{Index: index, Query: pql}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if ids := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(ids, []uint64{1}) {\n\t\t\tt.Fatalf(\"unexpected columns: observerd %+v;  expected '%+v'\", ids, []uint64{1})\n\t\t}\n\t})\n}\n\nfunc TestAPI_Ingest(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\tcoord := c.GetPrimary()\n\tindex := c.Idx()\n\tsetField := \"set\"\n\ttimeField := \"tq\"\n\tintField := \"int\"\n\n\t_, err := coord.API.CreateIndex(ctx, index, pilosa.IndexOptions{Keys: false, TrackExistence: true})\n\tif err != nil {\n\t\tt.Fatalf(\"creating index: %v\", err)\n\t}\n\t_, err = coord.API.CreateField(ctx, index, setField, pilosa.OptFieldTypeSet(\"none\", 0))\n\tif err != nil {\n\t\tt.Fatalf(\"creating field: %v\", err)\n\t}\n\t_, err = coord.API.CreateField(ctx, index, timeField, pilosa.OptFieldTypeTime(\"YMD\", \"0\"))\n\tif err != nil {\n\t\tt.Fatalf(\"creating field: %v\", err)\n\t}\n\t_, err = coord.API.CreateField(ctx, index, intField, pilosa.OptFieldTypeInt(0, 100000))\n\tif err != nil {\n\t\tt.Fatalf(\"creating field: %v\", err)\n\t}\n\n\tt.Run(\"ImportRoaringShard\", func(t *testing.T) {\n\t\tsetBuf := &bytes.Buffer{}\n\t\tsetBits := roaring.NewBitmap(7, pilosa.ShardWidth+7)\n\t\t_, _ = setBits.WriteTo(setBuf) // bytes.Buffer never errors\n\t\tintBuf := &bytes.Buffer{}\n\t\tintBits := roaring.NewBitmap(7, pilosa.ShardWidth*2+7)\n\t\t_, _ = intBits.WriteTo(intBuf) // bytes.Buffer never errors\n\t\trequest := &pilosa.ImportRoaringShardRequest{\n\t\t\tRemote: true,\n\t\t\tViews: []pilosa.RoaringUpdate{\n\t\t\t\t{\n\t\t\t\t\tField: setField,\n\t\t\t\t\tView:  \"standard\",\n\t\t\t\t\tSet:   setBuf.Bytes(),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tField: intField,\n\t\t\t\t\tView:  \"bsig_\" + intField,\n\t\t\t\t\tSet:   intBuf.Bytes(),\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t\tif err := coord.API.ImportRoaringShard(context.Background(), c.Idx(), 8, request); err != nil {\n\t\t\tt.Fatalf(\"ingesting: %v\", err)\n\t\t}\n\n\t\tmustQuery := func(t *testing.T, index, query string) pilosa.QueryResponse {\n\t\t\tres, err := coord.API.Query(context.Background(), &pilosa.QueryRequest{Index: index, Query: query})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"querying: %v\", err)\n\t\t\t}\n\t\t\treturn res\n\t\t}\n\n\t\tres := mustQuery(t, c.Idx(), \"Row(set=0)\")\n\t\tr := res.Results[0].(*pilosa.Row).Columns()\n\t\tif len(r) != 1 || r[0] != pilosa.ShardWidth*8+7 {\n\t\t\tt.Fatalf(\"expected row with pilosa.ShardWidth*8+7 set, got %d\", r)\n\t\t}\n\n\t\tres = mustQuery(t, c.Idx(), \"Row(set=1)\")\n\t\tr = res.Results[0].(*pilosa.Row).Columns()\n\t\tif len(r) != 1 || r[0] != pilosa.ShardWidth*8+7 {\n\t\t\tt.Fatalf(\"expected row with pilosa.ShardWidth*8+7 set, got %d\", r)\n\t\t}\n\n\t\tres = mustQuery(t, c.Idx(), \"Row(int==1)\")\n\t\tr = res.Results[0].(*pilosa.Row).Columns()\n\t\tif len(r) != 1 || r[0] != pilosa.ShardWidth*8+7 {\n\t\t\tt.Fatalf(\"expected row with, pilosa.ShardWidth*8+7 set, got %d\", r)\n\t\t}\n\n\t\trequest = &pilosa.ImportRoaringShardRequest{\n\t\t\tRemote: true,\n\t\t\tViews: []pilosa.RoaringUpdate{\n\t\t\t\t{\n\t\t\t\t\tField: setField,\n\t\t\t\t\tView:  \"standard\",\n\t\t\t\t\tClear: setBuf.Bytes(),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tField: intField,\n\t\t\t\t\tView:  \"bsig_\" + intField,\n\t\t\t\t\tClear: intBuf.Bytes(),\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t\tif err := coord.API.ImportRoaringShard(context.Background(), c.Idx(), 8, request); err != nil {\n\t\t\tt.Fatalf(\"ingesting: %v\", err)\n\t\t}\n\n\t\tres = mustQuery(t, c.Idx(), \"Row(set=0)\")\n\t\tr = res.Results[0].(*pilosa.Row).Columns()\n\t\tif len(r) != 0 {\n\t\t\tt.Fatalf(\"expected no values after clearing, got: %v\", r)\n\t\t}\n\n\t\tres = mustQuery(t, c.Idx(), \"Row(set=1)\")\n\t\tr = res.Results[0].(*pilosa.Row).Columns()\n\t\tif len(r) != 0 {\n\t\t\tt.Fatalf(\"expected no values after clearing, got: %v\", r)\n\t\t}\n\n\t\tres = mustQuery(t, c.Idx(), \"Row(int==1)\")\n\t\tr = res.Results[0].(*pilosa.Row).Columns()\n\t\tif len(r) != 0 {\n\t\t\tt.Fatalf(\"expected no values after clearing, got: %v\", r)\n\t\t}\n\n\t})\n}\n\nfunc TestAPI_ClearFlagForImportAndImportValues(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\n\t// plan:\n\t//  1. set a bit\n\t//  2. clear with Import() using the ImportRequest.Clear flag\n\t//  3. verifiy the clear is done.\n\t//  repeat for ImportValueRequest and ImportValues()\n\n\tm0 := c.GetNode(0)\n\tm0api := m0.API\n\n\tctx := context.Background()\n\tindex := c.Idx()\n\tfieldAcct0 := \"acct0\"\n\n\topts := pilosa.OptFieldTypeInt(-1000, 1000)\n\n\t_, err := m0api.CreateIndex(ctx, index, pilosa.IndexOptions{})\n\tif err != nil {\n\t\tt.Fatalf(\"creating index: %v\", err)\n\t}\n\t_, err = m0api.CreateField(ctx, index, fieldAcct0, opts)\n\tif err != nil {\n\t\tt.Fatalf(\"creating fieldAcct0: %v\", err)\n\t}\n\n\tiraField := \"ira\" // set field.\n\tiraRowID := uint64(3)\n\t_, err = m0api.CreateField(ctx, index, iraField)\n\tif err != nil {\n\t\tt.Fatalf(\"creating fieldIRA: %v\", err)\n\t}\n\n\tacctOwnerID := uint64(78) // ColumnID\n\tshard := acctOwnerID / ShardWidth\n\tacct0bal := int64(500)\n\n\tivr0 := &pilosa.ImportValueRequest{\n\t\tIndex:     index,\n\t\tField:     fieldAcct0,\n\t\tShard:     shard,\n\t\tColumnIDs: []uint64{acctOwnerID},\n\t\tValues:    []int64{acct0bal},\n\t}\n\tir0 := &pilosa.ImportRequest{\n\t\tIndex:     index,\n\t\tField:     iraField,\n\t\tShard:     shard,\n\t\tColumnIDs: []uint64{acctOwnerID},\n\t\tRowIDs:    []uint64{iraRowID},\n\t}\n\n\tqcx := m0api.Txf().NewQcx()\n\tif err := m0api.Import(ctx, qcx, ir0.Clone()); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif err := m0api.ImportValue(ctx, qcx, ivr0.Clone()); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tPanicOn(qcx.Finish())\n\n\tbitIsSet := func() bool {\n\t\tquery := fmt.Sprintf(\"Row(%v=%v)\", iraField, iraRowID)\n\t\tres, err := m0api.Query(context.Background(), &pilosa.QueryRequest{Index: index, Query: query})\n\t\tPanicOn(err)\n\t\tcols := res.Results[0].(*pilosa.Row).Columns()\n\t\tfor i := range cols {\n\t\t\tif cols[i] == acctOwnerID {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t\treturn false\n\t}\n\n\tif !bitIsSet() {\n\t\tPanicOn(\"IRA bit should have been set\")\n\t}\n\n\tqueryAcct := func(m0api *pilosa.API, acctOwnerID uint64, fieldAcct0, index string) (acctBal int64) {\n\t\tquery := fmt.Sprintf(\"FieldValue(field=%v, column=%v)\", fieldAcct0, acctOwnerID)\n\t\tres, err := m0api.Query(context.Background(), &pilosa.QueryRequest{Index: index, Query: query})\n\t\tPanicOn(err)\n\n\t\tif len(res.Results) == 0 {\n\t\t\treturn 0\n\t\t}\n\t\tvalCount := res.Results[0].(pilosa.ValCount)\n\t\treturn valCount.Val\n\t}\n\n\tbal := queryAcct(m0api, acctOwnerID, fieldAcct0, index)\n\n\tif bal != acct0bal {\n\t\tPanicOn(fmt.Sprintf(\"expected %v, observed %v starting acct0 balance\", acct0bal, bal))\n\t}\n\n\t// clear the bit\n\tqcx = m0api.Txf().NewQcx()\n\tir0.Clear = true\n\tif err := m0api.Import(ctx, qcx, ir0); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tPanicOn(qcx.Finish())\n\n\tif bitIsSet() {\n\t\tPanicOn(\"IRA bit should have been cleared\")\n\t}\n\n\t// clear the BSI\n\tqcx = m0api.Txf().NewQcx()\n\tivr0.Clear = true\n\tif err := m0api.ImportValue(ctx, qcx, ivr0); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tPanicOn(qcx.Finish())\n\n\tbal = queryAcct(m0api, acctOwnerID, fieldAcct0, index)\n\tif bal != 0 {\n\t\tPanicOn(fmt.Sprintf(\"expected %v, observed %v starting acct0 balance\", acct0bal, 0))\n\t}\n}\n\nfunc TestAPI_IDAlloc(t *testing.T) {\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\n\tprimary := c.GetPrimary().API\n\n\tt.Run(\"Normal\", func(t *testing.T) {\n\t\tkey := pilosa.IDAllocKey{\n\t\t\tIndex: \"normal\",\n\t\t\tKey:   \"key\",\n\t\t}\n\t\tvar session [32]byte\n\t\t_, err := rand.Read(session[:])\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"obtaining random bytes: %v\", err)\n\t\t}\n\n\t\tconst toReserve = 2\n\n\t\tids, err := primary.ReserveIDs(key, session, ^uint64(0), toReserve)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"reserving IDs: %v\", err)\n\t\t}\n\n\t\tvar numIds uint64\n\t\tfor _, idr := range ids {\n\t\t\tnumIds += (idr.Last - idr.First) + 1\n\t\t}\n\t\tif numIds != toReserve {\n\t\t\tt.Errorf(\"expected %d ids but got %d: %v\", toReserve, numIds, ids)\n\t\t}\n\n\t\terr = primary.CommitIDs(key, session, numIds)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"committing IDs: %v\", err)\n\t\t}\n\n\t\terr = primary.ResetIDAlloc(key.Index)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"resetting ID alloc: %v\", err)\n\t\t}\n\t})\n\tt.Run(\"Offset\", func(t *testing.T) {\n\t\tkey := pilosa.IDAllocKey{\n\t\t\tIndex: \"offset\",\n\t\t\tKey:   \"key\",\n\t\t}\n\t\tvar session [32]byte\n\t\t_, err := rand.Read(session[:])\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"obtaining random bytes: %v\", err)\n\t\t}\n\n\t\tids, err := primary.ReserveIDs(key, session, 0, 2)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"reserving IDs: %v\", err)\n\t\t}\n\n\t\t{\n\t\t\tvar numIds uint64\n\t\t\tfor _, idr := range ids {\n\t\t\t\tnumIds += (idr.Last - idr.First) + 1\n\t\t\t}\n\t\t\tif numIds != 2 {\n\t\t\t\tt.Errorf(\"expected %d ids but got %d: %v\", 2, numIds, ids)\n\t\t\t}\n\t\t}\n\n\t\t_, err = rand.Read(session[:])\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"obtaining random bytes: %v\", err)\n\t\t}\n\t\tids2, err := primary.ReserveIDs(key, session, 1, 2)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"reserving IDs with partially increased offset: %v\", err)\n\t\t}\n\n\t\tvar numIds uint64\n\t\tfor _, idr := range ids2 {\n\t\t\tnumIds += (idr.Last - idr.First) + 1\n\t\t}\n\t\tif numIds != 2 {\n\t\t\tt.Errorf(\"expected %d ids but got %d: %v\", 2, numIds, ids2)\n\t\t}\n\n\t\tif prevEnd, newStart := ids[len(ids)-1].Last, ids2[0].First; prevEnd != newStart {\n\t\t\tt.Errorf(\"expected reuse of last ID (%d), but started with %d\", prevEnd, newStart)\n\t\t}\n\n\t\terr = primary.CommitIDs(key, session, numIds)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"committing IDs: %v\", err)\n\t\t}\n\n\t\t_, err = rand.Read(session[:])\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"obtaining random bytes: %v\", err)\n\t\t}\n\t\tids3, err := primary.ReserveIDs(key, session, 0, 2)\n\t\tvar esync pilosa.IDOffsetDesyncError\n\t\tif errors.As(err, &esync) {\n\t\t\tif esync.Requested != 0 {\n\t\t\t\tt.Errorf(\"incorrect requested offset in error: provided %d but got %d\", 0, esync.Requested)\n\t\t\t}\n\t\t\tif esync.Base != 3 {\n\t\t\t\tt.Errorf(\"incorrect base offset: expected %d but got %d\", 3, esync.Base)\n\t\t\t}\n\t\t} else if err == nil {\n\t\t\tt.Errorf(\"successfully re-reserved at a committed offset: %v\", ids3)\n\t\t} else {\n\t\t\tt.Fatalf(\"unexpected error when reserving committed IDs: %v\", err)\n\t\t}\n\n\t\terr = primary.ResetIDAlloc(key.Index)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"resetting ID alloc: %v\", err)\n\t\t}\n\t})\n}\n\ntype mutexCheckIndex struct {\n\tindex     *pilosa.Index\n\tindexName string\n\tcreatedAt int64\n\tfields    map[bool]mutexCheckField\n}\n\ntype mutexCheckField struct {\n\tfieldName string\n\tfield     *pilosa.Field\n\tcreatedAt int64\n}\n\nfunc TestAPI_MutexCheck(t *testing.T) {\n\t// Can't share this one, because it has to get custom option settings and needs\n\t// replication.\n\tc := test.MustUnsharedCluster(t, 3)\n\tfor _, c := range c.Nodes {\n\t\tc.Config.Cluster.ReplicaN = 2\n\t}\n\tif err := c.Start(); err != nil {\n\t\tt.Fatalf(\"starting cluster: %v\", err)\n\t}\n\tdefer c.Close()\n\n\tm0 := c.GetNode(0)\n\tnodesByID := make(map[string]*test.Command, 3)\n\tqcxsByID := make(map[string]*pilosa.Qcx, 3)\n\tfor i := 0; i < 3; i++ {\n\t\tnode := c.GetNode(i)\n\t\tid := node.API.NodeID()\n\t\tnodesByID[id] = node\n\t}\n\n\tindexes := make(map[bool]mutexCheckIndex)\n\n\tctx := context.Background()\n\tfor _, keyedIndex := range []bool{false, true} {\n\t\tindexName := c.Idx(map[bool]string{false: \"u\", true: \"k\"}[keyedIndex])\n\t\tindex, err := m0.API.CreateIndex(ctx, indexName, pilosa.IndexOptions{Keys: keyedIndex, TrackExistence: true})\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating index: %v\", err)\n\t\t}\n\t\tif index.CreatedAt() == 0 {\n\t\t\tt.Fatal(\"index createdAt is empty\")\n\t\t}\n\t\tindexData := mutexCheckIndex{indexName: indexName, index: index, fields: make(map[bool]mutexCheckField), createdAt: index.CreatedAt()}\n\t\tfor _, keyedField := range []bool{false, true} {\n\t\t\tfieldName := fmt.Sprintf(\"f%t\", keyedField)\n\t\t\tvar field *pilosa.Field\n\t\t\tif keyedField {\n\t\t\t\tfield, err = m0.API.CreateField(ctx, indexName, fieldName, pilosa.OptFieldTypeMutex(pilosa.CacheTypeNone, 0), pilosa.OptFieldKeys())\n\t\t\t} else {\n\t\t\t\tfield, err = m0.API.CreateField(ctx, indexName, fieldName, pilosa.OptFieldTypeMutex(pilosa.CacheTypeNone, 0))\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"creating field: %v\", err)\n\t\t\t}\n\t\t\tif field.CreatedAt() == 0 {\n\t\t\t\tt.Fatal(\"field createdAt is empty\")\n\t\t\t}\n\t\t\tindexData.fields[keyedField] = mutexCheckField{fieldName: fieldName, field: field, createdAt: field.CreatedAt()}\n\t\t}\n\t\tindexes[keyedIndex] = indexData\n\t}\n\n\trowIDs := []uint64{0, 1, 2, 3}\n\tcolIDs := []uint64{0, 1, 2, 3}\n\trowKeysBase := []string{\"v0\", \"v1\", \"v2\", \"v3\"}\n\tcolKeysBase := []string{\"c0\", \"c1\", \"c2\", \"c3\"}\n\n\tconst nShards = 9\n\n\t// now, try the same thing for each combination of keyed/unkeyed. we\n\t// share code between keyed/unkeyed fields, but for indexes, the logic\n\t// is fundamentally different because we can't know shards in advance.\n\tindexData := indexes[false]\n\tfor keyedField, fieldData := range indexData.fields {\n\t\tt.Run(fmt.Sprintf(\"%s-%s\", indexData.indexName, fieldData.fieldName), func(t *testing.T) {\n\t\t\tfor id, node := range nodesByID {\n\t\t\t\tqcxsByID[id] = node.API.Txf().NewQcx()\n\t\t\t}\n\t\t\tfor shard := uint64(0); shard < nShards; shard++ {\n\t\t\t\t// restore row/col ID values which can get altered by imports\n\t\t\t\tfor i := range rowIDs {\n\t\t\t\t\trowIDs[i] = uint64(i)\n\t\t\t\t\tcolIDs[i] = (shard << shardwidth.Exponent) + uint64(i) + (shard % 4)\n\t\t\t\t}\n\t\t\t\treq := &pilosa.ImportRequest{\n\t\t\t\t\tIndex:          indexData.indexName,\n\t\t\t\t\tIndexCreatedAt: indexData.createdAt,\n\t\t\t\t\tField:          fieldData.fieldName,\n\t\t\t\t\tFieldCreatedAt: fieldData.createdAt,\n\t\t\t\t\tShard:          shard,\n\t\t\t\t\tColumnIDs:      colIDs,\n\t\t\t\t}\n\t\t\t\tif keyedField {\n\t\t\t\t\treq.RowKeys = rowKeysBase\n\t\t\t\t} else {\n\t\t\t\t\treq.RowIDs = rowIDs\n\t\t\t\t}\n\t\t\t\tnodesForShard, err := m0.API.ShardNodes(ctx, indexData.indexName, shard)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"obtaining shard list: %v\", err)\n\t\t\t\t}\n\t\t\t\tif len(nodesForShard) < 1 {\n\t\t\t\t\tt.Fatalf(\"no nodes for shard %d\", shard)\n\t\t\t\t}\n\t\t\t\tnode := nodesByID[nodesForShard[0].ID]\n\t\t\t\tif err := node.API.Import(ctx, qcxsByID[nodesForShard[0].ID], req); err != nil {\n\t\t\t\t\tt.Fatalf(\"importing data: %v\", err)\n\t\t\t\t}\n\t\t\t}\n\t\t\t// and then we break the mutex and close the Qcxs\n\t\t\tfor id, node := range nodesByID {\n\t\t\t\tfield, err := node.API.Field(ctx, indexData.indexName, fieldData.fieldName)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"requesting field %s from node %s: %v\", fieldData.fieldName, id, err)\n\t\t\t\t}\n\t\t\t\tpilosa.CorruptAMutex(t, field, qcxsByID[id])\n\t\t\t\terr = qcxsByID[id].Finish()\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"closing out transaction on node %s: %v\", id, err)\n\t\t\t\t}\n\t\t\t}\n\t\t\tqcx := m0.API.Txf().NewQcx()\n\t\t\tdefer qcx.Abort()\n\n\t\t\t// first two shards of each group of 4 should have a collision in\n\t\t\t// position 1\n\t\t\texpected := map[uint64]bool{\n\t\t\t\t(0 << shardwidth.Exponent) + 1: true,\n\t\t\t\t(1 << shardwidth.Exponent) + 1: true,\n\t\t\t\t(4 << shardwidth.Exponent) + 1: true,\n\t\t\t\t(5 << shardwidth.Exponent) + 1: true,\n\t\t\t\t(8 << shardwidth.Exponent) + 1: true,\n\t\t\t\t// So, nShards used to be 10. If you run a complete test,\n\t\t\t\t// with go test -race, and you have the sample input for the\n\t\t\t\t// unrelated TestImportMutexSampleData configured to use 64K\n\t\t\t\t// bit density and 2K rows, everything is fine. If you run a\n\t\t\t\t// partial test, everything is fine. If you run a complete\n\t\t\t\t// test with -race, but you skip TestImportMutexSampleData,\n\t\t\t\t// or reduce either the bit density or the row count, you get\n\t\t\t\t// a very strange panic where the go panic handler panics\n\t\t\t\t// trying to report what happened so we don't get a valid\n\t\t\t\t// stack dump. On Macs. This is as much as I could debug it\n\t\t\t\t// after about 6 hours. Since there's no special reason to\n\t\t\t\t// think we need all 10 shards, and 9 still tests the\n\t\t\t\t// behavior, we're leaving this one a mystery.\n\t\t\t\t// (9 << shardwidth.Exponent) + 1: true,\n\t\t\t}\n\n\t\t\tresults, err := m0.API.MutexCheck(ctx, qcx, indexData.indexName, fieldData.fieldName, true, 0)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"checking mutexes: %v\", err)\n\t\t\t}\n\n\t\t\tif keyedField {\n\t\t\t\tmapped, ok := results.(map[uint64][]string)\n\t\t\t\tif !ok {\n\t\t\t\t\tt.Fatalf(\"expected map[uint64][]string, got %T\", results)\n\t\t\t\t}\n\t\t\t\tseen := 0\n\t\t\t\tfor k, v := range mapped {\n\t\t\t\t\tseen++\n\t\t\t\t\tif !expected[k] {\n\t\t\t\t\t\tt.Fatalf(\"expected all collisions to be 1 shards (s %% 4 in [0,1]), got %d\", k)\n\t\t\t\t\t}\n\t\t\t\t\tif len(v) != 2 {\n\t\t\t\t\t\tt.Fatalf(\"expected exactly two collisions\")\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif seen != len(expected) {\n\t\t\t\t\tt.Fatalf(\"expected exactly %d records to have collisions\", len(expected))\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tmapped, ok := results.(map[uint64][]uint64)\n\t\t\t\tif !ok {\n\t\t\t\t\tt.Fatalf(\"expected map[uint64][]uint64, got %T\", results)\n\t\t\t\t}\n\t\t\t\tseen := 0\n\t\t\t\tfor k, v := range mapped {\n\t\t\t\t\tseen++\n\t\t\t\t\tif !expected[k] {\n\t\t\t\t\t\tt.Fatalf(\"expected all collisions to be 1 shards (s %% 4 in [0,1]), got %d\", k)\n\t\t\t\t\t}\n\t\t\t\t\tif len(v) != 2 {\n\t\t\t\t\t\tt.Fatalf(\"expected exactly two collisions\")\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif seen != len(expected) {\n\t\t\t\t\tt.Fatalf(\"expected exactly %d records to have collisions, got %d\", len(expected), seen)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// and let's try with no details and a limit of 3...\n\t\t\tresults, err = m0.API.MutexCheck(ctx, qcx, indexData.indexName, fieldData.fieldName, false, 3)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"checking mutexes: %v\", err)\n\t\t\t}\n\t\t\tmapped, ok := results.([]uint64)\n\t\t\tif !ok {\n\t\t\t\tt.Fatalf(\"expected []uint64, got %T\", results)\n\t\t\t}\n\t\t\tseen := 0\n\t\t\tfor _, k := range mapped {\n\t\t\t\tseen++\n\t\t\t\tif !expected[k] {\n\t\t\t\t\tt.Fatalf(\"expected all collisions to be position 1 in shards (s %% 4 in [0,1]), got %d\", k)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif seen != 3 {\n\t\t\t\tt.Fatalf(\"expected results limited to 3, got %d\", seen)\n\t\t\t}\n\t\t})\n\t}\n\tindexData = indexes[true]\n\tfor keyedField, fieldData := range indexData.fields {\n\t\tt.Run(fmt.Sprintf(\"%s-%s\", indexData.indexName, fieldData.fieldName), func(t *testing.T) {\n\t\t\tfor id, node := range nodesByID {\n\t\t\t\tqcxsByID[id] = node.API.Txf().NewQcx()\n\t\t\t}\n\t\t\treq := &pilosa.ImportRequest{\n\t\t\t\tIndex:          indexData.indexName,\n\t\t\t\tIndexCreatedAt: indexData.createdAt,\n\t\t\t\tField:          fieldData.fieldName,\n\t\t\t\tFieldCreatedAt: fieldData.createdAt,\n\t\t\t\tShard:          0, // ignored when using keys\n\t\t\t}\n\t\t\trowKeys := make([]string, 0, len(rowKeysBase)*nShards)\n\t\t\tcolKeys := make([]string, 0, len(rowKeysBase)*nShards)\n\t\t\trowIDs = rowIDs[:0]\n\t\t\tfor shard := uint64(0); shard < nShards; shard++ {\n\t\t\t\tfor i := range rowKeysBase {\n\t\t\t\t\tcolKeys = append(colKeys, fmt.Sprintf(\"s%d-%s\", shard, colKeysBase[i]))\n\t\t\t\t\tif keyedField {\n\t\t\t\t\t\trowKeys = append(rowKeys, rowKeysBase[i])\n\t\t\t\t\t} else {\n\t\t\t\t\t\trowIDs = append(rowIDs, uint64(i))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\treq.ColumnKeys = colKeys\n\t\t\tif keyedField {\n\t\t\t\treq.RowKeys = rowKeys\n\t\t\t} else {\n\t\t\t\treq.RowIDs = rowIDs\n\t\t\t}\n\t\t\tvar id string\n\t\t\tvar node *test.Command\n\t\t\tfor id, node = range nodesByID {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif err := node.API.Import(ctx, qcxsByID[id], req); err != nil {\n\t\t\t\tt.Fatalf(\"importing data: %v\", err)\n\t\t\t}\n\t\t\texpected, err := node.API.FindIndexKeys(ctx, indexData.indexName, colKeys...)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"looking up index keys: %v\", err)\n\t\t\t}\n\t\t\tfor key, id := range expected {\n\t\t\t\t// CorruptAMutex should only corrupt things in position 1 of their\n\t\t\t\t// shards...\n\t\t\t\tif id%(1<<shardwidth.Exponent) != 1 {\n\t\t\t\t\tdelete(expected, key)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif keyedField {\n\t\t\t\tfieldValues, err := node.API.FindFieldKeys(ctx, indexData.indexName, fieldData.fieldName, rowKeys...)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"looking up field keys: %v\", err)\n\t\t\t\t}\n\t\t\t\t// Figure out which key got the value 3, delete any records\n\t\t\t\t// which would have had that key, because they won't be\n\t\t\t\t// conflicts.\n\t\t\t\tfor key, value := range fieldValues {\n\t\t\t\t\tif value == 3 {\n\t\t\t\t\t\tfor offset, baseKey := range rowKeysBase {\n\t\t\t\t\t\t\tif baseKey == key {\n\t\t\t\t\t\t\t\tfor i := offset; i < len(rowKeys); i += len(rowKeysBase) {\n\t\t\t\t\t\t\t\t\tdelete(expected, colKeys[i])\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// we set rowKeys to 0-1-2-... for rowKeysBase items, which\n\t\t\t\t// tells us which keys we expect to be 3 already.\n\t\t\t\tfor i := 3; i < len(rowIDs); i += len(rowKeysBase) {\n\t\t\t\t\tdelete(expected, colKeys[i])\n\t\t\t\t}\n\t\t\t}\n\t\t\t// and then we break the mutex and close the Qcxs\n\t\t\tfor id, node := range nodesByID {\n\t\t\t\tfield, err := node.API.Field(ctx, indexData.indexName, fieldData.fieldName)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"requesting field %s from node %s: %v\", fieldData.fieldName, id, err)\n\t\t\t\t}\n\t\t\t\tpilosa.CorruptAMutex(t, field, qcxsByID[id])\n\t\t\t\terr = qcxsByID[id].Finish()\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"closing out transaction on node %s: %v\", id, err)\n\t\t\t\t}\n\t\t\t}\n\t\t\tqcx := m0.API.Txf().NewQcx()\n\t\t\tdefer qcx.Abort()\n\n\t\t\tresults, err := m0.API.MutexCheck(ctx, qcx, indexData.indexName, fieldData.fieldName, true, 0)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"checking mutexes: %v\", err)\n\t\t\t}\n\t\t\tif keyedField {\n\t\t\t\tmapped, ok := results.(map[string][]string)\n\t\t\t\tif !ok {\n\t\t\t\t\tt.Fatalf(\"expected map[string][]string, got %T\", results)\n\t\t\t\t}\n\t\t\t\tseen := 0\n\t\t\t\tfor k, v := range mapped {\n\t\t\t\t\tseen++\n\t\t\t\t\tif _, ok := expected[k]; !ok {\n\t\t\t\t\t\tt.Fatalf(\"unexpected collision on key %q\", k)\n\t\t\t\t\t}\n\t\t\t\t\tif len(v) != 2 {\n\t\t\t\t\t\tt.Fatalf(\"expected exactly two collisions\")\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif seen != len(expected) {\n\t\t\t\t\tt.Fatalf(\"expected exactly %d records to have collisions, got %d\", len(expected), seen)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tmapped, ok := results.(map[string][]uint64)\n\t\t\t\tif !ok {\n\t\t\t\t\tt.Fatalf(\"expected map[string][]uint64, got %T\", results)\n\t\t\t\t}\n\t\t\t\tseen := 0\n\t\t\t\tfor k, v := range mapped {\n\t\t\t\t\tseen++\n\t\t\t\t\tif _, ok := expected[k]; !ok {\n\t\t\t\t\t\tt.Fatalf(\"unexpected collision on key %q\", k)\n\t\t\t\t\t}\n\t\t\t\t\tif len(v) != 2 {\n\t\t\t\t\t\tt.Fatalf(\"expected exactly two collisions\")\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif seen != len(expected) {\n\t\t\t\t\tt.Fatalf(\"expected exactly %d records to have collisions, got %d\", len(expected), seen)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tresults, err = m0.API.MutexCheck(ctx, qcx, indexData.indexName, fieldData.fieldName, false, 3)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"checking mutexes: %v\", err)\n\t\t\t}\n\t\t\tmapped, ok := results.([]string)\n\t\t\tif !ok {\n\t\t\t\tt.Fatalf(\"expected []string, got %T\", results)\n\t\t\t}\n\t\t\tseen := 0\n\t\t\tfor _, k := range mapped {\n\t\t\t\tseen++\n\t\t\t\tif _, ok := expected[k]; !ok {\n\t\t\t\t\tt.Fatalf(\"unexpected collision on key %q\", k)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif seen != 3 {\n\t\t\t\tt.Fatalf(\"expected results limited to 3, got %d\", len(expected))\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc createIndexForTest(index string, coord *test.Command, t *testing.T) {\n\tctx := context.Background()\n\t_, err := coord.API.CreateIndex(ctx, index, pilosa.IndexOptions{Keys: true})\n\tif err != nil {\n\t\tt.Fatalf(\"creating index: %v\", err)\n\t}\n}\n\nfunc createFieldForTest(index string, field string, coord *test.Command, t *testing.T) {\n\tctx := context.Background()\n\t_, err := coord.API.CreateField(ctx, index, field, pilosa.OptFieldTypeInt(math.MinInt64, math.MaxInt64))\n\tif err != nil {\n\t\tt.Fatalf(\"creating field: %v\", err)\n\t}\n}\n\nfunc TestVariousApiTranslateCalls(t *testing.T) {\n\tfor i := 1; i < 8; i += 3 {\n\t\tc := test.MustRunCluster(t, i)\n\t\tdefer c.Close()\n\t\tnode := c.GetNode(0)\n\t\tapi := node.API\n\t\t// this should never actually get used because we're testing for errors here\n\t\tr := strings.NewReader(\"\")\n\t\t// test index\n\t\tidx, err := api.Holder().CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{})\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"%v: could not create test index\", err)\n\t\t}\n\t\tif _, err = idx.CreateFieldIfNotExistsWithOptions(\"field\", \"\", &pilosa.FieldOptions{Keys: false}); err != nil {\n\t\t\tt.Fatalf(\"creating field: %v\", err)\n\t\t}\n\t\tt.Run(\"translateIndexDbOnNilIndex\",\n\t\t\tfunc(t *testing.T) {\n\t\t\t\terr := api.TranslateIndexDB(context.Background(), \"nonExistentIndex\", 0, r)\n\t\t\t\texpected := fmt.Sprintf(\"index %q not found\", \"nonExistentIndex\")\n\t\t\t\tif err == nil || err.Error() != expected {\n\t\t\t\t\tt.Fatalf(\"expected '%#v', got '%#v'\", expected, err)\n\t\t\t\t}\n\t\t\t})\n\n\t\tt.Run(\"translateIndexDbOnNilTranslateStore\",\n\t\t\tfunc(t *testing.T) {\n\t\t\t\terr := api.TranslateIndexDB(context.Background(), c.Idx(), 0, r)\n\t\t\t\texpected := fmt.Sprintf(\"index %q has no translate store\", c.Idx())\n\t\t\t\tif err == nil || err.Error() != expected {\n\t\t\t\t\tt.Fatalf(\"expected '%#v', got '%#v'\", expected, err)\n\t\t\t\t}\n\t\t\t})\n\n\t\tt.Run(\"translateFieldDbOnNilIndex\",\n\t\t\tfunc(t *testing.T) {\n\t\t\t\terr := api.TranslateFieldDB(context.Background(), \"nonExistentIndex\", \"field\", r)\n\t\t\t\texpected := fmt.Sprintf(\"index %q not found\", \"nonExistentIndex\")\n\t\t\t\tif err == nil || err.Error() != expected {\n\t\t\t\t\tt.Fatalf(\"expected '%#v', got '%#v'\", expected, err)\n\t\t\t\t}\n\t\t\t})\n\n\t\tt.Run(\"translateFieldDbOnNilField\",\n\t\t\tfunc(t *testing.T) {\n\t\t\t\terr := api.TranslateFieldDB(context.Background(), c.Idx(), \"nonExistentField\", r)\n\t\t\t\texpected := fmt.Sprintf(\"field %q/%q not found\", c.Idx(), \"nonExistentField\")\n\t\t\t\tif err == nil || err.Error() != expected {\n\t\t\t\t\tt.Fatalf(\"expected '%#v', got '%#v'\", expected, err)\n\t\t\t\t}\n\t\t\t})\n\n\t\tt.Run(\"translateFieldDbNilField_keys\",\n\t\t\tfunc(t *testing.T) {\n\t\t\t\terr := api.TranslateFieldDB(context.Background(), c.Idx(), \"_keys\", r)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"expected 'nil', got '%#v'\", err)\n\t\t\t\t}\n\t\t\t})\n\t\t/*\n\t\t   TODO: this test will break, bc currently all fields create translate\n\t\t   stores, which is a bug, but one that we will eventually fix. when we do, this\n\t\t   test might come in handy t.Run(\"translateFieldDbOnNilTranslateStore\",\n\t\t   func(t *testing.T) {\n\t\t       err := api.TranslateFieldDB(context.Background(), c.Idx(), \"field\", r)\n\t\t       expected := fmt.Errorf(\"field %q/%q has no translate store\", c.Idx(), \"field\")\n\t\t       if !reflect.DeepEqual(err, expected) {\n\t\t           t.Fatalf(\"expected '%#v', got '%#v'\", expected, err)\n\t\t       }\n\t\t   })\n\t\t*/\n\t}\n}\n\nfunc TestAPI_CreateField(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\n\tnodes := make([]*test.Command, 3)\n\tfor i := range nodes {\n\t\tnodes[i] = c.GetNode(i)\n\t}\n\n\tif _, err := nodes[0].API.CreateIndex(ctx, c.Idx(), pilosa.IndexOptions{}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\teg, ctx := errgroup.WithContext(context.Background())\n\tfor _, n := range nodes {\n\t\tnode := n\n\t\teg.Go(func() error {\n\t\t\tfor i := 0; i < 10; i++ {\n\t\t\t\t_, err := node.API.CreateField(ctx, c.Idx(), fmt.Sprintf(\"f%d\", i))\n\t\t\t\tif err != nil && !errors.Is(err, pilosa.ErrFieldExists) {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t}\n\terr := eg.Wait()\n\tif err != nil {\n\t\tif errors.Is(err, pilosa.ErrFieldExists) {\n\t\t\tt.Fatalf(\"conflict error: %v\", err)\n\t\t}\n\t\tt.Fatalf(\"unexpected error: %T %v\", err, err)\n\t}\n}\n\nfunc TestAPI_RBFDebugInfo(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\n\tcoord := c.GetPrimary()\n\n\tif _, err := coord.API.CreateIndex(ctx, c.Idx(), pilosa.IndexOptions{}); err != nil {\n\t\tt.Fatal(err)\n\t} else if infos := coord.API.RBFDebugInfo(); infos == nil {\n\t\tt.Fatal(\"expected info\")\n\t}\n}\n\n// makeUser makes an authnUserInfo from groups and a name and a secret key\nfunc makeUser(t *testing.T, groups []authn.Group, name, secret string) *authn.UserInfo {\n\ttkn := jwt.New(jwt.SigningMethodHS256)\n\tclaims := tkn.Claims.(jwt.MapClaims)\n\tclaims[\"oid\"] = \"42\"\n\tclaims[\"name\"] = name\n\tsecretKey, _ := hex.DecodeString(secret)\n\n\tvalidToken, err := tkn.SignedString(secretKey)\n\tif err != nil {\n\t\tt.Fatalf(\"signing string %v\", err)\n\t}\n\n\treturn &authn.UserInfo{\n\t\tUserID:   \"fake\" + name,\n\t\tUserName: name,\n\t\tGroups:   groups,\n\t\tToken:    validToken,\n\t\tExpiry:   time.Time{},\n\t}\n}\n\nfunc TestAuth_MultiNode(t *testing.T) {\n\t// create permissions file\n\tpermissions := `\n\"user-groups\":\n  \"dca35310-ecda-4f23-86cd-876aee55906b\":\n    \"test\": \"read\"\n  \"dca35310-ecda-4f23-86cd-876aee55906f\":\n    \"test\": \"write\"\nadmin: \"ac97c9e2-346b-42a2-b6da-18bcb61a32fe\"`\n\tadminUser := makeUser(t, []authn.Group{{GroupID: \"ac97c9e2-346b-42a2-b6da-18bcb61a32fe\", GroupName: \"adminGroup\"}}, \"admin\", \"DEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEF\")\n\tadminCtx := authn.WithUserInfo(context.Background(), adminUser)\n\treadUser := makeUser(t, []authn.Group{{GroupID: \"dca35310-ecda-4f23-86cd-876aee55906b\", GroupName: \"readGroup\"}}, \"reader\", \"DEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEF\")\n\treadCtx := authn.WithUserInfo(context.Background(), readUser)\n\twriteUser := makeUser(t, []authn.Group{{GroupID: \"dca35310-ecda-4f23-86cd-876aee55906f\", GroupName: \"writeGroup\"}}, \"writer\", \"DEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEED\")\n\twriteCtx := authn.WithUserInfo(context.Background(), writeUser)\n\tsrv := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\ttoken, ok := r.Header[\"Authorization\"]\n\t\tif !ok || len(token) == 0 {\n\t\t\thttp.Error(w, \"BAD REQUEST\", http.StatusBadRequest)\n\t\t\treturn\n\t\t}\n\t\tg := []authn.Group{}\n\t\tswitch strings.TrimPrefix(token[0], \"Bearer \") {\n\t\tcase adminUser.Token:\n\t\t\tg = adminUser.Groups\n\t\tcase readUser.Token:\n\t\t\tg = readUser.Groups\n\t\tcase writeUser.Token:\n\t\t\tg = writeUser.Groups\n\t\t}\n\t\tif err := json.NewEncoder(w).Encode(authn.Groups{Groups: g}); err != nil {\n\t\t\tt.Fatalf(\"unexpected error marshalling groups response: %v\", err)\n\t\t}\n\t}))\n\n\t// authentication on\n\tauth := server.Auth{\n\t\tEnable:           true,\n\t\tClientId:         \"e9088663-eb08-41d7-8f65-efb5f54bbb71\",\n\t\tClientSecret:     \"DEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEF\",\n\t\tAuthorizeURL:     \"https://login.microsoftonline.com/4a137d66-d161-4ae4-b1e6-07e9920874b8/oauth2/v2.0/authorize\",\n\t\tTokenURL:         \"https://login.microsoftonline.com/4a137d66-d161-4ae4-b1e6-07e9920874b8/oauth2/v2.0/token\",\n\t\tGroupEndpointURL: srv.URL,\n\t\tRedirectBaseURL:  \"https://localhost:10101\",\n\t\tLogoutURL:        \"https://login.microsoftonline.com/common/oauth2/v2.0/logout\",\n\t\tScopes:           []string{\"https://graph.microsoft.com/.default\", \"offline_access\"},\n\t\tSecretKey:        \"DEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEF\",\n\t\tPermissionsFile:  writeTestFile(t, \"permissions.yaml\", permissions),\n\t\tQueryLogPath:     writeTestFile(t, \"queryLog.log\", \"\"),\n\t}\n\n\tconfig := server.NewConfig()\n\tconfig.Auth = auth\n\n\t// set up TLS certificates\n\tlocalhostCert := `-----BEGIN CERTIFICATE-----\nMIICEzCCAXygAwIBAgIQMIMChMLGrR+QvmQvpwAU6zANBgkqhkiG9w0BAQsFADAS\nMRAwDgYDVQQKEwdBY21lIENvMCAXDTcwMDEwMTAwMDAwMFoYDzIwODQwMTI5MTYw\nMDAwWjASMRAwDgYDVQQKEwdBY21lIENvMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCB\niQKBgQDuLnQAI3mDgey3VBzWnB2L39JUU4txjeVE6myuDqkM/uGlfjb9SjY1bIw4\niA5sBBZzHi3z0h1YV8QPuxEbi4nW91IJm2gsvvZhIrCHS3l6afab4pZBl2+XsDul\nrKBxKKtD1rGxlG4LjncdabFn9gvLZad2bSysqz/qTAUStTvqJQIDAQABo2gwZjAO\nBgNVHQ8BAf8EBAMCAqQwEwYDVR0lBAwwCgYIKwYBBQUHAwEwDwYDVR0TAQH/BAUw\nAwEB/zAuBgNVHREEJzAlggtleGFtcGxlLmNvbYcEfwAAAYcQAAAAAAAAAAAAAAAA\nAAAAATANBgkqhkiG9w0BAQsFAAOBgQCEcetwO59EWk7WiJsG4x8SY+UIAA+flUI9\ntyC4lNhbcF2Idq9greZwbYCqTTTr2XiRNSMLCOjKyI7ukPoPjo16ocHj+P3vZGfs\nh1fIw3cSS2OolhloGw/XM6RWPWtPAlGykKLciQrBru5NAPvCMsb/I1DAceTiotQM\nfblo6RBxUQ==\n-----END CERTIFICATE-----`\n\n\tlocalhostKey := `-----BEGIN RSA PRIVATE KEY-----\nMIICXgIBAAKBgQDuLnQAI3mDgey3VBzWnB2L39JUU4txjeVE6myuDqkM/uGlfjb9\nSjY1bIw4iA5sBBZzHi3z0h1YV8QPuxEbi4nW91IJm2gsvvZhIrCHS3l6afab4pZB\nl2+XsDulrKBxKKtD1rGxlG4LjncdabFn9gvLZad2bSysqz/qTAUStTvqJQIDAQAB\nAoGAGRzwwir7XvBOAy5tM/uV6e+Zf6anZzus1s1Y1ClbjbE6HXbnWWF/wbZGOpet\n3Zm4vD6MXc7jpTLryzTQIvVdfQbRc6+MUVeLKwZatTXtdZrhu+Jk7hx0nTPy8Jcb\nuJqFk541aEw+mMogY/xEcfbWd6IOkp+4xqjlFLBEDytgbIECQQDvH/E6nk+hgN4H\nqzzVtxxr397vWrjrIgPbJpQvBsafG7b0dA4AFjwVbFLmQcj2PprIMmPcQrooz8vp\njy4SHEg1AkEA/v13/5M47K9vCxmb8QeD/asydfsgS5TeuNi8DoUBEmiSJwma7FXY\nfFUtxuvL7XvjwjN5B30pNEbc6Iuyt7y4MQJBAIt21su4b3sjXNueLKH85Q+phy2U\nfQtuUE9txblTu14q3N7gHRZB4ZMhFYyDy8CKrN2cPg/Fvyt0Xlp/DoCzjA0CQQDU\ny2ptGsuSmgUtWj3NM9xuwYPm+Z/F84K6+ARYiZ6PYj013sovGKUFfYAqVXVlxtIX\nqyUBnu3X9ps8ZfjLZO7BAkEAlT4R5Yl6cGhaJQYZHOde3JEMhNRcVFMO8dJDaFeo\nf9Oeos0UUothgiDktdQHxdNEwLjQf7lJJBzV+5OtwswCWA==\n-----END RSA PRIVATE KEY-----`\n\n\tconfig.TLS.CertificateKeyPath = writeTestFile(t, \"certKey.pem\", localhostKey)\n\tconfig.TLS.CertificatePath = writeTestFile(t, \"cert.pem\", localhostCert)\n\n\tc := test.MustRunUnsharedCluster(t, 3,\n\t\t[]server.CommandOption{\n\t\t\tserver.OptCommandServerOptions(\n\t\t\t\tpilosa.OptServerNodeID(\"node0\"),\n\t\t\t),\n\t\t\tserver.OptCommandConfig(config),\n\t\t},\n\t\t[]server.CommandOption{\n\t\t\tserver.OptCommandServerOptions(\n\t\t\t\tpilosa.OptServerNodeID(\"node1\"),\n\t\t\t),\n\t\t\tserver.OptCommandConfig(config),\n\t\t},\n\t\t[]server.CommandOption{\n\t\t\tserver.OptCommandServerOptions(\n\t\t\t\tpilosa.OptServerNodeID(\"node2\"),\n\t\t\t),\n\t\t\tserver.OptCommandConfig(config),\n\t\t},\n\t)\n\tdefer c.Close()\n\n\tprimaryAPI := c.GetPrimary().API\n\n\t// needs internal/cluster/message\n\t// Note: This indexName wouldn't be safe on a shared cluster, but we have to use an\n\t// unshared cluster to set up the auth config anyway.\n\tindexName := \"test\"\n\t_, err := primaryAPI.CreateIndex(adminCtx, indexName, pilosa.IndexOptions{})\n\tif err != nil {\n\t\tt.Fatalf(\"creating index: %v\", err)\n\t}\n\t// needs internal/translate/data\n\tfieldName := \"f\"\n\t_, err = primaryAPI.CreateField(adminCtx, indexName, fieldName, pilosa.OptFieldTypeSet(pilosa.DefaultCacheType, 100))\n\tif err != nil {\n\t\tt.Fatalf(\"creating field: %v\", err)\n\t}\n\n\t_, err = primaryAPI.Query(readCtx, &pilosa.QueryRequest{\n\t\tIndex: indexName,\n\t\tQuery: fmt.Sprintf(`Set(1, %s=1)`, fieldName),\n\t})\n\tif err == nil {\n\t\tt.Fatalf(\"readCtx should not be able to set bits\")\n\t}\n\n\t_, err = primaryAPI.Query(writeCtx, &pilosa.QueryRequest{\n\t\tIndex: indexName,\n\t\tQuery: fmt.Sprintf(`Set(1, %s=1)`, fieldName),\n\t})\n\tif err != nil {\n\t\tt.Fatalf(\"writeCtx should be able to set bits: %v\", err)\n\t}\n\n\t_, err = primaryAPI.Query(adminCtx, &pilosa.QueryRequest{\n\t\tIndex: indexName,\n\t\tQuery: fmt.Sprintf(`Set(1, %s=1)`, fieldName),\n\t})\n\tif err != nil {\n\t\tt.Fatalf(\"adminCtx should be able to set bits: %v\", err)\n\t}\n\n\t_, err = primaryAPI.Query(readCtx, &pilosa.QueryRequest{\n\t\tIndex: indexName,\n\t\tQuery: fmt.Sprintf(`Count(Row(%s=1))`, fieldName),\n\t})\n\tif err != nil {\n\t\tt.Fatalf(\"readCtx should be able read: %v\", err)\n\t}\n\n\t_, err = primaryAPI.Query(writeCtx, &pilosa.QueryRequest{\n\t\tIndex: indexName,\n\t\tQuery: fmt.Sprintf(`Count(Row(%s=1))`, fieldName),\n\t})\n\tif err != nil {\n\t\tt.Fatalf(\"writeCtx should be able read: %v\", err)\n\t}\n\n\t_, err = primaryAPI.Query(adminCtx, &pilosa.QueryRequest{\n\t\tIndex: indexName,\n\t\tQuery: fmt.Sprintf(`Count(Row(%s=1))`, fieldName),\n\t})\n\tif err != nil {\n\t\tt.Fatalf(\"adminCtx should be able read: %v\", err)\n\t}\n}\n\nfunc writeTestFile(t *testing.T, filename, content string) string {\n\tt.Helper()\n\tfname := filepath.Join(t.TempDir(), filename)\n\tf, err := os.Create(fname)\n\tif err != nil {\n\t\tt.Fatalf(\"could not create file %v with err %v\", filename, err)\n\t}\n\t_, err = io.WriteString(f, content)\n\tif err != nil {\n\t\tt.Fatalf(\"could not write string %v\", err)\n\t}\n\tif err := f.Close(); err != nil {\n\t\tt.Fatalf(\"could not close file %v\", err)\n\t}\n\treturn fname\n}\n"
        },
        {
          "name": "apimethod_string.go",
          "type": "blob",
          "size": 2.0869140625,
          "content": "// Code generated by \"stringer -type=apiMethod\"; DO NOT EDIT.\n\npackage pilosa\n\nimport \"strconv\"\n\nfunc _() {\n\t// An \"invalid array index\" compiler error signifies that the constant values have changed.\n\t// Re-run the stringer command to generate them again.\n\tvar x [1]struct{}\n\t_ = x[apiClusterMessage-0]\n\t_ = x[apiCreateField-1]\n\t_ = x[apiCreateIndex-2]\n\t_ = x[apiDeleteField-3]\n\t_ = x[apiDeleteAvailableShard-4]\n\t_ = x[apiDeleteIndex-5]\n\t_ = x[apiDeleteView-6]\n\t_ = x[apiExportCSV-7]\n\t_ = x[apiFragmentBlockData-8]\n\t_ = x[apiFragmentBlocks-9]\n\t_ = x[apiFragmentData-10]\n\t_ = x[apiTranslateData-11]\n\t_ = x[apiFieldTranslateData-12]\n\t_ = x[apiField-13]\n\t_ = x[apiImport-14]\n\t_ = x[apiImportValue-15]\n\t_ = x[apiIndex-16]\n\t_ = x[apiQuery-17]\n\t_ = x[apiRecalculateCaches-18]\n\t_ = x[apiSchema-19]\n\t_ = x[apiShardNodes-20]\n\t_ = x[apiState-21]\n\t_ = x[apiViews-22]\n\t_ = x[apiApplySchema-23]\n\t_ = x[apiStartTransaction-24]\n\t_ = x[apiFinishTransaction-25]\n\t_ = x[apiTransactions-26]\n\t_ = x[apiGetTransaction-27]\n\t_ = x[apiActiveQueries-28]\n\t_ = x[apiPastQueries-29]\n\t_ = x[apiIDReserve-30]\n\t_ = x[apiIDCommit-31]\n\t_ = x[apiIDReset-32]\n\t_ = x[apiPartitionNodes-33]\n\t_ = x[apiMutexCheck-34]\n}\n\nconst _apiMethod_name = \"apiClusterMessageapiCreateFieldapiCreateIndexapiDeleteFieldapiDeleteAvailableShardapiDeleteIndexapiDeleteViewapiExportCSVapiFragmentBlockDataapiFragmentBlocksapiFragmentDataapiTranslateDataapiFieldTranslateDataapiFieldapiImportapiImportValueapiIndexapiQueryapiRecalculateCachesapiSchemaapiShardNodesapiStateapiViewsapiApplySchemaapiStartTransactionapiFinishTransactionapiTransactionsapiGetTransactionapiActiveQueriesapiPastQueriesapiIDReserveapiIDCommitapiIDResetapiPartitionNodesapiMutexCheck\"\n\nvar _apiMethod_index = [...]uint16{0, 17, 31, 45, 59, 82, 96, 109, 121, 141, 158, 173, 189, 210, 218, 227, 241, 249, 257, 277, 286, 299, 307, 315, 329, 348, 368, 383, 400, 416, 430, 442, 453, 463, 480, 493}\n\nfunc (i apiMethod) String() string {\n\tif i < 0 || i >= apiMethod(len(_apiMethod_index)-1) {\n\t\treturn \"apiMethod(\" + strconv.FormatInt(int64(i), 10) + \")\"\n\t}\n\treturn _apiMethod_name[_apiMethod_index[i]:_apiMethod_index[i+1]]\n}\n"
        },
        {
          "name": "apply.go",
          "type": "blob",
          "size": 18.783203125,
          "content": "// Copyright 2021 Molecula Corp. All rights reserved.\npackage pilosa\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/apache/arrow/go/v10/arrow\"\n\t\"github.com/apache/arrow/go/v10/arrow/array\"\n\t\"github.com/apache/arrow/go/v10/arrow/memory\"\n\t\"github.com/featurebasedb/featurebase/v3/pql\"\n\t\"github.com/featurebasedb/featurebase/v3/tracing\"\n\t\"github.com/featurebasedb/featurebase/v3/vprint\"\n\t\"github.com/gomem/gomem/pkg/dataframe\"\n\t\"github.com/pkg/errors\"\n\n\tivy \"robpike.io/ivy/arrow\"\n\tconfig \"robpike.io/ivy/config\"\n\t\"robpike.io/ivy/exec\"\n\t\"robpike.io/ivy/parse\"\n\t\"robpike.io/ivy/run\"\n\t\"robpike.io/ivy/scan\"\n\t\"robpike.io/ivy/value\"\n)\n\ntype (\n\tApplyResult *arrow.Column\n)\n\nfunc runIvyString(context value.Context, str string) (ok bool, err error) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr = r.(value.Error)\n\t\t}\n\t}()\n\tscanner := scan.New(context, \"<args>\", strings.NewReader(str))\n\tparser := parse.NewParser(\"<args>\", scanner, context)\n\tok = run.Run(parser, context, false)\n\treturn\n}\n\n// Possibly combine all arrays together then apply some interesting\n// computation at the end?\nfunc IvyReduce(reduceCode string, opCode string, opt *ExecOptions) (func(ctx context.Context, prev, v interface{}) interface{}, func() (*dataframe.DataFrame, error)) {\n\tvar accumulator value.Value\n\tmu := &sync.Mutex{}\n\tconcat := value.BinaryOps[opCode]\n\tconf := getDefaultConfig()\n\tctxIvy := exec.NewContext(&conf)\n\t// concat returned results at coordinating node.\n\treduceFn := func(ctx context.Context, prev, v interface{}) interface{} {\n\t\tif v == nil {\n\t\t\treturn prev\n\t\t}\n\t\tif accumulator == nil {\n\t\t\tswitch val := v.(type) {\n\t\t\tcase *dataframe.DataFrame:\n\t\t\t\tcol := val.ColumnAt(0)\n\t\t\t\tresolver := dataframe.NewChunkResolver(col)\n\t\t\t\taccumulator = value.NewArrowVector(col, &conf, &resolver)\n\t\t\tcase value.Value:\n\t\t\t\taccumulator = v.(value.Value)\n\t\t\tdefault:\n\t\t\t\treturn errors.New(fmt.Sprintf(\"ivy reduction failed first unexpected type %T\", v))\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t\tswitch val := v.(type) {\n\t\tcase *dataframe.DataFrame:\n\t\t\tcol := val.ColumnAt(0)\n\t\t\tresolver := dataframe.NewChunkResolver(col)\n\t\t\tx := value.NewArrowVector(col, &conf, &resolver)\n\t\t\tmu.Lock() // i'm being overyerly cautious..need to confirm this can be concurrent\n\t\t\taccumulator = concat.EvalBinary(ctxIvy, accumulator, x)\n\t\t\tmu.Unlock()\n\t\tcase value.Value:\n\t\t\tmu.Lock()\n\t\t\taccumulator = concat.EvalBinary(ctxIvy, accumulator, val)\n\t\t\tmu.Unlock()\n\t\tdefault:\n\t\t\treturn errors.New(fmt.Sprintf(\"ivy reduction failed unexpected type %T\", v))\n\t\t}\n\n\t\treturn nil\n\t}\n\ttablerFn := func() (*dataframe.DataFrame, error) {\n\t\tpool := memory.NewGoAllocator() // TODO(twg) 2022/09/01 singledton?\n\t\tif opt.Remote {\n\t\t\tcol := value.ToArrowColumn(accumulator, pool)\n\t\t\treturn dataframe.NewDataFrameFromColumns(pool, []arrow.Column{*col})\n\t\t}\n\t\t// only actually reduce on the initiating node i hate the network\n\t\t// over head but oh well\n\t\tctxIvy.AssignGlobal(\"_\", accumulator)\n\t\tok, err := runIvyString(ctxIvy, reduceCode)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif ok {\n\t\t\tv := ctxIvy.Global(\"_\")\n\t\t\tif v == nil {\n\t\t\t\treturn nil, errors.New(\"ivy reduction no result \")\n\t\t\t}\n\t\t\tcol := value.ToArrowColumn(ctxIvy.Global(\"_\"), pool)\n\n\t\t\treturn dataframe.NewDataFrameFromColumns(pool, []arrow.Column{*col})\n\t\t}\n\t\treturn nil, errors.New(\"ivy reduction failed \")\n\t}\n\n\treturn reduceFn, tablerFn\n}\n\n// executeApply executes a Apply() call.\nfunc (e *executor) executeApply(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (*dataframe.DataFrame, error) {\n\tif !e.dataframeEnabled {\n\t\treturn nil, errors.New(\"Dataframe support not enabled\")\n\t}\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"Executor.executeMax\")\n\tdefer span.Finish()\n\n\tif _, err := c.FirstStringArg(\"_ivy\"); err != nil {\n\t\treturn nil, errors.Wrap(err, \" no ivy program supplied\")\n\t}\n\n\tif len(c.Children) > 1 {\n\t\treturn nil, errors.New(\"Apply() only accepts a single bitmap input filter\")\n\t}\n\n\t// Execute calls in bulk on each remote node and merge.\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\treturn e.executeApplyShard(ctx, qcx, index, c, shard)\n\t}\n\tivyReduce, ok, err := c.StringArg(\"_ivyReduce\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treduceFn, tablerFn := IvyReduce(\"_\", \",\", opt)\n\tif ok {\n\t\treduceFn, tablerFn = IvyReduce(ivyReduce, \",\", opt)\n\t}\n\n\t_, err = e.mapReduce(ctx, index, shards, c, opt, mapFn, reduceFn)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn tablerFn()\n}\n\nfunc getDefaultConfig() config.Config {\n\tmaxbits := uint(1e9)   // \"maximum size of an integer, in bits; 0 means no limit\")\n\tmaxdigits := uint(1e4) // \"above this many `digits`, integers print as floating point; 0 disables\")\n\tmaxstack := uint(100000)\n\torigin := 1  // \"set index origin to `n` (must be 0 or 1)\")\n\tprompt := \"\" // flag.String(\"prompt\", \"\", \"command `prompt`\")\n\tformat := \"\"\n\t//      debugFlag := \"\" // flag.String(\"debug\", \"\", \"comma-separated `names` of debug settings to enable\")\n\tconf := config.Config{}\n\tconf.SetFormat(format)\n\tconf.SetMaxBits(maxbits)\n\tconf.SetMaxDigits(maxdigits)\n\tconf.SetMaxStack(maxstack)\n\tconf.SetOrigin(origin)\n\tconf.SetPrompt(prompt)\n\tconf.SetOutput(io.Discard)\n\tconf.SetErrOutput(io.Discard)\n\tconf.SetEmbedded(true) // needed to propagate panic\n\treturn conf\n}\n\nfunc filterDataframe(resolver dataframe.Resolver, pool memory.Allocator, filter []int64) (*dataframe.IndexResolver, error) {\n\tif resolver.NumRows() == 0 {\n\t\treturn nil, errors.New(\"No data\")\n\t}\n\tindexResolver := dataframe.NewIndexResolver(len(filter), uint32(ShardWidth-1))\n\tfor i, id := range filter {\n\t\tif int(id) >= resolver.NumRows() {\n\t\t\tcontinue\n\t\t}\n\t\tc, o := resolver.Resolve(int(id))\n\t\tindexResolver.Set(i, c, o)\n\n\t}\n\treturn indexResolver, nil\n}\n\nfunc (e *executor) executeApplyShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64) (value.Value, error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"Executor.executeApplyShard\")\n\tdefer span.Finish()\n\n\tivyProgram, ok, err := c.StringArg(\"_ivy\")\n\tif err != nil || !ok {\n\t\treturn nil, errors.Wrap(err, \"finding ivy program\")\n\t}\n\tvar filter *Row\n\tif len(c.Children) == 1 {\n\t\trow, err := e.executeBitmapCallShard(ctx, qcx, index, c.Children[0], shard)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfilter = row\n\t\tif !filter.Any() {\n\t\t\t// no need to actuall run the query for its not operating against any values\n\t\t\treturn value.NewVector([]value.Value{}), nil\n\t\t}\n\t}\n\t//\n\tpool := memory.NewGoAllocator() // TODO(twg) 2022/09/01 singledton?\n\n\tids := filter.ShardColumns() // needs to be shard columns\n\t// Fetch index.\n\tidx := e.Holder.Index(index)\n\tif idx == nil {\n\t\treturn nil, newNotFoundError(ErrIndexNotFound, index)\n\t}\n\n\tfname := idx.GetDataFramePath(shard)\n\n\tif !e.dataFrameExists(fname) {\n\t\treturn value.NewVector([]value.Value{}), nil\n\t}\n\n\ttable, err := e.getDataTable(ctx, fname, pool)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer table.Release()\n\tdf, err := dataframe.NewDataFrameFromTable(pool, table)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tp := dataframe.NewChunkResolver(df.ColumnAt(0))\n\tvar resolver dataframe.Resolver\n\tresolver = &p\n\tif filter != nil {\n\t\tif len(ids) == 0 {\n\t\t\treturn value.NewVector([]value.Value{}), nil\n\t\t}\n\t\tresolver, err = filterDataframe(resolver, pool, ids)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tconf := getDefaultConfig()\n\tcontext, err := ivy.RunArrow(dataframe.NewTableFacade(df), ivyProgram, conf, resolver)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"ivy map error: %w\", err)\n\t}\n\treturn context.Global(\"_\"), nil\n}\n\n// ///////////////////////////////////////////////////////\n// all the ingest supporting functions\n// ///////////////////////////////////////////////////////\n\nfunc NewShardFile(ctx context.Context, name string, mem memory.Allocator, e *executor) (*ShardFile, error) {\n\tif !e.dataFrameExists(name) {\n\t\treturn &ShardFile{dest: name, executor: e, strings: make(map[key][]string)}, nil\n\t}\n\t// else read in existing\n\ttable, err := e.getDataTable(ctx, name, mem)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &ShardFile{table: table, schema: table.Schema(), dest: name, executor: e, strings: make(map[key][]string)}, nil\n}\n\ntype NameType struct {\n\tName     string\n\tDataType arrow.DataType\n}\ntype ChangesetRequest struct {\n\tShardIds     []int64 // only shardwidth bits to provide 0 indexing inside shard file\n\tColumns      []interface{}\n\tSimpleSchema []NameType\n}\n\n// TODO(twg) 2022/09/30 Needs a refactor\nfunc cast(v interface{}) arrow.DataType {\n\tswitch v.(type) {\n\tcase *arrow.Int64Type:\n\t\treturn arrow.PrimitiveTypes.Int64\n\tcase int64:\n\t\treturn arrow.PrimitiveTypes.Int64\n\tcase *arrow.Float64Type:\n\t\treturn arrow.PrimitiveTypes.Float64\n\tcase float64:\n\t\treturn arrow.PrimitiveTypes.Float64\n\tcase *arrow.StringType:\n\t\treturn arrow.BinaryTypes.String\n\tdefault:\n\t\tvprint.VV(\"%T .... %v\", v, v)\n\t}\n\treturn arrow.PrimitiveTypes.Int64\n}\n\nfunc (cr *ChangesetRequest) ArrowSchema() *arrow.Schema {\n\tfields := make([]arrow.Field, len(cr.SimpleSchema))\n\tfor i := range cr.SimpleSchema {\n\t\tfields[i] = arrow.Field{Name: cr.SimpleSchema[i].Name, Type: cast(cr.SimpleSchema[i].DataType)}\n\t}\n\treturn arrow.NewSchema(fields, nil)\n}\n\ntype key struct {\n\tcol   int\n\tchunk int\n}\n\ntype ShardFile struct {\n\ttable      arrow.Table\n\tschema     *arrow.Schema\n\tbeforeRows int64\n\tadded      int64\n\tcolumns    []interface{}\n\tdest       string\n\texecutor   *executor\n\tstrings    map[key][]string\n}\n\nfunc compareSchema(s1, s2 *arrow.Schema) bool {\n\tif s1 == nil || s2 == nil {\n\t\treturn false\n\t}\n\tif len(s1.Fields()) != len(s2.Fields()) {\n\t\treturn false\n\t}\n\tfor i := 0; i < len(s1.Fields()); i++ {\n\t\tf1 := s1.Field(i)\n\t\tf2 := s2.Field(i)\n\t\tif f1.Name != f2.Name {\n\t\t\treturn false\n\t\t}\n\t\tif f1.Type != f2.Type {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc (sf *ShardFile) EnsureSchema(cs *ChangesetRequest) error {\n\tschema := cs.ArrowSchema()\n\tif sf.schema == nil {\n\t\tsf.schema = schema\n\t} else {\n\t\tif !compareSchema(sf.schema, schema) {\n\t\t\tvprint.VV(\"incomeing schema\", schema)\n\t\t\tvprint.VV(\"existing schema\", sf.schema)\n\t\t\treturn errors.New(\"dataframe schema's don't match\")\n\t\t}\n\t}\n\tsf.columns = make([]interface{}, len(sf.schema.Fields()))\n\treturn nil\n}\n\nfunc (sf *ShardFile) buildAppenders(maxid int64) {\n\tif sf.table != nil {\n\t\tsf.beforeRows = sf.table.NumRows()\n\t}\n\tif maxid < sf.beforeRows {\n\t\t// no need to add new rows\n\t\treturn\n\t}\n\tnewSize := maxid - sf.beforeRows + 1\n\tfor i := 0; i < len(sf.schema.Fields()); i++ {\n\t\tswitch sf.schema.Field(i).Type {\n\t\tcase arrow.PrimitiveTypes.Int64:\n\t\t\tsf.columns[i] = make([]int64, newSize)\n\t\tcase arrow.PrimitiveTypes.Float64:\n\t\t\tsf.columns[i] = make([]float64, newSize)\n\t\tcase arrow.BinaryTypes.String:\n\t\t\tsf.columns[i] = make([]string, newSize)\n\t\t}\n\t}\n\tsf.added = newSize\n}\n\n// the row offset must be reset to 0 for the slices being appended\nfunc (sf *ShardFile) SetIntValue(col int, row int64, val int64) {\n\tv := sf.columns[col].([]int64)\n\tv[row-sf.beforeRows] = val\n}\n\nfunc (sf *ShardFile) SetFloatValue(col int, row int64, val float64) {\n\tv := sf.columns[col].([]float64)\n\tv[row-sf.beforeRows] = val\n}\n\nfunc (sf *ShardFile) SetStringValue(col int, row int64, val string) {\n\tv := sf.columns[col].([]string)\n\tv[row-sf.beforeRows] = val\n}\n\nfunc (sf *ShardFile) Process(cs *ChangesetRequest) error {\n\terr := sf.process(cs)\n\tif err != nil {\n\t\treturn err\n\t}\n\trtemp := sf.dest + \".temp\"\n\terr = sf.Save(rtemp)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn os.Rename(rtemp+sf.executor.TableExtension(), sf.dest+sf.executor.TableExtension())\n}\n\nfunc (sf *ShardFile) LoadBlobs() error {\n\tfor col := 0; col < len(sf.schema.Fields()); col++ {\n\t\tcolumn := sf.table.Column(col)\n\t\tswitch column.DataType() {\n\t\tcase arrow.BinaryTypes.String:\n\t\t\tfor i, chunk := range column.Data().Chunks() {\n\t\t\t\tstringData := chunk.(*array.String)\n\t\t\t\tk := key{col: col, chunk: i}\n\t\t\t\tfor j := 0; j < stringData.Len(); j++ {\n\t\t\t\t\tv := stringData.Value(j)\n\t\t\t\t\tsf.strings[k] = append(sf.strings[k], v)\n\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (sf *ShardFile) ReplaceString(col, chunk, l int, s string) {\n\tsf.strings[key{col: col, chunk: chunk}][l] = s\n}\n\nfunc (sf *ShardFile) process(cs *ChangesetRequest) error {\n\toffset := 0\n\tif sf.table != nil {\n\t\t// need to load blobs prior\n\t\tsf.LoadBlobs()\n\t\tcolumn := sf.table.Column(0)\n\t\tresolver := dataframe.NewChunkResolver(column)\n\t\tfor i, rowid := range cs.ShardIds {\n\t\t\toffset = i\n\t\t\tif rowid >= sf.table.NumRows() {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tchunk, l := resolver.Resolve(int(rowid))\n\t\t\tfor col := 0; col < len(sf.schema.Fields()); col++ {\n\t\t\t\tcolumn := sf.table.Column(col)\n\t\t\t\tswitch column.DataType() {\n\t\t\t\tcase arrow.PrimitiveTypes.Int64:\n\t\t\t\t\tv := column.Data().Chunk(chunk).(*array.Int64).Int64Values()\n\t\t\t\t\tv[l] = cs.Columns[col].([]int64)[i]\n\t\t\t\tcase arrow.PrimitiveTypes.Float64:\n\t\t\t\t\tv := column.Data().Chunk(chunk).(*array.Float64).Float64Values()\n\t\t\t\t\tv[l] = cs.Columns[col].([]float64)[i]\n\t\t\t\tcase arrow.BinaryTypes.String:\n\t\t\t\t\t// TODO(twg) 2023/01/09 How to update existing?\n\t\t\t\t\tnew := cs.Columns[col].([]string)[i]\n\t\t\t\t\tsf.ReplaceString(col, chunk, l, new)\n\t\t\t\tdefault:\n\t\t\t\t\tpanic(fmt.Sprintf(\"Unknown Type %v\", column.DataType()))\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tmax := cs.ShardIds[len(cs.ShardIds)-1]\n\tsf.buildAppenders(max)\n\t// need to check if only replace and no apend\n\tif sf.added > 0 {\n\t\tfor i, rowid := range cs.ShardIds[offset:] {\n\t\t\ti += offset\n\n\t\t\tfor col := 0; col < len(sf.schema.Fields()); col++ {\n\t\t\t\tswitch sf.schema.Field(col).Type {\n\t\t\t\tcase arrow.PrimitiveTypes.Int64:\n\t\t\t\t\tsf.SetIntValue(col, rowid, cs.Columns[col].([]int64)[i])\n\t\t\t\tcase arrow.PrimitiveTypes.Float64:\n\t\t\t\t\tsf.SetFloatValue(col, rowid, cs.Columns[col].([]float64)[i])\n\t\t\t\tcase arrow.BinaryTypes.String:\n\t\t\t\t\tsf.SetStringValue(col, rowid, cs.Columns[col].([]string)[i])\n\t\t\t\tdefault:\n\t\t\t\t\tpanic(fmt.Sprintf(\"2 Unknown Type %v\", sf.schema.Field(col).Type))\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\ntype twoSlices struct {\n\tid_slice    []int\n\tlists_slice [][]string\n}\n\ntype SortByOther twoSlices\n\nfunc (sbo SortByOther) Len() int {\n\treturn len(sbo.id_slice)\n}\n\nfunc (sbo SortByOther) Swap(i, j int) {\n\tsbo.id_slice[i], sbo.id_slice[j] = sbo.id_slice[j], sbo.id_slice[i]\n\tsbo.lists_slice[i], sbo.lists_slice[j] = sbo.lists_slice[j], sbo.lists_slice[i]\n}\n\nfunc (sbo SortByOther) Less(i, j int) bool {\n\treturn sbo.id_slice[i] < sbo.id_slice[j]\n}\n\nfunc (sf *ShardFile) buildFromStrings(idx int, mem memory.Allocator) []arrow.Array {\n\tids := make([]int, 0)\n\tlists := make([][]string, 0)\n\tfor k, v := range sf.strings {\n\t\tif k.col == idx { // ugh not ordered :(\n\t\t\tids = append(ids, k.chunk)\n\t\t\tlists = append(lists, v)\n\t\t}\n\t}\n\t// sort ids/lists\n\tparts := twoSlices{id_slice: ids, lists_slice: lists}\n\tsort.Sort(SortByOther(parts))\n\n\tbuilder := array.NewStringBuilder(mem)\n\tchunks := make([]arrow.Array, 0)\n\tfor _, v := range parts.lists_slice {\n\t\tbuilder.AppendValues(v, nil)\n\t\tnewChunk := builder.NewArray()\n\t\tchunks = append(chunks, newChunk)\n\t}\n\treturn chunks\n}\n\nfunc (sf *ShardFile) Save(name string) error {\n\tparts := make([]arrow.Array, 0)\n\tmem := memory.NewGoAllocator()\n\tfor col := 0; col < len(sf.schema.Fields()); col++ {\n\t\tchunks := make([]arrow.Array, 0)\n\t\tif sf.table != nil {\n\t\t\t// we append if there was existing file\n\t\t\tcolumn := sf.table.Column(col)\n\t\t\t// if primitive type\n\t\t\tswitch column.DataType() {\n\t\t\tcase arrow.BinaryTypes.String:\n\t\t\t\tchunks = sf.buildFromStrings(col, mem)\n\t\t\tdefault:\n\t\t\t\tchunks = append(chunks, column.Data().Chunks()...)\n\t\t\t}\n\t\t\t// else binary type\n\t\t}\n\t\tswitch sf.schema.Field(col).Type {\n\t\tcase arrow.PrimitiveTypes.Int64:\n\t\t\t// case *arrow.Int64Type:\n\t\t\tif sf.added > 0 {\n\t\t\t\tibuild := array.NewInt64Builder(mem)\n\t\t\t\tibuild.AppendValues(sf.columns[col].([]int64), nil) // TODO(twg) 2022/09/28 need to handle null\n\t\t\t\tnewChunk := ibuild.NewArray()\n\t\t\t\tchunks = append(chunks, newChunk)\n\t\t\t}\n\t\t\trecord, err := array.Concatenate(chunks, mem)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tparts = append(parts, record)\n\t\tcase arrow.PrimitiveTypes.Float64:\n\t\t\t// case *arrow.Float64Type:\n\t\t\tif sf.added > 0 {\n\t\t\t\tfbuild := array.NewFloat64Builder(mem)\n\t\t\t\tfbuild.AppendValues(sf.columns[col].([]float64), nil) // TODO(twg) 2022/09/28 need to handle null\n\t\t\t\tnewChunk := fbuild.NewArray()\n\t\t\t\tchunks = append(chunks, newChunk)\n\t\t\t}\n\t\t\trecord, err := array.Concatenate(chunks, mem)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tparts = append(parts, record)\n\t\tcase arrow.BinaryTypes.String:\n\t\t\tif sf.added > 0 {\n\t\t\t\tfbuild := array.NewStringBuilder(mem)\n\t\t\t\tfbuild.AppendValues(sf.columns[col].([]string), nil) // TODO(twg) 2022/09/28 need to handle null\n\t\t\t\tnewChunk := fbuild.NewArray()\n\t\t\t\tchunks = append(chunks, newChunk)\n\t\t\t}\n\t\t\trecord, err := array.Concatenate(chunks, mem)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tparts = append(parts, record)\n\t\tdefault:\n\t\t\tvprint.VV(\"UNKNOWN %T\", sf.schema.Field(col).Type)\n\t\t}\n\t}\n\trec := array.NewRecord(sf.schema, parts, sf.beforeRows+sf.added)\n\ttable := array.NewTableFromRecords(sf.schema, []arrow.Record{rec})\n\n\treturn sf.executor.SaveTable(name, table, mem)\n}\n\n// TODO(twg) 2022/10/03 Not a huge fan of the global variable will look at adding to executor structure\n// when dataframe is fully integrated\nvar (\n\tdataframeShardLocks map[uint64]*sync.Mutex\n\tmuWriteDataframe    sync.Mutex\n)\n\nfunc init() {\n\tdataframeShardLocks = make(map[uint64]*sync.Mutex)\n}\n\nfunc getDataframeWritelock(shard uint64) *sync.Mutex {\n\tmuWriteDataframe.Lock()\n\tdefer muWriteDataframe.Unlock()\n\tlock, ok := dataframeShardLocks[shard]\n\tif ok {\n\t\treturn lock\n\t}\n\tnewLock := sync.Mutex{}\n\tdataframeShardLocks[shard] = &newLock\n\treturn &newLock\n}\n\nfunc (api *API) ApplyDataframeChangeset(ctx context.Context, index string, cs *ChangesetRequest, shard uint64) error {\n\t// TODO(twg) 2022/09/29 need to validate api call\n\tidx := api.Holder().Index(index)\n\n\t// check if dataframe exists\n\tfname := idx.GetDataFramePath(shard)\n\n\t// only 1 shard writer allowed at at time so wait for it to be available\n\tmu := getDataframeWritelock(shard)\n\tmu.Lock()\n\tdefer mu.Unlock()\n\tmem := memory.NewGoAllocator()\n\tshardFile, err := NewShardFile(ctx, fname, mem, api.server.executor)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = shardFile.EnsureSchema(cs)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn shardFile.Process(cs)\n}\n\ntype column struct {\n\tName string\n\tType string\n}\n\nfunc (api *API) GetDataframeSchema(ctx context.Context, indexName string) (interface{}, error) {\n\tidx, err := api.Index(ctx, indexName)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbase := idx.DataframesPath()\n\tdir, _ := os.Open(base)\n\tfiles, _ := dir.Readdir(0)\n\tparts := make([]column, 0)\n\tmem := memory.NewGoAllocator()\n\tfor i := range files {\n\t\tfile := files[i]\n\t\tname := file.Name()\n\t\tif api.server.executor.IsDataframeFile(name) {\n\t\t\t// strip off the parquet extenison\n\t\t\tname = strings.TrimSuffix(name, filepath.Ext(name))\n\t\t\t// read the parquet file and extract the schema\n\t\t\tfname := filepath.Join(base, name)\n\t\t\ttable, err := api.server.executor.getDataTable(ctx, fname, mem)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tfor i := 0; i < int(table.NumCols()); i++ {\n\t\t\t\tcol := table.Column(i)\n\t\t\t\tpart := column{Name: col.Name(), Type: col.DataType().String()}\n\t\t\t\tparts = append(parts, part)\n\t\t\t}\n\t\t\tbreak // only go on first file\n\t\t}\n\t}\n\treturn parts, nil\n}\n"
        },
        {
          "name": "arrow.go",
          "type": "blob",
          "size": 13.7724609375,
          "content": "// Copyright 2021 Molecula Corp. All rights reserved.\npackage pilosa\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/apache/arrow/go/v10/arrow\"\n\t\"github.com/apache/arrow/go/v10/arrow/array\"\n\t\"github.com/apache/arrow/go/v10/arrow/ipc\"\n\t\"github.com/apache/arrow/go/v10/arrow/memory\"\n\t\"github.com/apache/arrow/go/v10/parquet\"\n\t\"github.com/apache/arrow/go/v10/parquet/file\"\n\t\"github.com/apache/arrow/go/v10/parquet/pqarrow\"\n\t\"github.com/featurebasedb/featurebase/v3/pql\"\n\t\"github.com/featurebasedb/featurebase/v3/tracing\"\n\t\"github.com/gomem/gomem/pkg/dataframe\"\n\t\"github.com/pkg/errors\"\n)\n\n/*\nThe function Arrow provides filtered access to the raw values stored in the dataframe.\nIf Arrow is just provided a bitmap filter, such as ConstRow or any Bitmap Operation,\nall the values associated with each column are returned.  This set can be limited with\nthe addition of the header parameter\nExample:\nArrow(ConstRow(columns=[2,4,6]),header=[\"fval\"])\n*/\n\n// executeApply executes a Arrow() call.\nfunc (e *executor) executeArrow(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (arrow.Table, error) {\n\tif !e.dataframeEnabled {\n\t\treturn nil, errors.New(\"Dataframe support not enabled\")\n\t}\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"Executor.executeArrow\")\n\tdefer span.Finish()\n\tif len(c.Children) > 1 {\n\t\treturn nil, errors.New(\"Apply() only accepts a single bitmap input filter\")\n\t}\n\tvar columnFilter []string\n\tif cols, ok := c.Args[\"header\"].([]interface{}); ok {\n\t\tcolumnFilter = make([]string, 0, len(cols))\n\t\tfor _, v := range cols {\n\t\t\tcolumnFilter = append(columnFilter, v.(string))\n\t\t}\n\t}\n\tmapcounter := 0\n\treducecounter := 0\n\tpool := memory.NewGoAllocator() // TODO(twg) 2022/09/01 singledton?\n\t// Execute calls in bulk on each remote node and merge.\n\tmu := &sync.Mutex{}\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\tmu.Lock()\n\t\tmapcounter++\n\t\tmu.Unlock()\n\t\treturn e.executeArrowShard(ctx, qcx, index, c, shard, pool, columnFilter)\n\t}\n\ttables := make([]*BasicTable, 0)\n\n\treduceFn := func(ctx context.Context, prev, v interface{}) interface{} {\n\t\tmu.Lock()\n\t\treducecounter++\n\t\tmu.Unlock()\n\t\tif v == nil {\n\t\t\treturn prev\n\t\t}\n\t\tswitch t := v.(type) {\n\t\tcase *BasicTable:\n\n\t\t\tif t.resolver != nil {\n\t\t\t\tmu.Lock()\n\t\t\t\ttables = append(tables, t)\n\t\t\t\tmu.Unlock()\n\t\t\t}\n\t\tcase arrow.Table:\n\t\t\tif t.NumRows() > 0 {\n\t\t\t\tbt := BasicTableFromArrow(t, pool)\n\t\t\t\tmu.Lock()\n\t\t\t\ttables = append(tables, bt)\n\t\t\t\tmu.Unlock()\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\n\t_, err := e.mapReduce(ctx, index, shards, c, opt, mapFn, reduceFn)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif len(tables) == 0 {\n\t\treturn &BasicTable{name: \"empty\"}, nil\n\t}\n\ttbl := Concat(tables[0].Schema(), tables, pool)\n\tr := dataframe.NewChunkResolver(tbl.Column(0))\n\treturn &BasicTable{resolver: &r, table: tbl}, nil\n}\n\ntype BasicTable struct {\n\tresolver dataframe.Resolver\n\ttable    arrow.Table\n\tfiltered bool\n\tname     string\n}\n\nfunc (st *BasicTable) Name() string {\n\treturn st.name\n}\n\nfunc (st *BasicTable) Schema() *arrow.Schema {\n\tif st.table != nil {\n\t\treturn st.table.Schema()\n\t}\n\treturn &arrow.Schema{}\n}\n\nfunc (st *BasicTable) IsFiltered() bool {\n\treturn st.filtered\n}\n\nfunc (st *BasicTable) NumRows() int64 {\n\tif st.resolver == nil {\n\t\treturn 0\n\t}\n\treturn int64(st.resolver.NumRows())\n}\n\nfunc (st *BasicTable) NumCols() int64 {\n\tif st.table != nil {\n\t\treturn st.table.NumCols()\n\t}\n\treturn 0\n}\n\nfunc (st *BasicTable) Column(i int) *arrow.Column {\n\tif st.table != nil {\n\t\treturn st.table.Column(i)\n\t}\n\treturn nil\n}\n\nfunc (st *BasicTable) Retain() {\n\tif st.table != nil {\n\t\tst.table.Retain()\n\t}\n}\n\nfunc (st *BasicTable) Release() {\n\tif st.table != nil {\n\t\tst.table.Retain()\n\t}\n}\n\nfunc (st *BasicTable) Get(column, row int) interface{} {\n\tfield := st.Schema().Field(column)\n\tc, i := st.resolver.Resolve(row)\n\tnullable := field.Nullable\n\n\tchunk := st.Column(column).Data().Chunk(c)\n\t// TODO(twg) 2023/01/26 potential NULL support?\n\tif nullable && chunk.IsNull(i) {\n\t\treturn nil\n\t}\n\tswitch field.Type.(type) {\n\tcase *arrow.BooleanType:\n\t\treturn chunk.(*array.Boolean).Value(i)\n\tcase *arrow.Int8Type:\n\t\tv := chunk.(*array.Int8).Int8Values()\n\t\treturn int64(v[i])\n\tcase *arrow.Int16Type:\n\t\tv := chunk.(*array.Int16).Int16Values()\n\t\treturn int64(v[i])\n\tcase *arrow.Int32Type:\n\t\tv := chunk.(*array.Int32).Int32Values()\n\t\treturn int64(v[i])\n\tcase *arrow.Int64Type:\n\t\tv := chunk.(*array.Int64).Int64Values()\n\t\treturn int64(v[i])\n\tcase *arrow.Uint8Type:\n\t\tv := chunk.(*array.Uint8).Uint8Values()\n\t\treturn uint64(v[i])\n\tcase *arrow.Uint16Type:\n\t\tv := chunk.(*array.Uint16).Uint16Values()\n\t\treturn uint64(v[i])\n\tcase *arrow.Uint32Type:\n\t\tv := chunk.(*array.Uint32).Uint32Values()\n\t\treturn uint64(v[i])\n\tcase *arrow.Uint64Type:\n\t\tv := chunk.(*array.Uint64).Uint64Values()\n\t\treturn v[i]\n\tcase *arrow.Float32Type:\n\t\tv := chunk.(*array.Float32).Float32Values()\n\t\treturn float64(v[i])\n\tcase *arrow.Float64Type:\n\t\tv := chunk.(*array.Float64).Float64Values()\n\t\treturn v[i]\n\tcase *arrow.StringType:\n\t\treturn chunk.(*array.String).Value(i)\n\t}\n\treturn 0\n}\n\nfunc builderFrom(mem memory.Allocator, dt arrow.DataType, size int64) array.Builder {\n\tvar bldr array.Builder\n\tswitch dt := dt.(type) {\n\tcase *arrow.BooleanType:\n\t\tbldr = array.NewBooleanBuilder(mem)\n\tcase *arrow.Int8Type:\n\t\tbldr = array.NewInt8Builder(mem)\n\tcase *arrow.Int16Type:\n\t\tbldr = array.NewInt16Builder(mem)\n\tcase *arrow.Int32Type:\n\t\tbldr = array.NewInt32Builder(mem)\n\tcase *arrow.Int64Type:\n\t\tbldr = array.NewInt64Builder(mem)\n\tcase *arrow.Uint8Type:\n\t\tbldr = array.NewUint8Builder(mem)\n\tcase *arrow.Uint16Type:\n\t\tbldr = array.NewUint16Builder(mem)\n\tcase *arrow.Uint32Type:\n\t\tbldr = array.NewUint32Builder(mem)\n\tcase *arrow.Uint64Type:\n\t\tbldr = array.NewUint64Builder(mem)\n\tcase *arrow.Float32Type:\n\t\tbldr = array.NewFloat32Builder(mem)\n\tcase *arrow.Float64Type:\n\t\tbldr = array.NewFloat64Builder(mem)\n\tcase *arrow.StringType:\n\t\tbldr = array.NewStringBuilder(mem)\n\tdefault:\n\t\tpanic(fmt.Errorf(\"builderFrom: invalid Arrow type %v\", dt))\n\t}\n\tbldr.Reserve(int(size))\n\treturn bldr\n}\n\nfunc appendData(bldr array.Builder, v interface{}) {\n\tswitch bldr := bldr.(type) {\n\tcase *array.BooleanBuilder:\n\t\tbldr.Append(v.(bool))\n\tcase *array.Int8Builder:\n\t\tbldr.Append(v.(int8))\n\tcase *array.Int16Builder:\n\t\tbldr.Append(v.(int16))\n\tcase *array.Int32Builder:\n\t\tbldr.Append(v.(int32))\n\tcase *array.Int64Builder:\n\t\tbldr.Append(v.(int64))\n\tcase *array.Uint8Builder:\n\t\tbldr.Append(v.(uint8))\n\tcase *array.Uint16Builder:\n\t\tbldr.Append(v.(uint16))\n\tcase *array.Uint32Builder:\n\t\tbldr.Append(v.(uint32))\n\tcase *array.Uint64Builder:\n\t\tbldr.Append(v.(uint64))\n\tcase *array.Float32Builder:\n\t\tbldr.Append(v.(float32))\n\tcase *array.Float64Builder:\n\t\tbldr.Append(v.(float64))\n\tcase *array.StringBuilder:\n\t\tbldr.Append(v.(string))\n\tdefault:\n\t\tpanic(fmt.Errorf(\"appendData: invalid Arrow builder type %T\", bldr))\n\t}\n}\n\nfunc Concat(schema *arrow.Schema, tables []*BasicTable, mem memory.Allocator) arrow.Table {\n\tif len(tables) == 1 {\n\t\tif !tables[0].IsFiltered() {\n\t\t\treturn tables[0]\n\t\t}\n\t}\n\tcols := make([]arrow.Column, len(schema.Fields()))\n\n\tdefer func(cols []arrow.Column) {\n\t\tfor i := range cols {\n\t\t\tcols[i].Release()\n\t\t}\n\t}(cols)\n\tsz := 0\n\tfor i := range tables {\n\t\tsz += int(tables[i].NumRows())\n\t}\n\tfor i := range cols {\n\t\tfield := schema.Field(i)\n\t\tarrs := make([]arrow.Array, 0)\n\t\tbuilder := builderFrom(mem, field.Type, int64(sz))\n\t\tfor t := range tables {\n\t\t\ttable := tables[t]\n\t\t\tif table.IsFiltered() {\n\t\t\t\tfor row := 0; row < int(table.NumRows()); row++ {\n\t\t\t\t\tv := table.Get(i, row)\n\t\t\t\t\tappendData(builder, v)\n\t\t\t\t}\n\t\t\t\tarrs = append(arrs, builder.NewArray())\n\n\t\t\t} else {\n\t\t\t\tparts := table.Column(i).Data()\n\t\t\t\tarrs = append(arrs, parts.Chunks()...)\n\t\t\t}\n\t\t}\n\t\tchunk := arrow.NewChunked(field.Type, arrs)\n\t\tcols[i] = *arrow.NewColumn(field, chunk)\n\t\tchunk.Release()\n\t}\n\treturn array.NewTable(schema, cols, -1)\n}\n\nfunc (st *BasicTable) MarshalJSON() ([]byte, error) {\n\tresults := make(map[string]interface{})\n\tn := 0\n\tif st.table != nil {\n\t\tn = int(st.table.NumCols())\n\t}\n\tfor b := 0; b < n; b++ {\n\t\tcol := st.table.Column(b)\n\t\tresult := make([]interface{}, st.resolver.NumRows())\n\t\tfor n := st.resolver.NumRows() - 1; n >= 0; n-- {\n\t\t\tv := st.Get(b, n)\n\t\t\tresult[n] = v\n\n\t\t}\n\t\tresults[col.Name()] = result\n\t}\n\treturn json.Marshal(results)\n}\n\nfunc BasicTableFromArrow(table arrow.Table, mem memory.Allocator) *BasicTable {\n\tcol := table.Column(0)\n\tr := dataframe.NewChunkResolver(col)\n\treturn &BasicTable{resolver: &r, table: table}\n}\n\nfunc filterColumns(filters []string, table arrow.Table) arrow.Table {\n\tfilters = append(filters, \"_ID\")\n\tschema := table.Schema()\n\t// TODO(twg) 2022/11/09 add glob support\n\tallFields := schema.Fields()\n\tin := func(key string) bool {\n\t\tfor _, v := range filters {\n\t\t\tif v == key {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t\treturn false\n\t}\n\tcols := make([]arrow.Column, 0)\n\tfields := make([]arrow.Field, 0)\n\tfor i := range allFields {\n\t\tfield := allFields[i]\n\t\tif in(field.Name) {\n\t\t\tcols = append(cols, *table.Column(i))\n\t\t\tfields = append(fields, field)\n\t\t}\n\t}\n\n\tfilterdSchema := arrow.NewSchema(fields, nil) // TODO(twg) 2022/11/09 handle meta:w\n\treturn array.NewTable(filterdSchema, cols, table.NumRows())\n}\n\nfunc (e *executor) executeArrowShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64, pool memory.Allocator, columnFilter []string) (*BasicTable, error) {\n\tname := fmt.Sprintf(\"a. %v\", shard)\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"Executor.executeArrowShard\")\n\tdefer span.Finish()\n\n\tvar filter *Row\n\tif len(c.Children) == 1 {\n\t\trow, err := e.executeBitmapCallShard(ctx, qcx, index, c.Children[0], shard)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfilter = row\n\t\tif !filter.Any() {\n\t\t\t// no need to actuall run the query for its not operating against any values\n\t\t\treturn &BasicTable{name: name}, nil\n\t\t}\n\t}\n\t//\n\tids := filter.ShardColumns() // needs to be shard columns\n\t// Fetch index.\n\tidx := e.Holder.Index(index)\n\tif idx == nil {\n\t\treturn nil, newNotFoundError(ErrIndexNotFound, index)\n\t}\n\n\tfname := idx.GetDataFramePath(shard)\n\n\tif !e.dataFrameExists(fname) {\n\t\treturn &BasicTable{name: name}, nil\n\t}\n\n\ttable, err := e.getDataTable(ctx, fname, pool)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"arrow readTableParquet\")\n\t}\n\tdefer table.Release()\n\tif len(columnFilter) > 0 {\n\t\ttable = filterColumns(columnFilter, table)\n\t}\n\tdf, err := dataframe.NewDataFrameFromTable(pool, table)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"arrow NewDataFromTable\")\n\t}\n\tp := dataframe.NewChunkResolver(df.ColumnAt(0))\n\tvar resolver dataframe.Resolver\n\tresolver = &p\n\tif filter != nil {\n\t\tif len(ids) == 0 {\n\t\t\treturn &BasicTable{name: name}, nil\n\t\t}\n\t\tresolver, err = filterDataframe(resolver, pool, ids)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"filtering dataframe\")\n\t\t}\n\t}\n\ttable.Retain()\n\treturn &BasicTable{resolver: resolver, table: table, filtered: filter != nil, name: name}, nil\n}\n\nfunc (e *executor) dataFrameExists(fname string) bool {\n\tif e.typeIsParquet() {\n\t\tif _, err := os.Stat(fname + \".parquet\"); os.IsNotExist(err) {\n\t\t\treturn false\n\t\t}\n\t\treturn true\n\t}\n\tif _, err := os.Stat(fname + \".arrow\"); os.IsNotExist(err) {\n\t\treturn false\n\t}\n\treturn true\n}\n\nfunc (e *executor) getDataTable(ctx context.Context, fname string, mem memory.Allocator) (arrow.Table, error) {\n\tif e.typeIsParquet() {\n\t\ttable, err := readTableParquetCtx(ctx, fname, mem)\n\t\treturn table, err\n\t}\n\treturn readTableArrow(fname, mem)\n}\n\nfunc (e *executor) typeIsParquet() bool {\n\treturn e.datafameUseParquet\n}\n\nfunc (e *executor) IsDataframeFile(name string) bool {\n\tif e.typeIsParquet() {\n\t\treturn strings.HasSuffix(name, \".parquet\")\n\t}\n\treturn strings.HasSuffix(name, \".arrow\")\n}\n\nfunc (e *executor) SaveTable(name string, table arrow.Table, mem memory.Allocator) error {\n\tif e.typeIsParquet() {\n\t\treturn writeTableParquet(table, name)\n\t}\n\treturn writeTableArrow(table, name, mem)\n}\n\nfunc (e *executor) TableExtension() string {\n\tif e.typeIsParquet() {\n\t\treturn \".parquet\"\n\t}\n\treturn \".arrow\"\n}\n\nfunc readTableArrow(filename string, mem memory.Allocator) (arrow.Table, error) {\n\tr, err := os.Open(filename + \".arrow\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\trr, err := ipc.NewFileReader(r, ipc.WithAllocator(mem))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer rr.Close()\n\trecords := make([]arrow.Record, rr.NumRecords())\n\ti := 0\n\tfor {\n\t\trec, err := rr.Read()\n\t\tif err == io.EOF {\n\t\t\tbreak\n\t\t} else if err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\trecords[i] = rec\n\t\ti++\n\t}\n\trecords = records[:i]\n\ttable := array.NewTableFromRecords(rr.Schema(), records)\n\treturn table, nil\n}\n\nfunc readTableParquetCtx(ctx context.Context, filename string, mem memory.Allocator) (arrow.Table, error) {\n\tr, err := os.Open(filename + \".parquet\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer r.Close()\n\n\tpf, err := file.NewParquetReader(r)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treader, err := pqarrow.NewFileReader(pf, pqarrow.ArrowReadProperties{}, mem)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn reader.ReadTable(ctx)\n}\n\nfunc writeTableParquet(table arrow.Table, filename string) error {\n\tf, err := os.Create(filename + \".parquet\")\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer f.Close()\n\tprops := parquet.NewWriterProperties(parquet.WithDictionaryDefault(false))\n\tarrProps := pqarrow.DefaultWriterProps()\n\tchunkSize := 10 * 1024 * 1024\n\terr = pqarrow.WriteTable(table, f, int64(chunkSize), props, arrProps)\n\tif err != nil {\n\t\treturn err\n\t}\n\tf.Sync()\n\treturn nil\n}\n\nfunc writeTableArrow(table arrow.Table, filename string, mem memory.Allocator) error {\n\tf, err := os.Create(filename + \".arrow\")\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer f.Close()\n\twriter, err := ipc.NewFileWriter(f, ipc.WithAllocator(mem), ipc.WithSchema(table.Schema()))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tchunkSize := int64(0)\n\ttr := array.NewTableReader(table, chunkSize)\n\tdefer tr.Release()\n\tn := 0\n\tfor tr.Next() {\n\t\tarec := tr.Record()\n\t\terr = writer.Write(arec)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tn++\n\t}\n\terr = writer.Close()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tf.Sync()\n\treturn nil\n}\n"
        },
        {
          "name": "arrow_test.go",
          "type": "blob",
          "size": 1.357421875,
          "content": "// Copyright 2021 Molecula Corp. All rights reserved.\npackage pilosa\n\nimport (\n\t\"context\"\n\t\"encoding/hex\"\n\t\"math/rand\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"testing\"\n\n\t\"github.com/apache/arrow/go/v10/arrow\"\n\t\"github.com/apache/arrow/go/v10/arrow/array\"\n\t\"github.com/apache/arrow/go/v10/arrow/memory\"\n)\n\nfunc TempFileName(prefix string) string {\n\trandBytes := make([]byte, 16)\n\trand.Read(randBytes)\n\treturn filepath.Join(os.TempDir(), prefix+hex.EncodeToString(randBytes))\n}\n\nfunc Test_TableParquet(t *testing.T) {\n\t// create a arrow table\n\tschema := arrow.NewSchema(\n\t\t[]arrow.Field{\n\t\t\t{Name: \"num\", Type: arrow.PrimitiveTypes.Float64},\n\t\t},\n\t\tnil, // no metadata\n\t)\n\tmem := memory.NewGoAllocator()\n\tb := array.NewRecordBuilder(mem, schema)\n\tdefer b.Release()\n\tb.Field(0).(*array.Float64Builder).AppendValues([]float64{1.0, 1.5, 2.0}, nil)\n\ttable := array.NewTableFromRecords(schema, []arrow.Record{b.NewRecord()})\n\tdefer table.Release()\n\tfileName := TempFileName(\"pq-\")\n\t// save it as  a parquet file\n\terr := writeTableParquet(table, fileName)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer os.Remove(fileName)\n\n\t// read it back in and compare the result\n\tgot, err := readTableParquetCtx(context.Background(), fileName, mem)\n\tif err != nil {\n\t\tt.Fatalf(\"readTableParquetCtx() error = %v\", err)\n\t}\n\tif got.NumCols() != table.NumCols() {\n\t\tt.Errorf(\"got:%v expected:%v\", got.NumCols(), table.NumCols())\n\t}\n}\n"
        },
        {
          "name": "audit.go",
          "type": "blob",
          "size": 0.294921875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"github.com/featurebasedb/featurebase/v3/testhook\"\n)\n\nvar NewAuditor func() testhook.Auditor = NewNopAuditor\n\nfunc NewNopAuditor() testhook.Auditor {\n\treturn testhook.NewNopAuditor()\n}\n"
        },
        {
          "name": "audit_internal_test.go",
          "type": "blob",
          "size": 1.087890625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"fmt\"\n\t\"reflect\"\n\n\t\"github.com/featurebasedb/featurebase/v3/testhook\"\n)\n\n// These audit hooks are desireable during testing, but not in\n// production.\ntype auditorViewHooks struct{}\ntype auditorFragmentHooks struct{}\n\n// static type checks\nvar _ testhook.RegistryHookLive = &auditorViewHooks{}\nvar _ testhook.RegistryHookLive = &auditorFragmentHooks{}\n\nfunc (*auditorViewHooks) Live(o interface{}, entry *testhook.RegistryEntry) error {\n\tif entry != nil && entry.OpenCount != 0 {\n\t\treturn fmt.Errorf(\"view %s still open\", o.(*view).name)\n\t}\n\treturn nil\n}\n\nfunc (*auditorFragmentHooks) Live(o interface{}, entry *testhook.RegistryEntry) error {\n\tif entry != nil && entry.OpenCount != 0 {\n\t\treturn fmt.Errorf(\"fragment %s still open\", o.(*fragment).path())\n\t}\n\treturn nil\n}\n\nfunc GetInternalTestHooks() testhook.RegistryHooks {\n\treturn map[reflect.Type]testhook.RegistryHook{\n\t\treflect.TypeOf((*view)(nil)):     &auditorViewHooks{},\n\t\treflect.TypeOf((*fragment)(nil)): &auditorFragmentHooks{},\n\t}\n}\n"
        },
        {
          "name": "audit_test.go",
          "type": "blob",
          "size": 2.61328125,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa_test\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"reflect\"\n\n\t\"github.com/featurebasedb/featurebase/v3\"\n\t\"github.com/featurebasedb/featurebase/v3/testhook\"\n)\n\n// AuditLeaksOn is a global switch to turn on resource\n// leak checking at the end of a test run.\nvar AuditLeaksOn = true\n\n// for tests, we use a single shared auditor used by all of the holders.\nvar globalTestAuditor = testhook.NewVerifyCloseAuditor(testHooks)\n\n// These audit hooks are desireable during testing, but not in\n// production.\ntype auditorIndexHooks struct{}\ntype auditorFieldHooks struct{}\ntype auditorHolderHooks struct{}\n\n// static type checking\nvar _ testhook.RegistryHookLive = &auditorIndexHooks{}\nvar _ testhook.RegistryHookLive = &auditorFieldHooks{}\nvar _ testhook.RegistryHookPostDestroy = &auditorHolderHooks{}\nvar _ testhook.RegistryHookLive = &auditorHolderHooks{}\n\nvar testHooks = map[reflect.Type]testhook.RegistryHook{\n\treflect.TypeOf((*pilosa.Index)(nil)):  &auditorIndexHooks{},\n\treflect.TypeOf((*pilosa.Field)(nil)):  &auditorFieldHooks{},\n\treflect.TypeOf((*pilosa.Holder)(nil)): &auditorHolderHooks{},\n}\n\nfunc init() {\n\tif !AuditLeaksOn {\n\t\treturn\n\t}\n\tfor k, v := range pilosa.GetInternalTestHooks() {\n\t\ttestHooks[k] = v\n\t}\n\ttesthook.RegisterPreTestHook(func() error {\n\t\tpilosa.NewAuditor = NewTestAuditor\n\t\treturn nil\n\t})\n\ttesthook.RegisterPostTestHook(func() error {\n\t\terr, errs := globalTestAuditor.FinalCheck()\n\t\tif err != nil {\n\t\t\tfor i, e := range errs {\n\t\t\t\tfmt.Fprintf(os.Stderr, \"[%d]: %v\\n\", i, e)\n\t\t\t}\n\t\t}\n\t\treturn err\n\t})\n}\n\nfunc NewTestAuditor() testhook.Auditor {\n\treturn globalTestAuditor\n}\n\nfunc (*auditorIndexHooks) Live(o interface{}, entry *testhook.RegistryEntry) error {\n\tif entry != nil && entry.OpenCount != 0 {\n\t\treturn fmt.Errorf(\"index %s still open\", o.(*pilosa.Index).Name())\n\t}\n\treturn nil\n}\n\nfunc (*auditorFieldHooks) Live(o interface{}, entry *testhook.RegistryEntry) error {\n\tif entry != nil && entry.OpenCount != 0 {\n\t\treturn fmt.Errorf(\"field %s still open\", o.(*pilosa.Field).Name())\n\t}\n\treturn nil\n}\n\nfunc (*auditorHolderHooks) WasDestroyed(o interface{}, kv testhook.KV, ent *testhook.RegistryEntry, err error) error {\n\tpath := o.(*pilosa.Holder).Path()\n\tif path == \"\" {\n\t\tfmt.Fprintf(os.Stderr, \"OOPS: trying to destroy a holder with no path! created: %s\\n\",\n\t\t\tent.Stack)\n\t} else {\n\t\tos.RemoveAll(o.(*pilosa.Holder).Path())\n\t}\n\treturn err\n}\n\nfunc (*auditorHolderHooks) Live(o interface{}, entry *testhook.RegistryEntry) error {\n\tif entry != nil && entry.OpenCount != 0 {\n\t\treturn fmt.Errorf(\"holder %s still open\", o.(*pilosa.Holder).Path())\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "authn",
          "type": "tree",
          "content": null
        },
        {
          "name": "authz",
          "type": "tree",
          "content": null
        },
        {
          "name": "batch",
          "type": "tree",
          "content": null
        },
        {
          "name": "broadcast.go",
          "type": "blob",
          "size": 4.7763671875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/featurebasedb/featurebase/v3/disco\"\n\t\"github.com/pkg/errors\"\n)\n\n// Serializer is an interface for serializing pilosa types to bytes and back.\ntype Serializer interface {\n\tMarshal(Message) ([]byte, error)\n\tUnmarshal([]byte, Message) error\n}\n\n// NopSerializer represents a Serializer that doesn't do anything.\nvar NopSerializer Serializer = &nopSerializer{}\n\ntype nopSerializer struct{}\n\n// Marshal is a no-op implementation of Serializer Marshal method.\nfunc (*nopSerializer) Marshal(Message) ([]byte, error) { return nil, nil }\n\n// Unmarshal is a no-op implementation of Serializer Unmarshal method.\nfunc (*nopSerializer) Unmarshal([]byte, Message) error { return nil }\n\n// broadcaster is an interface for broadcasting messages.\ntype broadcaster interface {\n\tSendSync(Message) error\n\tSendAsync(Message) error\n\tSendTo(*disco.Node, Message) error\n}\n\n// Message is the interface implemented by all core pilosa types which can be serialized to messages.\n// TODO add at least a single \"isMessage()\" method.\ntype Message interface{}\n\n// NopBroadcaster represents a Broadcaster that doesn't do anything.\nvar NopBroadcaster broadcaster = &nopBroadcaster{}\n\ntype nopBroadcaster struct{}\n\n// SendSync A no-op implementation of Broadcaster SendSync method.\nfunc (nopBroadcaster) SendSync(Message) error { return nil }\n\n// SendAsync A no-op implementation of Broadcaster SendAsync method.\nfunc (nopBroadcaster) SendAsync(Message) error { return nil }\n\n// SendTo is a no-op implementation of Broadcaster SendTo method.\nfunc (nopBroadcaster) SendTo(*disco.Node, Message) error { return nil }\n\n// Broadcast message types.\nconst (\n\tmessageTypeCreateShard = iota\n\tmessageTypeCreateIndex\n\tmessageTypeDeleteIndex\n\tmessageTypeCreateField\n\tmessageTypeDeleteField\n\tmessageTypeCreateView\n\tmessageTypeDeleteView\n\tmessageTypeClusterStatus\n\tmessageTypeUNUSED0 // used to be ResizeInstruction\n\tmessageTypeUNUSED1 // used to be ResizeInstructionComplete\n\tmessageTypeNodeState\n\tmessageTypeRecalculateCaches\n\tmessageTypeLoadSchemaMessage\n\tmessageTypeNodeEvent\n\tmessageTypeNodeStatus\n\tmessageTypeTransaction\n\tmessageTypeUNUSED2 // used to be ResizeNodeMessage\n\tmessageTypeUNUSED3 // used to be ResizeAbortMessage\n\tmessageTypeUpdateField\n\tmessageTypeDeleteDataframe\n)\n\n// MarshalInternalMessage serializes the pilosa message and adds pilosa internal\n// type info which is used by the internal messaging stuff.\nfunc MarshalInternalMessage(m Message, s Serializer) ([]byte, error) {\n\ttyp := getMessageType(m)\n\tbuf, err := s.Marshal(m)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"marshaling\")\n\t}\n\treturn append([]byte{typ}, buf...), nil\n}\n\nfunc getMessage(typ byte) Message {\n\tswitch typ {\n\tcase messageTypeCreateShard:\n\t\treturn &CreateShardMessage{}\n\tcase messageTypeCreateIndex:\n\t\treturn &CreateIndexMessage{}\n\tcase messageTypeDeleteIndex:\n\t\treturn &DeleteIndexMessage{}\n\tcase messageTypeCreateField:\n\t\treturn &CreateFieldMessage{}\n\tcase messageTypeDeleteField:\n\t\treturn &DeleteFieldMessage{}\n\tcase messageTypeCreateView:\n\t\treturn &CreateViewMessage{}\n\tcase messageTypeDeleteView:\n\t\treturn &DeleteViewMessage{}\n\tcase messageTypeClusterStatus:\n\t\treturn &ClusterStatus{}\n\tcase messageTypeNodeState:\n\t\treturn &NodeStateMessage{}\n\tcase messageTypeRecalculateCaches:\n\t\treturn &RecalculateCaches{}\n\tcase messageTypeLoadSchemaMessage:\n\t\treturn &LoadSchemaMessage{}\n\tcase messageTypeNodeEvent:\n\t\treturn &NodeEvent{}\n\tcase messageTypeNodeStatus:\n\t\treturn &NodeStatus{}\n\tcase messageTypeTransaction:\n\t\treturn &TransactionMessage{}\n\tcase messageTypeUpdateField:\n\t\treturn &UpdateFieldMessage{}\n\tcase messageTypeDeleteDataframe:\n\t\treturn &DeleteDataframeMessage{}\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"unknown message type %d\", typ))\n\t}\n}\n\nfunc getMessageType(m Message) byte {\n\tswitch m.(type) {\n\tcase *CreateShardMessage:\n\t\treturn messageTypeCreateShard\n\tcase *CreateIndexMessage:\n\t\treturn messageTypeCreateIndex\n\tcase *DeleteIndexMessage:\n\t\treturn messageTypeDeleteIndex\n\tcase *CreateFieldMessage:\n\t\treturn messageTypeCreateField\n\tcase *DeleteFieldMessage:\n\t\treturn messageTypeDeleteField\n\tcase *CreateViewMessage:\n\t\treturn messageTypeCreateView\n\tcase *DeleteViewMessage:\n\t\treturn messageTypeDeleteView\n\tcase *ClusterStatus:\n\t\treturn messageTypeClusterStatus\n\tcase *NodeStateMessage:\n\t\treturn messageTypeNodeState\n\tcase *RecalculateCaches:\n\t\treturn messageTypeRecalculateCaches\n\tcase *LoadSchemaMessage:\n\t\treturn messageTypeLoadSchemaMessage\n\tcase *NodeEvent:\n\t\treturn messageTypeNodeEvent\n\tcase *NodeStatus:\n\t\treturn messageTypeNodeStatus\n\tcase *TransactionMessage:\n\t\treturn messageTypeTransaction\n\tcase *UpdateFieldMessage:\n\t\treturn messageTypeUpdateField\n\tcase *DeleteDataframeMessage:\n\t\treturn messageTypeDeleteDataframe\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"don't have type for message %#v\", m))\n\t}\n}\n"
        },
        {
          "name": "bsi.go",
          "type": "blob",
          "size": 6.892578125,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"math/bits\"\n\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n)\n\n// BSIData contains BSI-structured data.\ntype BSIData []*Row\n\n// PivotDescending loops over nonzero BSI values in descending order.\n// For each value, the provided function is called with the value and a slice of the associated columns.\n// If limit or offset are not-nil, they will be applied.\n// Applying a limit or offset may modify the pointed-to value.\nfunc (bsi BSIData) PivotDescending(filter *Row, branch uint64, limit, offset *uint64, fn func(uint64, ...uint64)) {\n\t// This \"pivot\" algorithm works by treating the BSI data as a tree.\n\t// Each branch of this tree corresponds to a power-of-2-sized range of BSI values.\n\t// Each range is subdivided into 2 ranges of half size, which form lower branches.\n\t// Eventually, a range of width 1 cannot be subdivided and forms a leaf.\n\t// At each branch and leaf, there is a bitmap of all columns within the corresponding range.\n\t// The lower branches are formed as a difference or intersect of the upper branch's bitmap with the BSI bit that subdivides the range.\n\t// This function uses a depth-first search over this virtual tree.\n\n\tswitch {\n\tcase !filter.Any():\n\t\t// There are no remaining data.\n\n\tcase offset != nil && *offset >= filter.Count():\n\t\t// Skip this entire branch.\n\t\t*offset -= filter.Count()\n\n\tcase limit != nil && *limit == 0:\n\t\t// The limit has been reached.\n\t\t// No more data is necessary.\n\n\tcase len(bsi) == 0:\n\t\t// This is a leaf node.\n\t\tcols := filter.Columns()\n\t\tif offset != nil {\n\t\t\tcols = cols[*offset:]\n\t\t\t*offset = 0\n\t\t}\n\t\tif limit != nil {\n\t\t\tif *limit < uint64(len(cols)) {\n\t\t\t\tcols = cols[:*limit]\n\t\t\t}\n\t\t\t*limit -= uint64(len(cols))\n\t\t}\n\t\tfn(branch, cols...)\n\n\tdefault:\n\t\t// Pivot over the highest bit.\n\t\tupperBranch, lowerBranch := branch|(1<<uint(len(bsi)-1)), branch\n\t\tsplitBit := bsi[len(bsi)-1]\n\t\tlowerBits := bsi[:len(bsi)-1]\n\t\tlowerBits.PivotDescending(filter.Intersect(splitBit), upperBranch, limit, offset, fn)\n\t\tlowerBits.PivotDescending(filter.Difference(splitBit), lowerBranch, limit, offset, fn)\n\t}\n}\n\n/*\n// distribution generates a BSI histogram for the input.\n// TODO: I forgot what I was going to use this for.\n// Could probbably use this for:\n// - quartile queries\n// - TopN on int\nfunc (bsi bsiData) distribution(filter *Row) bsiData {\n\tvar dist bsiData\n\tbsi.PivotDescending(filter, 0, nil, nil, func(count uint64, values ...uint64) {\n\t\tdist.insert(count, uint64(len(values)))\n\t})\n\treturn dist\n}\n*/\n\nvar placeholderBitmap = roaring.NewBitmap()\n\n// AddBSI adds two BSI bitmaps together.\n// It does not handle sign and has no concept of overflow.\nfunc AddBSI(x, y BSIData) BSIData {\n\t// Accumulate row segments.\n\tsegments := make([][]RowSegment, len(x)+len(y))\n\txsegs, ysegs := segments[:len(x)], segments[len(x):]\n\tfor i, r := range x {\n\t\txsegs[i] = r.Segments\n\t}\n\tfor i, r := range y {\n\t\tysegs[i] = r.Segments\n\t}\n\n\tvar dst BSIData\n\tvar xbitmaps, ybitmaps []*roaring.Bitmap\n\tfor {\n\t\t// Find the next shard.\n\t\tnext := ^uint64(0)\n\t\tfor _, s := range segments {\n\t\t\tif len(s) == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tshard := s[0].shard\n\t\t\tif shard < next {\n\t\t\t\tnext = shard\n\t\t\t}\n\t\t}\n\t\tif next == ^uint64(0) {\n\t\t\t// There are no remaining shards.\n\t\t\tbreak\n\t\t}\n\n\t\t// Accumulate bitmaps for this shard.\n\t\txbitmaps, ybitmaps = xbitmaps[:0], ybitmaps[:0]\n\t\tfor i, segs := range xsegs {\n\t\t\tif len(segs) == 0 || segs[0].shard != next {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\txsegs[i] = segs[1:]\n\t\t\tbm := segs[0].data\n\t\t\tif !bm.Any() {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfor len(xbitmaps) < i {\n\t\t\t\txbitmaps = append(xbitmaps, placeholderBitmap)\n\t\t\t}\n\t\t\txbitmaps = append(xbitmaps, bm)\n\t\t}\n\t\tfor i, segs := range ysegs {\n\t\t\tif len(segs) == 0 || segs[0].shard != next {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tysegs[i] = segs[1:]\n\t\t\tbm := segs[0].data\n\t\t\tif !bm.Any() {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfor len(ybitmaps) < i {\n\t\t\t\tybitmaps = append(ybitmaps, placeholderBitmap)\n\t\t\t}\n\t\t\tybitmaps = append(ybitmaps, bm)\n\t\t}\n\n\t\t// Add the shard values together.\n\t\tvar out []*roaring.Bitmap\n\t\tswitch {\n\t\tcase len(xbitmaps) == 0:\n\t\t\t// There are no values in x.\n\t\t\tout = ybitmaps\n\t\tcase len(ybitmaps) == 0:\n\t\t\t// There are no values in y.\n\t\t\tout = xbitmaps\n\t\tdefault:\n\t\t\tout = roaring.Add(xbitmaps, ybitmaps)\n\t\t}\n\n\t\t// Convert the bitmaps to output segments.\n\t\tfor i, b := range out {\n\t\t\tif !b.Any() {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfor len(dst) <= i {\n\t\t\t\tdst = append(dst, NewRow())\n\t\t\t}\n\t\t\tdst[i].Segments = append(dst[i].Segments, RowSegment{\n\t\t\t\tshard:    next,\n\t\t\t\twritable: true,\n\t\t\t\tdata:     b,\n\t\t\t\tn:        b.Count(),\n\t\t\t})\n\t\t}\n\t}\n\n\treturn dst\n}\n\n// rowBuilder builds a row quickly from individual values.\n// It is optimized for the case in which values are generated sequentially.\ntype rowBuilder struct {\n\tbm    *roaring.Bitmap\n\tmask  *[1024]uint64\n\tarray []uint16\n\tkey   uint64\n\tn     int32\n}\n\n// flushKey flushes the data at the current key to the bitmap.\nfunc (b *rowBuilder) flushKey() {\n\tvar c *roaring.Container\n\tswitch {\n\tcase b.mask != nil:\n\t\tc = roaring.NewContainerBitmapN(b.mask[:], b.n)\n\t\tb.mask = nil\n\tcase len(b.array) > 0:\n\t\tc = roaring.NewContainerArrayCopy(b.array)\n\t\tb.array = b.array[:0]\n\tdefault:\n\t\treturn\n\t}\n\n\tif b.bm == nil {\n\t\tb.bm = roaring.NewBitmap()\n\t}\n\tif old := b.bm.Containers.Get(b.key); old != nil {\n\t\tc = roaring.Union(c, old)\n\t}\n\tb.bm.Containers.Put(b.key, c)\n}\n\n// Add a value to the bitmap.\n// Values must be added sequentially.\nfunc (b *rowBuilder) Add(v uint64) {\n\tvkey := v / (1 << 16)\n\tif b.key != vkey {\n\t\t// This is a new key, so flush the old one.\n\t\tb.flushKey()\n\t\tb.key = vkey\n\t}\n\n\tif b.mask != nil {\n\t\t// Add to the mask.\n\t\tb.n += int32(1 &^ (b.mask[uint16(v)/64] >> (v % 64)))\n\t\tb.mask[uint16(v)/64] |= 1 << (v % 64)\n\t\treturn\n\t}\n\n\t// Add to an array.\n\tb.array = append(b.array, uint16(v))\n\tif len(b.array) >= roaring.ArrayMaxSize {\n\t\t// The array is too big.\n\t\t// Convert it to a bitmask.\n\t\tm := [1024]uint64{}\n\t\tfor _, v := range b.array {\n\t\t\tm[v/64] |= 1 << (v % 64)\n\t\t}\n\t\tb.n = int32(len(b.array))\n\t\tb.array = b.array[:0]\n\t\tb.mask = &m\n\t}\n}\n\n// Build a Row from stored data.\n// This resets the builder.\nfunc (b *rowBuilder) Build() *Row {\n\t// Flush the active key to the bitmap.\n\tb.flushKey()\n\n\t// Remove the bitmap and convert it to a Row.\n\tbm := b.bm\n\tb.bm = nil\n\tif bm == nil {\n\t\treturn NewRow()\n\t}\n\treturn NewRowFromBitmap(bm)\n}\n\n// bsiBuilder assembles BSI data.\n// It is optimized for the case in which values are generated sequentially.\ntype bsiBuilder []rowBuilder\n\n// Insert a value into the BSI data.\n// Columns must be inserted sequentially, and duplicates are not allowed.\nfunc (b *bsiBuilder) Insert(col, val uint64) {\n\tfor val != 0 {\n\t\ti := bits.TrailingZeros64(val)\n\t\tval &^= 1 << i\n\t\tfor len(*b) <= i {\n\t\t\t*b = append(*b, rowBuilder{})\n\t\t}\n\t\t(*b)[i].Add(col)\n\t}\n}\n\n// Build BSI data.\n// This resets the builder.\nfunc (b *bsiBuilder) Build() BSIData {\n\tbuilders := *b\n\t*b = builders[:0]\n\trows := make(BSIData, len(builders))\n\tfor i := range builders {\n\t\trows[i] = builders[i].Build()\n\t}\n\treturn rows\n}\n"
        },
        {
          "name": "bsi_test.go",
          "type": "blob",
          "size": 4.6533203125,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"fmt\"\n\t\"math/rand\"\n\t\"sort\"\n\t\"testing\"\n)\n\n// TestBSIAdd does a number of iterations. For each iteration, it\n// generates a random number of ids, and two random values for each id\n// to add together.\nfunc TestBSIAdd(t *testing.T) {\n\t// TODO wouldn't it be cool if our test suite had a randomized\n\t// burn-in mode where you could run any test which supported it\n\t// with a random seed and way more iterations?\n\trnd := rand.New(rand.NewSource(99))\n\t//numZipf := rand.NewZipf(rnd, 1.5, 2, ShardWidth-1)\n\tidZipf := rand.NewZipf(rnd, 1.8, 4, ShardWidth)\n\tvar builderA, builderB bsiBuilder\n\t// a and b are generated slices of numbers to add together\n\tvar a, b []uint64\n\t// idToIndex maps record ids to indexes in a and b\n\tidToIndex := make(map[int]int)\n\t// indexToID has the record id for each value in a and b\n\tindexToID := []uint64{}\n\n\tmin := 999999999\n\tmax := 0\n\n\tfor iteration := 0; iteration < 1; iteration++ {\n\t\tt.Run(fmt.Sprintf(\"%d\", iteration), func(t *testing.T) {\n\t\t\t// reset generated data\n\t\t\ta, b = a[:0], b[:0]\n\t\t\tindexToID = indexToID[:0]\n\t\t\tfor k := range idToIndex {\n\t\t\t\tdelete(idToIndex, k)\n\t\t\t}\n\n\t\t\t// z generates the values, they can be fairly large, but are usually small\n\t\t\tz := rand.NewZipf(rnd, 1.3, 7, 1<<44)\n\t\t\tid := -1\n\t\t\tfor i := 0; true; i++ {\n\t\t\t\t// get the next id, skipping a random amount\n\t\t\t\tid = id + int(idZipf.Uint64()+1)\n\t\t\t\tif id >= ShardWidth {\n\t\t\t\t\tif i < min {\n\t\t\t\t\t\tmin = i\n\t\t\t\t\t}\n\t\t\t\t\tif max < i {\n\t\t\t\t\t\tmax = i\n\t\t\t\t\t}\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tidToIndex[id] = int(i)\n\t\t\t\tindexToID = append(indexToID, uint64(id))\n\n\t\t\t\t// append a random value to each data slice\n\t\t\t\ta = append(a, z.Uint64())\n\t\t\t\tb = append(b, z.Uint64())\n\t\t\t}\n\n\t\t\t// build the BSIs based on the data slices and generated IDs\n\t\t\tfor index, id := range indexToID {\n\t\t\t\tva, vb := a[index], b[index]\n\t\t\t\tbuilderA.Insert(uint64(id), va)\n\t\t\t\tbuilderB.Insert(uint64(id), vb)\n\t\t\t}\n\t\t\tdataA, dataB := builderA.Build(), builderB.Build()\n\t\t\tdataC := AddBSI(dataA, dataB)\n\n\t\t\t// build results from added bsiData; results[i] should hold a[i]+b[i]\n\t\t\tresults := make([]uint64, len(a))\n\t\t\tdataC.PivotDescending(NewRow().Union(dataC...), 0, nil, nil, func(count uint64, ids ...uint64) {\n\t\t\t\tfor _, id := range ids {\n\t\t\t\t\tresults[idToIndex[int(id)]] = count\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tfor i, res := range results {\n\t\t\t\tif res != a[i]+b[i] {\n\t\t\t\t\tt.Errorf(\"Mismatch at %d\\na: %v\\nb: %v\\nr: %v\", i, a, b, results)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\ntype bsiAddCase struct {\n\tpositions []uint64\n\ta         []uint64\n\tb         []uint64\n}\n\nfunc (b bsiAddCase) Len() int {\n\treturn len(b.positions)\n}\n\n// Less reports whether the element with\n// index i should sort before the element with index j.\nfunc (b bsiAddCase) Less(i, j int) bool {\n\treturn b.positions[i] < b.positions[j]\n}\n\n// Swap swaps the elements with indexes i and j.\nfunc (b bsiAddCase) Swap(i, j int) {\n\tb.positions[i], b.positions[j] = b.positions[j], b.positions[i]\n\tb.a[i], b.a[j] = b.a[j], b.a[i]\n\tb.b[i], b.b[j] = b.b[j], b.b[i]\n}\n\n// TestBSIAddCases tests specific cases of bsiAdd (would generally be\n// pulled from randomly generated ones from TestBSIAdd upon failure).\nfunc TestBSIAddCases(t *testing.T) {\n\ttests := []bsiAddCase{\n\t\t{\n\t\t\tpositions: []uint64{161311, 611110, 82544, 996022, 836077, 64964, 480737, 156534, 240525, 580896, 239236, 54607, 1019438, 894260, 17570, 884645, 936658, 682651, 987695, 390274},\n\t\t\ta:         []uint64{17, 1, 2846, 45437619, 23781, 36, 88, 168691, 13417, 1301, 10, 71, 0, 176, 1010, 21, 1, 509, 17, 4},\n\t\t\tb:         []uint64{24, 288, 12737, 14, 150, 21, 24, 354, 0, 19, 5, 150, 3940, 121, 25, 621, 7, 9023592401, 6033, 7},\n\t\t},\n\t\t{\n\t\t\tpositions: []uint64{17570, 54607},\n\t\t\ta:         []uint64{1010, 71},\n\t\t\tb:         []uint64{25, 150},\n\t\t},\n\t}\n\n\tvar builderA, builderB bsiBuilder\n\tfor i, tst := range tests {\n\t\tt.Run(fmt.Sprintf(\"%d\", i), func(t *testing.T) {\n\t\t\tif len(tst.a) != len(tst.b) || len(tst.a) != len(tst.positions) {\n\t\t\t\tt.Fatalf(\"Malformed test, a is %d, but b is %d\", len(tst.a), len(tst.b))\n\t\t\t}\n\t\t\tsort.Sort(tst)\n\n\t\t\tfor i := 0; i < len(tst.a); i++ {\n\t\t\t\tbuilderA.Insert(tst.positions[i], tst.a[i])\n\t\t\t\tbuilderB.Insert(tst.positions[i], tst.b[i])\n\t\t\t}\n\n\t\t\tdataA, dataB := builderA.Build(), builderB.Build()\n\t\t\tdataC := AddBSI(dataA, dataB)\n\t\t\t// maps id to count\n\t\t\tresults := make(map[uint64]uint64)\n\t\t\tdataC.PivotDescending(NewRow().Union(dataC...), 0, nil, nil, func(count uint64, ids ...uint64) {\n\t\t\t\tfor _, id := range ids {\n\t\t\t\t\tresults[id] = count\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tfor i, id := range tst.positions {\n\t\t\t\tif results[id] != tst.a[i]+tst.b[i] {\n\t\t\t\t\tt.Fatalf(\"value %d mismatch, id: %d. got %d, want %d\", i, id, results[id], tst.a[i]+tst.b[i])\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "buffer",
          "type": "tree",
          "content": null
        },
        {
          "name": "bufferpool",
          "type": "tree",
          "content": null
        },
        {
          "name": "cache.go",
          "type": "blob",
          "size": 14.8173828125,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/lru\"\n\tpb \"github.com/featurebasedb/featurebase/v3/proto\"\n\t\"github.com/pkg/errors\"\n)\n\nconst (\n\t// thresholdFactor is used to calculate the threshold for new items entering the cache\n\tthresholdFactor = 1.1\n)\n\n// cache represents a cache of counts.\ntype cache interface {\n\tAdd(id uint64, n uint64)\n\tBulkAdd(id uint64, n uint64)\n\tGet(id uint64) uint64\n\tLen() int\n\n\t// Returns a list of all IDs.\n\tIDs() []uint64\n\n\t// Soft ask for the cache to be rebuilt - may not if it has been done recently.\n\tInvalidate()\n\n\t// Rebuilds the cache.\n\tRecalculate()\n\n\t// Returns an ordered list of the top ranked bitmaps.\n\tTop() []bitmapPair\n\n\t// Clear removes everything from the cache. If possible it should leave allocated structures in place to be reused.\n\tClear()\n}\n\n// lruCache represents a least recently used Cache implementation.\ntype lruCache struct {\n\tcache  *lru.Cache\n\tcounts map[uint64]uint64\n\t// maxEntries is saved to support Clear which recreates the cache.\n\tmaxEntries uint32\n}\n\n// newLRUCache returns a new instance of LRUCache.\nfunc newLRUCache(maxEntries uint32) *lruCache {\n\tc := &lruCache{\n\t\tcache:      lru.New(int(maxEntries)),\n\t\tcounts:     make(map[uint64]uint64),\n\t\tmaxEntries: maxEntries,\n\t}\n\tc.cache.OnEvicted = c.onEvicted\n\treturn c\n}\n\n// BulkAdd adds a count to the cache unsorted. You should Invalidate after completion.\nfunc (c *lruCache) BulkAdd(id, n uint64) {\n\tc.Add(id, n)\n}\n\n// Add adds a count to the cache.\nfunc (c *lruCache) Add(id, n uint64) {\n\tc.cache.Add(id, n)\n\tc.counts[id] = n\n}\n\n// Get returns a count for a given id.\nfunc (c *lruCache) Get(id uint64) uint64 {\n\tn, _ := c.cache.Get(id)\n\tnn, _ := n.(uint64)\n\treturn nn\n}\n\n// Len returns the number of items in the cache.\nfunc (c *lruCache) Len() int { return c.cache.Len() }\n\n// Invalidate is a no-op.\nfunc (c *lruCache) Invalidate() {}\n\n// Recalculate is a no-op.\nfunc (c *lruCache) Recalculate() {}\n\n// IDs returns a list of all IDs in the cache.\nfunc (c *lruCache) IDs() []uint64 {\n\ta := make([]uint64, 0, len(c.counts))\n\tfor id := range c.counts {\n\t\ta = append(a, id)\n\t}\n\tsort.Sort(uint64Slice(a))\n\treturn a\n}\n\n// Top returns all counts in the cache.\nfunc (c *lruCache) Top() []bitmapPair {\n\ta := make([]bitmapPair, 0, len(c.counts))\n\tfor id, n := range c.counts {\n\t\ta = append(a, bitmapPair{\n\t\t\tID:    id,\n\t\t\tCount: n,\n\t\t})\n\t}\n\tpairs := bitmapPairs(a)\n\tsort.Sort(&pairs)\n\treturn a\n}\n\nfunc (c *lruCache) Clear() {\n\tfor k := range c.counts {\n\t\tdelete(c.counts, k)\n\t}\n\tc.cache = lru.New(int(c.maxEntries))\n}\n\nfunc (c *lruCache) onEvicted(key lru.Key, _ interface{}) { delete(c.counts, key.(uint64)) }\n\n// Ensure LRUCache implements Cache.\nvar _ cache = &lruCache{}\n\n// rankCache represents a cache with sorted entries.\ntype rankCache struct {\n\t// TODO why does this have a lock and lruCache doesn't?\n\tmu           sync.Mutex\n\tentries      map[uint64]uint64\n\trankings     bitmapPairs // cached, ordered list\n\trankingsRead bool\n\tdirty        bool\n\n\tupdateN    int\n\tupdateTime time.Time\n\n\t// maxEntries is the user defined size of the cache\n\tmaxEntries uint32\n\n\t// thresholdBuffer is used the calculate the lowest cached threshold value\n\t// This threshold determines what new items are added to the cache\n\tthresholdBuffer int\n\n\t// thresholdValue is the value of the last item in the cache\n\tthresholdValue uint64\n}\n\n// NewRankCache returns a new instance of RankCache.\nfunc NewRankCache(maxEntries uint32) *rankCache {\n\treturn &rankCache{\n\t\tmaxEntries:      maxEntries,\n\t\tthresholdBuffer: int(thresholdFactor * float64(maxEntries)),\n\t\tentries:         make(map[uint64]uint64),\n\t}\n}\n\nfunc (c *rankCache) Clear() {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\tfor k := range c.entries {\n\t\tdelete(c.entries, k)\n\t}\n\tc.rankings = c.rankings[:0]\n\tc.rankingsRead = false\n\tc.dirty = false\n\n\tc.updateN = 0\n\tc.updateTime = time.Time{}\n\tc.thresholdValue = 0\n}\n\n// Add adds a count to the cache.\nfunc (c *rankCache) Add(id uint64, n uint64) {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\t// Flag the cache as dirty.\n\t// This forces recalculation if top is called before the cache is recalculated.\n\tc.dirty = true\n\n\t// Ignore if the column count is below the threshold,\n\t// unless the count is 0, which is effectively used\n\t// to clear the cache value.\n\tif n < c.thresholdValue && n > 0 {\n\t\tdelete(c.entries, id)\n\t\treturn\n\t}\n\n\tc.entries[id] = n\n\n\tc.invalidate()\n}\n\n// BulkAdd adds a count to the cache unsorted. You should Invalidate after completion.\nfunc (c *rankCache) BulkAdd(id uint64, n uint64) {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\t// Flag the cache as dirty.\n\t// This forces recalculation if top is called before the cache is recalculated.\n\tc.dirty = true\n\n\tif n < c.thresholdValue {\n\t\tdelete(c.entries, id)\n\t\treturn\n\t}\n\n\tc.entries[id] = n\n\n\t// FB-1206: Periodically invalidate the cache when we are bulk loading\n\t// as this can take up an upbounded amount of memory. This is especially\n\t// true when restoring shards as all rows will be added.\n\tif len(c.entries) > int(2*c.maxEntries) {\n\t\tCounterRecalculateCache.Inc()\n\t\tc.recalculate()\n\t}\n}\n\n// Get returns a count for a given id.\nfunc (c *rankCache) Get(id uint64) uint64 {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\treturn c.entries[id]\n}\n\n// Len returns the number of items in the cache.\nfunc (c *rankCache) Len() int {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\treturn len(c.entries)\n}\n\n// IDs returns a list of all IDs in the cache.\nfunc (c *rankCache) IDs() []uint64 {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\tif len(c.entries) == 0 {\n\t\treturn nil\n\t}\n\tids := make([]uint64, 0, len(c.entries))\n\tfor id := range c.entries {\n\t\tids = append(ids, id)\n\t}\n\tsort.Sort(uint64Slice(ids))\n\treturn ids\n}\n\n// Invalidate recalculates the entries by rank.\nfunc (c *rankCache) Invalidate() {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\tc.invalidate()\n}\n\n// Recalculate rebuilds the cache.\nfunc (c *rankCache) Recalculate() {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\tCounterRecalculateCache.Inc()\n\tc.recalculate()\n}\n\nfunc (c *rankCache) invalidate() {\n\t// Don't invalidate more than once every X seconds.\n\t// TODO: consider making this configurable.\n\tif time.Since(c.updateTime).Seconds() < 10 {\n\t\t// Skipping recalculation means that the ranked cache's growth is unbounded.\n\t\t// This is somewhat necessary for now since recalculation is not cheap.\n\t\t// The cache will remain flagged as dirty and will be recalculated if Top is called.\n\t\t// This may cause unexpected memory growth, so record it in metrics for debugging purposes.\n\t\tCounterInvalidateCacheSkipped.Inc()\n\t\t// Ensure that we're marked as dirty even if we weren't otherwise.\n\t\tc.dirty = true\n\t\treturn\n\t}\n\tCounterInvalidateCache.Inc()\n\tc.recalculate()\n}\n\nfunc (c *rankCache) recalculate() {\n\tif c.rankingsRead {\n\t\tc.rankings = nil\n\t\tc.rankingsRead = false\n\t}\n\n\t// Convert cache to a sorted list.\n\trankings := c.rankings[:0]\n\tif cap(rankings) < len(c.entries) {\n\t\trankings = make([]bitmapPair, 0, len(c.entries))\n\t}\n\tfor id, cnt := range c.entries {\n\t\trankings = append(rankings, bitmapPair{\n\t\t\tID:    id,\n\t\t\tCount: cnt,\n\t\t})\n\t}\n\tc.rankings = rankings\n\tsort.Sort(&c.rankings)\n\n\t// Store the count of the item at the threshold index.\n\tlength := len(c.rankings)\n\tGaugeRankCacheLength.Set(float64(length))\n\n\tvar removeItems []bitmapPair // cached, ordered list\n\tif length > int(c.maxEntries) {\n\t\tc.thresholdValue = rankings[c.maxEntries].Count\n\t\tremoveItems = c.rankings[c.maxEntries:]\n\t\tc.rankings = c.rankings[0:c.maxEntries]\n\t} else {\n\t\tc.thresholdValue = 1\n\t}\n\n\t// Reset counters.\n\tc.updateTime, c.updateN = time.Now(), 0\n\n\t// If size is larger than the threshold then trim it.\n\tif len(c.entries) > c.thresholdBuffer {\n\t\tCounterCacheThresholdReached.Inc()\n\t\tfor _, pair := range removeItems {\n\t\t\tdelete(c.entries, pair.ID)\n\t\t}\n\t}\n\n\t// The cache is no longer dirty.\n\tc.dirty = false\n}\n\n// Top returns an ordered list of pairs.\nfunc (c *rankCache) Top() []bitmapPair {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tif c.dirty {\n\t\t// The cache is dirty, so we need to recalculate it to get a consistent view.\n\t\tCounterReadDirtyCache.Inc()\n\t\tc.recalculate()\n\t}\n\n\tc.rankingsRead = true\n\treturn c.rankings\n}\n\n// WriteTo writes the cache to w.\nfunc (c *rankCache) WriteTo(w io.Writer) (n int64, err error) {\n\tpanic(\"FIXME: TODO\")\n}\n\n// ReadFrom read from r into the cache.\nfunc (c *rankCache) ReadFrom(r io.Reader) (n int64, err error) {\n\tpanic(\"FIXME: TODO\")\n}\n\n// Ensure RankCache implements Cache.\nvar _ cache = &rankCache{}\n\n// bitmapPair represents a id/count pair with an associated identifier.\ntype bitmapPair struct {\n\tID    uint64\n\tCount uint64\n}\n\n// bitmapPairs is a sortable list of BitmapPair objects.\ntype bitmapPairs []bitmapPair\n\nfunc (p *bitmapPairs) Swap(i, j int)      { (*p)[i], (*p)[j] = (*p)[j], (*p)[i] }\nfunc (p *bitmapPairs) Len() int           { return len(*p) }\nfunc (p *bitmapPairs) Less(i, j int) bool { return (*p)[i].Count > (*p)[j].Count }\n\n// Pair holds an id/count pair.\ntype Pair struct {\n\tID    uint64 `json:\"id\"`\n\tKey   string `json:\"key\"`\n\tCount uint64 `json:\"count\"`\n}\n\n// PairField is a Pair with its associated field.\ntype PairField struct {\n\tPair  Pair\n\tField string\n}\n\nfunc (p PairField) Clone() (r PairField) {\n\treturn PairField{\n\t\tPair:  p.Pair,\n\t\tField: p.Field,\n\t}\n}\n\n// ToTable implements the ToTabler interface.\nfunc (p PairField) ToTable() (*pb.TableResponse, error) {\n\treturn pb.RowsToTable(p, 1)\n}\n\n// ToRows implements the ToRowser interface.\nfunc (p PairField) ToRows(callback func(*pb.RowResponse) error) error {\n\tif p.Pair.Key != \"\" {\n\t\treturn callback(&pb.RowResponse{\n\t\t\tHeaders: []*pb.ColumnInfo{\n\t\t\t\t{Name: p.Field, Datatype: \"string\"},\n\t\t\t\t{Name: \"count\", Datatype: \"uint64\"},\n\t\t\t},\n\t\t\tColumns: []*pb.ColumnResponse{\n\t\t\t\t{ColumnVal: &pb.ColumnResponse_StringVal{StringVal: p.Pair.Key}},\n\t\t\t\t{ColumnVal: &pb.ColumnResponse_Uint64Val{Uint64Val: p.Pair.Count}},\n\t\t\t},\n\t\t})\n\t} else {\n\t\treturn callback(&pb.RowResponse{\n\t\t\tHeaders: []*pb.ColumnInfo{\n\t\t\t\t{Name: p.Field, Datatype: \"uint64\"},\n\t\t\t\t{Name: \"count\", Datatype: \"uint64\"},\n\t\t\t},\n\t\t\tColumns: []*pb.ColumnResponse{\n\t\t\t\t{ColumnVal: &pb.ColumnResponse_Uint64Val{Uint64Val: p.Pair.ID}},\n\t\t\t\t{ColumnVal: &pb.ColumnResponse_Uint64Val{Uint64Val: p.Pair.Count}},\n\t\t\t},\n\t\t})\n\t}\n}\n\n// MarshalJSON marshals PairField into a JSON-encoded byte slice,\n// excluding `Field`.\nfunc (p PairField) MarshalJSON() ([]byte, error) {\n\treturn json.Marshal(p.Pair)\n}\n\n// Pairs is a sortable slice of Pair objects.\ntype Pairs []Pair\n\nfunc (p Pairs) Swap(i, j int)      { p[i], p[j] = p[j], p[i] }\nfunc (p Pairs) Len() int           { return len(p) }\nfunc (p Pairs) Less(i, j int) bool { return p[i].Count > p[j].Count }\n\n// pairHeap is a heap implementation over a group of Pairs.\ntype pairHeap struct {\n\tPairs\n}\n\n// Less implemets the Sort interface.\n// reports whether the element with index i should sort before the element with index j.\nfunc (p pairHeap) Less(i, j int) bool { return p.Pairs[i].Count < p.Pairs[j].Count }\n\n// Push appends the element onto the Pair slice.\nfunc (p *Pairs) Push(x interface{}) {\n\t// Push and Pop use pointer receivers because they modify the slice's length,\n\t// not just its contents.\n\t*p = append(*p, x.(Pair))\n}\n\n// Pop removes the minimum element from the Pair slice.\nfunc (p *Pairs) Pop() interface{} {\n\told := *p\n\tn := len(old)\n\tx := old[n-1]\n\t*p = old[0 : n-1]\n\treturn x\n}\n\n// Add merges other into p and returns a new slice.\nfunc (p Pairs) Add(other []Pair) []Pair {\n\t// Create lookup of key/counts.\n\tm := make(map[uint64]uint64, len(p))\n\tfor _, pair := range p {\n\t\tm[pair.ID] = pair.Count\n\t}\n\n\t// Add/merge from other.\n\tfor _, pair := range other {\n\t\tm[pair.ID] += pair.Count\n\t}\n\n\t// Convert back to slice.\n\ta := make([]Pair, 0, len(m))\n\tfor k, v := range m {\n\t\ta = append(a, Pair{ID: k, Count: v})\n\t}\n\treturn a\n}\n\n// Keys returns a slice of all keys in p.\nfunc (p Pairs) Keys() []uint64 {\n\ta := make([]uint64, len(p))\n\tfor i := range p {\n\t\ta[i] = p[i].ID\n\t}\n\treturn a\n}\n\nfunc (p Pairs) String() string {\n\tvar buf bytes.Buffer\n\tbuf.WriteString(\"Pairs(\")\n\tfor i := range p {\n\t\tfmt.Fprintf(&buf, \"%d/%d\", p[i].ID, p[i].Count)\n\t\tif i < len(p)-1 {\n\t\t\tbuf.WriteString(\", \")\n\t\t}\n\t}\n\tbuf.WriteString(\")\")\n\treturn buf.String()\n}\n\n// PairsField is a Pairs object with its associated field.\ntype PairsField struct {\n\tPairs []Pair\n\tField string\n}\n\nfunc (p *PairsField) Clone() (r *PairsField) {\n\tr = &PairsField{\n\t\tPairs: make([]Pair, len(p.Pairs)),\n\t\tField: p.Field,\n\t}\n\tcopy(r.Pairs, p.Pairs)\n\treturn\n}\n\n// ToTable implements the ToTabler interface.\nfunc (p *PairsField) ToTable() (*pb.TableResponse, error) {\n\treturn pb.RowsToTable(p, len(p.Pairs))\n}\n\n// ToRows implements the ToRowser interface.\nfunc (p *PairsField) ToRows(callback func(*pb.RowResponse) error) error {\n\t// Determine if the ID has string keys.\n\tvar stringKeys bool\n\tif len(p.Pairs) > 0 {\n\t\tif p.Pairs[0].Key != \"\" {\n\t\t\tstringKeys = true\n\t\t}\n\t}\n\n\tdtype := \"uint64\"\n\tif stringKeys {\n\t\tdtype = \"string\"\n\t}\n\tci := []*pb.ColumnInfo{\n\t\t{Name: p.Field, Datatype: dtype},\n\t\t{Name: \"count\", Datatype: \"uint64\"},\n\t}\n\tfor _, pair := range p.Pairs {\n\t\tif stringKeys {\n\t\t\tif err := callback(&pb.RowResponse{\n\t\t\t\tHeaders: ci,\n\t\t\t\tColumns: []*pb.ColumnResponse{\n\t\t\t\t\t{ColumnVal: &pb.ColumnResponse_StringVal{StringVal: pair.Key}},\n\t\t\t\t\t{ColumnVal: &pb.ColumnResponse_Uint64Val{Uint64Val: uint64(pair.Count)}},\n\t\t\t\t}}); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"calling callback\")\n\t\t\t}\n\t\t} else {\n\t\t\tif err := callback(&pb.RowResponse{\n\t\t\t\tHeaders: ci,\n\t\t\t\tColumns: []*pb.ColumnResponse{\n\t\t\t\t\t{ColumnVal: &pb.ColumnResponse_Uint64Val{Uint64Val: uint64(pair.ID)}},\n\t\t\t\t\t{ColumnVal: &pb.ColumnResponse_Uint64Val{Uint64Val: uint64(pair.Count)}},\n\t\t\t\t}}); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"calling callback\")\n\t\t\t}\n\t\t}\n\t\tci = nil //only send on the first\n\t}\n\treturn nil\n}\n\n// MarshalJSON marshals PairsField into a JSON-encoded byte slice,\n// excluding `Field`.\nfunc (p PairsField) MarshalJSON() ([]byte, error) {\n\treturn json.Marshal(p.Pairs)\n}\n\n// int64Slice represents a sortable slice of int64 numbers.\ntype int64Slice []int64\n\nfunc (p int64Slice) Len() int           { return len(p) }\nfunc (p int64Slice) Less(i, j int) bool { return p[i] < p[j] }\nfunc (p int64Slice) Swap(i, j int)      { p[i], p[j] = p[j], p[i] }\n\n// uint64Slice represents a sortable slice of uint64 numbers.\ntype uint64Slice []uint64\n\nfunc (p uint64Slice) Swap(i, j int)      { p[i], p[j] = p[j], p[i] }\nfunc (p uint64Slice) Len() int           { return len(p) }\nfunc (p uint64Slice) Less(i, j int) bool { return p[i] < p[j] }\n\n// nopCache represents a no-op Cache implementation.\ntype nopCache struct{}\n\n// Ensure NopCache implements Cache.\nvar globalNopCache cache = nopCache{}\n\nfunc (c nopCache) Add(uint64, uint64)     {}\nfunc (c nopCache) BulkAdd(uint64, uint64) {}\nfunc (c nopCache) Get(uint64) uint64      { return 0 }\nfunc (c nopCache) IDs() []uint64          { return []uint64{} }\n\nfunc (c nopCache) Invalidate()  {}\nfunc (c nopCache) Len() int     { return 0 }\nfunc (c nopCache) Recalculate() {}\n\nfunc (c nopCache) Clear() {}\n\nfunc (c nopCache) Top() []bitmapPair {\n\treturn []bitmapPair{}\n}\n"
        },
        {
          "name": "cache_test.go",
          "type": "blob",
          "size": 2.1787109375,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa_test\n\nimport (\n\t\"reflect\"\n\t\"testing\"\n\n\tpilosa \"github.com/featurebasedb/featurebase/v3\"\n)\n\n// Ensure cache stays constrained to its configured size.\nfunc TestCache_Rank_Size(t *testing.T) {\n\tcacheSize := uint32(3)\n\tcache := pilosa.NewRankCache(cacheSize)\n\tfor i := 1; i < int(2*cacheSize); i++ {\n\t\tcache.Add(uint64(i), 3)\n\t}\n\tcache.Recalculate()\n\tif cache.Len() != int(cacheSize) {\n\t\tt.Fatalf(\"unexpected cache Size: %d!=%d expected\\n\", cache.Len(), cacheSize)\n\t}\n}\n\n// Ensure cache entries set below threshold are handled appropriately.\nfunc TestCache_Rank_Threshold(t *testing.T) {\n\tcacheSize := uint32(5)\n\tcache := pilosa.NewRankCache(cacheSize)\n\tfor i := 1; i < int(2*cacheSize); i++ {\n\t\tcache.Add(uint64(i), 3)\n\t}\n\n\t// Set the cache value for rows 4 and 5 to a number below the threshold\n\t// value (which is 3), and ensure that they gets zeroed out.\n\tcache.Add(4, 1)\n\tcache.BulkAdd(5, 1)\n\tcache.Recalculate()\n\n\tif cache.Get(4) != 0 {\n\t\tt.Fatalf(\"unexpected cache value after Add: %d!=%d expected\\n\", cache.Get(4), 0)\n\t}\n\tif cache.Get(5) != 0 {\n\t\tt.Fatalf(\"unexpected cache value after BulkAdd: %d!=%d expected\\n\", cache.Get(5), 0)\n\t}\n}\n\n// Test that consecutive writes show up in Top.\n// On later writes, the cache skips recalculation to save CPU time.\n// This used to mean that the later writes would not show up in Top.\n// Now, the cache is flagged as dirty and recalculated during the call to Top.\nfunc TestCache_Rank_Dirty(t *testing.T) {\n\tcacheSize := uint32(5)\n\tcache := pilosa.NewRankCache(cacheSize)\n\n\ttype pair struct{ ID, Count uint64 }\n\texpect := []pair{\n\t\t{5, 2},\n\t\t{4, 1},\n\t}\n\n\tfor _, v := range expect {\n\t\tcache.Add(v.ID, v.Count)\n\t}\n\n\tvar got []pair //nolint:prealloc\n\tfor _, p := range cache.Top() {\n\t\tgot = append(got, pair(p))\n\t}\n\n\tif !reflect.DeepEqual(expect, got) {\n\t\tt.Fatalf(\"wrote %v but got %v\", expect, got)\n\t}\n}\n\nfunc TestCache_Rank_BulkAdd(t *testing.T) {\n\tconst cacheSize = 10\n\tcache := pilosa.NewRankCache(uint32(cacheSize))\n\n\tfor i := uint64(0); i < 1000; i++ {\n\t\tcache.BulkAdd(i, i)\n\t\tif n := cache.Len(); n > cacheSize*2 {\n\t\t\tt.Fatalf(\"entry count exceed 2x cache size: %d\", n)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "catcher.go",
          "type": "blob",
          "size": 6.689453125,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\ttxkey \"github.com/featurebasedb/featurebase/v3/short_txkey\"\n\t\"github.com/featurebasedb/featurebase/v3/vprint\"\n)\n\n// catcher is useful to report error locations with a\n// Stack dump before the complexity\n// of the executor_test swallows up\n// the location of a PanicOn.\ntype catcherTx struct {\n\tb Tx\n}\n\nfunc newCatcherTx(b Tx) *catcherTx {\n\treturn &catcherTx{b: b}\n}\n\nfunc init() {\n\t// keep golangci-lint happy\n\t_ = newCatcherTx\n}\n\nvar _ Tx = (*catcherTx)(nil)\n\nfunc (c *catcherTx) ImportRoaringBits(index, field, view string, shard uint64, rit roaring.RoaringIterator, clear bool, log bool, rowSize uint64) (changed int, rowSet map[uint64]int, err error) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see ImportRoaringBits() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.ImportRoaringBits(index, field, view, shard, rit, clear, log, rowSize)\n}\n\nfunc (c *catcherTx) Rollback() {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see Rollback() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\tc.b.Rollback()\n}\n\nfunc (c *catcherTx) Commit() error {\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see Commit() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.Commit()\n}\n\nfunc (c *catcherTx) RoaringBitmap(index, field, view string, shard uint64) (*roaring.Bitmap, error) {\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see RoaringBitmap() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.RoaringBitmap(index, field, view, shard)\n}\n\nfunc (c *catcherTx) Container(index, field, view string, shard uint64, key uint64) (ct *roaring.Container, err error) {\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see Container() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.Container(index, field, view, shard, key)\n}\n\nfunc (c *catcherTx) PutContainer(index, field, view string, shard uint64, key uint64, rc *roaring.Container) error {\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see PutContainer() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.PutContainer(index, field, view, shard, key, rc)\n}\n\nfunc (c *catcherTx) RemoveContainer(index, field, view string, shard uint64, key uint64) error {\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see RemoveContainer() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.RemoveContainer(index, field, view, shard, key)\n}\n\nfunc (c *catcherTx) Add(index, field, view string, shard uint64, a ...uint64) (changeCount int, err error) {\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see Add() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.Add(index, field, view, shard, a...)\n}\n\nfunc (c *catcherTx) Remove(index, field, view string, shard uint64, a ...uint64) (changeCount int, err error) {\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see Remove() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.Remove(index, field, view, shard, a...)\n}\n\nfunc (c *catcherTx) Removed(index, field, view string, shard uint64, a ...uint64) (changed []uint64, err error) {\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see Removed() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.Removed(index, field, view, shard, a...)\n}\n\nfunc (c *catcherTx) Contains(index, field, view string, shard uint64, key uint64) (exists bool, err error) {\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see Contains() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.Contains(index, field, view, shard, key)\n}\n\nfunc (c *catcherTx) ContainerIterator(index, field, view string, shard uint64, firstRoaringContainerKey uint64) (citer roaring.ContainerIterator, found bool, err error) {\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see ContainerIterator() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.ContainerIterator(index, field, view, shard, firstRoaringContainerKey)\n}\n\nfunc (c *catcherTx) Count(index, field, view string, shard uint64) (uint64, error) {\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see Count() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.Count(index, field, view, shard)\n}\n\nfunc (c *catcherTx) Max(index, field, view string, shard uint64) (uint64, error) {\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see Max() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.Max(index, field, view, shard)\n}\n\nfunc (c *catcherTx) Min(index, field, view string, shard uint64) (uint64, bool, error) {\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see Min() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.Min(index, field, view, shard)\n}\n\nfunc (c *catcherTx) CountRange(index, field, view string, shard uint64, start, end uint64) (n uint64, err error) {\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see CountRange() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.CountRange(index, field, view, shard, start, end)\n}\n\nfunc (c *catcherTx) OffsetRange(index, field, view string, shard, offset, start, end uint64) (other *roaring.Bitmap, err error) {\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see OffsetRange() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.OffsetRange(index, field, view, shard, offset, start, end)\n}\n\nfunc (c *catcherTx) Type() string {\n\treturn c.b.Type()\n}\n\nfunc (c *catcherTx) ApplyFilter(index, field, view string, shard uint64, ckey uint64, filter roaring.BitmapFilter) (err error) {\n\treturn GenericApplyFilter(c, index, field, view, shard, ckey, filter)\n}\n\nfunc (c *catcherTx) ApplyRewriter(index, field, view string, shard uint64, ckey uint64, filter roaring.BitmapRewriter) (err error) {\n\treturn c.b.ApplyRewriter(index, field, view, shard, ckey, filter)\n}\n\nfunc (c *catcherTx) GetSortedFieldViewList(idx *Index, shard uint64) (fvs []txkey.FieldView, err error) {\n\treturn c.b.GetSortedFieldViewList(idx, shard)\n}\n\nfunc (c *catcherTx) GetFieldSizeBytes(index, field string) (uint64, error) {\n\treturn 0, nil\n}\n"
        },
        {
          "name": "cli",
          "type": "tree",
          "content": null
        },
        {
          "name": "client",
          "type": "tree",
          "content": null
        },
        {
          "name": "cluster.go",
          "type": "blob",
          "size": 28.4609375,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"log\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/dax\"\n\t\"github.com/featurebasedb/featurebase/v3/dax/computer\"\n\t\"github.com/featurebasedb/featurebase/v3/dax/storage\"\n\t\"github.com/featurebasedb/featurebase/v3/disco\"\n\t\"github.com/featurebasedb/featurebase/v3/logger\"\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\t\"github.com/pkg/errors\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\nconst (\n\tdefaultConfirmDownRetries = 10\n\tdefaultConfirmDownSleep   = 1 * time.Second\n)\n\n// cluster represents a collection of nodes.\ntype cluster struct { //nolint: maligned\n\tnoder disco.Noder\n\n\tid   string\n\tNode *disco.Node\n\n\t// Hashing algorithm used to assign partitions to nodes.\n\tHasher disco.Hasher\n\n\t// The number of partitions in the cluster.\n\tpartitionN int\n\n\t// The number of replicas a partition has.\n\tReplicaN int\n\n\t// Human-readable name of the cluster.\n\tName string\n\n\t// Maximum number of Set() or Clear() commands per request.\n\tmaxWritesPerRequest int\n\n\t// Data directory path.\n\tPath string\n\n\t// Distributed Consensus\n\tdisCo   disco.DisCo\n\tsharder disco.Sharder\n\n\tholder      *Holder\n\tbroadcaster broadcaster\n\n\ttranslationSyncer TranslationSyncer\n\n\tmu sync.RWMutex\n\n\t// Close management\n\twg      sync.WaitGroup\n\tclosing chan struct{}\n\n\tlogger logger.Logger\n\n\tInternalClient *InternalClient\n\n\tconfirmDownRetries int\n\tconfirmDownSleep   time.Duration\n\n\tpartitionAssigner string\n\n\tserverlessStorage *storage.ResourceManager\n\n\t// isComputeNode is set to true if this node is running as a DAX compute\n\t// node.\n\tisComputeNode bool\n}\n\n// newCluster returns a new instance of Cluster with defaults.\nfunc newCluster() *cluster {\n\treturn &cluster{\n\t\tHasher:     &disco.Jmphasher{},\n\t\tpartitionN: disco.DefaultPartitionN,\n\t\tReplicaN:   1,\n\n\t\tclosing: make(chan struct{}),\n\n\t\ttranslationSyncer: NopTranslationSyncer,\n\n\t\tInternalClient: &InternalClient{}, // TODO might have to fill this out a bit\n\n\t\tlogger: logger.NopLogger,\n\n\t\tconfirmDownRetries: defaultConfirmDownRetries,\n\t\tconfirmDownSleep:   defaultConfirmDownSleep,\n\n\t\tdisCo: disco.NopDisCo,\n\t\tnoder: disco.NewEmptyLocalNoder(),\n\t}\n}\n\nfunc (c *cluster) primaryNode() *disco.Node {\n\treturn c.unprotectedPrimaryNode()\n}\n\n// unprotectedPrimaryNode returns the primary node.\nfunc (c *cluster) unprotectedPrimaryNode() *disco.Node {\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := c.NewSnapshot()\n\treturn snap.PrimaryFieldTranslationNode()\n}\n\n// nodeIDs returns the list of IDs in the cluster.\nfunc (c *cluster) nodeIDs() []string {\n\treturn disco.Nodes(c.Nodes()).IDs()\n}\n\nfunc (c *cluster) State() (disco.ClusterState, error) {\n\treturn c.noder.ClusterState(context.Background())\n}\n\nfunc (c *cluster) nodeByID(id string) *disco.Node {\n\tc.mu.RLock()\n\tdefer c.mu.RUnlock()\n\treturn c.unprotectedNodeByID(id)\n}\n\n// unprotectedNodeByID returns a node reference by ID.\nfunc (c *cluster) unprotectedNodeByID(id string) *disco.Node {\n\tfor _, n := range c.noder.Nodes() {\n\t\tif n.ID == id {\n\t\t\treturn n\n\t\t}\n\t}\n\treturn nil\n}\n\n// nodePositionByID returns the position of the node in slice c.Nodes.\nfunc (c *cluster) nodePositionByID(nodeID string) int {\n\tfor i, n := range c.noder.Nodes() {\n\t\tif n.ID == nodeID {\n\t\t\treturn i\n\t\t}\n\t}\n\treturn -1\n}\n\n// Nodes returns a copy of the slice of nodes in the cluster. Safe for\n// concurrent use, result may be modified.\nfunc (c *cluster) Nodes() []*disco.Node {\n\tnodes := c.noder.Nodes()\n\t// duplicate the nodes since we're going to be altering them\n\tcopiedNodes := make([]disco.Node, len(nodes))\n\tresult := make([]*disco.Node, len(nodes))\n\n\tprimary := disco.PrimaryNode(nodes, c.Hasher)\n\n\t// Set node states and IsPrimary.\n\tfor i, node := range nodes {\n\t\tcopiedNodes[i] = *node\n\t\tresult[i] = &copiedNodes[i]\n\t\tif node == primary {\n\t\t\tcopiedNodes[i].IsPrimary = true\n\t\t}\n\t}\n\treturn result\n}\n\n// shardDistributionByIndex returns a map of [nodeID][primaryOrReplica][]uint64,\n// where the int slices are lists of shards.\nfunc (c *cluster) shardDistributionByIndex(indexName string) map[string]map[string][]uint64 {\n\tdist := make(map[string]map[string][]uint64)\n\n\tfor _, node := range c.noder.Nodes() {\n\t\tnodeDist := make(map[string][]uint64)\n\t\tnodeDist[\"primary-shards\"] = make([]uint64, 0)\n\t\tnodeDist[\"replica-shards\"] = make([]uint64, 0)\n\t\tdist[node.ID] = nodeDist\n\t}\n\n\tindex := c.holder.Index(indexName)\n\tavailable := index.AvailableShards(includeRemote).Slice()\n\n\tc.mu.RLock()\n\tdefer c.mu.RUnlock()\n\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := c.NewSnapshot()\n\n\tfor _, shard := range available {\n\t\tp := snap.ShardToShardPartition(indexName, shard)\n\t\tnodes := snap.PartitionNodes(p)\n\t\tdist[nodes[0].ID][\"primary-shards\"] = append(dist[nodes[0].ID][\"primary-shards\"], shard)\n\t\tfor k := 1; k < len(nodes); k++ {\n\t\t\tdist[nodes[k].ID][\"replica-shards\"] = append(dist[nodes[k].ID][\"replica-shards\"], shard)\n\t\t}\n\t}\n\n\treturn dist\n}\n\nfunc (c *cluster) close() error {\n\t// Notify goroutines of closing and wait for completion.\n\tclose(c.closing)\n\tc.wg.Wait()\n\n\treturn nil\n}\n\n// PrimaryReplicaNode returns the node listed before the current node in c.Nodes.\n// This is different than \"previous node\" as the first node always returns nil.\nfunc (c *cluster) PrimaryReplicaNode() *disco.Node {\n\tc.mu.RLock()\n\tdefer c.mu.RUnlock()\n\treturn c.unprotectedPrimaryReplicaNode()\n}\n\nfunc (c *cluster) unprotectedPrimaryReplicaNode() *disco.Node {\n\tpos := c.nodePositionByID(c.Node.ID)\n\tif pos <= 0 {\n\t\treturn nil\n\t}\n\tcNodes := c.noder.Nodes()\n\treturn cNodes[pos-1]\n}\n\n// TODO: remove this when it is no longer used\nfunc (c *cluster) translateFieldKeys(ctx context.Context, field *Field, keys []string, writable bool) ([]uint64, error) {\n\tvar trans map[string]uint64\n\tvar err error\n\tif writable {\n\t\ttrans, err = c.createFieldKeys(ctx, field, keys...)\n\t} else {\n\t\ttrans, err = c.findFieldKeys(ctx, field, keys...)\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tids := make([]uint64, len(keys))\n\tfor i, key := range keys {\n\t\tid, ok := trans[key]\n\t\tif !ok {\n\t\t\treturn nil, ErrTranslatingKeyNotFound\n\t\t}\n\n\t\tids[i] = id\n\t}\n\n\treturn ids, nil\n}\n\nfunc (c *cluster) findFieldKeys(ctx context.Context, field *Field, keys ...string) (map[string]uint64, error) {\n\tif idx := field.ForeignIndex(); idx != \"\" {\n\t\t// The field uses foreign index keys.\n\t\t// Therefore, the field keys are actually column keys on a different index.\n\t\treturn c.findIndexKeys(ctx, idx, keys...)\n\t}\n\tif !field.Keys() {\n\t\treturn nil, errors.Errorf(\"cannot find keys on unkeyed field %q\", field.Name())\n\t}\n\n\t// Attempt to find the keys locally.\n\tlocalTranslations, err := field.TranslateStore().FindKeys(keys...)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"translating field(%s/%s) keys(%v) locally\", field.Index(), field.Name(), keys)\n\t}\n\n\t// Check for missing keys.\n\tvar missing []string\n\tif len(keys) > len(localTranslations) {\n\t\t// There are either duplicate keys or missing keys.\n\t\t// This should work either way.\n\t\tmissing = make([]string, 0, len(keys)-len(localTranslations))\n\t\tfor _, k := range keys {\n\t\t\t_, found := localTranslations[k]\n\t\t\tif !found {\n\t\t\t\tmissing = append(missing, k)\n\t\t\t}\n\t\t}\n\t} else if len(localTranslations) > len(keys) {\n\t\tpanic(fmt.Sprintf(\"more translations than keys! translation count=%v, key count=%v\", len(localTranslations), len(keys)))\n\t}\n\tif len(missing) == 0 {\n\t\t// All keys were available locally.\n\t\treturn localTranslations, nil\n\t}\n\n\t// It is possible that the missing keys exist, but have not been synced to the local replica.\n\tprimary := c.primaryNode()\n\tif primary == nil {\n\t\treturn nil, errors.Errorf(\"translating field(%s/%s) keys(%v) - cannot find primary node\", field.Index(), field.Name(), keys)\n\t}\n\tif c.Node.ID == primary.ID {\n\t\t// The local copy is the authoritative copy.\n\t\treturn localTranslations, nil\n\t}\n\n\t// Forward the missing keys to the primary.\n\t// The primary has the authoritative copy.\n\tremoteTranslations, err := c.InternalClient.FindFieldKeysNode(ctx, &primary.URI, field.Index(), field.Name(), missing...)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"translating field(%s/%s) keys(%v) remotely\", field.Index(), field.Name(), keys)\n\t}\n\n\t// Merge the remote translations into the local translations.\n\ttranslations := localTranslations\n\tfor key, id := range remoteTranslations {\n\t\ttranslations[key] = id\n\t}\n\n\treturn translations, nil\n}\n\nfunc (c *cluster) appendFieldKeysWriteLog(ctx context.Context, qtid dax.QualifiedTableID, fieldName dax.FieldName, translations map[string]uint64) error {\n\t// TODO move marshaling somewhere more centralized and less... explicitly json-y\n\tmsg := computer.FieldKeyMap{\n\t\tTableKey:   qtid.Key(),\n\t\tField:      fieldName,\n\t\tStringToID: translations,\n\t}\n\n\tb, err := json.Marshal(msg)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"marshalling field key map to json\")\n\t}\n\tresource := c.serverlessStorage.GetFieldKeyResource(qtid, fieldName)\n\terr = resource.Append(b)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"appending field keys\")\n\t}\n\treturn nil\n\n}\n\nfunc (c *cluster) appendTableKeysWriteLog(ctx context.Context, qtid dax.QualifiedTableID, partition dax.PartitionNum, translations map[string]uint64) error {\n\tmsg := computer.PartitionKeyMap{\n\t\tTableKey:   qtid.Key(),\n\t\tPartition:  partition,\n\t\tStringToID: translations,\n\t}\n\n\tb, err := json.Marshal(msg)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"marshalling partition key map to json\")\n\t}\n\n\tresource := c.serverlessStorage.GetTableKeyResource(qtid, partition)\n\treturn errors.Wrap(resource.Append(b), \"appending table keys\")\n\n}\n\nfunc (c *cluster) createFieldKeys(ctx context.Context, field *Field, keys ...string) (map[string]uint64, error) {\n\tif idx := field.ForeignIndex(); idx != \"\" {\n\t\t// The field uses foreign index keys.\n\t\t// Therefore, the field keys are actually column keys on a different index.\n\t\treturn c.createIndexKeys(ctx, idx, keys...)\n\t}\n\n\tif !field.Keys() {\n\t\treturn nil, errors.Errorf(\"cannot create keys on unkeyed field %q\", field.Name())\n\t}\n\n\t// The primary is the only node that can create field keys, since it owns the authoritative copy.\n\tprimary := c.primaryNode()\n\tif primary == nil {\n\t\treturn nil, errors.Errorf(\"translating field(%s/%s) keys(%v) - cannot find primary node\", field.Index(), field.Name(), keys)\n\t}\n\tif c.Node.ID == primary.ID {\n\t\ttranslations, err := field.TranslateStore().CreateKeys(keys...)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Errorf(\"creating field(%s/%s) keys(%v)\", field.Index(), field.Name(), keys)\n\t\t}\n\n\t\t// If this is not a DAX compute node, bail early; there's no need to\n\t\t// send data to the write log.\n\t\tif !c.isComputeNode {\n\t\t\treturn translations, nil\n\t\t}\n\n\t\t// Send to write log.\n\t\ttkey := dax.TableKey(field.Index())\n\t\tqtid := tkey.QualifiedTableID()\n\t\tfieldName := dax.FieldName(field.Name())\n\t\terr = c.appendFieldKeysWriteLog(ctx, qtid, fieldName, translations)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"appending to write log\")\n\t\t}\n\n\t\treturn translations, nil\n\t}\n\n\t// Attempt to find the keys locally.\n\t// They cannot be created locally, but skipping keys that exist can reduce network usage.\n\tlocalTranslations, err := field.TranslateStore().FindKeys(keys...)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"translating field(%s/%s) keys(%v) locally\", field.Index(), field.Name(), keys)\n\t}\n\n\t// Check for missing keys.\n\tvar missing []string\n\tif len(keys) > len(localTranslations) {\n\t\t// There are either duplicate keys or missing keys.\n\t\t// This should work either way.\n\t\tmissing = make([]string, 0, len(keys)-len(localTranslations))\n\t\tfor _, k := range keys {\n\t\t\t_, found := localTranslations[k]\n\t\t\tif !found {\n\t\t\t\tmissing = append(missing, k)\n\t\t\t}\n\t\t}\n\t} else if len(localTranslations) > len(keys) {\n\t\tpanic(fmt.Sprintf(\"more translations than keys! translation count=%v, key count=%v\", len(localTranslations), len(keys)))\n\t}\n\tif len(missing) == 0 {\n\t\t// All keys exist locally.\n\t\t// There is no need to create anything.\n\t\treturn localTranslations, nil\n\t}\n\n\t// Forward the missing keys to the primary to be created.\n\tremoteTranslations, err := c.InternalClient.CreateFieldKeysNode(ctx, &primary.URI, field.Index(), field.Name(), missing...)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"translating field(%s/%s) keys(%v) remotely\", field.Index(), field.Name(), keys)\n\t}\n\n\t// Merge the remote translations into the local translations.\n\ttranslations := localTranslations\n\tfor key, id := range remoteTranslations {\n\t\ttranslations[key] = id\n\t}\n\n\treturn translations, nil\n}\n\nfunc (c *cluster) matchField(ctx context.Context, field *Field, like string) ([]uint64, error) {\n\t// The primary is the only node that can match field keys, since it is the only node with all of the keys.\n\tprimary := c.primaryNode()\n\tif primary == nil {\n\t\treturn nil, errors.Errorf(\"matching field(%s/%s) like %q - cannot find primary node\", field.Index(), field.Name(), like)\n\t}\n\tif c.Node.ID == primary.ID {\n\t\t// The local copy is the authoritative copy.\n\t\tplan := planLike(like)\n\t\tstore := field.TranslateStore()\n\t\tif store == nil {\n\t\t\treturn nil, ErrTranslateStoreNotFound\n\t\t}\n\t\treturn field.TranslateStore().Match(func(key []byte) bool {\n\t\t\treturn matchLike(key, plan...)\n\t\t})\n\t}\n\n\t// Forward the request to the primary.\n\treturn c.InternalClient.MatchFieldKeysNode(ctx, &primary.URI, field.Index(), field.Name(), like)\n}\n\nfunc (c *cluster) translateFieldIDs(ctx context.Context, field *Field, ids map[uint64]struct{}) (map[uint64]string, error) {\n\tidList := make([]uint64, len(ids))\n\t{\n\t\ti := 0\n\t\tfor id := range ids {\n\t\t\tidList[i] = id\n\t\t\ti++\n\t\t}\n\t}\n\n\tkeyList, err := c.translateFieldListIDs(ctx, field, idList)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tmapped := make(map[uint64]string, len(idList))\n\tfor i, key := range keyList {\n\t\tmapped[idList[i]] = key\n\t}\n\treturn mapped, nil\n}\n\nfunc (c *cluster) translateFieldListIDs(ctx context.Context, field *Field, ids []uint64) (keys []string, err error) {\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := c.NewSnapshot()\n\n\tprimary := snap.PrimaryFieldTranslationNode()\n\tif primary == nil {\n\t\treturn nil, errors.Errorf(\"translating field(%s/%s) ids(%v) - cannot find primary node\", field.Index(), field.Name(), ids)\n\t}\n\n\tif c.Node.ID == primary.ID {\n\t\tstore := field.TranslateStore()\n\t\tif store == nil {\n\t\t\treturn nil, ErrTranslateStoreNotFound\n\t\t}\n\t\tkeys, err = field.TranslateStore().TranslateIDs(ids)\n\t} else {\n\t\tkeys, err = c.InternalClient.TranslateIDsNode(ctx, &primary.URI, field.Index(), field.Name(), ids)\n\t}\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"translating field(%s/%s) ids(%v)\", field.Index(), field.Name(), ids)\n\t}\n\n\treturn keys, err\n}\n\n// TODO: remove this when it is no longer used\nfunc (c *cluster) translateIndexKey(ctx context.Context, indexName string, key string, writable bool) (uint64, error) {\n\tkeyMap, err := c.translateIndexKeySet(ctx, indexName, map[string]struct{}{key: {}}, writable)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn keyMap[key], nil\n}\n\n// TODO: remove this when it is no longer used\nfunc (c *cluster) translateIndexKeys(ctx context.Context, indexName string, keys []string, writable bool) ([]uint64, error) {\n\tvar trans map[string]uint64\n\tvar err error\n\tif writable {\n\t\ttrans, err = c.createIndexKeys(ctx, indexName, keys...)\n\t} else {\n\t\ttrans, err = c.findIndexKeys(ctx, indexName, keys...)\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tids := make([]uint64, len(keys))\n\tfor i, key := range keys {\n\t\tid, ok := trans[key]\n\t\tif !ok {\n\t\t\treturn nil, ErrTranslatingKeyNotFound\n\t\t}\n\n\t\tids[i] = id\n\t}\n\n\treturn ids, nil\n}\n\n// TODO: remove this when it is no longer used\nfunc (c *cluster) translateIndexKeySet(ctx context.Context, indexName string, keySet map[string]struct{}, writable bool) (map[string]uint64, error) {\n\tkeys := make([]string, 0, len(keySet))\n\tfor key := range keySet {\n\t\tkeys = append(keys, key)\n\t}\n\n\tif writable {\n\t\treturn c.createIndexKeys(ctx, indexName, keys...)\n\t}\n\n\ttrans, err := c.findIndexKeys(ctx, indexName, keys...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif len(trans) != len(keys) {\n\t\treturn nil, ErrTranslatingKeyNotFound\n\t}\n\n\treturn trans, nil\n}\n\nfunc (c *cluster) findIndexKeys(ctx context.Context, indexName string, keys ...string) (map[string]uint64, error) {\n\tdone := ctx.Done()\n\n\tidx := c.holder.Index(indexName)\n\tif idx == nil {\n\t\treturn nil, ErrIndexNotFound\n\t}\n\tif !idx.Keys() {\n\t\treturn nil, errors.Errorf(\"cannot find keys on unkeyed index %q\", indexName)\n\t}\n\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := c.NewSnapshot()\n\n\t// Split keys by partition.\n\tkeysByPartition := make(map[int][]string, c.partitionN)\n\tfor _, key := range keys {\n\t\tpartitionID := snap.KeyToKeyPartition(indexName, key)\n\t\tkeysByPartition[partitionID] = append(keysByPartition[partitionID], key)\n\n\t\t// This node only handles keys for the partition(s) that it owns.\n\t\tif c.isComputeNode {\n\t\t\tif !intInPartitions(partitionID, idx.translatePartitions) {\n\t\t\t\treturn nil, errors.Errorf(\"cannot find key on this partition: %s, %d\", key, partitionID)\n\t\t\t}\n\t\t}\n\t}\n\n\t// TODO: use local replicas to short-circuit network traffic\n\n\t// Group keys by node.\n\tkeysByNode := make(map[*disco.Node][]string)\n\tfor partitionID, keys := range keysByPartition {\n\t\t// Find the primary node for this partition.\n\t\tprimary := snap.PrimaryPartitionNode(partitionID)\n\t\tif primary == nil {\n\t\t\treturn nil, errors.Errorf(\"translating index(%s) keys(%v) on partition(%d) - cannot find primary node\", indexName, keys, partitionID)\n\t\t}\n\n\t\tif c.Node.ID == primary.ID {\n\t\t\t// The partition is local.\n\t\t\tcontinue\n\t\t}\n\n\t\t// Group the partition to be processed remotely.\n\t\tkeysByNode[primary] = append(keysByNode[primary], keys...)\n\n\t\t// Delete remote keys from the by-partition map so that it can be used for local translation.\n\t\tdelete(keysByPartition, partitionID)\n\t}\n\n\t// Start translating keys remotely.\n\t// On child calls, there are no remote results since we were only sent the keys that we own.\n\tremoteResults := make(chan map[string]uint64, len(keysByNode))\n\tvar g errgroup.Group\n\tdefer g.Wait() //nolint:errcheck\n\tfor node, keys := range keysByNode {\n\t\tnode, keys := node, keys\n\n\t\tg.Go(func() error {\n\t\t\ttranslations, err := c.InternalClient.FindIndexKeysNode(ctx, &node.URI, indexName, keys...)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Wrapf(err, \"translating index(%s) keys(%v) on node %s\", indexName, keys, node.ID)\n\t\t\t}\n\n\t\t\tremoteResults <- translations\n\t\t\treturn nil\n\t\t})\n\t}\n\n\t// Translate local keys.\n\ttranslations := make(map[string]uint64)\n\tfor partitionID, keys := range keysByPartition {\n\t\t// Handle cancellation.\n\t\tselect {\n\t\tcase <-done:\n\t\t\treturn nil, ctx.Err()\n\t\tdefault:\n\t\t}\n\n\t\t// Find the keys within the partition.\n\t\tt, err := idx.TranslateStore(partitionID).FindKeys(keys...)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrapf(err, \"translating index(%s) keys(%v) on partition(%d)\", idx.Name(), keys, partitionID)\n\t\t}\n\n\t\t// Merge the translations from this partition.\n\t\tfor key, id := range t {\n\t\t\ttranslations[key] = id\n\t\t}\n\t}\n\n\t// Wait for remote key sets.\n\tif err := g.Wait(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Merge the translations.\n\t// All data should have been written to here while we waited.\n\t// Closing the channel prevents the range from blocking.\n\tclose(remoteResults)\n\tfor t := range remoteResults {\n\t\tfor key, id := range t {\n\t\t\ttranslations[key] = id\n\t\t}\n\t}\n\treturn translations, nil\n}\n\nfunc (c *cluster) createIndexKeys(ctx context.Context, indexName string, keys ...string) (map[string]uint64, error) {\n\t// Check for early cancellation.\n\tdone := ctx.Done()\n\tselect {\n\tcase <-done:\n\t\treturn nil, ctx.Err()\n\tdefault:\n\t}\n\n\tidx := c.holder.Index(indexName)\n\tif idx == nil {\n\t\treturn nil, ErrIndexNotFound\n\t}\n\n\tif !idx.keys {\n\t\treturn nil, errors.Errorf(\"cannot create keys on unkeyed index %q\", indexName)\n\t}\n\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := c.NewSnapshot()\n\n\t// Split keys by partition.\n\tkeysByPartition := make(map[int][]string, c.partitionN)\n\tfor _, key := range keys {\n\t\tpartitionID := snap.KeyToKeyPartition(indexName, key)\n\t\tkeysByPartition[partitionID] = append(keysByPartition[partitionID], key)\n\n\t\t// This node only handles keys for the partition(s) that it owns.\n\t\tif c.isComputeNode {\n\t\t\tif !intInPartitions(partitionID, idx.translatePartitions) {\n\t\t\t\tlog.Printf(\"cannot create key on this partition: %s, %d\", key, partitionID)\n\t\t\t\treturn nil, errors.Errorf(\"cannot create key on this partition: %s, %d\", key, partitionID)\n\t\t\t}\n\t\t}\n\t}\n\n\t// TODO: use local replicas to short-circuit network traffic\n\n\t// Group keys by node.\n\t// Delete remote keys from the by-partition map so that it can be used for local translation.\n\tkeysByNode := make(map[*disco.Node][]string)\n\tfor partitionID, keys := range keysByPartition {\n\t\t// Find the primary node for this partition.\n\t\tprimary := snap.PrimaryPartitionNode(partitionID)\n\t\tif primary == nil {\n\t\t\treturn nil, errors.Errorf(\"translating index(%s) keys(%v) on partition(%d) - cannot find primary node\", indexName, keys, partitionID)\n\t\t}\n\n\t\tif c.Node.ID == primary.ID {\n\t\t\t// The partition is local.\n\t\t\tcontinue\n\t\t}\n\n\t\t// Group the partition to be processed remotely.\n\t\tkeysByNode[primary] = append(keysByNode[primary], keys...)\n\t\tdelete(keysByPartition, partitionID)\n\t}\n\n\ttranslateResults := make(chan map[string]uint64, len(keysByNode)+len(keysByPartition))\n\tvar g errgroup.Group\n\tdefer g.Wait() //nolint:errcheck\n\n\t// Start translating keys remotely.\n\t// On child calls, there are no remote results since we were only sent the keys that we own.\n\tfor node, keys := range keysByNode {\n\t\tnode, keys := node, keys\n\n\t\tg.Go(func() error {\n\t\t\ttranslations, err := c.InternalClient.CreateIndexKeysNode(ctx, &node.URI, indexName, keys...)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Wrapf(err, \"translating index(%s) keys(%v) on node %s\", indexName, keys, node.ID)\n\t\t\t}\n\n\t\t\ttranslateResults <- translations\n\t\t\treturn nil\n\t\t})\n\t}\n\n\t// Translate local keys.\n\t// TODO: make this less horrible (why fsync why?????)\n\t// \t\tThis is kinda terrible because each goroutine does an fsync, thus locking up an entire OS thread.\n\t// \t\tAHHHHHHHHHHHHHHHHHH\n\tfor partitionID, keys := range keysByPartition {\n\t\tpartitionID, keys := partitionID, keys\n\n\t\tg.Go(func() error {\n\t\t\t// Handle cancellation.\n\t\t\tselect {\n\t\t\tcase <-done:\n\t\t\t\treturn ctx.Err()\n\t\t\tdefault:\n\t\t\t}\n\n\t\t\ttranslations, err := idx.TranslateStore(partitionID).CreateKeys(keys...)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Wrapf(err, \"translating index(%s) keys(%v) on partition(%d)\", idx.Name(), keys, partitionID)\n\t\t\t}\n\n\t\t\ttranslateResults <- translations\n\n\t\t\t// If this is not a DAX compute node, bail early; there's no need to\n\t\t\t// send data to the write log.\n\t\t\tif !c.isComputeNode {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\t// Send to write log.\n\t\t\ttkey := dax.TableKey(idx.Name())\n\t\t\tqtid := tkey.QualifiedTableID()\n\t\t\tpartitionNum := dax.PartitionNum(partitionID)\n\t\t\treturn c.appendTableKeysWriteLog(ctx, qtid, partitionNum, translations)\n\t\t})\n\t}\n\n\t// Wait for remote key sets.\n\tif err := g.Wait(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Merge the translations.\n\t// All data should have been written to here while we waited.\n\t// Closing the channel prevents the range from blocking.\n\ttranslations := make(map[string]uint64, len(keys))\n\tclose(translateResults)\n\tfor t := range translateResults {\n\t\tfor key, id := range t {\n\t\t\ttranslations[key] = id\n\t\t}\n\t}\n\treturn translations, nil\n}\n\nfunc (c *cluster) translateIndexIDs(ctx context.Context, indexName string, ids []uint64) ([]string, error) {\n\tidSet := make(map[uint64]struct{})\n\tfor _, id := range ids {\n\t\tidSet[id] = struct{}{}\n\t}\n\n\tidMap, err := c.translateIndexIDSet(ctx, indexName, idSet)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tkeys := make([]string, len(ids))\n\tfor i := range ids {\n\t\tkeys[i] = idMap[ids[i]]\n\t}\n\treturn keys, nil\n}\n\nfunc (c *cluster) translateIndexIDSet(ctx context.Context, indexName string, idSet map[uint64]struct{}) (map[uint64]string, error) {\n\tidMap := make(map[uint64]string, len(idSet))\n\n\tindex := c.holder.Index(indexName)\n\tif index == nil {\n\t\treturn nil, newNotFoundError(ErrIndexNotFound, indexName)\n\t}\n\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := c.NewSnapshot()\n\n\t// Split ids by partition.\n\tidsByPartition := make(map[int][]uint64, c.partitionN)\n\tfor id := range idSet {\n\t\tpartitionID := snap.IDToShardPartition(indexName, id)\n\t\t// This node only handles keys for the partition(s) that it owns.\n\t\tif c.isComputeNode {\n\t\t\tif !intInPartitions(partitionID, index.translatePartitions) {\n\t\t\t\treturn nil, errors.Errorf(\"cannot find id on this partition: %d, %d\", id, partitionID)\n\t\t\t}\n\t\t}\n\t\tidsByPartition[partitionID] = append(idsByPartition[partitionID], id)\n\t}\n\n\t// Translate ids by partition.\n\tvar g errgroup.Group\n\tvar mu sync.Mutex\n\tfor partitionID := range idsByPartition {\n\t\tpartitionID := partitionID\n\t\tids := idsByPartition[partitionID]\n\n\t\tg.Go(func() (err error) {\n\t\t\tvar keys []string\n\n\t\t\tprimary := snap.PrimaryPartitionNode(partitionID)\n\t\t\tif primary == nil {\n\t\t\t\treturn errors.Errorf(\"translating index(%s) ids(%v) on partition(%d) - cannot find primary node\", indexName, ids, partitionID)\n\t\t\t}\n\n\t\t\tif c.Node.ID == primary.ID {\n\t\t\t\tkeys, err = index.TranslateStore(partitionID).TranslateIDs(ids)\n\t\t\t} else {\n\t\t\t\tkeys, err = c.InternalClient.TranslateIDsNode(ctx, &primary.URI, indexName, \"\", ids)\n\t\t\t}\n\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Wrapf(err, \"translating index(%s) ids(%v) on partition(%d)\", indexName, ids, partitionID)\n\t\t\t}\n\n\t\t\tmu.Lock()\n\t\t\tfor i, id := range ids {\n\t\t\t\tidMap[id] = keys[i]\n\t\t\t}\n\t\t\tmu.Unlock()\n\n\t\t\treturn nil\n\t\t})\n\t}\n\tif err := g.Wait(); err != nil {\n\t\treturn nil, err\n\t}\n\treturn idMap, nil\n}\n\nfunc (c *cluster) NewSnapshot() *disco.ClusterSnapshot {\n\treturn disco.NewClusterSnapshot(c.noder, c.Hasher, c.partitionAssigner, c.ReplicaN)\n}\n\n// ClusterStatus describes the status of the cluster including its\n// state and node topology.\ntype ClusterStatus struct {\n\tClusterID string\n\tState     string\n\tNodes     []*disco.Node\n\tSchema    *Schema\n}\n\n// Schema contains information about indexes and their configuration.\ntype Schema struct {\n\tIndexes []*IndexInfo `json:\"indexes\"`\n}\n\n// CreateShardMessage is an internal message indicating shard creation.\ntype CreateShardMessage struct {\n\tIndex string\n\tField string\n\tShard uint64\n}\n\n// CreateIndexMessage is an internal message indicating index creation.\ntype CreateIndexMessage struct {\n\tIndex     string\n\tCreatedAt int64\n\tOwner     string\n\n\tMeta IndexOptions\n}\n\n// DeleteIndexMessage is an internal message indicating index deletion.\ntype DeleteIndexMessage struct {\n\tIndex string\n}\n\n// CreateFieldMessage is an internal message indicating field creation.\ntype CreateFieldMessage struct {\n\tIndex     string\n\tField     string\n\tCreatedAt int64\n\tOwner     string\n\tMeta      *FieldOptions\n}\n\n// UpdateFieldMessage represents a change to an existing field. The\n// CreateFieldMessage holds the changed field, while the update shows\n// the change that was made.\ntype UpdateFieldMessage struct {\n\tCreateFieldMessage CreateFieldMessage\n\tUpdate             FieldUpdate\n}\n\n// DeleteFieldMessage is an internal message indicating field deletion.\ntype DeleteFieldMessage struct {\n\tIndex string\n\tField string\n}\n\n// DeleteAvailableShardMessage is an internal message indicating available shard deletion.\ntype DeleteAvailableShardMessage struct {\n\tIndex   string\n\tField   string\n\tShardID uint64\n}\n\n// CreateViewMessage is an internal message indicating view creation.\ntype CreateViewMessage struct {\n\tIndex string\n\tField string\n\tView  string\n}\n\n// DeleteViewMessage is an internal message indicating view deletion.\ntype DeleteViewMessage struct {\n\tIndex string\n\tField string\n\tView  string\n}\n\n// NodeStateMessage is an internal message for broadcasting a node's state.\ntype NodeStateMessage struct {\n\tNodeID string `protobuf:\"bytes,1,opt,name=NodeID,proto3\" json:\"NodeID,omitempty\"`\n\tState  string `protobuf:\"bytes,2,opt,name=State,proto3\" json:\"State,omitempty\"`\n}\n\n// NodeStatus is an internal message representing the contents of a node.\ntype NodeStatus struct {\n\tNode    *disco.Node\n\tIndexes []*IndexStatus\n\tSchema  *Schema\n}\n\n// IndexStatus is an internal message representing the contents of an index.\ntype IndexStatus struct {\n\tName      string\n\tCreatedAt int64\n\tFields    []*FieldStatus\n}\n\n// FieldStatus is an internal message representing the contents of a field.\ntype FieldStatus struct {\n\tName            string\n\tCreatedAt       int64\n\tAvailableShards *roaring.Bitmap\n}\n\n// RecalculateCaches is an internal message for recalculating all caches\n// within a holder.\ntype RecalculateCaches struct{}\n\n// Transaction Actions\nconst (\n\tTRANSACTION_START    = \"start\"\n\tTRANSACTION_FINISH   = \"finish\"\n\tTRANSACTION_VALIDATE = \"validate\"\n)\n\ntype TransactionMessage struct {\n\tTransaction *Transaction\n\tAction      string\n}\n\nfunc intInPartitions(i int, s dax.PartitionNums) bool {\n\tfor _, a := range s {\n\t\tif int(a) == i {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// DeleteDataframeMessage is an internal message indicating dataframe deletion.\ntype DeleteDataframeMessage struct {\n\tIndex string\n}\n"
        },
        {
          "name": "cluster_internal_test.go",
          "type": "blob",
          "size": 5.318359375,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"fmt\"\n\t\"math/rand\"\n\t\"reflect\"\n\t\"testing\"\n\t\"testing/quick\"\n\n\t\"github.com/featurebasedb/featurebase/v3/disco\"\n\tpnet \"github.com/featurebasedb/featurebase/v3/net\"\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n)\n\n// Ensure the cluster can fairly distribute partitions across the nodes.\nfunc TestCluster_Owners(t *testing.T) {\n\tc := cluster{\n\t\tnoder: disco.NewLocalNoder([]*disco.Node{\n\t\t\t{URI: NewTestURIFromHostPort(\"serverA\", 1000)},\n\t\t\t{URI: NewTestURIFromHostPort(\"serverB\", 1000)},\n\t\t\t{URI: NewTestURIFromHostPort(\"serverC\", 1000)},\n\t\t}),\n\t\tHasher:   &disco.Jmphasher{},\n\t\tReplicaN: 2,\n\t}\n\n\tcNodes := c.noder.Nodes()\n\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := c.NewSnapshot()\n\n\tassigned := make(map[int]int)\n\tfor i := 0; i < 256; i++ {\n\t\tnodes := snap.PartitionNodes(i)\n\t\tfor _, node := range nodes {\n\t\t\tfor j, n := range cNodes {\n\t\t\t\tif n == node {\n\t\t\t\t\tassigned[j]++\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\texpected := float64((256.0 * 2) / 3) // each partition is on two nodes, there's three nodes\n\tfor k, v := range assigned {\n\t\tratio := float64(v) / expected\n\t\t// Empirically, we expect 167/171/174\n\t\tif ratio < 0.97 || ratio > 1.03 {\n\t\t\tt.Fatalf(\"node %d has %d assigned partitions, expected about %.1f\", k, v, expected)\n\t\t}\n\t}\n}\n\n// Ensure the partitioner can assign a fragment to a partition.\nfunc TestCluster_Partition(t *testing.T) {\n\tif err := quick.Check(func(index string, shard uint64, partitionN int) bool {\n\t\tc := newCluster()\n\t\tc.partitionN = partitionN\n\n\t\tpartitionID := disco.ShardToShardPartition(index, shard, partitionN)\n\t\tif partitionID < 0 || partitionID >= partitionN {\n\t\t\tt.Errorf(\"partition out of range: shard=%d, p=%d, n=%d\", shard, partitionID, partitionN)\n\t\t}\n\n\t\treturn true\n\t}, &quick.Config{\n\t\tValues: func(values []reflect.Value, rand *rand.Rand) {\n\t\t\tvalues[0], _ = quick.Value(reflect.TypeOf(\"\"), rand)\n\t\t\tvalues[1] = reflect.ValueOf(uint64(rand.Uint32()))\n\t\t\tvalues[2] = reflect.ValueOf(rand.Intn(1000) + 1)\n\t\t},\n\t}); err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\n// Ensure the hasher can hash correctly.\nfunc TestHasher(t *testing.T) {\n\tfor _, tt := range []struct {\n\t\tkey    uint64\n\t\tbucket []int\n\t}{\n\t\t// Generated from the reference C++ code\n\t\t{0, []int{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}},\n\t\t{1, []int{0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 17, 17}},\n\t\t{0xdeadbeef, []int{0, 1, 2, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 16, 16, 16}},\n\t\t{0x0ddc0ffeebadf00d, []int{0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 15, 15, 15, 15}},\n\t} {\n\t\tfor i, v := range tt.bucket {\n\t\t\thasher := &disco.Jmphasher{}\n\t\t\tif got := hasher.Hash(tt.key, i+1); got != v {\n\t\t\t\tt.Errorf(\"hash(%v,%v)=%v, want %v\", tt.key, i+1, got, v)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Ensure ContainsShards can find the actual shard list for node and index.\nfunc TestCluster_ContainsShards(t *testing.T) {\n\tc := NewTestCluster(t, 5)\n\tc.ReplicaN = 3\n\tcNodes := c.noder.Nodes()\n\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := c.NewSnapshot()\n\n\tavailableShards := roaring.NewBitmap(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\tnodeCounts := make(map[uint64]int)\n\tfor _, n := range cNodes {\n\t\tshards := snap.ContainsShards(\"test\", availableShards, n)\n\t\tfor _, shard := range shards {\n\t\t\tnodeCounts[shard]++\n\t\t}\n\t}\n\tfor shard, count := range nodeCounts {\n\t\tif count != 3 {\n\t\t\tt.Fatalf(\"shard %d on %d nodes, expected 3\", shard, count)\n\t\t}\n\t}\n}\n\nfunc TestCluster_Nodes(t *testing.T) {\n\tconst urisCount = 4\n\tvar uris []pnet.URI\n\tarbitraryPorts := []int{17384, 17385, 17386, 17387}\n\tfor i := 0; i < urisCount; i++ {\n\t\turis = append(uris, NewTestURIFromHostPort(fmt.Sprintf(\"node%d\", i), uint16(arbitraryPorts[i])))\n\t}\n\n\tnode0 := &disco.Node{ID: \"node0\", URI: uris[0]}\n\tnode1 := &disco.Node{ID: \"node1\", URI: uris[1]}\n\tnode2 := &disco.Node{ID: \"node2\", URI: uris[2]}\n\tnode3 := &disco.Node{ID: \"node3\", URI: uris[3]}\n\n\tnodes := []*disco.Node{node0, node1, node2}\n\n\tt.Run(\"NodeIDs\", func(t *testing.T) {\n\t\tactual := disco.Nodes(nodes).IDs()\n\t\texpected := []string{node0.ID, node1.ID, node2.ID}\n\t\tif !reflect.DeepEqual(actual, expected) {\n\t\t\tt.Errorf(\"expected: %v, but got: %v\", expected, actual)\n\t\t}\n\t})\n\n\tt.Run(\"Filter\", func(t *testing.T) {\n\t\tactual := disco.Nodes(disco.Nodes(nodes).Filter(nodes[1])).URIs()\n\t\texpected := []pnet.URI{uris[0], uris[2]}\n\t\tif !reflect.DeepEqual(actual, expected) {\n\t\t\tt.Errorf(\"expected: %v, but got: %v\", expected, actual)\n\t\t}\n\t})\n\n\tt.Run(\"FilterURI\", func(t *testing.T) {\n\t\tactual := disco.Nodes(disco.Nodes(nodes).FilterURI(uris[1])).URIs()\n\t\texpected := []pnet.URI{uris[0], uris[2]}\n\t\tif !reflect.DeepEqual(actual, expected) {\n\t\t\tt.Errorf(\"expected: %v, but got: %v\", expected, actual)\n\t\t}\n\t})\n\n\tt.Run(\"Contains\", func(t *testing.T) {\n\t\tactualTrue := disco.Nodes(nodes).Contains(node1)\n\t\tactualFalse := disco.Nodes(nodes).Contains(node3)\n\t\tif !reflect.DeepEqual(actualTrue, true) {\n\t\t\tt.Errorf(\"expected: %v, but got: %v\", true, actualTrue)\n\t\t}\n\t\tif !reflect.DeepEqual(actualFalse, false) {\n\t\t\tt.Errorf(\"expected: %v, but got: %v\", false, actualTrue)\n\t\t}\n\t})\n\n\tt.Run(\"Clone\", func(t *testing.T) {\n\t\tclone := disco.Nodes(nodes).Clone()\n\t\tactual := disco.Nodes(clone).URIs()\n\t\texpected := []pnet.URI{uris[0], uris[1], uris[2]}\n\t\tif !reflect.DeepEqual(actual, expected) {\n\t\t\tt.Errorf(\"expected: %v, but got: %v\", expected, actual)\n\t\t}\n\t})\n}\n"
        },
        {
          "name": "cmd",
          "type": "tree",
          "content": null
        },
        {
          "name": "const_amd64.go",
          "type": "blob",
          "size": 0.177734375,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\n//go:build amd64\n// +build amd64\n\npackage pilosa\n\nconst TxInitialMmapSize = 4 << 30 // 4GB\n"
        },
        {
          "name": "const_other.go",
          "type": "blob",
          "size": 0.2314453125,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\n//go:build !amd64\n// +build !amd64\n\npackage pilosa\n\n// this is a stubbed out file to let 386/arm build.\n\nconst TxInitialMmapSize = 1 << 30 // 1GB\n"
        },
        {
          "name": "context",
          "type": "tree",
          "content": null
        },
        {
          "name": "ctl",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataframe_test.go",
          "type": "blob",
          "size": 7.22265625,
          "content": "package pilosa_test\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"math\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/apache/arrow/go/v10/arrow\"\n\tpilosa \"github.com/featurebasedb/featurebase/v3\"\n\t\"github.com/featurebasedb/featurebase/v3/server\"\n\t\"github.com/featurebasedb/featurebase/v3/test\"\n)\n\nfunc TestExecutor_Apply(t *testing.T) {\n\tc := test.MustRunCluster(t, 1, []server.CommandOption{\n\t\tserver.OptCommandServerOptions(\n\t\t\tpilosa.OptServerIsDataframeEnabled(true),\n\t\t),\n\t})\n\tdefer c.Close()\n\tindexName := \"ti\"\n\tfieldName := \"f\"\n\tctx := context.Background()\n\tapi := c.GetNode(0).API\n\tidx, err := api.CreateIndex(ctx, indexName, pilosa.IndexOptions{Keys: false, TrackExistence: true})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\topts := []pilosa.FieldOption{pilosa.OptFieldTypeInt(0, math.MaxInt64)}\n\t_, err = idx.CreateField(fieldName, \"\", opts...)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\treq := &pilosa.ImportValueRequest{}\n\treq.Index = indexName\n\treq.Field = fieldName\n\treq.ColumnIDs = []uint64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n\treq.Values = []int64{10, 20, 30, 40, 50, 60, 70, 80, 90, 100}\n\tqcx := api.Txf().NewQcx()\n\tdefer qcx.Abort()\n\n\tif err := api.ImportValue(ctx, qcx, req); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := qcx.Finish(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tt.Run(\"dataframe ingest\", func(t *testing.T) {\n\t\tcr := &pilosa.ChangesetRequest{}\n\t\t// for each row a list of columns\n\t\tcr.Columns = []interface{}{\n\t\t\t[]int64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10},\n\t\t\t[]int64{2, 4, 6, 8, 10, 12, 14, 16, 18, 20},\n\t\t\t[]float64{1, 1.414, 1.732, 2, 2.236, 2.449, 2.646, 2.828, 3, 3.162},\n\t\t\t[]string{\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"},\n\t\t}\n\t\tcr.ShardIds = []int64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n\t\tcr.SimpleSchema = []pilosa.NameType{\n\t\t\t{Name: \"_ID\", DataType: arrow.PrimitiveTypes.Int64},\n\t\t\t{Name: \"ival\", DataType: arrow.PrimitiveTypes.Int64},\n\t\t\t{Name: \"fval\", DataType: arrow.PrimitiveTypes.Float64},\n\t\t\t{Name: \"sval\", DataType: arrow.BinaryTypes.String},\n\t\t}\n\t\tshard := uint64(0)\n\t\terr := api.ApplyDataframeChangeset(ctx, indexName, cr, shard)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t})\n\tt.Run(\"dataframe schema\", func(t *testing.T) {\n\t\texpectedJSON := `[{\"Name\":\"_ID\",\"Type\":\"int64\"},{\"Name\":\"ival\",\"Type\":\"int64\"},{\"Name\":\"fval\",\"Type\":\"float64\"},{\"Name\":\"sval\",\"Type\":\"utf8\"}]`\n\t\tparts, err := api.GetDataframeSchema(ctx, indexName)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tw := new(bytes.Buffer)\n\t\tif err := json.NewEncoder(w).Encode(parts); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tgot := strings.Trim(w.String(), \"\\t \\n\")\n\t\tif strings.Compare(got, expectedJSON) != 0 {\n\t\t\tt.Fatalf(\"expected: %v got: %v\", expectedJSON, got)\n\t\t}\n\t})\n\tt.Run(\"dataframe apply all\", func(t *testing.T) {\n\t\tpql := `Apply(\"_ID\",\"_\")`\n\t\tif res, err := api.Query(ctx, &pilosa.QueryRequest{Index: indexName, Query: pql}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else {\n\t\t\t// qr := res.Results[0].(*dataframe.DataFrame)\n\t\t\texpectedJSON := `{\"Results\":[{\"_ID\":{\"int\":[10,9,8,7,6,5,4,3,2,1,0]}}],\"Err\":null,\"Profile\":null}`\n\t\t\tw := new(bytes.Buffer)\n\t\t\tif err := json.NewEncoder(w).Encode(res); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tgot := strings.Trim(w.String(), \"\\t \\n\")\n\t\t\tif strings.Compare(got, expectedJSON) != 0 {\n\t\t\t\tt.Fatalf(\"expected: %v got: %v\", expectedJSON, got)\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\"dataframe apply filter\", func(t *testing.T) {\n\t\t// TODO(twg) 2022/11/03 refactor this when changing marshalling\n\n\t\tpql := `Apply(ConstRow(columns=[2,4,6]),\"_ID+0\",\"_\")`\n\t\tif res, err := api.Query(ctx, &pilosa.QueryRequest{Index: indexName, Query: pql}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else {\n\t\t\texpectedJSON := `{\"Results\":[{\"I\":{\"int\":[6,4,2]}}],\"Err\":null,\"Profile\":null}`\n\t\t\tw := new(bytes.Buffer)\n\t\t\tif err := json.NewEncoder(w).Encode(res); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tgot := strings.Trim(w.String(), \"\\t \\n\")\n\t\t\tif strings.Compare(got, expectedJSON) != 0 {\n\t\t\t\tt.Fatalf(\"expected: %v got: %v\", expectedJSON, got)\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\"dataframe apply ivy err map\", func(t *testing.T) {\n\t\tpql := `Apply(\"barf\",\"_\")`\n\t\tif _, err := api.Query(ctx, &pilosa.QueryRequest{Index: indexName, Query: pql}); err == nil {\n\t\t\tt.Fatal(\"expected error\")\n\t\t}\n\t})\n\tt.Run(\"dataframe apply ivy err reduce\", func(t *testing.T) {\n\t\tpql := `Apply(\"_ID\",\"barfo\")`\n\t\tif _, err := api.Query(ctx, &pilosa.QueryRequest{Index: indexName, Query: pql}); err == nil {\n\t\t\tt.Fatal(\"expected error\")\n\t\t}\n\t})\n\tt.Run(\"dataframe arrow filter\", func(t *testing.T) {\n\t\tpql := `Arrow(ConstRow(columns=[2,4,6]))`\n\t\tif res, err := api.Query(ctx, &pilosa.QueryRequest{Index: indexName, Query: pql}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else {\n\t\t\texpectedJSON := `{\"Results\":[{\"_ID\":[2,4,6],\"fval\":[1.414,2,2.449],\"ival\":[4,8,12],\"sval\":[\"B\",\"D\",\"F\"]}],\"Err\":null,\"Profile\":null}`\n\t\t\tw := new(bytes.Buffer)\n\t\t\tif err := json.NewEncoder(w).Encode(res); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tgot := strings.Trim(w.String(), \"\\t \\n\")\n\t\t\tif strings.Compare(got, expectedJSON) != 0 {\n\t\t\t\tt.Fatalf(\"expected: %v got: %v\", expectedJSON, got)\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\"dataframe arrow filter with header\", func(t *testing.T) {\n\t\tpql := `Arrow(ConstRow(columns=[2,4,6]),header=[\"fval\"])`\n\t\tif res, err := api.Query(ctx, &pilosa.QueryRequest{Index: indexName, Query: pql}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else {\n\t\t\texpectedJSON := `{\"Results\":[{\"_ID\":[2,4,6],\"fval\":[1.414,2,2.449]}],\"Err\":null,\"Profile\":null}`\n\t\t\tw := new(bytes.Buffer)\n\t\t\tif err := json.NewEncoder(w).Encode(res); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tgot := strings.Trim(w.String(), \"\\t \\n\")\n\t\t\tif strings.Compare(got, expectedJSON) != 0 {\n\t\t\t\tt.Fatalf(\"expected: %v got: %v\", expectedJSON, got)\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\"dataframe ingest update\", func(t *testing.T) {\n\t\tcr := &pilosa.ChangesetRequest{}\n\t\t// for each row a list of columns\n\t\tcr.Columns = []interface{}{\n\t\t\t[]int64{1},\n\t\t\t[]int64{20},\n\t\t\t[]float64{10},\n\t\t\t[]string{\"A2\"},\n\t\t}\n\t\tcr.ShardIds = []int64{1}\n\t\tcr.SimpleSchema = []pilosa.NameType{\n\t\t\t{Name: \"_ID\", DataType: arrow.PrimitiveTypes.Int64},\n\t\t\t{Name: \"ival\", DataType: arrow.PrimitiveTypes.Int64},\n\t\t\t{Name: \"fval\", DataType: arrow.PrimitiveTypes.Float64},\n\t\t\t{Name: \"sval\", DataType: arrow.BinaryTypes.String},\n\t\t}\n\t\tshard := uint64(0)\n\t\terr := api.ApplyDataframeChangeset(ctx, indexName, cr, shard)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t})\n\tt.Run(\"dataframe arrow filter with header\", func(t *testing.T) {\n\t\tpql := `Arrow(ConstRow(columns=[1]),header=[\"ival\",\"fval\",\"sval\"])`\n\t\tif res, err := api.Query(ctx, &pilosa.QueryRequest{Index: indexName, Query: pql}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else {\n\t\t\texpectedJSON := `{\"Results\":[{\"_ID\":[1],\"fval\":[10],\"ival\":[20],\"sval\":[\"A2\"]}],\"Err\":null,\"Profile\":null}`\n\t\t\tw := new(bytes.Buffer)\n\t\t\tif err := json.NewEncoder(w).Encode(res); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tgot := strings.Trim(w.String(), \"\\t \\n\")\n\t\t\tif strings.Compare(got, expectedJSON) != 0 {\n\t\t\t\tt.Fatalf(\"expected: %v got: %v\", expectedJSON, got)\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\"dataframe delete\", func(t *testing.T) {\n\t\terr := api.DeleteDataframe(ctx, indexName)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\texpectedJSON := `[]`\n\t\tparts, err := api.GetDataframeSchema(ctx, indexName)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tw := new(bytes.Buffer)\n\t\tif err := json.NewEncoder(w).Encode(parts); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tgot := strings.Trim(w.String(), \"\\t \\n\")\n\t\tif strings.Compare(got, expectedJSON) != 0 {\n\t\t\tt.Fatalf(\"expected: %v got: %v\", expectedJSON, got)\n\t\t}\n\t})\n}\n"
        },
        {
          "name": "dax",
          "type": "tree",
          "content": null
        },
        {
          "name": "dbshard.go",
          "type": "blob",
          "size": 17.052734375,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\n\trbfcfg \"github.com/featurebasedb/featurebase/v3/rbf/cfg\"\n\ttxkey \"github.com/featurebasedb/featurebase/v3/short_txkey\"\n\t\"github.com/featurebasedb/featurebase/v3/storage\"\n\t\"github.com/pkg/errors\"\n\n\t\"github.com/featurebasedb/featurebase/v3/vprint\"\n)\n\nvar _ = sort.Sort\n\nconst (\n\t// backendsDir is the default backends directory used to store the\n\t// data for each backend.\n\tbackendsDir = \"backends\"\n)\n\n// types to support a database file per shard\n\ntype DBHolder struct {\n\tIndex map[string]*DBIndex\n}\n\nfunc NewDBHolder() *DBHolder {\n\treturn &DBHolder{\n\t\tIndex: make(map[string]*DBIndex),\n\t}\n}\n\ntype DBIndex struct {\n\tShard map[uint64]*DBShard\n}\n\ntype DBWrapper interface {\n\tNewTx(write bool, initialIndexName string, o Txo) (tx Tx, err error)\n\tClose() error\n\tDeleteFragment(index, field, view string, shard uint64, frag interface{}) error\n\tDeleteField(index, field, fieldPath string) error\n\tOpenListString() string\n\tPath() string\n\tHasData() (has bool, err error)\n\tSetHolder(h *Holder)\n\t//needed for restore\n\tCloseDB() error\n\tOpenDB() error\n}\n\ntype DBRegistry interface {\n\tOpenDBWrapper(path string, doAllocZero bool, cfg *storage.Config) (DBWrapper, error)\n}\n\ntype DBShard struct {\n\tHolderPath string\n\n\tIndex string\n\tShard uint64\n\tOpen  bool\n\n\ttyp  txtype\n\tstyp string\n\n\tW             DBWrapper\n\tParentDBIndex *DBIndex\n\n\tidx *Index\n\tper *DBPerShard\n\n\tclosed bool\n}\n\nfunc (dbs *DBShard) DeleteFragment(index, field, view string, shard uint64, frag interface{}) (err error) {\n\tif index != dbs.Index {\n\t\treturn fmt.Errorf(\"DeleteFragment called on DBShard for %q with index %q\", dbs.Index, index)\n\t}\n\tif shard != dbs.Shard {\n\t\treturn fmt.Errorf(\"DeleteFragment called on DBShard for %d with shard %d\", dbs.Shard, shard)\n\t}\n\treturn dbs.W.DeleteFragment(index, field, view, shard, frag)\n}\n\nfunc (dbs *DBShard) DeleteFieldFromStore(index, field, fieldPath string) (err error) {\n\tif index != dbs.Index {\n\t\treturn fmt.Errorf(\"DeleteFieldFromStore called on DBShard for %q with index %q\", dbs.Index, index)\n\t}\n\treturn dbs.W.DeleteField(index, field, fieldPath)\n}\n\nfunc (dbs *DBShard) Close() (err error) {\n\tdbs.closed = true\n\treturn dbs.W.Close()\n}\n\nfunc (dbs *DBShard) NewTx(write bool, initialIndexName string, o Txo) (tx Tx, err error) {\n\tif initialIndexName != dbs.Index {\n\t\treturn nil, fmt.Errorf(\"NewTx called on DBShard for %q with index %q\", dbs.Index, initialIndexName)\n\t}\n\tif o.dbs != dbs {\n\t\treturn nil, fmt.Errorf(\"dbs mismatch: TxFactory.NewTx() should have set o.dbs(%p) to equal dbs(%p)\", o.dbs, dbs)\n\t}\n\tif o.Shard != dbs.Shard {\n\t\treturn nil, fmt.Errorf(\"shard disagreement: o.Shard='%v' but dbs.Shard='%v'\", int(o.Shard), int(dbs.Shard))\n\t}\n\treturn dbs.W.NewTx(write, initialIndexName, o)\n}\n\ntype flatkey struct {\n\tindex string\n\tshard uint64\n}\n\ntype DBPerShard struct {\n\tMu sync.Mutex\n\n\tHolderDir string\n\n\tdbh *DBHolder\n\n\t// just flat, not buried within the Node heirarchy.\n\t// Easily see how many we have.\n\tFlatmap map[flatkey]*DBShard\n\n\ttyp txtype\n\n\ttxf    *TxFactory\n\tholder *Holder\n\n\t// cache the shards per index to avoid excessive\n\t// directory scans of the index directory.\n\t// Keep it up-to-date as we add shards to avoid doing\n\t// a filesystem rescan on new shard creation.\n\t//\n\t// index -> *shardSet\n\tindex2shards map[string]*shardSet\n\n\tStorageConfig *storage.Config\n\tRBFConfig     *rbfcfg.Config\n}\n\nfunc newIndex2Shards() (r map[string]*shardSet) {\n\tr = make(map[string]*shardSet)\n\treturn\n}\n\ntype shardSet struct {\n\tshardsMap map[uint64]struct{}\n\tshardsVer int64 // increment with each change.\n\n\t// give out readonly to repeated consumers if\n\t// readonlyVer == shardsVer\n\treadonly    map[uint64]struct{}\n\treadonlyVer int64\n}\n\nfunc (a *shardSet) unionInPlace(b *shardSet) {\n\tshards := b.CloneMaybe()\n\tfor shard := range shards {\n\t\ta.add(shard)\n\t}\n}\n\nfunc (a *shardSet) equals(b *shardSet) bool {\n\tif len(a.shardsMap) != len(b.shardsMap) {\n\t\treturn false\n\t}\n\tfor shardInA := range a.shardsMap {\n\t\t_, ok := b.shardsMap[shardInA]\n\t\tif !ok {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n\n}\n\nfunc (a *shardSet) shards() []uint64 {\n\ts := make([]uint64, 0, len(a.shardsMap))\n\tfor si := range a.shardsMap {\n\t\ts = append(s, si)\n\t}\n\treturn s\n}\n\nfunc (ss *shardSet) String() (r string) {\n\tr = \"[\"\n\tfor k := range ss.shardsMap {\n\t\tr += fmt.Sprintf(\"%v, \", k)\n\t}\n\tr += \"]\"\n\treturn\n}\n\nfunc (ss *shardSet) add(shard uint64) {\n\t_, already := ss.shardsMap[shard]\n\tif !already {\n\t\tss.shardsMap[shard] = struct{}{}\n\t\tss.shardsVer++\n\t}\n}\n\n// CloneMaybe maintains a re-usable readonly version\n// ss.shards that can be returned to multiple goroutine\n// reads as it will never change. A copy is only made\n// once for each change in the shard set.\nfunc (ss *shardSet) CloneMaybe() map[uint64]struct{} {\n\n\tif ss.readonlyVer == ss.shardsVer {\n\t\treturn ss.readonly\n\t}\n\n\t// readonlyVer is out of date.\n\t// readonly needs update. We cannot\n\t// modify the readonly map in place;\n\t// must make a fully new copy here.\n\tss.readonly = make(map[uint64]struct{})\n\n\tfor k := range ss.shardsMap {\n\t\tss.readonly[k] = struct{}{}\n\t}\n\tss.readonlyVer = ss.shardsVer\n\treturn ss.readonly\n}\n\nfunc newShardSet() *shardSet {\n\treturn &shardSet{\n\t\tshardsMap: make(map[uint64]struct{}),\n\t}\n}\n\nfunc (per *DBPerShard) LoadExistingDBs() (err error) {\n\tidxs := per.holder.Indexes()\n\n\tfor _, idx := range idxs {\n\n\t\tshardset, err := per.txf.GetShardsForIndex(idx, \"\", true)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfor shard := range shardset {\n\t\t\t_, err := per.GetDBShard(idx.name, shard, idx)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Wrap(err, \"DBPerShard.LoadExistingDBs GetDBShard()\")\n\t\t\t}\n\t\t}\n\t}\n\treturn\n}\n\nfunc (txf *TxFactory) NewDBPerShard(typ txtype, holderDir string, holder *Holder) (d *DBPerShard) {\n\tif holder.cfg == nil || holder.cfg.RBFConfig == nil || holder.cfg.StorageConfig == nil {\n\t\tvprint.PanicOn(\"must have holder.cfg.RBFConfig and holder.cfg.StorageConfig set here\")\n\t}\n\n\td = &DBPerShard{\n\t\ttyp:           typ,\n\t\tHolderDir:     holderDir,\n\t\tholder:        holder,\n\t\tdbh:           NewDBHolder(),\n\t\tFlatmap:       make(map[flatkey]*DBShard),\n\t\ttxf:           txf,\n\t\tindex2shards:  newIndex2Shards(),\n\t\tStorageConfig: holder.cfg.StorageConfig,\n\t\tRBFConfig:     holder.cfg.RBFConfig,\n\t}\n\treturn\n}\n\nfunc (per *DBPerShard) DeleteIndex(index string) (err error) {\n\n\tper.Mu.Lock()\n\tdefer per.Mu.Unlock()\n\n\tdbi, ok := per.dbh.Index[index]\n\tif !ok {\n\t\t// since we lazily make indexes upon use by a Tx now, we won't\n\t\t// have an index for server/ TestQuerySQLUnary/test-20 to delete.\n\t\t// Don't freak out. Just return nil.\n\t\treturn nil\n\t}\n\tfor _, dbs := range dbi.Shard {\n\t\terr = dbs.Close()\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"DBPerShard.DeleteIndex dbs.Close()\")\n\t\t}\n\t\tpath := dbs.pathForType(per.typ)\n\t\terr = os.RemoveAll(path)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, fmt.Sprintf(\"DBPerShard.DeleteIndex os.RemoveAll('%v')\", path))\n\t\t}\n\t\tdelete(per.index2shards, index)\n\t}\n\n\t// allow the index to be created again anew.\n\tdelete(per.dbh.Index, index)\n\n\treturn\n}\n\nfunc (per *DBPerShard) DeleteFieldFromStore(index, field, fieldPath string) (err error) {\n\tper.Mu.Lock()\n\tdefer func() {\n\t\tif fieldPath != \"\" {\n\t\t\t_ = os.RemoveAll(fieldPath)\n\t\t}\n\t\tper.Mu.Unlock()\n\t}()\n\n\tdbi, ok := per.dbh.Index[index]\n\tif !ok {\n\t\t// TestIndex_Existence_Delete in index_internal_test.go\n\t\t// will call us without having ever created a Tx or DB,\n\t\t// so we can't complain here.\n\t\treturn nil\n\t}\n\tfor _, dbs := range dbi.Shard {\n\t\tif e := dbs.W.DeleteField(index, field, fieldPath); e != nil && err == nil {\n\t\t\terr = errors.Wrap(e, \"DeleteFieldFromStore()\")\n\t\t}\n\t}\n\treturn err\n}\n\nfunc (per *DBPerShard) DeleteFragment(index, field, view string, shard uint64, frag *fragment) error {\n\n\tidx := per.txf.holder.Index(index)\n\tdbs, err := per.GetDBShard(index, shard, idx)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn dbs.DeleteFragment(index, field, view, shard, frag)\n}\n\n// if you know the shard, you can use this\n// pathForType and prefixForType must be kept in sync!\nfunc (dbs *DBShard) pathForType(ty txtype) string {\n\t// top level paths will end in \"@@\"\n\n\t// what here for roaring? well, roaringRegistrar.OpenDBWrapper()\n\t// is a no-op anyhow. so doesn't need to be correct atm.\n\n\tpath := dbs.HolderPath + sep + dbs.Index + sep + backendsDir + sep + ty.DirectoryName() + sep + fmt.Sprintf(\"shard.%04v\", dbs.Shard)\n\treturn path\n}\n\n// if you don't know the shard, you have to use this.\n// prefixForType and pathForType must be kept in sync!\nfunc (per *DBPerShard) prefixForType(idx *Index, ty txtype) string {\n\t// top level paths will end in \"@@\"\n\treturn per.HolderDir + sep + idx.name + sep + backendsDir + sep + ty.DirectoryName() + sep\n}\n\nvar ErrNoData = fmt.Errorf(\"no data\")\n\n// keep our cache of shards up-to-date in memory; after the initial\n// directory scan, this is all we should we need. Prevents us from\n// doing additional, expensive, directory scans.\n//\n// Caller must hold per.Mu.Lock() already.\nfunc (per *DBPerShard) updateIndex2ShardCacheWithNewShard(dbs *DBShard) {\n\tshardset, ok := per.index2shards[dbs.Index]\n\tif !ok {\n\t\tshardset = newShardSet()\n\t\tper.index2shards[dbs.Index] = shardset\n\t}\n\t// INVAR: shardset is present, not nil; a map that can be added to.\n\tshardset.add(dbs.Shard)\n}\n\nfunc (per *DBPerShard) GetDBShard(index string, shard uint64, idx *Index) (dbs *DBShard, err error) {\n\tper.Mu.Lock()\n\tdefer per.Mu.Unlock()\n\treturn per.unprotectedGetDBShard(index, shard, idx)\n}\n\nfunc (per *DBPerShard) unprotectedGetDBShard(index string, shard uint64, idx *Index) (dbs *DBShard, err error) {\n\n\tdbi, ok := per.dbh.Index[index]\n\tif !ok {\n\t\tdbi = &DBIndex{\n\t\t\tShard: make(map[uint64]*DBShard),\n\t\t}\n\t\tper.dbh.Index[index] = dbi\n\t}\n\tdbs, ok = dbi.Shard[shard]\n\tif dbs != nil && dbs.closed {\n\t\tvprint.PanicOn(fmt.Sprintf(\"cannot retain closed dbs across holder ReOpen dbs='%p'; per.typ='%v'\", dbs, per.typ))\n\t}\n\tif !ok {\n\t\tdbs = &DBShard{\n\t\t\ttyp:           per.typ,\n\t\t\tParentDBIndex: dbi,\n\t\t\tIndex:         index,\n\t\t\tShard:         shard,\n\t\t\tHolderPath:    per.HolderDir,\n\t\t\tidx:           idx,\n\t\t\tper:           per,\n\t\t}\n\t\tdbs.styp = per.typ.String()\n\t\tdbi.Shard[shard] = dbs\n\t\tper.updateIndex2ShardCacheWithNewShard(dbs)\n\t}\n\tif !dbs.Open {\n\t\tvar registry DBRegistry\n\t\tswitch dbs.typ {\n\t\tcase rbfTxn:\n\t\t\tregistry = globalRbfDBReg\n\t\t\tregistry.(*rbfDBRegistrar).SetRBFConfig(per.RBFConfig)\n\t\tdefault:\n\t\t\tvprint.PanicOn(fmt.Sprintf(\"unknown txtyp: '%v'\", dbs.typ))\n\t\t}\n\t\tpath := dbs.pathForType(dbs.typ)\n\t\tw, err := registry.OpenDBWrapper(path, DetectMemAccessPastTx, per.StorageConfig)\n\t\tvprint.PanicOn(err)\n\t\th := idx.Holder()\n\t\tw.SetHolder(h)\n\t\tdbs.Open = true\n\t\tper.Flatmap[flatkey{index: index, shard: shard}] = dbs\n\t\tdbs.W = w\n\t}\n\treturn dbs, nil\n}\n\nfunc (per *DBPerShard) Close() (err error) {\n\tper.Mu.Lock()\n\tdefer per.Mu.Unlock()\n\n\tfor _, dbi := range per.dbh.Index {\n\t\tfor _, dbs := range dbi.Shard {\n\t\t\terr = dbs.Close()\n\t\t\tvprint.PanicOn(err)\n\t\t}\n\t}\n\treturn\n}\n\n// DBPerShardGetShardsForIndex returns the shards for idx.\n// If requireData, we open the database and see that it has a key, rather\n// than assume that the database file presence is enough.\nfunc (f *TxFactory) GetShardsForIndex(idx *Index, roaringViewPath string, requireData bool) (map[uint64]struct{}, error) {\n\treturn f.dbPerShard.TypedDBPerShardGetShardsForIndex(f.typ, idx, roaringViewPath, requireData)\n}\n\n// requireData means open the database file and verify that at least one key is set.\n// The returned sliceOfShards should not be modified. We will cache it for subsequent\n// queries.\n//\n// when a new DBShard is made, we will update the list of shards then. Thus\n// the per.index2shard should always be up to date AFTER the first call here.\nfunc (per *DBPerShard) TypedDBPerShardGetShardsForIndex(ty txtype, idx *Index, roaringViewPath string, requireData bool) (shardMap map[uint64]struct{}, err error) {\n\n\t// use the cache, always\n\tper.Mu.Lock()\n\tdefer per.Mu.Unlock()\n\n\ti2ss := per.index2shards\n\n\tss, ok := i2ss[idx.name]\n\tif ok {\n\t\treturn ss.CloneMaybe(), nil\n\t}\n\t// INVAR: cache miss, and index2shards[ty] exists.\n\n\t// gotta read shards from disk directory layout.\n\tsetOfShards := newShardSet()\n\tper.index2shards[idx.name] = setOfShards\n\n\t// Upon return, cache the setOfShards value and reuse it next time\n\n\tpath := per.prefixForType(idx, ty)\n\n\tignoreEmpty := false\n\tincludeRoot := true\n\tdbf, err := listDirUnderDir(path, includeRoot, ignoreEmpty)\n\tvprint.PanicOn(err)\n\n\tfor _, nm := range dbf {\n\t\tbase := filepath.Base(nm)\n\n\t\t// We're only interested in \"shard.*\" files, so skip everything else.\n\t\tconst shardPrefix = \"shard.\"\n\t\tconst lenOfShardPrefix = len(shardPrefix)\n\t\tif !strings.HasPrefix(base, shardPrefix) {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Parse filename into integer.\n\t\tshard, err := strconv.ParseUint(base[lenOfShardPrefix:], 10, 64)\n\t\tif err != nil {\n\t\t\tvprint.PanicOn(err)\n\t\t\tcontinue\n\t\t}\n\n\t\t// exclude those without data?\n\t\thasData := false\n\n\t\tif requireData {\n\t\t\thasData, err = per.unprotectedTypedIndexShardHasData(ty, idx, shard)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tif hasData {\n\t\t\t\tsetOfShards.add(shard)\n\t\t\t}\n\t\t} else {\n\t\t\t// file presence is enough\n\t\t\tsetOfShards.add(shard)\n\t\t}\n\t}\n\treturn setOfShards.CloneMaybe(), nil\n}\n\nfunc (per *DBPerShard) unprotectedTypedIndexShardHasData(ty txtype, idx *Index, shard uint64) (hasData bool, err error) {\n\tif ty != per.typ {\n\t\treturn\n\t}\n\n\t// make the dbs if it doesn't get exist\n\tdbs, err := per.unprotectedGetDBShard(idx.name, shard, idx)\n\tif err != nil {\n\t\treturn false, errors.Wrap(err, fmt.Sprintf(\"DBPerShard.TypedIndexShardHasData() \"+\n\t\t\t\"per.GetDBShard(index='%v', shard='%v', ty='%v')\", idx.name, shard, ty.String()))\n\t}\n\n\treturn dbs.W.HasData()\n}\n\nfunc listDirUnderDir(root string, includeRoot bool, ignoreEmpty bool) (files []string, err error) {\n\tif !dirExists(root) {\n\t\treturn\n\t}\n\n\tn := len(root) + 1\n\tif includeRoot {\n\t\tn = 0\n\t}\n\terr = filepath.Walk(root, func(path string, info os.FileInfo, err error) error {\n\t\tif len(path) < n {\n\t\t\t// ignore\n\t\t} else {\n\t\t\tif info == nil {\n\t\t\t\t// re-opening an RBF database hit this, racing with a directory rename.\n\t\t\t\t// Don't freak out.\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tif !info.IsDir() {\n\t\t\t\t// ignore files\n\t\t\t} else {\n\t\t\t\tif ignoreEmpty && info.Size() == 0 {\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\t\t\t\tfiles = append(files, path[n:])\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\treturn\n}\n\ntype FieldView2Shards struct {\n\t// field -> view -> *shardSet\n\tm map[string]map[string]*shardSet\n}\n\nfunc (vs *FieldView2Shards) getViewsForField(field string) map[string]*shardSet {\n\treturn vs.m[field]\n}\n\nfunc (vs *FieldView2Shards) addViewShardSet(fv txkey.FieldView, ss *shardSet) {\n\n\tf, ok := vs.m[fv.Field]\n\tif !ok {\n\t\tf = make(map[string]*shardSet)\n\t\tvs.m[fv.Field] = f\n\t}\n\t// INVAR: f is ready to take ss.\n\n\t// existing stuff to merge with?\n\tprior, ok := f[fv.View]\n\tif !ok {\n\t\tf[fv.View] = ss\n\t\treturn\n\t}\n\t// merge ss and prior. No need to put the union back into f[fv.View]\n\t// because prior is a pointer.\n\tprior.unionInPlace(ss)\n}\n\nfunc (a *FieldView2Shards) equals(b *FieldView2Shards) bool {\n\tif a == nil && b == nil {\n\t\treturn true\n\t}\n\tif a == nil || b == nil {\n\t\treturn false\n\t}\n\tif len(a.m) != len(b.m) {\n\t\treturn false\n\t}\n\tfor field, viewmapA := range a.m {\n\t\tviewmapB, ok := b.m[field]\n\t\tif !ok {\n\t\t\treturn false\n\t\t}\n\t\tif len(viewmapB) != len(viewmapA) {\n\t\t\treturn false\n\t\t}\n\t\tfor k, va := range viewmapA {\n\t\t\tvb, ok := viewmapB[k]\n\t\t\tif !ok {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tif !va.equals(vb) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\nfunc NewFieldView2Shards() *FieldView2Shards {\n\treturn &FieldView2Shards{\n\t\tm: make(map[string]map[string]*shardSet), // expected response from GetView2ShardMapForIndex\n\t}\n}\n\nfunc (vs *FieldView2Shards) addShard(fv txkey.FieldView, shard uint64) {\n\tviewmap, ok := vs.m[fv.Field]\n\tif !ok {\n\t\tviewmap = make(map[string]*shardSet)\n\t\tvs.m[fv.Field] = viewmap\n\t}\n\tss, ok := viewmap[fv.View]\n\tif !ok {\n\t\tss = newShardSet()\n\t\tviewmap[fv.View] = ss\n\t}\n\tss.add(shard)\n}\n\nfunc (vs *FieldView2Shards) String() (r string) {\n\tr = \"\\n\"\n\tfor field, viewmap := range vs.m {\n\t\tfor view, shards := range viewmap {\n\t\t\tr += fmt.Sprintf(\"field '%v' view:'%v' shards:%v\\n\", field, view, shards)\n\t\t}\n\t}\n\tr += \"\\n\"\n\treturn\n}\n\nfunc (vs *FieldView2Shards) removeField(name string) {\n\tdelete(vs.m, name)\n}\n\nfunc (per *DBPerShard) GetFieldView2ShardsMapForIndex(idx *Index) (vs *FieldView2Shards, err error) {\n\tty := per.typ\n\n\tswitch ty {\n\tdefault:\n\t\tvs = NewFieldView2Shards()\n\n\t\tshardMap, err := per.TypedDBPerShardGetShardsForIndex(ty, idx, \"\", true)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tfor shard := range shardMap {\n\t\t\tdbs, err := per.GetDBShard(idx.name, shard, idx)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"DBPerShard.GetFieldView2ShardsMapForIndex GetDBShard()\")\n\t\t\t}\n\t\t\tfieldviews, err := dbs.AllFieldViews()\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"DBPerShard.GetFieldView2ShardsMapForIndex dbs.AllFieldViews()\")\n\t\t\t}\n\t\t\tfor _, fv := range fieldviews {\n\t\t\t\tvs.addShard(fv, shard)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn\n}\n\nfunc (dbs *DBShard) AllFieldViews() (fvs []txkey.FieldView, err error) {\n\n\ttx, err := dbs.NewTx(!writable, dbs.idx.name, Txo{Write: !writable, Shard: dbs.Shard, Index: dbs.idx, dbs: dbs})\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, fmt.Sprintf(\"dbshard.NewTx for index '%v', shard %v\", dbs.idx.name, dbs.Shard))\n\t}\n\tdefer tx.Rollback()\n\treturn tx.GetSortedFieldViewList(dbs.idx, dbs.Shard)\n}\n"
        },
        {
          "name": "dbshard_internal_test.go",
          "type": "blob",
          "size": 8.126953125,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/featurebasedb/featurebase/v3/rbf\"\n\t\"github.com/featurebasedb/featurebase/v3/shardwidth\"\n\ttxkey \"github.com/featurebasedb/featurebase/v3/short_txkey\"\n\t\"github.com/featurebasedb/featurebase/v3/testhook\"\n\t. \"github.com/featurebasedb/featurebase/v3/vprint\" // nolint:staticcheck\n)\n\n// Shard per db evaluation\nfunc TestShardPerDB_SetBit(t *testing.T) {\n\tf, idx, tx := mustOpenFragment(t)\n\t_ = idx\n\tdefer f.Clean(t)\n\n\t// Set bits on the fragment.\n\tif _, err := f.setBit(tx, 120, 1); err != nil {\n\t\tt.Fatal(err)\n\t} else if _, err := f.setBit(tx, 120, 6); err != nil {\n\t\tt.Fatal(err)\n\t} else if _, err := f.setBit(tx, 121, 0); err != nil {\n\t\tt.Fatal(err)\n\t}\n\t// should have two containers set in the fragment.\n\n\t// Verify counts on rows.\n\tif n := f.mustRow(tx, 120).Count(); n != 2 {\n\t\tt.Fatalf(\"unexpected count: %d\", n)\n\t} else if n := f.mustRow(tx, 121).Count(); n != 1 {\n\t\tt.Fatalf(\"unexpected count: %d\", n)\n\t}\n\n\t// commit the change, and verify it is still there\n\tPanicOn(tx.Commit())\n\n\t// Close and reopen the fragment & verify the data.\n\terr := f.Reopen() // roaring data not being flushed? red on roaring\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\ttx = idx.holder.txf.NewTx(Txo{Write: writable, Index: idx, Fragment: f, Shard: f.shard})\n\tdefer tx.Rollback()\n\n\tif n := f.mustRow(tx, 120).Count(); n != 2 {\n\t\tt.Fatalf(\"unexpected count (reopen): %d\", n)\n\t} else if n := f.mustRow(tx, 121).Count(); n != 1 {\n\t\tt.Fatalf(\"unexpected count (reopen): %d\", n)\n\t}\n}\n\n// test that we find all *local* shards\nfunc Test_DBPerShard_GetShardsForIndex_LocalOnly(t *testing.T) {\n\ttmpdir, err := testhook.TempDir(t, \"Test_DBPerShard_GetShardsForIndex_LocalOnly\")\n\tPanicOn(err)\n\tdefer os.RemoveAll(tmpdir)\n\n\tv2s := NewFieldView2Shards()\n\tstdShardSet := newShardSet()\n\tfor _, shard := range []uint64{93, 223, 221, 215, 219, 217} {\n\t\tstdShardSet.add(shard)\n\t}\n\tfor _, field := range []string{\"f\", \"_exists\"} {\n\t\tv2s.addViewShardSet(txkey.FieldView{Field: field, View: \"standard\"}, stdShardSet)\n\t}\n\n\tfor _, src := range []string{\"rbf\"} {\n\t\tholder := newTestHolder(t)\n\n\t\tindex := \"rick\"\n\t\tidx := makeSampleRoaringDir(t, tmpdir, index, src, 1, holder, v2s)\n\t\tif idx == nil {\n\t\t\tidx, err = NewIndex(holder, filepath.Join(tmpdir, index), index)\n\t\t\tPanicOn(err)\n\t\t}\n\t\tstd := \"rick/fields/f/views/standard\"\n\n\t\tshards, err := holder.txf.GetShardsForIndex(idx, tmpdir+sep+std, false)\n\t\tPanicOn(err)\n\n\t\tfor _, shard := range []uint64{93, 223, 221, 215, 219, 217} {\n\t\t\tif _, ok := shards[shard]; !ok {\n\t\t\t\tt.Fatalf(\"missing shard=%v from shards='%#v'\", shard, shards)\n\t\t\t}\n\t\t}\n\t\tfor _, shard := range []uint64{93, 223, 221, 215, 219, 217} {\n\t\t\ttx := idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Shard: shard})\n\t\t\tfvs, err := tx.GetSortedFieldViewList(idx, shard)\n\t\t\tPanicOn(err)\n\t\t\t// expect these same two field/views for all 6 shards\n\t\t\texpect0 := txkey.FieldView{Field: \"_exists\", View: \"standard\"}\n\t\t\texpect1 := txkey.FieldView{Field: \"f\", View: \"standard\"}\n\t\t\tif len(fvs) != 2 {\n\t\t\t\tt.Fatalf(\"fvs should be len 2, got '%#v' (%s)\", fvs, src)\n\t\t\t}\n\t\t\tif fvs[0] != expect0 {\n\t\t\t\tt.Fatalf(\"expected fvs[0]='%#v', but got '%#v'\", expect0, fvs[0])\n\t\t\t}\n\t\t\tif fvs[1] != expect1 {\n\t\t\t\tt.Fatalf(\"expected fvs[1]='%#v', but got '%#v'\", expect1, fvs[1])\n\t\t\t}\n\t\t\ttx.Rollback()\n\t\t}\n\t}\n}\n\n// data for Test_DBPerShard_GetShardsForIndex\nvar sampleRoaringDirList = map[string]string{\"roaring\": `\nrick/fields/f/views/standard/fragments/215.cache\nrick/fields/f/views/standard/fragments/221.cache\nrick/fields/f/views/standard/fragments/223.cache\nrick/fields/f/views/standard/fragments/93.cache\nrick/fields/f/views/standard/fragments/217.cache\nrick/fields/f/views/standard/fragments/219.cache\nrick/fields/f/views/standard/fragments/217\nrick/fields/f/views/standard/fragments/219\nrick/fields/f/views/standard/fragments/215\nrick/fields/f/views/standard/fragments/221\nrick/fields/f/views/standard/fragments/223\nrick/fields/f/views/standard/fragments/93\nrick/fields/_exists/views/standard/fragments/221\nrick/fields/_exists/views/standard/fragments/215\nrick/fields/_exists/views/standard/fragments/217\nrick/fields/_exists/views/standard/fragments/93\nrick/fields/_exists/views/standard/fragments/219\nrick/fields/_exists/views/standard/fragments/223\n`,\n\t\"rbf\": `\nrick/backends/backend-rbf/shard.0093-rbf\nrick/backends/backend-rbf/shard.0215-rbf\nrick/backends/backend-rbf/shard.0217-rbf\nrick/backends/backend-rbf/shard.0219-rbf\nrick/backends/backend-rbf/shard.0221-rbf\nrick/backends/backend-rbf/shard.0223-rbf\n`,\n}\n\nfunc makeSampleRoaringDir(t *testing.T, root, index, backend string, minBytes int, h *Holder, view2shards *FieldView2Shards) (idx *Index) {\n\tshards := []uint64{0, 93, 215, 217, 219, 221, 223}\n\tfns := strings.Split(sampleRoaringDirList[backend], \"\\n\")\n\tfirstDone := false\n\n\tfor i, fn := range fns {\n\t\t// This check is here because in sampleRoaringDirList, the first entry\n\t\t// of each map value is a line feed, so the strings.Split() above\n\t\t// results in a blank entry for the first item. This means that the\n\t\t// slice of shards above has an initial entry \"0\" which is not used.\n\t\tif fn == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\tvar shard uint64\n\t\tswitch backend {\n\t\tcase \"rbf\":\n\t\t\tshard = shards[i]\n\n\t\t\tidx = helperCreateDBShard(h, index, shard)\n\n\t\t\t// first time only, we'll actually make all the shards at this point because\n\t\t\t// view2shards has them all anyway.\n\t\t\tif !firstDone {\n\t\t\t\tfirstDone = true\n\t\t\t\tmakeTxTestDBWithViewsShards(t, h, idx, view2shards)\n\t\t\t}\n\t\t\tcontinue\n\t\tcase \"roaring\":\n\t\tdefault:\n\t\t\tt.Fatalf(\"invalid backend: %s\", backend)\n\t\t}\n\n\t\tpath := root + sep + filepath.Dir(fn)\n\t\tPanicOn(os.MkdirAll(path, 0755))\n\t\tfd, err := os.Create(root + sep + fn)\n\t\tPanicOn(err)\n\t\tif minBytes > 0 {\n\t\t\t_, err := fd.Write(make([]byte, minBytes))\n\t\t\tPanicOn(err)\n\t\t}\n\t\tfd.Close()\n\t}\n\treturn\n}\n\nfunc helperCreateDBShard(h *Holder, index string, shard uint64) *Index {\n\tidx, err := h.CreateIndexIfNotExists(index, \"\", IndexOptions{})\n\tPanicOn(err)\n\t// TODO: It's not clear that this is actually doing anything.\n\tdbs, err := h.txf.dbPerShard.GetDBShard(index, shard, idx)\n\tPanicOn(err)\n\t_ = dbs\n\treturn idx\n}\n\n// keep the ocd linter happy\nvar _ = makeRBFtestDB\n\nfunc makeRBFtestDB(path string, h *Holder, shard uint64) {\n\ti := uint64(1)\n\n\tdb := rbf.NewDB(path, nil)\n\terr := db.Open()\n\tPanicOn(err)\n\tdefer db.Close()\n\n\ttx, err := db.Begin(true)\n\tPanicOn(err)\n\n\terr = tx.CreateBitmap(\"x\")\n\tPanicOn(err)\n\n\t_, err = tx.Add(\"x\", i)\n\tPanicOn(err)\n\n\terr = tx.Commit()\n\tPanicOn(err)\n}\n\nfunc makeTxTestDBWithViewsShards(tb testing.TB, holder *Holder, idx *Index, exp *FieldView2Shards) {\n\n\t// TODO(jea): need date time quantum views!!\n\tfor field, viewmap := range exp.m {\n\t\tfor view, shset := range viewmap {\n\n\t\t\tss := shset.CloneMaybe()\n\t\t\tfor shard := range ss {\n\n\t\t\t\t// simply write 1 bit to each shard to force its creation.\n\t\t\t\tbits := []uint64{(shard << shardwidth.Exponent) + 1}\n\t\t\t\ttx := idx.holder.txf.NewTx(Txo{Write: writable, Index: idx, Shard: shard})\n\t\t\t\tchangeCount, err := tx.Add(idx.name, field, view, shard, bits...)\n\t\t\t\tPanicOn(err)\n\t\t\t\tif changeCount != len(bits) {\n\t\t\t\t\ttb.Fatalf(\"writing field '%v', view '%v' shard '%v', expected changeCount to equal len bits = %v but was %v\", field, view, shard, len(bits), changeCount)\n\t\t\t\t}\n\n\t\t\t\tPanicOn(tx.Commit())\n\t\t\t}\n\t\t}\n\t}\n\n}\n\n// test that rbf can give us a map[view]*shardSet\nfunc Test_DBPerShard_GetFieldView2Shards_map_from_RBF(t *testing.T) {\n\tholder := newTestHolder(t)\n\n\tindex := \"rick\"\n\tfield := \"f\"\n\n\tidx, err := holder.CreateIndex(index, \"\", IndexOptions{})\n\tif err != nil {\n\t\tt.Fatalf(\"creating index: %v\", err)\n\t}\n\n\texp := NewFieldView2Shards()\n\n\tstdShardSet := newShardSet()\n\tstdShardSet.add(12)\n\tstdShardSet.add(15)\n\texp.addViewShardSet(txkey.FieldView{Field: field, View: \"standard\"}, stdShardSet)\n\n\thrShardSet := newShardSet()\n\thrShardSet.add(7)\n\texp.addViewShardSet(txkey.FieldView{Field: field, View: \"standard_2019092416\"}, hrShardSet)\n\n\tmakeTxTestDBWithViewsShards(t, holder, idx, exp)\n\n\t// setup is done\n\tview2shard, err := holder.txf.GetFieldView2ShardsMapForIndex(idx)\n\tPanicOn(err)\n\n\t// compare against setup\n\tif !view2shard.equals(exp) {\n\t\tt.Fatalf(\"expected '%v' but got view2shard '%v'\", exp, view2shard)\n\t}\n}\n"
        },
        {
          "name": "dbshard_test.go",
          "type": "blob",
          "size": 2.3828125,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"testing\"\n\n\tpilosa \"github.com/featurebasedb/featurebase/v3\"\n\t\"github.com/featurebasedb/featurebase/v3/test\"\n\t. \"github.com/featurebasedb/featurebase/v3/vprint\" // nolint:staticcheck\n)\n\nfunc TestAPI_SimplerOneNode_ImportColumnKey(t *testing.T) {\n\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\n\tm0 := c.GetNode(0)\n\n\tt.Run(\"RowIDColumnKey\", func(t *testing.T) {\n\t\tctx := context.Background()\n\t\tindexName := c.Idx()\n\t\tfieldName := \"f\"\n\n\t\tindex, err := m0.API.CreateIndex(ctx, indexName, pilosa.IndexOptions{Keys: true, TrackExistence: true})\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating index: %v\", err)\n\t\t}\n\t\tif index.CreatedAt() == 0 {\n\t\t\tt.Fatal(\"index createdAt is empty\")\n\t\t}\n\n\t\tfield, err := m0.API.CreateField(ctx, indexName, fieldName, pilosa.OptFieldTypeSet(pilosa.DefaultCacheType, 100))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating field: %v\", err)\n\t\t}\n\t\tif field.CreatedAt() == 0 {\n\t\t\tt.Fatal(\"field createdAt is empty\")\n\t\t}\n\n\t\trowID := uint64(1)\n\t\ttimestamp := int64(0)\n\n\t\t// Generate some keyed records.\n\t\trowIDs := []uint64{}\n\t\ttimestamps := []int64{}\n\t\tfor i := 1; i <= 10; i++ {\n\t\t\trowIDs = append(rowIDs, rowID)\n\t\t\ttimestamps = append(timestamps, timestamp)\n\t\t}\n\n\t\t// Keys are sharded so ordering is not guaranteed.\n\t\tcolKeys := []string{\"col10\", \"col8\", \"col9\", \"col6\", \"col7\", \"col4\", \"col5\", \"col2\", \"col3\", \"col1\"}\n\n\t\t// Import data with keys to the primary and verify that it gets\n\t\t// translated and forwarded to the owner of shard 0\n\t\treq := &pilosa.ImportRequest{\n\t\t\tIndex:          indexName,\n\t\t\tIndexCreatedAt: index.CreatedAt(),\n\t\t\tField:          fieldName,\n\t\t\tFieldCreatedAt: field.CreatedAt(),\n\t\t\tShard:          0, // import is all on shard 0, why are we making bocu other shards? b/c this is ignored.\n\t\t\tRowIDs:         rowIDs,\n\t\t\tColumnKeys:     colKeys,\n\t\t\tTimestamps:     timestamps,\n\t\t}\n\n\t\tqcx := m0.API.Txf().NewQcx()\n\t\tif err := m0.API.Import(ctx, qcx, req); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tPanicOn(qcx.Finish())\n\n\t\t//select {}\n\n\t\tpql := fmt.Sprintf(\"Row(%s=%d)\", fieldName, rowID)\n\n\t\t// Query node0.\n\t\tres, err := m0.API.Query(ctx, &pilosa.QueryRequest{Index: indexName, Query: pql})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tkeys := res.Results[0].(*pilosa.Row).Keys\n\t\tif !sameStringSlice(keys, colKeys) {\n\t\t\tt.Fatalf(\"unexpected column keys: %#v\", keys)\n\t\t}\n\t})\n\n}\n"
        },
        {
          "name": "debugstats",
          "type": "tree",
          "content": null
        },
        {
          "name": "delete_test.go",
          "type": "blob",
          "size": 10.3388671875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"math\"\n\t\"sort\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\tpilosa \"github.com/featurebasedb/featurebase/v3\"\n\t\"github.com/featurebasedb/featurebase/v3/disco\"\n\t\"github.com/featurebasedb/featurebase/v3/test\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestExecutor_DeleteRecords(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tindexName := c.Idx()\n\tdefer c.Close()\n\tsetup := func(t *testing.T, r *require.Assertions, c *test.Cluster) {\n\t\tt.Helper()\n\t\tc.CreateField(t, indexName, pilosa.IndexOptions{TrackExistence: true}, \"setfield\")\n\t\tc.ImportBits(t, indexName, \"setfield\", [][2]uint64{\n\t\t\t{0, 0},\n\t\t\t{0, 1},\n\t\t\t{0, ShardWidth + 2},\n\t\t\t{10, 2},\n\t\t\t{10, ShardWidth},\n\t\t\t{10, 2 * ShardWidth},\n\t\t\t{10, ShardWidth + 1},\n\t\t\t{20, ShardWidth},\n\t\t})\n\t\tc.CreateField(t, indexName, pilosa.IndexOptions{TrackExistence: true}, \"bsi\", pilosa.OptFieldTypeInt(math.MinInt64, math.MaxInt64))\n\t\tc.ImportIntID(t, indexName, \"bsi\", []test.IntID{\n\t\t\t{ID: 0, Val: 4},\n\t\t\t{ID: 2, Val: 8},\n\t\t})\n\t\tc.CreateField(t, indexName, pilosa.IndexOptions{TrackExistence: true}, \"timefield\", pilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMDH\"), \"0\"))\n\t\tc.ImportBitsWithTimestamp(t, indexName, \"timefield\", [][2]uint64{\n\t\t\t{0, 0},\n\t\t\t{0, 1},\n\t\t\t{0, 1},\n\t\t\t{0, 1},\n\t\t\t{0, 1},\n\t\t}, []int64{\n\t\t\ttime.Date(2020, time.January, 2, 15, 45, 0, 0, time.UTC).Unix(),\n\t\t\ttime.Date(2019, time.January, 2, 16, 45, 0, 0, time.UTC).Unix(),\n\t\t\ttime.Date(2019, time.January, 2, 16, 45, 0, 0, time.UTC).Unix(),\n\t\t\ttime.Date(2019, time.January, 2, 17, 45, 0, 0, time.UTC).Unix(),\n\t\t\ttime.Date(2019, time.January, 2, 17, 45, 0, 0, time.UTC).Unix(),\n\t\t})\n\n\t}\n\tsetupBig := func(t *testing.T, r *require.Assertions, c *test.Cluster, Rows uint64) {\n\t\tt.Helper()\n\t\tfieldName := \"setfield\"\n\t\tc.CreateField(t, indexName, pilosa.IndexOptions{TrackExistence: true}, fieldName)\n\t\t// we don't need to populate the whole thing, just enough to get a sample of it\n\t\twidth := uint64(ShardWidth / 4)\n\t\trows := make([][2]uint64, width*Rows)\n\t\tn := 0\n\t\t// populate rows with decreasing density\n\t\tfor columnID := uint64(0); columnID < width; columnID++ {\n\t\t\tfor rowID := uint64(0); rowID < Rows; rowID++ {\n\t\t\t\tif (columnID % (rowID + 1)) == 0 {\n\t\t\t\t\trows[n] = [2]uint64{rowID, columnID}\n\t\t\t\t\tn++\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tc.ImportBits(t, indexName, \"setfield\", rows[:n])\n\t}\n\n\tsetupKeys := func(t *testing.T, r *require.Assertions, c *test.Cluster) {\n\t\tt.Helper()\n\t\tc.CreateField(t, indexName, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"timefield\", pilosa.OptFieldKeys(), pilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMDH\"), \"0\"))\n\t\tc.ImportTimeQuantumKey(t, indexName, \"timefield\", []test.TimeQuantumKey{\n\t\t\t{RowKey: \"fish\", ColKey: \"one\", Ts: time.Date(2019, time.January, 2, 17, 45, 0, 0, time.UTC).Unix()},\n\t\t\t{RowKey: \"fish\", ColKey: \"one\", Ts: time.Date(2020, time.January, 2, 17, 45, 0, 0, time.UTC).Unix()},\n\t\t\t{RowKey: \"fish\", ColKey: \"two\", Ts: time.Date(2019, time.January, 3, 17, 45, 0, 0, time.UTC).Unix()},\n\t\t})\n\t\tc.CreateField(t, indexName, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"keystuff\")\n\t\tc.ImportIDKey(t, indexName, \"keystuff\", []test.KeyID{\n\t\t\t{ID: 1, Key: \"A\"},\n\t\t\t{ID: 2, Key: \"B\"},\n\t\t\t{ID: 3, Key: \"C\"},\n\t\t\t{ID: 4, Key: \"D\"},\n\t\t})\n\n\t}\n\tsetupOverlap := func(t *testing.T, r *require.Assertions, c *test.Cluster) {\n\t\tt.Helper()\n\t\tc.CreateField(t, indexName, pilosa.IndexOptions{TrackExistence: true}, \"setfield\")\n\t\tc.ImportBits(t, indexName, \"setfield\", [][2]uint64{\n\t\t\t{0, 0},\n\t\t\t{0, 1},\n\t\t\t{1, 1},\n\t\t\t{2, 1},\n\t\t\t{3, 1},\n\t\t\t{0, ShardWidth},\n\t\t\t{2, ShardWidth},\n\t\t\t{4, ShardWidth},\n\t\t\t{6, ShardWidth},\n\t\t})\n\t}\n\ttearDown := func(t *testing.T, require *require.Assertions, c *test.Cluster) {\n\t\tt.Helper()\n\t\tapi := c.GetPrimary().API\n\t\terr := api.DeleteIndex(context.Background(), indexName)\n\t\trequire.NoErrorf(err, \"DeleteIndex %v\", indexName)\n\t}\n\trequire := require.New(t)\n\tt.Run(\"DeleteRecords\", func(t *testing.T) {\n\n\t\tt.Run(\"Delete\", func(t *testing.T) {\n\t\t\tsetup(t, require, c)\n\t\t\tdefer tearDown(t, require, c)\n\t\t\tresp := c.Query(t, indexName, `Extract(All())`)\n\t\t\tm := resp.Results[0].(pilosa.ExtractedTable)\n\t\t\tbefore := convert(m.Columns)\n\t\t\trequire.Equal([]uint64{0, 1, 2, ShardWidth, ShardWidth + 1, ShardWidth + 2, 2 * ShardWidth}, before, \"these records are expected\")\n\t\t\tresp = c.Query(t, indexName, fmt.Sprintf(`Delete(ConstRow(columns=[1,2,3,%v]))`, ShardWidth+1))\n\t\t\trequire.NotNil(resp, \"Response should not be nil\")\n\t\t\trequire.NotEmpty(resp.Results)\n\t\t\trequire.Equal(true, resp.Results[0], \"Change should have happened\")\n\n\t\t\tresp = c.Query(t, indexName, `Extract(All())`)\n\n\t\t\t//Note none of the removed records should remain\n\t\t\tm = resp.Results[0].(pilosa.ExtractedTable)\n\t\t\tafter := convert(m.Columns)\n\t\t\trequire.Equal([]uint64{0, ShardWidth, ShardWidth + 2, 2 * ShardWidth}, after, \"these records should be remaining\")\n\t\t})\n\t\tt.Run(\"DeleteKey\", func(t *testing.T) {\n\t\t\tsetupKeys(t, require, c)\n\t\t\tdefer tearDown(t, require, c)\n\t\t\tresp := c.Query(t, indexName, `Extract(All())`)\n\t\t\tm := resp.Results[0].(pilosa.ExtractedTable)\n\t\t\tbefore := convertKey(m.Columns)\n\t\t\tsort.Strings(before)\n\t\t\texpected := []string{\"A\", \"B\", \"C\", \"D\", \"one\", \"two\"}\n\t\t\trequire.Equal(expected, before, \"these keyed records before\")\n\t\t\tresp = c.Query(t, indexName, `Delete(ConstRow(columns=[\"A\",\"one\"]))`)\n\t\t\trequire.NotEmpty(resp.Results)\n\t\t\trequire.Equal(true, resp.Results[0], \"Change should have happened\")\n\n\t\t\tresp = c.Query(t, indexName, `Extract(All())`)\n\t\t\tm = resp.Results[0].(pilosa.ExtractedTable)\n\t\t\tafter := convertKey(m.Columns)\n\t\t\tsort.Strings(after)\n\t\t\trequire.Equal([]string{\"B\", \"C\", \"D\", \"two\"}, after, \"these keyed records after delete\")\n\t\t\t//validate that column keys got deleted\n\t\t\tnode := c.GetNode(0)\n\t\t\tkeys := []string{\"A\", \"one\"}\n\t\t\tres, err := node.API.FindIndexKeys(context.Background(), indexName, keys...)\n\t\t\trequire.Nil(err)\n\t\t\trequire.Empty(res)\n\t\t})\n\t\tt.Run(\"Delete Row\", func(t *testing.T) {\n\t\t\tsetup(t, require, c)\n\t\t\tdefer tearDown(t, require, c)\n\t\t\tresp := c.Query(t, indexName, `Delete(Row(setfield=20))`)\n\t\t\trequire.NotNil(resp, \"Response should not be nil\")\n\t\t\trequire.NotEmpty(resp.Results)\n\t\t\trequire.Equal(true, resp.Results[0], \"Change should have happened\")\n\n\t\t\tresp = c.Query(t, indexName, `Extract(All())`)\n\n\t\t\t//Note none of the removed records should remain\n\t\t\tm := resp.Results[0].(pilosa.ExtractedTable)\n\t\t\tafter := convert(m.Columns)\n\t\t\trequire.Equal([]uint64{0, 1, 2, ShardWidth + 1, ShardWidth + 2, 2 * ShardWidth}, after, \"these records are expected\")\n\t\t})\n\t\tt.Run(\"Delete Not Row\", func(t *testing.T) {\n\t\t\tsetup(t, require, c)\n\t\t\tdefer tearDown(t, require, c)\n\t\t\tresp := c.Query(t, indexName, `Delete(Not(Row(setfield=20)))`)\n\t\t\trequire.NotNil(resp, \"Response should not be nil\")\n\t\t\trequire.NotEmpty(resp.Results)\n\t\t\trequire.Equal(true, resp.Results[0], \"Change should have happened\")\n\n\t\t\tresp = c.Query(t, indexName, `Extract(All())`)\n\n\t\t\t//Note none of the removed records should remain\n\t\t\tm := resp.Results[0].(pilosa.ExtractedTable)\n\t\t\tafter := convert(m.Columns)\n\t\t\trequire.Equal([]uint64{ShardWidth}, after, \"these records are expected\")\n\t\t})\n\t\tt.Run(\"Delete All\", func(t *testing.T) {\n\t\t\tsetup(t, require, c)\n\t\t\tdefer tearDown(t, require, c)\n\t\t\tresp := c.Query(t, indexName, `Count(All())`)\n\t\t\t//Note none of the removed records should remain\n\t\t\tbefore := resp.Results[0].(uint64)\n\t\t\trequire.Equal(uint64(7), before, \"these records are expected\")\n\n\t\t\tresp = c.Query(t, indexName, `Delete(All())`)\n\t\t\trequire.NotNil(resp, \"Response should not be nil\")\n\t\t\trequire.NotEmpty(resp.Results)\n\t\t\trequire.Equal(true, resp.Results[0], \"Change should have happened\")\n\n\t\t\tresp = c.Query(t, indexName, `Count(All())`)\n\t\t\t//Note none of the removed records should remain\n\t\t\tafter := resp.Results[0].(uint64)\n\t\t\trequire.Equal(uint64(0), after, \"these records are expected\")\n\t\t})\n\t\tt.Run(\"DeleteOverlap\", func(t *testing.T) {\n\t\t\tsetupOverlap(t, require, c)\n\t\t\tdefer tearDown(t, require, c)\n\t\t\tresp := c.Query(t, indexName, `Extract(All())`)\n\t\t\tm := resp.Results[0].(pilosa.ExtractedTable)\n\t\t\tbefore := convert(m.Columns)\n\t\t\trequire.Equal([]uint64{0, 1, ShardWidth}, before, \"these records are expected\")\n\t\t\tresp = c.Query(t, indexName, fmt.Sprintf(`Delete(ConstRow(columns=[%v]))`, ShardWidth))\n\t\t\trequire.NotNil(resp, \"Response should not be nil\")\n\t\t\trequire.NotEmpty(resp.Results)\n\t\t\trequire.Equal(true, resp.Results[0], \"Change should have happened\")\n\n\t\t\tresp = c.Query(t, indexName, `Extract(All())`)\n\n\t\t\t//Note none of the removed records should remain\n\t\t\tm = resp.Results[0].(pilosa.ExtractedTable)\n\t\t\tafter := convert(m.Columns)\n\t\t\trequire.Equal([]uint64{0, 1}, after, \"these records should be remaining\")\n\t\t})\n\n\t\t// FB-1281: Delete() calls with an invalid bitmap filter would cause a panic.\n\t\t// This test validates that the error is correctly propagated to the caller.\n\t\tt.Run(\"DeleteWithBitmapError\", func(t *testing.T) {\n\t\t\tsetup(t, require, c)\n\t\t\tdefer tearDown(t, require, c)\n\t\t\t_, err := c.GetPrimary().API.Query(context.Background(), &pilosa.QueryRequest{Index: indexName, Query: `Delete(Row(setfield > 1))`})\n\t\t\t// we don't allow `>` operators on set fields\n\t\t\tif err == nil || !strings.Contains(err.Error(), \"row call: only support\") {\n\t\t\t\tt.Fatalf(\"unexpected error: %s\", err)\n\t\t\t}\n\t\t})\n\t})\n\tt.Run(\"DeleteRecordsBigWithRestart\", func(t *testing.T) {\n\t\t// restarting doesn't work correctly for a shared cluster\n\t\tc := test.MustRunUnsharedCluster(t, 1)\n\t\tdefer c.Close()\n\t\tsetupBig(t, require, c, 16)\n\t\tdefer tearDown(t, require, c)\n\t\tnode := c.GetNode(0)\n\t\tresp := c.Query(t, indexName, `Delete(Row(setfield=12))`)\n\t\trequire.NotNil(resp, \"Response should not be nil\")\n\t\trequire.NotEmpty(resp.Results)\n\t\trequire.Equal(true, resp.Results[0], \"Change should have happened\")\n\t\tresp = c.Query(t, indexName, `Count(Row(setfield=12))`)\n\t\trequire.NotNil(resp, \"Response should not be nil\")\n\t\trequire.NotEmpty(resp.Results)\n\t\trequire.Equal(uint64(0), resp.Results[0], \"Should have removed\")\n\t\terr := node.Reopen()\n\t\trequire.NoError(err, \"restart cluster DeleteRecordsBig\")\n\t\terr = c.AwaitState(disco.ClusterStateNormal, 10*time.Second)\n\t\trequire.NoError(err, \"backToNormal\")\n\t})\n}\n\nfunc convert(before []pilosa.ExtractedTableColumn) []uint64 {\n\tresult := make([]uint64, 0)\n\tfor _, i := range before {\n\t\tresult = append(result, i.Column.ID)\n\t}\n\treturn result\n}\nfunc convertKey(before []pilosa.ExtractedTableColumn) []string {\n\tresult := make([]string, 0)\n\tfor _, i := range before {\n\t\tresult = append(result, i.Column.Key)\n\t}\n\treturn result\n}\n"
        },
        {
          "name": "diagnostics.go",
          "type": "blob",
          "size": 9.1494140625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/logger\"\n\t\"github.com/pkg/errors\"\n)\n\n// Default version check URL.\nconst (\n\tdefaultVersionCheckURL = \"https://diagnostics.pilosa.com/v0/version\"\n)\n\ntype versionResponse struct {\n\tVersion string `json:\"version\"`\n\tMessage string `json:\"message\"`\n}\n\n// diagnosticsCollector represents a collector/sender of diagnostics data.\ntype diagnosticsCollector struct {\n\tmu          sync.Mutex\n\thost        string\n\tVersionURL  string\n\tversion     string\n\tlastVersion string\n\tstartTime   int64\n\tstart       time.Time\n\n\tmetrics map[string]interface{}\n\n\tclient *http.Client\n\n\tLogger logger.Logger\n\n\tserver *Server\n}\n\n// newDiagnosticsCollector returns a new DiagnosticsCollector given an addr in the format \"hostname:port\".\nfunc newDiagnosticsCollector(host string) *diagnosticsCollector { // nolint: unparam\n\treturn &diagnosticsCollector{\n\t\thost:       host,\n\t\tVersionURL: defaultVersionCheckURL,\n\t\tstartTime:  time.Now().Unix(),\n\t\tstart:      time.Now(),\n\t\tclient:     &http.Client{Timeout: 10 * time.Second},\n\t\tmetrics:    make(map[string]interface{}),\n\t\tLogger:     logger.NopLogger,\n\t}\n}\n\n// SetVersion of locally running Pilosa Cluster to check against master.\nfunc (d *diagnosticsCollector) SetVersion(v string) {\n\td.version = v\n\td.Set(\"Version\", v)\n}\n\n// Flush sends the current metrics.\nfunc (d *diagnosticsCollector) Flush() error {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\td.metrics[\"Uptime\"] = (time.Now().Unix() - d.startTime)\n\tbuf, err := d.encode()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"encoding\")\n\t}\n\treq, err := http.NewRequest(\"POST\", d.host, bytes.NewReader(buf))\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"making new request\")\n\t}\n\treq.Header.Set(\"Content-Type\", \"application/json\")\n\tresp, err := d.client.Do(req)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"posting\")\n\t}\n\t// Intentionally ignoring response body, as user does not need to be notified of error.\n\tdefer resp.Body.Close()\n\treturn nil\n}\n\n// CheckVersion of the local build against Pilosa master.\nfunc (d *diagnosticsCollector) CheckVersion() error {\n\tvar rsp versionResponse\n\treq, err := http.NewRequest(\"GET\", d.VersionURL, nil)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"making request\")\n\t}\n\tresp, err := d.client.Do(req)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting version\")\n\t}\n\tdefer resp.Body.Close()\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn fmt.Errorf(\"http: status=%d\", resp.StatusCode)\n\t} else if err := json.NewDecoder(resp.Body).Decode(&rsp); err != nil {\n\t\treturn fmt.Errorf(\"json decode: %s\", err)\n\t}\n\n\t// If version has not changed since the last check, return\n\tif rsp.Version == d.lastVersion {\n\t\treturn nil\n\t}\n\n\td.lastVersion = rsp.Version\n\tif err := d.compareVersion(rsp.Version); err != nil {\n\t\td.Logger.Infof(\"%s\\n\", err.Error())\n\t}\n\n\treturn nil\n}\n\n// compareVersion check version strings.\nfunc (d *diagnosticsCollector) compareVersion(value string) error {\n\tcurrentVersion := versionSegments(value)\n\tlocalVersion := versionSegments(d.version)\n\n\tif localVersion[0] < currentVersion[0] { //Major\n\t\treturn fmt.Errorf(\"you are running Pilosa %s, a newer version (%s) is available: https://github.com/featurebasedb/featurebase/releases\", d.version, value)\n\t} else if localVersion[1] < currentVersion[1] && localVersion[0] == currentVersion[0] { // Minor\n\t\treturn fmt.Errorf(\"you are running Pilosa %s, the latest minor release is %s: https://github.com/featurebasedb/featurebase/releases\", d.version, value)\n\t} else if localVersion[2] < currentVersion[2] && localVersion[0] == currentVersion[0] && localVersion[1] == currentVersion[1] { // Patch\n\t\treturn fmt.Errorf(\"there is a new patch release of Pilosa available: %s: https://github.com/featurebasedb/featurebase/releases\", value)\n\t}\n\n\treturn nil\n}\n\n// Encode metrics maps into the json message format.\nfunc (d *diagnosticsCollector) encode() ([]byte, error) {\n\treturn json.Marshal(d.metrics)\n}\n\n// Set adds a key value metric.\nfunc (d *diagnosticsCollector) Set(name string, value interface{}) {\n\tswitch v := value.(type) {\n\tcase string:\n\t\tif v == \"\" {\n\t\t\t// Do not set empty string\n\t\t\treturn\n\t\t}\n\t}\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\td.metrics[name] = value\n}\n\n// logErr logs the error and returns true if an error exists\nfunc (d *diagnosticsCollector) logErr(err error) bool {\n\tif err != nil {\n\t\td.Logger.Errorf(\"%v\", err)\n\t\treturn true\n\t}\n\treturn false\n}\n\n// EnrichWithCPUInfo adds CPU information to the diagnostics payload.\nfunc (d *diagnosticsCollector) EnrichWithCPUInfo() {\n\td.Set(\"CPUArch\", d.server.systemInfo.CPUArch())\n}\n\n// EnrichWithOSInfo adds OS information to the diagnostics payload.\nfunc (d *diagnosticsCollector) EnrichWithOSInfo() {\n\tuptime, err := d.server.systemInfo.Uptime()\n\tif !d.logErr(err) {\n\t\td.Set(\"HostUptime\", uptime)\n\t}\n\tplatform, err := d.server.systemInfo.Platform()\n\tif !d.logErr(err) {\n\t\td.Set(\"OSPlatform\", platform)\n\t}\n\tfamily, err := d.server.systemInfo.Family()\n\tif !d.logErr(err) {\n\t\td.Set(\"OSFamily\", family)\n\t}\n\tversion, err := d.server.systemInfo.OSVersion()\n\tif !d.logErr(err) {\n\t\td.Set(\"OSVersion\", version)\n\t}\n\tkernelVersion, err := d.server.systemInfo.KernelVersion()\n\tif !d.logErr(err) {\n\t\td.Set(\"OSKernelVersion\", kernelVersion)\n\t}\n}\n\n// EnrichWithMemoryInfo adds memory information to the diagnostics payload.\nfunc (d *diagnosticsCollector) EnrichWithMemoryInfo() {\n\tmemFree, err := d.server.systemInfo.MemFree()\n\tif !d.logErr(err) {\n\t\td.Set(\"MemFree\", memFree)\n\t}\n\tmemTotal, err := d.server.systemInfo.MemTotal()\n\tif !d.logErr(err) {\n\t\td.Set(\"MemTotal\", memTotal)\n\t}\n\tmemUsed, err := d.server.systemInfo.MemUsed()\n\tif !d.logErr(err) {\n\t\td.Set(\"MemUsed\", memUsed)\n\t}\n}\n\n// EnrichWithSchemaProperties adds schema info to the diagnostics payload.\nfunc (d *diagnosticsCollector) EnrichWithSchemaProperties() {\n\tvar numShards uint64\n\tnumFields := 0\n\tnumIndexes := 0\n\tbsiFieldCount := 0\n\ttimeQuantumEnabled := false\n\n\tfor _, index := range d.server.holder.Indexes() {\n\t\tnumShards += index.AvailableShards(includeRemote).Count()\n\t\tnumIndexes++\n\t\tfor _, field := range index.Fields() {\n\t\t\tnumFields++\n\t\t\tif field.Type() == FieldTypeInt || field.Type() == FieldTypeDecimal || field.Type() == FieldTypeTimestamp {\n\t\t\t\tbsiFieldCount++\n\t\t\t}\n\t\t\tif field.TimeQuantum() != \"\" {\n\t\t\t\ttimeQuantumEnabled = true\n\t\t\t}\n\t\t}\n\t}\n\n\td.Set(\"NumIndexes\", numIndexes)\n\td.Set(\"NumFields\", numFields)\n\td.Set(\"NumShards\", numShards)\n\td.Set(\"BSIFieldCount\", bsiFieldCount)\n\td.Set(\"TimeQuantumEnabled\", timeQuantumEnabled)\n}\n\n// versionSegments returns the numeric segments of the version as a slice of ints.\nfunc versionSegments(segments string) []int {\n\tsegments = strings.Trim(segments, \"v\")\n\tsegments = strings.Split(segments, \"-\")[0]\n\ts := strings.Split(segments, \".\")\n\tsegmentSlice := make([]int, len(s))\n\tfor i, v := range s {\n\t\tsegmentSlice[i], _ = strconv.Atoi(v)\n\t}\n\treturn segmentSlice\n}\n\n// SystemInfo collects information about the host OS.\ntype SystemInfo interface {\n\tUptime() (uint64, error)\n\tPlatform() (string, error)\n\tFamily() (string, error)\n\tOSVersion() (string, error)\n\tKernelVersion() (string, error)\n\tMemFree() (uint64, error)\n\tMemTotal() (uint64, error)\n\tMemUsed() (uint64, error)\n\tCPUModel() string\n\tCPUCores() (physical int, logical int, err error)\n\tCPUMHz() (int, error)\n\tCPUArch() string\n\tDiskCapacity(string) (uint64, error)\n}\n\n// newNopSystemInfo creates a no-op implementation of SystemInfo.\nfunc newNopSystemInfo() *nopSystemInfo {\n\treturn &nopSystemInfo{}\n}\n\n// nopSystemInfo is a no-op implementation of SystemInfo.\ntype nopSystemInfo struct {\n}\n\n// Uptime is a no-op implementation of SystemInfo.Uptime.\nfunc (n *nopSystemInfo) Uptime() (uint64, error) {\n\treturn 0, nil\n}\n\n// Platform is a no-op implementation of SystemInfo.Platform.\nfunc (n *nopSystemInfo) Platform() (string, error) {\n\treturn \"\", nil\n}\n\n// Family is a no-op implementation of SystemInfo.Family.\nfunc (n *nopSystemInfo) Family() (string, error) {\n\treturn \"\", nil\n}\n\n// OSVersion is a no-op implementation of SystemInfo.OSVersion.\nfunc (n *nopSystemInfo) OSVersion() (string, error) {\n\treturn \"\", nil\n}\n\n// KernelVersion is a no-op implementation of SystemInfo.KernelVersion.\nfunc (n *nopSystemInfo) KernelVersion() (string, error) {\n\treturn \"\", nil\n}\n\n// MemFree is a no-op implementation of SystemInfo.MemFree.\nfunc (n *nopSystemInfo) MemFree() (uint64, error) {\n\treturn 0, nil\n}\n\n// MemTotal is a no-op implementation of SystemInfo.MemTotal.\nfunc (n *nopSystemInfo) MemTotal() (uint64, error) {\n\treturn 0, nil\n}\n\n// MemUsed is a no-op implementation of SystemInfo.MemUsed.\nfunc (n *nopSystemInfo) MemUsed() (uint64, error) {\n\treturn 0, nil\n}\n\n// CPUArch returns the CPU architecture, such as amd64\nfunc (n *nopSystemInfo) CPUArch() string {\n\treturn \"\"\n}\n\n// CPUModel returns the CPU model string\nfunc (n *nopSystemInfo) CPUModel() string {\n\treturn \"unknown\"\n}\n\n// CPUMHz returns the CPU clock speed\nfunc (n *nopSystemInfo) CPUMHz() (int, error) {\n\treturn 0, nil\n}\n\n// CPUCores returns the number of CPU cores (physical or logical)\nfunc (n *nopSystemInfo) CPUCores() (physical, logical int, err error) {\n\treturn 0, 0, nil\n}\n\n// DiskCapacity returns the disk capacity\nfunc (n *nopSystemInfo) DiskCapacity(path string) (uint64, error) {\n\treturn 0, nil\n}\n"
        },
        {
          "name": "diagnostics_internal_test.go",
          "type": "blob",
          "size": 3.8349609375,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"encoding/json\"\n\t\"net/http\"\n\t\"net/http/httptest\"\n\t\"reflect\"\n\t\"runtime\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/featurebasedb/featurebase/v3/logger\"\n)\n\nfunc TestDiagnosticsClient(t *testing.T) {\n\t// Mock server.\n\tserver := httptest.NewServer(nil)\n\tdefer server.Close()\n\n\t// Create a new client.\n\td := newDiagnosticsCollector(server.URL)\n\n\td.Set(\"gg\", 10)\n\td.Set(\"ss\", \"ss\")\n\n\tdata, err := d.encode()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Test the recorded metrics, note that some types are skipped.\n\tvar eq bool\n\toutput1 := []byte(`{\"gg\":10,\"ss\":\"ss\"}`)\n\tif eq, err = compareJSON(data, output1); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif !eq {\n\t\tt.Fatalf(\"unexpected diagnostics: %+v\", string(data))\n\t}\n\n\t// Test the metrics after a flush.\n\td.Flush()\n\tdata, err = d.encode()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\toutput2 := []byte(`{\"gg\":10,\"ss\":\"ss\",\"Uptime\":0}`)\n\tif eq, err = compareJSON(data, output2); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif !eq {\n\t\tt.Fatalf(\"unexpected diagnostics after flush: %+v\", string(data))\n\t}\n}\n\nfunc TestDiagnosticsVersion_Parse(t *testing.T) {\n\tversion := \"0.1.1\"\n\tvs := versionSegments(version)\n\n\toutput := []int{0, 1, 1}\n\tif !reflect.DeepEqual(vs, output) {\n\t\tt.Fatalf(\"unexpected version: %+v\", vs)\n\t}\n}\n\nfunc TestDiagnosticsVersion_Compare(t *testing.T) {\n\td := newDiagnosticsCollector(\"localhost:10101\")\n\n\tversion := \"v0.1.1\"\n\td.SetVersion(version)\n\n\terr := d.compareVersion(\"v1.7.0\")\n\tif !strings.Contains(err.Error(), \"a newer version\") {\n\t\tt.Fatalf(\"Expected a newer version is available, actual error: %s\", err)\n\t}\n\terr = d.compareVersion(\"1.7.0\")\n\tif !strings.Contains(err.Error(), \"a newer version\") {\n\t\tt.Fatalf(\"Expected a newer version is available, actual error: %s\", err)\n\t}\n\terr = d.compareVersion(\"0.7.0\")\n\tif !strings.Contains(err.Error(), \"the latest minor release is\") {\n\t\tt.Fatalf(\"Expected Minor Version Missmatch, actual error: %s\", err)\n\t}\n\terr = d.compareVersion(\"0.1.2\")\n\tif !strings.Contains(err.Error(), \"there is a new patch release of Pilosa\") {\n\t\tt.Fatalf(\"Expected Patch Version Missmatch, actual error: %s\", err)\n\t}\n\terr = d.compareVersion(\"0.1.1\")\n\tif err != nil {\n\t\tt.Fatalf(\"Versions should match\")\n\t}\n\td.SetVersion(\"v1.7.0\")\n\terr = d.compareVersion(\"0.7.2\")\n\tif err != nil {\n\t\tt.Fatalf(\"Local version is greater\")\n\t}\n}\n\nfunc TestDiagnosticsVersion_Check(t *testing.T) {\n\t// Mock server.\n\tserver := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tw.WriteHeader(http.StatusOK)\n\t\terr := json.NewEncoder(w).Encode(versionResponse{\n\t\t\tVersion: \"1.1.1\",\n\t\t})\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"couldn't encode version response: %v\", err)\n\t\t}\n\t}))\n\tdefer server.Close()\n\n\t// Create a new client.\n\td := newDiagnosticsCollector(\"localhost:10101\")\n\n\tlogs := logger.NewBufferLogger()\n\td.Logger = logs\n\n\tversion := \"0.1.1\"\n\td.SetVersion(version)\n\td.VersionURL = server.URL\n\n\terr := d.CheckVersion()\n\tif err != nil {\n\t\tt.Fatalf(\"checking version: %v\", err)\n\t}\n\n\tallLogs, err := logs.ReadAll()\n\tif err != nil {\n\t\tt.Fatalf(\"reading all logs: %v\", err)\n\t}\n\n\tif !strings.Contains(string(allLogs), \"a newer version\") {\n\t\tt.Fatalf(\"expected version upgrade message, got '%s'\", allLogs)\n\t}\n}\n\nvar _ = compareJSON\n\nfunc compareJSON(a, b []byte) (bool, error) {\n\tvar j1, j2 interface{}\n\tif err := json.Unmarshal(a, &j1); err != nil {\n\t\treturn false, err\n\t}\n\tif err := json.Unmarshal(b, &j2); err != nil {\n\t\treturn false, err\n\t}\n\treturn reflect.DeepEqual(j1, j2), nil\n}\n\nfunc BenchmarkDiagnostics(b *testing.B) {\n\n\t// Mock server.\n\tserver := httptest.NewServer(nil)\n\tdefer server.Close()\n\n\t// Create a new client.\n\td := newDiagnosticsCollector(server.URL)\n\n\tprev := runtime.GOMAXPROCS(4)\n\tdefer runtime.GOMAXPROCS(prev)\n\n\tb.ResetTimer()\n\n\tb.RunParallel(func(pb *testing.PB) {\n\t\tfor pb.Next() {\n\t\t\td.Set(\"cc\", 1)\n\t\t\td.Set(\"gg\", \"test\")\n\t\t}\n\t})\n}\n"
        },
        {
          "name": "disco",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc.go",
          "type": "blob",
          "size": 0.26171875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\n/*\n\nPackage pilosa implements the core of the Pilosa distributed bitmap index. It\ncontains all the domain objects, interfaces, and logic that defines pilosa.\n\n*/\npackage pilosa\n"
        },
        {
          "name": "encoding",
          "type": "tree",
          "content": null
        },
        {
          "name": "errors",
          "type": "tree",
          "content": null
        },
        {
          "name": "et_test.go",
          "type": "blob",
          "size": 0.263671875,
          "content": "package pilosa_test\n\nimport (\n\t\"testing\"\n\n\t\"github.com/featurebasedb/featurebase/v3/test\"\n\t\"github.com/featurebasedb/featurebase/v3/vprint\"\n)\n\nfunc TestEtcd_StartStop(t *testing.T) {\n\tc := test.MustRunUnsharedCluster(t, 1)\n\terr := c.Close()\n\tvprint.VV(\"DONE %v\", err)\n}\n"
        },
        {
          "name": "etcd",
          "type": "tree",
          "content": null
        },
        {
          "name": "event.go",
          "type": "blob",
          "size": 0.4560546875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport \"github.com/featurebasedb/featurebase/v3/disco\"\n\n// NodeEventType are the types of node events.\ntype NodeEventType int\n\n// Constant node event types.\nconst (\n\tNodeJoin NodeEventType = iota\n\tNodeLeave\n\tNodeUpdate\n)\n\n// NodeEvent is a single event related to node activity in the cluster.\ntype NodeEvent struct {\n\tEvent NodeEventType\n\tNode  *disco.Node\n}\n"
        },
        {
          "name": "executor.go",
          "type": "blob",
          "size": 268.7841796875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"math\"\n\t\"math/bits\"\n\t\"reflect\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\t\"unsafe\"\n\n\t\"github.com/featurebasedb/featurebase/v3/dax\"\n\t\"github.com/featurebasedb/featurebase/v3/disco\"\n\t\"github.com/featurebasedb/featurebase/v3/pql\"\n\t\"github.com/featurebasedb/featurebase/v3/proto\"\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\t\"github.com/featurebasedb/featurebase/v3/shardwidth\"\n\t\"github.com/featurebasedb/featurebase/v3/task\"\n\t\"github.com/featurebasedb/featurebase/v3/testhook\"\n\t\"github.com/featurebasedb/featurebase/v3/tracing\"\n\t\"github.com/gomem/gomem/pkg/dataframe\"\n\t\"github.com/lib/pq\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\n// defaultField is the field used if one is not specified.\nconst (\n\tdefaultField = \"general\"\n\n\t// defaultMinThreshold is the lowest count to use in a Top-N operation when\n\t// looking for additional id/count pairs.\n\tdefaultMinThreshold = 1\n\n\tcolumnLabel = \"col\"\n\trowLabel    = \"row\"\n\n\terrConnectionRefused = \"connect: connection refused\"\n)\n\ntype Executor interface {\n\tExecute(context.Context, dax.TableKeyer, *pql.Query, []uint64, *ExecOptions) (QueryResponse, error)\n}\n\n// executor recursively executes calls in a PQL query across all shards.\ntype executor struct {\n\tHolder *Holder\n\n\t// Local hostname & cluster configuration.\n\tNode    *disco.Node\n\tCluster *cluster\n\n\t// Client used for remote requests.\n\tclient *InternalClient\n\n\t// Maximum number of Set() or Clear() commands per request.\n\tMaxWritesPerRequest int\n\n\tshutdown       chan struct{}\n\tworkers        *task.Pool\n\tworkerPoolSize int\n\twork           chan job\n\n\t// track active mapperLocal tasks so we can ensure we don't close\n\t// e.work while they're still waiting to send\n\tactiveMappers uint64\n\n\t// Maximum per-request memory usage (Extract() only)\n\tmaxMemory int64\n\n\t// Temporary flag to be removed when stablized\n\tdataframeEnabled   bool\n\tdatafameUseParquet bool\n}\n\n// executorOption is a functional option type for pilosa.executor\ntype executorOption func(e *executor) error\n\nfunc optExecutorInternalQueryClient(c *InternalClient) executorOption {\n\treturn func(e *executor) error {\n\t\te.client = c\n\t\treturn nil\n\t}\n}\n\nfunc optExecutorWorkerPoolSize(size int) executorOption {\n\treturn func(e *executor) error {\n\t\te.workerPoolSize = size\n\t\treturn nil\n\t}\n}\n\nfunc optExecutorMaxMemory(v int64) executorOption {\n\treturn func(e *executor) error {\n\t\te.maxMemory = v\n\t\treturn nil\n\t}\n}\n\nfunc emptyResult(c *pql.Call) interface{} {\n\tswitch c.Name {\n\tcase \"Clear\", \"ClearRow\":\n\t\treturn false\n\n\tcase \"Row\":\n\t\treturn &Row{Keys: []string{}}\n\n\tcase \"Rows\":\n\t\treturn RowIdentifiers{Keys: []string{}}\n\n\tcase \"IncludesColumn\":\n\t\treturn false\n\t}\n\n\treturn nil\n}\n\n// newExecutor returns a new instance of executor.\nfunc newExecutor(opts ...executorOption) *executor {\n\te := &executor{\n\t\tworkerPoolSize: 2,\n\t\tshutdown:       make(chan struct{}),\n\t}\n\tfor _, opt := range opts {\n\t\terr := opt(e)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n\t// this channel cap doesn't necessarily have to be the same as\n\t// workerPoolSize... any larger doesn't seem to have an effect in\n\t// the few tests we've done at scale with concurrent query\n\t// workloads. Possible that it could be smaller.\n\te.work = make(chan job, e.workerPoolSize)\n\t_ = testhook.Opened(NewAuditor(), e, nil)\n\te.workers = task.NewPool(e.workerPoolSize, e.doOneJob, e)\n\treturn e\n}\n\nfunc (e *executor) Close() error {\n\tselect {\n\tcase <-e.shutdown:\n\t\treturn nil\n\tdefault:\n\t}\n\tclose(e.shutdown)\n\t_ = testhook.Closed(NewAuditor(), e, nil)\n\t// can't close e.work while a mapper is active.\n\tfor atomic.LoadUint64(&e.activeMappers) > 0 {\n\t\ttime.Sleep(1 * time.Millisecond)\n\t}\n\tclose(e.work)\n\te.workers.Close()\n\treturn nil\n}\n\n// PoolSize is exported to let the task pool update us\nfunc (e *executor) PoolSize(n int) {\n\tif e.Holder != nil {\n\t\tGaugeWorkerTotal.Set(float64(n))\n\t}\n}\n\n// InitStats initializes stats counters. Must be called after Holder set.\nfunc (e *executor) InitStats() {\n\tif e.Holder != nil {\n\t\tCounterJobTotal.Add(0)\n\t\tl, _, _ := e.workers.Stats()\n\t\tGaugeWorkerTotal.Set(float64(l))\n\t}\n}\n\n// Execute executes a PQL query.\nfunc (e *executor) Execute(ctx context.Context, tableKeyer dax.TableKeyer, q *pql.Query, shards []uint64, opt *ExecOptions) (QueryResponse, error) {\n\tindex := string(tableKeyer.Key())\n\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.Execute\")\n\tspan.LogKV(\"pql\", q.String())\n\tdefer span.Finish()\n\n\tresp := QueryResponse{}\n\n\t// Check for query cancellation.\n\tif err := validateQueryContext(ctx); err != nil {\n\t\treturn resp, err\n\t}\n\n\t// Verify that an index is set.\n\tif index == \"\" {\n\t\treturn resp, ErrIndexRequired\n\t}\n\n\tidx := e.Holder.Index(index)\n\tif idx == nil {\n\t\treturn resp, newNotFoundError(ErrIndexNotFound, index)\n\t}\n\n\tneedWriteTxn := false\n\tnw := q.WriteCallN()\n\tif nw > 0 {\n\t\tneedWriteTxn = true\n\t}\n\n\t// Verify that the number of writes do not exceed the maximum.\n\tif e.MaxWritesPerRequest > 0 && nw > e.MaxWritesPerRequest {\n\t\treturn resp, ErrTooManyWrites\n\t}\n\n\t// Default options.\n\tif opt == nil {\n\t\topt = &ExecOptions{}\n\t}\n\t// Default maximum memory, if not passed in.\n\tif opt.MaxMemory == 0 && q.HasCall(\"Extract\") {\n\t\topt.MaxMemory = e.maxMemory\n\t}\n\n\tif opt.Profile {\n\t\tvar prof tracing.ProfiledSpan\n\t\tprof, ctx = tracing.StartProfiledSpanFromContext(ctx, \"Execute\")\n\t\tdefer prof.Finish()\n\t\tvar ok bool\n\t\tresp.Profile, ok = prof.(*tracing.Profile)\n\t\tif !ok {\n\t\t\treturn resp, fmt.Errorf(\"profiling execution failed: %T is not tracing.Profile\", prof)\n\t\t}\n\t}\n\n\t// Can't do NewTx() this high up, because we need a specific shard.\n\t// So start a qcx with a TxGroup and pass it down.\n\tvar qcx *Qcx\n\tif needWriteTxn {\n\t\tqcx = idx.holder.txf.NewWritableQcx()\n\t} else {\n\t\tqcx = idx.holder.txf.NewQcx()\n\t}\n\tdefer qcx.Abort()\n\n\tresults, err := e.execute(ctx, qcx, index, q, shards, opt)\n\tif err != nil {\n\t\treturn resp, err\n\t} else if err := validateQueryContext(ctx); err != nil {\n\t\treturn resp, err\n\t}\n\tresp.Results = results\n\n\t// Translate response objects from ids to keys, if necessary.\n\t// No need to translate a remote call.\n\tif !opt.Remote {\n\t\t// only translateResults if this local node is the final destination. only string/column keys.\n\t\tif err := e.translateResults(ctx, index, idx, q.Calls, results, opt.MaxMemory); err != nil {\n\t\t\tif errors.Cause(err) == ErrTranslatingKeyNotFound {\n\t\t\t\t// No error - return empty result\n\t\t\t\tresp.Results = make([]interface{}, len(q.Calls))\n\t\t\t\tfor i, c := range q.Calls {\n\t\t\t\t\tresp.Results[i] = emptyResult(c)\n\t\t\t\t}\n\t\t\t\treturn resp, nil\n\t\t\t}\n\t\t\treturn resp, err\n\t\t} else if err := validateQueryContext(ctx); err != nil {\n\t\t\treturn resp, err\n\t\t}\n\t}\n\t// Must copy out of Tx data before Commiting, because it will become invalid afterwards.\n\trespSafeNoTxData := safeCopy(resp)\n\n\t// Commit transactions if writing; else let the defer grp.Abort do the rollbacks.\n\tif needWriteTxn {\n\t\tif err := qcx.Finish(); err != nil {\n\t\t\treturn respSafeNoTxData, err\n\t\t}\n\t}\n\treturn respSafeNoTxData, nil\n}\n\n// safeCopy copies everything in resp that has Bitmap material,\n// to avoid anything coming from the mmap-ed Tx storage.\nfunc safeCopy(resp QueryResponse) (out QueryResponse) {\n\tout = QueryResponse{\n\t\tErr:     resp.Err,     //  error\n\t\tProfile: resp.Profile, //  *tracing.Profile\n\t}\n\t// Results can contain *roaring.Bitmap, so need to copy from Tx mmap-ed memory.\n\tfor _, v := range resp.Results {\n\t\tswitch x := v.(type) {\n\t\tcase *Row:\n\t\t\trowSafe := x.Clone()\n\t\t\tout.Results = append(out.Results, rowSafe)\n\t\tcase bool:\n\t\t\tout.Results = append(out.Results, x)\n\t\tcase nil:\n\t\t\tout.Results = append(out.Results, nil)\n\t\tcase uint64:\n\t\t\tout.Results = append(out.Results, x) // for counts\n\t\tcase *PairsField:\n\t\t\t// no bitmap material, so should be ok to skip Clone()\n\t\t\tout.Results = append(out.Results, x)\n\t\tcase PairField: // not PairsField but PairField\n\t\t\t// no bitmap material, so should be ok to skip Clone()\n\t\t\tout.Results = append(out.Results, x)\n\t\tcase ValCount:\n\t\t\t// no bitmap material, so should be ok to skip Clone()\n\t\t\tout.Results = append(out.Results, x)\n\t\tcase SignedRow:\n\t\t\t// has *Row in it, so has Bitmap material, and very likely needs Clone.\n\t\t\ty := x.Clone()\n\t\t\tout.Results = append(out.Results, *y)\n\t\tcase GroupCount:\n\t\t\t// no bitmap material, so should be ok to skip Clone()\n\t\t\tout.Results = append(out.Results, x)\n\t\tcase []GroupCount:\n\t\t\tout.Results = append(out.Results, x)\n\t\tcase *GroupCounts:\n\t\t\tout.Results = append(out.Results, x)\n\t\tcase ExtractedTable:\n\t\t\tout.Results = append(out.Results, x)\n\t\tcase ExtractedIDMatrix:\n\t\t\tout.Results = append(out.Results, x)\n\t\tcase RowIdentifiers:\n\t\t\t// no bitmap material, so should be ok to skip Clone()\n\t\t\tout.Results = append(out.Results, x)\n\t\tcase RowIDs:\n\t\t\t// defined as: type RowIDs []uint64\n\t\t\t// so does not contain bitmap material, and\n\t\t\t// should not need to be cloned.\n\t\t\tout.Results = append(out.Results, x)\n\t\tcase []*Row:\n\t\t\tsafe := make([]*Row, len(x))\n\t\t\tfor i, v := range x {\n\t\t\t\tsafe[i] = v.Clone()\n\t\t\t}\n\t\t\tout.Results = append(out.Results, safe)\n\t\tcase DistinctTimestamp:\n\t\t\tout.Results = append(out.Results, x)\n\t\tcase *SortedRow:\n\t\t\tout.Results = append(out.Results, x)\n\t\tcase *dataframe.DataFrame:\n\t\t\t// dumpTable(x)\n\t\t\tout.Results = append(out.Results, x)\n\t\tcase *BasicTable:\n\t\t\t// dumpTable(x)\n\t\t\tout.Results = append(out.Results, x)\n\t\tcase ExtractedIDMatrixSorted:\n\t\t\tout.Results = append(out.Results, x)\n\t\tdefault:\n\t\t\tpanic(fmt.Sprintf(\"handle %T here\", v))\n\t\t}\n\t}\n\treturn\n}\n\n// handlePreCalls traverses the call tree looking for calls that need\n// precomputed values (e.g. Distinct, UnionRows, ConstRow...).\nfunc (e *executor) handlePreCalls(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) error {\n\tif c.Name == \"Precomputed\" {\n\t\tidx := c.Args[\"valueidx\"].(int64)\n\t\tif idx >= 0 && idx < int64(len(opt.EmbeddedData)) {\n\t\t\trow := opt.EmbeddedData[idx]\n\t\t\tc.Precomputed = make(map[uint64]interface{}, len(row.Segments))\n\t\t\tfor _, segment := range row.Segments {\n\t\t\t\tc.Precomputed[segment.shard] = &Row{Segments: []RowSegment{segment}}\n\t\t\t}\n\t\t} else {\n\t\t\treturn fmt.Errorf(\"no precomputed data! index %d, len %d\", idx, len(opt.EmbeddedData))\n\t\t}\n\t\treturn nil\n\t}\n\tnewIndex := c.CallIndex()\n\t// A cross-index query is handled by precall. This is inefficient,\n\t// but we have to do it for now because shards might be different and\n\t// we haven't implemented the local precalls that would be enough\n\t// in some cases.\n\t//\n\t// This makes simple cross-index queries noticably inefficient.\n\t//\n\t// If you're here because of that: We should be using PrecallLocal\n\t// in cases where the call isn't already PrecallGlobal, and\n\t// PrecallLocal should wait until we're running on a specific node\n\t// to do the farming-out of just the sub-queries it has to run\n\t// for its local shards.\n\t//\n\t// As is, we have one node querying every node, then sending out\n\t// all the data to every node, including the data that node already\n\t// has. We could reduce the actual copying around dramatically,\n\t// but only in the cases where local is good enough -- not something\n\t// like Distinct, where you can't predict output shard for a result\n\t// from the shard being queried.\n\tif newIndex != \"\" && newIndex != index {\n\t\tc.Type = pql.PrecallGlobal\n\t\tindex = newIndex\n\t\t// we need to recompute shards, then\n\t\tshards = nil\n\t}\n\tif err := e.handlePreCallChildren(ctx, qcx, index, c, shards, opt); err != nil {\n\t\treturn err\n\t}\n\t// child calls already handled, no precall for this, so we're done\n\tif c.Type == pql.PrecallNone {\n\t\treturn nil\n\t}\n\t// We don't try to handle sub-calls from here. I'm not 100%\n\t// sure that's right, but I think the fact that they're happening\n\t// inside a precomputed call may mean they need different\n\t// handling. In any event, the sub-calls will get handled by\n\t// the executeCall when it gets to them...\n\n\t// We set c to look like a normal call, and actually execute it:\n\tc.Type = pql.PrecallNone\n\t// possibly override call index.\n\tv, err := e.executeCall(ctx, qcx, index, c, shards, opt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tvar row *Row\n\tswitch r := v.(type) {\n\tcase *Row:\n\t\trow = r\n\tcase SignedRow:\n\t\trow = r.Pos\n\tdefault:\n\t\treturn fmt.Errorf(\"precomputed call %s returned unexpected non-Row data: %T\", c.Name, v)\n\t}\n\tif err := ctx.Err(); err != nil {\n\t\treturn err\n\t}\n\tc.Children = []*pql.Call{}\n\tc.Name = \"Precomputed\"\n\tc.Args = map[string]interface{}{\"valueidx\": len(opt.EmbeddedData)}\n\t// stash a copy of the full results, which can be forwarded to other\n\t// shards if the query has to go to them\n\topt.EmbeddedData = append(opt.EmbeddedData, row)\n\t// and stash a copy locally, so local calls can use it\n\tif row != nil {\n\t\tc.Precomputed = make(map[uint64]interface{}, len(row.Segments))\n\t\tfor _, segment := range row.Segments {\n\t\t\tc.Precomputed[segment.shard] = &Row{Segments: []RowSegment{segment}}\n\t\t}\n\t}\n\treturn nil\n}\n\n// dumpPrecomputedCalls throws away precomputed call data. this is used so we\n// can drop any large data associated with a call once we've processed\n// the call.\nfunc (e *executor) dumpPrecomputedCalls(ctx context.Context, c *pql.Call) {\n\tfor _, call := range c.Children {\n\t\te.dumpPrecomputedCalls(ctx, call)\n\t}\n\tc.Precomputed = nil\n}\n\n// handlePreCallChildren handles any pre-calls in the children of a given call.\nfunc (e *executor) handlePreCallChildren(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) error {\n\tfor i := range c.Children {\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := e.handlePreCalls(ctx, qcx, index, c.Children[i], shards, opt); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tfor key, val := range c.Args {\n\t\t// Do not precompute GroupBy aggregates\n\t\tif key == \"aggregate\" {\n\t\t\tcontinue\n\t\t}\n\t\t// Handle Call() operations which exist inside named arguments, too.\n\t\tif call, ok := val.(*pql.Call); ok {\n\t\t\tif err := ctx.Err(); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif err := e.handlePreCalls(ctx, qcx, index, call, shards, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (e *executor) execute(ctx context.Context, qcx *Qcx, index string, q *pql.Query, shards []uint64, opt *ExecOptions) ([]interface{}, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.execute\")\n\tdefer span.Finish()\n\n\t// Apply translations if necessary.\n\tvar colTranslations map[string]map[string]uint64            // colID := colTranslations[index][key]\n\tvar rowTranslations map[string]map[string]map[string]uint64 // rowID := rowTranslations[index][field][key]\n\tif !opt.Remote {\n\t\tcols, rows, err := e.preTranslate(ctx, index, q.Calls...)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tcolTranslations, rowTranslations = cols, rows\n\t}\n\n\tneedShards := false\n\tif len(shards) == 0 {\n\t\tfor _, call := range q.Calls {\n\t\t\tif needsShards(call) {\n\t\t\t\tneedShards = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\tif needShards {\n\t\t// Round up the number of shards.\n\t\tidx := e.Holder.Index(index)\n\t\tif idx == nil {\n\t\t\treturn nil, newNotFoundError(ErrIndexNotFound, index)\n\t\t}\n\t\tshards = idx.AvailableShards(includeRemote).Slice()\n\t\tif len(shards) == 0 {\n\t\t\tshards = []uint64{0}\n\t\t}\n\t}\n\n\tlastWasWrite := false\n\t// Execute each call serially.\n\tresults := make([]interface{}, 0, len(q.Calls))\n\tfor i, call := range q.Calls {\n\t\tif lastWasWrite && needsShards(call) && needShards {\n\t\t\t// Round up the number of shards.\n\t\t\tidx := e.Holder.Index(index)\n\t\t\tif idx == nil {\n\t\t\t\treturn nil, newNotFoundError(ErrIndexNotFound, index)\n\t\t\t}\n\t\t\tshards = idx.AvailableShards(includeRemote).Slice()\n\t\t\tif len(shards) == 0 {\n\t\t\t\tshards = []uint64{0}\n\t\t\t}\n\t\t}\n\n\t\tlastWasWrite = call.IsWrite()\n\n\t\tif err := validateQueryContext(ctx); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Apply call translation.\n\t\tif !opt.Remote && !opt.PreTranslated {\n\t\t\ttranslated, err := e.translateCall(call, index, colTranslations, rowTranslations)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"translating call\")\n\t\t\t}\n\t\t\tif translated == nil {\n\t\t\t\tresults = append(results, emptyResult(call))\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tcall = translated\n\t\t}\n\n\t\t// If you actually make a top-level Distinct call, you\n\t\t// want a SignedRow back. Otherwise, it's something else\n\t\t// that will be using it as a row, and we only care\n\t\t// about the positive values, because only positive values\n\t\t// are valid column IDs. So we don't actually eat top-level\n\t\t// pre calls.\n\t\tif call.Name == \"Count\" {\n\t\t\t// Handle count specially, skipping the level directly underneath it.\n\t\t\tfor _, child := range call.Children {\n\t\t\t\terr := e.handlePreCallChildren(ctx, qcx, index, child, shards, opt)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\terr := e.handlePreCallChildren(ctx, qcx, index, call, shards, opt)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t\tvar v interface{}\n\t\tvar err error\n\t\t// Top-level calls don't need to precompute cross-index things,\n\t\t// because we can just pick whatever index we want, but we\n\t\t// still need to handle them. Since everything else was\n\t\t// already precomputed by handlePreCallChildren, though,\n\t\t// we don't need this logic in executeCall.\n\t\tnewIndex := call.CallIndex()\n\t\tif newIndex != \"\" && newIndex != index {\n\t\t\tv, err = e.executeCall(ctx, qcx, newIndex, call, nil, opt)\n\t\t} else {\n\t\t\tv, err = e.executeCall(ctx, qcx, index, call, shards, opt)\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif vc, ok := v.(ValCount); ok {\n\t\t\tvc.Cleanup()\n\t\t\tv = vc\n\t\t}\n\n\t\tresults = append(results, v)\n\t\t// Some Calls can have significant data associated with them\n\t\t// that gets generated during processing, such as Precomputed\n\t\t// values. Dumping the precomputed data, if any, lets the GC\n\t\t// free the memory before we get there.\n\t\te.dumpPrecomputedCalls(ctx, q.Calls[i])\n\t}\n\treturn results, nil\n}\n\n// cleanup removes the integer value (Val) from the ValCount if one of\n// the other fields is in use.\n//\n// ValCounts are normally holding data which is stored as a BSI\n// (integer) under the hood. Sometimes it's convenient to be able to\n// compare the underlying integer values rather than their\n// interpretation as decimal, timestamp, etc, so the lower level\n// functions may return both integer and the interpreted value, but we\n// don't want to pass that all the way back to the client, so we\n// remove it here.\nfunc (vc *ValCount) Cleanup() {\n\tif vc.Val != 0 && (vc.FloatVal != 0 || !vc.TimestampVal.IsZero() || vc.DecimalVal != nil) {\n\t\tvc.Val = 0\n\t}\n}\n\n// preprocessQuery expands any calls that need preprocessing.\nfunc (e *executor) preprocessQuery(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (*pql.Call, error) {\n\tswitch c.Name {\n\tcase \"All\":\n\t\t_, hasLimit, err := c.UintArg(\"limit\")\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\t_, hasOffset, err := c.UintArg(\"offset\")\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif !hasLimit && !hasOffset {\n\t\t\treturn c, nil\n\t\t}\n\n\t\t// Rewrite the All() w/ limit to Limit(All()).\n\t\tc.Children = []*pql.Call{\n\t\t\t{\n\t\t\t\tName: \"All\",\n\t\t\t},\n\t\t}\n\t\tc.Name = \"Limit\"\n\t\treturn c, nil\n\n\tdefault:\n\t\t// Recurse through child calls.\n\t\tout := make([]*pql.Call, len(c.Children))\n\t\tvar changed bool\n\t\tfor i, child := range c.Children {\n\t\t\tres, err := e.preprocessQuery(ctx, qcx, index, child, shards, opt)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tif res != child {\n\t\t\t\tchanged = true\n\t\t\t}\n\t\t\tout[i] = res\n\t\t}\n\t\tif changed {\n\t\t\tc = c.Clone()\n\t\t\tc.Children = out\n\t\t}\n\t\treturn c, nil\n\t}\n}\n\n// executeCall executes a call.\nfunc (e *executor) executeCall(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (interface{}, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeCall\")\n\tdefer span.Finish()\n\n\tif err := validateQueryContext(ctx); err != nil {\n\t\treturn nil, err\n\t} else if err := e.validateCallArgs(c); err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating args\")\n\t}\n\n\tlabels := prometheus.Labels{\"index\": index}\n\tstatFn := func(ctr *prometheus.CounterVec) {\n\t\tif !opt.Remote {\n\t\t\tctr.With(labels).Inc()\n\t\t}\n\t}\n\n\t// Fixes #2009\n\t// See: https://github.com/featurebasedb/featurebase/issues/2009\n\t// TODO: Remove at version 2.0\n\tif e.detectRangeCall(c) {\n\t\te.Holder.Logger.Infof(\"DEPRECATED: Range() is deprecated, please use Row() instead.\")\n\t}\n\n\t// If shards are specified, then use that value for shards. If shards aren't\n\t// specified, then include all of them.\n\tif shards == nil && needsShards(c) {\n\t\t// Round up the number of shards.\n\t\tidx := e.Holder.Index(index)\n\t\tif idx == nil {\n\t\t\treturn nil, newNotFoundError(ErrIndexNotFound, index)\n\t\t}\n\t\tshards = idx.AvailableShards(includeRemote).Slice()\n\t\tif len(shards) == 0 {\n\t\t\tshards = []uint64{0}\n\t\t}\n\t}\n\t// Preprocess the query.\n\tc, err := e.preprocessQuery(ctx, qcx, index, c, shards, opt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tswitch c.Name {\n\tcase \"Sum\":\n\t\tstatFn(CounterQuerySumTotal)\n\t\tres, err := e.executeSum(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeSum\")\n\tcase \"Min\":\n\t\tstatFn(CounterQueryMinTotal)\n\t\tres, err := e.executeMin(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeMin\")\n\tcase \"Max\":\n\t\tstatFn(CounterQueryMaxTotal)\n\t\tres, err := e.executeMax(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeMax\")\n\tcase \"MinRow\":\n\t\tstatFn(CounterQueryMinRowTotal)\n\t\tres, err := e.executeMinRow(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeMinRow\")\n\tcase \"MaxRow\":\n\t\tstatFn(CounterQueryMaxRowTotal)\n\t\tres, err := e.executeMaxRow(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeMaxRow\")\n\tcase \"Clear\":\n\t\tstatFn(CounterQueryClearTotal)\n\t\tres, err := e.executeClearBit(ctx, qcx, index, c, opt)\n\t\treturn res, errors.Wrap(err, \"executeClearBit\")\n\tcase \"ClearRow\":\n\t\tstatFn(CounterQueryClearRowTotal)\n\t\tres, err := e.executeClearRow(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeClearRow\")\n\tcase \"Distinct\":\n\t\tstatFn(CounterQueryDistinctTotal)\n\t\tres, err := e.executeDistinct(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeDistinct\")\n\tcase \"Store\":\n\t\tstatFn(CounterQueryStoreTotal)\n\t\tres, err := e.executeSetRow(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeSetRow\")\n\tcase \"Count\":\n\t\tstatFn(CounterQueryCountTotal)\n\t\tres, err := e.executeCount(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeCount\")\n\tcase \"Set\":\n\t\tstatFn(CounterQuerySetTotal)\n\t\tres, err := e.executeSet(ctx, qcx, index, c, opt)\n\t\treturn res, errors.Wrap(err, \"executeSet\")\n\tcase \"TopK\":\n\t\tstatFn(CounterQueryTopKTotal)\n\t\tres, err := e.executeTopK(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeTopK\")\n\tcase \"TopN\":\n\t\tstatFn(CounterQueryTopNTotal)\n\t\tres, err := e.executeTopN(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeTopN\")\n\tcase \"Rows\":\n\t\tstatFn(CounterQueryRowsTotal)\n\t\tres, err := e.executeRows(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeRows\")\n\tcase \"ExternalLookup\":\n\t\tstatFn(CounterQueryExternalLookupTotal)\n\t\tres, err := e.executeExternalLookup(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeExternalLookup\")\n\tcase \"Extract\":\n\t\tstatFn(CounterQueryExtractTotal)\n\t\tres, err := e.executeExtract(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeExtract\")\n\tcase \"GroupBy\":\n\t\tstatFn(CounterQueryGroupByTotal)\n\t\tres, err := e.executeGroupBy(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeGroupBy\")\n\tcase \"Options\":\n\t\tstatFn(CounterQueryOptionsTotal)\n\t\tres, err := e.executeOptionsCall(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeOptionsCall\")\n\tcase \"IncludesColumn\":\n\t\tstatFn(CounterQueryIncludesColumnTotal)\n\t\tres, err := e.executeIncludesColumnCall(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeIncludesColumnCall\")\n\tcase \"FieldValue\":\n\t\tstatFn(CounterQueryFieldValueTotal)\n\t\tres, err := e.executeFieldValueCall(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeFieldValueCall\")\n\tcase \"Precomputed\":\n\t\tstatFn(CounterQueryPrecomputedTotal)\n\t\tres, err := e.executePrecomputedCall(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executePrecomputedCall\")\n\tcase \"UnionRows\":\n\t\tstatFn(CounterQueryUnionRowsTotal)\n\t\tres, err := e.executeUnionRows(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeUnionRows\")\n\tcase \"ConstRow\":\n\t\tstatFn(CounterQueryConstRowTotal)\n\t\tres, err := e.executeConstRow(ctx, qcx, index, c, opt)\n\t\treturn res, errors.Wrap(err, \"executeConstRow\")\n\tcase \"Limit\":\n\t\tstatFn(CounterQueryLimitTotal)\n\t\tres, err := e.executeLimitCall(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeLimitCall\")\n\tcase \"Percentile\":\n\t\tstatFn(CounterQueryPercentileTotal)\n\t\tres, err := e.executePercentile(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executePercentile\")\n\tcase \"Delete\":\n\t\tstatFn(CounterQueryDeleteTotal)\n\t\tres, err := e.executeDeleteRecords(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeDelete\")\n\tcase \"Sort\":\n\t\tstatFn(CounterQuerySortTotal)\n\t\tres, err := e.executeSort(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeSort\")\n\tcase \"Apply\":\n\t\tstatFn(CounterQueryApplyTotal)\n\t\tres, err := e.executeApply(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeApply\")\n\tcase \"Arrow\":\n\t\tstatFn(CounterQueryArrowTotal)\n\t\tres, err := e.executeArrow(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeArrow\")\n\tdefault: // e.g. \"Row\", \"Union\", \"Intersect\" or anything that returns a bitmap.\n\t\tres, err := e.executeBitmapCall(ctx, qcx, index, c, shards, opt)\n\t\treturn res, errors.Wrap(err, \"executeBitmapCall\")\n\t}\n}\n\n// validateCallArgs ensures that the value types in call.Args are expected.\nfunc (e *executor) validateCallArgs(c *pql.Call) error {\n\tif _, ok := c.Args[\"ids\"]; ok {\n\t\tswitch v := c.Args[\"ids\"].(type) {\n\t\tcase []int64, []uint64:\n\t\t\t// noop\n\t\tcase []interface{}:\n\t\t\tb := make([]int64, len(v))\n\t\t\tfor i := range v {\n\t\t\t\tb[i] = v[i].(int64)\n\t\t\t}\n\t\t\tc.Args[\"ids\"] = b\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"invalid call.Args[ids]: %s\", v)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// if call args include time args (\"from\"/\"to\")\n// check if field is a time field\nfunc (e *executor) validateTimeCallArgs(c *pql.Call, indexName string) error {\n\tf, err := c.FieldArg()\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, from := c.Args[\"from\"]\n\t_, to := c.Args[\"to\"]\n\tfield := e.Holder.Field(indexName, f)\n\ttq := field.TimeQuantum()\n\tif (from || to) && tq == \"\" {\n\t\treturn fmt.Errorf(\"field %s is not a time-field, 'from' and 'to' are not valid options for this field type\", f)\n\t}\n\n\treturn nil\n}\n\nfunc (e *executor) executeOptionsCall(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (interface{}, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeOptionsCall\")\n\tdefer span.Finish()\n\n\toptCopy := &ExecOptions{}\n\t*optCopy = *opt\n\tif arg, ok := c.Args[\"shards\"]; ok {\n\t\tif optShards, ok := arg.([]interface{}); ok {\n\t\t\tshards = []uint64{}\n\t\t\tfor _, s := range optShards {\n\t\t\t\tif shard, ok := s.(int64); ok {\n\t\t\t\t\tshards = append(shards, uint64(shard))\n\t\t\t\t} else {\n\t\t\t\t\treturn nil, errors.New(\"Query(): shards must be a list of unsigned integers\")\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\treturn nil, errors.New(\"Query(): shards must be a list of unsigned integers\")\n\t\t}\n\t}\n\treturn e.executeCall(ctx, qcx, index, c.Children[0], shards, optCopy)\n}\n\n// executeIncludesColumnCall executes an IncludesColumn() call.\nfunc (e *executor) executeIncludesColumnCall(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (bool, error) {\n\t// Get the shard containing the column, since that's the only\n\t// shard that needs to execute this query.\n\tvar shard uint64\n\tcol, ok, err := c.UintArg(\"column\")\n\tif err != nil {\n\t\treturn false, errors.Wrap(err, \"getting column from args\")\n\t} else if !ok {\n\t\treturn false, errors.New(\"IncludesColumn call must specify a column\")\n\t}\n\tshard = col / ShardWidth\n\n\t// If shard is not in shards, bail early.\n\tif !uint64InSlice(shard, shards) {\n\t\treturn false, nil\n\t}\n\n\t// Execute calls in bulk on each remote node and merge.\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\treturn e.executeIncludesColumnCallShard(ctx, qcx, index, c, shard, col)\n\t}\n\n\t// Merge returned results at coordinating node.\n\treduceFn := func(ctx context.Context, prev, v interface{}) interface{} {\n\t\tother, _ := prev.(bool)\n\t\treturn other || v.(bool)\n\t}\n\n\tresult, err := e.mapReduce(ctx, index, []uint64{shard}, c, opt, mapFn, reduceFn)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\treturn result.(bool), nil\n}\n\n// executeFieldValueCall executes a FieldValue() call.\nfunc (e *executor) executeFieldValueCall(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (_ ValCount, err error) {\n\tfieldName, ok := c.Args[\"field\"].(string)\n\tif !ok || fieldName == \"\" {\n\t\treturn ValCount{}, ErrFieldRequired\n\t}\n\n\tcolKey, ok := c.Args[\"column\"]\n\tif !ok || colKey == \"\" {\n\t\treturn ValCount{}, ErrColumnRequired\n\t}\n\n\t// Fetch index.\n\tidx := e.Holder.Index(index)\n\tif idx == nil {\n\t\treturn ValCount{}, newNotFoundError(ErrIndexNotFound, index)\n\t}\n\n\t// Fetch field.\n\tfield := idx.Field(fieldName)\n\tif field == nil {\n\t\treturn ValCount{}, newNotFoundError(ErrFieldNotFound, fieldName)\n\t}\n\n\tcolID, ok, err := c.UintArg(\"column\")\n\tif !ok || err != nil {\n\t\treturn ValCount{}, errors.Wrap(err, \"getting column argument\")\n\t}\n\n\tshard := colID / ShardWidth\n\n\t// Execute calls in bulk on each remote node and merge.\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\treturn e.executeFieldValueCallShard(ctx, qcx, field, colID, shard)\n\t}\n\n\t// Select single returned result at coordinating node.\n\treduceFn := func(ctx context.Context, prev, v interface{}) interface{} {\n\t\tother, _ := prev.(ValCount)\n\t\tif other.Count == 1 {\n\t\t\treturn other\n\t\t}\n\t\treturn v\n\t}\n\n\tresult, err := e.mapReduce(ctx, index, []uint64{shard}, c, opt, mapFn, reduceFn)\n\tif err != nil {\n\t\treturn ValCount{}, errors.Wrap(err, \"map reduce\")\n\t}\n\tother, _ := result.(ValCount)\n\n\treturn other, nil\n}\n\nfunc (e *executor) executeFieldValueCallShard(ctx context.Context, qcx *Qcx, field *Field, col uint64, shard uint64) (_ ValCount, err0 error) {\n\tvalue, exists, err := field.Value(qcx, col)\n\tif err != nil {\n\t\treturn ValCount{}, errors.Wrap(err, \"getting field value\")\n\t} else if !exists {\n\t\treturn ValCount{}, nil\n\t}\n\n\tother := ValCount{\n\t\tCount: 1,\n\t}\n\n\tif field.Type() == FieldTypeInt {\n\t\tother.Val = value\n\t} else if field.Type() == FieldTypeDecimal {\n\t\tdec := pql.NewDecimal(value, field.Options().Scale)\n\t\tother.DecimalVal = &dec\n\t\tother.FloatVal = 0\n\t\tother.Val = 0\n\t} else if field.Type() == FieldTypeTimestamp {\n\t\tts, err := ValToTimestamp(field.Options().TimeUnit, value)\n\t\tif err != nil {\n\t\t\treturn ValCount{}, err\n\t\t}\n\t\tother.TimestampVal = ts\n\t}\n\n\treturn other, nil\n}\n\n// executeLimitCall executes a Limit() call.\nfunc (e *executor) executeLimitCall(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (*Row, error) {\n\tbitmapCall := c.Children[0]\n\n\tlimit, hasLimit, err := c.UintArg(\"limit\")\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"getting limit\")\n\t}\n\toffset, _, err := c.UintArg(\"offset\")\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"getting offset\")\n\t}\n\n\tif !hasLimit {\n\t\tlimit = math.MaxUint64\n\t}\n\n\t// Execute bitmap call, storing the full result on this node.\n\tres, err := e.executeCall(ctx, qcx, index, bitmapCall, shards, opt)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"limit map reduce\")\n\t}\n\tif res == nil {\n\t\tres = NewRow()\n\t}\n\n\tresult, ok := res.(*Row)\n\tif !ok {\n\t\treturn nil, errors.Errorf(\"expected Row but got %T\", result)\n\t}\n\n\tif offset != 0 {\n\t\ti := 0\n\t\tvar leadingBits []uint64\n\t\tfor i < len(result.Segments) && offset > 0 {\n\t\t\tseg := result.Segments[i]\n\t\t\tcount := seg.Count()\n\t\t\tif count > offset {\n\t\t\t\tdata := seg.Columns()\n\t\t\t\tdata = data[offset:]\n\t\t\t\tleadingBits = data\n\t\t\t\ti++\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\toffset -= count\n\t\t\ti++\n\t\t}\n\t\trow := NewRow(leadingBits...)\n\t\trow.Merge(&Row{Segments: result.Segments[i:]})\n\t\tresult = row\n\t}\n\tif limit < result.Count() {\n\t\ti := 0\n\t\tvar trailingBits []uint64\n\t\tfor i < len(result.Segments) && limit > 0 {\n\t\t\tseg := result.Segments[i]\n\t\t\tcount := seg.Count()\n\t\t\tif count > limit {\n\t\t\t\tdata := seg.Columns()\n\t\t\t\tdata = data[:limit]\n\t\t\t\ttrailingBits = data\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tlimit -= count\n\t\t\ti++\n\t\t}\n\t\trow := NewRow(trailingBits...)\n\t\trow.Merge(&Row{Segments: result.Segments[:i]})\n\t\tresult = row\n\t}\n\n\treturn result, nil\n}\n\n// executeIncludesColumnCallShard\nfunc (e *executor) executeIncludesColumnCallShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64, column uint64) (_ bool, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeIncludesColumnCallShard\")\n\tdefer span.Finish()\n\n\tif len(c.Children) == 1 {\n\t\trow, err := e.executeBitmapCallShard(ctx, qcx, index, c.Children[0], shard)\n\t\tif err != nil {\n\t\t\treturn false, errors.Wrap(err, \"executing bitmap call\")\n\t\t}\n\t\treturn row.Includes(column), nil\n\t}\n\n\treturn false, errors.New(\"IncludesColumn call must specify a row query\")\n}\n\n// executeSum executes a Sum() call.\nfunc (e *executor) executeSum(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (_ ValCount, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeSum\")\n\tdefer span.Finish()\n\n\tfieldName, err := c.FirstStringArg(\"field\", \"_field\")\n\tif err != nil {\n\t\treturn ValCount{}, errors.Wrap(err, \"Sum(): field required\")\n\t}\n\n\tif len(c.Children) > 1 {\n\t\treturn ValCount{}, errors.New(\"Sum() only accepts a single bitmap input\")\n\t}\n\n\t// Execute calls in bulk on each remote node and merge.\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\treturn e.executeSumCountShard(ctx, qcx, index, c, nil, shard)\n\t}\n\n\t// Merge returned results at coordinating node.\n\treduceFn := func(ctx context.Context, prev, v interface{}) interface{} {\n\t\tother, _ := prev.(ValCount)\n\t\treturn other.Add(v.(ValCount))\n\t}\n\n\tresult, err := e.mapReduce(ctx, index, shards, c, opt, mapFn, reduceFn)\n\tif err != nil {\n\t\treturn ValCount{}, err\n\t}\n\tother, _ := result.(ValCount)\n\n\tif other.Count == 0 {\n\t\treturn ValCount{}, nil\n\t}\n\n\t// scale summed response if it's a decimal field and this is\n\t// not a remote query (we're about to return to original client).\n\tif !opt.Remote {\n\t\tfield := e.Holder.Field(index, fieldName)\n\t\tif field == nil {\n\t\t\treturn ValCount{}, newNotFoundError(ErrFieldNotFound, fieldName)\n\t\t}\n\t\tif field.Type() == FieldTypeDecimal {\n\t\t\tdec := pql.NewDecimal(other.Val, field.Options().Scale)\n\t\t\tother.DecimalVal = &dec\n\t\t\tother.FloatVal = 0\n\t\t\tother.Val = 0\n\t\t}\n\t}\n\n\treturn other, nil\n}\n\n// executeDistinct executes a Distinct call on a field. It returns a\n// SignedRow for int fields and a *Row for set/mutex/time fields.\nfunc (e *executor) executeDistinct(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (interface{}, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeDistinct\")\n\tdefer span.Finish()\n\n\tfield, hasField, err := c.StringArg(\"field\")\n\tif err != nil {\n\t\treturn SignedRow{}, errors.Wrap(err, \"loading field option in Distinct query\")\n\t} else if !hasField {\n\t\treturn SignedRow{}, fmt.Errorf(\"missing field option in Distinct query\")\n\t}\n\n\t// Execute calls in bulk on each remote node and merge.\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\treturn e.executeDistinctShard(ctx, qcx, index, field, c, shard)\n\t}\n\n\t// Merge returned results at coordinating node.\n\treduceFn := func(ctx context.Context, prev, v interface{}) interface{} {\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tswitch other := prev.(type) {\n\t\tcase SignedRow:\n\t\t\treturn other.Union(v.(SignedRow))\n\t\tcase *Row:\n\t\t\tif other == nil {\n\t\t\t\treturn v\n\t\t\t} else if v.(*Row) == nil {\n\t\t\t\treturn other\n\t\t\t}\n\t\t\treturn other.Union(v.(*Row))\n\t\tcase nil:\n\t\t\treturn v\n\t\tcase DistinctTimestamp:\n\t\t\treturn other.Union(v.(DistinctTimestamp))\n\t\tdefault:\n\t\t\treturn errors.Errorf(\"unexpected return type from executeDistinctShard: %+v %T\", other, other)\n\t\t}\n\t}\n\n\tresult, err := e.mapReduce(ctx, index, shards, c, opt, mapFn, reduceFn)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"mapReduce\")\n\t}\n\n\tif other, ok := result.(SignedRow); ok {\n\t\tother.Field = field\n\t}\n\treturn result, nil\n}\n\n// executeMin executes a Min() call.\nfunc (e *executor) executeMin(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (_ ValCount, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeMin\")\n\tdefer span.Finish()\n\n\tif _, err := c.FirstStringArg(\"field\", \"_field\"); err != nil {\n\t\treturn ValCount{}, errors.Wrap(err, \"Min(): field required\")\n\t}\n\n\tif len(c.Children) > 1 {\n\t\treturn ValCount{}, errors.New(\"Min() only accepts a single bitmap input\")\n\t}\n\n\t// Execute calls in bulk on each remote node and merge.\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\treturn e.executeMinShard(ctx, qcx, index, c, shard)\n\t}\n\n\t// Merge returned results at coordinating node.\n\treduceFn := func(ctx context.Context, prev, v interface{}) interface{} {\n\t\tother, _ := prev.(ValCount)\n\t\treturn other.Smaller(v.(ValCount))\n\t}\n\n\tresult, err := e.mapReduce(ctx, index, shards, c, opt, mapFn, reduceFn)\n\tif err != nil {\n\t\treturn ValCount{}, err\n\t}\n\tother, _ := result.(ValCount)\n\n\tif other.Count == 0 {\n\t\treturn ValCount{}, nil\n\t}\n\treturn other, nil\n}\n\n// executeMax executes a Max() call.\nfunc (e *executor) executeMax(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (_ ValCount, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeMax\")\n\tdefer span.Finish()\n\n\tif _, err := c.FirstStringArg(\"field\", \"_field\"); err != nil {\n\t\treturn ValCount{}, errors.Wrap(err, \"Max(): field required\")\n\t}\n\n\tif len(c.Children) > 1 {\n\t\treturn ValCount{}, errors.New(\"Max() only accepts a single bitmap input\")\n\t}\n\n\t// Execute calls in bulk on each remote node and merge.\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\treturn e.executeMaxShard(ctx, qcx, index, c, shard)\n\t}\n\n\t// Merge returned results at coordinating node.\n\treduceFn := func(ctx context.Context, prev, v interface{}) interface{} {\n\t\tother, _ := prev.(ValCount)\n\t\treturn other.Larger(v.(ValCount))\n\t}\n\n\tresult, err := e.mapReduce(ctx, index, shards, c, opt, mapFn, reduceFn)\n\tif err != nil {\n\t\treturn ValCount{}, err\n\t}\n\tother, _ := result.(ValCount)\n\n\tif other.Count == 0 {\n\t\treturn ValCount{}, nil\n\t}\n\treturn other, nil\n}\n\n// executePercentile executes a Percentile() call.\n//\n// To compute the percentile, we find the maximum and minimum values that match\n// our filter (or an implicit filter of \"value isn't null\"), and also count values\n// matching our filter. We convert our percentile to an approximate number of\n// values that should be higher or lower than the desired value. If either of those\n// is zero, we return the minimum/maximum; otherwise, we do a binary search of\n// the range between minimum and maximum, looking for a value which has the\n// desired number of values higher or lower than it.\n//\n// Unfortunately, each step in this process is its own, separate, cluster-wide\n// query. this should be replaced with a modern probabilistic algorithm, which\n// could accumulate statistical information per shard and combine that information\n// in a single pass.\nfunc (e *executor) executePercentile(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (result interface{}, err error) {\n\t// defer func() {\n\t// \tfmt.Fprintf(os.Stderr, \"executePercentile %s: %#v, %v\\n\",\n\t// \t\tc.String(), result, err)\n\t// }()\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executePercentile\")\n\tdefer span.Finish()\n\n\t// get nth\n\tvar nthFloat float64\n\tnthArg := c.Args[\"nth\"]\n\tswitch nthArg := nthArg.(type) {\n\tcase pql.Decimal:\n\t\tnthFloat = nthArg.Float64()\n\tcase int64:\n\t\tnthFloat = float64(nthArg)\n\tcase nil:\n\t\treturn nil, errors.New(\"Percentile(): nth required\")\n\tdefault:\n\t\treturn nil, errors.Errorf(\"Percentile(): invalid nth='%v' of type (%[1]T), should be a number between 0 and 100 inclusive\", c.Args[\"nth\"])\n\t}\n\tif nthFloat < 0 || nthFloat > 100.0 {\n\t\treturn nil, errors.Errorf(\"Percentile(): invalid nth value (%f), should be a number between 0 and 100 inclusive\", nthFloat)\n\t}\n\n\t// get field\n\tfieldName, err := c.FirstStringArg(\"field\", \"_field\")\n\tif err != nil {\n\t\treturn nil, errors.New(\"Percentile(): field required\")\n\t}\n\tfield := e.Holder.Field(index, fieldName)\n\tif field == nil {\n\t\treturn nil, ErrFieldNotFound\n\t}\n\n\t// filter call for min & max\n\tvar filterCall *pql.Call\n\n\t// We want to know the total number of values, so that when we check\n\t// for values <X, or >X, we are also able to infer the number of values\n\t// equal to X.\n\tvar totalCountCall *pql.Call\n\t// check if filter provided\n\tif filterArg, ok := c.Args[\"filter\"].(*pql.Call); ok && filterArg != nil {\n\t\t// You could supply a filter like `Not(x=3)` which would yield values\n\t\t// which exist in the database but are null in this field, we don't\n\t\t// want that.\n\t\tfilterCall = filterArg\n\t\ttotalCountCall = &pql.Call{\n\t\t\tName: \"Count\",\n\t\t\tChildren: []*pql.Call{\n\t\t\t\t{\n\t\t\t\t\tName: \"Intersect\",\n\t\t\t\t\tChildren: []*pql.Call{\n\t\t\t\t\t\tfilterCall,\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName: \"Row\",\n\t\t\t\t\t\t\tArgs: map[string]interface{}{\n\t\t\t\t\t\t\t\tfieldName: &pql.Condition{\n\t\t\t\t\t\t\t\t\tOp:    pql.NEQ,\n\t\t\t\t\t\t\t\t\tValue: nil,\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t} else {\n\t\t// request a count of IS NOT NULL, aka Row(field!=null). We care about\n\t\t// the actual number of results that should exist.\n\t\ttotalCountCall = &pql.Call{\n\t\t\tName: \"Count\",\n\t\t\tChildren: []*pql.Call{\n\t\t\t\t{\n\t\t\t\t\tName: \"Row\",\n\t\t\t\t\tArgs: map[string]interface{}{\n\t\t\t\t\t\tfieldName: &pql.Condition{\n\t\t\t\t\t\t\tOp:    pql.NEQ,\n\t\t\t\t\t\t\tValue: nil,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t}\n\t// total values matched by the filter (if it exists) or that aren't null\n\ttotalCountInterface, err := e.executeCall(ctx, qcx, index, totalCountCall, shards, opt)\n\ttotalCount, ok := totalCountInterface.(uint64)\n\tif !ok || totalCount == 0 {\n\t\t// it's not an error, but the median of nothing is NULL.\n\t\treturn nil, nil\n\t}\n\n\t// We have totalCount values. If nth is 50, we want half the values to be\n\t// above us, and half below us. So for instance, if we have 6 values, we want\n\t// 3 above us, and 3 below us. For odd numbers, we can round these *both*\n\t// down -- for 7 values, we'd want 3 higher, and 3 lower.\n\tdesiredLess := uint64((float64(totalCount) * nthFloat) / 100.0)\n\tdesiredGreater := uint64((float64(totalCount) * (100 - nthFloat)) / 100.0)\n\n\t// get min\n\tvar minVal ValCount\n\n\tif desiredGreater != 0 {\n\t\tq, err := pql.ParseString(fmt.Sprintf(`Min(field=\"%s\")`, fieldName))\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"parsing max call for Percentile\")\n\t\t}\n\t\tminCall := q.Calls[0]\n\t\tif filterCall != nil {\n\t\t\tminCall.Children = append(minCall.Children, filterCall)\n\t\t}\n\t\tminVal, err = e.executeMin(ctx, qcx, index, minCall, shards, opt)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"executing Min call for Percentile\")\n\t\t}\n\n\t\tif desiredLess == 0 {\n\t\t\tif minVal.DecimalVal != nil {\n\t\t\t\tminVal.FloatVal = minVal.DecimalVal.Float64()\n\t\t\t}\n\t\t\treturn minVal, nil\n\t\t}\n\t}\n\n\t// get max\n\tq, err := pql.ParseString(fmt.Sprintf(`Max(field=\"%s\")`, fieldName))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"parsing max call for Percentile\")\n\t}\n\tmaxCall := q.Calls[0]\n\tif filterCall != nil {\n\t\tmaxCall.Children = append(maxCall.Children, filterCall)\n\t}\n\tmaxVal, err := e.executeMax(ctx, qcx, index, maxCall, shards, opt)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"executing Max call for Percentile\")\n\t}\n\n\tif desiredGreater == 0 {\n\t\tif maxVal.DecimalVal != nil {\n\t\t\tmaxVal.FloatVal = maxVal.DecimalVal.Float64()\n\t\t}\n\t\treturn maxVal, nil\n\t}\n\n\t// the logic here is basically identical whether we're doing a decimal field\n\t// or an integer field, but the actual code used to compare maximum and minimum\n\t// values, or extract values from valCount objects, differs.\n\t// So we set up generic functions which will produce the right values.\n\tvar averageMinMax func() interface{}\n\tvar minLessthanMax func() bool\n\tvar maxValueUnder func(interface{})\n\tvar minValueOver func(interface{})\n\n\tif field.options.Type == FieldTypeDecimal {\n\t\tminPtr := minVal.DecimalVal\n\t\tmaxPtr := maxVal.DecimalVal\n\t\tif minPtr == nil {\n\t\t\treturn nil, fmt.Errorf(\"unexpectedly nil min value in percentile\")\n\t\t}\n\t\tif maxPtr == nil {\n\t\t\treturn nil, fmt.Errorf(\"unexpectedly nil max value in percentile\")\n\t\t}\n\t\tmin := *minPtr\n\t\tmax := *maxPtr\n\t\ttwo := pql.NewDecimal(2, 0)\n\t\tone := pql.NewDecimal(1, field.options.Scale)\n\t\taverageMinMax = func() interface{} {\n\t\t\treturn pql.DivideDecimal(pql.AddDecimal(min, max), two)\n\t\t}\n\t\tminLessthanMax = func() bool {\n\t\t\treturn min.LessThan(max)\n\t\t}\n\t\tmaxValueUnder = func(v interface{}) {\n\t\t\tmax = pql.SubtractDecimal(v.(pql.Decimal), one)\n\t\t}\n\t\tminValueOver = func(v interface{}) {\n\t\t\tmin = pql.AddDecimal(v.(pql.Decimal), one)\n\t\t}\n\t} else {\n\t\t// plain BSI field\n\t\tmin := minVal.Val\n\t\tmax := maxVal.Val\n\t\taverageMinMax = func() interface{} {\n\t\t\t// min+max could overflow, in theory, but if they're both odd, we want one\n\t\t\t// higher than min/2 + max/2.\n\t\t\treturn (min / 2) + (max / 2) + (((min % 2) + (max % 2)) / 2)\n\t\t}\n\t\tminLessthanMax = func() bool {\n\t\t\treturn min < max\n\t\t}\n\t\tmaxValueUnder = func(v interface{}) {\n\t\t\tmax = v.(int64) - 1\n\t\t}\n\t\tminValueOver = func(v interface{}) {\n\t\t\tmin = v.(int64) + 1\n\t\t}\n\t}\n\n\t// set up reusable pql.Call objects representing a count (or intersectioncount,\n\t// if we have a filter) with a condition we can alter.\n\tvar countCall, rangeCall *pql.Call\n\trangeCondition := pql.Condition{\n\t\tOp:    pql.LT,\n\t\tValue: nil,\n\t}\n\trangeCall = &pql.Call{\n\t\tName: \"Row\",\n\t\tArgs: map[string]interface{}{\n\t\t\tfieldName: &rangeCondition,\n\t\t},\n\t}\n\tif filterCall == nil {\n\t\tcountCall = &pql.Call{\n\t\t\tName:     \"Count\",\n\t\t\tChildren: []*pql.Call{rangeCall},\n\t\t}\n\t} else {\n\t\tcountCall = &pql.Call{\n\t\t\tName: \"Count\",\n\t\t\tChildren: []*pql.Call{\n\t\t\t\t{\n\t\t\t\t\tName:     \"Intersect\",\n\t\t\t\t\tChildren: []*pql.Call{rangeCall, filterCall},\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t}\n\n\t// estimate nth val, eg median when nth=0.5\n\t// we start with a blind guess of minVal, so if min and max are equal,\n\t// we just fall out of the loop. If they're not, we compute the middle value\n\t// of whatever range we're looking at, and compare it to our expectations of\n\t// how many\n\tvar possibleNthVal interface{}\n\tif minVal.DecimalVal != nil {\n\t\tpossibleNthVal = minVal.DecimalVal\n\t} else {\n\t\tpossibleNthVal = minVal.Val\n\t}\n\tfor minLessthanMax() {\n\t\t// compute average without integer overflow, then correct for division of\n\t\t// odd numbers by 2\n\t\tpossibleNthVal = averageMinMax()\n\t\trangeCondition.Value = possibleNthVal\n\t\trangeCondition.Op = pql.LT\n\t\tleftCount, err := e.executeCount(ctx, qcx, index, countCall, shards, opt)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"executing Count call L for Percentile\")\n\t\t}\n\n\t\t// If there's more things less than possibleNthVal than our desired number\n\t\t// of things less, we need to look at the left side of this.\n\t\tif leftCount > desiredLess {\n\t\t\tmaxValueUnder(possibleNthVal)\n\t\t\tcontinue\n\t\t}\n\n\t\trangeCondition.Op = pql.GT\n\t\trightCount, err := e.executeCount(ctx, qcx, index, countCall, shards, opt)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"executing Count call R for Percentile\")\n\t\t}\n\t\t// If there's more things greater than the desired number, we need to look to the right.\n\t\tif rightCount > desiredGreater {\n\t\t\tminValueOver(possibleNthVal)\n\t\t\tcontinue\n\t\t}\n\n\t\t// min and max may be different, but the number of values above and below this\n\t\t// value are both reasonable. For instance, with 7 items and looking for median,\n\t\t// we'd have 3 less and 3 greater, and we can't really do better than that.\n\t\tbreak\n\t}\n\tswitch v := possibleNthVal.(type) {\n\tcase int64:\n\t\treturn ValCount{\n\t\t\tVal:   v,\n\t\t\tCount: 1,\n\t\t}, nil\n\tcase pql.Decimal:\n\t\treturn ValCount{\n\t\t\tDecimalVal: &v,\n\t\t\tFloatVal:   v.Float64(),\n\t\t\tCount:      1,\n\t\t}, nil\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unexpected percentile Nth value type %T\", possibleNthVal)\n\t}\n}\n\n// executeMinRow executes a MinRow() call.\nfunc (e *executor) executeMinRow(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (_ interface{}, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeMinRow\")\n\tdefer span.Finish()\n\n\tif field := c.Args[\"field\"]; field == \"\" {\n\t\treturn ValCount{}, errors.New(\"MinRow(): field required\")\n\t}\n\n\t// Execute calls in bulk on each remote node and merge.\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\treturn e.executeMinRowShard(ctx, qcx, index, c, shard)\n\t}\n\n\t// Merge returned results at coordinating node.\n\treduceFn := func(ctx context.Context, prev, v interface{}) interface{} {\n\t\t// if minRowID exists, and if it is smaller than the other one return it.\n\t\t// otherwise return the minRowID of the one which exists.\n\t\tif prev == nil {\n\t\t\treturn v\n\t\t} else if v == nil {\n\t\t\treturn prev\n\t\t}\n\t\tprevp, _ := prev.(PairField)\n\t\tvp, _ := v.(PairField)\n\t\tif prevp.Pair.Count > 0 && vp.Pair.Count > 0 {\n\t\t\tif prevp.Pair.ID < vp.Pair.ID {\n\t\t\t\treturn prevp\n\t\t\t}\n\t\t\treturn vp\n\t\t} else if prevp.Pair.Count > 0 {\n\t\t\treturn prevp\n\t\t}\n\t\treturn vp\n\t}\n\n\treturn e.mapReduce(ctx, index, shards, c, opt, mapFn, reduceFn)\n}\n\n// executeMaxRow executes a MaxRow() call.\nfunc (e *executor) executeMaxRow(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (_ interface{}, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeMaxRow\")\n\tdefer span.Finish()\n\n\tif field := c.Args[\"field\"]; field == \"\" {\n\t\treturn ValCount{}, errors.New(\"MaxRow(): field required\")\n\t}\n\n\t// Execute calls in bulk on each remote node and merge.\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\treturn e.executeMaxRowShard(ctx, qcx, index, c, shard)\n\t}\n\n\t// Merge returned results at coordinating node.\n\treduceFn := func(ctx context.Context, prev, v interface{}) interface{} {\n\t\t// if minRowID exists, and if it is smaller than the other one return it.\n\t\t// otherwise return the minRowID of the one which exists.\n\t\tif prev == nil {\n\t\t\treturn v\n\t\t} else if v == nil {\n\t\t\treturn prev\n\t\t}\n\t\tprevp, _ := prev.(PairField)\n\t\tvp, _ := v.(PairField)\n\t\tif prevp.Pair.Count > 0 && vp.Pair.Count > 0 {\n\t\t\tif prevp.Pair.ID > vp.Pair.ID {\n\t\t\t\treturn prevp\n\t\t\t}\n\t\t\treturn vp\n\t\t} else if prevp.Pair.Count > 0 {\n\t\t\treturn prevp\n\t\t}\n\t\treturn vp\n\t}\n\n\treturn e.mapReduce(ctx, index, shards, c, opt, mapFn, reduceFn)\n}\n\n// executePrecomputedCall pretends to execute a call that we have a precomputed value for.\nfunc (e *executor) executePrecomputedCall(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (_ *Row, err error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"executor.executePrecomputedCall\")\n\tdefer span.Finish()\n\tresult := NewRow()\n\n\tfor _, row := range c.Precomputed {\n\t\tresult.Merge(row.(*Row))\n\t}\n\treturn result, nil\n}\n\n// executeBitmapCall executes a call that returns a bitmap.\nfunc (e *executor) executeBitmapCall(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (_ *Row, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeBitmapCall\")\n\tspan.LogKV(\"pqlCallName\", c.Name)\n\tdefer span.Finish()\n\n\tlabels := prometheus.Labels{\"index\": index}\n\tstatFn := func(ctr *prometheus.CounterVec) {\n\t\tif !opt.Remote {\n\t\t\tctr.With(labels).Inc()\n\t\t}\n\t}\n\n\tif !opt.Remote {\n\t\tswitch c.Name {\n\t\tcase \"Row\":\n\t\t\t// We used to do this by checking for \"Condition\", but if we allow\n\t\t\t// checks against null for non-BSI fields, that's not accurate. We\n\t\t\t// can't do this check down in the per-shard stuff, because if we\n\t\t\t// did it there, we'd be incrementing the stats once per shard.\n\t\t\t// This is redundant with what we do there but shouldn't change\n\t\t\t// its behavior, except for possibly the spelling of the diagnostic.\n\t\t\tfieldName, err := c.FieldArg()\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tfield := e.Holder.Field(index, fieldName)\n\t\t\tif field == nil {\n\t\t\t\treturn nil, fmt.Errorf(\"row call with unknown field %s:%s\", index, fieldName)\n\t\t\t}\n\t\t\tbsig := field.bsiGroup(fieldName)\n\t\t\tif bsig != nil {\n\t\t\t\tstatFn(CounterQueryRowBSITotal)\n\t\t\t} else {\n\t\t\t\tstatFn(CounterQueryRowTotal)\n\t\t\t}\n\t\tcase \"Range\":\n\t\t\tstatFn(CounterQueryRangeTotal)\n\t\tcase \"Difference\":\n\t\t\tstatFn(CounterQueryBitmapTotal)\n\t\tcase \"Intersect\":\n\t\t\tstatFn(CounterQueryIntersectTotal)\n\t\tcase \"Union\":\n\t\t\tstatFn(CounterQueryUnionTotal)\n\t\tcase \"InnerUnionRows\":\n\t\t\tstatFn(CounterQueryInnerUnionRowsTotal)\n\t\tcase \"Xor\":\n\t\t\tstatFn(CounterQueryXorTotal)\n\t\tcase \"Not\":\n\t\t\tstatFn(CounterQueryNotTotal)\n\t\tcase \"Shift\":\n\t\t\tstatFn(CounterQueryShiftTotal)\n\t\tcase \"All\":\n\t\t\tstatFn(CounterQueryAllTotal)\n\t\tdefault:\n\t\t\tstatFn(CounterQueryBitmapTotal)\n\t\t}\n\t}\n\n\t// Execute calls in bulk on each remote node and merge.\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\treturn e.executeBitmapCallShard(ctx, qcx, index, c, shard)\n\t}\n\n\t// Merge returned results at coordinating node.\n\treduceFn := func(ctx context.Context, prev, v interface{}) interface{} {\n\t\tother, _ := prev.(*Row)\n\t\tif other == nil {\n\t\t\t// TODO... what's going on on the following line\n\t\t\tother = NewRow() // bug! this row ends up containing Badger Txn data that should be accessed outside the Txn.\n\t\t}\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tother.Merge(v.(*Row))\n\t\treturn other\n\t}\n\n\tother, err := e.mapReduce(ctx, index, shards, c, opt, mapFn, reduceFn)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"map reduce\")\n\t}\n\n\trow, _ := other.(*Row)\n\n\treturn row, nil\n}\n\n// executeBitmapCallShard executes a bitmap call for a single shard.\nfunc (e *executor) executeBitmapCallShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64) (_ *Row, err error) {\n\tif err := validateQueryContext(ctx); err != nil {\n\t\treturn nil, err\n\t}\n\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeBitmapCallShard\")\n\tdefer span.Finish()\n\n\tswitch c.Name {\n\tcase \"Row\", \"Range\":\n\t\treturn e.executeRowShard(ctx, qcx, index, c, shard)\n\tcase \"Difference\":\n\t\treturn e.executeDifferenceShard(ctx, qcx, index, c, shard)\n\tcase \"Intersect\":\n\t\treturn e.executeIntersectShard(ctx, qcx, index, c, shard)\n\tcase \"Union\":\n\t\treturn e.executeUnionShard(ctx, qcx, index, c, shard)\n\tcase \"InnerUnionRows\":\n\t\treturn e.executeInnerUnionRowsShard(ctx, qcx, index, c, shard)\n\tcase \"Xor\":\n\t\treturn e.executeXorShard(ctx, qcx, index, c, shard)\n\tcase \"Not\":\n\t\treturn e.executeNotShard(ctx, qcx, index, c, shard)\n\tcase \"Shift\":\n\t\treturn e.executeShiftShard(ctx, qcx, index, c, shard)\n\tcase \"All\": // Allow a shard computation to use All()\n\t\treturn e.executeAllCallShard(ctx, qcx, index, c, shard)\n\tcase \"Distinct\":\n\t\treturn nil, errors.New(\"Distinct shouldn't be hit as a bitmap call\")\n\tcase \"Precomputed\":\n\t\treturn e.executePrecomputedCallShard(ctx, qcx, index, c, shard)\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown call: %s\", c.Name)\n\t}\n}\n\n// executeDistinctShard executes a Distinct call on a single shard, yielding\n// a SignedRow of the values found.\nfunc (e *executor) executeDistinctShard(ctx context.Context, qcx *Qcx, index string, fieldName string, c *pql.Call, shard uint64) (result interface{}, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeDistinctShard\")\n\tdefer span.Finish()\n\n\tidx := e.Holder.Index(index)\n\tfield := e.Holder.Field(index, fieldName)\n\tif field == nil {\n\t\treturn nil, ErrFieldNotFound\n\t}\n\n\tbsig := field.bsiGroup(fieldName)\n\tif bsig == nil {\n\t\tresult = &Row{\n\t\t\tIndex: index,\n\t\t\tField: fieldName,\n\t\t}\n\t} else if field.Options().Type == FieldTypeTimestamp {\n\t\tresult = DistinctTimestamp{Name: fieldName}\n\t} else {\n\t\tresult = SignedRow{}\n\t}\n\n\tvar filter *Row\n\tvar filterBitmap *roaring.Bitmap\n\t// If a filter *is* specified, an empty filter means nothing, and any\n\t// filter at all means there's filtering to do. If a filter is *not*\n\t// specified, then we don't need to do any filtering. So a nil\n\t// filterBitmap (which we get if there's no children) means no filter.\n\tif len(c.Children) == 1 {\n\t\trow, err := e.executeBitmapCallShard(ctx, qcx, index, c.Children[0], shard)\n\t\tif err != nil {\n\t\t\treturn result, errors.Wrap(err, \"executing bitmap call\")\n\t\t}\n\t\tfilter = row\n\t\tif filter != nil && len(filter.Segments) > 0 {\n\t\t\tfilterBitmap = filter.Segments[0].data\n\t\t}\n\t\t// if we had a filter to consider, but it came back empty, we\n\t\t// can go ahead and save time by returning the empty results,\n\t\t// because the filter excluded everything.\n\t\tif filterBitmap == nil || !filterBitmap.Any() {\n\t\t\treturn result, nil\n\t\t}\n\t}\n\n\tif bsig == nil {\n\t\treturn executeDistinctShardSet(ctx, qcx, idx, fieldName, shard, filterBitmap)\n\t}\n\tif field.Options().Type == FieldTypeTimestamp {\n\t\tr, err := executeDistinctShardBSI(ctx, qcx, idx, fieldName, shard, bsig, filterBitmap)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\t// If we have a filter, or there's just no content for this shard, we\n\t\t// can end up with empty results. Rather than trying to synthesize\n\t\t// a result from this empty set, we just go ahead and use that.\n\t\tif r.Pos == nil {\n\t\t\treturn result, nil\n\t\t}\n\t\tcols := r.Pos.Columns()\n\t\tresults := make([]string, len(cols))\n\t\tfor i, val := range cols {\n\t\t\tt, err := ValToTimestamp(field.options.TimeUnit, int64(val)+bsig.Base)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"translating value to timestamp\")\n\t\t\t}\n\t\t\tresults[i] = t.Format(time.RFC3339Nano)\n\t\t}\n\t\tresult = DistinctTimestamp{Name: fieldName, Values: results}\n\t\treturn result, nil\n\t}\n\treturn executeDistinctShardBSI(ctx, qcx, idx, fieldName, shard, bsig, filterBitmap)\n}\n\ntype DistinctTimestamp struct {\n\tValues []string\n\tName   string\n}\n\nvar _ proto.ToRowser = DistinctTimestamp{}\n\n// ToRows implements the ToRowser interface.\nfunc (d DistinctTimestamp) ToRows(callback func(*proto.RowResponse) error) error {\n\tfor _, ts := range d.Values {\n\t\trow := &proto.RowResponse{\n\t\t\tHeaders: []*proto.ColumnInfo{\n\t\t\t\t{\n\t\t\t\t\tName:     d.Name,\n\t\t\t\t\tDatatype: \"timestamp\",\n\t\t\t\t},\n\t\t\t},\n\t\t\tColumns: []*proto.ColumnResponse{\n\t\t\t\t{\n\t\t\t\t\tColumnVal: &proto.ColumnResponse_TimestampVal{\n\t\t\t\t\t\tTimestampVal: ts,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t\tif err := callback(row); err != nil {\n\t\t\treturn errors.Wrap(err, \"calling callback\")\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// ToTable implements the ToTabler interface for DistinctTimestamp\nfunc (d DistinctTimestamp) ToTable() (*proto.TableResponse, error) {\n\treturn proto.RowsToTable(&d, len(d.Values))\n}\n\n// Union returns the union of the values of `d` and `other`\nfunc (d *DistinctTimestamp) Union(other DistinctTimestamp) DistinctTimestamp {\n\tboth := map[string]struct{}{}\n\tfor _, val := range d.Values {\n\t\tboth[val] = struct{}{}\n\t}\n\tfor _, val := range other.Values {\n\t\tboth[val] = struct{}{}\n\t}\n\tvals := []string{}\n\tfor key := range both {\n\t\tvals = append(vals, key)\n\t}\n\treturn DistinctTimestamp{Name: d.Name, Values: vals}\n}\n\nconst (\n\tErrViewNotFound = Error(\"view not found\")\n)\n\nfunc executeDistinctShardSet(ctx context.Context, qcx *Qcx, idx *Index, fieldName string, shard uint64, filterBitmap *roaring.Bitmap) (result *Row, err0 error) {\n\tindex := idx.Name()\n\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Index: idx, Shard: shard})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer finisher(&err0)\n\n\tfragData, _, err := tx.ContainerIterator(index, fieldName, \"standard\", shard, 0)\n\tswitch errors.Cause(err) {\n\tcase ErrViewNotFound, ErrFragmentNotFound:\n\t\t// It may seem reasonable to return `nil` here in the case where the\n\t\t// fragment for this shard does not exist. The problem with doing that\n\t\t// is that if this operation is being performed on a remote node, then\n\t\t// this result is going to get serialized as a QueryResponse and sent\n\t\t// back to the original, non-remote node. When this happens, the\n\t\t// encodeRow/decodeRow logic replaces `nil` with an empty Row. An empty\n\t\t// Row will cause problems during the union step of the reduce phase if\n\t\t// it is the \"left\" side of the union, because then the resulting Row\n\t\t// after the union will have blank Index and Field values. Here, we\n\t\t// ensure that we send a non-nil Row with valid Index and Field values\n\t\t// so that the union step doesn't cause problems.\n\t\treturn &Row{Index: index, Field: fieldName}, nil\n\tcase nil:\n\tdefault:\n\t\treturn nil, errors.Wrap(err, \"getting fragment data\")\n\t}\n\tdefer fragData.Close()\n\n\t// We can't grab the containers \"for each row\" from the set-type field,\n\t// because we don't know how many rows there are, and some of them\n\t// might be empty, so really, we're going to iterate through the\n\t// containers, and then intersect them with the filter if present.\n\tvar filter []*roaring.Container\n\tif filterBitmap != nil {\n\t\tfilter = make([]*roaring.Container, 1<<shardVsContainerExponent)\n\t\tfilterIterator, _ := filterBitmap.Containers.Iterator(0)\n\t\t// So let's get these all with a nice convenient 0 offset...\n\t\tfor filterIterator.Next() {\n\t\t\tk, c := filterIterator.Value()\n\t\t\tif c.N() == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfilter[k%(1<<shardVsContainerExponent)] = c\n\t\t}\n\t}\n\trows := roaring.NewSliceBitmap()\n\tprevRow := ^uint64(0)\n\tseenThisRow := false\n\tfor fragData.Next() {\n\t\tk, c := fragData.Value()\n\t\trow := k >> shardVsContainerExponent\n\t\tif row == prevRow {\n\t\t\tif seenThisRow {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t} else {\n\t\t\tseenThisRow = false\n\t\t\tprevRow = row\n\t\t}\n\t\tif filterBitmap != nil {\n\t\t\tif roaring.IntersectionAny(c, filter[k%(1<<shardVsContainerExponent)]) {\n\t\t\t\t_, err = rows.Add(row)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, errors.Wrap(err, \"collecting results\")\n\t\t\t\t}\n\t\t\t\tseenThisRow = true\n\t\t\t}\n\t\t} else if c.N() != 0 {\n\t\t\t_, err = rows.Add(row)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"recording results\")\n\t\t\t}\n\t\t\tseenThisRow = true\n\t\t}\n\t}\n\tresult = NewRowFromBitmap(rows)\n\tresult.Index = idx.Name()\n\tresult.Field = fieldName\n\treturn result, nil\n}\n\nfunc executeDistinctShardBSI(ctx context.Context, qcx *Qcx, idx *Index, fieldName string, shard uint64, bsig *bsiGroup, filterBitmap *roaring.Bitmap) (result SignedRow, err0 error) {\n\tview := viewBSIGroupPrefix + fieldName\n\tindex := idx.Name()\n\tdepth := uint64(bsig.BitDepth)\n\toffset := bsig.Base\n\n\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Index: idx, Shard: shard})\n\tif err != nil {\n\t\treturn SignedRow{}, err\n\t}\n\tdefer finisher(&err0)\n\n\texistsBitmap, err := tx.OffsetRange(index, fieldName, view, shard, ShardWidth*shard, ShardWidth*0, ShardWidth*1)\n\tif err != nil {\n\t\tswitch errors.Cause(err) {\n\t\tcase ErrViewNotFound, ErrFragmentNotFound:\n\t\t\treturn result, nil\n\t\t}\n\t\treturn result, errors.Wrap(err, \"getting exists bitmap\")\n\t}\n\tif filterBitmap != nil {\n\t\texistsBitmap = existsBitmap.Intersect(filterBitmap)\n\t}\n\tif !existsBitmap.Any() {\n\t\treturn result, nil\n\t}\n\n\tsignBitmap, err := tx.OffsetRange(index, fieldName, view, shard, ShardWidth*shard, ShardWidth*1, ShardWidth*2)\n\tif err != nil {\n\t\treturn result, errors.Wrap(err, \"getting sign bitmap\")\n\t}\n\n\tdataBitmaps := make([]*roaring.Bitmap, depth)\n\n\tfor i := uint64(0); i < depth; i++ {\n\t\tdataBitmaps[i], err = tx.OffsetRange(index, fieldName, view, shard, ShardWidth*shard, ShardWidth*(i+2), ShardWidth*(i+3))\n\t\tif err != nil {\n\t\t\treturn result, err\n\t\t}\n\t}\n\n\t// we need spaces for sign bit, existence/filter bit, and data\n\t// row bits, which we'll be grabbing 64K bits at a time\n\tstashWords := make([]uint64, 1024*(depth+2))\n\tbitStashes := make([][]uint64, depth)\n\tfor i := uint64(0); i < depth; i++ {\n\t\tstart := i * 1024\n\t\tlast := start + 1024\n\t\tbitStashes[i] = stashWords[start:last]\n\t\ti++\n\t}\n\tstashOffset := depth * 1024\n\texistStash := stashWords[stashOffset : stashOffset+1024]\n\tsignStash := stashWords[stashOffset+1024 : stashOffset+2048]\n\tdataBits := make([][]uint64, depth)\n\n\tposValues := make([]uint64, 0, 64)\n\tnegValues := make([]uint64, 0, 64)\n\n\tposBitmap := roaring.NewFileBitmap()\n\tnegBitmap := roaring.NewFileBitmap()\n\n\texistIterator, _ := existsBitmap.Containers.Iterator(0)\n\tfor existIterator.Next() {\n\t\tkey, value := existIterator.Value()\n\t\tif value.N() == 0 {\n\t\t\tcontinue\n\t\t}\n\t\texists := value.AsBitmap(existStash)\n\t\tsign := signBitmap.Containers.Get(key).AsBitmap(signStash)\n\t\tfor i := uint64(0); i < depth; i++ {\n\t\t\tdataBits[i] = dataBitmaps[i].Containers.Get(key).AsBitmap(bitStashes[i])\n\t\t}\n\t\tfor idx, word := range exists {\n\t\t\t// mask holds a mask we can test the other words against.\n\t\t\tmask := uint64(1)\n\t\t\tfor word != 0 {\n\t\t\t\tshift := uint(bits.TrailingZeros64(word))\n\t\t\t\t// we shift one *more* than that, to move the\n\t\t\t\t// actual one bit off.\n\t\t\t\tword >>= shift + 1\n\t\t\t\tmask <<= shift\n\t\t\t\tvalue := int64(0)\n\t\t\t\tfor b := uint64(0); b < depth; b++ {\n\t\t\t\t\tif dataBits[b][idx]&mask != 0 {\n\t\t\t\t\t\tvalue += (1 << b)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif sign[idx]&mask != 0 {\n\t\t\t\t\tvalue *= -1\n\t\t\t\t}\n\t\t\t\tvalue += int64(offset)\n\t\t\t\tif value < 0 {\n\t\t\t\t\tnegValues = append(negValues, uint64(-value))\n\t\t\t\t} else {\n\t\t\t\t\tposValues = append(posValues, uint64(value))\n\t\t\t\t}\n\t\t\t\t// and now we processed that bit, so we move the mask over one.\n\t\t\t\tmask <<= 1\n\t\t\t}\n\t\t\tif len(negValues) > 0 {\n\t\t\t\t_, _ = negBitmap.AddN(negValues...)\n\t\t\t\tnegValues = negValues[:0]\n\t\t\t}\n\t\t\tif len(posValues) > 0 {\n\t\t\t\t_, _ = posBitmap.AddN(posValues...)\n\t\t\t\tposValues = posValues[:0]\n\t\t\t}\n\t\t}\n\t}\n\n\tresult = SignedRow{\n\t\tNeg: NewRowFromBitmap(negBitmap),\n\t\tPos: NewRowFromBitmap(posBitmap),\n\t}\n\tresult.Neg.Index, result.Pos.Index = idx.Name(), idx.Name()\n\tresult.Neg.Field, result.Pos.Field = fieldName, fieldName\n\treturn result, nil\n}\n\n// executeSumCountShard calculates the sum and count for bsiGroups on a shard.\nfunc (e *executor) executeSumCountShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, filter *Row, shard uint64) (_ ValCount, err0 error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeSumCountShard\")\n\tdefer span.Finish()\n\n\t// use tx to keep consistency between\n\t// the filter and the later count.\n\tidx := e.Holder.Index(index)\n\n\t// Only calculate the filter if it doesn't exist and a child call as been passed in.\n\tif filter == nil && len(c.Children) == 1 {\n\n\t\trow, err := e.executeBitmapCallShard(ctx, qcx, index, c.Children[0], shard)\n\t\tif err != nil {\n\t\t\treturn ValCount{}, errors.Wrap(err, \"executing bitmap call\")\n\t\t}\n\t\tfilter = row\n\t}\n\n\tfieldName, err := c.FirstStringArg(\"field\", \"_field\")\n\tif err != nil {\n\t\treturn ValCount{}, errors.Wrap(err, \"Sum(): field required\")\n\t}\n\n\tfield := e.Holder.Field(index, fieldName)\n\tif field == nil {\n\t\treturn ValCount{}, ErrFieldNotFound\n\t}\n\n\tbsig := field.bsiGroup(fieldName)\n\tif bsig == nil {\n\t\treturn ValCount{}, nil\n\t}\n\n\tfragment := e.Holder.fragment(index, fieldName, viewBSIGroupPrefix+fieldName, shard)\n\tif fragment == nil {\n\t\treturn ValCount{}, nil\n\t}\n\n\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Index: idx, Fragment: fragment, Shard: shard})\n\tif err != nil {\n\t\treturn ValCount{}, err\n\t}\n\tdefer finisher(&err0)\n\n\tsumspan, _ := tracing.StartSpanFromContext(ctx, \"executor.executeSumCountShard_fragment.sum\")\n\tdefer sumspan.Finish()\n\tvsum, vcount, err := fragment.sum(tx, filter, bsig.BitDepth)\n\tif err != nil {\n\t\treturn ValCount{}, errors.Wrap(err, \"computing sum\")\n\t}\n\tout := ValCount{\n\t\tVal:   int64(vsum) + (int64(vcount) * bsig.Base),\n\t\tCount: int64(vcount),\n\t}\n\tif field.Type() == FieldTypeDecimal {\n\t\tout.FloatVal = float64(int64(vsum)+(int64(vcount)*bsig.Base)) / math.Pow(10, float64(bsig.Scale))\n\t\tdec := pql.NewDecimal((int64(vsum) + (int64(vcount) * bsig.Base)), bsig.Scale)\n\t\tout.DecimalVal = &dec\n\t}\n\treturn out, nil\n}\n\n// executeMinShard calculates the min for bsiGroups on a shard.\nfunc (e *executor) executeMinShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64) (_ ValCount, err0 error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeMinShard\")\n\tdefer span.Finish()\n\n\tvar filter *Row\n\tif len(c.Children) == 1 {\n\t\trow, err := e.executeBitmapCallShard(ctx, qcx, index, c.Children[0], shard)\n\t\tif err != nil {\n\t\t\treturn ValCount{}, err\n\t\t}\n\t\tfilter = row\n\t}\n\n\tfieldName, err := c.FirstStringArg(\"field\", \"_field\")\n\tif err != nil {\n\t\treturn ValCount{}, errors.Wrap(err, \"Min(): field required\")\n\t}\n\n\tfield := e.Holder.Field(index, fieldName)\n\tif field == nil {\n\t\treturn ValCount{}, ErrFieldNotFound\n\t}\n\treturn field.MinForShard(qcx, shard, filter)\n}\n\n// executeMaxShard calculates the max for bsiGroups on a shard.\nfunc (e *executor) executeMaxShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64) (_ ValCount, err0 error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"Executor.executeMaxShard\")\n\tdefer span.Finish()\n\n\tvar filter *Row\n\tif len(c.Children) == 1 {\n\t\trow, err := e.executeBitmapCallShard(ctx, qcx, index, c.Children[0], shard)\n\t\tif err != nil {\n\t\t\treturn ValCount{}, err\n\t\t}\n\t\tfilter = row\n\t}\n\n\tfieldName, err := c.FirstStringArg(\"field\", \"_field\")\n\tif err != nil {\n\t\treturn ValCount{}, errors.Wrap(err, \"Max(): field required\")\n\t}\n\n\tfield := e.Holder.Field(index, fieldName)\n\tif field == nil {\n\t\treturn ValCount{}, ErrFieldNotFound\n\t}\n\treturn field.MaxForShard(qcx, shard, filter)\n}\n\n// executeMinRowShard returns the minimum row ID for a shard.\nfunc (e *executor) executeMinRowShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64) (_ PairField, err0 error) {\n\tvar filter *Row\n\n\tif len(c.Children) == 1 {\n\t\trow, err := e.executeBitmapCallShard(ctx, qcx, index, c.Children[0], shard)\n\t\tif err != nil {\n\t\t\treturn PairField{}, err\n\t\t}\n\t\tfilter = row\n\t}\n\n\tfieldName, _ := c.Args[\"field\"].(string)\n\tfield := e.Holder.Field(index, fieldName)\n\tif field == nil {\n\t\treturn PairField{}, ErrFieldNotFound\n\t}\n\n\tfragment := e.Holder.fragment(index, fieldName, viewStandard, shard)\n\tif fragment == nil {\n\t\treturn PairField{}, nil\n\t}\n\n\tidx := e.Holder.Index(index)\n\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Index: idx, Fragment: fragment, Shard: fragment.shard})\n\tif err != nil {\n\t\treturn PairField{}, err\n\t}\n\n\tdefer finisher(&err0)\n\n\tminRowID, count, err := fragment.minRow(tx, filter)\n\tif err != nil {\n\t\treturn PairField{}, err\n\t}\n\n\treturn PairField{\n\t\tPair: Pair{\n\t\t\tID:    minRowID,\n\t\t\tCount: count,\n\t\t},\n\t\tField: fieldName,\n\t}, nil\n}\n\n// executeMaxRowShard returns the maximum row ID for a shard.\nfunc (e *executor) executeMaxRowShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64) (_ PairField, err0 error) {\n\tvar filter *Row\n\tif len(c.Children) == 1 {\n\t\trow, err := e.executeBitmapCallShard(ctx, qcx, index, c.Children[0], shard)\n\t\tif err != nil {\n\t\t\treturn PairField{}, err\n\t\t}\n\t\tfilter = row\n\t}\n\n\tfieldName, _ := c.Args[\"field\"].(string)\n\tfield := e.Holder.Field(index, fieldName)\n\tif field == nil {\n\t\treturn PairField{}, ErrFieldNotFound\n\t}\n\n\tfragment := e.Holder.fragment(index, fieldName, viewStandard, shard)\n\tif fragment == nil {\n\t\treturn PairField{}, nil\n\t}\n\n\tidx := e.Holder.Index(index)\n\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Index: idx, Shard: shard})\n\tif err != nil {\n\t\treturn PairField{}, ErrQcxDone\n\t}\n\tdefer finisher(&err0)\n\n\tmaxRowID, count, err := fragment.maxRow(tx, filter)\n\tif err != nil {\n\t\treturn PairField{}, nil\n\t}\n\n\treturn PairField{\n\t\tPair: Pair{\n\t\t\tID:    maxRowID,\n\t\t\tCount: count,\n\t\t},\n\t\tField: fieldName,\n\t}, nil\n}\n\nfunc (e *executor) executeTopK(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (interface{}, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeTopK\")\n\tdefer span.Finish()\n\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\treturn e.executeTopKShard(ctx, qcx, index, c, shard)\n\t}\n\n\treduceFn := func(ctx context.Context, prev, v interface{}) interface{} {\n\t\tx, _ := prev.([]*Row)\n\t\ty, _ := v.([]*Row)\n\t\treturn ([]*Row)(AddBSI(x, y))\n\t}\n\n\tother, err := e.mapReduce(ctx, index, shards, c, opt, mapFn, reduceFn)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tresults, _ := other.([]*Row)\n\n\tif opt.Remote {\n\t\treturn results, nil\n\t}\n\n\tk, hasK, err := c.UintArg(\"k\")\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"fetching k\")\n\t}\n\n\tvar limit *uint64\n\tif hasK {\n\t\tlimit = &k\n\t}\n\n\tvar dst []Pair\n\tBSIData(results).PivotDescending(NewRow().Union(results...), 0, limit, nil, func(count uint64, ids ...uint64) {\n\t\tfor _, id := range ids {\n\t\t\tdst = append(dst, Pair{\n\t\t\t\tID:    id,\n\t\t\t\tCount: count,\n\t\t\t})\n\t\t}\n\t})\n\n\tfieldName, hasFieldName, err := c.StringArg(\"_field\")\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"fetching TopK field\")\n\t} else if !hasFieldName {\n\t\treturn nil, errors.New(\"missing field in TopK\")\n\t}\n\n\treturn &PairsField{\n\t\tPairs: dst,\n\t\tField: fieldName,\n\t}, nil\n}\n\n// executeTopKShard builds a perpendicular BSI bitmap of a shard for TopK.\nfunc (e *executor) executeTopKShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64) (_ []*Row, err0 error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeTopKShard\")\n\tdefer span.Finish()\n\n\t// Look up the index.\n\tidx := e.Holder.Index(index)\n\tif idx == nil {\n\t\treturn nil, ErrIndexNotFound\n\t}\n\n\t// Look up the field.\n\tfieldName, hasFieldName, err := c.StringArg(\"_field\")\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"fetching TopK field\")\n\t} else if !hasFieldName {\n\t\treturn nil, errors.New(\"missing field in TopK\")\n\t}\n\tf := idx.Field(fieldName)\n\tif f == nil {\n\t\treturn nil, ErrFieldNotFound\n\t}\n\n\t// Parse \"from\" time, if set.\n\tvar fromTime time.Time\n\tif v, ok := c.Args[\"from\"]; ok {\n\t\tif fromTime, err = parseTime(v); err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"parsing from time\")\n\t\t}\n\t}\n\n\t// Parse \"to\" time, if set.\n\tvar toTime time.Time\n\tif v, ok := c.Args[\"to\"]; ok {\n\t\tif toTime, err = parseTime(v); err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"parsing to time\")\n\t\t}\n\t}\n\n\t// Fetch the filter.\n\tvar filterBitmap *Row\n\tif filter, hasFilter, err := c.CallArg(\"filter\"); err != nil {\n\t\treturn nil, err\n\t} else if hasFilter {\n\t\tfilterBitmap, err = e.executeBitmapCallShard(ctx, qcx, index, filter, shard)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif !filterBitmap.Any() {\n\t\t\treturn []*Row(nil), nil\n\t\t}\n\t}\n\n\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Index: idx, Shard: shard})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer finisher(&err0)\n\n\tftype := f.Type()\n\tswitch ftype {\n\tcase FieldTypeTime:\n\t\tif !(fromTime.IsZero() && toTime.IsZero()) {\n\t\t\treturn e.executeTopKShardTime(ctx, tx, filterBitmap, index, fieldName, shard, fromTime, toTime)\n\t\t}\n\t\tfallthrough\n\tcase FieldTypeSet, FieldTypeMutex:\n\t\treturn e.executeTopKShardSet(ctx, tx, filterBitmap, index, fieldName, shard)\n\tdefault:\n\t\treturn nil, errors.Errorf(\"field type %q is not yet supported by TopK\", ftype)\n\t}\n}\n\n// executeTopKShardSet builds a perpendicular BSI bitmap of a set field within a shard.\nfunc (e *executor) executeTopKShardSet(ctx context.Context, tx Tx, filter *Row, index, field string, shard uint64) ([]*Row, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeTopKShardSet\")\n\tdefer span.Finish()\n\n\tf := e.Holder.fragment(index, field, viewStandard, shard)\n\tif f == nil {\n\t\treturn nil, nil\n\t}\n\n\treturn topKFragments(ctx, tx, filter, f)\n}\n\n// executeTopKShardTime builds a perpendicular BSI bitmap of a time field within a shard.\nfunc (e *executor) executeTopKShardTime(ctx context.Context, tx Tx, filter *Row, index, field string, shard uint64, from, to time.Time) ([]*Row, error) {\n\t// Fetch index.\n\tidx := e.Holder.Index(index)\n\tif idx == nil {\n\t\treturn nil, newNotFoundError(ErrIndexNotFound, index)\n\t}\n\n\t// Fetch field.\n\tf := idx.Field(field)\n\tif f == nil {\n\t\treturn nil, newNotFoundError(ErrFieldNotFound, field)\n\t}\n\n\tviews, err := f.viewsByTimeRange(from, to)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Fetch fragments.\n\tfragments := make([]*fragment, 0, len(views))\n\tfor _, view := range views {\n\t\tf := e.Holder.fragment(index, field, view, shard)\n\t\tif f == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tfragments = append(fragments, f)\n\t}\n\n\treturn topKFragments(ctx, tx, filter, fragments...)\n}\n\n// topKFragments builds a perpendicular BSI bitmap from fragments.\n// The fragments are expected to be from set fields.\nfunc topKFragments(ctx context.Context, tx Tx, filter *Row, fragments ...*fragment) (BSIData, error) {\n\t// Acquire fragment container iterators.\n\titers := make([]roaring.ContainerIterator, len(fragments))\n\tfor i, f := range fragments {\n\t\tf.mu.RLock()\n\t\tdefer f.mu.RUnlock()\n\n\t\titer, _, err := tx.ContainerIterator(f.index(), f.field(), f.view(), f.shard, 0)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\titers[i] = iter\n\t}\n\n\t// Merge to a single container iterator.\n\tvar it roaring.ContainerIterator\n\tif len(iters) == 1 {\n\t\tit = iters[0]\n\t} else {\n\t\tit = mergerate(iters...)\n\t}\n\n\t// Extract filter data if a filter was provided.\n\tvar filterData *topKFilter\n\tif filter != nil {\n\t\tvar f topKFilter\n\t\tf.fill(filter)\n\t\tfilterData = &f\n\t}\n\n\treturn doTopK(ctx, it, filterData)\n}\n\n// mergerate returns a container iterator that unions many container iterators.\nfunc mergerate(iters ...roaring.ContainerIterator) *mergerator {\n\titerStates := make([]mergeState, len(iters))\n\tfor i, s := range iters {\n\t\titerStates[i].iter = s\n\t}\n\tm := mergerator{\n\t\titers: iterStates,\n\t\theap:  make(mergeratorHeap, 0, len(iters)),\n\t}\n\tfor i := range iterStates {\n\t\tm.pusherate(uint64(i))\n\t}\n\treturn &m\n}\n\n// mergerator is a container iterator that merges container iterators (via unioning).\ntype mergerator struct {\n\titers     []mergeState\n\theap      mergeratorHeap\n\tcontainer *roaring.Container\n\tkey       uint64\n}\n\n// pusherate pushes the iterator at the given index back onto the heap.\nfunc (m *mergerator) pusherate(idx uint64) {\n\tstate := &m.iters[idx]\n\tit := state.iter\n\tif !it.Next() {\n\t\tit.Close()\n\t\treturn\n\t}\n\tkey, c := it.Value()\n\tstate.c = c\n\tm.heap.push(mergeNode{\n\t\tkey: key,\n\t\tidx: idx,\n\t})\n}\n\nfunc (m *mergerator) Next() bool {\n\tnodes := m.heap.pop()\n\tif len(nodes) == 0 {\n\t\treturn false\n\t}\n\tkey := nodes[0].key\n\tvar container *roaring.Container\n\tfor _, n := range nodes {\n\t\tc := m.iters[n.idx].c\n\t\tif container != nil {\n\t\t\tcontainer = roaring.Union(container, c)\n\t\t} else {\n\t\t\tcontainer = c\n\t\t}\n\t\tm.pusherate(n.idx)\n\t}\n\tm.key, m.container = key, container\n\treturn true\n}\n\nfunc (m *mergerator) Value() (uint64, *roaring.Container) {\n\treturn m.key, m.container\n}\n\nfunc (m *mergerator) Close() {\n\tfor _, n := range m.heap {\n\t\tm.iters[n.idx].iter.Close()\n\t}\n\tm.heap = nil\n}\n\ntype mergeState struct {\n\tc    *roaring.Container\n\titer roaring.ContainerIterator\n}\n\n// mergeratorHeap is a binary min-heap over keys.\n// This is used to find the next iterator to hit.\ntype mergeratorHeap []mergeNode\n\ntype mergeNode struct {\n\tkey, idx uint64\n}\n\n// push a node onto the heap.\nfunc (h *mergeratorHeap) push(node mergeNode) {\n\ts := *h\n\ti := len(s)\n\ts = append(s, node)\n\tfor i != 0 && s[(i-1)/2].key > s[i].key {\n\t\ts[(i-1)/2], s[i] = s[i], s[(i-1)/2]\n\t\ti = (i - 1) / 2\n\t}\n\t*h = s\n}\n\n// pop the minimum key off of the heap.\n// If there are multiple iterators with this keys, this returns all of them.\nfunc (h *mergeratorHeap) pop() []mergeNode {\n\ts := *h\n\tif len(s) == 0 {\n\t\treturn nil\n\t}\n\n\tn := 0\n\tfor key := s[0].key; len(s) > n && s[0].key == key; n++ {\n\t\ts[0], s[len(s)-n-1] = s[len(s)-n-1], s[0]\n\t\ts[:len(s)-n-1].minHeapify()\n\t}\n\n\t*h = s[:len(s)-n]\n\treturn s[len(s)-n:]\n}\n\n// minHeapify fixes the heap invariant after updating the heap's root.\nfunc (h mergeratorHeap) minHeapify() {\n\ti := 0\n\tfor {\n\t\tl, r := 2*i+1, 2*i+2\n\t\tmin := i\n\t\tif l < len(h) && h[l].key < h[min].key {\n\t\t\tmin = l\n\t\t}\n\t\tif r < len(h) && h[r].key < h[min].key {\n\t\t\tmin = r\n\t\t}\n\t\tif min == i {\n\t\t\treturn\n\t\t}\n\t\th[min], h[i] = h[i], h[min]\n\t\ti = min\n\t}\n}\n\n// doTopK uses a raw Pilosa matrix to produce a perpendicular BSI bitmap.\n// It will apply a row filter if one is provided.\nfunc doTopK(ctx context.Context, it roaring.ContainerIterator, filter *topKFilter) (BSIData, error) {\n\trow := ^uint64(0)\n\tvar count uint64\n\n\tvar builder bsiBuilder\n\tvar i uint16\n\tfor it.Next() {\n\t\tif i == 0 {\n\t\t\tif err := ctx.Err(); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t\ti++\n\n\t\t// Fetch the next container.\n\t\tkey, container := it.Value()\n\t\tkeyrow, subkey := key/(ShardWidth>>16), key%(ShardWidth>>16)\n\t\tif keyrow != row {\n\t\t\t// The previous row has ended.\n\t\t\t// Flush the count to the BSI data.\n\t\t\tbuilder.Insert(row, count)\n\t\t\trow, count = keyrow, 0\n\t\t}\n\n\t\t// Add the selected bits to the count.\n\t\tif filter != nil {\n\t\t\tfc := filter[subkey]\n\t\t\tif fc == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcount += uint64(roaring.IntersectionCount(container, fc))\n\t\t} else {\n\t\t\tcount += uint64(container.N())\n\t\t}\n\t}\n\n\t// Add the final count to the BSI data.\n\tbuilder.Insert(row, count)\n\n\t// Construct the result.\n\treturn builder.Build(), nil\n}\n\n// topKFilter is a row filter for a TopK query.\n// It is represented as a contiguous array of containers.\ntype topKFilter [ShardWidth >> 16]*roaring.Container\n\n// fill the filter with the contents of a Row.\nfunc (f *topKFilter) fill(row *Row) {\n\tfor _, s := range row.Segments {\n\t\tit, _ := s.data.Containers.Iterator(0)\n\t\tf.fillIt(it)\n\t}\n\t// I don't think multiple segments make sense here?\n}\n\nfunc (f *topKFilter) fillIt(it roaring.ContainerIterator) {\n\tdefer it.Close()\n\n\tfor it.Next() {\n\t\tkey, c := it.Value()\n\n\t\tkey %= uint64(len(f))\n\n\t\tif f[key] != nil {\n\t\t\tpanic(\"duplicate container in topk filter\")\n\t\t}\n\t\tf[key] = c\n\t}\n}\n\n// executeTopN executes a TopN() call.\n// This first performs the TopN() to determine the top results and then\n// requeries to retrieve the full counts for each of the top results.\nfunc (e *executor) executeTopN(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (*PairsField, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeTopN\")\n\tdefer span.Finish()\n\n\tidsArg, _, err := c.UintSliceArg(\"ids\")\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"executeTopN: %v\", err)\n\t}\n\n\tfieldName, _ := c.Args[\"_field\"].(string)\n\tn, _, err := c.UintArg(\"n\")\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"executeTopN: %v\", err)\n\t}\n\n\t// Execute original query.\n\tpairs, err := e.executeTopNShards(ctx, qcx, index, c, shards, opt)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"finding top results\")\n\t}\n\n\t// If this call is against specific ids, or we didn't get results,\n\t// or we are part of a larger distributed query then don't refetch.\n\tif len(pairs.Pairs) == 0 || len(idsArg) > 0 || opt.Remote {\n\t\treturn &PairsField{\n\t\t\tPairs: pairs.Pairs,\n\t\t\tField: fieldName,\n\t\t}, nil\n\t}\n\t// Only the original caller should refetch the full counts.\n\t// TODO(@kuba--): ...but do we really need `Clone` here?\n\tother := c.Clone()\n\n\tids := Pairs(pairs.Pairs).Keys()\n\tsort.Sort(uint64Slice(ids))\n\tother.Args[\"ids\"] = ids\n\n\ttrimmedList, err := e.executeTopNShards(ctx, qcx, index, other, shards, opt)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"retrieving full counts\")\n\t}\n\n\tif n != 0 && int(n) < len(trimmedList.Pairs) {\n\t\ttrimmedList.Pairs = trimmedList.Pairs[0:n]\n\t}\n\n\treturn &PairsField{\n\t\tPairs: trimmedList.Pairs,\n\t\tField: fieldName,\n\t}, nil\n}\n\nfunc (e *executor) executeTopNShards(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (*PairsField, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeTopNShards\")\n\tdefer span.Finish()\n\n\t// Execute calls in bulk on each remote node and merge.\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\treturn e.executeTopNShard(ctx, qcx, index, c, shard)\n\t}\n\n\t// Merge returned results at coordinating node.\n\treduceFn := func(ctx context.Context, prev, v interface{}) interface{} {\n\t\tother, _ := prev.(*PairsField)\n\t\tvpf, _ := v.(*PairsField)\n\t\tif other == nil {\n\t\t\treturn vpf\n\t\t} else if vpf == nil {\n\t\t\treturn other\n\t\t}\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tother.Pairs = Pairs(other.Pairs).Add(vpf.Pairs)\n\t\treturn other\n\t}\n\n\tother, err := e.mapReduce(ctx, index, shards, c, opt, mapFn, reduceFn)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tresults, _ := other.(*PairsField)\n\n\t// Sort final merged results.\n\tsort.Sort(Pairs(results.Pairs))\n\n\treturn results, nil\n}\n\n// executeTopNShard executes a TopN call for a single shard.\nfunc (e *executor) executeTopNShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64) (_ *PairsField, err0 error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeTopNShard\")\n\tdefer span.Finish()\n\n\tfieldName, _ := c.Args[\"_field\"].(string)\n\tn, _, err := c.UintArg(\"n\")\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"executeTopNShard: %v\", err)\n\t} else if f := e.Holder.Field(index, fieldName); f != nil && (f.Type() == FieldTypeInt || f.Type() == FieldTypeDecimal || f.Type() == FieldTypeTimestamp) {\n\t\treturn nil, fmt.Errorf(\"cannot compute TopN() on integer, decimal, or timestamp field: %q\", fieldName)\n\t}\n\n\trowIDs, _, err := c.UintSliceArg(\"ids\")\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"executeTopNShard: %v\", err)\n\t}\n\tminThreshold, _, err := c.UintArg(\"threshold\")\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"executeTopNShard: %v\", err)\n\t}\n\ttanimotoThreshold, _, err := c.UintArg(\"tanimotoThreshold\")\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"executeTopNShard: %v\", err)\n\t}\n\n\t// Retrieve bitmap used to intersect.\n\tvar src *Row\n\tif len(c.Children) == 1 {\n\t\trow, err := e.executeBitmapCallShard(ctx, qcx, index, c.Children[0], shard)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tsrc = row\n\t} else if len(c.Children) > 1 {\n\t\treturn nil, errors.New(\"TopN() can only have one input bitmap\")\n\t}\n\n\t// Set default field.\n\tif fieldName == \"\" {\n\t\tfieldName = defaultField\n\t}\n\n\tf := e.Holder.fragment(index, fieldName, viewStandard, shard)\n\tif f == nil {\n\t\treturn &PairsField{}, nil\n\t} else if f.CacheType == CacheTypeNone {\n\t\treturn nil, fmt.Errorf(\"cannot compute TopN(), field has no cache: %q\", fieldName)\n\t}\n\n\tif minThreshold == 0 {\n\t\tminThreshold = defaultMinThreshold\n\t}\n\n\tif tanimotoThreshold > 100 {\n\t\treturn nil, errors.New(\"Tanimoto Threshold is from 1 to 100 only\")\n\t}\n\n\tidx := e.Holder.Index(index)\n\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Fragment: f, Index: idx, Shard: shard})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer finisher(&err0)\n\n\tpairs, err := f.top(tx, topOptions{\n\t\tN:                 int(n),\n\t\tSrc:               src,\n\t\tRowIDs:            rowIDs,\n\t\tMinThreshold:      minThreshold,\n\t\tTanimotoThreshold: tanimotoThreshold,\n\t})\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"getting top\")\n\t}\n\n\treturn &PairsField{\n\t\tPairs: pairs,\n\t}, nil\n}\n\n// executeDifferenceShard executes a difference() call for a local shard.\nfunc (e *executor) executeDifferenceShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64) (_ *Row, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeDifferenceShard\")\n\tdefer span.Finish()\n\n\tvar other *Row\n\tif len(c.Children) == 0 {\n\t\treturn nil, fmt.Errorf(\"empty Difference query is currently not supported\")\n\t}\n\tfor i, input := range c.Children {\n\t\trow, err := e.executeBitmapCallShard(ctx, qcx, index, input, shard)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif i == 0 {\n\t\t\tother = row\n\t\t} else {\n\t\t\tother = other.Difference(row)\n\t\t}\n\t}\n\tother.invalidateCount()\n\treturn other, nil\n}\n\n// RowIdentifiers is a return type for a list of\n// row ids or row keys. The names `Rows` and `Keys`\n// are meant to follow the same convention as the\n// Row query which returns `Columns` and `Keys`.\n// TODO: Rename this to something better. Anything.\ntype RowIdentifiers struct {\n\tRows  []uint64 `json:\"rows\"`\n\tKeys  []string `json:\"keys,omitempty\"`\n\tField string\n}\n\nfunc (r *RowIdentifiers) Clone() (clone *RowIdentifiers) {\n\tclone = &RowIdentifiers{\n\t\tField: r.Field,\n\t}\n\tif r.Rows != nil {\n\t\tclone.Rows = make([]uint64, len(r.Rows))\n\t\tcopy(clone.Rows, r.Rows)\n\t}\n\tif r.Keys != nil {\n\t\tclone.Keys = make([]string, len(r.Keys))\n\t\tcopy(clone.Keys, r.Keys)\n\t}\n\treturn\n}\n\n// ToTable implements the ToTabler interface.\nfunc (r RowIdentifiers) ToTable() (*proto.TableResponse, error) {\n\tvar n int\n\tif len(r.Keys) > 0 {\n\t\tn = len(r.Keys)\n\t} else {\n\t\tn = len(r.Rows)\n\t}\n\treturn proto.RowsToTable(&r, n)\n}\n\n// ToRows implements the ToRowser interface.\nfunc (r RowIdentifiers) ToRows(callback func(*proto.RowResponse) error) error {\n\tif len(r.Keys) > 0 {\n\t\tci := []*proto.ColumnInfo{{Name: r.Field, Datatype: \"string\"}}\n\t\tfor _, key := range r.Keys {\n\t\t\tif err := callback(&proto.RowResponse{\n\t\t\t\tHeaders: ci,\n\t\t\t\tColumns: []*proto.ColumnResponse{\n\t\t\t\t\t{ColumnVal: &proto.ColumnResponse_StringVal{StringVal: key}},\n\t\t\t\t},\n\t\t\t}); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"calling callback\")\n\t\t\t}\n\t\t\tci = nil\n\t\t}\n\t} else {\n\t\tci := []*proto.ColumnInfo{{Name: r.Field, Datatype: \"uint64\"}}\n\t\tfor _, id := range r.Rows {\n\t\t\tif err := callback(&proto.RowResponse{\n\t\t\t\tHeaders: ci,\n\t\t\t\tColumns: []*proto.ColumnResponse{\n\t\t\t\t\t{ColumnVal: &proto.ColumnResponse_Uint64Val{Uint64Val: uint64(id)}},\n\t\t\t\t},\n\t\t\t}); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"calling callback\")\n\t\t\t}\n\t\t\tci = nil\n\t\t}\n\t}\n\treturn nil\n}\n\n// RowIDs is a query return type for just uint64 row ids.\n// It should only be used internally (since RowIdentifiers\n// is the external return type), but it is exported because\n// the proto package needs access to it.\ntype RowIDs []uint64\n\nfunc (r RowIDs) Merge(other RowIDs, limit int) RowIDs {\n\ti, j := 0, 0\n\tresult := make(RowIDs, 0)\n\tfor i < len(r) && j < len(other) && len(result) < limit {\n\t\tav, bv := r[i], other[j]\n\t\tif av < bv {\n\t\t\tresult = append(result, av)\n\t\t\ti++\n\t\t} else if av > bv {\n\t\t\tresult = append(result, bv)\n\t\t\tj++\n\t\t} else {\n\t\t\tresult = append(result, bv)\n\t\t\ti++\n\t\t\tj++\n\t\t}\n\t}\n\tfor i < len(r) && len(result) < limit {\n\t\tresult = append(result, r[i])\n\t\ti++\n\t}\n\tfor j < len(other) && len(result) < limit {\n\t\tresult = append(result, other[j])\n\t\tj++\n\t}\n\treturn result\n}\n\n// order denotes sort ordercan be asc or desc (see constants below).\ntype order bool\n\nconst (\n\tasc  order = true\n\tdesc order = false\n)\n\n// groupCountSorter sorts the output of a GroupBy request (a\n// []GroupCount) according to sorting instructions encoded in \"fields\"\n// and \"order\".\n//\n// Each field in \"fields\" is an integer which can be -1 to denote\n// sorting on the Count and -2 to denote sorting on the\n// sum/aggregate. Currently nothing else is supported, but the idea\n// was that if there were positive integers they would be indexes into\n// GroupCount.FieldRow and allowing sorting on the values of different\n// fields in the group. Each item in \"order\" corresponds to the same\n// index in \"fields\" and denotes the order of the sort.\ntype groupCountSorter struct {\n\tfields []int\n\torder  []order\n\tdata   []GroupCount\n}\n\nfunc (g *groupCountSorter) Len() int      { return len(g.data) }\nfunc (g *groupCountSorter) Swap(i, j int) { g.data[i], g.data[j] = g.data[j], g.data[i] }\nfunc (g *groupCountSorter) Less(i, j int) bool {\n\tgci, gcj := g.data[i], g.data[j]\n\tfor idx, fieldIndex := range g.fields {\n\t\tfieldOrder := g.order[idx]\n\t\tswitch fieldIndex {\n\t\tcase -1: // Count\n\t\t\tif gci.Count < gcj.Count {\n\t\t\t\treturn fieldOrder == asc\n\t\t\t} else if gci.Count > gcj.Count {\n\t\t\t\treturn fieldOrder == desc\n\t\t\t}\n\t\tcase -2: // Aggregate\n\t\t\tif gci.Agg < gcj.Agg {\n\t\t\t\treturn fieldOrder == asc\n\t\t\t} else if gci.Agg > gcj.Agg {\n\t\t\t\treturn fieldOrder == desc\n\t\t\t}\n\t\tdefault:\n\t\t\tpanic(\"impossible\")\n\t\t}\n\t}\n\treturn false\n}\n\n// getSorter hackily parses the sortSpec and figures out how to sort\n// the GroupBy results.\nfunc getSorter(sortSpec string) (*groupCountSorter, error) {\n\tgcs := &groupCountSorter{\n\t\tfields: []int{},\n\t\torder:  []order{},\n\t}\n\tsortOn := strings.Split(sortSpec, \",\")\n\tfor _, sortField := range sortOn {\n\t\tsortField = strings.TrimSpace(sortField)\n\t\tfieldDir := strings.Fields(sortField)\n\t\tif len(fieldDir) == 0 {\n\t\t\treturn nil, errors.Errorf(\"invalid sorting directive: '%s'\", sortField)\n\t\t} else if fieldDir[0] == \"count\" {\n\t\t\tgcs.fields = append(gcs.fields, -1)\n\t\t} else if fieldDir[0] == \"aggregate\" || fieldDir[0] == \"sum\" {\n\t\t\tgcs.fields = append(gcs.fields, -2)\n\t\t} else {\n\t\t\treturn nil, errors.Errorf(\"sorting is only supported on count, aggregate, or sum, not '%s'\", fieldDir[0])\n\t\t}\n\n\t\tif len(fieldDir) == 1 {\n\t\t\tgcs.order = append(gcs.order, desc)\n\t\t} else if len(fieldDir) > 2 {\n\t\t\treturn nil, errors.Errorf(\"parsing sort directive: '%s': too many elements\", sortField)\n\t\t} else if fieldDir[1] == \"asc\" {\n\t\t\tgcs.order = append(gcs.order, asc)\n\t\t} else if fieldDir[1] == \"desc\" {\n\t\t\tgcs.order = append(gcs.order, desc)\n\t\t} else {\n\t\t\treturn nil, errors.Errorf(\"unknown sort direction '%s'\", fieldDir[1])\n\t\t}\n\t}\n\treturn gcs, nil\n}\n\n// findGroupCounts gets a safe-to-use but possibly empty []GroupCount from\n// an interface which might be a *GroupCounts or a []GroupCount.\nfunc findGroupCounts(v interface{}) []GroupCount {\n\tswitch gc := v.(type) {\n\tcase []GroupCount:\n\t\treturn gc\n\tcase *GroupCounts:\n\t\treturn gc.Groups()\n\t}\n\treturn nil\n}\n\nfunc (e *executor) executeGroupBy(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (*GroupCounts, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeGroupBy\")\n\tdefer span.Finish()\n\t// validate call\n\tif len(c.Children) == 0 {\n\t\treturn nil, errors.New(\"need at least one child call\")\n\t}\n\tlimit := int(^uint(0) >> 1)\n\tif lim, hasLimit, err := c.UintArg(\"limit\"); err != nil {\n\t\treturn nil, err\n\t} else if hasLimit {\n\t\tlimit = int(lim)\n\t}\n\tfilter, _, err := c.CallArg(\"filter\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar sorter *groupCountSorter\n\tif sortSpec, found, err := c.StringArg(\"sort\"); err != nil {\n\t\treturn nil, errors.Wrap(err, \"getting sort arg\")\n\t} else if found {\n\t\tsorter, err = getSorter(sortSpec)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"parsing sort spec\")\n\t\t}\n\t\t// don't want to prematurely limit the results if we're sorting\n\t\tlimit = int(^uint(0) >> 1)\n\t}\n\thaving, hasHaving, err := c.CallArg(\"having\")\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"getting 'having' argument\")\n\t} else if hasHaving {\n\t\t// don't want to prematurely limit the results if we're filtering some out\n\t\tlimit = int(^uint(0) >> 1)\n\t}\n\n\tidx := e.Holder.Index(index)\n\tif idx == nil {\n\t\treturn nil, newNotFoundError(ErrIndexNotFound, index)\n\t}\n\n\t// perform necessary Rows queries (any that have limit or columns args) -\n\t// TODO, call async? would only help if multiple Rows queries had a column\n\t// or limit arg.\n\t// TODO support TopN in here would be really cool - and pretty easy I think.\n\tbases := make(map[int]int64)\n\tchildRows := make([]RowIDs, len(c.Children))\n\tfor i, child := range c.Children {\n\t\t// Check \"field\" first for backwards compatibility, then set _field.\n\t\t// TODO: remove at Pilosa 2.0\n\t\tif fieldName, ok := child.Args[\"field\"].(string); ok {\n\t\t\tchild.Args[\"_field\"] = fieldName\n\t\t}\n\n\t\tif child.Name != \"Rows\" {\n\t\t\treturn nil, errors.Errorf(\"'%s' is not a valid child query for GroupBy, must be 'Rows'\", child.Name)\n\t\t}\n\t\t_, hasLimit, err := child.UintArg(\"limit\")\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"getting limit\")\n\t\t}\n\t\t_, hasCol, err := child.UintArg(\"column\")\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"getting column\")\n\t\t}\n\t\t_, hasLike, err := child.StringArg(\"like\")\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"getting like\")\n\t\t}\n\t\t_, hasIn, err := child.UintSliceArg(\"in\")\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"getting 'in'\")\n\t\t}\n\t\tfieldName, ok := child.Args[\"_field\"].(string)\n\t\tif !ok {\n\t\t\treturn nil, errors.Errorf(\"%s call must have field with valid (string) field name. Got %v of type %[2]T\", child.Name, child.Args[\"_field\"])\n\t\t}\n\t\tf := idx.Field(fieldName)\n\t\tif f == nil {\n\t\t\treturn nil, newNotFoundError(ErrFieldNotFound, fieldName)\n\t\t}\n\t\tswitch f.Type() {\n\t\tcase FieldTypeInt, FieldTypeTimestamp:\n\t\t\tbases[i] = f.bsiGroup(f.name).Base\n\t\t}\n\n\t\tif hasLimit || hasCol || hasLike || hasIn { // we need to perform this query cluster-wide ahead of executeGroupByShard\n\t\t\tif idx, ok := child.Args[\"valueidx\"].(int64); ok {\n\t\t\t\t// The rows query was already completed on the initiating node.\n\t\t\t\tchildRows[i] = opt.EmbeddedData[idx].Columns()\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tr, er := e.executeRows(ctx, qcx, index, child, shards, opt)\n\t\t\tif er != nil {\n\t\t\t\treturn nil, errors.Wrap(er, \"getting rows for \")\n\t\t\t}\n\t\t\t// need to sort because filters assume ordering\n\t\t\tsort.Slice(r, func(x, y int) bool { return r[x] < r[y] })\n\t\t\tchildRows[i] = r\n\t\t\tif len(childRows[i]) == 0 { // there are no results because this field has no values.\n\t\t\t\treturn &GroupCounts{}, nil\n\t\t\t}\n\n\t\t\t// Stuff the result into opt.EmbeddedData so that it gets sent to other nodes in the map-reduce.\n\t\t\t// This is flagged as \"NoSplit\" to ensure that the entire row gets sent out.\n\t\t\trowsRow := NewRow(childRows[i]...)\n\t\t\trowsRow.NoSplit = true\n\t\t\tchild.Args[\"valueidx\"] = int64(len(opt.EmbeddedData))\n\t\t\topt.EmbeddedData = append(opt.EmbeddedData, rowsRow)\n\t\t}\n\t}\n\n\tignoreLimit := sorter != nil\n\t// Execute calls in bulk on each remote node and merge.\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\treturn e.executeGroupByShard(ctx, qcx, index, c, filter, shard, childRows, bases, ignoreLimit)\n\t}\n\n\t// Merge returned results at coordinating node.\n\treduceFn := func(ctx context.Context, prev, v interface{}) interface{} {\n\t\tother := findGroupCounts(prev)\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tx := mergeGroupCounts(other, findGroupCounts(v), limit)\n\t\tfor i := range x {\n\t\t\tgc := &x[i]\n\t\t\tfor j := range gc.Group {\n\t\t\t\tfr := &gc.Group[j]\n\t\t\t\tif fr.FieldOptions == nil {\n\t\t\t\t\t// oops, options were omitted possibly by a remote. try to\n\t\t\t\t\t// guess them from our local options\n\t\t\t\t\tfield := e.Holder.Field(index, fr.Field)\n\t\t\t\t\tif field != nil {\n\t\t\t\t\t\toptions := field.Options()\n\t\t\t\t\t\tfr.FieldOptions = &options\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn x\n\t}\n\t// Get full result set.\n\tother, err := e.mapReduce(ctx, index, shards, c, opt, mapFn, reduceFn)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"mapReduce\")\n\t}\n\tresults, _ := other.([]GroupCount)\n\n\t// If there's no sorting, we want to apply limits before\n\t// calculating the Distinct aggregate which is expensive on a\n\t// per-result basis.\n\tif sorter == nil && !hasHaving {\n\t\tresults, err = applyLimitAndOffsetToGroupByResult(c, results)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"applying limit/offset\")\n\t\t}\n\t}\n\n\t// TODO as an optimization, we could apply some \"having\"\n\t// conditions here long as they aren't on the Count(Distinct)\n\t// aggregate\n\t// Calculate Count(Distinct) aggregate if requested.\n\taggregate, _, err := c.CallArg(\"aggregate\")\n\tif err == nil && aggregate != nil && aggregate.Name == \"Count\" && len(aggregate.Children) > 0 && aggregate.Children[0].Name == \"Distinct\" && !opt.Remote {\n\t\tfor n, gc := range results {\n\t\t\tintersectRows := make([]*pql.Call, 0, len(gc.Group))\n\t\t\tfor _, fr := range gc.Group {\n\t\t\t\tvar value interface{} = fr.RowID\n\t\t\t\t// use fr.Value instead of fr.RowID if set (from int fields)\n\t\t\t\tif fr.Value != nil {\n\t\t\t\t\tvalue = &pql.Condition{Op: pql.EQ, Value: *fr.Value}\n\t\t\t\t}\n\t\t\t\tintersectRows = append(intersectRows, &pql.Call{Name: \"Row\", Args: map[string]interface{}{fr.Field: value}})\n\t\t\t}\n\t\t\t// apply any filter, if present\n\t\t\tif filter != nil {\n\t\t\t\tintersectRows = append(intersectRows, filter)\n\t\t\t}\n\t\t\t// also intersect with any children of Distinct\n\t\t\tif len(aggregate.Children[0].Children) > 0 {\n\t\t\t\tintersectRows = append(intersectRows, aggregate.Children[0].Children[0])\n\t\t\t}\n\n\t\t\tcountDistinctIntersect := &pql.Call{\n\t\t\t\tName: \"Count\",\n\t\t\t\tChildren: []*pql.Call{\n\t\t\t\t\t{\n\t\t\t\t\t\tName: \"Distinct\",\n\t\t\t\t\t\tChildren: []*pql.Call{\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tName:     \"Intersect\",\n\t\t\t\t\t\t\t\tChildren: intersectRows,\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t\tArgs: aggregate.Children[0].Args,\n\t\t\t\t\t\tType: pql.PrecallGlobal,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t}\n\n\t\t\topt.PreTranslated = true\n\t\t\taggregateCount, err := e.execute(ctx, qcx, index, &pql.Query{Calls: []*pql.Call{countDistinctIntersect}}, []uint64{}, opt)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tresults[n].Agg = int64(aggregateCount[0].(uint64))\n\t\t}\n\t}\n\n\t// Apply having.\n\tif hasHaving && !opt.Remote {\n\t\t// parse the condition as PQL\n\t\tif having.Name != \"Condition\" {\n\t\t\treturn nil, errors.New(\"the only supported having call is Condition()\")\n\t\t}\n\t\tif len(having.Args) != 1 {\n\t\t\treturn nil, errors.New(\"Condition() must contain a single condition\")\n\t\t}\n\t\tfor subj, cond := range having.Args {\n\t\t\tswitch subj {\n\t\t\tcase \"count\", \"sum\":\n\t\t\t\tresults = ApplyConditionToGroupCounts(results, subj, cond.(*pql.Condition))\n\t\t\tdefault:\n\t\t\t\treturn nil, errors.New(\"Condition() only supports count or sum\")\n\t\t\t}\n\t\t}\n\t}\n\n\tif sorter != nil && !opt.Remote {\n\t\tsorter.data = results\n\t\tsort.Stable(sorter)\n\t\tresults, err = applyLimitAndOffsetToGroupByResult(c, results)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"applying limit/offset\")\n\t\t}\n\t} else if hasHaving && !opt.Remote {\n\t\tresults, err = applyLimitAndOffsetToGroupByResult(c, results)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"applying limit/offset\")\n\t\t}\n\n\t}\n\n\taggType := \"\"\n\tif aggregate != nil {\n\t\tswitch aggregate.Name {\n\t\tcase \"Sum\":\n\t\t\taggType = \"sum\"\n\t\tcase \"Count\":\n\t\t\taggType = \"aggregate\"\n\t\t}\n\t}\n\tfor _, res := range results {\n\t\tif res.DecimalAgg != nil && aggType == \"sum\" {\n\t\t\taggType = \"decimalSum\"\n\t\t\tbreak\n\t\t}\n\t}\n\n\treturn NewGroupCounts(aggType, results...), nil\n}\n\nfunc applyLimitAndOffsetToGroupByResult(c *pql.Call, results []GroupCount) ([]GroupCount, error) {\n\t// Apply offset.\n\tif offset, hasOffset, err := c.UintArg(\"offset\"); err != nil {\n\t\treturn nil, err\n\t} else if hasOffset {\n\t\tif int(offset) < len(results) {\n\t\t\tresults = results[offset:]\n\t\t}\n\t}\n\t// Apply limit.\n\tif limit, hasLimit, err := c.UintArg(\"limit\"); err != nil {\n\t\treturn nil, err\n\t} else if hasLimit {\n\t\tif int(limit) < len(results) {\n\t\t\tresults = results[:limit]\n\t\t}\n\t}\n\treturn results, nil\n}\n\n// FieldRow is used to distinguish rows in a group by result.\ntype FieldRow struct {\n\tField        string        `json:\"field\"`\n\tRowID        uint64        `json:\"rowID\"`\n\tRowKey       string        `json:\"rowKey,omitempty\"`\n\tValue        *int64        `json:\"value,omitempty\"`\n\tFieldOptions *FieldOptions `json:\"-\"`\n}\n\nfunc (fr *FieldRow) Clone() (clone *FieldRow) {\n\tclone = &FieldRow{\n\t\tField:  fr.Field,\n\t\tRowID:  fr.RowID,\n\t\tRowKey: fr.RowKey,\n\t}\n\tif fr.Value != nil {\n\t\t// deep copy, for safety.\n\t\tv := *fr.Value\n\t\tclone.Value = &v\n\t}\n\tif fr.FieldOptions != nil {\n\t\t// deep copy, for Extra Safety\n\t\tv := *fr.FieldOptions\n\t\tclone.FieldOptions = &v\n\t}\n\treturn\n}\n\n// MarshalJSON marshals FieldRow to JSON such that\n// either a Key or an ID is included.\nfunc (fr FieldRow) MarshalJSON() ([]byte, error) {\n\tif fr.Value != nil {\n\t\tif fr.FieldOptions.Type == FieldTypeTimestamp {\n\t\t\tts, err := ValToTimestamp(fr.FieldOptions.TimeUnit, int64(*fr.Value)+fr.FieldOptions.Base)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"translating value to timestamp\")\n\t\t\t}\n\t\t\treturn json.Marshal(struct {\n\t\t\t\tField string `json:\"field\"`\n\t\t\t\tValue string `json:\"value\"`\n\t\t\t}{\n\t\t\t\tField: fr.Field,\n\t\t\t\tValue: ts.Format(time.RFC3339Nano),\n\t\t\t})\n\t\t} else {\n\t\t\treturn json.Marshal(struct {\n\t\t\t\tField string `json:\"field\"`\n\t\t\t\tValue int64  `json:\"value\"`\n\t\t\t}{\n\t\t\t\tField: fr.Field,\n\t\t\t\tValue: *fr.Value,\n\t\t\t})\n\t\t}\n\t}\n\n\tif fr.RowKey != \"\" {\n\t\treturn json.Marshal(struct {\n\t\t\tField  string `json:\"field\"`\n\t\t\tRowKey string `json:\"rowKey\"`\n\t\t}{\n\t\t\tField:  fr.Field,\n\t\t\tRowKey: fr.RowKey,\n\t\t})\n\t}\n\n\treturn json.Marshal(struct {\n\t\tField string `json:\"field\"`\n\t\tRowID uint64 `json:\"rowID\"`\n\t}{\n\t\tField: fr.Field,\n\t\tRowID: fr.RowID,\n\t})\n}\n\n// String is the FieldRow stringer.\nfunc (fr FieldRow) String() string {\n\tif fr.Value != nil {\n\t\treturn fmt.Sprintf(\"%s.%d.%d.%s\", fr.Field, fr.RowID, *fr.Value, fr.RowKey)\n\t}\n\treturn fmt.Sprintf(\"%s.%d.%s\", fr.Field, fr.RowID, fr.RowKey)\n}\n\ntype aggregateType int\n\nconst (\n\tnilAggregate        aggregateType = 0\n\tsumAggregate        aggregateType = 1\n\tdistinctAggregate   aggregateType = 2\n\tdecimalSumAggregate aggregateType = 3\n)\n\n// GroupCounts is a list of GroupCount.\ntype GroupCounts struct {\n\tgroups        []GroupCount\n\taggregateType aggregateType\n}\n\n// AggregateColumn gives the likely column name to use for aggregates, because\n// for historical reasons we used \"sum\" when it was a sum, but don't want to\n// use that when it's something else. This will likely get revisited.\nfunc (g *GroupCounts) AggregateColumn() string {\n\tswitch g.aggregateType {\n\tcase sumAggregate:\n\t\treturn \"sum\"\n\tcase distinctAggregate:\n\t\treturn \"aggregate\"\n\tcase decimalSumAggregate:\n\t\treturn \"decimalSum\"\n\tdefault:\n\t\treturn \"\"\n\t}\n}\n\n// Groups is a convenience method to let us not worry as much about the\n// potentially-nil nature of a *GroupCounts.\nfunc (g *GroupCounts) Groups() []GroupCount {\n\tif g == nil {\n\t\treturn nil\n\t}\n\treturn g.groups\n}\n\n// NewGroupCounts creates a GroupCounts with the given type and slice\n// of GroupCount objects. There's intentionally no externally-accessible way\n// to change the []GroupCount after creation.\nfunc NewGroupCounts(agg string, groups ...GroupCount) *GroupCounts {\n\tvar aggType aggregateType\n\tswitch agg {\n\tcase \"sum\":\n\t\taggType = sumAggregate\n\tcase \"aggregate\":\n\t\taggType = distinctAggregate\n\tcase \"decimalSum\":\n\t\taggType = decimalSumAggregate\n\tcase \"\":\n\t\taggType = nilAggregate\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"invalid aggregate type %q\", agg))\n\t}\n\treturn &GroupCounts{aggregateType: aggType, groups: groups}\n}\n\n// ToTable implements the ToTabler interface.\nfunc (g *GroupCounts) ToTable() (*proto.TableResponse, error) {\n\treturn proto.RowsToTable(g, len(g.Groups()))\n}\n\n// ToRows implements the ToRowser interface.\nfunc (g *GroupCounts) ToRows(callback func(*proto.RowResponse) error) error {\n\tagg := g.AggregateColumn()\n\tfor i, gc := range g.Groups() {\n\t\tvar ci []*proto.ColumnInfo\n\t\tif i == 0 {\n\t\t\tfor _, fieldRow := range gc.Group {\n\t\t\t\tif fieldRow.RowKey != \"\" {\n\t\t\t\t\tci = append(ci, &proto.ColumnInfo{Name: fieldRow.Field, Datatype: \"string\"})\n\t\t\t\t} else if fieldRow.Value != nil {\n\t\t\t\t\tci = append(ci, &proto.ColumnInfo{Name: fieldRow.Field, Datatype: \"int64\"})\n\t\t\t\t} else {\n\t\t\t\t\tci = append(ci, &proto.ColumnInfo{Name: fieldRow.Field, Datatype: \"uint64\"})\n\t\t\t\t}\n\t\t\t}\n\t\t\tci = append(ci, &proto.ColumnInfo{Name: \"count\", Datatype: \"uint64\"})\n\t\t\tif agg != \"\" {\n\t\t\t\tci = append(ci, &proto.ColumnInfo{Name: agg, Datatype: \"int64\"})\n\t\t\t}\n\n\t\t}\n\t\trowResp := &proto.RowResponse{\n\t\t\tHeaders: ci,\n\t\t\tColumns: []*proto.ColumnResponse{},\n\t\t}\n\n\t\tfor _, fieldRow := range gc.Group {\n\t\t\tif fieldRow.RowKey != \"\" {\n\t\t\t\trowResp.Columns = append(rowResp.Columns, &proto.ColumnResponse{ColumnVal: &proto.ColumnResponse_StringVal{StringVal: fieldRow.RowKey}})\n\t\t\t} else if fieldRow.Value != nil {\n\t\t\t\trowResp.Columns = append(rowResp.Columns, &proto.ColumnResponse{ColumnVal: &proto.ColumnResponse_Int64Val{Int64Val: *fieldRow.Value}})\n\t\t\t} else {\n\t\t\t\trowResp.Columns = append(rowResp.Columns, &proto.ColumnResponse{ColumnVal: &proto.ColumnResponse_Uint64Val{Uint64Val: fieldRow.RowID}})\n\t\t\t}\n\t\t}\n\t\trowResp.Columns = append(rowResp.Columns,\n\t\t\t&proto.ColumnResponse{ColumnVal: &proto.ColumnResponse_Uint64Val{Uint64Val: gc.Count}})\n\t\tif agg != \"\" {\n\t\t\trowResp.Columns = append(rowResp.Columns,\n\t\t\t\t&proto.ColumnResponse{ColumnVal: &proto.ColumnResponse_Int64Val{Int64Val: gc.Agg}})\n\t\t}\n\t\tif err := callback(rowResp); err != nil {\n\t\t\treturn errors.Wrap(err, \"calling callback\")\n\t\t}\n\t}\n\treturn nil\n}\n\n// MarshalJSON makes GroupCounts satisfy interface json.Marshaler and\n// customizes the JSON output of the aggregate field label.\nfunc (g *GroupCounts) MarshalJSON() ([]byte, error) {\n\tgroups := g.Groups()\n\tvar counts interface{} = groups\n\n\tif len(groups) == 0 {\n\t\treturn []byte(\"[]\"), nil\n\t}\n\n\tswitch g.aggregateType {\n\tcase sumAggregate:\n\t\tcounts = *(*[]groupCountSum)(unsafe.Pointer(&groups))\n\tcase distinctAggregate:\n\t\tcounts = *(*[]groupCountAggregate)(unsafe.Pointer(&groups))\n\tcase decimalSumAggregate:\n\t\tcounts = *(*[]groupCountDecimalSum)(unsafe.Pointer(&groups))\n\t}\n\treturn json.Marshal(counts)\n}\n\n// GroupCount represents a result item for a group by query.\ntype GroupCount struct {\n\tGroup      []FieldRow   `json:\"group\"`\n\tCount      uint64       `json:\"count\"`\n\tAgg        int64        `json:\"-\"`\n\tDecimalAgg *pql.Decimal `json:\"-\"`\n}\n\ntype groupCountSum struct {\n\tGroup      []FieldRow   `json:\"group\"`\n\tCount      uint64       `json:\"count\"`\n\tAgg        int64        `json:\"sum\"`\n\tDecimalAgg *pql.Decimal `json:\"-\"`\n}\n\ntype groupCountAggregate struct {\n\tGroup      []FieldRow   `json:\"group\"`\n\tCount      uint64       `json:\"count\"`\n\tAgg        int64        `json:\"aggregate\"`\n\tDecimalAgg *pql.Decimal `json:\"-\"`\n}\n\ntype groupCountDecimalSum struct {\n\tGroup      []FieldRow   `json:\"group\"`\n\tCount      uint64       `json:\"count\"`\n\tAgg        int64        `json:\"-\"`\n\tDecimalAgg *pql.Decimal `json:\"sum\"`\n}\n\nvar (\n\t_ GroupCount = GroupCount(groupCountSum{})\n\t_ GroupCount = GroupCount(groupCountAggregate{})\n\t_ GroupCount = GroupCount(groupCountDecimalSum{})\n)\n\nfunc (g *GroupCount) Clone() (r *GroupCount) {\n\tr = &GroupCount{\n\t\tGroup:      make([]FieldRow, len(g.Group)),\n\t\tCount:      g.Count,\n\t\tAgg:        g.Agg,\n\t\tDecimalAgg: g.DecimalAgg,\n\t}\n\tfor i := range g.Group {\n\t\tr.Group[i] = *(g.Group[i].Clone())\n\t}\n\treturn\n}\n\n// mergeGroupCounts merges two slices of GroupCounts throwing away any that go\n// beyond the limit. It assume that the two slices are sorted by the row ids in\n// the fields of the group counts. It may modify its arguments.\nfunc mergeGroupCounts(a, b []GroupCount, limit int) []GroupCount {\n\tif limit > len(a)+len(b) {\n\t\tlimit = len(a) + len(b)\n\t}\n\tret := make([]GroupCount, 0, limit)\n\ti, j := 0, 0\n\tfor i < len(a) && j < len(b) && len(ret) < limit {\n\t\tswitch a[i].Compare(b[j]) {\n\t\tcase -1:\n\t\t\tret = append(ret, a[i])\n\t\t\ti++\n\t\tcase 0:\n\t\t\ta[i].Count += b[j].Count\n\t\t\ta[i].Agg += b[j].Agg\n\t\t\tif a[i].DecimalAgg != nil && b[j].DecimalAgg != nil {\n\t\t\t\tsum := pql.AddDecimal(*a[i].DecimalAgg, *b[j].DecimalAgg)\n\t\t\t\ta[i].DecimalAgg = &sum\n\t\t\t}\n\t\t\tret = append(ret, a[i])\n\t\t\ti++\n\t\t\tj++\n\t\tcase 1:\n\t\t\tret = append(ret, b[j])\n\t\t\tj++\n\t\t}\n\t}\n\tfor ; i < len(a) && len(ret) < limit; i++ {\n\t\tret = append(ret, a[i])\n\t}\n\tfor ; j < len(b) && len(ret) < limit; j++ {\n\t\tret = append(ret, b[j])\n\t}\n\treturn ret\n}\n\n// Compare is used in ordering two GroupCount objects.\nfunc (g GroupCount) Compare(o GroupCount) int {\n\tfor i, g1 := range g.Group {\n\t\tg2 := o.Group[i]\n\n\t\tif g1.Value != nil && g2.Value != nil {\n\t\t\tif *g1.Value < *g2.Value {\n\t\t\t\treturn -1\n\t\t\t}\n\t\t\tif *g1.Value > *g2.Value {\n\t\t\t\treturn 1\n\t\t\t}\n\t\t} else {\n\t\t\tif g1.RowID < g2.RowID {\n\t\t\t\treturn -1\n\t\t\t}\n\t\t\tif g1.RowID > g2.RowID {\n\t\t\t\treturn 1\n\t\t\t}\n\t\t}\n\t}\n\treturn 0\n}\n\nfunc (g GroupCount) satisfiesCondition(subj string, cond *pql.Condition) bool {\n\tswitch subj {\n\tcase \"count\":\n\t\tswitch cond.Op {\n\t\tcase pql.EQ, pql.NEQ, pql.LT, pql.LTE, pql.GT, pql.GTE:\n\t\t\tval, ok := cond.Uint64Value()\n\t\t\tif !ok {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tif cond.Op == pql.EQ {\n\t\t\t\tif g.Count == val {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t} else if cond.Op == pql.NEQ {\n\t\t\t\tif g.Count != val {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t} else if cond.Op == pql.LT {\n\t\t\t\tif g.Count < val {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t} else if cond.Op == pql.LTE {\n\t\t\t\tif g.Count <= val {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t} else if cond.Op == pql.GT {\n\t\t\t\tif g.Count > val {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t} else if cond.Op == pql.GTE {\n\t\t\t\tif g.Count >= val {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t}\n\t\tcase pql.BETWEEN, pql.BTWN_LT_LTE, pql.BTWN_LTE_LT, pql.BTWN_LT_LT:\n\t\t\tval, ok := cond.Uint64SliceValue()\n\t\t\tif !ok {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tif cond.Op == pql.BETWEEN {\n\t\t\t\tif val[0] <= g.Count && g.Count <= val[1] {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t} else if cond.Op == pql.BTWN_LT_LTE {\n\t\t\t\tif val[0] < g.Count && g.Count <= val[1] {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t} else if cond.Op == pql.BTWN_LTE_LT {\n\t\t\t\tif val[0] <= g.Count && g.Count < val[1] {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t} else if cond.Op == pql.BTWN_LT_LT {\n\t\t\t\tif val[0] < g.Count && g.Count < val[1] {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\tcase \"sum\":\n\t\tswitch cond.Op {\n\t\tcase pql.EQ, pql.NEQ, pql.LT, pql.LTE, pql.GT, pql.GTE:\n\t\t\tval, ok := cond.Int64Value()\n\t\t\tif !ok {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tif cond.Op == pql.EQ {\n\t\t\t\tif g.Agg == val {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t} else if cond.Op == pql.NEQ {\n\t\t\t\tif g.Agg != val {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t} else if cond.Op == pql.LT {\n\t\t\t\tif g.Agg < val {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t} else if cond.Op == pql.LTE {\n\t\t\t\tif g.Agg <= val {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t} else if cond.Op == pql.GT {\n\t\t\t\tif g.Agg > val {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t} else if cond.Op == pql.GTE {\n\t\t\t\tif g.Agg >= val {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t}\n\t\tcase pql.BETWEEN, pql.BTWN_LT_LTE, pql.BTWN_LTE_LT, pql.BTWN_LT_LT:\n\t\t\tval, ok := cond.Int64SliceValue()\n\t\t\tif !ok {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tif cond.Op == pql.BETWEEN {\n\t\t\t\tif val[0] <= g.Agg && g.Agg <= val[1] {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t} else if cond.Op == pql.BTWN_LT_LTE {\n\t\t\t\tif val[0] < g.Agg && g.Agg <= val[1] {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t} else if cond.Op == pql.BTWN_LTE_LT {\n\t\t\t\tif val[0] <= g.Agg && g.Agg < val[1] {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t} else if cond.Op == pql.BTWN_LT_LT {\n\t\t\t\tif val[0] < g.Agg && g.Agg < val[1] {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn false\n}\n\n// ApplyConditionToGroupCounts filters the contents of gcs according\n// to the condition. Currently, `count` and `sum` are the only\n// fields supported.\nfunc ApplyConditionToGroupCounts(gcs []GroupCount, subj string, cond *pql.Condition) []GroupCount {\n\tvar i int\n\tfor _, gc := range gcs {\n\t\tif !gc.satisfiesCondition(subj, cond) {\n\t\t\tcontinue // drop this GroupCount\n\t\t}\n\t\tgcs[i] = gc\n\t\ti++\n\t}\n\treturn gcs[:i]\n}\n\nfunc (e *executor) executeGroupByShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, filter *pql.Call, shard uint64, childRows []RowIDs, bases map[int]int64, ignoreLimit bool) (_ []GroupCount, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeGroupByShard\")\n\tdefer span.Finish()\n\n\tvar filterRow *Row\n\tif filter != nil {\n\t\tif filterRow, err = e.executeBitmapCallShard(ctx, qcx, index, filter, shard); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, \"executing group by filter for shard %d\", shard)\n\t\t}\n\t}\n\n\taggregate, _, err := c.CallArg(\"aggregate\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tnewspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeGroupByShard_newGroupByIterator\")\n\titer, err := newGroupByIterator(e, qcx, childRows, c.Children, aggregate, filterRow, index, shard, e.Holder)\n\tnewspan.Finish()\n\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"getting group by iterator for shard %d\", shard)\n\t}\n\tif iter == nil {\n\t\treturn []GroupCount{}, nil\n\t}\n\n\tlimit := int(^uint(0) >> 1)\n\tif lim, hasLimit, err := c.UintArg(\"limit\"); err != nil {\n\t\treturn nil, err\n\t} else if !ignoreLimit && hasLimit {\n\t\tlimit = int(lim)\n\t}\n\n\tresults := make([]GroupCount, 0)\n\n\tnum := 0\n\tfor gc, done, err := iter.Next(ctx); !done && num < limit; gc, done, err = iter.Next(ctx) {\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif gc.Count > 0 {\n\t\t\tnum++\n\t\t\tresults = append(results, gc)\n\t\t}\n\t}\n\n\t// Apply bases.\n\t//\n\t// SUP-139: The group value is shared across multiple groups so we can't\n\t// add the base to each one. Instead, we need to track which ones have been\n\t// seen already and avoid adding to those again in the future.\n\tfor i, base := range bases {\n\t\tm := make(map[*int64]struct{})\n\n\t\tfor _, r := range results {\n\t\t\tif _, ok := m[r.Group[i].Value]; ok {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t*r.Group[i].Value += base\n\t\t\tm[r.Group[i].Value] = struct{}{}\n\t\t}\n\t}\n\n\treturn results, nil\n}\n\nfunc (e *executor) executeRows(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (RowIDs, error) {\n\t// Fetch field name from argument.\n\t// Check \"field\" first for backwards compatibility.\n\t// TODO: remove at Pilosa 2.0\n\tvar fieldName string\n\tvar ok bool\n\tif fieldName, ok = c.Args[\"field\"].(string); ok {\n\t\tc.Args[\"_field\"] = fieldName\n\t}\n\tif fieldName, ok = c.Args[\"_field\"].(string); !ok {\n\t\treturn nil, errors.New(\"Rows() field required\")\n\t}\n\tif columnID, ok, err := c.UintArg(\"column\"); err != nil {\n\t\treturn nil, errors.Wrap(err, \"getting column\")\n\t} else if ok {\n\t\tshards = []uint64{columnID / ShardWidth}\n\t}\n\n\t// TODO, support \"in\" in conjunction w/ other args... or at least error if they're present together\n\tif ids, found, err := c.UintSliceArg(\"in\"); err != nil {\n\t\treturn nil, errors.Wrapf(err, \"'in' argument of Rows must be a slice\")\n\t} else if found {\n\t\t// \"in\" not supported with other args, so check here\n\t\tfor arg := range c.Args {\n\t\t\tif arg != \"field\" && arg != \"_field\" && arg != \"in\" {\n\t\t\t\treturn nil, errors.Errorf(\"Rows call with 'in' does not support other arguments, but found '%s'\", arg)\n\t\t\t}\n\t\t}\n\t\treturn ids, nil\n\t}\n\n\t// Execute calls in bulk on each remote node and merge.\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\treturn e.executeRowsShard(ctx, qcx, index, fieldName, c, shard)\n\t}\n\n\t// Determine limit so we can use it when reducing.\n\tlimit := int(^uint(0) >> 1)\n\tif lim, hasLimit, err := c.UintArg(\"limit\"); err != nil {\n\t\treturn nil, err\n\t} else if hasLimit {\n\t\tlimit = int(lim)\n\t}\n\n\t// Merge returned results at coordinating node.\n\treduceFn := func(ctx context.Context, prev, v interface{}) interface{} {\n\t\tother, _ := prev.(RowIDs)\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn other.Merge(v.(RowIDs), limit)\n\t}\n\t// Get full result set.\n\tother, err := e.mapReduce(ctx, index, shards, c, opt, mapFn, reduceFn)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tresults, _ := other.(RowIDs)\n\n\tif !opt.Remote {\n\t\tif like, hasLike, err := c.StringArg(\"like\"); err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"getting like pattern\")\n\t\t} else if hasLike {\n\t\t\tmatches, err := e.Cluster.matchField(ctx, e.Holder.Field(index, fieldName), like)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"matching like pattern\")\n\t\t\t}\n\n\t\t\ti, j, k := 0, 0, 0\n\t\t\tfor i < len(results) && j < len(matches) {\n\t\t\t\tx, y := results[i], matches[j]\n\t\t\t\tswitch {\n\t\t\t\tcase x < y:\n\t\t\t\t\ti++\n\t\t\t\tcase y < x:\n\t\t\t\t\tj++\n\t\t\t\tdefault:\n\t\t\t\t\tresults[k] = x\n\t\t\t\t\ti++\n\t\t\t\t\tj++\n\t\t\t\t\tk++\n\t\t\t\t}\n\t\t\t}\n\t\t\tresults = results[:k]\n\t\t}\n\t}\n\n\treturn results, nil\n}\n\nfunc (e *executor) executeRowsShard(ctx context.Context, qcx *Qcx, index string, fieldName string, c *pql.Call, shard uint64) (_ RowIDs, err0 error) {\n\t// Fetch index.\n\tidx := e.Holder.Index(index)\n\tif idx == nil {\n\t\treturn nil, newNotFoundError(ErrIndexNotFound, index)\n\t}\n\t// Fetch field.\n\tf := e.Holder.Field(index, fieldName)\n\tif f == nil {\n\t\treturn nil, newNotFoundError(ErrFieldNotFound, fieldName)\n\t}\n\n\t// rowIDs is the result set.\n\tvar rowIDs RowIDs\n\n\t// TODO, support \"in\" in conjunction w/ other args... or at least error if they're present together\n\tif ids, found, err := c.UintSliceArg(\"in\"); err != nil {\n\t\treturn nil, errors.Wrapf(err, \"'in' argument of Rows must be a slice\")\n\t} else if found {\n\t\treturn ids, nil\n\t}\n\n\t// views contains the list of views to inspect (and merge)\n\t// in order to represent `Rows` for the field.\n\tviews := []string{viewStandard}\n\n\tswitch f.Type() {\n\tcase FieldTypeSet, FieldTypeMutex:\n\tcase FieldTypeTime:\n\t\tvar err error\n\n\t\t// Parse \"from\" time, if set.\n\t\tvar fromTime time.Time\n\t\tif v, ok := c.Args[\"from\"]; ok {\n\t\t\tif fromTime, err = parseTime(v); err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"parsing from time\")\n\t\t\t}\n\t\t}\n\n\t\t// Parse \"to\" time, if set.\n\t\tvar toTime time.Time\n\t\tif v, ok := c.Args[\"to\"]; ok {\n\t\t\tif toTime, err = parseTime(v); err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"parsing to time\")\n\t\t\t}\n\t\t}\n\t\tviews, err = f.viewsByTimeRange(fromTime, toTime)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\tdefault:\n\t\treturn nil, errors.Errorf(\"%s fields not supported by Rows() query\", f.Type())\n\t}\n\n\tstart := uint64(0)\n\tif previous, ok, err := c.UintArg(\"previous\"); err != nil {\n\t\treturn nil, errors.Wrap(err, \"getting previous\")\n\t} else if ok {\n\t\tstart = previous + 1\n\t}\n\n\tfilters := []roaring.BitmapFilter{}\n\tif columnID, ok, err := c.UintArg(\"column\"); err != nil {\n\t\treturn nil, err\n\t} else if ok {\n\t\tcolShard := columnID >> shardwidth.Exponent\n\t\tif colShard != shard {\n\t\t\treturn rowIDs, nil\n\t\t}\n\t\tfilters = append(filters, roaring.NewBitmapColumnFilter(columnID))\n\t}\n\n\tlimit := int(^uint(0) >> 1)\n\tif lim, hasLimit, err := c.UintArg(\"limit\"); err != nil {\n\t\treturn nil, errors.Wrap(err, \"getting limit\")\n\t} else if hasLimit {\n\t\tfilters = append(filters, roaring.NewBitmapRowLimitFilter(lim))\n\t\tlimit = int(lim)\n\t}\n\n\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Index: idx, Shard: shard})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer finisher(&err0)\n\tfor _, view := range views {\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfrag := e.Holder.fragment(index, fieldName, view, shard)\n\t\tif frag == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tviewRows, err := frag.rows(ctx, tx, start, filters...)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\trowIDs = rowIDs.Merge(viewRows, limit)\n\t}\n\n\treturn rowIDs, nil\n}\n\ntype ExtractedTableField struct {\n\tName string `json:\"name\"`\n\tType string `json:\"type\"`\n}\n\ntype KeyOrID struct {\n\tID    uint64\n\tKey   string\n\tKeyed bool\n}\n\nfunc (kid KeyOrID) MarshalJSON() ([]byte, error) {\n\tif kid.Keyed {\n\t\treturn json.Marshal(kid.Key)\n\t}\n\n\treturn json.Marshal(kid.ID)\n}\n\ntype ExtractedTableColumn struct {\n\tColumn KeyOrID       `json:\"column\"`\n\tRows   []interface{} `json:\"rows\"`\n}\n\ntype ExtractedTable struct {\n\tFields  []ExtractedTableField  `json:\"fields\"`\n\tColumns []ExtractedTableColumn `json:\"columns\"`\n}\n\n// ToRows implements the ToRowser interface.\nfunc (t ExtractedTable) ToRows(callback func(*proto.RowResponse) error) error {\n\tif len(t.Columns) == 0 {\n\t\treturn nil\n\t}\n\n\theaders := make([]*proto.ColumnInfo, len(t.Fields)+1)\n\tcolType := \"uint64\"\n\tif t.Columns[0].Column.Keyed {\n\t\tcolType = \"string\"\n\t}\n\theaders[0] = &proto.ColumnInfo{\n\t\tName:     \"_id\",\n\t\tDatatype: colType,\n\t}\n\tdataHeaders := headers[1:]\n\tfor i, f := range t.Fields {\n\t\tdataHeaders[i] = &proto.ColumnInfo{\n\t\t\tName:     f.Name,\n\t\t\tDatatype: f.Type,\n\t\t}\n\t}\n\n\tfor _, c := range t.Columns {\n\t\tcols := make([]*proto.ColumnResponse, len(c.Rows)+1)\n\t\tif c.Column.Keyed {\n\t\t\tcols[0] = &proto.ColumnResponse{\n\t\t\t\tColumnVal: &proto.ColumnResponse_StringVal{\n\t\t\t\t\tStringVal: c.Column.Key,\n\t\t\t\t},\n\t\t\t}\n\t\t} else {\n\t\t\tcols[0] = &proto.ColumnResponse{\n\t\t\t\tColumnVal: &proto.ColumnResponse_Uint64Val{\n\t\t\t\t\tUint64Val: c.Column.ID,\n\t\t\t\t},\n\t\t\t}\n\t\t}\n\t\tvalCols := cols[1:]\n\t\tfor i, r := range c.Rows {\n\t\t\tvar col *proto.ColumnResponse\n\t\t\tswitch r := r.(type) {\n\t\t\tcase nil:\n\t\t\t\tcol = &proto.ColumnResponse{}\n\t\t\tcase bool:\n\t\t\t\tcol = &proto.ColumnResponse{\n\t\t\t\t\tColumnVal: &proto.ColumnResponse_BoolVal{\n\t\t\t\t\t\tBoolVal: r,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\tcase int64:\n\t\t\t\tcol = &proto.ColumnResponse{\n\t\t\t\t\tColumnVal: &proto.ColumnResponse_Int64Val{\n\t\t\t\t\t\tInt64Val: r,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\tcase uint64:\n\t\t\t\tcol = &proto.ColumnResponse{\n\t\t\t\t\tColumnVal: &proto.ColumnResponse_Uint64Val{\n\t\t\t\t\t\tUint64Val: r,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\tcase string:\n\t\t\t\tcol = &proto.ColumnResponse{\n\t\t\t\t\tColumnVal: &proto.ColumnResponse_StringVal{\n\t\t\t\t\t\tStringVal: r,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\tcase []uint64:\n\t\t\t\tcol = &proto.ColumnResponse{\n\t\t\t\t\tColumnVal: &proto.ColumnResponse_Uint64ArrayVal{\n\t\t\t\t\t\tUint64ArrayVal: &proto.Uint64Array{\n\t\t\t\t\t\t\tVals: r,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\tcase []string:\n\t\t\t\tcol = &proto.ColumnResponse{\n\t\t\t\t\tColumnVal: &proto.ColumnResponse_StringArrayVal{\n\t\t\t\t\t\tStringArrayVal: &proto.StringArray{\n\t\t\t\t\t\t\tVals: r,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\tcase pql.Decimal:\n\t\t\t\trValue := r.Value()\n\t\t\t\trValuePtr := &rValue\n\t\t\t\tcol = &proto.ColumnResponse{\n\t\t\t\t\tColumnVal: &proto.ColumnResponse_DecimalVal{\n\t\t\t\t\t\tDecimalVal: &proto.Decimal{\n\t\t\t\t\t\t\tValue: rValuePtr.Int64(),\n\t\t\t\t\t\t\tScale: r.Scale,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\tcase time.Time:\n\t\t\t\tcol = &proto.ColumnResponse{\n\t\t\t\t\tColumnVal: &proto.ColumnResponse_TimestampVal{\n\t\t\t\t\t\tTimestampVal: r.UTC().Format(time.RFC3339Nano),\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\treturn errors.Errorf(\"unsupported field value: %v (type: %T)\", r, r)\n\t\t\t}\n\t\t\tvalCols[i] = col\n\t\t}\n\t\terr := callback(&proto.RowResponse{\n\t\t\tHeaders: headers,\n\t\t\tColumns: cols,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// ToTable converts the table to protobuf format.\nfunc (t ExtractedTable) ToTable() (*proto.TableResponse, error) {\n\treturn proto.RowsToTable(t, len(t.Columns))\n}\n\ntype ExtractedIDColumn struct {\n\tColumnID uint64\n\tRows     [][]uint64\n}\n\ntype ExtractedIDMatrix struct {\n\tFields  []string\n\tColumns []ExtractedIDColumn\n}\n\nfunc (e *ExtractedIDMatrix) Append(m ExtractedIDMatrix) {\n\te.Columns = append(e.Columns, m.Columns...)\n\tif e.Fields == nil {\n\t\te.Fields = m.Fields\n\t}\n}\n\nvar (\n\ttypeSQLNullString = reflect.TypeOf(sql.NullString{})\n\ttypeSQLNullBool   = reflect.TypeOf(sql.NullBool{})\n\ttypeSQLNullInt32  = reflect.TypeOf(sql.NullInt32{})\n\ttypeSQLNullInt64  = reflect.TypeOf(sql.NullInt64{})\n)\n\nfunc (e *executor) executeExternalLookup(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (res ExtractedTable, err error) {\n\tif e.Holder.lookupDB == nil {\n\t\treturn ExtractedTable{}, errors.New(\"external DB connection is not configured\")\n\t}\n\n\tidx := e.Holder.Index(index)\n\tif idx == nil {\n\t\treturn ExtractedTable{}, errors.Errorf(\"index not found: %q\", index)\n\t}\n\n\tquery, ok, err := c.StringArg(\"query\")\n\tif err != nil {\n\t\treturn ExtractedTable{}, errors.Wrap(err, \"looking up query\")\n\t}\n\tif !ok {\n\t\treturn ExtractedTable{}, errors.New(\"missing query\")\n\t}\n\n\tswitch len(c.Children) {\n\tcase 0:\n\t\treturn ExtractedTable{}, errors.New(\"missing lookup input\")\n\tcase 1:\n\tdefault:\n\t\treturn ExtractedTable{}, errors.New(\"too many inputs to lookup query\")\n\t}\n\n\twrite, _, err := c.BoolArg(\"write\")\n\tif err != nil {\n\t\treturn ExtractedTable{}, errors.Wrap(err, \"parsing write argument\")\n\t}\n\n\trawArg, err := e.executeCall(ctx, qcx, index, c.Children[0], shards, opt)\n\tif err != nil {\n\t\treturn ExtractedTable{}, errors.Wrapf(err, \"evaluating SQL argument call %q\", c.String())\n\t}\n\n\tqr := []interface{}{rawArg}\n\terr = e.translateResults(ctx, index, idx, c.Children, qr, e.maxMemory)\n\tif err != nil {\n\t\treturn ExtractedTable{}, errors.Wrap(err, \"translating query result\")\n\t}\n\targRow, ok := qr[0].(*Row)\n\tif !ok {\n\t\treturn ExtractedTable{}, errors.Errorf(\"argument call result is a %T but expected a row\", qr[0])\n\t}\n\tif !argRow.Any() {\n\t\treturn ExtractedTable{}, nil\n\t}\n\n\tvar arg interface{}\n\tif argRow.Keys != nil {\n\t\targ = argRow.Keys\n\t} else {\n\t\targ = argRow.Columns()\n\t}\n\n\tif write {\n\t\ttx, err := e.Holder.lookupDB.BeginTx(ctx, nil)\n\t\tif err != nil {\n\t\t\treturn ExtractedTable{}, errors.Wrap(err, \"creating postgres transaction\")\n\t\t}\n\t\tdefer tx.Rollback() //nolint:errcheck\n\n\t\t_, err = tx.ExecContext(ctx, query, pq.Array(arg))\n\t\tif err != nil {\n\t\t\treturn ExtractedTable{}, errors.Wrap(err, \"executing postgres write\")\n\t\t}\n\n\t\treturn ExtractedTable{}, errors.Wrap(tx.Commit(), \"committing postgres transaction\")\n\t}\n\n\tresult, err := e.Holder.lookupDB.QueryContext(ctx, query, pq.Array(arg))\n\tif err != nil {\n\t\treturn ExtractedTable{}, errors.Wrapf(err, \"SQL query failed\")\n\t}\n\tdefer func() {\n\t\tcerr := result.Close()\n\t\tif cerr != nil && err == nil {\n\t\t\terr = cerr\n\t\t}\n\t}()\n\tif !result.Next() {\n\t\treturn ExtractedTable{}, errors.Wrap(result.Err(), \"reading SQL query result\")\n\t}\n\n\tcolTypes, err := result.ColumnTypes()\n\tif err != nil {\n\t\treturn ExtractedTable{}, errors.Wrapf(err, \"fetching SQL query result types\")\n\t}\n\n\tscanSlots := make([]interface{}, len(colTypes))\n\tscanMapper := make([]func() interface{}, len(colTypes))\n\theader := make([]ExtractedTableField, len(colTypes))\n\tfor i, colType := range colTypes {\n\t\tscanType := colType.ScanType()\n\tsetupScan:\n\t\tif scanType.PkgPath() != \"\" {\n\t\t\t// This is a named type.\n\t\t\tswitch scanType {\n\t\t\tcase typeSQLNullString:\n\t\t\t\tvar dst sql.NullString\n\t\t\t\tscanSlots[i] = &dst\n\t\t\t\tscanMapper[i] = func() interface{} {\n\t\t\t\t\tif !dst.Valid {\n\t\t\t\t\t\treturn nil\n\t\t\t\t\t}\n\n\t\t\t\t\treturn dst.String\n\t\t\t\t}\n\t\t\t\theader[i] = ExtractedTableField{\n\t\t\t\t\tName: colType.Name(),\n\t\t\t\t\tType: \"string\",\n\t\t\t\t}\n\n\t\t\tcase typeSQLNullBool:\n\t\t\t\tvar dst sql.NullBool\n\t\t\t\tscanSlots[i] = &dst\n\t\t\t\tscanMapper[i] = func() interface{} {\n\t\t\t\t\tif !dst.Valid {\n\t\t\t\t\t\treturn nil\n\t\t\t\t\t}\n\n\t\t\t\t\treturn dst.Bool\n\t\t\t\t}\n\t\t\t\theader[i] = ExtractedTableField{\n\t\t\t\t\tName: colType.Name(),\n\t\t\t\t\tType: \"bool\",\n\t\t\t\t}\n\n\t\t\tcase typeSQLNullInt32:\n\t\t\t\tvar dst sql.NullInt32\n\t\t\t\tscanSlots[i] = &dst\n\t\t\t\tscanMapper[i] = func() interface{} {\n\t\t\t\t\tif !dst.Valid {\n\t\t\t\t\t\treturn nil\n\t\t\t\t\t}\n\n\t\t\t\t\treturn int64(dst.Int32)\n\t\t\t\t}\n\t\t\t\theader[i] = ExtractedTableField{\n\t\t\t\t\tName: colType.Name(),\n\t\t\t\t\tType: \"int64\",\n\t\t\t\t}\n\n\t\t\tcase typeSQLNullInt64:\n\t\t\t\tvar dst sql.NullInt64\n\t\t\t\tscanSlots[i] = &dst\n\t\t\t\tscanMapper[i] = func() interface{} {\n\t\t\t\t\tif !dst.Valid {\n\t\t\t\t\t\treturn nil\n\t\t\t\t\t}\n\n\t\t\t\t\treturn dst.Int64\n\t\t\t\t}\n\t\t\t\theader[i] = ExtractedTableField{\n\t\t\t\t\tName: colType.Name(),\n\t\t\t\t\tType: \"int64\",\n\t\t\t\t}\n\n\t\t\tdefault:\n\t\t\t\treturn ExtractedTable{}, errors.Errorf(\"unable to process result type %v from SQL query %q\", scanType, query)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\tswitch scanType.Kind() {\n\t\tcase reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64:\n\t\t\tvar dst uint64\n\t\t\tscanSlots[i] = &dst\n\t\t\tscanMapper[i] = func() interface{} { return dst }\n\t\t\theader[i] = ExtractedTableField{\n\t\t\t\tName: colType.Name(),\n\t\t\t\tType: \"uint64\",\n\t\t\t}\n\n\t\tcase reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64:\n\t\t\t// The database may have lied and this may be nullable.\n\t\t\tscanType = typeSQLNullInt64\n\t\t\tgoto setupScan\n\n\t\tcase reflect.String:\n\t\t\t// The database may have lied and this may be nullable.\n\t\t\tscanType = typeSQLNullString\n\t\t\tgoto setupScan\n\n\t\tcase reflect.Bool:\n\t\t\t// The database may have lied and this may be nullable.\n\t\t\tscanType = typeSQLNullBool\n\t\t\tgoto setupScan\n\n\t\tdefault:\n\t\t\treturn ExtractedTable{}, errors.Errorf(\"unable to process result type %v from SQL query %q\", scanType, query)\n\t\t}\n\t}\n\n\tvar columns []ExtractedTableColumn\n\tfor {\n\t\terr := result.Scan(scanSlots...)\n\t\tif err != nil {\n\t\t\treturn ExtractedTable{}, errors.Wrap(err, \"scanning SQL result\")\n\t\t}\n\n\t\tvar col KeyOrID\n\t\tswitch v := scanMapper[0]().(type) {\n\t\tcase nil:\n\t\t\treturn ExtractedTable{}, errors.Errorf(\"missing primary key in result\")\n\t\tcase uint64:\n\t\t\tcol.ID = v\n\t\tcase int64:\n\t\t\tcol.ID = uint64(v)\n\t\tcase string:\n\t\t\tcol.Keyed = true\n\t\t\tcol.Key = v\n\t\tdefault:\n\t\t\treturn ExtractedTable{}, errors.Wrap(err, \"cannot use %v of type %T as primary key for result table\")\n\t\t}\n\n\t\tdata := make([]interface{}, len(scanMapper)-1)\n\t\tfor i, m := range scanMapper[1:] {\n\t\t\tdata[i] = m()\n\t\t}\n\n\t\tcolumns = append(columns, ExtractedTableColumn{\n\t\t\tColumn: col,\n\t\t\tRows:   data,\n\t\t})\n\n\t\tif !result.Next() {\n\t\t\tbreak\n\t\t}\n\t}\n\n\terr = result.Err()\n\tif err != nil {\n\t\treturn ExtractedTable{}, errors.Wrap(err, \"reading SQL result\")\n\t}\n\n\treturn ExtractedTable{\n\t\tFields:  header[1:],\n\t\tColumns: columns,\n\t}, nil\n}\n\ntype TimeArgs struct {\n\tFrom time.Time\n\tTo   time.Time\n}\n\nfunc extractFieldsFromRowsCalls(c *pql.Call) ([]string, []TimeArgs, error) {\n\tfields := make([]string, len(c.Children)-1)\n\ttimeArgs := make([]TimeArgs, len(c.Children)-1)\n\tfor i, rows := range c.Children[1:] {\n\t\tif rows.Name != \"Rows\" {\n\t\t\treturn fields, timeArgs, errors.Errorf(\"child call of Extract is %q but expected Rows\", rows.Name)\n\t\t}\n\t\tvar fieldName string\n\t\tvar ok bool\n\t\tvar timeArg TimeArgs\n\t\tfor k, v := range rows.Args {\n\t\t\tswitch k {\n\t\t\tcase \"field\", \"_field\":\n\t\t\t\tfieldName = v.(string)\n\t\t\t\tok = true\n\t\t\tcase \"from\":\n\t\t\t\tfromTime, err := parseTime(v)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn fields, timeArgs, errors.Wrap(err, \"parsing from time\")\n\t\t\t\t}\n\t\t\t\ttimeArg.From = fromTime\n\t\t\tcase \"to\":\n\t\t\t\ttoTime, err := parseTime(v)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn fields, timeArgs, errors.Wrap(err, \"parsing from time\")\n\t\t\t\t}\n\t\t\t\ttimeArg.To = toTime\n\t\t\tdefault:\n\t\t\t\treturn fields, timeArgs, errors.Errorf(\"unsupported Rows argument for Extract: %q\", k)\n\t\t\t}\n\t\t}\n\t\tif !ok {\n\t\t\treturn fields, timeArgs, errors.New(\"missing field specification in Rows\")\n\t\t}\n\t\tfields[i] = fieldName\n\t\ttimeArgs[i] = timeArg\n\t}\n\treturn fields, timeArgs, nil\n}\n\nfunc makeReduceFunc(sort_desc bool) func(ctx context.Context, prev, v interface{}) interface{} {\n\treturn func(ctx context.Context, prev, v interface{}) interface{} {\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tswitch other := v.(type) {\n\t\tcase ExtractedIDMatrixSorted:\n\t\t\tif prev == nil {\n\t\t\t\treturn other\n\t\t\t}\n\t\t\tif _, ok := prev.(ExtractedIDMatrixSorted); !ok {\n\t\t\t\treturn other\n\t\t\t}\n\t\t\tif out, err := MergeExtractedIDMatrixSorted(other, prev.(ExtractedIDMatrixSorted), sort_desc); err != nil {\n\t\t\t\treturn err\n\t\t\t} else {\n\t\t\t\treturn out\n\t\t\t}\n\n\t\tcase ExtractedIDMatrix:\n\t\t\tif prev != nil {\n\t\t\t\tif p, ok := prev.(ExtractedIDMatrix); ok {\n\t\t\t\t\tother.Append(p)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn other\n\t\tcase nil:\n\t\t\treturn prev\n\t\tdefault:\n\t\t\treturn ExtractedIDMatrix{}\n\t\t}\n\t}\n}\n\nfunc handleExtractResults(other interface{}, filter *pql.Call, opt *ExecOptions) (interface{}, error) {\n\tswitch results := other.(type) {\n\tcase ExtractedIDMatrix:\n\t\tsort.Slice(results.Columns, func(i, j int) bool {\n\t\t\treturn results.Columns[i].ColumnID < results.Columns[j].ColumnID\n\t\t})\n\t\treturn results, nil\n\tcase ExtractedIDMatrixSorted:\n\t\toffset, hasOffset, err := filter.UintArg(\"offset\")\n\t\tif err != nil {\n\t\t\treturn ExtractedIDMatrix{}, err\n\t\t}\n\t\tlimit, hasLimit, err := filter.UintArg(\"limit\")\n\t\tif err != nil {\n\t\t\treturn ExtractedIDMatrix{}, err\n\t\t}\n\t\tif hasOffset {\n\t\t\tresults.ExtractedIDMatrix.Columns = results.ExtractedIDMatrix.Columns[offset:]\n\t\t}\n\t\tif hasLimit && limit < uint64(len(results.RowKVs)) {\n\t\t\tresults.ExtractedIDMatrix.Columns = results.ExtractedIDMatrix.Columns[:limit]\n\t\t}\n\t\t// need to reture sorted to originating node in order to be able to sort properly\n\t\tif opt.Remote {\n\t\t\treturn results, nil\n\t\t}\n\t\treturn *results.ExtractedIDMatrix, nil\n\tdefault:\n\t\treturn ExtractedIDMatrix{}, errors.New(\"Extract, unexpected result type found\")\n\t}\n}\n\nfunc (e *executor) executeExtract(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (interface{}, error) {\n\t// Extract the column filter call.\n\tif len(c.Children) < 1 {\n\t\treturn ExtractedIDMatrix{}, errors.New(\"missing column filter in Extract\")\n\t}\n\tfilter := c.Children[0]\n\tvar sort_desc bool\n\tif filter.Name == \"Sort\" {\n\t\tsd, _, err := filter.BoolArg(\"sort-desc\")\n\t\tif err != nil {\n\t\t\treturn ExtractedIDMatrix{}, errors.Wrap(err, \"sort field error\")\n\t\t}\n\t\tsort_desc = sd\n\t}\n\n\t// Extract fields from rows calls.\n\tfields, timeArgs, err := extractFieldsFromRowsCalls(c)\n\tif err != nil {\n\t\treturn ExtractedIDMatrix{}, errors.Wrap(err, \"sort field error\")\n\t}\n\n\t// Execute calls in bulk on each remote node and merge.\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\treturn e.executeExtractShard(ctx, qcx, index, fields, filter, shard, mopt, timeArgs)\n\t}\n\n\t// Merge returned results at coordinating node.\n\treduceFn := makeReduceFunc(sort_desc)\n\t// Get full result set.\n\tother, err := e.mapReduce(ctx, index, shards, c, opt, mapFn, reduceFn)\n\tif err != nil {\n\t\treturn ExtractedIDMatrix{}, err\n\t}\n\treturn handleExtractResults(other, filter, opt)\n}\n\nfunc mergeBits(bits *Row, mask uint64, out map[uint64]uint64) {\n\tfor _, v := range bits.Columns() {\n\t\tout[v] |= mask\n\t}\n}\n\nvar (\n\ttrueRowFakeID  = []uint64{1}\n\tfalseRowFakeID = []uint64{0}\n)\n\nfunc (e *executor) executeExtractShard(ctx context.Context, qcx *Qcx, index string, fields []string, filter *pql.Call, shard uint64, mopt *mapOptions, timeArgs []TimeArgs) (_ interface{}, err0 error) {\n\tvar colsBitmap *Row\n\tvar cols []uint64\n\tvar sortedResult *SortedRow\n\tif filter.Name == \"Sort\" {\n\t\tres, err := e.executeSortShard(ctx, qcx, index, filter, shard)\n\t\tif err != nil {\n\t\t\treturn ExtractedIDMatrix{}, errors.Wrap(err, \"failed to get extraction sort column filter\")\n\t\t}\n\t\tcols = res.Columns()\n\t\tcolsBitmap = res.Row\n\t\tsortedResult = res\n\t} else {\n\t\t// Execute filter.\n\t\tres, err := e.executeBitmapCallShard(ctx, qcx, index, filter, shard)\n\t\tif err != nil {\n\t\t\treturn ExtractedIDMatrix{}, errors.Wrap(err, \"failed to get extraction column filter\")\n\t\t}\n\t\t// Decompress columns bitmap.\n\t\tcolsBitmap = res\n\t\tcols = colsBitmap.Columns()\n\t}\n\n\t// Fetch index.\n\tidx := e.Holder.Index(index)\n\tif idx == nil {\n\t\treturn ExtractedIDMatrix{}, newNotFoundError(ErrIndexNotFound, index)\n\t}\n\n\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Index: idx, Shard: shard})\n\tif err != nil {\n\t\treturn ExtractedIDMatrix{}, err\n\t}\n\tdefer finisher(&err0)\n\n\t// Generate a matrix to stuff the results into.\n\tm := make([]ExtractedIDColumn, len(cols))\n\t{\n\t\trowsBuf := make([][]uint64, len(m)*len(fields))\n\t\tfor i, c := range cols {\n\t\t\tm[i] = ExtractedIDColumn{\n\t\t\t\tColumnID: c,\n\t\t\t\tRows:     rowsBuf[i*len(fields) : (i+1)*len(fields) : (i+1)*len(fields)],\n\t\t\t}\n\t\t}\n\t}\n\tif len(m) == 0 {\n\t\treturn ExtractedIDMatrix{\n\t\t\tFields:  fields,\n\t\t\tColumns: m,\n\t\t}, nil\n\t}\n\tmLookup := make(map[uint64]int)\n\tfor i, j := range cols {\n\t\tmLookup[j] = i\n\t}\n\n\t// Process fields.\n\tfor i, name := range fields {\n\t\t// Look up the field.\n\t\tfield := idx.Field(name)\n\t\tif field == nil {\n\t\t\treturn ExtractedIDMatrix{}, newNotFoundError(ErrFieldNotFound, name)\n\t\t}\n\n\t\tif field.options.TrackExistence {\n\t\t\texistenceRow, err := field.Existing(tx, shard)\n\t\t\tif err != nil {\n\t\t\t\treturn ExtractedIDMatrix{}, fmt.Errorf(\"trying to find existence bit: %w\", err)\n\t\t\t}\n\t\t\t// filter out any rows which exist, but aren't in this data...\n\t\t\tif existenceRow != nil {\n\t\t\t\texistenceRow = existenceRow.Intersect(colsBitmap)\n\t\t\t\texisting := existenceRow.Columns()\n\t\t\t\tfor _, col := range existing {\n\t\t\t\t\tm[mLookup[col]].Rows[i] = []uint64{}\n\t\t\t\t}\n\t\t\t}\n\t\t} else if field.options.Type == FieldTypeSet || field.options.Type == FieldTypeTime {\n\t\t\t// time quantums and sets which don't have track-existence should treat every\n\t\t\t// column as a non-null empty set if it has no bits, rather than as a null.\n\t\t\tfor _, col := range cols {\n\t\t\t\tm[mLookup[col]].Rows[i] = []uint64{}\n\t\t\t}\n\t\t}\n\n\t\tswitch field.Type() {\n\t\tcase FieldTypeSet, FieldTypeMutex:\n\t\t\t// Handle a set field by listing the rows and then intersecting them with the filter.\n\n\t\t\t// Extract the standard view fragment.\n\t\t\tfragment := e.Holder.fragment(index, name, viewStandard, shard)\n\t\t\tif fragment == nil {\n\t\t\t\t// There is nothing here.\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// List all rows in the standard view.\n\t\t\trows, err := fragment.rows(ctx, tx, 0)\n\t\t\tif err != nil {\n\t\t\t\treturn ExtractedIDMatrix{}, errors.Wrap(err, \"listing rows in set field\")\n\t\t\t}\n\n\t\t\t// Loop over each row and scan the intersection with the filter.\n\t\t\tfor _, rowID := range rows {\n\t\t\t\t// Load row from fragment.\n\t\t\t\trow, err := fragment.row(tx, rowID)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn ExtractedIDMatrix{}, errors.Wrap(err, \"loading row from fragment\")\n\t\t\t\t}\n\n\t\t\t\t// Apply column filter to row.\n\t\t\t\trow = row.Intersect(colsBitmap)\n\n\t\t\t\t// Rotate vector into the matrix.\n\t\t\t\tfor _, columnID := range row.Columns() {\n\t\t\t\t\tfieldSlot := &m[mLookup[columnID]].Rows[i]\n\t\t\t\t\t*fieldSlot = append(*fieldSlot, rowID)\n\t\t\t\t}\n\t\t\t}\n\t\tcase FieldTypeTime:\n\t\t\t//Handle a set field by listing the rows and then intersecting them with the filter.\n\t\t\ttimeArg := timeArgs[i]\n\n\t\t\tviews, err := field.viewsByTimeRange(timeArg.From, timeArg.To)\n\t\t\tif err != nil {\n\t\t\t\treturn ExtractedIDMatrix{}, errors.Wrap(err, \"problem with from/to\")\n\t\t\t}\n\n\t\t\tdedup := make(map[uint64]map[uint64]struct{})\n\t\t\tfor _, view := range views {\n\t\t\t\tfragment := e.Holder.fragment(index, name, view, shard)\n\t\t\t\tif fragment == nil {\n\t\t\t\t\t// There is nothing here.\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\trows, err := fragment.rows(ctx, tx, 0)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn ExtractedIDMatrix{}, errors.Wrap(err, \"listing rows in set field\")\n\t\t\t\t}\n\n\t\t\t\t// Loop over each row and scan the intersection with the filter.\n\t\t\t\tfor _, rowID := range rows {\n\t\t\t\t\t// Load row from fragment.\n\t\t\t\t\trow, err := fragment.row(tx, rowID)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn ExtractedIDMatrix{}, errors.Wrap(err, \"loading row from fragment\")\n\t\t\t\t\t}\n\n\t\t\t\t\t// Apply column filter to row.\n\t\t\t\t\trow = row.Intersect(colsBitmap)\n\t\t\t\t\tif len(views) == 1 {\n\t\t\t\t\t\tfor _, columnID := range row.Columns() {\n\t\t\t\t\t\t\tfieldSlot := &m[mLookup[columnID]].Rows[i]\n\t\t\t\t\t\t\t*fieldSlot = append(*fieldSlot, rowID)\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// Rotate vector into the matrix.\n\t\t\t\t\t\tfor _, columnID := range row.Columns() {\n\t\t\t\t\t\t\tif _m, ok := dedup[columnID]; ok {\n\t\t\t\t\t\t\t\t_m[rowID] = struct{}{}\n\t\t\t\t\t\t\t\tdedup[columnID] = _m\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tdedup[columnID] = map[uint64]struct{}{rowID: {}}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif len(views) > 1 {\n\t\t\t\tfor columnID, rowsMap := range dedup {\n\t\t\t\t\trowids := make([]uint64, 0, len(rowsMap))\n\t\t\t\t\tfor k := range rowsMap {\n\t\t\t\t\t\trowids = append(rowids, k)\n\t\t\t\t\t}\n\t\t\t\t\tsort.Slice(rowids, func(i, j int) bool { return rowids[i] < rowids[j] })\n\t\t\t\t\tm[mLookup[columnID]].Rows[i] = rowids\n\t\t\t\t}\n\t\t\t}\n\t\tcase FieldTypeBool:\n\t\t\t// Handle bool fields by scanning the true and false rows and assigning an integer.\n\n\t\t\t// Extract the standard view fragment.\n\t\t\tfragment := e.Holder.fragment(index, name, viewStandard, shard)\n\t\t\tif fragment == nil {\n\t\t\t\t// There is nothing here.\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Fetch true and false rows.\n\t\t\ttrueRow, err := fragment.row(tx, trueRowID)\n\t\t\tif err != nil {\n\t\t\t\treturn ExtractedIDMatrix{}, errors.Wrap(err, \"loading true row from fragment\")\n\t\t\t}\n\t\t\tfalseRow, err := fragment.row(tx, falseRowID)\n\t\t\tif err != nil {\n\t\t\t\treturn ExtractedIDMatrix{}, errors.Wrap(err, \"loading true row from fragment\")\n\t\t\t}\n\n\t\t\t// Fetch values by column.\n\t\t\tfor j := range m {\n\t\t\t\tcol := m[j].ColumnID\n\t\t\t\tswitch {\n\t\t\t\tcase trueRow.Includes(col):\n\t\t\t\t\tm[j].Rows[i] = trueRowFakeID\n\t\t\t\tcase falseRow.Includes(col):\n\t\t\t\t\tm[j].Rows[i] = falseRowFakeID\n\t\t\t\t}\n\t\t\t}\n\n\t\tcase FieldTypeInt, FieldTypeDecimal, FieldTypeTimestamp:\n\t\t\t// Handle an int/decimal field by rotating a BSI matrix.\n\n\t\t\t// Extract the BSI view fragment.\n\t\t\tfragment := e.Holder.fragment(index, name, viewBSIGroupPrefix+name, shard)\n\t\t\tif fragment == nil {\n\t\t\t\t// There is nothing here.\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Load the BSI group.\n\t\t\tbsig := field.bsiGroup(name)\n\t\t\tif bsig == nil {\n\t\t\t\treturn ExtractedIDMatrix{}, ErrBSIGroupNotFound\n\t\t\t}\n\n\t\t\t// Load the BSI exists bit.\n\t\t\texists, err := fragment.row(tx, bsiExistsBit)\n\t\t\tif err != nil {\n\t\t\t\treturn ExtractedIDMatrix{}, errors.Wrap(err, \"loading BSI exists bit from fragment\")\n\t\t\t}\n\n\t\t\t// Filter BSI exists bit by selected columns.\n\t\t\texists = exists.Intersect(colsBitmap)\n\t\t\tif !exists.Any() {\n\t\t\t\t// No relevant BSI values are present in this fragment.\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Populate a map with the BSI data.\n\t\t\tdata := make(map[uint64]uint64)\n\t\t\tmergeBits(exists, 0, data)\n\n\t\t\t// Copy in the sign bit.\n\t\t\tsign, err := fragment.row(tx, bsiSignBit)\n\t\t\tif err != nil {\n\t\t\t\treturn ExtractedIDMatrix{}, errors.Wrap(err, \"loading BSI sign bit from fragment\")\n\t\t\t}\n\t\t\tsign = sign.Intersect(exists)\n\t\t\tmergeBits(sign, 1<<63, data)\n\n\t\t\t// Copy in the significand.\n\t\t\tfor i := uint64(0); i < bsig.BitDepth; i++ {\n\t\t\t\tbits, err := fragment.row(tx, bsiOffsetBit+uint64(i))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn ExtractedIDMatrix{}, errors.Wrap(err, \"loading BSI significand bit from fragment\")\n\t\t\t\t}\n\t\t\t\tbits = bits.Intersect(exists)\n\t\t\t\tmergeBits(bits, 1<<i, data)\n\t\t\t}\n\n\t\t\t// Store the results back into the matrix.\n\t\t\tfor columnID, val := range data {\n\t\t\t\t// Convert to two's complement and add base back to value.\n\t\t\t\tval = uint64((2*(int64(val)>>63)+1)*int64(val&^(1<<63)) + bsig.Base)\n\t\t\t\tm[mLookup[columnID]].Rows[i] = []uint64{val}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Emit the final matrix.\n\t// Like RowIDs, this is an internal type and will need to be converted.\n\tmatrix := ExtractedIDMatrix{\n\t\tFields:  fields,\n\t\tColumns: m,\n\t}\n\tif v := atomic.AddInt64(mopt.memoryAvailable, -calcResultMemory(matrix)); v < 0 {\n\t\treturn ExtractedIDMatrix{}, fmt.Errorf(\"result exceeds available memory\")\n\t}\n\n\tif sortedResult != nil {\n\t\treturn ExtractedIDMatrixSorted{\n\t\t\tExtractedIDMatrix: &matrix,\n\t\t\tRowKVs:            sortedResult.RowKVs,\n\t\t}, nil\n\t}\n\treturn matrix, nil\n}\n\n// getNullRowShard requests a row representing the elements of this shard\n// which are null, that is to say, the elements that exist for the index,\n// but not for this particular field. This is computed as row 0 of the\n// index's existence field, minus the bsiExistsBit of the existenceView.\n// We're using bsiExistsBit for consistency between BSI fields and non-BSI\n// fields. In an ideal world, we'd have done this sooner and BSI fields\n// would also be using a separate existenceView, perhaps? But we are not\n// in that world.\nfunc (e *executor) getNullRowShard(ctx context.Context, tx Tx, idx *Index, fld *Field, existenceView string, shard uint64) (*Row, error) {\n\t// Make sure the index supports existence tracking.\n\tif idx.existenceField() == nil {\n\t\treturn nil, errors.Errorf(\"index does not support existence tracking: %s\", idx.name)\n\t}\n\t// existenceView: the view we've been asked to look in for existence data\n\t// viewExistence: the string \"existence\", similar to the name \"viewStandard\",\n\t// denoting the existence view we use for non-BSI fields. BSI fields always\n\t// track existence.\n\tif !fld.options.TrackExistence && existenceView == viewExistence {\n\t\treturn nil, fmt.Errorf(\"field does not support existence tracking: %s\", fld.name)\n\t}\n\n\tvar existenceRow *Row\n\texistenceFrag := e.Holder.fragment(idx.name, existenceFieldName, viewStandard, shard)\n\tif existenceFrag == nil {\n\t\t// no existence frag -> no existence bits are set -> there are no\n\t\t// records in this shard. therefore no records in this shard are\n\t\t// null. more simply, we don't have to compute the things we want\n\t\t// to subtract from an empty row to figure out that the result\n\t\t// will stay empty.\n\t\treturn NewRow(), nil\n\t} else {\n\t\tvar err error\n\t\tif existenceRow, err = existenceFrag.row(tx, 0); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tvar notNull *Row\n\tvar err error\n\n\t// Retrieve notNull from fragment if it exists.\n\tif frag := e.Holder.fragment(idx.name, fld.name, existenceView, shard); frag != nil {\n\t\tif notNull, err = frag.notNull(tx); err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"getting fragment not null\")\n\t\t}\n\t} else {\n\t\treturn existenceRow, nil\n\t}\n\n\treturn existenceRow.Difference(notNull), nil\n}\n\n// getNonNullRowShard requests a row representing the existence of this shard\n// which are not null. We trust the existenceView for this field to be\n// correct, and not contain bits which don't exist at the index level, so we\n// are not checking against the index's existence field.\nfunc (e *executor) getNonNullRowShard(ctx context.Context, tx Tx, idx *Index, fld *Field, existenceView string, shard uint64) (*Row, error) {\n\t// existenceView: the view we've been asked to look in for existence data\n\t// viewExistence: the string \"existence\", similar to the name \"viewStandard\",\n\t// denoting the existence view we use for non-BSI fields. BSI fields always\n\t// track existence.\n\tif !fld.options.TrackExistence && existenceView == viewExistence {\n\t\treturn nil, fmt.Errorf(\"field does not support existence tracking: %s\", fld.name)\n\t}\n\t// Retrieve fragment.\n\tfrag := e.Holder.fragment(idx.name, fld.name, existenceView, shard)\n\tif frag == nil {\n\t\treturn NewRow(), nil\n\t}\n\treturn frag.notNull(tx)\n}\n\nfunc (e *executor) executeRowShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64) (_ *Row, err0 error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"executor.executeRowShard\")\n\tdefer span.Finish()\n\n\t// Fetch index.\n\tidx := e.Holder.Index(index)\n\tif idx == nil {\n\t\treturn nil, newNotFoundError(ErrIndexNotFound, index)\n\t}\n\n\t// Fetch field name from argument.\n\tfieldName, err := c.FieldArg()\n\tif err != nil {\n\t\treturn nil, errors.New(\"Row() argument required: field\")\n\t}\n\tf := idx.Field(fieldName)\n\tif f == nil {\n\t\treturn nil, newNotFoundError(ErrFieldNotFound, fieldName)\n\t}\n\n\tbsig := f.bsiGroup(fieldName)\n\tif bsig != nil {\n\t\treturn e.executeRowBSIGroupShard(ctx, qcx, f, bsig, c, shard)\n\t}\n\n\terr = e.validateTimeCallArgs(c, index)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Parse \"from\" time, if set.\n\tvar fromTime time.Time\n\tif v, ok := c.Args[\"from\"]; ok {\n\t\tif fromTime, err = parseTime(v); err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"parsing from time\")\n\t\t}\n\t}\n\n\t// Parse \"to\" time, if set.\n\tvar toTime time.Time\n\tif v, ok := c.Args[\"to\"]; ok {\n\t\tif toTime, err = parseTime(v); err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"parsing to time\")\n\t\t}\n\t}\n\n\tisNull, rowID, isEQ, err := c.FieldEquality(fieldName)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"row call: %v\", err)\n\t}\n\n\t// Return row if times are not set and standard view exists.\n\ttimeNotSet := fromTime.IsZero() && toTime.IsZero()\n\tif isNull {\n\t\tif !timeNotSet {\n\t\t\treturn nil, errors.New(\"can't use a time range with a check for/against null\")\n\t\t}\n\t\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Index: idx, Shard: shard})\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tdefer finisher(&err0)\n\t\tif isEQ {\n\t\t\treturn e.getNullRowShard(ctx, tx, f.idx, f, viewExistence, shard)\n\t\t} else {\n\t\t\treturn e.getNonNullRowShard(ctx, tx, f.idx, f, viewExistence, shard)\n\t\t}\n\t}\n\tif !isEQ {\n\t\treturn nil, errors.New(\"only support != for null, not for other values, on set/mutex fields\")\n\t}\n\tif c.Name == \"Row\" && timeNotSet && !f.options.NoStandardView {\n\t\tfrag := e.Holder.fragment(index, fieldName, viewStandard, shard)\n\t\tif frag == nil {\n\t\t\treturn NewRow(), nil\n\t\t}\n\n\t\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Fragment: frag, Index: idx, Shard: shard})\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tdefer finisher(&err0)\n\t\trow, err := frag.row(tx, rowID)\n\t\tif qcx.write && err == nil {\n\t\t\trow = row.Clone()\n\t\t}\n\t\treturn row, err\n\t}\n\n\tviews, err := f.viewsByTimeRange(fromTime, toTime)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Union bitmaps across all time-based views.\n\trows := make([]*Row, 0, len(views))\n\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Index: idx, Shard: shard})\n\tdefer finisher(&err0)\n\tfor _, view := range views {\n\t\tf := e.Holder.fragment(index, fieldName, view, shard)\n\t\tif f == nil {\n\t\t\tcontinue\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\trow, err := f.row(tx, rowID)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\trows = append(rows, row)\n\t}\n\tif len(rows) == 0 {\n\t\treturn &Row{}, nil\n\t} else if len(rows) == 1 {\n\t\tif qcx.write {\n\t\t\treturn rows[0].Clone(), nil\n\t\t}\n\t\treturn rows[0], nil\n\t}\n\trow := rows[0].Union(rows[1:]...)\n\tif qcx.write {\n\t\trow = row.Clone()\n\t}\n\treturn row, nil\n}\n\n// executeRowBSIGroupShard executes a range(bsiGroup) call for a local shard.\nfunc (e *executor) executeRowBSIGroupShard(ctx context.Context, qcx *Qcx, fld *Field, bsig *bsiGroup, c *pql.Call, shard uint64) (cloneable *Row, err0 error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"executor.executeRowBSIGroupShard\")\n\tdefer span.Finish()\n\n\t// Only one conditional should be present.\n\tif len(c.Args) == 0 {\n\t\treturn nil, errors.New(\"Row(): condition required\")\n\t} else if len(c.Args) > 1 {\n\t\treturn nil, errors.New(\"Row(): too many arguments\")\n\t}\n\n\top, value, err := c.FieldRange(fld.name)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tviewName := viewBSIGroupPrefix + fld.name\n\n\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Index: fld.idx, Shard: shard})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer finisher(&err0)\n\tdefer func() {\n\t\tif qcx.write && cloneable != nil {\n\t\t\tcloneable = cloneable.Clone()\n\t\t}\n\t}()\n\n\t// EQ null           _exists - frag.NotNull()\n\t// NEQ null          frag.NotNull()\n\t// BETWEEN a,b(in)   BETWEEN/frag.RowBetween()\n\t// BETWEEN a,b(out)  BETWEEN/frag.NotNull()\n\t// EQ <int>          frag.RangeOp\n\t// NEQ <int>         frag.RangeOp\n\n\t// Handle `!= null` and `== null`.\n\tif op == pql.NEQ && value == nil {\n\t\treturn e.getNonNullRowShard(ctx, tx, fld.idx, fld, viewName, shard)\n\t} else if op == pql.EQ && value == nil {\n\t\treturn e.getNullRowShard(ctx, tx, fld.idx, fld, viewName, shard)\n\t} else if op == pql.BETWEEN || op == pql.BTWN_LT_LT ||\n\t\top == pql.BTWN_LTE_LT || op == pql.BTWN_LT_LTE {\n\t\tpredicates, err := getCondIntSlice(fld, op, value)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"getting condition value\")\n\t\t}\n\n\t\t// Only support two integers for the between operation.\n\t\tif len(predicates) != 2 {\n\t\t\treturn nil, errors.New(\"Row(): BETWEEN condition requires exactly two integer values\")\n\t\t}\n\n\t\t// The reason we don't just call:\n\t\t//     return f.RowBetween(fieldName, predicates[0], predicates[1])\n\t\t// here is because we need the call to be shard-specific.\n\n\t\tbaseValueMin, baseValueMax, outOfRange := bsig.baseValueBetween(predicates[0], predicates[1])\n\t\tif outOfRange {\n\t\t\treturn NewRow(), nil\n\t\t}\n\n\t\t// Retrieve fragment.\n\t\tfrag := e.Holder.fragment(fld.idx.name, fld.name, viewName, shard)\n\t\tif frag == nil {\n\t\t\treturn NewRow(), nil\n\t\t}\n\n\t\t// If the query is asking for the entire valid range, just return\n\t\t// the not-null bitmap for the bsiGroup.\n\t\tif predicates[0] <= bsig.Min && predicates[1] >= bsig.Max {\n\t\t\treturn frag.notNull(tx)\n\t\t}\n\n\t\treturn frag.rangeBetween(tx, bsig.BitDepth, baseValueMin, baseValueMax)\n\n\t} else {\n\t\tvalue, err := getScaledInt(fld, value)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"getting scaled integer\")\n\t\t}\n\n\t\tbaseValue, outOfRange := bsig.baseValue(op, value)\n\t\tif outOfRange && op != pql.NEQ {\n\t\t\treturn NewRow(), nil\n\t\t}\n\n\t\t// Retrieve fragment.\n\t\tfrag := e.Holder.fragment(fld.idx.name, fld.name, viewName, shard)\n\t\tif frag == nil {\n\t\t\treturn NewRow(), nil\n\t\t}\n\n\t\t// LT[E] and GT[E] should return all not-null if selected range fully encompasses valid bsiGroup range.\n\t\tif (op == pql.LT && value > bsig.Max) || (op == pql.LTE && value >= bsig.Max) ||\n\t\t\t(op == pql.GT && value < bsig.Min) || (op == pql.GTE && value <= bsig.Min) {\n\t\t\treturn frag.notNull(tx)\n\t\t}\n\n\t\t// outOfRange for NEQ should return all not-null.\n\t\tif outOfRange && op == pql.NEQ {\n\t\t\treturn frag.notNull(tx)\n\t\t}\n\n\t\treturn frag.rangeOp(tx, op, bsig.BitDepth, baseValue)\n\t}\n}\n\n// executeIntersectShard executes a intersect() call for a local shard.\nfunc (e *executor) executeIntersectShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64) (_ *Row, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeIntersectShard\")\n\tdefer span.Finish()\n\n\tvar other *Row\n\tif len(c.Children) == 0 {\n\t\treturn nil, fmt.Errorf(\"empty Intersect query is currently not supported\")\n\t}\n\tfor i, input := range c.Children {\n\t\trow, err := e.executeBitmapCallShard(ctx, qcx, index, input, shard)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif i == 0 {\n\t\t\tother = row\n\t\t} else {\n\t\t\tother = other.Intersect(row)\n\t\t}\n\t}\n\tother.invalidateCount()\n\treturn other, nil\n}\n\n// executeUnionShard executes a union() call for a local shard.\nfunc (e *executor) executeUnionShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64) (out *Row, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeUnionShard\")\n\tdefer span.Finish()\n\n\tif len(c.Children) == 0 {\n\t\treturn NewRow(), nil\n\t}\n\tif len(c.Children) == 1 {\n\t\treturn e.executeBitmapCallShard(ctx, qcx, index, c.Children[0], shard)\n\t}\n\t// we have at least two, so...\n\trows := make([]*Row, len(c.Children))\n\tfor i, input := range c.Children {\n\t\trows[i], err = e.executeBitmapCallShard(ctx, qcx, index, input, shard)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\treturn rows[0].Union(rows[1:]...), nil\n}\n\n// executeInnerUnionRowsShard executes a special magical call which is actually\n// more like Row() than Union(), and takes a call plus a []uint64 of rows, and\n// generates the union of the rows in the []uint64.\nfunc (e *executor) executeInnerUnionRowsShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64) (out *Row, err0 error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"Executor.executeInnerUnionRowsShard\")\n\tdefer span.Finish()\n\n\t// Fetch index.\n\tidx := e.Holder.Index(index)\n\tif idx == nil {\n\t\treturn nil, newNotFoundError(ErrIndexNotFound, index)\n\t}\n\n\tfieldName, ok, err := c.StringArg(\"_field\")\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"finding field\")\n\t}\n\tif !ok {\n\t\treturn nil, errors.New(\"InnerUnionRows requires _field\")\n\t}\n\n\tf := idx.Field(fieldName)\n\tif f == nil {\n\t\treturn nil, newNotFoundError(ErrFieldNotFound, fieldName)\n\t}\n\n\t// Parse \"from\" time, if set.\n\tvar fromTime time.Time\n\tif v, ok := c.Args[\"from\"]; ok {\n\t\tif fromTime, err = parseTime(v); err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"parsing from time\")\n\t\t}\n\t}\n\n\t// Parse \"to\" time, if set.\n\tvar toTime time.Time\n\tif v, ok := c.Args[\"to\"]; ok {\n\t\tif toTime, err = parseTime(v); err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"parsing to time\")\n\t\t}\n\t}\n\n\trowIDs, rowOK, err := c.UintSliceArg(\"rows\")\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"extracting rows argument: %v\", err)\n\t}\n\tif !rowOK {\n\t\treturn nil, fmt.Errorf(\"InnerUnionRows() must specify rows\")\n\t}\n\n\t// Simply return row if times are not set.\n\ttimeNotSet := fromTime.IsZero() && toTime.IsZero()\n\tif timeNotSet {\n\t\tfrag := e.Holder.fragment(index, fieldName, viewStandard, shard)\n\t\tif frag == nil {\n\t\t\treturn NewRow(), nil\n\t\t}\n\n\t\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Fragment: frag, Index: idx, Shard: shard})\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tdefer finisher(&err0)\n\t\trow, err := frag.unionRows(ctx, tx, rowIDs)\n\t\tif qcx.write && err == nil {\n\t\t\trow = row.Clone()\n\t\t}\n\t\treturn row, err\n\t}\n\n\tviews, err := f.viewsByTimeRange(fromTime, toTime)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Union bitmaps across all time-based views.\n\trows := make([]*Row, 0, len(views))\n\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Index: idx, Shard: shard})\n\tdefer finisher(&err0)\n\tfor _, view := range views {\n\t\tf := e.Holder.fragment(index, fieldName, view, shard)\n\t\tif f == nil {\n\t\t\tcontinue\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\trow, err := f.unionRows(ctx, tx, rowIDs)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\trows = append(rows, row)\n\t}\n\tif len(rows) == 0 {\n\t\treturn &Row{}, nil\n\t} else if len(rows) == 1 {\n\t\tif qcx.write {\n\t\t\treturn rows[0].Clone(), nil\n\t\t}\n\t\treturn rows[0], nil\n\t}\n\trow := rows[0].Union(rows[1:]...)\n\tif qcx.write {\n\t\trow = row.Clone()\n\t}\n\treturn row, nil\n}\n\n// executeXorShard executes a xor() call for a local shard.\nfunc (e *executor) executeXorShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64) (_ *Row, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeXorShard\")\n\tdefer span.Finish()\n\n\tother := NewRow()\n\tfor i, input := range c.Children {\n\t\trow, err := e.executeBitmapCallShard(ctx, qcx, index, input, shard)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif i == 0 {\n\t\t\tother = row\n\t\t} else {\n\t\t\tother = other.Xor(row)\n\t\t}\n\t}\n\tother.invalidateCount()\n\treturn other, nil\n}\n\n// executePrecomputedCallShard pretends to execute a precomputed call for a local shard.\nfunc (e *executor) executePrecomputedCallShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64) (_ *Row, err error) {\n\tif c.Precomputed != nil {\n\t\tv := c.Precomputed[shard]\n\t\tif v == nil {\n\t\t\treturn NewRow(), nil\n\t\t}\n\t\tif r, ok := v.(*Row); ok {\n\t\t\tif r != nil {\n\t\t\t\treturn r, nil\n\t\t\t} else {\n\t\t\t\treturn NewRow(), nil\n\t\t\t}\n\t\t}\n\t\treturn nil, fmt.Errorf(\"precomputed value is not a row: %T\", v)\n\t}\n\treturn nil, fmt.Errorf(\"per-shard: missing precomputed values for shard %d\", shard)\n}\n\n// executeNotShard executes a Not() call for a local shard.\nfunc (e *executor) executeNotShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64) (_ *Row, err0 error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeNotShard\")\n\tdefer span.Finish()\n\n\tif len(c.Children) == 0 {\n\t\treturn nil, errors.New(\"Not() requires an input row\")\n\t} else if len(c.Children) > 1 {\n\t\treturn nil, errors.New(\"Not() only accepts a single row input\")\n\t}\n\n\t// Make sure the index supports existence tracking.\n\tidx := e.Holder.Index(index)\n\tif idx == nil {\n\t\treturn nil, newNotFoundError(ErrIndexNotFound, index)\n\t} else if idx.existenceField() == nil {\n\t\treturn nil, errors.Errorf(\"index does not support existence tracking: %s\", index)\n\t}\n\n\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Index: idx, Shard: shard})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer finisher(nil)\n\n\tvar existenceRow *Row\n\texistenceFrag := e.Holder.fragment(index, existenceFieldName, viewStandard, shard)\n\tif existenceFrag == nil {\n\t\texistenceRow = NewRow()\n\t} else {\n\t\tif existenceRow, err = existenceFrag.row(tx, 0); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif qcx.write {\n\t\t\texistenceRow = existenceRow.Clone()\n\t\t}\n\t}\n\t// the finishers returned by a write tx, which we might be in if there's\n\t// a higher-level write in this call OR ANY OTHER CALL, are safe to\n\t// double-call, but we have to be sure of finishing before starting a\n\t// bitmap call, or we lock against ourselves.\n\tfinisher(nil)\n\n\trow, err := e.executeBitmapCallShard(ctx, qcx, index, c.Children[0], shard)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn existenceRow.Difference(row), nil\n}\n\nfunc (e *executor) executeConstRow(ctx context.Context, qcx *Qcx, index string, c *pql.Call, opt *ExecOptions) (res *Row, err error) {\n\tidx := e.Holder.Index(index)\n\tif idx == nil {\n\t\treturn nil, newNotFoundError(ErrIndexNotFound, index)\n\t} else if idx.existenceField() == nil {\n\t\tids, ok := c.Args[\"columns\"].([]uint64)\n\t\tif !ok {\n\t\t\treturn nil, errors.New(\"missing columns list\")\n\t\t}\n\t\treturn NewRow(ids...), nil\n\t}\n\t// Fetch user-provided columns list.\n\tavailableBitmap := roaring.NewBitmap()\n\tvar records []uint64\n\tif opt.Remote {\n\t\tids, _ := c.Args[\"columns\"].([]interface{})\n\t\tif len(ids) == 0 {\n\t\t\treturn NewRow(), nil\n\t\t}\n\t\tfor _, idi := range ids {\n\t\t\tid := idi.(int64)\n\t\t\tavailableBitmap.Add(uint64(id) / ShardWidth)\n\t\t\trecords = append(records, uint64(id))\n\t\t}\n\n\t} else {\n\t\tids, ok := c.Args[\"columns\"].([]uint64)\n\t\tif !ok {\n\t\t\treturn nil, errors.New(\"missing columns list\")\n\t\t}\n\t\tif len(ids) == 0 {\n\t\t\treturn NewRow(), nil\n\t\t}\n\t\tfor _, id := range ids {\n\t\t\tavailableBitmap.Add(id / ShardWidth)\n\t\t\trecords = append(records, id)\n\t\t}\n\t}\n\n\tfilteredShards := availableBitmap.Slice()\n\t// Execute calls in bulk on each remote node and merge.\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\treturn e.executeConstRowShard(ctx, qcx, index, c, shard, NewRow(records...))\n\t}\n\t// Merge returned results at coordinating node.\n\treduceFn := func(ctx context.Context, prev, v interface{}) interface{} {\n\t\tother, _ := prev.(*Row)\n\t\tif other == nil {\n\t\t\tother = NewRow()\n\t\t}\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tother.Merge(v.(*Row))\n\t\treturn other\n\t}\n\n\tother, err := e.mapReduce(ctx, index, filteredShards, c, opt, mapFn, reduceFn)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"map reduce\")\n\t}\n\n\trow, _ := other.(*Row)\n\n\treturn row, nil\n}\n\nfunc (e *executor) executeConstRowShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64, src *Row) (res *Row, err error) {\n\tidx := e.Holder.Index(index)\n\tvar existenceRow *Row\n\texistenceFrag := e.Holder.fragment(index, existenceFieldName, viewStandard, shard)\n\tif existenceFrag == nil {\n\t\texistenceRow = NewRow()\n\t} else {\n\t\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Index: idx, Fragment: existenceFrag, Shard: shard})\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tdefer finisher(&err)\n\n\t\tif existenceRow, err = existenceFrag.row(tx, 0); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t}\n\treturn src.Intersect(existenceRow), nil\n}\n\nfunc (e *executor) executeUnionRows(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (*Row, error) {\n\t// Turn UnionRows(Rows(...)) into Union(Row(...), ...).\n\tvar rows []*pql.Call\n\tfor _, child := range c.Children {\n\t\t// Check that we can use the call.\n\t\tswitch child.Name {\n\t\tcase \"Rows\":\n\t\tcase \"TopN\":\n\t\tdefault:\n\t\t\treturn nil, errors.Errorf(\"cannot use %v as a rows query\", child)\n\t\t}\n\n\t\t// Execute the call.\n\t\trowsResult, err := e.executeCall(ctx, qcx, index, child, shards, opt)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Turn the results into rows calls.\n\t\tvar resultRows []*pql.Call\n\t\tswitch rowsResult := rowsResult.(type) {\n\t\tcase *PairsField:\n\t\t\t// Translate pairs into rows calls.\n\t\t\t// TODO: This should probably also be adjusted to use InnerUnionRows,\n\t\t\t// but we can't do that for the string key case.\n\t\t\tfor _, p := range rowsResult.Pairs {\n\t\t\t\tvar val interface{}\n\t\t\t\tswitch {\n\t\t\t\tcase p.Key != \"\":\n\t\t\t\t\tval = p.Key\n\t\t\t\tdefault:\n\t\t\t\t\tval = p.ID\n\t\t\t\t}\n\t\t\t\tresultRows = append(resultRows, &pql.Call{\n\t\t\t\t\tName: \"Row\",\n\t\t\t\t\tArgs: map[string]interface{}{\n\t\t\t\t\t\trowsResult.Field: val,\n\t\t\t\t\t},\n\t\t\t\t})\n\t\t\t}\n\t\tcase RowIDs:\n\t\t\t// Make a single InnerUnionRows from this\n\t\t\tresultRows = append(resultRows, &pql.Call{\n\t\t\t\tName: \"InnerUnionRows\",\n\t\t\t\tArgs: map[string]interface{}{\n\t\t\t\t\t\"_field\": child.Args[\"_field\"],\n\t\t\t\t\t\"rows\":   []uint64(rowsResult),\n\t\t\t\t},\n\t\t\t})\n\t\tdefault:\n\t\t\treturn nil, errors.Errorf(\"unexpected Rows type %T\", rowsResult)\n\t\t}\n\n\t\t// Propogate any special properties of the call.\n\t\tswitch child.Name {\n\t\tcase \"Rows\":\n\t\t\t// Propogate \"from\" time, if set.\n\t\t\tif v, ok := child.Args[\"from\"]; ok {\n\t\t\t\tfor _, rowCall := range resultRows {\n\t\t\t\t\trowCall.Args[\"from\"] = v\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Propogate \"to\" time, if set.\n\t\t\tif v, ok := child.Args[\"to\"]; ok {\n\t\t\t\tfor _, rowCall := range resultRows {\n\t\t\t\t\trowCall.Args[\"to\"] = v\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\trows = append(rows, resultRows...)\n\t}\n\n\t// Generate a Union call over the rows.\n\tc = &pql.Call{\n\t\tName:     \"Union\",\n\t\tChildren: rows,\n\t}\n\n\t// Execute the generated Union() call.\n\treturn e.executeBitmapCall(ctx, qcx, index, c, shards, opt)\n}\n\n// executeAllCallShard executes an All() call for a local shard.\nfunc (e *executor) executeAllCallShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64) (res *Row, err0 error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"executor.executeAllCallShard\")\n\tdefer span.Finish()\n\n\tif len(c.Children) > 0 {\n\t\treturn nil, errors.New(\"All() does not accept an input row\")\n\t}\n\n\t// Make sure the index supports existence tracking.\n\tidx := e.Holder.Index(index)\n\tif idx == nil {\n\t\treturn nil, newNotFoundError(ErrIndexNotFound, index)\n\t} else if idx.existenceField() == nil {\n\t\treturn nil, errors.Errorf(\"index does not support existence tracking: %s\", index)\n\t}\n\n\tvar existenceRow *Row\n\texistenceFrag := e.Holder.fragment(index, existenceFieldName, viewStandard, shard)\n\tif existenceFrag == nil {\n\t\texistenceRow = NewRow()\n\t} else {\n\t\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Index: idx, Fragment: existenceFrag, Shard: shard})\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tdefer finisher(&err0)\n\n\t\tif existenceRow, err = existenceFrag.row(tx, 0); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn existenceRow, nil\n}\n\n// executeShiftShard executes a shift() call for a local shard.\nfunc (e *executor) executeShiftShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64) (_ *Row, err error) {\n\tn, _, err := c.IntArg(\"n\")\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"executeShiftShard: %v\", err)\n\t}\n\n\tif len(c.Children) == 0 {\n\t\treturn nil, errors.New(\"Shift() requires an input row\")\n\t} else if len(c.Children) > 1 {\n\t\treturn nil, errors.New(\"Shift() only accepts a single row input\")\n\t}\n\n\trow, err := e.executeBitmapCallShard(ctx, qcx, index, c.Children[0], shard)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn row.Shift(n)\n}\n\n// executeCount executes a count() call.\nfunc (e *executor) executeCount(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (uint64, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeCount\")\n\tdefer span.Finish()\n\n\tif len(c.Children) == 0 {\n\t\treturn 0, errors.New(\"Count() requires an input bitmap\")\n\t} else if len(c.Children) > 1 {\n\t\treturn 0, errors.New(\"Count() only accepts a single bitmap input\")\n\t}\n\n\tchild := c.Children[0]\n\n\t// If the child is distinct/similar, execute it directly here and count the result.\n\tif child.Type == pql.PrecallGlobal {\n\t\tresult, err := e.executeCall(ctx, qcx, index, child, shards, opt)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\n\t\tswitch row := result.(type) {\n\t\tcase *Row:\n\t\t\treturn row.Count(), nil\n\t\tcase SignedRow:\n\t\t\treturn row.Pos.Count() + row.Neg.Count(), nil\n\t\tcase DistinctTimestamp:\n\t\t\treturn uint64(len(row.Values)), nil\n\t\tdefault:\n\t\t\treturn 0, errors.Errorf(\"cannot count result of type %T from call %q\", row, child.String())\n\t\t}\n\t}\n\n\t// Execute calls in bulk on each remote node and merge.\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\trow, err := e.executeBitmapCallShard(ctx, qcx, index, child, shard)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\treturn row.Count(), nil\n\t}\n\n\t// Merge returned results at coordinating node.\n\treduceFn := func(ctx context.Context, prev, v interface{}) interface{} {\n\t\tother, _ := prev.(uint64)\n\t\treturn other + v.(uint64)\n\t}\n\n\tresult, err := e.mapReduce(ctx, index, shards, c, opt, mapFn, reduceFn)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tn, _ := result.(uint64)\n\n\treturn n, nil\n}\n\n// executeClearBit executes a Clear() call.\nfunc (e *executor) executeClearBit(ctx context.Context, qcx *Qcx, index string, c *pql.Call, opt *ExecOptions) (bool, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeClearBit\")\n\tdefer span.Finish()\n\n\t// Read colID\n\tcolID, ok, err := c.UintArg(\"_\" + columnLabel)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"reading Clear() column: %v\", err)\n\t} else if !ok {\n\t\treturn false, fmt.Errorf(\"column argument to Clear(<COLUMN>, <FIELD>=<ROW>) required\")\n\t}\n\t// Read field name.\n\tfieldName, err := c.FieldArg()\n\tif err != nil {\n\t\treturn false, errors.New(\"Clear() argument required: field\")\n\t}\n\n\t// Retrieve field.\n\tidx := e.Holder.Index(index)\n\tif idx == nil {\n\t\treturn false, newNotFoundError(ErrIndexNotFound, index)\n\t}\n\n\tf := idx.Field(fieldName)\n\tif f == nil {\n\t\treturn false, newNotFoundError(ErrFieldNotFound, fieldName)\n\t}\n\n\t// BSI field\n\tif f.Type() == FieldTypeInt || f.Type() == FieldTypeDecimal || f.Type() == FieldTypeTimestamp {\n\t\treturn e.executeClearValueField(ctx, qcx, index, c, f, colID, opt)\n\t}\n\n\trowID, ok, err := c.UintArg(fieldName)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"reading Clear() row: %v\", err)\n\t} else if !ok {\n\t\treturn false, fmt.Errorf(\"row=<row> argument required to Clear() call\")\n\t}\n\n\treturn e.executeClearBitField(ctx, qcx, index, c, f, colID, rowID, opt)\n}\n\n// executeClearBitField executes a Clear() call for a field.\nfunc (e *executor) executeClearBitField(ctx context.Context, qcx *Qcx, index string, c *pql.Call, f *Field, colID, rowID uint64, opt *ExecOptions) (_ bool, err0 error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeClearBitField\")\n\tdefer span.Finish()\n\n\tshard := colID / ShardWidth\n\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := e.Cluster.NewSnapshot()\n\n\tret := false\n\tfor _, node := range snap.ShardNodes(index, shard) {\n\t\t// Update locally if host matches.\n\t\tif node.ID == e.Node.ID {\n\t\t\tval, err := f.ClearBit(qcx, rowID, colID)\n\t\t\tif err != nil {\n\t\t\t\treturn false, err\n\t\t\t} else if val {\n\t\t\t\tret = true\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\t// Do not forward call if this is already being forwarded.\n\t\tif opt.Remote {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Forward call to remote node otherwise.\n\t\tres, err := e.remoteExec(ctx, node, index, &pql.Query{Calls: []*pql.Call{c}}, nil, nil, 0)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\t\tret = res[0].(bool)\n\t}\n\treturn ret, nil\n}\n\n// executeClearRow executes a ClearRow() call.\nfunc (e *executor) executeClearRow(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (_ bool, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeClearRow\")\n\tdefer span.Finish()\n\n\t// Ensure the field type supports ClearRow().\n\tvar fieldName string\n\tfieldName, err = c.FieldArg()\n\tif err != nil {\n\t\treturn false, errors.New(\"ClearRow() argument required: field\")\n\t}\n\tfield := e.Holder.Field(index, fieldName)\n\tif field == nil {\n\t\treturn false, newNotFoundError(ErrFieldNotFound, fieldName)\n\t}\n\n\tswitch field.Type() {\n\tcase FieldTypeSet, FieldTypeTime, FieldTypeMutex, FieldTypeBool:\n\t\t// These field types support ClearRow().\n\tdefault:\n\t\treturn false, fmt.Errorf(\"ClearRow() is not supported on %s field types\", field.Type())\n\t}\n\n\t// Execute calls in bulk on each remote node and merge.\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\treturn e.executeClearRowShard(ctx, qcx, index, c, shard)\n\t}\n\n\t// Merge returned results at coordinating node.\n\treduceFn := func(ctx context.Context, prev, v interface{}) interface{} {\n\t\tval, ok := v.(bool)\n\t\tif !ok {\n\t\t\treturn errors.Errorf(\"executeClearRow.reduceFn: val is non-bool (%+v)\", v)\n\t\t}\n\t\tif prev == nil || val {\n\t\t\treturn val\n\t\t}\n\t\tpval, ok := prev.(bool)\n\t\tif !ok {\n\t\t\treturn errors.Errorf(\"executeClearRow.reduceFn: prev is non-bool (%+v)\", prev)\n\t\t}\n\t\treturn pval\n\t}\n\n\tresult, err := e.mapReduce(ctx, index, shards, c, opt, mapFn, reduceFn)\n\tif err != nil {\n\t\treturn false, errors.Wrap(err, \"mapreducing clearrow\")\n\t}\n\treturn result.(bool), err\n}\n\n// executeClearRowShard executes a ClearRow() call for a single shard.\nfunc (e *executor) executeClearRowShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64) (_ bool, err0 error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"executor.executeClearRowShard\")\n\tdefer span.Finish()\n\n\tfieldName, err := c.FieldArg()\n\tif err != nil {\n\t\treturn false, errors.New(\"ClearRow() argument required: field\")\n\t}\n\n\t// Read fields using labels.\n\tvar rowID uint64\n\tvar ok bool\n\trowID, ok, err = c.UintArg(fieldName)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"reading ClearRow() row: %v\", err)\n\t} else if !ok {\n\t\treturn false, fmt.Errorf(\"ClearRow() row argument '%v' required\", rowLabel)\n\t}\n\n\tfield := e.Holder.Field(index, fieldName)\n\tif field == nil {\n\t\treturn false, newNotFoundError(ErrFieldNotFound, fieldName)\n\t}\n\n\tidx := e.Holder.Index(index)\n\ttx, finisher, err := qcx.GetTx(Txo{Write: writable, Index: idx, Shard: shard})\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tdefer finisher(&err0)\n\n\t// Remove the row from all views.\n\tchanged := false\n\tfor _, view := range field.views() {\n\t\tfragment := e.Holder.fragment(index, fieldName, view.name, shard)\n\t\tif fragment == nil {\n\t\t\tcontinue\n\t\t}\n\t\tcleared, err := fragment.clearRow(tx, rowID)\n\t\tif err != nil {\n\t\t\treturn false, errors.Wrapf(err, \"clearing row %d on view %s shard %d\", rowID, view.name, shard)\n\t\t}\n\t\tchanged = changed || cleared\n\t}\n\n\treturn changed, nil\n}\n\n// executeSetRow executes a Store() call.\n\nfunc (e *executor) executeSetRow(ctx context.Context, qcx *Qcx, indexName string, c *pql.Call, shards []uint64, opt *ExecOptions) (bool, error) {\n\t// Parse arguments.\n\tfieldName, err := c.FieldArg()\n\tif err != nil {\n\t\treturn false, errors.New(\"field required for Store()\")\n\t}\n\n\tfield := e.Holder.Field(indexName, fieldName)\n\tif field == nil {\n\t\treturn false, newNotFoundError(ErrFieldNotFound, fieldName)\n\t}\n\t// Ensure the field type supports Store().\n\tif field.Type() != FieldTypeSet {\n\t\treturn false, fmt.Errorf(\"can't Store() on a %s field\", field.Type())\n\t}\n\n\t// Execute calls in bulk on each remote node and merge.\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\treturn e.executeSetRowShard(ctx, qcx, indexName, c, shard)\n\t}\n\n\t// Merge returned results at coordinating node.\n\treduceFn := func(ctx context.Context, prev, v interface{}) interface{} {\n\t\tval, ok := v.(bool)\n\t\tif !ok {\n\t\t\treturn errors.Errorf(\"executeSetRow.reduceFn: val is non-bool (%+v)\", v)\n\t\t}\n\t\tif prev == nil || val {\n\t\t\treturn val\n\t\t}\n\n\t\tpval, ok := prev.(bool)\n\t\tif !ok {\n\t\t\treturn errors.Errorf(\"executeSetRow.reduceFn: prev is non-bool (%+v)\", prev)\n\t\t}\n\t\treturn pval\n\t}\n\n\tresult, err := e.mapReduce(ctx, indexName, shards, c, opt, mapFn, reduceFn)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\n\tb, ok := result.(bool)\n\tif !ok {\n\t\treturn false, errors.New(\"unsupported result type\")\n\t}\n\treturn b, nil\n}\n\n// executeSetRowShard executes a SetRow() call for a single shard.\nfunc (e *executor) executeSetRowShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64) (_ bool, err0 error) {\n\tfieldName, err := c.FieldArg()\n\tif err != nil {\n\t\treturn false, errors.New(\"Store() argument required: field\")\n\t}\n\n\t// Read fields using labels.\n\tvar rowID uint64\n\tvar ok bool\n\trowID, ok, err = c.UintArg(fieldName)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"reading Store() row: %v\", err)\n\t} else if !ok {\n\t\treturn false, fmt.Errorf(\"need the <FIELD>=<ROW> argument on Store()\")\n\t}\n\n\tfield := e.Holder.Field(index, fieldName)\n\tif field == nil {\n\t\treturn false, newNotFoundError(ErrFieldNotFound, fieldName)\n\t}\n\n\t// Retrieve source row.\n\tvar src *Row\n\tif len(c.Children) == 1 {\n\t\trow, err := e.executeBitmapCallShard(ctx, qcx, index, c.Children[0], shard)\n\t\tif err != nil {\n\t\t\treturn false, errors.Wrap(err, \"getting source row\")\n\t\t}\n\t\tsrc = row\n\t} else {\n\t\treturn false, errors.New(\"Store() requires a source row\")\n\t}\n\n\t// Set the row on the standard view.\n\tchanged := false\n\tfragment := e.Holder.fragment(index, fieldName, viewStandard, shard)\n\tif fragment == nil {\n\t\t// Since the destination fragment doesn't exist, create one.\n\t\tview, err := field.createViewIfNotExists(viewStandard)\n\t\tif err != nil {\n\t\t\treturn false, errors.Wrap(err, \"creating view\")\n\t\t}\n\t\tfragment, err = view.CreateFragmentIfNotExists(shard)\n\t\tif err != nil {\n\t\t\treturn false, errors.Wrapf(err, \"creating fragment: %d\", shard)\n\t\t}\n\t}\n\n\tidx := e.Holder.Index(index)\n\ttx, finisher, err := qcx.GetTx(Txo{Write: writable, Index: idx, Shard: shard})\n\tif err != nil {\n\t\treturn false, err\n\t}\n\n\tdefer finisher(&err0)\n\n\tset, err := fragment.setRow(tx, src, rowID)\n\tif err != nil {\n\t\treturn false, errors.Wrapf(err, \"storing row %d on view %s shard %d\", rowID, viewStandard, shard)\n\t}\n\tchanged = changed || set\n\n\treturn changed, nil\n}\n\n// executeSet executes a Set() call.\nfunc (e *executor) executeSet(ctx context.Context, qcx *Qcx, index string, c *pql.Call, opt *ExecOptions) (_ bool, err0 error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeSet\")\n\tdefer span.Finish()\n\n\t// Read colID.\n\tcolID, ok, err := c.UintArg(\"_\" + columnLabel)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"reading Set() column: %v\", err)\n\t} else if !ok {\n\t\treturn false, fmt.Errorf(\"Set() column argument '%v' required\", columnLabel)\n\t}\n\n\tidx := e.Holder.Index(index)\n\tif idx == nil {\n\t\treturn false, ErrIndexNotFound\n\t}\n\n\t// Read field name.\n\tfieldName, err := c.FieldArg()\n\tif err != nil {\n\t\treturn false, errors.New(\"Set() argument required: field\")\n\t}\n\n\t// Retrieve field.\n\tf := idx.Field(fieldName)\n\tif f == nil {\n\t\treturn false, newNotFoundError(ErrFieldNotFound, fieldName)\n\t}\n\n\t// Set column on existence field.\n\tif ef := idx.existenceField(); ef != nil {\n\t\tif _, err := ef.SetBit(qcx, 0, colID, nil); err != nil {\n\t\t\treturn false, errors.Wrap(err, \"setting existence column\")\n\t\t}\n\t}\n\n\tswitch f.Type() {\n\tcase FieldTypeInt, FieldTypeDecimal, FieldTypeTimestamp:\n\t\t// Fetch field\n\t\tv, ok := c.Arg(fieldName)\n\t\tif !ok {\n\t\t\treturn false, fmt.Errorf(\"Set() row argument '%v' required\", rowLabel)\n\t\t}\n\n\t\t// Before we scale a decimal to an integer, we need to make sure the decimal\n\t\t// is between min/max for the field. If it's not, converting to an integer\n\t\t// can result in an overflow.\n\t\tif dec, ok := v.(pql.Decimal); ok && f.Options().Type == FieldTypeDecimal {\n\t\t\tif dec.LessThan(f.Options().Min) || dec.GreaterThan(f.Options().Max) {\n\t\t\t\treturn false, ErrDecimalOutOfRange\n\t\t\t}\n\t\t}\n\n\t\t// Read row value.\n\t\trowVal, err := getScaledInt(f, v)\n\t\tif err != nil {\n\t\t\treturn false, fmt.Errorf(\"reading Set() row (int/decimal): %v\", err)\n\t\t}\n\t\treturn e.executeSetValueField(ctx, qcx, index, c, f, colID, rowVal, opt)\n\n\tdefault:\n\t\t// Read row ID.\n\t\trowID, ok, err := c.UintArg(fieldName)\n\t\tif err != nil {\n\t\t\treturn false, fmt.Errorf(\"reading Set() row: %v\", err)\n\t\t} else if !ok {\n\t\t\treturn false, fmt.Errorf(\"Set() row argument '%v' required\", rowLabel)\n\t\t}\n\n\t\tvar timestamp *time.Time\n\t\tsTimestamp, ok := c.Args[\"_timestamp\"].(string)\n\t\tif ok {\n\t\t\tt, err := time.Parse(TimeFormat, sTimestamp)\n\t\t\tif err != nil {\n\t\t\t\treturn false, fmt.Errorf(\"invalid date: %s\", sTimestamp)\n\t\t\t}\n\t\t\ttimestamp = &t\n\t\t}\n\n\t\treturn e.executeSetBitField(ctx, qcx, index, c, f, colID, rowID, timestamp, opt)\n\t}\n}\n\n// executeSetBitField executes a Set() call for a specific field.\nfunc (e *executor) executeSetBitField(ctx context.Context, qcx *Qcx, index string, c *pql.Call, f *Field, colID, rowID uint64, timestamp *time.Time, opt *ExecOptions) (_ bool, err0 error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeSetBitField\")\n\tdefer span.Finish()\n\n\tshard := colID / ShardWidth\n\tret := false\n\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := e.Cluster.NewSnapshot()\n\n\tfor _, node := range snap.ShardNodes(index, shard) {\n\t\t// Update locally if host matches.\n\t\tif node.ID == e.Node.ID {\n\t\t\tval, err := f.SetBit(qcx, rowID, colID, timestamp)\n\t\t\tif err != nil {\n\t\t\t\treturn false, err\n\t\t\t} else if val {\n\t\t\t\tret = true\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// Do not forward call if this is already being forwarded.\n\t\tif opt.Remote {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Forward call to remote node otherwise.\n\t\tres, err := e.remoteExec(ctx, node, index, &pql.Query{Calls: []*pql.Call{c}}, nil, nil, 0)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\t\tret = res[0].(bool)\n\t}\n\treturn ret, nil\n}\n\n// executeSetValueField executes a Set() call for a specific int field.\nfunc (e *executor) executeSetValueField(ctx context.Context, qcx *Qcx, index string, c *pql.Call, f *Field, colID uint64, value int64, opt *ExecOptions) (_ bool, err0 error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeSetValueField\")\n\tdefer span.Finish()\n\n\tshard := colID / ShardWidth\n\tret := false\n\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := e.Cluster.NewSnapshot()\n\n\tfor _, node := range snap.ShardNodes(index, shard) {\n\t\t// Update locally if host matches.\n\t\tif node.ID == e.Node.ID {\n\t\t\tval, err := f.SetValue(qcx, colID, value)\n\t\t\tif err != nil {\n\t\t\t\treturn false, err\n\t\t\t} else if val {\n\t\t\t\tret = true\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// Do not forward call if this is already being forwarded.\n\t\tif opt.Remote {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Forward call to remote node otherwise.\n\t\tres, err := e.remoteExec(ctx, node, index, &pql.Query{Calls: []*pql.Call{c}}, nil, nil, 0)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\t\tret = res[0].(bool)\n\t}\n\treturn ret, nil\n}\n\n// executeClearValueField removes value for colID if present\nfunc (e *executor) executeClearValueField(ctx context.Context, qcx *Qcx, index string, c *pql.Call, f *Field, colID uint64, opt *ExecOptions) (_ bool, err0 error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeClearValueField\")\n\tdefer span.Finish()\n\n\tshard := colID / ShardWidth\n\tret := false\n\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := e.Cluster.NewSnapshot()\n\n\tfor _, node := range snap.ShardNodes(index, shard) {\n\t\t// Update locally if host matches.\n\t\tif node.ID == e.Node.ID {\n\t\t\tval, err := f.ClearValue(qcx, colID)\n\t\t\tif err != nil {\n\t\t\t\treturn false, err\n\t\t\t} else if val {\n\t\t\t\tret = true\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// Do not forward call if this is already being forwarded.\n\t\tif opt.Remote {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Forward call to remote node otherwise.\n\t\tres, err := e.remoteExec(ctx, node, index, &pql.Query{Calls: []*pql.Call{c}}, nil, nil, 0)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\t\tret = res[0].(bool)\n\t}\n\treturn ret, nil\n}\n\n// remoteExec executes a PQL query remotely for a set of shards on a node.\nfunc (e *executor) remoteExec(ctx context.Context, node *disco.Node, index string, q *pql.Query, shards []uint64, embed []*Row, maxMemory int64) (results []interface{}, err error) { // nolint: interfacer\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeExec\")\n\tdefer span.Finish()\n\n\t// Encode request object.\n\tpbreq := &QueryRequest{\n\t\tQuery:        q.String(),\n\t\tShards:       shards,\n\t\tRemote:       true,\n\t\tEmbeddedData: embed,\n\t\tMaxMemory:    maxMemory,\n\t}\n\n\taddr := dax.Address(node.URI.String())\n\tresp, err := e.client.QueryNode(ctx, addr, index, pbreq)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn resp.Results, resp.Err\n}\n\n// shardsByNode returns a mapping of nodes to shards.\n// Returns errShardUnavailable if a shard cannot be allocated to a node.\nfunc (e *executor) shardsByNode(nodes []*disco.Node, index string, shards []uint64) (map[*disco.Node][]uint64, error) {\n\tm := make(map[*disco.Node][]uint64)\n\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\t// We use e.Cluster.Nodes() here instead of e.Cluster.noder because we need\n\t// the node states in order to ensure that we don't include an unavailable\n\t// node in the map of nodes to which we distribute the query.\n\tsnap := disco.NewClusterSnapshot(disco.NewLocalNoder(e.Cluster.Nodes()), e.Cluster.Hasher, e.Cluster.partitionAssigner, e.Cluster.ReplicaN)\n\nloop:\n\tfor _, shard := range shards {\n\t\tfor _, node := range snap.ShardNodes(index, shard) {\n\t\t\t// If the node being considered is in any state other than STARTED,\n\t\t\t// then exclude it from the map. This way, one of that node's\n\t\t\t// healthy replicas will be included instead.\n\t\t\tif disco.Nodes(nodes).ContainsID(node.ID) && (node.State == disco.NodeStateStarted || node.State == disco.NodeStateUnknown) {\n\t\t\t\tm[node] = append(m[node], shard)\n\t\t\t\tcontinue loop\n\t\t\t}\n\t\t}\n\t\treturn nil, errors.Wrapf(errShardUnavailable, \"%s:%d:%v\", index, shard, nodes)\n\t}\n\treturn m, nil\n}\n\n// mapReduce maps and reduces data across the cluster.\n//\n// If a mapping of shards to a node fails then the shards are resplit across\n// secondary nodes and retried. This continues to occur until all nodes are exhausted.\n//\n// mapReduce has to ensure that it never returns before any work it spawned has\n// terminated. It's not enough to cancel the jobs; we have to wait for them to be\n// done, or we can unmap resources they're still using.\nfunc (e *executor) mapReduce(ctx context.Context, index string, shards []uint64, c *pql.Call, opt *ExecOptions, mapFn mapFunc, reduceFn reduceFunc) (result interface{}, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.mapReduce\")\n\tdefer span.Finish()\n\n\tch := make(chan mapResponse)\n\n\t// Wrap context with a cancel to kill goroutines on exit.\n\tctx, cancel := context.WithCancel(ctx)\n\t// Create an errgroup so we can wait for all the goroutines to exit\n\teg, ctx := errgroup.WithContext(ctx)\n\n\t// After we're done processing, we have to wait for any outstanding\n\t// functions in the ErrGroup to complete. If we didn't have an error\n\t// already at that point, we'll report any errors from the ErrGroup\n\t// instead.\n\tdefer func() {\n\t\tcancel()\n\t\terrWait := eg.Wait()\n\t\tif err == nil {\n\t\t\terr = errWait\n\t\t}\n\t}()\n\t// If this is the coordinating node then start with all nodes in the cluster.\n\t//\n\t// However, if this request is being sent from the primary then all\n\t// processing should be done locally so we start with just the local node.\n\tvar nodes []*disco.Node\n\tif !opt.Remote {\n\t\tnodes = disco.Nodes(e.Cluster.Nodes()).Clone()\n\t} else {\n\t\tnodes = []*disco.Node{e.Cluster.nodeByID(e.Node.ID)}\n\t}\n\n\t// Start mapping across all primary owners.\n\tif err = e.mapper(ctx, eg, ch, nodes, index, shards, c, opt, e.Cluster.ReplicaN == 1, mapFn, reduceFn); err != nil {\n\t\treturn nil, errors.Wrap(err, \"starting mapper\")\n\t}\n\n\t// Iterate over all map responses and reduce.\n\texpected := len(shards)\n\tdone := ctx.Done()\n\tfor expected > 0 {\n\t\tselect {\n\t\tcase <-done:\n\t\t\treturn nil, ctx.Err()\n\t\tcase resp := <-ch:\n\t\t\t// On error retry against remaining nodes. If an error returns then\n\t\t\t// the context will cancel and cause all open goroutines to return.\n\n\t\t\t// We distinguish here between an error which indicates that the\n\t\t\t// node is not available (and therefore we need to failover to a\n\t\t\t// replica) and a valid error from a healthy node. In the case of\n\t\t\t// the latter, there's no need to retry a replica, we should trust\n\t\t\t// the error from the healthy node and return that immediately.\n\t\t\tif resp.err != nil && strings.Contains(resp.err.Error(), errConnectionRefused) {\n\t\t\t\t// Filter out unavailable nodes.\n\t\t\t\tnodes = disco.Nodes(nodes).FilterID(resp.node.ID)\n\n\t\t\t\t// Begin mapper against secondary nodes.\n\t\t\t\tif err := e.mapper(ctx, eg, ch, nodes, index, resp.shards, c, opt, true, mapFn, reduceFn); errors.Cause(err) == errShardUnavailable {\n\t\t\t\t\treturn nil, resp.err\n\t\t\t\t} else if err != nil {\n\t\t\t\t\treturn nil, errors.Wrap(err, \"mapping on secondary node\")\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t} else if resp.err != nil {\n\t\t\t\treturn nil, errors.Wrap(resp.err, \"mapping on primary node\")\n\t\t\t}\n\t\t\t// if we got a response that we aren't discarding\n\t\t\t// because it's an error, subtract it from our count...\n\t\t\texpected -= len(resp.shards)\n\n\t\t\t// Reduce value.\n\t\t\tresult = reduceFn(ctx, result, resp.result)\n\t\t\tvar ok bool\n\t\t\t// note *not* shadowed.\n\t\t\tif err, ok = result.(error); ok {\n\t\t\t\tcancel()\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\t// note the deferred Wait above which might override this nil.\n\treturn result, nil\n}\n\n// makeEmbeddedDataForShards produces new rows containing the rowSegments\n// that would correspond to a given set of shards.\nfunc makeEmbeddedDataForShards(allRows []*Row, shards []uint64) []*Row {\n\tif len(allRows) == 0 || len(shards) == 0 {\n\t\treturn nil\n\t}\n\tnewRows := make([]*Row, len(allRows))\n\tfor i, row := range allRows {\n\t\tif row == nil || len(row.Segments) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tif row.NoSplit {\n\t\t\tnewRows[i] = row\n\t\t\tcontinue\n\t\t}\n\t\tsegments := row.Segments\n\t\tsegmentIndex := 0\n\t\tnewRows[i] = &Row{\n\t\t\tIndex: row.Index,\n\t\t\tField: row.Field,\n\t\t}\n\t\tfor _, shard := range shards {\n\t\t\tfor segmentIndex < len(segments) && segments[segmentIndex].shard < shard {\n\t\t\t\tsegmentIndex++\n\t\t\t}\n\t\t\t// no more segments in this row\n\t\t\tif segmentIndex >= len(segments) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif segments[segmentIndex].shard == shard {\n\t\t\t\tnewRows[i].Segments = append(newRows[i].Segments, segments[segmentIndex])\n\t\t\t\tsegmentIndex++\n\t\t\t\tif segmentIndex >= len(segments) {\n\t\t\t\t\t// no more segments, we're done\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\t// if we got here, segments[segmentIndex].shard exists\n\t\t\t// but is greater than the current shard, so we continue.\n\t\t}\n\t}\n\treturn newRows\n}\n\nfunc (e *executor) mapper(ctx context.Context, eg *errgroup.Group, ch chan mapResponse, nodes []*disco.Node, index string, shards []uint64, c *pql.Call, opt *ExecOptions, lastAttempt bool, mapFn mapFunc, reduceFn reduceFunc) (reterr error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.mapper\")\n\tdefer span.Finish()\n\n\t// Group shards together by nodes.\n\tm, err := e.shardsByNode(nodes, index, shards)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"shards by node\")\n\t}\n\tdone := ctx.Done()\n\n\t// Execute each node in a separate goroutine.\n\tvar memoryUsed int64\n\tvar mu sync.Mutex\n\tfor n, nodeShards := range m {\n\t\tn := n\n\t\tnodeShards := nodeShards\n\t\teg.Go(func() error {\n\t\t\t// Execute serially max memory is specified.\n\t\t\tif opt.MaxMemory > 0 {\n\t\t\t\tmu.Lock()\n\t\t\t\tdefer mu.Unlock()\n\t\t\t}\n\n\t\t\tresp := mapResponse{node: n, shards: nodeShards}\n\n\t\t\t// Calculate remaining memory. This applies to Extract() only.\n\t\t\t// Default to a high number if we are not tracking memory.\n\t\t\tmemoryAvailable := opt.MaxMemory - atomic.LoadInt64(&memoryUsed)\n\t\t\tif opt.MaxMemory <= 0 {\n\t\t\t\tmemoryAvailable = math.MaxInt64\n\t\t\t}\n\n\t\t\t// Send local shards to mapper, otherwise remote exec.\n\t\t\tif n.ID == e.Node.ID {\n\t\t\t\tresp.result, resp.err = e.mapperLocal(ctx, nodeShards, mapFn, reduceFn, memoryAvailable)\n\t\t\t} else if !opt.Remote {\n\t\t\t\tvar embeddedRowsForNode []*Row\n\t\t\t\tif opt.EmbeddedData != nil {\n\t\t\t\t\tembeddedRowsForNode = makeEmbeddedDataForShards(opt.EmbeddedData, nodeShards)\n\t\t\t\t}\n\t\t\t\tresults, err := e.remoteExec(ctx, n, index, &pql.Query{Calls: []*pql.Call{c}}, nodeShards, embeddedRowsForNode, memoryAvailable)\n\t\t\t\tif len(results) > 0 {\n\t\t\t\t\tresp.result = results[0]\n\t\t\t\t}\n\t\t\t\tresp.err = err\n\t\t\t}\n\n\t\t\t// Track total memory used in response.\n\t\t\tif v := atomic.AddInt64(&memoryUsed, calcResultMemory(resp.result)); opt.MaxMemory > 0 && v > opt.MaxMemory {\n\t\t\t\treturn fmt.Errorf(\"query result exceeded memory threshold\")\n\t\t\t}\n\n\t\t\t// Return response to the channel.\n\t\t\tselect {\n\t\t\tcase <-done:\n\t\t\t\t// If someone just canceled the context\n\t\t\t\t// arbitrarily, we could end up here with this\n\t\t\t\t// being the first non-nil error handed to\n\t\t\t\t// the ErrGroup, in which case, it's the best\n\t\t\t\t// explanation we have for why everything's\n\t\t\t\t// stopping.\n\t\t\t\treturn ctx.Err()\n\t\t\tcase ch <- resp:\n\t\t\t\t// If we return a non-nil error from this, the\n\t\t\t\t// entire errGroup gets canceled. So we don't\n\t\t\t\t// want to return a non-nil error if mapReduce\n\t\t\t\t// might try to run another mapper against a\n\t\t\t\t// different set of nodes. Note that this shouldn't\n\t\t\t\t// matter; we just sent the error to mapReduce\n\t\t\t\t// anyway, so it probably cancels the ErrGroup\n\t\t\t\t// too.\n\t\t\t\tif resp.err != nil && lastAttempt {\n\t\t\t\t\treturn resp.err\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t\tif reterr != nil {\n\t\t\treturn reterr // exit early if error occurs when running serially\n\t\t}\n\t}\n\treturn nil\n}\n\n// calcResultMemory recursively computes the total memory used by v.\nfunc calcResultMemory(v interface{}) (n int64) {\n\tswitch v := v.(type) {\n\tcase ExtractedIDColumn:\n\t\tn += 8 // ColumnID\n\t\tfor _, row := range v.Rows {\n\t\t\tn += 24 + int64(len(row)*8) // slice header + data\n\t\t}\n\t\treturn n\n\n\tcase ExtractedIDMatrix:\n\t\tn += 24 // slice size\n\t\tfor _, field := range v.Fields {\n\t\t\tn += 16 + int64(len(field)) // string header + data\n\t\t}\n\n\t\tn += 24 // Columns slice\n\t\tfor _, col := range v.Columns {\n\t\t\tn += calcResultMemory(col)\n\t\t}\n\t\treturn n\n\n\tcase ExtractedTableColumn:\n\t\tn += 8 + 16 + int64(len(v.Column.Key)) + 8 // KeyOrID\n\t\tfor _, row := range v.Rows {\n\t\t\tn += 8 + calcResultMemory(row) // ptr + value size\n\t\t}\n\t\treturn n\n\n\tcase string:\n\t\treturn 16 + int64(len(v))\n\tcase bool, int64, uint64:\n\t\treturn 8\n\tcase []string:\n\t\tn += 24 // slice header\n\t\tfor i := range v {\n\t\t\tn += 16 + int64(len(v[i]))\n\t\t}\n\t\treturn n\n\tcase []uint64:\n\t\treturn 24 + int64(8*len(v)) // slice header + data size\n\tcase pql.Decimal:\n\t\treturn 16\n\tcase time.Time:\n\t\treturn 24\n\tdefault:\n\t\treturn n\n\t}\n}\n\ntype job struct {\n\tshard           uint64\n\tmapFn           mapFunc\n\tctx             context.Context\n\tmemoryAvailable *int64 // shared, atomic value\n\tresultChan      chan mapResponse\n}\n\n// doOneJob had one job. *disappointed sigh*\nfunc (e *executor) doOneJob() {\n\tj, ok := <-e.work\n\tif !ok {\n\t\treturn\n\t}\n\t// Skip out early if the context is done, but still send\n\t// an ack so mapperLocal can be sure we aren't about to\n\t// work on something it sent us.\n\tif err := j.ctx.Err(); err != nil {\n\t\tj.resultChan <- mapResponse{result: nil, err: err}\n\t\treturn\n\t}\n\tresult, err := j.mapFn(j.ctx, j.shard, &mapOptions{memoryAvailable: j.memoryAvailable})\n\tj.resultChan <- mapResponse{result: result, err: err}\n}\n\nvar errShutdown = errors.New(\"executor has shut down\")\n\n// mapperLocal performs map & reduce entirely on the local node.\nfunc (e *executor) mapperLocal(ctx context.Context, shards []uint64, mapFn mapFunc, reduceFn reduceFunc, memoryAvailable int64) (_ interface{}, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.mapperLocal\")\n\tdefer span.Finish()\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer func() {\n\t\tcancel()\n\t}()\n\tdone := ctx.Done()\n\t// the close process needs to know whether any mappers are currently active.\n\t// note that we bump this BEFORE checking e.shutdown. if we check shutdown\n\t// first, then we can check shutdown, after which the close process closes\n\t// shutdown and checks the count of active mappers, after which we try to\n\t// bump that count.\n\tatomic.AddUint64(&e.activeMappers, 1)\n\tdefer atomic.AddUint64(&e.activeMappers, ^uint64(0))\n\tselect {\n\tcase <-e.shutdown:\n\t\treturn nil, errShutdown\n\tdefault:\n\t}\n\n\tch := make(chan mapResponse, len(shards))\n\n\texpected := 0\nshardLoop:\n\tfor _, shard := range shards {\n\t\tj := job{\n\t\t\tshard:           shard,\n\t\t\tmapFn:           mapFn,\n\t\t\tctx:             ctx,\n\t\t\tresultChan:      ch,\n\t\t\tmemoryAvailable: &memoryAvailable,\n\t\t}\n\t\tselect {\n\t\tcase <-done: // this request's context terminated\n\t\t\tbreak shardLoop\n\t\tcase <-e.shutdown: // whole executor shutting down\n\t\t\tbreak shardLoop\n\t\tcase e.work <- j:\n\t\t\texpected++\n\t\t}\n\t}\n\t// we *absolutely must* get responses for everything we successfully\n\t// transmitted to the work queue, or there could be ongoing access to\n\t// the parent Qcx's stuff.\n\t//\n\t// Even if our context is done, or the executor is shutting down,\n\t// we still have to wait for responses, because the responders are\n\t// going to send them and block waiting for us to receive them.\n\n\t// Reduce results\n\tvar result interface{}\n\tfor expected > 0 {\n\t\tresp := <-ch\n\t\texpected--\n\t\tif resp.err != nil && err == nil {\n\t\t\terr = resp.err\n\t\t}\n\t\tif resp.err == nil && ctx.Err() == nil {\n\t\t\t// Only useful to do a possibly-expensive\n\t\t\t// reduce if we don't already know we don't\n\t\t\t// need it.\n\t\t\tresult = reduceFn(ctx, result, resp.result)\n\t\t\tif resultErr, ok := result.(error); ok {\n\t\t\t\tcancel()\n\t\t\t\terr = resultErr\n\t\t\t}\n\t\t}\n\t}\n\treturn result, err\n}\n\nfunc (e *executor) preTranslate(ctx context.Context, index string, calls ...*pql.Call) (cols map[string]map[string]uint64, rows map[string]map[string]map[string]uint64, err error) {\n\t// Collect all of the required keys.\n\tcollector := keyCollector{\n\t\tcreateCols: make(map[string][]string),\n\t\tfindCols:   make(map[string][]string),\n\t\tcreateRows: make(map[string]map[string][]string),\n\t\tfindRows:   make(map[string]map[string][]string),\n\t}\n\tfor _, call := range calls {\n\t\terr := e.collectCallKeys(&collector, call, index)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t}\n\n\t// Create keys.\n\t// Both rows and columns need to be created first because of foreign index keys.\n\tcols = make(map[string]map[string]uint64)\n\trows = make(map[string]map[string]map[string]uint64)\n\tfor index, keys := range collector.createCols {\n\t\ttranslations, err := e.Cluster.createIndexKeys(ctx, index, keys...)\n\t\tif err != nil {\n\t\t\treturn nil, nil, errors.Wrap(err, \"creating query column keys\")\n\t\t}\n\t\tcols[index] = translations\n\t}\n\tfor index, fields := range collector.createRows {\n\t\tidxRows := make(map[string]map[string]uint64)\n\t\tidx := e.Holder.Index(index)\n\t\tif idx == nil {\n\t\t\treturn nil, nil, errors.Wrapf(ErrIndexNotFound, \"creating rows on index %q\", index)\n\t\t}\n\t\tfor field, keys := range fields {\n\t\t\tf := idx.Field(field)\n\t\t\tif f == nil {\n\t\t\t\treturn nil, nil, errors.Wrapf(ErrFieldNotFound, \"creating rows on field %q in index %q\", field, index)\n\t\t\t}\n\t\t\ttranslations, err := e.Cluster.createFieldKeys(ctx, f, keys...)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, nil, errors.Wrap(err, \"creating query row keys\")\n\t\t\t}\n\t\t\tidxRows[field] = translations\n\t\t}\n\t\trows[index] = idxRows\n\t}\n\n\t// Find other keys.\n\tfor index, keys := range collector.findCols {\n\t\ttranslations, err := e.Cluster.findIndexKeys(ctx, index, keys...)\n\t\tif err != nil {\n\t\t\treturn nil, nil, errors.Wrap(err, \"finding query column keys\")\n\t\t}\n\t\tif prev := cols[index]; prev != nil {\n\t\t\tfor key, id := range translations {\n\t\t\t\tprev[key] = id\n\t\t\t}\n\t\t} else {\n\t\t\tcols[index] = translations\n\t\t}\n\t}\n\tfor index, fields := range collector.findRows {\n\t\tidxRows := rows[index]\n\t\tif idxRows == nil {\n\t\t\tidxRows = make(map[string]map[string]uint64)\n\t\t\trows[index] = idxRows\n\t\t}\n\t\tidx := e.Holder.Index(index)\n\t\tif idx == nil {\n\t\t\treturn nil, nil, errors.Wrapf(ErrIndexNotFound, \"finding rows on index %q\", index)\n\t\t}\n\t\tfor field, keys := range fields {\n\t\t\tf := idx.Field(field)\n\t\t\tif f == nil {\n\t\t\t\treturn nil, nil, errors.Wrapf(ErrFieldNotFound, \"finding rows on field %q in index %q\", field, index)\n\t\t\t}\n\t\t\ttranslations, err := e.Cluster.findFieldKeys(ctx, f, keys...)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, nil, errors.Wrap(err, \"finding query row keys\")\n\t\t\t}\n\t\t\tif prev := idxRows[field]; prev != nil {\n\t\t\t\tfor key, id := range translations {\n\t\t\t\t\tprev[key] = id\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tidxRows[field] = translations\n\t\t\t}\n\t\t}\n\t}\n\n\treturn cols, rows, nil\n}\n\nfunc (e *executor) collectCallKeys(dst *keyCollector, c *pql.Call, index string) error {\n\t// Check for an overriding 'index' argument.\n\t// This also applies to all child calls.\n\tif callIndex := c.CallIndex(); callIndex != \"\" {\n\t\tindex = callIndex\n\t}\n\n\t// Handle the field arg.\n\tswitch c.Name {\n\tcase \"Set\":\n\t\tif field, err := c.FieldArg(); err == nil {\n\t\t\tif arg, ok := c.Args[field].(string); ok {\n\t\t\t\tdst.CreateRows(index, field, arg)\n\t\t\t}\n\t\t}\n\n\tcase \"Store\":\n\t\tif field, err := c.FieldArg(); err == nil {\n\t\t\tidx := e.Holder.Index(index)\n\t\t\tif idx == nil {\n\t\t\t\treturn errors.Wrapf(ErrIndexNotFound, \"translating store field argument\")\n\t\t\t}\n\t\t\tf := idx.Field(field)\n\t\t\tif f == nil {\n\t\t\t\t// Create the field.\n\t\t\t\t// This is messy, because if a query leading up to the store fails, we will have created the field without executing the store.\n\t\t\t\tvar keyed bool\n\t\t\t\tswitch v := c.Args[field].(type) {\n\t\t\t\tcase string:\n\t\t\t\t\tkeyed = true\n\t\t\t\tcase uint64:\n\t\t\t\tcase int64:\n\t\t\t\t\tif v < 0 {\n\t\t\t\t\t\treturn errors.Errorf(\"negative store row ID %d\", v)\n\t\t\t\t\t}\n\t\t\t\tdefault:\n\t\t\t\t\treturn errors.Errorf(\"invalid store row identifier: %v of %T\", v, v)\n\t\t\t\t}\n\t\t\t\topts := []FieldOption{OptFieldTypeSet(CacheTypeNone, 0)}\n\t\t\t\tif keyed {\n\t\t\t\t\topts = append(opts, OptFieldKeys())\n\t\t\t\t}\n\t\t\t\tif _, err := idx.CreateField(field, \"\", opts...); err != nil {\n\t\t\t\t\t// We wrap these because we want to indicate that it wasn't found,\n\t\t\t\t\t// but also the problem we encountered trying to create it.\n\t\t\t\t\treturn newNotFoundError(errors.Wrap(err, \"creating field\"), field)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif arg, ok := c.Args[field].(string); ok {\n\t\t\t\tdst.CreateRows(index, field, arg)\n\t\t\t}\n\t\t}\n\n\tcase \"Clear\", \"Row\", \"Range\", \"ClearRow\":\n\t\tif field, err := c.FieldArg(); err == nil {\n\t\t\tswitch arg := c.Args[field].(type) {\n\t\t\tcase string:\n\t\t\t\tdst.FindRows(index, field, arg)\n\t\t\tcase *pql.Condition:\n\t\t\t\t// This is a workaround to allow `==` and `!=` to work on foreign index fields.\n\t\t\t\tif key, ok := arg.Value.(string); ok {\n\t\t\t\t\tswitch arg.Op {\n\t\t\t\t\tcase pql.EQ, pql.NEQ:\n\t\t\t\t\t\tdst.FindRows(index, field, key)\n\t\t\t\t\tdefault:\n\t\t\t\t\t\treturn errors.Errorf(\"operator %v not defined on strings\", arg.Op)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Handle _col.\n\tif col, ok := c.Args[\"_col\"].(string); ok {\n\t\tswitch c.Name {\n\t\tcase \"Set\":\n\t\t\tdst.CreateColumns(index, col)\n\t\tdefault:\n\t\t\tdst.FindColumns(index, col)\n\t\t}\n\t}\n\n\t// Handle _row.\n\tif row, ok := c.Args[\"_row\"].(string); ok {\n\t\t// Find the field.\n\t\tfield, ok, err := c.StringArg(\"_field\")\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"finding field\")\n\t\t}\n\t\tif !ok {\n\t\t\treturn errors.Wrap(ErrFieldNotFound, \"finding field for _row argument\")\n\t\t}\n\n\t\tdst.FindRows(index, field, row)\n\t}\n\n\t// Handle queries that need a \"column\" argument.\n\tswitch c.Name {\n\tcase \"Rows\", \"GroupBy\", \"FieldValue\", \"IncludesColumn\":\n\t\tif col, ok := c.Args[\"column\"].(string); ok {\n\t\t\tdst.FindColumns(index, col)\n\t\t}\n\t}\n\n\t// Handle special per-query arguments.\n\tswitch c.Name {\n\tcase \"ConstRow\":\n\t\t// Translate the columns list.\n\t\tif cols, ok := c.Args[\"columns\"].([]interface{}); ok {\n\t\t\tkeys := make([]string, 0, len(cols))\n\t\t\tfor _, v := range cols {\n\t\t\t\tswitch v := v.(type) {\n\t\t\t\tcase string:\n\t\t\t\t\tkeys = append(keys, v)\n\t\t\t\tcase uint64:\n\t\t\t\tcase int64:\n\t\t\t\tdefault:\n\t\t\t\t\treturn errors.Errorf(\"invalid column identifier %v of type %T\", c, c)\n\t\t\t\t}\n\t\t\t}\n\t\t\tdst.FindColumns(index, keys...)\n\t\t}\n\n\tcase \"Rows\":\n\t\t// Find the field.\n\t\tvar field string\n\t\tif f, ok, err := c.StringArg(\"_field\"); err != nil {\n\t\t\treturn errors.Wrap(err, \"finding field for Rows previous translation\")\n\t\t} else if ok {\n\t\t\tfield = f\n\t\t} else if f, ok, err := c.StringArg(\"field\"); err != nil {\n\t\t\treturn errors.Wrap(err, \"finding field for Rows previous translation\")\n\t\t} else if ok {\n\t\t\tfield = f\n\t\t} else {\n\t\t\treturn errors.New(\"missing field in Rows call\")\n\t\t}\n\t\tif prev, ok := c.Args[\"previous\"].(string); ok {\n\t\t\tdst.FindRows(index, field, prev)\n\t\t}\n\t\tif in, ok := c.Args[\"in\"]; ok {\n\t\t\tinIn, ok := in.([]interface{})\n\t\t\tif !ok {\n\t\t\t\treturn errors.Errorf(\"unexpected type for argument 'in' %v of %[1]T\", inIn)\n\t\t\t}\n\t\t\tinStrs := make([]string, 0)\n\t\t\tfor _, v := range inIn {\n\t\t\t\tif vstr, ok := v.(string); ok {\n\t\t\t\t\tinStrs = append(inStrs, vstr)\n\t\t\t\t}\n\t\t\t}\n\t\t\tdst.FindRows(index, field, inStrs...)\n\t\t}\n\t}\n\n\t// Collect keys from child calls.\n\tfor _, child := range c.Children {\n\t\terr := e.collectCallKeys(dst, child, index)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Collect keys from argument calls.\n\tfor _, arg := range c.Args {\n\t\targCall, ok := arg.(*pql.Call)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\terr := e.collectCallKeys(dst, argCall, index)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\ntype keyCollector struct {\n\tcreateCols, findCols map[string][]string            // map[index] -> column keys\n\tcreateRows, findRows map[string]map[string][]string // map[index]map[field] -> row keys\n}\n\nfunc (c *keyCollector) CreateColumns(index string, columns ...string) {\n\tif len(columns) == 0 {\n\t\treturn\n\t}\n\tc.createCols[index] = append(c.createCols[index], columns...)\n}\n\nfunc (c *keyCollector) FindColumns(index string, columns ...string) {\n\tif len(columns) == 0 {\n\t\treturn\n\t}\n\tc.findCols[index] = append(c.findCols[index], columns...)\n}\n\nfunc (c *keyCollector) CreateRows(index string, field string, columns ...string) {\n\tif len(columns) == 0 {\n\t\treturn\n\t}\n\tidx := c.createRows[index]\n\tif idx == nil {\n\t\tidx = make(map[string][]string)\n\t\tc.createRows[index] = idx\n\t}\n\tidx[field] = append(idx[field], columns...)\n}\n\nfunc (c *keyCollector) FindRows(index string, field string, columns ...string) {\n\tif len(columns) == 0 {\n\t\treturn\n\t}\n\tidx := c.findRows[index]\n\tif idx == nil {\n\t\tidx = make(map[string][]string)\n\t\tc.findRows[index] = idx\n\t}\n\tidx[field] = append(idx[field], columns...)\n}\n\nfunc fieldValidateValue(f *Field, val interface{}) error {\n\tif val == nil {\n\t\treturn nil\n\t}\n\n\t// Validate special types.\n\tswitch val := val.(type) {\n\tcase string:\n\t\tif !f.Keys() {\n\t\t\treturn errors.Errorf(\"string value on unkeyed field %q\", f.Name())\n\t\t}\n\t\treturn nil\n\tcase *pql.Condition:\n\t\tswitch v := val.Value.(type) {\n\t\tcase nil:\n\t\tcase string:\n\t\tcase uint64:\n\t\tcase int64:\n\t\tcase float64:\n\t\tcase pql.Decimal:\n\t\tcase time.Time:\n\t\tcase []interface{}:\n\t\t\tfor _, v := range v {\n\t\t\t\tif err := fieldValidateValue(f, v); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\tdefault:\n\t\t\treturn errors.Errorf(\"invalid value %v in condition %q\", v, val.String())\n\t\t}\n\t\treturn fieldValidateValue(f, val.Value)\n\t}\n\n\tswitch f.Type() {\n\tcase FieldTypeSet, FieldTypeMutex, FieldTypeTime:\n\t\tswitch v := val.(type) {\n\t\tcase uint64:\n\t\tcase int64:\n\t\t\tif v < 0 {\n\t\t\t\treturn errors.Errorf(\"negative ID %d for set field %q\", v, f.Name())\n\t\t\t}\n\t\tdefault:\n\t\t\treturn errors.Errorf(\"invalid value %v for field %q of type %s\", v, f.Name(), f.Type())\n\t\t}\n\t\tif f.Keys() {\n\t\t\treturn errors.Errorf(\"found integer ID %d on keyed field %q\", val, f.Name())\n\t\t}\n\tcase FieldTypeBool:\n\t\tswitch v := val.(type) {\n\t\tcase bool:\n\t\tdefault:\n\t\t\treturn errors.Errorf(\"invalid value %v for bool field %q\", v, f.Name())\n\t\t}\n\tcase FieldTypeInt:\n\t\tswitch v := val.(type) {\n\t\tcase uint64:\n\t\t\tif v > 1<<63 {\n\t\t\t\treturn errors.Errorf(\"oversized integer %d for int field %q (range: -2^63 to 2^63-1)\", v, f.Name())\n\t\t\t}\n\t\tcase int64:\n\t\tdefault:\n\t\t\treturn errors.Errorf(\"invalid value %v for int field %q\", v, f.Name())\n\t\t}\n\tcase FieldTypeDecimal:\n\t\tswitch v := val.(type) {\n\t\tcase uint64:\n\t\tcase int64:\n\t\tcase float64:\n\t\tcase pql.Decimal:\n\t\tdefault:\n\t\t\treturn errors.Errorf(\"invalid value %v for decimal field %q\", v, f.Name())\n\t\t}\n\tcase FieldTypeTimestamp:\n\t\tswitch v := val.(type) {\n\t\tcase time.Time:\n\t\tcase int64: // TODO(twg) 2022/09/14 revisit this hack\n\t\tdefault:\n\t\t\treturn errors.Errorf(\"invalid value %v for timestamp field %q\", v, f.Name())\n\t\t}\n\tdefault:\n\t\treturn errors.Errorf(\"unsupported type %s of field %q\", f.Type(), f.Name())\n\t}\n\n\treturn nil\n}\n\nfunc (e *executor) translateCall(c *pql.Call, index string, columnKeys map[string]map[string]uint64, rowKeys map[string]map[string]map[string]uint64) (*pql.Call, error) {\n\t// Check for an overriding 'index' argument.\n\t// This also applies to all child calls.\n\tif callIndex := c.CallIndex(); callIndex != \"\" {\n\t\tindex = callIndex\n\t}\n\tidx := e.Holder.Index(index)\n\tif idx == nil {\n\t\treturn nil, errors.Wrapf(ErrIndexNotFound, \"translating query on index %q\", index)\n\t}\n\n\t// Fetch the column keys list for this index.\n\tindexCols, indexRows := columnKeys[index], rowKeys[index]\n\n\t// Handle the field arg.\n\tswitch c.Name {\n\tcase \"Set\", \"Store\":\n\t\tif field, err := c.FieldArg(); err == nil {\n\t\t\tf := e.Holder.Field(index, field)\n\t\t\tif f == nil {\n\t\t\t\treturn nil, errors.Wrapf(ErrFieldNotFound, \"validating value for field %q\", field)\n\t\t\t}\n\t\t\targ := c.Args[field]\n\t\t\tif err := fieldValidateValue(f, arg); err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"validating store value\")\n\t\t\t}\n\t\t\tswitch arg := arg.(type) {\n\t\t\tcase string:\n\t\t\t\tif translation, ok := indexRows[field][arg]; ok {\n\t\t\t\t\tc.Args[field] = translation\n\t\t\t\t} else {\n\t\t\t\t\treturn nil, errors.Wrapf(ErrTranslatingKeyNotFound, \"destination key not found %q in %q in index %q\", arg, field, index)\n\t\t\t\t}\n\t\t\tcase bool:\n\t\t\t\tif arg {\n\t\t\t\t\tc.Args[field] = trueRowID\n\t\t\t\t} else {\n\t\t\t\t\tc.Args[field] = falseRowID\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\tcase \"Clear\", \"Row\", \"Range\", \"ClearRow\":\n\t\tif field, err := c.FieldArg(); err == nil {\n\t\t\tf := e.Holder.Field(index, field)\n\t\t\tif f == nil {\n\t\t\t\treturn nil, errors.Wrapf(ErrFieldNotFound, \"validating value for field %q\", field)\n\t\t\t}\n\t\t\targ := c.Args[field]\n\t\t\tif err := fieldValidateValue(f, arg); err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"validating field parameter value\")\n\t\t\t}\n\t\t\tif c.Name == \"Row\" {\n\t\t\t\tswitch f.Type() {\n\t\t\t\tcase FieldTypeInt, FieldTypeDecimal, FieldTypeTimestamp:\n\t\t\t\t\tif _, ok := arg.(*pql.Condition); !ok {\n\t\t\t\t\t\t// This is workaround to support pql.ASSIGN ('=') as condition ('==') for BSI fields.\n\t\t\t\t\t\targ = &pql.Condition{\n\t\t\t\t\t\t\tOp:    pql.EQ,\n\t\t\t\t\t\t\tValue: arg,\n\t\t\t\t\t\t}\n\t\t\t\t\t\tc.Args[field] = arg\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tswitch arg := arg.(type) {\n\t\t\tcase string:\n\t\t\t\tif translation, ok := indexRows[field][arg]; ok {\n\t\t\t\t\tc.Args[field] = translation\n\t\t\t\t} else {\n\t\t\t\t\t// Rewrite the call into a zero value call.\n\t\t\t\t\treturn e.callZero(c), nil\n\t\t\t\t}\n\t\t\tcase bool:\n\t\t\t\tif arg {\n\t\t\t\t\tc.Args[field] = trueRowID\n\t\t\t\t} else {\n\t\t\t\t\tc.Args[field] = falseRowID\n\t\t\t\t}\n\t\t\tcase *pql.Condition:\n\t\t\t\t// This is a workaround to allow `==` and `!=` to work on foreign index fields.\n\t\t\t\tif key, ok := arg.Value.(string); ok {\n\t\t\t\t\tswitch arg.Op {\n\t\t\t\t\tcase pql.EQ, pql.NEQ:\n\t\t\t\t\t\tif translation, ok := indexRows[field][key]; ok {\n\t\t\t\t\t\t\targ.Value = translation\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t// Rewrite the call into a zero value call.\n\t\t\t\t\t\t\treturn e.callZero(c), nil\n\t\t\t\t\t\t}\n\t\t\t\t\tdefault:\n\t\t\t\t\t\treturn nil, errors.Errorf(\"operator %v not defined on strings\", arg.Op)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Handle _col.\n\tif col, ok := c.Args[\"_col\"].(string); ok {\n\t\tif !idx.Keys() {\n\t\t\treturn nil, errors.Wrapf(ErrTranslatingKeyNotFound, \"translating column on unkeyed index %q\", index)\n\t\t}\n\t\tif id, ok := indexCols[col]; ok {\n\t\t\tc.Args[\"_col\"] = id\n\t\t} else {\n\t\t\tswitch c.Name {\n\t\t\tcase \"Set\":\n\t\t\t\treturn nil, errors.Wrapf(ErrTranslatingKeyNotFound, \"destination key not found %q in index %q\", col, index)\n\t\t\tdefault:\n\t\t\t\treturn e.callZero(c), nil\n\t\t\t}\n\t\t}\n\t}\n\n\t// Handle _row.\n\tif row, ok := c.Args[\"_row\"]; ok {\n\t\t// Find the field.\n\t\tvar field string\n\t\tif f, ok, err := c.StringArg(\"_field\"); err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"finding field\")\n\t\t} else if ok {\n\t\t\tfield = f\n\t\t} else if f, ok, err := c.StringArg(\"field\"); err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"finding field\")\n\t\t} else if ok {\n\t\t\tfield = f\n\t\t} else {\n\t\t\treturn nil, errors.New(\"missing field\")\n\t\t}\n\n\t\tf := e.Holder.Field(index, field)\n\t\tif f == nil {\n\t\t\treturn nil, errors.Wrapf(ErrFieldNotFound, \"validating value for field %q\", field)\n\t\t}\n\t\tif err := fieldValidateValue(f, row); err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"validating row value\")\n\t\t}\n\t\tswitch row := row.(type) {\n\t\tcase string:\n\t\t\tif translation, ok := indexRows[field][row]; ok {\n\t\t\t\tc.Args[\"_row\"] = translation\n\t\t\t} else {\n\t\t\t\treturn e.callZero(c), nil\n\t\t\t}\n\t\t}\n\t}\n\n\t// Handle queries that need a \"column\" argument.\n\tswitch c.Name {\n\tcase \"Rows\", \"GroupBy\", \"FieldValue\", \"IncludesColumn\":\n\t\tif col, ok := c.Args[\"column\"].(string); ok {\n\t\t\tif translation, ok := indexCols[col]; ok {\n\t\t\t\tc.Args[\"column\"] = translation\n\t\t\t} else {\n\t\t\t\t// Rewrite the call into a zero value call.\n\t\t\t\treturn e.callZero(c), nil\n\t\t\t}\n\t\t}\n\t}\n\n\t// Handle special per-query arguments.\n\tswitch c.Name {\n\tcase \"ConstRow\":\n\t\t// Translate the columns list.\n\t\tif cols, ok := c.Args[\"columns\"].([]interface{}); ok {\n\t\t\tout := make([]uint64, 0, len(cols))\n\t\t\tfor _, v := range cols {\n\t\t\t\tswitch v := v.(type) {\n\t\t\t\tcase string:\n\t\t\t\t\tif id, ok := indexCols[v]; ok {\n\t\t\t\t\t\tout = append(out, id)\n\t\t\t\t\t}\n\t\t\t\tcase uint64:\n\t\t\t\t\tout = append(out, v)\n\t\t\t\tcase int64:\n\t\t\t\t\tout = append(out, uint64(v))\n\t\t\t\tdefault:\n\t\t\t\t\treturn nil, errors.Errorf(\"invalid column identifier %v of type %T\", c, c)\n\t\t\t\t}\n\t\t\t}\n\t\t\tc.Args[\"columns\"] = out\n\t\t}\n\n\tcase \"Rows\":\n\t\t// Find the field.\n\t\tvar field string\n\t\tif f, ok, err := c.StringArg(\"_field\"); err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"finding field for Rows previous translation\")\n\t\t} else if ok {\n\t\t\tfield = f\n\t\t} else if f, ok, err := c.StringArg(\"field\"); err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"finding field for Rows previous translation\")\n\t\t} else if ok {\n\t\t\tfield = f\n\t\t} else {\n\t\t\treturn nil, errors.New(\"missing field in Rows call\")\n\t\t}\n\t\t// Translate the previous row key.\n\t\tif prev, ok := c.Args[\"previous\"]; ok {\n\t\t\t// Validate the type.\n\t\t\tf := e.Holder.Field(index, field)\n\t\t\tif f == nil {\n\t\t\t\treturn nil, errors.Wrapf(ErrFieldNotFound, \"validating value for field %q\", field)\n\t\t\t}\n\t\t\tif err := fieldValidateValue(f, prev); err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"validating prev value\")\n\t\t\t}\n\n\t\t\tswitch prev := prev.(type) {\n\t\t\tcase string:\n\t\t\t\t// Look up a translation for the previous row key.\n\t\t\t\tif translation, ok := indexRows[field][prev]; ok {\n\t\t\t\t\tc.Args[\"previous\"] = translation\n\t\t\t\t} else {\n\t\t\t\t\treturn nil, errors.Wrapf(ErrTranslatingKeyNotFound, \"translating previous key %q from field %q in index %q in Rows call\", prev, field, index)\n\t\t\t\t}\n\t\t\tcase bool:\n\t\t\t\tif prev {\n\t\t\t\t\tc.Args[\"previous\"] = trueRowID\n\t\t\t\t} else {\n\t\t\t\t\tc.Args[\"previous\"] = falseRowID\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Check if \"like\" argument is applied to keyed fields.\n\t\tif _, found := c.Args[\"like\"].(string); found {\n\t\t\tfieldName, err := c.FirstStringArg(\"_field\", \"field\")\n\t\t\tif err != nil || fieldName == \"\" {\n\t\t\t\treturn nil, fmt.Errorf(\"cannot read field name for Rows call\")\n\t\t\t}\n\t\t\tif !idx.Field(fieldName).options.Keys {\n\t\t\t\treturn nil, fmt.Errorf(\"'%s' is not a set/mutex/time field with a string key\", fieldName)\n\t\t\t}\n\t\t}\n\n\t\tif in, ok := c.Args[\"in\"]; ok {\n\t\t\tinIn, ok := in.([]interface{})\n\t\t\tif !ok {\n\t\t\t\treturn nil, errors.Errorf(\"unexpected type for argument 'in' %v of %[1]T\", in)\n\t\t\t}\n\t\t\tinIDs := make([]interface{}, 0, len(inIn))\n\t\t\tfor _, inVal := range inIn {\n\t\t\t\tif inStr, ok := inVal.(string); ok {\n\t\t\t\t\tid, found := rowKeys[index][field][inStr]\n\t\t\t\t\tif found {\n\t\t\t\t\t\tinIDs = append(inIDs, id)\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tinIDs = append(inIDs, inVal)\n\t\t\t\t}\n\t\t\t}\n\t\t\tc.Args[\"in\"] = inIDs\n\t\t}\n\t}\n\n\t// Translate child calls.\n\tfor i, child := range c.Children {\n\t\ttranslated, err := e.translateCall(child, index, columnKeys, rowKeys)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tc.Children[i] = translated\n\t}\n\n\t// Translate argument calls.\n\tfor k, arg := range c.Args {\n\t\targCall, ok := arg.(*pql.Call)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\ttranslated, err := e.translateCall(argCall, index, columnKeys, rowKeys)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tc.Args[k] = translated\n\t}\n\n\treturn c, nil\n}\n\nfunc (e *executor) callZero(c *pql.Call) *pql.Call {\n\tswitch c.Name {\n\tcase \"Row\", \"Range\":\n\t\tif field, err := c.FieldArg(); err == nil {\n\t\t\tif cond, ok := c.Args[field].(*pql.Condition); ok {\n\t\t\t\tif cond.Op == pql.NEQ {\n\t\t\t\t\t// Turn not nothing into everything.\n\t\t\t\t\treturn &pql.Call{Name: \"All\"}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Use an empty union as a placeholder.\n\t\treturn &pql.Call{Name: \"Union\"}\n\n\tdefault:\n\t\treturn nil\n\t}\n}\n\nfunc (e *executor) translateResults(ctx context.Context, index string, idx *Index, calls []*pql.Call, results []interface{}, memoryAvailable int64) (err error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"executor.translateResults\")\n\tdefer span.Finish()\n\n\tidMap := make(map[uint64]string)\n\tif idx.Keys() {\n\t\t// Collect all index ids.\n\t\tidSet := make(map[uint64]struct{})\n\t\tfor i := range calls {\n\t\t\tif err := e.collectResultIDs(index, idx, calls[i], results[i], idSet); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif idMap, err = e.Cluster.translateIndexIDSet(ctx, index, idSet); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tfor i := range results {\n\t\tresults[i], err = e.translateResult(ctx, index, idx, calls[i], results[i], idMap, &memoryAvailable)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// translationStrategy denotes the several different ways the bits in\n// a *Row could be translated to string keys.\ntype translationStrategy int\n\nconst (\n\t// byCurrentIndex means to interpret the bits as IDs in \"top\n\t// level\" index for this query (e.g. the index specified in the\n\t// path of the HTTP request).\n\tbyCurrentIndex translationStrategy = iota + 1\n\t// byRowField means that the bits in this *Row are row IDs which\n\t// should be translated using the field's (*Row.Field) translation store.\n\tbyRowField\n\t// byRowFieldForeignIndex means that the bits in this *Row should\n\t// be interpreted as IDs in the foreign index of the *Row.Field.\n\tbyRowFieldForeignIndex\n\t// byRowIndex means the bits in this *Row should be translated\n\t// according to the index named by *Row.Index\n\tbyRowIndex\n\t// noTranslation means the bits should not be translated to string\n\t// keys.\n\tnoTranslation\n)\n\n// howToTranslate determines how a *Row object's bits should be\n// translated to keys (if at all). There are several different options\n// detailed by the various const values of translationStrategy. In\n// order to do this it has to figure out the row's index and field\n// which it also returns as the caller may need them to actually\n// execute the translation or do whatever else it's doing with the\n// translationStrategy information.\nfunc (e *executor) howToTranslate(idx *Index, row *Row) (rowIdx *Index, rowField *Field, strat translationStrategy, err error) {\n\t// First get the index and field the row specifies (if any).\n\trowIdx = idx\n\tif row.Index != \"\" && row.Index != idx.Name() {\n\t\trowIdx = e.Holder.Index(row.Index)\n\t\tif rowIdx == nil {\n\t\t\treturn nil, nil, 0, errors.Errorf(\"got a row with unknown index: %s\", row.Index)\n\t\t}\n\t}\n\tif row.Field != \"\" {\n\t\trowField = rowIdx.Field(row.Field)\n\t\tif rowField == nil {\n\t\t\treturn nil, nil, 0, errors.Errorf(\"got a row with unknown index/field %s/%s\", idx.Name(), row.Field)\n\t\t}\n\t}\n\n\t// Handle the case where the Row has specified a field.\n\tif rowField != nil {\n\t\t// Handle the case where field has a foreign index.\n\t\tif rowField.ForeignIndex() != \"\" {\n\t\t\tfidx := e.Holder.Index(rowField.ForeignIndex())\n\t\t\tif fidx == nil {\n\t\t\t\treturn nil, nil, 0, errors.Errorf(\"foreign index %s not found for field %s in index %s\", rowField.ForeignIndex(), rowField.Name(), rowField.Index())\n\t\t\t}\n\t\t\tif fidx.Keys() {\n\t\t\t\treturn rowIdx, rowField, byRowFieldForeignIndex, nil\n\t\t\t}\n\t\t} else if rowField.Keys() {\n\t\t\treturn rowIdx, rowField, byRowField, nil\n\t\t}\n\t\treturn rowIdx, rowField, noTranslation, nil\n\t}\n\n\t// In this case, the row has specified an index, but not a field,\n\t// so we translate according to that index.\n\tif rowIdx != idx && rowIdx.Keys() {\n\t\treturn rowIdx, rowField, byRowIndex, nil\n\t}\n\n\t// Handle the normal case (row represents a set of records in\n\t// the top level index, Row has not specifed a different index\n\t// or field).\n\tif rowIdx == idx && idx.Keys() && rowField == nil {\n\t\treturn rowIdx, rowField, byCurrentIndex, nil\n\t}\n\treturn rowIdx, rowField, noTranslation, nil\n}\n\nfunc (e *executor) collectResultIDs(index string, idx *Index, call *pql.Call, result interface{}, idSet map[uint64]struct{}) error {\n\tswitch result := result.(type) {\n\tcase *Row:\n\t\t// Only collect result IDs if they are in the current index.\n\t\t_, _, strategy, err := e.howToTranslate(idx, result)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"determining how to translate\")\n\t\t}\n\t\tif strategy == byCurrentIndex {\n\t\t\tfor _, segment := range result.Segments {\n\t\t\t\tfor _, col := range segment.Columns() {\n\t\t\t\t\tidSet[col] = struct{}{}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\tcase ExtractedIDMatrix:\n\t\tfor _, col := range result.Columns {\n\t\t\tidSet[col.ColumnID] = struct{}{}\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// preTranslateMatrixSet translates the IDs of a set field in an extracted matrix.\nfunc (e *executor) preTranslateMatrixSet(ctx context.Context, mat ExtractedIDMatrix, fieldIdx uint, field *Field) (map[uint64]string, error) {\n\tids := make(map[uint64]struct{}, len(mat.Columns))\n\tfor _, col := range mat.Columns {\n\t\tfor _, v := range col.Rows[fieldIdx] {\n\t\t\tids[v] = struct{}{}\n\t\t}\n\t}\n\n\treturn e.Cluster.translateFieldIDs(ctx, field, ids)\n}\n\nfunc (e *executor) translateResult(ctx context.Context, index string, idx *Index, call *pql.Call, result interface{}, idSet map[uint64]string, memoryAvailable *int64) (_ interface{}, err error) {\n\tswitch result := result.(type) {\n\tcase *Row:\n\t\trowIdx, rowField, strategy, err := e.howToTranslate(idx, result)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"determining translation strategy\")\n\t\t}\n\t\tswitch strategy {\n\t\tcase byCurrentIndex:\n\t\t\tother := &Row{}\n\t\t\tfor _, segment := range result.Segments {\n\t\t\t\tfor _, col := range segment.Columns() {\n\t\t\t\t\tother.Keys = append(other.Keys, idSet[col])\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn other, nil\n\t\tcase byRowField:\n\t\t\tkeys, err := e.Cluster.translateFieldListIDs(ctx, rowField, result.Columns())\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"translating Row to field keys\")\n\t\t\t}\n\t\t\tresult.Keys = keys\n\t\tcase byRowFieldForeignIndex:\n\t\t\tidx = e.Holder.Index(rowField.ForeignIndex())\n\t\t\tif idx == nil {\n\t\t\t\treturn nil, errors.Errorf(\"foreign index %s not found for field %s in index %s\", rowField.ForeignIndex(), rowField.Name(), rowField.Index())\n\t\t\t}\n\t\t\tfor _, segment := range result.Segments {\n\t\t\t\tkeys, err := e.Cluster.translateIndexIDs(context.Background(), rowField.ForeignIndex(), segment.Columns())\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, errors.Wrap(err, \"translating index ids\")\n\t\t\t\t}\n\t\t\t\tresult.Keys = append(result.Keys, keys...)\n\t\t\t}\n\n\t\tcase byRowIndex:\n\t\t\tfor _, segment := range result.Segments {\n\t\t\t\tkeys, err := e.Cluster.translateIndexIDs(context.Background(), rowIdx.Name(), segment.Columns())\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, errors.Wrap(err, \"translating index ids\")\n\t\t\t\t}\n\t\t\t\tresult.Keys = append(result.Keys, keys...)\n\t\t\t}\n\t\t\treturn result, nil\n\n\t\tcase noTranslation:\n\t\t\treturn result, nil\n\t\tdefault:\n\t\t\treturn nil, errors.Errorf(\"unknown translation strategy %d\", strategy)\n\t\t}\n\tcase SignedRow:\n\t\tsr, err := func() (*SignedRow, error) {\n\t\t\tfieldName := callArgString(call, \"field\")\n\t\t\tif fieldName == \"\" {\n\t\t\t\treturn nil, nil\n\t\t\t}\n\n\t\t\tfield := idx.Field(fieldName)\n\t\t\tif field == nil {\n\t\t\t\treturn nil, nil\n\t\t\t}\n\n\t\t\tif field.Keys() {\n\t\t\t\trslt := result.Pos\n\t\t\t\tif rslt == nil {\n\t\t\t\t\treturn &SignedRow{Pos: &Row{}}, nil\n\t\t\t\t}\n\t\t\t\tother := &Row{}\n\t\t\t\tfor _, segment := range rslt.Segments {\n\t\t\t\t\tkeys, err := e.Cluster.translateIndexIDs(context.Background(), field.ForeignIndex(), segment.Columns())\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn nil, errors.Wrap(err, \"translating index ids\")\n\t\t\t\t\t}\n\t\t\t\t\tother.Keys = append(other.Keys, keys...)\n\t\t\t\t}\n\t\t\t\treturn &SignedRow{Pos: other}, nil\n\t\t\t}\n\n\t\t\treturn nil, nil\n\t\t}()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t} else if sr != nil {\n\t\t\treturn *sr, nil\n\t\t}\n\n\tcase PairField:\n\t\tif fieldName := callArgString(call, \"field\"); fieldName != \"\" {\n\t\t\tfield := idx.Field(fieldName)\n\t\t\tif field == nil {\n\t\t\t\treturn nil, fmt.Errorf(\"field %q not found\", fieldName)\n\t\t\t}\n\t\t\tif field.Keys() {\n\t\t\t\tkey, err := field.TranslateStore().TranslateID(result.Pair.ID)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tif call.Name == \"MinRow\" || call.Name == \"MaxRow\" {\n\t\t\t\t\tresult.Pair.Key = key\n\t\t\t\t\treturn result, nil\n\t\t\t\t}\n\t\t\t\treturn PairField{\n\t\t\t\t\tPair:  Pair{Key: key, Count: result.Pair.Count},\n\t\t\t\t\tField: fieldName,\n\t\t\t\t}, nil\n\t\t\t}\n\t\t}\n\n\tcase *PairsField:\n\t\tif fieldName := callArgString(call, \"_field\"); fieldName != \"\" {\n\t\t\tfield := idx.Field(fieldName)\n\t\t\tif field == nil {\n\t\t\t\treturn nil, fmt.Errorf(\"field %q not found\", fieldName)\n\t\t\t}\n\t\t\tif field.Keys() {\n\t\t\t\tids := make([]uint64, len(result.Pairs))\n\t\t\t\tfor i := range result.Pairs {\n\t\t\t\t\tids[i] = result.Pairs[i].ID\n\t\t\t\t}\n\t\t\t\tkeys, err := e.Cluster.translateFieldListIDs(ctx, field, ids)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tother := make([]Pair, len(result.Pairs))\n\t\t\t\tfor i := range result.Pairs {\n\t\t\t\t\tother[i] = Pair{Key: keys[i], Count: result.Pairs[i].Count}\n\t\t\t\t}\n\t\t\t\treturn &PairsField{\n\t\t\t\t\tPairs: other,\n\t\t\t\t\tField: fieldName,\n\t\t\t\t}, nil\n\t\t\t}\n\t\t}\n\n\tcase *GroupCounts:\n\t\tfieldIDs := make(map[*Field]map[uint64]struct{})\n\t\tforeignIDs := make(map[*Field]map[uint64]struct{})\n\t\tgroups := result.Groups()\n\t\tfor _, gl := range groups {\n\t\t\tfor _, g := range gl.Group {\n\t\t\t\tfield := idx.Field(g.Field)\n\t\t\t\tif field == nil {\n\t\t\t\t\treturn nil, newNotFoundError(ErrFieldNotFound, g.Field)\n\t\t\t\t}\n\t\t\t\tif field.Keys() {\n\t\t\t\t\tif g.Value != nil {\n\t\t\t\t\t\tif fi := field.ForeignIndex(); fi != \"\" {\n\t\t\t\t\t\t\tm, ok := foreignIDs[field]\n\t\t\t\t\t\t\tif !ok {\n\t\t\t\t\t\t\t\tm = make(map[uint64]struct{}, len(groups))\n\t\t\t\t\t\t\t\tforeignIDs[field] = m\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tm[uint64(*g.Value)] = struct{}{}\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tm, ok := fieldIDs[field]\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\tm = make(map[uint64]struct{}, len(groups))\n\t\t\t\t\t\tfieldIDs[field] = m\n\t\t\t\t\t}\n\n\t\t\t\t\tm[g.RowID] = struct{}{}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tfieldTranslations := make(map[string]map[uint64]string)\n\t\tfor field, ids := range fieldIDs {\n\t\t\ttrans, err := e.Cluster.translateFieldIDs(ctx, field, ids)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrapf(err, \"translating IDs in field %q\", field.Name())\n\t\t\t}\n\t\t\tfieldTranslations[field.Name()] = trans\n\t\t}\n\n\t\tforeignTranslations := make(map[string]map[uint64]string)\n\t\tfor field, ids := range foreignIDs {\n\t\t\ttrans, err := e.Cluster.translateIndexIDSet(ctx, field.ForeignIndex(), ids)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrapf(err, \"translating foreign IDs from index %q\", field.ForeignIndex())\n\t\t\t}\n\t\t\tforeignTranslations[field.Name()] = trans\n\t\t}\n\n\t\t// We are reluctant to smash result, and I'm not sure we need\n\t\t// to be but I'm not sure we don't need to be.\n\t\tnewGroups := make([]GroupCount, len(groups))\n\t\tcopy(newGroups, groups)\n\t\tfor gi, gl := range groups {\n\n\t\t\tgroup := make([]FieldRow, len(gl.Group))\n\t\t\tfor i, g := range gl.Group {\n\t\t\t\tif ft, ok := fieldTranslations[g.Field]; ok {\n\t\t\t\t\tg.RowKey = ft[g.RowID]\n\t\t\t\t} else if ft, ok := foreignTranslations[g.Field]; ok && g.Value != nil {\n\t\t\t\t\tg.RowKey = ft[uint64(*g.Value)]\n\t\t\t\t\tg.Value = nil\n\t\t\t\t}\n\n\t\t\t\tgroup[i] = g\n\t\t\t}\n\t\t\t// Replace with translated group.\n\t\t\tnewGroups[gi].Group = group\n\t\t}\n\t\tother := &GroupCounts{}\n\t\tif result != nil {\n\t\t\tother.aggregateType = result.aggregateType\n\t\t}\n\t\tother.groups = newGroups\n\t\treturn other, nil\n\tcase RowIDs:\n\t\tfieldName := callArgString(call, \"_field\")\n\t\tif fieldName == \"\" {\n\t\t\treturn nil, ErrFieldNotFound\n\t\t}\n\n\t\tother := RowIdentifiers{\n\t\t\tField: fieldName,\n\t\t}\n\n\t\tif field := idx.Field(fieldName); field == nil {\n\t\t\treturn nil, newNotFoundError(ErrFieldNotFound, fieldName)\n\t\t} else if field.Keys() {\n\t\t\tkeys, err := e.Cluster.translateFieldListIDs(ctx, field, result)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"translating row IDs\")\n\t\t\t}\n\t\t\tother.Keys = keys\n\t\t} else {\n\t\t\tother.Rows = result\n\t\t}\n\n\t\treturn other, nil\n\n\tcase ExtractedIDMatrix:\n\t\ttype fieldMapper = func([]uint64) (_ interface{}, err error)\n\n\t\tfields := make([]ExtractedTableField, len(result.Fields))\n\t\tmappers := make([]fieldMapper, len(result.Fields))\n\t\tfor i, v := range result.Fields {\n\t\t\tfield := idx.Field(v)\n\t\t\tif field == nil {\n\t\t\t\treturn nil, newNotFoundError(ErrFieldNotFound, v)\n\t\t\t}\n\n\t\t\tvar mapper fieldMapper\n\t\t\tvar datatype string\n\t\t\tswitch typ := field.Type(); typ {\n\t\t\tcase FieldTypeBool:\n\t\t\t\tdatatype = \"bool\"\n\t\t\t\tmapper = func(ids []uint64) (_ interface{}, err error) {\n\t\t\t\t\tswitch len(ids) {\n\t\t\t\t\tcase 0:\n\t\t\t\t\t\treturn nil, nil\n\t\t\t\t\tcase 1:\n\t\t\t\t\t\tswitch ids[0] {\n\t\t\t\t\t\tcase 0:\n\t\t\t\t\t\t\treturn false, nil\n\t\t\t\t\t\tcase 1:\n\t\t\t\t\t\t\treturn true, nil\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\treturn nil, errors.Errorf(\"invalid ID for boolean %q: %d\", field.Name(), ids[0])\n\t\t\t\t\t\t}\n\t\t\t\t\tdefault:\n\t\t\t\t\t\treturn nil, errors.Errorf(\"boolean %q has too many values: %v\", field.Name(), ids)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tcase FieldTypeSet, FieldTypeTime:\n\t\t\t\tif field.Keys() {\n\t\t\t\t\tdatatype = \"[]string\"\n\t\t\t\t\ttranslations, err := e.preTranslateMatrixSet(ctx, result, uint(i), field)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn nil, errors.Wrapf(err, \"translating IDs of field %q\", v)\n\t\t\t\t\t}\n\t\t\t\t\tmapper = func(ids []uint64) (interface{}, error) {\n\t\t\t\t\t\tif ids == nil {\n\t\t\t\t\t\t\treturn []string(nil), nil\n\t\t\t\t\t\t}\n\t\t\t\t\t\tkeys := make([]string, len(ids))\n\t\t\t\t\t\tfor i, id := range ids {\n\t\t\t\t\t\t\tkeys[i] = translations[id]\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn keys, nil\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tdatatype = \"[]uint64\"\n\t\t\t\t\tmapper = func(ids []uint64) (interface{}, error) {\n\t\t\t\t\t\treturn ids, nil\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tcase FieldTypeMutex:\n\t\t\t\tif field.Keys() {\n\t\t\t\t\tdatatype = \"string\"\n\t\t\t\t\ttranslations, err := e.preTranslateMatrixSet(ctx, result, uint(i), field)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn nil, errors.Wrapf(err, \"translating IDs of field %q\", v)\n\t\t\t\t\t}\n\t\t\t\t\tmapper = func(ids []uint64) (interface{}, error) {\n\t\t\t\t\t\tswitch len(ids) {\n\t\t\t\t\t\tcase 0:\n\t\t\t\t\t\t\treturn nil, nil\n\t\t\t\t\t\tcase 1:\n\t\t\t\t\t\t\treturn translations[ids[0]], nil\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\treturn nil, errors.Errorf(\"mutex %q has too many values: %v\", field.Name(), ids)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tdatatype = \"uint64\"\n\t\t\t\t\tmapper = func(ids []uint64) (_ interface{}, err error) {\n\t\t\t\t\t\tswitch len(ids) {\n\t\t\t\t\t\tcase 0:\n\t\t\t\t\t\t\treturn nil, nil\n\t\t\t\t\t\tcase 1:\n\t\t\t\t\t\t\treturn ids[0], nil\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\treturn nil, errors.Errorf(\"mutex %q has too many values: %v\", field.Name(), ids)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tcase FieldTypeInt:\n\t\t\t\tif fi := field.ForeignIndex(); fi != \"\" {\n\t\t\t\t\tif field.Keys() {\n\t\t\t\t\t\tdatatype = \"string\"\n\t\t\t\t\t\tids := make(map[uint64]struct{}, len(result.Columns))\n\t\t\t\t\t\tfor _, col := range result.Columns {\n\t\t\t\t\t\t\tfor _, v := range col.Rows[i] {\n\t\t\t\t\t\t\t\tids[v] = struct{}{}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\ttrans, err := e.Cluster.translateIndexIDSet(ctx, field.ForeignIndex(), ids)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn nil, errors.Wrapf(err, \"translating foreign IDs from index %q\", field.ForeignIndex())\n\t\t\t\t\t\t}\n\t\t\t\t\t\tmapper = func(ids []uint64) (interface{}, error) {\n\t\t\t\t\t\t\tswitch len(ids) {\n\t\t\t\t\t\t\tcase 0:\n\t\t\t\t\t\t\t\treturn nil, nil\n\t\t\t\t\t\t\tcase 1:\n\t\t\t\t\t\t\t\treturn trans[ids[0]], nil\n\t\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\t\treturn nil, errors.Errorf(\"BSI field %q has too many values: %v\", field.Name(), ids)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tdatatype = \"uint64\"\n\t\t\t\t\t\tmapper = func(ids []uint64) (interface{}, error) {\n\t\t\t\t\t\t\tswitch len(ids) {\n\t\t\t\t\t\t\tcase 0:\n\t\t\t\t\t\t\t\treturn nil, nil\n\t\t\t\t\t\t\tcase 1:\n\t\t\t\t\t\t\t\treturn ids[0], nil\n\t\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\t\treturn nil, errors.Errorf(\"BSI field %q has too many values: %v\", field.Name(), ids)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tdatatype = \"int64\"\n\t\t\t\t\tmapper = func(ids []uint64) (interface{}, error) {\n\t\t\t\t\t\tswitch len(ids) {\n\t\t\t\t\t\tcase 0:\n\t\t\t\t\t\t\treturn nil, nil\n\t\t\t\t\t\tcase 1:\n\t\t\t\t\t\t\treturn int64(ids[0]), nil\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\treturn nil, errors.Errorf(\"BSI field %q has too many values: %v\", field.Name(), ids)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tcase FieldTypeDecimal:\n\t\t\t\tdatatype = \"decimal\"\n\t\t\t\tscale := field.Options().Scale\n\t\t\t\tmapper = func(ids []uint64) (_ interface{}, err error) {\n\t\t\t\t\tswitch len(ids) {\n\t\t\t\t\tcase 0:\n\t\t\t\t\t\treturn nil, nil\n\t\t\t\t\tcase 1:\n\t\t\t\t\t\treturn pql.NewDecimal(int64(ids[0]), scale), nil\n\t\t\t\t\tdefault:\n\t\t\t\t\t\treturn nil, errors.Errorf(\"BSI field %q has too many values: %v\", field.Name(), ids)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tcase FieldTypeTimestamp:\n\t\t\t\tdatatype = \"timestamp\"\n\t\t\t\tunit := field.Options().TimeUnit\n\t\t\t\tmapper = func(ids []uint64) (_ interface{}, err error) {\n\t\t\t\t\tswitch len(ids) {\n\t\t\t\t\tcase 0:\n\t\t\t\t\t\treturn nil, nil\n\t\t\t\t\tcase 1:\n\t\t\t\t\t\tts, err := ValToTimestamp(unit, int64(ids[0]))\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn nil, err\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn ts, nil\n\t\t\t\t\tdefault:\n\t\t\t\t\t\treturn nil, errors.Errorf(\"BSI field %q has too many values: %v\", field.Name(), ids)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\treturn nil, errors.Errorf(\"field type %q not yet supported\", typ)\n\t\t\t}\n\t\t\tmappers[i] = mapper\n\t\t\tfields[i] = ExtractedTableField{\n\t\t\t\tName: v,\n\t\t\t\tType: datatype,\n\t\t\t}\n\t\t}\n\n\t\tvar translateCol func(uint64) (KeyOrID, error)\n\t\tif idx.keys {\n\t\t\ttranslateCol = func(id uint64) (KeyOrID, error) {\n\t\t\t\treturn KeyOrID{Keyed: true, Key: idSet[id]}, nil\n\t\t\t}\n\t\t} else {\n\t\t\ttranslateCol = func(id uint64) (KeyOrID, error) {\n\t\t\t\treturn KeyOrID{ID: id}, nil\n\t\t\t}\n\t\t}\n\n\t\tcols := make([]ExtractedTableColumn, len(result.Columns))\n\t\tcolData := make([]interface{}, len(cols)*len(result.Fields))\n\t\tfor i, col := range result.Columns {\n\t\t\tdata := colData[i*len(result.Fields) : (i+1)*len(result.Fields) : (i+1)*len(result.Fields)]\n\t\t\tfor j, rows := range col.Rows {\n\t\t\t\tv, err := mappers[j](rows)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, errors.Wrap(err, \"translating extracted table value\")\n\t\t\t\t}\n\t\t\t\tdata[j] = v\n\t\t\t}\n\n\t\t\tcolTrans, err := translateCol(col.ColumnID)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"translating column ID in extracted table\")\n\t\t\t}\n\n\t\t\tcols[i] = ExtractedTableColumn{\n\t\t\t\tColumn: colTrans,\n\t\t\t\tRows:   data,\n\t\t\t}\n\t\t\tif *memoryAvailable -= calcResultMemory(cols[i]); *memoryAvailable < 0 {\n\t\t\t\treturn nil, fmt.Errorf(\"table exceeds available memory\")\n\t\t\t}\n\t\t}\n\n\t\treturn ExtractedTable{\n\t\t\tFields:  fields,\n\t\t\tColumns: cols,\n\t\t}, nil\n\t}\n\n\treturn result, nil\n}\n\n// ValToTimestamp takes a timeunit and an integer value and converts it to time.Time\nfunc ValToTimestamp(unit string, val int64) (time.Time, error) {\n\tswitch unit {\n\tcase TimeUnitSeconds:\n\t\treturn time.Unix(val, 0).UTC(), nil\n\tcase TimeUnitMilliseconds:\n\t\treturn time.UnixMilli(val).UTC(), nil\n\tcase TimeUnitMicroseconds, TimeUnitUSeconds:\n\t\treturn time.UnixMicro(val).UTC(), nil\n\tcase TimeUnitNanoseconds:\n\t\treturn time.Unix(0, val).UTC(), nil\n\tdefault:\n\t\treturn time.Time{}, errors.Errorf(\"Unknown time unit: '%v'\", unit)\n\t}\n}\n\n// TimestampToVal takes a time unit and a time.Time and converts it to an integer value\nfunc TimestampToVal(unit string, ts time.Time) int64 {\n\tswitch unit {\n\tcase TimeUnitSeconds:\n\t\treturn ts.Unix()\n\tcase TimeUnitMilliseconds:\n\t\treturn ts.UnixMilli()\n\tcase TimeUnitMicroseconds, TimeUnitUSeconds:\n\t\treturn ts.UnixMicro()\n\tcase TimeUnitNanoseconds:\n\t\treturn ts.UnixNano()\n\t}\n\treturn 0\n}\n\n// detectRangeCall returns true if the call or one of its children contains a Range call\n// TODO: Remove at version 2.0\nfunc (e *executor) detectRangeCall(c *pql.Call) bool {\n\t// detect whether there is a Range call\n\tif c.Name == \"Range\" {\n\t\treturn true\n\t}\n\tfor _, c := range c.Children {\n\t\tif e.detectRangeCall(c) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// validateQueryContext returns a query-appropriate error if the context is done.\nfunc validateQueryContext(ctx context.Context) error {\n\tselect {\n\tcase <-ctx.Done():\n\t\tswitch err := ctx.Err(); err {\n\t\tcase context.Canceled:\n\t\t\treturn ErrQueryCancelled\n\t\tcase context.DeadlineExceeded:\n\t\t\treturn ErrQueryTimeout\n\t\tdefault:\n\t\t\treturn err\n\t\t}\n\tdefault:\n\t\treturn nil\n\t}\n}\n\n// errShardUnavailable is a marker error if no nodes are available.\nvar errShardUnavailable = errors.New(\"shard unavailable\")\n\ntype mapFunc func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error)\n\ntype mapOptions struct {\n\tmemoryAvailable *int64\n}\n\ntype reduceFunc func(ctx context.Context, prev, v interface{}) interface{}\n\ntype mapResponse struct {\n\tnode   *disco.Node\n\tshards []uint64\n\n\tresult interface{}\n\terr    error\n}\n\n// ExecOptions represents an execution context for a single Execute() call.\ntype ExecOptions struct {\n\tRemote        bool\n\tProfile       bool\n\tPreTranslated bool\n\tEmbeddedData  []*Row\n\tMaxMemory     int64\n}\n\nfunc needsShards(call *pql.Call) bool {\n\tif call == nil {\n\t\treturn false\n\t}\n\tswitch call.Name {\n\tcase \"Clear\", \"Set\":\n\t\treturn false\n\tcase \"Count\", \"TopN\", \"Rows\":\n\t\treturn true\n\t}\n\t// default catches Bitmap calls\n\treturn true\n}\n\n// SignedRow represents a signed *Row with two (neg/pos) *Rows.\ntype SignedRow struct {\n\tNeg   *Row   `json:\"neg\"`\n\tPos   *Row   `json:\"pos\"`\n\tField string `json:\"-\"`\n}\n\nfunc (s *SignedRow) Clone() (r *SignedRow) {\n\tr = &SignedRow{\n\t\tNeg:   s.Neg.Clone(), // Row.Clone() returns nil for nil.\n\t\tPos:   s.Pos.Clone(),\n\t\tField: s.Field,\n\t}\n\treturn\n}\n\n// ToTable implements the ToTabler interface.\nfunc (s SignedRow) ToTable() (*proto.TableResponse, error) {\n\tvar n uint64\n\tif s.Neg != nil {\n\t\tn += s.Neg.Count()\n\t}\n\tif s.Pos != nil {\n\t\tn += s.Pos.Count()\n\t}\n\treturn proto.RowsToTable(&s, int(n))\n}\n\n// ToRows implements the ToRowser interface.\nfunc (s SignedRow) ToRows(callback func(*proto.RowResponse) error) error {\n\tci := []*proto.ColumnInfo{{Name: s.Field, Datatype: \"int64\"}}\n\tif s.Neg != nil {\n\t\tnegs := s.Neg.Columns()\n\t\tfor i := len(negs) - 1; i >= 0; i-- {\n\t\t\tval, err := toNegInt64(negs[i])\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Wrap(err, \"converting uint64 to int64 (negative)\")\n\t\t\t}\n\n\t\t\tif err := callback(&proto.RowResponse{\n\t\t\t\tHeaders: ci,\n\t\t\t\tColumns: []*proto.ColumnResponse{\n\t\t\t\t\t{ColumnVal: &proto.ColumnResponse_Int64Val{Int64Val: val}},\n\t\t\t\t},\n\t\t\t}); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"calling callback\")\n\t\t\t}\n\t\t\tci = nil\n\t\t}\n\t}\n\tif s.Pos != nil {\n\t\tfor _, id := range s.Pos.Columns() {\n\t\t\tval, err := toInt64(id)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Wrap(err, \"converting uint64 to int64 (positive)\")\n\t\t\t}\n\n\t\t\tif err := callback(&proto.RowResponse{\n\t\t\t\tHeaders: ci,\n\t\t\t\tColumns: []*proto.ColumnResponse{\n\t\t\t\t\t{ColumnVal: &proto.ColumnResponse_Int64Val{Int64Val: val}},\n\t\t\t\t},\n\t\t\t}); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"calling callback\")\n\t\t\t}\n\t\t\tci = nil\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc toNegInt64(n uint64) (int64, error) {\n\tconst absMinInt64 = uint64(1 << 63)\n\n\tif n > absMinInt64 {\n\t\treturn 0, errors.Errorf(\"value %d overflows int64\", n)\n\t}\n\n\tif n == absMinInt64 {\n\t\treturn int64(-1 << 63), nil\n\t}\n\n\t// n < 1 << 63\n\treturn -int64(n), nil\n}\n\nfunc toInt64(n uint64) (int64, error) {\n\tconst maxInt64 = uint64(1<<63) - 1\n\n\tif n > maxInt64 {\n\t\treturn 0, errors.Errorf(\"value %d overflows int64\", n)\n\t}\n\n\treturn int64(n), nil\n}\n\nfunc (sr *SignedRow) Union(other SignedRow) SignedRow {\n\tret := SignedRow{&Row{}, &Row{}, \"\"}\n\n\t// merge in sr\n\tif sr != nil {\n\t\tif sr.Neg != nil {\n\t\t\tret.Neg = ret.Neg.Union(sr.Neg)\n\t\t}\n\t\tif sr.Pos != nil {\n\t\t\tret.Pos = ret.Pos.Union(sr.Pos)\n\t\t}\n\t}\n\n\t// merge in other\n\tif other.Neg != nil {\n\t\tret.Neg = ret.Neg.Union(other.Neg)\n\t}\n\tif other.Pos != nil {\n\t\tret.Pos = ret.Pos.Union(other.Pos)\n\t}\n\n\treturn ret\n}\n\n// ValCount represents a grouping of sum & count for Sum() and Average() calls. Also Min, Max....\ntype ValCount struct {\n\tVal          int64        `json:\"value\"`\n\tFloatVal     float64      `json:\"floatValue\"`\n\tDecimalVal   *pql.Decimal `json:\"decimalValue\"`\n\tTimestampVal time.Time    `json:\"timestampValue\"`\n\tCount        int64        `json:\"count\"`\n}\n\nfunc (v *ValCount) Clone() (r *ValCount) {\n\tr = &ValCount{\n\t\tVal:          v.Val,\n\t\tFloatVal:     v.FloatVal,\n\t\tTimestampVal: v.TimestampVal,\n\t\tCount:        v.Count,\n\t}\n\tif v.DecimalVal != nil {\n\t\tr.DecimalVal = v.DecimalVal.Clone()\n\t}\n\treturn\n}\n\n// ToTable implements the ToTabler interface.\nfunc (v ValCount) ToTable() (*proto.TableResponse, error) {\n\treturn proto.RowsToTable(&v, 1)\n}\n\n// ToRows implements the ToRowser interface.\nfunc (v ValCount) ToRows(callback func(*proto.RowResponse) error) error {\n\tvar ci []*proto.ColumnInfo\n\t// ValCount can have a decimal, float, or integer value, but\n\t// not more than one (as of this writing).\n\tif v.DecimalVal != nil {\n\t\tci = []*proto.ColumnInfo{\n\t\t\t{Name: \"value\", Datatype: \"decimal\"},\n\t\t\t{Name: \"count\", Datatype: \"int64\"},\n\t\t}\n\t\tvValue := v.DecimalVal.Value()\n\t\tvValuePtr := &vValue\n\t\tif err := callback(&proto.RowResponse{\n\t\t\tHeaders: ci,\n\t\t\tColumns: []*proto.ColumnResponse{\n\t\t\t\t{ColumnVal: &proto.ColumnResponse_DecimalVal{DecimalVal: &proto.Decimal{Value: vValuePtr.Int64(), Scale: v.DecimalVal.Scale}}},\n\t\t\t\t{ColumnVal: &proto.ColumnResponse_Int64Val{Int64Val: v.Count}},\n\t\t\t},\n\t\t}); err != nil {\n\t\t\treturn errors.Wrap(err, \"calling callback\")\n\t\t}\n\t} else if v.FloatVal != 0 {\n\t\tci = []*proto.ColumnInfo{\n\t\t\t{Name: \"value\", Datatype: \"float64\"},\n\t\t\t{Name: \"count\", Datatype: \"int64\"},\n\t\t}\n\t\tif err := callback(&proto.RowResponse{\n\t\t\tHeaders: ci,\n\t\t\tColumns: []*proto.ColumnResponse{\n\t\t\t\t{ColumnVal: &proto.ColumnResponse_Float64Val{Float64Val: v.FloatVal}},\n\t\t\t\t{ColumnVal: &proto.ColumnResponse_Int64Val{Int64Val: v.Count}},\n\t\t\t},\n\t\t}); err != nil {\n\t\t\treturn errors.Wrap(err, \"calling callback\")\n\t\t}\n\t} else if !v.TimestampVal.IsZero() {\n\t\tci = []*proto.ColumnInfo{\n\t\t\t{Name: \"value\", Datatype: \"string\"},\n\t\t\t{Name: \"count\", Datatype: \"int64\"},\n\t\t}\n\t\tif err := callback(&proto.RowResponse{\n\t\t\tHeaders: ci,\n\t\t\tColumns: []*proto.ColumnResponse{\n\t\t\t\t{ColumnVal: &proto.ColumnResponse_StringVal{StringVal: v.TimestampVal.Format(time.RFC3339Nano)}},\n\t\t\t\t{ColumnVal: &proto.ColumnResponse_Int64Val{Int64Val: v.Count}},\n\t\t\t},\n\t\t}); err != nil {\n\t\t\treturn errors.Wrap(err, \"calling callback\")\n\t\t}\n\t} else {\n\t\tci = []*proto.ColumnInfo{\n\t\t\t{Name: \"value\", Datatype: \"int64\"},\n\t\t\t{Name: \"count\", Datatype: \"int64\"},\n\t\t}\n\t\tif err := callback(&proto.RowResponse{\n\t\t\tHeaders: ci,\n\t\t\tColumns: []*proto.ColumnResponse{\n\t\t\t\t{ColumnVal: &proto.ColumnResponse_Int64Val{Int64Val: v.Val}},\n\t\t\t\t{ColumnVal: &proto.ColumnResponse_Int64Val{Int64Val: v.Count}},\n\t\t\t},\n\t\t}); err != nil {\n\t\t\treturn errors.Wrap(err, \"calling callback\")\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (vc *ValCount) Add(other ValCount) ValCount {\n\treturn ValCount{\n\t\tVal:   vc.Val + other.Val,\n\t\tCount: vc.Count + other.Count,\n\t}\n}\n\n// smaller returns the smaller of the two ValCounts.\nfunc (vc *ValCount) Smaller(other ValCount) ValCount {\n\tif vc.DecimalVal != nil || other.DecimalVal != nil {\n\t\treturn vc.decimalSmaller(other)\n\t} else if vc.FloatVal != 0 || other.FloatVal != 0 {\n\t\treturn vc.floatSmaller(other)\n\t} else if !vc.TimestampVal.IsZero() || !other.TimestampVal.IsZero() {\n\t\treturn vc.timestampSmaller(other)\n\t}\n\tif vc.Count == 0 || (other.Val < vc.Val && other.Count > 0) {\n\t\treturn other\n\t}\n\textra := int64(0)\n\tif vc.Val == other.Val {\n\t\textra += other.Count\n\t}\n\treturn ValCount{\n\t\tVal:          vc.Val,\n\t\tCount:        vc.Count + extra,\n\t\tDecimalVal:   vc.DecimalVal,\n\t\tFloatVal:     vc.FloatVal,\n\t\tTimestampVal: vc.TimestampVal,\n\t}\n}\n\n// timestampSmaller returns the smaller of the two (vc or other), while merging the count\n// if they are equal.\nfunc (vc *ValCount) timestampSmaller(other ValCount) ValCount {\n\tif other.TimestampVal.Equal(time.Time{}) {\n\t\treturn *vc\n\t}\n\tif vc.Count == 0 || vc.TimestampVal.Equal(time.Time{}) || (other.TimestampVal.Before(vc.TimestampVal) && other.Count > 0) {\n\t\treturn other\n\t}\n\textra := int64(0)\n\tif vc.TimestampVal.Equal(other.TimestampVal) {\n\t\textra += other.Count\n\t}\n\treturn ValCount{\n\t\tVal:          vc.Val,\n\t\tTimestampVal: vc.TimestampVal,\n\t\tCount:        vc.Count + extra,\n\t}\n}\n\n// decimalSmaller returns the smaller of the two (vc or other), while merging the count\n// if they are equal.\nfunc (vc *ValCount) decimalSmaller(other ValCount) ValCount {\n\tif other.DecimalVal == nil {\n\t\treturn *vc\n\t}\n\tif vc.Count == 0 || vc.DecimalVal == nil || (other.DecimalVal.LessThan(*vc.DecimalVal) && other.Count > 0) {\n\t\treturn other\n\t}\n\textra := int64(0)\n\tif vc.DecimalVal.EqualTo(*other.DecimalVal) {\n\t\textra += other.Count\n\t}\n\treturn ValCount{\n\t\tDecimalVal: vc.DecimalVal,\n\t\tCount:      vc.Count + extra,\n\t}\n}\n\n// floatSmaller returns the smaller of the two (vc or other), while merging the count\n// if they are equal.\nfunc (vc *ValCount) floatSmaller(other ValCount) ValCount {\n\tif vc.Count == 0 || (other.FloatVal < vc.FloatVal && other.Count > 0) {\n\t\treturn other\n\t}\n\textra := int64(0)\n\tif vc.FloatVal == other.FloatVal {\n\t\textra += other.Count\n\t}\n\treturn ValCount{\n\t\tFloatVal: vc.FloatVal,\n\t\tCount:    vc.Count + extra,\n\t}\n}\n\n// larger returns the larger of the two ValCounts.\nfunc (vc *ValCount) Larger(other ValCount) ValCount {\n\tif vc.DecimalVal != nil || other.DecimalVal != nil {\n\t\treturn vc.decimalLarger(other)\n\t} else if vc.FloatVal != 0 || other.FloatVal != 0 {\n\t\treturn vc.floatLarger(other)\n\t} else if !vc.TimestampVal.Equal(time.Time{}) || !other.TimestampVal.Equal(time.Time{}) {\n\t\treturn vc.timestampLarger(other)\n\t}\n\tif vc.Count == 0 || (other.Val > vc.Val && other.Count > 0) {\n\t\treturn other\n\t}\n\textra := int64(0)\n\tif vc.Val == other.Val {\n\t\textra += other.Count\n\t}\n\treturn ValCount{\n\t\tVal:          vc.Val,\n\t\tCount:        vc.Count + extra,\n\t\tDecimalVal:   vc.DecimalVal,\n\t\tFloatVal:     vc.FloatVal,\n\t\tTimestampVal: vc.TimestampVal,\n\t}\n}\n\n// timestampLarger returns the larger of the two (vc or other), while merging the count\n// if they are equal.\nfunc (vc *ValCount) timestampLarger(other ValCount) ValCount {\n\tif other.TimestampVal.Equal(time.Time{}) {\n\t\treturn *vc\n\t}\n\tif vc.Count == 0 || vc.TimestampVal.Equal(time.Time{}) || (other.TimestampVal.After(vc.TimestampVal) && other.Count > 0) {\n\t\treturn other\n\t}\n\textra := int64(0)\n\tif vc.TimestampVal.Equal(other.TimestampVal) {\n\t\textra += other.Count\n\t}\n\treturn ValCount{\n\t\tVal:          vc.Val,\n\t\tTimestampVal: vc.TimestampVal,\n\t\tCount:        vc.Count + extra,\n\t}\n}\n\n// decimalLarger returns the larger of the two (vc or other), while merging the count\n// if they are equal.\nfunc (vc *ValCount) decimalLarger(other ValCount) ValCount {\n\tif other.DecimalVal == nil {\n\t\treturn *vc\n\t}\n\tif vc.Count == 0 || vc.DecimalVal == nil || (other.DecimalVal.GreaterThan(*vc.DecimalVal) && other.Count > 0) {\n\t\treturn other\n\t}\n\textra := int64(0)\n\tif vc.DecimalVal.EqualTo(*other.DecimalVal) {\n\t\textra += other.Count\n\t}\n\treturn ValCount{\n\t\tDecimalVal: vc.DecimalVal,\n\t\tCount:      vc.Count + extra,\n\t}\n}\n\n// floatLarger returns the larger of the two (vc or other), while merging the count\n// if they are equal.\nfunc (vc *ValCount) floatLarger(other ValCount) ValCount {\n\tif vc.Count == 0 || (other.FloatVal > vc.FloatVal && other.Count > 0) {\n\t\treturn other\n\t}\n\textra := int64(0)\n\tif vc.FloatVal == other.FloatVal {\n\t\textra += other.Count\n\t}\n\treturn ValCount{\n\t\tFloatVal: vc.FloatVal,\n\t\tCount:    vc.Count + extra,\n\t}\n}\n\nfunc callArgString(call *pql.Call, key string) string {\n\tvalue, ok := call.Args[key]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\ts, _ := value.(string)\n\treturn s\n}\n\n// groupByIterator contains several slices. Each slice contains a number of\n// elements equal to the number of fields in the group by (the number of Rows\n// calls).\ntype groupByIterator struct {\n\texecutor *executor\n\tqcx      *Qcx\n\tindex    string\n\tshard    uint64\n\n\t// rowIters contains a rowIterator for each of the fields in the Group By.\n\trowIters []rowIterator\n\t// rows contains the current row data for each of the fields in the Group\n\t// By. Each row is the intersection of itself and the rows of the fields\n\t// with an index lower than its own. This is a performance optimization so\n\t// that the expected common case of getting the next row in the furthest\n\t// field to the right require only a single intersect with the row of the\n\t// previous field to determine the count of the new group.\n\trows []struct {\n\t\trow   *Row\n\t\tid    uint64\n\t\tvalue *int64\n\t}\n\n\t// fields helps with the construction of GroupCount results by holding all\n\t// the field names that are being grouped by. Each results makes a copy of\n\t// fields and then sets the row ids.\n\tfields []FieldRow\n\tdone   bool\n\n\t// Optional filter row to intersect against first level of values.\n\tfilter *Row\n\n\t// Optional aggregate function to execute for each group.\n\taggregate *pql.Call\n}\n\n// newGroupByIterator initializes a new groupByIterator.\nfunc newGroupByIterator(executor *executor, qcx *Qcx, rowIDs []RowIDs, children []*pql.Call, aggregate *pql.Call, filter *Row, index string, shard uint64, holder *Holder) (_ *groupByIterator, err0 error) {\n\tgbi := &groupByIterator{\n\t\texecutor: executor,\n\t\tqcx:      qcx,\n\t\tindex:    index,\n\t\tshard:    shard,\n\t\trowIters: make([]rowIterator, len(children)),\n\t\trows: make([]struct {\n\t\t\trow   *Row\n\t\t\tid    uint64\n\t\t\tvalue *int64\n\t\t}, len(children)),\n\t\tfilter:    filter,\n\t\taggregate: aggregate,\n\t\tfields:    make([]FieldRow, len(children)),\n\t}\n\tidx := holder.Index(index)\n\n\tvar (\n\t\tfieldName string\n\t\tviewName  string\n\t\tok        bool\n\t\tviews     []string\n\t)\n\tignorePrev := false\n\tfor i, call := range children {\n\t\tvar isTimeField bool\n\t\tif fieldName, ok = call.Args[\"_field\"].(string); !ok {\n\t\t\treturn nil, errors.Errorf(\"%s call must have field with valid (string) field name. Got %v of type %[2]T\", call.Name, call.Args[\"_field\"])\n\t\t}\n\t\tfield := holder.Field(index, fieldName)\n\t\tif field == nil {\n\t\t\treturn nil, newNotFoundError(ErrFieldNotFound, fieldName)\n\t\t}\n\t\tgbi.fields[i].Field = fieldName\n\t\toptions := field.Options()\n\t\tgbi.fields[i].FieldOptions = &options\n\n\t\tswitch field.Type() {\n\t\tcase FieldTypeSet, FieldTypeMutex, FieldTypeBool:\n\t\t\tviewName = viewStandard\n\t\tcase FieldTypeTime:\n\t\t\tvar (\n\t\t\t\terr error\n\t\t\t\tv   interface{}\n\t\t\t)\n\n\t\t\t// Parse \"from\" time, if set.\n\t\t\tvar (\n\t\t\t\thasFrom  bool\n\t\t\t\tfromTime time.Time\n\t\t\t)\n\t\t\tif v, hasFrom = call.Args[\"from\"]; hasFrom {\n\t\t\t\tif fromTime, err = parseTime(v); err != nil {\n\t\t\t\t\treturn nil, errors.Wrap(err, \"parsing from time\")\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Parse \"to\" time, if set.\n\t\t\tvar (\n\t\t\t\thasTo  bool\n\t\t\t\ttoTime time.Time\n\t\t\t)\n\t\t\tif v, hasTo = call.Args[\"to\"]; hasTo {\n\t\t\t\tif toTime, err = parseTime(v); err != nil {\n\t\t\t\t\treturn nil, errors.Wrap(err, \"parsing to time\")\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif hasTo || hasFrom {\n\t\t\t\t// Determine the views based on the specified time range.\n\t\t\t\tvar err error\n\t\t\t\tviews, err = field.viewsByTimeRange(fromTime, toTime)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tisTimeField = true\n\t\t\t} else {\n\t\t\t\tviewName = viewStandard\n\t\t\t}\n\t\tcase FieldTypeInt, FieldTypeTimestamp:\n\t\t\tviewName = viewBSIGroupPrefix + fieldName\n\n\t\tdefault: // FieldTypeDecimal\n\t\t\treturn nil, errors.Errorf(\"%s call must have field of one of types: %s\",\n\t\t\t\tcall.Name, strings.Join([]string{FieldTypeSet, FieldTypeTime, FieldTypeMutex, FieldTypeBool, FieldTypeInt, FieldTypeTimestamp}, \",\"))\n\t\t}\n\n\t\tfilters := []roaring.BitmapFilter{}\n\t\tif len(rowIDs[i]) > 0 {\n\t\t\tfilters = append(filters, roaring.NewBitmapRowsFilter(rowIDs[i]))\n\t\t}\n\n\t\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Index: idx, Shard: shard})\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tdefer finisher(&err0)\n\n\t\t// Fetch fragment(s), get rowIterator\n\t\tif isTimeField {\n\t\t\tvar fragments []*fragment\n\t\t\tfor _, viewName := range views {\n\t\t\t\tfragment := holder.fragment(index, fieldName, viewName, shard)\n\t\t\t\tif fragment != nil {\n\t\t\t\t\tfragments = append(fragments, fragment)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif len(fragments) == 0 {\n\t\t\t\t// whole shard doesn't have all it needs to continue ?\n\t\t\t\treturn nil, nil\n\t\t\t}\n\n\t\t\tgbi.rowIters[i], err = timeFragmentsRowIterator(fragments, tx, i != 0, filters...)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t} else {\n\t\t\tfrag := holder.fragment(index, fieldName, viewName, shard)\n\t\t\tif frag == nil { // this means this whole shard doesn't have all it needs to continue\n\t\t\t\treturn nil, nil\n\t\t\t}\n\n\t\t\tgbi.rowIters[i], err = frag.rowIterator(tx, i != 0, filters...)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t}\n\n\t\tprev, hasPrev, err := call.UintArg(\"previous\")\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"getting previous\")\n\t\t} else if hasPrev && !ignorePrev {\n\t\t\tif i == len(children)-1 {\n\t\t\t\tprev++\n\t\t\t}\n\t\t\tgbi.rowIters[i].Seek(prev)\n\t\t}\n\t\tnextRow, rowID, value, wrapped, err := gbi.rowIters[i].Next()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t} else if nextRow == nil {\n\t\t\tgbi.done = true\n\t\t\treturn gbi, nil\n\t\t}\n\t\tgbi.rows[i].row = nextRow\n\t\tgbi.rows[i].id = rowID\n\t\tgbi.rows[i].value = value\n\t\tif hasPrev && rowID != prev {\n\t\t\t// ignorePrev signals that we didn't find a previous row, so all\n\t\t\t// Rows queries \"deeper\" than it need to ignore the previous\n\t\t\t// argument and start at the beginning.\n\t\t\tignorePrev = true\n\t\t}\n\t\tif wrapped {\n\t\t\t// if a field has wrapped, we need to get the next row for the\n\t\t\t// previous field, and if that one wraps we need to keep going\n\t\t\t// backward.\n\t\t\tfor j := i - 1; j >= 0; j-- {\n\t\t\t\tnextRow, rowID, value, wrapped, err := gbi.rowIters[j].Next()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t} else if nextRow == nil {\n\t\t\t\t\tgbi.done = true\n\t\t\t\t\treturn gbi, nil\n\t\t\t\t}\n\t\t\t\tgbi.rows[j].row = nextRow\n\t\t\t\tgbi.rows[j].id = rowID\n\t\t\t\tgbi.rows[j].value = value\n\t\t\t\tif !wrapped {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Apply filter to first level, if available.\n\tif gbi.filter != nil && len(gbi.rows) > 0 {\n\t\tgbi.rows[0].row = gbi.rows[0].row.Intersect(gbi.filter)\n\t}\n\n\tfor i := 1; i < len(gbi.rows)-1; i++ {\n\t\tgbi.rows[i].row = gbi.rows[i].row.Intersect(gbi.rows[i-1].row)\n\t}\n\n\treturn gbi, nil\n}\n\n// nextAtIdx is a recursive helper method for getting the next row for the field\n// at index i, and then updating the rows in the \"higher\" fields if it wraps.\nfunc (gbi *groupByIterator) nextAtIdx(ctx context.Context, i int) (err error) {\n\t// loop until we find a non-empty row. This is an optimization - the loop and if/break can be removed.\n\tfor {\n\t\tif err = ctx.Err(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tnr, rowID, value, wrapped, err := gbi.rowIters[i].Next()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t} else if nr == nil {\n\t\t\tgbi.done = true\n\t\t\treturn nil\n\t\t}\n\t\tif wrapped && i != 0 {\n\t\t\terr = gbi.nextAtIdx(ctx, i-1)\n\t\t\tif gbi.done || err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif i == 0 && gbi.filter != nil {\n\t\t\tgbi.rows[i].row = nr.Intersect(gbi.filter)\n\t\t} else if i == 0 || i == len(gbi.rows)-1 {\n\t\t\tgbi.rows[i].row = nr\n\t\t} else {\n\t\t\tgbi.rows[i].row = nr.Intersect(gbi.rows[i-1].row)\n\t\t}\n\t\tgbi.rows[i].id = rowID\n\t\tgbi.rows[i].value = value\n\n\t\tif !gbi.rows[i].row.IsEmpty() {\n\t\t\tbreak\n\t\t}\n\t}\n\treturn nil\n}\n\n// Next returns a GroupCount representing the next group by record. When there\n// are no more records it will return an empty GroupCount and done==true.\nfunc (gbi *groupByIterator) Next(ctx context.Context) (ret GroupCount, done bool, err error) {\n\t// loop until we find a result with count > 0\n\tfor {\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn ret, false, err\n\t\t}\n\t\tif gbi.done {\n\t\t\treturn ret, true, nil\n\t\t}\n\t\tif gbi.aggregate == nil || gbi.aggregate.Name == \"Count\" {\n\t\t\tif len(gbi.rows) == 1 {\n\t\t\t\tret.Count = gbi.rows[len(gbi.rows)-1].row.Count()\n\t\t\t} else {\n\t\t\t\tret.Count = gbi.rows[len(gbi.rows)-1].row.intersectionCount(gbi.rows[len(gbi.rows)-2].row)\n\t\t\t}\n\t\t} else {\n\t\t\tgr := gbi.rows[len(gbi.rows)-1]\n\t\t\tfilter := gr.row\n\t\t\tif len(gbi.rows) != 1 {\n\t\t\t\tfilter = filter.Intersect(gbi.rows[len(gbi.rows)-2].row)\n\t\t\t}\n\n\t\t\tswitch gbi.aggregate.Name {\n\t\t\tcase \"Sum\":\n\t\t\t\tresult, err := gbi.executor.executeSumCountShard(ctx, gbi.qcx, gbi.index, gbi.aggregate, filter, gbi.shard)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn ret, false, err\n\t\t\t\t}\n\t\t\t\tret.Count = uint64(result.Count)\n\t\t\t\tret.Agg = result.Val\n\t\t\t\tret.DecimalAgg = result.DecimalVal\n\t\t\t}\n\t\t}\n\t\tif ret.Count == 0 {\n\t\t\terr := gbi.nextAtIdx(ctx, len(gbi.rows)-1)\n\t\t\tif err != nil {\n\t\t\t\treturn ret, false, err\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tbreak\n\t}\n\n\tret.Group = make([]FieldRow, len(gbi.rows))\n\tcopy(ret.Group, gbi.fields)\n\tfor i, r := range gbi.rows {\n\t\tret.Group[i].RowID = r.id\n\t\tret.Group[i].Value = r.value\n\t}\n\n\t// set up for next call\n\terr = gbi.nextAtIdx(ctx, len(gbi.rows)-1)\n\n\treturn ret, false, err\n}\n\n// getCondIntSlice looks at the field, the op type (which is\n// expected to be one of the BETWEEN ops types), and the values in the\n// conditional and returns a slice of int64 which is scaled for\n// decimal fields and has the values modulated such that the BETWEEN\n// op can be treated as being of the form a<=x<=b.\nfunc getCondIntSlice(f *Field, op pql.Token, value interface{}) ([]int64, error) {\n\tval, ok := value.([]interface{})\n\tif !ok {\n\t\treturn nil, errors.Errorf(\"expected conditional to have []interface{} Value, but got %v of %[1]T\", value)\n\t}\n\n\tret := make([]int64, len(val))\n\tfor i, v := range val {\n\t\ts, err := getScaledInt(f, v)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"getting scaled integer\")\n\t\t}\n\t\tret[i] = s\n\t}\n\n\t// In the case where one (or both) of the predicates is on the\n\t// opposite edge, return early to avoid the increment/decrement\n\t// logic below and prevent an overflow.\n\tif ret[0] == math.MaxInt64 || ret[1] == math.MinInt64 {\n\t\treturn ret, nil\n\t}\n\n\tswitch op {\n\tcase pql.BTWN_LT_LTE: // a < x <= b\n\t\tret[0]++\n\tcase pql.BTWN_LTE_LT: // a <= x < b\n\t\tret[1]--\n\tcase pql.BTWN_LT_LT: // a < x < b\n\t\tret[0]++\n\t\tret[1]--\n\t}\n\n\treturn ret, nil\n}\n\n// getScaledInt gets the scaled integer value for v based on\n// the field type. In the `decimalToInt64()` function, the\n// returned int64 value will be adjusted to correspond to the\n// range of the field. This is only necessary for pql.Decimal\n// values. For example, if v is less than f.Options.Min, int64 will\n// return int64(f.Options.Min)-1, or math.MinInt64 if f.Options.Min\n// is already equal to math.MinInt64.\nfunc getScaledInt(f *Field, v interface{}) (int64, error) {\n\tvar value int64\n\n\topt := f.Options()\n\tif opt.Type == FieldTypeDecimal {\n\t\tswitch tv := v.(type) {\n\t\tcase uint64:\n\t\t\tif tv > math.MaxInt64 {\n\t\t\t\treturn 0, errors.Errorf(\"uint64 value out of range for pql.Decimal: %d\", tv)\n\t\t\t}\n\t\t\tdec := pql.NewDecimal(int64(tv), 0)\n\t\t\tvalue = decimalToInt64(dec, opt)\n\t\tcase int64:\n\t\t\tdec := pql.NewDecimal(tv, 0)\n\t\t\tvalue = decimalToInt64(dec, opt)\n\t\tcase pql.Decimal:\n\t\t\tvalue = decimalToInt64(tv, opt)\n\t\tcase float64:\n\t\t\tvalue = int64(tv * math.Pow10(int(opt.Scale)))\n\t\tdefault:\n\t\t\treturn 0, errors.Errorf(\"unexpected decimal value type %T, val %v\", tv, tv)\n\t\t}\n\t} else if opt.Type == FieldTypeTimestamp {\n\t\tswitch tv := v.(type) {\n\t\tcase time.Time:\n\t\t\tv := TimestampToVal(f.options.TimeUnit, tv)\n\t\t\tvalue = v\n\t\tcase int64:\n\t\t\tvalue = tv\n\t\tdefault:\n\t\t\treturn 0, errors.Errorf(\"unexpected timestamp value type %T, val %v\", tv, tv)\n\t\t}\n\t} else {\n\t\tswitch tv := v.(type) {\n\t\tcase int64:\n\t\t\tvalue = tv\n\t\tcase uint64:\n\t\t\tvalue = int64(tv)\n\t\tdefault:\n\t\t\treturn 0, errors.Errorf(\"unexpected value type %T, val %v\", tv, tv)\n\t\t}\n\t}\n\treturn value, nil\n}\n\nfunc decimalToInt64(dec pql.Decimal, opt FieldOptions) int64 {\n\tscale := opt.Scale\n\tif dec.GreaterThanOrEqualTo(opt.Min) && dec.LessThanOrEqualTo(opt.Max) {\n\t\treturn dec.ToInt64(scale)\n\t} else if dec.LessThan(opt.Min) {\n\t\tvalue := opt.Min.ToInt64(scale)\n\t\tif value != math.MinInt64 {\n\t\t\tvalue--\n\t\t}\n\t\treturn value\n\t} else if dec.GreaterThan(opt.Max) {\n\t\tvalue := opt.Max.ToInt64(scale)\n\t\tif value != math.MaxInt64 {\n\t\t\tvalue++\n\t\t}\n\t\treturn value\n\t}\n\n\treturn 0\n}\n\n// executeDeleteRecords executes a delete() call.\nfunc (e *executor) executeDeleteRecords(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (bool, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"executor.executeDelete\")\n\tdefer span.Finish()\n\n\tif len(c.Children) == 0 {\n\t\treturn false, errors.New(\"Delete() requires an input bitmap\")\n\t} else if len(c.Children) > 1 {\n\t\treturn false, errors.New(\"Delete() only accepts a single bitmap input\")\n\t}\n\tqcx.Abort()\n\tqcx.Reset() // release the qcx to allow for rbf checkpoint\n\n\t// Execute calls in bulk on each remote node and merge.\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\treturn e.executeDeleteRecordFromShard(ctx, index, c.Children[0], shard)\n\t}\n\n\t// Merge returned results at coordinating node.\n\treduceFn := func(ctx context.Context, prev, v interface{}) interface{} {\n\t\tother, _ := prev.(bool)\n\t\treturn other || v.(bool)\n\t}\n\n\tresult, err := e.mapReduce(ctx, index, shards, c, opt, mapFn, reduceFn)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tn, _ := result.(bool)\n\n\treturn n, nil\n}\n\nfunc transactExistRow(ctx context.Context, idx *Index, shard uint64, frag *fragment, src *Row) (uint64, error) {\n\tholder := idx.Holder()\n\ttx := holder.Txf().NewTx(Txo{Write: writable, Index: idx, Shard: shard})\n\trows, err := frag.rows(ctx, tx, 1)\n\tif err != nil {\n\t\ttx.Rollback()\n\t\treturn 0, err\n\n\t}\n\t// obtain a rowID which is higher than any currently present row ID.\n\trowID := uint64(1)\n\tif len(rows) > 0 {\n\t\trowID = rows[len(rows)-1] + 1\n\t}\n\t_, err = frag.setRow(tx, src, rowID)\n\tif err != nil {\n\t\ttx.Rollback()\n\t\treturn 0, err\n\t}\n\treturn rowID, tx.Commit()\n}\n\nfunc (e *executor) executeDeleteRecordFromShard(ctx context.Context, index string, bmCall *pql.Call, shard uint64) (changed bool, err error) {\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"Executor.executeDeleteRecordFromShard\")\n\tdefer span.Finish()\n\t// Fetch index.\n\tidx := e.Holder.Index(index)\n\tif idx == nil {\n\t\terr = newNotFoundError(ErrIndexNotFound, index)\n\t\treturn\n\t}\n\tqcx := e.Holder.Txf().NewQcx()\n\t// bmCall is a bitmap\n\trow, err := e.executeBitmapCallShard(ctx, qcx, index, bmCall, shard)\n\tqcx.Abort()\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tif len(row.Segments) == 0 {\n\t\treturn\n\t}\n\tcolumns := row.Segments[0].data\n\tif columns.Count() == 0 {\n\t\treturn\n\t}\n\tsrc := NewRowFromBitmap(columns)\n\treturn DeleteRowsWithFlow(ctx, src, idx, shard, true)\n}\n\nfunc DeleteRows(ctx context.Context, src *Row, idx *Index, shard uint64) (bool, error) {\n\treturn DeleteRowsWithFlow(ctx, src, idx, shard, false)\n}\n\nfunc DeleteRowsWithFlowWithKeys(ctx context.Context, columns *roaring.Bitmap, idx *Index, shard uint64, normalFlow bool) (bool, error) {\n\tvar existenceFragment *fragment\n\tvar deletedRowID uint64\n\tvar commitor Commitor = &NopCommitor{}\n\tvar err error // store columns in exits field ToBeDelete row commited\n\tholder := idx.Holder()\n\tif normalFlow { // normalFlow is the standard path, \"not normal\" is recoverory\n\t\texistenceFragment = holder.fragment(idx.Name(), existenceFieldName, viewStandard, shard)\n\t\tif existenceFragment == nil {\n\t\t\t// no exists field\n\t\t\treturn false, errors.New(\"can't bulk delete without existence field\")\n\t\t}\n\t\tsrc := NewRowFromBitmap(columns)\n\t\tdeletedRowID, err = transactExistRow(ctx, idx, shard, existenceFragment, src)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\t}\n\tcommitor, err = deleteKeyTranslation(ctx, idx, shard, columns)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\twriteTx := holder.Txf().NewTx(Txo{Write: writable, Index: idx, Shard: shard})\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tdefer writeTx.Rollback()\n\tchanged := false\n\tdefer func() {\n\t\t// if there is an error on the bit clearing rollback the keys\n\t\tif err != nil {\n\t\t\tchanged = false\n\t\t\tcommitor.Rollback()\n\t\t\treturn\n\t\t}\n\t\t// if there is an error in the key commit, then rollback the delete\n\t\t// write records before keys to remove possiblity of unmatch keys=records\n\t\terr = writeTx.Commit()\n\t\tif err != nil {\n\t\t\tchanged = false\n\t\t\tcommitor.Rollback()\n\t\t\treturn\n\t\t}\n\t\tif er := commitor.Commit(); er != nil {\n\t\t\terr = er\n\t\t}\n\t\tif err != nil {\n\t\t\tholder.Logger.Errorf(\"problems committing delete in rbf %v shard %v\", err, shard)\n\t\t}\n\t}()\n\n\tfor _, field := range idx.Fields() {\n\t\tfor _, view := range field.views() {\n\t\t\tfrag := view.Fragment(shard)\n\t\t\tif frag == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc, err := frag.clearRecordsByBitmap(writeTx, columns)\n\t\t\tif err != nil {\n\t\t\t\treturn false, err\n\t\t\t}\n\t\t\tif c {\n\t\t\t\tchanged = true\n\t\t\t}\n\t\t}\n\t}\n\tif existenceFragment != nil { // a string keys have been deleted and the deleteRow was created\n\t\tif normalFlow {\n\t\t\texistenceFragment.clearRow(writeTx, deletedRowID)\n\t\t} else {\n\t\t\t// this is if we are recovering from failure and cleaning up\n\t\t\trows, err := existenceFragment.rows(ctx, writeTx, 1)\n\t\t\tif err != nil {\n\t\t\t\treturn false, err\n\t\t\t}\n\t\t\tfor _, rowId := range rows {\n\t\t\t\texistenceFragment.clearRow(writeTx, rowId)\n\t\t\t}\n\t\t}\n\t}\n\treturn changed, nil\n}\n\nfunc DeleteRowsWithOutKeysFlow(ctx context.Context, columns *roaring.Bitmap, idx *Index, shard uint64, normalFlow bool) (changed bool, err error) {\n\tvar existenceFragment *fragment\n\tvar deletedRowID uint64\n\tvar commitor Commitor = &NopCommitor{}\n\tholder := idx.Holder()\n\twriteTx := holder.Txf().NewTx(Txo{Write: writable, Index: idx, Shard: shard})\n\tdefer writeTx.Rollback()\n\tdefer func() {\n\t\t// if there is an error in the key commit, then rollback the delete\n\t\t// write records before keys to remove possiblity of unmatch keys=records\n\t\terr := writeTx.Commit()\n\t\tif err != nil {\n\t\t\tchanged = false\n\t\t\tcommitor.Rollback()\n\t\t\treturn\n\t\t}\n\t}()\n\tfor _, field := range idx.Fields() {\n\t\tfor _, view := range field.views() {\n\t\t\tfrag := view.Fragment(shard)\n\t\t\tif frag == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc, err := frag.clearRecordsByBitmap(writeTx, columns)\n\t\t\tif err != nil {\n\t\t\t\treturn false, err\n\t\t\t}\n\t\t\tif c {\n\t\t\t\tchanged = true\n\t\t\t}\n\n\t\t}\n\t}\n\tif existenceFragment == nil { // a string keys have been deleted and the deleteRow was created\n\t\treturn changed, nil\n\t}\n\n\tif normalFlow {\n\t\texistenceFragment.clearRow(writeTx, deletedRowID)\n\t\treturn changed, nil\n\t}\n\n\t// this is if we are recovering from failure and cleaning up\n\trows, err := existenceFragment.rows(ctx, writeTx, 1)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tfor _, rowId := range rows {\n\t\texistenceFragment.clearRow(writeTx, rowId)\n\t}\n\treturn changed, nil\n}\n\nfunc DeleteRowsWithFlow(ctx context.Context, src *Row, idx *Index, shard uint64, normalFlow bool) (change bool, err error) {\n\tif len(src.Segments) == 0 { // nothing to remove\n\t\treturn false, nil\n\t}\n\tcolumns := src.Segments[0].data // should only be one segment\n\tif columns.Count() == 0 {\n\t\treturn false, nil\n\t}\n\tbits := src.Segments[0].data.Slice()\n\tmin := func(a, b int) int {\n\t\tif a <= b {\n\t\t\treturn a\n\t\t}\n\t\treturn b\n\t}\n\tlimit := idx.holder.cfg.RBFConfig.MaxDelete\n\tfor i := 0; i < len(bits); i += limit {\n\t\tbatch := roaring.NewBitmap(bits[i:min(i+limit, len(bits))]...)\n\t\tif idx.Keys() {\n\t\t\tchange, err = DeleteRowsWithFlowWithKeys(ctx, batch, idx, shard, normalFlow)\n\t\t} else {\n\t\t\tchange, err = DeleteRowsWithOutKeysFlow(ctx, batch, idx, shard, normalFlow)\n\t\t}\n\t\tif err != nil {\n\t\t\treturn change, err\n\t\t}\n\t}\n\treturn change, err\n}\n\ntype Commitor interface {\n\tRollback()\n\tCommit() error\n}\ntype NopCommitor struct{}\n\nfunc (c *NopCommitor) Rollback() {\n}\n\nfunc (c *NopCommitor) Commit() error {\n\treturn nil\n}\n\nfunc deleteKeyTranslation(ctx context.Context, idx *Index, shard uint64, records *roaring.Bitmap) (Commitor, error) {\n\t// ShardToShardParition ...\n\tparitionID := disco.ShardToShardPartition(idx.name, shard, idx.holder.partitionN)\n\n\treturn idx.TranslateStore(paritionID).Delete(records)\n}\n\nfunc (e *executor) executeSort(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shards []uint64, opt *ExecOptions) (*SortedRow, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"Executor.executeSort\")\n\tdefer span.Finish()\n\n\tsort_desc, _, err := c.BoolArg(\"sort-desc\")\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \" getting sort-desc\")\n\t}\n\n\tmapFn := func(ctx context.Context, shard uint64, mopt *mapOptions) (_ interface{}, err error) {\n\t\treturn e.executeSortShard(ctx, qcx, index, c, shard)\n\t}\n\n\treduceFn := func(ctx context.Context, prev, v interface{}) interface{} {\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// var result *SortedRow\n\t\tswitch other := prev.(type) {\n\t\tcase *SortedRow:\n\t\t\tif err := other.Merge(v.(*SortedRow), sort_desc); err != nil {\n\t\t\t\treturn err\n\t\t\t} else {\n\t\t\t\treturn other\n\t\t\t}\n\t\tcase *Row:\n\t\t\tother.Merge(v.(*Row))\n\t\t\treturn other\n\t\tcase nil:\n\t\t\tif v == nil {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tif out, ok := v.(*SortedRow); ok {\n\t\t\t\treturn out\n\t\t\t}\n\t\t\treturn v.(*Row)\n\t\tdefault:\n\t\t\treturn errors.Errorf(\"unexpected return type from executeSortShard: %+v %T\", other, other)\n\t\t}\n\t}\n\n\tres, err := e.mapReduce(ctx, index, shards, c, opt, mapFn, reduceFn)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"mapReduce\")\n\t}\n\n\toffset, hasOffset, err := c.UintArg(\"offset\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tlimit, hasLimit, err := c.UintArg(\"limit\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tresult := res.(*SortedRow)\n\tif hasOffset {\n\t\tresult.RowKVs = result.RowKVs[offset:]\n\t\tresult.Row = NewRow(result.Columns()...)\n\t}\n\tif hasLimit && limit < result.Row.Count() {\n\t\tresult.RowKVs = result.RowKVs[:limit]\n\t\tresult.Row = NewRow(result.Columns()...)\n\t}\n\treturn result, nil\n}\n\nfunc (e *executor) executeSortShard(ctx context.Context, qcx *Qcx, index string, c *pql.Call, shard uint64) (*SortedRow, error) {\n\tvar filter *Row\n\tif len(c.Children) == 1 {\n\t\trow, err := e.executeBitmapCallShard(ctx, qcx, index, c.Children[0], shard)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfilter = row\n\t}\n\n\tidx := e.Holder.Index(index)\n\tif idx == nil {\n\t\treturn nil, newNotFoundError(ErrIndexNotFound, index)\n\t}\n\n\tfieldName, err := c.FirstStringArg(\"field\", \"_field\")\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"getting field\")\n\t}\n\tf := idx.Field(fieldName)\n\tif f == nil {\n\t\treturn nil, newNotFoundError(ErrFieldNotFound, fieldName)\n\t}\n\n\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Index: idx, Shard: shard})\n\tif err != nil {\n\t\treturn nil, ErrQcxDone\n\t}\n\tdefer finisher(&err)\n\n\tsort_desc, _, err := c.BoolArg(\"sort-desc\")\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \" getting sort-desc\")\n\t}\n\n\tswitch f.Type() {\n\tcase FieldTypeBool:\n\t\tfragment := e.Holder.fragment(index, f.name, viewStandard, shard)\n\t\tif fragment == nil {\n\t\t\treturn nil, errors.New(\"bool fragment not found\")\n\t\t}\n\t\tfalses, err := fragment.row(tx, falseRowID)\n\t\tif err != nil {\n\t\t\treturn nil, errors.New(\"error loading false from fragment\")\n\t\t}\n\t\ttrues, err := fragment.row(tx, trueRowID)\n\t\tif err != nil {\n\t\t\treturn nil, errors.New(\"error loading true from fragment\")\n\t\t}\n\t\tfalses = filter.Intersect(falses)\n\t\ttrues = filter.Intersect(trues)\n\t\trowKVs := make([]RowKV, len(filter.Columns()))\n\t\tvar i int\n\t\tif sort_desc {\n\t\t\tfor _, col := range trues.Columns() {\n\t\t\t\trowKVs[i] = RowKV{\n\t\t\t\t\tRowID: col,\n\t\t\t\t\tValue: true,\n\t\t\t\t}\n\t\t\t\ti++\n\t\t\t}\n\t\t\tfor _, col := range falses.Columns() {\n\t\t\t\trowKVs[i] = RowKV{\n\t\t\t\t\tRowID: col,\n\t\t\t\t\tValue: false,\n\t\t\t\t}\n\t\t\t\ti++\n\t\t\t}\n\t\t} else {\n\t\t\tfor _, col := range falses.Columns() {\n\t\t\t\trowKVs[i] = RowKV{\n\t\t\t\t\tRowID: col,\n\t\t\t\t\tValue: false,\n\t\t\t\t}\n\t\t\t\ti++\n\t\t\t}\n\t\t\tfor _, col := range trues.Columns() {\n\t\t\t\trowKVs[i] = RowKV{\n\t\t\t\t\tRowID: col,\n\t\t\t\t\tValue: true,\n\t\t\t\t}\n\t\t\t\ti++\n\t\t\t}\n\t\t}\n\t\treturn &SortedRow{\n\t\t\tRow:    filter,\n\t\t\tRowKVs: rowKVs,\n\t\t}, nil\n\tcase FieldTypeDecimal, FieldTypeInt, FieldTypeTimestamp:\n\t\treturn f.SortShardRow(tx, shard, filter, sort_desc)\n\tcase FieldTypeMutex:\n\t\tfragment := e.Holder.fragment(index, f.name, viewStandard, shard)\n\t\tif fragment == nil {\n\t\t\treturn nil, errors.Errorf(\"fragment not found for field %s\", f.name)\n\t\t}\n\t\trows, err := fragment.rows(ctx, tx, 0)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \" ggettign rows error\")\n\t\t}\n\t\trowKVs := make([]RowKV, filter.Count())\n\t\ti := 0\n\t\tfor _, rowID := range rows {\n\t\t\trow, err := fragment.row(tx, rowID)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"couldn't load row from fragment\")\n\t\t\t}\n\t\t\trow = row.Intersect(filter)\n\t\t\tif row.Count() > 0 {\n\t\t\t\tif f.Keys() {\n\t\t\t\t\tif rowStr, err := f.translateStore.TranslateID(rowID); err != nil {\n\t\t\t\t\t\treturn nil, errors.Wrap(err, \"error getting translateIDs\")\n\t\t\t\t\t} else {\n\t\t\t\t\t\tfor _, v := range row.Columns() {\n\t\t\t\t\t\t\trowKVs[i] = RowKV{\n\t\t\t\t\t\t\t\tRowID: v,\n\t\t\t\t\t\t\t\tValue: rowStr,\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\ti++\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tfor _, v := range row.Columns() {\n\t\t\t\t\t\trowKVs[i] = RowKV{\n\t\t\t\t\t\t\tRowID: v,\n\t\t\t\t\t\t\tValue: rowID,\n\t\t\t\t\t\t}\n\t\t\t\t\t\ti++\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// this is to make sure compare function worked.\n\t\tok := true\n\t\tsort.SliceStable(rowKVs, func(i, j int) bool {\n\t\t\tif c, k := rowKVs[i].Compare(rowKVs[j], sort_desc); k {\n\t\t\t\treturn c\n\t\t\t} else {\n\t\t\t\tok = false\n\t\t\t\treturn !k\n\t\t\t}\n\t\t})\n\t\tif !ok {\n\t\t\treturn nil, errors.New(\"could not compare values for sort\")\n\t\t}\n\t\treturn &SortedRow{\n\t\t\tRow:    filter,\n\t\t\tRowKVs: rowKVs,\n\t\t}, nil\n\tdefault:\n\t\treturn nil, errors.Errorf(\"Sort of field type %s not implemented yet\", f.Type())\n\t}\n}\n\ntype SortedRow struct {\n\tRow    *Row\n\tRowKVs []RowKV\n}\n\nfunc (s *SortedRow) Columns() []uint64 {\n\tout := make([]uint64, len(s.RowKVs))\n\tfor i, rkv := range s.RowKVs {\n\t\tout[i] = rkv.RowID\n\t}\n\treturn out\n}\n\nfunc (s *SortedRow) ToRows(callback func(*proto.RowResponse) error) error {\n\tci := []*proto.ColumnInfo{{Name: \"_id\", Datatype: \"int64\"}}\n\n\tfor _, kvs := range s.RowKVs {\n\t\tval, err := toInt64(kvs.RowID)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"converting uint64 to int64 (positive)\")\n\t\t}\n\t\tif err := callback(&proto.RowResponse{\n\t\t\tHeaders: ci,\n\t\t\tColumns: []*proto.ColumnResponse{\n\t\t\t\t{ColumnVal: &proto.ColumnResponse_Int64Val{Int64Val: val}},\n\t\t\t},\n\t\t}); err != nil {\n\t\t\treturn errors.Wrap(err, \"calling callback\")\n\t\t}\n\t\tci = nil\n\t}\n\treturn nil\n}\n\nfunc (s *SortedRow) Merge(o *SortedRow, sort_desc bool) error {\n\trowKVs := make([]RowKV, len(s.RowKVs)+len(o.RowKVs))\n\n\ti, j, k := 0, 0, 0\n\tfor i < len(s.RowKVs) && j < len(o.RowKVs) {\n\t\tif c, ok := s.RowKVs[i].Compare(o.RowKVs[j], sort_desc); ok {\n\t\t\tif c {\n\t\t\t\trowKVs[k] = s.RowKVs[i]\n\t\t\t\ti++\n\t\t\t} else {\n\t\t\t\trowKVs[k] = o.RowKVs[j]\n\t\t\t\tj++\n\t\t\t}\n\t\t\tk++\n\t\t} else {\n\t\t\treturn errors.Errorf(\"Coultn't compare %v and %v\", s.RowKVs[i], o.RowKVs[j])\n\t\t}\n\t}\n\n\tfor i < len(s.RowKVs) {\n\t\trowKVs[k] = s.RowKVs[i]\n\t\ti++\n\t\tk++\n\t}\n\n\tfor j < len(o.RowKVs) {\n\t\trowKVs[k] = o.RowKVs[j]\n\t\tj++\n\t\tk++\n\t}\n\n\ts.RowKVs = rowKVs\n\ts.Row.Merge(o.Row)\n\treturn nil\n}\n\ntype ExtractedIDMatrixSorted struct {\n\tExtractedIDMatrix *ExtractedIDMatrix\n\tRowKVs            []RowKV\n}\n\nfunc MergeExtractedIDMatrixSorted(a, b ExtractedIDMatrixSorted, sort_desc bool) (ExtractedIDMatrixSorted, error) {\n\tcolumns := make([]ExtractedIDColumn, len(a.ExtractedIDMatrix.Columns)+len(b.ExtractedIDMatrix.Columns))\n\trowKVs := make([]RowKV, len(a.RowKVs)+len(b.RowKVs))\n\ti, j, k := 0, 0, 0\n\tfor i < len(a.ExtractedIDMatrix.Columns) && j < len(b.ExtractedIDMatrix.Columns) {\n\t\tif c, ok := a.RowKVs[i].Compare(b.RowKVs[j], sort_desc); ok {\n\t\t\tif c {\n\t\t\t\tcolumns[k] = a.ExtractedIDMatrix.Columns[i]\n\t\t\t\trowKVs[k] = a.RowKVs[i]\n\t\t\t\ti++\n\t\t\t} else {\n\t\t\t\tcolumns[k] = b.ExtractedIDMatrix.Columns[j]\n\t\t\t\trowKVs[k] = b.RowKVs[j]\n\t\t\t\tj++\n\t\t\t}\n\t\t\tk++\n\t\t} else {\n\t\t\treturn ExtractedIDMatrixSorted{}, errors.Errorf(\"error comparing %v and %v\", a.RowKVs[i], b.RowKVs[j])\n\t\t}\n\t}\n\tfor i < len(a.ExtractedIDMatrix.Columns) {\n\t\tcolumns[k] = a.ExtractedIDMatrix.Columns[i]\n\t\trowKVs[k] = a.RowKVs[i]\n\t\ti++\n\t\tk++\n\t}\n\tfor j < len(b.ExtractedIDMatrix.Columns) {\n\t\tcolumns[k] = b.ExtractedIDMatrix.Columns[j]\n\t\trowKVs[k] = b.RowKVs[j]\n\t\tj++\n\t\tk++\n\t}\n\tif a.ExtractedIDMatrix.Fields == nil {\n\t\ta.ExtractedIDMatrix.Fields = b.ExtractedIDMatrix.Fields\n\t}\n\treturn ExtractedIDMatrixSorted{\n\t\tExtractedIDMatrix: &ExtractedIDMatrix{\n\t\t\tColumns: columns,\n\t\t\tFields:  a.ExtractedIDMatrix.Fields,\n\t\t},\n\t\tRowKVs: rowKVs,\n\t}, nil\n}\n"
        },
        {
          "name": "executor_internal_test.go",
          "type": "blob",
          "size": 15.8447265625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"sort\"\n\t\"strconv\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/pql\"\n)\n\n// AssertEqual checks a given RowIdentifiers against expected values.\nfunc (r *RowIdentifiers) AssertEqual(tb testing.TB, other *RowIdentifiers) {\n\tsort.Slice(r.Rows, func(i, j int) bool { return r.Rows[i] < r.Rows[j] })\n\tsort.Slice(other.Rows, func(i, j int) bool { return other.Rows[i] < other.Rows[j] })\n\tif len(r.Rows) != len(other.Rows) {\n\t\ttb.Fatalf(\"row ID mismatch: got %d, expected %d\", r.Rows, other.Rows)\n\t}\n\tfor i := range r.Rows {\n\t\tif r.Rows[i] != other.Rows[i] {\n\t\t\ttb.Fatalf(\"row ID mismatch: got %d, expected %d\", r.Rows, other.Rows)\n\t\t}\n\t}\n\tsort.Strings(r.Keys)\n\tsort.Strings(other.Keys)\n\tif len(r.Keys) != len(other.Keys) {\n\t\ttb.Fatalf(\"row keys mismatch: got %s, expected %s\", r.Keys, other.Keys)\n\t}\n\tfor i := range r.Keys {\n\t\tif r.Keys[i] != other.Keys[i] {\n\t\t\ttb.Fatalf(\"row keys mismatch: got %s, expected %s\", r.Keys, other.Keys)\n\t\t}\n\t}\n}\n\nfunc TestExecutor_TranslateRowsOnBool(t *testing.T) {\n\tholder := newTestHolder(t)\n\n\te := &executor{\n\t\tHolder:  holder,\n\t\tCluster: NewTestCluster(t, 1),\n\t}\n\n\tidx, err := e.Holder.CreateIndex(\"i\", \"\", IndexOptions{})\n\tif err != nil {\n\t\tt.Fatalf(\"creating index: %v\", err)\n\t}\n\n\tqcx := holder.Txf().NewWritableQcx()\n\tdefer qcx.Abort()\n\n\tfb, errb := idx.CreateField(\"b\", \"\", OptFieldTypeBool())\n\t_, errbk := idx.CreateField(\"bk\", \"\", OptFieldTypeBool(), OptFieldKeys())\n\tif errb != nil || errbk != nil {\n\t\tt.Fatalf(\"creating fields %v, %v\", errb, errbk)\n\t}\n\n\t_, err1 := fb.SetBit(qcx, 1, 1, nil)\n\t_, err2 := fb.SetBit(qcx, 2, 2, nil)\n\t_, err3 := fb.SetBit(qcx, 3, 3, nil)\n\tif err1 != nil || err2 != nil || err3 != nil {\n\t\tt.Fatalf(\"setting bit %v, %v, %v\", err1, err2, err3)\n\t}\n\n\ttests := []struct {\n\t\tpql string\n\t}{\n\t\t{pql: \"Rows(b)\"},\n\t\t{pql: \"GroupBy(Rows(b))\"},\n\t\t{pql: \"Set(4, b=true)\"},\n\t}\n\n\tfor _, test := range tests {\n\t\tt.Run(test.pql, func(t *testing.T) {\n\t\t\tquery, err := pql.ParseString(test.pql)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"parsing query: %v\", err)\n\t\t\t}\n\n\t\t\tc := query.Calls[0]\n\t\t\tcolTranslations, rowTranslations, err := e.preTranslate(context.Background(), \"i\", c)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"pre-translating call: %v\", err)\n\t\t\t}\n\t\t\t_, err = e.translateCall(c, \"i\", colTranslations, rowTranslations)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"translating call: %v\", err)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestFieldRowMarshalJSON(t *testing.T) {\n\tfr := FieldRow{\n\t\tField:  \"blah\",\n\t\tRowID:  0,\n\t\tRowKey: \"ha\",\n\t}\n\tb, err := json.Marshal(fr)\n\tif err != nil {\n\t\tt.Fatalf(\"marshalling fieldrow: %v\", err)\n\t}\n\tif string(b) != `{\"field\":\"blah\",\"rowKey\":\"ha\"}` {\n\t\tt.Fatalf(\"unexpected json: %s\", b)\n\t}\n\n\tfr = FieldRow{\n\t\tField:  \"blah\",\n\t\tRowID:  2,\n\t\tRowKey: \"\",\n\t}\n\tb, err = json.Marshal(fr)\n\tif err != nil {\n\t\tt.Fatalf(\"marshalling fieldrow: %v\", err)\n\t}\n\tif string(b) != `{\"field\":\"blah\",\"rowID\":2}` {\n\t\tt.Fatalf(\"unexpected json: %s\", b)\n\t}\n}\n\nfunc TestExecutor_GroupCountCondition(t *testing.T) {\n\tt.Run(\"satisfiesCondition\", func(t *testing.T) {\n\t\ttype condCheck struct {\n\t\t\tcond string\n\t\t\texp  bool\n\t\t}\n\t\ttests := []struct {\n\t\t\tgroupCount GroupCount\n\t\t\tchecks     []condCheck\n\t\t}{\n\t\t\t{\n\t\t\t\tgroupCount: GroupCount{Count: 100},\n\t\t\t\tchecks: []condCheck{\n\t\t\t\t\t{cond: \"count == 99\", exp: false},\n\t\t\t\t\t{cond: \"count != 99\", exp: true},\n\t\t\t\t\t{cond: \"count < 99\", exp: false},\n\t\t\t\t\t{cond: \"count <= 99\", exp: false},\n\t\t\t\t\t{cond: \"count > 99\", exp: true},\n\t\t\t\t\t{cond: \"count >= 99\", exp: true},\n\n\t\t\t\t\t{cond: \"count == 100\", exp: true},\n\t\t\t\t\t{cond: \"count != 100\", exp: false},\n\t\t\t\t\t{cond: \"count < 100\", exp: false},\n\t\t\t\t\t{cond: \"count <= 100\", exp: true},\n\t\t\t\t\t{cond: \"count > 100\", exp: false},\n\t\t\t\t\t{cond: \"count >= 100\", exp: true},\n\n\t\t\t\t\t{cond: \"count == 101\", exp: false},\n\t\t\t\t\t{cond: \"count != 101\", exp: true},\n\t\t\t\t\t{cond: \"count < 101\", exp: true},\n\t\t\t\t\t{cond: \"count <= 101\", exp: true},\n\t\t\t\t\t{cond: \"count > 101\", exp: false},\n\t\t\t\t\t{cond: \"count >= 101\", exp: false},\n\n\t\t\t\t\t{cond: \"98 < count < 100\", exp: false},\n\t\t\t\t\t{cond: \"98 < count <= 100\", exp: true},\n\t\t\t\t\t{cond: \"98 < count < 101\", exp: true},\n\t\t\t\t\t{cond: \"100 <= count < 102\", exp: true},\n\t\t\t\t\t{cond: \"100 < count < 102\", exp: false},\n\t\t\t\t\t{cond: \"98 <= count <= 102\", exp: true},\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\tgroupCount: GroupCount{Agg: 100},\n\t\t\t\tchecks: []condCheck{\n\t\t\t\t\t{cond: \"sum == 99\", exp: false},\n\t\t\t\t\t{cond: \"sum != 99\", exp: true},\n\t\t\t\t\t{cond: \"sum < 99\", exp: false},\n\t\t\t\t\t{cond: \"sum <= 99\", exp: false},\n\t\t\t\t\t{cond: \"sum > 99\", exp: true},\n\t\t\t\t\t{cond: \"sum >= 99\", exp: true},\n\n\t\t\t\t\t{cond: \"sum == 100\", exp: true},\n\t\t\t\t\t{cond: \"sum != 100\", exp: false},\n\t\t\t\t\t{cond: \"sum < 100\", exp: false},\n\t\t\t\t\t{cond: \"sum <= 100\", exp: true},\n\t\t\t\t\t{cond: \"sum > 100\", exp: false},\n\t\t\t\t\t{cond: \"sum >= 100\", exp: true},\n\n\t\t\t\t\t{cond: \"sum == 101\", exp: false},\n\t\t\t\t\t{cond: \"sum != 101\", exp: true},\n\t\t\t\t\t{cond: \"sum < 101\", exp: true},\n\t\t\t\t\t{cond: \"sum <= 101\", exp: true},\n\t\t\t\t\t{cond: \"sum > 101\", exp: false},\n\t\t\t\t\t{cond: \"sum >= 101\", exp: false},\n\n\t\t\t\t\t{cond: \"98 < sum < 100\", exp: false},\n\t\t\t\t\t{cond: \"98 < sum <= 100\", exp: true},\n\t\t\t\t\t{cond: \"98 < sum < 101\", exp: true},\n\t\t\t\t\t{cond: \"100 <= sum < 102\", exp: true},\n\t\t\t\t\t{cond: \"100 < sum < 102\", exp: false},\n\t\t\t\t\t{cond: \"98 <= sum <= 102\", exp: true},\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\tgroupCount: GroupCount{Agg: -100},\n\t\t\t\tchecks: []condCheck{\n\t\t\t\t\t{cond: \"sum == -99\", exp: false},\n\t\t\t\t\t{cond: \"sum != -99\", exp: true},\n\t\t\t\t\t{cond: \"sum < -99\", exp: true},\n\t\t\t\t\t{cond: \"sum <= -99\", exp: true},\n\t\t\t\t\t{cond: \"sum > -99\", exp: false},\n\t\t\t\t\t{cond: \"sum >= -99\", exp: false},\n\n\t\t\t\t\t{cond: \"sum == -100\", exp: true},\n\t\t\t\t\t{cond: \"sum != -100\", exp: false},\n\t\t\t\t\t{cond: \"sum < -100\", exp: false},\n\t\t\t\t\t{cond: \"sum <= -100\", exp: true},\n\t\t\t\t\t{cond: \"sum > -100\", exp: false},\n\t\t\t\t\t{cond: \"sum >= -100\", exp: true},\n\n\t\t\t\t\t{cond: \"sum == -101\", exp: false},\n\t\t\t\t\t{cond: \"sum != -101\", exp: true},\n\t\t\t\t\t{cond: \"sum < -101\", exp: false},\n\t\t\t\t\t{cond: \"sum <= -101\", exp: false},\n\t\t\t\t\t{cond: \"sum > -101\", exp: true},\n\t\t\t\t\t{cond: \"sum >= -101\", exp: true},\n\n\t\t\t\t\t{cond: \"-100 < sum < -98\", exp: false},\n\t\t\t\t\t{cond: \"-100 <= sum < -98\", exp: true},\n\t\t\t\t\t{cond: \"-101 < sum < -98\", exp: true},\n\t\t\t\t\t{cond: \"-102 < sum <= -100\", exp: true},\n\t\t\t\t\t{cond: \"-102 < sum < -100\", exp: false},\n\t\t\t\t\t{cond: \"-102 <= sum <= -98\", exp: true},\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t\tfor i, test := range tests {\n\t\t\tt.Run(fmt.Sprintf(\"test (#%d):\", i), func(t *testing.T) {\n\t\t\t\tfor j, check := range test.checks {\n\t\t\t\t\tt.Run(fmt.Sprintf(\"check (#%d):\", j), func(t *testing.T) {\n\t\t\t\t\t\tquery, err := pql.ParseString(fmt.Sprintf(\"GroupBy(Rows(a), having=Condition(%s))\", check.cond))\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\tt.Fatalf(\"parsing query: %v\", err)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tc := query.Calls[0]\n\t\t\t\t\t\thaving := c.Args[\"having\"].(*pql.Call)\n\n\t\t\t\t\t\tvar got bool\n\t\t\t\t\t\tfor subj, cond := range having.Args {\n\t\t\t\t\t\t\tswitch subj {\n\t\t\t\t\t\t\tcase \"count\", \"sum\":\n\t\t\t\t\t\t\t\tcondition, ok := cond.(*pql.Condition)\n\t\t\t\t\t\t\t\tif !ok {\n\t\t\t\t\t\t\t\t\tt.Fatalf(\"not a valid condition\")\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\tgot = test.groupCount.satisfiesCondition(subj, condition)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif got != check.exp {\n\t\t\t\t\t\t\tt.Fatalf(\"expected: %v, but got: %v\", check.exp, got)\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t})\n\t\t}\n\t})\n}\n\nfunc TestValCountComparisons(t *testing.T) {\n\ttests := []struct {\n\t\tname       string\n\t\tvc         ValCount\n\t\tother      ValCount\n\t\texpLarger  ValCount\n\t\texpSmaller ValCount\n\t}{\n\t\t{\n\t\t\tname: \"zero\",\n\t\t},\n\t\t{\n\t\t\tname:       \"ints\",\n\t\t\tvc:         ValCount{Val: 10, Count: 1},\n\t\t\tother:      ValCount{Val: 3, Count: 2},\n\t\t\texpLarger:  ValCount{Val: 10, Count: 1},\n\t\t\texpSmaller: ValCount{Val: 3, Count: 2},\n\t\t},\n\t\t{\n\t\t\tname:       \"floats\",\n\t\t\tvc:         ValCount{FloatVal: 10.2, Count: 1},\n\t\t\tother:      ValCount{FloatVal: 3.4, Count: 2},\n\t\t\texpLarger:  ValCount{FloatVal: 10.2, Count: 1},\n\t\t\texpSmaller: ValCount{FloatVal: 3.4, Count: 2},\n\t\t},\n\t\t{\n\t\t\tname:       \"intsEquality\",\n\t\t\tvc:         ValCount{Val: 10, Count: 1},\n\t\t\tother:      ValCount{Val: 10, Count: 2},\n\t\t\texpLarger:  ValCount{Val: 10, Count: 3},\n\t\t\texpSmaller: ValCount{Val: 10, Count: 3},\n\t\t},\n\t\t{\n\t\t\tname:       \"floatsEquality\",\n\t\t\tvc:         ValCount{FloatVal: 10.7, Count: 1},\n\t\t\tother:      ValCount{FloatVal: 10.7, Count: 2},\n\t\t\texpLarger:  ValCount{FloatVal: 10.7, Count: 3},\n\t\t\texpSmaller: ValCount{FloatVal: 10.7, Count: 3},\n\t\t},\n\t\t{\n\t\t\tname:       \"timestampEquality\",\n\t\t\tvc:         ValCount{Val: -17782800, TimestampVal: time.Unix(0, -17782800*int64(time.Second)), Count: 1},\n\t\t\tother:      ValCount{Val: -17782800, TimestampVal: time.Unix(0, -17782800*int64(time.Second)), Count: 1},\n\t\t\texpLarger:  ValCount{Val: -17782800, TimestampVal: time.Unix(0, -17782800*int64(time.Second)), Count: 2},\n\t\t\texpSmaller: ValCount{Val: -17782800, TimestampVal: time.Unix(0, -17782800*int64(time.Second)), Count: 2},\n\t\t},\n\t\t{\n\t\t\tname:       \"timestamp\",\n\t\t\tvc:         ValCount{Val: -17782800, TimestampVal: time.Unix(0, -17782800*int64(time.Second)), Count: 1},\n\t\t\tother:      ValCount{Val: 1587399600, TimestampVal: time.Unix(0, 1587399600*int64(time.Second)), Count: 1},\n\t\t\texpLarger:  ValCount{Val: 1587399600, TimestampVal: time.Unix(0, 1587399600*int64(time.Second)), Count: 1},\n\t\t\texpSmaller: ValCount{Val: -17782800, TimestampVal: time.Unix(0, -17782800*int64(time.Second)), Count: 1},\n\t\t},\n\t}\n\n\tfor i, test := range tests {\n\t\tt.Run(test.name+strconv.Itoa(i), func(t *testing.T) {\n\t\t\tgotLarger := test.vc.Larger(test.other)\n\t\t\tif gotLarger != test.expLarger {\n\t\t\t\tt.Fatalf(\"larger failed, expected:\\n%+v\\ngot:\\n%+v\", test.expLarger, gotLarger)\n\t\t\t}\n\n\t\t\tgotSmaller := test.vc.Smaller(test.other)\n\t\t\tif gotSmaller != test.expSmaller {\n\t\t\t\tt.Fatalf(\"smaller failed, expected:\\n%+v\\ngot:\\n%+v\", test.expSmaller, gotSmaller)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestToNegInt64(t *testing.T) {\n\ttests := []struct {\n\t\tu64      uint64\n\t\ti64      int64\n\t\toverflow bool\n\t}{\n\t\t{\n\t\t\tu64: uint64(1 << 63),\n\t\t\ti64: int64(-1 << 63),\n\t\t},\n\t\t{\n\t\t\tu64: uint64(1<<63) - 1,\n\t\t\ti64: int64(-1<<63) + 1,\n\t\t},\n\t\t{\n\t\t\tu64:      uint64(1<<63) + 1,\n\t\t\toverflow: true,\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\tval, err := toNegInt64(tc.u64)\n\t\tif err != nil && !tc.overflow {\n\t\t\tt.Fatalf(\"error: %+v, expected: %+v\", err, tc)\n\t\t}\n\n\t\tif val != tc.i64 {\n\t\t\tt.Fatalf(\"Expected: %+v, Got: %+v\", tc.i64, val)\n\t\t}\n\t}\n}\n\nfunc TestToInt64(t *testing.T) {\n\ttests := []struct {\n\t\tu64      uint64\n\t\ti64      int64\n\t\toverflow bool\n\t}{\n\t\t{\n\t\t\tu64: uint64(1<<63) - 1,\n\t\t\ti64: 1<<63 - 1,\n\t\t},\n\t\t{\n\t\t\tu64: uint64(0),\n\t\t\ti64: 0,\n\t\t},\n\t\t{\n\t\t\tu64:      uint64(1 << 63),\n\t\t\toverflow: true,\n\t\t},\n\t\t{\n\t\t\tu64:      1<<64 - 1,\n\t\t\toverflow: true,\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\tval, err := toInt64(tc.u64)\n\t\tif err != nil && !tc.overflow {\n\t\t\tt.Fatalf(\"error: %+v, expected: %+v\", err, tc)\n\t\t}\n\n\t\tif val != tc.i64 {\n\t\t\tt.Fatalf(\"Expected: %+v, Got: %+v\", tc.i64, val)\n\t\t}\n\t}\n}\n\nfunc TestGetSorter(t *testing.T) {\n\ttests := []struct {\n\t\tsortSpec string\n\t\texpGCS   *groupCountSorter\n\t\texpErr   string\n\t}{\n\t\t{\n\t\t\tsortSpec: \"count asc\",\n\t\t\texpGCS:   &groupCountSorter{fields: []int{-1}, order: []order{asc}},\n\t\t},\n\t\t{\n\t\t\tsortSpec: \"count    asc\",\n\t\t\texpGCS:   &groupCountSorter{fields: []int{-1}, order: []order{asc}},\n\t\t},\n\t\t{\n\t\t\tsortSpec: \"  count asc\",\n\t\t\texpGCS:   &groupCountSorter{fields: []int{-1}, order: []order{asc}},\n\t\t},\n\t\t{\n\t\t\tsortSpec: \"  count asc    \",\n\t\t\texpGCS:   &groupCountSorter{fields: []int{-1}, order: []order{asc}},\n\t\t},\n\t\t{\n\t\t\tsortSpec: \"count\",\n\t\t\texpGCS:   &groupCountSorter{fields: []int{-1}, order: []order{desc}},\n\t\t},\n\t\t{\n\t\t\tsortSpec: \"sum asc\",\n\t\t\texpGCS:   &groupCountSorter{fields: []int{-2}, order: []order{asc}},\n\t\t},\n\t\t{\n\t\t\tsortSpec: \"aggregate asc\",\n\t\t\texpGCS:   &groupCountSorter{fields: []int{-2}, order: []order{asc}},\n\t\t},\n\t\t{\n\t\t\tsortSpec: \"boondoggle asc\",\n\t\t\texpErr:   \"sorting is only supported on count, aggregate, or sum, not 'boondoggle'\",\n\t\t},\n\t\t{\n\t\t\tsortSpec: \"sum asc, count desc\",\n\t\t\texpGCS:   &groupCountSorter{fields: []int{-2, -1}, order: []order{asc, desc}},\n\t\t},\n\t\t{\n\t\t\tsortSpec: \"count asc, sum desc\",\n\t\t\texpGCS:   &groupCountSorter{fields: []int{-1, -2}, order: []order{asc, desc}},\n\t\t},\n\t\t{\n\t\t\tsortSpec: \" count  asc ,  sum  desc \",\n\t\t\texpGCS:   &groupCountSorter{fields: []int{-1, -2}, order: []order{asc, desc}},\n\t\t},\n\t\t{\n\t\t\tsortSpec: \" count  asc ,  sum  desc blah\",\n\t\t\texpErr:   \"parsing sort directive: 'sum  desc blah': too many elements\",\n\t\t},\n\t\t{\n\t\t\tsortSpec: \"count asc, sum fesc\",\n\t\t\texpErr:   \"unknown sort direction 'fesc'\",\n\t\t},\n\t\t{\n\t\t\tsortSpec: \" , sum fesc\",\n\t\t\texpErr:   \"invalid sorting directive: ''\",\n\t\t},\n\t\t{\n\t\t\t// weird and useless, but I guess fine?\n\t\t\tsortSpec: \"count  asc,count asc \",\n\t\t\texpGCS:   &groupCountSorter{fields: []int{-1, -1}, order: []order{asc, asc}},\n\t\t},\n\t}\n\n\tfor i, tst := range tests {\n\t\tt.Run(fmt.Sprintf(\"%s_%d\", tst.sortSpec, i), func(t *testing.T) {\n\t\t\tgcs, err := getSorter(tst.sortSpec)\n\t\t\tif err != nil {\n\t\t\t\tif tst.expErr == \"\" {\n\t\t\t\t\tt.Errorf(\"unexpected error: %v\", err)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif tst.expErr != err.Error() {\n\t\t\t\t\tt.Errorf(\"mismatched errors got: '%v', exp: '%s'\", err, tst.expErr)\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif !reflect.DeepEqual(gcs, tst.expGCS) {\n\t\t\t\tt.Errorf(\"exp:\\n%+v\\ngot:\\n%v\\n\", tst.expGCS, gcs)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestExecutorSafeCopyDistinctTimestamp(t *testing.T) {\n\tresult := DistinctTimestamp{Values: []string{\"test\", \"test\"}, Name: \"test\"}\n\tresults := make([]interface{}, 1)\n\tresults[0] = result\n\n\tresponse := QueryResponse{Results: results, Err: nil, Profile: nil}\n\tcopied := safeCopy(response)\n\tif !reflect.DeepEqual(copied.Results, response.Results) {\n\t\tt.Fatalf(\"Did not copy results. got %+v, want %+v\", copied.Results, response.Results)\n\t}\n}\n\nfunc TestGetScaledInt(t *testing.T) {\n\t_, _, f := newTestField(t, OptFieldTypeTimestamp(time.Now(), \"ms\"))\n\t// check that fields with type timestamp return the int64 passed in to getScaledInt with nil err\n\tv := time.Now().Unix()\n\tres, err := getScaledInt(f, v)\n\tif err != nil {\n\t\tt.Errorf(\"got error %v, expected nil\", err)\n\t}\n\tif !reflect.DeepEqual(res, v) {\n\t\tt.Errorf(\"expected %v, got %v\", v, res)\n\t}\n}\n\nfunc TestDistinctTimestampUnion(t *testing.T) {\n\tcases := []struct {\n\t\tname     string\n\t\ta        DistinctTimestamp\n\t\tb        DistinctTimestamp\n\t\texpected DistinctTimestamp\n\t}{\n\t\t{\n\t\t\tname:     \"empty other\",\n\t\t\ta:        DistinctTimestamp{Name: \"a\", Values: []string{\"a\", \"b\", \"c\"}},\n\t\t\tb:        DistinctTimestamp{Name: \"a\", Values: []string{}},\n\t\t\texpected: DistinctTimestamp{Name: \"a\", Values: []string{\"a\", \"b\", \"c\"}},\n\t\t},\n\t\t{\n\t\t\tname:     \"one more in other\",\n\t\t\ta:        DistinctTimestamp{Name: \"a\", Values: []string{\"a\", \"b\", \"c\"}},\n\t\t\tb:        DistinctTimestamp{Name: \"a\", Values: []string{\"a\", \"b\", \"c\", \"d\"}},\n\t\t\texpected: DistinctTimestamp{Name: \"a\", Values: []string{\"a\", \"b\", \"c\", \"d\"}},\n\t\t},\n\t}\n\tfor _, test := range cases {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tres := test.a.Union(test.b)\n\t\t\tallThere := true\n\t\t\tfor _, val := range res.Values {\n\t\t\t\there := false\n\t\t\t\tfor _, expected := range test.expected.Values {\n\t\t\t\t\tif val == expected {\n\t\t\t\t\t\there = true\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tallThere = allThere && here\n\t\t\t}\n\t\t\tif !allThere {\n\t\t\t\tt.Errorf(\"expected %v, got %v\", test.expected, res)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestExecutor_DeleteRows(t *testing.T) {\n\tholder := newTestHolder(t)\n\n\tidx, err := holder.CreateIndex(\"i\", \"\", IndexOptions{TrackExistence: true})\n\tif err != nil {\n\t\tt.Fatalf(\"creating index: %v\", err)\n\t}\n\n\tf, err := idx.CreateField(\"f\", \"\")\n\tif err != nil {\n\t\tt.Fatalf(\"creating field: %v\", err)\n\t}\n\n\tqcx := holder.Txf().NewWritableQcx()\n\tdefer qcx.Abort()\n\tif _, err = f.SetBit(qcx, 1, 1, nil); err != nil {\n\t\tt.Fatalf(\"setting bit: %v\", err)\n\t}\n\n\t// We rely here on the fact that write Qcx autocommit constantly.\n\trow, err := f.Row(qcx, 1)\n\tif err != nil {\n\t\tt.Fatalf(\"failed to read row: %v\", err)\n\t}\n\n\tctx := context.Background()\n\tchanged, err := DeleteRows(ctx, row, idx, 0)\n\tif !changed || err != nil {\n\t\tt.Fatalf(\"failed to delete row: %v\", err)\n\t}\n\n\tchanged, err = DeleteRows(ctx, row, idx, 0)\n\tif err != nil {\n\t\tt.Fatalf(\"deleting rows: %v\", err)\n\t}\n\tif changed {\n\t\tt.Fatalf(\"expected delete to not clear bit but it did\")\n\t}\n}\n"
        },
        {
          "name": "executor_test.go",
          "type": "blob",
          "size": 316.0869140625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa_test\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/csv\"\n\t\"encoding/json\"\n\t\"flag\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"math/rand\"\n\t_ \"net/http/pprof\"\n\t\"os\"\n\t\"reflect\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/davecgh/go-spew/spew\"\n\tpilosa \"github.com/featurebasedb/featurebase/v3\"\n\t\"github.com/featurebasedb/featurebase/v3/ctl\"\n\t\"github.com/featurebasedb/featurebase/v3/disco\"\n\t\"github.com/featurebasedb/featurebase/v3/logger\"\n\t\"github.com/featurebasedb/featurebase/v3/pql\"\n\t\"github.com/featurebasedb/featurebase/v3/proto\"\n\t\"github.com/featurebasedb/featurebase/v3/server\"\n\t\"github.com/featurebasedb/featurebase/v3/test\"\n\t\"github.com/featurebasedb/featurebase/v3/testhook\"\n\t. \"github.com/featurebasedb/featurebase/v3/vprint\" // nolint:staticcheck\n\t\"github.com/google/go-cmp/cmp\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nvar TempDir = getTempDirString()\n\nfunc getTempDirString() (td *string) {\n\ttdflag := flag.Lookup(\"temp-dir\")\n\n\tif tdflag == nil {\n\t\ttd = flag.String(\"temp-dir\", \"\", \"Directory in which to place temporary data (e.g. for benchmarking). Useful if you are trying to benchmark different storage configurations.\")\n\t} else {\n\t\ts := tdflag.Value.String()\n\t\ttd = &s\n\t}\n\treturn td\n}\n\nfunc TestExecutor(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\t// Ensure a row query can be executed.\n\tt.Run(\"ExecuteRow\", func(t *testing.T) {\n\t\tt.Run(\"RowIDColumnID\", func(t *testing.T) {\n\t\t\twriteQuery := `` +\n\t\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", 3, 10) +\n\t\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", ShardWidth+1, 10) +\n\t\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", ShardWidth+1, 20) +\n\t\t\t\t`Set(1000, f=100)`\n\t\t\treadQueries := []string{`Row(f=10)`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries, nil)\n\t\t\tif bits := responses[0].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{3, ShardWidth + 1}) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"RowIDColumnKey\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\t\tSet(\"one-hundred\", f=1)\n\t\t\t\tSet(\"two-hundred\", f=1)`\n\t\t\treadQueries := []string{`Row(f=1)`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{Keys: true})\n\t\t\tkeys := responses[0].Results[0].(*pilosa.Row).Keys\n\t\t\tif !sameStringSlice(keys, []string{\"one-hundred\", \"two-hundred\"}) {\n\t\t\t\tt.Fatalf(\"unexpected keys: %+v\", keys)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnID\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\t\tSet(100, f=\"one\")\n\t\t\t\tSet(200, f=\"one\")`\n\t\t\treadQueries := []string{`Row(f=\"one\")`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\tnil, pilosa.OptFieldKeys())\n\t\t\tif columns := responses[0].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{100, 200}) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnKey\", func(t *testing.T) {\n\t\t\twriteQuery := `` +\n\t\t\t\t`Set(\"foo\", f=\"bar\")` + \"\\n\" +\n\t\t\t\t`Set(\"foo\", f=\"baz\")` + \"\\n\" +\n\t\t\t\t`Set(\"bat\", f=\"bar\")` + \"\\n\" +\n\t\t\t\t`Set(\"aaa\", f=\"bbb\")` + \"\\n\"\n\t\t\treadQueries := []string{`Row(f=\"bar\")`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{Keys: true},\n\t\t\t\tpilosa.OptFieldKeys())\n\t\t\tkeys := responses[0].Results[0].(*pilosa.Row).Keys\n\t\t\tif !sameStringSlice(keys, []string{\"bat\", \"foo\"}) {\n\t\t\t\tt.Fatalf(\"unexpected keys: %+v\", keys)\n\t\t\t}\n\t\t})\n\t})\n\n\tt.Run(\"Execute_Difference\", func(t *testing.T) {\n\t\tt.Run(\"RowIDColumnKey\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\t\tSet(\"one\", f=10)\n\t\t\t\tSet(\"two\", f=10)\n\t\t\t\tSet(\"three\", f=10)\n\t\t\t\tSet(\"two\", f=11)\n\t\t\t\tSet(\"four\", f=11)`\n\t\t\treadQueries := []string{`Difference(Row(f=10), Row(f=11))`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{Keys: true})\n\t\t\tkeys := responses[0].Results[0].(*pilosa.Row).Keys\n\t\t\tif !sameStringSlice(keys, []string{\"three\", \"one\"}) {\n\t\t\t\tt.Fatalf(\"unexpected keys: %+v\", keys)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnID\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\t\tSet(1, f=\"ten\")\n\t\t\t\tSet(2, f=\"ten\")\n\t\t\t\tSet(3, f=\"ten\")\n\t\t\t\tSet(2, f=\"eleven\")\n\t\t\t\tSet(4, f=\"eleven\")`\n\t\t\treadQueries := []string{`Difference(Row(f=\"ten\"), Row(f=\"eleven\"))`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\tnil, pilosa.OptFieldKeys())\n\t\t\tif columns := responses[0].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{1, 3}) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnKey\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\t\tSet(\"one\", f=\"ten\")\n\t\t\t\tSet(\"two\", f=\"ten\")\n\t\t\t\tSet(\"three\", f=\"ten\")\n\t\t\t\tSet(\"two\", f=\"eleven\")\n\t\t\t\tSet(\"four\", f=\"eleven\")`\n\t\t\treadQueries := []string{`Difference(Row(f=\"ten\"), Row(f=\"eleven\"))`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{Keys: true},\n\t\t\t\tpilosa.OptFieldKeys())\n\t\t\tkeys := responses[0].Results[0].(*pilosa.Row).Keys\n\t\t\tif !sameStringSlice(keys, []string{\"one\", \"three\"}) {\n\t\t\t\tt.Fatalf(\"unexpected keys: %+v\", keys)\n\t\t\t}\n\t\t})\n\t})\n\n\tt.Run(\"Intersect\", func(t *testing.T) {\n\t\tt.Run(\"RowIDColumnKey\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\t\tSet(\"one\", f=10)\n\t\t\t\tSet(\"one-hundred\", f=10)\n\t\t\t\tSet(\"two-hundred\", f=10)\n\t\t\t\tSet(\"one\", f=11)\n\t\t\t\tSet(\"two\", f=11)\n\t\t\t\tSet(\"two-hundred\", f=11)`\n\t\t\treadQueries := []string{`Intersect(Row(f=10), Row(f=11))`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{Keys: true})\n\t\t\tkeys := responses[0].Results[0].(*pilosa.Row).Keys\n\t\t\tif !sameStringSlice(keys, []string{\"one\", \"two-hundred\"}) {\n\t\t\t\tt.Fatalf(\"unexpected keys: %+v\", keys)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnID\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\t\tSet(1, f=\"ten\")\n\t\t\t\tSet(100, f=\"ten\")\n\t\t\t\tSet(200, f=\"ten\")\n\t\t\t\tSet(1, f=\"eleven\")\n\t\t\t\tSet(2, f=\"eleven\")\n\t\t\t\tSet(200, f=\"eleven\")`\n\t\t\treadQueries := []string{`Intersect(Row(f=\"ten\"), Row(f=\"eleven\"))`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\tnil, pilosa.OptFieldKeys())\n\t\t\tif columns := responses[0].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{1, 200}) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnKey\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\t\tSet(\"one\", f=\"ten\")\n\t\t\t\tSet(\"one-hundred\", f=\"ten\")\n\t\t\t\tSet(\"two-hundred\", f=\"ten\")\n\t\t\t\tSet(\"one\", f=\"eleven\")\n\t\t\t\tSet(\"two\", f=\"eleven\")\n\t\t\t\tSet(\"two-hundred\", f=\"eleven\")`\n\t\t\treadQueries := []string{`Intersect(Row(f=\"ten\"), Row(f=\"eleven\"))`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{Keys: true},\n\t\t\t\tpilosa.OptFieldKeys())\n\t\t\tkeys := responses[0].Results[0].(*pilosa.Row).Keys\n\t\t\tif !sameStringSlice(keys, []string{\"one\", \"two-hundred\"}) {\n\t\t\t\tt.Fatalf(\"unexpected keys: %+v\", keys)\n\t\t\t}\n\t\t})\n\t})\n\n\tt.Run(\"Union\", func(t *testing.T) {\n\t\tt.Run(\"RowIDColumnKey\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\t\tSet(\"one\", f=10)\n\t\t\t\tSet(\"one-hundred\", f=10)\n\t\t\t\tSet(\"two-hundred\", f=10)\n\t\t\t\tSet(\"one\", f=11)\n\t\t\t\tSet(\"two\", f=11)\n\t\t\t\tSet(\"two-hundred\", f=11)`\n\t\t\treadQueries := []string{`Union(Row(f=10), Row(f=11))`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{Keys: true})\n\t\t\tkeys := responses[0].Results[0].(*pilosa.Row).Keys\n\t\t\tif !sameStringSlice(keys, []string{\"one\", \"two-hundred\", \"one-hundred\", \"two\"}) {\n\t\t\t\tt.Fatalf(\"unexpected keys: %+v\", keys)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnID\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\t\tSet(1, f=\"ten\")\n\t\t\t\tSet(100, f=\"ten\")\n\t\t\t\tSet(200, f=\"ten\")\n\t\t\t\tSet(1, f=\"eleven\")\n\t\t\t\tSet(2, f=\"eleven\")\n\t\t\t\tSet(200, f=\"eleven\")`\n\t\t\treadQueries := []string{`Union(Row(f=\"ten\"), Row(f=\"eleven\"))`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\tnil, pilosa.OptFieldKeys())\n\t\t\tif columns := responses[0].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{1, 2, 100, 200}) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnKey\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\t\tSet(\"one\", f=\"ten\")\n\t\t\t\tSet(\"one-hundred\", f=\"ten\")\n\t\t\t\tSet(\"two-hundred\", f=\"ten\")\n\t\t\t\tSet(\"one\", f=\"eleven\")\n\t\t\t\tSet(\"two\", f=\"eleven\")\n\t\t\t\tSet(\"two-hundred\", f=\"eleven\")`\n\t\t\treadQueries := []string{`Union(Row(f=\"ten\"), Row(f=\"eleven\"))`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{Keys: true},\n\t\t\t\tpilosa.OptFieldKeys())\n\t\t\tkeys := responses[0].Results[0].(*pilosa.Row).Keys\n\t\t\tif !sameStringSlice(keys, []string{\"two-hundred\", \"two\", \"one-hundred\", \"one\"}) {\n\t\t\t\tt.Fatalf(\"unexpected keys: %+v\", keys)\n\t\t\t}\n\t\t})\n\t})\n\n\tt.Run(\"Xor\", func(t *testing.T) {\n\t\tt.Run(\"RowIDColumnKey\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\t\tSet(\"one\", f=10)\n\t\t\t\tSet(\"one-hundred\", f=10)\n\t\t\t\tSet(\"two-hundred\", f=10)\n\t\t\t\tSet(\"one\", f=11)\n\t\t\t\tSet(\"two\", f=11)\n\t\t\t\tSet(\"two-hundred\", f=11)`\n\t\t\treadQueries := []string{`Xor(Row(f=10), Row(f=11))`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{Keys: true})\n\t\t\tkeys := responses[0].Results[0].(*pilosa.Row).Keys\n\t\t\tif !sameStringSlice(keys, []string{\"two\", \"one-hundred\"}) {\n\t\t\t\tt.Fatalf(\"unexpected keys: %+v\", keys)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnID\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\t\tSet(1, f=\"ten\")\n\t\t\t\tSet(100, f=\"ten\")\n\t\t\t\tSet(200, f=\"ten\")\n\t\t\t\tSet(1, f=\"eleven\")\n\t\t\t\tSet(2, f=\"eleven\")\n\t\t\t\tSet(200, f=\"eleven\")`\n\t\t\treadQueries := []string{`Xor(Row(f=\"ten\"), Row(f=\"eleven\"))`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\tnil, pilosa.OptFieldKeys())\n\t\t\tif columns := responses[0].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{2, 100}) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnKey\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\t\tSet(\"one\", f=\"ten\")\n\t\t\t\tSet(\"one-hundred\", f=\"ten\")\n\t\t\t\tSet(\"two-hundred\", f=\"ten\")\n\t\t\t\tSet(\"one\", f=\"eleven\")\n\t\t\t\tSet(\"two\", f=\"eleven\")\n\t\t\t\tSet(\"two-hundred\", f=\"eleven\")`\n\t\t\treadQueries := []string{`Xor(Row(f=\"ten\"), Row(f=\"eleven\"))`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{Keys: true},\n\t\t\t\tpilosa.OptFieldKeys())\n\t\t\tkeys := responses[0].Results[0].(*pilosa.Row).Keys\n\t\t\tif !sameStringSlice(keys, []string{\"two\", \"one-hundred\"}) {\n\t\t\t\tt.Fatalf(\"unexpected keys: %+v\", keys)\n\t\t\t}\n\t\t})\n\t})\n\n\tt.Run(\"Count\", func(t *testing.T) {\n\t\tt.Run(\"RowIDColumnKey\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\t\tSet(\"three\", f=10)\n\t\t\t\tSet(\"one-hundred\", f=10)\n\t\t\t\tSet(\"two-hundred\", f=11)`\n\t\t\treadQueries := []string{`Count(Row(f=10))`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{Keys: true})\n\t\t\tif responses[0].Results[0] != uint64(2) {\n\t\t\t\tt.Fatalf(\"unexpected n: %d\", responses[0].Results[0])\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnID\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\t\tSet(1, f=\"ten\")\n\t\t\t\tSet(100, f=\"ten\")\n\t\t\t\tSet(200, f=\"eleven\")`\n\t\t\treadQueries := []string{`Count(Row(f=\"ten\"))`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\tnil, pilosa.OptFieldKeys())\n\t\t\tif responses[0].Results[0] != uint64(2) {\n\t\t\t\tt.Fatalf(\"unexpected n: %d\", responses[0].Results[0])\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnKey\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\t\tSet(\"one\", f=\"ten\")\n\t\t\t\tSet(\"one-hundred\", f=\"ten\")\n\t\t\t\tSet(\"two-hundred\", f=\"eleven\")`\n\t\t\treadQueries := []string{`Count(Row(f=\"ten\"))`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{Keys: true},\n\t\t\t\tpilosa.OptFieldKeys())\n\t\t\tif responses[0].Results[0] != uint64(2) {\n\t\t\t\tt.Fatalf(\"unexpected n: %d\", responses[0].Results[0])\n\t\t\t}\n\t\t})\n\t})\n\n\tt.Run(\"Set\", func(t *testing.T) {\n\t\tt.Run(\"RowIDColumnKey\", func(t *testing.T) {\n\t\t\treadQueries := []string{`Set(\"three\", f=10)`}\n\t\t\tresponses := runCallTest(c, t, \"\", readQueries,\n\t\t\t\t&pilosa.IndexOptions{Keys: true})\n\t\t\tif !responses[0].Results[0].(bool) {\n\t\t\t\tt.Fatalf(\"expected column changed\")\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnID\", func(t *testing.T) {\n\t\t\treadQueries := []string{`Set(1, f=\"ten\")`}\n\t\t\tresponses := runCallTest(c, t, \"\", readQueries,\n\t\t\t\tnil, pilosa.OptFieldKeys())\n\t\t\tif !responses[0].Results[0].(bool) {\n\t\t\t\tt.Fatalf(\"expected column changed\")\n\t\t\t}\n\t\t})\n\t})\n\n\tt.Run(\"Clear\", func(t *testing.T) {\n\t\tt.Run(\"RowIDColumnID\", func(t *testing.T) {\n\t\t\twriteQuery := `Set(3, f=10)`\n\t\t\treadQueries := []string{`Clear(3, f=10)`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries, nil)\n\t\t\tif !responses[0].Results[0].(bool) {\n\t\t\t\tt.Fatalf(\"expected column changed\")\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"RowIDColumnKey\", func(t *testing.T) {\n\t\t\twriteQuery := `Set(\"three\", f=10)`\n\t\t\treadQueries := []string{`Clear(\"three\", f=10)`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{Keys: true})\n\t\t\tif !responses[0].Results[0].(bool) {\n\t\t\t\tt.Fatalf(\"expected column changed\")\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnID\", func(t *testing.T) {\n\t\t\twriteQuery := `Set(1, f=\"ten\")`\n\t\t\treadQueries := []string{`Clear(1, f=\"ten\")`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\tnil, pilosa.OptFieldKeys())\n\t\t\tif !responses[0].Results[0].(bool) {\n\t\t\t\tt.Fatalf(\"expected column changed\")\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnKey\", func(t *testing.T) {\n\t\t\twriteQuery := `Set(\"one\", f=\"ten\")`\n\t\t\treadQueries := []string{`Clear(\"one\", f=\"ten\")`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{Keys: true},\n\t\t\t\tpilosa.OptFieldKeys())\n\t\t\tif !responses[0].Results[0].(bool) {\n\t\t\t\tt.Fatalf(\"expected column changed\")\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnKey_NotClearNot\", func(t *testing.T) {\n\t\t\twriteQuery := `Set(\"056009039|q2db_3385|11\", f=\"all_users\")`\n\t\t\treadQueries := []string{\n\t\t\t\t`Not(Row(f=\"has_deleted_date\"))`,\n\t\t\t\t`Clear(\"056009039|q2db_3385|11\", f=\"has_deleted_date\")`,\n\t\t\t\t`Not(Row(f=\"has_deleted_date\")) `,\n\t\t\t}\n\t\t\tresults := []interface{}{\n\t\t\t\t\"056009039|q2db_3385|11\",\n\t\t\t\tfalse,\n\t\t\t\t\"056009039|q2db_3385|11\",\n\t\t\t}\n\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries, &pilosa.IndexOptions{\n\t\t\t\tKeys:           true,\n\t\t\t\tTrackExistence: true,\n\t\t\t}, pilosa.OptFieldKeys())\n\t\t\tfor i, resp := range responses {\n\t\t\t\tif len(resp.Results) != 1 {\n\t\t\t\t\tt.Fatalf(\"response %d: len(results) expected: 1, got: %d\", i, len(resp.Results))\n\t\t\t\t}\n\n\t\t\t\tswitch r := resp.Results[0].(type) {\n\t\t\t\tcase bool:\n\t\t\t\t\tif results[i] != r {\n\t\t\t\t\t\tt.Fatalf(\"response %d: expected: %v, got: %v\", i, results[i], r)\n\t\t\t\t\t}\n\n\t\t\t\tcase *pilosa.Row:\n\t\t\t\t\tif len(r.Keys) != 1 {\n\t\t\t\t\t\tt.Fatalf(\"response %d: len(keys) expected: 1, got: %d\", i, len(r.Keys))\n\t\t\t\t\t}\n\t\t\t\t\tif results[i] != r.Keys[0] {\n\t\t\t\t\t\tt.Fatalf(\"response %d: expected: %v, got: %v\", i, results[i], r.Keys[0])\n\t\t\t\t\t}\n\n\t\t\t\tdefault:\n\t\t\t\t\tt.Fatalf(\"response %d: expected: %T, got: %T\", i, results[i], r)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t})\n\n\tt.Run(\"Range\", func(t *testing.T) {\n\t\tt.Run(\"RowIDColumnID\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\tSet(2, f=1, 1999-12-31T00:00)\n\t\t\tSet(3, f=1, 2000-01-01T00:00)\n\t\t\tSet(4, f=1, 2000-01-02T00:00)\n\t\t\tSet(5, f=1, 2000-02-01T00:00)\n\t\t\tSet(6, f=1, 2001-01-01T00:00)\n\t\t\tSet(7, f=1, 2002-01-01T02:00)\n\n\t\t\tSet(2, f=1, 1999-12-30T00:00)\n\t\t\tSet(2, f=1, 2002-02-01T00:00)\n\t\t\tSet(2, f=10, 2001-01-01T00:00)`\n\t\t\treadQueries := []string{\n\t\t\t\t`Row(f=1, from=1999-12-31T00:00, to=2002-01-01T03:00)`,\n\t\t\t\t`Row(f=1, from=1999-12-31T00:00)`,\n\t\t\t\t`Row(f=1, to=2002-01-01T02:00)`,\n\t\t\t\t`Clear( 2, f=1)`,\n\t\t\t\t`Row(f=1, from=1999-12-31T00:00, to=2002-01-01T03:00)`,\n\t\t\t}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\tnil, pilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMDH\"), \"0\"))\n\n\t\t\tt.Run(\"Standard\", func(t *testing.T) {\n\t\t\t\tif columns := responses[0].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{2, 3, 4, 5, 6, 7}) {\n\t\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"From\", func(t *testing.T) {\n\t\t\t\tif columns := responses[1].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{2, 3, 4, 5, 6, 7}) {\n\t\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"To\", func(t *testing.T) {\n\t\t\t\tif columns := responses[2].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{2, 3, 4, 5, 6}) {\n\t\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"Clear\", func(t *testing.T) {\n\t\t\t\tif columns := responses[4].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{3, 4, 5, 6, 7}) {\n\t\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t\t}\n\t\t\t})\n\t\t})\n\n\t\tt.Run(\"RowIDColumnKey\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\tSet(\"two\", f=1, 1999-12-31T00:00)\n\t\t\tSet(\"three\", f=1, 2000-01-01T00:00)\n\t\t\tSet(\"four\", f=1, 2000-01-02T00:00)\n\t\t\tSet(\"five\", f=1, 2000-02-01T00:00)\n\t\t\tSet(\"six\", f=1, 2001-01-01T00:00)\n\t\t\tSet(\"seven\", f=1, 2002-01-01T02:00)\n\n\t\t\tSet(\"two\", f=1, 1999-12-30T00:00)\n\t\t\tSet(\"two\", f=1, 2002-02-01T00:00)\n\t\t\tSet(\"two\", f=10, 2001-01-01T00:00)`\n\t\t\treadQueries := []string{\n\t\t\t\t`Row(f=1, from=1999-12-31T00:00, to=2002-01-01T03:00)`,\n\t\t\t\t`Clear(\"two\", f=1)`,\n\t\t\t\t`Row(f=1, from=1999-12-31T00:00, to=2002-01-01T03:00)`,\n\t\t\t}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{Keys: true},\n\t\t\t\tpilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMDH\"), \"0\"))\n\n\t\t\tt.Run(\"Standard\", func(t *testing.T) {\n\t\t\t\tkeys := responses[0].Results[0].(*pilosa.Row).Keys\n\t\t\t\tif !sameStringSlice(keys, []string{\"six\", \"four\", \"five\", \"seven\", \"two\", \"three\"}) {\n\t\t\t\t\tt.Fatalf(\"unexpected keys: %+v\", keys)\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"Clear\", func(t *testing.T) {\n\t\t\t\tkeys := responses[2].Results[0].(*pilosa.Row).Keys\n\t\t\t\tif !sameStringSlice(keys, []string{\"six\", \"four\", \"five\", \"seven\", \"three\"}) {\n\t\t\t\t\tt.Fatalf(\"unexpected keys: %+v\", keys)\n\t\t\t\t}\n\t\t\t})\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnID\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\tSet(2, f=\"foo\", 1999-12-31T00:00)\n\t\t\tSet(3, f=\"foo\", 2000-01-01T00:00)\n\t\t\tSet(4, f=\"foo\", 2000-01-02T00:00)\n\t\t\tSet(5, f=\"foo\", 2000-02-01T00:00)\n\t\t\tSet(6, f=\"foo\", 2001-01-01T00:00)\n\t\t\tSet(7, f=\"foo\", 2002-01-01T02:00)\n\n\t\t\tSet(2, f=\"foo\", 1999-12-30T00:00)\n\t\t\tSet(2, f=\"foo\", 2002-02-01T00:00)\n\t\t\tSet(2, f=\"bar\", 2001-01-01T00:00)`\n\t\t\treadQueries := []string{\n\t\t\t\t`Row(f=\"foo\", from=1999-12-31T00:00, to=2002-01-01T03:00)`,\n\t\t\t\t`Clear( 2, f=\"foo\")`,\n\t\t\t\t`Row(f=\"foo\", from=1999-12-31T00:00, to=2002-01-01T03:00)`,\n\t\t\t}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\tnil,\n\t\t\t\tpilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMDH\"), \"0\"),\n\t\t\t\tpilosa.OptFieldKeys())\n\n\t\t\tt.Run(\"Standard\", func(t *testing.T) {\n\t\t\t\tif columns := responses[0].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{2, 3, 4, 5, 6, 7}) {\n\t\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"Clear\", func(t *testing.T) {\n\t\t\t\tif columns := responses[2].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{3, 4, 5, 6, 7}) {\n\t\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t\t}\n\t\t\t})\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnKey\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\tSet(\"two\", f=\"foo\", 1999-12-31T00:00)\n\t\t\tSet(\"three\", f=\"foo\", 2000-01-01T00:00)\n\t\t\tSet(\"four\", f=\"foo\", 2000-01-02T00:00)\n\t\t\tSet(\"five\", f=\"foo\", 2000-02-01T00:00)\n\t\t\tSet(\"six\", f=\"foo\", 2001-01-01T00:00)\n\t\t\tSet(\"seven\", f=\"foo\", 2002-01-01T02:00)\n\n\t\t\tSet(\"two\", f=\"foo\", 1999-12-30T00:00)\n\t\t\tSet(\"two\", f=\"foo\", 2002-02-01T00:00)\n\t\t\tSet(\"two\", f=\"bar\", 2001-01-01T00:00)`\n\t\t\treadQueries := []string{\n\t\t\t\t`Row(f=\"foo\", from=1999-12-31T00:00, to=2002-01-01T03:00)`,\n\t\t\t\t`Clear(\"two\", f=\"foo\")`,\n\t\t\t\t`Row(f=\"foo\", from=1999-12-31T00:00, to=2002-01-01T03:00)`,\n\t\t\t}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{Keys: true},\n\t\t\t\tpilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMDH\"), \"0\"),\n\t\t\t\tpilosa.OptFieldKeys())\n\n\t\t\tt.Run(\"Standard\", func(t *testing.T) {\n\t\t\t\tkeys := responses[0].Results[0].(*pilosa.Row).Keys\n\t\t\t\tif !sameStringSlice(keys, []string{\"three\", \"two\", \"five\", \"seven\", \"six\", \"four\"}) {\n\t\t\t\t\tt.Fatalf(\"unexpected keys: %+v\", keys)\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"Clear\", func(t *testing.T) {\n\t\t\t\tkeys := responses[2].Results[0].(*pilosa.Row).Keys\n\t\t\t\tif !sameStringSlice(keys, []string{\"three\", \"five\", \"seven\", \"six\", \"four\"}) {\n\t\t\t\t\tt.Fatalf(\"unexpected keys: %+v\", keys)\n\t\t\t\t}\n\t\t\t})\n\t\t})\n\n\t\tt.Run(\"UnixTimestamp\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\tSet(2, f=1, 1999-12-31T00:00)\n\t\t\tSet(3, f=1, 2000-01-01T00:00)\n\t\t\tSet(4, f=1, 2000-01-02T00:00)\n\t\t\tSet(5, f=1, 2000-02-01T00:00)\n\t\t\tSet(6, f=1, 2001-01-01T00:00)\n\t\t\tSet(7, f=1, 2002-01-01T02:00)\n\n\t\t\tSet(2, f=1, 1999-12-30T00:00)\n\t\t\tSet(2, f=1, 2002-02-01T00:00)\n\t\t\tSet(2, f=10, 2001-01-01T00:00)`\n\t\t\treadQueries := []string{\n\t\t\t\t`Row(f=1, from=946598400, to=1009854000)`,\n\t\t\t\t`Clear( 2, f=1)`,\n\t\t\t\t`Row(f=1, from=946598400, to=1009854000)`,\n\t\t\t}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\tnil, pilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMDH\"), \"0\"))\n\n\t\t\tt.Run(\"Standard\", func(t *testing.T) {\n\t\t\t\tif columns := responses[0].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{2, 3, 4, 5, 6, 7}) {\n\t\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"Clear\", func(t *testing.T) {\n\t\t\t\tif columns := responses[2].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{3, 4, 5, 6, 7}) {\n\t\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t\t}\n\t\t\t})\n\t\t})\n\t})\n\n\tt.Run(\"Range_Deprecated\", func(t *testing.T) {\n\t\tt.Run(\"RowIDColumnID\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\tSet(2, f=1, 1999-12-31T00:00)\n\t\t\tSet(3, f=1, 2000-01-01T00:00)\n\t\t\tSet(4, f=1, 2000-01-02T00:00)\n\t\t\tSet(5, f=1, 2000-02-01T00:00)\n\t\t\tSet(6, f=1, 2001-01-01T00:00)\n\t\t\tSet(7, f=1, 2002-01-01T02:00)\n\n\t\t\tSet(2, f=1, 1999-12-30T00:00)\n\t\t\tSet(2, f=1, 2002-02-01T00:00)\n\t\t\tSet(2, f=10, 2001-01-01T00:00)`\n\t\t\treadQueries := []string{\n\t\t\t\t`Range(f=1, from=1999-12-31T00:00, to=2002-01-01T03:00)`,\n\t\t\t\t`Clear( 2, f=1)`,\n\t\t\t\t`Range(f=1, from=1999-12-31T00:00, to=2002-01-01T03:00)`,\n\t\t\t}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\tnil, pilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMDH\"), \"0\"))\n\n\t\t\tt.Run(\"Standard\", func(t *testing.T) {\n\t\t\t\tif columns := responses[0].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{2, 3, 4, 5, 6, 7}) {\n\t\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"Clear\", func(t *testing.T) {\n\t\t\t\tif columns := responses[2].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{3, 4, 5, 6, 7}) {\n\t\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"OtherRange\", func(t *testing.T) {\n\t\t\t\trq2 := []string{\n\t\t\t\t\t`Range(f=1, 1999-12-31T00:00, 2002-01-01T03:00)`,\n\t\t\t\t}\n\t\t\t\tresponses = runCallTest(c, t, writeQuery, rq2,\n\t\t\t\t\tnil, pilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMDH\"), \"0\"))\n\t\t\t\tt.Run(\"OldRange\", func(t *testing.T) {\n\t\t\t\t\tif columns := responses[0].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{2, 3, 4, 5, 6, 7}) {\n\t\t\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t})\n\t\t})\n\n\t\tt.Run(\"RowIDColumnKey\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\tSet(\"two\", f=1, 1999-12-31T00:00)\n\t\t\tSet(\"three\", f=1, 2000-01-01T00:00)\n\t\t\tSet(\"four\", f=1, 2000-01-02T00:00)\n\t\t\tSet(\"five\", f=1, 2000-02-01T00:00)\n\t\t\tSet(\"six\", f=1, 2001-01-01T00:00)\n\t\t\tSet(\"seven\", f=1, 2002-01-01T02:00)\n\n\t\t\tSet(\"two\", f=1, 1999-12-30T00:00)\n\t\t\tSet(\"two\", f=1, 2002-02-01T00:00)\n\t\t\tSet(\"two\", f=10, 2001-01-01T00:00)`\n\t\t\treadQueries := []string{\n\t\t\t\t`Range(f=1, from=1999-12-31T00:00, to=2002-01-01T03:00)`,\n\t\t\t\t`Clear(\"two\", f=1)`,\n\t\t\t\t`Range(f=1, from=1999-12-31T00:00, to=2002-01-01T03:00)`,\n\t\t\t}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{Keys: true},\n\t\t\t\tpilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMDH\"), \"0\"))\n\n\t\t\tt.Run(\"Standard\", func(t *testing.T) {\n\t\t\t\tkeys := responses[0].Results[0].(*pilosa.Row).Keys\n\t\t\t\tif !sameStringSlice(keys, []string{\"two\", \"three\", \"seven\", \"four\", \"five\", \"six\"}) {\n\t\t\t\t\tt.Fatalf(\"unexpected keys: %+v\", keys)\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"Clear\", func(t *testing.T) {\n\t\t\t\tkeys := responses[2].Results[0].(*pilosa.Row).Keys\n\t\t\t\tif !sameStringSlice(keys, []string{\"three\", \"seven\", \"four\", \"five\", \"six\"}) {\n\t\t\t\t\tt.Fatalf(\"unexpected keys: %+v\", keys)\n\t\t\t\t}\n\t\t\t})\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnID\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\tSet(2, f=\"foo\", 1999-12-31T00:00)\n\t\t\tSet(3, f=\"foo\", 2000-01-01T00:00)\n\t\t\tSet(4, f=\"foo\", 2000-01-02T00:00)\n\t\t\tSet(5, f=\"foo\", 2000-02-01T00:00)\n\t\t\tSet(6, f=\"foo\", 2001-01-01T00:00)\n\t\t\tSet(7, f=\"foo\", 2002-01-01T02:00)\n\n\t\t\tSet(2, f=\"foo\", 1999-12-30T00:00)\n\t\t\tSet(2, f=\"foo\", 2002-02-01T00:00)\n\t\t\tSet(2, f=\"bar\", 2001-01-01T00:00)`\n\t\t\treadQueries := []string{\n\t\t\t\t`Range(f=\"foo\", from=1999-12-31T00:00, to=2002-01-01T03:00)`,\n\t\t\t\t`Clear( 2, f=\"foo\")`,\n\t\t\t\t`Range(f=\"foo\", from=1999-12-31T00:00, to=2002-01-01T03:00)`,\n\t\t\t}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\tnil,\n\t\t\t\tpilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMDH\"), \"0\"),\n\t\t\t\tpilosa.OptFieldKeys())\n\n\t\t\tt.Run(\"Standard\", func(t *testing.T) {\n\t\t\t\tif columns := responses[0].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{2, 3, 4, 5, 6, 7}) {\n\t\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"Clear\", func(t *testing.T) {\n\t\t\t\tif columns := responses[2].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{3, 4, 5, 6, 7}) {\n\t\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t\t}\n\t\t\t})\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnKey\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\tSet(\"two\", f=\"foo\", 1999-12-31T00:00)\n\t\t\tSet(\"three\", f=\"foo\", 2000-01-01T00:00)\n\t\t\tSet(\"four\", f=\"foo\", 2000-01-02T00:00)\n\t\t\tSet(\"five\", f=\"foo\", 2000-02-01T00:00)\n\t\t\tSet(\"six\", f=\"foo\", 2001-01-01T00:00)\n\t\t\tSet(\"seven\", f=\"foo\", 2002-01-01T02:00)\n\n\t\t\tSet(\"two\", f=\"foo\", 1999-12-30T00:00)\n\t\t\tSet(\"two\", f=\"foo\", 2002-02-01T00:00)\n\t\t\tSet(\"two\", f=\"bar\", 2001-01-01T00:00)`\n\t\t\treadQueries := []string{\n\t\t\t\t`Range(f=\"foo\", from=1999-12-31T00:00, to=2002-01-01T03:00)`,\n\t\t\t\t`Clear(\"two\", f=\"foo\")`,\n\t\t\t\t`Range(f=\"foo\", from=1999-12-31T00:00, to=2002-01-01T03:00)`,\n\t\t\t}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{Keys: true},\n\t\t\t\tpilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMDH\"), \"0\"),\n\t\t\t\tpilosa.OptFieldKeys())\n\n\t\t\tt.Run(\"Standard\", func(t *testing.T) {\n\t\t\t\tkeys := responses[0].Results[0].(*pilosa.Row).Keys\n\t\t\t\tif !sameStringSlice(keys, []string{\"three\", \"five\", \"six\", \"two\", \"seven\", \"four\"}) {\n\t\t\t\t\tt.Fatalf(\"unexpected keys: %+v\", keys)\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"Clear\", func(t *testing.T) {\n\t\t\t\tkeys := responses[2].Results[0].(*pilosa.Row).Keys\n\t\t\t\tif !sameStringSlice(keys, []string{\"three\", \"five\", \"six\", \"seven\", \"four\"}) {\n\t\t\t\t\tt.Fatalf(\"unexpected keys: %+v\", keys)\n\t\t\t\t}\n\t\t\t})\n\t\t})\n\t})\n\n\tt.Run(\"Options\", func(t *testing.T) {\n\t\tt.Run(\"shards\", func(t *testing.T) {\n\t\t\twriteQuery := fmt.Sprintf(`\n\t\t\t\tSet(100, f=10)\n\t\t\t\tSet(%d, f=10)\n\t\t\t\tSet(%d, f=10)`, ShardWidth, ShardWidth*2)\n\t\t\treadQueries := []string{`Options(Row(f=10), shards=[0, 2])`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries, nil)\n\t\t\tif bits := responses[0].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{100, ShardWidth * 2}) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t\t}\n\t\t})\n\t})\n\n\tt.Run(\"Not\", func(t *testing.T) {\n\t\tt.Run(\"RowIDColumnID\", func(t *testing.T) {\n\t\t\twriteQuery := `` +\n\t\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", 3, 10) +\n\t\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", ShardWidth+1, 10) +\n\t\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", ShardWidth+2, 20)\n\t\t\treadQueries := []string{\n\t\t\t\t`Not(Row(f=20))`,\n\t\t\t\t`Not(Row(f=0))`,\n\t\t\t\t`Not(Union(Row(f=10), Row(f=20)))`,\n\t\t\t}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{TrackExistence: true})\n\n\t\t\tif bits := responses[0].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{3, ShardWidth + 1}) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t\t}\n\n\t\t\tif bits := responses[1].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{3, ShardWidth + 1, ShardWidth + 2}) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t\t}\n\n\t\t\tif bits := responses[2].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{}) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"RowIDColumnKey\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\t\tSet(\"three\", f=10)\n\t\t\t\tSet(\"sw1\", f=10)\n\t\t\t\tSet(\"sw2\", f=20)`\n\t\t\treadQueries := []string{`Not(Row(f=20))`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{\n\t\t\t\t\tTrackExistence: true,\n\t\t\t\t\tKeys:           true,\n\t\t\t\t})\n\t\t\tkeys := responses[0].Results[0].(*pilosa.Row).Keys\n\t\t\tif !sameStringSlice(keys, []string{\"three\", \"sw1\"}) {\n\t\t\t\tt.Fatalf(\"unexpected keys: %+v\", keys)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnID\", func(t *testing.T) {\n\t\t\twriteQuery := fmt.Sprintf(`\n\t\t\t\tSet(3, f=\"ten\")\n\t\t\t\tSet(%d, f=\"ten\")\n\t\t\t\tSet(%d, f=\"twenty\")`, ShardWidth+1, ShardWidth+2)\n\t\t\treadQueries := []string{`Not(Row(f=\"twenty\"))`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{TrackExistence: true},\n\t\t\t\tpilosa.OptFieldKeys(),\n\t\t\t)\n\t\t\tif cols := responses[0].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(cols, []uint64{3, ShardWidth + 1}) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", cols)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"RowKeyColumnKey\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\t\t\t\tSet(\"three\", f=\"ten\")\n\t\t\t\tSet(\"sw1\", f=\"ten\")\n\t\t\t\tSet(\"sw2\", f=\"twenty\")`\n\t\t\treadQueries := []string{`Not(Row(f=\"twenty\"))`}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{\n\t\t\t\t\tTrackExistence: true,\n\t\t\t\t\tKeys:           true,\n\t\t\t\t}, pilosa.OptFieldKeys())\n\t\t\tkeys := responses[0].Results[0].(*pilosa.Row).Keys\n\t\t\tif !sameStringSlice(keys, []string{\"sw1\", \"three\"}) {\n\t\t\t\tt.Fatalf(\"unexpected keys: %+v\", keys)\n\t\t\t}\n\t\t})\n\t})\n\n\tt.Run(\"ClearRow\", func(t *testing.T) {\n\t\t// Set and Mutex tests use the same data and queries\n\t\twriteQuery := `` +\n\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", 3, 10) +\n\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", ShardWidth-1, 10) +\n\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", ShardWidth+1, 10) +\n\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", 1, 20) +\n\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", ShardWidth+1, 20)\n\n\t\treadQueries := []string{\n\t\t\t`Row(f=10)`,\n\t\t\t`ClearRow(f=10)`,\n\t\t\t`ClearRow(f=10)`,\n\t\t\t`Row(f=10)`,\n\t\t\t`Row(f=20)`,\n\t\t}\n\n\t\tt.Run(\"Set\", func(t *testing.T) {\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{TrackExistence: true})\n\t\t\tif bits := responses[0].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{3, ShardWidth - 1, ShardWidth + 1}) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t\t}\n\n\t\t\t// Clear the row and ensure we get a `true` response.\n\t\t\tif res := responses[1].Results[0].(bool); !res {\n\t\t\t\tt.Fatalf(\"unexpected clear row result: %+v\", res)\n\t\t\t}\n\t\t\t// Clear the row again and ensure we get a `false` response.\n\t\t\tif res := responses[2].Results[0].(bool); res {\n\t\t\t\tt.Fatalf(\"unexpected clear row result: %+v\", res)\n\t\t\t}\n\n\t\t\t// Ensure the row is empty.\n\t\t\tif bits := responses[3].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{}) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t\t}\n\n\t\t\t// Ensure other rows were not affected.\n\t\t\tif bits := responses[4].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{1, ShardWidth + 1}) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"Mutex\", func(t *testing.T) {\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{TrackExistence: true},\n\t\t\t\tpilosa.OptFieldTypeMutex(\"none\", 0))\n\t\t\tif bits := responses[0].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{3, ShardWidth - 1}) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t\t}\n\n\t\t\t// Clear the row and ensure we get a `true` response.\n\t\t\tif res := responses[1].Results[0].(bool); !res {\n\t\t\t\tt.Fatalf(\"unexpected clear row result: %+v\", res)\n\t\t\t}\n\n\t\t\t// Clear the row again and ensure we get a `false` response.\n\t\t\tif res := responses[2].Results[0].(bool); res {\n\t\t\t\tt.Fatalf(\"unexpected clear row result: %+v\", res)\n\t\t\t}\n\n\t\t\t// Ensure the row is empty.\n\t\t\tif bits := responses[3].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{}) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t\t}\n\n\t\t\t// Ensure other rows were not affected.\n\t\t\tif bits := responses[4].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{1, ShardWidth + 1}) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"Time\", func(t *testing.T) {\n\t\t\twriteQuery := `\n\tSet(2, f=1, 1999-12-31T00:00)\n\tSet(3, f=1, 2000-01-01T00:00)\n\tSet(4, f=1, 2000-01-02T00:00)\n\tSet(5, f=1, 2000-02-01T00:00)\n\tSet(6, f=1, 2001-01-01T00:00)\n\tSet(7, f=1, 2002-01-01T02:00)\n\n\tSet(2, f=1, 1999-12-30T00:00)\n\tSet(2, f=1, 2002-02-01T00:00)\n\tSet(2, f=10, 2001-01-01T00:00)`\n\t\t\treadQueries := []string{\n\t\t\t\t`Row(f=1, from=1999-12-31T00:00, to=2003-01-01T03:00)`,\n\t\t\t\t`Row(f=1, from=2002-01-01T00:00, to=2002-01-02T00:00)`,\n\t\t\t\t`ClearRow(f=1)`,\n\t\t\t\t`Row(f=1, from=1999-12-31T00:00, to=2003-01-01T03:00)`,\n\t\t\t\t`Row(f=10, from=1999-12-31T00:00, to=2003-01-01T03:00)`,\n\t\t\t}\n\t\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\t\t&pilosa.IndexOptions{TrackExistence: true},\n\t\t\t\tpilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMD\"), \"0\"))\n\t\t\tif columns := responses[0].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{2, 3, 4, 5, 6, 7}) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t}\n\n\t\t\t// Single day query (regression test)\n\t\t\tif columns := responses[1].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{7}) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t}\n\n\t\t\t// Clear the row and ensure we get a `true` response.\n\t\t\tif res := responses[2].Results[0].(bool); !res {\n\t\t\t\tt.Fatalf(\"unexpected clear row result: %+v\", res)\n\t\t\t}\n\n\t\t\t// Ensure the row is empty.\n\t\t\tif columns := responses[3].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{}) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t}\n\n\t\t\t// Ensure other rows were not affected.\n\t\t\tif columns := responses[4].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{2}) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t}\n\t\t})\n\t\t// Ensure that ClearRow returns false when the row to clear needs translation.\n\t\tt.Run(\"WithKeys\", func(t *testing.T) {\n\t\t\twq := \"\"\n\t\t\trq := []string{\n\t\t\t\t`ClearRow(f=\"bar\")`,\n\t\t\t}\n\n\t\t\tresponses := runCallTest(c, t, wq, rq, &pilosa.IndexOptions{}, pilosa.OptFieldKeys())\n\t\t\tif res := responses[0].Results[0].(bool); res {\n\t\t\t\tt.Fatalf(\"unexpected result: %+v\", res)\n\t\t\t}\n\t\t})\n\t})\n\n\tt.Run(\"RowsTime\", func(t *testing.T) {\n\t\twriteQuery := fmt.Sprintf(`\n\t\tSet(9, f=1, 2001-01-01T00:00)\n\t\tSet(9, f=2, 2002-01-01T00:00)\n\t\tSet(9, f=3, 2003-01-01T00:00)\n\t\tSet(9, f=4, 2004-01-01T00:00)\n\n\t\tSet(%d, f=13, 2003-02-02T00:00)\n\t\t`, pilosa.ShardWidth+9)\n\t\treadQueries := []string{\n\t\t\t`Rows(f, from=1999-12-31T00:00, to=2002-01-01T03:00)`,\n\t\t\t`Rows(f, from=2002-01-01T00:00, to=2004-01-01T00:00)`,\n\t\t\t`Rows(f, from=1990-01-01T00:00, to=1999-01-01T00:00)`,\n\t\t\t`Rows(f)`,\n\t\t\t`Rows(f, from=2002-01-01T00:00)`,\n\t\t\t`Rows(f, to=2003-02-03T00:00)`,\n\t\t\t`Rows(f, from=2002-01-01T00:00, to=2002-01-02T00:00)`,\n\t\t}\n\t\texpResults := [][]uint64{\n\t\t\t{1},\n\t\t\t{2, 3, 13},\n\t\t\t{},\n\t\t\t{1, 2, 3, 4, 13},\n\t\t\t{2, 3, 4, 13},\n\t\t\t{1, 2, 3, 13},\n\t\t\t{2},\n\t\t}\n\n\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\tnil, pilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMD\"), \"0\", true))\n\n\t\tfor i := range responses {\n\t\t\tt.Run(fmt.Sprintf(\"response-%d\", i), func(t *testing.T) {\n\t\t\t\trows := responses[i].Results[0].(pilosa.RowIdentifiers)\n\t\t\t\trows.AssertEqual(t, &pilosa.RowIdentifiers{Rows: expResults[i]})\n\t\t\t})\n\t\t}\n\t})\n\n\t// This is a regression test which checks that queries over a time\n\t// range that encompass all available time views do not default to\n\t// using the standard view. This can be incorrect in the case\n\t// where some time views have been deleted.\n\tt.Run(\"TimeQueriesFullRange\", func(t *testing.T) {\n\t\tts := func(t time.Time) int64 {\n\t\t\treturn t.Unix() * 1e+9\n\t\t}\n\t\tindexName := c.Idx(\"tq_range\")\n\t\tc.CreateField(t, indexName, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"f1\", pilosa.OptFieldKeys(), pilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"D\"), \"0\"))\n\t\tc.ImportTimeQuantumKey(t, indexName, \"f1\", []test.TimeQuantumKey{\n\t\t\t// from edge cases\n\t\t\t{ColKey: \"C1\", RowKey: \"R1\", Ts: ts(time.Date(2022, 1, 10, 0, 0, 0, 0, time.UTC))},\n\t\t\t{ColKey: \"C2\", RowKey: \"R1\", Ts: ts(time.Date(2022, 1, 11, 0, 0, 0, 0, time.UTC))},\n\t\t\t{ColKey: \"C3\", RowKey: \"R1\", Ts: ts(time.Date(2022, 1, 12, 0, 0, 0, 0, time.UTC))},\n\t\t})\n\t\tc.ImportKeyKey(t, indexName, \"f1\", [][2]string{\n\t\t\t{\"R1\", \"C2\"},\n\t\t\t{\"R1\", \"C3\"},\n\t\t\t{\"R1\", \"C4\"},\n\t\t\t{\"R1\", \"C5\"},\n\t\t\t{\"R1\", \"C6\"},\n\t\t})\n\n\t\ttests := []struct {\n\t\t\tquery    string\n\t\t\texpected string\n\t\t}{\n\t\t\t{\n\t\t\t\tquery: \"Row(f1=R1, from='2022-01-10', to='2022-01-13')\",\n\t\t\t\texpected: `\n_id\nC1\nC3\nC2\n`[1:],\n\t\t\t},\n\t\t\t{\n\t\t\t\tquery: \"Row(f1=R1)\",\n\t\t\t\texpected: `\n_id\nC1\nC3\nC2\nC5\nC4\nC6\n`[1:],\n\t\t\t},\n\t\t\t{\n\t\t\t\tquery:    \"Row(f1=R1, to='2022-01-09')\",\n\t\t\t\texpected: \"\\n\",\n\t\t\t},\n\t\t}\n\n\t\tfor i, tst := range tests {\n\t\t\ttr := c.QueryGRPC(t, indexName, tst.query)\n\t\t\tcsvString, err := tableResponseToCSVString(tr)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"test: %d, converting to CSV: %v\", i, err)\n\t\t\t}\n\t\t\t// check that an unqualified query does use the standard view\n\t\t\tif csvString != tst.expected {\n\t\t\t\tt.Fatalf(\"%d expected:\\n'%s'\\ngot:\\n'%s'\\n\", i, tst.expected, csvString)\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc runCallTest(c *test.Cluster, t *testing.T, writeQuery string, readQueries []string, indexOptions *pilosa.IndexOptions, fieldOption ...pilosa.FieldOption) []pilosa.QueryResponse {\n\tt.Helper()\n\tindexName := c.Idx(t.Name())\n\n\tif indexOptions == nil {\n\t\tindexOptions = &pilosa.IndexOptions{}\n\t}\n\n\thldr := c.GetHolder(0)\n\tindex, err := hldr.CreateIndex(indexName, \"\", *indexOptions)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\t_, err = index.CreateField(\"f\", \"\", fieldOption...)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif writeQuery != \"\" {\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{\n\t\t\tIndex: indexName,\n\t\t\tQuery: writeQuery,\n\t\t}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tresponses := make([]pilosa.QueryResponse, len(readQueries))\n\tfor i, query := range readQueries {\n\t\tres, err := c.GetNode(0).API.Query(context.Background(),\n\t\t\t&pilosa.QueryRequest{\n\t\t\t\tIndex: indexName,\n\t\t\t\tQuery: query,\n\t\t\t})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tresponses[i] = res\n\t}\n\n\terr = c.GetNode(0).API.DeleteIndex(context.TODO(), indexName)\n\tif err != nil {\n\t\tt.Fatalf(\"cleaning up index: %v\", err)\n\t}\n\treturn responses\n}\n\nfunc TestExecutor_Execute_ConstRow(t *testing.T) {\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\n\t// without track existnce you just get back the columns you request\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"h\")\n\tc.ImportBits(t, c.Idx(), \"h\", [][2]uint64{\n\t\t{1, 2},\n\t\t{3, 4},\n\t\t{5, 6},\n\t})\n\n\tresp := c.Query(t, c.Idx(), `ConstRow(columns=[2,6,7])`)\n\texpect := []uint64{2, 6, 7}\n\tgot := resp.Results[0].(*pilosa.Row).Columns()\n\tif !reflect.DeepEqual(expect, got) {\n\t\tt.Errorf(\"expected %v but got %v\", expect, got)\n\t}\n}\n\nfunc TestExecutor_Execute_ConstRowTrackExistence(t *testing.T) {\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\n\t// with track existnce you\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{TrackExistence: true}, \"h\")\n\tc.ImportBits(t, c.Idx(), \"h\", [][2]uint64{\n\t\t{1, 2},\n\t\t{3, 4},\n\t\t{5, 6},\n\t})\n\n\tresp := c.Query(t, c.Idx(), `ConstRow(columns=[2,6,7])`)\n\texpect := []uint64{2, 6}\n\tgot := resp.Results[0].(*pilosa.Row).Columns()\n\tif !reflect.DeepEqual(expect, got) {\n\t\tt.Errorf(\"expected %v but got %v\", expect, got)\n\t}\n}\n\n// Ensure a difference query can be executed.\nfunc TestExecutor_Execute_Difference(t *testing.T) {\n\tt.Run(\"RowIDColumnID\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\thldr.SetBit(c.Idx(), \"general\", 10, 1)\n\t\thldr.SetBit(c.Idx(), \"general\", 10, 2)\n\t\thldr.SetBit(c.Idx(), \"general\", 10, 3)\n\t\thldr.SetBit(c.Idx(), \"general\", 11, 2)\n\t\thldr.SetBit(c.Idx(), \"general\", 11, 4)\n\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Difference(Row(general=10), Row(general=11))`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if columns := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{1, 3}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t}\n\t})\n}\n\n// Ensure an empty difference query behaves properly.\nfunc TestExecutor_Execute_Empty_Difference(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\thldr := c.GetHolder(0)\n\thldr.SetBit(c.Idx(), \"general\", 10, 1)\n\n\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Difference()`}); err == nil {\n\t\tt.Fatalf(\"Empty Difference query should give error, but got %v\", res)\n\t}\n}\n\n// Ensure an intersect query can be executed.\nfunc TestExecutor_Execute_Intersect(t *testing.T) {\n\tt.Run(\"RowIDColumnID\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\t\thldr.SetBit(c.Idx(), \"general\", 10, 1)\n\t\thldr.SetBit(c.Idx(), \"general\", 10, ShardWidth+1)\n\t\thldr.SetBit(c.Idx(), \"general\", 10, ShardWidth+2)\n\t\thldr.SetBit(c.Idx(), \"general\", 11, 1)\n\t\thldr.SetBit(c.Idx(), \"general\", 11, 2)\n\t\thldr.SetBit(c.Idx(), \"general\", 11, ShardWidth+2)\n\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Intersect(Row(general=10), Row(general=11))`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if columns := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{1, ShardWidth + 2}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t}\n\t})\n}\n\n// Ensure an empty intersect query behaves properly.\nfunc TestExecutor_Execute_Empty_Intersect(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\n\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Intersect()`}); err == nil {\n\t\tt.Fatalf(\"Empty Intersect query should give error, but got %v\", res)\n\t}\n}\n\n// Ensure a union query can be executed.\nfunc TestExecutor_Execute_Union(t *testing.T) {\n\tt.Run(\"RowIDColumnID\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\t\thldr.SetBit(c.Idx(), \"general\", 10, 0)\n\t\thldr.SetBit(c.Idx(), \"general\", 10, ShardWidth+1)\n\t\thldr.SetBit(c.Idx(), \"general\", 10, ShardWidth+2)\n\n\t\thldr.SetBit(c.Idx(), \"general\", 11, 2)\n\t\thldr.SetBit(c.Idx(), \"general\", 11, ShardWidth+2)\n\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Union(Row(general=10), Row(general=11))`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if columns := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{0, 2, ShardWidth + 1, ShardWidth + 2}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t}\n\t})\n}\n\n// Ensure an empty union query behaves properly.\nfunc TestExecutor_Execute_Empty_Union(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\thldr := c.GetHolder(0)\n\thldr.SetBit(c.Idx(), \"general\", 10, 0)\n\n\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Union()`}); err != nil {\n\t\tt.Fatal(err)\n\t} else if columns := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{}) {\n\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t}\n}\n\n// Ensure a xor query can be executed.\nfunc TestExecutor_Execute_Xor(t *testing.T) {\n\tt.Run(\"RowIDColumnID\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\thldr.SetBit(c.Idx(), \"general\", 10, 0)\n\t\thldr.SetBit(c.Idx(), \"general\", 10, ShardWidth+1)\n\t\thldr.SetBit(c.Idx(), \"general\", 10, ShardWidth+2)\n\n\t\thldr.SetBit(c.Idx(), \"general\", 11, 2)\n\t\thldr.SetBit(c.Idx(), \"general\", 11, ShardWidth+2)\n\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Xor(Row(general=10), Row(general=11))`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if columns := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{0, 2, ShardWidth + 1}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t}\n\t})\n}\n\n// Ensure a count query can be executed.\nfunc TestExecutor_Execute_Count(t *testing.T) {\n\tt.Run(\"RowIDColumnID\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\thldr.SetBit(c.Idx(), \"f\", 10, 3)\n\t\thldr.SetBit(c.Idx(), \"f\", 10, ShardWidth+1)\n\t\thldr.SetBit(c.Idx(), \"f\", 10, ShardWidth+2)\n\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Count(Row(f=10))`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if res.Results[0] != uint64(3) {\n\t\t\tt.Fatalf(\"unexpected n: %d\", res.Results[0])\n\t\t}\n\t})\n}\n\n// Ensure a set query can be executed.\nfunc TestExecutor_Execute_Set(t *testing.T) {\n\tt.Run(\"RowIDColumnID\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\tcmd := c.GetNode(0)\n\t\thldr := c.GetHolder(0)\n\t\thldr.SetBit(c.Idx(), \"f\", 1, 0) // creates and commits a Tx internally.\n\n\t\tt.Run(\"OK\", func(t *testing.T) {\n\t\t\thldr.ClearBit(c.Idx(), \"f\", 11, 1)\n\t\t\tif n := hldr.Row(c.Idx(), \"f\", 11).Count(); n != 0 {\n\t\t\t\tt.Fatalf(\"unexpected row count: %d\", n)\n\t\t\t}\n\n\t\t\tif res, err := cmd.API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(1, f=11)`}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if !res.Results[0].(bool) {\n\t\t\t\tt.Fatalf(\"expected column changed\")\n\t\t\t}\n\n\t\t\tif n := hldr.Row(c.Idx(), \"f\", 11).Count(); n != 1 {\n\t\t\t\tt.Fatalf(\"unexpected row count: %d\", n)\n\t\t\t}\n\t\t\tif res, err := cmd.API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(1, f=11)`}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if res.Results[0].(bool) {\n\t\t\t\tt.Fatalf(\"expected column unchanged\")\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"ErrInvalidColValueType\", func(t *testing.T) {\n\t\t\tif _, err := cmd.API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(\"foo\", f=1)`}); err == nil || !strings.Contains(err.Error(), \"unkeyed index\") {\n\t\t\t\tt.Fatalf(\"The error is: '%v'\", err)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"ErrInvalidRowValueType\", func(t *testing.T) {\n\t\t\tif _, err := cmd.API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(2, f=\"bar\")`}); err == nil || !strings.Contains(err.Error(), \"cannot create keys on unkeyed field\") {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t})\n\t})\n\n\tt.Run(\"RowKeyColumnKey\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\tcmd := c.GetNode(0)\n\t\thldr := c.GetHolder(0)\n\t\tidx := hldr.MustCreateIndexIfNotExists(c.Idx(), pilosa.IndexOptions{Keys: true})\n\n\t\tt.Run(\"OK\", func(t *testing.T) {\n\t\t\thldr.SetBit(c.Idx(), \"f\", 1, 0) // creates and Commits a Tx internally.\n\t\t\tif n := hldr.Row(c.Idx(), \"f\", 11).Count(); n != 0 {\n\t\t\t\tt.Fatalf(\"unexpected row count: %d\", n)\n\t\t\t}\n\n\t\t\tif res, err := cmd.API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(\"foo\", f=11)`}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if !res.Results[0].(bool) {\n\t\t\t\tt.Fatalf(\"expected column changed\")\n\t\t\t}\n\n\t\t\tif n := hldr.Row(c.Idx(), \"f\", 11).Count(); n != 1 {\n\t\t\t\tt.Fatalf(\"unexpected row count: %d\", n)\n\t\t\t}\n\t\t\tif res, err := cmd.API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(\"foo\", f=11)`}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if res.Results[0].(bool) {\n\t\t\t\tt.Fatalf(\"expected column unchanged\")\n\t\t\t}\n\n\t\t\tif res, err := cmd.API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(2, f=11)`}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if !res.Results[0].(bool) {\n\t\t\t\tt.Fatalf(\"expected column changed with integer column key\")\n\t\t\t}\n\n\t\t\tif res, err := cmd.API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(2, f=11)`}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if res.Results[0].(bool) {\n\t\t\t\tt.Fatalf(\"expected column unchanged with integer column key\")\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"ErrInvalidColValueType\", func(t *testing.T) {\n\t\t\thldr.SetBit(c.Idx(), \"f\", 1, 0) // creates and Commits a Tx internally.\n\n\t\t\tif err := idx.DeleteField(\"f\"); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tif _, err := idx.CreateField(\"f\", \"\"); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tif _, err := cmd.API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(2.1, f=1)`}); err == nil || !strings.Contains(err.Error(), \"parse error\") {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tif res, err := cmd.API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(2, f=1)`}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if !res.Results[0].(bool) {\n\t\t\t\tt.Fatalf(\"expected column changed with integer column key\")\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"ErrInvalidRowValueType\", func(t *testing.T) {\n\t\t\tidx := hldr.MustCreateIndexIfNotExists(c.Idx(\"inokey\"), pilosa.IndexOptions{})\n\t\t\tif _, err := idx.CreateField(\"f\", \"\", pilosa.OptFieldKeys()); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif _, err := cmd.API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(\"inokey\"), Query: `Set(2, f=1.2)`}); err == nil || !strings.Contains(err.Error(), \"invalid value\") {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tif res, err := cmd.API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(2, f=9)`}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if !res.Results[0].(bool) {\n\t\t\t\tt.Fatalf(\"expected column changed with integer column key\")\n\t\t\t}\n\t\t})\n\t})\n}\n\n// Ensure a set query can be executed on a bool field.\nfunc TestExecutor_Execute_SetBool(t *testing.T) {\n\tt.Run(\"Basic\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\t// Create fields.\n\t\tindex := hldr.MustCreateIndexIfNotExists(c.Idx(), pilosa.IndexOptions{})\n\t\tif _, err := index.CreateFieldIfNotExists(\"f\", \"\", pilosa.OptFieldTypeBool()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Set a true bit.\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(100, f=true)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !res.Results[0].(bool) {\n\t\t\tt.Fatalf(\"expected column changed\")\n\t\t}\n\n\t\t// Set the same bit to true again verify nothing changed.\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(100, f=true)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if res.Results[0].(bool) {\n\t\t\tt.Fatalf(\"expected column to be unchanged\")\n\t\t}\n\n\t\t// Set the same bit to false.\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(100, f=false)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !res.Results[0].(bool) {\n\t\t\tt.Fatalf(\"expected column changed\")\n\t\t}\n\n\t\t// Ensure that the false row is set.\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(f=false)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if columns := result.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{100}) {\n\t\t\tt.Fatalf(\"unexpected colums: %+v\", columns)\n\t\t}\n\n\t\t// Ensure that the true row is empty.\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(f=true)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if columns := result.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{}) {\n\t\t\tt.Fatalf(\"unexpected colums: %+v\", columns)\n\t\t}\n\t})\n\tt.Run(\"Error\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\t// Create fields.\n\t\tindex := hldr.MustCreateIndexIfNotExists(c.Idx(), pilosa.IndexOptions{})\n\t\tif _, err := index.CreateFieldIfNotExists(\"f\", \"\", pilosa.OptFieldTypeBool()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Set bool using a string value.\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(100, f=\"true\")`}); err == nil {\n\t\t\tt.Fatalf(\"expected invalid bool type error\")\n\t\t}\n\n\t\t// Set bool using an integer.\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(100, f=1)`}); err == nil {\n\t\t\tt.Fatalf(\"expected invalid bool type error\")\n\t\t}\n\t})\n}\n\n// Ensure a set query can be executed on a decimal field.\nfunc TestExecutor_Execute_SetDecimal(t *testing.T) {\n\tt.Run(\"Basic\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\t// Create fields.\n\t\tindex := hldr.MustCreateIndexIfNotExists(c.Idx(), pilosa.IndexOptions{})\n\t\tif _, err := index.CreateFieldIfNotExists(\"f\", \"\", pilosa.OptFieldTypeDecimal(2)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Set a value.\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(1000, f=1.5)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !res.Results[0].(bool) {\n\t\t\tt.Fatalf(\"expected column changed\")\n\t\t}\n\n\t\t// Set the same value again verify nothing changed.\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(1000, f=1.5)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if res.Results[0].(bool) {\n\t\t\tt.Fatalf(\"expected column to be unchanged\")\n\t\t}\n\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(f == 1.5)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if columns := result.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{1000}) {\n\t\t\tt.Fatalf(\"unexpected colums: %+v\", columns)\n\t\t}\n\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(f > 1.4999)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if columns := result.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{1000}) {\n\t\t\tt.Fatalf(\"unexpected colums: %+v\", columns)\n\t\t}\n\t})\n\tt.Run(\"Error\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\t// Create fields.\n\t\tindex := hldr.MustCreateIndexIfNotExists(c.Idx(), pilosa.IndexOptions{})\n\t\tif _, err := index.CreateFieldIfNotExists(\"f\", \"\", pilosa.OptFieldTypeDecimal(2)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Set decimal using a string value.\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(1000, f=\"1.5\")`}); err == nil {\n\t\t\tt.Fatalf(\"expected invalid decimal type error\")\n\t\t}\n\t})\n}\n\n// Ensure old PQL syntax doesn't break anything too badly.\nfunc TestExecutor_Execute_OldPQL(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\thldr := c.GetHolder(0)\n\n\t// set a bit so the view gets created.\n\thldr.SetBit(c.Idx(), \"f\", 1, 0)\n\n\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `SetBit(frame=f, row=11, col=1)`}); err == nil || errors.Cause(err).Error() != \"unknown call: SetBit\" {\n\t\tt.Fatalf(\"Expected error: 'unknown call: SetBit', got: %v. Full: %v\", errors.Cause(err), err)\n\t}\n}\n\n// Ensure a SetValue() query can be executed.\nfunc TestExecutor_Execute_SetValue(t *testing.T) {\n\tt.Run(\"OK\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\t// Create fields.\n\t\tindex := hldr.MustCreateIndexIfNotExists(c.Idx(), pilosa.IndexOptions{})\n\t\tif _, err := index.CreateFieldIfNotExists(\"f\", \"\", pilosa.OptFieldTypeInt(math.MinInt64, math.MaxInt64)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := index.CreateFieldIfNotExists(\"xxx\", \"\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Set bsiGroup values.\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(10, f=25)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(100, f=10)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Obtain transaction.\n\t\tqcx := hldr.Txf().NewQcx()\n\t\tdefer qcx.Abort()\n\n\t\tf := hldr.Field(c.Idx(), \"f\")\n\t\tif value, exists, err := f.Value(qcx, 10); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !exists {\n\t\t\tt.Fatal(\"expected value to exist\")\n\t\t} else if value != 25 {\n\t\t\tt.Fatalf(\"unexpected value: %v\", value)\n\t\t}\n\n\t\tif value, exists, err := f.Value(qcx, 100); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !exists {\n\t\t\tt.Fatal(\"expected value to exist\")\n\t\t} else if value != 10 {\n\t\t\tt.Fatalf(\"unexpected value: %v\", value)\n\t\t}\n\t})\n\n\tt.Run(\"Err\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\tindex := hldr.MustCreateIndexIfNotExists(c.Idx(), pilosa.IndexOptions{})\n\t\tif _, err := index.CreateFieldIfNotExists(\"f\", \"\", pilosa.OptFieldTypeInt(math.MinInt64, math.MaxInt64)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tt.Run(\"ColumnBSIGroupRequired\", func(t *testing.T) {\n\t\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(f=100)`}); err == nil || errors.Cause(err).Error() != `Set() column argument 'col' required` {\n\t\t\t\tt.Fatalf(\"unexpected error: %s\", err)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"ColumnBSIGroupValue\", func(t *testing.T) {\n\t\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(\"bad_column\", f=100)`}); err == nil || !strings.Contains(err.Error(), \"unkeyed index\") {\n\t\t\t\tt.Fatalf(\"unexpected error: %s\", err)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"InvalidBSIGroupValueType\", func(t *testing.T) {\n\t\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(10, f=\"hello\")`}); err == nil || !strings.Contains(err.Error(), \"cannot create keys on unkeyed field\") {\n\t\t\t\tt.Fatalf(\"unexpected error: %s\", err)\n\t\t\t}\n\t\t})\n\t})\n\n\tt.Run(\"Timestamp\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\t// Create fields.\n\t\tindex := hldr.MustCreateIndexIfNotExists(c.Idx(), pilosa.IndexOptions{})\n\t\tif _, err := index.CreateFieldIfNotExists(\"f\", \"\", pilosa.OptFieldTypeTimestamp(pilosa.DefaultEpoch, pilosa.TimeUnitSeconds)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := index.CreateFieldIfNotExists(\"xxx\", \"\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Set bsiGroup values.\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(10, f='2000-01-01T00:00:00.000000000Z')`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(100, f='2000-01-02T00:00:00Z')`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Obtain transaction.\n\t\tqcx := hldr.Txf().NewQcx()\n\t\tdefer qcx.Abort()\n\n\t\tf := hldr.Field(c.Idx(), \"f\")\n\t\tif value, exists, err := f.Value(qcx, 10); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !exists {\n\t\t\tt.Fatal(\"expected value to exist\")\n\t\t} else if value != time.Date(2000, time.January, 1, 0, 0, 0, 0, time.UTC).UnixNano()/int64(time.Second) {\n\t\t\tt.Fatalf(\"unexpected value: %v\", value)\n\t\t}\n\n\t\tif value, exists, err := f.Value(qcx, 100); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !exists {\n\t\t\tt.Fatal(\"expected value to exist\")\n\t\t} else if value != time.Date(2000, time.January, 2, 0, 0, 0, 0, time.UTC).UnixNano()/int64(time.Second) {\n\t\t\tt.Fatalf(\"unexpected value: %v\", value)\n\t\t}\n\t})\n}\n\nfunc TestExecutor_ExecuteTopK(t *testing.T) {\n\tbaseBits := [][2]uint64{\n\t\t{0, 0},\n\t\t{0, ShardWidth + 2},\n\t\t{10, 2},\n\t\t{10, ShardWidth},\n\t\t{10, 2 * ShardWidth},\n\t\t{10, ShardWidth + 1},\n\t\t{20, ShardWidth},\n\t}\n\ttests := []struct {\n\t\tfieldName    string\n\t\tfieldOptions []pilosa.FieldOption\n\t\tbits         [][2]uint64\n\t\tquery        string\n\t\tresult       []pilosa.Pair\n\t}{\n\t\t{\n\t\t\tfieldName: \"f\",\n\t\t\tbits:      append(baseBits, [2]uint64{0, 1}),\n\t\t\tquery:     \"TopK(f, k=2)\",\n\t\t\tresult: []pilosa.Pair{\n\t\t\t\t{ID: 10, Count: 4},\n\t\t\t\t{ID: 0, Count: 3},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tfieldName:    \"fmutex\",\n\t\t\tfieldOptions: []pilosa.FieldOption{pilosa.OptFieldTypeMutex(pilosa.CacheTypeRanked, 10)},\n\t\t\tbits:         baseBits,\n\t\t\tquery:        \"TopK(fmutex, k=2)\",\n\t\t\tresult: []pilosa.Pair{\n\t\t\t\t{ID: 10, Count: 3},\n\t\t\t\t{ID: 0, Count: 2},\n\t\t\t},\n\t\t},\n\t}\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\n\tfor _, tst := range tests {\n\t\tt.Run(tst.fieldName, func(t *testing.T) {\n\t\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{TrackExistence: true}, tst.fieldName, tst.fieldOptions...)\n\t\t\tc.ImportBits(t, c.Idx(), tst.fieldName, tst.bits)\n\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: tst.query}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if !reflect.DeepEqual(result.Results, []interface{}{&pilosa.PairsField{Pairs: tst.result, Field: tst.fieldName}}) {\n\t\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestExecutor_Execute_TopK_Time(t *testing.T) {\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\n\tisStandardEnabled := []bool{true, false}\n\n\tfor _, enabled := range isStandardEnabled {\n\t\t// Load some test data into a time field.\n\t\tindex := c.Idx(fmt.Sprintf(\"%t\", enabled))\n\t\tc.CreateField(t, index, pilosa.IndexOptions{TrackExistence: true}, \"f\", pilosa.OptFieldTypeTime(\"YMD\", \"0\", enabled))\n\t\tc.Query(t, index, `\n\t\tSet(0, f=0, 2016-01-02T00:00)\n\t\tSet(0, f=1, 2016-01-02T00:00)\n\t\tSet(0, f=0, 2016-01-03T00:00)\n\t\tSet(1, f=0, 2016-01-10T00:00)\n\t\tSet(100000000, f=2, 2016-02-02T00:00)\n\t\tSet(200000000, f=3, 2015-01-02T00:00)\n\t`)\n\n\t\t// Execute query.\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: index, Query: `TopK(f, k=3, from=2016-01-01T00:00, to=2016-01-11T00:00)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(result.Results, []interface{}{&pilosa.PairsField{\n\t\t\tPairs: []pilosa.Pair{\n\t\t\t\t{ID: 0, Count: 2},\n\t\t\t\t{ID: 1, Count: 1},\n\t\t\t},\n\t\t\tField: \"f\",\n\t\t}}) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t}\n\t}\n}\n\n// Ensure a TopN() query can be executed.\nfunc TestExecutor_Execute_TopN(t *testing.T) {\n\tt.Run(\"RowIDColumnID\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\t// Set columns for rows 0, 10, & 20 across two shards.\n\t\tif idx, err := hldr.CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := idx.CreateField(\"f\", \"\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := idx.CreateField(\"other\", \"\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\t\tSet(0, f=0)\n\t\t\tSet(1, f=0)\n\t\t\tSet(` + strconv.Itoa(ShardWidth) + `, f=0)\n\t\t\tSet(` + strconv.Itoa(ShardWidth+2) + `, f=0)\n\t\t\tSet(` + strconv.Itoa((5*ShardWidth)+100) + `, f=0)\n\t\t\tSet(0, f=10)\n\t\t\tSet(` + strconv.Itoa(ShardWidth) + `, f=10)\n\t\t\tSet(` + strconv.Itoa(ShardWidth) + `, f=20)\n\t\t\tSet(0, other=0)\n\t\t`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\terr := c.GetNode(0).RecalculateCaches(t)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"recalculating caches: %v\", err)\n\t\t}\n\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `TopN(f, n=2)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(result.Results[0], &pilosa.PairsField{\n\t\t\tPairs: []pilosa.Pair{\n\t\t\t\t{ID: 0, Count: 5},\n\t\t\t\t{ID: 10, Count: 2},\n\t\t\t},\n\t\t\tField: \"f\",\n\t\t}) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t}\n\t})\n\n\tt.Run(\"RowIDColumnKey\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\t// Set columns for rows 0, 10, & 20 across two shards.\n\t\tif idx, err := hldr.CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{Keys: true}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := idx.CreateField(\"f\", \"\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := idx.CreateField(\"other\", \"\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\t\tSet(\"zero\", f=0)\n\t\t\tSet(\"one\", f=0)\n\t\t\tSet(\"sw\", f=0)\n\t\t\tSet(\"sw2\", f=0)\n\t\t\tSet(\"sw3\", f=0)\n\t\t\tSet(\"zero\", f=10)\n\t\t\tSet(\"sw\", f=10)\n\t\t\tSet(\"sw\", f=20)\n\t\t\tSet(\"zero\", other=0)\n\t\t`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\terr := c.GetNode(0).RecalculateCaches(t)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"recalculating caches: %v\", err)\n\t\t}\n\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `TopN(f, n=2)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(result.Results[0], &pilosa.PairsField{\n\t\t\tPairs: []pilosa.Pair{\n\t\t\t\t{ID: 0, Count: 5},\n\t\t\t\t{ID: 10, Count: 2},\n\t\t\t},\n\t\t\tField: \"f\",\n\t\t}) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t}\n\t})\n\n\tt.Run(\"RowKeyColumnKey\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\t// Set columns for rows 0, 10, & 20 across two shards.\n\t\tif idx, err := hldr.CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{Keys: true}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := idx.CreateField(\"f\", \"\", pilosa.OptFieldKeys()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := idx.CreateField(\"other\", \"\", pilosa.OptFieldKeys()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\t\tSet(\"zero\", f=\"zero\")\n\t\t\tSet(\"one\", f=\"zero\")\n\t\t\tSet(\"sw\", f=\"zero\")\n\t\t\tSet(\"sw2\", f=\"zero\")\n\t\t\tSet(\"sw3\", f=\"zero\")\n\t\t\tSet(\"zero\", f=\"ten\")\n\t\t\tSet(\"sw\", f=\"ten\")\n\t\t\tSet(\"sw\", f=\"twenty\")\n\t\t\tSet(\"zero\", other=\"zero\")\n\t\t`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\terr := c.GetNode(0).RecalculateCaches(t)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"recalculating caches: %v\", err)\n\t\t}\n\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `TopN(f, n=2)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else {\n\t\t\tif !reflect.DeepEqual(result.Results[0], &pilosa.PairsField{\n\t\t\t\tPairs: []pilosa.Pair{\n\t\t\t\t\t{Key: \"zero\", Count: 5},\n\t\t\t\t\t{Key: \"ten\", Count: 2},\n\t\t\t\t},\n\t\t\t\tField: \"f\",\n\t\t\t}) {\n\t\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t\t}\n\t\t}\n\t})\n\n\tt.Run(\"RowKeyColumnKey\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\t// Set columns for rows 0, 10, & 20 across two shards.\n\t\tif idx, err := hldr.CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{Keys: true}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := idx.CreateField(\"f\", \"\", pilosa.OptFieldKeys()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := idx.CreateField(\"other\", \"\", pilosa.OptFieldKeys()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\t\tSet(\"a\", f=\"foo\")\n\t\t\tSet(\"b\", f=\"foo\")\n\t\t\tSet(\"c\", f=\"foo\")\n\t\t\tSet(\"d\", f=\"foo\")\n\t\t\tSet(\"e\", f=\"foo\")\n\t\t\tSet(\"a\", f=\"bar\")\n\t\t\tSet(\"b\", f=\"bar\")\n\t\t\tSet(\"b\", f=\"baz\")\n\t\t\tSet(\"a\", other=\"foo\")\n\t\t`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\terr := c.GetNode(0).RecalculateCaches(t)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"recalculating caches: %v\", err)\n\t\t}\n\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `TopN(f, n=2)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if diff := cmp.Diff(result.Results, []interface{}{\n\t\t\t&pilosa.PairsField{\n\t\t\t\tPairs: []pilosa.Pair{\n\t\t\t\t\t{Key: \"foo\", Count: 5},\n\t\t\t\t\t{Key: \"bar\", Count: 2},\n\t\t\t\t},\n\t\t\t\tField: \"f\",\n\t\t\t},\n\t\t}); diff != \"\" {\n\t\t\tt.Fatal(diff)\n\t\t}\n\t})\n\n\tt.Run(\"ErrFieldNotFound\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\t// Set data on the \"f\" field.\n\t\tif idx, err := hldr.CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := idx.CreateField(\"f\", \"\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\t\tSet(0, f=0)\n\t\t\tSet(0, f=1)\n\t\t`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := c.GetNode(0).RecalculateCaches(t); err != nil {\n\t\t\tt.Fatalf(\"recalculating caches: %v\", err)\n\t\t}\n\n\t\t// Attempt to query the \"g\" field.\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `TopN(g, n=2)`}); err == nil || err.Error() != `executing: field \"g\" not found` {\n\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t}\n\t})\n\n\tt.Run(\"ErrBSIField\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\t// Create BSI \"f\" field.\n\t\tif idx, err := hldr.CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := idx.CreateField(\"f\", \"\", pilosa.OptFieldTypeInt(0, 100)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `TopN(f, n=2)`}); err == nil || !strings.Contains(err.Error(), `finding top results: mapping on primary node: cannot compute TopN() on integer, decimal, or timestamp field: \"f\"`) {\n\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t}\n\t})\n\n\tt.Run(\"ErrCacheNone\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\tif idx, err := hldr.CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := idx.CreateField(\"f\", \"\", pilosa.OptFieldTypeSet(pilosa.CacheTypeNone, 0)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\t\tSet(0, f=0)\n\t\t\tSet(0, f=1)\n\t\t`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `TopN(f, n=2)`}); err == nil || !strings.Contains(err.Error(), `finding top results: mapping on primary node: cannot compute TopN(), field has no cache: \"f\"`) {\n\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t}\n\t})\n}\n\nfunc TestExecutor_Execute_TopN_fill(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\thldr := c.GetHolder(0)\n\n\t// Set columns for rows 0, 10, & 20 across two shards.\n\thldr.SetBit(c.Idx(), \"f\", 0, 0)\n\thldr.SetBit(c.Idx(), \"f\", 0, 1)\n\thldr.SetBit(c.Idx(), \"f\", 0, 2)\n\thldr.SetBit(c.Idx(), \"f\", 0, ShardWidth)\n\thldr.SetBit(c.Idx(), \"f\", 1, ShardWidth+2)\n\thldr.SetBit(c.Idx(), \"f\", 1, ShardWidth)\n\n\t// Execute query.\n\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `TopN(f, n=1)`}); err != nil {\n\t\tt.Fatal(err)\n\t} else if !reflect.DeepEqual(result.Results, []interface{}{&pilosa.PairsField{\n\t\tPairs: []pilosa.Pair{\n\t\t\t{ID: 0, Count: 4},\n\t\t},\n\t\tField: \"f\",\n\t}}) {\n\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t}\n}\n\n// Ensure\nfunc TestExecutor_Execute_TopN_fill_small(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\thldr := c.GetHolder(0)\n\n\thldr.SetBit(c.Idx(), \"f\", 0, 0)\n\thldr.SetBit(c.Idx(), \"f\", 0, ShardWidth)\n\thldr.SetBit(c.Idx(), \"f\", 0, 2*ShardWidth)\n\thldr.SetBit(c.Idx(), \"f\", 0, 3*ShardWidth)\n\thldr.SetBit(c.Idx(), \"f\", 0, 4*ShardWidth)\n\n\thldr.SetBit(c.Idx(), \"f\", 1, 0)\n\thldr.SetBit(c.Idx(), \"f\", 1, 1)\n\n\thldr.SetBit(c.Idx(), \"f\", 2, ShardWidth)\n\thldr.SetBit(c.Idx(), \"f\", 2, ShardWidth+1)\n\n\thldr.SetBit(c.Idx(), \"f\", 3, 2*ShardWidth)\n\thldr.SetBit(c.Idx(), \"f\", 3, 2*ShardWidth+1)\n\n\thldr.SetBit(c.Idx(), \"f\", 4, 3*ShardWidth)\n\thldr.SetBit(c.Idx(), \"f\", 4, 3*ShardWidth+1)\n\n\t// Execute query.\n\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `TopN(f, n=1)`}); err != nil {\n\t\tt.Fatal(err)\n\t} else if !reflect.DeepEqual(result.Results, []interface{}{&pilosa.PairsField{\n\t\tPairs: []pilosa.Pair{\n\t\t\t{ID: 0, Count: 5},\n\t\t},\n\t\tField: \"f\",\n\t}}) {\n\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t}\n}\n\n// Ensure a TopN() query with a source row can be executed.\nfunc TestExecutor_Execute_TopN_Src(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\thldr := c.GetHolder(0)\n\n\t// Set columns for rows 0, 10, & 20 across two shards.\n\thldr.SetBit(c.Idx(), \"f\", 0, 0)\n\thldr.SetBit(c.Idx(), \"f\", 0, 1)\n\thldr.SetBit(c.Idx(), \"f\", 0, ShardWidth)\n\thldr.SetBit(c.Idx(), \"f\", 10, ShardWidth)\n\thldr.SetBit(c.Idx(), \"f\", 10, ShardWidth+1)\n\thldr.SetBit(c.Idx(), \"f\", 20, ShardWidth)\n\thldr.SetBit(c.Idx(), \"f\", 20, ShardWidth+1)\n\thldr.SetBit(c.Idx(), \"f\", 20, ShardWidth+2)\n\n\t// Create an intersecting row.\n\thldr.SetBit(c.Idx(), \"other\", 100, ShardWidth)\n\thldr.SetBit(c.Idx(), \"other\", 100, ShardWidth+1)\n\thldr.SetBit(c.Idx(), \"other\", 100, ShardWidth+2)\n\n\terr := c.GetNode(0).RecalculateCaches(t)\n\tif err != nil {\n\t\tt.Fatalf(\"recalculating caches: %v\", err)\n\t}\n\n\t// Execute query.\n\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `TopN(f, Row(other=100), n=3)`}); err != nil {\n\t\tt.Fatal(err)\n\t} else if !reflect.DeepEqual(result.Results, []interface{}{&pilosa.PairsField{\n\t\tPairs: []pilosa.Pair{\n\t\t\t{ID: 20, Count: 3},\n\t\t\t{ID: 10, Count: 2},\n\t\t\t{ID: 0, Count: 1},\n\t\t},\n\t\tField: \"f\",\n\t}}) {\n\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t}\n}\n\n// Ensure Min()  and Max() queries can be executed.\nfunc TestExecutor_Execute_MinMax(t *testing.T) {\n\tt.Run(\"WithOffset\", func(t *testing.T) {\n\t\tt.Run(\"Int\", func(t *testing.T) {\n\t\t\tc := test.MustRunCluster(t, 1)\n\t\t\tdefer c.Close()\n\t\t\thldr := c.GetHolder(0)\n\n\t\t\tidx, err := hldr.CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\ttests := []struct {\n\t\t\t\tmin int64\n\t\t\t\tmax int64\n\t\t\t\tset int64\n\t\t\t}{\n\t\t\t\t{10, 20, 11},\n\t\t\t\t{-10, 20, 11},\n\t\t\t\t{-10, 20, -9},\n\t\t\t\t{-20, -10, -11},\n\t\t\t}\n\t\t\tfor i, test := range tests {\n\t\t\t\tfld := fmt.Sprintf(\"f%d\", i)\n\t\t\t\tt.Run(\"MinMaxField_\"+fld, func(t *testing.T) {\n\t\t\t\t\tif _, err := idx.CreateField(fld, \"\", pilosa.OptFieldTypeInt(test.min, test.max)); err != nil {\n\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t}\n\n\t\t\t\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: fmt.Sprintf(`\n\t\t\t\tSet(10, %s=%d)\n\t\t\t`, fld, test.set)}); err != nil {\n\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t}\n\n\t\t\t\t\tvar pql string\n\n\t\t\t\t\tt.Run(\"Min\", func(t *testing.T) {\n\t\t\t\t\t\tpql = fmt.Sprintf(`Min(field=%s)`, fld)\n\t\t\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{Val: test.set, Count: 1}) {\n\t\t\t\t\t\t\tt.Fatalf(\"unexpected min result, test %d: %s\", i, spew.Sdump(result))\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\n\t\t\t\t\tt.Run(\"Max\", func(t *testing.T) {\n\t\t\t\t\t\tpql = fmt.Sprintf(`Max(field=%s)`, fld)\n\t\t\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{Val: test.set, Count: 1}) {\n\t\t\t\t\t\t\tt.Fatalf(\"unexpected max result, test %d: %s\", i, spew.Sdump(result))\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\n\t\t\t\t\tt.Run(\"Min\", func(t *testing.T) {\n\t\t\t\t\t\tpql = fmt.Sprintf(`Min(field=\"%s\")`, fld)\n\t\t\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{Val: test.set, Count: 1}) {\n\t\t\t\t\t\t\tt.Fatalf(\"unexpected min result, test %d: %s\", i, spew.Sdump(result))\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\n\t\t\t\t\tt.Run(\"Max\", func(t *testing.T) {\n\t\t\t\t\t\tpql = fmt.Sprintf(`Max(field=\"%s\")`, fld)\n\t\t\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{Val: test.set, Count: 1}) {\n\t\t\t\t\t\t\tt.Fatalf(\"unexpected max result, test %d: %s\", i, spew.Sdump(result))\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\n\t\t\t\t\tt.Run(\"Min\", func(t *testing.T) {\n\t\t\t\t\t\tpql = fmt.Sprintf(`Min(%s)`, fld)\n\t\t\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{Val: test.set, Count: 1}) {\n\t\t\t\t\t\t\tt.Fatalf(\"unexpected min result, test %d: %s\", i, spew.Sdump(result))\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\n\t\t\t\t\tt.Run(\"Max\", func(t *testing.T) {\n\t\t\t\t\t\tpql = fmt.Sprintf(`Max(%s)`, fld)\n\t\t\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{Val: test.set, Count: 1}) {\n\t\t\t\t\t\t\tt.Fatalf(\"unexpected max result, test %d: %s\", i, spew.Sdump(result))\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\t\t\t\t})\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"Decimal\", func(t *testing.T) {\n\t\t\tc := test.MustRunCluster(t, 1)\n\t\t\tdefer c.Close()\n\t\t\thldr := c.GetHolder(0)\n\n\t\t\tidx, err := hldr.CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\ttests := []struct {\n\t\t\t\tscale int64\n\t\t\t\tmin   pql.Decimal\n\t\t\t\tmax   pql.Decimal\n\t\t\t\tset   pql.Decimal\n\t\t\t\texp   pql.Decimal\n\t\t\t}{\n\t\t\t\t{\n\t\t\t\t\t2,\n\t\t\t\t\tpql.NewDecimal(1, -1),\n\t\t\t\t\tpql.NewDecimal(2, -1),\n\t\t\t\t\tpql.NewDecimal(115, 1),\n\t\t\t\t\tpql.NewDecimal(1150, 2),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t2,\n\t\t\t\t\tpql.NewDecimal(-1, -1),\n\t\t\t\t\tpql.NewDecimal(2, -1),\n\t\t\t\t\tpql.NewDecimal(115, 1),\n\t\t\t\t\tpql.NewDecimal(1150, 2),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t2,\n\t\t\t\t\tpql.NewDecimal(-1, -1),\n\t\t\t\t\tpql.NewDecimal(2, -1),\n\t\t\t\t\tpql.NewDecimal(-95, 1),\n\t\t\t\t\tpql.NewDecimal(-950, 2),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t2,\n\t\t\t\t\tpql.NewDecimal(-2, -1),\n\t\t\t\t\tpql.NewDecimal(-1, -1),\n\t\t\t\t\tpql.NewDecimal(-115, 1),\n\t\t\t\t\tpql.NewDecimal(-1150, 2),\n\t\t\t\t},\n\t\t\t}\n\t\t\t// This extra field exists to make there be shards which are present,\n\t\t\t// but have no decimal values set, to make sure they don't break\n\t\t\t// the results.\n\t\t\tif _, err := idx.CreateFieldIfNotExists(\"z\", \"\"); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(1, z=0)`}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if !res.Results[0].(bool) {\n\t\t\t\tt.Fatalf(\"expected column changed\")\n\t\t\t}\n\t\t\t// set things in other shards, that won't have decimal values\n\t\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(1234567, z=0)`}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if !res.Results[0].(bool) {\n\t\t\t\tt.Fatalf(\"expected column changed\")\n\t\t\t}\n\t\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(2345678, z=0)`}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if !res.Results[0].(bool) {\n\t\t\t\tt.Fatalf(\"expected column changed\")\n\t\t\t}\n\t\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(3456789, z=0)`}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if !res.Results[0].(bool) {\n\t\t\t\tt.Fatalf(\"expected column changed\")\n\t\t\t}\n\t\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(4567890, z=0)`}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if !res.Results[0].(bool) {\n\t\t\t\tt.Fatalf(\"expected column changed\")\n\t\t\t}\n\t\t\tfor i, test := range tests {\n\t\t\t\tfld := fmt.Sprintf(\"f%d\", i)\n\t\t\t\tt.Run(\"MinMaxField_\"+fld, func(t *testing.T) {\n\t\t\t\t\tif _, err := idx.CreateField(fld, \"\", pilosa.OptFieldTypeDecimal(test.scale, test.min, test.max)); err != nil {\n\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t}\n\n\t\t\t\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: fmt.Sprintf(`\n                Set(6700000, %s=%s)\n            `, fld, test.set)}); err != nil {\n\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t}\n\n\t\t\t\t\tvar pql string\n\n\t\t\t\t\tt.Run(\"Min\", func(t *testing.T) {\n\t\t\t\t\t\tpql = fmt.Sprintf(`Min(field=%s)`, fld)\n\t\t\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{DecimalVal: &test.exp, Count: 1}) {\n\t\t\t\t\t\t\tt.Fatalf(\"unexpected min result, test %d: %s\", i, spew.Sdump(result))\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\n\t\t\t\t\tt.Run(\"Max\", func(t *testing.T) {\n\t\t\t\t\t\tpql = fmt.Sprintf(`Max(field=%s)`, fld)\n\t\t\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{DecimalVal: &test.exp, Count: 1}) {\n\t\t\t\t\t\t\tt.Fatalf(\"unexpected max result, test %d: %s\", i, spew.Sdump(result))\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\n\t\t\t\t\tt.Run(\"Min\", func(t *testing.T) {\n\t\t\t\t\t\tpql = fmt.Sprintf(`Min(%s)`, fld)\n\t\t\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{DecimalVal: &test.exp, Count: 1}) {\n\t\t\t\t\t\t\tt.Fatalf(\"unexpected min result, test %d: %s\", i, spew.Sdump(result))\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\n\t\t\t\t\tt.Run(\"Max\", func(t *testing.T) {\n\t\t\t\t\t\tpql = fmt.Sprintf(`Max(%s)`, fld)\n\t\t\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{DecimalVal: &test.exp, Count: 1}) {\n\t\t\t\t\t\t\tt.Fatalf(\"unexpected max result, test %d: %s\", i, spew.Sdump(result))\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\t\t\t\t})\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"Timestamp\", func(t *testing.T) {\n\t\t\tc := test.MustRunCluster(t, 1)\n\t\t\tdefer c.Close()\n\t\t\thldr := c.GetHolder(0)\n\n\t\t\tidx, err := hldr.CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\ttests := []struct {\n\t\t\t\tepoch time.Time\n\t\t\t\tset   time.Time\n\t\t\t}{\n\t\t\t\t{\n\t\t\t\t\ttime.Date(2000, time.January, 10, 0, 0, 0, 0, time.UTC),\n\t\t\t\t\ttime.Date(2000, time.January, 11, 0, 0, 0, 0, time.UTC),\n\t\t\t\t},\n\t\t\t}\n\t\t\tfor i, test := range tests {\n\t\t\t\tfld := fmt.Sprintf(\"f%d\", i)\n\t\t\t\tt.Run(\"MinMaxField_\"+fld, func(t *testing.T) {\n\t\t\t\t\tif _, err := idx.CreateField(fld, \"\", pilosa.OptFieldTypeTimestamp(test.epoch, pilosa.TimeUnitSeconds)); err != nil {\n\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t} else if _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: fmt.Sprintf(`Set(10, %s=\"%s\")`, fld, test.set.Format(time.RFC3339))}); err != nil {\n\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t}\n\n\t\t\t\t\tvar pql string\n\n\t\t\t\t\tt.Run(\"Min\", func(t *testing.T) {\n\t\t\t\t\t\tpql = fmt.Sprintf(`Min(field=%s)`, fld)\n\t\t\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{TimestampVal: test.set, Count: 1}) {\n\t\t\t\t\t\t\tt.Fatalf(\"unexpected min result, test %d: %s\", i, spew.Sdump(result))\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\n\t\t\t\t\tt.Run(\"Max\", func(t *testing.T) {\n\t\t\t\t\t\tpql = fmt.Sprintf(`Max(field=%s)`, fld)\n\t\t\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{TimestampVal: test.set, Count: 1}) {\n\t\t\t\t\t\t\tt.Fatalf(\"unexpected max result, test %d: %s\", i, spew.Sdump(result))\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\n\t\t\t\t\tt.Run(\"Min\", func(t *testing.T) {\n\t\t\t\t\t\tpql = fmt.Sprintf(`Min(field=\"%s\")`, fld)\n\t\t\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{TimestampVal: test.set, Count: 1}) {\n\t\t\t\t\t\t\tt.Fatalf(\"unexpected min result, test %d: %s\", i, spew.Sdump(result))\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\n\t\t\t\t\tt.Run(\"Max\", func(t *testing.T) {\n\t\t\t\t\t\tpql = fmt.Sprintf(`Max(field=\"%s\")`, fld)\n\t\t\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{TimestampVal: test.set, Count: 1}) {\n\t\t\t\t\t\t\tt.Fatalf(\"unexpected max result, test %d: %s\", i, spew.Sdump(result))\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\n\t\t\t\t\tt.Run(\"Min\", func(t *testing.T) {\n\t\t\t\t\t\tpql = fmt.Sprintf(`Min(%s)`, fld)\n\t\t\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{TimestampVal: test.set, Count: 1}) {\n\t\t\t\t\t\t\tt.Fatalf(\"unexpected min result, test %d: %s\", i, spew.Sdump(result))\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\n\t\t\t\t\tt.Run(\"Max\", func(t *testing.T) {\n\t\t\t\t\t\tpql = fmt.Sprintf(`Max(%s)`, fld)\n\t\t\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{TimestampVal: test.set, Count: 1}) {\n\t\t\t\t\t\t\tt.Fatalf(\"unexpected max result, test %d: %s\", i, spew.Sdump(result))\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\t\t\t\t})\n\t\t\t}\n\t\t})\n\t})\n\n\tt.Run(\"ColumnID\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\tidx, err := hldr.CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := idx.CreateField(\"x\", \"\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := idx.CreateField(\"f\", \"\", pilosa.OptFieldTypeInt(-1100, 1000)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\t\tSet(0, x=0)\n\t\t\tSet(3, x=0)\n\t\t\tSet(` + strconv.Itoa(ShardWidth+1) + `, x=0)\n\t\t\tSet(1, x=1)\n\t\t\tSet(` + strconv.Itoa(ShardWidth+2) + `, x=2)\n\n\t\t\tSet(0, f=20)\n\t\t\tSet(1, f=-5)\n\t\t\tSet(2, f=-5)\n\t\t\tSet(3, f=10)\n\t\t\tSet(` + strconv.Itoa(ShardWidth) + `, f=30)\n\t\t\tSet(` + strconv.Itoa(ShardWidth+2) + `, f=40)\n\t\t\tSet(` + strconv.Itoa((5*ShardWidth)+100) + `, f=50)\n\t\t\tSet(` + strconv.Itoa(ShardWidth+1) + `, f=60)\n\t\t`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tt.Run(\"Min\", func(t *testing.T) {\n\t\t\ttests := []struct {\n\t\t\t\tfilter string\n\t\t\t\texp    int64\n\t\t\t\tcnt    int64\n\t\t\t}{\n\t\t\t\t{filter: ``, exp: -5, cnt: 2},\n\t\t\t\t{filter: `Row(x=0)`, exp: 10, cnt: 1},\n\t\t\t\t{filter: `Row(x=1)`, exp: -5, cnt: 1},\n\t\t\t\t{filter: `Row(x=2)`, exp: 40, cnt: 1},\n\t\t\t}\n\t\t\tfor i, tt := range tests {\n\t\t\t\tvar pql string\n\t\t\t\tif tt.filter == \"\" {\n\t\t\t\t\tpql = `Min(field=f)`\n\t\t\t\t} else {\n\t\t\t\t\tpql = fmt.Sprintf(`Min(%s, field=f)`, tt.filter)\n\t\t\t\t}\n\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{Val: tt.exp, Count: tt.cnt}) {\n\t\t\t\t\tt.Fatalf(\"unexpected result, test %d: %s\", i, spew.Sdump(result))\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t})\n\n\tt.Run(\"ColumnKey\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\tidx, err := hldr.CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{Keys: true})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := idx.CreateField(\"x\", \"\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := idx.CreateField(\"f\", \"\", pilosa.OptFieldTypeInt(-1110, 1000)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\t\tSet(\"zero\", x=0)\n\t\t\tSet(\"three\", x=0)\n\t\t\tSet(\"sw1\", x=0)\n\t\t\tSet(\"one\", x=1)\n\t\t\tSet(\"sw2\", x=2)\n\n\t\t\tSet(\"zero\", f=20)\n\t\t\tSet(\"one\", f=-5)\n\t\t\tSet(\"two\", f=-5)\n\t\t\tSet(\"three\", f=10)\n\t\t\tSet(\"sw\", f=30)\n\t\t\tSet(\"sw2\", f=40)\n\t\t\tSet(\"sw3\", f=50)\n\t\t\tSet(\"sw1\", f=60)\n\t\t`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tt.Run(\"Min\", func(t *testing.T) {\n\t\t\ttests := []struct {\n\t\t\t\tfilter string\n\t\t\t\texp    int64\n\t\t\t\tcnt    int64\n\t\t\t}{\n\t\t\t\t{filter: ``, exp: -5, cnt: 2},\n\t\t\t\t{filter: `Row(x=0)`, exp: 10, cnt: 1},\n\t\t\t\t{filter: `Row(x=1)`, exp: -5, cnt: 1},\n\t\t\t\t{filter: `Row(x=2)`, exp: 40, cnt: 1},\n\t\t\t}\n\t\t\tfor i, tt := range tests {\n\t\t\t\tvar pql string\n\t\t\t\tif tt.filter == \"\" {\n\t\t\t\t\tpql = `Min(field=f)`\n\t\t\t\t} else {\n\t\t\t\t\tpql = fmt.Sprintf(`Min(%s, field=f)`, tt.filter)\n\t\t\t\t}\n\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{Val: tt.exp, Count: tt.cnt}) {\n\t\t\t\t\tt.Fatalf(\"unexpected result, test %d: %s\", i, spew.Sdump(result))\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"Max\", func(t *testing.T) {\n\t\t\ttests := []struct {\n\t\t\t\tfilter string\n\t\t\t\texp    int64\n\t\t\t\tcnt    int64\n\t\t\t}{\n\t\t\t\t{filter: ``, exp: 60, cnt: 1},\n\t\t\t\t{filter: `Row(x=0)`, exp: 60, cnt: 1},\n\t\t\t\t{filter: `Row(x=1)`, exp: -5, cnt: 1},\n\t\t\t\t{filter: `Row(x=2)`, exp: 40, cnt: 1},\n\t\t\t}\n\t\t\tfor i, tt := range tests {\n\t\t\t\tvar pql string\n\t\t\t\tif tt.filter == \"\" {\n\t\t\t\t\tpql = `Max(field=f)`\n\t\t\t\t} else {\n\t\t\t\t\tpql = fmt.Sprintf(`Max(%s, field=f)`, tt.filter)\n\t\t\t\t}\n\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{Val: tt.exp, Count: tt.cnt}) {\n\t\t\t\t\tt.Fatalf(\"unexpected result, test %d: %s\", i, spew.Sdump(result))\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t})\n}\n\n// Ensure MinRow() and MaxRow() queries can be executed.\nfunc TestExecutor_Execute_MinMaxRow(t *testing.T) {\n\tt.Run(\"RowID\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\tidx, err := hldr.CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := idx.CreateField(\"f\", \"\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\t\tSet(0, f=7000)\n\t\t\tSet(3, f=50)\n\t\t\tSet(` + strconv.Itoa(ShardWidth+1) + `, f=10000)\n\t\t\tSet(1000, f=1)\n\t\t\tSet(` + strconv.Itoa(ShardWidth+2) + `, f=5000)\n\t\t`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tt.Run(\"MinRow\", func(t *testing.T) {\n\t\t\tresult, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: \"MinRow(field=f)\"})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\ttarget := pilosa.PairField{\n\t\t\t\tPair:  pilosa.Pair{ID: 1, Count: 1},\n\t\t\t\tField: \"f\",\n\t\t\t}\n\t\t\tif !reflect.DeepEqual(target, result.Results[0]) {\n\t\t\t\tt.Fatalf(\"unexpected result %v != %v\", target, result.Results[0])\n\t\t\t}\n\t\t})\n\t\tt.Run(\"MinRowNonExistent\", func(t *testing.T) {\n\t\t\t_, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: \"MinRow(field=fake)\"})\n\t\t\tif got, exp := err.Error(), \"executing: executeMinRow: mapping on primary node: field not found\"; got != exp {\n\t\t\t\tt.Fatalf(\"expected %v, got %v\", exp, got)\n\t\t\t}\n\t\t})\n\t\tt.Run(\"MaxRow\", func(t *testing.T) {\n\t\t\tresult, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: \"MaxRow(field=f)\"})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\ttarget := pilosa.PairField{\n\t\t\t\tPair:  pilosa.Pair{ID: 10000, Count: 1},\n\t\t\t\tField: \"f\",\n\t\t\t}\n\t\t\tif !reflect.DeepEqual(target, result.Results[0]) {\n\t\t\t\tt.Fatalf(\"unexpected result %v != %v\", target, result.Results[0])\n\t\t\t}\n\t\t})\n\t\tt.Run(\"MaxRowNonExistent\", func(t *testing.T) {\n\t\t\t_, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: \"MaxRow(field=fake)\"})\n\t\t\tif got, exp := err.Error(), \"executing: executeMaxRow: mapping on primary node: field not found\"; got != exp {\n\t\t\t\tt.Fatalf(\"expected %v, got %v\", exp, got)\n\t\t\t}\n\t\t})\n\t})\n\n\tt.Run(\"RowKey\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\tidx, err := hldr.CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := idx.CreateField(\"f\", \"\", pilosa.OptFieldKeys()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\t\tSet(0, f=\"seven-thousand\")\n\t\t\tSet(3, f=\"fifty\")\n\t\t\tSet(` + strconv.Itoa(ShardWidth+1) + `, f=\"ten-thousand\")\n\t\t\tSet(1000, f=\"one\")\n\t\t\tSet(` + strconv.Itoa(ShardWidth+2) + `, f=\"five-thousand\")\n\t\t`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tt.Run(\"MinRow\", func(t *testing.T) {\n\t\t\tresult, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: \"MinRow(field=f)\"})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\ttarget := pilosa.PairField{\n\t\t\t\tPair:  pilosa.Pair{Key: \"seven-thousand\", ID: 1, Count: 1},\n\t\t\t\tField: \"f\",\n\t\t\t}\n\t\t\tif !reflect.DeepEqual(target, result.Results[0]) {\n\t\t\t\tt.Fatalf(\"unexpected result %v != %v\", target, result.Results[0])\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"MaxRow\", func(t *testing.T) {\n\t\t\tresult, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: \"MaxRow(field=f)\"})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\ttarget := pilosa.PairField{\n\t\t\t\tPair:  pilosa.Pair{Key: \"five-thousand\", ID: 5, Count: 1},\n\t\t\t\tField: \"f\",\n\t\t\t}\n\t\t\tif !reflect.DeepEqual(target, result.Results[0]) {\n\t\t\t\tt.Fatalf(\"unexpected result %v != %v\", target, result.Results[0])\n\t\t\t}\n\t\t})\n\t})\n}\n\n// Ensure a Sum() query can be executed.\nfunc TestExecutor_Execute_Sum(t *testing.T) {\n\tt.Run(\"ColumnID\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\tidx, err := hldr.CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := idx.CreateField(\"x\", \"\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := idx.CreateField(\"foo\", \"\", pilosa.OptFieldTypeInt(-990, 1000)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := idx.CreateField(\"bar\", \"\", pilosa.OptFieldTypeInt(math.MinInt64, math.MaxInt64)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := idx.CreateField(\"other\", \"\", pilosa.OptFieldTypeInt(math.MinInt64, math.MaxInt64)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := idx.CreateField(\"dec\", \"\", pilosa.OptFieldTypeDecimal(3)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\t\tSet(0, x=0)\n\t\t\tSet(` + strconv.Itoa(ShardWidth+1) + `, x=0)\n\n\t\t\tSet(0, foo=20)\n\t\t\tSet(0, bar=2000)\n\t\t\tSet(` + strconv.Itoa(ShardWidth) + `, foo=30)\n\t\t\tSet(` + strconv.Itoa(ShardWidth+2) + `, foo=40)\n\t\t\tSet(` + strconv.Itoa((5*ShardWidth)+100) + `, foo=50)\n\t\t\tSet(` + strconv.Itoa(ShardWidth+1) + `, foo=60)\n\t\t\tSet(0, other=1000)\n\n\t\t\tSet(0, dec=100.001)\n\t\t\tSet(` + strconv.Itoa(ShardWidth) + `, dec=200.002)\n\t\t\tSet(` + strconv.Itoa(ShardWidth+1) + `, dec=400.004)\n\t\t`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tt.Run(\"Integer\", func(t *testing.T) {\n\t\t\tt.Run(\"NoFilter\", func(t *testing.T) {\n\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Sum(field=foo)`}); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{Val: 200, Count: 5}) {\n\t\t\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"NoFilter\", func(t *testing.T) {\n\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Sum(field=\"foo\")`}); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{Val: 200, Count: 5}) {\n\t\t\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"NoFilter\", func(t *testing.T) {\n\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Sum(foo)`}); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{Val: 200, Count: 5}) {\n\t\t\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"WithFilter\", func(t *testing.T) {\n\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Sum(Row(x=0), field=foo)`}); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{Val: 80, Count: 2}) {\n\t\t\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"WithFilter\", func(t *testing.T) {\n\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Sum(foo, Row(x=0))`}); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{Val: 80, Count: 2}) {\n\t\t\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t\t\t}\n\t\t\t})\n\t\t})\n\n\t\tt.Run(\"SumNonExistent\", func(t *testing.T) {\n\t\t\t_, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Sum(field=fake)`})\n\t\t\tif err.Error() != \"executing: executeSum: mapping on primary node: field not found\" {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"Decimal\", func(t *testing.T) {\n\t\t\tt.Run(\"NoFilter\", func(t *testing.T) {\n\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Sum(field=dec)`}); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{DecimalVal: pql.NewDecimal(700007, 3).Clone(), Count: 3}) {\n\t\t\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"WithFilter\", func(t *testing.T) {\n\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Sum(Row(x=0), field=dec)`}); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{DecimalVal: pql.NewDecimal(500005, 3).Clone(), Count: 2}) {\n\t\t\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"NoFilter\", func(t *testing.T) {\n\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Sum(dec)`}); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{DecimalVal: pql.NewDecimal(700007, 3).Clone(), Count: 3}) {\n\t\t\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"WithFilter\", func(t *testing.T) {\n\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Sum(dec, Row(x=0))`}); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{DecimalVal: pql.NewDecimal(500005, 3).Clone(), Count: 2}) {\n\t\t\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t\t\t}\n\t\t\t})\n\t\t})\n\t})\n\n\tt.Run(\"ColumnKey\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\tidx, err := hldr.CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{Keys: true})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := idx.CreateField(\"x\", \"\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := idx.CreateField(\"foo\", \"\", pilosa.OptFieldTypeInt(-990, 1000)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := idx.CreateField(\"bar\", \"\", pilosa.OptFieldTypeInt(math.MinInt64, math.MaxInt64)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := idx.CreateField(\"other\", \"\", pilosa.OptFieldTypeInt(math.MinInt64, math.MaxInt64)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\t\tSet(\"zero\", x=0)\n\t\t\tSet(\"sw1\", x=0)\n\n\t\t\tSet(\"zero\", foo=20)\n\t\t\tSet(\"zero\", bar=2000)\n\t\t\tSet(\"sw\", foo=30)\n\t\t\tSet(\"sw2\", foo=40)\n\t\t\tSet(\"sw3\", foo=50)\n\t\t\tSet(\"sw1\", foo=60)\n\t\t\tSet(\"zero\", other=1000)\n\t\t`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tt.Run(\"NoFilter\", func(t *testing.T) {\n\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Sum(field=foo)`}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{Val: 200, Count: 5}) {\n\t\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"WithFilter\", func(t *testing.T) {\n\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Sum(Row(x=0), field=foo)`}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{Val: 80, Count: 2}) {\n\t\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t\t}\n\t\t})\n\t})\n}\n\n// Ensure decimal args are supported for Decimal fields.\nfunc TestExecutor_DecimalArgs(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\thldr := c.GetHolder(0)\n\n\tidx, err := hldr.CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tmin, err := pql.ParseDecimal(\"-10.5\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tmax, err := pql.ParseDecimal(\"10.5\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, err := idx.CreateField(\"f\", \"\", pilosa.OptFieldTypeDecimal(2, min, max)); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\tSet(0, f=0)\n\t`}); err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\n// Ensure a Row(bsiGroup) query can be executed.\nfunc TestExecutor_Execute_Row_BSIGroup(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\thldr := c.GetHolder(0)\n\n\tidx, err := hldr.CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{TrackExistence: true})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, err := idx.CreateField(\"f\", \"\"); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, err := idx.CreateField(\"foo\", \"\", pilosa.OptFieldTypeInt(-990, 1000)); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, err := idx.CreateField(\"bar\", \"\", pilosa.OptFieldTypeInt(math.MinInt64, math.MaxInt64)); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, err := idx.CreateField(\"other\", \"\", pilosa.OptFieldTypeInt(math.MinInt64, math.MaxInt64)); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, err := idx.CreateField(\"edge\", \"\", pilosa.OptFieldTypeInt(-900, 1000)); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, err := idx.CreateField(\"idset\", \"\", pilosa.OptFieldTypeSet(\"none\", 0)); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, err := idx.CreateField(\"tq\", \"\", pilosa.OptFieldTypeTime(\"YM\", \"0\")); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\tSet(0, f=0)\n\t\tSet(` + strconv.Itoa(ShardWidth+1) + `, f=0)\n\n\t\tSet(50, foo=20)\n\t\tSet(50, bar=2000)\n\t\tSet(` + strconv.Itoa(ShardWidth) + `, foo=30)\n\t\tSet(` + strconv.Itoa(ShardWidth+2) + `, foo=10)\n\t\tSet(` + strconv.Itoa((5*ShardWidth)+100) + `, foo=20)\n\t\tSet(` + strconv.Itoa(ShardWidth+1) + `, foo=60)\n\t\tSet(0, other=1000)\n\t\tSet(0, edge=100)\n\t\tSet(1, edge=-100)\n\t\tSet(0, idset=3)\n\t\tSet(1, idset=3)\n\t\tClear(0, idset=3)\n\t\tSet(50, tq=5, 2017-01-02T12:34)\n\t`}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tt.Run(\"EQ\", func(t *testing.T) {\n\t\t// EQ null\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(other == null)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{\n\t\t\t1,\n\t\t\t50,\n\t\t\tShardWidth,\n\t\t\tShardWidth + 1,\n\t\t\tShardWidth + 2,\n\t\t\t(5 * ShardWidth) + 100,\n\t\t}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %#v\", result.Results[0].(*pilosa.Row).Columns())\n\t\t}\n\t\t// EQ <int>\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(foo == 20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, exp := result.Results[0].(*pilosa.Row).Columns(), []uint64{50, (5 * ShardWidth) + 100}; !reflect.DeepEqual(exp, got) {\n\t\t\tt.Fatalf(\"Query().Row.Columns=%#v, expected %#v\", got, exp)\n\t\t}\n\n\t\t// time quantum EQ null\n\t\t_, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(tq == null, from=2010-01-01T00:00)`})\n\t\tif err == nil {\n\t\t\tt.Fatalf(\"expected error from invalid time quantum null query\")\n\t\t}\n\t\tif !strings.Contains(err.Error(), \"time range with a check\") {\n\t\t\tt.Fatalf(\"unexpected error; expecting can't use time range with a null check, got %v\", err)\n\t\t}\n\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(tq == null)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{\n\t\t\t0,\n\t\t\t1,\n\t\t\tShardWidth,\n\t\t\tShardWidth + 1,\n\t\t\tShardWidth + 2,\n\t\t\t(5 * ShardWidth) + 100,\n\t\t}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %#v\", result.Results[0].(*pilosa.Row).Columns())\n\t\t}\n\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(idset == null)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{\n\t\t\t50,\n\t\t\tShardWidth,\n\t\t\tShardWidth + 1,\n\t\t\tShardWidth + 2,\n\t\t\t(5 * ShardWidth) + 100,\n\t\t}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %#v\", result.Results[0].(*pilosa.Row).Columns())\n\t\t}\n\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(idset == 3)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{1}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %#v\", result.Results[0].(*pilosa.Row).Columns())\n\t\t}\n\n\t\t// EQ (single = form) <int>\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(foo = 20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, exp := result.Results[0].(*pilosa.Row).Columns(), []uint64{50, (5 * ShardWidth) + 100}; !reflect.DeepEqual(exp, got) {\n\t\t\tt.Fatalf(\"Query().Row.Columns=%#v, expected %#v\", got, exp)\n\t\t}\n\t})\n\n\tt.Run(\"NEQ\", func(t *testing.T) {\n\t\t// NEQ null\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(other != null)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{0}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %#v\", result.Results[0].(*pilosa.Row).Columns())\n\t\t}\n\t\t// NEQ <int>\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(foo != 20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{ShardWidth, ShardWidth + 1, ShardWidth + 2}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %#v\", result.Results[0].(*pilosa.Row).Columns())\n\t\t}\n\t\t// NEQ -<int>\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(other != -20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{0}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %#v\", result.Results[0].(*pilosa.Row).Columns())\n\t\t}\n\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(tq != null)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{50}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %#v\", result.Results[0].(*pilosa.Row).Columns())\n\t\t}\n\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(idset != null)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{0, 1}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %#v\", result.Results[0].(*pilosa.Row).Columns())\n\t\t}\n\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(idset != 3)`}); err == nil {\n\t\t\tt.Fatal(\"expected error from trying != 3 on a set field\")\n\t\t} else if !strings.Contains(err.Error(), \"only support != for null\") {\n\t\t\tt.Fatalf(\"expected error about only supporting != for null, got %v\", err)\n\t\t}\n\t})\n\n\tt.Run(\"LT\", func(t *testing.T) {\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(foo < 20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{ShardWidth + 2}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t}\n\t})\n\n\tt.Run(\"LTE\", func(t *testing.T) {\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(foo <= 20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, exp := result.Results[0].(*pilosa.Row).Columns(), []uint64{50, ShardWidth + 2, (5 * ShardWidth) + 100}; !reflect.DeepEqual(got, exp) {\n\t\t\tt.Fatalf(\"unexpected result: got=%v, exp=%v\", got, exp)\n\t\t}\n\t})\n\n\tt.Run(\"GT\", func(t *testing.T) {\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(foo > 20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{ShardWidth, ShardWidth + 1}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %v\", result.Results[0].(*pilosa.Row).Columns())\n\t\t}\n\t})\n\n\tt.Run(\"GTE\", func(t *testing.T) {\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(foo >= 20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{50, ShardWidth, ShardWidth + 1, (5 * ShardWidth) + 100}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %v\", result.Results[0].(*pilosa.Row).Columns())\n\t\t}\n\t})\n\n\tt.Run(\"BETWEEN\", func(t *testing.T) {\n\t\ttests := []struct {\n\t\t\tq   string\n\t\t\texp bool\n\t\t}{\n\t\t\t{q: `Row(0 < other < 1000)`, exp: false},\n\t\t\t{q: `Row(0 <= other < 1000)`, exp: false},\n\t\t\t{q: `Row(0 <= other <= 1000)`, exp: true},\n\t\t\t{q: `Row(0 < other <= 1000)`, exp: true},\n\n\t\t\t{q: `Row(1000 < other < 1000)`, exp: false},\n\t\t\t{q: `Row(1000 <= other < 1000)`, exp: false},\n\t\t\t{q: `Row(1000 <= other <= 1000)`, exp: true},\n\t\t\t{q: `Row(1000 < other <= 1000)`, exp: false},\n\n\t\t\t{q: `Row(1000 < other < 2000)`, exp: false},\n\t\t\t{q: `Row(1000 <= other < 20000)`, exp: true},\n\t\t\t{q: `Row(1000 <= other <= 2000)`, exp: true},\n\t\t\t{q: `Row(1000 < other <= 2000)`, exp: false},\n\t\t}\n\t\tfor i, test := range tests {\n\t\t\tt.Run(fmt.Sprintf(\"#%d_%s\", i, test.q), func(t *testing.T) {\n\t\t\t\texpected := []uint64{}\n\t\t\t\tif test.exp {\n\t\t\t\t\texpected = []uint64{0}\n\t\t\t\t}\n\t\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: test.q}); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t} else if !reflect.DeepEqual(expected, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\t\t\tt.Fatalf(\"unexpected result for query: %s (%#v)\", test.q, result.Results[0].(*pilosa.Row).Columns())\n\t\t\t\t}\n\t\t\t})\n\t\t}\n\t})\n\n\t// Ensure that the NotNull code path gets run.\n\tt.Run(\"NotNull\", func(t *testing.T) {\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(0 <= other <= 1000)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{0}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t}\n\t})\n\n\tt.Run(\"BelowMin\", func(t *testing.T) {\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(foo == 0)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t}\n\t})\n\n\tt.Run(\"AboveMax\", func(t *testing.T) {\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(foo == 200)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t}\n\t})\n\n\tt.Run(\"LTAboveMax\", func(t *testing.T) {\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(edge < 200)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{0, 1}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result.Results[0].(*pilosa.Row).Columns()))\n\t\t}\n\t})\n\n\tt.Run(\"GTBelowMin\", func(t *testing.T) {\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(edge > -1000)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{0, 1}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result.Results[0].(*pilosa.Row).Columns()))\n\t\t}\n\t})\n\n\tt.Run(\"ErrFieldNotFound\", func(t *testing.T) {\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(bad_field >= 20)`}); errors.Cause(err) != pilosa.ErrFieldNotFound {\n\t\t\tt.Fatal(err)\n\t\t}\n\t})\n}\n\n// Ensure a Row(bsiGroup) query can be executed (edge cases).\nfunc TestExecutor_Execute_Row_BSIGroupEdge(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\thldr := c.GetHolder(0)\n\n\tidx, err := hldr.CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tt.Run(\"LT\", func(t *testing.T) {\n\t\tif _, err := idx.CreateField(\"f1\", \"\", pilosa.OptFieldTypeInt(-2000, 2000)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Set a value at the edge of bitDepth (i.e. 2^n-1; here, n=3).\n\t\t// It must also be the max value in the field; in other words,\n\t\t// set the value to bsiGroup.bitDepthMax().\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\t\tSet(100, f1=7)\n\t\t`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(f1 < 10)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, exp := result.Results[0].(*pilosa.Row).Columns(), []uint64{100}; !reflect.DeepEqual(got, exp) {\n\t\t\tt.Fatalf(\"unexpected result: got=%v, exp=%v\", got, exp)\n\t\t}\n\t})\n\n\tt.Run(\"GT\", func(t *testing.T) {\n\t\tif _, err := idx.CreateField(\"f2\", \"\", pilosa.OptFieldTypeInt(-2000, 2000)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Set a value at the negative edge of bitDepth (i.e. -(2^n-1); here, n=3).\n\t\t// It must also be the min value in the field; in other words,\n\t\t// set the value to bsiGroup.bitDepthMin().\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\tSet(200, f2=-7)\n\t`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(f2 > -10)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, exp := result.Results[0].(*pilosa.Row).Columns(), []uint64{200}; !reflect.DeepEqual(got, exp) {\n\t\t\tt.Fatalf(\"unexpected result: got=%v, exp=%v\", got, exp)\n\t\t}\n\t})\n\n\tt.Run(\"BTWN_LT_LT\", func(t *testing.T) {\n\t\tif _, err := idx.CreateField(\"f3\", \"\", pilosa.OptFieldTypeInt(-2000, 2000)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Set a value anywhere in range.\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\tSet(300, f3=10)\n\t`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Query INT_MAX < x < INT_MIN. Because that's an invalid range, we should\n\t\t// get back an empty result set.\n\t\ttests := []struct {\n\t\t\tpredA int64\n\t\t\tpredB int64\n\t\t}{\n\t\t\t{math.MaxInt64, math.MinInt64},\n\t\t\t{math.MaxInt64, 1000},\n\t\t\t{-1000, math.MinInt64},\n\t\t}\n\n\t\tfor i, test := range tests {\n\t\t\tpql := fmt.Sprintf(\"Row(%d < f3 < %d)\", test.predA, test.predB)\n\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if got, exp := result.Results[0].(*pilosa.Row).Columns(), []uint64{}; !reflect.DeepEqual(got, exp) {\n\t\t\t\tt.Fatalf(\"test %d unexpected result: got=%v, exp=%v\", i, got, exp)\n\t\t\t}\n\t\t}\n\t})\n}\n\n// Ensure a Range(bsiGroup) query can be executed. (Deprecated)\nfunc TestExecutor_Execute_Range_BSIGroup_Deprecated(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\thldr := c.GetHolder(0)\n\n\tidx, err := hldr.CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, err := idx.CreateField(\"f\", \"\"); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, err := idx.CreateField(\"foo\", \"\", pilosa.OptFieldTypeInt(-990, 1000)); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, err := idx.CreateField(\"bar\", \"\", pilosa.OptFieldTypeInt(math.MinInt64, math.MaxInt64)); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, err := idx.CreateField(\"other\", \"\", pilosa.OptFieldTypeInt(math.MinInt64, math.MaxInt64)); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, err := idx.CreateField(\"edge\", \"\", pilosa.OptFieldTypeInt(-1100, 1000)); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\tSet(0, f=0)\n\t\tSet(` + strconv.Itoa(ShardWidth+1) + `, f=0)\n\n\t\tSet(50, foo=20)\n\t\tSet(50, bar=2000)\n\t\tSet(` + strconv.Itoa(ShardWidth) + `, foo=30)\n\t\tSet(` + strconv.Itoa(ShardWidth+2) + `, foo=10)\n\t\tSet(` + strconv.Itoa((5*ShardWidth)+100) + `, foo=20)\n\t\tSet(` + strconv.Itoa(ShardWidth+1) + `, foo=60)\n\t\tSet(0, other=1000)\n\t\tSet(0, edge=100)\n\t\tSet(1, edge=-100)\n\t`}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tt.Run(\"EQ\", func(t *testing.T) {\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Range(foo == 20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{50, (5 * ShardWidth) + 100}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t}\n\t})\n\n\tt.Run(\"NEQ\", func(t *testing.T) {\n\t\t// NEQ null\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Range(other != null)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{0}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t}\n\t\t// NEQ <int>\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Range(foo != 20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{ShardWidth, ShardWidth + 1, ShardWidth + 2}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t}\n\t\t// NEQ -<int>\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Range(other != -20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{0}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %v\", result.Results[0].(*pilosa.Row).Columns())\n\t\t}\n\t})\n\n\tt.Run(\"LT\", func(t *testing.T) {\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Range(foo < 20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{ShardWidth + 2}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t}\n\t})\n\n\tt.Run(\"LTE\", func(t *testing.T) {\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Range(foo <= 20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{50, ShardWidth + 2, (5 * ShardWidth) + 100}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t}\n\t})\n\n\tt.Run(\"GT\", func(t *testing.T) {\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Range(foo > 20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{ShardWidth, ShardWidth + 1}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t}\n\t})\n\n\tt.Run(\"GTE\", func(t *testing.T) {\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Range(foo >= 20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{50, ShardWidth, ShardWidth + 1, (5 * ShardWidth) + 100}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t}\n\t})\n\n\tt.Run(\"BETWEEN\", func(t *testing.T) {\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Range(0 < other < 1000)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t}\n\t})\n\n\t// Ensure that the NotNull code path gets run.\n\tt.Run(\"NotNull\", func(t *testing.T) {\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Range(0 <= other <= 1000)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{0}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t}\n\t})\n\n\tt.Run(\"BelowMin\", func(t *testing.T) {\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Range(foo == 0)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t}\n\t})\n\n\tt.Run(\"AboveMax\", func(t *testing.T) {\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Range(foo == 200)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result))\n\t\t}\n\t})\n\n\tt.Run(\"LTAboveMax\", func(t *testing.T) {\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Range(edge < 200)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{0, 1}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result.Results[0].(*pilosa.Row).Columns()))\n\t\t}\n\t})\n\n\tt.Run(\"GTBelowMin\", func(t *testing.T) {\n\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Range(edge > -1200)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual([]uint64{0, 1}, result.Results[0].(*pilosa.Row).Columns()) {\n\t\t\tt.Fatalf(\"unexpected result: %s\", spew.Sdump(result.Results[0].(*pilosa.Row).Columns()))\n\t\t}\n\t})\n\n\tt.Run(\"ErrFieldNotFound\", func(t *testing.T) {\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Range(bad_field >= 20)`}); errors.Cause(err) != pilosa.ErrFieldNotFound {\n\t\t\tt.Fatal(err)\n\t\t}\n\t})\n}\n\n// Ensure a remote query can return a row.\nfunc TestExecutor_Execute_Remote_Row(t *testing.T) {\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\thldr1 := c.GetHolder(1)\n\tchild := c.Idx(\"c\")\n\tparent := c.Idx(\"p\")\n\n\t_, err := c.GetPrimary().API.CreateIndex(context.Background(), c.Idx(), pilosa.IndexOptions{})\n\tif err != nil {\n\t\tt.Fatalf(\"creating index: %v\", err)\n\t}\n\t_, err = c.GetPrimary().API.CreateField(context.Background(), c.Idx(), \"f\", pilosa.OptFieldTypeSet(pilosa.DefaultCacheType, pilosa.DefaultCacheSize))\n\tif err != nil {\n\t\tt.Fatalf(\"creating field: %v\", err)\n\t}\n\tclient := MustNewClient(c.GetNode(0).URL(), pilosa.GetHTTPClient(nil))\n\treq := &pilosa.ImportRequest{\n\t\tIndex:     c.Idx(),\n\t\tField:     \"f\",\n\t\tRowIDs:    []uint64{10, 10, 10, 10},\n\t\tColumnIDs: []uint64{1, ShardWidth + 1, ShardWidth + 2, (3 * ShardWidth) + 4},\n\t\tShard:     ^uint64(0),\n\t}\n\terr = client.Import(context.Background(), nil, req, &pilosa.ImportOptions{})\n\tif err != nil {\n\t\tt.Fatalf(\"importing data: %v\", err)\n\t}\n\n\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(f=10)`}); err != nil {\n\t\tt.Fatal(err)\n\t} else if columns := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{1, ShardWidth + 1, ShardWidth + 2, (3 * ShardWidth) + 4}) {\n\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t}\n\n\tt.Run(\"Count\", func(t *testing.T) {\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Count(Row(f=10))`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if res.Results[0] != uint64(4) {\n\t\t\tt.Fatalf(\"unexpected n: %d\", res.Results[0])\n\t\t}\n\t})\n\n\tt.Run(\"Remote SetBit\", func(t *testing.T) {\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: fmt.Sprintf(`Set(%d, f=7)`, pilosa.ShardWidth+1)}); err != nil {\n\t\t\tt.Fatalf(\"querying remote: %v\", err)\n\t\t}\n\n\t\t// We shouldn't need to specify hldr1, and which holder we need varies in a way that is clearly broken.\n\t\tif !reflect.DeepEqual(hldr1.Row(c.Idx(), \"f\", 7).Columns(), []uint64{pilosa.ShardWidth + 1}) {\n\t\t\tt.Fatalf(\"unexpected cols from row 7: %v\", hldr1.Row(c.Idx(), \"f\", 7).Columns())\n\t\t}\n\t})\n\n\tt.Run(\"remote with timestamp\", func(t *testing.T) {\n\t\t_, err = c.GetPrimary().API.CreateField(context.Background(), c.Idx(), \"z\", pilosa.OptFieldTypeTime(\"Y\", \"0\"))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating field: %v\", err)\n\t\t}\n\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: fmt.Sprintf(`Set(%d, z=5, 2010-07-08T00:00)`, pilosa.ShardWidth+1)}); err != nil {\n\t\t\tt.Fatalf(\"querying remote: %v\", err)\n\t\t}\n\n\t\t// We shouldn't need to specify hldr1, and which holder we need varies in a way that is clearly broken.\n\t\tif !reflect.DeepEqual(hldr1.RowTime(c.Idx(), \"z\", 5, time.Date(2010, time.January, 1, 0, 0, 0, 0, time.UTC), \"Y\").Columns(), []uint64{pilosa.ShardWidth + 1}) {\n\t\t\tt.Fatalf(\"unexpected cols from row 7: %v\", hldr1.RowTime(c.Idx(), \"z\", 5, time.Date(2010, time.January, 1, 0, 0, 0, 0, time.UTC), \"Y\").Columns())\n\t\t}\n\t})\n\n\tt.Run(\"remote topn\", func(t *testing.T) {\n\t\t_, err = c.GetPrimary().API.CreateField(context.Background(), c.Idx(), \"fn\", pilosa.OptFieldTypeSet(pilosa.CacheTypeRanked, 100))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating field: %v\", err)\n\t\t}\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\tSet(500001, fn=5)\n\tSet(1500001, fn=5)\n\tSet(2500001, fn=5)\n\tSet(3500001, fn=5)\n\tSet(1500001, fn=3)\n\tSet(1500002, fn=3)\n\tSet(3500003, fn=3)\n\tSet(500001, fn=4)\n\tSet(4500001, fn=4)\n\t`}); err != nil {\n\t\t\tt.Fatalf(\"querying remote: %v\", err)\n\t\t}\n\n\t\tif res, err := c.GetNode(1).API.Query(context.Background(), &pilosa.QueryRequest{\n\t\t\tIndex: c.Idx(),\n\t\t\tQuery: `TopN(fn, n=3)`,\n\t\t}); err != nil {\n\t\t\tt.Fatalf(\"topn querying: %v\", err)\n\t\t} else if !reflect.DeepEqual(res.Results, []interface{}{&pilosa.PairsField{\n\t\t\tPairs: []pilosa.Pair{\n\t\t\t\t{ID: 5, Count: 4},\n\t\t\t\t{ID: 3, Count: 3},\n\t\t\t\t{ID: 4, Count: 2},\n\t\t\t},\n\t\t\tField: \"fn\",\n\t\t}}) {\n\t\t\tt.Fatalf(\"topn wrong results: %v\", res.Results)\n\t\t}\n\t})\n\n\tt.Run(\"remote groupBy\", func(t *testing.T) {\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{\n\t\t\tIndex: c.Idx(),\n\t\t\tQuery: `GroupBy(Rows(f))`,\n\t\t}); err != nil {\n\t\t\tt.Fatalf(\"GroupBy querying: %v\", err)\n\t\t} else {\n\t\t\texpected := []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"f\", RowID: 7}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"f\", RowID: 10}}, Count: 4},\n\t\t\t}\n\t\t\tresults := res.Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\ttest.CheckGroupBy(t, expected, results)\n\t\t}\n\t})\n\n\tt.Run(\"json format groupBy on timestamps\", func(t *testing.T) {\n\t\t// SUP-138\n\t\tc.CreateField(t, c.Idx(\"t\"), pilosa.IndexOptions{TrackExistence: true}, \"timestamp\", pilosa.OptFieldTypeTimestamp(pilosa.DefaultEpoch, pilosa.TimeUnitSeconds))\n\t\tc.Query(t, c.Idx(\"t\"), `\n\t\tSet(8, timestamp='2021-01-27T08:00:00Z')\n\t\tSet(9, timestamp='2000-01-27T09:00:00Z')\n\t\tSet(10, timestamp='2000-01-27T10:00:00Z')\n\t`)\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{\n\t\t\tIndex: c.Idx(\"t\"),\n\t\t\tQuery: `GroupBy(Rows(timestamp))`,\n\t\t}); err != nil {\n\t\t\tt.Fatalf(\"GroupBy querying: %v\", err)\n\t\t} else {\n\t\t\tb, _ := res.MarshalJSON()\n\t\t\texpected := `{\"results\":[[{\"group\":[{\"field\":\"timestamp\",\"value\":\"2000-01-27T09:00:00Z\"}],\"count\":1},{\"group\":[{\"field\":\"timestamp\",\"value\":\"2000-01-27T10:00:00Z\"}],\"count\":1},{\"group\":[{\"field\":\"timestamp\",\"value\":\"2021-01-27T08:00:00Z\"}],\"count\":1}]]}`\n\t\t\tif string(b) != expected {\n\t\t\t\tt.Fatalf(\"JSON FORMAT not as expected: %v\", err)\n\t\t\t}\n\t\t}\n\t})\n\n\tt.Run(\"remote groupBy on ints\", func(t *testing.T) {\n\t\t_, err = c.GetPrimary().API.CreateField(context.Background(), c.Idx(), \"fint\", pilosa.OptFieldTypeInt(-1000, 1000))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating field: %v\", err)\n\t\t}\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\tSet(0, fint=1)\n\t\tSet(1, fint=2)\n\n\t\tSet(2,fint=-2)\n\t\tSet(3,fint=-1)\n\n\t\tSet(4,fint=4)\n\n\t\tSet(10, fint=0)\n\t\tSet(100, fint=0)\n\t\tSet(1000, fint=0)\n\t\tSet(10000,fint=0)\n\t\tSet(100000,fint=0)\n\t\t`}); err != nil {\n\t\t\tt.Fatalf(\"querying remote: %v\", err)\n\t\t}\n\n\t\tif res, err := c.GetNode(1).API.Query(context.Background(), &pilosa.QueryRequest{\n\t\t\tIndex: c.Idx(),\n\t\t\tQuery: `GroupBy(Rows(fint), limit=4, filter=Union(Row(fint < 1), Row(fint > 2)))`,\n\t\t}); err != nil {\n\t\t\tt.Fatalf(\"GroupBy querying: %v\", err)\n\t\t} else {\n\t\t\tvar a, b, c, d int64 = -2, -1, 0, 4\n\t\t\texpected := []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"fint\", Value: &a}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"fint\", Value: &b}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"fint\", Value: &c}}, Count: 5},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"fint\", Value: &d}}, Count: 1},\n\t\t\t}\n\n\t\t\tresults := res.Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\ttest.CheckGroupBy(t, expected, results)\n\t\t}\n\t})\n\n\tt.Run(\"groupBy on ints with offset regression\", func(t *testing.T) {\n\t\t_, err = c.GetPrimary().API.CreateField(context.Background(), c.Idx(), \"hint\", pilosa.OptFieldTypeInt(1, 1000))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating field: %v\", err)\n\t\t}\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\tSet(0, hint=1)\n\t\tSet(1, hint=2)\n\t\tSet(2, hint=3)\n\t\t`}); err != nil {\n\t\t\tt.Fatalf(\"querying remote: %v\", err)\n\t\t}\n\n\t\tif res, err := c.GetNode(1).API.Query(context.Background(), &pilosa.QueryRequest{\n\t\t\tIndex: c.Idx(),\n\t\t\tQuery: `GroupBy(Rows(hint))`,\n\t\t}); err != nil {\n\t\t\tt.Fatalf(\"GroupBy querying: %v\", err)\n\t\t} else {\n\t\t\tvar a, b, c int64 = 1, 2, 3\n\t\t\texpected := []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"hint\", Value: &a}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"hint\", Value: &b}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"hint\", Value: &c}}, Count: 1},\n\t\t\t}\n\n\t\t\tresults := res.Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\ttest.CheckGroupBy(t, expected, results)\n\t\t}\n\t})\n\n\tt.Run(\"Row on ints with ASSIGN condition\", func(t *testing.T) {\n\t\t_, err := c.GetPrimary().API.CreateIndex(context.Background(), c.Idx(\"intidx\"), pilosa.IndexOptions{})\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating index: %v\", err)\n\t\t}\n\n\t\t_, err = c.GetPrimary().API.CreateField(context.Background(), c.Idx(\"intidx\"), \"gint\", pilosa.OptFieldTypeInt(-1000, 1000))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating field: %v\", err)\n\t\t}\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(\"intidx\"), Query: `\n\t\tSet(1000, gint=1)\n\t\tSet(2000, gint=2)\n\t\tSet(3000, gint=3)\n\t\t`}); err != nil {\n\t\t\tt.Fatalf(\"querying remote: %v\", err)\n\t\t}\n\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{\n\t\t\tIndex: c.Idx(\"intidx\"),\n\t\t\tQuery: `Row(gint=2)Row(gint==1)`,\n\t\t}); err != nil {\n\t\t\tt.Fatalf(\"Row querying: %v\", err)\n\t\t} else {\n\n\t\t\trow0, row1 := res.Results[0].(*pilosa.Row), res.Results[1].(*pilosa.Row)\n\t\t\tif len(row0.Columns()) != 1 || len(row1.Columns()) != 1 {\n\t\t\t\tt.Fatalf(`Expected: []uint64{2000} []uint64{1000}, Got: %+v %+v`, row0.Columns(), row1.Columns())\n\t\t\t}\n\t\t\tif row0.Columns()[0] != 2000 || row1.Columns()[0] != 1000 {\n\t\t\t\tt.Fatalf(`Expected: []uint64{2000} []uint64{1000}, Got: %+v %+v`, row0.Columns(), row1.Columns())\n\t\t\t}\n\t\t}\n\t})\n\n\tt.Run(\"Row on decimals with ASSIGN condition\", func(t *testing.T) {\n\t\t_, err := c.GetPrimary().API.CreateIndex(context.Background(), c.Idx(\"decidx\"), pilosa.IndexOptions{})\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating index: %v\", err)\n\t\t}\n\n\t\t_, err = c.GetPrimary().API.CreateField(context.Background(), c.Idx(\"decidx\"), \"fdec\", pilosa.OptFieldTypeDecimal(0))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating field: %v\", err)\n\t\t}\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(\"decidx\"), Query: `\n\t\tSet(11, fdec=1.1)\n\t\tSet(22, fdec=2.2)\n\t\tSet(33, fdec=3.3)\n\t\t`}); err != nil {\n\t\t\tt.Fatalf(\"querying remote: %v\", err)\n\t\t}\n\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{\n\t\t\tIndex: c.Idx(\"decidx\"),\n\t\t\tQuery: `Row(fdec=2.2)Row(fdec==1.1)`,\n\t\t}); err != nil {\n\t\t\tt.Fatalf(\"Row querying: %v\", err)\n\t\t} else {\n\t\t\trow0, row1 := res.Results[0].(*pilosa.Row), res.Results[1].(*pilosa.Row)\n\t\t\tif len(row0.Columns()) != 1 || len(row1.Columns()) != 1 {\n\t\t\t\tt.Fatalf(`Expected: []uint64{22} []uint64{11}, Got: %+v %+v`, row0.Columns(), row1.Columns())\n\t\t\t}\n\t\t\tif row0.Columns()[0] != 22 || row1.Columns()[0] != 11 {\n\t\t\t\tt.Fatalf(`Expected: []uint64{22} []uint64{11}, Got: %+v %+v`, row0.Columns(), row1.Columns())\n\t\t\t}\n\t\t}\n\t})\n\n\tt.Run(\"Row on foreign key with ASSIGN condition\", func(t *testing.T) {\n\t\t_, err := c.GetPrimary().API.CreateIndex(context.Background(), parent, pilosa.IndexOptions{Keys: true})\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating index: %v\", err)\n\t\t}\n\t\t_, err = c.GetPrimary().API.CreateField(context.Background(), parent, \"general\", pilosa.OptFieldTypeSet(pilosa.DefaultCacheType, pilosa.DefaultCacheSize))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating field: %v\", err)\n\t\t}\n\t\t_, err = c.GetPrimary().API.CreateIndex(context.Background(), child, pilosa.IndexOptions{Keys: false})\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating index: %v\", err)\n\t\t}\n\t\t_, err = c.GetPrimary().API.CreateField(context.Background(), child, \"parentid\",\n\t\t\tpilosa.OptFieldForeignIndex(parent),\n\t\t\tpilosa.OptFieldTypeInt(-9223372036854775808, 9223372036854775807),\n\t\t)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating field: %v\", err)\n\t\t}\n\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: child, Query: `\n\t\tSet(1, parentid=\"one\")\n\t\tSet(2, parentid=\"two\")\n\t\tSet(3, parentid=\"three\")\n\t\t`}); err != nil {\n\t\t\tt.Fatalf(\"querying remote: %v\", err)\n\t\t}\n\n\t\tif res, err := c.GetNode(1).API.Query(context.Background(), &pilosa.QueryRequest{\n\t\t\tIndex: child,\n\t\t\tQuery: `Row(parentid=\"two\")Row(parentid==\"one\")`,\n\t\t}); err != nil {\n\t\t\tt.Fatalf(\"Row querying: %v\", err)\n\t\t} else {\n\n\t\t\trow0, row1 := res.Results[0].(*pilosa.Row), res.Results[1].(*pilosa.Row)\n\t\t\tif len(row0.Columns()) != 1 || len(row1.Columns()) != 1 {\n\t\t\t\tt.Fatalf(`Expected: []uint64{1} []uint64{0}, Got: %+v %+v`, row0.Columns(), row1.Columns())\n\t\t\t}\n\t\t\tif row0.Columns()[0] != 2 || row1.Columns()[0] != 1 {\n\t\t\t\tt.Fatalf(`Expected: []uint64{1} []uint64{0}, Got: %+v %+v`, row0.Columns(), row1.Columns())\n\t\t\t}\n\t\t}\n\t})\n}\n\n// Ensure executor returns an error if too many writes are in a single request.\nfunc TestExecutor_Execute_ErrMaxWritesPerRequest(t *testing.T) {\n\tc := test.MustUnsharedCluster(t, 1)\n\tdefer c.Close()\n\tc.GetIdleNode(0).Config.MaxWritesPerRequest = 3\n\terr := c.Start()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\thldr := c.GetHolder(0)\n\thldr.MustCreateIndexIfNotExists(c.Idx(), pilosa.IndexOptions{})\n\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set() Clear() Set() Set()`}); errors.Cause(err) != pilosa.ErrTooManyWrites {\n\t\tt.Fatalf(\"unexpected error: %s\", err)\n\t}\n}\n\nfunc TestExecutor_Time_Clear_Quantums(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\thldr := c.GetHolder(0)\n\n\trangeTests := []struct {\n\t\tquantum  pilosa.TimeQuantum\n\t\texpected []uint64\n\t}{\n\t\t{quantum: \"Y\", expected: []uint64{3, 4, 5, 6}},\n\t\t{quantum: \"M\", expected: []uint64{3, 4, 5, 6}},\n\t\t{quantum: \"D\", expected: []uint64{3, 4, 5, 6}},\n\t\t{quantum: \"H\", expected: []uint64{3, 4, 5, 6, 7}},\n\t\t{quantum: \"YM\", expected: []uint64{3, 4, 5, 6}},\n\t\t{quantum: \"YMD\", expected: []uint64{3, 4, 5, 6}},\n\t\t{quantum: \"YMDH\", expected: []uint64{3, 4, 5, 6, 7}},\n\t\t{quantum: \"MD\", expected: []uint64{3, 4, 5, 6}},\n\t\t{quantum: \"MDH\", expected: []uint64{3, 4, 5, 6, 7}},\n\t\t{quantum: \"DH\", expected: []uint64{3, 4, 5, 6, 7}},\n\t}\n\tpopulateBatch := `\n\t\t\t\t  Set(2, f=1, 1999-12-31T00:00)\n\t\t\t\t  Set(3, f=1, 2000-01-01T00:00)\n\t\t\t\t  Set(4, f=1, 2000-01-02T00:00)\n\t\t\t\t  Set(5, f=1, 2000-02-01T00:00)\n\t\t\t\t  Set(6, f=1, 2001-01-01T00:00)\n\t\t\t\t  Set(7, f=1, 2002-01-01T02:00)\n\t\t\t\t  Set(2, f=1, 1999-12-30T00:00)\n\t\t\t\t  Set(2, f=1, 2002-02-01T00:00)\n\t\t\t\t  Set(2, f=10, 2001-01-01T00:00)\n\t\t\t`\n\tclearColumn := `Clear( 2, f=1)`\n\trangeCheckQuery := `Row(f=1, from=1999-12-31T00:00, to=2002-01-01T03:00)`\n\n\tfor i, tt := range rangeTests {\n\t\tt.Run(fmt.Sprintf(\"#%d Quantum %s\", i+1, tt.quantum), func(t *testing.T) {\n\t\t\t// Create index.\n\t\t\tindexName := c.Idx(strings.ToLower(string(tt.quantum)))\n\t\t\tindex := hldr.MustCreateIndexIfNotExists(indexName, pilosa.IndexOptions{})\n\t\t\t// Create field.\n\t\t\tif _, err := index.CreateFieldIfNotExists(\"f\", \"\", pilosa.OptFieldTypeTime(tt.quantum, \"0\")); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\t// Populate\n\t\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: indexName, Query: populateBatch}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: indexName, Query: clearColumn}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: indexName, Query: rangeCheckQuery}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if columns := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, tt.expected) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestReopenCluster(t *testing.T) {\n\tcommandOpts := make([][]server.CommandOption, 3)\n\tconfigs := make([]*server.Config, 3)\n\tfor i := range configs {\n\t\tconf := server.NewConfig()\n\t\tconfigs[i] = conf\n\t\tconf.Cluster.ReplicaN = 2\n\t\tcommandOpts[i] = append(commandOpts[i], server.OptCommandConfig(conf))\n\t}\n\t// Note: This cluster won't be shared because of the provided options. Which is good because\n\t// we're reopening something, which breaks clusters.\n\tc := test.MustRunCluster(t, 3, commandOpts...)\n\tdefer c.Close()\n\tc.CreateField(t, \"users\", pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"likenums\")\n\tc.ImportIDKey(t, \"users\", \"likenums\", []test.KeyID{\n\t\t{ID: 1, Key: \"userA\"},\n\t\t{ID: 2, Key: \"userB\"},\n\t\t{ID: 3, Key: \"userC\"},\n\t\t{ID: 4, Key: \"userD\"},\n\t\t{ID: 5, Key: \"userE\"},\n\t\t{ID: 6, Key: \"userF\"},\n\t\t{ID: 7, Key: \"userA\"},\n\t\t{ID: 7, Key: \"userB\"},\n\t\t{ID: 7, Key: \"userC\"},\n\t\t{ID: 7, Key: \"userD\"},\n\t\t// we intentionally leave user E out because then there is no\n\t\t// data for userE's shard for this field, which triggered a\n\t\t// \"fragment not found\" problem\n\t\t{ID: 7, Key: \"userF\"},\n\t})\n\n\tnode0 := c.GetNode(0)\n\tif err := node0.Reopen(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif err := c.AwaitState(disco.ClusterStateNormal, 10*time.Second); err != nil {\n\t\tt.Fatalf(\"restarting cluster: %v\", err)\n\t}\n\n\t// TODO this test was supposed to reproduce an issue where spooled messages got sent improperly in Server.Open in this area:\n\t//\n\t//   for i := range toSend {\n\t//     for {\n\t//\t\t err := s.holder.broadcaster.SendSync(toSend[i])\n\t//\n\t// It was previously sending &toSend[i] which wasn't a valid\n\t// pilosa.Message. Unfortunately because that's all happening in a\n\t// goroutine, the Reopen continues happily and things appear to\n\t// work whether the bug is present or not. I'd like to pass in a\n\t// logger and detect when \"completed initial cluster state sync\"\n\t// is logged, but it's more involved than I have time for at the\n\t// moment, so I'm creating a follow up ticket. (CORE-318)\n}\n\n// Ensure an existence field is maintained.\nfunc TestExecutor_Execute_Existence(t *testing.T) {\n\tt.Run(\"Row\", func(t *testing.T) {\n\t\t// Unshared because we're going to reopen it\n\t\tc := test.MustRunUnsharedCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\t\tindex := hldr.MustCreateIndexIfNotExists(c.Idx(), pilosa.IndexOptions{TrackExistence: true})\n\n\t\t_, err := index.CreateField(\"f\", \"\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tnode0 := c.GetNode(0)\n\t\t// Set bits.\n\t\tif _, err := node0.API.Query(context.Background(), &pilosa.QueryRequest{\n\t\t\tIndex: c.Idx(), Query: `` +\n\t\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", 3, 10) +\n\t\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", ShardWidth+1, 10) +\n\t\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", ShardWidth+2, 20),\n\t\t}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif res, err := node0.API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(f=10)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if bits := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{3, ShardWidth + 1}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t}\n\n\t\tif res, err := node0.API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Not(Row(f=10))`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if bits := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{ShardWidth + 2}) {\n\t\t\tt.Fatalf(\"unexpected columns after Not: %+v\", bits)\n\t\t}\n\n\t\t// Reopen cluster to ensure existence field is reloaded.\n\t\tif err := node0.Reopen(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif err := c.AwaitState(disco.ClusterStateNormal, 10*time.Second); err != nil {\n\t\t\tt.Fatalf(\"restarting cluster: %v\", err)\n\t\t}\n\n\t\thldr2 := c.GetHolder(0)\n\t\tindex2 := hldr2.Index(c.Idx())\n\t\t_ = index2\n\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Not(Row(f=10))`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if bits := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{ShardWidth + 2}) {\n\t\t\tt.Fatalf(\"unexpected columns after reopen: %+v\", bits)\n\t\t}\n\t})\n}\n\n// Ensure a not query can be executed.\nfunc TestExecutor_Execute_Not(t *testing.T) {\n}\n\n// Ensure an all query can be executed.\nfunc TestExecutor_Execute_FieldValue(t *testing.T) {\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\n\tnode0 := c.GetNode(0)\n\tnode1 := c.GetNode(1)\n\n\t// Index with IDs\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{Keys: false}, \"f\", pilosa.OptFieldTypeInt(-1100, 1000))\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{Keys: false}, \"dec\", pilosa.OptFieldTypeDecimal(3))\n\n\tif _, err := node0.API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\t\tSet(1, f=3)\n\t\t\tSet(2, f=-4)\n\t\t\tSet(` + strconv.Itoa(ShardWidth+1) + `, f=3)\n\t\t\tSet(1, dec=12.985)\n\t\t\tSet(2, dec=-4.234)\n\t\t`}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Index with Keys\n\tc.CreateField(t, c.Idx(\"ik\"), pilosa.IndexOptions{Keys: true}, \"f\", pilosa.OptFieldTypeInt(-1100, 1000))\n\tc.CreateField(t, c.Idx(\"ik\"), pilosa.IndexOptions{Keys: true}, \"dec\", pilosa.OptFieldTypeDecimal(3))\n\n\tif _, err := node0.API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(\"ik\"), Query: `\n\t\t\tSet(\"one\", f=3)\n\t\t\tSet(\"two\", f=-4)\n\t\t\tSet(\"one\", dec=12.985)\n\t\t\tSet(\"two\", dec=-4.234)\n\t\t`}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttests := []struct {\n\t\tindex  string\n\t\tqry    string\n\t\texpVal interface{}\n\t\texpErr string\n\t}{\n\t\t// IDs\n\t\t{index: c.Idx(), qry: \"FieldValue(field=f, column=1)\", expVal: int64(3)},\n\t\t{index: c.Idx(), qry: \"FieldValue(field=f, column=2)\", expVal: int64(-4)},\n\t\t{index: c.Idx(), qry: \"FieldValue(field=f, column=\" + strconv.Itoa(ShardWidth+1) + \")\", expVal: int64(3)},\n\n\t\t{index: c.Idx(), qry: \"FieldValue(field=dec, column=1)\", expVal: pql.NewDecimal(12985, 3)},\n\t\t{index: c.Idx(), qry: \"FieldValue(field=dec, column=2)\", expVal: pql.NewDecimal(-4234, 3)},\n\n\t\t// Keys\n\t\t{index: c.Idx(\"ik\"), qry: \"FieldValue(field=f, column='one')\", expVal: int64(3)},\n\t\t{index: c.Idx(\"ik\"), qry: \"FieldValue(field=f, column='two')\", expVal: int64(-4)},\n\n\t\t{index: c.Idx(\"ik\"), qry: \"FieldValue(field=dec, column='one')\", expVal: pql.NewDecimal(12985, 3)},\n\t\t{index: c.Idx(\"ik\"), qry: \"FieldValue(field=dec, column='two')\", expVal: pql.NewDecimal(-4234, 3)},\n\n\t\t// Errors\n\t\t{index: c.Idx(), qry: \"FieldValue()\", expErr: pilosa.ErrFieldRequired.Error()},\n\t\t{index: c.Idx(), qry: \"FieldValue(field=dec)\", expErr: pilosa.ErrColumnRequired.Error()},\n\t\t{index: c.Idx(\"ik\"), qry: \"FieldValue(field=f)\", expErr: pilosa.ErrColumnRequired.Error()},\n\t}\n\tfor n, node := range []*test.Command{node0, node1} {\n\t\tfor i, test := range tests {\n\t\t\tif res, err := node.API.Query(context.Background(), &pilosa.QueryRequest{Index: test.index, Query: test.qry}); err != nil && test.expErr == \"\" {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if err != nil && test.expErr != \"\" {\n\t\t\t\tif !strings.Contains(err.Error(), test.expErr) {\n\t\t\t\t\tt.Fatalf(\"test %d on node%d expected error: %s, but got: %s\", i, n, test.expErr, err)\n\t\t\t\t}\n\t\t\t} else if err == nil && test.expErr != \"\" {\n\t\t\t\tt.Fatalf(\"test %d on node%d expected error but got nil\", i, n)\n\t\t\t} else if vc, ok := res.Results[0].(pilosa.ValCount); !ok {\n\t\t\t\tt.Fatalf(\"test %d on node%d expected pilosa.ValCount, but got: %T\", i, n, res.Results[0])\n\t\t\t} else if vc.Count != 1 {\n\t\t\t\tt.Fatalf(\"test %d on node%d expected Count 1, but got: %d\", i, n, vc.Count)\n\t\t\t} else {\n\t\t\t\tswitch exp := test.expVal.(type) {\n\t\t\t\tcase pql.Decimal:\n\t\t\t\t\tif !vc.DecimalVal.EqualTo(exp) {\n\t\t\t\t\t\tt.Fatalf(\"test %d on node%d expected pql.Decimal(%s), but got: %s\", i, n, exp, vc.DecimalVal)\n\t\t\t\t\t}\n\t\t\t\tcase int64:\n\t\t\t\t\tif vc.Val != exp {\n\t\t\t\t\t\tt.Fatalf(\"test %d on node%d expected int64(%d), but got: %d\", i, n, exp, vc.Val)\n\t\t\t\t\t}\n\t\t\t\tdefault:\n\t\t\t\t\tt.Fatalf(\"test %d on node%d received unhandled type: %T\", i, n, test.expVal)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Ensure a Limit query can be executed.\nfunc TestExecutor_Execute_Limit(t *testing.T) {\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{TrackExistence: true}, \"f\")\n\tc.ImportBits(t, c.Idx(), \"f\", [][2]uint64{\n\t\t{1, 0},\n\t\t{1, 1},\n\t\t{1, ShardWidth + 1},\n\t})\n\tcolumns := []uint64{0, 1, ShardWidth + 1}\n\n\t// Test with only a limit specified.\n\tt.Run(\"Limit\", func(t *testing.T) {\n\t\tfor limit := 0; limit < 5; limit++ {\n\t\t\texpect := columns\n\t\t\tif limit < len(expect) {\n\t\t\t\texpect = expect[:limit]\n\t\t\t}\n\n\t\t\tresp := c.Query(t, c.Idx(), fmt.Sprintf(\"Limit(All(), limit=%d)\", limit))\n\t\t\tif len(resp.Results) != 1 {\n\t\t\t\tt.Fatalf(\"limit=%d: expected 1 result but got %v\", limit, resp.Results)\n\t\t\t}\n\t\t\trow, ok := resp.Results[0].(*pilosa.Row)\n\t\t\tif !ok {\n\t\t\t\tt.Fatalf(\"limit=%d: expected a row result but got %T\", limit, resp.Results[0])\n\t\t\t}\n\t\t\tgot := row.Columns()\n\t\t\tif !reflect.DeepEqual(expect, got) {\n\t\t\t\tt.Errorf(\"limit=%d: expected %v but got %v\", limit, expect, got)\n\t\t\t}\n\t\t}\n\t})\n\n\t// Test with only an offset specified.\n\tt.Run(\"Offset\", func(t *testing.T) {\n\t\tfor offset := 0; offset < 5; offset++ {\n\t\t\texpect := []uint64{}\n\t\t\tif offset <= len(columns) {\n\t\t\t\texpect = columns[offset:]\n\t\t\t}\n\n\t\t\tresp := c.Query(t, c.Idx(), fmt.Sprintf(\"Limit(All(), offset=%d)\", offset))\n\t\t\tif len(resp.Results) != 1 {\n\t\t\t\tt.Fatalf(\"offset=%d: expected 1 result but got %v\", offset, resp.Results)\n\t\t\t}\n\t\t\trow, ok := resp.Results[0].(*pilosa.Row)\n\t\t\tif !ok {\n\t\t\t\tt.Fatalf(\"offset=%d: expected a row result but got %T\", offset, resp.Results[0])\n\t\t\t}\n\t\t\tgot := row.Columns()\n\t\t\tif !reflect.DeepEqual(expect, got) {\n\t\t\t\tt.Errorf(\"offset=%d: expected %v but got %v\", offset, expect, got)\n\t\t\t}\n\t\t}\n\t})\n\n\t// Test with a limit and offset specified.\n\tt.Run(\"LimitOffset\", func(t *testing.T) {\n\t\tfor limit := 0; limit < 5; limit++ {\n\t\t\tfor offset := 0; offset < 5; offset++ {\n\t\t\t\texpect := []uint64{}\n\t\t\t\tif offset <= len(columns) {\n\t\t\t\t\texpect = columns[offset:]\n\t\t\t\t}\n\t\t\t\tif limit < len(expect) {\n\t\t\t\t\texpect = expect[:limit]\n\t\t\t\t}\n\n\t\t\t\tresp := c.Query(t, c.Idx(), fmt.Sprintf(\"Limit(All(), limit=%d, offset=%d)\", limit, offset))\n\t\t\t\tif len(resp.Results) != 1 {\n\t\t\t\t\tt.Fatalf(\"limit=%d,offset=%d: expected 1 result but got %v\", limit, offset, resp.Results)\n\t\t\t\t}\n\t\t\t\trow, ok := resp.Results[0].(*pilosa.Row)\n\t\t\t\tif !ok {\n\t\t\t\t\tt.Fatalf(\"limit=%d,offset=%d: expected a row result but got %T\", limit, offset, resp.Results[0])\n\t\t\t\t}\n\t\t\t\tgot := row.Columns()\n\t\t\t\tif !reflect.DeepEqual(expect, got) {\n\t\t\t\t\tt.Errorf(\"limit=%d,offset=%d: expected %v but got %v\", limit, offset, expect, got)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t})\n\n\tt.Run(\"Nested\", func(t *testing.T) {\n\t\tfor limit := 0; limit < 5; limit++ {\n\t\t\tfor offset := 0; offset < 5; offset++ {\n\t\t\t\texpect := []uint64{}\n\t\t\t\tif offset <= len(columns) {\n\t\t\t\t\texpect = columns[offset:]\n\t\t\t\t}\n\t\t\t\tif limit < len(expect) {\n\t\t\t\t\texpect = expect[:limit]\n\t\t\t\t}\n\n\t\t\t\tresp := c.Query(t, c.Idx(), fmt.Sprintf(\"Limit(Limit(All(), offset=%d), limit=%d)\", offset, limit))\n\t\t\t\tif len(resp.Results) != 1 {\n\t\t\t\t\tt.Fatalf(\"limit=%d,offset=%d: expected 1 result but got %v\", limit, offset, resp.Results)\n\t\t\t\t}\n\t\t\t\trow, ok := resp.Results[0].(*pilosa.Row)\n\t\t\t\tif !ok {\n\t\t\t\t\tt.Fatalf(\"limit=%d,offset=%d: expected a row result but got %T\", limit, offset, resp.Results[0])\n\t\t\t\t}\n\t\t\t\tgot := row.Columns()\n\t\t\t\tif !reflect.DeepEqual(expect, got) {\n\t\t\t\t\tt.Errorf(\"limit=%d,offset=%d: expected %v but got %v\", limit, offset, expect, got)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t})\n\n\tt.Run(\"Extract\", func(t *testing.T) {\n\t\tresp := c.Query(t, c.Idx(), \"Extract(Limit(All(), limit=1))\")\n\t\tif len(resp.Results) != 1 {\n\t\t\tt.Fatalf(\"expected 1 result but got %d\", len(resp.Results))\n\t\t}\n\t\tgot, ok := resp.Results[0].(pilosa.ExtractedTable)\n\t\tif !ok {\n\t\t\tt.Fatalf(\"expected a table result but got %T\", resp.Results[0])\n\t\t}\n\t\texpect := pilosa.ExtractedTable{\n\t\t\tFields: []pilosa.ExtractedTableField{},\n\t\t\tColumns: []pilosa.ExtractedTableColumn{\n\t\t\t\t{\n\t\t\t\t\tColumn: pilosa.KeyOrID{\n\t\t\t\t\t\tID: 0,\n\t\t\t\t\t},\n\t\t\t\t\tRows: []interface{}{},\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t\tif !reflect.DeepEqual(expect, got) {\n\t\t\tt.Errorf(\"expected %v but got %v\", expect, got)\n\t\t}\n\t})\n}\n\nfunc TestExecutor_Sort(t *testing.T) {\n\tt.Run(\"Sort\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\n\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{TrackExistence: true}, \"bsint\", pilosa.OptFieldTypeInt(math.MinInt64, math.MaxInt64))\n\t\tc.Query(t, c.Idx(), `\n\t\t\tSet(0, bsint = 1)\n\t\t\tSet(1, bsint = -1)\n\t\t\tSet(2, bsint = 2)\n\t\t\tSet(3, bsint = -2)\n\t\t\tSet(4, bsint = 3)\n\t\t\tSet(5, bsint = 4)\n\t\t\t`)\n\n\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{TrackExistence: true}, \"bool\", pilosa.OptFieldTypeBool())\n\t\tc.Query(t, c.Idx(), `\n\t\t\t\tSet(0, bool=true)\n\t\t\t\tSet(1, bool=false)\n\t\t\t\tSet(2, bool=false)\n\t\t\t\tSet(3, bool=true)\n\t\t\t\tSet(4, bool=false)\n\t\t\t\tSet(5, bool=true)\n\t\t\t`)\n\n\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{TrackExistence: true}, \"keymutex\", pilosa.OptFieldKeys(), pilosa.OptFieldTypeMutex(pilosa.CacheTypeRanked, 5000))\n\t\tc.Query(t, c.Idx(), `\n\t\t\t\tSet(0, keymutex=\"h\")\n\t\t\t\tSet(1, keymutex=\"xyzzy\")\n\t\t\t\tSet(2, keymutex=\"ra\")\n\t\t\t\tSet(3, keymutex=\"plugh\")\n\t\t\t\tSet(4, keymutex=\"wl\")\n\t\t\t\tSet(5, keymutex=\"ig\")\n\t\t\t`)\n\n\t\tqueries := []string{\n\t\t\t\"Extract(Sort(Row(bsint > 1), field = bsint, limit = 2, offset = 1), Rows(bsint))\",\n\t\t\t\"Extract(Sort(Row(bsint < -1), field = bool, limit = 1, sort-desc = true), Rows(bool))\",\n\t\t\t\"Extract(Sort(All(), field = keymutex, limit = 1), Rows(keymutex))\",\n\t\t}\n\n\t\texpect := []pilosa.ExtractedTable{\n\t\t\t{\n\t\t\t\tFields: []pilosa.ExtractedTableField{\n\t\t\t\t\t{\n\t\t\t\t\t\tName: \"bsint\",\n\t\t\t\t\t\tType: \"int64\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tColumns: []pilosa.ExtractedTableColumn{\n\t\t\t\t\t{\n\t\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 4},\n\t\t\t\t\t\tRows: []interface{}{\n\t\t\t\t\t\t\tint64(3),\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\t{\n\t\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 5},\n\t\t\t\t\t\tRows: []interface{}{\n\t\t\t\t\t\t\tint64(4),\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\tFields: []pilosa.ExtractedTableField{\n\t\t\t\t\t{\n\t\t\t\t\t\tName: \"bool\",\n\t\t\t\t\t\tType: \"bool\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tColumns: []pilosa.ExtractedTableColumn{\n\t\t\t\t\t{\n\t\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 3},\n\t\t\t\t\t\tRows: []interface{}{\n\t\t\t\t\t\t\ttrue,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\tFields: []pilosa.ExtractedTableField{\n\t\t\t\t\t{\n\t\t\t\t\t\tName: \"keymutex\",\n\t\t\t\t\t\tType: \"string\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tColumns: []pilosa.ExtractedTableColumn{\n\t\t\t\t\t{\n\t\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 0},\n\t\t\t\t\t\tRows: []interface{}{\n\t\t\t\t\t\t\t\"h\",\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\n\t\tfor i, q := range queries {\n\t\t\tresp := c.Query(t, c.Idx(), q)\n\t\t\tif !reflect.DeepEqual(expect[i], resp.Results[0]) {\n\t\t\t\tt.Errorf(\"expected %v but got %v\", expect[i], resp.Results[0])\n\t\t\t}\n\t\t}\n\t})\n}\n\n// Ensure an all query can be executed.\nfunc TestExecutor_Execute_All(t *testing.T) {\n\tt.Run(\"ColumnID\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\t\tindex := hldr.MustCreateIndexIfNotExists(c.Idx(), pilosa.IndexOptions{TrackExistence: true})\n\t\tfld, err := index.CreateField(\"f\", \"\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Create an import request that sets things on either end\n\t\t// of a shard, plus a couple bits set on either side of it,\n\t\t// and a final bit set in a fourth shard.\n\t\t//\n\t\t//    shard0     shard1     shard2     shard3\n\t\t// |----------|----------|----------|----------|\n\t\t// |        **|**      **|**        | *\n\t\t//\n\t\tbitCount := 100 + 5\n\t\treq := &pilosa.ImportRequest{\n\t\t\tIndex:     index.Name(),\n\t\t\tField:     fld.Name(),\n\t\t\tShard:     0,\n\t\t\tRowIDs:    make([]uint64, bitCount),\n\t\t\tColumnIDs: make([]uint64, bitCount),\n\t\t}\n\t\tfor i := 0; i < bitCount/2; i++ {\n\t\t\treq.RowIDs[i] = 10\n\t\t\treq.ColumnIDs[i] = uint64(i + ShardWidth - 2)\n\t\t}\n\t\tfor i := bitCount / 2; i < bitCount-1; i++ {\n\t\t\treq.RowIDs[i] = 10\n\t\t\treq.ColumnIDs[i] = uint64(i + (ShardWidth * 2) - bitCount + 5)\n\t\t}\n\t\treq.RowIDs[bitCount-1] = 10\n\t\treq.ColumnIDs[bitCount-1] = uint64((3 * ShardWidth) + 2)\n\n\t\tm0 := c.GetNode(0)\n\t\t// the request gets altered by the Import operation now...\n\t\treqs := req.Clone().SortToShards()\n\t\tqcx := m0.API.Txf().NewQcx()\n\t\tfor _, r := range reqs {\n\t\t\t// we can ignore the key (which is the shard) because each req\n\t\t\t// also got its internal key set.\n\t\t\tif err := m0.API.Import(context.Background(), qcx, r); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t}\n\t\tPanicOn(qcx.Finish())\n\n\t\ti0, err := m0.API.Index(context.Background(), c.Idx())\n\t\tPanicOn(err)\n\t\tif i0 == nil {\n\t\t\tPanicOn(\"nil index i0?\")\n\t\t}\n\n\t\ttests := []struct {\n\t\t\tqry     string\n\t\t\texpCols []uint64\n\t\t\texpCnt  uint64\n\t\t}{\n\t\t\t{qry: \"All()\", expCols: req.ColumnIDs, expCnt: uint64(bitCount)},\n\t\t\t{qry: \"All(limit=1)\", expCols: req.ColumnIDs[:1], expCnt: 1},\n\t\t\t{qry: \"All(limit=4)\", expCols: req.ColumnIDs[:4], expCnt: 4},\n\t\t\t{qry: \"All(limit=4, offset=4)\", expCols: req.ColumnIDs[4:8], expCnt: 4},\n\t\t\t{qry: fmt.Sprintf(\"All(limit=4, offset=%d)\", bitCount-5), expCols: req.ColumnIDs[bitCount-5 : bitCount-1], expCnt: 4},\n\t\t\t{qry: fmt.Sprintf(\"All(limit=1, offset=%d)\", bitCount-2), expCols: req.ColumnIDs[bitCount-2 : bitCount-1], expCnt: 1},\n\t\t\t{qry: fmt.Sprintf(\"All(limit=1, offset=%d)\", bitCount-2), expCols: req.ColumnIDs[bitCount-2 : bitCount-1], expCnt: 1},\n\t\t\t{qry: fmt.Sprintf(\"All(limit=4, offset=%d)\", bitCount-2), expCols: req.ColumnIDs[bitCount-2:], expCnt: 2},\n\t\t\t{qry: fmt.Sprintf(\"All(limit=4, offset=%d)\", bitCount+1), expCols: []uint64{}, expCnt: 0},\n\t\t\t{qry: fmt.Sprintf(\"All(limit=2, offset=%d)\", bitCount-3), expCols: req.ColumnIDs[bitCount-3 : bitCount-1], expCnt: 2},\n\t\t\t{qry: fmt.Sprintf(\"All(limit=2, offset=%d)\", bitCount-5), expCols: req.ColumnIDs[bitCount-5 : bitCount-3], expCnt: 2},\n\t\t\t{qry: \"All(limit=2, offset=2)\", expCols: req.ColumnIDs[2:4], expCnt: 2},\n\t\t\t{qry: \"All(limit=1, offset=1)\", expCols: req.ColumnIDs[1:2], expCnt: 1},\n\t\t\t{qry: fmt.Sprintf(\"All(limit=%d, offset=2)\", bitCount-3), expCols: req.ColumnIDs[2 : bitCount-1], expCnt: uint64(bitCount - 3)},\n\t\t}\n\t\tfor i, test := range tests {\n\t\t\tif res, err := m0.API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: test.qry}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if cnt := res.Results[0].(*pilosa.Row).Count(); cnt != test.expCnt {\n\t\t\t\tt.Fatalf(\"test %d, unexpected count, got: %d, but expected: %d\", i, cnt, test.expCnt)\n\t\t\t} else if cols := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(cols, test.expCols) {\n\t\t\t\t// If the error results are too large, just show the count.\n\t\t\t\tif len(cols) > 1000 || len(test.expCols) > 1000 {\n\t\t\t\t\tt.Fatalf(\"test %d, unexpected columns, got: len(%d), but expected: len(%d)\", i, len(cols), len(test.expCols))\n\t\t\t\t} else {\n\t\t\t\t\tt.Fatalf(\"test %d, unexpected columns, got: %v, but expected: %v\", i, cols, test.expCols)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t})\n\n\tt.Run(\"ColumnKey\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\t\tindex := hldr.MustCreateIndexIfNotExists(c.Idx(), pilosa.IndexOptions{TrackExistence: true, Keys: true})\n\t\tfld, err := index.CreateField(\"f\", \"\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Create an import request that sets key columns\n\t\t//\n\t\t//    shard0\n\t\t// |----------|\n\t\t// |****      |\n\t\t//\n\t\tbitCount := 4\n\t\treq := &pilosa.ImportRequest{\n\t\t\tIndex:      index.Name(),\n\t\t\tField:      fld.Name(),\n\t\t\tShard:      0,\n\t\t\tRowIDs:     make([]uint64, bitCount),\n\t\t\tColumnKeys: make([]string, bitCount),\n\t\t}\n\t\tfor i := 0; i < bitCount; i++ {\n\t\t\treq.RowIDs[i] = 10\n\t\t\treq.ColumnKeys[i] = fmt.Sprintf(\"c%d\", i)\n\t\t}\n\n\t\tqcx := c.GetNode(0).API.Txf().NewQcx()\n\t\tif err := c.GetNode(0).API.Import(context.Background(), qcx, req); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tPanicOn(qcx.Finish())\n\n\t\ttests := []struct {\n\t\t\tqry     string\n\t\t\texpCols []string\n\t\t\texpCnt  uint64\n\t\t}{\n\t\t\t{qry: \"All()\", expCols: []string{\"c1\", \"c0\", \"c3\", \"c2\"}, expCnt: uint64(bitCount)},\n\t\t\t{qry: \"All(limit=1)\", expCols: []string{\"c1\"}, expCnt: 1},\n\t\t\t{qry: \"All(limit=4)\", expCols: []string{\"c1\", \"c0\", \"c3\", \"c2\"}, expCnt: 4},\n\t\t\t{qry: \"All(limit=5)\", expCols: []string{\"c1\", \"c0\", \"c3\", \"c2\"}, expCnt: 4},\n\t\t\t{qry: \"All(limit=1, offset=1)\", expCols: []string{\"c0\"}, expCnt: 1},\n\t\t\t{qry: \"All(limit=4, offset=1)\", expCols: []string{\"c0\", \"c3\", \"c2\"}, expCnt: 3},\n\t\t\t{qry: \"All(limit=4, offset=5)\", expCols: nil, expCnt: 0},\n\t\t}\n\t\tfor i, test := range tests {\n\t\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: test.qry}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if cnt := len(res.Results[0].(*pilosa.Row).Keys); uint64(cnt) != test.expCnt {\n\t\t\t\tt.Fatalf(\"test %d, unexpected count, got: %d, but expected: %d\", i, cnt, test.expCnt)\n\t\t\t} else if cols := res.Results[0].(*pilosa.Row).Keys; !reflect.DeepEqual(cols, test.expCols) {\n\t\t\t\t// If the error results are too large, just show the count.\n\t\t\t\tif len(cols) > 1000 || len(test.expCols) > 1000 {\n\t\t\t\t\tt.Fatalf(\"test %d, unexpected columns, got: len(%d), but expected: len(%d)\", i, len(cols), len(test.expCols))\n\t\t\t\t} else {\n\t\t\t\t\tt.Fatalf(\"test %d, unexpected columns, got: %#v, but expected: %#v\", i, cols, test.expCols)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t})\n\n\t// Ensure that a query which uses All() at the shard level can call it.\n\tt.Run(\"AllShard\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\t\tindex := hldr.MustCreateIndexIfNotExists(c.Idx(), pilosa.IndexOptions{TrackExistence: true})\n\t\t_, err := index.CreateField(\"f\", \"\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\nSet(3001, f=3)\nSet(5001, f=5)\nSet(5002, f=5)\n`}); err != nil {\n\t\t\tt.Fatalf(\"querying remote: %v\", err)\n\t\t}\n\n\t\texpCols := []uint64{5001, 5002}\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: \"Intersect(All(), Row(f=5))\"}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if cols := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(cols, expCols) {\n\t\t\tt.Fatalf(\"unexpected columns, got: %v, but expected: %v\", cols, expCols)\n\t\t}\n\t})\n}\n\n// Ensure a row can be cleared.\nfunc TestExecutor_Execute_ClearRow(t *testing.T) {\n\tt.Run(\"Int\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\t\tindex := hldr.MustCreateIndexIfNotExists(c.Idx(), pilosa.IndexOptions{TrackExistence: true})\n\t\t_, err := index.CreateField(\"f\", \"\", pilosa.OptFieldTypeInt(math.MinInt64, math.MaxInt64))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Ensure that clearing a row raises an error.\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `ClearRow(f=1)`}); err == nil {\n\t\t\tt.Fatal(\"expected clear row to return an error\")\n\t\t}\n\t})\n\n\tt.Run(\"TopN\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\t\tindex := hldr.MustCreateIndexIfNotExists(c.Idx(), pilosa.IndexOptions{TrackExistence: true})\n\t\t_, err := index.CreateField(\"f\", \"\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tcc := `\n\t\t\tSet(2, f=1)\n\t\t\tSet(3, f=1)\n\t\t\tSet(4, f=1)\n\t\t\tSet(5, f=1)\n\t\t\tSet(6, f=1)\n\t\t\tSet(7, f=1)\n\t\t\tSet(8, f=1)\n\n\t\t\tSet(2, f=2)\n\t\t\tSet(3, f=2)\n\t\t\tSet(4, f=2)\n\t\t\tSet(5, f=2)\n\t\t\tSet(6, f=2)\n\t\t\tSet(7, f=2)\n\n\t\t\tSet(2, f=3)\n\t\t\tSet(3, f=3)\n\t\t\tSet(4, f=3)\n\t\t\tSet(5, f=3)\n\t\t\tSet(6, f=3)\n\t\t`\n\n\t\t// Set bits.\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: cc}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif err := c.GetNode(0).RecalculateCaches(t); err != nil {\n\t\t\tt.Fatalf(\"recalculating caches: %v\", err)\n\t\t}\n\n\t\t// Check the TopN results.\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `TopN(f, n=5)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(res.Results, []interface{}{&pilosa.PairsField{\n\t\t\tPairs: []pilosa.Pair{\n\t\t\t\t{ID: 1, Count: 7},\n\t\t\t\t{ID: 2, Count: 6},\n\t\t\t\t{ID: 3, Count: 5},\n\t\t\t},\n\t\t\tField: \"f\",\n\t\t}}) {\n\t\t\tt.Fatalf(\"topn wrong results: %v\", res.Results)\n\t\t}\n\n\t\t// Clear the row and ensure we get a `true` response.\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `ClearRow(f=2)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if res := res.Results[0].(bool); !res {\n\t\t\tt.Fatalf(\"unexpected clear row result: %+v\", res)\n\t\t}\n\n\t\t// Ensure that the cleared row doesn't show up in TopN (i.e. it was removed from the cache).\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `TopN(f, n=5)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(res.Results, []interface{}{&pilosa.PairsField{\n\t\t\tPairs: []pilosa.Pair{\n\t\t\t\t{ID: 1, Count: 7},\n\t\t\t\t{ID: 3, Count: 5},\n\t\t\t},\n\t\t\tField: \"f\",\n\t\t}}) {\n\t\t\tt.Fatalf(\"topn wrong results: %v\", res.Results)\n\t\t}\n\t})\n}\n\n// Ensure a row can be set.\nfunc TestExecutor_Execute_SetRow(t *testing.T) {\n\tt.Run(\"Set_NewRow\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\t\tindex := hldr.MustCreateIndexIfNotExists(c.Idx(), pilosa.IndexOptions{TrackExistence: true})\n\t\tif _, err := index.CreateField(\"f\", \"\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif _, err := index.CreateField(\"tmp\", \"\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Set bits.\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{\n\t\t\tIndex: c.Idx(), Query: `` +\n\t\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", 3, 10) +\n\t\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", ShardWidth-1, 10) +\n\t\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", ShardWidth+1, 10),\n\t\t}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(f=10)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if bits := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{3, ShardWidth - 1, ShardWidth + 1}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t}\n\n\t\t// Store row 10 into a different row.\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Store(Row(f=10), tmp=20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if res := res.Results[0].(bool); !res {\n\t\t\tt.Fatalf(\"unexpected set row result: %+v\", res)\n\t\t}\n\n\t\t// Ensure the row was populated.\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(tmp=20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if bits := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{3, ShardWidth - 1, ShardWidth + 1}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t}\n\n\t\t// Store row 10 into a table which doesn't exist.\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Store(Row(f=10), nonexistent=20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if res := res.Results[0].(bool); !res {\n\t\t\tt.Fatalf(\"unexpected set row result: %+v\", res)\n\t\t}\n\n\t\t// Ensure the row was populated.\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(nonexistent=20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if bits := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{3, ShardWidth - 1, ShardWidth + 1}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t}\n\t})\n\tt.Run(\"Set_NoSource\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\t\tidx := hldr.MustCreateIndexIfNotExists(c.Idx(), pilosa.IndexOptions{TrackExistence: true})\n\t\t_, err := idx.CreateField(\"f\", \"\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Set bits.\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{\n\t\t\tIndex: c.Idx(), Query: `` +\n\t\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", 3, 10) +\n\t\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", ShardWidth-1, 10) +\n\t\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", ShardWidth+1, 10),\n\t\t}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(f=10)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if bits := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{3, ShardWidth - 1, ShardWidth + 1}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t}\n\n\t\t// Store row 9 (which doesn't exist) into a different row.\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Store(Row(f=9), f=20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if res := res.Results[0].(bool); !res {\n\t\t\tt.Fatalf(\"unexpected set row result: %+v\", res)\n\t\t}\n\n\t\t// Ensure the row was populated.\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(f=20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if bits := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t}\n\n\t\t// Store row 9 (which doesn't exist) into a row that does exist.\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Store(Row(f=9), f=10)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if res := res.Results[0].(bool); !res {\n\t\t\tt.Fatalf(\"unexpected set row result: %+v\", res)\n\t\t}\n\n\t\t// Ensure the row was populated.\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(f=10)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if bits := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t}\n\t})\n\tt.Run(\"Set_ExistingDestination\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\t\tindex := hldr.MustCreateIndexIfNotExists(c.Idx(), pilosa.IndexOptions{TrackExistence: true})\n\t\t_, err := index.CreateField(\"f\", \"\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Set bits.\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{\n\t\t\tIndex: c.Idx(), Query: `` +\n\t\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", 3, 10) +\n\t\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", ShardWidth-1, 10) +\n\t\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", ShardWidth+1, 10) +\n\t\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", 1, 20) +\n\t\t\t\tfmt.Sprintf(\"Set(%d, f=%d)\\n\", ShardWidth+1, 20),\n\t\t}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(f=20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if bits := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{1, ShardWidth + 1}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t}\n\n\t\t// Store row 10 into an existing row.\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Store(Row(f=10), f=20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if res := res.Results[0].(bool); !res {\n\t\t\tt.Fatalf(\"unexpected set row result: %+v\", res)\n\t\t}\n\n\t\t// Ensure the row was populated.\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(f=20)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if bits := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{3, ShardWidth - 1, ShardWidth + 1}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t}\n\t})\n\tt.Run(\"Set_Keyed\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\t\tindex := hldr.MustCreateIndexIfNotExists(c.Idx(), pilosa.IndexOptions{TrackExistence: true})\n\t\tif _, err := index.CreateField(\"f\", \"\", pilosa.OptFieldKeys()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Set bits.\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(1, f=\"a\")`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(f=\"a\")`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if bits := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{1}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t}\n\n\t\t// Store row a into a different row.\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Store(Row(f=\"a\"), f=\"b\")`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if res := res.Results[0].(bool); !res {\n\t\t\tt.Fatalf(\"unexpected set row result: %+v\", res)\n\t\t}\n\n\t\t// Ensure the row was populated.\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(f=\"b\")`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if bits := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{1}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t}\n\n\t\t// Store row 10 into a table which doesn't exist.\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Store(Row(f=\"a\"), nonexistent=\"c\")`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if res := res.Results[0].(bool); !res {\n\t\t\tt.Fatalf(\"unexpected set row result: %+v\", res)\n\t\t}\n\n\t\t// Ensure the row was populated.\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Row(nonexistent=\"c\")`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if bits := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(bits, []uint64{1}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", bits)\n\t\t}\n\t})\n}\n\nfunc benchmarkExistence(nn bool, b *testing.B) {\n\tc := test.MustUnsharedCluster(b, 1)\n\tvar err error\n\tc.GetIdleNode(0).Config.DataDir, err = testhook.TempDir(b, \"benchmarkExistence\")\n\tif err != nil {\n\t\tb.Fatalf(\"getting temp dir: %v\", err)\n\t}\n\terr = c.Start()\n\tif err != nil {\n\t\tb.Fatalf(\"starting cluster: %v\", err)\n\t}\n\tdefer c.Close()\n\thldr := c.GetHolder(0)\n\n\tindexName := c.Idx()\n\tfieldName := \"f\"\n\n\tindex := hldr.MustCreateIndexIfNotExists(indexName, pilosa.IndexOptions{TrackExistence: nn})\n\t// Create field.\n\tif _, err := index.CreateFieldIfNotExists(fieldName, \"\"); err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\tbitCount := 10000\n\treq := &pilosa.ImportRequest{\n\t\tIndex:     indexName,\n\t\tField:     fieldName,\n\t\tShard:     0,\n\t\tRowIDs:    make([]uint64, bitCount),\n\t\tColumnIDs: make([]uint64, bitCount),\n\t}\n\tfor i := 0; i < bitCount; i++ {\n\t\treq.RowIDs[i] = uint64(rand.Intn(100000))\n\t\treq.ColumnIDs[i] = uint64(rand.Intn(1 << 20))\n\t}\n\n\tb.ResetTimer()\n\tnodeAPI := c.GetNode(0).API\n\tfor i := 0; i < b.N; i++ {\n\t\tqcx := nodeAPI.Txf().NewQcx()\n\t\tif err := nodeAPI.Import(context.Background(), qcx, req); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t\tPanicOn(qcx.Finish())\n\t}\n}\n\nfunc BenchmarkExecutor_Existence_True(b *testing.B)  { benchmarkExistence(true, b) }\nfunc BenchmarkExecutor_Existence_False(b *testing.B) { benchmarkExistence(false, b) }\n\nfunc TestExecutor_Execute_Extract(t *testing.T) {\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{TrackExistence: true}, \"set\")\n\tc.ImportBits(t, c.Idx(), \"set\", [][2]uint64{\n\t\t{0, 1},\n\t\t{0, 2},\n\t\t{3, 1},\n\t\t{4, 1},\n\t\t{4, 4 * ShardWidth},\n\t\t{5, ShardWidth},\n\t})\n\tc.Query(t, c.Idx(), fmt.Sprintf(\"Clear(%d, set=5)\", ShardWidth))\n\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{TrackExistence: true}, \"keyset\", pilosa.OptFieldKeys())\n\tc.Query(t, c.Idx(), `\n\t\tSet(0, keyset=\"h\")\n\t\tSet(1, keyset=\"xyzzy\")\n\t\tSet(0, keyset=\"plugh\")\n\t`)\n\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{TrackExistence: true}, \"mutex\", pilosa.OptFieldTypeMutex(pilosa.CacheTypeRanked, 5000))\n\tc.ImportBits(t, c.Idx(), \"mutex\", [][2]uint64{\n\t\t{0, 1},\n\t\t{0, 2},\n\t\t{4, 4 * ShardWidth},\n\t})\n\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{TrackExistence: true}, \"keymutex\", pilosa.OptFieldKeys(), pilosa.OptFieldTypeMutex(pilosa.CacheTypeRanked, 5000))\n\tc.Query(t, c.Idx(), `\n\t\tSet(0, keymutex=\"h\")\n\t\tSet(1, keymutex=\"xyzzy\")\n\t\tSet(3, keymutex=\"plugh\")\n\t`)\n\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{TrackExistence: true}, \"time\", pilosa.OptFieldTypeTime(\"YMDH\", \"0\"))\n\tc.Query(t, c.Idx(), `\n\t\tSet(0, time=1, 2016-01-01T00:00)\n\t\tSet(1, time=2, 2017-01-01T00:00)\n\t\tSet(3, time=3, 2018-01-01T00:00)\n\t`)\n\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{TrackExistence: true}, \"keytime\", pilosa.OptFieldKeys(), pilosa.OptFieldTypeTime(\"YMDH\", \"0\"))\n\tc.Query(t, c.Idx(), `\n\t\tSet(0, keytime=\"h\", 2016-01-01T00:00)\n\t\tSet(1, keytime=\"xyzzy\", 2017-01-01T00:00)\n\t\tSet(0, keytime=\"plugh\", 2018-01-01T00:00)\n\t`)\n\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{TrackExistence: true}, \"bsint\", pilosa.OptFieldTypeInt(-100, 100))\n\tc.Query(t, c.Idx(), `\n\t\tSet(0, bsint=1)\n\t\tSet(1, bsint=-1)\n\t\tSet(3, bsint=2)\n\t`)\n\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{TrackExistence: true}, \"bsidecimal\", pilosa.OptFieldTypeDecimal(2))\n\tc.Query(t, c.Idx(), `\n\t\tSet(0, bsidecimal=0.01)\n\t\tSet(1, bsidecimal=1.00)\n\t\tSet(3, bsidecimal=-1.01)\n\t`)\n\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{TrackExistence: true}, \"timestamp\", pilosa.OptFieldTypeTimestamp(pilosa.DefaultEpoch, pilosa.TimeUnitSeconds))\n\tc.Query(t, c.Idx(), `\n\t\tSet(0, timestamp='2000-01-01T00:00:00Z')\n\t\tSet(1, timestamp='2000-01-01T00:00:01Z')\n\t\tSet(3, timestamp='2000-01-01T00:00:03Z')\n\t`)\n\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{TrackExistence: true}, \"bool\", pilosa.OptFieldTypeBool())\n\tc.Query(t, c.Idx(), `\n\t\tSet(0, bool=true)\n\t\tSet(1, bool=false)\n\t\tSet(3, bool=true)\n\t`)\n\n\tresp := c.Query(t, c.Idx(), `Extract(All(), Rows(set), Rows(keyset), Rows(mutex), Rows(keymutex), Rows(time), Rows(keytime), Rows(bsint), Rows(bsidecimal), Rows(timestamp), Rows(bool))`)\n\texpect := []interface{}{\n\t\tpilosa.ExtractedTable{\n\t\t\tFields: []pilosa.ExtractedTableField{\n\t\t\t\t{\n\t\t\t\t\tName: \"set\",\n\t\t\t\t\tType: \"[]uint64\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tName: \"keyset\",\n\t\t\t\t\tType: \"[]string\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tName: \"mutex\",\n\t\t\t\t\tType: \"uint64\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tName: \"keymutex\",\n\t\t\t\t\tType: \"string\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tName: \"time\",\n\t\t\t\t\tType: \"[]uint64\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tName: \"keytime\",\n\t\t\t\t\tType: \"[]string\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tName: \"bsint\",\n\t\t\t\t\tType: \"int64\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tName: \"bsidecimal\",\n\t\t\t\t\tType: \"decimal\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tName: \"timestamp\",\n\t\t\t\t\tType: \"timestamp\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tName: \"bool\",\n\t\t\t\t\tType: \"bool\",\n\t\t\t\t},\n\t\t\t},\n\t\t\tColumns: []pilosa.ExtractedTableColumn{\n\t\t\t\t{\n\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 0},\n\t\t\t\t\tRows: []interface{}{\n\t\t\t\t\t\t[]uint64(nil),\n\t\t\t\t\t\t[]string{\n\t\t\t\t\t\t\t\"h\",\n\t\t\t\t\t\t\t\"plugh\",\n\t\t\t\t\t\t},\n\t\t\t\t\t\tnil,\n\t\t\t\t\t\t\"h\",\n\t\t\t\t\t\t[]uint64{\n\t\t\t\t\t\t\t1,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t[]string{\n\t\t\t\t\t\t\t\"h\",\n\t\t\t\t\t\t\t\"plugh\",\n\t\t\t\t\t\t},\n\t\t\t\t\t\tint64(1),\n\t\t\t\t\t\tpql.NewDecimal(1, 2),\n\t\t\t\t\t\ttime.Date(2000, time.January, 1, 0, 0, 0, 0, time.UTC),\n\t\t\t\t\t\ttrue,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 1},\n\t\t\t\t\tRows: []interface{}{\n\t\t\t\t\t\t[]uint64{\n\t\t\t\t\t\t\t0,\n\t\t\t\t\t\t\t3,\n\t\t\t\t\t\t\t4,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t[]string{\n\t\t\t\t\t\t\t\"xyzzy\",\n\t\t\t\t\t\t},\n\t\t\t\t\t\tuint64(0),\n\t\t\t\t\t\t\"xyzzy\",\n\t\t\t\t\t\t[]uint64{\n\t\t\t\t\t\t\t2,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t[]string{\n\t\t\t\t\t\t\t\"xyzzy\",\n\t\t\t\t\t\t},\n\t\t\t\t\t\tint64(-1),\n\t\t\t\t\t\tpql.NewDecimal(100, 2),\n\t\t\t\t\t\ttime.Date(2000, time.January, 1, 0, 0, 1, 0, time.UTC),\n\t\t\t\t\t\tfalse,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 2},\n\t\t\t\t\tRows: []interface{}{\n\t\t\t\t\t\t[]uint64{\n\t\t\t\t\t\t\t0,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t[]string(nil),\n\t\t\t\t\t\tuint64(0),\n\t\t\t\t\t\tnil,\n\t\t\t\t\t\t[]uint64(nil),\n\t\t\t\t\t\t[]string(nil),\n\t\t\t\t\t\tnil,\n\t\t\t\t\t\tnil,\n\t\t\t\t\t\tnil,\n\t\t\t\t\t\tnil,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 3},\n\t\t\t\t\tRows: []interface{}{\n\t\t\t\t\t\t[]uint64(nil),\n\t\t\t\t\t\t[]string(nil),\n\t\t\t\t\t\tnil,\n\t\t\t\t\t\t\"plugh\",\n\t\t\t\t\t\t[]uint64{\n\t\t\t\t\t\t\t3,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t[]string(nil),\n\t\t\t\t\t\tint64(2),\n\t\t\t\t\t\tpql.NewDecimal(-101, 2),\n\t\t\t\t\t\ttime.Date(2000, time.January, 1, 0, 0, 3, 0, time.UTC),\n\t\t\t\t\t\ttrue,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tColumn: pilosa.KeyOrID{ID: ShardWidth},\n\t\t\t\t\tRows: []interface{}{\n\t\t\t\t\t\t[]uint64(nil),\n\t\t\t\t\t\t[]string(nil),\n\t\t\t\t\t\tnil,\n\t\t\t\t\t\tnil,\n\t\t\t\t\t\t[]uint64(nil),\n\t\t\t\t\t\t[]string(nil),\n\t\t\t\t\t\tnil,\n\t\t\t\t\t\tnil,\n\t\t\t\t\t\tnil,\n\t\t\t\t\t\tnil,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 4 * ShardWidth},\n\t\t\t\t\tRows: []interface{}{\n\t\t\t\t\t\t[]uint64{\n\t\t\t\t\t\t\t4,\n\t\t\t\t\t\t},\n\t\t\t\t\t\t[]string(nil),\n\t\t\t\t\t\tuint64(4),\n\t\t\t\t\t\tnil,\n\t\t\t\t\t\t[]uint64(nil),\n\t\t\t\t\t\t[]string(nil),\n\t\t\t\t\t\tnil,\n\t\t\t\t\t\tnil,\n\t\t\t\t\t\tnil,\n\t\t\t\t\t\tnil,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\trequire.Equal(t, expect, resp.Results)\n}\n\nfunc TestExecutor_Execute_Extract_Keyed(t *testing.T) {\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{TrackExistence: true, Keys: true}, \"set\")\n\tc.Query(t, c.Idx(), `\n\t\tSet(\"h\", set=1)\n\t\tSet(\"h\", set=2)\n\t\tSet(\"xyzzy\", set=2)\n\t\tSet(\"plugh\", set=1)\n\t\tClear(\"plugh\", set=1)\n\t`)\n\n\tresp := c.Query(t, c.Idx(), `Extract(All(), Rows(set))`)\n\texpect := pilosa.ExtractedTable{\n\t\tFields: []pilosa.ExtractedTableField{\n\t\t\t{\n\t\t\t\tName: \"set\",\n\t\t\t\tType: \"[]uint64\",\n\t\t\t},\n\t\t},\n\t\t// The order of these probably shouldn't matter, but currently depends indirectly on the\n\t\t// index.\n\t\tColumns: []pilosa.ExtractedTableColumn{\n\t\t\t{\n\t\t\t\tColumn: pilosa.KeyOrID{Keyed: true, Key: \"h\"},\n\t\t\t\tRows: []interface{}{\n\t\t\t\t\t[]uint64{\n\t\t\t\t\t\t1,\n\t\t\t\t\t\t2,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\tColumn: pilosa.KeyOrID{Keyed: true, Key: \"xyzzy\"},\n\t\t\t\tRows: []interface{}{\n\t\t\t\t\t[]uint64{\n\t\t\t\t\t\t2,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\tColumn: pilosa.KeyOrID{Keyed: true, Key: \"plugh\"},\n\t\t\t\tRows: []interface{}{\n\t\t\t\t\t[]uint64{},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\tif len(resp.Results) != 1 {\n\t\tt.Fail()\n\t}\n\tres := resp.Results[0].(pilosa.ExtractedTable)\n\tif !reflect.DeepEqual(expect.Fields, res.Fields) {\n\t\tt.Errorf(\"expected:\\n%v\\nbut got:\\n%v\", expect, resp.Results)\n\t}\n\tassert.ElementsMatch(t, expect.Columns, res.Columns)\n}\n\nfunc TestExecutor_Execute_MaxMemory(t *testing.T) {\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{TrackExistence: true}, \"set\")\n\tc.ImportBits(t, c.Idx(), \"set\", [][2]uint64{\n\t\t{0, 1},\n\t\t{0, 2},\n\t\t{3, 1},\n\t\t{4, 1},\n\t\t{4, 4 * ShardWidth},\n\t\t{5, ShardWidth},\n\t})\n\tc.Query(t, c.Idx(), fmt.Sprintf(\"Clear(%d, set=5)\", ShardWidth))\n\n\tresp := c.GetPrimary().QueryAPI(t, &pilosa.QueryRequest{\n\t\tIndex:     c.Idx(),\n\t\tQuery:     `Extract(All(), Rows(set))`,\n\t\tMaxMemory: 1000,\n\t})\n\texpect := []interface{}{\n\t\tpilosa.ExtractedTable{\n\t\t\tFields: []pilosa.ExtractedTableField{\n\t\t\t\t{\n\t\t\t\t\tName: \"set\",\n\t\t\t\t\tType: \"[]uint64\",\n\t\t\t\t},\n\t\t\t},\n\t\t\tColumns: []pilosa.ExtractedTableColumn{\n\t\t\t\t{\n\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 1},\n\t\t\t\t\tRows: []interface{}{\n\t\t\t\t\t\t[]uint64{\n\t\t\t\t\t\t\t0,\n\t\t\t\t\t\t\t3,\n\t\t\t\t\t\t\t4,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 2},\n\t\t\t\t\tRows: []interface{}{\n\t\t\t\t\t\t[]uint64{\n\t\t\t\t\t\t\t0,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tColumn: pilosa.KeyOrID{ID: ShardWidth},\n\t\t\t\t\tRows: []interface{}{\n\t\t\t\t\t\t[]uint64(nil),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 4 * ShardWidth},\n\t\t\t\t\tRows: []interface{}{\n\t\t\t\t\t\t[]uint64{\n\t\t\t\t\t\t\t4,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\trequire.Equal(t, expect, resp.Results)\n}\n\nfunc TestExecutor_Execute_Rows(t *testing.T) {\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"general\")\n\tc.ImportBits(t, c.Idx(), \"general\", [][2]uint64{\n\t\t{10, 0},\n\t\t{10, ShardWidth + 1},\n\t\t{11, 2},\n\t\t{11, ShardWidth + 2},\n\t\t{12, 2},\n\t\t{12, ShardWidth + 2},\n\t\t{13, 3},\n\t})\n\n\trows := c.Query(t, c.Idx(), `Rows(general)`).Results[0].(pilosa.RowIdentifiers)\n\trows.AssertEqual(t, &pilosa.RowIdentifiers{Rows: []uint64{10, 11, 12, 13}})\n\n\t// backwards compatibility\n\t// TODO: remove at Pilosa 2.0\n\trows = c.Query(t, c.Idx(), `Rows(field=general)`).Results[0].(pilosa.RowIdentifiers)\n\trows.AssertEqual(t, &pilosa.RowIdentifiers{Rows: []uint64{10, 11, 12, 13}})\n\n\trows = c.Query(t, c.Idx(), `Rows(general, limit=2)`).Results[0].(pilosa.RowIdentifiers)\n\trows.AssertEqual(t, &pilosa.RowIdentifiers{Rows: []uint64{10, 11}})\n\n\trows = c.Query(t, c.Idx(), `Rows(general, previous=10,limit=2)`).Results[0].(pilosa.RowIdentifiers)\n\trows.AssertEqual(t, &pilosa.RowIdentifiers{Rows: []uint64{11, 12}})\n\n\trows = c.Query(t, c.Idx(), `Rows(general, column=2)`).Results[0].(pilosa.RowIdentifiers)\n\trows.AssertEqual(t, &pilosa.RowIdentifiers{Rows: []uint64{11, 12}})\n}\n\n// Ensure that an empty time field returns empty Rows().\nfunc TestExecutor_Execute_RowsTimeEmpty(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"x\", pilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMD\"), \"0\", true))\n\trows := c.Query(t, c.Idx(), `Rows(x, from=1999-12-31T00:00, to=2002-01-01T03:00)`).Results[0].(pilosa.RowIdentifiers)\n\trows.AssertEqual(t, &pilosa.RowIdentifiers{})\n}\n\nfunc TestExecutor_Execute_Query_Error(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"general\")\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"integer\", pilosa.OptFieldTypeInt(-1000, 1000))\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"decimal\", pilosa.OptFieldTypeDecimal(2))\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"bool\", pilosa.OptFieldTypeBool())\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"keys\", pilosa.OptFieldKeys())\n\n\ttests := []struct {\n\t\tquery string\n\t\terror string\n\t}{\n\t\t{\n\t\t\tquery: \"GroupBy(Rows())\",\n\t\t\terror: \"missing field in Rows call\",\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(\\\"true\\\"))\",\n\t\t\terror: \"parsing: parsing:\",\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(1))\",\n\t\t\terror: \"parsing: parsing:\",\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(general, limit=-1))\",\n\t\t\terror: \"must be positive, but got\",\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(general), limit=-1)\",\n\t\t\terror: \"must be positive, but got\",\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(general), filter=Rows(general))\",\n\t\t\terror: \"parsing: parsing:\",\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(integer), prev=-1)\",\n\t\t\terror: \"unknown arg 'prev'\",\n\t\t},\n\t\t{\n\t\t\tquery: \"Rows(integer)\",\n\t\t\terror: \"int fields not supported by Rows() query\",\n\t\t},\n\t\t{\n\t\t\tquery: \"Rows(decimal)\",\n\t\t\terror: \"decimal fields not supported by Rows() query\",\n\t\t},\n\t\t{\n\t\t\tquery: \"Rows(bool)\",\n\t\t\terror: \"bool fields not supported by Rows() query\",\n\t\t},\n\t\t{\n\t\t\tquery: `Row(keys=1)`,\n\t\t\terror: `found integer ID 1 on keyed field \"keys\"`,\n\t\t},\n\t\t{\n\t\t\tquery: `Rows(keys, in=[\"a\", \"b\"], column=3)`,\n\t\t\terror: `Rows call with 'in' does not support other arguments`,\n\t\t},\n\t\t{\n\t\t\tquery: `GroupBy(Rows(keys, in=[\"a\", \"b\"], column=3))`,\n\t\t\terror: `Rows call with 'in' does not support other arguments`,\n\t\t},\n\t\t{\n\t\t\tquery: `Rows(keys, in=[\"a\", \"b\"], like=\"%sd\")`,\n\t\t\terror: `Rows call with 'in' does not support other arguments`,\n\t\t},\n\t\t{\n\t\t\tquery: `GroupBy(Rows(keys, in=[\"a\", \"b\"], like=\"%sd\"))`,\n\t\t\terror: `Rows call with 'in' does not support other arguments`,\n\t\t},\n\t}\n\n\tfor i, test := range tests {\n\t\tt.Run(fmt.Sprintf(\"%d\", i), func(t *testing.T) {\n\t\t\tr, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{\n\t\t\t\tIndex: c.Idx(),\n\t\t\t\tQuery: test.query,\n\t\t\t})\n\t\t\tif err == nil {\n\t\t\t\tt.Fatalf(\"should have gotten an error on invalid rows query, but got %#v\", r)\n\t\t\t}\n\t\t\tif !strings.Contains(err.Error(), test.error) {\n\t\t\t\tt.Fatalf(\"unexpected error message:\\n%s != %s\", test.error, err.Error())\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestExecutor_GroupByStrings(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{Keys: true}, \"generals\", pilosa.OptFieldKeys())\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{Keys: true}, \"v\", pilosa.OptFieldTypeInt(0, 1000))\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{Keys: true}, \"vv\", pilosa.OptFieldTypeInt(0, 1000))\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{Keys: true}, \"nv\", pilosa.OptFieldTypeInt(-1000, 1000))\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{Keys: true}, \"dv\", pilosa.OptFieldTypeDecimal(2))\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{Keys: true}, \"ndv\", pilosa.OptFieldTypeDecimal(1))\n\n\tif err := c.GetNode(0).API.Import(context.Background(), nil, &pilosa.ImportRequest{\n\t\tIndex:      c.Idx(),\n\t\tField:      \"generals\",\n\t\tShard:      0,\n\t\tRowKeys:    []string{\"r1\", \"r2\", \"r1\", \"r2\", \"r1\", \"r2\", \"r1\", \"r2\", \"r1\", \"r2\"},\n\t\tColumnKeys: []string{\"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\", \"c10\"},\n\t}); err != nil {\n\t\tt.Fatalf(\"importing: %v\", err)\n\t}\n\tm0 := c.GetNode(0)\n\tqcx := m0.API.Txf().NewQcx()\n\tdefer qcx.Abort()\n\n\tvar v1, v2, v3, v4, v5, v6, v7, v8, v9, v10 int64 = 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n\tvar nv1, nv2, nv3, nv4 int64 = -1, -2, -3, -4\n\tvar dv1, dv2, dv3, dv4, dv5, dv6, dv7, dv8, dv9, dv10 int64 = 111, 222, 333, 444, 555, 666, 777, 888, 999, 1000\n\tvar ndv1, ndv2, ndv3, ndv4, ndv5, ndv6, ndv7, ndv8, ndv9, ndv10 int64 = -111, -222, -333, -444, -555, -666, -777, -888, -999, -1000\n\tif err := m0.API.ImportValue(context.Background(), qcx, &pilosa.ImportValueRequest{\n\t\tIndex:      c.Idx(),\n\t\tField:      \"v\",\n\t\tShard:      0,\n\t\tColumnKeys: []string{\"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\", \"c10\"},\n\t\tValues:     []int64{v1, v2, v3, v4, v5, v6, v7, v8, v9, v10},\n\t}); err != nil {\n\t\tt.Fatalf(\"importing: %v\", err)\n\t}\n\n\tif err := m0.API.ImportValue(context.Background(), qcx, &pilosa.ImportValueRequest{\n\t\tIndex:      c.Idx(),\n\t\tField:      \"vv\",\n\t\tShard:      0,\n\t\tColumnKeys: []string{\"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\", \"c10\"},\n\t\tValues:     []int64{v1, v2, v2, v3, v3, v3, v4, v4, v4, v4},\n\t}); err != nil {\n\t\tt.Fatalf(\"importing: %v\", err)\n\t}\n\n\tif err := m0.API.ImportValue(context.Background(), qcx, &pilosa.ImportValueRequest{\n\t\tIndex:      c.Idx(),\n\t\tField:      \"nv\",\n\t\tShard:      0,\n\t\tColumnKeys: []string{\"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\", \"c10\"},\n\t\tValues:     []int64{nv1, nv2, nv2, nv3, nv3, nv3, nv4, nv4, nv4, nv4},\n\t}); err != nil {\n\t\tt.Fatalf(\"importing: %v\", err)\n\t}\n\n\tif err := m0.API.ImportValue(context.Background(), qcx, &pilosa.ImportValueRequest{\n\t\tIndex:      c.Idx(),\n\t\tField:      \"dv\",\n\t\tShard:      0,\n\t\tColumnKeys: []string{\"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\", \"c10\"},\n\t\tValues:     []int64{dv1, dv2, dv3, dv4, dv5, dv6, dv7, dv8, dv9, dv10},\n\t}); err != nil {\n\t\tt.Fatalf(\"importing: %v\", err)\n\t}\n\n\tif err := m0.API.ImportValue(context.Background(), qcx, &pilosa.ImportValueRequest{\n\t\tIndex:      c.Idx(),\n\t\tField:      \"ndv\",\n\t\tShard:      0,\n\t\tColumnKeys: []string{\"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\", \"c10\"},\n\t\tValues:     []int64{ndv1, ndv2, ndv3, ndv4, ndv5, ndv6, ndv7, ndv8, ndv9, ndv10},\n\t}); err != nil {\n\t\tt.Fatalf(\"importing: %v\", err)\n\t}\n\n\ttests := []struct {\n\t\tquery    string\n\t\texpected []pilosa.GroupCount\n\t}{\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(generals))\",\n\t\t\texpected: []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"generals\", RowID: 1, RowKey: \"r1\"}}, Count: 5},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"generals\", RowID: 2, RowKey: \"r2\"}}, Count: 5},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(generals), filter=Row(generals=r2))\",\n\t\t\texpected: []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"generals\", RowID: 2, RowKey: \"r2\"}}, Count: 5},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(generals), aggregate=Sum(field=v))\",\n\t\t\texpected: []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"generals\", RowID: 1, RowKey: \"r1\"}}, Count: 5, Agg: 25},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"generals\", RowID: 2, RowKey: \"r2\"}}, Count: 5, Agg: 30},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(generals), aggregate=Sum(field=dv))\",\n\t\t\texpected: []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"generals\", RowID: 1, RowKey: \"r1\"}}, Count: 5, Agg: 2775, DecimalAgg: pql.NewDecimal(2775, 2).Clone()},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"generals\", RowID: 2, RowKey: \"r2\"}}, Count: 5, Agg: 3220, DecimalAgg: pql.NewDecimal(3220, 2).Clone()},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(generals), aggregate=Sum(field=ndv))\",\n\t\t\texpected: []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"generals\", RowID: 1, RowKey: \"r1\"}}, Count: 5, Agg: -2775, DecimalAgg: pql.NewDecimal(-2775, 1).Clone()},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"generals\", RowID: 2, RowKey: \"r2\"}}, Count: 5, Agg: -3220, DecimalAgg: pql.NewDecimal(-3220, 1).Clone()},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(generals), aggregate=Sum(field=v), having=Condition(sum>25))\",\n\t\t\texpected: []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"generals\", RowID: 2, RowKey: \"r2\"}}, Count: 5, Agg: 30},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(generals), aggregate=Sum(field=v), having=Condition(-5<sum<27))\",\n\t\t\texpected: []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"generals\", RowID: 1, RowKey: \"r1\"}}, Count: 5, Agg: 25},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tquery:    \"GroupBy(Rows(generals), aggregate=Sum(field=v), having=Condition(count>5))\",\n\t\t\texpected: []pilosa.GroupCount{},\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(v))\",\n\t\t\texpected: []pilosa.GroupCount{\n\t\t\t\t{\n\t\t\t\t\tGroup: []pilosa.FieldRow{{Field: \"v\", Value: &v1}},\n\t\t\t\t\tCount: 1,\n\t\t\t\t\tAgg:   0,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tGroup: []pilosa.FieldRow{{Field: \"v\", Value: &v2}},\n\t\t\t\t\tCount: 1,\n\t\t\t\t\tAgg:   0,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tGroup: []pilosa.FieldRow{{Field: \"v\", Value: &v3}},\n\t\t\t\t\tCount: 1,\n\t\t\t\t\tAgg:   0,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tGroup: []pilosa.FieldRow{{Field: \"v\", Value: &v4}},\n\t\t\t\t\tCount: 1,\n\t\t\t\t\tAgg:   0,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tGroup: []pilosa.FieldRow{{Field: \"v\", Value: &v5}},\n\t\t\t\t\tCount: 1,\n\t\t\t\t\tAgg:   0,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tGroup: []pilosa.FieldRow{{Field: \"v\", Value: &v6}},\n\t\t\t\t\tCount: 1,\n\t\t\t\t\tAgg:   0,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tGroup: []pilosa.FieldRow{{Field: \"v\", Value: &v7}},\n\t\t\t\t\tCount: 1,\n\t\t\t\t\tAgg:   0,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tGroup: []pilosa.FieldRow{{Field: \"v\", Value: &v8}},\n\t\t\t\t\tCount: 1,\n\t\t\t\t\tAgg:   0,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tGroup: []pilosa.FieldRow{{Field: \"v\", Value: &v9}},\n\t\t\t\t\tCount: 1,\n\t\t\t\t\tAgg:   0,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tGroup: []pilosa.FieldRow{{Field: \"v\", Value: &v10}},\n\t\t\t\t\tCount: 1,\n\t\t\t\t\tAgg:   0,\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(vv), aggregate=Sum(field=vv), having=Condition(count > 2))\",\n\t\t\texpected: []pilosa.GroupCount{\n\t\t\t\t{\n\t\t\t\t\tGroup: []pilosa.FieldRow{{Field: \"vv\", Value: &v3}},\n\t\t\t\t\tCount: 3,\n\t\t\t\t\tAgg:   9,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tGroup: []pilosa.FieldRow{{Field: \"vv\", Value: &v4}},\n\t\t\t\t\tCount: 4,\n\t\t\t\t\tAgg:   16,\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(nv), aggregate=Sum(field=nv), limit=2)\",\n\t\t\texpected: []pilosa.GroupCount{\n\t\t\t\t{\n\t\t\t\t\tGroup: []pilosa.FieldRow{{Field: \"nv\", Value: &nv4}},\n\t\t\t\t\tCount: 4,\n\t\t\t\t\tAgg:   -16,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tGroup: []pilosa.FieldRow{{Field: \"nv\", Value: &nv3}},\n\t\t\t\t\tCount: 3,\n\t\t\t\t\tAgg:   -9,\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(nv), aggregate=Sum(field=nv), having=Condition(count > 2), limit=2)\",\n\t\t\texpected: []pilosa.GroupCount{\n\t\t\t\t{\n\t\t\t\t\tGroup: []pilosa.FieldRow{{Field: \"nv\", Value: &nv4}},\n\t\t\t\t\tCount: 4,\n\t\t\t\t\tAgg:   -16,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tGroup: []pilosa.FieldRow{{Field: \"nv\", Value: &nv3}},\n\t\t\t\t\tCount: 3,\n\t\t\t\t\tAgg:   -9,\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(vv), Rows(nv), aggregate=Sum(field=vv), having=Condition(count > 2))\",\n\t\t\texpected: []pilosa.GroupCount{\n\t\t\t\t{\n\t\t\t\t\tGroup: []pilosa.FieldRow{\n\t\t\t\t\t\t{Field: \"vv\", Value: &v3},\n\t\t\t\t\t\t{Field: \"nv\", Value: &nv3},\n\t\t\t\t\t},\n\t\t\t\t\tCount: 3,\n\t\t\t\t\tAgg:   9,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tGroup: []pilosa.FieldRow{\n\t\t\t\t\t\t{Field: \"vv\", Value: &v4},\n\t\t\t\t\t\t{Field: \"nv\", Value: &nv4},\n\t\t\t\t\t},\n\t\t\t\t\tCount: 4,\n\t\t\t\t\tAgg:   16,\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor i, tst := range tests {\n\t\tt.Run(fmt.Sprintf(\"%s%d\", tst.query, i), func(t *testing.T) {\n\t\t\tr, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{\n\t\t\t\tIndex: c.Idx(),\n\t\t\t\tQuery: tst.query,\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tresults := r.Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\ttest.CheckGroupBy(t, tst.expected, results)\n\t\t})\n\t}\n}\n\nfunc TestExecutor_Execute_Rows_Keys(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\n\t_, err := c.GetNode(0).API.CreateIndex(context.Background(), c.Idx(), pilosa.IndexOptions{Keys: true})\n\tif err != nil {\n\t\tt.Fatalf(\"creating index: %v\", err)\n\t}\n\n\t_, err = c.GetNode(0).API.CreateField(context.Background(), c.Idx(), \"f\", pilosa.OptFieldKeys())\n\tif err != nil {\n\t\tt.Fatalf(\"creating field: %v\", err)\n\t}\n\n\t_, err = c.GetNode(0).API.CreateField(context.Background(), c.Idx(), \"f_id\")\n\tif err != nil {\n\t\tt.Fatalf(\"creating field: %v\", err)\n\t}\n\n\t// setup some data. 10 bits in each of shards 0 through 9. starting at\n\t// row/col shardNum and progressing to row/col shardNum+10. Also set the\n\t// previous 2 for each bit if row >0.\n\tquery := strings.Builder{}\n\tfor shard := 0; shard < 10; shard++ {\n\t\tfor i := shard; i < shard+10; i++ {\n\t\t\tfor row := i; row >= 0 && row > i-3; row-- {\n\t\t\t\tquery.WriteString(fmt.Sprintf(\"Set(\\\"%d\\\", f=\\\"%d\\\")\", shard*pilosa.ShardWidth+i, row))\n\t\t\t}\n\t\t}\n\t}\n\t_, err = c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{\n\t\tIndex: c.Idx(),\n\t\tQuery: query.String(),\n\t})\n\tif err != nil {\n\t\tt.Fatalf(\"querying: %v\", err)\n\t}\n\n\ttests := []struct {\n\t\tq      string\n\t\texp    []string\n\t\texpErr string\n\t}{\n\t\t{\n\t\t\tq:   `Rows(f)`,\n\t\t\texp: []string{\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\"},\n\t\t},\n\t\t// backwards compatibility\n\t\t// TODO: remove at Pilosa 2.0\n\t\t{\n\t\t\tq:   `Rows(field=f)`,\n\t\t\texp: []string{\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\"},\n\t\t},\n\t\t{\n\t\t\tq:   `Rows(f, limit=2)`,\n\t\t\texp: []string{\"0\", \"1\"},\n\t\t},\n\t\t// backwards compatibility\n\t\t// TODO: remove at Pilosa 2.0\n\t\t{\n\t\t\tq:   `Rows(field=f, limit=2)`,\n\t\t\texp: []string{\"0\", \"1\"},\n\t\t},\n\t\t{\n\t\t\tq:   `Rows(f, previous=\"15\")`,\n\t\t\texp: []string{\"16\", \"17\", \"18\"},\n\t\t},\n\t\t{\n\t\t\tq:   `Rows(f, previous=\"11\", limit=2)`,\n\t\t\texp: []string{\"12\", \"13\"},\n\t\t},\n\t\t{\n\t\t\tq:   `Rows(f, previous=\"17\", limit=5)`,\n\t\t\texp: []string{\"18\"},\n\t\t},\n\t\t{\n\t\t\tq:   `Rows(f, previous=\"18\")`,\n\t\t\texp: []string{},\n\t\t},\n\t\t{\n\t\t\tq:   `Rows(f, previous=\"1\", limit=0)`,\n\t\t\texp: []string{},\n\t\t},\n\t\t{\n\t\t\tq:   `Rows(f, column=\"1\")`,\n\t\t\texp: []string{\"0\", \"1\"},\n\t\t},\n\t\t{\n\t\t\tq:   `Rows(f, column=\"2\")`,\n\t\t\texp: []string{\"0\", \"1\", \"2\"},\n\t\t},\n\t\t{\n\t\t\tq:   `Rows(f, column=\"3\")`,\n\t\t\texp: []string{\"1\", \"2\", \"3\"},\n\t\t},\n\t\t{\n\t\t\tq:   `Rows(f, limit=2, column=\"3\")`,\n\t\t\texp: []string{\"1\", \"2\"},\n\t\t},\n\t\t{\n\t\t\tq:   fmt.Sprintf(`Rows(f, previous=\"15\", column=\"%d\")`, ShardWidth*9+17),\n\t\t\texp: []string{\"16\", \"17\"},\n\t\t},\n\t\t{\n\t\t\tq:   fmt.Sprintf(`Rows(f, previous=\"11\", limit=2, column=\"%d\")`, ShardWidth*5+14),\n\t\t\texp: []string{\"12\", \"13\"},\n\t\t},\n\t\t{\n\t\t\tq:   fmt.Sprintf(`Rows(f, previous=\"17\", limit=5, column=\"%d\")`, ShardWidth*9+18),\n\t\t\texp: []string{\"18\"},\n\t\t},\n\t\t{\n\t\t\tq:   `Rows(f, previous=\"18\", column=\"19\")`,\n\t\t\texp: []string{},\n\t\t},\n\t\t{\n\t\t\tq:   `Rows(f, previous=\"1\", limit=0, column=\"0\")`,\n\t\t\texp: []string{},\n\t\t},\n\t\t{\n\t\t\tq:   `Rows(f, like=\"__\")`,\n\t\t\texp: []string{\"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\"},\n\t\t},\n\t\t{\n\t\t\tq:      `Rows(f_id, like=7)`,\n\t\t\texpErr: \"parsing:\",\n\t\t},\n\t\t{\n\t\t\tq:      `Rows(f_id, like=\"__\")`,\n\t\t\texpErr: \"executing: translating call:\",\n\t\t},\n\t}\n\n\tfor i, test := range tests {\n\t\tt.Run(fmt.Sprintf(\"#%d_%s\", i, test.q), func(t *testing.T) {\n\t\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: test.q}); err != nil {\n\t\t\t\tif !strings.HasPrefix(err.Error(), test.expErr) {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif test.expErr != \"\" {\n\t\t\t\t\tt.Fatalf(\"got success, expected error similar to: %+v\", test.expErr)\n\t\t\t\t}\n\t\t\t\trows := res.Results[0].(pilosa.RowIdentifiers)\n\t\t\t\tif !assert.ElementsMatch(t, rows.Keys, test.exp) {\n\t\t\t\t\tt.Fatalf(\"\\ngot: %+v\\nexp: %+v\", rows.Keys, test.exp)\n\t\t\t\t} else if rows.Rows != nil {\n\t\t\t\t\tif test.exp == nil {\n\t\t\t\t\t\tif res.Results != nil {\n\t\t\t\t\t\t\tt.Fatalf(\"\\ngot: %+v\\nexp: nil, %[1]T, %#[1]v\", res.Results)\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\trows := res.Results[0].(pilosa.RowIdentifiers)\n\t\t\t\t\t\tif !reflect.DeepEqual(rows.Keys, test.exp) {\n\t\t\t\t\t\t\tt.Fatalf(\"\\ngot: %+v %[1]T\\nexp: %+v  %[2]T\", rows.Keys, test.exp)\n\t\t\t\t\t\t} else if rows.Rows != nil {\n\t\t\t\t\t\t\tt.Fatalf(\"\\ngot: %+v %[1]T\\nexp: nil\", rows.Rows)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestExecutor_ForeignIndex(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\tchild := c.Idx(\"c\")\n\tparent := c.Idx(\"p\")\n\tstepChild := c.Idx(\"d\")\n\n\tc.CreateField(t, parent, pilosa.IndexOptions{Keys: true}, \"general\")\n\tc.CreateField(t, child, pilosa.IndexOptions{}, \"parent_id\",\n\t\tpilosa.OptFieldTypeInt(0, math.MaxInt64),\n\t\tpilosa.OptFieldForeignIndex(parent),\n\t)\n\tc.CreateField(t, child, pilosa.IndexOptions{}, \"parent_set_id\",\n\t\tpilosa.OptFieldForeignIndex(parent),\n\t)\n\tc.CreateField(t, child, pilosa.IndexOptions{}, \"color\",\n\t\tpilosa.OptFieldKeys(),\n\t)\n\n\t// stepchild/other field needs to have usesKeys=true\n\tcrashSchemaJson := fmt.Sprintf(`{\"indexes\": [{\"name\": \"%q\",\"createdAt\": 1611247966371721700,\"options\": {\"keys\": true,\"trackExistence\": true},\"shardWidth\": 1048576},{\"name\": \"%d\",\"createdAt\": 1611247953796662800,\"options\": {\"keys\": true,\"trackExistence\": true},\"shardWidth\": 1048576,\"fields\": [{\"name\": \"parent_id\",\"createdAt\": 1611247953797265700,\"options\": {\"type\": \"int\",\"base\": 0,\"bitDepth\": 28,\"min\": -9223372036854776000,\"max\": 9223372036854776000,\"keys\": false,\"foreignIndex\": \"%q\"}},{\"name\": \"other\",\"createdAt\": 1611247953796814000,\"options\": {\"type\": \"int\",\"base\": 0,\"bitDepth\": 17,\"min\": -9223372036854776000,\"max\": 9223372036854776000,\"keys\": true,\"foreignIndex\": \"\"}}]}]}`, c, c, c)\n\n\tcrashSchema := &pilosa.Schema{}\n\terr := json.Unmarshal([]byte(crashSchemaJson), &crashSchema)\n\tif err != nil {\n\t\tt.Fatalf(\"json unmarshall: %v\", err)\n\t}\n\terr = c.GetNode(0).API.ApplySchema(context.Background(), crashSchema, false)\n\tif err != nil {\n\t\tt.Fatalf(\"applying JSON schema: %v\", err)\n\t}\n\n\t// Populate parent data.\n\tc.Query(t, parent, fmt.Sprintf(`\n\t\t\tSet(\"one\", general=1)\n\t\t\tSet(\"two\", general=1)\n\t\t\tSet(\"three\", general=1)\n\n\t\t\tSet(\"twenty-one\", general=2)\n\t\t\tSet(\"twenty-two\", general=2)\n\t\t\tSet(\"twenty-three\", general=2)\n\n\t\t\tSet(\"one\", general=%d)\n\t\t\tSet(\"twenty-one\", general=%d)\n\t\t`, ShardWidth, ShardWidth))\n\n\t// Populate child data.\n\tc.Query(t, child, fmt.Sprintf(`\n\t\t\tSet(1, parent_id=\"one\")\n\t\t\tSet(2, parent_id=\"two\")\n\t\t\tSet(%d, parent_id=\"one\")\n\t\t\tSet(4, parent_id=\"twenty-one\")\n\t\t`, ShardWidth))\n\tc.Query(t, child, fmt.Sprintf(`\n\t\t\tSet(1, parent_set_id=\"one\")\n\t\t\tSet(2, parent_set_id=\"two\")\n\t\t\tSet(%d, parent_set_id=\"one\")\n\t\t\tSet(4, parent_set_id=\"twenty-one\")\n\t\t`, ShardWidth))\n\n\t// Populate color data.\n\tc.Query(t, child, fmt.Sprintf(`\n\t\t\tSet(1, color=\"red\")\n\t\t\tSet(2, color=\"blue\")\n\t\t\tSet(%d, color=\"blue\")\n\t\t\tSet(4, color=\"red\")\n\t\t`, ShardWidth))\n\n\tdistinct := c.Query(t, child, fmt.Sprintf(`Distinct(index=%c, field=\"parent_id\")`, c)).Results[0].(pilosa.SignedRow)\n\tif !sameStringSlice(distinct.Pos.Keys, []string{\"one\", \"two\", \"twenty-one\"}) {\n\t\tt.Fatalf(\"unexpected keys: %v\", distinct.Pos.Keys)\n\t}\n\trow := c.Query(t, child, fmt.Sprintf(`Distinct(index=%c, field=\"parent_set_id\")`, c)).Results[0].(*pilosa.Row)\n\tif !sameStringSlice(row.Keys, []string{\"one\", \"two\", \"twenty-one\"}) {\n\t\tt.Fatalf(\"unexpected keys: %v\", row.Keys)\n\t}\n\n\tcrash := c.Query(t, stepChild, `Distinct(Row(parent_id=3), field=other)`).Results[0].(pilosa.SignedRow)\n\tif !sameStringSlice(crash.Pos.Keys, []string{}) {\n\t\t// empty result; error condition does not require data\n\t\tt.Fatalf(\"unexpected columns: %v\", crash.Pos.Keys)\n\t}\n\n\teq := c.Query(t, child, `Row(parent_id==\"one\")`).Results[0].(*pilosa.Row)\n\tif !reflect.DeepEqual(eq.Columns(), []uint64{1, ShardWidth}) {\n\t\tt.Fatalf(\"unexpected columns: %v\", eq.Columns())\n\t}\n\n\tneq := c.Query(t, child, `Row(parent_id!=\"one\")`).Results[0].(*pilosa.Row)\n\tif !reflect.DeepEqual(neq.Columns(), []uint64{2, 4}) {\n\t\tt.Fatalf(\"unexpected columns: %v\", neq.Columns())\n\t}\n\n\tjoin := c.Query(t, parent, fmt.Sprintf(`Intersect(Row(general=%d), Distinct(Row(color=\"blue\"), index=%c, field=\"parent_id\"))`, ShardWidth, c)).Results[0].(*pilosa.Row)\n\tif !reflect.DeepEqual(join.Keys, []string{\"one\"}) {\n\t\tt.Fatalf(\"unexpected keys: %v\", join.Keys)\n\t}\n\tjoin = c.Query(t, parent, fmt.Sprintf(`Intersect(Row(general=%d), Distinct(Row(color=\"blue\"), index=%c, field=\"parent_set_id\"))`, ShardWidth, c)).Results[0].(*pilosa.Row)\n\tif !reflect.DeepEqual(join.Keys, []string{\"one\"}) {\n\t\tt.Fatalf(\"unexpected keys: %v\", join.Keys)\n\t}\n}\n\n// sameStringSlice is a helper function which compares two string\n// slices without enforcing order.\nfunc sameStringSlice(x, y []string) bool {\n\tif len(x) != len(y) {\n\t\treturn false\n\t}\n\t// create a map of string -> int\n\tdiff := make(map[string]int, len(x))\n\tfor _, _x := range x {\n\t\t// 0 value for int is 0, so just increment a counter for the string\n\t\tdiff[_x]++\n\t}\n\tfor _, _y := range y {\n\t\t// If the string _y is not in diff bail out early\n\t\tif _, ok := diff[_y]; !ok {\n\t\t\treturn false\n\t\t}\n\t\tdiff[_y] -= 1\n\t\tif diff[_y] == 0 {\n\t\t\tdelete(diff, _y)\n\t\t}\n\t}\n\treturn len(diff) == 0\n}\n\nfunc TestExecutor_Execute_DistinctFailure(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"general\")\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"v\", pilosa.OptFieldTypeInt(0, 1000))\n\tc.ImportBits(t, c.Idx(), \"general\", [][2]uint64{\n\t\t{10, 0},\n\t\t{10, 1},\n\t\t{10, ShardWidth + 1},\n\t\t{11, 2},\n\t\t{11, ShardWidth + 2},\n\t\t{12, 2},\n\t\t{12, ShardWidth + 2},\n\t})\n\n\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(0, v=10)`}); err != nil {\n\t\tt.Fatal(err)\n\t} else if _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(1, v=100)`}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tt.Run(\"BasicDistinct\", func(t *testing.T) {\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Distinct(field=\"v\")`}); err != nil {\n\t\t\tt.Fatalf(\"unexpected error: \\\"%v\\\"\", err)\n\t\t}\n\t})\n}\n\nfunc TestExecutor_Execute_GroupBy(t *testing.T) {\n\tgroupByTest := func(t *testing.T, clusterSize int) {\n\t\tc := test.MustRunCluster(t, clusterSize)\n\t\tdefer c.Close()\n\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"general\")\n\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"sub\")\n\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"tq\", pilosa.OptFieldTypeTime(\"YMDH\", \"0\"))\n\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"v\", pilosa.OptFieldTypeInt(0, 1000))\n\t\tc.ImportBits(t, c.Idx(), \"general\", [][2]uint64{\n\t\t\t{10, 0},\n\t\t\t{10, 1},\n\t\t\t{10, ShardWidth + 1},\n\t\t\t{11, 2},\n\t\t\t{11, ShardWidth + 2},\n\t\t\t{12, 2},\n\t\t\t{12, ShardWidth + 2},\n\t\t})\n\n\t\tc.ImportBits(t, c.Idx(), \"sub\", [][2]uint64{\n\t\t\t{100, 0},\n\t\t\t{100, 1},\n\t\t\t{100, 3},\n\t\t\t{100, ShardWidth + 1},\n\n\t\t\t{110, 2},\n\t\t\t{110, 0},\n\t\t})\n\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(0, v=10)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Set(1, v=100)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: fmt.Sprintf(`Set(%d, v=100)`, ShardWidth+10)}); err != nil { // Workaround distinct bug where v must be set in every shard\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tt.Run(\"No Field List Arguments\", func(t *testing.T) {\n\t\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `GroupBy()`}); err != nil {\n\t\t\t\tif !strings.Contains(err.Error(), \"need at least one child call\") {\n\t\t\t\t\tt.Fatalf(\"unexpected error: \\\"%v\\\"\", err)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"Unknown Field \", func(t *testing.T) {\n\t\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `GroupBy(Rows(missing))`}); err != nil {\n\t\t\t\tif errors.Cause(err) != pilosa.ErrFieldNotFound {\n\t\t\t\t\tt.Fatalf(\"unexpected error\\n\\\"%s\\\" not returned instead \\n\\\"%s\\\"\", pilosa.ErrFieldNotFound, err)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\n\t\t// backwards compatibility\n\t\t// TODO: remove at Pilosa 2.0\n\t\tt.Run(\"BasicLegacy\", func(t *testing.T) {\n\t\t\texpected := []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 10}, {Field: \"sub\", RowID: 100}}, Count: 3},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 10}, {Field: \"sub\", RowID: 110}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 11}, {Field: \"sub\", RowID: 110}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 12}, {Field: \"sub\", RowID: 110}}, Count: 1},\n\t\t\t}\n\n\t\t\tresults := c.Query(t, c.Idx(), `GroupBy(Rows(field=general), Rows(sub))`).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\ttest.CheckGroupBy(t, expected, results)\n\t\t})\n\n\t\tt.Run(\"Basic\", func(t *testing.T) {\n\t\t\texpected := []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 10}, {Field: \"sub\", RowID: 100}}, Count: 3},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 10}, {Field: \"sub\", RowID: 110}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 11}, {Field: \"sub\", RowID: 110}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 12}, {Field: \"sub\", RowID: 110}}, Count: 1},\n\t\t\t}\n\n\t\t\tresults := c.Query(t, c.Idx(), `GroupBy(Rows(general), Rows(sub))`).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\ttest.CheckGroupBy(t, expected, results)\n\t\t})\n\n\t\tt.Run(\"Filter\", func(t *testing.T) {\n\t\t\texpected := []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 10}, {Field: \"sub\", RowID: 100}}, Count: 3},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 10}, {Field: \"sub\", RowID: 110}}, Count: 1},\n\t\t\t}\n\n\t\t\tresults := c.Query(t, c.Idx(), `GroupBy(Rows(general), Rows(sub), filter=Row(general=10))`).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\ttest.CheckGroupBy(t, expected, results)\n\t\t})\n\n\t\tt.Run(\"Aggregate\", func(t *testing.T) {\n\t\t\texpected := []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 10}, {Field: \"sub\", RowID: 100}}, Count: 2, Agg: 110},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 10}, {Field: \"sub\", RowID: 110}}, Count: 1, Agg: 10},\n\t\t\t}\n\n\t\t\tresults := c.Query(t, c.Idx(), `GroupBy(Rows(general), Rows(sub), aggregate=Sum(field=v))`).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\ttest.CheckGroupBy(t, expected, results)\n\t\t})\n\n\t\tt.Run(\"AggregateCountDistinct\", func(t *testing.T) {\n\t\t\texpected := []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 10}, {Field: \"sub\", RowID: 100}}, Count: 3, Agg: 2},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 10}, {Field: \"sub\", RowID: 110}}, Count: 1, Agg: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 11}, {Field: \"sub\", RowID: 110}}, Count: 1, Agg: 0},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 12}, {Field: \"sub\", RowID: 110}}, Count: 1, Agg: 0},\n\t\t\t}\n\n\t\t\tresults := c.Query(t, c.Idx(), `GroupBy(Rows(general), Rows(sub), aggregate=Count(Distinct(field=v)))`).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\ttest.CheckGroupBy(t, expected, results)\n\t\t})\n\n\t\tt.Run(\"AggregateCountDistinctFilter\", func(t *testing.T) {\n\t\t\texpected := []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 10}, {Field: \"sub\", RowID: 100}}, Count: 1, Agg: 1},\n\t\t\t}\n\n\t\t\tresults := c.Query(t, c.Idx(), `GroupBy(Rows(general), Rows(sub), filter=Row(v > 10), aggregate=Count(Distinct(field=v)))`).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\ttest.CheckGroupBy(t, expected, results)\n\t\t})\n\n\t\tt.Run(\"AggregateCountDistinctFilterDistinct\", func(t *testing.T) {\n\t\t\texpected := []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 10}, {Field: \"sub\", RowID: 100}}, Count: 3, Agg: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 10}, {Field: \"sub\", RowID: 110}}, Count: 1, Agg: 0},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 11}, {Field: \"sub\", RowID: 110}}, Count: 1, Agg: 0},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 12}, {Field: \"sub\", RowID: 110}}, Count: 1, Agg: 0},\n\t\t\t}\n\n\t\t\tresults := c.Query(t, c.Idx(), `GroupBy(Rows(general), Rows(sub), aggregate=Count(Distinct(Row(v > 10), field=v)))`).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\ttest.CheckGroupBy(t, expected, results)\n\t\t})\n\n\t\tt.Run(\"check field offset no limit\", func(t *testing.T) {\n\t\t\texpected := []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 11}}, Count: 2},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 12}}, Count: 2},\n\t\t\t}\n\n\t\t\tresults := c.Query(t, c.Idx(), `GroupBy(Rows(general, previous=10))`).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\ttest.CheckGroupBy(t, expected, results)\n\t\t})\n\n\t\tt.Run(\"check field offset limit\", func(t *testing.T) {\n\t\t\texpected := []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 11}}, Count: 2},\n\t\t\t}\n\n\t\t\tresults := c.Query(t, c.Idx(), `GroupBy(Rows(general, previous=10), limit=1)`).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\ttest.CheckGroupBy(t, expected, results)\n\t\t})\n\n\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"a\")\n\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"b\")\n\t\tc.ImportBits(t, c.Idx(), \"a\", [][2]uint64{\n\t\t\t{0, 1},\n\t\t\t{1, ShardWidth + 1},\n\t\t})\n\t\tc.ImportBits(t, c.Idx(), \"b\", [][2]uint64{\n\t\t\t{0, ShardWidth + 1},\n\t\t\t{1, 1},\n\t\t})\n\n\t\tt.Run(\"tricky data\", func(t *testing.T) {\n\t\t\texpected := []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"a\", RowID: 0}, {Field: \"b\", RowID: 1}}, Count: 1},\n\t\t\t}\n\n\t\t\tresults := c.Query(t, c.Idx(), `GroupBy(Rows(a), Rows(b), limit=1)`).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\ttest.CheckGroupBy(t, expected, results)\n\t\t})\n\n\t\t// set the same bits in a single shard in three fields\n\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"wa\")\n\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"wb\")\n\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"wc\")\n\t\tc.ImportBits(t, c.Idx(), \"wa\", [][2]uint64{\n\t\t\t{0, 0}, {0, 1}, {0, 2}, // all\n\t\t\t{1, 1},         // odds\n\t\t\t{2, 0}, {2, 2}, // evens\n\t\t\t{3, 3}, // no overlap\n\t\t})\n\t\tc.ImportBits(t, c.Idx(), \"wb\", [][2]uint64{\n\t\t\t{0, 0},\n\t\t\t{0, 1},\n\t\t\t{0, 2},\n\t\t\t{1, 1},\n\t\t\t{2, 0},\n\t\t\t{2, 2},\n\t\t\t{3, 3},\n\t\t})\n\t\tc.ImportBits(t, c.Idx(), \"wc\", [][2]uint64{\n\t\t\t{0, 0},\n\t\t\t{0, 1},\n\t\t\t{0, 2},\n\t\t\t{1, 1},\n\t\t\t{2, 0},\n\t\t\t{2, 2},\n\t\t\t{3, 3},\n\t\t})\n\n\t\tt.Run(\"test wrapping with previous\", func(t *testing.T) {\n\t\t\tresults := c.Query(t, c.Idx(), `GroupBy(Rows(wa), Rows(wb), Rows(wc, previous=1), limit=3)`).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\texpected := []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"wa\", RowID: 0}, {Field: \"wb\", RowID: 0}, {Field: \"wc\", RowID: 2}}, Count: 2},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"wa\", RowID: 0}, {Field: \"wb\", RowID: 1}, {Field: \"wc\", RowID: 0}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"wa\", RowID: 0}, {Field: \"wb\", RowID: 1}, {Field: \"wc\", RowID: 1}}, Count: 1},\n\t\t\t}\n\t\t\ttest.CheckGroupBy(t, expected, results)\n\t\t})\n\n\t\tt.Run(\"test previous is last result\", func(t *testing.T) {\n\t\t\tresults := c.Query(t, c.Idx(), `GroupBy(Rows(wa, previous=3), Rows(wb, previous=3), Rows(wc, previous=3), limit=3)`).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\tif len(results) > 0 {\n\t\t\t\tt.Fatalf(\"expected no results because previous specified last result\")\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"test wrapping multiple\", func(t *testing.T) {\n\t\t\tresults := c.Query(t, c.Idx(), `GroupBy(Rows(wa), Rows(wb, previous=2), Rows(wc, previous=2), limit=1)`).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\texpected := []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"wa\", RowID: 1}, {Field: \"wb\", RowID: 0}, {Field: \"wc\", RowID: 0}}, Count: 1},\n\t\t\t}\n\t\t\ttest.CheckGroupBy(t, expected, results)\n\t\t})\n\n\t\t// test multiple shards with distinct results (different rows) and same\n\t\t// rows to ensure ordering, limit behavior and correctness\n\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"ma\")\n\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"mb\")\n\t\tc.ImportBits(t, c.Idx(), \"ma\", [][2]uint64{\n\t\t\t{0, 0},\n\t\t\t{1, ShardWidth},\n\t\t\t{2, 0},\n\t\t\t{3, ShardWidth},\n\t\t})\n\t\tc.ImportBits(t, c.Idx(), \"mb\", [][2]uint64{\n\t\t\t{0, 0},\n\t\t\t{1, ShardWidth},\n\t\t\t{2, 0},\n\t\t\t{3, ShardWidth},\n\t\t})\n\t\tt.Run(\"distinct rows in different shards\", func(t *testing.T) {\n\t\t\tresults := c.Query(t, c.Idx(), `GroupBy(Rows(ma), Rows(mb), limit=5)`).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\texpected := []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"ma\", RowID: 0}, {Field: \"mb\", RowID: 0}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"ma\", RowID: 0}, {Field: \"mb\", RowID: 2}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"ma\", RowID: 1}, {Field: \"mb\", RowID: 1}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"ma\", RowID: 1}, {Field: \"mb\", RowID: 3}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"ma\", RowID: 2}, {Field: \"mb\", RowID: 0}}, Count: 1},\n\t\t\t}\n\t\t\ttest.CheckGroupBy(t, expected, results)\n\t\t})\n\n\t\tt.Run(\"distinct rows in different shards with row limit\", func(t *testing.T) {\n\t\t\tresults := c.Query(t, c.Idx(), `GroupBy(Rows(ma), Rows(mb, limit=2), limit=5)`).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\texpected := []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"ma\", RowID: 0}, {Field: \"mb\", RowID: 0}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"ma\", RowID: 1}, {Field: \"mb\", RowID: 1}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"ma\", RowID: 2}, {Field: \"mb\", RowID: 0}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"ma\", RowID: 3}, {Field: \"mb\", RowID: 1}}, Count: 1},\n\t\t\t}\n\t\t\ttest.CheckGroupBy(t, expected, results)\n\t\t})\n\n\t\tt.Run(\"distinct rows in different shards with column arg\", func(t *testing.T) {\n\t\t\tresults := c.Query(t, c.Idx(), fmt.Sprintf(`GroupBy(Rows(ma), Rows(mb, column=%d), limit=5)`, ShardWidth)).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\texpected := []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"ma\", RowID: 1}, {Field: \"mb\", RowID: 1}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"ma\", RowID: 1}, {Field: \"mb\", RowID: 3}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"ma\", RowID: 3}, {Field: \"mb\", RowID: 1}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"ma\", RowID: 3}, {Field: \"mb\", RowID: 3}}, Count: 1},\n\t\t\t}\n\t\t\ttest.CheckGroupBy(t, expected, results)\n\t\t})\n\n\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"na\")\n\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"nb\")\n\t\tc.ImportBits(t, c.Idx(), \"na\", [][2]uint64{\n\t\t\t{0, 0},\n\t\t\t{0, ShardWidth},\n\t\t\t{1, 0},\n\t\t\t{1, ShardWidth},\n\t\t})\n\t\tc.ImportBits(t, c.Idx(), \"nb\", [][2]uint64{\n\t\t\t{0, 0},\n\t\t\t{0, ShardWidth},\n\t\t\t{1, 0},\n\t\t\t{1, ShardWidth},\n\t\t})\n\t\tt.Run(\"same rows in different shards\", func(t *testing.T) {\n\t\t\tresults := c.Query(t, c.Idx(), `GroupBy(Rows(na), Rows(nb))`).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\texpected := []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"na\", RowID: 0}, {Field: \"nb\", RowID: 0}}, Count: 2},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"na\", RowID: 0}, {Field: \"nb\", RowID: 1}}, Count: 2},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"na\", RowID: 1}, {Field: \"nb\", RowID: 0}}, Count: 2},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"na\", RowID: 1}, {Field: \"nb\", RowID: 1}}, Count: 2},\n\t\t\t}\n\t\t\ttest.CheckGroupBy(t, expected, results)\n\t\t})\n\n\t\t// test paging over results using previous. set the same bits in three\n\t\t// fields\n\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"ppa\")\n\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"ppb\")\n\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"ppc\")\n\t\tc.ImportBits(t, c.Idx(), \"ppa\", [][2]uint64{\n\t\t\t{0, 0},\n\t\t\t{1, 0},\n\t\t\t{2, 0},\n\t\t\t{3, 0},\n\t\t\t{3, 91000},\n\t\t\t{3, ShardWidth},\n\t\t\t{3, ShardWidth * 2},\n\t\t\t{3, ShardWidth * 3},\n\t\t})\n\t\tc.ImportBits(t, c.Idx(), \"ppb\", [][2]uint64{\n\t\t\t{0, 0},\n\t\t\t{1, 0},\n\t\t\t{2, 0},\n\t\t\t{3, 0},\n\t\t\t{3, 91000},\n\t\t\t{3, ShardWidth},\n\t\t\t{3, ShardWidth * 2},\n\t\t\t{3, ShardWidth * 3},\n\t\t})\n\t\tc.ImportBits(t, c.Idx(), \"ppc\", [][2]uint64{\n\t\t\t{0, 0},\n\t\t\t{1, 0},\n\t\t\t{2, 0},\n\t\t\t{3, 0},\n\t\t\t{3, 91000},\n\t\t\t{3, ShardWidth},\n\t\t\t{3, ShardWidth * 2},\n\t\t\t{3, ShardWidth * 3},\n\t\t})\n\n\t\tt.Run(\"test wrapping with previous\", func(t *testing.T) {\n\t\t\ttotalResults := make([]pilosa.GroupCount, 0)\n\t\t\tresults := c.Query(t, c.Idx(), `GroupBy(Rows(ppa), Rows(ppb), Rows(ppc), limit=3)`).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\ttotalResults = append(totalResults, results...)\n\t\t\tfor len(totalResults) < 64 {\n\t\t\t\tlastGroup := results[len(results)-1].Group\n\t\t\t\tquery := fmt.Sprintf(\"GroupBy(Rows(ppa, previous=%d), Rows(ppb, previous=%d), Rows(ppc, previous=%d), limit=3)\", lastGroup[0].RowID, lastGroup[1].RowID, lastGroup[2].RowID)\n\t\t\t\tresults = c.Query(t, c.Idx(), query).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\t\ttotalResults = append(totalResults, results...)\n\t\t\t}\n\n\t\t\texpected := make([]pilosa.GroupCount, 64)\n\t\t\tfor i := 0; i < 64; i++ {\n\t\t\t\texpected[i] = pilosa.GroupCount{Group: []pilosa.FieldRow{{Field: \"ppa\", RowID: uint64(i / 16)}, {Field: \"ppb\", RowID: uint64((i % 16) / 4)}, {Field: \"ppc\", RowID: uint64(i % 4)}}, Count: 1}\n\t\t\t}\n\t\t\texpected[63].Count = 5\n\n\t\t\ttest.CheckGroupBy(t, expected, totalResults)\n\t\t})\n\n\t\t// test row keys\n\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"generalk\", pilosa.OptFieldKeys())\n\t\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"subk\", pilosa.OptFieldKeys())\n\t\tc.Query(t, c.Idx(), `\n\t\t\tSet(0, generalk=\"ten\")\n\t\t\tSet(1, generalk=\"ten\")\n\t\t\tSet(1001, generalk=\"ten\")\n\t\t\tSet(2, generalk=\"eleven\")\n\t\t\tSet(1002, generalk=\"eleven\")\n\t\t\tSet(2, generalk=\"twelve\")\n\t\t\tSet(1002, generalk=\"twelve\")\n\n\t\t\tSet(0, subk=\"one-hundred\")\n\t\t\tSet(1, subk=\"one-hundred\")\n\t\t\tSet(3, subk=\"one-hundred\")\n\t\t\tSet(1001, subk=\"one-hundred\")\n\t\t\tSet(2, subk=\"one-hundred-ten\")\n\t\t\tSet(0, subk=\"one-hundred-ten\")\n\t\t`)\n\n\t\tt.Run(\"test row keys\", func(t *testing.T) {\n\t\t\t// the execututor returns row IDs when the field has keys, so they should be included in the target.\n\t\t\texpected := []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"generalk\", RowID: 1, RowKey: \"ten\"}, {Field: \"subk\", RowID: 1, RowKey: \"one-hundred\"}}, Count: 3},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"generalk\", RowID: 1, RowKey: \"ten\"}, {Field: \"subk\", RowID: 2, RowKey: \"one-hundred-ten\"}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"generalk\", RowID: 2, RowKey: \"eleven\"}, {Field: \"subk\", RowID: 2, RowKey: \"one-hundred-ten\"}}, Count: 1},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"generalk\", RowID: 3, RowKey: \"twelve\"}, {Field: \"subk\", RowID: 2, RowKey: \"one-hundred-ten\"}}, Count: 1},\n\t\t\t}\n\n\t\t\tresults := c.Query(t, c.Idx(), `GroupBy(Rows(generalk), Rows(subk))`).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\ttest.CheckGroupBy(t, expected, results)\n\t\t})\n\n\t\t// Foreign Index\n\t\tc.CreateField(t, c.Idx(\"fip\"), pilosa.IndexOptions{Keys: true}, \"parent\")\n\t\tc.CreateField(t, c.Idx(\"fic\"), pilosa.IndexOptions{}, \"child\",\n\t\t\tpilosa.OptFieldTypeInt(0, math.MaxInt64),\n\t\t\tpilosa.OptFieldForeignIndex(c.Idx(\"fip\")),\n\t\t)\n\t\t// Set data on the parent so we have some index keys.\n\t\tc.Query(t, c.Idx(\"fip\"), `\n\t\t\tSet(\"one\", parent=1)\n\t\t\tSet(\"two\", parent=2)\n\t\t\tSet(\"three\", parent=3)\n\t\t\tSet(\"four\", parent=4)\n\t\t\tSet(\"five\", parent=5)\n\t\t`)\n\t\t// Set data on the child to align with the foreign index keys.\n\t\tc.Query(t, c.Idx(\"fic\"), `\n\t\t\tSet(1, child=\"one\")\n\t\t\tSet(2, child=\"one\")\n\t\t\tSet(3, child=\"one\")\n\t\t\tSet(4, child=\"three\")\n\t\t\tSet(5, child=\"three\")\n\t\t\tSet(6, child=\"five\")\n\t\t`)\n\n\t\tt.Run(\"test foreign index with keys\", func(t *testing.T) {\n\t\t\t// The execututor returns row IDs when the field has keys, but we\n\t\t\t// don't include them because they are not necessary in the result\n\t\t\t// comparison. Because of this, we use the CheckGroupByOnKey\n\t\t\t// function here to check equality only on the key field.\n\t\t\texpected := []pilosa.GroupCount{\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"child\", RowKey: \"one\"}}, Count: 3},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"child\", RowKey: \"three\"}}, Count: 2},\n\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"child\", RowKey: \"five\"}}, Count: 1},\n\t\t\t}\n\n\t\t\tresults := c.Query(t, c.Idx(\"fic\"), `GroupBy(Rows(child), sort=\"count desc\")`).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\ttest.CheckGroupByOnKey(t, expected, results)\n\t\t})\n\n\t\t// SUP-139: GroupBy returns incorrect results when two or more Integer Range Fields are used to define the grouping\n\t\tt.Run(\"CountByIntegersWithMinMax\", func(t *testing.T) {\n\t\t\tc.CreateField(t, c.Idx(\"cbimm\"), pilosa.IndexOptions{}, \"year\", pilosa.OptFieldTypeInt(2019, 2020))\n\t\t\tc.CreateField(t, c.Idx(\"cbimm\"), pilosa.IndexOptions{}, \"quarter\", pilosa.OptFieldTypeInt(1, 4))\n\n\t\t\tc.ImportIntID(t, c.Idx(\"cbimm\"), \"year\", []test.IntID{{ID: 1, Val: 2019}, {ID: 2, Val: 2019}, {ID: 3, Val: 2019}, {ID: 4, Val: 2019}})\n\t\t\tc.ImportIntID(t, c.Idx(\"cbimm\"), \"quarter\", []test.IntID{{ID: 1, Val: 1}, {ID: 2, Val: 1}, {ID: 3, Val: 1}, {ID: 4, Val: 2}})\n\n\t\t\tyear2019 := int64(2019)\n\t\t\tquarter1, quarter2 := int64(1), int64(2)\n\n\t\t\tresults := c.Query(t, c.Idx(\"cbimm\"), `GroupBy(Rows(year), Rows(quarter))`).Results[0].(*pilosa.GroupCounts).Groups()\n\n\t\t\ttest.CheckGroupBy(t,\n\t\t\t\t[]pilosa.GroupCount{\n\t\t\t\t\t{Group: []pilosa.FieldRow{\n\t\t\t\t\t\t{Field: \"year\", RowID: 0, Value: &year2019},\n\t\t\t\t\t\t{Field: \"quarter\", RowID: 0, Value: &quarter1},\n\t\t\t\t\t}, Count: 3},\n\t\t\t\t\t{Group: []pilosa.FieldRow{\n\t\t\t\t\t\t{Field: \"year\", RowID: 0, Value: &year2019},\n\t\t\t\t\t\t{Field: \"quarter\", RowID: 0, Value: &quarter2},\n\t\t\t\t\t}, Count: 1},\n\t\t\t\t},\n\t\t\t\tresults,\n\t\t\t)\n\t\t})\n\t\t// Create some time-quantum data:\n\t\tc.Query(t, c.Idx(), \"Set(0, tq=1, 2022-01-01T01:01)\")\n\t\tc.Query(t, c.Idx(), \"Set(1, tq=1, 2021-01-01T01:01)\")\n\t\tt.Run(\"GroupByWithTime\", func(t *testing.T) {\n\t\t\texpected := map[string][]pilosa.GroupCount{\n\t\t\t\t// no time specified\n\t\t\t\t\"GroupBy(Rows(tq), Rows(general))\": {\n\t\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"tq\", RowID: 1}, {Field: \"general\", RowID: 10}}, Count: 2},\n\t\t\t\t},\n\t\t\t\t// time specified but includes all data\n\t\t\t\t\"GroupBy(Rows(tq, from=2020-01-01T01:01), Rows(general))\": {\n\t\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"tq\", RowID: 1}, {Field: \"general\", RowID: 10}}, Count: 2},\n\t\t\t\t},\n\t\t\t\t// same but in a different order\n\t\t\t\t\"GroupBy(Rows(general), Rows(tq, from=2020-01-01T01:01))\": {\n\t\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 10}, {Field: \"tq\", RowID: 1}}, Count: 2},\n\t\t\t\t},\n\t\t\t\t// time excludes any data\n\t\t\t\t\"GroupBy(Rows(general), Rows(tq, from=2022-01-01T01:01))\": {\n\t\t\t\t\t{Group: []pilosa.FieldRow{{Field: \"general\", RowID: 10}, {Field: \"tq\", RowID: 1}}, Count: 1},\n\t\t\t\t},\n\t\t\t\t// limit excludes all data\n\t\t\t\t\"GroupBy(Rows(general), Rows(tq, from=2023-01-01T01:01))\": {},\n\t\t\t}\n\n\t\t\tfor query, want := range expected {\n\t\t\t\tresults := c.Query(t, c.Idx(), query).Results[0].(*pilosa.GroupCounts).Groups()\n\t\t\t\ttest.CheckGroupBy(t, want, results)\n\t\t\t}\n\t\t})\n\t}\n\tfor _, size := range []int{1, 3} {\n\t\tt.Run(fmt.Sprintf(\"%d_nodes\", size), func(t *testing.T) {\n\t\t\tgroupByTest(t, size)\n\t\t})\n\t}\n}\n\nfunc BenchmarkGroupBy(b *testing.B) {\n\tc := test.MustUnsharedCluster(b, 1)\n\tvar err error\n\tc.GetIdleNode(0).Config.DataDir, err = testhook.TempDir(b, \"benchmarkGroupBy-\")\n\tif err != nil {\n\t\tb.Fatalf(\"getting temp dir: %v\", err)\n\t}\n\terr = c.Start()\n\tif err != nil {\n\t\tb.Fatalf(\"starting cluster: %v\", err)\n\t}\n\tdefer c.Close()\n\tc.CreateField(b, c.Idx(), pilosa.IndexOptions{}, \"a\")\n\tc.CreateField(b, c.Idx(), pilosa.IndexOptions{}, \"b\")\n\tc.CreateField(b, c.Idx(), pilosa.IndexOptions{}, \"c\")\n\t// Set up identical representative data in 3 fields. In each row, we'll set\n\t// a certain bit pattern for 100 bits, then skip 1000 up to ShardWidth.\n\tbits := make([][2]uint64, 0)\n\tfor i := uint64(0); i < ShardWidth; i++ {\n\t\t// row 0 has 100 bit runs\n\t\tbits = append(bits, [2]uint64{0, i})\n\t\tif i%2 == 1 {\n\t\t\t// row 1 has odd bits set\n\t\t\tbits = append(bits, [2]uint64{1, i})\n\t\t}\n\t\tif i%2 == 0 {\n\t\t\t// row 2 has even bits set\n\t\t\tbits = append(bits, [2]uint64{2, i})\n\t\t}\n\t\tif i%27 == 0 {\n\t\t\t// row 3 has every 27th bit set\n\t\t\tbits = append(bits, [2]uint64{3, i})\n\t\t}\n\t\tif i%100 == 99 {\n\t\t\ti += 1000\n\t\t}\n\t}\n\tc.ImportBits(b, c.Idx(), \"a\", bits)\n\tc.ImportBits(b, c.Idx(), \"b\", bits)\n\tc.ImportBits(b, c.Idx(), \"c\", bits)\n\n\tb.Run(\"single shard group by\", func(b *testing.B) {\n\t\tb.ResetTimer()\n\t\tb.ReportAllocs()\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tc.Query(b, c.Idx(), `GroupBy(Rows(a), Rows(b), Rows(c))`)\n\t\t}\n\t})\n\n\tb.Run(\"single shard with limit\", func(b *testing.B) {\n\t\tb.ResetTimer()\n\t\tb.ReportAllocs()\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tc.Query(b, c.Idx(), `GroupBy(Rows(a), Rows(b), Rows(c), limit=4)`)\n\t\t}\n\t})\n\n\t// TODO benchmark over multiple shards\n\n\t// TODO benchmark paging over large numbers of rows\n}\n\n// NOTE: The shift function in its current state is unsupported.\n// If any of these tests fail due to improvements made to the roaring\n// code, it is reasonable to remove these tests. See the `Shift()`\n// method on `Row` in `row.go`.\nfunc TestExecutor_Execute_Shift(t *testing.T) {\n\tt.Run(\"Shift Bit 0\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\t\thldr.SetBit(c.Idx(), \"general\", 10, 0)\n\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Shift(Row(general=10), n=1)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if columns := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{1}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t}\n\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Shift(Shift(Row(general=10), n=1), n=1)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if columns := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{2}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t}\n\t})\n\n\tt.Run(\"Shift container boundary\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\t\thldr.SetBit(c.Idx(), \"general\", 10, 65535)\n\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Shift(Row(general=10), n=1)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if columns := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{65536}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t}\n\t})\n\n\tt.Run(\"Shift shard boundary\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\n\t\torig := []uint64{1, ShardWidth - 1, ShardWidth + 1}\n\t\tshift1 := []uint64{2, ShardWidth, ShardWidth + 2}\n\t\tshift2 := []uint64{3, ShardWidth + 1, ShardWidth + 3}\n\n\t\tfor _, bit := range orig {\n\t\t\thldr.SetBit(c.Idx(), \"general\", 10, bit)\n\t\t}\n\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Shift(Row(general=10), n=1)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if columns := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, shift1) {\n\t\t\tt.Fatalf(\"unexpected shift by 1: expected: %+v, but got: %+v\", shift1, columns)\n\t\t}\n\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Shift(Row(general=10), n=2)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if columns := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, shift2) {\n\t\t\tt.Fatalf(\"unexpected shift by 2: expected: %+v, but got: %+v\", shift2, columns)\n\t\t}\n\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Shift(Shift(Row(general=10)))`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if columns := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, orig) {\n\t\t\tt.Fatalf(\"unexpected shift by 0: expected: %+v, but got: %+v\", orig, columns)\n\t\t}\n\t})\n\n\tt.Run(\"Shift shard boundary no create\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\t\thldr.SetBit(c.Idx(), \"general\", 10, ShardWidth-2) // shardwidth -1\n\t\thldr.SetBit(c.Idx(), \"general\", 10, ShardWidth-1) // shardwidth\n\t\thldr.SetBit(c.Idx(), \"general\", 10, ShardWidth)   // shardwidth +1\n\t\thldr.SetBit(c.Idx(), \"general\", 10, ShardWidth+2) // shardwidth +3\n\n\t\texp := []uint64{ShardWidth - 1, ShardWidth, ShardWidth + 1, ShardWidth + 3}\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Shift(Row(general=10), n=1)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if columns := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, exp) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", columns)\n\t\t}\n\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `Shift(Shift(Row(general=10), n=1), n=1)`}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if columns := res.Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, []uint64{ShardWidth, ShardWidth + 1, ShardWidth + 2, ShardWidth + 4}) {\n\t\t\tt.Fatalf(\"unexpected columns: \\n%+v\\n%+v\", columns, exp)\n\t\t}\n\t})\n}\n\nfunc TestExecutor_Execute_IncludesColumn(t *testing.T) {\n\tt.Run(\"results-ids\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\t\thldr.SetBit(c.Idx(), \"general\", 10, 1)\n\t\thldr.SetBit(c.Idx(), \"general\", 10, ShardWidth)\n\t\thldr.SetBit(c.Idx(), \"general\", 10, 2*ShardWidth)\n\n\t\tfor i, tt := range []struct {\n\t\t\tcol         uint64\n\t\t\texpIncluded bool\n\t\t}{\n\t\t\t{1, true},\n\t\t\t{2, false},\n\t\t\t{ShardWidth, true},\n\t\t\t{ShardWidth + 1, false},\n\t\t\t{2 * ShardWidth, true},\n\t\t\t{(2 * ShardWidth) + 1, false},\n\t\t} {\n\t\t\tt.Run(fmt.Sprint(i), func(t *testing.T) {\n\t\t\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: fmt.Sprintf(\"IncludesColumn(Row(general=10), column=%d)\", tt.col)}); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t} else if tt.expIncluded && !res.Results[0].(bool) {\n\t\t\t\t\tt.Fatalf(\"expected to find column: %d\", tt.col)\n\t\t\t\t} else if !tt.expIncluded && res.Results[0].(bool) {\n\t\t\t\t\tt.Fatalf(\"did not expect to find column: %d\", tt.col)\n\t\t\t\t}\n\t\t\t})\n\t\t}\n\t})\n\tt.Run(\"results-keys\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\tcmd := c.GetNode(0)\n\t\thldr := c.GetHolder(0)\n\t\tindex := hldr.MustCreateIndexIfNotExists(c.Idx(), pilosa.IndexOptions{Keys: true})\n\t\tif _, err := index.CreateField(\"general\", \"\", pilosa.OptFieldKeys()); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif _, err := cmd.API.Query(\n\t\t\tcontext.Background(),\n\t\t\t&pilosa.QueryRequest{\n\t\t\t\tIndex: c.Idx(),\n\t\t\t\tQuery: `Set(\"one\", general=\"ten\") Set(\"eleven\", general=\"ten\") Set(\"twentyone\", general=\"ten\")`,\n\t\t\t}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tfor i, tt := range []struct {\n\t\t\tcol         string\n\t\t\texpIncluded bool\n\t\t}{\n\t\t\t{\"one\", true},\n\t\t\t{\"two\", false},\n\t\t\t{\"eleven\", true},\n\t\t\t{\"twelve\", false},\n\t\t\t{\"twentyone\", true},\n\t\t\t{\"twentytwo\", false},\n\t\t} {\n\t\t\tt.Run(fmt.Sprint(i), func(t *testing.T) {\n\t\t\t\tif res, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: fmt.Sprintf(\"IncludesColumn(Row(general=ten), column=%s)\", tt.col)}); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t} else if tt.expIncluded && !res.Results[0].(bool) {\n\t\t\t\t\tt.Fatalf(\"expected to find column: %s\", tt.col)\n\t\t\t\t} else if !tt.expIncluded && res.Results[0].(bool) {\n\t\t\t\t\tt.Fatalf(\"did not expect to find column: %s\", tt.col)\n\t\t\t\t}\n\t\t\t})\n\t\t}\n\t})\n\tt.Run(\"errors\", func(t *testing.T) {\n\t\tc := test.MustRunCluster(t, 1)\n\t\tdefer c.Close()\n\t\thldr := c.GetHolder(0)\n\t\thldr.SetBit(c.Idx(), \"general\", 10, 1)\n\n\t\tt.Run(\"no column\", func(t *testing.T) {\n\t\t\texpErr := \"IncludesColumn call must specify a column\"\n\t\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `IncludesColumn(Row(general=10))`}); err == nil {\n\t\t\t\tt.Fatalf(\"expected to get an error\")\n\t\t\t} else if !strings.Contains(err.Error(), expErr) {\n\t\t\t\tt.Fatalf(\"expected error: %s, but got: %s\", expErr, err.Error())\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"no row query\", func(t *testing.T) {\n\t\t\texpErr := \"IncludesColumn call must specify a row query\"\n\t\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `IncludesColumn(column=1)`}); err == nil {\n\t\t\t\tt.Fatalf(\"expected to get an error\")\n\t\t\t} else if !strings.Contains(err.Error(), expErr) {\n\t\t\t\tt.Fatalf(\"expected error: %s, but got: %s\", expErr, err.Error())\n\t\t\t}\n\t\t})\n\t})\n}\n\nfunc TestExecutor_Execute_MinMaxCountEqual(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\thldr := c.GetHolder(0)\n\n\tidx, err := hldr.CreateIndex(c.Idx(), \"\", pilosa.IndexOptions{})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, err := idx.CreateField(\"x\", \"\"); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, err := idx.CreateField(\"f\", \"\", pilosa.OptFieldTypeInt(-1100, 1000)); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, err := idx.CreateField(\"dec\", \"\", pilosa.OptFieldTypeDecimal(3)); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: `\n\t\t\tSet(0, f=3)\n\t\t\tSet(1, f=3)\n\t\t\tSet(2, f=4)\n\t\t\tSet(3, f=5)\n\t\t\tSet(4, f=5)\n\t\t\tSet(` + strconv.Itoa(ShardWidth+1) + `, f=3)\n\t\t\tSet(` + strconv.Itoa(ShardWidth+2) + `, f=5)\n\t\t\tSet(` + strconv.Itoa(ShardWidth+3) + `, f=5)\n\t\t\tSet(` + strconv.Itoa(ShardWidth+4) + `, f=5)\n\t\t\tSet(` + strconv.Itoa(ShardWidth+5) + `, f=4)\n\t\t\tSet(` + strconv.Itoa(2*ShardWidth+1) + `, f=3)\n\t\t\tSet(0, x=3)\n\t\t\tSet(1, x=3)\n            Set(0, dec=5.122)\n            Set(1, dec=12.985)\n            Set(2, dec=4.234)\n            Set(3, dec=12.985)\n\n\t\t`}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tt.Run(\"Min\", func(t *testing.T) {\n\t\ttests := []struct {\n\t\t\tfilter string\n\t\t\texp    int64\n\t\t\tcnt    int64\n\t\t}{\n\t\t\t{filter: ``, exp: 3, cnt: 4},\n\t\t\t{filter: `Row(x=3)`, exp: 3, cnt: 2},\n\t\t}\n\t\tfor i, tt := range tests {\n\t\t\tvar pql string\n\t\t\tif tt.filter == \"\" {\n\t\t\t\tpql = `Min(field=f)`\n\t\t\t} else {\n\t\t\t\tpql = fmt.Sprintf(`Min(%s, field=f)`, tt.filter)\n\t\t\t}\n\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{Val: tt.exp, Count: tt.cnt}) {\n\t\t\t\tt.Fatalf(\"unexpected result, test %d: %s\", i, spew.Sdump(result))\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\"MinNonExistent\", func(t *testing.T) {\n\t\tpql := `Min(field=fake)`\n\t\t_, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql})\n\t\tif err.Error() != \"executing: executeMin: mapping on primary node: field not found\" {\n\t\t\tt.Fatal(err)\n\t\t}\n\t})\n\tt.Run(\"MaxNonExistent\", func(t *testing.T) {\n\t\tpql := `Max(field=fake)`\n\t\t_, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql})\n\t\tif err.Error() != \"executing: executeMax: mapping on primary node: field not found\" {\n\t\t\tt.Fatal(err)\n\t\t}\n\t})\n\tt.Run(\"MinDec\", func(t *testing.T) {\n\t\ttests := []struct {\n\t\t\tfilter string\n\t\t\texp    pql.Decimal\n\t\t\tcnt    int64\n\t\t}{\n\t\t\t{filter: ``, exp: pql.NewDecimal(4234, 3), cnt: 1},\n\t\t\t{filter: `Row(x=3)`, exp: pql.NewDecimal(5122, 3), cnt: 1},\n\t\t}\n\t\tfor i, tt := range tests {\n\t\t\tvar pql string\n\t\t\tif tt.filter == \"\" {\n\t\t\t\tpql = `Min(field=dec)`\n\t\t\t} else {\n\t\t\t\tpql = fmt.Sprintf(`Min(%s, field=dec)`, tt.filter)\n\t\t\t}\n\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{DecimalVal: &tt.exp, Count: tt.cnt}) {\n\t\t\t\tt.Fatalf(\"unexpected result, test %d: %s\", i, spew.Sdump(result.Results[0]))\n\t\t\t}\n\t\t}\n\t})\n\n\tt.Run(\"Max\", func(t *testing.T) {\n\t\ttests := []struct {\n\t\t\tfilter string\n\t\t\texp    int64\n\t\t\tcnt    int64\n\t\t}{\n\t\t\t{filter: ``, exp: 5, cnt: 5},\n\t\t}\n\t\tfor i, tt := range tests {\n\t\t\tvar pql string\n\t\t\tif tt.filter == \"\" {\n\t\t\t\tpql = `Max(field=f)`\n\t\t\t} else {\n\t\t\t\tpql = fmt.Sprintf(`Max(%s, field=f)`, tt.filter)\n\t\t\t}\n\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{Val: tt.exp, Count: tt.cnt}) {\n\t\t\t\tt.Fatalf(\"unexpected result, test %d: %s\", i, spew.Sdump(result))\n\t\t\t}\n\t\t}\n\t})\n\n\tt.Run(\"MaxDec\", func(t *testing.T) {\n\t\ttests := []struct {\n\t\t\tfilter string\n\t\t\texp    pql.Decimal\n\t\t\tcnt    int64\n\t\t}{\n\t\t\t{filter: ``, exp: pql.NewDecimal(12985, 3), cnt: 2},\n\t\t\t{filter: `Row(x=3)`, exp: pql.NewDecimal(12985, 3), cnt: 1},\n\t\t}\n\t\tfor i, tt := range tests {\n\t\t\tvar pql string\n\t\t\tif tt.filter == \"\" {\n\t\t\t\tpql = `Max(field=dec)`\n\t\t\t} else {\n\t\t\t\tpql = fmt.Sprintf(`Max(%s, field=dec)`, tt.filter)\n\t\t\t}\n\t\t\tif result, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if !reflect.DeepEqual(result.Results[0], pilosa.ValCount{DecimalVal: &tt.exp, Count: tt.cnt}) {\n\t\t\t\tt.Fatalf(\"unexpected result, test %d: %s\", i, spew.Sdump(result.Results[0]))\n\t\t\t}\n\t\t}\n\t})\n\n\tt.Run(\"MinMaxRangeError\", func(t *testing.T) {\n\t\t// Min\n\t\tpql := `Set(4, dec=-92233720368547758.08)`\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err == nil {\n\t\t\tt.Fatalf(\"expected error but got: nil\")\n\t\t} else if errors.Cause(err) != pilosa.ErrDecimalOutOfRange {\n\t\t\tt.Fatalf(\"expected error: %s, but got: %s\", pilosa.ErrDecimalOutOfRange, err)\n\t\t}\n\t\t// Max\n\t\tpql = `Set(4, dec=92233720368547758.07)`\n\t\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: c.Idx(), Query: pql}); err == nil {\n\t\t\tt.Fatalf(\"expected error but got: nil\")\n\t\t} else if errors.Cause(err) != pilosa.ErrDecimalOutOfRange {\n\t\t\tt.Fatalf(\"expected error: %s, but got: %s\", pilosa.ErrDecimalOutOfRange, err)\n\t\t}\n\t})\n}\n\nfunc TestExecutor_Execute_NoIndex(t *testing.T) {\n\tt.Helper()\n\tindexOptions := &pilosa.IndexOptions{}\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\thldr := c.GetHolder(0)\n\tindex := hldr.MustCreateIndexIfNotExists(c.Idx(), *indexOptions)\n\t_, err := index.CreateField(\"f\", \"\")\n\tif err != nil {\n\t\tt.Fatal(\"should work\")\n\t}\n\n\tif _, err := c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{\n\t\tIndex: c.Idx(),\n\t\tQuery: \"Count(Distinct(Row(gpu_tag='GTX'), index=systems, field=jarvis_id))\",\n\t}); errors.Cause(err) != pilosa.ErrIndexNotFound {\n\t\tt.Fatal(\"expecting error: 'index systems does not exist'\")\n\t}\n}\n\nfunc TestExecutor_Execute_CountDistinct(t *testing.T) {\n\t// This schema has indexes named e, p, and s. We can then\n\t// use c.Idx(e) or Sprintf(%e, idx) to match these names up.\n\tdata, err := os.ReadFile(\"testdata/schema.json\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tc := test.MustRunCluster(t, 1)\n\n\tdefer c.Close()\n\tapi := c.GetNode(0).API\n\n\tschema := &pilosa.Schema{}\n\tif err := json.NewDecoder(bytes.NewReader(data)).Decode(schema); err != nil {\n\t\tt.Fatal(err)\n\t}\n\t// convert index names to be test-specific\n\tfor i, idx := range schema.Indexes {\n\t\tschema.Indexes[i].Name = c.Idx(idx.Name)\n\t}\n\n\tif err := api.ApplySchema(context.TODO(), schema, false); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// AntitodePoint == row 1 b/c keys field.\n\t// Note: type=TwoPoints should match 100/101, and no row in type\n\t// matches 102, but 102 is present in equip_id at all.\n\twriteQuery := `\n\t\tSet(100, type=AntidotePoint)\n\t\tSet(100, type=TwoPoints)\n\t\tSet(101, type=TwoPoints)\n\t\tSet(100, equip_id=100)\n\t\tSet(101, equip_id=101)\n\t\tSet(102, equip_id=102)\n\t\tSet(100, site_id=100)\n\t\tSet(100, id=100)\n\t`\n\tfor k, i := range schema.Indexes {\n\t\t_ = k\n\t\tif _, err := api.Query(context.TODO(), &pilosa.QueryRequest{Index: i.Name, Query: writeQuery}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\tsites := c.Idx(\"s\")\n\n\t// test query - Distinct of Distincts\n\tpql := fmt.Sprintf(`Distinct(\n\t\tIntersect(\n\t\t\tDistinct(\n\t\t\t\tIntersect(Row(type=AntidotePoint)),\n\t\t\tindex=%e, field=equip_id),\n\t\t\tDistinct(\n\t\t\t\tIntersect(Row(type=TwoPoints)),\n\t\t\tindex=%s, field=equip_id)\n\t\t), index=%t, field=site_id)`, c, c, c)\n\n\t// Check if test query gives correct results (one column 100)\n\tt.Run(\"Distinct\", func(t *testing.T) {\n\t\tresp, err := api.Query(context.TODO(), &pilosa.QueryRequest{\n\t\t\tIndex: sites,\n\t\t\tQuery: pql,\n\t\t})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tr, ok := resp.Results[0].(pilosa.SignedRow)\n\t\tif !ok {\n\t\t\tt.Fatalf(\"invalid response type, expected: pilosa.SignedRow, got: %T\", resp.Results[0])\n\t\t}\n\t\tif r.Pos.Count() != 1 {\n\t\t\tt.Fatalf(\"invalid pilosa.SignedRow.Pos.Count, expected: 1, got: %v\", r.Pos.Count())\n\t\t}\n\t\tif r.Pos.Columns()[0] != 100 {\n\t\t\tt.Fatalf(\"invalid pilosa.SignedRow.Pos.Columns, expected: [100], got: %v\", r.Pos.Columns())\n\t\t}\n\t})\n\n\t// Following tests check if wrapping Distinct of Distincts query by Count and GroupBy\n\t// is fixed and does not give an error: 'unknown call: Distinct' error.\n\n\t// Check if Count on test query gives correct, exactly 1 result\n\tt.Run(\"Count(Distinct)\", func(t *testing.T) {\n\t\tresp, err := api.Query(context.TODO(), &pilosa.QueryRequest{\n\t\t\tIndex: sites,\n\t\t\tQuery: fmt.Sprintf(\"Count(%s)\", pql),\n\t\t})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tcnt, ok := resp.Results[0].(uint64)\n\t\tif !ok {\n\t\t\tt.Fatalf(\"invalid response type, expected: uint64, got: %T\", resp.Results[0])\n\t\t}\n\t\tif cnt != 1 {\n\t\t\tt.Fatalf(\"invalid result, expected: 1, got: %v\", cnt)\n\t\t}\n\t})\n\n\t// Check if GroupBy on test query gives correct, exactly 1 result\n\tt.Run(\"GroupBy(Distinct)\", func(t *testing.T) {\n\t\tresp, err := api.Query(context.TODO(), &pilosa.QueryRequest{\n\t\t\tIndex: sites,\n\t\t\tQuery: fmt.Sprintf(\"GroupBy(Rows(type), filter=%s)\", pql),\n\t\t})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tgcc, ok := resp.Results[0].(*pilosa.GroupCounts)\n\t\tif !ok {\n\t\t\tt.Fatalf(\"invalid response type, expected: []pilosa.GroupCount, got: %T\", resp.Results[0])\n\t\t}\n\t\tgc := gcc.Groups()\n\t\tif len(gc) != 2 {\n\t\t\tt.Fatalf(\"invalid group count length, expected: 2, got: %v\", len(gc))\n\t\t}\n\t\tif gc[0].Count != 1 {\n\t\t\tt.Fatalf(\"invalid group-by count for %d, expected: 1, got: %v\", gc[0].Group[0].RowID, gc[0].Count)\n\t\t}\n\t\tif gc[1].Count != 1 {\n\t\t\tt.Fatalf(\"invalid group-by count for %d, expected: 1, got: %v\", gc[1].Group[0].RowID, gc[1].Count)\n\t\t}\n\t})\n\tt.Run(\"Store(Distinct)\", func(t *testing.T) {\n\t\t_, err = api.Query(context.TODO(), &pilosa.QueryRequest{\n\t\t\tIndex: sites,\n\t\t\tQuery: `Store(Distinct(field=equip_id), type=\"a\")`,\n\t\t})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tresp, err := api.Query(context.TODO(), &pilosa.QueryRequest{\n\t\t\tIndex: sites,\n\t\t\tQuery: `Row(type=\"a\")`,\n\t\t})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tres, ok := resp.Results[0].(*pilosa.Row)\n\t\tif !ok {\n\t\t\tt.Fatalf(\"invalid response type, expected: *pilosa.Row, got: %T\", resp.Results[0])\n\t\t}\n\t\tcols := res.Columns()\n\t\tif !eq(cols, []uint64{100, 101, 102}) {\n\t\t\tt.Fatalf(\"expected [100, 101, 102], got %d\", cols)\n\t\t}\n\n\t\t_, err = api.Query(context.TODO(), &pilosa.QueryRequest{\n\t\t\tIndex: sites,\n\t\t\tQuery: `Store(Distinct(Row(type=\"TwoPoints\"), field=equip_id), type=\"b\")`,\n\t\t})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tresp, err = api.Query(context.TODO(), &pilosa.QueryRequest{\n\t\t\tIndex: sites,\n\t\t\tQuery: `Row(type=\"b\")`,\n\t\t})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tres, ok = resp.Results[0].(*pilosa.Row)\n\t\tif !ok {\n\t\t\tt.Fatalf(\"invalid response type, expected: *pilosa.Row, got: %T\", resp.Results[0])\n\t\t}\n\t\tcols = res.Columns()\n\t\tif !eq(cols, []uint64{100, 101}) {\n\t\t\tt.Fatalf(\"expected [100, 101], got %d\", cols)\n\t\t}\n\t})\n}\n\nfunc variousQueriesCountDistinctTimestamp(t *testing.T, c *test.Cluster) {\n\tindex := c.Idx(\"tsidx\")\n\tfield := \"ts\"\n\n\t// create an index and timestamp field\n\tc.CreateField(t, index, pilosa.IndexOptions{TrackExistence: true}, field, pilosa.OptFieldTypeTimestamp(time.Unix(0, 0), \"s\"))\n\tc.CreateField(t, index, pilosa.IndexOptions{TrackExistence: true}, \"set\")\n\n\t// add some data\n\tdata := []string{\"2010-01-02T12:32:00Z\", \"2010-04-20T12:32:00Z\", \"2011-04-20T12:59:00Z\", \"2011-04-20T12:40:00Z\", \"2011-04-20T12:32:00Z\"}\n\n\tfor i, datum := range data {\n\t\tc.Query(t, index, fmt.Sprintf(\"Set(%d, ts=\\\"%s\\\")\", i*ShardWidth, datum))\n\t}\n\t// set something in shard 8 so there's a shard present with no timestamp data\n\tc.Query(t, index, fmt.Sprintf(\"Set(%d, set=0)\", 8*ShardWidth))\n\n\t// query the Count of Distinct vals in field ts\n\tcount := c.Query(t, index, \"Count(Distinct(field=ts))\").Results[0]\n\tif count != uint64(len(data)) {\n\t\tt.Fatalf(\"expected %v got %v\", len(data), count)\n\t}\n\n\t// query the ones that are in or after 2011, expecting 3. this helps us\n\t// hit an edge case that only happens if you have no data *because of\n\t// a filter*.\n\tcount = c.Query(t, index, \"Count(Distinct(Row(ts > \\\"2011-01-01T00:00:00Z\\\"), field=ts))\").Results[0]\n\tif count != uint64(3) {\n\t\tt.Fatalf(\"expected %v got %v\", 3, count)\n\t}\n}\n\n// Ensure that a top-level, bare distinct on multiple nodes\n// is handled correctly.\nfunc TestExecutor_BareDistinct(t *testing.T) {\n\tt.Helper()\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\t// build a name that will match %c\n\tindexName := c.Idx(\"c\")\n\n\tc.CreateField(t, indexName, pilosa.IndexOptions{}, \"ints\",\n\t\tpilosa.OptFieldTypeInt(0, math.MaxInt64),\n\t)\n\tc.CreateField(t, indexName, pilosa.IndexOptions{}, \"filter\")\n\n\t// Populate integer data.\n\tc.Query(t, indexName, fmt.Sprintf(`\n\t\t\tSet(0, ints=1)\n\t\t\tSet(%d, ints=2)\n\t\t`, ShardWidth))\n\tc.Query(t, indexName, fmt.Sprintf(`\n\t\t\t Set(0, filter=1)\n\t\t\t Set(%d, filter=1)\n\t        `, 65537))\n\n\tfor _, pql := range []string{\n\t\t`Distinct(field=\"ints\")`,\n\t\tfmt.Sprintf(`Distinct(index=%c, field=\"ints\")`, c),\n\t} {\n\t\texp := []uint64{1, 2}\n\t\tres := c.Query(t, indexName, pql).Results[0].(pilosa.SignedRow)\n\t\tif got := res.Pos.Columns(); !reflect.DeepEqual(exp, got) {\n\t\t\tt.Fatalf(\"expected: %v, but got: %v\", exp, got)\n\t\t}\n\t}\n}\n\nfunc TestExecutor_Execute_TopNDistinct(t *testing.T) {\n\tdata, err := os.ReadFile(\"testdata/schema.json\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tc := test.MustRunCluster(t, 1)\n\n\tdefer c.Close()\n\tapi := c.GetNode(0).API\n\n\tschema := &pilosa.Schema{}\n\tif err := json.NewDecoder(bytes.NewReader(data)).Decode(schema); err != nil {\n\t\tt.Fatal(err)\n\t}\n\t// convert index names to be test-specific\n\tfor i, idx := range schema.Indexes {\n\t\tschema.Indexes[i].Name = c.Idx(idx.Name)\n\t}\n\n\tif err := api.ApplySchema(context.TODO(), schema, false); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\twriteQuery := `Set(100, type=AntidotePoint)Set(100, equip_id=100)Set(100, site_id=100)Set(100, id=100)`\n\tfor _, i := range schema.Indexes {\n\t\tif _, err := api.Query(context.TODO(), &pilosa.QueryRequest{Index: i.Name, Query: writeQuery}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tpql := fmt.Sprintf(`TopN(type, Distinct(Row(type=AntidotePoint), index=%s, field=equip_id))`, c)\n\n\t// Check if test query gives correct results (one column 100)\n\tt.Run(\"TopN\", func(t *testing.T) {\n\t\tresp, err := api.Query(context.TODO(), &pilosa.QueryRequest{\n\t\t\tIndex: c.Idx(\"e\"),\n\t\t\tQuery: pql,\n\t\t})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tpf, ok := resp.Results[0].(*pilosa.PairsField)\n\t\tif !ok {\n\t\t\tt.Fatalf(\"invalid response type, expected: *pilosa.PairsField, got: %T\", resp.Results[0])\n\t\t}\n\t\tif len(pf.Pairs) != 1 {\n\t\t\tt.Fatalf(\"invalid Pairs length, expected: 1, got: %v\", len(pf.Pairs))\n\t\t}\n\t\tif pf.Pairs[0].Count != 1 {\n\t\t\tt.Fatalf(\"invalid Pairs count, expected: 1, got: %v\", pf.Pairs[0].Count)\n\t\t}\n\t})\n}\n\nfunc Test_Executor_Execute_UnionRows(t *testing.T) {\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{}, \"s\",\n\t\tpilosa.OptFieldTypeSet(pilosa.CacheTypeRanked, 50000),\n\t)\n\n\t// Populate data.\n\tc.Query(t, c.Idx(), `\n\t\t\tSet(0, s=1)\n\t\t\tSet(1, s=2)\n\t\t\tSet(2, s=3)\n\t\t\tSet(3, s=1)\n\t\t\tSet(3, s=5)\n\t\t`)\n\n\tif res := c.Query(t, c.Idx(), `Count(UnionRows(TopN(s, n=1)))`); res.Results[0] != uint64(2) {\n\t\tt.Errorf(\"expected 2 columns, got %v\", res.Results[0])\n\t}\n\tif res := c.Query(t, c.Idx(), `Count(UnionRows(Rows(s)))`); res.Results[0] != uint64(4) {\n\t\tt.Errorf(\"expected 4 columns, got %v\", res.Results[0])\n\t}\n}\n\nfunc TestTimelessClearRegression(t *testing.T) {\n\tdata, err := os.ReadFile(\"testdata/timeRegressionSchema.json\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\n\tapi := c.GetNode(0).API\n\n\tschema := &pilosa.Schema{}\n\tif err := json.NewDecoder(bytes.NewReader(data)).Decode(schema); err != nil {\n\t\tt.Fatal(err)\n\t}\n\t// convert index names to be test-specific\n\tfor i, idx := range schema.Indexes {\n\t\tschema.Indexes[i].Name = c.Idx(idx.Name)\n\t}\n\tif err := api.ApplySchema(context.TODO(), schema, false); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tidxName := schema.Indexes[0].Name\n\n\tsetQuery := `Set(511, stargazer=376)`\n\tif _, err := api.Query(context.TODO(), &pilosa.QueryRequest{Index: idxName, Query: setQuery}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tsetQuery = `Set(512, stargazer=300, 2017-05-18T00:00)`\n\tif _, err := api.Query(context.TODO(), &pilosa.QueryRequest{Index: idxName, Query: setQuery}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tclearQuery := `Clear(511, stargazer=376)`\n\tif res, err := api.Query(context.TODO(), &pilosa.QueryRequest{Index: idxName, Query: clearQuery}); err != nil {\n\t\tt.Fatal(err)\n\t} else if res.Results[0] != true {\n\t\tt.Fatal(\"clear supposedly failed\")\n\t}\n}\n\nfunc TestMissingKeyRegression(t *testing.T) {\n\t// this used to be explicitly roaring backend... I'm not sure\n\t// whether it is a useful test in post-roaring world.\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"f\", pilosa.OptFieldKeys())\n\n\ttests := []struct {\n\t\tname     string\n\t\tquery    string\n\t\texpected []interface{}\n\t}{\n\t\t{\n\t\t\tname:     \"RowGarbage\",\n\t\t\tquery:    `Row(f=\"garbage\")`,\n\t\t\texpected: []interface{}{[]string(nil)},\n\t\t},\n\t\t{\n\t\t\tname:     \"Set\",\n\t\t\tquery:    `Set(\"a\", f=\"example\")`,\n\t\t\texpected: []interface{}{true},\n\t\t},\n\t\t{\n\t\t\tname:     \"Count\",\n\t\t\tquery:    `Count(Row(f=\"example\"))`,\n\t\t\texpected: []interface{}{uint64(1)},\n\t\t},\n\t\t{\n\t\t\tname:     \"NotGarbage\",\n\t\t\tquery:    `Not(Row(f=\"garbage\"))`,\n\t\t\texpected: []interface{}{[]string{\"a\"}},\n\t\t},\n\t\t{\n\t\t\tname:     \"DifferenceGarbage\",\n\t\t\tquery:    `Difference(All(), Row(f=\"garbage\"))`,\n\t\t\texpected: []interface{}{[]string{\"a\"}},\n\t\t},\n\t\t{\n\t\t\tname: \"SetAndCount\",\n\t\t\tquery: `Set(\"b\", f=\"boo\")` + \"\\n\" +\n\t\t\t\t`Count(Row(f=\"boo\"))`,\n\t\t\texpected: []interface{}{true, uint64(1)},\n\t\t},\n\t\t{\n\t\t\tname:     \"CountNothing\",\n\t\t\tquery:    `Count(Row(f=\"garbage\"))`,\n\t\t\texpected: []interface{}{uint64(0)},\n\t\t},\n\t\t{\n\t\t\tname:     \"StoreInvertSelf\",\n\t\t\tquery:    `Store(Not(Row(f=\"xyzzy\")), f=\"xyzzy\")`,\n\t\t\texpected: []interface{}{true},\n\t\t},\n\t\t{\n\t\t\tname: \"SetClear\",\n\t\t\tquery: `Set(\"b\", f=\"plugh\")` + \"\\n\" +\n\t\t\t\t`Clear(\"b\", f=\"plugh\")`,\n\t\t\texpected: []interface{}{true, true},\n\t\t},\n\t\t{\n\t\t\tname: \"ClearMix\",\n\t\t\tquery: `Clear(\"a\", f=\"garbage\")` + \"\\n\" +\n\t\t\t\t`Clear(\"a\", f=\"example\")`,\n\t\t\texpected: []interface{}{false, true},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tresp := c.Query(t, c.Idx(), tc.query)\n\t\t\tif len(resp.Results) != len(tc.expected) {\n\t\t\t\tt.Errorf(\"expected %d results but got %d\", len(resp.Results), len(tc.expected))\n\t\t\t\treturn\n\t\t\t}\n\t\t\tfor i, r := range resp.Results {\n\t\t\t\tif row, ok := r.(*pilosa.Row); ok {\n\t\t\t\t\tr = row.Keys\n\t\t\t\t}\n\t\t\t\texpect := tc.expected[i]\n\t\t\t\tif !reflect.DeepEqual(r, expect) {\n\t\t\t\t\tt.Errorf(\"result %d differs: expected %v but got %v\", i, expect, r)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\n// TestVariousQueries has originally been written to test out a\n// variety of scenarios with Distinct, but it's structure is more\n// general purpose. My vision is to eventually have any test which\n// needs to test a single query be in here, and have a robust enough\n// test data set loaded at the start which covers what we want to\n// test.\n//\n// I'd also like to have it automatically run a matrix of scenarios\n// (single and multi-node clusters, different endpoints for the\n// queries (HTTP, GRPC, Postgres), etc.).\nfunc TestVariousQueries(t *testing.T) {\n\tfor _, clusterSize := range []int{1, 3, 5} {\n\t\tclusterSize := clusterSize\n\t\t// the VariousQueries tests should be able to run in parallel with each other.\n\t\tt.Run(fmt.Sprintf(\"%d-node\", clusterSize), func(t *testing.T) {\n\t\t\t// Unshared because we want to do the backup tests against these, which means we\n\t\t\t// don't want them to have other indexes.\n\t\t\tt.Parallel()\n\t\t\tc := test.MustRunUnsharedCluster(t, clusterSize)\n\t\t\tdefer c.Close()\n\n\t\t\t// put a variety of data into the cluster\n\t\t\tpopulateTestData(t, c)\n\t\t\tt.Run(\"backup-users\", func(t *testing.T) {\n\t\t\t\tbackupTest(t, c, usersIndex)\n\t\t\t})\n\n\t\t\tvariousQueries(t, c)\n\t\t\tvariousQueriesOnTimeFields(t, c)\n\t\t\tvariousQueriesOnPercentiles(t, c)\n\t\t\tvariousQueriesCountDistinctTimestamp(t, c)\n\t\t\tvariousQueriesOnIntFields(t, c)\n\t\t\tvariousQueriesOnTimestampFields(t, c)\n\t\t\tvariousQueriesOnLargeEpoch(t, c)\n\t\t\tt.Run(\"backup-full\", func(t *testing.T) {\n\t\t\t\tbackupTest(t, c, \"\") // test backup/restore of all indexes\n\t\t\t})\n\t\t\tt.Run(\"backuptar-full\", func(t *testing.T) {\n\t\t\t\tbackupTarTest(t, c, \"\") // test backup/restore of all indexes\n\t\t\t})\n\t\t})\n\t}\n}\n\nfunc backupTest(t *testing.T, c *test.Cluster, index string) {\n\t// should this really be in executor? No. But all these\n\t// integration-y query tests probably shouldn't be either. My goal\n\t// putting this here is to take advantage of already-existing\n\t// clusters and data.\n\tsum := chkSumCluster(t, c)\n\n\tbackupDir := backupCluster(t, c, index)\n\n\tcnew := test.MustRunUnsharedCluster(t, 3) // this way we test 1->3 3->3 5->3\n\tdefer cnew.Close()\n\n\trestoreCluster(t, backupDir, cnew)\n\n\tsumNew := chkSumCluster(t, cnew)\n\n\tif sum != sumNew {\n\t\tt.Fatalf(\"old/new checksum mismatch, old:\\n%s\\nnew:\\n%s\", sum, sumNew)\n\t}\n}\n\nfunc backupTarTest(t *testing.T, c *test.Cluster, index string) {\n\t// should this really be in executor? No. But all these\n\t// integration-y query tests probably shouldn't be either. My goal\n\t// putting this here is to take advantage of already-existing\n\t// clusters and data.\n\tsum := chkSumCluster(t, c)\n\n\tbackupDir := backupClusterTar(t, c, index)\n\n\tcnew := test.MustRunUnsharedCluster(t, 3) // this way we test 1->3 3->3 5->3\n\tdefer cnew.Close()\n\n\trestoreClusterTar(t, backupDir, cnew)\n\n\tsumNew := chkSumCluster(t, cnew)\n\n\tif sum != sumNew {\n\t\tt.Fatalf(\"old/new checksum mismatch, old:\\n%s\\nnew:\\n%s\", sum, sumNew)\n\t}\n}\n\nfunc chkSumCluster(t *testing.T, c *test.Cluster) string {\n\tt.Helper()\n\terrBuf := &bytes.Buffer{}\n\toutBuf := &bytes.Buffer{}\n\tchkSumLog := logger.NewStandardLogger(errBuf)\n\tchkSum := ctl.NewChkSumCommand(chkSumLog, outBuf)\n\tchkSum.Host = c.Nodes[len(c.Nodes)-1].URL()\n\tif err := chkSum.Run(context.Background()); err != nil {\n\t\tt.Fatalf(\"running checksum: %v\", err)\n\t}\n\n\treturn outBuf.String()\n}\n\nfunc backupCluster(t *testing.T, c *test.Cluster, index string) (backupDir string) {\n\ttd, err := testhook.TempDir(t, \"backupTest\")\n\tif err != nil {\n\t\tt.Fatalf(\"can't even get a temp dir, what a ripoff: %v\", err)\n\t}\n\ttd = td + \"/backupTest\"\n\n\tbuf := &bytes.Buffer{}\n\tbackupLog := logger.NewStandardLogger(buf)\n\n\tbackupCommand := ctl.NewBackupCommand(backupLog)\n\tbackupCommand.Host = c.Nodes[len(c.Nodes)-1].URL() // don't pick node 0 so we don't always get primary (better code coverage)\n\tbackupCommand.Index = index\n\tbackupCommand.OutputDir = td\n\n\tif err := backupCommand.Run(context.Background()); err != nil {\n\t\tt.Log(buf.String())\n\t\tt.Fatalf(\"running backup: %v\", err)\n\t}\n\treturn td\n}\n\nfunc restoreCluster(t *testing.T, backupDir string, c *test.Cluster) {\n\tbuf := &bytes.Buffer{}\n\trestoreLog := logger.NewStandardLogger(buf)\n\trestore := ctl.NewRestoreCommand(restoreLog)\n\trestore.Host = c.Nodes[len(c.Nodes)-1].URL()\n\trestore.Path = backupDir\n\tif err := restore.Run(context.Background()); err != nil {\n\t\tt.Fatalf(\"restoring: %v\", err)\n\t}\n}\n\nfunc backupClusterTar(t *testing.T, c *test.Cluster, index string) (backupFileName string) {\n\ttd, err := testhook.TempDir(t, \"backupTestTar\")\n\tif err != nil {\n\t\tt.Fatalf(\"can't even get a temp dir, what a ripoff: %v\", err)\n\t}\n\ttd = td + \"/backupTestTar.tar\"\n\n\tbuf := &bytes.Buffer{}\n\t// backupLog := logger.NewStandardLogger(buf)\n\n\tbackupTarCommand := ctl.NewBackupTarCommand(buf)\n\tbackupTarCommand.Host = c.Nodes[len(c.Nodes)-1].URL() // don't pick node 0 so we don't always get primary (better code coverage)\n\tbackupTarCommand.Index = index\n\tbackupTarCommand.OutputPath = td\n\n\tif err := backupTarCommand.Run(context.Background()); err != nil {\n\t\tt.Log(buf.String())\n\t\tt.Fatalf(\"running backuptar: %v\", err)\n\t}\n\treturn td\n}\n\nfunc restoreClusterTar(t *testing.T, backupFileDir string, c *test.Cluster) {\n\tbuf := &bytes.Buffer{}\n\trestoreLog := logger.NewStandardLogger(buf)\n\trestore := ctl.NewRestoreTarCommand(restoreLog)\n\trestore.Host = c.Nodes[len(c.Nodes)-1].URL()\n\trestore.Path = backupFileDir\n\tif err := restore.Run(context.Background()); err != nil {\n\t\tt.Fatalf(\"restoring: %v\", err)\n\t}\n}\n\n// tests for abbreviating time values in queries\nfunc variousQueriesOnPercentiles(t *testing.T, c *test.Cluster) {\n\t// todo, make rand more random, 42 isnt the answer to everything\n\t// however, to make tests reproducible, seed should be printed\n\t// on failure?\n\tr := rand.New(rand.NewSource(42))\n\n\t// gen Numbers to test percentile query on, shuffle for extra spice\n\t// size should always be greater than 0\n\ttype testValue struct {\n\t\tcolKey string\n\t\tnum    int64\n\t\trowKey string\n\t}\n\tsize := 100\n\n\ttestValues := make([]testValue, size)\n\trowKeys := [2]string{\"foo\", \"bar\"}\n\tfor i := 0; i < size; i++ {\n\t\tnum := int64(r.Uint32())\n\t\t// flip coin to negate\n\t\tif r.Uint64()%2 == 0 {\n\t\t\tnum = -num\n\t\t}\n\t\ttestValues[i] = testValue{\n\t\t\tcolKey: fmt.Sprintf(\"user%d\", i+1),\n\t\t\tnum:    num,\n\t\t\trowKey: rowKeys[r.Uint64()%2], // flip a coin\n\t\t}\n\t}\n\n\t// filter out nums that fulfil predicate\n\tvar nums []int64\n\tfor _, v := range testValues {\n\t\tif v.rowKey == \"foo\" {\n\t\t\tnums = append(nums, v.num)\n\t\t}\n\t}\n\n\t// get min and max for calculating both expected median\n\t// and bounds for bsi field\n\t// get min & max\n\n\t// helper function for calculating percentiles to\n\t// cross-check with Pilosa's results\n\tgetExpectedPercentile := func(nums []int64, nth float64) int64 {\n\t\tmin, max := nums[0], nums[0]\n\t\tfor _, num := range nums {\n\t\t\tif num < min {\n\t\t\t\tmin = num\n\t\t\t}\n\t\t\tif num > max {\n\t\t\t\tmax = num\n\t\t\t}\n\t\t}\n\t\tif nth == 0.0 {\n\t\t\treturn min\n\t\t}\n\t\tif nth == 100.0 {\n\t\t\treturn max\n\t\t}\n\t\tpossibleNthVal := int64(0)\n\t\tdesiredLess := int((float64(len(nums)) * nth) / 100.0)\n\t\tdesiredGreater := int((float64(len(nums)) * (100 - nth)) / 100.0)\n\t\tif desiredLess == 0 {\n\t\t\treturn min\n\t\t}\n\t\tif desiredGreater == 0 {\n\t\t\treturn max\n\t\t}\n\t\t// bin search\n\t\tfor min < max {\n\t\t\tpossibleNthVal = ((max / 2) + (min / 2)) + (((max % 2) + (min % 2)) / 2)\n\t\t\tleftCount, rightCount := 0, 0\n\t\t\tfor _, num := range nums {\n\t\t\t\tif num < possibleNthVal {\n\t\t\t\t\tleftCount++\n\t\t\t\t} else if num > possibleNthVal {\n\t\t\t\t\trightCount++\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif leftCount > desiredLess {\n\t\t\t\tmax = possibleNthVal - 1\n\t\t\t} else if rightCount > desiredGreater {\n\t\t\t\tmin = possibleNthVal + 1\n\t\t\t} else { // perfectly balanced, as all things should be\n\t\t\t\treturn possibleNthVal\n\t\t\t}\n\t\t}\n\t\treturn min\n\t}\n\n\t// generate numeric entries for index\n\tintEntries := make([]test.IntKey, size)\n\tfor i := 0; i < size; i++ {\n\t\tkey := testValues[i].colKey\n\t\tval := testValues[i].num\n\t\tintEntries[i] = test.IntKey{Key: key, Val: val}\n\t}\n\n\t// generate string-set entries for index\n\tstringEntries := make([][2]string, 0, len(testValues))\n\tfor _, v := range testValues {\n\t\tstringEntries = append(stringEntries,\n\t\t\t[2]string{v.rowKey, v.colKey})\n\t}\n\n\t// get min max for bsi bounds\n\tmin, max := testValues[0].num, testValues[0].num\n\tfor _, v := range testValues {\n\t\tif v.num < min {\n\t\t\tmin = v.num\n\t\t}\n\t\tif v.num > max {\n\t\t\tmax = v.num\n\t\t}\n\t}\n\n\t// generic index\n\tc.CreateField(t, \"users2\", pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"net_worth\", pilosa.OptFieldTypeInt(min, max))\n\tc.ImportIntKey(t, \"users2\", \"net_worth\", intEntries)\n\n\tc.CreateField(t, \"users2\", pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"val\", pilosa.OptFieldKeys())\n\tc.ImportKeyKey(t, \"users2\", \"val\", stringEntries)\n\n\tsplitSortBackToCSV := func(csvStr string) string {\n\t\tss := strings.Split(csvStr[:len(csvStr)-1], \"\\n\")\n\t\tsort.Strings(ss)\n\t\treturn strings.Join(ss, \"\\n\") + \"\\n\"\n\t}\n\n\ttype testCase struct {\n\t\tquery       string\n\t\tcsvVerifier string\n\t}\n\n\t// generate test cases per each nth argument\n\tnthsFloat := []float64{0, 10, 25, 50, 75, 90, 99}\n\ttests := make([]testCase, 0, len(nthsFloat))\n\tfor _, nth := range nthsFloat {\n\t\tquery := fmt.Sprintf(`Percentile(field=\"net_worth\", filter=Row(val=\"foo\"), nth=%f)`, nth)\n\t\texpectedPercentile := getExpectedPercentile(nums, nth)\n\t\ttests = append(tests, testCase{\n\t\t\tquery:       query,\n\t\t\tcsvVerifier: fmt.Sprintf(\"%d,1\\n\", expectedPercentile),\n\t\t})\n\t}\n\tnthsInt := []int64{0, 10, 100}\n\tfor _, nth := range nthsInt {\n\t\tquery := fmt.Sprintf(`Percentile(field=\"net_worth\", filter=Row(val=\"foo\"), nth=%d)`, nth)\n\t\texpectedPercentile := getExpectedPercentile(nums, float64(nth))\n\t\ttests = append(tests, testCase{\n\t\t\tquery:       query,\n\t\t\tcsvVerifier: fmt.Sprintf(\"%d,1\\n\", expectedPercentile),\n\t\t})\n\t\tquery2 := fmt.Sprintf(`Percentile(field=net_worth, filter=Row(val=\"foo\"), nth=%d)`, nth)\n\t\ttests = append(tests, testCase{\n\t\t\tquery:       query2,\n\t\t\tcsvVerifier: fmt.Sprintf(\"%d,1\\n\", expectedPercentile),\n\t\t})\n\t}\n\n\tfor i, tst := range tests {\n\t\tt.Run(fmt.Sprintf(\"%d-%s\", i, tst.query), func(t *testing.T) {\n\t\t\t// resp := c.Query(t, \"users2\", tst.query)\n\t\t\ttr := c.QueryGRPC(t, \"users2\", tst.query)\n\t\t\tcsvString, err := tableResponseToCSVString(tr)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\t// verify everything after header\n\t\t\tgot := splitSortBackToCSV(csvString[strings.Index(csvString, \"\\n\")+1:])\n\t\t\tif got != tst.csvVerifier {\n\t\t\t\tt.Errorf(\"expected:\\n%s\\ngot:\\n%s\", tst.csvVerifier, got)\n\t\t\t}\n\n\t\t\t// TODO: add HTTP and Postgres and ability to convert\n\t\t\t// those results to CSV to run through CSV verifier\n\t\t})\n\t}\n}\n\nfunc lineBreaker(s string) string {\n\treturn strings.Join(strings.Split(s, \" \"), \"\\n\") + \"\\n\"\n}\n\n// tests for abbreviating time values in queries\nfunc variousQueriesOnTimeFields(t *testing.T, c *test.Cluster) {\n\tts := func(t time.Time) int64 {\n\t\treturn t.Unix() * 1e+9\n\t}\n\n\t// generic index\n\t// worth noting, since we are using YMDH resolution, both C4 & C5\n\t// get binned to the same hour\n\tc.CreateField(t, \"t_index\", pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"f1\", pilosa.OptFieldKeys(), pilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMDH\"), \"0\"))\n\tc.ImportTimeQuantumKey(t, \"t_index\", \"f1\", []test.TimeQuantumKey{\n\t\t// from edge cases\n\t\t{ColKey: \"C1\", RowKey: \"R1\", Ts: ts(time.Date(2019, 1, 1, 0, 0, 0, 0, time.UTC))},\n\t\t{ColKey: \"C2\", RowKey: \"R2\", Ts: ts(time.Date(2019, 8, 1, 0, 0, 0, 0, time.UTC))},\n\t\t{ColKey: \"C3\", RowKey: \"R3\", Ts: ts(time.Date(2019, 8, 4, 0, 0, 0, 0, time.UTC))},\n\t\t{ColKey: \"C4\", RowKey: \"R4\", Ts: ts(time.Date(2019, 8, 4, 14, 0, 0, 0, time.UTC))},\n\t\t{ColKey: \"C5\", RowKey: \"R5\", Ts: ts(time.Date(2019, 8, 4, 14, 36, 0, 0, time.UTC))},\n\t\t// to edge cases\n\t\t{ColKey: \"C6\", RowKey: \"R6\", Ts: ts(time.Date(2019, 8, 4, 16, 0, 0, 0, time.UTC))},\n\t\t{ColKey: \"C7\", RowKey: \"R7\", Ts: ts(time.Date(2019, 8, 5, 0, 0, 0, 0, time.UTC))},\n\t\t{ColKey: \"C8\", RowKey: \"R8\", Ts: ts(time.Date(2019, 12, 1, 0, 0, 0, 0, time.UTC))},\n\t\t{ColKey: \"C9\", RowKey: \"R9\", Ts: ts(time.Date(2020, 1, 1, 0, 0, 0, 0, time.UTC))},\n\t})\n\n\t// in this field, all columns have the same row value to simplify test queries for Row\n\tc.CreateField(t, \"t_index\", pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"f2\", pilosa.OptFieldKeys(), pilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMDH\"), \"0\"))\n\tc.ImportTimeQuantumKey(t, \"t_index\", \"f2\", []test.TimeQuantumKey{\n\t\t// from\n\t\t{ColKey: \"C1\", RowKey: \"R\", Ts: ts(time.Date(2019, 1, 1, 0, 0, 0, 0, time.UTC))},\n\t\t{ColKey: \"C2\", RowKey: \"R\", Ts: ts(time.Date(2019, 8, 1, 0, 0, 0, 0, time.UTC))},\n\t\t{ColKey: \"C3\", RowKey: \"R\", Ts: ts(time.Date(2019, 8, 4, 0, 0, 0, 0, time.UTC))},\n\t\t{ColKey: \"C4\", RowKey: \"R\", Ts: ts(time.Date(2019, 8, 4, 14, 0, 0, 0, time.UTC))},\n\t\t{ColKey: \"C5\", RowKey: \"R\", Ts: ts(time.Date(2019, 8, 4, 14, 36, 0, 0, time.UTC))},\n\t\t// to\n\t\t{ColKey: \"C6\", RowKey: \"R\", Ts: ts(time.Date(2019, 8, 4, 16, 0, 0, 0, time.UTC))},\n\t\t{ColKey: \"C7\", RowKey: \"R\", Ts: ts(time.Date(2019, 8, 5, 0, 0, 0, 0, time.UTC))},\n\t\t{ColKey: \"C8\", RowKey: \"R\", Ts: ts(time.Date(2019, 12, 1, 0, 0, 0, 0, time.UTC))},\n\t\t{ColKey: \"C9\", RowKey: \"R\", Ts: ts(time.Date(2020, 1, 1, 0, 0, 0, 0, time.UTC))},\n\t})\n\n\tsplitSortBackToCSV := func(csvStr string) string {\n\t\tss := strings.Split(csvStr[:len(csvStr)-1], \"\\n\")\n\t\tsort.Strings(ss)\n\t\treturn strings.Join(ss, \"\\n\") + \"\\n\"\n\t}\n\n\ttype testCase struct {\n\t\tquery       string\n\t\tcsvVerifier string\n\t}\n\n\ttests := []testCase{\n\t\t// Rows\n\t\t{\n\t\t\tquery:       `Rows(f1, from='2019-08-04T14:36', to='2019-08-04T16:00')`,\n\t\t\tcsvVerifier: lineBreaker(\"R4 R5\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Rows(f1, from='2019-08-04T14', to='2019-08-04T17:00')`,\n\t\t\tcsvVerifier: lineBreaker(\"R4 R5 R6\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Rows(f1, from='2019-08-04', to='2019-08-05')`,\n\t\t\tcsvVerifier: lineBreaker(\"R3 R4 R5 R6\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Rows(f1, from='2019-08', to='2019-12')`,\n\t\t\tcsvVerifier: lineBreaker(\"R2 R3 R4 R5 R6 R7\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Rows(f1, from='2019', to='2020')`,\n\t\t\tcsvVerifier: lineBreaker(\"R1 R2 R3 R4 R5 R6 R7 R8\"),\n\t\t},\n\t\t// Row\n\t\t{\n\t\t\tquery:       `Row(f2='R', from='2019-08-04T14:36', to='2019-08-04T16:00')`,\n\t\t\tcsvVerifier: lineBreaker(\"C4 C5\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(f2='R', from='2019-08-04T14', to='2019-08-04T17:00')`,\n\t\t\tcsvVerifier: lineBreaker(\"C4 C5 C6\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(f2='R', from='2019-08-04', to='2019-08-05')`,\n\t\t\tcsvVerifier: lineBreaker(\"C3 C4 C5 C6\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(f2='R', from='2019-08', to='2019-12')`,\n\t\t\tcsvVerifier: lineBreaker(\"C2 C3 C4 C5 C6 C7\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(f2='R', from='2019', to='2020')`,\n\t\t\tcsvVerifier: lineBreaker(\"C1 C2 C3 C4 C5 C6 C7 C8\"),\n\t\t},\n\t}\n\n\tfor i, tst := range tests {\n\t\tt.Run(fmt.Sprintf(\"%d-%s\", i, tst.query), func(t *testing.T) {\n\t\t\ttr := c.QueryGRPC(t, \"t_index\", tst.query)\n\t\t\tcsvString, err := tableResponseToCSVString(tr)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\t// verify everything after header\n\t\t\tgot := splitSortBackToCSV(csvString[strings.Index(csvString, \"\\n\")+1:])\n\t\t\tif got != tst.csvVerifier {\n\t\t\t\tt.Errorf(\"expected:\\n%s\\ngot:\\n%s\", tst.csvVerifier, got)\n\t\t\t}\n\n\t\t\t// TODO: add HTTP and Postgres and ability to convert\n\t\t\t// those results to CSV to run through CSV verifier\n\t\t})\n\t}\n}\n\n// tests queries on IntFields with various options (min, max) and values\n// this is a test of the correctness of imported values\n// test of values outside of min/max is included in IDK:csv_test\nfunc variousQueriesOnIntFields(t *testing.T, c *test.Cluster) {\n\tindex := \"int_test\"\n\n\tc.CreateField(t, index, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"neg_neg\", pilosa.OptFieldTypeInt(-10, -1))\n\tc.ImportIntKey(t, index, \"neg_neg\", []test.IntKey{\n\t\t{Val: -10, Key: \"userB\"},\n\t\t{Val: -5, Key: \"userC\"},\n\t\t{Val: -4, Key: \"userD\"},\n\t\t{Val: -3, Key: \"userE\"},\n\t\t{Val: -1, Key: \"userG\"},\n\t})\n\n\tc.CreateField(t, index, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"neg_pos\", pilosa.OptFieldTypeInt(-10, 10))\n\tc.ImportIntKey(t, index, \"neg_pos\", []test.IntKey{\n\t\t{Val: -10, Key: \"userB\"},\n\t\t{Val: -5, Key: \"userC\"},\n\t\t{Val: 0, Key: \"userD\"},\n\t\t{Val: 5, Key: \"userE\"},\n\t\t{Val: 10, Key: \"userG\"},\n\t})\n\n\tc.CreateField(t, index, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"zero_pos\", pilosa.OptFieldTypeInt(0, 10))\n\tc.ImportIntKey(t, index, \"zero_pos\", []test.IntKey{\n\t\t{Val: 0, Key: \"userB\"},\n\t\t{Val: 2, Key: \"userC\"},\n\t\t{Val: 3, Key: \"userD\"},\n\t\t{Val: 4, Key: \"userE\"},\n\t\t{Val: 10, Key: \"userG\"},\n\t})\n\n\tc.CreateField(t, index, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"pos_pos\", pilosa.OptFieldTypeInt(5, 10))\n\tc.ImportIntKey(t, index, \"pos_pos\", []test.IntKey{\n\t\t{Val: 5, Key: \"userB\"},\n\t\t{Val: 6, Key: \"userC\"},\n\t\t{Val: 7, Key: \"userD\"},\n\t\t{Val: 9, Key: \"userE\"},\n\t\t{Val: 10, Key: \"userG\"},\n\t})\n\n\tsplitSortBackToCSV := func(csvStr string) string {\n\t\tss := strings.Split(csvStr[:len(csvStr)-1], \"\\n\")\n\t\tsort.Strings(ss)\n\t\treturn strings.Join(ss, \"\\n\") + \"\\n\"\n\t}\n\n\ttype testCase struct {\n\t\tquery       string\n\t\tcsvVerifier string\n\t}\n\n\ttests := []testCase{\n\t\t{\n\t\t\tquery: \"extract(All(), Rows(neg_neg), Rows(neg_pos), Rows(zero_pos), Rows(pos_pos))\",\n\t\t\tcsvVerifier: `userB,-10,-10,0,5\nuserC,-5,-5,2,6\nuserD,-4,0,3,7\nuserE,-3,5,4,9\nuserG,-1,10,10,10\n`,\n\t\t},\n\t}\n\n\tfor i, tst := range tests {\n\t\tt.Run(fmt.Sprintf(\"%d-%s\", i, tst.query), func(t *testing.T) {\n\t\t\ttr := c.QueryGRPC(t, index, tst.query)\n\t\t\tcsvString, err := tableResponseToCSVString(tr)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\t// verify everything after header\n\t\t\tgot := splitSortBackToCSV(csvString[strings.Index(csvString, \"\\n\")+1:])\n\t\t\tif got != tst.csvVerifier {\n\t\t\t\tt.Errorf(\"expected:\\n%s\\ngot:\\n%s\", tst.csvVerifier, got)\n\t\t\t}\n\t\t})\n\t}\n}\n\n// Constants used in TimestampField testing\nvar (\n\tminTime  = pilosa.MinTimestamp\n\tmaxTime  = pilosa.MaxTimestamp\n\tminSec   = minTime.Unix()\n\tmaxSec   = maxTime.Unix()\n\tminMilli = minTime.UnixMilli()\n\tmaxMilli = maxTime.UnixMilli()\n\tminMicro = minTime.UnixMicro()\n\tmaxMicro = maxTime.UnixMicro()\n\tminNano  = pilosa.MinTimestampNano.UnixNano()\n\tmaxNano  = pilosa.MaxTimestampNano.UnixNano()\n)\n\n// variousQueriesOnTimestampFields tests queries on Timestamp Fields at various granularities using the default epoch\nfunc variousQueriesOnTimestampFields(t *testing.T, c *test.Cluster) {\n\tindex := \"ts_test01\"\n\n\t// Testing whether the max and min timestamps can be represented for seconds\n\tc.CreateField(t, index, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"unix_sec\", pilosa.OptFieldTypeTimestamp(time.Unix(0, 0), \"s\"))\n\tc.ImportIntKey(t, index, \"unix_sec\", []test.IntKey{\n\t\t{Val: minSec, Key: \"userA\"},\n\t\t{Val: maxSec, Key: \"userB\"},\n\t})\n\n\t// Testing whether the max and min timestamps can be represented for milliseconds\n\tc.CreateField(t, index, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"unix_milli\", pilosa.OptFieldTypeTimestamp(time.Unix(0, 0), \"ms\"))\n\tc.ImportIntKey(t, index, \"unix_milli\", []test.IntKey{\n\t\t{Val: minMilli, Key: \"userA\"},\n\t\t{Val: maxMilli, Key: \"userB\"},\n\t})\n\n\t// Testing whether the max and min timestamps can be represented for microseconds\n\tc.CreateField(t, index, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"unix_micro\", pilosa.OptFieldTypeTimestamp(time.Unix(0, 0), \"us\"))\n\tc.ImportIntKey(t, index, \"unix_micro\", []test.IntKey{\n\t\t{Val: minMicro, Key: \"userA\"},\n\t\t{Val: maxMicro, Key: \"userB\"},\n\t})\n\n\t// Note that min and max values that can be represented for Nanos is a much smaller range than any of the above granularities.\n\tc.CreateField(t, index, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"unix_nano\", pilosa.OptFieldTypeTimestamp(time.Unix(0, 0), \"ns\"))\n\tc.ImportIntKey(t, index, \"unix_nano\", []test.IntKey{\n\t\t{Val: minNano, Key: \"userA\"},\n\t\t{Val: maxNano, Key: \"userB\"},\n\t})\n\n\tsplitSortBackToCSV := func(csvStr string) string {\n\t\tif len(csvStr) == 0 {\n\t\t\treturn \"\"\n\t\t}\n\t\tss := strings.Split(csvStr[:len(csvStr)-1], \"\\n\")\n\t\tsort.Strings(ss)\n\t\treturn strings.Join(ss, \"\\n\") + \"\\n\"\n\t}\n\n\ttype testCase struct {\n\t\tquery       string\n\t\tcsvVerifier string\n\t}\n\n\ttests := []testCase{\n\t\t{\n\t\t\tquery: \"extract(All(), Rows(unix_sec))\",\n\t\t\tcsvVerifier: `userA,0001-01-01T00:00:01Z\nuserB,9999-12-31T23:59:59Z\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"extract(All(), Rows(unix_milli))\",\n\t\t\tcsvVerifier: `userA,0001-01-01T00:00:01Z\nuserB,9999-12-31T23:59:59Z\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"extract(All(), Rows(unix_micro))\",\n\t\t\tcsvVerifier: `userA,0001-01-01T00:00:01Z\nuserB,9999-12-31T23:59:59Z\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"extract(All(), Rows(unix_nano))\",\n\t\t\tcsvVerifier: `userA,1833-11-24T17:31:44Z\nuserB,2106-02-07T06:28:16Z\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"All()\",\n\t\t\tcsvVerifier: `userA\nuserB\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"count(All())\",\n\t\t\tcsvVerifier: `2\n`,\n\t\t},\n\t\t// Found an existing bug: Distinct on timestamp does not return values prior to the epoch.\n\t\t// These tests need to be uncommented when issue is fixed.\n\t\t// \t\t{\n\t\t// \t\t\tquery: \"Distinct(field=unix_sec)\",\n\t\t// \t\t\tcsvVerifier: `0001-01-01T00:00:01Z\n\t\t// 9999-12-31T23:59:59Z\n\t\t// `,\n\t\t// \t\t},\n\t\t// \t\t{\n\t\t// \t\t\tquery: \"Distinct(field=unix_milli)\",\n\t\t// \t\t\tcsvVerifier: `0001-01-01T00:00:01Z\n\t\t// 9999-12-31T23:59:59Z\n\t\t// `,\n\t\t// \t\t},\n\t\t{\n\t\t\tquery: \"Min(unix_sec)\",\n\t\t\tcsvVerifier: `0001-01-01T00:00:01Z,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Max(unix_sec)\",\n\t\t\tcsvVerifier: `9999-12-31T23:59:59Z,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Min(unix_milli)\",\n\t\t\tcsvVerifier: `0001-01-01T00:00:01Z,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Max(unix_milli)\",\n\t\t\tcsvVerifier: `9999-12-31T23:59:59Z,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Min(unix_micro)\",\n\t\t\tcsvVerifier: `0001-01-01T00:00:01Z,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Max(unix_micro)\",\n\t\t\tcsvVerifier: `9999-12-31T23:59:59Z,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Min(unix_nano)\",\n\t\t\tcsvVerifier: `1833-11-24T17:31:44Z,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Max(unix_nano)\",\n\t\t\tcsvVerifier: `2106-02-07T06:28:16Z,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(unix_micro))\",\n\t\t\tcsvVerifier: `-62135596799000000,1\n253402300799000000,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(unix_sec=\"0001-01-01T00:00:01Z\")`,\n\t\t\tcsvVerifier: lineBreaker(\"userA\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(unix_sec=\"9999-12-31T23:59:59Z\")`,\n\t\t\tcsvVerifier: lineBreaker(\"userB\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(unix_milli=\"0001-01-01T00:00:01Z\")`,\n\t\t\tcsvVerifier: lineBreaker(\"userA\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(unix_milli=\"9999-12-31T23:59:59Z\")`,\n\t\t\tcsvVerifier: lineBreaker(\"userB\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(unix_micro=\"0001-01-01T00:00:01Z\")`,\n\t\t\tcsvVerifier: lineBreaker(\"userA\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(unix_micro=\"9999-12-31T23:59:59Z\")`,\n\t\t\tcsvVerifier: lineBreaker(\"userB\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(unix_nano=\"1833-11-24T17:31:44Z\")`,\n\t\t\tcsvVerifier: lineBreaker(\"userA\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(unix_nano=\"2106-02-07T06:28:16Z\")`,\n\t\t\tcsvVerifier: lineBreaker(\"userB\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Union(Row(unix_nano=\"2106-02-07T06:28:16Z\"), Row(unix_micro=\"0001-01-01T00:00:01Z\"))`,\n\t\t\tcsvVerifier: lineBreaker(\"userA\\nuserB\"),\n\t\t},\n\t\t{\n\t\t\tquery: `Set(\"userA\", unix_milli=\"2000-12-31T23:59:59.999Z\")`,\n\t\t\tcsvVerifier: `true\n`,\n\t\t},\n\t\t{\n\t\t\tquery: `Clear(\"userA\", unix_milli=\"2000-12-31T23:59:59.999Z\")`,\n\t\t\tcsvVerifier: `true\n`,\n\t\t},\n\t\t// Not Supported for Timestamp\n\t\t// {\n\t\t// \tquery:       `ClearRow(unix_milli=\"2000-12-31T23:59:59.999Z\")`,\n\t\t// \tcsvVerifier: `true`,\n\t\t// },\n\t}\n\n\tfor i, tst := range tests {\n\t\tt.Run(fmt.Sprintf(\"%d-%s\", i, tst.query), func(t *testing.T) {\n\t\t\ttr := c.QueryGRPC(t, index, tst.query)\n\t\t\tcsvString, err := tableResponseToCSVString(tr)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\t// verify everything after header\n\t\t\tgot := splitSortBackToCSV(csvString[strings.Index(csvString, \"\\n\")+1:])\n\t\t\tif got != tst.csvVerifier {\n\t\t\t\tt.Errorf(\"expected:\\n%s\\ngot:\\n%s\", tst.csvVerifier, got)\n\t\t\t}\n\t\t})\n\t}\n}\n\n// variousQueriesOnLargeEpoch tests queries on TimestampFields when the epoch is set either to\n// the min or max timestamp allowed.\nfunc variousQueriesOnLargeEpoch(t *testing.T, c *test.Cluster) {\n\tindex := \"ts_epoch_test20321\"\n\t// mag := int64(10000000000)\n\n\t// These large constants are close to min and max int64 but not quite since go has to account for the difference between Unix Epoch and Go's Epoch\n\tc.CreateField(t, index, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"unix_sec_min\", pilosa.OptFieldTypeTimestamp(minTime, \"s\"))\n\tc.ImportIntKey(t, index, \"unix_sec_min\", []test.IntKey{\n\t\t{Val: 0, Key: \"userA\"},\n\t\t{Val: -minSec, Key: \"userB\"},\n\t\t{Val: -minSec + maxSec, Key: \"userC\"},\n\t})\n\n\t// vprint.VV(\"-MaxSec: %+v\", -maxSec)\n\t// vprint.VV(\"-MaxSec + minsec: %+v\", -maxSec+minSec)\n\n\tc.CreateField(t, index, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"unix_sec_max\", pilosa.OptFieldTypeTimestamp(maxTime, \"s\"))\n\tc.ImportIntKey(t, index, \"unix_sec_max\", []test.IntKey{\n\t\t{Val: 0, Key: \"userA\"}, // 9999-12-31\n\t\t// {Val: 1, Key: \"userE\"},\n\t\t// {Val: -1, Key: \"userD\"},\n\t\t{Val: -maxSec, Key: \"userB\"},          // 1970....\n\t\t{Val: -maxSec + minSec, Key: \"userC\"}, // 0001\n\t})\n\n\tc.CreateField(t, index, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"unix_milli_min\", pilosa.OptFieldTypeTimestamp(minTime, \"ms\"))\n\tc.ImportIntKey(t, index, \"unix_milli_min\", []test.IntKey{\n\t\t{Val: 0, Key: \"userA\"},\n\t\t{Val: -minMilli, Key: \"userB\"},\n\t\t{Val: -minMilli + maxMilli, Key: \"userC\"},\n\t})\n\n\tc.CreateField(t, index, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"unix_milli_max\", pilosa.OptFieldTypeTimestamp(maxTime, \"ms\"))\n\tc.ImportIntKey(t, index, \"unix_milli_max\", []test.IntKey{\n\t\t{Val: 0, Key: \"userA\"},\n\t\t{Val: -maxMilli, Key: \"userB\"},\n\t\t{Val: -maxMilli + minMilli, Key: \"userC\"},\n\t})\n\n\tc.CreateField(t, index, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"unix_micro_min\", pilosa.OptFieldTypeTimestamp(minTime, \"us\"))\n\tc.ImportIntKey(t, index, \"unix_micro_min\", []test.IntKey{\n\t\t{Val: 0, Key: \"userA\"},\n\t\t{Val: -minMicro, Key: \"userB\"},\n\t\t{Val: -minMicro + maxMicro, Key: \"userC\"},\n\t})\n\n\tc.CreateField(t, index, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"unix_micro_max\", pilosa.OptFieldTypeTimestamp(maxTime, \"us\"))\n\tc.ImportIntKey(t, index, \"unix_micro_max\", []test.IntKey{\n\t\t{Val: 0, Key: \"userA\"},\n\t\t{Val: -maxMicro, Key: \"userB\"},\n\t\t{Val: -maxMicro + minMicro, Key: \"userC\"},\n\t})\n\n\tc.CreateField(t, index, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"unix_nano_min\", pilosa.OptFieldTypeTimestamp(pilosa.MinTimestampNano, \"ns\"))\n\tc.ImportIntKey(t, index, \"unix_nano_min\", []test.IntKey{\n\t\t{Val: 0, Key: \"userA\"},\n\t\t{Val: -minNano - 1, Key: \"userB\"},\n\t\t{Val: -minNano, Key: \"userC\"},\n\t})\n\n\tc.CreateField(t, index, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"unix_nano_max\", pilosa.OptFieldTypeTimestamp(pilosa.MaxTimestampNano, \"ns\"))\n\tc.ImportIntKey(t, index, \"unix_nano_max\", []test.IntKey{\n\t\t{Val: 0, Key: \"userA\"},\n\t\t{Val: -maxNano + 1, Key: \"userB\"},\n\t\t{Val: -maxNano, Key: \"userC\"},\n\t})\n\n\tsplitSortBackToCSV := func(csvStr string) string {\n\t\tif csvStr == \"\" {\n\t\t\treturn \"\"\n\t\t}\n\t\tss := strings.Split(csvStr[:len(csvStr)-1], \"\\n\")\n\t\tsort.Strings(ss)\n\t\treturn strings.Join(ss, \"\\n\") + \"\\n\"\n\t}\n\n\ttype testCase struct {\n\t\tquery       string\n\t\tcsvVerifier string\n\t}\n\n\ttests := []testCase{\n\t\t{\n\t\t\tquery: \"extract(All(), Rows(unix_sec_min))\",\n\t\t\tcsvVerifier: `userA,0001-01-01T00:00:01Z\nuserB,1970-01-01T00:00:00Z\nuserC,9999-12-31T23:59:59Z\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"extract(All(), Rows(unix_sec_max))\",\n\t\t\tcsvVerifier: `userA,9999-12-31T23:59:59Z\nuserB,1970-01-01T00:00:00Z\nuserC,0001-01-01T00:00:01Z\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"extract(All(), Rows(unix_milli_min))\",\n\t\t\tcsvVerifier: `userA,0001-01-01T00:00:01Z\nuserB,1970-01-01T00:00:00Z\nuserC,9999-12-31T23:59:59Z\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"extract(All(), Rows(unix_milli_max))\",\n\t\t\tcsvVerifier: `userA,9999-12-31T23:59:59Z\nuserB,1970-01-01T00:00:00Z\nuserC,0001-01-01T00:00:01Z\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"extract(All(), Rows(unix_micro_min))\",\n\t\t\tcsvVerifier: `userA,0001-01-01T00:00:01Z\nuserB,1970-01-01T00:00:00Z\nuserC,9999-12-31T23:59:59Z\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"extract(All(), Rows(unix_micro_max))\",\n\t\t\tcsvVerifier: `userA,9999-12-31T23:59:59Z\nuserB,1970-01-01T00:00:00Z\nuserC,0001-01-01T00:00:01Z\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"extract(All(), Rows(unix_nano_min))\",\n\t\t\tcsvVerifier: `userA,1833-11-24T17:31:44Z\nuserB,1969-12-31T23:59:59.999999999Z\nuserC,1970-01-01T00:00:00Z\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"extract(All(), Rows(unix_nano_max))\",\n\t\t\tcsvVerifier: `userA,2106-02-07T06:28:16Z\nuserB,1970-01-01T00:00:00.000000001Z\nuserC,1970-01-01T00:00:00Z\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"All()\",\n\t\t\tcsvVerifier: `userA\nuserB\nuserC\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"count(All())\",\n\t\t\tcsvVerifier: `3\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Min(unix_sec_min)\",\n\t\t\tcsvVerifier: `0001-01-01T00:00:01Z,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Max(unix_sec_min)\",\n\t\t\tcsvVerifier: `9999-12-31T23:59:59Z,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Min(unix_sec_max)\",\n\t\t\tcsvVerifier: `0001-01-01T00:00:01Z,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Max(unix_sec_max)\",\n\t\t\tcsvVerifier: `9999-12-31T23:59:59Z,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Min(unix_milli_min)\",\n\t\t\tcsvVerifier: `0001-01-01T00:00:01Z,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Max(unix_milli_min)\",\n\t\t\tcsvVerifier: `9999-12-31T23:59:59Z,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Max(unix_milli_max)\",\n\t\t\tcsvVerifier: `9999-12-31T23:59:59Z,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Min(unix_micro_min)\",\n\t\t\tcsvVerifier: `0001-01-01T00:00:01Z,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Max(unix_micro_max)\",\n\t\t\tcsvVerifier: `9999-12-31T23:59:59Z,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Min(unix_nano_min)\",\n\t\t\tcsvVerifier: `1833-11-24T17:31:44Z,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Max(unix_nano_min)\",\n\t\t\tcsvVerifier: `1970-01-01T00:00:00Z,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Min(unix_nano_max)\",\n\t\t\tcsvVerifier: `1970-01-01T00:00:00Z,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Max(unix_nano_max)\",\n\t\t\tcsvVerifier: `2106-02-07T06:28:16Z,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(unix_micro_min))\",\n\t\t\tcsvVerifier: `-62135596799000000,1\n0,1\n253402300799000000,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(unix_sec_min=\"0001-01-01T00:00:01Z\")`,\n\t\t\tcsvVerifier: lineBreaker(\"userA\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(unix_sec_max=\"0001-01-01T00:00:01Z\")`,\n\t\t\tcsvVerifier: lineBreaker(\"userC\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(unix_sec_max=\"9999-12-31T23:59:59Z\")`,\n\t\t\tcsvVerifier: lineBreaker(\"userA\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(unix_milli_min=\"0001-01-01T00:00:01Z\")`,\n\t\t\tcsvVerifier: lineBreaker(\"userA\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(unix_milli_min=\"9999-12-31T23:59:59Z\")`,\n\t\t\tcsvVerifier: lineBreaker(\"userC\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(unix_micro_max=\"0001-01-01T00:00:01Z\")`,\n\t\t\tcsvVerifier: lineBreaker(\"userC\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(unix_micro_max=\"9999-12-31T23:59:59Z\")`,\n\t\t\tcsvVerifier: lineBreaker(\"userA\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(unix_nano_min=\"1833-11-24T17:31:44Z\")`,\n\t\t\tcsvVerifier: lineBreaker(\"userA\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(unix_nano_min=\"1969-12-31T23:59:59.999999999Z\")`,\n\t\t\tcsvVerifier: lineBreaker(\"userB\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(unix_nano_min=\"1970-01-01T00:00:00Z\")`,\n\t\t\tcsvVerifier: lineBreaker(\"userC\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(unix_nano_max=\"2106-02-07T06:28:16Z\")`,\n\t\t\tcsvVerifier: lineBreaker(\"userA\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(unix_nano_max=\"1970-01-01T00:00:00.000000001Z\")`,\n\t\t\tcsvVerifier: lineBreaker(\"userB\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Row(unix_nano_max=\"1970-01-01T00:00:00Z\")`,\n\t\t\tcsvVerifier: lineBreaker(\"userC\"),\n\t\t},\n\t\t{\n\t\t\tquery:       `Union(Row(unix_nano_max=\"2106-02-07T06:28:16Z\"), Row(unix_micro_max=\"0001-01-01T00:00:01Z\"))`,\n\t\t\tcsvVerifier: lineBreaker(\"userA\\nuserC\"),\n\t\t},\n\t\t{\n\t\t\tquery: `Set(\"userA\", unix_sec_min=\"2000-12-31T23:59:59.999Z\")`,\n\t\t\tcsvVerifier: `true\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"extract(All(), Rows(unix_sec_min))\",\n\t\t\tcsvVerifier: `userA,2000-12-31T23:59:59Z\nuserB,1970-01-01T00:00:00Z\nuserC,9999-12-31T23:59:59Z\n`,\n\t\t},\n\t\t{\n\t\t\tquery: `Clear(\"userA\", unix_sec_min=\"2000-12-31T23:59:59.999Z\")`,\n\t\t\tcsvVerifier: `true\n`,\n\t\t},\n\t\t{\n\t\t\tquery: `Set(\"userA\", unix_milli_max=\"2000-12-31T23:59:59.999Z\")`,\n\t\t\tcsvVerifier: `true\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"extract(All(), Rows(unix_milli_max))\",\n\t\t\tcsvVerifier: `userA,2000-12-31T23:59:59.999Z\nuserB,1970-01-01T00:00:00Z\nuserC,0001-01-01T00:00:01Z\n`,\n\t\t},\n\t\t{\n\t\t\tquery: `Clear(\"userA\", unix_milli_max=\"2000-12-31T23:59:59.999Z\")`,\n\t\t\tcsvVerifier: `true\n`,\n\t\t},\n\t\t{\n\t\t\tquery: `Set(\"userA\", unix_nano_max=\"2050-02-01T00:00:00.000000002Z\")`,\n\t\t\tcsvVerifier: `true\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"extract(All(), Rows(unix_nano_max))\",\n\t\t\tcsvVerifier: `userA,2050-02-01T00:00:00.000000002Z\nuserB,1970-01-01T00:00:00.000000001Z\nuserC,1970-01-01T00:00:00Z\n`,\n\t\t},\n\t\t{\n\t\t\tquery: `Clear(\"userA\", unix_nano_max=\"1970-01-01T00:00:00.000000002Z\")`,\n\t\t\tcsvVerifier: `true\n`,\n\t\t},\n\t\t{\n\t\t\tquery: `Set(\"userA\", unix_nano_min=\"1969-12-31T23:59:59.999999998Z\")`,\n\t\t\tcsvVerifier: `true\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"extract(All(), Rows(unix_nano_min))\",\n\t\t\tcsvVerifier: `userA,1969-12-31T23:59:59.999999998Z\nuserB,1969-12-31T23:59:59.999999999Z\nuserC,1970-01-01T00:00:00Z\n`,\n\t\t},\n\t\t{\n\t\t\tquery: `Clear(\"userA\", unix_nano_min=\"1969-12-31T23:59:59.999999998Z\")`,\n\t\t\tcsvVerifier: `true\n`,\n\t\t},\n\t}\n\n\tfor i, tst := range tests {\n\t\tt.Run(fmt.Sprintf(\"%d-%s\", i, tst.query), func(t *testing.T) {\n\t\t\ttr := c.QueryGRPC(t, index, tst.query)\n\t\t\tcsvString, err := tableResponseToCSVString(tr)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\t// verify everything after header\n\t\t\tgot := splitSortBackToCSV(csvString[strings.Index(csvString, \"\\n\")+1:])\n\t\t\tif got != tst.csvVerifier {\n\t\t\t\tt.Errorf(\"expected:\\n%s\\ngot:\\n%s\", tst.csvVerifier, got)\n\t\t\t}\n\t\t})\n\t}\n}\n\nvar usersIndex = \"users\"\n\nfunc populateTestData(t *testing.T, c *test.Cluster) {\n\t// Create and populate \"likenums\" similar to \"likes\", but without keys on the field.\n\tc.CreateField(t, usersIndex, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"likenums\")\n\tc.ImportIDKey(t, usersIndex, \"likenums\", []test.KeyID{\n\t\t{ID: 1, Key: \"userA\"},\n\t\t{ID: 2, Key: \"userB\"},\n\t\t{ID: 3, Key: \"userC\"},\n\t\t{ID: 4, Key: \"userD\"},\n\t\t{ID: 5, Key: \"userE\"},\n\t\t{ID: 6, Key: \"userF\"},\n\t\t{ID: 7, Key: \"userA\"},\n\t\t{ID: 7, Key: \"userB\"},\n\t\t{ID: 7, Key: \"userC\"},\n\t\t{ID: 7, Key: \"userD\"},\n\t\t// we intentionally leave user E out because then there is no\n\t\t// data for userE's shard for this field, which triggered a\n\t\t// \"fragment not found\" problem\n\t\t{ID: 7, Key: \"userF\"},\n\t})\n\n\t// Create and populate \"likes\" field.\n\tc.CreateField(t, usersIndex, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"likes\", pilosa.OptFieldKeys())\n\tc.ImportKeyKey(t, usersIndex, \"likes\", [][2]string{\n\t\t{\"molecula\", \"userA\"},\n\t\t{\"pilosa\", \"userB\"},\n\t\t{\"pangolin\", \"userC\"},\n\t\t{\"zebra\", \"userD\"},\n\t\t{\"toucan\", \"userE\"},\n\t\t{\"dog\", \"userF\"},\n\t\t{\"icecream\", \"userA\"},\n\t\t{\"icecream\", \"userB\"},\n\t\t{\"icecream\", \"userC\"},\n\t\t{\"icecream\", \"userD\"},\n\t\t{\"icecream\", \"userE\"},\n\t\t{\"icecream\", \"userF\"},\n\t})\n\n\t// Create and populate \"dinner\" field.\n\tc.CreateField(t, usersIndex, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"dinner\", pilosa.OptFieldKeys())\n\tc.ImportKeyKey(t, usersIndex, \"dinner\", [][2]string{\n\t\t{\"leftovers\", \"userB\"},\n\t\t{\"pizza\", \"userA\"},\n\t\t{\"pizza\", \"userB\"},\n\t\t{\"chinese\", \"userA\"},\n\t\t{\"chinese\", \"userB\"},\n\t\t{\"chinese\", \"userF\"},\n\t})\n\n\t// Create and populate \"places_visited\" time field.\n\tc.CreateField(t, usersIndex, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"places_visited\", pilosa.OptFieldKeys(), pilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YM\"), \"0\"))\n\tts2019Jan01 := int64(1546300800) * 1e+9 // 2019 January 1st 0:00:00\n\tts2019Aug01 := int64(1564617600) * 1e+9 // 2019 August  1st 0:00:00\n\tts2020Jan01 := int64(1577836800) * 1e+9 // 2020 January 1st 0:00:00\n\tc.ImportTimeQuantumKey(t, usersIndex, \"places_visited\", []test.TimeQuantumKey{\n\t\t// 2019 January: nairobi, paris, austin, toronto\n\t\t{RowKey: \"nairobi\", ColKey: \"userB\", Ts: ts2019Jan01},\n\t\t{RowKey: \"paris\", ColKey: \"userC\", Ts: ts2019Jan01},\n\t\t{RowKey: \"austin\", ColKey: \"userF\", Ts: ts2019Jan01},\n\t\t{RowKey: \"toronto\", ColKey: \"userA\", Ts: ts2019Jan01},\n\t\t// 2019 August: toronto only\n\t\t{RowKey: \"toronto\", ColKey: \"userB\", Ts: ts2019Aug01},\n\t\t{RowKey: \"toronto\", ColKey: \"userC\", Ts: ts2019Aug01},\n\t\t// 2020: toronto, mombasa, sydney, nairobi\n\t\t{RowKey: \"toronto\", ColKey: \"userB\", Ts: ts2020Jan01},\n\t\t{RowKey: \"toronto\", ColKey: \"userD\", Ts: ts2020Jan01},\n\t\t{RowKey: \"toronto\", ColKey: \"userE\", Ts: ts2020Jan01},\n\t\t{RowKey: \"toronto\", ColKey: \"userF\", Ts: ts2020Jan01},\n\t\t{RowKey: \"mombasa\", ColKey: \"userA\", Ts: ts2020Jan01},\n\t\t{RowKey: \"sydney\", ColKey: \"userD\", Ts: ts2020Jan01},\n\t\t{RowKey: \"nairobi\", ColKey: \"userE\", Ts: ts2020Jan01},\n\t})\n\n\t// Create and populate \"affinity\" int field with negative, positive, zero and null values.\n\tc.CreateField(t, usersIndex, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"affinity\", pilosa.OptFieldTypeInt(-1000, 1000))\n\tc.ImportIntKey(t, usersIndex, \"affinity\", []test.IntKey{\n\t\t{Val: 10, Key: \"userA\"},\n\t\t{Val: -10, Key: \"userB\"},\n\t\t{Val: 5, Key: \"userC\"},\n\t\t{Val: -5, Key: \"userD\"},\n\t\t{Val: 0, Key: \"userE\"},\n\t})\n\n\t// Create and populate \"net_worth\" int field with positive values.\n\tc.CreateField(t, usersIndex, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"net_worth\", pilosa.OptFieldTypeInt(-100000000, 100000000))\n\tc.ImportIntKey(t, usersIndex, \"net_worth\", []test.IntKey{\n\t\t{Val: 1, Key: \"userA\"},\n\t\t{Val: 10, Key: \"userB\"},\n\t\t{Val: 100, Key: \"userC\"},\n\t\t{Val: 1000, Key: \"userD\"},\n\t\t{Val: 10000, Key: \"userE\"},\n\t\t{Val: 100000, Key: \"userF\"},\n\t})\n\n\tc.CreateField(t, usersIndex, pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"zip_code\", pilosa.OptFieldTypeInt(0, 100000))\n\tc.ImportIntKey(t, usersIndex, \"zip_code\", []test.IntKey{\n\t\t{Val: 78739, Key: \"userA\"},\n\t\t{Val: 78739, Key: \"userB\"},\n\t\t{Val: 19707, Key: \"userC\"},\n\t\t{Val: 19707, Key: \"userD\"},\n\t\t{Val: 86753, Key: \"userE\"},\n\t\t{Val: 78739, Key: \"userG\"},\n\t})\n}\n\nfunc variousQueries(t *testing.T, c *test.Cluster) {\n\t// NOTE: this relies on populateTestData being called first\n\n\t// define and run a bunch of tests\n\ttests := []struct {\n\t\tquery       string\n\t\tqrVerifier  func(t *testing.T, resp pilosa.QueryResponse)\n\t\tcsvVerifier string\n\t}{\n\t\t{ // 2020 & 2019 All\n\t\t\tquery: `GroupBy(Rows(places_visited, from='2019-01-01T00:00', to='2020-12-31T23:59'))`,\n\t\t\tcsvVerifier: `nairobi,2\nparis,1\naustin,1\ntoronto,6\nmombasa,1\nsydney,1\n`,\n\t\t},\n\t\t{ // 2019 January only\n\t\t\tquery: `GroupBy(Rows(places_visited, from='2019-01-01T00:00', to='2019-02-01T00:00'))`,\n\t\t\tcsvVerifier: `nairobi,1\nparis,1\naustin,1\ntoronto,1\n`,\n\t\t},\n\t\t{ // 2019 All\n\t\t\tquery: `GroupBy(Rows(places_visited, from='2019-01-01T00:00', to='2019-12-31T23:59'))`,\n\t\t\tcsvVerifier: `nairobi,1\nparis,1\naustin,1\ntoronto,3\n`,\n\t\t},\n\t\t{ // 2019 All, this excludes userC (who likes pangolin & icecream) from the count.\n\t\t\t// UserC visited Paris and Toronto in 2019\n\t\t\tquery: `GroupBy(\n\t\t\t\t\tRows(places_visited, from='2019-01-01T00:00', to='2019-12-31T23:59'),\n\t\t\t\t\tfilter=Not(Intersect(Row(likes='pangolin'), Row(likes='icecream')))\n\t\t\t\t)`,\n\t\t\tcsvVerifier: `nairobi,1\naustin,1\ntoronto,2\n`,\n\t\t},\n\t\t{ // After excluding UserC, this gets the sum of the networth of everyone per cities travelled\n\t\t\tquery: `GroupBy(\n\t\t\t\t\tRows(places_visited, from='2019-01-01T00:00', to='2019-12-31T23:59'),\n\t\t\t\t\tfilter=Not(Intersect(Row(likes='pangolin'), Row(likes='icecream'))),\n\t\t\t\t\taggregate=Sum(field=net_worth)\n\t\t\t\t)`,\n\t\t\tcsvVerifier: `nairobi,1,10\naustin,1,100000\ntoronto,2,11\n`,\n\t\t},\n\t\t{ // 2020 & 2019 All\n\t\t\tquery:       `Rows(places_visited, from='2019-01-01T00:00', to='2020-12-31T23:59')`,\n\t\t\tcsvVerifier: \"nairobi\\nparis\\naustin\\ntoronto\\nmombasa\\nsydney\\n\",\n\t\t},\n\t\t{ // 2019 All\n\t\t\tquery:       `Rows(places_visited, from='2019-01-01T00:00', to='2019-12-31T23:59')`,\n\t\t\tcsvVerifier: \"nairobi\\nparis\\naustin\\ntoronto\\n\",\n\t\t},\n\t\t{ // 2019 January only\n\t\t\tquery:       `Rows(places_visited, from='2019-01-01T00:00', to='2019-02-01T00:00')`,\n\t\t\tcsvVerifier: \"nairobi\\nparis\\naustin\\ntoronto\\n\",\n\t\t},\n\t\t{\n\t\t\tquery: \"Count(All())\",\n\t\t\tqrVerifier: func(t *testing.T, resp pilosa.QueryResponse) {\n\t\t\t\tif resp.Results[0].(uint64) != 7 {\n\t\t\t\t\tt.Errorf(\"expected 7, got %+v\", resp.Results[0])\n\t\t\t\t}\n\t\t\t},\n\t\t\tcsvVerifier: \"7\\n\",\n\t\t},\n\t\t{\n\t\t\tquery: \"Count(Distinct(field=likenums))\",\n\t\t\tqrVerifier: func(t *testing.T, resp pilosa.QueryResponse) {\n\t\t\t\tif resp.Results[0].(uint64) != 7 {\n\t\t\t\t\tt.Errorf(\"wrong count: %+v\", resp.Results[0])\n\t\t\t\t}\n\t\t\t},\n\t\t\tcsvVerifier: \"7\\n\",\n\t\t},\n\t\t{\n\t\t\tquery: \"Distinct(field=likenums)\",\n\t\t\tqrVerifier: func(t *testing.T, resp pilosa.QueryResponse) {\n\t\t\t\tif !reflect.DeepEqual(resp.Results[0].(*pilosa.Row).Columns(), []uint64{1, 2, 3, 4, 5, 6, 7}) {\n\t\t\t\t\tt.Errorf(\"wrong values: %+v %+v\", resp.Results[0].(*pilosa.Row).Columns(), resp.Results[0].(*pilosa.Row))\n\t\t\t\t}\n\t\t\t},\n\t\t\tcsvVerifier: \"1\\n2\\n3\\n4\\n5\\n6\\n7\\n\",\n\t\t},\n\t\t{\n\t\t\tquery: \"Count(Distinct(field=likes))\",\n\t\t\tqrVerifier: func(t *testing.T, resp pilosa.QueryResponse) {\n\t\t\t\tif resp.Results[0].(uint64) != 7 {\n\t\t\t\t\tt.Errorf(\"wrong count: %+v\", resp.Results[0])\n\t\t\t\t}\n\t\t\t},\n\t\t\tcsvVerifier: \"7\\n\",\n\t\t},\n\t\t{\n\t\t\tquery: \"Distinct(field=affinity)\",\n\t\t\tqrVerifier: func(t *testing.T, resp pilosa.QueryResponse) {\n\t\t\t\tif !reflect.DeepEqual(resp.Results[0].(pilosa.SignedRow).Pos.Columns(), []uint64{0, 5, 10}) {\n\t\t\t\t\tt.Errorf(\"wrong positive records: %+v\", resp.Results[0].(pilosa.SignedRow).Pos.Columns())\n\t\t\t\t}\n\t\t\t\tif !reflect.DeepEqual(resp.Results[0].(pilosa.SignedRow).Neg.Columns(), []uint64{5, 10}) {\n\t\t\t\t\tt.Errorf(\"wrong negative records: %+v\", resp.Results[0].(pilosa.SignedRow).Neg.Columns())\n\t\t\t\t}\n\t\t\t},\n\t\t\tcsvVerifier: \"-10\\n-5\\n0\\n5\\n10\\n\",\n\t\t},\n\t\t{\n\t\t\tquery: \"Count(Distinct(field=affinity))\",\n\t\t\tqrVerifier: func(t *testing.T, resp pilosa.QueryResponse) {\n\t\t\t\tif resp.Results[0].(uint64) != 5 {\n\t\t\t\t\tt.Errorf(\"wrong number of values: %+v\", resp.Results[0])\n\t\t\t\t}\n\t\t\t},\n\t\t\tcsvVerifier: \"5\\n\",\n\t\t},\n\t\t{\n\t\t\tquery: \"Distinct(Row(affinity>=0),field=affinity)\",\n\t\t\tqrVerifier: func(t *testing.T, resp pilosa.QueryResponse) {\n\t\t\t\tif !reflect.DeepEqual(resp.Results[0].(pilosa.SignedRow).Pos.Columns(), []uint64{0, 5, 10}) {\n\t\t\t\t\tt.Errorf(\"wrong positive records: %+v\", resp.Results[0].(pilosa.SignedRow).Pos.Columns())\n\t\t\t\t}\n\t\t\t\tif !reflect.DeepEqual(resp.Results[0].(pilosa.SignedRow).Neg.Columns(), []uint64{}) {\n\t\t\t\t\tt.Errorf(\"wrong negative records: %+v\", resp.Results[0].(pilosa.SignedRow).Neg.Columns())\n\t\t\t\t}\n\t\t\t},\n\t\t\tcsvVerifier: \"0\\n5\\n10\\n\",\n\t\t},\n\t\t{\n\t\t\tquery: \"Count(Distinct(Row(affinity>=0),field=affinity))\",\n\t\t\tqrVerifier: func(t *testing.T, resp pilosa.QueryResponse) {\n\t\t\t\tif resp.Results[0].(uint64) != 3 {\n\t\t\t\t\tt.Errorf(\"wrong number of values: %+v\", resp.Results[0])\n\t\t\t\t}\n\t\t\t},\n\t\t\tcsvVerifier: \"3\\n\",\n\t\t},\n\n\t\t// Handling this case properly will require changing the way\n\t\t// that precomputed data is stored on Call objects. Currently\n\t\t// if a Distinct is at all nested (e.g. within a Count) it\n\t\t// gets handled by executor.handlePreCalls which assumes that\n\t\t// only the positive values are worthwhile.\n\t\t//\n\t\t// {\n\t\t// \tquery: \"Count(Distinct(field=affinity))\",\n\t\t// \tverifier: func(t *testing.T, resp pilosa.QueryResponse) {\n\t\t// \t\tif resp.Results[0].(uint64) != 5 {\n\t\t// \t\t\tt.Errorf(\"wrong number of values: %+v\", resp.Results[0])\n\t\t// \t\t}\n\t\t// \t},\n\t\t// },\n\t\t{\n\t\t\tquery: \"Distinct(Row(affinity<0),field=likes)\",\n\t\t\tqrVerifier: func(t *testing.T, resp pilosa.QueryResponse) {\n\t\t\t\tif !reflect.DeepEqual(resp.Results[0].(*pilosa.Row).Keys, []string{\"pilosa\", \"zebra\", \"icecream\"}) {\n\t\t\t\t\tt.Errorf(\"wrong values: %+v\", resp.Results[0])\n\t\t\t\t}\n\t\t\t},\n\t\t\tcsvVerifier: \"pilosa\\nzebra\\nicecream\\n\",\n\t\t},\n\t\t{\n\t\t\tquery: \"Distinct(Row(affinity>0),field=likes)\",\n\t\t\tqrVerifier: func(t *testing.T, resp pilosa.QueryResponse) {\n\t\t\t\tif !reflect.DeepEqual(resp.Results[0].(*pilosa.Row).Keys, []string{\"molecula\", \"pangolin\", \"icecream\"}) {\n\t\t\t\t\tt.Errorf(\"wrong values: %+v\", resp.Results[0])\n\t\t\t\t}\n\t\t\t},\n\t\t\tcsvVerifier: \"molecula\\npangolin\\nicecream\\n\",\n\t\t},\n\t\t{\n\t\t\tquery: \"Distinct(Row(likenums=1),field=likes)\",\n\t\t\tqrVerifier: func(t *testing.T, resp pilosa.QueryResponse) {\n\t\t\t\tif !reflect.DeepEqual(resp.Results[0].(*pilosa.Row).Keys, []string{\"molecula\", \"icecream\"}) {\n\t\t\t\t\tt.Errorf(\"wrong values: %+v\", resp.Results[0])\n\t\t\t\t}\n\t\t\t},\n\t\t\tcsvVerifier: \"molecula\\nicecream\\n\",\n\t\t},\n\t\t{\n\t\t\tquery: \"Distinct(field=likes)\",\n\t\t\tqrVerifier: func(t *testing.T, resp pilosa.QueryResponse) {\n\t\t\t\tif !reflect.DeepEqual(resp.Results[0].(*pilosa.Row).Keys, []string{\"molecula\", \"pilosa\", \"pangolin\", \"zebra\", \"toucan\", \"dog\", \"icecream\"}) {\n\t\t\t\t\tt.Errorf(\"wrong values: %+v\", resp.Results[0])\n\t\t\t\t}\n\t\t\t},\n\t\t\tcsvVerifier: \"molecula\\npilosa\\npangolin\\nzebra\\ntoucan\\ndog\\nicecream\\n\",\n\t\t},\n\t\t{\n\t\t\tquery: \"Distinct(All(),field=likes)\",\n\t\t\tqrVerifier: func(t *testing.T, resp pilosa.QueryResponse) {\n\t\t\t\tif !reflect.DeepEqual(resp.Results[0].(*pilosa.Row).Keys, []string{\"molecula\", \"pilosa\", \"pangolin\", \"zebra\", \"toucan\", \"dog\", \"icecream\"}) {\n\t\t\t\t\tt.Errorf(\"wrong values: %+v\", resp.Results[0])\n\t\t\t\t}\n\t\t\t},\n\t\t\tcsvVerifier: \"molecula\\npilosa\\npangolin\\nzebra\\ntoucan\\ndog\\nicecream\\n\",\n\t\t},\n\t\t{\n\t\t\tquery: \"Distinct(field=likes )\",\n\t\t\tqrVerifier: func(t *testing.T, resp pilosa.QueryResponse) {\n\t\t\t\tif !reflect.DeepEqual(resp.Results[0].(*pilosa.Row).Keys, []string{\"molecula\", \"pilosa\", \"pangolin\", \"zebra\", \"toucan\", \"dog\", \"icecream\"}) {\n\t\t\t\t\tt.Errorf(\"wrong values: %+v\", resp.Results[0])\n\t\t\t\t}\n\t\t\t},\n\t\t\tcsvVerifier: \"molecula\\npilosa\\npangolin\\nzebra\\ntoucan\\ndog\\nicecream\\n\",\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(field=likes))\",\n\t\t\tcsvVerifier: `molecula,1\npilosa,1\npangolin,1\nzebra,1\ntoucan,1\ndog,1\nicecream,6\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(field=likes), aggregate=Sum(field=net_worth), limit=2, having=Condition(sum>10))\",\n\t\t\tcsvVerifier: `pangolin,1,100\nzebra,1,1000\n`,\n\t\t},\n\t\t{\n\t\t\tquery:       \"GroupBy(Rows(field=likes), having=Condition(count>5))\",\n\t\t\tcsvVerifier: \"icecream,6\\n\",\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(field=likes), filter=Row(affinity>-7))\",\n\t\t\tcsvVerifier: `molecula,1\npangolin,1\nzebra,1\ntoucan,1\nicecream,4\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(field=likes), aggregate=Count(Distinct(field=zip_code)))\",\n\t\t\tcsvVerifier: `molecula,1,1\npilosa,1,1\npangolin,1,1\nzebra,1,1\ntoucan,1,1\ndog,1,0\nicecream,6,3\n`,\n\t\t},\n\t\t{\n\t\t\tquery:       \"GroupBy(Rows(field=likes), aggregate=Count(Distinct(field=zip_code)), having=Condition(sum>2))\",\n\t\t\tcsvVerifier: \"icecream,6,3\\n\",\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(field=likes), filter=Row(affinity>-11), aggregate=Count(Distinct(field=zip_code)))\",\n\t\t\tcsvVerifier: `molecula,1,1\npilosa,1,1\npangolin,1,1\nzebra,1,1\ntoucan,1,1\nicecream,5,3\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(field=likes), filter=Row(affinity>-11), aggregate=Count(Distinct(Row(affinity>-7), field=zip_code)))\",\n\t\t\tcsvVerifier: `molecula,1,1\npilosa,1,0\npangolin,1,1\nzebra,1,1\ntoucan,1,1\nicecream,5,3\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(field=likes), sort=\\\"count desc\\\")\",\n\t\t\tcsvVerifier: `icecream,6\nmolecula,1\npilosa,1\npangolin,1\nzebra,1\ntoucan,1\ndog,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(field=likes), aggregate=Sum(field=net_worth), sort=\\\"aggregate desc, count asc\\\")\",\n\t\t\tcsvVerifier: `icecream,6,111111\ndog,1,100000\ntoucan,1,10000\nzebra,1,1000\npangolin,1,100\npilosa,1,10\nmolecula,1,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(field=likes), aggregate=Sum(field=net_worth), sort=\\\"aggregate desc, count asc\\\", limit=3)\",\n\t\t\tcsvVerifier: `icecream,6,111111\ndog,1,100000\ntoucan,1,10000\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(field=likes), aggregate=Sum(field=net_worth),sort=\\\"aggregate desc, count asc\\\",limit=3,offset=2)\",\n\t\t\tcsvVerifier: `toucan,1,10000\nzebra,1,1000\npangolin,1,100\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(field=affinity), aggregate=Count(Distinct(field=zip_code)))\",\n\t\t\tcsvVerifier: `-10,1,1\n-5,1,1\n0,1,1\n5,1,1\n10,1,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(field=dinner), sort=\\\"count desc\\\", limit=2)\",\n\t\t\tcsvVerifier: `chinese,3\npizza,2\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"TopK(dinner)\",\n\t\t\tcsvVerifier: `chinese,3\npizza,2\nleftovers,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"TopK(field=dinner)\",\n\t\t\tcsvVerifier: `chinese,3\npizza,2\nleftovers,1\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(places_visited, in=[nairobi, toronto]))\",\n\t\t\tcsvVerifier: `nairobi,2\ntoronto,6\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(places_visited, in=[nairobi, toronto, neverland]))\",\n\t\t\tcsvVerifier: `nairobi,2\ntoronto,6\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Rows(places_visited, in=[nairobi, toronto])\",\n\t\t\tcsvVerifier: `nairobi\ntoronto\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Rows(places_visited, in=[nairobi, toronto, neverland])\",\n\t\t\tcsvVerifier: `nairobi\ntoronto\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"Rows(likenums, in=[4, 5])\",\n\t\t\tcsvVerifier: `4\n5\n`,\n\t\t},\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(likenums, in=[4, 5]))\",\n\t\t\tcsvVerifier: `4,1\n5,1\n`,\n\t\t},\n\t}\n\n\tfor i, tst := range tests {\n\t\tt.Run(fmt.Sprintf(\"%d-%s\", i, tst.query), func(t *testing.T) {\n\t\t\tresp := c.Query(t, usersIndex, tst.query)\n\t\t\ttr := c.QueryGRPC(t, usersIndex, tst.query)\n\t\t\tif tst.qrVerifier != nil {\n\t\t\t\ttst.qrVerifier(t, resp)\n\t\t\t}\n\t\t\tcsvString, err := tableResponseToCSVString(tr)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\t// verify everything after header\n\t\t\tgot := csvString[strings.Index(csvString, \"\\n\")+1:]\n\t\t\tif got != tst.csvVerifier {\n\t\t\t\tt.Errorf(\"expected:\\n%s\\ngot:\\n%s\", tst.csvVerifier, got)\n\t\t\t}\n\n\t\t\t// TODO: add HTTP and Postgres and ability to convert\n\t\t\t// those results to CSV to run through CSV verifier\n\t\t})\n\t}\n}\n\n// TestVariousSingleShardQueries tests queries on a dataset which\n// consists of an unkeyed index, and data only in the first\n// shard. Turns out that there are some interesting failure modes\n// which only crop up with one shard. An example is that the mapReduce\n// logic does its first reduce call with a nil interface{} value, and\n// an actual result value. If this is the only reduce that gets done\n// (because there's only one shard), the result might never go through\n// normal merge/reduction logic and might e.g. be nil instead of an\n// empty struct resulting in an NPE later on.\nfunc TestVariousSingleShardQueries(t *testing.T) {\n\tfor _, clusterSize := range []int{1, 4} {\n\t\tt.Run(fmt.Sprintf(\"%d-node\", clusterSize), func(t *testing.T) {\n\t\t\tvariousSingleShardQueries(t, clusterSize)\n\t\t})\n\t}\n}\n\nfunc variousSingleShardQueries(t *testing.T, clusterSize int) {\n\tc := test.MustRunCluster(t, clusterSize)\n\tdefer c.Close()\n\n\tev := c.Idx(\"e\")\n\n\t// Create and populate \"likenums\" similar to \"likes\", but without keys on the field.\n\tc.CreateField(t, ev, pilosa.IndexOptions{Keys: false, TrackExistence: true}, \"lostcount\", pilosa.OptFieldTypeInt(0, 1000000000))\n\tc.ImportIntID(t, ev, \"lostcount\", []test.IntID{\n\t\t{Val: 0, ID: 1},\n\t\t{Val: 1, ID: 2},\n\t\t{Val: 0, ID: 3},\n\t\t{Val: 2, ID: 4},\n\t\t{Val: 2, ID: 5},\n\t\t{Val: 0, ID: 6},\n\t\t{Val: 3, ID: 7},\n\t\t{Val: 3, ID: 8},\n\t\t{Val: 3, ID: 9},\n\t\t{Val: 0, ID: 10},\n\t})\n\n\tc.CreateField(t, ev, pilosa.IndexOptions{Keys: false, TrackExistence: true}, \"jittermax\", pilosa.OptFieldTypeInt(0, 1000000000))\n\tc.ImportIntID(t, ev, \"jittermax\", []test.IntID{\n\t\t{Val: 17, ID: 1},\n\t\t{Val: 3, ID: 2},\n\t\t{Val: 42, ID: 3},\n\t\t{Val: 9, ID: 4},\n\t\t{Val: 17, ID: 5},\n\t\t{Val: 3, ID: 6},\n\t\t{Val: 42, ID: 7},\n\t\t{Val: 9, ID: 8},\n\t\t{Val: 17, ID: 9},\n\t\t{Val: 3, ID: 10},\n\t})\n\n\ttests := []struct {\n\t\tquery       string\n\t\tcsvVerifier string\n\t}{\n\t\t{\n\t\t\tquery: \"GroupBy(Rows(lostcount), aggregate=Count(Distinct(field=jittermax)))\",\n\t\t\tcsvVerifier: `0,4,3\n1,1,1\n2,2,2\n3,3,3\n`,\n\t\t},\n\t}\n\n\tfor i, tst := range tests {\n\t\tt.Run(fmt.Sprintf(\"%d-%s\", i, tst.query), func(t *testing.T) {\n\t\t\ttr := c.QueryGRPC(t, ev, tst.query)\n\t\t\tcsvString, err := tableResponseToCSVString(tr)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\t// verify everything after header\n\t\t\tgot := csvString[strings.Index(csvString, \"\\n\")+1:]\n\t\t\tif got != tst.csvVerifier {\n\t\t\t\tt.Errorf(\"expected:\\n%s\\ngot:\\n%s\", tst.csvVerifier, got)\n\t\t\t}\n\t\t})\n\t}\n}\n\n// tableResponseToCSV converts a generic TableResponse to a CSV format\n// and writes it to the writer.\nfunc tableResponseToCSV(m *proto.TableResponse, w io.Writer) error {\n\twriter := csv.NewWriter(w)\n\trecord := make([]string, len(m.Headers))\n\tfor i, h := range m.Headers {\n\t\trecord[i] = h.Name\n\t}\n\terr := writer.Write(record)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"writing header\")\n\t}\n\tfor i, row := range m.Rows {\n\t\trecord = record[:0]\n\t\tfor colIndex, col := range row.Columns {\n\t\t\tswitch m.Headers[colIndex].Datatype {\n\t\t\tcase \"[]string\":\n\t\t\t\trecord = append(record, fmt.Sprintf(\"%v\", col.GetStringArrayVal()))\n\t\t\tcase \"[]uint64\":\n\t\t\t\trecord = append(record, fmt.Sprintf(\"%v\", col.GetUint64ArrayVal()))\n\t\t\tcase \"string\":\n\t\t\t\trecord = append(record, fmt.Sprintf(\"%v\", col.GetStringVal()))\n\t\t\tcase \"uint64\":\n\t\t\t\trecord = append(record, fmt.Sprintf(\"%v\", col.GetUint64Val()))\n\t\t\tcase \"decimal\":\n\t\t\t\trecord = append(record, fmt.Sprintf(\"%v\", col.GetDecimalVal().String()))\n\t\t\tcase \"bool\":\n\t\t\t\trecord = append(record, fmt.Sprintf(\"%v\", col.GetBoolVal()))\n\t\t\tcase \"int64\":\n\t\t\t\trecord = append(record, fmt.Sprintf(\"%v\", col.GetInt64Val()))\n\t\t\tcase \"timestamp\":\n\t\t\t\trecord = append(record, fmt.Sprintf(\"%v\", col.GetTimestampVal()))\n\n\t\t\t}\n\t\t}\n\t\terr := writer.Write(record)\n\t\tif err != nil {\n\t\t\treturn errors.Wrapf(err, \"writing row %d\", i)\n\t\t}\n\t}\n\twriter.Flush()\n\treturn errors.Wrap(writer.Error(), \"writing or flushing CSV\")\n}\n\n// tableResponseToCSVString converts a generic TableResponse to a CSV format\n// and returns it as a string.\nfunc tableResponseToCSVString(m *proto.TableResponse) (string, error) {\n\tbuf := &bytes.Buffer{}\n\terr := tableResponseToCSV(m, buf)\n\tif err != nil {\n\t\treturn \"\", errors.Wrap(err, \"writing tableResponse CSV to bytes.Buffer\")\n\t}\n\treturn buf.String(), nil\n}\n\nvar dbDSN string\n\nfunc init() {\n\tflag.StringVar(&dbDSN, \"externalLookupDSN\", \"\", \"SQL DSN to use for external database access\")\n}\n\nfunc TestExternalLookup(t *testing.T) {\n\t// Set up access to a SQL database.\n\tif dbDSN == \"\" {\n\t\tt.Skip(\"no database provided\")\n\t}\n\tdb, err := sql.Open(\"postgres\", dbDSN)\n\tif err != nil {\n\t\tt.Fatalf(\"failed to set up test database: %v\", err)\n\t}\n\tdefer func() {\n\t\tif cerr := db.Close(); cerr != nil {\n\t\t\tt.Errorf(\"failed to close test database: %v\", cerr)\n\t\t}\n\t}()\n\n\t// Set up some data to use in the SQL DB.\n\t// This creates 3 tables:\n\t// - \"lookup\" - which stores an id->string mapping\n\t// - \"misc\" - which has misc non-nullable fields\n\t// - \"nullable\" - to test handling of nullable fields\n\tfunc() {\n\t\t// Set up a write transaction.\n\t\t// This uses a function scope so that the defer is scoped appropriately.\n\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n\t\tdefer cancel()\n\t\ttx, err := db.BeginTx(ctx, nil)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"failed to create transaction: %v\", err)\n\t\t}\n\t\tvar ok bool\n\t\tdefer func() {\n\t\t\tif !ok {\n\t\t\t\trerr := tx.Rollback()\n\t\t\t\tif rerr != nil {\n\t\t\t\t\tt.Errorf(\"failed to roll-back transaction: %v\", rerr)\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\n\t\t// Delete all tables so that we start fresh.\n\t\t_, err = tx.Exec(`DROP INDEX IF EXISTS lookupIndex`)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"deleting lookup index: %v\", err)\n\t\t}\n\t\t_, err = tx.Exec(`DROP TABLE IF EXISTS lookup`)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"deleting lookup table: %v\", err)\n\t\t}\n\t\t_, err = tx.Exec(`DROP TABLE IF EXISTS misc`)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"deleting misc table: %v\", err)\n\t\t}\n\t\t_, err = tx.Exec(`DROP TABLE IF EXISTS nullable`)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"deleting nullable table: %v\", err)\n\t\t}\n\n\t\t_, err = tx.Exec(`CREATE TABLE lookup (\n\t\t\tid int NOT NULL,\n\t\t\tdata text NOT NULL\n\t\t)`)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"failed to create lookup table: %v\", err)\n\t\t}\n\t\t_, err = tx.Exec(`INSERT INTO lookup (id, data) VALUES\n\t\t\t(0, 'h'),\n\t\t\t(1, 'xyzzy'),\n\t\t\t(2, 'plugh'),\n\t\t\t(3, 'wow')\n\t\t`)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"failed to populate lookup table: %v\", err)\n\t\t}\n\t\t_, err = tx.Exec(`CREATE UNIQUE INDEX lookupIndex ON lookup (id);`)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"failed to index lookup table: %v\", err)\n\t\t}\n\n\t\t_, err = tx.Exec(`CREATE TABLE misc (\n\t\t\tid int NOT NULL,\n\t\t\tstringval text NOT NULL,\n\t\t\tboolval boolean NOT NULL,\n\t\t\tintval int NOT NULL\n\t\t)`)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"failed to create misc table: %v\", err)\n\t\t}\n\t\t_, err = tx.Exec(`INSERT INTO misc (id, stringval, boolval, intval) VALUES\n\t\t\t(0, 'h', true, 4),\n\t\t\t(1, 'y', false, 11)\n\t\t`)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"failed to populate misc table: %v\", err)\n\t\t}\n\n\t\t_, err = tx.Exec(`CREATE TABLE nullable (\n\t\t\tid int NOT NULL,\n\t\t\tstringval text,\n\t\t\tboolval boolean,\n\t\t\tintval int\n\t\t)`)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"failed to create nullable table: %v\", err)\n\t\t}\n\t\t_, err = tx.Exec(`INSERT INTO nullable (id, stringval, boolval, intval) VALUES\n\t\t\t(0, 'h', true, 4),\n\t\t\t(1, 'y', false, 11),\n\t\t\t(2, null, null, null),\n\t\t\t(3, null, true, null),\n\t\t\t(4, 'plugh', null, 0)\n\t\t;`)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"failed to populate nullable table: %v\", err)\n\t\t}\n\n\t\terr = tx.Commit()\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"failed to commit DB setup transaction: %v\", err)\n\t\t}\n\n\t\tok = true\n\t}()\n\n\t// Start up a Pilosa cluster with access to the DB.\n\tc := test.MustRunCluster(t, 3, []server.CommandOption{server.OptCommandServerOptions(pilosa.OptServerLookupDB(dbDSN))})\n\tdefer c.Close()\n\n\t// Populate a field with some data that can be used in queries.\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{TrackExistence: true}, \"f\")\n\tc.ImportBits(t, c.Idx(), \"f\", [][2]uint64{\n\t\t{0, 0},\n\t\t{0, 4},\n\t\t{1, 1},\n\t\t{1, 3},\n\t\t{2, 2},\n\t\t{2, 3},\n\t})\n\n\tcases := []struct {\n\t\tname   string\n\t\tquery  string\n\t\texpect pilosa.QueryResponse\n\t}{\n\t\t{\n\t\t\tname:  \"Empty\",\n\t\t\tquery: `ExternalLookup(Union(), query=\"select * from lookup where id = ANY($1)\")`,\n\t\t\texpect: pilosa.QueryResponse{\n\t\t\t\tResults: []interface{}{\n\t\t\t\t\tpilosa.ExtractedTable{},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:  \"ConstRowLookup\",\n\t\t\tquery: `ExternalLookup(ConstRow(columns=[1, 3]), query=\"select id, data from lookup where id = ANY($1)\")`,\n\t\t\texpect: pilosa.QueryResponse{\n\t\t\t\tResults: []interface{}{\n\t\t\t\t\tpilosa.ExtractedTable{\n\t\t\t\t\t\tFields: []pilosa.ExtractedTableField{\n\t\t\t\t\t\t\t{Name: \"data\", Type: \"string\"},\n\t\t\t\t\t\t},\n\t\t\t\t\t\tColumns: []pilosa.ExtractedTableColumn{\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 1},\n\t\t\t\t\t\t\t\tRows:   []interface{}{\"xyzzy\"},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 3},\n\t\t\t\t\t\t\t\tRows:   []interface{}{\"wow\"},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:  \"ComputedFilter\",\n\t\t\tquery: `ExternalLookup(Intersect(Row(f=1), Row(f=2)), query=\"select id, data from lookup where id = ANY($1)\")`,\n\t\t\texpect: pilosa.QueryResponse{\n\t\t\t\tResults: []interface{}{\n\t\t\t\t\tpilosa.ExtractedTable{\n\t\t\t\t\t\tFields: []pilosa.ExtractedTableField{\n\t\t\t\t\t\t\t{Name: \"data\", Type: \"string\"},\n\t\t\t\t\t\t},\n\t\t\t\t\t\tColumns: []pilosa.ExtractedTableColumn{\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 3},\n\t\t\t\t\t\t\t\tRows:   []interface{}{\"wow\"},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:  \"Misc\",\n\t\t\tquery: `ExternalLookup(ConstRow(columns=[0, 1]), query=\"select id, stringval, boolval, intval from misc where id = ANY($1)\")`,\n\t\t\texpect: pilosa.QueryResponse{\n\t\t\t\tResults: []interface{}{\n\t\t\t\t\tpilosa.ExtractedTable{\n\t\t\t\t\t\tFields: []pilosa.ExtractedTableField{\n\t\t\t\t\t\t\t{Name: \"stringval\", Type: \"string\"},\n\t\t\t\t\t\t\t{Name: \"boolval\", Type: \"bool\"},\n\t\t\t\t\t\t\t{Name: \"intval\", Type: \"int64\"},\n\t\t\t\t\t\t},\n\t\t\t\t\t\tColumns: []pilosa.ExtractedTableColumn{\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 0},\n\t\t\t\t\t\t\t\tRows:   []interface{}{\"h\", true, int64(4)},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 1},\n\t\t\t\t\t\t\t\tRows:   []interface{}{\"y\", false, int64(11)},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:  \"Nullable\",\n\t\t\tquery: `ExternalLookup(ConstRow(columns=[0, 1, 2, 3, 4]), query=\"select id, stringval, boolval, intval from nullable where id = ANY($1)\")`,\n\t\t\texpect: pilosa.QueryResponse{\n\t\t\t\tResults: []interface{}{\n\t\t\t\t\tpilosa.ExtractedTable{\n\t\t\t\t\t\tFields: []pilosa.ExtractedTableField{\n\t\t\t\t\t\t\t{Name: \"stringval\", Type: \"string\"},\n\t\t\t\t\t\t\t{Name: \"boolval\", Type: \"bool\"},\n\t\t\t\t\t\t\t{Name: \"intval\", Type: \"int64\"},\n\t\t\t\t\t\t},\n\t\t\t\t\t\tColumns: []pilosa.ExtractedTableColumn{\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 0},\n\t\t\t\t\t\t\t\tRows:   []interface{}{\"h\", true, int64(4)},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 1},\n\t\t\t\t\t\t\t\tRows:   []interface{}{\"y\", false, int64(11)},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 2},\n\t\t\t\t\t\t\t\tRows:   []interface{}{nil, nil, nil},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 3},\n\t\t\t\t\t\t\t\tRows:   []interface{}{nil, true, nil},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tColumn: pilosa.KeyOrID{ID: 4},\n\t\t\t\t\t\t\t\tRows:   []interface{}{\"plugh\", nil, int64(0)},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\tt.Run(\"Query\", func(t *testing.T) {\n\t\tfor _, tc := range cases {\n\t\t\ttc := tc\n\t\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\t\tt.Parallel()\n\n\t\t\t\tresult := c.Query(t, c.Idx(), tc.query)\n\t\t\t\tif tc.expect.SameAs(&result) != nil {\n\t\t\t\t\tt.Errorf(\"expected %v but got %v\", tc.expect, result)\n\t\t\t\t}\n\t\t\t})\n\t\t}\n\t})\n\tt.Run(\"Delete\", func(t *testing.T) {\n\t\tc.Query(t, c.Idx(), `ExternalLookup(All(), query=\"delete from lookup where id = ANY($1)\", write=true)`)\n\t\tres := c.Query(t, c.Idx(), `ExternalLookup(All(), query=\"select id from lookup where id = ANY($1)\")`)\n\t\ttbl := res.Results[0].(pilosa.ExtractedTable)\n\t\tif len(tbl.Columns) != 0 {\n\t\t\tt.Errorf(\"unexpected remaining records: %v\", tbl)\n\t\t}\n\t})\n}\n\nfunc TestToRows(t *testing.T) {\n\tids := &pilosa.RowIdentifiers{\n\t\tRows: []uint64{1, 2, 3},\n\t}\n\tc := ids.Clone()\n\tif c == nil {\n\t\tt.Fatal(\"Shouldn't be nil \")\n\t}\n\te := ids.ToRows(func(*proto.RowResponse) error {\n\t\treturn nil\n\t})\n\tif e != nil {\n\t\tt.Fatal(\"Shouldn't be err \", e)\n\t}\n\n\tkeys := &pilosa.RowIdentifiers{\n\t\tKeys: []string{\"a\", \"b\"},\n\t}\n\tc = keys.Clone()\n\tif c == nil {\n\t\tt.Fatal(\"Shouldn't be nil \")\n\t}\n\te = keys.ToRows(func(*proto.RowResponse) error {\n\t\treturn nil\n\t})\n\tif e != nil {\n\t\tt.Fatal(\"Shouldn't be err \", e)\n\t}\n\tv := &pilosa.ValCount{\n\t\tTimestampVal: time.Now(),\n\t\tCount:        1,\n\t}\n\tx := v.Clone()\n\tif x == nil {\n\t\tt.Fatal(\"Shouldn't be nil \")\n\t}\n\te = v.ToRows(func(*proto.RowResponse) error {\n\t\treturn nil\n\t})\n\tif e != nil {\n\t\tt.Fatal(\"Shouldn't be err \", e)\n\t}\n\tv.DecimalVal = pql.NewDecimal(1, 1).Clone()\n\te = v.ToRows(func(*proto.RowResponse) error {\n\t\treturn nil\n\t})\n\tif e != nil {\n\t\tt.Fatal(\"Shouldn't be err \", e)\n\t}\n\tv.DecimalVal = nil\n\tv.FloatVal = 3.0\n\te = v.ToRows(func(*proto.RowResponse) error {\n\t\treturn nil\n\t})\n\tif e != nil {\n\t\tt.Fatal(\"Shouldn't be err \", e)\n\t}\n\n\tpfi := &pilosa.PairField{\n\t\tPair:  pilosa.Pair{ID: 1, Count: 1},\n\t\tField: \"f\",\n\t}\n\tz := pfi.Clone()\n\tif z.Pair.ID != pfi.Pair.ID {\n\t\tt.Fatal(\"Should be equal \", z, pfi)\n\t}\n\te = pfi.ToRows(func(*proto.RowResponse) error {\n\t\treturn nil\n\t})\n\tif e != nil {\n\t\tt.Fatal(\"Shouldn't be err \", e)\n\t}\n\tpfk := &pilosa.PairField{\n\t\tPair:  pilosa.Pair{Key: \"a\", Count: 1},\n\t\tField: \"f\",\n\t}\n\to := pfk.Clone()\n\tif o.Pair.Key != pfk.Pair.Key {\n\t\tt.Fatal(\"Should be equal \")\n\t}\n\te = pfk.ToRows(func(*proto.RowResponse) error {\n\t\treturn nil\n\t})\n\tif e != nil {\n\t\tt.Fatal(\"Shouldn't be err \", e)\n\t}\n\tpfs := &pilosa.PairsField{\n\t\tPairs: []pilosa.Pair{{ID: 1, Count: 1}},\n\t\tField: \"f\",\n\t}\n\tf := pfs.Clone()\n\tif f.Pairs[0].ID != pfs.Pairs[0].ID {\n\t\tt.Fatal(\"Should be equal \")\n\t}\n\te = pfs.ToRows(func(*proto.RowResponse) error {\n\t\treturn nil\n\t})\n\tif e != nil {\n\t\tt.Fatal(\"Shouldn't be err \", e)\n\t}\n\n\tr4 := server.ResultUint64(1)\n\te = r4.ToRows(func(*proto.RowResponse) error {\n\t\treturn nil\n\t})\n\tif e != nil {\n\t\tt.Fatal(\"Shouldn't be err \", e)\n\t}\n}\n\n// TestMinMaxTimestampVariableNode tests Min() and Max() queries on\n// timestamp values on a clusters of varying size.\nfunc TestMinMaxTimestampVariableNode(t *testing.T) {\n\tfor i := 1; i < 4; i++ {\n\t\tt.Run(fmt.Sprintf(\"%d\", i), func(t *testing.T) {\n\t\t\t// separate function so we can defer closing the cluster\n\t\t\t// and so it looks prettier\n\t\t\tMinMaxTimestampNodeTester(t, i)\n\t\t})\n\t}\n}\n\n// MinMaxTimestampNodeTester tests Min() and Max() queries on\n// timestamp values on a cluster with `numNodes` nodes.\n// fails if the min or max values are not correct\nfunc MinMaxTimestampNodeTester(t *testing.T, numNodes int) {\n\tc := test.MustRunCluster(t, numNodes)\n\tdefer c.Close()\n\tindex := c.Idx(\"tsidx\")\n\tfield := \"ts\"\n\n\t// create an index and timestamp field\n\tc.CreateField(t, index, pilosa.IndexOptions{TrackExistence: true}, field, pilosa.OptFieldTypeTimestamp(time.Unix(0, 0), \"s\"))\n\n\t// add some data\n\texpected := \"2010-01-02T12:32:00Z\"\n\tc.Query(t, index, \"Set(10, ts=\\\"\"+expected+\"\\\")\")\n\n\t// run a query on the cluster\n\tmin := c.Query(t, index, \"Min(\"+field+\")\").Results[0].(pilosa.ValCount).TimestampVal.Format(time.RFC3339Nano)\n\tif min != expected {\n\t\tt.Fatalf(\"incorrect min timestamp val. expected: %v, got %v\\n\", expected, min)\n\t}\n\n\tmax := c.Query(t, index, \"Max(\"+field+\")\").Results[0].(pilosa.ValCount).TimestampVal.Format(time.RFC3339Nano)\n\tif max != expected {\n\t\tt.Fatalf(\"incorrect max timestamp val. expected: %v, got %v\\n\", expected, min)\n\t}\n}\n\n// DistinctTimestamp ToRows should properly encode the timestamp\nfunc TestDistinctTimestampToRows(t *testing.T) {\n\td := pilosa.DistinctTimestamp{\n\t\tValues: []string{\n\t\t\t\"2022-03-24T12:08:37Z\",\n\t\t\t\"2022-03-24T12:08:47Z\",\n\t\t\t\"2022-03-24T12:08:57Z\",\n\t\t},\n\t\tName: \"timestamp\",\n\t}\n\n\texpectedHeaders := []*proto.ColumnInfo{\n\t\t{\n\t\t\tName:     d.Name,\n\t\t\tDatatype: \"timestamp\",\n\t\t},\n\t}\n\texpected := []*proto.RowResponse{\n\t\t{\n\t\t\tHeaders: expectedHeaders,\n\t\t\tColumns: []*proto.ColumnResponse{\n\t\t\t\t{\n\t\t\t\t\tColumnVal: &proto.ColumnResponse_TimestampVal{\n\t\t\t\t\t\tTimestampVal: \"2022-03-24T12:08:37Z\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tHeaders: expectedHeaders,\n\t\t\tColumns: []*proto.ColumnResponse{\n\t\t\t\t{\n\t\t\t\t\tColumnVal: &proto.ColumnResponse_TimestampVal{\n\t\t\t\t\t\tTimestampVal: \"2022-03-24T12:08:47Z\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tHeaders: expectedHeaders,\n\t\t\tColumns: []*proto.ColumnResponse{\n\t\t\t\t{\n\t\t\t\t\tColumnVal: &proto.ColumnResponse_TimestampVal{\n\t\t\t\t\t\tTimestampVal: \"2022-03-24T12:08:57Z\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\trows := []*proto.RowResponse{}\n\td.ToRows(func(r *proto.RowResponse) error {\n\t\trows = append(rows, r)\n\t\treturn nil\n\t})\n\n\tfor i, row := range rows {\n\t\tif !reflect.DeepEqual(row, expected[i]) {\n\t\t\tt.Errorf(\"expected %v, got %v\", expected[i], row)\n\t\t}\n\t}\n}\n\nfunc TestExecutor_Execute_ExtractWithTime(t *testing.T) {\n\tts := func(t time.Time) int64 {\n\t\treturn t.Unix() * 1e+9\n\t}\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{Keys: true, TrackExistence: true}, \"segment\", pilosa.OptFieldKeys(), pilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"D\"), \"0\"))\n\tc.ImportTimeQuantumKey(t, c.Idx(), \"segment\", []test.TimeQuantumKey{\n\t\t// from edge cases\n\t\t{ColKey: \"C1\", RowKey: \"R1\", Ts: ts(time.Date(2022, 7, 1, 0, 0, 0, 0, time.UTC))},\n\t\t{ColKey: \"C2\", RowKey: \"R1\", Ts: ts(time.Date(2022, 7, 3, 0, 0, 0, 0, time.UTC))},\n\t})\n\n\tt.Run(\"Extract With From Time\", func(t *testing.T) {\n\t\tresp := c.Query(t, c.Idx(), \"Extract(All(), Rows(segment,from=2022-07-03T00:00))\")\n\t\t// resp := c.Query(t, c.Idx(), \"Extract(All(), Rows(segment, from=))\")\n\t\tif len(resp.Results) != 1 {\n\t\t\tt.Fatalf(\"expected 1 result but got %d\", len(resp.Results))\n\t\t}\n\t\tgot, ok := resp.Results[0].(pilosa.ExtractedTable)\n\t\tif !ok {\n\t\t\tt.Fatalf(\"expected a table result but got %T\", resp.Results[0])\n\t\t}\n\t\texpect := pilosa.ExtractedTable{\n\t\t\tFields: []pilosa.ExtractedTableField{{Name: \"segment\", Type: \"[]string\"}},\n\t\t\tColumns: []pilosa.ExtractedTableColumn{\n\t\t\t\t{\n\t\t\t\t\tColumn: pilosa.KeyOrID{\n\t\t\t\t\t\tKey:   \"C1\",\n\t\t\t\t\t\tKeyed: true,\n\t\t\t\t\t},\n\t\t\t\t\tRows: []interface{}{[]string{}},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tColumn: pilosa.KeyOrID{\n\t\t\t\t\t\tKey:   \"C2\",\n\t\t\t\t\t\tKeyed: true,\n\t\t\t\t\t},\n\t\t\t\t\tRows: []interface{}{[]string{\"R1\"}},\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t\tif !reflect.DeepEqual(expect, got) {\n\t\t\tt.Errorf(\"expected %v but got %v\", expect, got)\n\t\t}\n\t})\n\tt.Run(\"Extract With Time No Opt\", func(t *testing.T) {\n\t\tresp := c.Query(t, c.Idx(), \"Extract(All(), Rows(segment))\")\n\t\t// resp := c.Query(t, c.Idx(), \"Extract(All(), Rows(segment, from=))\")\n\t\tif len(resp.Results) != 1 {\n\t\t\tt.Fatalf(\"expected 1 result but got %d\", len(resp.Results))\n\t\t}\n\t\tgot, ok := resp.Results[0].(pilosa.ExtractedTable)\n\t\tif !ok {\n\t\t\tt.Fatalf(\"expected a table result but got %T\", resp.Results[0])\n\t\t}\n\t\texpect := pilosa.ExtractedTable{\n\t\t\tFields: []pilosa.ExtractedTableField{{Name: \"segment\", Type: \"[]string\"}},\n\t\t\tColumns: []pilosa.ExtractedTableColumn{\n\t\t\t\t{\n\t\t\t\t\tColumn: pilosa.KeyOrID{\n\t\t\t\t\t\tKey:   \"C1\",\n\t\t\t\t\t\tKeyed: true,\n\t\t\t\t\t},\n\t\t\t\t\tRows: []interface{}{[]string{\"R1\"}},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tColumn: pilosa.KeyOrID{\n\t\t\t\t\t\tKey:   \"C2\",\n\t\t\t\t\t\tKeyed: true,\n\t\t\t\t\t},\n\t\t\t\t\tRows: []interface{}{[]string{\"R1\"}},\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t\tif !reflect.DeepEqual(expect, got) {\n\t\t\tt.Errorf(\"expected %v but got %v\", expect, got)\n\t\t}\n\t})\n\n\tt.Run(\"Extract With ToTime \", func(t *testing.T) {\n\t\tresp := c.Query(t, c.Idx(), \"Extract(All(), Rows(segment,to=2022-07-02T00:00))\")\n\t\t// resp := c.Query(t, c.Idx(), \"Extract(All(), Rows(segment, from=))\")\n\t\tif len(resp.Results) != 1 {\n\t\t\tt.Fatalf(\"expected 1 result but got %d\", len(resp.Results))\n\t\t}\n\t\tgot, ok := resp.Results[0].(pilosa.ExtractedTable)\n\t\tif !ok {\n\t\t\tt.Fatalf(\"expected a table result but got %T\", resp.Results[0])\n\t\t}\n\t\texpect := pilosa.ExtractedTable{\n\t\t\tFields: []pilosa.ExtractedTableField{{Name: \"segment\", Type: \"[]string\"}},\n\t\t\tColumns: []pilosa.ExtractedTableColumn{\n\t\t\t\t{\n\t\t\t\t\tColumn: pilosa.KeyOrID{\n\t\t\t\t\t\tKey:   \"C1\",\n\t\t\t\t\t\tKeyed: true,\n\t\t\t\t\t},\n\t\t\t\t\tRows: []interface{}{[]string{\"R1\"}},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tColumn: pilosa.KeyOrID{\n\t\t\t\t\t\tKey:   \"C2\",\n\t\t\t\t\t\tKeyed: true,\n\t\t\t\t\t},\n\t\t\t\t\tRows: []interface{}{[]string{}},\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t\tif !reflect.DeepEqual(expect, got) {\n\t\t\tt.Errorf(\"expected %v but got %v\", expect, got)\n\t\t}\n\t})\n}\n\nfunc TestExecutorTimeRange(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\n\t// test error path - field is a not a time field, from/to options not allowed in query\n\tt.Run(\"Field not a time field\", func(t *testing.T) {\n\t\twriteQuery := `\n\t\tSet(1, f=1)\n\t\tSet(2, f=1)`\n\t\treadQueries := []string{\n\t\t\t`Row(f=1, from=1999-12-31T00:00, to=2002-01-01T03:00)`,\n\t\t\t`Row(f=1, from=1999-12-31T00:00)`,\n\t\t\t`Row(f=1, to=2002-01-01T02:00)`,\n\t\t}\n\t\tindexName := c.Idx(t.Name())\n\t\thldr := c.GetHolder(0)\n\t\tindex, err := hldr.CreateIndex(indexName, \"\", pilosa.IndexOptions{})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\t_, err = index.CreateField(\"f\", \"\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t_, err = c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{\n\t\t\tIndex: indexName,\n\t\t\tQuery: writeQuery,\n\t\t})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tfor _, query := range readQueries {\n\t\t\t_, err := c.GetNode(0).API.Query(context.Background(),\n\t\t\t\t&pilosa.QueryRequest{\n\t\t\t\t\tIndex: indexName,\n\t\t\t\t\tQuery: query,\n\t\t\t\t})\n\t\t\tif !strings.Contains(err.Error(), \"not a time-field, 'from' and 'to' are not valid options for this field type\") {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t}\n\t})\n\n\t// test standard view disabled\n\t// if from/to in query, return union of views\n\t// if from/to not in query, return union of all views\n\tt.Run(\"Standard View Disabled\", func(t *testing.T) {\n\t\twriteQuery := `\n\t\t\tSet(2, f=1, 1999-12-31T00:00)\n\t\t\tSet(3, f=2, 2000-01-01T00:00)\n\t\t\tSet(4, f=3, 2000-01-02T00:00)\n\t\t\tSet(5, f=1, 2001-01-01T00:00)\n\t\t\tSet(6, f=1, 2006-01-01T00:00)`\n\t\treadQueries := []string{\n\t\t\t`Row(f=1, from=1999-12-31T00:00, to=2002-01-01T03:00)`,\n\t\t\t`Row(f=1, from=1999-12-31T00:00)`,\n\t\t\t`Row(f=1, to=2002-01-01T02:00)`,\n\t\t\t`Row(f=1)`,\n\t\t}\n\n\t\texpResp := [][]uint64{\n\t\t\t{2, 5},\n\t\t\t{2, 5, 6},\n\t\t\t{2, 5},\n\t\t\t{2, 5, 6},\n\t\t}\n\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\tnil, pilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMDH\"), \"0\", true))\n\n\t\tfor i, exp := range expResp {\n\t\t\tif columns := responses[i].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, exp) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v for query: %+v\", columns, readQueries[i])\n\t\t\t}\n\t\t}\n\t})\n\n\t// test standard view enabled\n\t// if from/to in query, return union of views\n\t// if from/to not in query, return union of all views\n\tt.Run(\"Standard View Enabled\", func(t *testing.T) {\n\t\twriteQuery := `\n\t\t\t\tSet(2, f=1, 1999-12-31T00:00)\n\t\t\t\tSet(3, f=2, 2000-01-01T00:00)\n\t\t\t\tSet(4, f=3, 2000-01-02T00:00)\n\t\t\t\tSet(5, f=1, 2001-01-01T00:00)\n\t\t\t\tSet(6, f=1, 2006-01-01T00:00)\n\t\t\t\tSet(7, f=1, 2010-01-01T00:00)`\n\t\treadQueries := []string{\n\t\t\t`Row(f=1, from=1999-12-31T00:00, to=2002-01-01T03:00)`,\n\t\t\t`Row(f=1, from=1999-12-31T00:00)`,\n\t\t\t`Row(f=1, to=2002-01-01T02:00)`,\n\t\t\t`Row(f=1)`,\n\t\t}\n\n\t\texpectedResponse := [][]uint64{\n\t\t\t{2, 5},\n\t\t\t{2, 5, 6, 7},\n\t\t\t{2, 5},\n\t\t\t{2, 5, 6, 7},\n\t\t}\n\t\tresponses := runCallTest(c, t, writeQuery, readQueries,\n\t\t\tnil, pilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMDH\"), \"0\", false))\n\n\t\tfor i, exp := range expectedResponse {\n\t\t\tif columns := responses[i].Results[0].(*pilosa.Row).Columns(); !reflect.DeepEqual(columns, exp) {\n\t\t\t\tt.Fatalf(\"unexpected columns: %+v for query: %+v\", columns, readQueries[i])\n\t\t\t}\n\t\t}\n\t})\n}\n"
        },
        {
          "name": "extendiblehash",
          "type": "tree",
          "content": null
        },
        {
          "name": "field.go",
          "type": "blob",
          "size": 74.427734375,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"math\"\n\t\"math/bits\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\t\"unicode\"\n\n\t\"github.com/featurebasedb/featurebase/v3/pql\"\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\t\"github.com/featurebasedb/featurebase/v3/testhook\"\n\t\"github.com/featurebasedb/featurebase/v3/tracing\"\n\t\"github.com/pkg/errors\"\n)\n\n// Default field settings.\nconst (\n\tDefaultFieldType = FieldTypeSet\n\n\tDefaultCacheType = CacheTypeRanked\n\n\t// Default ranked field cache\n\tDefaultCacheSize = 50000\n\n\tbitsPerWord = 32 << (^uint(0) >> 63) // either 32 or 64\n\tmaxInt      = 1<<(bitsPerWord-1) - 1 // either 1<<31 - 1 or 1<<63 - 1\n\n)\n\n// Field types.\nconst (\n\tFieldTypeSet       = \"set\"\n\tFieldTypeInt       = \"int\"\n\tFieldTypeTime      = \"time\"\n\tFieldTypeMutex     = \"mutex\"\n\tFieldTypeBool      = \"bool\"\n\tFieldTypeDecimal   = \"decimal\"\n\tFieldTypeTimestamp = \"timestamp\"\n)\n\ntype protected struct {\n\tmu       sync.Mutex\n\tduration time.Duration\n}\n\nfunc (p *protected) Set(d time.Duration) {\n\tp.mu.Lock()\n\tdefer p.mu.Unlock()\n\tp.duration = d\n}\nfunc (p *protected) Get() time.Duration {\n\tp.mu.Lock()\n\tdefer p.mu.Unlock()\n\treturn p.duration\n}\n\nvar availableShardFileFlushDuration = &protected{\n\tduration: 5 * time.Second,\n}\n\n// Field represents a container for views.\ntype Field struct {\n\tmu            sync.RWMutex\n\tcreatedAt     int64\n\towner         string\n\tpath          string\n\tindex         string\n\tname          string\n\tqualifiedName string\n\n\tidx *Index\n\n\tviewMap map[string]*view\n\n\tbroadcaster broadcaster\n\tserializer  Serializer\n\n\t// Field options.\n\toptions FieldOptions\n\n\tbsiGroups []*bsiGroup\n\n\t// Shards with data on any node in the cluster, according to this node.\n\tremoteAvailableShardsMu sync.Mutex\n\tremoteAvailableShards   *roaring.Bitmap\n\n\ttranslateStore TranslateStore\n\n\t// Instantiates new translation stores\n\tOpenTranslateStore OpenTranslateStoreFunc\n\n\t// Used for looking up a foreign index.\n\tholder *Holder\n\n\t// Stores whether or not the field has keys enabled.\n\t// This is most helpful for cases where the keys are\n\t// based on a foreign index; this prevents having to\n\t// call holder.index.Keys() every time.\n\tusesKeys bool\n\n\t// Synchronization primitives needed for async writing of\n\t// the remoteAvailableShards\n\tavailableShardChan chan struct{}\n\twg                 sync.WaitGroup\n\n\t// track whether we're shutting down\n\tclosing chan struct{}\n}\n\n// FieldOption is a functional option type for pilosa.fieldOptions.\ntype FieldOption func(fo *FieldOptions) error\n\n// OptFieldKeys is a functional option on FieldOptions\n// used to specify whether keys are used for this field.\nfunc OptFieldKeys() FieldOption {\n\treturn func(fo *FieldOptions) error {\n\t\tfo.Keys = true\n\t\treturn nil\n\t}\n}\n\n// OptFieldForeignIndex marks this field as a foreign key to another\n// index. That is, the values of this field should be interpreted as\n// referencing records (Pilosa columns) in another index. TODO explain\n// where/how this is used by Pilosa.\nfunc OptFieldForeignIndex(index string) FieldOption {\n\treturn func(fo *FieldOptions) error {\n\t\tfo.ForeignIndex = index\n\t\treturn nil\n\t}\n}\n\n// OptFieldTypeDefault is a functional option on FieldOptions\n// used to set the field type and cache setting to the default values.\nfunc OptFieldTypeDefault() FieldOption {\n\treturn func(fo *FieldOptions) error {\n\t\tif fo.Type != \"\" {\n\t\t\treturn errors.Errorf(\"field type is already set to: %s\", fo.Type)\n\t\t}\n\t\tfo.Type = FieldTypeSet\n\t\tfo.CacheType = DefaultCacheType\n\t\tfo.CacheSize = DefaultCacheSize\n\t\treturn nil\n\t}\n}\n\n// OptFieldTypeSet is a functional option on FieldOptions\n// used to specify the field as being type `set` and to\n// provide any respective configuration values.\nfunc OptFieldTypeSet(cacheType string, cacheSize uint32) FieldOption {\n\treturn func(fo *FieldOptions) error {\n\t\tif fo.Type != \"\" {\n\t\t\treturn errors.Errorf(\"field type is already set to: %s\", fo.Type)\n\t\t}\n\t\tfo.Type = FieldTypeSet\n\t\tfo.CacheType = cacheType\n\t\tfo.CacheSize = cacheSize\n\t\treturn nil\n\t}\n}\n\n// OptFieldTypeInt is a functional option on FieldOptions\n// used to specify the field as being type `int` and to\n// provide any respective configuration values.\nfunc OptFieldTypeInt(min, max int64) FieldOption {\n\treturn func(fo *FieldOptions) error {\n\t\tif fo.Type != \"\" {\n\t\t\treturn errors.Errorf(\"field type is already set to: %s\", fo.Type)\n\t\t}\n\t\tif min > max {\n\t\t\treturn errors.New(\"int field min cannot be greater than max\")\n\t\t}\n\t\tfo.Type = FieldTypeInt\n\t\tfo.Min = pql.NewDecimal(min, 0)\n\t\tfo.Max = pql.NewDecimal(max, 0)\n\t\tfo.Base = bsiBase(min, max)\n\t\treturn nil\n\t}\n}\n\n// OptFieldTypeTimestamp is a functional option on FieldOptions\n// used to specify the field as being type `timestamp` and to\n// provide any respective configuration values.\nfunc OptFieldTypeTimestamp(epoch time.Time, timeUnit string) FieldOption {\n\treturn func(fo *FieldOptions) error {\n\t\tif fo.Type != \"\" {\n\t\t\treturn errors.Errorf(\"field type is already set to: %s\", fo.Type)\n\t\t}\n\n\t\tminTime := MinTimestamp\n\t\tmaxTime := MaxTimestamp\n\n\t\tvar base, minInt, maxInt int64\n\t\tswitch timeUnit {\n\t\tcase TimeUnitSeconds:\n\t\t\tbase = epoch.Unix()\n\t\t\tminInt = minTime.Unix() - base\n\t\t\tmaxInt = maxTime.Unix() - base\n\t\tcase TimeUnitMilliseconds:\n\t\t\tbase = epoch.UnixMilli()\n\t\t\tminInt = minTime.UnixMilli() - base\n\t\t\tmaxInt = maxTime.UnixMilli() - base\n\t\tcase TimeUnitMicroseconds, TimeUnitUSeconds:\n\t\t\tbase = epoch.UnixMicro()\n\t\t\tminInt = minTime.UnixMicro() - base\n\t\t\tmaxInt = maxTime.UnixMicro() - base\n\t\tcase TimeUnitNanoseconds:\n\t\t\t// Note: For nano, the min and max values are also the min and max integer\n\t\t\t// values we support. Also, keep in mind that MinNano is a negative\n\t\t\t// number. So if base is positive and we do MinNano - base...it would increase minInt\n\t\t\t// beyond what we support. This isn't an issue with larger granularities.\n\t\t\tbase = epoch.UnixNano()\n\t\t\tif base > 0 {\n\t\t\t\tmaxInt = MaxTimestampNano.UnixNano() - base\n\t\t\t\tminInt = MinTimestampNano.UnixNano()\n\t\t\t} else {\n\t\t\t\tmaxInt = MaxTimestampNano.UnixNano()\n\t\t\t\tminInt = MinTimestampNano.UnixNano() - base\n\t\t\t}\n\t\t\tminTime = MinTimestampNano\n\t\t\tmaxTime = MaxTimestampNano\n\t\tdefault:\n\t\t\treturn errors.Errorf(\"invalid time unit: '%q'\", fo.TimeUnit)\n\t\t}\n\n\t\tif err := CheckEpochOutOfRange(epoch, minTime, maxTime); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tfo.Type = FieldTypeTimestamp\n\t\tfo.TimeUnit = timeUnit\n\t\tfo.Base = base\n\t\tfo.Min = pql.NewDecimal(minInt, 0)\n\t\tfo.Max = pql.NewDecimal(maxInt, 0)\n\n\t\treturn nil\n\t}\n\n}\n\n// OptFieldTypeDecimal is a functional option for creating a `decimal` field.\n// Unless we decide to expand the range of supported values, `scale` is\n// restricted to the range [0,19]. This supports anything from:\n//\n// scale = 0:\n// min: -9223372036854775808.\n// max:  9223372036854775807.\n//\n// to:\n//\n// scale = 19:\n// min: -0.9223372036854775808\n// max:  0.9223372036854775807\n//\n// While it's possible to support scale values outside of this range,\n// the coverage for those scales are no longer continuous. For example,\n//\n// scale = -2:\n// min : [-922337203685477580800, -100]\n// GAPs: [-99, -1], [-199, -101] ... [-922337203685477580799, -922337203685477580701]\n//\n//\t0\n//\n// max : [100, 922337203685477580700]\n// GAPs: [1, 99], [101, 199] ... [922337203685477580601, 922337203685477580699]\n//\n// An alternative to this gap strategy would be to scale the supported range\n// to a continuous 64-bit space (which is not unreasonable using bsiGroup.Base).\n// The issue with this approach is that we would need to know which direction\n// to favor. For example, there are two possible ranges for `scale = -2`:\n//\n// min : [-922337203685477580800, -922337203685477580800+(2^64)]\n// max : [922337203685477580700-(2^64), 922337203685477580700]\nfunc OptFieldTypeDecimal(scale int64, minmax ...pql.Decimal) FieldOption {\n\treturn func(fo *FieldOptions) error {\n\t\tif fo.Type != \"\" {\n\t\t\treturn errors.Errorf(\"can't set field type to 'decimal', already set to: %s\", fo.Type)\n\t\t}\n\t\tif scale < 0 || scale > 19 {\n\t\t\treturn errors.Errorf(\"scale values outside the range [0,19] are not supported: %d\", scale)\n\t\t}\n\n\t\tfo.Min, fo.Max = pql.MinMax(scale)\n\t\tif len(minmax) == 2 {\n\t\t\tmin := minmax[0]\n\t\t\tmax := minmax[1]\n\t\t\tif !min.IsValid() || !max.IsValid() {\n\t\t\t\treturn errors.Errorf(\"min/max range %s-%s is not supported\", min, max)\n\t\t\t} else if !min.SupportedByScale(scale) || !max.SupportedByScale(scale) {\n\t\t\t\treturn errors.Errorf(\"min/max range %s-%s is not supported by scale %d\", min, max, scale)\n\t\t\t} else if min.GreaterThan(max) {\n\t\t\t\treturn errors.Errorf(\"decimal field min cannot be greater than max, got %s, %s\", min, max)\n\t\t\t}\n\t\t\tfo.Min = min\n\t\t\tfo.Max = max\n\t\t} else if len(minmax) > 2 {\n\t\t\treturn errors.Errorf(\"unknown extra parameters beyond min and max: %v\", minmax)\n\t\t} else if len(minmax) == 1 {\n\t\t\tmin := minmax[0]\n\t\t\tif !min.IsValid() {\n\t\t\t\treturn errors.Errorf(\"min %s is not supported\", min)\n\t\t\t} else if !min.SupportedByScale(scale) {\n\t\t\t\treturn errors.Errorf(\"min %s is not supported by scale %d\", min, scale)\n\t\t\t}\n\t\t\tfo.Min = min\n\t\t}\n\t\tfo.Type = FieldTypeDecimal\n\t\tfo.Base = bsiBase(fo.Min.ToInt64(scale), fo.Max.ToInt64(scale))\n\t\tfo.Scale = scale\n\t\treturn nil\n\t}\n}\n\n// OptFieldTypeTime is a functional option on FieldOptions\n// used to specify the field as being type `time` and to\n// provide any respective configuration values.\n// Pass true to skip creation of the standard view.\nfunc OptFieldTypeTime(timeQuantum TimeQuantum, ttl string, opt ...bool) FieldOption {\n\treturn func(fo *FieldOptions) error {\n\t\tif fo.Type != \"\" {\n\t\t\treturn errors.Errorf(\"field type is already set to: %s\", fo.Type)\n\t\t}\n\t\tif !timeQuantum.Valid() {\n\t\t\treturn ErrInvalidTimeQuantum\n\t\t}\n\t\tfo.Type = FieldTypeTime\n\t\tfo.TimeQuantum = timeQuantum\n\t\tttlParsed, err := time.ParseDuration(ttl)\n\t\tif err != nil {\n\t\t\treturn errors.Errorf(\"cannot parse ttl: %s\", ttl)\n\t\t}\n\t\tif ttlParsed < 0 {\n\t\t\treturn errors.Errorf(\"ttl can't be negative: %s\", ttl)\n\t\t}\n\t\tfo.TTL = ttlParsed\n\t\tfo.NoStandardView = len(opt) >= 1 && opt[0]\n\t\treturn nil\n\t}\n}\n\n// OptFieldTypeMutex is a functional option on FieldOptions\n// used to specify the field as being type `mutex` and to\n// provide any respective configuration values.\nfunc OptFieldTypeMutex(cacheType string, cacheSize uint32) FieldOption {\n\treturn func(fo *FieldOptions) error {\n\t\tif fo.Type != \"\" {\n\t\t\treturn errors.Errorf(\"field type is already set to: %s\", fo.Type)\n\t\t}\n\t\tfo.Type = FieldTypeMutex\n\t\tfo.CacheType = cacheType\n\t\tfo.CacheSize = cacheSize\n\t\treturn nil\n\t}\n}\n\n// OptFieldTypeBool is a functional option on FieldOptions\n// used to specify the field as being type `bool` and to\n// provide any respective configuration values.\nfunc OptFieldTypeBool() FieldOption {\n\treturn func(fo *FieldOptions) error {\n\t\tif fo.Type != \"\" {\n\t\t\treturn errors.Errorf(\"field type is already set to: %s\", fo.Type)\n\t\t}\n\t\tfo.Type = FieldTypeBool\n\t\treturn nil\n\t}\n}\n\n// OptFieldTrackExistence exists mostly to allow the\n// FieldFromFieldOptions/FieldOptionsFromField round-trip to work.\n// If you are actually creating a field, via api.CreateField,\n// it will be turned on unconditionally. You can't turn it\n// off.\nfunc OptFieldTrackExistence() FieldOption {\n\treturn func(fo *FieldOptions) error {\n\t\tfo.TrackExistence = true\n\t\treturn nil\n\t}\n}\n\n// newField returns a new instance of field (without name validation).\nfunc newField(holder *Holder, path, index, name string, opts ...FieldOption) (*Field, error) {\n\t// Apply functional option.\n\tfo := FieldOptions{}\n\tfor _, opt := range opts {\n\t\terr := opt(&fo)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"applying option\")\n\t\t}\n\t}\n\n\tf := &Field{\n\t\tpath:          path,\n\t\tindex:         index,\n\t\tname:          name,\n\t\tqualifiedName: FormatQualifiedFieldName(index, name),\n\n\t\tviewMap: make(map[string]*view),\n\n\t\tbroadcaster: NopBroadcaster,\n\t\tserializer:  NopSerializer,\n\n\t\toptions: applyDefaultOptions(&fo),\n\n\t\tremoteAvailableShards: roaring.NewBitmap(),\n\n\t\tholder: holder,\n\n\t\tOpenTranslateStore: OpenInMemTranslateStore,\n\t}\n\n\treturn f, nil\n}\n\n// Name returns the name the field was initialized with.\nfunc (f *Field) Name() string { return f.name }\n\n// CreatedAt is an timestamp for a specific version of field.\nfunc (f *Field) CreatedAt() int64 {\n\tf.mu.RLock()\n\tdefer f.mu.RUnlock()\n\n\treturn f.createdAt\n}\n\n// Index returns the index name the field was initialized with.\nfunc (f *Field) Index() string { return f.index }\n\n// Path returns the path the field was initialized with.\nfunc (f *Field) Path() string { return f.path }\n\n// TranslateStorePath returns the translation database path for the field.\nfunc (f *Field) TranslateStorePath() string {\n\treturn filepath.Join(f.path, \"keys\")\n}\n\n// TranslateStore returns the field's translation store.\nfunc (f *Field) TranslateStore() TranslateStore {\n\treturn f.translateStore\n}\n\n// AvailableShards returns a bitmap of shards that contain data.\nfunc (f *Field) AvailableShards(localOnly bool) *roaring.Bitmap {\n\tf.mu.RLock()\n\tdefer f.mu.RUnlock()\n\tf.remoteAvailableShardsMu.Lock()\n\tdefer f.remoteAvailableShardsMu.Unlock()\n\n\tvar b *roaring.Bitmap\n\tif localOnly {\n\t\tb = roaring.NewBitmap()\n\t} else {\n\t\tb = f.remoteAvailableShards.Clone()\n\t}\n\tfor viewname, view := range f.viewMap {\n\t\tavailableShards := view.availableShards()\n\t\tif availableShards == nil || availableShards.Containers == nil {\n\t\t\tf.holder.Logger.Warnf(\"empty available shards for view: %s on field %s available shards: %v\", viewname, f.name, availableShards)\n\t\t\tcontinue\n\t\t}\n\t\tb.UnionInPlace(view.availableShards())\n\t}\n\treturn b\n}\n\n// LocalAvailableShards returns a bitmap of shards that contain data, but\n// only from the local node. This prevents txfactory from making\n// db-per-shard for remote shards.\nfunc (f *Field) LocalAvailableShards() *roaring.Bitmap {\n\tf.mu.RLock()\n\tdefer f.mu.RUnlock()\n\n\tb := roaring.NewBitmap()\n\tfor _, view := range f.viewMap {\n\t\tb.UnionInPlace(view.availableShards())\n\t}\n\treturn b\n}\n\n// AddRemoteAvailableShards merges the set of available shards into the current known set\n// and saves the set to a file.\nfunc (f *Field) AddRemoteAvailableShards(b *roaring.Bitmap) error {\n\tf.mergeRemoteAvailableShards(b)\n\t// Save the updated bitmap to the data store.\n\treturn f.saveAvailableShards()\n}\n\n// mergeRemoteAvailableShards merges the set of available shards into the current known set.\nfunc (f *Field) mergeRemoteAvailableShards(b *roaring.Bitmap) {\n\tf.remoteAvailableShardsMu.Lock()\n\tdefer f.remoteAvailableShardsMu.Unlock()\n\tf.remoteAvailableShards = f.remoteAvailableShards.Union(b)\n}\n\n// loadAvailableShards reads remoteAvailableShards data for the field, if any.\nfunc (f *Field) loadAvailableShards() error {\n\tshards, err := f.holder.sharder.Shards(context.Background(), f.index, f.name)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"loading available shards\")\n\t}\n\n\tbm := roaring.NewBitmap()\n\tfor _, s := range shards {\n\t\tb := roaring.NewBitmap()\n\t\tif err = b.UnmarshalBinary(s); err != nil {\n\t\t\treturn errors.Wrap(err, \"available shards corrupt\")\n\t\t}\n\t\tbm.UnionInPlace(b)\n\t}\n\t// Merge bitmap from file into field.\n\tf.mergeRemoteAvailableShards(bm)\n\n\treturn nil\n}\n\n// saveAvailableShards writes remoteAvailableShards data for the field.\nfunc (f *Field) saveAvailableShards() error {\n\tselect {\n\tcase f.availableShardChan <- struct{}{}:\n\tdefault:\n\t}\n\treturn nil\n}\n\n// RemoveAvailableShard removes a shard from the bitmap cache.\n//\n// NOTE: This can be overridden on the next sync so all nodes should be updated.\nfunc (f *Field) RemoveAvailableShard(v uint64) error {\n\tf.remoteAvailableShardsMu.Lock()\n\tdefer f.remoteAvailableShardsMu.Unlock()\n\n\tb := f.remoteAvailableShards.Clone()\n\tif _, err := b.Remove(v); err != nil {\n\t\treturn err\n\t}\n\tf.remoteAvailableShards = b\n\n\treturn f.saveAvailableShards()\n}\n\n// Type returns the field type.\nfunc (f *Field) Type() string {\n\tf.mu.RLock()\n\tdefer f.mu.RUnlock()\n\treturn f.options.Type\n}\n\n// CacheSize returns the ranked field cache size.\nfunc (f *Field) CacheSize() uint32 {\n\tf.mu.RLock()\n\tv := f.options.CacheSize\n\tf.mu.RUnlock()\n\treturn v\n}\n\n// Options returns all options for this field.\nfunc (f *Field) Options() FieldOptions {\n\tf.mu.RLock()\n\tdefer f.mu.RUnlock()\n\treturn f.options\n}\n\n// Open opens and initializes the field.\nfunc (f *Field) Open() error {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\tif err := func() (err error) {\n\t\t// Ensure the field's path exists.\n\t\tf.holder.Logger.Debugf(\"ensure field path exists: %s\", f.path)\n\t\tif err := os.MkdirAll(f.path, 0750); err != nil {\n\t\t\treturn errors.Wrap(err, \"creating field dir\")\n\t\t}\n\n\t\tf.holder.Logger.Debugf(\"load available shards for index/field: %s/%s\", f.index, f.name)\n\n\t\tif err := f.loadAvailableShards(); err != nil {\n\t\t\treturn errors.Wrap(err, \"loading available shards\")\n\t\t}\n\n\t\t// Apply the field options loaded from etcd (or set via setOptions()).\n\t\tf.holder.Logger.Debugf(\"apply options for index/field: %s/%s\", f.index, f.name)\n\t\tif err := f.applyOptions(f.options); err != nil {\n\t\t\treturn errors.Wrap(err, \"applying options\")\n\t\t}\n\n\t\tf.holder.Logger.Debugf(\"open views for index/field: %s/%s\", f.index, f.name)\n\t\tif err := f.openViews(); err != nil {\n\t\t\treturn errors.Wrap(err, \"opening views\")\n\t\t}\n\n\t\t// Apply the field-specific translateStore.\n\t\tif err := f.applyTranslateStore(); err != nil {\n\t\t\treturn errors.Wrap(err, \"applying translate store\")\n\t\t}\n\n\t\t// If the field has a foreign index, make sure the index\n\t\t// exists.\n\t\tif f.options.ForeignIndex != \"\" {\n\t\t\tif err := f.holder.checkForeignIndex(f); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"checking foreign index\")\n\t\t\t}\n\t\t}\n\n\t\tf.availableShardChan = make(chan struct{}, 1)\n\t\tf.wg.Add(1)\n\t\tgo f.writeAvailableShards()\n\t\treturn nil\n\t}(); err != nil {\n\t\tf.unprotectedClose()\n\t\treturn err\n\t}\n\tf.closing = make(chan struct{})\n\n\t_ = testhook.Opened(f.holder.Auditor, f, nil)\n\tf.holder.Logger.Debugf(\"successfully opened field index/field: %s/%s\", f.index, f.name)\n\treturn nil\n}\n\nfunc (f *Field) protectedRemoteAvailableShards() *roaring.Bitmap {\n\tf.remoteAvailableShardsMu.Lock()\n\tdefer f.remoteAvailableShardsMu.Unlock()\n\n\tf.remoteAvailableShards.Optimize()\n\treturn f.remoteAvailableShards.Clone()\n}\n\nfunc (f *Field) flushAvailableShards(ctx context.Context) {\n\tshards := f.protectedRemoteAvailableShards()\n\tvar buf bytes.Buffer\n\tif _, err := shards.WriteTo(&buf); err != nil {\n\t\tf.holder.Logger.Errorf(\"writting available shards: %v\", err)\n\t\treturn\n\t}\n\n\tif err := f.holder.sharder.SetShards(ctx, f.index, f.name, buf.Bytes()); err != nil {\n\t\tf.holder.Logger.Errorf(\"setting available shards: %v\", err)\n\t}\n}\n\nfunc (f *Field) writeAvailableShards() {\n\tdefer f.wg.Done()\n\n\tinterval := availableShardFileFlushDuration.Get()\n\ttimer := time.NewTimer(interval)\n\tdefer timer.Stop()\n\n\tfor range f.availableShardChan {\n\t\t// Available shards have been updated.\n\n\t\t// Wait a bit so that we batch writes.\n\ttimerWait:\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase _, ok := <-f.availableShardChan:\n\t\t\t\tif !ok {\n\t\t\t\t\t// The server is shutting down.\n\t\t\t\t\t// Do the write immediately.\n\t\t\t\t\ttimer.Stop()\n\t\t\t\t\tbreak timerWait\n\t\t\t\t}\n\n\t\t\tcase <-timer.C:\n\t\t\t\t// We have waited long enough.\n\t\t\t\tbreak timerWait\n\t\t\t}\n\t\t}\n\n\t\t// Set the timer for the next flush.\n\t\ttimer.Reset(interval)\n\n\t\t// Actually write the shards.\n\t\tf.flushAvailableShards(context.Background())\n\t}\n}\n\n// applyTranslateStore opens the configured translate store.\nfunc (f *Field) applyTranslateStore() error {\n\t// Instantiate & open translation store.\n\tvar err error\n\tf.translateStore, err = f.OpenTranslateStore(f.TranslateStorePath(), f.index, f.name, -1, -1, f.holder.cfg.StorageConfig.FsyncEnabled)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"opening field translate store\")\n\t}\n\tf.usesKeys = f.options.Keys\n\n\t// In the case where the field has a foreign index, set\n\t// the usesKeys value accordingly.\n\tif foreignIndexName := f.ForeignIndex(); foreignIndexName != \"\" {\n\t\tif foreignIndex := f.holder.Index(foreignIndexName); foreignIndex != nil {\n\t\t\tf.usesKeys = foreignIndex.Keys()\n\t\t}\n\t}\n\treturn nil\n}\n\n// applyForeignIndex used to set the field's translateStore to\n// that of the foreign index, but since moving to partitioned\n// translate stores on indexes, that doesn't happen anymore.\n// So now all this method does is check that the foreign index\n// actually exists. If we decided this was unnecessary (which\n// it kind of is), we could remove the field.holder and all\n// the logic which does this check on holder open after all\n// indexes have opened.\nfunc (f *Field) applyForeignIndex() error {\n\tforeignIndex := f.holder.Index(f.options.ForeignIndex)\n\tif foreignIndex == nil {\n\t\treturn errors.Wrapf(ErrForeignIndexNotFound, \"%s\", f.options.ForeignIndex)\n\t}\n\tf.usesKeys = foreignIndex.Keys()\n\treturn nil\n}\n\n// ForeignIndex returns the foreign index name attached to the field.\n// Returns blank string if no foreign index exists.\nfunc (f *Field) ForeignIndex() string {\n\treturn f.options.ForeignIndex\n}\n\n// TTL returns the ttl of the field.\nfunc (f *Field) TTL() time.Duration {\n\treturn f.options.TTL\n}\n\nfunc (f *Field) bitDepth() (uint64, error) {\n\tvar maxBitDepth uint64\n\n\tview2shards := f.idx.fieldView2shard.getViewsForField(f.name)\n\tfor name, shardset := range view2shards {\n\t\tview := f.view(name)\n\t\tif view == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tbd, err := view.bitDepth(shardset.shards())\n\t\tif err != nil {\n\t\t\treturn 0, errors.Wrapf(err, \"getting view(%s) bit depth\", name)\n\t\t}\n\t\tif bd > maxBitDepth {\n\t\t\tmaxBitDepth = bd\n\t\t}\n\t}\n\n\treturn maxBitDepth, nil\n}\n\n// cacheBitDepth is used by Index.setFieldBitDepths() to updated the in-memory\n// bitDepth values for each field and its bsiGroup.\nfunc (f *Field) cacheBitDepth(bd uint64) error {\n\t// Get the assocated bsiGroup so that its bitDepth can be updated as well.\n\tbsig := f.bsiGroup(f.name)\n\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\n\tif f.options.BitDepth < bd {\n\t\tf.options.BitDepth = bd\n\t}\n\n\tif bsig != nil && bsig.BitDepth < bd {\n\t\tbsig.BitDepth = bd\n\t}\n\n\treturn nil\n}\n\n// openViews opens and initializes the views inside the field.\nfunc (f *Field) openViews() error {\n\tview2shards := f.idx.fieldView2shard.getViewsForField(f.name)\n\tif view2shards == nil {\n\t\t// no data\n\t\treturn nil\n\t}\n\n\tfor name, shardset := range view2shards {\n\t\tview := f.newView(f.viewPath(name), name)\n\t\tif err := view.openWithShardSet(shardset); err != nil {\n\t\t\treturn fmt.Errorf(\"opening view: view=%s, err=%s\", view.name, err)\n\t\t}\n\n\t\tf.holder.Logger.Debugf(\"add index/field/view to field.viewMap: %s/%s/%s\", f.index, f.name, view.name)\n\t\tf.viewMap[view.name] = view\n\t}\n\treturn nil\n}\n\n// setOptions saves options for final application during Open().\nfunc (f *Field) setOptions(opts *FieldOptions) {\n\tf.options = applyDefaultOptions(opts)\n}\n\n// applyOptions configures the field based on opt.\nfunc (f *Field) applyOptions(opt FieldOptions) error {\n\tswitch opt.Type {\n\tcase FieldTypeSet, FieldTypeMutex, \"\":\n\t\tfldType := opt.Type\n\t\tif fldType == \"\" {\n\t\t\tfldType = FieldTypeSet\n\t\t}\n\t\tf.options.Type = fldType\n\t\tif opt.CacheType != \"\" {\n\t\t\tf.options.CacheType = opt.CacheType\n\t\t}\n\t\tif opt.CacheType == CacheTypeNone {\n\t\t\tf.options.CacheSize = 0\n\t\t} else if opt.CacheSize != 0 {\n\t\t\tf.options.CacheSize = opt.CacheSize\n\t\t}\n\t\tf.options.Min = pql.Decimal{}\n\t\tf.options.Max = pql.Decimal{}\n\t\tf.options.Base = 0\n\t\tf.options.BitDepth = 0\n\t\tf.options.TimeQuantum = \"\"\n\t\tf.options.TTL = 0\n\t\tf.options.Keys = opt.Keys\n\t\tf.options.ForeignIndex = opt.ForeignIndex\n\tcase FieldTypeInt, FieldTypeDecimal, FieldTypeTimestamp:\n\t\tf.options.Type = opt.Type\n\t\tf.options.CacheType = CacheTypeNone\n\t\tf.options.CacheSize = 0\n\t\tf.options.Min = opt.Min\n\t\tf.options.Max = opt.Max\n\t\tf.options.Base = opt.Base\n\t\tf.options.Scale = opt.Scale\n\t\tf.options.BitDepth = opt.BitDepth\n\t\tf.options.TimeUnit = opt.TimeUnit\n\t\tf.options.TimeQuantum = \"\"\n\t\tf.options.TTL = 0\n\t\tf.options.Keys = opt.Keys\n\t\tf.options.ForeignIndex = opt.ForeignIndex\n\n\t\t// Create new bsiGroup.\n\t\tbsig := &bsiGroup{\n\t\t\tName:     f.name,\n\t\t\tType:     bsiGroupTypeInt,\n\t\t\tMin:      opt.Min.ToInt64(opt.Scale),\n\t\t\tMax:      opt.Max.ToInt64(opt.Scale),\n\t\t\tBase:     opt.Base,\n\t\t\tScale:    opt.Scale,\n\t\t\tTimeUnit: opt.TimeUnit,\n\t\t\tBitDepth: opt.BitDepth,\n\t\t}\n\t\t// Validate and create bsiGroup.\n\t\tif err := f.createBSIGroup(bsig); err != nil {\n\t\t\treturn errors.Wrap(err, \"creating bsigroup\")\n\t\t}\n\tcase FieldTypeTime:\n\t\tf.options.Type = opt.Type\n\t\tf.options.CacheType = CacheTypeNone\n\t\tf.options.CacheSize = 0\n\t\tf.options.Min = pql.Decimal{}\n\t\tf.options.Max = pql.Decimal{}\n\t\tf.options.Base = 0\n\t\tf.options.BitDepth = 0\n\t\tf.options.Keys = opt.Keys\n\t\tf.options.NoStandardView = opt.NoStandardView\n\t\t// Validate the time quantum.\n\t\tif !opt.TimeQuantum.Valid() {\n\t\t\treturn ErrInvalidTimeQuantum\n\t\t}\n\t\tf.options.TimeQuantum = opt.TimeQuantum\n\t\tf.options.TTL = opt.TTL\n\t\tf.options.ForeignIndex = opt.ForeignIndex\n\tcase FieldTypeBool:\n\t\tf.options.Type = FieldTypeBool\n\t\tf.options.CacheType = CacheTypeNone\n\t\tf.options.CacheSize = 0\n\t\tf.options.Min = pql.Decimal{}\n\t\tf.options.Max = pql.Decimal{}\n\t\tf.options.Base = 0\n\t\tf.options.BitDepth = 0\n\t\tf.options.TimeQuantum = \"\"\n\t\tf.options.TTL = 0\n\t\tf.options.Keys = false\n\t\tf.options.ForeignIndex = \"\"\n\tdefault:\n\t\treturn errors.New(\"invalid field type\")\n\t}\n\n\treturn nil\n}\n\n// Close closes the field and its views.\nfunc (f *Field) Close() error {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\treturn f.unprotectedClose()\n}\n\n// unprotectedClose is the actual closing part of the operation, without the\n// locking.\nfunc (f *Field) unprotectedClose() error {\n\tif f.closing != nil {\n\t\tselect {\n\t\tcase <-f.closing:\n\t\t\t// already closed. prevent double-close\n\t\t\treturn errors.New(\"double close of field\")\n\t\tdefault:\n\t\t}\n\t\tclose(f.closing)\n\t}\n\tdefer func() {\n\t\t_ = testhook.Closed(f.holder.Auditor, f, nil)\n\t}()\n\t// Shutdown the available shards writer\n\tif f.availableShardChan != nil {\n\t\tclose(f.availableShardChan)\n\t\tf.wg.Wait()\n\t\tf.availableShardChan = nil\n\t}\n\n\t// Close field translation store.\n\tif f.translateStore != nil {\n\t\tif err := f.translateStore.Close(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Close all views.\n\tfor _, view := range f.viewMap {\n\t\tif err := view.close(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tf.viewMap = make(map[string]*view)\n\n\treturn nil\n}\n\nfunc (f *Field) flushCaches() {\n\t// look up the close channel so if we somehow end up living until the\n\t// field gets reopened, we don't have a data race, but correctly detect\n\t// that the old one is closed.\n\tf.mu.RLock()\n\tclosing := f.closing\n\tf.mu.RUnlock()\n\tfor _, v := range f.views() {\n\t\tselect {\n\t\tcase <-closing:\n\t\t\treturn\n\t\tdefault:\n\t\t\tv.flushCaches()\n\t\t}\n\t}\n}\n\n// Keys returns true if the field uses string keys.\nfunc (f *Field) Keys() bool {\n\tf.mu.RLock()\n\tdefer f.mu.RUnlock()\n\treturn f.usesKeys\n}\n\n// bsiGroup returns a bsiGroup by name.\nfunc (f *Field) bsiGroup(name string) *bsiGroup {\n\tf.mu.RLock()\n\tdefer f.mu.RUnlock()\n\tfor _, bsig := range f.bsiGroups {\n\t\tif bsig.Name == name {\n\t\t\treturn bsig\n\t\t}\n\t}\n\treturn nil\n}\n\n// hasBSIGroup returns true if a bsiGroup exists on the field.\nfunc (f *Field) hasBSIGroup(name string) bool {\n\tfor _, bsig := range f.bsiGroups {\n\t\tif bsig.Name == name {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// cleanupViewName yields a \"corrected\" view name, handling some\n// idioms we used elsewhere in code. Given an empty string,\n// it yields a default view name (either \"standard\" or the BSI view\n// for BSI fields). Given a string starting with numbers, it\n// yields the corresponding time quantum view (prefixing \"standard_\").\n// It yields an error if the view name given does not correspond\n// to a view which should exist. For instance, the \"standard\" or\n// \"existence\" views for a BSI field, or a time quantum view for\n// a non-time field.\nfunc (f *Field) cleanupViewName(viewName string) (string, error) {\n\tif viewName == \"\" {\n\t\tswitch f.options.Type {\n\t\tcase FieldTypeInt, FieldTypeDecimal, FieldTypeTimestamp:\n\t\t\treturn \"bsig_\" + f.name, nil\n\t\tdefault:\n\t\t\treturn viewStandard, nil\n\t\t}\n\t}\n\tswitch f.options.Type {\n\tcase FieldTypeInt, FieldTypeDecimal, FieldTypeTimestamp:\n\t\tif viewName == \"bsig_\"+f.name {\n\t\t\treturn viewName, nil\n\t\t}\n\t\treturn viewName, fmt.Errorf(\"BSI-type field view should be named bsig_[fieldname], got %q\", viewName)\n\tcase FieldTypeTime:\n\t\tswitch {\n\t\tcase viewName == viewStandard, viewName == viewExistence:\n\t\t\treturn viewName, nil\n\t\tcase strings.HasPrefix(viewName, viewStandard):\n\t\t\treturn viewName, nil\n\t\tcase unicode.IsDigit(rune(viewName[0])):\n\t\t\treturn viewStandard + \"_\" + viewName, nil\n\t\tdefault:\n\t\t\treturn viewName, fmt.Errorf(\"time field views are %q, %q, or %q_[digits], got %q\", viewStandard, viewExistence, viewStandard, viewName)\n\t\t}\n\tdefault:\n\t\tswitch viewName {\n\t\tcase viewStandard, viewExistence:\n\t\t\treturn viewName, nil\n\t\tdefault:\n\t\t\treturn viewName, fmt.Errorf(\"unexpected view name %q, expecting %q or %q\", viewName, viewStandard, viewExistence)\n\t\t}\n\t}\n}\n\n// createBSIGroup creates a new bsiGroup on the field.\nfunc (f *Field) createBSIGroup(bsig *bsiGroup) error {\n\t// Append bsiGroup.\n\tif err := bsig.validate(); err != nil {\n\t\treturn errors.Wrap(err, \"validating bsigroup\")\n\t} else if f.hasBSIGroup(bsig.Name) {\n\t\treturn ErrBSIGroupExists\n\t}\n\n\t// Add bsiGroup to list.\n\tf.bsiGroups = append(f.bsiGroups, bsig)\n\n\t// Sort bsiGroups by name.\n\tsort.Slice(f.bsiGroups, func(i, j int) bool {\n\t\treturn f.bsiGroups[i].Name < f.bsiGroups[j].Name\n\t})\n\n\treturn nil\n}\n\n// TimeQuantum returns the time quantum for the field.\nfunc (f *Field) TimeQuantum() TimeQuantum {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\treturn f.options.TimeQuantum\n}\n\n// viewsByTimeRange is a wrapper on the non-method viewsByTimeRange,\n// which computes views for a specific field for a given time\n// range. The difference is that, it can return \"standard\" if from/to\n// are not set and can automatically coerce from/to times to match the\n// actual range present in the field.\nfunc (f *Field) viewsByTimeRange(from, to time.Time) (views []string, err error) {\n\t// if field is not a time field, return an error\n\t// If we can't find time views at all, and standard view is available,\n\t// yield standard view\n\t// if we can't find time views, and standard view is disabled,\n\t// yield union of all views\n\t// yield \"standard\" if from and to were both not set and there is a\n\t// standard view.\n\tq := f.TimeQuantum()\n\tif q == \"\" {\n\t\treturn nil, fmt.Errorf(\"field %s is not a time-field, 'from' and 'to' are not valid options for this field type\", f.name)\n\t}\n\n\tif from.IsZero() && to.IsZero() && !f.options.NoStandardView {\n\t\treturn []string{viewStandard}, nil\n\t}\n\n\t// Get min/max based on existing views.\n\tfv := f.views()\n\tvs := make([]string, 0, len(fv))\n\tfor _, v := range fv {\n\t\tvs = append(vs, v.name)\n\t}\n\tmin, max := minMaxViews(vs, q)\n\n\t// If min/max are empty, there were no time views.\n\tif min == \"\" || max == \"\" {\n\t\treturn []string{}, nil\n\t}\n\n\t// Convert min/max from string to time.Time.\n\tminTime, err := timeOfView(min, false)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"getting min time from view: %s\", min)\n\t}\n\tif from.IsZero() || from.Before(minTime) {\n\t\tfrom = minTime\n\t}\n\n\tmaxTime, err := timeOfView(max, true)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"getting max time from view: %s\", max)\n\t}\n\tif to.IsZero() || to.After(maxTime) {\n\t\tto = maxTime\n\t}\n\treturn viewsByTimeRange(viewStandard, from, to, q), nil\n}\n\n// RowTime gets the row at the particular time with the granularity specified by\n// the quantum.\nfunc (f *Field) RowTime(qcx *Qcx, rowID uint64, time time.Time, quantum string) (*Row, error) {\n\tif !TimeQuantum(quantum).Valid() {\n\t\treturn nil, ErrInvalidTimeQuantum\n\t}\n\tviewname := viewByTimeUnit(viewStandard, time, rune(quantum[len(quantum)-1]))\n\tview := f.view(viewname)\n\tif view == nil {\n\t\treturn nil, errors.Errorf(\"view with quantum %v not found.\", quantum)\n\t}\n\n\treturn view.row(qcx, rowID)\n}\n\n// viewPath returns the path to a view in the field.\nfunc (f *Field) viewPath(name string) string {\n\treturn filepath.Join(f.path, \"views\", name)\n}\n\n// view returns a view in the field by name.\nfunc (f *Field) view(name string) *view {\n\tf.mu.RLock()\n\tdefer f.mu.RUnlock()\n\treturn f.unprotectedView(name)\n}\n\nfunc (f *Field) unprotectedView(name string) *view { return f.viewMap[name] }\n\n// views returns a list of all views in the field.\nfunc (f *Field) views() []*view {\n\tf.mu.RLock()\n\tdefer f.mu.RUnlock()\n\n\tother := make([]*view, 0, len(f.viewMap))\n\tfor _, view := range f.viewMap {\n\t\tother = append(other, view)\n\t}\n\treturn other\n}\n\n// recalculateCaches recalculates caches on every view in the field.\nfunc (f *Field) recalculateCaches() {\n\tfor _, view := range f.views() {\n\t\tview.recalculateCaches()\n\t}\n}\n\n// createViewIfNotExists returns the named view, creating it if necessary.\n// Additionally, a CreateViewMessage is sent to the cluster.\nfunc (f *Field) createViewIfNotExists(name string) (*view, error) {\n\tcvm := &CreateViewMessage{\n\t\tIndex: f.index,\n\t\tField: f.name,\n\t\tView:  name,\n\t}\n\n\t// call this base method to isolate the mu.Lock and ensure we aren't holding\n\t// the lock while calling SendSync below.\n\tview, created, err := f.createViewIfNotExistsBase(cvm)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif created {\n\t\t// Broadcast view creation to the cluster.\n\t\terr := f.holder.sendOrSpool(cvm)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"sending CreateView message\")\n\t\t}\n\t}\n\n\treturn view, nil\n}\n\n// createViewIfNotExistsBase returns the named view, creating it if necessary.\n// One purpose of isolating this method from createViewIfNotExists() is that we\n// need to enforce the mu.Lock on everything in this method, but we can't be\n// holding the lock when broadcasting the CreateViewMessage view\n// broadcaster.SendSync(); calling that SendSync() while holding the lock can\n// result in a deadlock waiting on the remote node to give up its lock obtained\n// by performing the same action. The returned bool indicates whether the view\n// was created or not.\nfunc (f *Field) createViewIfNotExistsBase(cvm *CreateViewMessage) (*view, bool, error) {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\n\t// If we already have this view, we can probably assume etcd already\n\t// has it.\n\tif view := f.viewMap[cvm.View]; view != nil && !view.isClosing() {\n\t\treturn view, false, nil\n\t}\n\n\t// Create the view in etcd as the system of record.\n\t// Don't persist views related to the existence field.\n\tif f.name != existenceFieldName {\n\t\tif err := f.persistView(context.Background(), cvm); err != nil {\n\t\t\treturn nil, false, errors.Wrap(err, \"persisting view\")\n\t\t}\n\t}\n\tview := f.newView(f.viewPath(cvm.View), cvm.View)\n\n\tif err := view.openEmpty(); err != nil {\n\t\treturn nil, false, errors.Wrap(err, \"opening view\")\n\t}\n\tf.viewMap[view.name] = view\n\n\treturn view, true, nil\n}\n\nfunc (f *Field) newView(path, name string) *view {\n\tview := newView(f.holder, path, f.index, f.name, name, f.options)\n\tview.idx = f.idx\n\tview.fld = f\n\tview.broadcaster = f.broadcaster\n\treturn view\n}\n\n// deleteView removes the view from the field.\nfunc (f *Field) deleteView(name string) error {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\tview := f.viewMap[name]\n\tif view == nil {\n\t\treturn ErrInvalidView\n\t}\n\n\t// Delete the view from etcd as the system of record.\n\tif err := f.holder.Schemator.DeleteView(context.TODO(), f.index, f.name, name); err != nil {\n\t\treturn errors.Wrapf(err, \"deleting view from etcd: %s/%s/%s\", f.index, f.name, name)\n\t}\n\n\t// Close data files before deletion.\n\tif err := view.close(); err != nil {\n\t\treturn errors.Wrap(err, \"closing view\")\n\t}\n\n\t// Delete view directory.\n\tif err := os.RemoveAll(view.path); err != nil {\n\t\treturn errors.Wrap(err, \"deleting directory\")\n\t}\n\n\tdelete(f.viewMap, name)\n\n\treturn nil\n}\n\n// Row returns a row of the standard view.\n// It seems this method is only being used by the test\n// package, and the fact that it's only allowed on\n// `set`,`mutex`, and `bool` fields is odd. This may\n// be considered for deprecation in a future version.\nfunc (f *Field) Row(qcx *Qcx, rowID uint64) (*Row, error) {\n\tswitch f.Type() {\n\tcase FieldTypeSet, FieldTypeMutex, FieldTypeBool:\n\t\tview := f.view(viewStandard)\n\t\tif view == nil {\n\t\t\treturn nil, ErrInvalidView\n\t\t}\n\t\treturn view.row(qcx, rowID)\n\tdefault:\n\t\treturn nil, errors.Errorf(\"row method unsupported for field type: %s\", f.Type())\n\t}\n}\n\n// mutexCheck performs a sanity-check on the available fragments for a\n// field. The return is map[column]map[shard][]values for collisions only.\nfunc (f *Field) MutexCheck(ctx context.Context, qcx *Qcx, details bool, limit int) (map[uint64]map[uint64][]uint64, error) {\n\tif f.Type() != FieldTypeMutex {\n\t\treturn nil, errors.New(\"mutex check only valid for mutex fields\")\n\t}\n\n\t// Rather than deferring the unlock, we grab the standard view\n\t// from the field's viewMap and unlock immediately. This avoids\n\t// holding the rlock for a potentially long time which blocks any\n\t// write lock, and pending write locks block other read locks.\n\tf.mu.RLock()\n\tstandard := f.viewMap[viewStandard]\n\tf.mu.RUnlock()\n\n\tif standard == nil {\n\t\t// no standard view present means we've never needed to create it,\n\t\t// so it has no bits set, so it has no extra bits set.\n\t\treturn nil, nil\n\t}\n\treturn standard.mutexCheck(ctx, qcx, details, limit)\n}\n\n// SetBit sets a bit on a view within the field.\nfunc (f *Field) SetBit(qcx *Qcx, rowID, colID uint64, t *time.Time) (changed bool, err error) {\n\tviewName := viewStandard\n\tif !f.options.NoStandardView {\n\t\t// Retrieve view. Exit if it doesn't exist.\n\t\tview, err := f.createViewIfNotExists(viewName)\n\t\tif err != nil {\n\t\t\treturn changed, errors.Wrap(err, \"creating view\")\n\t\t}\n\n\t\t// Set non-time bit.\n\t\tif v, err := view.setBit(qcx, rowID, colID); err != nil {\n\t\t\treturn changed, errors.Wrap(err, \"setting on view\")\n\t\t} else if v {\n\t\t\tchanged = v\n\t\t}\n\t\tif f.options.TrackExistence {\n\t\t\tview, err := f.createViewIfNotExists(viewExistence)\n\t\t\tif err != nil {\n\t\t\t\treturn changed, errors.Wrap(err, \"creating existence view\")\n\t\t\t}\n\t\t\tif _, err := view.setBit(qcx, bsiExistsBit, colID); err != nil {\n\t\t\t\treturn changed, errors.Wrap(err, \"setting existence on view\")\n\t\t\t}\n\t\t}\n\t}\n\n\t// Exit early if no timestamp is specified.\n\tif t == nil {\n\t\treturn changed, nil\n\t}\n\n\t// If a timestamp is specified then set bits across all views for the quantum.\n\tfor _, subname := range viewsByTime(viewName, *t, f.TimeQuantum()) {\n\t\tview, err := f.createViewIfNotExists(subname)\n\t\tif err != nil {\n\t\t\treturn changed, errors.Wrapf(err, \"creating view %s\", subname)\n\t\t}\n\n\t\tif c, err := view.setBit(qcx, rowID, colID); err != nil {\n\t\t\treturn changed, errors.Wrapf(err, \"setting on view %s\", subname)\n\t\t} else if c {\n\t\t\tchanged = true\n\t\t}\n\t}\n\n\treturn changed, nil\n}\n\n// ClearBit clears a bit within the field.\n//\n// This does not, for now, create existence bits for the field, because it\n// doesn't create them for the index.\nfunc (f *Field) ClearBit(qcx *Qcx, rowID, colID uint64) (changed bool, err error) {\n\tviewName := viewStandard\n\n\t// Retrieve view. Exit if it doesn't exist.\n\tview, present := f.viewMap[viewName]\n\tif !present {\n\t\treturn false, errors.Wrap(err, \"clearing missing view\")\n\t}\n\n\t// Clear non-time bit.\n\tif v, err := view.clearBit(qcx, rowID, colID); err != nil {\n\t\treturn false, errors.Wrap(err, \"clearing on view\")\n\t} else if v {\n\t\tchanged = changed || v\n\t\tif changed && f.options.TrackExistence && f.options.Type == FieldTypeMutex {\n\t\t\t// we also want to try to clear any existence bit\n\t\t\texistView, ok := f.viewMap[viewExistence]\n\t\t\tif ok {\n\t\t\t\t_, err := existView.clearBit(qcx, 0, colID)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn false, errors.Wrap(err, \"clearing existence bit\")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\t// We used to check length of view map here. Now that we might\n\t// have an existence view, that won't work.\n\tif f.options.Type != FieldTypeTime { // assuming no time views\n\t\treturn changed, nil\n\t}\n\tlastViewNameSize := 0\n\tlevel := 0\n\tskipAbove := maxInt\n\tfor _, view := range f.allTimeViewsSortedByQuantum() {\n\t\tif lastViewNameSize < len(view.name) {\n\t\t\tlevel++\n\t\t} else if lastViewNameSize > len(view.name) {\n\t\t\tlevel--\n\t\t}\n\t\tif level < skipAbove {\n\t\t\tcleared, err := view.clearBit(qcx, rowID, colID)\n\t\t\tchanged = changed || cleared\n\t\t\tif err != nil {\n\t\t\t\treturn changed, errors.Wrapf(err, \"clearing on view %s\", view.name)\n\t\t\t}\n\t\t\tif !cleared {\n\t\t\t\tskipAbove = level + 1\n\t\t\t} else {\n\t\t\t\tskipAbove = maxInt\n\t\t\t}\n\t\t}\n\t\tlastViewNameSize = len(view.name)\n\t}\n\n\treturn changed, nil\n}\n\nfunc groupCompare(a, b string, offset int) (lt, eq bool) {\n\tif len(a) > offset {\n\t\ta = a[:offset]\n\t}\n\tif len(b) > offset {\n\t\tb = b[:offset]\n\t}\n\tv := strings.Compare(a, b)\n\treturn v < 0, v == 0\n}\n\nfunc (f *Field) allTimeViewsSortedByQuantum() (me []*view) {\n\tme = make([]*view, len(f.viewMap))\n\tprefix := viewStandard + \"_\"\n\toffset := len(viewStandard) + 1\n\ti := 0\n\tfor _, v := range f.viewMap {\n\t\tif len(v.name) > offset && strings.Compare(v.name[:offset], prefix) == 0 { // skip non-time views\n\t\t\tme[i] = v\n\t\t\ti++\n\t\t}\n\t}\n\t// return the empty list if there weren't any. this could happen\n\t// if we got called because this is a time field, but in fact\n\t// no time views have been created.\n\tif i == 0 {\n\t\treturn me[:0]\n\t}\n\tme = me[:i]\n\tyear := strings.Index(me[0].name, \"_\") + 4\n\tmonth := year + 2\n\tday := month + 2\n\tsort.Slice(me, func(i, j int) (lt bool) {\n\t\tvar eq bool\n\t\t// group by quantum from year to hour\n\t\tif lt, eq = groupCompare(me[i].name, me[j].name, year); eq {\n\t\t\tif lt, eq = groupCompare(me[i].name, me[j].name, month); eq {\n\t\t\t\tif lt, eq = groupCompare(me[i].name, me[j].name, day); eq {\n\t\t\t\t\tlt = strings.Compare(me[i].name, me[j].name) > 0\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn lt\n\t})\n\treturn me\n}\n\n// StringValue reads an integer field value for a column, and converts\n// it to a string based on a foreign index string key.\nfunc (f *Field) StringValue(qcx *Qcx, columnID uint64) (value string, exists bool, err error) {\n\tbsig := f.bsiGroup(f.name)\n\tif bsig == nil {\n\t\treturn value, false, ErrBSIGroupNotFound\n\t}\n\n\tval, exists, err := f.Value(qcx, columnID)\n\tif exists {\n\t\tvalue, err = f.translateStore.TranslateID(uint64(val))\n\t}\n\treturn value, exists, err\n}\n\n// Value reads a field value for a column.\nfunc (f *Field) Value(qcx *Qcx, columnID uint64) (value int64, exists bool, err error) {\n\tbsig := f.bsiGroup(f.name)\n\tif bsig == nil {\n\t\treturn 0, false, ErrBSIGroupNotFound\n\t}\n\n\t// Fetch target view.\n\tview := f.view(viewBSIGroupPrefix + f.name)\n\tif view == nil {\n\t\treturn 0, false, nil\n\t}\n\n\tv, exists, err := view.value(qcx, columnID, bsig.BitDepth)\n\tif err != nil {\n\t\treturn 0, false, err\n\t} else if !exists {\n\t\treturn 0, false, nil\n\t}\n\treturn int64(v) + bsig.Base, true, nil\n}\n\n// SetValue sets a field value for a column.\nfunc (f *Field) SetValue(qcx *Qcx, columnID uint64, value int64) (changed bool, err error) {\n\t// Fetch bsiGroup & validate min/max.\n\tbsig := f.bsiGroup(f.name)\n\tif bsig == nil {\n\t\treturn false, ErrBSIGroupNotFound\n\t}\n\n\t// Determine base value to store.\n\tbaseValue := int64(value - bsig.Base)\n\t//Timestamp expects incoming value to already be relative to epoch\n\tif f.Type() == FieldTypeTimestamp {\n\t\tvalue = baseValue\n\t}\n\tif value < bsig.Min {\n\t\treturn false, errors.Wrapf(ErrBSIGroupValueTooLow, \"index = %v, field = %v, column ID = %v, value %v is smaller than min allowed %v\", f.index, f.name, columnID, value, bsig.Min)\n\t} else if value > bsig.Max {\n\t\treturn false, errors.Wrapf(ErrBSIGroupValueTooHigh, \"index = %v, field = %v, column ID = %v, value %v is larger than max allowed %v\", f.index, f.name, columnID, value, bsig.Max)\n\t}\n\n\trequiredBitDepth := bitDepthInt64(baseValue)\n\n\t// Increase bit depth value if the unsigned value is greater.\n\tif requiredBitDepth > bsig.BitDepth {\n\t\tuvalue := uint64(baseValue)\n\t\tif value < 0 {\n\t\t\tuvalue = uint64(-baseValue)\n\t\t}\n\t\tbitDepth := bitDepth(uvalue)\n\n\t\tf.mu.Lock()\n\t\tbsig.BitDepth = bitDepth\n\t\tf.options.BitDepth = bitDepth\n\t\tf.mu.Unlock()\n\t}\n\n\t// Fetch target view.\n\tview, err := f.createViewIfNotExists(viewBSIGroupPrefix + f.name)\n\tif err != nil {\n\t\treturn false, errors.Wrap(err, \"creating view\")\n\t}\n\tif view.holder == nil {\n\t\tpanic(\"view.holder should not be nil\")\n\t}\n\tif view.idx == nil {\n\t\tpanic(\"view.idx should not be nil\")\n\t}\n\tview.holder.addIndex(view.idx)\n\n\treturn view.setValue(qcx, columnID, bsig.BitDepth, baseValue)\n}\n\n// ClearValue removes a field value for a column.\nfunc (f *Field) ClearValue(qcx *Qcx, columnID uint64) (changed bool, err error) {\n\tbsig := f.bsiGroup(f.name)\n\tif bsig == nil {\n\t\treturn false, ErrBSIGroupNotFound\n\t}\n\t// Fetch target view.\n\tview := f.view(viewBSIGroupPrefix + f.name)\n\tif view == nil {\n\t\treturn false, nil\n\t}\n\tvalue, exists, err := view.value(qcx, columnID, bsig.BitDepth)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tif exists {\n\t\treturn view.clearValue(qcx, columnID, bsig.BitDepth, value)\n\t}\n\treturn false, nil\n}\n\nfunc (f *Field) MaxForShard(qcx *Qcx, shard uint64, filter *Row) (ValCount, error) {\n\ttx, finisher, err := qcx.GetTx(Txo{Write: false, Index: f.idx, Shard: shard})\n\tdefer finisher(&err)\n\tbsig := f.bsiGroup(f.name)\n\tif bsig == nil {\n\t\treturn ValCount{}, ErrBSIGroupNotFound\n\t}\n\n\tview := f.view(viewBSIGroupPrefix + f.name)\n\tif view == nil {\n\t\treturn ValCount{}, nil\n\t}\n\n\tfragment := view.Fragment(shard)\n\tif fragment == nil {\n\t\treturn ValCount{}, nil\n\t}\n\n\tmax, cnt, err := fragment.max(tx, filter, bsig.BitDepth)\n\tif err != nil {\n\t\treturn ValCount{}, errors.Wrap(err, \"calling fragment.max\")\n\t}\n\n\tv, err := f.valCountize(max, cnt, bsig)\n\treturn v, err\n}\n\n// MinForShard returns the minimum value which appears in this shard\n// (this field must be an Int or Decimal field). It also returns the\n// number of times the minimum value appears.\nfunc (f *Field) MinForShard(qcx *Qcx, shard uint64, filter *Row) (ValCount, error) {\n\ttx, finisher, err := qcx.GetTx(Txo{Write: false, Index: f.idx, Shard: shard})\n\tdefer finisher(&err)\n\tbsig := f.bsiGroup(f.name)\n\tif bsig == nil {\n\t\treturn ValCount{}, ErrBSIGroupNotFound\n\t}\n\n\tview := f.view(viewBSIGroupPrefix + f.name)\n\tif view == nil {\n\t\treturn ValCount{}, nil\n\t}\n\n\tfragment := view.Fragment(shard)\n\tif fragment == nil {\n\t\treturn ValCount{}, nil\n\t}\n\n\tmin, cnt, err := fragment.min(tx, filter, bsig.BitDepth)\n\tif err != nil {\n\t\treturn ValCount{}, errors.Wrap(err, \"calling fragment.min\")\n\t}\n\n\tv, err := f.valCountize(min, cnt, bsig)\n\treturn v, err\n}\n\n// valCountize takes the \"raw\" value and count we get from the\n// fragment and calculates the cooked values for this field\n// (timestamping, decimaling, or just adding in the base). It always\n// includes the int64 \"Val\\\" value to make comparisons easier in the\n// executor (at time of writing, Percentile takes advantage of this,\n// but we might be able to simplify logic in other places as well).\n//\n// Note that the ValCount returned has bsig.Base included, or if\n// you specify a nil bsig, includes the field's bsig.Base. Which is\n// to say, don't use this if you have a value that's already been\n// adjusted by base.\nfunc (f *Field) valCountize(val int64, cnt uint64, bsig *bsiGroup) (ValCount, error) {\n\tif bsig == nil {\n\t\tbsig = f.bsiGroup(f.name)\n\t\tif bsig == nil {\n\t\t\treturn ValCount{}, ErrBSIGroupNotFound\n\t\t}\n\n\t}\n\tvalCount := ValCount{Count: int64(cnt)}\n\n\tif f.options.Type == FieldTypeDecimal {\n\t\tdec := pql.NewDecimal(val+bsig.Base, bsig.Scale)\n\t\tvalCount.DecimalVal = &dec\n\t} else if f.options.Type == FieldTypeTimestamp {\n\t\tts, err := ValToTimestamp(f.options.TimeUnit, val+bsig.Base)\n\t\tif err != nil {\n\t\t\treturn ValCount{}, errors.Wrap(err, \"translating value to timestamp\")\n\t\t}\n\t\tvalCount.TimestampVal = ts\n\t\t// valCount.TimestampVal = time.Unix(0, (val+bsig.Base)*TimeUnitNanos(f.options.TimeUnit)).UTC()\n\t}\n\n\tvalCount.Val = val + bsig.Base\n\treturn valCount, nil\n}\n\n// Range performs a conditional operation on Field.\nfunc (f *Field) Range(qcx *Qcx, name string, op pql.Token, predicate int64) (*Row, error) {\n\t// Retrieve and validate bsiGroup.\n\tbsig := f.bsiGroup(name)\n\tif bsig == nil {\n\t\treturn nil, ErrBSIGroupNotFound\n\t} else if predicate < bsig.Min || predicate > bsig.Max {\n\t\treturn nil, nil\n\t}\n\n\t// Retrieve bsiGroup's view.\n\tview := f.view(viewBSIGroupPrefix + name)\n\tif view == nil {\n\t\treturn nil, nil\n\t}\n\n\tbaseValue, outOfRange := bsig.baseValue(op, predicate)\n\tif outOfRange {\n\t\treturn NewRow(), nil\n\t}\n\n\treturn view.rangeOp(qcx, op, bsig.BitDepth, baseValue)\n}\n\n// existenceViewName reports the field we should use row 0 of\n// for existence data. For a BSI field (integer, decimal,\n// timestamp) this is the single BSI group. For other fields,\n// it's viewExistence, which is probably \"existence\".\nfunc (f *Field) existenceViewName() string {\n\tif len(f.bsiGroups) > 0 {\n\t\treturn f.bsiGroups[0].Name\n\t}\n\treturn viewExistence\n}\n\n// MarkExisting sets a range of column IDs as existing. The columnIDs\n// are assumed to include the shard offset, but this will also work if\n// they are shard-relative, as it's just stripping the offset.\n//\n// Positions aren't the same as column IDs; this function takes advantage\n// of the fact that we're always doing row 0, so we don't have to think\n// hard about this. It doesn't overwrite its input because the column IDs\n// could be reused by other things.\n//\n// Note that this is subtly inefficient; if you're tracking existence for\n// a field, we're computing the same column ID set to write to the index's\n// existence field as we're using for the field's existence view. We don't\n// have a good way to coalesce those, yet. (Also, that's not accurate in\n// the ImportValue case, where we don't write to the existence view, etc.)\nfunc (f *Field) MarkExisting(tx Tx, columnIDs []uint64, shard uint64) error {\n\treturn f.markExistingInView(tx, columnIDs, f.existenceViewName(), shard)\n}\n\n// markExistingInView implements the internals of MarkExisting, but lets\n// you use a non-standard view. It's only interesting for the existence field.\nfunc (f *Field) markExistingInView(tx Tx, columnIDs []uint64, viewName string, shard uint64) error {\n\tcopyCols := make([]uint64, len(columnIDs))\n\tfor i := range columnIDs {\n\t\tcopyCols[i] = columnIDs[i] % ShardWidth\n\t}\n\teView, err := f.createViewIfNotExists(viewName)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"creating view %s\", viewName)\n\t}\n\n\teFrag, err := eView.CreateFragmentIfNotExists(shard)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"creating fragment\")\n\t}\n\treturn eFrag.importPositions(tx, copyCols, nil, map[uint64]struct{}{0: {}})\n}\n\n// MarkNotExisting is just like MarkExisting, except it is clearing bits,\n// so it doesn't have to create the view or fragment if it doesn't exist.\n// Because the bits reported to us in the case we wrote this for are likely\n// to be sorted by position in the fragment, not by column ID, we sort the\n// list after stripping the rows from the positions.\nfunc (f *Field) MarkNotExisting(tx Tx, columnIDs []uint64, shard uint64) error {\n\tviewName := f.existenceViewName()\n\tv := f.view(viewName)\n\tif v == nil {\n\t\treturn nil\n\t}\n\tfrag := v.Fragment(shard)\n\tif frag == nil {\n\t\treturn nil\n\t}\n\tcopyCols := make([]uint64, len(columnIDs))\n\tfor i := range columnIDs {\n\t\tcopyCols[i] = columnIDs[i] % ShardWidth\n\t}\n\tsort.Slice(copyCols, func(i, j int) bool { return copyCols[i] < copyCols[j] })\n\treturn frag.importPositions(tx, nil, copyCols, map[uint64]struct{}{0: {}})\n}\n\n// Existing returns the existence row for this field, which\n// comes from either the BSI view or the existence view.\nfunc (f *Field) Existing(tx Tx, shard uint64) (*Row, error) {\n\tviewName := f.existenceViewName()\n\tv := f.view(viewName)\n\tif v == nil {\n\t\treturn nil, nil\n\t}\n\tfrag := v.Fragment(shard)\n\tif frag == nil {\n\t\treturn nil, nil\n\t}\n\treturn frag.row(tx, bsiExistsBit)\n}\n\n// Import bulk imports data.\nfunc (f *Field) Import(qcx *Qcx, rowIDs, columnIDs []uint64, timestamps []int64, shard uint64, options *ImportOptions) (err0 error) {\n\t// Determine quantum if timestamps are set.\n\tq := f.TimeQuantum()\n\tif len(timestamps) > 0 {\n\t\tif q == \"\" {\n\t\t\treturn errors.New(\"time quantum not set in field\")\n\t\t} else if options.Clear {\n\t\t\treturn errors.New(\"import clear is not supported with timestamps\")\n\t\t}\n\t} else {\n\t\tif f.options.NoStandardView {\n\t\t\treturn errors.New(\"can't import data with no timestamps into a field with no standard view\")\n\t\t}\n\t\t// short path: if we don't have any timestamps, we only need\n\t\t// to write to exactly one view, which is always viewStandard,\n\t\t// and *every* bit goes into that view, and we already verified that\n\t\t// everything is in the same shard, so we can skip most of this.\n\t\tfieldType := f.Type()\n\t\tif fieldType == FieldTypeBool {\n\t\t\tfor _, rowID := range rowIDs {\n\t\t\t\tif rowID > 1 {\n\t\t\t\t\treturn errors.New(\"bool field imports only support values 0 and 1\")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\ttx, finisher, err := qcx.GetTx(Txo{Write: true, Index: f.idx, Shard: shard})\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"qcx.GetTx\")\n\t\t}\n\t\tvar err1 error\n\t\tdefer finisher(&err1)\n\t\tview, err := f.createViewIfNotExists(viewStandard)\n\t\tif err != nil {\n\t\t\treturn errors.Wrapf(err, \"creating view %s\", viewStandard)\n\t\t}\n\n\t\tfrag, err := view.CreateFragmentIfNotExists(shard)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"creating fragment\")\n\t\t}\n\t\tif f.options.TrackExistence {\n\t\t\t// if we're clearing a mutex, we do something fancy. otherwise,\n\t\t\t// if we're not clearing, we mark the existence bits. either way,\n\t\t\t// we then fall on out to the default behavior of importing the\n\t\t\t// bits.\n\t\t\tswitch {\n\t\t\tcase options.Clear && fieldType == FieldTypeMutex:\n\t\t\t\t// special fancy case; we have to try to clear the bits first, to\n\t\t\t\t// find out WHICH bits we cleared, so we can mark those bits as\n\t\t\t\t// null.\n\t\t\t\tvar changed []uint64\n\t\t\t\tchanged, err1 = frag.clearBitsReportingChanges(tx, rowIDs, columnIDs)\n\t\t\t\tif err1 != nil {\n\t\t\t\t\treturn err1\n\t\t\t\t}\n\t\t\t\terr1 = f.MarkNotExisting(tx, changed, shard)\n\t\t\t\treturn err1\n\t\t\tcase !options.Clear:\n\t\t\t\terr1 = f.MarkExisting(tx, columnIDs, shard)\n\t\t\t\tif err1 != nil {\n\t\t\t\t\treturn err1\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\t// nothing to do. we'll fall on out of the TrackExistence\n\t\t\t\t// case and go ahead and import those bits naively\n\t\t\t}\n\t\t}\n\t\terr1 = frag.bulkImport(tx, rowIDs, columnIDs, options)\n\t\treturn err1\n\t}\n\n\tfieldType := f.Type()\n\n\t// Split import data by fragment.\n\tviews := make(map[string]*importData)\n\tvar timeStringBuf []byte\n\tvar timeViews [][]byte\n\tif len(q) > 0 {\n\t\t// We're supporting time quantums, so we need to store bits in a\n\t\t// number of views for every entry with a timestamp. We want to compute\n\t\t// time quantum view names for whatever combination of YMDH views\n\t\t// we have. But we don't want to allocate four strings per entry, or\n\t\t// recompute and recreate the entire string. We know that only the\n\t\t// YYYYMMDDHH part of the string changes over time.\n\t\ttimeStringBuf = make([]byte, len(viewStandard)+11)\n\t\tcopy(timeStringBuf, []byte(viewStandard))\n\t\tcopy(timeStringBuf[len(viewStandard):], []byte(\"_YYYYMMDDHH\"))\n\t\t// Now we have a buffer that contains\n\t\t// `standard_YYYYMMDDHH`. We also need storage space to hold several\n\t\t// slice headers, one per entry in q. These will hold the view names\n\t\t// corresponding to each letter in q.\n\t\ttimeViews = make([][]byte, len(q))\n\t}\n\t// This helper function records that a given column/row pair is relevant\n\t// to a specific view. We use a map lookup for the strings, but do the\n\t// actual operations using a slice so we're only writing each map entry\n\t// once, not once on every update.\n\tsee := func(name []byte, columnID uint64, rowID uint64) {\n\t\tvar ok bool\n\t\tvar data *importData\n\t\tif data, ok = views[string(name)]; !ok {\n\t\t\tdata = &importData{}\n\t\t\tviews[string(name)] = data\n\t\t}\n\t\tdata.RowIDs = append(data.RowIDs, rowID)\n\t\tdata.ColumnIDs = append(data.ColumnIDs, columnID)\n\t}\n\tfor i := range rowIDs {\n\t\trowID, columnID := rowIDs[i], columnIDs[i]\n\n\t\t// Bool-specific data validation.\n\t\tif fieldType == FieldTypeBool && rowID > 1 {\n\t\t\treturn errors.New(\"bool field imports only support values 0 and 1\")\n\t\t}\n\n\t\thasTime := len(timestamps) > i && timestamps[i] != 0\n\n\t\t// attach bit to standard view unless we have a timestamp and\n\t\t// have the NoStandardView option set\n\t\tif !hasTime || !f.options.NoStandardView {\n\t\t\tsee([]byte(viewStandard), columnID, rowID)\n\t\t}\n\t\tif hasTime {\n\t\t\t// attach bit to all the views for this timestamp. note that the\n\t\t\t// `timeViews` slice gets resliced and reused by this process, so\n\t\t\t// we don't have to allocate millions of tiny slices of slice headers.\n\t\t\ttimeViews = viewsByTimeInto(timeStringBuf, timeViews, time.Unix(0, timestamps[i]).UTC(), q)\n\t\t\tfor _, v := range timeViews {\n\t\t\t\tsee(v, columnID, rowID)\n\t\t\t}\n\t\t}\n\t}\n\ttx, finisher, err := qcx.GetTx(Txo{Write: true, Index: f.idx, Shard: shard})\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"qcx.GetTx\")\n\t}\n\tvar err1 error\n\tdefer finisher(&err1)\n\tfor viewName, data := range views {\n\t\tview, err := f.createViewIfNotExists(viewName)\n\t\tif err != nil {\n\t\t\treturn errors.Wrapf(err, \"creating view %s\", viewName)\n\t\t}\n\n\t\tfrag, err := view.CreateFragmentIfNotExists(shard)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"creating fragment\")\n\t\t}\n\n\t\terr1 = frag.bulkImport(tx, data.RowIDs, data.ColumnIDs, options)\n\t\tif err1 != nil {\n\t\t\treturn err1\n\t\t}\n\t}\n\t// If we are tracking existence, and don't have NoStandardView, we\n\t// create the existence view.\n\tif f.options.TrackExistence && !f.options.NoStandardView {\n\t\t// this dance with err1 is so the finisher gets called with\n\t\t// the right error value if we hit an error\n\t\terr1 = f.MarkExisting(tx, columnIDs, shard)\n\t\tif err1 != nil {\n\t\t\treturn err1\n\t\t}\n\t}\n\treturn nil\n}\n\n// importFloatValue imports floating point values. In current usage, this\n// should only ever be called with data for a single shard; the API calls\n// around this are splitting it up per shard.\nfunc (f *Field) importFloatValue(qcx *Qcx, columnIDs []uint64, values []float64, shard uint64, options *ImportOptions) error {\n\t// convert values to int64 values based on scale\n\tivalues := make([]int64, len(values))\n\tbsig := f.bsiGroup(f.name)\n\tif bsig == nil {\n\t\treturn errors.Wrap(ErrBSIGroupNotFound, f.name)\n\t}\n\tmult := math.Pow10(int(bsig.Scale))\n\tfor i, fval := range values {\n\t\tivalues[i] = int64(fval * mult)\n\t}\n\t// then call importValue\n\treturn f.importValue(qcx, columnIDs, ivalues, shard, options)\n}\n\n// importTimestampValue imports timestamp values. In current usage, this\n// should only ever be called with data for a single shard; the API calls\n// around this are splitting it up per shard.\nfunc (f *Field) importTimestampValue(qcx *Qcx, columnIDs []uint64, values []time.Time, shard uint64, options *ImportOptions) error {\n\tivalues := make([]int64, len(values))\n\tbsig := f.bsiGroup(f.name)\n\tif bsig == nil {\n\t\treturn errors.Wrap(ErrBSIGroupNotFound, f.name)\n\t}\n\n\tfor i, t := range values {\n\t\tivalues[i] = TimestampToVal(f.options.TimeUnit, t)\n\t}\n\treturn f.importValue(qcx, columnIDs, ivalues, shard, options)\n}\n\n// importValue bulk imports range-encoded value data. This function should\n// only be called with data for a single shard; the API calls that wrap\n// this handle splitting the data up per-shard.\nfunc (f *Field) importValue(qcx *Qcx, columnIDs []uint64, values []int64, shard uint64, options *ImportOptions) (err0 error) {\n\t// no data to import\n\tif len(columnIDs) == 0 {\n\t\treturn nil\n\t}\n\tif len(values) != len(columnIDs) {\n\t\treturn fmt.Errorf(\"importValue: mismatch between column IDs and values: %d != %d\", len(columnIDs), len(values))\n\t}\n\tviewName := viewBSIGroupPrefix + f.name\n\t// Get the bsiGroup so we know bitDepth.\n\tbsig := f.bsiGroup(f.name)\n\tif bsig == nil {\n\t\treturn errors.Wrap(ErrBSIGroupNotFound, f.name)\n\t}\n\n\t// We want to determine the required bit depth, in case the field doesn't\n\t// have as many bits currently as would be needed to represent these values,\n\t// but only if the values are in-range for the field.\n\tmin, max := values[0], values[0]\n\n\t// Check for minimum/maximum in case we need to expand the field's\n\t// stated bit depth.\n\tfor i := range columnIDs {\n\t\tcolumnID, value := columnIDs[i], values[i]\n\t\tif value > bsig.Max {\n\t\t\treturn errors.Wrapf(ErrBSIGroupValueTooHigh, \"index = %v, field = %v, column ID = %v, value %v is larger than max allowed %v\", f.index, f.name, columnID, value, bsig.Max)\n\t\t} else if value < bsig.Min {\n\t\t\treturn errors.Wrapf(ErrBSIGroupValueTooLow, \"index = %v, field = %v, column ID = %v, value %v is smaller than min allowed %v\", f.index, f.name, columnID, value, bsig.Min)\n\t\t}\n\t\tif value > max {\n\t\t\tmax = value\n\t\t}\n\t\tif value < min {\n\t\t\tmin = value\n\t\t}\n\n\t}\n\n\t// Timestamps differ from other BSI fields in that integer representations\n\t// of timestamps are already relative to the epoch (base).\n\t// So a user may set an epoch to 2022-03-01 as the start of a race\n\t// and import finishing times in seconds.\n\t// Timestamps ingested as timestamps are of course absolute, but by the time\n\t// we get here it would be a relative integer.\n\tif f.Type() != FieldTypeTimestamp {\n\t\tmin -= bsig.Base\n\t\tmax -= bsig.Base\n\t}\n\n\t// Determine the highest bit depth required by the min & max.\n\trequiredDepth := bitDepthInt64(min)\n\tif v := bitDepthInt64(max); v > requiredDepth {\n\t\trequiredDepth = v\n\t}\n\t// Increase bit depth if required.\n\tf.mu.Lock()\n\tbitDepth := bsig.BitDepth\n\tif requiredDepth > bitDepth {\n\t\tbsig.BitDepth = requiredDepth\n\t\tf.options.BitDepth = requiredDepth\n\t} else {\n\t\trequiredDepth = bitDepth\n\t}\n\tf.mu.Unlock()\n\n\tif columnIDs[0]/ShardWidth != shard {\n\t\treturn fmt.Errorf(\"requested import for shard %d, got record ID for shard %d\", shard, columnIDs[0]/ShardWidth)\n\t}\n\n\tview, err := f.createViewIfNotExists(viewName)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"creating view\")\n\t}\n\n\tfrag, err := view.CreateFragmentIfNotExists(shard)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"creating fragment\")\n\t}\n\n\tif bsig.Base != 0 {\n\t\tfor i, v := range values {\n\t\t\t// for Timestamps, values are already relative to their base (epoch)\n\t\t\t// for other types (IntFields), values need to be subtracted from their base (either Min or Max)\n\t\t\tif f.Type() == FieldTypeTimestamp {\n\t\t\t\tvalues[i] = v\n\t\t\t} else {\n\t\t\t\tvalues[i] = v - bsig.Base\n\t\t\t}\n\t\t}\n\t}\n\n\t// now we know which shard we discovered.\n\ttx, finisher, err := qcx.GetTx(Txo{Write: writable, Index: f.idx, Shard: frag.shard})\n\tif err != nil {\n\t\treturn err\n\t}\n\t// defer the finisher, so it will check the error returned and\n\t// possibly rollback.\n\tdefer finisher(&err0)\n\n\treturn frag.importValue(tx, columnIDs, values, requiredDepth, options.Clear)\n}\n\nfunc (f *Field) importRoaring(ctx context.Context, tx Tx, data []byte, shard uint64, viewName string, clear bool) error {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"Field.importRoaring\")\n\tdefer span.Finish()\n\n\tif viewName == \"\" {\n\t\tviewName = viewStandard\n\t}\n\tspan.LogKV(\"view\", viewName, \"bytes\", len(data), \"shard\", shard)\n\tview, err := f.createViewIfNotExists(viewName)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"creating view\")\n\t}\n\n\tfrag, err := view.CreateFragmentIfNotExists(shard)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"creating fragment\")\n\t}\n\tif err := frag.importRoaring(ctx, tx, data, clear); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\nfunc (f *Field) GetIndex() *Index {\n\treturn f.idx\n}\n\nfunc (f *Field) importRoaringOverwrite(ctx context.Context, tx Tx, data []byte, shard uint64, viewName string, block int) error {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"Field.importRoaringOverwrite\")\n\tdefer span.Finish()\n\n\tif viewName == \"\" {\n\t\tviewName = viewStandard\n\t}\n\tspan.LogKV(\"view\", viewName, \"bytes\", len(data), \"shard\", shard)\n\tview, err := f.createViewIfNotExists(viewName)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"creating view\")\n\t}\n\n\tfrag, err := view.CreateFragmentIfNotExists(shard)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"creating fragment\")\n\t}\n\tif err := frag.importRoaringOverwrite(ctx, tx, data, block); err != nil {\n\t\treturn err\n\t}\n\n\t// If field is int, decimal, or timestamp, then we need to update\n\t// field.options.BitDepth and bsiGroup.BitDepth based on the imported data.\n\tswitch f.Options().Type {\n\tcase FieldTypeInt, FieldTypeDecimal, FieldTypeTimestamp:\n\t\tfrag.mu.Lock()\n\t\tmaxRowID, _, err := frag.maxRow(tx, nil)\n\t\tfrag.mu.Unlock()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tvar bitDepth uint64\n\t\tif maxRowID+1 > bsiOffsetBit {\n\t\t\tbitDepth = uint64(maxRowID + 1 - bsiOffsetBit)\n\t\t}\n\n\t\tbsig := f.bsiGroup(f.name)\n\n\t\tf.mu.Lock()\n\t\tdefer f.mu.Unlock()\n\t\tif bitDepth > f.options.BitDepth {\n\t\t\tf.options.BitDepth = bitDepth\n\t\t}\n\t\tif bsig != nil {\n\t\t\tbsig.BitDepth = bitDepth\n\t\t}\n\t}\n\n\treturn nil\n}\n\ntype fieldSlice []*Field\n\nfunc (p fieldSlice) Swap(i, j int)      { p[i], p[j] = p[j], p[i] }\nfunc (p fieldSlice) Len() int           { return len(p) }\nfunc (p fieldSlice) Less(i, j int) bool { return p[i].Name() < p[j].Name() }\n\n// FieldInfo represents schema information for a field.\ntype FieldInfo struct {\n\tName        string       `json:\"name\"`\n\tCreatedAt   int64        `json:\"createdAt,omitempty\"`\n\tOwner       string       `json:\"owner\"`\n\tOptions     FieldOptions `json:\"options\"`\n\tCardinality *uint64      `json:\"cardinality,omitempty\"`\n\tViews       []*ViewInfo  `json:\"views,omitempty\"`\n}\n\ntype fieldInfoSlice []*FieldInfo\n\nfunc (p fieldInfoSlice) Swap(i, j int)      { p[i], p[j] = p[j], p[i] }\nfunc (p fieldInfoSlice) Len() int           { return len(p) }\nfunc (p fieldInfoSlice) Less(i, j int) bool { return p[i].Name < p[j].Name }\n\n// FieldOptions represents options to set when initializing a field.\ntype FieldOptions struct {\n\tBase           int64         `json:\"base,omitempty\"`\n\tBitDepth       uint64        `json:\"bitDepth,omitempty\"`\n\tMin            pql.Decimal   `json:\"min,omitempty\"`\n\tMax            pql.Decimal   `json:\"max,omitempty\"`\n\tScale          int64         `json:\"scale,omitempty\"`\n\tKeys           bool          `json:\"keys\"`\n\tNoStandardView bool          `json:\"noStandardView,omitempty\"`\n\tTrackExistence bool          `json:\"trackExistence,omitempty\"`\n\tCacheSize      uint32        `json:\"cacheSize,omitempty\"`\n\tCacheType      string        `json:\"cacheType,omitempty\"`\n\tType           string        `json:\"type,omitempty\"`\n\tTimeUnit       string        `json:\"timeUnit,omitempty\"`\n\tTimeQuantum    TimeQuantum   `json:\"timeQuantum,omitempty\"`\n\tForeignIndex   string        `json:\"foreignIndex\"`\n\tTTL            time.Duration `json:\"ttl,omitempty\"`\n}\n\n// newFieldOptions returns a new instance of FieldOptions\n// with applied and validated functional options.\nfunc newFieldOptions(opts ...FieldOption) (*FieldOptions, error) {\n\tfo := FieldOptions{}\n\tfor _, opt := range opts {\n\t\terr := opt(&fo)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tif fo.Keys {\n\t\tswitch fo.Type {\n\t\tcase FieldTypeInt:\n\t\t\treturn nil, ErrIntFieldWithKeys\n\n\t\tcase FieldTypeDecimal:\n\t\t\treturn nil, ErrDecimalFieldWithKeys\n\n\t\tcase FieldTypeTimestamp:\n\t\t\treturn nil, ErrTimestampFieldWithKeys\n\t\t}\n\t}\n\n\treturn &fo, nil\n}\n\n// applyDefaultOptions updates FieldOptions with the default\n// values if o does not contain a valid type.\nfunc applyDefaultOptions(o *FieldOptions) FieldOptions {\n\tif o == nil {\n\t\to = &FieldOptions{}\n\t}\n\tif o.Type == \"\" {\n\t\to.Type = DefaultFieldType\n\t\to.CacheType = DefaultCacheType\n\t\to.CacheSize = DefaultCacheSize\n\t}\n\treturn *o\n}\n\n// ActuallyTrackingExistence reflects the distinction between the\n// TrackExistence bool, which is enabled by default for most fields,\n// and whether we actually do existence tracking. Specifically,\n// we don't do existence tracking for time quantum fields which don't\n// have a standard view, or for BSI fields.\nfunc (o *FieldOptions) ActuallyTrackingExistence() bool {\n\tswitch o.Type {\n\tcase FieldTypeTime:\n\t\treturn o.TrackExistence && !o.NoStandardView\n\tcase FieldTypeInt, FieldTypeDecimal, FieldTypeTimestamp:\n\t\treturn false\n\tdefault:\n\t\treturn o.TrackExistence\n\t}\n}\n\n// MarshalJSON marshals FieldOptions to JSON such that\n// only those attributes associated to the field type\n// are included.\nfunc (o *FieldOptions) MarshalJSON() ([]byte, error) {\n\tswitch o.Type {\n\tcase FieldTypeSet, \"\":\n\t\treturn json.Marshal(struct {\n\t\t\tType      string `json:\"type\"`\n\t\t\tCacheType string `json:\"cacheType\"`\n\t\t\tCacheSize uint32 `json:\"cacheSize\"`\n\t\t\tKeys      bool   `json:\"keys\"`\n\t\t}{\n\t\t\to.Type,\n\t\t\to.CacheType,\n\t\t\to.CacheSize,\n\t\t\to.Keys,\n\t\t})\n\tcase FieldTypeInt:\n\t\treturn json.Marshal(struct {\n\t\t\tType         string      `json:\"type\"`\n\t\t\tBase         int64       `json:\"base\"`\n\t\t\tBitDepth     uint64      `json:\"bitDepth\"`\n\t\t\tMin          pql.Decimal `json:\"min\"`\n\t\t\tMax          pql.Decimal `json:\"max\"`\n\t\t\tKeys         bool        `json:\"keys\"`\n\t\t\tForeignIndex string      `json:\"foreignIndex\"`\n\t\t}{\n\t\t\to.Type,\n\t\t\to.Base,\n\t\t\to.BitDepth,\n\t\t\to.Min,\n\t\t\to.Max,\n\t\t\to.Keys,\n\t\t\to.ForeignIndex,\n\t\t})\n\tcase FieldTypeDecimal:\n\t\treturn json.Marshal(struct {\n\t\t\tType     string      `json:\"type\"`\n\t\t\tBase     int64       `json:\"base\"`\n\t\t\tScale    int64       `json:\"scale\"`\n\t\t\tBitDepth uint64      `json:\"bitDepth\"`\n\t\t\tMin      pql.Decimal `json:\"min\"`\n\t\t\tMax      pql.Decimal `json:\"max\"`\n\t\t\tKeys     bool        `json:\"keys\"`\n\t\t}{\n\t\t\to.Type,\n\t\t\to.Base,\n\t\t\to.Scale,\n\t\t\to.BitDepth,\n\t\t\to.Min,\n\t\t\to.Max,\n\t\t\to.Keys,\n\t\t})\n\tcase FieldTypeTimestamp:\n\t\tepoch, err := ValToTimestamp(o.TimeUnit, o.Base)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"translating val to timestamp\")\n\t\t}\n\n\t\treturn json.Marshal(struct {\n\t\t\tType     string      `json:\"type\"`\n\t\t\tEpoch    time.Time   `json:\"epoch\"`\n\t\t\tBitDepth uint64      `json:\"bitDepth\"`\n\t\t\tMin      pql.Decimal `json:\"min\"`\n\t\t\tMax      pql.Decimal `json:\"max\"`\n\t\t\tTimeUnit string      `json:\"timeUnit\"`\n\t\t}{\n\t\t\to.Type,\n\t\t\tepoch,\n\t\t\to.BitDepth,\n\t\t\to.Min,\n\t\t\to.Max,\n\t\t\to.TimeUnit,\n\t\t})\n\tcase FieldTypeTime:\n\t\treturn json.Marshal(struct {\n\t\t\tType           string        `json:\"type\"`\n\t\t\tTimeQuantum    TimeQuantum   `json:\"timeQuantum\"`\n\t\t\tKeys           bool          `json:\"keys\"`\n\t\t\tNoStandardView bool          `json:\"noStandardView\"`\n\t\t\tTTL            time.Duration `json:\"ttl\"`\n\t\t}{\n\t\t\to.Type,\n\t\t\to.TimeQuantum,\n\t\t\to.Keys,\n\t\t\to.NoStandardView,\n\t\t\to.TTL,\n\t\t})\n\tcase FieldTypeMutex:\n\t\treturn json.Marshal(struct {\n\t\t\tType      string `json:\"type\"`\n\t\t\tCacheType string `json:\"cacheType\"`\n\t\t\tCacheSize uint32 `json:\"cacheSize\"`\n\t\t\tKeys      bool   `json:\"keys\"`\n\t\t}{\n\t\t\to.Type,\n\t\t\to.CacheType,\n\t\t\to.CacheSize,\n\t\t\to.Keys,\n\t\t})\n\tcase FieldTypeBool:\n\t\treturn json.Marshal(struct {\n\t\t\tType string `json:\"type\"`\n\t\t}{\n\t\t\to.Type,\n\t\t})\n\t}\n\treturn nil, errors.Errorf(\"invalid field type: '%s'\", o.Type)\n}\n\n// List of bsiGroup types.\nconst (\n\tbsiGroupTypeInt = \"int\"\n)\n\nfunc isValidBSIGroupType(v string) bool {\n\tswitch v {\n\tcase bsiGroupTypeInt:\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// bsiBase is a helper function used to determine the default value\n// for base. Because base is not exposed as a field option argument,\n// it defaults to min, max, or 0 depending on the min/max range.\nfunc bsiBase(min, max int64) int64 {\n\tif min > 0 {\n\t\treturn min\n\t} else if max < 0 {\n\t\treturn max\n\t}\n\treturn 0\n}\n\n// bsiGroup represents a group of range-encoded rows on a field.\ntype bsiGroup struct {\n\tName     string `json:\"name,omitempty\"`\n\tType     string `json:\"type,omitempty\"`\n\tMin      int64  `json:\"min,omitempty\"`\n\tMax      int64  `json:\"max,omitempty\"`\n\tBase     int64  `json:\"base,omitempty\"`\n\tScale    int64  `json:\"scale,omitempty\"`\n\tTimeUnit string `json:\"timeUnit,omitempty\"`\n\tBitDepth uint64 `json:\"bitDepth,omitempty\"`\n}\n\n// baseValue adjusts the value to align with the range for Field for a certain\n// operation type.\n// Note: There is an edge case for GT and LT where this returns a baseValue\n// that does not fully encompass the range.\n// ex: Field.Min = 0, Field.Max = 1023\n// baseValue(LT, 2000) returns 1023, which will perform \"LT 1023\" and effectively\n// exclude any columns with value = 1023.\nfunc (b *bsiGroup) baseValue(op pql.Token, value int64) (baseValue int64, outOfRange bool) {\n\tmin, max := b.bitDepthMin(), b.bitDepthMax()\n\n\tif op == pql.GT || op == pql.GTE {\n\t\tif value > max {\n\t\t\treturn baseValue, true\n\t\t} else if value < min {\n\t\t\tbaseValue = int64(min - b.Base)\n\t\t\t// Address edge case noted in comments above.\n\t\t\tif op == pql.GT {\n\t\t\t\tbaseValue--\n\t\t\t}\n\t\t} else {\n\t\t\tbaseValue = int64(value - b.Base)\n\t\t}\n\t} else if op == pql.LT || op == pql.LTE {\n\t\tif value < min {\n\t\t\treturn baseValue, true\n\t\t} else if value > max {\n\t\t\tbaseValue = int64(max - b.Base)\n\t\t\t// Address edge case noted in comments above.\n\t\t\tif op == pql.LT {\n\t\t\t\tbaseValue++\n\t\t\t}\n\t\t} else {\n\t\t\tbaseValue = int64(value - b.Base)\n\t\t}\n\t} else if op == pql.EQ || op == pql.NEQ {\n\t\tif value < min || value > max {\n\t\t\treturn baseValue, true\n\t\t}\n\t\tbaseValue = int64(value - b.Base)\n\t}\n\treturn baseValue, false\n}\n\n// baseValueBetween adjusts the min/max value to align with the range for Field.\nfunc (b *bsiGroup) baseValueBetween(lo, hi int64) (baseValueLo, baseValueHi int64, outOfRange bool) {\n\tmin, max := b.bitDepthMin(), b.bitDepthMax()\n\tif hi < min || lo > max || hi < lo {\n\t\treturn 0, 0, true\n\t}\n\n\t// Limit lo/hi to possible bit range.\n\tif lo < min {\n\t\tlo = min\n\t}\n\tif hi > max {\n\t\thi = max\n\t}\n\treturn lo - b.Base, hi - b.Base, false\n}\n\nfunc (b *bsiGroup) validate() error {\n\tif b.Name == \"\" {\n\t\treturn ErrBSIGroupNameRequired\n\t} else if !isValidBSIGroupType(b.Type) {\n\t\treturn ErrInvalidBSIGroupType\n\t}\n\treturn nil\n}\n\n// bitDepthMin returns the minimum value possible for the current bit depth.\nfunc (b *bsiGroup) bitDepthMin() int64 {\n\treturn b.Base - (1 << b.BitDepth) + 1\n}\n\n// bitDepthMax returns the maximum value possible for the current bit depth.\nfunc (b *bsiGroup) bitDepthMax() int64 {\n\treturn b.Base + (1 << b.BitDepth) - 1\n}\n\n// Cache types.\nconst (\n\tCacheTypeLRU    = \"lru\"\n\tCacheTypeRanked = \"ranked\"\n\tCacheTypeNone   = \"none\"\n)\n\n// isValidCacheType returns true if v is a valid cache type.\nfunc isValidCacheType(v string) bool {\n\tswitch v {\n\tcase CacheTypeLRU, CacheTypeRanked, CacheTypeNone:\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// bitDepth returns the number of bits required to store a value.\nfunc bitDepth(v uint64) uint64 {\n\treturn uint64(bits.Len64(v))\n}\n\n// bitDepthInt64 returns the required bit depth for abs(v).\nfunc bitDepthInt64(v int64) uint64 {\n\tif v < 0 {\n\t\treturn bitDepth(uint64(-v))\n\t}\n\treturn bitDepth(uint64(v))\n}\n\n// FormatQualifiedFieldName generates a qualified name for the field to be used with Tx operations.\nfunc FormatQualifiedFieldName(index, field string) string {\n\treturn fmt.Sprintf(\"%s\\x00%s\\x00\", index, field)\n}\n\n// persistView stores the view information in etcd.\nfunc (f *Field) persistView(ctx context.Context, cvm *CreateViewMessage) error {\n\tif cvm.Index == \"\" {\n\t\treturn ErrIndexRequired\n\t} else if cvm.Field == \"\" {\n\t\treturn ErrFieldRequired\n\t} else if cvm.View == \"\" {\n\t\treturn ErrViewRequired\n\t}\n\n\treturn f.holder.Schemator.CreateView(ctx, cvm.Index, cvm.Field, cvm.View)\n}\n\n// Timestamp field ranges.\nvar (\n\tDefaultEpoch     = time.Unix(0, 0).UTC()            // 1970-01-01T00:00:00Z\n\tMinTimestampNano = time.Unix(-1<<32, 0).UTC()       // 1833-11-24T17:31:44Z\n\tMaxTimestampNano = time.Unix(1<<32, 0).UTC()        // 2106-02-07T06:28:16Z\n\tMinTimestamp     = time.Unix(-62135596799, 0).UTC() // 0001-01-01T00:00:01Z\n\tMaxTimestamp     = time.Unix(253402300799, 0).UTC() // 9999-12-31T23:59:59Z\n)\n\n// Constants related to timestamp.\nconst (\n\tTimeUnitSeconds      = \"s\"\n\tTimeUnitMilliseconds = \"ms\"\n\tTimeUnitMicroseconds = \"s\"\n\tTimeUnitUSeconds     = \"us\"\n\tTimeUnitNanoseconds  = \"ns\"\n)\n\n// IsValidTimeUnit returns true if unit is valid.\nfunc IsValidTimeUnit(unit string) bool {\n\tswitch unit {\n\tcase TimeUnitSeconds, TimeUnitMilliseconds, TimeUnitMicroseconds, TimeUnitUSeconds, TimeUnitNanoseconds:\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// TimeUnitNanos returns the number of nanoseconds in unit.\nfunc TimeUnitNanos(unit string) int64 {\n\tswitch unit {\n\tcase TimeUnitSeconds:\n\t\treturn int64(time.Second)\n\tcase TimeUnitMilliseconds:\n\t\treturn int64(time.Millisecond)\n\tcase TimeUnitMicroseconds, TimeUnitUSeconds:\n\t\treturn int64(time.Microsecond)\n\tdefault:\n\t\treturn int64(time.Nanosecond)\n\t}\n}\n\n// CheckEpochOutOfRange checks if the epoch is after max or before min\nfunc CheckEpochOutOfRange(epoch, min, max time.Time) error {\n\tif epoch.After(max) || epoch.Before(min) {\n\t\treturn errors.Errorf(\"custom epoch too far from Unix epoch: %s\", epoch)\n\t}\n\treturn nil\n}\n\nfunc (f *Field) SortShardRow(tx Tx, shard uint64, filter *Row, sort_desc bool) (*SortedRow, error) {\n\tbsig := f.bsiGroup(f.name)\n\tif bsig == nil {\n\t\treturn nil, errors.New(\"bsig is nil\")\n\t}\n\n\tview := f.view(viewBSIGroupPrefix + f.name)\n\tif view == nil {\n\t\treturn nil, errors.New(\"view is nil\")\n\t}\n\n\tfragment := view.Fragment(shard)\n\tif fragment == nil {\n\t\treturn nil, errors.New(\"fragment is nil\")\n\t}\n\n\treturn fragment.sortBsiData(tx, filter, bsig.BitDepth, sort_desc)\n}\n"
        },
        {
          "name": "field_internal_test.go",
          "type": "blob",
          "size": 25.69921875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"math\"\n\t\"reflect\"\n\t\"strconv\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/pql\"\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\t\"github.com/featurebasedb/featurebase/v3/shardwidth\"\n\t. \"github.com/featurebasedb/featurebase/v3/vprint\" // nolint:staticcheck\n)\n\n// CorruptAMutex breaks a mutex in order to test the mutex-corruption stuff.\n// Note the horrible crime here: This is an exported function which exists only\n// in test builds. This is so that external tests, which aren't inside this\n// package, can call this exported function, and thus get access to functionality\n// that would otherwise not be available to them.\n//\n// This always sets row 3 in column 0 of each shard it finds. Populate the\n// field with existing shards first.\nfunc CorruptAMutex(tb testing.TB, field *Field, qcx *Qcx) {\n\tv := field.view(viewStandard)\n\tif v == nil {\n\t\ttb.Fatalf(\"creating view failed\")\n\t}\n\tfrags := v.allFragments()\n\tfor _, frag := range frags {\n\t\tfunc() {\n\t\t\ttx, finisher, err := qcx.GetTx(Txo{Write: true, Index: field.idx, Shard: frag.shard})\n\t\t\tdefer finisher(&err)\n\t\t\tif err != nil {\n\t\t\t\ttb.Fatalf(\"getting tx: %v\", err)\n\t\t\t}\n\t\t\t// set a bonus bit, bypassing the mutex handling\n\t\t\tfrag.mu.Lock()\n\t\t\t_, err = frag.unprotectedSetBit(tx, 3, (frag.shard<<shardwidth.Exponent)+1)\n\t\t\tfrag.mu.Unlock()\n\t\t\tif err != nil {\n\t\t\t\ttb.Fatalf(\"setting bit: %v\", err)\n\t\t\t}\n\t\t}()\n\t}\n}\n\n// Ensure a bsiGroup can adjust to its baseValue.\nfunc TestBSIGroup_BaseValue(t *testing.T) {\n\tb0 := &bsiGroup{\n\t\tName:     \"b0\",\n\t\tType:     bsiGroupTypeInt,\n\t\tBase:     -100,\n\t\tBitDepth: 10,\n\t\tMin:      -1000,\n\t\tMax:      1000,\n\t}\n\tb1 := &bsiGroup{\n\t\tName:     \"b1\",\n\t\tType:     bsiGroupTypeInt,\n\t\tBase:     0,\n\t\tBitDepth: 8,\n\t\tMin:      -255,\n\t\tMax:      255,\n\t}\n\tb2 := &bsiGroup{\n\t\tName:     \"b2\",\n\t\tType:     bsiGroupTypeInt,\n\t\tBase:     100,\n\t\tBitDepth: 11,\n\t\tMin:      math.MinInt64,\n\t\tMax:      math.MaxInt64,\n\t}\n\n\tt.Run(\"Normal Condition\", func(t *testing.T) {\n\t\tfor i, tt := range []struct {\n\t\t\tf             *bsiGroup\n\t\t\top            pql.Token\n\t\t\tval           int64\n\t\t\texpBaseValue  int64\n\t\t\texpOutOfRange bool\n\t\t}{\n\t\t\t// LT\n\t\t\t{b0, pql.LT, 5, 105, false},\n\t\t\t{b0, pql.LT, -8, 92, false},\n\t\t\t{b0, pql.LT, -108, -8, false},\n\t\t\t{b0, pql.LT, 1005, 1024, false},\n\t\t\t{b0, pql.LT, 0, 100, false},\n\n\t\t\t{b1, pql.LT, 5, 5, false},\n\t\t\t{b1, pql.LT, -8, -8, false},\n\t\t\t{b1, pql.LT, 1005, 256, false},\n\t\t\t{b1, pql.LT, 0, 0, false},\n\n\t\t\t{b2, pql.LT, 5, -95, false},\n\t\t\t{b2, pql.LT, -8, -108, false},\n\t\t\t{b2, pql.LT, 105, 5, false},\n\t\t\t{b2, pql.LT, 1105, 1005, false},\n\n\t\t\t// GT\n\t\t\t{b0, pql.GT, -5, 95, false},\n\t\t\t{b0, pql.GT, 5, 105, false},\n\t\t\t{b0, pql.GT, 905, 1005, false},\n\t\t\t{b0, pql.GT, 0, 100, false},\n\n\t\t\t{b1, pql.GT, 5, 5, false},\n\t\t\t{b1, pql.GT, -8, -8, false},\n\t\t\t{b1, pql.GT, 1005, 0, true},\n\t\t\t{b1, pql.GT, 0, 0, false},\n\t\t\t{b1, pql.GT, -300, -256, false},\n\n\t\t\t{b2, pql.GT, 5, -95, false},\n\t\t\t{b2, pql.GT, -8, -108, false},\n\t\t\t{b2, pql.GT, 105, 5, false},\n\t\t\t{b2, pql.GT, 1105, 1005, false},\n\n\t\t\t// EQ\n\t\t\t{b0, pql.EQ, -105, -5, false},\n\t\t\t{b0, pql.EQ, 5, 105, false},\n\t\t\t{b0, pql.EQ, 905, 1005, false},\n\t\t\t{b0, pql.EQ, 0, 100, false},\n\n\t\t\t{b1, pql.EQ, 5, 5, false},\n\t\t\t{b1, pql.EQ, -8, -8, false},\n\t\t\t{b1, pql.EQ, 1005, 0, true},\n\t\t\t{b1, pql.EQ, 0, 0, false},\n\n\t\t\t{b2, pql.EQ, 5, -95, false},\n\t\t\t{b2, pql.EQ, -8, -108, false},\n\t\t\t{b2, pql.EQ, 105, 5, false},\n\t\t\t{b2, pql.EQ, 1105, 1005, false},\n\t\t} {\n\t\t\tt.Run(fmt.Sprint(i), func(t *testing.T) {\n\t\t\t\tbv, oor := tt.f.baseValue(tt.op, tt.val)\n\t\t\t\tif oor != tt.expOutOfRange || !reflect.DeepEqual(bv, tt.expBaseValue) {\n\t\t\t\t\tt.Errorf(\"%s) baseValue(%s, %v)=(%v, %v), expected (%v, %v)\", tt.f.Name, tt.op, tt.val, bv, oor, tt.expBaseValue, tt.expOutOfRange)\n\t\t\t\t}\n\t\t\t})\n\t\t}\n\t})\n\n\tt.Run(\"Between Condition\", func(t *testing.T) {\n\t\tfor i, tt := range []struct {\n\t\t\tf               *bsiGroup\n\t\t\tpredMin         int64\n\t\t\tpredMax         int64\n\t\t\texpBaseValueMin int64\n\t\t\texpBaseValueMax int64\n\t\t\texpOutOfRange   bool\n\t\t}{\n\n\t\t\t{b0, -205, -105, -105, -5, false},\n\t\t\t{b0, -105, 80, -5, 180, false},\n\t\t\t{b0, 5, 20, 105, 120, false},\n\t\t\t{b0, 20, 1005, 120, 1023, false},\n\t\t\t{b0, 1005, 2000, 0, 0, true},\n\n\t\t\t{b1, -105, -5, -105, -5, false},\n\t\t\t{b1, -5, 20, -5, 20, false},\n\t\t\t{b1, 5, 20, 5, 20, false},\n\t\t\t{b1, 20, 1005, 20, 255, false},\n\t\t\t{b1, 1005, 2000, 0, 0, true},\n\t\t\t{b1, 0, -1, 0, 0, true},\n\n\t\t\t{b2, 5, 95, -95, -5, false},\n\t\t\t{b2, 95, 120, -5, 20, false},\n\t\t\t{b2, 105, 120, 5, 20, false},\n\t\t\t{b2, 120, 1105, 20, 1005, false},\n\t\t\t{b2, 1105, 2000, 1005, 1900, false},\n\t\t} {\n\t\t\tmin, max, oor := tt.f.baseValueBetween(tt.predMin, tt.predMax)\n\t\t\tif !reflect.DeepEqual(min, tt.expBaseValueMin) || !reflect.DeepEqual(max, tt.expBaseValueMax) || oor != tt.expOutOfRange {\n\t\t\t\tt.Errorf(\"%d. %s) baseValueBetween(%v, %v)=(%v, %v, %v), expected (%v, %v, %v)\", i, tt.f.Name, tt.predMin, tt.predMax, min, max, oor, tt.expBaseValueMin, tt.expBaseValueMax, tt.expOutOfRange)\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc TestField_ValCountize(t *testing.T) {\n\t_, _, f := newTestField(t)\n\t// check that you get an empty val count and err\n\t// BSIGroupNotFound on nil bsig from\n\t// f.bsiGroup(f.name)\n\tf.bsiGroups = []*bsiGroup{}\n\tv, err := f.valCountize(42, 42, nil)\n\tif !reflect.DeepEqual(v, ValCount{}) {\n\t\tt.Errorf(\"expected %v, got %v\", ValCount{}, v)\n\t}\n\tif err != ErrBSIGroupNotFound {\n\t\tt.Errorf(\"expected %v, got %v\", ErrBSIGroupNotFound, err)\n\t}\n\n}\n\n// Ensure field can open and retrieve a view.\nfunc TestField_DeleteView(t *testing.T) {\n\t_, _, f := newTestField(t)\n\n\tviewName := viewStandard + \"_v\"\n\n\t// Create view.\n\tview, err := f.createViewIfNotExists(viewName)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t} else if view == nil {\n\t\tt.Fatal(\"expected view\")\n\t}\n\n\terr = f.deleteView(viewName)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif f.view(viewName) != nil {\n\t\tt.Fatal(\"view still exists in field\")\n\t}\n\n\t// Recreate view with same name, verify that the old view was not reused.\n\tview2, err := f.createViewIfNotExists(viewName)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t} else if view == view2 {\n\t\tt.Fatal(\"failed to create new view\")\n\t}\n}\n\n// reopenTestField closes the field's parent index, then\n// reopens it using its cached schema, and returns the corresponding\n// field data structure from the reopened index.\nfunc reopenTestField(t testing.TB, f *Field) (*Field, error) {\n\tname := f.Name()\n\tif err := f.idx.Close(); err != nil {\n\t\tf.idx = nil\n\t\treturn nil, err\n\t}\n\tschema, err := f.holder.Schemator.Schema(context.Background())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := f.idx.OpenWithSchema(schema[f.idx.name]); err != nil {\n\t\tf.idx = nil\n\t\treturn nil, err\n\t}\n\treturn f.idx.Field(name), nil\n}\n\n// testFieldSetBit sets a bit and checks for an error, using a provided qcx and an\n// optional timestamp or series of timestamps. if multiple times are provided,\n// the underlying set bit operation is repeated for all of them.\nfunc testFieldSetBit(tb testing.TB, qcx *Qcx, f *Field, row, col uint64, ts ...time.Time) {\n\tif len(ts) == 0 {\n\t\t_, err := f.SetBit(qcx, row, col, nil)\n\t\tif err != nil {\n\t\t\ttb.Fatalf(\"setting bit: %v\", err)\n\t\t}\n\t}\n\tfor _, t := range ts {\n\t\t_, err := f.SetBit(qcx, row, col, &t)\n\t\tif err != nil {\n\t\t\ttb.Fatalf(\"setting bit: %v\", err)\n\t\t}\n\t}\n}\n\n// Ensure field can open and retrieve a view.\nfunc TestField_CreateViewIfNotExists(t *testing.T) {\n\t_, _, f := newTestField(t)\n\n\t// Create view.\n\tview, err := f.createViewIfNotExists(\"v\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t} else if view == nil {\n\t\tt.Fatal(\"expected view\")\n\t}\n\n\t// Retrieve existing view.\n\tview2, err := f.createViewIfNotExists(\"v\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t} else if view != view2 {\n\t\tt.Fatal(\"view mismatch\")\n\t}\n\n\tif view != f.view(\"v\") {\n\t\tt.Fatal(\"view mismatch\")\n\t}\n}\n\nfunc TestField_SetTimeQuantum(t *testing.T) {\n\t_, _, f := newTestField(t, OptFieldTypeTime(TimeQuantum(\"YMDH\"), \"0\"))\n\n\t// Retrieve time quantum.\n\tif q := f.TimeQuantum(); q != TimeQuantum(\"YMDH\") {\n\t\tt.Fatalf(\"unexpected quantum: %s\", q)\n\t}\n\n\t// Reload field and verify that it is persisted.\n\tf, err := reopenTestField(t, f)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t} else if q := f.TimeQuantum(); q != TimeQuantum(\"YMDH\") {\n\t\tt.Fatalf(\"unexpected quantum (reopen): %s\", q)\n\t}\n}\n\nfunc TestField_RowTime(t *testing.T) {\n\t_, _, f := newTestField(t, OptFieldTypeTime(TimeQuantum(\"YMDH\"), \"0\"))\n\n\t// Obtain transaction.\n\tqcx := f.holder.Txf().NewWritableQcx()\n\tdefer qcx.Abort()\n\n\ttestFieldSetBit(t, qcx, f, 1, 1, time.Date(2010, time.January, 5, 12, 0, 0, 0, time.UTC))\n\ttestFieldSetBit(t, qcx, f, 1, 2, time.Date(2011, time.January, 5, 12, 0, 0, 0, time.UTC))\n\ttestFieldSetBit(t, qcx, f, 1, 3, time.Date(2010, time.February, 5, 12, 0, 0, 0, time.UTC))\n\ttestFieldSetBit(t, qcx, f, 1, 4, time.Date(2010, time.January, 6, 12, 0, 0, 0, time.UTC))\n\ttestFieldSetBit(t, qcx, f, 1, 5, time.Date(2010, time.January, 5, 13, 0, 0, 0, time.UTC))\n\n\t// Warning: Right now this is misleading, and doesn't really do anything. We\n\t// already committed each change as we got there. SOME DAY we will fix this.\n\tPanicOn(qcx.Finish())\n\n\tqcx = f.holder.Txf().NewQcx()\n\tdefer qcx.Abort()\n\n\t// obtain 2nd transaction to read it back.\n\tqcx = f.holder.Txf().NewQcx()\n\tdefer qcx.Abort()\n\n\tif r, err := f.RowTime(qcx, 1, time.Date(2010, time.November, 5, 12, 0, 0, 0, time.UTC), \"Y\"); err != nil {\n\t\tt.Fatal(err)\n\t} else if !reflect.DeepEqual(r.Columns(), []uint64{1, 3, 4, 5}) {\n\t\tt.Fatalf(\"wrong columns: %#v\", r.Columns())\n\t}\n\n\tif r, err := f.RowTime(qcx, 1, time.Date(2010, time.February, 7, 13, 0, 0, 0, time.UTC), \"YM\"); err != nil {\n\t\tt.Fatal(err)\n\t} else if !reflect.DeepEqual(r.Columns(), []uint64{3}) {\n\t\tt.Fatalf(\"wrong columns: %#v\", r.Columns())\n\t}\n\n\tif r, err := f.RowTime(qcx, 1, time.Date(2010, time.February, 7, 13, 0, 0, 0, time.UTC), \"M\"); err != nil {\n\t\tt.Fatal(err)\n\t} else if !reflect.DeepEqual(r.Columns(), []uint64{3}) {\n\t\tt.Fatalf(\"wrong columns: %#v\", r.Columns())\n\t}\n\n\tif r, err := f.RowTime(qcx, 1, time.Date(2010, time.January, 5, 12, 0, 0, 0, time.UTC), \"MD\"); err != nil {\n\t\tt.Fatal(err)\n\t} else if !reflect.DeepEqual(r.Columns(), []uint64{1, 5}) {\n\t\tt.Fatalf(\"wrong columns: %#v\", r.Columns())\n\t}\n\n\tif r, err := f.RowTime(qcx, 1, time.Date(2010, time.January, 5, 13, 0, 0, 0, time.UTC), \"MDH\"); err != nil {\n\t\tt.Fatal(err)\n\t} else if !reflect.DeepEqual(r.Columns(), []uint64{5}) {\n\t\tt.Fatalf(\"wrong columns: %#v\", r.Columns())\n\t}\n\n}\n\nfunc TestField_PersistAvailableShards(t *testing.T) {\n\tavailableShardFileFlushDuration.Set(200 * time.Millisecond) //shorten the default time to force a file write\n\t_, _, f := newTestField(t)\n\n\t// bm represents remote available shards.\n\tbm := roaring.NewBitmap(1, 2, 3)\n\n\tif err := f.AddRemoteAvailableShards(bm); err != nil {\n\t\tt.Fatal(err)\n\t}\n\ttime.Sleep(2 * availableShardFileFlushDuration.Get())\n\n\t// Reload field and verify that shard data is persisted.\n\tf, err := reopenTestField(t, f)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t} else if !reflect.DeepEqual(f.protectedRemoteAvailableShards().Slice(), bm.Slice()) {\n\t\tt.Fatalf(\"unexpected available shards (reopen). expected: %v, but got: %v\", bm.Slice(), f.protectedRemoteAvailableShards().Slice())\n\t}\n\n}\n\n// Ensure that FieldOptions.Base defaults to the correct value.\nfunc TestBSIGroup_BaseDefaultValue(t *testing.T) {\n\tfor i, tt := range []struct {\n\t\tmin     int64\n\t\tmax     int64\n\t\texpBase int64\n\t}{\n\t\t{100, 200, 100},\n\t\t{-100, 100, 0},\n\t\t{-200, -100, -100},\n\t} {\n\t\tfn := OptFieldTypeInt(tt.min, tt.max)\n\n\t\t// Apply functional option.\n\t\tfo := FieldOptions{}\n\t\terr := fn(&fo)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"test %d, applying functional option: %s\", i, err.Error())\n\t\t}\n\n\t\tif fo.Base != tt.expBase {\n\t\t\tt.Fatalf(\"test %d, unexpected FieldOptions.Base value. expected: %d, but got: %d\", i, tt.expBase, fo.Base)\n\t\t}\n\t}\n}\n\nfunc TestField_ApplyOptions(t *testing.T) {\n\tfor i, tt := range []struct {\n\t\topts    FieldOptions\n\t\texpOpts FieldOptions\n\t}{\n\t\t{\n\t\t\tFieldOptions{\n\t\t\t\tType:      FieldTypeSet,\n\t\t\t\tCacheType: CacheTypeNone,\n\t\t\t\tCacheSize: 0,\n\t\t\t},\n\t\t\tFieldOptions{\n\t\t\t\tType:      FieldTypeSet,\n\t\t\t\tCacheType: CacheTypeNone,\n\t\t\t\tCacheSize: 0,\n\t\t\t},\n\t\t},\n\t} {\n\n\t\tfld := &Field{}\n\t\tfld.options = applyDefaultOptions(&FieldOptions{})\n\n\t\tif err := fld.applyOptions(tt.opts); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif fld.options.CacheType != tt.expOpts.CacheType {\n\t\t\tt.Fatalf(\"test %d, unexpected FieldOptions.CacheType value. expected: %s, but got: %s\", i, tt.expOpts.CacheType, fld.options.CacheType)\n\t\t} else if fld.options.CacheSize != tt.expOpts.CacheSize {\n\t\t\tt.Fatalf(\"test %d, unexpected FieldOptions.CacheSize value. expected: %d, but got: %d\", i, tt.expOpts.CacheSize, fld.options.CacheSize)\n\t\t}\n\t}\n}\n\n// Ensure that importValue handles requiredDepth correctly.\n// This test sets the same column value to 1, then 8, then 1.\n// A previous bug was incorrectly determining bitDepth based\n// on the values in the import, and not taking existing values\n// into consideration. This would cause an import of 1/8/1\n// to result in a value of 9 instead of 1.\nfunc TestBSIGroup_importValue(t *testing.T) {\n\t_, _, f := newTestField(t, OptFieldTypeInt(-100, 200))\n\n\tqcx := f.idx.holder.txf.NewQcx()\n\tdefer qcx.Abort()\n\n\toptions := &ImportOptions{}\n\tfor i, tt := range []struct {\n\t\tcolumnIDs []uint64\n\t\tvalues    []int64\n\t\tcheckVal  int64\n\t\texpCols   []uint64\n\t}{\n\t\t{\n\t\t\t[]uint64{100},\n\t\t\t[]int64{1},\n\t\t\t1,\n\t\t\t[]uint64{100},\n\t\t},\n\t\t{\n\t\t\t[]uint64{100},\n\t\t\t[]int64{8},\n\t\t\t8,\n\t\t\t[]uint64{100},\n\t\t},\n\t\t{\n\t\t\t[]uint64{100},\n\t\t\t[]int64{1},\n\t\t\t1,\n\t\t\t[]uint64{100},\n\t\t},\n\t} {\n\t\tif err := f.importValue(qcx, tt.columnIDs, tt.values, 0, options); err != nil {\n\t\t\tt.Fatalf(\"test %d, importing values: %s\", i, err.Error())\n\t\t}\n\t\tPanicOn(qcx.Finish())\n\t\tqcx.Reset()\n\t\tif row, err := f.Range(qcx, f.name, pql.EQ, tt.checkVal); err != nil {\n\t\t\tt.Fatalf(\"test %d, getting range: %s\", i, err.Error())\n\t\t} else if !reflect.DeepEqual(row.Columns(), tt.expCols) {\n\t\t\tt.Fatalf(\"test %d, expected columns: %v, but got: %v\", i, tt.expCols, row.Columns())\n\t\t}\n\t\tPanicOn(qcx.Finish())\n\t\tqcx.Reset()\n\t} // loop\n}\n\n// benchmarkImportValues is a helper function to explore, very roughly, the cost\n// of setting values using the special setter used for imports.\nfunc benchmarkFieldImportValues(b *testing.B, qcx *Qcx, bitDepth uint64, f *Field, cfunc func(uint64) uint64) {\n\tbatches := makeBenchmarkImportValueData(b, bitDepth, cfunc)\n\tfor _, req := range batches {\n\t\t// NOTE: We assume everything's in Shard 0 for now.\n\t\terr := f.importValue(qcx, req.ColumnIDs, req.Values, 0, &ImportOptions{})\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"error importing values: %s\", err)\n\t\t}\n\t}\n}\n\n// Benchmark performance of setValue for BSI ranges.\nfunc BenchmarkField_ImportValue(b *testing.B) {\n\tdepths := []uint64{4, 8, 16, 32}\n\n\tfor _, bitDepth := range depths {\n\t\t_, _, f := newTestField(b, OptFieldTypeInt(0, 1<<bitDepth))\n\n\t\tqcx := f.idx.holder.txf.NewQcx()\n\t\tdefer qcx.Abort()\n\t\tname := fmt.Sprintf(\"Depth%d\", bitDepth)\n\t\tb.Run(name+\"_Sparse\", func(b *testing.B) {\n\t\t\tbenchmarkFieldImportValues(b, qcx, bitDepth, f, func(u uint64) uint64 { return (u + 19) & (ShardWidth - 1) })\n\t\t})\n\t\tb.Run(name+\"_Dense\", func(b *testing.B) {\n\t\t\tbenchmarkFieldImportValues(b, qcx, bitDepth, f, func(u uint64) uint64 { return (u + 1) & (ShardWidth - 1) })\n\t\t})\n\t}\n}\n\nfunc TestIntField_MinMaxForShard(t *testing.T) {\n\t_, _, f := newTestField(t, OptFieldTypeInt(-100, 200))\n\n\tqcx := f.idx.holder.txf.NewQcx()\n\tdefer qcx.Abort()\n\n\toptions := &ImportOptions{}\n\tfor i, test := range []struct {\n\t\tname      string\n\t\tcolumnIDs []uint64\n\t\tvalues    []int64\n\t\texpMax    ValCount\n\t\texpMin    ValCount\n\t}{\n\t\t{\n\t\t\tname:      \"zero\",\n\t\t\tcolumnIDs: []uint64{},\n\t\t\tvalues:    []int64{},\n\t\t},\n\t\t{\n\t\t\tname:      \"single\",\n\t\t\tcolumnIDs: []uint64{1},\n\t\t\tvalues:    []int64{10},\n\t\t\texpMax:    ValCount{Val: 10, Count: 1},\n\t\t\texpMin:    ValCount{Val: 10, Count: 1},\n\t\t},\n\t\t{\n\t\t\tname:      \"twovals\",\n\t\t\tcolumnIDs: []uint64{1, 2},\n\t\t\tvalues:    []int64{10, 20},\n\t\t\texpMax:    ValCount{Val: 20, Count: 1},\n\t\t\texpMin:    ValCount{Val: 10, Count: 1},\n\t\t},\n\t\t{\n\t\t\tname:      \"multiplecounts\",\n\t\t\tcolumnIDs: []uint64{1, 2, 3, 4, 5},\n\t\t\tvalues:    []int64{10, 20, 10, 10, 20},\n\t\t\texpMax:    ValCount{Val: 20, Count: 2},\n\t\t\texpMin:    ValCount{Val: 10, Count: 3},\n\t\t},\n\t\t{\n\t\t\tname:      \"middlevals\",\n\t\t\tcolumnIDs: []uint64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10},\n\t\t\tvalues:    []int64{10, 20, 10, 10, 20, 11, 12, 11, 13, 11},\n\t\t\texpMax:    ValCount{Val: 20, Count: 2},\n\t\t\texpMin:    ValCount{Val: 10, Count: 3},\n\t\t},\n\t} {\n\t\tt.Run(test.name+strconv.Itoa(i), func(t *testing.T) {\n\t\t\tif err := f.importValue(qcx, test.columnIDs, test.values, 0, options); err != nil {\n\t\t\t\tt.Fatalf(\"test %d, importing values: %s\", i, err.Error())\n\t\t\t}\n\t\t\tPanicOn(qcx.Finish())\n\t\t\tqcx.Reset()\n\n\t\t\tshard := uint64(0)\n\t\t\t// Rollback below manually, because we are in a loop.\n\n\t\t\tmaxvc, err := f.MaxForShard(qcx, shard, nil)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"getting max for shard: %v\", err)\n\t\t\t}\n\t\t\tif maxvc != test.expMax {\n\t\t\t\tt.Fatalf(\"max expected:\\n%+v\\ngot:\\n%+v\", test.expMax, maxvc)\n\t\t\t}\n\n\t\t\tminvc, err := f.MinForShard(qcx, shard, nil)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"getting min for shard: %v\", err)\n\t\t\t}\n\t\t\tif minvc != test.expMin {\n\t\t\t\tt.Fatalf(\"min expected:\\n%+v\\ngot:\\n%+v\", test.expMin, minvc)\n\t\t\t}\n\t\t})\n\t}\n}\n\n// Ensure we get errors when they are expected.\nfunc TestDecimalField_MinMaxBoundaries(t *testing.T) {\n\tth := newTestHolder(t)\n\n\tfor i, test := range []struct {\n\t\tscale  int64\n\t\tmin    pql.Decimal\n\t\tmax    pql.Decimal\n\t\texpErr bool\n\t}{\n\t\t{\n\t\t\tscale:  3,\n\t\t\tmin:    pql.NewDecimal(math.MinInt64, 0),\n\t\t\tmax:    pql.NewDecimal(math.MaxInt64, 0),\n\t\t\texpErr: true,\n\t\t},\n\t\t{\n\t\t\tscale:  3,\n\t\t\tmin:    pql.NewDecimal(math.MinInt64, 3),\n\t\t\tmax:    pql.NewDecimal(math.MaxInt64, 3),\n\t\t\texpErr: false,\n\t\t},\n\t\t{\n\t\t\tscale:  3,\n\t\t\tmin:    pql.NewDecimal(44, 0),\n\t\t\tmax:    pql.NewDecimal(88, 0),\n\t\t\texpErr: false,\n\t\t},\n\t\t{\n\t\t\tscale:  3,\n\t\t\tmin:    pql.NewDecimal(-44, 0),\n\t\t\tmax:    pql.NewDecimal(88, 0),\n\t\t\texpErr: false,\n\t\t},\n\t\t{\n\t\t\tscale:  19,\n\t\t\tmin:    pql.NewDecimal(1, 0),\n\t\t\tmax:    pql.NewDecimal(2, 0),\n\t\t\texpErr: true,\n\t\t},\n\t\t{\n\t\t\tscale:  19,\n\t\t\tmin:    pql.NewDecimal(math.MinInt64, 18),\n\t\t\tmax:    pql.NewDecimal(math.MaxInt64, 18),\n\t\t\texpErr: true,\n\t\t},\n\t\t{\n\t\t\tscale:  0,\n\t\t\tmin:    pql.NewDecimal(1, 20),\n\t\t\tmax:    pql.NewDecimal(2, 20),\n\t\t\texpErr: true,\n\t\t},\n\t\t{\n\t\t\tscale:  0,\n\t\t\tmin:    pql.NewDecimal(1, -1),\n\t\t\tmax:    pql.NewDecimal(2, -1),\n\t\t\texpErr: false,\n\t\t},\n\t\t{\n\t\t\tscale:  0,\n\t\t\tmin:    pql.NewDecimal(1, -19),\n\t\t\tmax:    pql.NewDecimal(2, -19),\n\t\t\texpErr: true,\n\t\t},\n\t} {\n\t\tt.Run(\"minmax\"+strconv.Itoa(i), func(t *testing.T) {\n\t\t\t_, err := newField(th, \"no-path\", \"i\", \"f\", OptFieldTypeDecimal(test.scale, test.min, test.max))\n\t\t\tif err != nil && test.expErr {\n\t\t\t\tif !strings.Contains(err.Error(), \"is not supported\") {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t} else if err != nil && !test.expErr {\n\t\t\t\tt.Fatalf(\"did not expect error, but got: %s\", err)\n\t\t\t} else if err == nil && test.expErr {\n\t\t\t\tt.Fatal(\"expected error, but got none\")\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestDecimalField_MinMaxForShard(t *testing.T) {\n\t_, _, f := newTestField(t, OptFieldTypeDecimal(3))\n\n\toptions := &ImportOptions{}\n\tfor i, test := range []struct {\n\t\tname      string\n\t\tcolumnIDs []uint64\n\t\tvalues    []float64\n\t\texpMax    ValCount\n\t\texpMin    ValCount\n\t}{\n\t\t{\n\t\t\tname:      \"zero\",\n\t\t\tcolumnIDs: []uint64{},\n\t\t\tvalues:    []float64{},\n\t\t},\n\t\t{\n\t\t\tname:      \"single\",\n\t\t\tcolumnIDs: []uint64{1},\n\t\t\tvalues:    []float64{10.1},\n\t\t\texpMax:    ValCount{Val: 10100, DecimalVal: pql.NewDecimal(10100, 3).Clone(), Count: 1},\n\t\t\texpMin:    ValCount{Val: 10100, DecimalVal: pql.NewDecimal(10100, 3).Clone(), Count: 1},\n\t\t},\n\t\t{\n\t\t\tname:      \"twovals\",\n\t\t\tcolumnIDs: []uint64{1, 2},\n\t\t\tvalues:    []float64{10.1, 20.2},\n\t\t\texpMax:    ValCount{Val: 20200, DecimalVal: pql.NewDecimal(20200, 3).Clone(), Count: 1},\n\t\t\texpMin:    ValCount{Val: 10100, DecimalVal: pql.NewDecimal(10100, 3).Clone(), Count: 1},\n\t\t},\n\t\t{\n\t\t\tname:      \"multiplecounts\",\n\t\t\tcolumnIDs: []uint64{1, 2, 3, 4, 5},\n\t\t\tvalues:    []float64{10.1, 20.2, 10.1, 10.1, 20.2},\n\t\t\texpMax:    ValCount{Val: 20200, DecimalVal: pql.NewDecimal(20200, 3).Clone(), Count: 2},\n\t\t\texpMin:    ValCount{Val: 10100, DecimalVal: pql.NewDecimal(10100, 3).Clone(), Count: 3},\n\t\t},\n\t\t{\n\t\t\tname:      \"middlevals\",\n\t\t\tcolumnIDs: []uint64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10},\n\t\t\tvalues:    []float64{10.1, 20.2, 10.1, 10.1, 20.2, 11, 12, 11, 13, 11},\n\t\t\texpMax:    ValCount{Val: 20200, DecimalVal: pql.NewDecimal(20200, 3).Clone(), Count: 2},\n\t\t\texpMin:    ValCount{Val: 10100, DecimalVal: pql.NewDecimal(10100, 3).Clone(), Count: 3},\n\t\t},\n\t} {\n\t\tt.Run(test.name+strconv.Itoa(i), func(t *testing.T) {\n\t\t\tqcx := f.idx.holder.txf.NewQcx()\n\n\t\t\terr := f.importFloatValue(qcx, test.columnIDs, test.values, 0, options)\n\t\t\tif err != nil {\n\t\t\t\tqcx.Abort()\n\t\t\t\tt.Fatalf(\"test %d, importing values: %s\", i, err.Error())\n\t\t\t}\n\t\t\tqcx.Abort()\n\n\t\t\tshard := uint64(0)\n\n\t\t\tqcx = f.idx.holder.txf.NewQcx()\n\t\t\tdefer qcx.Abort()\n\t\t\tmaxvc, err := f.MaxForShard(qcx, shard, nil)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"getting max for shard: %v\", err)\n\t\t\t}\n\t\t\tif !reflect.DeepEqual(maxvc, test.expMax) {\n\t\t\t\tt.Fatalf(\"max expected:\\n%+v\\ngot:\\n%+v\", test.expMax, maxvc)\n\t\t\t}\n\n\t\t\tminvc, err := f.MinForShard(qcx, shard, nil)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"getting min for shard: %v\", err)\n\t\t\t}\n\t\t\tif !reflect.DeepEqual(minvc, test.expMin) {\n\t\t\t\tt.Fatalf(\"min expected:\\n%+v\\ngot:\\n%+v\", test.expMin, minvc)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestBSIGroup_TxReopenDB(t *testing.T) {\n\t_, _, f := newTestField(t, OptFieldTypeInt(-100, 200))\n\n\tqcx := f.idx.holder.txf.NewQcx()\n\tdefer qcx.Abort()\n\n\toptions := &ImportOptions{}\n\tfor i, tt := range []struct {\n\t\tcolumnIDs []uint64\n\t\tvalues    []int64\n\t\tcheckVal  int64\n\t\texpCols   []uint64\n\t}{\n\t\t{\n\t\t\t[]uint64{100},\n\t\t\t[]int64{1},\n\t\t\t1,\n\t\t\t[]uint64{100},\n\t\t},\n\t\t{\n\t\t\t[]uint64{100},\n\t\t\t[]int64{8},\n\t\t\t8,\n\t\t\t[]uint64{100},\n\t\t},\n\t\t{\n\t\t\t[]uint64{100},\n\t\t\t[]int64{1},\n\t\t\t1,\n\t\t\t[]uint64{100},\n\t\t},\n\t} {\n\t\tif err := f.importValue(qcx, tt.columnIDs, tt.values, 0, options); err != nil {\n\t\t\tt.Fatalf(\"test %d, importing values: %s\", i, err.Error())\n\t\t}\n\t\tPanicOn(qcx.Finish())\n\t\tqcx.Reset()\n\n\t\tif row, err := f.Range(qcx, f.name, pql.EQ, tt.checkVal); err != nil {\n\t\t\tt.Fatalf(\"test %d, getting range: %s\", i, err.Error())\n\t\t} else if !reflect.DeepEqual(row.Columns(), tt.expCols) {\n\t\t\tt.Fatalf(\"test %d, expected columns: %v, but got: %v\", i, tt.expCols, row.Columns())\n\t\t}\n\t\tPanicOn(qcx.Finish())\n\t\tqcx.Reset()\n\t} // loop\n\n\t// the test: can we re-open a BSI fragment under Tx store\n\t_, err := reopenTestField(t, f)\n\tif err != nil {\n\t\tt.Fatalf(\"reopening test field: %v\", err)\n\t}\n}\n\n// Ensure that an integer field has the same BitDepth after reopening.\nfunc TestField_SaveMeta(t *testing.T) {\n\t_, _, f := newTestField(t, OptFieldTypeInt(-10, 1000))\n\n\tcolID := uint64(1)\n\tval := int64(88)\n\texpBitDepth := uint64(7)\n\n\t// Obtain transaction.\n\tqcx := f.holder.Txf().NewWritableQcx()\n\tdefer qcx.Abort()\n\n\tif changed, err := f.SetValue(qcx, colID, val); err != nil {\n\t\tt.Fatal(err)\n\t} else if !changed {\n\t\tt.Fatal(\"expected SetValue to return changed = true\")\n\t}\n\n\tif f.options.BitDepth != expBitDepth {\n\t\tt.Fatalf(\"expected BitDepth after set to be: %d, got: %d\", expBitDepth, f.options.BitDepth)\n\t}\n\n\tif rslt, ok, err := f.Value(qcx, colID); err != nil {\n\t\tt.Fatal(err)\n\t} else if !ok {\n\t\tt.Fatal(\"expected Value() to return exists = true\")\n\t} else if rslt != val {\n\t\tt.Fatalf(\"expected value to be: %d, got: %d\", val, rslt)\n\t}\n\n\tif err := qcx.Finish(); err != nil {\n\t\tt.Fatalf(\"error finishing qcx: %v\", err)\n\t}\n\n\t// Reload field and verify that it is persisted.\n\tf, err := reopenTestField(t, f)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tqcx = f.holder.Txf().NewQcx()\n\tdefer qcx.Abort()\n\n\tif f.options.BitDepth != expBitDepth {\n\t\tt.Fatalf(\"expected BitDepth after reopen to be: %d, got: %d\", expBitDepth, f.options.BitDepth)\n\t}\n\n\tif rslt, ok, err := f.Value(qcx, colID); err != nil {\n\t\tt.Fatal(err)\n\t} else if !ok {\n\t\tt.Fatal(\"expected Value() after reopen to return exists = true\")\n\t} else if rslt != val {\n\t\tt.Fatalf(\"expected value after reopen to be: %d, got: %d\", val, rslt)\n\t}\n}\n\nfunc TestFieldViewsByTimeRange(t *testing.T) {\n\t_, _, f := newTestField(t, OptFieldTypeTime(\"YMD\", \"0\", false))\n\tfor _, date := range []string{\n\t\t// a handful of YMD parameters describing dates that we could have data for\n\t\t\"2021\",\n\t\t\"202112\",\n\t\t\"20211229\",\n\t\t\"20211230\",\n\t\t\"20211231\",\n\t\t\"2022\",\n\t\t\"202201\",\n\t\t\"20220101\",\n\t\t\"20220102\",\n\t} {\n\t\t_, err := f.createViewIfNotExists(viewStandard + \"_\" + date)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"creating view for %s: %v\", date, err)\n\t\t}\n\t}\n\tvar testCases = []struct {\n\t\tfrom, to string\n\t\texpected []string\n\t}{\n\t\t{\"\", \"\", []string{viewStandard}}, // this is interpreted as no time range being specified at all\n\t\t{\"2020-12-31T00:00\", \"2023-01-03T00:00\", []string{\"standard_2021\", \"standard_2022\"}},\n\t\t{\"2021-01-01T00:00\", \"2022-01-01T00:00\", []string{\"standard_2021\"}},\n\t\t{\"2021-01-01T00:00\", \"2022-01-02T00:00\", []string{\"standard_2021\", \"standard_20220101\"}},\n\t\t{\"\", \"2022-01-02T00:00\", []string{\"standard_2021\", \"standard_20220101\"}},\n\t\t{\"\", \"2020-01-02T00:00\", []string{}},\n\t\t{\"2021-12-01T00:00\", \"\", []string{\"standard_202112\", \"standard_2022\"}},\n\t\t{\"2021-12-30T00:00\", \"2022-02-01T00:00\", []string{\"standard_20211230\", \"standard_20211231\", \"standard_202201\"}},\n\t}\n\tfor _, tc := range testCases {\n\t\tt.Logf(\"checking %q to %q\", tc.from, tc.to)\n\t\tvar fromTime, toTime time.Time\n\t\tvar err error\n\t\tif tc.from != \"\" {\n\t\t\tfromTime, err = time.Parse(\"2006-01-02T15:04\", tc.from)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"invalid time %q: %v\", tc.from, err)\n\t\t\t}\n\t\t}\n\t\tif tc.to != \"\" {\n\t\t\ttoTime, err = time.Parse(\"2006-01-02T15:04\", tc.to)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"invalid time %q: %v\", tc.to, err)\n\t\t\t}\n\t\t}\n\t\tviews, err := f.viewsByTimeRange(fromTime, toTime)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unexpected error getting views for %s-%s: %v\", tc.from, tc.to, err)\n\t\t}\n\t\tfor i, v := range tc.expected {\n\t\t\tif len(views) <= i {\n\t\t\t\tt.Fatalf(\"expected view %q, didn't get it\", v)\n\t\t\t} else {\n\t\t\t\tif views[i] != v {\n\t\t\t\t\tt.Fatalf(\"expected view %q, got %q\", v, views[i])\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif len(views) > len(tc.expected) {\n\t\t\tt.Fatalf(\"unexpected view %q\", views[len(tc.expected)])\n\t\t}\n\t\tt.Logf(\"views: %v\", views)\n\t}\n}\n"
        },
        {
          "name": "field_test.go",
          "type": "blob",
          "size": 8.6845703125,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa_test\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"math\"\n\t\"testing\"\n\t\"time\"\n\n\tpilosa \"github.com/featurebasedb/featurebase/v3\"\n\t\"github.com/featurebasedb/featurebase/v3/pql\"\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\t\"github.com/featurebasedb/featurebase/v3/test\"\n\t\"github.com/google/go-cmp/cmp\"\n\t\"github.com/pkg/errors\"\n)\n\n// Ensure a field can set & read a bsiGroup value.\nfunc TestField_SetValue(t *testing.T) {\n\tt.Run(\"OK\", func(t *testing.T) {\n\t\th, idx := test.MustOpenIndex(t)\n\n\t\tf, err := idx.CreateField(\"f\", \"\", pilosa.OptFieldTypeInt(math.MinInt64, math.MaxInt64))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tqcx := h.Txf().NewWritableQcx()\n\t\tdefer qcx.Abort()\n\t\t// You're going to note the lack of any commits here. That's\n\t\t// because, when you have a writable Qcx, *every individual\n\t\t// sub-transaction commits immediately*. In theory, we ought\n\t\t// to be doing provisional writes and the entire set of writes\n\t\t// ought to be able to be reverted. Actually no. We're just committing\n\t\t// everything as we go anyway.\n\n\t\t// Set value on field.\n\t\tif changed, err := f.SetValue(qcx, 100, 21); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !changed {\n\t\t\tt.Fatal(\"expected change\")\n\t\t}\n\n\t\t// Read value.\n\t\tif value, exists, err := f.Value(qcx, 100); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if value != 21 {\n\t\t\tt.Fatalf(\"unexpected value: %d\", value)\n\t\t} else if !exists {\n\t\t\tt.Fatal(\"expected value to exist\")\n\t\t}\n\n\t\t// Setting value should return no change.\n\t\tif changed, err := f.SetValue(qcx, 100, 21); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if changed {\n\t\t\tt.Fatal(\"expected no change\")\n\t\t}\n\t})\n\n\tt.Run(\"Overwrite\", func(t *testing.T) {\n\t\th, idx := test.MustOpenIndex(t)\n\n\t\tf, err := idx.CreateField(\"f\", \"\", pilosa.OptFieldTypeInt(math.MinInt64, math.MaxInt64))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tqcx := h.Txf().NewWritableQcx()\n\t\tdefer qcx.Abort()\n\n\t\t// Set value.\n\t\tif changed, err := f.SetValue(qcx, 100, 21); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !changed {\n\t\t\tt.Fatal(\"expected change\")\n\t\t}\n\n\t\t// Set different value.\n\t\tif changed, err := f.SetValue(qcx, 100, 23); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !changed {\n\t\t\tt.Fatal(\"expected change\")\n\t\t}\n\n\t\t// Read value.\n\t\tif value, exists, err := f.Value(qcx, 100); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if value != 23 {\n\t\t\tt.Fatalf(\"unexpected value: %d\", value)\n\t\t} else if !exists {\n\t\t\tt.Fatal(\"expected value to exist\")\n\t\t}\n\t})\n\n\tt.Run(\"ErrBSIGroupNotFound\", func(t *testing.T) {\n\t\th, idx := test.MustOpenIndex(t)\n\n\t\tf, err := idx.CreateField(\"f\", \"\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tqcx := h.Txf().NewWritableQcx()\n\t\tdefer qcx.Abort()\n\n\t\t// Set value.\n\t\tif _, err := f.SetValue(qcx, 100, 21); err != pilosa.ErrBSIGroupNotFound {\n\t\t\tt.Fatalf(\"unexpected error: %s\", err)\n\t\t}\n\t})\n\n\tt.Run(\"ErrBSIGroupValueTooLow\", func(t *testing.T) {\n\t\th, idx := test.MustOpenIndex(t)\n\n\t\tf, err := idx.CreateField(\"f\", \"\", pilosa.OptFieldTypeInt(20, 30))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tqcx := h.Txf().NewWritableQcx()\n\t\tdefer qcx.Abort()\n\t\t// Set value.\n\t\tif _, err := f.SetValue(qcx, 100, 15); !errors.Is(err, pilosa.ErrBSIGroupValueTooLow) {\n\t\t\tt.Fatalf(\"unexpected error: %s\", err)\n\t\t}\n\t})\n\n\tt.Run(\"ErrBSIGroupValueTooHigh\", func(t *testing.T) {\n\t\th, idx := test.MustOpenIndex(t)\n\n\t\tf, err := idx.CreateField(\"f\", \"\", pilosa.OptFieldTypeInt(20, 30))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tqcx := h.Txf().NewWritableQcx()\n\t\tdefer qcx.Abort()\n\n\t\t// Set value.\n\t\tif _, err := f.SetValue(qcx, 100, 31); !errors.Is(err, pilosa.ErrBSIGroupValueTooHigh) {\n\t\t\tt.Fatalf(\"unexpected error: %s\", err)\n\t\t}\n\t})\n}\n\n// Ensure that field name validation is consistent.\nfunc TestField_NameValidation(t *testing.T) {\n\tvalidFieldNames := []string{\n\t\t\"foo\",\n\t\t\"hyphen-ated\",\n\t\t\"under_score\",\n\t\t\"abc123\",\n\t\t\"trailing_\",\n\t\t\"charact2301234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890\",\n\t}\n\tinvalidFieldNames := []string{\n\t\t\"\",\n\t\t\"123abc\",\n\t\t\"x.y\",\n\t\t\"_foo\",\n\t\t\"-bar\",\n\t\t\"abc def\",\n\t\t\"camelCase\",\n\t\t\"UPPERCASE\",\n\t\t\".meta\",\n\t\t\"charact23112345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901\",\n\t}\n\n\t_, idx := test.MustOpenIndex(t)\n\tfor _, name := range validFieldNames {\n\t\t_, err := idx.CreateField(name, \"\")\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unexpected field name: %s %s\", name, err)\n\t\t}\n\t}\n\tfor _, name := range invalidFieldNames {\n\t\t_, err := idx.CreateField(name, \"\")\n\t\tif err == nil {\n\t\t\tt.Fatalf(\"expected error on field name: %s\", name)\n\t\t}\n\t}\n}\n\nconst includeRemote = false // for calls to Index.AvailableShards(localOnly bool)\n\n// Ensure can update and delete available shards.\nfunc TestField_AvailableShards(t *testing.T) {\n\th, idx := test.MustOpenIndex(t)\n\n\tf, err := idx.CreateField(\"fld-shards\", \"\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tqcx := h.Txf().NewWritableQcx()\n\tdefer qcx.Abort()\n\n\t// Set values on shards 0 & 2, and verify.\n\tif _, err := f.SetBit(qcx, 0, 100, nil); err != nil {\n\t\tt.Fatal(err)\n\t} else if _, err := f.SetBit(qcx, 0, ShardWidth*2, nil); err != nil {\n\t\tt.Fatal(err)\n\t} else if diff := cmp.Diff(f.AvailableShards(includeRemote).Slice(), []uint64{0, 2}); diff != \"\" {\n\t\tt.Fatal(diff)\n\t}\n\n\t// Set remote shards and verify.\n\tif err := f.AddRemoteAvailableShards(roaring.NewBitmap(1, 2, 4)); err != nil {\n\t\tt.Fatalf(\"adding remote shards: %v\", err)\n\t}\n\tif diff := cmp.Diff(f.AvailableShards(includeRemote).Slice(), []uint64{0, 1, 2, 4}); diff != \"\" {\n\t\tt.Fatal(diff)\n\t}\n\n\t// Delete shards; only local shards should remain.\n\tfor i := uint64(0); i < 5; i++ {\n\t\terr := f.RemoveAvailableShard(i)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"removing shard %d: %v\", i, err)\n\t\t}\n\t}\n\tif diff := cmp.Diff(f.AvailableShards(includeRemote).Slice(), []uint64{0, 2}); diff != \"\" {\n\t\tt.Fatal(diff)\n\t}\n}\n\nfunc TestField_ClearValue(t *testing.T) {\n\tt.Run(\"OK\", func(t *testing.T) {\n\t\th, idx := test.MustOpenIndex(t)\n\n\t\tf, err := idx.CreateField(\"f\", \"\", pilosa.OptFieldTypeInt(math.MinInt64, math.MaxInt64))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tqcx := h.Txf().NewWritableQcx()\n\t\tdefer qcx.Abort()\n\n\t\t// Set value on field.\n\t\tif changed, err := f.SetValue(qcx, 100, 21); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !changed {\n\t\t\tt.Fatal(\"expected change\")\n\t\t}\n\n\t\t// Read value.\n\t\tif value, exists, err := f.Value(qcx, 100); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if value != 21 {\n\t\t\tt.Fatalf(\"unexpected value: %d\", value)\n\t\t} else if !exists {\n\t\t\tt.Fatal(\"expected value to exist\")\n\t\t}\n\n\t\tif changed, err := f.ClearValue(qcx, 100); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !changed {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Read value.\n\t\tif _, exists, err := f.Value(qcx, 100); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if exists {\n\t\t\tt.Fatal(\"expected value to not exist\")\n\t\t}\n\t})\n}\n\nfunc TestFieldInfoMarshal(t *testing.T) {\n\tf := &pilosa.FieldInfo{Name: \"timestamp\", CreatedAt: 1649270079233541000,\n\t\tOptions: pilosa.FieldOptions{\n\t\t\tBase:           0,\n\t\t\tBitDepth:       0x0,\n\t\t\tMin:            pql.NewDecimal(-4294967296, 0),\n\t\t\tMax:            pql.NewDecimal(4294967296, 0),\n\t\t\tScale:          0,\n\t\t\tKeys:           false,\n\t\t\tNoStandardView: false,\n\t\t\tCacheType:      \"\",\n\t\t\tType:           \"timestamp\",\n\t\t\tTimeUnit:       \"s\",\n\t\t\tTimeQuantum:    \"\",\n\t\t\tForeignIndex:   \"\",\n\t\t\tTTL:            0,\n\t\t},\n\t\tCardinality: (*uint64)(nil),\n\t}\n\ta, err := json.Marshal(f)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected error marshalling index info,  %v\", err)\n\t}\n\texpected := []byte(`{\"name\":\"timestamp\",\"createdAt\":1649270079233541000,\"owner\":\"\",\"options\":{\"type\":\"timestamp\",\"epoch\":\"1970-01-01T00:00:00Z\",\"bitDepth\":0,\"min\":-4294967296,\"max\":4294967296,\"timeUnit\":\"s\"}}`)\n\tif !bytes.Equal(a, expected) {\n\t\tt.Fatalf(\"expected %s, got %s\", expected, a)\n\t}\n}\n\nfunc TestCheckUnixNanoOverflow(t *testing.T) {\n\tminNano = pilosa.MinTimestampNano.UnixNano()\n\tmaxNano = pilosa.MaxTimestampNano.UnixNano()\n\ttests := []struct {\n\t\tname    string\n\t\tepoch   time.Time\n\t\twantErr bool\n\t}{\n\t\t{\n\t\t\tname:    \"too small\",\n\t\t\tepoch:   time.Unix(-1, minNano),\n\t\t\twantErr: true,\n\t\t},\n\t\t{\n\t\t\tname:    \"just right-1\",\n\t\t\tepoch:   time.Unix(0, minNano),\n\t\t\twantErr: false,\n\t\t},\n\t\t{\n\t\t\tname:    \"just right-2\",\n\t\t\tepoch:   time.Unix(0, maxNano),\n\t\t\twantErr: false,\n\t\t},\n\t\t{\n\t\t\tname:    \"too large\",\n\t\t\tepoch:   time.Unix(1, maxNano),\n\t\t\twantErr: true,\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tif err := pilosa.CheckEpochOutOfRange(tt.epoch, pilosa.MinTimestampNano, pilosa.MaxTimestampNano); (err != nil) != tt.wantErr {\n\t\t\t\tt.Errorf(\"checkUnixNanoOverflow() error = %v, wantErr %v\", err, tt.wantErr)\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "filesystem.go",
          "type": "blob",
          "size": 0.7060546875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"fmt\"\n\t\"net/http\"\n)\n\n// Ensure nopFileSystem implements interface.\nvar _ FileSystem = &nopFileSystem{}\n\n// FileSystem represents an interface for file system for serving the Lattice UI.\ntype FileSystem interface {\n\tNew() (http.FileSystem, error)\n}\n\nfunc init() {\n\tNopFileSystem = &nopFileSystem{}\n}\n\n// NopFileSystem represents a FileSystem that returns an error if called.\nvar NopFileSystem FileSystem\n\ntype nopFileSystem struct{}\n\n// New is a no-op implementation of FileSystem New method.\nfunc (n *nopFileSystem) New() (http.FileSystem, error) {\n\treturn nil, fmt.Errorf(\"file system not implemented\")\n}\n"
        },
        {
          "name": "fragment.go",
          "type": "blob",
          "size": 83.568359375,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"archive/tar\"\n\t\"bytes\"\n\t\"container/heap\"\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"math/bits\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/logger\"\n\t\"github.com/featurebasedb/featurebase/v3/pb\"\n\t\"github.com/featurebasedb/featurebase/v3/pql\"\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\t\"github.com/featurebasedb/featurebase/v3/shardwidth\"\n\t\"github.com/featurebasedb/featurebase/v3/testhook\"\n\t\"github.com/featurebasedb/featurebase/v3/tracing\"\n\t\"github.com/featurebasedb/featurebase/v3/vprint\"\n\t\"github.com/gogo/protobuf/proto\"\n\t\"github.com/pkg/errors\"\n)\n\nconst (\n\t// ShardWidth is the number of column IDs in a shard. It must be a power of 2 greater than or equal to 16.\n\t// shardWidthExponent = 20 // set in shardwidthNN.go files\n\tShardWidth = 1 << shardwidth.Exponent\n\n\t// shardVsContainerExponent is the power of 2 of ShardWith minus the power\n\t// of two of roaring container width (which is 16).\n\t// 2^shardVsContainerExponent is the number of containers in a shard row.\n\t//\n\t// It is represented in this rather awkward way because calculating the row\n\t// which a given container is in means dividing by the number of rows per\n\t// container which is performantly expressed as a right shift by this\n\t// exponent.\n\tshardVsContainerExponent = shardwidth.Exponent - 16\n\n\t// width of roaring containers is 2^16\n\tcontainerWidth = 1 << 16\n\n\t// cacheExt is the file extension for persisted cache ids.\n\tcacheExt = \".cache\"\n\n\t// HashBlockSize is the number of rows in a merkle hash block.\n\tHashBlockSize = 100\n\n\t// Row ids used for boolean fields.\n\tfalseRowID = uint64(0)\n\ttrueRowID  = uint64(1)\n\n\t// BSI bits used to check existence & sign.\n\tbsiExistsBit = 0\n\tbsiSignBit   = 1\n\tbsiOffsetBit = 2\n)\n\nfunc (f *fragment) index() string {\n\treturn f.idx.name\n}\n\nfunc (f *fragment) field() string {\n\treturn f.fld.name\n}\n\nfunc (f *fragment) view() string {\n\treturn f._view.name\n}\nfunc (f *fragment) path() string {\n\treturn filepath.Join(f._view.path, \"fragments\", strconv.FormatUint(f.shard, 10))\n}\n\n// fragment represents the intersection of a field and shard in an index.\ntype fragment struct {\n\tmu sync.RWMutex\n\n\t// We save 20GB worth strings on some data sets by not duplicating\n\t// the path, index, field, view strings on every fragment.\n\t// Instead assemble strings on demand in field(), view(), path(), index().\n\tfld   *Field\n\t_view *view\n\n\tshard uint64\n\n\t// idx cached to avoid repeatedly looking it up everywhere.\n\tidx *Index\n\n\t// parent holder\n\tholder *Holder\n\n\t// Cache for row counts.\n\tCacheType string // passed in by field\n\n\t// cache keeps a local rowid,count ranking:\n\t// telling us which is the most populated rows in that field.\n\t// Is only on \"set fields\" with rowCache enabled. So\n\t// BSI, mutex, bool fields do not have this.\n\t// Good: it Only has a string and a count, so cannot use Tx memory.\n\tcache cache\n\n\tCacheSize uint32\n\n\t// Cached checksums for each block.\n\tchecksums map[int][]byte\n\n\t// Logger used for out-of-band log entries.\n\tLogger logger.Logger\n\n\t// mutexVector is used for mutex field types. It's checked for an\n\t// existing value (to clear) prior to setting a new value.\n\tmutexVector vector\n}\n\n// newFragment returns a new instance of fragment.\nfunc newFragment(holder *Holder, idx *Index, fld *Field, vw *view, shard uint64) *fragment {\n\tcheckIdx := holder.Index(idx.name)\n\n\tif checkIdx == nil {\n\t\tvprint.PanicOn(fmt.Sprintf(\"got nil idx back for '%v' from holder!\", idx.Name()))\n\t}\n\n\tf := &fragment{\n\t\t_view: vw,\n\t\tfld:   fld,\n\t\tshard: shard,\n\t\tidx:   idx,\n\n\t\tCacheType: DefaultCacheType,\n\t\tCacheSize: DefaultCacheSize,\n\n\t\tholder: holder,\n\t}\n\treturn f\n}\n\n// cachePath returns the path to the fragment's cache data.\nfunc (f *fragment) cachePath() string { return f.path() + cacheExt }\n\nfunc (f *fragment) bitDepth() (uint64, error) {\n\tf.mu.RLock()\n\tdefer f.mu.RUnlock()\n\ttx, err := f.holder.BeginTx(false, f.idx, f.shard)\n\tif err != nil {\n\t\treturn 0, errors.Wrapf(err, \"beginning new tx(false, %s, %d)\", f.index(), f.shard)\n\t}\n\tdefer tx.Rollback()\n\n\tmaxRowID, _, err := f.maxRow(tx, nil)\n\tif err != nil {\n\t\treturn 0, errors.Wrapf(err, \"getting fragment max row id\")\n\t}\n\n\tif maxRowID+1 > bsiOffsetBit {\n\t\treturn maxRowID + 1 - bsiOffsetBit, nil\n\t}\n\treturn 0, nil\n}\n\ntype FragmentInfo struct {\n\tBitmapInfo roaring.BitmapInfo\n}\n\nfunc (f *fragment) Index() *Index {\n\treturn f.holder.Index(f.index())\n}\n\n// Open opens the underlying storage.\nfunc (f *fragment) Open() error {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\n\tif err := func() error {\n\t\t// Fill cache with rows persisted to disk.\n\t\tif err := f.openCache(); err != nil {\n\t\t\treturn errors.Wrap(err, \"opening cache\")\n\t\t}\n\n\t\t// Clear checksums.\n\t\tf.checksums = make(map[int][]byte)\n\t\treturn nil\n\t}(); err != nil {\n\t\tf.close()\n\t\treturn err\n\t}\n\n\t_ = testhook.Opened(f.holder.Auditor, f, nil)\n\treturn nil\n}\n\n// openCache initializes the cache from row ids persisted to disk.\nfunc (f *fragment) openCache() error {\n\t// Determine cache type from field name.\n\tswitch f.CacheType {\n\tcase CacheTypeRanked:\n\t\tf.cache = NewRankCache(f.CacheSize)\n\tcase CacheTypeLRU:\n\t\tf.cache = newLRUCache(f.CacheSize)\n\tcase CacheTypeNone:\n\t\tf.cache = globalNopCache\n\t\treturn nil\n\tdefault:\n\t\treturn ErrInvalidCacheType\n\t}\n\n\t// Read cache data from disk.\n\tpath := f.cachePath()\n\tbuf, err := os.ReadFile(path)\n\tif os.IsNotExist(err) {\n\t\treturn nil\n\t} else if err != nil {\n\t\treturn fmt.Errorf(\"open cache: %s\", err)\n\t}\n\n\t// Unmarshal cache data.\n\tvar pb pb.Cache\n\tif err := proto.Unmarshal(buf, &pb); err != nil {\n\t\tf.holder.Logger.Errorf(\"error unmarshaling cache data, skipping: path=%s, err=%s\", path, err)\n\t\treturn nil\n\t}\n\n\ttx := f.idx.holder.txf.NewTx(Txo{Write: !writable, Index: f.idx, Fragment: f, Shard: f.shard})\n\tdefer tx.Rollback()\n\n\t// Read in all rows by ID.\n\t// This will cause them to be added to the cache.\n\tfor _, id := range pb.IDs {\n\t\tn, err := tx.CountRange(f.index(), f.field(), f.view(), f.shard, id*ShardWidth, (id+1)*ShardWidth)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"CountRange\")\n\t\t}\n\t\tf.cache.BulkAdd(id, n)\n\t}\n\tf.cache.Invalidate()\n\n\treturn nil\n}\n\n// Close flushes the underlying storage, closes the file and unlocks it.\nfunc (f *fragment) Close() error {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\tdefer func() {\n\t\t_ = testhook.Closed(f.holder.Auditor, f, nil)\n\t}()\n\treturn f.close()\n}\n\nfunc (f *fragment) close() error {\n\t// Flush cache if closing gracefully.\n\tif err := f.flushCache(); err != nil {\n\t\tf.holder.Logger.Errorf(\"fragment: error flushing cache on close: err=%s, path=%s\", err, f.path())\n\t\treturn errors.Wrap(err, \"flushing cache\")\n\t}\n\n\t// Remove checksums.\n\tf.checksums = nil\n\n\treturn nil\n}\n\n// mutexCheck checks for any entries in fragment which violate the mutex\n// property of having only one value set for a given column ID.\nfunc (f *fragment) mutexCheck(tx Tx, details bool, limit int) (map[uint64][]uint64, error) {\n\tdup := roaring.NewBitmapMutexDupFilter(f.shard<<shardwidth.Exponent, details, limit)\n\terr := tx.ApplyFilter(f.index(), f.field(), f.view(), f.shard, 0, dup)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn dup.Report(), nil\n}\n\n// row returns a row by ID.\nfunc (f *fragment) row(tx Tx, rowID uint64) (*Row, error) {\n\tf.mu.RLock()\n\tdefer f.mu.RUnlock()\n\treturn f.unprotectedRow(tx, rowID)\n}\n\n// mustRow returns a row by ID. Panic on error. Only used for testing.\nfunc (f *fragment) mustRow(tx Tx, rowID uint64) *Row {\n\trow, err := f.row(tx, rowID)\n\tif err != nil {\n\t\tvprint.PanicOn(err)\n\t}\n\treturn row\n}\n\n// unprotectedRow returns a row from the row cache if available or from storage\n// (updating the cache).\nfunc (f *fragment) unprotectedRow(tx Tx, rowID uint64) (*Row, error) {\n\trow, err := f.rowFromStorage(tx, rowID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn row, nil\n}\n\n// rowFromStorage clones a row data out of fragment storage and returns it as a\n// Row object.\nfunc (f *fragment) rowFromStorage(tx Tx, rowID uint64) (*Row, error) {\n\t// Only use a subset of the containers.\n\t// NOTE: The start & end ranges must be divisible by container width.\n\t//\n\t// Note that OffsetRange now returns a new bitmap which uses frozen\n\t// containers which will use copy-on-write semantics. The actual bitmap\n\t// and Containers object are new and not shared, but the containers are\n\t// shared.\n\tdata, err := tx.OffsetRange(f.index(), f.field(), f.view(), f.shard, f.shard*ShardWidth, rowID*ShardWidth, (rowID+1)*ShardWidth)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trow := &Row{\n\t\tSegments: []RowSegment{{\n\t\t\tdata:     data,\n\t\t\tshard:    f.shard,\n\t\t\twritable: true,\n\t\t}},\n\t}\n\trow.invalidateCount()\n\n\treturn row, nil\n}\n\n// setBit sets a bit for a given column & row within the fragment.\n// This updates both the on-disk storage and the in-cache bitmap.\nfunc (f *fragment) setBit(tx Tx, rowID, columnID uint64) (changed bool, err error) {\n\tf.mu.Lock() // controls access to the file.\n\tdefer f.mu.Unlock()\n\n\tdoSetFunc := func() error {\n\t\t// handle mutux field type\n\t\tif f.mutexVector != nil {\n\t\t\tif err := f.handleMutex(tx, rowID, columnID); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"handling mutex\")\n\t\t\t}\n\t\t}\n\t\tchanged, err = f.unprotectedSetBit(tx, rowID, columnID)\n\t\treturn err\n\t}\n\terr = doSetFunc()\n\treturn changed, err\n}\n\n// handleMutex will clear an existing row and store the new row\n// in the vector.\nfunc (f *fragment) handleMutex(tx Tx, rowID, columnID uint64) error {\n\tif existingRowID, found, err := f.mutexVector.Get(tx, columnID); err != nil {\n\t\treturn errors.Wrap(err, \"getting mutex vector data\")\n\t} else if found && existingRowID != rowID {\n\t\tif _, err := f.unprotectedClearBit(tx, existingRowID, columnID); err != nil {\n\t\t\treturn errors.Wrap(err, \"clearing mutex value\")\n\t\t}\n\t}\n\treturn nil\n}\n\n// unprotectedSetBit TODO should be replaced by an invocation of importPositions with a single bit to set.\nfunc (f *fragment) unprotectedSetBit(tx Tx, rowID, columnID uint64) (changed bool, err error) {\n\n\t// Determine the position of the bit in the storage.\n\tpos, err := f.pos(rowID, columnID)\n\tif err != nil {\n\t\treturn false, errors.Wrap(err, \"getting bit pos\")\n\t}\n\n\t// Write to storage.\n\tchangeCount := 0\n\tchangeCount, err = tx.Add(f.index(), f.field(), f.view(), f.shard, pos)\n\tchanged = changeCount > 0\n\tif err != nil {\n\t\treturn false, errors.Wrap(err, \"writing\")\n\t}\n\n\t// Don't update the cache if nothing changed.\n\tif !changed {\n\t\treturn changed, nil\n\t}\n\n\t// Invalidate block checksum.\n\tdelete(f.checksums, int(rowID/HashBlockSize))\n\n\t// If we're using a cache, update it. Otherwise skip the\n\t// possibly-expensive count operation.\n\tif f.CacheType != CacheTypeNone {\n\t\tn, err := tx.CountRange(f.index(), f.field(), f.view(), f.shard, rowID*ShardWidth, (rowID+1)*ShardWidth)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\t\tf.cache.Add(rowID, n)\n\t}\n\n\tCounterSetBit.Inc()\n\n\treturn changed, nil\n}\n\n// clearBit clears a bit for a given column & row within the fragment.\n// This updates both the on-disk storage and the in-cache bitmap.\nfunc (f *fragment) clearBit(tx Tx, rowID, columnID uint64) (changed bool, err error) {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\treturn f.unprotectedClearBit(tx, rowID, columnID)\n}\n\n// unprotectedClearBit TODO should be replaced by an invocation of\n// importPositions with a single bit to clear.\nfunc (f *fragment) unprotectedClearBit(tx Tx, rowID, columnID uint64) (changed bool, err error) {\n\t// Determine the position of the bit in the storage.\n\tpos, err := f.pos(rowID, columnID)\n\tif err != nil {\n\t\treturn false, errors.Wrap(err, \"getting bit pos\")\n\t}\n\n\t// Write to storage.\n\tchangeCount := 0\n\tif changeCount, err = tx.Remove(f.index(), f.field(), f.view(), f.shard, pos); err != nil {\n\t\treturn false, errors.Wrap(err, \"writing\")\n\t}\n\n\t// Don't update the cache if nothing changed.\n\tif changeCount <= 0 {\n\t\treturn false, nil\n\t} else {\n\t\tchanged = true\n\t}\n\n\t// Invalidate block checksum.\n\tdelete(f.checksums, int(rowID/HashBlockSize))\n\n\t// If we're using a cache, update it. Otherwise skip the\n\t// possibly-expensive count operation.\n\tif f.CacheType != CacheTypeNone {\n\t\tn, err := tx.CountRange(f.index(), f.field(), f.view(), f.shard, rowID*ShardWidth, (rowID+1)*ShardWidth)\n\t\tif err != nil {\n\t\t\treturn changed, err\n\t\t}\n\t\tf.cache.Add(rowID, n)\n\t}\n\n\tCounterClearBit.Inc()\n\n\treturn changed, nil\n}\n\n// setRow replaces an existing row (specified by rowID) with the given\n// Row. This updates both the on-disk storage and the in-cache bitmap.\nfunc (f *fragment) setRow(tx Tx, row *Row, rowID uint64) (changed bool, err error) {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\treturn f.unprotectedSetRow(tx, row, rowID)\n}\n\nfunc (f *fragment) unprotectedSetRow(tx Tx, row *Row, rowID uint64) (changed bool, err error) {\n\t// TODO: In order to return `changed`, we need to first compare\n\t// the existing row with the given row. Determine if the overhead\n\t// of this is worth having `changed`.\n\t// For now we will assume changed is always true.\n\tchanged = true\n\n\t// First container of the row in storage.\n\theadContainerKey := rowID << shardVsContainerExponent\n\n\t// Remove every existing container in the row.\n\tfor i := uint64(0); i < (1 << shardVsContainerExponent); i++ {\n\t\tif err := tx.RemoveContainer(f.index(), f.field(), f.view(), f.shard, headContainerKey+i); err != nil {\n\t\t\treturn changed, err\n\t\t}\n\t}\n\n\t// From the given row, get the rowSegment for this shard.\n\tseg := row.segment(f.shard)\n\tif seg != nil {\n\t\t// Put each container from rowSegment to fragment storage.\n\t\tciter, _ := seg.data.Containers.Iterator(f.shard << shardVsContainerExponent)\n\t\tfor citer.Next() {\n\t\t\tk, c := citer.Value()\n\t\t\tif err := tx.PutContainer(f.index(), f.field(), f.view(), f.shard, headContainerKey+(k%(1<<shardVsContainerExponent)), c); err != nil {\n\t\t\t\treturn changed, err\n\t\t\t}\n\t\t}\n\n\t\t// Update the row in cache.\n\t\tif f.CacheType != CacheTypeNone {\n\t\t\tn, err := tx.CountRange(f.index(), f.field(), f.view(), f.shard, rowID*ShardWidth, (rowID+1)*ShardWidth)\n\t\t\tif err != nil {\n\t\t\t\treturn changed, err\n\t\t\t}\n\t\t\tf.cache.BulkAdd(rowID, n)\n\t\t}\n\t} else {\n\t\tif f.CacheType != CacheTypeNone {\n\t\t\tf.cache.BulkAdd(rowID, 0)\n\t\t}\n\t}\n\n\tCounterSetRow.Inc()\n\n\treturn changed, nil\n}\n\n// clearRow clears a row for a given rowID within the fragment.\n// This updates both the on-disk storage and the in-cache bitmap.\nfunc (f *fragment) clearRow(tx Tx, rowID uint64) (changed bool, err error) {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\treturn f.unprotectedClearRow(tx, rowID)\n}\n\nfunc (f *fragment) unprotectedClearRow(tx Tx, rowID uint64) (changed bool, err error) {\n\tchanged = false\n\n\t// First container of the row in storage.\n\theadContainerKey := rowID << shardVsContainerExponent\n\n\t// Remove every container in the row.\n\tfor i := uint64(0); i < (1 << shardVsContainerExponent); i++ {\n\t\tk := headContainerKey + i\n\t\t// Technically we could bypass the Get() call and only\n\t\t// call Remove(), but the Get() gives us the ability\n\t\t// to return true if any existing data was removed.\n\t\tif cont, err := tx.Container(f.index(), f.field(), f.view(), f.shard, k); err != nil {\n\t\t\treturn changed, err\n\t\t} else if cont != nil {\n\t\t\tif err := tx.RemoveContainer(f.index(), f.field(), f.view(), f.shard, k); err != nil {\n\t\t\t\treturn changed, err\n\t\t\t}\n\t\t\tchanged = true\n\t\t}\n\t}\n\n\t// Clear the row in cache.\n\tf.cache.Add(rowID, 0)\n\n\treturn changed, nil\n}\n\n// clearBlock clears all rows for a given block.\n// This updates both the on-disk storage and the in-cache bitmap.\nfunc (f *fragment) clearBlock(tx Tx, block int) (changed bool, err error) {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\n\tfirstRow := uint64(block * HashBlockSize)\n\terr = func() error {\n\t\tvar rowChanged bool\n\t\tfor rowID := uint64(firstRow); rowID < firstRow+HashBlockSize; rowID++ {\n\t\t\tif chang, err := f.unprotectedClearRow(tx, rowID); err != nil {\n\t\t\t\treturn errors.Wrapf(err, \"clearing row: %d\", rowID)\n\t\t\t} else if chang {\n\t\t\t\trowChanged = true\n\t\t\t}\n\t\t}\n\t\tchanged = rowChanged\n\t\treturn nil\n\t}()\n\treturn changed, err\n}\n\nfunc (f *fragment) bit(tx Tx, rowID, columnID uint64) (bool, error) {\n\tpos, err := f.pos(rowID, columnID)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\treturn tx.Contains(f.index(), f.field(), f.view(), f.shard, pos)\n}\n\n// value uses a column of bits to read a multi-bit value.\nfunc (f *fragment) value(tx Tx, columnID uint64, bitDepth uint64) (value int64, exists bool, err error) {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\n\t// If existence bit is unset then ignore remaining bits.\n\tif v, err := f.bit(tx, bsiExistsBit, columnID); err != nil {\n\t\treturn 0, false, errors.Wrap(err, \"getting existence bit\")\n\t} else if !v {\n\t\treturn 0, false, nil\n\t}\n\n\t// Compute other bits into a value.\n\tfor i := uint64(0); i < bitDepth; i++ {\n\t\tif v, err := f.bit(tx, uint64(bsiOffsetBit+i), columnID); err != nil {\n\t\t\treturn 0, false, errors.Wrapf(err, \"getting value bit %d\", i)\n\t\t} else if v {\n\t\t\tvalue |= (1 << i)\n\t\t}\n\t}\n\n\t// Negate if sign bit set.\n\tif v, err := f.bit(tx, bsiSignBit, columnID); err != nil {\n\t\treturn 0, false, errors.Wrap(err, \"getting sign bit\")\n\t} else if v {\n\t\tvalue = -value\n\t}\n\n\treturn value, true, nil\n}\n\n// clearValue uses a column of bits to clear a multi-bit value.\nfunc (f *fragment) clearValue(tx Tx, columnID uint64, bitDepth uint64, value int64) (changed bool, err error) {\n\treturn f.setValueBase(tx, columnID, bitDepth, value, true)\n}\n\n// setValue uses a column of bits to set a multi-bit value.\nfunc (f *fragment) setValue(tx Tx, columnID uint64, bitDepth uint64, value int64) (changed bool, err error) {\n\treturn f.setValueBase(tx, columnID, bitDepth, value, false)\n}\n\nfunc (f *fragment) positionsForValue(columnID uint64, bitDepth uint64, value int64, clear bool, toSet, toClear []uint64) ([]uint64, []uint64, error) {\n\t// Convert value to an unsigned representation.\n\tuvalue := uint64(value)\n\tif value < 0 {\n\t\tuvalue = uint64(-value)\n\t}\n\n\t// Mark value as set.\n\tif bit, err := f.pos(bsiExistsBit, columnID); err != nil {\n\t\treturn toSet, toClear, errors.Wrap(err, \"getting not-null pos\")\n\t} else if clear {\n\t\ttoClear = append(toClear, bit)\n\t} else {\n\t\ttoSet = append(toSet, bit)\n\t}\n\n\t// Mark sign.\n\tif bit, err := f.pos(bsiSignBit, columnID); err != nil {\n\t\treturn toSet, toClear, errors.Wrap(err, \"getting sign pos\")\n\t} else if value >= 0 || clear {\n\t\ttoClear = append(toClear, bit)\n\t} else {\n\t\ttoSet = append(toSet, bit)\n\t}\n\n\tfor i := uint64(0); i < bitDepth; i++ {\n\t\tbit, err := f.pos(uint64(bsiOffsetBit+i), columnID)\n\t\tif err != nil {\n\t\t\treturn toSet, toClear, errors.Wrap(err, \"getting pos\")\n\t\t}\n\t\tif uvalue&(1<<i) != 0 {\n\t\t\ttoSet = append(toSet, bit)\n\t\t} else {\n\t\t\ttoClear = append(toClear, bit)\n\t\t}\n\t}\n\n\treturn toSet, toClear, nil\n}\n\n// TODO get rid of this and use positionsForValue to generate a single write op, and set that with importPositions.\nfunc (f *fragment) setValueBase(tx Tx, columnID uint64, bitDepth uint64, value int64, clear bool) (changed bool, err error) {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\n\terr = func() error {\n\t\t// Convert value to an unsigned representation.\n\t\tuvalue := uint64(value)\n\t\tif value < 0 {\n\t\t\tuvalue = uint64(-value)\n\t\t}\n\n\t\tfor i := uint64(0); i < bitDepth; i++ {\n\t\t\tif uvalue&(1<<i) != 0 {\n\t\t\t\tif c, err := f.unprotectedSetBit(tx, uint64(bsiOffsetBit+i), columnID); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t} else if c {\n\t\t\t\t\tchanged = true\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif c, err := f.unprotectedClearBit(tx, uint64(bsiOffsetBit+i), columnID); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t} else if c {\n\t\t\t\t\tchanged = true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Mark value as set (or cleared).\n\t\tif clear {\n\t\t\tif c, err := f.unprotectedClearBit(tx, uint64(bsiExistsBit), columnID); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"clearing not-null\")\n\t\t\t} else if c {\n\t\t\t\tchanged = true\n\t\t\t}\n\t\t} else {\n\t\t\tif c, err := f.unprotectedSetBit(tx, uint64(bsiExistsBit), columnID); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"marking not-null\")\n\t\t\t} else if c {\n\t\t\t\tchanged = true\n\t\t\t}\n\t\t}\n\n\t\t// Mark sign bit (or clear).\n\t\tif value >= 0 || clear {\n\t\t\tif c, err := f.unprotectedClearBit(tx, uint64(bsiSignBit), columnID); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"clearing sign\")\n\t\t\t} else if c {\n\t\t\t\tchanged = true\n\t\t\t}\n\t\t} else {\n\t\t\tif c, err := f.unprotectedSetBit(tx, uint64(bsiSignBit), columnID); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"marking sign\")\n\t\t\t} else if c {\n\t\t\t\tchanged = true\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t}()\n\treturn changed, err\n}\n\n// sum returns the sum of a given bsiGroup as well as the number of columns involved.\n// A bitmap can be passed in to optionally filter the computed columns.\nfunc (f *fragment) sum(tx Tx, filter *Row, bitDepth uint64) (sum int64, count uint64, err error) {\n\t// If there's a provided filter, but it has no contents for this particular\n\t// shard, we're done and can return early. If there's no provided filter,\n\t// though, we want to run with no-filter, as opposed to an empty filter.\n\tvar filterData *roaring.Bitmap\n\tif filter != nil {\n\t\tfor _, seg := range filter.Segments {\n\t\t\tif seg.shard == f.shard {\n\t\t\t\tfilterData = seg.data\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\t// if filter is empty, we're done\n\t\tif filterData == nil {\n\t\t\treturn 0, 0, nil\n\t\t}\n\t}\n\tbsiFilt := roaring.NewBitmapBSICountFilter(filterData)\n\terr = tx.ApplyFilter(f.index(), f.field(), f.view(), f.shard, 0, bsiFilt)\n\tif err != nil && err != io.EOF {\n\t\treturn sum, count, errors.Wrap(err, \"finding existing positions\")\n\t}\n\n\tc32, sum := bsiFilt.Total()\n\n\treturn sum, uint64(c32), nil\n}\n\n// min returns the min of a given bsiGroup as well as the number of columns involved.\n// A bitmap can be passed in to optionally filter the computed columns.\nfunc (f *fragment) min(tx Tx, filter *Row, bitDepth uint64) (min int64, count uint64, err error) {\n\tconsider, err := f.row(tx, bsiExistsBit)\n\tif err != nil {\n\t\treturn min, count, err\n\t} else if filter != nil {\n\t\tconsider = consider.Intersect(filter)\n\t}\n\n\t// If there are no columns to consider, return early.\n\tif consider.Count() == 0 {\n\t\treturn 0, 0, nil\n\t}\n\n\t// If we have negative values, we should find the highest unsigned value\n\t// from that set, then negate it, and return it. For example, if values\n\t// (-1, -2) exist, they are stored unsigned (1,2) with a negative sign bit\n\t// set. We take the highest of that set (2) and negate it and return it.\n\tif row, err := f.row(tx, bsiSignBit); err != nil {\n\t\treturn min, count, err\n\t} else if row = row.Intersect(consider); row.Any() {\n\t\tmin, count, err := f.maxUnsigned(tx, row, bitDepth)\n\t\treturn -min, count, err\n\t}\n\n\t// Otherwise find lowest positive number.\n\treturn f.minUnsigned(tx, consider, bitDepth)\n}\n\n// minUnsigned the lowest value without considering the sign bit. Filter is required.\nfunc (f *fragment) minUnsigned(tx Tx, filter *Row, bitDepth uint64) (min int64, count uint64, err error) {\n\tcount = filter.Count()\n\tfor i := int(bitDepth - 1); i >= 0; i-- {\n\t\trow, err := f.row(tx, uint64(bsiOffsetBit+i))\n\t\tif err != nil {\n\t\t\treturn min, count, err\n\t\t}\n\t\trow = filter.Difference(row)\n\t\tcount = row.Count()\n\t\tif count > 0 {\n\t\t\tfilter = row\n\t\t} else {\n\t\t\tmin += (1 << uint(i))\n\t\t\tif i == 0 {\n\t\t\t\tcount = filter.Count()\n\t\t\t}\n\t\t}\n\t}\n\treturn min, count, nil\n}\n\n// max returns the max of a given bsiGroup as well as the number of columns involved.\n// A bitmap can be passed in to optionally filter the computed columns.\nfunc (f *fragment) max(tx Tx, filter *Row, bitDepth uint64) (max int64, count uint64, err error) {\n\tconsider, err := f.row(tx, bsiExistsBit)\n\tif err != nil {\n\t\treturn max, count, err\n\t} else if filter != nil {\n\t\tconsider = consider.Intersect(filter)\n\t}\n\n\t// If there are no columns to consider, return early.\n\tif !consider.Any() {\n\t\treturn 0, 0, nil\n\t}\n\n\t// Find lowest negative number w/o sign and negate, if no positives are available.\n\trow, err := f.row(tx, bsiSignBit)\n\tif err != nil {\n\t\treturn max, count, err\n\t}\n\tpos := consider.Difference(row)\n\tif !pos.Any() {\n\t\tmax, count, err = f.minUnsigned(tx, consider, bitDepth)\n\t\treturn -max, count, err\n\t}\n\n\t// Otherwise find highest positive number.\n\treturn f.maxUnsigned(tx, pos, bitDepth)\n}\n\n// maxUnsigned the highest value without considering the sign bit. Filter is required.\nfunc (f *fragment) maxUnsigned(tx Tx, filter *Row, bitDepth uint64) (max int64, count uint64, err error) {\n\tcount = filter.Count()\n\tfor i := int(bitDepth - 1); i >= 0; i-- {\n\t\trow, err := f.row(tx, uint64(bsiOffsetBit+i))\n\t\tif err != nil {\n\t\t\treturn max, count, err\n\t\t}\n\t\trow = row.Intersect(filter)\n\n\t\tcount = row.Count()\n\t\tif count > 0 {\n\t\t\tmax += (1 << uint(i))\n\t\t\tfilter = row\n\t\t} else if i == 0 {\n\t\t\tcount = filter.Count()\n\t\t}\n\t}\n\treturn max, count, nil\n}\n\n// minRow returns minRowID of the rows in the filter and its count.\n// if filter is nil, it returns fragment.minRowID, 1\n// if fragment has no rows, it returns 0, 0\nfunc (f *fragment) minRow(tx Tx, filter *Row) (uint64, uint64, error) {\n\tminRowID, hasRowID, err := f.minRowID(tx)\n\tif err != nil {\n\t\treturn 0, 0, err\n\t}\n\tif hasRowID {\n\t\tif filter == nil {\n\t\t\treturn minRowID, 1, nil\n\t\t}\n\n\t\t// Read last bit to determine max row.\n\t\tmaxRowID, err := f.maxRowID(tx)\n\t\tif err != nil {\n\t\t\treturn 0, 0, err\n\t\t}\n\n\t\t// iterate from min row ID and return the first that intersects with filter.\n\t\tfor i := minRowID; i <= maxRowID; i++ {\n\t\t\trow, err := f.row(tx, i)\n\t\t\tif err != nil {\n\t\t\t\treturn 0, 0, err\n\t\t\t}\n\t\t\trow = row.Intersect(filter)\n\n\t\t\tcount := row.Count()\n\t\t\tif count > 0 {\n\t\t\t\treturn i, count, nil\n\t\t\t}\n\t\t}\n\t}\n\treturn 0, 0, nil\n}\n\n// maxRow returns maxRowID of the rows in the filter and its count.\n// if filter is nil, it returns fragment.maxRowID, 1\n// if fragment has no rows, it returns 0, 0\nfunc (f *fragment) maxRow(tx Tx, filter *Row) (uint64, uint64, error) {\n\tminRowID, hasRowID, err := f.minRowID(tx)\n\tif err != nil {\n\t\treturn 0, 0, err\n\t}\n\tif hasRowID {\n\t\tmaxRowID, err := f.maxRowID(tx)\n\t\tif err != nil {\n\t\t\treturn 0, 0, err\n\t\t}\n\n\t\tif filter == nil {\n\t\t\treturn maxRowID, 1, nil\n\t\t}\n\t\t// iterate back from max row ID and return the first that intersects with filter.\n\t\t// TODO: implement reverse container iteration to improve performance here for sparse data. --Jaffee\n\t\tfor i := maxRowID; i >= minRowID; i-- {\n\t\t\trow, err := f.row(tx, i)\n\t\t\tif err != nil {\n\t\t\t\treturn 0, 0, err\n\t\t\t}\n\t\t\trow = row.Intersect(filter)\n\n\t\t\tcount := row.Count()\n\t\t\tif count > 0 {\n\t\t\t\treturn i, count, nil\n\t\t\t}\n\t\t}\n\t}\n\treturn 0, 0, nil\n}\n\n// maxRowID determines the field's maxRowID value based\n// on the contents of its storage, and sets the struct argument.\nfunc (f *fragment) maxRowID(tx Tx) (_ uint64, err error) {\n\tmax, err := tx.Max(f.index(), f.field(), f.view(), f.shard)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn max / ShardWidth, nil\n}\n\n// rangeOp returns bitmaps with a bsiGroup value encoding matching the predicate.\nfunc (f *fragment) rangeOp(tx Tx, op pql.Token, bitDepth uint64, predicate int64) (*Row, error) {\n\tswitch op {\n\tcase pql.EQ:\n\t\treturn f.rangeEQ(tx, bitDepth, predicate)\n\tcase pql.NEQ:\n\t\treturn f.rangeNEQ(tx, bitDepth, predicate)\n\tcase pql.LT, pql.LTE:\n\t\treturn f.rangeLT(tx, bitDepth, predicate, op == pql.LTE)\n\tcase pql.GT, pql.GTE:\n\t\treturn f.rangeGT(tx, bitDepth, predicate, op == pql.GTE)\n\tdefault:\n\t\treturn nil, ErrInvalidRangeOperation\n\t}\n}\n\nfunc absInt64(v int64) uint64 {\n\tswitch {\n\tcase v > 0:\n\t\treturn uint64(v)\n\tcase v == -9223372036854775808:\n\t\treturn 9223372036854775808\n\tdefault:\n\t\treturn uint64(-v)\n\t}\n}\n\nfunc (f *fragment) rangeEQ(tx Tx, bitDepth uint64, predicate int64) (*Row, error) {\n\t// Start with set of columns with values set.\n\tb, err := f.row(tx, bsiExistsBit)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tupredicate := absInt64(predicate)\n\tif uint64(bits.Len64(upredicate)) > bitDepth {\n\t\t// Predicate is out of range.\n\t\treturn NewRow(), nil\n\t}\n\n\t// Filter to only positive/negative numbers.\n\tr, err := f.row(tx, bsiSignBit)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif predicate < 0 {\n\t\tb = b.Intersect(r) // only negatives\n\t} else {\n\t\tb = b.Difference(r) // only positives\n\t}\n\n\t// Filter any bits that don't match the current bit value.\n\tfor i := int(bitDepth - 1); i >= 0; i-- {\n\t\trow, err := f.row(tx, uint64(bsiOffsetBit+i))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tbit := (upredicate >> uint(i)) & 1\n\n\t\tif bit == 1 {\n\t\t\tb = b.Intersect(row)\n\t\t} else {\n\t\t\tb = b.Difference(row)\n\t\t}\n\t}\n\n\treturn b, nil\n}\n\nfunc (f *fragment) rangeNEQ(tx Tx, bitDepth uint64, predicate int64) (*Row, error) {\n\t// Start with set of columns with values set.\n\tb, err := f.row(tx, bsiExistsBit)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Get the equal bitmap.\n\teq, err := f.rangeEQ(tx, bitDepth, predicate)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Not-null minus the equal bitmap.\n\tb = b.Difference(eq)\n\n\treturn b, nil\n}\n\nfunc (f *fragment) rangeLT(tx Tx, bitDepth uint64, predicate int64, allowEquality bool) (*Row, error) {\n\tif predicate == 1 && !allowEquality {\n\t\tpredicate, allowEquality = 0, true\n\t}\n\n\t// Start with set of columns with values set.\n\tb, err := f.row(tx, bsiExistsBit)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Get the sign bit row.\n\tsign, err := f.row(tx, bsiSignBit)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Create predicate without sign bit.\n\tupredicate := absInt64(predicate)\n\n\tswitch {\n\tcase predicate == 0 && !allowEquality:\n\t\t// Match all negative integers.\n\t\treturn b.Intersect(sign), nil\n\tcase predicate == 0 && allowEquality:\n\t\t// Match all integers that are either negative or 0.\n\t\tzeroes, err := f.rangeEQ(tx, bitDepth, 0)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn b.Intersect(sign).Union(zeroes), nil\n\tcase predicate < 0:\n\t\t// Match all every negative number beyond the predicate.\n\t\treturn f.rangeGTUnsigned(tx, b.Intersect(sign), bitDepth, upredicate, allowEquality)\n\tdefault:\n\t\t// Match positive numbers less than the predicate, and all negatives.\n\t\tpos, err := f.rangeLTUnsigned(tx, b.Difference(sign), bitDepth, upredicate, allowEquality)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tneg := b.Intersect(sign)\n\t\treturn pos.Union(neg), nil\n\t}\n}\n\n// rangeLTUnsigned returns all bits LT/LTE the predicate without considering the sign bit.\nfunc (f *fragment) rangeLTUnsigned(tx Tx, filter *Row, bitDepth uint64, predicate uint64, allowEquality bool) (*Row, error) {\n\tswitch {\n\tcase uint64(bits.Len64(predicate)) > bitDepth:\n\t\tfallthrough\n\tcase predicate == (1<<bitDepth)-1 && allowEquality:\n\t\t// This query matches all possible values.\n\t\treturn filter, nil\n\tcase predicate == (1<<bitDepth)-1 && !allowEquality:\n\t\t// This query matches everything that is not (1<<bitDepth)-1.\n\t\tmatches := NewRow()\n\t\tfor i := uint64(0); i < bitDepth; i++ {\n\t\t\trow, err := f.row(tx, uint64(bsiOffsetBit+i))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tmatches = matches.Union(filter.Difference(row))\n\t\t}\n\t\treturn matches, nil\n\tcase allowEquality:\n\t\tpredicate++\n\t}\n\n\t// Compare intermediate bits.\n\tmatched := NewRow()\n\tremaining := filter\n\tfor i := int(bitDepth - 1); i >= 0 && predicate > 0 && remaining.Any(); i-- {\n\t\trow, err := f.row(tx, uint64(bsiOffsetBit+i))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tzeroes := remaining.Difference(row)\n\t\tswitch (predicate >> uint(i)) & 1 {\n\t\tcase 1:\n\t\t\t// Match everything with a zero bit here.\n\t\t\tmatched = matched.Union(zeroes)\n\t\t\tpredicate &^= 1 << uint(i)\n\t\tcase 0:\n\t\t\t// Discard everything with a one bit here.\n\t\t\tremaining = zeroes\n\t\t}\n\t}\n\n\treturn matched, nil\n}\n\nfunc (f *fragment) rangeGT(tx Tx, bitDepth uint64, predicate int64, allowEquality bool) (*Row, error) {\n\tif predicate == -1 && !allowEquality {\n\t\tpredicate, allowEquality = 0, true\n\t}\n\n\tb, err := f.row(tx, bsiExistsBit)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// Create predicate without sign bit.\n\tupredicate := absInt64(predicate)\n\n\tsign, err := f.row(tx, bsiSignBit)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tswitch {\n\tcase predicate == 0 && !allowEquality:\n\t\t// Match all positive numbers except zero.\n\t\tnonzero, err := f.rangeNEQ(tx, bitDepth, 0)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tb = nonzero\n\t\tfallthrough\n\tcase predicate == 0 && allowEquality:\n\t\t// Match all positive numbers.\n\t\treturn b.Difference(sign), nil\n\tcase predicate >= 0:\n\t\t// Match all positive numbers greater than the predicate.\n\t\treturn f.rangeGTUnsigned(tx, b.Difference(sign), bitDepth, upredicate, allowEquality)\n\tdefault:\n\t\t// Match all positives and greater negatives.\n\t\tneg, err := f.rangeLTUnsigned(tx, b.Intersect(sign), bitDepth, upredicate, allowEquality)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tpos := b.Difference(sign)\n\t\treturn pos.Union(neg), nil\n\t}\n}\n\nfunc (f *fragment) rangeGTUnsigned(tx Tx, filter *Row, bitDepth uint64, predicate uint64, allowEquality bool) (*Row, error) {\nprep:\n\tswitch {\n\tcase predicate == 0 && allowEquality:\n\t\t// This query matches all possible values.\n\t\treturn filter, nil\n\tcase predicate == 0 && !allowEquality:\n\t\t// This query matches everything that is not 0.\n\t\tmatches := NewRow()\n\t\tfor i := uint64(0); i < bitDepth; i++ {\n\t\t\trow, err := f.row(tx, uint64(bsiOffsetBit+i))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tmatches = matches.Union(filter.Intersect(row))\n\t\t}\n\t\treturn matches, nil\n\tcase !allowEquality && uint64(bits.Len64(predicate)) > bitDepth:\n\t\t// The predicate is bigger than the BSI width, so nothing can be bigger.\n\t\treturn NewRow(), nil\n\tcase allowEquality:\n\t\tpredicate--\n\t\tallowEquality = false\n\t\tgoto prep\n\t}\n\n\t// Compare intermediate bits.\n\tmatched := NewRow()\n\tremaining := filter\n\tpredicate |= (^uint64(0)) << bitDepth\n\tfor i := int(bitDepth - 1); i >= 0 && predicate < ^uint64(0) && remaining.Any(); i-- {\n\t\trow, err := f.row(tx, uint64(bsiOffsetBit+i))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tones := remaining.Intersect(row)\n\t\tswitch (predicate >> uint(i)) & 1 {\n\t\tcase 1:\n\t\t\t// Discard everything with a zero bit here.\n\t\t\tremaining = ones\n\t\tcase 0:\n\t\t\t// Match everything with a one bit here.\n\t\t\tmatched = matched.Union(ones)\n\t\t\tpredicate |= 1 << uint(i)\n\t\t}\n\t}\n\n\treturn matched, nil\n}\n\n// notNull returns the exists row.\nfunc (f *fragment) notNull(tx Tx) (*Row, error) {\n\treturn f.row(tx, bsiExistsBit)\n}\n\n// rangeBetween returns bitmaps with a bsiGroup value encoding matching any value between predicateMin and predicateMax.\nfunc (f *fragment) rangeBetween(tx Tx, bitDepth uint64, predicateMin, predicateMax int64) (*Row, error) {\n\tb, err := f.row(tx, bsiExistsBit)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Convert predicates to unsigned values.\n\tupredicateMin, upredicateMax := absInt64(predicateMin), absInt64(predicateMax)\n\n\tswitch {\n\tcase predicateMin == predicateMax:\n\t\treturn f.rangeEQ(tx, bitDepth, predicateMin)\n\tcase predicateMin >= 0:\n\t\t// Handle positive-only values.\n\t\tr, err := f.row(tx, bsiSignBit)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.rangeBetweenUnsigned(tx, b.Difference(r), bitDepth, upredicateMin, upredicateMax)\n\tcase predicateMax < 0:\n\t\t// Handle negative-only values. Swap unsigned min/max predicates.\n\t\tr, err := f.row(tx, bsiSignBit)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.rangeBetweenUnsigned(tx, b.Intersect(r), bitDepth, upredicateMax, upredicateMin)\n\tdefault:\n\t\t// If predicate crosses positive/negative boundary then handle separately and union.\n\t\tr0, err := f.row(tx, bsiSignBit)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tpos, err := f.rangeLTUnsigned(tx, b.Difference(r0), bitDepth, upredicateMax, true)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tr1, err := f.row(tx, bsiSignBit)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tneg, err := f.rangeLTUnsigned(tx, b.Intersect(r1), bitDepth, upredicateMin, true)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn pos.Union(neg), nil\n\t}\n}\n\n// rangeBetweenUnsigned returns BSI columns for a range of values. Disregards the sign bit.\nfunc (f *fragment) rangeBetweenUnsigned(tx Tx, filter *Row, bitDepth uint64, predicateMin, predicateMax uint64) (*Row, error) {\n\tswitch {\n\tcase predicateMax > (1<<bitDepth)-1:\n\t\t// The upper bound cannot be violated.\n\t\treturn f.rangeGTUnsigned(tx, filter, bitDepth, predicateMin, true)\n\tcase predicateMin == 0:\n\t\t// The lower bound cannot be violated.\n\t\treturn f.rangeLTUnsigned(tx, filter, bitDepth, predicateMax, true)\n\t}\n\n\t// Compare any upper bits which are equal.\n\tdiffLen := bits.Len64(predicateMax ^ predicateMin)\n\tremaining := filter\n\tfor i := int(bitDepth - 1); i >= diffLen; i-- {\n\t\trow, err := f.row(tx, uint64(bsiOffsetBit+i))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tswitch (predicateMin >> uint(i)) & 1 {\n\t\tcase 1:\n\t\t\tremaining = remaining.Intersect(row)\n\t\tcase 0:\n\t\t\tremaining = remaining.Difference(row)\n\t\t}\n\t}\n\n\t// Clear the bits we just compared.\n\tequalMask := (^uint64(0)) << diffLen\n\tpredicateMin &^= equalMask\n\tpredicateMax &^= equalMask\n\n\tvar err error\n\tremaining, err = f.rangeGTUnsigned(tx, remaining, uint64(diffLen), predicateMin, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tremaining, err = f.rangeLTUnsigned(tx, remaining, uint64(diffLen), predicateMax, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn remaining, nil\n}\n\n// pos translates the row ID and column ID into a position in the storage bitmap.\nfunc (f *fragment) pos(rowID, columnID uint64) (uint64, error) {\n\t// Return an error if the column ID is out of the range of the fragment's shard.\n\tminColumnID := f.shard * ShardWidth\n\tif columnID < minColumnID || columnID >= minColumnID+ShardWidth {\n\t\treturn 0, errors.Errorf(\"column:%d out of bounds for shard %d\", columnID, f.shard)\n\t}\n\treturn pos(rowID, columnID), nil\n}\n\n// top returns the top rows from the fragment.\n// If opt.Src is specified then only rows which intersect src are returned.\nfunc (f *fragment) top(tx Tx, opt topOptions) ([]Pair, error) {\n\t// Retrieve pairs. If no row ids specified then return from cache.\n\tpairs, err := f.topBitmapPairs(tx, opt.RowIDs)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// If row ids are provided, we don't want to truncate the result set\n\tif len(opt.RowIDs) > 0 {\n\t\topt.N = 0\n\t}\n\n\t// Use `tanimotoThreshold > 0` to indicate whether or not we are considering Tanimoto.\n\tvar tanimotoThreshold uint64\n\tvar minTanimoto, maxTanimoto float64\n\tvar srcCount uint64\n\tif opt.TanimotoThreshold > 0 && opt.Src != nil {\n\t\ttanimotoThreshold = opt.TanimotoThreshold\n\t\tsrcCount = opt.Src.Count()\n\t\tminTanimoto = float64(srcCount*tanimotoThreshold) / 100\n\t\tmaxTanimoto = float64(srcCount*100) / float64(tanimotoThreshold)\n\t}\n\n\t// Iterate over rankings and add to results until we have enough.\n\tresults := &pairHeap{}\n\tfor _, pair := range pairs {\n\t\trowID, cnt := pair.ID, pair.Count\n\n\t\t// Ignore empty rows.\n\t\tif cnt == 0 {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Check against either Tanimoto threshold or minimum threshold.\n\t\tif tanimotoThreshold > 0 {\n\t\t\t// Ignore counts outside of the Tanimoto min/max values.\n\t\t\tif float64(cnt) <= minTanimoto || float64(cnt) >= maxTanimoto {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t} else {\n\t\t\t// Ignore counts less than MinThreshold.\n\t\t\tif cnt < opt.MinThreshold {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\t// The initial n pairs should simply be added to the results.\n\t\tif opt.N == 0 || results.Len() < opt.N {\n\t\t\t// Calculate count and append.\n\t\t\tcount := cnt\n\t\t\tif opt.Src != nil {\n\t\t\t\tr, err := f.row(tx, rowID)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tcount = opt.Src.intersectionCount(r)\n\t\t\t}\n\t\t\tif count == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Check against either Tanimoto threshold or minimum threshold.\n\t\t\tif tanimotoThreshold > 0 {\n\t\t\t\ttanimoto := math.Ceil(float64(count*100) / float64(cnt+srcCount-count))\n\t\t\t\tif tanimoto <= float64(tanimotoThreshold) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif count < opt.MinThreshold {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\n\t\t\theap.Push(results, Pair{ID: rowID, Count: count})\n\n\t\t\t// If we reach the requested number of pairs and we are not computing\n\t\t\t// intersections then simply exit. If we are intersecting then sort\n\t\t\t// and then only keep pairs that are higher than the lowest count.\n\t\t\tif opt.N > 0 && results.Len() == opt.N {\n\t\t\t\tif opt.Src == nil {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tcontinue\n\t\t}\n\n\t\t// Retrieve the lowest count we have.\n\t\t// If it's too low then don't try finding anymore pairs.\n\t\tthreshold := results.Pairs[0].Count\n\n\t\t// If the row doesn't have enough columns set before the intersection\n\t\t// then we can assume that any remaining rows also have a count too low.\n\t\tif threshold < opt.MinThreshold || cnt < threshold {\n\t\t\tbreak\n\t\t}\n\n\t\t// Calculate the intersecting column count and skip if it's below our\n\t\t// last row in our current result set.\n\t\tr, err := f.row(tx, rowID)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tcount := opt.Src.intersectionCount(r)\n\t\tif count < threshold {\n\t\t\tcontinue\n\t\t}\n\n\t\theap.Push(results, Pair{ID: rowID, Count: count})\n\t}\n\n\t//Pop first opt.N elements out of heap\n\tr := make(Pairs, results.Len())\n\tx := results.Len()\n\ti := 1\n\tfor results.Len() > 0 {\n\t\tr[x-i] = heap.Pop(results).(Pair)\n\t\ti++\n\t}\n\treturn r, nil\n}\n\nfunc (f *fragment) topBitmapPairs(tx Tx, rowIDs []uint64) ([]bitmapPair, error) {\n\t// Don't retrieve from storage if CacheTypeNone.\n\tif f.CacheType == CacheTypeNone {\n\t\treturn f.cache.Top(), nil\n\t}\n\t// If no specific rows are requested, retrieve top rows.\n\tif len(rowIDs) == 0 {\n\t\tf.mu.Lock()\n\t\tdefer f.mu.Unlock()\n\t\tf.cache.Invalidate()\n\t\treturn f.cache.Top(), nil\n\t}\n\n\t// Otherwise retrieve specific rows.\n\tpairs := make([]bitmapPair, 0, len(rowIDs))\n\tfor _, rowID := range rowIDs {\n\t\t// Look up cache first, if available.\n\t\tif n := f.cache.Get(rowID); n > 0 {\n\t\t\tpairs = append(pairs, bitmapPair{\n\t\t\t\tID:    rowID,\n\t\t\t\tCount: n,\n\t\t\t})\n\t\t\tcontinue\n\t\t}\n\n\t\trow, err := f.row(tx, rowID)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif row.Count() > 0 {\n\t\t\t// Otherwise load from storage.\n\t\t\tpairs = append(pairs, bitmapPair{\n\t\t\t\tID:    rowID,\n\t\t\t\tCount: row.Count(),\n\t\t\t})\n\t\t}\n\t}\n\tsortPairs := bitmapPairs(pairs)\n\tsort.Sort(&sortPairs)\n\treturn pairs, nil\n}\n\n// topOptions represents options passed into the Top() function.\ntype topOptions struct {\n\t// Number of rows to return.\n\tN int\n\n\t// Bitmap to intersect with.\n\tSrc *Row\n\n\t// Specific rows to filter against.\n\tRowIDs       []uint64\n\tMinThreshold uint64\n\n\tTanimotoThreshold uint64\n}\n\n// bulkImport bulk imports a set of bits.\n// The cache is updated to reflect the new data.\nfunc (f *fragment) bulkImport(tx Tx, rowIDs, columnIDs []uint64, options *ImportOptions) error {\n\t// Verify that there are an equal number of row ids and column ids.\n\tif len(rowIDs) != len(columnIDs) {\n\t\treturn fmt.Errorf(\"mismatch of row/column len: %d != %d\", len(rowIDs), len(columnIDs))\n\t}\n\n\tif f.mutexVector != nil && !options.Clear {\n\t\treturn f.bulkImportMutex(tx, rowIDs, columnIDs, options)\n\t}\n\treturn f.bulkImportStandard(tx, rowIDs, columnIDs, options)\n}\n\n// clearBitsReportingChanges is a special fancy case. For existence-tracking,\n// if we're clearing bits in a mutex, *successfully* cleared bits become null\n// records, so we have to report, not how many records we cleared, but which\n// records specifically became clear. The returned set of bits is the column\n// IDs that actually got a bit cleared from them.\n//\n// This is basically following the logic of bulkImportStandard and\n// importPositions, except that it combines them and drops some of the\n// no longer needed branches.\nfunc (f *fragment) clearBitsReportingChanges(tx Tx, rowIDs, columnIDs []uint64) ([]uint64, error) {\n\t// Verify that there are an equal number of row ids and column ids.\n\tif len(rowIDs) != len(columnIDs) {\n\t\treturn nil, fmt.Errorf(\"mismatch of row/column len: %d != %d\", len(rowIDs), len(columnIDs))\n\t}\n\n\t// rowSet maintains the set of rowIDs present in this import. It allows the\n\t// cache to be updated once per row, instead of once per bit. TODO: consider\n\t// sorting by rowID/columnID first and avoiding the map allocation here. (we\n\t// could reuse rowIDs to store the list of unique row IDs)\n\trowSet := make(map[uint64]struct{})\n\tlastRowID := uint64(1 << 63)\n\n\t// replace columnIDs with calculated positions to avoid allocation.\n\tprevRow, prevCol := ^uint64(0), ^uint64(0)\n\tnext := 0\n\tfor i := 0; i < len(columnIDs); i++ {\n\t\trowID, columnID := rowIDs[i], columnIDs[i]\n\t\tif rowID == prevRow && columnID == prevCol {\n\t\t\tcontinue\n\t\t}\n\t\tprevRow, prevCol = rowID, columnID\n\t\tpos, err := f.pos(rowID, columnID)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tcolumnIDs[next] = pos\n\t\tnext++\n\n\t\t// Add row to rowSet.\n\t\tif rowID != lastRowID {\n\t\t\tlastRowID = rowID\n\t\t\trowSet[rowID] = struct{}{}\n\t\t}\n\t}\n\tclear := columnIDs[:next]\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\tCounterClearingingN.Add(float64(len(clear)))\n\tchanged, err := tx.Removed(f.index(), f.field(), f.view(), f.shard, clear...)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"clearing positions\")\n\t}\n\tCounterClearedN.Add(float64(len(changed)))\n\treturn changed, f.updateCaching(tx, rowSet)\n}\n\n// bulkImportStandard performs a bulk import on a standard fragment. May mutate\n// its rowIDs and columnIDs arguments.\nfunc (f *fragment) bulkImportStandard(tx Tx, rowIDs, columnIDs []uint64, options *ImportOptions) (err error) {\n\t// rowSet maintains the set of rowIDs present in this import. It allows the\n\t// cache to be updated once per row, instead of once per bit. TODO: consider\n\t// sorting by rowID/columnID first and avoiding the map allocation here. (we\n\t// could reuse rowIDs to store the list of unique row IDs)\n\trowSet := make(map[uint64]struct{})\n\tlastRowID := uint64(1 << 63)\n\n\t// replace columnIDs with calculated positions to avoid allocation.\n\tprevRow, prevCol := ^uint64(0), ^uint64(0)\n\tnext := 0\n\tfor i := 0; i < len(columnIDs); i++ {\n\t\trowID, columnID := rowIDs[i], columnIDs[i]\n\t\tif rowID == prevRow && columnID == prevCol {\n\t\t\tcontinue\n\t\t}\n\t\tprevRow, prevCol = rowID, columnID\n\t\tpos, err := f.pos(rowID, columnID)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcolumnIDs[next] = pos\n\t\tnext++\n\n\t\t// Add row to rowSet.\n\t\tif rowID != lastRowID {\n\t\t\tlastRowID = rowID\n\t\t\trowSet[rowID] = struct{}{}\n\t\t}\n\t}\n\tpositions := columnIDs[:next]\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\tif options.Clear {\n\t\terr = f.importPositions(tx, nil, positions, rowSet)\n\t} else {\n\t\terr = f.importPositions(tx, positions, nil, rowSet)\n\t}\n\treturn errors.Wrap(err, \"bulkImportStandard\")\n}\n\n// parallelSlices provides a sort.Interface for corresponding slices of\n// column and row values, and also allows pruning of duplicate values,\n// meaning values for the same column, as you might want for a mutex.\n//\n// The slices should be parallel and of the same length.\ntype parallelSlices struct {\n\tcols, rows []uint64\n}\n\n// prune eliminates values which have the same column key and are\n// adjacent in the slice. It doesn't handle non-adjacent keys, but\n// does report whether it saw any. See fullPrune for what you probably\n// want to be using.\nfunc (p *parallelSlices) prune() (unsorted bool) {\n\tl := len(p.cols)\n\tif l == 0 {\n\t\treturn\n\t}\n\tn := 0\n\tprev := p.cols[0]\n\t// At any point, n is the index of the last value we wrote\n\t// (we pretend we copied 0 to 0 before starting). If the next\n\t// value would have the same column ID, it should replace\n\t// the one we just wrote, otherwise we move n to point to a\n\t// new slot before writing. If there are no duplicates, n is\n\t// always equal to i. we don't check for this because skipping\n\t// those writes would require an extra branch...\n\tfor i := 1; i < l; i++ {\n\t\tnext := p.cols[i]\n\t\tif next < prev {\n\t\t\tunsorted = true\n\t\t}\n\t\tif next != prev {\n\t\t\tn++\n\t\t}\n\t\tprev = next\n\t\tp.cols[n] = p.cols[i]\n\t\tp.rows[n] = p.rows[i]\n\t}\n\tp.rows = p.rows[:n+1]\n\tp.cols = p.cols[:n+1]\n\treturn unsorted\n}\n\n// fullPrune trims any adjacent values with identical column keys (and\n// the corresponding row values), and if it notices that anything was unsorted,\n// does a stable sort by column key and tries that again, ensuring that\n// there's no items with the same column key. The last entry with a given\n// column key wins.\nfunc (p *parallelSlices) fullPrune() {\n\tif len(p.cols) == 0 {\n\t\treturn\n\t}\n\tif len(p.rows) != len(p.cols) {\n\t\tvprint.PanicOn(\"parallelSlices must have same length for rows and columns\")\n\t}\n\tunsorted := p.prune()\n\tif unsorted {\n\n\t\t// Q: why sort.Stable instead of sort.Sort?\n\t\t//\n\t\t// A: Because we need to ensure that the last entry\n\t\t// is the one that wins. The last entry is the most recent update, and\n\t\t// so the mutex field should reflect that one and not earlier\n\t\t// updates. Mutex fields are special in that only 1 bit can\n\t\t// be hot (set to 1), so the last update overrides all the others. We\n\t\t// exploit this to eliminate irrelevant earlier writes.\n\t\t//\n\t\t// So if the input columns were {1, 2, 1},\n\t\t// and input rows were          {3, 4, 5},\n\t\t// we need to be sure that we end up with cols:{1, 2}\n\t\t//                                        rows:{5, 4} // right, last update won.\n\t\t//                                and not rows:{3, 4} // wrong, first update won.\n\t\t//\n\t\t// illustrated: (dk = don't know state)\n\t\t//\n\t\t// the new, raw data before later updates \"win\":\n\t\t//\n\t\t//         col0  col1  col2\n\t\t// row3    dk    1     dk\n\t\t// row4    dk    dk    1\n\t\t// row5    dk    1     dk\n\t\t//\n\t\t// after last one wins, to be written to the backend:\n\t\t//\n\t\t//         col0  col1  col2\n\t\t// row3    dk    0     0\n\t\t// row4    dk    0     1\n\t\t// row5    dk    1     0\n\t\t//               ^\n\t\t//                \\-- on col1. The last one won, b/c its a mutex all other rows go to 0 for that column.\n\t\t//\n\t\t// Which means we need a stable sort, ensuring that if two things\n\t\t// have the same column key, they stay in the same relative order,\n\t\t// and then the prune algorithm always keeps the last.\n\t\tsort.Stable(p)\n\n\t\t_ = p.prune()\n\t}\n}\n\nfunc (p *parallelSlices) Len() int {\n\treturn len(p.cols)\n}\n\nfunc (p *parallelSlices) Less(i, j int) bool {\n\treturn p.cols[i] < p.cols[j]\n}\n\nfunc (p parallelSlices) Swap(i, j int) {\n\tp.cols[i], p.cols[j] = p.cols[j], p.cols[i]\n\tp.rows[i], p.rows[j] = p.rows[j], p.rows[i]\n}\n\n// importPositions takes slices of positions within the fragment to set and\n// clear in storage. One must also pass in the set of unique rows which are\n// affected by the set and clear operations. It is unprotected (f.mu must be\n// locked when calling it). No position should appear in both set and clear.\n//\n// importPositions tries to intelligently decide whether or not to do a full\n// snapshot of the fragment or just do in-memory updates while appending\n// operations to the op log.\nfunc (f *fragment) importPositions(tx Tx, set, clear []uint64, rowSet map[uint64]struct{}) error {\n\tif len(set) > 0 {\n\t\tCounterImportingN.Add(float64(len(set)))\n\n\t\t// TODO benchmark Add/RemoveN behavior with sorted/unsorted positions\n\t\tchangedN, err := tx.Add(f.index(), f.field(), f.view(), f.shard, set...)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"adding positions\")\n\t\t}\n\t\tCounterImportedN.Add(float64(changedN))\n\t}\n\n\tif len(clear) > 0 {\n\t\tCounterClearingingN.Add(float64(len(clear)))\n\t\tchangedN, err := tx.Remove(f.index(), f.field(), f.view(), f.shard, clear...)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"clearing positions\")\n\t\t}\n\t\tCounterClearedN.Add(float64(changedN))\n\t}\n\treturn f.updateCaching(tx, rowSet)\n}\n\n// updateCaching clears checksums for rows, and clears any existing TopN\n// cache for them, and marks the cache for needing updates. I'm not sure\n// that's correct. This was originally the tail end of importPositions, but\n// we want to be able to access the same logic from elsewhere.\nfunc (f *fragment) updateCaching(tx Tx, rowSet map[uint64]struct{}) error {\n\t// Update cache counts for all affected rows.\n\tfor rowID := range rowSet {\n\t\t// Invalidate block checksum.\n\t\tdelete(f.checksums, int(rowID/HashBlockSize))\n\n\t\tif f.CacheType != CacheTypeNone {\n\t\t\tstart := rowID * ShardWidth\n\t\t\tend := (rowID + 1) * ShardWidth\n\n\t\t\tn, err := tx.CountRange(f.index(), f.field(), f.view(), f.shard, start, end)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Wrap(err, \"CountRange\")\n\t\t\t}\n\n\t\t\tf.cache.BulkAdd(rowID, n)\n\t\t}\n\t}\n\n\tif f.CacheType != CacheTypeNone {\n\t\tf.cache.Invalidate()\n\t}\n\treturn nil\n}\n\n// bulkImportMutex performs a bulk import on a fragment while ensuring\n// mutex restrictions. Because the mutex requirements must be checked\n// against storage, this method must acquire a write lock on the fragment\n// during the entire process, and it handles every bit independently.\nfunc (f *fragment) bulkImportMutex(tx Tx, rowIDs, columnIDs []uint64, options *ImportOptions) error {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\n\tp := parallelSlices{cols: columnIDs, rows: rowIDs}\n\tp.fullPrune()\n\tcolumnIDs = p.cols\n\trowIDs = p.rows\n\n\t// create a mask of columns we care about\n\tcolumns := roaring.NewSliceBitmap(columnIDs...)\n\n\t// we now need to find existing rows for these bits.\n\trowSet := make(map[uint64]struct{}, len(rowIDs))\n\tunsorted := false\n\tprev := uint64(0)\n\tfor i := range rowIDs {\n\t\trowID, columnID := rowIDs[i], columnIDs[i]\n\t\trowSet[rowID] = struct{}{}\n\t\tpos, err := f.pos(rowID, columnID)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, fmt.Sprintf(\"finding pos for row %d, col %d\", rowID, columnID))\n\t\t}\n\t\t// positions are sorted by columns, but not by absolute\n\t\t// position. we might want them sorted, though.\n\t\tif pos < prev {\n\t\t\tunsorted = true\n\t\t}\n\t\tprev = pos\n\t\trowIDs[i] = pos\n\t}\n\ttoSet := rowIDs\n\tif unsorted {\n\t\tsort.Slice(toSet, func(i, j int) bool { return toSet[i] < toSet[j] })\n\t}\n\n\tnextKey := toSet[0] >> 16\n\tscratchContainer := roaring.NewContainerArray([]uint16{})\n\trewriteExisting := roaring.NewBitmapBitmapTrimmer(columns, func(key roaring.FilterKey, data *roaring.Container, filter *roaring.Container, writeback roaring.ContainerWriteback) error {\n\t\tvar inserting []uint64\n\t\tfor roaring.FilterKey(nextKey) < key {\n\t\t\tthisKey := roaring.FilterKey(nextKey)\n\t\t\tinserting, toSet, nextKey = roaring.GetMatchingKeysFrom(toSet, nextKey)\n\t\t\tscratchContainer = roaring.RemakeContainerFrom(scratchContainer, inserting)\n\n\t\t\terr := writeback(thisKey, scratchContainer)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\t// we only wanted to insert the data we had before the end, there's\n\t\t// no actual data here to modify.\n\t\tif data == nil {\n\t\t\treturn nil\n\t\t}\n\t\tif roaring.FilterKey(nextKey) > key {\n\t\t\t// simple path: we only have to remove things from the filter,\n\t\t\t// if there are any.\n\t\t\tif filter.N() == 0 {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\texisting := data.N()\n\t\t\tdata = data.DifferenceInPlace(filter)\n\t\t\tif data.N() != existing {\n\t\t\t\trowSet[key.Row()] = struct{}{}\n\t\t\t\treturn writeback(key, data)\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t\t// nextKey has to be the same as key. we have values to insert, and\n\t\t// necessarily have a filter to remove which matches them. so we're\n\t\t// going to remove everything in the filter, then add all the values\n\t\t// we have to insert. but! in the case where a bit is already set,\n\t\t// and we remove it and re-add it, we don't want to count that.\n\t\tinserting, toSet, nextKey = roaring.GetMatchingKeysFrom(toSet, nextKey)\n\n\t\texisting := data.N()\n\n\t\t// so, we want to remove anything that's in the filter, *but*, if a\n\t\t// thing is in the filter, and we then add it back, that doesn't\n\t\t// count. but if a thing is in the filter, but *wasn't originally\n\t\t// there*, that counts. But we can't check that *after* we compute\n\t\t// the difference, so...\n\t\treAdds := 0\n\t\tfor _, v := range inserting {\n\t\t\tif filter.Contains(uint16(v)) && data.Contains(uint16(v)) {\n\t\t\t\treAdds++\n\t\t\t}\n\t\t}\n\t\tdata = data.DifferenceInPlace(filter)\n\t\tremoves := int(existing - data.N())\n\t\tvar changed bool\n\t\tadds := 0\n\t\tfor _, v := range inserting {\n\t\t\tdata, changed = data.Add(uint16(v))\n\t\t\tif changed {\n\t\t\t\tadds++\n\t\t\t}\n\t\t}\n\t\t// if we added more things than were being readded, or removed more\n\t\t// things than were being readded, we changed something.\n\t\tif adds > reAdds || removes > reAdds {\n\t\t\trowSet[key.Row()] = struct{}{}\n\t\t\treturn writeback(key, data)\n\t\t}\n\t\treturn nil\n\t})\n\n\terr := tx.ApplyRewriter(f.index(), f.field(), f.view(), f.shard, 0, rewriteExisting)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn f.updateCaching(tx, rowSet)\n}\n\n// ClearRecords deletes all bits for the given records. It's basically\n// the remove-only part of setting a mutex.\nfunc (f *fragment) ClearRecords(tx Tx, recordIDs []uint64) (bool, error) {\n\t// create a mask of columns we care about\n\tcolumns := roaring.NewSliceBitmap(recordIDs...)\n\treturn f.clearRecordsByBitmap(tx, columns)\n}\n\nfunc (f *fragment) clearRecordsByBitmap(tx Tx, columns *roaring.Bitmap) (changed bool, err error) {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\treturn f.unprotectedClearRecordsByBitmap(tx, columns)\n}\n\n// clearRecordsByBitmap clears bits in a fragment that correspond to those\n// positions within the bitmap.\nfunc (f *fragment) unprotectedClearRecordsByBitmap(tx Tx, columns *roaring.Bitmap) (changed bool, err error) {\n\trowSet := make(map[uint64]struct{})\n\trewriteExisting := roaring.NewBitmapBitmapTrimmer(columns, func(key roaring.FilterKey, data *roaring.Container, filter *roaring.Container, writeback roaring.ContainerWriteback) error {\n\t\tif filter.N() == 0 {\n\t\t\treturn nil\n\t\t}\n\t\texisting := data.N()\n\t\t// nothing to delete. this can't happen normally, but the rewriter calls\n\t\t// us with an empty data container when it's done.\n\t\tif existing == 0 {\n\t\t\treturn nil\n\t\t}\n\t\tdata = data.DifferenceInPlace(filter)\n\t\tif data.N() != existing {\n\t\t\trowSet[key.Row()] = struct{}{}\n\t\t\tchanged = true\n\t\t\treturn writeback(key, data)\n\t\t}\n\t\treturn nil\n\t})\n\n\terr = tx.ApplyRewriter(f.index(), f.field(), f.view(), f.shard, 0, rewriteExisting)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\treturn changed, f.updateCaching(tx, rowSet)\n}\n\n// importValue bulk imports a set of range-encoded values.\nfunc (f *fragment) importValue(tx Tx, columnIDs []uint64, values []int64, bitDepth uint64, clear bool) error {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\n\t// Verify that there are an equal number of column ids and values.\n\tif len(columnIDs) != len(values) {\n\t\treturn fmt.Errorf(\"mismatch of column/value len: %d != %d\", len(columnIDs), len(values))\n\t}\n\tpositionsByDepth := make([][]uint64, bitDepth+2)\n\ttoSetByDepth := make([]int, bitDepth+2)\n\ttoClearByDepth := make([]int, bitDepth+2)\n\tbatchSize := len(columnIDs)\n\tif batchSize > 65536 {\n\t\tbatchSize = 65536\n\t}\n\tfor i := 0; i < int(bitDepth)+2; i++ {\n\t\tpositionsByDepth[i] = make([]uint64, batchSize)\n\t\ttoClearByDepth[i] = batchSize\n\t}\n\n\trow := 0\n\tcolumnID := uint64(0)\n\tvalue := int64(0)\n\t// arbitrarily set prev to be not equal to the first column ID\n\t// we will encounter.\n\tprev := columnIDs[len(columnIDs)-1] + 1\n\tfor len(columnIDs) > 0 {\n\t\tdownTo := len(columnIDs) - batchSize\n\t\tif downTo < 0 {\n\t\t\tdownTo = 0\n\t\t}\n\t\tfor i := range positionsByDepth {\n\t\t\ttoSetByDepth[i] = 0\n\t\t\ttoClearByDepth[i] = batchSize\n\t\t}\n\t\tfor i := len(columnIDs) - 1; i >= downTo; i-- {\n\t\t\tcolumnID, value = columnIDs[i], values[i]\n\t\t\tcolumnID = columnID % ShardWidth\n\t\t\tif columnID == prev {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tprev = columnID\n\t\t\trow = 0\n\t\t\tif clear {\n\t\t\t\ttoClearByDepth[row]--\n\t\t\t\tpositionsByDepth[row][toClearByDepth[row]] = columnID\n\t\t\t} else {\n\t\t\t\tpositionsByDepth[row][toSetByDepth[row]] = columnID\n\t\t\t\ttoSetByDepth[row]++\n\t\t\t}\n\t\t\trow++\n\t\t\tcolumnID += ShardWidth\n\t\t\tif value < 0 {\n\t\t\t\tpositionsByDepth[row][toSetByDepth[row]] = columnID\n\t\t\t\ttoSetByDepth[row]++\n\t\t\t\tvalue *= -1\n\t\t\t} else {\n\t\t\t\ttoClearByDepth[row]--\n\t\t\t\tpositionsByDepth[row][toClearByDepth[row]] = columnID\n\t\t\t}\n\t\t\trow++\n\t\t\tcolumnID += ShardWidth\n\t\t\tfor j := 0; j < int(bitDepth); j++ {\n\t\t\t\tif value&1 != 0 {\n\t\t\t\t\tpositionsByDepth[row][toSetByDepth[row]] = columnID\n\t\t\t\t\ttoSetByDepth[row]++\n\t\t\t\t} else {\n\t\t\t\t\ttoClearByDepth[row]--\n\t\t\t\t\tpositionsByDepth[row][toClearByDepth[row]] = columnID\n\t\t\t\t}\n\t\t\t\trow++\n\t\t\t\tcolumnID += ShardWidth\n\t\t\t\tvalue >>= 1\n\t\t\t}\n\t\t}\n\n\t\tfor i := range positionsByDepth {\n\t\t\terr := f.importPositions(tx, positionsByDepth[i][:toSetByDepth[i]], positionsByDepth[i][toClearByDepth[i]:], nil)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Wrap(err, \"importing positions\")\n\t\t\t}\n\t\t}\n\t\tcolumnIDs = columnIDs[:downTo]\n\t}\n\n\treturn nil\n}\n\n// importRoaring imports from the official roaring data format defined at\n// https://github.com/RoaringBitmap/RoaringFormatSpec or from pilosa's version\n// of the roaring format. The cache is updated to reflect the new data.\nfunc (f *fragment) importRoaring(ctx context.Context, tx Tx, data []byte, clear bool) error {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"fragment.importRoaring\")\n\tdefer span.Finish()\n\n\trowSet, updateCache, err := f.doImportRoaring(ctx, tx, data, clear)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"doImportRoaring\")\n\t}\n\tif updateCache {\n\t\treturn f.updateCachePostImport(ctx, rowSet)\n\t}\n\treturn nil\n}\n\n// ImportRoaringClearAndSet simply clears the bits in clear and sets the bits in set.\nfunc (f *fragment) ImportRoaringClearAndSet(ctx context.Context, tx Tx, clear, set []byte) error {\n\tclearIter, err := roaring.NewContainerIterator(clear)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting clear iterator\")\n\t}\n\tsetIter, err := roaring.NewContainerIterator(set)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting set iterator\")\n\t}\n\n\trewriter, err := roaring.NewClearAndSetRewriter(clearIter, setIter)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting rewriter\")\n\t}\n\n\terr = tx.ApplyRewriter(f.index(), f.field(), f.view(), f.shard, 0, rewriter)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"pilosa.ImportRoaringClearAndSet: %s\", err)\n\t}\n\tif f.CacheType != CacheTypeNone {\n\t\t// TODO this may be quite a bit slower than the way\n\t\t// importRoaring does it as it tracks the number of bits\n\t\t// changed per row. We could do that, but I think it'd require\n\t\t// significant changes to the Rewriter API.\n\t\tf.mu.Lock()\n\t\tdefer f.mu.Unlock()\n\t\treturn f.rebuildRankCache(ctx, tx)\n\t}\n\treturn nil\n}\n\n// ImportRoaringBSI interprets \"clear\" as a single row specifying\n// records to be cleared, and \"set\" as specifying the values to be set\n// which implies clearing any other values in those columns.\nfunc (f *fragment) ImportRoaringBSI(ctx context.Context, tx Tx, clear, set []byte) error {\n\t// In this first block, we take the first row of clear as records\n\t// we want to unconditionally clear, and the first row of set as\n\t// records we also want to clear because they're going to get set\n\t// and Union the two together into a single clearing iterator.\n\tclearclearIter, err := roaring.NewRepeatedRowIteratorFromBytes(clear)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting clear iterator\")\n\t}\n\tsetClearIter, err := roaring.NewRepeatedRowIteratorFromBytes(set)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting set/clear iterator\")\n\t}\n\tclearIter := roaring.NewUnionContainerIterator(clearclearIter, setClearIter)\n\n\t// Then we get the set iterator and create the rewriter.\n\tsetIter, err := roaring.NewContainerIterator(set)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting set iterator\")\n\t}\n\trewriter, err := roaring.NewClearAndSetRewriter(clearIter, setIter)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting rewriter\")\n\t}\n\n\terr = tx.ApplyRewriter(f.index(), f.field(), f.view(), f.shard, 0, rewriter)\n\treturn errors.Wrap(err, \"pilosa.ImportRoaringBSI: \")\n}\n\n// ImportRoaringSingleValued treats \"clear\" as a single row and clears\n// all the columns specified, then sets all the bits in set. It's very\n// similar to ImportRoaringBSI, but doesn't treate the first row of\n// \"set\" as the existence row to also be cleared. Essentially it's for\n// FieldTypeMutex.\nfunc (f *fragment) ImportRoaringSingleValued(ctx context.Context, tx Tx, clear, set []byte) error {\n\tclearIter, err := roaring.NewRepeatedRowIteratorFromBytes(clear)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting cleariterator\")\n\t}\n\tsetIter, err := roaring.NewContainerIterator(set)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting set iterator\")\n\t}\n\n\trewriter, err := roaring.NewClearAndSetRewriter(clearIter, setIter)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting rewriter\")\n\t}\n\n\terr = tx.ApplyRewriter(f.index(), f.field(), f.view(), f.shard, 0, rewriter)\n\treturn errors.Wrap(err, \"pilosa.ImportRoaringSingleValued: \")\n}\n\nfunc (f *fragment) doImportRoaring(ctx context.Context, tx Tx, data []byte, clear bool) (map[uint64]int, bool, error) {\n\tf.mu.RLock()\n\tdefer f.mu.RUnlock()\n\trowSize := uint64(1 << shardVsContainerExponent)\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"importRoaring.ImportRoaringBits\")\n\tdefer span.Finish()\n\n\tvar rowSet map[uint64]int\n\terr := func() (err error) {\n\t\tvar rit roaring.RoaringIterator\n\t\trit, err = roaring.NewRoaringIterator(data)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t_, rowSet, err = tx.ImportRoaringBits(f.index(), f.field(), f.view(), f.shard, rit, clear, true, rowSize)\n\t\treturn err\n\t}()\n\tif err != nil {\n\t\treturn nil, false, err\n\t}\n\n\tupdateCache := f.CacheType != CacheTypeNone\n\treturn rowSet, updateCache, err\n}\n\nfunc (f *fragment) updateCachePostImport(ctx context.Context, rowSet map[uint64]int) error {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\tanyChanged := false\n\n\tfor rowID, changes := range rowSet {\n\t\tif changes == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tanyChanged = true\n\t\tif changes < 0 {\n\t\t\tabsChanges := uint64(-1 * changes)\n\t\t\tif absChanges <= f.cache.Get(rowID) {\n\t\t\t\tf.cache.BulkAdd(rowID, f.cache.Get(rowID)-absChanges)\n\t\t\t} else {\n\t\t\t\tf.cache.BulkAdd(rowID, 0)\n\t\t\t}\n\t\t} else {\n\t\t\tf.cache.BulkAdd(rowID, f.cache.Get(rowID)+uint64(changes))\n\t\t}\n\t}\n\t// we only set this if we need to update the cache\n\tif anyChanged {\n\t\tf.cache.Invalidate()\n\t}\n\n\treturn nil\n}\n\n// importRoaringOverwrite overwrites the specified block with the provided data.\nfunc (f *fragment) importRoaringOverwrite(ctx context.Context, tx Tx, data []byte, block int) error {\n\t// Clear the existing data from fragment block.\n\tif _, err := f.clearBlock(tx, block); err != nil {\n\t\treturn errors.Wrapf(err, \"clearing block: %d\", block)\n\t}\n\n\t// Union the new block data with the fragment data.\n\treturn f.importRoaring(ctx, tx, data, false)\n}\n\n// RecalculateCache rebuilds the cache regardless of invalidate time delay.\nfunc (f *fragment) RecalculateCache() {\n\tf.mu.Lock()\n\tf.cache.Recalculate()\n\tf.mu.Unlock()\n}\n\n// FlushCache writes the cache data to disk.\nfunc (f *fragment) FlushCache() error {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\treturn f.flushCache()\n}\n\nfunc (f *fragment) rebuildRankCache(ctx context.Context, tx Tx) error {\n\tif f.CacheType != CacheTypeRanked {\n\t\treturn nil // only rebuild ranked caches\n\t}\n\n\tf.cache.Clear()\n\trows, err := f.unprotectedRows(ctx, tx, uint64(0))\n\tif err != nil {\n\t\treturn err\n\t}\n\tfor _, id := range rows {\n\t\tn, err := tx.CountRange(f.index(), f.field(), f.view(), f.shard, id*ShardWidth, (id+1)*ShardWidth)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"CountRange\")\n\t\t}\n\t\tf.cache.BulkAdd(id, n)\n\t}\n\tf.cache.Invalidate()\n\treturn nil\n}\n\nfunc (f *fragment) RebuildRankCache(ctx context.Context) error {\n\tif f.CacheType != CacheTypeRanked {\n\t\treturn nil //only rebuild ranked caches\n\t}\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\ttx, err := f.holder.BeginTx(false, f.idx, f.shard)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer tx.Rollback()\n\treturn f.rebuildRankCache(ctx, tx)\n}\n\nfunc (f *fragment) flushCache() error {\n\tif f.cache == nil {\n\t\treturn nil\n\t}\n\n\tif f.CacheType == CacheTypeNone {\n\t\treturn nil\n\t}\n\n\t// Retrieve a list of row ids from the cache.\n\tids := f.cache.IDs()\n\n\t// Marshal cache data to bytes.\n\tbuf, err := proto.Marshal(&pb.Cache{IDs: ids})\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"marshalling\")\n\t}\n\n\tif err := os.MkdirAll(filepath.Dir(f.cachePath()), 0750); err != nil {\n\t\treturn errors.Wrap(err, \"mkdir\")\n\t}\n\t// Write to disk.\n\tif err := os.WriteFile(f.cachePath(), buf, 0600); err != nil {\n\t\treturn errors.Wrap(err, \"writing\")\n\t}\n\n\treturn nil\n}\n\n// WriteTo writes the fragment's data to w.\nfunc (f *fragment) WriteTo(w io.Writer) (n int64, err error) {\n\t// Force cache flush.\n\tif err := f.FlushCache(); err != nil {\n\t\treturn 0, errors.Wrap(err, \"flushing cache\")\n\t}\n\n\t// Write out data and cache to a tar archive.\n\ttw := tar.NewWriter(w)\n\tif err := f.writeStorageToArchive(tw); err != nil {\n\t\treturn 0, fmt.Errorf(\"write storage: %s\", err)\n\t}\n\tif err := f.writeCacheToArchive(tw); err != nil {\n\t\treturn 0, fmt.Errorf(\"write cache: %s\", err)\n\t}\n\treturn 0, nil\n}\n\n// used in shipping the slices across the network for a resize.\nfunc (f *fragment) writeStorageToArchive(tw *tar.Writer) error {\n\n\ttx := f.idx.holder.txf.NewTx(Txo{Write: !writable, Index: f.idx, Shard: f.shard})\n\tdefer tx.Rollback()\n\trbm, err := tx.RoaringBitmap(f.index(), f.field(), f.view(), f.shard)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"RoaringBitmapReader RoaringBitmap\")\n\t}\n\tvar buf bytes.Buffer\n\tsz, err := rbm.WriteTo(&buf)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"RoaringBitmapReader rbm.WriteTo(buf)\")\n\t}\n\n\t// Write archive header.\n\tif err := tw.WriteHeader(&tar.Header{\n\t\tName:    \"data\",\n\t\tMode:    0600,\n\t\tSize:    sz,\n\t\tModTime: time.Now(),\n\t}); err != nil {\n\t\treturn errors.Wrap(err, \"writing header\")\n\t}\n\n\t// Copy the file up to the last known size.\n\t// This is done outside the lock because the storage format is append-only.\n\tif _, err := io.CopyN(tw, &buf, sz); err != nil {\n\t\treturn errors.Wrap(err, \"copying\")\n\t}\n\treturn nil\n}\n\nfunc (f *fragment) writeCacheToArchive(tw *tar.Writer) error {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\n\t// Read cache into buffer.\n\tbuf, err := os.ReadFile(f.cachePath())\n\tif os.IsNotExist(err) {\n\t\treturn nil\n\t} else if err != nil {\n\t\treturn errors.Wrap(err, \"reading cache\")\n\t}\n\n\t// Write archive header.\n\tif err := tw.WriteHeader(&tar.Header{\n\t\tName:    \"cache\",\n\t\tMode:    0600,\n\t\tSize:    int64(len(buf)),\n\t\tModTime: time.Now(),\n\t}); err != nil {\n\t\treturn errors.Wrap(err, \"writing header\")\n\t}\n\n\t// Write data to archive.\n\tif _, err := tw.Write(buf); err != nil {\n\t\treturn errors.Wrap(err, \"writing\")\n\t}\n\treturn nil\n}\n\n// ReadFrom reads a data file from r and loads it into the fragment.\nfunc (f *fragment) ReadFrom(r io.Reader) (n int64, err error) {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\n\ttr := tar.NewReader(r)\n\tfor {\n\t\t// Read next tar header.\n\t\thdr, err := tr.Next()\n\t\tif err == io.EOF {\n\t\t\tbreak\n\t\t} else if err != nil {\n\t\t\treturn 0, errors.Wrap(err, \"opening\")\n\t\t}\n\n\t\t// Process file based on file name.\n\t\tswitch hdr.Name {\n\t\tcase \"data\":\n\t\t\ttx := f.holder.txf.NewTx(Txo{Write: writable, Index: f.idx, Fragment: f, Shard: f.shard})\n\t\t\tdefer tx.Rollback()\n\t\t\tif err := f.fillFragmentFromArchive(tx, tr); err != nil {\n\t\t\t\treturn 0, errors.Wrap(err, \"reading storage\")\n\t\t\t}\n\t\t\tif err := tx.Commit(); err != nil {\n\t\t\t\treturn 0, errors.Wrap(err, \"Commit after tx.ReadFragmentFromArchive\")\n\t\t\t}\n\t\tcase \"cache\":\n\t\t\tif err := f.readCacheFromArchive(tr); err != nil {\n\t\t\t\treturn 0, errors.Wrap(err, \"reading cache\")\n\t\t\t}\n\t\tdefault:\n\t\t\treturn 0, fmt.Errorf(\"invalid fragment archive file: %s\", hdr.Name)\n\t\t}\n\t}\n\n\treturn 0, nil\n}\n\n// should be morally equivalent to fragment.readStorageFromArchive()\n// below for RoaringTx, but also work on any Tx because it uses\n// tx.ImportRoaringBits().\nfunc (f *fragment) fillFragmentFromArchive(tx Tx, r io.Reader) error {\n\n\t// this is reading from inside a tarball, so definitely no need\n\t// to close it here.\n\tdata, err := io.ReadAll(r)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"fillFragmentFromArchive io.ReadAll(r)\")\n\t}\n\tif len(data) == 0 {\n\t\treturn nil\n\t}\n\n\t// For reference, compare to what fragment.go:313 fragment.importStorage() does.\n\n\tclear := false\n\tlog := false\n\trowSize := uint64(0)\n\titr, err := roaring.NewRoaringIterator(data)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"fillFragmentFromArchive NewRoaringIterator\")\n\t}\n\tchanged, rowSet, err := tx.ImportRoaringBits(f.index(), f.field(), f.view(), f.shard, itr, clear, log, rowSize)\n\t_, _ = changed, rowSet\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"fillFragmentFromArchive ImportRoaringBits\")\n\t}\n\treturn nil\n}\n\nfunc (f *fragment) readCacheFromArchive(r io.Reader) error {\n\t// Slurp data from reader and write to disk.\n\tbuf, err := io.ReadAll(r)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"reading\")\n\t} else if err := os.WriteFile(f.cachePath(), buf, 0600); err != nil {\n\t\treturn errors.Wrap(err, \"writing\")\n\t}\n\n\t// Re-open cache.\n\tif err := f.openCache(); err != nil {\n\t\treturn errors.Wrap(err, \"opening\")\n\t}\n\n\treturn nil\n}\n\nfunc (f *fragment) minRowID(tx Tx) (uint64, bool, error) {\n\tmin, ok, err := tx.Min(f.index(), f.field(), f.view(), f.shard)\n\treturn min / ShardWidth, ok, err\n}\n\n// rows returns all rows starting from 'start'. Filters will be applied in\n// order. All filters must return true to include the row. Once a row is\n// included, further containers in that row will be skipped. So, for a row to be\n// included, there must be one container in that row where all filters return\n// true. For a row to be skipped, at least one filter must return false for each\n// container in that row (it need not be the same filter for each). Any filter\n// returning done == true will cause processing to stop after all filters for\n// this container have been processed. The rows accumulated up to this point\n// (including this row if all filters passed) will be returned.\nfunc (f *fragment) rows(ctx context.Context, tx Tx, start uint64, filters ...roaring.BitmapFilter) ([]uint64, error) {\n\tf.mu.RLock()\n\tdefer f.mu.RUnlock()\n\treturn f.unprotectedRows(ctx, tx, start, filters...)\n}\n\n// unprotectedRows calls rows without grabbing the mutex.\nfunc (f *fragment) unprotectedRows(ctx context.Context, tx Tx, start uint64, filters ...roaring.BitmapFilter) ([]uint64, error) {\n\tvar rows []uint64\n\tcb := func(row uint64) error {\n\t\trows = append(rows, row)\n\t\treturn nil\n\t}\n\tstartKey := rowToKey(start)\n\tfilter := roaring.NewBitmapRowFilter(cb, filters...)\n\terr := tx.ApplyFilter(f.index(), f.field(), f.view(), f.shard, startKey, filter)\n\tif err != nil {\n\t\treturn nil, err\n\t} else {\n\t\treturn rows, nil\n\t}\n}\n\n// unionRows yields the union of the given rows in this fragment\nfunc (f *fragment) unionRows(ctx context.Context, tx Tx, rows []uint64) (*Row, error) {\n\tf.mu.RLock()\n\tdefer f.mu.RUnlock()\n\treturn f.unprotectedUnionRows(ctx, tx, rows)\n}\n\n// unprotectedRows calls rows without grabbing the mutex.\nfunc (f *fragment) unprotectedUnionRows(ctx context.Context, tx Tx, rows []uint64) (*Row, error) {\n\tfilter := roaring.NewBitmapRowsUnion(rows)\n\terr := tx.ApplyFilter(f.index(), f.field(), f.view(), f.shard, 0, filter)\n\tif err != nil {\n\t\treturn nil, err\n\t} else {\n\t\trow := &Row{\n\t\t\tSegments: []RowSegment{{\n\t\t\t\tdata:     filter.Results(f.shard),\n\t\t\t\tshard:    f.shard,\n\t\t\t\twritable: true,\n\t\t\t}},\n\t\t}\n\t\trow.invalidateCount()\n\t\treturn row, nil\n\t}\n}\n\ntype rowIterator interface {\n\t// TODO(kuba) linter suggests to use io.Seeker\n\t// Seek(offset int64, whence int) (int64, error)\n\tSeek(uint64)\n\n\tNext() (*Row, uint64, *int64, bool, error)\n}\n\nfunc (f *fragment) rowIterator(tx Tx, wrap bool, filters ...roaring.BitmapFilter) (rowIterator, error) {\n\tif strings.HasPrefix(f.view(), viewBSIGroupPrefix) {\n\t\treturn f.intRowIterator(tx, wrap, filters...)\n\t}\n\t// viewStandard\n\t// TODO(kuba) - IMHO we should check if f.view() is viewStandard,\n\t// but because of testing the function returns set iterator as default one.\n\treturn f.setRowIterator(tx, wrap, filters...)\n}\n\ntype timeRowIterator struct {\n\ttx               Tx\n\tcur              int\n\twrap             bool\n\tallRowIDs        []uint64\n\trowIDToFragments map[uint64][]*fragment\n}\n\nfunc timeFragmentsRowIterator(fragments []*fragment, tx Tx, wrap bool, filters ...roaring.BitmapFilter) (rowIterator, error) {\n\tif len(fragments) == 0 {\n\t\treturn nil, fmt.Errorf(\"there should be at least 1 fragment\")\n\t} else if len(fragments) == 1 {\n\t\treturn fragments[0].setRowIterator(tx, wrap, filters...)\n\t}\n\n\tit := &timeRowIterator{\n\t\ttx:   tx,\n\t\tcur:  0,\n\t\twrap: wrap,\n\t}\n\n\t// create a sort of inverted index that maps each\n\t// rowID back to the fragments that have that rowID\n\trowIDToFragments := make(map[uint64][]*fragment)\n\tfor _, f := range fragments {\n\t\trowIDs, err := f.rows(context.Background(), tx, 0, filters...)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfor _, rowID := range rowIDs {\n\t\t\tfs := append(rowIDToFragments[rowID], f)\n\t\t\trowIDToFragments[rowID] = fs\n\t\t}\n\t}\n\n\t// if len(rowIDToFragments) == 0 what to do ??\n\t// ie all fragments returned empty rowIDs, is this possible\n\t// is this an error\n\n\t// collect all rowIDs from inverted index to a slice\n\tallRowIDs := make([]uint64, len(rowIDToFragments))\n\ti := 0\n\tfor rowID := range rowIDToFragments {\n\t\tallRowIDs[i] = rowID\n\t\ti++\n\t}\n\tsort.Slice(allRowIDs, func(i, j int) bool { return allRowIDs[i] < allRowIDs[j] })\n\n\tit.rowIDToFragments = rowIDToFragments\n\tit.allRowIDs = allRowIDs\n\n\treturn it, nil\n}\n\nfunc (it *timeRowIterator) Seek(rowID uint64) {\n\tidx := sort.Search(len(it.allRowIDs), func(i int) bool {\n\t\treturn it.allRowIDs[i] >= rowID\n\t})\n\tit.cur = idx\n}\n\nfunc (it *timeRowIterator) Next() (r *Row, rowID uint64, _ *int64, wrapped bool, err error) {\n\tif it.cur >= len(it.allRowIDs) {\n\t\tif !it.wrap || len(it.allRowIDs) == 0 {\n\t\t\treturn nil, 0, nil, true, nil\n\t\t}\n\t\tit.Seek(0)\n\t\twrapped = true\n\t}\n\n\t// gather rows\n\trowID = it.allRowIDs[it.cur]\n\tfragments := it.rowIDToFragments[rowID]\n\trows := make([]*Row, 0, len(fragments))\n\tfor _, fragment := range fragments {\n\t\trow, err := fragment.row(it.tx, rowID)\n\t\tif err != nil {\n\t\t\treturn row, rowID, nil, wrapped, err\n\t\t}\n\t\trows = append(rows, row)\n\t}\n\n\t// union rows\n\tr = rows[0].Union(rows[1:]...)\n\n\tit.cur++\n\treturn r, rowID, nil, wrapped, nil\n}\n\ntype intRowIterator struct {\n\tf      *fragment\n\tvalues int64Slice         // sorted slice of int values\n\tcolIDs map[int64][]uint64 // [int value] -> [column IDs]\n\tcur    int                // current value index() (rowID)\n\twrap   bool\n}\n\nfunc (f *fragment) intRowIterator(tx Tx, wrap bool, filters ...roaring.BitmapFilter) (rowIterator, error) {\n\tit := intRowIterator{\n\t\tf:      f,\n\t\tcolIDs: make(map[int64][]uint64),\n\t\tcur:    0,\n\t\twrap:   wrap,\n\t}\n\n\t// accumulator [column ID] -> [int value]\n\tacc := make(map[uint64]int64)\n\n\tf.mu.RLock()\n\tdefer f.mu.RUnlock()\n\tcallback := func(rid uint64) error {\n\t\t// skip exist(0) and sign(1) rows\n\t\tif rid == bsiExistsBit || rid == bsiSignBit {\n\t\t\treturn nil\n\t\t}\n\n\t\tval := int64(1 << (rid - bsiOffsetBit))\n\t\tr, err := f.unprotectedRow(tx, rid)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfor _, cid := range r.Columns() {\n\t\t\tacc[cid] |= val\n\t\t}\n\t\treturn nil\n\t}\n\tif err := f.foreachRow(tx, filters, callback); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// apply exist and sign bits\n\tr0, err := f.unprotectedRow(tx, 0)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tallCols := r0.Columns()\n\n\tr1, err := f.unprotectedRow(tx, 1)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsignCols := r1.Columns()\n\tsignIdx, signLen := 0, len(signCols)\n\n\t// all distinct values\n\tvalues := make(map[int64]struct{})\n\tfor _, cid := range allCols {\n\t\t// apply sign bit\n\t\tif signIdx < signLen && cid == signCols[signIdx] {\n\t\t\tif tmp, ok := acc[cid]; ok {\n\t\t\t\tacc[cid] = -tmp\n\t\t\t}\n\n\t\t\tsignIdx++\n\t\t}\n\n\t\tval := acc[cid]\n\t\tit.colIDs[val] = append(it.colIDs[val], cid)\n\n\t\tif _, ok := values[val]; !ok {\n\t\t\tit.values = append(it.values, val)\n\t\t\tvalues[val] = struct{}{}\n\t\t}\n\t}\n\tsort.Sort(it.values)\n\n\treturn &it, nil\n}\n\nfunc (f *fragment) foreachRow(tx Tx, filters []roaring.BitmapFilter, fn func(rid uint64) error) error {\n\tfilter := roaring.NewBitmapRowFilter(fn, filters...)\n\terr := tx.ApplyFilter(f.index(), f.field(), f.view(), f.shard, 0, filter)\n\treturn errors.Wrap(err, \"pilosa.foreachRow: \")\n}\n\nfunc (it *intRowIterator) Seek(rowID uint64) {\n\tidx := sort.Search(len(it.values), func(i int) bool {\n\t\treturn it.values[i] >= it.values[rowID]\n\t})\n\tit.cur = idx\n}\n\nfunc (it *intRowIterator) Next() (r *Row, rowID uint64, value *int64, wrapped bool, err error) {\n\tif it.cur >= len(it.values) {\n\t\tif !it.wrap || len(it.values) == 0 {\n\t\t\treturn nil, 0, nil, true, nil\n\t\t}\n\t\twrapped = true\n\t\tit.cur = 0\n\t}\n\tif it.cur >= 0 {\n\t\trowID = uint64(it.cur)\n\t\tvalue = &it.values[rowID]\n\t\tr = NewRow(it.colIDs[*value]...)\n\t}\n\tit.cur++\n\treturn r, rowID, value, wrapped, nil\n}\n\ntype setRowIterator struct {\n\ttx     Tx\n\tf      *fragment\n\trowIDs []uint64\n\tcur    int\n\twrap   bool\n}\n\nfunc (f *fragment) setRowIterator(tx Tx, wrap bool, filters ...roaring.BitmapFilter) (rowIterator, error) {\n\trows, err := f.rows(context.Background(), tx, 0, filters...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &setRowIterator{\n\t\ttx:     tx,\n\t\tf:      f,\n\t\trowIDs: rows, // TODO: this may be memory intensive in high cardinality cases\n\t\twrap:   wrap,\n\t}, nil\n}\n\nfunc (it *setRowIterator) Seek(rowID uint64) {\n\tidx := sort.Search(len(it.rowIDs), func(i int) bool {\n\t\treturn it.rowIDs[i] >= rowID\n\t})\n\tit.cur = idx\n}\n\nfunc (it *setRowIterator) Next() (r *Row, rowID uint64, _ *int64, wrapped bool, err error) {\n\tif it.cur >= len(it.rowIDs) {\n\t\tif !it.wrap || len(it.rowIDs) == 0 {\n\t\t\treturn nil, 0, nil, true, nil\n\t\t}\n\t\tit.Seek(0)\n\t\twrapped = true\n\t}\n\tid := it.rowIDs[it.cur]\n\tr, err = it.f.row(it.tx, id)\n\tif err != nil {\n\t\treturn r, rowID, nil, wrapped, err\n\t}\n\n\trowID = id\n\n\tit.cur++\n\treturn r, rowID, nil, wrapped, nil\n}\n\n// pos returns the row position of a row/column pair.\nfunc pos(rowID, columnID uint64) uint64 {\n\treturn (rowID * ShardWidth) + (columnID % ShardWidth)\n}\n\n// vector stores the mapping of colID to rowID.\n// It's used for a mutex field type.\ntype vector interface {\n\tGet(tx Tx, colID uint64) (uint64, bool, error)\n}\n\n// rowsVector implements the vector interface by looking\n// at row data as needed.\ntype rowsVector struct {\n\tf *fragment\n}\n\n// newRowsVector returns a rowsVector for a given fragment.\nfunc newRowsVector(f *fragment) *rowsVector {\n\treturn &rowsVector{\n\t\tf: f,\n\t}\n}\n\n// Get returns the rowID associated to the given colID.\n// Additionally, it returns true if a value was found,\n// otherwise it returns false. Ensure that you already\n// have the mutex before calling this.\nfunc (v *rowsVector) Get(tx Tx, colID uint64) (uint64, bool, error) {\n\trows, err := v.f.unprotectedRows(context.Background(), tx, 0, roaring.NewBitmapColumnFilter(colID))\n\tif err != nil {\n\t\treturn 0, false, err\n\t} else if len(rows) > 1 {\n\t\treturn 0, false, errors.New(\"found multiple row values for column\")\n\t} else if len(rows) == 1 {\n\t\treturn rows[0], true, nil\n\t}\n\treturn 0, false, nil\n}\n\n// rowToKey converts a Pilosa row ID to the key of the container which starts\n// that row in the bitmap which represents this entire fragment. A fragment is\n// all the rows within a shard within a field concatenated together.\nfunc rowToKey(rowID uint64) (key uint64) {\n\treturn rowID * (ShardWidth / containerWidth)\n}\n\n// boolVector implements the vector interface by looking\n// at data in rows 0 and 1.\ntype boolVector struct {\n\tf *fragment\n}\n\n// newBoolVector returns a boolVector for a given fragment.\nfunc newBoolVector(f *fragment) *boolVector {\n\treturn &boolVector{\n\t\tf: f,\n\t}\n}\n\n// Get returns the rowID associated to the given colID.\n// Additionally, it returns true if a value was found,\n// otherwise it returns false. Ensure that you already\n// have the fragment mutex before calling this.\nfunc (v *boolVector) Get(tx Tx, colID uint64) (uint64, bool, error) {\n\trows, err := v.f.unprotectedRows(context.Background(), tx, 0, roaring.NewBitmapColumnFilter(colID))\n\tif err != nil {\n\t\treturn 0, false, err\n\t} else if len(rows) > 1 {\n\t\treturn 0, false, errors.New(\"found multiple row values for column\")\n\t} else if len(rows) == 1 {\n\t\tswitch rows[0] {\n\t\tcase falseRowID, trueRowID:\n\t\t\treturn rows[0], true, nil\n\t\tdefault:\n\t\t\treturn 0, false, errors.New(\"found non-boolean value\")\n\t\t}\n\t}\n\treturn 0, false, nil\n}\n\n// FormatQualifiedFragmentName generates a qualified name for the fragment to be used with Tx operations.\nfunc FormatQualifiedFragmentName(index, field, view string, shard uint64) string {\n\treturn fmt.Sprintf(\"%s\\x00%s\\x00%s\\x00%d\", index, field, view, shard)\n}\n\n// ParseQualifiedFragmentName parses a qualified name into its parts.\nfunc ParseQualifiedFragmentName(name string) (index, field, view string, shard uint64, err error) {\n\ta := strings.Split(name, \"\\x00\")\n\tif len(a) < 4 {\n\t\treturn \"\", \"\", \"\", 0, fmt.Errorf(\"invalid qualified name: %q\", name)\n\t}\n\tindex, field, view = string(a[0]), string(a[1]), string(a[2])\n\tif shard, err = strconv.ParseUint(a[3], 10, 64); err != nil {\n\t\treturn \"\", \"\", \"\", 0, fmt.Errorf(\"invalid qualified name: %q\", name)\n\t}\n\treturn index, field, view, shard, nil\n}\n\ntype RowKV struct {\n\tRowID uint64      `json:\"id\"`\n\tValue interface{} `json:\"value\"`\n}\n\nfunc (r *RowKV) Compare(o RowKV, desc bool) (bool, bool) {\n\tswitch val := r.Value.(type) {\n\tcase string:\n\t\tif oVal, ok := o.Value.(string); ok {\n\t\t\treturn desc != (val < oVal), true\n\t\t}\n\t\treturn desc, false\n\tcase bool:\n\t\tif oVal, ok := o.Value.(bool); ok {\n\t\t\treturn desc != oVal, true\n\t\t}\n\t\treturn desc, false\n\tcase int64:\n\t\tif oVal, ok := o.Value.(int64); ok {\n\t\t\treturn desc != (val < oVal), true\n\t\t}\n\t\treturn desc, false\n\tdefault:\n\t\treturn desc, false\n\t}\n}\n\n// sortBSIData, fetches the rows and seperates the positive and negetive values.\n// these values and sorted seperately and appended\nfunc (f *fragment) sortBsiData(tx Tx, filter *Row, bitDepth uint64, sort_desc bool) (*SortedRow, error) {\n\tconsider, err := f.row(tx, bsiExistsBit)\n\tif err != nil {\n\t\treturn nil, err\n\t} else if filter != nil {\n\t\tconsider = consider.Intersect(filter)\n\t}\n\trow, err := f.row(tx, bsiSignBit)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpos := consider.Difference(row)\n\tneg := consider.Difference(pos)\n\n\tvar sortedRowIds []RowKV\n\tf.flattenRowValues(tx, &sortedRowIds, neg, bitDepth, -1)\n\tok := true\n\tf.flattenRowValues(tx, &sortedRowIds, pos, bitDepth, 1)\n\tsort.SliceStable(sortedRowIds, func(i, j int) bool {\n\t\tif c, k := sortedRowIds[i].Compare(sortedRowIds[j], sort_desc); k {\n\t\t\treturn c\n\t\t} else {\n\t\t\tok = false\n\t\t\treturn !k\n\t\t}\n\t})\n\tif !ok {\n\t\treturn nil, errors.New(\"Couldn't compare field type for sorting\")\n\t}\n\n\treturn &SortedRow{\n\t\tRow:    consider,\n\t\tRowKVs: sortedRowIds,\n\t}, nil\n}\n\nfunc (f *fragment) flattenRowValues(tx Tx, sortedRowIds *[]RowKV, filter *Row, bitDepth uint64, sign int64) error {\n\tm := make(map[uint64]int64)\n\tfor i := int(bitDepth - 1); i >= 0; i-- {\n\t\trow, err := f.row(tx, uint64(bsiOffsetBit+i))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\trow = row.Intersect(filter)\n\n\t\tfor _, v := range row.Columns() {\n\t\t\tif val, ok := m[v]; ok {\n\t\t\t\tm[v] = val | (1 << i)\n\t\t\t} else {\n\t\t\t\tm[v] = (1 << i)\n\t\t\t}\n\t\t}\n\n\t}\n\n\tfor k, v := range m {\n\t\t*sortedRowIds = append(*sortedRowIds, RowKV{\n\t\t\tRowID: k,\n\t\t\tValue: (v * sign),\n\t\t})\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "fragment_internal_test.go",
          "type": "blob",
          "size": 131.7705078125,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"flag\"\n\t\"fmt\"\n\t\"io\"\n\t\"math/rand\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"sort\"\n\t\"strconv\"\n\t\"sync\"\n\t\"testing\"\n\t\"testing/quick\"\n\n\t\"github.com/davecgh/go-spew/spew\"\n\t\"github.com/featurebasedb/featurebase/v3/pql\"\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\t\"github.com/featurebasedb/featurebase/v3/testhook\"\n\t. \"github.com/featurebasedb/featurebase/v3/vprint\" // nolint:staticcheck\n\t\"github.com/pkg/errors\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\n// Test flags\nvar (\n\t// In order to generate the sample fragment file,\n\t// run an import and copy PILOSA_DATA_DIR/INDEX_NAME/FRAME_NAME/0 to testdata/sample_view\n\tFragmentPath = flag.String(\"fragment\", \"testdata/sample_view/0\", \"fragment path\")\n)\n\n// Ensure a fragment can set a bit and retrieve it.\nfunc TestFragment_SetBit(t *testing.T) {\n\tf, idx, tx := mustOpenFragment(t)\n\tdefer f.Clean(t)\n\n\t// Set bits on the fragment.\n\tif _, err := f.setBit(tx, 120, 1); err != nil {\n\t\tt.Fatal(err)\n\t} else if _, err := f.setBit(tx, 120, 6); err != nil {\n\t\tt.Fatal(err)\n\t} else if _, err := f.setBit(tx, 121, 0); err != nil {\n\t\tt.Fatal(err)\n\t}\n\t// should have two containers set in the fragment.\n\n\t// Verify counts on rows.\n\tif n := f.mustRow(tx, 120).Count(); n != 2 {\n\t\tt.Fatalf(\"unexpected count: %d\", n)\n\t} else if n := f.mustRow(tx, 121).Count(); n != 1 {\n\t\tt.Fatalf(\"unexpected count: %d\", n)\n\t}\n\n\t// commit the change, and verify it is still there\n\tPanicOn(tx.Commit())\n\n\t// Close and reopen the fragment & verify the data.\n\terr := f.Reopen()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\ttx = idx.holder.txf.NewTx(Txo{Write: writable, Index: idx, Fragment: f, Shard: f.shard})\n\tdefer tx.Rollback()\n\n\tif n := f.mustRow(tx, 120).Count(); n != 2 {\n\t\tt.Fatalf(\"unexpected count (reopen): %d\", n)\n\t} else if n := f.mustRow(tx, 121).Count(); n != 1 {\n\t\tt.Fatalf(\"unexpected count (reopen): %d\", n)\n\t}\n}\n\n// Ensure a fragment can clear a set bit.\nfunc TestFragment_ClearBit(t *testing.T) {\n\tf, idx, tx := mustOpenFragment(t)\n\t_ = idx\n\tdefer f.Clean(t)\n\n\t// Set and then clear bits on the fragment.\n\tif _, err := f.setBit(tx, 1000, 1); err != nil {\n\t\tt.Fatal(err)\n\t} else if _, err := f.setBit(tx, 1000, 2); err != nil {\n\t\tt.Fatal(err)\n\t} else if _, err := f.clearBit(tx, 1000, 1); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Verify count on row.\n\tif n := f.mustRow(tx, 1000).Count(); n != 1 {\n\t\tt.Fatalf(\"unexpected count: %d\", n)\n\t}\n\t// The Reopen below implies this test is looking at storage consistency.\n\t// In that spirit, we will check that the Tx Commit is visible afterwards.\n\tPanicOn(tx.Commit())\n\n\ttx = idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: f.shard})\n\tdefer tx.Rollback()\n\n\t// Close and reopen the fragment & verify the data.\n\tif err := f.Reopen(); err != nil {\n\t\tt.Fatal(err)\n\t} else if n := f.mustRow(tx, 1000).Count(); n != 1 {\n\t\tt.Fatalf(\"unexpected count (reopen): %d\", n)\n\t}\n}\n\n// Ensure a fragment can clear a row.\nfunc TestFragment_ClearRow(t *testing.T) {\n\tf, idx, tx := mustOpenFragment(t)\n\t_ = idx\n\tdefer f.Clean(t)\n\n\t// Set and then clear bits on the fragment.\n\tif _, err := f.setBit(tx, 1000, 1); err != nil {\n\t\tt.Fatal(err)\n\t} else if _, err := f.setBit(tx, 1000, 65536); err != nil {\n\t\tt.Fatal(err)\n\t} else if _, err := f.unprotectedClearRow(tx, 1000); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Verify count on row.\n\tif n := f.mustRow(tx, 1000).Count(); n != 0 {\n\t\tt.Fatalf(\"unexpected count: %d\", n)\n\t}\n\tPanicOn(tx.Commit())\n\ttx = idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: f.shard})\n\tdefer tx.Rollback()\n\n\t// Close and reopen the fragment & verify the data.\n\tif err := f.Reopen(); err != nil {\n\t\tt.Fatal(err)\n\t} else if n := f.mustRow(tx, 1000).Count(); n != 0 {\n\t\tt.Fatalf(\"unexpected count (reopen): %d\", n)\n\t}\n}\n\n// Ensure a fragment can set a row.\nfunc TestFragment_SetRow(t *testing.T) {\n\tf, idx, tx := mustOpenFragment(t)\n\t_ = idx\n\tdefer f.Clean(t)\n\n\t// Obtain transction.\n\n\trowID := uint64(1000)\n\n\t// Set bits on the fragment.\n\tif _, err := f.setBit(tx, rowID, 1); err != nil {\n\t\tt.Fatal(err)\n\t} else if _, err := f.setBit(tx, rowID, 65536); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Verify data on row.\n\tif cols := f.mustRow(tx, rowID).Columns(); !reflect.DeepEqual(cols, []uint64{1, 65536}) {\n\t\tt.Fatalf(\"unexpected columns: %+v\", cols)\n\t}\n\t// Verify count on row.\n\tif n := f.mustRow(tx, rowID).Count(); n != 2 {\n\t\tt.Fatalf(\"unexpected count: %d\", n)\n\t}\n\n\t// Set row (overwrite existing data).\n\trow := NewRow(1, 65537, 140000)\n\tif changed, err := f.unprotectedSetRow(tx, row, rowID); err != nil {\n\t\tt.Fatal(err)\n\t} else if !changed {\n\t\tt.Fatalf(\"expected changed value: %v\", changed)\n\t}\n\n\tPanicOn(tx.Commit())\n\ttx = idx.holder.txf.NewTx(Txo{Write: writable, Index: idx, Fragment: f, Shard: f.shard})\n\tdefer tx.Rollback()\n\n\t// Verify data on row.\n\tif cols := f.mustRow(tx, rowID).Columns(); !reflect.DeepEqual(cols, []uint64{1, 65537, 140000}) {\n\t\tt.Fatalf(\"unexpected columns after set row: %+v\", cols)\n\t}\n\t// Verify count on row.\n\tif n := f.mustRow(tx, rowID).Count(); n != 3 {\n\t\tt.Fatalf(\"unexpected count after set row: %d\", n)\n\t}\n\n\tPanicOn(tx.Commit())\n\ttx = idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: f.shard})\n\n\t// Close and reopen the fragment & verify the data.\n\tif err := f.Reopen(); err != nil {\n\t\tt.Fatal(err)\n\t} else if n := f.mustRow(tx, rowID).Count(); n != 3 {\n\t\tt.Fatalf(\"unexpected count (reopen): %d\", n)\n\t}\n\n\ttx.Rollback()\n\ttx = idx.holder.txf.NewTx(Txo{Write: writable, Index: idx, Fragment: f, Shard: f.shard})\n\tdefer tx.Rollback()\n\n\t// verify that setting something from a row which lacks a segment for\n\t// this fragment's shard still clears this fragment correctly.\n\tnotOurs := NewRow(8*ShardWidth + 1024)\n\tif changed, err := f.unprotectedSetRow(tx, notOurs, rowID); err != nil {\n\t\tt.Fatal(err)\n\t} else if !changed {\n\t\tt.Fatalf(\"setRow didn't report a change\")\n\t}\n\tif cols := f.mustRow(tx, rowID).Columns(); len(cols) != 0 {\n\t\tt.Fatalf(\"expected setting a row with no entries to clear the cache\")\n\t}\n\tPanicOn(tx.Commit())\n}\n\n// Ensure a fragment can set & read a value.\nfunc TestFragment_SetValue(t *testing.T) {\n\tt.Run(\"OK\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t)\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\n\t\t// Set value.\n\t\tif changed, err := f.setValue(tx, 100, 16, 3829); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !changed {\n\t\t\tt.Fatal(\"expected change\")\n\t\t}\n\n\t\t// Read value.\n\t\tif value, exists, err := f.value(tx, 100, 16); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if value != 3829 {\n\t\t\tt.Fatalf(\"unexpected value: %d\", value)\n\t\t} else if !exists {\n\t\t\tt.Fatal(\"expected to exist\")\n\t\t}\n\n\t\t// Setting value should return no change.\n\t\tif changed, err := f.setValue(tx, 100, 16, 3829); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if changed {\n\t\t\tt.Fatal(\"expected no change\")\n\t\t}\n\n\t\t// same after Commit\n\t\tif err := tx.Commit(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\ttx = idx.holder.txf.NewTx(Txo{Write: writable, Index: idx, Fragment: f, Shard: f.shard})\n\t\tdefer tx.Rollback()\n\n\t\t// Read value.\n\t\tif value, exists, err := f.value(tx, 100, 16); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if value != 3829 {\n\t\t\tt.Fatalf(\"unexpected value: %d\", value)\n\t\t} else if !exists {\n\t\t\tt.Fatal(\"expected to exist\")\n\t\t}\n\n\t\t// Setting value should return no change.\n\t\tif changed, err := f.setValue(tx, 100, 16, 3829); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if changed {\n\t\t\tt.Fatal(\"expected no change\")\n\t\t}\n\t})\n\n\tt.Run(\"Overwrite\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t)\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\n\t\t// Set value.\n\t\tif changed, err := f.setValue(tx, 100, 16, 3829); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !changed {\n\t\t\tt.Fatal(\"expected change\")\n\t\t}\n\n\t\t// Overwriting value should overwrite all bits.\n\t\tif changed, err := f.setValue(tx, 100, 16, 2028); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !changed {\n\t\t\tt.Fatal(\"expected change\")\n\t\t}\n\n\t\t// Read value.\n\t\tif value, exists, err := f.value(tx, 100, 16); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if value != 2028 {\n\t\t\tt.Fatalf(\"unexpected value: %d\", value)\n\t\t} else if !exists {\n\t\t\tt.Fatal(\"expected to exist\")\n\t\t}\n\n\t\t// Read value after commit.\n\t\tif err := tx.Commit(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\ttx = idx.holder.txf.NewTx(Txo{Write: writable, Index: idx, Fragment: f, Shard: f.shard})\n\t\tdefer tx.Rollback()\n\n\t\tif value, exists, err := f.value(tx, 100, 16); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if value != 2028 {\n\t\t\tt.Fatalf(\"unexpected value: %d\", value)\n\t\t} else if !exists {\n\t\t\tt.Fatal(\"expected to exist\")\n\t\t}\n\n\t})\n\n\tt.Run(\"Clear\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t)\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\n\t\t// Set value.\n\t\tif changed, err := f.setValue(tx, 100, 16, 3829); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !changed {\n\t\t\tt.Fatal(\"expected change\")\n\t\t}\n\n\t\t// Clear value should overwrite all bits, and set not-null to 0.\n\t\tif changed, err := f.clearValue(tx, 100, 16, 2028); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !changed {\n\t\t\tt.Fatal(\"expected change\")\n\t\t}\n\n\t\t// Read value.\n\t\tif value, exists, err := f.value(tx, 100, 16); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if value != 0 {\n\t\t\tt.Fatalf(\"unexpected value: %d\", value)\n\t\t} else if exists {\n\t\t\tt.Fatal(\"expected to not exist\")\n\t\t}\n\n\t\t// Same after Commit\n\t\tif err := tx.Commit(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\ttx = idx.holder.txf.NewTx(Txo{Write: writable, Index: idx, Fragment: f, Shard: f.shard})\n\t\tdefer tx.Rollback()\n\n\t\tif value, exists, err := f.value(tx, 100, 16); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if value != 0 {\n\t\t\tt.Fatalf(\"unexpected value: %d\", value)\n\t\t} else if exists {\n\t\t\tt.Fatal(\"expected to not exist\")\n\t\t}\n\t})\n\n\tt.Run(\"NotExists\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t)\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\t\tdefer tx.Rollback()\n\n\t\t// Set value.\n\t\tif changed, err := f.setValue(tx, 100, 10, 20); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !changed {\n\t\t\tt.Fatal(\"expected change\")\n\t\t}\n\n\t\t// Non-existent value.\n\t\tif value, exists, err := f.value(tx, 101, 11); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if value != 0 {\n\t\t\tt.Fatalf(\"unexpected value: %d\", value)\n\t\t} else if exists {\n\t\t\tt.Fatal(\"expected to not exist\")\n\t\t}\n\n\t})\n\n\tt.Run(\"QuickCheck\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t)\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\t\ttx.Rollback()\n\n\t\tif err := quick.Check(func(bitDepth uint64, bitN uint64, values []uint64) bool {\n\t\t\ttx = idx.holder.txf.NewTx(Txo{Write: true, Index: idx, Fragment: f, Shard: f.shard})\n\t\t\tdefer tx.Rollback()\n\t\t\t// Limit bit depth & maximum values.\n\t\t\tbitDepth = (bitDepth % 8) + 1\n\t\t\tbitN = (bitN % 99) + 1\n\n\t\t\tfor i := range values {\n\t\t\t\tvalues[i] = values[i] % (1 << bitDepth)\n\t\t\t}\n\n\t\t\t// Set values.\n\t\t\tm := make(map[uint64]int64)\n\t\t\tfor _, value := range values {\n\t\t\t\tcolumnID := value % bitN\n\n\t\t\t\tm[columnID] = int64(value)\n\n\t\t\t\tif _, err := f.setValue(tx, columnID, bitDepth, int64(value)); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Ensure values are set.\n\t\t\tfor columnID, value := range m {\n\t\t\t\tv, exists, err := f.value(tx, columnID, bitDepth)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t} else if value != int64(v) {\n\t\t\t\t\tt.Fatalf(\"value mismatch: columnID=%d, bitdepth=%d, value: %d != %d\", columnID, bitDepth, value, v)\n\t\t\t\t} else if !exists {\n\t\t\t\t\tt.Fatalf(\"value should exist: columnID=%d\", columnID)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Same after Commit\n\t\t\tif err := tx.Commit(); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\ttx = idx.holder.txf.NewTx(Txo{Write: false, Index: idx, Fragment: f, Shard: f.shard})\n\t\t\tdefer tx.Rollback()\n\n\t\t\t// Ensure values are set.\n\t\t\tfor columnID, value := range m {\n\t\t\t\tv, exists, err := f.value(tx, columnID, bitDepth)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t} else if value != int64(v) {\n\t\t\t\t\tt.Fatalf(\"value mismatch: columnID=%d, bitdepth=%d, value: %d != %d\", columnID, bitDepth, value, v)\n\t\t\t\t} else if !exists {\n\t\t\t\t\tt.Fatalf(\"value should exist: columnID=%d\", columnID)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn true\n\t\t}, nil); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t})\n}\n\n// Ensure a fragment can sum values.\nfunc TestFragment_Sum(t *testing.T) {\n\tconst bitDepth = 16\n\n\tf, idx, tx := mustOpenFragment(t)\n\tdefer f.Clean(t)\n\n\t// Set values.\n\tvals := []struct {\n\t\tcid uint64\n\t\tval int64\n\t}{\n\t\t{1000, 382},\n\t\t{2000, 300},\n\t\t{2500, -600},\n\t\t{3000, 2818},\n\t\t{4000, 300},\n\t}\n\tfor _, v := range vals {\n\t\tif _, err := f.setValue(tx, v.cid, bitDepth, v.val); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tPanicOn(tx.Commit())\n\ttx = idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: f.shard})\n\tdefer tx.Rollback()\n\n\tt.Run(\"NoFilter\", func(t *testing.T) {\n\t\tif sum, n, err := f.sum(tx, nil, bitDepth); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if n != 5 {\n\t\t\tt.Fatalf(\"unexpected count: %d\", n)\n\t\t} else if got, exp := sum, int64(382+300-600+2818+300); got != exp {\n\t\t\tt.Fatalf(\"unexpected sum: got: %d, exp: %d\", sum, exp)\n\t\t}\n\t})\n\n\tt.Run(\"WithFilter\", func(t *testing.T) {\n\t\tif sum, n, err := f.sum(tx, NewRow(2000, 4000, 5000), bitDepth); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if n != 2 {\n\t\t\tt.Fatalf(\"unexpected count: %d\", n)\n\t\t} else if got, exp := sum, int64(300+300); got != exp {\n\t\t\tt.Fatalf(\"unexpected sum: got: %d, exp: %d\", sum, exp)\n\t\t}\n\t})\n\n\ttx.Rollback()\n\ttx = idx.holder.txf.NewTx(Txo{Write: writable, Index: idx, Fragment: f, Shard: f.shard})\n\tdefer tx.Rollback()\n\n\t// verify that clearValue clears values\n\tif _, err := f.clearValue(tx, 1000, bitDepth, 23); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tPanicOn(tx.Commit())\n\ttx = idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: f.shard})\n\tdefer tx.Rollback()\n\n\tt.Run(\"ClearValue\", func(t *testing.T) {\n\t\tif sum, n, err := f.sum(tx, nil, bitDepth); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if n != 4 {\n\t\t\tt.Fatalf(\"unexpected count: %d\", n)\n\t\t} else if got, exp := sum, int64(3800-382-600); got != exp {\n\t\t\tt.Fatalf(\"unexpected sum: got: %d, exp: %d\", sum, exp)\n\t\t}\n\t})\n}\n\n// Ensure a fragment can find the min and max of values.\nfunc TestFragment_MinMax(t *testing.T) {\n\tconst bitDepth = 16\n\n\tf, idx, tx := mustOpenFragment(t)\n\tdefer f.Clean(t)\n\n\t// Set values.\n\tif _, err := f.setValue(tx, 1000, bitDepth, 382); err != nil {\n\t\tt.Fatal(err)\n\t} else if _, err := f.setValue(tx, 2000, bitDepth, 300); err != nil {\n\t\tt.Fatal(err)\n\t} else if _, err := f.setValue(tx, 3000, bitDepth, 2818); err != nil {\n\t\tt.Fatal(err)\n\t} else if _, err := f.setValue(tx, 4000, bitDepth, 300); err != nil {\n\t\tt.Fatal(err)\n\t} else if _, err := f.setValue(tx, 5000, bitDepth, 2818); err != nil {\n\t\tt.Fatal(err)\n\t} else if _, err := f.setValue(tx, 6000, bitDepth, 2817); err != nil {\n\t\tt.Fatal(err)\n\t} else if _, err := f.setValue(tx, 7000, bitDepth, 0); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tPanicOn(tx.Commit())\n\n\t// the new tx is shared by Min/Max below.\n\ttx = idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: f.shard})\n\tdefer tx.Rollback()\n\n\tt.Run(\"Min\", func(t *testing.T) {\n\t\ttests := []struct {\n\t\t\tfilter *Row\n\t\t\texp    int64\n\t\t\tcnt    uint64\n\t\t}{\n\t\t\t{filter: nil, exp: 0, cnt: 1},\n\t\t\t{filter: NewRow(2000, 4000, 5000), exp: 300, cnt: 2},\n\t\t\t{filter: NewRow(2000, 4000), exp: 300, cnt: 2},\n\t\t\t{filter: NewRow(1), exp: 0, cnt: 0},\n\t\t\t{filter: NewRow(1000), exp: 382, cnt: 1},\n\t\t\t{filter: NewRow(7000), exp: 0, cnt: 1},\n\t\t}\n\t\tfor i, test := range tests {\n\t\t\tif min, cnt, err := f.min(tx, test.filter, bitDepth); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if min != test.exp {\n\t\t\t\tt.Errorf(\"test %d expected min: %v, but got: %v\", i, test.exp, min)\n\t\t\t} else if cnt != test.cnt {\n\t\t\t\tt.Errorf(\"test %d expected cnt: %v, but got: %v\", i, test.cnt, cnt)\n\t\t\t}\n\t\t}\n\t})\n\n\tt.Run(\"Max\", func(t *testing.T) {\n\t\ttests := []struct {\n\t\t\tfilter *Row\n\t\t\texp    int64\n\t\t\tcnt    uint64\n\t\t}{\n\t\t\t{filter: nil, exp: 2818, cnt: 2},\n\t\t\t{filter: NewRow(2000, 4000, 5000), exp: 2818, cnt: 1},\n\t\t\t{filter: NewRow(2000, 4000), exp: 300, cnt: 2},\n\t\t\t{filter: NewRow(1), exp: 0, cnt: 0},\n\t\t\t{filter: NewRow(1000), exp: 382, cnt: 1},\n\t\t\t{filter: NewRow(7000), exp: 0, cnt: 1},\n\t\t}\n\t\tfor i, test := range tests {\n\t\t\tvar columns []uint64\n\t\t\tif test.filter != nil {\n\t\t\t\tcolumns = test.filter.Columns()\n\t\t\t}\n\n\t\t\tif max, cnt, err := f.max(tx, test.filter, bitDepth); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if max != test.exp || cnt != test.cnt {\n\t\t\t\tt.Errorf(\"%d. max(%v, %v)=(%v, %v), expected (%v, %v)\", i, columns, bitDepth, max, cnt, test.exp, test.cnt)\n\t\t\t}\n\t\t}\n\t})\n}\n\n// Ensure a fragment query for matching values.\nfunc TestFragment_Range(t *testing.T) {\n\tconst bitDepth = 16\n\n\tt.Run(\"EQ\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t)\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\n\t\t// Set values.\n\t\tif _, err := f.setValue(tx, 1000, bitDepth, 382); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 2000, bitDepth, 300); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 3000, bitDepth, 2818); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 4000, bitDepth, 300); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Query for equality.\n\t\tif b, err := f.rangeOp(tx, pql.EQ, bitDepth, 300); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(b.Columns(), []uint64{2000, 4000}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", b.Columns())\n\t\t}\n\t})\n\n\tt.Run(\"EQOversizeRegression\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t)\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\n\t\t// Set values.\n\t\tif _, err := f.setValue(tx, 1000, 1, 0); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 2000, 1, 1); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Query for equality.\n\t\tif b, err := f.rangeOp(tx, pql.EQ, 1, 3); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(b.Columns(), []uint64{}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", b.Columns())\n\t\t}\n\t\tif b, err := f.rangeOp(tx, pql.EQ, 1, 4); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(b.Columns(), []uint64{}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", b.Columns())\n\t\t}\n\t})\n\n\tt.Run(\"NEQ\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t)\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\n\t\t// Set values.\n\t\tif _, err := f.setValue(tx, 1000, bitDepth, 382); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 2000, bitDepth, 300); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 3000, bitDepth, 2818); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 4000, bitDepth, 300); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Query for inequality.\n\t\tif b, err := f.rangeOp(tx, pql.NEQ, bitDepth, 300); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(b.Columns(), []uint64{1000, 3000}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", b.Columns())\n\t\t}\n\t})\n\n\tt.Run(\"LT\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t)\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\n\t\t// Set values.\n\t\tif _, err := f.setValue(tx, 1000, bitDepth, 382); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 2000, bitDepth, 300); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 3000, bitDepth, 2817); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 4000, bitDepth, 301); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 5000, bitDepth, 1); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 6000, bitDepth, 0); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Query for values less than (ending with set column).\n\t\tif b, err := f.rangeOp(tx, pql.LT, bitDepth, 301); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(b.Columns(), []uint64{2000, 5000, 6000}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", b.Columns())\n\t\t}\n\n\t\t// Query for values less than (ending with unset column).\n\t\tif b, err := f.rangeOp(tx, pql.LT, bitDepth, 300); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(b.Columns(), []uint64{5000, 6000}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", b.Columns())\n\t\t}\n\n\t\t// Query for values less than or equal to (ending with set column).\n\t\tif b, err := f.rangeOp(tx, pql.LTE, bitDepth, 301); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(b.Columns(), []uint64{2000, 4000, 5000, 6000}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", b.Columns())\n\t\t}\n\n\t\t// Query for values less than or equal to (ending with unset column).\n\t\tif b, err := f.rangeOp(tx, pql.LTE, bitDepth, 300); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(b.Columns(), []uint64{2000, 5000, 6000}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", b.Columns())\n\t\t}\n\t})\n\n\tt.Run(\"LTRegression\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t)\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\n\t\tif _, err := f.setValue(tx, 1, 1, 1); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif b, err := f.rangeOp(tx, pql.LT, 1, 2); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(b.Columns(), []uint64{1}) {\n\t\t\tt.Fatalf(\"unepxected coulmns: %+v\", b.Columns())\n\t\t}\n\t})\n\n\tt.Run(\"LTMaxRegression\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t)\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\n\t\tif _, err := f.setValue(tx, 1, 2, 3); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 2, 2, 0); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif b, err := f.rangeLTUnsigned(tx, NewRow(1, 2), 2, 3, false); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(b.Columns(), []uint64{2}) {\n\t\t\tt.Fatalf(\"unepxected coulmns: %+v\", b.Columns())\n\t\t}\n\t})\n\n\tt.Run(\"GT\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t)\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\n\t\t// Set values.\n\t\tif _, err := f.setValue(tx, 1000, bitDepth, 382); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 2000, bitDepth, 300); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 3000, bitDepth, 2817); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 4000, bitDepth, 301); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 5000, bitDepth, 1); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 6000, bitDepth, 0); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Query for values greater than (ending with unset bit).\n\t\tif b, err := f.rangeOp(tx, pql.GT, bitDepth, 300); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(b.Columns(), []uint64{1000, 3000, 4000}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", b.Columns())\n\t\t}\n\n\t\t// Query for values greater than (ending with set bit).\n\t\tif b, err := f.rangeOp(tx, pql.GT, bitDepth, 301); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(b.Columns(), []uint64{1000, 3000}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", b.Columns())\n\t\t}\n\n\t\t// Query for values greater than or equal to (ending with unset bit).\n\t\tif b, err := f.rangeOp(tx, pql.GTE, bitDepth, 300); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(b.Columns(), []uint64{1000, 2000, 3000, 4000}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", b.Columns())\n\t\t}\n\n\t\t// Query for values greater than or equal to (ending with set bit).\n\t\tif b, err := f.rangeOp(tx, pql.GTE, bitDepth, 301); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(b.Columns(), []uint64{1000, 3000, 4000}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", b.Columns())\n\t\t}\n\t})\n\n\tt.Run(\"GTMinRegression\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t)\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\n\t\tif _, err := f.setValue(tx, 1, 2, 0); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 2, 2, 1); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif b, err := f.rangeGTUnsigned(tx, NewRow(1, 2), 2, 0, false); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(b.Columns(), []uint64{2}) {\n\t\t\tt.Fatalf(\"unepxected coulmns: %+v\", b.Columns())\n\t\t}\n\t})\n\n\tt.Run(\"GTOversizeRegression\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t)\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\n\t\tif _, err := f.setValue(tx, 1, 2, 0); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 2, 2, 1); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif b, err := f.rangeGTUnsigned(tx, NewRow(1, 2), 2, 4, false); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(b.Columns(), []uint64{}) {\n\t\t\tt.Fatalf(\"unepxected coulmns: %+v\", b.Columns())\n\t\t}\n\t})\n\n\tt.Run(\"BETWEEN\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t)\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\n\t\t// Set values.\n\t\tif _, err := f.setValue(tx, 1000, bitDepth, 382); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 2000, bitDepth, 300); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 3000, bitDepth, 2817); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 4000, bitDepth, 301); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 5000, bitDepth, 1); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 6000, bitDepth, 0); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Query for values greater than (ending with unset column).\n\t\tif b, err := f.rangeBetween(tx, bitDepth, 300, 2817); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(b.Columns(), []uint64{1000, 2000, 3000, 4000}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", b.Columns())\n\t\t}\n\n\t\t// Query for values greater than (ending with set column).\n\t\tif b, err := f.rangeBetween(tx, bitDepth, 301, 2817); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(b.Columns(), []uint64{1000, 3000, 4000}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", b.Columns())\n\t\t}\n\n\t\t// Query for values greater than or equal to (ending with unset column).\n\t\tif b, err := f.rangeBetween(tx, bitDepth, 301, 2816); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(b.Columns(), []uint64{1000, 4000}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", b.Columns())\n\t\t}\n\n\t\t// Query for values greater than or equal to (ending with set column).\n\t\tif b, err := f.rangeBetween(tx, bitDepth, 300, 2816); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(b.Columns(), []uint64{1000, 2000, 4000}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", b.Columns())\n\t\t}\n\t})\n\n\tt.Run(\"BetweenCommonBitsRegression\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t)\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\n\t\tif _, err := f.setValue(tx, 1, 64, 0xf0); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setValue(tx, 2, 64, 0xf1); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif b, err := f.rangeBetweenUnsigned(tx, NewRow(1, 2), 64, 0xf0, 0xf1); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(b.Columns(), []uint64{1, 2}) {\n\t\t\tt.Fatalf(\"unepxected coulmns: %+v\", b.Columns())\n\t\t}\n\t})\n}\n\n// benchmarkSetValues is a helper function to explore, very roughly, the cost\n// of setting values.\nfunc benchmarkSetValues(b *testing.B, tx Tx, bitDepth uint64, f *fragment, cfunc func(uint64) uint64) {\n\tcolumn := uint64(0)\n\tfor i := 0; i < b.N; i++ {\n\t\t// We're not checking the error because this is a benchmark.\n\t\t// That does mean the result could be completely wrong...\n\t\t_, _ = f.setValue(tx, column, bitDepth, int64(i))\n\t\tcolumn = cfunc(column)\n\t}\n}\n\n// Benchmark performance of setValue for BSI ranges.\nfunc BenchmarkFragment_SetValue(b *testing.B) {\n\tdepths := []uint64{4, 8, 16}\n\tfor _, bitDepth := range depths {\n\t\tname := fmt.Sprintf(\"Depth%d\", bitDepth)\n\t\tf, idx, tx := mustOpenFragment(b, OptFieldTypeSet(\"none\", 0))\n\t\t_ = idx\n\n\t\tb.Run(name+\"_Sparse\", func(b *testing.B) {\n\t\t\tbenchmarkSetValues(b, tx, bitDepth, f, func(u uint64) uint64 { return (u + 70000) & (ShardWidth - 1) })\n\t\t})\n\t\tf.Clean(b)\n\t\tf, idx, tx = mustOpenFragment(b, OptFieldTypeSet(\"none\", 0))\n\t\t_ = idx\n\t\tb.Run(name+\"_Dense\", func(b *testing.B) {\n\t\t\tbenchmarkSetValues(b, tx, bitDepth, f, func(u uint64) uint64 { return (u + 1) & (ShardWidth - 1) })\n\t\t})\n\t\tf.Clean(b)\n\t}\n}\n\n// makeBenchmarkImportValueData produces data that's supposed to be all within\n// the same shard; for fragment purposes, implicitly shard 0. This also gets\n// used by the field tests, but import requests are supposed to be per-shard,\n// so it's important that we generate values only within a given shard.\nfunc makeBenchmarkImportValueData(b *testing.B, bitDepth uint64, cfunc func(uint64) uint64) []ImportValueRequest {\n\tb.StopTimer()\n\tcolumn := uint64(0)\n\t// we don't average an alloc-per-bit, so we use a much larger N to get\n\t// meaningful data from -benchmem\n\tn := b.N * 10000\n\tbatches := make([]ImportValueRequest, 0, (n/ShardWidth)+1)\n\tmask := int64(1<<bitDepth) - 1\n\tprev := uint64(0)\n\tvar values []int64\n\tvar columns []uint64\n\tfor i := 0; i < n; i++ {\n\t\tvalues = append(values, int64(i)&mask)\n\t\tcolumns = append(columns, column)\n\t\tcolumn = cfunc(column)\n\t\tif column < prev {\n\t\t\treq := ImportValueRequest{ColumnIDs: columns, Values: values}\n\t\t\tcolumns = []uint64{}\n\t\t\tvalues = []int64{}\n\t\t\tbatches = append(batches, req)\n\t\t}\n\t\tprev = column\n\t}\n\tif len(columns) > 0 {\n\t\treq := ImportValueRequest{ColumnIDs: columns, Values: values}\n\t\tbatches = append(batches, req)\n\t}\n\tb.StartTimer()\n\treturn batches\n}\n\n// benchmarkImportValues is a helper function to explore, very roughly, the cost\n// of setting values using the special setter used for imports.\nfunc benchmarkImportValues(b *testing.B, tx Tx, bitDepth uint64, f *fragment, cfunc func(uint64) uint64) {\n\tbatches := makeBenchmarkImportValueData(b, bitDepth, cfunc)\n\tfor _, req := range batches {\n\t\terr := f.importValue(tx, req.ColumnIDs, req.Values, bitDepth, false)\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"error importing values: %s\", err)\n\t\t}\n\t}\n}\n\n// Benchmark performance of setValue for BSI ranges.\nfunc BenchmarkFragment_ImportValue(b *testing.B) {\n\tdepths := []uint64{4, 8, 16, 32}\n\tfor _, bitDepth := range depths {\n\t\tname := fmt.Sprintf(\"Depth%d\", bitDepth)\n\t\tf, idx, tx := mustOpenFragment(b)\n\t\t_ = idx\n\t\tb.Run(name+\"_Sparse\", func(b *testing.B) {\n\t\t\tbenchmarkImportValues(b, tx, bitDepth, f, func(u uint64) uint64 { return (u + 19) & (ShardWidth - 1) })\n\t\t})\n\t\tf.Clean(b)\n\t\tf, idx, tx = mustOpenFragment(b)\n\t\t_ = idx\n\t\tb.Run(name+\"_Dense\", func(b *testing.B) {\n\t\t\tbenchmarkImportValues(b, tx, bitDepth, f, func(u uint64) uint64 { return (u + 1) & (ShardWidth - 1) })\n\t\t})\n\t\tf.Clean(b)\n\t}\n}\n\n// BenchmarkFragment_RepeatedSmallImports tests the situation where updates are\n// constantly coming in across a large column space so that each fragment gets\n// only a few updates during each import.\n//\n// We test a variety of combinations of the number of separate updates(imports),\n// the number of bits in the import, the number of rows in the fragment (which\n// is a pretty good proxy for fragment size on disk).\nfunc BenchmarkFragment_RepeatedSmallImports(b *testing.B) {\n\tfor _, numUpdates := range []int{100} {\n\t\tfor _, bitsPerUpdate := range []int{100, 1000} {\n\t\t\tfor _, numRows := range []int{1000, 100000, 1000000} {\n\t\t\t\tb.Run(fmt.Sprintf(\"Rows%dUpdates%dBits%d\", numRows, numUpdates, bitsPerUpdate), func(b *testing.B) {\n\t\t\t\t\tfor a := 0; a < b.N; a++ {\n\t\t\t\t\t\tb.StopTimer()\n\t\t\t\t\t\t// build the update data set all at once - this will get applied\n\t\t\t\t\t\t// to a fragment in numUpdates batches\n\t\t\t\t\t\tupdateRows := make([]uint64, numUpdates*bitsPerUpdate)\n\t\t\t\t\t\tupdateCols := make([]uint64, numUpdates*bitsPerUpdate)\n\t\t\t\t\t\tfor i := 0; i < numUpdates*bitsPerUpdate; i++ {\n\t\t\t\t\t\t\tupdateRows[i] = uint64(rand.Int63n(int64(numRows))) // row id\n\t\t\t\t\t\t\tupdateCols[i] = uint64(rand.Int63n(ShardWidth))     // column id\n\t\t\t\t\t\t}\n\t\t\t\t\t\tf, idx, tx := mustOpenFragment(b)\n\t\t\t\t\t\t_ = idx\n\t\t\t\t\t\tdefer f.Clean(b)\n\n\t\t\t\t\t\terr := f.importRoaringT(tx, getZipfRowsSliceRoaring(uint64(numRows), 1, 0, ShardWidth), false)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\tb.Fatalf(\"importing base data for benchmark: %v\", err)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tb.StartTimer()\n\t\t\t\t\t\tfor i := 0; i < numUpdates; i++ {\n\t\t\t\t\t\t\terr := f.bulkImportStandard(tx,\n\t\t\t\t\t\t\t\tupdateRows[bitsPerUpdate*i:bitsPerUpdate*(i+1)],\n\t\t\t\t\t\t\t\tupdateRows[bitsPerUpdate*i:bitsPerUpdate*(i+1)],\n\t\t\t\t\t\t\t\t&ImportOptions{},\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\t\tb.Fatalf(\"doing small bulk import: %v\", err)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\ttx.Rollback() // don't exhaust the Tx space under b.N iterations.\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc BenchmarkFragment_RepeatedSmallImportsRoaring(b *testing.B) {\n\tfor _, numUpdates := range []int{100} {\n\t\tfor _, bitsPerUpdate := range []uint64{100, 1000} {\n\t\t\tfor _, numRows := range []uint64{1000, 100000, 1000000} {\n\t\t\t\tb.Run(fmt.Sprintf(\"Rows%dUpdates%dBits%d\", numRows, numUpdates, bitsPerUpdate), func(b *testing.B) {\n\t\t\t\t\tfor a := 0; a < b.N; a++ {\n\t\t\t\t\t\tb.StopTimer()\n\t\t\t\t\t\t// build the update data set all at once - this will get applied\n\t\t\t\t\t\t// to a fragment in numUpdates batches\n\t\t\t\t\t\tf, idx, tx := mustOpenFragment(b)\n\t\t\t\t\t\t_ = idx\n\t\t\t\t\t\tdefer f.Clean(b)\n\n\t\t\t\t\t\terr := f.importRoaringT(tx, getZipfRowsSliceRoaring(numRows, 1, 0, ShardWidth), false)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\tb.Fatalf(\"importing base data for benchmark: %v\", err)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfor i := 0; i < numUpdates; i++ {\n\t\t\t\t\t\t\tdata := getUpdataRoaring(numRows, bitsPerUpdate, int64(i))\n\t\t\t\t\t\t\tb.StartTimer()\n\t\t\t\t\t\t\terr := f.importRoaringT(tx, data, false)\n\t\t\t\t\t\t\tb.StopTimer()\n\t\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\t\tb.Fatalf(\"doing small roaring import: %v\", err)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc BenchmarkFragment_RepeatedSmallValueImports(b *testing.B) {\n\tinitialCols := make([]uint64, 0, ShardWidth)\n\tinitialVals := make([]int64, 0, ShardWidth)\n\tfor i := uint64(0); i < ShardWidth; i++ {\n\t\t// every 29 columns, skip between 0 and 12 columns\n\t\tif i%29 == 0 {\n\t\t\ti += i % 13\n\t\t}\n\t\tinitialCols = append(initialCols, i)\n\t\tinitialVals = append(initialVals, int64(rand.Int63n(1<<21)))\n\t}\n\n\tfor _, numUpdates := range []int{100} {\n\t\tfor _, valsPerUpdate := range []int{10, 100} {\n\t\t\tupdateCols := make([]uint64, numUpdates*valsPerUpdate)\n\t\t\tupdateVals := make([]int64, numUpdates*valsPerUpdate)\n\t\t\tfor i := 0; i < numUpdates*valsPerUpdate; i++ {\n\t\t\t\tupdateCols[i] = uint64(rand.Int63n(ShardWidth))\n\t\t\t\tupdateVals[i] = int64(rand.Int63n(1 << 21))\n\t\t\t}\n\n\t\t\tb.Run(fmt.Sprintf(\"Updates%dVals%d\", numUpdates, valsPerUpdate), func(b *testing.B) {\n\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\tb.StopTimer()\n\t\t\t\t\tf, _, tx := mustOpenFragment(b)\n\n\t\t\t\t\terr := f.importValue(tx, initialCols, initialVals, 21, false)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tb.Fatalf(\"initial value import: %v\", err)\n\t\t\t\t\t}\n\t\t\t\t\tb.StartTimer()\n\t\t\t\t\tfor j := 0; j < numUpdates; j++ {\n\t\t\t\t\t\terr := f.importValue(tx,\n\t\t\t\t\t\t\tupdateCols[valsPerUpdate*j:valsPerUpdate*(j+1)],\n\t\t\t\t\t\t\tupdateVals[valsPerUpdate*j:valsPerUpdate*(j+1)],\n\t\t\t\t\t\t\t21,\n\t\t\t\t\t\t\tfalse,\n\t\t\t\t\t\t)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\tb.Fatalf(\"importing values: %v\", err)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\ttx.Rollback() // don't exhaust the Tx over the b.N iterations.\n\t\t\t\t}\n\t\t\t})\n\t\t}\n\t}\n}\n\n// Ensure a fragment can return the top n results.\nfunc TestFragment_Top(t *testing.T) {\n\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeRanked, DefaultCacheSize))\n\t_ = idx\n\tdefer f.Clean(t)\n\n\t// Set bits on the rows 100, 101, & 102.\n\tf.mustSetBits(tx, 100, 1, 3, 200)\n\tf.mustSetBits(tx, 101, 1)\n\tf.mustSetBits(tx, 102, 1, 2)\n\tf.RecalculateCache()\n\n\t// Retrieve top rows.\n\tif pairs, err := f.top(tx, topOptions{N: 2}); err != nil {\n\t\tt.Fatal(err)\n\t} else if len(pairs) != 2 {\n\t\tt.Fatalf(\"unexpected count: %d\", len(pairs))\n\t} else if pairs[0] != (Pair{ID: 100, Count: 3}) {\n\t\tt.Fatalf(\"unexpected pair(0): %v\", pairs[0])\n\t} else if pairs[1] != (Pair{ID: 102, Count: 2}) {\n\t\tt.Fatalf(\"unexpected pair(1): %v\", pairs[1])\n\t}\n}\n\n// Ensure a fragment can return top rows that intersect with an input row.\nfunc TestFragment_TopN_Intersect(t *testing.T) {\n\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeRanked, DefaultCacheSize))\n\t_ = idx\n\tdefer f.Clean(t)\n\n\t// Create an intersecting input row.\n\tsrc := NewRow(1, 2, 3)\n\n\t// Set bits on various rows.\n\tf.mustSetBits(tx, 100, 1, 10, 11, 12)    // one intersection\n\tf.mustSetBits(tx, 101, 1, 2, 3, 4)       // three intersections\n\tf.mustSetBits(tx, 102, 1, 2, 4, 5, 6)    // two intersections\n\tf.mustSetBits(tx, 103, 1000, 1001, 1002) // no intersection\n\tf.RecalculateCache()\n\n\t// Retrieve top rows.\n\tif pairs, err := f.top(tx, topOptions{N: 3, Src: src}); err != nil {\n\t\tt.Fatal(err)\n\t} else if !reflect.DeepEqual(pairs, []Pair{\n\t\t{ID: 101, Count: 3},\n\t\t{ID: 102, Count: 2},\n\t\t{ID: 100, Count: 1},\n\t}) {\n\t\tt.Fatalf(\"unexpected pairs: %s\", spew.Sdump(pairs))\n\t}\n}\n\n// Ensure a fragment can return top rows that have many columns set.\nfunc TestFragment_TopN_Intersect_Large(t *testing.T) {\n\tif testing.Short() {\n\t\tt.Skip(\"short mode\")\n\t}\n\n\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeRanked, DefaultCacheSize))\n\t_ = idx\n\tdefer f.Clean(t)\n\n\t// Create an intersecting input row.\n\tsrc := NewRow(\n\t\t980, 981, 982, 983, 984, 985, 986, 987, 988, 989,\n\t\t990, 991, 992, 993, 994, 995, 996, 997, 998, 999,\n\t)\n\n\tbm := roaring.NewBTreeBitmap()\n\t// Set bits on rows 0 - 999. Higher rows have higher bit counts.\n\tfor i := uint64(0); i < 1000; i++ {\n\t\tfor j := uint64(0); j < i; j++ {\n\t\t\taddToBitmap(bm, i, j)\n\t\t}\n\t}\n\tb := &bytes.Buffer{}\n\t_, err := bm.WriteTo(b)\n\tif err != nil {\n\t\tt.Fatalf(\"writing to bytes: %v\", err)\n\t}\n\terr = f.importRoaringT(tx, b.Bytes(), false)\n\tif err != nil {\n\t\tt.Fatalf(\"importing data: %v\", err)\n\t}\n\tf.RecalculateCache()\n\n\t// Retrieve top rows.\n\tif pairs, err := f.top(tx, topOptions{N: 10, Src: src}); err != nil {\n\t\tt.Fatal(err)\n\t} else if !reflect.DeepEqual(pairs, []Pair{\n\t\t{ID: 999, Count: 19},\n\t\t{ID: 998, Count: 18},\n\t\t{ID: 997, Count: 17},\n\t\t{ID: 996, Count: 16},\n\t\t{ID: 995, Count: 15},\n\t\t{ID: 994, Count: 14},\n\t\t{ID: 993, Count: 13},\n\t\t{ID: 992, Count: 12},\n\t\t{ID: 991, Count: 11},\n\t\t{ID: 990, Count: 10},\n\t}) {\n\t\tt.Fatalf(\"unexpected pairs: %s\", spew.Sdump(pairs))\n\t}\n}\n\n// Ensure a fragment can return top rows when specified by ID.\nfunc TestFragment_TopN_IDs(t *testing.T) {\n\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeRanked, DefaultCacheSize))\n\t_ = idx\n\tdefer f.Clean(t)\n\n\t// Set bits on various rows.\n\tf.mustSetBits(tx, 100, 1, 2, 3)\n\tf.mustSetBits(tx, 101, 4, 5, 6, 7)\n\tf.mustSetBits(tx, 102, 8, 9, 10, 11, 12)\n\n\t// Retrieve top rows.\n\tif pairs, err := f.top(tx, topOptions{RowIDs: []uint64{100, 101, 200}}); err != nil {\n\t\tt.Fatal(err)\n\t} else if !reflect.DeepEqual(pairs, []Pair{\n\t\t{ID: 101, Count: 4},\n\t\t{ID: 100, Count: 3},\n\t}) {\n\t\tt.Fatalf(\"unexpected pairs: %s\", spew.Sdump(pairs))\n\t}\n}\n\n// Ensure a fragment return none if CacheTypeNone is set\nfunc TestFragment_TopN_NopCache(t *testing.T) {\n\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeNone, 0))\n\t_ = idx\n\tdefer f.Clean(t)\n\n\t// Set bits on various rows.\n\tf.mustSetBits(tx, 100, 1, 2, 3)\n\tf.mustSetBits(tx, 101, 4, 5, 6, 7)\n\tf.mustSetBits(tx, 102, 8, 9, 10, 11, 12)\n\n\t// Retrieve top rows.\n\tif pairs, err := f.top(tx, topOptions{RowIDs: []uint64{100, 101, 200}}); err != nil {\n\t\tt.Fatal(err)\n\t} else if !reflect.DeepEqual(pairs, []Pair{}) {\n\t\tt.Fatalf(\"unexpected pairs: %s\", spew.Sdump(pairs))\n\t}\n}\n\n// Ensure the fragment cache limit works\nfunc TestFragment_TopN_CacheSize(t *testing.T) {\n\tshard := uint64(0)\n\tcacheSize := uint32(3)\n\n\t// Create Index.\n\tindex := mustOpenIndex(t, IndexOptions{})\n\n\t// Create field.\n\tfield, err := index.CreateFieldIfNotExists(\"f\", \"\", OptFieldTypeSet(CacheTypeRanked, cacheSize))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Create view.\n\tview, err := field.createViewIfNotExists(viewStandard)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Create fragment.\n\tfrag, err := view.CreateFragmentIfNotExists(shard)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\t// Close the storage so we can re-open it without encountering a flock.\n\tfrag.Close()\n\n\tf := frag\n\tif err := f.Open(); err != nil {\n\t\tPanicOn(err)\n\t}\n\tdefer f.Clean(t)\n\n\t// Obtain transaction.\n\ttx := index.holder.txf.NewTx(Txo{Write: writable, Index: index, Fragment: f, Shard: f.shard})\n\tdefer tx.Rollback()\n\n\t// Set bits on various rows.\n\tf.mustSetBits(tx, 100, 1, 2, 3)\n\tf.mustSetBits(tx, 101, 4, 5, 6, 7)\n\tf.mustSetBits(tx, 102, 8, 9, 10, 11, 12)\n\tf.mustSetBits(tx, 103, 8, 9, 10, 11, 12, 13)\n\tf.mustSetBits(tx, 104, 8, 9, 10, 11, 12, 13, 14)\n\tf.mustSetBits(tx, 105, 10, 11)\n\n\tf.RecalculateCache()\n\n\tp := []Pair{\n\t\t{ID: 104, Count: 7},\n\t\t{ID: 103, Count: 6},\n\t\t{ID: 102, Count: 5},\n\t}\n\n\t// Retrieve top rows.\n\tif pairs, err := f.top(tx, topOptions{N: 5}); err != nil {\n\t\tt.Fatal(err)\n\t} else if len(pairs) > int(cacheSize) {\n\t\tt.Fatalf(\"TopN count cannot exceed cache size: %d\", cacheSize)\n\t} else if pairs[0] != (Pair{ID: 104, Count: 7}) {\n\t\tt.Fatalf(\"unexpected pair(0): %v\", pairs)\n\t} else if !reflect.DeepEqual(pairs, p) {\n\t\tt.Fatalf(\"Invalid TopN result set: %s\", spew.Sdump(pairs))\n\t}\n}\n\n// Ensure a fragment's cache can be persisted between restarts.\nfunc TestFragment_LRUCache_Persistence(t *testing.T) {\n\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeLRU, 0))\n\t_ = idx\n\tdefer f.Clean(t)\n\n\t// Set bits on the fragment.\n\tfor i := uint64(0); i < 1000; i++ {\n\t\tif _, err := f.setBit(tx, i, 0); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\t// Verify correct cache type and size.\n\tif cache, ok := f.cache.(*lruCache); !ok {\n\t\tt.Fatalf(\"unexpected cache: %T\", f.cache)\n\t} else if cache.Len() != 1000 {\n\t\tt.Fatalf(\"unexpected cache len: %d\", cache.Len())\n\t}\n\n\tPanicOn(tx.Commit())\n\n\t// Reopen the fragment.\n\tif err := f.Reopen(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Re-verify correct cache type and size.\n\tif cache, ok := f.cache.(*lruCache); !ok {\n\t\tt.Fatalf(\"unexpected cache: %T\", f.cache)\n\t} else if cache.Len() != 1000 {\n\t\tt.Fatalf(\"unexpected cache len: %d\", cache.Len())\n\t}\n}\n\n// Ensure a fragment can be copied to another fragment.\nfunc TestFragment_WriteTo_ReadFrom(t *testing.T) {\n\tf0, _, tx := mustOpenFragment(t)\n\tdefer f0.Clean(t)\n\n\t// Set and then clear bits on the fragment.\n\tif _, err := f0.setBit(tx, 1000, 1); err != nil {\n\t\tt.Fatal(err)\n\t} else if _, err := f0.setBit(tx, 1000, 2); err != nil {\n\t\tt.Fatal(err)\n\t} else if _, err := f0.clearBit(tx, 1000, 1); err != nil {\n\t\tt.Fatal(err)\n\t}\n\terr := tx.Commit()\n\tif err != nil {\n\t\tt.Fatalf(\"committing write: %v\", err)\n\t}\n\n\t// Verify cache is populated.\n\tif n := f0.cache.Len(); n != 1 {\n\t\tt.Fatalf(\"unexpected cache size: %d\", n)\n\t}\n\n\t// Write fragment to a buffer.\n\tvar buf bytes.Buffer\n\twn, err := f0.WriteTo(&buf)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Read into another fragment.\n\tf1, idx, tx := mustOpenFragment(t)\n\ttx.Rollback()\n\n\tdefer f1.Clean(t)\n\n\tif rn, err := f1.ReadFrom(&buf); err != nil { // eventually calls fragment.fillFragmentFromArchive\n\t\tt.Fatal(err)\n\t} else if wn != rn {\n\t\tt.Fatalf(\"read/write byte count mismatch: wn=%d, rn=%d\", wn, rn)\n\t}\n\t// make a read-only Tx after ReadFrom has committed.\n\ttx = idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f1, Shard: f1.shard})\n\tdefer tx.Rollback()\n\n\t// Verify cache is in other fragment.\n\tif n := f1.cache.Len(); n != 1 {\n\t\tt.Fatalf(\"unexpected cache size: %d\", n)\n\t}\n\n\t// Verify data in other fragment.\n\tif a := f1.mustRow(tx, 1000).Columns(); !reflect.DeepEqual(a, []uint64{2}) {\n\t\tt.Fatalf(\"unexpected columns: %+v\", a)\n\t}\n\n\t// Close and reopen the fragment & verify the data.\n\tif err := f1.Reopen(); err != nil {\n\t\tt.Fatal(err)\n\t} else if n := f1.cache.Len(); n != 1 {\n\t\tt.Fatalf(\"unexpected cache size (reopen): %d\", n)\n\t} else if a := f1.mustRow(tx, 1000).Columns(); !reflect.DeepEqual(a, []uint64{2}) {\n\t\tt.Fatalf(\"unexpected columns (reopen): %+v\", a)\n\t}\n}\n\nfunc BenchmarkFragment_IntersectionCount(b *testing.B) {\n\tf, idx, tx := mustOpenFragment(b)\n\tdefer f.Clean(b)\n\n\t// Generate some intersecting data.\n\tfor i := 0; i < 10000; i += 2 {\n\t\tif _, err := f.setBit(tx, 1, uint64(i)); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n\tfor i := 0; i < 10000; i += 3 {\n\t\tif _, err := f.setBit(tx, 2, uint64(i)); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n\n\tPanicOn(tx.Commit())\n\ttx = idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: f.shard})\n\tdefer tx.Rollback()\n\n\t// Start benchmark\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tif n := f.mustRow(tx, 1).intersectionCount(f.mustRow(tx, 2)); n == 0 {\n\t\t\tb.Fatalf(\"unexpected count: %d\", n)\n\t\t}\n\t}\n}\n\nfunc TestFragment_Tanimoto(t *testing.T) {\n\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeRanked, DefaultCacheSize))\n\t_ = idx\n\tdefer f.Clean(t)\n\n\tsrc := NewRow(1, 2, 3)\n\n\t// Set bits on the rows 100, 101, & 102.\n\tf.mustSetBits(tx, 100, 1, 3, 2, 200)\n\tf.mustSetBits(tx, 101, 1, 3)\n\tf.mustSetBits(tx, 102, 1, 2, 10, 12)\n\tf.RecalculateCache()\n\n\tif pairs, err := f.top(tx, topOptions{TanimotoThreshold: 50, Src: src}); err != nil {\n\t\tt.Fatal(err)\n\t} else if len(pairs) != 2 {\n\t\tt.Fatalf(\"unexpected count: %d\", len(pairs))\n\t} else if pairs[0] != (Pair{ID: 100, Count: 3}) {\n\t\tt.Fatalf(\"unexpected pair(0): %v\", pairs[0])\n\t} else if pairs[1] != (Pair{ID: 101, Count: 2}) {\n\t\tt.Fatalf(\"unexpected pair(1): %v\", pairs[1])\n\t}\n}\n\nfunc TestFragment_Zero_Tanimoto(t *testing.T) {\n\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeRanked, DefaultCacheSize))\n\t_ = idx\n\tdefer f.Clean(t)\n\n\tsrc := NewRow(1, 2, 3)\n\n\t// Set bits on the rows 100, 101, & 102.\n\tf.mustSetBits(tx, 100, 1, 3, 2, 200)\n\tf.mustSetBits(tx, 101, 1, 3)\n\tf.mustSetBits(tx, 102, 1, 2, 10, 12)\n\tf.RecalculateCache()\n\n\tif pairs, err := f.top(tx, topOptions{TanimotoThreshold: 0, Src: src}); err != nil {\n\t\tt.Fatal(err)\n\t} else if len(pairs) != 3 {\n\t\tt.Fatalf(\"unexpected count: %d\", len(pairs))\n\t} else if pairs[0] != (Pair{ID: 100, Count: 3}) {\n\t\tt.Fatalf(\"unexpected pair(0): %v\", pairs[0])\n\t} else if pairs[1] != (Pair{ID: 101, Count: 2}) {\n\t\tt.Fatalf(\"unexpected pair(1): %v\", pairs[1])\n\t} else if pairs[2] != (Pair{ID: 102, Count: 2}) {\n\t\tt.Fatalf(\"unexpected pair(1): %v\", pairs[2])\n\t}\n}\n\n// Ensure a fragment can set mutually exclusive values.\nfunc TestFragment_SetMutex(t *testing.T) {\n\tf, _, tx := mustOpenFragment(t, OptFieldTypeMutex(DefaultCacheType, DefaultCacheSize))\n\tdefer f.Clean(t)\n\n\tvar cols []uint64\n\n\t// Set a value on column 100.\n\tif _, err := f.setBit(tx, 1, 100); err != nil {\n\t\tt.Fatal(err)\n\t}\n\t// Verify the value was set.\n\tcols = f.mustRow(tx, 1).Columns()\n\tif !reflect.DeepEqual(cols, []uint64{100}) {\n\t\tt.Fatalf(\"mutex unexpected columns: %v\", cols)\n\t}\n\n\t// Set a different value on column 100.\n\tif _, err := f.setBit(tx, 2, 100); err != nil {\n\t\tt.Fatal(err)\n\t}\n\t// Verify that value (row 1) was replaced (by row 2).\n\tcols = f.mustRow(tx, 1).Columns()\n\tif !reflect.DeepEqual(cols, []uint64{}) {\n\t\tt.Fatalf(\"mutex unexpected columns: %v\", cols)\n\t}\n\tcols = f.mustRow(tx, 2).Columns()\n\tif !reflect.DeepEqual(cols, []uint64{100}) {\n\t\tt.Fatalf(\"mutex unexpected columns: %v\", cols)\n\t}\n}\n\n// Ensure a fragment can import into set fields.\nfunc TestFragment_ImportSet(t *testing.T) {\n\ttests := []struct {\n\t\tsetRowIDs   []uint64\n\t\tsetColIDs   []uint64\n\t\tsetExp      map[uint64][]uint64\n\t\tclearRowIDs []uint64\n\t\tclearColIDs []uint64\n\t\tclearExp    map[uint64][]uint64\n\t}{\n\t\t{\n\t\t\t[]uint64{1, 1, 1, 1},\n\t\t\t[]uint64{0, 1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t},\n\t\t\t[]uint64{},\n\t\t\t[]uint64{},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{1, 1, 1, 1, 2, 2, 2, 2},\n\t\t\t[]uint64{0, 1, 2, 3, 0, 1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t\t2: {0, 1, 2, 3},\n\t\t\t},\n\t\t\t[]uint64{1, 1, 2},\n\t\t\t[]uint64{1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 3},\n\t\t\t\t2: {0, 1, 2},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{1, 1, 1, 1, 2},\n\t\t\t[]uint64{0, 1, 2, 3, 1},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t\t2: {1},\n\t\t\t},\n\t\t\t[]uint64{1, 1, 1, 1},\n\t\t\t[]uint64{0, 1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {},\n\t\t\t\t2: {1},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{1, 1, 1, 1, 2, 2, 1},\n\t\t\t[]uint64{0, 1, 2, 3, 1, 8, 1},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t\t2: {1, 8},\n\t\t\t},\n\t\t\t[]uint64{1, 1},\n\t\t\t[]uint64{0, 0},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {1, 2, 3},\n\t\t\t\t2: {1, 8},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{1, 2, 3},\n\t\t\t[]uint64{8, 8, 8},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {8},\n\t\t\t\t2: {8},\n\t\t\t\t3: {8},\n\t\t\t},\n\t\t\t[]uint64{1, 2, 3},\n\t\t\t[]uint64{9, 9, 9},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {8},\n\t\t\t\t2: {8},\n\t\t\t\t3: {8},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor i, test := range tests {\n\t\tt.Run(fmt.Sprintf(\"importset%d\", i), func(t *testing.T) {\n\t\t\tf, idx, tx := mustOpenFragment(t)\n\t\t\t_ = idx\n\t\t\tdefer f.Clean(t)\n\n\t\t\t// Set import.\n\t\t\terr := f.bulkImport(tx, test.setRowIDs, test.setColIDs, &ImportOptions{})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"bulk importing ids: %v\", err)\n\t\t\t}\n\n\t\t\t// Check for expected results.\n\t\t\tfor k, v := range test.setExp {\n\t\t\t\tcols := f.mustRow(tx, k).Columns()\n\t\t\t\tif !reflect.DeepEqual(cols, v) {\n\t\t\t\t\tt.Fatalf(\"expected: %v, but got: %v\", v, cols)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Clear import.\n\t\t\terr = f.bulkImport(tx, test.clearRowIDs, test.clearColIDs, &ImportOptions{Clear: true})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"bulk clearing ids: %v\", err)\n\t\t\t}\n\n\t\t\t// Check for expected results.\n\t\t\tfor k, v := range test.clearExp {\n\t\t\t\tcols := f.mustRow(tx, k).Columns()\n\t\t\t\tif !reflect.DeepEqual(cols, v) {\n\t\t\t\t\tt.Fatalf(\"expected: %v, but got: %v\", v, cols)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestFragment_ImportSet_WithTxCommit(t *testing.T) {\n\ttests := []struct {\n\t\tsetRowIDs   []uint64\n\t\tsetColIDs   []uint64\n\t\tsetExp      map[uint64][]uint64\n\t\tclearRowIDs []uint64\n\t\tclearColIDs []uint64\n\t\tclearExp    map[uint64][]uint64\n\t}{\n\t\t{\n\t\t\t[]uint64{1, 1, 1, 1},\n\t\t\t[]uint64{0, 1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t},\n\t\t\t[]uint64{},\n\t\t\t[]uint64{},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{1, 1, 1, 1, 2, 2, 2, 2},\n\t\t\t[]uint64{0, 1, 2, 3, 0, 1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t\t2: {0, 1, 2, 3},\n\t\t\t},\n\t\t\t[]uint64{1, 1, 2},\n\t\t\t[]uint64{1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 3},\n\t\t\t\t2: {0, 1, 2},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{1, 1, 1, 1, 2},\n\t\t\t[]uint64{0, 1, 2, 3, 1},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t\t2: {1},\n\t\t\t},\n\t\t\t[]uint64{1, 1, 1, 1},\n\t\t\t[]uint64{0, 1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {},\n\t\t\t\t2: {1},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{1, 1, 1, 1, 2, 2, 1},\n\t\t\t[]uint64{0, 1, 2, 3, 1, 8, 1},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t\t2: {1, 8},\n\t\t\t},\n\t\t\t[]uint64{1, 1},\n\t\t\t[]uint64{0, 0},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {1, 2, 3},\n\t\t\t\t2: {1, 8},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{1, 2, 3},\n\t\t\t[]uint64{8, 8, 8},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {8},\n\t\t\t\t2: {8},\n\t\t\t\t3: {8},\n\t\t\t},\n\t\t\t[]uint64{1, 2, 3},\n\t\t\t[]uint64{9, 9, 9},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {8},\n\t\t\t\t2: {8},\n\t\t\t\t3: {8},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor i, test := range tests {\n\t\tt.Run(fmt.Sprintf(\"importset%d\", i), func(t *testing.T) {\n\t\t\tf, idx, tx := mustOpenFragment(t)\n\t\t\t_ = idx\n\t\t\tdefer f.Clean(t)\n\n\t\t\t// Set import.\n\t\t\terr := f.bulkImport(tx, test.setRowIDs, test.setColIDs, &ImportOptions{})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"bulk importing ids: %v\", err)\n\t\t\t}\n\n\t\t\tPanicOn(tx.Commit())\n\t\t\ttx = idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: f.shard})\n\t\t\tdefer tx.Rollback()\n\n\t\t\t// Check for expected results.\n\t\t\tfor k, v := range test.setExp {\n\t\t\t\tcols := f.mustRow(tx, k).Columns()\n\t\t\t\tif !reflect.DeepEqual(cols, v) {\n\t\t\t\t\tt.Fatalf(\"expected: %v, but got: %v\", v, cols)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\ttx.Rollback()\n\t\t\ttx = idx.holder.txf.NewTx(Txo{Write: writable, Index: idx, Fragment: f, Shard: f.shard})\n\t\t\tdefer tx.Rollback()\n\n\t\t\t// Clear import.\n\t\t\terr = f.bulkImport(tx, test.clearRowIDs, test.clearColIDs, &ImportOptions{Clear: true})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"bulk clearing ids: %v\", err)\n\t\t\t}\n\n\t\t\tPanicOn(tx.Commit())\n\t\t\ttx = idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: f.shard})\n\t\t\tdefer tx.Rollback()\n\n\t\t\t// Check for expected results.\n\t\t\tfor k, v := range test.clearExp {\n\t\t\t\tcols := f.mustRow(tx, k).Columns()\n\t\t\t\tif !reflect.DeepEqual(cols, v) {\n\t\t\t\t\tt.Fatalf(\"expected: %v, but got: %v\", v, cols)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestFragment_ConcurrentImport(t *testing.T) {\n\tt.Run(\"bulkImportStandard\", func(t *testing.T) {\n\t\tshard := uint64(0)\n\t\tf, idx, tx := mustOpenFragment(t)\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\t\t// note: write Tx must be used on the same goroutine that created them.\n\t\t// So we close out the \"default\" Tx created by mustOpenFragment, and\n\t\t// have the goroutines below each make their own. One should get the\n\t\t// write lock first, and thus they should get serialized.\n\t\ttx.Rollback()\n\n\t\teg := errgroup.Group{}\n\t\teg.Go(func() error {\n\t\t\ttx := idx.holder.txf.NewTx(Txo{Write: writable, Index: idx, Fragment: f, Shard: shard})\n\t\t\tdefer func() { PanicOn(tx.Commit()) }()\n\t\t\treturn f.bulkImportStandard(tx, []uint64{1, 2}, []uint64{1, 2}, &ImportOptions{})\n\t\t})\n\t\teg.Go(func() error {\n\t\t\ttx := idx.holder.txf.NewTx(Txo{Write: writable, Index: idx, Fragment: f, Shard: shard})\n\t\t\tdefer func() { PanicOn(tx.Commit()) }()\n\t\t\treturn f.bulkImportStandard(tx, []uint64{3, 4}, []uint64{3, 4}, &ImportOptions{})\n\t\t})\n\t\terr := eg.Wait()\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"importing data to fragment: %v\", err)\n\t\t}\n\t})\n}\n\n// Ensure a fragment can import mutually exclusive values.\nfunc TestFragment_ImportMutex(t *testing.T) {\n\ttests := []struct {\n\t\tsetRowIDs   []uint64\n\t\tsetColIDs   []uint64\n\t\tsetExp      map[uint64][]uint64\n\t\tclearRowIDs []uint64\n\t\tclearColIDs []uint64\n\t\tclearExp    map[uint64][]uint64\n\t}{\n\t\t{\n\t\t\t[]uint64{1, 1, 1, 1},\n\t\t\t[]uint64{0, 1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t},\n\t\t\t[]uint64{},\n\t\t\t[]uint64{},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{1, 1, 1, 1, 2, 2, 2, 2},\n\t\t\t[]uint64{0, 1, 2, 3, 0, 1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {},\n\t\t\t\t2: {0, 1, 2, 3},\n\t\t\t},\n\t\t\t[]uint64{1, 1, 2},\n\t\t\t[]uint64{1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {},\n\t\t\t\t2: {0, 1, 2},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{1, 1, 1, 1, 2},\n\t\t\t[]uint64{0, 1, 2, 3, 1},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 2, 3},\n\t\t\t\t2: {1},\n\t\t\t},\n\t\t\t[]uint64{1, 1, 1, 1},\n\t\t\t[]uint64{0, 1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {},\n\t\t\t\t2: {1},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{1, 1, 1, 1, 2, 2, 1},\n\t\t\t[]uint64{0, 1, 2, 3, 1, 8, 1},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t\t2: {8},\n\t\t\t},\n\t\t\t[]uint64{1, 1},\n\t\t\t[]uint64{0, 0},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {1, 2, 3},\n\t\t\t\t2: {8},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{1, 2, 3},\n\t\t\t[]uint64{8, 8, 8},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {},\n\t\t\t\t2: {},\n\t\t\t\t3: {8},\n\t\t\t},\n\t\t\t[]uint64{1, 2, 3},\n\t\t\t[]uint64{9, 9, 9},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {},\n\t\t\t\t2: {},\n\t\t\t\t3: {8},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor i, test := range tests {\n\t\tt.Run(fmt.Sprintf(\"importmutex%d\", i), func(t *testing.T) {\n\t\t\tf, _, tx := mustOpenFragment(t, OptFieldTypeMutex(DefaultCacheType, DefaultCacheSize))\n\t\t\tdefer f.Clean(t)\n\n\t\t\t// Set import.\n\t\t\terr := f.bulkImport(tx, test.setRowIDs, test.setColIDs, &ImportOptions{})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"bulk importing ids: %v\", err)\n\t\t\t}\n\n\t\t\t// Check for expected results.\n\t\t\tfor k, v := range test.setExp {\n\t\t\t\tcols := f.mustRow(tx, k).Columns()\n\t\t\t\tif !reflect.DeepEqual(cols, v) {\n\t\t\t\t\tt.Fatalf(\"row: %d, expected: %v, but got: %v\", k, v, cols)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Clear import.\n\t\t\terr = f.bulkImport(tx, test.clearRowIDs, test.clearColIDs, &ImportOptions{Clear: true})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"bulk clearing ids: %v\", err)\n\t\t\t}\n\n\t\t\t// Check for expected results.\n\t\t\tfor k, v := range test.clearExp {\n\t\t\t\tcols := f.mustRow(tx, k).Columns()\n\t\t\t\tif !reflect.DeepEqual(cols, v) {\n\t\t\t\t\tt.Fatalf(\"row: %d expected: %v, but got: %v\", k, v, cols)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\n// Ensure a fragment can import mutually exclusive values.\n// Now with Commits in the middle.\nfunc TestFragment_ImportMutex_WithTxCommit(t *testing.T) {\n\ttests := []struct {\n\t\tsetRowIDs   []uint64\n\t\tsetColIDs   []uint64\n\t\tsetExp      map[uint64][]uint64\n\t\tclearRowIDs []uint64\n\t\tclearColIDs []uint64\n\t\tclearExp    map[uint64][]uint64\n\t}{\n\t\t{\n\t\t\t[]uint64{1, 1, 1, 1},\n\t\t\t[]uint64{0, 1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t},\n\t\t\t[]uint64{},\n\t\t\t[]uint64{},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{1, 1, 1, 1, 2, 2, 2, 2},\n\t\t\t[]uint64{0, 1, 2, 3, 0, 1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {},\n\t\t\t\t2: {0, 1, 2, 3},\n\t\t\t},\n\t\t\t[]uint64{1, 1, 2},\n\t\t\t[]uint64{1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {},\n\t\t\t\t2: {0, 1, 2},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{1, 1, 1, 1, 2},\n\t\t\t[]uint64{0, 1, 2, 3, 1},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 2, 3},\n\t\t\t\t2: {1},\n\t\t\t},\n\t\t\t[]uint64{1, 1, 1, 1},\n\t\t\t[]uint64{0, 1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {},\n\t\t\t\t2: {1},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{1, 1, 1, 1, 2, 2, 1},\n\t\t\t[]uint64{0, 1, 2, 3, 1, 8, 1},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t\t2: {8},\n\t\t\t},\n\t\t\t[]uint64{1, 1},\n\t\t\t[]uint64{0, 0},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {1, 2, 3},\n\t\t\t\t2: {8},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{1, 2, 3},\n\t\t\t[]uint64{8, 8, 8},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {},\n\t\t\t\t2: {},\n\t\t\t\t3: {8},\n\t\t\t},\n\t\t\t[]uint64{1, 2, 3},\n\t\t\t[]uint64{9, 9, 9},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {},\n\t\t\t\t2: {},\n\t\t\t\t3: {8},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor i, test := range tests {\n\t\tt.Run(fmt.Sprintf(\"importmutex%d\", i), func(t *testing.T) {\n\t\t\tf, idx, tx := mustOpenFragment(t, OptFieldTypeMutex(DefaultCacheType, DefaultCacheSize))\n\t\t\tdefer f.Clean(t)\n\n\t\t\t// Set import.\n\t\t\terr := f.bulkImport(tx, test.setRowIDs, test.setColIDs, &ImportOptions{})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"bulk importing ids: %v\", err)\n\t\t\t}\n\n\t\t\tPanicOn(tx.Commit())\n\t\t\ttx = idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: f.shard})\n\t\t\tdefer tx.Rollback()\n\n\t\t\t// Check for expected results.\n\t\t\tfor k, v := range test.setExp {\n\t\t\t\tcols := f.mustRow(tx, k).Columns()\n\t\t\t\tif !reflect.DeepEqual(cols, v) {\n\t\t\t\t\tt.Fatalf(\"row: %d, expected: %v, but got: %v\", k, v, cols)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\ttx.Rollback()\n\t\t\ttx = idx.holder.txf.NewTx(Txo{Write: writable, Index: idx, Fragment: f, Shard: f.shard})\n\t\t\tdefer tx.Rollback()\n\n\t\t\t// Clear import.\n\t\t\terr = f.bulkImport(tx, test.clearRowIDs, test.clearColIDs, &ImportOptions{Clear: true})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"bulk clearing ids: %v\", err)\n\t\t\t}\n\n\t\t\tPanicOn(tx.Commit())\n\t\t\ttx = idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: f.shard})\n\t\t\tdefer tx.Rollback()\n\n\t\t\t// Check for expected results.\n\t\t\tfor k, v := range test.clearExp {\n\t\t\t\tcols := f.mustRow(tx, k).Columns()\n\t\t\t\tif !reflect.DeepEqual(cols, v) {\n\t\t\t\t\tt.Fatalf(\"row: %d expected: %v, but got: %v\", k, v, cols)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\n// Ensure a fragment can import bool values.\nfunc TestFragment_ImportBool(t *testing.T) {\n\ttests := []struct {\n\t\tsetRowIDs   []uint64\n\t\tsetColIDs   []uint64\n\t\tsetExp      map[uint64][]uint64\n\t\tclearRowIDs []uint64\n\t\tclearColIDs []uint64\n\t\tclearExp    map[uint64][]uint64\n\t}{\n\t\t{\n\t\t\t[]uint64{1, 1, 1, 1},\n\t\t\t[]uint64{0, 1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t},\n\t\t\t[]uint64{},\n\t\t\t[]uint64{},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{0, 0, 0, 0, 1, 1, 1, 1},\n\t\t\t[]uint64{0, 1, 2, 3, 0, 1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t0: {},\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t},\n\t\t\t[]uint64{1, 1, 2},\n\t\t\t[]uint64{1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t0: {},\n\t\t\t\t1: {0, 3},\n\t\t\t\t2: {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{0, 0, 0, 0, 1},\n\t\t\t[]uint64{0, 1, 2, 3, 1},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t0: {0, 2, 3},\n\t\t\t\t1: {1},\n\t\t\t},\n\t\t\t[]uint64{1, 1, 1, 1},\n\t\t\t[]uint64{0, 1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t0: {0, 2, 3},\n\t\t\t\t1: {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{1, 1, 1, 1, 0, 0, 1},\n\t\t\t[]uint64{0, 1, 2, 3, 1, 8, 1},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t0: {8},\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t},\n\t\t\t[]uint64{1, 1},\n\t\t\t[]uint64{0, 0},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t0: {8},\n\t\t\t\t1: {1, 2, 3},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{0, 1, 2},\n\t\t\t[]uint64{8, 8, 8},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t0: {},\n\t\t\t\t1: {}, // This isn't {8} because fragment doesn't validate bool values.\n\t\t\t\t2: {8},\n\t\t\t},\n\t\t\t[]uint64{1, 2, 3},\n\t\t\t[]uint64{9, 9, 9},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t0: {},\n\t\t\t\t1: {},\n\t\t\t\t2: {8},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor i, test := range tests {\n\t\tt.Run(fmt.Sprintf(\"importmutex%d\", i), func(t *testing.T) {\n\t\t\tf, _, tx := mustOpenFragment(t, OptFieldTypeBool())\n\t\t\tdefer f.Clean(t)\n\n\t\t\t// Set import.\n\t\t\terr := f.bulkImport(tx, test.setRowIDs, test.setColIDs, &ImportOptions{})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"bulk importing ids: %v\", err)\n\t\t\t}\n\n\t\t\t// Check for expected results.\n\t\t\tfor k, v := range test.setExp {\n\t\t\t\tcols := f.mustRow(tx, k).Columns()\n\t\t\t\tif !reflect.DeepEqual(cols, v) {\n\t\t\t\t\tt.Fatalf(\"expected: %v, but got: %v\", v, cols)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Clear import.\n\t\t\terr = f.bulkImport(tx, test.clearRowIDs, test.clearColIDs, &ImportOptions{Clear: true})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"bulk importing ids: %v\", err)\n\t\t\t}\n\n\t\t\t// Check for expected results.\n\t\t\tfor k, v := range test.clearExp {\n\t\t\t\tcols := f.mustRow(tx, k).Columns()\n\t\t\t\tif !reflect.DeepEqual(cols, v) {\n\t\t\t\t\tt.Fatalf(\"expected: %v, but got: %v\", v, cols)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\n// Ensure a fragment can import bool values, with Commits in between writes and reads.\nfunc TestFragment_ImportBool_WithTxCommit(t *testing.T) {\n\ttests := []struct {\n\t\tsetRowIDs   []uint64\n\t\tsetColIDs   []uint64\n\t\tsetExp      map[uint64][]uint64\n\t\tclearRowIDs []uint64\n\t\tclearColIDs []uint64\n\t\tclearExp    map[uint64][]uint64\n\t}{\n\t\t{\n\t\t\t[]uint64{1, 1, 1, 1},\n\t\t\t[]uint64{0, 1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t},\n\t\t\t[]uint64{},\n\t\t\t[]uint64{},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{0, 0, 0, 0, 1, 1, 1, 1},\n\t\t\t[]uint64{0, 1, 2, 3, 0, 1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t0: {},\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t},\n\t\t\t[]uint64{1, 1, 2},\n\t\t\t[]uint64{1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t0: {},\n\t\t\t\t1: {0, 3},\n\t\t\t\t2: {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{0, 0, 0, 0, 1},\n\t\t\t[]uint64{0, 1, 2, 3, 1},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t0: {0, 2, 3},\n\t\t\t\t1: {1},\n\t\t\t},\n\t\t\t[]uint64{1, 1, 1, 1},\n\t\t\t[]uint64{0, 1, 2, 3},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t0: {0, 2, 3},\n\t\t\t\t1: {},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{1, 1, 1, 1, 0, 0, 1},\n\t\t\t[]uint64{0, 1, 2, 3, 1, 8, 1},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t0: {8},\n\t\t\t\t1: {0, 1, 2, 3},\n\t\t\t},\n\t\t\t[]uint64{1, 1},\n\t\t\t[]uint64{0, 0},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t0: {8},\n\t\t\t\t1: {1, 2, 3},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t[]uint64{0, 1, 2},\n\t\t\t[]uint64{8, 8, 8},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t0: {},\n\t\t\t\t1: {}, // This isn't {8} because fragment doesn't validate bool values.\n\t\t\t\t2: {8},\n\t\t\t},\n\t\t\t[]uint64{1, 2, 3},\n\t\t\t[]uint64{9, 9, 9},\n\t\t\tmap[uint64][]uint64{\n\t\t\t\t0: {},\n\t\t\t\t1: {},\n\t\t\t\t2: {8},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor i, test := range tests {\n\t\tt.Run(fmt.Sprintf(\"importmutex%d\", i), func(t *testing.T) {\n\t\t\tf, idx, tx := mustOpenFragment(t, OptFieldTypeBool())\n\t\t\tdefer f.Clean(t)\n\n\t\t\t// Set import.\n\t\t\terr := f.bulkImport(tx, test.setRowIDs, test.setColIDs, &ImportOptions{})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"bulk importing ids: %v\", err)\n\t\t\t}\n\n\t\t\tPanicOn(tx.Commit())\n\t\t\ttx = idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: f.shard})\n\t\t\tdefer tx.Rollback()\n\n\t\t\t// Check for expected results.\n\t\t\tfor k, v := range test.setExp {\n\t\t\t\tcols := f.mustRow(tx, k).Columns()\n\t\t\t\tif !reflect.DeepEqual(cols, v) {\n\t\t\t\t\tt.Fatalf(\"expected: %v, but got: %v\", v, cols)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\ttx.Rollback()\n\t\t\ttx = idx.holder.txf.NewTx(Txo{Write: writable, Index: idx, Fragment: f, Shard: f.shard})\n\t\t\tdefer tx.Rollback()\n\n\t\t\t// Clear import.\n\t\t\terr = f.bulkImport(tx, test.clearRowIDs, test.clearColIDs, &ImportOptions{Clear: true})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"bulk importing ids: %v\", err)\n\t\t\t}\n\n\t\t\tPanicOn(tx.Commit())\n\t\t\ttx = idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: f.shard})\n\t\t\tdefer tx.Rollback()\n\n\t\t\t// Check for expected results.\n\t\t\tfor k, v := range test.clearExp {\n\t\t\t\tcols := f.mustRow(tx, k).Columns()\n\t\t\t\tif !reflect.DeepEqual(cols, v) {\n\t\t\t\t\tt.Fatalf(\"expected: %v, but got: %v\", v, cols)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc BenchmarkFragment_Import(b *testing.B) {\n\tb.StopTimer()\n\tmaxX := ShardWidth * 5 * 2\n\tsz := maxX\n\trows := make([]uint64, sz)\n\tcols := make([]uint64, sz)\n\ti := 0\n\tfor row := 0; row < 100; row++ {\n\t\tval := 1\n\t\tfor col := 0; col < ShardWidth/2; col++ {\n\t\t\trows[i] = uint64(row)\n\t\t\tcols[i] = uint64(val)\n\t\t\tval += 2\n\t\t\ti++\n\t\t}\n\t\tif i == maxX {\n\t\t\tbreak\n\t\t}\n\t}\n\trowsUse, colsUse := make([]uint64, len(rows)), make([]uint64, len(cols))\n\tb.ReportAllocs()\n\toptions := &ImportOptions{}\n\tfor i := 0; i < b.N; i++ {\n\t\t// since bulkImport modifies the input slices, we make new copies for each round\n\t\tcopy(rowsUse, rows)\n\t\tcopy(colsUse, cols)\n\t\tf, idx, tx := mustOpenFragment(b)\n\t\t_ = idx\n\t\tb.StartTimer()\n\t\tif err := f.bulkImport(tx, rowsUse, colsUse, options); err != nil {\n\t\t\tb.Errorf(\"Error Building Sample: %s\", err)\n\t\t}\n\t\tb.StopTimer()\n\t\ttx.Rollback()\n\t\tf.Clean(b)\n\t}\n}\n\nvar (\n\trowCases         = []uint64{2, 50, 1000, 10000, 100000}\n\tcolCases         = []uint64{20, 1000, 5000, 50000, 500000}\n\tconcurrencyCases = []int{2, 4, 16}\n\tcacheCases       = []string{CacheTypeNone, CacheTypeRanked}\n)\n\nfunc BenchmarkImportRoaring(b *testing.B) {\n\tfor _, numRows := range rowCases {\n\t\tdata := getZipfRowsSliceRoaring(numRows, 1, 0, ShardWidth)\n\t\tb.Logf(\"%dRows: %.2fMB\\n\", numRows, float64(len(data))/1024/1024)\n\t\tfor _, cacheType := range cacheCases {\n\t\t\tb.Run(fmt.Sprintf(\"Rows%dCache_%s\", numRows, cacheType), func(b *testing.B) {\n\t\t\t\tb.StopTimer()\n\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\tf, _, tx := mustOpenFragment(b, OptFieldTypeSet(cacheType, 0))\n\t\t\t\t\tb.StartTimer()\n\n\t\t\t\t\terr := f.importRoaringT(tx, data, false)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tf.Clean(b)\n\t\t\t\t\t\tb.Fatalf(\"import error: %v\", err)\n\t\t\t\t\t}\n\t\t\t\t\tb.StopTimer()\n\t\t\t\t\tf.Clean(b)\n\t\t\t\t}\n\t\t\t})\n\t\t}\n\t}\n}\n\nfunc BenchmarkImportRoaringConcurrent(b *testing.B) {\n\tif testing.Short() {\n\t\tb.SkipNow()\n\t}\n\tfor _, numRows := range rowCases {\n\t\tdata := make([][]byte, 0, len(concurrencyCases))\n\t\tdata = append(data, getZipfRowsSliceRoaring(numRows, 0, 0, ShardWidth))\n\t\tb.Logf(\"%dRows: %.2fMB\\n\", numRows, float64(len(data[0]))/1024/1024)\n\t\tfor _, concurrency := range concurrencyCases {\n\t\t\t// add more data sets\n\t\t\tfor j := len(data); j < concurrency; j++ {\n\t\t\t\tdata = append(data, getZipfRowsSliceRoaring(numRows, int64(j), 0, ShardWidth))\n\t\t\t}\n\t\t\tfor _, cacheType := range cacheCases {\n\t\t\t\tb.Run(fmt.Sprintf(\"Rows%dConcurrency%dCache_%s\", numRows, concurrency, cacheType), func(b *testing.B) {\n\t\t\t\t\tb.StopTimer()\n\t\t\t\t\tfrags := make([]*fragment, concurrency)\n\t\t\t\t\ttxs := make([]Tx, concurrency)\n\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\tfor j := 0; j < concurrency; j++ {\n\t\t\t\t\t\t\tfrags[j], _, txs[j] = mustOpenFragment(b, OptFieldTypeSet(cacheType, 0))\n\t\t\t\t\t\t}\n\t\t\t\t\t\teg := errgroup.Group{}\n\t\t\t\t\t\tb.StartTimer()\n\t\t\t\t\t\tfor j := 0; j < concurrency; j++ {\n\t\t\t\t\t\t\tj := j\n\t\t\t\t\t\t\teg.Go(func() error {\n\t\t\t\t\t\t\t\tdefer txs[j].Rollback()\n\n\t\t\t\t\t\t\t\terr := frags[j].importRoaringT(txs[j], data[j], false)\n\t\t\t\t\t\t\t\treturn err\n\t\t\t\t\t\t\t})\n\t\t\t\t\t\t}\n\t\t\t\t\t\terr := eg.Wait()\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\tb.Errorf(\"importing fragment: %v\", err)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tb.StopTimer()\n\t\t\t\t\t\tfor j := 0; j < concurrency; j++ {\n\t\t\t\t\t\t\tfrags[j].Clean(b)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc BenchmarkImportStandard(b *testing.B) {\n\tfor _, cacheType := range cacheCases {\n\t\tfor _, numRows := range rowCases {\n\t\t\trowIDsOrig, columnIDsOrig := getZipfRowsSliceStandard(numRows, 1, 0, ShardWidth)\n\t\t\trowIDs, columnIDs := make([]uint64, len(rowIDsOrig)), make([]uint64, len(columnIDsOrig))\n\t\t\tb.Run(fmt.Sprintf(\"Rows%dCache_%s\", numRows, cacheType), func(b *testing.B) {\n\t\t\t\tb.StopTimer()\n\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\tcopy(rowIDs, rowIDsOrig)\n\t\t\t\t\tcopy(columnIDs, columnIDsOrig)\n\t\t\t\t\tf, idx, tx := mustOpenFragment(b, OptFieldTypeSet(cacheType, 0))\n\t\t\t\t\t_ = idx\n\t\t\t\t\tb.StartTimer()\n\t\t\t\t\terr := f.bulkImport(tx, rowIDs, columnIDs, &ImportOptions{})\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tb.Errorf(\"import error: %v\", err)\n\t\t\t\t\t}\n\t\t\t\t\tb.StopTimer()\n\t\t\t\t\ttx.Rollback()\n\t\t\t\t\tf.Clean(b)\n\t\t\t\t}\n\t\t\t})\n\t\t}\n\t}\n}\n\nfunc BenchmarkImportRoaringUpdate(b *testing.B) {\n\tfileSize := make(map[string]int64)\n\tnames := []string{}\n\tfor _, numRows := range rowCases {\n\t\tdata := getZipfRowsSliceRoaring(numRows, 1, 0, ShardWidth)\n\t\tfor _, numCols := range colCases {\n\t\t\tupdata := getUpdataRoaring(numRows, numCols, 1)\n\t\t\tfor _, cacheType := range cacheCases {\n\t\t\t\tname := fmt.Sprintf(\"Rows%dCols%dCache_%s\", numRows, numCols, cacheType)\n\t\t\t\tnames = append(names, name)\n\t\t\t\tb.Run(name, func(b *testing.B) {\n\t\t\t\t\tb.StopTimer()\n\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\tf, idx, tx := mustOpenFragment(b, OptFieldTypeSet(cacheType, 0))\n\t\t\t\t\t\t_ = idx\n\n\t\t\t\t\t\titr, err := roaring.NewRoaringIterator(data)\n\t\t\t\t\t\tPanicOn(err)\n\t\t\t\t\t\t_, _, err = tx.ImportRoaringBits(f.index(), f.field(), f.view(), f.shard, itr, false, false, 0)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\tb.Errorf(\"import error: %v\", err)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tb.StartTimer()\n\t\t\t\t\t\terr = f.importRoaringT(tx, updata, false)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\tf.Clean(b)\n\t\t\t\t\t\t\tb.Errorf(\"import error: %v\", err)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tb.StopTimer()\n\t\t\t\t\t\tvar stat os.FileInfo\n\t\t\t\t\t\tvar statTarget io.Writer\n\t\t\t\t\t\terr = func() error {\n\t\t\t\t\t\t\ttargetFile, ok := statTarget.(*os.File)\n\t\t\t\t\t\t\tif ok {\n\t\t\t\t\t\t\t\tstat, _ = targetFile.Stat()\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tb.Errorf(\"couldn't stat file\")\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\treturn nil\n\t\t\t\t\t\t}()\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\tb.Errorf(\"transaction error: %v\", err)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfileSize[name] = stat.Size()\n\t\t\t\t\t\tf.Clean(b)\n\t\t\t\t\t}\n\t\t\t\t})\n\n\t\t\t}\n\t\t}\n\t}\n\tfor _, name := range names {\n\t\tb.Logf(\"%s: %.2fMB\\n\", name, float64(fileSize[name])/1024/1024)\n\t}\n}\n\n// BenchmarkUpdatePathological imports more data into an existing fragment than\n// already exists, but the larger data comes after the smaller data. If\n// SliceContainers are in use, this can cause horrible performance.\nfunc BenchmarkUpdatePathological(b *testing.B) {\n\texists := getZipfRowsSliceRoaring(100000, 1, 0, 400000)\n\tinc := getZipfRowsSliceRoaring(100000, 2, 400000, ShardWidth)\n\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tb.StopTimer()\n\t\tf, idx, tx := mustOpenFragment(b, OptFieldTypeSet(DefaultCacheType, 0))\n\t\t_ = idx\n\n\t\terr := f.importRoaringT(tx, exists, false)\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"importing roaring: %v\", err)\n\t\t}\n\t\tb.StartTimer()\n\t\terr = f.importRoaringT(tx, inc, false)\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"importing second: %v\", err)\n\t\t}\n\n\t}\n\n}\n\nvar bigFrag string\n\nfunc initBigFrag(tb testing.TB) {\n\tif bigFrag == \"\" {\n\t\tf, _, tx := mustOpenFragment(tb, OptFieldTypeSet(DefaultCacheType, 0))\n\t\tfor i := int64(0); i < 10; i++ {\n\t\t\t// 10 million rows, 1 bit per column, random seeded by i\n\t\t\tdata := getZipfRowsSliceRoaring(10000000, i, 0, ShardWidth)\n\t\t\terr := f.importRoaringT(tx, data, false)\n\t\t\tif err != nil {\n\t\t\t\tPanicOn(fmt.Sprintf(\"setting up fragment data: %v\", err))\n\t\t\t}\n\t\t}\n\t\terr := f.Close()\n\t\tif err != nil {\n\t\t\tPanicOn(fmt.Sprintf(\"closing fragment: %v\", err))\n\t\t}\n\t\tbigFrag = f.path()\n\t\tPanicOn(tx.Commit())\n\t}\n}\n\nfunc BenchmarkImportIntoLargeFragment(b *testing.B) {\n\tb.StopTimer()\n\tinitBigFrag(b)\n\trowsOrig, colsOrig := getUpdataSlices(10000000, 11000, 0)\n\trows, cols := make([]uint64, len(rowsOrig)), make([]uint64, len(colsOrig))\n\topts := &ImportOptions{}\n\tfor i := 0; i < b.N; i++ {\n\t\torigF, err := os.Open(bigFrag)\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"opening frag file: %v\", err)\n\t\t}\n\t\tfi, err := testhook.TempFile(b, \"\")\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"getting temp file: %v\", err)\n\t\t}\n\t\t_, err = io.Copy(fi, origF)\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"copying fragment file: %v\", err)\n\t\t}\n\t\torigF.Close()\n\t\tfi.Close()\n\n\t\th, idx, _, _, f := newTestFragment(b)\n\n\t\terr = f.Open()\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"opening fragment: %v\", err)\n\t\t}\n\n\t\t// Obtain transaction.\n\t\ttx := h.txf.NewTx(Txo{Write: writable, Index: idx, Fragment: f, Shard: f.shard})\n\t\tdefer tx.Rollback()\n\n\t\tcopy(rows, rowsOrig)\n\t\tcopy(cols, colsOrig)\n\t\tb.StartTimer()\n\t\terr = f.bulkImport(tx, rows, cols, opts)\n\t\tb.StopTimer()\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"bulkImport: %v\", err)\n\t\t}\n\t\tPanicOn(tx.Commit())\n\t\tf.Clean(b)\n\t}\n}\n\nfunc BenchmarkImportRoaringIntoLargeFragment(b *testing.B) {\n\tb.StopTimer()\n\tinitBigFrag(b)\n\tupdata := getUpdataRoaring(10000000, 11000, 0)\n\tfor i := 0; i < b.N; i++ {\n\t\torigF, err := os.Open(bigFrag)\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"opening frag file: %v\", err)\n\t\t}\n\t\tfi, err := testhook.TempFile(b, \"\")\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"getting temp file: %v\", err)\n\t\t}\n\t\t_, err = io.Copy(fi, origF)\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"copying fragment file: %v\", err)\n\t\t}\n\t\torigF.Close()\n\t\tfi.Close()\n\n\t\th, idx, _, _, f := newTestFragment(b)\n\n\t\tdefer f.Clean(b)\n\n\t\ttx := h.txf.NewTx(Txo{Write: writable, Index: idx, Fragment: f, Shard: f.shard})\n\t\tdefer tx.Rollback()\n\n\t\terr = f.Open()\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"opening fragment: %v\", err)\n\t\t}\n\t\tb.StartTimer()\n\t\terr = f.importRoaringT(tx, updata, false)\n\t\tb.StopTimer()\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"bulkImport: %v\", err)\n\t\t}\n\t}\n}\n\nfunc TestGetZipfRowsSliceRoaring(t *testing.T) {\n\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(DefaultCacheType, 0))\n\t_ = idx\n\n\tdata := getZipfRowsSliceRoaring(10, 1, 0, ShardWidth)\n\terr := f.importRoaringT(tx, data, false)\n\tif err != nil {\n\t\tt.Fatalf(\"importing roaring: %v\", err)\n\t}\n\trows, err := f.rows(context.Background(), tx, 0)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t} else if !reflect.DeepEqual(rows, []uint64{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}) {\n\t\tt.Fatalf(\"unexpected rows: %v\", rows)\n\t}\n\tfor i := uint64(1); i < 10; i++ {\n\t\tif f.mustRow(tx, i).Count() >= f.mustRow(tx, i-1).Count() {\n\t\t\tt.Fatalf(\"suspect distribution from getZipfRowsSliceRoaring\")\n\t\t}\n\t}\n\tf.Clean(t)\n}\n\nfunc prepareSampleRowData(b *testing.B, bits int, rows uint64, width uint64) (*fragment, *Index, Tx) {\n\tf, idx, tx := mustOpenFragment(b, OptFieldTypeSet(\"none\", 0))\n\tfor i := 0; i < bits; i++ {\n\t\tdata := getUniformRowsSliceRoaring(rows, int64(rows)+int64(i), 0, width)\n\t\terr := f.importRoaringT(tx, data, false)\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"creating sample data: %v\", err)\n\t\t}\n\t}\n\treturn f, idx, tx\n}\n\ntype txFrag struct {\n\trows, width uint64\n\ttx          Tx\n\tidx         *Index\n\tfrag        *fragment\n}\n\nfunc benchmarkRowsOnTestcase(b *testing.B, ctx context.Context, txf txFrag) {\n\tcol := uint64(0)\n\tfor i := 0; i < b.N; i++ {\n\t\t_, err := txf.frag.rows(ctx, txf.tx, 0, roaring.NewBitmapColumnFilter(col))\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"retrieving rows for col %d: %v\", col, err)\n\t\t}\n\t\tcol = (col + 1) % txf.width\n\t}\n}\n\n// benchmarkRowsMaybeWritable uses a local flag which shadows the package-scope\n// const writable. Neat, huh.\nfunc benchmarkRowsMaybeWritable(b *testing.B, writable bool) {\n\tvar depths = []uint64{384, 2048}\n\ttestCases := make([]txFrag, 0, len(depths))\n\tdefer func() {\n\t\tfor _, testCase := range testCases {\n\t\t\ttestCase.frag.Clean(b)\n\t\t}\n\t}()\n\tbg := context.Background()\n\tfor _, rows := range depths {\n\t\tfrag, idx, tx := prepareSampleRowData(b, 3, rows, ShardWidth)\n\t\tif !writable {\n\t\t\terr := tx.Commit()\n\t\t\tif err != nil {\n\t\t\t\tb.Fatalf(\"error committing sample data: %v\", err)\n\t\t\t}\n\t\t\ttx = idx.holder.txf.NewTx(Txo{Write: writable, Index: idx, Fragment: frag, Shard: 0})\n\t\t\tdefer tx.Rollback()\n\t\t}\n\t\ttestCases = append(testCases, txFrag{\n\t\t\trows:  rows,\n\t\t\twidth: ShardWidth,\n\t\t\tfrag:  frag,\n\t\t\tidx:   idx,\n\t\t\ttx:    tx,\n\t\t})\n\t}\n\tfor _, testCase := range testCases {\n\t\tb.Run(fmt.Sprintf(\"%d\", testCase.rows), func(b *testing.B) {\n\t\t\tbenchmarkRowsOnTestcase(b, bg, testCase)\n\t\t})\n\t}\n}\nfunc BenchmarkRows(b *testing.B) {\n\tb.Run(\"writable\", func(b *testing.B) {\n\t\tbenchmarkRowsMaybeWritable(b, true)\n\t})\n\tb.Run(\"readonly\", func(b *testing.B) {\n\t\tbenchmarkRowsMaybeWritable(b, false)\n\t})\n}\n\n// getZipfRowsSliceRoaring generates a random fragment with the given number of\n// rows, and 1 bit set in each column. The row each bit is set in is chosen via\n// the Zipf generator, and so will be skewed toward lower row numbers. If this\n// is edited to change the data distribution, getZipfRowsSliceStandard should be\n// edited as well. TODO switch to generating row-major for perf boost\nfunc getZipfRowsSliceRoaring(numRows uint64, seed int64, startCol, endCol uint64) []byte {\n\tb := roaring.NewBTreeBitmap()\n\ts := rand.NewSource(seed)\n\tr := rand.New(s)\n\tz := rand.NewZipf(r, 1.6, 50, numRows-1)\n\tbufSize := 1 << 14\n\tposBuf := make([]uint64, 0, bufSize)\n\tfor i := uint64(startCol); i < endCol; i++ {\n\t\trow := z.Uint64()\n\t\tposBuf = append(posBuf, row*ShardWidth+i)\n\t\tif len(posBuf) == bufSize {\n\t\t\tsort.Slice(posBuf, func(i int, j int) bool { return posBuf[i] < posBuf[j] })\n\t\t\tb.DirectAddN(posBuf...)\n\t\t}\n\t}\n\tb.DirectAddN(posBuf...)\n\tbuf := bytes.NewBuffer(make([]byte, 0, 100000))\n\t_, err := b.WriteTo(buf)\n\tif err != nil {\n\t\tPanicOn(err)\n\t}\n\treturn buf.Bytes()\n}\n\n// getUniformRowsSliceRoaring produces evenly-distributed values as a roaring\n// bitmap slice. This is useful if you want to avoid the higher rows being\n// sparse; see also getZipfRowsSliceRoaring.\nfunc getUniformRowsSliceRoaring(numRows uint64, seed int64, startCol, endCol uint64) []byte {\n\tb := roaring.NewBTreeBitmap()\n\ts := rand.NewSource(seed)\n\tr := rand.New(s)\n\tbufSize := 1 << 14\n\tposBuf := make([]uint64, 0, bufSize)\n\tfor i := uint64(startCol); i < endCol; i++ {\n\t\trow := uint64(r.Int63n(int64(numRows)))\n\t\tposBuf = append(posBuf, row*ShardWidth+i)\n\t\tif len(posBuf) == bufSize {\n\t\t\tsort.Slice(posBuf, func(i int, j int) bool { return posBuf[i] < posBuf[j] })\n\t\t\tb.DirectAddN(posBuf...)\n\t\t}\n\t}\n\tb.DirectAddN(posBuf...)\n\tbuf := bytes.NewBuffer(make([]byte, 0, 100000))\n\t_, err := b.WriteTo(buf)\n\tif err != nil {\n\t\tPanicOn(err)\n\t}\n\treturn buf.Bytes()\n}\n\nfunc getUpdataSlices(numRows, numCols uint64, seed int64) (rows, cols []uint64) {\n\tgetUpdataInto(func(row, col uint64) bool {\n\t\trows = append(rows, row)\n\t\tcols = append(cols, col)\n\t\treturn true\n\t}, numRows, numCols, seed)\n\treturn rows, cols\n}\n\n// getUpdataRoaring gets a byte slice containing a roaring bitmap which\n// represents numCols set bits distributed randomly throughout a shard's column\n// space and zipfianly throughout numRows rows.\nfunc getUpdataRoaring(numRows, numCols uint64, seed int64) []byte {\n\tb := roaring.NewBitmap()\n\n\tgetUpdataInto(func(row, col uint64) bool {\n\t\treturn b.DirectAdd(row*ShardWidth + col)\n\t}, numRows, numCols, seed)\n\n\tbuf := bytes.NewBuffer(make([]byte, 0, 100000))\n\t_, err := b.WriteTo(buf)\n\tif err != nil {\n\t\tPanicOn(err)\n\t}\n\treturn buf.Bytes()\n}\n\nfunc getUpdataInto(f func(row, col uint64) bool, numRows, numCols uint64, seed int64) int {\n\ts := rand.NewSource(seed)\n\tr := rand.New(s)\n\tz := rand.NewZipf(r, 1.6, 50, numRows-1)\n\n\ti := uint64(0)\n\t// ensure we get exactly the number we asked for. it turns out we had\n\t// a horrible pathological edge case for exactly 10,000 entries in an\n\t// imported bitmap, and hit it only occasionally...\n\tfor i < numCols {\n\t\tcol := uint64(r.Int63n(ShardWidth))\n\t\trow := z.Uint64()\n\t\tif f(row, col) {\n\t\t\ti++\n\t\t}\n\t}\n\treturn int(i)\n}\n\n// getZipfRowsSliceStandard is the same as getZipfRowsSliceRoaring, but returns\n// row and column ids instead of a byte slice containing roaring bitmap data.\nfunc getZipfRowsSliceStandard(numRows uint64, seed int64, startCol, endCol uint64) (rowIDs, columnIDs []uint64) {\n\ts := rand.NewSource(seed)\n\tr := rand.New(s)\n\tz := rand.NewZipf(r, 1.6, 50, numRows-1)\n\trowIDs, columnIDs = make([]uint64, ShardWidth), make([]uint64, ShardWidth)\n\tfor i := uint64(startCol); i < endCol; i++ {\n\t\trowIDs[i] = z.Uint64()\n\t\tcolumnIDs[i] = i\n\t}\n\treturn rowIDs, columnIDs\n}\n\n// Clean used to delete fragments, but doesn't anymore -- deleting is\n// handled by the testhook.TempDir when appropriate.\n// TODO(jaffee): this can likely go away entirely... it was doing snapshot/source/generation stuff that it no longer needs to.\nfunc (f *fragment) Clean(t testing.TB) {\n\terrc := f.Close()\n\tif errc != nil {\n\t\tt.Fatalf(\"error closing fragment: %v\", errc)\n\t}\n}\n\n// importRoaringT calls importRoaring with context.Background() for convenience\nfunc (f *fragment) importRoaringT(tx Tx, data []byte, clear bool) error {\n\n\treturn f.importRoaring(context.Background(), tx, data, clear)\n}\n\nfunc newTestHolder(tb testing.TB) *Holder {\n\tpath := tb.TempDir()\n\th := NewHolder(path, TestHolderConfig())\n\terr := h.Open()\n\tif err != nil {\n\t\ttb.Fatalf(\"opening test holder: %v\", err)\n\t}\n\ttesthook.Cleanup(tb, func() {\n\t\th.Close()\n\t})\n\n\treturn h\n}\n\n// newTestField creates a field with the specified field options\nfunc newTestField(tb testing.TB, fieldOpts ...FieldOption) (*Holder, *Index, *Field) {\n\tif len(fieldOpts) == 0 {\n\t\tfieldOpts = []FieldOption{OptFieldTypeDefault()}\n\t}\n\th := newTestHolder(tb)\n\tidx, err := h.CreateIndex(\"i\", \"\", IndexOptions{})\n\tif err != nil {\n\t\ttb.Fatalf(\"creating test index: %v\", err)\n\t}\n\tfld, err := idx.CreateField(\"f\", \"\", fieldOpts...)\n\tif err != nil {\n\t\ttb.Fatalf(\"creating test field: %v\", err)\n\t}\n\treturn h, idx, fld\n}\n\n// newTestView creates a view with the specified options.\nfunc newTestView(tb testing.TB, fieldOpts ...FieldOption) (*Holder, *Index, *Field, *view) {\n\th, idx, fld := newTestField(tb, fieldOpts...)\n\tv, _, err := fld.createViewIfNotExistsBase(&CreateViewMessage{Index: \"i\", Field: \"f\", View: \"v\"})\n\tif err != nil {\n\t\ttb.Fatalf(\"creating test view: %v\", err)\n\t}\n\treturn h, idx, fld, v\n}\n\n// newTestFragment makes the default /i/f/v/0 fragment, and returns the\n// things. the test holder will be deleted automatically in test cleanup.\nfunc newTestFragment(tb testing.TB, fieldOpts ...FieldOption) (*Holder, *Index, *Field, *view, *fragment) {\n\th, idx, fld, v := newTestView(tb, fieldOpts...)\n\tf := v.newFragment(0)\n\treturn h, idx, fld, v, f\n}\n\n// mustOpenFragment returns a new instance of Fragment with a temporary path.\nfunc mustOpenFragment(tb testing.TB, fieldOpts ...FieldOption) (*fragment, *Index, Tx) {\n\tth, idx, fld, v, f := newTestFragment(tb, fieldOpts...)\n\tfragDir := filepath.Join(idx.path, fld.name, \"views\", v.name, \"fragments\")\n\terr := os.MkdirAll(fragDir, 0700)\n\tif err != nil {\n\t\ttb.Fatalf(\"creating fragment directory: %v\", err)\n\t}\n\n\ttx := th.txf.NewTx(Txo{Write: writable, Index: idx, Fragment: f, Shard: 0})\n\ttesthook.Cleanup(tb, func() {\n\t\ttx.Rollback()\n\n\t\tif err := th.txf.CloseIndex(idx); err != nil {\n\t\t\ttb.Fatalf(\"closing index after test: %v\", err)\n\t\t}\n\t})\n\n\tf.CacheType = fld.options.CacheType\n\n\tif err := f.Open(); err != nil {\n\t\ttb.Fatalf(\"opening fragment: %v\", err)\n\t}\n\treturn f, idx, tx\n}\n\n// Reopen closes the fragment and reopens it as a new instance.\nfunc (f *fragment) Reopen() error {\n\tif err := f.Close(); err != nil {\n\t\treturn err\n\t}\n\tif err := f.Open(); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// mustSetBits sets columns on a row. Panic on error.\n// This function does not accept a timestamp or quantum.\nfunc (f *fragment) mustSetBits(tx Tx, rowID uint64, columnIDs ...uint64) {\n\tfor _, columnID := range columnIDs {\n\t\tif _, err := f.setBit(tx, rowID, columnID); err != nil {\n\t\t\tPanicOn(err)\n\t\t}\n\t}\n}\n\nfunc addToBitmap(bm *roaring.Bitmap, rowID uint64, columnIDs ...uint64) {\n\t// we'll reuse the columnIDs slice and fill it with positions for DirectAddN\n\tfor i, c := range columnIDs {\n\t\tcolumnIDs[i] = rowID*ShardWidth + c%ShardWidth\n\t}\n\tbm.DirectAddN(columnIDs...)\n}\n\n// Test Various methods of retrieving RowIDs\nfunc TestFragment_RowsIteration(t *testing.T) {\n\tt.Run(\"firstContainer\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t)\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\n\t\texpectedAll := make([]uint64, 0)\n\t\texpectedOdd := make([]uint64, 0)\n\t\tfor i := uint64(100); i < uint64(200); i++ {\n\t\t\tif _, err := f.setBit(tx, i, i%2); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\texpectedAll = append(expectedAll, i)\n\t\t\tif i%2 == 1 {\n\t\t\t\texpectedOdd = append(expectedOdd, i)\n\t\t\t}\n\t\t}\n\n\t\tids, err := f.rows(context.Background(), tx, 0)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(expectedAll, ids) {\n\t\t\tt.Fatalf(\"Do not match %v %v\", expectedAll, ids)\n\t\t}\n\n\t\tids, err = f.rows(context.Background(), tx, 0, roaring.NewBitmapColumnFilter(1))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(expectedOdd, ids) {\n\t\t\tt.Fatalf(\"Do not match %v %v\", expectedOdd, ids)\n\t\t}\n\t})\n\n\tt.Run(\"secondRow\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t)\n\t\t_ = idx\n\n\t\tdefer f.Clean(t)\n\n\t\texpected := []uint64{1, 2}\n\t\tif _, err := f.setBit(tx, 1, 66000); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setBit(tx, 2, 66000); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if _, err := f.setBit(tx, 2, 166000); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tPanicOn(tx.Commit())\n\n\t\ttx = idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: f.shard})\n\t\tdefer tx.Rollback()\n\n\t\tids, err := f.rows(context.Background(), tx, 0)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(expected, ids) {\n\t\t\tt.Fatalf(\"Do not match %v %v\", expected, ids)\n\t\t}\n\n\t\tids, err = f.rows(context.Background(), tx, 0, roaring.NewBitmapColumnFilter(66000))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(expected, ids) {\n\t\t\tt.Fatalf(\"Do not match %v %v\", expected, ids)\n\t\t}\n\t})\n\n\tt.Run(\"combinations\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t)\n\t\t_ = idx\n\n\t\tdefer f.Clean(t)\n\n\t\texpectedRows := make([]uint64, 0)\n\t\tfor r := uint64(1); r < uint64(10000); r += 250 {\n\t\t\texpectedRows = append(expectedRows, r)\n\t\t\tfor c := uint64(1); c < uint64(ShardWidth-1); c += (ShardWidth >> 5) {\n\t\t\t\tif _, err := f.setBit(tx, r, c); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\n\t\t\t\tids, err := f.rows(context.Background(), tx, 0)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t} else if !reflect.DeepEqual(expectedRows, ids) {\n\t\t\t\t\tt.Fatalf(\"Do not match %v %v\", expectedRows, ids)\n\t\t\t\t}\n\t\t\t\tids, err = f.rows(context.Background(), tx, 0, roaring.NewBitmapColumnFilter(c))\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t} else if !reflect.DeepEqual(expectedRows, ids) {\n\t\t\t\t\tt.Fatalf(\"Do not match %v %v\", expectedRows, ids)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t})\n}\n\n// Test Importing roaring data.\nfunc TestFragment_RoaringImport(t *testing.T) {\n\ttests := [][][]uint64{\n\t\t{\n\t\t\t[]uint64{0},\n\t\t\t[]uint64{1},\n\t\t},\n\t\t{\n\t\t\t[]uint64{0, 65535, 65536, 65537, 65538, 65539, 130000},\n\t\t\t[]uint64{1000, 67000, 130000},\n\t\t},\n\t\t{\n\t\t\t[]uint64{0, 65535, 65536, 65537, 65538, 65539, 130000},\n\t\t\t[]uint64{0, 65535, 65536, 65537, 65538, 65539, 130000},\n\t\t},\n\t\t{\n\t\t\t[]uint64{0, 65535, 65536, 1<<20 + 1, 1<<20*2 + 1},\n\t\t\t[]uint64{1, 1<<20 + 2, 1<<20*2 + 2},\n\t\t},\n\t}\n\n\tfor i, test := range tests {\n\t\tt.Run(fmt.Sprintf(\"importroaring%d\", i), func(t *testing.T) {\n\t\t\tf, idx, tx := mustOpenFragment(t)\n\t\t\t_ = idx\n\t\t\tdefer f.Clean(t)\n\n\t\t\tfor num, input := range test {\n\t\t\t\tbuf := &bytes.Buffer{}\n\t\t\t\tbm := roaring.NewBitmap(input...)\n\t\t\t\t_, err := bm.WriteTo(buf)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"writing to buffer: %v\", err)\n\t\t\t\t}\n\t\t\t\terr = f.importRoaringT(tx, buf.Bytes(), false)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"importing roaring: %v\", err)\n\t\t\t\t}\n\n\t\t\t\texp := calcExpected(test[:num+1]...)\n\t\t\t\tfor row, expCols := range exp {\n\t\t\t\t\tcols := f.mustRow(tx, uint64(row)).Columns()\n\t\t\t\t\tt.Logf(\"\\nrow: %d\\n  exp:%v\\n  got:%v\", row, expCols, cols)\n\t\t\t\t\tif !reflect.DeepEqual(cols, expCols) {\n\t\t\t\t\t\tt.Fatalf(\"input%d, row %d\\n  exp:%v\\n  got:%v\", num, row, expCols, cols)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\n// Test Importing roaring data.\nfunc TestFragment_RoaringImportTopN(t *testing.T) {\n\ttests := []struct {\n\t\trowIDs  []uint64\n\t\tcolIDs  []uint64\n\t\trowIDs2 []uint64\n\t\tcolIDs2 []uint64\n\t\troaring []uint64\n\t}{\n\t\t{\n\t\t\trowIDs:  []uint64{4, 4, 4, 4},\n\t\t\tcolIDs:  []uint64{0, 1, 2, 3},\n\t\t\trowIDs2: []uint64{5, 5, 5, 5, 5},\n\t\t\tcolIDs2: []uint64{0, 1, 2, 3, 4},\n\t\t\troaring: []uint64{0, 65535, 65536, 1<<20 + 1, 1<<20 + 2, 1<<20*2 + 1},\n\t\t},\n\t}\n\n\tfor i, test := range tests {\n\t\tt.Run(fmt.Sprintf(\"importroaring%d\", i), func(t *testing.T) {\n\t\t\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeRanked, DefaultCacheSize))\n\t\t\t_ = idx\n\t\t\tdefer f.Clean(t)\n\n\t\t\toptions := &ImportOptions{}\n\t\t\terr := f.bulkImport(tx, test.rowIDs, test.colIDs, options)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"bulk importing ids: %v\", err)\n\t\t\t}\n\n\t\t\texpPairs := calcTop(test.rowIDs, test.colIDs)\n\t\t\tpairs, err := f.top(tx, topOptions{})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"executing top after bulk import: %v\", err)\n\t\t\t}\n\t\t\tif !reflect.DeepEqual(expPairs, pairs) {\n\t\t\t\tt.Fatalf(\"post bulk import:\\n  exp: %v\\n  got: %v\\n\", expPairs, pairs)\n\t\t\t}\n\n\t\t\terr = f.bulkImport(tx, test.rowIDs2, test.colIDs2, options)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"bulk importing ids: %v\", err)\n\t\t\t}\n\t\t\ttest.rowIDs = append(test.rowIDs, test.rowIDs2...)\n\t\t\ttest.colIDs = append(test.colIDs, test.colIDs2...)\n\t\t\texpPairs = calcTop(test.rowIDs, test.colIDs)\n\t\t\tpairs, err = f.top(tx, topOptions{})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"executing top after bulk import: %v\", err)\n\t\t\t}\n\t\t\tif !reflect.DeepEqual(expPairs, pairs) {\n\t\t\t\tt.Fatalf(\"post bulk import2:\\n  exp: %v\\n  got: %v\\n\", expPairs, pairs)\n\t\t\t}\n\n\t\t\tbuf := &bytes.Buffer{}\n\t\t\tbm := roaring.NewBitmap(test.roaring...)\n\t\t\t_, err = bm.WriteTo(buf)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"writing to buffer: %v\", err)\n\t\t\t}\n\t\t\terr = f.importRoaringT(tx, buf.Bytes(), false)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"importing roaring: %v\", err)\n\t\t\t}\n\t\t\trows, cols := toRowsCols(test.roaring)\n\t\t\texpPairs = calcTop(append(test.rowIDs, rows...), append(test.colIDs, cols...))\n\t\t\tpairs, err = f.top(tx, topOptions{})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"executing top after roaring import: %v\", err)\n\t\t\t}\n\t\t\tif !reflect.DeepEqual(expPairs, pairs) {\n\t\t\t\tt.Fatalf(\"post Roaring:\\n  exp: %v\\n  got: %v\\n\", expPairs, pairs)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc toRowsCols(roaring []uint64) (rowIDs, colIDs []uint64) {\n\trowIDs, colIDs = make([]uint64, len(roaring)), make([]uint64, len(roaring))\n\tfor i, bit := range roaring {\n\t\trowIDs[i] = bit / ShardWidth\n\t\tcolIDs[i] = bit % ShardWidth\n\t}\n\treturn rowIDs, colIDs\n}\n\nfunc calcTop(rowIDs, colIDs []uint64) []Pair {\n\tif len(rowIDs) != len(colIDs) {\n\t\tPanicOn(\"row and col ids must be of equal len\")\n\t}\n\t// make map of rowID to colID set in order to dedup\n\tcounts := make(map[uint64]map[uint64]struct{})\n\tfor i := 0; i < len(rowIDs); i++ {\n\t\trow, col := rowIDs[i], colIDs[i]\n\t\tm, ok := counts[row]\n\t\tif !ok {\n\t\t\tm = make(map[uint64]struct{})\n\t\t\tcounts[row] = m\n\t\t}\n\t\tm[col] = struct{}{}\n\t}\n\n\t// build slice of pairs from map\n\tret := make([]Pair, 0)\n\tfor row, cols := range counts {\n\t\tret = append(ret, Pair{ID: row, Count: uint64(len(cols))})\n\t}\n\n\t// reverse sort by count\n\tsort.Slice(ret, func(i, j int) bool { return ret[i].Count > ret[j].Count })\n\treturn ret\n}\n\n// calcExpected takes a number of slices of uint64 represented data to be added\n// to a fragment. It calculates which rows and bits set in each row would be\n// expected after importing that data, and returns a [][]uint64 where the index\n// into the first slice is row id.\nfunc calcExpected(inputs ...[]uint64) [][]uint64 {\n\t// create map of row id to set of column values in that row.\n\trows := make(map[uint64]map[uint64]struct{})\n\tvar maxrow uint64\n\tfor _, input := range inputs {\n\t\tfor _, val := range input {\n\t\t\trow := val / ShardWidth\n\t\t\tif row > maxrow {\n\t\t\t\tmaxrow = row\n\t\t\t}\n\t\t\tm, ok := rows[row]\n\t\t\tif !ok {\n\t\t\t\tm = make(map[uint64]struct{})\n\t\t\t\trows[row] = m\n\t\t\t}\n\t\t\tm[val%ShardWidth] = struct{}{}\n\t\t}\n\t}\n\n\t// initialize ret slice\n\tret := make([][]uint64, maxrow+1)\n\tfor i := range ret {\n\t\tret[i] = make([]uint64, 0)\n\t}\n\n\t// populate ret slices from rows map\n\tfor row, vals := range rows {\n\t\tfor val := range vals {\n\t\t\tret[row] = append(ret[row], val)\n\t\t}\n\t}\n\n\t// sort ret slices\n\tfor _, slice := range ret {\n\t\tsort.Slice(slice, func(i int, j int) bool { return slice[i] < slice[j] })\n\t}\n\n\treturn ret\n}\n\nfunc TestFragmentRowIterator(t *testing.T) {\n\tt.Run(\"basic\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeRanked, DefaultCacheSize))\n\t\t_ = idx\n\n\t\tdefer f.Clean(t)\n\n\t\tf.mustSetBits(tx, 0, 0)\n\t\tf.mustSetBits(tx, 1, 0)\n\t\tf.mustSetBits(tx, 2, 0)\n\t\tf.mustSetBits(tx, 3, 0)\n\n\t\titer, err := f.rowIterator(tx, false)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tfor i := uint64(0); i < 4; i++ {\n\t\t\trow, id, _, wrapped, err := iter.Next()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif id != i {\n\t\t\t\tt.Fatalf(\"expected row %d but got %d\", i, id)\n\t\t\t}\n\t\t\tif wrapped {\n\t\t\t\tt.Fatalf(\"shouldn't have wrapped\")\n\t\t\t}\n\t\t\tif !reflect.DeepEqual(row.Columns(), []uint64{0}) {\n\t\t\t\tt.Fatalf(\"got wrong columns back on iteration %d - should just be 0 but %v\", i, row.Columns())\n\t\t\t}\n\t\t}\n\t\trow, id, _, wrapped, err := iter.Next()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif row != nil {\n\t\t\tt.Fatalf(\"row should be nil after iterator is exhausted, got %v\", row.Columns())\n\t\t}\n\t\tif id != 0 {\n\t\t\tt.Fatalf(\"id should be 0 after iterator is exhausted, got %d\", id)\n\t\t}\n\t\tif !wrapped {\n\t\t\tt.Fatalf(\"wrapped should be true after iterator is exhausted\")\n\t\t}\n\t})\n\n\tt.Run(\"skipped rows\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeRanked, DefaultCacheSize))\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\n\t\tf.mustSetBits(tx, 1, 0)\n\t\tf.mustSetBits(tx, 3, 0)\n\t\tf.mustSetBits(tx, 5, 0)\n\t\tf.mustSetBits(tx, 7, 0)\n\n\t\titer, err := f.rowIterator(tx, false)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tfor i := uint64(1); i < 8; i += 2 {\n\t\t\trow, id, _, wrapped, err := iter.Next()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif id != i {\n\t\t\t\tt.Fatalf(\"expected row %d but got %d\", i, id)\n\t\t\t}\n\t\t\tif wrapped {\n\t\t\t\tt.Fatalf(\"shouldn't have wrapped\")\n\t\t\t}\n\t\t\tif !reflect.DeepEqual(row.Columns(), []uint64{0}) {\n\t\t\t\tt.Fatalf(\"got wrong columns back on iteration %d - should just be 0 but %v\", i, row.Columns())\n\t\t\t}\n\t\t}\n\t\trow, id, _, wrapped, err := iter.Next()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif row != nil {\n\t\t\tt.Fatalf(\"row should be nil after iterator is exhausted, got %v\", row.Columns())\n\t\t}\n\t\tif id != 0 {\n\t\t\tt.Fatalf(\"id should be 0 after iterator is exhausted, got %d\", id)\n\t\t}\n\t\tif !wrapped {\n\t\t\tt.Fatalf(\"wrapped should be true after iterator is exhausted\")\n\t\t}\n\t})\n\n\tt.Run(\"basic wrapped\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeRanked, DefaultCacheSize))\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\n\t\tf.mustSetBits(tx, 0, 0)\n\t\tf.mustSetBits(tx, 1, 0)\n\t\tf.mustSetBits(tx, 2, 0)\n\t\tf.mustSetBits(tx, 3, 0)\n\n\t\titer, err := f.rowIterator(tx, true)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tfor i := uint64(0); i < 5; i++ {\n\t\t\trow, id, _, wrapped, err := iter.Next()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif id != i%4 {\n\t\t\t\tt.Fatalf(\"expected row %d but got %d\", i%4, id)\n\t\t\t}\n\t\t\tif wrapped && i < 4 {\n\t\t\t\tt.Fatalf(\"shouldn't have wrapped\")\n\t\t\t} else if !wrapped && i >= 4 {\n\t\t\t\tt.Fatalf(\"should have wrapped\")\n\t\t\t}\n\t\t\tif !reflect.DeepEqual(row.Columns(), []uint64{0}) {\n\t\t\t\tt.Fatalf(\"got wrong columns back on iteration %d - should just be 0 but %v\", i, row.Columns())\n\t\t\t}\n\t\t}\n\t})\n\n\tt.Run(\"skipped rows wrapped\", func(t *testing.T) {\n\t\tf, _, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeRanked, DefaultCacheSize))\n\t\tdefer f.Clean(t)\n\n\t\tf.mustSetBits(tx, 1, 0)\n\t\tf.mustSetBits(tx, 3, 0)\n\t\tf.mustSetBits(tx, 5, 0)\n\t\tf.mustSetBits(tx, 7, 0)\n\n\t\titer, err := f.rowIterator(tx, true)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tfor i := uint64(1); i < 10; i += 2 {\n\t\t\trow, id, _, wrapped, err := iter.Next()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif id != i%8 {\n\t\t\t\tt.Errorf(\"expected row %d but got %d\", i%8, id)\n\t\t\t}\n\t\t\tif wrapped && i < 8 {\n\t\t\t\tt.Errorf(\"shouldn't have wrapped\")\n\t\t\t} else if !wrapped && i >= 8 {\n\t\t\t\tt.Errorf(\"should have wrapped\")\n\t\t\t}\n\t\t\tif !reflect.DeepEqual(row.Columns(), []uint64{0}) {\n\t\t\t\tt.Fatalf(\"got wrong columns back on iteration %d - should just be 0 but %v\", i, row.Columns())\n\t\t\t}\n\t\t}\n\t})\n}\n\n// same, with commits\nfunc TestFragmentRowIterator_WithTxCommit(t *testing.T) {\n\tt.Run(\"basic\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeRanked, DefaultCacheSize))\n\t\t_ = idx\n\n\t\tdefer f.Clean(t)\n\n\t\tf.mustSetBits(tx, 0, 0)\n\t\tf.mustSetBits(tx, 1, 0)\n\t\tf.mustSetBits(tx, 2, 0)\n\t\tf.mustSetBits(tx, 3, 0)\n\n\t\tPanicOn(tx.Commit())\n\t\ttx = idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: f.shard})\n\t\tdefer tx.Rollback()\n\n\t\titer, err := f.rowIterator(tx, false)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tfor i := uint64(0); i < 4; i++ {\n\t\t\trow, id, _, wrapped, err := iter.Next()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif id != i {\n\t\t\t\tt.Fatalf(\"expected row %d but got %d\", i, id)\n\t\t\t}\n\t\t\tif wrapped {\n\t\t\t\tt.Fatalf(\"shouldn't have wrapped\")\n\t\t\t}\n\t\t\tif !reflect.DeepEqual(row.Columns(), []uint64{0}) {\n\t\t\t\tt.Fatalf(\"got wrong columns back on iteration %d - should just be 0 but %v\", i, row.Columns())\n\t\t\t}\n\t\t}\n\t\trow, id, _, wrapped, err := iter.Next()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif row != nil {\n\t\t\tt.Fatalf(\"row should be nil after iterator is exhausted, got %v\", row.Columns())\n\t\t}\n\t\tif id != 0 {\n\t\t\tt.Fatalf(\"id should be 0 after iterator is exhausted, got %d\", id)\n\t\t}\n\t\tif !wrapped {\n\t\t\tt.Fatalf(\"wrapped should be true after iterator is exhausted\")\n\t\t}\n\t})\n\n\tt.Run(\"skipped rows\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeRanked, DefaultCacheSize))\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\n\t\tf.mustSetBits(tx, 1, 0)\n\t\tf.mustSetBits(tx, 3, 0)\n\t\tf.mustSetBits(tx, 5, 0)\n\t\tf.mustSetBits(tx, 7, 0)\n\n\t\tPanicOn(tx.Commit())\n\t\ttx = idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: f.shard})\n\t\tdefer tx.Rollback()\n\n\t\titer, err := f.rowIterator(tx, false)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tfor i := uint64(1); i < 8; i += 2 {\n\t\t\trow, id, _, wrapped, err := iter.Next()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif id != i {\n\t\t\t\tt.Fatalf(\"expected row %d but got %d\", i, id)\n\t\t\t}\n\t\t\tif wrapped {\n\t\t\t\tt.Fatalf(\"shouldn't have wrapped\")\n\t\t\t}\n\t\t\tif !reflect.DeepEqual(row.Columns(), []uint64{0}) {\n\t\t\t\tt.Fatalf(\"got wrong columns back on iteration %d - should just be 0 but %v\", i, row.Columns())\n\t\t\t}\n\t\t}\n\t\trow, id, _, wrapped, err := iter.Next()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif row != nil {\n\t\t\tt.Fatalf(\"row should be nil after iterator is exhausted, got %v\", row.Columns())\n\t\t}\n\t\tif id != 0 {\n\t\t\tt.Fatalf(\"id should be 0 after iterator is exhausted, got %d\", id)\n\t\t}\n\t\tif !wrapped {\n\t\t\tt.Fatalf(\"wrapped should be true after iterator is exhausted\")\n\t\t}\n\t})\n\n\tt.Run(\"basic wrapped\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeRanked, DefaultCacheSize))\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\n\t\tf.mustSetBits(tx, 0, 0)\n\t\tf.mustSetBits(tx, 1, 0)\n\t\tf.mustSetBits(tx, 2, 0)\n\t\tf.mustSetBits(tx, 3, 0)\n\n\t\tPanicOn(tx.Commit())\n\t\ttx = idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: f.shard})\n\t\tdefer tx.Rollback()\n\n\t\titer, err := f.rowIterator(tx, true)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tfor i := uint64(0); i < 5; i++ {\n\t\t\trow, id, _, wrapped, err := iter.Next()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif id != i%4 {\n\t\t\t\tt.Fatalf(\"expected row %d but got %d\", i%4, id)\n\t\t\t}\n\t\t\tif wrapped && i < 4 {\n\t\t\t\tt.Fatalf(\"shouldn't have wrapped\")\n\t\t\t} else if !wrapped && i >= 4 {\n\t\t\t\tt.Fatalf(\"should have wrapped\")\n\t\t\t}\n\t\t\tif !reflect.DeepEqual(row.Columns(), []uint64{0}) {\n\t\t\t\tt.Fatalf(\"got wrong columns back on iteration %d - should just be 0 but %v\", i, row.Columns())\n\t\t\t}\n\t\t}\n\t})\n\n\tt.Run(\"skipped rows wrapped\", func(t *testing.T) {\n\t\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeRanked, DefaultCacheSize))\n\t\t_ = idx\n\t\tdefer f.Clean(t)\n\n\t\tf.mustSetBits(tx, 1, 0)\n\t\tf.mustSetBits(tx, 3, 0)\n\t\tf.mustSetBits(tx, 5, 0)\n\t\tf.mustSetBits(tx, 7, 0)\n\n\t\tPanicOn(tx.Commit())\n\t\ttx = idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: f.shard})\n\t\tdefer tx.Rollback()\n\n\t\titer, err := f.rowIterator(tx, true)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tfor i := uint64(1); i < 10; i += 2 {\n\t\t\trow, id, _, wrapped, err := iter.Next()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if row == nil {\n\t\t\t\tt.Fatal(\"expected row\")\n\t\t\t}\n\t\t\tif id != i%8 {\n\t\t\t\tt.Errorf(\"expected row %d but got %d\", i%8, id)\n\t\t\t}\n\t\t\tif wrapped && i < 8 {\n\t\t\t\tt.Errorf(\"shouldn't have wrapped\")\n\t\t\t} else if !wrapped && i >= 8 {\n\t\t\t\tt.Errorf(\"should have wrapped\")\n\t\t\t}\n\t\t\tif !reflect.DeepEqual(row.Columns(), []uint64{0}) {\n\t\t\t\tt.Fatalf(\"got wrong columns back on iteration %d - should just be 0 but got %v\", i, row.Columns())\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc TestFragmentPositionsForValue(t *testing.T) {\n\tf, _, _ := mustOpenFragment(t, OptFieldTypeSet(CacheTypeNone, 0))\n\tdefer f.Clean(t)\n\n\ttests := []struct {\n\t\tcolumnID uint64\n\t\tbitDepth uint64\n\t\tvalue    int64\n\t\tclear    bool\n\t\ttoSet    []uint64\n\t\ttoClear  []uint64\n\t}{\n\t\t{\n\t\t\tcolumnID: 0,\n\t\t\tbitDepth: 1,\n\t\t\tvalue:    0,\n\t\t\ttoSet:    []uint64{0},                          // exists bit only\n\t\t\ttoClear:  []uint64{ShardWidth, ShardWidth * 2}, // sign bit & 1-position\n\t\t},\n\t\t{\n\t\t\tcolumnID: 0,\n\t\t\tbitDepth: 3,\n\t\t\tvalue:    0,\n\t\t\ttoSet:    []uint64{0},                                                              // exists bit only\n\t\t\ttoClear:  []uint64{ShardWidth * 1, ShardWidth * 2, ShardWidth * 3, ShardWidth * 4}, // sign bit, 1, 2, 4\n\t\t},\n\t\t{\n\t\t\tcolumnID: 1,\n\t\t\tbitDepth: 3,\n\t\t\tvalue:    0,\n\t\t\ttoSet:    []uint64{1},                                                                    // exists bit only\n\t\t\ttoClear:  []uint64{ShardWidth + 1, ShardWidth*2 + 1, ShardWidth*3 + 1, ShardWidth*4 + 1}, // sign bit, 1, 2, 4\n\t\t},\n\t\t{\n\t\t\tcolumnID: 0,\n\t\t\tbitDepth: 1,\n\t\t\tvalue:    1,\n\t\t\ttoSet:    []uint64{0, ShardWidth * 2}, // exists bit, 1\n\t\t\ttoClear:  []uint64{ShardWidth},        // sign bit only\n\t\t},\n\t\t{\n\t\t\tcolumnID: 0,\n\t\t\tbitDepth: 4,\n\t\t\tvalue:    10,\n\t\t\ttoSet:    []uint64{0, ShardWidth * 3, ShardWidth * 5},              // exists bit, 2, 8\n\t\t\ttoClear:  []uint64{ShardWidth * 1, ShardWidth * 2, ShardWidth * 4}, // sign bit, 1, 4\n\t\t},\n\t\t{\n\t\t\tcolumnID: 0,\n\t\t\tbitDepth: 5,\n\t\t\tvalue:    10,\n\t\t\ttoSet:    []uint64{0, ShardWidth * 3, ShardWidth * 5},                              // exists bit, 2, 8\n\t\t\ttoClear:  []uint64{ShardWidth * 1, ShardWidth * 2, ShardWidth * 4, ShardWidth * 6}, // sign bit, 1, 4, 16\n\t\t},\n\t}\n\n\tfor i, test := range tests {\n\t\ttoSet, toClear := make([]uint64, 0), make([]uint64, 0)\n\t\tvar err error\n\t\tt.Run(fmt.Sprintf(\"%d\", i), func(t *testing.T) {\n\t\t\ttoSet, toClear, err = f.positionsForValue(test.columnID, test.bitDepth, test.value, test.clear, toSet, toClear)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"getting positions: %v\", err)\n\t\t\t}\n\n\t\t\tif len(toSet) != len(test.toSet) || len(toClear) != len(test.toClear) {\n\t\t\t\tt.Fatalf(\"differing lengths (exp/got): set\\n%v\\n%v\\nclear\\n%v\\n%v\\n\", test.toSet, toSet, test.toClear, toClear)\n\t\t\t}\n\t\t\tfor i := 0; i < len(toSet); i++ {\n\t\t\t\tif toSet[i] != test.toSet[i] {\n\t\t\t\t\tt.Errorf(\"toSet don't match at %d - exp:%d and got:%d\", i, test.toSet[i], toSet[i])\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor i := 0; i < len(toClear); i++ {\n\t\t\t\tif toClear[i] != test.toClear[i] {\n\t\t\t\t\tt.Errorf(\"toClear don't match at %d - exp:%d and got:%d\", i, test.toClear[i], toClear[i])\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestIntLTRegression(t *testing.T) {\n\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeNone, 0))\n\t_ = idx\n\tdefer f.Clean(t)\n\n\t_, err := f.setValue(tx, 1, 6, 33)\n\tif err != nil {\n\t\tt.Fatalf(\"setting value: %v\", err)\n\t}\n\n\trow, err := f.rangeOp(tx, pql.LT, 6, 33)\n\tif err != nil {\n\t\tt.Fatalf(\"doing range of: %v\", err)\n\t}\n\n\tif !row.IsEmpty() {\n\t\tt.Errorf(\"expected nothing, but got: %v\", row.Columns())\n\t}\n}\n\nfunc sliceEq(x, y []uint64) bool {\n\tif len(x) == 0 {\n\t\tx = nil\n\t}\n\tif len(y) == 0 {\n\t\ty = nil\n\t}\n\treturn reflect.DeepEqual(x, y)\n}\n\nfunc TestFragmentBSIUnsigned(t *testing.T) {\n\tshard := uint64(0)\n\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeNone, 0))\n\tdefer f.Clean(t)\n\n\t// Number of bits to test.\n\tconst k = 6\n\n\t// Load all numbers into an effectively diagonal matrix.\n\tfor i := 0; i < 1<<k; i++ {\n\t\tok, err := f.setValue(tx, uint64(i), k, int64(i))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"failed to set col %d to %d: %v\", uint64(i), int64(i), err)\n\t\t} else if !ok {\n\t\t\tt.Fatalf(\"no change when setting col %d to %d\", uint64(i), int64(i))\n\t\t}\n\t}\n\tPanicOn(tx.Commit()) // t.Run beolow on different goro and so need their own Tx anyway.\n\n\t// Generate a list of columns.\n\tcols := make([]uint64, 1<<k)\n\tfor i := range cols {\n\t\tcols[i] = uint64(i)\n\t}\n\n\t// Minimum and maximum checking bounds.\n\t// This is mostly arbitrary.\n\tminCheck, maxCheck := -3, 1<<(k+1)\n\n\tt.Run(\"<\", func(t *testing.T) {\n\n\t\ttx := idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: shard})\n\t\tdefer tx.Rollback()\n\n\t\tfor i := minCheck; i < maxCheck; i++ {\n\t\t\trow, err := f.rangeLT(tx, k, int64(i), false)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"failed to query fragment: %v\", err)\n\t\t\t}\n\t\t\tvar expect []uint64\n\t\t\tswitch {\n\t\t\tcase i < 0:\n\t\t\tcase i < len(cols):\n\t\t\t\texpect = cols[:i]\n\t\t\tdefault:\n\t\t\t\texpect = cols\n\t\t\t}\n\t\t\tgot := row.Columns()\n\t\t\tif !sliceEq(expect, got) {\n\t\t\t\tt.Errorf(\"expected %v but got %v for x < %d\", expect, got, i)\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\"<=\", func(t *testing.T) {\n\n\t\ttx := idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: shard})\n\t\tdefer tx.Rollback()\n\n\t\tfor i := minCheck; i < maxCheck; i++ {\n\t\t\trow, err := f.rangeLT(tx, k, int64(i), true)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"failed to query fragment: %v\", err)\n\t\t\t}\n\t\t\tvar expect []uint64\n\t\t\tswitch {\n\t\t\tcase i < 0:\n\t\t\tcase i < len(cols)-1:\n\t\t\t\texpect = cols[:i+1]\n\t\t\tdefault:\n\t\t\t\texpect = cols\n\t\t\t}\n\t\t\tgot := row.Columns()\n\t\t\tif !sliceEq(expect, got) {\n\t\t\t\tt.Errorf(\"expected %v but got %v for x <= %d\", expect, got, i)\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\">\", func(t *testing.T) {\n\n\t\ttx := idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: shard})\n\t\tdefer tx.Rollback()\n\n\t\tfor i := minCheck; i < maxCheck; i++ {\n\t\t\trow, err := f.rangeGT(tx, k, int64(i), false)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"failed to query fragment: %v\", err)\n\t\t\t}\n\t\t\tvar expect []uint64\n\t\t\tswitch {\n\t\t\tcase i < 0:\n\t\t\t\texpect = cols\n\t\t\tcase i < len(cols)-1:\n\t\t\t\texpect = cols[i+1:]\n\t\t\tdefault:\n\t\t\t}\n\t\t\tgot := row.Columns()\n\t\t\tif !sliceEq(expect, got) {\n\t\t\t\tt.Errorf(\"expected %v but got %v for x > %d\", expect, got, i)\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\">=\", func(t *testing.T) {\n\t\ttx := idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: shard})\n\t\tdefer tx.Rollback()\n\n\t\tfor i := minCheck; i < maxCheck; i++ {\n\t\t\trow, err := f.rangeGT(tx, k, int64(i), true)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"failed to query fragment: %v\", err)\n\t\t\t}\n\t\t\tvar expect []uint64\n\t\t\tswitch {\n\t\t\tcase i < 0:\n\t\t\t\texpect = cols\n\t\t\tcase i < len(cols):\n\t\t\t\texpect = cols[i:]\n\t\t\tdefault:\n\t\t\t}\n\t\t\tgot := row.Columns()\n\t\t\tif !sliceEq(expect, got) {\n\t\t\t\tt.Errorf(\"expected %v but got %v for x >= %d\", expect, got, i)\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\"Range\", func(t *testing.T) {\n\n\t\ttx := idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: shard})\n\t\tdefer tx.Rollback()\n\n\t\tfor i := minCheck; i < maxCheck; i++ {\n\t\t\tfor j := i; j < maxCheck; j++ {\n\t\t\t\trow, err := f.rangeBetween(tx, k, int64(i), int64(j))\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"failed to query fragment: %v\", err)\n\t\t\t\t}\n\t\t\t\tvar lower, upper int\n\t\t\t\tswitch {\n\t\t\t\tcase i < 0:\n\t\t\t\t\tlower = 0\n\t\t\t\tcase i > len(cols):\n\t\t\t\t\tlower = len(cols)\n\t\t\t\tdefault:\n\t\t\t\t\tlower = i\n\t\t\t\t}\n\t\t\t\tswitch {\n\t\t\t\tcase j < 0:\n\t\t\t\t\tupper = 0\n\t\t\t\tcase j >= len(cols):\n\t\t\t\t\tupper = len(cols)\n\t\t\t\tdefault:\n\t\t\t\t\tupper = j + 1\n\t\t\t\t}\n\t\t\t\texpect := cols[lower:upper]\n\t\t\t\tgot := row.Columns()\n\t\t\t\tif !sliceEq(expect, got) {\n\t\t\t\t\tt.Errorf(\"expected %v but got %v for %d <= x <= %d\", expect, got, i, j)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\"==\", func(t *testing.T) {\n\n\t\ttx := idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: shard})\n\t\tdefer tx.Rollback()\n\n\t\tfor i := minCheck; i < maxCheck; i++ {\n\t\t\trow, err := f.rangeEQ(tx, k, int64(i))\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"failed to query fragment: %v\", err)\n\t\t\t}\n\t\t\tvar expect []uint64\n\t\t\tif i >= 0 && i < len(cols) {\n\t\t\t\texpect = cols[i : i+1]\n\t\t\t}\n\t\t\tgot := row.Columns()\n\t\t\tif !sliceEq(expect, got) {\n\t\t\t\tt.Errorf(\"expected %v but got %v for x == %d\", expect, got, i)\n\t\t\t}\n\t\t}\n\t})\n}\n\n// same, WithTxCommit version\nfunc TestFragmentBSIUnsigned_WithTxCommit(t *testing.T) {\n\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeNone, 0))\n\t_ = idx\n\tdefer f.Clean(t)\n\n\t// Number of bits to test.\n\tconst k = 6\n\n\t// Load all numbers into an effectively diagonal matrix.\n\tfor i := 0; i < 1<<k; i++ {\n\t\tok, err := f.setValue(tx, uint64(i), k, int64(i))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"failed to set col %d to %d: %v\", uint64(i), int64(i), err)\n\t\t} else if !ok {\n\t\t\tt.Fatalf(\"no change when setting col %d to %d\", uint64(i), int64(i))\n\t\t}\n\t}\n\n\tPanicOn(tx.Commit())\n\ttx = idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: f.shard})\n\tdefer tx.Rollback()\n\n\t// Generate a list of columns.\n\tcols := make([]uint64, 1<<k)\n\tfor i := range cols {\n\t\tcols[i] = uint64(i)\n\t}\n\n\t// Minimum and maximum checking bounds.\n\t// This is mostly arbitrary.\n\tminCheck, maxCheck := -3, 1<<(k+1)\n\n\tt.Run(\"<\", func(t *testing.T) {\n\t\tfor i := minCheck; i < maxCheck; i++ {\n\t\t\trow, err := f.rangeLT(tx, k, int64(i), false)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"failed to query fragment: %v\", err)\n\t\t\t}\n\t\t\tvar expect []uint64\n\t\t\tswitch {\n\t\t\tcase i < 0:\n\t\t\tcase i < len(cols):\n\t\t\t\texpect = cols[:i]\n\t\t\tdefault:\n\t\t\t\texpect = cols\n\t\t\t}\n\t\t\tgot := row.Columns()\n\t\t\tif !sliceEq(expect, got) {\n\t\t\t\tt.Errorf(\"expected %v but got %v for x < %d\", expect, got, i)\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\"<=\", func(t *testing.T) {\n\t\tfor i := minCheck; i < maxCheck; i++ {\n\t\t\trow, err := f.rangeLT(tx, k, int64(i), true)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"failed to query fragment: %v\", err)\n\t\t\t}\n\t\t\tvar expect []uint64\n\t\t\tswitch {\n\t\t\tcase i < 0:\n\t\t\tcase i < len(cols)-1:\n\t\t\t\texpect = cols[:i+1]\n\t\t\tdefault:\n\t\t\t\texpect = cols\n\t\t\t}\n\t\t\tgot := row.Columns()\n\t\t\tif !sliceEq(expect, got) {\n\t\t\t\tt.Errorf(\"expected %v but got %v for x <= %d\", expect, got, i)\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\">\", func(t *testing.T) {\n\t\tfor i := minCheck; i < maxCheck; i++ {\n\t\t\trow, err := f.rangeGT(tx, k, int64(i), false)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"failed to query fragment: %v\", err)\n\t\t\t}\n\t\t\tvar expect []uint64\n\t\t\tswitch {\n\t\t\tcase i < 0:\n\t\t\t\texpect = cols\n\t\t\tcase i < len(cols)-1:\n\t\t\t\texpect = cols[i+1:]\n\t\t\tdefault:\n\t\t\t}\n\t\t\tgot := row.Columns()\n\t\t\tif !sliceEq(expect, got) {\n\t\t\t\tt.Errorf(\"expected %v but got %v for x > %d\", expect, got, i)\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\">=\", func(t *testing.T) {\n\t\tfor i := minCheck; i < maxCheck; i++ {\n\t\t\trow, err := f.rangeGT(tx, k, int64(i), true)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"failed to query fragment: %v\", err)\n\t\t\t}\n\t\t\tvar expect []uint64\n\t\t\tswitch {\n\t\t\tcase i < 0:\n\t\t\t\texpect = cols\n\t\t\tcase i < len(cols):\n\t\t\t\texpect = cols[i:]\n\t\t\tdefault:\n\t\t\t}\n\t\t\tgot := row.Columns()\n\t\t\tif !sliceEq(expect, got) {\n\t\t\t\tt.Errorf(\"expected %v but got %v for x >= %d\", expect, got, i)\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\"Range\", func(t *testing.T) {\n\t\tfor i := minCheck; i < maxCheck; i++ {\n\t\t\tfor j := i; j < maxCheck; j++ {\n\t\t\t\trow, err := f.rangeBetween(tx, k, int64(i), int64(j))\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"failed to query fragment: %v\", err)\n\t\t\t\t}\n\t\t\t\tvar lower, upper int\n\t\t\t\tswitch {\n\t\t\t\tcase i < 0:\n\t\t\t\t\tlower = 0\n\t\t\t\tcase i > len(cols):\n\t\t\t\t\tlower = len(cols)\n\t\t\t\tdefault:\n\t\t\t\t\tlower = i\n\t\t\t\t}\n\t\t\t\tswitch {\n\t\t\t\tcase j < 0:\n\t\t\t\t\tupper = 0\n\t\t\t\tcase j >= len(cols):\n\t\t\t\t\tupper = len(cols)\n\t\t\t\tdefault:\n\t\t\t\t\tupper = j + 1\n\t\t\t\t}\n\t\t\t\texpect := cols[lower:upper]\n\t\t\t\tgot := row.Columns()\n\t\t\t\tif !sliceEq(expect, got) {\n\t\t\t\t\tt.Errorf(\"expected %v but got %v for %d <= x <= %d\", expect, got, i, j)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\"==\", func(t *testing.T) {\n\t\tfor i := minCheck; i < maxCheck; i++ {\n\t\t\trow, err := f.rangeEQ(tx, k, int64(i))\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"failed to query fragment: %v\", err)\n\t\t\t}\n\t\t\tvar expect []uint64\n\t\t\tif i >= 0 && i < len(cols) {\n\t\t\t\texpect = cols[i : i+1]\n\t\t\t}\n\t\t\tgot := row.Columns()\n\t\t\tif !sliceEq(expect, got) {\n\t\t\t\tt.Errorf(\"expected %v but got %v for x == %d\", expect, got, i)\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc TestFragmentBSISigned(t *testing.T) {\n\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeNone, 0))\n\t_ = idx\n\tdefer f.Clean(t)\n\n\t// Number of bits to test.\n\tconst k = 6\n\n\t// Load all numbers into an effectively diagonal matrix.\n\tminVal, maxVal := 1-(1<<k), (1<<k)-1\n\tfor i := minVal; i <= maxVal; i++ {\n\t\tok, err := f.setValue(tx, uint64(i-minVal), k, int64(i))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"failed to set col %d to %d: %v\", uint64(i-minVal), int64(i), err)\n\t\t} else if !ok {\n\t\t\tt.Fatalf(\"no change when setting col %d to %d\", uint64(i-minVal), int64(i))\n\t\t}\n\t}\n\n\tPanicOn(tx.Commit())\n\ttx = idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: f.shard})\n\tdefer tx.Rollback()\n\n\t// Generate a list of columns.\n\tcols := make([]uint64, (maxVal-minVal)+1)\n\tfor i := range cols {\n\t\tcols[i] = uint64(i)\n\t}\n\n\t// Minimum and maximum checking bounds.\n\t// This is mostly arbitrary.\n\tminCheck, maxCheck := 2*minVal, 2*maxVal\n\n\tt.Run(\"<\", func(t *testing.T) {\n\t\tfor i := minCheck; i < maxCheck; i++ {\n\t\t\trow, err := f.rangeLT(tx, k, int64(i), false)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"failed to query fragment: %v\", err)\n\t\t\t}\n\t\t\tvar expect []uint64\n\t\t\tswitch {\n\t\t\tcase i < minVal:\n\t\t\tcase i <= maxVal:\n\t\t\t\texpect = cols[:i-minVal]\n\t\t\tdefault:\n\t\t\t\texpect = cols\n\t\t\t}\n\t\t\tgot := row.Columns()\n\t\t\tif !sliceEq(expect, got) {\n\t\t\t\tt.Errorf(\"expected %v but got %v for x < %d\", expect, got, i)\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\"<=\", func(t *testing.T) {\n\t\tfor i := minCheck; i < maxCheck; i++ {\n\t\t\trow, err := f.rangeLT(tx, k, int64(i), true)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"failed to query fragment: %v\", err)\n\t\t\t}\n\t\t\tvar expect []uint64\n\t\t\tswitch {\n\t\t\tcase i < minVal:\n\t\t\tcase i < maxVal:\n\t\t\t\texpect = cols[:(i-minVal)+1]\n\t\t\tdefault:\n\t\t\t\texpect = cols\n\t\t\t}\n\t\t\tgot := row.Columns()\n\t\t\tif !sliceEq(expect, got) {\n\t\t\t\tt.Errorf(\"expected %v but got %v for x <= %d\", expect, got, i)\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\">\", func(t *testing.T) {\n\t\tfor i := minCheck; i < maxCheck; i++ {\n\t\t\trow, err := f.rangeGT(tx, k, int64(i), false)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"failed to query fragment: %v\", err)\n\t\t\t}\n\t\t\tvar expect []uint64\n\t\t\tswitch {\n\t\t\tcase i < minVal:\n\t\t\t\texpect = cols\n\t\t\tcase i < maxVal:\n\t\t\t\texpect = cols[(i-minVal)+1:]\n\t\t\tdefault:\n\t\t\t}\n\t\t\tgot := row.Columns()\n\t\t\tif !sliceEq(expect, got) {\n\t\t\t\tt.Errorf(\"expected %v but got %v for x > %d\", expect, got, i)\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\">=\", func(t *testing.T) {\n\t\tfor i := minCheck; i < maxCheck; i++ {\n\t\t\trow, err := f.rangeGT(tx, k, int64(i), true)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"failed to query fragment: %v\", err)\n\t\t\t}\n\t\t\tvar expect []uint64\n\t\t\tswitch {\n\t\t\tcase i < minVal:\n\t\t\t\texpect = cols\n\t\t\tcase i <= maxVal:\n\t\t\t\texpect = cols[i-minVal:]\n\t\t\tdefault:\n\t\t\t}\n\t\t\tgot := row.Columns()\n\t\t\tif !sliceEq(expect, got) {\n\t\t\t\tt.Errorf(\"expected %v but got %v for x >= %d\", expect, got, i)\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\"Range\", func(t *testing.T) {\n\t\tfor i := minCheck; i < maxCheck; i++ {\n\t\t\tfor j := i; j < maxCheck; j++ {\n\t\t\t\trow, err := f.rangeBetween(tx, k, int64(i), int64(j))\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"failed to query fragment: %v\", err)\n\t\t\t\t}\n\t\t\t\tvar lower, upper int\n\t\t\t\tswitch {\n\t\t\t\tcase i < minVal:\n\t\t\t\t\tlower = 0\n\t\t\t\tcase i > maxVal:\n\t\t\t\t\tlower = len(cols)\n\t\t\t\tdefault:\n\t\t\t\t\tlower = i - minVal\n\t\t\t\t}\n\t\t\t\tswitch {\n\t\t\t\tcase j < minVal:\n\t\t\t\t\tupper = 0\n\t\t\t\tcase j > maxVal:\n\t\t\t\t\tupper = len(cols)\n\t\t\t\tdefault:\n\t\t\t\t\tupper = (j - minVal) + 1\n\t\t\t\t}\n\t\t\t\texpect := cols[lower:upper]\n\t\t\t\tgot := row.Columns()\n\t\t\t\tif !sliceEq(expect, got) {\n\t\t\t\t\tt.Errorf(\"expected %v but got %v for %d <= x <= %d\", expect, got, i, j)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\"==\", func(t *testing.T) {\n\t\tfor i := minCheck; i < maxCheck; i++ {\n\t\t\trow, err := f.rangeEQ(tx, k, int64(i))\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"failed to query fragment: %v\", err)\n\t\t\t}\n\t\t\tvar expect []uint64\n\t\t\tif i >= minVal && i <= maxVal {\n\t\t\t\texpect = cols[i-minVal : (i-minVal)+1]\n\t\t\t}\n\t\t\tgot := row.Columns()\n\t\t\tif !sliceEq(expect, got) {\n\t\t\t\tt.Errorf(\"expected %v but got %v for x == %d\", expect, got, i)\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc TestImportValueConcurrent(t *testing.T) {\n\tf, idx, tx := mustOpenFragment(t)\n\tdefer f.Clean(t)\n\t// we will be making a new Tx each time, so we can rollback the default provided one.\n\ttx.Rollback()\n\n\teg := &errgroup.Group{}\n\tfor i := 0; i < 4; i++ {\n\t\ti := i\n\t\teg.Go(func() error {\n\t\t\ttx := idx.holder.txf.NewTx(Txo{Write: writable, Index: idx, Fragment: f, Shard: f.shard})\n\t\t\tdefer tx.Rollback()\n\t\t\tfor j := uint64(0); j < 10; j++ {\n\t\t\t\terr := f.importValue(tx, []uint64{j}, []int64{int64(rand.Int63n(1000))}, 10, i%2 == 0)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t}\n\terr := eg.Wait()\n\tif err != nil {\n\t\tt.Fatalf(\"concurrently importing values: %v\", err)\n\t}\n}\n\nfunc TestImportMultipleValues(t *testing.T) {\n\ttests := []struct {\n\t\tcols      []uint64\n\t\tvals      []int64\n\t\tcheckCols []uint64\n\t\tcheckVals []int64\n\t\tdepth     uint64\n\t}{\n\t\t{\n\t\t\tcols:      []uint64{0, 0},\n\t\t\tvals:      []int64{97, 100},\n\t\t\tdepth:     7,\n\t\t\tcheckCols: []uint64{0},\n\t\t\tcheckVals: []int64{100},\n\t\t},\n\t}\n\n\tfor i, test := range tests {\n\t\tt.Run(fmt.Sprintf(\"%d\", i), func(t *testing.T) {\n\t\t\tf, _, tx := mustOpenFragment(t)\n\t\t\tdefer f.Clean(t)\n\n\t\t\terr := f.importValue(tx, test.cols, test.vals, test.depth, false)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"importing values: %v\", err)\n\t\t\t}\n\n\t\t\tfor i := range test.checkCols {\n\t\t\t\tcc, cv := test.checkCols[i], test.checkVals[i]\n\t\t\t\tn, exists, err := f.value(tx, cc, test.depth)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"getting value: %v\", err)\n\t\t\t\t}\n\t\t\t\tif !exists {\n\t\t\t\t\tt.Errorf(\"column %d should exist\", cc)\n\t\t\t\t}\n\t\t\t\tif n != cv {\n\t\t\t\t\tt.Errorf(\"wrong value: %d is not %d\", n, cv)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestImportValueRowCache(t *testing.T) {\n\ttype testCase struct {\n\t\tcols      []uint64\n\t\tvals      []int64\n\t\tcheckCols []uint64\n\t\tdepth     uint64\n\t}\n\ttests := []struct {\n\t\ttc1 testCase\n\t\ttc2 testCase\n\t}{\n\t\t{\n\t\t\ttc1: testCase{\n\t\t\t\tcols:      []uint64{2},\n\t\t\t\tvals:      []int64{1},\n\t\t\t\tdepth:     1,\n\t\t\t\tcheckCols: []uint64{2},\n\t\t\t},\n\t\t\ttc2: testCase{\n\t\t\t\tcols:      []uint64{1000},\n\t\t\t\tvals:      []int64{1},\n\t\t\t\tdepth:     1,\n\t\t\t\tcheckCols: []uint64{2, 1000},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor i, test := range tests {\n\t\tt.Run(fmt.Sprintf(\"%d\", i), func(t *testing.T) {\n\t\t\tf, _, tx := mustOpenFragment(t)\n\t\t\tdefer f.Clean(t)\n\n\t\t\t// First import (tc1)\n\t\t\tif err := f.importValue(tx, test.tc1.cols, test.tc1.vals, test.tc1.depth, false); err != nil {\n\t\t\t\tt.Fatalf(\"importing values: %v\", err)\n\t\t\t}\n\n\t\t\tif r, err := f.rangeOp(tx, pql.GT, test.tc1.depth, 0); err != nil {\n\t\t\t\tt.Error(\"getting range of values\")\n\t\t\t} else if !reflect.DeepEqual(r.Columns(), test.tc1.checkCols) {\n\t\t\t\tt.Errorf(\"wrong column values. expected: %v, but got: %v\", test.tc1.checkCols, r.Columns())\n\t\t\t}\n\n\t\t\t// Second import (tc2)\n\t\t\tif err := f.importValue(tx, test.tc2.cols, test.tc2.vals, test.tc2.depth, false); err != nil {\n\t\t\t\tt.Fatalf(\"importing values: %v\", err)\n\t\t\t}\n\n\t\t\tif r, err := f.rangeOp(tx, pql.GT, test.tc2.depth, 0); err != nil {\n\t\t\t\tt.Error(\"getting range of values\")\n\t\t\t} else if !reflect.DeepEqual(r.Columns(), test.tc2.checkCols) {\n\t\t\t\tt.Errorf(\"wrong column values. expected: %v, but got: %v\", test.tc2.checkCols, r.Columns())\n\t\t\t}\n\t\t})\n\t}\n}\n\n// part of copy-on-write patch: test for races\n// do we see races/corruption around concurrent read/write.\n// especially on writes to the row cache.\nfunc TestFragmentConcurrentReadWrite(t *testing.T) {\n\tf, idx, tx := mustOpenFragment(t, OptFieldTypeSet(CacheTypeRanked, DefaultCacheSize))\n\tdefer f.Clean(t)\n\ttx.Rollback()\n\n\teg := &errgroup.Group{}\n\teg.Go(func() error {\n\n\t\tltx := idx.holder.txf.NewTx(Txo{Write: writable, Index: idx, Fragment: f, Shard: f.shard})\n\n\t\tfor i := uint64(0); i < 1000; i++ {\n\t\t\t_, err := f.setBit(ltx, i%4, i)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Wrap(err, \"setting bit\")\n\t\t\t}\n\t\t}\n\t\tPanicOn(ltx.Commit())\n\t\treturn nil\n\t})\n\n\t// need read-only Tx so as not to block on the writer finishing above.\n\ttx1 := idx.holder.txf.NewTx(Txo{Write: !writable, Index: idx, Fragment: f, Shard: f.shard})\n\tdefer tx1.Rollback()\n\n\tacc := uint64(0)\n\tfor i := uint64(0); i < 100; i++ {\n\t\tr := f.mustRow(tx1, i%4)\n\t\tacc += r.Count()\n\t}\n\tif err := eg.Wait(); err != nil {\n\t\tt.Errorf(\"error from setting a bit: %v\", err)\n\t}\n\n\tt.Logf(\"%d\", acc)\n}\n\nfunc TestFragment_Bug_Q2DoubleDelete(t *testing.T) {\n\tf, idx, tx := mustOpenFragment(t)\n\t_ = idx\n\t// byShardWidth is a map of the same roaring (fragment) data generated\n\t// with different shard widths.\n\t// TODO: a better approach may be to generate this in the test based\n\t// on shard width.\n\tbyShardWidth := make(map[uint64][]byte)\n\t// row/col: 1/1\n\tbyShardWidth[1<<20] = []byte{60, 48, 0, 0, 1, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 24, 0, 0, 0, 1, 0}\n\tbyShardWidth[1<<22] = []byte{60, 48, 0, 0, 1, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 24, 0, 0, 0, 1, 0}\n\n\tvar b []byte\n\tif data, ok := byShardWidth[ShardWidth]; ok {\n\t\tb = data\n\t}\n\n\t_ = idx\n\tdefer f.Clean(t)\n\n\terr := f.importRoaringT(tx, b, false)\n\tif err != nil {\n\t\tt.Fatalf(\"importing roaring: %v\", err)\n\t}\n\n\t//check the bit\n\tres := f.mustRow(tx, 1).Columns()\n\tif len(res) < 1 || f.mustRow(tx, 1).Columns()[0] != 1 {\n\t\tt.Fatalf(\"expecting 1 got: %v\", res)\n\t}\n\t//clear the bit\n\tchanged, _ := f.clearBit(tx, 1, 1)\n\tif !changed {\n\t\tt.Fatalf(\"expected change got %v\", changed)\n\t}\n\n\t//check missing\n\tres = f.mustRow(tx, 1).Columns()\n\tif len(res) != 0 {\n\t\tt.Fatalf(\"expected nothing got %v\", res)\n\t}\n\n\t// import again\n\terr = f.importRoaringT(tx, b, false)\n\tif err != nil {\n\t\tt.Fatalf(\"importing roaring: %v\", err)\n\t}\n\n\t//check\n\tres = f.mustRow(tx, 1).Columns()\n\tif len(res) < 1 || f.mustRow(tx, 1).Columns()[0] != 1 {\n\t\tt.Fatalf(\"again expecting 1 got: %v\", res)\n\t}\n\n\tchanged, _ = f.clearBit(tx, 1, 1)\n\tif !changed {\n\t\tt.Fatalf(\"again expected change got %v\", changed) // again expected change got false\n\t}\n\n\t//check missing\n\tres = f.mustRow(tx, 1).Columns()\n\tif len(res) != 0 {\n\t\tt.Fatalf(\"expected nothing got %v\", res)\n\t}\n}\n\nvar mutexSamplesPrepared sync.Once\n\nfunc requireMutexSampleData(tb testing.TB) {\n\tmutexSamplesPrepared.Do(func() { prepareMutexSampleData(tb) })\n}\n\n// a few mutex tests want common largeish pools of mutex data\ntype mutexSampleData struct {\n\tname           string\n\trng            mutexSampleRange\n\tcolIDs, rowIDs [3][]uint64\n}\n\n// scratchSpace copies the values over corresponding entries in slices,\n// reusing existing storage when possible.\nfunc (m *mutexSampleData) scratchSpace(idx int, cols, rows []uint64) ([]uint64, []uint64) {\n\tcolIDs := m.colIDs[idx]\n\trowIDs := m.rowIDs[idx]\n\n\tif cap(cols) < len(colIDs) {\n\t\tcols = make([]uint64, len(colIDs))\n\t} else {\n\t\tcols = cols[:len(colIDs)]\n\t}\n\tcopy(cols, colIDs)\n\n\tif cap(rows) < len(rowIDs) {\n\t\trows = make([]uint64, len(rowIDs))\n\t} else {\n\t\trows = rows[:len(rowIDs)]\n\t}\n\tcopy(rows, rowIDs)\n\treturn cols, rows\n}\n\n// mutex data wants to exist for differing numbers of rows, and different\n// densities, and reasonably-large N. But we don't want a huge pool of nested\n// maps, so...\ntype mutexSampleRange int16\n\n// density should be able to range from every bit filled to almost no\n// bits filled. So, how many bits per container? How about pow(2, N), where N\n// can be negative. 16 is the highest possible value, and -16 is the lowest,\n// and 0 means about one bit per container on average.\nfunc (m mutexSampleRange) density() int8 {\n\treturn int8(m >> 8)\n}\n\n// bottom 8 bits are log2 of number of rows.\nfunc (m mutexSampleRange) rows() uint32 {\n\treturn uint32(1) << (m & 0x1F)\n}\n\n// just a convenience thing to hide the implementation\nfunc newMutexSampleRange(density int8, rows uint8) mutexSampleRange {\n\tif density > 16 {\n\t\tdensity = 16\n\t}\n\tif density < -16 {\n\t\tdensity = -16\n\t}\n\treturn mutexSampleRange((int16(density) << 8) | int16(rows))\n}\n\nvar sampleMutexData = map[string]*mutexSampleData{}\n\ntype mutexDensity struct {\n\tname    string\n\tdensity int8\n}\n\ntype mutexSize struct {\n\tname string\n\trows uint8\n}\n\nvar mutexDensities = []mutexDensity{\n\t// {\"64K\", 16},\n\t// {\"32K\", 15}, // 50-50\n\t//{\"16K\", 14}, // 1/4\n\t{\"8K\", 13},\n\t// {\"4K\", 12}, // a fair number of things\n\t{\"1K\", 10},\n\t// {\"1\", 0},   // about one per container\n\t// {\"empty\", -14}, // almost none\n}\n\nvar mutexSizes = []mutexSize{\n\t// {\"4r\", 2},\n\t{\"16r\", 4},\n\t// {\"256r\", 8},\n\t// {\"2Kr\", 11},\n\t// {\"65Kr\", 16},\n}\n\nvar mutexCaches = []string{\n\t\"ranked\",\n\t\"none\",\n}\n\nconst mutexSampleDataSize = ShardWidth * len(mutexSampleData{}.colIDs)\n\n// prepareMutexSampleData creates multiple sets of data for each density and\n// number of rows, so that we can test performance when overwriting also.\nfunc prepareMutexSampleData(tb testing.TB) {\n\tmyrand := rand.New(rand.NewSource(9))\n\tfor _, d := range mutexDensities {\n\t\t// at density 16, we want everything to be adjacent.\n\t\t// at density 0, we want about 65k between items.\n\t\t// The average spacing we want is 1<<(16 - density),\n\t\t// so random numbers between 0 and twice that would\n\t\t// be close, but we never want 0, so, subtract 1 from\n\t\t// \"twice that\", then add 1 to the result.\n\t\t//\n\t\t// So for density 16, we compute spacing of 1, then\n\t\t// draw random numbers in [0,1), and add 1 to them.\n\t\tspacing := ((1 << (16 - d.density)) * 2) - 1\n\t\tfor _, s := range mutexSizes {\n\t\t\trng := newMutexSampleRange(d.density, s.rows)\n\t\t\tcol := uint64(0)\n\n\t\t\trows := (int64(1) << s.rows)\n\n\t\t\tcolIDs := make([]uint64, mutexSampleDataSize)\n\t\t\trowIDs := make([]uint64, mutexSampleDataSize)\n\t\t\tdata := &mutexSampleData{name: d.name + \"/\" + s.name, rng: rng}\n\t\t\tprev := uint64(0)\n\t\t\tgenerated := 0\n\t\t\tfor idx := 0; int(prev) < len(data.colIDs); idx++ {\n\t\t\t\tif spacing > 1 {\n\t\t\t\t\tcol += uint64(myrand.Int63n(int64(spacing))) + 1\n\t\t\t\t} else {\n\t\t\t\t\tcol++\n\t\t\t\t}\n\t\t\t\t// can only import one fragment at a time,\n\t\t\t\t// though!\n\t\t\t\tif col/ShardWidth > prev {\n\t\t\t\t\tdata.colIDs[prev] = colIDs[generated:idx:idx]\n\t\t\t\t\tdata.rowIDs[prev] = rowIDs[generated:idx:idx]\n\t\t\t\t\tgenerated = idx\n\t\t\t\t\tprev = col / ShardWidth\n\t\t\t\t\tif int(prev) >= len(data.colIDs) {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\trow := uint64(myrand.Int63n(rows))\n\t\t\t\tcolIDs[idx] = col % ShardWidth\n\t\t\t\trowIDs[idx] = row\n\t\t\t}\n\t\t\tsampleMutexData[data.name] = data\n\t\t}\n\t}\n\td := mutexSampleData{\n\t\tname: \"extra\",\n\t\trowIDs: [3][]uint64{\n\t\t\t{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1},\n\t\t\t{1},\n\t\t\t{2},\n\t\t},\n\t\tcolIDs: [3][]uint64{\n\t\t\t{0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 1, 3, 5, 7, 8, 11, 13, 15, 17, 19, 21, 23},\n\t\t\t{29},\n\t\t\t{33},\n\t\t},\n\t\trng: 2,\n\t}\n\tfor i := 0; i < 16; i++ {\n\t\tif (i % 16) != 4 {\n\t\t\td.colIDs[0][i] += 65536\n\t\t}\n\t}\n\tsampleMutexData[d.name] = &d\n}\n\nvar importBatchSizes = []int{40, 80, 240, 2048}\n\nfunc TestImportMutexSampleData(t *testing.T) {\n\trequireMutexSampleData(t)\n\tvar scratchCols [3][]uint64\n\tvar scratchRows [3][]uint64\n\tfor _, data := range sampleMutexData {\n\t\tfor i := range scratchRows {\n\t\t\tscratchCols[i], scratchRows[i] = data.scratchSpace(i, scratchCols[i], scratchRows[i])\n\t\t}\n\t\tseen := make(map[uint64]struct{})\n\t\tt.Run(data.name, func(t *testing.T) {\n\t\t\tbatchSize := 16384\n\t\t\tf, _, tx := mustOpenFragment(t, OptFieldTypeMutex(DefaultCacheType, DefaultCacheSize))\n\t\t\tdefer f.Clean(t)\n\t\t\t// Set import.\n\t\t\tvar err error\n\n\t\t\tfor i := 0; i < len(scratchCols); i++ {\n\t\t\t\tcols := scratchCols[i]\n\t\t\t\trows := scratchRows[i]\n\t\t\t\tfor _, v := range cols {\n\t\t\t\t\tseen[v] = struct{}{}\n\t\t\t\t}\n\n\t\t\t\tfor j := 0; j < len(cols); j += batchSize {\n\t\t\t\t\tmax := j + batchSize\n\t\t\t\t\tif len(cols) < max {\n\t\t\t\t\t\tmax = len(cols)\n\t\t\t\t\t}\n\t\t\t\t\terr = f.bulkImport(tx, rows[j:max:max], cols[j:max:max], &ImportOptions{})\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tt.Fatalf(\"bulk importing ids [%d/3] [%d:%d]: %v\", i+1, j, max, err)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tcount := uint64(0)\n\t\t\t\tfor k := uint32(0); k < data.rng.rows(); k++ {\n\t\t\t\t\tc := f.mustRow(tx, uint64(k)).Count()\n\t\t\t\t\tcount += c\n\t\t\t\t}\n\t\t\t\tif int(count) != len(seen) {\n\t\t\t\t\tt.Fatalf(\"for %d rows, %d density, import %d/3: expected %d results, got %d\",\n\t\t\t\t\t\tdata.rng.rows(), data.rng.density(), i+1, len(seen), count)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\n// BenchmarkImportMutexSampleData tries to time importing mutex data.\n// The tricky part is defining a meaningful b.N that can apply across\n// different batch sizes, densities, and so on. So, basically, we take\n// b.N, and multiply by 65536, to get \"N containers\" of data, meaning\n// that the amount of data we want to process is independent of all of\n// the other factors. But for sparse data sets, that means rewriting\n// the same data a number of times, which isn't ideal.\nfunc BenchmarkImportMutexSampleData(b *testing.B) {\n\trequireMutexSampleData(b)\n\tvar cols []uint64\n\tvar rows []uint64\n\tvar data *mutexSampleData\n\tvar cache string\n\tvar batchSize int\n\tvar frag *fragment\n\tvar tx Tx\n\tvar idx *Index\n\tbenchmarkOneFragmentImports := func(b *testing.B, idx int) {\n\t\tcols, rows = data.scratchSpace(idx, cols, rows)\n\t\ttoDo := b.N << 16\n\t\tstart := 0\n\t\tfor toDo > 0 {\n\t\t\tmax := start + batchSize\n\t\t\tif len(cols) < max {\n\t\t\t\tmax = len(cols)\n\t\t\t}\n\t\t\terr := frag.bulkImport(tx, rows[start:max:max], cols[start:max:max], &ImportOptions{})\n\t\t\tif err != nil {\n\t\t\t\tb.Fatalf(\"bulk importing ids [%d:%d]: %v\", start, max, err)\n\t\t\t}\n\t\t\ttoDo -= (max - start)\n\t\t\tstart = max\n\t\t\tif start >= len(cols) {\n\t\t\t\tstart = 0\n\t\t\t\tb.StopTimer()\n\t\t\t\t// recreate data again because bulkImport overwrote it\n\t\t\t\tcols, rows = data.scratchSpace(idx, cols, rows)\n\t\t\t\tb.StartTimer()\n\t\t\t}\n\t\t}\n\t}\n\tbenchmarkFragmentImports := func(b *testing.B) {\n\t\tfrag, idx, tx = mustOpenFragment(b, OptFieldTypeMutex(cache, DefaultCacheSize))\n\t\tdefer frag.Clean(b)\n\t\tfor i := range data.colIDs {\n\t\t\tb.Run(fmt.Sprintf(\"write-%d\", i), func(b *testing.B) {\n\t\t\t\tbenchmarkOneFragmentImports(b, i)\n\t\t\t})\n\t\t\t// Then commit that write and do another one as a new Tx.\n\t\t\terr := tx.Commit()\n\t\t\tif err != nil {\n\t\t\t\tb.Fatalf(\"error commiting write: %v\", err)\n\t\t\t}\n\t\t\ttx = idx.holder.txf.NewTx(Txo{Write: writable, Index: idx, Fragment: frag, Shard: 0})\n\t\t\tdefer tx.Rollback()\n\t\t}\n\t}\n\tfor _, data = range sampleMutexData {\n\t\tb.Run(data.name, func(b *testing.B) {\n\t\t\tfor _, batchSize = range importBatchSizes {\n\t\t\t\tb.Run(strconv.Itoa(batchSize), func(b *testing.B) {\n\t\t\t\t\tfor _, cache = range mutexCaches {\n\t\t\t\t\t\tb.Run(cache, benchmarkFragmentImports)\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc testOneParallelSlice(t *testing.T, p *parallelSlices) {\n\t// the easy answer\n\tseen := make(map[uint64]uint64, len(p.cols))\n\t//t.Logf(\"cols %d, rows %d\", p.cols, p.rows)\n\tfor i, c := range p.cols {\n\t\tseen[c] = p.rows[i]\n\t}\n\tunsorted := p.prune()\n\tt.Logf(\"unsorted %t, cols %d, rows %d\", unsorted, p.cols, p.rows)\n\tif unsorted {\n\t\tsort.Stable(p)\n\t}\n\tunsorted = p.prune()\n\tif unsorted {\n\t\tt.Fatalf(\"slice still unsorted after sort\")\n\t}\n\tt.Logf(\"pruned/sorted cols %d, rows %d\", p.cols, p.rows)\n\tif len(p.cols) != len(seen) {\n\t\tt.Fatalf(\"expected %d entries, found %d\", len(seen), len(p.cols))\n\t}\n\tfor i, c := range p.cols {\n\t\tif seen[c] != p.rows[i] {\n\t\t\tt.Fatalf(\"expected %d:%d, found :%d\", c, seen[c], p.rows[i])\n\t\t}\n\t}\n}\n\nfunc TestParallelSlices(t *testing.T) {\n\tcols := make([]uint64, 256)\n\trows := make([]uint64, 256)\n\t// ensure at least some overlap by coercing columns into a range\n\t// smaller than number of entries\n\tfor i := range cols {\n\t\tcols[i] = rand.Uint64() & ((uint64(len(cols)) / 2) - 1)\n\t\trows[i] = rand.Uint64() & 0xff\n\t}\n\tt.Run(\"random\", func(t *testing.T) {\n\t\ttestOneParallelSlice(t, &parallelSlices{cols: cols, rows: rows})\n\t})\n\tcols = cols[:cap(cols)]\n\trows = rows[:cap(rows)]\n\t// in-order but no overlap\n\tcol := uint64(0)\n\tfor i := range cols {\n\t\tcols[i] = col\n\t\tcol = col + (rand.Uint64() & 3) + 1\n\t\trows[i] = rand.Uint64() & 0xff\n\t}\n\tt.Run(\"ordered\", func(t *testing.T) {\n\t\ttestOneParallelSlice(t, &parallelSlices{cols: cols, rows: rows})\n\t})\n\tcols = cols[:cap(cols)]\n\trows = rows[:cap(rows)]\n\t// in-order with\n\tcol = uint64(0)\n\tfor i := range cols {\n\t\tcols[i] = col\n\t\tcol = col + (rand.Uint64() & 3)\n\t\trows[i] = rand.Uint64() & 0xff\n\t}\n\tt.Run(\"orderlapping\", func(t *testing.T) {\n\t\ttestOneParallelSlice(t, &parallelSlices{cols: cols, rows: rows})\n\t})\n}\n\nfunc testOneParallelSliceFullPrune(t *testing.T, p *parallelSlices) {\n\t// the easy answer\n\tseen := make(map[uint64]uint64, len(p.cols))\n\t//t.Logf(\"cols %d, rows %d\", p.cols, p.rows)\n\tfor i, c := range p.cols {\n\t\tseen[c] = p.cols[i]\n\t}\n\t//t.Logf(\"before fullPrune: cols %d, rows %d\", p.cols, p.rows)\n\tp.fullPrune()\n\n\t//t.Logf(\"after fullPrune, pruned/sorted cols %d, rows %d\", p.cols, p.rows)\n\tif len(p.cols) != len(seen) {\n\t\tt.Fatalf(\"expected %d entries, found %d\", len(seen), len(p.cols))\n\t}\n\tfor i := range p.cols {\n\t\tif i == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tif p.cols[i] <= p.cols[i-1] {\n\t\t\tt.Fatalf(\"expected p.cols[i=%v]=%v <= p.cols[i-1=%v]=%v\", i, p.cols[i], i-1, p.cols[i-1])\n\t\t}\n\t}\n}\n\nfunc TestParallelSlicesFullPrune(t *testing.T) {\n\tcols := make([]uint64, 0)\n\trows := make([]uint64, 0)\n\n\tt.Run(\"no_loss\", func(t *testing.T) {\n\t\ttestOneParallelSliceFullPrune(t, &parallelSlices{cols: cols, rows: rows})\n\t})\n\n\tcols = make([]uint64, 1)\n\trows = make([]uint64, 1)\n\tcols[0] = 3\n\n\tt.Run(\"no_loss\", func(t *testing.T) {\n\t\ttestOneParallelSliceFullPrune(t, &parallelSlices{cols: cols, rows: rows})\n\t})\n\n\tcols = make([]uint64, 2)\n\trows = make([]uint64, 2)\n\tcols[1] = 1 // sorted\n\n\tt.Run(\"no_loss\", func(t *testing.T) {\n\t\ttestOneParallelSliceFullPrune(t, &parallelSlices{cols: cols, rows: rows})\n\t})\n\n\tcols = make([]uint64, 2)\n\trows = make([]uint64, 2)\n\t// one duplicate 0\n\n\tt.Run(\"no_loss\", func(t *testing.T) {\n\t\ttestOneParallelSliceFullPrune(t, &parallelSlices{cols: cols, rows: rows})\n\t})\n\n\tcols = make([]uint64, 2)\n\trows = make([]uint64, 2)\n\tcols[0] = 1 // unsorted\n\n\tt.Run(\"no_loss\", func(t *testing.T) {\n\t\ttestOneParallelSliceFullPrune(t, &parallelSlices{cols: cols, rows: rows})\n\t})\n\n\tcols = make([]uint64, 3)\n\trows = make([]uint64, 3)\n\tcols[0] = 2 // unsorted\n\tcols[1] = 1 // unsorted\n\n\tt.Run(\"no_loss\", func(t *testing.T) {\n\t\ttestOneParallelSliceFullPrune(t, &parallelSlices{cols: cols, rows: rows})\n\t})\n\n\tcols = make([]uint64, 3)\n\trows = make([]uint64, 3)\n\t// three duplicate 0s\n\n\tt.Run(\"no_loss\", func(t *testing.T) {\n\t\ttestOneParallelSliceFullPrune(t, &parallelSlices{cols: cols, rows: rows})\n\t})\n\n\tcols = make([]uint64, 3)\n\trows = make([]uint64, 3)\n\t// three duplicate 1s\n\tcols[0] = 1\n\tcols[1] = 1\n\tcols[2] = 1\n\n\tt.Run(\"no_loss\", func(t *testing.T) {\n\t\ttestOneParallelSliceFullPrune(t, &parallelSlices{cols: cols, rows: rows})\n\t})\n\n\tcols = make([]uint64, 256)\n\trows = make([]uint64, 256)\n\n\t// ensure at least some overlap by coercing columns into a range\n\t// smaller than number of entries\n\tfor i := range cols {\n\t\tcols[i] = rand.Uint64() & ((uint64(len(cols)) / 2) - 1)\n\t\trows[i] = rand.Uint64() & 0xff\n\t}\n\tt.Run(\"random\", func(t *testing.T) {\n\t\ttestOneParallelSliceFullPrune(t, &parallelSlices{cols: cols, rows: rows})\n\t})\n\n\tcols = cols[:cap(cols)]\n\trows = rows[:cap(rows)]\n\t// in-order but no overlap\n\tcol := uint64(0)\n\tfor i := range cols {\n\t\tcols[i] = col\n\t\tcol = col + (rand.Uint64() & 3) + 1\n\t\trows[i] = rand.Uint64() & 0xff\n\t}\n\tt.Run(\"ordered\", func(t *testing.T) {\n\t\ttestOneParallelSliceFullPrune(t, &parallelSlices{cols: cols, rows: rows})\n\t})\n\tcols = cols[:cap(cols)]\n\trows = rows[:cap(rows)]\n\t// in-order with\n\tcol = uint64(0)\n\tfor i := range cols {\n\t\tcols[i] = col\n\t\tcol = col + (rand.Uint64() & 3)\n\t\trows[i] = rand.Uint64() & 0xff\n\t}\n\tt.Run(\"orderlapping\", func(t *testing.T) {\n\t\ttestOneParallelSliceFullPrune(t, &parallelSlices{cols: cols, rows: rows})\n\t})\n}\n\nfunc TestImportRoaringSingleValued(t *testing.T) {\n\tf, _, tx := mustOpenFragment(t)\n\tdefer f.Clean(t)\n\n\tclear := roaring.NewBitmap(0, 1, ShardWidth-1)\n\tset := roaring.NewBitMatrix(ShardWidth, [][]uint64{\n\t\t{},\n\t\t{},\n\t\t{0, 1, ShardWidth - 1},\n\t}...)\n\n\terr := f.ImportRoaringSingleValued(context.Background(), tx, clear.Roaring(), set.Roaring())\n\tif err != nil {\n\t\tt.Fatalf(\"importing: %v\", err)\n\t}\n\n\t// try setting stuff before\n\tset = roaring.NewBitMatrix(ShardWidth, [][]uint64{\n\t\t{},\n\t\t{0, 1, ShardWidth - 1},\n\t}...)\n\tif err := f.ImportRoaringSingleValued(context.Background(), tx, clear.Roaring(), set.Roaring()); err != nil {\n\t\tt.Fatalf(\"importing: %v\", err)\n\t}\n\n\t// try setting stuff after\n\tset = roaring.NewBitMatrix(ShardWidth, [][]uint64{\n\t\t{},\n\t\t{},\n\t\t{},\n\t\t{0, 1, ShardWidth - 1},\n\t}...)\n\n\tif err := f.ImportRoaringSingleValued(context.Background(), tx, clear.Roaring(), set.Roaring()); err != nil {\n\t\tt.Fatalf(\"importing: %v\", err)\n\t}\n\n\tresult, err := tx.RoaringBitmap(\"i\", \"f\", \"v\", 0)\n\tif err != nil {\n\t\tt.Fatalf(\"getting bitmap: %v\", err)\n\t}\n\n\tif !reflect.DeepEqual(result.Slice(), []uint64{ShardWidth * 3, ShardWidth*3 + 1, ShardWidth*4 - 1}) {\n\t\tt.Fatalf(\"unexpected result: %v\", result.Slice())\n\t}\n\n}\n"
        },
        {
          "name": "gc.go",
          "type": "blob",
          "size": 0.6943359375,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\n// Ensure nopGCNotifier implements interface.\nvar _ GCNotifier = &nopGCNotifier{}\n\n// GCNotifier represents an interface for garbage collection notificationss.\ntype GCNotifier interface {\n\tClose()\n\tAfterGC() <-chan struct{}\n}\n\n// NopGCNotifier represents a GCNotifier that doesn't do anything.\nvar NopGCNotifier GCNotifier = &nopGCNotifier{}\n\ntype nopGCNotifier struct{}\n\n// Close is a no-op implementation of GCNotifier Close method.\nfunc (n *nopGCNotifier) Close() {}\n\n// AfterGC is a no-op implementation of GCNotifier AfterGC method.\nfunc (n *nopGCNotifier) AfterGC() <-chan struct{} {\n\treturn nil\n}\n"
        },
        {
          "name": "gcnotify",
          "type": "tree",
          "content": null
        },
        {
          "name": "generator",
          "type": "tree",
          "content": null
        },
        {
          "name": "gid.go",
          "type": "blob",
          "size": 4.3779296875,
          "content": "// Copyright (c) 2014 The Go Authors. All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n// * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n// \t* Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n// * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\npackage pilosa\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"fmt\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"sync\"\n)\n\n// Sourced https://github.com/bradfitz/http2/blob/dc0c5c000ec33e263612939744d51a3b68b9cece/gotrack.go\nvar goroutineSpace = []byte(\"goroutine \")\nvar littleBuf = sync.Pool{\n\tNew: func() interface{} {\n\t\tbuf := make([]byte, 64)\n\t\treturn &buf\n\t},\n}\n\nvar _ = curGID // happy linter\n\nfunc curGID() uint64 {\n\tif true {\n\t\treturn 0 // avoid doing too much work during production profiling.\n\t}\n\n\tbp := littleBuf.Get().(*[]byte)\n\tdefer littleBuf.Put(bp)\n\tb := *bp\n\tb = b[:runtime.Stack(b, false)]\n\t// Parse the 4707 out of \"goroutine 4707 [\"\n\tb = bytes.TrimPrefix(b, goroutineSpace)\n\ti := bytes.IndexByte(b, ' ')\n\tif i < 0 {\n\t\tpanic(fmt.Sprintf(\"No space found in %q\", b))\n\t}\n\tb = b[:i]\n\tn, err := parseUintBytes(b, 10, 64)\n\tif err != nil {\n\t\tpanic(fmt.Sprintf(\"Failed to parse goroutine ID out of %q: %v\", b, err))\n\t}\n\treturn n\n}\n\n// parseUintBytes is like strconv.ParseUint, but using a []byte.\nfunc parseUintBytes(s []byte, base int, bitSize int) (n uint64, err error) {\n\tvar cutoff, maxVal uint64\n\n\tif bitSize == 0 {\n\t\tbitSize = int(strconv.IntSize)\n\t}\n\n\ts0 := s\n\tswitch {\n\tcase len(s) < 1:\n\t\terr = strconv.ErrSyntax\n\t\treturn n, &strconv.NumError{Func: \"ParseUint\", Num: string(s0), Err: err}\n\n\tcase 2 <= base && base <= 36:\n\t\t// valid base; nothing to do\n\n\tcase base == 0:\n\t\t// Look for octal, hex prefix.\n\t\tswitch {\n\t\tcase s[0] == '0' && len(s) > 1 && (s[1] == 'x' || s[1] == 'X'):\n\t\t\tbase = 16\n\t\t\ts = s[2:]\n\t\t\tif len(s) < 1 {\n\t\t\t\terr = strconv.ErrSyntax\n\t\t\t\treturn n, &strconv.NumError{Func: \"ParseUint\", Num: string(s0), Err: err}\n\t\t\t}\n\t\tcase s[0] == '0':\n\t\t\tbase = 8\n\t\tdefault:\n\t\t\tbase = 10\n\t\t}\n\n\tdefault:\n\t\terr = errors.New(\"invalid base \" + strconv.Itoa(base))\n\t\treturn n, &strconv.NumError{Func: \"ParseUint\", Num: string(s0), Err: err}\n\t}\n\n\tn = 0\n\tcutoff = cutoff64(base)\n\tmaxVal = 1<<uint(bitSize) - 1\n\n\tfor i := 0; i < len(s); i++ {\n\t\tvar v byte\n\t\td := s[i]\n\t\tswitch {\n\t\tcase '0' <= d && d <= '9':\n\t\t\tv = d - '0'\n\t\tcase 'a' <= d && d <= 'z':\n\t\t\tv = d - 'a' + 10\n\t\tcase 'A' <= d && d <= 'Z':\n\t\t\tv = d - 'A' + 10\n\t\tdefault:\n\t\t\tn = 0\n\t\t\terr = strconv.ErrSyntax\n\t\t\treturn n, &strconv.NumError{Func: \"ParseUint\", Num: string(s0), Err: err}\n\t\t}\n\t\tif int(v) >= base {\n\t\t\tn = 0\n\t\t\terr = strconv.ErrSyntax\n\t\t\treturn n, &strconv.NumError{Func: \"ParseUint\", Num: string(s0), Err: err}\n\t\t}\n\n\t\tif n >= cutoff {\n\t\t\t// n*base overflows\n\t\t\tn = 1<<64 - 1\n\t\t\terr = strconv.ErrRange\n\t\t\treturn n, &strconv.NumError{Func: \"ParseUint\", Num: string(s0), Err: err}\n\t\t}\n\t\tn *= uint64(base)\n\n\t\tn1 := n + uint64(v)\n\t\tif n1 < n || n1 > maxVal {\n\t\t\t// n+v overflows\n\t\t\tn = 1<<64 - 1\n\t\t\terr = strconv.ErrRange\n\t\t\treturn n, &strconv.NumError{Func: \"ParseUint\", Num: string(s0), Err: err}\n\t\t}\n\t\tn = n1\n\t}\n\n\treturn n, nil\n}\n\n// Return the first number n such that n*base >= 1<<64.\nfunc cutoff64(base int) uint64 {\n\tif base < 2 {\n\t\treturn 0\n\t}\n\treturn (1<<64-1)/uint64(base) + 1\n}\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 11.0146484375,
          "content": "module github.com/featurebasedb/featurebase/v3\n\nreplace github.com/go-avro/avro => github.com/pilosa/avro v0.0.0-20200626214113-bc1bf9fd41c1\n\nreplace github.com/gomem/gomem => github.com/tgruben/gomem v0.0.0-20221021111114-79fdc77dcf61\n\nreplace robpike.io/ivy => github.com/tgruben/ivy v0.0.0-20230111144143-b80a659caeaf\n\nrequire (\n\tgithub.com/CAFxX/gcnotifier v0.0.0-20220409005548-0153238b886a\n\tgithub.com/DataDog/datadog-go v4.8.3+incompatible // indirect\n\tgithub.com/HdrHistogram/hdrhistogram-go v1.1.2 // indirect\n\tgithub.com/Microsoft/go-winio v0.5.2 // indirect\n\tgithub.com/alexbrainman/odbc v0.0.0-20211220213544-9c9a2e61c5e2\n\tgithub.com/aws/aws-sdk-go v1.42.39\n\tgithub.com/beevik/ntp v0.3.0\n\tgithub.com/benbjohnson/immutable v0.4.0\n\tgithub.com/cespare/xxhash v1.1.0\n\tgithub.com/chzyer/readline v1.5.1\n\tgithub.com/confluentinc/confluent-kafka-go v1.9.1\n\tgithub.com/davecgh/go-spew v1.1.1\n\tgithub.com/denisenkom/go-mssqldb v0.11.0\n\tgithub.com/felixge/fgprof v0.9.2\n\tgithub.com/getsentry/sentry-go v0.13.0\n\tgithub.com/glycerine/vprint v0.0.0-20200730000117-76cea49a68ea\n\tgithub.com/go-avro/avro v0.0.0-20171219232920-444163702c11\n\tgithub.com/go-openapi/strfmt v0.21.2 // indirect\n\tgithub.com/go-sql-driver/mysql v1.7.0\n\tgithub.com/go-test/deep v1.0.7\n\tgithub.com/gogo/protobuf v1.3.2\n\tgithub.com/golang-jwt/jwt v3.2.2+incompatible\n\tgithub.com/golang/protobuf v1.5.2\n\tgithub.com/google/go-cmp v0.5.8\n\tgithub.com/gopherjs/gopherjs v0.0.0-20200217142428-fce0ec30dd00 // indirect\n\tgithub.com/gorilla/handlers v1.3.0\n\tgithub.com/gorilla/mux v1.8.0\n\tgithub.com/gorilla/securecookie v1.1.1\n\tgithub.com/hashicorp/go-retryablehttp v0.7.1\n\tgithub.com/improbable-eng/grpc-web v0.15.0\n\tgithub.com/jedib0t/go-pretty v4.3.0+incompatible\n\tgithub.com/lib/pq v1.10.7\n\tgithub.com/molecula/apophenia v0.0.0-20190827192002-68b7a14a478b\n\tgithub.com/opentracing/opentracing-go v1.2.0\n\tgithub.com/pelletier/go-toml v1.9.5\n\tgithub.com/pkg/errors v0.9.1\n\tgithub.com/prometheus/client_golang v1.14.0\n\tgithub.com/prometheus/client_model v0.3.0\n\tgithub.com/prometheus/prom2json v1.3.1\n\tgithub.com/rakyll/statik v0.1.7\n\tgithub.com/remyoudompheng/bigfft v0.0.0-20200410134404-eec4a21b6bb0 // indirect\n\tgithub.com/ricochet2200/go-disk-usage/du v0.0.0-20210707232629-ac9918953285\n\tgithub.com/rs/cors v1.8.2 // indirect\n\tgithub.com/satori/go.uuid v1.2.1-0.20180404165556-75cca531ea76\n\tgithub.com/segmentio/kafka-go v0.4.29\n\tgithub.com/shirou/gopsutil/v3 v3.22.5\n\tgithub.com/spf13/cobra v1.6.1\n\tgithub.com/spf13/pflag v1.0.5\n\tgithub.com/spf13/viper v1.8.1\n\tgithub.com/stretchr/testify v1.8.1\n\tgithub.com/uber/jaeger-client-go v2.25.0+incompatible\n\tgithub.com/uber/jaeger-lib v2.4.1+incompatible // indirect\n\tgithub.com/zeebo/blake3 v0.2.3\n\tgo.etcd.io/bbolt v1.3.6\n\tgo.etcd.io/etcd v3.3.27+incompatible\n\tgo.etcd.io/etcd/api/v3 v3.5.5\n\tgo.etcd.io/etcd/client/pkg/v3 v3.5.5\n\tgo.etcd.io/etcd/client/v3 v3.5.5\n\tgo.etcd.io/etcd/server/v3 v3.5.5\n\tgolang.org/x/exp v0.0.0-20220827204233-334a2380cb91\n\tgolang.org/x/mod v0.7.0\n\tgolang.org/x/oauth2 v0.0.0-20220608161450-d0670ef3b1eb\n\tgolang.org/x/sync v0.1.0\n\tgolang.org/x/time v0.0.0-20211116232009-f0f3c7e86c11\n\tgopkg.in/DataDog/dd-trace-go.v1 v1.38.1\n\tgopkg.in/yaml.v2 v2.4.0\n\tmodernc.org/mathutil v1.5.0\n\tmodernc.org/strutil v1.1.3\n\tsigs.k8s.io/yaml v1.2.0\n\tvitess.io/vitess v3.0.0-rc.3.0.20190602171040-12bfde34629c+incompatible\n)\n\nrequire (\n\tgithub.com/PaesslerAG/gval v1.0.0\n\tgithub.com/PaesslerAG/jsonpath v0.1.1\n\tgithub.com/apache/arrow/go/v10 v10.0.0-20221021053532-2f627c213fc3\n\tgithub.com/benhoyt/goawk v1.21.0\n\tgithub.com/gobuffalo/nulls v0.4.2\n\tgithub.com/gobuffalo/pop/v6 v6.1.1\n\tgithub.com/gobuffalo/validate/v3 v3.3.3\n\tgithub.com/gofrs/uuid v4.3.1+incompatible\n\tgithub.com/gomem/gomem v0.1.0\n\tgithub.com/google/uuid v1.3.0\n\tgithub.com/jaffee/commandeer v0.6.0\n\tgithub.com/linkedin/goavro/v2 v2.11.1\n\tgoogle.golang.org/grpc v1.49.0\n\tgoogle.golang.org/protobuf v1.28.1\n\trobpike.io/ivy v0.2.9\n)\n\nrequire (\n\tgithub.com/DataDog/datadog-agent/pkg/obfuscate v0.0.0-20211129110424-6491aa3bf583 // indirect\n\tgithub.com/DataDog/sketches-go v1.0.0 // indirect\n\tgithub.com/Masterminds/semver/v3 v3.1.1 // indirect\n\tgithub.com/aymerick/douceur v0.2.0 // indirect\n\tgithub.com/dgraph-io/ristretto v0.1.0 // indirect\n\tgithub.com/fatih/color v1.13.0 // indirect\n\tgithub.com/fatih/structs v1.1.0 // indirect\n\tgithub.com/gobuffalo/envy v1.10.2 // indirect\n\tgithub.com/gobuffalo/fizz v1.14.4 // indirect\n\tgithub.com/gobuffalo/flect v1.0.0 // indirect\n\tgithub.com/gobuffalo/github_flavored_markdown v1.1.3 // indirect\n\tgithub.com/gobuffalo/helpers v0.6.7 // indirect\n\tgithub.com/gobuffalo/plush/v4 v4.1.18 // indirect\n\tgithub.com/gobuffalo/tags/v3 v3.1.4 // indirect\n\tgithub.com/gorilla/css v1.0.0 // indirect\n\tgithub.com/jackc/chunkreader/v2 v2.0.1 // indirect\n\tgithub.com/jackc/pgconn v1.13.0 // indirect\n\tgithub.com/jackc/pgio v1.0.0 // indirect\n\tgithub.com/jackc/pgpassfile v1.0.0 // indirect\n\tgithub.com/jackc/pgproto3/v2 v2.3.1 // indirect\n\tgithub.com/jackc/pgservicefile v0.0.0-20200714003250-2b9c44734f2b // indirect\n\tgithub.com/jackc/pgtype v1.12.0 // indirect\n\tgithub.com/jackc/pgx/v4 v4.17.2 // indirect\n\tgithub.com/jmoiron/sqlx v1.3.5 // indirect\n\tgithub.com/joho/godotenv v1.4.0 // indirect\n\tgithub.com/josharian/intern v1.0.0 // indirect\n\tgithub.com/kballard/go-shellquote v0.0.0-20180428030007-95032a82bc51 // indirect\n\tgithub.com/luna-duclos/instrumentedsql v1.1.3 // indirect\n\tgithub.com/mailru/easyjson v0.7.7 // indirect\n\tgithub.com/mattn/go-colorable v0.1.11 // indirect\n\tgithub.com/mattn/go-isatty v0.0.16 // indirect\n\tgithub.com/mattn/go-sqlite3 v1.14.16 // indirect\n\tgithub.com/microcosm-cc/bluemonday v1.0.20 // indirect\n\tgithub.com/philhofer/fwd v1.1.1 // indirect\n\tgithub.com/rogpeppe/go-internal v1.9.0 // indirect\n\tgithub.com/sergi/go-diff v1.2.0 // indirect\n\tgithub.com/sourcegraph/annotate v0.0.0-20160123013949-f4cad6c6324d // indirect\n\tgithub.com/sourcegraph/syntaxhighlight v0.0.0-20170531221838-bd320f5d308e // indirect\n\tgithub.com/tinylib/msgp v1.1.2 // indirect\n\tgonum.org/v1/gonum v0.11.0 // indirect\n)\n\nrequire (\n\tgithub.com/DataDog/datadog-go/v5 v5.1.0 // indirect\n\tgithub.com/DataDog/gostackparse v0.5.0 // indirect\n\tgithub.com/JohnCGriffin/overflow v0.0.0-20211019200055-46fa312c352c // indirect\n\tgithub.com/andybalholm/brotli v1.0.4 // indirect\n\tgithub.com/apache/thrift v0.16.0 // indirect\n\tgithub.com/asaskevich/govalidator v0.0.0-20200907205600-7a23bdc65eef // indirect\n\tgithub.com/beorn7/perks v1.0.1 // indirect\n\tgithub.com/cenkalti/backoff/v4 v4.1.3 // indirect\n\tgithub.com/cespare/xxhash/v2 v2.1.2 // indirect\n\tgithub.com/coreos/go-semver v0.3.0 // indirect\n\tgithub.com/coreos/go-systemd/v22 v22.3.2 // indirect\n\tgithub.com/desertbit/timer v0.0.0-20180107155436-c41aec40b27f // indirect\n\tgithub.com/dustin/go-humanize v1.0.0 // indirect\n\tgithub.com/form3tech-oss/jwt-go v3.2.3+incompatible // indirect\n\tgithub.com/fsnotify/fsnotify v1.4.9 // indirect\n\tgithub.com/go-ole/go-ole v1.2.6 // indirect\n\tgithub.com/go-openapi/errors v0.19.8 // indirect\n\tgithub.com/go-stack/stack v1.8.0 // indirect\n\tgithub.com/gobwas/httphead v0.0.0-20200921212729-da3d93bc3c58 // indirect\n\tgithub.com/gobwas/pool v0.2.1 // indirect\n\tgithub.com/gobwas/ws v1.0.4 // indirect\n\tgithub.com/goccy/go-json v0.9.11 // indirect\n\tgithub.com/golang-sql/civil v0.0.0-20190719163853-cb61b32ac6fe // indirect\n\tgithub.com/golang/glog v0.0.0-20160126235308-23def4e6c14b // indirect\n\tgithub.com/golang/snappy v0.0.4 // indirect\n\tgithub.com/google/btree v1.0.1 // indirect\n\tgithub.com/google/flatbuffers v2.0.8+incompatible // indirect\n\tgithub.com/google/pprof v0.0.0-20211214055906-6f57359322fd // indirect\n\tgithub.com/gorilla/websocket v1.5.0 // indirect\n\tgithub.com/grpc-ecosystem/go-grpc-middleware v1.3.0 // indirect\n\tgithub.com/grpc-ecosystem/go-grpc-prometheus v1.2.0 // indirect\n\tgithub.com/grpc-ecosystem/grpc-gateway v1.16.0 // indirect\n\tgithub.com/hashicorp/go-cleanhttp v0.5.2 // indirect\n\tgithub.com/hashicorp/hcl v1.0.0 // indirect\n\tgithub.com/inconshreveable/mousetrap v1.0.1 // indirect\n\tgithub.com/jmespath/go-jmespath v0.4.0 // indirect\n\tgithub.com/jonboulle/clockwork v0.2.2 // indirect\n\tgithub.com/json-iterator/go v1.1.12 // indirect\n\tgithub.com/klauspost/asmfmt v1.3.2 // indirect\n\tgithub.com/klauspost/compress v1.15.9 // indirect\n\tgithub.com/klauspost/cpuid/v2 v2.0.12 // indirect\n\tgithub.com/lufia/plan9stats v0.0.0-20211012122336-39d0f177ccd0 // indirect\n\tgithub.com/magiconair/properties v1.8.5 // indirect\n\tgithub.com/mattetti/filebuffer v1.0.1 // indirect\n\tgithub.com/mattn/go-runewidth v0.0.2 // indirect\n\tgithub.com/matttproud/golang_protobuf_extensions v1.0.1 // indirect\n\tgithub.com/minio/asm2plan9s v0.0.0-20200509001527-cdd76441f9d8 // indirect\n\tgithub.com/minio/c2goasm v0.0.0-20190812172519-36a3d3bbc4f3 // indirect\n\tgithub.com/mitchellh/mapstructure v1.5.0 // indirect\n\tgithub.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd // indirect\n\tgithub.com/modern-go/reflect2 v1.0.2 // indirect\n\tgithub.com/oklog/ulid v1.3.1 // indirect\n\tgithub.com/pierrec/lz4/v4 v4.1.15 // indirect\n\tgithub.com/pmezard/go-difflib v1.0.0 // indirect\n\tgithub.com/power-devops/perfstat v0.0.0-20210106213030-5aafc221ea8c // indirect\n\tgithub.com/prometheus/common v0.37.0 // indirect\n\tgithub.com/prometheus/procfs v0.8.0 // indirect\n\tgithub.com/sajari/regression v1.0.1\n\tgithub.com/sirupsen/logrus v1.9.0 // indirect\n\tgithub.com/soheilhy/cmux v0.1.5 // indirect\n\tgithub.com/spf13/afero v1.6.0 // indirect\n\tgithub.com/spf13/cast v1.3.1 // indirect\n\tgithub.com/spf13/jwalterweatherman v1.1.0 // indirect\n\tgithub.com/stretchr/objx v0.5.0 // indirect\n\tgithub.com/subosito/gotenv v1.2.0 // indirect\n\tgithub.com/tklauser/go-sysconf v0.3.10 // indirect\n\tgithub.com/tklauser/numcpus v0.4.0 // indirect\n\tgithub.com/tmc/grpc-websocket-proxy v0.0.0-20201229170055-e5319fda7802 // indirect\n\tgithub.com/xiang90/probing v0.0.0-20190116061207-43a291ad63a2 // indirect\n\tgithub.com/yusufpapurcu/wmi v1.2.2 // indirect\n\tgithub.com/zeebo/xxh3 v1.0.2\n\tgo.etcd.io/etcd/client/v2 v2.305.5 // indirect\n\tgo.etcd.io/etcd/pkg/v3 v3.5.5 // indirect\n\tgo.etcd.io/etcd/raft/v3 v3.5.5 // indirect\n\tgo.mongodb.org/mongo-driver v1.7.5 // indirect\n\tgo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc v0.25.0 // indirect\n\tgo.opentelemetry.io/otel v1.0.1 // indirect\n\tgo.opentelemetry.io/otel/exporters/otlp/otlptrace v1.0.1 // indirect\n\tgo.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc v1.0.1 // indirect\n\tgo.opentelemetry.io/otel/sdk v1.0.1 // indirect\n\tgo.opentelemetry.io/otel/trace v1.0.1 // indirect\n\tgo.opentelemetry.io/proto/otlp v0.9.0 // indirect\n\tgo.uber.org/atomic v1.7.0 // indirect\n\tgo.uber.org/multierr v1.6.0 // indirect\n\tgo.uber.org/zap v1.17.0 // indirect\n\tgolang.org/x/crypto v0.1.0 // indirect\n\tgolang.org/x/net v0.7.0 // indirect\n\tgolang.org/x/sys v0.5.0 // indirect\n\tgolang.org/x/text v0.7.0 // indirect\n\tgolang.org/x/tools v0.3.0 // indirect\n\tgolang.org/x/xerrors v0.0.0-20220609144429-65e65417b02f // indirect\n\tgoogle.golang.org/appengine v1.6.7 // indirect\n\tgoogle.golang.org/genproto v0.0.0-20220503193339-ba3ae3f07e29 // indirect\n\tgopkg.in/ini.v1 v1.62.0 // indirect\n\tgopkg.in/natefinch/lumberjack.v2 v2.0.0 // indirect\n\tgopkg.in/yaml.v3 v3.0.1 // indirect\n\tnhooyr.io/websocket v1.8.6 // indirect\n)\n\ngo 1.18\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 182.791015625,
          "content": "cloud.google.com/go v0.26.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\ncloud.google.com/go v0.34.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\ncloud.google.com/go v0.38.0/go.mod h1:990N+gfupTy94rShfmMCWGDn0LpTmnzTp2qbd1dvSRU=\ncloud.google.com/go v0.44.1/go.mod h1:iSa0KzasP4Uvy3f1mN/7PiObzGgflwredwwASm/v6AU=\ncloud.google.com/go v0.44.2/go.mod h1:60680Gw3Yr4ikxnPRS/oxxkBccT6SA1yMk63TGekxKY=\ncloud.google.com/go v0.45.1/go.mod h1:RpBamKRgapWJb87xiFSdk4g1CME7QZg3uwTez+TSTjc=\ncloud.google.com/go v0.46.3/go.mod h1:a6bKKbmY7er1mI7TEI4lsAkts/mkhTSZK8w33B4RAg0=\ncloud.google.com/go v0.50.0/go.mod h1:r9sluTvynVuxRIOHXQEHMFffphuXHOMZMycpNR5e6To=\ncloud.google.com/go v0.52.0/go.mod h1:pXajvRH/6o3+F9jDHZWQ5PbGhn+o8w9qiu/CffaVdO4=\ncloud.google.com/go v0.53.0/go.mod h1:fp/UouUEsRkN6ryDKNW/Upv/JBKnv6WDthjR6+vze6M=\ncloud.google.com/go v0.54.0/go.mod h1:1rq2OEkV3YMf6n/9ZvGWI3GWw0VoqH/1x2nd8Is/bPc=\ncloud.google.com/go v0.56.0/go.mod h1:jr7tqZxxKOVYizybht9+26Z/gUq7tiRzu+ACVAMbKVk=\ncloud.google.com/go v0.57.0/go.mod h1:oXiQ6Rzq3RAkkY7N6t3TcE6jE+CIBBbA36lwQ1JyzZs=\ncloud.google.com/go v0.62.0/go.mod h1:jmCYTdRCQuc1PHIIJ/maLInMho30T/Y0M4hTdTShOYc=\ncloud.google.com/go v0.65.0/go.mod h1:O5N8zS7uWy9vkA9vayVHs65eM1ubvY4h553ofrNHObY=\ncloud.google.com/go v0.72.0/go.mod h1:M+5Vjvlc2wnp6tjzE102Dw08nGShTscUx2nZMufOKPI=\ncloud.google.com/go v0.74.0/go.mod h1:VV1xSbzvo+9QJOxLDaJfTjx5e+MePCpCWwvftOeQmWk=\ncloud.google.com/go v0.78.0/go.mod h1:QjdrLG0uq+YwhjoVOLsS1t7TW8fs36kLs4XO5R5ECHg=\ncloud.google.com/go v0.79.0/go.mod h1:3bzgcEeQlzbuEAYu4mrWhKqWjmpprinYgKJLgKHnbb8=\ncloud.google.com/go v0.81.0 h1:at8Tk2zUz63cLPR0JPWm5vp77pEZmzxEQBEfRKn1VV8=\ncloud.google.com/go v0.81.0/go.mod h1:mk/AM35KwGk/Nm2YSeZbxXdrNK3KZOYHmLkOqC2V6E0=\ncloud.google.com/go/bigquery v1.0.1/go.mod h1:i/xbL2UlR5RvWAURpBYZTtm/cXjCha9lbfbpx4poX+o=\ncloud.google.com/go/bigquery v1.3.0/go.mod h1:PjpwJnslEMmckchkHFfq+HTD2DmtT67aNFKH1/VBDHE=\ncloud.google.com/go/bigquery v1.4.0/go.mod h1:S8dzgnTigyfTmLBfrtrhyYhwRxG72rYxvftPBK2Dvzc=\ncloud.google.com/go/bigquery v1.5.0/go.mod h1:snEHRnqQbz117VIFhE8bmtwIDY80NLUZUMb4Nv6dBIg=\ncloud.google.com/go/bigquery v1.7.0/go.mod h1://okPTzCYNXSlb24MZs83e2Do+h+VXtc4gLoIoXIAPc=\ncloud.google.com/go/bigquery v1.8.0/go.mod h1:J5hqkt3O0uAFnINi6JXValWIb1v0goeZM77hZzJN/fQ=\ncloud.google.com/go/datastore v1.0.0/go.mod h1:LXYbyblFSglQ5pkeyhO+Qmw7ukd3C+pD7TKLgZqpHYE=\ncloud.google.com/go/datastore v1.1.0/go.mod h1:umbIZjpQpHh4hmRpGhH4tLFup+FVzqBi1b3c64qFpCk=\ncloud.google.com/go/firestore v1.1.0/go.mod h1:ulACoGHTpvq5r8rxGJ4ddJZBZqakUQqClKRT5SZwBmk=\ncloud.google.com/go/pubsub v1.0.1/go.mod h1:R0Gpsv3s54REJCy4fxDixWD93lHJMoZTyQ2kNxGRt3I=\ncloud.google.com/go/pubsub v1.1.0/go.mod h1:EwwdRX2sKPjnvnqCa270oGRyludottCI76h+R3AArQw=\ncloud.google.com/go/pubsub v1.2.0/go.mod h1:jhfEVHT8odbXTkndysNHCcx0awwzvfOlguIAii9o8iA=\ncloud.google.com/go/pubsub v1.3.1/go.mod h1:i+ucay31+CNRpDW4Lu78I4xXG+O1r/MAHgjpRVR+TSU=\ncloud.google.com/go/pubsub v1.4.0/go.mod h1:LFrqilwgdw4X2cJS9ALgzYmMu+ULyrUN6IHV3CPK4TM=\ncloud.google.com/go/storage v1.0.0/go.mod h1:IhtSnM/ZTZV8YYJWCY8RULGVqBDmpoyjwiyrjsg+URw=\ncloud.google.com/go/storage v1.5.0/go.mod h1:tpKbwo567HUNpVclU5sGELwQWBDZ8gh0ZeosJ0Rtdos=\ncloud.google.com/go/storage v1.6.0/go.mod h1:N7U0C8pVQ/+NIKOBQyamJIeKQKkZ+mxpohlUTyfDhBk=\ncloud.google.com/go/storage v1.8.0/go.mod h1:Wv1Oy7z6Yz3DshWRJFhqM/UCfaWIRTdp0RXyy7KQOVs=\ncloud.google.com/go/storage v1.10.0/go.mod h1:FLPqc6j+Ki4BU591ie1oL6qBQGu2Bl/tZ9ullr3+Kg0=\ndmitri.shuralyov.com/gpu/mtl v0.0.0-20190408044501-666a987793e9/go.mod h1:H6x//7gZCb22OMCxBHrMx7a5I7Hp++hsVxbQ4BYO7hU=\ngithub.com/Azure/go-autorest/autorest v0.9.0/go.mod h1:xyHB1BMZT0cuDHU7I0+g046+BFDTQ8rEZB0s4Yfa6bI=\ngithub.com/Azure/go-autorest/autorest/adal v0.5.0/go.mod h1:8Z9fGy2MpX0PvDjB1pEgQTmVqjGhiHBW7RJJEciWzS0=\ngithub.com/Azure/go-autorest/autorest/date v0.1.0/go.mod h1:plvfp3oPSKwf2DNjlBjWF/7vwR+cUD/ELuzDCXwHUVA=\ngithub.com/Azure/go-autorest/autorest/mocks v0.1.0/go.mod h1:OTyCOPRA2IgIlWxVYxBee2F5Gr4kF2zd2J5cFRaIDN0=\ngithub.com/Azure/go-autorest/autorest/mocks v0.2.0/go.mod h1:OTyCOPRA2IgIlWxVYxBee2F5Gr4kF2zd2J5cFRaIDN0=\ngithub.com/Azure/go-autorest/logger v0.1.0/go.mod h1:oExouG+K6PryycPJfVSxi/koC6LSNgds39diKLz7Vrc=\ngithub.com/Azure/go-autorest/tracing v0.5.0/go.mod h1:r/s2XiOKccPW3HrqB+W0TQzfbtp2fGCgRFtBroKn4Dk=\ngithub.com/BurntSushi/toml v0.3.1 h1:WXkYYl6Yr3qBf1K79EBnL4mak0OimBfB0XUf9Vl28OQ=\ngithub.com/BurntSushi/toml v0.3.1/go.mod h1:xHWCNGjB5oqiDr8zfno3MHue2Ht5sIBksp03qcyfWMU=\ngithub.com/BurntSushi/xgb v0.0.0-20160522181843-27f122750802/go.mod h1:IVnqGOEym/WlBOVXweHU+Q+/VP0lqqI8lqeDx9IjBqo=\ngithub.com/CAFxX/gcnotifier v0.0.0-20220409005548-0153238b886a h1:cMnbU9ppNDmnjj1jfBMw43mzxqwGbECart3yjM+54EQ=\ngithub.com/CAFxX/gcnotifier v0.0.0-20220409005548-0153238b886a/go.mod h1:Rn2zM2MnHze07LwkneP48TWt6UiZhzQTwCvw6djVGfE=\ngithub.com/DataDog/datadog-agent/pkg/obfuscate v0.0.0-20211129110424-6491aa3bf583 h1:3nVO1nQyh64IUY6BPZUpMYMZ738Pu+LsMt3E0eqqIYw=\ngithub.com/DataDog/datadog-agent/pkg/obfuscate v0.0.0-20211129110424-6491aa3bf583/go.mod h1:EP9f4GqaDJyP1F5jTNMtzdIpw3JpNs3rMSJOnYywCiw=\ngithub.com/DataDog/datadog-go v3.2.0+incompatible/go.mod h1:LButxg5PwREeZtORoXG3tL4fMGNddJ+vMq1mwgfaqoQ=\ngithub.com/DataDog/datadog-go v4.8.2+incompatible/go.mod h1:LButxg5PwREeZtORoXG3tL4fMGNddJ+vMq1mwgfaqoQ=\ngithub.com/DataDog/datadog-go v4.8.3+incompatible h1:fNGaYSuObuQb5nzeTQqowRAd9bpDIRRV4/gUtIBjh8Q=\ngithub.com/DataDog/datadog-go v4.8.3+incompatible/go.mod h1:LButxg5PwREeZtORoXG3tL4fMGNddJ+vMq1mwgfaqoQ=\ngithub.com/DataDog/datadog-go/v5 v5.0.2/go.mod h1:ZI9JFB4ewXbw1sBnF4sxsR2k1H3xjV+PUAOUsHvKpcU=\ngithub.com/DataDog/datadog-go/v5 v5.1.0 h1:Zmq3tCk9+Tdq8Du73M71Zo6Dyx+cEo9QkCSCqQlHFaQ=\ngithub.com/DataDog/datadog-go/v5 v5.1.0/go.mod h1:KhiYb2Badlv9/rofz+OznKoEF5XKTonWyhx5K83AP8E=\ngithub.com/DataDog/gostackparse v0.5.0 h1:jb72P6GFHPHz2W0onsN51cS3FkaMDcjb0QzgxxA4gDk=\ngithub.com/DataDog/gostackparse v0.5.0/go.mod h1:lTfqcJKqS9KnXQGnyQMCugq3u1FP6UZMfWR0aitKFMM=\ngithub.com/DataDog/sketches-go v1.0.0 h1:chm5KSXO7kO+ywGWJ0Zs6tdmWU8PBXSbywFVciL6BG4=\ngithub.com/DataDog/sketches-go v1.0.0/go.mod h1:O+XkJHWk9w4hDwY2ZUDU31ZC9sNYlYo8DiFsxjYeo1k=\ngithub.com/DataDog/zstd v1.3.5/go.mod h1:1jcaCB/ufaK+sKp1NBhlGmpz41jOoPQ35bpF36t7BBo=\ngithub.com/HdrHistogram/hdrhistogram-go v1.1.2 h1:5IcZpTvzydCQeHzK4Ef/D5rrSqwxob0t8PQPMybUNFM=\ngithub.com/HdrHistogram/hdrhistogram-go v1.1.2/go.mod h1:yDgFjdqOqDEKOvasDdhWNXYg9BVp4O+o5f6V/ehm6Oo=\ngithub.com/JohnCGriffin/overflow v0.0.0-20211019200055-46fa312c352c h1:RGWPOewvKIROun94nF7v2cua9qP+thov/7M50KEoeSU=\ngithub.com/JohnCGriffin/overflow v0.0.0-20211019200055-46fa312c352c/go.mod h1:X0CRv0ky0k6m906ixxpzmDRLvX58TFUKS2eePweuyxk=\ngithub.com/Knetic/govaluate v3.0.1-0.20171022003610-9aa49832a739+incompatible/go.mod h1:r7JcOSlj0wfOMncg0iLm8Leh48TZaKVeNIfJntJ2wa0=\ngithub.com/Masterminds/semver/v3 v3.1.1 h1:hLg3sBzpNErnxhQtUy/mmLR2I9foDujNK030IGemrRc=\ngithub.com/Masterminds/semver/v3 v3.1.1/go.mod h1:VPu/7SZ7ePZ3QOrcuXROw5FAcLl4a0cBrbBpGY/8hQs=\ngithub.com/Microsoft/go-winio v0.5.0/go.mod h1:JPGBdM1cNvN/6ISo+n8V5iA4v8pBzdOpzfwIujj1a84=\ngithub.com/Microsoft/go-winio v0.5.1/go.mod h1:JPGBdM1cNvN/6ISo+n8V5iA4v8pBzdOpzfwIujj1a84=\ngithub.com/Microsoft/go-winio v0.5.2 h1:a9IhgEQBCUEk6QCdml9CiJGhAws+YwffDHEMp1VMrpA=\ngithub.com/Microsoft/go-winio v0.5.2/go.mod h1:WpS1mjBmmwHBEWmogvA2mj8546UReBk4v8QkMxJ6pZY=\ngithub.com/NYTimes/gziphandler v0.0.0-20170623195520-56545f4a5d46/go.mod h1:3wb06e3pkSAbeQ52E9H9iFoQsEEwGN64994WTCIhntQ=\ngithub.com/OneOfOne/xxhash v1.2.2 h1:KMrpdQIwFcEqXDklaen+P1axHaj9BSKzvpUUfnHldSE=\ngithub.com/OneOfOne/xxhash v1.2.2/go.mod h1:HSdplMjZKSmBqAxg5vPj2TmRDmfkzw+cTzAElWljhcU=\ngithub.com/PaesslerAG/gval v1.0.0 h1:GEKnRwkWDdf9dOmKcNrar9EA1bz1z9DqPIO1+iLzhd8=\ngithub.com/PaesslerAG/gval v1.0.0/go.mod h1:y/nm5yEyTeX6av0OfKJNp9rBNj2XrGhAf5+v24IBN1I=\ngithub.com/PaesslerAG/jsonpath v0.1.0/go.mod h1:4BzmtoM/PI8fPO4aQGIusjGxGir2BzcV0grWtFzq1Y8=\ngithub.com/PaesslerAG/jsonpath v0.1.1 h1:c1/AToHQMVsduPAa4Vh6xp2U0evy4t8SWp8imEsylIk=\ngithub.com/PaesslerAG/jsonpath v0.1.1/go.mod h1:lVboNxFGal/VwW6d9JzIy56bUsYAP6tH/x80vjnCseY=\ngithub.com/PuerkitoBio/purell v1.0.0/go.mod h1:c11w/QuzBsJSee3cPx9rAFu61PvFxuPbtSwDGJws/X0=\ngithub.com/PuerkitoBio/urlesc v0.0.0-20160726150825-5bd2802263f2/go.mod h1:uGdkoq3SwY9Y+13GIhn11/XLaGBb4BfwItxLd5jeuXE=\ngithub.com/Shopify/sarama v1.19.0/go.mod h1:FVkBWblsNy7DGZRfXLU0O9RCGt5g3g3yEuWXgklEdEo=\ngithub.com/Shopify/sarama v1.22.0/go.mod h1:lm3THZ8reqBDBQKQyb5HB3sY1lKp3grEbQ81aWSgPp4=\ngithub.com/Shopify/toxiproxy v2.1.4+incompatible/go.mod h1:OXgGpZ6Cli1/URJOF1DMxUHB2q5Ap20/P/eIdh4G0pI=\ngithub.com/VividCortex/gohistogram v1.0.0/go.mod h1:Pf5mBqqDxYaXu3hDrrU+w6nw50o/4+TcAqDqk/vUH7g=\ngithub.com/actgardner/gogen-avro/v10 v10.1.0/go.mod h1:o+ybmVjEa27AAr35FRqU98DJu1fXES56uXniYFv4yDA=\ngithub.com/actgardner/gogen-avro/v10 v10.2.1/go.mod h1:QUhjeHPchheYmMDni/Nx7VB0RsT/ee8YIgGY/xpEQgQ=\ngithub.com/actgardner/gogen-avro/v9 v9.1.0/go.mod h1:nyTj6wPqDJoxM3qdnjcLv+EnMDSDFqE0qDpva2QRmKc=\ngithub.com/afex/hystrix-go v0.0.0-20180502004556-fa1af6a1f4f5/go.mod h1:SkGFH1ia65gfNATL8TAiHDNxPzPdmEL5uirI2Uyuz6c=\ngithub.com/ajstarks/svgo v0.0.0-20180226025133-644b8db467af/go.mod h1:K08gAheRH3/J6wwsYMMT4xOr94bZjxIelGM0+d/wbFw=\ngithub.com/alecthomas/template v0.0.0-20160405071501-a0175ee3bccc/go.mod h1:LOuyumcjzFXgccqObfd/Ljyb9UuFJ6TxHnclSeseNhc=\ngithub.com/alecthomas/template v0.0.0-20190718012654-fb15b899a751/go.mod h1:LOuyumcjzFXgccqObfd/Ljyb9UuFJ6TxHnclSeseNhc=\ngithub.com/alecthomas/units v0.0.0-20151022065526-2efee857e7cf/go.mod h1:ybxpYRFXyAe+OPACYpWeL0wqObRcbAqCMya13uyzqw0=\ngithub.com/alecthomas/units v0.0.0-20190717042225-c3de453c63f4/go.mod h1:ybxpYRFXyAe+OPACYpWeL0wqObRcbAqCMya13uyzqw0=\ngithub.com/alecthomas/units v0.0.0-20190924025748-f65c72e2690d/go.mod h1:rBZYJk541a8SKzHPHnH3zbiI+7dagKZ0cgpgrD7Fyho=\ngithub.com/alexbrainman/odbc v0.0.0-20211220213544-9c9a2e61c5e2 h1:090cWAt7zsbdvRegKCBVwcCTghjxhUh1PK2KNSq82vw=\ngithub.com/alexbrainman/odbc v0.0.0-20211220213544-9c9a2e61c5e2/go.mod h1:c5eyz5amZqTKvY3ipqerFO/74a/8CYmXOahSr40c+Ww=\ngithub.com/andybalholm/brotli v1.0.2/go.mod h1:loMXtMfwqflxFJPmdbJO0a3KNoPuLBgiu3qAvBg8x/Y=\ngithub.com/andybalholm/brotli v1.0.4 h1:V7DdXeJtZscaqfNuAdSRuRFzuiKlHSC/Zh3zl9qY3JY=\ngithub.com/andybalholm/brotli v1.0.4/go.mod h1:fO7iG3H7G2nSZ7m0zPUDn85XEX2GTukHGRSepvi9Eig=\ngithub.com/antihax/optional v1.0.0/go.mod h1:uupD/76wgC+ih3iEmQUL+0Ugr19nfwCT1kdvxnR2qWY=\ngithub.com/apache/arrow/go/v10 v10.0.0-20221021053532-2f627c213fc3 h1:PpXyIaFzGGj6B7llkhQ82AFJ4oGAPiPxuZzjy3vHGVM=\ngithub.com/apache/arrow/go/v10 v10.0.0-20221021053532-2f627c213fc3/go.mod h1:YvhnlEePVnBS4+0z3fhPfUy7W1Ikj0Ih0vcRo/gZ1M0=\ngithub.com/apache/thrift v0.12.0/go.mod h1:cp2SuWMxlEZw2r+iP2GNCdIi4C1qmUzdZFSVb+bacwQ=\ngithub.com/apache/thrift v0.13.0/go.mod h1:cp2SuWMxlEZw2r+iP2GNCdIi4C1qmUzdZFSVb+bacwQ=\ngithub.com/apache/thrift v0.16.0 h1:qEy6UW60iVOlUy+b9ZR0d5WzUWYGOo4HfopoyBaNmoY=\ngithub.com/apache/thrift v0.16.0/go.mod h1:PHK3hniurgQaNMZYaCLEqXKsYK8upmhPbmdP2FXSqgU=\ngithub.com/armon/circbuf v0.0.0-20150827004946-bbbad097214e/go.mod h1:3U/XgcO3hCbHZ8TKRvWD2dDTCfh9M9ya+I9JpbB7O8o=\ngithub.com/armon/consul-api v0.0.0-20180202201655-eb2c6b5be1b6/go.mod h1:grANhF5doyWs3UAsr3K4I6qtAmlQcZDesFNEHPZAzj8=\ngithub.com/armon/go-metrics v0.0.0-20180917152333-f0300d1749da/go.mod h1:Q73ZrmVTwzkszR9V5SSuryQ31EELlFMUz1kKyl939pY=\ngithub.com/armon/go-metrics v0.3.0/go.mod h1:zXjbSimjXTd7vOpY8B0/2LpvNvDoXBuplAD+gJD3GYs=\ngithub.com/armon/go-radix v0.0.0-20180808171621-7fddfc383310/go.mod h1:ufUuZ+zHj4x4TnLV4JWEpy2hxWSpsRywHrMgIH9cCH8=\ngithub.com/aryann/difflib v0.0.0-20170710044230-e206f873d14a/go.mod h1:DAHtR1m6lCRdSC2Tm3DSWRPvIPr6xNKyeHdqDQSQT+A=\ngithub.com/asaskevich/govalidator v0.0.0-20200907205600-7a23bdc65eef h1:46PFijGLmAjMPwCCCo7Jf0W6f9slllCkkv7vyc1yOSg=\ngithub.com/asaskevich/govalidator v0.0.0-20200907205600-7a23bdc65eef/go.mod h1:WaHUgvxTVq04UNunO+XhnAqY/wQc+bxr74GqbsZ/Jqw=\ngithub.com/aws/aws-lambda-go v1.13.3/go.mod h1:4UKl9IzQMoD+QF79YdCuzCwp8VbmG4VAQwij/eHl5CU=\ngithub.com/aws/aws-sdk-go v1.25.37/go.mod h1:KmX6BPdI08NWTb3/sm4ZGu5ShLoqVDhKgpiN924inxo=\ngithub.com/aws/aws-sdk-go v1.27.0/go.mod h1:KmX6BPdI08NWTb3/sm4ZGu5ShLoqVDhKgpiN924inxo=\ngithub.com/aws/aws-sdk-go v1.34.28/go.mod h1:H7NKnBqNVzoTJpGfLrQkkD+ytBA93eiDYi/+8rV9s48=\ngithub.com/aws/aws-sdk-go v1.42.39 h1:6Lso73VoCI8Zmv3zAMv4BNg2gHAKNOlbLv1s/ew90SI=\ngithub.com/aws/aws-sdk-go v1.42.39/go.mod h1:OGr6lGMAKGlG9CVrYnWYDKIyb829c6EVBRjxqjmPepc=\ngithub.com/aws/aws-sdk-go-v2 v0.18.0/go.mod h1:JWVYvqSMppoMJC0x5wdwiImzgXTI9FuZwxzkQq9wy+g=\ngithub.com/aws/aws-sdk-go-v2 v1.0.0/go.mod h1:smfAbmpW+tcRVuNUjo3MOArSZmW72t62rkCzc2i0TWM=\ngithub.com/aws/aws-sdk-go-v2/config v1.0.0/go.mod h1:WysE/OpUgE37tjtmtJd8GXgT8s1euilE5XtUkRNUQ1w=\ngithub.com/aws/aws-sdk-go-v2/credentials v1.0.0/go.mod h1:/SvsiqBf509hG4Bddigr3NB12MIpfHhZapyBurJe8aY=\ngithub.com/aws/aws-sdk-go-v2/feature/ec2/imds v1.0.0/go.mod h1:wpMHDCXvOXZxGCRSidyepa8uJHY4vaBGfY2/+oKU/Bc=\ngithub.com/aws/aws-sdk-go-v2/service/internal/presigned-url v1.0.0/go.mod h1:3jExOmpbjgPnz2FJaMOfbSk1heTkZ66aD3yNtVhnjvI=\ngithub.com/aws/aws-sdk-go-v2/service/sqs v1.0.0/go.mod h1:w5BclCU8ptTbagzXS/fHBr+vAyXUjggg/72qDIURKMk=\ngithub.com/aws/aws-sdk-go-v2/service/sts v1.0.0/go.mod h1:5f+cELGATgill5Pu3/vK3Ebuigstc+qYEHW5MvGWZO4=\ngithub.com/aws/smithy-go v1.0.0/go.mod h1:EzMw8dbp/YJL4A5/sbhGddag+NPT7q084agLbB9LgIw=\ngithub.com/aws/smithy-go v1.11.0/go.mod h1:3xHYmszWVx2c0kIwQeEVf9uSm4fYZt67FBJnwub1bgM=\ngithub.com/aymerick/douceur v0.2.0 h1:Mv+mAeH1Q+n9Fr+oyamOlAkUNPWPlA8PPGR0QAaYuPk=\ngithub.com/aymerick/douceur v0.2.0/go.mod h1:wlT5vV2O3h55X9m7iVYN0TBM0NH/MmbLnd30/FjWUq4=\ngithub.com/beevik/ntp v0.3.0 h1:xzVrPrE4ziasFXgBVBZJDP0Wg/KpMwk2KHJ4Ba8GrDw=\ngithub.com/beevik/ntp v0.3.0/go.mod h1:hIHWr+l3+/clUnF44zdK+CWW7fO8dR5cIylAQ76NRpg=\ngithub.com/benbjohnson/immutable v0.4.0 h1:CTqXbEerYso8YzVPxmWxh2gnoRQbbB9X1quUC8+vGZA=\ngithub.com/benbjohnson/immutable v0.4.0/go.mod h1:iAr8OjJGLnLmVUr9MZ/rz4PWUy6Ouc2JLYuMArmvAJM=\ngithub.com/benhoyt/goawk v1.21.0 h1:GASuhJXHMFZ/2TJBPh+2Ah3kclVGNvGjt+uh3ajMdLk=\ngithub.com/benhoyt/goawk v1.21.0/go.mod h1:UG1Ld6CjkkHhoyQmErQGSTwmavsTqFnCDYsLSJbovqU=\ngithub.com/beorn7/perks v0.0.0-20180321164747-3a771d992973/go.mod h1:Dwedo/Wpr24TaqPxmxbtue+5NUziq4I4S80YR8gNf3Q=\ngithub.com/beorn7/perks v1.0.0/go.mod h1:KWe93zE9D1o94FZ5RNwFwVgaQK1VOXiVxmqh+CedLV8=\ngithub.com/beorn7/perks v1.0.1 h1:VlbKKnNfV8bJzeqoa4cOKqO6bYr3WgKZxO8Z16+hsOM=\ngithub.com/beorn7/perks v1.0.1/go.mod h1:G2ZrVWU2WbWT9wwq4/hrbKbnv/1ERSJQ0ibhJ6rlkpw=\ngithub.com/bgentry/speakeasy v0.1.0/go.mod h1:+zsyZBPWlz7T6j88CTgSN5bM796AkVf0kBD4zp0CCIs=\ngithub.com/bitly/go-hostpool v0.0.0-20171023180738-a3a6125de932/go.mod h1:NOuUCSz6Q9T7+igc/hlvDOUdtWKryOrtFyIVABv/p7k=\ngithub.com/bketelsen/crypt v0.0.3-0.20200106085610-5cbc8cc4026c/go.mod h1:MKsuJmJgSg28kpZDP6UIiPt0e0Oz0kqKNGyRaWEPv84=\ngithub.com/bketelsen/crypt v0.0.4/go.mod h1:aI6NrJ0pMGgvZKL1iVgXLnfIFJtfV+bKCoqOes/6LfM=\ngithub.com/bmizerany/assert v0.0.0-20160611221934-b7ed37b82869/go.mod h1:Ekp36dRnpXw/yCqJaO+ZrUyxD+3VXMFFr56k5XYrpB4=\ngithub.com/bradfitz/gomemcache v0.0.0-20220106215444-fb4bf637b56d/go.mod h1:H0wQNHz2YrLsuXOZozoeDmnHXkNCRmMW0gwFWDfEZDA=\ngithub.com/casbin/casbin/v2 v2.1.2/go.mod h1:YcPU1XXisHhLzuxH9coDNf2FbKpjGlbCg3n9yuLkIJQ=\ngithub.com/cenkalti/backoff v2.2.1+incompatible/go.mod h1:90ReRw6GdpyfrHakVjL/QHaoyV4aDUVVkXQJJJ3NXXM=\ngithub.com/cenkalti/backoff/v4 v4.1.1/go.mod h1:scbssz8iZGpm3xbr14ovlUdkxfGXNInqkPWOWmG2CLw=\ngithub.com/cenkalti/backoff/v4 v4.1.3 h1:cFAlzYUlVYDysBEH2T5hyJZMh3+5+WCBvSnK6Q8UtC4=\ngithub.com/cenkalti/backoff/v4 v4.1.3/go.mod h1:scbssz8iZGpm3xbr14ovlUdkxfGXNInqkPWOWmG2CLw=\ngithub.com/census-instrumentation/opencensus-proto v0.2.1/go.mod h1:f6KPmirojxKA12rnyqOA5BBL4O983OfeGPqjHWSTneU=\ngithub.com/certifi/gocertifi v0.0.0-20191021191039-0944d244cd40/go.mod h1:sGbDF6GwGcLpkNXPUTkMRoywsNa/ol15pxFe6ERfguA=\ngithub.com/certifi/gocertifi v0.0.0-20200922220541-2c3bb06c6054 h1:uH66TXeswKn5PW5zdZ39xEwfS9an067BirqA+P4QaLI=\ngithub.com/certifi/gocertifi v0.0.0-20200922220541-2c3bb06c6054/go.mod h1:sGbDF6GwGcLpkNXPUTkMRoywsNa/ol15pxFe6ERfguA=\ngithub.com/cespare/xxhash v1.1.0 h1:a6HrQnmkObjyL+Gs60czilIUGqrzKutQD6XZog3p+ko=\ngithub.com/cespare/xxhash v1.1.0/go.mod h1:XrSqR1VqqWfGrhpAt58auRo0WTKS1nRRg3ghfAqPWnc=\ngithub.com/cespare/xxhash/v2 v2.1.1/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=\ngithub.com/cespare/xxhash/v2 v2.1.2 h1:YRXhKfTDauu4ajMg1TPgFO5jnlC2HCbmLXMcTG5cbYE=\ngithub.com/cespare/xxhash/v2 v2.1.2/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=\ngithub.com/chzyer/logex v1.1.10/go.mod h1:+Ywpsq7O8HXn0nuIou7OrIPyXbp3wmkHB+jjWRnGsAI=\ngithub.com/chzyer/logex v1.2.1 h1:XHDu3E6q+gdHgsdTPH6ImJMIp436vR6MPtH8gP05QzM=\ngithub.com/chzyer/logex v1.2.1/go.mod h1:JLbx6lG2kDbNRFnfkgvh4eRJRPX1QCoOIWomwysCBrQ=\ngithub.com/chzyer/readline v0.0.0-20180603132655-2972be24d48e/go.mod h1:nSuG5e5PlCu98SY8svDHJxuZscDgtXS6KTTbou5AhLI=\ngithub.com/chzyer/readline v1.5.1 h1:upd/6fQk4src78LMRzh5vItIt361/o4uq553V8B5sGI=\ngithub.com/chzyer/readline v1.5.1/go.mod h1:Eh+b79XXUwfKfcPLepksvw2tcLE/Ct21YObkaSkeBlk=\ngithub.com/chzyer/test v0.0.0-20180213035817-a1ea475d72b1/go.mod h1:Q3SI9o4m/ZMnBNeIyt5eFwwo7qiLfzFZmjNmxjkiQlU=\ngithub.com/chzyer/test v1.0.0 h1:p3BQDXSxOhOG0P9z6/hGnII4LGiEPOYBhs8asl/fC04=\ngithub.com/chzyer/test v1.0.0/go.mod h1:2JlltgoNkt4TW/z9V/IzDdFaMTM2JPIi26O1pF38GC8=\ngithub.com/circonus-labs/circonus-gometrics v2.3.1+incompatible/go.mod h1:nmEj6Dob7S7YxXgwXpfOuvO54S+tGdZdw9fuRZt25Ag=\ngithub.com/circonus-labs/circonusllhist v0.1.3/go.mod h1:kMXHVDlOchFAehlya5ePtbp5jckzBHf4XRpQvBOLI+I=\ngithub.com/clbanning/x2j v0.0.0-20191024224557-825249438eec/go.mod h1:jMjuTZXRI4dUb/I5gc9Hdhagfvm9+RyrPryS/auMzxE=\ngithub.com/client9/misspell v0.3.4/go.mod h1:qj6jICC3Q7zFZvVWo7KLAzC3yx5G7kyvSDkc90ppPyw=\ngithub.com/cncf/udpa/go v0.0.0-20191209042840-269d4d468f6f/go.mod h1:M8M6+tZqaGXZJjfX53e64911xZQV5JYwmTeXPW+k8Sc=\ngithub.com/cncf/udpa/go v0.0.0-20200629203442-efcf912fb354/go.mod h1:WmhPx2Nbnhtbo57+VJT5O0JRkEi1Wbu0z5j0R8u5Hbk=\ngithub.com/cncf/udpa/go v0.0.0-20201120205902-5459f2c99403/go.mod h1:WmhPx2Nbnhtbo57+VJT5O0JRkEi1Wbu0z5j0R8u5Hbk=\ngithub.com/cncf/udpa/go v0.0.0-20210930031921-04548b0d99d4/go.mod h1:6pvJx4me5XPnfI9Z40ddWsdw2W/uZgQLFXToKeRcDiI=\ngithub.com/cncf/xds/go v0.0.0-20210312221358-fbca930ec8ed/go.mod h1:eXthEFrGJvWHgFFCl3hGmgk+/aYT6PnTQLykKQRLhEs=\ngithub.com/cncf/xds/go v0.0.0-20210805033703-aa0b78936158/go.mod h1:eXthEFrGJvWHgFFCl3hGmgk+/aYT6PnTQLykKQRLhEs=\ngithub.com/cncf/xds/go v0.0.0-20210922020428-25de7278fc84/go.mod h1:eXthEFrGJvWHgFFCl3hGmgk+/aYT6PnTQLykKQRLhEs=\ngithub.com/cncf/xds/go v0.0.0-20211001041855-01bcc9b48dfe/go.mod h1:eXthEFrGJvWHgFFCl3hGmgk+/aYT6PnTQLykKQRLhEs=\ngithub.com/cncf/xds/go v0.0.0-20211011173535-cb28da3451f1/go.mod h1:eXthEFrGJvWHgFFCl3hGmgk+/aYT6PnTQLykKQRLhEs=\ngithub.com/cockroachdb/apd v1.1.0 h1:3LFP3629v+1aKXU5Q37mxmRxX/pIu1nijXydLShEq5I=\ngithub.com/cockroachdb/apd v1.1.0/go.mod h1:8Sl8LxpKi29FqWXR16WEFZRNSz3SoPzUzeMeY4+DwBQ=\ngithub.com/cockroachdb/datadriven v0.0.0-20190809214429-80d97fb3cbaa/go.mod h1:zn76sxSg3SzpJ0PPJaLDCu+Bu0Lg3sKTORVIj19EIF8=\ngithub.com/cockroachdb/datadriven v0.0.0-20200714090401-bf6692d28da5 h1:xD/lrqdvwsc+O2bjSSi3YqY73Ke3LAiSCx49aCesA0E=\ngithub.com/cockroachdb/datadriven v0.0.0-20200714090401-bf6692d28da5/go.mod h1:h6jFvWxBdQXxjopDMZyH2UVceIRfR84bdzbkoKrsWNo=\ngithub.com/cockroachdb/errors v1.2.4 h1:Lap807SXTH5tri2TivECb/4abUkMZC9zRoLarvcKDqs=\ngithub.com/cockroachdb/errors v1.2.4/go.mod h1:rQD95gz6FARkaKkQXUksEje/d9a6wBJoCr5oaCLELYA=\ngithub.com/cockroachdb/logtags v0.0.0-20190617123548-eb05cc24525f h1:o/kfcElHqOiXqcou5a3rIlMc7oJbMQkeLk0VQJ7zgqY=\ngithub.com/cockroachdb/logtags v0.0.0-20190617123548-eb05cc24525f/go.mod h1:i/u985jwjWRlyHXQbwatDASoW0RMlZ/3i9yJHE2xLkI=\ngithub.com/codahale/hdrhistogram v0.0.0-20161010025455-3a0bb77429bd/go.mod h1:sE/e/2PUdi/liOCUjSTXgM1o87ZssimdTWN964YiIeI=\ngithub.com/confluentinc/confluent-kafka-go v1.4.0/go.mod h1:u2zNLny2xq+5rWeTQjFHbDzzNuba4P1vo31r9r4uAdg=\ngithub.com/confluentinc/confluent-kafka-go v1.9.1 h1:L3aW6KvTyrq/+BOMnDm9xJylhAEoAgqhoaJbMPe3GQI=\ngithub.com/confluentinc/confluent-kafka-go v1.9.1/go.mod h1:ptXNqsuDfYbAE/LBW6pnwWZElUoWxHoV8E43DCrliyo=\ngithub.com/coreos/bbolt v1.3.2/go.mod h1:iRUV2dpdMOn7Bo10OQBFzIJO9kkE559Wcmn+qkEiiKk=\ngithub.com/coreos/etcd v3.3.10+incompatible/go.mod h1:uF7uidLiAD3TWHmW31ZFd/JWoc32PjwdhPthX9715RE=\ngithub.com/coreos/etcd v3.3.13+incompatible h1:8F3hqu9fGYLBifCmRCJsicFqDx/D68Rt3q1JMazcgBQ=\ngithub.com/coreos/etcd v3.3.13+incompatible/go.mod h1:uF7uidLiAD3TWHmW31ZFd/JWoc32PjwdhPthX9715RE=\ngithub.com/coreos/go-semver v0.2.0/go.mod h1:nnelYz7RCh+5ahJtPPxZlU+153eP4D4r3EedlOD2RNk=\ngithub.com/coreos/go-semver v0.3.0 h1:wkHLiw0WNATZnSG7epLsujiMCgPAc9xhjJ4tgnAxmfM=\ngithub.com/coreos/go-semver v0.3.0/go.mod h1:nnelYz7RCh+5ahJtPPxZlU+153eP4D4r3EedlOD2RNk=\ngithub.com/coreos/go-systemd v0.0.0-20180511133405-39ca1b05acc7/go.mod h1:F5haX7vjVVG0kc13fIWeqUViNPyEJxv/OmvnBo0Yme4=\ngithub.com/coreos/go-systemd v0.0.0-20190321100706-95778dfbb74e/go.mod h1:F5haX7vjVVG0kc13fIWeqUViNPyEJxv/OmvnBo0Yme4=\ngithub.com/coreos/go-systemd v0.0.0-20190719114852-fd7a80b32e1f/go.mod h1:F5haX7vjVVG0kc13fIWeqUViNPyEJxv/OmvnBo0Yme4=\ngithub.com/coreos/go-systemd/v22 v22.3.2 h1:D9/bQk5vlXQFZ6Kwuu6zaiXJ9oTPe68++AzAJc1DzSI=\ngithub.com/coreos/go-systemd/v22 v22.3.2/go.mod h1:Y58oyj3AT4RCenI/lSvhwexgC+NSVTIJ3seZv2GcEnc=\ngithub.com/coreos/pkg v0.0.0-20160727233714-3ac0863d7acf/go.mod h1:E3G3o1h8I7cfcXa63jLwjI0eiQQMgzzUDFVpN/nH/eA=\ngithub.com/coreos/pkg v0.0.0-20180928190104-399ea9e2e55f/go.mod h1:E3G3o1h8I7cfcXa63jLwjI0eiQQMgzzUDFVpN/nH/eA=\ngithub.com/cpuguy83/go-md2man/v2 v2.0.0-20190314233015-f79a8a8ca69d/go.mod h1:maD7wRr/U5Z6m/iR4s+kqSMx2CaBsrgA7czyZG/E6dU=\ngithub.com/cpuguy83/go-md2man/v2 v2.0.0/go.mod h1:maD7wRr/U5Z6m/iR4s+kqSMx2CaBsrgA7czyZG/E6dU=\ngithub.com/cpuguy83/go-md2man/v2 v2.0.2/go.mod h1:tgQtvFlXSQOSOSIRvRPT7W67SCa46tRHOmNcaadrF8o=\ngithub.com/creack/pty v1.1.7/go.mod h1:lj5s0c3V2DBrqTV7llrYr5NG6My20zk30Fl46Y7DoTY=\ngithub.com/creack/pty v1.1.9/go.mod h1:oKZEueFk5CKHvIhNR5MUki03XCEU+Q6VDXinZuGJ33E=\ngithub.com/creack/pty v1.1.11/go.mod h1:oKZEueFk5CKHvIhNR5MUki03XCEU+Q6VDXinZuGJ33E=\ngithub.com/davecgh/go-spew v0.0.0-20151105211317-5215b55f46b2/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/denisenkom/go-mssqldb v0.0.0-20200428022330-06a60b6afbbc/go.mod h1:xbL0rPBG9cCiLr28tMa8zpbdarY27NDyej4t/EjAShU=\ngithub.com/denisenkom/go-mssqldb v0.11.0 h1:9rHa233rhdOyrz2GcP9NM+gi2psgJZ4GWDpL/7ND8HI=\ngithub.com/denisenkom/go-mssqldb v0.11.0/go.mod h1:xbL0rPBG9cCiLr28tMa8zpbdarY27NDyej4t/EjAShU=\ngithub.com/desertbit/timer v0.0.0-20180107155436-c41aec40b27f h1:U5y3Y5UE0w7amNe7Z5G/twsBW0KEalRQXZzf8ufSh9I=\ngithub.com/desertbit/timer v0.0.0-20180107155436-c41aec40b27f/go.mod h1:xH/i4TFMt8koVQZ6WFms69WAsDWr2XsYL3Hkl7jkoLE=\ngithub.com/dgraph-io/ristretto v0.1.0 h1:Jv3CGQHp9OjuMBSne1485aDpUkTKEcUqF+jm/LuerPI=\ngithub.com/dgraph-io/ristretto v0.1.0/go.mod h1:fux0lOrBhrVCJd3lcTHsIJhq1T2rokOu6v9Vcb3Q9ug=\ngithub.com/dgrijalva/jwt-go v3.2.0+incompatible/go.mod h1:E3ru+11k8xSBh+hMPgOLZmtrrCbhqsmaPHjLKYnJCaQ=\ngithub.com/dgryski/go-farm v0.0.0-20190423205320-6a90982ecee2 h1:tdlZCpZ/P9DhczCTSixgIKmwPv6+wP5DGjqLYw5SUiA=\ngithub.com/dgryski/go-farm v0.0.0-20190423205320-6a90982ecee2/go.mod h1:SqUrOPUnsFjfmXRMNPybcSiG0BgUW2AuFH8PAnS2iTw=\ngithub.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f/go.mod h1:cuUVRXasLTGF7a8hSLbxyZXjz+1KgoB3wDUb6vlszIc=\ngithub.com/dgryski/go-sip13 v0.0.0-20181026042036-e10d5fee7954/go.mod h1:vAd38F8PWV+bWy6jNmig1y/TA+kYO4g3RSRF0IAv0no=\ngithub.com/docker/spdystream v0.0.0-20160310174837-449fdfce4d96/go.mod h1:Qh8CwZgvJUkLughtfhJv5dyTYa91l1fOUCrgjqmcifM=\ngithub.com/dustin/go-humanize v0.0.0-20171111073723-bb3d318650d4/go.mod h1:HtrtbFcZ19U5GC7JDqmcUSB87Iq5E25KnS6fMYU6eOk=\ngithub.com/dustin/go-humanize v1.0.0 h1:VSnTsYCnlFHaM2/igO1h6X3HA71jcobQuxemgkq4zYo=\ngithub.com/dustin/go-humanize v1.0.0/go.mod h1:HtrtbFcZ19U5GC7JDqmcUSB87Iq5E25KnS6fMYU6eOk=\ngithub.com/eapache/go-resiliency v1.1.0/go.mod h1:kFI+JgMyC7bLPUVY133qvEBtVayf5mFgVsvEsIPBvNs=\ngithub.com/eapache/go-xerial-snappy v0.0.0-20180814174437-776d5712da21/go.mod h1:+020luEh2TKB4/GOp8oxxtq0Daoen/Cii55CzbTV6DU=\ngithub.com/eapache/queue v1.1.0/go.mod h1:6eCeP0CKFpHLu8blIFXhExK/dRa7WDZfr6jVFPTqq+I=\ngithub.com/edsrzf/mmap-go v1.0.0/go.mod h1:YO35OhQPt3KJa3ryjFM5Bs14WD66h8eGKpfaBNrHW5M=\ngithub.com/elastic/go-elasticsearch/v6 v6.8.5/go.mod h1:UwaDJsD3rWLM5rKNFzv9hgox93HoX8utj1kxD9aFUcI=\ngithub.com/elastic/go-elasticsearch/v7 v7.17.1/go.mod h1:OJ4wdbtDNk5g503kvlHLyErCgQwwzmDtaFC4XyOxXA4=\ngithub.com/elazarl/goproxy v0.0.0-20170405201442-c4fc26588b6e/go.mod h1:/Zj4wYkgs4iZTTu3o/KG3Itv/qCCa8VVMlb3i9OVuzc=\ngithub.com/emicklei/go-restful v0.0.0-20170410110728-ff4f55a20633/go.mod h1:otzb+WCGbkyDHkqmQmT5YD2WR4BBwUdeQoFo8l/7tVs=\ngithub.com/envoyproxy/go-control-plane v0.6.9/go.mod h1:SBwIajubJHhxtWwsL9s8ss4safvEdbitLhGGK48rN6g=\ngithub.com/envoyproxy/go-control-plane v0.9.0/go.mod h1:YTl/9mNaCwkRvm6d1a2C3ymFceY/DCBVvsKhRF0iEA4=\ngithub.com/envoyproxy/go-control-plane v0.9.1-0.20191026205805-5f8ba28d4473/go.mod h1:YTl/9mNaCwkRvm6d1a2C3ymFceY/DCBVvsKhRF0iEA4=\ngithub.com/envoyproxy/go-control-plane v0.9.4/go.mod h1:6rpuAdCZL397s3pYoYcLgu1mIlRU8Am5FuJP05cCM98=\ngithub.com/envoyproxy/go-control-plane v0.9.7/go.mod h1:cwu0lG7PUMfa9snN8LXBig5ynNVH9qI8YYLbd1fK2po=\ngithub.com/envoyproxy/go-control-plane v0.9.9-0.20201210154907-fd9021fe5dad/go.mod h1:cXg6YxExXjJnVBQHBLXeUAgxn2UodCpnH306RInaBQk=\ngithub.com/envoyproxy/go-control-plane v0.9.9-0.20210217033140-668b12f5399d/go.mod h1:cXg6YxExXjJnVBQHBLXeUAgxn2UodCpnH306RInaBQk=\ngithub.com/envoyproxy/go-control-plane v0.9.9-0.20210512163311-63b5d3c536b0/go.mod h1:hliV/p42l8fGbc6Y9bQ70uLwIvmJyVE5k4iMKlh8wCQ=\ngithub.com/envoyproxy/go-control-plane v0.9.10-0.20210907150352-cf90f659a021/go.mod h1:AFq3mo9L8Lqqiid3OhADV3RfLJnjiw63cSpi+fDTRC0=\ngithub.com/envoyproxy/go-control-plane v0.10.2-0.20220325020618-49ff273808a1/go.mod h1:KJwIaB5Mv44NWtYuAOFCVOjcI94vtpEz2JU/D2v6IjE=\ngithub.com/envoyproxy/protoc-gen-validate v0.1.0/go.mod h1:iSmxcyjqTsJpI2R4NaDN7+kN2VEUnK/pcBlmesArF7c=\ngithub.com/erikstmartin/go-testdb v0.0.0-20160219214506-8d10e4a1bae5/go.mod h1:a2zkGnVExMxdzMo3M0Hi/3sEU+cWnZpSni0O6/Yb/P0=\ngithub.com/evanphx/json-patch v4.2.0+incompatible/go.mod h1:50XU6AFN0ol/bzJsmQLiYLvXMP4fmwYFNcr97nuDLSk=\ngithub.com/fatih/color v1.7.0/go.mod h1:Zm6kSWBoL9eyXnKyktHP6abPY2pDugNf5KwzbycvMj4=\ngithub.com/fatih/color v1.9.0/go.mod h1:eQcE1qtQxscV5RaZvpXrrb8Drkc3/DdQ+uUYCNjL+zU=\ngithub.com/fatih/color v1.13.0 h1:8LOYc1KYPPmyKMuN8QV2DNRWNbLo6LZ0iLs8+mlH53w=\ngithub.com/fatih/color v1.13.0/go.mod h1:kLAiJbzzSOZDVNGyDpeOxJ47H46qBXwg5ILebYFFOfk=\ngithub.com/fatih/structs v1.1.0 h1:Q7juDM0QtcnhCpeyLGQKyg4TOIghuNXrkL32pHAUMxo=\ngithub.com/fatih/structs v1.1.0/go.mod h1:9NiDSp5zOcgEDl+j00MP/WkGVPOlPRLejGD8Ga6PJ7M=\ngithub.com/felixge/fgprof v0.9.2 h1:tAMHtWMyl6E0BimjVbFt7fieU6FpjttsZN7j0wT5blc=\ngithub.com/felixge/fgprof v0.9.2/go.mod h1:+VNi+ZXtHIQ6wIw6bUT8nXQRefQflWECoFyRealT5sg=\ngithub.com/fogleman/gg v1.2.1-0.20190220221249-0403632d5b90/go.mod h1:R/bRT+9gY/C5z7JzPU0zXsXHKM4/ayA+zqcVNZzPa1k=\ngithub.com/form3tech-oss/jwt-go v3.2.3+incompatible h1:7ZaBxOI7TMoYBfyA3cQHErNNyAWIKUMIwqxEtgHOs5c=\ngithub.com/form3tech-oss/jwt-go v3.2.3+incompatible/go.mod h1:pbq4aXjuKjdthFRnoDwaVPLA+WlJuPGy+QneDUgJi2k=\ngithub.com/fortytw2/leaktest v1.3.0/go.mod h1:jDsjWgpAGjm2CA7WthBh/CdZYEPF31XHquHwclZch5g=\ngithub.com/franela/goblin v0.0.0-20200105215937-c9ffbefa60db/go.mod h1:7dvUGVsVBjqR7JHJk0brhHOZYGmfBYOrK0ZhYMEtBr4=\ngithub.com/franela/goreq v0.0.0-20171204163338-bcd34c9993f8/go.mod h1:ZhphrRTfi2rbfLwlschooIH4+wKKDR4Pdxhh+TRoA20=\ngithub.com/frankban/quicktest v1.2.2/go.mod h1:Qh/WofXFeiAFII1aEBu529AtJo6Zg2VHscnEsbBnJ20=\ngithub.com/frankban/quicktest v1.7.2/go.mod h1:jaStnuzAqU1AJdCO0l53JDCJrVDKcS03DbaAcR7Ks/o=\ngithub.com/frankban/quicktest v1.10.0/go.mod h1:ui7WezCLWMWxVWr1GETZY3smRy0G4KWq9vcPtJmFl7Y=\ngithub.com/frankban/quicktest v1.13.0/go.mod h1:qLE0fzW0VuyUAJgPU19zByoIr0HtCHN/r/VLSOOIySU=\ngithub.com/frankban/quicktest v1.14.0/go.mod h1:NeW+ay9A/U67EYXNFA1nPE8e/tnQv/09mUdL/ijj8og=\ngithub.com/fsnotify/fsnotify v1.4.7/go.mod h1:jwhsz4b93w/PPRr/qN1Yymfu8t87LnFCMoQvtojpjFo=\ngithub.com/fsnotify/fsnotify v1.4.9 h1:hsms1Qyu0jgnwNXIxa+/V/PDsU6CfLf6CNO8H7IWoS4=\ngithub.com/fsnotify/fsnotify v1.4.9/go.mod h1:znqG4EE+3YCdAaPaxE2ZRY/06pZUdp0tY4IgpuI1SZQ=\ngithub.com/garyburd/redigo v1.6.3/go.mod h1:rTb6epsqigu3kYKBnaF028A7Tf/Aw5s0cqA47doKKqw=\ngithub.com/getsentry/raven-go v0.2.0 h1:no+xWJRb5ZI7eE8TWgIq1jLulQiIoLG0IfYxv5JYMGs=\ngithub.com/getsentry/raven-go v0.2.0/go.mod h1:KungGk8q33+aIAZUIVWZDr2OfAEBsO49PX4NzFV5kcQ=\ngithub.com/getsentry/sentry-go v0.13.0 h1:20dgTiUSfxRB/EhMPtxcL9ZEbM1ZdR+W/7f7NWD+xWo=\ngithub.com/getsentry/sentry-go v0.13.0/go.mod h1:EOsfu5ZdvKPfeHYV6pTVQnsjfp30+XA7//UooKNumH0=\ngithub.com/ghodss/yaml v0.0.0-20150909031657-73d445a93680/go.mod h1:4dBDuWmgqj2HViK6kFavaiC9ZROes6MMH2rRYeMEF04=\ngithub.com/ghodss/yaml v1.0.0/go.mod h1:4dBDuWmgqj2HViK6kFavaiC9ZROes6MMH2rRYeMEF04=\ngithub.com/gin-contrib/sse v0.1.0 h1:Y/yl/+YNO8GZSjAhjMsSuLt29uWRFHdHYUb5lYOV9qE=\ngithub.com/gin-contrib/sse v0.1.0/go.mod h1:RHrZQHXnP2xjPF+u1gW/2HnVO7nvIa9PG3Gm+fLHvGI=\ngithub.com/gin-gonic/gin v1.6.3/go.mod h1:75u5sXoLsGZoRN5Sgbi1eraJ4GU3++wFwWzhwvtwp4M=\ngithub.com/gin-gonic/gin v1.7.0/go.mod h1:jD2toBW3GZUr5UMcdrwQA10I7RuaFOl/SGeDjXkfUtY=\ngithub.com/gin-gonic/gin v1.7.7 h1:3DoBmSbJbZAWqXJC3SLjAPfutPJJRN1U5pALB7EeTTs=\ngithub.com/globalsign/mgo v0.0.0-20181015135952-eeefdecb41b8/go.mod h1:xkRDCp4j0OGD1HRkm4kmhM+pmpv3AKq5SU7GMg4oO/Q=\ngithub.com/glycerine/vprint v0.0.0-20200730000117-76cea49a68ea h1:Uiuhuh77mImdrAMjPfw2V8tWw4AF6r9dxbNkECo23SA=\ngithub.com/glycerine/vprint v0.0.0-20200730000117-76cea49a68ea/go.mod h1:q7RHAiHHxYrXtGEkX14OuACg+cHODdKnVvTgpBnOzHk=\ngithub.com/go-asn1-ber/asn1-ber v1.3.1/go.mod h1:hEBeB/ic+5LoWskz+yKT7vGhhPYkProFKoKdwZRWMe0=\ngithub.com/go-chi/chi v1.5.0/go.mod h1:REp24E+25iKvxgeTfHmdUoL5x15kBiDBlnIl5bCwe2k=\ngithub.com/go-chi/chi/v5 v5.0.0/go.mod h1:BBug9lr0cqtdAhsu6R4AAdvufI0/XBzAQSsUqJpoZOs=\ngithub.com/go-errors/errors v1.0.1 h1:LUHzmkK3GUKUrL/1gfBUxAHzcev3apQlezX/+O7ma6w=\ngithub.com/go-gl/glfw v0.0.0-20190409004039-e6da0acd62b1/go.mod h1:vR7hzQXu2zJy9AVAgeJqvqgH9Q5CA+iKCZ2gyEVpxRU=\ngithub.com/go-gl/glfw/v3.3/glfw v0.0.0-20191125211704-12ad95a8df72/go.mod h1:tQ2UAYgL5IevRw8kRxooKSPJfGvJ9fJQFa0TUsXzTg8=\ngithub.com/go-gl/glfw/v3.3/glfw v0.0.0-20200222043503-6f7a984d4dc4/go.mod h1:tQ2UAYgL5IevRw8kRxooKSPJfGvJ9fJQFa0TUsXzTg8=\ngithub.com/go-kit/kit v0.8.0/go.mod h1:xBxKIO96dXMWWy0MnWVtmwkA9/13aqxPnvrjFYMA2as=\ngithub.com/go-kit/kit v0.9.0/go.mod h1:xBxKIO96dXMWWy0MnWVtmwkA9/13aqxPnvrjFYMA2as=\ngithub.com/go-kit/kit v0.10.0/go.mod h1:xUsJbQ/Fp4kEt7AFgCuvyX4a71u8h9jB8tj/ORgOZ7o=\ngithub.com/go-kit/log v0.1.0/go.mod h1:zbhenjAZHb184qTLMA9ZjW7ThYL0H2mk7Q6pNt4vbaY=\ngithub.com/go-kit/log v0.2.0/go.mod h1:NwTd00d/i8cPZ3xOwwiv2PO5MOcx78fFErGNcVmBjv0=\ngithub.com/go-ldap/ldap/v3 v3.1.3/go.mod h1:3rbOH3jRS2u6jg2rJnKAMLE/xQyCKIveG2Sa/Cohzb8=\ngithub.com/go-logfmt/logfmt v0.3.0/go.mod h1:Qt1PoO58o5twSAckw1HlFXLmHsOX5/0LbT9GBnD5lWE=\ngithub.com/go-logfmt/logfmt v0.4.0/go.mod h1:3RMwSq7FuexP4Kalkev3ejPJsZTpXXBr9+V4qmtdjCk=\ngithub.com/go-logfmt/logfmt v0.5.0/go.mod h1:wCYkCAKZfumFQihp8CzCvQ3paCTfi41vtzG1KdI/P7A=\ngithub.com/go-logfmt/logfmt v0.5.1/go.mod h1:WYhtIu8zTZfxdn5+rREduYbwxfcBr/Vr6KEVveWlfTs=\ngithub.com/go-logr/logr v0.1.0/go.mod h1:ixOQHD9gLJUVQQ2ZOR7zLEifBX6tGkNJF4QyIY7sIas=\ngithub.com/go-ole/go-ole v1.2.5/go.mod h1:pprOEPIfldk/42T2oK7lQ4v4JSDwmV0As9GaiUsvbm0=\ngithub.com/go-ole/go-ole v1.2.6 h1:/Fpf6oFPoeFik9ty7siob0G6Ke8QvQEuVcuChpwXzpY=\ngithub.com/go-ole/go-ole v1.2.6/go.mod h1:pprOEPIfldk/42T2oK7lQ4v4JSDwmV0As9GaiUsvbm0=\ngithub.com/go-openapi/errors v0.19.8 h1:doM+tQdZbUm9gydV9yR+iQNmztbjj7I3sW4sIcAwIzc=\ngithub.com/go-openapi/errors v0.19.8/go.mod h1:cM//ZKUKyO06HSwqAelJ5NsEMMcpa6VpXe8DOa1Mi1M=\ngithub.com/go-openapi/jsonpointer v0.0.0-20160704185906-46af16f9f7b1/go.mod h1:+35s3my2LFTysnkMfxsJBAMHj/DoqoB9knIWoYG/Vk0=\ngithub.com/go-openapi/jsonreference v0.0.0-20160704190145-13c6e3589ad9/go.mod h1:W3Z9FmVs9qj+KR4zFKmDPGiLdk1D9Rlm7cyMvf57TTg=\ngithub.com/go-openapi/spec v0.0.0-20160808142527-6aced65f8501/go.mod h1:J8+jY1nAiCcj+friV/PDoE1/3eeccG9LYBs0tYvLOWc=\ngithub.com/go-openapi/strfmt v0.21.2 h1:5NDNgadiX1Vhemth/TH4gCGopWSTdDjxl60H3B7f+os=\ngithub.com/go-openapi/strfmt v0.21.2/go.mod h1:I/XVKeLc5+MM5oPNN7P6urMOpuLXEcNrCX/rPGuWb0k=\ngithub.com/go-openapi/swag v0.0.0-20160704191624-1d0bd113de87/go.mod h1:DXUve3Dpr1UfpPtxFw+EFuQ41HhCWZfha5jSVRG7C7I=\ngithub.com/go-pg/pg/v10 v10.0.0/go.mod h1:XHU1AkQW534GFuUdSiQ46+Xw6Ah+9+b8DlT4YwhiXL8=\ngithub.com/go-pg/zerochecker v0.2.0/go.mod h1:NJZ4wKL0NmTtz0GKCoJ8kym6Xn/EQzXRl2OnAe7MmDo=\ngithub.com/go-playground/assert/v2 v2.0.1/go.mod h1:VDjEfimB/XKnb+ZQfWdccd7VUvScMdVu0Titje2rxJ4=\ngithub.com/go-playground/locales v0.13.0 h1:HyWk6mgj5qFqCT5fjGBuRArbVDfE4hi8+e8ceBS/t7Q=\ngithub.com/go-playground/locales v0.13.0/go.mod h1:taPMhCMXrRLJO55olJkUXHZBHCxTMfnGwq/HNwmWNS8=\ngithub.com/go-playground/universal-translator v0.17.0 h1:icxd5fm+REJzpZx7ZfpaD876Lmtgy7VtROAbHHXk8no=\ngithub.com/go-playground/universal-translator v0.17.0/go.mod h1:UkSxE5sNxxRwHyU+Scu5vgOQjsIJAF8j9muTVoKLVtA=\ngithub.com/go-playground/validator/v10 v10.2.0/go.mod h1:uOYAAleCW8F/7oMFd6aG0GOhaH6EGOAJShg8Id5JGkI=\ngithub.com/go-playground/validator/v10 v10.4.1 h1:pH2c5ADXtd66mxoE0Zm9SUhxE20r7aM3F26W0hOn+GE=\ngithub.com/go-playground/validator/v10 v10.4.1/go.mod h1:nlOn6nFhuKACm19sB/8EGNn9GlaMV7XkbRSipzJ0Ii4=\ngithub.com/go-redis/redis v6.15.9+incompatible/go.mod h1:NAIEuMOZ/fxfXJIrKDQDz8wamY7mA7PouImQ2Jvg6kA=\ngithub.com/go-redis/redis/v7 v7.1.0/go.mod h1:JDNMw23GTyLNC4GZu9njt15ctBQVn7xjRfnwdHj/Dcg=\ngithub.com/go-redis/redis/v8 v8.0.0/go.mod h1:isLoQT/NFSP7V67lyvM9GmdvLdyZ7pEhsXvvyQtnQTo=\ngithub.com/go-sql-driver/mysql v1.4.0/go.mod h1:zAC/RDZ24gD3HViQzih4MyKcchzm+sOG5ZlKdlhCg5w=\ngithub.com/go-sql-driver/mysql v1.5.0/go.mod h1:DCzpHaOWr8IXmIStZouvnhqoel9Qv2LBy8hT2VhHyBg=\ngithub.com/go-sql-driver/mysql v1.6.0/go.mod h1:DCzpHaOWr8IXmIStZouvnhqoel9Qv2LBy8hT2VhHyBg=\ngithub.com/go-sql-driver/mysql v1.7.0 h1:ueSltNNllEqE3qcWBTD0iQd3IpL/6U+mJxLkazJ7YPc=\ngithub.com/go-sql-driver/mysql v1.7.0/go.mod h1:OXbVy3sEdcQ2Doequ6Z5BW6fXNQTmx+9S1MCJN5yJMI=\ngithub.com/go-stack/stack v1.8.0 h1:5SgMzNM5HxrEjV0ww2lTmX6E2Izsfxas4+YHWRs3Lsk=\ngithub.com/go-stack/stack v1.8.0/go.mod h1:v0f6uXyyMGvRgIKkXu+yp6POWl0qKG85gN/melR3HDY=\ngithub.com/go-task/slim-sprig v0.0.0-20210107165309-348f09dbbbc0/go.mod h1:fyg7847qk6SyHyPtNmDHnmrv/HOrqktSC+C9fM+CJOE=\ngithub.com/go-test/deep v1.0.2-0.20181118220953-042da051cf31/go.mod h1:wGDj63lr65AM2AQyKZd/NYHGb0R+1RLqB8NKt3aSFNA=\ngithub.com/go-test/deep v1.0.2/go.mod h1:wGDj63lr65AM2AQyKZd/NYHGb0R+1RLqB8NKt3aSFNA=\ngithub.com/go-test/deep v1.0.7 h1:/VSMRlnY/JSyqxQUzQLKVMAskpY/NZKFA5j2P+0pP2M=\ngithub.com/go-test/deep v1.0.7/go.mod h1:QV8Hv/iy04NyLBxAdO9njL0iVPN1S4d/A3NVv1V36o8=\ngithub.com/gobuffalo/attrs v0.0.0-20190224210810-a9411de4debd/go.mod h1:4duuawTqi2wkkpB4ePgWMaai6/Kc6WEz83bhFwpHzj0=\ngithub.com/gobuffalo/attrs v1.0.3/go.mod h1:KvDJCE0avbufqS0Bw3UV7RQynESY0jjod+572ctX4t8=\ngithub.com/gobuffalo/depgen v0.0.0-20190329151759-d478694a28d3/go.mod h1:3STtPUQYuzV0gBVOY3vy6CfMm/ljR4pABfrTeHNLHUY=\ngithub.com/gobuffalo/depgen v0.1.0/go.mod h1:+ifsuy7fhi15RWncXQQKjWS9JPkdah5sZvtHc2RXGlg=\ngithub.com/gobuffalo/envy v1.6.15/go.mod h1:n7DRkBerg/aorDM8kbduw5dN3oXGswK5liaSCx4T5NI=\ngithub.com/gobuffalo/envy v1.7.0/go.mod h1:n7DRkBerg/aorDM8kbduw5dN3oXGswK5liaSCx4T5NI=\ngithub.com/gobuffalo/envy v1.10.2 h1:EIi03p9c3yeuRCFPOKcSfajzkLb3hrRjEpHGI8I2Wo4=\ngithub.com/gobuffalo/envy v1.10.2/go.mod h1:qGAGwdvDsaEtPhfBzb3o0SfDea8ByGn9j8bKmVft9z8=\ngithub.com/gobuffalo/fizz v1.14.4 h1:8uume7joF6niTNWN582IQ2jhGTUoa9g1fiV/tIoGdBs=\ngithub.com/gobuffalo/fizz v1.14.4/go.mod h1:9/2fGNXNeIFOXEEgTPJwiK63e44RjG+Nc4hfMm1ArGM=\ngithub.com/gobuffalo/flect v0.1.0/go.mod h1:d2ehjJqGOH/Kjqcoz+F7jHTBbmDb38yXA598Hb50EGs=\ngithub.com/gobuffalo/flect v0.1.1/go.mod h1:8JCgGVbRjJhVgD6399mQr4fx5rRfGKVzFjbj6RE/9UI=\ngithub.com/gobuffalo/flect v0.1.3/go.mod h1:8JCgGVbRjJhVgD6399mQr4fx5rRfGKVzFjbj6RE/9UI=\ngithub.com/gobuffalo/flect v0.3.0/go.mod h1:5pf3aGnsvqvCj50AVni7mJJF8ICxGZ8HomberC3pXLE=\ngithub.com/gobuffalo/flect v1.0.0 h1:eBFmskjXZgAOagiTXJH25Nt5sdFwNRcb8DKZsIsAUQI=\ngithub.com/gobuffalo/flect v1.0.0/go.mod h1:l9V6xSb4BlXwsxEMj3FVEub2nkdQjWhPvD8XTTlHPQc=\ngithub.com/gobuffalo/genny v0.0.0-20190329151137-27723ad26ef9/go.mod h1:rWs4Z12d1Zbf19rlsn0nurr75KqhYp52EAGGxTbBhNk=\ngithub.com/gobuffalo/genny v0.0.0-20190403191548-3ca520ef0d9e/go.mod h1:80lIj3kVJWwOrXWWMRzzdhW3DsrdjILVil/SFKBzF28=\ngithub.com/gobuffalo/genny v0.1.0/go.mod h1:XidbUqzak3lHdS//TPu2OgiFB+51Ur5f7CSnXZ/JDvo=\ngithub.com/gobuffalo/genny v0.1.1/go.mod h1:5TExbEyY48pfunL4QSXxlDOmdsD44RRq4mVZ0Ex28Xk=\ngithub.com/gobuffalo/genny/v2 v2.1.0/go.mod h1:4yoTNk4bYuP3BMM6uQKYPvtP6WsXFGm2w2EFYZdRls8=\ngithub.com/gobuffalo/gitgen v0.0.0-20190315122116-cc086187d211/go.mod h1:vEHJk/E9DmhejeLeNt7UVvlSGv3ziL+djtTr3yyzcOw=\ngithub.com/gobuffalo/github_flavored_markdown v1.1.3 h1:rSMPtx9ePkFB22vJ+dH+m/EUBS8doQ3S8LeEXcdwZHk=\ngithub.com/gobuffalo/github_flavored_markdown v1.1.3/go.mod h1:IzgO5xS6hqkDmUh91BW/+Qxo/qYnvfzoz3A7uLkg77I=\ngithub.com/gobuffalo/gogen v0.0.0-20190315121717-8f38393713f5/go.mod h1:V9QVDIxsgKNZs6L2IYiGR8datgMhB577vzTDqypH360=\ngithub.com/gobuffalo/gogen v0.1.0/go.mod h1:8NTelM5qd8RZ15VjQTFkAW6qOMx5wBbW4dSCS3BY8gg=\ngithub.com/gobuffalo/gogen v0.1.1/go.mod h1:y8iBtmHmGc4qa3urIyo1shvOD8JftTtfcKi+71xfDNE=\ngithub.com/gobuffalo/helpers v0.6.7 h1:C9CedoRSfgWg2ZoIkVXgjI5kgmSpL34Z3qdnzpfNVd8=\ngithub.com/gobuffalo/helpers v0.6.7/go.mod h1:j0u1iC1VqlCaJEEVkZN8Ia3TEzfj/zoXANqyJExTMTA=\ngithub.com/gobuffalo/logger v0.0.0-20190315122211-86e12af44bc2/go.mod h1:QdxcLw541hSGtBnhUc4gaNIXRjiDppFGaDqzbrBd3v8=\ngithub.com/gobuffalo/logger v1.0.7/go.mod h1:u40u6Bq3VVvaMcy5sRBclD8SXhBYPS0Qk95ubt+1xJM=\ngithub.com/gobuffalo/mapi v1.0.1/go.mod h1:4VAGh89y6rVOvm5A8fKFxYG+wIW6LO1FMTG9hnKStFc=\ngithub.com/gobuffalo/mapi v1.0.2/go.mod h1:4VAGh89y6rVOvm5A8fKFxYG+wIW6LO1FMTG9hnKStFc=\ngithub.com/gobuffalo/nulls v0.4.2 h1:GAqBR29R3oPY+WCC7JL9KKk9erchaNuV6unsOSZGQkw=\ngithub.com/gobuffalo/nulls v0.4.2/go.mod h1:EElw2zmBYafU2R9W4Ii1ByIj177wA/pc0JdjtD0EsH8=\ngithub.com/gobuffalo/packd v0.0.0-20190315124812-a385830c7fc0/go.mod h1:M2Juc+hhDXf/PnmBANFCqx4DM3wRbgDvnVWeG2RIxq4=\ngithub.com/gobuffalo/packd v0.1.0/go.mod h1:M2Juc+hhDXf/PnmBANFCqx4DM3wRbgDvnVWeG2RIxq4=\ngithub.com/gobuffalo/packd v1.0.2/go.mod h1:sUc61tDqGMXON80zpKGp92lDb86Km28jfvX7IAyxFT8=\ngithub.com/gobuffalo/packr/v2 v2.0.9/go.mod h1:emmyGweYTm6Kdper+iywB6YK5YzuKchGtJQZ0Odn4pQ=\ngithub.com/gobuffalo/packr/v2 v2.2.0/go.mod h1:CaAwI0GPIAv+5wKLtv8Afwl+Cm78K/I/VCm/3ptBN+0=\ngithub.com/gobuffalo/plush/v4 v4.1.16/go.mod h1:6t7swVsarJ8qSLw1qyAH/KbrcSTwdun2ASEQkOznakg=\ngithub.com/gobuffalo/plush/v4 v4.1.18 h1:bnPjdMTEUQHqj9TNX2Ck3mxEXYZa+0nrFMNM07kpX9g=\ngithub.com/gobuffalo/plush/v4 v4.1.18/go.mod h1:xi2tJIhFI4UdzIL8sxZtzGYOd2xbBpcFbLZlIPGGZhU=\ngithub.com/gobuffalo/pop/v6 v6.1.1 h1:eUDBaZcb0gYrmFnKwpuTEUA7t5ZHqNfvS4POqJYXDZY=\ngithub.com/gobuffalo/pop/v6 v6.1.1/go.mod h1:1n7jAmI1i7fxuXPZjZb0VBPQDbksRtCoFnrDV5IsvaI=\ngithub.com/gobuffalo/syncx v0.0.0-20190224160051-33c29581e754/go.mod h1:HhnNqWY95UYwwW3uSASeV7vtgYkT2t16hJgV3AEPUpw=\ngithub.com/gobuffalo/tags/v3 v3.1.4 h1:X/ydLLPhgXV4h04Hp2xlbI2oc5MDaa7eub6zw8oHjsM=\ngithub.com/gobuffalo/tags/v3 v3.1.4/go.mod h1:ArRNo3ErlHO8BtdA0REaZxijuWnWzF6PUXngmMXd2I0=\ngithub.com/gobuffalo/validate/v3 v3.3.3 h1:o7wkIGSvZBYBd6ChQoLxkz2y1pfmhbI4jNJYh6PuNJ4=\ngithub.com/gobuffalo/validate/v3 v3.3.3/go.mod h1:YC7FsbJ/9hW/VjQdmXPvFqvRis4vrRYFxr69WiNZw6g=\ngithub.com/gobwas/httphead v0.0.0-20180130184737-2c6c146eadee/go.mod h1:L0fX3K22YWvt/FAX9NnzrNzcI4wNYi9Yku4O0LKYflo=\ngithub.com/gobwas/httphead v0.0.0-20200921212729-da3d93bc3c58 h1:YyrUZvJaU8Q0QsoVo+xLFBgWDTam29PKea6GYmwvSiQ=\ngithub.com/gobwas/httphead v0.0.0-20200921212729-da3d93bc3c58/go.mod h1:L0fX3K22YWvt/FAX9NnzrNzcI4wNYi9Yku4O0LKYflo=\ngithub.com/gobwas/pool v0.2.0/go.mod h1:q8bcK0KcYlCgd9e7WYLm9LpyS+YeLd8JVDW6WezmKEw=\ngithub.com/gobwas/pool v0.2.1 h1:xfeeEhW7pwmX8nuLVlqbzVc7udMDrwetjEv+TZIz1og=\ngithub.com/gobwas/pool v0.2.1/go.mod h1:q8bcK0KcYlCgd9e7WYLm9LpyS+YeLd8JVDW6WezmKEw=\ngithub.com/gobwas/ws v1.0.2/go.mod h1:szmBTxLgaFppYjEmNtny/v3w89xOydFnnZMcgRRu/EM=\ngithub.com/gobwas/ws v1.0.4 h1:5eXU1CZhpQdq5kXbKb+sECH5Ia5KiO6CYzIzdlVx6Bs=\ngithub.com/gobwas/ws v1.0.4/go.mod h1:szmBTxLgaFppYjEmNtny/v3w89xOydFnnZMcgRRu/EM=\ngithub.com/goccy/go-json v0.9.11 h1:/pAaQDLHEoCq/5FFmSKBswWmK6H0e8g4159Kc/X/nqk=\ngithub.com/goccy/go-json v0.9.11/go.mod h1:6MelG93GURQebXPDq3khkgXZkazVtN9CRI+MGFi0w8I=\ngithub.com/gocql/gocql v0.0.0-20220224095938-0eacd3183625/go.mod h1:3gM2c4D3AnkISwBxGnMMsS8Oy4y2lhbPRsH4xnJrHG8=\ngithub.com/godbus/dbus/v5 v5.0.4/go.mod h1:xhWf0FNVPg57R7Z0UbKHbJfkEywrmjJnf7w5xrFpKfA=\ngithub.com/gofiber/fiber/v2 v2.11.0/go.mod h1:oZTLWqYnqpMMuF922SjGbsYZsdpE1MCfh416HNdweIM=\ngithub.com/gofrs/uuid v3.2.0+incompatible/go.mod h1:b2aQJv3Z4Fp6yNu3cdSllBxTCLRxnplIgP/c0N/04lM=\ngithub.com/gofrs/uuid v4.0.0+incompatible/go.mod h1:b2aQJv3Z4Fp6yNu3cdSllBxTCLRxnplIgP/c0N/04lM=\ngithub.com/gofrs/uuid v4.2.0+incompatible/go.mod h1:b2aQJv3Z4Fp6yNu3cdSllBxTCLRxnplIgP/c0N/04lM=\ngithub.com/gofrs/uuid v4.3.1+incompatible h1:0/KbAdpx3UXAx1kEOWHJeOkpbgRFGHVgv+CFIY7dBJI=\ngithub.com/gofrs/uuid v4.3.1+incompatible/go.mod h1:b2aQJv3Z4Fp6yNu3cdSllBxTCLRxnplIgP/c0N/04lM=\ngithub.com/gogo/googleapis v1.1.0/go.mod h1:gf4bu3Q80BeJ6H1S1vYPm8/ELATdvryBaNFGgqEef3s=\ngithub.com/gogo/protobuf v1.1.1/go.mod h1:r8qH/GZQm5c6nD/R0oafs1akxWv10x8SbQlK7atdtwQ=\ngithub.com/gogo/protobuf v1.2.0/go.mod h1:r8qH/GZQm5c6nD/R0oafs1akxWv10x8SbQlK7atdtwQ=\ngithub.com/gogo/protobuf v1.2.1/go.mod h1:hp+jE20tsWTFYpLwKvXlhS1hjn+gTNwPg2I6zVXpSg4=\ngithub.com/gogo/protobuf v1.2.2-0.20190723190241-65acae22fc9d/go.mod h1:SlYgWuQ5SjCEi6WLHjHCa1yvBfUnHcTbrrZtXPKa29o=\ngithub.com/gogo/protobuf v1.3.1/go.mod h1:SlYgWuQ5SjCEi6WLHjHCa1yvBfUnHcTbrrZtXPKa29o=\ngithub.com/gogo/protobuf v1.3.2 h1:Ov1cvc58UF3b5XjBnZv7+opcTcQFZebYjWzi34vdm4Q=\ngithub.com/gogo/protobuf v1.3.2/go.mod h1:P1XiOD3dCwIKUDQYPy72D8LYyHL2YPYrpS2s69NZV8Q=\ngithub.com/golang-jwt/jwt v3.2.2+incompatible h1:IfV12K8xAKAnZqdXVzCZ+TOjboZ2keLg81eXfW3O+oY=\ngithub.com/golang-jwt/jwt v3.2.2+incompatible/go.mod h1:8pz2t5EyA70fFQQSrl6XZXzqecmYZeUEB8OUGHkxJ+I=\ngithub.com/golang-sql/civil v0.0.0-20190719163853-cb61b32ac6fe h1:lXe2qZdvpiX5WZkZR4hgp4KJVfY3nMkvmwbVkpv1rVY=\ngithub.com/golang-sql/civil v0.0.0-20190719163853-cb61b32ac6fe/go.mod h1:8vg3r2VgvsThLBIFL93Qb5yWzgyZWhEmBwUJWevAkK0=\ngithub.com/golang/freetype v0.0.0-20170609003504-e2365dfdc4a0/go.mod h1:E/TSTwGwJL78qG/PmXZO1EjYhfJinVAhrmmHX6Z8B9k=\ngithub.com/golang/glog v0.0.0-20160126235308-23def4e6c14b h1:VKtxabqXZkF25pY9ekfRL6a582T4P37/31XEstQ5p58=\ngithub.com/golang/glog v0.0.0-20160126235308-23def4e6c14b/go.mod h1:SBH7ygxi8pfUlaOkMMuAQtPIUF8ecWP5IEl/CR7VP2Q=\ngithub.com/golang/groupcache v0.0.0-20160516000752-02826c3e7903/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/groupcache v0.0.0-20190129154638-5b532d6fd5ef/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/groupcache v0.0.0-20190702054246-869f871628b6/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/groupcache v0.0.0-20191227052852-215e87163ea7/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/groupcache v0.0.0-20200121045136-8c9f03a8e57e/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/groupcache v0.0.0-20210331224755-41bb18bfe9da/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/mock v1.1.1/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=\ngithub.com/golang/mock v1.2.0/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=\ngithub.com/golang/mock v1.3.1/go.mod h1:sBzyDLLjw3U8JLTeZvSv8jJB+tU5PVekmnlKIyFUx0Y=\ngithub.com/golang/mock v1.4.0/go.mod h1:UOMv5ysSaYNkG+OFQykRIcU/QvvxJf3p21QfJ2Bt3cw=\ngithub.com/golang/mock v1.4.1/go.mod h1:UOMv5ysSaYNkG+OFQykRIcU/QvvxJf3p21QfJ2Bt3cw=\ngithub.com/golang/mock v1.4.3/go.mod h1:UOMv5ysSaYNkG+OFQykRIcU/QvvxJf3p21QfJ2Bt3cw=\ngithub.com/golang/mock v1.4.4/go.mod h1:l3mdAwkq5BuhzHwde/uurv3sEJeZMXNpwsxVWU71h+4=\ngithub.com/golang/mock v1.5.0/go.mod h1:CWnOUgYIOo4TcNZ0wHX3YZCqsaM1I1Jvs6v3mP3KVu8=\ngithub.com/golang/mock v1.6.0/go.mod h1:p6yTPP+5HYm5mzsMV8JkE6ZKdX+/wYM6Hr+LicevLPs=\ngithub.com/golang/protobuf v0.0.0-20161109072736-4bd1920723d7/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.1/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.2/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.3/go.mod h1:vzj43D7+SQXF/4pzW/hwtAqwc6iTitCiVSaWz5lYuqw=\ngithub.com/golang/protobuf v1.3.4/go.mod h1:vzj43D7+SQXF/4pzW/hwtAqwc6iTitCiVSaWz5lYuqw=\ngithub.com/golang/protobuf v1.3.5/go.mod h1:6O5/vntMXwX2lRkT1hjjk0nAC1IDOTvTlVgjlRvqsdk=\ngithub.com/golang/protobuf v1.4.0-rc.1/go.mod h1:ceaxUfeHdC40wWswd/P6IGgMaK3YpKi5j83Wpe3EHw8=\ngithub.com/golang/protobuf v1.4.0-rc.1.0.20200221234624-67d41d38c208/go.mod h1:xKAWHe0F5eneWXFV3EuXVDTCmh+JuBKY0li0aMyXATA=\ngithub.com/golang/protobuf v1.4.0-rc.2/go.mod h1:LlEzMj4AhA7rCAGe4KMBDvJI+AwstrUpVNzEA03Pprs=\ngithub.com/golang/protobuf v1.4.0-rc.4.0.20200313231945-b860323f09d0/go.mod h1:WU3c8KckQ9AFe+yFwt9sWVRKCVIyN9cPHBJSNnbL67w=\ngithub.com/golang/protobuf v1.4.0/go.mod h1:jodUvKwWbYaEsadDk5Fwe5c77LiNKVO9IDvqG2KuDX0=\ngithub.com/golang/protobuf v1.4.1/go.mod h1:U8fpvMrcmy5pZrNK1lt4xCsGvpyWQ/VVv6QDs8UjoX8=\ngithub.com/golang/protobuf v1.4.2/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=\ngithub.com/golang/protobuf v1.4.3/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=\ngithub.com/golang/protobuf v1.5.0/go.mod h1:FsONVRAS9T7sI+LIUmWTfcYkHO4aIWwzhcaSAoJOfIk=\ngithub.com/golang/protobuf v1.5.1/go.mod h1:DopwsBzvsk0Fs44TXzsVbJyPhcCPeIwnvohx4u74HPM=\ngithub.com/golang/protobuf v1.5.2 h1:ROPKBNFfQgOUMifHyP+KYbvpjbdoFNs+aK7DXlji0Tw=\ngithub.com/golang/protobuf v1.5.2/go.mod h1:XVQd3VNwM+JqD3oG2Ue2ip4fOMUkwXdXDdiuN0vRsmY=\ngithub.com/golang/snappy v0.0.0-20180518054509-2e65f85255db/go.mod h1:/XxbfmMg8lxefKM7IXC3fBNl/7bRcc72aCRzEWrmP2Q=\ngithub.com/golang/snappy v0.0.1/go.mod h1:/XxbfmMg8lxefKM7IXC3fBNl/7bRcc72aCRzEWrmP2Q=\ngithub.com/golang/snappy v0.0.3/go.mod h1:/XxbfmMg8lxefKM7IXC3fBNl/7bRcc72aCRzEWrmP2Q=\ngithub.com/golang/snappy v0.0.4 h1:yAGX7huGHXlcLOEtBnF4w7FQwA26wojNCwOYAEhLjQM=\ngithub.com/golang/snappy v0.0.4/go.mod h1:/XxbfmMg8lxefKM7IXC3fBNl/7bRcc72aCRzEWrmP2Q=\ngithub.com/gomodule/redigo v1.7.0/go.mod h1:B4C85qUVwatsJoIUNIfCRsp7qO0iAmpGFZ4EELWSbC4=\ngithub.com/google/btree v0.0.0-20180813153112-4030bb1f1f0c/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=\ngithub.com/google/btree v1.0.0/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=\ngithub.com/google/btree v1.0.1 h1:gK4Kx5IaGY9CD5sPJ36FHiBJ6ZXl0kilRiiCj+jdYp4=\ngithub.com/google/btree v1.0.1/go.mod h1:xXMiIv4Fb/0kKde4SpL7qlzvu5cMJDRkFDxJfI9uaxA=\ngithub.com/google/flatbuffers v2.0.8+incompatible h1:ivUb1cGomAB101ZM1T0nOiWz9pSrTMoa9+EiY7igmkM=\ngithub.com/google/flatbuffers v2.0.8+incompatible/go.mod h1:1AeVuKshWv4vARoZatz6mlQ0JxURH0Kv5+zNeJKJCa8=\ngithub.com/google/go-cmp v0.2.0/go.mod h1:oXzfMopK8JAjlY9xF4vHSVASa0yLyX7SntLO5aqRK0M=\ngithub.com/google/go-cmp v0.2.1-0.20190312032427-6f77996f0c42/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.3.1/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.4.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.4.1/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.1/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.2/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.3/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.4/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.5/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.6/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.7/go.mod h1:n+brtR0CgQNWTVd5ZUFpTBC8YFBDLK/h/bpaJ8/DtOE=\ngithub.com/google/go-cmp v0.5.8 h1:e6P7q2lk1O+qJJb4BtCQXlK8vWEO8V1ZeuEdJNOqZyg=\ngithub.com/google/go-cmp v0.5.8/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=\ngithub.com/google/gofuzz v0.0.0-20161122191042-44d81051d367/go.mod h1:HP5RmnzzSNb993RKQDq4+1A4ia9nllfqcQFTQJedwGI=\ngithub.com/google/gofuzz v1.0.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=\ngithub.com/google/gofuzz v1.2.0 h1:xRy4A+RhZaiKjJ1bPfwQ8sedCA+YS2YcCHW6ec7JMi0=\ngithub.com/google/gofuzz v1.2.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=\ngithub.com/google/martian v2.1.0+incompatible/go.mod h1:9I4somxYTbIHy5NJKHRl3wXiIaQGbYVAs8BPL6v8lEs=\ngithub.com/google/martian/v3 v3.0.0/go.mod h1:y5Zk1BBys9G+gd6Jrk0W3cC1+ELVxBWuIGO+w/tUAp0=\ngithub.com/google/martian/v3 v3.1.0/go.mod h1:y5Zk1BBys9G+gd6Jrk0W3cC1+ELVxBWuIGO+w/tUAp0=\ngithub.com/google/pprof v0.0.0-20181206194817-3ea8567a2e57/go.mod h1:zfwlbNMJ+OItoe0UupaVj+oy1omPYYDuagoSzA8v9mc=\ngithub.com/google/pprof v0.0.0-20190515194954-54271f7e092f/go.mod h1:zfwlbNMJ+OItoe0UupaVj+oy1omPYYDuagoSzA8v9mc=\ngithub.com/google/pprof v0.0.0-20191218002539-d4f498aebedc/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\ngithub.com/google/pprof v0.0.0-20200212024743-f11f1df84d12/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\ngithub.com/google/pprof v0.0.0-20200229191704-1ebb73c60ed3/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\ngithub.com/google/pprof v0.0.0-20200430221834-fc25d7d30c6d/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\ngithub.com/google/pprof v0.0.0-20200708004538-1a94d8640e99/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\ngithub.com/google/pprof v0.0.0-20201023163331-3e6fc7fc9c4c/go.mod h1:kpwsk12EmLew5upagYY7GY0pfYCcupk39gWOCRROcvE=\ngithub.com/google/pprof v0.0.0-20201203190320-1bf35d6f28c2/go.mod h1:kpwsk12EmLew5upagYY7GY0pfYCcupk39gWOCRROcvE=\ngithub.com/google/pprof v0.0.0-20210122040257-d980be63207e/go.mod h1:kpwsk12EmLew5upagYY7GY0pfYCcupk39gWOCRROcvE=\ngithub.com/google/pprof v0.0.0-20210226084205-cbba55b83ad5/go.mod h1:kpwsk12EmLew5upagYY7GY0pfYCcupk39gWOCRROcvE=\ngithub.com/google/pprof v0.0.0-20210423192551-a2663126120b/go.mod h1:kpwsk12EmLew5upagYY7GY0pfYCcupk39gWOCRROcvE=\ngithub.com/google/pprof v0.0.0-20211008130755-947d60d73cc0/go.mod h1:KgnwoLYCZ8IQu3XUZ8Nc/bM9CCZFOyjUNOSygVozoDg=\ngithub.com/google/pprof v0.0.0-20211214055906-6f57359322fd h1:1FjCyPC+syAzJ5/2S8fqdZK1R22vvA0J7JZKcuOIQ7Y=\ngithub.com/google/pprof v0.0.0-20211214055906-6f57359322fd/go.mod h1:KgnwoLYCZ8IQu3XUZ8Nc/bM9CCZFOyjUNOSygVozoDg=\ngithub.com/google/renameio v0.1.0/go.mod h1:KWCgfxg9yswjAJkECMjeO8J8rahYeXnNhOm40UhjYkI=\ngithub.com/google/uuid v1.0.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/google/uuid v1.1.1/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/google/uuid v1.1.2/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/google/uuid v1.3.0 h1:t6JiXgmwXMjEs8VusXIJk2BXHsn+wx8BZdTaoZ5fu7I=\ngithub.com/google/uuid v1.3.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/googleapis/gax-go/v2 v2.0.4/go.mod h1:0Wqv26UfaUD9n4G6kQubkQ+KchISgw+vpHVxEJEs9eg=\ngithub.com/googleapis/gax-go/v2 v2.0.5/go.mod h1:DWXyrwAJ9X0FpwwEdw+IPEYBICEFu5mhpdKc/us6bOk=\ngithub.com/googleapis/gnostic v0.0.0-20170729233727-0c5108395e2d/go.mod h1:sJBsCZ4ayReDTBIg8b9dl28c5xFWyhBTVRp3pOg5EKY=\ngithub.com/gophercloud/gophercloud v0.1.0/go.mod h1:vxM41WHh5uqHVBMZHzuwNOHh8XEoIEcSTewFxm1c5g8=\ngithub.com/gopherjs/gopherjs v0.0.0-20181017120253-0766667cb4d1/go.mod h1:wJfORRmW1u3UXTncJ5qlYoELFm8eSnnEO6hX4iZ3EWY=\ngithub.com/gopherjs/gopherjs v0.0.0-20200217142428-fce0ec30dd00 h1:l5lAOZEym3oK3SQ2HBHWsJUfbNBiTXJDeW2QDxw9AQ0=\ngithub.com/gopherjs/gopherjs v0.0.0-20200217142428-fce0ec30dd00/go.mod h1:wJfORRmW1u3UXTncJ5qlYoELFm8eSnnEO6hX4iZ3EWY=\ngithub.com/gorilla/context v1.1.1/go.mod h1:kBGZzfjB9CEq2AlWe17Uuf7NDRt0dE0s8S51q0aT7Yg=\ngithub.com/gorilla/css v1.0.0 h1:BQqNyPTi50JCFMTw/b67hByjMVXZRwGha6wxVGkeihY=\ngithub.com/gorilla/css v1.0.0/go.mod h1:Dn721qIggHpt4+EFCcTLTU/vk5ySda2ReITrtgBl60c=\ngithub.com/gorilla/handlers v1.3.0 h1:tsg9qP3mjt1h4Roxp+M1paRjrVBfPSOpBuVclh6YluI=\ngithub.com/gorilla/handlers v1.3.0/go.mod h1:Qkdc/uu4tH4g6mTK6auzZ766c4CA0Ng8+o/OAirnOIQ=\ngithub.com/gorilla/mux v1.5.0/go.mod h1:1lud6UwP+6orDFRuTfBEV8e9/aOM/c4fVVCaMa2zaAs=\ngithub.com/gorilla/mux v1.6.2/go.mod h1:1lud6UwP+6orDFRuTfBEV8e9/aOM/c4fVVCaMa2zaAs=\ngithub.com/gorilla/mux v1.7.3/go.mod h1:1lud6UwP+6orDFRuTfBEV8e9/aOM/c4fVVCaMa2zaAs=\ngithub.com/gorilla/mux v1.8.0 h1:i40aqfkR1h2SlN9hojwV5ZA91wcXFOvkdNIeFDP5koI=\ngithub.com/gorilla/mux v1.8.0/go.mod h1:DVbg23sWSpFRCP0SfiEN6jmj59UnW/n46BH5rLB71So=\ngithub.com/gorilla/securecookie v1.1.1 h1:miw7JPhV+b/lAHSXz4qd/nN9jRiAFV5FwjeKyCS8BvQ=\ngithub.com/gorilla/securecookie v1.1.1/go.mod h1:ra0sb63/xPlUeL+yeDciTfxMRAA+MP+HVt/4epWDjd4=\ngithub.com/gorilla/websocket v0.0.0-20170926233335-4201258b820c/go.mod h1:E7qHFY5m1UJ88s3WnNqhKjPHQ0heANvMoAMk2YaljkQ=\ngithub.com/gorilla/websocket v1.4.0/go.mod h1:E7qHFY5m1UJ88s3WnNqhKjPHQ0heANvMoAMk2YaljkQ=\ngithub.com/gorilla/websocket v1.4.1/go.mod h1:YR8l580nyteQvAITg2hZ9XVh4b55+EU/adAjf1fMHhE=\ngithub.com/gorilla/websocket v1.4.2/go.mod h1:YR8l580nyteQvAITg2hZ9XVh4b55+EU/adAjf1fMHhE=\ngithub.com/gorilla/websocket v1.5.0 h1:PPwGk2jz7EePpoHN/+ClbZu8SPxiqlu12wZP/3sWmnc=\ngithub.com/gorilla/websocket v1.5.0/go.mod h1:YR8l580nyteQvAITg2hZ9XVh4b55+EU/adAjf1fMHhE=\ngithub.com/graph-gophers/graphql-go v1.3.0/go.mod h1:9CQHMSxwO4MprSdzoIEobiHpoLtHm77vfxsvsIN5Vuc=\ngithub.com/gregjones/httpcache v0.0.0-20180305231024-9cad4c3443a7/go.mod h1:FecbI9+v66THATjSRHfNgh1IVFe/9kFxbXtjV0ctIMA=\ngithub.com/grpc-ecosystem/go-grpc-middleware v1.0.0/go.mod h1:FiyG127CGDf3tlThmgyCl78X/SZQqEOJBCDaAfeWzPs=\ngithub.com/grpc-ecosystem/go-grpc-middleware v1.0.1-0.20190118093823-f849b5445de4/go.mod h1:FiyG127CGDf3tlThmgyCl78X/SZQqEOJBCDaAfeWzPs=\ngithub.com/grpc-ecosystem/go-grpc-middleware v1.2.2/go.mod h1:EaizFBKfUKtMIF5iaDEhniwNedqGo9FuLFzppDr3uwI=\ngithub.com/grpc-ecosystem/go-grpc-middleware v1.3.0 h1:+9834+KizmvFV7pXQGSXQTsaWhq2GjuNUt0aUU0YBYw=\ngithub.com/grpc-ecosystem/go-grpc-middleware v1.3.0/go.mod h1:z0ButlSOZa5vEBq9m2m2hlwIgKw+rp3sdCBRoJY+30Y=\ngithub.com/grpc-ecosystem/go-grpc-prometheus v1.2.0 h1:Ovs26xHkKqVztRpIrF/92BcuyuQ/YW4NSIpoGtfXNho=\ngithub.com/grpc-ecosystem/go-grpc-prometheus v1.2.0/go.mod h1:8NvIoxWQoOIhqOTXgfV/d3M/q6VIi02HzZEHgUlZvzk=\ngithub.com/grpc-ecosystem/grpc-gateway v1.9.0/go.mod h1:vNeuVxBJEsws4ogUvrchl83t/GYV9WGTSLVdBhOQFDY=\ngithub.com/grpc-ecosystem/grpc-gateway v1.9.5/go.mod h1:vNeuVxBJEsws4ogUvrchl83t/GYV9WGTSLVdBhOQFDY=\ngithub.com/grpc-ecosystem/grpc-gateway v1.16.0 h1:gmcG1KaJ57LophUzW0Hy8NmPhnMZb4M0+kPpLofRdBo=\ngithub.com/grpc-ecosystem/grpc-gateway v1.16.0/go.mod h1:BDjrQk3hbvj6Nolgz8mAMFbcEtjT1g+wF4CSlocrBnw=\ngithub.com/hailocab/go-hostpool v0.0.0-20160125115350-e80d13ce29ed/go.mod h1:tMWxXQ9wFIaZeTI9F+hmhFiGpFmhOHzyShyFUhRm0H4=\ngithub.com/hamba/avro v1.5.6/go.mod h1:3vNT0RLXXpFm2Tb/5KC71ZRJlOroggq1Rcitb6k4Fr8=\ngithub.com/hashicorp/consul/api v1.0.0/go.mod h1:mbFwfRxOTDHZpT3iUsMAFcLNoVm6Xbe1xZ6KiSm8FY0=\ngithub.com/hashicorp/consul/api v1.1.0/go.mod h1:VmuI/Lkw1nC05EYQWNKwWGbkg+FbDBtguAZLlVdkD9Q=\ngithub.com/hashicorp/consul/api v1.3.0/go.mod h1:MmDNSzIMUjNpY/mQ398R4bk2FnqQLoPndWW5VkKPlCE=\ngithub.com/hashicorp/consul/internal v0.1.0/go.mod h1:zi9bMZYbiPHyAjgBWo7kCUcy5l2NrTdrkVupCc7Oo6c=\ngithub.com/hashicorp/consul/sdk v0.1.1/go.mod h1:VKf9jXwCTEY1QZP2MOLRhb5i/I/ssyNV1vwHyQBF0x8=\ngithub.com/hashicorp/consul/sdk v0.3.0/go.mod h1:VKf9jXwCTEY1QZP2MOLRhb5i/I/ssyNV1vwHyQBF0x8=\ngithub.com/hashicorp/errwrap v1.0.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=\ngithub.com/hashicorp/errwrap v1.1.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=\ngithub.com/hashicorp/go-cleanhttp v0.5.0/go.mod h1:JpRdi6/HCYpAwUzNwuwqhbovhLtngrth3wmdIIUrZ80=\ngithub.com/hashicorp/go-cleanhttp v0.5.1/go.mod h1:JpRdi6/HCYpAwUzNwuwqhbovhLtngrth3wmdIIUrZ80=\ngithub.com/hashicorp/go-cleanhttp v0.5.2 h1:035FKYIWjmULyFRBKPs8TBQoi0x6d9G4xc9neXJWAZQ=\ngithub.com/hashicorp/go-cleanhttp v0.5.2/go.mod h1:kO/YDlP8L1346E6Sodw+PrpBSV4/SoxCXGY6BqNFT48=\ngithub.com/hashicorp/go-hclog v0.0.0-20180709165350-ff2cf002a8dd/go.mod h1:9bjs9uLqI8l75knNv3lV1kA55veR+WUPSiKIWcQHudI=\ngithub.com/hashicorp/go-hclog v0.9.2/go.mod h1:5CU+agLiy3J7N7QjHK5d05KxGsuXiQLrjA0H7acj2lQ=\ngithub.com/hashicorp/go-hclog v0.12.0/go.mod h1:whpDNt7SSdeAju8AWKIWsul05p54N/39EeqMAyrmvFQ=\ngithub.com/hashicorp/go-hclog v0.16.2 h1:K4ev2ib4LdQETX5cSZBG0DVLk1jwGqSPXBjdah3veNs=\ngithub.com/hashicorp/go-hclog v0.16.2/go.mod h1:whpDNt7SSdeAju8AWKIWsul05p54N/39EeqMAyrmvFQ=\ngithub.com/hashicorp/go-immutable-radix v1.0.0/go.mod h1:0y9vanUI8NX6FsYoO3zeMjhV/C5i9g4Q3DwcSNZ4P60=\ngithub.com/hashicorp/go-immutable-radix v1.3.1/go.mod h1:0y9vanUI8NX6FsYoO3zeMjhV/C5i9g4Q3DwcSNZ4P60=\ngithub.com/hashicorp/go-kms-wrapping/entropy v0.1.0/go.mod h1:d1g9WGtAunDNpek8jUIEJnBlbgKS1N2Q61QkHiZyR1g=\ngithub.com/hashicorp/go-msgpack v0.5.3/go.mod h1:ahLV/dePpqEmjfWmKiqvPkv/twdG7iPBM1vqhUKIvfM=\ngithub.com/hashicorp/go-multierror v1.0.0/go.mod h1:dHtQlpGsu+cZNNAkkCN/P3hoUDHhCYQXV3UM06sGGrk=\ngithub.com/hashicorp/go-multierror v1.1.0/go.mod h1:spPvp8C1qA32ftKqdAHm4hHTbPw+vmowP0z+KUhOZdA=\ngithub.com/hashicorp/go-multierror v1.1.1/go.mod h1:iw975J/qwKPdAO1clOe2L8331t/9/fmwbPZ6JB6eMoM=\ngithub.com/hashicorp/go-plugin v1.0.1/go.mod h1:++UyYGoz3o5w9ZzAdZxtQKrWWP+iqPBn3cQptSMzBuY=\ngithub.com/hashicorp/go-retryablehttp v0.5.3/go.mod h1:9B5zBasrRhHXnJnui7y6sL7es7NDiJgTc6Er0maI1Xs=\ngithub.com/hashicorp/go-retryablehttp v0.6.6/go.mod h1:vAew36LZh98gCBJNLH42IQ1ER/9wtLZZ8meHqQvEYWY=\ngithub.com/hashicorp/go-retryablehttp v0.7.1 h1:sUiuQAnLlbvmExtFQs72iFW/HXeUn8Z1aJLQ4LJJbTQ=\ngithub.com/hashicorp/go-retryablehttp v0.7.1/go.mod h1:vAew36LZh98gCBJNLH42IQ1ER/9wtLZZ8meHqQvEYWY=\ngithub.com/hashicorp/go-rootcerts v1.0.0/go.mod h1:K6zTfqpRlCUIjkwsN4Z+hiSfzSTQa6eBIzfwKfwNnHU=\ngithub.com/hashicorp/go-rootcerts v1.0.2/go.mod h1:pqUvnprVnM5bf7AOirdbb01K4ccR319Vf4pU3K5EGc8=\ngithub.com/hashicorp/go-sockaddr v1.0.0/go.mod h1:7Xibr9yA9JjQq1JpNB2Vw7kxv8xerXegt+ozgdvDeDU=\ngithub.com/hashicorp/go-sockaddr v1.0.2/go.mod h1:rB4wwRAUzs07qva3c5SdrY/NEtAUjGlgmH/UkBUC97A=\ngithub.com/hashicorp/go-syslog v1.0.0/go.mod h1:qPfqrKkXGihmCqbJM2mZgkZGvKG1dFdvsLplgctolz4=\ngithub.com/hashicorp/go-uuid v1.0.0/go.mod h1:6SBZvOh/SIDV7/2o3Jml5SYk/TvGqwFJ/bN7x4byOro=\ngithub.com/hashicorp/go-uuid v1.0.1/go.mod h1:6SBZvOh/SIDV7/2o3Jml5SYk/TvGqwFJ/bN7x4byOro=\ngithub.com/hashicorp/go-uuid v1.0.2/go.mod h1:6SBZvOh/SIDV7/2o3Jml5SYk/TvGqwFJ/bN7x4byOro=\ngithub.com/hashicorp/go-version v1.1.0/go.mod h1:fltr4n8CU8Ke44wwGCBoEymUuxUHl09ZGVZPK5anwXA=\ngithub.com/hashicorp/go-version v1.2.0/go.mod h1:fltr4n8CU8Ke44wwGCBoEymUuxUHl09ZGVZPK5anwXA=\ngithub.com/hashicorp/go.net v0.0.1/go.mod h1:hjKkEWcCURg++eb33jQU7oqQcI9XDCnUzHA0oac0k90=\ngithub.com/hashicorp/golang-lru v0.5.0/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\ngithub.com/hashicorp/golang-lru v0.5.1/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\ngithub.com/hashicorp/golang-lru v0.5.4/go.mod h1:iADmTwqILo4mZ8BN3D2Q6+9jd8WM5uGBxy+E8yxSoD4=\ngithub.com/hashicorp/hcl v1.0.0 h1:0Anlzjpi4vEasTeNFn2mLJgTSwt0+6sfsiTG8qcWGx4=\ngithub.com/hashicorp/hcl v1.0.0/go.mod h1:E5yfLk+7swimpb2L/Alb/PJmXilQ/rhwaUYs4T20WEQ=\ngithub.com/hashicorp/logutils v1.0.0/go.mod h1:QIAnNjmIWmVIIkWDTG1z5v++HQmx9WQRO+LraFDTW64=\ngithub.com/hashicorp/mdns v1.0.0/go.mod h1:tL+uN++7HEJ6SQLQ2/p+z2pH24WQKWjBPkE0mNTz8vQ=\ngithub.com/hashicorp/memberlist v0.1.3/go.mod h1:ajVTdAv/9Im8oMAAj5G31PhhMCZJV2pPBoIllUwCN7I=\ngithub.com/hashicorp/memberlist v0.1.6/go.mod h1:5VDNHjqFMgEcclnwmkCnC99IPwxBmIsxwY8qn+Nl0H4=\ngithub.com/hashicorp/serf v0.8.2/go.mod h1:6hOLApaqBFA1NXqRQAsxw9QxuDEvNxSQRwA/JwenrHc=\ngithub.com/hashicorp/serf v0.8.6/go.mod h1:P/AVgr4UHsUYqVHG1y9eFhz8S35pqhGhLZaDpfGKIMo=\ngithub.com/hashicorp/vault/api v1.1.0/go.mod h1:R3Umvhlxi2TN7Ex2hzOowyeNb+SfbVWI973N+ctaFMk=\ngithub.com/hashicorp/vault/sdk v0.1.14-0.20200519221838-e0cfd64bc267/go.mod h1:WX57W2PwkrOPQ6rVQk+dy5/htHIaB4aBM70EwKThu10=\ngithub.com/hashicorp/yamux v0.0.0-20180604194846-3520598351bb/go.mod h1:+NfK9FKeTrX5uv1uIXGdwYDTeHna2qgaIlx54MXqjAM=\ngithub.com/heetch/avro v0.3.1/go.mod h1:4xn38Oz/+hiEUTpbVfGVLfvOg0yKLlRP7Q9+gJJILgA=\ngithub.com/hpcloud/tail v1.0.0/go.mod h1:ab1qPbhIpdTxEkNHXyeSf5vhxWSCs/tWer42PpOxQnU=\ngithub.com/hudl/fargo v1.3.0/go.mod h1:y3CKSmjA+wD2gak7sUSXTAoopbhU08POFhmITJgmKTg=\ngithub.com/iancoleman/orderedmap v0.0.0-20190318233801-ac98e3ecb4b0/go.mod h1:N0Wam8K1arqPXNWjMo21EXnBPOPp36vB07FNRdD2geA=\ngithub.com/ianlancetaylor/demangle v0.0.0-20181102032728-5e5cf60278f6/go.mod h1:aSSvb/t6k1mPoxDqO4vJh6VOCGPwU4O0C2/Eqndh1Sc=\ngithub.com/ianlancetaylor/demangle v0.0.0-20200824232613-28f6c0f3b639/go.mod h1:aSSvb/t6k1mPoxDqO4vJh6VOCGPwU4O0C2/Eqndh1Sc=\ngithub.com/ianlancetaylor/demangle v0.0.0-20210905161508-09a460cdf81d/go.mod h1:aYm2/VgdVmcIU8iMfdMvDMsRAQjcfZSKFby6HOFvi/w=\ngithub.com/imdario/mergo v0.3.5/go.mod h1:2EnlNZ0deacrJVfApfmtdGgDfMuh/nq6Ok1EcJh5FfA=\ngithub.com/improbable-eng/grpc-web v0.15.0 h1:BN+7z6uNXZ1tQGcNAuaU1YjsLTApzkjt2tzCixLaUPQ=\ngithub.com/improbable-eng/grpc-web v0.15.0/go.mod h1:1sy9HKV4Jt9aEs9JSnkWlRJPuPtwNr0l57L4f878wP8=\ngithub.com/inconshreveable/mousetrap v1.0.0/go.mod h1:PxqpIevigyE2G7u3NXJIT2ANytuPF1OarO4DADm73n8=\ngithub.com/inconshreveable/mousetrap v1.0.1 h1:U3uMjPSQEBMNp1lFxmllqCPM6P5u/Xq7Pgzkat/bFNc=\ngithub.com/inconshreveable/mousetrap v1.0.1/go.mod h1:vpF70FUmC8bwa3OWnCshd2FqLfsEA9PFc4w1p2J65bw=\ngithub.com/influxdata/influxdb1-client v0.0.0-20191209144304-8bf82d3c094d/go.mod h1:qj24IKcXYK6Iy9ceXlo3Tc+vtHo9lIhSX5JddghvEPo=\ngithub.com/invopop/jsonschema v0.4.0/go.mod h1:O9uiLokuu0+MGFlyiaqtWxwqJm41/+8Nj0lD7A36YH0=\ngithub.com/jackc/chunkreader v1.0.0/go.mod h1:RT6O25fNZIuasFJRyZ4R/Y2BbhasbmZXF9QQ7T3kePo=\ngithub.com/jackc/chunkreader/v2 v2.0.0/go.mod h1:odVSm741yZoC3dpHEUXIqA9tQRhFrgOHwnPIn9lDKlk=\ngithub.com/jackc/chunkreader/v2 v2.0.1 h1:i+RDz65UE+mmpjTfyz0MoVTnzeYxroil2G82ki7MGG8=\ngithub.com/jackc/chunkreader/v2 v2.0.1/go.mod h1:odVSm741yZoC3dpHEUXIqA9tQRhFrgOHwnPIn9lDKlk=\ngithub.com/jackc/pgconn v0.0.0-20190420214824-7e0022ef6ba3/go.mod h1:jkELnwuX+w9qN5YIfX0fl88Ehu4XC3keFuOJJk9pcnA=\ngithub.com/jackc/pgconn v0.0.0-20190824142844-760dd75542eb/go.mod h1:lLjNuW/+OfW9/pnVKPazfWOgNfH2aPem8YQ7ilXGvJE=\ngithub.com/jackc/pgconn v0.0.0-20190831204454-2fabfa3c18b7/go.mod h1:ZJKsE/KZfsUgOEh9hBm+xYTstcNHg7UPMVJqRfQxq4s=\ngithub.com/jackc/pgconn v1.4.0/go.mod h1:Y2O3ZDF0q4mMacyWV3AstPJpeHXWGEetiFttmq5lahk=\ngithub.com/jackc/pgconn v1.5.0/go.mod h1:QeD3lBfpTFe8WUnPZWN5KY/mB8FGMIYRdd8P8Jr0fAI=\ngithub.com/jackc/pgconn v1.5.1-0.20200601181101-fa742c524853/go.mod h1:QeD3lBfpTFe8WUnPZWN5KY/mB8FGMIYRdd8P8Jr0fAI=\ngithub.com/jackc/pgconn v1.6.4/go.mod h1:w2pne1C2tZgP+TvjqLpOigGzNqjBgQW9dUw/4Chex78=\ngithub.com/jackc/pgconn v1.8.0/go.mod h1:1C2Pb36bGIP9QHGBYCjnyhqu7Rv3sGshaQUvmfGIB/o=\ngithub.com/jackc/pgconn v1.9.0/go.mod h1:YctiPyvzfU11JFxoXokUOOKQXQmDMoJL9vJzHH8/2JY=\ngithub.com/jackc/pgconn v1.9.1-0.20210724152538-d89c8390a530/go.mod h1:4z2w8XhRbP1hYxkpTuBjTS3ne3J48K83+u0zoyvg2pI=\ngithub.com/jackc/pgconn v1.10.1/go.mod h1:4z2w8XhRbP1hYxkpTuBjTS3ne3J48K83+u0zoyvg2pI=\ngithub.com/jackc/pgconn v1.13.0 h1:3L1XMNV2Zvca/8BYhzcRFS70Lr0WlDg16Di6SFGAbys=\ngithub.com/jackc/pgconn v1.13.0/go.mod h1:AnowpAqO4CMIIJNZl2VJp+KrkAZciAkhEl0W0JIobpI=\ngithub.com/jackc/pgio v1.0.0 h1:g12B9UwVnzGhueNavwioyEEpAmqMe1E/BN9ES+8ovkE=\ngithub.com/jackc/pgio v1.0.0/go.mod h1:oP+2QK2wFfUWgr+gxjoBH9KGBb31Eio69xUb0w5bYf8=\ngithub.com/jackc/pgmock v0.0.0-20190831213851-13a1b77aafa2/go.mod h1:fGZlG77KXmcq05nJLRkk0+p82V8B8Dw8KN2/V9c/OAE=\ngithub.com/jackc/pgmock v0.0.0-20201204152224-4fe30f7445fd/go.mod h1:hrBW0Enj2AZTNpt/7Y5rr2xe/9Mn757Wtb2xeBzPv2c=\ngithub.com/jackc/pgmock v0.0.0-20210724152146-4ad1a8207f65 h1:DadwsjnMwFjfWc9y5Wi/+Zz7xoE5ALHsRQlOctkOiHc=\ngithub.com/jackc/pgmock v0.0.0-20210724152146-4ad1a8207f65/go.mod h1:5R2h2EEX+qri8jOWMbJCtaPWkrrNc7OHwsp2TCqp7ak=\ngithub.com/jackc/pgpassfile v1.0.0 h1:/6Hmqy13Ss2zCq62VdNG8tM1wchn8zjSGOBJ6icpsIM=\ngithub.com/jackc/pgpassfile v1.0.0/go.mod h1:CEx0iS5ambNFdcRtxPj5JhEz+xB6uRky5eyVu/W2HEg=\ngithub.com/jackc/pgproto3 v1.1.0/go.mod h1:eR5FA3leWg7p9aeAqi37XOTgTIbkABlvcPB3E5rlc78=\ngithub.com/jackc/pgproto3/v2 v2.0.0-alpha1.0.20190420180111-c116219b62db/go.mod h1:bhq50y+xrl9n5mRYyCBFKkpRVTLYJVWeCc+mEAI3yXA=\ngithub.com/jackc/pgproto3/v2 v2.0.0-alpha1.0.20190609003834-432c2951c711/go.mod h1:uH0AWtUmuShn0bcesswc4aBTWGvw0cAxIJp+6OB//Wg=\ngithub.com/jackc/pgproto3/v2 v2.0.0-rc3/go.mod h1:ryONWYqW6dqSg1Lw6vXNMXoBJhpzvWKnT95C46ckYeM=\ngithub.com/jackc/pgproto3/v2 v2.0.0-rc3.0.20190831210041-4c03ce451f29/go.mod h1:ryONWYqW6dqSg1Lw6vXNMXoBJhpzvWKnT95C46ckYeM=\ngithub.com/jackc/pgproto3/v2 v2.0.1/go.mod h1:WfJCnwN3HIg9Ish/j3sgWXnAfK8A9Y0bwXYU5xKaEdA=\ngithub.com/jackc/pgproto3/v2 v2.0.2/go.mod h1:WfJCnwN3HIg9Ish/j3sgWXnAfK8A9Y0bwXYU5xKaEdA=\ngithub.com/jackc/pgproto3/v2 v2.0.6/go.mod h1:WfJCnwN3HIg9Ish/j3sgWXnAfK8A9Y0bwXYU5xKaEdA=\ngithub.com/jackc/pgproto3/v2 v2.1.1/go.mod h1:WfJCnwN3HIg9Ish/j3sgWXnAfK8A9Y0bwXYU5xKaEdA=\ngithub.com/jackc/pgproto3/v2 v2.2.0/go.mod h1:WfJCnwN3HIg9Ish/j3sgWXnAfK8A9Y0bwXYU5xKaEdA=\ngithub.com/jackc/pgproto3/v2 v2.3.1 h1:nwj7qwf0S+Q7ISFfBndqeLwSwxs+4DPsbRFjECT1Y4Y=\ngithub.com/jackc/pgproto3/v2 v2.3.1/go.mod h1:WfJCnwN3HIg9Ish/j3sgWXnAfK8A9Y0bwXYU5xKaEdA=\ngithub.com/jackc/pgservicefile v0.0.0-20200307190119-3430c5407db8/go.mod h1:vsD4gTJCa9TptPL8sPkXrLZ+hDuNrZCnj29CQpr4X1E=\ngithub.com/jackc/pgservicefile v0.0.0-20200714003250-2b9c44734f2b h1:C8S2+VttkHFdOOCXJe+YGfa4vHYwlt4Zx+IVXQ97jYg=\ngithub.com/jackc/pgservicefile v0.0.0-20200714003250-2b9c44734f2b/go.mod h1:vsD4gTJCa9TptPL8sPkXrLZ+hDuNrZCnj29CQpr4X1E=\ngithub.com/jackc/pgtype v0.0.0-20190421001408-4ed0de4755e0/go.mod h1:hdSHsc1V01CGwFsrv11mJRHWJ6aifDLfdV3aVjFF0zg=\ngithub.com/jackc/pgtype v0.0.0-20190824184912-ab885b375b90/go.mod h1:KcahbBH1nCMSo2DXpzsoWOAfFkdEtEJpPbVLq8eE+mc=\ngithub.com/jackc/pgtype v0.0.0-20190828014616-a8802b16cc59/go.mod h1:MWlu30kVJrUS8lot6TQqcg7mtthZ9T0EoIBFiJcmcyw=\ngithub.com/jackc/pgtype v1.2.0/go.mod h1:5m2OfMh1wTK7x+Fk952IDmI4nw3nPrvtQdM0ZT4WpC0=\ngithub.com/jackc/pgtype v1.3.1-0.20200510190516-8cd94a14c75a/go.mod h1:vaogEUkALtxZMCH411K+tKzNpwzCKU+AnPzBKZ+I+Po=\ngithub.com/jackc/pgtype v1.3.1-0.20200606141011-f6355165a91c/go.mod h1:cvk9Bgu/VzJ9/lxTO5R5sf80p0DiucVtN7ZxvaC4GmQ=\ngithub.com/jackc/pgtype v1.4.2/go.mod h1:JCULISAZBFGrHaOXIIFiyfzW5VY0GRitRr8NeJsrdig=\ngithub.com/jackc/pgtype v1.8.1-0.20210724151600-32e20a603178/go.mod h1:C516IlIV9NKqfsMCXTdChteoXmwgUceqaLfjg2e3NlM=\ngithub.com/jackc/pgtype v1.9.0/go.mod h1:LUMuVrfsFfdKGLw+AFFVv6KtHOFMwRgDDzBt76IqCA4=\ngithub.com/jackc/pgtype v1.12.0 h1:Dlq8Qvcch7kiehm8wPGIW0W3KsCCHJnRacKW0UM8n5w=\ngithub.com/jackc/pgtype v1.12.0/go.mod h1:LUMuVrfsFfdKGLw+AFFVv6KtHOFMwRgDDzBt76IqCA4=\ngithub.com/jackc/pgx/v4 v4.0.0-20190420224344-cc3461e65d96/go.mod h1:mdxmSJJuR08CZQyj1PVQBHy9XOp5p8/SHH6a0psbY9Y=\ngithub.com/jackc/pgx/v4 v4.0.0-20190421002000-1b8f0016e912/go.mod h1:no/Y67Jkk/9WuGR0JG/JseM9irFbnEPbuWV2EELPNuM=\ngithub.com/jackc/pgx/v4 v4.0.0-pre1.0.20190824185557-6972a5742186/go.mod h1:X+GQnOEnf1dqHGpw7JmHqHc1NxDoalibchSk9/RWuDc=\ngithub.com/jackc/pgx/v4 v4.5.0/go.mod h1:EpAKPLdnTorwmPUUsqrPxy5fphV18j9q3wrfRXgo+kA=\ngithub.com/jackc/pgx/v4 v4.6.1-0.20200510190926-94ba730bb1e9/go.mod h1:t3/cdRQl6fOLDxqtlyhe9UWgfIi9R8+8v8GKV5TRA/o=\ngithub.com/jackc/pgx/v4 v4.6.1-0.20200606145419-4e5062306904/go.mod h1:ZDaNWkt9sW1JMiNn0kdYBaLelIhw7Pg4qd+Vk6tw7Hg=\ngithub.com/jackc/pgx/v4 v4.8.1/go.mod h1:4HOLxrl8wToZJReD04/yB20GDwf4KBYETvlHciCnwW0=\ngithub.com/jackc/pgx/v4 v4.12.1-0.20210724153913-640aa07df17c/go.mod h1:1QD0+tgSXP7iUjYm9C1NxKhny7lq6ee99u/z+IHFcgs=\ngithub.com/jackc/pgx/v4 v4.14.0/go.mod h1:jT3ibf/A0ZVCp89rtCIN0zCJxcE74ypROmHEZYsG/j8=\ngithub.com/jackc/pgx/v4 v4.17.2 h1:0Ut0rpeKwvIVbMQ1KbMBU4h6wxehBI535LK6Flheh8E=\ngithub.com/jackc/pgx/v4 v4.17.2/go.mod h1:lcxIZN44yMIrWI78a5CpucdD14hX0SBDbNRvjDBItsw=\ngithub.com/jackc/puddle v0.0.0-20190413234325-e4ced69a3a2b/go.mod h1:m4B5Dj62Y0fbyuIc15OsIqK0+JU8nkqQjsgx7dvjSWk=\ngithub.com/jackc/puddle v0.0.0-20190608224051-11cab39313c9/go.mod h1:m4B5Dj62Y0fbyuIc15OsIqK0+JU8nkqQjsgx7dvjSWk=\ngithub.com/jackc/puddle v1.1.0/go.mod h1:m4B5Dj62Y0fbyuIc15OsIqK0+JU8nkqQjsgx7dvjSWk=\ngithub.com/jackc/puddle v1.1.1/go.mod h1:m4B5Dj62Y0fbyuIc15OsIqK0+JU8nkqQjsgx7dvjSWk=\ngithub.com/jackc/puddle v1.1.3/go.mod h1:m4B5Dj62Y0fbyuIc15OsIqK0+JU8nkqQjsgx7dvjSWk=\ngithub.com/jackc/puddle v1.2.0/go.mod h1:m4B5Dj62Y0fbyuIc15OsIqK0+JU8nkqQjsgx7dvjSWk=\ngithub.com/jackc/puddle v1.3.0/go.mod h1:m4B5Dj62Y0fbyuIc15OsIqK0+JU8nkqQjsgx7dvjSWk=\ngithub.com/jaffee/commandeer v0.6.0 h1:YI44XLWcJN21euhh32sZW8vM/tljPYxhsXIfEPkQKcs=\ngithub.com/jaffee/commandeer v0.6.0/go.mod h1:kCwfuSvZ2T0NVEr3LDSo6fDUgi0xSBnAVDdkOKTtpLQ=\ngithub.com/jedib0t/go-pretty v4.3.0+incompatible h1:CGs8AVhEKg/n9YbUenWmNStRW2PHJzaeDodcfvRAbIo=\ngithub.com/jedib0t/go-pretty v4.3.0+incompatible/go.mod h1:XemHduiw8R651AF9Pt4FwCTKeG3oo7hrHJAoznj9nag=\ngithub.com/jhump/gopoet v0.0.0-20190322174617-17282ff210b3/go.mod h1:me9yfT6IJSlOL3FCfrg+L6yzUEZ+5jW6WHt4Sk+UPUI=\ngithub.com/jhump/gopoet v0.1.0/go.mod h1:me9yfT6IJSlOL3FCfrg+L6yzUEZ+5jW6WHt4Sk+UPUI=\ngithub.com/jhump/goprotoc v0.5.0/go.mod h1:VrbvcYrQOrTi3i0Vf+m+oqQWk9l72mjkJCYo7UvLHRQ=\ngithub.com/jhump/protoreflect v1.11.0/go.mod h1:U7aMIjN0NWq9swDP7xDdoMfRHb35uiuTd3Z9nFXJf5E=\ngithub.com/jhump/protoreflect v1.12.0/go.mod h1:JytZfP5d0r8pVNLZvai7U/MCuTWITgrI4tTg7puQFKI=\ngithub.com/jinzhu/gorm v1.9.1/go.mod h1:Vla75njaFJ8clLU1W44h34PjIkijhjHIYnZxMqCdxqo=\ngithub.com/jinzhu/inflection v1.0.0/go.mod h1:h+uFLlag+Qp1Va5pdKtLDYj+kHp5pxUVkryuEj+Srlc=\ngithub.com/jinzhu/now v1.1.1/go.mod h1:d3SSVoowX0Lcu0IBviAWJpolVfI5UJVZZ7cO71lE/z8=\ngithub.com/jinzhu/now v1.1.3/go.mod h1:d3SSVoowX0Lcu0IBviAWJpolVfI5UJVZZ7cO71lE/z8=\ngithub.com/jmespath/go-jmespath v0.0.0-20180206201540-c2b33e8439af/go.mod h1:Nht3zPeWKUH0NzdCt2Blrr5ys8VGpn0CEB0cQHVjt7k=\ngithub.com/jmespath/go-jmespath v0.4.0 h1:BEgLn5cpjn8UN1mAw4NjwDrS35OdebyEtFe+9YPoQUg=\ngithub.com/jmespath/go-jmespath v0.4.0/go.mod h1:T8mJZnbsbmF+m6zOOFylbeCJqk5+pHWvzYPziyZiYoo=\ngithub.com/jmespath/go-jmespath/internal/testify v1.5.1 h1:shLQSRRSCCPj3f2gpwzGwWFoC7ycTf1rcQZHOlsJ6N8=\ngithub.com/jmespath/go-jmespath/internal/testify v1.5.1/go.mod h1:L3OGu8Wl2/fWfCI6z80xFu9LTZmf1ZRjMHUOPmWr69U=\ngithub.com/jmoiron/sqlx v1.2.0/go.mod h1:1FEQNm3xlJgrMD+FBdI9+xvCksHtbpVBBw5dYhBSsks=\ngithub.com/jmoiron/sqlx v1.3.5 h1:vFFPA71p1o5gAeqtEAwLU4dnX2napprKtHr7PYIcN3g=\ngithub.com/jmoiron/sqlx v1.3.5/go.mod h1:nRVWtLre0KfCLJvgxzCsLVMogSvQ1zNJtpYr2Ccp0mQ=\ngithub.com/joho/godotenv v1.3.0/go.mod h1:7hK45KPybAkOC6peb+G5yklZfMxEjkZhHbwpqxOKXbg=\ngithub.com/joho/godotenv v1.4.0 h1:3l4+N6zfMWnkbPEXKng2o2/MR5mSwTrBih4ZEkkz1lg=\ngithub.com/joho/godotenv v1.4.0/go.mod h1:f4LDr5Voq0i2e/R5DDNOoa2zzDfwtkZa6DnEwAbqwq4=\ngithub.com/jonboulle/clockwork v0.1.0/go.mod h1:Ii8DK3G1RaLaWxj9trq07+26W01tbo22gdxWY5EU2bo=\ngithub.com/jonboulle/clockwork v0.2.2 h1:UOGuzwb1PwsrDAObMuhUnj0p5ULPj8V/xJ7Kx9qUBdQ=\ngithub.com/jonboulle/clockwork v0.2.2/go.mod h1:Pkfl5aHPm1nk2H9h0bjmnJD/BcgbGXUBGnn1kMkgxc8=\ngithub.com/josharian/intern v1.0.0 h1:vlS4z54oSdjm0bgjRigI+G1HpF+tI+9rE5LLzOg8HmY=\ngithub.com/josharian/intern v1.0.0/go.mod h1:5DoeVV0s6jJacbCEi61lwdGj/aVlrQvzHFFd8Hwg//Y=\ngithub.com/jpillora/backoff v1.0.0/go.mod h1:J/6gKK9jxlEcS3zixgDgUAsiuZ7yrSoa/FX5e0EB2j4=\ngithub.com/json-iterator/go v0.0.0-20180612202835-f2b4162afba3/go.mod h1:+SdeFBvtyEkXs7REEP0seUULqWtbJapLOCVDaaPEHmU=\ngithub.com/json-iterator/go v1.1.6/go.mod h1:+SdeFBvtyEkXs7REEP0seUULqWtbJapLOCVDaaPEHmU=\ngithub.com/json-iterator/go v1.1.7/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=\ngithub.com/json-iterator/go v1.1.8/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=\ngithub.com/json-iterator/go v1.1.9/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=\ngithub.com/json-iterator/go v1.1.10/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=\ngithub.com/json-iterator/go v1.1.11/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=\ngithub.com/json-iterator/go v1.1.12 h1:PV8peI4a0ysnczrg+LtxykD8LfKY9ML6u2jnxaEnrnM=\ngithub.com/json-iterator/go v1.1.12/go.mod h1:e30LSqwooZae/UwlEbR2852Gd8hjQvJoHmT4TnhNGBo=\ngithub.com/jstemmer/go-junit-report v0.0.0-20190106144839-af01ea7f8024/go.mod h1:6v2b51hI/fHJwM22ozAgKL4VKDeJcHhJFhtBdhmNjmU=\ngithub.com/jstemmer/go-junit-report v0.9.1/go.mod h1:Brl9GWCQeLvo8nXZwPNNblvFj/XSXhF0NWZEnDohbsk=\ngithub.com/jtolds/gls v4.20.0+incompatible h1:xdiiI2gbIgH/gLH7ADydsJ1uDOEzR8yvV7C0MuV77Wo=\ngithub.com/jtolds/gls v4.20.0+incompatible/go.mod h1:QJZ7F/aHp+rZTRtaJ1ow/lLfFfVYBRgL+9YlvaHOwJU=\ngithub.com/juju/qthttptest v0.1.1/go.mod h1:aTlAv8TYaflIiTDIQYzxnl1QdPjAg8Q8qJMErpKy6A4=\ngithub.com/julienschmidt/httprouter v1.1.0/go.mod h1:SYymIcj16QtmaHHD7aYtjjsJG7VTCxuUUipMqKk8s4w=\ngithub.com/julienschmidt/httprouter v1.2.0/go.mod h1:SYymIcj16QtmaHHD7aYtjjsJG7VTCxuUUipMqKk8s4w=\ngithub.com/julienschmidt/httprouter v1.3.0/go.mod h1:JR6WtHb+2LUe8TCKY3cZOxFyyO8IZAc4RVcycCCAKdM=\ngithub.com/jung-kurt/gofpdf v1.0.3-0.20190309125859-24315acbbda5/go.mod h1:7Id9E/uU8ce6rXgefFLlgrJj/GYY22cpxn+r32jIOes=\ngithub.com/karrick/godirwalk v1.8.0/go.mod h1:H5KPZjojv4lE+QYImBI8xVtrBRgYrIVsaRPx4tDPEn4=\ngithub.com/karrick/godirwalk v1.10.3/go.mod h1:RoGL9dQei4vP9ilrpETWE8CLOZ1kiN0LhBygSwrAsHA=\ngithub.com/kballard/go-shellquote v0.0.0-20180428030007-95032a82bc51 h1:Z9n2FFNUXsshfwJMBgNA0RU6/i7WVaAegv3PtuIHPMs=\ngithub.com/kballard/go-shellquote v0.0.0-20180428030007-95032a82bc51/go.mod h1:CzGEWj7cYgsdH8dAjBGEr58BoE7ScuLd+fwFZ44+/x8=\ngithub.com/kisielk/errcheck v1.1.0/go.mod h1:EZBBE59ingxPouuu3KfxchcWSUPOHkagtvWXihfKN4Q=\ngithub.com/kisielk/errcheck v1.2.0/go.mod h1:/BMXB+zMLi60iA8Vv6Ksmxu/1UDYcXs4uQLJ+jE2L00=\ngithub.com/kisielk/errcheck v1.5.0/go.mod h1:pFxgyoBC7bSaBwPgfKdkLd5X25qrDl4LWUI2bnpBCr8=\ngithub.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO+MFFWcvkIS/tQcRk01m1F5IRFswLeQ+oQHNcck=\ngithub.com/klauspost/asmfmt v1.3.2 h1:4Ri7ox3EwapiOjCki+hw14RyKk201CN4rzyCJRFLpK4=\ngithub.com/klauspost/asmfmt v1.3.2/go.mod h1:AG8TuvYojzulgDAMCnYn50l/5QV3Bs/tp6j0HLHbNSE=\ngithub.com/klauspost/compress v1.9.5/go.mod h1:RyIbtBH6LamlWaDj8nUwkbUhJ87Yi3uG0guNDohfE1A=\ngithub.com/klauspost/compress v1.10.3/go.mod h1:aoV0uJVorq1K+umq18yTdKaF57EivdYsUV+/s2qKfXs=\ngithub.com/klauspost/compress v1.11.7/go.mod h1:aoV0uJVorq1K+umq18yTdKaF57EivdYsUV+/s2qKfXs=\ngithub.com/klauspost/compress v1.12.2/go.mod h1:8dP1Hq4DHOhN9w426knH3Rhby4rFm6D8eO+e+Dq5Gzg=\ngithub.com/klauspost/compress v1.13.4/go.mod h1:8dP1Hq4DHOhN9w426knH3Rhby4rFm6D8eO+e+Dq5Gzg=\ngithub.com/klauspost/compress v1.13.6/go.mod h1:/3/Vjq9QcHkK5uEr5lBEmyoZ1iFhe47etQ6QUkpK6sk=\ngithub.com/klauspost/compress v1.14.2/go.mod h1:/3/Vjq9QcHkK5uEr5lBEmyoZ1iFhe47etQ6QUkpK6sk=\ngithub.com/klauspost/compress v1.15.9 h1:wKRjX6JRtDdrE9qwa4b/Cip7ACOshUI4smpCQanqjSY=\ngithub.com/klauspost/compress v1.15.9/go.mod h1:PhcZ0MbTNciWF3rruxRgKxI5NkcHHrHUDtV4Yw2GlzU=\ngithub.com/klauspost/cpuid/v2 v2.0.12 h1:p9dKCg8i4gmOxtv35DvrYoWqYzQrvEVdjQ762Y0OqZE=\ngithub.com/klauspost/cpuid/v2 v2.0.12/go.mod h1:g2LTdtYhdyuGPqyWyv7qRAmj1WBqxuObKfj5c0PQa7c=\ngithub.com/konsorten/go-windows-terminal-sequences v1.0.1/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\ngithub.com/konsorten/go-windows-terminal-sequences v1.0.2/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\ngithub.com/konsorten/go-windows-terminal-sequences v1.0.3/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\ngithub.com/kr/fs v0.1.0/go.mod h1:FFnZGqtBN9Gxj7eW1uZ42v5BccTP0vu6NEaFoC2HwRg=\ngithub.com/kr/logfmt v0.0.0-20140226030751-b84e30acd515/go.mod h1:+0opPa2QZZtGFBFZlji/RkVcI2GknAs/DXo4wKdlNEc=\ngithub.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=\ngithub.com/kr/pretty v0.2.0/go.mod h1:ipq/a2n7PKx3OHsz4KJII5eveXtPO4qwEXGdVfWzfnI=\ngithub.com/kr/pretty v0.2.1/go.mod h1:ipq/a2n7PKx3OHsz4KJII5eveXtPO4qwEXGdVfWzfnI=\ngithub.com/kr/pretty v0.3.0 h1:WgNl7dwNpEZ6jJ9k1snq4pZsg7DOEN8hP9Xw0Tsjwk0=\ngithub.com/kr/pretty v0.3.0/go.mod h1:640gp4NfQd8pI5XOwp5fnNeVWj67G7CFk/SaSQn7NBk=\ngithub.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=\ngithub.com/kr/pty v1.1.8/go.mod h1:O1sed60cT9XZ5uDucP5qwvh+TE3NnUj51EiZO/lmSfw=\ngithub.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=\ngithub.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=\ngithub.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=\ngithub.com/labstack/echo v3.3.10+incompatible/go.mod h1:0INS7j/VjnFxD4E2wkz67b8cVwCLbBmJyDaka6Cmk1s=\ngithub.com/labstack/echo/v4 v4.2.0/go.mod h1:AA49e0DZ8kk5jTOOCKNuPR6oTnBS0dYiM4FW1e6jwpg=\ngithub.com/labstack/gommon v0.3.0/go.mod h1:MULnywXg0yavhxWKc+lOruYdAhDwPK9wf0OL7NoOu+k=\ngithub.com/labstack/gommon v0.3.1/go.mod h1:uW6kP17uPlLJsD3ijUYn3/M5bAxtlZhMI6m3MFxTMTM=\ngithub.com/leodido/go-urn v1.2.0 h1:hpXL4XnriNwQ/ABnpepYM/1vCLWNDfUNts8dX3xTG6Y=\ngithub.com/leodido/go-urn v1.2.0/go.mod h1:+8+nEpDfqqsY+g338gtMEUOtuK+4dEMhiQEgxpxOKII=\ngithub.com/lib/pq v1.0.0/go.mod h1:5WUZQaWbwv1U+lTReE5YruASi9Al49XbQIvNi/34Woo=\ngithub.com/lib/pq v1.1.0/go.mod h1:5WUZQaWbwv1U+lTReE5YruASi9Al49XbQIvNi/34Woo=\ngithub.com/lib/pq v1.2.0/go.mod h1:5WUZQaWbwv1U+lTReE5YruASi9Al49XbQIvNi/34Woo=\ngithub.com/lib/pq v1.3.0/go.mod h1:5WUZQaWbwv1U+lTReE5YruASi9Al49XbQIvNi/34Woo=\ngithub.com/lib/pq v1.10.2/go.mod h1:AlVN5x4E4T544tWzH6hKfbfQvm3HdbOxrmggDNAPY9o=\ngithub.com/lib/pq v1.10.7 h1:p7ZhMD+KsSRozJr34udlUrhboJwWAgCg34+/ZZNvZZw=\ngithub.com/lib/pq v1.10.7/go.mod h1:AlVN5x4E4T544tWzH6hKfbfQvm3HdbOxrmggDNAPY9o=\ngithub.com/lightstep/lightstep-tracer-common/golang/gogo v0.0.0-20190605223551-bc2310a04743/go.mod h1:qklhhLq1aX+mtWk9cPHPzaBjWImj5ULL6C7HFJtXQMM=\ngithub.com/lightstep/lightstep-tracer-go v0.18.1/go.mod h1:jlF1pusYV4pidLvZ+XD0UBX0ZE6WURAspgAczcDHrL4=\ngithub.com/linkedin/goavro v2.1.0+incompatible/go.mod h1:bBCwI2eGYpUI/4820s67MElg9tdeLbINjLjiM2xZFYM=\ngithub.com/linkedin/goavro/v2 v2.10.0/go.mod h1:UgQUb2N/pmueQYH9bfqFioWxzYCZXSfF8Jw03O5sjqA=\ngithub.com/linkedin/goavro/v2 v2.10.1/go.mod h1:UgQUb2N/pmueQYH9bfqFioWxzYCZXSfF8Jw03O5sjqA=\ngithub.com/linkedin/goavro/v2 v2.11.1 h1:4cuAtbDfqkKnBXp9E+tRkIJGa6W6iAjwonwt8O1f4U0=\ngithub.com/linkedin/goavro/v2 v2.11.1/go.mod h1:UgQUb2N/pmueQYH9bfqFioWxzYCZXSfF8Jw03O5sjqA=\ngithub.com/lufia/plan9stats v0.0.0-20211012122336-39d0f177ccd0 h1:6E+4a0GO5zZEnZ81pIr0yLvtUWk2if982qA3F3QD6H4=\ngithub.com/lufia/plan9stats v0.0.0-20211012122336-39d0f177ccd0/go.mod h1:zJYVVT2jmtg6P3p1VtQj7WsuWi/y4VnjVBn7F8KPB3I=\ngithub.com/luna-duclos/instrumentedsql v1.1.3 h1:t7mvC0z1jUt5A0UQ6I/0H31ryymuQRnJcWCiqV3lSAA=\ngithub.com/luna-duclos/instrumentedsql v1.1.3/go.mod h1:9J1njvFds+zN7y85EDhN9XNQLANWwZt2ULeIC8yMNYs=\ngithub.com/lyft/protoc-gen-validate v0.0.13/go.mod h1:XbGvPuh87YZc5TdIa2/I4pLk0QoUACkjt2znoq26NVQ=\ngithub.com/magiconair/properties v1.8.0/go.mod h1:PppfXfuXeibc/6YijjN8zIbojt8czPbwD3XqdrwzmxQ=\ngithub.com/magiconair/properties v1.8.1/go.mod h1:PppfXfuXeibc/6YijjN8zIbojt8czPbwD3XqdrwzmxQ=\ngithub.com/magiconair/properties v1.8.5 h1:b6kJs+EmPFMYGkow9GiUyCyOvIwYetYJ3fSaWak/Gls=\ngithub.com/magiconair/properties v1.8.5/go.mod h1:y3VJvCyxH9uVvJTWEGAELF3aiYNyPKd5NZ3oSwXrF60=\ngithub.com/mailru/easyjson v0.0.0-20160728113105-d5b7844b561a/go.mod h1:C1wdFJiN94OJF2b5HbByQZoLdCWB1Yqtg26g4irojpc=\ngithub.com/mailru/easyjson v0.0.0-20180730094502-03f2033d19d5/go.mod h1:C1wdFJiN94OJF2b5HbByQZoLdCWB1Yqtg26g4irojpc=\ngithub.com/mailru/easyjson v0.7.7 h1:UGYAvKxe3sBsEDzO8ZeWOSlIQfWFlxbzLZe7hwFURr0=\ngithub.com/mailru/easyjson v0.7.7/go.mod h1:xzfreul335JAWq5oZzymOObrkdz5UnU4kGfJJLY9Nlc=\ngithub.com/markbates/oncer v0.0.0-20181203154359-bf2de49a0be2/go.mod h1:Ld9puTsIW75CHf65OeIOkyKbteujpZVXDpWK6YGZbxE=\ngithub.com/markbates/safe v1.0.1/go.mod h1:nAqgmRi7cY2nqMc92/bSEeQA+R4OheNU2T1kNSCBdG0=\ngithub.com/mattetti/filebuffer v1.0.1 h1:gG7pyfnSIZCxdoKq+cPa8T0hhYtD9NxCdI4D7PTjRLM=\ngithub.com/mattetti/filebuffer v1.0.1/go.mod h1:YdMURNDOttIiruleeVr6f56OrMc+MydEnTcXwtkxNVs=\ngithub.com/mattn/go-colorable v0.0.9/go.mod h1:9vuHe8Xs5qXnSaW/c/ABM9alt+Vo+STaOChaDxuIBZU=\ngithub.com/mattn/go-colorable v0.1.1/go.mod h1:FuOcm+DKB9mbwrcAfNl7/TZVBZ6rcnceauSikq3lYCQ=\ngithub.com/mattn/go-colorable v0.1.2/go.mod h1:U0ppj6V5qS13XJ6of8GYAs25YV2eR4EVcfRqFIhoBtE=\ngithub.com/mattn/go-colorable v0.1.4/go.mod h1:U0ppj6V5qS13XJ6of8GYAs25YV2eR4EVcfRqFIhoBtE=\ngithub.com/mattn/go-colorable v0.1.6/go.mod h1:u6P/XSegPjTcexA+o6vUJrdnUu04hMope9wVRipJSqc=\ngithub.com/mattn/go-colorable v0.1.7/go.mod h1:u6P/XSegPjTcexA+o6vUJrdnUu04hMope9wVRipJSqc=\ngithub.com/mattn/go-colorable v0.1.9/go.mod h1:u6P/XSegPjTcexA+o6vUJrdnUu04hMope9wVRipJSqc=\ngithub.com/mattn/go-colorable v0.1.11 h1:nQ+aFkoE2TMGc0b68U2OKSexC+eq46+XwZzWXHRmPYs=\ngithub.com/mattn/go-colorable v0.1.11/go.mod h1:u5H1YNBxpqRaxsYJYSkiCWKzEfiAb1Gb520KVy5xxl4=\ngithub.com/mattn/go-isatty v0.0.3/go.mod h1:M+lRXTBqGeGNdLjl/ufCoiOlB5xdOkqRJdNxMWT7Zi4=\ngithub.com/mattn/go-isatty v0.0.4/go.mod h1:M+lRXTBqGeGNdLjl/ufCoiOlB5xdOkqRJdNxMWT7Zi4=\ngithub.com/mattn/go-isatty v0.0.5/go.mod h1:Iq45c/XA43vh69/j3iqttzPXn0bhXyGjM0Hdxcsrc5s=\ngithub.com/mattn/go-isatty v0.0.7/go.mod h1:Iq45c/XA43vh69/j3iqttzPXn0bhXyGjM0Hdxcsrc5s=\ngithub.com/mattn/go-isatty v0.0.8/go.mod h1:Iq45c/XA43vh69/j3iqttzPXn0bhXyGjM0Hdxcsrc5s=\ngithub.com/mattn/go-isatty v0.0.9/go.mod h1:YNRxwqDuOph6SZLI9vUUz6OYw3QyUt7WiY2yME+cCiQ=\ngithub.com/mattn/go-isatty v0.0.10/go.mod h1:qgIWMr58cqv1PHHyhnkY9lrL7etaEgOFcMEpPG5Rm84=\ngithub.com/mattn/go-isatty v0.0.11/go.mod h1:PhnuNfih5lzO57/f3n+odYbM4JtupLOxQOAqxQCu2WE=\ngithub.com/mattn/go-isatty v0.0.12/go.mod h1:cbi8OIDigv2wuxKPP5vlRcQ1OAZbq2CE4Kysco4FUpU=\ngithub.com/mattn/go-isatty v0.0.14/go.mod h1:7GGIvUiUoEMVVmxf/4nioHXj79iQHKdU27kJ6hsGG94=\ngithub.com/mattn/go-isatty v0.0.16 h1:bq3VjFmv/sOjHtdEhmkEV4x1AJtvUvOJ2PFAZ5+peKQ=\ngithub.com/mattn/go-isatty v0.0.16/go.mod h1:kYGgaQfpe5nmfYZH+SKPsOc2e4SrIfOl2e/yFXSvRLM=\ngithub.com/mattn/go-runewidth v0.0.2 h1:UnlwIPBGaTZfPQ6T1IGzPI0EkYAQmT9fAEJ/poFC63o=\ngithub.com/mattn/go-runewidth v0.0.2/go.mod h1:LwmH8dsx7+W8Uxz3IHJYH5QSwggIsqBzpuz5H//U1FU=\ngithub.com/mattn/go-sqlite3 v1.9.0/go.mod h1:FPy6KqzDD04eiIsT53CuJW3U88zkxoIYsOqkbpncsNc=\ngithub.com/mattn/go-sqlite3 v1.14.6/go.mod h1:NyWgC/yNuGj7Q9rpYnZvas74GogHl5/Z4A/KQRfk6bU=\ngithub.com/mattn/go-sqlite3 v1.14.12/go.mod h1:NyWgC/yNuGj7Q9rpYnZvas74GogHl5/Z4A/KQRfk6bU=\ngithub.com/mattn/go-sqlite3 v1.14.15/go.mod h1:2eHXhiwb8IkHr+BDWZGa96P6+rkvnG63S2DGjv9HUNg=\ngithub.com/mattn/go-sqlite3 v1.14.16 h1:yOQRA0RpS5PFz/oikGwBEqvAWhWg5ufRz4ETLjwpU1Y=\ngithub.com/mattn/go-sqlite3 v1.14.16/go.mod h1:2eHXhiwb8IkHr+BDWZGa96P6+rkvnG63S2DGjv9HUNg=\ngithub.com/matttproud/golang_protobuf_extensions v1.0.1 h1:4hp9jkHxhMHkqkrB3Ix0jegS5sx/RkqARlsWZ6pIwiU=\ngithub.com/matttproud/golang_protobuf_extensions v1.0.1/go.mod h1:D8He9yQNgCq6Z5Ld7szi9bcBfOoFv/3dc6xSMkL2PC0=\ngithub.com/microcosm-cc/bluemonday v1.0.20 h1:flpzsq4KU3QIYAYGV/szUat7H+GPOXR0B2JU5A1Wp8Y=\ngithub.com/microcosm-cc/bluemonday v1.0.20/go.mod h1:yfBmMi8mxvaZut3Yytv+jTXRY8mxyjJ0/kQBTElld50=\ngithub.com/miekg/dns v1.0.14/go.mod h1:W1PPwlIAgtquWBMBEV9nkV9Cazfe8ScdGz/Lj7v3Nrg=\ngithub.com/miekg/dns v1.1.25/go.mod h1:bPDLeHnStXmXAq1m/Ch/hvfNHr14JKNPMBo3VZKjuso=\ngithub.com/minio/asm2plan9s v0.0.0-20200509001527-cdd76441f9d8 h1:AMFGa4R4MiIpspGNG7Z948v4n35fFGB3RR3G/ry4FWs=\ngithub.com/minio/asm2plan9s v0.0.0-20200509001527-cdd76441f9d8/go.mod h1:mC1jAcsrzbxHt8iiaC+zU4b1ylILSosueou12R++wfY=\ngithub.com/minio/c2goasm v0.0.0-20190812172519-36a3d3bbc4f3 h1:+n/aFZefKZp7spd8DFdX7uMikMLXX4oubIzJF4kv/wI=\ngithub.com/minio/c2goasm v0.0.0-20190812172519-36a3d3bbc4f3/go.mod h1:RagcQ7I8IeTMnF8JTXieKnO4Z6JCsikNEzj0DwauVzE=\ngithub.com/mitchellh/cli v1.0.0/go.mod h1:hNIlj7HEI86fIcpObd7a0FcrxTWetlwJDGcceTlRvqc=\ngithub.com/mitchellh/copystructure v1.0.0/go.mod h1:SNtv71yrdKgLRyLFxmLdkAbkKEFWgYaq1OVrnRcwhnw=\ngithub.com/mitchellh/go-homedir v1.0.0/go.mod h1:SfyaCUpYCn1Vlf4IUYiD9fPX4A5wJrkLzIz1N1q0pr0=\ngithub.com/mitchellh/go-homedir v1.1.0/go.mod h1:SfyaCUpYCn1Vlf4IUYiD9fPX4A5wJrkLzIz1N1q0pr0=\ngithub.com/mitchellh/go-testing-interface v0.0.0-20171004221916-a61a99592b77/go.mod h1:kRemZodwjscx+RGhAo8eIhFbs2+BFgRtFPeD/KE+zxI=\ngithub.com/mitchellh/go-testing-interface v1.0.0/go.mod h1:kRemZodwjscx+RGhAo8eIhFbs2+BFgRtFPeD/KE+zxI=\ngithub.com/mitchellh/go-wordwrap v1.0.0/go.mod h1:ZXFpozHsX6DPmq2I0TCekCxypsnAUbP2oI0UX1GXzOo=\ngithub.com/mitchellh/gox v0.4.0/go.mod h1:Sd9lOJ0+aimLBi73mGofS1ycjY8lL3uZM3JPS42BGNg=\ngithub.com/mitchellh/iochan v1.0.0/go.mod h1:JwYml1nuB7xOzsp52dPpHFffvOCDupsG0QubkSMEySY=\ngithub.com/mitchellh/mapstructure v0.0.0-20160808181253-ca63d7c062ee/go.mod h1:FVVH3fgwuzCH5S8UJGiWEs2h04kUh9fWfEaFds41c1Y=\ngithub.com/mitchellh/mapstructure v1.1.2/go.mod h1:FVVH3fgwuzCH5S8UJGiWEs2h04kUh9fWfEaFds41c1Y=\ngithub.com/mitchellh/mapstructure v1.3.2/go.mod h1:bFUtVrKA4DC2yAKiSyO/QUcy7e+RRV2QTWOzhPopBRo=\ngithub.com/mitchellh/mapstructure v1.3.3/go.mod h1:bFUtVrKA4DC2yAKiSyO/QUcy7e+RRV2QTWOzhPopBRo=\ngithub.com/mitchellh/mapstructure v1.4.1/go.mod h1:bFUtVrKA4DC2yAKiSyO/QUcy7e+RRV2QTWOzhPopBRo=\ngithub.com/mitchellh/mapstructure v1.4.2/go.mod h1:bFUtVrKA4DC2yAKiSyO/QUcy7e+RRV2QTWOzhPopBRo=\ngithub.com/mitchellh/mapstructure v1.5.0 h1:jeMsZIYE/09sWLaz43PL7Gy6RuMjD2eJVyuac5Z2hdY=\ngithub.com/mitchellh/mapstructure v1.5.0/go.mod h1:bFUtVrKA4DC2yAKiSyO/QUcy7e+RRV2QTWOzhPopBRo=\ngithub.com/mitchellh/reflectwalk v1.0.0/go.mod h1:mSTlrgnPZtwu0c4WaC2kGObEpuNDbx0jmZXqmk4esnw=\ngithub.com/modern-go/concurrent v0.0.0-20180228061459-e0a39a4cb421/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=\ngithub.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd h1:TRLaZ9cD/w8PVh93nsPXa1VrQ6jlwL5oN8l14QlcNfg=\ngithub.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=\ngithub.com/modern-go/reflect2 v0.0.0-20180320133207-05fbef0ca5da/go.mod h1:bx2lNnkwVCuqBIxFjflWJWanXIb3RllmbCylyMrvgv0=\ngithub.com/modern-go/reflect2 v0.0.0-20180701023420-4b7aa43c6742/go.mod h1:bx2lNnkwVCuqBIxFjflWJWanXIb3RllmbCylyMrvgv0=\ngithub.com/modern-go/reflect2 v1.0.1/go.mod h1:bx2lNnkwVCuqBIxFjflWJWanXIb3RllmbCylyMrvgv0=\ngithub.com/modern-go/reflect2 v1.0.2 h1:xBagoLtFs94CBntxluKeaWgTMpvLxC4ur3nMaC9Gz0M=\ngithub.com/modern-go/reflect2 v1.0.2/go.mod h1:yWuevngMOJpCy52FWWMvUC8ws7m/LJsjYzDa0/r8luk=\ngithub.com/molecula/apophenia v0.0.0-20190827192002-68b7a14a478b h1:cZADDaNYM7xn/nklO3g198JerGQjadFuA0ofxBJgK0Y=\ngithub.com/molecula/apophenia v0.0.0-20190827192002-68b7a14a478b/go.mod h1:uXd1BiH7xLmgkhVmspdJLENv6uGWrTL/MQX2TN7Yz9s=\ngithub.com/montanaflynn/stats v0.0.0-20171201202039-1bf9dbcd8cbe/go.mod h1:wL8QJuTMNUDYhXwkmfOly8iTdp5TEcJFWZD2D7SIkUc=\ngithub.com/munnerz/goautoneg v0.0.0-20120707110453-a547fc61f48d/go.mod h1:+n7T8mK8HuQTcFwEeznm/DIxMOiR9yIdICNftLE1DvQ=\ngithub.com/mwitkow/go-conntrack v0.0.0-20161129095857-cc309e4a2223/go.mod h1:qRWi+5nqEBWmkhHvq77mSJWrCKwh8bxhgT7d/eI7P4U=\ngithub.com/mwitkow/go-conntrack v0.0.0-20190716064945-2f068394615f h1:KUppIJq7/+SVif2QVs3tOP0zanoHgBEVAwHxUSIzRqU=\ngithub.com/mwitkow/go-conntrack v0.0.0-20190716064945-2f068394615f/go.mod h1:qRWi+5nqEBWmkhHvq77mSJWrCKwh8bxhgT7d/eI7P4U=\ngithub.com/mwitkow/grpc-proxy v0.0.0-20181017164139-0f1106ef9c76/go.mod h1:x5OoJHDHqxHS801UIuhqGl6QdSAEJvtausosHSdazIo=\ngithub.com/mxk/go-flowrate v0.0.0-20140419014527-cca7078d478f/go.mod h1:ZdcZmHo+o7JKHSa8/e818NopupXU1YMK5fe1lsApnBw=\ngithub.com/nats-io/jwt v0.3.0/go.mod h1:fRYCDE99xlTsqUzISS1Bi75UBJ6ljOJQOAAu5VglpSg=\ngithub.com/nats-io/jwt v0.3.2/go.mod h1:/euKqTS1ZD+zzjYrY7pseZrTtWQSjujC7xjPc8wL6eU=\ngithub.com/nats-io/nats-server/v2 v2.1.2/go.mod h1:Afk+wRZqkMQs/p45uXdrVLuab3gwv3Z8C4HTBu8GD/k=\ngithub.com/nats-io/nats.go v1.9.1/go.mod h1:ZjDU1L/7fJ09jvUSRVBR2e7+RnLiiIQyqyzEE/Zbp4w=\ngithub.com/nats-io/nkeys v0.1.0/go.mod h1:xpnFELMwJABBLVhffcfd1MZx6VsNRFpEugbxziKVo7w=\ngithub.com/nats-io/nkeys v0.1.3/go.mod h1:xpnFELMwJABBLVhffcfd1MZx6VsNRFpEugbxziKVo7w=\ngithub.com/nats-io/nuid v1.0.1/go.mod h1:19wcPz3Ph3q0Jbyiqsd0kePYG7A95tJPxeL+1OSON2c=\ngithub.com/niemeyer/pretty v0.0.0-20200227124842-a10e7caefd8e/go.mod h1:zD1mROLANZcx1PVRCS0qkT7pwLkGfwJo4zjcN/Tysno=\ngithub.com/nrwiersma/avro-benchmarks v0.0.0-20210913175520-21aec48c8f76/go.mod h1:iKyFMidsk/sVYONJRE372sJuX/QTRPacU7imPqqsu7g=\ngithub.com/nxadm/tail v1.4.4/go.mod h1:kenIhsEOeOJmVchQTgglprH7qJGnHDVpk1VPCcaMI8A=\ngithub.com/nxadm/tail v1.4.8/go.mod h1:+ncqLTQzXmGhMZNUePPaPqPvBxHAIsmXswZKocGu+AU=\ngithub.com/oklog/oklog v0.3.2/go.mod h1:FCV+B7mhrz4o+ueLpx+KqkyXRGMWOYEvfiXtdGtbWGs=\ngithub.com/oklog/run v1.0.0/go.mod h1:dlhp/R75TPv97u0XWUtDeV/lRKWPKSdTuV0TZvrmrQA=\ngithub.com/oklog/ulid v1.3.1 h1:EGfNDEx6MqHz8B3uNV6QAib1UR2Lm97sHi3ocA6ESJ4=\ngithub.com/oklog/ulid v1.3.1/go.mod h1:CirwcVhetQ6Lv90oh/F+FBtV6XMibvdAFo93nm5qn4U=\ngithub.com/olekukonko/tablewriter v0.0.0-20170122224234-a0225b3f23b5/go.mod h1:vsDQFd/mU46D+Z4whnwzcISnGGzXWMclvtLoiIKAKIo=\ngithub.com/onsi/ginkgo v0.0.0-20170829012221-11459a886d9c/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\ngithub.com/onsi/ginkgo v1.6.0/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\ngithub.com/onsi/ginkgo v1.7.0/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\ngithub.com/onsi/ginkgo v1.10.1/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\ngithub.com/onsi/ginkgo v1.12.1/go.mod h1:zj2OWP4+oCPe1qIXoGWkgMRwljMUYCdkwsT2108oapk=\ngithub.com/onsi/ginkgo v1.14.0/go.mod h1:iSB4RoI2tjJc9BBv4NKIKWKya62Rps+oPG/Lv9klQyY=\ngithub.com/onsi/ginkgo v1.14.1/go.mod h1:iSB4RoI2tjJc9BBv4NKIKWKya62Rps+oPG/Lv9klQyY=\ngithub.com/onsi/ginkgo v1.16.4/go.mod h1:dX+/inL/fNMqNlz0e9LfyB9TswhZpCVdJM/Z6Vvnwo0=\ngithub.com/onsi/gomega v0.0.0-20170829124025-dcabb60a477c/go.mod h1:C1qb7wdrVGGVU+Z6iS04AVkA3Q65CEZX59MT0QO5uiA=\ngithub.com/onsi/gomega v1.4.3/go.mod h1:ex+gbHU/CVuBBDIJjb2X0qEXbFg53c61hWP/1CpauHY=\ngithub.com/onsi/gomega v1.7.0/go.mod h1:ex+gbHU/CVuBBDIJjb2X0qEXbFg53c61hWP/1CpauHY=\ngithub.com/onsi/gomega v1.7.1/go.mod h1:XdKZgCCFLUoM/7CFJVPcG8C1xQ1AJ0vpAezJrB7JYyY=\ngithub.com/onsi/gomega v1.10.1/go.mod h1:iN09h71vgCQne3DLsj+A5owkum+a2tYe+TOCB1ybHNo=\ngithub.com/onsi/gomega v1.10.2/go.mod h1:iN09h71vgCQne3DLsj+A5owkum+a2tYe+TOCB1ybHNo=\ngithub.com/onsi/gomega v1.16.0/go.mod h1:HnhC7FXeEQY45zxNK3PPoIUhzk/80Xly9PcubAlGdZY=\ngithub.com/op/go-logging v0.0.0-20160315200505-970db520ece7/go.mod h1:HzydrMdWErDVzsI23lYNej1Htcns9BCg93Dk0bBINWk=\ngithub.com/opentracing-contrib/go-observer v0.0.0-20170622124052-a52f23424492/go.mod h1:Ngi6UdF0k5OKD5t5wlmGhe/EDKPoUM3BXZSSfIuJbis=\ngithub.com/opentracing/basictracer-go v1.0.0/go.mod h1:QfBfYuafItcjQuMwinw9GhYKwFXS9KnPs5lxoYwgW74=\ngithub.com/opentracing/opentracing-go v1.0.2/go.mod h1:UkNAQd3GIcIGf0SeVgPpRdFStlNbqXla1AfSYxPUl2o=\ngithub.com/opentracing/opentracing-go v1.1.0/go.mod h1:UkNAQd3GIcIGf0SeVgPpRdFStlNbqXla1AfSYxPUl2o=\ngithub.com/opentracing/opentracing-go v1.2.0 h1:uEJPy/1a5RIPAJ0Ov+OIO8OxWu77jEv+1B0VhjKrZUs=\ngithub.com/opentracing/opentracing-go v1.2.0/go.mod h1:GxEUsuufX4nBwe+T+Wl9TAgYrxe9dPLANfrWvHYVTgc=\ngithub.com/openzipkin-contrib/zipkin-go-opentracing v0.4.5/go.mod h1:/wsWhb9smxSfWAKL3wpBW7V8scJMt8N8gnaMCS9E/cA=\ngithub.com/openzipkin/zipkin-go v0.1.6/go.mod h1:QgAqvLzwWbR/WpD4A3cGpPtJrZXNIiJc5AZX7/PBEpw=\ngithub.com/openzipkin/zipkin-go v0.2.1/go.mod h1:NaW6tEwdmWMaCDZzg8sh+IBNOxHMPnhQw8ySjnjRyN4=\ngithub.com/openzipkin/zipkin-go v0.2.2/go.mod h1:NaW6tEwdmWMaCDZzg8sh+IBNOxHMPnhQw8ySjnjRyN4=\ngithub.com/pact-foundation/pact-go v1.0.4/go.mod h1:uExwJY4kCzNPcHRj+hCR/HBbOOIwwtUjcrb0b5/5kLM=\ngithub.com/pascaldekloe/goe v0.0.0-20180627143212-57f6aae5913c/go.mod h1:lzWF7FIEvWOWxwDKqyGYQf6ZUaNfKdP144TG7ZOy1lc=\ngithub.com/pascaldekloe/goe v0.1.0/go.mod h1:lzWF7FIEvWOWxwDKqyGYQf6ZUaNfKdP144TG7ZOy1lc=\ngithub.com/pborman/uuid v1.2.0/go.mod h1:X/NO0urCmaxf9VXbdlT7C2Yzkj2IKimNn4k+gtPdI/k=\ngithub.com/pelletier/go-toml v1.2.0/go.mod h1:5z9KED0ma1S8pY6P1sdut58dfprrGBbd/94hg7ilaic=\ngithub.com/pelletier/go-toml v1.7.0/go.mod h1:vwGMzjaWMwyfHwgIBhI2YUM4fB6nL6lVAvS1LBMMhTE=\ngithub.com/pelletier/go-toml v1.9.3/go.mod h1:u1nR/EPcESfeI/szUZKdtJ0xRNbUoANCkoOuaOx1Y+c=\ngithub.com/pelletier/go-toml v1.9.5 h1:4yBQzkHv+7BHq2PQUZF3Mx0IYxG7LsP222s7Agd3ve8=\ngithub.com/pelletier/go-toml v1.9.5/go.mod h1:u1nR/EPcESfeI/szUZKdtJ0xRNbUoANCkoOuaOx1Y+c=\ngithub.com/performancecopilot/speed v3.0.0+incompatible/go.mod h1:/CLtqpZ5gBg1M9iaPbIdPPGyKcA8hKdoy6hAWba7Yac=\ngithub.com/peterbourgon/diskv v2.0.1+incompatible/go.mod h1:uqqh8zWWbv1HBMNONnaR/tNboyR3/BZd58JJSHlUSCU=\ngithub.com/philhofer/fwd v1.1.1 h1:GdGcTjf5RNAxwS4QLsiMzJYj5KEvPJD3Abr261yRQXQ=\ngithub.com/philhofer/fwd v1.1.1/go.mod h1:gk3iGcWd9+svBvR0sR+KPcfE+RNWozjowpeBVG3ZVNU=\ngithub.com/pierrec/lz4 v0.0.0-20190327172049-315a67e90e41/go.mod h1:3/3N9NVKO0jef7pBehbT1qWhCMrIgbYNnFAZCqQ5LRc=\ngithub.com/pierrec/lz4 v1.0.2-0.20190131084431-473cd7ce01a1/go.mod h1:3/3N9NVKO0jef7pBehbT1qWhCMrIgbYNnFAZCqQ5LRc=\ngithub.com/pierrec/lz4 v2.0.5+incompatible/go.mod h1:pdkljMzZIN41W+lC3N2tnIh5sFi+IEE17M5jbnwPHcY=\ngithub.com/pierrec/lz4 v2.5.2+incompatible/go.mod h1:pdkljMzZIN41W+lC3N2tnIh5sFi+IEE17M5jbnwPHcY=\ngithub.com/pierrec/lz4/v4 v4.1.14/go.mod h1:gZWDp/Ze/IJXGXf23ltt2EXimqmTUXEy0GFuRQyBid4=\ngithub.com/pierrec/lz4/v4 v4.1.15 h1:MO0/ucJhngq7299dKLwIMtgTfbkoSPF6AoMYDd8Q4q0=\ngithub.com/pierrec/lz4/v4 v4.1.15/go.mod h1:gZWDp/Ze/IJXGXf23ltt2EXimqmTUXEy0GFuRQyBid4=\ngithub.com/pilosa/avro v0.0.0-20200626214113-bc1bf9fd41c1 h1:VEiqiLmebtgmPmpMhcByL5oXeLmDZyCLx+JmCJxDlG4=\ngithub.com/pilosa/avro v0.0.0-20200626214113-bc1bf9fd41c1/go.mod h1:evNEPnmTS6kdXAZXtqlZgYvM+FwoJ9YdGc7YbLb2RVU=\ngithub.com/pingcap/errors v0.11.4 h1:lFuQV/oaUMGcD2tqt+01ROSmJs75VG1ToEOkZIZ4nE4=\ngithub.com/pkg/diff v0.0.0-20210226163009-20ebb0f2a09e/go.mod h1:pJLUxLENpZxwdsKMEsNbx1VGcRFpLqf3715MtcvvzbA=\ngithub.com/pkg/errors v0.8.0/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pkg/errors v0.8.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=\ngithub.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pkg/profile v1.2.1/go.mod h1:hJw3o1OdXxsrSjjVksARp5W95eeEaEfptyVZyv6JUPA=\ngithub.com/pkg/sftp v1.10.1/go.mod h1:lYOWFsE0bwd1+KfKJaKeuokY15vzFx25BLbzYYoAxZI=\ngithub.com/pmezard/go-difflib v0.0.0-20151028094244-d8ed2627bdf0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/posener/complete v1.1.1/go.mod h1:em0nMJCgc9GFtwrmVmEMR/ZL6WyhyjMBndrE9hABlRI=\ngithub.com/power-devops/perfstat v0.0.0-20210106213030-5aafc221ea8c h1:ncq/mPwQF4JjgDlrVEn3C11VoGHZN7m8qihwgMEtzYw=\ngithub.com/power-devops/perfstat v0.0.0-20210106213030-5aafc221ea8c/go.mod h1:OmDBASR4679mdNQnz2pUhc2G8CO2JrUAVFDRBDP/hJE=\ngithub.com/prometheus/client_golang v0.9.1/go.mod h1:7SWBe2y4D6OKWSNQJUaRYU/AaXPKyh/dDVn+NZz0KFw=\ngithub.com/prometheus/client_golang v0.9.2/go.mod h1:OsXs2jCmiKlQ1lTBmv21f2mNfw4xf/QclQDMrYNZzcM=\ngithub.com/prometheus/client_golang v0.9.3-0.20190127221311-3c4408c8b829/go.mod h1:p2iRAGwDERtqlqzRXnrOVns+ignqQo//hLXqYxZYVNs=\ngithub.com/prometheus/client_golang v0.9.3/go.mod h1:/TN21ttK/J9q6uSwhBd54HahCDft0ttaMvbicHlPoso=\ngithub.com/prometheus/client_golang v1.0.0/go.mod h1:db9x61etRT2tGnBNRi70OPL5FsnadC4Ky3P0J6CfImo=\ngithub.com/prometheus/client_golang v1.3.0/go.mod h1:hJaj2vgQTGQmVCsAACORcieXFeDPbaTKGT+JTgUa3og=\ngithub.com/prometheus/client_golang v1.7.1/go.mod h1:PY5Wy2awLA44sXw4AOSfFBetzPP4j5+D6mVACh+pe2M=\ngithub.com/prometheus/client_golang v1.11.0/go.mod h1:Z6t4BnS23TR94PD6BsDNk8yVqroYurpAkEiz0P2BEV0=\ngithub.com/prometheus/client_golang v1.11.1/go.mod h1:Z6t4BnS23TR94PD6BsDNk8yVqroYurpAkEiz0P2BEV0=\ngithub.com/prometheus/client_golang v1.12.1/go.mod h1:3Z9XVyYiZYEO+YQWt3RD2R3jrbd179Rt297l4aS6nDY=\ngithub.com/prometheus/client_golang v1.14.0 h1:nJdhIvne2eSX/XRAFV9PcvFFRbrjbcTUj0VP62TMhnw=\ngithub.com/prometheus/client_golang v1.14.0/go.mod h1:8vpkKitgIVNcqrRBWh1C4TIUQgYNtG/XQE4E/Zae36Y=\ngithub.com/prometheus/client_model v0.0.0-20180712105110-5c3871d89910/go.mod h1:MbSGuTsp3dbXC40dX6PRTWyKYBIrTGTE9sqQNg2J8bo=\ngithub.com/prometheus/client_model v0.0.0-20190115171406-56726106282f/go.mod h1:MbSGuTsp3dbXC40dX6PRTWyKYBIrTGTE9sqQNg2J8bo=\ngithub.com/prometheus/client_model v0.0.0-20190129233127-fd36f4220a90/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\ngithub.com/prometheus/client_model v0.0.0-20190812154241-14fe0d1b01d4/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\ngithub.com/prometheus/client_model v0.1.0/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\ngithub.com/prometheus/client_model v0.2.0/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\ngithub.com/prometheus/client_model v0.3.0 h1:UBgGFHqYdG/TPFD1B1ogZywDqEkwp3fBMvqdiQ7Xew4=\ngithub.com/prometheus/client_model v0.3.0/go.mod h1:LDGWKZIo7rky3hgvBe+caln+Dr3dPggB5dvjtD7w9+w=\ngithub.com/prometheus/common v0.0.0-20181113130724-41aa239b4cce/go.mod h1:daVV7qP5qjZbuso7PdcryaAu0sAZbrN9i7WWcTMWvro=\ngithub.com/prometheus/common v0.0.0-20181126121408-4724e9255275/go.mod h1:daVV7qP5qjZbuso7PdcryaAu0sAZbrN9i7WWcTMWvro=\ngithub.com/prometheus/common v0.2.0/go.mod h1:TNfzLD0ON7rHzMJeJkieUDPYmFC7Snx/y86RQel1bk4=\ngithub.com/prometheus/common v0.4.0/go.mod h1:TNfzLD0ON7rHzMJeJkieUDPYmFC7Snx/y86RQel1bk4=\ngithub.com/prometheus/common v0.4.1/go.mod h1:TNfzLD0ON7rHzMJeJkieUDPYmFC7Snx/y86RQel1bk4=\ngithub.com/prometheus/common v0.7.0/go.mod h1:DjGbpBbp5NYNiECxcL/VnbXCCaQpKd3tt26CguLLsqA=\ngithub.com/prometheus/common v0.10.0/go.mod h1:Tlit/dnDKsSWFlCLTWaA1cyBgKHSMdTB80sz/V91rCo=\ngithub.com/prometheus/common v0.15.0/go.mod h1:U+gB1OBLb1lF3O42bTCL+FK18tX9Oar16Clt/msog/s=\ngithub.com/prometheus/common v0.26.0/go.mod h1:M7rCNAaPfAosfx8veZJCuw84e35h3Cfd9VFqTh1DIvc=\ngithub.com/prometheus/common v0.32.1/go.mod h1:vu+V0TpY+O6vW9J44gczi3Ap/oXXR10b+M/gUGO4Hls=\ngithub.com/prometheus/common v0.33.0/go.mod h1:gB3sOl7P0TvJabZpLY5uQMpUqRCPPCyRLCZYc7JZTNE=\ngithub.com/prometheus/common v0.37.0 h1:ccBbHCgIiT9uSoFY0vX8H3zsNR5eLt17/RQLUvn8pXE=\ngithub.com/prometheus/common v0.37.0/go.mod h1:phzohg0JFMnBEFGxTDbfu3QyL5GI8gTQJFhYO5B3mfA=\ngithub.com/prometheus/procfs v0.0.0-20181005140218-185b4288413d/go.mod h1:c3At6R/oaqEKCNdg8wHV1ftS6bRYblBhIjjI8uT2IGk=\ngithub.com/prometheus/procfs v0.0.0-20181204211112-1dc9a6cbc91a/go.mod h1:c3At6R/oaqEKCNdg8wHV1ftS6bRYblBhIjjI8uT2IGk=\ngithub.com/prometheus/procfs v0.0.0-20190117184657-bf6a532e95b1/go.mod h1:c3At6R/oaqEKCNdg8wHV1ftS6bRYblBhIjjI8uT2IGk=\ngithub.com/prometheus/procfs v0.0.0-20190507164030-5867b95ac084/go.mod h1:TjEm7ze935MbeOT/UhFTIMYKhuLP4wbCsTZCD3I8kEA=\ngithub.com/prometheus/procfs v0.0.2/go.mod h1:TjEm7ze935MbeOT/UhFTIMYKhuLP4wbCsTZCD3I8kEA=\ngithub.com/prometheus/procfs v0.0.8/go.mod h1:7Qr8sr6344vo1JqZ6HhLceV9o3AJ1Ff+GxbHq6oeK9A=\ngithub.com/prometheus/procfs v0.1.3/go.mod h1:lV6e/gmhEcM9IjHGsFOCxxuZ+z1YqCvr4OA4YeYWdaU=\ngithub.com/prometheus/procfs v0.3.0/go.mod h1:lV6e/gmhEcM9IjHGsFOCxxuZ+z1YqCvr4OA4YeYWdaU=\ngithub.com/prometheus/procfs v0.6.0/go.mod h1:cz+aTbrPOrUb4q7XlbU9ygM+/jj0fzG6c1xBZuNvfVA=\ngithub.com/prometheus/procfs v0.7.3/go.mod h1:cz+aTbrPOrUb4q7XlbU9ygM+/jj0fzG6c1xBZuNvfVA=\ngithub.com/prometheus/procfs v0.8.0 h1:ODq8ZFEaYeCaZOJlZZdJA2AbQR98dSHSM1KW/You5mo=\ngithub.com/prometheus/procfs v0.8.0/go.mod h1:z7EfXMXOkbkqb9IINtpCn86r/to3BnA0uaxHdg830/4=\ngithub.com/prometheus/prom2json v1.3.1 h1:OogL5hsrJpLPz3jZ4LPz4sJRTtADzViCNRQoqrzUQvk=\ngithub.com/prometheus/prom2json v1.3.1/go.mod h1:A8Oy9aiQx4wrJY9ya1i4nHOySGmkVp5EO0aU1iSJR+g=\ngithub.com/prometheus/tsdb v0.7.1/go.mod h1:qhTCs0VvXwvX/y3TZrWD7rabWM+ijKTux40TwIPHuXU=\ngithub.com/rakyll/statik v0.1.7 h1:OF3QCZUuyPxuGEP7B4ypUa7sB/iHtqOTDYZXGM8KOdQ=\ngithub.com/rakyll/statik v0.1.7/go.mod h1:AlZONWzMtEnMs7W4e/1LURLiI49pIMmp6V9Unghqrcc=\ngithub.com/rcrowley/go-metrics v0.0.0-20181016184325-3113b8401b8a/go.mod h1:bCqnVzQkZxMG4s8nGwiZ5l3QUCyqpo9Y+/ZMZ9VjZe4=\ngithub.com/remyoudompheng/bigfft v0.0.0-20200410134404-eec4a21b6bb0 h1:OdAsTTz6OkFY5QxjkYwrChwuRruF69c169dPK26NUlk=\ngithub.com/remyoudompheng/bigfft v0.0.0-20200410134404-eec4a21b6bb0/go.mod h1:qqbHyh8v60DhA7CoWK5oRCqLrMHRGoxYCSS9EjAz6Eo=\ngithub.com/ricochet2200/go-disk-usage/du v0.0.0-20210707232629-ac9918953285 h1:d54EL9l+XteliUfUCGsEwwuk65dmmxX85VXF+9T6+50=\ngithub.com/ricochet2200/go-disk-usage/du v0.0.0-20210707232629-ac9918953285/go.mod h1:fxIDly1xtudczrZeOOlfaUvd2OPb2qZAPuWdU2BsBTk=\ngithub.com/rogpeppe/clock v0.0.0-20190514195947-2896927a307a/go.mod h1:4r5QyqhjIWCcK8DO4KMclc5Iknq5qVBAlbYYzAbUScQ=\ngithub.com/rogpeppe/fastuuid v0.0.0-20150106093220-6724a57986af/go.mod h1:XWv6SoW27p1b0cqNHllgS5HIMJraePCO15w5zCzIWYg=\ngithub.com/rogpeppe/fastuuid v1.2.0/go.mod h1:jVj6XXZzXRy/MSR5jhDC/2q6DgLz+nrA6LYCDYWNEvQ=\ngithub.com/rogpeppe/go-internal v1.1.0/go.mod h1:M8bDsm7K2OlrFYOpmOWEs/qY81heoFRclV5y23lUDJ4=\ngithub.com/rogpeppe/go-internal v1.2.2/go.mod h1:M8bDsm7K2OlrFYOpmOWEs/qY81heoFRclV5y23lUDJ4=\ngithub.com/rogpeppe/go-internal v1.3.0/go.mod h1:M8bDsm7K2OlrFYOpmOWEs/qY81heoFRclV5y23lUDJ4=\ngithub.com/rogpeppe/go-internal v1.6.1/go.mod h1:xXDCJY+GAPziupqXw64V24skbSoqbTEfhy4qGm1nDQc=\ngithub.com/rogpeppe/go-internal v1.8.0/go.mod h1:WmiCO8CzOY8rg0OYDC4/i/2WRWAB6poM+XZ2dLUbcbE=\ngithub.com/rogpeppe/go-internal v1.9.0 h1:73kH8U+JUqXU8lRuOHeVHaa/SZPifC7BkcraZVejAe8=\ngithub.com/rogpeppe/go-internal v1.9.0/go.mod h1:WtVeX8xhTBvf0smdhujwtBcq4Qrzq/fJaraNFVN+nFs=\ngithub.com/rs/cors v1.7.0/go.mod h1:gFx+x8UowdsKA9AchylcLynDq+nNFfI8FkUZdN/jGCU=\ngithub.com/rs/cors v1.8.2 h1:KCooALfAYGs415Cwu5ABvv9n9509fSiG5SQJn/AQo4U=\ngithub.com/rs/cors v1.8.2/go.mod h1:XyqrcTp5zjWr1wsJ8PIRZssZ8b/WMcMf71DJnit4EMU=\ngithub.com/rs/xid v1.2.1/go.mod h1:+uKXf+4Djp6Md1KODXJxgGQPKngRmWyn10oCKFzNHOQ=\ngithub.com/rs/zerolog v1.13.0/go.mod h1:YbFCdg8HfsridGWAh22vktObvhZbQsZXe4/zB0OKkWU=\ngithub.com/rs/zerolog v1.15.0/go.mod h1:xYTKnLHcpfU2225ny5qZjxnj9NvkumZYjJHlAThCjNc=\ngithub.com/russross/blackfriday/v2 v2.0.1/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=\ngithub.com/russross/blackfriday/v2 v2.1.0/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=\ngithub.com/ryanuber/columnize v0.0.0-20160712163229-9b3edd62028f/go.mod h1:sm1tb6uqfes/u+d4ooFouqFdy9/2g9QGwK3SQygK0Ts=\ngithub.com/ryanuber/columnize v2.1.0+incompatible/go.mod h1:sm1tb6uqfes/u+d4ooFouqFdy9/2g9QGwK3SQygK0Ts=\ngithub.com/ryanuber/go-glob v1.0.0/go.mod h1:807d1WSdnB0XRJzKNil9Om6lcp/3a0v4qIHxIXzX/Yc=\ngithub.com/sajari/regression v1.0.1 h1:iTVc6ZACGCkoXC+8NdqH5tIreslDTT/bXxT6OmHR5PE=\ngithub.com/sajari/regression v1.0.1/go.mod h1:NeG/XTW1lYfGY7YV/Z0nYDV/RGh3wxwd1yW46835flM=\ngithub.com/samuel/go-zookeeper v0.0.0-20190923202752-2cc03de413da/go.mod h1:gi+0XIa01GRL2eRQVjQkKGqKF3SF9vZR/HnPullcV2E=\ngithub.com/santhosh-tekuri/jsonschema/v5 v5.0.0/go.mod h1:FKdcjfQW6rpZSnxxUvEA5H/cDPdvJ/SZJQLWWXWGrZ0=\ngithub.com/satori/go.uuid v1.2.0/go.mod h1:dA0hQrYB0VpLJoorglMZABFdXlWrHn1NEOzdhQKdks0=\ngithub.com/satori/go.uuid v1.2.1-0.20180404165556-75cca531ea76 h1:ofyVTM1w4iyKwaQIlRR6Ip06mXXx5Cnz7a4mTGYq1hE=\ngithub.com/satori/go.uuid v1.2.1-0.20180404165556-75cca531ea76/go.mod h1:dA0hQrYB0VpLJoorglMZABFdXlWrHn1NEOzdhQKdks0=\ngithub.com/sean-/seed v0.0.0-20170313163322-e2103e2c3529/go.mod h1:DxrIzT+xaE7yg65j358z/aeFdxmN0P9QXhEzd20vsDc=\ngithub.com/segmentio/kafka-go v0.4.29 h1:4ujULpikzHG0HqKhjumDghFjy/0RRCSl/7lbriwQAH0=\ngithub.com/segmentio/kafka-go v0.4.29/go.mod h1:m1lXeqJtIFYZayv0shM/tjrAFljvWLTprxBHd+3PnaU=\ngithub.com/sergi/go-diff v1.2.0 h1:XU+rvMAioB0UC3q1MFrIQy4Vo5/4VsRDQQXHsEya6xQ=\ngithub.com/sergi/go-diff v1.2.0/go.mod h1:STckp+ISIX8hZLjrqAeVduY0gWCT9IjLuqbuNXdaHfM=\ngithub.com/shirou/gopsutil/v3 v3.22.5 h1:atX36I/IXgFiB81687vSiBI5zrMsxcIBkP9cQMJQoJA=\ngithub.com/shirou/gopsutil/v3 v3.22.5/go.mod h1:so9G9VzeHt/hsd0YwqprnjHnfARAUktauykSbr+y2gA=\ngithub.com/shopspring/decimal v0.0.0-20180709203117-cd690d0c9e24/go.mod h1:M+9NzErvs504Cn4c5DxATwIqPbtswREoFCre64PpcG4=\ngithub.com/shopspring/decimal v0.0.0-20200227202807-02e2044944cc/go.mod h1:DKyhrW/HYNuLGql+MJL6WCR6knT2jwCFRcu2hWCYk4o=\ngithub.com/shopspring/decimal v1.2.0 h1:abSATXmQEYyShuxI4/vyW3tV1MrKAJzCZ/0zLUXYbsQ=\ngithub.com/shopspring/decimal v1.2.0/go.mod h1:DKyhrW/HYNuLGql+MJL6WCR6knT2jwCFRcu2hWCYk4o=\ngithub.com/shurcooL/sanitized_anchor_name v1.0.0/go.mod h1:1NzhyTcUVG4SuEtjjoZeVRXNmyL/1OwPU0+IJeTBvfc=\ngithub.com/sirupsen/logrus v1.2.0/go.mod h1:LxeOpSwHxABJmUn/MG1IvRgCAasNZTLOkJPxbbu5VWo=\ngithub.com/sirupsen/logrus v1.4.0/go.mod h1:LxeOpSwHxABJmUn/MG1IvRgCAasNZTLOkJPxbbu5VWo=\ngithub.com/sirupsen/logrus v1.4.1/go.mod h1:ni0Sbl8bgC9z8RoU9G6nDWqqs/fq4eDPysMBDgk/93Q=\ngithub.com/sirupsen/logrus v1.4.2/go.mod h1:tLMulIdttU9McNUspp0xgXVQah82FyeX6MwdIuYE2rE=\ngithub.com/sirupsen/logrus v1.6.0/go.mod h1:7uNnSEd1DgxDLC74fIahvMZmmYsHGZGEOFrfsX/uA88=\ngithub.com/sirupsen/logrus v1.7.0/go.mod h1:yWOB1SBYBC5VeMP7gHvWumXLIWorT60ONWic61uBYv0=\ngithub.com/sirupsen/logrus v1.9.0 h1:trlNQbNUG3OdDrDil03MCb1H2o9nJ1x4/5LYw7byDE0=\ngithub.com/sirupsen/logrus v1.9.0/go.mod h1:naHLuLoDiP4jHNo9R0sCBMtWGeIprob74mVsIT4qYEQ=\ngithub.com/smartystreets/assertions v0.0.0-20180927180507-b2de0cb4f26d h1:zE9ykElWQ6/NYmHa3jpm/yHnI4xSofP+UP6SpjHcSeM=\ngithub.com/smartystreets/assertions v0.0.0-20180927180507-b2de0cb4f26d/go.mod h1:OnSkiWE9lh6wB0YB77sQom3nweQdgAjqCqsofrRNTgc=\ngithub.com/smartystreets/go-aws-auth v0.0.0-20180515143844-0c1422d1fdb9/go.mod h1:SnhjPscd9TpLiy1LpzGSKh3bXCfxxXuqd9xmQJy3slM=\ngithub.com/smartystreets/goconvey v1.6.4 h1:fv0U8FUIMPNf1L9lnHLvLhgicrIVChEkdzIKYqbNC9s=\ngithub.com/smartystreets/goconvey v1.6.4/go.mod h1:syvi0/a8iFYH4r/RixwvyeAJjdLS9QV7WQ/tjFTllLA=\ngithub.com/soheilhy/cmux v0.1.4/go.mod h1:IM3LyeVVIOuxMH7sFAkER9+bJ4dT7Ms6E4xg4kGIyLM=\ngithub.com/soheilhy/cmux v0.1.5 h1:jjzc5WVemNEDTLwv9tlmemhC73tI08BNOIGwBOo10Js=\ngithub.com/soheilhy/cmux v0.1.5/go.mod h1:T7TcVDs9LWfQgPlPsdngu6I6QIoyIFZDDC6sNE1GqG0=\ngithub.com/sony/gobreaker v0.4.1/go.mod h1:ZKptC7FHNvhBz7dN2LGjPVBz2sZJmc0/PkyDJOjmxWY=\ngithub.com/sourcegraph/annotate v0.0.0-20160123013949-f4cad6c6324d h1:yKm7XZV6j9Ev6lojP2XaIshpT4ymkqhMeSghO5Ps00E=\ngithub.com/sourcegraph/annotate v0.0.0-20160123013949-f4cad6c6324d/go.mod h1:UdhH50NIW0fCiwBSr0co2m7BnFLdv4fQTgdqdJTHFeE=\ngithub.com/sourcegraph/syntaxhighlight v0.0.0-20170531221838-bd320f5d308e h1:qpG93cPwA5f7s/ZPBJnGOYQNK/vKsaDaseuKT5Asee8=\ngithub.com/sourcegraph/syntaxhighlight v0.0.0-20170531221838-bd320f5d308e/go.mod h1:HuIsMU8RRBOtsCgI77wP899iHVBQpCmg4ErYMZB+2IA=\ngithub.com/spaolacci/murmur3 v0.0.0-20180118202830-f09979ecbc72 h1:qLC7fQah7D6K1B0ujays3HV9gkFtllcxhzImRR7ArPQ=\ngithub.com/spaolacci/murmur3 v0.0.0-20180118202830-f09979ecbc72/go.mod h1:JwIasOWyU6f++ZhiEuf87xNszmSA2myDM2Kzu9HwQUA=\ngithub.com/spf13/afero v1.1.2/go.mod h1:j4pytiNVoe2o6bmDsKpLACNPDBIoEAkihy7loJ1B0CQ=\ngithub.com/spf13/afero v1.2.2/go.mod h1:9ZxEEn6pIJ8Rxe320qSDBk6AsU0r9pR7Q4OcevTdifk=\ngithub.com/spf13/afero v1.6.0 h1:xoax2sJ2DT8S8xA2paPFjDCScCNeWsg75VG0DLRreiY=\ngithub.com/spf13/afero v1.6.0/go.mod h1:Ai8FlHk4v/PARR026UzYexafAt9roJ7LcLMAmO6Z93I=\ngithub.com/spf13/cast v1.3.0/go.mod h1:Qx5cxh0v+4UWYiBimWS+eyWzqEqokIECu5etghLkUJE=\ngithub.com/spf13/cast v1.3.1 h1:nFm6S0SMdyzrzcmThSipiEubIDy8WEXKNZ0UOgiRpng=\ngithub.com/spf13/cast v1.3.1/go.mod h1:Qx5cxh0v+4UWYiBimWS+eyWzqEqokIECu5etghLkUJE=\ngithub.com/spf13/cobra v0.0.3/go.mod h1:1l0Ry5zgKvJasoi3XT1TypsSe7PqH0Sj9dhYf7v3XqQ=\ngithub.com/spf13/cobra v1.1.3/go.mod h1:pGADOWyqRD/YMrPZigI/zbliZ2wVD/23d+is3pSWzOo=\ngithub.com/spf13/cobra v1.6.1 h1:o94oiPyS4KD1mPy2fmcYYHHfCxLqYjJOhGsCHFZtEzA=\ngithub.com/spf13/cobra v1.6.1/go.mod h1:IOw/AERYS7UzyrGinqmz6HLUo219MORXGxhbaJUqzrY=\ngithub.com/spf13/jwalterweatherman v1.0.0/go.mod h1:cQK4TGJAtQXfYWX+Ddv3mKDzgVb68N+wFjFa4jdeBTo=\ngithub.com/spf13/jwalterweatherman v1.1.0 h1:ue6voC5bR5F8YxI5S67j9i582FU4Qvo2bmqnqMYADFk=\ngithub.com/spf13/jwalterweatherman v1.1.0/go.mod h1:aNWZUN0dPAAO/Ljvb5BEdw96iTZ0EXowPYD95IqWIGo=\ngithub.com/spf13/pflag v0.0.0-20170130214245-9ff6c6923cff/go.mod h1:DYY7MBk1bdzusC3SYhjObp+wFpr4gzcvqqNjLnInEg4=\ngithub.com/spf13/pflag v1.0.1/go.mod h1:DYY7MBk1bdzusC3SYhjObp+wFpr4gzcvqqNjLnInEg4=\ngithub.com/spf13/pflag v1.0.3/go.mod h1:DYY7MBk1bdzusC3SYhjObp+wFpr4gzcvqqNjLnInEg4=\ngithub.com/spf13/pflag v1.0.5 h1:iy+VFUOCP1a+8yFto/drg2CJ5u0yRoB7fZw3DKv/JXA=\ngithub.com/spf13/pflag v1.0.5/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=\ngithub.com/spf13/viper v1.4.0/go.mod h1:PTJ7Z/lr49W6bUbkmS1V3by4uWynFiR9p7+dSq/yZzE=\ngithub.com/spf13/viper v1.7.0/go.mod h1:8WkrPz2fc9jxqZNCJI/76HCieCp4Q8HaLFoCha5qpdg=\ngithub.com/spf13/viper v1.8.1 h1:Kq1fyeebqsBfbjZj4EL7gj2IO0mMaiyjYUWcUsl2O44=\ngithub.com/spf13/viper v1.8.1/go.mod h1:o0Pch8wJ9BVSWGQMbra6iw0oQ5oktSIBaujf1rJH9Ns=\ngithub.com/streadway/amqp v0.0.0-20190404075320-75d898a42a94/go.mod h1:AZpEONHx3DKn8O/DFsRAY58/XVQiIPMTMB1SddzLXVw=\ngithub.com/streadway/amqp v0.0.0-20190827072141-edfb9018d271/go.mod h1:AZpEONHx3DKn8O/DFsRAY58/XVQiIPMTMB1SddzLXVw=\ngithub.com/streadway/handy v0.0.0-20190108123426-d5acb3125c2a/go.mod h1:qNTQ5P5JnDBl6z3cMAg/SywNDC5ABu5ApDIw6lUbRmI=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/objx v0.1.1/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/objx v0.2.0/go.mod h1:qt09Ya8vawLte6SNmTgCsAVtYtaKzEcn8ATUoHMkEqE=\ngithub.com/stretchr/objx v0.4.0/go.mod h1:YvHI0jy2hoMjB+UWwv71VJQ9isScKT/TqJzVSSt89Yw=\ngithub.com/stretchr/objx v0.5.0 h1:1zr/of2m5FGMsad5YfcqgdqdWrIhu+EBEJRhR1U7z/c=\ngithub.com/stretchr/objx v0.5.0/go.mod h1:Yh+to48EsGEfYuaHDzXPcE3xhTkx73EhmCGUpEOglKo=\ngithub.com/stretchr/testify v0.0.0-20151208002404-e3a8ff8ce365/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=\ngithub.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=\ngithub.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\ngithub.com/stretchr/testify v1.3.1-0.20190311161405-34c6fa2dc709/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\ngithub.com/stretchr/testify v1.4.0/go.mod h1:j7eGeouHqKxXV5pUuKE4zz7dFj8WfuZ+81PSLYec5m4=\ngithub.com/stretchr/testify v1.5.1/go.mod h1:5W2xD1RspED5o8YsWQXVCued0rvSQ+mT+I5cxcmMvtA=\ngithub.com/stretchr/testify v1.6.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.7.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.8.0/go.mod h1:yNjHg4UonilssWZ8iaSj1OCr/vHnekPRkoO+kdMU+MU=\ngithub.com/stretchr/testify v1.8.1 h1:w7B6lhMri9wdJUVmEZPGGhZzrYTPvgJArz7wNPgYKsk=\ngithub.com/stretchr/testify v1.8.1/go.mod h1:w2LPCIKwWwSfY2zedu0+kehJoqGctiVI29o6fzry7u4=\ngithub.com/subosito/gotenv v1.2.0 h1:Slr1R9HxAlEKefgq5jn9U+DnETlIUa6HfgEzj0g5d7s=\ngithub.com/subosito/gotenv v1.2.0/go.mod h1:N0PQaV/YGNqwC0u51sEeR/aUtSLEXKX9iv69rRypqCw=\ngithub.com/syndtr/goleveldb v1.0.0/go.mod h1:ZVVdQEZoIme9iO1Ch2Jdy24qqXrMMOU6lpPAyBWyWuQ=\ngithub.com/tgruben/gomem v0.0.0-20221021111114-79fdc77dcf61 h1:3RJ3IN/m4w4B7PeYF8PPOdGLHqu045LIxtymF4lQO7g=\ngithub.com/tgruben/gomem v0.0.0-20221021111114-79fdc77dcf61/go.mod h1:avNE+ynGJYvQNY+/5Gk6aHxtlOtt2aO6V9ShcONBvvI=\ngithub.com/tgruben/ivy v0.0.0-20230111144143-b80a659caeaf h1:+bnHPov8gMIztSP1zxMbUXei8IiVNXbQre4+u9lpkQ0=\ngithub.com/tgruben/ivy v0.0.0-20230111144143-b80a659caeaf/go.mod h1:/COPfnSdd23BhmpDXFKsYPk1kK2sJpUr21TsZ1nsgcg=\ngithub.com/tidwall/btree v0.3.0/go.mod h1:huei1BkDWJ3/sLXmO+bsCNELL+Bp2Kks9OLyQFkzvA8=\ngithub.com/tidwall/btree v1.1.0/go.mod h1:TzIRzen6yHbibdSfK6t8QimqbUnoxUSrZfeW7Uob0q4=\ngithub.com/tidwall/buntdb v1.2.0/go.mod h1:XLza/dhlwzO6dc5o/KWor4kfZSt3BP8QV+77ZMKfI58=\ngithub.com/tidwall/gjson v1.6.7/go.mod h1:zeFuBCIqD4sN/gmqBzZ4j7Jd6UcA2Fc56x7QFsv+8fI=\ngithub.com/tidwall/gjson v1.6.8/go.mod h1:zeFuBCIqD4sN/gmqBzZ4j7Jd6UcA2Fc56x7QFsv+8fI=\ngithub.com/tidwall/gjson v1.12.1/go.mod h1:/wbyibRr2FHMks5tjHJ5F8dMZh3AcwJEMf5vlfC0lxk=\ngithub.com/tidwall/grect v0.1.0/go.mod h1:sa5O42oP6jWfTShL9ka6Sgmg3TgIK649veZe05B7+J8=\ngithub.com/tidwall/grect v0.1.4/go.mod h1:9FBsaYRaR0Tcy4UwefBX/UDcDcDy9V5jUcxHzv2jd5Q=\ngithub.com/tidwall/match v1.0.3/go.mod h1:eRSPERbgtNPcGhD8UCthc6PmLEQXEWd3PRB5JTxsfmM=\ngithub.com/tidwall/match v1.1.1/go.mod h1:eRSPERbgtNPcGhD8UCthc6PmLEQXEWd3PRB5JTxsfmM=\ngithub.com/tidwall/pretty v1.0.0/go.mod h1:XNkn88O1ChpSDQmQeStsy+sBenx6DDtFZJxhVysOjyk=\ngithub.com/tidwall/pretty v1.0.2/go.mod h1:XNkn88O1ChpSDQmQeStsy+sBenx6DDtFZJxhVysOjyk=\ngithub.com/tidwall/pretty v1.2.0 h1:RWIZEg2iJ8/g6fDDYzMpobmaoGh5OLl4AXtGUGPcqCs=\ngithub.com/tidwall/pretty v1.2.0/go.mod h1:ITEVvHYasfjBbM0u2Pg8T2nJnzm8xPwvNhhsoaGGjNU=\ngithub.com/tidwall/rtred v0.1.2/go.mod h1:hd69WNXQ5RP9vHd7dqekAz+RIdtfBogmglkZSRxCHFQ=\ngithub.com/tidwall/tinyqueue v0.1.1/go.mod h1:O/QNHwrnjqr6IHItYrzoHAKYhBkLI67Q096fQP5zMYw=\ngithub.com/tinylib/msgp v1.1.2 h1:gWmO7n0Ys2RBEb7GPYB9Ujq8Mk5p2U08lRnmMcGy6BQ=\ngithub.com/tinylib/msgp v1.1.2/go.mod h1:+d+yLhGm8mzTaHzB+wgMYrodPfmZrzkirds8fDWklFE=\ngithub.com/tklauser/go-sysconf v0.3.10 h1:IJ1AZGZRWbY8T5Vfk04D9WOA5WSejdflXxP03OUqALw=\ngithub.com/tklauser/go-sysconf v0.3.10/go.mod h1:C8XykCvCb+Gn0oNCWPIlcb0RuglQTYaQ2hGm7jmxEFk=\ngithub.com/tklauser/numcpus v0.4.0 h1:E53Dm1HjH1/R2/aoCtXtPgzmElmn51aOkhCFSuZq//o=\ngithub.com/tklauser/numcpus v0.4.0/go.mod h1:1+UI3pD8NW14VMwdgJNJ1ESk2UnwhAnz5hMwiKKqXCQ=\ngithub.com/tmc/grpc-websocket-proxy v0.0.0-20170815181823-89b8d40f7ca8/go.mod h1:ncp9v5uamzpCO7NfCPTXjqaC+bZgJeR0sMTm6dMHP7U=\ngithub.com/tmc/grpc-websocket-proxy v0.0.0-20190109142713-0ad062ec5ee5/go.mod h1:ncp9v5uamzpCO7NfCPTXjqaC+bZgJeR0sMTm6dMHP7U=\ngithub.com/tmc/grpc-websocket-proxy v0.0.0-20201229170055-e5319fda7802 h1:uruHq4dN7GR16kFc5fp3d1RIYzJW5onx8Ybykw2YQFA=\ngithub.com/tmc/grpc-websocket-proxy v0.0.0-20201229170055-e5319fda7802/go.mod h1:ncp9v5uamzpCO7NfCPTXjqaC+bZgJeR0sMTm6dMHP7U=\ngithub.com/tmthrgd/go-hex v0.0.0-20190904060850-447a3041c3bc/go.mod h1:bciPuU6GHm1iF1pBvUfxfsH0Wmnc2VbpgvbI9ZWuIRs=\ngithub.com/tv42/httpunix v0.0.0-20150427012821-b75d8614f926/go.mod h1:9ESjWnEqriFuLhtthL60Sar/7RFoluCcXsuvEwTV5KM=\ngithub.com/twitchtv/twirp v8.1.1+incompatible/go.mod h1:RRJoFSAmTEh2weEqWtpPE3vFK5YBhA6bqp2l1kfCC5A=\ngithub.com/uber/jaeger-client-go v2.25.0+incompatible h1:IxcNZ7WRY1Y3G4poYlx24szfsn/3LvK9QHCq9oQw8+U=\ngithub.com/uber/jaeger-client-go v2.25.0+incompatible/go.mod h1:WVhlPFC8FDjOFMMWRy2pZqQJSXxYSwNYOkTr/Z6d3Kk=\ngithub.com/uber/jaeger-lib v2.4.1+incompatible h1:td4jdvLcExb4cBISKIpHuGoVXh+dVKhn2Um6rjCsSsg=\ngithub.com/uber/jaeger-lib v2.4.1+incompatible/go.mod h1:ComeNDZlWwrWnDv8aPp0Ba6+uUTzImX/AauajbLI56U=\ngithub.com/ugorji/go v1.1.4/go.mod h1:uQMGLiO92mf5W77hV/PUCpI3pbzQx3CRekS0kk+RGrc=\ngithub.com/ugorji/go v1.1.7 h1:/68gy2h+1mWMrwZFeD1kQialdSzAb432dtpeJ42ovdo=\ngithub.com/ugorji/go v1.1.7/go.mod h1:kZn38zHttfInRq0xu/PH0az30d+z6vm202qpg1oXVMw=\ngithub.com/ugorji/go/codec v1.1.7 h1:2SvQaVZ1ouYrrKKwoSk2pzd4A9evlKJb9oTL+OaLUSs=\ngithub.com/ugorji/go/codec v1.1.7/go.mod h1:Ax+UKWsSmolVDwsd+7N3ZtXu+yMGCf907BLYF3GoBXY=\ngithub.com/urfave/cli v1.20.0/go.mod h1:70zkFmudgCuE/ngEzBv17Jvp/497gISqfk5gWijbERA=\ngithub.com/urfave/cli v1.22.1/go.mod h1:Gos4lmkARVdJ6EkW0WaNv/tZAAMe9V7XWyB60NtXRu0=\ngithub.com/urfave/negroni v1.0.0/go.mod h1:Meg73S6kFm/4PpbYdq35yYWoCZ9mS/YSx+lKnmiohz4=\ngithub.com/valyala/bytebufferpool v1.0.0/go.mod h1:6bBcMArwyJ5K/AmCkWv1jt77kVWyCJ6HpOuEn7z0Csc=\ngithub.com/valyala/fasthttp v1.26.0/go.mod h1:cmWIqlu99AO/RKcp1HWaViTqc57FswJOfYYdPJBl8BA=\ngithub.com/valyala/fasthttp v1.32.0/go.mod h1:2rsYD01CKFrjjsvFxx75KlEUNpWNBY9JWD3K/7o2Cus=\ngithub.com/valyala/fasttemplate v1.0.1/go.mod h1:UQGH1tvbgY+Nz5t2n7tXsz52dQxojPUpymEIMZ47gx8=\ngithub.com/valyala/fasttemplate v1.2.1/go.mod h1:KHLXt3tVN2HBp8eijSv/kGJopbvo7S+qRAEEKiv+SiQ=\ngithub.com/valyala/tcplisten v1.0.0/go.mod h1:T0xQ8SeCZGxckz9qRXTfG43PvQ/mcWh7FwZEA7Ioqkc=\ngithub.com/vmihailenco/bufpool v0.1.11/go.mod h1:AFf/MOy3l2CFTKbxwt0mp2MwnqjNEs5H/UxrkA5jxTQ=\ngithub.com/vmihailenco/msgpack/v4 v4.3.11/go.mod h1:gborTTJjAo/GWTqqRjrLCn9pgNN+NXzzngzBKDPIqw4=\ngithub.com/vmihailenco/msgpack/v5 v5.0.0-beta.1/go.mod h1:xlngVLeyQ/Qi05oQxhQ+oTuqa03RjMwMfk/7/TCs+QI=\ngithub.com/vmihailenco/msgpack/v5 v5.3.4/go.mod h1:7xyJ9e+0+9SaZT0Wt1RGleJXzli6Q/V5KbhBonMG9jc=\ngithub.com/vmihailenco/tagparser v0.1.1/go.mod h1:OeAg3pn3UbLjkWt+rN9oFYB6u/cQgqMEUPoW2WPyhdI=\ngithub.com/vmihailenco/tagparser v0.1.2/go.mod h1:OeAg3pn3UbLjkWt+rN9oFYB6u/cQgqMEUPoW2WPyhdI=\ngithub.com/vmihailenco/tagparser/v2 v2.0.0/go.mod h1:Wri+At7QHww0WTrCBeu4J6bNtoV6mEfg5OIWRZA9qds=\ngithub.com/xdg-go/pbkdf2 v1.0.0/go.mod h1:jrpuAogTd400dnrH08LKmI/xc1MbPOebTwRqcT5RDeI=\ngithub.com/xdg-go/scram v1.0.2/go.mod h1:1WAq6h33pAW+iRreB34OORO2Nf7qel3VV3fjBj+hCSs=\ngithub.com/xdg-go/stringprep v1.0.2/go.mod h1:8F9zXuvzgwmyT5DUm4GUfZGDdT3W+LCvS6+da4O5kxM=\ngithub.com/xdg/scram v0.0.0-20180814205039-7eeb5667e42c h1:u40Z8hqBAAQyv+vATcGgV0YCnDjqSL7/q/JyPhhJSPk=\ngithub.com/xdg/scram v0.0.0-20180814205039-7eeb5667e42c/go.mod h1:lB8K/P019DLNhemzwFU4jHLhdvlE6uDZjXFejJXr49I=\ngithub.com/xdg/stringprep v1.0.0 h1:d9X0esnoa3dFsV0FG35rAT0RIhYFlPq7MiP+DW89La0=\ngithub.com/xdg/stringprep v1.0.0/go.mod h1:Jhud4/sHMO4oL310DaZAKk9ZaJ08SJfe+sJh0HrGL1Y=\ngithub.com/xiang90/probing v0.0.0-20190116061207-43a291ad63a2 h1:eY9dn8+vbi4tKz5Qo6v2eYzo7kUS51QINcR5jNpbZS8=\ngithub.com/xiang90/probing v0.0.0-20190116061207-43a291ad63a2/go.mod h1:UETIi67q53MR2AWcXfiuqkDkRtnGDLqkBTpCHuJHxtU=\ngithub.com/xordataexchange/crypt v0.0.3-0.20170626215501-b2862e3d0a77/go.mod h1:aYKd//L2LvnjZzWKhF00oedf4jCCReLcmhLdhm1A27Q=\ngithub.com/youmark/pkcs8 v0.0.0-20181117223130-1be2e3e5546d/go.mod h1:rHwXgn7JulP+udvsHwJoVG1YGAP6VLg4y9I5dyZdqmA=\ngithub.com/yuin/goldmark v1.1.25/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\ngithub.com/yuin/goldmark v1.1.27/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\ngithub.com/yuin/goldmark v1.1.32/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\ngithub.com/yuin/goldmark v1.2.1/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\ngithub.com/yuin/goldmark v1.3.5/go.mod h1:mwnBkeHKe2W/ZEtQ+71ViKU8L12m81fl3OWwC1Zlc8k=\ngithub.com/yuin/goldmark v1.4.13/go.mod h1:6yULJ656Px+3vBD8DxQVa3kxgyrAnzto9xy5taEt/CY=\ngithub.com/yusufpapurcu/wmi v1.2.2 h1:KBNDSne4vP5mbSWnJbO+51IMOXJB67QiYCSBrubbPRg=\ngithub.com/yusufpapurcu/wmi v1.2.2/go.mod h1:SBZ9tNy3G9/m5Oi98Zks0QjeHVDvuK0qfxQmPyzfmi0=\ngithub.com/zeebo/assert v1.1.0/go.mod h1:Pq9JiuJQpG8JLJdtkwrJESF0Foym2/D9XMU5ciN/wJ0=\ngithub.com/zeebo/assert v1.3.0 h1:g7C04CbJuIDKNPFHmsk4hwZDO5O+kntRxzaUoNXj+IQ=\ngithub.com/zeebo/blake3 v0.2.3 h1:TFoLXsjeXqRNFxSbk35Dk4YtszE/MQQGK10BH4ptoTg=\ngithub.com/zeebo/blake3 v0.2.3/go.mod h1:mjJjZpnsyIVtVgTOSpJ9vmRE4wgDeyt2HU3qXvvKCaQ=\ngithub.com/zeebo/pcg v1.0.1 h1:lyqfGeWiv4ahac6ttHs+I5hwtH/+1mrhlCtVNQM2kHo=\ngithub.com/zeebo/pcg v1.0.1/go.mod h1:09F0S9iiKrwn9rlI5yjLkmrug154/YRW6KnnXVDM/l4=\ngithub.com/zeebo/xxh3 v1.0.2 h1:xZmwmqxHZA8AI603jOQ0tMqmBr9lPeFwGg6d+xy9DC0=\ngithub.com/zeebo/xxh3 v1.0.2/go.mod h1:5NWz9Sef7zIDm2JHfFlcQvNekmcEl9ekUZQQKCYaDcA=\ngithub.com/zenazn/goji v0.9.0/go.mod h1:7S9M489iMyHBNxwZnk9/EHS098H4/F6TATF2mIxtB1Q=\ngithub.com/zenazn/goji v1.0.1/go.mod h1:7S9M489iMyHBNxwZnk9/EHS098H4/F6TATF2mIxtB1Q=\ngo.etcd.io/bbolt v1.3.2/go.mod h1:IbVyRI1SCnLcuJnV2u8VeU0CEYM7e686BmAb1XKL+uU=\ngo.etcd.io/bbolt v1.3.3/go.mod h1:IbVyRI1SCnLcuJnV2u8VeU0CEYM7e686BmAb1XKL+uU=\ngo.etcd.io/bbolt v1.3.6 h1:/ecaJf0sk1l4l6V4awd65v2C3ILy7MSj+s/x1ADCIMU=\ngo.etcd.io/bbolt v1.3.6/go.mod h1:qXsaaIqmgQH0T+OPdb99Bf+PKfBBQVAdyD6TY9G8XM4=\ngo.etcd.io/etcd v0.0.0-20191023171146-3cf2f69b5738/go.mod h1:dnLIgRNXwCJa5e+c6mIZCrds/GIG4ncV9HhK5PX7jPg=\ngo.etcd.io/etcd v3.3.27+incompatible h1:5hMrpf6REqTHV2LW2OclNpRtxI0k9ZplMemJsMSWju0=\ngo.etcd.io/etcd v3.3.27+incompatible/go.mod h1:yaeTdrJi5lOmYerz05bd8+V7KubZs8YSFZfzsF9A6aI=\ngo.etcd.io/etcd/api/v3 v3.5.0/go.mod h1:cbVKeC6lCfl7j/8jBhAK6aIYO9XOjdptoxU/nLQcPvs=\ngo.etcd.io/etcd/api/v3 v3.5.5 h1:BX4JIbQ7hl7+jL+g+2j5UAr0o1bctCm6/Ct+ArBGkf0=\ngo.etcd.io/etcd/api/v3 v3.5.5/go.mod h1:KFtNaxGDw4Yx/BA4iPPwevUTAuqcsPxzyX8PHydchN8=\ngo.etcd.io/etcd/client/pkg/v3 v3.5.0/go.mod h1:IJHfcCEKxYu1Os13ZdwCwIUTUVGYTSAM3YSwc9/Ac1g=\ngo.etcd.io/etcd/client/pkg/v3 v3.5.5 h1:9S0JUVvmrVl7wCF39iTQthdaaNIiAaQbmK75ogO6GU8=\ngo.etcd.io/etcd/client/pkg/v3 v3.5.5/go.mod h1:ggrwbk069qxpKPq8/FKkQ3Xq9y39kbFR4LnKszpRXeQ=\ngo.etcd.io/etcd/client/v2 v2.305.0/go.mod h1:h9puh54ZTgAKtEbut2oe9P4L/oqKCVB6xsXlzd7alYQ=\ngo.etcd.io/etcd/client/v2 v2.305.5 h1:DktRP60//JJpnPC0VBymAN/7V71GHMdjDCBt4ZPXDjI=\ngo.etcd.io/etcd/client/v2 v2.305.5/go.mod h1:zQjKllfqfBVyVStbt4FaosoX2iYd8fV/GRy/PbowgP4=\ngo.etcd.io/etcd/client/v3 v3.5.5 h1:q++2WTJbUgpQu4B6hCuT7VkdwaTP7Qz6Daak3WzbrlI=\ngo.etcd.io/etcd/client/v3 v3.5.5/go.mod h1:aApjR4WGlSumpnJ2kloS75h6aHUmAyaPLjHMxpc7E7c=\ngo.etcd.io/etcd/pkg/v3 v3.5.5 h1:Ablg7T7OkR+AeeeU32kdVhw/AGDsitkKPl7aW73ssjU=\ngo.etcd.io/etcd/pkg/v3 v3.5.5/go.mod h1:6ksYFxttiUGzC2uxyqiyOEvhAiD0tuIqSZkX3TyPdaE=\ngo.etcd.io/etcd/raft/v3 v3.5.5 h1:Ibz6XyZ60OYyRopu73lLM/P+qco3YtlZMOhnXNS051I=\ngo.etcd.io/etcd/raft/v3 v3.5.5/go.mod h1:76TA48q03g1y1VpTue92jZLr9lIHKUNcYdZOOGyx8rI=\ngo.etcd.io/etcd/server/v3 v3.5.5 h1:jNjYm/9s+f9A9r6+SC4RvNaz6AqixpOvhrFdT0PvIj0=\ngo.etcd.io/etcd/server/v3 v3.5.5/go.mod h1:rZ95vDw/jrvsbj9XpTqPrTAB9/kzchVdhRirySPkUBc=\ngo.mongodb.org/mongo-driver v1.5.1/go.mod h1:gRXCHX4Jo7J0IJ1oDQyUxF7jfy19UfxniMS4xxMmUqw=\ngo.mongodb.org/mongo-driver v1.7.5 h1:ny3p0reEpgsR2cfA5cjgwFZg3Cv/ofFh/8jbhGtz9VI=\ngo.mongodb.org/mongo-driver v1.7.5/go.mod h1:VXEWRZ6URJIkUq2SCAyapmhH0ZLRBP+FT4xhp5Zvxng=\ngo.opencensus.io v0.20.1/go.mod h1:6WKK9ahsWS3RSO+PY9ZHZUfv2irvY6gN279GOPZjmmk=\ngo.opencensus.io v0.20.2/go.mod h1:6WKK9ahsWS3RSO+PY9ZHZUfv2irvY6gN279GOPZjmmk=\ngo.opencensus.io v0.21.0/go.mod h1:mSImk1erAIZhrmZN+AvHh14ztQfjbGwt4TtuofqLduU=\ngo.opencensus.io v0.22.0/go.mod h1:+kGneAE2xo2IficOXnaByMWTGM9T73dGwxeWcUqIpI8=\ngo.opencensus.io v0.22.2/go.mod h1:yxeiOL68Rb0Xd1ddK5vPZ/oVn4vY4Ynel7k9FzqtOIw=\ngo.opencensus.io v0.22.3/go.mod h1:yxeiOL68Rb0Xd1ddK5vPZ/oVn4vY4Ynel7k9FzqtOIw=\ngo.opencensus.io v0.22.4/go.mod h1:yxeiOL68Rb0Xd1ddK5vPZ/oVn4vY4Ynel7k9FzqtOIw=\ngo.opencensus.io v0.22.5/go.mod h1:5pWMHQbX5EPX2/62yrJeAkowc+lfs/XD7Uxpq3pI6kk=\ngo.opencensus.io v0.23.0/go.mod h1:XItmlyltB5F7CS4xOC1DcqMoFqwtC6OG2xF7mCv7P7E=\ngo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc v0.25.0 h1:Wx7nFnvCaissIUZxPkBqDz2963Z+Cl+PkYbDKzTxDqQ=\ngo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc v0.25.0/go.mod h1:E5NNboN0UqSAki0Atn9kVwaN7I+l25gGxDqBueo/74E=\ngo.opentelemetry.io/otel v0.11.0/go.mod h1:G8UCk+KooF2HLkgo8RHX9epABH/aRGYET7gQOqBVdB0=\ngo.opentelemetry.io/otel v1.0.1 h1:4XKyXmfqJLOQ7feyV5DB6gsBFZ0ltB8vLtp6pj4JIcc=\ngo.opentelemetry.io/otel v1.0.1/go.mod h1:OPEOD4jIT2SlZPMmwT6FqZz2C0ZNdQqiWcoK6M0SNFU=\ngo.opentelemetry.io/otel/exporters/otlp/otlptrace v1.0.1 h1:ofMbch7i29qIUf7VtF+r0HRF6ac0SBaPSziSsKp7wkk=\ngo.opentelemetry.io/otel/exporters/otlp/otlptrace v1.0.1/go.mod h1:Kv8liBeVNFkkkbilbgWRpV+wWuu+H5xdOT6HAgd30iw=\ngo.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc v1.0.1 h1:CFMFNoz+CGprjFAFy+RJFrfEe4GBia3RRm2a4fREvCA=\ngo.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc v1.0.1/go.mod h1:xOvWoTOrQjxjW61xtOmD/WKGRYb/P4NzRo3bs65U6Rk=\ngo.opentelemetry.io/otel/sdk v1.0.1 h1:wXxFEWGo7XfXupPwVJvTBOaPBC9FEg0wB8hMNrKk+cA=\ngo.opentelemetry.io/otel/sdk v1.0.1/go.mod h1:HrdXne+BiwsOHYYkBE5ysIcv2bvdZstxzmCQhxTcZkI=\ngo.opentelemetry.io/otel/trace v1.0.1 h1:StTeIH6Q3G4r0Fiw34LTokUFESZgIDUr0qIJ7mKmAfw=\ngo.opentelemetry.io/otel/trace v1.0.1/go.mod h1:5g4i4fKLaX2BQpSBsxw8YYcgKpMMSW3x7ZTuYBr3sUk=\ngo.opentelemetry.io/proto/otlp v0.7.0/go.mod h1:PqfVotwruBrMGOCsRd/89rSnXhoiJIqeYNgFYFoEGnI=\ngo.opentelemetry.io/proto/otlp v0.9.0 h1:C0g6TWmQYvjKRnljRULLWUVJGy8Uvu0NEL/5frY2/t4=\ngo.opentelemetry.io/proto/otlp v0.9.0/go.mod h1:1vKfU9rv61e9EVGthD1zNvUbiwPcimSsOPU9brfSHJg=\ngo.uber.org/atomic v1.3.2/go.mod h1:gD2HeocX3+yG+ygLZcrzQJaqmWj9AIm7n08wl/qW/PE=\ngo.uber.org/atomic v1.4.0/go.mod h1:gD2HeocX3+yG+ygLZcrzQJaqmWj9AIm7n08wl/qW/PE=\ngo.uber.org/atomic v1.5.0/go.mod h1:sABNBOSYdrvTF6hTgEIbc7YasKWGhgEQZyfxyTvoXHQ=\ngo.uber.org/atomic v1.6.0/go.mod h1:sABNBOSYdrvTF6hTgEIbc7YasKWGhgEQZyfxyTvoXHQ=\ngo.uber.org/atomic v1.7.0 h1:ADUqmZGgLDDfbSL9ZmPxKTybcoEYHgpYfELNoN+7hsw=\ngo.uber.org/atomic v1.7.0/go.mod h1:fEN4uk6kAWBTFdckzkM89CLk9XfWZrxpCo0nPH17wJc=\ngo.uber.org/multierr v1.1.0/go.mod h1:wR5kodmAFQ0UK8QlbwjlSNy0Z68gJhDJUG5sjR94q/0=\ngo.uber.org/multierr v1.3.0/go.mod h1:VgVr7evmIr6uPjLBxg28wmKNXyqE9akIJ5XnfpiKl+4=\ngo.uber.org/multierr v1.5.0/go.mod h1:FeouvMocqHpRaaGuG9EjoKcStLC43Zu/fmqdUMPcKYU=\ngo.uber.org/multierr v1.6.0 h1:y6IPFStTAIT5Ytl7/XYmHvzXQ7S3g/IeZW9hyZ5thw4=\ngo.uber.org/multierr v1.6.0/go.mod h1:cdWPpRnG4AhwMwsgIHip0KRBQjJy5kYEpYjJxpXp9iU=\ngo.uber.org/tools v0.0.0-20190618225709-2cfd321de3ee/go.mod h1:vJERXedbb3MVM5f9Ejo0C68/HhF8uaILCdgjnY+goOA=\ngo.uber.org/zap v1.9.1/go.mod h1:vwi/ZaCAaUcBkycHslxD9B2zi4UTXhF60s6SWpuDF0Q=\ngo.uber.org/zap v1.10.0/go.mod h1:vwi/ZaCAaUcBkycHslxD9B2zi4UTXhF60s6SWpuDF0Q=\ngo.uber.org/zap v1.13.0/go.mod h1:zwrFLgMcdUuIBviXEYEH1YKNaOBnKXsx2IPda5bBwHM=\ngo.uber.org/zap v1.17.0 h1:MTjgFu6ZLKvY6Pvaqk97GlxNBuMpV4Hy/3P6tRGlI2U=\ngo.uber.org/zap v1.17.0/go.mod h1:MXVU+bhUf/A7Xi2HNOnopQOrmycQ5Ih87HtOu4q5SSo=\ngolang.org/x/crypto v0.0.0-20180904163835-0709b304e793/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=\ngolang.org/x/crypto v0.0.0-20180910181607-0e37d006457b/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=\ngolang.org/x/crypto v0.0.0-20181029021203-45a5f77698d3/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=\ngolang.org/x/crypto v0.0.0-20190211182817-74369b46fc67/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=\ngolang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\ngolang.org/x/crypto v0.0.0-20190325154230-a5d413f7728c/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\ngolang.org/x/crypto v0.0.0-20190404164418-38d8ce5564a5/go.mod h1:WFFai1msRO1wXaEeE5yQxYXgSfI8pQAWXbQop6sCtWE=\ngolang.org/x/crypto v0.0.0-20190411191339-88737f569e3a/go.mod h1:WFFai1msRO1wXaEeE5yQxYXgSfI8pQAWXbQop6sCtWE=\ngolang.org/x/crypto v0.0.0-20190418165655-df01cb2cc480/go.mod h1:WFFai1msRO1wXaEeE5yQxYXgSfI8pQAWXbQop6sCtWE=\ngolang.org/x/crypto v0.0.0-20190422162423-af44ce270edf/go.mod h1:WFFai1msRO1wXaEeE5yQxYXgSfI8pQAWXbQop6sCtWE=\ngolang.org/x/crypto v0.0.0-20190506204251-e1dfcc566284/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20190510104115-cbcb75029529/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20190605123033-f99c8df09eb5/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20190701094942-4def268fd1a4/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20190820162420-60c769a6c586/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20190911031432-227b76d455e7/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20190923035154-9ee001bba392/go.mod h1:/lpIB1dKB+9EgE3H3cr1v9wB50oz8l4C4h62xy7jSTY=\ngolang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20200302210943-78000ba7a073/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\ngolang.org/x/crypto v0.0.0-20200323165209-0ec3e9974c59/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\ngolang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\ngolang.org/x/crypto v0.0.0-20200820211705-5c72a883971a/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\ngolang.org/x/crypto v0.0.0-20201203163018-be400aefbc4c/go.mod h1:jdWPYTVW3xRLrWPugEBEK3UY2ZEsg3UU495nc5E+M+I=\ngolang.org/x/crypto v0.0.0-20210513164829-c07d793c2f9a/go.mod h1:P+XmwS30IXTQdn5tA2iutPOUgjI07+tq3H3K9MVA1s8=\ngolang.org/x/crypto v0.0.0-20210616213533-5ff15b29337e/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=\ngolang.org/x/crypto v0.0.0-20210711020723-a769d52b0f97/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=\ngolang.org/x/crypto v0.0.0-20210921155107-089bfa567519/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=\ngolang.org/x/crypto v0.0.0-20220411220226-7b82a4e95df4/go.mod h1:IxCIyHEi3zRg3s0A5j5BB6A9Jmi73HwBIUl50j+osU4=\ngolang.org/x/crypto v0.0.0-20220722155217-630584e8d5aa/go.mod h1:IxCIyHEi3zRg3s0A5j5BB6A9Jmi73HwBIUl50j+osU4=\ngolang.org/x/crypto v0.1.0 h1:MDRAIl0xIo9Io2xV565hzXHw3zVseKrJKodhohM5CjU=\ngolang.org/x/crypto v0.1.0/go.mod h1:RecgLatLF4+eUMCP1PoPZQb+cVrJcOPbHkTkbkB9sbw=\ngolang.org/x/exp v0.0.0-20180321215751-8460e604b9de/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20180807140117-3d87b88a115f/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20190121172915-509febef88a4/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20190125153040-c74c464bbbf2/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20190306152737-a1d7652674e8/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20190510132918-efd6b22b2522/go.mod h1:ZjyILWgesfNpC6sMxTJOJm9Kp84zZh5NQWvqDGG3Qr8=\ngolang.org/x/exp v0.0.0-20190829153037-c13cbed26979/go.mod h1:86+5VVa7VpoJ4kLfm080zCjGlMRFzhUhsZKEZO7MGek=\ngolang.org/x/exp v0.0.0-20191030013958-a1ab85dbe136/go.mod h1:JXzH8nQsPlswgeRAPE3MuO9GYsAcnJvJ4vnMwN/5qkY=\ngolang.org/x/exp v0.0.0-20191129062945-2f5052295587/go.mod h1:2RIsYlXP63K8oxa1u096TMicItID8zy7Y6sNkU49FU4=\ngolang.org/x/exp v0.0.0-20191227195350-da58074b4299/go.mod h1:2RIsYlXP63K8oxa1u096TMicItID8zy7Y6sNkU49FU4=\ngolang.org/x/exp v0.0.0-20200119233911-0405dc783f0a/go.mod h1:2RIsYlXP63K8oxa1u096TMicItID8zy7Y6sNkU49FU4=\ngolang.org/x/exp v0.0.0-20200207192155-f17229e696bd/go.mod h1:J/WKrq2StrnmMY6+EHIKF9dgMWnmCNThgcyBT1FY9mM=\ngolang.org/x/exp v0.0.0-20200224162631-6cc2880d07d6/go.mod h1:3jZMyOhIsHpP37uCMkUooju7aAi5cS1Q23tOzKc+0MU=\ngolang.org/x/exp v0.0.0-20200331195152-e8c3332aa8e5/go.mod h1:4M0jN8W1tt0AVLNr8HDosyJCDCDuyL9N9+3m7wDWgKw=\ngolang.org/x/exp v0.0.0-20200901203048-c4f52b2c50aa/go.mod h1:3jZMyOhIsHpP37uCMkUooju7aAi5cS1Q23tOzKc+0MU=\ngolang.org/x/exp v0.0.0-20200908183739-ae8ad444f925/go.mod h1:1phAWC201xIgDyaFpmDeZkgf70Q4Pd/CNqfRtVPtxNw=\ngolang.org/x/exp v0.0.0-20220827204233-334a2380cb91 h1:tnebWN09GYg9OLPss1KXj8txwZc6X6uMr6VFdcGNbHw=\ngolang.org/x/exp v0.0.0-20220827204233-334a2380cb91/go.mod h1:cyybsKvd6eL0RnXn6p/Grxp8F5bW7iYuBgsNCOHpMYE=\ngolang.org/x/image v0.0.0-20180708004352-c73c2afc3b81/go.mod h1:ux5Hcp/YLpHSI86hEcLt0YII63i6oz57MZXIpbrjZUs=\ngolang.org/x/image v0.0.0-20190227222117-0694c2d4d067/go.mod h1:kZ7UVZpmo3dzQBMxlp+ypCbDeSB+sBbTgSJuh5dn5js=\ngolang.org/x/image v0.0.0-20190802002840-cff245a6509b/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\ngolang.org/x/lint v0.0.0-20181026193005-c67002cb31c3/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=\ngolang.org/x/lint v0.0.0-20190227174305-5b3e6a55c961/go.mod h1:wehouNa3lNwaWXcvxsM5YxQ5yQlVC4a0KAMCusXpPoU=\ngolang.org/x/lint v0.0.0-20190301231843-5614ed5bae6f/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=\ngolang.org/x/lint v0.0.0-20190313153728-d0100b6bd8b3/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/lint v0.0.0-20190409202823-959b441ac422/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/lint v0.0.0-20190909230951-414d861bb4ac/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/lint v0.0.0-20190930215403-16217165b5de/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/lint v0.0.0-20191125180803-fdd1cda4f05f/go.mod h1:5qLYkcX4OjUUV8bRuDixDT3tpyyb+LUpUlRWLxfhWrs=\ngolang.org/x/lint v0.0.0-20200130185559-910be7a94367/go.mod h1:3xt1FjdF8hUf6vQPIChWIBhFzV8gjjsPE/fR3IyQdNY=\ngolang.org/x/lint v0.0.0-20200302205851-738671d3881b/go.mod h1:3xt1FjdF8hUf6vQPIChWIBhFzV8gjjsPE/fR3IyQdNY=\ngolang.org/x/lint v0.0.0-20201208152925-83fdc39ff7b5/go.mod h1:3xt1FjdF8hUf6vQPIChWIBhFzV8gjjsPE/fR3IyQdNY=\ngolang.org/x/lint v0.0.0-20210508222113-6edffad5e616/go.mod h1:3xt1FjdF8hUf6vQPIChWIBhFzV8gjjsPE/fR3IyQdNY=\ngolang.org/x/mobile v0.0.0-20190312151609-d3739f865fa6/go.mod h1:z+o9i4GpDbdi3rU15maQ/Ox0txvL9dWGYEHz965HBQE=\ngolang.org/x/mobile v0.0.0-20190719004257-d2bd2a29d028/go.mod h1:E/iHnbuqvinMTCcRqshq8CkpyQDoeVncDDYHnLhea+o=\ngolang.org/x/mod v0.0.0-20190513183733-4bf6d317e70e/go.mod h1:mXi4GBBbnImb6dmsKGUJ2LatrhH/nqhxcFungHvyanc=\ngolang.org/x/mod v0.1.0/go.mod h1:0QHyrYULN0/3qlju5TqG8bIK38QM8yzMo5ekMj3DlcY=\ngolang.org/x/mod v0.1.1-0.20191105210325-c90efee705ee/go.mod h1:QqPTAvyqsEbceGzBzNggFXnrqF1CaUcvgkdR5Ot7KZg=\ngolang.org/x/mod v0.1.1-0.20191107180719-034126e5016b/go.mod h1:QqPTAvyqsEbceGzBzNggFXnrqF1CaUcvgkdR5Ot7KZg=\ngolang.org/x/mod v0.2.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/mod v0.3.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/mod v0.3.1-0.20200828183125-ce943fd02449/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/mod v0.4.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/mod v0.4.1/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/mod v0.4.2/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/mod v0.6.0-dev.0.20220419223038-86c51ed26bb4/go.mod h1:jJ57K6gSWd91VN4djpZkiMVwK6gcyfeH4XE8wZrZaV4=\ngolang.org/x/mod v0.7.0 h1:LapD9S96VoQRhi/GrNTqeBJFrUjs5UHCAtTlgwA5oZA=\ngolang.org/x/mod v0.7.0/go.mod h1:iBbtSCu2XBx23ZKBPSOrRkjjQPZFPuis4dIYUhu/chs=\ngolang.org/x/net v0.0.0-20170114055629-f2499483f923/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20180724234803-3673e40ba225/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20180826012351-8a410e7b638d/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20180906233101-161cd47e91fd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20181023162649-9b4f9f5ad519/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20181114220301-adae6a3d119a/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20181201002055-351d144fa1fc/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20181220203305-927f97764cc3/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190108225652-1e06a53dbb7e/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190125091013-d26f9f9a57f3/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190213061140-3a22650c66bd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190501004415-9ce7a6920f09/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190503192946-f4e77d36d62c/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190522155817-f3200d17e092/go.mod h1:HSz+uSET+XFnRR8LxR5pz3Of3rY3CfYBVs4xY44aLks=\ngolang.org/x/net v0.0.0-20190603091049-60506f45cf65/go.mod h1:HSz+uSET+XFnRR8LxR5pz3Of3rY3CfYBVs4xY44aLks=\ngolang.org/x/net v0.0.0-20190613194153-d28f0bde5980/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20190628185345-da137c7871d7/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20190724013045-ca1201d0de80/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20190813141303-74dc4d7220e7/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20190923162816-aa69164e4478/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20191004110552-13f9640d40b9/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20191209160850-c0dbc17a3553/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200114155413-6afb5195e5aa/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200202094626-16171245cfb2/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200222125558-5a598a2470a0/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200226121028-0de0cce0169b/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200301022130-244492dfa37a/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200324143707-d3edc9973b7e/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200421231249-e086a090c8fd/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200501053045-e0ff5e5a1de5/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200505041828-1ed23360d12c/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200506145744-7e3656a0809f/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200513185701-a91f0712d120/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200520004742-59133d7f0dd7/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200520182314-0ba52f642ac2/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200602114024-627f9648deb9/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200625001655-4c5254603344/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\ngolang.org/x/net v0.0.0-20200707034311-ab3426394381/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\ngolang.org/x/net v0.0.0-20200822124328-c89045814202/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\ngolang.org/x/net v0.0.0-20200904194848-62affa334b73/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\ngolang.org/x/net v0.0.0-20201021035429-f5854403a974/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=\ngolang.org/x/net v0.0.0-20201031054903-ff519b6c9102/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=\ngolang.org/x/net v0.0.0-20201110031124-69a78807bb2b/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=\ngolang.org/x/net v0.0.0-20201202161906-c7110b5ffcbb/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=\ngolang.org/x/net v0.0.0-20201209123823-ac852fbbde11/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=\ngolang.org/x/net v0.0.0-20210119194325-5f4716e94777/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=\ngolang.org/x/net v0.0.0-20210226172049-e18ecbb05110/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=\ngolang.org/x/net v0.0.0-20210316092652-d523dce5a7f4/go.mod h1:RBQZq4jEuRlivfhVLdyRGr576XBO4/greRjx4P4O3yc=\ngolang.org/x/net v0.0.0-20210405180319-a5a99cb37ef4/go.mod h1:p54w0d4576C0XHj96bSt6lcn1PtDYWL6XObtHCRCNQM=\ngolang.org/x/net v0.0.0-20210428140749-89ef3d95e781/go.mod h1:OJAsFXCWl8Ukc7SiCT/9KSuxbyM7479/AVlXFRxuMCk=\ngolang.org/x/net v0.0.0-20210510120150-4163338589ed/go.mod h1:9nx3DQGgdP8bBQD5qxJ1jj9UTztislL4KSBs9R2vV5Y=\ngolang.org/x/net v0.0.0-20210525063256-abc453219eb5/go.mod h1:9nx3DQGgdP8bBQD5qxJ1jj9UTztislL4KSBs9R2vV5Y=\ngolang.org/x/net v0.0.0-20210805182204-aaa1db679c0d/go.mod h1:9nx3DQGgdP8bBQD5qxJ1jj9UTztislL4KSBs9R2vV5Y=\ngolang.org/x/net v0.0.0-20211020060615-d418f374d309/go.mod h1:9nx3DQGgdP8bBQD5qxJ1jj9UTztislL4KSBs9R2vV5Y=\ngolang.org/x/net v0.0.0-20211112202133-69e39bad7dc2/go.mod h1:9nx3DQGgdP8bBQD5qxJ1jj9UTztislL4KSBs9R2vV5Y=\ngolang.org/x/net v0.0.0-20211216030914-fe4d6282115f/go.mod h1:9nx3DQGgdP8bBQD5qxJ1jj9UTztislL4KSBs9R2vV5Y=\ngolang.org/x/net v0.0.0-20220127200216-cd36cc0744dd/go.mod h1:CfG3xpIq0wQ8r1q4Su4UZFWDARRcnwPjda9FqA0JpMk=\ngolang.org/x/net v0.0.0-20220225172249-27dd8689420f/go.mod h1:CfG3xpIq0wQ8r1q4Su4UZFWDARRcnwPjda9FqA0JpMk=\ngolang.org/x/net v0.0.0-20220722155237-a158d28d115b/go.mod h1:XRhObCWvk6IyKnWLug+ECip1KBveYUHfp+8e9klMJ9c=\ngolang.org/x/net v0.0.0-20220826154423-83b083e8dc8b/go.mod h1:YDH+HFinaLZZlnHAfSS6ZXJJ9M9t4Dl22yv3iI2vPwk=\ngolang.org/x/net v0.0.0-20221002022538-bcab6841153b/go.mod h1:YDH+HFinaLZZlnHAfSS6ZXJJ9M9t4Dl22yv3iI2vPwk=\ngolang.org/x/net v0.7.0 h1:rJrUqqhjsgNp7KqAIc25s9pZnjU7TUcSY7HcVZjdn1g=\ngolang.org/x/net v0.7.0/go.mod h1:2Tu9+aMcznHK/AK1HMvgo6xiTLG5rD5rZLDS+rp2Bjs=\ngolang.org/x/oauth2 v0.0.0-20180821212333-d2e6202438be/go.mod h1:N/0e6XlmueqKjAGxoOufVs8QHGRruUQn6yWY3a++T0U=\ngolang.org/x/oauth2 v0.0.0-20190226205417-e64efc72b421/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/oauth2 v0.0.0-20190604053449-0f29369cfe45/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/oauth2 v0.0.0-20191202225959-858c2ad4c8b6/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/oauth2 v0.0.0-20200107190931-bf48bf16ab8d/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/oauth2 v0.0.0-20200902213428-5d25da1a8d43/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\ngolang.org/x/oauth2 v0.0.0-20201109201403-9fd604954f58/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\ngolang.org/x/oauth2 v0.0.0-20201208152858-08078c50e5b5/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\ngolang.org/x/oauth2 v0.0.0-20210218202405-ba52d332ba99/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\ngolang.org/x/oauth2 v0.0.0-20210220000619-9bb904979d93/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\ngolang.org/x/oauth2 v0.0.0-20210313182246-cd4f82c27b84/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\ngolang.org/x/oauth2 v0.0.0-20210402161424-2e8d93401602/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\ngolang.org/x/oauth2 v0.0.0-20210514164344-f6687ab2804c/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\ngolang.org/x/oauth2 v0.0.0-20220223155221-ee480838109b/go.mod h1:DAh4E804XQdzx2j+YRIaUnCqCV2RuMz24cGBJ5QYIrc=\ngolang.org/x/oauth2 v0.0.0-20220608161450-d0670ef3b1eb h1:8tDJ3aechhddbdPAxpycgXHJRMLpk/Ab+aa4OgdN5/g=\ngolang.org/x/oauth2 v0.0.0-20220608161450-d0670ef3b1eb/go.mod h1:jaDAt6Dkxork7LmZnYtzbRWj0W47D86a3TGe0YHBvmE=\ngolang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20181108010431-42b317875d0f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190227155943-e225da77a7e6/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190412183630-56d357773e84/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190911185100-cd5d95a43a6e/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20200317015054-43a5402ce75a/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20200625203802-6e8e738ad208/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20201020160332-67f06af15bc9/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20201207232520-09787c993a3a/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20210220032951-036812b2e83c/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20220722155255-886fb9371eb4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20220929204114-8fcdb60fdcc0/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.1.0 h1:wsuoTGHzEhffawBOhz5CYhcrV4IdKZbEyZjBMuTp12o=\ngolang.org/x/sync v0.1.0/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sys v0.0.0-20170830134202-bb24a47a89ea/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20180823144017-11551d06cbcc/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20180830151530-49385e6e1522/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20180905080454-ebe1bf3edb33/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20180909124046-d0be0721c37e/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20181026203630-95b1ffbd15a5/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20181107165924-66b7b1311ac8/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20181116152217-5ac8a444bdc5/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20181122145206-62eef0e2fa9b/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190129075346-302c3dd5f1cc/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190209173611-3b5209105503/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190222072716-a9d3bda3a223/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190312061237-fead79001313/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190403152447-81d4e9dc473e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190419153524-e8e3143a4f4a/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190422165155-953cdadca894/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190502145724-3ef323f4f1fd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190507160741-ecd444e8653b/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190531175056-4c3a928424d2/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190606165138-5da285871e9c/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190624142023-c5567b49c5d0/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190726091711-fc99dfbffb4e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190813064441-fde4db37ae7a/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190826190057-c7b8b68b1456/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190904154756-749cb33beabd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190916202348-b4ddaad3f8a3/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190922100055-0a153f010e69/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190924154521-2837fb4f24fe/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191001151750-bb3f8db39f24/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191005200804-aed5e4c7ecf9/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191008105621-543471e840be/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191010194322-b09406accb47/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191026070338-33540a1f6037/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191120155948-bd437916bb0e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191204072324-ce4227a45e2e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191220142924-d4481acd189f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191228213918-04cbcbbfeed8/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200106162015-b016eb3dc98e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200113162924-86b910548bc1/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200116001909-b77594299b42/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200122134326-e047566fdf82/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200202164722-d101bd2416d5/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200212091648-12a6c2dcc1e4/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200223170610-d5e6a3e2c0ae/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200302150141-5c8b2ff67527/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200323222414-85ca7c5b95cd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200331124033-c3d80250170d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200420163511-1957bb5e6d1f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200501052902-10377860bb8e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200511232937-7e40ca221e25/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200515095857-1151b9dac4a9/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200519105757-fe76b779f299/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200523222454-059865788121/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200615200032-f1bc736245b1/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200625212154-ddb9806d33ae/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200803210538-64077c9b5642/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200826173525-f9321e4c35a6/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200905004654-be1d3432aa8f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200923182605-d9f96fdee20d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200930185726-fdedc70b468f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20201119102817-f84b799fce68/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20201201145000-ef89a241ccb3/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20201204225414-ed752295db88/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210104204734-6f8348627aad/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210112080510-489259a85091/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210119212857-b64e53b001e4/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210124154548-22da62e12c0c/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210220050731-9a76102bfb43/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210305230114-8fe3ee5dd75b/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210315160823-c6e025ad8005/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210320140829-1e4c9ba3b0c4/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210330210617-4fbd30eecc44/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210403161142-5e06dd20ab57/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210423082822-04245dca01da/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210423185535-09eb48e85fd7/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210510120138-977fb7262007/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20210514084401-e8d321eab015/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20210603081109-ebe580a85c40/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20210615035016-665e8c7367d1/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20210630005230-0f9fa26af87c/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20210809222454-d867a43fc93e/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20210927094055-39ccf1dd6fa6/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20211007075335-d3039528d8ac/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20211103235746-7861aae1554b/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20211216021012-1d35b9e2eb4e/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220114195835-da31bd327af9/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220128215802-99c3d69c2c27/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220227234510-4e6760a101f9/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220310020820-b874c991c1a5/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220520151302-bc2c85ada10a/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220715151400-c0bba94af5f8/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220722155257-8c9f86f7a55f/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220728004956-3c1f35247d10/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220811171246-fbc7d0a398ab/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.5.0 h1:MUK/U/4lj1t1oPg0HfuXDN/Z1wv31ZJ/YcPiGccS4DU=\ngolang.org/x/sys v0.5.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/term v0.0.0-20201117132131-f5c789dd3221/go.mod h1:Nr5EML6q2oocZ2LXRh80K7BxOlk5/8JxuGnuhpl+muw=\ngolang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=\ngolang.org/x/term v0.0.0-20210927222741-03fcf44c2211/go.mod h1:jbD1KX2456YbFQfuXm/mYQcufACuNUgVhRMnK/tPxf8=\ngolang.org/x/term v0.0.0-20220722155259-a9ba230a4035/go.mod h1:jbD1KX2456YbFQfuXm/mYQcufACuNUgVhRMnK/tPxf8=\ngolang.org/x/text v0.0.0-20160726164857-2910a502d2bf/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.0.0-20170915032832-14c0d48ead0c/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.1-0.20180807135948-17ff2d5776d2/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=\ngolang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.4/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.5/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.6/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.7/go.mod h1:u+2+/6zg+i71rQMx5EYifcz6MCKuco9NR6JIITiCfzQ=\ngolang.org/x/text v0.7.0 h1:4BRB4x83lYWy72KwLD/qYDuTu7q9PjSagHvijDw7cLo=\ngolang.org/x/text v0.7.0/go.mod h1:mrYo+phRRbMaCq/xk9113O4dZlRixOauAjOtrjsXDZ8=\ngolang.org/x/time v0.0.0-20180412165947-fbb02b2291d2/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\ngolang.org/x/time v0.0.0-20181108054448-85acf8d2951c/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\ngolang.org/x/time v0.0.0-20190308202827-9d24e82272b4/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\ngolang.org/x/time v0.0.0-20191024005414-555d28b269f0/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\ngolang.org/x/time v0.0.0-20200416051211-89c76fbcd5d1/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\ngolang.org/x/time v0.0.0-20201208040808-7e3f01d25324/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\ngolang.org/x/time v0.0.0-20210220033141-f8bda1e9f3ba/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\ngolang.org/x/time v0.0.0-20211116232009-f0f3c7e86c11 h1:GZokNIeuVkl3aZHJchRrr13WCsols02MLUcz1U9is6M=\ngolang.org/x/time v0.0.0-20211116232009-f0f3c7e86c11/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\ngolang.org/x/tools v0.0.0-20180221164845-07fd8470d635/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20180525024113-a5b4c53f6e8b/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20180828015842-6cd1fcedba52/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20181011042414-1f849cf54d09/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20181030221726-6c7e314b6563/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20190114222345-bf090417da8b/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20190206041539-40960b6deb8e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20190226205152-f727befe758c/go.mod h1:9Yl7xja0Znq3iFh3HoIrodX9oNMXvdceNzlUR8zjMvY=\ngolang.org/x/tools v0.0.0-20190311212946-11955173bddd/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190312151545-0bb0c0a6e846/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190312170243-e65039ee4138/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190328211700-ab21143f2384/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190329151228-23e29df326fe/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190416151739-9c9e1878f421/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190420181800-aa740d480789/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190425150028-36563e24a262/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\ngolang.org/x/tools v0.0.0-20190425163242-31fd60d6bfdc/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\ngolang.org/x/tools v0.0.0-20190506145303-2d16b83fe98c/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\ngolang.org/x/tools v0.0.0-20190524140312-2c0ae7006135/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\ngolang.org/x/tools v0.0.0-20190531172133-b3315ee88b7d/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\ngolang.org/x/tools v0.0.0-20190606124116-d0a3d012864b/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\ngolang.org/x/tools v0.0.0-20190621195816-6e04913cbbac/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\ngolang.org/x/tools v0.0.0-20190628153133-6cdbf07be9d0/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\ngolang.org/x/tools v0.0.0-20190816200558-6889da9d5479/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20190823170909-c4a336ef6a2f/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20190907020128-2ca718005c18/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20190911174233-4f2ddba30aff/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191012152004-8de300cfc20a/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191029041327-9cc4af7d6b2c/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191029190741-b9c20aec41a5/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191112195655-aa38f8e97acc/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191113191852-77e3bb0ad9e7/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191115202509-3a792d9c32b2/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191125144606-a911d9008d1f/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191130070609-6e064ea0cf2d/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191216173652-a0e659d51361/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20191227053925-7b8e75db28f4/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200103221440-774c71fcf114/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200117161641-43d50277825c/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200122220014-bf1340f18c4a/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200130002326-2f3ba24bd6e7/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200204074204-1cc6d1ef6c74/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200207183749-b753a1ba74fa/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200212150539-ea181f53ac56/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200224181240-023911ca70b2/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200227222343-706bc42d1f0d/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200304193943-95d2e580d8eb/go.mod h1:o4KQGtdN14AW+yjsvvwRTJJuXz8XRtIHtEnmAXLyFUw=\ngolang.org/x/tools v0.0.0-20200312045724-11d5b4c81c7d/go.mod h1:o4KQGtdN14AW+yjsvvwRTJJuXz8XRtIHtEnmAXLyFUw=\ngolang.org/x/tools v0.0.0-20200331025713-a30bf2db82d4/go.mod h1:Sl4aGygMT6LrqrWclx+PTx3U+LnKx/seiNR+3G19Ar8=\ngolang.org/x/tools v0.0.0-20200501065659-ab2804fb9c9d/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20200505023115-26f46d2f7ef8/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20200512131952-2bc93b1c0c88/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20200515010526-7d3b6ebf133d/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20200527183253-8e7acdbce89d/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20200618134242-20370b0cb4b2/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20200619180055-7c47624df98f/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20200729194436-6467de6f59a7/go.mod h1:njjCfa9FT2d7l9Bc6FUM5FLjQPp3cFF28FI3qnDFljA=\ngolang.org/x/tools v0.0.0-20200804011535-6c149bb5ef0d/go.mod h1:njjCfa9FT2d7l9Bc6FUM5FLjQPp3cFF28FI3qnDFljA=\ngolang.org/x/tools v0.0.0-20200825202427-b303f430e36d/go.mod h1:njjCfa9FT2d7l9Bc6FUM5FLjQPp3cFF28FI3qnDFljA=\ngolang.org/x/tools v0.0.0-20200904185747-39188db58858/go.mod h1:Cj7w3i3Rnn0Xh82ur9kSqwfTHTeVxaDqrfMjpcNT6bE=\ngolang.org/x/tools v0.0.0-20201110124207-079ba7bd75cd/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=\ngolang.org/x/tools v0.0.0-20201201161351-ac6f37ff4c2a/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=\ngolang.org/x/tools v0.0.0-20201208233053-a543418bbed2/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=\ngolang.org/x/tools v0.0.0-20201224043029-2b0845dc783e/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=\ngolang.org/x/tools v0.0.0-20210105154028-b0ab187a4818/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=\ngolang.org/x/tools v0.0.0-20210106214847-113979e3529a/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=\ngolang.org/x/tools v0.1.0/go.mod h1:xkSsbof2nBLbhDlRMhhhyNLN/zl3eTqcnHD5viDpcZ0=\ngolang.org/x/tools v0.1.1/go.mod h1:o0xws9oXOQQZyjljx8fwUC0k7L1pTE6eaCbjGeHmOkk=\ngolang.org/x/tools v0.1.2/go.mod h1:o0xws9oXOQQZyjljx8fwUC0k7L1pTE6eaCbjGeHmOkk=\ngolang.org/x/tools v0.1.12/go.mod h1:hNGJHUnrk76NpqgfD5Aqm5Crs+Hm0VOH/i9J2+nxYbc=\ngolang.org/x/tools v0.3.0 h1:SrNbZl6ECOS1qFzgTdQfWXZM9XBkiA6tkFrH9YSTPHM=\ngolang.org/x/tools v0.3.0/go.mod h1:/rWhSS2+zyEVwoJf8YAX6L2f0ntZ7Kn/mGgAWcipA5k=\ngolang.org/x/xerrors v0.0.0-20190410155217-1f06c39b4373/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20190513163551-3ee3066db522/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191011141410-1b5146add898/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20220609144429-65e65417b02f h1:uF6paiQQebLeSXkrTqHqz0MXhXXS1KgF41eUdBNvxK0=\ngolang.org/x/xerrors v0.0.0-20220609144429-65e65417b02f/go.mod h1:K8+ghG5WaK9qNqU5K3HdILfMLy1f3aNYFI/wnl100a8=\ngonum.org/v1/gonum v0.0.0-20180816165407-929014505bf4/go.mod h1:Y+Yx5eoAFn32cQvJDxZx5Dpnq+c3wtXuadVZAcxbbBo=\ngonum.org/v1/gonum v0.8.2/go.mod h1:oe/vMfY3deqTw+1EZJhuvEW2iwGF1bW9wwu7XCu0+v0=\ngonum.org/v1/gonum v0.11.0 h1:f1IJhK4Km5tBJmaiJXtk/PkL4cdVX6J+tGiM187uT5E=\ngonum.org/v1/gonum v0.11.0/go.mod h1:fSG4YDCxxUZQJ7rKsQrj0gMOg00Il0Z96/qMA4bVQhA=\ngonum.org/v1/netlib v0.0.0-20190313105609-8cb42192e0e0/go.mod h1:wa6Ws7BG/ESfp6dHfk7C6KdzKA7wR7u/rKwOGE66zvw=\ngonum.org/v1/plot v0.0.0-20190515093506-e2840ee46a6b/go.mod h1:Wt8AAjI+ypCyYX3nZBvf6cAIx93T+c/OS2HFAYskSZc=\ngoogle.golang.org/api v0.3.1/go.mod h1:6wY9I6uQWHQ8EM57III9mq/AjF+i8G65rmVagqKMtkk=\ngoogle.golang.org/api v0.4.0/go.mod h1:8k5glujaEP+g9n7WNsDg8QP6cUVNI86fCNMcbazEtwE=\ngoogle.golang.org/api v0.7.0/go.mod h1:WtwebWUNSVBH/HAw79HIFXZNqEvBhG+Ra+ax0hx3E3M=\ngoogle.golang.org/api v0.8.0/go.mod h1:o4eAsZoiT+ibD93RtjEohWalFOjRDx6CVaqeizhEnKg=\ngoogle.golang.org/api v0.9.0/go.mod h1:o4eAsZoiT+ibD93RtjEohWalFOjRDx6CVaqeizhEnKg=\ngoogle.golang.org/api v0.13.0/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=\ngoogle.golang.org/api v0.14.0/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=\ngoogle.golang.org/api v0.15.0/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=\ngoogle.golang.org/api v0.17.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\ngoogle.golang.org/api v0.18.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\ngoogle.golang.org/api v0.19.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\ngoogle.golang.org/api v0.20.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\ngoogle.golang.org/api v0.22.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\ngoogle.golang.org/api v0.24.0/go.mod h1:lIXQywCXRcnZPGlsd8NbLnOjtAoL6em04bJ9+z0MncE=\ngoogle.golang.org/api v0.25.0/go.mod h1:lIXQywCXRcnZPGlsd8NbLnOjtAoL6em04bJ9+z0MncE=\ngoogle.golang.org/api v0.28.0/go.mod h1:lIXQywCXRcnZPGlsd8NbLnOjtAoL6em04bJ9+z0MncE=\ngoogle.golang.org/api v0.29.0/go.mod h1:Lcubydp8VUV7KeIHD9z2Bys/sm/vGKnG1UHuDBSrHWM=\ngoogle.golang.org/api v0.30.0/go.mod h1:QGmEvQ87FHZNiUVJkT14jQNYJ4ZJjdRF23ZXz5138Fc=\ngoogle.golang.org/api v0.35.0/go.mod h1:/XrVsuzM0rZmrsbjJutiuftIzeuTQcEeaYcSk/mQ1dg=\ngoogle.golang.org/api v0.36.0/go.mod h1:+z5ficQTmoYpPn8LCUNVpK5I7hwkpjbcgqA7I34qYtE=\ngoogle.golang.org/api v0.40.0/go.mod h1:fYKFpnQN0DsDSKRVRcQSDQNtqWPfM9i+zNPxepjRCQ8=\ngoogle.golang.org/api v0.41.0/go.mod h1:RkxM5lITDfTzmyKFPt+wGrCJbVfniCr2ool8kTBzRTU=\ngoogle.golang.org/api v0.43.0/go.mod h1:nQsDGjRXMo4lvh5hP0TKqF244gqhGcr/YSIykhUk/94=\ngoogle.golang.org/api v0.44.0/go.mod h1:EBOGZqzyhtvMDoxwS97ctnh0zUmYY6CxqXsc1AvkYD8=\ngoogle.golang.org/appengine v1.1.0/go.mod h1:EbEs0AVv82hx2wNQdGPgUI5lhzA/G0D9YwlJXL52JkM=\ngoogle.golang.org/appengine v1.2.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\ngoogle.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\ngoogle.golang.org/appengine v1.5.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\ngoogle.golang.org/appengine v1.6.1/go.mod h1:i06prIuMbXzDqacNJfV5OdTW448YApPu5ww/cMBSeb0=\ngoogle.golang.org/appengine v1.6.5/go.mod h1:8WjMMxjGQR8xUklV/ARdw2HLXBOI7O7uCIDZVag1xfc=\ngoogle.golang.org/appengine v1.6.6/go.mod h1:8WjMMxjGQR8xUklV/ARdw2HLXBOI7O7uCIDZVag1xfc=\ngoogle.golang.org/appengine v1.6.7 h1:FZR1q0exgwxzPzp/aF+VccGrSfxfPpkBqjIIEq3ru6c=\ngoogle.golang.org/appengine v1.6.7/go.mod h1:8WjMMxjGQR8xUklV/ARdw2HLXBOI7O7uCIDZVag1xfc=\ngoogle.golang.org/genproto v0.0.0-20180817151627-c66870c02cf8/go.mod h1:JiN7NxoALGmiZfu7CAH4rXhgtRTLTxftemlI0sWmxmc=\ngoogle.golang.org/genproto v0.0.0-20190307195333-5fe7a883aa19/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\ngoogle.golang.org/genproto v0.0.0-20190418145605-e7d98fc518a7/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\ngoogle.golang.org/genproto v0.0.0-20190425155659-357c62f0e4bb/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\ngoogle.golang.org/genproto v0.0.0-20190502173448-54afdca5d873/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\ngoogle.golang.org/genproto v0.0.0-20190530194941-fb225487d101/go.mod h1:z3L6/3dTEVtUr6QSP8miRzeRqwQOioJ9I66odjN4I7s=\ngoogle.golang.org/genproto v0.0.0-20190801165951-fa694d86fc64/go.mod h1:DMBHOl98Agz4BDEuKkezgsaosCRResVns1a3J2ZsMNc=\ngoogle.golang.org/genproto v0.0.0-20190819201941-24fa4b261c55/go.mod h1:DMBHOl98Agz4BDEuKkezgsaosCRResVns1a3J2ZsMNc=\ngoogle.golang.org/genproto v0.0.0-20190911173649-1774047e7e51/go.mod h1:IbNlFCBrqXvoKpeg0TB2l7cyZUmoaFKYIwrEpbDKLA8=\ngoogle.golang.org/genproto v0.0.0-20191108220845-16a3f7862a1a/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20191115194625-c23dd37a84c9/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20191216164720-4f79533eabd1/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20191230161307-f3c370f40bfb/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20200115191322-ca5a22157cba/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20200122232147-0452cf42e150/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20200204135345-fa8e72b47b90/go.mod h1:GmwEX6Z4W5gMy59cAlVYjN9JhxgbQH6Gn+gFDQe2lzA=\ngoogle.golang.org/genproto v0.0.0-20200212174721-66ed5ce911ce/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200224152610-e50cd9704f63/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200228133532-8c2c7df3a383/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200305110556-506484158171/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200312145019-da6875a35672/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200331122359-1ee6d9798940/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200423170343-7949de9c1215/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200430143042-b979b6f78d84/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200511104702-f5ebc3bea380/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200513103714-09dca8ec2884/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200515170657-fc4c6c6a6587/go.mod h1:YsZOwe1myG/8QRHRsmBRE1LrgQY60beZKjly0O1fX9U=\ngoogle.golang.org/genproto v0.0.0-20200526211855-cb27e3aa2013/go.mod h1:NbSheEEYHJ7i3ixzK3sjbqSGDJWnxyFXZblF3eUsNvo=\ngoogle.golang.org/genproto v0.0.0-20200528110217-3d3490e7e671/go.mod h1:jDfRM7FcilCzHH/e9qn6dsT145K34l5v+OpcnNgKAAA=\ngoogle.golang.org/genproto v0.0.0-20200618031413-b414f8b61790/go.mod h1:jDfRM7FcilCzHH/e9qn6dsT145K34l5v+OpcnNgKAAA=\ngoogle.golang.org/genproto v0.0.0-20200726014623-da3ae01ef02d/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20200729003335-053ba62fc06f/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20200804131852-c06518451d9c/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20200825200019-8632dd797987/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20200904004341-0bd0a958aa1d/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20201109203340-2640f1f9cdfb/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20201201144952-b05cb90ed32e/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20201210142538-e3217bee35cc/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20201214200347-8c77b98c765d/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20210126160654-44e461bb6506/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20210222152913-aa3ee6e6a81c/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20210303154014-9728d6b83eeb/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20210310155132-4ce2db91004e/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20210319143718-93e7006c17a6/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20210402141018-6c239bbf2bb1/go.mod h1:9lPAdzaEmUacj36I+k7YKbEc5CXzPIeORRgDAUOu28A=\ngoogle.golang.org/genproto v0.0.0-20210602131652-f16073e35f0c/go.mod h1:UODoCrxHCcBojKKwX1terBiRUaqAsFqJiF615XL43r0=\ngoogle.golang.org/genproto v0.0.0-20220503193339-ba3ae3f07e29 h1:DJUvgAPiJWeMBiT+RzBVcJGQN7bAEWS5UEoMshES9xs=\ngoogle.golang.org/genproto v0.0.0-20220503193339-ba3ae3f07e29/go.mod h1:RAyBrSAP7Fh3Nc84ghnVLDPuV51xc9agzmm4Ph6i0Q4=\ngoogle.golang.org/grpc v1.14.0/go.mod h1:yo6s7OP7yaDglbqo1J04qKzAhqBH6lvTonzMVmEdcZw=\ngoogle.golang.org/grpc v1.17.0/go.mod h1:6QZJwpn2B+Zp71q/5VxRsJ6NXXVCE5NRUHRo+f3cWCs=\ngoogle.golang.org/grpc v1.19.0/go.mod h1:mqu4LbDTu4XGKhr4mRzUsmM4RtVoemTSY81AxZiDr8c=\ngoogle.golang.org/grpc v1.20.0/go.mod h1:chYK+tFQF0nDUGJgXMSgLCQk3phJEuONr2DCgLDdAQM=\ngoogle.golang.org/grpc v1.20.1/go.mod h1:10oTOabMzJvdu6/UiuZezV6QK5dSlG84ov/aaiqXj38=\ngoogle.golang.org/grpc v1.21.0/go.mod h1:oYelfM1adQP15Ek0mdvEgi9Df8B9CZIaU1084ijfRaM=\ngoogle.golang.org/grpc v1.21.1/go.mod h1:oYelfM1adQP15Ek0mdvEgi9Df8B9CZIaU1084ijfRaM=\ngoogle.golang.org/grpc v1.22.0/go.mod h1:Y5yQAOtifL1yxbo5wqy6BxZv8vAUGQwXBOALyacEbxg=\ngoogle.golang.org/grpc v1.22.1/go.mod h1:Y5yQAOtifL1yxbo5wqy6BxZv8vAUGQwXBOALyacEbxg=\ngoogle.golang.org/grpc v1.23.0/go.mod h1:Y5yQAOtifL1yxbo5wqy6BxZv8vAUGQwXBOALyacEbxg=\ngoogle.golang.org/grpc v1.23.1/go.mod h1:Y5yQAOtifL1yxbo5wqy6BxZv8vAUGQwXBOALyacEbxg=\ngoogle.golang.org/grpc v1.25.1/go.mod h1:c3i+UQWmh7LiEpx4sFZnkU36qjEYZ0imhYfXVyQciAY=\ngoogle.golang.org/grpc v1.26.0/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=\ngoogle.golang.org/grpc v1.27.0/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=\ngoogle.golang.org/grpc v1.27.1/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=\ngoogle.golang.org/grpc v1.28.0/go.mod h1:rpkK4SK4GF4Ach/+MFLZUBavHOvF2JJB5uozKKal+60=\ngoogle.golang.org/grpc v1.29.1/go.mod h1:itym6AZVZYACWQqET3MqgPpjcuV5QH3BxFS3IjizoKk=\ngoogle.golang.org/grpc v1.30.0/go.mod h1:N36X2cJ7JwdamYAgDz+s+rVMFjt3numwzf/HckM8pak=\ngoogle.golang.org/grpc v1.31.0/go.mod h1:N36X2cJ7JwdamYAgDz+s+rVMFjt3numwzf/HckM8pak=\ngoogle.golang.org/grpc v1.31.1/go.mod h1:N36X2cJ7JwdamYAgDz+s+rVMFjt3numwzf/HckM8pak=\ngoogle.golang.org/grpc v1.32.0/go.mod h1:N36X2cJ7JwdamYAgDz+s+rVMFjt3numwzf/HckM8pak=\ngoogle.golang.org/grpc v1.33.1/go.mod h1:fr5YgcSWrqhRRxogOsw7RzIpsmvOZ6IcH4kBYTpR3n0=\ngoogle.golang.org/grpc v1.33.2/go.mod h1:JMHMWHQWaTccqQQlmk3MJZS+GWXOdAesneDmEnv2fbc=\ngoogle.golang.org/grpc v1.34.0/go.mod h1:WotjhfgOW/POjDeRt8vscBtXq+2VjORFy659qA51WJ8=\ngoogle.golang.org/grpc v1.35.0/go.mod h1:qjiiYl8FncCW8feJPdyg3v6XW24KsRHe+dy9BAGRRjU=\ngoogle.golang.org/grpc v1.36.0/go.mod h1:qjiiYl8FncCW8feJPdyg3v6XW24KsRHe+dy9BAGRRjU=\ngoogle.golang.org/grpc v1.36.1/go.mod h1:qjiiYl8FncCW8feJPdyg3v6XW24KsRHe+dy9BAGRRjU=\ngoogle.golang.org/grpc v1.37.1/go.mod h1:NREThFqKR1f3iQ6oBuvc5LadQuXVGo9rkm5ZGrQdJfM=\ngoogle.golang.org/grpc v1.38.0/go.mod h1:NREThFqKR1f3iQ6oBuvc5LadQuXVGo9rkm5ZGrQdJfM=\ngoogle.golang.org/grpc v1.40.0/go.mod h1:ogyxbiOoUXAkP+4+xa6PZSE9DZgIHtSpzjDTB9KAK34=\ngoogle.golang.org/grpc v1.41.0/go.mod h1:U3l9uK9J0sini8mHphKoXyaqDA/8VyGnDee1zzIUK6k=\ngoogle.golang.org/grpc v1.46.0/go.mod h1:vN9eftEi1UMyUsIF80+uQXhHjbXYbm0uXoFCACuMGWk=\ngoogle.golang.org/grpc v1.49.0 h1:WTLtQzmQori5FUH25Pq4WT22oCsv8USpQ+F6rqtsmxw=\ngoogle.golang.org/grpc v1.49.0/go.mod h1:ZgQEeidpAuNRZ8iRrlBKXZQP1ghovWIVhdJRyCDK+GI=\ngoogle.golang.org/protobuf v0.0.0-20200109180630-ec00e32a8dfd/go.mod h1:DFci5gLYBciE7Vtevhsrf46CRTquxDuWsQurQQe4oz8=\ngoogle.golang.org/protobuf v0.0.0-20200221191635-4d8936d0db64/go.mod h1:kwYJMbMJ01Woi6D6+Kah6886xMZcty6N08ah7+eCXa0=\ngoogle.golang.org/protobuf v0.0.0-20200228230310-ab0ca4ff8a60/go.mod h1:cfTl7dwQJ+fmap5saPgwCLgHXTUD7jkjRqWcaiX5VyM=\ngoogle.golang.org/protobuf v1.20.1-0.20200309200217-e05f789c0967/go.mod h1:A+miEFZTKqfCUM6K7xSMQL9OKL/b6hQv+e19PK+JZNE=\ngoogle.golang.org/protobuf v1.21.0/go.mod h1:47Nbq4nVaFHyn7ilMalzfO3qCViNmqZ2kzikPIcrTAo=\ngoogle.golang.org/protobuf v1.22.0/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\ngoogle.golang.org/protobuf v1.23.0/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\ngoogle.golang.org/protobuf v1.23.1-0.20200526195155-81db48ad09cc/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\ngoogle.golang.org/protobuf v1.24.0/go.mod h1:r/3tXBNzIEhYS9I1OUVjXDlt8tc493IdKGjtUeSXeh4=\ngoogle.golang.org/protobuf v1.25.0/go.mod h1:9JNX74DMeImyA3h4bdi1ymwjUzf21/xIlbajtzgsN7c=\ngoogle.golang.org/protobuf v1.26.0-rc.1/go.mod h1:jlhhOSvTdKEhbULTjvd4ARK9grFBp09yW+WbY/TyQbw=\ngoogle.golang.org/protobuf v1.26.0/go.mod h1:9q0QmTI4eRPtz6boOQmLYwt+qCgq0jsYwAQnmE0givc=\ngoogle.golang.org/protobuf v1.27.1/go.mod h1:9q0QmTI4eRPtz6boOQmLYwt+qCgq0jsYwAQnmE0givc=\ngoogle.golang.org/protobuf v1.28.0/go.mod h1:HV8QOd/L58Z+nl8r43ehVNZIU/HEI6OcFqwMG9pJV4I=\ngoogle.golang.org/protobuf v1.28.1 h1:d0NfwRgPtno5B1Wa6L2DAG+KivqkdutMf1UhdNx175w=\ngoogle.golang.org/protobuf v1.28.1/go.mod h1:HV8QOd/L58Z+nl8r43ehVNZIU/HEI6OcFqwMG9pJV4I=\ngopkg.in/DataDog/dd-trace-go.v1 v1.38.1 h1:nAKgcpJLXRHF56cKCP3bN8gTTQmmNAZFEblbyGKhKTo=\ngopkg.in/DataDog/dd-trace-go.v1 v1.38.1/go.mod h1:GBhK4yaMJ1h329ivtKAqRNe1EZ944UnZwtz5lh7CnJc=\ngopkg.in/alecthomas/kingpin.v2 v2.2.6/go.mod h1:FMv+mEhP44yOT+4EoQTLFTRgOQ1FBLkstjWtayDeSgw=\ngopkg.in/avro.v0 v0.0.0-20171217001914-a730b5802183 h1:PGIdqvwfpMUyUP+QAlAnKTSWQ671SmYjoou2/5j7HXk=\ngopkg.in/avro.v0 v0.0.0-20171217001914-a730b5802183/go.mod h1:FvqrFXt+jCsyQibeRv4xxEJBL5iG2DDW5aeJwzDiq4A=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20200227125254-8fa46927fb4f/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=\ngopkg.in/cheggaaa/pb.v1 v1.0.25/go.mod h1:V/YB90LKu/1FcN3WVnfiiE5oMCibMjukxqG/qStrOgw=\ngopkg.in/errgo.v1 v1.0.0/go.mod h1:CxwszS/Xz1C49Ucd2i6Zil5UToP1EmyrFhKaMVbg1mk=\ngopkg.in/errgo.v2 v2.1.0/go.mod h1:hNsd1EY+bozCKY1Ytp96fpM3vjJbqLJn88ws8XvfDNI=\ngopkg.in/fsnotify.v1 v1.4.7/go.mod h1:Tz8NjZHkW78fSQdbUxIjBTcgA1z1m8ZHf0WmKUhAMys=\ngopkg.in/gcfg.v1 v1.2.3/go.mod h1:yesOnuUOFQAhST5vPY4nbZsb/huCgGGXlipJsBn0b3o=\ngopkg.in/httprequest.v1 v1.2.1/go.mod h1:x2Otw96yda5+8+6ZeWwHIJTFkEHWP/qP8pJOzqEtWPM=\ngopkg.in/inconshreveable/log15.v2 v2.0.0-20180818164646-67afb5ed74ec/go.mod h1:aPpfJ7XW+gOuirDoZ8gHhLh3kZ1B08FtV2bbmy7Jv3s=\ngopkg.in/inf.v0 v0.9.1/go.mod h1:cWUDdTG/fYaXco+Dcufb5Vnc6Gp2YChqWtbxRZE0mXw=\ngopkg.in/ini.v1 v1.51.0/go.mod h1:pNLf8WUiyNEtQjuu5G5vTm06TEv9tsIgeAvK8hOrP4k=\ngopkg.in/ini.v1 v1.62.0 h1:duBzk771uxoUuOlyRLkHsygud9+5lrlGjdFBb4mSKDU=\ngopkg.in/ini.v1 v1.62.0/go.mod h1:pNLf8WUiyNEtQjuu5G5vTm06TEv9tsIgeAvK8hOrP4k=\ngopkg.in/jinzhu/gorm.v1 v1.9.1/go.mod h1:56JJPUzbikvTVnoyP1nppSkbJ2L8sunqTBDY2fDrmFg=\ngopkg.in/mgo.v2 v2.0.0-20190816093944-a6b53ec6cb22/go.mod h1:yeKp02qBN3iKW1OzL3MGk2IdtZzaj7SFntXj72NppTA=\ngopkg.in/natefinch/lumberjack.v2 v2.0.0 h1:1Lc07Kr7qY4U2YPouBjpCLxpiyxIVoxqXgkXLknAOE8=\ngopkg.in/natefinch/lumberjack.v2 v2.0.0/go.mod h1:l0ndWWf7gzL7RNwBG7wST/UCcT4T24xpD6X8LsfU/+k=\ngopkg.in/olivere/elastic.v3 v3.0.75/go.mod h1:yDEuSnrM51Pc8dM5ov7U8aI/ToR3PG0llA8aRv2qmw0=\ngopkg.in/olivere/elastic.v5 v5.0.84/go.mod h1:LXF6q9XNBxpMqrcgax95C6xyARXWbbCXUrtTxrNrxJI=\ngopkg.in/resty.v1 v1.12.0/go.mod h1:mDo4pnntr5jdWRML875a/NmxYqAlA73dVijT2AXvQQo=\ngopkg.in/retry.v1 v1.0.3/go.mod h1:FJkXmWiMaAo7xB+xhvDF59zhfjDWyzmyAxiT4dB688g=\ngopkg.in/square/go-jose.v2 v2.5.1/go.mod h1:M9dMgbHiYLoDGQrXy7OpJDJWiKiU//h+vD76mk0e1AI=\ngopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7/go.mod h1:dt/ZhP58zS4L8KSrWDmTeBkI65Dw0HsyUHuEVlX15mw=\ngopkg.in/warnings.v0 v0.1.2/go.mod h1:jksf8JmL6Qr/oQM2OXTHunEvvTAsrWBLb6OOjuVWRNI=\ngopkg.in/yaml.v2 v2.0.0-20170812160011-eb3733d160e7/go.mod h1:JAlM8MvJe8wmxCU4Bli9HhUf9+ttbYbLASfIpnQbh74=\ngopkg.in/yaml.v2 v2.2.1/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.3/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.4/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.5/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.7/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.8/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.3.0/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.4.0 h1:D8xgwECY7CYvx+Y2n4sBz93Jn9JRvxdiyyo8CTfuKaY=\ngopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=\ngopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngopkg.in/yaml.v3 v3.0.0-20200605160147-a5ece683394c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngopkg.in/yaml.v3 v3.0.0-20210107192922-496545a6307b/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngorm.io/driver/mysql v1.0.1/go.mod h1:KtqSthtg55lFp3S5kUXqlGaelnWpKitn4k1xZTnoiPw=\ngorm.io/driver/postgres v1.0.0/go.mod h1:wtMFcOzmuA5QigNsgEIb7O5lhvH1tHAF1RbWmLWV4to=\ngorm.io/driver/sqlserver v1.0.4/go.mod h1:ciEo5btfITTBCj9BkoUVDvgQbUdLWQNqdFY5OGuGnRg=\ngorm.io/gorm v1.9.19/go.mod h1:0HFTzE/SqkGTzK6TlDPPQbAYCluiVvhzoA1+aVyzenw=\ngorm.io/gorm v1.20.0/go.mod h1:0HFTzE/SqkGTzK6TlDPPQbAYCluiVvhzoA1+aVyzenw=\ngorm.io/gorm v1.20.6/go.mod h1:0HFTzE/SqkGTzK6TlDPPQbAYCluiVvhzoA1+aVyzenw=\nhonnef.co/go/tools v0.0.0-20180728063816-88497007e858/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.0-20190102054323-c2f93a96b099/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.0-20190106161140-3f1c8253044a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.0-20190418001031-e561f6794a2a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.0-20190523083050-ea95bdfd59fc/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.1-2019.2.3/go.mod h1:a3bituU0lyd329TUQxRnasdCoJDkEUEAqEt0JzvZhAg=\nhonnef.co/go/tools v0.0.1-2020.1.3/go.mod h1:X/FiERA/W4tHapMX5mGpAtMSVEeEUOyHaw9vFzvIQ3k=\nhonnef.co/go/tools v0.0.1-2020.1.4/go.mod h1:X/FiERA/W4tHapMX5mGpAtMSVEeEUOyHaw9vFzvIQ3k=\nk8s.io/api v0.17.0/go.mod h1:npsyOePkeP0CPwyGfXDHxvypiYMJxBWAMpQxCaJ4ZxI=\nk8s.io/apimachinery v0.17.0/go.mod h1:b9qmWdKlLuU9EBh+06BtLcSf/Mu89rWL33naRxs1uZg=\nk8s.io/client-go v0.17.0/go.mod h1:TYgR6EUHs6k45hb6KWjVD6jFZvJV4gHDikv/It0xz+k=\nk8s.io/gengo v0.0.0-20190128074634-0689ccc1d7d6/go.mod h1:ezvh/TsK7cY6rbqRK0oQQ8IAqLxYwwyPxAX1Pzy0ii0=\nk8s.io/klog v0.0.0-20181102134211-b9b56d5dfc92/go.mod h1:Gq+BEi5rUBO/HRz0bTSXDUcqjScdoY3a9IHpCEIOOfk=\nk8s.io/klog v0.3.0/go.mod h1:Gq+BEi5rUBO/HRz0bTSXDUcqjScdoY3a9IHpCEIOOfk=\nk8s.io/klog v1.0.0/go.mod h1:4Bi6QPql/J/LkTDqv7R/cd3hPo4k2DG6Ptcz060Ez5I=\nk8s.io/kube-openapi v0.0.0-20191107075043-30be4d16710a/go.mod h1:1TqjTSzOxsLGIKfj0lK8EeCP7K1iUG65v09OM0/WG5E=\nk8s.io/utils v0.0.0-20191114184206-e782cd3c129f/go.mod h1:sZAwmy6armz5eXlNoLmJcl4F1QuKu7sr+mFQ0byX7Ew=\nmellium.im/sasl v0.2.1/go.mod h1:ROaEDLQNuf9vjKqE1SrAfnsobm2YKXT1gnN1uDp1PjQ=\nmodernc.org/mathutil v1.5.0 h1:rV0Ko/6SfM+8G+yKiyI830l3Wuz1zRutdslNoQ0kfiQ=\nmodernc.org/mathutil v1.5.0/go.mod h1:mZW8CKdRPY1v87qxC/wUdX5O1qDzXMP5TH3wjfpga6E=\nmodernc.org/strutil v1.1.3 h1:fNMm+oJklMGYfU9Ylcywl0CO5O6nTfaowNsh2wpPjzY=\nmodernc.org/strutil v1.1.3/go.mod h1:MEHNA7PdEnEwLvspRMtWTNnp2nnyvMfkimT1NKNAGbw=\nnhooyr.io/websocket v1.8.6 h1:s+C3xAMLwGmlI31Nyn/eAehUlZPwfYZu2JXM621Q5/k=\nnhooyr.io/websocket v1.8.6/go.mod h1:B70DZP8IakI65RVQ51MsWP/8jndNma26DVA/nFSCgW0=\nrsc.io/binaryregexp v0.2.0/go.mod h1:qTv7/COck+e2FymRvadv62gMdZztPaShugOCi3I+8D8=\nrsc.io/pdf v0.1.1/go.mod h1:n8OzWcQ6Sp37PL01nO98y4iUCRdTGarVfzxY20ICaU4=\nrsc.io/quote/v3 v3.1.0/go.mod h1:yEA65RcK8LyAZtP9Kv3t0HmxON59tX3rD+tICJqUlj0=\nrsc.io/sampler v1.3.0/go.mod h1:T1hPZKmBbMNahiBKFy5HrXp6adAjACjK9JXDnKaTXpA=\nsigs.k8s.io/structured-merge-diff v0.0.0-20190525122527-15d366b2352e/go.mod h1:wWxsB5ozmmv/SG7nM11ayaAW51xMvak/t1r0CSlcokI=\nsigs.k8s.io/yaml v1.1.0/go.mod h1:UJmg0vDUVViEyp3mgSv9WPwZCDxu4rQW1olrI1uml+o=\nsigs.k8s.io/yaml v1.2.0 h1:kr/MCeFWJWTwyaHoR9c8EjH9OumOmoF9YGiZd7lFm/Q=\nsigs.k8s.io/yaml v1.2.0/go.mod h1:yfXDCHCao9+ENCvLSE62v9VSji2MKu5jeNfTrofGhJc=\nsourcegraph.com/sourcegraph/appdash v0.0.0-20190731080439-ebfcffb1b5c0/go.mod h1:hI742Nqp5OhwiqlzhgfbWU4mW4yO10fP+LoT9WOswdU=\nvitess.io/vitess v3.0.0-rc.3.0.20190602171040-12bfde34629c+incompatible h1:GWnLrAdetgJM0Co5bwwczO49iFZBSInpyGAT77BP9Y0=\nvitess.io/vitess v3.0.0-rc.3.0.20190602171040-12bfde34629c+incompatible/go.mod h1:h4qvkyNYTOC0xI+vcidSWoka0gQAZc9ZPHbkHo48gP0=\n"
        },
        {
          "name": "gopsutil",
          "type": "tree",
          "content": null
        },
        {
          "name": "hack.go",
          "type": "blob",
          "size": 2.03515625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"math\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/pb\"\n\t\"github.com/featurebasedb/featurebase/v3/pql\"\n\t\"github.com/gogo/protobuf/proto\"\n)\n\nfunc UnmarshalIndexOptions(name string, createdAt int64, buf []byte) (*IndexOptions, error) {\n\tvar io pb.IndexMeta\n\t// Read data from meta file.\n\tif err := proto.Unmarshal(buf, &io); err != nil {\n\t\treturn nil, err\n\t}\n\treturn &IndexOptions{\n\t\tKeys:           io.Keys,\n\t\tTrackExistence: io.TrackExistence,\n\t}, nil\n\n}\nfunc UnmarshalFieldOptions(name string, createdAt int64, buf []byte) (*FieldInfo, error) {\n\tvar pbi pb.FieldOptions\n\n\t// Read data from meta file.\n\tif err := proto.Unmarshal(buf, &pbi); err != nil {\n\t\treturn nil, err\n\t}\n\tfi := &FieldInfo{}\n\tfi.Name = name\n\tfi.CreatedAt = createdAt\n\tfi.Options = FieldOptions{}\n\t// Initialize \"base\" to \"min\" when upgrading from v1 BSI format.\n\tif pbi.BitDepth == 0 {\n\t\tpbi.Base = bsiBase(pbi.OldMin, pbi.OldMax)\n\t\tpbi.BitDepth = uint64(bitDepthInt64(pbi.OldMax - pbi.OldMin))\n\t\tif pbi.BitDepth == 0 {\n\t\t\tpbi.BitDepth = 1\n\t\t}\n\t}\n\n\t// Copy metadata fields.\n\tfi.Options.Type = pbi.Type\n\tif pbi.Type == \"decimal\" {\n\t\tfi.Options.Scale = 3\n\t}\n\tfi.Options.CacheType = pbi.CacheType\n\tfi.Options.CacheSize = pbi.CacheSize\n\tfi.Options.Base = pbi.Base\n\tfi.Options.BitDepth = pbi.BitDepth\n\tfi.Options.TimeQuantum = TimeQuantum(pbi.TimeQuantum)\n\tttlValue, err := time.ParseDuration(pbi.TTL)\n\tif err != nil {\n\t\tttlValue = 0\n\t}\n\tfi.Options.TTL = ttlValue\n\tfi.Options.Keys = pbi.Keys\n\tfi.Options.NoStandardView = pbi.NoStandardView\n\n\t// hard code for the unmarshal go broken with a version change\n\tswitch fi.Options.Type {\n\tcase \"int\":\n\t\tmin := int64(math.MinInt64)\n\t\tmax := int64(math.MaxInt64)\n\t\tfi.Options.Min = pql.NewDecimal(min, 0)\n\t\tfi.Options.Max = pql.NewDecimal(max, 0)\n\t\tfi.Options.Base = 0\n\tcase \"decimal\":\n\t\tscale := int64(3)\n\t\tmin, max := pql.MinMax(scale)\n\t\tfi.Options.Min = min\n\t\tfi.Options.Max = max\n\t\tfi.Options.Base = 0\n\t\tfi.Options.Scale = scale\n\t}\n\n\treturn fi, nil\n}\n"
        },
        {
          "name": "handler.go",
          "type": "blob",
          "size": 16.1748046875,
          "content": "// Copyright 2021 Molecula Corp. All rights reserved.\npackage pilosa\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"math/bits\"\n\t\"reflect\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/shardwidth\"\n\t\"github.com/featurebasedb/featurebase/v3/tracing\"\n\t\"github.com/pkg/errors\"\n)\n\n// QueryRequest represent a request to process a query.\ntype QueryRequest struct {\n\t// Index to execute query against.\n\tIndex string\n\n\t// The query string to parse and execute.\n\tQuery string\n\n\t// The SQL source query, if applicable.\n\tSQLQuery string\n\n\t// The shards to include in the query execution.\n\t// If empty, all shards are included.\n\tShards []uint64\n\n\t// If true, indicates that query is part of a larger distributed query.\n\t// If false, this request is on the originating node.\n\tRemote bool\n\n\t// Query has already been translated. This is only used if Remote\n\t// is false, Remote=true implies this.\n\tPreTranslated bool\n\n\t// Should we profile this query?\n\tProfile bool\n\n\t// Additional data associated with the query, in cases where there's\n\t// row-style inputs for precomputed values.\n\tEmbeddedData []*Row\n\n\t// Limit on memory used by request (Extract() only)\n\tMaxMemory int64\n}\n\n// QueryResponse represent a response from a processed query.\ntype QueryResponse struct {\n\t// Result for each top-level query call.\n\t// The result type differs depending on the query; types\n\t// include: Row, RowIdentifiers, GroupCounts, SignedRow,\n\t// ValCount, Pair, Pairs, bool, uint64.\n\tResults []interface{}\n\n\t// Error during parsing or execution.\n\tErr error\n\n\t// Profiling data, if any\n\tProfile *tracing.Profile\n}\n\n// MarshalJSON marshals QueryResponse into a JSON-encoded byte slice\nfunc (resp *QueryResponse) MarshalJSON() ([]byte, error) {\n\tif resp.Err != nil {\n\t\treturn json.Marshal(struct {\n\t\t\tErr string `json:\"error\"`\n\t\t}{Err: resp.Err.Error()})\n\t}\n\n\treturn json.Marshal(struct {\n\t\tResults []interface{}    `json:\"results\"`\n\t\tProfile *tracing.Profile `json:\"profile,omitempty\"`\n\t}{\n\t\tResults: resp.Results,\n\t\tProfile: resp.Profile,\n\t})\n}\n\n// SameAs compares one QueryResponse to another and returns nil if the Results\n// and Err are both identical, or a descriptive error if they differ.\n// This function replaces using reflect.DeepEqual directly on the\n// QueryResponse, since QueryResponse contains an error field and reflect.DeepEqual\n// should not be used on errors.\nfunc (qr *QueryResponse) SameAs(other *QueryResponse) error {\n\tswitch {\n\tcase !reflect.DeepEqual(qr.Results, other.Results):\n\t\treturn fmt.Errorf(\"responses contained different results\")\n\tcase qr.Err == nil && other.Err == nil:\n\t\treturn nil\n\tcase qr.Err == nil && other.Err != nil:\n\t\treturn fmt.Errorf(\"unexpected error: %w\", other.Err)\n\tcase qr.Err != nil && other.Err == nil:\n\t\treturn fmt.Errorf(\"missing error: expected %v, got no error\", qr.Err)\n\tcase qr.Err.Error() != other.Err.Error():\n\t\treturn fmt.Errorf(\"wrong error: expected %v, got %w\", qr.Err, other.Err)\n\tdefault:\n\t\treturn nil\n\t}\n}\n\n// HandlerI is the interface for the data handler, a wrapper around\n// Pilosa's data store.\ntype HandlerI interface {\n\tServe() error\n\tClose() error\n}\n\ntype nopHandler struct{}\n\nfunc (n nopHandler) Serve() error {\n\treturn nil\n}\n\nfunc (n nopHandler) Close() error {\n\treturn nil\n}\n\n// NopHandler is a no-op implementation of the Handler interface.\nvar NopHandler HandlerI = nopHandler{}\n\n// ImportValueRequest describes the import request structure\n// for a value (BSI) import.\n// Note: no RowIDs here. have to convert BSI Values into RowIDs internally.\ntype ImportValueRequest struct {\n\tIndex          string\n\tIndexCreatedAt int64\n\tField          string\n\tFieldCreatedAt int64\n\t// if Shard is MaxUint64 (an impossible shard value), this\n\t// indicates that the column IDs may come from multiple shards.\n\tShard           uint64\n\tColumnIDs       []uint64 // e.g. weather stationID\n\tColumnKeys      []string\n\tValues          []int64 // e.g. temperature, humidity, barometric pressure\n\tFloatValues     []float64\n\tTimestampValues []time.Time\n\tStringValues    []string\n\tClear           bool\n\tscratch         []int // scratch space to allow us to get a stable sort in reasonable time\n}\n\nfunc (ivr *ImportValueRequest) Clone() *ImportValueRequest {\n\tnewIVR := &ImportValueRequest{}\n\tif ivr == nil {\n\t\treturn newIVR\n\t}\n\t*newIVR = *ivr\n\t// don't copy the internal scratch buffer\n\tnewIVR.scratch = nil\n\tif len(ivr.ColumnIDs) > 0 {\n\t\tnewIVR.ColumnIDs = make([]uint64, len(ivr.ColumnIDs))\n\t\tcopy(newIVR.ColumnIDs, ivr.ColumnIDs)\n\t}\n\tif len(ivr.ColumnKeys) > 0 {\n\t\tnewIVR.ColumnKeys = make([]string, len(ivr.ColumnKeys))\n\t\tcopy(newIVR.ColumnKeys, ivr.ColumnKeys)\n\t}\n\tif len(ivr.Values) > 0 {\n\t\tnewIVR.Values = make([]int64, len(ivr.Values))\n\t\tcopy(newIVR.Values, ivr.Values)\n\t}\n\tif len(ivr.FloatValues) > 0 {\n\t\tnewIVR.FloatValues = make([]float64, len(ivr.FloatValues))\n\t\tcopy(newIVR.FloatValues, ivr.FloatValues)\n\t}\n\tif len(ivr.TimestampValues) > 0 {\n\t\tnewIVR.TimestampValues = make([]time.Time, len(ivr.TimestampValues))\n\t\tcopy(newIVR.TimestampValues, ivr.TimestampValues)\n\t}\n\tif len(ivr.StringValues) > 0 {\n\t\tnewIVR.StringValues = make([]string, len(ivr.StringValues))\n\t\tcopy(newIVR.StringValues, ivr.StringValues)\n\t}\n\treturn newIVR\n}\n\n// AtomicRecord applies all its Ivr and Ivr atomically, in a Tx.\n// The top level Shard has to agree with Ivr[i].Shard and the Iv[i].Shard\n// for all i included (in Ivr and Ir). The same goes for the top level Index: all records\n// have to be writes to the same Index. These requirements are checked.\ntype AtomicRecord struct {\n\tIndex string\n\tShard uint64\n\n\tIvr []*ImportValueRequest // BSI values\n\tIr  []*ImportRequest      // other field types, e.g. single bit\n}\n\nfunc (ar *AtomicRecord) Clone() *AtomicRecord {\n\tnewAR := &AtomicRecord{Index: ar.Index, Shard: ar.Shard}\n\tnewAR.Ivr = make([]*ImportValueRequest, len(ar.Ivr))\n\tfor i, vr := range ar.Ivr {\n\t\tnewAR.Ivr[i] = vr.Clone()\n\t}\n\tnewAR.Ir = make([]*ImportRequest, len(ar.Ir))\n\tfor i, vr := range ar.Ir {\n\t\tnewAR.Ir[i] = vr.Clone()\n\t}\n\treturn newAR\n}\n\nfunc (ivr *ImportValueRequest) Len() int { return len(ivr.ColumnIDs) }\nfunc (ivr *ImportValueRequest) Less(i, j int) bool {\n\tif ivr.ColumnIDs[i] < ivr.ColumnIDs[j] {\n\t\treturn true\n\t}\n\tif ivr.ColumnIDs[i] > ivr.ColumnIDs[j] {\n\t\treturn false\n\t}\n\tif len(ivr.scratch) > 0 {\n\t\treturn ivr.scratch[i] < ivr.scratch[j]\n\t}\n\treturn false\n}\n\nfunc (ivr *ImportValueRequest) Swap(i, j int) {\n\tivr.ColumnIDs[i], ivr.ColumnIDs[j] = ivr.ColumnIDs[j], ivr.ColumnIDs[i]\n\tif len(ivr.Values) > 0 {\n\t\tivr.Values[i], ivr.Values[j] = ivr.Values[j], ivr.Values[i]\n\t} else if len(ivr.FloatValues) > 0 {\n\t\tivr.FloatValues[i], ivr.FloatValues[j] = ivr.FloatValues[j], ivr.FloatValues[i]\n\t} else if len(ivr.TimestampValues) > 0 {\n\t\tivr.TimestampValues[i], ivr.TimestampValues[j] = ivr.TimestampValues[j], ivr.TimestampValues[i]\n\t} else if len(ivr.StringValues) > 0 {\n\t\tivr.StringValues[i], ivr.StringValues[j] = ivr.StringValues[j], ivr.StringValues[i]\n\t}\n\tif len(ivr.scratch) > 0 {\n\t\tivr.scratch[i], ivr.scratch[j] = ivr.scratch[j], ivr.scratch[i]\n\t}\n}\n\n// Validate ensures that the payload of the request is valid.\nfunc (ivr *ImportValueRequest) Validate() error {\n\treturn ivr.ValidateWithTimestamp(ivr.IndexCreatedAt, ivr.FieldCreatedAt)\n}\n\n// ValidateWithTimestamp ensures that the payload of the request is valid.\nfunc (ivr *ImportValueRequest) ValidateWithTimestamp(indexCreatedAt, fieldCreatedAt int64) error {\n\tif ivr.Index == \"\" || ivr.Field == \"\" {\n\t\treturn errors.Errorf(\"index and field required, but got '%s' and '%s'\", ivr.Index, ivr.Field)\n\t}\n\tif len(ivr.ColumnIDs) != 0 && len(ivr.ColumnKeys) != 0 {\n\t\treturn errors.Errorf(\"must pass either column ids or keys, but not both\")\n\t}\n\tvar valueSetCount int\n\tif len(ivr.Values) != 0 {\n\t\tvalueSetCount++\n\t}\n\tif len(ivr.FloatValues) != 0 {\n\t\tvalueSetCount++\n\t}\n\tif len(ivr.TimestampValues) != 0 {\n\t\tvalueSetCount++\n\t}\n\tif len(ivr.StringValues) != 0 {\n\t\tvalueSetCount++\n\t}\n\tif valueSetCount > 1 {\n\t\treturn errors.Errorf(\"must pass ints, floats, or strings but not multiple\")\n\t}\n\n\tif (ivr.IndexCreatedAt != 0 && ivr.IndexCreatedAt != indexCreatedAt) ||\n\t\t(ivr.FieldCreatedAt != 0 && ivr.FieldCreatedAt != fieldCreatedAt) {\n\t\treturn ErrPreconditionFailed\n\t}\n\treturn nil\n}\n\n// ImportRequest describes the import request structure\n// for an import.  BSIs use the ImportValueRequest instead.\ntype ImportRequest struct {\n\tIndex          string\n\tIndexCreatedAt int64\n\tField          string\n\tFieldCreatedAt int64\n\tShard          uint64\n\tRowIDs         []uint64\n\tColumnIDs      []uint64\n\tRowKeys        []string\n\tColumnKeys     []string\n\tTimestamps     []int64\n\tClear          bool\n}\n\n// Clone allows copying an import request. Normally you wouldn't, but\n// some import functions are destructive on their inputs, and if you\n// want to *re-use* an import request, you might need this. If you're\n// using this outside tx_test, something is probably wrong.\nfunc (ir *ImportRequest) Clone() *ImportRequest {\n\tnewIR := &ImportRequest{}\n\tif ir == nil {\n\t\treturn newIR\n\t}\n\t*newIR = *ir\n\tif ir.RowIDs != nil {\n\t\tnewIR.RowIDs = make([]uint64, len(ir.RowIDs))\n\t\tcopy(newIR.RowIDs, ir.RowIDs)\n\t}\n\tif ir.ColumnIDs != nil {\n\t\tnewIR.ColumnIDs = make([]uint64, len(ir.ColumnIDs))\n\t\tcopy(newIR.ColumnIDs, ir.ColumnIDs)\n\t}\n\tif ir.RowKeys != nil {\n\t\tnewIR.RowKeys = make([]string, len(ir.RowKeys))\n\t\tcopy(newIR.RowKeys, ir.RowKeys)\n\t}\n\tif ir.ColumnKeys != nil {\n\t\tnewIR.ColumnKeys = make([]string, len(ir.ColumnKeys))\n\t\tcopy(newIR.ColumnKeys, ir.ColumnKeys)\n\t}\n\tif ir.Timestamps != nil {\n\t\tnewIR.Timestamps = make([]int64, len(ir.Timestamps))\n\t\tcopy(newIR.Timestamps, ir.Timestamps)\n\t}\n\treturn newIR\n}\n\n// SortToShards takes an import request which has been translated, but may\n// not be sorted, and turns it into a map from shard IDs to individual import\n// requests. We don't sort the entries within each shard because the correct\n// sorting depends on the field type and we don't want to deal with that\n// here.\nfunc (ir *ImportRequest) SortToShards() (result map[uint64]*ImportRequest) {\n\tif len(ir.ColumnIDs) == 0 {\n\t\treturn nil\n\t}\n\tdiffMask := uint64(0)\n\tprev := ir.ColumnIDs[0]\n\tfor _, r := range ir.ColumnIDs[1:] {\n\t\tdiffMask |= r ^ prev\n\t\tprev = r\n\t}\n\tbitsRemaining := bits.Len64(diffMask)\n\tif bitsRemaining <= shardwidth.Exponent {\n\t\tshard := ir.ColumnIDs[0] >> shardwidth.Exponent\n\t\tir.Shard = shard\n\t\tir.ColumnKeys = nil\n\t\tir.RowKeys = nil\n\t\treturn map[uint64]*ImportRequest{shard: ir}\n\t}\n\toutput := make(map[uint64]*ImportRequest)\n\tsortToShardsInto(ir, bitsRemaining-8, output)\n\treturn output\n}\n\n// sortToShardsInto puts the shards it finds into the given map, so that\n// as we split off buckets, they can be inserted into the same map.\nfunc sortToShardsInto(ir *ImportRequest, shift int, into map[uint64]*ImportRequest) {\n\tif shift < shardwidth.Exponent {\n\t\tshift = shardwidth.Exponent\n\t}\n\tnextShift := shift - 8\n\tif nextShift < shardwidth.Exponent {\n\t\tnextShift = shardwidth.Exponent\n\t}\n\t// count things that belong in each of the 256 buckets\n\tvar buckets [256]int\n\tvar starts [256]int\n\n\t// compute the buckets ourselves\n\tfor _, r := range ir.ColumnIDs {\n\t\tb := (r >> shift) & 0xFF\n\t\tbuckets[b]++\n\t}\n\ttotal := 0\n\t// compute starting points of each bucket, converting the\n\t// bucket counts into ends\n\tfor i := range buckets {\n\t\tstarts[i] = total\n\t\ttotal += buckets[i]\n\t\tbuckets[i] = total\n\t}\n\t// starts[n] is the index of the first thing that should\n\t// go in that bucket, buckets[n] is the index of the first\n\t// thing that shouldn't\n\tvar bucketOp ImportRequest = *ir\n\tbucketOp.ColumnKeys = nil\n\tbucketOp.RowKeys = nil\n\torigStarts := make([]int, len(starts))\n\tcopy(origStarts, starts[:])\n\tfor bucket, start := range origStarts {\n\t\tend := buckets[bucket]\n\t\tif end <= start {\n\t\t\tcontinue\n\t\t}\n\t\tfor j := start; j < end; j++ {\n\t\t\twant := int((ir.ColumnIDs[j] >> shift) & 0xFF)\n\t\t\tfor want != bucket {\n\t\t\t\t// move this to the beginning of the\n\t\t\t\t// bucket it wants to be in, swapping\n\t\t\t\t// the thing there here\n\t\t\t\tdst := starts[want]\n\t\t\t\tir.ColumnIDs[j], ir.ColumnIDs[dst] = ir.ColumnIDs[dst], ir.ColumnIDs[j]\n\t\t\t\tif ir.RowIDs != nil {\n\t\t\t\t\tir.RowIDs[j], ir.RowIDs[dst] = ir.RowIDs[dst], ir.RowIDs[j]\n\t\t\t\t}\n\t\t\t\tif ir.Timestamps != nil {\n\t\t\t\t\tir.Timestamps[j], ir.Timestamps[dst] = ir.Timestamps[dst], ir.Timestamps[j]\n\t\t\t\t}\n\t\t\t\tstarts[want]++\n\t\t\t\twant = int((ir.ColumnIDs[j] >> shift) & 0xFF)\n\t\t\t}\n\t\t}\n\t\t// If shift == shardwidth.Exponent, then this is a completed\n\t\t// shard and can go into the sharded output. otherwise, we\n\t\t// can subdivide it.\n\t\tbucketOp.ColumnIDs = ir.ColumnIDs[start:end]\n\t\tif ir.RowIDs != nil {\n\t\t\tbucketOp.RowIDs = ir.RowIDs[start:end]\n\t\t}\n\t\tif ir.Timestamps != nil {\n\t\t\tbucketOp.Timestamps = ir.Timestamps[start:end]\n\t\t}\n\t\tif shift == shardwidth.Exponent {\n\t\t\tx := bucketOp\n\t\t\tshard := ir.ColumnIDs[start] >> shardwidth.Exponent\n\t\t\tx.Shard = shard\n\t\t\tinto[shard] = &x\n\t\t} else {\n\t\t\tsortToShardsInto(&bucketOp, nextShift, into)\n\t\t}\n\t}\n}\n\n// ValidateWithTimestamp ensures that the payload of the request is valid.\nfunc (ir *ImportRequest) ValidateWithTimestamp(indexCreatedAt, fieldCreatedAt int64) error {\n\tif (ir.IndexCreatedAt != 0 && ir.IndexCreatedAt != indexCreatedAt) ||\n\t\t(ir.FieldCreatedAt != 0 && ir.FieldCreatedAt != fieldCreatedAt) {\n\t\treturn ErrPreconditionFailed\n\t}\n\n\treturn nil\n}\n\nconst (\n\tRequestActionSet       = \"set\"\n\tRequestActionClear     = \"clear\"\n\tRequestActionOverwrite = \"overwrite\"\n)\n\n// ImportRoaringRequest describes the import request structure\n// for an import containing roaring-encoded data.\ntype ImportRoaringRequest struct {\n\tIndexCreatedAt  int64\n\tFieldCreatedAt  int64\n\tClear           bool\n\tAction          string // [set, clear, overwrite]\n\tBlock           int\n\tViews           map[string][]byte\n\tUpdateExistence bool\n\tSuppressLog     bool\n}\n\n// ImportRoaringShardRequest is the request for the shard\n// transactional endpoint.\ntype ImportRoaringShardRequest struct {\n\t// Has this request already been forwarded to all replicas? If\n\t// Remote=false, then the handling server is responsible for\n\t// ensuring this request is sent to all repliacs before returning\n\t// a successful response to the client.\n\tRemote bool\n\tViews  []RoaringUpdate\n\n\t// SuppressLog requests we not write to the write log. Typically\n\t// that would be because this request is being replayed from a\n\t// write log.\n\tSuppressLog bool\n}\n\n// RoaringUpdate represents the bits to clear and then set in a particular view.\ntype RoaringUpdate struct {\n\tField string\n\tView  string\n\n\t// Clear is a roaring encoded bitmatrix of bits to clear. For\n\t// mutex or int-like fields, only the first row is looked at and\n\t// the bits in that row are cleared from every row.\n\tClear []byte\n\n\t// Set is the roaring encoded bitmatrix of bits to set. If this is\n\t// a mutex or int-like field, we'll assume the first shard width\n\t// of containers is the exists row and we will first clear all\n\t// bits in those columns and then set\n\tSet []byte\n\n\t// ClearRecords, when true, denotes that Clear should be\n\t// interpreted as a single row which will be subtracted from every\n\t// row in this view.\n\tClearRecords bool\n}\n\n// ValidateWithTimestamp ensures that the payload of the request is valid.\nfunc (irr *ImportRoaringRequest) ValidateWithTimestamp(indexCreatedAt, fieldCreatedAt int64) error {\n\tif (irr.IndexCreatedAt != 0 && irr.IndexCreatedAt != indexCreatedAt) ||\n\t\t(irr.FieldCreatedAt != 0 && irr.FieldCreatedAt != fieldCreatedAt) {\n\t\treturn ErrPreconditionFailed\n\t}\n\treturn nil\n}\n\n// ImportResponse is the structured response of an import.\ntype ImportResponse struct {\n\tErr string\n}\n\n// BlockDataRequest describes the structure of a request\n// for fragment block data.\ntype BlockDataRequest struct {\n\tIndex string\n\tField string\n\tView  string\n\tShard uint64\n\tBlock uint64\n}\n\n// BlockDataResponse is the structured response of a block\n// data request.\ntype BlockDataResponse struct {\n\tRowIDs    []uint64\n\tColumnIDs []uint64\n}\n\n// TranslateKeysRequest describes the structure of a request\n// for a batch of key translations.\ntype TranslateKeysRequest struct {\n\tIndex string\n\tField string\n\tKeys  []string\n\n\t// NotWritable is an awkward name, but it's just to keep backward compatibility with client and idk.\n\tNotWritable bool\n}\n\n// TranslateKeysResponse is the structured response of a key\n// translation request.\ntype TranslateKeysResponse struct {\n\tIDs []uint64\n}\n\n// TranslateIDsRequest describes the structure of a request\n// for a batch of id translations.\ntype TranslateIDsRequest struct {\n\tIndex string\n\tField string\n\tIDs   []uint64\n}\n\n// TranslateIDsResponse is the structured response of a id\n// translation request.\ntype TranslateIDsResponse struct {\n\tKeys []string\n}\n"
        },
        {
          "name": "handler_test.go",
          "type": "blob",
          "size": 1.8896484375,
          "content": "// Copyright 2022 Molecula Corp. All rights reserved.\npackage pilosa_test\n\nimport (\n\t\"fmt\"\n\t\"math/rand\"\n\t\"strings\"\n\t\"testing\"\n\n\tpilosa \"github.com/featurebasedb/featurebase/v3\"\n)\n\nfunc TestSortToShards(t *testing.T) {\n\tvar ir pilosa.ImportRequest\n\texpected := make(map[uint64][]uint64)\n\tconst n = 50\n\trng := rand.New(rand.NewSource(3))\n\tfor i := 0; i < n; i++ {\n\t\tx := uint64(rng.Intn(8 * pilosa.ShardWidth))\n\t\tshard := x / pilosa.ShardWidth\n\t\texpected[shard] = append(expected[shard], x)\n\t\tir.ColumnIDs = append(ir.ColumnIDs, x)\n\t}\n\tout := ir.SortToShards()\n\tfor shard, values := range out {\n\t\tif len(values.ColumnIDs) != len(expected[shard]) {\n\t\t\tt.Fatalf(\"shard %d: expected values %d, got values %d\", shard, expected[shard], values.ColumnIDs)\n\t\t}\n\t}\n}\n\nfunc TestQueryResponseSameAs(t *testing.T) {\n\tvar qr1, qr2 pilosa.QueryResponse\n\tqr1.Results = []any{\"testing different results\"}\n\tqr2.Results = []any{\"with SCIENCE!\"}\n\tqr1.Err = nil\n\tqr2.Err = nil\n\tif err := qr1.SameAs(&qr2); err.Error() != \"responses contained different results\" {\n\t\tt.Fatalf(\"expected responses to contain different results\")\n\t}\n\tqr1.Results = []any{\"this time, for sure!\"}\n\tqr2.Results = []any{\"this time, for sure!\"}\n\tif err := qr1.SameAs(&qr2); err != nil {\n\t\tt.Fatalf(\"expected responses to be the same\")\n\t}\n\terr1 := fmt.Errorf(\"out of cheese\")\n\terr2 := fmt.Errorf(\"out of dryd frorg pills\")\n\tqr1.Err = err1\n\tif err := qr1.SameAs(&qr2); !strings.Contains(err.Error(), \"missing error\") {\n\t\tt.Fatalf(\"should have had a missing error\")\n\t}\n\tqr1.Err = nil\n\tqr2.Err = err2\n\tif err := qr1.SameAs(&qr2); !strings.Contains(err.Error(), \"unexpected error\") {\n\t\tt.Fatalf(\"should have had an unexpected error\")\n\t}\n\tqr1.Err = err1\n\tif err := qr1.SameAs(&qr2); !strings.Contains(err.Error(), \"wrong error\") {\n\t\tt.Fatalf(\"should have had a wrong error\")\n\t}\n\tqr2.Err = err1\n\tif err := qr1.SameAs(&qr2); err != nil {\n\t\tt.Fatalf(\"expected responses top be the same\")\n\t}\n}\n"
        },
        {
          "name": "hash",
          "type": "tree",
          "content": null
        },
        {
          "name": "holder.go",
          "type": "blob",
          "size": 51.9990234375,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/dax\"\n\t\"github.com/featurebasedb/featurebase/v3/disco\"\n\t\"github.com/featurebasedb/featurebase/v3/logger\"\n\trbfcfg \"github.com/featurebasedb/featurebase/v3/rbf/cfg\"\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\t\"github.com/featurebasedb/featurebase/v3/storage\"\n\t\"github.com/featurebasedb/featurebase/v3/testhook\"\n\t\"github.com/featurebasedb/featurebase/v3/vprint\"\n\t\"github.com/pkg/errors\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\nconst (\n\t// defaultCacheFlushInterval is the default value for Fragment.CacheFlushInterval.\n\tdefaultCacheFlushInterval = 1 * time.Minute\n\n\t// existenceFieldName is the name of the internal field used to store existence values.\n\texistenceFieldName = \"_exists\"\n\n\t// DiscoDir is the default data directory used by the disco implementation.\n\tDiscoDir = \"disco\"\n\n\t// IndexesDir is the default indexes directory used by the holder.\n\tIndexesDir = \"indexes\"\n\n\t// FieldsDir is the default fields directory used by each index.\n\tFieldsDir = \"fields\"\n\n\t// DataframesDir is the directory where we store the dataframe files (currently Apache Arrow)\n\tDataframesDir = \"dataframes\"\n)\n\nfunc init() {\n\t// needed to get the most I/O throughtpu.\n\truntime.GOMAXPROCS(runtime.NumCPU())\n\n\t// For performance tuning, leave these readily available:\n\t// CPUProfileForDur(time.Minute, \"server.cpu.pprof\")\n\t// MemProfileForDur(2*time.Minute, \"server.mem.pprof\")\n}\n\n// Holder represents a container for indexes.\ntype Holder struct {\n\tmu sync.RWMutex\n\n\t// our configuration\n\tcfg *HolderConfig\n\n\t// Partition count used by translation.\n\tpartitionN int\n\n\t// opened channel is closed once Open() completes.\n\topened lockedChan\n\n\tbroadcaster broadcaster\n\tSchemator   disco.Schemator\n\tsharder     disco.Sharder\n\tserializer  Serializer\n\n\t// executor, which we use only to get access to its worker pool\n\texecutor *executor\n\n\t// Close management\n\twg      sync.WaitGroup\n\tclosing chan struct{}\n\n\t// Data directory path.\n\tpath string\n\n\t// The interval at which the cached row ids are persisted to disk.\n\tcacheFlushInterval time.Duration\n\n\tLogger logger.Logger\n\n\t// Instantiates new translation stores\n\tOpenTranslateStore  OpenTranslateStoreFunc\n\tOpenTranslateReader OpenTranslateReaderFunc\n\n\t// Func to open whatever implementation of transaction store we're using.\n\tOpenTransactionStore OpenTransactionStoreFunc\n\n\t// Func to open the ID allocator.\n\tOpenIDAllocator func(string, bool) (*idAllocator, error)\n\n\t// transactionManager\n\ttransactionManager *TransactionManager\n\n\ttranslationSyncer TranslationSyncer\n\n\tida *idAllocator\n\n\t// Queue of fields (having a foreign index) which have\n\t// opened before their foreign index has opened.\n\tforeignIndexFields   []*Field\n\tforeignIndexFieldsMu sync.Mutex\n\n\t// Queue of messages to broadcast in bulk when the cluster comes up.\n\t// This is wrong, but. . . yeah.\n\tstartMsgs   []Message\n\tstartMsgsMu sync.Mutex\n\n\t// opening is set to true while Holder is opening.\n\t// It's used to determine if foreign index application\n\t// needs to be queued and completed after all indexes\n\t// have opened.\n\topening bool\n\n\tOpts HolderOpts\n\n\tAuditor testhook.Auditor\n\n\ttxf *TxFactory\n\n\tlookupDB *sql.DB\n\n\t// a separate lock out for indexes, to avoid the deadlock/race dilema\n\t// on holding mu.\n\timu     sync.RWMutex\n\tindexes map[string]*Index\n\n\t// directive is the latest directive applied to the node.\n\tdirective *dax.Directive\n\n\t// directiveApplied is used for testing (in an attempt to avoid sleeps). It\n\t// should be removed once we sort out the logic between controller and\n\t// computer nodes. For example, the Controller really needs to send out\n\t// directives asynchronously and allow a computer to load data from\n\t// snapshotter/writelogger; then the Controller should only start directing\n\t// queries to that computer once it has completed applying the snapshot.\n\tdirectiveApplied bool\n}\n\n// HolderOpts holds information about the holder which other things might want\n// to look up later while using the holder.\ntype HolderOpts struct {\n\t// StorageBackend controls the tx/storage engine we instatiate. Set by\n\t// server.go OptServerStorageConfig\n\tStorageBackend string\n}\n\nfunc (h *Holder) Directive() dax.Directive {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\n\tif h.directive == nil {\n\t\treturn dax.Directive{}\n\t}\n\treturn *h.directive\n}\n\nfunc (h *Holder) SetDirective(d *dax.Directive) {\n\tif d == nil {\n\t\treturn\n\t}\n\n\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\n\t// Only set the cached directive if the incoming version is newer than that\n\t// of the existing directive's version.\n\tif h.directive == nil || d.Version > h.directive.Version {\n\t\th.directive = d\n\t\th.directiveApplied = false\n\t}\n}\n\n// DirectiveApplied returns true if the Holder's latest directive has been fully\n// applied and is safe for queries. This is primarily used in testing and will\n// likely evolve to something smarter.\nfunc (h *Holder) DirectiveApplied() bool {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\treturn h.directiveApplied\n}\n\n// SetDirectiveApplied sets the value of directiveApplied. See the node on the\n// DirectiveApplied method.\nfunc (h *Holder) SetDirectiveApplied(a bool) {\n\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\th.directiveApplied = a\n}\n\nfunc (h *Holder) StartTransaction(ctx context.Context, id string, timeout time.Duration, exclusive bool) (*Transaction, error) {\n\treturn h.transactionManager.Start(ctx, id, timeout, exclusive)\n}\n\nfunc (h *Holder) FinishTransaction(ctx context.Context, id string) (*Transaction, error) {\n\treturn h.transactionManager.Finish(ctx, id)\n}\n\nfunc (h *Holder) Transactions(ctx context.Context) (map[string]*Transaction, error) {\n\treturn h.transactionManager.List(ctx)\n}\n\nfunc (h *Holder) GetTransaction(ctx context.Context, id string) (*Transaction, error) {\n\treturn h.transactionManager.Get(ctx, id)\n}\n\n// lockedChan looks a little ridiculous admittedly, but exists for good reason.\n// The channel within is used (for example) to signal to other goroutines when\n// the Holder has finished opening (via closing the channel). However, it is\n// possible for the holder to be closed and then reopened, but a channel which\n// is closed cannot be re-opened. We must create a new channel - this creates a\n// data race with any goroutine which might be accessing the channel. To ensure\n// that there is no data race on the value of the channel itself, we wrap any\n// operation on it with an RWMutex so that we can guarantee that nothing is\n// trying to listen on it when it gets swapped.\ntype lockedChan struct {\n\tch chan struct{}\n\tmu sync.RWMutex\n}\n\nfunc (lc *lockedChan) Close() {\n\tlc.mu.RLock()\n\tdefer lc.mu.RUnlock()\n\tclose(lc.ch)\n}\n\nfunc (lc *lockedChan) Recv() {\n\tlc.mu.RLock()\n\tdefer lc.mu.RUnlock()\n\t<-lc.ch\n}\n\n// HolderConfig holds configuration details that need to be set up at\n// initial holder creation. NewHolder takes a *HolderConfig, which can be\n// nil. Use DefaultHolderConfig to get a default-valued HolderConfig you\n// can then alter.\ntype HolderConfig struct {\n\tPartitionN           int\n\tOpenTranslateStore   OpenTranslateStoreFunc\n\tOpenTranslateReader  OpenTranslateReaderFunc\n\tOpenTransactionStore OpenTransactionStoreFunc\n\tOpenIDAllocator      OpenIDAllocatorFunc\n\tTranslationSyncer    TranslationSyncer\n\tSerializer           Serializer\n\tSchemator            disco.Schemator\n\tSharder              disco.Sharder\n\tCacheFlushInterval   time.Duration\n\tLogger               logger.Logger\n\n\tStorageConfig *storage.Config\n\tRBFConfig     *rbfcfg.Config\n\n\tLookupDBDSN string\n}\n\n// DefaultHolderConfig provides a holder config with reasonable\n// defaults. Note that a production server would almost certainly\n// need to override these; that's usually handled by server options\n// such as OptServerOpenTranslateStore.\nfunc DefaultHolderConfig() *HolderConfig {\n\treturn &HolderConfig{\n\t\tPartitionN:           disco.DefaultPartitionN,\n\t\tOpenTranslateStore:   OpenInMemTranslateStore,\n\t\tOpenTranslateReader:  nil,\n\t\tOpenTransactionStore: OpenInMemTransactionStore,\n\t\tOpenIDAllocator:      func(string, bool) (*idAllocator, error) { return &idAllocator{}, nil },\n\t\tTranslationSyncer:    NopTranslationSyncer,\n\t\tSerializer:           GobSerializer,\n\t\tSchemator:            disco.NewInMemSchemator(),\n\t\tSharder:              disco.NewInMemSharder(),\n\t\tCacheFlushInterval:   defaultCacheFlushInterval,\n\t\tLogger:               logger.NopLogger,\n\t\tStorageConfig:        storage.NewDefaultConfig(),\n\t\tRBFConfig:            rbfcfg.NewDefaultConfig(),\n\t}\n}\n\n// TestHolderConfig provides a holder config with reasonable\n// defaults for tests. This means it tries to disable fsync\n// and sets significantly smaller file size limits for RBF,\n// for instance. Do not use this outside of the test\n// infrastructure.\nfunc TestHolderConfig() *HolderConfig {\n\tcfg := DefaultHolderConfig()\n\tcfg.StorageConfig.FsyncEnabled = false\n\tcfg.RBFConfig.FsyncEnabled = false\n\tcfg.RBFConfig.MaxSize = (1 << 28)\n\tcfg.RBFConfig.MaxWALSize = (1 << 28)\n\treturn cfg\n}\n\n// NewHolder returns a new instance of Holder for the given path.\nfunc NewHolder(path string, cfg *HolderConfig) *Holder {\n\tif cfg == nil {\n\t\tcfg = DefaultHolderConfig()\n\t}\n\tif cfg.StorageConfig == nil {\n\t\tcfg.StorageConfig = storage.NewDefaultConfig()\n\t}\n\tif cfg.RBFConfig == nil {\n\t\tcfg.RBFConfig = rbfcfg.NewDefaultConfig()\n\t}\n\n\th := &Holder{\n\t\tcfg:     cfg,\n\t\tclosing: make(chan struct{}),\n\n\t\topened: lockedChan{ch: make(chan struct{})},\n\n\t\tbroadcaster: NopBroadcaster,\n\n\t\tpartitionN:           cfg.PartitionN,\n\t\tcacheFlushInterval:   cfg.CacheFlushInterval,\n\t\tOpenTranslateStore:   cfg.OpenTranslateStore,\n\t\tOpenTranslateReader:  cfg.OpenTranslateReader,\n\t\tOpenTransactionStore: cfg.OpenTransactionStore,\n\t\tOpenIDAllocator:      cfg.OpenIDAllocator,\n\t\ttranslationSyncer:    cfg.TranslationSyncer,\n\t\tserializer:           cfg.Serializer,\n\t\tsharder:              cfg.Sharder,\n\t\tSchemator:            cfg.Schemator,\n\t\tLogger:               cfg.Logger,\n\t\tOpts:                 HolderOpts{StorageBackend: cfg.StorageConfig.Backend},\n\n\t\tAuditor: NewAuditor(),\n\n\t\tpath: path,\n\n\t\tindexes: make(map[string]*Index),\n\t}\n\n\ttxf, err := NewTxFactory(cfg.StorageConfig.Backend, h.IndexesPath(), h)\n\tvprint.PanicOn(err)\n\th.txf = txf\n\n\t_ = testhook.Created(h.Auditor, h, nil)\n\treturn h\n}\n\n// Path returns the path directory the holder was created with.\nfunc (h *Holder) Path() string {\n\treturn h.path\n}\n\n// IndexesPath returns the path of the indexes directory.\nfunc (h *Holder) IndexesPath() string {\n\treturn filepath.Join(h.path, IndexesDir)\n}\n\nfunc (h *Holder) deletePerShard(index *Index, shard uint64) error {\n\tinprocessRecords := NewRow()\n\n\tfrag := h.fragment(index.name, existenceFieldName, viewStandard, shard)\n\tif frag == nil {\n\t\treturn nil\n\t}\n\n\ttx := h.Txf().NewTx(Txo{Write: !writable, Index: index, Shard: shard})\n\tdefer tx.Rollback()\n\n\t// filter rows based on having _exists>=1, which is used to flag delete in-flight\n\trows, err := frag.rows(context.Background(), tx, 1)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// check if any rows are found\n\tif len(rows) == 0 {\n\t\treturn nil\n\t}\n\n\tfor _, record := range rows {\n\t\trow, err2 := frag.row(tx, record)\n\t\tif err2 != nil {\n\t\t\treturn fmt.Errorf(\"getting row IDs: %v\", err2)\n\t\t}\n\t\tinprocessRecords = inprocessRecords.Union(row)\n\t}\n\th.Logger.Printf(\"retrying delete: index=%v shard=%v record count=%v\", index.name, shard, inprocessRecords.Count())\n\n\ttx.Rollback() // release the read tx in case a checksum is needed in DeleteRows\n\n\t_, err = DeleteRows(context.Background(), inprocessRecords, index, shard)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"deleting rows: %v\", err)\n\t}\n\treturn nil\n}\n\n// processDeleteInflight checks if deletion was in progress when server shutdown\n// the _exists field is set to row+1 when delete is started. Upon completion, the row is deleted.\n// if _exists>=1, we finish deleting the rows\nfunc (h *Holder) processDeleteInflight() error {\n\tfor _, index := range h.Indexes() {\n\t\tif index.trackExistence {\n\t\t\tshards := index.AvailableShards(includeRemote).Slice()\n\t\t\tindex := index\n\t\t\tch := make(chan uint64, len(shards))\n\t\t\tfor _, shard := range shards {\n\t\t\t\tch <- shard\n\t\t\t}\n\t\t\tclose(ch)\n\n\t\t\tg := new(errgroup.Group)\n\t\t\tfor i := 0; i < runtime.NumCPU(); i++ {\n\t\t\t\tg.Go(func() error {\n\t\t\t\t\tfor shard := range ch {\n\t\t\t\t\t\tif err := h.deletePerShard(index, shard); err != nil {\n\t\t\t\t\t\t\treturn fmt.Errorf(\"delete shard %d: %w\", shard, err)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\treturn nil\n\t\t\t\t})\n\t\t\t}\n\t\t\tif err := g.Wait(); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// Open initializes the root data directory for the holder.\nfunc (h *Holder) Open() error {\n\th.opening = true\n\tdefer func() { h.opening = false }()\n\n\tif h.txf == nil {\n\t\ttxf, err := NewTxFactory(h.cfg.StorageConfig.Backend, h.IndexesPath(), h)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"Holder.Open NewTxFactory()\")\n\t\t}\n\t\th.txf = txf\n\t}\n\n\t// Reset closing in case Holder is being reopened.\n\th.closing = make(chan struct{})\n\n\th.Logger.Printf(\"open holder path: %s\", h.path)\n\tif err := os.MkdirAll(h.IndexesPath(), 0o750); err != nil {\n\t\treturn errors.Wrap(err, \"creating directory\")\n\t}\n\n\ttstore, err := h.OpenTransactionStore(h.path)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"opening transaction store\")\n\t}\n\th.transactionManager = NewTransactionManager(tstore)\n\th.transactionManager.Log = h.Logger\n\n\t// Open ID allocator.\n\th.ida, err = h.OpenIDAllocator(filepath.Join(h.path, \"idalloc.db\"), h.cfg.StorageConfig.FsyncEnabled)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"opening ID allocator\")\n\t}\n\n\t// Load schema from etcd.\n\tschema, err := h.Schemator.Schema(context.Background())\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting schema\")\n\t}\n\n\tfor idxKey, idx := range schema {\n\t\t// decode the CreateIndexMessage from the schema data in order to\n\t\t// get its metadata, such as CreateAt.\n\t\tcim, err := decodeCreateIndexMessage(h.serializer, idx.Data)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"decoding create index message\")\n\t\t}\n\n\t\th.Logger.Printf(\"opening index: %s\", idxKey)\n\n\t\tindex, err := h.newIndex(h.IndexPath(idxKey), idxKey)\n\t\tif errors.Cause(err) == ErrName {\n\t\t\th.Logger.Errorf(\"opening index: %s, err=%s\", idxKey, err)\n\t\t\tcontinue\n\t\t} else if err != nil {\n\t\t\treturn errors.Wrap(err, \"opening index\")\n\t\t}\n\n\t\t// Since we don't have createdAt and the other metadata stored on disk within the data\n\t\t// directory, we need to populate it from the etcd schema data.\n\n\t\t// TODO: we may no longer need the createdAt value stored in memory on\n\t\t// the index struct; it may only be needed in the schema return value\n\t\t// from the API, which already comes from etcd. In that case, this logic\n\t\t// could be removed, and the createdAt on the index struct could be\n\t\t// removed.\n\t\tindex.createdAt = cim.CreatedAt\n\t\tindex.owner = cim.Owner\n\t\tindex.description = cim.Meta.Description\n\n\t\terr = index.OpenWithSchema(idx)\n\t\tif err != nil {\n\t\t\t_ = h.txf.Close()\n\t\t\tif err == ErrName {\n\t\t\t\th.Logger.Errorf(\"opening index: %s, err=%s\", index.Name(), err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn fmt.Errorf(\"open index: name=%s, err=%s\", index.Name(), err)\n\t\t}\n\t\th.addIndex(index)\n\t}\n\n\t// If any fields were opened before their foreign index\n\t// was opened, it's safe to process those now since all index\n\t// opens have completed by this point.\n\tif err := h.processForeignIndexFields(); err != nil {\n\t\treturn errors.Wrap(err, \"processing foreign index fields\")\n\t}\n\n\t// Check if deletion was in progress when server was shutdown\n\th.processDeleteInflight()\n\n\th.opened.Close()\n\n\t_ = testhook.Opened(h.Auditor, h, nil)\n\n\tif err := h.txf.Open(); err != nil {\n\t\treturn errors.Wrap(err, \"Holder.Open h.txf.Open()\")\n\t}\n\n\tif h.cfg.LookupDBDSN != \"\" {\n\t\th.Logger.Printf(\"connecting to lookup database\")\n\n\t\tdb, err := sql.Open(\"postgres\", h.cfg.LookupDBDSN)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"connecting to lookup database\")\n\t\t}\n\t\tif err := db.Ping(); err != nil {\n\t\t\treturn errors.Wrap(err, \"pinging lookup database\")\n\t\t}\n\n\t\th.Logger.Printf(\"connection to lookup database succeeded, connection stats: %+v\", db.Stats())\n\n\t\th.lookupDB = db\n\t}\n\n\th.Logger.Printf(\"open holder: complete\")\n\n\treturn nil\n}\n\nfunc (h *Holder) sendOrSpool(msg Message) error {\n\tif h.maybeSpool(msg) {\n\t\treturn nil\n\t}\n\n\treturn h.broadcaster.SendSync(msg)\n}\n\nfunc (h *Holder) maybeSpool(msg Message) bool {\n\th.startMsgsMu.Lock()\n\tdefer h.startMsgsMu.Unlock()\n\n\tif h.startMsgs == nil {\n\t\t// Startup is done.\n\t\treturn false\n\t}\n\n\th.startMsgs = append(h.startMsgs, msg)\n\treturn true\n}\n\n// Activate runs the background tasks relevant to keeping a holder in\n// a stable state, such as flushing caches. This is separate from\n// opening because, while a server would nearly always want to do\n// this, other use cases (like consistency checks of a data directory)\n// need to avoid it even getting started.\nfunc (h *Holder) Activate() {\n\t// Periodically flush cache.\n\th.wg.Add(1)\n\tgo func() { defer h.wg.Done(); h.monitorCacheFlush() }()\n}\n\n// checkForeignIndex is a check before applying a foreign\n// index to a field; if the index is not yet available,\n// (because holder is still opening and may not have opened\n// the index yet), this method queues it up to be processed\n// once all indexes have been opened.\nfunc (h *Holder) checkForeignIndex(f *Field) error {\n\tif h.opening {\n\t\tif fi := h.Index(f.options.ForeignIndex); fi == nil {\n\t\t\th.foreignIndexFieldsMu.Lock()\n\t\t\tdefer h.foreignIndexFieldsMu.Unlock()\n\t\t\th.foreignIndexFields = append(h.foreignIndexFields, f)\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn f.applyForeignIndex()\n}\n\n// processForeignIndexFields applies a foreign index to any\n// fields which were opened before their foreign index.\nfunc (h *Holder) processForeignIndexFields() error {\n\tfor _, f := range h.foreignIndexFields {\n\t\tif err := f.applyForeignIndex(); err != nil {\n\t\t\treturn errors.Wrap(err, \"applying foreign index\")\n\t\t}\n\t}\n\th.foreignIndexFields = h.foreignIndexFields[:0] // reset\n\treturn nil\n}\n\n// Close closes all open fragments.\nfunc (h *Holder) Close() error {\n\tif h == nil {\n\t\treturn nil\n\t}\n\n\tif globalUseStatTx {\n\t\tfmt.Printf(\"%v\\n\", globalCallStats.report())\n\t}\n\n\t// Notify goroutines of closing and wait for completion.\n\tclose(h.closing)\n\th.wg.Wait()\n\tfor _, index := range h.Indexes() {\n\t\tif err := index.Close(); err != nil {\n\t\t\treturn errors.Wrap(err, \"closing index\")\n\t\t}\n\t}\n\tif err := h.txf.Close(); err != nil {\n\t\treturn errors.Wrap(err, \"holder.Txf.Close()\")\n\t}\n\tif err := h.ida.Close(); err != nil {\n\t\treturn errors.Wrap(err, \"closing ID allocator\")\n\t}\n\n\t// Reset opened in case Holder needs to be reopened.\n\th.txf = nil\n\th.opened.mu.Lock()\n\th.opened.ch = make(chan struct{})\n\th.opened.mu.Unlock()\n\n\tif h.lookupDB != nil {\n\t\terr := h.lookupDB.Close()\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"closing DB\")\n\t\t}\n\t\th.lookupDB = nil\n\t}\n\n\t_ = testhook.Closed(h.Auditor, h, nil)\n\n\treturn nil\n}\n\n// HasData returns true if Holder contains at least one index.\n// This is used to determine if the rebalancing of data is necessary\n// when a node joins the cluster.\nfunc (h *Holder) HasData() (bool, error) {\n\th.mu.RLock()\n\tdefer h.mu.RUnlock()\n\tif len(h.Indexes()) > 0 {\n\t\treturn true, nil\n\t}\n\t// Open path to read all index directories.\n\tif _, err := os.Stat(h.IndexesPath()); os.IsNotExist(err) {\n\t\treturn false, nil\n\t} else if err != nil {\n\t\treturn false, errors.Wrap(err, \"statting data dir\")\n\t}\n\n\tf, err := os.Open(h.IndexesPath())\n\tif err != nil {\n\t\treturn false, errors.Wrap(err, \"opening data dir\")\n\t}\n\tdefer f.Close()\n\n\tfis, err := f.Readdir(0)\n\tif err != nil {\n\t\treturn false, errors.Wrap(err, \"reading data dir\")\n\t}\n\n\tfor _, fi := range fis {\n\t\tif !fi.IsDir() {\n\t\t\tcontinue\n\t\t}\n\t\treturn true, nil\n\t}\n\treturn false, nil\n}\n\n// availableShardsByIndex returns a bitmap of all shards by indexes.\nfunc (h *Holder) availableShardsByIndex() map[string]*roaring.Bitmap {\n\tm := make(map[string]*roaring.Bitmap)\n\tfor _, index := range h.Indexes() {\n\t\tm[index.Name()] = index.AvailableShards(includeRemote)\n\t}\n\treturn m\n}\n\n// Schema returns schema information for all indexes, fields, and views.\nfunc (h *Holder) Schema() ([]*IndexInfo, error) {\n\treturn h.schema(context.TODO(), true)\n}\n\n// limitedSchema returns schema information for all indexes and fields.\nfunc (h *Holder) limitedSchema() ([]*IndexInfo, error) {\n\treturn h.schema(context.TODO(), false)\n}\n\nfunc (h *Holder) schema(ctx context.Context, includeViews bool) ([]*IndexInfo, error) {\n\tschema, err := h.Schemator.Schema(ctx)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"getting schema via Schemator\")\n\t}\n\n\ta := make([]*IndexInfo, 0, len(schema))\n\n\tfor _, index := range schema {\n\t\tcim, err := decodeCreateIndexMessage(h.serializer, index.Data)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"decoding CreateIndexMessage\")\n\t\t}\n\n\t\tdi := &IndexInfo{\n\t\t\tName:       cim.Index,\n\t\t\tCreatedAt:  cim.CreatedAt,\n\t\t\tOwner:      cim.Owner,\n\t\t\tOptions:    cim.Meta,\n\t\t\tShardWidth: ShardWidth,\n\t\t\tFields:     make([]*FieldInfo, 0, len(index.Fields)),\n\t\t}\n\t\tupdatedAt := cim.CreatedAt\n\t\tlastUpdateUser := cim.Owner\n\t\tfor fieldName, field := range index.Fields {\n\t\t\tif fieldName == existenceFieldName {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcfm, err := decodeCreateFieldMessage(h.serializer, field.Data)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"decoding CreateFieldMessage\")\n\t\t\t}\n\t\t\tif cfm.CreatedAt > updatedAt {\n\t\t\t\tupdatedAt = cfm.CreatedAt\n\t\t\t\tlastUpdateUser = cfm.Owner\n\t\t\t}\n\t\t\tfi := &FieldInfo{\n\t\t\t\tName:      cfm.Field,\n\t\t\t\tCreatedAt: cfm.CreatedAt,\n\t\t\t\tOwner:     cfm.Owner,\n\t\t\t\tOptions:   *cfm.Meta,\n\t\t\t}\n\t\t\tif includeViews {\n\t\t\t\tfor viewName := range field.Views {\n\t\t\t\t\tfi.Views = append(fi.Views, &ViewInfo{Name: viewName})\n\t\t\t\t}\n\t\t\t\tsort.Sort(viewInfoSlice(fi.Views))\n\t\t\t}\n\t\t\tdi.Fields = append(di.Fields, fi)\n\t\t}\n\t\tdi.UpdatedAt = updatedAt\n\t\tdi.LastUpdateUser = lastUpdateUser\n\t\tsort.Sort(fieldInfoSlice(di.Fields))\n\t\ta = append(a, di)\n\t}\n\tsort.Sort(indexInfoSlice(a))\n\treturn a, nil\n}\n\n// applySchema applies an internal Schema to Holder.\nfunc (h *Holder) applySchema(schema *Schema) error {\n\t// Create indexes.\n\t// We use h.CreateIndex() instead of h.CreateIndexIfNotExists() because we\n\t// want to limit the use of this method for now to only new indexes.\n\tfor _, i := range schema.Indexes {\n\t\tidx, err := h.CreateIndex(i.Name, i.Owner, i.Options)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"creating index\")\n\t\t}\n\n\t\t// Create fields that don't exist.\n\t\tfor _, f := range i.Fields {\n\t\t\tfld, err := idx.CreateFieldIfNotExistsWithOptions(f.Name, \"\", &f.Options)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Wrap(err, \"creating field\")\n\t\t\t}\n\n\t\t\t// Create views that don't exist.\n\t\t\tfor _, v := range f.Views {\n\t\t\t\t_, err := fld.createViewIfNotExists(v.Name)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errors.Wrap(err, \"creating view\")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Send the load schema message to all nodes.\n\tif err := h.sendOrSpool(&LoadSchemaMessage{}); err != nil {\n\t\treturn errors.Wrap(err, \"sending LoadSchemaMessage\")\n\t}\n\n\treturn nil\n}\n\n// IndexPath returns the path where a given index is stored.\nfunc (h *Holder) IndexPath(name string) string {\n\treturn filepath.Join(h.IndexesPath(), name)\n}\n\n// Index returns the index by name.\nfunc (h *Holder) Index(name string) (idx *Index) {\n\th.imu.RLock()\n\tidx = h.indexes[name]\n\th.imu.RUnlock()\n\treturn\n}\n\n// Indexes returns a list of all indexes in the holder.\nfunc (h *Holder) Indexes() []*Index {\n\th.imu.RLock()\n\t// sizing and copying has to be done under the lock to avoid\n\t// a logical race with a deletion/addition to indexes.\n\tcp := make([]*Index, 0, len(h.indexes))\n\tfor _, idx := range h.indexes {\n\t\tcp = append(cp, idx)\n\t}\n\th.imu.RUnlock()\n\tsort.Sort(indexSlice(cp))\n\treturn cp\n}\n\n// CreateIndex creates an index.\n// An error is returned if the index already exists.\nfunc (h *Holder) CreateIndex(name string, requestUserID string, opt IndexOptions) (*Index, error) {\n\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\n\t// Ensure index doesn't already exist.\n\tif h.Index(name) != nil {\n\t\treturn nil, newConflictError(ErrIndexExists)\n\t}\n\n\tts := timestamp()\n\tcim := &CreateIndexMessage{\n\t\tIndex:     name,\n\t\tCreatedAt: ts,\n\t\tOwner:     requestUserID,\n\t\tMeta:      opt,\n\t}\n\n\t// Create the index in etcd as the system of record.\n\tif err := h.persistIndex(context.Background(), cim); err != nil {\n\t\treturn nil, errors.Wrap(err, \"persisting index\")\n\t}\n\n\treturn h.createIndex(cim, false)\n}\n\n// LoadSchemaMessage is an internal message used to inform a node to load the\n// latest schema from etcd.\ntype LoadSchemaMessage struct{}\n\n// LoadSchema creates all indexes based on the information stored in Schemator.\n// It does not return an error if an index already exists. The thinking is that\n// this method will load all indexes that don't already exist. We likely want to\n// revisit this; for example, we might want to confirm that the createdAt\n// timestamps on each of the indexes matches the value in etcd.\nfunc (h *Holder) LoadSchema() error {\n\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\n\treturn h.loadSchema()\n}\n\n// LoadIndex creates an index based on the information stored in Schemator.\n// An error is returned if the index already exists.\nfunc (h *Holder) LoadIndex(name string) (*Index, error) {\n\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\n\t// Ensure index doesn't already exist.\n\tif h.Index(name) != nil {\n\t\treturn nil, newConflictError(ErrIndexExists)\n\t}\n\treturn h.loadIndex(name)\n}\n\n// LoadField creates a field based on the information stored in Schemator.\n// An error is returned if the field already exists.\nfunc (h *Holder) LoadField(index, field string) (*Field, error) {\n\t// Ensure field doesn't already exist.\n\tif h.Field(index, field) != nil {\n\t\treturn nil, newConflictError(ErrFieldExists)\n\t}\n\n\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\n\treturn h.loadField(index, field)\n}\n\n// LoadView creates a view based on the information stored in Schemator. Unlike\n// index and field, it is not considered an error if the view already exists.\nfunc (h *Holder) LoadView(index, field, view string) (*view, error) {\n\t// If the view already exists, just return with it here.\n\tif v := h.view(index, field, view); v != nil {\n\t\treturn v, nil\n\t}\n\n\treturn h.loadView(index, field, view)\n}\n\n// CreateIndexAndBroadcast creates an index locally, then broadcasts the\n// creation to other nodes so they can create locally as well. An error is\n// returned if the index already exists.\nfunc (h *Holder) CreateIndexAndBroadcast(ctx context.Context, cim *CreateIndexMessage) (*Index, error) {\n\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\n\t// Ensure index doesn't already exist.\n\tif h.Index(cim.Index) != nil {\n\t\treturn nil, newConflictError(ErrIndexExists)\n\t}\n\n\t// Create the index in etcd as the system of record.\n\tif err := h.persistIndex(ctx, cim); err != nil {\n\t\treturn nil, errors.Wrap(err, \"persisting index\")\n\t}\n\n\treturn h.createIndex(cim, true)\n}\n\n// CreateIndexIfNotExists returns an index by name.\n// The index is created if it does not already exist.\nfunc (h *Holder) CreateIndexIfNotExists(name string, requestUserID string, opt IndexOptions) (*Index, error) {\n\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\n\tts := timestamp()\n\tcim := &CreateIndexMessage{\n\t\tIndex:     name,\n\t\tCreatedAt: ts,\n\t\tOwner:     requestUserID,\n\t\tMeta:      opt,\n\t}\n\n\t// Create the index in etcd as the system of record.\n\terr := h.persistIndex(context.Background(), cim)\n\tif err != nil && errors.Cause(err) != disco.ErrIndexExists {\n\t\treturn nil, errors.Wrap(err, \"persisting index\")\n\t}\n\n\tif index := h.Index(name); index != nil {\n\t\treturn index, nil\n\t}\n\n\t// It may happen that index is not in memory, but it's already in etcd,\n\t// then we need to create it locally.\n\treturn h.createIndex(cim, false)\n}\n\n// persistIndex stores the index information in etcd.\nfunc (h *Holder) persistIndex(ctx context.Context, cim *CreateIndexMessage) error {\n\tif cim.Index == \"\" {\n\t\treturn ErrIndexRequired\n\t}\n\n\tif err := ValidateName(cim.Index); err != nil {\n\t\treturn errors.Wrap(err, \"validating name\")\n\t}\n\n\tif b, err := h.serializer.Marshal(cim); err != nil {\n\t\treturn errors.Wrap(err, \"marshaling\")\n\t} else if err := h.Schemator.CreateIndex(ctx, cim.Index, b); err != nil {\n\t\treturn errors.Wrapf(err, \"writing index to disco: %s\", cim.Index)\n\t}\n\treturn nil\n}\n\nfunc (h *Holder) createIndex(cim *CreateIndexMessage, broadcast bool) (*Index, error) {\n\tif cim.Index == \"\" {\n\t\treturn nil, errors.New(\"index name required\")\n\t}\n\n\t// Otherwise create a new index.\n\tindex, err := h.newIndex(h.IndexPath(cim.Index), cim.Index)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating\")\n\t}\n\n\tindex.keys = cim.Meta.Keys\n\tindex.trackExistence = cim.Meta.TrackExistence\n\tindex.createdAt = cim.CreatedAt\n\tindex.owner = cim.Owner\n\tindex.description = cim.Meta.Description\n\n\tif err = index.Open(); err != nil {\n\t\treturn nil, errors.Wrap(err, \"opening\")\n\t}\n\n\t// Update options.\n\th.addIndex(index)\n\n\tif broadcast {\n\t\t// Send the create index message to all nodes.\n\t\tif err := h.broadcaster.SendSync(cim); err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"sending CreateIndex message\")\n\t\t}\n\t}\n\n\t// Since this is a new index, we need to kick off\n\t// its translation sync.\n\tif err := h.translationSyncer.Reset(); err != nil {\n\t\treturn nil, errors.Wrap(err, \"resetting translation sync\")\n\t}\n\n\treturn index, nil\n}\n\n// createIndexWithPartitions is similar to createIndex, but it takes a list of\n// partitions for which this node is responsible. This ensures that the node\n// doesn't instantiate more partition TranslateStores than is necessary.\nfunc (h *Holder) createIndexWithPartitions(cim *CreateIndexMessage, translatePartitions dax.PartitionNums) (*Index, error) {\n\tif cim.Index == \"\" {\n\t\treturn nil, errors.New(\"index name required\")\n\t}\n\n\t// Otherwise create a new index.\n\tindex, err := h.newIndex(h.IndexPath(cim.Index), cim.Index)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating\")\n\t}\n\n\tindex.keys = cim.Meta.Keys\n\tindex.trackExistence = cim.Meta.TrackExistence\n\tindex.createdAt = cim.CreatedAt\n\tindex.translatePartitions = translatePartitions\n\n\tif err = index.Open(); err != nil {\n\t\treturn nil, errors.Wrap(err, \"opening\")\n\t}\n\n\t// Update options.\n\th.addIndex(index)\n\n\t// Since this is a new index, we need to kick off\n\t// its translation sync.\n\tif err := h.translationSyncer.Reset(); err != nil {\n\t\treturn nil, errors.Wrap(err, \"resetting translation sync\")\n\t}\n\n\treturn index, nil\n}\n\nfunc (h *Holder) loadSchema() error {\n\tschema, err := h.Schemator.Schema(context.TODO())\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"getting schema\")\n\t}\n\n\t// TODO: This is kind of inefficient because we're ignoring the index.Data\n\t// and field.Data values, which contains the index and field information,\n\t// and only using the map key to call loadIndex() and loadField(). These\n\t// make another call to Schemator to get the same index and field\n\t// information that we already have in the map. It probably makes sense to\n\t// either copy the parts of the loadIndex and loadField methods here (like\n\t// decodeCreateIndexMessage) or split loadIndex and loadField into smaller\n\t// methods that we could reuse here.\n\tfor indexName, index := range schema {\n\t\t_, err := h.loadIndex(indexName)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"loading index\")\n\t\t}\n\t\tfor fieldName, field := range index.Fields {\n\t\t\t_, err := h.loadField(indexName, fieldName)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Wrap(err, \"loading field\")\n\t\t\t}\n\t\t\tfor viewName := range field.Views {\n\t\t\t\t_, err := h.loadView(indexName, fieldName, viewName)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errors.Wrap(err, \"loading view\")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (h *Holder) loadIndex(indexName string) (*Index, error) {\n\tb, err := h.Schemator.Index(context.TODO(), indexName)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"getting index: %s\", indexName)\n\t}\n\n\tcim, err := decodeCreateIndexMessage(h.serializer, b)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"decoding CreateIndexMessage\")\n\t}\n\n\treturn h.createIndex(cim, false)\n}\n\nfunc (h *Holder) loadField(indexName, fieldName string) (*Field, error) {\n\tb, err := h.Schemator.Field(context.TODO(), indexName, fieldName)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"getting field: %s/%s\", indexName, fieldName)\n\t}\n\n\t// Get index.\n\tidx := h.Index(indexName)\n\tif idx == nil {\n\t\treturn nil, errors.Errorf(\"local index not found: %s\", indexName)\n\t}\n\n\tcfm, err := decodeCreateFieldMessage(h.serializer, b)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"decoding CreateFieldMessage\")\n\t}\n\n\treturn idx.createFieldIfNotExists(cfm)\n}\n\nfunc (h *Holder) loadView(indexName, fieldName, viewName string) (*view, error) {\n\tb, err := h.Schemator.View(context.Background(), indexName, fieldName, viewName)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"getting view: %s/%s/%s\", indexName, fieldName, viewName)\n\t} else if !b {\n\t\treturn nil, errors.Wrapf(err, \"tried to load a nonexistent view: %s/%s/%s\", indexName, fieldName, viewName)\n\t}\n\n\t// Get field.\n\tfld := h.Field(indexName, fieldName)\n\tif fld == nil {\n\t\treturn nil, errors.Errorf(\"local field not found: %s/%s\", indexName, fieldName)\n\t}\n\n\treturn fld.createViewIfNotExists(viewName)\n}\n\nfunc (h *Holder) newIndex(path, name string) (*Index, error) {\n\tindex, err := NewIndex(h, path, name)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tindex.broadcaster = h.broadcaster\n\tindex.serializer = h.serializer\n\tindex.OpenTranslateStore = h.OpenTranslateStore\n\tindex.translationSyncer = h.translationSyncer\n\treturn index, nil\n}\n\n// DeleteIndex removes an index from the holder.\nfunc (h *Holder) DeleteIndex(name string) error {\n\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\treturn h.deleteIndex(name)\n}\n\n// deleteIndex is a non-locking version of DeleteIndex().\nfunc (h *Holder) deleteIndex(name string) error {\n\t// Confirm index exists.\n\tindex := h.Index(name)\n\tif index == nil {\n\t\treturn newNotFoundError(ErrIndexNotFound, name)\n\t}\n\n\t// Delete the index from etcd as the system of record.\n\tif err := h.Schemator.DeleteIndex(context.TODO(), name); err != nil {\n\t\treturn errors.Wrapf(err, \"deleting index from etcd: %s\", name)\n\t}\n\n\t// Close index.\n\tif err := index.Close(); err != nil {\n\t\treturn errors.Wrap(err, \"closing\")\n\t}\n\n\t// remove any backing store.\n\tif err := h.txf.DeleteIndex(name); err != nil {\n\t\treturn errors.Wrap(err, \"h.Txf.DeleteIndex\")\n\t}\n\n\t// Delete index directory.\n\tif err := os.RemoveAll(h.IndexPath(name)); err != nil {\n\t\t// There is a rare edge case here: If a cache flush was happening, RemoveAll\n\t\t// can fail because a file gets created, say in a fragment directory, after\n\t\t// RemoveAll has deleted everything it found in the directory, but before\n\t\t// the actual directory is unlinked. In theory, though, this can't happen\n\t\t// twice; by the time we get here, everything was closed, so at most one\n\t\t// more file should get created.\n\t\terr = os.RemoveAll(h.IndexPath(name))\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"removing directory\")\n\t\t}\n\t}\n\n\t// Remove reference.\n\th.deleteIndexFromMap(name)\n\n\t// I'm not sure if calling Reset() here is necessary\n\t// since closing the index stops its translation\n\t// sync processes.\n\treturn h.translationSyncer.Reset()\n}\n\nfunc (h *Holder) deleteIndexFromMap(index string) {\n\th.imu.Lock()\n\tdelete(h.indexes, index)\n\th.imu.Unlock()\n}\n\n// Field returns the field for an index and name.\nfunc (h *Holder) Field(index, name string) *Field {\n\tidx := h.Index(index)\n\tif idx == nil {\n\t\treturn nil\n\t}\n\treturn idx.Field(name)\n}\n\n// view returns the view for an index, field, and name.\nfunc (h *Holder) view(index, field, name string) *view {\n\tf := h.Field(index, field)\n\tif f == nil {\n\t\treturn nil\n\t}\n\treturn f.view(name)\n}\n\n// fragment returns the fragment for an index, field & shard.\nfunc (h *Holder) fragment(index, field, view string, shard uint64) *fragment {\n\tv := h.view(index, field, view)\n\tif v == nil {\n\t\treturn nil\n\t}\n\treturn v.Fragment(shard)\n}\n\n// monitorCacheFlush periodically flushes all fragment caches sequentially.\n// This is run in a goroutine.\nfunc (h *Holder) monitorCacheFlush() {\n\tticker := time.NewTicker(h.cacheFlushInterval)\n\tdefer ticker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-h.closing:\n\t\t\treturn\n\t\tcase <-ticker.C:\n\t\t\th.flushCaches()\n\t\t}\n\t}\n}\n\nfunc (h *Holder) flushCaches() {\n\tfor _, index := range h.Indexes() {\n\t\tselect {\n\t\tcase <-h.closing:\n\t\t\treturn\n\t\tdefault:\n\t\t\tindex.flushCaches()\n\t\t}\n\t}\n}\n\n// recalculateCaches recalculates caches on every index in the holder. This is\n// probably not practical to call in real-world workloads, but makes writing\n// integration tests much eaiser, since one doesn't have to wait 10 seconds\n// after setting bits to get expected response.\n// This is mostly unnecessary now, as caches will automatically recalculate on read.\n// However, a user may explicitly request calculation, in which case we should not defer it.\nfunc (h *Holder) recalculateCaches() {\n\tfor _, index := range h.Indexes() {\n\t\tindex.recalculateCaches()\n\t}\n}\n\n// Log startup time and version to $DATA_DIR/.startup.log\nfunc (h *Holder) logStartup() error {\n\tRFC3339NanoFixedWidth := \"2006-01-02T15:04:05.000000 07:00\"\n\ttime := time.Now().Format(RFC3339NanoFixedWidth)\n\tlogLine := fmt.Sprintf(\"%s\\t%s\\n\", time, Version)\n\n\tif err := os.MkdirAll(h.path, 0o750); err != nil {\n\t\treturn errors.Wrap(err, \"creating data directory\")\n\t}\n\n\tf, err := os.OpenFile(h.path+\"/startup.log\", os.O_APPEND|os.O_WRONLY|os.O_CREATE, 0o600)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"opening startup log\")\n\t}\n\n\tif _, err = f.WriteString(logLine); err != nil {\n\t\treturn errors.Wrap(err, \"writing startup log\")\n\t}\n\n\treturn f.Close()\n}\n\n// holderSyncer is an active anti-entropy tool that compares the local holder\n// with a remote holder based on block checksums and resolves differences.\ntype holderSyncer struct {\n\tmu sync.Mutex\n\n\tHolder *Holder\n\n\tNode    *disco.Node\n\tCluster *cluster\n\n\t// Translation sync handling.\n\treaders                     []TranslateEntryReader\n\treadersMu                   sync.Mutex\n\tpendingReaders              int\n\tstopInitializeReplicationCh chan struct{}\n\n\tsyncers errgroup.Group\n\n\t// Signals that the sync should stop.\n\tClosing <-chan struct{}\n}\n\n// resetTranslationSync reinitializes streaming sync of translation data.\nfunc (s *holderSyncer) resetTranslationSync() error {\n\tif s.stopInitializeReplicationCh == nil {\n\t\t// suppose stopTranslationSync[S] holds the lock s.readersMu and tries\n\t\t// to send a signal to s.stopInitializeRepliationCh. If\n\t\t// s.stopInitializeReplicationCh is unbufferd and\n\t\t// s.initializeReplication[I] is trying to acquire s.readersMu, this\n\t\t// will result in a deadlock.\n\t\t// Hence, the channel is buffered to prevent this scenario. Once\n\t\t// [S] releases the lock, [I] will acquire it, however, the value\n\t\t// of s.pendingReaders will be -1, at this point [I] should not\n\t\t// attempt to add any more readers since those readers will be 'stale'\n\t\t// [I] should also drain the channel to prevent the next invocation\n\t\t// of [I] receiving a stop signal that was meant for the current one\n\t\t// This is needlessly complicated and is the result of me running\n\t\t// into various deadlocks while trying to fix handling of column\n\t\t// key replication.\n\t\ts.stopInitializeReplicationCh = make(chan struct{})\n\t}\n\t// Stop existing streams.\n\tif err := s.stopTranslationSync(); err != nil {\n\t\treturn errors.Wrap(err, \"stop translation sync\")\n\t}\n\n\t// Create a snapshot of the cluster to use for node/partition calculations.\n\tsnap := s.Cluster.NewSnapshot()\n\n\t// Set read-only flag for all translation stores.\n\ts.setTranslateReadOnlyFlags(snap)\n\n\tif err := s.initializeReplication(snap); err != nil {\n\t\treturn errors.Wrap(err, \"initializing translation replication\")\n\t}\n\treturn nil\n}\n\n////////////////////////////////////////////////////////////\n\n// TranslationSyncer provides an interface allowing a function\n// to notify the server that an action has occurred which requires\n// the translation sync process to be reset. In general, this\n// includes anything which modifies schema (add/remove index, etc),\n// or anything that changes the cluster topology (add/remove node).\n// I originally considered leveraging the broadcaster since that was\n// already in place and provides similar event messages, but the\n// broadcaster is really meant for notifiying other nodes, while\n// this is more akin to an internal message bus. In fact, I think\n// a future iteration on this may be to make it more generic so\n// it can act as an internal message bus where one of the messages\n// being published is \"translationSyncReset\".\ntype TranslationSyncer interface {\n\tReset() error\n}\n\n// NopTranslationSyncer represents a translationSyncer that doesn't do anything.\nvar NopTranslationSyncer TranslationSyncer = &nopTranslationSyncer{}\n\ntype nopTranslationSyncer struct{}\n\n// Reset is a no-op implementation of translationSyncer Reset method.\nfunc (nopTranslationSyncer) Reset() error { return nil }\n\n// activeTranslationSyncer represents a translationSyncer that resets\n// the server's translation syncer.\ntype activeTranslationSyncer struct {\n\tch chan struct{}\n}\n\n// newActiveTranslationSyncer returns a new instance of activeTranslationSyncer.\nfunc newActiveTranslationSyncer(ch chan struct{}) *activeTranslationSyncer {\n\treturn &activeTranslationSyncer{\n\t\tch: ch,\n\t}\n}\n\n// Reset resets the server's translation syncer.\nfunc (a *activeTranslationSyncer) Reset() error {\n\t// just in case some other part of the code has fired\n\t// off a translation sync and it hasn't been received yet\n\t// therefore we don't want to block on send since a.ch is\n\t// (for now) unbuffered. One translationSync is as good as\n\t// another\n\tselect {\n\tcase a.ch <- struct{}{}:\n\tdefault:\n\t}\n\treturn nil\n}\n\n////////////////////////////////////////////////////////////\n\n// stopTranslationSync closes and waits for all outstanding translation readers\n// to complete. This should be called before reconnecting to the cluster in case\n// of a cluster resize or schema change.\nfunc (s *holderSyncer) stopTranslationSync() error {\n\ts.readersMu.Lock()\n\tdefer func() {\n\t\ts.readers = nil // will be populated by initializeReplication\n\t\ts.readersMu.Unlock()\n\t}()\n\t// send signal to stop initializing more readers\n\tif s.pendingReaders > 0 {\n\t\ts.pendingReaders = -1\n\t\tclose(s.stopInitializeReplicationCh)\n\t\ts.stopInitializeReplicationCh = make(chan struct{})\n\t}\n\tvar g errgroup.Group\n\tfor i := range s.readers {\n\t\trd := s.readers[i]\n\t\tg.Go(func() error {\n\t\t\treturn rd.Close()\n\t\t})\n\t}\n\tg.Go(s.syncers.Wait)\n\treturn g.Wait()\n}\n\n// setTranslateReadOnlyFlags updates all translation stores to enable or disable\n// writing new translation keys. Index stores are writable if the node owns the\n// partition. Field stores are writable if the node is the primary.\nfunc (s *holderSyncer) setTranslateReadOnlyFlags(snap *disco.ClusterSnapshot) {\n\ts.Cluster.mu.RLock()\n\tisPrimaryFieldTranslator := snap.IsPrimaryFieldTranslationNode(s.Cluster.Node.ID)\n\n\tfor _, index := range s.Holder.Indexes() {\n\t\t// There is a race condition here:\n\t\t// if Indexes() returns idx1, and then in another\n\t\t// process, holder.DeleteIndex(idx1) is called,\n\t\t// then the next step trying to get TranslateStore(partitionID)\n\t\t// for an index that is closed (and therefore its transateStores\n\t\t// no longer exist) will fail with a nil pointer error.\n\t\t// For now, I just checked that the translateStore hasn't been\n\t\t// set to nil before trying to use it, but another option may\n\t\t// be to prevent the translateStores from being zeroed out\n\t\t// while this process is active. Checking for nil as we do\n\t\t// really obviates the need for the RLock around the for loop.\n\n\t\t// Obtain a read lock on index to prevent Index.Close() from\n\t\t// destroying the Index.translateStores map before this is\n\t\t// done using it.\n\t\t//\n\t\t// Update: there was another path down to Index.Close(), so\n\t\t// we shrink to lock to be inside index.TranslateStore() now.\n\t\tfor partitionID := 0; partitionID < snap.PartitionN; partitionID++ {\n\t\t\tprimary := snap.PrimaryPartitionNode(partitionID)\n\t\t\tisPrimary := primary != nil && s.Node.ID == primary.ID\n\n\t\t\tif ts := index.TranslateStore(partitionID); ts != nil {\n\t\t\t\tts.SetReadOnly(!isPrimary)\n\t\t\t}\n\t\t}\n\n\t\tfor _, field := range index.Fields() {\n\t\t\tfield.TranslateStore().SetReadOnly(!isPrimaryFieldTranslator)\n\t\t}\n\t}\n\ts.Cluster.mu.RUnlock()\n}\n\n// initializeReplication builds a map of nodes for which we need to replicate\n// any key translation, whether that's field keys (every node replicates these\n// from the primary) or index keys (only the replica nodes for each partition\n// replicate these from whichever node is primary for that partition).\nfunc (s *holderSyncer) initializeReplication(snap *disco.ClusterSnapshot) error {\n\tnodeMaps := make(map[string]TranslateOffsetMap)\n\tif snap.ReplicaN > 1 {\n\t\tif err := s.populateIndexReplication(nodeMaps, snap); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif err := s.populateFieldReplication(nodeMaps, snap); err != nil {\n\t\treturn err\n\t}\n\n\t// filter out empty nodes\n\tnodes := make(map[*disco.Node]bool)\n\tfor _, node := range snap.Nodes {\n\t\tm := nodeMaps[node.ID]\n\t\tif !m.Empty() {\n\t\t\tnodes[node] = true\n\t\t}\n\t}\n\n\t// connect to remote nodes and set up readers\n\treadersCh := make(chan TranslateEntryReader, len(nodes))\n\tctx, cancelAddingMoreReaders := context.WithCancel(context.Background())\n\tdefer cancelAddingMoreReaders()\n\ts.readersMu.Lock()\n\ts.pendingReaders = len(nodes)\n\ts.readersMu.Unlock()\n\tgo func() {\n\t\tfor {\n\t\t\tfor node := range nodes {\n\t\t\t\t// check if ctx cancelled\n\t\t\t\t// this means there was a signal sent to stop further init\n\t\t\t\t// of readers\n\t\t\t\tselect {\n\t\t\t\tcase <-s.Closing:\n\t\t\t\t\treturn\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\tclose(readersCh)\n\t\t\t\t\treturn\n\t\t\t\tdefault:\n\t\t\t\t}\n\t\t\t\t// connect to remote node\n\t\t\t\tm := nodeMaps[node.ID]\n\t\t\t\trd, err := s.Holder.OpenTranslateReader(context.Background(), node.URI.String(), m)\n\t\t\t\tif err != nil {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\treadersCh <- rd\n\t\t\t\tdelete(nodes, node)\n\t\t\t}\n\t\t\tif len(nodes) == 0 {\n\t\t\t\tclose(readersCh)\n\t\t\t\treturn\n\t\t\t}\n\t\t\ttime.Sleep(10 * time.Second)\n\n\t\t}\n\t}()\n\n\tfor {\n\t\tselect {\n\t\tcase <-s.Closing:\n\t\t\tcancelAddingMoreReaders()\n\t\t\treturn nil\n\t\tcase <-s.stopInitializeReplicationCh:\n\t\t\treturn nil\n\t\tcase rd, ok := <-readersCh:\n\t\t\t// all translate readers have been launched, hence channel is\n\t\t\t// closed\n\t\t\tif !ok {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\ts.readersMu.Lock()\n\t\t\t// [S] has been initiated and acquired the lock first\n\t\t\t// at this point we should close the reader we've recieved rather\n\t\t\t// than start replication on it.\n\t\t\t// [S] should have already closed all the rest\n\t\t\t// of the reads if they were still in action.\n\t\t\t// we are also draining the channel since the signal for stopping\n\t\t\t// further replication was meant for us\n\t\t\tif s.pendingReaders == -1 {\n\t\t\t\trd.Close()\n\t\t\t\tcancelAddingMoreReaders()\n\t\t\tdrain:\n\t\t\t\tfor {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase <-s.stopInitializeReplicationCh:\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tbreak drain\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\ts.readersMu.Unlock()\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\ts.pendingReaders--\n\t\t\ts.readers = append(s.readers, rd)\n\t\t\ts.syncers.Go(func() error {\n\t\t\t\tdefer rd.Close()\n\t\t\t\ts.readBothTranslateReader(rd, snap)\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\ts.readersMu.Unlock()\n\t\t}\n\t}\n}\n\n// populateFieldReplication populates a map from node IDs to TranslateOffsetMaps\n// to record that we need to translate fields which have key translation\n// from the primary node.\nfunc (s *holderSyncer) populateFieldReplication(nodeMaps map[string]TranslateOffsetMap, snap *disco.ClusterSnapshot) error {\n\t// Set up field translation\n\tif !snap.IsPrimaryFieldTranslationNode(s.Cluster.Node.ID) {\n\t\tprimaryID := snap.PrimaryFieldTranslationNode().ID\n\t\t// Build a map of field key offsets to stream from.\n\t\tm := nodeMaps[primaryID]\n\t\tif m == nil {\n\t\t\tm = make(TranslateOffsetMap)\n\t\t\tnodeMaps[primaryID] = m\n\t\t}\n\t\tfor _, index := range s.Holder.Indexes() {\n\t\t\tfor _, field := range index.Fields() {\n\t\t\t\tstore := field.TranslateStore()\n\t\t\t\t// I think right now this is supposed to be impossible;\n\t\t\t\t// we use an InMemTranslateStore by default even if\n\t\t\t\t// no translate store is being used or attempted.\n\t\t\t\tif store == nil {\n\t\t\t\t\treturn fmt.Errorf(\"no translate store for field %q/%q\", index.Name(), field.Name())\n\t\t\t\t}\n\t\t\t\toffset, err := store.MaxID()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errors.Wrapf(err, \"cannot determine max id for %q/%q\", index.Name(), field.Name())\n\t\t\t\t}\n\t\t\t\tm.SetFieldOffset(index.Name(), field.Name(), offset)\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// populateIndexReplication populates a map of node IDs to TranslateOffsetMaps\n// to record which nodes we need to replicate index key translation for.\n// That means nodes which are the primary for a partition that we're a\n// non-primary replica for.\nfunc (s *holderSyncer) populateIndexReplication(nodeMaps map[string]TranslateOffsetMap, snap *disco.ClusterSnapshot) error {\n\tfor _, node := range snap.Nodes {\n\t\tif node.ID == s.Node.ID {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Build a map of partition offsets to stream from.\n\t\tm := make(TranslateOffsetMap)\n\t\tfor _, index := range s.Holder.Indexes() {\n\t\t\tif !index.Keys() {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfor partitionID := 0; partitionID < snap.PartitionN; partitionID++ {\n\t\t\t\tpartitionNodes := snap.PartitionNodes(partitionID)\n\t\t\t\tisPrimary := partitionNodes[0].ID == node.ID                       // remote is primary?\n\t\t\t\tisReplica := disco.Nodes(partitionNodes[1:]).ContainsID(s.Node.ID) // local is replica?\n\t\t\t\tif !isPrimary || !isReplica {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tstore := index.TranslateStore(partitionID)\n\t\t\t\tif store == nil {\n\t\t\t\t\treturn fmt.Errorf(\"no store available for index %q, partition %d\", index.Name(), partitionID)\n\t\t\t\t}\n\t\t\t\toffset, err := store.MaxID()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errors.Wrapf(err, \"cannot determine max id for %q\", index.Name())\n\t\t\t\t}\n\t\t\t\tm.SetIndexPartitionOffset(index.Name(), partitionID, offset)\n\t\t\t}\n\t\t}\n\n\t\t// Skip if no replication required.\n\t\tif len(m) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tnodeMaps[node.ID] = m\n\t}\n\treturn nil\n}\n\n// readBothTranslateReader reads key translation for field keys or\n// index keys from a remote node. Both field and index keys may be sent,\n// the distinction is that field keys have a non-empty field name.\nfunc (s *holderSyncer) readBothTranslateReader(rd TranslateEntryReader, snap *disco.ClusterSnapshot) {\n\tfor {\n\t\tvar entry TranslateEntry\n\t\tif err := rd.ReadEntry(&entry); err != nil {\n\t\t\ts.Holder.Logger.Errorf(\"cannot read translate entry: %s\", err)\n\t\t\treturn\n\t\t}\n\n\t\tvar store TranslateStore\n\t\tif entry.Field != \"\" {\n\t\t\t// Find appropriate store.\n\t\t\tf := s.Holder.Field(entry.Index, entry.Field)\n\t\t\tif f == nil {\n\t\t\t\ts.Holder.Logger.Errorf(\"field not found: %s/%s\", entry.Index, entry.Field)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tstore = f.TranslateStore()\n\t\t\tif store == nil {\n\t\t\t\ts.Holder.Logger.Errorf(\"no translate store suitable for index %q, field %q, key %q\", entry.Index, entry.Field, entry.Key)\n\t\t\t\treturn\n\t\t\t}\n\t\t} else {\n\t\t\t// Find appropriate store.\n\t\t\tidx := s.Holder.Index(entry.Index)\n\t\t\tif idx == nil {\n\t\t\t\ts.Holder.Logger.Errorf(\"index not found: %q\", entry.Index)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tstore = idx.TranslateStore(snap.KeyToKeyPartition(entry.Index, entry.Key))\n\t\t\tif store == nil {\n\t\t\t\ts.Holder.Logger.Errorf(\"no translate store suitable for index %q, key %q\", entry.Index, entry.Key)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\t// Apply replication to store.\n\t\tif err := store.ForceSet(entry.ID, entry.Key); err != nil {\n\t\t\ts.Holder.Logger.Errorf(\"cannot force set field translation data: %d=%q\", entry.ID, entry.Key)\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc uint64InSlice(i uint64, s []uint64) bool {\n\tfor _, o := range s {\n\t\tif i == o {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// used by Index.openFields(), enabling Tx / Txf by telling\n// the holder about its own indexes.\nfunc (h *Holder) addIndex(idx *Index) {\n\th.imu.Lock()\n\th.indexes[idx.name] = idx\n\th.imu.Unlock()\n}\n\nfunc (h *Holder) Txf() *TxFactory {\n\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\treturn h.txf\n}\n\n// BeginTx starts a transaction on the holder. The index and shard\n// must be specified.\nfunc (h *Holder) BeginTx(writable bool, idx *Index, shard uint64) (Tx, error) {\n\treturn h.txf.NewTx(Txo{Write: writable, Index: idx, Shard: shard}), nil\n}\n\nfunc decodeCreateIndexMessage(ser Serializer, b []byte) (*CreateIndexMessage, error) {\n\tvar cim CreateIndexMessage\n\tif err := ser.Unmarshal(b, &cim); err != nil {\n\t\treturn nil, errors.Wrap(err, \"unmarshaling\")\n\t}\n\treturn &cim, nil\n}\n\nfunc decodeCreateFieldMessage(ser Serializer, b []byte) (*CreateFieldMessage, error) {\n\tvar cfm CreateFieldMessage\n\tif err := ser.Unmarshal(b, &cfm); err != nil {\n\t\treturn nil, errors.Wrap(err, \"unmarshaling\")\n\t}\n\treturn &cfm, nil\n}\n\nfunc (h *Holder) DeleteDataframe(name string) error {\n\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\n\t// Confirm index exists.\n\tidx := h.Index(name)\n\tif idx == nil {\n\t\treturn newNotFoundError(ErrIndexNotFound, name)\n\t}\n\n\tpath := idx.DataframesPath()\n\t// Delete dataframe directory.\n\tif err := os.RemoveAll(path); err != nil {\n\t\t// There is a rare edge case here: If a cache flush was happening, RemoveAll\n\t\t// can fail because a file gets created, say in a fragment directory, after\n\t\t// RemoveAll has deleted everything it found in the directory, but before\n\t\t// the actual directory is unlinked. In theory, though, this can't happen\n\t\t// twice; by the time we get here, everything was closed, so at most one\n\t\t// more file should get created.\n\t\terr = os.RemoveAll(path)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"removing directory\")\n\t\t}\n\t}\n\tif err := os.Mkdir(path, 0o750); err != nil {\n\t\treturn errors.Wrap(err, \"creating dataframe\")\n\t}\n\n\treturn nil\n}\n"
        },
        {
          "name": "holder_internal_test.go",
          "type": "blob",
          "size": 2.216796875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"testing\"\n)\n\nfunc setupTest(t *testing.T, h *Holder, rowCol []rowCols, indexName string) (*Index, *Field) {\n\tidx, err := h.CreateIndexIfNotExists(indexName, \"\", IndexOptions{TrackExistence: true})\n\tif err != nil {\n\t\tt.Fatalf(\"failed to create index %v: %v\", indexName, err)\n\t}\n\tf, err := idx.CreateFieldIfNotExists(\"f\", \"\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to create field in index %v: %v\", indexName, err)\n\t}\n\texistencefield := idx.existenceFld\n\n\tqcx := h.Txf().NewWritableQcx()\n\tdefer qcx.Abort()\n\n\tfor _, r := range rowCol {\n\t\t_, err = f.SetBit(qcx, r.row, r.col, nil)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"failed to set bit in index %v: %v\", indexName, err)\n\t\t}\n\n\t\t_, err = existencefield.SetBit(qcx, r.row, r.col, nil)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"failed to set bit in index %v: %v\", indexName, err)\n\t\t}\n\t}\n\n\tif err = qcx.Finish(); err != nil {\n\t\tt.Fatalf(\"failed to commit tx for index %v: %v\", indexName, err)\n\t}\n\n\tshardsFound := idx.AvailableShards(includeRemote).Slice()\n\tif len(shardsFound) != 3 {\n\t\tt.Fatalf(\"expected 3 shards for index %v, got %v\", indexName, len(shardsFound))\n\t}\n\treturn idx, f\n}\n\ntype rowCols struct {\n\trow uint64\n\tcol uint64\n}\n\nfunc TestHolder_ProcessDeleteInflight(t *testing.T) {\n\th := newTestHolder(t)\n\n\trowCol := []rowCols{\n\t\t{1, 1},\n\t\t{1, 2},\n\t\t{10, ShardWidth + 1},\n\t\t{1, ShardWidth * 2},\n\t}\n\n\tidx1, f1 := setupTest(t, h, rowCol, \"idxdelete1\")\n\tidx2, f2 := setupTest(t, h, rowCol, \"idxdelete2\")\n\n\terr := h.processDeleteInflight()\n\tif err != nil {\n\t\tt.Fatalf(\"failed to delete: %v\", err)\n\t}\n\n\ttests := []struct {\n\t\tidx *Index\n\t\tf   *Field\n\t}{\n\t\t{idx1, f1},\n\t\t{idx2, f2},\n\t}\n\n\tfor _, test := range tests {\n\t\tfunc() {\n\t\t\tidx, f := test.idx, test.f\n\t\t\tqcx := h.Txf().NewQcx()\n\t\t\tdefer qcx.Abort()\n\t\t\tfor _, r := range rowCol {\n\t\t\t\trow, err := f.Row(qcx, r.row)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"failed to get row: %v\", err)\n\t\t\t\t}\n\t\t\t\texistenceRow, err := idx.existenceFld.Row(qcx, r.row)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"failed to get row: %v\", err)\n\t\t\t\t}\n\t\t\t\tif len(row.Columns()) != 0 || len(existenceRow.Columns()) != 0 {\n\t\t\t\t\tt.Fatalf(\"expected columns for fields to be empty after delete\")\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\n\t}\n}\n"
        },
        {
          "name": "holder_test.go",
          "type": "blob",
          "size": 5.78125,
          "content": "// Copyright 2021 Molecula Corp. All rights reserved.\npackage pilosa_test\n\nimport (\n\t\"os\"\n\t\"strings\"\n\t\"testing\"\n\n\tpilosa \"github.com/featurebasedb/featurebase/v3\"\n\t\"github.com/featurebasedb/featurebase/v3/test\"\n\t\"github.com/pkg/errors\"\n)\n\nfunc TestHolder_Open(t *testing.T) {\n\tt.Run(\"ErrIndexPermission\", func(t *testing.T) {\n\t\tif os.Geteuid() == 0 {\n\t\t\tt.Skip(\"Skipping permissions test since user is root.\")\n\t\t}\n\t\t// Manual open because MustOpenHolder closes automatically and fails the test on\n\t\t// double-close.\n\t\th := test.NewHolder(t)\n\t\terr := h.Open()\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"opening holder: %v\", err)\n\t\t}\n\t\t// no automatic close here, because we manually close this, and then\n\t\t// *fail* to reopen it.\n\n\t\tif _, err := h.CreateIndex(\"test\", \"\", pilosa.IndexOptions{}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := h.Close(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if err := os.Chmod(h.IndexPath(\"test\"), 0000); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tdefer func() {\n\t\t\t_ = os.Chmod(h.IndexPath(\"test\"), 0755)\n\t\t}()\n\n\t\tif err := h.Reopen(); err == nil || !strings.Contains(err.Error(), \"permission denied\") {\n\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t}\n\t})\n\tt.Run(\"ForeignIndex\", func(t *testing.T) {\n\t\tt.Run(\"ErrForeignIndexNotFound\", func(t *testing.T) {\n\t\t\th := test.MustOpenHolder(t)\n\n\t\t\tif idx, err := h.CreateIndex(\"foo\", \"\", pilosa.IndexOptions{}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else {\n\t\t\t\t_, err := idx.CreateField(\"bar\", \"\", pilosa.OptFieldTypeInt(0, 100), pilosa.OptFieldForeignIndex(\"nonexistent\"))\n\t\t\t\tif err == nil {\n\t\t\t\t\tt.Fatalf(\"expected error: %s\", pilosa.ErrForeignIndexNotFound)\n\t\t\t\t} else if errors.Cause(err) != pilosa.ErrForeignIndexNotFound {\n\t\t\t\t\tt.Fatalf(\"expected error: %s, but got: %s\", pilosa.ErrForeignIndexNotFound, err)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\n\t\t// Foreign index zzz is opened after foo/bar.\n\t\tt.Run(\"ForeignIndexNotOpenYet\", func(t *testing.T) {\n\t\t\th := test.MustOpenHolder(t)\n\n\t\t\tif _, err := h.CreateIndex(\"zzz\", \"\", pilosa.IndexOptions{}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if idx, err := h.CreateIndex(\"foo\", \"\", pilosa.IndexOptions{}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if _, err := idx.CreateField(\"bar\", \"\", pilosa.OptFieldTypeInt(0, 100), pilosa.OptFieldForeignIndex(\"zzz\")); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if err := h.Holder.Close(); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tif err := h.Reopen(); err != nil {\n\t\t\t\tt.Fatalf(\"unexpected error: %s\", err)\n\t\t\t}\n\t\t})\n\n\t\t// Foreign index aaa is opened before foo/bar.\n\t\tt.Run(\"ForeignIndexIsOpen\", func(t *testing.T) {\n\t\t\th := test.MustOpenHolder(t)\n\n\t\t\tif _, err := h.CreateIndex(\"aaa\", \"\", pilosa.IndexOptions{}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if idx, err := h.CreateIndex(\"foo\", \"\", pilosa.IndexOptions{}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if _, err := idx.CreateField(\"bar\", \"\", pilosa.OptFieldTypeInt(0, 100), pilosa.OptFieldForeignIndex(\"aaa\")); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if err := h.Holder.Close(); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tif err := h.Reopen(); err != nil {\n\t\t\t\tt.Fatalf(\"unexpected error: %s\", err)\n\t\t\t}\n\t\t})\n\n\t\t// Try to re-create existing index\n\t\tt.Run(\"CreateIndexIfNotExists\", func(t *testing.T) {\n\t\t\th := test.MustOpenHolder(t)\n\n\t\t\tidx1, err := h.CreateIndexIfNotExists(\"aaa\", \"\", pilosa.IndexOptions{})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tif _, err = h.CreateIndex(\"aaa\", \"\", pilosa.IndexOptions{}); err == nil {\n\t\t\t\tt.Fatalf(\"expected: ConflictError, got: nil\")\n\t\t\t} else if _, ok := err.(pilosa.ConflictError); !ok {\n\t\t\t\tt.Fatalf(\"expected: ConflictError, got: %s\", err)\n\t\t\t}\n\n\t\t\tidx2, err := h.CreateIndexIfNotExists(\"aaa\", \"\", pilosa.IndexOptions{})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tif idx1 != idx2 {\n\t\t\t\tt.Fatalf(\"expected the same indexes, got: %s and %s\", idx1.Name(), idx2.Name())\n\t\t\t}\n\t\t})\n\t})\n}\n\nfunc TestHolder_HasData(t *testing.T) {\n\tt.Run(\"IndexDirectory\", func(t *testing.T) {\n\t\th := test.MustOpenHolder(t)\n\n\t\tif ok, err := h.HasData(); ok || err != nil {\n\t\t\tt.Fatal(\"expected HasData to return false, no err, but\", ok, err)\n\t\t}\n\n\t\tif _, err := h.CreateIndex(\"test\", \"\", pilosa.IndexOptions{}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif ok, err := h.HasData(); !ok || err != nil {\n\t\t\tt.Fatal(\"expected HasData to return true, but \", ok, err)\n\t\t}\n\t})\n\n\tt.Run(\"Peek\", func(t *testing.T) {\n\t\th := test.MustOpenHolder(t)\n\n\t\tif ok, err := h.HasData(); ok || err != nil {\n\t\t\tt.Fatal(\"expected HasData to return false, no err, but\", ok, err)\n\t\t}\n\n\t\t// Create an index directory to indicate data exists.\n\t\tif err := os.Mkdir(h.IndexPath(\"test\"), 0750); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif ok, err := h.HasData(); !ok || err != nil {\n\t\t\tt.Fatal(\"expected HasData to return true, no err, but\", ok, err)\n\t\t}\n\t})\n\n\tt.Run(\"Peek at missing directory\", func(t *testing.T) {\n\t\t// Ensure that hasData is false when dir doesn't exist.\n\n\t\t// Note that we are intentionally not using test.NewHolder,\n\t\t// because we want to create a Holder object with an invalid path,\n\t\t// rather than creating a valid holder with a temporary path.\n\t\th := pilosa.NewHolder(\"bad-path\", pilosa.TestHolderConfig())\n\n\t\tif ok, err := h.HasData(); ok || err != nil {\n\t\t\tt.Fatal(\"expected HasData to return false, no err, but\", ok, err)\n\t\t}\n\t})\n}\n\n// Ensure holder can delete an index and its underlying files.\nfunc TestHolder_DeleteIndex(t *testing.T) {\n\n\thldr := test.MustOpenHolder(t)\n\n\t// Write bits to separate indexes.\n\thldr.SetBit(\"i0\", \"f\", 100, 200)\n\thldr.SetBit(\"i1\", \"f\", 100, 200)\n\n\t// Ensure i0 exists.\n\tif _, err := os.Stat(hldr.IndexPath(\"i0\")); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Delete i0.\n\tif err := hldr.DeleteIndex(\"i0\"); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Ensure i0 files are removed & i1 still exists.\n\tif _, err := os.Stat(hldr.IndexPath(\"i0\")); !os.IsNotExist(err) {\n\t\tt.Fatal(\"expected i0 file deletion\")\n\t} else if _, err := os.Stat(hldr.IndexPath(\"i1\")); err != nil {\n\t\tt.Fatal(\"expected i1 files to still exist\", err)\n\t}\n}\n"
        },
        {
          "name": "http_handler.go",
          "type": "blob",
          "size": 133.0390625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto/tls\"\n\t\"encoding/gob\"\n\t\"encoding/hex\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"log\"\n\t\"math\"\n\t\"mime\"\n\t\"net\"\n\t\"net/http\"\n\t_ \"net/http/pprof\" // Imported for its side-effect of registering pprof endpoints with the server.\n\t\"net/url\"\n\t\"os\"\n\t\"reflect\"\n\t\"runtime/debug\"\n\t\"runtime/pprof\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/apache/arrow/go/v10/arrow\"\n\t\"github.com/featurebasedb/featurebase/v3/authn\"\n\t\"github.com/featurebasedb/featurebase/v3/authz\"\n\tfbcontext \"github.com/featurebasedb/featurebase/v3/context\"\n\t\"github.com/featurebasedb/featurebase/v3/dax\"\n\t\"github.com/featurebasedb/featurebase/v3/disco\"\n\t\"github.com/featurebasedb/featurebase/v3/logger\"\n\t\"github.com/featurebasedb/featurebase/v3/monitor\"\n\t\"github.com/featurebasedb/featurebase/v3/pql\"\n\t\"github.com/featurebasedb/featurebase/v3/rbf\"\n\t\"github.com/featurebasedb/featurebase/v3/sql3/planner/types\"\n\t\"github.com/featurebasedb/featurebase/v3/storage\"\n\t\"github.com/featurebasedb/featurebase/v3/tracing\"\n\t\"github.com/featurebasedb/featurebase/v3/wireprotocol\"\n\t\"github.com/felixge/fgprof\"\n\t\"github.com/gorilla/handlers\"\n\t\"github.com/gorilla/mux\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\tdto \"github.com/prometheus/client_model/go\"\n\t\"github.com/prometheus/prom2json\"\n\tuuid \"github.com/satori/go.uuid\"\n\t\"github.com/zeebo/blake3\"\n)\n\nconst (\n\t// HeaderRequestUserID is request userid header\n\tHeaderRequestUserID = \"X-Request-Userid\"\n)\n\n// Handler represents an HTTP handler.\ntype Handler struct {\n\tHandler http.Handler\n\n\tfileSystem FileSystem\n\n\tlogger logger.Logger\n\n\tqueryLogger logger.Logger\n\n\t// Keeps the query argument validators for each handler\n\tvalidators map[string]*queryValidationSpec\n\n\tapi *API\n\n\tln net.Listener\n\t// url is used to hold the advertise bind address for printing a log during startup.\n\turl string\n\n\tcloseTimeout time.Duration\n\n\tserializer        Serializer\n\troaringSerializer Serializer\n\n\tserver *http.Server\n\n\tmiddleware []func(http.Handler) http.Handler\n\n\tpprofCPUProfileBuffer *bytes.Buffer\n\n\tauth *authn.Auth\n\n\tpermissions *authz.GroupPermissions\n}\n\n// externalPrefixFlag denotes endpoints that are intended to be exposed to clients.\n// This is used for stats tagging.\nvar externalPrefixFlag = map[string]bool{\n\t\"schema\":  true,\n\t\"query\":   true,\n\t\"import\":  true,\n\t\"export\":  true,\n\t\"index\":   true,\n\t\"field\":   true,\n\t\"nodes\":   true,\n\t\"version\": true,\n}\n\nconst (\n\t// OriginalIPHeader is the original IP for client\n\t// It is used mainly for authenticating on remote nodes\n\t// ForwardedIPHeader gets updated to the node's IP\n\t// when requests are forward to other nodes in the cluster\n\tOriginalIPHeader = \"X-Molecula-Original-IP\"\n\n\t// ForwardedIPHeader is part of the standard header\n\t// it is used to identify the originating IP of a client\n\tForwardedIPHeader = \"X-Forwarded-For\"\n\n\t// AllowedNetworksGroupName is used for the admin group authorization\n\t// when authentication is completed through checking the client IP\n\t// against the allowed networks\n\tAllowedNetworksGroupName = \"allowed-networks\"\n)\n\ntype errorResponse struct {\n\tError string `json:\"error\"`\n}\n\n// handlerOption is a functional option type for Handler\ntype handlerOption func(s *Handler) error\n\nfunc OptHandlerMiddleware(middleware func(http.Handler) http.Handler) handlerOption {\n\treturn func(h *Handler) error {\n\t\th.middleware = append(h.middleware, middleware)\n\t\treturn nil\n\t}\n}\n\nfunc OptHandlerAllowedOrigins(origins []string) handlerOption {\n\treturn func(h *Handler) error {\n\t\th.middleware = append(h.middleware, handlers.CORS(\n\t\t\thandlers.AllowedOrigins(origins),\n\t\t\thandlers.AllowedHeaders([]string{\"Content-Type\"}),\n\t\t))\n\t\treturn nil\n\t}\n}\n\nfunc OptHandlerAPI(api *API) handlerOption {\n\treturn func(h *Handler) error {\n\t\th.api = api\n\t\treturn nil\n\t}\n}\n\nfunc OptHandlerAuthN(authn *authn.Auth) handlerOption {\n\treturn func(h *Handler) error {\n\t\th.auth = authn\n\t\treturn nil\n\t}\n}\n\nfunc OptHandlerAuthZ(gp *authz.GroupPermissions) handlerOption {\n\treturn func(h *Handler) error {\n\t\th.permissions = gp\n\t\treturn nil\n\t}\n}\n\nfunc OptHandlerFileSystem(fs FileSystem) handlerOption {\n\treturn func(h *Handler) error {\n\t\th.fileSystem = fs\n\t\treturn nil\n\t}\n}\n\nfunc OptHandlerLogger(logger logger.Logger) handlerOption {\n\treturn func(h *Handler) error {\n\t\th.logger = logger\n\t\treturn nil\n\t}\n}\n\nfunc OptHandlerQueryLogger(logger logger.Logger) handlerOption {\n\treturn func(h *Handler) error {\n\t\th.queryLogger = logger\n\t\treturn nil\n\t}\n}\n\nfunc OptHandlerSerializer(s Serializer) handlerOption {\n\treturn func(h *Handler) error {\n\t\th.serializer = s\n\t\treturn nil\n\t}\n}\n\nfunc OptHandlerRoaringSerializer(s Serializer) handlerOption {\n\treturn func(h *Handler) error {\n\t\th.roaringSerializer = s\n\t\treturn nil\n\t}\n}\n\n// OptHandlerListener set the listener that will be used by the HTTP server.\n// Url must be the advertised URL. It will be used to show a log to the user\n// about where the Web UI is. This option is mandatory.\nfunc OptHandlerListener(ln net.Listener, url string) handlerOption {\n\treturn func(h *Handler) error {\n\t\th.ln = ln\n\t\th.url = url\n\t\treturn nil\n\t}\n}\n\n// OptHandlerCloseTimeout controls how long to wait for the http Server to\n// shutdown cleanly before forcibly destroying it. Default is 30 seconds.\nfunc OptHandlerCloseTimeout(d time.Duration) handlerOption {\n\treturn func(h *Handler) error {\n\t\th.closeTimeout = d\n\t\treturn nil\n\t}\n}\n\nvar (\n\tmakeImportOk sync.Once\n\timportOk     []byte\n)\n\n// NewHandler returns a new instance of Handler with a default logger.\nfunc NewHandler(opts ...handlerOption) (*Handler, error) {\n\thandler := &Handler{\n\t\tfileSystem:   NopFileSystem,\n\t\tlogger:       logger.NopLogger,\n\t\tcloseTimeout: time.Second * 30,\n\t}\n\n\tfor _, opt := range opts {\n\t\terr := opt(handler)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"applying option\")\n\t\t}\n\t}\n\tif handler.serializer == nil || handler.roaringSerializer == nil {\n\t\treturn nil, errors.New(\"must use serializer options when creating handler\")\n\t}\n\tmakeImportOk.Do(func() {\n\t\tvar err error\n\t\timportOk, err = handler.serializer.Marshal(&ImportResponse{Err: \"\"})\n\t\tif err != nil {\n\t\t\tpanic(fmt.Sprintf(\"trying to cache import-OK response: %v\", err))\n\t\t}\n\t})\n\n\t// if OptHandlerFileSystem is used, it must be before newRouter is called\n\thandler.Handler = newRouter(handler)\n\thandler.populateValidators()\n\n\tif handler.api == nil {\n\t\treturn nil, errors.New(\"must pass OptHandlerAPI\")\n\t}\n\n\tif handler.ln == nil {\n\t\treturn nil, errors.New(\"must pass OptHandlerListener\")\n\t}\n\n\thandler.server = &http.Server{Handler: handler}\n\n\treturn handler, nil\n}\n\nfunc (h *Handler) Serve() error {\n\terr := h.server.Serve(h.ln)\n\tif err != nil && err.Error() != \"http: Server closed\" {\n\t\th.logger.Errorf(\"HTTP handler terminated with error: %s\\n\", err)\n\t\treturn errors.Wrap(err, \"serve http\")\n\t}\n\treturn nil\n}\n\n// Close tries to cleanly shutdown the HTTP server, and failing that, after a\n// timeout, calls Server.Close.\nfunc (h *Handler) Close() error {\n\tdeadlineCtx, cancelFunc := context.WithDeadline(context.Background(), time.Now().Add(h.closeTimeout))\n\tdefer cancelFunc()\n\terr := h.server.Shutdown(deadlineCtx)\n\tif err != nil {\n\t\terr = h.server.Close()\n\t}\n\treturn errors.Wrap(err, \"shutdown/close http server\")\n}\n\nfunc (h *Handler) populateValidators() {\n\th.validators = map[string]*queryValidationSpec{}\n\th.validators[\"GetExport\"] = queryValidationSpecRequired(\"index\", \"field\", \"shard\")\n\th.validators[\"GetIndexes\"] = queryValidationSpecRequired()\n\th.validators[\"GetIndex\"] = queryValidationSpecRequired()\n\th.validators[\"PostIndex\"] = queryValidationSpecRequired()\n\th.validators[\"DeleteIndex\"] = queryValidationSpecRequired()\n\th.validators[\"GetTranslateData\"] = queryValidationSpecRequired(\"index\").Optional(\"partition\", \"field\")\n\th.validators[\"PostTranslateKeys\"] = queryValidationSpecRequired()\n\th.validators[\"PostField\"] = queryValidationSpecRequired()\n\th.validators[\"DeleteField\"] = queryValidationSpecRequired()\n\th.validators[\"PostImport\"] = queryValidationSpecRequired().Optional(\"clear\", \"ignoreKeyCheck\")\n\th.validators[\"PostImportAtomicRecord\"] = queryValidationSpecRequired().Optional(\"simPowerLossAfter\")\n\th.validators[\"PostImportRoaring\"] = queryValidationSpecRequired().Optional(\"remote\", \"clear\")\n\th.validators[\"PostQuery\"] = queryValidationSpecRequired().Optional(\"shards\", \"excludeColumns\", \"profile\", \"remote\")\n\th.validators[\"GetInfo\"] = queryValidationSpecRequired()\n\th.validators[\"RecalculateCaches\"] = queryValidationSpecRequired()\n\th.validators[\"GetSchema\"] = queryValidationSpecRequired().Optional(\"views\")\n\th.validators[\"PostSchema\"] = queryValidationSpecRequired().Optional(\"remote\")\n\th.validators[\"GetStatus\"] = queryValidationSpecRequired()\n\th.validators[\"GetVersion\"] = queryValidationSpecRequired()\n\th.validators[\"PostClusterMessage\"] = queryValidationSpecRequired()\n\th.validators[\"GetFragmentBlockData\"] = queryValidationSpecRequired()\n\th.validators[\"GetFragmentBlocks\"] = queryValidationSpecRequired(\"index\", \"field\", \"view\", \"shard\")\n\th.validators[\"GetFragmentData\"] = queryValidationSpecRequired(\"index\", \"field\", \"view\", \"shard\")\n\th.validators[\"GetFragmentNodes\"] = queryValidationSpecRequired(\"shard\", \"index\")\n\th.validators[\"GetPartitionNodes\"] = queryValidationSpecRequired(\"partition\")\n\th.validators[\"GetNodes\"] = queryValidationSpecRequired()\n\th.validators[\"GetShardMax\"] = queryValidationSpecRequired()\n\th.validators[\"GetTransactionList\"] = queryValidationSpecRequired()\n\th.validators[\"GetTransactions\"] = queryValidationSpecRequired()\n\th.validators[\"GetTransaction\"] = queryValidationSpecRequired()\n\th.validators[\"PostTransaction\"] = queryValidationSpecRequired()\n\th.validators[\"PostFinishTransaction\"] = queryValidationSpecRequired()\n\th.validators[\"DeleteDataframe\"] = queryValidationSpecRequired()\n}\n\ntype contextKeyQuery int\n\nconst (\n\tcontextKeyQueryRequest contextKeyQuery = iota\n\tcontextKeyQueryError\n\tcontextKeyGroupMembership\n)\n\n// addQueryContext puts the results of handler.readQueryRequest into the Context for use by\n// both other middleware and any handlers.\nfunc (h *Handler) addQueryContext(next http.Handler) http.Handler {\n\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tpathParts := strings.Split(r.URL.Path, \"/\")\n\t\tif len(pathParts) > 3 && pathParts[3] == \"query\" {\n\t\t\treq, err := h.readQueryRequest(r)\n\t\t\tctx := context.WithValue(r.Context(), contextKeyQueryRequest, req)\n\t\t\tctx = context.WithValue(ctx, contextKeyQueryError, err)\n\t\t\tnext.ServeHTTP(w, r.WithContext(ctx))\n\t\t} else {\n\t\t\tnext.ServeHTTP(w, r)\n\t\t}\n\t})\n}\n\nfunc (h *Handler) queryArgValidator(next http.Handler) http.Handler {\n\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tkey := mux.CurrentRoute(r).GetName()\n\n\t\tif validator, ok := h.validators[key]; ok {\n\t\t\tif err := validator.validate(r.URL.Query()); err != nil {\n\t\t\t\terrText := err.Error()\n\t\t\t\tif validHeaderAcceptJSON(r.Header) {\n\t\t\t\t\tresponse := errorResponse{Error: errText}\n\t\t\t\t\tdata, err := json.Marshal(response)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\th.logger.Errorf(\"failed to encode error %q as JSON: %v\", errText, err)\n\t\t\t\t\t} else {\n\t\t\t\t\t\terrText = string(data)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\thttp.Error(w, errText, http.StatusBadRequest)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tnext.ServeHTTP(w, r)\n\t})\n}\n\nfunc (h *Handler) extractTracing(next http.Handler) http.Handler {\n\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tspan, ctx := tracing.GlobalTracer.ExtractHTTPHeaders(r)\n\t\tspan.LogKV(\"http.url\", r.URL.String())\n\t\tdefer span.Finish()\n\n\t\tnext.ServeHTTP(w, r.WithContext(ctx))\n\t})\n}\n\nfunc (h *Handler) collectStats(next http.Handler) http.Handler {\n\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tt := time.Now()\n\t\tnext.ServeHTTP(w, r)\n\t\tdur := time.Since(t)\n\n\t\tisSlow := \"false\"\n\t\tlongQueryTime := h.api.LongQueryTime()\n\t\tif longQueryTime > 0 && dur > longQueryTime {\n\t\t\tqueryRequest := r.Context().Value(contextKeyQueryRequest)\n\n\t\t\tvar queryString string\n\t\t\tif req, ok := queryRequest.(*QueryRequest); ok {\n\t\t\t\tqueryString = req.Query\n\t\t\t}\n\n\t\t\th.logger.Printf(\"HTTP query duration %v exceeds %v: %s %s %s\", dur, longQueryTime, r.Method, r.URL.String(), queryString)\n\t\t\tisSlow = \"true\"\n\t\t}\n\n\t\twhere := \"\"\n\t\tpathParts := strings.Split(r.URL.Path, \"/\")\n\t\tif externalPrefixFlag[pathParts[1]] {\n\t\t\twhere = \"external\"\n\t\t} else {\n\t\t\twhere = \"internal\"\n\t\t}\n\t\tpath, err := mux.CurrentRoute(r).GetPathTemplate()\n\t\tif err != nil {\n\t\t\tpath = \"\"\n\t\t}\n\t\tSummaryHttpRequests.With(prometheus.Labels{\n\t\t\t\"method\":    r.Method,\n\t\t\t\"path\":      path,\n\t\t\t\"slow\":      isSlow,\n\t\t\t\"useragent\": r.UserAgent(),\n\t\t\t\"where\":     where,\n\t\t}).Observe(dur.Seconds())\n\t})\n}\n\nfunc (h *Handler) monitorPerformance(next http.Handler) http.Handler {\n\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tif !monitor.IsOn() {\n\t\t\tnext.ServeHTTP(w, r)\n\t\t\treturn\n\t\t}\n\t\t// %% begin sonarcloud ignore %%\n\t\tprefixes := make(map[string]struct{})\n\t\tprefixes[\"index\"] = struct{}{}\n\t\tprefixes[\"info\"] = struct{}{}\n\t\tprefixes[\"schema\"] = struct{}{}\n\t\tprefixes[\"status\"] = struct{}{}\n\t\tprefixes[\"queries\"] = struct{}{}\n\t\tprefixes[\"query-history\"] = struct{}{}\n\n\t\tpathParts := strings.Split(r.URL.Path, \"/\")\n\t\t// checks < 5 to exclude import endpoints for now from sentry transactions as\n\t\t// there could potentially be many of them. Pricing is per transaction.\n\t\tif len(pathParts) > 1 && len(pathParts) < 5 {\n\t\t\tif _, ok := prefixes[pathParts[1]]; ok {\n\t\t\t\tpath := scrubPath(pathParts)\n\t\t\t\tspan := monitor.StartSpan(r.Context(), \"HTTP\", path)\n\t\t\t\tqreq := r.Context().Value(contextKeyQueryRequest)\n\t\t\t\treq, ok := qreq.(*QueryRequest)\n\t\t\t\tif ok && req != nil {\n\t\t\t\t\tspan.SetTag(\"PQL Query\", req.Query)\n\t\t\t\t\tspan.SetTag(\"SQL Query\", req.SQLQuery)\n\t\t\t\t\tspan.SetTag(\"Index\", mux.Vars(r)[\"index\"])\n\t\t\t\t}\n\t\t\t\tnext.ServeHTTP(w, r)\n\t\t\t\tspan.Finish()\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tnext.ServeHTTP(w, r)\n\t\t// %% end sonarcloud ignore %%\n\t})\n}\n\nfunc scrubPath(pathParts []string) string {\n\tl := len(pathParts)\n\tif l > 1 && pathParts[1] == \"index\" {\n\t\tswitch {\n\t\tcase l > 4:\n\t\t\tpathParts[4] = \"{field}\"\n\t\t\tfallthrough\n\t\tcase l > 2:\n\t\t\tpathParts[2] = \"{index}\"\n\t\t}\n\t}\n\treturn strings.Join(pathParts, \"/\")\n}\n\n// latticeRoutes lists the frontend routes that do not directly correspond to\n// backend routes, and require special handling.\nvar latticeRoutes = []string{\"/tables\", \"/query\", \"/querybuilder\", \"/signin\"} // TODO somehow pull this from some metadata in the lattice directory\n\n// newRouter creates a new mux http router.\nfunc newRouter(handler *Handler) http.Handler {\n\trouter := mux.NewRouter()\n\n\t// TODO: figure out how to protect these if needed\n\trouter.PathPrefix(\"/debug/pprof/\").Handler(http.DefaultServeMux).Methods(\"GET\")\n\trouter.PathPrefix(\"/debug/fgprof\").Handler(fgprof.Handler()).Methods(\"GET\")\n\trouter.Handle(\"/metrics\", promhttp.Handler())\n\n\trouter.HandleFunc(\"/metrics.json\", handler.chkAuthZ(handler.handleGetMetricsJSON, authz.Admin)).Methods(\"GET\").Name(\"GetMetricsJSON\")\n\trouter.HandleFunc(\"/export\", handler.chkAuthZ(handler.handleGetExport, authz.Read)).Methods(\"GET\").Name(\"GetExport\")\n\trouter.HandleFunc(\"/import-atomic-record\", handler.chkAuthZ(handler.handlePostImportAtomicRecord, authz.Admin)).Methods(\"POST\").Name(\"PostImportAtomicRecord\")\n\trouter.HandleFunc(\"/index\", handler.chkAuthZ(handler.handleGetIndexes, authz.Read)).Methods(\"GET\").Name(\"GetIndexes\")\n\trouter.HandleFunc(\"/index\", handler.chkAuthZ(handler.handlePostIndex, authz.Admin)).Methods(\"POST\").Name(\"PostIndex\")\n\trouter.HandleFunc(\"/index/\", handler.chkAuthZ(handler.handlePostIndex, authz.Admin)).Methods(\"POST\").Name(\"PostIndex\")\n\trouter.HandleFunc(\"/index/{index}\", handler.chkAuthZ(handler.handleGetIndex, authz.Read)).Methods(\"GET\").Name(\"GetIndex\")\n\trouter.HandleFunc(\"/index/{index}\", handler.chkAuthZ(handler.handlePostIndex, authz.Admin)).Methods(\"POST\").Name(\"PostIndex\")\n\trouter.HandleFunc(\"/index/{index}\", handler.chkAuthZ(handler.handleDeleteIndex, authz.Admin)).Methods(\"DELETE\").Name(\"DeleteIndex\")\n\trouter.HandleFunc(\"/index/{index}/dataframe/{shard}\", handler.chkAuthZ(handler.handlePostDataframe, authz.Write)).Methods(\"POST\").Name(\"PostDataframe\")\n\trouter.HandleFunc(\"/index/{index}/dataframe/{shard}\", handler.chkAuthZ(handler.handleGetDataframe, authz.Read)).Methods(\"GET\").Name(\"GetDataframe\")\n\trouter.HandleFunc(\"/index/{index}/dataframe\", handler.chkAuthZ(handler.handleGetDataframeSchema, authz.Read)).Methods(\"GET\").Name(\"GetDataframeSchema\")\n\trouter.HandleFunc(\"/index/{index}/dataframe\", handler.chkAuthZ(handler.handleDeleteDataframe, authz.Write)).Methods(\"DELETE\").Name(\"DeleteDataframe\")\n\trouter.HandleFunc(\"/index/{index}/field\", handler.chkAuthZ(handler.handlePostField, authz.Write)).Methods(\"POST\").Name(\"PostField\")\n\trouter.HandleFunc(\"/index/{index}/field/\", handler.chkAuthZ(handler.handlePostField, authz.Write)).Methods(\"POST\").Name(\"PostField\")\n\trouter.HandleFunc(\"/index/{index}/field/{field}/view\", handler.chkAuthZ(handler.handleGetView, authz.Admin)).Methods(\"GET\")\n\trouter.HandleFunc(\"/index/{index}/field/{field}/view/{view}\", handler.chkAuthZ(handler.handleDeleteView, authz.Admin)).Methods(\"DELETE\").Name(\"DeleteView\")\n\trouter.HandleFunc(\"/index/{index}/field/{field}\", handler.chkAuthZ(handler.handlePostField, authz.Write)).Methods(\"POST\").Name(\"PostField\")\n\trouter.HandleFunc(\"/index/{index}/field/{field}\", handler.chkAuthZ(handler.handlePatchField, authz.Write)).Methods(\"PATCH\").Name(\"PatchField\")\n\trouter.HandleFunc(\"/index/{index}/field/{field}\", handler.chkAuthZ(handler.handleDeleteField, authz.Write)).Methods(\"DELETE\").Name(\"DeleteField\")\n\trouter.HandleFunc(\"/index/{index}/field/{field}/import\", handler.chkAuthZ(handler.handlePostImport, authz.Write)).Methods(\"POST\").Name(\"PostImport\")\n\trouter.HandleFunc(\"/index/{index}/field/{field}/mutex-check\", handler.chkAuthZ(handler.handleGetMutexCheck, authz.Read)).Methods(\"GET\").Name(\"GetMutexCheck\")\n\trouter.HandleFunc(\"/index/{index}/field/{field}/import-roaring/{shard}\", handler.chkAuthZ(handler.handlePostImportRoaring, authz.Write)).Methods(\"POST\").Name(\"PostImportRoaring\")\n\trouter.HandleFunc(\"/index/{index}/shard/{shard}/import-roaring\", handler.chkAuthZ(handler.handlePostShardImportRoaring, authz.Write)).Methods(\"POST\").Name(\"PostImportRoaring\")\n\trouter.HandleFunc(\"/index/{index}/query\", handler.chkAuthZ(handler.handlePostQuery, authz.Read)).Methods(\"POST\").Name(\"PostQuery\")\n\trouter.HandleFunc(\"/info\", handler.chkAuthZ(handler.handleGetInfo, authz.Admin)).Methods(\"GET\").Name(\"GetInfo\")\n\trouter.HandleFunc(\"/recalculate-caches\", handler.chkAuthZ(handler.handleRecalculateCaches, authz.Admin)).Methods(\"POST\").Name(\"RecalculateCaches\")\n\trouter.HandleFunc(\"/schema\", handler.chkAuthZ(handler.handleGetSchema, authz.Read)).Methods(\"GET\").Name(\"GetSchema\")\n\trouter.HandleFunc(\"/schema/details\", handler.chkAuthZ(handler.handleGetSchemaDetails, authz.Read)).Methods(\"GET\").Name(\"GetSchemaDetails\")\n\trouter.HandleFunc(\"/schema\", handler.chkAuthZ(handler.handlePostSchema, authz.Admin)).Methods(\"POST\").Name(\"PostSchema\")\n\trouter.HandleFunc(\"/status\", handler.chkAuthZ(handler.handleGetStatus, authz.Read)).Methods(\"GET\").Name(\"GetStatus\")\n\trouter.HandleFunc(\"/transaction\", handler.chkAuthZ(handler.handlePostTransaction, authz.Read)).Methods(\"POST\").Name(\"PostTransaction\")\n\trouter.HandleFunc(\"/transaction/\", handler.chkAuthZ(handler.handlePostTransaction, authz.Read)).Methods(\"POST\").Name(\"PostTransaction\")\n\trouter.HandleFunc(\"/transaction/{id}\", handler.chkAuthZ(handler.handleGetTransaction, authz.Read)).Methods(\"GET\").Name(\"GetTransaction\")\n\trouter.HandleFunc(\"/transaction/{id}\", handler.chkAuthZ(handler.handlePostTransaction, authz.Read)).Methods(\"POST\").Name(\"PostTransaction\")\n\trouter.HandleFunc(\"/transaction/{id}/finish\", handler.chkAuthZ(handler.handlePostFinishTransaction, authz.Read)).Methods(\"POST\").Name(\"PostFinishTransaction\")\n\trouter.HandleFunc(\"/transactions\", handler.chkAuthZ(handler.handleGetTransactions, authz.Read)).Methods(\"GET\").Name(\"GetTransactions\")\n\trouter.HandleFunc(\"/queries\", handler.chkAuthZ(handler.handleGetActiveQueries, authz.Admin)).Methods(\"GET\").Name(\"GetActiveQueries\")\n\n\trouter.HandleFunc(\"/sql\", handler.chkAuthZ(handler.handlePostSQL, authz.Admin)).Methods(\"POST\").Name(\"PostSQL\")\n\t// internal endpoint\n\trouter.HandleFunc(\"/sql-exec-graph\", handler.chkAuthZ(handler.handlePostSQLPlanOperator, authz.Admin)).Methods(\"POST\").Name(\"PostSQLPlanOperator\")\n\n\trouter.HandleFunc(\"/query-history\", handler.chkAuthZ(handler.handleGetPastQueries, authz.Admin)).Methods(\"GET\").Name(\"GetPastQueries\")\n\trouter.HandleFunc(\"/version\", handler.handleGetVersion).Methods(\"GET\").Name(\"GetVersion\")\n\n\t// /ui endpoints are for UI use; they may change at any time.\n\trouter.HandleFunc(\"/ui/transaction\", handler.chkAuthZ(handler.handleGetTransactionList, authz.Read)).Methods(\"GET\").Name(\"GetTransactionList\")\n\trouter.HandleFunc(\"/ui/transaction/\", handler.chkAuthZ(handler.handleGetTransactionList, authz.Read)).Methods(\"GET\").Name(\"GetTransactionList\")\n\trouter.HandleFunc(\"/ui/shard-distribution\", handler.chkAuthZ(handler.handleGetShardDistribution, authz.Admin)).Methods(\"GET\").Name(\"GetShardDistribution\")\n\n\t// /internal endpoints are for internal use only; they may change at any time.\n\t// DO NOT rely on these for external applications!\n\n\t// Truly used internally by featurebase\n\trouter.HandleFunc(\"/internal/cluster/message\", handler.chkInternal(handler.handlePostClusterMessage)).Methods(\"POST\").Name(\"PostClusterMessage\")\n\trouter.HandleFunc(\"/internal/translate/data\", handler.chkAuthZ(handler.handleGetTranslateData, authz.Read)).Methods(\"GET\").Name(\"GetTranslateData\")\n\trouter.HandleFunc(\"/internal/translate/data\", handler.chkAuthZ(handler.handlePostTranslateData, authz.Write)).Methods(\"POST\").Name(\"PostTranslateData\")\n\n\t// other ones\n\trouter.HandleFunc(\"/internal/mem-usage\", handler.chkAuthZ(handler.handleGetMemUsage, authz.Read)).Methods(\"GET\").Name(\"GetUsage\")\n\trouter.HandleFunc(\"/internal/disk-usage\", handler.chkAuthZ(handler.handleGetDiskUsage, authz.Read)).Methods(\"GET\").Name(\"GetUsage\")\n\trouter.HandleFunc(\"/internal/disk-usage/{index}\", handler.chkAuthZ(handler.handleGetDiskUsage, authz.Read)).Methods(\"GET\").Name(\"GetUsage\")\n\trouter.HandleFunc(\"/internal/fragment/block/data\", handler.chkAuthN(handler.handleGetFragmentBlockData)).Methods(\"GET\").Name(\"GetFragmentBlockData\")\n\trouter.HandleFunc(\"/internal/fragment/blocks\", handler.chkAuthN(handler.handleGetFragmentBlocks)).Methods(\"GET\").Name(\"GetFragmentBlocks\")\n\trouter.HandleFunc(\"/internal/fragment/data\", handler.chkAuthN(handler.handleGetFragmentData)).Methods(\"GET\").Name(\"GetFragmentData\")\n\trouter.HandleFunc(\"/internal/fragment/nodes\", handler.chkAuthN(handler.handleGetFragmentNodes)).Methods(\"GET\").Name(\"GetFragmentNodes\")\n\trouter.HandleFunc(\"/internal/partition/nodes\", handler.chkAuthN(handler.handleGetPartitionNodes)).Methods(\"GET\").Name(\"GetPartitionNodes\")\n\trouter.HandleFunc(\"/internal/translate/keys\", handler.chkAuthN(handler.handlePostTranslateKeys)).Methods(\"POST\").Name(\"PostTranslateKeys\")\n\trouter.HandleFunc(\"/internal/translate/ids\", handler.chkAuthN(handler.handlePostTranslateIDs)).Methods(\"POST\").Name(\"PostTranslateIDs\")\n\trouter.HandleFunc(\"/internal/index/{index}/field/{field}/mutex-check\", handler.chkAuthZ(handler.handleInternalGetMutexCheck, authz.Read)).Methods(\"GET\").Name(\"InternalGetMutexCheck\")\n\trouter.HandleFunc(\"/internal/index/{index}/field/{field}/remote-available-shards/{shardID}\", handler.chkAuthZ(handler.handleDeleteRemoteAvailableShard, authz.Admin)).Methods(\"DELETE\")\n\trouter.HandleFunc(\"/internal/index/{index}/shard/{shard}/snapshot\", handler.chkAuthZ(handler.handleGetIndexShardSnapshot, authz.Read)).Methods(\"GET\").Name(\"GetIndexShardSnapshot\")\n\trouter.HandleFunc(\"/internal/index/{index}/shards\", handler.chkAuthZ(handler.handleGetIndexAvailableShards, authz.Read)).Methods(\"GET\").Name(\"GetIndexAvailableShards\")\n\trouter.HandleFunc(\"/internal/nodes\", handler.chkAuthN(handler.handleGetNodes)).Methods(\"GET\").Name(\"GetNodes\")\n\trouter.HandleFunc(\"/internal/shards/max\", handler.chkAuthN(handler.handleGetShardsMax)).Methods(\"GET\").Name(\"GetShardsMax\") // TODO: deprecate, but it's being used by the client\n\n\trouter.HandleFunc(\"/internal/translate/index/{index}/keys/find\", handler.chkAuthZ(handler.handleFindIndexKeys, authz.Admin)).Methods(\"POST\").Name(\"FindIndexKeys\")\n\trouter.HandleFunc(\"/internal/translate/index/{index}/keys/create\", handler.chkAuthZ(handler.handleCreateIndexKeys, authz.Admin)).Methods(\"POST\").Name(\"CreateIndexKeys\")\n\trouter.HandleFunc(\"/internal/translate/index/{index}/{partition}\", handler.chkAuthZ(handler.handlePostTranslateIndexDB, authz.Admin)).Methods(\"POST\").Name(\"PostTranslateIndexDB\")\n\trouter.HandleFunc(\"/internal/translate/field/{index}/{field}\", handler.chkAuthZ(handler.handlePostTranslateFieldDB, authz.Admin)).Methods(\"POST\").Name(\"PostTranslateFieldDB\")\n\trouter.HandleFunc(\"/internal/translate/field/{index}/{field}/keys/find\", handler.chkAuthZ(handler.handleFindFieldKeys, authz.Admin)).Methods(\"POST\").Name(\"FindFieldKeys\")\n\trouter.HandleFunc(\"/internal/translate/field/{index}/{field}/keys/create\", handler.chkAuthZ(handler.handleCreateFieldKeys, authz.Admin)).Methods(\"POST\").Name(\"CreateFieldKeys\")\n\trouter.HandleFunc(\"/internal/translate/field/{index}/{field}/keys/like\", handler.chkAuthZ(handler.handleMatchField, authz.Read)).Methods(\"POST\").Name(\"MatchFieldKeys\")\n\n\trouter.HandleFunc(\"/internal/idalloc/reserve\", handler.chkAuthN(handler.handleReserveIDs)).Methods(\"POST\").Name(\"ReserveIDs\")\n\trouter.HandleFunc(\"/internal/idalloc/commit\", handler.chkAuthN(handler.handleCommitIDs)).Methods(\"POST\").Name(\"CommitIDs\")\n\trouter.HandleFunc(\"/internal/idalloc/restore\", handler.chkAuthN(handler.handleRestoreIDAlloc)).Methods(\"POST\").Name(\"RestoreIDAllocData\")\n\trouter.HandleFunc(\"/internal/idalloc/reset/{index}\", handler.chkAuthN(handler.handleResetIDAlloc)).Methods(\"POST\").Name(\"ResetIDAlloc\")\n\trouter.HandleFunc(\"/internal/idalloc/data\", handler.chkAuthN(handler.handleIDAllocData)).Methods(\"GET\").Name(\"IDAllocData\")\n\n\trouter.HandleFunc(\"/internal/restore/{index}/{shardID}\", handler.chkAuthZ(handler.handlePostRestore, authz.Admin)).Methods(\"POST\").Name(\"Restore\")\n\trouter.HandleFunc(\"/internal/dataframe/restore/{index}/{shardID}\", handler.chkAuthZ(handler.handlePostDataframeRestore, authz.Admin)).Methods(\"POST\").Name(\"Restore\")\n\n\trouter.HandleFunc(\"/internal/debug/rbf\", handler.chkAuthZ(handler.handleGetInternalDebugRBFJSON, authz.Admin)).Methods(\"GET\").Name(\"GetInternalDebugRBFJSON\")\n\n\t// endpoints for collecting cpu profiles from a chosen begin point to\n\t// when the client wants to stop. Used for profiling imports that\n\t// could be long or short.\n\trouter.HandleFunc(\"/cpu-profile/start\", handler.chkAuthZ(handler.handleCPUProfileStart, authz.Admin)).Methods(\"GET\").Name(\"CPUProfileStart\")\n\trouter.HandleFunc(\"/cpu-profile/stop\", handler.chkAuthZ(handler.handleCPUProfileStop, authz.Admin)).Methods(\"GET\").Name(\"CPUProfileStop\")\n\n\trouter.HandleFunc(\"/login\", handler.handleLogin).Methods(\"GET\").Name(\"Login\")\n\trouter.HandleFunc(\"/logout\", handler.handleLogout).Methods(\"GET\").Name(\"Logout\")\n\trouter.HandleFunc(\"/redirect\", handler.handleRedirect).Methods(\"GET\").Name(\"Redirect\")\n\trouter.HandleFunc(\"/auth\", handler.handleCheckAuthentication).Methods(\"GET\").Name(\"CheckAuthentication\")\n\trouter.HandleFunc(\"/userinfo\", handler.handleUserInfo).Methods(\"GET\").Name(\"UserInfo\")\n\trouter.HandleFunc(\"/internal/oauth-config\", handler.handleOAuthConfig).Methods(\"GET\").Name(\"GetOAuthConfig\")\n\n\trouter.HandleFunc(\"/health\", handler.handleGetHealth).Methods(\"GET\").Name(\"GetHealth\")\n\trouter.HandleFunc(\"/directive\", handler.handleGetDirective).Methods(\"GET\").Name(\"GetDirective\")\n\trouter.HandleFunc(\"/directive\", handler.handlePostDirective).Methods(\"POST\").Name(\"PostDirective\")\n\trouter.HandleFunc(\"/snapshot/shard-data\", handler.handlePostSnapshotShardData).Methods(\"POST\").Name(\"PostShapshotShardData\")\n\trouter.HandleFunc(\"/snapshot/table-keys\", handler.handlePostSnapshotTableKeys).Methods(\"POST\").Name(\"PostShapshotTableKeys\")\n\trouter.HandleFunc(\"/snapshot/field-keys\", handler.handlePostSnapshotFieldKeys).Methods(\"POST\").Name(\"PostShapshotFieldKeys\")\n\n\t// Endpoints to support lattice UI embedded via statik.\n\t// The messiness here reflects the fact that assets live in a nontrivial\n\t// directory structure that is controlled externally.\n\tlatticeHandler := newStatikHandler(handler)\n\trouter.PathPrefix(\"/static\").Handler(latticeHandler)\n\trouter.Path(\"/\").Handler(latticeHandler)\n\trouter.Path(\"/favicon.png\").Handler(latticeHandler)\n\trouter.Path(\"/favicon.svg\").Handler(latticeHandler)\n\trouter.Path(\"/manifest.json\").Handler(latticeHandler)\n\tfor _, route := range latticeRoutes {\n\t\trouter.Path(route).Handler(latticeHandler)\n\t}\n\n\trouter.Use(handler.queryArgValidator)\n\trouter.Use(handler.addQueryContext)\n\trouter.Use(handler.extractTracing)\n\trouter.Use(handler.monitorPerformance)\n\trouter.Use(handler.collectStats)\n\tvar h http.Handler = router\n\tfor _, middleware := range handler.middleware {\n\t\t// Ideally, we would use `router.Use` to inject middleware,\n\t\t// instead of wrapping the handler. The reason we can't is\n\t\t// because the router will only apply middleware to matched\n\t\t// handlers. In this case, it won't match handlers with the\n\t\t// OPTIONS method, needed by the CORS middleware. This issue\n\t\t// is described in detail here:\n\t\t// https://github.com/gorilla/handlers/issues/142\n\t\th = middleware(h)\n\t}\n\treturn h\n}\n\n// ServeHTTP handles an HTTP request.\nfunc (h *Handler) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n\tdefer func() {\n\t\tif err := recover(); err != nil {\n\t\t\tw.WriteHeader(http.StatusInternalServerError)\n\t\t\tstack := debug.Stack()\n\t\t\tmsg := \"%s\\n%s\"\n\t\t\th.logger.Panicf(msg, err, stack)\n\t\t\tfmt.Fprintf(w, msg, err, stack)\n\t\t}\n\t}()\n\n\th.Handler.ServeHTTP(w, r)\n}\n\nfunc (h *Handler) chkInternal(handler http.HandlerFunc) http.HandlerFunc {\n\treturn func(w http.ResponseWriter, r *http.Request) {\n\t\tif h.auth != nil {\n\t\t\tsecret, ok := r.Header[\"X-Feature-Key\"]\n\t\t\tvar secretString string\n\t\t\tif ok {\n\t\t\t\tsecretString = secret[0]\n\t\t\t}\n\t\t\tdecodedString, err := hex.DecodeString(secretString)\n\t\t\tif err != nil || !ok || !bytes.Equal(decodedString, h.auth.SecretKey()) {\n\t\t\t\thttp.Error(w, \"internal secret key validation failed\", http.StatusUnauthorized)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\thandler.ServeHTTP(w, r)\n\t}\n}\n\nfunc (h *Handler) chkAllowedNetworks(r *http.Request) (bool, context.Context) {\n\t// for every request, get IP of the request and check against configured IPs\n\treqIP := GetIP(r)\n\tif reqIP == \"\" {\n\t\treturn false, r.Context()\n\t}\n\n\t// if client IP is in allowed networks\n\t// add it to the context for key X-Molecula-Original-IP\n\tif h.auth.CheckAllowedNetworks(reqIP) {\n\t\tctx := fbcontext.WithOriginalIP(r.Context(), reqIP)\n\t\treturn true, ctx\n\t}\n\treturn false, r.Context()\n}\n\nfunc (h *Handler) chkAuthN(handler http.HandlerFunc) http.HandlerFunc {\n\treturn func(w http.ResponseWriter, r *http.Request) {\n\t\tctx := r.Context()\n\n\t\t// if the request is unauthenticated and we have the appropriate header get the userid from the header\n\t\trequestUserID := r.Header.Get(HeaderRequestUserID)\n\t\tctx = fbcontext.WithUserID(ctx, requestUserID)\n\n\t\tif h.auth == nil {\n\t\t\thandler.ServeHTTP(w, r.WithContext(ctx))\n\t\t\treturn\n\t\t}\n\n\t\t// if IP is in allowed networks, then serve the request\n\t\tallowedNetwork, ctx := h.chkAllowedNetworks(r)\n\t\tif allowedNetwork {\n\t\t\thandler.ServeHTTP(w, r.WithContext(ctx))\n\t\t\treturn\n\t\t}\n\n\t\taccess, refresh := getTokens(r)\n\t\tuinfo, err := h.auth.Authenticate(access, refresh)\n\t\tif err != nil {\n\t\t\thttp.Error(w, errors.Wrap(err, \"authenticating\").Error(), http.StatusUnauthorized)\n\t\t\treturn\n\t\t}\n\n\t\t// prefer the user id from an authenticated request over one in a header\n\t\tctx = fbcontext.WithUserID(ctx, uinfo.UserID)\n\n\t\t// just in case it got refreshed\n\t\tctx = authn.WithAccessToken(ctx, \"Bearer\"+access)\n\t\tctx = authn.WithRefreshToken(ctx, refresh)\n\t\th.auth.SetCookie(w, uinfo.Token, uinfo.RefreshToken, uinfo.Expiry)\n\n\t\thandler.ServeHTTP(w, r.WithContext(ctx))\n\t}\n}\n\nfunc (h *Handler) chkAuthZ(handler http.HandlerFunc, perm authz.Permission) http.HandlerFunc {\n\treturn func(w http.ResponseWriter, r *http.Request) {\n\t\tctx := r.Context()\n\n\t\t// if the request is unauthenticated and we have the appropriate header get the userid from the header\n\t\trequestUserID := r.Header.Get(HeaderRequestUserID)\n\t\tctx = fbcontext.WithUserID(ctx, requestUserID)\n\n\t\t// handle the case when auth is not turned on\n\t\tif h.auth == nil {\n\t\t\thandler.ServeHTTP(w, r.WithContext(ctx))\n\t\t\treturn\n\t\t}\n\n\t\t// check if IP is in allowed networks, if yes give it admin permissions\n\t\tallowedNetwork, ctx := h.chkAllowedNetworks(r)\n\t\tif allowedNetwork {\n\t\t\tctx = context.WithValue(ctx, contextKeyGroupMembership, []string{AllowedNetworksGroupName, h.permissions.Admin})\n\t\t\thandler.ServeHTTP(w, r.WithContext(ctx))\n\t\t\treturn\n\t\t}\n\n\t\t// make a copy of the requested permissions\n\t\tlperm := perm\n\n\t\t// check if the user is authenticated\n\t\taccess, refresh := getTokens(r)\n\n\t\tuinfo, err := h.auth.Authenticate(access, refresh)\n\t\tif err != nil {\n\t\t\thttp.Error(w, errors.Wrap(err, \"authenticating\").Error(), http.StatusForbidden)\n\t\t\treturn\n\t\t}\n\n\t\t// prefer the user id from an authenticated request over one in a header\n\t\tctx = fbcontext.WithUserID(ctx, uinfo.UserID)\n\n\t\tctx = authn.WithAccessToken(ctx, \"Bearer \"+access)\n\t\tctx = authn.WithRefreshToken(ctx, refresh)\n\n\t\t// just in case it got refreshed\n\t\th.auth.SetCookie(w, uinfo.Token, uinfo.RefreshToken, uinfo.Expiry)\n\n\t\t// put the user's authN/Z info in the context\n\t\tctx = context.WithValue(ctx, contextKeyGroupMembership, uinfo.Groups)\n\t\tctx = authn.WithAccessToken(ctx, \"Bearer \"+uinfo.Token)\n\t\tctx = authn.WithRefreshToken(ctx, uinfo.RefreshToken)\n\t\t// unlikely h.permissions will be nil, but we'll check to be safe\n\t\tif h.permissions == nil {\n\t\t\th.logger.Errorf(\"authentication is turned on without authorization permissions set\")\n\t\t\thttp.Error(w, \"authorizing\", http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\n\t\t// figure out what the user is querying for\n\t\tqueryString := \"\"\n\t\tqueryRequest := ctx.Value(contextKeyQueryRequest)\n\t\tif req, ok := queryRequest.(*QueryRequest); ok {\n\t\t\tqueryString = req.Query\n\n\t\t\tq, err := pql.ParseString(queryString)\n\t\t\tif err != nil {\n\t\t\t\thttp.Error(w, errors.Wrap(err, \"parsing query string\").Error(), http.StatusBadRequest)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// if there are write calls, and the needed perms don't already\n\t\t\t// satisfy write permissions, then make them write permissions\n\t\t\tif q.WriteCallN() > 0 && !lperm.Satisfies(authz.Write) {\n\t\t\t\tlperm = authz.Write\n\t\t\t}\n\t\t}\n\t\t// make the query string pretty\n\t\tqueryString = strings.Replace(queryString, \"\\n\", \"\", -1)\n\n\t\t// figure out if we should log this query\n\t\ttoLog := true\n\t\tfor _, ep := range []string{\"/status\", \"/metrics\", \"/info\", \"/internal\"} {\n\t\t\tif strings.HasPrefix(r.URL.Path, ep) {\n\t\t\t\ttoLog = false\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif toLog {\n\t\t\th.queryLogger.Infof(\"%v, %v, %v, %v, %v, %v\", GetIP(r), r.UserAgent(), r.URL.Path, uinfo.UserID, uinfo.UserName, queryString)\n\t\t}\n\n\t\t// if they're an admin, they can do whatever they want\n\t\tif h.permissions.IsAdmin(uinfo.Groups) {\n\t\t\thandler.ServeHTTP(w, r.WithContext(ctx))\n\t\t\treturn\n\t\t} else if lperm == authz.Admin {\n\t\t\t// if they're not an admin, and they need to be, we can just\n\t\t\t// error right here\n\t\t\thttp.Error(w, \"Insufficient permissions: user does not have admin permission\", http.StatusForbidden)\n\t\t\treturn\n\t\t}\n\n\t\t// try to get the index name\n\t\tindexName, ok := mux.Vars(r)[\"index\"]\n\t\tif !ok {\n\t\t\tindexName = r.URL.Query().Get(\"index\")\n\t\t}\n\n\t\t// if we have an index name, then we check the user permissions\n\t\t// against that index\n\t\tif indexName != \"\" {\n\t\t\tp, err := h.permissions.GetPermissions(uinfo, indexName)\n\t\t\tif err != nil {\n\t\t\t\tw.Header().Add(\"Content-Type\", \"text/plain\")\n\t\t\t\thttp.Error(w, errors.Wrap(err, \"Insufficient Permissions\").Error(), http.StatusForbidden)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// if they're not permitted to access this index, error\n\t\t\tif !p.Satisfies(lperm) {\n\t\t\t\tw.Header().Add(\"Content-Type\", \"text/plain\")\n\t\t\t\thttp.Error(w, \"Insufficient permissions\", http.StatusForbidden)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\thandler.ServeHTTP(w, r.WithContext(ctx))\n\t}\n}\n\nfunc GetIP(r *http.Request) string {\n\t// check if original IP was set in the request\n\tog := r.Header.Get(OriginalIPHeader)\n\tif og != \"\" {\n\t\treturn og\n\t}\n\n\t// check if original IP is in the context\n\tif ogIP, ok := fbcontext.OriginalIP(r.Context()); ok && ogIP != \"\" {\n\t\treturn ogIP\n\t}\n\n\t// X-Forwarded-For can have multiple IPs\n\t// the first IP will always be the originating client IP\n\t// the remaining IPs will be for any proxies the request went through\n\tforwarded := r.Header.Get(ForwardedIPHeader)\n\tforwardedList := strings.Split(forwarded, \",\")\n\tif forwardedList[0] != \"\" {\n\t\treturn forwardedList[0]\n\t}\n\n\treturn r.RemoteAddr\n}\n\n// statikHandler implements the http.Handler interface, and responds to\n// requests for static assets with the appropriate file contents embedded\n// in a statik filesystem.\ntype statikHandler struct {\n\thandler  *Handler\n\tstatikFS http.FileSystem\n}\n\n// newStatikHandler returns a new instance of statikHandler\nfunc newStatikHandler(h *Handler) statikHandler {\n\tfs, err := h.fileSystem.New()\n\tif err == nil {\n\t\th.logger.Printf(\"enabled Web UI at %s\", h.url)\n\t}\n\n\treturn statikHandler{\n\t\thandler:  h,\n\t\tstatikFS: fs,\n\t}\n}\n\nfunc (s statikHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n\tif strings.HasPrefix(r.UserAgent(), \"curl\") {\n\t\tmsg := \"Welcome. FeatureBase v\" + s.handler.api.Version() + \" is running. Visit https://docs.featurebase.com for more information.\"\n\t\tif s.statikFS != nil {\n\t\t\tmsg += \" Try the Web UI by visiting this URL in your browser.\"\n\t\t}\n\t\thttp.Error(w, msg, http.StatusNotFound)\n\t\treturn\n\t}\n\n\tif s.statikFS == nil {\n\t\tmsg := \"Web UI is not available. Please run `make generate-statik` before building Pilosa with `make install`.\"\n\t\ts.handler.logger.Infof(msg)\n\t\thttp.Error(w, msg, http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\t// Without this check, refreshing the UI at e.g. /query\n\t// will request a nonexistent resource and return 404.\n\tfor _, route := range latticeRoutes {\n\t\tif r.URL.String() == route {\n\t\t\turl, _ := url.Parse(\"/\")\n\t\t\tr.URL = url\n\t\t}\n\t}\n\n\thttp.FileServer(s.statikFS).ServeHTTP(w, r)\n}\n\n// successResponse is a general success/error struct for http responses.\ntype successResponse struct {\n\th         *Handler\n\tSuccess   bool       `json:\"success\"`\n\tName      string     `json:\"name,omitempty\"`\n\tCreatedAt int64      `json:\"createdAt,omitempty\"`\n\tError     *HTTPError `json:\"error,omitempty\"`\n}\n\n// HTTPError defines a standard application error.\ntype HTTPError struct {\n\t// Human-readable message.\n\tMessage string `json:\"message\"`\n}\n\n// Error returns the string representation of the error message.\nfunc (e *HTTPError) Error() string {\n\treturn e.Message\n}\n\n// check determines success or failure based on the error.\n// It also returns the corresponding http status code.\nfunc (r *successResponse) check(err error) (statusCode int) {\n\tif err == nil {\n\t\tr.Success = true\n\t\treturn 0\n\t}\n\n\tcause := errors.Cause(err)\n\n\t// Determine HTTP status code based on the error type.\n\tswitch cause.(type) {\n\tcase BadRequestError:\n\t\tstatusCode = http.StatusBadRequest\n\tcase ConflictError:\n\t\tstatusCode = http.StatusConflict\n\tcase NotFoundError:\n\t\t// TODO I think any error matches NotFoundError because it's a `type NotFoundError error`\n\t\tstatusCode = http.StatusNotFound\n\tdefault:\n\t\tstatusCode = http.StatusInternalServerError\n\t}\n\n\tr.Success = false\n\tr.Error = &HTTPError{Message: err.Error()}\n\n\treturn statusCode\n}\n\n// write sends a response to the http.ResponseWriter based on the success\n// status and the error.\nfunc (r *successResponse) write(w http.ResponseWriter, err error) {\n\t// Apply the error and get the status code.\n\tstatusCode := r.check(err)\n\n\t// Marshal the json response.\n\tmsg, err := json.Marshal(r)\n\tif err != nil {\n\t\thttp.Error(w, string(msg), http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\t// Write the response.\n\tif statusCode == 0 {\n\t\tw.Header().Set(\"Content-Type\", \"application/json\")\n\t\t_, err := w.Write(msg)\n\t\tif err != nil {\n\t\t\tr.h.logger.Errorf(\"error writing response: %v\", err)\n\t\t\treturn\n\t\t}\n\t\t_, err = w.Write([]byte(\"\\n\"))\n\t\tif err != nil {\n\t\t\tr.h.logger.Errorf(\"error writing newline after response: %v\", err)\n\t\t\treturn\n\t\t}\n\t} else {\n\t\thttp.Error(w, string(msg), statusCode)\n\t}\n}\n\n// validHeaderAcceptJSON returns false if one or more Accept\n// headers are present, but none of them are \"application/json\"\n// (or any matching wildcard). Otherwise returns true.\nfunc validHeaderAcceptJSON(header http.Header) bool {\n\treturn validHeaderAcceptType(header, \"application\", \"json\")\n}\n\nfunc validHeaderAcceptType(header http.Header, typ, subtyp string) bool {\n\tif v, found := header[\"Accept\"]; found {\n\t\tfor _, v := range v {\n\t\t\tt, _, err := mime.ParseMediaType(v)\n\t\t\tif err != nil {\n\t\t\t\tswitch err {\n\t\t\t\tcase mime.ErrInvalidMediaParameter:\n\t\t\t\t\t// This is an optional feature, so we can keep going anyway.\n\t\t\t\tdefault:\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t\tspl := strings.SplitN(t, \"/\", 2)\n\t\t\tif len(spl) < 2 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tswitch {\n\t\t\tcase spl[0] == typ && spl[1] == subtyp:\n\t\t\t\treturn true\n\t\t\tcase spl[0] == \"*\" && spl[1] == subtyp:\n\t\t\t\treturn true\n\t\t\tcase spl[0] == typ && spl[1] == \"*\":\n\t\t\t\treturn true\n\t\t\tcase spl[0] == \"*\" && spl[1] == \"*\":\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t\treturn false\n\t}\n\treturn true\n}\n\n// headerAcceptRoaringRow tells us that the request should accept roaring\n// rows in response.\nfunc headerAcceptRoaringRow(header http.Header) bool {\n\tfor _, v := range header[\"X-Pilosa-Row\"] {\n\t\tif v == \"roaring\" {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc (h *Handler) filterSchema(schema []*IndexInfo, g []authn.Group) []*IndexInfo {\n\tif !h.permissions.IsAdmin(g) {\n\t\tvar filtered []*IndexInfo\n\t\tallowed := h.permissions.GetAuthorizedIndexList(g, authz.Read)\n\t\tfor _, s := range schema {\n\t\t\tfor _, index := range allowed {\n\t\t\t\tif s.Name == index {\n\t\t\t\t\tfiltered = append(filtered, s)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tschema = filtered\n\t}\n\treturn schema\n}\n\nfunc (h *Handler) getGroupMembership(r *http.Request) (g []authn.Group) {\n\t// if IP is in allowed networks, then give admin group membership\n\tif h.auth.CheckAllowedNetworks(GetIP(r)) {\n\t\tg = []authn.Group{\n\t\t\t{\n\t\t\t\tGroupID:   h.permissions.Admin,\n\t\t\t\tGroupName: AllowedNetworksGroupName,\n\t\t\t},\n\t\t}\n\t\treturn g\n\t}\n\n\t// check if group membership was already set in request when auth token was obtained\n\tg = r.Context().Value(contextKeyGroupMembership).([]authn.Group)\n\treturn g\n}\n\n// handleGetSchema handles GET /schema requests.\nfunc (h *Handler) handleGetSchema(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tq := r.URL.Query()\n\twithViews := q.Get(\"views\") == \"true\"\n\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tschema, err := h.api.Schema(r.Context(), withViews)\n\tif err != nil {\n\t\th.logger.Printf(\"getting schema error: %s\", err)\n\t}\n\n\t// if auth is turned on, filter response to only include authorized indexes\n\tif h.auth != nil {\n\t\tg := h.getGroupMembership(r)\n\t\tif g == nil {\n\t\t\thttp.Error(w, \"Forbidden\", http.StatusForbidden)\n\t\t\treturn\n\t\t}\n\t\tschema = h.filterSchema(schema, g)\n\t}\n\n\tif err := json.NewEncoder(w).Encode(Schema{Indexes: schema}); err != nil {\n\t\th.logger.Errorf(\"write schema response error: %s\", err)\n\t}\n}\n\n// handleGetSchema handles GET /schema/details requests. This is essentially the\n// same thing as a GET /schema request, except WithViews is turned on by default.\n// Previously, /schema/details returned the cardinality of each field, but this was\n// removed for performance reasons. If, at some point in the future, there is a more\n// performant way to get the cardinality of a field, that information would be\n// included here.\nfunc (h *Handler) handleGetSchemaDetails(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tschema, err := h.api.Schema(r.Context(), true)\n\tif err != nil {\n\t\th.logger.Printf(\"error getting detailed schema: %s\", err)\n\t\treturn\n\t}\n\n\t// if auth is turned on, filter response to only include authorized indexes\n\tif h.auth != nil {\n\t\tg := h.getGroupMembership(r)\n\t\tif g == nil {\n\t\t\thttp.Error(w, \"Forbidden\", http.StatusForbidden)\n\t\t\treturn\n\t\t}\n\t\tschema = h.filterSchema(schema, g)\n\t}\n\n\tif err := json.NewEncoder(w).Encode(Schema{Indexes: schema}); err != nil {\n\t\th.logger.Printf(\"write schema response error: %s\", err)\n\t}\n}\n\nfunc (h *Handler) handlePostSchema(w http.ResponseWriter, r *http.Request) {\n\tq := r.URL.Query()\n\tremoteStr := q.Get(\"remote\")\n\tvar remote bool\n\tif remoteStr == \"true\" {\n\t\tremote = true\n\t}\n\n\tschema := &Schema{}\n\tif err := json.NewDecoder(r.Body).Decode(schema); err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"decoding request as JSON Pilosa schema: %v\", err), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tif err := h.api.ApplySchema(r.Context(), schema, remote); err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"apply schema to Pilosa: %v\", err), http.StatusBadRequest)\n\t\treturn\n\t}\n\tw.WriteHeader(http.StatusNoContent)\n}\n\n// handleGetMemUsage handles GET /internal/mem-usage requests.\nfunc (h *Handler) handleGetMemUsage(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tuse, err := GetMemoryUsage()\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tif err := json.NewEncoder(w).Encode(use); err != nil {\n\t\th.logger.Errorf(\"write mem usage response error: %s\", err)\n\t}\n}\n\n// handleGetDiskUsage handles GET /internal/disk-usage requests.\nfunc (h *Handler) handleGetDiskUsage(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tu := h.api.server.dataDir\n\tindexName, ok := mux.Vars(r)[\"index\"]\n\tif ok {\n\t\tu = fmt.Sprintf(\"%s/indexes/%s\", u, indexName)\n\t}\n\n\tuse, err := GetDiskUsage(u)\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tif err := json.NewEncoder(w).Encode(use); err != nil {\n\t\th.logger.Errorf(\"write disk usage response error: %s\", err)\n\t}\n}\n\n// handleGetShardDistribution handles GET /ui/shard-distribution requests.\nfunc (h *Handler) handleGetShardDistribution(w http.ResponseWriter, r *http.Request) {\n\tdist := h.api.ShardDistribution(r.Context())\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tif err := json.NewEncoder(w).Encode(dist); err != nil {\n\t\th.logger.Errorf(\"write status response error: %s\", err)\n\t}\n}\n\n// handleGetStatus handles GET /status requests.\nfunc (h *Handler) handleGetStatus(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tstate, err := h.api.State()\n\tif err != nil {\n\t\thttp.Error(w, \"getting cluster state error: \"+err.Error(), http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\tstatus := getStatusResponse{\n\t\tState:       string(state),\n\t\tNodes:       h.api.Hosts(r.Context()),\n\t\tLocalID:     h.api.Node().ID,\n\t\tClusterName: h.api.ClusterName(),\n\t}\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tif err := json.NewEncoder(w).Encode(status); err != nil {\n\t\th.logger.Errorf(\"write status response error: %s\", err)\n\t}\n}\n\nfunc (h *Handler) handleGetInfo(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\tinfo := h.api.Info()\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tif err := json.NewEncoder(w).Encode(info); err != nil {\n\t\th.logger.Errorf(\"write info response error: %s\", err)\n\t}\n}\n\ntype getSchemaResponse struct {\n\tIndexes []*IndexInfo `json:\"indexes\"`\n}\n\ntype getStatusResponse struct {\n\tState       string        `json:\"state\"`\n\tNodes       []*disco.Node `json:\"nodes\"`\n\tLocalID     string        `json:\"localID\"`\n\tClusterName string        `json:\"clusterName\"`\n}\n\nfunc httpHash(s string) string {\n\thasher := blake3.New()\n\t_, _ = hasher.Write([]byte(s))\n\tvar buf [16]byte\n\t_, _ = hasher.Digest().Read(buf[0:])\n\n\treturn fmt.Sprintf(\"%x\", buf)\n}\n\nvar DoPerQueryProfiling = false\n\n// handlePostQuery handles /query requests.\nfunc (h *Handler) handlePostQuery(w http.ResponseWriter, r *http.Request) {\n\t// Read previouly parsed request from context\n\tqreq := r.Context().Value(contextKeyQueryRequest)\n\tqerr := r.Context().Value(contextKeyQueryError)\n\treq, ok := qreq.(*QueryRequest)\n\n\tif DoPerQueryProfiling {\n\t\tbackend := storage.DefaultBackend\n\t\treqHash := httpHash(req.Query)\n\n\t\tqlen := len(req.Query)\n\t\tif qlen > 100 {\n\t\t\tqlen = 100\n\t\t}\n\t\tname := \"_query.\" + reqHash + \".\" + backend + \".\" + time.Now().Format(\"20060102150405\") + \".\" + req.Query[:qlen]\n\t\tf, err := os.Create(name)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tdefer f.Close()\n\n\t\t_ = pprof.StartCPUProfile(f)\n\t\tdefer pprof.StopCPUProfile()\n\n\t} // end DoPerQueryProfiling\n\n\tvar err error\n\terr, _ = qerr.(error)\n\tif err != nil || !ok {\n\t\tw.WriteHeader(http.StatusBadRequest)\n\t\te := h.writeQueryResponse(w, r, &QueryResponse{Err: err})\n\t\tif e != nil {\n\t\t\th.logger.Errorf(\"write query response error: %v (while trying to write another error: %v)\", e, err)\n\t\t}\n\t\treturn\n\t}\n\t// TODO: Remove\n\treq.Index = mux.Vars(r)[\"index\"]\n\n\tresp, err := h.api.Query(r.Context(), req)\n\tif err != nil {\n\t\tswitch errors.Cause(err) {\n\t\tcase ErrTooManyWrites:\n\t\t\tw.WriteHeader(http.StatusRequestEntityTooLarge)\n\t\tcase ErrTranslateStoreReadOnly:\n\t\t\tu := h.api.PrimaryReplicaNodeURL()\n\t\t\tu.Path, u.RawQuery = r.URL.Path, r.URL.RawQuery\n\t\t\thttp.Redirect(w, r, u.String(), http.StatusFound)\n\t\t\treturn\n\t\tdefault:\n\t\t\tw.WriteHeader(http.StatusBadRequest)\n\t\t}\n\t\te := h.writeQueryResponse(w, r, &QueryResponse{Err: err})\n\t\tif e != nil {\n\t\t\th.logger.Errorf(\"write query response error: %v (while trying to write another error: %v)\", e, err)\n\t\t}\n\t\treturn\n\t}\n\n\t// Set appropriate status code, if there is an error. It doesn't appear that\n\t// resp.Err could ever be set in API.Query, so this code block is probably\n\t// doing nothing right now.\n\tif resp.Err != nil {\n\t\tswitch errors.Cause(resp.Err) {\n\t\tcase ErrTooManyWrites:\n\t\t\tw.WriteHeader(http.StatusRequestEntityTooLarge)\n\t\tdefault:\n\t\t\tw.WriteHeader(http.StatusBadRequest)\n\t\t}\n\t}\n\n\t// Write response back to client.\n\tif err := h.writeQueryResponse(w, r, &resp); err != nil {\n\t\th.logger.Errorf(\"write query response error: %s\", err)\n\t}\n}\n\nfunc (h *Handler) writeBadRequest(w http.ResponseWriter, r *http.Request, err error) {\n\tw.WriteHeader(http.StatusBadRequest)\n\te := h.writeQueryResponse(w, r, &QueryResponse{Err: err})\n\tif e != nil {\n\t\th.logger.Errorf(\"write query response error: %v (while trying to write another error: %v)\", e, err)\n\t}\n}\n\n// handlePostSQLOperator handles an internal sql3 plan operator execution request\n// these requests come from other nodes in the cluster\n// handlePostSQLOperator will 'rehydrate' a plan operator and return data in the\n// featurebase wire format for effciency\n// we do not track these requests as user requests\n// TODO(pok) - thus is there anything we need here to align with how we do this for other nodes\nfunc (h *Handler) handlePostSQLPlanOperator(w http.ResponseWriter, r *http.Request) {\n\twriteError := func(err error) {\n\t\tif err != nil {\n\t\t\tw.Write(wireprotocol.WriteError(err))\n\t\t}\n\t}\n\n\t// always finish with a done message\n\tdefer w.Write(wireprotocol.WriteDone())\n\n\tctx := r.Context()\n\n\trootOperator, err := h.api.RehydratePlanOperator(ctx, r.Body)\n\tif err != nil {\n\t\twriteError(err)\n\t\treturn\n\t}\n\n\t// get a query iterator.\n\titer, err := rootOperator.Iterator(ctx, nil)\n\tif err != nil {\n\t\twriteError(err)\n\t\treturn\n\t}\n\t// read schema & write to response.\n\tcolumns := rootOperator.Schema()\n\tb, err := wireprotocol.WriteSchema(columns)\n\tif err != nil {\n\t\twriteError(err)\n\t\treturn\n\t}\n\tw.Write(b)\n\n\tvar rowErr error\n\tvar currentRow types.Row\n\tvar nextErr error\n\n\tfor currentRow, nextErr = iter.Next(ctx); nextErr == nil; currentRow, nextErr = iter.Next(ctx) {\n\t\tb, err := wireprotocol.WriteRow(currentRow, columns)\n\t\tif err != nil {\n\t\t\trowErr = err\n\t\t\tbreak\n\t\t}\n\t\tw.Write(b)\n\t}\n\tif nextErr != nil && nextErr != types.ErrNoMoreRows {\n\t\trowErr = nextErr\n\t}\n\twriteError(rowErr)\n}\n\n// handlePostSQL handles /sql requests\n// supports a ?plan=true|false parameter to send back the plan in the\n// query response\nfunc (h *Handler) handlePostSQL(w http.ResponseWriter, r *http.Request) {\n\tincludePlan := false\n\tincludePlanValue := r.URL.Query().Get(\"plan\")\n\tif len(includePlanValue) > 0 {\n\t\tvar err error\n\t\tincludePlan, err = strconv.ParseBool(includePlanValue)\n\t\tif err != nil {\n\t\t\th.writeBadRequest(w, r, err)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// get the body\n\tb, err := io.ReadAll(r.Body)\n\tif err != nil {\n\t\th.writeBadRequest(w, r, err)\n\t\treturn\n\t}\n\n\trequestID, err := uuid.NewV4()\n\tif err != nil {\n\t\th.writeBadRequest(w, r, err)\n\t}\n\t// put the requestId in the context\n\tctx := fbcontext.WithRequestID(r.Context(), requestID.String())\n\n\t// update the counter for requests\n\tPerfCounterSQLRequestSec.Add(1)\n\n\t// Write response back to client.\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\n\t// the pandas data frame format in json\n\n\t// Opening bracket.\n\tw.Write([]byte(\"{\"))\n\n\t// Write the closing bracket on any exit from this method.\n\tdefer func() {\n\t\tvar execTime int64\n\t\t// we are going to make best effort here - don't actually care about the error\n\t\t// if there was an error, the request.ElapsedTime will be zero\n\t\trequest, _ := h.api.server.SystemLayer.ExecutionRequests().GetRequest(requestID.String())\n\t\texecTime = request.ElapsedTime.Microseconds()\n\n\t\tvar value []byte\n\t\tvalue, err = json.Marshal(execTime)\n\t\tif err != nil {\n\t\t\tw.Write([]byte(`,\"execution-time\": 0`))\n\t\t} else {\n\t\t\tw.Write([]byte(`,\"execution-time\":`))\n\t\t\tw.Write(value)\n\t\t}\n\t\tw.Write([]byte(\"}\"))\n\t}()\n\n\t// writeError is a helper function that can be called anywhere during the\n\t// output handling to insert an error into the json output.\n\twriteError := func(err error, withComma bool) {\n\t\tif err != nil {\n\t\t\terrMsg, err := json.Marshal(err.Error())\n\t\t\tif err != nil {\n\t\t\t\terrMsg = []byte(`\"PROBLEM ENCODING ERROR MESSAGE\"`)\n\t\t\t}\n\t\t\tif withComma {\n\t\t\t\tw.Write([]byte(`,\"error\":`))\n\t\t\t} else {\n\t\t\t\tw.Write([]byte(`\"error\":`))\n\t\t\t}\n\t\t\tw.Write(errMsg)\n\t\t}\n\t}\n\n\t// writeWarnings is a helper function that can be called anywhere during the\n\t// output handling to insert warnings into the json output.\n\twriteWarnings := func(warnings []string) {\n\t\tif len(warnings) > 0 {\n\t\t\tw.Write([]byte(`,\"warnings\": [`))\n\t\t\tfor i, warn := range warnings {\n\t\t\t\twarnMsg, err := json.Marshal(warn)\n\t\t\t\tif err != nil {\n\t\t\t\t\twarnMsg = []byte(`\"PROBLEM ENCODING WARNING\"`)\n\t\t\t\t}\n\t\t\t\tw.Write(warnMsg)\n\t\t\t\tif i < len(warnings)-1 {\n\t\t\t\t\tw.Write([]byte(`,`))\n\t\t\t\t}\n\t\t\t}\n\t\t\tw.Write([]byte(`]`))\n\t\t}\n\t}\n\n\twritePlan := func(plan map[string]interface{}) {\n\t\tif plan != nil && includePlan {\n\t\t\tplanBytes, err := json.Marshal(plan)\n\t\t\tif err != nil {\n\t\t\t\tplanBytes = []byte(`\"PROBLEM ENCODING QUERY PLAN\"`)\n\t\t\t}\n\t\t\tw.Write([]byte(`,\"query-plan\":`))\n\t\t\tw.Write(planBytes)\n\t\t}\n\t}\n\n\tsql := string(b)\n\trootOperator, err := h.api.CompilePlan(ctx, sql)\n\tif err != nil {\n\t\twriteError(err, false)\n\t\treturn\n\t}\n\n\t// Get a query iterator.\n\titer, err := rootOperator.Iterator(ctx, nil)\n\tif err != nil {\n\t\twriteError(err, false)\n\t\twriteWarnings(rootOperator.Warnings())\n\t\treturn\n\t}\n\n\t// Read schema & write to response.\n\tcolumns := rootOperator.Schema()\n\tschema := WireQuerySchema{\n\t\tFields: make([]*WireQueryField, len(columns)),\n\t}\n\tfor i, col := range columns {\n\t\tbtype, err := dax.BaseTypeFromString(col.Type.BaseTypeName())\n\t\tif err != nil {\n\t\t\twriteError(err, false)\n\t\t\twriteWarnings(rootOperator.Warnings())\n\t\t\treturn\n\t\t}\n\t\tschema.Fields[i] = &WireQueryField{\n\t\t\tName:     dax.FieldName(col.ColumnName),\n\t\t\tType:     col.Type.TypeDescription(),\n\t\t\tBaseType: btype,\n\t\t\tTypeInfo: col.Type.TypeInfo(),\n\t\t}\n\t}\n\tw.Write([]byte(`\"schema\":`))\n\tjsonSchema, err := json.Marshal(schema)\n\tif err != nil {\n\t\th.logger.Errorf(\"write schema response error: %s\", err)\n\t\t// Provide an empty list as the schema value to maintain valid json.\n\t\tw.Write([]byte(\"[]\"))\n\t\twriteError(err, false)\n\t\twriteWarnings(rootOperator.Warnings())\n\t\treturn\n\t}\n\tw.Write(jsonSchema)\n\n\t// Write the data (rows).\n\tw.Write([]byte(`,\"data\":[`))\n\n\tvar rowErr error\n\tvar currentRow types.Row\n\tvar nextErr error\n\n\trowCounter := 1\n\tfor currentRow, nextErr = iter.Next(ctx); nextErr == nil; currentRow, nextErr = iter.Next(ctx) {\n\t\tjsonRow, err := json.Marshal(currentRow)\n\t\tif err != nil {\n\t\t\th.logger.Errorf(\"json encoding error: %s\", err)\n\t\t\trowErr = err\n\t\t\tbreak\n\t\t}\n\n\t\tif rowCounter > 1 {\n\t\t\t// Include a comma between data rows.\n\t\t\tw.Write([]byte(\",\"))\n\t\t}\n\t\tw.Write(jsonRow)\n\n\t\trowCounter++\n\t}\n\tif nextErr != nil && nextErr != types.ErrNoMoreRows {\n\t\trowErr = nextErr\n\t}\n\n\tw.Write([]byte(\"]\"))\n\n\twriteError(rowErr, true)\n\twriteWarnings(rootOperator.Warnings())\n\twritePlan(rootOperator.Plan())\n}\n\nfunc (h *Handler) handleCPUProfileStart(w http.ResponseWriter, r *http.Request) {\n\tif h.pprofCPUProfileBuffer == nil {\n\t\th.pprofCPUProfileBuffer = bytes.NewBuffer(nil)\n\t} else {\n\t\thttp.Error(w, \"cpu profile already in progress\", http.StatusBadRequest)\n\t\treturn\n\t}\n\terr := pprof.StartCPUProfile(h.pprofCPUProfileBuffer)\n\tif err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"%v\", err), http.StatusBadRequest)\n\t\th.pprofCPUProfileBuffer = nil\n\t\treturn\n\t}\n\tw.WriteHeader(http.StatusOK)\n}\n\nfunc (h *Handler) handleCPUProfileStop(w http.ResponseWriter, r *http.Request) {\n\tif h.pprofCPUProfileBuffer == nil {\n\t\thttp.Error(w, \"no cpu profile in progress\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tpprof.StopCPUProfile()\n\n\t// match what pprof usually returns:\n\t// HTTP/1.1 200 OK\n\t// Content-Disposition: attachment; filename=\"profile\"\n\t// Content-Type: application/octet-stream\n\t// X-Content-Type-Options: nosniff\n\t// Date: Tue, 03 Nov 2020 18:31:36 GMT\n\t// Content-Length: 939\n\n\t// Send the headers\n\tby := h.pprofCPUProfileBuffer.Bytes()\n\tw.Header().Set(\"Content-Disposition\", \"attachment; filename=\\\"profile\\\"\")\n\tw.Header().Set(\"Content-Type\", \"application/octet-stream\")\n\tw.Header().Set(\"Content-Length\", fmt.Sprintf(\"%v\", len(by)))\n\tw.Header().Set(\"X-Content-Type-Options\", \"nosniff\")\n\n\t_, err := io.Copy(w, h.pprofCPUProfileBuffer)\n\th.pprofCPUProfileBuffer = nil\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\treturn\n\t}\n}\n\n// handleGetIndexAvailableShards handles GET /internal/index/:index/shards requests.\nfunc (h *Handler) handleGetIndexAvailableShards(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tindexName := mux.Vars(r)[\"index\"]\n\tshards, err := h.api.AvailableShards(r.Context(), indexName)\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tif err := json.NewEncoder(w).Encode(getIndexAvailableShardsResponse{Shards: shards.Slice()}); err != nil {\n\t\th.logger.Errorf(\"write shards-max response error: %s\", err)\n\t}\n}\n\ntype getIndexAvailableShardsResponse struct {\n\tShards []uint64 `json:\"shards\"`\n}\n\n// handleGetShardsMax handles GET /internal/shards/max requests.\nfunc (h *Handler) handleGetShardsMax(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tif err := json.NewEncoder(w).Encode(getShardsMaxResponse{\n\t\tStandard: h.api.MaxShards(r.Context()),\n\t}); err != nil {\n\t\th.logger.Errorf(\"write shards-max response error: %s\", err)\n\t}\n}\n\ntype getShardsMaxResponse struct {\n\tStandard map[string]uint64 `json:\"standard\"`\n}\n\n// handleGetIndexes handles GET /index request.\nfunc (h *Handler) handleGetIndexes(w http.ResponseWriter, r *http.Request) {\n\th.handleGetSchema(w, r)\n}\n\n// handleGetIndex handles GET /index/<indexname> requests.\nfunc (h *Handler) handleGetIndex(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\tq := r.URL.Query()\n\twithViews := q.Get(\"views\") == \"true\"\n\n\tindexName := mux.Vars(r)[\"index\"]\n\tschema, err := h.api.Schema(r.Context(), withViews)\n\tif err != nil {\n\t\th.logger.Printf(\"getting schema error: %s\", err)\n\t}\n\n\tfor _, idx := range schema {\n\t\tif idx.Name == indexName {\n\t\t\tw.Header().Set(\"Content-Type\", \"application/json\")\n\t\t\tif err := json.NewEncoder(w).Encode(idx); err != nil {\n\t\t\t\th.logger.Errorf(\"write response error: %s\", err)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n\thttp.Error(w, fmt.Sprintf(\"Index %s Not Found\", indexName), http.StatusNotFound)\n}\n\n// handleGetView handles GET /index/<indexname>/field/<fieldname>/view requests.\nfunc (h *Handler) handleGetView(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tindexName := mux.Vars(r)[\"index\"]\n\tfieldName := mux.Vars(r)[\"field\"]\n\tindex, err := h.api.Index(r.Context(), indexName)\n\n\tif err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"Index %s Not Found\", indexName), http.StatusNotFound)\n\t\treturn\n\t} else {\n\t\tw.Header().Set(\"Content-Type\", \"application/json\")\n\t\tvar viewsList []viewReponse\n\t\tfor _, field := range index.fields {\n\t\t\tif field.name == fieldName {\n\t\t\t\tfor _, view := range field.views() {\n\t\t\t\t\tviewsList = append(viewsList, viewReponse{Name: view.name, Type: view.fieldType, Field: view.field, Index: view.index})\n\t\t\t\t}\n\n\t\t\t\tif err := json.NewEncoder(w).Encode(viewsList); err != nil {\n\t\t\t\t\th.logger.Errorf(\"write response error: %s\", err)\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\thttp.Error(w, fmt.Sprintf(\"Field %s Not Found\", fieldName), http.StatusNotFound)\n}\n\ntype viewReponse struct {\n\tName  string `json:\"name\"`\n\tType  string `json:\"type\"`\n\tField string `json:\"field\"`\n\tIndex string `json:\"index\"`\n}\n\n// handleDeleteIndex handles DELETE /index/<indexname>/field/<fieldname>/view/<viewname> request.\nfunc (h *Handler) handleDeleteView(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tindexName := mux.Vars(r)[\"index\"]\n\tfieldName := mux.Vars(r)[\"field\"]\n\tviewName := mux.Vars(r)[\"view\"]\n\n\tresp := successResponse{h: h}\n\terr := h.api.DeleteView(r.Context(), indexName, fieldName, viewName)\n\tresp.write(w, err)\n}\n\ntype postIndexRequest struct {\n\tOptions IndexOptions `json:\"options\"`\n}\n\n// _postIndexRequest is necessary to avoid recursion while decoding.\ntype _postIndexRequest postIndexRequest\n\n// Custom Unmarshal JSON to validate request body when creating a new index.\nfunc (p *postIndexRequest) UnmarshalJSON(b []byte) error {\n\t// m is an overflow map used to capture additional, unexpected keys.\n\tm := make(map[string]interface{})\n\tif err := json.Unmarshal(b, &m); err != nil {\n\t\treturn errors.Wrap(err, \"unmarshalling unexpected values\")\n\t}\n\n\tvalidIndexOptions := getValidOptions(IndexOptions{})\n\terr := validateOptions(m, validIndexOptions)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// Unmarshal expected values.\n\t_p := _postIndexRequest{\n\t\tOptions: IndexOptions{\n\t\t\tDescription:    \"\",\n\t\t\tKeys:           false,\n\t\t\tTrackExistence: true,\n\t\t},\n\t}\n\tif err := json.Unmarshal(b, &_p); err != nil {\n\t\treturn errors.Wrap(err, \"unmarshalling expected values\")\n\t}\n\n\tp.Options = _p.Options\n\n\treturn nil\n}\n\nfunc getValidOptions(option interface{}) []string {\n\tvalidOptions := []string{}\n\tval := reflect.ValueOf(option)\n\tfor i := 0; i < val.Type().NumField(); i++ {\n\t\tjsonTag := val.Type().Field(i).Tag.Get(\"json\")\n\t\ts := strings.Split(jsonTag, \",\")\n\t\tvalidOptions = append(validOptions, s[0])\n\t}\n\treturn validOptions\n}\n\n// Raise errors for any unknown key\nfunc validateOptions(data map[string]interface{}, validIndexOptions []string) error {\n\tfor k, v := range data {\n\t\tswitch k {\n\t\tcase \"options\":\n\t\t\toptions, ok := v.(map[string]interface{})\n\t\t\tif !ok {\n\t\t\t\treturn errors.New(\"options is not map[string]interface{}\")\n\t\t\t}\n\t\t\tfor kk, vv := range options {\n\t\t\t\tif !foundItem(validIndexOptions, kk) {\n\t\t\t\t\treturn fmt.Errorf(\"unknown key: %v:%v\", kk, vv)\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"unknown key: %v:%v\", k, v)\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc foundItem(items []string, item string) bool {\n\tfor _, i := range items {\n\t\tif item == i {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// handleDeleteIndex handles DELETE /index request.\nfunc (h *Handler) handleDeleteIndex(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tindexName := mux.Vars(r)[\"index\"]\n\n\tresp := successResponse{h: h}\n\terr := h.api.DeleteIndex(r.Context(), indexName)\n\tresp.write(w, err)\n}\n\n// handlePostIndex handles POST /index request.\nfunc (h *Handler) handlePostIndex(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\tindexName, ok := mux.Vars(r)[\"index\"]\n\tif !ok {\n\t\thttp.Error(w, \"index name is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tresp := successResponse{h: h, Name: indexName}\n\n\t// Decode request.\n\treq := postIndexRequest{\n\t\tOptions: IndexOptions{\n\t\t\tDescription:    \"\",\n\t\t\tKeys:           false,\n\t\t\tTrackExistence: true,\n\t\t},\n\t}\n\terr := json.NewDecoder(r.Body).Decode(&req)\n\tif err != nil && err != io.EOF {\n\t\tresp.write(w, err)\n\t\treturn\n\t}\n\tindex, err := h.api.CreateIndex(r.Context(), indexName, req.Options)\n\n\tif index != nil {\n\t\tresp.CreatedAt = index.CreatedAt()\n\t} else if _, ok = errors.Cause(err).(ConflictError); ok {\n\t\tif index, _ = h.api.Index(r.Context(), indexName); index != nil {\n\t\t\tresp.CreatedAt = index.CreatedAt()\n\t\t}\n\t}\n\tresp.write(w, err)\n}\n\nfunc (h *Handler) handleGetActiveQueries(w http.ResponseWriter, r *http.Request) {\n\tvar rtype string\n\tswitch {\n\tcase validHeaderAcceptType(r.Header, \"text\", \"plain\"):\n\t\trtype = \"text/plain\"\n\tcase validHeaderAcceptJSON(r.Header):\n\t\trtype = \"application/json\"\n\tdefault:\n\t\thttp.Error(w, \"no acceptable response type selected\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\tqueries, err := h.api.ActiveQueries(r.Context())\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\treturn\n\t}\n\tw.Header().Set(\"Content-Type\", rtype)\n\tswitch rtype {\n\tcase \"text/plain\":\n\t\tdurations := make([]string, len(queries))\n\t\tfor i, q := range queries {\n\t\t\tdurations[i] = q.Age.String()\n\t\t}\n\t\tvar maxlen int\n\t\tfor _, l := range durations {\n\t\t\tif len(l) > maxlen {\n\t\t\t\tmaxlen = len(l)\n\t\t\t}\n\t\t}\n\t\tfor i, q := range queries {\n\t\t\t_, err := fmt.Fprintf(w, \"%*s%q\\n\", -(maxlen + 2), durations[i], q.PQL)\n\t\t\tif err != nil {\n\t\t\t\th.logger.Errorf(\"sending GetActiveQueries response: %s\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tif _, err := w.Write([]byte{'\\n'}); err != nil {\n\t\t\th.logger.Errorf(\"sending GetActiveQueries response: %s\", err)\n\t\t}\n\tcase \"application/json\":\n\t\tif err := json.NewEncoder(w).Encode(queries); err != nil {\n\t\t\th.logger.Errorf(\"encoding GetActiveQueries response: %s\", err)\n\t\t}\n\t}\n}\n\nfunc (h *Handler) handleGetPastQueries(w http.ResponseWriter, r *http.Request) {\n\tq := r.URL.Query()\n\tremoteStr := q.Get(\"remote\")\n\tvar remote bool\n\tif remoteStr == \"true\" {\n\t\tremote = true\n\t}\n\n\tqueries, err := h.api.PastQueries(r.Context(), remote)\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tif err := json.NewEncoder(w).Encode(queries); err != nil {\n\t\th.logger.Errorf(\"encoding GetActiveQueries response: %s\", err)\n\t}\n}\n\nfunc fieldOptionsToFunctionalOpts(opt fieldOptions) []FieldOption {\n\t// Convert json options into functional options.\n\tvar fos []FieldOption\n\tswitch opt.Type {\n\tcase FieldTypeSet:\n\t\tfos = append(fos, OptFieldTypeSet(*opt.CacheType, *opt.CacheSize))\n\tcase FieldTypeInt:\n\t\tmin, max := pql.MinMax(0)\n\t\t// ensure the provided bounds are valid\n\t\tif opt.Max != nil && max.LessThan(*opt.Max) {\n\t\t\topt.Max = &max\n\t\t}\n\t\tif opt.Max == nil {\n\t\t\topt.Max = &max\n\t\t}\n\n\t\tif opt.Min != nil && min.GreaterThan(*opt.Min) {\n\t\t\topt.Min = &min\n\t\t}\n\t\tif opt.Min == nil {\n\t\t\topt.Min = &min\n\t\t}\n\t\tfos = append(fos, OptFieldTypeInt(opt.Min.ToInt64(0), opt.Max.ToInt64(0)))\n\tcase FieldTypeDecimal:\n\t\tscale := int64(0)\n\t\tif opt.Scale != nil {\n\t\t\tscale = *opt.Scale\n\t\t\tmin, max := pql.MinMax(scale)\n\t\t\t// ensure the provided bounds are valid\n\t\t\tif opt.Max != nil && max.LessThan(*opt.Max) {\n\t\t\t\topt.Max = &max\n\t\t\t}\n\t\t\tif opt.Max == nil {\n\t\t\t\topt.Max = &max\n\t\t\t}\n\n\t\t\tif opt.Min != nil && min.GreaterThan(*opt.Min) {\n\t\t\t\topt.Min = &min\n\t\t\t}\n\t\t\tif opt.Min == nil {\n\t\t\t\topt.Min = &min\n\t\t\t}\n\t\t}\n\n\t\tif opt.Min == nil {\n\t\t\tmin := pql.NewDecimal(int64(math.MinInt64), scale)\n\t\t\topt.Min = &min\n\t\t}\n\t\tif opt.Max == nil {\n\t\t\tmax := pql.NewDecimal(int64(math.MaxInt64), scale)\n\t\t\topt.Max = &max\n\t\t}\n\t\tvar minmax []pql.Decimal\n\t\tif opt.Min != nil {\n\t\t\tminmax = []pql.Decimal{\n\t\t\t\t*opt.Min,\n\t\t\t}\n\t\t\tif opt.Max != nil {\n\t\t\t\tminmax = append(minmax, *opt.Max)\n\t\t\t}\n\t\t}\n\t\tfos = append(fos, OptFieldTypeDecimal(scale, minmax...))\n\tcase FieldTypeTimestamp:\n\t\tif opt.Epoch == nil {\n\t\t\tepoch := DefaultEpoch\n\t\t\topt.Epoch = &epoch\n\t\t}\n\t\tfos = append(fos, OptFieldTypeTimestamp(opt.Epoch.UTC(), *opt.TimeUnit))\n\tcase FieldTypeTime:\n\t\tif opt.TTL != nil {\n\t\t\tfos = append(fos, OptFieldTypeTime(*opt.TimeQuantum, *opt.TTL, opt.NoStandardView))\n\t\t} else {\n\t\t\tfos = append(fos, OptFieldTypeTime(*opt.TimeQuantum, \"0\", opt.NoStandardView))\n\t\t}\n\tcase FieldTypeMutex:\n\t\tfos = append(fos, OptFieldTypeMutex(*opt.CacheType, *opt.CacheSize))\n\tcase FieldTypeBool:\n\t\tfos = append(fos, OptFieldTypeBool())\n\t}\n\tif opt.Keys != nil {\n\t\tif *opt.Keys {\n\t\t\tfos = append(fos, OptFieldKeys())\n\t\t}\n\t}\n\tif opt.ForeignIndex != nil {\n\t\tfos = append(fos, OptFieldForeignIndex(*opt.ForeignIndex))\n\t}\n\treturn fos\n}\n\n// handlePostField handles POST /field request.\nfunc (h *Handler) handlePostField(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tindexName, ok := mux.Vars(r)[\"index\"]\n\tif !ok {\n\t\thttp.Error(w, \"index name is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tfieldName, ok := mux.Vars(r)[\"field\"]\n\tif !ok {\n\t\thttp.Error(w, \"field name is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tresp := successResponse{h: h, Name: fieldName}\n\n\t// Decode request.\n\tvar req postFieldRequest\n\tdec := json.NewDecoder(r.Body)\n\tdec.DisallowUnknownFields()\n\terr := dec.Decode(&req)\n\tif err != nil && err != io.EOF {\n\t\tresp.write(w, err)\n\t\treturn\n\t}\n\n\t// Validate field options.\n\tif err := req.Options.validate(); err != nil {\n\t\tresp.write(w, err)\n\t\treturn\n\t}\n\n\tfos := fieldOptionsToFunctionalOpts(req.Options)\n\tfield, err := h.api.CreateField(r.Context(), indexName, fieldName, fos...)\n\tif _, ok = err.(BadRequestError); ok {\n\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\treturn\n\t}\n\tif field != nil {\n\t\tresp.CreatedAt = field.CreatedAt()\n\t} else if _, ok = errors.Cause(err).(ConflictError); ok {\n\t\tif field, _ = h.api.Field(r.Context(), indexName, fieldName); field != nil {\n\t\t\tresp.CreatedAt = field.CreatedAt()\n\t\t}\n\t}\n\tresp.write(w, err)\n}\n\n// handlePatchField handles updates to field schema at /index/{index}/field/{field}\nfunc (h *Handler) handlePatchField(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tindexName, ok := mux.Vars(r)[\"index\"]\n\tif !ok {\n\t\thttp.Error(w, \"index name is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tfieldName, ok := mux.Vars(r)[\"field\"]\n\tif !ok {\n\t\thttp.Error(w, \"field name is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tresp := successResponse{h: h, Name: fieldName}\n\n\t// Decode request.\n\tvar req FieldUpdate\n\tdec := json.NewDecoder(r.Body)\n\tdec.DisallowUnknownFields()\n\terr := dec.Decode(&req)\n\tif err != nil && err != io.EOF {\n\t\tresp.write(w, err)\n\t\treturn\n\t}\n\n\terr = h.api.UpdateField(r.Context(), indexName, fieldName, req)\n\tresp.write(w, err)\n}\n\ntype postFieldRequest struct {\n\tOptions fieldOptions `json:\"options\"`\n}\n\n// fieldOptions tracks FieldOptions. It is made up of pointers to values,\n// and used for input validation.\ntype fieldOptions struct {\n\tType           string       `json:\"type,omitempty\"`\n\tCacheType      *string      `json:\"cacheType,omitempty\"`\n\tCacheSize      *uint32      `json:\"cacheSize,omitempty\"`\n\tMin            *pql.Decimal `json:\"min,omitempty\"`\n\tMax            *pql.Decimal `json:\"max,omitempty\"`\n\tScale          *int64       `json:\"scale,omitempty\"`\n\tEpoch          *time.Time   `json:\"epoch,omitempty\"`\n\tTimeUnit       *string      `json:\"timeUnit,omitempty\"`\n\tTimeQuantum    *TimeQuantum `json:\"timeQuantum,omitempty\"`\n\tKeys           *bool        `json:\"keys,omitempty\"`\n\tNoStandardView bool         `json:\"noStandardView,omitempty\"`\n\tForeignIndex   *string      `json:\"foreignIndex,omitempty\"`\n\tTTL            *string      `json:\"ttl,omitempty\"`\n\tBase           *int64       `json:\"base,omitempty\"`\n}\n\nfunc (o *fieldOptions) validate() error {\n\t// Pointers to default values.\n\tdefaultCacheType := DefaultCacheType\n\tdefaultCacheSize := uint32(DefaultCacheSize)\n\n\tswitch o.Type {\n\tcase FieldTypeSet, \"\":\n\t\t// Because FieldTypeSet is the default, its arguments are\n\t\t// not required. Instead, the defaults are applied whenever\n\t\t// a value does not exist.\n\t\tif o.Type == \"\" {\n\t\t\to.Type = FieldTypeSet\n\t\t}\n\t\tif o.CacheType == nil {\n\t\t\to.CacheType = &defaultCacheType\n\t\t}\n\t\tif o.CacheSize == nil {\n\t\t\to.CacheSize = &defaultCacheSize\n\t\t}\n\t\tif o.Min != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"min does not apply to field type set\"))\n\t\t} else if o.Max != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"max does not apply to field type set\"))\n\t\t} else if o.TimeQuantum != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"timeQuantum does not apply to field type set\"))\n\t\t} else if o.TTL != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"ttl does not apply to field type set\"))\n\t\t}\n\tcase FieldTypeInt:\n\t\tif o.CacheType != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"cacheType does not apply to field type int\"))\n\t\t} else if o.CacheSize != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"cacheSize does not apply to field type int\"))\n\t\t} else if o.TimeQuantum != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"timeQuantum does not apply to field type int\"))\n\t\t} else if o.TTL != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"ttl does not apply to field type int\"))\n\t\t}\n\tcase FieldTypeDecimal:\n\t\tif o.Scale == nil {\n\t\t\treturn NewBadRequestError(errors.New(\"decimal field requires a scale argument\"))\n\t\t} else if o.CacheType != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"cacheType does not apply to field type int\"))\n\t\t} else if o.CacheSize != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"cacheSize does not apply to field type int\"))\n\t\t} else if o.TimeQuantum != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"timeQuantum does not apply to field type int\"))\n\t\t} else if o.TTL != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"ttl does not apply to field type int\"))\n\t\t} else if o.ForeignIndex != nil && o.Type == FieldTypeDecimal {\n\t\t\treturn NewBadRequestError(errors.New(\"decimal field cannot be a foreign key\"))\n\t\t}\n\tcase FieldTypeTimestamp:\n\t\tif o.TimeUnit == nil {\n\t\t\treturn NewBadRequestError(errors.New(\"timestamp field requires a timeUnit argument\"))\n\t\t} else if !IsValidTimeUnit(*o.TimeUnit) {\n\t\t\treturn NewBadRequestError(errors.New(\"invalid timeUnit argument\"))\n\t\t} else if o.CacheType != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"cacheType does not apply to field type timestamp\"))\n\t\t} else if o.CacheSize != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"cacheSize does not apply to field type timestamp\"))\n\t\t} else if o.TimeQuantum != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"timeQuantum does not apply to field type timestamp\"))\n\t\t} else if o.TTL != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"ttl does not apply to field type timestamp\"))\n\t\t} else if o.ForeignIndex != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"timestamp field cannot be a foreign key\"))\n\t\t}\n\tcase FieldTypeTime:\n\t\tif o.CacheType != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"cacheType does not apply to field type time\"))\n\t\t} else if o.CacheSize != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"cacheSize does not apply to field type time\"))\n\t\t} else if o.Min != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"min does not apply to field type time\"))\n\t\t} else if o.Max != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"max does not apply to field type time\"))\n\t\t} else if o.TimeQuantum == nil {\n\t\t\treturn NewBadRequestError(errors.New(\"timeQuantum is required for field type time\"))\n\t\t}\n\tcase FieldTypeMutex:\n\t\tif o.CacheType == nil {\n\t\t\to.CacheType = &defaultCacheType\n\t\t}\n\t\tif o.CacheSize == nil {\n\t\t\to.CacheSize = &defaultCacheSize\n\t\t}\n\t\tif o.Min != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"min does not apply to field type mutex\"))\n\t\t} else if o.Max != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"max does not apply to field type mutex\"))\n\t\t} else if o.TimeQuantum != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"timeQuantum does not apply to field type mutex\"))\n\t\t} else if o.TTL != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"ttl does not apply to field type mutex\"))\n\t\t}\n\tcase FieldTypeBool:\n\t\tif o.CacheType != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"cacheType does not apply to field type bool\"))\n\t\t} else if o.CacheSize != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"cacheSize does not apply to field type bool\"))\n\t\t} else if o.Min != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"min does not apply to field type bool\"))\n\t\t} else if o.Max != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"max does not apply to field type bool\"))\n\t\t} else if o.TimeQuantum != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"timeQuantum does not apply to field type bool\"))\n\t\t} else if o.Keys != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"keys does not apply to field type bool\"))\n\t\t} else if o.TTL != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"ttl does not apply to field type bool\"))\n\t\t} else if o.ForeignIndex != nil {\n\t\t\treturn NewBadRequestError(errors.New(\"bool field cannot be a foreign key\"))\n\t\t}\n\tdefault:\n\t\treturn errors.Errorf(\"invalid field type: %s\", o.Type)\n\t}\n\treturn nil\n}\n\n// handleDeleteField handles DELETE /field request.\nfunc (h *Handler) handleDeleteField(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tindexName := mux.Vars(r)[\"index\"]\n\tfieldName := mux.Vars(r)[\"field\"]\n\n\tresp := successResponse{h: h}\n\terr := h.api.DeleteField(r.Context(), indexName, fieldName)\n\tresp.write(w, err)\n}\n\nfunc (h *Handler) handleGetTransactionList(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\ttrnsMap, err := h.api.Transactions(r.Context())\n\tif err != nil {\n\t\tswitch errors.Cause(err) {\n\t\tcase ErrNodeNotPrimary:\n\t\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\tdefault:\n\t\t\thttp.Error(w, \"problem getting transactions: \"+err.Error(), http.StatusInternalServerError)\n\t\t}\n\t\treturn\n\t}\n\n\t// Convert the map of transactions to a slice.\n\ttrnsList := make([]*Transaction, len(trnsMap))\n\tvar i int\n\tfor _, v := range trnsMap {\n\t\ttrnsList[i] = v\n\t\ti++\n\t}\n\n\t// Sort the slice by createdAt.\n\tsort.Slice(trnsList, func(i, j int) bool {\n\t\treturn trnsList[i].CreatedAt.Before(trnsList[j].CreatedAt)\n\t})\n\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tif err := json.NewEncoder(w).Encode(trnsList); err != nil {\n\t\th.logger.Errorf(\"encoding GetTransactionList response: %s\", err)\n\t}\n}\n\nfunc (h *Handler) handleGetTransactions(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\ttrnsMap, err := h.api.Transactions(r.Context())\n\tif err != nil {\n\t\tswitch errors.Cause(err) {\n\t\tcase ErrNodeNotPrimary:\n\t\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\tdefault:\n\t\t\thttp.Error(w, \"problem getting transactions: \"+err.Error(), http.StatusInternalServerError)\n\t\t}\n\t\treturn\n\t}\n\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tif err := json.NewEncoder(w).Encode(trnsMap); err != nil {\n\t\th.logger.Errorf(\"encoding GetTransactions response: %s\", err)\n\t}\n}\n\ntype TransactionResponse struct {\n\tTransaction *Transaction `json:\"transaction,omitempty\"`\n\tError       string       `json:\"error,omitempty\"`\n}\n\nfunc (h *Handler) doTransactionResponse(w http.ResponseWriter, err error, trns *Transaction) {\n\tif err != nil {\n\t\tswitch errors.Cause(err) {\n\t\tcase ErrNodeNotPrimary, ErrTransactionExists:\n\t\t\tw.WriteHeader(http.StatusBadRequest)\n\t\tcase ErrTransactionExclusive:\n\t\t\tw.WriteHeader(http.StatusConflict)\n\t\tcase ErrTransactionNotFound:\n\t\t\tw.WriteHeader(http.StatusNotFound)\n\t\tdefault:\n\t\t\tw.WriteHeader(http.StatusInternalServerError)\n\t\t}\n\t}\n\n\tvar errString string\n\tif err != nil {\n\t\terrString = err.Error()\n\t}\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\terr = json.NewEncoder(w).Encode(\n\t\tTransactionResponse{Error: errString, Transaction: trns})\n\tif err != nil {\n\t\th.logger.Errorf(\"encoding transaction response: %v\", err)\n\t}\n}\n\nfunc (h *Handler) handleGetTransaction(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\tid := mux.Vars(r)[\"id\"]\n\ttrns, err := h.api.GetTransaction(r.Context(), id, false)\n\th.doTransactionResponse(w, err, trns)\n}\n\nfunc (h *Handler) handlePostTransaction(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\treqTrns := &Transaction{}\n\tif err := json.NewDecoder(r.Body).Decode(reqTrns); err != nil || reqTrns.Timeout == 0 {\n\t\tif err == nil {\n\t\t\thttp.Error(w, \"timeout is required and cannot be 0\", http.StatusBadRequest)\n\t\t} else {\n\t\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\t}\n\t\treturn\n\t}\n\n\tid, ok := mux.Vars(r)[\"id\"]\n\tif !ok {\n\t\tid = reqTrns.ID\n\t}\n\n\tif primary := h.api.PrimaryNode(); h.api.NodeID() == primary.ID {\n\t\ttrns, err := h.api.StartTransaction(r.Context(), id, reqTrns.Timeout, reqTrns.Exclusive, false)\n\t\th.doTransactionResponse(w, err, trns)\n\t\treturn\n\t} else {\n\t\thttp.Redirect(w, r, primary.URI.Normalize()+\"/transaction/\"+id, http.StatusSeeOther)\n\t\treturn\n\t}\n}\n\nfunc (h *Handler) handlePostFinishTransaction(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\tid := mux.Vars(r)[\"id\"]\n\ttrns, err := h.api.FinishTransaction(r.Context(), id, false)\n\th.doTransactionResponse(w, err, trns)\n}\n\n// handleDeleteRemoteAvailableShard handles DELETE /field/{field}/available-shards/{shardID} request.\nfunc (h *Handler) handleDeleteRemoteAvailableShard(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tindexName := mux.Vars(r)[\"index\"]\n\tfieldName := mux.Vars(r)[\"field\"]\n\tshardID, _ := strconv.ParseUint(mux.Vars(r)[\"shardID\"], 10, 64)\n\n\tresp := successResponse{h: h}\n\terr := h.api.DeleteAvailableShard(r.Context(), indexName, fieldName, shardID)\n\tresp.write(w, err)\n}\n\n// handleGetIndexShardSnapshot handles GET /internal/index/{index}/shard/{shard}/snapshot requests.\nfunc (h *Handler) handleGetIndexShardSnapshot(w http.ResponseWriter, r *http.Request) {\n\tindexName := mux.Vars(r)[\"index\"]\n\tshard, err := strconv.ParseUint(mux.Vars(r)[\"shard\"], 10, 64)\n\tif err != nil {\n\t\thttp.Error(w, \"Invalid shard parameter\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\trc, err := h.api.IndexShardSnapshot(r.Context(), indexName, shard, false)\n\tif err != nil {\n\t\tswitch errors.Cause(err) {\n\t\tcase ErrIndexNotFound:\n\t\t\thttp.Error(w, err.Error(), http.StatusNotFound)\n\t\tdefault:\n\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\t}\n\t\treturn\n\t}\n\tdefer rc.Close()\n\n\t// Copy data to response body.\n\tif _, err := io.CopyBuffer(&passthroughWriter{w}, rc, make([]byte, rbf.PageSize)); err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\treturn\n\t}\n}\n\n// readQueryRequest parses an query parameters from r.\nfunc (h *Handler) readQueryRequest(r *http.Request) (*QueryRequest, error) {\n\tswitch r.Header.Get(\"Content-Type\") {\n\tcase \"application/x-protobuf\":\n\t\treturn h.readProtobufQueryRequest(r)\n\tdefault:\n\t\treturn h.readURLQueryRequest(r)\n\t}\n}\n\n// passthroughWriter is used to remove non-Writer interfaces from an io.Writer.\n// For example, a writer that implements io.ReaderFrom can change io.Copy() behavior.\ntype passthroughWriter struct {\n\tw io.Writer\n}\n\nfunc (w *passthroughWriter) Write(p []byte) (int, error) {\n\treturn w.w.Write(p)\n}\n\n// readProtobufQueryRequest parses query parameters in protobuf from r.\nfunc (h *Handler) readProtobufQueryRequest(r *http.Request) (*QueryRequest, error) {\n\t// Slurp the body.\n\tbody, err := readBody(r)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"reading\")\n\t}\n\n\tqreq := &QueryRequest{}\n\terr = h.serializer.Unmarshal(body, qreq)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"unmarshalling query request\")\n\t}\n\treturn qreq, nil\n}\n\n// readURLQueryRequest parses query parameters from URL parameters from r.\nfunc (h *Handler) readURLQueryRequest(r *http.Request) (*QueryRequest, error) {\n\tq := r.URL.Query()\n\n\t// Parse query string.\n\tbuf, err := readBody(r)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"reading\")\n\t}\n\tquery := string(buf)\n\n\t// Parse list of shards.\n\tshards, err := parseUint64Slice(q.Get(\"shards\"))\n\tif err != nil {\n\t\treturn nil, errors.New(\"invalid shard argument\")\n\t}\n\n\tremote := parseBool(q.Get(\"remote\"))\n\n\t// Optional profiling\n\tprofile := false\n\tprofileString := q.Get(\"profile\")\n\tif profileString != \"\" {\n\t\tprofile, err = strconv.ParseBool(q.Get(\"profile\"))\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"invalid profile argument: '%s' (should be true/false)\", profileString)\n\t\t}\n\t}\n\n\treturn &QueryRequest{\n\t\tQuery:   query,\n\t\tRemote:  remote,\n\t\tShards:  shards,\n\t\tProfile: profile,\n\t}, nil\n}\n\nfunc parseBool(a string) bool {\n\treturn strings.ToLower(a) == \"true\"\n}\n\n// writeQueryResponse writes the response from the executor to w.\nfunc (h *Handler) writeQueryResponse(w http.ResponseWriter, r *http.Request, resp *QueryResponse) error {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\tw.Header().Set(\"Content-Type\", \"application/protobuf\")\n\t\treturn h.writeProtobufQueryResponse(w, resp, headerAcceptRoaringRow(r.Header))\n\t}\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\treturn h.writeJSONQueryResponse(w, resp)\n}\n\n// writeProtobufQueryResponse writes the response from the executor to w as protobuf.\nfunc (h *Handler) writeProtobufQueryResponse(w io.Writer, resp *QueryResponse, writeRoaring bool) error {\n\tserializer := h.serializer\n\tif writeRoaring {\n\t\tserializer = h.roaringSerializer\n\t}\n\tif buf, err := serializer.Marshal(resp); err != nil {\n\t\treturn errors.Wrap(err, \"marshalling\")\n\t} else if _, err := w.Write(buf); err != nil {\n\t\treturn errors.Wrap(err, \"writing\")\n\t}\n\treturn nil\n}\n\n// writeJSONQueryResponse writes the response from the executor to w as JSON.\nfunc (h *Handler) writeJSONQueryResponse(w io.Writer, resp *QueryResponse) error {\n\treturn json.NewEncoder(w).Encode(resp)\n}\n\nfunc validateProtobufHeader(r *http.Request) (error string, code int) {\n\tif r.Header.Get(\"Content-Type\") != \"application/x-protobuf\" {\n\t\treturn \"Unsupported media type\", http.StatusUnsupportedMediaType\n\t}\n\tif r.Header.Get(\"Accept\") != \"application/x-protobuf\" {\n\t\treturn \"Not acceptable\", http.StatusNotAcceptable\n\t}\n\treturn\n}\n\n// handleGetInternalDebugRBFJSON handles /internal/debug/rbf requests.\nfunc (h *Handler) handleGetInternalDebugRBFJSON(w http.ResponseWriter, r *http.Request) {\n\tbuf, err := json.MarshalIndent(h.api.RBFDebugInfo(), \"\", \"  \")\n\tif err != nil {\n\t\thttp.Error(w, \"marshal json: \"+err.Error(), http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tw.Write(buf)\n}\n\n// handleGetMetricsJSON handles /metrics.json requests, translating text metrics results to more consumable JSON.\nfunc (h *Handler) handleGetMetricsJSON(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\n\tmetrics := make(map[string][]*prom2json.Family)\n\ttransport := http.DefaultTransport.(*http.Transport).Clone()\n\tfor _, node := range h.api.Hosts(r.Context()) {\n\t\tmetricsURI := node.URI.String() + \"/metrics\"\n\n\t\t// The buffer size of 60 is performance controlling, but we\n\t\t// haven't studied what the optimal setting is. It was\n\t\t// earlier set to this value to capture all output from\n\t\t// prom2json at once. The output got larger recently, so\n\t\t// now we handle unlimited size output using a goroutine.\n\t\tmfChan := make(chan *dto.MetricFamily, 60)\n\t\terrChan := make(chan error)\n\t\tgo func() {\n\t\t\terr := prom2json.FetchMetricFamilies(metricsURI, mfChan, transport)\n\t\t\terrChan <- err\n\t\t}()\n\n\t\tnodeMetrics := []*prom2json.Family{}\n\t\tfor mf := range mfChan {\n\t\t\tnodeMetrics = append(nodeMetrics, prom2json.NewFamily(mf))\n\t\t}\n\t\terr := <-errChan\n\t\tif err != nil {\n\t\t\thttp.Error(w, \"fetching metrics: \"+err.Error(), http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t\tmetrics[node.ID] = nodeMetrics\n\t}\n\n\terr := json.NewEncoder(w).Encode(metrics)\n\tif err != nil {\n\t\th.logger.Errorf(\"json write error: %s\", err)\n\t}\n}\n\n// handleGetExport handles /export requests.\nfunc (h *Handler) handleGetExport(w http.ResponseWriter, r *http.Request) {\n\tswitch r.Header.Get(\"Accept\") {\n\tcase \"text/csv\":\n\t\th.handleGetExportCSV(w, r)\n\tdefault:\n\t\thttp.Error(w, \"Not acceptable\", http.StatusNotAcceptable)\n\t}\n}\n\nfunc (h *Handler) handleGetExportCSV(w http.ResponseWriter, r *http.Request) {\n\t// Parse query parameters.\n\tq := r.URL.Query()\n\tindex, field := q.Get(\"index\"), q.Get(\"field\")\n\n\tshard, err := strconv.ParseUint(q.Get(\"shard\"), 10, 64)\n\tif err != nil {\n\t\thttp.Error(w, \"invalid shard\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tif err = h.api.ExportCSV(r.Context(), index, field, shard, w); err != nil {\n\t\tswitch errors.Cause(err) {\n\t\tcase ErrFragmentNotFound:\n\t\t\tbreak\n\t\tcase ErrClusterDoesNotOwnShard:\n\t\t\thttp.Error(w, err.Error(), http.StatusPreconditionFailed)\n\t\tdefault:\n\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\t}\n\t\treturn\n\t}\n}\n\n// handleGetFragmentNodes handles /internal/fragment/nodes requests.\nfunc (h *Handler) handleGetFragmentNodes(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\tq := r.URL.Query()\n\tindex := q.Get(\"index\")\n\n\t// Read shard parameter.\n\tshard, err := strconv.ParseUint(q.Get(\"shard\"), 10, 64)\n\tif err != nil {\n\t\thttp.Error(w, \"shard should be an unsigned integer\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\t// Retrieve fragment owner nodes.\n\tnodes, err := h.api.ShardNodes(r.Context(), index, shard)\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\t// Write to response.\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tif err := json.NewEncoder(w).Encode(nodes); err != nil {\n\t\th.logger.Errorf(\"json write error: %s\", err)\n\t}\n}\n\n// handleGetPartitionNodes handles /internal/partition/nodes requests.\nfunc (h *Handler) handleGetPartitionNodes(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\tq := r.URL.Query()\n\n\t// Read partition parameter.\n\tpartitionID, err := strconv.ParseInt(q.Get(\"partition\"), 10, 64)\n\tif err != nil {\n\t\thttp.Error(w, \"shard should be an unsigned integer\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\t// Retrieve fragment owner nodes.\n\tnodes, err := h.api.PartitionNodes(r.Context(), int(partitionID))\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\t// Write to response.\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tif err := json.NewEncoder(w).Encode(nodes); err != nil {\n\t\th.logger.Errorf(\"json write error: %s\", err)\n\t}\n}\n\n// handleGetNodes handles /internal/nodes requests.\nfunc (h *Handler) handleGetNodes(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\t// Retrieve all nodes.\n\tnodes := h.api.Hosts(r.Context())\n\n\t// Write to response.\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tif err := json.NewEncoder(w).Encode(nodes); err != nil {\n\t\th.logger.Errorf(\"json write error: %s\", err)\n\t}\n}\n\n// handleGetFragmentBlockData handles GET /internal/fragment/block/data requests.\nfunc (h *Handler) handleGetFragmentBlockData(w http.ResponseWriter, r *http.Request) {\n\thttp.Error(w, \"fragment blocks feature removed\", http.StatusNotFound)\n}\n\n// handleGetFragmentBlocks handles GET /internal/fragment/blocks requests.\nfunc (h *Handler) handleGetFragmentBlocks(w http.ResponseWriter, r *http.Request) {\n\thttp.Error(w, \"fragment blocks feature removed\", http.StatusNotFound)\n}\n\n// handleGetFragmentData handles GET /internal/fragment/data requests.\nfunc (h *Handler) handleGetFragmentData(w http.ResponseWriter, r *http.Request) {\n\t// Read shard parameter.\n\tq := r.URL.Query()\n\tshard, err := strconv.ParseUint(q.Get(\"shard\"), 10, 64)\n\tif err != nil {\n\t\thttp.Error(w, \"shard required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\t// Retrieve fragment data from holder.\n\tf, err := h.api.FragmentData(r.Context(), q.Get(\"index\"), q.Get(\"field\"), q.Get(\"view\"), shard)\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusNotFound)\n\t\treturn\n\t}\n\t// Stream fragment to response body.\n\tif _, err := f.WriteTo(w); err != nil {\n\t\th.logger.Errorf(\"error streaming fragment data: %s\", err)\n\t}\n}\n\n// handleGetTranslateData handles GET /internal/translate/data requests.\nfunc (h *Handler) handleGetTranslateData(w http.ResponseWriter, r *http.Request) {\n\tq := r.URL.Query()\n\n\t// Perform field translation copy, if field specified.\n\tif fieldName := q.Get(\"field\"); fieldName != \"\" {\n\t\t// Retrieve field data from holder.\n\t\tp, err := h.api.FieldTranslateData(r.Context(), q.Get(\"index\"), fieldName)\n\t\tif err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusNotFound)\n\t\t\treturn\n\t\t}\n\t\ttx, err := p.Begin(false)\n\t\tif err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t\tdefer tx.Rollback()\n\n\t\t// Stream translate data to response body.\n\t\tif _, err := tx.WriteTo(w); err != nil {\n\t\t\th.logger.Errorf(\"error streaming translation data: %s\", err)\n\t\t}\n\t\treturn\n\t}\n\n\t// Otherwise read partition parameter for index translation copy.\n\tpartition, err := strconv.ParseUint(q.Get(\"partition\"), 10, 32)\n\tif err != nil {\n\t\thttp.Error(w, \"partition or field required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\t// Retrieve partition data from holder.\n\tp, err := h.api.TranslateData(r.Context(), q.Get(\"index\"), int(partition))\n\tif redir, ok := err.(RedirectError); ok {\n\t\tnewURL := *r.URL\n\t\tnewURL.Host = redir.HostPort\n\t\thttp.Redirect(w, r, newURL.String(), http.StatusSeeOther)\n\t\treturn\n\t}\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusNotFound)\n\t\treturn\n\t}\n\ttx, err := p.Begin(false)\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\treturn\n\t}\n\tdefer tx.Rollback()\n\t// Stream translate partition to response body.\n\tif _, err := tx.WriteTo(w); err != nil {\n\t\th.logger.Errorf(\"error streaming translation data: %s\", err)\n\t}\n}\n\n// handleGetVersion handles /version requests.\nfunc (h *Handler) handleGetVersion(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\terr := json.NewEncoder(w).Encode(struct {\n\t\tVersion string `json:\"version\"`\n\t}{\n\t\tVersion: h.api.Version(),\n\t})\n\tif err != nil {\n\t\th.logger.Errorf(\"write version response error: %s\", err)\n\t}\n}\n\n// QueryResult types.\nconst (\n\tQueryResultTypeRow uint32 = iota\n\tQueryResultTypePairs\n\tQueryResultTypeUint64\n)\n\n// parseUint64Slice returns a slice of uint64s from a comma-delimited string.\nfunc parseUint64Slice(s string) ([]uint64, error) {\n\tss := strings.Split(s, \",\")\n\ta := make([]uint64, 0, len(ss))\n\tfor _, str := range ss {\n\t\t// Ignore blanks.\n\t\tif str == \"\" {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Parse number.\n\t\tnum, err := strconv.ParseUint(str, 10, 64)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"parsing int\")\n\t\t}\n\t\ta = append(a, num)\n\t}\n\treturn a, nil\n}\n\nfunc (h *Handler) handleRecalculateCaches(w http.ResponseWriter, r *http.Request) {\n\terr := h.api.RecalculateCaches(r.Context())\n\tif err != nil {\n\t\thttp.Error(w, \"recalculating caches: \"+err.Error(), http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\tw.WriteHeader(http.StatusNoContent)\n}\n\nfunc (h *Handler) handlePostClusterMessage(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\t// Verify that request is only communicating over protobufs.\n\tif r.Header.Get(\"Content-Type\") != \"application/x-protobuf\" {\n\t\thttp.Error(w, \"Unsupported media type\", http.StatusUnsupportedMediaType)\n\t\treturn\n\t}\n\terr := h.api.ClusterMessage(r.Context(), r.Body)\n\tif err != nil {\n\t\tswitch err := err.(type) {\n\t\tcase MessageProcessingError:\n\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\tdefault:\n\t\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\t}\n\t\treturn\n\t}\n\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tif err := json.NewEncoder(w).Encode(defaultClusterMessageResponse{}); err != nil {\n\t\th.logger.Errorf(\"response encoding error: %s\", err)\n\t}\n}\n\ntype defaultClusterMessageResponse struct{}\n\nfunc (h *Handler) handlePostTranslateData(w http.ResponseWriter, r *http.Request) {\n\t// Parse offsets for all indexes and fields from POST body.\n\toffsets := make(TranslateOffsetMap)\n\tif err := json.NewDecoder(r.Body).Decode(&offsets); err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\treturn\n\t}\n\t// Stream all translation data.\n\trd, err := h.api.GetTranslateEntryReader(r.Context(), offsets)\n\tif errors.Cause(err) == ErrNotImplemented {\n\t\thttp.Error(w, err.Error(), http.StatusNotImplemented)\n\t\treturn\n\t} else if err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\treturn\n\t}\n\tdefer rd.Close()\n\n\t// Flush header so client can continue.\n\tw.WriteHeader(http.StatusOK)\n\tw.(http.Flusher).Flush()\n\n\t// Copy from reader to client until store or client disconnect.\n\tenc := json.NewEncoder(w)\n\tfor {\n\t\t// Read from store.\n\t\tvar entry TranslateEntry\n\t\tif err := rd.ReadEntry(&entry); err == io.EOF {\n\t\t\treturn\n\t\t} else if err != nil {\n\t\t\th.logger.Errorf(\"http: translate store read error: %s\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Write to response & flush.\n\t\tif err := enc.Encode(&entry); err != nil {\n\t\t\treturn\n\t\t}\n\t\tw.(http.Flusher).Flush()\n\t}\n}\n\ntype queryValidationSpec struct {\n\trequired []string\n\targs     map[string]struct{}\n}\n\nfunc queryValidationSpecRequired(requiredArgs ...string) *queryValidationSpec {\n\targs := map[string]struct{}{}\n\tfor _, arg := range requiredArgs {\n\t\targs[arg] = struct{}{}\n\t}\n\n\treturn &queryValidationSpec{\n\t\trequired: requiredArgs,\n\t\targs:     args,\n\t}\n}\n\nfunc (s *queryValidationSpec) Optional(args ...string) *queryValidationSpec {\n\tfor _, arg := range args {\n\t\ts.args[arg] = struct{}{}\n\t}\n\treturn s\n}\n\nfunc (s queryValidationSpec) validate(query url.Values) error {\n\tfor _, req := range s.required {\n\t\tif query.Get(req) == \"\" {\n\t\t\treturn errors.Errorf(\"%s is required\", req)\n\t\t}\n\t}\n\tfor k := range query {\n\t\tif _, ok := s.args[k]; !ok {\n\t\t\treturn errors.Errorf(\"%s is not a valid argument\", k)\n\t\t}\n\t}\n\treturn nil\n}\n\ntype ClientOption func(client *http.Client, dialer *net.Dialer) *http.Client\n\nfunc ClientResponseHeaderTimeoutOption(dur time.Duration) ClientOption {\n\treturn func(client *http.Client, dialer *net.Dialer) *http.Client {\n\t\tclient.Transport.(*http.Transport).ResponseHeaderTimeout = dur\n\t\treturn client\n\t}\n}\n\nfunc ClientDialTimeoutOption(dur time.Duration) ClientOption {\n\treturn func(client *http.Client, dialer *net.Dialer) *http.Client {\n\t\tdialer.Timeout = dur\n\t\treturn client\n\t}\n}\n\nfunc GetHTTPClient(t *tls.Config, opts ...ClientOption) *http.Client {\n\tdialer := &net.Dialer{\n\t\tTimeout:   30 * time.Second,\n\t\tKeepAlive: 15 * time.Second,\n\t\tDualStack: true,\n\t}\n\ttransport := &http.Transport{\n\t\tProxy:                 http.ProxyFromEnvironment,\n\t\tDialContext:           dialer.DialContext,\n\t\tMaxIdleConns:          1000,\n\t\tMaxIdleConnsPerHost:   200,\n\t\tIdleConnTimeout:       20 * time.Second,\n\t\tTLSHandshakeTimeout:   10 * time.Second,\n\t\tExpectContinueTimeout: 1 * time.Second,\n\t}\n\tif t != nil {\n\t\ttransport.TLSClientConfig = t\n\t}\n\n\tclient := &http.Client{Transport: transport}\n\tfor _, opt := range opts {\n\t\tclient = opt(client, dialer)\n\t}\n\treturn client\n}\n\n// handlePostImportAtomicRecord handles /import-atomic-record requests\nfunc (h *Handler) handlePostImportAtomicRecord(w http.ResponseWriter, r *http.Request) {\n\t// Verify that request is only communicating over protobufs.\n\tif error, code := validateProtobufHeader(r); error != \"\" {\n\t\thttp.Error(w, error, code)\n\t\treturn\n\t}\n\n\t// Read entire body.\n\tbody, err := readBody(r)\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\treturn\n\t}\n\t// Unmarshal request based on field type.\n\n\tq := r.URL.Query()\n\tsLoss := q.Get(\"simPowerLossAfter\")\n\tloss := 0\n\tif sLoss != \"\" {\n\t\tl, err := strconv.ParseInt(sLoss, 10, 64)\n\t\tloss = int(l)\n\t\tif err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\t}\n\t}\n\topt := func(o *ImportOptions) error {\n\t\to.SimPowerLossAfter = loss\n\t\treturn nil\n\t}\n\n\treq := &AtomicRecord{}\n\tif err := h.serializer.Unmarshal(body, req); err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tqcx := h.api.Txf().NewQcx()\n\terr = h.api.ImportAtomicRecord(r.Context(), qcx, req, opt)\n\tif err == nil {\n\t\terr = qcx.Finish()\n\t} else {\n\t\tqcx.Abort()\n\t}\n\tif err != nil {\n\t\tswitch errors.Cause(err) {\n\t\tcase ErrClusterDoesNotOwnShard, ErrPreconditionFailed:\n\t\t\thttp.Error(w, err.Error(), http.StatusPreconditionFailed)\n\t\tcase ErrBSIGroupValueTooLow, ErrBSIGroupValueTooHigh, ErrDecimalOutOfRange:\n\t\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\tdefault:\n\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\t}\n\t\treturn\n\t}\n\n\t// Write response.\n\t_, err = w.Write(importOk)\n\tif err != nil {\n\t\th.logger.Errorf(\"writing import response: %v\", err)\n\t}\n}\n\n// handlePostImport handles /import requests.\nfunc (h *Handler) handlePostImport(w http.ResponseWriter, r *http.Request) {\n\t// Verify that request is only communicating over protobufs.\n\tif err, code := validateProtobufHeader(r); err != \"\" {\n\t\thttp.Error(w, err, code)\n\t\treturn\n\t}\n\n\t// Get index and field type to determine how to handle the\n\t// import data.\n\tindexName := mux.Vars(r)[\"index\"]\n\tindex, err := h.api.Index(r.Context(), indexName)\n\tif err != nil {\n\t\tif errors.Cause(err) == ErrIndexNotFound {\n\t\t\thttp.Error(w, err.Error(), http.StatusNotFound)\n\t\t} else {\n\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\t}\n\t\treturn\n\t}\n\tfieldName := mux.Vars(r)[\"field\"]\n\tfield := index.Field(fieldName)\n\tif field == nil {\n\t\thttp.Error(w, ErrFieldNotFound.Error(), http.StatusNotFound)\n\t\treturn\n\t}\n\n\t// If the clear flag is true, treat the import as clear bits.\n\tq := r.URL.Query()\n\tdoClear := q.Get(\"clear\") == \"true\"\n\tdoIgnoreKeyCheck := q.Get(\"ignoreKeyCheck\") == \"true\"\n\n\topts := []ImportOption{\n\t\tOptImportOptionsClear(doClear),\n\t\tOptImportOptionsIgnoreKeyCheck(doIgnoreKeyCheck),\n\t}\n\n\t// Read entire body.\n\tbody, err := readBody(r)\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\treturn\n\t}\n\t// Unmarshal request based on field type.\n\tif field.Type() == FieldTypeInt || field.Type() == FieldTypeDecimal || field.Type() == FieldTypeTimestamp {\n\t\t// Field type: Int\n\t\t// Marshal into request object.\n\t\treq := &ImportValueRequest{}\n\t\tif err := h.serializer.Unmarshal(body, req); err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\t\treturn\n\t\t}\n\n\t\tqcx := h.api.Txf().NewQcx()\n\t\tdefer qcx.Abort()\n\n\t\tif err := h.api.ImportValue(r.Context(), qcx, req, opts...); err != nil {\n\t\t\tswitch errors.Cause(err) {\n\t\t\tcase ErrClusterDoesNotOwnShard, ErrPreconditionFailed:\n\t\t\t\thttp.Error(w, err.Error(), http.StatusPreconditionFailed)\n\t\t\tcase ErrBSIGroupValueTooLow, ErrBSIGroupValueTooHigh, ErrDecimalOutOfRange:\n\t\t\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\t\tdefault:\n\t\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\t\t}\n\n\t\t\treturn\n\t\t}\n\t\terr := qcx.Finish()\n\t\tif err != nil {\n\t\t\thttp.Error(w, fmt.Sprintf(\"error in qcx.Finish(): '%v'\", err.Error()), http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t} else {\n\t\t// Field type: set, time, mutex\n\t\t// Marshal into request object.\n\t\treq := &ImportRequest{}\n\t\tif err := h.serializer.Unmarshal(body, req); err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\t\treturn\n\t\t}\n\n\t\tqcx := h.api.Txf().NewQcx()\n\t\tdefer qcx.Abort()\n\n\t\tif err := h.api.Import(r.Context(), qcx, req, opts...); err != nil {\n\t\t\tswitch errors.Cause(err) {\n\t\t\tcase ErrClusterDoesNotOwnShard, ErrPreconditionFailed:\n\t\t\t\thttp.Error(w, err.Error(), http.StatusPreconditionFailed)\n\t\t\tcase ErrBSIGroupValueTooLow, ErrBSIGroupValueTooHigh, ErrDecimalOutOfRange:\n\t\t\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\t\tdefault:\n\t\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\terr := qcx.Finish()\n\t\tif err != nil {\n\t\t\thttp.Error(w, fmt.Sprintf(\"error in qcx.Finish() on set,time,mutex: '%v'\", err.Error()), http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Write response.\n\t_, err = w.Write(importOk)\n\tif err != nil {\n\t\th.logger.Errorf(\"writing import response: %v\", err)\n\t}\n}\n\n// handleGetMutexCheck handles /mutex-check requests.\nfunc (h *Handler) handleGetMutexCheck(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\t// Get index and field type to determine how to handle the\n\t// import data.\n\tindexName, fieldName := mux.Vars(r)[\"index\"], mux.Vars(r)[\"field\"]\n\tq := r.URL.Query()\n\tlimit := 0\n\tdetails := q.Get(\"details\") == \"true\"\n\tlimitStr := q.Get(\"limit\")\n\tif limitStr != \"\" {\n\t\tvar err error\n\t\tlimit, err = strconv.Atoi(limitStr)\n\t\tif err != nil {\n\t\t\thttp.Error(w, \"limit must be numeric\", http.StatusBadRequest)\n\t\t}\n\t}\n\tqcx := h.api.Txf().NewQcx()\n\tdefer qcx.Abort()\n\tout, err := h.api.MutexCheck(r.Context(), qcx, indexName, fieldName, details, limit)\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\treturn\n\t}\n\toutBytes, err := json.Marshal(out)\n\tif err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"marshalling response: %v\", err), http.StatusInternalServerError)\n\t}\n\t_, err = w.Write(outBytes)\n\tif err != nil {\n\t\th.logger.Errorf(\"writing mutex-check response: %v\", err)\n\t}\n}\n\n// handleInternalGetMutexCheck handles internal (non-forwarding )/mutex-check requests.\nfunc (h *Handler) handleInternalGetMutexCheck(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\t// Get index and field type to determine how to handle the\n\t// import data.\n\tindexName, fieldName := mux.Vars(r)[\"index\"], mux.Vars(r)[\"field\"]\n\tq := r.URL.Query()\n\tlimit := 0\n\tdetails := q.Get(\"details\") == \"true\"\n\tlimitStr := q.Get(\"limit\")\n\tif limitStr != \"\" {\n\t\tvar err error\n\t\tlimit, err = strconv.Atoi(limitStr)\n\t\tif err != nil {\n\t\t\thttp.Error(w, \"limit must be numeric\", http.StatusBadRequest)\n\t\t}\n\t}\n\tqcx := h.api.Txf().NewQcx()\n\tdefer qcx.Abort()\n\tout, err := h.api.MutexCheckNode(r.Context(), qcx, indexName, fieldName, details, limit)\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\treturn\n\t}\n\toutBytes, err := json.Marshal(out)\n\tif err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"marshalling response: %v\", err), http.StatusInternalServerError)\n\t}\n\t_, err = w.Write(outBytes)\n\tif err != nil {\n\t\th.logger.Errorf(\"writing mutex-check response: %v\", err)\n\t}\n}\n\n// handlePostImportRoaring\nfunc (h *Handler) handlePostImportRoaring(w http.ResponseWriter, r *http.Request) {\n\t// Verify that request is only communicating over protobufs.\n\tif error, code := validateProtobufHeader(r); error != \"\" {\n\t\thttp.Error(w, error, code)\n\t\treturn\n\t}\n\n\t// Get index and field type to determine how to handle the\n\t// import data.\n\tindexName := mux.Vars(r)[\"index\"]\n\tfieldName := mux.Vars(r)[\"field\"]\n\n\tq := r.URL.Query()\n\tremoteStr := q.Get(\"remote\")\n\tvar remote bool\n\tif remoteStr == \"true\" {\n\t\tremote = true\n\t}\n\n\tctx := r.Context()\n\n\t// Read entire body.\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"io.ReadAll-Body\")\n\tbody, err := readBody(r)\n\tspan.LogKV(\"bodySize\", len(body))\n\tspan.Finish()\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\treq := &ImportRoaringRequest{}\n\tspan, _ = tracing.StartSpanFromContext(ctx, \"Unmarshal\")\n\terr = h.serializer.Unmarshal(body, req)\n\tspan.Finish()\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\turlVars := mux.Vars(r)\n\tshard, err := strconv.ParseUint(urlVars[\"shard\"], 10, 64)\n\tif err != nil {\n\t\thttp.Error(w, \"shard should be an unsigned integer\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tresp := &ImportResponse{}\n\t// TODO give meaningful stats for import\n\terr = h.api.ImportRoaring(ctx, indexName, fieldName, shard, remote, req)\n\tif err != nil {\n\t\tresp.Err = err.Error()\n\t\tif _, ok := err.(BadRequestError); ok {\n\t\t\tw.WriteHeader(http.StatusBadRequest)\n\t\t} else if _, ok := err.(NotFoundError); ok {\n\t\t\tw.WriteHeader(http.StatusNotFound)\n\t\t} else if _, ok := err.(PreconditionFailedError); ok {\n\t\t\tw.WriteHeader(http.StatusPreconditionFailed)\n\t\t} else {\n\t\t\tw.WriteHeader(http.StatusInternalServerError)\n\t\t}\n\t}\n\n\t// Marshal response object.\n\tbuf, err := h.serializer.Marshal(resp)\n\tif err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"marshal import-roaring response: %v\", err), http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\t// Write response.\n\t_, err = w.Write(buf)\n\tif err != nil {\n\t\th.logger.Errorf(\"writing import-roaring response: %v\", err)\n\t\treturn\n\t}\n}\n\n// handlePostShardImportRoaring takes data for multiple fields for a\n// particular shard and imports it all in a single transaction. It was\n// developed in the post-RBF world and should probably ultimately\n// replace most of the other import endpoints.\nfunc (h *Handler) handlePostShardImportRoaring(w http.ResponseWriter, r *http.Request) {\n\t// Verify that request is only communicating over protobufs.\n\tif error, code := validateProtobufHeader(r); error != \"\" {\n\t\thttp.Error(w, error, code)\n\t\treturn\n\t}\n\n\t// Get index and field type to determine how to handle the\n\t// import data.\n\tindexName := mux.Vars(r)[\"index\"]\n\n\tctx := r.Context()\n\n\t// Read entire body.\n\tspan, _ := tracing.StartSpanFromContext(ctx, \"io.ReadAll-Body\")\n\tbody, err := readBody(r)\n\tspan.LogKV(\"bodySize\", len(body))\n\tspan.Finish()\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\treq := &ImportRoaringShardRequest{}\n\tspan, _ = tracing.StartSpanFromContext(ctx, \"Unmarshal\")\n\terr = h.serializer.Unmarshal(body, req)\n\tspan.Finish()\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\turlVars := mux.Vars(r)\n\tshard, err := strconv.ParseUint(urlVars[\"shard\"], 10, 64)\n\tif err != nil {\n\t\thttp.Error(w, \"shard should be an unsigned integer\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tresp := &ImportResponse{}\n\t// TODO give meaningful stats for import\n\terr = h.api.ImportRoaringShard(ctx, indexName, shard, req)\n\tif err != nil {\n\t\tresp.Err = err.Error()\n\t\tif errors.Is(err, ErrIndexNotFound) {\n\t\t\tw.WriteHeader(http.StatusNotFound)\n\t\t} else if errors.As(err, &BadRequestError{}) {\n\t\t\tw.WriteHeader(http.StatusBadRequest)\n\t\t} else if _, ok := errors.Cause(err).(NotFoundError); ok {\n\t\t\tw.WriteHeader(http.StatusNotFound)\n\t\t} else if errors.As(err, &PreconditionFailedError{}) {\n\t\t\tw.WriteHeader(http.StatusPreconditionFailed)\n\t\t} else {\n\t\t\tw.WriteHeader(http.StatusInternalServerError)\n\t\t}\n\t}\n\n\t// Marshal response object.\n\tbuf, err := h.serializer.Marshal(resp)\n\tif err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"marshal shard-import-roaring response: %v\", err), http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\t// Write response.\n\t_, err = w.Write(buf)\n\tif err != nil {\n\t\th.logger.Errorf(\"writing shard-import-roaring response: %v\", err)\n\t\treturn\n\t}\n}\n\nfunc (h *Handler) handlePostTranslateKeys(w http.ResponseWriter, r *http.Request) {\n\t// Verify that request is only communicating over protobufs.\n\tif r.Header.Get(\"Content-Type\") != \"application/x-protobuf\" {\n\t\thttp.Error(w, \"Unsupported media type\", http.StatusUnsupportedMediaType)\n\t\treturn\n\t} else if r.Header.Get(\"Accept\") != \"application/x-protobuf\" {\n\t\thttp.Error(w, \"Not acceptable\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tbuf, err := h.api.TranslateKeys(r.Context(), r.Body)\n\tswitch errors.Cause(err) {\n\tcase nil:\n\t\t// Write response.\n\t\tif _, err = w.Write(buf); err != nil {\n\t\t\th.logger.Errorf(\"writing translate keys response: %v\", err)\n\t\t}\n\n\tcase ErrTranslatingKeyNotFound:\n\t\thttp.Error(w, fmt.Sprintf(\"translate keys: %v\", err), http.StatusNotFound)\n\n\tcase ErrTranslateStoreReadOnly:\n\t\thttp.Error(w, fmt.Sprintf(\"translate keys: %v\", err), http.StatusPreconditionFailed)\n\n\tdefault:\n\t\thttp.Error(w, fmt.Sprintf(\"translate keys: %v\", err), http.StatusInternalServerError)\n\t}\n}\n\nfunc (h *Handler) handlePostTranslateIDs(w http.ResponseWriter, r *http.Request) {\n\t// Verify that request is only communicating over protobufs.\n\tif r.Header.Get(\"Content-Type\") != \"application/x-protobuf\" {\n\t\thttp.Error(w, \"Unsupported media type\", http.StatusUnsupportedMediaType)\n\t\treturn\n\t} else if r.Header.Get(\"Accept\") != \"application/x-protobuf\" {\n\t\thttp.Error(w, \"Not acceptable\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\tbuf, err := h.api.TranslateIDs(r.Context(), r.Body)\n\tif err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"translate ids: %v\", err), http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\t// Write response.\n\t_, err = w.Write(buf)\n\tif err != nil {\n\t\th.logger.Errorf(\"writing translate keys response: %v\", err)\n\t}\n}\n\n// Read entire request body.\nfunc readBody(r *http.Request) ([]byte, error) {\n\tvar contentLength int64 = bytes.MinRead\n\tif r.ContentLength > 0 {\n\t\tcontentLength = r.ContentLength\n\t}\n\n\tbuf := bytes.NewBuffer(make([]byte, 0, 1+contentLength))\n\tif _, err := buf.ReadFrom(r.Body); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn buf.Bytes(), nil\n}\n\nfunc (h *Handler) handlePostTranslateFieldDB(w http.ResponseWriter, r *http.Request) {\n\tindexName, ok := mux.Vars(r)[\"index\"]\n\tif !ok {\n\t\thttp.Error(w, \"index name is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tfieldName, ok := mux.Vars(r)[\"field\"]\n\tif !ok {\n\t\thttp.Error(w, \"field name is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tbd, err := readBody(r)\n\tif err != nil {\n\t\thttp.Error(w, \"failed to read body\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tbr := bytes.NewReader(bd)\n\n\terr = h.api.TranslateFieldDB(r.Context(), indexName, fieldName, br)\n\tresp := successResponse{h: h, Name: fieldName}\n\tresp.check(err)\n\tresp.write(w, err)\n}\n\nfunc (h *Handler) handlePostTranslateIndexDB(w http.ResponseWriter, r *http.Request) {\n\tindexName, ok := mux.Vars(r)[\"index\"]\n\tif !ok {\n\t\thttp.Error(w, \"index name is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tpartitionArg, ok := mux.Vars(r)[\"partition\"]\n\tif !ok {\n\t\thttp.Error(w, \"partition is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tpartition, err := strconv.ParseUint(partitionArg, 10, 64)\n\tif err != nil {\n\t\thttp.Error(w, \"bad partition\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tbd, err := readBody(r)\n\tif err != nil {\n\t\thttp.Error(w, \"failed to read body\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tbr := bytes.NewReader(bd)\n\terr = h.api.TranslateIndexDB(r.Context(), indexName, int(partition), br)\n\tresp := successResponse{h: h, Name: indexName}\n\tresp.check(err)\n\tresp.write(w, err)\n}\n\nfunc (h *Handler) handleFindOrCreateKeys(w http.ResponseWriter, r *http.Request, requireField bool, create bool) {\n\t// Verify input and output types\n\tif r.Header.Get(\"Content-Type\") != \"application/json\" {\n\t\thttp.Error(w, \"Unsupported media type\", http.StatusUnsupportedMediaType)\n\t\treturn\n\t}\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"Not acceptable\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\tvar indexName, fieldName string\n\tvar keys []string\n\terr := func() error {\n\t\tvar ok bool\n\t\tindexName, ok = mux.Vars(r)[\"index\"]\n\t\tif !ok {\n\t\t\treturn errors.New(\"index name is required\")\n\t\t}\n\n\t\tif requireField {\n\t\t\tfieldName, ok = mux.Vars(r)[\"field\"]\n\t\t\tif !ok {\n\t\t\t\treturn errors.New(\"field name is required\")\n\t\t\t}\n\t\t}\n\n\t\tbd, err := readBody(r)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to read body: %v\", err)\n\t\t}\n\n\t\terr = json.Unmarshal(bd, &keys)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to decode request: %v\", err)\n\t\t}\n\t\treturn nil\n\t}()\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t}\n\tvar translations map[string]uint64\n\tswitch {\n\tcase requireField && create:\n\t\ttranslations, err = h.api.CreateFieldKeys(r.Context(), indexName, fieldName, keys...)\n\tcase requireField && !create:\n\t\ttranslations, err = h.api.FindFieldKeys(r.Context(), indexName, fieldName, keys...)\n\tcase !requireField && create:\n\t\ttranslations, err = h.api.CreateIndexKeys(r.Context(), indexName, keys...)\n\tcase !requireField && !create:\n\t\ttranslations, err = h.api.FindIndexKeys(r.Context(), indexName, keys...)\n\t}\n\tif err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"translating keys: %v\", err), http.StatusInternalServerError)\n\t\treturn\n\t}\n\tdata, err := json.Marshal(translations)\n\tif err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"encoding response: %v\", err), http.StatusInternalServerError)\n\t}\n\t_, err = w.Write(data)\n\tif err != nil {\n\t\th.logger.Printf(\"writing CreateFieldKeys response: %v\", err)\n\t}\n}\n\nfunc (h *Handler) handleFindIndexKeys(w http.ResponseWriter, r *http.Request) {\n\th.handleFindOrCreateKeys(w, r, false, false)\n}\n\nfunc (h *Handler) handleFindFieldKeys(w http.ResponseWriter, r *http.Request) {\n\th.handleFindOrCreateKeys(w, r, true, false)\n}\n\nfunc (h *Handler) handleCreateIndexKeys(w http.ResponseWriter, r *http.Request) {\n\th.handleFindOrCreateKeys(w, r, false, true)\n}\n\nfunc (h *Handler) handleCreateFieldKeys(w http.ResponseWriter, r *http.Request) {\n\th.handleFindOrCreateKeys(w, r, true, true)\n}\n\nfunc (h *Handler) handleMatchField(w http.ResponseWriter, r *http.Request) {\n\t// Verify output type.\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"Not acceptable\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tindexName, ok := mux.Vars(r)[\"index\"]\n\tif !ok {\n\t\thttp.Error(w, \"index name is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tfieldName, ok := mux.Vars(r)[\"field\"]\n\tif !ok {\n\t\thttp.Error(w, \"field name is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tbd, err := readBody(r)\n\tif err != nil {\n\t\thttp.Error(w, \"failed to read body\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tmatches, err := h.api.MatchField(r.Context(), indexName, fieldName, string(bd))\n\tif err != nil {\n\t\thttp.Error(w, \"failed to match pattern\", http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\terr = json.NewEncoder(w).Encode(matches)\n\tif err != nil {\n\t\thttp.Error(w, \"encoding result\", http.StatusBadRequest)\n\t\treturn\n\t}\n}\n\nfunc (h *Handler) handleReserveIDs(w http.ResponseWriter, r *http.Request) {\n\t// Verify input and output types\n\tif r.Header.Get(\"Content-Type\") != \"application/json\" {\n\t\thttp.Error(w, \"Unsupported media type\", http.StatusUnsupportedMediaType)\n\t\treturn\n\t}\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"Not acceptable\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tbd, err := readBody(r)\n\tif err != nil {\n\t\thttp.Error(w, \"failed to read body\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tvar req IDAllocReserveRequest\n\treq.Offset = ^uint64(0)\n\terr = json.Unmarshal(bd, &req)\n\tif err != nil {\n\t\thttp.Error(w, \"failed to decode request\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tids, err := h.api.ReserveIDs(req.Key, req.Session, req.Offset, req.Count)\n\tif err != nil {\n\t\tvar esync IDOffsetDesyncError\n\t\tif errors.As(err, &esync) {\n\t\t\tw.Header().Add(\"Content-Type\", \"application/json\")\n\t\t\tw.WriteHeader(http.StatusConflict)\n\t\t\terr = json.NewEncoder(w).Encode(struct {\n\t\t\t\tIDOffsetDesyncError\n\t\t\t\tErr string `json:\"error\"`\n\t\t\t}{\n\t\t\t\tIDOffsetDesyncError: esync,\n\t\t\t\tErr:                 err.Error(),\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\th.logger.Debugf(\"failed to send desync error: %v\", err)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\thttp.Error(w, fmt.Sprintf(\"reserving IDs: %v\", err.Error()), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tw.Header().Add(\"Content-Type\", \"application/json\")\n\terr = json.NewEncoder(w).Encode(ids)\n\tif err != nil {\n\t\thttp.Error(w, \"encoding result\", http.StatusBadRequest)\n\t\treturn\n\t}\n}\n\nfunc (h *Handler) handleCommitIDs(w http.ResponseWriter, r *http.Request) {\n\t// Verify input and output types\n\tif r.Header.Get(\"Content-Type\") != \"application/json\" {\n\t\thttp.Error(w, \"Unsupported media type\", http.StatusUnsupportedMediaType)\n\t\treturn\n\t} else if !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"Not acceptable\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tbd, err := readBody(r)\n\tif err != nil {\n\t\thttp.Error(w, \"failed to read body\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tvar req IDAllocCommitRequest\n\terr = json.Unmarshal(bd, &req)\n\tif err != nil {\n\t\thttp.Error(w, \"failed to decode request\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\terr = h.api.CommitIDs(req.Key, req.Session, req.Count)\n\tif err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"committing IDs: %v\", err.Error()), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tw.WriteHeader(http.StatusNoContent)\n}\n\nfunc (h *Handler) handleResetIDAlloc(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptType(r.Header, \"text\", \"plain\") {\n\t\thttp.Error(w, \"text/plain is not an acceptable response type\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\tindexName, ok := mux.Vars(r)[\"index\"]\n\tif !ok {\n\t\thttp.Error(w, \"index name is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\terr := h.api.ResetIDAlloc(indexName)\n\tif err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"resetting ID allocation: %v\", err.Error()), http.StatusBadRequest)\n\t\treturn\n\t}\n\tw.Header().Add(\"Content-Type\", \"text/plain\")\n\tw.WriteHeader(http.StatusOK)\n\tw.Write([]byte(\"OK\")) //nolint:errcheck\n}\n\nfunc (h *Handler) handleIDAllocData(w http.ResponseWriter, r *http.Request) {\n\tw.Header().Add(\"Content-Type\", \"application/octet-stream\")\n\tif err := h.api.WriteIDAllocDataTo(w); err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"writeing id allocation data: %v\", err.Error()), http.StatusInternalServerError)\n\t\treturn\n\t}\n}\n\nfunc (h *Handler) handleRestoreIDAlloc(w http.ResponseWriter, r *http.Request) {\n\tif err := h.api.RestoreIDAlloc(r.Body); err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"restoring id allocation: %v\", err.Error()), http.StatusInternalServerError)\n\t\treturn\n\t}\n\tw.Header().Add(\"Content-Type\", \"text/plain\")\n\tw.WriteHeader(http.StatusOK)\n\tw.Write([]byte(\"OK\")) //nolint:errcheck\n}\n\nfunc (h *Handler) handlePostRestore(w http.ResponseWriter, r *http.Request) {\n\tindexName, ok := mux.Vars(r)[\"index\"]\n\tif !ok {\n\t\thttp.Error(w, \"index name is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tshardID, ok := mux.Vars(r)[\"shardID\"]\n\tif !ok {\n\t\thttp.Error(w, \"shardID is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tshard, err := strconv.ParseUint(shardID, 10, 64)\n\tif err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"failed to parse shard %v %v err:%v\", indexName, shardID, err), http.StatusBadRequest)\n\t\treturn\n\t}\n\tctx := context.Background()\n\t// validate shard for this node\n\terr = h.api.RestoreShard(ctx, indexName, shard, r.Body)\n\tif err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"failed to restore shard %v %v err:%v\", indexName, shard, err), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tw.Header().Add(\"Content-Type\", \"text/plain\")\n\tw.WriteHeader(http.StatusOK)\n\tw.Write([]byte(\"OK\")) //nolint:errcheck\n}\n\n// handleDeleteDataframe handles DELETE /index/dataframe request.\nfunc (h *Handler) handleDeleteDataframe(w http.ResponseWriter, r *http.Request) {\n\tif !h.api.server.dataframeEnabled {\n\t\thttp.Error(w, \"Dataframe is disabled\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tindexName := mux.Vars(r)[\"index\"]\n\n\tresp := successResponse{h: h}\n\terr := h.api.DeleteDataframe(r.Context(), indexName)\n\tresp.write(w, err)\n}\n\nfunc (h *Handler) handlePostDataframeRestore(w http.ResponseWriter, r *http.Request) {\n\tindexName, ok := mux.Vars(r)[\"index\"]\n\tif !ok {\n\t\thttp.Error(w, \"index name is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tshardID, ok := mux.Vars(r)[\"shardID\"]\n\tif !ok {\n\t\thttp.Error(w, \"shardID is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tshard, err := strconv.ParseUint(shardID, 10, 64)\n\tif err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"failed to parse shard %v %v err:%v\", indexName, shardID, err), http.StatusBadRequest)\n\t\treturn\n\t}\n\tidx, err := h.api.Index(r.Context(), indexName)\n\tif err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"Index %s Not Found\", indexName), http.StatusNotFound)\n\t\treturn\n\t}\n\tfilename := idx.GetDataFramePath(shard) + h.api.server.executor.TableExtension()\n\tdest, err := os.Create(filename)\n\tif err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"failed to create restore dataframe shard %v %v err:%v\", indexName, shard, err), http.StatusBadRequest)\n\t\treturn\n\t}\n\t_, err = io.Copy(dest, r.Body)\n\tif err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"failed to copy restore dataframe shard %v %v err:%v\", indexName, shard, err), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tw.Header().Add(\"Content-Type\", \"text/plain\")\n\tw.WriteHeader(http.StatusOK)\n\tw.Write([]byte(\"OK\")) //nolint:errcheck\n}\n\nfunc (h *Handler) handleLogin(w http.ResponseWriter, r *http.Request) {\n\tif h.auth == nil {\n\t\thttp.Error(w, \"\", http.StatusNoContent)\n\t\treturn\n\t}\n\n\th.auth.Login(w, r)\n}\n\nfunc (h *Handler) handleRedirect(w http.ResponseWriter, r *http.Request) {\n\tif h.auth == nil {\n\t\thttp.Error(w, \"\", http.StatusNoContent)\n\t\treturn\n\t}\n\th.auth.Redirect(w, r)\n}\n\n// handleOAuthConfig handles requests for a cleaned version of our oAuthConfig. We\n// use this endpoint /internal/oauth-config in the `featurebase auth-token`\n// subcommand to create a RedirectURL on the fly.\nfunc (h *Handler) handleOAuthConfig(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tif h.auth == nil {\n\t\thttp.Error(w, \"auth not enabled: no OAuthConfig\", http.StatusNotFound)\n\t\treturn\n\t}\n\n\tconfig := h.auth.CleanOAuthConfig()\n\tif err := json.NewEncoder(w).Encode(config); err != nil {\n\t\th.logger.Errorf(\"writing oauth-config info: %s\", err)\n\t}\n}\n\nfunc (h *Handler) handleCheckAuthentication(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\tif h.auth == nil {\n\t\thttp.Error(w, \"\", http.StatusNoContent)\n\t\treturn\n\t}\n\n\taccess, refresh := getTokens(r)\n\tuinfo, err := h.auth.Authenticate(access, refresh)\n\tif uinfo == nil || err != nil {\n\t\tw.Header().Add(\"Content-Type\", \"text/plain\")\n\t\thttp.Error(w, err.Error(), http.StatusUnauthorized)\n\t\treturn\n\t}\n\t// just in case it got refreshed\n\th.auth.SetCookie(w, uinfo.Token, uinfo.RefreshToken, uinfo.Expiry)\n\n\tw.Header().Add(\"Content-Type\", \"text/plain\")\n\tw.WriteHeader(http.StatusOK)\n\tw.Write([]byte(\"OK\")) //nolint:errcheck\n}\n\nfunc (h *Handler) handleUserInfo(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\tif h.auth == nil {\n\t\thttp.Error(w, \"\", http.StatusNoContent)\n\t\treturn\n\t}\n\n\taccess, refresh := getTokens(r)\n\tuinfo, err := h.auth.Authenticate(access, refresh)\n\tif err != nil {\n\t\th.logger.Errorf(\"error authenticating: %v\", err)\n\t\thttp.Error(w, err.Error(), http.StatusForbidden)\n\t\treturn\n\t}\n\t// just in case it got refreshed\n\th.auth.SetCookie(w, uinfo.Token, uinfo.RefreshToken, uinfo.Expiry)\n\n\tif err := json.NewEncoder(w).Encode(uinfo); err != nil {\n\t\th.logger.Errorf(\"writing user info: %s\", err)\n\t}\n}\n\nfunc (h *Handler) handleLogout(w http.ResponseWriter, r *http.Request) {\n\tif h.auth == nil {\n\t\thttp.Error(w, \"\", http.StatusNoContent)\n\t\treturn\n\t}\n\th.auth.Logout(w, r)\n}\n\n// getTokens gets the access and refresh tokens from the request,\n// returning empty strings if they aren't in the request.\nfunc getTokens(r *http.Request) (string, string) {\n\tvar access, refresh string\n\tif token, ok := r.Header[\"Authorization\"]; ok && len(token) > 0 {\n\t\tparts := strings.Split(token[0], \"Bearer \")\n\t\tif len(parts) >= 2 {\n\t\t\taccess = parts[1]\n\t\t}\n\t}\n\n\tif token, ok := r.Header[authn.RefreshHeaderName]; ok && len(token) > 0 {\n\t\trefresh = token[0]\n\t}\n\n\tif access == \"\" {\n\t\taccessCookie, err := r.Cookie(authn.AccessCookieName)\n\t\tif err != nil {\n\t\t\treturn access, refresh\n\t\t}\n\t\taccess = accessCookie.Value\n\t}\n\n\tif refresh == \"\" {\n\t\trefreshCookie, err := r.Cookie(authn.RefreshCookieName)\n\t\tif err != nil {\n\t\t\treturn access, refresh\n\t\t}\n\t\trefresh = refreshCookie.Value\n\t}\n\n\treturn access, refresh\n}\n\nfunc (h *Handler) handleGetDirective(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tapplied, err := h.api.DirectiveApplied(r.Context())\n\tif err != nil {\n\t\thttp.Error(w, \"getting directive applied error: \"+err.Error(), http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\tresp := struct {\n\t\tApplied bool `json:\"applied\"`\n\t}{\n\t\tApplied: applied,\n\t}\n\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tif err := json.NewEncoder(w).Encode(resp); err != nil {\n\t\th.logger.Errorf(\"write status response error: %s\", err)\n\t}\n}\n\nfunc (h *Handler) handlePostDirective(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tbody := r.Body\n\tdefer body.Close()\n\n\td := &dax.Directive{}\n\tif err := json.NewDecoder(body).Decode(d); err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tif err := h.api.Directive(r.Context(), d); err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tw.WriteHeader(http.StatusOK)\n}\n\n// POST /snapshot/shard-data\nfunc (h *Handler) handlePostSnapshotShardData(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tbody := r.Body\n\tdefer body.Close()\n\n\treq := &dax.SnapshotShardDataRequest{}\n\tif err := json.NewDecoder(body).Decode(req); err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tif err := h.api.SnapshotShardData(r.Context(), req); err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tw.WriteHeader(http.StatusOK)\n}\n\n// POST /snapshot/table-keys\nfunc (h *Handler) handlePostSnapshotTableKeys(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tbody := r.Body\n\tdefer body.Close()\n\n\treq := &dax.SnapshotTableKeysRequest{}\n\tif err := json.NewDecoder(body).Decode(req); err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tif err := h.api.SnapshotTableKeys(r.Context(), req); err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tw.WriteHeader(http.StatusOK)\n}\n\n// POST /snapshot/field-keys\nfunc (h *Handler) handlePostSnapshotFieldKeys(w http.ResponseWriter, r *http.Request) {\n\tif !validHeaderAcceptJSON(r.Header) {\n\t\thttp.Error(w, \"JSON only acceptable response\", http.StatusNotAcceptable)\n\t\treturn\n\t}\n\n\tbody := r.Body\n\tdefer body.Close()\n\n\treq := &dax.SnapshotFieldKeysRequest{}\n\tif err := json.NewDecoder(body).Decode(req); err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tif err := h.api.SnapshotFieldKeys(r.Context(), req); err != nil {\n\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tw.WriteHeader(http.StatusOK)\n}\n\n// GET /health\nfunc (h *Handler) handleGetHealth(w http.ResponseWriter, r *http.Request) {\n\tw.WriteHeader(http.StatusOK)\n}\n\nfunc init() {\n\tgob.Register(arrow.PrimitiveTypes.Int64)\n\tgob.Register(arrow.PrimitiveTypes.Float64)\n\tgob.Register(arrow.BinaryTypes.String)\n}\n\n// EXPERIMENTAL API MAY CHANGE\n\nfunc (h *Handler) handleGetDataframeSchema(w http.ResponseWriter, r *http.Request) {\n\tif !h.api.server.dataframeEnabled {\n\t\thttp.Error(w, \"Dataframe is disabled\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tindexName, ok := mux.Vars(r)[\"index\"]\n\tif !ok {\n\t\thttp.Error(w, \"index name is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tparts, err := h.api.GetDataframeSchema(r.Context(), indexName)\n\tif err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"%v\", err), http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tif err := json.NewEncoder(w).Encode(parts); err != nil {\n\t\th.logger.Errorf(\"dataframe schema err: %s\", err)\n\t}\n}\n\nfunc (h *Handler) handleGetDataframe(w http.ResponseWriter, r *http.Request) {\n\tif !h.api.server.dataframeEnabled {\n\t\thttp.Error(w, \"Dataframe is disabled\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tindexName, ok := mux.Vars(r)[\"index\"]\n\n\tif !ok {\n\t\thttp.Error(w, \"index name is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tshardString, ok := mux.Vars(r)[\"shard\"]\n\tif !ok {\n\t\thttp.Error(w, \"shard is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tshard, err := strconv.ParseUint(shardString, 10, 64)\n\tif err != nil {\n\t\thttp.Error(w, \"shard should be an unsigned integer\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tidx, err := h.api.Index(r.Context(), indexName)\n\tif err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"Index %s Not Found\", indexName), http.StatusNotFound)\n\t\treturn\n\t}\n\tfilename := idx.GetDataFramePath(shard) + h.api.server.executor.TableExtension()\n\thttp.ServeFile(w, r, filename)\n}\n\nfunc (h *Handler) handlePostDataframe(w http.ResponseWriter, r *http.Request) {\n\tif !h.api.server.dataframeEnabled {\n\t\thttp.Error(w, \"Dataframe is disabled\", http.StatusBadRequest)\n\t\treturn\n\t}\n\t// TODO(twg) 2022/09/29 validate the request\n\tindexName, ok := mux.Vars(r)[\"index\"]\n\tif !ok {\n\t\thttp.Error(w, \"index name is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tshardString, ok := mux.Vars(r)[\"shard\"]\n\tif !ok {\n\t\thttp.Error(w, \"shard is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tshard, err := strconv.ParseUint(shardString, 10, 64)\n\tif err != nil {\n\t\thttp.Error(w, \"shard should be an unsigned integer\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tbody, err := readBody(r)\n\tif err != nil {\n\t\thttp.Error(w, \"reading body\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tblob := bytes.NewReader(body)\n\tdec := gob.NewDecoder(blob)\n\tvar changesetRequest ChangesetRequest\n\terr = dec.Decode(&changesetRequest)\n\tif err != nil {\n\t\thttp.Error(w, \"decoding request\", http.StatusBadRequest)\n\t\treturn\n\t}\n\terr = h.api.ApplyDataframeChangeset(r.Context(), indexName, &changesetRequest, shard)\n\tif err != nil {\n\t\thttp.Error(w, fmt.Sprintf(\"err:%v\", err), http.StatusBadRequest)\n\t\treturn\n\t}\n\tmsg := \"\"\n\n\tresp := struct {\n\t\tIndex string\n\t\tErr   string\n\t}{\n\t\tIndex: indexName,\n\t\tErr:   msg,\n\t}\n\tw.Header().Add(\"Content-Type\", \"application/json\")\n\terr = json.NewEncoder(w).Encode(resp)\n\tif err != nil {\n\t\thttp.Error(w, err.Error(), 500)\n\t\treturn\n\t}\n}\n\nfunc (h *Handler) DiscardHTTPServerLogs() {\n\th.server.ErrorLog = log.New(io.Discard, \"\", 0)\n}\n"
        },
        {
          "name": "http_handler_internal_test.go",
          "type": "blob",
          "size": 29.7119140625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/hex\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"net/http/httptest\"\n\t\"net/url\"\n\t\"os\"\n\t\"reflect\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/authn\"\n\t\"github.com/golang-jwt/jwt\"\n\t\"golang.org/x/oauth2\"\n\n\t\"github.com/featurebasedb/featurebase/v3/authz\"\n\t\"github.com/featurebasedb/featurebase/v3/logger\"\n\t\"github.com/featurebasedb/featurebase/v3/pql\"\n)\n\n// Test custom UnmarshalJSON for postIndexRequest object\nfunc TestPostIndexRequestUnmarshalJSON(t *testing.T) {\n\ttests := []struct {\n\t\tjson     string\n\t\texpected postIndexRequest\n\t\terr      string\n\t}{\n\t\t{json: `{\"options\": {}}`, expected: postIndexRequest{Options: IndexOptions{TrackExistence: true}}},\n\t\t{json: `{\"options\": {\"trackExistence\": false}}`, expected: postIndexRequest{Options: IndexOptions{TrackExistence: false}}},\n\t\t{json: `{\"options\": {\"keys\": true}}`, expected: postIndexRequest{Options: IndexOptions{Keys: true, TrackExistence: true}}},\n\t\t{json: `{\"options\": 4}`, err: \"options is not map[string]interface{}\"},\n\t\t{json: `{\"option\": {}}`, err: \"unknown key: option:map[]\"},\n\t\t{json: `{\"options\": {\"badKey\": \"test\"}}`, err: \"unknown key: badKey:test\"},\n\t}\n\tfor _, test := range tests {\n\t\tactual := &postIndexRequest{}\n\t\terr := json.Unmarshal([]byte(test.json), actual)\n\n\t\tif err != nil {\n\t\t\tif test.err == \"\" || test.err != err.Error() {\n\t\t\t\tt.Errorf(\"expected error: %v, but got result: %v\", test.err, err)\n\t\t\t}\n\t\t} else {\n\t\t\tif test.err != \"\" {\n\t\t\t\tt.Errorf(\"expected error: %v, but got no error\", test.err)\n\t\t\t}\n\t\t}\n\n\t\tif test.err == \"\" {\n\t\t\tif !reflect.DeepEqual(*actual, test.expected) {\n\t\t\t\tt.Errorf(\"expected: %v, but got: %v for JSON: %s\", test.expected, *actual, test.json)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Test custom UnmarshalJSON for postFieldRequest object\nfunc TestPostFieldRequestUnmarshalJSON(t *testing.T) {\n\tfoo := \"foo\"\n\ttests := []struct {\n\t\tjson     string\n\t\texpected postFieldRequest\n\t\terr      string\n\t}{\n\t\t{json: `{\"options\": {}}`, expected: postFieldRequest{}},\n\t\t{json: `{\"options\": 4}`, err: \"json: cannot unmarshal number\"},\n\t\t{json: `{\"option\": {}}`, err: `json: unknown field \"option\"`},\n\t\t{json: `{\"options\": {\"badKey\": \"test\"}}`, err: `json: unknown field \"badKey\"`},\n\t\t{json: `{\"options\": {\"inverseEnabled\": true}}`, err: `json: unknown field \"inverseEnabled\"`},\n\t\t{json: `{\"options\": {\"cacheType\": \"foo\"}}`, expected: postFieldRequest{Options: fieldOptions{CacheType: &foo}}},\n\t\t{json: `{\"options\": {\"inverse\": true, \"cacheType\": \"foo\"}}`, err: `json: unknown field \"inverse\"`},\n\t}\n\tfor i, test := range tests {\n\t\tactual := &postFieldRequest{}\n\t\tdec := json.NewDecoder(bytes.NewReader([]byte(test.json)))\n\t\tdec.DisallowUnknownFields()\n\t\terr := dec.Decode(actual)\n\t\tif err != nil {\n\t\t\tif test.err == \"\" || !strings.HasPrefix(err.Error(), test.err) {\n\t\t\t\tt.Errorf(\"test %d: expected error: %v, but got result: %v\", i, test.err, err)\n\t\t\t}\n\t\t}\n\n\t\tif test.err == \"\" {\n\t\t\tif !reflect.DeepEqual(*actual, test.expected) {\n\t\t\t\tt.Errorf(\"test %d: expected: %v, but got: %v\", i, test.expected, *actual)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc stringPtr(s string) *string {\n\treturn &s\n}\n\nfunc decimalPtr(d pql.Decimal) *pql.Decimal {\n\treturn &d\n}\n\n// Test fieldOption validation.\nfunc TestFieldOptionValidation(t *testing.T) {\n\ttimeQuantum := TimeQuantum(\"YMD\")\n\tdefaultCacheSize := uint32(DefaultCacheSize)\n\ttests := []struct {\n\t\tjson     string\n\t\texpected postFieldRequest\n\t\terr      string\n\t}{\n\t\t// FieldType: Set\n\t\t{json: `{\"options\": {}}`, expected: postFieldRequest{Options: fieldOptions{\n\t\t\tType:      FieldTypeSet,\n\t\t\tCacheType: stringPtr(DefaultCacheType),\n\t\t\tCacheSize: &defaultCacheSize,\n\t\t}}},\n\t\t{json: `{\"options\": {\"type\": \"set\"}}`, expected: postFieldRequest{Options: fieldOptions{\n\t\t\tType:      FieldTypeSet,\n\t\t\tCacheType: stringPtr(DefaultCacheType),\n\t\t\tCacheSize: &defaultCacheSize,\n\t\t}}},\n\t\t{json: `{\"options\": {\"type\": \"set\", \"cacheType\": \"lru\"}}`, expected: postFieldRequest{Options: fieldOptions{\n\t\t\tType:      FieldTypeSet,\n\t\t\tCacheType: stringPtr(\"lru\"),\n\t\t\tCacheSize: &defaultCacheSize,\n\t\t}}},\n\t\t{json: `{\"options\": {\"type\": \"set\", \"min\": 0}}`, err: \"min does not apply to field type set\"},\n\t\t{json: `{\"options\": {\"type\": \"set\", \"max\": 100}}`, err: \"max does not apply to field type set\"},\n\t\t{json: `{\"options\": {\"type\": \"set\", \"timeQuantum\": \"YMD\"}}`, err: \"timeQuantum does not apply to field type set\"},\n\t\t{json: `{\"options\": {\"type\": \"set\", \"ttl\": \"1h\"}}`, err: \"ttl does not apply to field type set\"},\n\n\t\t// FieldType: Int\n\t\t{json: `{\"options\": {\"type\": \"int\"}}`, err: \"min is required for field type int\"},\n\t\t{json: `{\"options\": {\"type\": \"int\", \"min\": 0}}`, err: \"max is required for field type int\"},\n\t\t{json: `{\"options\": {\"type\": \"int\", \"min\": 0, \"max\": 1001}}`, expected: postFieldRequest{Options: fieldOptions{\n\t\t\tType: FieldTypeInt,\n\t\t\tMin:  decimalPtr(pql.NewDecimal(0, 0)),\n\t\t\tMax:  decimalPtr(pql.NewDecimal(1001, 0)),\n\t\t}}},\n\t\t{json: `{\"options\": {\"type\": \"int\", \"min\": 0, \"max\": 1000, \"cacheType\": \"ranked\"}}`, err: \"cacheType does not apply to field type int\"},\n\t\t{json: `{\"options\": {\"type\": \"int\", \"min\": 0, \"max\": 1000, \"cacheSize\": 1000}}`, err: \"cacheSize does not apply to field type int\"},\n\t\t{json: `{\"options\": {\"type\": \"int\", \"min\": 0, \"max\": 1000, \"timeQuantum\": \"YMD\"}}`, err: \"timeQuantum does not apply to field type int\"},\n\t\t{json: `{\"options\": {\"type\": \"int\", \"min\": 0, \"max\": 1000, \"ttl\": \"1h\"}}`, err: \"ttl does not apply to field type int\"},\n\n\t\t// FieldType: Time\n\t\t{json: `{\"options\": {\"type\": \"time\"}}`, err: \"timeQuantum is required for field type time\"},\n\t\t{json: `{\"options\": {\"type\": \"time\", \"timeQuantum\": \"YMD\"}}`, expected: postFieldRequest{Options: fieldOptions{\n\t\t\tType:        FieldTypeTime,\n\t\t\tTimeQuantum: &timeQuantum,\n\t\t}}},\n\t\t{json: `{\"options\": {\"type\": \"time\", \"timeQuantum\": \"YMD\", \"min\": 0}}`, err: \"min does not apply to field type time\"},\n\t\t{json: `{\"options\": {\"type\": \"time\", \"timeQuantum\": \"YMD\", \"max\": 1000}}`, err: \"max does not apply to field type time\"},\n\t\t{json: `{\"options\": {\"type\": \"time\", \"timeQuantum\": \"YMD\", \"cacheType\": \"ranked\"}}`, err: \"cacheType does not apply to field type time\"},\n\t\t{json: `{\"options\": {\"type\": \"time\", \"timeQuantum\": \"YMD\", \"cacheSize\": 1000}}`, err: \"cacheSize does not apply to field type time\"},\n\t}\n\tfor i, test := range tests {\n\t\tactual := &postFieldRequest{}\n\t\tdec := json.NewDecoder(bytes.NewReader([]byte(test.json)))\n\t\tdec.DisallowUnknownFields()\n\t\terr := dec.Decode(actual)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"test %d: %v\", i, err)\n\t\t}\n\n\t\t// Validate field options.\n\t\tif err := actual.Options.validate(); err != nil {\n\t\t\tif test.err == \"\" || test.err != err.Error() {\n\t\t\t\tt.Errorf(\"test %d: expected error: %v, but got result: %v\", i, test.err, err)\n\t\t\t}\n\t\t}\n\n\t\tif test.err == \"\" {\n\t\t\tif !reflect.DeepEqual(*actual, test.expected) {\n\t\t\t\tt.Errorf(\"test %d: expected: %v, but got: %v\", i, test.expected, *actual)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc readResponse(w *httptest.ResponseRecorder) ([]byte, error) {\n\tres := w.Result()\n\tdefer res.Body.Close()\n\treturn io.ReadAll(res.Body)\n}\n\n// common variables used for testing auth\nvar (\n\tClientID         = \"e9088663-eb08-41d7-8f65-efb5f54bbb71\"\n\tClientSecret     = \"DEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEF\"\n\tAuthorizeURL     = \"https://login.microsoftonline.com/4a137d66-d161-4ae4-b1e6-07e9920874b8/oauth2/v2.0/authorize\"\n\tTokenURL         = \"https://login.microsoftonline.com/4a137d66-d161-4ae4-b1e6-07e9920874b8/oauth2/v2.0/token\"\n\tGroupEndpointURL = \"https://graph.microsoft.com/v1.0/me/transitiveMemberOf/microsoft.graph.group?$count=true\"\n\tLogoutURL        = \"https://login.microsoftonline.com/common/oauth2/v2.0/logout\"\n\tScopes           = []string{\"https://graph.microsoft.com/.default\", \"offline_access\"}\n\tKey              = \"DEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEF\"\n\tConfiguredIPs    = []string{}\n)\n\nfunc TestAuthentication(t *testing.T) {\n\ttype evaluate func(w *httptest.ResponseRecorder, data []byte)\n\ttype endpoint func(w http.ResponseWriter, r *http.Request)\n\n\ttype Group struct {\n\t\tGroupID   string `json:\"id\"`\n\t\tGroupName string `json:\"displayName\"`\n\t}\n\n\t// Groups holds a slice of Group for marshalling from JSON\n\ttype Groups struct {\n\t\tNextLink string  `json:\"@odata.nextLink\"`\n\t\tGroups   []Group `json:\"value\"`\n\t}\n\n\tgroupSrv := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tbody, err := json.Marshal(\n\t\t\tGroups{\n\t\t\t\tGroups: []Group{\n\t\t\t\t\t{\n\t\t\t\t\t\tGroupID:   \"what are you?\",\n\t\t\t\t\t\tGroupName: \"i am a carbon-based bipedal life form descended from an ape\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unexpected error marshalling groups response: %v\", err)\n\t\t}\n\t\tfmt.Fprintf(w, \"%s\", body)\n\t}))\n\tdefer groupSrv.Close()\n\n\tGroupEndpointURL = groupSrv.URL\n\tsecretKey, _ := hex.DecodeString(Key)\n\n\ta := NewTestAuth(t)\n\n\th := Handler{\n\t\tlogger:      logger.NewStandardLogger(os.Stdout),\n\t\tqueryLogger: logger.NewStandardLogger(os.Stdout),\n\t\tauth:        a,\n\t}\n\n\thOff := Handler{}\n\n\t// make a valid token\n\ttkn := jwt.New(jwt.SigningMethodHS256)\n\tclaims := tkn.Claims.(jwt.MapClaims)\n\tclaims[\"oid\"] = \"42\"\n\tclaims[\"name\"] = \"todd\"\n\tvalidToken, err := tkn.SignedString([]byte(secretKey))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tvalidToken = \"Bearer \" + validToken\n\n\ttoken := oauth2.Token{\n\t\tTokenType:    \"Bearer\",\n\t\tAccessToken:  \"asdf\",\n\t\tRefreshToken: \"abcdef\",\n\t\tExpiry:       time.Now().Add(time.Hour),\n\t}\n\n\t// make an expired token\n\tclaims[\"exp\"] = \"1\"\n\texpiredToken, err := tkn.SignedString([]byte(secretKey))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\texpiredToken = \"Bearer \" + expiredToken\n\n\tvalidCookie := &http.Cookie{\n\t\tName:     authn.AccessCookieName,\n\t\tValue:    token.AccessToken,\n\t\tPath:     \"/\",\n\t\tSecure:   true,\n\t\tHttpOnly: true,\n\t\tExpires:  token.Expiry,\n\t}\n\n\tpermissions1 := `\"user-groups\":\n  \"dca35310-ecda-4f23-86cd-876aee559900\":\n    \"test\": \"write\"\nadmin: \"ac97c9e2-346b-42a2-b6da-18bcb61a32fe\"`\n\n\ttests := []struct {\n\t\tname     string\n\t\tpath     string\n\t\tkind     string\n\t\tmethod   string\n\t\tyamlData string\n\t\ttoken    string\n\t\tcookie   *http.Cookie\n\t\thandler  endpoint\n\t\tfn       evaluate\n\t}{\n\t\t{\n\t\t\tname:    \"Login\",\n\t\t\tpath:    \"/login\",\n\t\t\tkind:    \"type1\",\n\t\t\tcookie:  validCookie,\n\t\t\thandler: h.handleLogin,\n\t\t\tfn: func(w *httptest.ResponseRecorder, data []byte) {\n\t\t\t\tif strings.Index(string(data), AuthorizeURL) != 9 {\n\t\t\t\t\tt.Errorf(\"incorrect redirect url: expected: %s, got: %s\", AuthorizeURL, string(data))\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:    \"Logout\",\n\t\t\tpath:    \"/logout\",\n\t\t\tkind:    \"type1\",\n\t\t\tcookie:  validCookie,\n\t\t\thandler: h.handleLogout,\n\t\t\tfn: func(w *httptest.ResponseRecorder, data []byte) {\n\t\t\t\tif w.Result().Cookies()[0].Value != \"\" {\n\t\t\t\t\tt.Errorf(\"expected cookie to be cleared, got: %+v\", w.Result().Cookies()[0].Value)\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:    \"Authenticate-ValidToken\",\n\t\t\tpath:    \"/auth\",\n\t\t\tkind:    \"bearer\",\n\t\t\ttoken:   validToken,\n\t\t\thandler: h.handleCheckAuthentication,\n\t\t\tfn: func(w *httptest.ResponseRecorder, data []byte) {\n\t\t\t\tif w.Result().StatusCode != 200 {\n\t\t\t\t\tbody, _ := readResponse(w)\n\t\t\t\t\tt.Errorf(\"expected http code 200, got: %+v with body: %+v\", w.Result().StatusCode, body)\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:    \"Authenticate-NoToken\",\n\t\t\tpath:    \"/auth\",\n\t\t\tkind:    \"type1\",\n\t\t\thandler: h.handleCheckAuthentication,\n\t\t\tfn: func(w *httptest.ResponseRecorder, data []byte) {\n\t\t\t\t// not token at all == status forbidden\n\t\t\t\tif w.Result().StatusCode != 401 {\n\t\t\t\t\tt.Errorf(\"expected http code 401, got: %+v\", w.Result().StatusCode)\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:    \"Authenticate-InvalidToken\",\n\t\t\tpath:    \"/auth\",\n\t\t\tkind:    \"type1\",\n\t\t\ttoken:   \"this isn't a real token\",\n\t\t\thandler: h.handleCheckAuthentication,\n\t\t\tfn: func(w *httptest.ResponseRecorder, data []byte) {\n\t\t\t\t// no valid token in header == Unauthorized\n\t\t\t\tif w.Result().StatusCode != http.StatusUnauthorized {\n\t\t\t\t\tt.Errorf(\"expected http code 401, got: %+v\", w.Result().StatusCode)\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:    \"Authenticate-ExpiredToken\",\n\t\t\tpath:    \"/auth\",\n\t\t\tkind:    \"type1\",\n\t\t\ttoken:   expiredToken,\n\t\t\thandler: h.handleCheckAuthentication,\n\t\t\tfn: func(w *httptest.ResponseRecorder, data []byte) {\n\t\t\t\t// expired token == unauthorized\n\t\t\t\tif w.Result().StatusCode != 401 {\n\t\t\t\t\tt.Errorf(\"expected http code 403, got: %+v\", w.Result().StatusCode)\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:    \"UserInfo\",\n\t\t\tpath:    \"/userinfo\",\n\t\t\tkind:    \"bearer\",\n\t\t\ttoken:   validToken,\n\t\t\thandler: h.handleUserInfo,\n\t\t\tfn: func(w *httptest.ResponseRecorder, data []byte) {\n\t\t\t\tuinfo := authn.UserInfo{}\n\t\t\t\terr = json.Unmarshal(data, &uinfo)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Errorf(\"unmarshalling userinfo\")\n\t\t\t\t}\n\t\t\t\tif uinfo.UserID != \"42\" && uinfo.UserName != \"todd\" {\n\t\t\t\t\tt.Errorf(\"expected http code 400, got: %+v\", uinfo)\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:    \"UserInfo-NoCookie\",\n\t\t\tpath:    \"/userinfo\",\n\t\t\tkind:    \"bearer\",\n\t\t\ttoken:   \"\",\n\t\t\thandler: h.handleUserInfo,\n\t\t\tfn: func(w *httptest.ResponseRecorder, data []byte) {\n\t\t\t\tif got := w.Result().StatusCode; got != http.StatusForbidden {\n\t\t\t\t\tt.Errorf(\"expected 403, got %v\", got)\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:    \"Redirect-NoAuthCode\",\n\t\t\tpath:    \"/redirect\",\n\t\t\tkind:    \"type1\",\n\t\t\tcookie:  validCookie,\n\t\t\thandler: h.handleRedirect,\n\t\t\tfn: func(w *httptest.ResponseRecorder, data []byte) {\n\t\t\t\tif strings.Index(string(data), AuthorizeURL) != 9 {\n\t\t\t\t\tif w.Result().StatusCode != 400 {\n\t\t\t\t\t\tt.Errorf(\"expected http code 400, got: %+v\", w.Result().StatusCode)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:    \"Redirect-SomeAuthCode\",\n\t\t\tpath:    \"/redirect\",\n\t\t\tkind:    \"type2\",\n\t\t\tcookie:  validCookie,\n\t\t\thandler: h.handleRedirect,\n\t\t\tfn: func(w *httptest.ResponseRecorder, data []byte) {\n\t\t\t\tif strings.Index(string(data), AuthorizeURL) != 9 {\n\t\t\t\t\tif w.Result().StatusCode != 400 {\n\t\t\t\t\t\tt.Errorf(\"expected http code 400, got: %+v\", w.Result().StatusCode)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:    \"Login-AuthOff\",\n\t\t\tpath:    \"/login\",\n\t\t\tkind:    \"type1\",\n\t\t\tcookie:  validCookie,\n\t\t\thandler: hOff.handleLogin,\n\t\t\tfn: func(w *httptest.ResponseRecorder, data []byte) {\n\t\t\t\tif strings.Index(string(data), AuthorizeURL) != 9 {\n\t\t\t\t\tif w.Result().StatusCode != 204 {\n\t\t\t\t\t\tt.Errorf(\"expected http code 204, got: %+v\", w.Result().StatusCode)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:    \"Logout-AuthOff\",\n\t\t\tpath:    \"/logout\",\n\t\t\tkind:    \"type1\",\n\t\t\tcookie:  validCookie,\n\t\t\thandler: hOff.handleLogout,\n\t\t\tfn: func(w *httptest.ResponseRecorder, data []byte) {\n\t\t\t\tif strings.Index(string(data), AuthorizeURL) != 9 {\n\t\t\t\t\tif w.Result().StatusCode != 204 {\n\t\t\t\t\t\tt.Errorf(\"expected http code 204, got: %+v\", w.Result().StatusCode)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:    \"UserInfo-AuthOff\",\n\t\t\tpath:    \"/userinfo\",\n\t\t\tkind:    \"type1\",\n\t\t\tcookie:  validCookie,\n\t\t\thandler: hOff.handleUserInfo,\n\t\t\tfn: func(w *httptest.ResponseRecorder, data []byte) {\n\t\t\t\tif strings.Index(string(data), AuthorizeURL) != 9 {\n\t\t\t\t\tif w.Result().StatusCode != 204 {\n\t\t\t\t\t\tt.Errorf(\"expected http code 204, got: %+v\", w.Result().StatusCode)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:    \"Authenticate-AuthOff\",\n\t\t\tpath:    \"/auth\",\n\t\t\tkind:    \"type1\",\n\t\t\tcookie:  validCookie,\n\t\t\thandler: hOff.handleCheckAuthentication,\n\t\t\tfn: func(w *httptest.ResponseRecorder, data []byte) {\n\t\t\t\tif strings.Index(string(data), AuthorizeURL) != 9 {\n\t\t\t\t\tif w.Result().StatusCode != 204 {\n\t\t\t\t\t\tt.Errorf(\"expected http code 204, got: %+v\", w.Result().StatusCode)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:    \"Redirect-AuthOff\",\n\t\t\tpath:    \"/redirect\",\n\t\t\tkind:    \"type1\",\n\t\t\tcookie:  validCookie,\n\t\t\thandler: hOff.handleRedirect,\n\t\t\tfn: func(w *httptest.ResponseRecorder, data []byte) {\n\t\t\t\tif strings.Index(string(data), AuthorizeURL) != 9 {\n\t\t\t\t\tif w.Result().StatusCode != 204 {\n\t\t\t\t\t\tt.Errorf(\"expected http code 204, got: %+v\", w.Result().StatusCode)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"MW-AuthOff\",\n\t\t\tpath:   \"/index/{index}/query\",\n\t\t\tkind:   \"middleware\",\n\t\t\tcookie: validCookie,\n\t\t\thandler: func(w http.ResponseWriter, r *http.Request) {\n\t\t\t\tf := hOff.chkAuthZ(hOff.handlePostQuery, authz.Admin)\n\t\t\t\tf(w, r)\n\t\t\t},\n\t\t\tfn: func(w *httptest.ResponseRecorder, data []byte) {\n\t\t\t\tif w.Result().StatusCode != 400 {\n\t\t\t\t\tt.Errorf(\"expected http code 400, got: %+v\", w.Result().StatusCode)\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"MW-CreateIndexInsufficientPerms\",\n\t\t\tpath:   \"/index/abcd\",\n\t\t\tkind:   \"bearer\",\n\t\t\tmethod: http.MethodPost,\n\t\t\ttoken:  validToken,\n\t\t\thandler: func(w http.ResponseWriter, r *http.Request) {\n\t\t\t\th := h\n\t\t\t\tvar p authz.GroupPermissions\n\t\t\t\tif err := p.ReadPermissionsFile(strings.NewReader(permissions1)); err != nil {\n\t\t\t\t\tt.Errorf(\"Error: %s\", err)\n\t\t\t\t}\n\t\t\t\th.permissions = &p\n\n\t\t\t\tf := h.chkAuthZ(h.handlePostIndex, authz.Admin)\n\t\t\t\tf(w, r)\n\t\t\t},\n\t\t\tfn: func(w *httptest.ResponseRecorder, data []byte) {\n\t\t\t\tif got, want := w.Result().StatusCode, http.StatusForbidden; got != want {\n\t\t\t\t\tt.Errorf(\"expected %v, got %v\", want, got)\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t// this tests that there are no permissions read in even though\n\t\t\t// auth is turned on, so we get a 500\n\t\t\tname:  \"MW-NoPermissions\",\n\t\t\tpath:  \"/index/{index}/query\",\n\t\t\tkind:  \"bearer\",\n\t\t\ttoken: validToken,\n\t\t\thandler: func(w http.ResponseWriter, r *http.Request) {\n\t\t\t\th := h\n\t\t\t\tf := h.chkAuthZ(h.handlePostQuery, authz.Write)\n\t\t\t\tf(w, r)\n\t\t\t},\n\t\t\tfn: func(w *httptest.ResponseRecorder, data []byte) {\n\t\t\t\tif got, want := w.Result().StatusCode, http.StatusInternalServerError; got != want {\n\t\t\t\t\tt.Errorf(\"expected %v, got %v\", want, got)\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:  \"MW-NoQuery\",\n\t\t\tpath:  \"/index/{index}/query\",\n\t\t\tkind:  \"bearer\",\n\t\t\ttoken: validToken,\n\t\t\thandler: func(w http.ResponseWriter, r *http.Request) {\n\t\t\t\th := h\n\t\t\t\tvar p authz.GroupPermissions\n\t\t\t\tif err := p.ReadPermissionsFile(strings.NewReader(permissions1)); err != nil {\n\t\t\t\t\tt.Errorf(\"Error: %s\", err)\n\t\t\t\t}\n\t\t\t\th.permissions = &p\n\t\t\t\tf := h.chkAuthZ(h.handlePostQuery, authz.Write)\n\t\t\t\tf(w, r)\n\t\t\t},\n\t\t\tfn: func(w *httptest.ResponseRecorder, data []byte) {\n\t\t\t\tif got, want := w.Result().StatusCode, http.StatusBadRequest; got != want {\n\t\t\t\t\tt.Errorf(\"expected %v, got: %+v\", want, got)\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tswitch test.kind {\n\t\tcase \"type1\", \"middleware\":\n\t\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\t\tr := httptest.NewRequest(http.MethodGet, test.path, nil)\n\t\t\t\tw := httptest.NewRecorder()\n\t\t\t\tif test.cookie != nil {\n\t\t\t\t\tr.AddCookie(test.cookie)\n\t\t\t\t}\n\t\t\t\ttest.handler(w, r)\n\t\t\t\tdata, err := readResponse(w)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Errorf(\"expected no errors reading response, got: %+v\", err)\n\t\t\t\t}\n\t\t\t\ttest.fn(w, data)\n\t\t\t})\n\t\tcase \"type2\":\n\t\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\t\tr := httptest.NewRequest(http.MethodGet, test.path, nil)\n\t\t\t\tw := httptest.NewRecorder()\n\t\t\t\tr.Form = url.Values{}\n\t\t\t\tr.Header.Set(\"Content-Type\", \"application/x-www-form-urlencoded\")\n\t\t\t\tr.Form.Add(\"code\", \"junk\")\n\n\t\t\t\ttest.handler(w, r)\n\t\t\t\tdata, err := readResponse(w)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Errorf(\"expected no errors reading response, got: %+v\", err)\n\t\t\t\t}\n\n\t\t\t\ttest.fn(w, data)\n\n\t\t\t})\n\t\tcase \"bearer\":\n\t\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\t\tif test.method == \"\" {\n\t\t\t\t\ttest.method = http.MethodGet\n\t\t\t\t}\n\t\t\t\tr := httptest.NewRequest(test.method, test.path, nil)\n\t\t\t\tw := httptest.NewRecorder()\n\t\t\t\tif test.token != \"\" {\n\t\t\t\t\tr.Header.Add(\"Authorization\", test.token)\n\t\t\t\t}\n\t\t\t\ttest.handler(w, r)\n\t\t\t\tdata, err := readResponse(w)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Errorf(\"expected no errors reading response, got: %+v\", err)\n\t\t\t\t}\n\t\t\t\ttest.fn(w, data)\n\t\t\t})\n\t\t}\n\n\t}\n\n}\n\nfunc TestChkAuthN(t *testing.T) {\n\ta := NewTestAuth(t)\n\th := Handler{\n\t\tlogger:      logger.NewStandardLogger(os.Stdout),\n\t\tqueryLogger: logger.NewStandardLogger(os.Stdout),\n\t\tauth:        a,\n\t}\n\n\t// make a valid token\n\ttkn := jwt.New(jwt.SigningMethodHS256)\n\tclaims := tkn.Claims.(jwt.MapClaims)\n\tclaims[\"oid\"] = \"42\"\n\tclaims[\"name\"] = \"A. Token\"\n\tvalidToken, err := tkn.SignedString(a.SecretKey())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tvalidToken = \"Bearer \" + validToken\n\n\t// make an invalid token\n\tinvalidToken := \"Bearer \" + \"thisis.a.bad.token\"\n\n\t// make an expired token\n\tclaims[\"exp\"] = \"1\"\n\texpiredToken, err := tkn.SignedString(a.SecretKey())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\texpiredToken = \"Bearer \" + expiredToken\n\n\ttestingHandler := func(w http.ResponseWriter, r *http.Request) {\n\t\t_, _ = w.Write([]byte(\"good\"))\n\t}\n\n\tcases := []struct {\n\t\tname     string\n\t\tendpoint string\n\t\ttoken    string\n\t\thandler  http.HandlerFunc\n\t\terr      string\n\t}{\n\t\t{\n\t\t\tname:    \"ValidToken-ButNotForMicrosoft\",\n\t\t\ttoken:   validToken,\n\t\t\thandler: h.chkAuthN(testingHandler),\n\t\t\terr:     \"authenticating: getting groups: getting group membership info\",\n\t\t},\n\t\t{\n\t\t\tname:    \"Invalid\",\n\t\t\ttoken:   invalidToken,\n\t\t\thandler: h.chkAuthN(testingHandler),\n\t\t\terr:     \"authenticating: parsing auth token\",\n\t\t},\n\t\t{\n\t\t\tname:    \"Expired\",\n\t\t\ttoken:   expiredToken,\n\t\t\thandler: h.chkAuthN(testingHandler),\n\t\t\terr:     \"authenticating: token is expired\",\n\t\t},\n\t}\n\tfor _, test := range cases {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tw := httptest.NewRecorder()\n\t\t\tr := httptest.NewRequest(\"GET\", \"/whatever\", nil)\n\t\t\tr.Header.Add(\"Authorization\", test.token)\n\t\t\ttest.handler(w, r)\n\t\t\tresp := w.Result()\n\t\t\tbody, err := io.ReadAll(resp.Body)\n\t\t\tdefer resp.Body.Close()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif !strings.HasPrefix(string(body), test.err) {\n\t\t\t\tt.Fatalf(\"expected error %s, got: %s\", test.err, string(body))\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestChkInternal(t *testing.T) {\n\ta := NewTestAuth(t)\n\tauthKey := \"DEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEF\"\n\th := Handler{\n\t\tlogger:      logger.NewStandardLogger(os.Stdout),\n\t\tqueryLogger: logger.NewStandardLogger(os.Stdout),\n\t\tauth:        a,\n\t}\n\n\ttestingHandler := func(w http.ResponseWriter, r *http.Request) {\n\t\tw.Write([]byte(\"good\"))\n\t}\n\n\tcases := []struct {\n\t\tname       string\n\t\tstatusCode int\n\t\thandler    http.HandlerFunc\n\t\tkey        string\n\t}{\n\t\t{\n\t\t\tname:       \"happyPath\",\n\t\t\tstatusCode: http.StatusOK,\n\t\t\thandler:    h.chkInternal(testingHandler),\n\t\t\tkey:        authKey,\n\t\t},\n\t\t{\n\t\t\tname:       \"unhappyPath-empty\",\n\t\t\tstatusCode: http.StatusUnauthorized,\n\t\t\thandler:    h.chkInternal(testingHandler),\n\t\t\tkey:        \"\",\n\t\t},\n\t\t{\n\t\t\tname:       \"unhappyPath-wrong\",\n\t\t\tstatusCode: http.StatusUnauthorized,\n\t\t\thandler:    h.chkInternal(testingHandler),\n\t\t\tkey:        \"BEABBEEFBEABBEEFBEABBEEFBEABBEEFBEABBEEFBEABBEEFBEABBEEFBEABBEEF\",\n\t\t},\n\t}\n\tfor _, test := range cases {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tw := httptest.NewRecorder()\n\t\t\tr := httptest.NewRequest(\"GET\", \"/whatever\", nil)\n\t\t\tif test.key != \"\" {\n\t\t\t\tr.Header.Add(\"X-Feature-Key\", test.key)\n\t\t\t}\n\t\t\ttest.handler(w, r)\n\t\t\tresp := w.Result()\n\t\t\tif resp.StatusCode != test.statusCode {\n\t\t\t\tt.Fatalf(\"expected %v, got %v\", test.statusCode, resp.StatusCode)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc NewTestAuth(t *testing.T) *authn.Auth {\n\tt.Helper()\n\ta, err := authn.NewAuth(\n\t\tlogger.NewStandardLogger(os.Stdout),\n\t\t\"http://localhost:10101/\",\n\t\tScopes,\n\t\tAuthorizeURL,\n\t\tTokenURL,\n\t\tGroupEndpointURL,\n\t\tLogoutURL,\n\t\tClientID,\n\t\tClientSecret,\n\t\tKey,\n\t\tConfiguredIPs,\n\t)\n\tif err != nil {\n\t\tt.Fatalf(\"building auth object%s\", err)\n\t}\n\treturn a\n}\n\nfunc TestHandleGetMemUsage(t *testing.T) {\n\th := Handler{\n\t\tlogger:      logger.NewStandardLogger(os.Stdout),\n\t\tqueryLogger: logger.NewStandardLogger(os.Stdout),\n\t}\n\tw := httptest.NewRecorder()\n\tr := httptest.NewRequest(\"GET\", \"/whatever\", nil)\n\n\th.handleGetMemUsage(w, r)\n\n\tresp := w.Result()\n\tif resp.StatusCode != http.StatusOK {\n\t\tt.Fatalf(\"expected %v, got %v\", http.StatusOK, resp.StatusCode)\n\t}\n}\n\nfunc TestHandleGetDiskUsage(t *testing.T) {\n\th := Handler{\n\t\tlogger:      logger.NewStandardLogger(os.Stdout),\n\t\tqueryLogger: logger.NewStandardLogger(os.Stdout),\n\t\tapi: &API{\n\t\t\tserver: &Server{\n\t\t\t\tdataDir: t.TempDir(),\n\t\t\t},\n\t\t},\n\t}\n\tw := httptest.NewRecorder()\n\tr := httptest.NewRequest(\"GET\", \"/whatever\", nil)\n\n\th.handleGetDiskUsage(w, r)\n\n\tresp := w.Result()\n\tif resp.StatusCode != http.StatusOK {\n\t\tt.Fatalf(\"expected %v, got %v\", http.StatusOK, resp.StatusCode)\n\t}\n}\n\nfunc TestHandleOAuthConfig(t *testing.T) {\n\th := Handler{\n\t\tlogger:      logger.NewStandardLogger(os.Stdout),\n\t\tqueryLogger: logger.NewStandardLogger(os.Stdout),\n\t\tauth:        NewTestAuth(t),\n\t\tapi: &API{\n\t\t\tserver: &Server{\n\t\t\t\tdataDir: t.TempDir(),\n\t\t\t},\n\t\t},\n\t}\n\tw := httptest.NewRecorder()\n\tr := httptest.NewRequest(\"GET\", \"/whatever\", nil)\n\n\th.handleOAuthConfig(w, r)\n\n\tresp := w.Result()\n\tif resp.StatusCode != http.StatusOK {\n\t\tt.Fatalf(\"expected %v, got %v\", http.StatusOK, resp.StatusCode)\n\t}\n\n\tdefer resp.Body.Close()\n\tvar rsp oauth2.Config\n\tif err := json.NewDecoder(resp.Body).Decode(&rsp); err != nil {\n\t\tt.Fatalf(\"unexpected error decoding body: %v\", err)\n\t}\n\tif exp := h.auth.CleanOAuthConfig(); !reflect.DeepEqual(exp, rsp) {\n\t\tt.Fatalf(\"expected %v, got %v\", exp, rsp)\n\t}\n}\n\nfunc TestAuthzAllowedIPs(t *testing.T) {\n\ttests := []struct {\n\t\tconfiguredIPs []string\n\t\tclientIP      string\n\t\tstatusCode    int\n\t\tpermission    authz.Permission\n\t}{\n\t\t// client IP is in configured IP list\n\t\t{\n\t\t\tconfiguredIPs: []string{\"10.0.0.0\", \"10.0.0.1\", \"10.0.2.0/32\"},\n\t\t\tclientIP:      \"10.0.0.0\",\n\t\t\tstatusCode:    http.StatusOK,\n\t\t\tpermission:    authz.Admin,\n\t\t},\n\t\t// client IP is in configured IP list, testing with CIDR address\n\t\t{\n\t\t\tconfiguredIPs: []string{\"10.0.0.0/30\"},\n\t\t\tclientIP:      \"10.0.0.1\",\n\t\t\tstatusCode:    http.StatusOK,\n\t\t\tpermission:    authz.Write,\n\t\t},\n\t\t// client IP has multiple IPs in X-Forwarded-For header\n\t\t// originating IP is in configured IP list\n\t\t{\n\t\t\tconfiguredIPs: []string{\"10.0.0.0\", \"10.0.0.1/32\", \"10.0.0.2\"},\n\t\t\tclientIP:      \"10.0.0.2,10.0.0.255\",\n\t\t\tstatusCode:    http.StatusOK,\n\t\t\tpermission:    authz.Read,\n\t\t},\n\t\t// client IP has multiple IPs in X-Forwarded-For header\n\t\t// originating IP is not in configured IP list\n\t\t{\n\t\t\tconfiguredIPs: []string{\"10.0.0.0/30\"},\n\t\t\tclientIP:      \"10.0.0.255,10.0.0.2\",\n\t\t\tstatusCode:    http.StatusForbidden,\n\t\t\tpermission:    authz.Read,\n\t\t},\n\t\t// client IP is not in configured IP list\n\t\t{\n\t\t\tconfiguredIPs: []string{\"10.0.0.0\", \"10.0.0.1\", \"10.0.2.0/32\"},\n\t\t\tclientIP:      \"10.0.0.3\",\n\t\t\tstatusCode:    http.StatusForbidden,\n\t\t\tpermission:    authz.Write,\n\t\t},\n\t\t// client IP is not in configured IP list\n\t\t// X-Forwarded-For header is an empty string\n\t\t{\n\t\t\tconfiguredIPs: []string{\"10.0.0.0\", \"10.0.0.1\", \"10.0.2.0/32\"},\n\t\t\tclientIP:      \"\",\n\t\t\tstatusCode:    http.StatusForbidden,\n\t\t\tpermission:    authz.Write,\n\t\t},\n\t}\n\n\ttestingHandler := func(w http.ResponseWriter, r *http.Request) {\n\t\tw.Write([]byte(\"good\"))\n\t}\n\n\tpermissions1 := `\"user-groups\":\n  \"dca35310-ecda-4f23-86cd-876aee559900\":\n    \"test\": \"write\"\nadmin: \"ac97c9e2-346b-42a2-b6da-18bcb61a32fe\"`\n\n\tfor i, test := range tests {\n\t\tt.Run(fmt.Sprintf(\"ChkAuthz-%d\", i), func(t *testing.T) {\n\t\t\tConfiguredIPs = test.configuredIPs\n\t\t\ta := NewTestAuth(t)\n\n\t\t\th := Handler{\n\t\t\t\tlogger:      logger.NewStandardLogger(os.Stdout),\n\t\t\t\tqueryLogger: logger.NewStandardLogger(os.Stdout),\n\t\t\t\tauth:        a,\n\t\t\t}\n\n\t\t\tvar p authz.GroupPermissions\n\t\t\tif err := p.ReadPermissionsFile(strings.NewReader(permissions1)); err != nil {\n\t\t\t\tt.Errorf(\"Error: %s\", err)\n\t\t\t}\n\t\t\th.permissions = &p\n\n\t\t\tr := httptest.NewRequest(\"GET\", \"/index/authz-abcd\", nil)\n\t\t\tr.Header.Set(ForwardedIPHeader, test.clientIP)\n\t\t\tw := httptest.NewRecorder()\n\n\t\t\thandler := h.chkAuthZ(testingHandler, authz.Read)\n\t\t\thandler(w, r)\n\t\t\tresp := w.Result()\n\t\t\tif resp.StatusCode != test.statusCode {\n\t\t\t\tt.Fatalf(\"expected %v, got %v\", test.statusCode, resp.StatusCode)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestAuthnAllowedIPs(t *testing.T) {\n\tIPList := []string{\"10.0.0.0\", \"10.0.0.1\", \"10.0.0.2\"}\n\tValidForwardedIP := \"10.0.0.0, 10.0.0.3, 10.0.0.4\"\n\tInvalidForwardedIP := \"10.0.0.3, 10.0.0.4\"\n\n\ttests := []struct {\n\t\tconfiguredIPs []string\n\t\tclientIP      string\n\t\tstatusCode    int\n\t\tsecretKey     string\n\t}{\n\t\t// test client IP was in configured IP list - happy path\n\t\t{\n\t\t\tconfiguredIPs: IPList,\n\t\t\tclientIP:      IPList[0],\n\t\t\tstatusCode:    http.StatusOK,\n\t\t},\n\t\t// test empty configured IP list\n\t\t{\n\t\t\tconfiguredIPs: []string{\"\"},\n\t\t\tclientIP:      IPList[0],\n\t\t\tstatusCode:    http.StatusUnauthorized,\n\t\t},\n\t\t// test client IP is not in configured IP list\n\t\t{\n\t\t\tconfiguredIPs: IPList,\n\t\t\tclientIP:      \"10.0.0.4\",\n\t\t\tstatusCode:    http.StatusUnauthorized,\n\t\t},\n\t\t// test multiple client IPs in X-forwarded-IP\n\t\t// originating IP is in configured IP list\n\t\t{\n\t\t\tconfiguredIPs: IPList,\n\t\t\tclientIP:      ValidForwardedIP,\n\t\t\tstatusCode:    http.StatusOK,\n\t\t},\n\t\t// test multiple client IPs in X-forwarded-IP\n\t\t// originating IP is not in configured IP list\n\t\t{\n\t\t\tconfiguredIPs: IPList,\n\t\t\tclientIP:      InvalidForwardedIP,\n\t\t\tstatusCode:    http.StatusUnauthorized,\n\t\t},\n\t}\n\n\ttestingHandler := func(w http.ResponseWriter, r *http.Request) {\n\t\tw.Write([]byte(\"good\"))\n\t}\n\n\tfor i, test := range tests {\n\t\tt.Run(fmt.Sprintf(\"ChkAuthn-%d\", i), func(t *testing.T) {\n\t\t\tConfiguredIPs = test.configuredIPs\n\t\t\ta := NewTestAuth(t)\n\n\t\t\th := Handler{\n\t\t\t\tlogger:      logger.NewStandardLogger(os.Stdout),\n\t\t\t\tqueryLogger: logger.NewStandardLogger(os.Stdout),\n\t\t\t\tauth:        a,\n\t\t\t}\n\n\t\t\tr := httptest.NewRequest(\"GET\", \"/index/authn-abcd\", nil)\n\t\t\tr.Header.Set(ForwardedIPHeader, test.clientIP)\n\t\t\tr = r.WithContext(context.Background())\n\t\t\tw := httptest.NewRecorder()\n\n\t\t\thandler := h.chkAuthN(testingHandler)\n\t\t\thandler(w, r)\n\t\t\tresp := w.Result()\n\t\t\tif resp.StatusCode != test.statusCode {\n\t\t\t\tt.Fatalf(\"expected %v, got %v\", test.statusCode, resp.StatusCode)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc Test_scrubPath(t *testing.T) {\n\ttests := []struct {\n\t\tname      string\n\t\tpathParts []string\n\t\twant      string\n\t}{\n\t\t{\n\t\t\tname:      \"happyPath\",\n\t\t\tpathParts: []string{\"\", \"index\", \"iname\", \"field\", \"fname\"},\n\t\t\twant:      \"/index/{index}/field/{field}\",\n\t\t},\n\t\t{\n\t\t\tname:      \"kindaHappyPath\",\n\t\t\tpathParts: []string{\"\", \"index\", \"iname\", \"field\", \"fname\"},\n\t\t\twant:      \"/index/{index}/field/{field}\",\n\t\t},\n\t\t{\n\t\t\tname:      \"sadPath\",\n\t\t\tpathParts: []string{\"\", \"notindex\", \"iname\", \"field\", \"fname\", \"field2\", \"fname2\"},\n\t\t\twant:      \"/notindex/iname/field/fname/field2/fname2\",\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tif got := scrubPath(tt.pathParts); got != tt.want {\n\t\t\t\tt.Errorf(\"scrubPath() = %v, want %v\", got, tt.want)\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "http_handler_test.go",
          "type": "blob",
          "size": 25.375,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa_test\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"net/http\"\n\t\"os\"\n\t\"path\"\n\t\"reflect\"\n\t\"sort\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\tpilosa \"github.com/featurebasedb/featurebase/v3\"\n\t\"github.com/featurebasedb/featurebase/v3/encoding/proto\"\n\t\"github.com/featurebasedb/featurebase/v3/server\"\n\t\"github.com/featurebasedb/featurebase/v3/test\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestHandlerOptions(t *testing.T) {\n\t_, err := pilosa.NewHandler()\n\tif err == nil {\n\t\tt.Fatalf(\"expected error making handler without options, got nil\")\n\t}\n\t_, err = pilosa.NewHandler(pilosa.OptHandlerAPI(&pilosa.API{}))\n\tif err == nil {\n\t\tt.Fatalf(\"expected error making handler without options, got nil\")\n\t}\n\n\tln, err := net.Listen(\"tcp\", \"localhost:0\")\n\tif err != nil {\n\t\tt.Fatalf(\"creating listener: %v\", err)\n\t}\n\n\t_, err = pilosa.NewHandler(pilosa.OptHandlerListener(ln, ln.Addr().String()))\n\tif err == nil {\n\t\tt.Fatalf(\"expected error making handler without options, got nil\")\n\t}\n\n\t_, err = pilosa.NewHandler(pilosa.OptHandlerListener(ln, ln.Addr().String()), pilosa.OptHandlerSerializer(proto.Serializer{}), pilosa.OptHandlerSerializer(proto.RoaringSerializer))\n\tif err == nil {\n\t\tt.Fatalf(\"expected error making handler without enough options, got nil\")\n\t}\n\n}\n\nfunc TestMarshalUnmarshalTransactionResponse(t *testing.T) {\n\ttests := []struct {\n\t\tname string\n\t\ttr   *pilosa.TransactionResponse\n\t}{\n\t\t{\n\t\t\tname: \"nil transaction\",\n\t\t\ttr:   &pilosa.TransactionResponse{},\n\t\t},\n\t\t{\n\t\t\tname: \"empty transaction\",\n\t\t\ttr:   &pilosa.TransactionResponse{Transaction: &pilosa.Transaction{}},\n\t\t},\n\t}\n\n\tfor _, tst := range tests {\n\t\tt.Run(tst.name, func(t *testing.T) {\n\t\t\tdata, err := json.Marshal(tst.tr)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"marshaling: %v\", err)\n\t\t\t}\n\n\t\t\tmytr := &pilosa.TransactionResponse{}\n\t\t\terr = json.Unmarshal(data, mytr)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"unmarshalling: %v\", err)\n\t\t\t}\n\n\t\t\tif mytr.Error != tst.tr.Error {\n\t\t\t\tt.Errorf(\"errors mismatch:exp/got \\n%v\\n%v\", tst.tr.Error, mytr.Error)\n\t\t\t}\n\t\t\ttest.CompareTransactions(t, tst.tr.Transaction, mytr.Transaction)\n\t\t})\n\t}\n}\n\nfunc TestUpdateFieldTTL(t *testing.T) {\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\n\ttests := []struct {\n\t\tname      string\n\t\tfield     string\n\t\tttl       string\n\t\tttlOption string\n\t\texpStatus int\n\t\texpErr    string\n\t\texpTTL    time.Duration\n\t}{\n\t\t{\n\t\t\t// test update ttl: set initial ttl to 10s, then update ttl to 48h\n\t\t\tname:      \"t1_48h\",\n\t\t\tfield:     \"t1_48h\",\n\t\t\tttl:       \"10s\",\n\t\t\tttlOption: `{\"option\": \"ttl\", \"value\": \"48h\"}`,\n\t\t\texpStatus: 200,\n\t\t\texpErr:    \"\",\n\t\t\texpTTL:    time.Hour * 48,\n\t\t},\n\t\t{\n\t\t\t// test unknown unit for ttl value\n\t\t\tname:      \"t2_unknown_unit\",\n\t\t\tfield:     \"t2_unknown_unit\",\n\t\t\tttl:       \"20s\",\n\t\t\tttlOption: `{\"option\": \"ttl\", \"value\": \"24abc\"}`,\n\t\t\texpStatus: 400,\n\t\t\texpErr:    `unknown unit \"abc\"`,\n\t\t\texpTTL:    time.Second * 20,\n\t\t},\n\t\t{\n\t\t\t// test invalid ttl value\n\t\t\tname:      \"t3_invalid\",\n\t\t\tfield:     \"t3_invalid\",\n\t\t\tttl:       \"30s\",\n\t\t\tttlOption: `{\"option\": \"ttl\", \"value\": \"abcdef\"}`,\n\t\t\texpStatus: 400,\n\t\t\texpErr:    `invalid duration \"abcdef\"`,\n\t\t\texpTTL:    time.Second * 30,\n\t\t},\n\t\t{\n\t\t\t// test empty ttl value\n\t\t\tname:      \"t4_invalid_empty\",\n\t\t\tfield:     \"t4_invalid_empty\",\n\t\t\tttl:       \"40s\",\n\t\t\tttlOption: `{\"option\": \"ttl\", \"value\": \"\"}`,\n\t\t\texpStatus: 400,\n\t\t\texpErr:    `invalid duration \"\"`,\n\t\t\texpTTL:    time.Second * 40,\n\t\t},\n\t\t{\n\t\t\t// test non existent field name\n\t\t\tname:      \"t5_diff_field_name\",\n\t\t\tfield:     \"t5_diff_field\",\n\t\t\tttl:       \"50s\",\n\t\t\tttlOption: `{\"option\": \"ttl\", \"value\": \"\"}`,\n\t\t\texpStatus: 404,\n\t\t\texpErr:    `key does not exist`,\n\t\t\texpTTL:    time.Second * 50,\n\t\t},\n\t\t{\n\t\t\t// test without field name\n\t\t\tname:      \"t6_empty_field_name\",\n\t\t\tfield:     \"\",\n\t\t\tttl:       \"60s\",\n\t\t\tttlOption: `{\"option\": \"ttl\", \"value\": \"12h\"}`,\n\t\t\texpStatus: 405,\n\t\t\texpErr:    `405 Method Not Allowed`,\n\t\t\texpTTL:    time.Second * 60,\n\t\t},\n\t\t{\n\t\t\t// test negative value for ttl\n\t\t\tname:      \"t7_negative\",\n\t\t\tfield:     \"t7_negative\",\n\t\t\tttl:       \"70s\",\n\t\t\tttlOption: `{\"option\": \"ttl\", \"value\": \"-12h\"}`,\n\t\t\texpStatus: 400,\n\t\t\texpErr:    `ttl can't be negative`,\n\t\t\texpTTL:    time.Second * 70,\n\t\t},\n\t}\n\n\tindexName := c.Idx(\"s\")\n\tfor i, test := range tests {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tc.CreateField(t, indexName, pilosa.IndexOptions{}, test.name, pilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMD\"), test.ttl))\n\t\t\tnodeURL := fmt.Sprintf(\"%s/index/%s/field/%s\", c.Nodes[0].URL(), c, test.field)\n\t\t\treq, err := http.NewRequest(\"PATCH\", nodeURL, strings.NewReader(test.ttlOption))\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\treq.Header.Set(\"Content-Type\", \"application/json\")\n\t\t\tresp, err := http.DefaultClient.Do(req)\n\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"doing option request: %v\", err)\n\t\t\t}\n\n\t\t\tif resp.StatusCode != test.expStatus {\n\t\t\t\tt.Errorf(\"expected status: '%d', got: '%d'\", test.expStatus, resp.StatusCode)\n\t\t\t}\n\n\t\t\tif resp.StatusCode == 400 || resp.StatusCode == 404 {\n\t\t\t\t// unmarshal error message to check against expErr\n\t\t\t\tvar respBody map[string]interface{}\n\t\t\t\terr := json.NewDecoder(resp.Body).Decode(&respBody)\n\t\t\t\tassert.NoError(t, err)\n\t\t\t\terrMsg := respBody[\"error\"].(map[string]interface{})[\"message\"].(string)\n\n\t\t\t\tif !strings.Contains(errMsg, test.expErr) {\n\t\t\t\t\tt.Errorf(\"expected error: '%s', got: '%s'\", test.expErr, errMsg)\n\t\t\t\t}\n\t\t\t} else if resp.StatusCode == 405 {\n\t\t\t\tif !strings.Contains(resp.Status, test.expErr) {\n\t\t\t\t\tt.Errorf(\"expected error: '%s', got: '%s'\", test.expErr, resp.Status)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// find updated field and check its TTL\n\t\t\tfor _, node := range c.Nodes {\n\t\t\t\tii, err := node.API.Schema(context.Background(), false)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"getting schema: %v\", err)\n\t\t\t\t}\n\t\t\t\tfor _, idx := range ii {\n\t\t\t\t\tif idx.Name == indexName {\n\t\t\t\t\t\tif len(idx.Fields) <= i {\n\t\t\t\t\t\t\tt.Fatalf(\"expected %d fields, last %s, got %d fields\",\n\t\t\t\t\t\t\t\ti, test.name, len(idx.Fields))\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif idx.Fields[i].Name == test.name {\n\t\t\t\t\t\t\tif idx.Fields[i].Options.TTL != test.expTTL {\n\t\t\t\t\t\t\t\tt.Errorf(\"expected noStandardView value: '%s', got: '%s'\", test.expTTL, idx.Fields[i].Options.TTL)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tt.Errorf(\"unexpected field: '%s', got: '%s'\", test.name, idx.Fields[i].Name)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t})\n\t}\n\n}\n\nfunc TestUpdateFieldNoStandardView(t *testing.T) {\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\n\ttests := []struct {\n\t\tname              string\n\t\tfield             string\n\t\tfieldOption       string\n\t\texpStatus         int\n\t\texpErr            string\n\t\texpNoStandardView bool\n\t}{\n\t\t{\n\t\t\t// test update noStandardView to true\n\t\t\tname:              \"t1_true\",\n\t\t\tfield:             \"t1_true\",\n\t\t\tfieldOption:       `{\"option\": \"noStandardView\", \"value\": \"true\"}`,\n\t\t\texpStatus:         200,\n\t\t\texpErr:            \"\",\n\t\t\texpNoStandardView: true,\n\t\t},\n\t\t{\n\t\t\t// test update noStandardView to false\n\t\t\tname:              \"t2_false\",\n\t\t\tfield:             \"t2_false\",\n\t\t\tfieldOption:       `{\"option\": \"noStandardView\", \"value\": \"false\"}`,\n\t\t\texpStatus:         200,\n\t\t\texpErr:            \"\",\n\t\t\texpNoStandardView: false,\n\t\t},\n\t\t{\n\t\t\t// test update noStandardView with invalid value\n\t\t\tname:              \"t3_invalid\",\n\t\t\tfield:             \"t3_invalid\",\n\t\t\tfieldOption:       `{\"option\": \"noStandardView\", \"value\": \"123\"}`,\n\t\t\texpStatus:         400,\n\t\t\texpErr:            `invalid value for noStandardView: '123'`,\n\t\t\texpNoStandardView: false,\n\t\t},\n\t\t{\n\t\t\t// test udpate noStandardView with empty value\n\t\t\tname:              \"t4_empty\",\n\t\t\tfield:             \"t4_empty\",\n\t\t\tfieldOption:       `{\"option\": \"noStandardView\", \"value\": \"\"}`,\n\t\t\texpStatus:         400,\n\t\t\texpErr:            `invalid value for noStandardView: ''`,\n\t\t\texpNoStandardView: false,\n\t\t},\n\t}\n\t// \"s\" to make it match %s behavior of a cluster\n\tindexName := c.Idx(\"s\")\n\n\tfor i, test := range tests {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tc.CreateField(t, indexName, pilosa.IndexOptions{}, test.name, pilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMD\"), \"0\"))\n\t\t\tnodeURL := fmt.Sprintf(\"%s/index/%s/field/%s\", c.Nodes[0].URL(), c, test.field)\n\t\t\treq, err := http.NewRequest(\"PATCH\", nodeURL, strings.NewReader(test.fieldOption))\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\treq.Header.Set(\"Content-Type\", \"application/json\")\n\t\t\tresp, err := http.DefaultClient.Do(req)\n\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"doing option request: %v\", err)\n\t\t\t}\n\n\t\t\tif resp.StatusCode != test.expStatus {\n\t\t\t\tt.Errorf(\"expected status: '%d', got: '%d'\", test.expStatus, resp.StatusCode)\n\t\t\t}\n\n\t\t\tif resp.StatusCode == 400 {\n\t\t\t\t// unmarshal error message to check against expErr\n\t\t\t\tvar respBody map[string]interface{}\n\t\t\t\terr := json.NewDecoder(resp.Body).Decode(&respBody)\n\t\t\t\tassert.NoError(t, err)\n\t\t\t\terrMsg := respBody[\"error\"].(map[string]interface{})[\"message\"].(string)\n\n\t\t\t\tif !strings.Contains(errMsg, test.expErr) {\n\t\t\t\t\tt.Errorf(\"expected error: '%s', got: '%s'\", test.expErr, errMsg)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// find updated field and check its noStandardView\n\t\t\tfor _, node := range c.Nodes {\n\t\t\t\tii, err := node.API.Schema(context.Background(), false)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"getting schema: %v\", err)\n\t\t\t\t}\n\t\t\t\tfor _, idx := range ii {\n\t\t\t\t\tif idx.Name == indexName {\n\t\t\t\t\t\tif len(idx.Fields) <= i {\n\t\t\t\t\t\t\tt.Fatalf(\"expected %d fields, last %s, got %d fields\",\n\t\t\t\t\t\t\t\ti, test.name, len(idx.Fields))\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif idx.Fields[i].Name == test.name {\n\t\t\t\t\t\t\tif idx.Fields[i].Options.NoStandardView != test.expNoStandardView {\n\t\t\t\t\t\t\t\tt.Errorf(\"expected noStandardView value: '%t', got: '%t'\", test.expNoStandardView, idx.Fields[i].Options.NoStandardView)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tt.Errorf(\"unexpected field: '%s', got: '%s'\", test.name, idx.Fields[i].Name)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestPostFieldWithTTL(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\tindexName := c.Idx(\"s\")\n\t_, err := c.GetNode(0).API.CreateIndex(context.Background(), indexName, pilosa.IndexOptions{Keys: true})\n\tif err != nil {\n\t\tt.Fatalf(\"creating index: %v\", err)\n\t}\n\tm := c.GetPrimary()\n\n\ttests := []struct {\n\t\tname      string\n\t\turl       string\n\t\toption    string\n\t\texpStatus int\n\t\texpErr    string\n\t\texpTTL    time.Duration\n\t}{\n\t\t{\n\t\t\tname:      \"t1_48h\",\n\t\t\turl:       fmt.Sprintf(\"%s/index/%s/field/t1_48h\", m.URL(), c),\n\t\t\toption:    `{ \"options\": {\"timeQuantum\":\"YMDH\",\"type\":\"time\",\"ttl\":\"48h\" }}`,\n\t\t\texpStatus: 200,\n\t\t\texpErr:    `\"success\":true`,\n\t\t\texpTTL:    time.Hour * 48,\n\t\t},\n\t\t{\n\t\t\tname:      \"t2_unknown_unit\",\n\t\t\turl:       fmt.Sprintf(\"%s/index/%s/field/t2_unknown_unit\", m.URL(), c),\n\t\t\toption:    `{ \"options\": {\"timeQuantum\":\"YMDH\",\"type\":\"time\",\"ttl\":\"24abc\" }}`,\n\t\t\texpStatus: 400,\n\t\t\texpErr:    \"cannot parse ttl\",\n\t\t},\n\t\t{\n\t\t\tname:      \"t3_invalid\",\n\t\t\turl:       fmt.Sprintf(\"%s/index/%s/field/t3_invalid\", m.URL(), c),\n\t\t\toption:    `{ \"options\": {\"timeQuantum\":\"YMDH\",\"type\":\"time\",\"ttl\":\"abcdef\" }}`,\n\t\t\texpStatus: 400,\n\t\t\texpErr:    \"cannot parse ttl\",\n\t\t},\n\t\t{\n\t\t\tname:      \"t4_invalid_empty\",\n\t\t\turl:       fmt.Sprintf(\"%s/index/%s/field/t4_invalid_empty\", m.URL(), c),\n\t\t\toption:    `{ \"options\": {\"timeQuantum\":\"YMDH\",\"type\":\"time\",\"ttl\":\"\" }}`,\n\t\t\texpStatus: 400,\n\t\t\texpErr:    \"cannot parse ttl\",\n\t\t},\n\t\t{\n\t\t\tname:      \"t5_negative\",\n\t\t\turl:       fmt.Sprintf(\"%s/index/%s/field/t5_negative\", m.URL(), c),\n\t\t\toption:    `{ \"options\": {\"timeQuantum\":\"YMDH\",\"type\":\"time\",\"ttl\":\"-24h\" }}`,\n\t\t\texpStatus: 400,\n\t\t\texpErr:    \"ttl can't be negative\",\n\t\t},\n\t}\n\n\tfor i, test_i := range tests {\n\t\tt.Run(test_i.name, func(t *testing.T) {\n\t\t\trespField := test.Do(t, \"POST\", test_i.url, test_i.option)\n\t\t\tif respField.StatusCode != test_i.expStatus {\n\t\t\t\tt.Errorf(\"expected status: '%d', got: '%d'\", test_i.expStatus, respField.StatusCode)\n\t\t\t}\n\n\t\t\tif !strings.Contains(respField.Body, test_i.expErr) {\n\t\t\t\tt.Errorf(\"expected error: '%s', got: '%s'\", test_i.expErr, respField.Body)\n\t\t\t}\n\n\t\t\t// find the created field and check its TTL\n\t\t\t// since only first test (t1_48h) is successful in creating a field,\n\t\t\t// dont find other fields from other test cases, it will cause index out of bound\n\t\t\tif test_i.name == \"t1_48h\" {\n\t\t\t\tfor _, node := range c.Nodes {\n\t\t\t\t\tii, err := node.API.Schema(context.Background(), false)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tt.Fatalf(\"getting schema: %v\", err)\n\t\t\t\t\t}\n\n\t\t\t\t\tfor _, idx := range ii {\n\t\t\t\t\t\tif idx.Name == indexName {\n\t\t\t\t\t\t\tif len(idx.Fields) <= i {\n\t\t\t\t\t\t\t\tt.Fatalf(\"expected %d fields, last %s, got %d fields\",\n\t\t\t\t\t\t\t\t\ti, test_i.name, len(idx.Fields))\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif idx.Fields[i].Name == test_i.name {\n\t\t\t\t\t\t\t\tif idx.Fields[i].Options.TTL != test_i.expTTL {\n\t\t\t\t\t\t\t\t\tt.Errorf(\"expected TTL: '%s', got: '%s'\", test_i.expTTL, idx.Fields[i].Options.TTL)\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tt.Errorf(\"unexpected field: '%s', got: '%s'\", test_i.name, idx.Fields[i].Name)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestGetViewAndDelete(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\n\tm := c.GetPrimary()\n\n\t_, err := m.API.CreateIndex(context.Background(), c.Idx(\"s\"), pilosa.IndexOptions{})\n\tif err != nil {\n\t\tt.Fatalf(\"creating index: %v\", err)\n\t}\n\t_, err = m.API.CreateField(context.Background(), c.Idx(\"s\"), \"test_view\", pilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMDH\"), \"0\"))\n\tif err != nil {\n\t\tt.Fatalf(\"creating field: %v\", err)\n\t}\n\n\t// Send sample data\n\tpostQueryUrl := fmt.Sprintf(\"%s/index/%s/query\", m.URL(), c)\n\tqueryOption := `\n\tSet(1,test_view=1,2001-02-03T04:05)\n\t`\n\trespQuery := test.Do(t, \"POST\", postQueryUrl, string(queryOption))\n\tif respQuery.StatusCode != http.StatusOK {\n\t\tt.Errorf(\"posting query, status: %d, body=%s\", respQuery.StatusCode, respQuery.Body)\n\t}\n\n\t// The above sample data should create these views:\n\texpectedViewNames := []string{\n\t\t\"existence\",\n\t\t\"standard\",\n\t\t\"standard_2001\",\n\t\t\"standard_200102\",\n\t\t\"standard_20010203\",\n\t\t\"standard_2001020304\",\n\t}\n\n\t// Call view to get data\n\tviewUrl := fmt.Sprintf(\"%s/index/%s/field/test_view/view\", m.URL(), c)\n\trespView := test.Do(t, \"GET\", viewUrl, \"\")\n\tif respView.StatusCode != http.StatusOK {\n\t\tt.Errorf(\"view handler, status: %d, body=%s\", respView.StatusCode, respView.Body)\n\t}\n\n\ttype viewReponse struct {\n\t\tName  string `json:\"name\"`\n\t\tType  string `json:\"type\"`\n\t\tField string `json:\"field\"`\n\t\tIndex string `json:\"index\"`\n\t}\n\n\tvar parsedViews []viewReponse\n\tif err := json.Unmarshal([]byte(respView.Body), &parsedViews); err != nil {\n\t\tt.Errorf(\"parsing view, err: %s\", err)\n\t}\n\n\t// check if data from view matches with expectedViewNames\n\tparseViewNames := []string{}\n\tfor _, view := range parsedViews {\n\t\tparseViewNames = append(parseViewNames, view.Name)\n\t}\n\tsort.Strings(parseViewNames)\n\n\tif !reflect.DeepEqual(expectedViewNames, parseViewNames) {\n\t\tt.Fatalf(\"expected %v, but got %v\", expectedViewNames, parseViewNames)\n\t}\n\n\t// call delete on view standard_2001020304\n\tdeleteViewUrl := fmt.Sprintf(\"%s/index/%s/field/test_view/view/standard_2001020304\", m.URL(), c)\n\trespDelete := test.Do(t, \"DELETE\", deleteViewUrl, \"\")\n\tif respDelete.StatusCode != http.StatusOK {\n\t\tt.Errorf(\"delete handler, status: %d, body=%s\", respDelete.StatusCode, respDelete.Body)\n\t}\n\n\t// remove view that was deleted (standard_2001020304) from expectedViewNames\n\texpectedViewNames = expectedViewNames[:len(expectedViewNames)-1]\n\n\t// call view again\n\tviewUrl = fmt.Sprintf(\"%s/index/%s/field/test_view/view\", m.URL(), c)\n\trespView = test.Do(t, \"GET\", viewUrl, \"\")\n\tif respView.StatusCode != http.StatusOK {\n\t\tt.Errorf(\"view handler after delete, status: %d, body=%s\", respView.StatusCode, respView.Body)\n\t}\n\n\tif err := json.Unmarshal([]byte(respView.Body), &parsedViews); err != nil {\n\t\tt.Errorf(\"parsing view, err: %s\", err)\n\t}\n\n\t// check if data from view matches with expectedViewNames\n\tparseViewNames = []string{}\n\tfor _, view := range parsedViews {\n\t\tparseViewNames = append(parseViewNames, view.Name)\n\t}\n\tsort.Strings(parseViewNames)\n\n\tif !reflect.DeepEqual(expectedViewNames, parseViewNames) {\n\t\tt.Fatalf(\"after delete, expected %v, but got %v\", expectedViewNames, parseViewNames)\n\t}\n}\n\n// TestHandlerSQL tests that the json coming back from a POST /sql request has\n// the expected json tags.\nfunc TestHandlerSQL(t *testing.T) {\n\tcfg := server.NewConfig()\n\tc := test.MustRunCluster(t, 1, []server.CommandOption{\n\t\tserver.OptCommandConfig(cfg),\n\t})\n\tdefer c.Close()\n\n\tm := c.GetPrimary()\n\n\ttests := []struct {\n\t\tname    string\n\t\turl     string\n\t\tsql     string\n\t\texpKeys []string\n\t}{\n\t\t{\n\t\t\tname:    \"sql\",\n\t\t\turl:     \"/sql\",\n\t\t\tsql:     \"show tables\",\n\t\t\texpKeys: []string{\"schema\", \"data\", \"execution-time\"},\n\t\t},\n\t\t{\n\t\t\tname:    \"sql-with-plan\",\n\t\t\turl:     \"/sql?plan=1\",\n\t\t\tsql:     \"show tables\",\n\t\t\texpKeys: []string{\"schema\", \"data\", \"query-plan\", \"execution-time\"},\n\t\t},\n\t\t{\n\t\t\tname:    \"invalid-sql\",\n\t\t\turl:     \"/sql\",\n\t\t\tsql:     \"invalid sql\",\n\t\t\texpKeys: []string{\"error\", \"execution-time\"},\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tsqlURL := fmt.Sprintf(\"%s%s\", m.URL(), tt.url)\n\t\t\tresp := test.Do(t, \"POST\", sqlURL, tt.sql)\n\t\t\tif resp.StatusCode != http.StatusOK {\n\t\t\t\tt.Errorf(\"post sql, status: %d, body=%s\", resp.StatusCode, resp.Body)\n\t\t\t}\n\n\t\t\tout := make(map[string]interface{})\n\t\t\tassert.NoError(t, json.Unmarshal([]byte(resp.Body), &out))\n\n\t\t\tkeys := make([]string, 0, len(out))\n\t\t\tfor k := range out {\n\t\t\t\tkeys = append(keys, k)\n\t\t\t}\n\n\t\t\tassert.ElementsMatch(t, tt.expKeys, keys)\n\t\t})\n\t}\n}\n\nfunc TestTranslationHandlers(t *testing.T) {\n\t// reusable data for the tests\n\tnameBytes, err := json.Marshal([]string{\"a\", \"b\", \"c\"})\n\tif err != nil {\n\t\tt.Fatalf(\"marshalling json: %v\", err)\n\t}\n\tnames := string(nameBytes)\n\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\tm := c.GetPrimary()\n\t_, err = m.API.CreateIndex(context.Background(), c.Idx(\"s\"), pilosa.IndexOptions{Keys: true})\n\tif err != nil {\n\t\tt.Fatalf(\"creating index: %v\", err)\n\t}\n\t_, err = m.API.CreateField(context.Background(), c.Idx(\"s\"), \"stringset\", pilosa.OptFieldTypeSet(\"ranked\", 100000), pilosa.OptFieldKeys())\n\tif err != nil {\n\t\tt.Fatalf(\"creating field: %v\", err)\n\t}\n\n\tbaseURLs := []string{\n\t\tfmt.Sprintf(\"%s/internal/translate/index/%s/\", m.URL(), c),\n\t\tfmt.Sprintf(\"%s/internal/translate/field/%s/stringset/\", m.URL(), c),\n\t\tfmt.Sprintf(\"%s/internal/translate/field/%s/nonexistent/\", m.URL(), c),\n\t}\n\tfor _, url := range baseURLs {\n\t\texpectFailure := strings.HasSuffix(url, \"/nonexistent/\")\n\t\tcreateURL := url + \"keys/create\"\n\t\tfindURL := url + \"keys/find\"\n\t\tvar results map[string]uint64\n\n\t\tif expectFailure {\n\t\t\tresp := test.Do(t, \"POST\", findURL, names)\n\t\t\tif resp.StatusCode != http.StatusInternalServerError {\n\t\t\t\tt.Fatalf(\"invalid status: %d, body=%s\", resp.StatusCode, resp.Body)\n\t\t\t}\n\t\t\tresp = test.Do(t, \"POST\", createURL, names)\n\t\t\tif resp.StatusCode != http.StatusInternalServerError {\n\t\t\t\tt.Fatalf(\"invalid status: %d, body=%s\", resp.StatusCode, resp.Body)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// try to find them when they don't exist\n\t\tresp := test.Do(t, \"POST\", findURL, names)\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\tt.Fatalf(\"invalid status: %d, body=%s\", resp.StatusCode, resp.Body)\n\t\t}\n\t\terr := json.Unmarshal([]byte(resp.Body), &results)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unmarshalling result: %v\", err)\n\t\t}\n\t\tif len(results) != 0 {\n\t\t\tt.Fatalf(\"finding keys before any were set: expected no results, got %d (%q)\", len(results), results)\n\t\t}\n\n\t\t// try to create them, but malformed, so we expect an error\n\t\tresp = test.Do(t, \"POST\", createURL, names[:6])\n\t\tif resp.StatusCode != http.StatusBadRequest {\n\t\t\tt.Fatalf(\"invalid status: expected 400, got %d, body=%s\", resp.StatusCode, resp.Body)\n\t\t}\n\n\t\t// try to create them\n\t\tresp = test.Do(t, \"POST\", createURL, names)\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\tt.Fatalf(\"invalid status: %d, body=%s\", resp.StatusCode, resp.Body)\n\t\t}\n\t\terr = json.Unmarshal([]byte(resp.Body), &results)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unmarshalling result: %v\", err)\n\t\t}\n\t\tif len(results) != 3 {\n\t\t\tt.Fatalf(\"finding keys before any were set: expected 3 results, got %d (%q)\", len(results), results)\n\t\t}\n\n\t\t// try to find them now that they exist\n\t\tresp = test.Do(t, \"POST\", findURL, names)\n\t\tif resp.StatusCode != http.StatusOK {\n\t\t\tt.Fatalf(\"invalid status: %d, body=%s\", resp.StatusCode, resp.Body)\n\t\t}\n\t\terr = json.Unmarshal([]byte(resp.Body), &results)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unmarshalling result: %v\", err)\n\t\t}\n\t\tif len(results) != 3 {\n\t\t\tt.Fatalf(\"finding keys before any were set: expected 3 results, got %d (%q)\", len(results), results)\n\t\t}\n\t}\n}\n\nfunc TestAuthAllowedNetworks(t *testing.T) {\n\tpermissions1 := `\n\"user-groups\":\n  \"dca35310-ecda-4f23-86cd-876aee55906b\":\n    \"test\": \"read\"\nadmin: \"ac97c9e2-346b-42a2-b6da-18bcb61a32fe\"`\n\n\ttmpDir := t.TempDir()\n\tpermissionsPath := path.Join(tmpDir, \"test-permissions.yaml\")\n\terr := os.WriteFile(permissionsPath, []byte(permissions1), 0600)\n\tif err != nil {\n\t\tt.Fatalf(\"failed to write permissions file: %v\", err)\n\t}\n\n\tqueryLogPath := path.Join(tmpDir, \"query.log\")\n\t_, err = os.Create(queryLogPath)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvalidIP := \"10.0.0.2\"\n\n\tclusterSize := 3\n\tcommandOpts := make([][]server.CommandOption, clusterSize)\n\tconfigs := make([]*server.Config, clusterSize)\n\tfor i := range configs {\n\t\tconf := server.NewConfig()\n\t\tconfigs[i] = conf\n\t\tconf.TLS.CertificatePath = \"./testdata/certs/localhost.crt\"\n\t\tconf.TLS.CertificateKeyPath = \"./testdata/certs/localhost.key\"\n\t\tconf.Auth.Enable = true\n\t\tconf.Auth.ClientId = \"e9088663-eb08-41d7-8f65-efb5f54bbb71\"\n\t\tconf.Auth.ClientSecret = \"DEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEF\"\n\t\tconf.Auth.AuthorizeURL = \"https://login.microsoftonline.com/4a137d66-d161-4ae4-b1e6-07e9920874b8/oauth2/v2.0/authorize\"\n\t\tconf.Auth.TokenURL = \"https://login.microsoftonline.com/4a137d66-d161-4ae4-b1e6-07e9920874b8/oauth2/v2.0/authorize\"\n\t\tconf.Auth.GroupEndpointURL = \"https://graph.microsoft.com/v1.0/me/transitiveMemberOf/microsoft.graph.group?$count=true\"\n\t\tconf.Auth.LogoutURL = \"https://login.microsoftonline.com/common/oauth2/v2.0/logout\"\n\t\tconf.Auth.RedirectBaseURL = \"https://localhost:10101/\"\n\t\tconf.Auth.QueryLogPath = queryLogPath\n\t\tconf.Auth.SecretKey = \"DEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEFDEADBEEF\"\n\t\tconf.Auth.PermissionsFile = permissionsPath\n\t\tconf.Auth.Scopes = []string{\"https://graph.microsoft.com/.default\", \"offline_access\"}\n\t\tconf.Auth.ConfiguredIPs = []string{validIP}\n\t\tcommandOpts[i] = append(commandOpts[i], server.OptCommandConfig(conf))\n\t}\n\n\tc := test.MustRunCluster(t, clusterSize, commandOpts...)\n\tdefer c.Close()\n\n\tm := c.GetPrimary()\n\tindex := c.Idx(\"s\")\n\tkeyedIndex := c.Idx(\"k\")\n\tfield := \"field1\"\n\n\t// This keyed index used to be created by the Post-Schema subtest, but that doesn't\n\t// exist anymore, so we create it up here. We don't create the other one because it's\n\t// supposed to get created by Post-Index.\n\t_, err = m.API.CreateIndex(context.Background(), c.Idx(\"k\"), pilosa.IndexOptions{Keys: true})\n\tif err != nil {\n\t\tt.Fatalf(\"creating index: %v\", err)\n\t}\n\n\t// needed for key translation\n\tnameBytes, err := json.Marshal([]string{\"a\", \"b\", \"c\"})\n\tif err != nil {\n\t\tt.Fatalf(\"marshalling json: %v\", err)\n\t}\n\tnames := string(nameBytes)\n\n\tIPTests := []struct {\n\t\tTestName   string\n\t\tClientIP   string\n\t\tStatusCode int\n\t}{\n\t\t{TestName: \"ValidIP\", ClientIP: validIP, StatusCode: http.StatusOK},\n\t\t{TestName: \"InvalidIP\", ClientIP: \"10.0.1.1\", StatusCode: http.StatusForbidden},\n\t}\n\n\ttests := []struct {\n\t\ttestName string\n\t\tmethod   string\n\t\turl      string\n\t\tbody     string\n\t}{\n\t\t{\n\t\t\ttestName: \"Post-Index\",\n\t\t\tmethod:   \"POST\",\n\t\t\turl:      fmt.Sprintf(\"%s/index/%s\", m.URL(), index),\n\t\t\tbody:     \"\",\n\t\t},\n\t\t{\n\t\t\ttestName: \"Post-field\",\n\t\t\tmethod:   \"POST\",\n\t\t\turl:      fmt.Sprintf(\"%s/index/%s/field/%s\", m.URL(), index, field),\n\t\t\tbody:     \"\",\n\t\t},\n\t\t{\n\t\t\ttestName: \"Get-Schema\",\n\t\t\tmethod:   \"GET\",\n\t\t\turl:      fmt.Sprintf(\"%s/schema\", m.URL()),\n\t\t\tbody:     \"\",\n\t\t},\n\t\t{\n\t\t\ttestName: \"Get-Shards\",\n\t\t\tmethod:   \"GET\",\n\t\t\turl:      fmt.Sprintf(\"%s/internal/index/%s/shards\", m.URL(), keyedIndex),\n\t\t\tbody:     \"\",\n\t\t},\n\t\t{\n\t\t\ttestName: \"Post-CreateKeys\",\n\t\t\tmethod:   \"POST\",\n\t\t\turl:      fmt.Sprintf(\"%s/internal/translate/index/%s/keys/create\", m.URL(), keyedIndex),\n\t\t\tbody:     names,\n\t\t},\n\t\t{\n\t\t\ttestName: \"Get-MemoryUsage\",\n\t\t\tmethod:   \"GET\",\n\t\t\turl:      fmt.Sprintf(\"%s/internal/mem-usage\", m.URL()),\n\t\t\tbody:     \"\",\n\t\t},\n\t\t{\n\t\t\ttestName: \"Get-Status\",\n\t\t\tmethod:   \"GET\",\n\t\t\turl:      fmt.Sprintf(\"%s/status\", m.URL()),\n\t\t\tbody:     \"\",\n\t\t},\n\t}\n\n\tfor _, ipTest := range IPTests {\n\t\tfor _, test := range tests {\n\t\t\tt.Run(ipTest.TestName+\"-\"+test.testName, func(t *testing.T) {\n\t\t\t\tvar req *http.Request\n\t\t\t\tif test.body != \"\" {\n\t\t\t\t\treq, err = http.NewRequest(test.method, test.url, strings.NewReader(test.body))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\treq, err = http.NewRequest(test.method, test.url, nil)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\treq.Header.Set(\"Content-Type\", \"application/json\")\n\t\t\t\treq.Header.Set(\"X-Forwarded-For\", ipTest.ClientIP)\n\t\t\t\tresp, err := http.DefaultClient.Do(req)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"failed to send request: %v\", err)\n\t\t\t\t}\n\t\t\t\tif resp.StatusCode != ipTest.StatusCode {\n\t\t\t\t\tt.Fatalf(\"expected %v response code, got %v, body: %v\", ipTest.StatusCode, resp.StatusCode, resp.Body)\n\t\t\t\t}\n\n\t\t\t\tif ipTest.StatusCode == 200 {\n\t\t\t\t\tbody, err := io.ReadAll(resp.Body)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tt.Fatalf(\"reading resp body :%v\", err)\n\t\t\t\t\t}\n\n\t\t\t\t\tif test.testName == \"Post-CreateKeys\" {\n\t\t\t\t\t\tvar results map[string]uint64\n\t\t\t\t\t\terr = json.Unmarshal([]byte(body), &results)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\tt.Fatalf(\"unmarshalling result: %v\", err)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif len(results) != 3 {\n\t\t\t\t\t\t\tt.Fatalf(\"finding keys before any were set: expected 3 results, got %d (%q)\", len(results), results)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t})\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "http_translator.go",
          "type": "blob",
          "size": 3.62890625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"reflect\"\n\t\"sync\"\n\n\t\"github.com/featurebasedb/featurebase/v3/logger\"\n)\n\nfunc GetOpenTranslateReaderFunc(client *http.Client) OpenTranslateReaderFunc {\n\treturn GetOpenTranslateReaderWithLockerFunc(client, nopLocker{})\n}\n\nfunc GetOpenTranslateReaderWithLockerFunc(client *http.Client, locker sync.Locker) OpenTranslateReaderFunc {\n\tlockType := reflect.TypeOf(locker)\n\tif lockType.Kind() == reflect.Ptr {\n\t\tlockType = lockType.Elem()\n\t}\n\treturn func(ctx context.Context, nodeURL string, offsets TranslateOffsetMap) (TranslateEntryReader, error) {\n\t\treturn openTranslateReader(ctx, nodeURL, offsets, client, reflect.New(lockType).Interface().(sync.Locker))\n\t}\n}\n\nfunc openTranslateReader(ctx context.Context, nodeURL string, offsets TranslateOffsetMap, client *http.Client, locker sync.Locker) (TranslateEntryReader, error) {\n\tr := NewTranslateEntryReader(ctx, client)\n\tr.locker = locker\n\n\tr.URL = nodeURL + \"/internal/translate/data\"\n\tr.Offsets = offsets\n\tif err := r.Open(); err != nil {\n\t\treturn nil, err\n\t}\n\treturn r, nil\n}\n\ntype nopLocker struct{}\n\nfunc (nopLocker) Lock()   {}\nfunc (nopLocker) Unlock() {}\n\n// HTTPTranslateEntryReader represents an implementation of TranslateEntryReader.\n// It consolidates all index & field translate entries into a single reader.\ntype HTTPTranslateEntryReader struct {\n\tlocker sync.Locker\n\n\tctx    context.Context\n\tcancel func()\n\n\tbody io.ReadCloser\n\tdec  *json.Decoder\n\n\t// Lookup of offsets for each index & field.\n\t// Must be set before calling Open().\n\tOffsets TranslateOffsetMap\n\n\t// URL to stream entries from.\n\t// Must be set before calling Open().\n\tURL string\n\n\tHTTPClient *http.Client\n\n\tLogger logger.Logger\n}\n\n// NewTranslateEntryReader returns a new instance of TranslateEntryReader.\nfunc NewTranslateEntryReader(ctx context.Context, client *http.Client) *HTTPTranslateEntryReader {\n\tif client == nil {\n\t\tclient = http.DefaultClient\n\t}\n\tr := &HTTPTranslateEntryReader{locker: nopLocker{}, HTTPClient: client, Logger: logger.NopLogger}\n\tr.ctx, r.cancel = context.WithCancel(ctx)\n\treturn r\n}\n\n// Open initiates the reader.\nfunc (r *HTTPTranslateEntryReader) Open() error {\n\t// Serialize map of offsets to request body.\n\trequestBody, err := json.Marshal(r.Offsets)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Connect a stream to the remote server.\n\treq, err := http.NewRequest(\"POST\", r.URL, bytes.NewReader(requestBody))\n\tif err != nil {\n\t\treturn err\n\t}\n\treq = req.WithContext(r.ctx)\n\n\t// Connect a stream to the remote server.\n\tresp, err := r.HTTPClient.Do(req)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"http: cannot connect to translate store endpoint: url=%s err=%s\", r.URL, err)\n\t}\n\tr.body = resp.Body\n\tr.dec = json.NewDecoder(r.body)\n\n\t// Handle error codes.\n\tif resp.StatusCode == http.StatusNotImplemented {\n\t\tr.body.Close()\n\t\treturn ErrNotImplemented\n\t} else if resp.StatusCode != http.StatusOK {\n\t\tbody, _ := io.ReadAll(resp.Body)\n\t\tr.body.Close()\n\t\treturn fmt.Errorf(\"http: invalid translate store endpoint status: code=%d url=%s body=%q\", resp.StatusCode, r.URL, bytes.TrimSpace(body))\n\t}\n\treturn nil\n}\n\n// Close stops the reader.\nfunc (r *HTTPTranslateEntryReader) Close() error {\n\tif r.cancel != nil {\n\t\tr.cancel()\n\t}\n\tif r.body != nil {\n\t\tr.locker.Lock()\n\t\terr := r.body.Close()\n\t\tr.locker.Unlock()\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// ReadEntry reads the next entry from the stream into entry.\n// Returns io.EOF at the end of the stream.\nfunc (r *HTTPTranslateEntryReader) ReadEntry(entry *TranslateEntry) error {\n\tr.locker.Lock()\n\tdefer r.locker.Unlock()\n\n\treturn r.dec.Decode(&entry)\n}\n"
        },
        {
          "name": "http_translator_test.go",
          "type": "blob",
          "size": 4.021484375,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\tpilosa \"github.com/featurebasedb/featurebase/v3\"\n\t\"github.com/featurebasedb/featurebase/v3/test\"\n)\n\nfunc TestTranslateStore_EntryReader(t *testing.T) {\n\t// Ensure client can connect and stream the translate store data.\n\tt.Run(\"OK\", func(t *testing.T) {\n\t\tt.Run(\"ServerDisconnect\", func(t *testing.T) {\n\t\t\t// This test is currently flawed\n\t\t\tt.Skip(\"failing with error: invalid memory address or nil pointer dereference when reading entry\")\n\n\t\t\tcluster := test.MustRunCluster(t, 1)\n\t\t\tdefer cluster.Close()\n\t\t\tprimary := cluster.GetNode(0)\n\n\t\t\thldr := test.Holder{Holder: primary.Server.Holder()}\n\t\t\tindex := hldr.MustCreateIndexIfNotExists(\"i\", pilosa.IndexOptions{Keys: true})\n\t\t\t_, err := index.CreateField(\"f\", \"\")\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\t// Set data on the primary node.\n\t\t\tif _, err := primary.API.Query(context.Background(), &pilosa.QueryRequest{Index: \"i\", Query: `` +\n\t\t\t\tfmt.Sprintf(\"Set(%s, f=%d)\\n\", `\"foo\"`, 10) +\n\t\t\t\tfmt.Sprintf(\"Set(%s, f=%d)\\n\", `\"bar\"`, 10) +\n\t\t\t\tfmt.Sprintf(\"Set(%s, f=%d)\\n\", `\"baz\"`, 10),\n\t\t\t}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\t// Connect to server and stream all available data.\n\t\t\tr := pilosa.NewTranslateEntryReader(context.Background(), nil)\n\t\t\tr.URL = primary.URL()\n\n\t\t\t// Wait to ensure writes make it to translate store\n\t\t\ttime.Sleep(500 * time.Millisecond)\n\n\t\t\t// Close the primary to disconnect reader.\n\t\t\tprimary.Close()\n\n\t\t\tvar entry pilosa.TranslateEntry\n\t\t\tif err := r.ReadEntry(&entry); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if got, want := entry.ID, uint64(1); got != want {\n\t\t\t\tt.Fatalf(\"entry.ID=%v, want %v\", got, want)\n\t\t\t} else if got, want := entry.Key, \"-\"; got != want {\n\t\t\t\tt.Fatalf(\"entry.Key=%v, want %v\", got, want)\n\t\t\t}\n\n\t\t\tif err := r.Close(); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t})\n\t})\n}\n\nfunc benchmarkSetup(b *testing.B, ctx context.Context, key string, nkeys int) (string, pilosa.TranslateOffsetMap, func()) {\n\tb.Helper()\n\n\tcluster := test.MustRunCluster(b, 1)\n\tprimary := cluster.GetNode(0)\n\n\tidx := primary.MustCreateIndex(b, \"i\", pilosa.IndexOptions{})\n\tfld := primary.MustCreateField(b, idx.Name(), \"f\", pilosa.OptFieldKeys())\n\toffset := make(pilosa.TranslateOffsetMap)\n\toffset.SetIndexPartitionOffset(idx.Name(), 0, 1)\n\toffset.SetFieldOffset(idx.Name(), fld.Name(), 1)\n\n\t// Set data on the primary node.\n\tfor k := 0; k < nkeys; k++ {\n\t\tif _, err := primary.API.Query(ctx, &pilosa.QueryRequest{\n\t\t\tIndex: idx.Name(),\n\t\t\tQuery: fmt.Sprintf(`Set(%d, %s=\"%s%[1]d\")`, k, fld.Name(), key),\n\t\t}); err != nil {\n\t\t\tb.Fatalf(\"quering api: %+v\", err)\n\t\t}\n\t}\n\n\treturn primary.URL(), offset, func() {\n\t\tb.Helper()\n\n\t\tif err := primary.API.DeleteIndex(ctx, idx.Name()); err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tif err := cluster.Close(); err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n}\n\nfunc benchmarkReadEntry(b *testing.B, r pilosa.TranslateEntryReader, key string, nkeys int) {\n\tvar entry pilosa.TranslateEntry\n\tfor k := 0; k < nkeys; k++ {\n\t\tif err := r.ReadEntry(&entry); err != nil {\n\t\t\tb.Fatalf(\"reading entry: %+v\", err)\n\t\t}\n\t\tif entry.Key != fmt.Sprintf(\"%s%d\", key, k) {\n\t\t\tb.Fatalf(\"got: %s, expected: %s%d\", entry.Key, key, k)\n\t\t}\n\t}\n}\n\nconst (\n\tkey   = \"foo\"\n\tnkeys = 1000\n)\n\nfunc BenchmarkReadEntryNoMutex(b *testing.B) {\n\tctx := context.Background()\n\turl, offset, teardown := benchmarkSetup(b, ctx, key, nkeys)\n\tdefer teardown()\n\n\tfor n := 0; n < b.N; n++ {\n\t\tr, err := pilosa.GetOpenTranslateReaderFunc(nil)(ctx, url, offset)\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"opening translate reader: %+v\", err)\n\t\t}\n\t\tbenchmarkReadEntry(b, r, key, nkeys)\n\t\tr.Close()\n\t}\n}\n\nfunc BenchmarkReadEntryWithMutex(b *testing.B) {\n\tctx := context.Background()\n\turl, offset, teardown := benchmarkSetup(b, ctx, key, nkeys)\n\tdefer teardown()\n\n\tfor n := 0; n < b.N; n++ {\n\t\tr, err := pilosa.GetOpenTranslateReaderWithLockerFunc(nil, &sync.Mutex{})(ctx, url, offset)\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"opening translate reader: %+v\", err)\n\t\t}\n\t\tbenchmarkReadEntry(b, r, key, nkeys)\n\t\tr.Close()\n\t}\n}\n"
        },
        {
          "name": "idalloc.go",
          "type": "blob",
          "size": 13.8330078125,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\t\"math/bits\"\n\t\"os\"\n\t\"sort\"\n\t\"time\"\n\n\t\"github.com/pkg/errors\"\n\tbolt \"go.etcd.io/bbolt\"\n)\n\n// IDAllocKey is an ID allocation key.\ntype IDAllocKey struct {\n\tIndex string `json:\"index\"`\n\tKey   string `json:\"key,omitempty\"`\n}\n\ntype IDAllocReserveRequest struct {\n\tKey     IDAllocKey `json:\"key\"`\n\tSession [32]byte   `json:\"session\"`\n\tOffset  uint64     `json:\"offset\"`\n\tCount   uint64     `json:\"count\"`\n}\n\ntype IDAllocCommitRequest struct {\n\tKey     IDAllocKey `json:\"key\"`\n\tSession [32]byte   `json:\"session\"`\n\tCount   uint64     `json:\"count\"`\n}\n\nfunc (k IDAllocKey) String() string {\n\t// This is a pretty printing format.\n\t// **DO NOT** attempt to parse this.\n\treturn k.Index + \":\" + k.Key\n}\n\ntype idAllocator struct {\n\tdb           *bolt.DB\n\tfsyncEnabled bool\n}\n\ntype OpenIDAllocatorFunc func(path string, enableFsync bool) (*idAllocator, error) // whyyyyyyyyy\n\nfunc OpenIDAllocator(path string, enableFsync bool) (*idAllocator, error) {\n\tdb, err := bolt.Open(path, 0600, &bolt.Options{Timeout: 1 * time.Second, NoSync: !enableFsync})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &idAllocator{db: db, fsyncEnabled: enableFsync}, nil\n}\n\nfunc (ida *idAllocator) Replace(reader io.Reader) error {\n\tnewFile := ida.db.Path() + \".bak\"\n\tliveFile := ida.db.Path()\n\tfile, err := os.OpenFile(newFile, os.O_RDWR|os.O_CREATE, 0600)\n\tif err != nil {\n\t\treturn nil\n\t}\n\t_, err = io.Copy(file, reader)\n\tif err != nil {\n\t\treturn err\n\t}\n\tfile.Close()\n\terr = ida.db.Close()\n\tif err != nil {\n\t\treturn err\n\t}\n\terr = os.Rename(liveFile, liveFile+\".sav\")\n\tif err != nil {\n\t\treturn err\n\t}\n\terr = os.Rename(newFile, liveFile)\n\tif err != nil {\n\t\terr = os.Rename(liveFile+\".sav\", liveFile)\n\t\treturn err\n\t} else {\n\t\t_ = os.Remove(liveFile + \".sav\")\n\t}\n\tdb, err := bolt.Open(liveFile, 0600, &bolt.Options{Timeout: 1 * time.Second, NoSync: !ida.fsyncEnabled})\n\tida.db = db\n\treturn err\n}\n\nfunc (ida *idAllocator) Close() error {\n\tif ida == nil || ida.db == nil {\n\t\treturn nil\n\t}\n\tdefer func() { ida.db = nil }()\n\treturn ida.db.Close()\n}\n\nfunc (ida *idAllocator) WriteTo(w io.Writer) (int64, error) {\n\tif ida == nil || ida.db == nil {\n\t\treturn 0, fmt.Errorf(\"idAllocator closed\")\n\t}\n\n\ttx, err := ida.db.Begin(false)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tdefer func() { _ = tx.Rollback() }()\n\n\treturn tx.WriteTo(w)\n}\n\n// IDOffsetDesyncError is an error generated when attempting to reserve IDs at a committed offset.\n// This will typically happen when kafka partitions are moved between kafka ingesters - there may be a brief period in which 2 ingesters are processing the same messages at the same time.\n// The ingester can resolve this by ignoring messages under base.\ntype IDOffsetDesyncError struct {\n\t// Requested is the offset that the client attempted to reserve.\n\tRequested uint64 `json:\"requested\"`\n\n\t// Base is the next uncommitted offset for which IDs may be reserved.\n\tBase uint64 `json:\"base\"`\n}\n\nfunc (err IDOffsetDesyncError) Error() string {\n\treturn fmt.Sprintf(\"attempted to reserve IDs at committed offset %d (base offset: %d)\", err.Requested, err.Base)\n}\n\nfunc (ida *idAllocator) reserve(key IDAllocKey, session [32]byte, offset, count uint64) ([]IDRange, error) {\n\tif session == [32]byte{} {\n\t\treturn nil, errors.New(\"detected a broken session key\")\n\t}\n\n\tvar ranges []IDRange\n\terr := ida.db.Update(func(tx *bolt.Tx) error {\n\t\t// Find the bucket associated with the key.\n\t\tbkt, err := key.findBucket(tx)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Fetch the old reservation.\n\t\tvar res idReservation\n\t\terr = res.decode(bkt.Get([]byte(key.Key + \"\\x00\")))\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"decoding old reservation\")\n\t\t}\n\t\tres.lock = session\n\t\tif offset != ^uint64(0) && offset != res.offset {\n\t\t\t// Offset control is in use and the offset differs.\n\t\t\tif offset < res.offset {\n\t\t\t\t// This probbably means that 2 clients are running at the same time.\n\t\t\t\t// This is fine - just tell the client that is behind what had been dealt with.\n\t\t\t\treturn IDOffsetDesyncError{\n\t\t\t\t\tRequested: offset,\n\t\t\t\t\tBase:      res.offset,\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Roll the IDs forward.\n\t\t\tdiff := offset - res.offset\n\t\t\tfor diff > 0 && len(res.ranges) > 0 {\n\t\t\t\tif res.ranges[0].width() > diff-1 {\n\t\t\t\t\tres.ranges[0].First += diff\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\tdiff -= res.ranges[0].width() + 1\n\t\t\t\tres.ranges = res.ranges[1:]\n\t\t\t}\n\t\t}\n\t\tres.offset = offset\n\n\t\t// Count already-reserved ranges.\n\t\tvar preReserved uint64\n\t\tfor _, r := range res.ranges {\n\t\t\tsum, overflow := bits.Add64(preReserved, r.width(), 1)\n\t\t\tif overflow != 0 {\n\t\t\t\treturn errors.New(\"impossibly large reservation\")\n\t\t\t}\n\t\t\tpreReserved = sum\n\t\t}\n\n\t\tswitch {\n\t\tcase preReserved == count:\n\t\t\t// The client probbably failed while ingesting this.\n\t\t\t// Spit out the same ID ranges a second time.\n\t\t\tranges = res.ranges\n\n\t\tcase preReserved > count:\n\t\t\t// This could happen????\n\t\t\tranges = make([]IDRange, 0, len(res.ranges))\n\t\t\tfor _, r := range res.ranges {\n\t\t\t\tif count == 0 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\tif count-1 < r.width() {\n\t\t\t\t\tranges = append(ranges, IDRange{\n\t\t\t\t\t\tFirst: r.First,\n\t\t\t\t\t\tLast:  r.First + count - 1,\n\t\t\t\t\t})\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\tranges = append(ranges, r)\n\t\t\t\tcount -= r.width() + 1\n\t\t\t}\n\n\t\tdefault:\n\t\t\t// Reserve additional IDs.\n\t\t\treserved, err := doReserveIDs(bkt, count-preReserved)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\treserved = append(res.ranges, reserved...)\n\t\t\tres.ranges = reserved\n\t\t\tranges = reserved\n\t\t}\n\n\t\t// Save the updated reservation.\n\t\tencoded, err := res.encode()\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"encoding updated ID reservation entry\")\n\t\t}\n\t\terr = bkt.Put([]byte(key.Key+\"\\x00\"), encoded)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"saving updated ID reservation entry\")\n\t\t}\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"acquiring IDs from key %s in session %v\", key, session)\n\t}\n\treturn ranges, nil\n}\n\nfunc (ida *idAllocator) commit(key IDAllocKey, session [32]byte, count uint64) error {\n\tif session == [32]byte{} {\n\t\treturn errors.New(\"detected a broken session key\")\n\t}\n\n\terr := ida.db.Update(func(tx *bolt.Tx) error {\n\t\t// Find the bucket associated with the key.\n\t\tbkt, err := key.findBucket(tx)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Fetch the old reservation.\n\t\tprev := bkt.Get([]byte(key.Key + \"\\x00\"))\n\t\tif prev == nil {\n\t\t\t// There is nothing to commit.\n\t\t\treturn errors.New(\"nothing to commit\")\n\t\t}\n\t\tvar res idReservation\n\t\terr = res.decode(prev)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"decoding old reservation\")\n\t\t}\n\t\tif res.lock != session {\n\t\t\t// This probbably means that there are multiple clients attempting to manipulate the same ID allocator key.\n\t\t\treturn errors.New(\"lock collision\")\n\t\t}\n\n\t\tif res.offset != ^uint64(0) {\n\t\t\t// Update the offset.\n\t\t\tif count > ^uint64(1)-res.offset {\n\t\t\t\treturn errors.New(\"offset overflow\")\n\t\t\t}\n\t\t\tres.offset += count\n\t\t}\n\n\t\t// Discard processed ID ranges.\n\t\tn := count\n\t\tfor n > 0 && len(res.ranges) > 0 {\n\t\t\tr := &res.ranges[0]\n\t\t\tw := r.width()\n\t\t\tif n-1 < w {\n\t\t\t\tr.First += n\n\t\t\t\tn = 0\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tn -= w + 1\n\t\t\tres.ranges = res.ranges[1:]\n\t\t}\n\t\tif n > 0 {\n\t\t\t// The client attempted to commit more IDs than they requested.\n\t\t\treturn errors.New(\"overcommitted processed IDs\")\n\t\t}\n\n\t\tif res.offset == ^uint64(0) && len(res.ranges) > 0 {\n\t\t\t// Return unused ranges to the central list.\n\t\t\terr = doReleaseIDs(bkt, res.ranges...)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Wrap(err, \"releasing unused IDs\")\n\t\t\t}\n\t\t\tres.ranges = nil\n\t\t}\n\n\t\tif res.offset == ^uint64(0) && len(res.ranges) == 0 {\n\t\t\t// Delete the reservation entry, as it no longer contains any useful information.\n\t\t\terr = bkt.Delete([]byte(key.Key + \"\\x00\"))\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Wrap(err, \"deleting used ID reservation metadata\")\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\n\t\t// Save the updated reservation.\n\t\tencoded, err := res.encode()\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"encoding updated ID reservation entry\")\n\t\t}\n\t\terr = bkt.Put([]byte(key.Key+\"\\x00\"), encoded)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"saving updated ID reservation entry\")\n\t\t}\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"committing %d IDs in allocation session %x with key %s\", count, session, key)\n\t}\n\treturn nil\n}\n\nfunc (ida *idAllocator) reset(index string) error {\n\terr := ida.db.Update(func(tx *bolt.Tx) error {\n\t\tif tx.Bucket([]byte(index)) == nil {\n\t\t\treturn nil\n\t\t}\n\n\t\terr := tx.DeleteBucket([]byte(index))\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"deleting bucket\")\n\t\t}\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"resetting ID allocation for index %q\", index)\n\t}\n\treturn nil\n}\n\nfunc (k IDAllocKey) findBucket(tx *bolt.Tx) (*bolt.Bucket, error) {\n\tibkt, err := tx.CreateBucketIfNotExists([]byte(k.Index))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating index bucket\")\n\t}\n\treturn ibkt, nil\n}\n\n// IDRange is a reserved ID range.\ntype IDRange struct {\n\tFirst uint64 `json:\"first\"`\n\tLast  uint64 `json:\"last\"`\n}\n\nfunc (r *IDRange) decode(data []byte) error {\n\tif len(data) != 16 {\n\t\treturn errors.New(\"wrong size for ID range\")\n\t}\n\tfirst := binary.LittleEndian.Uint64(data[:8])\n\tlast := binary.LittleEndian.Uint64(data[8:16])\n\tif last < first {\n\t\treturn errors.New(\"backwards ID range\")\n\t}\n\t*r = IDRange{First: first, Last: last}\n\treturn nil\n}\n\n// width of the range (count-1).\nfunc (r IDRange) width() uint64 {\n\treturn r.Last - r.First\n}\n\nfunc decodeRanges(data []byte) ([]IDRange, error) {\n\tif len(data)%16 != 0 {\n\t\treturn nil, errors.New(\"incorrectly sized ranges list\")\n\t}\n\n\tranges := make([]IDRange, len(data)/16)\n\tfor i := range ranges {\n\t\terr := ranges[i].decode(data[16*i:][:16])\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrapf(err, \"decoding range %d\", i)\n\t\t}\n\t}\n\n\treturn ranges, nil\n}\n\nfunc encodeRanges(ranges []IDRange) ([]byte, error) {\n\tdata := make([]byte, 16*len(ranges))\n\tfor i, r := range ranges {\n\t\tif r.Last < r.First {\n\t\t\treturn nil, errors.Errorf(\"backwards ID range at index %d\", i)\n\t\t}\n\t\trdat := data[16*i:][:16]\n\t\tbinary.LittleEndian.PutUint64(rdat[0:8], r.First)\n\t\tbinary.LittleEndian.PutUint64(rdat[8:16], r.Last)\n\t}\n\treturn data, nil\n}\n\ntype idReservation struct {\n\tranges []IDRange\n\toffset uint64\n\tlock   [32]byte\n}\n\nfunc (r *idReservation) decode(data []byte) error {\n\tif data == nil {\n\t\t*r = idReservation{}\n\t\treturn nil\n\t}\n\n\tif len(data) < 8+len(r.lock) {\n\t\treturn errors.New(\"invalid ID reservation entry\")\n\t}\n\n\toffset := binary.LittleEndian.Uint64(data[0:8])\n\tlock := data[8 : 8+len(r.lock)]\n\n\tranges, err := decodeRanges(data[8+len(r.lock):])\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"decoding ranges in reservation entry\")\n\t}\n\n\tr.ranges = ranges\n\tr.offset = offset\n\tcopy(r.lock[:], lock)\n\n\treturn nil\n}\n\nfunc (r idReservation) encode() ([]byte, error) {\n\tvar odat [8]byte\n\tbinary.LittleEndian.PutUint64(odat[:], r.offset)\n\trdat, err := encodeRanges(r.ranges)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn append(append(odat[:], r.lock[:]...), rdat...), nil\n}\n\nfunc doReserveIDs(bkt *bolt.Bucket, count uint64) ([]IDRange, error) {\n\tvar avail []IDRange\n\tif prev := bkt.Get([]byte{1}); prev != nil {\n\t\tr, err := decodeRanges(prev)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tavail = r\n\t} else {\n\t\tavail = []IDRange{{1, ^uint64(0)}}\n\t}\n\treserved, avail, err := reserveIDs(avail, count)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"reserving IDs\")\n\t}\n\tencoded, err := encodeRanges(avail)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"encoding available ID ranges\")\n\t}\n\terr = bkt.Put([]byte{1}, encoded)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"saving available ID ranges\")\n\t}\n\treturn reserved, nil\n}\n\nfunc reserveIDs(avail []IDRange, count uint64) (reserved []IDRange, unused []IDRange, err error) {\n\t// Min-heapify the available ranges by size.\n\tfor i, v := range avail {\n\t\twidth := v.width()\n\t\tfor width < avail[(i-1)/2].width() {\n\t\t\tavail[(i-1)/2], avail[i] = v, avail[(i-1)/2]\n\t\t\ti = (i - 1) / 2\n\t\t}\n\t}\n\n\t// Reserve ranges in ascending size order.\n\tfor count > 0 && len(avail) > 0 {\n\t\tif count-1 < avail[0].width() {\n\t\t\t// The smallest available range is larger than what is needed.\n\t\t\treserved = append(reserved, IDRange{\n\t\t\t\tFirst: avail[0].First,\n\t\t\t\tLast:  avail[0].First + count - 1,\n\t\t\t})\n\t\t\tavail[0].First += count\n\t\t\tcount = 0\n\t\t\tbreak\n\t\t}\n\n\t\t// Completely reserve the smallest range.\n\t\tcount -= avail[0].width() + 1\n\t\treserved = append(reserved, avail[0])\n\t\tavail[0] = avail[len(avail)-1]\n\t\tavail = avail[:len(avail)-1]\n\t\ti := 0\n\t\tfor {\n\t\t\tmin := i\n\t\t\tif l := 2*i + 1; l < len(avail) && avail[l].width() < avail[min].width() {\n\t\t\t\tmin = l\n\t\t\t}\n\t\t\tif r := 2*i + 2; r < len(avail) && avail[r].width() < avail[min].width() {\n\t\t\t\tmin = r\n\t\t\t}\n\t\t\tif min == i {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tavail[i], avail[min] = avail[min], avail[i]\n\t\t\ti = min\n\t\t}\n\t}\n\tif count > 0 {\n\t\treturn nil, nil, errors.Errorf(\"insufficient available ID space (missing %d IDs)\", count)\n\t}\n\n\treturn reserved, avail, nil\n}\n\nfunc doReleaseIDs(bkt *bolt.Bucket, ranges ...IDRange) error {\n\tvar avail []IDRange\n\tif prev := bkt.Get([]byte{1}); prev != nil {\n\t\tr, err := decodeRanges(prev)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tavail = r\n\t} else {\n\t\treturn errors.New(\"cannot return IDs to the void\")\n\t}\n\tavail, err := mergeIDs(avail, ranges)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"returning IDs\")\n\t}\n\tencoded, err := encodeRanges(avail)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"encoding available ID ranges\")\n\t}\n\terr = bkt.Put([]byte{1}, encoded)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"saving available ID ranges\")\n\t}\n\treturn nil\n}\n\nfunc mergeIDs(main, returned []IDRange) ([]IDRange, error) {\n\tif len(main) == 0 && len(returned) == 0 {\n\t\treturn nil, nil\n\t}\n\n\tmain = append(main, returned...)\n\n\tsort.Slice(main, func(i, j int) bool { return main[i].First < main[j].First })\n\n\ti := 1\n\tfor _, v := range main[1:] {\n\t\tprev := &main[i-1]\n\t\tswitch {\n\t\tcase v.First == prev.First || v.First <= prev.Last:\n\t\t\treturn nil, errors.Errorf(\"found unexpected overlap between ranges %v and %v\", prev, v)\n\t\tcase v.First == prev.Last+1:\n\t\t\tprev.Last = v.Last\n\t\tdefault:\n\t\t\tmain[i] = v\n\t\t\ti++\n\t\t}\n\t}\n\n\treturn main[:i], nil\n}\n"
        },
        {
          "name": "idalloc_test.go",
          "type": "blob",
          "size": 5.9951171875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"crypto/rand\"\n\t\"io\"\n\t\"reflect\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/testhook\"\n\tbolt \"go.etcd.io/bbolt\"\n)\n\nfunc TestIDAlloc(t *testing.T) {\n\t// Acquire a temporary file.\n\tf, err := testhook.TempFile(t, \"idalloc\")\n\tif err != nil {\n\t\tt.Errorf(\"acquiring temporary file: %v\", err)\n\t\treturn\n\t}\n\tdefer func() {\n\t\tcerr := f.Close()\n\t\tif cerr != nil {\n\t\t\tt.Errorf(\"closing temporary file: %v\", cerr)\n\t\t}\n\t}()\n\n\t// Open bolt.\n\tdb, err := bolt.Open(f.Name(), 0600, &bolt.Options{Timeout: 1 * time.Second, NoSync: true})\n\tif err != nil {\n\t\tt.Errorf(\"opening bolt: %v\", err)\n\t\treturn\n\t}\n\tdefer func() {\n\t\tcerr := db.Close()\n\t\tif cerr != nil {\n\t\t\tt.Errorf(\"closing bolt: %v\", cerr)\n\t\t}\n\t}()\n\n\talloc := idAllocator{db: db}\n\n\ta := IDAllocKey{\n\t\tIndex: \"h\",\n\t\tKey:   \"a\",\n\t}\n\tb := IDAllocKey{\n\t\tIndex: \"h\",\n\t\tKey:   \"b\",\n\t}\n\tvar sessions [11][32]byte\n\tfor i := range sessions {\n\t\t_, err = io.ReadFull(rand.Reader, sessions[i][:])\n\t\tif err != nil {\n\t\t\tt.Errorf(\"failed to obtain entropy: %v\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Reserve 2 batches of IDs and then commit both.\n\tif ranges, err := alloc.reserve(a, sessions[0], ^uint64(0), 3); err != nil {\n\t\tt.Errorf(\"failed to reserve 3 IDs: %v\", err)\n\t} else {\n\t\texpect := []IDRange{{1, 3}}\n\t\tif !reflect.DeepEqual(expect, ranges) {\n\t\t\tt.Errorf(\"expeced %v but got %v\", expect, ranges)\n\t\t}\n\t}\n\tif ranges, err := alloc.reserve(b, sessions[1], ^uint64(0), 3); err != nil {\n\t\tt.Errorf(\"failed to reserve 3 IDs: %v\", err)\n\t} else {\n\t\texpect := []IDRange{{4, 6}}\n\t\tif !reflect.DeepEqual(expect, ranges) {\n\t\t\tt.Errorf(\"expeced %v but got %v\", expect, ranges)\n\t\t}\n\t}\n\tif err := alloc.commit(a, sessions[0], 3); err != nil {\n\t\tt.Errorf(\"failed to commit reservation of 3 IDs: %v\", err)\n\t}\n\tif err := alloc.commit(b, sessions[1], 3); err != nil {\n\t\tt.Errorf(\"failed to commit reservation of 3 IDs: %v\", err)\n\t}\n\n\t// Test collision handling.\n\tif ranges, err := alloc.reserve(a, sessions[2], ^uint64(0), 4); err != nil {\n\t\tt.Errorf(\"failed to reserve 4 IDs: %v\", err)\n\t} else {\n\t\texpect := []IDRange{{7, 10}}\n\t\tif !reflect.DeepEqual(expect, ranges) {\n\t\t\tt.Errorf(\"expeced %v but got %v\", expect, ranges)\n\t\t}\n\t}\n\tif ranges, err := alloc.reserve(a, sessions[3], ^uint64(0), 4); err != nil {\n\t\tt.Errorf(\"failed to reserve 4 IDs: %v\", err)\n\t} else {\n\t\t// Instead of reserving new ranges, we have taken over the previous reservation.\n\t\texpect := []IDRange{{7, 10}}\n\t\tif !reflect.DeepEqual(expect, ranges) {\n\t\t\tt.Errorf(\"expeced %v but got %v\", expect, ranges)\n\t\t}\n\t}\n\tif err := alloc.commit(a, sessions[2], 4); err == nil {\n\t\t// This was taken over by reservation 3, so this should have warned us that something went wrong.\n\t\tt.Error(\"unexpected commit success for clobbered reservation\")\n\t}\n\tif err := alloc.commit(a, sessions[3], 4); err != nil {\n\t\tt.Errorf(\"failed to commit reservation of 4 IDs: %v\", err)\n\t}\n\n\t// Test key return.\n\tif ranges, err := alloc.reserve(a, sessions[4], ^uint64(0), 100); err != nil {\n\t\tt.Errorf(\"failed to reserve 100 IDs: %v\", err)\n\t} else {\n\t\t// Instead of reserving new ranges, we have taken over the previous reservation.\n\t\texpect := []IDRange{{11, 110}}\n\t\tif !reflect.DeepEqual(expect, ranges) {\n\t\t\tt.Errorf(\"expeced %v but got %v\", expect, ranges)\n\t\t}\n\t}\n\tif err := alloc.commit(a, sessions[4], 1); err != nil {\n\t\tt.Errorf(\"failed to commit reservation of 1 ID: %v\", err)\n\t}\n\tif ranges, err := alloc.reserve(a, sessions[5], ^uint64(0), 100); err != nil {\n\t\tt.Errorf(\"failed to reserve 100 IDs: %v\", err)\n\t} else {\n\t\t// Instead of reserving new ranges, we have taken over the previous reservation.\n\t\texpect := []IDRange{{12, 111}}\n\t\tif !reflect.DeepEqual(expect, ranges) {\n\t\t\tt.Errorf(\"expeced %v but got %v\", expect, ranges)\n\t\t}\n\t}\n\tif err := alloc.commit(a, sessions[5], 100); err != nil {\n\t\tt.Errorf(\"failed to commit reservation of 100 IDs: %v\", err)\n\t}\n\n\t// Test offset-based allocation.\n\tp1 := IDAllocKey{\n\t\tIndex: \"xyzzy\",\n\t\tKey:   \"kafka-partition-1\",\n\t}\n\tp2 := IDAllocKey{\n\t\tIndex: \"xyzzy\",\n\t\tKey:   \"kafka-partition-2\",\n\t}\n\t// Start processsing on 2 partitions.\n\tif ranges, err := alloc.reserve(p1, sessions[6], 0, 3); err != nil {\n\t\tt.Errorf(\"failed to reserve 3 IDs: %v\", err)\n\t} else {\n\t\texpect := []IDRange{{1, 3}}\n\t\tif !reflect.DeepEqual(expect, ranges) {\n\t\t\tt.Errorf(\"expeced %v but got %v\", expect, ranges)\n\t\t}\n\t}\n\tif ranges, err := alloc.reserve(p2, sessions[7], 0, 3); err != nil {\n\t\tt.Errorf(\"failed to reserve 3 IDs: %v\", err)\n\t} else {\n\t\texpect := []IDRange{{4, 6}}\n\t\tif !reflect.DeepEqual(expect, ranges) {\n\t\t\tt.Errorf(\"expeced %v but got %v\", expect, ranges)\n\t\t}\n\t}\n\t// Partition 2 finishes a batch of 2 IDs.\n\tif err := alloc.commit(p2, sessions[7], 2); err != nil {\n\t\tt.Errorf(\"failed to commit reservation of 2 IDs: %v\", err)\n\t}\n\t// Partition 2 reserves more IDs.\n\tif ranges, err := alloc.reserve(p2, sessions[8], 2, 3); err != nil {\n\t\tt.Errorf(\"failed to reserve 3 IDs: %v\", err)\n\t} else {\n\t\t// This could be coalesced here, but that is not true in the general case.\n\t\texpect := []IDRange{{6, 6}, {7, 8}}\n\t\tif !reflect.DeepEqual(expect, ranges) {\n\t\t\tt.Errorf(\"expeced %v but got %v\", expect, ranges)\n\t\t}\n\t}\n\t// Partition 1 ingester failed and was restarted.\n\tif ranges, err := alloc.reserve(p1, sessions[9], 0, 3); err != nil {\n\t\tt.Errorf(\"failed to reserve 3 IDs: %v\", err)\n\t} else {\n\t\texpect := []IDRange{{1, 3}}\n\t\tif !reflect.DeepEqual(expect, ranges) {\n\t\t\tt.Errorf(\"expeced %v but got %v\", expect, ranges)\n\t\t}\n\t}\n\t// This time, the partition 1 ingester finishes the batch.\n\t// It commits the offset to Kafka but fails to commit to the ID allocator.\n\t// It is restarted at Kafka's next offset.\n\tif ranges, err := alloc.reserve(p1, sessions[10], 3, 4); err != nil {\n\t\tt.Errorf(\"failed to reserve 3 IDs: %v\", err)\n\t} else {\n\t\texpect := []IDRange{{9, 12}}\n\t\tif !reflect.DeepEqual(expect, ranges) {\n\t\t\tt.Errorf(\"expeced %v but got %v\", expect, ranges)\n\t\t}\n\t}\n\t// Partition 1 finishes a batch of 4 IDs.\n\tif err := alloc.commit(p1, sessions[10], 4); err != nil {\n\t\tt.Errorf(\"failed to commit reservation of 4 IDs: %v\", err)\n\t}\n}\n"
        },
        {
          "name": "idk",
          "type": "tree",
          "content": null
        },
        {
          "name": "importer.go",
          "type": "blob",
          "size": 4.396484375,
          "content": "package pilosa\n\nimport (\n\t\"context\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/dax\"\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\t\"github.com/pkg/errors\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\ntype Importer interface {\n\tStartTransaction(ctx context.Context, id string, timeout time.Duration, exclusive bool, requestTimeout time.Duration) (*Transaction, error)\n\tFinishTransaction(ctx context.Context, id string) (*Transaction, error)\n\tCreateTableKeys(ctx context.Context, tid dax.TableID, keys ...string) (map[string]uint64, error)\n\tCreateFieldKeys(ctx context.Context, tid dax.TableID, fname dax.FieldName, keys ...string) (map[string]uint64, error)\n\tImportRoaringBitmap(ctx context.Context, tid dax.TableID, fld *dax.Field, shard uint64, views map[string]*roaring.Bitmap, clear bool) error\n\tImportRoaringShard(ctx context.Context, tid dax.TableID, shard uint64, request *ImportRoaringShardRequest) error\n\tEncodeImportValues(ctx context.Context, tid dax.TableID, fld *dax.Field, shard uint64, vals []int64, ids []uint64, clear bool) (path string, data []byte, err error)\n\tEncodeImport(ctx context.Context, tid dax.TableID, fld *dax.Field, shard uint64, vals, ids []uint64, clear bool) (path string, data []byte, err error)\n\tDoImport(ctx context.Context, tid dax.TableID, fld *dax.Field, shard uint64, path string, data []byte) error\n}\n\n// Ensure type implements interface.\nvar _ Importer = &onPremImporter{}\n\n// onPremImporter is a wrapper around API which implements the Importer\n// interface. This is currently only used by sql3 running locally in standard\n// (i.e not \"serverless\") mode. Because sql3 always sets\n// `useShardTransactionalEndpoint = true`, There are several methods which this\n// implemtation of the Importer interface does not use, and therefore they\n// intentionally no-op.\ntype onPremImporter struct {\n\tapi    *API\n\tclient *InternalClient\n}\n\nfunc NewOnPremImporter(api *API) *onPremImporter {\n\treturn &onPremImporter{\n\t\tapi:    api,\n\t\tclient: api.holder.executor.client,\n\t}\n}\n\nfunc (i *onPremImporter) StartTransaction(ctx context.Context, id string, timeout time.Duration, exclusive bool, requestTimeout time.Duration) (*Transaction, error) {\n\treturn i.api.StartTransaction(ctx, id, timeout, exclusive, false)\n}\n\nfunc (i *onPremImporter) FinishTransaction(ctx context.Context, id string) (*Transaction, error) {\n\treturn i.api.FinishTransaction(ctx, id, false)\n}\n\nfunc (i *onPremImporter) CreateTableKeys(ctx context.Context, tid dax.TableID, keys ...string) (map[string]uint64, error) {\n\treturn i.api.CreateIndexKeys(ctx, string(tid), keys...)\n}\n\nfunc (i *onPremImporter) CreateFieldKeys(ctx context.Context, tid dax.TableID, fname dax.FieldName, keys ...string) (map[string]uint64, error) {\n\treturn i.api.CreateFieldKeys(ctx, string(tid), string(fname), keys...)\n}\n\nfunc (i *onPremImporter) ImportRoaringBitmap(ctx context.Context, tid dax.TableID, fld *dax.Field, shard uint64, views map[string]*roaring.Bitmap, clear bool) error {\n\t// This intentionally no-ops. See comment on struct.\n\treturn nil\n}\n\nfunc (i *onPremImporter) ImportRoaringShard(ctx context.Context, tid dax.TableID, shard uint64, request *ImportRoaringShardRequest) error {\n\tnodes, err := i.api.ShardNodes(ctx, string(tid), shard)\n\tif err != nil {\n\t\treturn err\n\t}\n\teg := errgroup.Group{}\n\tfor _, node := range nodes {\n\t\tnode := node\n\t\tif node.ID == i.api.NodeID() { // local\n\t\t\teg.Go(func() error {\n\t\t\t\treturn i.api.ImportRoaringShard(ctx, string(tid), shard, request)\n\t\t\t})\n\t\t} else {\n\t\t\teg.Go(func() error { // forward on\n\t\t\t\treturn i.client.ImportRoaringShard(ctx, &node.URI, string(tid), shard, true, request)\n\t\t\t})\n\t\t}\n\t}\n\terr = eg.Wait()\n\treturn errors.Wrap(err, \"importing\")\n}\n\nfunc (i *onPremImporter) EncodeImportValues(ctx context.Context, tid dax.TableID, fld *dax.Field, shard uint64, vals []int64, ids []uint64, clear bool) (path string, data []byte, err error) {\n\t// This intentionally no-ops. See comment on struct.\n\treturn \"\", nil, nil\n}\n\nfunc (i *onPremImporter) EncodeImport(ctx context.Context, tid dax.TableID, fld *dax.Field, shard uint64, vals, ids []uint64, clear bool) (path string, data []byte, err error) {\n\t// This intentionally no-ops. See comment on struct.\n\treturn \"\", nil, nil\n}\n\nfunc (i *onPremImporter) DoImport(ctx context.Context, _ dax.TableID, fld *dax.Field, shard uint64, path string, data []byte) error {\n\t// This intentionally no-ops. See comment on struct.\n\treturn nil\n}\n\nfunc (i *onPremImporter) StatsTiming(name string, value time.Duration, rate float64) {}\n"
        },
        {
          "name": "index.go",
          "type": "blob",
          "size": 31.5400390625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"math/big\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strconv\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/dax\"\n\t\"github.com/featurebasedb/featurebase/v3/disco\"\n\t\"github.com/featurebasedb/featurebase/v3/pql\"\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\t\"github.com/featurebasedb/featurebase/v3/testhook\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\n// Index represents a container for fields.\ntype Index struct {\n\tmu          sync.RWMutex\n\tcreatedAt   int64\n\towner       string\n\tdescription string\n\n\tpath          string\n\tname          string\n\tqualifiedName string\n\tkeys          bool // use string keys\n\n\t// Existence tracking.\n\ttrackExistence bool\n\texistenceFld   *Field\n\n\t// Fields by name.\n\tfields map[string]*Field\n\n\tbroadcaster broadcaster\n\tserializer  Serializer\n\n\t// Passed to field for foreign-index lookup.\n\tholder *Holder\n\n\t// Per-partition translation stores\n\ttranslatePartitions dax.PartitionNums\n\ttranslateStores     map[int]TranslateStore\n\n\ttranslationSyncer TranslationSyncer\n\n\t// Instantiates new translation stores\n\tOpenTranslateStore OpenTranslateStoreFunc\n\n\t// track the subset of shards available to our views\n\tfieldView2shard *FieldView2Shards\n\n\t// indicate that we're closing and should wrap up and not allow new actions\n\tclosing chan struct{}\n}\n\n// NewIndex returns an existing (but possibly empty) instance of\n// Index at path. It will not erase any prior content.\nfunc NewIndex(holder *Holder, path, name string) (*Index, error) {\n\t// Emulate what the spf13/cobra does, letting env vars override\n\t// the defaults, because we may be under a simple \"go test\" run where\n\t// not all that command line machinery has been spun up.\n\n\terr := ValidateName(name)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating name\")\n\t}\n\n\tidx := &Index{\n\t\tpath:   path,\n\t\tname:   name,\n\t\tfields: make(map[string]*Field),\n\n\t\tbroadcaster:    NopBroadcaster,\n\t\tholder:         holder,\n\t\ttrackExistence: true,\n\n\t\tserializer: NopSerializer,\n\n\t\ttranslateStores: make(map[int]TranslateStore),\n\n\t\ttranslationSyncer: NopTranslationSyncer,\n\n\t\tOpenTranslateStore: OpenInMemTranslateStore,\n\t}\n\treturn idx, nil\n}\n\nfunc (i *Index) NewTx(txo Txo) Tx {\n\treturn i.holder.txf.NewTx(txo)\n}\n\n// CreatedAt is an timestamp for a specific version of an index.\nfunc (i *Index) CreatedAt() int64 {\n\ti.mu.RLock()\n\tdefer i.mu.RUnlock()\n\treturn i.createdAt\n}\n\n// DataframePath returns the path of the dataframes specific to an index\nfunc (i *Index) DataframesPath() string {\n\treturn filepath.Join(i.path, DataframesDir)\n}\n\n// Name returns name of the index.\nfunc (i *Index) Name() string { return i.name }\n\n// Holder yields this index's Holder.\nfunc (i *Index) Holder() *Holder { return i.holder }\n\n// QualifiedName returns the qualified name of the index.\nfunc (i *Index) QualifiedName() string { return i.qualifiedName }\n\n// Path returns the path the index was initialized with.\nfunc (i *Index) Path() string {\n\treturn i.path\n}\n\n// FieldsPath returns the path of the fields directory.\nfunc (i *Index) FieldsPath() string {\n\treturn filepath.Join(i.path, FieldsDir)\n}\n\n// TranslateStorePath returns the translation database path for a partition.\nfunc (i *Index) TranslateStorePath(partitionID int) string {\n\treturn filepath.Join(i.path, translateStoreDir, strconv.Itoa(partitionID))\n}\n\n// TranslateStore returns the translation store for a given partition.\nfunc (i *Index) TranslateStore(partitionID int) TranslateStore {\n\ti.mu.RLock() // avoid race with Index.Close() doing i.translateStores = make(map[int]TranslateStore)\n\tdefer i.mu.RUnlock()\n\treturn i.translateStores[partitionID]\n}\n\n// Keys returns true if the index uses string keys.\nfunc (i *Index) Keys() bool { return i.keys }\n\n// Options returns all options for this index.\nfunc (i *Index) Options() IndexOptions {\n\ti.mu.RLock()\n\tdefer i.mu.RUnlock()\n\treturn i.options()\n}\n\nfunc (i *Index) options() IndexOptions {\n\treturn IndexOptions{\n\t\tDescription:    i.description,\n\t\tKeys:           i.keys,\n\t\tTrackExistence: i.trackExistence,\n\t}\n}\n\n// Open opens and initializes the index.\nfunc (i *Index) Open() error {\n\treturn i.open(nil)\n}\n\n// OpenWithSchema opens the index and uses the provided schema to verify that\n// the index's fields are expected.\nfunc (i *Index) OpenWithSchema(idx *disco.Index) error {\n\tif idx == nil {\n\t\treturn ErrInvalidSchema\n\t}\n\n\t// decode the CreateIndexMessage from the schema data in order to\n\t// get its metadata.\n\tcim, err := decodeCreateIndexMessage(i.serializer, idx.Data)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"decoding create index message\")\n\t}\n\ti.createdAt = cim.CreatedAt\n\ti.trackExistence = cim.Meta.TrackExistence\n\ti.keys = cim.Meta.Keys\n\n\treturn i.open(idx)\n}\n\n// open opens the index with an optional schema (disco.Index). If a schema is\n// provided, it will apply the metadata from the schema to the index, and then\n// open all fields found in the schema. If a schema is not provided, the\n// metadata for the index is not changed from its existing value, and fields are\n// not validated against the schema as they are opened.\nfunc (i *Index) open(idx *disco.Index) (err error) {\n\ti.mu.Lock()\n\tdefer i.mu.Unlock()\n\t// Ensure the path exists.\n\ti.holder.Logger.Debugf(\"ensure index path exists: %s\", i.FieldsPath())\n\tif err := os.MkdirAll(i.FieldsPath(), 0o750); err != nil {\n\t\treturn errors.Wrap(err, \"creating directory\")\n\t}\n\n\t// Ensure the dataframes path exists\n\ti.holder.Logger.Debugf(\"ensure dataframes path exists: %s\", i.DataframesPath())\n\tif err := os.MkdirAll(i.DataframesPath(), 0o750); err != nil {\n\t\treturn errors.Wrap(err, \"creating dataframes directory\")\n\t}\n\n\ti.closing = make(chan struct{})\n\t// fmt.Printf(\"new channel %p for index %p\\n\", i.closing, i)\n\n\t// we don't want to open *all* the views for each shard, since\n\t// most are empty when we are doing time quantums. It slows\n\t// down startup dramatically. So we ask for the meta data\n\t// of what fields/views/shards are present with data up front.\n\tfieldView2shard, err := i.holder.txf.GetFieldView2ShardsMapForIndex(i)\n\tif err != nil {\n\t\treturn errors.Wrap(err, fmt.Sprintf(\"i.holder.txf.GetFieldView2ShardsMapForIndex('%v')\", i.name))\n\t}\n\ti.fieldView2shard = fieldView2shard\n\n\t// Add index to a map in holder. Used by openFields.\n\ti.holder.addIndex(i)\n\n\ti.holder.Logger.Debugf(\"open fields for index: %s\", i.name)\n\tif err := i.openFields(idx); err != nil {\n\t\treturn errors.Wrap(err, \"opening fields\")\n\t}\n\n\t// Set bit depths.\n\t// This is called in Index.open() (as opposed to Field.Open()) because the\n\t// Field.bitDepth() method uses a transaction which relies on the index and\n\t// its entry for the field in the Index.field map. If we try to set a\n\t// field's BitDepth in Field.Open(), which itself might be inside the\n\t// Index.openField() loop, then the field has not yet been added to the\n\t// Index.field map. I think it would be better if Field.bitDepth didn't rely\n\t// on its index at all, but perhaps with transactions that not possible. I\n\t// don't know.\n\tif err := i.setFieldBitDepths(); err != nil {\n\t\treturn errors.Wrap(err, \"setting field bitDepths\")\n\t}\n\n\tif i.trackExistence {\n\t\tif err := i.openExistenceField(); err != nil {\n\t\t\treturn errors.Wrap(err, \"opening existence field\")\n\t\t}\n\t}\n\n\tif i.keys {\n\t\ti.holder.Logger.Debugf(\"open translate store for index: %s\", i.name)\n\n\t\tvar g errgroup.Group\n\t\tvar mu sync.Mutex\n\t\t// TODO(tlt): this for loop doesn't work because if we assign a\n\t\t// translate partition to this node later (after the table has been\n\t\t// created with a sub-set of translatePartitions), then the new\n\t\t// TranslateStores don't get initialized. For now I just put it back so\n\t\t// it opens a TranslateStore for every partition no matter what, but we\n\t\t// really need to have the ApplyDirective logic able to initialize any\n\t\t// TranslateStore which doesn't already exist (and perhaps shut down any\n\t\t// that are to be removed).\n\t\t//\n\t\t// for _, partition := range i.translatePartitions {\n\t\t//\t  partitionID := int(partition.Num)\n\t\t//\n\t\t//\n\t\t// TODO(tlt): instead of i.holder.partitionN, we need to use\n\t\t// len(i.translatePartitions), or actually we need to know the\n\t\t// keypartitions for the qtbl (i don't think we can rely on the length\n\t\t// of this slice) but that will only apply here... we need to go through\n\t\t// all the code and see where these are being used:\n\t\t// - i.holder.partitionN\n\t\t// - DefaultPartitionN\n\t\t//\n\t\t//\n\t\tfor partitionID := 0; partitionID < i.holder.partitionN; partitionID++ {\n\t\t\tpartitionID := partitionID\n\n\t\t\tg.Go(func() error {\n\t\t\t\tstore, err := i.OpenTranslateStore(i.TranslateStorePath(partitionID), i.name, \"\", partitionID, i.holder.partitionN, i.holder.cfg.StorageConfig.FsyncEnabled)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errors.Wrapf(err, \"opening index translate store: partition=%d\", partitionID)\n\t\t\t\t}\n\n\t\t\t\tmu.Lock()\n\t\t\t\tdefer mu.Unlock()\n\t\t\t\ti.translateStores[partitionID] = store\n\t\t\t\treturn nil\n\t\t\t})\n\t\t}\n\t\tif err := g.Wait(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t_ = testhook.Opened(i.holder.Auditor, i, nil)\n\treturn nil\n}\n\nvar indexQueue = make(chan struct{}, 8)\n\n// openFields opens and initializes the fields inside the index.\nfunc (i *Index) openFields(idx *disco.Index) error {\n\teg, ctx := errgroup.WithContext(context.Background())\n\tvar mu sync.Mutex\n\n\tif idx == nil {\n\t\treturn nil\n\t}\nfileLoop:\n\tfor fname, fld := range idx.Fields {\n\t\tlfname := fname\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\tbreak fileLoop\n\t\tdefault:\n\t\t\t// Decode the CreateFieldMessage from the schema data in order to\n\t\t\t// get its metadata.\n\t\t\tcfm, err := decodeCreateFieldMessage(i.holder.serializer, fld.Data)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Wrap(err, \"decoding create field message\")\n\t\t\t}\n\n\t\t\tindexQueue <- struct{}{}\n\t\t\teg.Go(func() error {\n\t\t\t\tdefer func() {\n\t\t\t\t\t<-indexQueue\n\t\t\t\t}()\n\t\t\t\ti.holder.Logger.Debugf(\"open field: %s\", lfname)\n\n\t\t\t\t_, err := i.openField(&mu, cfm, lfname)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errors.Wrap(err, \"opening field\")\n\t\t\t\t}\n\n\t\t\t\treturn nil\n\t\t\t})\n\t\t}\n\t}\n\n\terr := eg.Wait()\n\tif err != nil {\n\t\t// Close any fields which got opened, since the overall\n\t\t// index won't be open.\n\t\tfor n, f := range i.fields {\n\t\t\tf.Close()\n\t\t\tdelete(i.fields, n)\n\t\t}\n\t}\n\treturn err\n}\n\n// openField opens the field directory, initializes the field, and adds it to\n// the in-memory map of fields maintained by Index.\nfunc (i *Index) openField(mu *sync.Mutex, cfm *CreateFieldMessage, file string) (*Field, error) {\n\tmu.Lock()\n\tfld, err := i.newField(i.fieldPath(filepath.Base(file)), filepath.Base(file))\n\tmu.Unlock()\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(ErrName, \"'%s'\", file)\n\t}\n\n\t// Pass holder through to the field for use in looking\n\t// up a foreign index.\n\tfld.holder = i.holder\n\n\tfld.createdAt = cfm.CreatedAt\n\tfld.options = applyDefaultOptions(cfm.Meta)\n\n\t// open the views we have data for.\n\tif err := fld.Open(); err != nil {\n\t\treturn nil, fmt.Errorf(\"open field: name=%s, err=%s\", fld.Name(), err)\n\t}\n\n\ti.holder.Logger.Debugf(\"add field to index.fields: %s\", file)\n\tmu.Lock()\n\ti.fields[fld.Name()] = fld\n\tmu.Unlock()\n\n\treturn fld, nil\n}\n\n// openExistenceField gets or creates the existence field and associates it to the index.\nfunc (i *Index) openExistenceField() error {\n\tcfm := &CreateFieldMessage{\n\t\tIndex:     i.name,\n\t\tField:     existenceFieldName,\n\t\tOwner:     \"\",\n\t\tCreatedAt: 0,\n\t\tMeta:      &FieldOptions{Type: FieldTypeSet, CacheType: CacheTypeNone, CacheSize: 0},\n\t}\n\n\t// First try opening the existence field from disk. If it doesn't already\n\t// exist on disk, then we fall through to the code path which creates it.\n\tvar mu sync.Mutex\n\tfld, err := i.openField(&mu, cfm, existenceFieldName)\n\tif err == nil {\n\t\ti.existenceFld = fld\n\t\treturn nil\n\t} else if errors.Cause(err) != ErrName {\n\t\treturn errors.Wrap(err, \"opening existence file\")\n\t}\n\n\t// If we have gotten here, it means that we couldn't successfully open the\n\t// existence field from disk, so we need to create it.\n\n\tf, err := i.createFieldIfNotExists(cfm)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"creating existence field\")\n\t}\n\ti.existenceFld = f\n\treturn nil\n}\n\n// setFieldBitDepths sets the BitDepth for all int and decimal fields in the index.\nfunc (i *Index) setFieldBitDepths() error {\n\tfor name, f := range i.fields {\n\t\tswitch f.Type() {\n\t\tcase FieldTypeInt, FieldTypeDecimal, FieldTypeTimestamp:\n\t\t\t// pass\n\t\tdefault:\n\t\t\tcontinue\n\t\t}\n\t\tbd, err := f.bitDepth()\n\t\tif err != nil {\n\t\t\treturn errors.Wrapf(err, \"getting bit depth for field: %s\", name)\n\t\t}\n\t\tif err := f.cacheBitDepth(bd); err != nil {\n\t\t\treturn errors.Wrapf(err, \"caching field bitDepth: %d\", bd)\n\t\t}\n\t}\n\treturn nil\n}\n\n// Close closes the index and its fields.\nfunc (i *Index) Close() error {\n\ti.mu.Lock()\n\tdefer i.mu.Unlock()\n\t// flag that we're trying to shut down\n\tif i.closing != nil {\n\t\tselect {\n\t\tcase <-i.closing:\n\t\t\t// already closed. prevent double-close\n\t\t\treturn errors.New(\"double close of index\")\n\t\tdefault:\n\t\t}\n\t\tclose(i.closing)\n\t}\n\tdefer func() {\n\t\t_ = testhook.Closed(i.holder.Auditor, i, nil)\n\t}()\n\n\terr := i.holder.txf.CloseIndex(i)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"closing index\")\n\t}\n\n\t// Close partitioned translation stores.\n\tfor _, store := range i.translateStores {\n\t\tif err := store.Close(); err != nil {\n\t\t\treturn errors.Wrap(err, \"closing translation store\")\n\t\t}\n\t}\n\ti.translateStores = make(map[int]TranslateStore)\n\n\t// Close all fields.\n\tfor _, f := range i.fields {\n\t\tif err := f.Close(); err != nil {\n\t\t\treturn errors.Wrap(err, \"closing field\")\n\t\t}\n\t}\n\ti.fields = make(map[string]*Field)\n\n\treturn nil\n}\n\nfunc (i *Index) flushCaches() {\n\t// look up the close channel so if we somehow end up living until the\n\t// index gets reopened, we don't have a data race, but correctly detect\n\t// that the old one is closed.\n\ti.mu.RLock()\n\tclosing := i.closing\n\ti.mu.RUnlock()\n\tfor _, field := range i.Fields() {\n\t\tselect {\n\t\tcase <-closing:\n\t\t\treturn\n\t\tdefault:\n\t\t\tfield.flushCaches()\n\t\t}\n\t}\n}\n\n// make it clear what the Index.AvailableShards() calls are trying to obtain.\nconst includeRemote = false\n\n// AvailableShards returns a bitmap of all shards with data in the index.\nfunc (i *Index) AvailableShards(localOnly bool) *roaring.Bitmap {\n\tif i == nil {\n\t\treturn roaring.NewBitmap()\n\t}\n\n\ti.mu.RLock()\n\tdefer i.mu.RUnlock()\n\n\tb := roaring.NewBitmap()\n\tfor _, f := range i.fields {\n\t\t// b.Union(f.AvailableShards(localOnly))\n\t\tb.UnionInPlace(f.AvailableShards(localOnly))\n\t}\n\n\tGaugeIndexMaxShard.With(prometheus.Labels{\"index\": i.name}).Set(float64(b.Max()))\n\treturn b\n}\n\n// Begin starts a transaction on a shard of the index.\nfunc (i *Index) BeginTx(writable bool, shard uint64) (Tx, error) {\n\treturn i.holder.txf.NewTx(Txo{Write: writable, Index: i, Shard: shard}), nil\n}\n\n// fieldPath returns the path to a field in the index.\nfunc (i *Index) fieldPath(name string) string { return filepath.Join(i.FieldsPath(), name) }\n\n// Field returns a field in the index by name.\nfunc (i *Index) Field(name string) *Field {\n\ti.mu.RLock()\n\tdefer i.mu.RUnlock()\n\treturn i.field(name)\n}\n\nfunc (i *Index) field(name string) *Field {\n\treturn i.fields[name]\n}\n\n// Fields returns a list of all fields in the index.\nfunc (i *Index) Fields() []*Field {\n\ti.mu.RLock()\n\tdefer i.mu.RUnlock()\n\n\ta := make([]*Field, 0, len(i.fields))\n\tfor _, f := range i.fields {\n\t\ta = append(a, f)\n\t}\n\tsort.Sort(fieldSlice(a))\n\n\treturn a\n}\n\n// existenceField returns the internal field used to track column existence.\nfunc (i *Index) existenceField() *Field {\n\ti.mu.RLock()\n\tdefer i.mu.RUnlock()\n\n\treturn i.existenceFld\n}\n\n// recalculateCaches recalculates caches on every field in the index.\nfunc (i *Index) recalculateCaches() {\n\tfor _, field := range i.Fields() {\n\t\tfield.recalculateCaches()\n\t}\n}\n\n// createNullableField is just like CreateField, except that it allows\n// the field to not have TrackExistence enabled. This should be used\n// only for existing fields which were already actually created, where\n// what we're really doing now is reifying them, so for instance, this\n// shows up in api_directive:createField to apply directives, which\n// are assumed to correctly reflect the intended behavior.\nfunc (i *Index) createNullableField(name string, requestUserID string, opts ...FieldOption) (*Field, error) {\n\terr := ValidateName(name)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating name\")\n\t}\n\n\t// Grab lock, check for field existing, release lock. We don't want\n\t// to stay holding the lock, but we might care about the ErrFieldExists\n\t// part of this.\n\terr = func() error {\n\t\ti.mu.Lock()\n\t\tdefer i.mu.Unlock()\n\n\t\t// Ensure field doesn't already exist.\n\t\tif i.fields[name] != nil {\n\t\t\treturn newConflictError(ErrFieldExists)\n\t\t}\n\t\treturn nil\n\t}()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Apply and validate functional options.\n\tfo, err := newFieldOptions(opts...)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"applying option\")\n\t}\n\treturn i.createFieldWithOptions(name, requestUserID, fo)\n}\n\n// CreateField creates a field. This interface enforces the setting\n// of the TrackExistence flag; if you don't want that, use\n// createNullableField, but actually don't. That should be used only\n// for applying previously-created fields.\nfunc (i *Index) CreateField(name string, requestUserID string, opts ...FieldOption) (*Field, error) {\n\terr := ValidateName(name)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating name\")\n\t}\n\n\t// Grab lock, check for field existing, release lock. We don't want\n\t// to stay holding the lock, but we might care about the ErrFieldExists\n\t// part of this.\n\terr = func() error {\n\t\ti.mu.Lock()\n\t\tdefer i.mu.Unlock()\n\n\t\t// Ensure field doesn't already exist.\n\t\tif i.fields[name] != nil {\n\t\t\treturn newConflictError(ErrFieldExists)\n\t\t}\n\t\treturn nil\n\t}()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Apply and validate functional options.\n\tfo, err := newFieldOptions(opts...)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"applying option\")\n\t}\n\tfo.TrackExistence = true\n\treturn i.createFieldWithOptions(name, requestUserID, fo)\n}\n\n// createFieldWithOptions creates a field given a finalized FieldOptions\n// structure, instead of functional options.\nfunc (i *Index) createFieldWithOptions(name string, requestUserID string, fo *FieldOptions) (*Field, error) {\n\tts := timestamp()\n\tcfm := &CreateFieldMessage{\n\t\tIndex:     i.name,\n\t\tField:     name,\n\t\tCreatedAt: ts,\n\t\tOwner:     requestUserID,\n\t\tMeta:      fo,\n\t}\n\n\t// Create the field in etcd as the system of record. We do this without\n\t// the lock held because it can take an arbitrary amount of time...\n\tif err := i.persistField(context.Background(), cfm); errors.Cause(err) == ErrFieldExists {\n\t\treturn nil, newConflictError(ErrFieldExists)\n\t} else if err != nil {\n\t\treturn nil, errors.Wrap(err, \"persisting field\")\n\t}\n\n\t// This is identical to the previous check, because we could get super\n\t// unlucky and have the persist-field thing happen, and somehow the field\n\t// gets created, before we get to run again, and the specific nature of\n\t// the error can matter to the backend.\n\ti.mu.Lock()\n\tdefer i.mu.Unlock()\n\n\t// Ensure field doesn't already exist.\n\tif i.fields[name] != nil {\n\t\treturn nil, newConflictError(ErrFieldExists)\n\t}\n\n\t// Actually do the internal bookkeeping.\n\treturn i.createField(cfm)\n}\n\n// CreateFieldIfNotExists creates a field with the given options if it doesn't exist.\n//\n// Does NOT apply the \"default\" TrackExistence.\nfunc (i *Index) CreateFieldIfNotExists(name string, requestUserID string, opts ...FieldOption) (*Field, error) {\n\terr := ValidateName(name)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating name\")\n\t}\n\n\ti.mu.Lock()\n\tdefer i.mu.Unlock()\n\n\t// Find field in cache first.\n\tif f := i.fields[name]; f != nil {\n\t\treturn f, nil\n\t}\n\n\t// Apply and validate functional options.\n\tfo, err := newFieldOptions(opts...)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"applying option\")\n\t}\n\n\tts := timestamp()\n\tcfm := &CreateFieldMessage{\n\t\tIndex:     i.name,\n\t\tField:     name,\n\t\tCreatedAt: ts,\n\t\tOwner:     requestUserID,\n\t\tMeta:      fo,\n\t}\n\n\t// Create the field in etcd as the system of record.\n\tif err := i.persistField(context.Background(), cfm); err != nil && errors.Cause(err) != ErrFieldExists {\n\t\t// There is a case where the index is not in memory, but it is in\n\t\t// persistent storage. In that case, this will return an \"index exists\"\n\t\t// error, which in that case should return the index. TODO: We may need\n\t\t// to allow for that in the future.\n\t\treturn nil, errors.Wrap(err, \"persisting field\")\n\t}\n\n\treturn i.createField(cfm)\n}\n\n// CreateFieldIfNotExistsWithOptions is a method which I created because I\n// needed the functionality of CreateFieldIfNotExists, but instead of taking\n// function options, taking a *FieldOptions struct. TODO: This should\n// definintely be refactored so we don't have these virtually equivalent\n// methods, but I'm puttin this here for now just to see if it works.\n//\n// Does NOT apply the \"default\" TrackExistence.\nfunc (i *Index) CreateFieldIfNotExistsWithOptions(name string, requestUserID string, opt *FieldOptions) (*Field, error) {\n\terr := ValidateName(name)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"validating name\")\n\t}\n\n\ti.mu.Lock()\n\tdefer i.mu.Unlock()\n\n\t// Find field in cache first.\n\tif f := i.fields[name]; f != nil {\n\t\treturn f, nil\n\t}\n\tif opt != nil && (opt.Type == FieldTypeInt || opt.Type == FieldTypeTimestamp) {\n\t\tmin, max := pql.MinMax(0)\n\t\t// ensure the provided bounds are valid\n\t\tzero := big.NewInt(0)\n\t\tmaxv := opt.Max.Value()\n\t\tif maxv.Cmp(zero) == 0 {\n\t\t\topt.Max = max\n\t\t} else if max.LessThan(opt.Max) {\n\t\t\topt.Max = max\n\t\t}\n\t\tminv := opt.Min.Value()\n\t\tif minv.Cmp(zero) == 0 {\n\t\t\topt.Min = min\n\t\t} else if min.GreaterThan(opt.Min) {\n\t\t\topt.Min = min\n\t\t}\n\t}\n\t// added for backward compatablity with old schemas\n\tif opt != nil && opt.Type == FieldTypeDecimal {\n\t\tmin, max := pql.MinMax(opt.Scale)\n\t\tzero := big.NewInt(0)\n\n\t\t// ensure the provided bounds are valid\n\t\tmaxv := opt.Max.Value()\n\t\tif maxv.Cmp(zero) == 0 {\n\t\t\topt.Max = max\n\t\t} else if max.LessThan(opt.Max) {\n\t\t\topt.Max = max\n\t\t}\n\n\t\tminv := opt.Min.Value()\n\t\tif minv.Cmp(zero) == 0 {\n\t\t\topt.Min = min\n\t\t} else if min.GreaterThan(opt.Min) {\n\t\t\topt.Min = min\n\t\t}\n\t}\n\n\tts := timestamp()\n\tcfm := &CreateFieldMessage{\n\t\tIndex:     i.name,\n\t\tField:     name,\n\t\tCreatedAt: ts,\n\t\tOwner:     requestUserID,\n\t\tMeta:      opt,\n\t}\n\n\t// Create the field in etcd as the system of record.\n\tif err := i.persistField(context.Background(), cfm); err != nil && errors.Cause(err) != ErrFieldExists {\n\t\t// There is a case where the index is not in memory, but it is in\n\t\t// persistent storage. In that case, this will return an \"index exists\"\n\t\t// error, which in that case should return the index. TODO: We may need\n\t\t// to allow for that in the future.\n\t\treturn nil, errors.Wrap(err, \"persisting field\")\n\t}\n\n\treturn i.createField(cfm)\n}\n\n// persistField stores the field information in etcd.\nfunc (i *Index) persistField(ctx context.Context, cfm *CreateFieldMessage) error {\n\tif cfm.Index == \"\" {\n\t\treturn ErrIndexRequired\n\t} else if cfm.Field == \"\" {\n\t\treturn ErrFieldRequired\n\t}\n\n\tif err := ValidateName(cfm.Field); err != nil {\n\t\treturn errors.Wrap(err, \"validating name\")\n\t}\n\n\tif b, err := i.serializer.Marshal(cfm); err != nil {\n\t\treturn errors.Wrap(err, \"marshaling field\")\n\t} else if err := i.holder.Schemator.CreateField(ctx, cfm.Index, cfm.Field, b); errors.Cause(err) == disco.ErrFieldExists {\n\t\treturn ErrFieldExists\n\t} else if err != nil {\n\t\treturn errors.Wrapf(err, \"writing field to disco: %s/%s\", cfm.Index, cfm.Field)\n\t}\n\treturn nil\n}\n\nfunc (i *Index) persistUpdateField(ctx context.Context, cfm *CreateFieldMessage) error {\n\tif cfm.Index == \"\" {\n\t\treturn ErrIndexRequired\n\t} else if cfm.Field == \"\" {\n\t\treturn ErrFieldRequired\n\t}\n\n\tif b, err := i.serializer.Marshal(cfm); err != nil {\n\t\treturn errors.Wrap(err, \"marshaling field\")\n\t} else if err := i.holder.Schemator.UpdateField(ctx, cfm.Index, cfm.Field, b); errors.Cause(err) == disco.ErrFieldDoesNotExist {\n\t\treturn ErrFieldNotFound\n\t} else if err != nil {\n\t\treturn errors.Wrapf(err, \"writing field to disco: %s/%s\", cfm.Index, cfm.Field)\n\t}\n\treturn nil\n}\n\nfunc (i *Index) UpdateField(ctx context.Context, name string, requestUserID string, update FieldUpdate) (*CreateFieldMessage, error) {\n\t// Get field from etcd\n\tbuf, err := i.holder.Schemator.Field(ctx, i.name, name)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"getting field '%s' from etcd\", name)\n\t}\n\tcfm, err := decodeCreateFieldMessage(i.holder.serializer, buf)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"decoding CreateFieldMessage\")\n\t} else if cfm == nil {\n\t\treturn nil, errors.New(\"got nil CreateFieldMessage when decoding\")\n\t}\n\n\t// Handle the options we know how to update, or error.\n\tswitch update.Option {\n\tcase \"TTL\", \"ttl\":\n\t\tif cfm.Meta.Type != FieldTypeTime {\n\t\t\treturn nil, NewBadRequestError(errors.Errorf(\"can only add TTL to a 'time' type field, not '%s'\", cfm.Meta.Type))\n\t\t}\n\t\tdur, err := time.ParseDuration(update.Value)\n\t\tif err != nil {\n\t\t\treturn nil, NewBadRequestError(errors.Wrap(err, \"parsing duration\"))\n\t\t}\n\t\tif dur < 0 {\n\t\t\treturn nil, NewBadRequestError(errors.Errorf(\"ttl can't be negative: '%s'\", update.Value))\n\t\t}\n\t\tcfm.Meta.TTL = dur\n\tcase \"noStandardView\":\n\t\tif cfm.Meta.Type != FieldTypeTime {\n\t\t\treturn nil, NewBadRequestError(errors.Errorf(\"can only update 'noStandardView' on a 'time' type field, not '%s'\", cfm.Meta.Type))\n\t\t}\n\t\tboolValue, err := strconv.ParseBool(update.Value)\n\t\tif err != nil {\n\t\t\treturn nil, NewBadRequestError(errors.Errorf(\"invalid value for noStandardView: '%s'\", update.Value))\n\t\t}\n\t\tcfm.Meta.NoStandardView = boolValue\n\tdefault:\n\t\treturn nil, NewBadRequestError(errors.Errorf(\"updates for option '%s' are not supported\", update.Option))\n\t}\n\n\t// Persist the updated field to etcd.\n\tif err := i.persistUpdateField(ctx, cfm); err != nil {\n\t\treturn nil, errors.Wrap(err, \"persisting updated field\")\n\t}\n\n\ti.mu.Lock()\n\tdefer i.mu.Unlock()\n\n\treturn cfm, nil\n}\n\nfunc (i *Index) UpdateFieldLocal(cfm *CreateFieldMessage, update FieldUpdate) error {\n\t// Update local structures. This assumes we don't need to do\n\t// anything else... which is fine for TTL specifically, but I'm\n\t// not sure about other things, so be aware when adding new update\n\t// abilities.\n\ti.mu.Lock()\n\tdefer i.mu.Unlock()\n\tfield := i.field(cfm.Field)\n\tif field == nil {\n\t\treturn errors.Errorf(\"field '%s' not found locally\", cfm.Field)\n\t}\n\tif err := field.applyOptions(*cfm.Meta); err != nil {\n\t\treturn errors.Wrap(err, \"updating local field options\")\n\t}\n\n\treturn nil\n}\n\n// createFieldIfNotExists creates the field if it does not already exist in the\n// in-memory index structure. This is not related to whether or not the field\n// exists in etcd.\nfunc (i *Index) createFieldIfNotExists(cfm *CreateFieldMessage) (*Field, error) {\n\ti.mu.Lock()\n\tdefer i.mu.Unlock()\n\n\t// Find field in cache first.\n\tif f := i.fields[cfm.Field]; f != nil {\n\t\treturn f, nil\n\t}\n\n\treturn i.createField(cfm)\n}\n\n// createField does the internal field creation logic, creating the in-memory\n// data structure, and kicking translation sync if appropriate. It does not\n// notify other nodes; that's done from the API's initial CreateField call\n// now.\nfunc (i *Index) createField(cfm *CreateFieldMessage) (*Field, error) {\n\topt := cfm.Meta\n\tif opt == nil {\n\t\topt = &FieldOptions{}\n\t}\n\n\t// TODO: can we do a general FieldOption validation here instead of just cache type?\n\tif cfm.Field == \"\" {\n\t\treturn nil, errors.New(\"field name required\")\n\t} else if opt.CacheType != \"\" && !isValidCacheType(opt.CacheType) {\n\t\treturn nil, ErrInvalidCacheType\n\t}\n\n\t// Initialize field.\n\tf, err := i.newField(i.fieldPath(cfm.Field), cfm.Field)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"initializing\")\n\t}\n\tf.createdAt = cfm.CreatedAt\n\tf.owner = cfm.Owner\n\n\t// Pass holder through to the field for use in looking\n\t// up a foreign index.\n\tf.holder = i.holder\n\n\tf.setOptions(opt)\n\n\t// Open field.\n\tif err := f.Open(); err != nil {\n\t\treturn nil, errors.Wrap(err, \"opening\")\n\t}\n\n\t// Add to index's field lookup.\n\ti.fields[cfm.Field] = f\n\n\t// enable Txf to find the index in field_test.go TestField_SetValue\n\tf.idx = i\n\n\t// Kick off the field's translation sync process.\n\tif err := i.translationSyncer.Reset(); err != nil {\n\t\treturn nil, errors.Wrap(err, \"resetting translation syncer\")\n\t}\n\n\treturn f, nil\n}\n\nfunc (i *Index) newField(path, name string) (*Field, error) {\n\tf, err := newField(i.holder, path, i.name, name, OptFieldTypeDefault())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tf.idx = i\n\tf.broadcaster = i.broadcaster\n\tf.serializer = i.serializer\n\tf.OpenTranslateStore = i.OpenTranslateStore\n\treturn f, nil\n}\n\n// DeleteField removes a field from the index.\nfunc (i *Index) DeleteField(name string) error {\n\ti.mu.Lock()\n\tdefer i.mu.Unlock()\n\n\t// Disallow deleting the existence field.\n\tif name == existenceFieldName {\n\t\treturn newNotFoundError(ErrFieldNotFound, existenceFieldName)\n\t}\n\n\t// Confirm field exists.\n\tf := i.field(name)\n\tif f == nil {\n\t\treturn newNotFoundError(ErrFieldNotFound, name)\n\t}\n\n\t// Delete the field from etcd as the system of record.\n\tif err := i.holder.Schemator.DeleteField(context.TODO(), i.name, name); err != nil {\n\t\treturn errors.Wrapf(err, \"deleting field from etcd: %s/%s\", i.name, name)\n\t}\n\n\t// Close field.\n\tif err := f.Close(); err != nil {\n\t\treturn errors.Wrap(err, \"closing\")\n\t}\n\n\tif err := i.holder.txf.DeleteFieldFromStore(i.name, name, i.fieldPath(name)); err != nil {\n\t\treturn errors.Wrap(err, \"Txf.DeleteFieldFromStore\")\n\t}\n\n\t// Remove reference.\n\tdelete(i.fields, name)\n\n\t// remove shard metadata for field\n\ti.fieldView2shard.removeField(name)\n\treturn i.translationSyncer.Reset()\n}\n\n// SetTranslatePartitions sets the cached value: translatePartitions.\n//\n// There's already logic in api_directive.go which creates a new index with\n// partitions. This particular function is used when the index already exists on\n// the node, but we get a Directive which changes its partition list. In that\n// case, we need to update this cached value. Really, this is kind of hacky and\n// we need to revisit the ApplyDirective logic so that it's more intuitive with\n// respect to index.translatePartitions.\nfunc (i *Index) SetTranslatePartitions(tp dax.PartitionNums) {\n\ti.mu.Lock()\n\tdefer i.mu.Unlock()\n\n\ti.translatePartitions = tp\n}\n\n// TODO (twg) refine parquet strategy a bit\nfunc (i *Index) GetDataFramePath(shard uint64) string {\n\tpath := i.DataframesPath()\n\tos.MkdirAll(i.path, 0o750)\n\tshardpad := fmt.Sprintf(\"%04d\", shard)\n\treturn filepath.Join(path, shardpad)\n}\n\ntype indexSlice []*Index\n\nfunc (p indexSlice) Swap(i, j int)      { p[i], p[j] = p[j], p[i] }\nfunc (p indexSlice) Len() int           { return len(p) }\nfunc (p indexSlice) Less(i, j int) bool { return p[i].Name() < p[j].Name() }\n\n// IndexInfo represents schema information for an index.\ntype IndexInfo struct {\n\tName           string       `json:\"name\"`\n\tCreatedAt      int64        `json:\"createdAt,omitempty\"`\n\tUpdatedAt      int64        `json:\"updatedAt\"`\n\tOwner          string       `json:\"owner\"`\n\tLastUpdateUser string       `json:\"lastUpdatedUser\"`\n\tOptions        IndexOptions `json:\"options\"`\n\tFields         []*FieldInfo `json:\"fields\"`\n\tShardWidth     uint64       `json:\"shardWidth\"`\n}\n\n// Field returns the FieldInfo the provided field name. If the field does not\n// exist, it returns nil\nfunc (ii *IndexInfo) Field(name string) *FieldInfo {\n\tfor _, fld := range ii.Fields {\n\t\tif fld.Name == name {\n\t\t\treturn fld\n\t\t}\n\t}\n\treturn nil\n}\n\ntype indexInfoSlice []*IndexInfo\n\nfunc (p indexInfoSlice) Swap(i, j int)      { p[i], p[j] = p[j], p[i] }\nfunc (p indexInfoSlice) Len() int           { return len(p) }\nfunc (p indexInfoSlice) Less(i, j int) bool { return p[i].Name < p[j].Name }\n\n// IndexOptions represents options to set when initializing an index.\ntype IndexOptions struct {\n\tKeys           bool   `json:\"keys\"`\n\tTrackExistence bool   `json:\"trackExistence\"`\n\tPartitionN     int    `json:\"partitionN\"`\n\tDescription    string `json:\"description\"`\n}\n\ntype importData struct {\n\tRowIDs    []uint64\n\tColumnIDs []uint64\n}\n\n// FormatQualifiedIndexName generates a qualified name for the index to be used with Tx operations.\nfunc FormatQualifiedIndexName(index string) string {\n\treturn fmt.Sprintf(\"%s\\x00\", index)\n}\n"
        },
        {
          "name": "index_internal_test.go",
          "type": "blob",
          "size": 0.447265625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"testing\"\n)\n\n// mustOpenIndex returns a new, opened index at a temporary path. Panic on error.\nfunc mustOpenIndex(tb testing.TB, opt IndexOptions) *Index {\n\th := newTestHolder(tb)\n\tindex, err := h.CreateIndex(\"i\", \"\", opt)\n\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tindex.keys = opt.Keys\n\tindex.trackExistence = opt.TrackExistence\n\n\treturn index\n}\n"
        },
        {
          "name": "index_test.go",
          "type": "blob",
          "size": 9.3505859375,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"math/rand\"\n\t\"reflect\"\n\t\"testing\"\n\t\"time\"\n\n\tpilosa \"github.com/featurebasedb/featurebase/v3\"\n\t\"github.com/featurebasedb/featurebase/v3/disco\"\n\t\"github.com/featurebasedb/featurebase/v3/pql\"\n\t\"github.com/featurebasedb/featurebase/v3/test\"\n\t\"github.com/pkg/errors\"\n)\n\n// ShardWidth is a helper reference to use when testing.\nconst ShardWidth = pilosa.ShardWidth\n\n// Ensure index can open and retrieve a field.\nfunc TestIndex_CreateFieldIfNotExists(t *testing.T) {\n\t_, index := test.MustOpenIndex(t)\n\n\t// Create field.\n\tf, err := index.CreateFieldIfNotExists(\"f\", \"\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t} else if f == nil {\n\t\tt.Fatal(\"expected field\")\n\t}\n\n\t// Retrieve existing field.\n\tother, err := index.CreateFieldIfNotExists(\"f\", \"\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t} else if f.Field != other.Field {\n\t\tt.Fatal(\"field mismatch\")\n\t}\n\n\tif f.Field != index.Field(\"f\") {\n\t\tt.Fatal(\"field mismatch\")\n\t}\n}\n\nfunc TestIndex_CreateField(t *testing.T) {\n\t// Ensure time quantum can be set appropriately on a new field.\n\tt.Run(\"TimeQuantum\", func(t *testing.T) {\n\t\tt.Run(\"Explicit\", func(t *testing.T) {\n\t\t\t_, index := test.MustOpenIndex(t)\n\n\t\t\t// Create field with explicit quantum.\n\t\t\tf, err := index.CreateField(\"f\", \"\", pilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMDH\"), \"0\"))\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if q := f.TimeQuantum(); q != pilosa.TimeQuantum(\"YMDH\") {\n\t\t\t\tt.Fatalf(\"unexpected field time quantum: %s\", q)\n\t\t\t}\n\t\t})\n\t})\n\n\t// Ensure time quantum can be set appropriately on a new field.\n\tt.Run(\"TimeQuantumNoStandardView\", func(t *testing.T) {\n\t\tt.Run(\"Explicit\", func(t *testing.T) {\n\t\t\t_, index := test.MustOpenIndex(t)\n\n\t\t\t// Create field with explicit quantum with no standard view\n\t\t\tf, err := index.CreateField(\"f\", \"\", pilosa.OptFieldTypeTime(pilosa.TimeQuantum(\"YMDH\"), \"0\", true))\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if q := f.TimeQuantum(); q != pilosa.TimeQuantum(\"YMDH\") {\n\t\t\t\tt.Fatalf(\"unexpected field time quantum: %s\", q)\n\t\t\t}\n\t\t})\n\t})\n\n\t// Ensure field can include range columns.\n\tt.Run(\"BSIFields\", func(t *testing.T) {\n\t\tt.Run(\"Int\", func(t *testing.T) {\n\t\t\t_, index := test.MustOpenIndex(t)\n\n\t\t\t// Create field with schema and verify it exists.\n\t\t\tif f, err := index.CreateField(\"f\", \"\", pilosa.OptFieldTypeInt(-990, 1000)); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if !reflect.DeepEqual(f.Type(), pilosa.FieldTypeInt) {\n\t\t\t\tt.Fatalf(\"unexpected type: %#v\", f.Type())\n\t\t\t}\n\n\t\t\t// Reopen the index & verify the fields are loaded.\n\t\t\tif err := index.Reopen(); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if f := index.Field(\"f\"); !reflect.DeepEqual(f.Type(), pilosa.FieldTypeInt) {\n\t\t\t\tt.Fatalf(\"unexpected type after reopen: %#v\", f.Type())\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"Timestamp\", func(t *testing.T) {\n\t\t\t_, index := test.MustOpenIndex(t)\n\n\t\t\t// Create field with schema and verify it exists.\n\t\t\tif f, err := index.CreateField(\"f\", \"\", pilosa.OptFieldTypeTimestamp(pilosa.DefaultEpoch, pilosa.TimeUnitSeconds)); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if !reflect.DeepEqual(f.Type(), pilosa.FieldTypeTimestamp) {\n\t\t\t\tt.Fatalf(\"unexpected type: %#v\", f.Type())\n\t\t\t}\n\n\t\t\t// Reopen the index & verify the fields are loaded.\n\t\t\tif err := index.Reopen(); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t} else if f := index.Field(\"f\"); !reflect.DeepEqual(f.Type(), pilosa.FieldTypeTimestamp) {\n\t\t\t\tt.Fatalf(\"unexpected type after reopen: %#v\", f.Type())\n\t\t\t}\n\t\t})\n\n\t\t// TODO: These errors don't apply  here. Instead, we need these tests\n\t\t// on field creation FieldOptions validation.\n\t\t/*\n\t\t\tt.Run(\"ErrRangeCacheAllowed\", func(t *testing.T) {\n\t\t\t\t_, index := test.MustOpenIndex(t)\n\n\t\t\t\tif _, err := index.CreateField(\"f\", pilosa.FieldOptions{\n\t\t\t\t\tCacheType: pilosa.CacheTypeRanked,\n\t\t\t\t}); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"BSIFieldsWithCacheTypeNone\", func(t *testing.T) {\n\t\t\t\t_, index := test.MustOpenIndex(t)\n\t\t\t\tif _, err := index.CreateField(\"f\", pilosa.FieldOptions{\n\t\t\t\t\tCacheType: pilosa.CacheTypeNone,\n\t\t\t\t\tCacheSize: uint32(5),\n\t\t\t\t}); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"ErrFieldFieldsAllowed\", func(t *testing.T) {\n\t\t\t\t_, index := test.MustOpenIndex(t)\n\n\t\t\t\tif _, err := index.CreateField(\"f\", pilosa.FieldOptions{\n\t\t\t\t\tFields: []*pilosa.Field{\n\t\t\t\t\t\t{Name: \"field0\", Type: pilosa.FieldTypeInt},\n\t\t\t\t\t},\n\t\t\t\t}); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"ErrFieldNameRequired\", func(t *testing.T) {\n\t\t\t\t_, index := test.MustOpenIndex(t)\n\n\t\t\t\tif _, err := index.CreateField(\"f\", pilosa.FieldOptions{\n\t\t\t\t\tFields: []*pilosa.Field{\n\t\t\t\t\t\t{Name: \"\", Type: pilosa.FieldTypeInt},\n\t\t\t\t\t},\n\t\t\t\t}); err != pilosa.ErrFieldNameRequired {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"ErrInvalidFieldType\", func(t *testing.T) {\n\t\t\t\t_, index := test.MustOpenIndex(t)\n\n\t\t\t\tif _, err := index.CreateField(\"f\", pilosa.FieldOptions{\n\t\t\t\t\tFields: []*pilosa.Field{\n\t\t\t\t\t\t{Name: \"field0\", Type: \"bad_type\"},\n\t\t\t\t\t},\n\t\t\t\t}); err != pilosa.ErrInvalidFieldType {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"ErrInvalidBSIGroupRange\", func(t *testing.T) {\n\t\t\t\t_, index := test.MustOpenIndex(t)\n\n\t\t\t\tif _, err := index.CreateField(\"f\", pilosa.FieldOptions{\n\t\t\t\t\tFields: []*pilosa.Field{\n\t\t\t\t\t\t{Name: \"field0\", Type: pilosa.FieldTypeInt, Min: 100, Max: 50},\n\t\t\t\t\t},\n\t\t\t\t}); err != pilosa.ErrInvalidBSIGroupRange {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t})\n\t\t*/\n\t})\n\n\tt.Run(\"WithKeys\", func(t *testing.T) {\n\t\t// Don't allow an int field to be created with keys=true\n\t\tt.Run(\"IntField\", func(t *testing.T) {\n\t\t\t_, index := test.MustOpenIndex(t)\n\n\t\t\t_, err := index.CreateField(\"f\", \"\", pilosa.OptFieldTypeInt(-1, 1), pilosa.OptFieldKeys())\n\t\t\tif errors.Cause(err) != pilosa.ErrIntFieldWithKeys {\n\t\t\t\tt.Fatal(\"int field cannot be created with keys=true\")\n\t\t\t}\n\t\t})\n\n\t\t// Don't allow a decimal field to be created with keys=true\n\t\tt.Run(\"DecimalField\", func(t *testing.T) {\n\t\t\t_, index := test.MustOpenIndex(t)\n\n\t\t\t_, err := index.CreateField(\"f\", \"\", pilosa.OptFieldTypeDecimal(1, pql.NewDecimal(-1, 0), pql.NewDecimal(1, 0)), pilosa.OptFieldKeys())\n\t\t\tif errors.Cause(err) != pilosa.ErrDecimalFieldWithKeys {\n\t\t\t\tt.Fatal(\"decimal field cannot be created with keys=true\")\n\t\t\t}\n\t\t})\n\t})\n}\n\n// Ensure index can delete a field.\nfunc TestIndex_DeleteField(t *testing.T) {\n\t_, index := test.MustOpenIndex(t)\n\n\t// Create field.\n\tif _, err := index.CreateFieldIfNotExists(\"f\", \"\"); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Delete field & verify it's gone.\n\tif err := index.DeleteField(\"f\"); err != nil {\n\t\tt.Fatal(err)\n\t} else if index.Field(\"f\") != nil {\n\t\tt.Fatal(\"expected nil field\")\n\t}\n\n\t// Delete again to make sure it errors.\n\terr := index.DeleteField(\"f\")\n\tif !isNotFoundError(err) {\n\t\tt.Fatalf(\"expected 'field not found' error, got: %#v\", err)\n\t}\n}\n\n// Ensure index can validate its name.\nfunc TestIndex_InvalidName(t *testing.T) {\n\tholder := test.NewHolder(t).Holder\n\tindex, err := pilosa.NewIndex(holder, holder.IndexPath(\"ABC\"), \"ABC\")\n\tif err == nil {\n\t\tt.Fatalf(\"should have gotten an error on index name with caps\")\n\t}\n\tif index != nil {\n\t\tt.Fatalf(\"unexpected index name %v\", index)\n\t}\n}\n\nfunc isNotFoundError(err error) bool {\n\troot := errors.Cause(err)\n\t_, ok := root.(pilosa.NotFoundError)\n\treturn ok\n}\n\n// Ensure that after node/cluster restart, deleting and recreating a field\n// does not cause a deadlock\n// This is a regression test after a customer experienced the same deadlock.\n// For details, check out https://molecula.atlassian.net/browse/CORE-919\nfunc TestIndex_RecreateFieldOnRestart(t *testing.T) {\n\tc := test.MustRunUnsharedCluster(t, 1)\n\tdefer func() {\n\t\t// We anticipate a deadlock, and if we hit the deadlock,\n\t\t// closing would ALSO deadlock, so we won't want to do that.\n\t\t//\n\t\t// The alternative, of trying to call os.Exit, prevents us\n\t\t// from reporting anything at all, because testing doesn't\n\t\t// actually display messages until it's done.\n\t\t//\n\t\t// So, if we're bailing because of a fatal error, we don't\n\t\t// try to close the cluster, because Something Went Wrong and\n\t\t// it may well have been a deadlock. We leak one cluster on\n\t\t// a failed test, but we correctly report the test as failed\n\t\t// before also reporting the unclosed resources.\n\t\tif !t.Failed() {\n\t\t\tc.Close()\n\t\t}\n\t}()\n\n\t// create index\n\tindexName := fmt.Sprintf(\"idx_%d\", rand.Uint64())\n\tholder := c.GetHolder(0)\n\t_, err := holder.CreateIndex(indexName, \"\", pilosa.IndexOptions{\n\t\tKeys: false,\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// create field\n\tfieldName := fmt.Sprintf(\"field_%d\", rand.Uint64())\n\t_, err = c.GetNode(0).API.CreateField(context.Background(), indexName, fieldName)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// set value\n\t_, err = c.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{\n\t\tIndex: indexName,\n\t\tQuery: fmt.Sprintf(`Set(1, %s=1)`, fieldName),\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// restart node\n\tnode := c.GetNode(0)\n\tif err := node.Reopen(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif err := c.AwaitState(disco.ClusterStateNormal, 10*time.Second); err != nil {\n\t\tt.Fatalf(\"restarting cluster: %v\", err)\n\t}\n\n\t// delete field\n\terr = c.GetNode(0).API.DeleteField(context.Background(), indexName, fieldName)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// recreate field\n\terrCh := make(chan error)\n\tgo func() {\n\t\t_, err := c.GetNode(0).API.CreateField(context.Background(), indexName,\n\t\t\tfieldName)\n\t\terrCh <- err\n\t}()\n\tselect {\n\tcase <-time.After(10 * time.Second):\n\t\tt.Fatal(\"recreating field took too long\")\n\tcase err := <-errCh:\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n}\n"
        },
        {
          "name": "install",
          "type": "tree",
          "content": null
        },
        {
          "name": "internal",
          "type": "tree",
          "content": null
        },
        {
          "name": "internal_client.go",
          "type": "blob",
          "size": 74.3232421875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"math/rand\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"os\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\tfbcontext \"github.com/featurebasedb/featurebase/v3/context\"\n\n\t\"github.com/featurebasedb/featurebase/v3/authn\"\n\t\"github.com/featurebasedb/featurebase/v3/dax\"\n\t\"github.com/featurebasedb/featurebase/v3/disco\"\n\t\"github.com/featurebasedb/featurebase/v3/logger\"\n\tpnet \"github.com/featurebasedb/featurebase/v3/net\"\n\t\"github.com/featurebasedb/featurebase/v3/tracing\"\n\t\"github.com/hashicorp/go-retryablehttp\"\n\t\"github.com/pkg/errors\"\n\t\"golang.org/x/oauth2\"\n)\n\n// InternalClient represents a client to the Pilosa cluster.\ntype InternalClient struct {\n\tdefaultURI *pnet.URI\n\tserializer Serializer\n\n\tlog logger.Logger\n\n\t// The client to use for HTTP communication.\n\thttpClient      *http.Client\n\tretryableClient *retryablehttp.Client\n\t// nearly-identical clients, except they have a CheckRedirect that tries to forward\n\t// authentication\n\tauthHttpClient      *http.Client\n\tauthRetryableClient *retryablehttp.Client\n\t// the local node's API, used for operations that we can short-circuit that way\n\tapi *API\n\n\t// secret Key for auth across nodes\n\tsecretKey string\n\n\t// pathPrefix is prepended to every URL path. This is used, for example,\n\t// when running a compute nodes as a sub-service of the featurebase command.\n\t// In that case, a path might look like `localhost:8080/compute/schema`,\n\t// where `/compute` is the pathPrefix.\n\tpathPrefix string\n}\n\n// NewInternalClient returns a new instance of InternalClient to connect to host.\n// If api is non-nil, the client uses it for some same-host operations instead\n// of going through http.\nfunc NewInternalClient(host string, remoteClient *http.Client, opts ...InternalClientOption) (*InternalClient, error) {\n\tif host == \"\" {\n\t\treturn nil, ErrHostRequired\n\t}\n\n\turi, err := pnet.NewURIFromAddress(host)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"getting URI\")\n\t}\n\n\tclient := NewInternalClientFromURI(uri, remoteClient, opts...)\n\treturn client, nil\n}\n\ntype InternalClientOption func(c *InternalClient)\n\nfunc WithSerializer(s Serializer) InternalClientOption {\n\treturn func(c *InternalClient) {\n\t\tc.serializer = s\n\t}\n}\n\n// WithSecretKey adds the secretKey used for inter-node communication when auth\n// is enabled\nfunc WithSecretKey(secretKey string) InternalClientOption {\n\treturn func(c *InternalClient) {\n\t\tc.secretKey = secretKey\n\t}\n}\n\n// WithClientRetryPeriod is the max amount of total time the client will\n// retry failed requests using exponential backoff.\nfunc WithClientRetryPeriod(period time.Duration) InternalClientOption {\n\tmin := time.Millisecond * 100\n\n\t// do some math to figure out how many attempts we need to get our\n\t// total sleep time close to the period\n\tattempts := math.Log2(float64(period)) - math.Log2(float64(min))\n\tattempts += 0.3 // mmmm, fudge\n\tif attempts < 1 {\n\t\tattempts = 1\n\t}\n\n\treturn func(c *InternalClient) {\n\t\trc := retryablehttp.NewClient()\n\t\trc.HTTPClient = c.httpClient\n\t\trc.RetryWaitMin = min\n\t\trc.RetryMax = int(attempts)\n\t\trc.CheckRetry = retryWith400Policy\n\t\trc.Logger = logger.NopLogger\n\t\tc.retryableClient = rc\n\t}\n}\n\nfunc WithClientLogger(log logger.Logger) InternalClientOption {\n\treturn func(c *InternalClient) {\n\t\tc.log = log\n\t}\n}\n\n// WithPathPrefix sets the http path prefix.\nfunc WithPathPrefix(prefix string) InternalClientOption {\n\treturn func(c *InternalClient) {\n\t\tc.pathPrefix = prefix\n\t}\n}\n\nfunc noRetryPolicy(ctx context.Context, resp *http.Response, err error) (bool, error) {\n\treturn false, nil\n}\n\ntype statusAndError struct {\n\tstatusCode int\n\tmsg        string\n\terr        error\n}\n\ntype statusesAndErrors struct {\n\terrs []statusAndError\n}\n\n// recordEvent figures out how it wants to display a given\n// error or response-without-error that is nonetheless possibly\n// error-shaped (such as a 4xx or 5xx status), and appends it\n// to the list. It may read from resp.Body, so it may be destructive,\n// and it does not close resp.Body.\nfunc (s *statusesAndErrors) recordEvent(resp *http.Response, err error) {\n\tvar message string = \"[no response, yielding 500 error]\"\n\tstatusCode := http.StatusInternalServerError\n\tif resp != nil {\n\t\t// grab an initial chunk of response body -- not too long\n\t\t// because we don't know whether it's sensical -- in case it's a\n\t\t// legible error message\n\t\tmsg := make([]byte, 128)\n\t\tn, err := resp.Body.Read(msg)\n\t\tif err != nil {\n\t\t\tmessage = fmt.Sprintf(\"[error reading resp body: %v]\", err)\n\t\t} else {\n\t\t\tmessage = string(msg[:n])\n\t\t}\n\t\tstatusCode = resp.StatusCode\n\t}\n\ts.errs = append(s.errs, statusAndError{statusCode: statusCode, msg: message, err: err})\n}\n\ntype statusContextKey struct{}\n\nfunc newStatusTrackingContext(ctx context.Context) (*statusesAndErrors, context.Context) {\n\tstatuses := &statusesAndErrors{}\n\tif ctx == nil {\n\t\tctx = context.TODO()\n\t}\n\treturn statuses, context.WithValue(ctx, statusContextKey{}, statuses)\n}\n\n// retryWith400Policy is a retry policy for retryable http, which retries\n// on 4xx status codes, but also logs the status codes and errors handed to it in\n// the slice you provide a pointer to, so you can display them later. We do this\n// because we have spurious 4xx errors that we need to fix.\nfunc retryWith400Policy(ctx context.Context, resp *http.Response, err error) (bool, error) {\n\tif err != nil || resp.StatusCode >= 300 {\n\t\tstatuses := ctx.Value(statusContextKey{})\n\t\tif statuses != nil {\n\t\t\terrs, ok := statuses.(*statusesAndErrors)\n\t\t\tif ok {\n\t\t\t\terrs.recordEvent(resp, err)\n\t\t\t}\n\t\t}\n\t}\n\tif resp != nil && resp.StatusCode >= 400 {\n\t\treturn true, nil\n\t}\n\treturn retryablehttp.DefaultRetryPolicy(ctx, resp, err)\n}\n\nfunc NewInternalClientFromURI(defaultURI *pnet.URI, remoteClient *http.Client, opts ...InternalClientOption) *InternalClient {\n\tic := &InternalClient{\n\t\tdefaultURI: defaultURI,\n\t\thttpClient: remoteClient,\n\t\tlog:        logger.NewStandardLogger(os.Stderr),\n\t}\n\n\tfor _, opt := range opts {\n\t\topt(ic)\n\t}\n\n\tif ic.retryableClient == nil {\n\t\trc := retryablehttp.NewClient()\n\t\trc.HTTPClient = ic.httpClient\n\t\trc.CheckRetry = noRetryPolicy\n\t\trc.Logger = logger.NopLogger\n\t\tic.retryableClient = rc\n\t}\n\n\t// and now, we duplicate the clients for auth forwarding:\n\tauthClient := *ic.httpClient\n\tauthClient.CheckRedirect = func(req *http.Request, via []*http.Request) error {\n\t\tif len(via) > 0 {\n\t\t\taccess, refresh := getTokens(via[0])\n\t\t\treq.Header.Set(\"Authorization\", \"Bearer \"+access)\n\t\t\treq.Header.Set(authn.RefreshHeaderName, refresh)\n\t\t}\n\t\treturn nil\n\t}\n\n\trc := retryablehttp.NewClient()\n\trc.HTTPClient = &authClient\n\trc.RetryWaitMin = ic.retryableClient.RetryWaitMin\n\trc.RetryMax = ic.retryableClient.RetryMax\n\trc.CheckRetry = retryWith400Policy\n\trc.Logger = logger.NopLogger\n\tic.authRetryableClient = rc\n\tic.authHttpClient = &authClient\n\treturn ic\n}\n\n// AddAuthToken checks in a couple spots for our authorization token and\n// adds it to the Authorization Header in the request if it finds it. It does the\n// same for refresh tokens as well.\nfunc AddAuthToken(ctx context.Context, header *http.Header) {\n\tvar access, refresh string\n\tif token, ok := authn.GetAccessToken(ctx); ok {\n\t\t// the AccessToken value should be prefixed with \"Bearer\"\n\t\taccess = token\n\t}\n\tif token, ok := authn.GetRefreshToken(ctx); ok {\n\t\trefresh = token\n\t}\n\n\t// not combining these ifs so we don't call ctx.Value unless we have to\n\tif access == \"\" || refresh == \"\" {\n\t\tif uinfo, _ := authn.GetUserInfo(ctx); uinfo != nil {\n\t\t\tif access == \"\" {\n\t\t\t\t// UserInfo.Token is not prefixed with \"Bearer\"\n\t\t\t\taccess = \"Bearer \" + uinfo.Token\n\t\t\t}\n\t\t\tif refresh == \"\" {\n\t\t\t\trefresh = uinfo.RefreshToken\n\t\t\t}\n\t\t}\n\t}\n\n\t// set ogIP to request for remote calls\n\tif ogIP, ok := fbcontext.OriginalIP(ctx); ok && ogIP != \"\" {\n\t\theader.Set(OriginalIPHeader, ogIP)\n\t}\n\n\theader.Set(\"Authorization\", access)\n\theader.Set(authn.RefreshHeaderName, refresh)\n}\n\n// prefix is a helper function which allows us to provide a pathPrefix value as\n// \"compute\" instead of \"/compute\".\nfunc (c *InternalClient) prefix() string {\n\tif c.pathPrefix == \"\" {\n\t\treturn \"\"\n\t}\n\treturn \"/\" + c.pathPrefix\n}\n\n// MaxShardByIndex returns the number of shards on a server by index.\nfunc (c *InternalClient) MaxShardByIndex(ctx context.Context) (map[string]uint64, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.MaxShardByIndex\")\n\tdefer span.Finish()\n\treturn c.maxShardByIndex(ctx)\n}\n\n// maxShardByIndex returns the number of shards on a server by index.\nfunc (c *InternalClient) maxShardByIndex(ctx context.Context) (map[string]uint64, error) {\n\t// Execute request against the host.\n\tpath := fmt.Sprintf(\"%s/internal/shards/max\", c.prefix())\n\tu := uriPathToURL(c.defaultURI, path)\n\n\t// Build request.\n\treq, err := http.NewRequest(\"GET\", u.String(), nil)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\treq.Header.Set(\"Accept\", \"application/json\")\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\n\tvar rsp getShardsMaxResponse\n\tif err := json.NewDecoder(resp.Body).Decode(&rsp); err != nil {\n\t\treturn nil, fmt.Errorf(\"json decode: %s\", err)\n\t}\n\n\treturn rsp.Standard, nil\n}\n\n// AvailableShards returns a list of shards for an index.\nfunc (c *InternalClient) AvailableShards(ctx context.Context, indexName string) ([]uint64, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.AvailableShards\")\n\tdefer span.Finish()\n\n\t// Execute request against the host.\n\tpath := fmt.Sprintf(\"%s/internal/index/%s/shards\", c.prefix(), indexName)\n\tu := uriPathToURL(c.defaultURI, path)\n\n\t// Build request.\n\treq, err := http.NewRequest(\"GET\", u.String(), nil)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\treq.Header.Set(\"Accept\", \"application/json\")\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\n\tvar rsp getIndexAvailableShardsResponse\n\tif err := json.NewDecoder(resp.Body).Decode(&rsp); err != nil {\n\t\treturn nil, fmt.Errorf(\"json decode: %s\", err)\n\t}\n\treturn rsp.Shards, nil\n}\n\n// SchemaNode returns all index and field schema information from the specified\n// node.\nfunc (c *InternalClient) SchemaNode(ctx context.Context, uri *pnet.URI, views bool) ([]*IndexInfo, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.Schema\")\n\tdefer span.Finish()\n\n\t// TODO: /?views parameter will be ignored, till we implement schemator!\n\t// Execute request against the host.\n\tu := uri.Path(fmt.Sprintf(\"%s/schema?views=%v\", c.prefix(), views))\n\n\t// Build request.\n\treq, err := http.NewRequest(\"GET\", u, nil)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\treq.Header.Set(\"Accept\", \"application/json\")\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\n\tvar rsp getSchemaResponse\n\tif err := json.NewDecoder(resp.Body).Decode(&rsp); err != nil {\n\t\treturn nil, fmt.Errorf(\"json decode: %s\", err)\n\t}\n\treturn rsp.Indexes, nil\n}\n\n// Schema returns all index and field schema information.\nfunc (c *InternalClient) Schema(ctx context.Context) ([]*IndexInfo, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.Schema\")\n\tdefer span.Finish()\n\n\t// Execute request against the host.\n\tu := c.defaultURI.Path(fmt.Sprintf(\"%s/schema\", c.prefix()))\n\n\t// Build request.\n\treq, err := http.NewRequest(\"GET\", u, nil)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\treq.Header.Set(\"Accept\", \"application/json\")\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\n\tvar rsp getSchemaResponse\n\tif err := json.NewDecoder(resp.Body).Decode(&rsp); err != nil {\n\t\treturn nil, fmt.Errorf(\"json decode: %s\", err)\n\t}\n\treturn rsp.Indexes, nil\n}\n\n// MutexCheck uses the mutex-check endpoint to request mutex collision data\n// from a single node. It produces per-shard results, and does not translate\n// them.\nfunc (c *InternalClient) MutexCheck(ctx context.Context, uri *pnet.URI, indexName string, fieldName string, details bool, limit int) (map[uint64]map[uint64][]uint64, error) {\n\tif uri == nil {\n\t\turi = c.defaultURI\n\t}\n\t// This is not actually a \"Path\", but reworking this to support queries\n\t// is messier than I have resources to pursue just now.\n\tu := uri.Path(fmt.Sprintf(\"%s/internal/index/%s/field/%s/mutex-check?details=%t&limit=%d\", c.prefix(), indexName, fieldName, details, limit))\n\treq, err := http.NewRequest(\"GET\", u, nil)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\treq.Header.Set(\"Accept\", \"application/json\")\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\tAddAuthToken(ctx, &req.Header)\n\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"executing request\")\n\t}\n\tdefer resp.Body.Close()\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn nil, errors.Errorf(\"unexpected status code: %s\", resp.Status)\n\t}\n\tvar out map[uint64]map[uint64][]uint64\n\tdec := json.NewDecoder(resp.Body)\n\terr = dec.Decode(&out)\n\treturn out, err\n}\n\nfunc (c *InternalClient) PostSchema(ctx context.Context, uri *pnet.URI, s *Schema, remote bool) error {\n\tu := uri.Path(fmt.Sprintf(\"%s/schema?remote=%v\", c.prefix(), remote))\n\tbuf, err := json.Marshal(s)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"marshalling schema\")\n\t}\n\treq, err := http.NewRequest(\"POST\", u, bytes.NewReader(buf))\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"creating request\")\n\t}\n\n\treq.Header.Set(\"Content-Length\", strconv.Itoa(len(buf)))\n\treq.Header.Set(\"Content-Type\", \"application/json\")\n\treq.Header.Set(\"Accept\", \"application/json\")\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\tAddAuthToken(ctx, &req.Header)\n\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"executing request\")\n\t}\n\tdefer resp.Body.Close()\n\tif resp.StatusCode != http.StatusNoContent {\n\t\treturn errors.Errorf(\"unexpected status code: %s\", resp.Status)\n\t}\n\treturn nil\n}\n\n// CreateIndex creates a new index on the server.\nfunc (c *InternalClient) CreateIndex(ctx context.Context, index string, opt IndexOptions) error {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.CreateIndex\")\n\tdefer span.Finish()\n\n\t// Get the primary node. Schema changes must go through\n\t// primary to avoid weird race conditions.\n\tnodes, err := c.Nodes(ctx)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"getting nodes: %s\", err)\n\t}\n\tcoord := getPrimaryNode(nodes)\n\tif coord == nil {\n\t\treturn fmt.Errorf(\"could not find the primary node\")\n\t}\n\n\t// Encode query request.\n\tbuf, err := json.Marshal(&postIndexRequest{\n\t\tOptions: opt,\n\t})\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"encoding request\")\n\t}\n\n\t// Create URL & HTTP request.\n\tu := uriPathToURL(&coord.URI, fmt.Sprintf(\"%s/index/%s\", c.prefix(), index))\n\treq, err := http.NewRequest(\"POST\", u.String(), bytes.NewReader(buf))\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"creating request\")\n\t}\n\treq.Header.Set(\"Content-Length\", strconv.Itoa(len(buf)))\n\treq.Header.Set(\"Content-Type\", \"application/json\")\n\treq.Header.Set(\"Accept\", \"application/json\")\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request against the host.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\tif resp != nil && resp.StatusCode == http.StatusConflict {\n\t\t\treturn ErrIndexExists\n\t\t}\n\t\treturn err\n\t}\n\treturn errors.Wrap(resp.Body.Close(), \"closing response body\")\n}\n\n// FragmentNodes returns a list of nodes that own a shard.\nfunc (c *InternalClient) FragmentNodes(ctx context.Context, index string, shard uint64) ([]*disco.Node, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.FragmentNodes\")\n\tdefer span.Finish()\n\n\t// Execute request against the host.\n\tu := uriPathToURL(c.defaultURI, fmt.Sprintf(\"%s/internal/fragment/nodes\", c.prefix()))\n\tu.RawQuery = (url.Values{\"index\": {index}, \"shard\": {strconv.FormatUint(shard, 10)}}).Encode()\n\n\t// Build request.\n\treq, err := http.NewRequest(\"GET\", u.String(), nil)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\treq.Header.Set(\"Accept\", \"application/json\")\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\n\tvar a []*disco.Node\n\tif err := json.NewDecoder(resp.Body).Decode(&a); err != nil {\n\t\treturn nil, fmt.Errorf(\"json decode: %s\", err)\n\t}\n\treturn a, nil\n}\n\n// Nodes returns a list of all nodes.\nfunc (c *InternalClient) Nodes(ctx context.Context) ([]*disco.Node, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.Nodes\")\n\tdefer span.Finish()\n\n\t// Execute request against the host.\n\tu := uriPathToURL(c.defaultURI, fmt.Sprintf(\"%s/internal/nodes\", c.prefix()))\n\n\t// Build request.\n\treq, err := http.NewRequest(\"GET\", u.String(), nil)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\treq.Header.Set(\"Accept\", \"application/json\")\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\n\tvar a []*disco.Node\n\tif err := json.NewDecoder(resp.Body).Decode(&a); err != nil {\n\t\treturn nil, fmt.Errorf(\"json decode: %s\", err)\n\t}\n\treturn a, nil\n}\n\n// Query executes query against the index.\nfunc (c *InternalClient) Query(ctx context.Context, index string, queryRequest *QueryRequest) (*QueryResponse, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.Query\")\n\tdefer span.Finish()\n\taddr := dax.Address(c.defaultURI.String())\n\treturn c.QueryNode(ctx, addr, index, queryRequest)\n}\n\n// QueryNode executes query against the index, sending the request to the node specified.\nfunc (c *InternalClient) QueryNode(ctx context.Context, addr dax.Address, index string, queryRequest *QueryRequest) (*QueryResponse, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"QueryNode\")\n\tdefer span.Finish()\n\n\tif index == \"\" {\n\t\treturn nil, ErrIndexRequired\n\t} else if queryRequest.Query == \"\" {\n\t\treturn nil, ErrQueryRequired\n\t}\n\tbuf, err := c.serializer.Marshal(queryRequest)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"marshaling queryRequest\")\n\t}\n\n\t// Create HTTP request.\n\tu := fmt.Sprintf(\"%s/index/%s/query\", addr.WithScheme(\"http\"), index)\n\treq, err := http.NewRequest(\"POST\", u, bytes.NewReader(buf))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\tAddAuthToken(ctx, &req.Header)\n\n\treq.Header.Set(\"Content-Length\", strconv.Itoa(len(buf)))\n\treq.Header.Set(\"Content-Type\", \"application/x-protobuf\")\n\treq.Header.Set(\"Accept\", \"application/x-protobuf\")\n\treq.Header.Set(\"X-Pilosa-Row\", \"roaring\")\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\n\t// Execute request against the host.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"'%s'\", queryRequest.Query)\n\t}\n\tdefer resp.Body.Close()\n\n\t// Read body and unmarshal response.\n\tbody, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"reading\")\n\t}\n\n\tqresp := &QueryResponse{}\n\tif err := c.serializer.Unmarshal(body, qresp); err != nil {\n\t\treturn nil, fmt.Errorf(\"unmarshal response: %s\", err)\n\t} else if qresp.Err != nil {\n\t\treturn nil, qresp.Err\n\t}\n\n\treturn qresp, nil\n}\n\nfunc getPrimaryNode(nodes []*disco.Node) *disco.Node {\n\tfor _, node := range nodes {\n\t\tif node.IsPrimary {\n\t\t\treturn node\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (c *InternalClient) EnsureIndex(ctx context.Context, name string, options IndexOptions) error {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.EnsureIndex\")\n\tdefer span.Finish()\n\n\terr := c.CreateIndex(ctx, name, options)\n\tif err == nil || errors.Cause(err) == ErrIndexExists {\n\t\treturn nil\n\t}\n\treturn err\n}\n\nfunc (c *InternalClient) EnsureField(ctx context.Context, indexName string, fieldName string) error {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.EnsureField\")\n\tdefer span.Finish()\n\treturn c.EnsureFieldWithOptions(ctx, indexName, fieldName, FieldOptions{})\n}\n\nfunc (c *InternalClient) EnsureFieldWithOptions(ctx context.Context, indexName string, fieldName string, opt FieldOptions) error {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.EnsureFieldWithOptions\")\n\tdefer span.Finish()\n\terr := c.CreateFieldWithOptions(ctx, indexName, fieldName, opt)\n\tif err == nil || errors.Cause(err) == ErrFieldExists {\n\t\treturn nil\n\t}\n\treturn err\n}\n\n// importNode sends a pre-marshaled import request to a node.\nfunc (c *InternalClient) importNode(ctx context.Context, node *disco.Node, index, field string, buf []byte, opts *ImportOptions) error {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.importNode\")\n\tdefer span.Finish()\n\n\t// Create URL & HTTP request.\n\tpath := fmt.Sprintf(\"%s/index/%s/field/%s/import\", c.prefix(), index, field)\n\tu := nodePathToURL(node, path)\n\n\tvals := url.Values{}\n\tif opts.Clear {\n\t\tvals.Set(\"clear\", \"true\")\n\t}\n\tif opts.IgnoreKeyCheck {\n\t\tvals.Set(\"ignoreKeyCheck\", \"true\")\n\t}\n\turl := fmt.Sprintf(\"%s?%s\", u.String(), vals.Encode())\n\n\treq, err := http.NewRequest(\"POST\", url, bytes.NewReader(buf))\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"creating request\")\n\t}\n\treq.Header.Set(\"Content-Length\", strconv.Itoa(len(buf)))\n\treq.Header.Set(\"Content-Type\", \"application/x-protobuf\")\n\treq.Header.Set(\"Accept\", \"application/x-protobuf\")\n\treq.Header.Set(\"X-Pilosa-Row\", \"roaring\")\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request against the host.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer resp.Body.Close()\n\n\t// Read body and unmarshal response.\n\tbody, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"reading\")\n\t}\n\n\tvar isresp ImportResponse\n\tif err := c.serializer.Unmarshal(body, &isresp); err != nil {\n\t\treturn fmt.Errorf(\"unmarshal import response: %s\", err)\n\t} else if s := isresp.Err; s != \"\" {\n\t\treturn errors.New(s)\n\t}\n\n\treturn nil\n}\n\n// importHelper is an experiment to see whether SonarCloud's code duplication\n// complaints make sense to address in this context, given the impracticality\n// of refactoring ImportRequest/ImportValueRequest right now. The process\n// function exists because we would use either api.ImportValueWithTx or\n// api.ImportWithTx, passing it the actual underlying-type of req, but doing\n// that in here with a type switch seems messy. Similarly, index/field/shard\n// exist because we can't access those members of the two slightly different\n// structs.\nfunc (c *InternalClient) importHelper(ctx context.Context, req Message, process func() error, index string, field string, shard uint64, options *ImportOptions) error {\n\t// If we don't actually know what shards we're sending to, and we have\n\t// a local API and a qcx, we'll have a process function that uses the local\n\t// API. Otherwise, even if we have an API\n\tvar nodes []*disco.Node\n\tvar err error\n\tif shard != ^uint64(0) {\n\t\t// we need a list of nodes specific to this shard.\n\t\tnodes, err = c.FragmentNodes(ctx, index, shard)\n\t\tif err != nil {\n\t\t\treturn errors.Errorf(\"shard nodes: %s\", err)\n\t\t}\n\t} else {\n\t\t// We don't know what shard to use, any shard is fine, local host\n\t\t// is better if available.\n\t\tif process != nil {\n\t\t\t// skip the HTTP round-trip if we can.\n\t\t\terr = process()\n\t\t\t// Note that Wrap(nil, ...) is still nil.\n\t\t\treturn errors.Wrap(err, \"local import\")\n\t\t}\n\t\t// get the complete list of nodes, so if we have an API, we can\n\t\t// pick our local node and probably avoid actually sending the http\n\t\t// request over the wire, even though we still have to go through\n\t\t// the http interface.\n\t\tnodes, err = c.Nodes(ctx)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"getting nodes\")\n\t\t}\n\t}\n\n\t// \"us\" is a usable local node if any, \"them\" is every node that we need\n\t// to process which isn't that node. We start out with us == nil and\n\t// them = the whole set of nodes.\n\tvar us *disco.Node\n\tvar them []*disco.Node = nodes\n\n\t// If we have an API, we know what node we are. Even if we don't have\n\t// a Qcx, we still care, because looping back to the local node will\n\t// be faster than going to another node.\n\tif c.api != nil {\n\t\tmyID := c.api.NodeID()\n\t\tfor i, node := range nodes {\n\t\t\tif myID == node.ID {\n\t\t\t\t// swap our node into the first position\n\t\t\t\tnodes[i], nodes[0] = nodes[0], nodes[i]\n\t\t\t\t// If we have a qcx, we'll treat our node even MORE\n\t\t\t\t// specially.\n\t\t\t\tif process != nil {\n\t\t\t\t\tus, them = nodes[0], nodes[1:]\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\t// If we had a valid API and Qcx, and shard was ^0, we'd have handled\n\t// it previously. So if we get here, we don't have both a Qcx and an API,\n\t// but we might have an API, in which case we'll have shuffled our node\n\t// into the first position. Otherwise we're just taking whatever the first\n\t// node is.\n\tif shard == ^uint64(0) {\n\t\tthem = them[:1]\n\t}\n\n\t// We handle remote nodes first, for two distinct reasons. One is that\n\t// the local API ImportWithTx is allowed to modify its inputs, and if we\n\t// ran that before serializing, we'd get corrupt data serialized.\n\t// The other is that if we were to hold a write lock that started with\n\t// that import and ended when we hit the end of our Qcx, we wouldn't want\n\t// to hold it during all our requests to the remote nodes.\n\tif len(them) > 0 {\n\t\tbuf, err := c.serializer.Marshal(req)\n\t\tif err != nil {\n\t\t\treturn errors.Errorf(\"marshal import request: %s\", err)\n\t\t}\n\t\t// We process remote nodes first so we won't be actually holding our\n\t\t// write lock yet, in theory. This doesn't actually matter yet, but is\n\t\t// helpful for future planned refactoring.\n\t\tfor _, node := range them {\n\t\t\tif err = c.importNode(ctx, node, index, field, buf, options); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"remote import\")\n\t\t\t}\n\t\t}\n\t}\n\n\t// Write to the local node if we have one.\n\tif us != nil {\n\t\t// WARNING: ImportWithTx can alter its inputs. However, we can\n\t\t// only ever do this once, and if we're going to need a marshalled\n\t\t// form, we already made it.\n\t\tif err = process(); err != nil {\n\t\t\treturn errors.Wrap(err, \"local import after remote imports\")\n\t\t}\n\t}\n\treturn nil\n}\n\n// Import imports values using an ImportRequest, whether or not it's keyed.\n// It may modify the contents of req.\n//\n// If a request comes in with Shard -1, it will be sent to only one node,\n// which will translate if necessary, split into shards, and loop back\n// through this for each sub-request. If a request uses record keys,\n// it will be set to use shard = -1 unconditionally, because we know\n// that it has to be translated and possibly reshuffled. Value keys\n// don't override the shard.\n//\n// If we get a non-nil qcx, and have an associated API, we'll use that API\n// directly for the local shard.\nfunc (c *InternalClient) Import(ctx context.Context, qcx *Qcx, req *ImportRequest, options *ImportOptions) error {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.Import\")\n\tdefer span.Finish()\n\n\tif req.ColumnKeys != nil {\n\t\treq.Shard = ^uint64(0)\n\t}\n\tvar process func() error\n\tif c.api != nil && qcx != nil {\n\t\tprocess = func() error {\n\t\t\treturn c.api.ImportWithTx(ctx, qcx, req, options)\n\t\t}\n\t}\n\treturn c.importHelper(ctx, req, process, req.Index, req.Field, req.Shard, options)\n}\n\n// ImportValue imports values using an ImportValueRequest, whether or not it's\n// keyed. It may modify the contents of req.\n//\n// If a request comes in with Shard -1, it will be sent to only one node,\n// which will translate if necessary, split into shards, and loop back\n// through this for each sub-request. If a request uses record keys,\n// it will be set to use shard = -1 unconditionally, because we know\n// that it has to be translated and possibly reshuffled. Value keys\n// don't override the shard.\n//\n// If we get a non-nil qcx, and have an associated API, we'll use that API\n// directly for the local shard.\nfunc (c *InternalClient) ImportValue(ctx context.Context, qcx *Qcx, req *ImportValueRequest, options *ImportOptions) error {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.Import\")\n\tdefer span.Finish()\n\tif req.ColumnKeys != nil {\n\t\treq.Shard = ^uint64(0)\n\t}\n\tvar process func() error\n\tif c.api != nil && qcx != nil {\n\t\tprocess = func() error {\n\t\t\treturn c.api.ImportValueWithTx(ctx, qcx, req, options)\n\t\t}\n\t}\n\treturn c.importHelper(ctx, req, process, req.Index, req.Field, req.Shard, options)\n}\n\n// ImportRoaring does fast import of raw bits in roaring format (pilosa or\n// official format, see API.ImportRoaring).\nfunc (c *InternalClient) ImportRoaring(ctx context.Context, uri *pnet.URI, index, field string, shard uint64, remote bool, req *ImportRoaringRequest) error {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.ImportRoaring\")\n\tdefer span.Finish()\n\n\tif index == \"\" {\n\t\treturn ErrIndexRequired\n\t} else if field == \"\" {\n\t\treturn ErrFieldRequired\n\t}\n\tif uri == nil {\n\t\turi = c.defaultURI\n\t}\n\n\tvals := url.Values{}\n\tvals.Set(\"remote\", strconv.FormatBool(remote))\n\turl := fmt.Sprintf(\"%s%s/index/%s/field/%s/import-roaring/%d?%s\", uri, c.prefix(), index, field, shard, vals.Encode())\n\n\t// Marshal data to protobuf.\n\tdata, err := c.serializer.Marshal(req)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"marshal import request\")\n\t}\n\n\treturn c.executeProtobufRequest(ctx, url, data)\n}\n\n// ExportCSV bulk exports data for a single shard from a host to CSV format.\nfunc (c *InternalClient) ExportCSV(ctx context.Context, index, field string, shard uint64, w io.Writer) error {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.ExportCSV\")\n\tdefer span.Finish()\n\n\tif index == \"\" {\n\t\treturn ErrIndexRequired\n\t} else if field == \"\" {\n\t\treturn ErrFieldRequired\n\t}\n\n\t// Retrieve a list of nodes that own the shard.\n\tnodes, err := c.FragmentNodes(ctx, index, shard)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"shard nodes: %s\", err)\n\t}\n\n\t// Attempt nodes in random order.\n\tvar e error\n\tfor _, i := range rand.Perm(len(nodes)) {\n\t\tnode := nodes[i]\n\n\t\tif err := c.exportNodeCSV(ctx, node, index, field, shard, w); err != nil {\n\t\t\te = fmt.Errorf(\"export node: host=%s, err=%s\", node.URI, err)\n\t\t\tcontinue\n\t\t} else {\n\t\t\treturn nil\n\t\t}\n\t}\n\n\treturn e\n}\n\n// exportNode copies a CSV export from a node to w.\nfunc (c *InternalClient) exportNodeCSV(ctx context.Context, node *disco.Node, index, field string, shard uint64, w io.Writer) error {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.exportNodeCSV\")\n\tdefer span.Finish()\n\n\t// Create URL.\n\tu := nodePathToURL(node, fmt.Sprintf(\"%s/export\", c.prefix()))\n\tu.RawQuery = url.Values{\n\t\t\"index\": {index},\n\t\t\"field\": {field},\n\t\t\"shard\": {strconv.FormatUint(shard, 10)},\n\t}.Encode()\n\n\t// Generate HTTP request.\n\treq, err := http.NewRequest(\"GET\", u.String(), nil)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"creating request\")\n\t}\n\treq.Header.Set(\"Accept\", \"text/csv\")\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request against the host.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer resp.Body.Close()\n\n\t// Copy body to writer.\n\tif _, err := io.Copy(w, resp.Body); err != nil {\n\t\treturn errors.Wrap(err, \"copying\")\n\t}\n\n\treturn nil\n}\n\n// RetrieveShardFromURI returns a ReadCloser which contains the data of the\n// specified shard from the specified node. Caller *must* close the returned\n// ReadCloser or risk leaking goroutines/tcp connections.\nfunc (c *InternalClient) RetrieveShardFromURI(ctx context.Context, index, field, view string, shard uint64, uri pnet.URI) (io.ReadCloser, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.RetrieveShardFromURI\")\n\tdefer span.Finish()\n\n\tnode := &disco.Node{\n\t\tURI: uri,\n\t}\n\n\tu := nodePathToURL(node, fmt.Sprintf(\"%s/internal/fragment/data\", c.prefix()))\n\tu.RawQuery = url.Values{\n\t\t\"index\": {index},\n\t\t\"field\": {field},\n\t\t\"view\":  {view},\n\t\t\"shard\": {strconv.FormatUint(shard, 10)},\n\t}.Encode()\n\n\t// Build request.\n\treq, err := http.NewRequest(\"GET\", u.String(), nil)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\tif resp != nil && resp.StatusCode == http.StatusNotFound {\n\t\t\treturn nil, ErrFragmentNotFound\n\t\t}\n\t\treturn nil, err\n\t}\n\n\treturn resp.Body, nil\n}\n\nfunc (c *InternalClient) CreateField(ctx context.Context, index, field string) error {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.CreateField\")\n\tdefer span.Finish()\n\treturn c.CreateFieldWithOptions(ctx, index, field, FieldOptions{})\n}\n\n// CreateFieldWithOptions creates a new field on the server.\nfunc (c *InternalClient) CreateFieldWithOptions(ctx context.Context, index, field string, opt FieldOptions) error {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.CreateFieldWithOptions\")\n\tdefer span.Finish()\n\n\tif index == \"\" {\n\t\treturn ErrIndexRequired\n\t}\n\n\t// convert FieldOptions to fieldOptions\n\t//\n\t// TODO this kind of sucks because it's one more place that needs\n\t// changes when we change anything with field options (and there\n\t// are a lot of places already). It's not clear to me that this is\n\t// providing a lot of value, but I think this kind of validation\n\t// should probably happen in the field anyway??\n\tfieldOpt := fieldOptions{\n\t\tType: opt.Type,\n\t}\n\tswitch fieldOpt.Type {\n\tcase FieldTypeSet, FieldTypeMutex:\n\t\tfieldOpt.CacheType = &opt.CacheType\n\t\tfieldOpt.CacheSize = &opt.CacheSize\n\t\tfieldOpt.Keys = &opt.Keys\n\tcase FieldTypeInt:\n\t\tfieldOpt.Min = &opt.Min\n\t\tfieldOpt.Max = &opt.Max\n\tcase FieldTypeTime:\n\t\tfieldOpt.TimeQuantum = &opt.TimeQuantum\n\t\tttlString := opt.TTL.String()\n\t\tfieldOpt.TTL = &ttlString\n\tcase FieldTypeBool:\n\t\t// pass\n\tcase FieldTypeDecimal:\n\t\tfieldOpt.Min = &opt.Min\n\t\tfieldOpt.Max = &opt.Max\n\t\tfieldOpt.Scale = &opt.Scale\n\tdefault:\n\t\tfieldOpt.Type = DefaultFieldType\n\t\tfieldOpt.Keys = &opt.Keys\n\t}\n\n\t// TODO: remove buf completely? (depends on whether importer needs to create specific field types)\n\t// Encode query request.\n\tbuf, err := json.Marshal(&postFieldRequest{\n\t\tOptions: fieldOpt,\n\t})\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"marshaling\")\n\t}\n\n\t// Get the primary node. Schema changes must go through\n\t// primary to avoid weird race conditions.\n\tnodes, err := c.Nodes(ctx)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"getting nodes: %s\", err)\n\t}\n\tcoord := getPrimaryNode(nodes)\n\tif coord == nil {\n\t\treturn fmt.Errorf(\"could not find the primary node\")\n\t}\n\n\t// Create URL & HTTP request.\n\tu := uriPathToURL(&coord.URI, fmt.Sprintf(\"%s/index/%s/field/%s\", c.prefix(), index, field))\n\treq, err := http.NewRequest(\"POST\", u.String(), bytes.NewReader(buf))\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"creating request\")\n\t}\n\treq.Header.Set(\"Content-Length\", strconv.Itoa(len(buf)))\n\treq.Header.Set(\"Content-Type\", \"application/json\")\n\treq.Header.Set(\"Accept\", \"application/json\")\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request against the host.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\tif resp != nil && resp.StatusCode == http.StatusConflict {\n\t\t\treturn ErrFieldExists\n\t\t}\n\t\treturn err\n\t}\n\n\treturn errors.Wrap(resp.Body.Close(), \"closing response body\")\n}\n\n// SendMessage posts a message synchronously.\nfunc (c *InternalClient) SendMessage(ctx context.Context, uri *pnet.URI, msg []byte) error {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.SendMessage\")\n\tdefer span.Finish()\n\n\tu := uriPathToURL(uri, fmt.Sprintf(\"%s/internal/cluster/message\", c.prefix()))\n\treq, err := http.NewRequest(\"POST\", u.String(), bytes.NewReader(msg))\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"making new request\")\n\t}\n\n\treq.Header.Set(\"Content-Type\", \"application/x-protobuf\")\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\treq.Header.Set(\"Accept\", \"application/json\")\n\treq.Header.Set(\"Connection\", \"keep-alive\")\n\tif c.secretKey != \"\" {\n\t\treq.Header.Set(\"X-Feature-Key\", c.secretKey)\n\t}\n\n\t// Execute request.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"executing request\")\n\t}\n\tdefer resp.Body.Close()\n\t_, err = io.Copy(io.Discard, resp.Body)\n\treturn errors.Wrap(err, \"draining SendMessage response body\")\n}\n\n// TranslateKeysNode function is mainly called to translate keys from primary node.\n// If primary node returns 404 error the function wraps it with ErrTranslatingKeyNotFound.\nfunc (c *InternalClient) TranslateKeysNode(ctx context.Context, uri *pnet.URI, index, field string, keys []string, writable bool) ([]uint64, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"TranslateKeysNode\")\n\tdefer span.Finish()\n\n\tif index == \"\" {\n\t\treturn nil, ErrIndexRequired\n\t}\n\n\tbuf, err := c.serializer.Marshal(&TranslateKeysRequest{\n\t\tIndex:       index,\n\t\tField:       field,\n\t\tKeys:        keys,\n\t\tNotWritable: !writable,\n\t})\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"marshaling TranslateKeysRequest\")\n\t}\n\n\t// Create HTTP request.\n\tu := uri.Path(fmt.Sprintf(\"%s/internal/translate/keys\", c.prefix()))\n\treq, err := http.NewRequest(\"POST\", u, bytes.NewReader(buf))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\treq.Header.Set(\"Content-Length\", strconv.Itoa(len(buf)))\n\treq.Header.Set(\"Content-Type\", \"application/x-protobuf\")\n\treq.Header.Set(\"Accept\", \"application/x-protobuf\")\n\treq.Header.Set(\"X-Pilosa-Row\", \"roaring\")\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request against the host.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\tif resp != nil && resp.StatusCode == http.StatusNotFound {\n\t\t\treturn nil, errors.Wrap(ErrTranslatingKeyNotFound, err.Error())\n\t\t}\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\n\t// Read body and unmarshal response.\n\tbody, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"reading\")\n\t}\n\n\ttkresp := &TranslateKeysResponse{}\n\tif err := c.serializer.Unmarshal(body, tkresp); err != nil {\n\t\treturn nil, fmt.Errorf(\"unmarshal response: %s\", err)\n\t}\n\treturn tkresp.IDs, nil\n}\n\n// TranslateIDsNode sends an id translation request to a specific node.\nfunc (c *InternalClient) TranslateIDsNode(ctx context.Context, uri *pnet.URI, index, field string, ids []uint64) ([]string, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"TranslateIDsNode\")\n\tdefer span.Finish()\n\n\tif index == \"\" {\n\t\treturn nil, ErrIndexRequired\n\t}\n\n\tbuf, err := c.serializer.Marshal(&TranslateIDsRequest{\n\t\tIndex: index,\n\t\tField: field,\n\t\tIDs:   ids,\n\t})\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"marshaling TranslateIDsRequest\")\n\t}\n\n\t// Create HTTP request.\n\tu := uri.Path(fmt.Sprintf(\"%s/internal/translate/ids\", c.prefix()))\n\treq, err := http.NewRequest(\"POST\", u, bytes.NewReader(buf))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\treq.Header.Set(\"Content-Length\", strconv.Itoa(len(buf)))\n\treq.Header.Set(\"Content-Type\", \"application/x-protobuf\")\n\treq.Header.Set(\"Accept\", \"application/x-protobuf\")\n\treq.Header.Set(\"X-Pilosa-Row\", \"roaring\")\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request against the host.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\n\t// Read body and unmarshal response.\n\tbody, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"reading\")\n\t}\n\n\ttkresp := &TranslateIDsResponse{}\n\tif err := c.serializer.Unmarshal(body, tkresp); err != nil {\n\t\treturn nil, fmt.Errorf(\"unmarshal response: %s\", err)\n\t}\n\treturn tkresp.Keys, nil\n}\n\n// GetPastQueries retrieves the query history log for the specified node.\nfunc (c *InternalClient) GetPastQueries(ctx context.Context, uri *pnet.URI) ([]PastQueryStatus, error) {\n\tu := uri.Path(fmt.Sprintf(\"%s/query-history?remote=true\", c.prefix()))\n\treq, err := http.NewRequest(\"GET\", u, nil)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\treq.Header.Set(\"Accept\", \"application/json\")\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request against the host.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\n\t// Read body and unmarshal response.\n\tbody, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"reading\")\n\t}\n\n\tqueries := make([]PastQueryStatus, 100)\n\tif err := json.Unmarshal(body, &queries); err != nil {\n\t\treturn nil, fmt.Errorf(\"unmarshal response: %s\", err)\n\t}\n\treturn queries, nil\n}\n\nfunc (c *InternalClient) FindIndexKeysNode(ctx context.Context, uri *pnet.URI, index string, keys ...string) (transMap map[string]uint64, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.FindIndexKeysNode\")\n\tdefer span.Finish()\n\n\t// Create HTTP request.\n\tu := uriPathToURL(uri, fmt.Sprintf(\"%s/internal/translate/index/%s/keys/find\", c.prefix(), index))\n\treqData, err := json.Marshal(keys)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"marshalling request\")\n\t}\n\treq, err := http.NewRequest(\"POST\", u.String(), bytes.NewReader(reqData))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\t// Apply headers.\n\treq.Header.Set(\"Content-Length\", strconv.Itoa(len(reqData)))\n\treq.Header.Set(\"Content-Type\", \"application/json\")\n\treq.Header.Set(\"Accept\", \"application/json\")\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Send the request.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"executing request\")\n\t}\n\tdefer func() {\n\t\tcerr := resp.Body.Close()\n\t\tif cerr != nil && err == nil {\n\t\t\terr = errors.Wrap(cerr, \"closing response body\")\n\t\t}\n\t}()\n\n\t// Read the response body.\n\tresult, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"reading response\")\n\t}\n\n\t// Decode the translations.\n\ttransMap = make(map[string]uint64, len(keys))\n\terr = json.Unmarshal(result, &transMap)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"json decoding\")\n\t}\n\n\treturn transMap, nil\n}\n\nfunc (c *InternalClient) FindFieldKeysNode(ctx context.Context, uri *pnet.URI, index string, field string, keys ...string) (transMap map[string]uint64, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.FindFieldKeysNode\")\n\tdefer span.Finish()\n\n\t// Create HTTP request.\n\tu := uriPathToURL(uri, fmt.Sprintf(\"%s/internal/translate/field/%s/%s/keys/find\", c.prefix(), index, field))\n\tq := u.Query()\n\tq.Add(\"remote\", \"true\")\n\tu.RawQuery = q.Encode()\n\treqData, err := json.Marshal(keys)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"marshalling request\")\n\t}\n\treq, err := http.NewRequest(\"POST\", u.String(), bytes.NewReader(reqData))\n\t// Apply headers.\n\treq.Header.Set(\"Content-Length\", strconv.Itoa(len(reqData)))\n\treq.Header.Set(\"Content-Type\", \"application/json\")\n\treq.Header.Set(\"Accept\", \"application/json\")\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Send the request.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"executing request\")\n\t}\n\tdefer func() {\n\t\tcerr := resp.Body.Close()\n\t\tif cerr != nil && err == nil {\n\t\t\terr = errors.Wrap(cerr, \"closing response body\")\n\t\t}\n\t}()\n\n\t// Read the response body.\n\tresult, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"reading response\")\n\t}\n\n\t// Decode the translations.\n\ttransMap = make(map[string]uint64, len(keys))\n\terr = json.Unmarshal(result, &transMap)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"json decoding\")\n\t}\n\n\treturn transMap, nil\n}\n\nfunc (c *InternalClient) CreateIndexKeysNode(ctx context.Context, uri *pnet.URI, index string, keys ...string) (transMap map[string]uint64, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.CreateIndexKeysNode\")\n\tdefer span.Finish()\n\n\t// Create HTTP request.\n\tu := uriPathToURL(uri, fmt.Sprintf(\"%s/internal/translate/index/%s/keys/create\", c.prefix(), index))\n\treqData, err := json.Marshal(keys)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"marshalling request\")\n\t}\n\treq, err := http.NewRequest(\"POST\", u.String(), bytes.NewReader(reqData))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\t// Apply headers.\n\treq.Header.Set(\"Content-Length\", strconv.Itoa(len(reqData)))\n\treq.Header.Set(\"Content-Type\", \"application/json\")\n\treq.Header.Set(\"Accept\", \"application/json\")\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Send the request.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"executing request\")\n\t}\n\tdefer func() {\n\t\tcerr := resp.Body.Close()\n\t\tif cerr != nil && err == nil {\n\t\t\terr = errors.Wrap(cerr, \"closing response body\")\n\t\t}\n\t}()\n\n\t// Read the response body.\n\tresult, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"reading response\")\n\t}\n\n\t// Decode the translations.\n\ttransMap = make(map[string]uint64, len(keys))\n\terr = json.Unmarshal(result, &transMap)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"json decoding\")\n\t}\n\n\treturn transMap, nil\n}\n\nfunc (c *InternalClient) CreateFieldKeysNode(ctx context.Context, uri *pnet.URI, index string, field string, keys ...string) (transMap map[string]uint64, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.CreateFieldKeysNode\")\n\tdefer span.Finish()\n\n\t// Create HTTP request.\n\tu := uriPathToURL(uri, fmt.Sprintf(\"%s/internal/translate/field/%s/%s/keys/create\", c.prefix(), index, field))\n\tq := u.Query()\n\tq.Add(\"remote\", \"true\")\n\tu.RawQuery = q.Encode()\n\treqData, err := json.Marshal(keys)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"marshalling request\")\n\t}\n\treq, err := http.NewRequest(\"POST\", u.String(), bytes.NewReader(reqData))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\t// Apply headers.\n\treq.Header.Set(\"Content-Length\", strconv.Itoa(len(reqData)))\n\treq.Header.Set(\"Content-Type\", \"application/json\")\n\treq.Header.Set(\"Accept\", \"application/json\")\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Send the request.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"executing request\")\n\t}\n\tdefer func() {\n\t\tcerr := resp.Body.Close()\n\t\tif cerr != nil && err == nil {\n\t\t\terr = errors.Wrap(cerr, \"closing response body\")\n\t\t}\n\t}()\n\n\t// Read the response body.\n\tresult, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"reading response\")\n\t}\n\n\t// Decode the translations.\n\ttransMap = make(map[string]uint64, len(keys))\n\terr = json.Unmarshal(result, &transMap)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"json decoding\")\n\t}\n\n\treturn transMap, nil\n}\n\nfunc (c *InternalClient) MatchFieldKeysNode(ctx context.Context, uri *pnet.URI, index string, field string, like string) (matches []uint64, err error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.MatchFieldKeysNode\")\n\tdefer span.Finish()\n\n\t// Create HTTP request.\n\tu := uriPathToURL(uri, fmt.Sprintf(\"%s/internal/translate/field/%s/%s/keys/like\", c.prefix(), index, field))\n\treq, err := http.NewRequest(\"POST\", u.String(), strings.NewReader(like))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\t// Apply headers.\n\treq.Header.Set(\"Content-Length\", strconv.Itoa(len(like)))\n\treq.Header.Set(\"Accept\", \"application/json\")\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Send the request.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"executing request\")\n\t}\n\tdefer func() {\n\t\tcerr := resp.Body.Close()\n\t\tif cerr != nil && err == nil {\n\t\t\terr = errors.Wrap(cerr, \"closing response body\")\n\t\t}\n\t}()\n\n\t// Read the response body.\n\tresult, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"reading response\")\n\t}\n\n\t// Decode the translations.\n\terr = json.Unmarshal(result, &matches)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"json decoding\")\n\t}\n\n\treturn matches, nil\n}\n\nfunc (c *InternalClient) Transactions(ctx context.Context) (map[string]*Transaction, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.Transactions\")\n\tdefer span.Finish()\n\n\tu := uriPathToURL(c.defaultURI, fmt.Sprintf(\"%s/transactions\", c.prefix()))\n\treq, err := http.NewRequest(\"GET\", u.String(), nil)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating transactions request\")\n\t}\n\treq.Header.Set(\"Accept\", \"application/json\")\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\tAddAuthToken(ctx, &req.Header)\n\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"executing request\")\n\t}\n\tdefer func() {\n\t\t_, _ = io.Copy(io.Discard, resp.Body)\n\t\t_ = resp.Body.Close()\n\t}()\n\ttrnsMap := make(map[string]*Transaction)\n\terr = json.NewDecoder(resp.Body).Decode(&trnsMap)\n\treturn trnsMap, errors.Wrap(err, \"json decoding\")\n}\n\nfunc (c *InternalClient) StartTransaction(ctx context.Context, id string, timeout time.Duration, exclusive bool) (*Transaction, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.StartTransaction\")\n\tdefer span.Finish()\n\tbuf, err := json.Marshal(&Transaction{\n\t\tID:        id,\n\t\tTimeout:   timeout,\n\t\tExclusive: exclusive,\n\t})\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"marshalling payload\")\n\t}\n\t// We're using the defaultURI here because this is only used by\n\t// tests, and we want to test requests against all hosts. A robust\n\t// client implementation would ensure that these requests go to\n\t// the primary.\n\tu := uriPathToURL(c.defaultURI, fmt.Sprintf(\"%s/transaction/%s\", c.prefix(), id))\n\treq, err := http.NewRequest(\"POST\", u.String(), bytes.NewReader(buf))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating post transaction request\")\n\t}\n\treq.Header.Set(\"Content-Length\", strconv.Itoa(len(buf)))\n\treq.Header.Set(\"Content-Type\", \"application/json\")\n\treq.Header.Set(\"Accept\", \"application/json\")\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\tAddAuthToken(ctx, &req.Header)\n\n\tresp, err := c.executeRequest(req.WithContext(ctx), giveRawResponse(true))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"executing request\")\n\t}\n\tdefer func() {\n\t\t_, _ = io.Copy(io.Discard, resp.Body)\n\t\t_ = resp.Body.Close()\n\t}()\n\ttr := &TransactionResponse{}\n\terr = json.NewDecoder(resp.Body).Decode(tr)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"decoding response\")\n\t}\n\tif resp.StatusCode == 409 {\n\t\terr = ErrTransactionExclusive\n\t} else if tr.Error != \"\" {\n\t\terr = errors.New(tr.Error)\n\t}\n\treturn tr.Transaction, err\n}\n\nfunc (c *InternalClient) FinishTransaction(ctx context.Context, id string) (*Transaction, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.FinishTransaction\")\n\tdefer span.Finish()\n\n\tu := uriPathToURL(c.defaultURI, fmt.Sprintf(\"%s/transaction/%s/finish\", c.prefix(), id))\n\treq, err := http.NewRequest(\"POST\", u.String(), nil)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating finish transaction request\")\n\t}\n\n\treq.Header.Set(\"Accept\", \"application/json\")\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\tAddAuthToken(ctx, &req.Header)\n\n\tresp, err := c.executeRequest(req.WithContext(ctx), giveRawResponse(true))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"executing request\")\n\t}\n\tdefer func() {\n\t\t_, _ = io.Copy(io.Discard, resp.Body)\n\t\t_ = resp.Body.Close()\n\t}()\n\ttr := &TransactionResponse{}\n\terr = json.NewDecoder(resp.Body).Decode(tr)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"decoding response\")\n\t}\n\n\tif tr.Error != \"\" {\n\t\terr = errors.New(tr.Error)\n\t}\n\treturn tr.Transaction, err\n}\n\nfunc (c *InternalClient) GetTransaction(ctx context.Context, id string) (*Transaction, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.GetTransaction\")\n\tdefer span.Finish()\n\n\t// We're using the defaultURI here because this is only used by\n\t// tests, and we want to test requests against all hosts. A robust\n\t// client implementation would ensure that these requests go to\n\t// the primary.\n\tu := uriPathToURL(c.defaultURI, fmt.Sprintf(\"%s/transaction/%s\", c.prefix(), id))\n\treq, err := http.NewRequest(\"GET\", u.String(), nil)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating get transaction request\")\n\t}\n\treq.Header.Set(\"Accept\", \"application/json\")\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\tAddAuthToken(ctx, &req.Header)\n\n\tresp, err := c.executeRequest(req.WithContext(ctx), giveRawResponse(true))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"executing request\")\n\t}\n\tdefer func() {\n\t\t_, _ = io.Copy(io.Discard, resp.Body)\n\t\t_ = resp.Body.Close()\n\t}()\n\ttr := &TransactionResponse{}\n\terr = json.NewDecoder(resp.Body).Decode(tr)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"decoding response\")\n\t}\n\n\tif tr.Error != \"\" {\n\t\terr = errors.New(tr.Error)\n\t}\n\treturn tr.Transaction, err\n}\n\nfunc (c *InternalClient) GetDataframeShard(ctx context.Context, index string, shard uint64) (*http.Response, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.GetDataframeShard\")\n\tdefer span.Finish()\n\n\t// Execute request against the host.\n\tpath := fmt.Sprintf(\"/index/%v/dataframe/%04d\", index, shard)\n\t// Build request.\n\turl := uriPathToURL(c.defaultURI, path)\n\treq, err := http.NewRequest(\"GET\", url.String(), nil)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\treq.Header.Set(\"Accept\", \"application/octet-stream\")\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request.\n\treturn c.executeRequest(req.WithContext(ctx), giveRawResponse(true))\n}\n\ntype executeOpts struct {\n\t// giveRawResponse instructs executeRequest not to process the\n\t// respStatusCode and try to extract errors or whatever.\n\tgiveRawResponse bool\n\t// forwardAuthHeader instructs executeRequest not to follow redirects\n\tforwardAuthHeader bool\n}\n\ntype executeRequestOption func(*executeOpts)\n\nfunc giveRawResponse(b bool) executeRequestOption {\n\treturn func(eo *executeOpts) {\n\t\teo.giveRawResponse = b\n\t}\n}\n\nfunc forwardAuthHeader(b bool) executeRequestOption {\n\treturn func(eo *executeOpts) {\n\t\teo.forwardAuthHeader = b\n\t}\n}\n\n// executeRequest executes the given request and checks the Response. For\n// responses with non-2XX status, the body is read and closed, and an error is\n// returned. If the error is nil, the caller must ensure that the response body\n// is closed.\nfunc (c *InternalClient) executeRequest(req *http.Request, opts ...executeRequestOption) (*http.Response, error) {\n\treturn c.executeRetryableRequest(&retryablehttp.Request{Request: req}, opts...)\n}\n\nfunc (c *InternalClient) executeRetryableRequest(req *retryablehttp.Request, opts ...executeRequestOption) (*http.Response, error) {\n\ttracing.GlobalTracer.InjectHTTPHeaders(req.Request)\n\treq.Close = false\n\teo := &executeOpts{}\n\tfor _, opt := range opts {\n\t\topt(eo)\n\t}\n\n\t// Wrap any existing context with a context with an associated error-tracker\n\terrs, ctx := newStatusTrackingContext(req.Context())\n\treq = req.WithContext(ctx)\n\n\tvar resp *http.Response\n\tvar err error\n\tif eo.forwardAuthHeader {\n\t\t// use the shared retryableClient that uses CheckRedirect to fixup auth\n\t\tresp, err = c.authRetryableClient.Do(req)\n\t} else {\n\t\t// use the existing shared retryableClient\n\t\tresp, err = c.retryableClient.Do(req)\n\t}\n\n\tif len(errs.errs) > 0 {\n\t\tvar logfn func(string, ...interface{})\n\t\tif err != nil {\n\t\t\tlogfn = c.log.Errorf\n\t\t} else {\n\t\t\tlogfn = c.log.Infof\n\t\t}\n\n\t\tlogfn(\"executeRetryableRequest: %d retried errors:\", len(errs.errs))\n\t\tfor _, e := range errs.errs {\n\t\t\tif e.err != nil {\n\t\t\t\t// display error if the request reported an error\n\t\t\t\tlogfn(\"  %d: %v\", e.statusCode, e.err)\n\t\t\t} else {\n\t\t\t\t// attempt to display message body or some part thereof\n\t\t\t\tlogfn(\"  %d: %q\", e.statusCode, e.msg)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn c.handleResponse(req.Request, eo, resp, err)\n}\n\nfunc (c *InternalClient) handleResponse(req *http.Request, eo *executeOpts, resp *http.Response, err error) (*http.Response, error) {\n\tif err != nil {\n\t\tif resp != nil {\n\t\t\tresp.Body.Close()\n\t\t}\n\t\treturn nil, errors.Wrap(err, \"getting response\")\n\t}\n\tif eo.giveRawResponse {\n\t\treturn resp, nil\n\t}\n\n\tif resp.StatusCode < 200 || resp.StatusCode >= 300 {\n\t\tdefer resp.Body.Close()\n\t\tbuf, err := io.ReadAll(resp.Body)\n\t\tif err != nil {\n\t\t\treturn resp, errors.Wrapf(err, \"bad status '%s' and err reading body\", resp.Status)\n\t\t}\n\t\tvar msg string\n\t\t// try to decode a JSON response\n\t\tvar sr successResponse\n\t\tqr := &QueryResponse{}\n\t\tif err = json.Unmarshal(buf, &sr); err == nil {\n\t\t\tmsg = sr.Error.Error()\n\t\t} else if err := c.serializer.Unmarshal(buf, qr); err == nil {\n\t\t\tmsg = qr.Err.Error()\n\t\t} else {\n\t\t\tmsg = string(buf)\n\t\t}\n\t\treturn resp, errors.Errorf(\"against %s %s: '%s'\", req.URL.String(), resp.Status, msg)\n\t}\n\treturn resp, nil\n}\n\n// Bit represents the intersection of a row and a column. It can be specified by\n// integer ids or string keys.\ntype Bit struct {\n\tRowID     uint64\n\tColumnID  uint64\n\tRowKey    string\n\tColumnKey string\n\tTimestamp int64\n}\n\n// Bits is a slice of Bit.\ntype Bits []Bit\n\nfunc (p Bits) Swap(i, j int) { p[i], p[j] = p[j], p[i] }\nfunc (p Bits) Len() int      { return len(p) }\n\nfunc (p Bits) Less(i, j int) bool {\n\tif p[i].RowID == p[j].RowID {\n\t\tif p[i].ColumnID < p[j].ColumnID {\n\t\t\treturn p[i].Timestamp < p[j].Timestamp\n\t\t}\n\t\treturn p[i].ColumnID < p[j].ColumnID\n\t}\n\treturn p[i].RowID < p[j].RowID\n}\n\n// HasRowKeys returns true if any values use a row key.\nfunc (p Bits) HasRowKeys() bool {\n\tfor i := range p {\n\t\tif p[i].RowKey != \"\" {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// HasColumnKeys returns true if any values use a column key.\nfunc (p Bits) HasColumnKeys() bool {\n\tfor i := range p {\n\t\tif p[i].ColumnKey != \"\" {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// RowIDs returns a slice of all the row IDs.\nfunc (p Bits) RowIDs() []uint64 {\n\tif p.HasRowKeys() {\n\t\treturn nil\n\t}\n\tother := make([]uint64, len(p))\n\tfor i := range p {\n\t\tother[i] = p[i].RowID\n\t}\n\treturn other\n}\n\n// ColumnIDs returns a slice of all the column IDs.\nfunc (p Bits) ColumnIDs() []uint64 {\n\tif p.HasColumnKeys() {\n\t\treturn nil\n\t}\n\tother := make([]uint64, len(p))\n\tfor i := range p {\n\t\tother[i] = p[i].ColumnID\n\t}\n\treturn other\n}\n\n// RowKeys returns a slice of all the row keys.\nfunc (p Bits) RowKeys() []string {\n\tif !p.HasRowKeys() {\n\t\treturn nil\n\t}\n\tother := make([]string, len(p))\n\tfor i := range p {\n\t\tother[i] = p[i].RowKey\n\t}\n\treturn other\n}\n\n// ColumnKeys returns a slice of all the column keys.\nfunc (p Bits) ColumnKeys() []string {\n\tif !p.HasColumnKeys() {\n\t\treturn nil\n\t}\n\tother := make([]string, len(p))\n\tfor i := range p {\n\t\tother[i] = p[i].ColumnKey\n\t}\n\treturn other\n}\n\n// Timestamps returns a slice of all the timestamps.\nfunc (p Bits) Timestamps() []int64 {\n\tother := make([]int64, len(p))\n\tfor i := range p {\n\t\tother[i] = p[i].Timestamp\n\t}\n\treturn other\n}\n\n// GroupByShard returns a map of bits by shard.\nfunc (p Bits) GroupByShard() map[uint64][]Bit {\n\tm := make(map[uint64][]Bit)\n\tfor _, bit := range p {\n\t\tshard := bit.ColumnID / ShardWidth\n\t\tm[shard] = append(m[shard], bit)\n\t}\n\n\tfor shard, bits := range m {\n\t\tsort.Sort(Bits(bits))\n\t\tm[shard] = bits\n\t}\n\n\treturn m\n}\n\n// FieldValue represents the value for a column within a\n// range-encoded field.\ntype FieldValue struct {\n\tColumnID  uint64\n\tColumnKey string\n\tValue     int64\n}\n\n// FieldValues represents a slice of field values.\ntype FieldValues []FieldValue\n\nfunc (p FieldValues) Swap(i, j int) { p[i], p[j] = p[j], p[i] }\nfunc (p FieldValues) Len() int      { return len(p) }\n\nfunc (p FieldValues) Less(i, j int) bool {\n\treturn p[i].ColumnID < p[j].ColumnID\n}\n\n// HasColumnKeys returns true if any values use a column key.\nfunc (p FieldValues) HasColumnKeys() bool {\n\tfor i := range p {\n\t\tif p[i].ColumnKey != \"\" {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// ColumnIDs returns a slice of all the column IDs.\nfunc (p FieldValues) ColumnIDs() []uint64 {\n\tif p.HasColumnKeys() {\n\t\treturn nil\n\t}\n\tother := make([]uint64, len(p))\n\tfor i := range p {\n\t\tother[i] = p[i].ColumnID\n\t}\n\treturn other\n}\n\n// ColumnKeys returns a slice of all the column keys.\nfunc (p FieldValues) ColumnKeys() []string {\n\tif !p.HasColumnKeys() {\n\t\treturn nil\n\t}\n\tother := make([]string, len(p))\n\tfor i := range p {\n\t\tother[i] = p[i].ColumnKey\n\t}\n\treturn other\n}\n\n// Values returns a slice of all the values.\nfunc (p FieldValues) Values() []int64 {\n\tother := make([]int64, len(p))\n\tfor i := range p {\n\t\tother[i] = p[i].Value\n\t}\n\treturn other\n}\n\n// GroupByShard returns a map of field values by shard.\nfunc (p FieldValues) GroupByShard() map[uint64][]FieldValue {\n\tm := make(map[uint64][]FieldValue)\n\tfor _, val := range p {\n\t\tshard := val.ColumnID / ShardWidth\n\t\tm[shard] = append(m[shard], val)\n\t}\n\n\tfor shard, vals := range m {\n\t\tsort.Sort(FieldValues(vals))\n\t\tm[shard] = vals\n\t}\n\n\treturn m\n}\n\n// BitsByPos is a slice of bits sorted row then column.\ntype BitsByPos []Bit\n\nfunc (p BitsByPos) Swap(i, j int) { p[i], p[j] = p[j], p[i] }\nfunc (p BitsByPos) Len() int      { return len(p) }\nfunc (p BitsByPos) Less(i, j int) bool {\n\tp0, p1 := pos(p[i].RowID, p[i].ColumnID), pos(p[j].RowID, p[j].ColumnID)\n\tif p0 == p1 {\n\t\treturn p[i].Timestamp < p[j].Timestamp\n\t}\n\treturn p0 < p1\n}\n\nfunc uriPathToURL(uri *pnet.URI, path string) url.URL {\n\treturn url.URL{\n\t\tScheme: uri.Scheme,\n\t\tHost:   uri.HostPort(),\n\t\tPath:   path,\n\t}\n}\n\nfunc nodePathToURL(node *disco.Node, path string) url.URL {\n\treturn url.URL{\n\t\tScheme: node.URI.Scheme,\n\t\tHost:   node.URI.HostPort(),\n\t\tPath:   path,\n\t}\n}\n\n// RetrieveTranslatePartitionFromURI returns a ReadCloser which contains the data of the\n// specified translate partition from the specified node. Caller *must* close the returned\n// ReadCloser or risk leaking goroutines/tcp connections.\nfunc (c *InternalClient) RetrieveTranslatePartitionFromURI(ctx context.Context, index string, partition int, uri pnet.URI) (io.ReadCloser, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.RetrieveTranslatePartitionFromURI\")\n\tdefer span.Finish()\n\n\tnode := &disco.Node{\n\t\tURI: uri,\n\t}\n\n\tu := nodePathToURL(node, fmt.Sprintf(\"%s/internal/translate/data\", c.prefix()))\n\tu.RawQuery = url.Values{\n\t\t\"index\":     {index},\n\t\t\"partition\": {strconv.FormatInt(int64(partition), 10)},\n\t}.Encode()\n\n\t// Build request.\n\treq, err := http.NewRequest(\"GET\", u.String(), nil)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\tif resp != nil && resp.StatusCode == http.StatusNotFound {\n\t\t\treturn nil, ErrFragmentNotFound\n\t\t}\n\t\treturn nil, err\n\t}\n\n\treturn resp.Body, nil\n}\n\nfunc (c *InternalClient) ImportIndexKeys(ctx context.Context, uri *pnet.URI, index string, partitionID int, remote bool, readerFunc func() (io.Reader, error)) error {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.ImportIndexKeys\")\n\tdefer span.Finish()\n\n\tif index == \"\" {\n\t\treturn ErrIndexRequired\n\t}\n\n\tif uri == nil {\n\t\turi = c.defaultURI\n\t}\n\n\tvals := url.Values{}\n\tvals.Set(\"remote\", strconv.FormatBool(remote))\n\turl := fmt.Sprintf(\"%s%s/internal/translate/index/%s/%d\", uri, c.prefix(), index, partitionID)\n\n\t// Generate HTTP request.\n\thttpReq, err := retryablehttp.NewRequest(\"POST\", url, readerFunc)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"creating request\")\n\t}\n\thttpReq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\n\tAddAuthToken(ctx, &httpReq.Header)\n\n\t// Execute request against the host.\n\tresp, err := c.executeRetryableRequest(httpReq.WithContext(ctx))\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer resp.Body.Close()\n\treturn nil\n}\n\nfunc (c *InternalClient) ImportFieldKeys(ctx context.Context, uri *pnet.URI, index, field string, remote bool, readerFunc func() (io.Reader, error)) error {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.ImportFieldKeys\")\n\tdefer span.Finish()\n\n\tif index == \"\" {\n\t\treturn ErrIndexRequired\n\t}\n\n\tif uri == nil {\n\t\turi = c.defaultURI\n\t}\n\n\tvals := url.Values{}\n\tvals.Set(\"remote\", strconv.FormatBool(remote))\n\turl := fmt.Sprintf(\"%s%s/internal/translate/field/%s/%s\", uri, c.prefix(), index, field)\n\n\t// Generate HTTP request.\n\thttpReq, err := retryablehttp.NewRequest(\"POST\", url, readerFunc)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"creating request\")\n\t}\n\thttpReq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\n\tAddAuthToken(ctx, &httpReq.Header)\n\n\t// Execute request against the host.\n\tresp, err := c.executeRetryableRequest(httpReq.WithContext(ctx))\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer resp.Body.Close()\n\treturn nil\n}\n\n// ShardReader returns a reader that provides a snapshot of the current shard RBF data.\nfunc (c *InternalClient) ShardReader(ctx context.Context, index string, shard uint64) (io.ReadCloser, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.ShardReader\")\n\tdefer span.Finish()\n\n\t// Execute request against the host.\n\tu := fmt.Sprintf(\"%s%s/internal/index/%s/shard/%d/snapshot\", c.defaultURI, c.prefix(), index, shard)\n\n\t// Build request.\n\treq, err := http.NewRequest(\"GET\", u, nil)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\treq.Header.Set(\"Accept\", \"application/octet-stream\")\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp.Body, nil\n}\n\n// IDAllocDataReader returns a reader that provides a snapshot of ID allocation data.\nfunc (c *InternalClient) IDAllocDataReader(ctx context.Context) (io.ReadCloser, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.IDAllocDataReader\")\n\tdefer span.Finish()\n\n\t// Build request.\n\turi := fmt.Sprintf(\"%s%s/internal/idalloc/data\", c.defaultURI, c.prefix())\n\treq, err := http.NewRequest(\"GET\", uri, nil)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\treq.Header.Set(\"Accept\", \"application/octet-stream\")\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp.Body, nil\n}\n\nfunc (c *InternalClient) IDAllocDataWriter(ctx context.Context, f io.Reader, primary *disco.Node) error {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.IDAllocDataWriter\")\n\tdefer span.Finish()\n\n\tu := primary.URI.Path(fmt.Sprintf(\"%s/internal/idalloc/restore\", c.prefix()))\n\n\t// Build request.\n\treq, err := http.NewRequest(\"POST\", u, f)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"creating request\")\n\t}\n\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\treq.Header.Set(\"Accept\", \"application/octet-stream\")\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request.\n\t_, err = c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn err\n}\n\n// IndexTranslateDataReader returns a reader that provides a snapshot of\n// translation data for a partition in an index.\nfunc (c *InternalClient) IndexTranslateDataReader(ctx context.Context, index string, partitionID int) (io.ReadCloser, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.IndexTranslateDataReader\")\n\tdefer span.Finish()\n\n\t// Execute request against the host.\n\tu := fmt.Sprintf(\"%s%s/internal/translate/data?index=%s&partition=%d\", c.defaultURI, c.prefix(), url.QueryEscape(index), partitionID)\n\n\t// Build request.\n\treq, err := http.NewRequest(\"GET\", u, nil)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\treq.Header.Set(\"Accept\", \"application/octet-stream\")\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request.\n\tresp, err := c.executeRequest(req.WithContext(ctx), forwardAuthHeader(true))\n\tif resp != nil && resp.StatusCode == http.StatusNotFound {\n\t\tresp.Body.Close()\n\t\treturn nil, ErrTranslateStoreNotFound\n\t} else if err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp.Body, nil\n}\n\n// FieldTranslateDataReader returns a reader that provides a snapshot of\n// translation data for a field.\nfunc (c *InternalClient) FieldTranslateDataReader(ctx context.Context, index, field string) (io.ReadCloser, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.FieldTranslateDataReader\")\n\tdefer span.Finish()\n\tnodes, err := c.Nodes(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tprimary := getPrimaryNode(nodes)\n\tif primary == nil {\n\t\treturn nil, errors.New(\"no primary\")\n\t}\n\t// Execute request against the host.\n\tu := fmt.Sprintf(\"%s%s/internal/translate/data?index=%s&field=%s\", primary.URI, c.prefix(), url.QueryEscape(index), url.QueryEscape(field))\n\n\t// Build request.\n\treq, err := http.NewRequest(\"GET\", u, nil)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\treq.Header.Set(\"Accept\", \"application/octet-stream\")\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif resp != nil && resp.StatusCode == http.StatusNotFound {\n\t\tresp.Body.Close()\n\t\treturn nil, ErrTranslateStoreNotFound\n\t} else if err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp.Body, nil\n}\n\n// Status returns pilosa cluster state as a string (\"NORMAL\", \"DEGRADED\", \"DOWN\", ...)\nfunc (c *InternalClient) Status(ctx context.Context) (string, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.Status\")\n\tdefer span.Finish()\n\n\t// Execute request against the host.\n\tu := c.defaultURI.Path(fmt.Sprintf(\"%s/status\", c.prefix()))\n\n\t// Build request.\n\treq, err := http.NewRequest(\"GET\", u, nil)\n\tif err != nil {\n\t\treturn \"\", errors.Wrap(err, \"creating request\")\n\t}\n\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\treq.Header.Set(\"Accept\", \"application/json\")\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tdefer resp.Body.Close()\n\n\tvar rsp getStatusResponse\n\tif err := json.NewDecoder(resp.Body).Decode(&rsp); err != nil {\n\t\treturn \"\", fmt.Errorf(\"json decode: %s\", err)\n\t}\n\treturn rsp.State, nil\n}\n\nfunc (c *InternalClient) PartitionNodes(ctx context.Context, partitionID int) ([]*disco.Node, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.PartitionNodes\")\n\tdefer span.Finish()\n\n\t// Execute request against the host.\n\tu := uriPathToURL(c.defaultURI, fmt.Sprintf(\"%s/internal/partition/nodes\", c.prefix()))\n\tu.RawQuery = (url.Values{\"partition\": {strconv.FormatInt(int64(partitionID), 10)}}).Encode()\n\n\t// Build request.\n\treq, err := http.NewRequest(\"GET\", u.String(), nil)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating request\")\n\t}\n\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\treq.Header.Set(\"Accept\", \"application/json\")\n\tAddAuthToken(ctx, &req.Header)\n\n\t// Execute request.\n\tresp, err := c.executeRequest(req.WithContext(ctx))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\n\tvar a []*disco.Node\n\tif err := json.NewDecoder(resp.Body).Decode(&a); err != nil {\n\t\treturn nil, fmt.Errorf(\"json decode: %s\", err)\n\t}\n\treturn a, nil\n}\n\nfunc (c *InternalClient) SetInternalAPI(api *API) {\n\tc.api = api\n}\n\nfunc (c *InternalClient) OAuthConfig() (rsp oauth2.Config, err error) {\n\tu := uriPathToURL(c.defaultURI, fmt.Sprintf(\"%s/internal/oauth-config\", c.prefix()))\n\n\treq, err := http.NewRequest(\"GET\", u.String(), nil)\n\tif err != nil {\n\t\treturn rsp, errors.Wrap(err, \"creating request\")\n\t}\n\n\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\treq.Header.Set(\"Accept\", \"application/json\")\n\tresp, err := c.executeRequest(req)\n\tif err != nil {\n\t\treturn rsp, fmt.Errorf(\"getting config: %w\", err)\n\t}\n\tdefer resp.Body.Close()\n\n\tif err := json.NewDecoder(resp.Body).Decode(&rsp); err != nil {\n\t\treturn rsp, fmt.Errorf(\"json decode: %s\", err)\n\t}\n\n\treturn rsp, nil\n}\n\n// GetDiskUsage gets the size of data directory across all nodes.\nfunc (c *InternalClient) GetDiskUsage(ctx context.Context) (DiskUsage, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.GetDiskUsage\")\n\tdefer span.Finish()\n\treturn c.getDiskUsage(ctx, \"\")\n}\n\n// GetIndexUsage gets the size of an index across all nodes.\nfunc (c *InternalClient) GetIndexUsage(ctx context.Context, index string) (DiskUsage, error) {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.GetIndexUsage\")\n\tdefer span.Finish()\n\treturn c.getDiskUsage(ctx, index)\n}\n\n// getDiskUsage returns size of data directory if index is zero value.\nfunc (c *InternalClient) getDiskUsage(ctx context.Context, index string) (DiskUsage, error) {\n\tnodes, err := c.Nodes(ctx)\n\tif err != nil {\n\t\treturn DiskUsage{}, fmt.Errorf(\"getting nodes: %s\", err)\n\t}\n\n\tvar sum DiskUsage\n\tfor _, node := range nodes {\n\t\tpath := \"/internal/disk-usage\"\n\t\tif index != \"\" {\n\t\t\tpath = path + \"/\" + index\n\t\t}\n\t\tu := uriPathToURL(&node.URI, path)\n\n\t\treq, err := http.NewRequest(\"GET\", u.String(), nil)\n\t\tif err != nil {\n\t\t\treturn DiskUsage{}, errors.Wrap(err, \"creating request\")\n\t\t}\n\n\t\treq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\t\treq.Header.Set(\"Accept\", \"application/json\")\n\t\tAddAuthToken(ctx, &req.Header)\n\n\t\t// Execute request.\n\t\tresp, err := c.executeRequest(req.WithContext(ctx))\n\t\tif err != nil {\n\t\t\treturn DiskUsage{}, err\n\t\t}\n\t\tdefer resp.Body.Close()\n\n\t\tvar rsp DiskUsage\n\t\tif err := json.NewDecoder(resp.Body).Decode(&rsp); err != nil {\n\t\t\treturn DiskUsage{}, fmt.Errorf(\"json decode: %s\", err)\n\t\t}\n\n\t\tsum.Usage += rsp.Usage\n\t}\n\n\treturn sum, nil\n}\n\nfunc (c *InternalClient) executeProtobufRequest(ctx context.Context, url string, data []byte) error {\n\thttpReq, err := http.NewRequest(\"POST\", url, bytes.NewBuffer(data))\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"creating request\")\n\t}\n\thttpReq.Header.Set(\"Content-Type\", \"application/x-protobuf\")\n\thttpReq.Header.Set(\"Accept\", \"application/x-protobuf\")\n\thttpReq.Header.Set(\"X-Pilosa-Row\", \"roaring\")\n\thttpReq.Header.Set(\"User-Agent\", \"pilosa/\"+Version)\n\tAddAuthToken(ctx, &httpReq.Header)\n\n\t// Execute request against the host.\n\tresp, err := c.executeRequest(httpReq.WithContext(ctx))\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer resp.Body.Close()\n\n\tdec := json.NewDecoder(resp.Body)\n\trbody := &ImportResponse{}\n\terr = dec.Decode(rbody)\n\t// Decode can return EOF when no error occurred. helpful!\n\tif err != nil && err != io.EOF {\n\t\treturn errors.Wrap(err, \"decoding response body\")\n\t}\n\tif rbody.Err != \"\" {\n\t\treturn errors.Wrap(errors.New(rbody.Err), \"importing roaring\")\n\t}\n\treturn nil\n}\n\n// ImportRoaringShard(ctx, node, string(tid), shard, request\nfunc (c *InternalClient) ImportRoaringShard(ctx context.Context, uri *pnet.URI, index string, shard uint64, remote bool, req *ImportRoaringShardRequest) error {\n\tspan, ctx := tracing.StartSpanFromContext(ctx, \"InternalClient.ImportRoaringShard\")\n\tdefer span.Finish()\n\n\tif index == \"\" {\n\t\treturn ErrIndexRequired\n\t}\n\tif uri == nil {\n\t\turi = c.defaultURI\n\t}\n\n\tvals := url.Values{}\n\tvals.Set(\"remote\", strconv.FormatBool(remote))\n\turl := fmt.Sprintf(\"%s%s/index/%s/shard/%d/import-roaring?%s\", uri, c.prefix(), index, shard, vals.Encode())\n\n\t// Marshal data to protobuf.\n\tdata, err := c.serializer.Marshal(req)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"marshal import roaring shard request\")\n\t}\n\treturn c.executeProtobufRequest(ctx, url, data)\n}\n"
        },
        {
          "name": "internal_client_test.go",
          "type": "blob",
          "size": 52.91015625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa_test\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/hex\"\n\t\"fmt\"\n\tgohttp \"net/http\"\n\t\"reflect\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/davecgh/go-spew/spew\"\n\tpilosa \"github.com/featurebasedb/featurebase/v3\"\n\t\"github.com/featurebasedb/featurebase/v3/authn\"\n\tfbcontext \"github.com/featurebasedb/featurebase/v3/context\"\n\t\"github.com/featurebasedb/featurebase/v3/disco\"\n\t\"github.com/featurebasedb/featurebase/v3/encoding/proto\"\n\t\"github.com/featurebasedb/featurebase/v3/pql\"\n\t\"github.com/featurebasedb/featurebase/v3/test\"\n\t\"github.com/featurebasedb/featurebase/v3/vprint\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/ricochet2200/go-disk-usage/du\"\n)\n\n// Test distributed TopN Row count across 3 nodes.\nfunc TestClient_MultiNode(t *testing.T) {\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\n\t_, err := c.GetNode(0).API.CreateIndex(context.Background(), c.Idx(), pilosa.IndexOptions{})\n\tif err != nil {\n\t\tt.Fatalf(\"creating index: %v\", err)\n\t}\n\t_, err = c.GetNode(0).API.CreateField(context.Background(), c.Idx(), \"f\", pilosa.OptFieldTypeSet(pilosa.DefaultCacheType, 100))\n\tif err != nil {\n\t\tt.Fatalf(\"creating field: %v\", err)\n\t}\n\n\t// Connect to each node to compare results.\n\tclient := make([]*Client, 3)\n\tclient[0] = MustNewClient(c.GetNode(0).URL(), pilosa.GetHTTPClient(nil))\n\tclient[1] = MustNewClient(c.GetNode(1).URL(), pilosa.GetHTTPClient(nil))\n\tclient[2] = MustNewClient(c.GetNode(2).URL(), pilosa.GetHTTPClient(nil))\n\n\tb0 := uint64(ShardWidth * 0)\n\tb1 := uint64(ShardWidth * 1)\n\tb2 := uint64(ShardWidth * 2)\n\n\t// helper to let us avoid repeating the b0+, etc, over and over\n\tcollate := func(a, b, c []uint64) []uint64 {\n\t\td := make([]uint64, len(a)+len(b)+len(c))\n\t\tn := 0\n\t\tfor _, v := range a {\n\t\t\td[n] = b0 + v\n\t\t\tn++\n\t\t}\n\t\tfor _, v := range b {\n\t\t\td[n] = b1 + v\n\t\t\tn++\n\t\t}\n\t\tfor _, v := range c {\n\t\t\td[n] = b2 + v\n\t\t\tn++\n\t\t}\n\t\treturn d\n\t}\n\t// data set. part of the goal of this is to have different top counts using a single node than\n\t// using all three nodes\n\trows := map[uint64][]uint64{\n\t\t100: collate([]uint64{10}, []uint64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, []uint64{10}),\n\t\t4:   collate([]uint64{10, 11, 12, 13, 14, 15}, nil, nil),\n\t\t2:   collate([]uint64{1, 2, 3, 4}, nil, nil),\n\t\t3:   collate([]uint64{1, 2, 3, 4, 5}, nil, nil),\n\t\t99:  collate(nil, []uint64{1, 2, 3, 4}, []uint64{10, 11, 12}),\n\t\t98:  collate(nil, []uint64{1, 2, 3, 4, 5, 6}, []uint64{10, 11}),\n\t\t22:  collate([]uint64{1, 2}, []uint64{1, 2, 3, 4, 5}, []uint64{10, 11, 12}),\n\t\t1:   collate(nil, []uint64{4}, nil),\n\t\t21:  collate(nil, nil, []uint64{10}),\n\t}\n\n\tbits := 0\n\tfor _, v := range rows {\n\t\tbits += len(v)\n\t}\n\trowIDs := make([]uint64, bits)\n\tcolIDs := make([]uint64, bits)\n\tn := 0\n\tfor k, cols := range rows {\n\t\tfor _, v := range cols {\n\t\t\trowIDs[n] = k\n\t\t\tcolIDs[n] = v\n\t\t\tn++\n\t\t}\n\t}\n\n\treq := &pilosa.ImportRequest{\n\t\tIndex:     c.Idx(),\n\t\tField:     \"f\",\n\t\tRowIDs:    rowIDs,\n\t\tColumnIDs: colIDs,\n\t\tShard:     ^uint64(0),\n\t}\n\terr = client[0].Import(context.Background(), nil, req, &pilosa.ImportOptions{})\n\tif err != nil {\n\t\tt.Fatalf(\"importing data: %v\", err)\n\t}\n\t// Rebuild the RankCache.\n\t// We have to do this to avoid the 10-second cache invalidation delay\n\t// built into cache.Invalidate()\n\terr = c.GetNode(0).RecalculateCaches(t)\n\tif err != nil {\n\t\tt.Fatalf(\"recalculating cache: %v\", err)\n\t}\n\terr = c.GetNode(1).RecalculateCaches(t)\n\tif err != nil {\n\t\tt.Fatalf(\"recalculating cache: %v\", err)\n\t}\n\terr = c.GetNode(2).RecalculateCaches(t)\n\tif err != nil {\n\t\tt.Fatalf(\"recalculating cache: %v\", err)\n\t}\n\n\ttopN := 4\n\tqueryRequest := &pilosa.QueryRequest{\n\t\tQuery:  fmt.Sprintf(`TopN(f, n=%d)`, topN),\n\t\tRemote: false,\n\t}\n\n\tresult, err := client[0].Query(context.Background(), c.Idx(), queryRequest)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Test must return exactly N results.\n\tpairsField := result.Results[0].(*pilosa.PairsField)\n\tif len(pairsField.Pairs) != topN {\n\t\tt.Fatalf(\"unexpected number of TopN results: %s\", spew.Sdump(result))\n\t}\n\tp := []pilosa.Pair{\n\t\t{ID: 100, Count: 12},\n\t\t{ID: 22, Count: 10},\n\t\t{ID: 98, Count: 8},\n\t\t{ID: 99, Count: 7}}\n\n\t// Valdidate the Top 4 result counts.\n\tif !reflect.DeepEqual(pairsField.Pairs, p) {\n\t\tt.Fatalf(\"Invalid TopN result set: %s\", spew.Sdump(result))\n\t}\n\n\tresult1, err := client[1].Query(context.Background(), c.Idx(), queryRequest)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tresult2, err := client[2].Query(context.Background(), c.Idx(), queryRequest)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Compare TopN results across all nodes in the cluster.\n\tif result.SameAs(result1) != nil {\n\t\tt.Fatalf(\"TopN result should be the same on node0 and node1: %s\", spew.Sdump(result1))\n\t}\n\n\tif result.SameAs(result2) != nil {\n\t\tt.Fatalf(\"TopN result should be the same on node0 and node2: %s\", spew.Sdump(result2))\n\t}\n}\n\n// Ensure client can export data.\nfunc TestClient_Export(t *testing.T) {\n\tcluster := test.MustRunCluster(t, 1)\n\tdefer cluster.Close()\n\tcmd := cluster.GetNode(0)\n\tkeyed := cluster.Idx(\"k\")\n\tunkeyed := cluster.Idx(\"u\")\n\n\thost := cmd.URL()\n\n\tcmd.MustCreateIndex(t, keyed, pilosa.IndexOptions{Keys: true})\n\tcmd.MustCreateIndex(t, unkeyed, pilosa.IndexOptions{Keys: false})\n\n\tcmd.MustCreateField(t, keyed, \"keyedf\", pilosa.OptFieldTypeSet(pilosa.CacheTypeRanked, 1000), pilosa.OptFieldKeys())\n\tcmd.MustCreateField(t, keyed, \"unkeyedf\", pilosa.OptFieldTypeSet(pilosa.CacheTypeRanked, 1000))\n\tcmd.MustCreateField(t, unkeyed, \"keyedf\", pilosa.OptFieldTypeSet(pilosa.CacheTypeRanked, 1000), pilosa.OptFieldKeys())\n\tcmd.MustCreateField(t, unkeyed, \"unkeyedf\", pilosa.OptFieldTypeSet(pilosa.CacheTypeRanked, 1000))\n\n\tc := MustNewClient(host, pilosa.GetHTTPClient(nil))\n\tdata := []pilosa.Bit{\n\t\t{RowID: 1, ColumnID: 100, RowKey: \"row1\", ColumnKey: \"col100\"},\n\t\t{RowID: 1, ColumnID: 101, RowKey: \"row1\", ColumnKey: \"col101\"},\n\t\t{RowID: 1, ColumnID: 102, RowKey: \"row1\", ColumnKey: \"col102\"},\n\t\t{RowID: 1, ColumnID: 103, RowKey: \"row1\", ColumnKey: \"col103\"},\n\t\t{RowID: 2, ColumnID: 200, RowKey: \"row2\", ColumnKey: \"col200\"},\n\t\t{RowID: 2, ColumnID: 201, RowKey: \"row2\", ColumnKey: \"col201\"},\n\t\t{RowID: 2, ColumnID: 202, RowKey: \"row2\", ColumnKey: \"col202\"},\n\t\t{RowID: 2, ColumnID: 203, RowKey: \"row2\", ColumnKey: \"col203\"},\n\t}\n\n\tt.Run(\"Export unkeyed,unkeyedf\", func(t *testing.T) {\n\t\t// Populate data.\n\t\tfor _, bit := range data {\n\t\t\t_, err := c.Query(context.Background(), unkeyed, &pilosa.QueryRequest{\n\t\t\t\tQuery:  fmt.Sprintf(`Set(%d, unkeyedf=%d)`, bit.ColumnID, bit.RowID),\n\t\t\t\tRemote: false,\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t}\n\n\t\tbuf := bytes.NewBuffer(nil)\n\t\tbw := bufio.NewWriter(buf)\n\n\t\t// Send export request.\n\t\tif err := c.ExportCSV(context.Background(), unkeyed, \"unkeyedf\", 0, bw); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tgot := buf.String()\n\n\t\t// Expected output.\n\t\texp := \"\"\n\t\tfor _, bit := range data {\n\t\t\texp += fmt.Sprintf(\"%d,%d\\n\", bit.RowID, bit.ColumnID)\n\t\t}\n\n\t\t// Verify data.\n\t\tif got != exp {\n\t\t\tt.Fatalf(\"unexpected export data: %s\", got)\n\t\t}\n\t})\n\n\tt.Run(\"Export unkeyed,keyedf\", func(t *testing.T) {\n\t\t// Populate data.\n\t\tfor _, bit := range data {\n\t\t\t_, err := c.Query(context.Background(), unkeyed, &pilosa.QueryRequest{\n\t\t\t\tQuery:  fmt.Sprintf(`Set(%d, keyedf=%s)`, bit.ColumnID, bit.RowKey),\n\t\t\t\tRemote: false,\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t}\n\n\t\tbuf := bytes.NewBuffer(nil)\n\t\tbw := bufio.NewWriter(buf)\n\n\t\t// Send export request.\n\t\tif err := c.ExportCSV(context.Background(), unkeyed, \"keyedf\", 0, bw); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tgot := buf.String()\n\n\t\t// Expected output.\n\t\texp := \"\"\n\t\tfor _, bit := range data {\n\t\t\texp += fmt.Sprintf(\"%s,%d\\n\", bit.RowKey, bit.ColumnID)\n\t\t}\n\n\t\t// Verify data.\n\t\tif got != exp {\n\t\t\tt.Fatalf(\"unexpected export data: %s\", got)\n\t\t}\n\t})\n\n\tt.Run(\"Export keyed,unkeyedf\", func(t *testing.T) {\n\t\t// Populate data.\n\t\tfor _, bit := range data {\n\t\t\t_, err := c.Query(context.Background(), keyed, &pilosa.QueryRequest{\n\t\t\t\tQuery:  fmt.Sprintf(`Set(\"%s\", unkeyedf=%d)`, bit.ColumnKey, bit.RowID),\n\t\t\t\tRemote: false,\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t}\n\n\t\tbuf := bytes.NewBuffer(nil)\n\t\tbw := bufio.NewWriter(buf)\n\n\t\t// Send export request for every partition.\n\t\tfor i := 0; i < disco.DefaultPartitionN; i++ {\n\t\t\tif err := c.ExportCSV(context.Background(), keyed, \"unkeyedf\", uint64(i), bw); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t}\n\n\t\tgot := buf.String()\n\t\tgotSlice := strings.Split(got, \"\\n\")\n\n\t\t// Expected output is not sorted because of key sharding.\n\t\texpSlice := []string{\n\t\t\t\"1,col103\",\n\t\t\t\"1,col102\",\n\t\t\t\"1,col101\",\n\t\t\t\"1,col100\",\n\t\t\t\"2,col200\",\n\t\t\t\"2,col201\",\n\t\t\t\"2,col202\",\n\t\t\t\"2,col203\",\n\t\t\t\"\",\n\t\t}\n\t\tif !sameStringSlice(gotSlice, expSlice) {\n\t\t\tt.Fatalf(\"unexpected results: %q\", gotSlice)\n\t\t}\n\t})\n\n\tt.Run(\"Export keyed,keyedf\", func(t *testing.T) {\n\t\t// Populate data.\n\t\tfor _, bit := range data {\n\t\t\t_, err := c.Query(context.Background(), keyed, &pilosa.QueryRequest{\n\t\t\t\tQuery:  fmt.Sprintf(`Set(\"%s\", keyedf=%s)`, bit.ColumnKey, bit.RowKey),\n\t\t\t\tRemote: false,\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t}\n\n\t\tbuf := bytes.NewBuffer(nil)\n\t\tbw := bufio.NewWriter(buf)\n\n\t\t// Send export request.\n\t\tfor i := 0; i < disco.DefaultPartitionN; i++ {\n\t\t\tif err := c.ExportCSV(context.Background(), keyed, \"keyedf\", uint64(i), bw); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t}\n\n\t\tgot := buf.String()\n\t\tgotSlice := strings.Split(got, \"\\n\")\n\n\t\t// Expected output is not sorted because of key sharding.\n\t\texpSlice := []string{\n\t\t\t\"row1,col103\",\n\t\t\t\"row1,col102\",\n\t\t\t\"row1,col101\",\n\t\t\t\"row1,col100\",\n\t\t\t\"row2,col200\",\n\t\t\t\"row2,col201\",\n\t\t\t\"row2,col202\",\n\t\t\t\"row2,col203\",\n\t\t\t\"\",\n\t\t}\n\t\tif !sameStringSlice(gotSlice, expSlice) {\n\t\t\tt.Fatalf(\"unexpected results: %q\", gotSlice)\n\t\t}\n\t})\n}\n\n// Ensure client can bulk import data.\nfunc TestClient_Import(t *testing.T) {\n\t// Need a cluster to verify hitting multiple nodes\n\tcluster := test.MustRunCluster(t, 3)\n\tdefer cluster.Close()\n\tcmd := cluster.GetNode(0)\n\thost := cmd.URL()\n\tapi := cmd.API\n\tkeyed := cluster.Idx(\"k\")\n\tunkeyed := cluster.Idx(\"u\")\n\n\tcmd.MustCreateIndex(t, keyed, pilosa.IndexOptions{Keys: true})\n\tcmd.MustCreateIndex(t, unkeyed, pilosa.IndexOptions{Keys: false})\n\n\tcmd.MustCreateField(t, keyed, \"keyedf\", pilosa.OptFieldTypeSet(pilosa.CacheTypeNone, 0), pilosa.OptFieldKeys())\n\tcmd.MustCreateField(t, keyed, \"unkeyedf\", pilosa.OptFieldTypeSet(pilosa.CacheTypeNone, 0))\n\tcmd.MustCreateField(t, unkeyed, \"keyedf\", pilosa.OptFieldTypeSet(pilosa.CacheTypeNone, 0), pilosa.OptFieldKeys())\n\tcmd.MustCreateField(t, unkeyed, \"unkeyedf\", pilosa.OptFieldTypeSet(pilosa.CacheTypeNone, 0))\n\n\tindexes := map[bool]string{true: keyed, false: unkeyed}\n\tfields := map[bool]string{true: \"keyedf\", false: \"unkeyedf\"}\n\n\trecKeys := []string{\"rec-a\", \"rec-b\", \"rec-c\"}\n\tvalueKeys := []string{\"val-a\", \"val-b\", \"val-c\"}\n\trecIDs := []uint64{0, 3, 7}\n\tvalueIDs := []uint64{0, 3, 7}\n\n\tc := MustNewClient(host, pilosa.GetHTTPClient(nil))\n\t// set API to point at the local node\n\tc.SetInternalAPI(cmd.API)\n\n\tcheckResults := func(results pilosa.QueryResponse, keyed bool, maxN int) {\n\t\tfor i, r := range results.Results {\n\t\t\trow, ok := r.(*pilosa.Row)\n\t\t\tif !ok {\n\t\t\t\tt.Fatalf(\"expected row, got %T\", r)\n\t\t\t}\n\t\t\tif i >= maxN {\n\t\t\t\t// we cleared these, so they should be empty\n\t\t\t\tif keyed {\n\t\t\t\t\tvals := row.Keys\n\t\t\t\t\tif len(vals) != 0 {\n\t\t\t\t\t\tt.Fatalf(\"expected empty row, got %d result(s) back, first result %q\",\n\t\t\t\t\t\t\tlen(vals), vals[0])\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tvals := row.Columns()\n\t\t\t\t\tif len(vals) != 0 {\n\t\t\t\t\t\tt.Fatalf(\"expected empty row, got %d result(s) back, first result %d\",\n\t\t\t\t\t\t\tlen(vals), vals[0])\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif keyed {\n\t\t\t\tvals := row.Keys\n\t\t\t\tif len(vals) != 1 {\n\t\t\t\t\tt.Fatalf(\"expected one result, didn't get it\")\n\t\t\t\t}\n\t\t\t\tif vals[0] != recKeys[i] {\n\t\t\t\t\tt.Fatalf(\"expected %q, got %q\", recKeys[i], vals[0])\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tvals := row.Columns()\n\t\t\t\tif len(vals) != 1 {\n\t\t\t\t\tt.Fatalf(\"expected one result, didn't get it\")\n\t\t\t\t}\n\t\t\t\tif vals[0] != recIDs[i] {\n\t\t\t\t\tt.Fatalf(\"expected %d, got %d\", recIDs[i], vals[0])\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tfor useKeys, indexName := range indexes {\n\t\tfor _, fieldName := range fields {\n\t\t\treq := pilosa.ImportRequest{\n\t\t\t\tIndex: indexName,\n\t\t\t\tField: fieldName,\n\t\t\t}\n\t\t\tif indexName == keyed {\n\t\t\t\treq.ColumnKeys = recKeys\n\t\t\t\treq.Shard = ^uint64(0)\n\t\t\t} else {\n\t\t\t\treq.ColumnIDs = recIDs\n\t\t\t\treq.Shard = 0\n\t\t\t}\n\t\t\tif fieldName == \"keyedf\" {\n\t\t\t\treq.RowKeys = valueKeys\n\t\t\t} else {\n\t\t\t\treq.RowIDs = valueIDs\n\t\t\t}\n\t\t\t// clone request because some imports will modify their input parameters\n\t\t\tif err := c.Import(context.Background(), nil, req.Clone(), &pilosa.ImportOptions{}); err != nil {\n\t\t\t\tt.Fatalf(\"%s/%s: %v\",\n\t\t\t\t\tindexName, fieldName, err)\n\t\t\t}\n\n\t\t\t// Now do a query to see whether it worked...\n\t\t\tvar pql string\n\t\t\tif fieldName == \"keyedf\" {\n\t\t\t\tpql = `Row(keyedf=\"val-a\") Row(keyedf=\"val-b\") Row(keyedf=\"val-c\")`\n\t\t\t} else {\n\t\t\t\tpql = `Row(unkeyedf=0) Row(unkeyedf=3) Row(unkeyedf=7)`\n\t\t\t}\n\t\t\tresults, err := api.Query(context.Background(), &pilosa.QueryRequest{Index: indexName, Query: pql})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tcheckResults(results, useKeys, 3)\n\n\t\t\t// Now clear a bit...\n\t\t\treq = pilosa.ImportRequest{\n\t\t\t\tIndex: indexName,\n\t\t\t\tField: fieldName,\n\t\t\t}\n\t\t\tif indexName == keyed {\n\t\t\t\treq.ColumnKeys = recKeys[2:]\n\t\t\t\treq.Shard = ^uint64(0)\n\t\t\t} else {\n\t\t\t\treq.ColumnIDs = recIDs[2:]\n\t\t\t\treq.Shard = 0\n\t\t\t}\n\t\t\tif fieldName == \"keyedf\" {\n\t\t\t\treq.RowKeys = valueKeys[2:]\n\t\t\t} else {\n\t\t\t\treq.RowIDs = valueIDs[2:]\n\t\t\t}\n\t\t\t// do a clear. also, do the clear with a Qcx.\n\t\t\tfunc() {\n\t\t\t\t// inner function so the deferred abort isn't delayed a lot\n\t\t\t\tqcx := api.Txf().NewQcx()\n\t\t\t\tdefer qcx.Abort()\n\t\t\t\tif err := c.Import(context.Background(), qcx, req.Clone(), &pilosa.ImportOptions{Clear: true}); err != nil {\n\t\t\t\t\tt.Fatalf(\"%s/%s: %v\",\n\t\t\t\t\t\tindexName, fieldName, err)\n\t\t\t\t}\n\t\t\t\tif err := qcx.Finish(); err != nil {\n\t\t\t\t\tt.Fatalf(\"committing write: %v\", err)\n\t\t\t\t}\n\t\t\t}()\n\t\t\t// Now do a query to see whether it worked...\n\t\t\tif fieldName == \"keyedf\" {\n\t\t\t\tpql = `Row(keyedf=\"val-a\") Row(keyedf=\"val-b\") Row(keyedf=\"val-c\")`\n\t\t\t} else {\n\t\t\t\tpql = `Row(unkeyedf=0) Row(unkeyedf=3) Row(unkeyedf=7)`\n\t\t\t}\n\t\t\tresults, err = api.Query(context.Background(), &pilosa.QueryRequest{Index: indexName, Query: pql})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tcheckResults(results, useKeys, 2)\n\t\t}\n\t}\n}\n\n// Ensure client can bulk import data.\nfunc TestClient_ImportRoaring(t *testing.T) {\n\tcluster := test.MustUnsharedCluster(t, 3)\n\t// Unshared because we want to set ReplicaN = 3 so we can verify data present on all nodes\n\tfor _, c := range cluster.Nodes {\n\t\tc.Config.Cluster.ReplicaN = 3\n\t}\n\terr := cluster.Start()\n\tif err != nil {\n\t\tt.Fatalf(\"starting cluster: %v\", err)\n\t}\n\tdefer cluster.Close()\n\n\t_, err = cluster.GetNode(0).API.CreateIndex(context.Background(), cluster.Idx(), pilosa.IndexOptions{})\n\tif err != nil {\n\t\tt.Fatalf(\"creating index: %v\", err)\n\t}\n\t_, err = cluster.GetNode(0).API.CreateField(context.Background(), cluster.Idx(), \"f\", pilosa.OptFieldTypeSet(pilosa.CacheTypeRanked, 100))\n\tif err != nil {\n\t\tt.Fatalf(\"creating field: %v\", err)\n\t}\n\t_, err = cluster.GetNode(0).API.Query(context.Background(), &pilosa.QueryRequest{Index: cluster.Idx(), Query: \"Set(0, f=1)\"})\n\tif err != nil {\n\t\tt.Fatalf(\"querying: %v\", err)\n\t}\n\n\t// Send import request.\n\thost := cluster.GetNode(0).URL()\n\tc := MustNewClient(host, pilosa.GetHTTPClient(nil))\n\t// [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 65537]\n\troaringReq := makeImportRoaringRequest(false, \"3B3001000100000900010000000100010009000100\")\n\tif err := c.ImportRoaring(context.Background(), &cluster.GetNode(0).API.Node().URI, cluster.Idx(), \"f\", 0, false, roaringReq); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\thldr := test.Holder{Holder: cluster.GetNode(0).Server.Holder()}\n\t// Verify data on node 0.\n\tif a := hldr.Row(cluster.Idx(), \"f\", 0).Columns(); !reflect.DeepEqual(a, []uint64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 65537}) {\n\t\tt.Fatalf(\"unexpected columns: %+v\", a)\n\t}\n\tif a := hldr.Row(cluster.Idx(), \"f\", 1).Columns(); !reflect.DeepEqual(a, []uint64{0}) {\n\t\tt.Fatalf(\"unexpected columns: %+v\", a)\n\t}\n\n\thldr2 := test.Holder{Holder: cluster.GetNode(1).Server.Holder()}\n\t// Verify data on node 1.\n\tif a := hldr2.Row(cluster.Idx(), \"f\", 0).Columns(); !reflect.DeepEqual(a, []uint64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 65537}) {\n\t\tt.Fatalf(\"unexpected columns: %+v\", a)\n\t}\n\tif a := hldr2.Row(cluster.Idx(), \"f\", 1).Columns(); !reflect.DeepEqual(a, []uint64{0}) {\n\t\tt.Fatalf(\"unexpected columns: %+v\", a)\n\t}\n\n\t// Ensure that sending a roaring import with the clear flag works as expected.\n\t// [65539, 65540]\n\troaringReq = makeImportRoaringRequest(true, \"3A30000001000000010001001000000003000400\")\n\tif err := c.ImportRoaring(context.Background(), &cluster.GetNode(0).API.Node().URI, cluster.Idx(), \"f\", 0, false, roaringReq); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Verify data on node 0.\n\tif a := hldr.Row(cluster.Idx(), \"f\", 0).Columns(); !reflect.DeepEqual(a, []uint64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 65537}) {\n\t\tt.Fatalf(\"unexpected clear columns: %+v\", a)\n\t}\n\tif a := hldr.Row(cluster.Idx(), \"f\", 1).Columns(); !reflect.DeepEqual(a, []uint64{0}) {\n\t\tt.Fatalf(\"unexpected clear columns: %+v\", a)\n\t}\n\n\t// Verify data on node 1.\n\tif a := hldr2.Row(cluster.Idx(), \"f\", 0).Columns(); !reflect.DeepEqual(a, []uint64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 65537}) {\n\t\tt.Fatalf(\"unexpected clear columns: %+v\", a)\n\t}\n\tif a := hldr2.Row(cluster.Idx(), \"f\", 1).Columns(); !reflect.DeepEqual(a, []uint64{0}) {\n\t\tt.Fatalf(\"unexpected clear columns: %+v\", a)\n\t}\n\n\t// Ensure that sending a roaring import with the clear flag works as expected.\n\t// [4, 6, 65537, 65539]\n\troaringReq = makeImportRoaringRequest(true, \"3A300000020000000000010001000100180000001C0000000400060001000300\")\n\tif err := c.ImportRoaring(context.Background(), &cluster.GetNode(0).API.Node().URI, cluster.Idx(), \"f\", 0, false, roaringReq); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Verify data on node 0.\n\tif a := hldr.Row(cluster.Idx(), \"f\", 0).Columns(); !reflect.DeepEqual(a, []uint64{1, 2, 3, 5, 7, 8, 9, 10}) {\n\t\tt.Fatalf(\"unexpected clear columns: %+v\", a)\n\t}\n\tif a := hldr.Row(cluster.Idx(), \"f\", 1).Columns(); !reflect.DeepEqual(a, []uint64{0}) {\n\t\tt.Fatalf(\"unexpected clear columns: %+v\", a)\n\t}\n\n\t// Verify data on node 1.\n\tif a := hldr2.Row(cluster.Idx(), \"f\", 0).Columns(); !reflect.DeepEqual(a, []uint64{1, 2, 3, 5, 7, 8, 9, 10}) {\n\t\tt.Fatalf(\"unexpected clear columns: %+v\", a)\n\t}\n\tif a := hldr2.Row(cluster.Idx(), \"f\", 1).Columns(); !reflect.DeepEqual(a, []uint64{0}) {\n\t\tt.Fatalf(\"unexpected clear columns: %+v\", a)\n\t}\n\n\t// Ensure that sending a roaring import with the clear flag works as expected.\n\t// [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 65537]\n\troaringReq = makeImportRoaringRequest(true, \"3B3001000100000900010000000100010009000100\")\n\tif err := c.ImportRoaring(context.Background(), &cluster.GetNode(0).API.Node().URI, cluster.Idx(), \"f\", 0, false, roaringReq); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Verify data on node 0.\n\tif a := hldr.Row(cluster.Idx(), \"f\", 0).Columns(); !reflect.DeepEqual(a, []uint64{}) {\n\t\tt.Fatalf(\"unexpected clear columns: %+v\", a)\n\t}\n\tif a := hldr.Row(cluster.Idx(), \"f\", 1).Columns(); !reflect.DeepEqual(a, []uint64{0}) {\n\t\tt.Fatalf(\"unexpected clear columns: %+v\", a)\n\t}\n\n\t// Verify data on node 1.\n\tif a := hldr2.Row(cluster.Idx(), \"f\", 0).Columns(); !reflect.DeepEqual(a, []uint64{}) {\n\t\tt.Fatalf(\"unexpected clear columns: %+v\", a)\n\t}\n\tif a := hldr2.Row(cluster.Idx(), \"f\", 1).Columns(); !reflect.DeepEqual(a, []uint64{0}) {\n\t\tt.Fatalf(\"unexpected clear columns: %+v\", a)\n\t}\n}\n\n// Ensure client can bulk import data with multiple views and not deadlock.\nfunc TestClient_ImportRoaring_MultiView(t *testing.T) {\n\tcluster := test.MustUnsharedCluster(t, 2)\n\tfor _, c := range cluster.Nodes {\n\t\tc.Config.Cluster.ReplicaN = 2\n\t}\n\terr := cluster.Start()\n\tif err != nil {\n\t\tt.Fatalf(\"starting cluster: %v\", err)\n\t}\n\tdefer cluster.Close()\n\n\tapi := cluster.GetNode(0).API\n\n\t_, err = api.CreateIndex(context.Background(), cluster.Idx(), pilosa.IndexOptions{})\n\tif err != nil {\n\t\tt.Fatalf(\"creating index: %v\", err)\n\t}\n\t_, err = api.CreateField(context.Background(), cluster.Idx(), \"f\", pilosa.OptFieldTypeSet(pilosa.CacheTypeRanked, 100))\n\tif err != nil {\n\t\tt.Fatalf(\"creating field: %v\", err)\n\t}\n\t_, err = api.Query(context.Background(), &pilosa.QueryRequest{Index: cluster.Idx(), Query: \"Set(0, f=1)\"})\n\tif err != nil {\n\t\tt.Fatalf(\"querying: %v\", err)\n\t}\n\n\t// Send import request.\n\thost := cluster.GetNode(0).URL()\n\tc := MustNewClient(host, pilosa.GetHTTPClient(nil))\n\treq := &pilosa.ImportRoaringRequest{Views: map[string][]byte{}}\n\treq.Views[\"standard\"], _ = hex.DecodeString(\"3B3001000100000900010000000100010009000100\")\n\treq.Views[\"existence\"], _ = hex.DecodeString(\"3B3001000100000900010000000100010009000100\")\n\tif err := c.ImportRoaring(context.Background(), &cluster.GetNode(0).API.Node().URI, cluster.Idx(), \"f\", 0, false, req); err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\n// Ensure client can bulk import data.\nfunc TestClient_ImportKeys(t *testing.T) {\n\tt.Run(\"SingleNode\", func(t *testing.T) {\n\t\tcluster := test.MustRunCluster(t, 1)\n\t\tdefer cluster.Close()\n\t\tcmd := cluster.GetNode(0)\n\t\thost := cmd.URL()\n\t\tkeyed := cluster.Idx(\"k\")\n\t\tunkeyed := cluster.Idx(\"u\")\n\n\t\tcmd.MustCreateIndex(t, keyed, pilosa.IndexOptions{Keys: true})\n\t\tcmd.MustCreateIndex(t, unkeyed, pilosa.IndexOptions{Keys: false})\n\n\t\tcmd.MustCreateField(t, keyed, \"keyedf\", pilosa.OptFieldTypeSet(pilosa.CacheTypeRanked, 1000), pilosa.OptFieldKeys())\n\t\tcmd.MustCreateField(t, keyed, \"unkeyedf\", pilosa.OptFieldTypeSet(pilosa.CacheTypeRanked, 1000))\n\t\tcmd.MustCreateField(t, unkeyed, \"keyedf\", pilosa.OptFieldTypeSet(pilosa.CacheTypeRanked, 1000), pilosa.OptFieldKeys())\n\n\t\t// Send import request.\n\t\tc := MustNewClient(host, pilosa.GetHTTPClient(nil))\n\t\tbaseReq := &pilosa.ImportRequest{\n\t\t\tIndex:      keyed,\n\t\t\tField:      \"keyedf\",\n\t\t\tColumnKeys: []string{\"eve\", \"alice\", \"bob\", \"eve\", \"alice\", \"eve\"},\n\t\t\tColumnIDs:  []uint64{1, 2, 3, 1, 2, 1},\n\t\t\tRowKeys:    []string{\"green\", \"green\", \"green\", \"blue\", \"blue\", \"purple\"},\n\t\t\tRowIDs:     []uint64{1, 1, 1, 2, 2, 3},\n\t\t}\n\n\t\tt.Run(\"Import keyed,keyed\", func(t *testing.T) {\n\t\t\treq := baseReq.Clone()\n\t\t\treq.Index = keyed\n\t\t\treq.Field = \"keyedf\"\n\t\t\treq.ColumnIDs, req.RowIDs = nil, nil\n\t\t\tif err := c.Import(context.Background(), nil, req, &pilosa.ImportOptions{}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tresp := cmd.QueryAPI(t, &pilosa.QueryRequest{\n\t\t\t\tIndex: keyed,\n\t\t\t\tQuery: \"TopN(keyedf)\",\n\t\t\t})\n\t\t\tif pairs, ok := resp.Results[0].(*pilosa.PairsField); !ok {\n\t\t\t\tt.Fatalf(\"unexpected response type %T\", resp.Results[0])\n\t\t\t} else if !reflect.DeepEqual(pairs.Pairs, []pilosa.Pair{\n\t\t\t\t{Key: \"green\", Count: 3},\n\t\t\t\t{Key: \"blue\", Count: 2},\n\t\t\t\t{Key: \"purple\", Count: 1},\n\t\t\t}) {\n\t\t\t\tt.Fatalf(\"unexpected topn result: %v\", pairs.Pairs)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"Import keyed,unkeyedf\", func(t *testing.T) {\n\t\t\treq := baseReq.Clone()\n\t\t\treq.Index = keyed\n\t\t\treq.Field = \"unkeyedf\"\n\t\t\treq.ColumnIDs, req.RowKeys = nil, nil\n\t\t\tif err := c.Import(context.Background(), nil, req, &pilosa.ImportOptions{}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tresp := cmd.QueryAPI(t, &pilosa.QueryRequest{\n\t\t\t\tIndex: keyed,\n\t\t\t\tQuery: \"TopN(unkeyedf)\",\n\t\t\t})\n\t\t\tif pairs, ok := resp.Results[0].(*pilosa.PairsField); !ok {\n\t\t\t\tt.Fatalf(\"unexpected response type %T\", resp.Results[0])\n\t\t\t} else if !reflect.DeepEqual(pairs.Pairs, []pilosa.Pair{\n\t\t\t\t{ID: 1, Count: 3},\n\t\t\t\t{ID: 2, Count: 2},\n\t\t\t\t{ID: 3, Count: 1},\n\t\t\t}) {\n\t\t\t\tt.Fatalf(\"unexpected topn result: %v\", pairs.Pairs)\n\t\t\t}\n\t\t})\n\n\t\tt.Run(\"Import unkeyed,keyed\", func(t *testing.T) {\n\t\t\treq := baseReq.Clone()\n\t\t\treq.Index = unkeyed\n\t\t\treq.Field = \"keyedf\"\n\t\t\treq.ColumnKeys, req.RowIDs = nil, nil\n\t\t\tif err := c.Import(context.Background(), nil, req, &pilosa.ImportOptions{}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tresp := cmd.QueryAPI(t, &pilosa.QueryRequest{\n\t\t\t\tIndex: unkeyed,\n\t\t\t\tQuery: \"TopN(keyedf)\",\n\t\t\t})\n\t\t\tif pairs, ok := resp.Results[0].(*pilosa.PairsField); !ok {\n\t\t\t\tt.Fatalf(\"unexpected response type %T\", resp.Results[0])\n\t\t\t} else if !reflect.DeepEqual(pairs.Pairs, []pilosa.Pair{\n\t\t\t\t{Key: \"green\", Count: 3},\n\t\t\t\t{Key: \"blue\", Count: 2},\n\t\t\t\t{Key: \"purple\", Count: 1},\n\t\t\t}) {\n\t\t\t\tt.Fatalf(\"unexpected topn result: %v\", pairs.Pairs)\n\t\t\t}\n\t\t})\n\t})\n\n\tt.Run(\"MultiNode\", func(t *testing.T) {\n\t\tcluster := test.MustRunCluster(t, 3)\n\t\tdefer cluster.Close()\n\t\tcmd0 := cluster.GetNode(0)\n\t\tcmd1 := cluster.GetNode(1)\n\t\thost0 := cmd0.URL()\n\t\thost1 := cmd1.URL()\n\t\tkeyed := cluster.Idx(\"k\")\n\n\t\tcmd0.MustCreateIndex(t, keyed, pilosa.IndexOptions{Keys: true})\n\t\tcmd0.MustCreateField(t, keyed, \"keyedf0\", pilosa.OptFieldTypeSet(pilosa.CacheTypeRanked, 1000), pilosa.OptFieldKeys())\n\t\tcmd0.MustCreateField(t, keyed, \"keyedf1\", pilosa.OptFieldTypeSet(pilosa.CacheTypeRanked, 1000), pilosa.OptFieldKeys())\n\n\t\t// Send import request.\n\t\tc0 := MustNewClient(host0, pilosa.GetHTTPClient(nil))\n\t\tc1 := MustNewClient(host1, pilosa.GetHTTPClient(nil))\n\n\t\t// Import to node0.\n\t\tt.Run(\"Import node0\", func(t *testing.T) {\n\t\t\treq := &pilosa.ImportRequest{\n\t\t\t\tIndex:      keyed,\n\t\t\t\tField:      \"keyedf0\",\n\t\t\t\tColumnKeys: []string{\"eve\", \"alice\", \"bob\", \"eve\", \"alice\", \"eve\"},\n\t\t\t\tRowKeys:    []string{\"green\", \"green\", \"green\", \"blue\", \"blue\", \"purple\"},\n\t\t\t}\n\t\t\tif err := c0.Import(context.Background(), nil, req, &pilosa.ImportOptions{}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tresp := cmd0.QueryAPI(t, &pilosa.QueryRequest{\n\t\t\t\tIndex: keyed,\n\t\t\t\tQuery: \"TopN(keyedf0)\",\n\t\t\t})\n\t\t\tif pairs, ok := resp.Results[0].(*pilosa.PairsField); !ok {\n\t\t\t\tt.Fatalf(\"unexpected response type %T\", resp.Results[0])\n\t\t\t} else if !reflect.DeepEqual(pairs.Pairs, []pilosa.Pair{\n\t\t\t\t{Key: \"green\", Count: 3},\n\t\t\t\t{Key: \"blue\", Count: 2},\n\t\t\t\t{Key: \"purple\", Count: 1},\n\t\t\t}) {\n\t\t\t\tt.Fatalf(\"unexpected topn result: %v\", pairs.Pairs)\n\t\t\t}\n\t\t})\n\n\t\t// Import to node1 (ensure import is routed to primary for translation).\n\t\tt.Run(\"Import node1\", func(t *testing.T) {\n\t\t\treq := &pilosa.ImportRequest{\n\t\t\t\tIndex:      keyed,\n\t\t\t\tField:      \"keyedf1\",\n\t\t\t\tColumnKeys: []string{\"eve\", \"alice\", \"bob\", \"eve\", \"alice\", \"eve\"},\n\t\t\t\tRowKeys:    []string{\"green\", \"green\", \"green\", \"blue\", \"blue\", \"purple\"},\n\t\t\t}\n\t\t\tif err := c1.Import(context.Background(), nil, req, &pilosa.ImportOptions{}); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\t// Wait for translation replication.\n\t\t\ttime.Sleep(500 * time.Millisecond)\n\n\t\t\tresp := cmd1.QueryAPI(t, &pilosa.QueryRequest{\n\t\t\t\tIndex: keyed,\n\t\t\t\tQuery: \"TopN(keyedf1)\",\n\t\t\t})\n\t\t\tif pairs, ok := resp.Results[0].(*pilosa.PairsField); !ok {\n\t\t\t\tt.Fatalf(\"unexpected response type %T\", resp.Results[0])\n\t\t\t} else if !reflect.DeepEqual(pairs.Pairs, []pilosa.Pair{\n\t\t\t\t{Key: \"green\", Count: 3},\n\t\t\t\t{Key: \"blue\", Count: 2},\n\t\t\t\t{Key: \"purple\", Count: 1},\n\t\t\t}) {\n\t\t\t\tt.Fatalf(\"unexpected topn result: %#v\", pairs.Pairs)\n\t\t\t}\n\t\t})\n\t})\n\n\tt.Run(\"IntegerFieldSingleNode\", func(t *testing.T) {\n\t\tcluster := test.MustRunCluster(t, 1)\n\t\tdefer cluster.Close()\n\t\tcmd := cluster.GetNode(0)\n\t\thost := cmd.URL()\n\t\tholder := cmd.Server.Holder()\n\t\thldr := test.Holder{Holder: holder}\n\n\t\tfldName := \"f\"\n\n\t\t// Load bitmap into cache to ensure cache gets updated.\n\t\tindex := hldr.MustCreateIndexIfNotExists(cluster.Idx(), pilosa.IndexOptions{Keys: true})\n\t\t_, err := index.CreateFieldIfNotExists(fldName, \"\", pilosa.OptFieldTypeInt(-100, 100))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Send import request.\n\t\tc := MustNewClient(host, pilosa.GetHTTPClient(nil))\n\t\treq := &pilosa.ImportValueRequest{\n\t\t\tIndex:      cluster.Idx(),\n\t\t\tField:      \"f\",\n\t\t\tColumnKeys: []string{\"col1\", \"col2\", \"col3\"},\n\t\t\tValues:     []int64{-10, 20, 40},\n\t\t}\n\t\tif err := c.ImportValue(context.Background(), nil, req, &pilosa.ImportOptions{}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Verify range.\n\t\tqueryRequest := &pilosa.QueryRequest{\n\t\t\tQuery:  fmt.Sprintf(`Row(%s>10)`, fldName),\n\t\t\tRemote: false,\n\t\t}\n\n\t\tresult, err := c.Query(context.Background(), cluster.Idx(), queryRequest)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif !reflect.DeepEqual(result.Results[0].(*pilosa.Row).Keys, []string{\"col2\", \"col3\"}) {\n\t\t\tt.Fatalf(\"unexpected column keys: %s\", spew.Sdump(result))\n\t\t}\n\n\t\t// Clear data.\n\t\treq = &pilosa.ImportValueRequest{\n\t\t\tIndex:      cluster.Idx(),\n\t\t\tField:      \"f\",\n\t\t\tColumnKeys: []string{\"col2\"},\n\t\t\tValues:     []int64{20},\n\t\t}\n\t\tif err := c.ImportValue(context.Background(), nil, req, &pilosa.ImportOptions{Clear: true}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Verify Range.\n\t\tqueryRequest = &pilosa.QueryRequest{\n\t\t\tQuery:  fmt.Sprintf(`Row(%s>10)`, fldName),\n\t\t\tRemote: false,\n\t\t}\n\n\t\tresult, err = c.Query(context.Background(), cluster.Idx(), queryRequest)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif !reflect.DeepEqual(result.Results[0].(*pilosa.Row).Keys, []string{\"col3\"}) {\n\t\t\tt.Fatalf(\"unexpected column keys: %s\", spew.Sdump(result))\n\t\t}\n\t})\n}\n\nfunc TestClient_ImportIDs(t *testing.T) {\n\t// Ensure that running a query between two imports does\n\t// not affect the result set. It turns out, this is caused\n\t// by the fragment.rowCache failing to be cleared after an\n\t// importValue. This ensures that the rowCache is cleared\n\t// after an import.\n\tt.Run(\"ImportRangeImport\", func(t *testing.T) {\n\t\tcluster := test.MustRunCluster(t, 1)\n\t\tdefer cluster.Close()\n\t\tcmd := cluster.GetNode(0)\n\t\thost := cmd.URL()\n\t\tholder := cmd.Server.Holder()\n\t\thldr := test.Holder{Holder: holder}\n\n\t\tidxName := cluster.Idx()\n\t\tfldName := \"f\"\n\n\t\t// Load bitmap into cache to ensure cache gets updated.\n\t\tindex := hldr.MustCreateIndexIfNotExists(idxName, pilosa.IndexOptions{Keys: false})\n\t\t_, err := index.CreateFieldIfNotExists(fldName, \"\", pilosa.OptFieldTypeInt(-10000, 10000))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Send import request.\n\t\tc := MustNewClient(host, pilosa.GetHTTPClient(nil))\n\t\treq := &pilosa.ImportValueRequest{\n\t\t\tIndex:     idxName,\n\t\t\tField:     fldName,\n\t\t\tColumnIDs: []uint64{2},\n\t\t\tValues:    []int64{1},\n\t\t}\n\t\tif err := c.ImportValue(context.Background(), nil, req, &pilosa.ImportOptions{}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Verify range.\n\t\tqueryRequest := &pilosa.QueryRequest{\n\t\t\tQuery:  fmt.Sprintf(`Row(%s>0)`, fldName),\n\t\t\tRemote: false,\n\t\t}\n\n\t\tif result, err := c.Query(context.Background(), idxName, queryRequest); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else {\n\t\t\tres := result.Results[0].(*pilosa.Row).Columns()\n\t\t\tif !reflect.DeepEqual(res, []uint64{2}) {\n\t\t\t\tt.Fatalf(\"unexpected column ids: %v\", res)\n\t\t\t}\n\t\t}\n\n\t\t// Send import request.\n\t\treq = &pilosa.ImportValueRequest{\n\t\t\tIndex:     idxName,\n\t\t\tField:     fldName,\n\t\t\tColumnIDs: []uint64{1000},\n\t\t\tValues:    []int64{1},\n\t\t}\n\t\tif err := c.ImportValue(context.Background(), nil, req, &pilosa.ImportOptions{}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Verify range.\n\t\tif result, err := c.Query(context.Background(), idxName, queryRequest); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else {\n\t\t\tres := result.Results[0].(*pilosa.Row).Columns()\n\t\t\tif !reflect.DeepEqual(res, []uint64{2, 1000}) {\n\t\t\t\tt.Fatalf(\"unexpected column ids: %v\", res)\n\t\t\t}\n\t\t}\n\t})\n}\n\n// Ensure client can bulk import value data.\nfunc TestClient_ImportValue(t *testing.T) {\n\tcluster := test.MustRunCluster(t, 1)\n\tdefer cluster.Close()\n\tcmd := cluster.GetNode(0)\n\thost := cmd.URL()\n\tholder := cmd.Server.Holder()\n\thldr := test.Holder{Holder: holder}\n\n\tfldName := \"f\"\n\n\t// Load bitmap into cache to ensure cache gets updated.\n\tindex := hldr.MustCreateIndexIfNotExists(cluster.Idx(), pilosa.IndexOptions{})\n\t_, err := index.CreateFieldIfNotExists(fldName, \"\", pilosa.OptFieldTypeInt(-100, 100))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Send import request.\n\tc := MustNewClient(host, pilosa.GetHTTPClient(nil))\n\treq := &pilosa.ImportValueRequest{\n\t\tIndex:     cluster.Idx(),\n\t\tField:     \"f\",\n\t\tColumnIDs: []uint64{1, 2, 3},\n\t\tValues:    []int64{-10, 20, 40},\n\t}\n\tif err := c.ImportValue(context.Background(), nil, req, &pilosa.ImportOptions{}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Verify Sum.\n\tif resp, err := c.Query(context.Background(), cluster.Idx(), &pilosa.QueryRequest{Query: `Sum(field=f)`}); err != nil {\n\t\tt.Fatal(err)\n\t} else if vc, ok := resp.Results[0].(pilosa.ValCount); !ok {\n\t\tt.Fatalf(\"expected ValCount; got %T\", resp.Results[0])\n\t} else if vc.Val != 50 || vc.Count != 3 {\n\t\tt.Fatalf(\"unexpected values: got sum=%v, count=%v; expected sum=50, cnt=3\", vc.Val, vc.Count)\n\t}\n\n\t// Verify Max.\n\tif resp, err := c.Query(context.Background(), cluster.Idx(), &pilosa.QueryRequest{Query: `Max(field=f)`}); err != nil {\n\t\tt.Fatal(err)\n\t} else if vc, ok := resp.Results[0].(pilosa.ValCount); !ok {\n\t\tt.Fatalf(\"expected ValCount; got %T\", resp.Results[0])\n\t} else if vc.Val != 40 || vc.Count != 1 {\n\t\tt.Fatalf(\"unexpected values: got max=%v, count=%v; expected max=40, cnt=1\", vc.Val, vc.Count)\n\t}\n\n\t// Calculate Data Usage before Import\n\tpreDUsage, err := c.GetDiskUsage(context.Background())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tpreIUsage, err := c.GetIndexUsage(context.Background(), cluster.Idx())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Send import request.\n\treq = &pilosa.ImportValueRequest{\n\t\tIndex:     cluster.Idx(),\n\t\tField:     \"f\",\n\t\tColumnIDs: []uint64{1, 3},\n\t\tValues:    []int64{-10, 40},\n\t}\n\tif err := c.ImportValue(context.Background(), nil, req, &pilosa.ImportOptions{Clear: true}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Check Equivalent growth in data directory and index\n\tpostDUsage, err := c.GetDiskUsage(context.Background())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tpostIUsage, err := c.GetIndexUsage(context.Background(), cluster.Idx())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tfreeSpace := du.NewDiskUsage(\"/dev/disk1s5\").Free()\n\tvprint.VV(\"freespace: %+v\", freeSpace)\n\n\tif (postDUsage.Usage - preDUsage.Usage) != (postIUsage.Usage - preIUsage.Usage) {\n\t\tt.Errorf(\"expected size of data directory to grow the same amount as size of index: Before Import: disk usage: %v, index usage: %v, After Import: disk usage: %v, index usage: %v\", preDUsage.Usage, preIUsage.Usage, postDUsage.Usage, preIUsage.Usage)\n\t\treturn\n\t}\n\tif postDUsage.Usage <= postIUsage.Usage {\n\t\tt.Errorf(\"expected disk usage to be greater than index usage\")\n\t\treturn\n\t}\n\n\t// Verify Sum.\n\tif resp, err := c.Query(context.Background(), cluster.Idx(), &pilosa.QueryRequest{Query: `Sum(field=f)`}); err != nil {\n\t\tt.Fatal(err)\n\t} else if vc, ok := resp.Results[0].(pilosa.ValCount); !ok {\n\t\tt.Fatalf(\"expected ValCount; got %T\", resp.Results[0])\n\t} else if vc.Val != 20 || vc.Count != 1 {\n\t\tt.Fatalf(\"unexpected values: got sum=%v, count=%v; expected sum=20, cnt=1\", vc.Val, vc.Count)\n\t}\n\n\t// Verify Max.\n\tif resp, err := c.Query(context.Background(), cluster.Idx(), &pilosa.QueryRequest{Query: `Max(field=f)`}); err != nil {\n\t\tt.Fatal(err)\n\t} else if vc, ok := resp.Results[0].(pilosa.ValCount); !ok {\n\t\tt.Fatalf(\"expected ValCount; got %T\", resp.Results[0])\n\t} else if vc.Val != 20 || vc.Count != 1 {\n\t\tt.Fatalf(\"unexpected values: got max=%v, count=%v; expected max=20, cnt=1\", vc.Val, vc.Count)\n\t}\n}\n\n// Ensure client can bulk import data while tracking existence.\nfunc TestClient_ImportExistence(t *testing.T) {\n\tcluster := test.MustRunCluster(t, 1)\n\tdefer cluster.Close()\n\tcmd := cluster.GetNode(0)\n\thost := cmd.URL()\n\tholder := cmd.Server.Holder()\n\thldr := test.Holder{Holder: holder}\n\n\tt.Run(\"Set\", func(t *testing.T) {\n\t\tidxName := cluster.Idx(\"s\")\n\t\tfldName := \"fset\"\n\n\t\tindex := hldr.MustCreateIndexIfNotExists(idxName, pilosa.IndexOptions{TrackExistence: true})\n\t\t_, err := index.CreateFieldIfNotExists(fldName, \"\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Send import request.\n\t\tc := MustNewClient(host, pilosa.GetHTTPClient(nil))\n\t\treq := &pilosa.ImportRequest{\n\t\t\tIndex:     cluster.Idx(\"s\"),\n\t\t\tField:     \"fset\",\n\t\t\tColumnIDs: []uint64{1, 5, 6},\n\t\t\tRowIDs:    []uint64{0, 0, 200},\n\t\t}\n\t\tif err := c.Import(context.Background(), nil, req, &pilosa.ImportOptions{}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Verify data.\n\t\tif a := hldr.Row(idxName, fldName, 0).Columns(); !reflect.DeepEqual(a, []uint64{1, 5}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", a)\n\t\t}\n\t\tif a := hldr.Row(idxName, fldName, 200).Columns(); !reflect.DeepEqual(a, []uint64{6}) {\n\t\t\tt.Fatalf(\"unexpected columns: %+v\", a)\n\t\t}\n\n\t\t// Verify existence.\n\t\tif a := hldr.ReadRow(idxName, \"_exists\", 0).Columns(); !reflect.DeepEqual(a, []uint64{1, 5, 6}) {\n\t\t\tt.Fatalf(\"unexpected existence columns: %+v\", a)\n\t\t}\n\t})\n\n\tt.Run(\"Int\", func(t *testing.T) {\n\t\tidxName := cluster.Idx(\"i\")\n\t\tfldName := \"fint\"\n\n\t\tindex := hldr.MustCreateIndexIfNotExists(idxName, pilosa.IndexOptions{TrackExistence: true})\n\t\t_, err := index.CreateFieldIfNotExists(fldName, \"\", pilosa.OptFieldTypeInt(-100, 100))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Send import request.\n\t\tc := MustNewClient(host, pilosa.GetHTTPClient(nil))\n\t\treq := &pilosa.ImportValueRequest{\n\t\t\tIndex:     cluster.Idx(\"i\"),\n\t\t\tField:     \"fint\",\n\t\t\tColumnIDs: []uint64{1, 2, 3},\n\t\t\tValues:    []int64{-10, 20, 40},\n\t\t}\n\t\tif err := c.ImportValue(context.Background(), nil, req, &pilosa.ImportOptions{}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Verify Sum.\n\t\tif resp, err := c.Query(context.Background(), idxName, &pilosa.QueryRequest{Query: fmt.Sprintf(`Sum(field=%s)`, fldName)}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if vc, ok := resp.Results[0].(pilosa.ValCount); !ok {\n\t\t\tt.Fatalf(\"expected ValCount; got %T\", resp.Results[0])\n\t\t} else if vc.Val != 50 || vc.Count != 3 {\n\t\t\tt.Fatalf(\"unexpected values: got sum=%v, count=%v; expected sum=50, cnt=3\", vc.Val, vc.Count)\n\t\t}\n\n\t\t// Verify existence.\n\t\tif a := hldr.ReadRow(idxName, \"_exists\", 0).Columns(); !reflect.DeepEqual(a, []uint64{1, 2, 3}) {\n\t\t\tt.Fatalf(\"unexpected existence columns: %+v\", a)\n\t\t}\n\t})\n}\n\n// Try to request translation data which won't exist.\nfunc TestClient_IndexTranslateDataReader(t *testing.T) {\n\tcluster := test.MustRunCluster(t, 1)\n\tdefer cluster.Close()\n\tcmd := cluster.GetNode(0)\n\n\tholder := cmd.Server.Holder()\n\thldr := test.Holder{Holder: holder}\n\n\thldr.SetBit(\"i\", \"f\", 0, 1)\n\tc := MustNewClient(cmd.URL(), pilosa.GetHTTPClient(nil))\n\treader, err := c.IndexTranslateDataReader(context.Background(), \"i\", 0)\n\tif reader != nil {\n\t\treader.Close()\n\t}\n\tif err == nil {\n\t\tt.Fatalf(\"expected to get an error reading translation data that shouldn't exist\")\n\t}\n}\n\nfunc TestClient_CreateTimeField(t *testing.T) {\n\tcluster := test.MustRunCluster(t, 1)\n\tdefer cluster.Close()\n\tcmd := cluster.GetNode(0)\n\n\tc := MustNewClient(cmd.URL(), pilosa.GetHTTPClient(nil))\n\n\tindex := cluster.Idx()\n\terr := c.CreateIndex(context.Background(), index, pilosa.IndexOptions{})\n\tif err != nil {\n\t\tt.Fatalf(\"creating index: %v\", err)\n\t}\n\n\tfield := \"field\"\n\terr = c.CreateFieldWithOptions(context.Background(), index, field, pilosa.FieldOptions{Type: pilosa.FieldTypeTime, TimeQuantum: \"YMDH\"})\n\tif err != nil {\n\t\tt.Fatalf(\"creating field: %v\", err)\n\t}\n\tfld, err := cmd.API.Field(context.Background(), index, field)\n\tif err != nil {\n\t\tt.Fatalf(\"getting field: %v\", err)\n\t}\n\tif fld.TTL() != 0 {\n\t\tt.Fatalf(\"expected TTL to be 0, got: %+v\", fld.Options().TTL.String())\n\t}\n\n\tfieldTTL := \"field_ttl\"\n\terr = c.CreateFieldWithOptions(context.Background(), index, fieldTTL, pilosa.FieldOptions{Type: pilosa.FieldTypeTime, TimeQuantum: \"YMDH\", TTL: time.Hour})\n\tif err != nil {\n\t\tt.Fatalf(\"creating field: %v\", err)\n\t}\n\tfldTTL, err := cmd.API.Field(context.Background(), index, fieldTTL)\n\tif err != nil {\n\t\tt.Fatalf(\"getting field: %v\", err)\n\t}\n\tif fldTTL.TTL() != time.Hour {\n\t\tt.Fatalf(\"expected TTL 1 hour, got: %+v\", fldTTL.TTL().String())\n\t}\n}\n\nfunc TestClient_CreateDecimalField(t *testing.T) {\n\tcluster := test.MustRunCluster(t, 1)\n\tdefer cluster.Close()\n\tcmd := cluster.GetNode(0)\n\n\tc := MustNewClient(cmd.URL(), pilosa.GetHTTPClient(nil))\n\n\tindex := cluster.Idx()\n\terr := c.CreateIndex(context.Background(), index, pilosa.IndexOptions{})\n\tif err != nil {\n\t\tt.Fatalf(\"creating index: %v\", err)\n\t}\n\tfield := \"dfield\"\n\terr = c.CreateFieldWithOptions(context.Background(), index, field, pilosa.FieldOptions{Type: pilosa.FieldTypeDecimal, Scale: 1, Min: pql.NewDecimal(-1000, 0), Max: pql.NewDecimal(1000, 0)})\n\tif err != nil {\n\t\tt.Fatalf(\"creating field: %v\", err)\n\t}\n\n\tfld, err := cmd.API.Field(context.Background(), index, field)\n\tif err != nil {\n\t\tt.Fatalf(\"getting field: %v\", err)\n\t}\n\tif fld.Options().Scale != 1 {\n\t\tt.Fatalf(\"expected Scale 1, got: %+v\", fld.Options())\n\t}\n\n\terr = c.ImportValue(context.Background(), nil, &pilosa.ImportValueRequest{Index: index, Field: field, ColumnIDs: []uint64{1, 2, 3}, Shard: 0, FloatValues: []float64{1.1, 2.2, 3.3}}, &pilosa.ImportOptions{})\n\tif err != nil {\n\t\tt.Fatalf(\"importing float values: %v\", err)\n\t}\n\n\t// Integer predicate.\n\tresp, err := c.Query(context.Background(), index, &pilosa.QueryRequest{Index: index, Query: \"Row(dfield>2)\"})\n\tif err != nil {\n\t\tt.Fatalf(\"querying: %v\", err)\n\t}\n\tif !reflect.DeepEqual(resp.Results[0].(*pilosa.Row).Columns(), []uint64{2, 3}) {\n\t\tt.Fatalf(\"unexpected results: %v\", resp.Results[0].(*pilosa.Row).Columns())\n\t}\n\n\t// Float predicate.\n\tresp, err = c.Query(context.Background(), index, &pilosa.QueryRequest{Index: index, Query: \"Row(dfield>2.1)\"})\n\tif err != nil {\n\t\tt.Fatalf(\"querying: %v\", err)\n\t}\n\tif !reflect.DeepEqual(resp.Results[0].(*pilosa.Row).Columns(), []uint64{2, 3}) {\n\t\tt.Fatalf(\"unexpected results: %v\", resp.Results[0].(*pilosa.Row).Columns())\n\t}\n\n\t// Integer predicates.\n\tresp, err = c.Query(context.Background(), index, &pilosa.QueryRequest{Index: index, Query: \"Row(1<dfield<3)\"})\n\tif err != nil {\n\t\tt.Fatalf(\"querying: %v\", err)\n\t}\n\tif !reflect.DeepEqual(resp.Results[0].(*pilosa.Row).Columns(), []uint64{1, 2}) {\n\t\tt.Fatalf(\"unexpected results: %v\", resp.Results[0].(*pilosa.Row).Columns())\n\t}\n\n\t// Float predicates.\n\tresp, err = c.Query(context.Background(), index, &pilosa.QueryRequest{Index: index, Query: \"Row(1.1<dfield<3.3)\"})\n\tif err != nil {\n\t\tt.Fatalf(\"querying: %v\", err)\n\t}\n\tif !reflect.DeepEqual(resp.Results[0].(*pilosa.Row).Columns(), []uint64{2}) {\n\t\tt.Fatalf(\"unexpected results: %v\", resp.Results[0].(*pilosa.Row).Columns())\n\t}\n\n\tresp, err = c.Query(context.Background(), index, &pilosa.QueryRequest{Index: index, Query: \"Row(1.1<=dfield<3.3)\"})\n\tif err != nil {\n\t\tt.Fatalf(\"querying: %v\", err)\n\t}\n\tif !reflect.DeepEqual(resp.Results[0].(*pilosa.Row).Columns(), []uint64{1, 2}) {\n\t\tt.Fatalf(\"unexpected results: %v\", resp.Results[0].(*pilosa.Row).Columns())\n\t}\n\n\tresp, err = c.Query(context.Background(), index, &pilosa.QueryRequest{Index: index, Query: \"Row(1.1<dfield<=3.3)\"})\n\tif err != nil {\n\t\tt.Fatalf(\"querying: %v\", err)\n\t}\n\tif !reflect.DeepEqual(resp.Results[0].(*pilosa.Row).Columns(), []uint64{2, 3}) {\n\t\tt.Fatalf(\"unexpected results: %v\", resp.Results[0].(*pilosa.Row).Columns())\n\t}\n\n\tresp, err = c.Query(context.Background(), index, &pilosa.QueryRequest{Index: index, Query: \"Row(dfield<3.3)\"})\n\tif err != nil {\n\t\tt.Fatalf(\"querying: %v\", err)\n\t}\n\tif !reflect.DeepEqual(resp.Results[0].(*pilosa.Row).Columns(), []uint64{1, 2}) {\n\t\tt.Fatalf(\"unexpected results: %v\", resp.Results[0].(*pilosa.Row).Columns())\n\t}\n\n\tresp, err = c.Query(context.Background(), index, &pilosa.QueryRequest{Index: index, Query: \"Row(dfield>2.2)\"})\n\tif err != nil {\n\t\tt.Fatalf(\"querying: %v\", err)\n\t}\n\tif !reflect.DeepEqual(resp.Results[0].(*pilosa.Row).Columns(), []uint64{3}) {\n\t\tt.Fatalf(\"unexpected results: %v\", resp.Results[0].(*pilosa.Row).Columns())\n\t}\n\n\tresp, err = c.Query(context.Background(), index, &pilosa.QueryRequest{Index: index, Query: \"Row(dfield>=2.2)\"})\n\tif err != nil {\n\t\tt.Fatalf(\"querying: %v\", err)\n\t}\n\tif !reflect.DeepEqual(resp.Results[0].(*pilosa.Row).Columns(), []uint64{2, 3}) {\n\t\tt.Fatalf(\"unexpected results: %v\", resp.Results[0].(*pilosa.Row).Columns())\n\t}\n}\n\nfunc TestClientTransactions(t *testing.T) {\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\n\tcoord := c.GetPrimary()\n\tother := c.GetNonPrimary()\n\n\tclient0 := MustNewClient(coord.URL(), pilosa.GetHTTPClient(nil))\n\tclient1 := MustNewClient(other.URL(), pilosa.GetHTTPClient(nil))\n\n\t// can create, list, get, and finish a transaction\n\tvar expDeadline time.Time\n\tif trns, err := client0.StartTransaction(context.Background(), \"blah\", time.Minute, false); err != nil {\n\t\tt.Fatalf(\"error starting transaction: %v\", err)\n\t} else {\n\t\texpDeadline = time.Now().Add(time.Minute)\n\t\ttest.CompareTransactions(t,\n\t\t\t&pilosa.Transaction{ID: \"blah\", Timeout: time.Minute, Active: true, Deadline: expDeadline},\n\t\t\ttrns)\n\t}\n\n\tif trnsMap, err := client0.Transactions(context.Background()); err != nil {\n\t\tt.Errorf(\"listing transactions: %v\", err)\n\t} else {\n\t\tif len(trnsMap) != 1 {\n\t\t\tt.Errorf(\"unexpected trnsMap: %+v\", trnsMap)\n\t\t}\n\t\ttest.CompareTransactions(t,\n\t\t\t&pilosa.Transaction{ID: \"blah\", Timeout: time.Minute, Active: true, Deadline: expDeadline},\n\t\t\ttrnsMap[\"blah\"])\n\t}\n\n\tif trns, err := client0.GetTransaction(context.Background(), \"blah\"); err != nil {\n\t\tt.Fatalf(\"error getting transaction: %v\", err)\n\t} else {\n\t\ttest.CompareTransactions(t,\n\t\t\t&pilosa.Transaction{ID: \"blah\", Timeout: time.Minute, Active: true, Deadline: expDeadline},\n\t\t\ttrns)\n\t}\n\n\tif trns, err := client0.FinishTransaction(context.Background(), \"blah\"); err != nil {\n\t\tt.Fatalf(\"error finishing transaction: %v\", err)\n\t} else {\n\t\ttest.CompareTransactions(t,\n\t\t\t&pilosa.Transaction{ID: \"blah\", Timeout: time.Minute, Active: true, Deadline: expDeadline},\n\t\t\ttrns)\n\t}\n\n\t// can create exclusive transaction\n\tif trns, err := client0.StartTransaction(context.Background(), \"blahe\", time.Minute, true); err != nil {\n\t\tt.Fatalf(\"error starting transaction: %v\", err)\n\t} else {\n\t\texpDeadline = time.Now().Add(time.Minute)\n\t\ttest.CompareTransactions(t,\n\t\t\t&pilosa.Transaction{ID: \"blahe\", Timeout: time.Minute, Active: true, Exclusive: true, Deadline: expDeadline},\n\t\t\ttrns)\n\t}\n\n\t// cannot start new transaction - correct error and exclusive transaction are returned\n\tif trns, err := client0.StartTransaction(context.Background(), \"blah\", time.Minute, false); errors.Cause(err) != pilosa.ErrTransactionExclusive {\n\t\tt.Fatalf(\"shouldn't be able to start transaction while an exclusive is running, but got: %+v, %v\", trns, err)\n\t} else {\n\t\ttest.CompareTransactions(t,\n\t\t\t&pilosa.Transaction{ID: \"blahe\", Timeout: time.Minute, Active: true, Exclusive: true, Deadline: expDeadline},\n\t\t\ttrns)\n\t}\n\n\t// finish exclusive transaction\n\tif trns, err := client0.FinishTransaction(context.Background(), \"blahe\"); err != nil {\n\t\tt.Fatalf(\"error finishing transaction: %v\", err)\n\t} else {\n\t\ttest.CompareTransactions(t,\n\t\t\t&pilosa.Transaction{ID: \"blahe\", Timeout: time.Minute, Active: true, Exclusive: true, Deadline: expDeadline},\n\t\t\ttrns)\n\t}\n\n\t// start new transaction\n\tif trns, err := client0.StartTransaction(context.Background(), \"blah\", time.Minute, false); err != nil {\n\t\tt.Fatalf(\"error starting transaction: %v\", err)\n\t} else {\n\t\texpDeadline = time.Now().Add(time.Minute)\n\t\ttest.CompareTransactions(t,\n\t\t\t&pilosa.Transaction{ID: \"blah\", Timeout: time.Minute, Active: true, Deadline: expDeadline},\n\t\t\ttrns)\n\t}\n\n\t// try to start same transaction\n\tif trns, err := client0.StartTransaction(context.Background(), \"blah\", time.Minute, false); err == nil ||\n\t\t!strings.Contains(err.Error(), pilosa.ErrTransactionExists.Error()) {\n\t\tt.Fatalf(\"expected ErrTransactionExists, but got: %v\", err)\n\t} else {\n\t\ttest.CompareTransactions(t,\n\t\t\t&pilosa.Transaction{ID: \"blah\", Timeout: time.Minute, Active: true, Deadline: expDeadline},\n\t\t\ttrns)\n\t}\n\n\t// start an exclusive transaction which can't go active\n\tif trns, err := client0.StartTransaction(context.Background(), \"blahe\", time.Minute, true); err != nil {\n\t\tt.Fatalf(\"error starting transaction: %v\", err)\n\t} else {\n\t\texpDeadline = time.Now().Add(time.Minute)\n\t\ttest.CompareTransactions(t,\n\t\t\t&pilosa.Transaction{ID: \"blahe\", Timeout: time.Minute, Active: false, Exclusive: true, Deadline: expDeadline},\n\t\t\ttrns)\n\t}\n\n\t// finish exclusive transaction that never went active\n\tif trns, err := client0.FinishTransaction(context.Background(), \"blahe\"); err != nil {\n\t\tt.Fatalf(\"error finishing transaction: %v\", err)\n\t} else {\n\t\ttest.CompareTransactions(t,\n\t\t\t&pilosa.Transaction{ID: \"blahe\", Timeout: time.Minute, Active: false, Exclusive: true, Deadline: expDeadline},\n\t\t\ttrns)\n\t}\n\n\t// finish non-existent transaction\n\tif trns, err := client0.FinishTransaction(context.Background(), \"zzz\"); err == nil ||\n\t\t!strings.Contains(err.Error(), pilosa.ErrTransactionNotFound.Error()) {\n\t\tt.Fatalf(\"unexpected error finishing nonexistent transaction: %v\", err)\n\t} else {\n\t\ttest.CompareTransactions(t,\n\t\t\tnil,\n\t\t\ttrns)\n\t}\n\n\t// get non-existent transaction\n\tif trns, err := client0.GetTransaction(context.Background(), \"xxx\"); err == nil ||\n\t\t!strings.Contains(err.Error(), pilosa.ErrTransactionNotFound.Error()) {\n\t\tt.Fatalf(\"unexpected error getting nonexistent transaction: %v\", err)\n\t} else {\n\t\ttest.CompareTransactions(t,\n\t\t\tnil,\n\t\t\ttrns)\n\t}\n\n\t// non-primary\n\tif trns, err := client1.StartTransaction(context.Background(), \"blah\", time.Minute, false); err != nil {\n\t\tt.Fatalf(\"unexpected error starting on non-primary: %v\", err)\n\t} else {\n\t\ttest.CompareTransactions(t,\n\t\t\t&pilosa.Transaction{ID: \"blah\", Timeout: time.Minute, Active: true, Exclusive: false, Deadline: expDeadline},\n\t\t\ttrns)\n\t}\n\n\t// start transaction with blank id\n\tif trns, err := client0.StartTransaction(context.Background(), \"\", time.Minute, false); err != nil {\n\t\tt.Fatalf(\"error starting transaction: %v\", err)\n\t} else {\n\t\texpDeadline = time.Now().Add(time.Minute)\n\t\tif len(trns.ID) != 36 {\n\t\t\tt.Errorf(\"expected generated UUID, but got '%s'\", trns.ID)\n\t\t}\n\t\ttest.CompareTransactions(t,\n\t\t\t&pilosa.Transaction{ID: trns.ID, Timeout: time.Minute, Active: true, Deadline: expDeadline},\n\t\t\ttrns)\n\t}\n\n}\n\n// Client represents a test wrapper for pilosa.Client.\ntype Client struct {\n\t*pilosa.InternalClient\n}\n\n// MustNewClient returns a new instance of Client. Panic on error.\nfunc MustNewClient(host string, h *gohttp.Client) *Client {\n\tc, err := pilosa.NewInternalClient(host, h, pilosa.WithSerializer(proto.Serializer{}))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn &Client{InternalClient: c}\n}\n\nfunc makeImportRoaringRequest(clear bool, viewData string) *pilosa.ImportRoaringRequest {\n\troaringData, _ := hex.DecodeString(viewData)\n\treturn &pilosa.ImportRoaringRequest{\n\t\tClear: clear,\n\t\tViews: map[string][]byte{\n\t\t\t\"\": roaringData,\n\t\t},\n\t}\n}\n\n// verify that serverInfo has Backend\nfunc TestClient_ServerInfoHasBackend(t *testing.T) {\n\t//srcs := []string{\"roaring\", \"rbf\", \"lmdb\"}\n\tcluster := test.MustRunCluster(t, 1)\n\tdefer cluster.Close()\n\tcmd := cluster.GetNode(0)\n\tsi := cmd.API.Info()\n\tif si.StorageBackend == \"\" {\n\t\tpanic(\"should have gotten a StorageBackend back\")\n\t}\n\tpilosa.MustBackendToTxtype(si.StorageBackend) // panics if invalid\n}\nfunc TestClient_ImportRoaringExists(t *testing.T) {\n\tcluster := test.MustRunCluster(t, 1)\n\tdefer cluster.Close()\n\n\tnode := cluster.GetNode(0)\n\t_, err := node.API.CreateIndex(context.Background(), cluster.Idx(), pilosa.IndexOptions{TrackExistence: true})\n\tif err != nil {\n\t\tt.Fatalf(\"creating index: %v\", err)\n\t}\n\t_, err = node.API.CreateField(context.Background(), cluster.Idx(), \"f\", pilosa.OptFieldTypeSet(pilosa.CacheTypeRanked, 100))\n\tif err != nil {\n\t\tt.Fatalf(\"creating field: %v\", err)\n\t}\n\t// Send import request.\n\thost := node.URL()\n\tc := MustNewClient(host, pilosa.GetHTTPClient(nil))\n\t// [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 65537]\n\troaringReq := makeImportRoaringRequest(false, \"3B3001000100000900010000000100010009000100\")\n\n\tif err := c.ImportRoaring(context.Background(), &cluster.GetNode(0).API.Node().URI, cluster.Idx(), \"f\", 0, false, roaringReq); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tqr, err := node.API.Query(context.Background(), &pilosa.QueryRequest{Index: cluster.Idx(), Query: \"All()\"})\n\tif err != nil {\n\t\tt.Fatalf(\" %v \", err)\n\t}\n\tgot := qr.Results[0].(*pilosa.Row).Columns()\n\tif !reflect.DeepEqual(got, []uint64{}) {\n\t\tt.Fatalf(\" Row unexpected columns: got %+v  expected: %+v\", got, []uint64{})\n\t}\n\troaringReq.UpdateExistence = true\n\tif err := c.ImportRoaring(context.Background(), &cluster.GetNode(0).API.Node().URI, cluster.Idx(), \"f\", 0, false, roaringReq); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\texpected := []uint64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 65537}\n\tqr, err = node.API.Query(context.Background(), &pilosa.QueryRequest{Index: cluster.Idx(), Query: \"All()\"})\n\tif err != nil {\n\t\tt.Fatalf(\"Query error: %+v\", err)\n\t}\n\tgot = qr.Results[0].(*pilosa.Row).Columns()\n\tif !reflect.DeepEqual(got, expected) {\n\t\tt.Fatalf(\"All unexpected columns: got %+v  expected: %+v\", got, expected)\n\t}\n\n}\n\nfunc TestAddAuthToken(t *testing.T) {\n\tt.Run(\"none\", func(t *testing.T) {\n\t\treq, err := gohttp.NewRequest(\"GET\", \"dontmatternone\", strings.NewReader(\"this doesn't matter\"))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t}\n\t\tpilosa.AddAuthToken(context.Background(), &req.Header)\n\t\tif req.Header.Get(\"Authorization\") != \"\" {\n\t\t\tt.Fatalf(\"Authorization header set when it should be empty\")\n\t\t}\n\t})\n\tt.Run(\"userinfo\", func(t *testing.T) {\n\t\treq, err := gohttp.NewRequest(\"GET\", \"dontmatternone\", strings.NewReader(\"this doesn't matter\"))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t}\n\t\tuinfo := &authn.UserInfo{Token: \"ayo\"}\n\t\tpilosa.AddAuthToken(authn.WithUserInfo(context.Background(), uinfo), &req.Header)\n\t\tif got := req.Header.Get(\"Authorization\"); got != \"Bearer \"+uinfo.Token {\n\t\t\tt.Fatalf(\"got '%v', expected 'Bearer %v'\", got, uinfo.Token)\n\t\t}\n\t})\n\tt.Run(\"token\", func(t *testing.T) {\n\t\treq, err := gohttp.NewRequest(\"GET\", \"dontmatternone\", strings.NewReader(\"this doesn't matter\"))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t}\n\t\ttok := \"Bearer thisisatoken\"\n\t\tpilosa.AddAuthToken(\n\t\t\tauthn.WithAccessToken(context.Background(), tok),\n\t\t\t&req.Header,\n\t\t)\n\t\tif got := req.Header.Get(\"Authorization\"); got != tok {\n\t\t\tt.Fatalf(\"got '%v', expected '%v'\", got, tok)\n\t\t}\n\t})\n\tt.Run(\"originalIP\", func(t *testing.T) {\n\t\treq, err := gohttp.NewRequest(\"GET\", \"dontmatternone\", strings.NewReader(\"this doesn't matter\"))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t\t}\n\t\togIP := \"10.0.0.1\"\n\t\tpilosa.AddAuthToken(fbcontext.WithOriginalIP(context.Background(), ogIP), &req.Header)\n\t\tif got := req.Header.Get(pilosa.OriginalIPHeader); got != ogIP {\n\t\t\tt.Fatalf(\"got '%v', expected '%v'\", got, ogIP)\n\t\t}\n\t})\n}\n"
        },
        {
          "name": "iterator.go",
          "type": "blob",
          "size": 2.9150390625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"fmt\"\n)\n\n// iterator is an interface for looping over row/column pairs.\ntype iterator interface {\n\tSeek(rowID, columnID uint64)\n\tNext() (rowID, columnID uint64, eof bool)\n}\n\n// bufIterator wraps an iterator to provide the ability to unread values.\ntype bufIterator struct {\n\tbuf struct {\n\t\trowID    uint64\n\t\tcolumnID uint64\n\t\teof      bool\n\t\tfull     bool\n\t}\n\titr iterator\n}\n\n// newBufIterator returns a buffered iterator that wraps itr.\nfunc newBufIterator(itr iterator) *bufIterator {\n\treturn &bufIterator{itr: itr}\n}\n\n// Seek moves to the first pair equal to or greater than pseek/bseek.\nfunc (itr *bufIterator) Seek(rowID, columnID uint64) {\n\titr.buf.full = false\n\titr.itr.Seek(rowID, columnID)\n}\n\n// Next returns the next pair in the row.\n// If a value has been buffered then it is returned and the buffer is cleared.\nfunc (itr *bufIterator) Next() (rowID, columnID uint64, eof bool) {\n\tif itr.buf.full {\n\t\titr.buf.full = false\n\t\treturn itr.buf.rowID, itr.buf.columnID, itr.buf.eof\n\t}\n\n\t// Read values onto buffer in case of unread.\n\titr.buf.rowID, itr.buf.columnID, itr.buf.eof = itr.itr.Next()\n\n\treturn itr.buf.rowID, itr.buf.columnID, itr.buf.eof\n}\n\n// Peek reads the next value but leaves it on the buffer.\nfunc (itr *bufIterator) Peek() (rowID, columnID uint64, eof bool) {\n\trowID, columnID, eof = itr.Next()\n\titr.Unread()\n\treturn\n}\n\n// Unread pushes previous pair on to the buffer.\n// Panics if the buffer is already full.\nfunc (itr *bufIterator) Unread() {\n\tif itr.buf.full {\n\t\tpanic(\"pilosa.BufIterator: buffer full\")\n\t}\n\titr.buf.full = true\n}\n\n// sliceIterator iterates over a pair of row/column ID slices.\ntype sliceIterator struct {\n\trowIDs    []uint64\n\tcolumnIDs []uint64\n\n\ti, n int\n}\n\n// newSliceIterator returns an iterator to iterate over a set of row/column ID pairs.\n// Both slices MUST have an equal length. Otherwise the function will panic.\nfunc newSliceIterator(rowIDs, columnIDs []uint64) *sliceIterator {\n\tif len(columnIDs) != len(rowIDs) {\n\t\tpanic(fmt.Sprintf(\"pilosa.SliceIterator: pair length mismatch: %d != %d\", len(rowIDs), len(columnIDs)))\n\t}\n\n\treturn &sliceIterator{\n\t\trowIDs:    rowIDs,\n\t\tcolumnIDs: columnIDs,\n\n\t\tn: len(rowIDs),\n\t}\n}\n\n// Seek moves the cursor to a given pair.\n// If the pair is not found, the iterator seeks to the next pair.\nfunc (itr *sliceIterator) Seek(bseek, pseek uint64) {\n\tfor i := 0; i < itr.n; i++ {\n\t\trowID := itr.rowIDs[i]\n\t\tcolumnID := itr.columnIDs[i]\n\n\t\tif (bseek == rowID && pseek <= columnID) || bseek < rowID {\n\t\t\titr.i = i\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Seek to the end of the slice if all values are less than seek pair.\n\titr.i = itr.n\n}\n\n// Next returns the next row/column ID pair.\nfunc (itr *sliceIterator) Next() (rowID, columnID uint64, eof bool) {\n\tif itr.i >= itr.n {\n\t\treturn 0, 0, true\n\t}\n\n\trowID = itr.rowIDs[itr.i]\n\tcolumnID = itr.columnIDs[itr.i]\n\n\titr.i++\n\treturn rowID, columnID, false\n}\n"
        },
        {
          "name": "iterator_internal_test.go",
          "type": "blob",
          "size": 1.84765625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"reflect\"\n\t\"testing\"\n)\n\n// Ensure slice iterator and iterate over a set of pairs.\nfunc TestSliceIterator(t *testing.T) {\n\t// Initialize iterator.\n\titr := newSliceIterator(\n\t\t[]uint64{0, 0, 2, 4},\n\t\t[]uint64{0, 1, 0, 10},\n\t)\n\n\t// Iterate over all pairs.\n\tvar pairs [][2]uint64\n\tfor pid, bid, eof := itr.Next(); !eof; pid, bid, eof = itr.Next() {\n\t\tpairs = append(pairs, [2]uint64{pid, bid})\n\t}\n\n\t// Verify pairs output correctly.\n\tif !reflect.DeepEqual(pairs, [][2]uint64{\n\t\t{0, 0},\n\t\t{0, 1},\n\t\t{2, 0},\n\t\t{4, 10},\n\t}) {\n\t\tt.Fatalf(\"unexpected pairs: %+v\", pairs)\n\t}\n}\n\n// Ensure buffered iterator can unread values on to the buffer.\nfunc TestBufIterator(t *testing.T) {\n\titr := newBufIterator(newSliceIterator(\n\t\t[]uint64{0, 0, 1, 2},\n\t\t[]uint64{1, 3, 0, 100},\n\t))\n\titr.Seek(0, 2)\n\tif pid, bid, eof := itr.Next(); pid != 0 || bid != 3 || eof {\n\t\tt.Fatalf(\"unexpected seek: (%d, %d, %v)\", pid, bid, eof)\n\t} else if pid, bid, eof := itr.Next(); pid != 1 || bid != 0 || eof {\n\t\tt.Fatalf(\"unexpected next: (%d, %d, %v)\", pid, bid, eof)\n\t}\n\n\titr.Unread()\n\tif pid, bid, eof := itr.Next(); pid != 1 || bid != 0 || eof {\n\t\tt.Fatalf(\"unexpected next(buffered): (%d, %d, %v)\", pid, bid, eof)\n\t}\n\n\tif pid, bid, eof := itr.Next(); pid != 2 || bid != 100 || eof {\n\t\tt.Fatalf(\"unexpected next: (%d, %d, %v)\", pid, bid, eof)\n\t} else if _, _, eof := itr.Next(); !eof {\n\t\tt.Fatal(\"expected eof\")\n\t}\n}\n\n// Ensure buffered iterator will panic if unreading onto a full buffer.\nfunc TestBufIterator_DoubleFillPanic(t *testing.T) {\n\tvar v interface{}\n\tfunc() {\n\t\tdefer func() { v = recover() }()\n\n\t\titr := newBufIterator(newSliceIterator(nil, nil))\n\t\titr.Unread()\n\t\titr.Unread()\n\t}()\n\n\tif !reflect.DeepEqual(v, \"pilosa.BufIterator: buffer full\") {\n\t\tt.Fatalf(\"unexpected panic value: %#v\", v)\n\t}\n}\n"
        },
        {
          "name": "lattice",
          "type": "tree",
          "content": null
        },
        {
          "name": "license.exceptions",
          "type": "blob",
          "size": 0.57421875,
          "content": "# names of files which we do not expect to have our license header\n./apimethod_string.go\n./pql/pql.peg.go\n./pb/private.pb.go\n./pb/public.pb.go\n./lru/lru.go\n./roaring/btree.go\n./roaring/btree_test.go\n./roaring/containerarchetype_string.go\n./proto/pilosa.pb.go\n./logger/filewriter.go\n./logger/filewriter_test.go\n./vprint.go\n./vprint_test.go\n./rbf/vprint.go\n./rbf/page_map.go\n./cmd/slurp/vprint.go\n./cmd/badloader/vprint.go\n./gid.go\n./synthload/vprint.go\n./proto/vdsm/vdsm.proto\n./proto/vdsm/vdsm.pb.go\n./cmd/pilosa-fsck/vprint.go\n./cmd/random-query/vprint.go\n./barrier.go\n./barrier_test.go\n"
        },
        {
          "name": "like.go",
          "type": "blob",
          "size": 5.515625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"bytes\"\n\t\"strings\"\n\t\"unicode/utf8\"\n)\n\n// tokenizeLike turns a \"like\" pattern into a list of tokens.\n// Every token is either a string to exactly match or a combination of % and _ placeholders.\nfunc tokenizeLike(like string) []string {\n\tvar tokens []string\n\tfor like != \"\" {\n\t\tvar token string\n\t\ti := strings.IndexAny(like, \"%_\")\n\t\tswitch i {\n\t\tcase 0:\n\t\t\t// Generate a token of placeholders.\n\n\t\t\t// Iterate bytewise over the string to find the end of the token.\n\t\t\t// The % and _ characters are ASCII, so we do not have to worry about Unicode right here.\n\t\t\tj := 1\n\t\t\tfor j < len(like) && (like[j] == '%' || like[j] == '_') {\n\t\t\t\tj++\n\t\t\t}\n\t\t\ttoken, like = like[:j], like[j:]\n\t\tcase -1:\n\t\t\t// There are no more placeholders - generate the last token.\n\t\t\ttoken, like = like, \"\"\n\t\tdefault:\n\t\t\t// Generate an exact match token.\n\t\t\ttoken, like = like[:i], like[i:]\n\t\t}\n\t\ttokens = append(tokens, token)\n\t}\n\treturn tokens\n}\n\n// filterStepKind is a kind of step in a like filter.\ntype filterStepKind uint8\n\nconst (\n\tfilterStepPrefix      filterStepKind = iota // x...\n\tfilterStepSkipN                             // __...\n\tfilterStepSkipThrough                       // %x...\n\tfilterStepSuffix                            // %x\n\tfilterStepMinLength                         // _%\n)\n\n// filterStep is a step in a like filter.\ntype filterStep struct {\n\t// kind is the step kind.\n\tkind filterStepKind\n\n\t// str is the substring for a prefix/skipthrough/suffix step.\n\tstr []byte\n\n\t// n is the number of underscores in the step (if relevant).\n\tn int\n}\n\n// planLike generates a filtering plan for a like pattern.\nfunc planLike(like string) []filterStep {\n\t// Tokenize the like pattern.\n\ttokens := tokenizeLike(like)\n\n\tsteps := make([]filterStep, 0, len(tokens))\n\tvar merged bool\n\tfor i, t := range tokens {\n\t\tif merged {\n\t\t\t// The token was already merged into the previous step.\n\t\t\tmerged = false\n\t\t\tcontinue\n\t\t}\n\n\t\t// Convert the token to a step.\n\t\tvar step filterStep\n\t\thasPercent := strings.ContainsRune(t, '%')\n\t\tunderscores := strings.Count(t, \"_\")\n\t\tswitch {\n\t\tcase hasPercent && i+1 < len(tokens):\n\t\t\t// Generate a step to skip through the next token.\n\t\t\tstep = filterStep{\n\t\t\t\tkind: filterStepSkipThrough,\n\t\t\t\tstr:  []byte(tokens[i+1]),\n\t\t\t\tn:    underscores,\n\t\t\t}\n\t\t\tmerged = true\n\t\tcase hasPercent:\n\t\t\t// Generate a terminating step to absorb the remainder of the string.\n\t\t\tstep = filterStep{\n\t\t\t\tkind: filterStepMinLength,\n\t\t\t\tn:    underscores,\n\t\t\t}\n\t\tcase underscores > 0:\n\t\t\t// Generate a step to absorb _ placeholders.\n\t\t\tstep = filterStep{\n\t\t\t\tkind: filterStepSkipN,\n\t\t\t\tn:    underscores,\n\t\t\t}\n\t\tdefault:\n\t\t\t// Generate a step to process an exact match of the beginning of a string.\n\t\t\tstep = filterStep{\n\t\t\t\tkind: filterStepPrefix,\n\t\t\t\tstr:  []byte(t),\n\t\t\t}\n\t\t}\n\t\tsteps = append(steps, step)\n\t}\n\n\t// Optimize suffix matching.\n\tif len(steps) > 0 && steps[len(steps)-1].kind == filterStepSkipThrough {\n\t\tsteps[len(steps)-1].kind = filterStepSuffix\n\t}\n\n\treturn steps\n}\n\n// matchLike matches a string using a like plan.\nfunc matchLike(key []byte, like ...filterStep) bool {\n\tfor i, step := range like {\n\t\tswitch step.kind {\n\t\tcase filterStepPrefix:\n\t\t\t// Match a prefix.\n\t\t\tif !bytes.HasPrefix(key, step.str) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tkey = key[len(step.str):]\n\t\tcase filterStepSkipN:\n\t\t\t// Skip some placeholders.\n\t\t\tn := step.n\n\t\t\tfor j := 0; j < n; j++ {\n\t\t\t\t_, len := utf8.DecodeRune(key)\n\t\t\t\tif len == 0 {\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t\tkey = key[len:]\n\t\t\t}\n\t\tcase filterStepSkipThrough:\n\t\t\t// Skip through a string.\n\n\t\t\t// Skip through placeholders.\n\t\t\tvar skipped int\n\t\t\tfor skipped < step.n {\n\t\t\t\tj := bytes.Index(key, step.str)\n\t\t\t\tswitch j {\n\t\t\t\tcase -1:\n\t\t\t\t\t// There are no more matches.\n\t\t\t\t\treturn false\n\t\t\t\tcase 0:\n\t\t\t\t\t// Skip a single rune to ensure forward progress.\n\t\t\t\t\t// This is somewhat inefficient since we have to search the string again next time.\n\t\t\t\t\t// This will hopefully not have to be used very frequently.\n\t\t\t\t\t_, len := utf8.DecodeRune(key)\n\t\t\t\t\tkey = key[len:]\n\t\t\t\t\tskipped += len\n\t\t\t\tdefault:\n\t\t\t\t\t// Skip until the substring and count the skipped runes.\n\t\t\t\t\tk := -1\n\t\t\t\t\tfor k = range key[:j] {\n\t\t\t\t\t}\n\t\t\t\t\tskipped += k + 1\n\n\t\t\t\t\tkey = key[j:]\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Iterate through the substring matches until the rest of the pattern matches.\n\t\t\tremaining := like[i+1:]\n\t\t\tfor {\n\t\t\t\t// Find the next substring match.\n\t\t\t\tj := bytes.Index(key, step.str)\n\t\t\t\tswitch {\n\t\t\t\tcase j == -1:\n\t\t\t\t\t// There are no more matches.\n\t\t\t\t\treturn false\n\t\t\t\tcase j > 0:\n\t\t\t\t\t// Skip the data before the substring.\n\t\t\t\t\tkey = key[j:]\n\t\t\t\t}\n\n\t\t\t\t// Apply the rest of the filter.\n\t\t\t\tif matchLike(key[len(step.str):], remaining...) {\n\t\t\t\t\t// This instance matches, no need to search any more.\n\t\t\t\t\treturn true\n\t\t\t\t}\n\n\t\t\t\t// Skip the first rune of the substring so we do not rescan this substring match.\n\t\t\t\t_, len := utf8.DecodeRune(key)\n\t\t\t\tkey = key[len:]\n\t\t\t}\n\t\tcase filterStepSuffix:\n\t\t\t// Match a suffix.\n\t\t\tif !bytes.HasSuffix(key, step.str) {\n\t\t\t\t// Suffix not present.\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tif step.n <= 0 {\n\t\t\t\t// No skip length check necessary.\n\t\t\t\treturn true\n\t\t\t}\n\n\t\t\t// Check length of the substring before the suffix.\n\t\t\tkey = key[:len(key)-len(step.str)]\n\t\t\tfallthrough\n\t\tcase filterStepMinLength:\n\t\t\tif len(key) < step.n {\n\t\t\t\t// The string is definitely too short.\n\t\t\t\treturn false\n\t\t\t}\n\n\t\t\t// Check if the string is long enough.\n\t\t\treturn utf8.RuneCount(key) >= step.n\n\t\tdefault:\n\t\t\tpanic(\"invalid step\")\n\t\t}\n\t}\n\n\t// If there is any unmatched data left, this is not a match.\n\treturn len(key) == 0\n}\n"
        },
        {
          "name": "like_test.go",
          "type": "blob",
          "size": 4.224609375,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"reflect\"\n\t\"testing\"\n)\n\nfunc TestPlanLike(t *testing.T) {\n\tcases := []struct {\n\t\tname            string\n\t\tlike            string\n\t\tplan            []filterStep\n\t\tmatch, nonmatch []string\n\t}{\n\t\t{\n\t\t\tname:     \"Empty\",\n\t\t\tlike:     \"\",\n\t\t\tplan:     []filterStep{},\n\t\t\tmatch:    []string{\"\"},\n\t\t\tnonmatch: []string{\"a\", \" \"},\n\t\t},\n\t\t{\n\t\t\tname: \"Exact\",\n\t\t\tlike: \"x\",\n\t\t\tplan: []filterStep{\n\t\t\t\t{\n\t\t\t\t\tkind: filterStepPrefix,\n\t\t\t\t\tstr:  []byte(\"x\"),\n\t\t\t\t},\n\t\t\t},\n\t\t\tmatch:    []string{\"x\"},\n\t\t\tnonmatch: []string{\"\", \"y\", \"z\", \"xy\", \"yx\"},\n\t\t},\n\t\t{\n\t\t\tname: \"Anything\",\n\t\t\tlike: \"%\",\n\t\t\tplan: []filterStep{\n\t\t\t\t{\n\t\t\t\t\tkind: filterStepMinLength,\n\t\t\t\t\tn:    0,\n\t\t\t\t},\n\t\t\t},\n\t\t\tmatch: []string{\"\", \"a\", \"b\", \"ab\"},\n\t\t},\n\t\t{\n\t\t\tname: \"Prefix\",\n\t\t\tlike: \"x%\",\n\t\t\tplan: []filterStep{\n\t\t\t\t{\n\t\t\t\t\tkind: filterStepPrefix,\n\t\t\t\t\tstr:  []byte(\"x\"),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tkind: filterStepMinLength,\n\t\t\t\t\tn:    0,\n\t\t\t\t},\n\t\t\t},\n\t\t\tmatch:    []string{\"xy\", \"xyz\", \"xyzzy\"},\n\t\t\tnonmatch: []string{\"plugh\", \"yx\", \"\"},\n\t\t},\n\t\t{\n\t\t\tname: \"Suffix\",\n\t\t\tlike: \"%x\",\n\t\t\tplan: []filterStep{\n\t\t\t\t{\n\t\t\t\t\tkind: filterStepSuffix,\n\t\t\t\t\tstr:  []byte(\"x\"),\n\t\t\t\t},\n\t\t\t},\n\t\t\tmatch:    []string{\"x\", \"xx\", \"ax\"},\n\t\t\tnonmatch: []string{\"\", \"a\", \"x^\"},\n\t\t},\n\t\t{\n\t\t\tname: \"Sandwich\",\n\t\t\tlike: \"x%y\",\n\t\t\tplan: []filterStep{\n\t\t\t\t{\n\t\t\t\t\tkind: filterStepPrefix,\n\t\t\t\t\tstr:  []byte(\"x\"),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tkind: filterStepSuffix,\n\t\t\t\t\tstr:  []byte(\"y\"),\n\t\t\t\t},\n\t\t\t},\n\t\t\tmatch:    []string{\"xy\", \"xzy\", \"xyzzy\"},\n\t\t\tnonmatch: []string{\"plugh\", \".xy.\", \".x.y\", \"x.y.\"},\n\t\t},\n\t\t{\n\t\t\tname: \"DoubleDeckerSandwich\",\n\t\t\tlike: \"x%y%z\",\n\t\t\tplan: []filterStep{\n\t\t\t\t{\n\t\t\t\t\tkind: filterStepPrefix,\n\t\t\t\t\tstr:  []byte(\"x\"),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tkind: filterStepSkipThrough,\n\t\t\t\t\tstr:  []byte(\"y\"),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tkind: filterStepSuffix,\n\t\t\t\t\tstr:  []byte(\"z\"),\n\t\t\t\t},\n\t\t\t},\n\t\t\tmatch:    []string{\"xyz\", \"xzyzz\", \"x.y.z\", \"x.y.y..z\"},\n\t\t\tnonmatch: []string{\"plugh\", \".xyz.\", \".x.y.z\", \"x.y.z.\"},\n\t\t},\n\t\t{\n\t\t\tname: \"Skips\",\n\t\t\tlike: \"a_b_%_c_%_%_d\",\n\t\t\tplan: []filterStep{\n\t\t\t\t{\n\t\t\t\t\tkind: filterStepPrefix,\n\t\t\t\t\tstr:  []byte(\"a\"),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tkind: filterStepSkipN,\n\t\t\t\t\tn:    1,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tkind: filterStepPrefix,\n\t\t\t\t\tstr:  []byte(\"b\"),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tkind: filterStepSkipThrough,\n\t\t\t\t\tstr:  []byte(\"c\"),\n\t\t\t\t\tn:    2,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tkind: filterStepSuffix,\n\t\t\t\t\tstr:  []byte(\"d\"),\n\t\t\t\t\tn:    3,\n\t\t\t\t},\n\t\t\t},\n\t\t\tmatch:    []string{\"a1b234c5678d\"},\n\t\t\tnonmatch: []string{\"abcd\", \"a1b2345678d\", \"a1b2c5678d\"},\n\t\t},\n\t\t{\n\t\t\tname: \"SingleRune\",\n\t\t\tlike: \"_\",\n\t\t\tplan: []filterStep{\n\t\t\t\t{\n\t\t\t\t\tkind: filterStepSkipN,\n\t\t\t\t\tn:    1,\n\t\t\t\t},\n\t\t\t},\n\t\t\tmatch:    []string{\"a\", \"\", \"\"},\n\t\t\tnonmatch: []string{\"ab\", \"a\", \"h\", \"\"},\n\t\t},\n\t\t{\n\t\t\tname: \"DoubleRune\",\n\t\t\tlike: \"__\",\n\t\t\tplan: []filterStep{\n\t\t\t\t{\n\t\t\t\t\tkind: filterStepSkipN,\n\t\t\t\t\tn:    2,\n\t\t\t\t},\n\t\t\t},\n\t\t\tmatch:    []string{\"ab\", \"a\", \"h\"},\n\t\t\tnonmatch: []string{\"a\", \"\", \"\", \"abc\"},\n\t\t},\n\t\t{\n\t\t\tname: \"MiddleBlank\",\n\t\t\tlike: \"x_y\",\n\t\t\tplan: []filterStep{\n\t\t\t\t{\n\t\t\t\t\tkind: filterStepPrefix,\n\t\t\t\t\tstr:  []byte(\"x\"),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tkind: filterStepSkipN,\n\t\t\t\t\tn:    1,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tkind: filterStepPrefix,\n\t\t\t\t\tstr:  []byte(\"y\"),\n\t\t\t\t},\n\t\t\t},\n\t\t\tmatch:    []string{\"x.y\", \"xay\", \"x y\", \"xy\"},\n\t\t\tnonmatch: []string{\"x++y\", \"\", \"a\"},\n\t\t},\n\t\t{\n\t\t\tname: \"MinLength\",\n\t\t\tlike: \"_%_\",\n\t\t\tplan: []filterStep{\n\t\t\t\t{\n\t\t\t\t\tkind: filterStepMinLength,\n\t\t\t\t\tn:    2,\n\t\t\t\t},\n\t\t\t},\n\t\t\tmatch:    []string{\"ab\", \"a\", \"abc\", \"pilosa\"},\n\t\t\tnonmatch: []string{\"h\", \"\", \".\", \"\"},\n\t\t},\n\t}\n\tt.Run(\"Plan\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tfor _, c := range cases {\n\t\t\tc := c\n\t\t\tt.Run(c.name, func(t *testing.T) {\n\t\t\t\tt.Parallel()\n\n\t\t\t\tplan := planLike(c.like)\n\t\t\t\tif !reflect.DeepEqual(plan, c.plan) {\n\t\t\t\t\tt.Errorf(\"incorrect plan: %v\", plan)\n\t\t\t\t}\n\t\t\t})\n\t\t}\n\t})\n\tt.Run(\"Match\", func(t *testing.T) {\n\t\tt.Parallel()\n\n\t\tfor _, c := range cases {\n\t\t\tc := c\n\t\t\tt.Run(c.name, func(t *testing.T) {\n\t\t\t\tt.Parallel()\n\n\t\t\t\tfor _, m := range c.match {\n\t\t\t\t\tif !matchLike([]byte(m), c.plan...) {\n\t\t\t\t\t\tt.Errorf(\"key %q was not matched\", m)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfor _, nm := range c.nonmatch {\n\t\t\t\t\tif matchLike([]byte(nm), c.plan...) {\n\t\t\t\t\t\tt.Errorf(\"key %q was matched\", nm)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t})\n\t\t}\n\t})\n}\n"
        },
        {
          "name": "logger",
          "type": "tree",
          "content": null
        },
        {
          "name": "lru",
          "type": "tree",
          "content": null
        },
        {
          "name": "main_test.go",
          "type": "blob",
          "size": 0.603515625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa_test\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"testing\"\n\n\t\"net/http\"\n\t_ \"net/http/pprof\"\n\n\t\"github.com/featurebasedb/featurebase/v3/testhook\"\n)\n\nfunc TestMain(m *testing.M) {\n\tl, err := net.Listen(\"tcp\", \"localhost:0\")\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tport := l.Addr().(*net.TCPAddr).Port\n\tfmt.Printf(\"featurebase/ TestMain: online stack-traces: curl http://localhost:%v/debug/pprof/goroutine?debug=2\\n\", port)\n\tgo func() {\n\t\terr := http.Serve(l, nil)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}()\n\ttesthook.RunTestsWithHooks(m)\n\n}\n"
        },
        {
          "name": "metrics.go",
          "type": "blob",
          "size": 25.353515625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport \"github.com/prometheus/client_golang/prometheus\"\n\nconst (\n\tMetricCreateIndex                     = \"create_index_total\"\n\tMetricDeleteIndex                     = \"delete_index_total\"\n\tMetricCreateField                     = \"create_field_total\"\n\tMetricDeleteField                     = \"delete_field_total\"\n\tMetricDeleteAvailableShard            = \"delete_available_shard_total\"\n\tMetricRecalculateCache                = \"recalculate_cache_total\"\n\tMetricInvalidateCache                 = \"invalidate_cache_total\"\n\tMetricInvalidateCacheSkipped          = \"invalidate_cache_skipped_total\"\n\tMetricReadDirtyCache                  = \"dirty_cache_total\"\n\tMetricRankCacheLength                 = \"rank_cache_length\"\n\tMetricCacheThresholdReached           = \"cache_threshold_reached_total\"\n\tMetricRow                             = \"query_row_total\"\n\tMetricRowBSI                          = \"query_row_bsi_total\"\n\tMetricSetBit                          = \"set_bit_total\"\n\tMetricClearBit                        = \"clear_bit_total\"\n\tMetricImportingN                      = \"importing_total\"\n\tMetricImportedN                       = \"imported_total\"\n\tMetricClearingN                       = \"clearing_total\"\n\tMetricClearedN                        = \"cleared_total\"\n\tMetricSnapshotDurationSeconds         = \"snapshot_duration_seconds\"\n\tMetricBlockRepair                     = \"block_repair_total\"\n\tMetricSyncFieldDurationSeconds        = \"sync_field_duration_seconds\"\n\tMetricSyncIndexDurationSeconds        = \"sync_index_duration_seconds\"\n\tMetricHTTPRequest                     = \"http_request_duration_seconds\"\n\tMetricGRPCUnaryQueryDurationSeconds   = \"grpc_request_pql_unary_query_duration_seconds\"\n\tMetricGRPCUnaryFormatDurationSeconds  = \"grpc_request_pql_unary_format_duration_seconds\"\n\tMetricGRPCStreamQueryDurationSeconds  = \"grpc_request_pql_stream_query_duration_seconds\"\n\tMetricGRPCStreamFormatDurationSeconds = \"grpc_request_pql_stream_format_duration_seconds\"\n\tMetricMaxShard                        = \"maximum_shard\"\n\tMetricAntiEntropy                     = \"antientropy_total\"\n\tMetricAntiEntropyDurationSeconds      = \"antientropy_duration_seconds\"\n\tMetricGarbageCollection               = \"garbage_collection_total\"\n\tMetricGoroutines                      = \"goroutines\"\n\tMetricOpenFiles                       = \"open_files\"\n\tMetricHeapAlloc                       = \"heap_alloc\"\n\tMetricHeapInuse                       = \"heap_inuse\"\n\tMetricStackInuse                      = \"stack_inuse\"\n\tMetricMallocs                         = \"mallocs\"\n\tMetricFrees                           = \"frees\"\n\tMetricTransactionStart                = \"transaction_start\"\n\tMetricTransactionEnd                  = \"transaction_end\"\n\tMetricTransactionBlocked              = \"transaction_blocked\"\n\tMetricExclusiveTransactionRequest     = \"transaction_exclusive_request\"\n\tMetricExclusiveTransactionActive      = \"transaction_exclusive_active\"\n\tMetricExclusiveTransactionEnd         = \"transaction_exclusive_end\"\n\tMetricExclusiveTransactionBlocked     = \"transaction_exclusive_blocked\"\n\tMetricPqlQueries                      = \"pql_queries_total\"\n\tMetricSqlQueries                      = \"sql_queries_total\"\n\tMetricDeleteDataframe                 = \"delete_dataframe\"\n)\n\nconst (\n\t// MetricBatchImportDurationSeconds records the full time of the\n\t// RecordBatch.Import call. This includes starting and finishing a\n\t// transaction, doing key translation, building fragments locally,\n\t// importing all data, and resetting internal structures.\n\tMetricBatchImportDurationSeconds = \"batch_import_duration_seconds\"\n\n\t// MetricBatchFlushDurationSeconds records the full time for\n\t// RecordBatch.Flush (if splitBatchMode is in use). This includes\n\t// starting and finishing a transaction, importing all data, and\n\t// resetting internal structures.\n\tMetricBatchFlushDurationSeconds = \"batch_flush_duration_seconds\"\n\n\t// MetricBatchShardImportBuildRequestsSeconds is the time it takes\n\t// after making fragments to build the shard-transactional request\n\t// objects (but not actually import them or do any network activity).\n\tMetricBatchShardImportBuildRequestsSeconds = \"batch_shard_import_build_requests_seconds\"\n\n\t// MetricBatchShardImportDurationSeconds is the time it takes to\n\t// import all data for all shards in the batch using the\n\t// shard-transactional endpoint. This does not include the time it\n\t// takes to build the requests locally.\n\tMetricBatchShardImportDurationSeconds = \"batch_shard_import_duration_seconds\"\n)\n\n// server related\n\nvar CounterJobTotal = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"job_total\",\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar GaugeWorkerTotal = prometheus.NewGauge(\n\tprometheus.GaugeOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"worker_total\",\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterPQLQueries = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricPqlQueries,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterSQLQueries = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricSqlQueries,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterGarbageCollection = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricGarbageCollection,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar GaugeGoroutines = prometheus.NewGauge(\n\tprometheus.GaugeOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricGoroutines,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar GaugeOpenFiles = prometheus.NewGauge(\n\tprometheus.GaugeOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricOpenFiles,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar GaugeHeapAlloc = prometheus.NewGauge(\n\tprometheus.GaugeOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricHeapAlloc,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar GaugeHeapInUse = prometheus.NewGauge(\n\tprometheus.GaugeOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricHeapInuse,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar GaugeStackInUse = prometheus.NewGauge(\n\tprometheus.GaugeOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricStackInuse,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar GaugeMallocs = prometheus.NewGauge(\n\tprometheus.GaugeOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricMallocs,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar GaugeFrees = prometheus.NewGauge(\n\tprometheus.GaugeOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricFrees,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar SummaryHttpRequests = prometheus.NewSummaryVec(\n\tprometheus.SummaryOpts{\n\t\tNamespace:  \"pilosa\",\n\t\tName:       MetricHTTPRequest,\n\t\tHelp:       \"TODO\",\n\t\tObjectives: map[float64]float64{0.5: 0.05, 0.9: 0.01, 0.99: 0.001},\n\t},\n\t[]string{\n\t\t\"method\",\n\t\t\"path\",\n\t\t\"slow\",\n\t\t\"useragent\",\n\t\t\"where\",\n\t},\n)\n\nvar CounterCreateIndex = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricCreateIndex,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterDeleteIndex = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricDeleteIndex,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterDeleteDataframe = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricDeleteDataframe,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterCreateField = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricCreateField,\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterDeleteField = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricDeleteField,\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterDeleteAvailableShard = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricDeleteAvailableShard,\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterExclusiveTransactionRequest = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricExclusiveTransactionRequest,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterTransactionStart = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricTransactionStart,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterExclusiveTransactionBlocked = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricExclusiveTransactionBlocked,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterTransactionBlocked = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricTransactionBlocked,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterExclusiveTransactionActive = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricExclusiveTransactionActive,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterExclusiveTransactionEnd = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricExclusiveTransactionEnd,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterTransactionEnd = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricTransactionEnd,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\n// TODO(pok) do these need index names?\n\nvar CounterRecalculateCache = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricRecalculateCache,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterInvalidateCacheSkipped = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricInvalidateCacheSkipped,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterInvalidateCache = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricInvalidateCache,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar GaugeRankCacheLength = prometheus.NewGauge(\n\tprometheus.GaugeOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricRankCacheLength,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterCacheThresholdReached = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricCacheThresholdReached,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterReadDirtyCache = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricReadDirtyCache,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterSetBit = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricSetBit,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterClearBit = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricClearBit,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterSetRow = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"setRow\",\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterImportingN = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricImportingN,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterImportedN = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricImportedN,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterClearingingN = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricClearingN,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar CounterClearedN = prometheus.NewCounter(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricClearedN,\n\t\tHelp:      \"TODO\",\n\t},\n)\n\nvar SummaryGRPCStreamQueryDurationSeconds = prometheus.NewSummary(\n\tprometheus.SummaryOpts{\n\t\tNamespace:  \"pilosa\",\n\t\tName:       MetricGRPCStreamQueryDurationSeconds,\n\t\tHelp:       \"TODO\",\n\t\tObjectives: map[float64]float64{0.5: 0.05, 0.9: 0.01, 0.99: 0.001},\n\t},\n)\n\nvar SummaryGRPCStreamFormatDurationSeconds = prometheus.NewSummary(\n\tprometheus.SummaryOpts{\n\t\tNamespace:  \"pilosa\",\n\t\tName:       MetricGRPCStreamFormatDurationSeconds,\n\t\tHelp:       \"TODO\",\n\t\tObjectives: map[float64]float64{0.5: 0.05, 0.9: 0.01, 0.99: 0.001},\n\t},\n)\n\nvar SummaryGRPCUnaryQueryDurationSeconds = prometheus.NewSummary(\n\tprometheus.SummaryOpts{\n\t\tNamespace:  \"pilosa\",\n\t\tName:       MetricGRPCUnaryQueryDurationSeconds,\n\t\tHelp:       \"TODO\",\n\t\tObjectives: map[float64]float64{0.5: 0.05, 0.9: 0.01, 0.99: 0.001},\n\t},\n)\n\nvar SummaryGRPCUnaryFormatDurationSeconds = prometheus.NewSummary(\n\tprometheus.SummaryOpts{\n\t\tNamespace:  \"pilosa\",\n\t\tName:       MetricGRPCUnaryFormatDurationSeconds,\n\t\tHelp:       \"TODO\",\n\t\tObjectives: map[float64]float64{0.5: 0.05, 0.9: 0.01, 0.99: 0.001},\n\t},\n)\n\nvar SummaryBatchImportDurationSeconds = prometheus.NewSummary(\n\tprometheus.SummaryOpts{\n\t\tNamespace:  \"pilosa\",\n\t\tName:       MetricBatchImportDurationSeconds,\n\t\tHelp:       \"TODO\",\n\t\tObjectives: map[float64]float64{0.5: 0.05, 0.9: 0.01, 0.99: 0.001},\n\t},\n)\n\nvar SummaryBatchFlushDurationSeconds = prometheus.NewSummary(\n\tprometheus.SummaryOpts{\n\t\tNamespace:  \"pilosa\",\n\t\tName:       MetricBatchFlushDurationSeconds,\n\t\tHelp:       \"TODO\",\n\t\tObjectives: map[float64]float64{0.5: 0.05, 0.9: 0.01, 0.99: 0.001},\n\t},\n)\n\nvar SummaryBatchShardImportBuildRequestsSeconds = prometheus.NewSummary(\n\tprometheus.SummaryOpts{\n\t\tNamespace:  \"pilosa\",\n\t\tName:       MetricBatchShardImportBuildRequestsSeconds,\n\t\tHelp:       \"TODO\",\n\t\tObjectives: map[float64]float64{0.5: 0.05, 0.9: 0.01, 0.99: 0.001},\n\t},\n)\n\nvar SummaryBatchShardImportDurationSeconds = prometheus.NewSummary(\n\tprometheus.SummaryOpts{\n\t\tNamespace:  \"pilosa\",\n\t\tName:       MetricBatchShardImportDurationSeconds,\n\t\tHelp:       \"TODO\",\n\t\tObjectives: map[float64]float64{0.5: 0.05, 0.9: 0.01, 0.99: 0.001},\n\t},\n)\n\n// index pql call related\n\nvar CounterQuerySumTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_sum_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryMinTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_min_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryMaxTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_max_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryMinRowTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_minrow_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryMaxRowTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_maxrow_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryClearTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_clear_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryClearRowTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_clearrow_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryDistinctTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_distinct_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryStoreTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_store_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryCountTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_count_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQuerySetTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_set_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryTopKTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_topk_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryTopNTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_topn_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryRowsTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_rows_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryExternalLookupTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_externallookup_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryExtractTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_extract_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryGroupByTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_groupby_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryOptionsTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_options_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryIncludesColumnTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_includescolumn_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryFieldValueTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_fieldvalue_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryPrecomputedTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_precomputed_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryUnionRowsTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_unionrows_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryConstRowTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_constrow_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryLimitTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_limit_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryPercentileTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_percentile_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryDeleteTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_delete_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQuerySortTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_sort_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryApplyTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_apply_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryArrowTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_arrow_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\n// CounterQueryBitmapTotal represents bitmap calls.\nvar CounterQueryBitmapTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_bitmap_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryRowTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_row_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryRowBSITotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_row_bsi_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryRangeTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_range_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryDifferenceTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_difference_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryIntersectTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_intersect_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryUnionTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_union_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryInnerUnionRowsTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_innerunionrows_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryXorTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_xor_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryNotTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_not_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryShiftTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_shift_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nvar CounterQueryAllTotal = prometheus.NewCounterVec(\n\tprometheus.CounterOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      \"query_all_total\",\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\n// index related\n\nvar GaugeIndexMaxShard = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tNamespace: \"pilosa\",\n\t\tName:      MetricMaxShard,\n\t\tHelp:      \"TODO\",\n\t},\n\t[]string{\n\t\t\"index\",\n\t},\n)\n\nfunc init() {\n\t// server related\n\tprometheus.MustRegister(CounterJobTotal)\n\tprometheus.MustRegister(GaugeWorkerTotal)\n\tprometheus.MustRegister(CounterPQLQueries)\n\tprometheus.MustRegister(CounterSQLQueries)\n\tprometheus.MustRegister(CounterGarbageCollection)\n\tprometheus.MustRegister(GaugeGoroutines)\n\tprometheus.MustRegister(GaugeOpenFiles)\n\tprometheus.MustRegister(GaugeHeapAlloc)\n\tprometheus.MustRegister(GaugeHeapInUse)\n\tprometheus.MustRegister(GaugeStackInUse)\n\tprometheus.MustRegister(GaugeMallocs)\n\tprometheus.MustRegister(GaugeFrees)\n\tprometheus.MustRegister(SummaryHttpRequests)\n\tprometheus.MustRegister(CounterCreateIndex)\n\tprometheus.MustRegister(CounterDeleteIndex)\n\tprometheus.MustRegister(CounterCreateField)\n\tprometheus.MustRegister(CounterDeleteField)\n\tprometheus.MustRegister(CounterDeleteAvailableShard)\n\tprometheus.MustRegister(CounterDeleteDataframe)\n\tprometheus.MustRegister(CounterExclusiveTransactionRequest)\n\tprometheus.MustRegister(CounterTransactionStart)\n\tprometheus.MustRegister(CounterExclusiveTransactionBlocked)\n\tprometheus.MustRegister(CounterTransactionBlocked)\n\tprometheus.MustRegister(CounterExclusiveTransactionActive)\n\tprometheus.MustRegister(CounterExclusiveTransactionEnd)\n\tprometheus.MustRegister(CounterTransactionEnd)\n\tprometheus.MustRegister(CounterRecalculateCache)\n\tprometheus.MustRegister(CounterInvalidateCacheSkipped)\n\tprometheus.MustRegister(CounterInvalidateCache)\n\tprometheus.MustRegister(GaugeRankCacheLength)\n\tprometheus.MustRegister(CounterCacheThresholdReached)\n\tprometheus.MustRegister(CounterReadDirtyCache)\n\tprometheus.MustRegister(CounterSetBit)\n\tprometheus.MustRegister(CounterClearBit)\n\tprometheus.MustRegister(CounterSetRow)\n\tprometheus.MustRegister(CounterImportingN)\n\tprometheus.MustRegister(CounterImportedN)\n\tprometheus.MustRegister(CounterClearingingN)\n\tprometheus.MustRegister(CounterClearedN)\n\tprometheus.MustRegister(SummaryGRPCStreamQueryDurationSeconds)\n\tprometheus.MustRegister(SummaryGRPCStreamFormatDurationSeconds)\n\tprometheus.MustRegister(SummaryGRPCUnaryQueryDurationSeconds)\n\tprometheus.MustRegister(SummaryGRPCUnaryFormatDurationSeconds)\n\tprometheus.MustRegister(SummaryBatchImportDurationSeconds)\n\tprometheus.MustRegister(SummaryBatchFlushDurationSeconds)\n\tprometheus.MustRegister(SummaryBatchShardImportBuildRequestsSeconds)\n\tprometheus.MustRegister(SummaryBatchShardImportDurationSeconds)\n\n\t// pql calls\n\tprometheus.MustRegister(CounterQuerySumTotal)\n\tprometheus.MustRegister(CounterQueryMinTotal)\n\tprometheus.MustRegister(CounterQueryMaxTotal)\n\tprometheus.MustRegister(CounterQueryMinRowTotal)\n\tprometheus.MustRegister(CounterQueryMaxRowTotal)\n\tprometheus.MustRegister(CounterQueryClearTotal)\n\tprometheus.MustRegister(CounterQueryClearRowTotal)\n\tprometheus.MustRegister(CounterQueryDistinctTotal)\n\tprometheus.MustRegister(CounterQueryStoreTotal)\n\tprometheus.MustRegister(CounterQueryCountTotal)\n\tprometheus.MustRegister(CounterQuerySetTotal)\n\tprometheus.MustRegister(CounterQueryTopKTotal)\n\tprometheus.MustRegister(CounterQueryTopNTotal)\n\tprometheus.MustRegister(CounterQueryRowsTotal)\n\tprometheus.MustRegister(CounterQueryExternalLookupTotal)\n\tprometheus.MustRegister(CounterQueryExtractTotal)\n\tprometheus.MustRegister(CounterQueryGroupByTotal)\n\tprometheus.MustRegister(CounterQueryOptionsTotal)\n\tprometheus.MustRegister(CounterQueryIncludesColumnTotal)\n\tprometheus.MustRegister(CounterQueryFieldValueTotal)\n\tprometheus.MustRegister(CounterQueryPrecomputedTotal)\n\tprometheus.MustRegister(CounterQueryUnionRowsTotal)\n\tprometheus.MustRegister(CounterQueryConstRowTotal)\n\tprometheus.MustRegister(CounterQueryLimitTotal)\n\tprometheus.MustRegister(CounterQueryPercentileTotal)\n\tprometheus.MustRegister(CounterQueryDeleteTotal)\n\tprometheus.MustRegister(CounterQuerySortTotal)\n\tprometheus.MustRegister(CounterQueryApplyTotal)\n\tprometheus.MustRegister(CounterQueryArrowTotal)\n\tprometheus.MustRegister(CounterQueryBitmapTotal)\n\tprometheus.MustRegister(CounterQueryRowTotal)\n\tprometheus.MustRegister(CounterQueryRowBSITotal)\n\tprometheus.MustRegister(CounterQueryRangeTotal)\n\tprometheus.MustRegister(CounterQueryDifferenceTotal)\n\tprometheus.MustRegister(CounterQueryIntersectTotal)\n\tprometheus.MustRegister(CounterQueryUnionTotal)\n\tprometheus.MustRegister(CounterQueryInnerUnionRowsTotal)\n\tprometheus.MustRegister(CounterQueryXorTotal)\n\tprometheus.MustRegister(CounterQueryNotTotal)\n\tprometheus.MustRegister(CounterQueryShiftTotal)\n\tprometheus.MustRegister(CounterQueryAllTotal)\n\n\t// index related\n\tprometheus.MustRegister(GaugeIndexMaxShard)\n\n}\n"
        },
        {
          "name": "mock",
          "type": "tree",
          "content": null
        },
        {
          "name": "monitor",
          "type": "tree",
          "content": null
        },
        {
          "name": "net",
          "type": "tree",
          "content": null
        },
        {
          "name": "nfpm.yaml",
          "type": "blob",
          "size": 1.3994140625,
          "content": "name: \"featurebase\"\narch: \"${GOARCH}\"\nversion: \"${VERSION}\"\nsection: \"default\"\npriority: \"extra\"\nmaintainer: \"Molecula Corp. <info@molecula.com>\"\ndescription: \"FeatureBase is a feature extraction and storage technology that enables real-time analytics.\"\nvendor: \"Molecula\"\nhomepage: \"https://molecula.com\"\ncontents:\n  - src: ./featurebase\n    dst: /usr/bin/featurebase\n  - src: ./fbsql\n    dst: /usr/bin/fbsql\n  - src: ./install/featurebase.conf\n    dst: /etc/featurebase/featurebase.conf\n    type: config|noreplace\n    file_info:\n      owner: featurebase\n      group: featurebase\n  - src: ./install/featurebase.redhat.service\n    dst: /usr/lib/systemd/system/featurebase.service\n    packager: rpm\n  - src: ./install/featurebase.debian.service\n    dst: /lib/systemd/system/featurebase.service\n    packager: deb\n  - dst: /var/log/molecula # We use vendor name on log directory in case other molecula components need it.\n    type: dir\n    file_info:\n      mode: 0755\n      owner: featurebase\n      group: featurebase\n  - dst: /var/lib/featurebase\n    type: dir\n    file_info:\n      mode: 0755\n      owner: featurebase\n      group: featurebase\n  - dst: /etc/featurebase # Keeping the config writable by the featurebase user is necessary for the agent.\n    type: dir\n    file_info:\n      mode: 0755\n      owner: featurebase\n      group: featurebase\n\nscripts:\n  preinstall: ./install/preinstall.sh\n  postinstall: ./install/postinstall.sh\n"
        },
        {
          "name": "null_test.go",
          "type": "blob",
          "size": 18.1357421875,
          "content": "// Copyright 2023 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\n\tpilosa \"github.com/featurebasedb/featurebase/v3\"\n\t\"github.com/featurebasedb/featurebase/v3/batch\"\n\t\"github.com/featurebasedb/featurebase/v3/dax\"\n\t\"github.com/featurebasedb/featurebase/v3/test\"\n\t\"github.com/stretchr/testify/require\"\n)\n\n// the new \"handle nulls for non-BSI fields\" logic has a lot of\n// implications, and needs to test things across multiple ways\n// of importing data, so we want to have some consistent\n// behavior.\n//\n// We're not testing keyed indexes, because the way index keys\n// work doesn't have any relation to the code that's used to\n// track existence, but we do want to test both keyed and\n// unkeyed fields, and we want to test direct Set/Clear\n// operations, SetRow, ClearRow (maybe? for mutexes?),\n// Import operations, and the batch code. Note that direct\n// use of ImportRoaring, outside of the batch code, makes\n// no guarantees; it's up to a user providing roaring bitmaps\n// to handle existence views.\n//\n// This is necessary because it's simply not *possible* to\n// detect the distinction between \"no bits provided\" and\n// \"an empty set\" from the bits written to a view other than\n// an existence view. We could, in principle, spend a lot\n// of time computing a best-guess that all non-empty sets\n// are non-null, but this would be less accurate and much\n// more expensive than doing that work on the batch side.\n\n// setupNullHandlingSchema yields a cluster, and API, which point at an\n// index with the given name, containing fields {mu, mk, su, sk, tu, tk}\n// which are mutex/set/timequantum fields which are unkeyed/keyed respectively.\nfunc setupNullHandlingSchema(t *testing.T, indexSuffix string) (*test.Cluster, string, *pilosa.API) {\n\tc := test.MustRunCluster(t, 3)\n\tapi := c.GetNode(0).API\n\tindex := c.Idx(indexSuffix)\n\t// we have six cases we care about; {mutex,set,timequantum} * {keyed, unkeyed}\n\tc.CreateField(t, index, pilosa.IndexOptions{TrackExistence: true}, \"mu\", pilosa.OptFieldTypeMutex(pilosa.CacheTypeNone, 0))\n\tc.CreateField(t, index, pilosa.IndexOptions{TrackExistence: true}, \"mk\", pilosa.OptFieldTypeMutex(pilosa.CacheTypeNone, 0), pilosa.OptFieldKeys())\n\tc.CreateField(t, index, pilosa.IndexOptions{TrackExistence: true}, \"su\", pilosa.OptFieldTypeSet(pilosa.CacheTypeNone, 0))\n\tc.CreateField(t, index, pilosa.IndexOptions{TrackExistence: true}, \"sk\", pilosa.OptFieldTypeSet(pilosa.CacheTypeNone, 0), pilosa.OptFieldKeys())\n\tc.CreateField(t, index, pilosa.IndexOptions{TrackExistence: true}, \"tu\", pilosa.OptFieldTypeTime(\"YMD\", \"0\"))\n\tc.CreateField(t, index, pilosa.IndexOptions{TrackExistence: true}, \"tk\", pilosa.OptFieldTypeTime(\"YMD\", \"0\"), pilosa.OptFieldKeys())\n\treturn c, index, api\n}\n\nvar nullHandlingFieldMasks = map[string]int{\n\t\"mu\": 1,\n\t\"mk\": 2,\n\t\"su\": 4,\n\t\"sk\": 8,\n\t\"tu\": 16,\n\t\"tk\": 32,\n}\nvar nullHandlingExpectedResults = generateNullHandlingExpectedResults()\n\ntype nullSet map[bool][]uint64\n\nfunc (n nullSet) null(v uint64) {\n\tn[true] = append(n[true], v)\n}\n\nfunc (n nullSet) notNull(v uint64) {\n\tn[false] = append(n[false], v)\n}\n\nfunc (n nullSet) clone() nullSet {\n\tf := n[false]\n\tt := n[true]\n\tnf := make([]uint64, len(f))\n\tnt := make([]uint64, len(t))\n\tcopy(nf, f)\n\tcopy(nt, t)\n\treturn nullSet{false: nf, true: nt}\n}\n\n// trimSlice removes elements of s for which func returns\n// true, returning the modified slice. it is destructive.\nfunc trimSlice(s []uint64, fn func(uint64) bool) []uint64 {\n\tn := 0\n\tfor i, v := range s {\n\t\tif fn(v) {\n\t\t\tcontinue\n\t\t} else {\n\t\t\ts[n] = s[i]\n\t\t\tn++\n\t\t}\n\t}\n\treturn s[:n]\n}\n\n// trim removes values matching f\nfunc (ns nullSet) trim(fn func(uint64) bool) {\n\tns[false] = trimSlice(ns[false], fn)\n\tns[true] = trimSlice(ns[true], fn)\n}\n\n// This outlines the expected results of a series of steps.\n// We have six fields, keyed/unkeyed sets, mutexes, and time\n// quantums. We only ever set value 0/\"a\" in them.\n//\n// First, we set those bits for the records 0..63, by treating\n// each field as having a bitmask 1/2/4/8/16/32, and setting the\n// bits for each field that's a 1 in the record's ID.\n//\n// Then, we *clear* every bit for every 5th record. I used to do\n// every 3rd, but this worked out poorly, because 63%3 == 0. This\n// means that all those entries should still exist, but should be\n// considered nulls.\n//\n// Then, we delete everything that has either the 16 or the 32\n// bit (or both) set. This means that all the records which are\n// *not* divisible by 5 should no longer be considered null, because\n// the records don't exist. The ones which are divisible by 5 are\n// still null.\n//\n// Then, we set a bit in \"tk\" for record 63. At this point, the\n// others should all be null. Remember that, prior to the delete,\n// they were all non-null; 63 was the record that had every bit set.\n// So we're verifying that, after a delete, recreating the record\n// does not show things as not-null because they had been set *prior*\n// to the delete.\nfunc generateNullHandlingExpectedResults() map[int]map[string]nullSet {\n\tout := map[int]map[string]nullSet{}\n\tfor phase := 0; phase < 4; phase++ {\n\t\tout[phase] = map[string]nullSet{}\n\t\tfor k := range nullHandlingFieldMasks {\n\t\t\tout[phase][k] = nullSet{}\n\t\t}\n\t}\n\t// We can't combine these two inner loops, because a field is\n\t// only null for records which exist, so record 0, having no\n\t// values set at all, isn't even a null.\n\tphase := 0\n\tfor i := 0; i < (1 << 6); i++ {\n\t\tmadeAny := false\n\t\tfor k, v := range nullHandlingFieldMasks {\n\t\t\tif i&v != 0 {\n\t\t\t\tout[phase][k].notNull(uint64(i))\n\t\t\t\tmadeAny = true\n\t\t\t}\n\t\t}\n\t\tif madeAny {\n\t\t\tfor k, v := range nullHandlingFieldMasks {\n\t\t\t\tif i&v == 0 {\n\t\t\t\t\tout[phase][k].null(uint64(i))\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\t// phase 1: we clear every bit in records divisible by 5.\n\t// this should not change nullness of sets or time quantums,\n\t// but should change their results. it should make mutexes null.\n\t// we also clear several bits in row 1/\"b\" -- bits which were\n\t// never set. this should have no impact on anything, so we don't\n\t// reflect it here.\n\tphase = 1\n\tfor i := 0; i < (1 << 6); i++ {\n\t\tmadeAny := false\n\t\tfor k, v := range nullHandlingFieldMasks {\n\t\t\tif i&v != 0 {\n\t\t\t\tif k[0] == 'm' && (i%5) == 0 {\n\t\t\t\t\tout[phase][k].null(uint64(i))\n\t\t\t\t} else {\n\t\t\t\t\tout[phase][k].notNull(uint64(i))\n\t\t\t\t}\n\t\t\t\t// even if the only value was the mutex field, the record\n\t\t\t\t// was ever created, so the record still exists even if\n\t\t\t\t// every field is null.\n\t\t\t\tmadeAny = true\n\t\t\t}\n\t\t}\n\t\tif madeAny {\n\t\t\tfor k, v := range nullHandlingFieldMasks {\n\t\t\t\tif i&v == 0 {\n\t\t\t\t\tout[phase][k].null(uint64(i))\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// phase 2: delete the &16 and &32 rows. we expect deleted records\n\t// to be neither null nor not null. We delete both the &16 and &32\n\t// rows because the delete flow is different for keys and no-keys.\n\tphase = 2\n\tfor k := range nullHandlingFieldMasks {\n\t\tns := out[1][k].clone()\n\t\tns.trim(func(v uint64) bool { return v >= 16 && (v%5) != 0 })\n\t\tout[phase][k] = ns\n\t}\n\n\t// phase 3: create a record which had previously existed.\n\t// we expect previously-existing fields to now show as null,\n\t// unless we created them again.\n\tphase = 3\n\tfor k, v := range nullHandlingFieldMasks {\n\t\tns := out[2][k].clone()\n\t\tif v == 32 {\n\t\t\tns.notNull(63)\n\t\t} else {\n\t\t\tns.null(63)\n\t\t}\n\t\tout[phase][k] = ns\n\t}\n\treturn out\n}\n\nfunc nullTestQuery(t *testing.T, api *pilosa.API, req *pilosa.QueryRequest) pilosa.QueryResponse {\n\tt.Helper()\n\tresp, err := api.Query(context.Background(), req)\n\tif err != nil {\n\t\tt.Fatalf(\"running request: %v\", err)\n\t}\n\tif resp.Err != nil {\n\t\tt.Fatalf(\"request returned unexpected error: %v\", resp.Err)\n\t}\n\treturn resp\n}\n\nfunc nullTestRows(t *testing.T, api *pilosa.API, req *pilosa.QueryRequest) [][]uint64 {\n\tt.Helper()\n\tresp := nullTestQuery(t, api, req)\n\tout := make([][]uint64, 0, len(resp.Results))\n\tfor _, result := range resp.Results {\n\t\trow, ok := result.(*pilosa.Row)\n\t\tif !ok {\n\t\t\tt.Fatalf(\"expected row result from query, got %T\", result)\n\t\t}\n\t\tout = append(out, row.Columns())\n\t}\n\treturn out\n}\n\nfunc nullTestImport(t *testing.T, api *pilosa.API, req *pilosa.ImportRequest) {\n\tt.Helper()\n\tqcx := api.Txf().NewQcx()\n\tdefer qcx.Abort()\n\terr := api.Import(context.Background(), qcx, req)\n\tif err != nil {\n\t\tt.Fatalf(\"importing: %v\", err)\n\t}\n\terr = qcx.Finish()\n\tif err != nil {\n\t\tt.Fatalf(\"committing: %v\", err)\n\t}\n}\n\nfunc nullTestExpectResults(t *testing.T, api *pilosa.API, index string, phase int) {\n\tt.Helper()\n\tfor k, expected := range nullHandlingExpectedResults[phase] {\n\t\tt.Run(fmt.Sprintf(\"%s-phase-%d\", k, phase), func(t *testing.T) {\n\t\t\treq := &pilosa.QueryRequest{\n\t\t\t\tIndex: index,\n\t\t\t\tQuery: fmt.Sprintf(`Row(%s == null)\n\tRow(%s != null)`, k, k),\n\t\t\t}\n\t\t\trows := nullTestRows(t, api, req)\n\t\t\tt.Run(\"true\", func(t *testing.T) {\n\t\t\t\trequire.Equal(t, expected[true], rows[0])\n\t\t\t})\n\t\t\tt.Run(\"false\", func(t *testing.T) {\n\t\t\t\trequire.Equal(t, expected[false], rows[1])\n\t\t\t})\n\t\t})\n\t}\n}\n\n// TestNullHandlingSet only tests the behavior of null values\n// in non-BSI fields. It tests this using set/clear.\nfunc TestNullHandlingSet(t *testing.T) {\n\tc, index, api := setupNullHandlingSchema(t, \"i\")\n\tdefer c.Close()\n\n\tphase := 0\n\tvar reqs []string\n\tfor i := 0; i < (1 << 6); i++ {\n\t\tfor k, v := range nullHandlingFieldMasks {\n\t\t\tif i&v != 0 {\n\t\t\t\tif k[1] == 'k' {\n\t\t\t\t\treqs = append(reqs, fmt.Sprintf(`Set(%d, %s=\"a\")`, i, k))\n\t\t\t\t} else {\n\t\t\t\t\treqs = append(reqs, fmt.Sprintf(`Set(%d, %s=0)`, i, k))\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treq := &pilosa.QueryRequest{\n\t\tIndex: index,\n\t\tQuery: strings.Join(reqs, \"\\n\"),\n\t}\n\tresp := nullTestQuery(t, api, req)\n\tfor i, v := range resp.Results {\n\t\tif v != true {\n\t\t\tt.Fatalf(\"result %d (request %s): %#v\", i, reqs[i], v)\n\t\t}\n\t}\n\tnullTestExpectResults(t, api, index, phase)\n\n\tphase = 1\n\treqs = reqs[:0]\n\tfor i := 0; i < (1 << 6); i += 5 {\n\t\tfor k := range nullHandlingFieldMasks {\n\t\t\tif k[1] == 'k' {\n\t\t\t\treqs = append(reqs, fmt.Sprintf(`Clear(%d, %s=\"a\")`, i, k))\n\t\t\t} else {\n\t\t\t\treqs = append(reqs, fmt.Sprintf(`Clear(%d, %s=0)`, i, k))\n\t\t\t}\n\t\t}\n\t}\n\t// clear a lot of bits that weren't set in the first place. this should have\n\t// no effect on null/not-null state.\n\tfor i := 16; i < 48; i++ {\n\t\tfor k := range nullHandlingFieldMasks {\n\t\t\tif k[1] == 'k' {\n\t\t\t\treqs = append(reqs, fmt.Sprintf(`Clear(%d, %s=\"b\")`, i, k))\n\t\t\t} else {\n\t\t\t\treqs = append(reqs, fmt.Sprintf(`Clear(%d, %s=1)`, i, k))\n\t\t\t}\n\t\t}\n\t}\n\treq = &pilosa.QueryRequest{\n\t\tIndex: index,\n\t\tQuery: strings.Join(reqs, \"\\n\"),\n\t}\n\tresp = nullTestQuery(t, api, req)\n\tfor i, v := range resp.Results {\n\t\t// it's okay to get either true or false, because some of those bits\n\t\t// wouldn't have existed before, so the clear would fail. that's fine.\n\t\tif v != true && v != false {\n\t\t\tt.Fatalf(\"result %d (request %s): %#v\", i, reqs[i], v)\n\t\t}\n\t}\n\tnullTestExpectResults(t, api, index, phase)\n\n\tphase = 2\n\treq = &pilosa.QueryRequest{\n\t\tIndex: index,\n\t\tQuery: `Delete(Row(tk=\"a\")) Delete(Row(tu=0))`,\n\t}\n\tresp = nullTestQuery(t, api, req)\n\tif len(resp.Results) != 2 || resp.Results[0] != true || resp.Results[1] != true {\n\t\tt.Fatalf(\"expected two trues, got %#v\", resp.Results)\n\t}\n\tnullTestExpectResults(t, api, index, phase)\n\n\tphase = 3\n\treq = &pilosa.QueryRequest{\n\t\tIndex: index,\n\t\tQuery: `Set(63, tk=\"a\")`,\n\t}\n\tresp = nullTestQuery(t, api, req)\n\tif len(resp.Results) != 1 || resp.Results[0] != true {\n\t\tt.Fatalf(\"expected single true result, got %#v\", resp.Results)\n\t}\n\tnullTestExpectResults(t, api, index, phase)\n}\n\n// TestNullHandlingImport only tests the behavior of null values\n// in non-BSI fields. It tests this using the old Import API to\n// import values.\nfunc TestNullHandlingImport(t *testing.T) {\n\tc, index, api := setupNullHandlingSchema(t, \"i\")\n\tdefer c.Close()\n\n\tphase := 0\n\treqs := map[string]*pilosa.ImportRequest{}\n\tfor k := range nullHandlingFieldMasks {\n\t\treqs[k] = &pilosa.ImportRequest{\n\t\t\tIndex: index,\n\t\t\tField: k,\n\t\t\tShard: ^uint64(0),\n\t\t}\n\t}\n\tfor i := 0; i < (1 << 6); i++ {\n\t\tfor k, v := range nullHandlingFieldMasks {\n\t\t\tif i&v != 0 {\n\t\t\t\tif k[1] == 'k' {\n\t\t\t\t\treqs[k].ColumnIDs = append(reqs[k].ColumnIDs, uint64(i))\n\t\t\t\t\treqs[k].RowKeys = append(reqs[k].RowKeys, \"a\")\n\t\t\t\t} else {\n\t\t\t\t\treqs[k].ColumnIDs = append(reqs[k].ColumnIDs, uint64(i))\n\t\t\t\t\treqs[k].RowIDs = append(reqs[k].RowIDs, 0)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tfor _, req := range reqs {\n\t\tnullTestImport(t, api, req)\n\t}\n\tnullTestExpectResults(t, api, index, phase)\n\n\tphase = 1\n\tfor k := range nullHandlingFieldMasks {\n\t\treqs[k].ColumnIDs = reqs[k].ColumnIDs[:0]\n\t\tif k[1] == 'k' {\n\t\t\treqs[k].RowKeys = reqs[k].RowKeys[:0]\n\t\t\t// the import stashed its computed RowIDs in the req,\n\t\t\t// remove them.\n\t\t\treqs[k].RowIDs = nil\n\t\t} else {\n\t\t\treqs[k].RowIDs = reqs[k].RowIDs[:0]\n\t\t}\n\t\treqs[k].Clear = true\n\t\treqs[k].Shard = ^uint64(0)\n\t}\n\n\tfor i := 0; i < (1 << 6); i += 5 {\n\t\tfor k := range nullHandlingFieldMasks {\n\t\t\tif k[1] == 'k' {\n\t\t\t\treqs[k].ColumnIDs = append(reqs[k].ColumnIDs, uint64(i))\n\t\t\t\treqs[k].RowKeys = append(reqs[k].RowKeys, \"a\")\n\t\t\t\treqs[k].ColumnIDs = append(reqs[k].ColumnIDs, uint64(i))\n\t\t\t\treqs[k].RowKeys = append(reqs[k].RowKeys, \"b\")\n\t\t\t} else {\n\t\t\t\treqs[k].ColumnIDs = append(reqs[k].ColumnIDs, uint64(i))\n\t\t\t\treqs[k].RowIDs = append(reqs[k].RowIDs, 0)\n\t\t\t\treqs[k].ColumnIDs = append(reqs[k].ColumnIDs, uint64(i))\n\t\t\t\treqs[k].RowIDs = append(reqs[k].RowIDs, 0)\n\t\t\t}\n\t\t}\n\t}\n\t// clear a lot of bits that never previously got set, expecting no impact.\n\tfor i := 16; i < 48; i++ {\n\t\tfor k := range nullHandlingFieldMasks {\n\t\t\tif k[1] == 'k' {\n\t\t\t\treqs[k].ColumnIDs = append(reqs[k].ColumnIDs, uint64(i))\n\t\t\t\treqs[k].RowKeys = append(reqs[k].RowKeys, \"b\")\n\t\t\t} else {\n\t\t\t\treqs[k].ColumnIDs = append(reqs[k].ColumnIDs, uint64(i))\n\t\t\t\treqs[k].RowIDs = append(reqs[k].RowIDs, 1)\n\t\t\t}\n\t\t}\n\t}\n\tfor _, req := range reqs {\n\t\tt.Logf(\"req: %#v\", req)\n\t\tnullTestImport(t, api, req)\n\t}\n\tnullTestExpectResults(t, api, index, phase)\n\n\tphase = 2\n\t// no Delete in Import API, we use the old API for it.\n\treq := &pilosa.QueryRequest{\n\t\tIndex: index,\n\t\tQuery: `Delete(Row(tk=\"a\")) Delete(Row(tu=0))`,\n\t}\n\tresp := nullTestQuery(t, api, req)\n\tif len(resp.Results) != 2 || resp.Results[0] != true || resp.Results[1] != true {\n\t\tt.Fatalf(\"expected two trues, got %#v\", resp.Results)\n\t}\n\tnullTestExpectResults(t, api, index, phase)\n\tphase = 3\n\treqImport := &pilosa.ImportRequest{\n\t\tIndex:     index,\n\t\tField:     \"tk\",\n\t\tColumnIDs: []uint64{63},\n\t\tRowKeys:   []string{\"a\"},\n\t\tShard:     ^uint64(0),\n\t}\n\tnullTestImport(t, api, reqImport)\n\tnullTestExpectResults(t, api, index, phase)\n}\n\nfunc TestNullHandlingBatch(t *testing.T) {\n\tc, index, api := setupNullHandlingSchema(t, \"i\")\n\tdefer c.Close()\n\tctx := context.Background()\n\tfapi := pilosa.NewOnPremSchema(api)\n\timp := pilosa.NewOnPremImporter(api)\n\ttbl, err := fapi.TableByName(ctx, dax.TableName(index))\n\tif err != nil {\n\t\tt.Fatalf(\"getting table defs: %v\", err)\n\t}\n\tidxInfoBase := pilosa.TableToIndexInfo(tbl)\n\tfields := make([]*pilosa.FieldInfo, 0, len(nullHandlingFieldMasks))\n\tfor k := range nullHandlingFieldMasks {\n\t\tfields = append(fields, idxInfoBase.Field(k))\n\t}\n\n\tphase := 0\n\tb, err := batch.NewBatch(imp, 10000, tbl, fields, batch.OptUseShardTransactionalEndpoint(true))\n\tif err != nil {\n\t\tt.Fatalf(\"getting batch: %v\", err)\n\t}\n\tvar row batch.Row\n\trow.Values = make([]interface{}, len(fields))\n\t// we don't add the all-null row for ID 0\n\tfor i := 1; i < (1 << 6); i++ {\n\t\trow.ID = uint64(i)\n\t\trow.Values = row.Values[:0]\n\t\tfor _, fld := range fields {\n\t\t\tif i&nullHandlingFieldMasks[fld.Name] != 0 {\n\t\t\t\tif fld.Name[1] == 'k' {\n\t\t\t\t\trow.Values = append(row.Values, \"a\")\n\t\t\t\t} else {\n\t\t\t\t\trow.Values = append(row.Values, uint64(0))\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\trow.Values = append(row.Values, nil)\n\t\t\t}\n\t\t}\n\t\terr := b.Add(row)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"adding row: %v\", err)\n\t\t}\n\t}\n\tif err := b.Import(); err != nil {\n\t\tt.Fatalf(\"importing batch: %v\", err)\n\t}\n\tnullTestExpectResults(t, api, index, phase)\n\n\tphase = 1\n\tb, err = batch.NewBatch(imp, 10000, tbl, fields, batch.OptUseShardTransactionalEndpoint(true))\n\tif err != nil {\n\t\tt.Fatalf(\"getting batch: %v\", err)\n\t}\n\t// delete everything which is a multiple of 5, but\n\t// don't try to delete 0 because this creates an all-null\n\t// record.\n\tfor i := 5; i < (1 << 6); i += 5 {\n\t\trow.ID = uint64(i)\n\t\trow.Values = row.Values[:0]\n\t\trow.Clears = map[int]interface{}{}\n\t\tfor i, fld := range fields {\n\t\t\trow.Values = append(row.Values, nil)\n\t\t\tif fld.Name[0] == 'm' {\n\t\t\t\t// can't specify a particular bit to clear in a mutex\n\t\t\t\trow.Clears[i] = nil\n\t\t\t} else {\n\t\t\t\tif fld.Name[1] == 'k' {\n\t\t\t\t\trow.Clears[i] = \"a\"\n\t\t\t\t} else {\n\t\t\t\t\trow.Clears[i] = uint64(0)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\terr := b.Add(row)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"adding row: %v\", err)\n\t\t}\n\t}\n\tif err := b.Import(); err != nil {\n\t\tt.Fatalf(\"importing batch: %v\", err)\n\t}\n\tb, err = batch.NewBatch(imp, 10000, tbl, fields, batch.OptUseShardTransactionalEndpoint(true))\n\tif err != nil {\n\t\tt.Fatalf(\"getting batch: %v\", err)\n\t}\n\t// delete everything which is a multiple of 5\n\tfor i := 16; i < 48; i++ {\n\t\trow.ID = uint64(i)\n\t\trow.Values = row.Values[:0]\n\t\trow.Clears = map[int]interface{}{}\n\t\tfor i, fld := range fields {\n\t\t\trow.Values = append(row.Values, nil)\n\t\t\t// no way to specify \"clear a specific bit\", so we don't clear the mutexes.\n\t\t\tif fld.Name[0] != 'm' {\n\t\t\t\tif fld.Name[1] == 'k' {\n\t\t\t\t\trow.Clears[i] = \"b\"\n\t\t\t\t} else {\n\t\t\t\t\trow.Clears[i] = uint64(1)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\terr := b.Add(row)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"adding row: %v\", err)\n\t\t}\n\t}\n\tif err := b.Import(); err != nil {\n\t\tt.Fatalf(\"importing batch: %v\", err)\n\t}\n\tnullTestExpectResults(t, api, index, phase)\n\n\tphase = 2\n\t// no Delete in batch API, we use the old API for it.\n\treq := &pilosa.QueryRequest{\n\t\tIndex: index,\n\t\tQuery: `Delete(Row(tk=\"a\")) Delete(Row(tu=0))`,\n\t}\n\tresp := nullTestQuery(t, api, req)\n\tif len(resp.Results) != 2 || resp.Results[0] != true || resp.Results[1] != true {\n\t\tt.Fatalf(\"expected two trues, got %#v\", resp.Results)\n\t}\n\tnullTestExpectResults(t, api, index, phase)\n\n\tphase = 3\n\t// truncate fields to only have the one field in it\n\tfor _, f := range fields {\n\t\tif f.Name == \"tk\" {\n\t\t\tfields[0] = f\n\t\t\tfields = fields[:1]\n\t\t\tbreak\n\t\t}\n\t}\n\tb, err = batch.NewBatch(imp, 10000, tbl, fields, batch.OptUseShardTransactionalEndpoint(true))\n\tif err != nil {\n\t\tt.Fatalf(\"getting batch: %v\", err)\n\t}\n\trow.ID = uint64(63)\n\trow.Values = []interface{}{[]string{\"a\"}}\n\terr = b.Add(row)\n\tif err != nil {\n\t\tt.Fatalf(\"adding row: %v\", err)\n\t}\n\tif err := b.Import(); err != nil {\n\t\tt.Fatalf(\"importing batch: %v\", err)\n\t}\n\tnullTestExpectResults(t, api, index, phase)\n}\n"
        },
        {
          "name": "pb",
          "type": "tree",
          "content": null
        },
        {
          "name": "performancecounters.go",
          "type": "blob",
          "size": 5.630859375,
          "content": "package pilosa\n\nimport (\n\t\"sync/atomic\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n)\n\n// PerformanceCounter holds data about a performance counter for external consumers\ntype PerformanceCounter struct {\n\tNameSpace   string\n\tSubSystem   string\n\tCounterName string\n\tHelp        string\n\tValue       int64\n\tCounterType int64\n}\n\n// constants for the counter types\nconst (\n\t// raw - for when you just want a count of something\n\tCTR_TYPE_RAW = 0\n\t// per second - for when you accumulate counts of things\n\t// a consumer would sample this at intervals to arrive at a delta\n\t// then divide by the time in seconds between the samples to get a\n\t// per-second value\n\tCTR_TYPE_PER_SECOND = 1\n\t// ratio - for when you accumulate a count of something that you\n\t// want to use as a numerator in a ratio calculation\n\t// e.g. 'cache hits' could be a counter of this type and you could\n\t// divide it by a 'cache lookups' counter to get the hit ratio (see below)\n\tCTR_TYPE_RATIO = 2\n\t// ratio base - for when you accumulate a count of something that you\n\t// want to use as a denominator in a ratio calculation\n\t// e.g. 'cache lookups' could be a counter of this type and you could\n\t// use it as the denominator in a division with a 'cache hits' counter\n\t// as the numerator to get the hit ratio\n\tCTR_TYPE_RATIO_BASE = 3\n)\n\ntype PerformanceCounters struct {\n\tcounterValues [5]perfCtrWrapper\n}\n\ntype perfCtr struct {\n\tnameSpace   string\n\tsubSystem   string\n\tname        string\n\thelp        string\n\tvalue       int64\n\tcounterType int64\n}\n\nfunc (p *perfCtr) Add(increment int64) {\n\tatomic.AddInt64(&p.value, increment)\n}\n\ntype perfCtrWrapper struct {\n\tctr *perfCtr\n\tfun prometheus.CounterFunc\n}\n\nvar PerfCounterSQLRequestSec = perfCtr{\n\tnameSpace:   \"pilosa\",\n\tsubSystem:   \"sql_statistics\",\n\tname:        \"sql_requests_sec\",\n\thelp:        \"TODO\",\n\tvalue:       0,\n\tcounterType: CTR_TYPE_PER_SECOND,\n}\n\nvar PerfCounterSQLInsertsSec = perfCtr{\n\tnameSpace:   \"pilosa\",\n\tsubSystem:   \"sql_statistics\",\n\tname:        \"sql_inserts_sec\",\n\thelp:        \"TODO\",\n\tvalue:       0,\n\tcounterType: CTR_TYPE_PER_SECOND,\n}\n\nvar PerfCounterSQLBulkInsertsSec = perfCtr{\n\tnameSpace:   \"pilosa\",\n\tsubSystem:   \"sql_statistics\",\n\tname:        \"sql_bulk_inserts_sec\",\n\thelp:        \"TODO\",\n\tvalue:       0,\n\tcounterType: CTR_TYPE_PER_SECOND,\n}\n\nvar PerfCounterSQLBulkInsertBatchesSec = perfCtr{\n\tnameSpace:   \"pilosa\",\n\tsubSystem:   \"sql_statistics\",\n\tname:        \"sql_bulk_insert_batches_sec\",\n\thelp:        \"TODO\",\n\tvalue:       0,\n\tcounterType: CTR_TYPE_PER_SECOND,\n}\n\nvar PerfCounterSQLDeletesSec = perfCtr{\n\tnameSpace:   \"pilosa\",\n\tsubSystem:   \"sql_statistics\",\n\tname:        \"sql_deletes_sec\",\n\thelp:        \"TODO\",\n\tvalue:       0,\n\tcounterType: CTR_TYPE_PER_SECOND,\n}\n\nvar PerfCounters *PerformanceCounters = newPerformanceCounters()\n\nfunc newPerformanceCounters() *PerformanceCounters {\n\tctrs := &PerformanceCounters{\n\t\tcounterValues: [5]perfCtrWrapper{\n\t\t\t{\n\t\t\t\t&PerfCounterSQLRequestSec,\n\t\t\t\tprometheus.NewCounterFunc(\n\t\t\t\t\tprometheus.CounterOpts{\n\t\t\t\t\t\tNamespace: PerfCounterSQLRequestSec.nameSpace,\n\t\t\t\t\t\tSubsystem: PerfCounterSQLRequestSec.subSystem,\n\t\t\t\t\t\tName:      PerfCounterSQLRequestSec.name,\n\t\t\t\t\t\tHelp:      PerfCounterSQLRequestSec.help,\n\t\t\t\t\t},\n\t\t\t\t\tfunc() float64 {\n\t\t\t\t\t\treturn float64(atomic.LoadInt64(&PerfCounterSQLRequestSec.value))\n\t\t\t\t\t}),\n\t\t\t},\n\t\t\t{\n\t\t\t\t&PerfCounterSQLInsertsSec,\n\t\t\t\tprometheus.NewCounterFunc(\n\t\t\t\t\tprometheus.CounterOpts{\n\t\t\t\t\t\tNamespace: PerfCounterSQLInsertsSec.nameSpace,\n\t\t\t\t\t\tSubsystem: PerfCounterSQLInsertsSec.subSystem,\n\t\t\t\t\t\tName:      PerfCounterSQLInsertsSec.name,\n\t\t\t\t\t\tHelp:      PerfCounterSQLInsertsSec.help,\n\t\t\t\t\t},\n\t\t\t\t\tfunc() float64 {\n\t\t\t\t\t\treturn float64(atomic.LoadInt64(&PerfCounterSQLInsertsSec.value))\n\t\t\t\t\t}),\n\t\t\t},\n\t\t\t{\n\t\t\t\t&PerfCounterSQLBulkInsertsSec,\n\t\t\t\tprometheus.NewCounterFunc(\n\t\t\t\t\tprometheus.CounterOpts{\n\t\t\t\t\t\tNamespace: PerfCounterSQLBulkInsertsSec.nameSpace,\n\t\t\t\t\t\tSubsystem: PerfCounterSQLBulkInsertsSec.subSystem,\n\t\t\t\t\t\tName:      PerfCounterSQLBulkInsertsSec.name,\n\t\t\t\t\t\tHelp:      PerfCounterSQLBulkInsertsSec.help,\n\t\t\t\t\t},\n\t\t\t\t\tfunc() float64 {\n\t\t\t\t\t\treturn float64(atomic.LoadInt64(&PerfCounterSQLBulkInsertsSec.value))\n\t\t\t\t\t}),\n\t\t\t},\n\t\t\t{\n\t\t\t\t&PerfCounterSQLBulkInsertBatchesSec,\n\t\t\t\tprometheus.NewCounterFunc(\n\t\t\t\t\tprometheus.CounterOpts{\n\t\t\t\t\t\tNamespace: PerfCounterSQLBulkInsertBatchesSec.nameSpace,\n\t\t\t\t\t\tSubsystem: PerfCounterSQLBulkInsertBatchesSec.subSystem,\n\t\t\t\t\t\tName:      PerfCounterSQLBulkInsertBatchesSec.name,\n\t\t\t\t\t\tHelp:      PerfCounterSQLBulkInsertBatchesSec.help,\n\t\t\t\t\t},\n\t\t\t\t\tfunc() float64 {\n\t\t\t\t\t\treturn float64(atomic.LoadInt64(&PerfCounterSQLBulkInsertBatchesSec.value))\n\t\t\t\t\t}),\n\t\t\t},\n\t\t\t{\n\t\t\t\t&PerfCounterSQLDeletesSec,\n\t\t\t\tprometheus.NewCounterFunc(\n\t\t\t\t\tprometheus.CounterOpts{\n\t\t\t\t\t\tNamespace: PerfCounterSQLDeletesSec.nameSpace,\n\t\t\t\t\t\tSubsystem: PerfCounterSQLDeletesSec.subSystem,\n\t\t\t\t\t\tName:      PerfCounterSQLDeletesSec.name,\n\t\t\t\t\t\tHelp:      PerfCounterSQLDeletesSec.help,\n\t\t\t\t\t},\n\t\t\t\t\tfunc() float64 {\n\t\t\t\t\t\treturn float64(atomic.LoadInt64(&PerfCounterSQLDeletesSec.value))\n\t\t\t\t\t}),\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, w := range ctrs.counterValues {\n\t\tprometheus.MustRegister(w.fun)\n\t}\n\n\treturn ctrs\n}\n\n// list all the counters\n// we can just read here without locking because if the counters get changed\n// midway thru the loop, absent evidence to the contrary, the world will not end\nfunc (p *PerformanceCounters) ListCounters() ([]PerformanceCounter, error) {\n\tresult := make([]PerformanceCounter, len(p.counterValues))\n\tfor i, c := range p.counterValues {\n\t\tresult[i] = PerformanceCounter{\n\t\t\tNameSpace:   c.ctr.nameSpace,\n\t\t\tSubSystem:   c.ctr.subSystem,\n\t\t\tCounterName: c.ctr.name,\n\t\t\tValue:       c.ctr.value,\n\t\t\tCounterType: c.ctr.counterType,\n\t\t}\n\t}\n\treturn result, nil\n}\n"
        },
        {
          "name": "pilosa.go",
          "type": "blob",
          "size": 6.1220703125,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"regexp\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/disco\"\n\tpnet \"github.com/featurebasedb/featurebase/v3/net\"\n\t\"github.com/pkg/errors\"\n)\n\n// System errors.\nvar (\n\tErrHostRequired = errors.New(\"host required\")\n\n\tErrIndexRequired = errors.New(\"index required\")\n\tErrIndexExists   = disco.ErrIndexExists\n\tErrIndexNotFound = errors.New(\"index not found\")\n\n\tErrInvalidAddress = errors.New(\"invalid address\")\n\tErrInvalidSchema  = errors.New(\"invalid schema\")\n\n\tErrForeignIndexNotFound = errors.New(\"foreign index not found\")\n\n\t// ErrFieldRequired is returned when no field is specified.\n\tErrFieldRequired  = errors.New(\"field required\")\n\tErrColumnRequired = errors.New(\"column required\")\n\tErrFieldExists    = disco.ErrFieldExists\n\tErrFieldNotFound  = errors.New(\"field not found\")\n\n\tErrBSIGroupNotFound         = errors.New(\"bsigroup not found\")\n\tErrBSIGroupExists           = errors.New(\"bsigroup already exists\")\n\tErrBSIGroupNameRequired     = errors.New(\"bsigroup name required\")\n\tErrInvalidBSIGroupType      = errors.New(\"invalid bsigroup type\")\n\tErrInvalidBSIGroupRange     = errors.New(\"invalid bsigroup range\")\n\tErrInvalidBSIGroupValueType = errors.New(\"invalid bsigroup value type\")\n\tErrBSIGroupValueTooLow      = errors.New(\"value too low for configured field range\")\n\tErrBSIGroupValueTooHigh     = errors.New(\"value too high for configured field range\")\n\tErrInvalidRangeOperation    = errors.New(\"invalid range operation\")\n\tErrInvalidBetweenValue      = errors.New(\"invalid value for between operation\")\n\tErrDecimalOutOfRange        = errors.New(\"decimal value out of range\")\n\n\tErrViewRequired     = errors.New(\"view required\")\n\tErrViewExists       = disco.ErrViewExists\n\tErrInvalidView      = errors.New(\"invalid view\")\n\tErrInvalidCacheType = errors.New(\"invalid cache type\")\n\n\tErrName = errors.New(\"invalid index or field name, must match [a-z][a-z0-9_-]* and contain at most 230 characters\")\n\n\t// ErrFragmentNotFound is returned when a fragment does not exist.\n\tErrFragmentNotFound = errors.New(\"fragment not found\")\n\tErrQueryRequired    = errors.New(\"query required\")\n\tErrQueryCancelled   = errors.New(\"query cancelled\")\n\tErrQueryTimeout     = errors.New(\"query timeout\")\n\tErrTooManyWrites    = errors.New(\"too many write commands\")\n\n\t// TODO(2.0) poorly named - used when a *node* doesn't own a shard. Probably\n\t// we won't need this error at all by 2.0 though.\n\tErrClusterDoesNotOwnShard = errors.New(\"node does not own shard\")\n\n\t// ErrPreconditionFailed is returned when specified index/field createdAt timestamps don't match\n\tErrPreconditionFailed = errors.New(\"precondition failed\")\n\n\tErrNodeIDNotExists = errors.New(\"node with provided ID does not exist\")\n\tErrNodeNotPrimary  = errors.New(\"node is not the primary\")\n\n\tErrNotImplemented            = errors.New(\"not implemented\")\n\tErrFieldsArgumentRequired    = errors.New(\"fields argument required\")\n\tErrExpectedFieldListArgument = errors.New(\"expected field list argument\")\n\n\tErrIntFieldWithKeys       = errors.New(\"int field cannot be created with 'keys=true' option\")\n\tErrDecimalFieldWithKeys   = errors.New(\"decimal field cannot be created with 'keys=true' option\")\n\tErrTimestampFieldWithKeys = errors.New(\"timestamp field cannot be created with 'keys=true' option\")\n)\n\n// apiMethodNotAllowedError wraps an error value indicating that a particular\n// API method is not allowed in the current cluster state.\ntype apiMethodNotAllowedError struct {\n\terror\n}\n\n// newAPIMethodNotAllowedError returns err wrapped in an ApiMethodNotAllowedError.\nfunc newAPIMethodNotAllowedError(err error) apiMethodNotAllowedError {\n\treturn apiMethodNotAllowedError{err}\n}\n\n// BadRequestError wraps an error value to signify that a request could not be\n// read, decoded, or parsed such that in an HTTP scenario, http.StatusBadRequest\n// would be returned.\ntype BadRequestError struct {\n\terror\n}\n\n// NewBadRequestError returns err wrapped in a BadRequestError.\nfunc NewBadRequestError(err error) BadRequestError {\n\treturn BadRequestError{err}\n}\n\n// ConflictError wraps an error value to signify that a conflict with an\n// existing resource occurred such that in an HTTP scenario, http.StatusConflict\n// would be returned.\ntype ConflictError struct {\n\terror\n}\n\n// newConflictError returns err wrapped in a ConflictError.\nfunc newConflictError(err error) ConflictError {\n\treturn ConflictError{err}\n}\n\n// Unwrap makes it so that a ConflictError wrapping ErrFieldExists gets a\n// true from errors.Is(ErrFieldExists).\nfunc (c ConflictError) Unwrap() error {\n\treturn c.error\n}\n\n// NotFoundError wraps an error value to signify that a resource was not found\n// such that in an HTTP scenario, http.StatusNotFound would be returned.\ntype NotFoundError error\n\n// newNotFoundError returns err wrapped in a NotFoundError.\nfunc newNotFoundError(err error, name string) NotFoundError {\n\treturn NotFoundError(errors.WithMessage(err, name))\n}\n\ntype PreconditionFailedError struct {\n\terror\n}\n\n// newPreconditionFailedError returns err wrapped in a PreconditionFailedError.\nfunc newPreconditionFailedError(err error) PreconditionFailedError {\n\treturn PreconditionFailedError{err}\n}\n\n// Regular expression to validate index and field names.\n// The lowest limitation I've seen on any filesystem we care about is 255\n// characters. 230 leaves enough space that an index or field could be\n// backed up and have a timestamp and file extension appended while\n// still allowing for much longer index and field names. --Jaffee\nvar nameRegexp = regexp.MustCompile(`^[a-z][a-z0-9_-]{0,229}$`)\n\n// TimeFormat is the go-style time format used to parse string dates.\nconst TimeFormat = \"2006-01-02T15:04\"\n\n// ValidateName ensures that the index or field or view name is a valid format.\nfunc ValidateName(name string) error {\n\tif !nameRegexp.Match([]byte(name)) {\n\t\treturn errors.Wrapf(ErrName, \"'%s'\", name)\n\t}\n\treturn nil\n}\n\nfunc timestamp() int64 {\n\treturn time.Now().UnixNano()\n}\n\n// AddressWithDefaults converts addr into a valid address,\n// using defaults when necessary.\nfunc AddressWithDefaults(addr string) (*pnet.URI, error) {\n\tif addr == \"\" {\n\t\treturn pnet.DefaultURI(), nil\n\t}\n\treturn pnet.NewURIFromAddress(addr)\n}\n"
        },
        {
          "name": "pilosa_internal_test.go",
          "type": "blob",
          "size": 1.869140625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"bytes\"\n\t\"reflect\"\n\t\"testing\"\n\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\t. \"github.com/featurebasedb/featurebase/v3/vprint\" // nolint:staticcheck\n)\n\nfunc TestValidateName(t *testing.T) {\n\tnames := []string{\n\t\t\"a\", \"ab\", \"ab1\", \"b-c\", \"d_e\", \"exists\",\n\t\t\"longbutnottoolongaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa12345689012345689012345678901234567890\",\n\t}\n\tfor _, name := range names {\n\t\tif ValidateName(name) != nil {\n\t\t\tt.Fatalf(\"Should be valid index name: %s\", name)\n\t\t}\n\t}\n}\n\nfunc TestValidateNameInvalid(t *testing.T) {\n\tnames := []string{\n\t\t\"\", \"'\", \"^\", \"/\", \"\\\\\", \"A\", \"*\", \"a:b\", \"valid?no\", \"yce\", \"1\", \"_\", \"-\",\n\t\t\"long123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa1\", \"_exists\",\n\t}\n\tfor _, name := range names {\n\t\tif ValidateName(name) == nil {\n\t\t\tt.Fatalf(\"Should be invalid index name: %s\", name)\n\t\t}\n\t}\n}\n\nfunc TestAPI_CombineForExistence(t *testing.T) {\n\tbm := roaring.NewBitmap(pos(1, 1), pos(1, 2), pos(1, 3), pos(1, 65537), pos(1, 65538), pos(2, 1), pos(2, 2), pos(2, 5), pos(2, 65537), pos(2, 65538))\n\tbuf := new(bytes.Buffer)\n\t_, err := bm.WriteTo(buf)\n\tPanicOn(err)\n\traw := buf.Bytes()\n\tresults, err := combineForExistence(raw)\n\tif err != nil {\n\t\tt.Fatalf(\"failure to combine: %v\", err)\n\t}\n\tbm2 := roaring.NewBitmap()\n\t_, _, err = bm2.ImportRoaringBits(results, false, false, 1<<shardVsContainerExponent)\n\tPanicOn(err)\n\texpected := []uint64{1, 2, 3, 5, 65537, 65538}\n\tgot := bm2.Slice()\n\tif !reflect.DeepEqual(got, expected) {\n\t\tt.Fatalf(\"expected:%v got:%v\", expected, got)\n\t}\n\n}\n"
        },
        {
          "name": "pilosa_test.go",
          "type": "blob",
          "size": 2.1552734375,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa_test\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/featurebasedb/featurebase/v3\"\n\t_ \"github.com/featurebasedb/featurebase/v3/test\"\n)\n\nfunc TestAddressWithDefaults(t *testing.T) {\n\ttests := []struct {\n\t\taddr     string\n\t\texpected string\n\t\terr      string\n\t}{\n\t\t{addr: \"\", expected: \"localhost:10101\"},\n\t\t{addr: \":\", expected: \"localhost:10101\"},\n\t\t{addr: \"localhost\", expected: \"localhost:10101\"},\n\t\t{addr: \"localhost:\", expected: \"localhost:10101\"},\n\t\t{addr: \"127.0.0.1:10101\", expected: \"127.0.0.1:10101\"},\n\t\t{addr: \"127.0.0.1:\", expected: \"127.0.0.1:10101\"},\n\t\t{addr: \":10101\", expected: \"localhost:10101\"},\n\t\t{addr: \":55555\", expected: \"localhost:55555\"},\n\t\t{addr: \"1.2.3.4\", expected: \"1.2.3.4:10101\"},\n\t\t{addr: \"1.2.3.4:\", expected: \"1.2.3.4:10101\"},\n\t\t{addr: \"1.2.3.4:55555\", expected: \"1.2.3.4:55555\"},\n\t\t// The following tests check the error conditions.\n\t\t{addr: \"[invalid][addr]:port\", err: pilosa.ErrInvalidAddress.Error()},\n\t}\n\tfor _, test := range tests {\n\t\tactual, err := pilosa.AddressWithDefaults(test.addr)\n\t\tif err != nil {\n\t\t\tif !strings.Contains(err.Error(), test.err) {\n\t\t\t\tt.Errorf(\"expected error: %v, but got: %v\", test.err, err)\n\t\t\t}\n\t\t} else {\n\t\t\tif actual.HostPort() != test.expected {\n\t\t\t\tt.Errorf(\"expected: %v, but got: %v\", test.expected, actual)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestValidateName(t *testing.T) {\n\ttests := []struct {\n\t\tname string\n\t\terr  error\n\t}{\n\t\t{name: \"a_name\", err: nil},\n\t\t{name: \"a-name\", err: nil},\n\t\t{name: \"a-name-10\", err: nil},\n\t\t{name: \"A-name\", err: fmt.Errorf(\"'A-name': %s\", pilosa.ErrName)},\n\t\t{name: \"-a-name\", err: fmt.Errorf(\"'-a-name': %s\", pilosa.ErrName)},\n\t\t{name: \"8th_name\", err: fmt.Errorf(\"'8th_name': %s\", pilosa.ErrName)},\n\t\t{name: \"_name\", err: fmt.Errorf(\"'_name': %s\", pilosa.ErrName)},\n\t\t{name: \"a_NaMe\", err: fmt.Errorf(\"'a_NaMe': %s\", pilosa.ErrName)},\n\t\t{name: \"indexname\", err: nil},\n\t}\n\tfor _, test := range tests {\n\t\terr := pilosa.ValidateName(test.name)\n\t\tif !(err == nil && test.err == nil) {\n\t\t\tif err.Error() != test.err.Error() {\n\t\t\t\tt.Errorf(\"expected error: %v, but got: %v\", test.err, err)\n\t\t\t}\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "pprof.go",
          "type": "blob",
          "size": 2.4892578125,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"runtime\"\n\t\"runtime/pprof\"\n\t\"time\"\n\n\t_ \"net/http/pprof\" // Imported for its side-effect of registering pprof endpoints with the server.\n\n\t\"github.com/featurebasedb/featurebase/v3/storage\"\n\t\"github.com/featurebasedb/featurebase/v3/vprint\"\n)\n\n// CPUProfileForDur (where \"Dur\" is short for \"Duration\"), is used for\n// performance tuning during development. It's only calledbut is currently\n// commented outin holder.go.\nfunc CPUProfileForDur(dur time.Duration, outpath string) {\n\t// per-query pprof output:\n\tbackend := storage.DefaultBackend\n\tpath := outpath + \".\" + backend\n\tf, err := os.Create(path)\n\tvprint.PanicOn(err)\n\n\tif dur == 0 {\n\t\tdur = time.Minute\n\t}\n\tvprint.AlwaysPrintf(\"starting cpu profile for dur '%v', output to '%v'\", dur, path)\n\t_ = pprof.StartCPUProfile(f)\n\tgo func() {\n\t\t<-time.After(dur)\n\t\tpprof.StopCPUProfile()\n\t\tf.Close()\n\t\tvprint.AlwaysPrintf(\"stopping cpu profile after dur '%v', output: '%v'\", dur, path)\n\t}()\n}\n\n// MemProfileForDur (where \"Dur\" is short for \"Duration\"), is used for\n// performance tuning during development. It's only calledbut is currently\n// commented outin holder.go.\nfunc MemProfileForDur(dur time.Duration, outpath string) {\n\t// per-query pprof output:\n\tbackend := storage.DefaultBackend\n\tpath := outpath + \".\" + backend\n\tf, err := os.Create(path)\n\tvprint.PanicOn(err)\n\n\tif dur == 0 {\n\t\tdur = time.Minute\n\t}\n\tvprint.AlwaysPrintf(\"will write memory profile after dur '%v', output to '%v'\", dur, path)\n\tgo func() {\n\t\t<-time.After(dur)\n\t\truntime.GC() // get up-to-date statistics\n\t\tif err := pprof.WriteHeapProfile(f); err != nil {\n\t\t\tvprint.PanicOn(fmt.Sprintf(\"could not write memory profile: %v\", err))\n\t\t}\n\t\tf.Close()\n\t\tvprint.AlwaysPrintf(\"wrote memory profile after dur '%v', output: '%v'\", dur, path)\n\t}()\n}\n\ntype pprofProfile struct {\n\tfdCpu *os.File\n}\n\nvar _ = newPprof\nvar _ = pprofProfile{}\n\n// for manually calling Close() to stop profiling.\nfunc newPprof() (pp *pprofProfile) {\n\tpp = &pprofProfile{}\n\tf, err := os.Create(\"cpu.manual.pprof\")\n\tvprint.PanicOn(err)\n\tpp.fdCpu = f\n\n\t_ = pprof.StartCPUProfile(pp.fdCpu)\n\treturn\n}\n\nfunc (pp *pprofProfile) Close() {\n\n\tpprof.StopCPUProfile()\n\tpp.fdCpu.Close()\n\n\tf, err := os.Create(\"mem.manual.pprof\")\n\tvprint.PanicOn(err)\n\n\truntime.GC() // get up-to-date statistics\n\tif err := pprof.WriteHeapProfile(f); err != nil {\n\t\tvprint.PanicOn(fmt.Sprintf(\"could not write memory profile: %v\", err))\n\t}\n\tf.Close()\n}\n"
        },
        {
          "name": "pql",
          "type": "tree",
          "content": null
        },
        {
          "name": "prometheus",
          "type": "tree",
          "content": null
        },
        {
          "name": "proto",
          "type": "tree",
          "content": null
        },
        {
          "name": "qa",
          "type": "tree",
          "content": null
        },
        {
          "name": "querycontext",
          "type": "tree",
          "content": null
        },
        {
          "name": "rbf.go",
          "type": "blob",
          "size": 16.9638671875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"os\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/featurebasedb/featurebase/v3/rbf\"\n\trbfcfg \"github.com/featurebasedb/featurebase/v3/rbf/cfg\"\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\ttxkey \"github.com/featurebasedb/featurebase/v3/short_txkey\"\n\t\"github.com/featurebasedb/featurebase/v3/storage\"\n\n\t\"github.com/pkg/errors\"\n)\n\n// RbfDBWrapper wraps an *rbf.DB\ntype RbfDBWrapper struct {\n\tpath string\n\tdb   *rbf.DB\n\tcfg  *rbfcfg.Config\n\treg  *rbfDBRegistrar\n\tmuDb sync.Mutex\n\n\topenTx map[*RBFTx]bool\n\n\t// make Close() idempotent, avoiding panic on double Close()\n\tclosed bool\n\n\t//DeleteEmptyContainer bool // needed for roaring compat?\n\n\tdoAllocZero bool\n}\n\nfunc (w *RbfDBWrapper) Path() string {\n\treturn w.path\n}\n\nfunc (w *RbfDBWrapper) SetHolder(h *Holder) {\n\t// don't need it at the moment\n\t//w.h = h\n}\n\nfunc (w *RbfDBWrapper) CleanupTx(tx Tx) {\n\tr := tx.(*RBFTx)\n\tr.mu.Lock()\n\tif r.done {\n\t\tr.mu.Unlock()\n\t\treturn\n\t}\n\tr.done = true\n\tr.mu.Unlock()\n\n\t// try not to hold r.mu while locking w.muDb\n\tw.muDb.Lock()\n\n\tdelete(w.openTx, r)\n\n\tw.muDb.Unlock()\n}\n\n// rbfDBRegistrar also allows opening the same path twice to\n// result in sharing the same open database handle, and\n// thus the same transactional guarantees.\ntype rbfDBRegistrar struct {\n\tmu sync.Mutex\n\tmp map[*RbfDBWrapper]bool\n\n\tpath2db map[string]*RbfDBWrapper\n\n\trbfConfig *rbfcfg.Config\n}\n\nfunc (r *rbfDBRegistrar) SetRBFConfig(cfg *rbfcfg.Config) {\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\tr.rbfConfig = cfg\n}\n\nfunc (r *rbfDBRegistrar) Size() int {\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\tnmp := len(r.mp)\n\tnpa := len(r.path2db)\n\tif nmp != npa {\n\t\tpanic(fmt.Sprintf(\"nmp=%v, vs npa=%v\", nmp, npa))\n\t}\n\treturn nmp\n}\n\nvar globalRbfDBReg *rbfDBRegistrar = newRbfDBRegistrar()\n\nfunc newRbfDBRegistrar() *rbfDBRegistrar {\n\treturn &rbfDBRegistrar{\n\t\tmp:      make(map[*RbfDBWrapper]bool),\n\t\tpath2db: make(map[string]*RbfDBWrapper),\n\t}\n}\n\n// register each rbf.DB created, so we dedup and can\n// can clean them up. This is called by OpenDBWrapper() while\n// holding the r.mu.Lock, since it needs to atomically\n// check the registry and make a new instance only\n// if one does not exist for its path, and otherwise\n// return the existing instance.\nfunc (r *rbfDBRegistrar) unprotectedRegister(w *RbfDBWrapper) {\n\tr.mp[w] = true\n\tr.path2db[w.path] = w\n}\n\n// unregister removes w from r\nfunc (r *rbfDBRegistrar) unregister(w *RbfDBWrapper) {\n\tr.mu.Lock()\n\tdelete(r.mp, w)\n\tdelete(r.path2db, w.path)\n\tr.mu.Unlock()\n}\n\n// OpenDBWrapper opens the database in the path directory\n// without deleting any prior content. Any\n// database directory will have the \"-rbf\" suffix.\n//\n// OpenDBWrapper will check the registry and make a new instance only\n// if one does not exist for its path. Otherwise it returns\n// the existing instance. This insures only one RbfDBWrapper\n// per bpath in this pilosa node.\nfunc (r *rbfDBRegistrar) OpenDBWrapper(path string, doAllocZero bool, cfg *storage.Config) (DBWrapper, error) {\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\tw, ok := r.path2db[path]\n\tif ok {\n\t\t// creates the effect of having only one DB open per pilosa node.\n\t\treturn w, nil\n\t}\n\tif r.rbfConfig == nil {\n\t\tr.rbfConfig = rbfcfg.NewDefaultConfig()\n\t\tr.rbfConfig.DoAllocZero = doAllocZero\n\t\tr.rbfConfig.FsyncEnabled = cfg.FsyncEnabled\n\t}\n\tdb := rbf.NewDB(path, r.rbfConfig)\n\n\tw = &RbfDBWrapper{\n\t\treg:         r,\n\t\tpath:        path,\n\t\tdb:          db,\n\t\tdoAllocZero: doAllocZero,\n\t\topenTx:      make(map[*RBFTx]bool),\n\t\tcfg:         r.rbfConfig,\n\t}\n\tr.unprotectedRegister(w)\n\n\terr := db.Open()\n\tif err != nil {\n\t\tpanic(fmt.Sprintf(\"cannot open rbfDB at path '%v': '%v'\", path, err))\n\t}\n\treturn w, nil\n}\n\ntype RBFTx struct {\n\t// initialIndex is only a debugging aid. Transactions\n\t// can cross indexes. It can be left empty without consequence.\n\tinitialIndex string\n\ttx           *rbf.Tx\n\to            Txo\n\tDb           *RbfDBWrapper\n\n\tdone bool\n\tmu   sync.Mutex // protect done as it changes state\n}\n\nfunc (tx *RBFTx) DBPath() string {\n\treturn tx.tx.DBPath()\n}\n\nfunc (tx *RBFTx) Type() string {\n\treturn RBFTxn\n}\n\nfunc (tx *RBFTx) Rollback() {\n\ttx.tx.Rollback()\n\ttx.Db.CleanupTx(tx)\n}\n\nfunc (tx *RBFTx) Commit() (err error) {\n\terr = tx.tx.Commit()\n\ttx.Db.CleanupTx(tx)\n\treturn err\n}\n\nfunc (tx *RBFTx) RoaringBitmap(index, field, view string, shard uint64) (*roaring.Bitmap, error) {\n\treturn tx.tx.RoaringBitmap(rbfName(index, field, view, shard))\n}\n\nfunc (tx *RBFTx) Container(index, field, view string, shard uint64, key uint64) (*roaring.Container, error) {\n\treturn tx.tx.Container(rbfName(index, field, view, shard), key)\n}\n\nfunc (tx *RBFTx) PutContainer(index, field, view string, shard uint64, key uint64, c *roaring.Container) error {\n\treturn tx.tx.PutContainer(rbfName(index, field, view, shard), key, c)\n}\n\nfunc (tx *RBFTx) RemoveContainer(index, field, view string, shard uint64, key uint64) error {\n\treturn tx.tx.RemoveContainer(rbfName(index, field, view, shard), key)\n}\n\n// Add sets all the a bits hot in the specified fragment.\nfunc (tx *RBFTx) Add(index, field, view string, shard uint64, a ...uint64) (changeCount int, err error) {\n\treturn tx.addOrRemove(index, field, view, shard, false, a...)\n}\n\n// Remove clears all the specified a bits in the chosen fragment.\nfunc (tx *RBFTx) Remove(index, field, view string, shard uint64, a ...uint64) (changeCount int, err error) {\n\treturn tx.addOrRemove(index, field, view, shard, true, a...)\n}\n\n// Removed clears the specified bits and tells you which ones it actually removed.\nfunc (tx *RBFTx) Removed(index, field, view string, shard uint64, a ...uint64) (changed []uint64, err error) {\n\tif len(a) == 0 {\n\t\treturn a, nil\n\t}\n\tname := rbfName(index, field, view, shard)\n\t// this special case can/should possibly go away, except that it\n\t// turns out to be by far the most common case, and we need to know\n\t// there's at least two items to simplify the check-sorted thing.\n\tif len(a) == 1 {\n\t\thi, lo := highbits(a[0]), lowbits(a[0])\n\t\trc, err := tx.tx.Container(name, hi)\n\t\tif err != nil {\n\t\t\treturn a[:0], errors.Wrap(err, \"failed to retrieve container\")\n\t\t}\n\t\tif rc.N() == 0 {\n\t\t\treturn a[:0], nil\n\t\t}\n\t\trc1, chng := rc.Remove(lo)\n\t\tif !chng {\n\t\t\treturn a[:0], nil\n\t\t}\n\t\tif rc1.N() == 0 {\n\t\t\terr = tx.tx.RemoveContainer(name, hi)\n\t\t} else {\n\t\t\terr = tx.tx.PutContainer(name, hi, rc1)\n\t\t}\n\t\tif err != nil {\n\t\t\treturn a[:0], err\n\t\t}\n\t\treturn a[:1], nil\n\t}\n\n\tchangeCount := 0\n\tchanged = a\n\n\tvar lastHi uint64 = math.MaxUint64 // highbits is always less than this starter.\n\tvar rc *roaring.Container\n\tvar hi uint64\n\tvar lo uint16\n\n\tfor i, v := range a {\n\t\thi, lo = highbits(v), lowbits(v)\n\t\tif hi != lastHi {\n\t\t\t// either first time through, or changed to a different container.\n\t\t\t// do we need put the last updated container now?\n\t\t\tif i > 0 {\n\t\t\t\t// not first time through, write what we got.\n\t\t\t\tif rc == nil || rc.N() == 0 {\n\t\t\t\t\terr = tx.tx.RemoveContainer(name, lastHi)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn a[:0], errors.Wrap(err, \"failed to remove container\")\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\terr = tx.tx.PutContainer(name, lastHi, rc)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn a[:0], errors.Wrap(err, \"failed to put container\")\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t// get the next container\n\t\t\trc, err = tx.tx.Container(name, hi)\n\t\t\tif err != nil {\n\t\t\t\treturn a[:0], errors.Wrap(err, \"failed to retrieve container\")\n\t\t\t}\n\t\t} // else same container, keep adding bits to rct.\n\t\tchng := false\n\t\trc, chng = rc.Remove(lo)\n\t\tif chng {\n\t\t\tchanged[changeCount] = v\n\t\t\tchangeCount++\n\t\t}\n\t\tlastHi = hi\n\t}\n\t// write the last updates.\n\n\tif rc == nil || rc.N() == 0 {\n\t\terr = tx.tx.RemoveContainer(name, hi)\n\t\tif err != nil {\n\t\t\treturn a[:0], errors.Wrap(err, \"failed to remove container\")\n\t\t}\n\t} else {\n\t\terr = tx.tx.PutContainer(name, hi, rc)\n\t\tif err != nil {\n\t\t\treturn a[:0], errors.Wrap(err, \"failed to put container\")\n\t\t}\n\t}\n\treturn changed[:changeCount], nil\n}\n\n// sortedParanoia is a flag to enable a check for unsorted inputs to addOrRemove,\n// which is expensive in practice and only really useful occasionally.\nconst sortedParanoia = false\n\nfunc (tx *RBFTx) addOrRemove(index, field, view string, shard uint64, remove bool, a ...uint64) (changeCount int, err error) {\n\tif len(a) == 0 {\n\t\treturn 0, nil\n\t}\n\tname := rbfName(index, field, view, shard)\n\t// this special case can/should possibly go away, except that it\n\t// turns out to be by far the most common case, and we need to know\n\t// there's at least two items to simplify the check-sorted thing.\n\tif len(a) == 1 {\n\t\thi, lo := highbits(a[0]), lowbits(a[0])\n\t\trc, err := tx.tx.Container(name, hi)\n\t\tif err != nil {\n\t\t\treturn 0, errors.Wrap(err, \"failed to retrieve container\")\n\t\t}\n\t\tif remove {\n\t\t\tif rc.N() == 0 {\n\t\t\t\treturn 0, nil\n\t\t\t}\n\t\t\trc1, chng := rc.Remove(lo)\n\t\t\tif !chng {\n\t\t\t\treturn 0, nil\n\t\t\t}\n\t\t\tif rc1.N() == 0 {\n\t\t\t\terr = tx.tx.RemoveContainer(name, hi)\n\t\t\t} else {\n\t\t\t\terr = tx.tx.PutContainer(name, hi, rc1)\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\treturn 1, nil\n\t\t} else {\n\t\t\trc2, chng := rc.Add(lo)\n\t\t\tif !chng {\n\t\t\t\treturn 0, nil\n\t\t\t}\n\t\t\terr = tx.tx.PutContainer(name, hi, rc2)\n\t\t\tif err != nil {\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\treturn 1, nil\n\t\t}\n\t}\n\n\tvar lastHi uint64 = math.MaxUint64 // highbits is always less than this starter.\n\tvar rc *roaring.Container\n\tvar hi uint64\n\tvar lo uint16\n\n\t// we can accept sorted either ascending or descending.\n\tsign := a[1] - a[0]\n\tprev := a[0] - sign\n\tsign >>= 63\n\tfor i, v := range a {\n\t\t// This check is noticably expensive (a few percent in some\n\t\t// use cases) and as long as it passes occasionally it's probably\n\t\t// not important to run it all the time, and anyway panic is\n\t\t// not a good choice outside of testing.\n\t\tif sortedParanoia {\n\t\t\tif (v-prev)>>63 != sign {\n\t\t\t\texplain := fmt.Sprintf(\"addOrRemove: %d < %d != %d < %d\", v, prev, a[1], a[0])\n\t\t\t\tpanic(explain)\n\t\t\t}\n\t\t\tif v == prev {\n\t\t\t\texplain := fmt.Sprintf(\"addOrRemove: %d twice\", v)\n\t\t\t\tpanic(explain)\n\t\t\t}\n\t\t}\n\t\tprev = v\n\t\thi, lo = highbits(v), lowbits(v)\n\t\tif hi != lastHi {\n\t\t\t// either first time through, or changed to a different container.\n\t\t\t// do we need put the last updated container now?\n\t\t\tif i > 0 {\n\t\t\t\t// not first time through, write what we got.\n\t\t\t\tif remove && (rc == nil || rc.N() == 0) {\n\t\t\t\t\terr = tx.tx.RemoveContainer(name, lastHi)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn 0, errors.Wrap(err, \"failed to remove container\")\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\terr = tx.tx.PutContainer(name, lastHi, rc)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn 0, errors.Wrap(err, \"failed to put container\")\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t// get the next container\n\t\t\trc, err = tx.tx.Container(name, hi)\n\t\t\tif err != nil {\n\t\t\t\treturn 0, errors.Wrap(err, \"failed to retrieve container\")\n\t\t\t}\n\t\t} // else same container, keep adding bits to rct.\n\t\tchng := false\n\t\t// rc can be nil before, and nil after, in both Remove/Add below.\n\t\t// The roaring container add() and remove() methods handle this.\n\t\tif remove {\n\t\t\trc, chng = rc.Remove(lo)\n\t\t} else {\n\t\t\trc, chng = rc.Add(lo)\n\t\t}\n\t\tif chng {\n\t\t\tchangeCount++\n\t\t}\n\t\tlastHi = hi\n\t}\n\t// write the last updates.\n\tif remove {\n\t\tif rc == nil || rc.N() == 0 {\n\t\t\terr = tx.tx.RemoveContainer(name, hi)\n\t\t\tif err != nil {\n\t\t\t\treturn 0, errors.Wrap(err, \"failed to remove container\")\n\t\t\t}\n\t\t} else {\n\t\t\terr = tx.tx.PutContainer(name, hi, rc)\n\t\t\tif err != nil {\n\t\t\t\treturn 0, errors.Wrap(err, \"failed to put container\")\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif rc == nil || rc.N() == 0 {\n\t\t\tpanic(\"there should be no way to have an empty bitmap AFTER an Add() operation\")\n\t\t}\n\t\terr = tx.tx.PutContainer(name, hi, rc)\n\t\tif err != nil {\n\t\t\treturn 0, errors.Wrap(err, \"failed to put container\")\n\t\t}\n\t}\n\treturn\n}\n\nfunc (tx *RBFTx) Contains(index, field, view string, shard uint64, v uint64) (exists bool, err error) {\n\treturn tx.tx.Contains(rbfName(index, field, view, shard), v)\n}\n\nfunc (tx *RBFTx) ContainerIterator(index, field, view string, shard uint64, key uint64) (citer roaring.ContainerIterator, found bool, err error) {\n\treturn tx.tx.ContainerIterator(rbfName(index, field, view, shard), key)\n}\n\nfunc (tx *RBFTx) Count(index, field, view string, shard uint64) (uint64, error) {\n\treturn tx.tx.Count(rbfName(index, field, view, shard))\n}\n\nfunc (tx *RBFTx) Max(index, field, view string, shard uint64) (uint64, error) {\n\treturn tx.tx.Max(rbfName(index, field, view, shard))\n}\n\nfunc (tx *RBFTx) Min(index, field, view string, shard uint64) (uint64, bool, error) {\n\treturn tx.tx.Min(rbfName(index, field, view, shard))\n}\n\n// CountRange returns the count of hot bits in the start, end range on the fragment.\n// roaring.countRange counts the number of bits set between [start, end).\nfunc (tx *RBFTx) CountRange(index, field, view string, shard uint64, start, end uint64) (n uint64, err error) {\n\treturn tx.tx.CountRange(rbfName(index, field, view, shard), start, end)\n}\n\nfunc (tx *RBFTx) OffsetRange(index, field, view string, shard uint64, offset, start, end uint64) (*roaring.Bitmap, error) {\n\treturn tx.tx.OffsetRange(rbfName(index, field, view, shard), offset, start, end)\n}\n\nfunc (tx *RBFTx) ImportRoaringBits(index, field, view string, shard uint64, rit roaring.RoaringIterator, clear bool, log bool, rowSize uint64) (changed int, rowSet map[uint64]int, err error) {\n\treturn tx.tx.ImportRoaringBits(rbfName(index, field, view, shard), rit, clear, log, rowSize)\n}\n\nfunc (tx *RBFTx) ApplyFilter(index, field, view string, shard uint64, ckey uint64, filter roaring.BitmapFilter) (err error) {\n\terr = tx.tx.ApplyFilter(rbfName(index, field, view, shard), ckey, filter)\n\treturn errors.Wrap(err, fmt.Sprintf(\"applying  filter for index %s, field %s, view %s, shard %d\", index, field, view, shard))\n}\n\nfunc (tx *RBFTx) ApplyRewriter(index, field, view string, shard uint64, ckey uint64, filter roaring.BitmapRewriter) (err error) {\n\terr = tx.tx.ApplyRewriter(rbfName(index, field, view, shard), ckey, filter)\n\treturn errors.Wrap(err, fmt.Sprintf(\"applying rewriter for index %s, field %s, view %s, shard %d\", index, field, view, shard))\n}\n\nfunc (tx *RBFTx) GetSortedFieldViewList(idx *Index, shard uint64) (fvs []txkey.FieldView, err error) {\n\treturn tx.tx.GetSortedFieldViewList()\n}\n\nfunc (tx *RBFTx) GetFieldSizeBytes(index, field string) (uint64, error) {\n\treturn tx.tx.GetSizeBytesWithPrefix(string(txkey.FieldPrefix(index, field)))\n}\n\n// SnapshotReader returns a reader that provides a snapshot of the current database.\nfunc (tx *RBFTx) SnapshotReader() (io.Reader, error) {\n\treturn tx.tx.SnapshotReader()\n}\n\n// rbfName returns a NULL-separated key used for identifying bitmap maps in RBF.\nfunc rbfName(index, field, view string, shard uint64) string {\n\treturn string(txkey.Prefix(index, field, view, shard))\n}\n\n// rbfFieldPrefix returns a prefix for field keys in RBF.\nfunc rbfFieldPrefix(index, field string) string {\n\t//return fmt.Sprintf(\"%s\\x00%s\\x00\", index, field)\n\treturn string(txkey.FieldPrefix(index, field))\n}\n\nfunc (w *RbfDBWrapper) HasData() (has bool, err error) {\n\tw.muDb.Lock()\n\tdefer w.muDb.Unlock()\n\treturn w.db.HasData(false) // false => any prior attempt at write means we \"have data\"\n}\n\nfunc (w *RbfDBWrapper) DeleteField(index, field, fieldPath string) error {\n\tw.muDb.Lock()\n\tdefer w.muDb.Unlock()\n\n\tif err := os.RemoveAll(fieldPath); err != nil {\n\t\treturn errors.Wrap(err, \"removing directory\")\n\t}\n\n\ttx, err := w.db.Begin(true)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer tx.Rollback()\n\n\tif err := tx.DeleteBitmapsWithPrefix(rbfFieldPrefix(index, field)); err != nil {\n\t\treturn err\n\t}\n\treturn tx.Commit()\n}\n\nfunc (w *RbfDBWrapper) DeleteIndex(indexName string) error {\n\n\tif strings.Contains(indexName, \"'\") {\n\t\treturn fmt.Errorf(\"error: bad indexName `%v` in RbfDBWrapper.DeleteIndex() call: indexName cannot contain apostrophes/single quotes\", indexName)\n\t}\n\tprefix := txkey.IndexOnlyPrefix(indexName)\n\n\tw.muDb.Lock()\n\tdefer w.muDb.Unlock()\n\n\ttx, err := w.db.Begin(true)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer tx.Rollback()\n\n\tif err := tx.DeleteBitmapsWithPrefix(string(prefix)); err != nil {\n\t\treturn err\n\t}\n\treturn tx.Commit()\n}\n\nfunc (w *RbfDBWrapper) Close() error {\n\tw.muDb.Lock()\n\tdefer w.muDb.Unlock()\n\tif !w.closed {\n\t\tw.reg.unregister(w)\n\t\tw.closed = true\n\t}\n\treturn w.db.Close()\n}\n\n// needed to handle the special case on reload, the close method unregisters the wrapper and all that is\n// required is the backing file get reloaded\n\nfunc (w *RbfDBWrapper) CloseDB() error {\n\tw.muDb.Lock()\n\tdefer w.muDb.Unlock()\n\tw.closed = true\n\treturn w.db.Close()\n}\nfunc (w *RbfDBWrapper) OpenDB() error {\n\tw.muDb.Lock()\n\tdefer w.muDb.Unlock()\n\terr := w.db.Open()\n\tif err != nil {\n\t\treturn err\n\t}\n\tw.closed = false\n\treturn nil\n}\n\nfunc (w *RbfDBWrapper) NewTx(write bool, initialIndex string, o Txo) (_ Tx, err error) {\n\ttx, err := w.db.Begin(write)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trtx := &RBFTx{\n\t\ttx:           tx,\n\t\tinitialIndex: initialIndex,\n\t\to:            o,\n\t\tDb:           w,\n\t}\n\n\tw.muDb.Lock()\n\tw.openTx[rtx] = true\n\tw.muDb.Unlock()\n\n\treturn rtx, nil\n}\n\nfunc (w *RbfDBWrapper) DeleteFragment(index, field, view string, shard uint64, frag interface{}) error {\n\ttx, err := w.db.Begin(true)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer tx.Rollback()\n\n\terr = tx.DeleteBitmapsWithPrefix(rbfName(index, field, view, shard))\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn tx.Commit()\n}\n\nfunc (w *RbfDBWrapper) OpenListString() (r string) {\n\treturn \"rbf OpenListString not implemented yet\"\n}\n"
        },
        {
          "name": "rbf",
          "type": "tree",
          "content": null
        },
        {
          "name": "roaring",
          "type": "tree",
          "content": null
        },
        {
          "name": "row.go",
          "type": "blob",
          "size": 17.875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"encoding/json\"\n\t\"sort\"\n\n\tpb \"github.com/featurebasedb/featurebase/v3/proto\"\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\t\"github.com/pkg/errors\"\n)\n\n// Row is a set of integers (the associated columns).\ntype Row struct {\n\tSegments []RowSegment\n\n\t// String keys translated to/from segment columns.\n\tKeys []string\n\n\t// Index tells what index this row is from - needed for key translation.\n\tIndex string\n\n\t// Field tells what field this row is from if it's a \"vertical\"\n\t// row. It may be the result of a Distinct query or Rows\n\t// query. Knowing the index and field, we can figure out how to\n\t// interpret the row data.\n\tField string\n\n\t// NoSplit indicates that this row may not be split.\n\t// This is used for `Rows` calls in a GroupBy.\n\tNoSplit bool\n}\n\n// NewRow returns a new instance of Row.\nfunc NewRow(columns ...uint64) *Row {\n\tr := &Row{}\n\tfor _, i := range columns {\n\t\tr.SetBit(i)\n\t}\n\treturn r\n}\n\nfunc (r *Row) Clone() (clone *Row) {\n\tif r == nil {\n\t\treturn nil\n\t}\n\tvar keyClone []string\n\tif len(r.Keys) > 0 {\n\t\tkeyClone = make([]string, len(r.Keys))\n\t\tcopy(keyClone, r.Keys)\n\t}\n\n\tclone = &Row{\n\t\tKeys:  keyClone,\n\t\tIndex: r.Index,\n\t\tField: r.Field,\n\t}\n\n\tfor _, seg := range r.Segments {\n\t\tsegClone := RowSegment{\n\t\t\tshard:    seg.shard,\n\t\t\twritable: true, // we know it is safe; it is a copy.\n\t\t\tn:        seg.n,\n\t\t}\n\t\tif seg.data != nil {\n\t\t\tsegClone.data = seg.data.Clone() // *roaring.Bitmap\n\t\t}\n\t\t// segClone.InvalidateCount() // not needed?\n\t\tclone.Segments = append(clone.Segments, segClone)\n\t}\n\treturn clone\n}\n\n// NewRowFromBitmap divides a bitmap into rows, which it now calls shards. This\n// transposes; data that was in any shard for Row 0 is now considered shard 0,\n// etcetera.\nfunc NewRowFromBitmap(b *roaring.Bitmap) *Row {\n\tr := &Row{}\n\tif b == nil {\n\t\treturn r\n\t}\n\trowNum := uint64(0)\n\tfor col, ok := b.MinAt(rowNum * ShardWidth); ok; col, ok = b.MinAt(rowNum * ShardWidth) {\n\t\trowNum = col / ShardWidth\n\t\tseg := RowSegment{\n\t\t\tshard:    rowNum,\n\t\t\tdata:     b.OffsetRange(rowNum*ShardWidth, rowNum*ShardWidth, (rowNum+1)*ShardWidth),\n\t\t\twritable: true,\n\t\t}\n\t\tseg.n = seg.data.Count()\n\t\tr.Segments = append(r.Segments, seg)\n\t\trowNum++\n\t}\n\treturn r\n}\n\n// NewRowFromRoaring parses a roaring data file as a row, dividing it into\n// bitmaps and rowSegments based on shard width.\nfunc NewRowFromRoaring(data []byte) *Row {\n\tbitmaps, shards := roaring.RoaringToBitmaps(data, ShardWidth)\n\tr := &Row{Segments: make([]RowSegment, len(bitmaps))}\n\tfor i := range bitmaps {\n\t\tsegment := RowSegment{\n\t\t\tshard:    shards[i],\n\t\t\tdata:     bitmaps[i],\n\t\t\twritable: false,\n\t\t\tn:        bitmaps[i].Count(),\n\t\t}\n\t\tr.Segments[i] = segment\n\t}\n\treturn r\n}\n\n// ToTable implements the ToTabler interface.\nfunc (r *Row) ToTable() (*pb.TableResponse, error) {\n\tvar n int\n\tif len(r.Keys) > 0 {\n\t\tn = len(r.Keys)\n\t} else {\n\t\tn = len(r.Columns())\n\t}\n\treturn pb.RowsToTable(r, n)\n}\n\n// Hash calculate checksum code be useful in block hash join\nfunc (r *Row) Hash() uint64 {\n\thash := uint64(0)\n\tfor i := range r.Segments {\n\t\thash = r.Segments[i].data.Hash(hash)\n\t}\n\treturn hash\n}\n\n// ToRows implements the ToRowser interface.\nfunc (r *Row) ToRows(callback func(*pb.RowResponse) error) error {\n\tif len(r.Keys) > 0 {\n\t\t// Column keys\n\t\tci := []*pb.ColumnInfo{\n\t\t\t{Name: \"_id\", Datatype: \"string\"},\n\t\t}\n\t\tfor _, x := range r.Keys {\n\t\t\tif err := callback(&pb.RowResponse{\n\t\t\t\tHeaders: ci,\n\t\t\t\tColumns: []*pb.ColumnResponse{\n\t\t\t\t\t{ColumnVal: &pb.ColumnResponse_StringVal{StringVal: x}},\n\t\t\t\t},\n\t\t\t}); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"calling callback\")\n\t\t\t}\n\t\t\tci = nil // only send on the first\n\t\t}\n\t} else {\n\t\t// Column IDs\n\t\tci := []*pb.ColumnInfo{\n\t\t\t{Name: \"_id\", Datatype: \"uint64\"},\n\t\t}\n\t\tfor _, x := range r.Columns() {\n\t\t\tif err := callback(&pb.RowResponse{\n\t\t\t\tHeaders: ci,\n\t\t\t\tColumns: []*pb.ColumnResponse{\n\t\t\t\t\t{ColumnVal: &pb.ColumnResponse_Uint64Val{Uint64Val: x}},\n\t\t\t\t},\n\t\t\t}); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"calling callback\")\n\t\t\t}\n\t\t\tci = nil // only send on the first\n\t\t}\n\t}\n\treturn nil\n}\n\n// Roaring returns the row treated as a unified roaring bitmap.\nfunc (r *Row) Roaring() []byte {\n\tbitmaps := make([]*roaring.Bitmap, len(r.Segments))\n\tfor i := range r.Segments {\n\t\tbitmaps[i] = r.Segments[i].data\n\t}\n\treturn roaring.BitmapsToRoaring(bitmaps)\n}\n\n// IsEmpty returns true if the row doesn't contain any set bits.\nfunc (r *Row) IsEmpty() bool {\n\tif len(r.Segments) == 0 {\n\t\treturn true\n\t}\n\tfor i := range r.Segments {\n\t\tif r.Segments[i].n > 0 {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc (r *Row) Freeze() {\n\tfor _, s := range r.Segments {\n\t\ts.Freeze()\n\t}\n}\n\n// Merge merges data from other into r.\nfunc (r *Row) Merge(other *Row) {\n\tvar segments []RowSegment\n\n\titr := newMergeSegmentIterator(r.Segments, other.Segments)\n\tfor s0, s1 := itr.next(); s0 != nil || s1 != nil; s0, s1 = itr.next() {\n\t\t// Use the other row's data if segment is missing.\n\t\tif s0 == nil {\n\t\t\tsegments = append(segments, *s1)\n\t\t\tcontinue\n\t\t} else if s1 == nil {\n\t\t\tsegments = append(segments, *s0)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Otherwise merge.\n\t\ts0.Merge(s1)\n\t\tsegments = append(segments, *s0)\n\t}\n\n\tr.Segments = segments\n\tr.invalidateCount()\n}\n\n// intersectionCount returns the number of intersections between r and other.\nfunc (r *Row) intersectionCount(other *Row) uint64 {\n\tvar n uint64\n\n\titr := newMergeSegmentIterator(r.Segments, other.Segments)\n\tfor s0, s1 := itr.next(); s0 != nil || s1 != nil; s0, s1 = itr.next() {\n\t\t// Ignore non-overlapping segments.\n\t\tif s0 == nil || s1 == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tn += s0.IntersectionCount(s1)\n\t}\n\treturn n\n}\n\n// Intersect returns the itersection of r and other.\nfunc (r *Row) Intersect(other *Row) *Row {\n\tvar segments []RowSegment\n\n\titr := newMergeSegmentIterator(r.Segments, other.Segments)\n\tfor s0, s1 := itr.next(); s0 != nil || s1 != nil; s0, s1 = itr.next() {\n\t\t// Ignore non-overlapping segments.\n\t\tif s0 == nil || s1 == nil {\n\t\t\tcontinue\n\t\t}\n\t\tsegments = append(segments, *s0.Intersect(s1))\n\t}\n\n\treturn &Row{Segments: segments}\n}\n\n// Any returns true if row contains any bits.\nfunc (r *Row) Any() bool {\n\tfor _, s := range r.Segments {\n\t\tif s.data.Any() {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Xor returns the xor of r and other.\nfunc (r *Row) Xor(other *Row) *Row {\n\tvar segments []RowSegment\n\n\titr := newMergeSegmentIterator(r.Segments, other.Segments)\n\tfor s0, s1 := itr.next(); s0 != nil || s1 != nil; s0, s1 = itr.next() {\n\t\tif s1 == nil {\n\t\t\tsegments = append(segments, *s0)\n\t\t\tcontinue\n\t\t} else if s0 == nil {\n\t\t\tsegments = append(segments, *s1)\n\t\t\tcontinue\n\t\t}\n\n\t\tsegments = append(segments, *s0.Xor(s1))\n\t}\n\n\treturn &Row{Segments: segments}\n}\n\n// Union returns the bitwise union of r and other.\nfunc (r *Row) Union(others ...*Row) *Row {\n\tsegments := make([][]RowSegment, 0, len(others)+1)\n\tif len(r.Segments) > 0 {\n\t\tsegments = append(segments, r.Segments)\n\t}\n\tnextSegs := make([][]RowSegment, 0, len(others)+1)\n\ttoProcess := make([]*RowSegment, 0, len(others)+1)\n\tvar output []RowSegment\n\tfor _, other := range others {\n\t\tif len(other.Segments) > 0 {\n\t\t\tsegments = append(segments, other.Segments)\n\t\t}\n\t}\n\tfor len(segments) > 0 {\n\t\tshard := segments[0][0].shard\n\t\tfor _, segs := range segments {\n\t\t\tif segs[0].shard < shard {\n\t\t\t\tshard = segs[0].shard\n\t\t\t}\n\t\t}\n\t\tnextSegs = nextSegs[:0]\n\t\ttoProcess = toProcess[:0]\n\t\tfor _, segs := range segments {\n\t\t\tif segs[0].shard == shard {\n\t\t\t\ttoProcess = append(toProcess, &segs[0])\n\t\t\t\tsegs = segs[1:]\n\t\t\t}\n\t\t\tif len(segs) > 0 {\n\t\t\t\tnextSegs = append(nextSegs, segs)\n\t\t\t}\n\t\t}\n\t\t// at this point, \"toProcess\" is a list of all the segments\n\t\t// sharing the lowest ID, and nextSegs is a list of all the others.\n\t\t// Swap the segment lists (so we don't have to reallocate it)\n\t\tsegments, nextSegs = nextSegs, segments\n\t\tif len(toProcess) == 1 {\n\t\t\toutput = append(output, *toProcess[0])\n\t\t} else {\n\t\t\toutput = append(output, *toProcess[0].Union(toProcess[1:]...))\n\t\t}\n\t}\n\treturn &Row{Index: r.Index, Field: r.Field, Segments: output}\n}\n\n// Difference returns the diff of r and other.\nfunc (r *Row) Difference(others ...*Row) *Row {\n\tvar output []RowSegment\n\to := make(map[uint64][]*RowSegment)\n\n\tfor x := range others {\n\t\tfor y := range others[x].Segments {\n\t\t\tsegment := others[x].Segments[y]\n\t\t\to[segment.shard] = append(o[segment.shard], &segment)\n\t\t}\n\t}\n\tfor _, segment := range r.Segments {\n\n\t\tdest, ok := o[segment.shard]\n\t\tif ok {\n\t\t\toutput = append(output, *segment.Difference(dest...))\n\t\t} else {\n\t\t\toutput = append(output, segment)\n\t\t}\n\t}\n\treturn &Row{Segments: output}\n}\n\n// Shift returns the bitwise shift of r by n bits.\n// Currently only positive shift values are supported.\n//\n// NOTE: the Shift method is currently unsupported, and\n// is considerred to be incorrect. Please DO NOT use it.\n// We are leaving it here in case someone internally wants\n// to use it with the understanding that the results may\n// be incorrect.\n//\n// Why unsupported? For a full description, see:\n// https://github.com/featurebasedb/pilosa/issues/403.\n// In short, the current implementation will shift a bit\n// at the edge of a shard out of the shard and into a\n// container which is assumed to be an invalid container\n// for the shard. So for example, shifting the last bit\n// of shard 0 (containers 0-15) will shift that bit out\n// to container 16. While this \"sort of\" works, it\n// breaks an assumption about containers, and might stop\n// working in the future if that assumption is enforced.\nfunc (r *Row) Shift(n int64) (*Row, error) {\n\tif n < 0 {\n\t\treturn nil, errors.New(\"cannot shift by negative values\")\n\t} else if n == 0 {\n\t\treturn r, nil\n\t}\n\n\twork := r\n\tvar segments []RowSegment\n\tfor i := int64(0); i < n; i++ {\n\t\tsegments = segments[:0]\n\t\tfor _, segment := range work.Segments {\n\t\t\tshifted, err := segment.Shift()\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"shifting row segment\")\n\t\t\t}\n\t\t\tsegments = append(segments, *shifted)\n\t\t}\n\t\twork = &Row{Segments: segments}\n\t}\n\n\treturn work, nil\n}\n\n// SetBit sets the i-th column of the row.\nfunc (r *Row) SetBit(i uint64) (changed bool) {\n\treturn r.createSegmentIfNotExists(i / ShardWidth).SetBit(i)\n}\n\n// segment returns a segment for a given shard.\n// Returns nil if segment does not exist.\nfunc (r *Row) segment(shard uint64) *RowSegment {\n\tif i := sort.Search(len(r.Segments), func(i int) bool {\n\t\treturn r.Segments[i].shard >= shard\n\t}); i < len(r.Segments) && r.Segments[i].shard == shard {\n\t\treturn &r.Segments[i]\n\t}\n\treturn nil\n}\n\nfunc (r *Row) createSegmentIfNotExists(shard uint64) *RowSegment {\n\ti := sort.Search(len(r.Segments), func(i int) bool {\n\t\treturn r.Segments[i].shard >= shard\n\t})\n\n\t// Return exact match.\n\tif i < len(r.Segments) && r.Segments[i].shard == shard {\n\t\treturn &r.Segments[i]\n\t}\n\n\t// Insert new segment.\n\tr.Segments = append(r.Segments, RowSegment{data: roaring.NewSliceBitmap()})\n\tif i < len(r.Segments) {\n\t\tcopy(r.Segments[i+1:], r.Segments[i:])\n\t}\n\tr.Segments[i] = RowSegment{\n\t\tdata:     roaring.NewSliceBitmap(),\n\t\tshard:    shard,\n\t\twritable: true,\n\t}\n\n\treturn &r.Segments[i]\n}\n\n// invalidateCount updates the cached count in the row.\nfunc (r *Row) invalidateCount() {\n\tfor i := range r.Segments {\n\t\tr.Segments[i].InvalidateCount()\n\t}\n}\n\n// Count returns the number of columns in the row.\nfunc (r *Row) Count() uint64 {\n\tvar n uint64\n\tif r == nil {\n\t\t// Count(Distinct()) on an empty field panics here\n\t\treturn n\n\t}\n\tfor i := range r.Segments {\n\t\tn += r.Segments[i].Count()\n\t}\n\treturn n\n}\n\n// MarshalJSON returns a JSON-encoded byte slice of r.\nfunc (r *Row) MarshalJSON() ([]byte, error) {\n\tvar o struct {\n\t\tColumns []uint64 `json:\"columns\"`\n\t\tKeys    []string `json:\"keys,omitempty\"`\n\t}\n\to.Columns = r.Columns()\n\to.Keys = r.Keys\n\n\treturn json.Marshal(&o)\n}\n\n// Columns returns the columns in r as a slice of ints.\nfunc (r *Row) Columns() []uint64 {\n\t// We occasionally hit cases where we want to call Columns on something\n\t// that might not exist, but a nil slice would be fine.\n\tif r == nil {\n\t\treturn nil\n\t}\n\ta := make([]uint64, 0, r.Count())\n\tfor i := range r.Segments {\n\t\ta = append(a, r.Segments[i].Columns()...)\n\t}\n\treturn a\n}\n\nfunc (r *Row) ShardColumns() []int64 {\n\t// We occasionally hit cases where we want to call Columns on something\n\t// that might not exist, but a nil slice would be fine.\n\tif r == nil {\n\t\treturn nil\n\t}\n\ta := make([]int64, 0, r.Count())\n\tfor i := range r.Segments {\n\t\ta = append(a, r.Segments[i].ShardColumns()...)\n\t}\n\treturn a\n}\n\n// Includes returns true if the row contains the given column.\nfunc (r *Row) Includes(col uint64) bool {\n\tshard := col / ShardWidth\n\tfor i := range r.Segments {\n\t\tif r.Segments[i].shard == shard {\n\t\t\treturn r.Segments[i].data.Contains(col)\n\t\t}\n\t}\n\treturn false\n}\n\n// RowSegment holds a subset of a row.\n// This could point to a mmapped roaring bitmap or an in-memory bitmap. The\n// width of the segment will always match the shard width.\ntype RowSegment struct {\n\t// Shard this segment belongs to\n\tshard uint64\n\n\t// Underlying raw bitmap implementation.\n\t// This is an mmapped bitmap if writable is false. Otherwise\n\t// it is a heap allocated bitmap which can be manipulated.\n\tdata     *roaring.Bitmap\n\twritable bool\n\n\t// Bit count\n\tn uint64\n}\n\nfunc (s *RowSegment) Shard() uint64 {\n\treturn s.shard\n}\n\nfunc (s *RowSegment) Freeze() {\n\ts.data = s.data.Freeze()\n}\n\n/*\n// Raw returns the row segment as a byte slice.\n// It may be used by the gRPC server to deliver results\n// as a roaring bitmap instead of a stream of RowResults.\nfunc (s *rowSegment) Raw() (uint64, []byte) {\n\tvar buf bytes.Buffer\n\ts.data.WriteTo(&buf)\n\treturn s.shard, buf.Bytes()\n}\n*/\n\n// Merge adds chunks from other to s.\n// Chunks in s are overwritten if they exist in other.\nfunc (s *RowSegment) Merge(other *RowSegment) {\n\ts.ensureWritable()\n\n\titr := other.data.Iterator()\n\tfor v, eof := itr.Next(); !eof; v, eof = itr.Next() {\n\t\ts.SetBit(v)\n\t}\n}\n\n// IntersectionCount returns the number of intersections between s and other.\nfunc (s *RowSegment) IntersectionCount(other *RowSegment) uint64 {\n\treturn s.data.IntersectionCount(other.data)\n}\n\n// Intersect returns the itersection of s and other.\nfunc (s *RowSegment) Intersect(other *RowSegment) *RowSegment {\n\tdata := s.data.Intersect(other.data)\n\n\treturn &RowSegment{\n\t\tdata:  data,\n\t\tshard: s.shard,\n\t\tn:     data.Count(),\n\t}\n}\n\n// Union returns the bitwise union of s and other.\nfunc (s *RowSegment) Union(others ...*RowSegment) *RowSegment {\n\tdatas := make([]*roaring.Bitmap, len(others))\n\tfor i, other := range others {\n\t\tdatas[i] = other.data\n\t}\n\tdata := s.data.Union(datas...)\n\n\treturn &RowSegment{\n\t\tdata:  data,\n\t\tshard: s.shard,\n\t\tn:     data.Count(),\n\t}\n}\n\n// Difference returns the diff of s and other.\nfunc (s *RowSegment) Difference(others ...*RowSegment) *RowSegment {\n\tdatas := make([]*roaring.Bitmap, len(others))\n\tfor i, other := range others {\n\t\tdatas[i] = other.data\n\t}\n\tdata := s.data.Difference(datas...)\n\n\treturn &RowSegment{\n\t\tdata:  data,\n\t\tshard: s.shard,\n\t\tn:     data.Count(),\n\t}\n}\n\n// Xor returns the xor of s and other.\nfunc (s *RowSegment) Xor(other *RowSegment) *RowSegment {\n\tdata := s.data.Xor(other.data)\n\n\treturn &RowSegment{\n\t\tdata:  data,\n\t\tshard: s.shard,\n\t\tn:     data.Count(),\n\t}\n}\n\n// Shift returns s shifted by 1 bit.\nfunc (s *RowSegment) Shift() (*RowSegment, error) {\n\t// TODO: deal with overflow\n\t// See issue: https://github.com/featurebasedb/pilosa/issues/403\n\tdata, err := s.data.Shift(1)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"shifting roaring data\")\n\t}\n\n\treturn &RowSegment{\n\t\tdata:  data,\n\t\tshard: s.shard,\n\t\tn:     data.Count(),\n\t}, nil\n}\n\n// SetBit sets the i-th column of the row.\nfunc (s *RowSegment) SetBit(i uint64) (changed bool) {\n\ts.ensureWritable()\n\tchanged, _ = s.data.Add(i)\n\tif changed {\n\t\ts.n++\n\t}\n\treturn changed\n}\n\n// ClearBit clears the i-th column of the row.\nfunc (s *RowSegment) ClearBit(i uint64) (changed bool) {\n\ts.ensureWritable()\n\n\tchanged, _ = s.data.Remove(i)\n\tif changed {\n\t\ts.n--\n\t}\n\treturn changed\n}\n\n// InvalidateCount updates the cached count in the row.\nfunc (s *RowSegment) InvalidateCount() {\n\ts.n = s.data.Count()\n}\n\n// Columns returns a list of all columns set in the segment.\nfunc (s *RowSegment) Columns() []uint64 {\n\ta := make([]uint64, 0, s.Count())\n\titr := s.data.Iterator()\n\tfor v, eof := itr.Next(); !eof; v, eof = itr.Next() {\n\t\ta = append(a, v)\n\t}\n\treturn a\n}\n\n// Columns returns a list of all columns set in the segment, normalized from 0-shardwidth-1\nfunc (s *RowSegment) ShardColumns() []int64 {\n\ta := make([]int64, 0, s.Count())\n\titr := s.data.Iterator()\n\tmask := uint64(ShardWidth - 1)\n\tfor v, eof := itr.Next(); !eof; v, eof = itr.Next() {\n\t\ta = append(a, int64(mask&v))\n\t}\n\treturn a\n}\n\n// Count returns the number of set columns in the row.\nfunc (s *RowSegment) Count() uint64 { return s.n }\n\n// ensureWritable clones the segment if it is pointing to non-writable data.\nfunc (s *RowSegment) ensureWritable() {\n\tif s.writable {\n\t\treturn\n\t}\n\n\t// This doesn't actually clone all the containers, but does clone\n\t// the bitmap itself -- we get a new bitmap, but it just marks the\n\t// containers as frozen and shares them. It's now safe to write to\n\t// this bitmap, but the actual containers are copy-on-write.\n\ts.data = s.data.Freeze()\n\ts.writable = true\n}\n\n// mergeSegmentIterator produces an iterator that loops through two sets of segments.\ntype mergeSegmentIterator struct {\n\ta0, a1 []RowSegment\n}\n\n// newMergeSegmentIterator returns a new instance of mergeSegmentIterator.\nfunc newMergeSegmentIterator(a0, a1 []RowSegment) mergeSegmentIterator {\n\treturn mergeSegmentIterator{a0: a0, a1: a1}\n}\n\n// next returns the next set of segments.\nfunc (itr *mergeSegmentIterator) next() (s0, s1 *RowSegment) {\n\t// Find current segments.\n\tif len(itr.a0) > 0 {\n\t\ts0 = &itr.a0[0]\n\t}\n\tif len(itr.a1) > 0 {\n\t\ts1 = &itr.a1[0]\n\t}\n\n\t// Return if either or both are nil.\n\tif s0 == nil && s1 == nil {\n\t\treturn\n\t} else if s0 == nil {\n\t\titr.a1 = itr.a1[1:]\n\t\treturn\n\t} else if s1 == nil {\n\t\titr.a0 = itr.a0[1:]\n\t\treturn\n\t}\n\n\t// Otherwise determine which is first.\n\tif s0.shard < s1.shard {\n\t\titr.a0 = itr.a0[1:]\n\t\treturn s0, nil\n\t} else if s0.shard > s1.shard {\n\t\titr.a1 = itr.a1[1:]\n\t\treturn s1, nil\n\t}\n\n\t// Return both if shards are equal.\n\titr.a0, itr.a1 = itr.a0[1:], itr.a1[1:]\n\treturn s0, s1\n}\n"
        },
        {
          "name": "row_test.go",
          "type": "blob",
          "size": 4.689453125,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa_test\n\nimport (\n\t\"fmt\"\n\t\"reflect\"\n\t\"testing\"\n\n\t\"github.com/featurebasedb/featurebase/v3\"\n)\n\n// Ensure a row can be merged\nfunc TestRow_Merge(t *testing.T) {\n\ttests := []struct {\n\t\tr1  *pilosa.Row\n\t\tr2  *pilosa.Row\n\t\texp uint64\n\t}{\n\t\t{\n\t\t\tr1:  pilosa.NewRow(1, 2, 3, ShardWidth+1, 2*ShardWidth),\n\t\t\tr2:  pilosa.NewRow(3, 4, 5),\n\t\t\texp: 7,\n\t\t},\n\t\t{\n\t\t\tr1:  pilosa.NewRow(),\n\t\t\tr2:  pilosa.NewRow(2, 66000, 70000, 70001, 70002, 70003, 70004),\n\t\t\texp: 7,\n\t\t},\n\t}\n\n\tfor i, test := range tests {\n\t\tt.Run(fmt.Sprintf(\"#%d:\", i), func(t *testing.T) {\n\t\t\ttest.r1.Merge(test.r2)\n\t\t\tif cnt := test.r1.Count(); cnt != test.exp {\n\t\t\t\tt.Fatalf(\"merged count %d is not %d\", cnt, test.exp)\n\t\t\t}\n\t\t\tif length := len(test.r1.Columns()); uint64(length) != test.exp {\n\t\t\t\tt.Fatalf(\"merged length %d is not %d\", length, test.exp)\n\t\t\t}\n\t\t})\n\t}\n}\n\n// Ensure a row can Xor'ed\nfunc TestRow_Xor(t *testing.T) {\n\tr1 := pilosa.NewRow(0, 1, ShardWidth)\n\tr2 := pilosa.NewRow(0, 2*ShardWidth)\n\texp := []uint64{1, ShardWidth, 2 * ShardWidth}\n\n\tres := r1.Xor(r2)\n\tif res.Count() != 3 {\n\t\tt.Fatalf(\"Test 1 Count after xor %d != 3\\n\", res.Count())\n\t}\n\n\tif !reflect.DeepEqual(res.Columns(), exp) {\n\t\tt.Fatalf(\"Test 2 Results %v != expected %v\\n\", res.Columns(), exp)\n\t}\n\tres = r2.Xor(r1)\n\tif res.Count() != 3 {\n\t\tt.Fatalf(\"Test 3 Count after xor %d != 3\\n\", res.Count())\n\t}\n\tif !reflect.DeepEqual(res.Columns(), exp) {\n\t\tt.Fatalf(\"Test 4 Results %v != expected %v\\n\", res.Columns(), exp)\n\t}\n\n}\nfunc TestRow_Union_Segment(t *testing.T) {\n\tr1 := pilosa.NewRow(0, 1, ShardWidth)\n\tr2 := pilosa.NewRow(0, 2*ShardWidth)\n\texp := []uint64{0, 1, ShardWidth, 2 * ShardWidth}\n\tres := r1.Union(r2)\n\n\tif res.Count() != 4 {\n\t\tt.Fatalf(\"Test 1 Count after Union %d != 5\\n\", res.Count())\n\t}\n\tif !reflect.DeepEqual(res.Columns(), exp) {\n\t\tt.Fatalf(\"Test 2 Union Results %v != expected %v\\n\", res.Columns(), exp)\n\t}\n\tres = r2.Union(r1)\n\tif res.Count() != 4 {\n\t\tt.Fatalf(\"Test 3 Count after xor %d != 5\\n\", res.Count())\n\t}\n\tif !reflect.DeepEqual(res.Columns(), exp) {\n\t\tt.Fatalf(\"Test 2 Union Results %v != expected %v\\n\", res.Columns(), exp)\n\t}\n}\n\nfunc TestRow_Difference_Segment(t *testing.T) {\n\tr1 := pilosa.NewRow(0, 1, ShardWidth)\n\tr2 := pilosa.NewRow(0, 2*ShardWidth)\n\texp := []uint64{1, ShardWidth}\n\tres := r1.Difference(r2)\n\n\tif res.Count() != 2 {\n\t\tt.Fatalf(\"Test 1 Count after Difference %d != 5\\n\", res.Count())\n\t}\n\tif !reflect.DeepEqual(res.Columns(), exp) {\n\t\tt.Fatalf(\"Test 2 Difference Results %v != expected %v\\n\", res.Columns(), exp)\n\t}\n}\n\nfunc TestRow_IsEmpty(t *testing.T) {\n\tr1 := pilosa.NewRow(1, ShardWidth)\n\tr2 := pilosa.NewRow(0, 2*ShardWidth)\n\tres := r2.Intersect(r1)\n\n\tif r1.IsEmpty() {\n\t\tt.Fatal(\"r1 Should Not Be Empty\\n\")\n\t}\n\tif !res.IsEmpty() {\n\t\tt.Fatal(\"Result Should Be Empty\\n\")\n\t}\n}\n\nfunc TestRow_Includes(t *testing.T) {\n\trow := pilosa.NewRow(0, 2*ShardWidth)\n\n\tif !row.Includes(0) {\n\t\tt.Fatal(\"row should include 0\")\n\t}\n\tif row.Includes(1) {\n\t\tt.Fatal(\"row should not include 1\")\n\t}\n\tif !row.Includes(2 * ShardWidth) {\n\t\tt.Fatalf(\"row should include %d\", 2*ShardWidth)\n\t}\n}\n\nfunc TestRow_DifferenceInPlace(t *testing.T) {\n\trow0 := pilosa.NewRow(0)\n\trow1 := pilosa.NewRow()\n\trow2 := pilosa.NewRow(0)\n\tres := row0.Difference(row1, row2)\n\n\tif !row0.Includes(0) {\n\t\tt.Fatal(\"row should include 0\")\n\t}\n\tif res.Count() != 0 {\n\t\tt.Fatal(\"results should be empty\")\n\t}\n}\nfunc eq(a, b []uint64) bool {\n\tif len(a) != len(b) {\n\t\treturn false\n\t}\n\tfor i, v := range a {\n\t\tif v != b[i] {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\nfunc TestRow_DifferenceInPlace2(t *testing.T) {\n\tlucky := []uint64{1, 3, 7, 9, 13, 15, 21, 25, 31, 33, 37, 43, 49, 51, 63, 67, 69, 73, 75, 79, 87, 93, 99, 105, 111, 115, 127, 129, 133, 135, 141, 151, 159, 163, 169, 171, 189, 193, 195, 201, 205, 211, 219, 223, 231, 235, 237, 241, 259, 261, 267, 273, 283, 285, 289, 297}\n\tsrc := pilosa.NewRow(lucky...)\n\trow1 := pilosa.NewRow()\n\tm := uint64(9)\n\tfor i := m; i <= lucky[len(lucky)-1]; i += m {\n\t\trow1.SetBit(i)\n\t}\n\tm = uint64(3)\n\trow2 := pilosa.NewRow()\n\tfor i := m; i <= lucky[len(lucky)-1]; i += m {\n\t\trow2.SetBit(i)\n\t}\n\trow3 := pilosa.NewRow()\n\tm = uint64(5)\n\tfor i := m; i <= lucky[len(lucky)-1]; i += m {\n\t\trow3.SetBit(i)\n\t}\n\trow4 := pilosa.NewRow()\n\tm = uint64(7)\n\tfor i := m; i <= lucky[len(lucky)-1]; i += m {\n\t\trow4.SetBit(i)\n\t}\n\tres5 := src.Difference(row1, row2, row3, row4)\n\tres6 := src.Difference(row4, row3, row2, row1)\n\tres7 := src.Difference(row4).Difference(row3).Difference(row2).Difference(row1)\n\tif !eq(res5.Columns(), res6.Columns()) {\n\t\tt.Fatalf(\"results do not match: %v, %v\", res5.Columns(), res6.Columns())\n\t}\n\tif !eq(res6.Columns(), res7.Columns()) {\n\t\tt.Fatalf(\"results do not match: %v, %v\", res6.Columns(), res7.Columns())\n\t}\n}\n"
        },
        {
          "name": "runners",
          "type": "tree",
          "content": null
        },
        {
          "name": "schema.go",
          "type": "blob",
          "size": 14.0458984375,
          "content": "package pilosa\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"sort\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/dax\"\n\t\"github.com/featurebasedb/featurebase/v3/errors\"\n\t\"github.com/featurebasedb/featurebase/v3/pql\"\n)\n\n// Ensure type implements interface.\nvar _ SchemaAPI = (*onPremSchema)(nil)\n\ntype onPremSchema struct {\n\tapi *API\n}\n\nfunc NewOnPremSchema(api *API) *onPremSchema {\n\treturn &onPremSchema{\n\t\tapi: api,\n\t}\n}\n\nfunc (s *onPremSchema) CreateDatabase(context.Context, *dax.Database) error {\n\treturn errors.Errorf(\"unimplemented: onPremSchema.CreateDatabase()\")\n}\nfunc (s *onPremSchema) DropDatabase(context.Context, dax.DatabaseID) error {\n\treturn errors.Errorf(\"unimplemented: onPremSchema.DropDatabase()\")\n}\n\nfunc (s *onPremSchema) DatabaseByName(ctx context.Context, dbname dax.DatabaseName) (*dax.Database, error) {\n\treturn nil, errors.Errorf(\"unimplemented: onPremSchema.DatabaseByName()\")\n}\nfunc (s *onPremSchema) DatabaseByID(ctx context.Context, dbid dax.DatabaseID) (*dax.Database, error) {\n\treturn nil, errors.Errorf(\"unimplemented: onPremSchema.DatabaseByID()\")\n}\nfunc (s *onPremSchema) SetDatabaseOption(ctx context.Context, dbid dax.DatabaseID, option string, value string) error {\n\treturn nil\n}\nfunc (s *onPremSchema) Databases(context.Context, ...dax.DatabaseID) ([]*dax.Database, error) {\n\treturn []*dax.Database{}, nil\n}\n\nfunc (s *onPremSchema) TableByName(ctx context.Context, tname dax.TableName) (*dax.Table, error) {\n\tidx, err := s.api.IndexInfo(context.Background(), string(tname))\n\tif err != nil {\n\t\tif err == ErrIndexNotFound {\n\t\t\treturn nil, dax.NewErrTableNameDoesNotExist(tname)\n\t\t}\n\t\treturn nil, errors.Wrapf(err, \"getting index info for table name: %s\", tname)\n\t}\n\n\treturn IndexInfoToTable(idx), nil\n}\n\nfunc (s *onPremSchema) TableByID(ctx context.Context, tid dax.TableID) (*dax.Table, error) {\n\tidx, err := s.api.IndexInfo(context.Background(), string(tid))\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"getting index info for table id: %s\", tid)\n\t}\n\n\treturn IndexInfoToTable(idx), nil\n}\n\nfunc (s *onPremSchema) Tables(ctx context.Context) ([]*dax.Table, error) {\n\tidxs, err := s.api.Schema(ctx, false)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"getting schema\")\n\t}\n\n\treturn IndexInfosToTables(idxs), nil\n}\n\nfunc (s *onPremSchema) CreateTable(ctx context.Context, tbl *dax.Table) error {\n\t// We make a slice of fields with the _id field removed. Also, while we're\n\t// at it, we can use the type of the _id field to determine if the index\n\t// should be keyed.\n\tvar keyed bool\n\tflds := make([]*dax.Field, 0)\n\tfor _, fld := range tbl.Fields {\n\t\tif fld.Name == \"_id\" {\n\t\t\tif fld.Type == dax.BaseTypeString {\n\t\t\t\tkeyed = true\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tflds = append(flds, fld)\n\t}\n\n\tiopts := IndexOptions{\n\t\tKeys:           keyed,\n\t\tTrackExistence: true,\n\t\tPartitionN:     tbl.PartitionN,\n\t\tDescription:    tbl.Description,\n\t}\n\n\t// Add the index.\n\tif _, err := s.api.CreateIndex(ctx, string(tbl.Name), iopts); err != nil {\n\t\treturn err\n\t}\n\n\t// Now add fields.\n\tfor _, fld := range flds {\n\t\tif err := s.CreateField(ctx, tbl.Name, fld); err != nil {\n\t\t\treturn errors.Wrapf(err, \"creating field: %s\", fld.Name)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (s *onPremSchema) CreateField(ctx context.Context, tname dax.TableName, fld *dax.Field) error {\n\topts, err := FieldOptionsFromField(fld)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"creating field options from field: %s\", fld.Name)\n\t}\n\n\t_, err = s.api.CreateField(ctx, string(tname), string(fld.Name), opts...)\n\treturn err\n}\n\nfunc (s *onPremSchema) DeleteTable(ctx context.Context, tname dax.TableName) error {\n\treturn s.api.DeleteIndex(ctx, string(tname))\n}\n\nfunc (s *onPremSchema) DeleteField(ctx context.Context, tname dax.TableName, fname dax.FieldName) error {\n\treturn s.api.DeleteField(ctx, string(tname), string(fname))\n}\n\n//////////////////////////////////////////////////////////////////////////////\n// The following are helper functions which convert between\n// featurebase.IndexInfo and dax.Table, and between featurebase.FieldInfo and\n// dax.Field.\n//////////////////////////////////////////////////////////////////////////////\n\n//\n// Functions to convert from featurebase to dax.\n//\n\n// IndexInfosToTables converts a slice of featurebase.IndexInfo to a slice of\n// dax.Table.\nfunc IndexInfosToTables(iis []*IndexInfo) []*dax.Table {\n\ttbls := make([]*dax.Table, 0, len(iis))\n\tfor _, ii := range iis {\n\t\ttbls = append(tbls, IndexInfoToTable(ii))\n\t}\n\treturn tbls\n}\n\n// IndexInfoToTable converts a featurebase.IndexInfo to a dax.Table.\nfunc IndexInfoToTable(ii *IndexInfo) *dax.Table {\n\ttbl := &dax.Table{\n\t\t// TODO(tlt): be careful here. This ID=Name logic only applies to \"onPrem\".\n\t\tID:         dax.TableID(ii.Name),\n\t\tName:       dax.TableName(ii.Name),\n\t\tFields:     make([]*dax.Field, 0, len(ii.Fields)+1), // +1 to account for the _id field\n\t\tPartitionN: dax.DefaultPartitionN,\n\n\t\tDescription: ii.Options.Description,\n\t\tOwner:       ii.Owner,\n\t\tUpdatedBy:   ii.LastUpdateUser,\n\t}\n\n\t// Sort ii.Fields by CreatedAt before adding them to sortedFields.\n\tsort.Slice(ii.Fields, func(i, j int) bool {\n\t\treturn ii.Fields[i].CreatedAt < ii.Fields[j].CreatedAt\n\t})\n\n\t// Add the _id Field.\n\tvar idType dax.BaseType = dax.BaseTypeID\n\tif ii.Options.Keys {\n\t\tidType = dax.BaseTypeString\n\t}\n\ttbl.Fields = append(tbl.Fields, &dax.Field{\n\t\tName: \"_id\",\n\t\tType: idType,\n\t})\n\n\t// Populate the rest of the fields.\n\tfor _, fld := range ii.Fields {\n\t\ttbl.Fields = append(tbl.Fields, FieldInfoToField(fld))\n\t}\n\n\treturn tbl\n}\n\n// FieldInfoToField converts a featurebase.FieldInfo to a dax.Field.\nfunc FieldInfoToField(fi *FieldInfo) *dax.Field {\n\t// Initialize field options; to be overridden based on field type specific\n\t// options.\n\tvar fieldType dax.BaseType\n\tvar min pql.Decimal\n\tvar max pql.Decimal\n\tvar scale int64\n\tvar cacheType string\n\tvar cacheSize uint32\n\tvar timeUnit string\n\tvar epoch time.Time\n\tvar foreignIndex string\n\tvar timeQuantum dax.TimeQuantum\n\n\tfo := &fi.Options\n\n\tswitch fo.Type {\n\tcase FieldTypeMutex:\n\t\tif fo.Keys {\n\t\t\tfieldType = dax.BaseTypeString\n\t\t} else {\n\t\t\tfieldType = dax.BaseTypeID\n\t\t}\n\t\tcacheType = fo.CacheType\n\t\tcacheSize = fo.CacheSize\n\tcase FieldTypeSet:\n\t\tif fo.Keys {\n\t\t\tfieldType = dax.BaseTypeStringSet\n\t\t} else {\n\t\t\tfieldType = dax.BaseTypeIDSet\n\t\t}\n\t\tcacheType = fo.CacheType\n\t\tcacheSize = fo.CacheSize\n\tcase FieldTypeInt:\n\t\tmin = fo.Min\n\t\tmax = fo.Max\n\t\tfieldType = dax.BaseTypeInt\n\t\tforeignIndex = fo.ForeignIndex\n\tcase FieldTypeDecimal:\n\t\tmin = fo.Min\n\t\tmax = fo.Max\n\t\tscale = fo.Scale\n\t\tfieldType = dax.BaseTypeDecimal\n\tcase FieldTypeTimestamp:\n\t\tepoch = featurebaseFieldOptionsToEpoch(fo)\n\t\ttimeUnit = fo.TimeUnit\n\t\tfieldType = dax.BaseTypeTimestamp\n\tcase FieldTypeBool:\n\t\tfieldType = dax.BaseTypeBool\n\tcase FieldTypeTime:\n\t\tif fo.Keys {\n\t\t\tfieldType = dax.BaseTypeStringSetQ\n\t\t} else {\n\t\t\tfieldType = dax.BaseTypeIDSetQ\n\t\t}\n\t\ttimeQuantum = dax.TimeQuantum(fo.TimeQuantum)\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"unhandled featurebase field type: %s\", fo.Type))\n\t}\n\n\treturn &dax.Field{\n\t\tName: dax.FieldName(fi.Name),\n\t\tType: fieldType,\n\t\tOptions: dax.FieldOptions{\n\t\t\tMin:            min,\n\t\t\tMax:            max,\n\t\t\tScale:          scale,\n\t\t\tNoStandardView: fo.NoStandardView,\n\t\t\tCacheType:      cacheType,\n\t\t\tCacheSize:      cacheSize,\n\t\t\tTimeUnit:       timeUnit,\n\t\t\tEpoch:          epoch,\n\t\t\tTimeQuantum:    timeQuantum,\n\t\t\tTTL:            fo.TTL,\n\t\t\tForeignIndex:   foreignIndex,\n\t\t\tTrackExistence: fo.TrackExistence,\n\t\t},\n\t}\n}\n\n// featurebaseFieldOptionsToEpoch produces an Epoch (time.Time) value based on\n// the given featurebase FieldOptions.\nfunc featurebaseFieldOptionsToEpoch(fo *FieldOptions) time.Time {\n\tepochNano := fo.Base * TimeUnitNanos(fo.TimeUnit)\n\treturn time.Unix(0, epochNano)\n}\n\n// FieldInfosToFields converts a []*featurebase.FieldInfo to a []*dax.Field.\nfunc FieldInfosToFields(fis []*FieldInfo) []*dax.Field {\n\tfs := make([]*dax.Field, 0, len(fis))\n\tfor i := range fis {\n\t\tfs = append(fs, FieldInfoToField(fis[i]))\n\t}\n\treturn fs\n}\n\n//\n// Functions to convert from dax to featurebase.\n//\n\n// TablesToIndexInfos converts a slice of dax.Table to a slice of\n// featurease.IndexInfo.\nfunc TablesToIndexInfos(tbls []*dax.Table) []*IndexInfo {\n\tiis := make([]*IndexInfo, 0, len(tbls))\n\tfor _, tbl := range tbls {\n\t\tiis = append(iis, TableToIndexInfo(tbl))\n\t}\n\treturn iis\n}\n\n// TableToIndexInfo converts a dax.Table to a featurease.IndexInfo.\nfunc TableToIndexInfo(tbl *dax.Table) *IndexInfo {\n\tii := &IndexInfo{\n\t\tName:           string(tbl.Name),\n\t\tOwner:          tbl.Owner,\n\t\tLastUpdateUser: tbl.UpdatedBy,\n\t\tOptions: IndexOptions{\n\t\t\tKeys:           tbl.StringKeys(),\n\t\t\tTrackExistence: true,\n\t\t\tDescription:    tbl.Description,\n\t\t},\n\t\tShardWidth: ShardWidth,\n\t}\n\n\t// fields\n\tfields := make([]*FieldInfo, 0, len(tbl.Fields)-1)\n\tfor i := range tbl.Fields {\n\t\tif tbl.Fields[i].Name == \"_id\" {\n\t\t\tcontinue\n\t\t}\n\t\tfields = append(fields, FieldToFieldInfo(tbl.Fields[i]))\n\t}\n\tii.Fields = fields\n\n\treturn ii\n}\n\n// FieldToFieldInfo converts a dax.Field to a featurebase.FieldInfo. Note: it\n// does not return errors; there is one scenario where a timestamp epoch could\n// be out of range. In that case, this function will only log the error, and the\n// proceed with timestamp option values which are likely incorrect. We are going\n// to leave this as is for now because, since this is used for internal\n// conversions of types which already exist and have been validated, we assume\n// the option values are valid.\n// TODO(tlt): add error handling to this function; worst case: panic.\nfunc FieldToFieldInfo(fld *dax.Field) *FieldInfo {\n\tvar timeUnit string\n\tvar base int64\n\tmin := fld.Options.Min\n\tmax := fld.Options.Max\n\n\tswitch fld.Type {\n\tcase dax.BaseTypeTimestamp:\n\t\ttimestampOptions, err := fieldOptionsForTimestamp(fld.Options)\n\t\tif err != nil {\n\t\t\tlog.Printf(\"ERROR: converting timestamp options: %v\", err)\n\t\t}\n\t\ttimeUnit = timestampOptions.TimeUnit\n\t\tbase = timestampOptions.Base\n\t\tmin = timestampOptions.Min\n\t\tmax = timestampOptions.Max\n\t}\n\n\treturn &FieldInfo{\n\t\tName: string(fld.Name),\n\t\tOptions: FieldOptions{\n\t\t\tType:           fieldToFieldType(fld),\n\t\t\tBase:           base,\n\t\t\tMin:            min,\n\t\t\tMax:            max,\n\t\t\tScale:          fld.Options.Scale,\n\t\t\tKeys:           fld.StringKeys(),\n\t\t\tNoStandardView: fld.Options.NoStandardView,\n\t\t\tCacheType:      fld.Options.CacheType,\n\t\t\tCacheSize:      fld.Options.CacheSize,\n\t\t\tTimeUnit:       timeUnit,\n\t\t\tTimeQuantum:    TimeQuantum(fld.Options.TimeQuantum),\n\t\t\tTTL:            fld.Options.TTL,\n\t\t\tForeignIndex:   fld.Options.ForeignIndex,\n\t\t\tTrackExistence: fld.Options.TrackExistence,\n\t\t},\n\t\tViews: nil, // TODO(tlt): do we need views populated?\n\t}\n}\n\n// fieldOptionsForTimestamp produces a featurebase.FieldOptions value with the\n// timestamp-related options populated.\nfunc fieldOptionsForTimestamp(fo dax.FieldOptions) (*FieldOptions, error) {\n\tout := &FieldOptions{}\n\n\t// Check if the epoch will overflow when converted to nano.\n\tif err := CheckEpochOutOfRange(fo.Epoch, MinTimestampNano, MaxTimestampNano); err != nil {\n\t\treturn out, errors.Wrap(err, \"checking overflow\")\n\t}\n\n\tout.TimeUnit = fo.TimeUnit\n\tout.Base = fo.Epoch.UnixNano() / TimeUnitNanos(fo.TimeUnit)\n\tout.Min = pql.NewDecimal(MinTimestamp.UnixNano()/TimeUnitNanos(fo.TimeUnit), 0)\n\tout.Max = pql.NewDecimal(MaxTimestamp.UnixNano()/TimeUnitNanos(fo.TimeUnit), 0)\n\n\treturn out, nil\n}\n\n// fieldToFieldType returns the featurebase.FieldType for the given dax.Field.\nfunc fieldToFieldType(f *dax.Field) string {\n\tswitch f.Type {\n\tcase dax.BaseTypeID, dax.BaseTypeString:\n\t\tif f.Name == dax.PrimaryKeyFieldName {\n\t\t\treturn string(f.Type)\n\t\t}\n\t\treturn \"mutex\"\n\n\tcase dax.BaseTypeIDSet, dax.BaseTypeStringSet:\n\t\treturn \"set\"\n\n\tcase dax.BaseTypeIDSetQ, dax.BaseTypeStringSetQ:\n\t\treturn \"time\"\n\n\tdefault:\n\t\treturn string(f.Type)\n\t}\n}\n\n// FieldFromFieldOptions creates a dax.Field given a set of existing\n// field options. It should possibly be unconditionally setting\n// TrackExistence, because it's called in two places in SQL3 both\n// of which are creating new tables, but for now I'm trying to keep\n// its behavior transparent, and handle the enabling of TrackExistence\n// in the code that knows it is creating a field, thus, in sql's\n// create/alter table, or in api.CreateField.\nfunc FieldFromFieldOptions(fname dax.FieldName, opts ...FieldOption) (*dax.Field, error) {\n\tfo, err := newFieldOptions(opts...)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"creating new field options\")\n\t}\n\n\tfi := &FieldInfo{\n\t\tName:    string(fname),\n\t\tOptions: *fo,\n\t}\n\n\treturn FieldInfoToField(fi), nil\n}\n\n// FieldOptionsFromField returns a slice of featurebase.FieldOption based on the\n// given dax.Field.\nfunc FieldOptionsFromField(fld *dax.Field) ([]FieldOption, error) {\n\t// Set the cache type and size (or use default) for those fields which\n\t// require them.\n\tcacheType := DefaultCacheType\n\tcacheSize := uint32(DefaultCacheSize)\n\tif fld.Options.CacheType != \"\" {\n\t\tcacheType = fld.Options.CacheType\n\t\tcacheSize = fld.Options.CacheSize\n\t}\n\n\topts := []FieldOption{}\n\n\tswitch fld.Type {\n\tcase dax.BaseTypeBool:\n\t\topts = append(opts,\n\t\t\tOptFieldTypeBool(),\n\t\t)\n\tcase dax.BaseTypeDecimal:\n\t\topts = append(opts,\n\t\t\tOptFieldTypeDecimal(fld.Options.Scale, fld.Options.Min, fld.Options.Max),\n\t\t)\n\tcase dax.BaseTypeID:\n\t\topts = append(opts,\n\t\t\tOptFieldTypeMutex(cacheType, cacheSize),\n\t\t)\n\tcase dax.BaseTypeIDSet:\n\t\topts = append(opts,\n\t\t\tOptFieldTypeSet(cacheType, cacheSize),\n\t\t)\n\tcase dax.BaseTypeIDSetQ:\n\t\topts = append(opts,\n\t\t\tOptFieldTypeTime(TimeQuantum(fld.Options.TimeQuantum), fld.Options.TTL.String()),\n\t\t)\n\tcase dax.BaseTypeInt:\n\t\topts = append(opts,\n\t\t\tOptFieldTypeInt(fld.Options.Min.ToInt64(0), fld.Options.Max.ToInt64(0)),\n\t\t)\n\tcase dax.BaseTypeString:\n\t\topts = append(opts,\n\t\t\tOptFieldTypeMutex(cacheType, cacheSize),\n\t\t\tOptFieldKeys(),\n\t\t)\n\tcase dax.BaseTypeStringSet:\n\t\topts = append(opts,\n\t\t\tOptFieldTypeSet(cacheType, cacheSize),\n\t\t\tOptFieldKeys(),\n\t\t)\n\tcase dax.BaseTypeStringSetQ:\n\t\topts = append(opts,\n\t\t\tOptFieldTypeTime(TimeQuantum(fld.Options.TimeQuantum), fld.Options.TTL.String()),\n\t\t\tOptFieldKeys(),\n\t\t)\n\tcase dax.BaseTypeTimestamp:\n\t\topts = append(opts,\n\t\t\tOptFieldTypeTimestamp(fld.Options.Epoch, fld.Options.TimeUnit),\n\t\t)\n\tdefault:\n\t\treturn nil, errors.Errorf(\"unsupport field type: %s\", fld.Type)\n\t}\n\tif fld.Options.TrackExistence {\n\t\topts = append(opts, OptFieldTrackExistence())\n\t}\n\n\treturn opts, nil\n}\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "serializer.go",
          "type": "blob",
          "size": 1.544921875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"bytes\"\n\t\"encoding/gob\"\n\t\"fmt\"\n\n\t\"github.com/pkg/errors\"\n)\n\n// GobSerializer represents a Serializer that uses gob encoding. This is only\n// used in tests; there's really no reason to use this instead of the proto\n// serializer except that, as it's currently implemented, the proto serializer\n// can't be used in internal tests (i.e test in the pilosa package) because the\n// proto package imports the pilosa package, so it would result in circular\n// imports. We really need all the pilosa types to be in a sub-package of\n// pilosa, so that both proto and pilosa can import them without resulting in\n// circular imports.\nvar GobSerializer Serializer = &gobSerializer{}\n\ntype gobSerializer struct{}\n\n// Marshal is a gob-encoded implementation of the Serializer Marshal method.\nfunc (s *gobSerializer) Marshal(msg Message) ([]byte, error) {\n\tvar buf bytes.Buffer\n\tenc := gob.NewEncoder(&buf)\n\tif err := enc.Encode(msg); err != nil {\n\t\treturn nil, errors.Wrap(err, \"gob encoding message\")\n\t}\n\treturn buf.Bytes(), nil\n}\n\n// Unmarshal is a gob-encoded implementation of the Serializer Unmarshal method.\nfunc (s *gobSerializer) Unmarshal(b []byte, m Message) error {\n\tswitch mt := m.(type) {\n\tcase *CreateIndexMessage, *CreateFieldMessage:\n\t\tdec := gob.NewDecoder(bytes.NewReader(b))\n\t\terr := dec.Decode(mt)\n\t\tif err != nil {\n\t\t\treturn errors.Wrapf(err, \"decoding %T\", mt)\n\t\t}\n\t\treturn nil\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"unhandled Message of type %T: %#v\", mt, m))\n\t}\n}\n"
        },
        {
          "name": "server.go",
          "type": "blob",
          "size": 41.0166015625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"log\"\n\t\"os\"\n\t\"os/exec\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\tuuid \"github.com/satori/go.uuid\"\n\n\tdaxstorage \"github.com/featurebasedb/featurebase/v3/dax/storage\"\n\t\"github.com/featurebasedb/featurebase/v3/disco\"\n\t\"github.com/featurebasedb/featurebase/v3/logger\"\n\tpnet \"github.com/featurebasedb/featurebase/v3/net\"\n\trbfcfg \"github.com/featurebasedb/featurebase/v3/rbf/cfg\"\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\t\"github.com/featurebasedb/featurebase/v3/sql3\"\n\t\"github.com/featurebasedb/featurebase/v3/sql3/parser\"\n\tplanner_types \"github.com/featurebasedb/featurebase/v3/sql3/planner/types\"\n\t\"github.com/featurebasedb/featurebase/v3/storage\"\n\t\"github.com/pkg/errors\"\n\t\"golang.org/x/sync/errgroup\"\n\n\t_ \"github.com/lib/pq\"\n)\n\n// Default server settings.\nconst (\n\tdefaultDiagnosticServer = \"https://diagnostics.pilosa.com/v0/diagnostics\"\n)\n\n// Ensure Server implements interfaces.\nvar _ broadcaster = &Server{}\n\n// Server represents a holder wrapped by a running HTTP server.\ntype Server struct { // nolint: maligned\n\t// Close management.\n\twg      sync.WaitGroup\n\tmuWG    sync.Mutex\n\tclosing chan struct{}\n\n\t// Internal\n\tholder           *Holder\n\tcluster          *cluster\n\tdiagnostics      *diagnosticsCollector\n\texecutor         *executor\n\texecutorPoolSize int\n\tserializer       Serializer\n\n\tSystemLayer SystemLayerAPI\n\n\t// Distributed Consensus\n\tdisCo   disco.DisCo\n\tnoder   disco.Noder\n\tsharder disco.Sharder\n\n\t// External\n\tsystemInfo  SystemInfo\n\tgcNotifier  GCNotifier\n\tlogger      logger.Logger\n\tqueryLogger logger.Logger\n\n\tnodeID               string\n\turi                  pnet.URI\n\tgrpcURI              pnet.URI\n\tmetricInterval       time.Duration\n\tdiagnosticInterval   time.Duration\n\tviewsRemovalInterval time.Duration\n\tmaxWritesPerRequest  int\n\tconfirmDownSleep     time.Duration\n\tconfirmDownRetries   int\n\tsyncer               holderSyncer\n\tmaxQueryMemory       int64\n\n\ttranslationSyncer      TranslationSyncer\n\tresetTranslationSyncCh chan struct{}\n\t// HolderConfig stashes server options that are really Holder options.\n\tholderConfig *HolderConfig\n\n\tdefaultClient *InternalClient\n\tdataDir       string\n\tverChkAddress string\n\tuuidFile      string\n\n\t// Threshold for logging long-running queries\n\tlongQueryTime      time.Duration\n\tqueryHistoryLength int\n\n\texecutionPlannerFn ExecutionPlannerFn\n\n\tserverlessStorage *daxstorage.ResourceManager\n\n\tdataframeEnabled    bool\n\tdataframeUseParquet bool\n}\n\ntype ExecutionPlannerFn func(executor Executor, api *API, sql string) sql3.CompilePlanner\n\n// Holder returns the holder for server.\nfunc (s *Server) Holder() *Holder {\n\treturn s.holder\n}\n\n// addToWaitGroup adds to the server WaitGroup but makes sure the server isn't\n// closing, and that the WaitGroup is not already waiting before it adds\nfunc (s *Server) addToWaitGroup(delta int) bool {\n\tselect {\n\tcase <-s.closing:\n\t\treturn false\n\tdefault:\n\t\ts.muWG.Lock()\n\t\tdefer s.muWG.Unlock()\n\t\tselect {\n\t\tcase <-s.closing:\n\t\t\t// if we're closing after having gotten the lock, stop!!\n\t\t\treturn false\n\t\tdefault:\n\t\t\ts.wg.Add(delta)\n\t\t\treturn true\n\t\t}\n\t}\n}\n\n// ServerOption is a functional option type for pilosa.Server\ntype ServerOption func(s *Server) error\n\n// OptServerLogger is a functional option on Server\n// used to set the logger.\nfunc OptServerLogger(l logger.Logger) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.logger = l\n\t\ts.holderConfig.Logger = l\n\t\treturn nil\n\t}\n}\n\nfunc OptServerQueryLogger(l logger.Logger) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.queryLogger = l\n\t\treturn nil\n\t}\n}\n\n// OptServerReplicaN is a functional option on Server\n// used to set the number of replicas.\nfunc OptServerReplicaN(n int) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.cluster.ReplicaN = n\n\t\treturn nil\n\t}\n}\n\n// OptServerDataDir is a functional option on Server\n// used to set the data directory.\nfunc OptServerDataDir(dir string) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.dataDir = dir\n\t\treturn nil\n\t}\n}\n\n// OptServerVerChkAddress is a functional option on Server\n// used to set the address to check for the current version.\nfunc OptServerVerChkAddress(addr string) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.verChkAddress = addr\n\t\treturn nil\n\t}\n}\n\n// OptServerUUIDFile is a functional option on Server\n// used to set the file name for storing the checkin UUID.\nfunc OptServerUUIDFile(uf string) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.uuidFile = uf\n\t\treturn nil\n\t}\n}\n\n// OptServerViewsRemovalInterval is a functional option on Server\n// used to set the ttl removal interval.\nfunc OptServerViewsRemovalInterval(interval time.Duration) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.viewsRemovalInterval = interval\n\t\treturn nil\n\t}\n}\n\n// OptServerLongQueryTime is a functional option on Server\n// used to set long query duration.\nfunc OptServerLongQueryTime(dur time.Duration) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.longQueryTime = dur\n\t\treturn nil\n\t}\n}\n\n// OptServerMaxWritesPerRequest is a functional option on Server\n// used to set the maximum number of writes allowed per request.\nfunc OptServerMaxWritesPerRequest(n int) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.maxWritesPerRequest = n\n\t\treturn nil\n\t}\n}\n\n// OptServerMetricInterval is a functional option on Server\n// used to set the interval between metric samples.\nfunc OptServerMetricInterval(dur time.Duration) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.metricInterval = dur\n\t\treturn nil\n\t}\n}\n\n// OptServerSystemInfo is a functional option on Server\n// used to set the system information source.\nfunc OptServerSystemInfo(si SystemInfo) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.systemInfo = si\n\t\treturn nil\n\t}\n}\n\n// OptServerGCNotifier is a functional option on Server\n// used to set the garbage collection notification source.\nfunc OptServerGCNotifier(gcn GCNotifier) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.gcNotifier = gcn\n\t\treturn nil\n\t}\n}\n\n// OptServerInternalClient is a functional option on Server\n// used to set the implementation of InternalClient.\nfunc OptServerInternalClient(c *InternalClient) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.defaultClient = c\n\t\ts.cluster.InternalClient = c\n\t\treturn nil\n\t}\n}\n\nfunc OptServerExecutorPoolSize(size int) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.executorPoolSize = size\n\t\treturn nil\n\t}\n}\n\n// OptServerPrimaryTranslateStore has been deprecated.\nfunc OptServerPrimaryTranslateStore(store TranslateStore) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.logger.Infof(\"DEPRECATED: OptServerPrimaryTranslateStore\")\n\t\treturn nil\n\t}\n}\n\n// OptServerDiagnosticsInterval is a functional option on Server\n// used to specify the duration between diagnostic checks.\nfunc OptServerDiagnosticsInterval(dur time.Duration) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.diagnosticInterval = dur\n\t\treturn nil\n\t}\n}\n\n// OptServerNodeDownRetries is a functional option on Server\n// used to specify the retries and sleep duration for node down\n// checks.\nfunc OptServerNodeDownRetries(retries int, sleep time.Duration) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.confirmDownRetries = retries\n\t\ts.confirmDownSleep = sleep\n\t\treturn nil\n\t}\n}\n\n// OptServerURI is a functional option on Server\n// used to set the server URI.\nfunc OptServerURI(uri *pnet.URI) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.uri = *uri\n\t\treturn nil\n\t}\n}\n\n// OptServerGRPCURI is a functional option on Server\n// used to set the server gRPC URI.\nfunc OptServerGRPCURI(uri *pnet.URI) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.grpcURI = *uri\n\t\treturn nil\n\t}\n}\n\n// OptServerClusterName sets the human-readable cluster name.\nfunc OptServerClusterName(name string) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.cluster.Name = name\n\t\treturn nil\n\t}\n}\n\n// OptServerSerializer is a functional option on Server\n// used to set the serializer.\nfunc OptServerSerializer(ser Serializer) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.serializer = ser\n\t\treturn nil\n\t}\n}\n\n// OptServerNodeID is a functional option on Server\n// used to set the server node ID.\nfunc OptServerNodeID(nodeID string) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.nodeID = nodeID\n\t\treturn nil\n\t}\n}\n\n// OptServerClusterHasher is a functional option on Server\n// used to specify the consistent hash algorithm for data\n// location within the cluster.\nfunc OptServerClusterHasher(h disco.Hasher) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.cluster.Hasher = h\n\t\treturn nil\n\t}\n}\n\n// OptServerOpenTranslateStore is a functional option on Server\n// used to specify the translation data store type.\nfunc OptServerOpenTranslateStore(fn OpenTranslateStoreFunc) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.holderConfig.OpenTranslateStore = fn\n\t\treturn nil\n\t}\n}\n\n// OptServerOpenIDAllocator is a functional option on Server\n// used to specify the ID allocator data store type.\n// Except not really (because there's only one at this time).\nfunc OptServerOpenIDAllocator(fn OpenIDAllocatorFunc) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.holderConfig.OpenIDAllocator = fn\n\t\treturn nil\n\t}\n}\n\n// OptServerOpenTranslateReader is a functional option on Server\n// used to specify the remote translation data reader.\nfunc OptServerOpenTranslateReader(fn OpenTranslateReaderFunc) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.holderConfig.OpenTranslateReader = fn\n\t\treturn nil\n\t}\n}\n\n// OptServerStorageConfig is a functional option on Server used to specify the\n// transactional-storage backend to use, resulting in RoaringTx or RbfTx\n// being used for all Tx interface calls.\nfunc OptServerStorageConfig(cfg *storage.Config) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.holderConfig.StorageConfig = cfg\n\t\t// For historical reasons, RBF's config can ignore the storage config\n\t\t// in some cases.\n\t\ts.holderConfig.RBFConfig.FsyncEnabled = s.holderConfig.StorageConfig.FsyncEnabled\n\t\treturn nil\n\t}\n}\n\n// OptServerRBFConfig conveys the RBF flags to the Holder.\nfunc OptServerRBFConfig(cfg *rbfcfg.Config) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.holderConfig.RBFConfig = cfg\n\t\treturn nil\n\t}\n}\n\n// OptServerQueryHistoryLength is a functional option on Server\n// used to specify the length of the query history buffer that maintains\n// the information returned at /query-history.\nfunc OptServerQueryHistoryLength(length int) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.queryHistoryLength = length\n\t\treturn nil\n\t}\n}\n\n// OptServerMaxQueryMemory sets the memory used per Extract() and SELECT query.\nfunc OptServerMaxQueryMemory(v int64) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.maxQueryMemory = v\n\t\treturn nil\n\t}\n}\n\n// OptServerDisCo is a functional option on Server\n// used to set the Distributed Consensus implementation.\nfunc OptServerDisCo(disCo disco.DisCo,\n\tnoder disco.Noder,\n\tsharder disco.Sharder,\n\tschemator disco.Schemator,\n) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.disCo = disCo\n\t\ts.noder = noder\n\t\ts.sharder = sharder\n\t\ts.holderConfig.Schemator = schemator\n\t\treturn nil\n\t}\n}\n\n// OptServerLookupDB configures a connection to an external postgres database for ExternalLookup queries.\nfunc OptServerLookupDB(dsn string) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.holderConfig.LookupDBDSN = dsn\n\t\treturn nil\n\t}\n}\n\nfunc OptServerPartitionAssigner(p string) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.cluster.partitionAssigner = p\n\t\treturn nil\n\t}\n}\n\nfunc OptServerExecutionPlannerFn(fn ExecutionPlannerFn) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.executionPlannerFn = fn\n\t\treturn nil\n\t}\n}\n\nfunc OptServerServerlessStorage(mm *daxstorage.ResourceManager) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.serverlessStorage = mm\n\t\treturn nil\n\t}\n}\n\n// OptServerIsComputeNode specifies that this node is running as a DAX compute node.\nfunc OptServerIsComputeNode(is bool) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.cluster.isComputeNode = is\n\t\treturn nil\n\t}\n}\n\n// OptServerIsDataframeEnabled specifies if experimental dataframe support available\nfunc OptServerIsDataframeEnabled(is bool) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.dataframeEnabled = is\n\t\treturn nil\n\t}\n}\n\nfunc OptServerDataframeUseParquet(is bool) ServerOption {\n\treturn func(s *Server) error {\n\t\ts.dataframeUseParquet = is\n\t\treturn nil\n\t}\n}\n\n// NewServer returns a new instance of Server.\nfunc NewServer(opts ...ServerOption) (*Server, error) {\n\tcluster := newCluster()\n\n\ts := &Server{\n\t\tclosing:       make(chan struct{}),\n\t\tcluster:       cluster,\n\t\tdiagnostics:   newDiagnosticsCollector(defaultDiagnosticServer),\n\t\tsystemInfo:    newNopSystemInfo(),\n\t\tdefaultClient: &InternalClient{}, // TODO may need to make this a valid thing\n\n\t\tgcNotifier: NopGCNotifier,\n\n\t\tmetricInterval:       0,\n\t\tdiagnosticInterval:   0,\n\t\tviewsRemovalInterval: time.Hour,\n\n\t\tdisCo:      disco.NopDisCo,\n\t\tnoder:      disco.NewEmptyLocalNoder(),\n\t\tsharder:    disco.NopSharder,\n\t\tserializer: NopSerializer,\n\n\t\tconfirmDownRetries: defaultConfirmDownRetries,\n\t\tconfirmDownSleep:   defaultConfirmDownSleep,\n\n\t\tresetTranslationSyncCh: make(chan struct{}, 1),\n\n\t\tlogger: logger.NopLogger,\n\n\t\texecutionPlannerFn: func(e Executor, a *API, s string) sql3.CompilePlanner {\n\t\t\treturn sql3.NewNopCompilePlanner()\n\t\t},\n\t}\n\ts.cluster.InternalClient = s.defaultClient\n\n\ts.translationSyncer = newActiveTranslationSyncer(s.resetTranslationSyncCh)\n\ts.cluster.translationSyncer = s.translationSyncer\n\n\ts.diagnostics.server = s\n\ts.holderConfig = DefaultHolderConfig()\n\ts.holderConfig.TranslationSyncer = s.translationSyncer\n\ts.holderConfig.Logger = s.logger\n\n\tfor _, opt := range opts {\n\t\terr := opt(s)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"applying option\")\n\t\t}\n\t}\n\n\tmemTotal, err := s.systemInfo.MemTotal()\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"mem total\")\n\t}\n\n\t// Default memory to 20% of total.\n\tmaxQueryMemory := s.maxQueryMemory\n\tif maxQueryMemory == 0 {\n\t\tmaxQueryMemory = int64(float64(memTotal) * .20)\n\t}\n\n\t// set up executor after server opts have been processed\n\texecutorOpts := []executorOption{\n\t\toptExecutorInternalQueryClient(s.defaultClient),\n\t\toptExecutorMaxMemory(maxQueryMemory),\n\t}\n\tif s.executorPoolSize > 0 {\n\t\texecutorOpts = append(executorOpts, optExecutorWorkerPoolSize(s.executorPoolSize))\n\t}\n\ts.executor = newExecutor(executorOpts...)\n\ts.executor.dataframeEnabled = s.dataframeEnabled\n\ts.executor.datafameUseParquet = s.dataframeUseParquet\n\n\tpath, err := expandDirName(s.dataDir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ts.holder = NewHolder(path, s.holderConfig)\n\tcwd, err := os.Getwd()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ts.holder.Logger.Infof(\"cwd: %v\", cwd)\n\ts.holder.Logger.Infof(\"cmd line: %v\", strings.Join(os.Args, \" \"))\n\n\ts.cluster.Path = path\n\ts.cluster.logger = s.logger\n\ts.cluster.holder = s.holder\n\ts.cluster.disCo = s.disCo\n\ts.cluster.noder = s.noder\n\ts.cluster.sharder = s.sharder\n\ts.cluster.serverlessStorage = s.serverlessStorage\n\n\ts.executor.Holder = s.holder\n\ts.holder.executor = s.executor\n\ts.executor.Cluster = s.cluster\n\ts.executor.MaxWritesPerRequest = s.maxWritesPerRequest\n\ts.cluster.broadcaster = s\n\ts.cluster.maxWritesPerRequest = s.maxWritesPerRequest\n\ts.cluster.confirmDownRetries = s.confirmDownRetries\n\ts.cluster.confirmDownSleep = s.confirmDownSleep\n\ts.holder.broadcaster = s\n\ts.holder.sharder = s.sharder\n\ts.holder.serializer = s.serializer\n\n\t// Initial stats must be invoked after the executor obtains reference to the holder.\n\ts.executor.InitStats()\n\n\treturn s, nil\n}\n\nfunc (s *Server) InternalClient() *InternalClient {\n\treturn s.defaultClient\n}\n\nfunc (s *Server) GRPCURI() pnet.URI {\n\treturn s.grpcURI\n}\n\nfunc (s *Server) SetAPI(api *API) {\n\ts.defaultClient.SetInternalAPI(api)\n}\n\n// UpAndDown brings the server up minimally and shuts it down\n// again; basically, it exists for testing holder open and close.\nfunc (s *Server) UpAndDown() error {\n\t// Log startup\n\terr := s.holder.logStartup()\n\tif err != nil {\n\t\tlog.Println(errors.Wrap(err, \"logging startup\"))\n\t}\n\ts.logger.Infof(\"open server. PID %v\", os.Getpid())\n\tif err = s.Open(); err != nil {\n\t\treturn errors.Wrap(err, \"starting server\")\n\t}\n\terr = s.Close()\n\treturn errors.Wrap(err, \"shutting down server\")\n}\n\n// Open opens and initializes the server.\nfunc (s *Server) Open() error {\n\ts.logger.Infof(\"open server. PID %v\", os.Getpid())\n\n\t// Log startup\n\terr := s.holder.logStartup()\n\tif err != nil {\n\t\tlog.Println(errors.Wrap(err, \"logging startup\"))\n\t}\n\n\t// Do version check in. This is in a goroutine so that we don't block server\n\t// startup if the server endpoint is down/having issues.\n\tgo func() {\n\t\ts.logger.Printf(\"Beginning featurebase version check-in\")\n\t\tvc := newVersionChecker(s.cluster.Path, s.verChkAddress, s.uuidFile)\n\t\tresp, err := vc.checkIn()\n\t\tif err != nil {\n\t\t\ts.logger.Errorf(\"doing version checkin. Error was %s\", err)\n\t\t\treturn\n\t\t}\n\t\tif resp.Error != \"\" {\n\t\t\ts.logger.Printf(\"Version check-in failed, endpoint response was %s\", resp.Error)\n\t\t} else {\n\t\t\ts.logger.Printf(\"Version check-in complete. Latest version is %s\", resp.Version)\n\t\t}\n\t}()\n\n\t// Start DisCo.\n\tctx, cancel := context.WithTimeout(context.Background(), 120*time.Second)\n\tdefer cancel()\n\tinitState, err := s.disCo.Start(ctx)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"starting DisCo\")\n\t}\n\t// I'm pretty sure this can't happen, because the path that would have led to it\n\t// happening now generates an error already, but let's be careful.\n\tif initState == disco.InitialClusterStateExisting {\n\t\treturn errors.New(\"disco reports existing cluster, but this is not supported\")\n\t}\n\n\t// Set node ID.\n\ts.nodeID = s.disCo.ID()\n\n\tnodeState := disco.NodeStateUnknown\n\tif s.cluster.isComputeNode {\n\t\tnodeState = disco.NodeStateStarted\n\t}\n\n\tnode := &disco.Node{\n\t\tID:        s.nodeID,\n\t\tURI:       s.uri,\n\t\tGRPCURI:   s.grpcURI,\n\t\tState:     nodeState,\n\t\tIsPrimary: s.IsPrimary(),\n\t}\n\n\tif err := s.noder.SetMetadata(context.Background(), node); err != nil {\n\t\treturn errors.Wrap(err, \"setting metadata\")\n\t}\n\n\ts.cluster.Node = node\n\ts.executor.Node = node\n\n\t// Set up the holderSyncer.\n\ts.syncer.Holder = s.holder\n\ts.syncer.Node = node\n\ts.syncer.Cluster = s.cluster\n\ts.syncer.Closing = s.closing\n\n\t// Start background process listening for translation\n\t// sync resets.\n\tif ok := s.addToWaitGroup(1); !ok {\n\t\treturn fmt.Errorf(\"closing server while opening server is NOT allowed\")\n\t}\n\n\tgo func() { defer s.wg.Done(); s.monitorResetTranslationSync() }()\n\tgo func() { _ = s.translationSyncer.Reset() }()\n\n\t// Open holder.\n\tfunc() {\n\t\ts.holder.startMsgsMu.Lock()\n\t\tdefer s.holder.startMsgsMu.Unlock()\n\n\t\ts.holder.startMsgs = []Message{}\n\t}()\n\tif err := s.holder.Open(); err != nil {\n\t\treturn errors.Wrap(err, \"opening Holder\")\n\t}\n\t// bring up the background tasks for the holder.\n\ts.holder.Activate()\n\n\tif err := s.noder.SetState(context.Background(), disco.NodeStateStarted); err != nil {\n\t\treturn errors.Wrap(err, \"setting nodeState\")\n\t}\n\n\tif ok := s.addToWaitGroup(3); !ok {\n\t\treturn fmt.Errorf(\"closing server while opening server is NOT allowed\")\n\t}\n\tgo func() { defer s.wg.Done(); s.monitorRuntime() }()\n\tgo func() { defer s.wg.Done(); s.monitorDiagnostics() }()\n\tgo func() { defer s.wg.Done(); s.monitorViewsRemoval() }()\n\n\ttoSend := func() []Message {\n\t\ts.holder.startMsgsMu.Lock()\n\t\tdefer s.holder.startMsgsMu.Unlock()\n\n\t\ttoSend := s.holder.startMsgs\n\t\ts.holder.startMsgs = nil\n\t\treturn toSend\n\t}()\n\n\tif ok := s.addToWaitGroup(1); !ok {\n\t\treturn fmt.Errorf(\"closing server while opening server is NOT allowed\")\n\t}\n\tgo func() {\n\t\tdefer s.wg.Done()\n\n\t\tctx, cancel := context.WithCancel(context.Background())\n\t\tdefer cancel()\n\t\tif ok := s.addToWaitGroup(1); !ok {\n\t\t\t// the server is closing, stop!!\n\t\t\treturn\n\t\t}\n\t\tgo func() {\n\t\t\tdefer s.wg.Done()\n\t\t\tdefer cancel()\n\t\t\tselect {\n\t\t\tcase <-s.closing:\n\t\t\tcase <-ctx.Done():\n\t\t\t}\n\t\t}()\n\n\t\ttimer := time.NewTimer(0)\n\t\tdefer timer.Stop()\n\t\tif !timer.Stop() {\n\t\t\t<-timer.C\n\t\t}\n\t\t// wait for cluster to achieve Normal state\n\t\t//\n\t\t// This used to loop as long as the cluster was Starting, Down,\n\t\t// or Unknown. It would come up in a Degraded state, except\n\t\t// that during startup, as long as at least one node was Starting,\n\t\t// we'd stay in Starting rather than Degraded. We've dropped the\n\t\t// special case of Starting state, so now we just want to wait\n\t\t// for Normal.\n\t\tfor {\n\t\t\tstate, err := s.noder.ClusterState(ctx)\n\t\t\tif err != nil {\n\t\t\t\ts.logger.Printf(\"failed to check cluster state: %v\", err)\n\t\t\t}\n\t\t\tif state == disco.ClusterStateNormal {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\ttimer.Reset(time.Second)\n\t\t\tselect {\n\t\t\tcase <-s.closing:\n\t\t\t\treturn\n\t\t\tcase <-timer.C:\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tstart := time.Now()\n\t\tprevMsg := start\n\t\tnumMsgs := uint(len(toSend))\n\t\ts.logger.Printf(\"start initial cluster state sync\")\n\t\tfor i := range toSend {\n\t\t\tfor {\n\t\t\t\terr := s.holder.broadcaster.SendSync(toSend[i])\n\t\t\t\tif err != nil {\n\t\t\t\t\ts.logger.Printf(\"failed to broadcast startup cluster message (trying again in a bit): %v\", err)\n\t\t\t\t\ttimer.Reset(time.Second)\n\t\t\t\t\tselect {\n\t\t\t\t\tcase <-s.closing:\n\t\t\t\t\t\treturn\n\t\t\t\t\tcase <-timer.C:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tif now := time.Now(); now.Sub(prevMsg) > time.Second {\n\t\t\t\testimate, pctDone := GetLoopProgress(start, now, uint(i), numMsgs)\n\t\t\t\ts.logger.Printf(\"synced %d/%d messages (%.2f%% complete; %s remaining)\", i+1, numMsgs, pctDone, estimate)\n\t\t\t\tprevMsg = now\n\t\t\t}\n\t\t}\n\t\ts.logger.Printf(\"completed initial cluster state sync in %s\", time.Since(start).String())\n\t}()\n\n\treturn nil\n}\n\n// Close closes the server and waits for it to shutdown.\nfunc (s *Server) Close() error {\n\tselect {\n\tcase <-s.closing:\n\t\treturn nil\n\tdefault:\n\t\t// get the muWG lock so that noone adds to the WaitGroup while it Waits\n\t\ts.muWG.Lock()\n\t\tdefer s.muWG.Unlock()\n\t\t// Notify goroutines to stop.\n\t\tclose(s.closing)\n\t\ts.wg.Wait()\n\n\t\terrE := s.executor.Close()\n\n\t\tvar errh, errd error\n\t\tvar errhs error\n\t\tvar errc error\n\t\tvar errSS error\n\n\t\tif s.cluster != nil {\n\t\t\terrc = s.cluster.close()\n\t\t}\n\t\terrhs = s.syncer.stopTranslationSync()\n\t\tif s.disCo != nil {\n\t\t\terrd = s.disCo.Close()\n\t\t}\n\t\tif s.holder != nil {\n\t\t\terrh = s.holder.Close()\n\t\t}\n\t\tif s.serverlessStorage != nil {\n\t\t\terrSS = s.serverlessStorage.RemoveAll()\n\t\t}\n\n\t\t// prefer to return holder error over cluster\n\t\t// error. This order is somewhat arbitrary. It would be better if we had\n\t\t// some way to combine all the errors, but probably not important enough to\n\t\t// warrant the extra complexity.\n\t\tif errh != nil {\n\t\t\treturn errors.Wrap(errh, \"closing holder\")\n\t\t}\n\t\tif errhs != nil {\n\t\t\treturn errors.Wrap(errhs, \"terminating holder translation sync\")\n\t\t}\n\t\tif errc != nil {\n\t\t\treturn errors.Wrap(errc, \"closing cluster\")\n\t\t}\n\t\tif errd != nil {\n\t\t\treturn errors.Wrap(errd, \"closing disco\")\n\t\t}\n\t\tif errE != nil {\n\t\t\treturn errors.Wrap(errE, \"closing executor\")\n\t\t}\n\t\treturn errors.Wrap(errSS, \"unlocking all serverless storage\")\n\t}\n}\n\n// NodeID returns the server's node id.\nfunc (s *Server) NodeID() string { return s.nodeID }\n\n// monitorResetTranslationSync is a background process which\n// listens for events indicating the need to reset the translation\n// sync processes.\nfunc (s *Server) monitorResetTranslationSync() {\n\ts.logger.Infof(\"holder translation sync monitor initializing\")\n\tfor {\n\t\t// Wait for a reset or a close.\n\t\tselect {\n\t\tcase <-s.closing:\n\t\t\treturn\n\t\tcase <-s.resetTranslationSyncCh:\n\t\t\tif ok := s.addToWaitGroup(1); !ok {\n\t\t\t\t// the server is closing!!! stop!!\n\t\t\t\treturn\n\t\t\t}\n\t\t\ts.logger.Infof(\"holder translation sync beginning\")\n\t\t\tgo func() {\n\t\t\t\t// Obtaining this lock ensures that there is only\n\t\t\t\t// one instance of resetTranslationSync() running\n\t\t\t\t// at once.\n\t\t\t\ts.syncer.mu.Lock()\n\t\t\t\tdefer s.syncer.mu.Unlock()\n\t\t\t\tdefer s.wg.Done()\n\t\t\t\tif err := s.syncer.resetTranslationSync(); err != nil {\n\t\t\t\t\ts.logger.Errorf(\"holder translation sync error: err=%s\", err)\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\t}\n}\n\nfunc (s *Server) monitorViewsRemoval() {\n\tctx := context.Background()\n\t// Run ViewsRemoval on server start\n\ts.ViewsRemoval(ctx)\n\tticker := time.NewTicker(s.viewsRemovalInterval)\n\tfor {\n\t\tselect {\n\t\tcase <-s.closing:\n\t\t\treturn\n\t\tcase <-ticker.C:\n\t\t\ts.ViewsRemoval(ctx)\n\t\t}\n\t}\n}\n\n// Remove views based on these criterias:\n// 1. views that are older than specified TTL\n// 2. \"standard\" view of a field if its \"noStandardView\" option is set to true\nfunc (s *Server) ViewsRemoval(ctx context.Context) {\n\tfor _, index := range s.holder.Indexes() {\n\t\tfor _, field := range index.Fields() {\n\t\t\tif field.Options().Type == \"time\" {\n\t\t\t\tif field.Options().TTL > 0 {\n\t\t\t\t\tfor _, view := range field.views() {\n\t\t\t\t\t\t// view names follow the format of \"standard_(time_quantum)\"\n\t\t\t\t\t\t// to get view time, we split the view.name by \"_\"\n\t\t\t\t\t\t// then grab the second value (the time quantum)\n\t\t\t\t\t\tviewName := strings.Split(view.name, \"_\")\n\t\t\t\t\t\tif len(viewName) == 2 {\n\t\t\t\t\t\t\t// when getting the view time, we want to grab the end date\n\t\t\t\t\t\t\t// because start date will aways be older\n\t\t\t\t\t\t\tviewTime, err := timeOfView(view.name, true)\n\t\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\t\ts.logger.Printf(\"view: %s; err: %s\", viewName, err)\n\t\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\ttimeSince := time.Since(viewTime)\n\n\t\t\t\t\t\t\tif timeSince >= field.Options().TTL {\n\t\t\t\t\t\t\t\tfor _, shard := range field.AvailableShards(true).Slice() {\n\t\t\t\t\t\t\t\t\terr := s.holder.txf.DeleteFragmentFromStore(index.Name(), field.Name(), view.name, shard, nil)\n\t\t\t\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\t\t\t\ts.logger.Errorf(\"view: %s, shard: %d, ttl delete fragment: %s\", shard, viewName, err)\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t\terr := s.defaultClient.api.DeleteView(ctx, index.Name(), field.Name(), view.name)\n\t\t\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\t\t\ts.logger.Errorf(\"view: %s, ttl delete view: %s\", viewName, err)\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\ts.logger.Infof(\"ttl deleted - index: %s, field: %s, view: %s \", index.name, field.name, view.name)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif field.Options().NoStandardView {\n\t\t\t\t\tif field.view(viewStandard) != nil {\n\t\t\t\t\t\t// delete view \"standard\" if NoStandardView is true and view \"standard\" exists\n\t\t\t\t\t\tfor _, shard := range field.AvailableShards(true).Slice() {\n\t\t\t\t\t\t\terr := s.holder.txf.DeleteFragmentFromStore(index.Name(), field.Name(), viewStandard, shard, nil)\n\t\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\t\ts.logger.Errorf(\"delete view %s from shard %d: %s\", viewStandard, shard, err)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\terr := s.defaultClient.api.DeleteView(ctx, index.Name(), field.Name(), viewStandard)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\ts.logger.Errorf(\"view: %s, delete view: %s\", viewStandard, err)\n\t\t\t\t\t\t}\n\t\t\t\t\t\ts.logger.Infof(\"view %s deleted - index: %s, field: %s \", viewStandard, index.name, field.name)\n\t\t\t\t\t}\n\t\t\t\t\tif field.view(viewExistence) != nil {\n\t\t\t\t\t\t// delete view \"existence\" if NoStandardView is true and view \"existence\" exists\n\t\t\t\t\t\tfor _, shard := range field.AvailableShards(true).Slice() {\n\t\t\t\t\t\t\terr := s.holder.txf.DeleteFragmentFromStore(index.Name(), field.Name(), viewExistence, shard, nil)\n\t\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\t\ts.logger.Errorf(\"delete view %s from shard %d: %s\", viewExistence, shard, err)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\terr := s.defaultClient.api.DeleteView(ctx, index.Name(), field.Name(), viewExistence)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\ts.logger.Errorf(\"view: %s, delete view: %s\", viewExistence, err)\n\t\t\t\t\t\t}\n\t\t\t\t\t\ts.logger.Infof(\"view %s deleted - index: %s, field: %s \", viewExistence, index.name, field.name)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n// receiveMessage represents an implementation of BroadcastHandler.\nfunc (s *Server) receiveMessage(m Message) error {\n\tswitch obj := m.(type) {\n\tcase *CreateShardMessage:\n\t\tf := s.holder.Field(obj.Index, obj.Field)\n\t\tif f == nil {\n\t\t\treturn fmt.Errorf(\"local field not found: %s/%s\", obj.Index, obj.Field)\n\t\t}\n\t\tif err := f.AddRemoteAvailableShards(roaring.NewBitmap(obj.Shard)); err != nil {\n\t\t\treturn errors.Wrap(err, \"adding remote available shards\")\n\t\t}\n\n\tcase *CreateIndexMessage:\n\t\tif _, err := s.holder.LoadIndex(obj.Index); err != nil {\n\t\t\treturn err\n\t\t}\n\n\tcase *DeleteIndexMessage:\n\t\tif err := s.holder.DeleteIndex(obj.Index); err != nil {\n\t\t\treturn err\n\t\t}\n\n\tcase *CreateFieldMessage:\n\t\tif _, err := s.holder.LoadField(obj.Index, obj.Field); err != nil {\n\t\t\treturn err\n\t\t}\n\n\tcase *UpdateFieldMessage:\n\t\tidx := s.holder.Index(obj.CreateFieldMessage.Index)\n\t\tif err := idx.UpdateFieldLocal(&obj.CreateFieldMessage, obj.Update); err != nil {\n\t\t\treturn err\n\t\t}\n\n\tcase *DeleteFieldMessage:\n\t\tidx := s.holder.Index(obj.Index)\n\t\tif err := idx.DeleteField(obj.Field); err != nil {\n\t\t\treturn err\n\t\t}\n\n\tcase *DeleteAvailableShardMessage:\n\t\tf := s.holder.Field(obj.Index, obj.Field)\n\t\tif err := f.RemoveAvailableShard(obj.ShardID); err != nil {\n\t\t\treturn err\n\t\t}\n\n\tcase *CreateViewMessage:\n\t\tif _, err := s.holder.LoadView(obj.Index, obj.Field, obj.View); err != nil {\n\t\t\treturn err\n\t\t}\n\n\tcase *DeleteViewMessage:\n\t\tf := s.holder.Field(obj.Index, obj.Field)\n\t\tif f == nil {\n\t\t\treturn fmt.Errorf(\"local field not found: %s\", obj.Field)\n\t\t}\n\t\terr := f.deleteView(obj.View)\n\t\tif errors.Cause(err) == ErrInvalidView {\n\t\t\ts.logger.Infof(\"got intra-cluster message requesting delete of view: %s, but it did not exist (this is usually fine). Index: %s, field: %s \", obj.View, obj.Index, obj.Field)\n\t\t} else if err != nil {\n\t\t\treturn err\n\t\t}\n\n\tcase *RecalculateCaches:\n\t\ts.holder.recalculateCaches()\n\n\tcase *LoadSchemaMessage:\n\t\terr := s.holder.LoadSchema()\n\t\tif err != nil {\n\t\t\treturn errors.Wrapf(err, \"handling load schema message: %v\", obj)\n\t\t}\n\n\tcase *NodeStatus:\n\t\ts.handleRemoteStatus(obj)\n\n\tcase *TransactionMessage:\n\t\terr := s.handleTransactionMessage(obj)\n\t\tif err != nil {\n\t\t\treturn errors.Wrapf(err, \"handling transaction message: %v\", obj)\n\t\t}\n\tcase *DeleteDataframeMessage:\n\t\tif err := s.holder.DeleteDataframe(obj.Index); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (s *Server) handleTransactionMessage(tm *TransactionMessage) error {\n\tmtrns := tm.Transaction // message transaction\n\tctx := context.Background()\n\tswitch tm.Action {\n\tcase TRANSACTION_START:\n\t\t_, err := s.StartTransaction(ctx, mtrns.ID, mtrns.Timeout, mtrns.Exclusive, true)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"starting transaction locally\")\n\t\t}\n\tcase TRANSACTION_FINISH:\n\t\t_, err := s.FinishTransaction(ctx, mtrns.ID, true)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"finishing transaction locally\")\n\t\t}\n\tcase TRANSACTION_VALIDATE:\n\t\ttrns, err := s.GetTransaction(ctx, mtrns.ID, true)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"getting local transaction to validate\")\n\t\t}\n\t\treturn CompareTransactions(mtrns, trns)\n\tdefault:\n\t\treturn errors.Errorf(\"unknown transaction action: '%s'\", tm.Action)\n\t}\n\treturn nil\n}\n\n// SendSync represents an implementation of Broadcaster.\nfunc (s *Server) SendSync(m Message) error {\n\tvar eg errgroup.Group\n\tmsg, err := s.serializer.Marshal(m)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"marshaling message: %v\", err)\n\t}\n\tmsg = append([]byte{getMessageType(m)}, msg...)\n\n\tfor _, node := range s.cluster.Nodes() {\n\t\tnode := node\n\t\turi := node.URI // URI is a struct value\n\n\t\t// Don't forward the message to ourselves.\n\t\tif s.uri == uri {\n\t\t\tcontinue\n\t\t}\n\n\t\teg.Go(func() error {\n\t\t\treturn s.defaultClient.SendMessage(context.Background(), &uri, msg)\n\t\t})\n\t}\n\n\treturn eg.Wait()\n}\n\n// SendAsync represents an implementation of Broadcaster.\nfunc (s *Server) SendAsync(m Message) error {\n\treturn ErrNotImplemented\n}\n\n// SendTo represents an implementation of Broadcaster.\nfunc (s *Server) SendTo(node *disco.Node, m Message) error {\n\tmsg, err := s.serializer.Marshal(m)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"marshaling message: %v\", err)\n\t}\n\tmsg = append([]byte{getMessageType(m)}, msg...)\n\n\turi := node.URI // URI is a struct value\n\n\treturn s.defaultClient.SendMessage(context.Background(), &uri, msg)\n}\n\n// node returns the pilosa.node object. It is used by membership protocols to\n// get this node's name(ID), location(URI), and primary status.\nfunc (s *Server) node() *disco.Node {\n\treturn s.cluster.Node.Clone()\n}\n\n// handleRemoteStatus receives incoming NodeStatus from remote nodes.\nfunc (s *Server) handleRemoteStatus(pb Message) {\n\tstate, err := s.cluster.State()\n\tif err != nil {\n\t\ts.logger.Printf(\"getting cluster state: %s\", err)\n\t\treturn\n\t}\n\n\t// Ignore NodeStatus messages until the cluster is in a Normal state.\n\tif state != disco.ClusterStateNormal {\n\t\treturn\n\t}\n\n\tgo func() {\n\t\t// Make sure the holder has opened.\n\t\ts.holder.opened.Recv()\n\n\t\terr := s.mergeRemoteStatus(pb.(*NodeStatus))\n\t\tif err != nil {\n\t\t\ts.logger.Errorf(\"merge remote status: %s\", err)\n\t\t}\n\t}()\n}\n\nfunc (s *Server) mergeRemoteStatus(ns *NodeStatus) error {\n\t// Ignore status updates from self.\n\tif s.nodeID == ns.Node.ID {\n\t\treturn nil\n\t}\n\n\t// Sync schema.\n\tif err := s.holder.applySchema(ns.Schema); err != nil {\n\t\treturn errors.Wrap(err, \"applying schema\")\n\t}\n\n\t// Sync available shards.\n\tfor _, is := range ns.Indexes {\n\t\tfor _, fs := range is.Fields {\n\t\t\tf := s.holder.Field(is.Name, fs.Name)\n\n\t\t\t// if we don't know about a field locally, log an error because\n\t\t\t// fields should be created and synced prior to shard creation\n\t\t\tif f == nil {\n\t\t\t\ts.logger.Errorf(\"local field not found: %s/%s\", is.Name, fs.Name)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err := f.AddRemoteAvailableShards(fs.AvailableShards); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"adding remote available shards\")\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// IsPrimary returns if this node is primary right now or not.\nfunc (s *Server) IsPrimary() bool {\n\treturn s.nodeID == s.noder.PrimaryNodeID(s.cluster.Hasher)\n}\n\n// monitorDiagnostics periodically polls the Pilosa Indexes for cluster info.\nfunc (s *Server) monitorDiagnostics() {\n\t// Do not send more than once a minute\n\tif s.diagnosticInterval < time.Minute {\n\t\ts.logger.Infof(\"diagnostics disabled\")\n\t\treturn\n\t}\n\ts.logger.Infof(\"Pilosa is currently configured to send small diagnostics reports to our team every %v. More information here: https://www.pilosa.com/docs/latest/administration/#diagnostics\", s.diagnosticInterval)\n\n\ts.diagnostics.Logger = s.logger\n\ts.diagnostics.SetVersion(Version)\n\ts.diagnostics.Set(\"Host\", s.uri.Host)\n\ts.diagnostics.Set(\"Cluster\", strings.Join(s.cluster.nodeIDs(), \",\"))\n\ts.diagnostics.Set(\"NumNodes\", len(s.cluster.noder.Nodes()))\n\ts.diagnostics.Set(\"NumCPU\", runtime.NumCPU())\n\ts.diagnostics.Set(\"NodeID\", s.nodeID)\n\ts.diagnostics.Set(\"ClusterID\", s.cluster.id)\n\ts.diagnostics.EnrichWithCPUInfo()\n\ts.diagnostics.EnrichWithOSInfo()\n\n\t// Flush the diagnostics metrics at startup, then on each tick interval\n\tflush := func() {\n\t\topenFiles, err := countOpenFiles()\n\t\tif err == nil {\n\t\t\ts.diagnostics.Set(\"OpenFiles\", openFiles)\n\t\t}\n\t\ts.diagnostics.Set(\"GoRoutines\", runtime.NumGoroutine())\n\t\ts.diagnostics.EnrichWithMemoryInfo()\n\t\ts.diagnostics.EnrichWithSchemaProperties()\n\t\terr = s.diagnostics.CheckVersion()\n\t\tif err != nil {\n\t\t\ts.logger.Errorf(\"can't check version: %v\", err)\n\t\t}\n\t\terr = s.diagnostics.Flush()\n\t\tif err != nil {\n\t\t\ts.logger.Errorf(\"diagnostics error: %s\", err)\n\t\t}\n\t}\n\n\tticker := time.NewTicker(s.diagnosticInterval)\n\tdefer ticker.Stop()\n\tflush()\n\tfor {\n\t\t// Wait for tick or a close.\n\t\tselect {\n\t\tcase <-s.closing:\n\t\t\treturn\n\t\tcase <-ticker.C:\n\t\t\tflush()\n\t\t}\n\t}\n}\n\n// monitorRuntime periodically polls the Go runtime metrics.\nfunc (s *Server) monitorRuntime() {\n\t// Disable metrics when poll interval is zero.\n\tif s.metricInterval <= 0 {\n\t\treturn\n\t}\n\n\tvar m runtime.MemStats\n\tticker := time.NewTicker(s.metricInterval)\n\tdefer ticker.Stop()\n\n\tdefer s.gcNotifier.Close()\n\n\ts.logger.Infof(\"runtime stats initializing (%s interval)\", s.metricInterval)\n\n\tfor {\n\t\t// Wait for tick or a close.\n\t\tselect {\n\t\tcase <-s.closing:\n\t\t\treturn\n\t\tcase <-s.gcNotifier.AfterGC():\n\t\t\t// GC just ran.\n\t\t\tCounterGarbageCollection.Inc()\n\t\tcase <-ticker.C:\n\t\t}\n\n\t\t// Record the number of go routines.\n\t\tGaugeGoroutines.Set(float64(runtime.NumGoroutine()))\n\n\t\topenFiles, err := countOpenFiles()\n\t\t// Open File handles.\n\t\tif err == nil {\n\t\t\tGaugeOpenFiles.Set(float64(openFiles))\n\t\t}\n\n\t\t// Runtime memory metrics.\n\t\truntime.ReadMemStats(&m)\n\t\tGaugeHeapAlloc.Set(float64(m.HeapAlloc))\n\t\tGaugeHeapInUse.Set(float64(m.HeapInuse))\n\t\tGaugeStackInUse.Set(float64(m.StackInuse))\n\t\tGaugeMallocs.Set(float64(m.Mallocs))\n\t\tGaugeFrees.Set(float64(m.Frees))\n\t}\n}\n\nfunc (srv *Server) StartTransaction(ctx context.Context, id string, timeout time.Duration, exclusive bool, remote bool) (*Transaction, error) {\n\tsnap := srv.cluster.NewSnapshot()\n\tnode := srv.node()\n\tif !remote && !snap.IsPrimaryFieldTranslationNode(node.ID) && len(srv.cluster.Nodes()) > 1 {\n\t\treturn nil, ErrNodeNotPrimary\n\t}\n\tif remote && (snap.IsPrimaryFieldTranslationNode(node.ID) || len(srv.cluster.Nodes()) == 1) {\n\t\treturn nil, errors.New(\"unexpected remote start call to primary or single node cluster\")\n\t}\n\n\tif remote {\n\t\treturn srv.holder.StartTransaction(ctx, id, timeout, exclusive)\n\t}\n\n\t// empty string id should generate an id\n\tif id == \"\" {\n\t\tuid, err := uuid.NewV4()\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"creating id\")\n\t\t}\n\t\tid = uid.String()\n\t}\n\ttrns, err := srv.holder.StartTransaction(ctx, id, timeout, exclusive)\n\tif err != nil {\n\t\treturn trns, errors.Wrap(err, \"starting transaction\")\n\t}\n\terr = srv.SendSync(\n\t\t&TransactionMessage{\n\t\t\tAction:      TRANSACTION_START,\n\t\t\tTransaction: trns,\n\t\t})\n\tif err != nil {\n\t\t// try to clean up, but ignore errors\n\t\t_, errLocal := srv.holder.FinishTransaction(ctx, id)\n\t\terrBroadcast := srv.SendSync(\n\t\t\t&TransactionMessage{\n\t\t\t\tAction:      TRANSACTION_FINISH,\n\t\t\t\tTransaction: trns,\n\t\t\t},\n\t\t)\n\t\tif errLocal != nil || errBroadcast != nil {\n\t\t\tsrv.logger.Errorf(\"error(s) while trying to clean up transaction which failed to start, local: %v, broadcast: %v\",\n\t\t\t\terrLocal,\n\t\t\t\terrBroadcast,\n\t\t\t)\n\t\t}\n\t\treturn trns, errors.Wrap(err, \"broadcasting transaction start\")\n\t}\n\treturn trns, nil\n}\n\nfunc (srv *Server) FinishTransaction(ctx context.Context, id string, remote bool) (*Transaction, error) {\n\tsnap := srv.cluster.NewSnapshot()\n\tnode := srv.node()\n\tif !remote && !snap.IsPrimaryFieldTranslationNode(node.ID) && len(srv.cluster.Nodes()) > 1 {\n\t\treturn nil, ErrNodeNotPrimary\n\t}\n\tif remote && (snap.IsPrimaryFieldTranslationNode(node.ID) || len(srv.cluster.Nodes()) == 1) {\n\t\treturn nil, errors.New(\"unexpected remote finish call to primary or single node cluster\")\n\t}\n\n\tif remote {\n\t\treturn srv.holder.FinishTransaction(ctx, id)\n\t}\n\ttrns, err := srv.holder.FinishTransaction(ctx, id)\n\tif err != nil {\n\t\treturn trns, errors.Wrap(err, \"finishing transaction\")\n\t}\n\terr = srv.SendSync(\n\t\t&TransactionMessage{\n\t\t\tAction:      TRANSACTION_FINISH,\n\t\t\tTransaction: trns,\n\t\t},\n\t)\n\tif err != nil {\n\t\tsrv.logger.Errorf(\"error broadcasting transaction finish: %v\", err)\n\t\t// TODO retry?\n\t}\n\treturn trns, nil\n}\n\nfunc (srv *Server) Transactions(ctx context.Context) (map[string]*Transaction, error) {\n\tsnap := srv.cluster.NewSnapshot()\n\tnode := srv.node()\n\tif !snap.IsPrimaryFieldTranslationNode(node.ID) && len(srv.cluster.Nodes()) > 1 {\n\t\treturn nil, ErrNodeNotPrimary\n\t}\n\n\treturn srv.holder.Transactions(ctx)\n}\n\nfunc (srv *Server) GetTransaction(ctx context.Context, id string, remote bool) (*Transaction, error) {\n\tsnap := srv.cluster.NewSnapshot()\n\n\tnode := srv.node()\n\tif !remote && !snap.IsPrimaryFieldTranslationNode(node.ID) && len(srv.cluster.Nodes()) > 1 {\n\t\treturn nil, ErrNodeNotPrimary\n\t}\n\n\tif remote && (snap.IsPrimaryFieldTranslationNode(node.ID) || len(srv.cluster.Nodes()) == 1) {\n\t\treturn nil, errors.New(\"unexpected remote get call to primary or single node cluster\")\n\t}\n\n\ttrns, err := srv.holder.GetTransaction(ctx, id)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"getting transaction\")\n\t}\n\n\t// The way a client would find out that the exclusive transaction\n\t// it requested is active is by polling the GetTransaction\n\t// endpoint. Therefore, returning an active, exclusive\n\t// transaction, from here is what truly makes the transaction\n\t// \"live\". Before doing so, we want to make sure all nodes\n\t// agree. (in case other nodes have activity on this transaction\n\t// we're not aware of)\n\tif !remote && trns.Exclusive && trns.Active {\n\t\terr := srv.SendSync(\n\t\t\t&TransactionMessage{\n\t\t\t\tAction:      TRANSACTION_VALIDATE,\n\t\t\t\tTransaction: trns,\n\t\t\t},\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"contacting remote hosts\")\n\t\t}\n\t\treturn trns, nil\n\t}\n\treturn trns, nil\n}\n\n// CompileExecutionPlan parses and compiles an execution plan from a SQL\n// statement using a new parser and planner.\nfunc (s *Server) CompileExecutionPlan(ctx context.Context, q string) (planner_types.PlanOperator, error) {\n\tst, err := parser.NewParser(strings.NewReader(q)).ParseStatement()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn s.executionPlannerFn(s.executor, s.executor.client.api, q).CompilePlan(ctx, st)\n}\n\nfunc (s *Server) RehydratePlanOperator(ctx context.Context, reader io.Reader) (planner_types.PlanOperator, error) {\n\treturn s.executionPlannerFn(s.executor, s.executor.client.api, \"\").RehydratePlanOp(ctx, reader)\n}\n\n// countOpenFiles on operating systems that support lsof.\nfunc countOpenFiles() (int, error) {\n\tswitch runtime.GOOS {\n\tcase \"darwin\", \"linux\", \"unix\", \"freebsd\":\n\t\t// -b option avoid kernel blocks\n\t\tpid := os.Getpid()\n\t\tout, err := exec.Command(\"/bin/sh\", \"-c\", fmt.Sprintf(\"lsof -b -p %v\", pid)).Output()\n\t\tif err != nil {\n\t\t\treturn 0, fmt.Errorf(\"calling lsof: %s\", err)\n\t\t}\n\t\t// only count lines with our pid, avoiding warning messages from -b\n\t\tlines := strings.Split(string(out), strconv.Itoa(pid))\n\t\treturn len(lines), nil\n\tcase \"windows\":\n\t\treturn 0, errors.New(\"countOpenFiles() on Windows is not supported\")\n\tdefault:\n\t\treturn 0, errors.New(\"countOpenFiles() on this OS is not supported\")\n\t}\n}\n\nfunc expandDirName(path string) (string, error) {\n\tprefix := \"~\" + string(filepath.Separator)\n\tif strings.HasPrefix(path, prefix) {\n\t\tHomeDir := os.Getenv(\"HOME\")\n\t\tif HomeDir == \"\" {\n\t\t\treturn \"\", errors.New(\"data directory not specified and no home dir available\")\n\t\t}\n\t\treturn filepath.Join(HomeDir, strings.TrimPrefix(path, prefix)), nil\n\t}\n\treturn path, nil\n}\n"
        },
        {
          "name": "server",
          "type": "tree",
          "content": null
        },
        {
          "name": "server_internal_test.go",
          "type": "blob",
          "size": 1.5224609375,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/storage\"\n\t\"github.com/featurebasedb/featurebase/v3/testhook\"\n)\n\nfunc TestMonitorAntiEntropyZero(t *testing.T) {\n\n\ttd, err := testhook.TempDir(t, \"\")\n\tif err != nil {\n\t\tt.Fatalf(\"getting temp dir: %v\", err)\n\t}\n\tcfg := &storage.Config{FsyncEnabled: false, Backend: storage.DefaultBackend}\n\ts, err := NewServer(OptServerDataDir(td), OptServerStorageConfig(cfg))\n\tif err != nil {\n\t\tt.Fatalf(\"making new server: %v\", err)\n\t}\n\tdefer s.Close()\n\n\tch := make(chan struct{})\n\tgo func() {\n\t\tclose(ch)\n\t}()\n\n\tselect {\n\tcase <-ch:\n\tcase <-time.After(time.Second):\n\t\tt.Fatalf(\"monitorAntiEntropy should have returned immediately with duration 0\")\n\t}\n}\n\nfunc TestAddToWaitGroup(t *testing.T) {\n\t// if this test times out / panics we have a problem, otherwise we're fine\n\ttd := t.TempDir()\n\tcfg := &storage.Config{FsyncEnabled: false, Backend: storage.DefaultBackend}\n\ts, err := NewServer(OptServerDataDir(td), OptServerStorageConfig(cfg))\n\tif err != nil {\n\t\tt.Fatalf(\"making new server: %v\", err)\n\t}\n\n\toks := make(chan bool, 10)\n\tfor i := 0; i < 10; i++ {\n\t\tgo func() {\n\t\t\toks <- s.addToWaitGroup(1)\n\t\t\ttime.Sleep(10 * time.Millisecond)\n\t\t\tdefer s.wg.Done()\n\t\t}()\n\t}\n\n\tfor i := 0; i < 10; i++ {\n\t\tok := <-oks\n\t\tif !ok {\n\t\t\tt.Fatalf(\"unexpected close during WaitGroup add\")\n\t\t}\n\t}\n\n\ts.Close()\n\tif ok := s.addToWaitGroup(1); ok {\n\t\tt.Fatalf(\"shouldn't be able to add while server is closing\")\n\t}\n}\n"
        },
        {
          "name": "server_test.go",
          "type": "blob",
          "size": 11.32421875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"sort\"\n\t\"testing\"\n\t\"time\"\n\n\tpilosa \"github.com/featurebasedb/featurebase/v3\"\n\t\"github.com/featurebasedb/featurebase/v3/test\"\n)\n\nfunc TestViewsRemovalTTL(t *testing.T) {\n\tcluster := test.MustRunCluster(t, 1)\n\tnode := cluster.GetNode(0)\n\tdefer cluster.Close()\n\n\t// Create a client\n\tclient := node.Client()\n\n\tindexName := cluster.Idx()\n\n\t// Create indexes and field with ttl lasting 24 hours\n\tif err := client.CreateIndex(context.Background(), indexName, pilosa.IndexOptions{TrackExistence: true}); err != nil && err != pilosa.ErrIndexExists {\n\t\tt.Fatalf(\"creating index, err: %v\", err)\n\t}\n\n\tdateNow := time.Now().UTC()\n\tdateYesterday := dateNow.Add(-24 * time.Hour)\n\tdateCurrentMonth := time.Date(dateNow.Year(), dateNow.Month(), 1, 0, 0, 0, 0, time.UTC)\n\tdateLastDayOfMonth := dateCurrentMonth.AddDate(0, 1, 0).Add(-time.Nanosecond)\n\n\tvar tests = []struct {\n\t\tname     string\n\t\tdate     string\n\t\texpViews []string\n\t}{\n\t\t{\n\t\t\tname:     \"date_old\",\n\t\t\tdate:     \"2001-02-03T04:05\",\n\t\t\texpViews: []string{\"existence\", \"standard\"},\n\t\t\t/* date_old (2001-02-03T04:05), this will create these views:\n\t\t\t- standard\n\t\t\t- standard_2001\n\t\t\t- standard_200102\n\t\t\t- standard_20010203\n\t\t\t- standard_2001020304\n\t\t\tSince the sample date here is over 20 years all views except \"standard\" should get deleted\n\t\t\t*/\n\t\t},\n\t\t{\n\t\t\tname: \"date_now\",\n\t\t\tdate: fmt.Sprintf(\"%d-%02d-%02dT%02d:%02d\", dateNow.Year(), dateNow.Month(), dateNow.Day(), dateNow.Hour(), dateNow.Minute()),\n\t\t\texpViews: []string{\n\t\t\t\t\"existence\",\n\t\t\t\t\"standard\",\n\t\t\t\t\"standard_\" + fmt.Sprintf(\"%d\", dateNow.Year()),\n\t\t\t\t\"standard_\" + fmt.Sprintf(\"%d%02d\", dateNow.Year(), dateNow.Month()),\n\t\t\t\t\"standard_\" + fmt.Sprintf(\"%d%02d%02d\", dateNow.Year(), dateNow.Month(), dateNow.Day()),\n\t\t\t\t\"standard_\" + fmt.Sprintf(\"%d%02d%02d%02d\", dateNow.Year(), dateNow.Month(), dateNow.Day(), dateNow.Hour()),\n\t\t\t},\n\t\t\t/* For example: current time is 2022-05-11T15:17 (also when the 24 hrs ttl countdown starts) will generate these views:\n\t\t\t- standard_2022\t\t\t-> end date is 2023_01_01 T00:00, in future -> keep\n\t\t\t- standard_202205\t\t-> end date is 2022_06_01 T00:00, in future -> keep\n\t\t\t- standard_20220511\t\t-> end date is 2022_05_12 T00:00, in future -> keep\n\t\t\t- standard_2022051115\t-> end date is 2022_05_11 T18:00, in future -> keep\n\t\t\t*/\n\t\t},\n\t\t{\n\t\t\tname: \"date_yesterday\",\n\t\t\tdate: fmt.Sprintf(\"%d-%02d-%02dT%02d:%02d\", dateYesterday.Year(), dateYesterday.Month(), dateYesterday.Day(), dateYesterday.Hour(), dateYesterday.Minute()),\n\t\t\texpViews: []string{\n\t\t\t\t\"existence\",\n\t\t\t\t\"standard\",\n\t\t\t\t\"standard_\" + fmt.Sprintf(\"%d\", dateYesterday.Year()),\n\t\t\t\t\"standard_\" + fmt.Sprintf(\"%d%02d\", dateYesterday.Year(), dateYesterday.Month()),\n\t\t\t\t\"standard_\" + fmt.Sprintf(\"%d%02d%02d\", dateYesterday.Year(), dateYesterday.Month(), dateYesterday.Day()),\n\t\t\t\t\"standard_\" + fmt.Sprintf(\"%d%02d%02d%02d\", dateYesterday.Year(), dateYesterday.Month(), dateYesterday.Day(), dateYesterday.Hour()),\n\t\t\t},\n\t\t\t/* Example: current time is 2022-05-11T15:17, date_yesterday (2022-05-10T15:17) will generate these views:\n\t\t\t- standard_2022\t\t\t-> end date is 2023_01_01 T00:00, in future -> keep\n\t\t\t- standard_202205\t\t-> end date is 2022_06_01 T00:00, in future -> keep\n\t\t\t- standard_20220510\t\t-> end date is 2022_05_11 T00:00, within 24 hrs -> keep\n\t\t\t- standard_2022051015\t-> end date is 2022_05_10 T16:00, within 24 hrs -> keep\n\t\t\t*/\n\t\t},\n\t\t// {\n\t\t// \tname: \"date_first_of_month\",\n\t\t// \tdate: fmt.Sprintf(\"%d-%02d-%02dT%02d:%02d\", dateCurrentMonth.Year(), dateCurrentMonth.Month(), 1, 0, 0),\n\t\t// \texpViews: []string{\n\t\t// \t\t\"standard\",\n\t\t// \t\t\"standard_\" + fmt.Sprintf(\"%d\", dateCurrentMonth.Year()),\n\t\t// \t\t\"standard_\" + fmt.Sprintf(\"%d%02d\", dateCurrentMonth.Year(), dateCurrentMonth.Month()),\n\t\t// \t},\n\t\t// \t/* Example: current time is 2022-05-11T15:17, date_first_of_month (2022-05-01T00:00) will generate these views:\n\t\t// \t- standard_2022\t\t\t-> end date is 2023_01_01 T00:00, in future -> keep\n\t\t// \t- standard_202205\t\t-> end date is 2022_06_01 T00:00, in future -> keep\n\t\t// \t- standard_20220501\t\t-> end date is 2022_05_02 T00:00, older than ttl -> delete\n\t\t// \t- standard_2022050115\t-> end date is 2022_05_01 T01:00, older than ttl -> delete\n\t\t// \t!!! commenting this test out for now, there is a special case where if today's date is also same as date_first_of_month\n\t\t// \tin that case, the expected views would have all 4, instead of 2 in the above example, since they are not older than the 24 hr ttl\n\t\t// \t*/\n\t\t// },\n\t\t{\n\t\t\tname: \"date_last_of_month\",\n\t\t\tdate: fmt.Sprintf(\"%d-%02d-%02dT%02d:%02d\", dateLastDayOfMonth.Year(), dateLastDayOfMonth.Month(), dateLastDayOfMonth.Day(), dateLastDayOfMonth.Hour(), dateLastDayOfMonth.Minute()),\n\t\t\texpViews: []string{\n\t\t\t\t\"existence\",\n\t\t\t\t\"standard\",\n\t\t\t\t\"standard_\" + fmt.Sprintf(\"%d\", dateLastDayOfMonth.Year()),\n\t\t\t\t\"standard_\" + fmt.Sprintf(\"%d%02d\", dateLastDayOfMonth.Year(), dateLastDayOfMonth.Month()),\n\t\t\t\t\"standard_\" + fmt.Sprintf(\"%d%02d%02d\", dateNow.Year(), dateNow.Month(), dateLastDayOfMonth.Day()),\n\t\t\t\t\"standard_\" + fmt.Sprintf(\"%d%02d%02d%02d\", dateNow.Year(), dateNow.Month(), dateLastDayOfMonth.Day(), dateLastDayOfMonth.Hour()),\n\t\t\t},\n\t\t\t/* Example: current time is 2022-05-11T15:17, date_last_of_month (2022-05-31T23:59) will generate these views:\n\t\t\t- standard_2022\t\t\t-> end date is 2023_01_01 T00:00, in future -> keep\n\t\t\t- standard_202205\t\t-> end date is 2022_06_01 T00:00, in future -> keep\n\t\t\t- standard_20220531\t\t-> end date is 2022_06_01 T00:00, in future -> keep\n\t\t\t- standard_2022053123\t-> end date is 2022_06_01 T00:00, in future -> keep\n\t\t\t*/\n\t\t},\n\t\t{\n\t\t\tname: \"date_first_hour_day\",\n\t\t\tdate: fmt.Sprintf(\"%d-%02d-%02dT%02d:%02d\", dateNow.Year(), dateNow.Month(), dateNow.Day(), 0, 0),\n\t\t\texpViews: []string{\n\t\t\t\t\"existence\",\n\t\t\t\t\"standard\",\n\t\t\t\t\"standard_\" + fmt.Sprintf(\"%d\", dateNow.Year()),\n\t\t\t\t\"standard_\" + fmt.Sprintf(\"%d%02d\", dateNow.Year(), dateNow.Month()),\n\t\t\t\t\"standard_\" + fmt.Sprintf(\"%d%02d%02d\", dateNow.Year(), dateNow.Month(), dateNow.Day()),\n\t\t\t\t\"standard_\" + fmt.Sprintf(\"%d%02d%02d%02d\", dateNow.Year(), dateNow.Month(), dateNow.Day(), 0),\n\t\t\t},\n\t\t\t/* Example: current time is 2022-05-11T15:17, date_first_hour_day (2022-05-11T00:00) will generate these views:\n\t\t\t- standard_2022\t\t\t-> end date is 2023_01_01 T00:00, in future -> keep\n\t\t\t- standard_202205\t\t-> end date is 2022_06_01 T00:00, in future -> keep\n\t\t\t- standard_20220511\t\t-> end date is 2022_05_12 T00:00, in future -> keep\n\t\t\t- standard_2022051100\t-> end date is 2022_05_01 T01:00, within TTL -> keep\n\t\t\t*/\n\t\t},\n\t\t{\n\t\t\tname: \"date_last_hour_day\",\n\t\t\tdate: fmt.Sprintf(\"%d-%02d-%02dT%02d:%02d\", dateNow.Year(), dateNow.Month(), dateNow.Day(), 23, 59),\n\t\t\texpViews: []string{\n\t\t\t\t\"existence\",\n\t\t\t\t\"standard\",\n\t\t\t\t\"standard_\" + fmt.Sprintf(\"%d\", dateNow.Year()),\n\t\t\t\t\"standard_\" + fmt.Sprintf(\"%d%02d\", dateNow.Year(), dateNow.Month()),\n\t\t\t\t\"standard_\" + fmt.Sprintf(\"%d%02d%02d\", dateNow.Year(), dateNow.Month(), dateNow.Day()),\n\t\t\t\t\"standard_\" + fmt.Sprintf(\"%d%02d%02d%02d\", dateNow.Year(), dateNow.Month(), dateNow.Day(), 23),\n\t\t\t},\n\t\t\t/* Example: current time is 2022-05-11T15:17, date_last_hour_day (2022-05-11T23:59) will generate these views:\n\t\t\t- standard_2022\t\t\t-> end date is 2023_01_01 T00:00, in future -> keep\n\t\t\t- standard_202205\t\t-> end date is 2022_06_01 T00:00, in future -> keep\n\t\t\t- standard_20220511\t\t-> end date is 2022_05_12 T00:00, in future -> keep\n\t\t\t- standard_2022051123\t-> end date is 2022_05_12 T00:00, in future -> keep\n\t\t\t*/\n\t\t},\n\t}\n\tfor _, test := range tests {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tif err := client.CreateFieldWithOptions(context.Background(), indexName, test.name, pilosa.FieldOptions{TTL: time.Hour * 24, Type: pilosa.FieldTypeTime, TimeQuantum: \"YMDH\"}); err != nil {\n\t\t\t\tt.Fatalf(\"creating field, err: %v\", err)\n\t\t\t}\n\n\t\t\t// set data\n\t\t\t_, err := client.Query(context.Background(), indexName, &pilosa.QueryRequest{Index: indexName, Query: \"Set(1, \" + test.name + \"=1, \" + test.date + \")\"})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"setting sample data, err: %v\", err)\n\t\t\t}\n\n\t\t\t// run ViewsRemoval\n\t\t\tnode.Server.ViewsRemoval(context.Background())\n\n\t\t\t// Get all the views for given index + field\n\t\t\tviews, err := node.API.Views(context.Background(), indexName, test.name)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tvar viewNames []string\n\t\t\tfor _, view := range views {\n\t\t\t\tviewNames = append(viewNames, view.Name())\n\t\t\t}\n\t\t\tsort.Strings(viewNames)\n\n\t\t\tif !reflect.DeepEqual(test.expViews, viewNames) {\n\t\t\t\tt.Fatalf(\"after ttl removal, expected %v, but got %v\", test.expViews, viewNames)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestViewsRemovalStandard(t *testing.T) {\n\tcluster := test.MustRunCluster(t, 1)\n\tnode := cluster.GetNode(0)\n\tdefer cluster.Close()\n\n\t// Create a client\n\tclient := node.Client()\n\n\tindexName := cluster.Idx()\n\n\t// Create indexes and field with ttl lasting 24 hours\n\tif err := client.CreateIndex(context.Background(), indexName, pilosa.IndexOptions{TrackExistence: true}); err != nil && err != pilosa.ErrIndexExists {\n\t\tt.Fatalf(\"creating index, err: %v\", err)\n\t}\n\n\tvar tests = []struct {\n\t\tname           string\n\t\tdate           string\n\t\tnoStandardView string\n\t\texpViews       []string\n\t}{\n\t\t{\n\t\t\tname:           \"t1_delete_standard\",\n\t\t\tdate:           \"2001-02-03T04:05\",\n\t\t\tnoStandardView: \"true\",\n\t\t\texpViews:       nil,\n\t\t\t/* date 2001-02-03T04:05, this will create these views:\n\t\t\t- standard\n\t\t\t- standard_2001\n\t\t\t- standard_200102\n\t\t\t- standard_20010203\n\t\t\t- standard_2001020304\n\t\t\tSince the sample date here is over 20 years, all views with dates should get deleted\n\t\t\tSince noStandardView is true, 'standard' view should get deleted\n\t\t\t*/\n\t\t},\n\t\t{\n\t\t\tname:           \"t2_keep_standard\",\n\t\t\tdate:           \"2001-02-03T04:05\",\n\t\t\tnoStandardView: \"false\",\n\t\t\texpViews:       []string{\"existence\", \"standard\"},\n\t\t\t/* date 2001-02-03T04:05, this will create these views:\n\t\t\t- standard\n\t\t\t- standard_2001\n\t\t\t- standard_200102\n\t\t\t- standard_20010203\n\t\t\t- standard_2001020304\n\t\t\tSince the sample date here is over 20 years, all views with dates should get deleted\n\t\t\tSince noStandardView is false, 'standard' view should NOT get deleted\n\t\t\t*/\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tif err := client.CreateFieldWithOptions(context.Background(), indexName, test.name, pilosa.FieldOptions{TTL: time.Hour * 24, Type: pilosa.FieldTypeTime, TimeQuantum: \"YMDH\"}); err != nil {\n\t\t\t\tt.Fatalf(\"creating field, err: %v\", err)\n\t\t\t}\n\n\t\t\t// set data\n\t\t\t_, err := client.Query(context.Background(), indexName, &pilosa.QueryRequest{Index: indexName, Query: \"Set(1, \" + test.name + \"=1, \" + test.date + \")\"})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"setting sample data, err: %v\", err)\n\t\t\t}\n\n\t\t\t// update noStandardView value\n\t\t\terr = node.API.UpdateField(context.Background(), indexName, test.name, pilosa.FieldUpdate{Option: \"noStandardView\", Value: test.noStandardView})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"updating noStandardView, err: %v\", err)\n\t\t\t}\n\n\t\t\t// run ViewsRemoval\n\t\t\tnode.Server.ViewsRemoval(context.Background())\n\n\t\t\t// get all the views for given index + field\n\t\t\tviews, err := node.API.Views(context.Background(), indexName, test.name)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tvar viewNames []string\n\t\t\tfor _, view := range views {\n\t\t\t\tviewNames = append(viewNames, view.Name())\n\t\t\t}\n\t\t\tsort.Strings(viewNames)\n\t\t\tif !reflect.DeepEqual(test.expViews, viewNames) {\n\t\t\t\tt.Fatalf(\"after ttl removal, expected %v, but got %v\", test.expViews, viewNames)\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "shardwidth",
          "type": "tree",
          "content": null
        },
        {
          "name": "short_txkey",
          "type": "tree",
          "content": null
        },
        {
          "name": "sql",
          "type": "tree",
          "content": null
        },
        {
          "name": "sql3",
          "type": "tree",
          "content": null
        },
        {
          "name": "sql_test.go",
          "type": "blob",
          "size": 3.013671875,
          "content": "package pilosa_test\n\nimport (\n\t\"encoding/json\"\n\t\"testing\"\n\n\tfeaturebase \"github.com/featurebasedb/featurebase/v3\"\n\t\"github.com/featurebasedb/featurebase/v3/pql\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestSQL(t *testing.T) {\n\tt.Run(\"SQLResponse\", func(t *testing.T) {\n\t\tt.Run(\"Error\", func(t *testing.T) {\n\t\t\tbody := `{\"error\": \"bad\"}`\n\t\t\tsqlResponse := &featurebase.WireQueryResponse{}\n\t\t\terr := json.Unmarshal([]byte(body), sqlResponse)\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.Equal(t, \"bad\", sqlResponse.Error)\n\t\t})\n\n\t\tt.Run(\"Typed\", func(t *testing.T) {\n\t\t\tbody := `{\n\t\t\t\t\"schema\":{\n\t\t\t\t\t\"fields\":[\n\t\t\t\t\t\t{\"name\":\"an_int\",\"type\":\"int\",\"base-type\":\"int\"},\n\t\t\t\t\t\t{\"name\":\"a_decimal\",\"type\":\"decimal\",\"base-type\":\"decimal\",\"type-info\":{\"scale\":2}},\n\t\t\t\t\t\t{\"name\":\"a_stringset\",\"type\":\"stringset\",\"base-type\":\"stringset\"},\n\t\t\t\t\t\t{\"name\":\"an_idset\",\"type\":\"idset\",\"base-type\":\"idset\"},\n\t\t\t\t\t\t{\"name\":\"a_bool\",\"type\":\"bool\",\"base-type\":\"bool\"},\n\t\t\t\t\t\t{\"name\":\"a_string\",\"type\":\"string\",\"base-type\":\"string\"},\n\t\t\t\t\t\t{\"name\":\"an_id\",\"type\":\"id\",\"base-type\":\"id\"}\n\t\t\t\t\t]\n\t\t\t\t},\n\t\t\t\t\"data\":[\n\t\t\t\t\t[1, 12.34, [\"foo\", \"bar\"], [4,5], true, \"foobar\", 8]\n\t\t\t\t],\n\t\t\t\t\"error\":\"\",\n\t\t\t\t\"warnings\":null\n\t\t\t}`\n\t\t\tsqlResponse := &featurebase.WireQueryResponse{}\n\t\t\t//err := json.Unmarshal([]byte(body), sqlResponse)\n\t\t\terr := sqlResponse.UnmarshalJSONTyped([]byte(body), true)\n\t\t\tassert.NoError(t, err)\n\n\t\t\trow0 := sqlResponse.Data[0]\n\n\t\t\tassert.Equal(t, int64(1), row0[0])\n\t\t\tassert.Equal(t, pql.NewDecimal(1234, 2), row0[1])\n\t\t\tassert.Equal(t, featurebase.StringSet([]string{\"foo\", \"bar\"}), row0[2])\n\t\t\tassert.Equal(t, featurebase.IDSet([]int64{4, 5}), row0[3])\n\t\t\tassert.Equal(t, true, row0[4])\n\t\t\tassert.Equal(t, \"foobar\", row0[5])\n\t\t\tassert.Equal(t, int64(8), row0[6])\n\n\t\t\t// Check the set stringers.\n\t\t\tassert.Equal(t, \"['foo', 'bar']\", row0[2].(featurebase.StringSet).String())\n\t\t\tassert.Equal(t, \"[4, 5]\", row0[3].(featurebase.IDSet).String())\n\t\t})\n\n\t\tt.Run(\"Untyped\", func(t *testing.T) {\n\t\t\tbody := `{\n\t\t\t\t\"schema\":{\n\t\t\t\t\t\"fields\":[\n\t\t\t\t\t\t{\"name\":\"an_int\",\"type\":\"int\",\"base-type\":\"int\"},\n\t\t\t\t\t\t{\"name\":\"a_decimal\",\"type\":\"decimal\",\"base-type\":\"decimal\",\"type-info\":{\"scale\":2}},\n\t\t\t\t\t\t{\"name\":\"a_stringset\",\"type\":\"stringset\",\"base-type\":\"stringset\"},\n\t\t\t\t\t\t{\"name\":\"an_idset\",\"type\":\"idset\",\"base-type\":\"idset\"},\n\t\t\t\t\t\t{\"name\":\"a_bool\",\"type\":\"bool\",\"base-type\":\"bool\"},\n\t\t\t\t\t\t{\"name\":\"a_string\",\"type\":\"string\",\"base-type\":\"string\"},\n\t\t\t\t\t\t{\"name\":\"an_id\",\"type\":\"id\",\"base-type\":\"id\"}\n\t\t\t\t\t]\n\t\t\t\t},\n\t\t\t\t\"data\":[\n\t\t\t\t\t[1, 12.34, [\"foo\", \"bar\"], [4,5], true, \"foobar\", 8]\n\t\t\t\t],\n\t\t\t\t\"error\":\"\",\n\t\t\t\t\"warnings\":null\n\t\t\t}`\n\t\t\tsqlResponse := &featurebase.WireQueryResponse{}\n\t\t\terr := json.Unmarshal([]byte(body), sqlResponse)\n\t\t\tassert.NoError(t, err)\n\n\t\t\trow0 := sqlResponse.Data[0]\n\n\t\t\tassert.Equal(t, int64(1), row0[0])\n\t\t\tassert.Equal(t, pql.NewDecimal(1234, 2), row0[1])\n\t\t\tassert.Equal(t, []string{\"foo\", \"bar\"}, row0[2])\n\t\t\tassert.Equal(t, []int64{4, 5}, row0[3])\n\t\t\tassert.Equal(t, true, row0[4])\n\t\t\tassert.Equal(t, \"foobar\", row0[5])\n\t\t\tassert.Equal(t, int64(8), row0[6])\n\t\t})\n\t})\n}\n"
        },
        {
          "name": "statik",
          "type": "tree",
          "content": null
        },
        {
          "name": "stats",
          "type": "tree",
          "content": null
        },
        {
          "name": "stattx.go",
          "type": "blob",
          "size": 11.6904296875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"runtime\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/debugstats\"\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\ttxkey \"github.com/featurebasedb/featurebase/v3/short_txkey\"\n\t\"github.com/featurebasedb/featurebase/v3/storage\"\n\t\"github.com/featurebasedb/featurebase/v3/vprint\"\n)\n\n// statTx is useful to profile on a\n// per method basis, and to play with\n// read/write locking.\ntype statTx struct {\n\tb     Tx\n\tstats *callStats\n}\n\n// for now, just track call stats globally. But each statTx has\n// a pointer to a callStats, so could be made per index or per shard, etc.\nvar globalCallStats = newCallStats()\n\ntype callStats struct {\n\t// protect elap\n\tmu sync.Mutex\n\n\t// track how much time each call took.\n\telap map[kall]*elapsed\n}\n\ntype elapsed struct {\n\tdur []float64\n}\n\nfunc newCallStats() *callStats {\n\tw := &callStats{}\n\tw.reset()\n\treturn w\n}\n\nfunc (w *callStats) reset() {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\n\tw.elap = make(map[kall]*elapsed)\n\tfor i := kall(0); i < kLast; i++ {\n\t\tw.elap[i] = &elapsed{}\n\t}\n}\n\nfunc (c *callStats) report() (r string) {\n\tbackend := storage.DefaultBackend\n\tr = fmt.Sprintf(\"callStats: (%v)\\n\", backend)\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\tvar lines []*debugstats.LineSorter\n\tfor i := kall(0); i < kLast; i++ {\n\t\tslc := c.elap[i].dur\n\t\tn := len(slc)\n\t\tif n == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tmean, sd, totaltm := computeMeanSd(slc)\n\t\tif n == 1 {\n\t\t\tsd = 0\n\t\t\tmean = slc[0]\n\t\t\ttotaltm = slc[0]\n\t\t}\n\t\tline := fmt.Sprintf(\"  %20v  N=%8v   avg/op: %12v   sd: %12v  total: %12v\\n\", i.String(), n, time.Duration(mean), time.Duration(sd), time.Duration(totaltm))\n\t\tlines = append(lines, &debugstats.LineSorter{Line: line, Tot: totaltm})\n\t}\n\tsort.Sort(debugstats.SortByTot(lines))\n\tfor i := range lines {\n\t\tr += lines[i].Line\n\t}\n\n\tvar m1 runtime.MemStats\n\truntime.ReadMemStats(&m1)\n\tr += fmt.Sprintf(\"\\n m1.TotalAlloc = %v\\n\", m1.TotalAlloc)\n\n\treturn\n}\n\nvar NaN = math.NaN()\n\nfunc computeMeanSd(slc []float64) (mean, sd, tot float64) {\n\tif len(slc) < 2 {\n\t\treturn NaN, NaN, NaN\n\t}\n\tfor _, v := range slc {\n\t\ttot += v\n\t}\n\tn := float64(len(slc))\n\tmean = tot / n\n\n\tvariance := 0.0\n\tfor _, v := range slc {\n\t\ttmp := (v - mean)\n\t\tvariance += tmp * tmp\n\t}\n\tvariance = variance / n // biased, but we don't care b/c we can have very small n\n\tsd = math.Sqrt(variance)\n\tif sd < 1e-8 {\n\t\t// sd is super close to zero, NaN out the z-score rather than +/- Inf\n\t\tsd = NaN\n\t}\n\treturn\n}\n\nfunc (c *callStats) add(k kall, dur time.Duration) {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\te := c.elap[k]\n\te.dur = append(e.dur, float64(dur))\n}\n\nfunc newStatTx(b Tx) *statTx {\n\tw := &statTx{\n\t\tb: b,\n\n\t\t// For now, just track call stats globally.\n\t\t// But this could be made per-Tx by making this be stats: newCallStats(),\n\t\t// for example.\n\t\tstats: globalCallStats,\n\t}\n\treturn w\n}\n\ntype kall int\n\n// constants for kall argument to callStats.add()\nconst (\n\tkNewTxIterator kall = iota\n\tkImportRoaringBits\n\tkRollback\n\tkCommit\n\tkRoaringBitmap\n\tkContainer\n\tkPutContainer\n\tkRemoveContainer\n\tkAdd\n\tkRemove\n\tkRemoved\n\tkContains\n\tkContainerIterator\n\tkCount\n\tkMax\n\tkMin\n\tkCountRange\n\tkOffsetRange\n\tkLast // mark the end, always keep this last. The following aren't tracked atm:\n\tkType\n)\n\nfunc (k kall) String() string {\n\tswitch k {\n\tcase kNewTxIterator:\n\t\treturn \"kNewTxIterator\"\n\tcase kImportRoaringBits:\n\t\treturn \"kImportRoaringBits\"\n\tcase kRollback:\n\t\treturn \"kRollback\"\n\tcase kCommit:\n\t\treturn \"kCommit\"\n\tcase kRoaringBitmap:\n\t\treturn \"kRoaringBitmap\"\n\tcase kContainer:\n\t\treturn \"kContainer\"\n\tcase kPutContainer:\n\t\treturn \"kPutContainer\"\n\tcase kRemoveContainer:\n\t\treturn \"kRemoveContainer\"\n\tcase kAdd:\n\t\treturn \"kAdd\"\n\tcase kRemove:\n\t\treturn \"kRemove\"\n\tcase kRemoved:\n\t\treturn \"kRemoved\"\n\tcase kContains:\n\t\treturn \"kContains\"\n\tcase kContainerIterator:\n\t\treturn \"kContainerIterator\"\n\tcase kCount:\n\t\treturn \"kCount\"\n\tcase kMax:\n\t\treturn \"kMax\"\n\tcase kMin:\n\t\treturn \"kMin\"\n\tcase kCountRange:\n\t\treturn \"kCountRange\"\n\tcase kOffsetRange:\n\t\treturn \"kOffsetRange\"\n\tcase kLast:\n\t\treturn \"kLast\"\n\tcase kType:\n\t\treturn \"kType\"\n\t}\n\tvprint.PanicOn(fmt.Sprintf(\"unknown kall '%v'\", int(k)))\n\treturn \"\"\n}\n\nvar _ Tx = (*statTx)(nil)\n\nfunc (c *statTx) ImportRoaringBits(index, field, view string, shard uint64, rit roaring.RoaringIterator, clear bool, log bool, rowSize uint64) (changed int, rowSet map[uint64]int, err error) {\n\tme := kImportRoaringBits\n\n\tt0 := time.Now()\n\tdefer func() {\n\t\tc.stats.add(me, time.Since(t0))\n\t}()\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see ImportRoaringBits() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.ImportRoaringBits(index, field, view, shard, rit, clear, log, rowSize)\n}\n\nfunc (c *statTx) Rollback() {\n\tme := kRollback\n\n\tt0 := time.Now()\n\tdefer func() {\n\t\tc.stats.add(me, time.Since(t0))\n\t}()\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see Rollback() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\tc.b.Rollback()\n}\n\nfunc (c *statTx) Commit() error {\n\tme := kCommit\n\n\tt0 := time.Now()\n\tdefer func() {\n\t\tc.stats.add(me, time.Since(t0))\n\t}()\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see Commit() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.Commit()\n}\n\nfunc (c *statTx) RoaringBitmap(index, field, view string, shard uint64) (*roaring.Bitmap, error) {\n\tme := kRoaringBitmap\n\n\tt0 := time.Now()\n\tdefer func() {\n\t\tc.stats.add(me, time.Since(t0))\n\t}()\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see RoaringBitmap() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.RoaringBitmap(index, field, view, shard)\n}\n\nfunc (c *statTx) Container(index, field, view string, shard uint64, key uint64) (ct *roaring.Container, err error) {\n\tme := kContainer\n\n\tt0 := time.Now()\n\tdefer func() {\n\t\tc.stats.add(me, time.Since(t0))\n\t}()\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see Container() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.Container(index, field, view, shard, key)\n}\n\nfunc (c *statTx) PutContainer(index, field, view string, shard uint64, key uint64, rc *roaring.Container) error {\n\tme := kPutContainer\n\n\tt0 := time.Now()\n\tdefer func() {\n\t\tc.stats.add(me, time.Since(t0))\n\t}()\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see PutContainer() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.PutContainer(index, field, view, shard, key, rc)\n}\n\nfunc (c *statTx) RemoveContainer(index, field, view string, shard uint64, key uint64) error {\n\tme := kRemoveContainer\n\n\tt0 := time.Now()\n\tdefer func() {\n\t\tc.stats.add(me, time.Since(t0))\n\t}()\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see RemoveContainer() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.RemoveContainer(index, field, view, shard, key)\n}\n\nfunc (c *statTx) Add(index, field, view string, shard uint64, a ...uint64) (changeCount int, err error) {\n\tme := kAdd\n\n\tt0 := time.Now()\n\tdefer func() {\n\t\tc.stats.add(me, time.Since(t0))\n\t}()\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see Add() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.Add(index, field, view, shard, a...)\n}\n\nfunc (c *statTx) Remove(index, field, view string, shard uint64, a ...uint64) (changeCount int, err error) {\n\tme := kRemove\n\n\tt0 := time.Now()\n\tdefer func() {\n\t\tc.stats.add(me, time.Since(t0))\n\t}()\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see Remove() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.Remove(index, field, view, shard, a...)\n}\n\nfunc (c *statTx) Removed(index, field, view string, shard uint64, a ...uint64) (changed []uint64, err error) {\n\tme := kRemoved\n\n\tt0 := time.Now()\n\tdefer func() {\n\t\tc.stats.add(me, time.Since(t0))\n\t}()\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see Removed() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.Removed(index, field, view, shard, a...)\n}\n\nfunc (c *statTx) Contains(index, field, view string, shard uint64, key uint64) (exists bool, err error) {\n\tme := kContains\n\n\tt0 := time.Now()\n\tdefer func() {\n\t\tc.stats.add(me, time.Since(t0))\n\t}()\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see Contains() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.Contains(index, field, view, shard, key)\n}\n\nfunc (c *statTx) ContainerIterator(index, field, view string, shard uint64, firstRoaringContainerKey uint64) (citer roaring.ContainerIterator, found bool, err error) {\n\tme := kContainerIterator\n\n\tt0 := time.Now()\n\tdefer func() {\n\t\tc.stats.add(me, time.Since(t0))\n\t}()\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see ContainerIterator() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.ContainerIterator(index, field, view, shard, firstRoaringContainerKey)\n}\n\nfunc (c *statTx) ApplyFilter(index, field, view string, shard uint64, ckey uint64, filter roaring.BitmapFilter) (err error) {\n\treturn GenericApplyFilter(c, index, field, view, shard, ckey, filter)\n}\n\nfunc (c *statTx) ApplyRewriter(index, field, view string, shard uint64, ckey uint64, filter roaring.BitmapRewriter) (err error) {\n\treturn c.b.ApplyRewriter(index, field, view, shard, ckey, filter)\n}\n\nfunc (c *statTx) Count(index, field, view string, shard uint64) (uint64, error) {\n\tme := kCount\n\n\tt0 := time.Now()\n\tdefer func() {\n\t\tc.stats.add(me, time.Since(t0))\n\t}()\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see Count() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.Count(index, field, view, shard)\n}\n\nfunc (c *statTx) Max(index, field, view string, shard uint64) (uint64, error) {\n\tme := kMax\n\n\tt0 := time.Now()\n\tdefer func() {\n\t\tc.stats.add(me, time.Since(t0))\n\t}()\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see Max() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.Max(index, field, view, shard)\n}\n\nfunc (c *statTx) Min(index, field, view string, shard uint64) (uint64, bool, error) {\n\tme := kMin\n\n\tt0 := time.Now()\n\tdefer func() {\n\t\tc.stats.add(me, time.Since(t0))\n\t}()\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see Min() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.Min(index, field, view, shard)\n}\n\nfunc (c *statTx) CountRange(index, field, view string, shard uint64, start, end uint64) (n uint64, err error) {\n\tme := kCountRange\n\n\tt0 := time.Now()\n\tdefer func() {\n\t\tc.stats.add(me, time.Since(t0))\n\t}()\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see CountRange() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.CountRange(index, field, view, shard, start, end)\n}\n\nfunc (c *statTx) OffsetRange(index, field, view string, shard, offset, start, end uint64) (other *roaring.Bitmap, err error) {\n\tme := kOffsetRange\n\tt0 := time.Now()\n\tdefer func() {\n\t\tc.stats.add(me, time.Since(t0))\n\t}()\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tvprint.AlwaysPrintf(\"see OffsetRange() PanicOn '%v' at '%v'\", r, vprint.Stack())\n\t\t\tvprint.PanicOn(r)\n\t\t}\n\t}()\n\treturn c.b.OffsetRange(index, field, view, shard, offset, start, end)\n}\n\nfunc (c *statTx) Type() string {\n\treturn c.b.Type()\n}\n\nfunc (c *statTx) GetSortedFieldViewList(idx *Index, shard uint64) (fvs []txkey.FieldView, err error) {\n\treturn c.b.GetSortedFieldViewList(idx, shard)\n}\n\nfunc (tx *statTx) GetFieldSizeBytes(index, field string) (uint64, error) {\n\treturn 0, nil\n}\n"
        },
        {
          "name": "storage",
          "type": "tree",
          "content": null
        },
        {
          "name": "systemlayer.go",
          "type": "blob",
          "size": 2.732421875,
          "content": "package pilosa\n\nimport (\n\t\"strings\"\n\t\"time\"\n)\n\n// ExecutionRequest holds data about an (sql) execution request\ntype ExecutionRequest struct {\n\t// the id of the request\n\tRequestID string\n\t// the id of the user\n\tUserID string\n\t// time the request started\n\tStartTime time.Time\n\t// time the request finished - zero iif it has not finished\n\tEndTime time.Time\n\t// status of the request 'running' or 'complete' now, could have other values later\n\tStatus string\n\t// future: if the request is waiting, the type of wait that is occuring\n\tWaitType string\n\t// future: the cumulative wait time for this request\n\tWaitTime time.Duration\n\t// futuure: if the request is waiting, the thing it is waiting on\n\tWaitResource string\n\t// future: the cululative cpu time for this request\n\tCPUTime time.Duration\n\t// the elapsed time for this request\n\tElapsedTime time.Duration\n\t// future: the cumulative number of physical reads for this request\n\tReads int64\n\t// future: the cumulative number of physical writes for this request\n\tWrites int64\n\t// future: the cumulative number of logical reads for this request\n\tLogicalReads int64\n\t// future: the cumulative number of rows affected for this request\n\tRowCount int64\n\t// the query plan for this request formatted in json\n\tPlan string\n\t// the sql for this request\n\tSQL string\n}\n\n// Copy returns a copy of the ExecutionRequest passed\nfunc (e *ExecutionRequest) Copy() ExecutionRequest {\n\tvar elapsedTime time.Duration\n\tif !strings.EqualFold(e.Status, \"complete\") {\n\t\telapsedTime = time.Since(e.StartTime)\n\t} else {\n\t\telapsedTime = e.EndTime.Sub(e.StartTime)\n\t}\n\n\treturn ExecutionRequest{\n\t\tRequestID:    e.RequestID,\n\t\tUserID:       e.UserID,\n\t\tStartTime:    e.StartTime,\n\t\tEndTime:      e.EndTime,\n\t\tStatus:       e.Status,\n\t\tWaitType:     e.WaitType,\n\t\tWaitTime:     e.WaitTime,\n\t\tWaitResource: e.WaitResource,\n\t\tCPUTime:      e.CPUTime,\n\t\tElapsedTime:  elapsedTime,\n\t\tSQL:          e.SQL,\n\t\tPlan:         e.Plan,\n\t}\n}\n\n// ExecutionRequestsAPI defines the API for storing, updating and querying internal state\n// around (sql) execution requests\ntype ExecutionRequestsAPI interface {\n\t// add a request\n\tAddRequest(requestID string, userID string, startTime time.Time, sql string) error\n\n\t// update a request\n\tUpdateRequest(requestID string,\n\t\tendTime time.Time,\n\t\tstatus string,\n\t\twaitType string,\n\t\twaitTime time.Duration,\n\t\twaitResource string,\n\t\tcpuTime time.Duration,\n\t\treads int64,\n\t\twrites int64,\n\t\tlogicalReads int64,\n\t\trowCount int64,\n\t\tplan string) error\n\n\t// list all the requests\n\tListRequests() ([]ExecutionRequest, error)\n\n\t// get a specific request\n\tGetRequest(requestID string) (ExecutionRequest, error)\n}\n\n// SystemLayerAPI defines an api to allow access to internal FeatureBase state\ntype SystemLayerAPI interface {\n\tExecutionRequests() ExecutionRequestsAPI\n}\n"
        },
        {
          "name": "systemlayer",
          "type": "tree",
          "content": null
        },
        {
          "name": "syswrap",
          "type": "tree",
          "content": null
        },
        {
          "name": "task",
          "type": "tree",
          "content": null
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "testdata",
          "type": "tree",
          "content": null
        },
        {
          "name": "testhook",
          "type": "tree",
          "content": null
        },
        {
          "name": "time.go",
          "type": "blob",
          "size": 15.79296875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n)\n\n// ErrInvalidTimeQuantum is returned when parsing a time quantum.\nvar ErrInvalidTimeQuantum = errors.New(\"invalid time quantum\")\n\n// TimeQuantum represents a time granularity for time-based bitmaps.\ntype TimeQuantum string\n\n// HasYear returns true if the quantum contains a 'Y' unit.\nfunc (q TimeQuantum) HasYear() bool { return strings.ContainsRune(string(q), 'Y') }\n\n// HasMonth returns true if the quantum contains a 'M' unit.\nfunc (q TimeQuantum) HasMonth() bool { return strings.ContainsRune(string(q), 'M') }\n\n// HasDay returns true if the quantum contains a 'D' unit.\nfunc (q TimeQuantum) HasDay() bool { return strings.ContainsRune(string(q), 'D') }\n\n// HasHour returns true if the quantum contains a 'H' unit.\nfunc (q TimeQuantum) HasHour() bool { return strings.ContainsRune(string(q), 'H') }\n\n// IsEmpty returns true if the quantum is empty.\nfunc (q TimeQuantum) IsEmpty() bool { return string(q) == \"\" }\n\nfunc (q TimeQuantum) Granularity() rune {\n\tvar g rune\n\tfor _, g = range q {\n\t}\n\treturn g\n}\n\n// Valid returns true if q is a valid time quantum value.\nfunc (q TimeQuantum) Valid() bool {\n\tswitch q {\n\tcase \"Y\", \"YM\", \"YMD\", \"YMDH\",\n\t\t\"M\", \"MD\", \"MDH\",\n\t\t\"D\", \"DH\",\n\t\t\"H\",\n\t\t\"\":\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// The following methods are required to implement pflag Value interface.\n\n// Set sets the time quantum value.\nfunc (q *TimeQuantum) Set(value string) error {\n\t*q = TimeQuantum(value)\n\treturn nil\n}\n\nfunc (q TimeQuantum) String() string {\n\treturn string(q)\n}\n\n// Type returns the type of a time quantum value.\nfunc (q TimeQuantum) Type() string {\n\treturn \"TimeQuantum\"\n}\n\n// viewByTimeUnit returns the view name for time with a given quantum unit.\nfunc viewByTimeUnit(name string, t time.Time, unit rune) string {\n\tswitch unit {\n\tcase 'Y':\n\t\treturn fmt.Sprintf(\"%s_%s\", name, t.Format(\"2006\"))\n\tcase 'M':\n\t\treturn fmt.Sprintf(\"%s_%s\", name, t.Format(\"200601\"))\n\tcase 'D':\n\t\treturn fmt.Sprintf(\"%s_%s\", name, t.Format(\"20060102\"))\n\tcase 'H':\n\t\treturn fmt.Sprintf(\"%s_%s\", name, t.Format(\"2006010215\"))\n\tdefault:\n\t\treturn \"\"\n\t}\n}\n\n// YYYYMMDDHH lengths. Note that this is a []int, not a map[byte]int, so\n// the lookups can be cheaper.\nvar lengthsByQuantum = []int{\n\t'Y': 4,\n\t'M': 6,\n\t'D': 8,\n\t'H': 10,\n}\n\n// viewsByTimeInto computes the list of views for a given time. It expects\n// to be given an initial buffer of the form `name_YYYYMMDDHH`, and a slice\n// of []bytes. This allows us to reuse the buffer for all the sub-buffers,\n// and also to reuse the slice of slices, to eliminate all those allocations.\n// This might seem crazy, but even including the JSON parsing and all the\n// disk activity, the straightforward viewsByTime implementation was 25%\n// of runtime in an ingest test.\nfunc viewsByTimeInto(fullBuf []byte, into [][]byte, t time.Time, q TimeQuantum) [][]byte {\n\tl := len(fullBuf) - 10\n\tdate := fullBuf[l : l+10]\n\ty, m, d := t.Date()\n\th := t.Hour()\n\t// Did you know that Sprintf, Printf, and other things like that all\n\t// do allocations, and that doing allocations in a tight loop like this\n\t// is stunningly expensive? viewsByTime was 25% of an ingest test's\n\t// total CPU, not counting the garbage collector overhead. This is about\n\t// 3%. No, I'm not totally sure that justifies it.\n\tif y < 1000 {\n\t\tys := fmt.Sprintf(\"%04d\", y)\n\t\tcopy(date[0:4], []byte(ys))\n\t} else if y >= 10000 {\n\t\t// This is probably a bad answer but there isn't really a\n\t\t// good answer.\n\t\tys := fmt.Sprintf(\"%04d\", y%1000)\n\t\tcopy(date[0:4], []byte(ys))\n\t} else {\n\t\tstrconv.AppendInt(date[:0], int64(y), 10)\n\t}\n\tdate[4] = '0' + byte(m/10)\n\tdate[5] = '0' + byte(m%10)\n\tdate[6] = '0' + byte(d/10)\n\tdate[7] = '0' + byte(d%10)\n\tdate[8] = '0' + byte(h/10)\n\tdate[9] = '0' + byte(h%10)\n\tinto = into[:0]\n\tfor _, unit := range q {\n\t\tif int(unit) < len(lengthsByQuantum) && lengthsByQuantum[unit] != 0 {\n\t\t\tinto = append(into, fullBuf[:l+lengthsByQuantum[unit]])\n\t\t}\n\t}\n\treturn into\n}\n\n// viewsByTime returns a list of views for a given timestamp.\nfunc viewsByTime(name string, t time.Time, q TimeQuantum) []string { // nolint: unparam\n\ty, m, d := t.Date()\n\th := t.Hour()\n\tfull := fmt.Sprintf(\"%s_%04d%02d%02d%02d\", name, y, m, d, h)\n\tl := len(name) + 1\n\ta := make([]string, 0, len(q))\n\tfor _, unit := range q {\n\t\tif int(unit) < len(lengthsByQuantum) && lengthsByQuantum[unit] != 0 {\n\t\t\ta = append(a, full[:l+lengthsByQuantum[unit]])\n\t\t}\n\t}\n\treturn a\n}\n\n// viewsByTimeRange returns a list of views to traverse to query a time range.\nfunc viewsByTimeRange(name string, start, end time.Time, q TimeQuantum) []string { // nolint: unparam\n\tt := start\n\n\t// Save flags for performance.\n\thasYear := q.HasYear()\n\thasMonth := q.HasMonth()\n\thasDay := q.HasDay()\n\thasHour := q.HasHour()\n\n\tvar results []string\n\n\t// Walk up from smallest units to largest units.\n\tif hasHour || hasDay || hasMonth {\n\t\tfor t.Before(end) {\n\t\t\tif hasHour {\n\t\t\t\tif !nextDayGTE(t, end) {\n\t\t\t\t\tbreak\n\t\t\t\t} else if t.Hour() != 0 {\n\t\t\t\t\tresults = append(results, viewByTimeUnit(name, t, 'H'))\n\t\t\t\t\tt = t.Add(time.Hour)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t}\n\n\t\t\tif hasDay {\n\t\t\t\tif !nextMonthGTE(t, end) {\n\t\t\t\t\tbreak\n\t\t\t\t} else if t.Day() != 1 {\n\t\t\t\t\tresults = append(results, viewByTimeUnit(name, t, 'D'))\n\t\t\t\t\tt = t.AddDate(0, 0, 1)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif hasMonth {\n\t\t\t\tif !nextYearGTE(t, end) {\n\t\t\t\t\tbreak\n\t\t\t\t} else if t.Month() != 1 {\n\t\t\t\t\tresults = append(results, viewByTimeUnit(name, t, 'M'))\n\t\t\t\t\tt = addMonth(t)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// If a unit exists but isn't set and there are no larger units\n\t\t\t// available then we need to exit the loop because we are no longer\n\t\t\t// making progress.\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// Walk back down from largest units to smallest units.\n\tfor t.Before(end) {\n\t\tif hasYear && nextYearGTE(t, end) {\n\t\t\tresults = append(results, viewByTimeUnit(name, t, 'Y'))\n\t\t\tt = t.AddDate(1, 0, 0)\n\t\t} else if hasMonth && nextMonthGTE(t, end) {\n\t\t\tresults = append(results, viewByTimeUnit(name, t, 'M'))\n\t\t\tt = addMonth(t)\n\t\t} else if hasDay && nextDayGTE(t, end) {\n\t\t\tresults = append(results, viewByTimeUnit(name, t, 'D'))\n\t\t\tt = t.AddDate(0, 0, 1)\n\t\t} else if hasHour {\n\t\t\tresults = append(results, viewByTimeUnit(name, t, 'H'))\n\t\t\tt = t.Add(time.Hour)\n\t\t} else {\n\t\t\tbreak\n\t\t}\n\t}\n\n\treturn results\n}\n\n// addMonth adds a month similar to time.AddDate(0, 1, 0), but\n// in certain edge cases it doesn't normalize for days late in the month.\n// In the \"YM\" case where t.Day is greater than 28, there are\n// edge cases where using time.AddDate() to add a month will result\n// in two \"months\" being added (Jan 31 + 1mo = March 2).\nfunc addMonth(t time.Time) time.Time {\n\tif t.Day() > 28 {\n\t\tt = time.Date(t.Year(), t.Month(), 1, t.Hour(), 0, 0, 0, t.Location())\n\t}\n\tt = t.AddDate(0, 1, 0)\n\treturn t\n}\n\nfunc nextYearGTE(t time.Time, end time.Time) bool {\n\tnext := t.AddDate(1, 0, 0)\n\tif next.Year() == end.Year() {\n\t\treturn true\n\t}\n\treturn end.After(next)\n}\n\nfunc nextMonthGTE(t time.Time, end time.Time) bool {\n\tnext := t.AddDate(0, 1, 0)\n\ty1, m1, _ := next.Date()\n\ty2, m2, _ := end.Date()\n\tif (y1 == y2) && (m1 == m2) {\n\t\treturn true\n\t}\n\treturn end.After(next)\n}\n\nfunc nextDayGTE(t time.Time, end time.Time) bool {\n\tnext := t.AddDate(0, 0, 1)\n\ty1, m1, d1 := next.Date()\n\ty2, m2, d2 := end.Date()\n\tif (y1 == y2) && (m1 == m2) && (d1 == d2) {\n\t\treturn true\n\t}\n\treturn end.After(next)\n}\n\n// parseTime parses a string or int64 into a time.Time value.\nfunc parseTime(t interface{}) (time.Time, error) {\n\tvar err error\n\tvar calcTime time.Time\n\tswitch v := t.(type) {\n\tcase string:\n\t\tif calcTime, err = time.Parse(TimeFormat, v); err != nil {\n\t\t\t// if the default parsing fails, check if user tried to\n\t\t\t// supply partial time eg year and month\n\t\t\tcalcTime, err = parsePartialTime(v)\n\t\t\treturn calcTime, err\n\t\t}\n\tcase int64:\n\t\tcalcTime = time.Unix(v, 0).UTC()\n\tdefault:\n\t\treturn time.Time{}, errors.New(\"arg must be a timestamp\")\n\t}\n\treturn calcTime, nil\n}\n\n// parsePartialTime parses strings where the time provided is only partial\n// eg given 2006-02, it extracts the year and month and the rest of the\n// components are set to the default values. The time must have the format\n// used in parseTime. The year must be present. The rest of the components are\n// optional but if a component is present in the input, this implies that all\n// the preceding components are also specified. For example, if the hour is provided\n// then the day, month and year must be present. This function could and should be\n// simplified\nfunc parsePartialTime(t string) (time.Time, error) {\n\t// helper parseCustomHourMinute parses strings of the form HH:MM to\n\t// hour and minute component\n\tparseHourMinute := func(t string) (hour, minute int, err error) {\n\t\t// time should have the format HH:MM\n\t\tsubStrings := strings.Split(t, \":\")\n\t\tswitch len(subStrings) {\n\t\tcase 2:\n\t\t\t// has minutes\n\t\t\tminute, err = strconv.Atoi(subStrings[1])\n\t\t\tif err != nil {\n\t\t\t\treturn -1, -1, errors.New(\"invalid time\")\n\t\t\t}\n\t\t\tfallthrough\n\t\tcase 1:\n\t\t\thour, err = strconv.Atoi(subStrings[0])\n\t\t\tif err != nil {\n\t\t\t\treturn -1, -1, errors.New(\"invalid time\")\n\t\t\t}\n\t\tdefault:\n\t\t\treturn -1, -1, errors.New(\"invalid time\")\n\t\t}\n\n\t\treturn\n\t}\n\t// helper trim function\n\ttrim := func(subMatches []string) (filtered []string, err error) {\n\t\trestAreEmpty := func(ss []string) bool {\n\t\t\tfor _, s := range ss {\n\t\t\t\tif s != \"\" {\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn true\n\t\t}\n\t\tif len(subMatches) <= 1 {\n\t\t\treturn nil, errors.New(\"invalid time\")\n\t\t}\n\t\t// ignore full match which is at index 0\n\t\tsubMatches = subMatches[1:] // ignore full match which is at index 0\n\t\tfor i, s := range subMatches {\n\t\t\tif s != \"\" {\n\t\t\t\tif i > 0 {\n\t\t\t\t\ts = s[1:] // remove preceding hyphen or T\n\t\t\t\t}\n\t\t\t\tfiltered = append(filtered, s)\n\t\t\t} else {\n\t\t\t\t// rest must be empty for date-time to be valid\n\t\t\t\tif !restAreEmpty(subMatches[i:]) {\n\t\t\t\t\treturn nil, errors.New(\"invalid date-time\")\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\treturn filtered, nil\n\t}\n\n\tvar errInvalidTime error = errors.New(\"cannot parse string time\")\n\tvar regex = regexp.MustCompile(`^(\\d{4})(-\\d{2})?(-\\d{2})?(T.+)?$`)\n\tsubMatches := regex.FindStringSubmatch(t)\n\tsubMatches, err := trim(subMatches)\n\tif err != nil {\n\t\treturn time.Time{}, errInvalidTime\n\t}\n\t// defaults\n\tvar (\n\t\tyr    int\n\t\tmonth = time.January\n\t\tday   = 1\n\t\thour  = 0\n\t\tmin   = 0\n\t)\n\t// year must be set, the rest are optional\n\tswitch len(subMatches) {\n\tcase 4:\n\t\t// time\n\t\thour, min, err = parseHourMinute(subMatches[3])\n\t\tif err != nil {\n\t\t\treturn time.Time{}, errInvalidTime\n\t\t}\n\t\tfallthrough\n\tcase 3:\n\t\t// day\n\t\tday, err = strconv.Atoi(subMatches[2])\n\t\tif err != nil {\n\t\t\treturn time.Time{}, errInvalidTime\n\t\t}\n\t\tfallthrough\n\tcase 2:\n\t\t// month\n\t\tmonthNum, err := strconv.Atoi(subMatches[1])\n\t\tmonth = time.Month(monthNum)\n\t\tif err != nil {\n\t\t\treturn time.Time{}, errInvalidTime\n\t\t}\n\t\tfallthrough\n\tcase 1:\n\t\t// year\n\t\tyr, err = strconv.Atoi(subMatches[0])\n\t\tif err != nil {\n\t\t\treturn time.Time{}, errInvalidTime\n\t\t}\n\tdefault:\n\t\treturn time.Time{}, errInvalidTime\n\t}\n\treturn time.Date(yr, month, day, hour, min, 0, 0, time.UTC), nil\n}\n\n// minMaxViews returns the min and max view from a list of views\n// with a time quantum taken into consideration. It assumes that\n// all views represent the same base view name (the logic depends\n// on the views sorting correctly in alphabetical order).\nfunc minMaxViews(views []string, q TimeQuantum) (min string, max string) {\n\t// Sort the list of views.\n\tsort.Strings(views)\n\n\t// get the lowest granularity quantum available from the given views\n\tlowestQuantumFromViews := getLowestGranularityQuantum(views)\n\n\t/*\n\t\t- lowestQuantumFromViews was added because of a unique case where the view for least precise quantum was somehow deleted:\n\t\t\t- ex: q=\"YMDH\" but views only have \"DH\" (ex: std_20220531, std_2022053123)\n\t\t\t- without lowestQuantumFromViews, this function would return empty for min, max (as if there were no time views)\n\t\t\t\tbecause it would look for \"Y\" views but cant find it since it was deleted\n\t\t\t- with lowestQuantumFromViews, this function will look at the available quantum from views (ex: \"DH\")\n\t\t\t\tgrab the least precise quantum (\"D\")\n\t\t\t\tand use D (day) view (ex: std_20220531)\n\t\t- use lowestQuantumFromViews if\n\t\t1. lowestQuantumFromViews is not empty\n\t\t2. lowestQuantumFromViews is actually a substring of q\n\t\t - lowestQuantumFromViews has to be a substring of q because if q=\"Y\" but views quantum=\"DH\" (ex: std_20220531, std_2022053123),\n\t\t the \"DH\" quantum wont matter since originally q of \"Y\" didnt include \"DH\"\n\t*/\n\n\tif !(lowestQuantumFromViews.IsEmpty()) && strings.Contains(q.String(), lowestQuantumFromViews.String()) {\n\t\tq = lowestQuantumFromViews\n\t}\n\n\t// Determine the least precise quantum and set that as the\n\t// number of string characters to compare against.\n\tvar chars int\n\tif q.HasYear() {\n\t\tchars = 4\n\t} else if q.HasMonth() {\n\t\tchars = 6\n\t} else if q.HasDay() {\n\t\tchars = 8\n\t} else if q.HasHour() {\n\t\tchars = 10\n\t}\n\n\t// min: get the first view with the matching number of time chars.\n\tfor _, v := range views {\n\t\tif len(viewTimePart(v)) == chars {\n\t\t\tmin = v\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// max: get the first view (from the end) with the matching number of time chars.\n\tfor i := len(views) - 1; i >= 0; i-- {\n\t\tif len(viewTimePart(views[i])) == chars {\n\t\t\tmax = views[i]\n\t\t\tbreak\n\t\t}\n\t}\n\n\treturn min, max\n}\n\n// timeOfView returns a valid time.Time based on the view string.\n// For upper bound use, the result can be adjusted by one by setting\n// the `adj` argument to `true`.\nfunc timeOfView(v string, adj bool) (time.Time, error) {\n\tif v == \"\" {\n\t\treturn time.Time{}, nil\n\t}\n\n\tlayout := \"2006010215\"\n\ttimePart := viewTimePart(v)\n\n\tswitch len(timePart) {\n\tcase 4: // year\n\t\tt, err := time.Parse(layout[:4], timePart)\n\t\tif err != nil {\n\t\t\treturn time.Time{}, err\n\t\t}\n\t\tif adj {\n\t\t\tt = t.AddDate(1, 0, 0)\n\t\t}\n\t\treturn t, nil\n\tcase 6: // month\n\t\tt, err := time.Parse(layout[:6], timePart)\n\t\tif err != nil {\n\t\t\treturn time.Time{}, err\n\t\t}\n\t\tif adj {\n\t\t\tt = addMonth(t)\n\t\t}\n\t\treturn t, nil\n\tcase 8: // day\n\t\tt, err := time.Parse(layout[:8], timePart)\n\t\tif err != nil {\n\t\t\treturn time.Time{}, err\n\t\t}\n\t\tif adj {\n\t\t\tt = t.AddDate(0, 0, 1)\n\t\t}\n\t\treturn t, nil\n\tcase 10: // hour\n\t\tt, err := time.Parse(layout[:10], timePart)\n\t\tif err != nil {\n\t\t\treturn time.Time{}, err\n\t\t}\n\t\tif adj {\n\t\t\tt = t.Add(time.Hour)\n\t\t}\n\t\treturn t, nil\n\t}\n\n\treturn time.Time{}, fmt.Errorf(\"invalid time format on view: %s\", v)\n}\n\n// viewTimePart returns the time portion of a string view name.\n// e.g. the view \"string_201901\" would return \"201901\".\nfunc viewTimePart(v string) string {\n\tparts := strings.Split(v, \"_\")\n\tif _, err := strconv.Atoi(parts[len(parts)-1]); err != nil {\n\t\t// it's not a number!\n\t\treturn \"\"\n\t}\n\treturn parts[len(parts)-1]\n}\n\n// getLowestGranularityQuantum returns lowest granularity quantum from a list of views\n// e.g.\n//\n//\t[std_2001, std_200102, std_20010203, std_2001020304] - returns \"Y\" since year is the lowest granularity\n//\t[std_2001020304, std_200102, std_20010203] - returns \"M\", the order of views should not affect lowest granularity\nfunc getLowestGranularityQuantum(views []string) TimeQuantum {\n\n\t// Time quantum with the highest level of granularity we support\n\ttimeQuantum := \"YMDH\"\n\n\twrite_Y := false\n\twrite_M := false\n\twrite_D := false\n\twrite_H := false\n\tfor _, v := range views {\n\t\tviewTime := viewTimePart(v)\n\t\tif viewTime != \"\" {\n\t\t\tif len(viewTime) == 4 {\n\t\t\t\tif !write_Y {\n\t\t\t\t\twrite_Y = true\n\t\t\t\t}\n\t\t\t} else if len(viewTime) == 6 {\n\t\t\t\tif !write_M {\n\t\t\t\t\twrite_M = true\n\t\t\t\t}\n\t\t\t} else if len(viewTime) == 8 {\n\t\t\t\tif !write_D {\n\t\t\t\t\twrite_D = true\n\t\t\t\t}\n\t\t\t} else if len(viewTime) == 10 {\n\t\t\t\tif !write_H {\n\t\t\t\t\twrite_H = true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tlowestGranularity := \"\"\n\tif write_Y {\n\t\t// Y\n\t\tlowestGranularity = timeQuantum[:1]\n\t} else if !write_Y && write_M {\n\t\t// M\n\t\tlowestGranularity = timeQuantum[1:2]\n\t} else if !write_Y && !write_M && write_D {\n\t\t// D\n\t\tlowestGranularity = timeQuantum[2:3]\n\t} else if !write_Y && !write_M && !write_D && write_H {\n\t\t// H\n\t\tlowestGranularity = timeQuantum[3:4]\n\t}\n\n\treturn TimeQuantum(lowestGranularity)\n}\n"
        },
        {
          "name": "time_internal_test.go",
          "type": "blob",
          "size": 15.7587890625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"reflect\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n)\n\n// Ensure string can be parsed into time quantum.\nfunc TestParseTimeQuantum(t *testing.T) {\n\tt.Run(\"OK\", func(t *testing.T) {\n\t\tif q, err := parseTimeQuantum(\"YMDH\"); err != nil {\n\t\t\tt.Fatalf(\"unexpected error: %s\", err)\n\t\t} else if q != TimeQuantum(\"YMDH\") {\n\t\t\tt.Fatalf(\"unexpected quantum: %#v\", q)\n\t\t}\n\t})\n\n\tt.Run(\"ErrInvalidTimeQuantum\", func(t *testing.T) {\n\t\tif _, err := parseTimeQuantum(\"BADQUANTUM\"); err != ErrInvalidTimeQuantum {\n\t\t\tt.Fatalf(\"unexpected error: %s\", err)\n\t\t}\n\t})\n}\n\n// Ensure generated view name can be returned for a given time unit.\nfunc TestViewByTimeUnit(t *testing.T) {\n\tts := time.Date(2000, time.January, 2, 3, 4, 5, 6, time.UTC)\n\n\tt.Run(\"Y\", func(t *testing.T) {\n\t\tif s := viewByTimeUnit(\"F\", ts, 'Y'); s != \"F_2000\" {\n\t\t\tt.Fatalf(\"unexpected name: %s\", s)\n\t\t}\n\t})\n\tt.Run(\"M\", func(t *testing.T) {\n\t\tif s := viewByTimeUnit(\"F\", ts, 'M'); s != \"F_200001\" {\n\t\t\tt.Fatalf(\"unexpected name: %s\", s)\n\t\t}\n\t})\n\tt.Run(\"D\", func(t *testing.T) {\n\t\tif s := viewByTimeUnit(\"F\", ts, 'D'); s != \"F_20000102\" {\n\t\t\tt.Fatalf(\"unexpected name: %s\", s)\n\t\t}\n\t})\n\tt.Run(\"H\", func(t *testing.T) {\n\t\tif s := viewByTimeUnit(\"F\", ts, 'H'); s != \"F_2000010203\" {\n\t\t\tt.Fatalf(\"unexpected name: %s\", s)\n\t\t}\n\t})\n}\n\n// Ensure all applicable field names can be generated when mutating a time bit.\nfunc TestViewsByTime(t *testing.T) {\n\tts := time.Date(2000, time.January, 2, 3, 4, 5, 6, time.UTC)\n\n\tt.Run(\"YMDH\", func(t *testing.T) {\n\t\ta := viewsByTime(\"F\", ts, mustParseTimeQuantum(\"YMDH\"))\n\t\tif !reflect.DeepEqual(a, []string{\"F_2000\", \"F_200001\", \"F_20000102\", \"F_2000010203\"}) {\n\t\t\tt.Fatalf(\"unexpected names: %+v\", a)\n\t\t}\n\t})\n\n\tt.Run(\"D\", func(t *testing.T) {\n\t\ta := viewsByTime(\"F\", ts, mustParseTimeQuantum(\"D\"))\n\t\tif !reflect.DeepEqual(a, []string{\"F_20000102\"}) {\n\t\t\tt.Fatalf(\"unexpected names: %+v\", a)\n\t\t}\n\t})\n}\n\nfunc TestViewsByTimeInto(t *testing.T) {\n\tts := time.Date(2000, time.January, 2, 3, 4, 5, 6, time.UTC)\n\ts := []byte(\"F_YYYYMMDDHH\")\n\tvar timeViews [][]byte\n\n\tt.Run(\"YMDH\", func(t *testing.T) {\n\t\ta := viewsByTime(\"F\", ts, mustParseTimeQuantum(\"YMDH\"))\n\t\tb := viewsByTimeInto(s, timeViews, ts, mustParseTimeQuantum(\"YMDH\"))\n\t\tif len(a) != len(b) {\n\t\t\tt.Fatalf(\"mismatch: viewsByTime: %q, viewsByTimeInto: %q\", a, b)\n\t\t}\n\t\tfor i := range a {\n\t\t\tif a[i] != string(b[i]) {\n\t\t\t\tt.Fatalf(\"mismatch: viewsByTime: %q, viewsByTimeInto: %q\", a, b)\n\t\t\t}\n\t\t}\n\t})\n\n\tt.Run(\"D\", func(t *testing.T) {\n\t\ta := viewsByTime(\"F\", ts, mustParseTimeQuantum(\"D\"))\n\t\tb := viewsByTimeInto(s, timeViews, ts, mustParseTimeQuantum(\"D\"))\n\t\tif len(a) != len(b) {\n\t\t\tt.Fatalf(\"mismatch: viewsByTime: %q, viewsByTimeInto: %q\", a, b)\n\t\t}\n\t\tfor i := range a {\n\t\t\tif a[i] != string(b[i]) {\n\t\t\t\tt.Fatalf(\"mismatch: viewsByTime: %q, viewsByTimeInto: %q\", a, b)\n\t\t\t}\n\t\t}\n\t})\n}\n\n// Ensure sets of fields can be returned for a given time range.\nfunc TestViewsByTimeRange(t *testing.T) {\n\tt.Run(\"Y\", func(t *testing.T) {\n\t\ta := viewsByTimeRange(\"F\", mustParseTime(\"2000-01-01 00:00\"), mustParseTime(\"2002-01-01 00:00\"), mustParseTimeQuantum(\"Y\"))\n\t\tif !reflect.DeepEqual(a, []string{\"F_2000\", \"F_2001\"}) {\n\t\t\tt.Fatalf(\"unexpected fields: %#v\", a)\n\t\t}\n\t})\n\tt.Run(\"YM\", func(t *testing.T) {\n\t\ta := viewsByTimeRange(\"F\", mustParseTime(\"2000-11-01 00:00\"), mustParseTime(\"2003-03-01 00:00\"), mustParseTimeQuantum(\"YM\"))\n\t\tif !reflect.DeepEqual(a, []string{\"F_200011\", \"F_200012\", \"F_2001\", \"F_2002\", \"F_200301\", \"F_200302\"}) {\n\t\t\tt.Fatalf(\"unexpected fields: %#v\", a)\n\t\t}\n\t})\n\tt.Run(\"YM31up\", func(t *testing.T) {\n\t\ta := viewsByTimeRange(\"F\", mustParseTime(\"2001-10-31 00:00\"), mustParseTime(\"2003-04-01 00:00\"), mustParseTimeQuantum(\"YM\"))\n\t\tif !reflect.DeepEqual(a, []string{\"F_200110\", \"F_200111\", \"F_200112\", \"F_2002\", \"F_200301\", \"F_200302\", \"F_200303\"}) {\n\t\t\tt.Fatalf(\"unexpected fields: %#v\", a)\n\t\t}\n\t})\n\tt.Run(\"YM31mid\", func(t *testing.T) {\n\t\ta := viewsByTimeRange(\"F\", mustParseTime(\"1999-12-31 00:00\"), mustParseTime(\"2000-04-01 00:00\"), mustParseTimeQuantum(\"YM\"))\n\t\tif !reflect.DeepEqual(a, []string{\"F_199912\", \"F_200001\", \"F_200002\", \"F_200003\"}) {\n\t\t\tt.Fatalf(\"unexpected fields: %#v\", a)\n\t\t}\n\t})\n\tt.Run(\"YM31down\", func(t *testing.T) {\n\t\ta := viewsByTimeRange(\"F\", mustParseTime(\"2000-01-31 00:00\"), mustParseTime(\"2001-04-01 00:00\"), mustParseTimeQuantum(\"YM\"))\n\t\tif !reflect.DeepEqual(a, []string{\"F_2000\", \"F_200101\", \"F_200102\", \"F_200103\"}) {\n\t\t\tt.Fatalf(\"unexpected fields: %#v\", a)\n\t\t}\n\t})\n\tt.Run(\"YMD\", func(t *testing.T) {\n\t\ta := viewsByTimeRange(\"F\", mustParseTime(\"2000-11-28 00:00\"), mustParseTime(\"2003-03-02 00:00\"), mustParseTimeQuantum(\"YMD\"))\n\t\tif !reflect.DeepEqual(a, []string{\"F_20001128\", \"F_20001129\", \"F_20001130\", \"F_200012\", \"F_2001\", \"F_2002\", \"F_200301\", \"F_200302\", \"F_20030301\"}) {\n\t\t\tt.Fatalf(\"unexpected fields: %#v\", a)\n\t\t}\n\t})\n\tt.Run(\"YMDH\", func(t *testing.T) {\n\t\ta := viewsByTimeRange(\"F\", mustParseTime(\"2000-11-28 22:00\"), mustParseTime(\"2002-03-01 03:00\"), mustParseTimeQuantum(\"YMDH\"))\n\t\tif !reflect.DeepEqual(a, []string{\"F_2000112822\", \"F_2000112823\", \"F_20001129\", \"F_20001130\", \"F_200012\", \"F_2001\", \"F_200201\", \"F_200202\", \"F_2002030100\", \"F_2002030101\", \"F_2002030102\"}) {\n\t\t\tt.Fatalf(\"unexpected fields: %#v\", a)\n\t\t}\n\t})\n\tt.Run(\"M\", func(t *testing.T) {\n\t\ta := viewsByTimeRange(\"F\", mustParseTime(\"2000-01-01 00:00\"), mustParseTime(\"2000-03-01 00:00\"), mustParseTimeQuantum(\"M\"))\n\t\tif !reflect.DeepEqual(a, []string{\"F_200001\", \"F_200002\"}) {\n\t\t\tt.Fatalf(\"unexpected fields: %#v\", a)\n\t\t}\n\t})\n\tt.Run(\"MD\", func(t *testing.T) {\n\t\ta := viewsByTimeRange(\"F\", mustParseTime(\"2000-11-29 00:00\"), mustParseTime(\"2002-02-03 00:00\"), mustParseTimeQuantum(\"MD\"))\n\t\tif !reflect.DeepEqual(a, []string{\"F_20001129\", \"F_20001130\", \"F_200012\", \"F_200101\", \"F_200102\", \"F_200103\", \"F_200104\", \"F_200105\", \"F_200106\", \"F_200107\", \"F_200108\", \"F_200109\", \"F_200110\", \"F_200111\", \"F_200112\", \"F_200201\", \"F_20020201\", \"F_20020202\"}) {\n\t\t\tt.Fatalf(\"unexpected fields: %#v\", a)\n\t\t}\n\t})\n\tt.Run(\"MDH\", func(t *testing.T) {\n\t\ta := viewsByTimeRange(\"F\", mustParseTime(\"2000-11-29 22:00\"), mustParseTime(\"2002-03-02 03:00\"), mustParseTimeQuantum(\"MDH\"))\n\t\tif !reflect.DeepEqual(a, []string{\"F_2000112922\", \"F_2000112923\", \"F_20001130\", \"F_200012\", \"F_200101\", \"F_200102\", \"F_200103\", \"F_200104\", \"F_200105\", \"F_200106\", \"F_200107\", \"F_200108\", \"F_200109\", \"F_200110\", \"F_200111\", \"F_200112\", \"F_200201\", \"F_200202\", \"F_20020301\", \"F_2002030200\", \"F_2002030201\", \"F_2002030202\"}) {\n\t\t\tt.Fatalf(\"unexpected fields: %#v\", a)\n\t\t}\n\t})\n\tt.Run(\"D\", func(t *testing.T) {\n\t\ta := viewsByTimeRange(\"F\", mustParseTime(\"2000-01-01 00:00\"), mustParseTime(\"2000-01-04 00:00\"), mustParseTimeQuantum(\"D\"))\n\t\tif !reflect.DeepEqual(a, []string{\"F_20000101\", \"F_20000102\", \"F_20000103\"}) {\n\t\t\tt.Fatalf(\"unexpected fields: %#v\", a)\n\t\t}\n\t})\n\tt.Run(\"DH\", func(t *testing.T) {\n\t\ta := viewsByTimeRange(\"F\", mustParseTime(\"2000-01-01 22:00\"), mustParseTime(\"2000-03-01 02:00\"), mustParseTimeQuantum(\"DH\"))\n\t\tif !reflect.DeepEqual(a, []string{\"F_2000010122\", \"F_2000010123\", \"F_20000102\", \"F_20000103\", \"F_20000104\", \"F_20000105\", \"F_20000106\", \"F_20000107\", \"F_20000108\", \"F_20000109\", \"F_20000110\", \"F_20000111\", \"F_20000112\", \"F_20000113\", \"F_20000114\", \"F_20000115\", \"F_20000116\", \"F_20000117\", \"F_20000118\", \"F_20000119\", \"F_20000120\", \"F_20000121\", \"F_20000122\", \"F_20000123\", \"F_20000124\", \"F_20000125\", \"F_20000126\", \"F_20000127\", \"F_20000128\", \"F_20000129\", \"F_20000130\", \"F_20000131\", \"F_20000201\", \"F_20000202\", \"F_20000203\", \"F_20000204\", \"F_20000205\", \"F_20000206\", \"F_20000207\", \"F_20000208\", \"F_20000209\", \"F_20000210\", \"F_20000211\", \"F_20000212\", \"F_20000213\", \"F_20000214\", \"F_20000215\", \"F_20000216\", \"F_20000217\", \"F_20000218\", \"F_20000219\", \"F_20000220\", \"F_20000221\", \"F_20000222\", \"F_20000223\", \"F_20000224\", \"F_20000225\", \"F_20000226\", \"F_20000227\", \"F_20000228\", \"F_20000229\", \"F_2000030100\", \"F_2000030101\"}) {\n\t\t\tt.Fatalf(\"unexpected fields: %#v\", a)\n\t\t}\n\t})\n\tt.Run(\"H\", func(t *testing.T) {\n\t\ta := viewsByTimeRange(\"F\", mustParseTime(\"2000-01-01 00:00\"), mustParseTime(\"2000-01-01 02:00\"), mustParseTimeQuantum(\"H\"))\n\t\tif !reflect.DeepEqual(a, []string{\"F_2000010100\", \"F_2000010101\"}) {\n\t\t\tt.Fatalf(\"unexpected fields: %#v\", a)\n\t\t}\n\t})\n}\n\nfunc TestMinMaxViews(t *testing.T) {\n\tt.Run(\"Combos\", func(t *testing.T) {\n\t\ttests := []struct {\n\t\t\tviews []string\n\t\t\tq     TimeQuantum\n\t\t\tmin   string\n\t\t\tmax   string\n\t\t}{\n\t\t\t{\n\t\t\t\t[]string{\"\"},\n\t\t\t\tmustParseTimeQuantum(\"Y\"),\n\t\t\t\t\"\",\n\t\t\t\t\"\",\n\t\t\t},\n\t\t\t{\n\t\t\t\t[]string{\"std_2019\", \"std_2020\", \"std_202002\", \"std_202002\", \"std_2022\"},\n\t\t\t\tmustParseTimeQuantum(\"Y\"),\n\t\t\t\t\"std_2019\",\n\t\t\t\t\"std_2022\",\n\t\t\t},\n\t\t\t{\n\t\t\t\t// unordered views should not affect the results\n\t\t\t\t[]string{\"std_202002\", \"std_2022073123\", \"std_2020\", \"std_202002\", \"std_2022\", \"std_2019\", \"std_2022063023\"},\n\t\t\t\tmustParseTimeQuantum(\"Y\"),\n\t\t\t\t\"std_2019\",\n\t\t\t\t\"std_2022\",\n\t\t\t},\n\t\t\t{\n\t\t\t\t[]string{\"std_201902\", \"std_201901\"},\n\t\t\t\tmustParseTimeQuantum(\"M\"),\n\t\t\t\t\"std_201901\",\n\t\t\t\t\"std_201902\",\n\t\t\t},\n\t\t\t{\n\t\t\t\t[]string{\"std_201902\", \"std_201901\"},\n\t\t\t\tmustParseTimeQuantum(\"D\"),\n\t\t\t\t\"\",\n\t\t\t\t\"\",\n\t\t\t},\n\t\t\t{\n\t\t\t\t[]string{\"std_20190201\"},\n\t\t\t\tmustParseTimeQuantum(\"D\"),\n\t\t\t\t\"std_20190201\",\n\t\t\t\t\"std_20190201\",\n\t\t\t},\n\t\t\t{\n\t\t\t\t[]string{\"foo\", \"bar\"},\n\t\t\t\tmustParseTimeQuantum(\"D\"),\n\t\t\t\t\"\",\n\t\t\t\t\"\",\n\t\t\t},\n\t\t\t{\n\t\t\t\t// quantum is YMDH, views only have \"DH\" views\n\t\t\t\t[]string{\"std_20220531\", \"std_2022063023\", \"std_20220731\", \"std_2022073123\"},\n\t\t\t\tmustParseTimeQuantum(\"YMDH\"),\n\t\t\t\t\"std_20220531\",\n\t\t\t\t\"std_20220731\",\n\t\t\t},\n\t\t\t{\n\t\t\t\t// quantum is Y, views have D,H but dont have Y views\n\t\t\t\t[]string{\"std_20220531\", \"std_2022053123\"},\n\t\t\t\tmustParseTimeQuantum(\"Y\"),\n\t\t\t\t\"\",\n\t\t\t\t\"\",\n\t\t\t},\n\t\t\t{\n\t\t\t\t// quantum is M, views ignore Y view, only look at M view\n\t\t\t\t[]string{\"std_2022\", \"std_202205\", \"std_20220531\", \"std_2022053123\"},\n\t\t\t\tmustParseTimeQuantum(\"M\"),\n\t\t\t\t\"std_202205\",\n\t\t\t\t\"std_202205\",\n\t\t\t},\n\t\t}\n\t\tfor i, test := range tests {\n\t\t\tif min, max := minMaxViews(test.views, test.q); min != test.min {\n\t\t\t\tt.Errorf(\"test %d expected min: %v, but got: %v\", i, test.min, min)\n\t\t\t} else if max != test.max {\n\t\t\t\tt.Errorf(\"test %d expected max: %v, but got: %v\", i, test.max, max)\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc TestTimeOfView(t *testing.T) {\n\tt.Run(\"Combos\", func(t *testing.T) {\n\t\ttests := []struct {\n\t\t\tview   string\n\t\t\texp    time.Time\n\t\t\texpAdj time.Time\n\t\t\texpErr string\n\t\t}{\n\t\t\t{\n\t\t\t\t\"std_2019\",\n\t\t\t\ttime.Date(2019, 1, 1, 0, 0, 0, 0, time.UTC),\n\t\t\t\ttime.Date(2020, 1, 1, 0, 0, 0, 0, time.UTC),\n\t\t\t\t\"\",\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"std_201902\",\n\t\t\t\ttime.Date(2019, 2, 1, 0, 0, 0, 0, time.UTC),\n\t\t\t\ttime.Date(2019, 3, 1, 0, 0, 0, 0, time.UTC),\n\t\t\t\t\"\",\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"std_20190203\",\n\t\t\t\ttime.Date(2019, 2, 3, 0, 0, 0, 0, time.UTC),\n\t\t\t\ttime.Date(2019, 2, 4, 0, 0, 0, 0, time.UTC),\n\t\t\t\t\"\",\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"std_2019020308\",\n\t\t\t\ttime.Date(2019, 2, 3, 8, 0, 0, 0, time.UTC),\n\t\t\t\ttime.Date(2019, 2, 3, 9, 0, 0, 0, time.UTC),\n\t\t\t\t\"\",\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"foo\",\n\t\t\t\ttime.Time{},\n\t\t\t\ttime.Time{},\n\t\t\t\t\"invalid time format on view: foo\",\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"std_201902030801\",\n\t\t\t\ttime.Time{},\n\t\t\t\ttime.Time{},\n\t\t\t\t\"invalid time format on view: std_201902030801\",\n\t\t\t},\n\t\t}\n\t\tfor i, test := range tests {\n\t\t\t// adj: false\n\t\t\tif tm, err := timeOfView(test.view, false); err != nil {\n\t\t\t\tif err.Error() != test.expErr {\n\t\t\t\t\tt.Errorf(\"test %d got unexpected error: %s\", i, err)\n\t\t\t\t}\n\t\t\t} else if test.expErr != \"\" {\n\t\t\t\tt.Errorf(\"test %d expected error: %s but got none\", i, test.expErr)\n\t\t\t} else if tm != test.exp {\n\t\t\t\tt.Errorf(\"test %d expected time: %v, but got: %v\", i, test.exp, tm)\n\t\t\t}\n\t\t\t// adj: true\n\t\t\tif tm, err := timeOfView(test.view, true); err != nil {\n\t\t\t\tif err.Error() != test.expErr {\n\t\t\t\t\tt.Errorf(\"test %d got unexpected error: %s\", i, err)\n\t\t\t\t}\n\t\t\t} else if test.expErr != \"\" {\n\t\t\t\tt.Errorf(\"test %d expected error: %s but got none\", i, test.expErr)\n\t\t\t} else if tm != test.expAdj {\n\t\t\t\tt.Errorf(\"test %d expected time: %v, but got: %v\", i, test.expAdj, tm)\n\t\t\t}\n\t\t}\n\t})\n}\n\n// defaultTimeLayout is the time layout used by the tests.\nconst defaultTimeLayout = \"2006-01-02 15:04\"\n\n// mustParseTime parses value using DefaultTimeLayout. Panic on error.\nfunc mustParseTime(value string) time.Time {\n\tv, err := time.Parse(defaultTimeLayout, value)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn v\n}\n\n// mustParseTimeQuantum parses v into a time quantum. Panic on error.\nfunc mustParseTimeQuantum(v string) TimeQuantum {\n\tq, err := parseTimeQuantum(v)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn q\n}\n\n// parseTimeQuantum parses v into a time quantum.\nfunc parseTimeQuantum(v string) (TimeQuantum, error) {\n\tq := TimeQuantum(strings.ToUpper(v))\n\tif !q.Valid() {\n\t\treturn \"\", ErrInvalidTimeQuantum\n\t}\n\treturn q, nil\n}\n\nfunc TestParsePartialTime(t *testing.T) {\n\t// test handling of valud inputs\n\ttestCases := []struct {\n\t\tuserInput    string\n\t\texpectedTime time.Time\n\t}{\n\t\t{\n\t\t\t\"2006\",\n\t\t\ttime.Date(2006, 1, 1, 0, 0, 0, 0, time.UTC),\n\t\t},\n\t\t{\n\t\t\t\"2006-07\",\n\t\t\ttime.Date(2006, time.July, 1, 0, 0, 0, 0, time.UTC),\n\t\t},\n\t\t{\n\t\t\t\"2006-07-02\",\n\t\t\ttime.Date(2006, time.July, 2, 0, 0, 0, 0, time.UTC),\n\t\t},\n\t\t{\n\t\t\t\"2006-07-02T15\",\n\t\t\ttime.Date(2006, time.July, 2, 15, 0, 0, 0, time.UTC),\n\t\t},\n\t\t{\n\t\t\t\"2006-07-02T15:04\",\n\t\t\ttime.Date(2006, time.July, 2, 15, 4, 0, 0, time.UTC),\n\t\t},\n\t}\n\tfor _, tc := range testCases {\n\t\tgot, err := parsePartialTime(tc.userInput)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"expected nil error given parsing for '%s'\", tc.userInput)\n\t\t}\n\t\tif got != tc.expectedTime {\n\t\t\tt.Errorf(\"expected %v, got %v\", tc.expectedTime, got)\n\t\t}\n\t}\n\n\t// test handling of invalid inputs\n\tinvalidInputs := []string{\n\t\t\"  2006-01-02 \",\n\t\t\" foo-bar \",\n\t\t\"2006-\",\n\t\t\"2006-01-\",\n\t\t\"2006-01-02T\",\n\t\t\"2006T\",\n\t\t\"2006T04\",\n\t\t\"01-02\",\n\t\t\"2006-01T04\",\n\t\t\"2006-01T04:\",\n\t\t\"2006-01T:04\",\n\t}\n\tfor _, invalidInput := range invalidInputs {\n\t\t_, err := parsePartialTime(invalidInput)\n\t\tif err == nil {\n\t\t\tt.Errorf(\"for input '%s', error on parse expected\", invalidInput)\n\t\t}\n\t}\n\n}\n\nfunc TestViewTimePart(t *testing.T) {\n\tfor input, want := range map[string]string{\n\t\t\"standard\":         \"\",\n\t\t\"standard_1234567\": \"1234567\",\n\t\t\"standard1234567\":  \"\",\n\t} {\n\t\tif got := viewTimePart(input); got != want {\n\t\t\tt.Errorf(\"expected %v got %v\", want, got)\n\t\t}\n\t}\n}\n\nfunc TestGetLowestGranularityQuantum(t *testing.T) {\n\ttests := []struct {\n\t\tname       string\n\t\tviews      []string\n\t\texpQuantum TimeQuantum\n\t}{\n\t\t{\n\t\t\tname:       \"Y\",\n\t\t\tviews:      []string{\"std_2022\", \"std_202205\", \"std_20220531\", \"std_2022053123\"},\n\t\t\texpQuantum: TimeQuantum(\"Y\"),\n\t\t},\n\t\t{\n\t\t\tname:       \"empty\",\n\t\t\tviews:      []string{},\n\t\t\texpQuantum: TimeQuantum(\"\"),\n\t\t},\n\t\t{\n\t\t\tname:       \"empty, not number\",\n\t\t\tviews:      []string{\"std_abc\"},\n\t\t\texpQuantum: TimeQuantum(\"\"),\n\t\t},\n\t\t{\n\t\t\tname:       \"duplicate\",\n\t\t\tviews:      []string{\"std_2022\", \"std_202205\", \"std_20220531\", \"std_2022053123\", \"std_2022\", \"std_202205\", \"std_20220531\", \"std_2022053123\"},\n\t\t\texpQuantum: TimeQuantum(\"Y\"),\n\t\t},\n\t\t{\n\t\t\tname:       \"only Y\",\n\t\t\tviews:      []string{\"std_2022\"},\n\t\t\texpQuantum: TimeQuantum(\"Y\"),\n\t\t},\n\t\t{\n\t\t\tname:       \"only M\",\n\t\t\tviews:      []string{\"std_202205\"},\n\t\t\texpQuantum: TimeQuantum(\"M\"),\n\t\t},\n\t\t{\n\t\t\tname:       \"only D\",\n\t\t\tviews:      []string{\"std_20220531\"},\n\t\t\texpQuantum: TimeQuantum(\"D\"),\n\t\t},\n\t\t{\n\t\t\tname:       \"only H\",\n\t\t\tviews:      []string{\"std_2022053123\"},\n\t\t\texpQuantum: TimeQuantum(\"H\"),\n\t\t},\n\t\t{\n\t\t\tname:       \"Y unordered\",\n\t\t\tviews:      []string{\"std_202205\", \"std_20220531\", \"std_2022053123\", \"std_2022\"},\n\t\t\texpQuantum: TimeQuantum(\"Y\"),\n\t\t},\n\t\t{\n\t\t\tname:       \"M unordered\",\n\t\t\tviews:      []string{\"std_2022053123\", \"std_202205\", \"std_20220531\"},\n\t\t\texpQuantum: TimeQuantum(\"M\"),\n\t\t},\n\t\t{\n\t\t\tname:       \"D unordered\",\n\t\t\tviews:      []string{\"std_2022053123\", \"std_20220531\"},\n\t\t\texpQuantum: TimeQuantum(\"D\"),\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tquantum := getLowestGranularityQuantum(test.views)\n\n\t\t\tif quantum.String() != test.expQuantum.String() {\n\t\t\t\tt.Errorf(\"expected field: '%v', got: '%v'\", test.expQuantum.String(), quantum.String())\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "toml",
          "type": "tree",
          "content": null
        },
        {
          "name": "tracing",
          "type": "tree",
          "content": null
        },
        {
          "name": "tracker.go",
          "type": "blob",
          "size": 4.326171875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n)\n\ntype ActiveQueryStatus struct {\n\tPQL   string        `json:\"PQL\"`\n\tSQL   string        `json:\"SQL,omitempty\"`\n\tNode  string        `json:\"node\"`\n\tIndex string        `json:\"index\"`\n\tAge   time.Duration `json:\"age\"`\n}\n\ntype PastQueryStatus struct {\n\tPQL       string        `json:\"PQL\"`\n\tSQL       string        `json:\"SQL,omitempty\"`\n\tNode      string        `json:\"nodeID\"`\n\tIndex     string        `json:\"index\"`\n\tStart     time.Time     `json:\"start\"`\n\tRuntime   time.Duration `json:\"runtime\"` // deprecated\n\tRuntimeNs time.Duration `json:\"runtimeNanoseconds\"`\n}\n\ntype activeQuery struct {\n\tPQL     string\n\tSQL     string\n\tnode    string\n\tindex   string\n\tstarted time.Time\n}\n\ntype pastQuery struct {\n\tPQL     string\n\tSQL     string\n\tnode    string\n\tindex   string\n\tstarted time.Time\n\truntime time.Duration\n}\n\ntype queryStatusUpdate struct {\n\tq       *activeQuery\n\tend     bool\n\tendTime time.Time\n}\n\ntype queryTracker struct {\n\tupdates chan<- queryStatusUpdate\n\tchecks  chan<- chan<- []*activeQuery\n\thistory *ringBuffer\n\n\twg   sync.WaitGroup\n\tstop chan struct{}\n}\n\ntype ringBuffer struct {\n\tqueries []pastQuery\n\tstart   int\n\tcount   int\n\tmu      sync.Mutex\n}\n\n// newRingBuffer initializes an empty RingBuffer of specified capacity.\nfunc newRingBuffer(n int) *ringBuffer {\n\treturn &ringBuffer{\n\t\tqueries: make([]pastQuery, n),\n\t\tstart:   0,\n\t\tcount:   0,\n\t}\n}\n\n// add adds a new element to the queue, overwriting the oldest if it is already full.\nfunc (b *ringBuffer) add(q pastQuery) {\n\tb.mu.Lock()\n\tdefer b.mu.Unlock()\n\n\t// len(b.queries) is used here as the *capacity* of the ringBuffer\n\tb.queries[(b.start+b.count)%len(b.queries)] = q\n\tif b.count == len(b.queries) {\n\t\tb.start = (b.start + 1) % len(b.queries)\n\t} else {\n\t\tb.count++\n\t}\n}\n\n// slice returns the contents of the RingBuffer, in insertion order.\nfunc (b *ringBuffer) slice() []pastQuery {\n\tb.mu.Lock()\n\tdefer b.mu.Unlock()\n\treturn append(b.queries[b.start:b.count], b.queries[0:b.start]...)\n}\n\nfunc newQueryTracker(historyLength int) *queryTracker {\n\tdone := make(chan struct{})\n\tupdates := make(chan queryStatusUpdate, 128)\n\tchecks := make(chan chan<- []*activeQuery)\n\thistory := newRingBuffer(historyLength)\n\ttracker := &queryTracker{\n\t\tupdates: updates,\n\t\tchecks:  checks,\n\t\thistory: history,\n\t\tstop:    done,\n\t}\n\ttracker.wg.Add(1)\n\tgo func() {\n\t\tdefer tracker.wg.Done()\n\n\t\tactiveQueries := make(map[*activeQuery]struct{})\n\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase update := <-updates:\n\t\t\t\tif update.end {\n\t\t\t\t\tpq := pastQuery{update.q.PQL, update.q.SQL, update.q.node, update.q.index, update.q.started, update.endTime.Sub(update.q.started)}\n\t\t\t\t\ttracker.history.add(pq)\n\t\t\t\t\tdelete(activeQueries, update.q)\n\t\t\t\t} else {\n\t\t\t\t\tactiveQueries[update.q] = struct{}{}\n\t\t\t\t}\n\t\t\tcase check := <-checks:\n\t\t\t\tout := make([]*activeQuery, len(activeQueries))\n\t\t\t\ti := 0\n\t\t\t\tfor q := range activeQueries {\n\t\t\t\t\tout[i] = q\n\t\t\t\t\ti++\n\t\t\t\t}\n\t\t\t\tcheck <- out\n\t\t\t\tclose(check)\n\t\t\tcase <-done:\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\treturn tracker\n}\n\nfunc (t *queryTracker) Start(pql, sql, nodeID, index string, start time.Time) *activeQuery {\n\tq := &activeQuery{pql, sql, nodeID, index, start}\n\tt.updates <- queryStatusUpdate{q, false, time.Time{}}\n\treturn q\n}\n\nfunc (t *queryTracker) Finish(q *activeQuery) {\n\tt.updates <- queryStatusUpdate{q, true, time.Now()}\n}\n\nfunc (t *queryTracker) ActiveQueries() []ActiveQueryStatus {\n\tch := make(chan []*activeQuery, 1)\n\tt.checks <- ch\n\tqueries := <-ch\n\tsort.Slice(queries, func(i, j int) bool {\n\t\tswitch {\n\t\tcase queries[i].started.Before(queries[j].started):\n\t\t\treturn true\n\t\tcase queries[i].started.After(queries[j].started):\n\t\t\treturn false\n\t\tcase queries[i].PQL < queries[j].PQL:\n\t\t\treturn true\n\t\tcase queries[i].PQL > queries[j].PQL:\n\t\t\treturn false\n\t\tdefault:\n\t\t\treturn false\n\t\t}\n\t})\n\tnow := time.Now()\n\tout := make([]ActiveQueryStatus, len(queries))\n\tfor i, v := range queries {\n\t\tout[i] = ActiveQueryStatus{v.PQL, v.SQL, v.node, v.index, now.Sub(v.started)}\n\t}\n\treturn out\n}\n\nfunc (t *queryTracker) PastQueries() []PastQueryStatus {\n\tqueries := t.history.slice()\n\tout := make([]PastQueryStatus, len(queries))\n\tfor i, v := range queries {\n\t\tout[i] = PastQueryStatus{v.PQL, v.SQL, v.node, v.index, v.started, v.runtime, v.runtime}\n\t}\n\treturn out\n\n}\n\nfunc (t *queryTracker) Stop() {\n\tclose(t.stop)\n\tt.wg.Wait()\n}\n"
        },
        {
          "name": "tracker_test.go",
          "type": "blob",
          "size": 2.0927734375,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\t\"time\"\n)\n\nfunc TestRingBuffer(t *testing.T) {\n\ttests := []struct {\n\t\tstart   int\n\t\tcount   int\n\t\tqueries []string\n\t}{\n\t\t{start: 0, count: 0, queries: []string{}},\n\t\t{start: 0, count: 1, queries: []string{\"0\"}},\n\t\t{start: 0, count: 2, queries: []string{\"0\", \"1\"}},\n\t\t{start: 0, count: 3, queries: []string{\"0\", \"1\", \"2\"}},\n\t\t{start: 0, count: 4, queries: []string{\"0\", \"1\", \"2\", \"3\"}},\n\t\t{start: 0, count: 5, queries: []string{\"0\", \"1\", \"2\", \"3\", \"4\"}},\n\t\t{start: 1, count: 5, queries: []string{\"1\", \"2\", \"3\", \"4\", \"5\"}},\n\t\t{start: 2, count: 5, queries: []string{\"2\", \"3\", \"4\", \"5\", \"6\"}},\n\t\t{start: 3, count: 5, queries: []string{\"3\", \"4\", \"5\", \"6\", \"7\"}},\n\t\t{start: 4, count: 5, queries: []string{\"4\", \"5\", \"6\", \"7\", \"8\"}},\n\t\t{start: 0, count: 5, queries: []string{\"5\", \"6\", \"7\", \"8\", \"9\"}},\n\t\t{start: 1, count: 5, queries: []string{\"6\", \"7\", \"8\", \"9\", \"10\"}},\n\t\t{start: 2, count: 5, queries: []string{\"7\", \"8\", \"9\", \"10\", \"11\"}},\n\t}\n\tbuffer := newRingBuffer(5)\n\tfor k := 0; k < len(tests); k++ {\n\t\tif !(buffer.start == tests[k].start && buffer.count == tests[k].count) {\n\t\t\tt.Fatalf(\"expected %d %d, found %d %d\", tests[k].start, tests[k].count, buffer.start, buffer.count)\n\t\t}\n\n\t\tfor n, q := range buffer.slice() {\n\t\t\tif q.PQL != tests[k].queries[n] {\n\t\t\t\tt.Fatalf(\"test[%d], buffer[%d] expected querystring '%s', found '%s'\", k, n, tests[k].queries[n], q.PQL)\n\t\t\t}\n\t\t}\n\t\tbuffer.add(pastQuery{PQL: fmt.Sprintf(\"%d\", k)})\n\t}\n}\n\nfunc TestQueryTracker(t *testing.T) {\n\ttracker := newQueryTracker(5)\n\tdefer tracker.Stop()\n\n\tif queries := tracker.ActiveQueries(); len(queries) > 0 {\n\t\tt.Fatalf(\"expected no active queries; found %v\", queries)\n\t}\n\n\tqs := tracker.Start(\"test query\", \"test SQL\", \"node0\", \"i\", time.Now())\n\n\tvar queries []ActiveQueryStatus\n\tfor len(queries) < 1 {\n\t\tqueries = tracker.ActiveQueries()\n\t}\n\tif len(queries) > 1 || queries[0].PQL != \"test query\" {\n\t\tt.Fatalf(\"unexpected queries: %v\", queries)\n\t}\n\n\ttracker.Finish(qs)\n\n\tfor len(queries) > 0 {\n\t\tqueries = tracker.ActiveQueries()\n\t}\n}\n"
        },
        {
          "name": "transaction.go",
          "type": "blob",
          "size": 13.5498046875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"regexp\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/logger\"\n\t\"github.com/pkg/errors\"\n)\n\nvar txIDRegexp = regexp.MustCompile(\"^[A-Za-z0-9_-]*$\")\n\n// Transaction contains information related to a block of work that\n// needs to be tracked and spans multiple API calls.\ntype Transaction struct {\n\t// ID is an arbitrary string identifier. All transactions must have a unique ID.\n\tID string `json:\"id\"`\n\n\t// Active notes whether an exclusive transaction is active, or\n\t// still pending (if other active transactions exist). All\n\t// non-exclusive transactions are always active.\n\tActive bool `json:\"active\"`\n\n\t// Exclusive is set to true for transactions which can only become active when no other\n\t// transactions exist.\n\tExclusive bool `json:\"exclusive\"`\n\n\t// Timeout is the minimum idle time for which this transaction should continue to exist.\n\tTimeout time.Duration `json:\"timeout\"`\n\n\t// CreatedAt is the timestamp at which the transaction was created. This supports\n\t// the case of listing transactions in a useful order.\n\tCreatedAt time.Time `json:\"createdAt\"`\n\n\t// Deadline is calculated from Timeout. TODO reset deadline each time there is activity\n\t// on the transaction. (we can't do this until there is some method of associating a\n\t// request/call with a transaction)\n\tDeadline time.Time `json:\"deadline\"`\n\n\t// Stats track statistics for the transaction. Not yet used.\n\tStats TransactionStats `json:\"stats\"`\n}\n\ntype TransactionStats struct{}\n\n// TransactionManager enforces the rules for transactions on a single\n// node. It is goroutine-safe. It should be created by a call to\n// NewTransactionManager where it takes a TransactionStore. If logging\n// is desired, Log should be set before an instance of\n// TransactionManager is used.\ntype TransactionManager struct {\n\tmu sync.RWMutex\n\n\tLog logger.Logger\n\n\tstore TransactionStore\n\n\tcheckingDeadlines bool\n}\n\n// NewTransactionManager creates a new TransactionManager with the\n// given store, and starts a deadline-checker in a goroutine.\nfunc NewTransactionManager(store TransactionStore) *TransactionManager {\n\ttm := &TransactionManager{\n\t\tLog:               logger.NopLogger,\n\t\tstore:             store,\n\t\tcheckingDeadlines: true,\n\t}\n\t// start deadline checker in case we've just started up, but there is already state in the store.\n\tgo tm.deadlineChecker()\n\treturn tm\n}\n\n// Start starts a new transaction with the given parameters. If an\n// exclusive transaction is pending or in progress,\n// ErrTransactionExclusive is returned. If a transaction with the same\n// id already exists, that transaction is returned along with\n// ErrTransactionExists. If there is no error, the created transaction\n// is returnedthis is primarily so that the caller can discover if an\n// exclusive transaction has been made immediately active or if they\n// need to poll.\nfunc (tm *TransactionManager) Start(ctx context.Context, id string, timeout time.Duration, exclusive bool) (*Transaction, error) {\n\ttm.mu.Lock()\n\tdefer tm.mu.Unlock()\n\n\tif !txIDRegexp.Match([]byte(id)) {\n\t\treturn nil, errors.New(\"invalid transaction ID, must match [A-Za-z0-9_-]\")\n\t}\n\n\ttrnsMap, err := tm.store.List()\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"listing transactions in Start\")\n\t}\n\n\t// check for an exclusive transaction\n\tfor _, trns := range trnsMap {\n\t\tif trns.Exclusive {\n\t\t\t// if someone wants a transaction, and we're not able to\n\t\t\t// give it to them, we want to be checking deadlines.\n\t\t\t// TODO: it would be nice if we could identify whether\n\t\t\t// this trns has expired, and if so, automatically remove\n\t\t\t// it and continue without returning ErrTransactionExclusive\n\t\t\t// on this iteration of the loop. One way we could do that is\n\t\t\t// to call tm.checkDeadlines() here (note that we'd have to\n\t\t\t// have an unprotectedCheckDeadlines()), and then after that\n\t\t\t// check if trns still exists in tm.store. If not, continue.\n\t\t\ttm.startDeadlineChecker()\n\t\t\treturn trns, ErrTransactionExclusive\n\t\t}\n\t}\n\tif trns, ok := trnsMap[id]; ok {\n\t\treturn trns, ErrTransactionExists\n\t}\n\n\t// set new transaction to active if it is not exclusive or if\n\t// there are no other transactions.\n\tactive := !exclusive || (len(trnsMap) == 0)\n\n\t// set deadline according to timeout\n\tcreatedAt := time.Now()\n\tdeadline := createdAt.Add(timeout)\n\ttrns := &Transaction{\n\t\tID:        id,\n\t\tActive:    active,\n\t\tExclusive: exclusive,\n\t\tTimeout:   timeout,\n\t\tCreatedAt: createdAt,\n\t\tDeadline:  deadline,\n\t}\n\tif err = tm.store.Put(trns); err != nil {\n\t\treturn nil, errors.Wrap(err, \"adding to store\")\n\t}\n\n\t// we won't check deadlines unless there's actually a\n\t// transaction pending\n\tif exclusive && !active {\n\t\ttm.startDeadlineChecker()\n\t}\n\n\treturn trns, nil\n}\n\n// Finish completes and removes a transaction, returning the completed\n// transaction (so that the caller can e.g. view the Stats)\nfunc (tm *TransactionManager) Finish(ctx context.Context, id string) (*Transaction, error) {\n\ttm.mu.Lock()\n\tdefer tm.mu.Unlock()\n\treturn tm.finish(id)\n}\n\n// finish is the unprotected implementation of Finish\nfunc (tm *TransactionManager) finish(id string) (*Transaction, error) {\n\ttrns, err := tm.store.Remove(id)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// After removing, check to see if we need to activate an exclusive transaction\n\ttrnsMap, err := tm.store.List()\n\tif err != nil {\n\t\ttm.log().Errorf(\"error listing transactions in Finish: %v\", err)\n\t\treturn trns, nil\n\t}\n\n\tif len(trnsMap) == 1 {\n\t\tfor _, etrans := range trnsMap {\n\t\t\tif etrans.Exclusive {\n\t\t\t\tif etrans.Active { // sanity check\n\t\t\t\t\tpanic(\"we just removed a transaction, and the sole remaining exclusive transaction was already active\")\n\t\t\t\t}\n\t\t\t\tetrans.Active = true\n\t\t\t\tetrans.Deadline = time.Now().Add(etrans.Timeout)\n\t\t\t\tif err := tm.store.Put(etrans); err != nil {\n\t\t\t\t\ttm.log().Errorf(\"activating exclusive transaction after finishing last transaction: %v\", err)\n\t\t\t\t\treturn trns, nil\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn trns, nil\n}\n\n// Get retrieves the transaction with the given ID. Returns ErrTransactionNotFound\n// if there isn't one.\nfunc (tm *TransactionManager) Get(ctx context.Context, id string) (*Transaction, error) {\n\ttm.mu.RLock()\n\tdefer tm.mu.RUnlock()\n\n\treturn tm.store.Get(id)\n}\n\n// List returns map of all transactions by their ID. It is a copy and\n// so may be retained and modified by the caller.\nfunc (tm *TransactionManager) List(ctx context.Context) (map[string]*Transaction, error) {\n\ttm.mu.RLock()\n\tdefer tm.mu.RUnlock()\n\treturn tm.store.List()\n}\n\n// ResetDeadline updates the deadline for the transaction with the\n// given ID to be equal to the current time plus the transaction's\n// timeout.\nfunc (tm *TransactionManager) ResetDeadline(ctx context.Context, id string) (*Transaction, error) {\n\ttm.mu.Lock()\n\tdefer tm.mu.Unlock()\n\ttrns, err := tm.store.Get(id)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"getting transaction\")\n\t}\n\n\ttrns.Deadline = time.Now().Add(trns.Timeout)\n\n\terr = tm.store.Put(trns)\n\treturn trns, errors.Wrap(err, \"storing transaction with new timeout\")\n}\n\n// startDeadlineChecker may only be called while tm.mu is held.\nfunc (tm *TransactionManager) startDeadlineChecker() {\n\tif !tm.checkingDeadlines {\n\t\ttm.checkingDeadlines = true\n\t\tgo tm.deadlineChecker()\n\t}\n}\n\n// deadlineChecker loops continuously checking for expired\n// deadlines. It stops when there are no upcoming deadlines.\nfunc (tm *TransactionManager) deadlineChecker() {\n\tinterval := tm.checkDeadlines()\n\tfor interval != 0 {\n\t\ttime.Sleep(interval)\n\t\tinterval = tm.checkDeadlines()\n\t}\n\ttm.mu.Lock()\n\ttm.checkingDeadlines = false\n\ttm.mu.Unlock()\n}\n\n// checkDeadlines finishes transactions which are past their\n// deadlines. It returns the duration until the next deadline. If\n// there are no exclusive transactions, it does nothing and returns 0\n// as a signal to stop checking.\nfunc (tm *TransactionManager) checkDeadlines() time.Duration {\n\ttm.mu.Lock()\n\tdefer tm.mu.Unlock()\n\n\ttrnsMap, err := tm.store.List()\n\tif err != nil {\n\t\ttm.log().Errorf(\"transaction deadline checker couldn't list transactions: %v\", err)\n\t\treturn 0\n\t}\n\n\thasExclusive := false\n\tfor _, trns := range trnsMap {\n\t\tif trns.Exclusive {\n\t\t\thasExclusive = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif !hasExclusive {\n\t\treturn 0 // no need to expire things if nothing is waiting\n\t}\n\n\tnow := time.Now()\n\t// track the time interval to next deadline\n\tnextInterval := time.Duration(0)\n\tfor id, trns := range trnsMap {\n\t\tif !trns.Active {\n\t\t\tcontinue\n\t\t}\n\t\tif !now.Before(trns.Deadline) {\n\t\t\ttrnsF, err := tm.finish(id)\n\t\t\tif err != nil {\n\t\t\t\ttm.log().Errorf(\"error finishing expired transaction '%s': %+v: %v\", id, trnsF, err)\n\t\t\t} else {\n\t\t\t\ttm.log().Infof(\"cleared expired transaction: %+v\", trnsF)\n\t\t\t}\n\t\t} else {\n\t\t\tinterval := trns.Deadline.Sub(now)\n\t\t\tif nextInterval == 0 || interval < nextInterval {\n\t\t\t\tnextInterval = interval\n\t\t\t}\n\t\t}\n\t}\n\treturn nextInterval\n}\n\nfunc (tm *TransactionManager) log() logger.Logger {\n\tif tm.Log != nil {\n\t\treturn tm.Log\n\t}\n\treturn logger.NopLogger\n}\n\n// TransactionStore declares the functionality which a store for\n// Pilosa transactions must implement.\ntype TransactionStore interface {\n\t// Put stores a new transaction or replaces an existing transaction with the given one.\n\tPut(trns *Transaction) error\n\t// Get retrieves the transaction at id or returns ErrTransactionNotFound if there isn't one.\n\tGet(id string) (*Transaction, error)\n\t// List returns a map of all transactions by ID. The map must be safe to modify by the caller.\n\tList() (map[string]*Transaction, error)\n\t// Remove deletes the transaction from the store. It must return ErrTransactionNotFound if there isn't one.\n\tRemove(id string) (*Transaction, error)\n}\n\ntype OpenTransactionStoreFunc func(path string) (TransactionStore, error)\n\nfunc OpenInMemTransactionStore(path string) (TransactionStore, error) {\n\treturn NewInMemTransactionStore(), nil\n}\n\n// InMemTransactionStore does not persist transaction data and is only\n// useful for testing.\ntype InMemTransactionStore struct {\n\tmu   sync.RWMutex\n\ttmap map[string]*Transaction\n}\n\nfunc NewInMemTransactionStore() *InMemTransactionStore {\n\treturn &InMemTransactionStore{\n\t\ttmap: make(map[string]*Transaction),\n\t}\n}\n\nfunc (s *InMemTransactionStore) Put(trns *Transaction) error {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\ts.tmap[trns.ID] = trns.Copy()\n\treturn nil\n}\n\nfunc (s *InMemTransactionStore) Get(id string) (*Transaction, error) {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\n\tif trns, ok := s.tmap[id]; ok {\n\t\treturn trns.Copy(), nil\n\t}\n\treturn nil, ErrTransactionNotFound\n}\n\nfunc (s *InMemTransactionStore) List() (map[string]*Transaction, error) {\n\tcp := make(map[string]*Transaction)\n\tfor id, trns := range s.tmap {\n\t\tcp[id] = trns.Copy()\n\t}\n\treturn cp, nil\n}\n\nfunc (s *InMemTransactionStore) Remove(id string) (*Transaction, error) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tif trns, ok := s.tmap[id]; ok {\n\t\tdelete(s.tmap, id)\n\t\treturn trns.Copy(), nil\n\t}\n\treturn nil, ErrTransactionNotFound\n}\n\ntype Error string\n\nfunc (e Error) Error() string { return string(e) }\n\nconst ErrTransactionNotFound = Error(\"transaction not found\")\nconst ErrTransactionExclusive = Error(\"there is an exclusive transaction, try later\")\nconst ErrTransactionExists = Error(\"transaction with the given id already exists\")\n\nfunc CompareTransactions(t1, t2 *Transaction) error {\n\tif t1 == nil && t2 == nil {\n\t\treturn nil\n\t}\n\tif t1 == nil || t2 == nil {\n\t\treturn errors.Errorf(\"transactions are not equal: %+v %+v\", t1, t2)\n\t}\n\tif t1.ID != t2.ID {\n\t\treturn errors.Errorf(\"transaction IDs not equal: %+v %+v\", t1, t2)\n\t}\n\tif t1.Active != t2.Active {\n\t\treturn errors.Errorf(\"transaction Actives not equal: %+v %+v\", t1, t2)\n\t}\n\tif t1.Exclusive != t2.Exclusive {\n\t\treturn errors.Errorf(\"transaction Exclusives not equal: %+v %+v\", t1, t2)\n\t}\n\tif t1.Timeout != t2.Timeout {\n\t\treturn errors.Errorf(\"transaction Timeouts not equal: %+v %+v\", t1, t2)\n\t}\n\t// don't care about Deadline or Stats\n\treturn nil\n}\n\nfunc (trns *Transaction) UnmarshalJSON(b []byte) error {\n\ttmp := &struct {\n\t\tID        string      `json:\"id\"`\n\t\tActive    bool        `json:\"active\"`\n\t\tExclusive bool        `json:\"exclusive\"`\n\t\tTimeout   interface{} `json:\"timeout\"`\n\t\tDeadline  string      `json:\"deadline\"`\n\t}{}\n\terr := json.Unmarshal(b, tmp)\n\tif err != nil {\n\t\treturn err\n\t}\n\ttrns.ID = tmp.ID\n\ttrns.Active = tmp.Active\n\ttrns.Exclusive = tmp.Exclusive\n\tswitch tm := tmp.Timeout.(type) {\n\tcase string:\n\t\tdur, err := time.ParseDuration(tm)\n\t\tif err != nil {\n\t\t\treturn errors.Wrapf(err, \"timeout as string must be a valid duration got: '%s'\", tm)\n\t\t}\n\t\ttrns.Timeout = dur\n\tcase float64:\n\t\t// interpret as number of seconds\n\t\tseconds := int64(tm)\n\t\tnsec := (tm - float64(seconds)) * 1e9\n\t\ttrns.Timeout = time.Duration(seconds*1e9 + int64(nsec))\n\tcase nil:\n\t\tbreak\n\tdefault:\n\t\treturn errors.New(\"timeout must be float64 or string\")\n\t}\n\n\tif tmp.Deadline != \"\" {\n\t\ttrns.Deadline, err = time.Parse(time.RFC3339Nano, tmp.Deadline)\n\t}\n\treturn errors.Wrap(err, \"parsing deadline\")\n}\n\nfunc (trns *Transaction) MarshalJSON() ([]byte, error) {\n\treturn json.Marshal(&struct {\n\t\tID        string `json:\"id\"`\n\t\tActive    bool   `json:\"active\"`\n\t\tExclusive bool   `json:\"exclusive\"`\n\t\tTimeout   string `json:\"timeout\"`\n\t\tCreatedAt string `json:\"createdAt\"`\n\t\tDeadline  string `json:\"deadline\"`\n\t}{\n\t\tID:        trns.ID,\n\t\tActive:    trns.Active,\n\t\tExclusive: trns.Exclusive,\n\t\tTimeout:   trns.Timeout.String(),\n\t\tCreatedAt: trns.CreatedAt.In(time.UTC).Format(time.RFC3339Nano),\n\t\tDeadline:  trns.Deadline.In(time.UTC).Format(time.RFC3339Nano),\n\t})\n}\n\nfunc (trns *Transaction) Copy() *Transaction {\n\treturn &Transaction{\n\t\tID:        trns.ID,\n\t\tActive:    trns.Active,\n\t\tExclusive: trns.Exclusive,\n\t\tTimeout:   trns.Timeout,\n\t\tCreatedAt: trns.CreatedAt,\n\t\tDeadline:  trns.Deadline,\n\t\tStats:     trns.Stats,\n\t}\n}\n"
        },
        {
          "name": "transaction_test.go",
          "type": "blob",
          "size": 10.419921875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa_test\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"testing\"\n\t\"time\"\n\n\tpilosa \"github.com/featurebasedb/featurebase/v3\"\n\t\"github.com/featurebasedb/featurebase/v3/logger\"\n\t\"github.com/featurebasedb/featurebase/v3/test\"\n)\n\n// TestTransactionManager currently uses an in memory transaction\n// store, but tests a variety of timeouts, and therefore could be\n// sensitive to slowness in the implementation. Especially if a store\n// were used that actually wrote things to disk.\nfunc TestTransactionManager(t *testing.T) {\n\tstore := pilosa.NewInMemTransactionStore()\n\n\ttm := pilosa.NewTransactionManager(store)\n\ttm.Log = logger.NewBufferLogger()\n\tctx := context.Background()\n\n\t// can add a non-exclusive transaction\n\ttrns1 := mustStart(t, tm, \"a\", time.Microsecond, false)\n\ttest.CompareTransactions(t, &pilosa.Transaction{ID: \"a\", Active: true, Timeout: time.Microsecond, Deadline: time.Now()}, trns1)\n\n\t// can have two non exclusive transactions\n\ttrns2 := mustStart(t, tm, \"b\", time.Microsecond, false)\n\ttest.CompareTransactions(t, &pilosa.Transaction{ID: \"b\", Active: true, Timeout: time.Microsecond, Deadline: time.Now()}, trns2)\n\n\t// trying to start a transaction with same name errors and returns previous transaction\n\tt3, err := tm.Start(ctx, \"a\", time.Second, true)\n\tif err != pilosa.ErrTransactionExists {\n\t\tt.Errorf(\"expected transaction exists, but got: '%v'\", err)\n\t}\n\ttest.CompareTransactions(t, trns1, t3)\n\n\t// can get an existing transaction\n\ttrns2_2 := mustGet(t, tm, \"b\")\n\ttest.CompareTransactions(t, trns2, trns2_2)\n\n\t// can list all transactions\n\ttrnsMap := mustList(t, tm)\n\tif len(trnsMap) != 2 {\n\t\tt.Errorf(\"unexpected number of transactions in map: %d\", len(trnsMap))\n\t}\n\ttest.CompareTransactions(t, trnsMap[\"a\"], trns1)\n\ttest.CompareTransactions(t, trnsMap[\"b\"], trns2)\n\n\t// can submit an exclusive transaction\n\ttrnsE := mustStart(t, tm, \"ce\", 100*time.Millisecond, true)\n\ttest.CompareTransactions(t, &pilosa.Transaction{ID: \"ce\", Active: false, Exclusive: true, Timeout: 100 * time.Millisecond, Deadline: time.Now().Add(100 * time.Millisecond)}, trnsE)\n\n\t// can't start new transactions while an exclusive transaction is pending\n\tif _, err := tm.Start(ctx, \"d\", time.Millisecond, false); err != pilosa.ErrTransactionExclusive {\n\t\tt.Errorf(\"unexpected error starting transaction while an exclusive transaction exists: %v\", err)\n\t}\n\n\t// can't start new exclusive transactions while an exclusive transaction is pending\n\tif _, err := tm.Start(ctx, \"ee\", time.Millisecond, true); err != pilosa.ErrTransactionExclusive {\n\t\tt.Errorf(\"unexpected error starting transaction while an exclusive transaction exists: %v\", err)\n\t}\n\n\t// exclusive transaction becomes active after deadlines expire\n\tfor i := 0; true; i++ {\n\t\ttime.Sleep(time.Millisecond)\n\t\ttrnsE, err := tm.Get(ctx, \"ce\")\n\t\tif err != nil {\n\t\t\tt.Errorf(\"error retrieving exclusive transaction: %v\", err)\n\t\t}\n\t\tif trnsE.Active {\n\t\t\tbreak\n\t\t}\n\t\tif i > 10000 {\n\t\t\tt.Fatalf(\"exclusive transaction never became active: %+v\", trnsE)\n\t\t}\n\t}\n\n\t// can't start new transactions while an exclusive transaction is active\n\tif _, err := tm.Start(ctx, \"f\", time.Millisecond, false); err != pilosa.ErrTransactionExclusive {\n\t\tt.Errorf(\"unexpected error starting transaction while an exclusive transaction exists: %v\", err)\n\t}\n\n\t// can't start new exclusive transactions while an exclusive transaction is active\n\tif _, err := tm.Start(ctx, \"ge\", time.Millisecond, true); err != pilosa.ErrTransactionExclusive {\n\t\tt.Errorf(\"unexpected error starting transaction while an exclusive transaction exists: %v\", err)\n\t}\n\n\t// exclusive transaction gets expired after other transactions have attempted to start\n\tfor i := 0; true; i++ {\n\t\ttime.Sleep(time.Millisecond * 20)\n\t\ttrnsE, err := tm.Get(ctx, \"ce\")\n\t\tif err == nil {\n\t\t\tif i > 10 {\n\t\t\t\tt.Fatalf(\"exclusive transaction didn't expire: %+v\", trnsE)\n\t\t\t}\n\t\t} else if err != pilosa.ErrTransactionNotFound {\n\t\t\tt.Errorf(\"unexpected error fetching transaction while waiting for expiration: %v\", err)\n\t\t} else {\n\t\t\tbreak // transaction was not found, therefore it expired and we can happily continue\n\t\t}\n\t}\n\n\t// can start a new exclusive transaction and it's immediately active\n\ttrnsHE := mustStart(t, tm, \"he\", time.Hour, true)\n\ttest.CompareTransactions(t, &pilosa.Transaction{ID: \"he\", Active: true, Exclusive: true, Timeout: time.Hour, Deadline: time.Now().Add(time.Hour)}, trnsHE)\n\n\t// can't start new transactions while an exclusive transaction is active\n\tif _, err := tm.Start(ctx, \"i\", time.Millisecond, false); err != pilosa.ErrTransactionExclusive {\n\t\tt.Errorf(\"unexpected error starting transaction while an exclusive transaction exists: %v\", err)\n\t}\n\n\t// can finish an active exclusive transaction\n\ttrnsHE_finish := mustFinish(t, tm, \"he\")\n\ttest.CompareTransactions(t, trnsHE, trnsHE_finish)\n\n\t// can start normal transaction after finishing exclusive transaction\n\ttrnsJ := mustStart(t, tm, \"j\", time.Hour, false)\n\ttest.CompareTransactions(t, &pilosa.Transaction{ID: \"j\", Active: true, Timeout: time.Hour, Deadline: time.Now().Add(time.Hour)}, trnsJ)\n\n\t// can finish normal transaction\n\ttrnsJ_finish := mustFinish(t, tm, \"j\")\n\ttest.CompareTransactions(t, trnsJ, trnsJ_finish)\n\n\t// can start normal transaction after finishing normal transaction\n\ttrnsK := mustStart(t, tm, \"k\", time.Hour, false)\n\ttest.CompareTransactions(t, &pilosa.Transaction{ID: \"k\", Active: true, Timeout: time.Hour, Deadline: time.Now().Add(time.Hour)}, trnsK)\n\n\t// can start new exclusive transaction, but not immediately active\n\ttrnsLE := mustStart(t, tm, \"le\", time.Hour, true)\n\ttest.CompareTransactions(t, &pilosa.Transaction{ID: \"le\", Exclusive: true, Timeout: time.Hour, Deadline: time.Now().Add(time.Hour)}, trnsLE)\n\n\t// finishing k should activate le\n\ttrnsK_finish := mustFinish(t, tm, \"k\")\n\ttest.CompareTransactions(t, trnsK, trnsK_finish)\n\ttrnsLE_active := mustGet(t, tm, \"le\")\n\ttrnsLE.Active = true\n\ttest.CompareTransactions(t, trnsLE, trnsLE_active)\n\n\tmustFinish(t, tm, \"le\")\n\n\t// can start normal transaction to test deadline reset\n\ttrnsM := mustStart(t, tm, \"m\", time.Millisecond*400, false)\n\ttest.CompareTransactions(t, &pilosa.Transaction{ID: \"m\", Active: true, Timeout: time.Millisecond * 400, Deadline: time.Now().Add(time.Millisecond * 400)}, trnsM)\n\n\t// start new exclusive transaction to trigger deadline check\n\ttrnsNE := mustStart(t, tm, \"ne\", time.Hour, true)\n\ttest.CompareTransactions(t, &pilosa.Transaction{ID: \"ne\", Exclusive: true, Timeout: time.Hour, Deadline: time.Now().Add(time.Hour)}, trnsNE)\n\n\t// sleep for most of the deadline\n\ttime.Sleep(time.Millisecond * 300)\n\n\t// reset deadline\n\ttrnsM_reset, err := tm.ResetDeadline(ctx, \"m\")\n\tif err != nil {\n\t\tt.Errorf(\"resetting deadline: %v\", err)\n\t}\n\ttrnsM.Deadline = time.Now().Add(time.Millisecond * 400)\n\ttest.CompareTransactions(t, trnsM, trnsM_reset)\n\n\t// sleep until past the original deadline\n\ttime.Sleep(time.Millisecond * 200)\n\n\t// verify that trnsM still exists\n\ttrnsM_again := mustGet(t, tm, \"m\")\n\ttest.CompareTransactions(t, trnsM, trnsM_again)\n\n}\n\nfunc mustStart(t *testing.T, tm *pilosa.TransactionManager, id string, timeout time.Duration, exclusive bool) *pilosa.Transaction {\n\tt.Helper()\n\ttrns, err := tm.Start(context.Background(), id, timeout, exclusive)\n\tif err != nil {\n\t\tt.Errorf(\"starting transaction: %v\", err)\n\t}\n\treturn trns\n}\n\nfunc mustFinish(t *testing.T, tm *pilosa.TransactionManager, id string) *pilosa.Transaction {\n\tt.Helper()\n\ttrns, err := tm.Finish(context.Background(), id)\n\tif err != nil {\n\t\tt.Errorf(\"finishing transaction: %v\", err)\n\t}\n\treturn trns\n}\n\nfunc mustGet(t *testing.T, tm *pilosa.TransactionManager, id string) *pilosa.Transaction {\n\tt.Helper()\n\ttrns, err := tm.Get(context.Background(), id)\n\tif err != nil {\n\t\tt.Errorf(\"getting transaction %s: %v\", id, err)\n\t}\n\treturn trns\n}\n\nfunc mustList(t *testing.T, tm *pilosa.TransactionManager) map[string]*pilosa.Transaction {\n\tt.Helper()\n\ttrnsMap, err := tm.List(context.Background())\n\tif err != nil {\n\t\tt.Errorf(\"getting transaction list: %v\", err)\n\t}\n\treturn trnsMap\n}\n\nfunc TestInMemTransactionStore(t *testing.T) {\n\tims := pilosa.NewInMemTransactionStore()\n\n\terr := ims.Put(&pilosa.Transaction{ID: \"blah\", Timeout: time.Second})\n\tif err != nil {\n\t\tt.Fatalf(\"adding blah: %v\", err)\n\t}\n\n\ttrns, err := ims.Get(\"blah\")\n\tif err != nil {\n\t\tt.Fatalf(\"getting blah: %v\", err)\n\t}\n\tif trns.ID != \"blah\" || trns.Timeout != time.Second {\n\t\tt.Fatalf(\"unexpected transaction for blah: %+v\", t)\n\t}\n\n\t_, err = ims.Get(\"nope\")\n\tif err != pilosa.ErrTransactionNotFound {\n\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t}\n\n\tl, err := ims.List()\n\tif err != nil {\n\t\tt.Fatalf(\"listing transactions: %v\", err)\n\t}\n\tif len(l) != 1 {\n\t\tt.Errorf(\"unexpected number of transactions: %d\", len(l))\n\t}\n\tif l[\"blah\"].ID != \"blah\" || l[\"blah\"].Timeout != time.Second {\n\t\tt.Errorf(\"unexpected transaction at blah: %+v\", l[\"blah\"])\n\t}\n\n}\n\nfunc TestMarshalUnmarshalTransaction(t *testing.T) {\n\ttests := []struct {\n\t\tname        string\n\t\ttransaction *pilosa.Transaction\n\t}{\n\t\t{\n\t\t\tname:        \"empty\",\n\t\t\ttransaction: &pilosa.Transaction{},\n\t\t},\n\t\t{\n\t\t\tname: \"basic\",\n\t\t\ttransaction: &pilosa.Transaction{\n\t\t\t\tID:        \"blah\",\n\t\t\t\tActive:    true,\n\t\t\t\tExclusive: true,\n\t\t\t\tTimeout:   time.Minute,\n\t\t\t\tDeadline:  time.Now(),\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tst := range tests {\n\t\tt.Run(tst.name, func(t *testing.T) {\n\t\t\tbytes, err := json.Marshal(tst.transaction)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"marshalling: %v\", err)\n\t\t\t}\n\n\t\t\tnt := &pilosa.Transaction{}\n\t\t\terr = json.Unmarshal(bytes, nt)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"unmarshalling: %v\", err)\n\t\t\t}\n\n\t\t\ttest.CompareTransactions(t, tst.transaction, nt)\n\t\t})\n\t}\n}\n\nfunc TestUnmarshalTransaction(t *testing.T) {\n\ttests := []struct {\n\t\tname            string\n\t\ttransactionJSON string\n\t\texp             *pilosa.Transaction\n\t}{\n\t\t{\n\t\t\tname:            \"empty\",\n\t\t\ttransactionJSON: `{}`,\n\t\t\texp:             &pilosa.Transaction{},\n\t\t},\n\t\t{\n\t\t\tname:            \"basicPost\",\n\t\t\ttransactionJSON: `{\"id\": \"blah\", \"exclusive\": false, \"timeout\": \"1m\"}`,\n\t\t\texp:             &pilosa.Transaction{ID: \"blah\", Timeout: time.Minute},\n\t\t},\n\t\t{\n\t\t\tname:            \"basicPostFloatTimeout\",\n\t\t\ttransactionJSON: `{\"id\": \"blah\", \"exclusive\": false, \"timeout\": 10.5}`,\n\t\t\texp:             &pilosa.Transaction{ID: \"blah\", Timeout: time.Second*10 + time.Second/2},\n\t\t},\n\t}\n\n\tfor _, tst := range tests {\n\t\tt.Run(tst.name, func(t *testing.T) {\n\t\t\tnt := &pilosa.Transaction{}\n\t\t\terr := json.Unmarshal([]byte(tst.transactionJSON), nt)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"unmarshalling: %v\", err)\n\t\t\t}\n\n\t\t\ttest.CompareTransactions(t, tst.exp, nt)\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "translate.go",
          "type": "blob",
          "size": 9.4970703125,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"sync\"\n\n\t\"github.com/featurebasedb/featurebase/v3/disco\"\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\t\"github.com/pkg/errors\"\n)\n\nconst (\n\t// translateStoreDir is the subdirctory into which the partitioned\n\t// translate store data is stored.\n\ttranslateStoreDir = \"_keys\"\n)\n\n// Translate store errors.\nvar (\n\tErrTranslateStoreClosed       = errors.New(\"translate store closed\")\n\tErrTranslateStoreReaderClosed = errors.New(\"translate store reader closed\")\n\tErrReplicationNotSupported    = errors.New(\"replication not supported\")\n\tErrTranslateStoreReadOnly     = errors.New(\"translate store could not find or create key, translate store read only\")\n\tErrTranslateStoreNotFound     = errors.New(\"translate store not found\")\n\tErrTranslatingKeyNotFound     = errors.New(\"translating key not found\")\n)\n\n// TranslateStore is the storage for translation string-to-uint64 values.\n// For BoltDB implementation an empty string will be converted into the sentinel byte slice:\n//\n//\tvar emptyKey = []byte{\n//\t\t0x00, 0x00, 0x00,\n//\t\t0x4d, 0x54, 0x4d, 0x54, // MTMT\n//\t\t0x00,\n//\t\t0xc2, 0xa0, // NO-BREAK SPACE\n//\t\t0x00,\n//\t}\ntype TranslateStore interface { // TODO: refactor this interface; readonly should be part of the type and replication should be an impl detail\n\tio.Closer\n\n\t// Returns the maximum ID set on the store.\n\tMaxID() (uint64, error)\n\n\t// Retrieves the partition ID associated with the store.\n\t// Only applies to index stores.\n\tPartitionID() int\n\n\t// Sets & retrieves whether the store is read-only.\n\tReadOnly() bool\n\tSetReadOnly(v bool)\n\n\t// FindKeys looks up the ID for each key.\n\t// Keys are not created if they do not exist.\n\t// Missing keys are not considered errors, so the length of the result may be less than that of the input.\n\tFindKeys(keys ...string) (map[string]uint64, error)\n\n\t// CreateKeys maps all keys to IDs, creating the IDs if they do not exist.\n\t// If the translator is read-only, this will return an error.\n\tCreateKeys(keys ...string) (map[string]uint64, error)\n\n\t// Match finds IDs of strings matching the filter.\n\tMatch(filter func([]byte) bool) ([]uint64, error)\n\n\t// Converts an integer ID to its associated string key.\n\tTranslateID(id uint64) (string, error)\n\tTranslateIDs(id []uint64) ([]string, error)\n\n\t// Forces the write of a key/id pair, even if read only. Used by replication.\n\tForceSet(id uint64, key string) error\n\n\t// Returns a reader from the given ID offset.\n\tEntryReader(ctx context.Context, offset uint64) (TranslateEntryReader, error)\n\n\tBegin(write bool) (TranslatorTx, error)\n\n\t// ReadFrom ensures that the TranslateStore implements io.ReaderFrom.\n\t// It should read from the reader and replace the data store with\n\t// the read payload.\n\tReadFrom(io.Reader) (int64, error)\n\n\tDelete(records *roaring.Bitmap) (Commitor, error)\n}\n\n// TranslatorTx reproduces a subset of the methods on the BoltDB Tx\n// object. Others may be needed in the future and we should just add\n// them here. The idea is not to scatter direct references to bolt\n// stuff throughout the whole codebase.\ntype TranslatorTx interface {\n\tWriteTo(io.Writer) (int64, error)\n\tRollback() error\n\t// e.g. Commit() error\n}\n\n// OpenTranslateStoreFunc represents a function for instantiating and opening a TranslateStore.\ntype OpenTranslateStoreFunc func(path, index, field string, partitionID, partitionN int, fsyncEnabled bool) (TranslateStore, error)\n\n// GenerateNextPartitionedID returns the next ID within the same partition.\nfunc GenerateNextPartitionedID(index string, prev uint64, partitionID, partitionN int) uint64 {\n\t// If the translation store is not partitioned, just return\n\t// the next ID.\n\tif partitionID == -1 {\n\t\treturn prev + 1\n\t}\n\t// Try to use the next ID if it is in the same partition.\n\t// Otherwise find ID in next shard that has a matching partition.\n\tfor id := prev + 1; ; id += ShardWidth {\n\t\tif disco.ShardToShardPartition(index, id/ShardWidth, partitionN) == partitionID {\n\t\t\treturn id\n\t\t}\n\t}\n}\n\n// TranslateEntryReader represents a stream of translation entries.\ntype TranslateEntryReader interface {\n\tio.Closer\n\tReadEntry(entry *TranslateEntry) error\n}\n\n// OpenTranslateReaderFunc represents a function for instantiating and opening a TranslateStore.\ntype OpenTranslateReaderFunc func(ctx context.Context, nodeURL string, offsets TranslateOffsetMap) (TranslateEntryReader, error)\n\n// TranslateEntry represents a key/ID pair from a TranslateStore.\ntype TranslateEntry struct {\n\tIndex string `json:\"index,omitempty\"`\n\tField string `json:\"field,omitempty\"`\n\tID    uint64 `json:\"id,omitempty\"`\n\tKey   string `json:\"key,omitempty\"`\n}\n\n// MultiTranslateEntryReader reads from multiple TranslateEntryReader instances\n// and merges them into a single reader.\ntype MultiTranslateEntryReader struct {\n\tctx    context.Context\n\tcancel func()\n\n\twg sync.WaitGroup\n\n\tch      chan readEntryResponse\n\treaders []TranslateEntryReader\n}\n\n// NewMultiTranslateEntryReader returns a new instance of MultiTranslateEntryReader.\nfunc NewMultiTranslateEntryReader(ctx context.Context, readers []TranslateEntryReader) *MultiTranslateEntryReader {\n\tr := &MultiTranslateEntryReader{\n\t\treaders: readers,\n\t\tch:      make(chan readEntryResponse),\n\t}\n\tr.ctx, r.cancel = context.WithCancel(ctx)\n\n\tr.wg.Add(len(r.readers))\n\tfor i := range r.readers {\n\t\tgo func(tr TranslateEntryReader) { defer r.wg.Done(); r.monitor(tr) }(r.readers[i])\n\t}\n\n\treturn r\n}\n\n// Close stops the reader & child readers and waits for all goroutines to stop.\nfunc (r *MultiTranslateEntryReader) Close() error {\n\tr.cancel()\n\tfor i := range r.readers {\n\t\tr.readers[i].Close() // nolint: errcheck\n\t}\n\tr.wg.Wait()\n\treturn nil\n}\n\n// ReadEntry reads the next available entry into entry. Returns an error if\n// any of the child readers error. Returns io.EOF if reader is closed.\nfunc (r *MultiTranslateEntryReader) ReadEntry(entry *TranslateEntry) error {\n\tif len(r.readers) == 0 {\n\t\treturn io.EOF\n\t}\n\n\tselect {\n\tcase <-r.ctx.Done():\n\t\treturn io.EOF\n\tcase resp := <-r.ch:\n\t\tif resp.err != nil {\n\t\t\treturn resp.err\n\t\t}\n\t\t*entry = resp.entry\n\t\treturn nil\n\t}\n}\n\n// monitor runs in a separate goroutine and sends entry reads to the channel.\nfunc (r *MultiTranslateEntryReader) monitor(tr TranslateEntryReader) {\n\tfor {\n\t\tvar entry TranslateEntry\n\t\terr := tr.ReadEntry(&entry)\n\n\t\tselect {\n\t\tcase <-r.ctx.Done():\n\t\t\treturn\n\t\tcase r.ch <- readEntryResponse{entry: entry, err: err}:\n\t\t}\n\t}\n}\n\ntype readEntryResponse struct {\n\tentry TranslateEntry\n\terr   error\n}\n\n// TranslateOffsetMap maintains a set of offsets for both indexes & fields.\ntype TranslateOffsetMap map[string]*IndexTranslateOffsetMap\n\n// IndexOffset returns the offset for the given index.\nfunc (m TranslateOffsetMap) IndexPartitionOffset(name string, partitionID int) uint64 {\n\tif m[name] == nil {\n\t\treturn 0\n\t}\n\treturn m[name].Partitions[partitionID]\n}\n\n// SetIndexOffset sets the offset for the given index.\nfunc (m TranslateOffsetMap) SetIndexPartitionOffset(name string, partitionID int, offset uint64) {\n\tif m[name] == nil {\n\t\tm[name] = NewIndexTranslateOffsetMap()\n\t}\n\tm[name].Partitions[partitionID] = offset\n}\n\n// FieldOffset returns the offset for the given field.\nfunc (m TranslateOffsetMap) FieldOffset(index, name string) uint64 {\n\tif m[index] == nil {\n\t\treturn 0\n\t}\n\treturn m[index].Fields[name]\n}\n\n// Empty reports whether there are any actual entries in the map. This\n// is distinct from len(m) == 0 in that an entry in this map which is\n// itself empty doesn't count as non-empty.\nfunc (m TranslateOffsetMap) Empty() bool {\n\tfor _, sub := range m {\n\t\tif !sub.Empty() {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// SetFieldOffset sets the offset for the given field.\nfunc (m TranslateOffsetMap) SetFieldOffset(index, name string, offset uint64) {\n\tif m[index] == nil {\n\t\tm[index] = NewIndexTranslateOffsetMap()\n\t}\n\tm[index].Fields[name] = offset\n}\n\ntype IndexTranslateOffsetMap struct {\n\tPartitions map[int]uint64    `json:\"partitions\"`\n\tFields     map[string]uint64 `json:\"fields\"`\n}\n\n// Empty reports whether this map has neither partitions nor fields.\nfunc (i *IndexTranslateOffsetMap) Empty() bool {\n\treturn len(i.Partitions) == 0 && len(i.Fields) == 0\n}\n\nfunc NewIndexTranslateOffsetMap() *IndexTranslateOffsetMap {\n\treturn &IndexTranslateOffsetMap{\n\t\tPartitions: make(map[int]uint64),\n\t\tFields:     make(map[string]uint64),\n\t}\n}\n\nvar _ OpenTranslateStoreFunc = OpenInMemTranslateStore\n\n// OpenInMemTranslateStore returns a new instance of a BoltDB based\n// TranslateStore which removes all its files when it's closed, and\n// tries to operate off a RAM disk if one is configured and set in the\n// environment.  Implements OpenTranslateStoreFunc.\nfunc OpenInMemTranslateStore(rawurl, index, field string, partitionID, partitionN int, fsyncEnabled bool) (TranslateStore, error) {\n\tbt := NewBoltTranslateStore(index, field, partitionID, partitionN, false)\n\tiname := index\n\tif len(iname) > 10 {\n\t\tiname = iname[:10]\n\t}\n\tfname := field\n\tif len(fname) > 10 {\n\t\tfname = fname[:10]\n\t}\n\n\ttf, err := os.CreateTemp(\"\", fmt.Sprintf(\"bolt-i%s-f%s-%d-%d-\", iname, fname, partitionID, partitionN))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"making temp file for boltdb key translation\")\n\t}\n\tbt.Path = tf.Name()\n\terr = bt.Open()\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"opening in mem boltdb\")\n\t}\n\treturn &BoltInMemTranslateStore{bt}, err\n}\n\ntype BoltInMemTranslateStore struct {\n\t*BoltTranslateStore\n}\n\nfunc (b *BoltInMemTranslateStore) Close() error {\n\tdefer os.RemoveAll(b.BoltTranslateStore.Path)\n\terr := b.BoltTranslateStore.Close()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"closing in mem bolt translate store\")\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "translate_boltdb.go",
          "type": "blob",
          "size": 17.5703125,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime/pprof\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\t\"github.com/pkg/errors\"\n\tbolt \"go.etcd.io/bbolt\"\n)\n\nvar _ = pprof.StartCPUProfile\n\nvar (\n\t// ErrTranslateStoreClosed is returned when reading from an TranslateEntryReader\n\t// and the underlying store is closed.\n\tErrBoltTranslateStoreClosed = errors.New(\"boltdb: translate store closing\")\n\n\t// ErrTranslateKeyNotFound is returned when translating key\n\t// and the underlying store returns an empty set\n\tErrTranslateKeyNotFound = errors.New(\"boltdb: translating key returned empty set\")\n\n\tbucketKeys = []byte(\"keys\")\n\tbucketIDs  = []byte(\"ids\")\n\tbucketFree = []byte(\"free\")\n\tfreeKey    = []byte(\"free\")\n)\n\nconst (\n\t// snapshotExt is the file extension used for an in-process snapshot.\n\tsnapshotExt = \".snapshotting\"\n\n\terrFmtTranslateBucketNotFound = \"boltdb: translate bucket '%s' not found\"\n)\n\n// OpenTranslateStore opens and initializes a boltdb translation store.\nfunc OpenTranslateStore(path, index, field string, partitionID, partitionN int, fsyncEnabled bool) (TranslateStore, error) {\n\ts := NewBoltTranslateStore(index, field, partitionID, partitionN, fsyncEnabled)\n\ts.Path = path\n\tif err := s.Open(); err != nil {\n\t\treturn nil, err\n\t}\n\treturn s, nil\n}\n\n// Ensure type implements interface.\nvar _ TranslateStore = &BoltTranslateStore{}\n\n// BoltTranslateStore is an on-disk storage engine for translating string-to-uint64 values.\n// An empty string will be converted into the sentinel byte slice:\n//\n//\tvar emptyKey = []byte{\n//\t\t0x00, 0x00, 0x00,\n//\t\t0x4d, 0x54, 0x4d, 0x54, // MTMT\n//\t\t0x00,\n//\t\t0xc2, 0xa0, // NO-BREAK SPACE\n//\t\t0x00,\n//\t}\ntype BoltTranslateStore struct {\n\tmu sync.RWMutex\n\tdb *bolt.DB\n\n\tindex       string\n\tfield       string\n\tpartitionID int\n\tpartitionN  int\n\n\tonce    sync.Once\n\tclosing chan struct{}\n\n\treadOnly     bool\n\tfsyncEnabled bool\n\twriteNotify  chan struct{}\n\n\t// File path to database file.\n\tPath string\n}\n\n// NewBoltTranslateStore returns a new instance of TranslateStore.\nfunc NewBoltTranslateStore(index, field string, partitionID, partitionN int, fsyncEnabled bool) *BoltTranslateStore {\n\treturn &BoltTranslateStore{\n\t\tindex:        index,\n\t\tfield:        field,\n\t\tpartitionID:  partitionID,\n\t\tpartitionN:   partitionN,\n\t\tclosing:      make(chan struct{}),\n\t\twriteNotify:  make(chan struct{}),\n\t\tfsyncEnabled: fsyncEnabled,\n\t}\n}\n\n// Open opens the translate file.\nfunc (s *BoltTranslateStore) Open() (err error) {\n\t// add the path to the problem database if we panic handling it.\n\tdefer func() {\n\t\tr := recover()\n\t\tif r != nil {\n\t\t\tpanic(fmt.Sprintf(\"pilosa/boltdb/TranslateStore.Open(s.Path='%v') panic with '%v'\", s.Path, r))\n\t\t}\n\t}()\n\n\tif err := os.MkdirAll(filepath.Dir(s.Path), 0o750); err != nil {\n\t\treturn errors.Wrapf(err, \"mkdir %s\", filepath.Dir(s.Path))\n\t} else if s.db, err = bolt.Open(s.Path, 0o600, &bolt.Options{Timeout: 1 * time.Second, NoSync: !s.fsyncEnabled, InitialMmapSize: 0}); err != nil {\n\t\treturn errors.Wrapf(err, \"open file: %s\", err)\n\t}\n\n\t// Initialize buckets.\n\tif err := s.db.Update(func(tx *bolt.Tx) error {\n\t\tif _, err := tx.CreateBucketIfNotExists(bucketKeys); err != nil {\n\t\t\treturn err\n\t\t} else if _, err := tx.CreateBucketIfNotExists(bucketIDs); err != nil {\n\t\t\treturn err\n\t\t} else if _, err := tx.CreateBucketIfNotExists(bucketFree); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t}); err != nil {\n\t\ts.db.Close()\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// Close closes the underlying database.\nfunc (s *BoltTranslateStore) Close() (err error) {\n\ts.once.Do(func() { close(s.closing) })\n\n\tif s.db != nil {\n\t\tif err := s.db.Close(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// PartitionID returns the partition id the store was initialized with.\nfunc (s *BoltTranslateStore) PartitionID() int {\n\treturn s.partitionID\n}\n\n// ReadOnly returns true if the store is in read-only mode.\nfunc (s *BoltTranslateStore) ReadOnly() bool {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\treturn s.readOnly\n}\n\n// SetReadOnly toggles whether store is in read-only mode.\nfunc (s *BoltTranslateStore) SetReadOnly(v bool) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\ts.readOnly = v\n}\n\n// Size returns the number of bytes in the data file.\nfunc (s *BoltTranslateStore) Size() int64 {\n\tif s.db == nil {\n\t\treturn 0\n\t}\n\ttx, err := s.db.Begin(false)\n\tif err != nil {\n\t\treturn 0\n\t}\n\tdefer func() { _ = tx.Rollback() }()\n\treturn tx.Size()\n}\n\n// FindKeys looks up the ID for each key.\n// Keys are not created if they do not exist.\n// Missing keys are not considered errors, so the length of the result may be less than that of the input.\nfunc (s *BoltTranslateStore) FindKeys(keys ...string) (map[string]uint64, error) {\n\tresult := make(map[string]uint64, len(keys))\n\terr := s.db.View(func(tx *bolt.Tx) error {\n\t\tbkt := tx.Bucket(bucketKeys)\n\t\tif bkt == nil {\n\t\t\treturn errors.Errorf(errFmtTranslateBucketNotFound, bucketKeys)\n\t\t}\n\t\tfor _, key := range keys {\n\t\t\tid, _ := findIDByKey(bkt, key)\n\t\t\tif id == 0 {\n\t\t\t\t// The key does not exist.\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tresult[key] = id\n\t\t}\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn result, nil\n}\n\n// translateTransactionSize governs the number of writes to a single\n// boltDB bucket we will make in a single db.Update(), before starting\n// a new Update. We do this because Put() is quadratic, but Commit is\n// expensive enough that we want to do a fair number of updates before\n// paying for it.\nconst translateTransactionSize = 16384\n\n// CreateKeys maps all keys to IDs, creating the IDs if they do not exist.\n// If the translator is read-only, this will return an error.\nfunc (s *BoltTranslateStore) CreateKeys(keys ...string) (map[string]uint64, error) {\n\tif s.ReadOnly() {\n\t\treturn nil, ErrTranslateStoreReadOnly\n\t}\n\n\twritten := false\n\tresult := make(map[string]uint64, len(keys))\n\tidScratch := make([]byte, translateTransactionSize*8)\n\tfor len(keys) > 0 {\n\t\t// boltdb performs badly if you write really large numbers of\n\t\t// keys all at once...\n\t\terr := s.db.Update(func(tx *bolt.Tx) error {\n\t\t\tkeyBucket := tx.Bucket(bucketKeys)\n\t\t\tif keyBucket == nil {\n\t\t\t\treturn errors.Errorf(errFmtTranslateBucketNotFound, bucketKeys)\n\t\t\t}\n\t\t\tidBucket := tx.Bucket(bucketIDs)\n\t\t\tif idBucket == nil {\n\t\t\t\treturn errors.Errorf(errFmtTranslateBucketNotFound, bucketIDs)\n\t\t\t}\n\t\t\tfreeBucket := tx.Bucket(bucketFree)\n\t\t\tif freeBucket == nil {\n\t\t\t\treturn errors.Errorf(errFmtTranslateBucketNotFound, bucketFree)\n\t\t\t}\n\t\t\tputs := 0\n\n\t\t\t// we create a freeIDGetter to reduce marshalling\n\t\t\tgetter := newFreeIDGetter(freeBucket)\n\t\t\tdefer getter.Close()\n\n\t\t\tfor idx, key := range keys {\n\t\t\t\tid, boltKey := findIDByKey(keyBucket, key)\n\t\t\t\tif id != 0 {\n\t\t\t\t\tresult[key] = id\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\t// see if we can re-use any IDs first\n\t\t\t\tif id = getter.GetFreeID(); id == 0 {\n\t\t\t\t\tid = GenerateNextPartitionedID(s.index, maxID(tx), s.partitionID, s.partitionN)\n\t\t\t\t}\n\t\t\t\tidBytes := idScratch[puts*8 : puts*8+8]\n\t\t\t\tbinary.BigEndian.PutUint64(idBytes, id)\n\t\t\t\tputs++\n\t\t\t\tif err := keyBucket.Put(boltKey, idBytes); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t} else if err := idBucket.Put(idBytes, boltKey); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tresult[key] = id\n\t\t\t\twritten = true\n\t\t\t\tif puts == translateTransactionSize {\n\t\t\t\t\tkeys = keys[idx+1:]\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\t\t\t}\n\t\t\tkeys = keys[len(keys):]\n\t\t\treturn nil\n\t\t})\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tif written {\n\t\ts.notifyWrite()\n\t}\n\n\treturn result, nil\n}\n\n// Match finds the IDs of all keys matching a filter.\nfunc (s *BoltTranslateStore) Match(filter func([]byte) bool) ([]uint64, error) {\n\tvar matches []uint64\n\terr := s.db.View(func(tx *bolt.Tx) error {\n\t\t// This uses the id bucket instead of the key bucket so that matches are produced in sorted order.\n\t\tidBucket := tx.Bucket(bucketIDs)\n\t\tif idBucket == nil {\n\t\t\treturn errors.Errorf(errFmtTranslateBucketNotFound, bucketIDs)\n\t\t}\n\n\t\treturn idBucket.ForEach(func(id, key []byte) error {\n\t\t\tif bytes.Equal(key, emptyKey) {\n\t\t\t\tkey = nil\n\t\t\t}\n\n\t\t\tif filter(key) {\n\t\t\t\tmatches = append(matches, btou64(id))\n\t\t\t}\n\n\t\t\treturn nil\n\t\t})\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn matches, nil\n}\n\n// TranslateID converts an integer ID to a string key.\n// Returns a blank string if ID does not exist.\nfunc (s *BoltTranslateStore) TranslateID(id uint64) (string, error) {\n\ttx, err := s.db.Begin(false)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tdefer func() { _ = tx.Rollback() }()\n\treturn findKeyByID(tx.Bucket(bucketIDs), id), nil\n}\n\n// TranslateIDs converts a list of integer IDs to a list of string keys.\nfunc (s *BoltTranslateStore) TranslateIDs(ids []uint64) ([]string, error) {\n\tif len(ids) == 0 {\n\t\treturn nil, nil\n\t}\n\n\ttx, err := s.db.Begin(false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() { _ = tx.Rollback() }()\n\n\tbucket := tx.Bucket(bucketIDs)\n\n\tkeys := make([]string, len(ids))\n\tfor i, id := range ids {\n\t\tkeys[i] = findKeyByID(bucket, id)\n\t}\n\treturn keys, nil\n}\n\n// ForceSet writes the id/key pair to the store even if read only. Used by replication.\nfunc (s *BoltTranslateStore) ForceSet(id uint64, key string) error {\n\tif err := s.db.Update(func(tx *bolt.Tx) (err error) {\n\t\tif err := tx.Bucket(bucketKeys).Put([]byte(key), u64tob(id)); err != nil {\n\t\t\treturn err\n\t\t} else if err := tx.Bucket(bucketIDs).Put(u64tob(id), []byte(key)); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t}); err != nil {\n\t\treturn err\n\t}\n\ts.notifyWrite()\n\treturn nil\n}\n\n// EntryReader returns a reader that streams the underlying data file.\nfunc (s *BoltTranslateStore) EntryReader(ctx context.Context, offset uint64) (TranslateEntryReader, error) {\n\tctx, cancel := context.WithCancel(ctx)\n\treturn &BoltTranslateEntryReader{ctx: ctx, cancel: cancel, store: s, offset: offset}, nil\n}\n\n// WriteNotify returns a channel that is closed when a new entry is written.\nfunc (s *BoltTranslateStore) WriteNotify() <-chan struct{} {\n\ts.mu.RLock()\n\tch := s.writeNotify\n\ts.mu.RUnlock()\n\treturn ch\n}\n\n// notifyWrite sends a write notification under write lock.\nfunc (s *BoltTranslateStore) notifyWrite() {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\tclose(s.writeNotify)\n\ts.writeNotify = make(chan struct{})\n}\n\n// MaxID returns the highest id in the store.\nfunc (s *BoltTranslateStore) MaxID() (max uint64, err error) {\n\tif err := s.db.View(func(tx *bolt.Tx) error {\n\t\tmax = maxID(tx)\n\t\treturn nil\n\t}); err != nil {\n\t\treturn 0, err\n\t}\n\treturn max, nil\n}\n\n// Begin starts and returns a transaction on the underlying store.\nfunc (s *BoltTranslateStore) Begin(write bool) (TranslatorTx, error) {\n\treturn s.db.Begin(write)\n}\n\n// ReadFrom reads the content and overwrites the existing store.\nfunc (s *BoltTranslateStore) ReadFrom(r io.Reader) (n int64, err error) {\n\t// Close store.\n\tif err := s.Close(); err != nil {\n\t\treturn 0, errors.Wrap(err, \"closing store\")\n\t}\n\n\t// Create a temporary file to snapshot to.\n\tsnapshotPath := s.Path + snapshotExt\n\tfile, err := os.Create(snapshotPath)\n\tif err != nil {\n\t\treturn n, errors.Wrap(err, \"creating snapshot file\")\n\t}\n\n\t// Write payload to snapshot.\n\tif n, err = io.Copy(file, r); err != nil {\n\t\tfile.Close()\n\t\treturn n, errors.Wrap(err, \"snapshot write to\")\n\t}\n\n\t// we close the file here so we don't still have it open when trying\n\t// to open it in a moment.\n\tfile.Close()\n\n\t// Move snapshot to data file location.\n\tif err := os.Rename(snapshotPath, s.Path); err != nil {\n\t\treturn n, errors.Wrap(err, \"renaming snapshot\")\n\t}\n\n\t// Re-open the store.\n\tif err := s.Open(); err != nil {\n\t\treturn n, errors.Wrap(err, \"re-opening store\")\n\t}\n\n\treturn n, nil\n}\n\n// MaxID returns the highest id in the store.\nfunc maxID(tx *bolt.Tx) uint64 {\n\tif key, _ := tx.Bucket(bucketIDs).Cursor().Last(); key != nil {\n\t\treturn btou64(key)\n\t}\n\treturn 0\n}\n\ntype BoltTranslateEntryReader struct {\n\tctx    context.Context\n\tstore  *BoltTranslateStore\n\toffset uint64\n\tcancel func()\n}\n\n// Close closes the reader.\nfunc (r *BoltTranslateEntryReader) Close() error {\n\tr.cancel()\n\treturn nil\n}\n\n// ReadEntry reads the next entry from the underlying translate store.\nfunc (r *BoltTranslateEntryReader) ReadEntry(entry *TranslateEntry) error {\n\t// Ensure reader has not been closed before read.\n\tselect {\n\tcase <-r.ctx.Done():\n\t\treturn r.ctx.Err()\n\tcase <-r.store.closing:\n\t\treturn ErrBoltTranslateStoreClosed\n\tdefault:\n\t}\n\n\tfor {\n\t\t// Obtain notification channel before read to ensure concurrency issues.\n\t\twriteNotify := r.store.WriteNotify()\n\n\t\t// Find next ID/key pair in transaction.\n\t\tvar found bool\n\t\tif err := r.store.db.View(func(tx *bolt.Tx) error {\n\t\t\t// Find ID/key lookup at offset or later.\n\t\t\tcur := tx.Bucket(bucketIDs).Cursor()\n\t\t\tkey, value := cur.Seek(u64tob(r.offset))\n\t\t\tif key == nil {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\t// Copy ID & key to entry and mark as found.\n\t\t\tfound = true\n\t\t\tentry.Index = r.store.index\n\t\t\tentry.Field = r.store.field\n\t\t\tentry.ID = btou64(key)\n\t\t\tentry.Key = string(value)\n\n\t\t\t// Update offset position.\n\t\t\tr.offset = entry.ID + 1\n\n\t\t\treturn nil\n\t\t}); err != nil {\n\t\t\treturn err\n\t\t} else if found {\n\t\t\treturn nil\n\t\t}\n\n\t\t// If no entry found, wait for new write or reader close.\n\t\tselect {\n\t\tcase <-r.ctx.Done():\n\t\t\treturn r.ctx.Err()\n\t\tcase <-r.store.closing:\n\t\t\treturn ErrBoltTranslateStoreClosed\n\t\tcase <-writeNotify:\n\t\t}\n\t}\n}\n\ntype boltWrapper struct {\n\ttx *bolt.Tx\n}\n\nfunc (w *boltWrapper) Commit() error {\n\tif w.tx != nil {\n\t\treturn w.tx.Commit()\n\t}\n\treturn nil\n}\n\nfunc (w *boltWrapper) Rollback() {\n\tif w.tx != nil {\n\t\tw.tx.Rollback()\n\t}\n}\n\nfunc (s *BoltTranslateStore) FreeIDs() (*roaring.Bitmap, error) {\n\tresult := roaring.NewBitmap()\n\terr := s.db.View(func(tx *bolt.Tx) error {\n\t\tbkt := tx.Bucket(bucketFree)\n\t\tif bkt == nil {\n\t\t\treturn errors.Errorf(errFmtTranslateBucketNotFound, bucketKeys)\n\t\t}\n\t\tb := bkt.Get(freeKey)\n\t\terr := result.UnmarshalBinary(b)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n\treturn result, err\n}\n\nfunc (s *BoltTranslateStore) MergeFree(tx *bolt.Tx, newIDs *roaring.Bitmap) error {\n\tbkt := tx.Bucket(bucketFree)\n\tb := bkt.Get(freeKey)\n\tbuf := new(bytes.Buffer)\n\tif b != nil { // if existing combine with newIDs\n\t\tbefore := roaring.NewBitmap()\n\t\terr := before.UnmarshalBinary(b)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfinal := newIDs.Union(before)\n\t\t_, err = final.WriteTo(buf)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\tnewIDs.WriteTo(buf)\n\t}\n\treturn bkt.Put(freeKey, buf.Bytes())\n}\n\n// Delete removes the lookeup pairs in order to make avialble for reuse but doesn't commit the\n// transaction for that is tied to the associated rbf transaction being successful\nfunc (s *BoltTranslateStore) Delete(records *roaring.Bitmap) (Commitor, error) {\n\ttx, err := s.db.Begin(true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tkeyBucket := tx.Bucket(bucketKeys)\n\tidBucket := tx.Bucket(bucketIDs)\n\tids := records.Slice()\n\tfor i := range ids {\n\t\tid := u64tob(ids[i])\n\t\tboltKey := idBucket.Get(id)\n\t\terr = keyBucket.Delete(boltKey)\n\t\tif err != nil {\n\t\t\ttx.Rollback()\n\t\t\treturn &boltWrapper{}, err\n\t\t}\n\t\terr = idBucket.Delete(id)\n\t\tif err != nil {\n\t\t\ttx.Rollback()\n\t\t\treturn &boltWrapper{}, err\n\t\t}\n\n\t}\n\treturn &boltWrapper{tx: tx}, s.MergeFree(tx, records)\n}\n\n// emptyKey is a sentinel byte slice which stands for \"\" as a key.\nvar emptyKey = []byte{\n\t0x00, 0x00, 0x00,\n\t0x4d, 0x54, 0x4d, 0x54, // MTMT\n\t0x00,\n\t0xc2, 0xa0, // NO-BREAK SPACE\n\t0x00,\n}\n\nfunc findIDByKey(bkt *bolt.Bucket, key string) (uint64, []byte) {\n\tvar boltKey []byte\n\tif key == \"\" {\n\t\tboltKey = emptyKey\n\t} else {\n\t\tboltKey = []byte(key)\n\t}\n\n\tif value := bkt.Get(boltKey); value != nil {\n\t\treturn btou64(value), boltKey\n\t}\n\treturn 0, boltKey\n}\n\n// freeIDGetter reduces the amount of marshaling required to get multiple ids\ntype freeIDGetter struct {\n\tfreeBucket *bolt.Bucket\n\tb          *roaring.Bitmap\n\tchanged    bool\n}\n\n// newFreeIDGetter initializes a new freeIDGetter. If at any point there is a\n// failure, it returns an error.\n//\n// NOTE: For changes to be persisted to the bucket, you must call\n// (*freeIDGetter).Close()\nfunc newFreeIDGetter(freeBucket *bolt.Bucket) *freeIDGetter {\n\tg := &freeIDGetter{\n\t\tfreeBucket: freeBucket,\n\t}\n\t// we ignore this value because it's okay if we dont have a bitmap just yet\n\t_ = g.getBitmap()\n\treturn g\n}\n\nfunc (g *freeIDGetter) getBitmap() bool {\n\tif g.b == nil {\n\t\t// get the bitmap from freeBucket\n\t\tvalue := g.freeBucket.Get(freeKey)\n\t\tif value == nil {\n\t\t\treturn false\n\t\t}\n\t\t// turn the value into a bitmap\n\t\tb := roaring.NewBitmap()\n\t\tif err := b.UnmarshalBinary(value); err != nil {\n\t\t\treturn false\n\t\t}\n\t\tg.b = b\n\t}\n\treturn true\n}\n\n// GetFreeID tries to get a free ID from the free id bucket. If at any point it\n// fails to do so, it returns a 0. Otherwise, it returns the first free ID in the\n// bucket\nfunc (g *freeIDGetter) GetFreeID() (id uint64) {\n\tif !g.getBitmap() {\n\t\treturn 0\n\t}\n\t// get the first free id\n\tid, ok := g.b.Min()\n\tif !ok {\n\t\treturn 0\n\t}\n\t// remove that id from the free id bitmap\n\tif changed, err := g.b.RemoveN(id); changed == 0 || err != nil {\n\t\treturn 0\n\t} else {\n\t\tg.changed = true\n\t}\n\treturn id\n}\n\n// Close persists any changes to the bitmap back to the bucket and then nils the\n// references for safety.\nfunc (g *freeIDGetter) Close() error {\n\tif g.changed {\n\t\t// convert bitmap to binary\n\t\tbuf, err := g.b.MarshalBinary()\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"closing free ID Getter\")\n\t\t}\n\t\t// put updated bitmap back into the freeBucket\n\t\tif err := g.freeBucket.Put(freeKey, buf); err != nil {\n\t\t\treturn errors.Wrap(err, \"closing free ID Getter\")\n\t\t}\n\t}\n\tg.b = nil\n\tg.freeBucket = nil\n\treturn nil\n}\n\nfunc findKeyByID(bkt *bolt.Bucket, id uint64) string {\n\tboltKey := bkt.Get(u64tob(id))\n\tif bytes.Equal(boltKey, emptyKey) {\n\t\treturn \"\"\n\t}\n\treturn string(boltKey)\n}\n\n// u64tob encodes v to big endian encoding.\nfunc u64tob(v uint64) []byte {\n\tb := make([]byte, 8)\n\tbinary.BigEndian.PutUint64(b, v)\n\treturn b\n}\n\n// btou64 decodes b from big endian encoding.\nfunc btou64(b []byte) uint64 { return binary.BigEndian.Uint64(b) }\n"
        },
        {
          "name": "translate_boltdb_internal_test.go",
          "type": "blob",
          "size": 2.6787109375,
          "content": "package pilosa\n\nimport (\n\t\"path/filepath\"\n\t\"testing\"\n\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\tbolt \"go.etcd.io/bbolt\"\n)\n\nfunc TestGetFreeID(t *testing.T) {\n\tboltDir := t.TempDir()\n\tdb, err := bolt.Open(filepath.Join(boltDir, \"testDB\"), 0600, nil)\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected error opening test boltdb: %v\", err)\n\t}\n\tdefer db.Close()\n\n\tmakeTestBucket := func(tx *bolt.Tx, b *roaring.Bitmap) *bolt.Bucket {\n\t\tif b == nil {\n\t\t\tt.Fatalf(\"unexpected nil bitmap\")\n\t\t}\n\t\tfree, err := tx.CreateBucketIfNotExists(bucketFree)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unexpected error making freeBucket: %v\", err)\n\t\t}\n\t\tbuf, err := b.MarshalBinary()\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unexpected error marshaling bitmap (%v) to binary: %v\", b, err)\n\t\t}\n\t\tif err := free.Put(freeKey, buf); err != nil {\n\t\t\tt.Fatalf(\"unexpected error adding data (%v) to freeBucket: %v\", b, err)\n\t\t}\n\t\treturn free\n\t}\n\n\tfor name, test := range map[string]struct {\n\t\tbits *roaring.Bitmap\n\t\twant uint64\n\t}{\n\t\t\"bucket is there, but nobody's home\": {\n\t\t\tbits: roaring.NewBitmap(),\n\t\t\twant: 0,\n\t\t},\n\t\t\"good bucket\": {\n\t\t\tbits: roaring.NewBitmap(1, 2, 34, 55, 9000),\n\t\t\twant: 1,\n\t\t},\n\t} {\n\t\tt.Run(name, func(t *testing.T) {\n\t\t\ttx, err := db.Begin(true)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"unexpected error starting bolt transaction: %v\", err)\n\t\t\t}\n\t\t\tdefer tx.Rollback()\n\t\t\tfreeBucket := makeTestBucket(tx, test.bits)\n\n\t\t\tgetter := newFreeIDGetter(freeBucket)\n\t\t\tdefer getter.Close()\n\t\t\tif got := getter.GetFreeID(); got != test.want {\n\t\t\t\tt.Fatalf(\"expected %v got %v\", test.want, got)\n\t\t\t}\n\t\t})\n\t}\n\n\tt.Run(\"CorrectOrdering\", func(t *testing.T) {\n\t\ttx, err := db.Begin(true)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unexpected error starting bolt transaction: %v\", err)\n\t\t}\n\t\tdefer tx.Rollback()\n\n\t\tbucket := makeTestBucket(tx, roaring.NewBitmap(1, 34, 2, 55, 9000))\n\n\t\tgetter := newFreeIDGetter(bucket)\n\t\tdefer getter.Close()\n\t\tfor _, want := range []uint64{1, 2, 34, 55, 9000} {\n\t\t\tif got := getter.GetFreeID(); got != want {\n\t\t\t\tt.Fatalf(\"expected %v got %v\", want, got)\n\t\t\t}\n\t\t}\n\t\tif got := getter.GetFreeID(); got != 0 {\n\t\t\tt.Fatalf(\"expected 0 got %v\", got)\n\t\t}\n\t})\n\n\tt.Run(\"NotABitmap\", func(t *testing.T) {\n\t\ttx, err := db.Begin(true)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unexpected error starting bolt transaction: %v\", err)\n\t\t}\n\t\tdefer tx.Rollback()\n\n\t\tfree, err := tx.CreateBucketIfNotExists(bucketFree)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"unexpected error making freeBucket: %v\", err)\n\t\t}\n\t\tif err := free.Put(freeKey, []byte(\"this isn't right!\")); err != nil {\n\t\t\tt.Fatalf(\"unexpected error adding data to freeBucket: %v\", err)\n\t\t}\n\t\tgetter := newFreeIDGetter(free)\n\t\tdefer getter.Close()\n\t\tif got := getter.GetFreeID(); got != 0 {\n\t\t\tt.Fatalf(\"expected 0 got %v\", got)\n\t\t}\n\t})\n}\n"
        },
        {
          "name": "translate_boltdb_test.go",
          "type": "blob",
          "size": 13.09375,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa_test\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"strconv\"\n\t\"testing\"\n\t\"time\"\n\n\tpilosa \"github.com/featurebasedb/featurebase/v3\"\n\t\"github.com/featurebasedb/featurebase/v3/disco\"\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\t\"github.com/featurebasedb/featurebase/v3/testhook\"\n\t\"github.com/stretchr/testify/require\"\n)\n\n//var vv = pilosa.VV\n\nfunc TestTranslateStore_CreateKeys(t *testing.T) {\n\ts := MustOpenNewTranslateStore(t)\n\tdefer MustCloseTranslateStore(s)\n\n\tids, err := s.CreateKeys(\"abc\", \"abc\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t} else if _, ok := ids[\"abc\"]; !ok {\n\t\tt.Fatalf(`missing \"abc\"; got %v`, ids)\n\t} else if len(ids) > 1 {\n\t\tt.Fatalf(\"expected one key, got %d in %v\", len(ids), ids)\n\t}\n\n\t// Ensure different keys translate to different IDs.\n\tids1, err := s.CreateKeys(\"foo\", \"bar\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t} else if foo, bar := ids1[\"foo\"], ids1[\"bar\"]; foo == bar {\n\t\tt.Fatalf(`\"foo\" and \"bar\" map back to the same ID %d`, foo)\n\t}\n\n\t// Ensure retranslation returns original IDs.\n\tif ids, err := s.CreateKeys(\"bar\", \"foo\"); err != nil {\n\t\tt.Fatal(err)\n\t} else if !reflect.DeepEqual(ids, ids1) {\n\t\tt.Fatalf(\"retranslation produced result %v which is different from original translation %v\", ids, ids1)\n\t}\n\n\t// Ensure retranslating with existing and non-existing keys returns correctly.\n\tif ids, err := s.CreateKeys(\"foo\", \"baz\", \"bar\"); err != nil {\n\t\tt.Fatal(err)\n\t} else if got, want := ids[\"foo\"], ids1[\"foo\"]; got != want {\n\t\tt.Fatalf(`mismatched ID %d for \"foo\" (previously %d)`, got, want)\n\t} else if _, ok := ids[\"baz\"]; !ok {\n\t\tt.Fatalf(`missing translation for \"baz\"; got %v`, ids)\n\t} else if got, want := ids[\"bar\"], ids1[\"bar\"]; got != want {\n\t\tt.Fatalf(`mismatched ID %d for \"bar\" (previously %d)`, got, want)\n\t}\n}\n\nfunc TestTranslateStore_TranslateID(t *testing.T) {\n\ts := MustOpenNewTranslateStore(t)\n\tdefer MustCloseTranslateStore(s)\n\n\t// Setup initial keys.\n\tids, err := s.CreateKeys(\"foo\", \"bar\", \"\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Ensure IDs can be translated back to keys.\n\tfor key, id := range ids {\n\t\tk, err := s.TranslateID(id)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif k != key {\n\t\t\tt.Fatalf(\"TranslateID()=%s, want %s\", k, key)\n\t\t}\n\t}\n}\n\nfunc TestTranslateStore_TranslateIDs(t *testing.T) {\n\ts := MustOpenNewTranslateStore(t)\n\tdefer MustCloseTranslateStore(s)\n\n\t// Setup initial keys.\n\tids, err := s.CreateKeys(\"foo\", \"bar\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Ensure IDs can be translated back to keys.\n\tif keys, err := s.TranslateIDs([]uint64{ids[\"foo\"], ids[\"bar\"], 1}); err != nil {\n\t\tt.Fatal(err)\n\t} else if got, want := keys[0], \"foo\"; got != want {\n\t\tt.Fatalf(\"TranslateIDs()[0]=%s, want %s\", got, want)\n\t} else if got, want := keys[1], \"bar\"; got != want {\n\t\tt.Fatalf(\"TranslateIDs()[1]=%s, want %s\", got, want)\n\t} else if got, want := keys[2], \"\"; got != want {\n\t\tt.Fatalf(\"TranslateIDs()[2]=%s, want %s\", got, want)\n\t}\n}\n\nfunc TestTranslateStore_FindKeys(t *testing.T) {\n\tcases := []struct {\n\t\tname   string\n\t\tdata   []string\n\t\tlookup []string\n\t}{\n\t\t{\n\t\t\tname:   \"All\",\n\t\t\tdata:   []string{\"plugh\", \"xyzzy\", \"h\"},\n\t\t\tlookup: []string{\"plugh\", \"xyzzy\", \"h\"},\n\t\t},\n\t\t{\n\t\t\tname:   \"Extra\",\n\t\t\tdata:   []string{\"plugh\", \"xyzzy\", \"h\"},\n\t\t\tlookup: []string{\"plugh\", \"xyzzy\", \"h\", \"65\"},\n\t\t},\n\t\t{\n\t\t\tname:   \"None\",\n\t\t\tdata:   []string{\"a\", \"b\", \"c\"},\n\t\t\tlookup: []string{\"d\", \"e\"},\n\t\t},\n\t\t{\n\t\t\tname:   \"Empty\",\n\t\t\tlookup: []string{\"h\"},\n\t\t},\n\t\t{\n\t\t\tname: \"LookupNothing\",\n\t\t},\n\t}\n\n\tfor _, c := range cases {\n\t\tc := c\n\t\tt.Run(c.name, func(t *testing.T) {\n\t\t\ts := MustOpenNewTranslateStore(t)\n\t\t\tdefer MustCloseTranslateStore(s)\n\n\t\t\tvar naiveMap map[string]uint64\n\t\t\tif c.data != nil {\n\t\t\t\t// Load in key data.\n\t\t\t\tkeys := c.data\n\t\t\t\tids, err := s.CreateKeys(keys...)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Errorf(\"failed to import keys: %v\", err)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif len(ids) != len(keys) {\n\t\t\t\t\tt.Errorf(\"mapped %d keys to %d ids\", len(keys), len(ids))\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tnaiveMap = ids\n\t\t\t}\n\n\t\t\t// Compute expected lookup result.\n\t\t\tresult := map[string]uint64{}\n\t\t\tfor _, key := range c.lookup {\n\t\t\t\tid, ok := naiveMap[key]\n\t\t\t\tif !ok {\n\t\t\t\t\t// The key is expected to be missing.\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tresult[key] = id\n\t\t\t}\n\n\t\t\t// Find the keys.\n\t\t\tfound, err := s.FindKeys(c.lookup...)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"failed to find keys: %v\", err)\n\t\t\t} else if !reflect.DeepEqual(result, found) {\n\t\t\t\tt.Errorf(\"expected %v but found %v\", result, found)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestTranslateStore_MaxID(t *testing.T) {\n\ts := MustOpenNewTranslateStore(t)\n\tdefer MustCloseTranslateStore(s)\n\n\t// Generate a bunch of keys.\n\tvar lastk uint64\n\tfor i := 0; i < 1026; i++ {\n\t\tkey := strconv.Itoa(i)\n\t\tids, err := s.CreateKeys(key)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"translating %d: %v\", i, err)\n\t\t}\n\t\tlastk = ids[key]\n\t}\n\n\t// Verify the max ID.\n\tmax, err := s.MaxID()\n\tif err != nil {\n\t\tt.Fatalf(\"checking max ID: %v\", err)\n\t}\n\tif max != lastk {\n\t\tt.Fatalf(\"last key is %d but max is %d\", lastk, max)\n\t}\n}\n\nfunc TestBoltTranslateStore_EntryReader(t *testing.T) {\n\tt.Run(\"OK\", func(t *testing.T) {\n\t\ts := MustOpenNewTranslateStore(t)\n\t\tdefer MustCloseTranslateStore(s)\n\n\t\t// Create multiple new keys.\n\t\tids1, err := s.CreateKeys(\"foo\", \"bar\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Start reader from initial position.\n\t\tvar entry pilosa.TranslateEntry\n\t\tr, err := s.EntryReader(context.Background(), 0)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tdefer r.Close()\n\n\t\t// Read first entry.\n\t\tif err := r.ReadEntry(&entry); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := entry.ID, ids1[\"foo\"]; got != want {\n\t\t\tt.Fatalf(\"ReadEntry() ID=%d, want %d\", got, want)\n\t\t} else if got, want := entry.Key, \"foo\"; got != want {\n\t\t\tt.Fatalf(\"ReadEntry() Key=%s, want %s\", got, want)\n\t\t}\n\n\t\t// Read next entry.\n\t\tif err := r.ReadEntry(&entry); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := entry.ID, ids1[\"bar\"]; got != want {\n\t\t\tt.Fatalf(\"ReadEntry() ID=%d, want %d\", got, want)\n\t\t} else if got, want := entry.Key, \"bar\"; got != want {\n\t\t\tt.Fatalf(\"ReadEntry() Key=%s, want %s\", got, want)\n\t\t}\n\n\t\t// Insert next key while reader is open.\n\t\tids2, err := s.CreateKeys(\"baz\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Read newly created entry.\n\t\tif err := r.ReadEntry(&entry); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := entry.ID, ids2[\"baz\"]; got != want {\n\t\t\tt.Fatalf(\"ReadEntry() ID=%d, want %d\", got, want)\n\t\t} else if got, want := entry.Key, \"baz\"; got != want {\n\t\t\tt.Fatalf(\"ReadEntry() Key=%s, want %s\", got, want)\n\t\t}\n\n\t\t// Ensure reader closes cleanly.\n\t\tif err := r.Close(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t})\n\n\t// Ensure reader will read as soon as a new write comes in using WriteNotify().\n\tt.Run(\"WriteNotify\", func(t *testing.T) {\n\t\ts := MustOpenNewTranslateStore(t)\n\t\tdefer MustCloseTranslateStore(s)\n\n\t\t// Start reader from initial position.\n\t\tr, err := s.EntryReader(context.Background(), 0)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tdefer r.Close()\n\n\t\t// cache holds the translated key id so we can check it later\n\t\tcache := make(chan uint64)\n\n\t\t// Insert key in separate goroutine.\n\t\t// Sleep momentarily to reader hangs.\n\t\ttranslateErr := make(chan error)\n\t\tgo func() {\n\t\t\ttime.Sleep(100 * time.Millisecond)\n\t\t\tids, err := s.CreateKeys(\"foo\")\n\t\t\tif err != nil {\n\t\t\t\ttranslateErr <- err\n\t\t\t}\n\t\t\tcache <- ids[\"foo\"]\n\t\t}()\n\n\t\tvar entry pilosa.TranslateEntry\n\t\tif err := r.ReadEntry(&entry); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if got, want := entry.ID, <-cache; got != want {\n\t\t\tt.Fatalf(\"ReadEntry() ID=%d, want %d\", got, want)\n\t\t} else if got, want := entry.Key, \"foo\"; got != want {\n\t\t\tt.Fatalf(\"ReadEntry() Key=%s, want %s\", got, want)\n\t\t}\n\n\t\tselect {\n\t\tcase err := <-translateErr:\n\t\t\tt.Fatalf(\"translate error: %s\", err)\n\t\tdefault:\n\t\t}\n\t})\n\n\t// Ensure exits read on close.\n\tt.Run(\"Close\", func(t *testing.T) {\n\t\ts := MustOpenNewTranslateStore(t)\n\t\tdefer MustCloseTranslateStore(s)\n\n\t\t// Start reader from initial position.\n\t\tr, err := s.EntryReader(context.Background(), 0)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tdefer r.Close()\n\n\t\t// Insert key in separate goroutine.\n\t\t// Sleep momentarily to reader hangs.\n\t\tcloseErr := make(chan error)\n\t\tgo func() {\n\t\t\ttime.Sleep(100 * time.Millisecond)\n\t\t\tif err := r.Close(); err != nil {\n\t\t\t\tcloseErr <- err\n\t\t\t}\n\t\t}()\n\n\t\tvar entry pilosa.TranslateEntry\n\t\tif err := r.ReadEntry(&entry); err != context.Canceled {\n\t\t\tt.Fatalf(\"unexpected error: %#v\", err)\n\t\t}\n\n\t\tselect {\n\t\tcase err := <-closeErr:\n\t\t\tt.Fatalf(\"close error: %s\", err)\n\t\tdefault:\n\t\t}\n\t})\n\n\t// Ensure exits read on store close.\n\tt.Run(\"StoreClose\", func(t *testing.T) {\n\t\ts := MustOpenNewTranslateStore(t)\n\t\tdefer MustCloseTranslateStore(s)\n\n\t\t// Start reader from initial position.\n\t\tr, err := s.EntryReader(context.Background(), 0)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tdefer r.Close()\n\n\t\t// Insert key in separate goroutine.\n\t\t// Sleep momentarily to reader hangs.\n\t\tcloseErr := make(chan error)\n\t\tgo func() {\n\t\t\ttime.Sleep(100 * time.Millisecond)\n\t\t\tif err := s.Close(); err != nil {\n\t\t\t\tcloseErr <- err\n\t\t\t}\n\t\t}()\n\n\t\tvar entry pilosa.TranslateEntry\n\t\tif err := r.ReadEntry(&entry); err != pilosa.ErrBoltTranslateStoreClosed {\n\t\t\tt.Fatalf(\"unexpected error: %#v\", err)\n\t\t}\n\n\t\tselect {\n\t\tcase err := <-closeErr:\n\t\t\tt.Fatalf(\"close error: %s\", err)\n\t\tdefault:\n\t\t}\n\t})\n}\n\n// MustNewTranslateStore returns a new TranslateStore with a temporary path.\nfunc MustNewTranslateStore(tb testing.TB) *pilosa.BoltTranslateStore {\n\tf, err := testhook.TempFile(tb, \"translate-store\")\n\tif err != nil {\n\t\tpanic(err)\n\t} else if err := f.Close(); err != nil {\n\t\tpanic(err)\n\t}\n\n\ts := pilosa.NewBoltTranslateStore(\"I\", \"F\", 0, disco.DefaultPartitionN, false)\n\ts.Path = f.Name()\n\treturn s\n}\nfunc TestTranslateStore_Delete(t *testing.T) {\n\ts := MustOpenNewTranslateStore(t)\n\tdefer MustCloseTranslateStore(s)\n\n\t// Setup initial keys.\n\tids, err := s.CreateKeys(\"foo\", \"bar\", \"deleteme\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\trecords := roaring.NewBitmap(ids[\"deleteme\"])\n\tc, err := s.Delete(records)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif err = c.Commit(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tr, e := s.FreeIDs()\n\tif e != nil {\n\t\tt.Fatal(err)\n\t}\n\tfreeids := r.Slice()\n\tif len(freeids) == 0 {\n\t\tt.Fatalf(\"expected to have free id\")\n\t}\n\tif freeids[0] != ids[\"deleteme\"] {\n\t\tt.Fatalf(\"expected [%v] and got %v\", ids[\"deleteme\"], freeids[0])\n\t}\n\n\trecords2 := roaring.NewBitmap(ids[\"foo\"])\n\tc, err = s.Delete(records2)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif err = c.Commit(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tr, e = s.FreeIDs()\n\tif e != nil {\n\t\tt.Fatal(err)\n\t}\n\tfreeids = r.Slice()\n\tif len(freeids) != 2 {\n\t\tt.Fatalf(\"expected to have 2 free ids\")\n\t}\n}\nfunc TestTranslateStore_ReadWrite(t *testing.T) {\n\tt.Run(\"WriteTo_ReadFrom\", func(t *testing.T) {\n\t\ts := MustOpenNewTranslateStore(t)\n\t\tdefer MustCloseTranslateStore(s)\n\n\t\tbatch0 := []string{}\n\t\tfor i := 0; i < 100; i++ {\n\t\t\tbatch0 = append(batch0, fmt.Sprintf(\"key%d\", i))\n\t\t}\n\t\tbatch1 := []string{}\n\t\tfor i := 100; i < 200; i++ {\n\t\t\tbatch1 = append(batch1, fmt.Sprintf(\"key%d\", i))\n\t\t}\n\n\t\t// Populate the store with the keys in batch0.\n\t\tbatch0IDs, err := s.CreateKeys(batch0...)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Put the contents of the store into a buffer.\n\t\tbuf := bytes.NewBuffer(nil)\n\t\texpN := s.Size()\n\n\t\t// wrap in a func so we can defer rollback. Need rollback to\n\t\t// happen before the end of the test. I'm not entirely sure\n\t\t// why, but it hangs if you don't.\n\t\tfunc() {\n\t\t\ttx, err := s.Begin(false)\n\t\t\trequire.NoError(t, err)\n\t\t\tdefer tx.Rollback()\n\n\t\t\t// After this, the buffer should contain batch0.\n\t\t\tn, err := tx.WriteTo(buf)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Equal(t, expN, n)\n\t\t}()\n\n\t\t// Populate the store with the keys in batch1.\n\t\tbatch1IDs, err := s.CreateKeys(batch1...)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\texpIDs := map[string]uint64{\n\t\t\t\"key50\":  batch0IDs[\"key50\"],\n\t\t\t\"key150\": batch1IDs[\"key150\"],\n\t\t}\n\n\t\t// Check the IDs for a key from each batch.\n\t\tif ids, err := s.FindKeys(\"key50\", \"key150\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(expIDs, ids) {\n\t\t\tt.Fatalf(\"first expected ids: %v, but got: %v\", expIDs, ids)\n\t\t}\n\n\t\t// Reset the contents of the store with the data in the buffer.\n\t\tif n, err := s.ReadFrom(buf); err != nil {\n\t\t\tt.Fatalf(\"reading from buffer: %s\", err)\n\t\t} else if n != expN {\n\t\t\tt.Fatalf(\"expected buffer size: %d, but got: %d\", expN, n)\n\t\t}\n\n\t\t// This time, we expect the second key to be different because\n\t\t// we overwrote the store, and then just set that key.\n\t\tif ids, err := s.CreateKeys(\"key50\", \"key150\"); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if ids[\"key50\"] != expIDs[\"key50\"] {\n\t\t\tt.Fatalf(\"last expected ids[key50]: %d, but got: %d\", expIDs[\"key50\"], ids[\"key50\"])\n\t\t} else if ids[\"key150\"] == expIDs[\"key150\"] {\n\t\t\tt.Fatalf(\"last expected different ids[key150]: %d, but got: %d\", expIDs[\"key150\"], ids[\"key150\"])\n\t\t}\n\t})\n}\n\n// MustOpenNewTranslateStore returns a new, opened TranslateStore.\nfunc MustOpenNewTranslateStore(tb testing.TB) *pilosa.BoltTranslateStore {\n\ts := MustNewTranslateStore(tb)\n\tif err := s.Open(); err != nil {\n\t\ttb.Fatalf(\"opening s: %v\", err)\n\t}\n\treturn s\n}\n\n// MustCloseTranslateStore closes s and removes the underlying data file.\nfunc MustCloseTranslateStore(s *pilosa.BoltTranslateStore) {\n\tif err := s.Close(); err != nil {\n\t\tpanic(err)\n\t}\n}\n"
        },
        {
          "name": "translator_test.go",
          "type": "blob",
          "size": 15.2470703125,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa_test\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"reflect\"\n\t\"testing\"\n\t\"time\"\n\n\tpilosa \"github.com/featurebasedb/featurebase/v3\"\n\t\"github.com/featurebasedb/featurebase/v3/mock\"\n\t\"github.com/featurebasedb/featurebase/v3/test\"\n\t\"github.com/google/go-cmp/cmp\"\n\t\"github.com/pkg/errors\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\nfunc TestMultiTranslateEntryReader(t *testing.T) {\n\tt.Run(\"None\", func(t *testing.T) {\n\t\tr := pilosa.NewMultiTranslateEntryReader(context.Background(), nil)\n\t\tdefer r.Close()\n\n\t\tvar entry pilosa.TranslateEntry\n\t\tif err := r.ReadEntry(&entry); err != io.EOF {\n\t\t\tt.Fatal(err)\n\t\t}\n\t})\n\n\tt.Run(\"Single\", func(t *testing.T) {\n\t\tvar r0 mock.TranslateEntryReader\n\t\tr0.CloseFunc = func() error { return nil }\n\t\tr0.ReadEntryFunc = func(entry *pilosa.TranslateEntry) error {\n\t\t\t*entry = pilosa.TranslateEntry{Index: \"i\", Field: \"f\", ID: 1, Key: \"foo\"}\n\t\t\treturn nil\n\t\t}\n\t\tr := pilosa.NewMultiTranslateEntryReader(context.Background(), []pilosa.TranslateEntryReader{&r0})\n\t\tdefer r.Close()\n\n\t\tvar entry pilosa.TranslateEntry\n\t\tif err := r.ReadEntry(&entry); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if diff := cmp.Diff(entry, pilosa.TranslateEntry{Index: \"i\", Field: \"f\", ID: 1, Key: \"foo\"}); diff != \"\" {\n\t\t\tt.Fatal(diff)\n\t\t}\n\t\tif err := r.Close(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t})\n\n\tt.Run(\"Multi\", func(t *testing.T) {\n\t\tready0, ready1 := make(chan struct{}), make(chan struct{})\n\n\t\tvar r0 mock.TranslateEntryReader\n\t\tr0.CloseFunc = func() error { return nil }\n\t\tr0.ReadEntryFunc = func(entry *pilosa.TranslateEntry) error {\n\t\t\tif _, ok := <-ready0; !ok {\n\t\t\t\treturn io.EOF\n\t\t\t}\n\t\t\t*entry = pilosa.TranslateEntry{Index: \"i0\", Field: \"f0\", ID: 1, Key: \"foo\"}\n\t\t\treturn nil\n\t\t}\n\n\t\tvar r1 mock.TranslateEntryReader\n\t\tr1.CloseFunc = func() error { return nil }\n\t\tr1.ReadEntryFunc = func(entry *pilosa.TranslateEntry) error {\n\t\t\tif _, ok := <-ready1; !ok {\n\t\t\t\treturn io.EOF\n\t\t\t}\n\t\t\t*entry = pilosa.TranslateEntry{Index: \"i1\", Field: \"f1\", ID: 2, Key: \"bar\"}\n\t\t\treturn nil\n\t\t}\n\n\t\tr := pilosa.NewMultiTranslateEntryReader(context.Background(), []pilosa.TranslateEntryReader{&r1, &r0})\n\t\tdefer r.Close()\n\n\t\t// Ensure r0 is read first\n\t\tready0 <- struct{}{}\n\t\tvar entry pilosa.TranslateEntry\n\t\tif err := r.ReadEntry(&entry); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if diff := cmp.Diff(entry, pilosa.TranslateEntry{Index: \"i0\", Field: \"f0\", ID: 1, Key: \"foo\"}); diff != \"\" {\n\t\t\tt.Fatal(diff)\n\t\t}\n\n\t\t// Unblock r1.\n\t\tready1 <- struct{}{}\n\n\t\t// Read from r1 next.\n\t\tif err := r.ReadEntry(&entry); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if diff := cmp.Diff(entry, pilosa.TranslateEntry{Index: \"i1\", Field: \"f1\", ID: 2, Key: \"bar\"}); diff != \"\" {\n\t\t\tt.Fatal(diff)\n\t\t}\n\n\t\t// Close both readers.\n\t\tclose(ready0)\n\t\tclose(ready1)\n\t\tif err := r.Close(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t})\n\n\tt.Run(\"Error\", func(t *testing.T) {\n\t\tvar r0 mock.TranslateEntryReader\n\t\tr0.CloseFunc = func() error { return nil }\n\t\tr0.ReadEntryFunc = func(entry *pilosa.TranslateEntry) error {\n\t\t\treturn errors.New(\"marker\")\n\t\t}\n\t\tr := pilosa.NewMultiTranslateEntryReader(context.Background(), []pilosa.TranslateEntryReader{&r0})\n\t\tdefer r.Close()\n\n\t\tvar entry pilosa.TranslateEntry\n\t\tif err := r.ReadEntry(&entry); err == nil || err.Error() != `marker` {\n\t\t\tt.Fatalf(\"unexpected error: %s\", err)\n\t\t}\n\t\tif err := r.Close(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t})\n}\n\nfunc TestTranslation_KeyNotFound(t *testing.T) {\n\tc := test.MustRunCluster(t, 4)\n\tdefer c.Close()\n\n\tnode0 := c.GetNode(0)\n\tnode1 := c.GetNode(1)\n\tnode2 := c.GetNode(2)\n\tnode3 := c.GetNode(3)\n\n\tctx := context.Background()\n\tindex, fld := c.Idx(), \"f\"\n\t// Create an index with keys.\n\tif _, err := node0.API.CreateIndex(ctx, index, pilosa.IndexOptions{Keys: true}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\t// Create an index with keys.\n\tif _, err := node0.API.CreateField(ctx, index, fld, pilosa.OptFieldKeys()); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// write a new key and get id\n\treq, err := node0.API.Serializer.Marshal(&pilosa.TranslateKeysRequest{\n\t\tIndex:       index,\n\t\tField:       fld,\n\t\tKeys:        []string{\"k1\"},\n\t\tNotWritable: false,\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif buf, err := node0.API.TranslateKeys(ctx, bytes.NewReader(req)); err != nil {\n\t\tt.Fatal(err)\n\t} else {\n\t\tvar resp pilosa.TranslateKeysResponse\n\t\tif err = node0.API.Serializer.Unmarshal(buf, &resp); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tid1 := resp.IDs[0]\n\n\t\t// read non-existing key\n\t\treq, err = node3.API.Serializer.Marshal(&pilosa.TranslateKeysRequest{\n\t\t\tIndex:       index,\n\t\t\tField:       fld,\n\t\t\tKeys:        []string{\"k2\"},\n\t\t\tNotWritable: true,\n\t\t})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif buf, err = node3.API.TranslateKeys(ctx, bytes.NewReader(req)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif err = node3.API.Serializer.Unmarshal(buf, &resp); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if resp.IDs != nil {\n\t\t\tt.Fatalf(\"TranslateKeys(%+v): expected: nil, got: %d\", string(req), resp)\n\t\t}\n\n\t\treq, err = node1.API.Serializer.Marshal(&pilosa.TranslateKeysRequest{\n\t\t\tIndex:       index,\n\t\t\tKeys:        []string{\"k2\"},\n\t\t\tNotWritable: true,\n\t\t})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif buf, err = node1.API.TranslateKeys(ctx, bytes.NewReader(req)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif err = node1.API.Serializer.Unmarshal(buf, &resp); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if resp.IDs != nil {\n\t\t\tt.Fatalf(\"TranslateKeys(%+v): expected: nil, got: %d\", req, resp)\n\t\t}\n\n\t\treq, err = node2.API.Serializer.Marshal(&pilosa.TranslateKeysRequest{\n\t\t\tIndex:       index,\n\t\t\tField:       fld,\n\t\t\tKeys:        []string{\"k2\", \"k1\"},\n\t\t\tNotWritable: false,\n\t\t})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif buf, err = node2.API.TranslateKeys(ctx, bytes.NewReader(req)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif err = node2.API.Serializer.Unmarshal(buf, &resp); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif resp.IDs[0] != id1+1 || resp.IDs[1] != id1 {\n\t\t\tt.Fatalf(\"TranslateKeys(%+v): expected: %d,%d, got: %d,%d\", req, id1+1, id1, resp.IDs[0], resp.IDs[1])\n\t\t}\n\t}\n}\n\nfunc TestTranslation_TranslateIDsOnCluster(t *testing.T) {\n\tc := test.MustRunCluster(t, 4)\n\tdefer c.Close()\n\n\tcoord := c.GetPrimary()\n\tother := c.GetNonPrimary()\n\n\tctx := context.Background()\n\tindex, fld := c.Idx(), \"f\"\n\t// Create an index with keys.\n\tif _, err := coord.API.CreateIndex(ctx, index, pilosa.IndexOptions{Keys: true}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Create an index with keys.\n\tif _, err := coord.API.CreateField(ctx, index, fld, pilosa.OptFieldKeys()); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tkeys := []string{\"k0\", \"k1\", \"k2\", \"k3\", \"k4\", \"k5\", \"k6\", \"k7\", \"k8\", \"k9\"}\n\t// write a new key and get id\n\treq, err := coord.API.Serializer.Marshal(&pilosa.TranslateKeysRequest{\n\t\tIndex:       index,\n\t\tField:       fld,\n\t\tKeys:        keys,\n\t\tNotWritable: false,\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif buf, err := coord.API.TranslateKeys(ctx, bytes.NewReader(req)); err != nil {\n\t\tt.Fatal(err)\n\t} else {\n\t\tvar (\n\t\t\trespKeys pilosa.TranslateKeysResponse\n\t\t\trespIDs  pilosa.TranslateIDsResponse\n\t\t)\n\t\tif err = other.API.Serializer.Unmarshal(buf, &respKeys); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tids := respKeys.IDs\n\n\t\t// translate ids\n\t\treq, err = other.API.Serializer.Marshal(&pilosa.TranslateIDsRequest{\n\t\t\tIndex: index,\n\t\t\tField: fld,\n\t\t\tIDs:   ids,\n\t\t})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif buf, err = other.API.TranslateIDs(ctx, bytes.NewReader(req)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif err = other.API.Serializer.Unmarshal(buf, &respIDs); err != nil {\n\t\t\tt.Fatal(err)\n\t\t} else if !reflect.DeepEqual(respIDs.Keys, keys) {\n\t\t\tt.Fatalf(\"TranslateIDs(%+v): expected: %+v, got: %+v\", ids, keys, respIDs.Keys)\n\t\t}\n\t}\n}\n\nfunc TestTranslation_Cluster_CreateFind(t *testing.T) {\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\n\tc.CreateField(t, c.Idx(), pilosa.IndexOptions{Keys: true}, \"f\", pilosa.OptFieldKeys())\n\n\tctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n\tdefer cancel()\n\n\t// Use the alphabet to test keys.\n\ttestKeys := make(map[string]struct{})\n\tfor i := 'a'; i <= 'z'; i++ {\n\t\ttestKeys[string(i)] = struct{}{}\n\t}\n\n\tt.Run(\"Index\", func(t *testing.T) {\n\t\t// Create all index keys, split across nodes.\n\t\t{\n\t\t\tparts := make([][]string, len(c.Nodes))\n\t\t\t{\n\t\t\t\t// Randomly partition the keys.\n\t\t\t\ti := 0\n\t\t\t\tfor k := range testKeys {\n\t\t\t\t\tparts[i%len(c.Nodes)] = append(parts[i%len(c.Nodes)], k)\n\t\t\t\t\ti++\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Create some keys on each node.\n\t\t\tvar g errgroup.Group\n\t\t\tdefer g.Wait() //nolint:errcheck\n\t\t\tfor i, keys := range parts {\n\t\t\t\ti, keys := i, keys\n\t\t\t\tg.Go(func() error {\n\t\t\t\t\t_, err := c.GetNode(i).API.CreateIndexKeys(ctx, c.Idx(), keys...)\n\t\t\t\t\treturn err\n\t\t\t\t})\n\t\t\t}\n\t\t\tif err := g.Wait(); err != nil {\n\t\t\t\tt.Errorf(\"creating keys: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\t// Check that all index keys exist, and consistently map to the same IDs.\n\t\t{\n\t\t\t// Convert the keys to a list.\n\t\t\tkeyList := make([]string, 0, len(testKeys))\n\t\t\tfor k := range testKeys {\n\t\t\t\tkeyList = append(keyList, k)\n\t\t\t}\n\n\t\t\t// Obtain authoritative translations for the keys.\n\t\t\ttranslations, err := c.GetPrimary().API.FindIndexKeys(ctx, c.Idx(), keyList...)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"obtaining authoritative translations: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tfor _, k := range keyList {\n\t\t\t\tif _, ok := translations[k]; !ok {\n\t\t\t\t\tt.Errorf(\"key %q is missing\", k)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Check that all nodes agree on these translations.\n\t\t\tvar g errgroup.Group\n\t\t\tdefer g.Wait() //nolint:errcheck\n\t\t\tfor i, n := range c.Nodes {\n\t\t\t\tx, api := i, n.API\n\t\t\t\tg.Go(func() (err error) {\n\t\t\t\t\tdefer func() { err = errors.Wrapf(err, \"translating on node %d\", x) }()\n\t\t\t\t\tlocalTranslations, err := api.FindIndexKeys(ctx, c.Idx(), keyList...)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn errors.Wrap(err, \"finding translations\")\n\t\t\t\t\t}\n\t\t\t\t\treturn compareTranslations(translations, localTranslations)\n\t\t\t\t})\n\t\t\t}\n\t\t\tif err := g.Wait(); err != nil {\n\t\t\t\tt.Errorf(\"finding keys: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Check that re-invoking create returns the original translations.\n\t\t\tfor i, n := range c.Nodes {\n\t\t\t\tx, api := i, n.API\n\t\t\t\tg.Go(func() (err error) {\n\t\t\t\t\tdefer func() { err = errors.Wrapf(err, \"translating on node %d\", x) }()\n\t\t\t\t\tlocalTranslations, err := api.CreateIndexKeys(ctx, c.Idx(), keyList...)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn errors.Wrap(err, \"finding translations\")\n\t\t\t\t\t}\n\t\t\t\t\treturn compareTranslations(translations, localTranslations)\n\t\t\t\t})\n\t\t\t}\n\t\t\tif err := g.Wait(); err != nil {\n\t\t\t\tt.Errorf(\"checking re-create of keys: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t})\n\tt.Run(\"Field\", func(t *testing.T) {\n\t\t// Create all field keys, split across nodes.\n\t\t{\n\t\t\tparts := make([][]string, len(c.Nodes))\n\t\t\t{\n\t\t\t\t// Randomly partition the keys.\n\t\t\t\ti := 0\n\t\t\t\tfor k := range testKeys {\n\t\t\t\t\tparts[i%len(c.Nodes)] = append(parts[i%len(c.Nodes)], k)\n\t\t\t\t\ti++\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Create some keys on each node.\n\t\t\tvar g errgroup.Group\n\t\t\tdefer g.Wait() //nolint:errcheck\n\t\t\tfor i, keys := range parts {\n\t\t\t\ti, keys := i, keys\n\t\t\t\tg.Go(func() error {\n\t\t\t\t\t_, err := c.GetNode(i).API.CreateFieldKeys(ctx, c.Idx(), \"f\", keys...)\n\t\t\t\t\treturn err\n\t\t\t\t})\n\t\t\t}\n\t\t\tif err := g.Wait(); err != nil {\n\t\t\t\tt.Errorf(\"creating keys: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\t// Check that all field keys exist, and consistently map to the same IDs.\n\t\t{\n\t\t\t// Convert the keys to a list.\n\t\t\tkeyList := make([]string, 0, len(testKeys))\n\t\t\tfor k := range testKeys {\n\t\t\t\tkeyList = append(keyList, k)\n\t\t\t}\n\n\t\t\t// Obtain authoritative translations for the keys.\n\t\t\ttranslations, err := c.GetPrimary().API.FindFieldKeys(ctx, c.Idx(), \"f\", keyList...)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"obtaining authoritative translations: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tfor _, k := range keyList {\n\t\t\t\tif _, ok := translations[k]; !ok {\n\t\t\t\t\tt.Errorf(\"key %q is missing\", k)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Check that all nodes agree on these translations.\n\t\t\tvar g errgroup.Group\n\t\t\tdefer g.Wait() //nolint:errcheck\n\t\t\tfor i, n := range c.Nodes {\n\t\t\t\tx, api := i, n.API\n\t\t\t\tg.Go(func() (err error) {\n\t\t\t\t\tdefer func() { err = errors.Wrapf(err, \"translating on node %d\", x) }()\n\t\t\t\t\tlocalTranslations, err := api.FindFieldKeys(ctx, c.Idx(), \"f\", keyList...)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn errors.Wrap(err, \"finding translations\")\n\t\t\t\t\t}\n\t\t\t\t\treturn compareTranslations(translations, localTranslations)\n\t\t\t\t})\n\t\t\t}\n\t\t\tif err := g.Wait(); err != nil {\n\t\t\t\tt.Errorf(\"finding keys: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Check that re-invoking create returns the original translations.\n\t\t\tfor i, n := range c.Nodes {\n\t\t\t\tx, api := i, n.API\n\t\t\t\tg.Go(func() (err error) {\n\t\t\t\t\tdefer func() { err = errors.Wrapf(err, \"translating on node %d\", x) }()\n\t\t\t\t\tlocalTranslations, err := api.CreateFieldKeys(ctx, c.Idx(), \"f\", keyList...)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn errors.Wrap(err, \"finding translations\")\n\t\t\t\t\t}\n\t\t\t\t\treturn compareTranslations(translations, localTranslations)\n\t\t\t\t})\n\t\t\t}\n\t\t\tif err := g.Wait(); err != nil {\n\t\t\t\tt.Errorf(\"checking re-create of keys: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc TestTranslation_Cluster_CreateFindUnkeyed(t *testing.T) {\n\tc := test.MustRunCluster(t, 3)\n\tdefer c.Close()\n\ti := c.Idx()\n\n\tc.CreateField(t, i, pilosa.IndexOptions{}, \"f\")\n\n\tt.Run(\"Index\", func(t *testing.T) {\n\t\tt.Run(\"Create\", func(t *testing.T) {\n\t\t\tctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n\t\t\tdefer cancel()\n\n\t\t\t_, err := c.GetNonPrimary().API.CreateIndexKeys(ctx, i, \"foo\")\n\t\t\tif err == nil {\n\t\t\t\tt.Fatal(\"unexpected success\")\n\t\t\t}\n\t\t\texpect := fmt.Sprintf(`cannot create keys on unkeyed index \"%s\"`, i)\n\t\t\tif got := err.Error(); got != expect {\n\t\t\t\tt.Fatalf(\"expected error %q but got %q\", expect, got)\n\t\t\t}\n\t\t})\n\t\tt.Run(\"Find\", func(t *testing.T) {\n\t\t\tctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n\t\t\tdefer cancel()\n\n\t\t\t_, err := c.GetNonPrimary().API.FindIndexKeys(ctx, i, \"foo\")\n\t\t\tif err == nil {\n\t\t\t\tt.Fatal(\"unexpected success\")\n\t\t\t}\n\t\t\texpect := fmt.Sprintf(`cannot find keys on unkeyed index \"%s\"`, i)\n\t\t\tif got := err.Error(); got != expect {\n\t\t\t\tt.Fatalf(\"expected error %q but got %q\", expect, got)\n\t\t\t}\n\t\t})\n\t})\n\tt.Run(\"Field\", func(t *testing.T) {\n\t\tt.Run(\"Create\", func(t *testing.T) {\n\t\t\tctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n\t\t\tdefer cancel()\n\n\t\t\t_, err := c.GetNonPrimary().API.CreateFieldKeys(ctx, i, \"f\", \"foo\")\n\t\t\tif err == nil {\n\t\t\t\tt.Fatal(\"unexpected success\")\n\t\t\t}\n\t\t\texpect := `cannot create keys on unkeyed field \"f\"`\n\t\t\tif got := err.Error(); got != expect {\n\t\t\t\tt.Fatalf(\"expected error %q but got %q\", expect, got)\n\t\t\t}\n\t\t})\n\t\tt.Run(\"Find\", func(t *testing.T) {\n\t\t\tctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n\t\t\tdefer cancel()\n\n\t\t\t_, err := c.GetNonPrimary().API.FindFieldKeys(ctx, i, \"f\", \"foo\")\n\t\t\tif err == nil {\n\t\t\t\tt.Fatal(\"unexpected success\")\n\t\t\t}\n\t\t\texpect := `cannot find keys on unkeyed field \"f\"`\n\t\t\tif got := err.Error(); got != expect {\n\t\t\t\tt.Fatalf(\"expected error %q but got %q\", expect, got)\n\t\t\t}\n\t\t})\n\t})\n}\n\nfunc compareTranslations(expected, got map[string]uint64) error {\n\tfor key, id := range got {\n\t\tif realID, ok := expected[key]; !ok {\n\t\t\treturn errors.Errorf(\"unexpected key %q mapped to ID %d\", key, id)\n\t\t} else if id != realID {\n\t\t\treturn errors.Errorf(\"mismatched translation: expected %q:%d but got %q:%d\", key, realID, key, id)\n\t\t}\n\t}\n\tfor key, realID := range expected {\n\t\tif id, ok := got[key]; !ok {\n\t\t\treturn errors.Errorf(\"missing translation of key %q\", key)\n\t\t} else if id != realID {\n\t\t\treturn errors.Errorf(\"mismatched translation: expected %q:%d but got %q:%d\", key, realID, key, id)\n\t\t}\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "tx.go",
          "type": "blob",
          "size": 8.1298828125,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\ttxkey \"github.com/featurebasedb/featurebase/v3/short_txkey\"\n\t//txkey \"github.com/featurebasedb/featurebase/v3/txkey\"\n)\n\n// writable initializes Tx that update, use !writable for read-only.\nconst writable = true\n\n// Tx providers offer transactional storage for high-level roaring.Bitmaps and\n// low-level roaring.Containers.\n//\n// The common 4-tuple of (index, field, view, shard) jointly specify a fragment.\n// A fragment conceptually holds one roaring.Bitmap.\n//\n// Within the fragment, the ckey or container-key is the uint64 that specifies\n// the high 48-bits of the roaring.Bitmap 64-bit space.\n// The ckey is used to retrieve a specific roaring.Container that\n// is either a run, array, or raw bitmap. The roaring.Container is the\n// low 16-bits of the roaring.Bitmap space. Its size is at most\n// 8KB (2^16 bits / (8 bits / byte) == 8192 bytes).\n//\n// The grain of the transaction is guaranteed to be at least at the shard\n// within one index. Therefore updates to any of the fields within\n// the same shard will be atomically visible only once the transaction commits.\n// Reads from another, concurrently open, transaction will not see updates\n// that have not been committed.\ntype Tx interface {\n\n\t// Type returns \"roaring\", \"rbf\", or one of the other\n\t// Tx types at the top of txfactory.go\n\tType() string\n\n\t// Rollback must be called at the end of read-only transactions. Either\n\t// Rollback or Commit must be called at the end of writable transactions.\n\t// It is safe to call Rollback multiple times, but it must be\n\t// called at least once to release resources. Any Rollback after\n\t// a Commit is ignored, so 'defer tx.Rollback()' should be commonly\n\t// written after starting a new transaction.\n\t//\n\t// If there is an error during internal Rollback processing,\n\t// this would be quite serious, and the underlying storage is\n\t// expected to panic. Hence there is no explicit error returned\n\t// from Rollback that needs to be checked.\n\tRollback()\n\n\t// Commit makes the updates in the Tx visible to subsequent transactions.\n\tCommit() error\n\n\t// ContainerIterator loops over the containers in the conceptual\n\t// roaring.Bitmap for the specified fragment.\n\t// Calling Next() on the returned roaring.ContainerIterator gives\n\t// you a roaring.Container that is either run, array, or raw bitmap.\n\t// Return value 'found' is true when the ckey container was present.\n\t// ckey of 0 gives all containers (in the fragment).\n\t//\n\t// ContainerIterator must not have side-effects.\n\t//\n\t// citer.Close() must be called when the client is done using it.\n\tContainerIterator(index, field, view string, shard uint64, ckey uint64) (citer roaring.ContainerIterator, found bool, err error)\n\n\t// ApplyFilter applies a roaring.BitmapFilter to a specified shard,\n\t// starting at the given container key. The filter's ConsiderData\n\t// method may be called with transient Container objects which *must\n\t// not* be retained or referenced after that function exits. Similarly,\n\t// their data must not be retained. If you need the data later, you\n\t// must copy it into some other memory.\n\tApplyFilter(index, field, view string, shard uint64, ckey uint64, filter roaring.BitmapFilter) (err error)\n\n\t// ApplyRewriter applies a roaring.BitmapRewriter to a specified shard,\n\t// starting at the given container key. The filter's ConsiderData\n\t// method may be called with transient Container objects which *must\n\t// not* be retained or referenced after that function exits. Similarly,\n\t// their data must not be retained. If you need the data later, you\n\t// must copy it into some other memory. However, it is safe to overwrite\n\t// the returned container; for instance, you can DifferenceInPlace on\n\t// it.\n\tApplyRewriter(index, field, view string, shard uint64, ckey uint64, filter roaring.BitmapRewriter) (err error)\n\n\t// RoaringBitmap retrieves the roaring.Bitmap for the entire shard.\n\tRoaringBitmap(index, field, view string, shard uint64) (*roaring.Bitmap, error)\n\n\t// Container returns the roaring.Container for the given ckey\n\t// (container-key or highbits) in the chosen fragment.\n\tContainer(index, field, view string, shard uint64, ckey uint64) (*roaring.Container, error)\n\n\t// PutContainer stores c under the given ckey (container-key) in the specified fragment.\n\tPutContainer(index, field, view string, shard uint64, ckey uint64, c *roaring.Container) error\n\n\t// RemoveContainer deletes the roaring.Container under the given ckey (container-key)\n\t// in the specified fragment.\n\tRemoveContainer(index, field, view string, shard uint64, ckey uint64) error\n\n\t// Add adds the 'a' values to the Bitmap for the fragment.\n\tAdd(index, field, view string, shard uint64, a ...uint64) (changeCount int, err error)\n\n\t// Remove removes the 'a' values from the Bitmap for the fragment.\n\tRemove(index, field, view string, shard uint64, a ...uint64) (changeCount int, err error)\n\n\t// Removed removes values, returning the set of values it removed.\n\t// It may overwrite the slice passed to it.\n\tRemoved(index, field, view string, shard uint64, a ...uint64) (changed []uint64, err error)\n\n\t// Contains tests if the uint64 v is stored in the fragment's Bitmap.\n\tContains(index, field, view string, shard uint64, v uint64) (exists bool, err error)\n\n\t// Count returns the count of hot bits on the fragment.\n\tCount(index, field, view string, shard uint64) (uint64, error)\n\n\t// Max returns the maximum value set in the Bitmap for the fragment.\n\tMax(index, field, view string, shard uint64) (uint64, error)\n\n\t// Min returns the minimum value set in the Bitmap for the fragment.\n\tMin(index, field, view string, shard uint64) (uint64, bool, error)\n\n\t// CountRange returns the count of hot bits in the [start, end) range on the\n\t// fragment.\n\tCountRange(index, field, view string, shard uint64, start, end uint64) (uint64, error)\n\n\t// OffsetRange returns a *roaring.Bitmap containing the portion of the Bitmap for the fragment\n\t// which is specified by a combination of (offset, [start, end)).\n\t//\n\t// start  - The value at which to start reading. This must be the zero value\n\t//          of a container; i.e. [0, 65536, ...]\n\t// end    - The value at which to end reading. This must be the zero value\n\t//          of a container; i.e. [0, 65536, ...]\n\t// offset - The number of positions to shift the resulting bitmap. This must\n\t//          be the zero value of a container; i.e. [0, 65536, ...]\n\t//\n\t// For example, if (index, field, view, shard) represents the following bitmap:\n\t// [1, 2, 3, 65536, 65539]\n\t//\n\t// then the following results are achieved based on (offset, start, end):\n\t// (0, 0, 131072)          => [1, 2, 3, 65536, 65539]\n\t// (0, 65536, 131072)      => [0, 3]\n\t// (65536, 65536, 131072)  => [65536, 65539]\n\t// (262144, 65536, 131072) => [262144, 262147]\n\t//\n\tOffsetRange(index, field, view string, shard uint64, offset, start, end uint64) (*roaring.Bitmap, error)\n\n\t// ImportRoaringBits does efficient bulk import using rit, a roaring.RoaringIterator.\n\t//\n\t// See the roaring package for details of the RoaringIterator.\n\t//\n\t// If clear is true, the bits from rit are cleared, otherwise they are set in the\n\t// specifed fragment.\n\t//\n\t// ImportRoaringBits return values changed and rowSet may be inaccurate if\n\t// the data []byte is supplied (the RoaringTx implementation neglects this for speed).\n\tImportRoaringBits(index, field, view string, shard uint64, rit roaring.RoaringIterator, clear bool, log bool, rowSize uint64) (changed int, rowSet map[uint64]int, err error)\n\n\t// GetSortedFieldViewList gets the set of FieldView(s)\n\tGetSortedFieldViewList(idx *Index, shard uint64) (fvs []txkey.FieldView, err error)\n\n\tGetFieldSizeBytes(index, field string) (uint64, error)\n}\n\n// GenericApplyFilter implements ApplyFilter in terms of tx.ContainerIterator,\n// as a convenience if a Tx backend hasn't implemented this new function yet.\nfunc GenericApplyFilter(tx Tx, index, field, view string, shard uint64, ckey uint64, filter roaring.BitmapFilter) (err error) {\n\titer, _, err := tx.ContainerIterator(index, field, view, shard, ckey)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// ApplyFilterToIterator closes the iterator for us.\n\treturn roaring.ApplyFilterToIterator(filter, iter)\n}\n"
        },
        {
          "name": "tx_internal_test.go",
          "type": "blob",
          "size": 4.013671875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"bytes\"\n\t\"sync\"\n\t\"testing\"\n\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n)\n\nconst countRangeMaxN = 8192\n\nvar countRangeSampleData []byte\nvar prepareCountRangeSampleData sync.Once\n\n// The sample data for the counter is just a series of containers,\n// each with cardinality equal to its container key.\nfunc requireCountRangeSampleData(tb testing.TB) (*fragment, Tx) {\n\tprepareCountRangeSampleData.Do(func() {\n\t\tvar arraySample [4096]uint16\n\t\t// This horrible hack relies on a quirk of roaring's internals: It'll\n\t\t// copy the bitmap if its length isn't exactly 1024. This lets us\n\t\t// request that each container get its own copy of the bitmap.\n\t\tvar bitmapSample [1025]uint64\n\t\tfor i := range arraySample {\n\t\t\tarraySample[i] = uint16(i * 2)\n\t\t}\n\t\t// Put corresponding bits in the bitmap...\n\t\tfor i := 0; i < 4096/32; i++ {\n\t\t\t// bit 0 is 0x1, bit 2 is 0x4, so even-numbered bits\n\t\t\t// are 0x5555....\n\t\t\tbitmapSample[i] = 0x5555555555555555\n\t\t}\n\t\tbm := roaring.NewSliceBitmap()\n\t\tfor n := 0; n < 4096 && n < countRangeMaxN; n++ {\n\t\t\tc := roaring.NewContainerArray(arraySample[:n])\n\t\t\tbm.Put(uint64(n), c)\n\t\t}\n\t\t// Start filling in the missing bits. This starts us out with\n\t\t// bitmap containers, but then eventually converts to things\n\t\t// that are more likely to be run containers. At the end of this,\n\t\t// we should have exactly the first 8,192 bits set, for a single\n\t\t// run of 8k.\n\t\tfor n := 4096; n < 8192; n++ {\n\t\t\tc := roaring.NewContainerBitmapN(bitmapSample[:], int32(n))\n\t\t\tbm.Put(uint64(n), c)\n\t\t\tw := n - 4096\n\t\t\tbitmapSample[w/32] |= 1 << (((n % 32) * 2) + 1)\n\t\t}\n\t\tvar asBytes bytes.Buffer\n\t\tn, err := bm.WriteTo(&asBytes)\n\t\tif err != nil {\n\t\t\ttb.Fatalf(\"writing bitmap: %v\", err)\n\t\t}\n\t\tcountRangeSampleData = asBytes.Bytes()\n\t\ttb.Logf(\"creating bitmap: %d containers, %d bytes of data\", countRangeMaxN, n)\n\t})\n\tf, idx, tx := mustOpenFragment(tb)\n\t// Properly close this transaction, but not the next one we create that the\n\t// caller will be responsible for. The deferred callback will\n\t// be a nop if the Commit happened.\n\tdefer tx.Rollback()\n\terr := f.importRoaringT(tx, countRangeSampleData, false)\n\tif err != nil {\n\t\ttb.Fatalf(\"importing sample data: %v\", err)\n\t}\n\terr = tx.Commit()\n\tif err != nil {\n\t\ttb.Fatalf(\"committing sample data: %v\", err)\n\t}\n\ttx = idx.holder.txf.NewTx(Txo{Write: false, Index: idx, Fragment: f, Shard: 0})\n\treturn f, tx\n}\n\nfunc TestTx_CountRange(t *testing.T) {\n\tf, tx := requireCountRangeSampleData(t)\n\tdefer f.Clean(t)\n\tdefer tx.Rollback()\n\t// CountRange accesses the fragment without locking. Normally we only\n\t// call it from inside a fragment routine with locking. Otherwise, you\n\t// can have a race condition with snapshots, for instance.\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\n\texpected := uint64(0)\n\tj := uint64(0)\n\tfor i := uint64(0); i < countRangeMaxN; i += 7 {\n\t\texpected += i\n\t\tif i%4 == 3 {\n\t\t\texpected -= (j * 7) + 21\n\t\t\tj += 7\n\t\t}\n\t\t// Every other bit gets set, for a total of i bits in container\n\t\t// i, so they're all in the first (i*2) bits of the container.\n\t\tgot, err := tx.CountRange(\"i\", \"f\", \"v\", 0, uint64(j)<<16, (uint64(i)<<16)+(i*2))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"counting range: %v\", err)\n\t\t}\n\t\tif got != expected {\n\t\t\tt.Fatalf(\"counting from container %d to %d, expected %d, got %d\",\n\t\t\t\tj, i, expected, got)\n\t\t}\n\t\t// The -i here undoes the +i at the top of this loop.\n\t\texpected += (i * 7) + 21 - i\n\t}\n}\n\nfunc BenchmarkTx_CountRange(b *testing.B) {\n\tf, tx := requireCountRangeSampleData(b)\n\tdefer f.Clean(b)\n\tdefer tx.Rollback()\n\n\tfor k := 0; k < b.N; k++ {\n\t\texpected := uint64(0)\n\t\tj := uint64(0)\n\t\tfor i := uint64(0); i < countRangeMaxN; i += 7 {\n\t\t\tif i%4 == 3 {\n\t\t\t\texpected -= (j * 7) + 21\n\t\t\t\tj += 7\n\t\t\t}\n\t\t\tgot, err := tx.CountRange(\"i\", \"f\", \"v\", 0, uint64(j)<<16, uint64(i)<<16)\n\t\t\tif err != nil {\n\t\t\t\tb.Fatalf(\"counting range: %v\", err)\n\t\t\t}\n\t\t\tif got != expected {\n\t\t\t\tb.Fatalf(\"counting from container %d to %d, expected %d, got %d\",\n\t\t\t\t\tj, i, expected, got)\n\t\t\t}\n\t\t\texpected += (i * 7) + 21\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "tx_test.go",
          "type": "blob",
          "size": 7.13671875,
          "content": "// Copyright 2021 Molecula Corp. All rights reserved.\npackage pilosa_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"testing\"\n\n\tpilosa \"github.com/featurebasedb/featurebase/v3\"\n\t\"github.com/featurebasedb/featurebase/v3/test\"\n)\n\nfunc queryIRABit(t *testing.T, m0api *pilosa.API, acctOwnerID uint64, iraField string, iraRowID uint64, index string) (bit bool) {\n\tquery := fmt.Sprintf(\"Row(%v=%v)\", iraField, iraRowID) // acctOwnerID)\n\tres, err := m0api.Query(context.Background(), &pilosa.QueryRequest{Index: index, Query: query})\n\tif err != nil {\n\t\tt.Fatalf(\"querying IRA bit: %v\", err)\n\t}\n\tcols := res.Results[0].(*pilosa.Row).Columns()\n\tfor i := range cols {\n\t\tif cols[i] == acctOwnerID {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc mustQueryAcct(t *testing.T, m0api *pilosa.API, acctOwnerID uint64, fieldAcct0, index string) (acctBal int64) {\n\tquery := fmt.Sprintf(\"FieldValue(field=%v, column=%v)\", fieldAcct0, acctOwnerID)\n\tres, err := m0api.Query(context.Background(), &pilosa.QueryRequest{Index: index, Query: query})\n\tif err != nil {\n\t\tt.Fatalf(\"querying account: %v\", err)\n\t}\n\n\tif len(res.Results) == 0 {\n\t\treturn 0\n\t}\n\tvalCount := res.Results[0].(pilosa.ValCount)\n\treturn valCount.Val\n}\n\nfunc queryBalances(t *testing.T, m0api *pilosa.API, acctOwnerID uint64, fldAcct0, fldAcct1, index string) (acct0bal, acct1bal int64) {\n\n\tacct0bal = mustQueryAcct(t, m0api, acctOwnerID, fldAcct0, index)\n\tacct1bal = mustQueryAcct(t, m0api, acctOwnerID, fldAcct1, index)\n\treturn\n}\n\nfunc TestAPI_ImportAtomicRecord(t *testing.T) {\n\tc := test.MustRunCluster(t, 1)\n\tdefer c.Close()\n\n\tm0 := c.GetNode(0)\n\tm0api := m0.API\n\n\tctx := context.Background()\n\tindex := c.Idx()\n\n\tfieldAcct0 := \"acct0\"\n\tfieldAcct1 := \"acct1\"\n\n\ttransferUSD := int64(100)\n\t_ = transferUSD\n\topts := pilosa.OptFieldTypeInt(-1000, 1000)\n\n\t_, err := m0api.CreateIndex(ctx, index, pilosa.IndexOptions{})\n\tif err != nil {\n\t\tt.Fatalf(\"creating index: %v\", err)\n\t}\n\t_, err = m0api.CreateField(ctx, index, fieldAcct0, opts)\n\tif err != nil {\n\t\tt.Fatalf(\"creating fieldAcct0: %v\", err)\n\t}\n\t_, err = m0api.CreateField(ctx, index, fieldAcct1, opts)\n\tif err != nil {\n\t\tt.Fatalf(\"creating fieldAcct1: %v\", err)\n\t}\n\n\tiraField := \"ira\" // set field.\n\tiraRowID := uint64(3)\n\t_, err = m0api.CreateField(ctx, index, iraField)\n\tif err != nil {\n\t\tt.Fatalf(\"creating fieldIRA: %v\", err)\n\t}\n\n\tacctOwnerID := uint64(78) // ColumnID\n\tshard := acctOwnerID / ShardWidth\n\n\t// setup 500 USD in acct1 and 700 USD in acct2.\n\t// transfer 100 USD.\n\t// should see 400 USD in acct, and 800 USD in acct2.\n\t//\n\n\t// setup initial balances\n\n\tcreateAIRUpdate := func(acct0bal, acct1bal int64) (air *pilosa.AtomicRecord) {\n\t\tivr0 := &pilosa.ImportValueRequest{\n\t\t\tIndex:     index,\n\t\t\tField:     fieldAcct0,\n\t\t\tShard:     shard,\n\t\t\tColumnIDs: []uint64{acctOwnerID},\n\t\t\tValues:    []int64{acct0bal},\n\t\t}\n\t\tivr1 := &pilosa.ImportValueRequest{\n\t\t\tIndex:     index,\n\t\t\tField:     fieldAcct1,\n\t\t\tShard:     shard,\n\t\t\tColumnIDs: []uint64{acctOwnerID},\n\t\t\tValues:    []int64{acct1bal},\n\t\t}\n\n\t\tir0 := &pilosa.ImportRequest{\n\t\t\tIndex:     index,\n\t\t\tField:     iraField,\n\t\t\tShard:     shard,\n\t\t\tColumnIDs: []uint64{acctOwnerID},\n\t\t\tRowIDs:    []uint64{iraRowID},\n\t\t}\n\n\t\tair = &pilosa.AtomicRecord{\n\t\t\tIndex: index,\n\t\t\tShard: shard,\n\t\t\tIvr: []*pilosa.ImportValueRequest{\n\t\t\t\tivr0, ivr1,\n\t\t\t},\n\t\t\tIr: []*pilosa.ImportRequest{ir0},\n\t\t}\n\t\treturn\n\t}\n\n\texpectedBalStartingAcct0 := int64(500)\n\texpectedBalStartingAcct1 := int64(700)\n\n\tair := createAIRUpdate(expectedBalStartingAcct0, expectedBalStartingAcct1)\n\n\t//vv(\"BEFORE the first ImportAtomicRecord!\")\n\n\tqcx := m0api.Txf().NewQcx()\n\tif err := m0api.ImportAtomicRecord(ctx, qcx, air); err != nil {\n\t\tqcx.Abort()\n\t\tt.Fatal(err)\n\t}\n\tif err := qcx.Finish(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t//vv(\"AFTER the first ImportAtomicRecord!\")\n\n\tiraBit := queryIRABit(t, m0api, acctOwnerID, iraField, iraRowID, index)\n\tif !iraBit {\n\t\tt.Fatal(\"IRA bit should have been set\")\n\t}\n\n\tstartingBalanceAcct0, startingBalanceAcct1 := queryBalances(t, m0api, acctOwnerID, fieldAcct0, fieldAcct1, index)\n\t//vv(\"starting balance: acct0=%v,  acct1=%v\", startingBalanceAcct0, startingBalanceAcct1)\n\n\tif startingBalanceAcct0 != expectedBalStartingAcct0 {\n\t\tt.Fatalf(\"expected %v, observed %v starting acct0 balance\", expectedBalStartingAcct0, startingBalanceAcct0)\n\t}\n\tif startingBalanceAcct1 != expectedBalStartingAcct1 {\n\t\tt.Fatalf(\"expected %v, observed %v starting acct1 balance\", expectedBalStartingAcct1, startingBalanceAcct1)\n\t}\n\n\t//vv(\"sad path: transferUSD %v from %v -> %v, with power loss half-way through\", transferUSD, fieldAcct0, fieldAcct1)\n\n\topt := func(o *pilosa.ImportOptions) error {\n\t\to.SimPowerLossAfter = 1\n\t\treturn nil\n\t}\n\texpectedBalEndingAcct0 := expectedBalStartingAcct0 - 100\n\texpectedBalEndingAcct1 := expectedBalStartingAcct1 + 100\n\n\tair = createAIRUpdate(expectedBalEndingAcct0, expectedBalEndingAcct1)\n\n\tqcx = m0api.Txf().NewQcx()\n\t//vv(\"just before the SECOND ImportAtomicRecord, qcx is %p, should NOT BE NIL\", qcx)\n\terr = m0api.ImportAtomicRecord(ctx, qcx, air.Clone(), opt)\n\t//err = m0api.ImportAtomicRecord(ctx, nil, air, opt)\n\tif err != pilosa.ErrAborted {\n\t\tt.Fatalf(\"expected ErrTxnAborted but got err='%#v'\", err)\n\t}\n\t// sad path, cleanup\n\tqcx.Abort()\n\tqcx = nil\n\n\tb0, b1 := queryBalances(t, m0api, acctOwnerID, fieldAcct0, fieldAcct1, index)\n\t//vv(\"after power failure tx, balance: acct0=%v,  acct1=%v\", b0, b1)\n\n\tif b0 != expectedBalStartingAcct0 {\n\t\tt.Fatalf(\"expected %v, observed %v starting acct0 balance\", expectedBalStartingAcct0, b0)\n\t}\n\tif b1 != expectedBalStartingAcct1 {\n\t\tt.Fatalf(\"expected %v, observed %v starting acct1 balance\", expectedBalStartingAcct1, b1)\n\t}\n\t//vv(\"good: with power loss half-way, no change in account balances; acct0=%v; acct1=%v\", b0, b1)\n\n\t// next part of the test, just make sure we do the update.\n\t//vv(\"happy path: transferUSD %v from %v -> %v, with no interruption.\", transferUSD, fieldAcct0, fieldAcct1)\n\n\t// happy path with no power failure half-way through.\n\n\tqcx = m0api.Txf().NewQcx()\n\terr = m0api.ImportAtomicRecord(ctx, qcx, air.Clone())\n\tif err != nil {\n\t\tt.Fatalf(\"importing record: %v\", err)\n\t}\n\tif err := qcx.Finish(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\teb0, eb1 := queryBalances(t, m0api, acctOwnerID, fieldAcct0, fieldAcct1, index)\n\n\t// should have been applied this time.\n\tif eb0 != expectedBalEndingAcct0 ||\n\t\teb1 != expectedBalEndingAcct1 {\n\t\tt.Fatalf(\"problem: transaction did not get committed/applied. transferUSD=%v, but we see: startingBalanceAcct0=%v -> endingBalanceAcct0=%v; startingBalanceAcct1=%v -> endingBalanceAcct1=%v\", transferUSD, startingBalanceAcct0, eb0, startingBalanceAcct1, eb1)\n\t}\n\t//vv(\"ending balance: acct0=%v,  acct1=%v\", eb0, eb1)\n\n\t// clear all the bits\n\tair.Ivr[0].Clear = true\n\tair.Ivr[1].Clear = true\n\tair.Ir[0].Clear = true\n\n\tqcx = m0api.Txf().NewQcx()\n\terr = m0api.ImportAtomicRecord(ctx, qcx, air)\n\tif err != nil {\n\t\tt.Fatalf(\"importing record: %v\", err)\n\t}\n\tif err := qcx.Finish(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\teb0, eb1 = queryBalances(t, m0api, acctOwnerID, fieldAcct0, fieldAcct1, index)\n\tif eb0 != 0 ||\n\t\teb1 != 0 {\n\t\tt.Fatal(\"problem: bits did not clear\")\n\t}\n\t//vv(\"cleared balances: acct0=%v,  acct1=%v\", eb0, eb1)\n\n\tiraBit = queryIRABit(t, m0api, acctOwnerID, iraField, iraRowID, index)\n\tif iraBit {\n\t\tt.Fatal(\"IRA bit should have been cleared\")\n\t}\n}\n"
        },
        {
          "name": "txfactory.go",
          "type": "blob",
          "size": 18.8994140625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/featurebasedb/featurebase/v3/task\"\n\t\"github.com/featurebasedb/featurebase/v3/testhook\"\n\t\"github.com/featurebasedb/featurebase/v3/vprint\"\n\t\"github.com/pkg/errors\"\n)\n\n// public strings that pilosa/server/config.go can reference\nconst (\n\tRBFTxn string = \"rbf\"\n)\n\n// DetectMemAccessPastTx true helps us catch places in api and executor\n// where mmapped memory is being accessed after the point in time\n// which the transaction has committed or rolled back. Since\n// memory segments will be recycled by the underlying databases,\n// this can lead to corruption. When DetectMemAccessPastTx is true,\n// code in bolt.go will copy the transactionally viewed memory before\n// returning it for bitmap reading, and then zero it or overwrite it\n// with -2 when the Tx completes.\n//\n// Should be false for production.\nconst DetectMemAccessPastTx = false\n\nvar sep = string(os.PathSeparator)\n\n// Qcx is a (Pilosa) Query Context.\n//\n// It flexibly expresses the desired grouping of Tx for mass\n// rollback at a query's end. It provides one-time commit for\n// an atomic import write Tx that involves multiple fragments.\n//\n// The most common use of Qcx is to call GetTx() to obtain a Tx locally,\n// once the index/shard pair is known:\n//\n//\t  someFunc(qcx Qcx, idx *Index, shard uint64) (err0 error) {\n//\t\t\ttx, finisher := qcx.GetTx(Txo{Write: true, Index:idx, Shard:shard, ...})\n//\t\t\tdefer finisher(&err0)\n//\t     ...\n//\t  }\n//\n// Qcx reuses read-only Tx on the same index/shard pair. See\n// the Qcx.GetTx() for further discussion. The caveat is of\n// course that your \"new\" read Tx actually has an \"old\" view\n// of the database.\n//\n// At the moment, most\n// writes to individual shards are commited eagerly and locally\n// when the `defer finisher(&err0)` is run.\n// This is done by returning a finisher that actually Commits,\n// thus freeing the one write slot for re-use. A single\n// writer is also required by RBF, so this design accomodates\n// both.\n//\n// In contrast, the default read Tx generated (or re-used) will\n// return a no-op finisher and the group of reads as a whole\n// will be rolled back (mmap memory released) en-mass when\n// Qcx.Abort() is called at the top-most level.\n//\n// Local use of a (Tx, finisher) pair obtained from Qcx.GetTx()\n// doesn't need to care about these details. Local use should\n// always invoke finisher(&err0) or finisher(nil) to complete\n// the Tx within the local function scope.\n//\n// In summary write Tx are typically \"local\"\n// and are never saved into the TxGroup. The parallelism\n// supplied by TxGroup typically applies only to read Tx.\n//\n// The one exception is this rule is for the one write Tx\n// used during the api.ImportAtomicRecord routine. There\n// we make a special write Tx and use it for all matching writes.\n// This is then committed at the final, top-level, Qcx.Finish() call.\n//\n// See also the Qcx.GetTx() example and the TxGroup description below.\ntype Qcx struct {\n\tGrp     *TxGroup\n\tTxf     *TxFactory\n\tworkers *task.Pool\n\n\t// if we go back to using Qcx values, this must become a pointer,\n\t// or otherwise be dealt with because copies of Mutex are a no-no.\n\tmu sync.Mutex\n\n\t// RequiredForAtomicWriteTx is used by api.ImportAtomicRecord\n\t// to ensure that all writes happen on this one Tx.\n\tRequiredForAtomicWriteTx *Tx\n\n\t// efficient access to the options for RequiredForAtomicWriteTx\n\tRequiredTxo *Txo\n\n\tisRoaring bool\n\n\t// top-level context is for a write, so re-use a\n\t// writable tx for all reads and writes on each given\n\t// shard\n\twrite bool\n\n\t// don't allow automatic reuse now. Must manually call Reset, or NewQcx().\n\tdone bool\n}\n\n// Finish commits/rollsback all stored Tx. It no longer resets the\n// Qcx for further operations automatically. User must call Reset()\n// or NewQxc() again.\nfunc (q *Qcx) Finish() (err error) {\n\tq.mu.Lock()\n\tdefer q.mu.Unlock()\n\tif q.RequiredForAtomicWriteTx != nil {\n\t\tif q.RequiredTxo.Write {\n\t\t\terr = (*q.RequiredForAtomicWriteTx).Commit() // PanicOn here on 2nd. is this a double commit?\n\t\t} else {\n\t\t\t(*q.RequiredForAtomicWriteTx).Rollback()\n\t\t}\n\t}\n\terr2 := q.Grp.FinishGroup()\n\t// drop the old group so we aren't holding references to all those Tx\n\tq.Grp = q.Txf.NewTxGroup()\n\tif !q.done {\n\t\t_ = testhook.Closed(q.Txf.holder.Auditor, q, nil)\n\t}\n\tq.done = true\n\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn err2\n}\n\n// Abort rolls back all Tx generated and stored within the Qcx.\n// The Qcx is then reset and can be used again immediately.\nfunc (q *Qcx) Abort() {\n\tq.mu.Lock()\n\tdefer q.mu.Unlock()\n\tif q.RequiredForAtomicWriteTx != nil {\n\t\t(*q.RequiredForAtomicWriteTx).Rollback()\n\t}\n\tq.Grp.AbortGroup()\n\t// drop the old group so we aren't holding references to all those Tx\n\tq.Grp = q.Txf.NewTxGroup()\n\tif !q.done {\n\t\t_ = testhook.Closed(q.Txf.holder.Auditor, q, nil)\n\t}\n\tq.done = true\n}\n\n// Reset forgets everything are starts fresh with an empty\n// group, ready for use again as if NewQcx() had been called.\nfunc (q *Qcx) Reset() {\n\tq.mu.Lock()\n\tdefer q.mu.Unlock()\n\tq.unprotected_reset()\n}\n\nfunc (q *Qcx) unprotected_reset() {\n\tq.RequiredForAtomicWriteTx = nil\n\tq.RequiredTxo = nil\n\tq.Grp = q.Txf.NewTxGroup()\n\tq.done = false\n}\n\n// NewQcx allocates a freshly allocated and empty Grp.\n// The top-level Qcx is not marked writable. Non-writable\n// Qcx should not be used to request write Tx.\nfunc (f *TxFactory) NewQcx() (qcx *Qcx) {\n\tqcx = &Qcx{\n\t\tGrp: f.NewTxGroup(),\n\t\tTxf: f,\n\t}\n\tif f.typeOfTx == \"roaring\" {\n\t\tqcx.isRoaring = true\n\t}\n\tif f.holder != nil {\n\t\tif f.holder.executor != nil {\n\t\t\tqcx.workers = f.holder.executor.workers\n\t\t}\n\t\t_ = testhook.Opened(f.holder.Auditor, qcx, nil)\n\t}\n\treturn\n}\n\n// NewWritableQcx allocates a freshly allocated and empty Grp.\n// The resulting Qcx is marked writable.\nfunc (f *TxFactory) NewWritableQcx() (qcx *Qcx) {\n\tqcx = &Qcx{\n\t\tGrp: f.NewTxGroup(),\n\t\tTxf: f,\n\t}\n\tif f.holder != nil && f.holder.executor != nil {\n\t\tqcx.workers = f.holder.executor.workers\n\t}\n\tif f.typeOfTx == \"roaring\" {\n\t\tqcx.isRoaring = true\n\t}\n\t_ = testhook.Opened(f.holder.Auditor, qcx, nil)\n\tqcx.write = true\n\treturn\n}\n\nvar NoopFinisher = func(perr *error) {}\n\nvar ErrQcxDone = fmt.Errorf(\"Qcx already Aborted or Finished, so must call reset before re-use\")\n\n// GetTx is used like this:\n//\n// someFunc(ctx context.Context, shard uint64) (_ interface{}, err0 error) {\n//\n//\t\ttx, finisher := qcx.GetTx(Txo{Write: !writable, Index: idx, Shard: shard})\n//\t\tdefer finisher(&err0)\n//\n//\t\treturn e.executeIncludesColumnCallShard(ctx, tx, index, c, shard, col)\n//\t}\n//\n// Note we are tracking the returned err0 error value of someFunc(). An option instead is to say\n//\n//\tdefer finisher(nil)\n//\n// This means always Commit writes, ignoring if there were errors. This style\n// is expected to be rare compared to the typical\n//\n//\tdefer finisher(&err0)\n//\n// invocation, where err0 is your return from the enclosing function error.\n// If the Tx is local and not a part of a group, then the finisher\n// consults that error to decides whether to Commit() or Rollback().\n//\n// If instead the Tx becomes part of a group, then the local finisher() is\n// always a no-op, in deference to the Qcx.Finish()\n// or Qcx.Abort() calls.\n//\n// Take care the finisher(&err) is capturing the address of the\n// enclosing function's err and that it has not been shadowed\n// locally by another _, err := f() call. For this reason, it can\n// be clearer (and much safer) to rename the enclosing functions 'err' to 'err0',\n// to make it clear we are referring to the first and final error.\nfunc (qcx *Qcx) GetTx(o Txo) (tx Tx, finisher func(perr *error), err error) {\n\tif qcx.workers != nil {\n\t\tqcx.workers.Block()\n\t\tdefer qcx.workers.Unblock()\n\t}\n\tqcx.mu.Lock()\n\tdefer qcx.mu.Unlock()\n\n\tif qcx.done {\n\t\treturn nil, nil, ErrQcxDone\n\t}\n\n\t// roaring uses finer grain, a file per fragment rather than\n\t// db per shard. So we can't re-use the readTx. Moreover,\n\t// roaring Tx are No-ops anyway, so just give it a new Tx\n\t// everytime.\n\tif qcx.isRoaring {\n\t\treturn qcx.Txf.NewTx(o), NoopFinisher, nil\n\t}\n\n\t// qcx.write reflects the top executor determination\n\t// if a write will be happen at some point, in which case, to avoid\n\t// locking problems with multi-shard things, we (probably incorrectly)\n\t// treat every Tx as its own individual separate Tx.\n\t//\n\t// But we still want to open non-write transactions individually, we\n\t// just can't recycle them (because write operations will come in and\n\t// we want them to work and commit right away so we're not holding a write\n\t// lock for long).\n\twriteLogic := o.Write || qcx.write\n\n\t// In general, we make ALL write transactions local, and never reuse them\n\t// below. Previously this was to help lmdb.\n\t//\n\t// *However* there is one exception: when we have set RequiredForAtomicWriteTx\n\t// for the importing of an AtomicRequest, then we must use that\n\t// our single RequiredForAtomicWriteTx for all writes until it\n\t// is cleared. This one is kept separately from the read TxGroup.\n\t//\n\tif o.Write && qcx.RequiredForAtomicWriteTx != nil {\n\t\t// verify that shard and index match!\n\t\tro := qcx.RequiredTxo\n\t\tif o.Shard != ro.Shard {\n\t\t\tvprint.PanicOn(fmt.Sprintf(\"shard mismatch: o.Shard = %v while qcx.RequiredTxo.Shard = %v\", o.Shard, ro.Shard))\n\t\t}\n\t\tif o.Index == nil {\n\t\t\tvprint.PanicOn(\"o.Index annot be nil\")\n\t\t}\n\t\tif ro.Index == nil {\n\t\t\tvprint.PanicOn(\"ro.Index annot be nil\")\n\t\t}\n\t\tif o.Index.name != ro.Index.name {\n\t\t\tvprint.PanicOn(fmt.Sprintf(\"index mismatch: o.Index = %v while qcx.RequiredTxo.Index = %v\", o.Index.name, ro.Index.name))\n\t\t}\n\t\treturn *qcx.RequiredForAtomicWriteTx, NoopFinisher, nil\n\t}\n\n\tif !writeLogic && qcx.Grp != nil {\n\t\t// read, with a group in place.\n\t\tfinisher = func(perr *error) {} // finisher is a returned value\n\n\t\talready := false\n\t\ttx, already = qcx.Grp.AlreadyHaveTx(o)\n\t\tif already {\n\t\t\treturn\n\t\t}\n\t\ttx = qcx.Txf.NewTx(o)\n\t\tqcx.Grp.AddTx(tx, o)\n\t\treturn\n\t}\n\n\t// non atomic writes or not grouped reads\n\ttx = qcx.Txf.NewTx(o)\n\tif o.Write {\n\t\tfinisherDone := false\n\t\tfinisher = func(perr *error) {\n\t\t\tif finisherDone {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tfinisherDone = true // only Commit once.\n\t\t\t// so defer finisher(nil) means always Commit writes, ignoring\n\t\t\t// the enclosing functions return status.\n\t\t\tif perr == nil || *perr == nil {\n\t\t\t\tvprint.PanicOn(tx.Commit())\n\t\t\t} else {\n\t\t\t\ttx.Rollback()\n\t\t\t}\n\t\t}\n\t} else {\n\t\t// read-only txn\n\t\tfinisher = func(perr *error) {\n\t\t\ttx.Rollback()\n\t\t}\n\t}\n\treturn\n}\n\n// StartAtomicWriteTx allocates a Tx and stores it\n// in qcx.RequiredForAtomicWriteTx. All subsequent writes\n// to this shard/index will re-use it.\nfunc (qcx *Qcx) StartAtomicWriteTx(o Txo) {\n\tif !o.Write {\n\t\tvprint.PanicOn(\"must have o.Write true\")\n\t}\n\tqcx.mu.Lock()\n\tdefer qcx.mu.Unlock()\n\n\tif qcx.RequiredForAtomicWriteTx == nil {\n\t\t// new Tx needed\n\t\ttx := qcx.Txf.NewTx(o)\n\t\tqcx.RequiredForAtomicWriteTx = &tx\n\t\tqcx.RequiredTxo = &o\n\t\treturn\n\t}\n\n\t// re-using existing\n\n\t// verify that shard and index match!\n\tro := qcx.RequiredTxo\n\tif o.Shard != ro.Shard {\n\t\tvprint.PanicOn(fmt.Sprintf(\"shard mismatch: o.Shard = %v while qcx.RequiredTxo.Shard = %v\", o.Shard, ro.Shard))\n\t}\n\tif o.Index == nil {\n\t\tvprint.PanicOn(\"o.Index annot be nil\")\n\t}\n\tif ro.Index == nil {\n\t\tvprint.PanicOn(\"ro.Index annot be nil\")\n\t}\n\tif o.Index.name != ro.Index.name {\n\t\tvprint.PanicOn(fmt.Sprintf(\"index mismatch: o.Index = %v while qcx.RequiredTxo.Index = %v\", o.Index.name, ro.Index.name))\n\t}\n}\n\nfunc (qcx *Qcx) ListOpenTx() string {\n\treturn qcx.Grp.String()\n}\n\n// TxFactory abstracts the creation of Tx interface-level\n// transactions so that RBF, or Roaring-fragment-files, or several\n// of these at once in parallel, is used as the storage and transction layer.\ntype TxFactory struct {\n\ttypeOfTx string\n\n\ttyp txtype\n\n\tdbsClosed bool // idemopotent CloseDB()\n\n\tdbPerShard *DBPerShard\n\n\tholder *Holder\n}\n\n// integer types for fast switch{}\ntype txtype int\n\nconst (\n\tnoneTxn txtype = 0\n\trbfTxn  txtype = 2\n)\n\n// DirectoryName just returns a string version of the transaction type. We\n// really need to consolidate the storage backend and tx stuff because it's\n// currently rather confusing. This method should be addressed (i.e.\n// replaced/removed) during that refactor.\nfunc (ty txtype) DirectoryName() string {\n\tswitch ty {\n\tcase rbfTxn:\n\t\treturn \"rbf\"\n\t}\n\tvprint.PanicOn(fmt.Sprintf(\"unkown txtype %v\", int(ty)))\n\treturn \"\"\n}\n\nfunc MustBackendToTxtype(backend string) (typ txtype) {\n\tif strings.Contains(backend, \"_\") {\n\t\tpanic(\"blue-green comparisons removed\")\n\t}\n\n\tswitch backend {\n\tcase RBFTxn: // \"rbf\"\n\t\treturn rbfTxn\n\t}\n\tpanic(fmt.Sprintf(\"unknown backend '%v'\", backend))\n}\n\n// NewTxFactory always opens an existing database. If you\n// want to a fresh database, os.RemoveAll on dir/name ahead of time.\n// We always store files in a subdir of holderDir.\nfunc NewTxFactory(backend string, holderDir string, holder *Holder) (f *TxFactory, err error) {\n\ttyp := MustBackendToTxtype(backend)\n\n\tf = &TxFactory{\n\t\ttyp:      typ,\n\t\ttypeOfTx: backend,\n\t\tholder:   holder,\n\t}\n\tf.dbPerShard = f.NewDBPerShard(typ, holderDir, holder)\n\n\tif f.hasRBF() {\n\t\tholder.Logger.Infof(\"rbf config = %#v\", holder.cfg.RBFConfig)\n\t}\n\n\treturn f, err\n}\n\n// Open should be called only once the index metadata is loaded\n// from Holder.Open(), so we find all of our indexes.\nfunc (f *TxFactory) Open() error {\n\treturn f.dbPerShard.LoadExistingDBs()\n}\n\n// Txo holds the transaction options\ntype Txo struct {\n\tWrite    bool\n\tField    *Field\n\tIndex    *Index\n\tFragment *fragment\n\tShard    uint64\n\n\tdbs *DBShard\n}\n\nfunc (f *TxFactory) TxType() string {\n\treturn f.typeOfTx\n}\n\nfunc (f *TxFactory) TxTyp() txtype {\n\treturn f.typ\n}\n\nfunc (f *TxFactory) DeleteIndex(name string) (err error) {\n\treturn f.dbPerShard.DeleteIndex(name)\n}\n\nfunc (f *TxFactory) DeleteFieldFromStore(index, field, fieldPath string) (err error) {\n\treturn f.dbPerShard.DeleteFieldFromStore(index, field, fieldPath)\n}\n\nfunc (f *TxFactory) DeleteFragmentFromStore(\n\tindex, field, view string, shard uint64, frag *fragment,\n) (err error) {\n\treturn f.dbPerShard.DeleteFragment(index, field, view, shard, frag)\n}\n\n// CloseIndex is a no-op. This seems to be in place for debugging purposes.\nfunc (f *TxFactory) CloseIndex(idx *Index) error {\n\treturn nil\n}\n\nfunc (f *TxFactory) Close() (err error) {\n\tif f.dbsClosed {\n\t\treturn nil\n\t}\n\tf.dbsClosed = true\n\treturn f.dbPerShard.Close()\n}\n\nvar globalUseStatTx = false\n\nfunc init() {\n\tv := os.Getenv(\"PILOSA_CALLSTAT\")\n\tif v != \"\" {\n\t\tglobalUseStatTx = true\n\t}\n}\n\n// TxGroup holds a set of read transactions\n// that will en-mass have Rollback() (for the read set) called on\n// them when TxGroup.Finish() is invoked.\n// Alternatively, TxGroup.Abort() will call Rollback()\n// on all Tx group memebers.\n//\n// It used to have writes but we never actually used that because\n// of the Qcx needing to make every commit get its own transaction.\ntype TxGroup struct {\n\tmu       sync.Mutex\n\tfac      *TxFactory\n\treads    []Tx\n\tfinished bool\n\n\tall map[grpkey]Tx\n}\n\ntype grpkey struct {\n\tindex string\n\tshard uint64\n}\n\nfunc mustHaveIndexShard(o *Txo) {\n\tif o.Index == nil || o.Index.name == \"\" {\n\t\tvprint.PanicOn(\"index must be set on Txo\")\n\t}\n}\n\nfunc (g *TxGroup) AlreadyHaveTx(o Txo) (tx Tx, already bool) {\n\tmustHaveIndexShard(&o)\n\tg.mu.Lock()\n\tdefer g.mu.Unlock()\n\tkey := grpkey{index: o.Index.name, shard: o.Shard}\n\ttx, already = g.all[key]\n\treturn\n}\n\nfunc (g *TxGroup) String() (r string) {\n\tg.mu.Lock()\n\tdefer g.mu.Unlock()\n\tif len(g.reads) == 0 {\n\t\treturn \"<empty-TxGroup>\"\n\t}\n\tr += \"\\n\"\n\tfor i, tx := range g.reads {\n\t\tr += fmt.Sprintf(\"[%v]read: %#v,\\n\", i, tx)\n\t}\n\treturn r\n}\n\n// NewTxGroup\nfunc (f *TxFactory) NewTxGroup() (g *TxGroup) {\n\tg = &TxGroup{\n\t\tfac: f,\n\t\tall: make(map[grpkey]Tx),\n\t}\n\treturn\n}\n\n// AddTx adds tx to the group.\nfunc (g *TxGroup) AddTx(tx Tx, o Txo) {\n\tg.mu.Lock()\n\tdefer g.mu.Unlock()\n\tif g.finished {\n\t\tvprint.PanicOn(\"in TxGroup.Finish(): TxGroup already finished\")\n\t}\n\n\tg.reads = append(g.reads, tx)\n\n\tkey := grpkey{index: o.Index.name, shard: o.Shard}\n\tprior, ok := g.all[key]\n\tif ok {\n\t\tvprint.PanicOn(fmt.Sprintf(\"already have Tx in group for this, we should have re-used it! prior is '%v'; tx='%v'\", prior, tx))\n\t}\n\tg.all[key] = tx\n}\n\n// Finish commits the write tx and calls Rollback() on\n// the read tx contained in the group. Either Abort() or Finish() must\n// be called on the TxGroup exactly once.\nfunc (g *TxGroup) FinishGroup() (err error) {\n\tg.mu.Lock()\n\tdefer g.mu.Unlock()\n\tif g.finished {\n\t\tvprint.PanicOn(\"in TxGroup.Finish(): TxGroup already finished\")\n\t}\n\tg.finished = true\n\tfor _, r := range g.reads {\n\t\tr.Rollback()\n\t}\n\treturn\n}\n\n// Abort calls Rollback() on all the group Tx, and marks\n// the group as finished. Either Abort() or Finish() must\n// be called on the TxGroup.\nfunc (g *TxGroup) AbortGroup() {\n\tg.mu.Lock()\n\tdefer g.mu.Unlock()\n\tif g.finished {\n\t\t// defer Abort() probably gets here often by default, just ignore.\n\t\treturn\n\t}\n\tg.finished = true\n\n\tfor _, r := range g.reads {\n\t\tr.Rollback()\n\t}\n}\n\nfunc (f *TxFactory) NewTx(o Txo) (txn Tx) {\n\tdefer func() {\n\t\tif globalUseStatTx {\n\t\t\ttxn = newStatTx(txn)\n\t\t}\n\t}()\n\n\tindexName := \"\"\n\tif o.Index != nil {\n\t\tindexName = o.Index.name\n\t}\n\n\tif o.Fragment != nil {\n\t\tif o.Fragment.index() != indexName {\n\t\t\tvprint.PanicOn(fmt.Sprintf(\"inconsistent NewTx request: o.Fragment.index='%v' but indexName='%v'\", o.Fragment.index(), indexName))\n\t\t}\n\t\tif o.Fragment.shard != o.Shard {\n\t\t\tvprint.PanicOn(fmt.Sprintf(\"inconsistent NewTx request: o.Fragment.shard='%v' but o.Shard='%v'\", o.Fragment.shard, o.Shard))\n\t\t}\n\t}\n\n\t// look up in the collection of open databases, and get our\n\t// per-shard database. Opens a new one if needed.\n\tdbs, err := f.dbPerShard.GetDBShard(indexName, o.Shard, o.Index)\n\tvprint.PanicOn(err)\n\n\tif dbs.Shard != o.Shard {\n\t\tvprint.PanicOn(fmt.Sprintf(\"asked for o.Shard=%v but got dbs.Shard=%v\", int(o.Shard), int(dbs.Shard)))\n\t}\n\t//vv(\"got dbs='%p' for o.Index='%v'; shard='%v'; dbs.typ='%#v'; dbs.W='%#v'\", dbs, o.Index.name, o.Shard, dbs.typ, dbs.W)\n\to.dbs = dbs\n\n\ttx, err := dbs.NewTx(o.Write, indexName, o)\n\tif err != nil {\n\t\tvprint.PanicOn(errors.Wrap(err, \"dbs.NewTx transaction errored\"))\n\t}\n\treturn tx\n}\n\n// has to match the const strings at the top of the file.\nfunc (ty txtype) String() string {\n\tswitch ty {\n\tcase noneTxn:\n\t\treturn \"noneTxn\"\n\tcase rbfTxn:\n\t\treturn \"rbf\"\n\t}\n\tvprint.PanicOn(fmt.Sprintf(\"unhandled ty '%v' in txtype.String()\", int(ty)))\n\treturn \"\"\n}\n\nfunc dirExists(name string) bool {\n\tfi, err := os.Stat(name)\n\tif err != nil {\n\t\treturn false\n\t}\n\tif fi.IsDir() {\n\t\treturn true\n\t}\n\treturn false\n}\n\nvar _ = anyGlobalDBWrappersStillOpen // happy linter\n\nfunc anyGlobalDBWrappersStillOpen() bool {\n\treturn globalRbfDBReg.Size() != 0\n}\n\nfunc (f *TxFactory) hasRBF() bool {\n\treturn f.typ == rbfTxn\n}\n\nfunc (f *TxFactory) GetDBShardPath(index string, shard uint64, idx *Index, ty txtype, write bool) (shardPath string, err error) {\n\tdbs, err := f.dbPerShard.GetDBShard(index, shard, idx)\n\tif err != nil {\n\t\treturn \"\", errors.Wrap(err, fmt.Sprintf(\"GetDBShardPath(index='%v', shard='%v', ty='%v')\", index, shard, ty.String()))\n\t}\n\tshardPath = dbs.pathForType(ty)\n\treturn\n}\n\nfunc (txf *TxFactory) GetFieldView2ShardsMapForIndex(idx *Index) (vs *FieldView2Shards, err error) {\n\treturn txf.dbPerShard.GetFieldView2ShardsMapForIndex(idx)\n}\n"
        },
        {
          "name": "txfactory_internal_test.go",
          "type": "blob",
          "size": 0.490234375,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"testing\"\n)\n\nfunc Test_TxFactory_verifyStringConstantsMatch(t *testing.T) {\n\t// txtype.String() method MUST return strings that match\n\t// our const definitions at the top of txfactory.go.\n\tcheck := []txtype{rbfTxn}\n\texpect := []string{RBFTxn}\n\tfor i, chk := range check {\n\t\tobs := chk.String()\n\t\tif obs != expect[i] {\n\t\t\tt.Fatalf(\"expected '%v' but got '%v'\", expect[i], obs)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "txkey",
          "type": "tree",
          "content": null
        },
        {
          "name": "util.go",
          "type": "blob",
          "size": 3.1591796875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\n// util.go: a place for generic, reusable utilities.\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"os/user\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/shirou/gopsutil/v3/mem\"\n)\n\n// LeftShifted16MaxContainerKey is 0xffffffffffff0000. It is similar\n// to the roaring.maxContainerKey  0x0000ffffffffffff, but\n// shifted 16 bits to the left so its domain is the full [0, 2^64) bit space.\n// It is used to match the semantics of the roaring.OffsetRange() API.\n// This is the maximum endx value for Tx.OffsetRange(), because the lowbits,\n// as in the roaring.OffsetRange(), are not allowed to be set.\n// It is used in Tx.RoaringBitamp() to obtain the full contents of a fragment\n// from a call from tx.OffsetRange() by requesting [0, LeftShifted16MaxContainerKey)\n// with an offset of 0.\nconst LeftShifted16MaxContainerKey = uint64(0xffffffffffff0000) // or math.MaxUint64 - (1<<16 - 1), or 18446744073709486080\n\n//////////////////////////////////\n// helper utility functions\n\nfunc highbits(v uint64) uint64 { return v >> 16 }\nfunc lowbits(v uint64) uint16  { return uint16(v & 0xFFFF) }\n\n// GetLoopProgress returns the estimated remaining time to iterate through some\n// items as well as the loop completion percentage with the following\n// parameters:\n// the start time, the current time, the iteration, and the number of items\nfunc GetLoopProgress(start time.Time, now time.Time, iteration uint, total uint) (remaining time.Duration, pctDone float64) {\n\titemsLeft := total - (iteration + 1)\n\tavgItemTime := float64(now.Sub(start)) / float64(iteration+1)\n\tpctDone = (float64(iteration+1) / float64(total)) * 100\n\treturn time.Duration(avgItemTime * float64(itemsLeft)), pctDone\n}\n\ntype MemoryUsage struct {\n\tCapacity uint64 `json:\"capacity\"`\n\tTotalUse uint64 `json:\"totalUsed\"`\n}\n\n// GetMemoryUsage gets the memory usage\nfunc GetMemoryUsage() (MemoryUsage, error) {\n\tusage, err := mem.VirtualMemory()\n\tif usage == nil || err != nil {\n\t\treturn MemoryUsage{}, fmt.Errorf(\"reading virtual memory: %v\", err)\n\t}\n\treturn MemoryUsage{Capacity: usage.Total, TotalUse: usage.Used}, nil\n}\n\ntype DiskUsage struct {\n\tUsage int64 `json:\"usage\"`\n}\n\n// GetDiskUsage gets the disk usage of the path\nfunc GetDiskUsage(path string) (DiskUsage, error) {\n\tusr, _ := user.Current()\n\tdir := usr.HomeDir\n\tif path == \"~\" {\n\t\tpath = dir\n\t} else if strings.HasPrefix(path, \"~/\") {\n\t\tpath = filepath.Join(dir, path[2:])\n\t}\n\n\tvar size int64\n\terr := filepath.Walk(path, func(_ string, info os.FileInfo, err error) error {\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif !info.IsDir() {\n\t\t\tsize += info.Size()\n\t\t}\n\t\treturn err\n\t})\n\treturn DiskUsage{size}, err\n}\n\n// Rev reverses a string\nfunc Rev(input string) string {\n\tn := 0\n\trunes := make([]rune, len(input))\n\tfor _, r := range input {\n\t\trunes[n] = r\n\t\tn++\n\t}\n\trunes = runes[0:n]\n\n\tfor i := 0; i < n/2; i++ {\n\t\trunes[i], runes[n-1-i] = runes[n-1-i], runes[i]\n\t}\n\n\treturn string(runes)\n}\n\n// ReplaceFirstFromBack replaces the first instance of toReplace from the back of\n// the string s\nfunc ReplaceFirstFromBack(s, toReplace, replacement string) string {\n\treturn Rev(strings.Replace(Rev(s), Rev(toReplace), Rev(replacement), 1))\n}\n"
        },
        {
          "name": "util_test.go",
          "type": "blob",
          "size": 2.275390625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\n// util_test.go has unit tests for utility functions from util.go\n\nimport (\n\t\"testing\"\n\t\"time\"\n)\n\nfunc TestGetLoopProgress(t *testing.T) {\n\t// TODO: try to find more sneaky cases\n\tcases := []struct {\n\t\tname  string\n\t\tstart time.Time\n\t\tnow   time.Time\n\t\ti     uint\n\t\ttotal uint\n\t}{\n\t\t{\n\t\t\t\"one minute, half done\",\n\t\t\ttime.Date(1969, time.June, 9, 4, 20, 0, 0, time.UTC),\n\t\t\ttime.Date(1969, time.June, 9, 4, 21, 0, 0, time.UTC),\n\t\t\t10,\n\t\t\t20,\n\t\t},\n\t\t{\n\t\t\t\"four seconds, half done\",\n\t\t\ttime.Date(1969, time.June, 9, 4, 20, 0, 0, time.UTC),\n\t\t\ttime.Date(1969, time.June, 9, 4, 20, 4, 0, time.UTC),\n\t\t\t10,\n\t\t\t20,\n\t\t},\n\t\t{\n\t\t\t\"one minute, one second, 5 s, half done\",\n\t\t\ttime.Date(1969, time.June, 9, 4, 20, 0, 0, time.UTC),\n\t\t\ttime.Date(1969, time.June, 9, 4, 21, 1, 5, time.UTC),\n\t\t\t10,\n\t\t\t20,\n\t\t},\n\t\t{\n\t\t\t\"one minute, one done\",\n\t\t\ttime.Date(1969, time.June, 9, 4, 20, 0, 0, time.UTC),\n\t\t\ttime.Date(1969, time.June, 9, 4, 21, 0, 0, time.UTC),\n\t\t\t1,\n\t\t\t20,\n\t\t},\n\t\t{\n\t\t\t\"one minute, 1/5 done\",\n\t\t\ttime.Date(1969, time.June, 9, 4, 20, 0, 0, time.UTC),\n\t\t\ttime.Date(1969, time.June, 9, 4, 21, 0, 0, time.UTC),\n\t\t\t10,\n\t\t\t50,\n\t\t},\n\t}\n\n\tfor _, c := range cases {\n\t\tt.Run(c.name, func(t *testing.T) {\n\t\t\t// we expect that it will be the avg time per message times\n\t\t\t// the number of remaining messages\n\t\t\texpected := time.Duration((float64(c.now.Sub(c.start)) / float64(c.i+1)) * float64(c.total-(c.i+1)))\n\t\t\texpectedPct := 100 * (float64(c.i+1) / float64(c.total))\n\n\t\t\ttimeLeft, pctDone := GetLoopProgress(c.start, c.now, c.i, c.total)\n\t\t\tif timeLeft != expected {\n\t\t\t\tt.Errorf(\"Time left was incorrect, expected: %d, but got: %d\", expected, timeLeft)\n\t\t\t}\n\t\t\tif pctDone != expectedPct {\n\t\t\t\tt.Errorf(\"Percentage done was incorrect, expected: %f, but got: %f\", expectedPct, pctDone)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestGetMemoryUsage(t *testing.T) {\n\tif _, err := GetMemoryUsage(); err != nil {\n\t\tt.Fatalf(\"unexpected error getting memory usage: %v\", err)\n\t}\n}\n\nfunc TestGetDiskUsage(t *testing.T) {\n\ttdir := t.TempDir()\n\tif _, err := GetDiskUsage(tdir); err != nil {\n\t\tt.Fatalf(\"unexpected error getting disk usage: %v\", err)\n\t}\n\n\tif _, err := GetDiskUsage(\"\"); err == nil {\n\t\tt.Fatal(\"expected error getting disk usage but got nil\")\n\t}\n\n}\n"
        },
        {
          "name": "utils_internal_test.go",
          "type": "blob",
          "size": 2.4765625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/disco\"\n\t\"github.com/featurebasedb/featurebase/v3/etcd\"\n\tpnet \"github.com/featurebasedb/featurebase/v3/net\"\n\t\"github.com/featurebasedb/featurebase/v3/testhook\"\n)\n\n// utilities used by tests\n\n// NewTestCluster returns a cluster with n nodes and uses a mod-based hasher.\nfunc NewTestCluster(tb testing.TB, n int) *cluster {\n\tif n > 1 && !etcd.AllowCluster() {\n\t\ttb.Skipf(\"cluster size %d not supported in unclustered mode\", n)\n\t}\n\tpath, err := testhook.TempDir(tb, \"pilosa-cluster-\")\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tavailableShardFileFlushDuration.Set(100 * time.Millisecond)\n\tc := newCluster()\n\tc.ReplicaN = 1\n\tc.Hasher = &disco.Jmphasher{}\n\tc.Path = path\n\n\tnodes := make([]*disco.Node, 0, n)\n\n\tfor i := 0; i < n; i++ {\n\t\tnodes = append(nodes, &disco.Node{\n\t\t\tID:  fmt.Sprintf(\"node%d\", i),\n\t\t\tURI: NewTestURI(\"http\", fmt.Sprintf(\"host%d\", i), uint16(0)),\n\t\t})\n\t}\n\tc.noder = disco.NewLocalNoder(nodes)\n\n\tcNodes := c.noder.Nodes()\n\n\tc.Node = cNodes[0]\n\treturn c\n}\n\n// NewTestURI is a test URI creator that intentionally swallows errors.\nfunc NewTestURI(scheme, host string, port uint16) pnet.URI {\n\turi := pnet.DefaultURI()\n\t_ = uri.SetScheme(scheme)\n\t_ = uri.SetHost(host)\n\turi.SetPort(port)\n\treturn *uri\n}\n\nfunc NewTestURIFromHostPort(host string, port uint16) pnet.URI {\n\turi := pnet.DefaultURI()\n\t_ = uri.SetHost(host)\n\turi.SetPort(port)\n\treturn *uri\n}\n\nfunc TestReplaceFirstFromBack(t *testing.T) {\n\tfor name, test := range map[string]struct {\n\t\tinput       string\n\t\texp         string\n\t\ttoReplace   string\n\t\treplacement string\n\t}{\n\t\t\"url\": {\n\t\t\tinput:       \"https://login.microsoftonline.com/4a137d66-d161-4ae4-b1e6-07e9920874b8/oauth2/v2.0/authorize\",\n\t\t\texp:         \"https://login.microsoftonline.com/4a137d66-d161-4ae4-b1e6-07e9920874b8/oauth2/v2.0/devicecode\",\n\t\t\ttoReplace:   \"authorize\",\n\t\t\treplacement: \"devicecode\",\n\t\t},\n\t\t\"unicode\": {\n\t\t\tinput:       \"\",\n\t\t\texp:         \"\",\n\t\t\ttoReplace:   \"\",\n\t\t\treplacement: \"\",\n\t\t},\n\t\t\"multiple\": {\n\t\t\tinput:       \"cowscowscowscowscows\",\n\t\t\texp:         \"cowscowscowscowscats\",\n\t\t\ttoReplace:   \"cows\",\n\t\t\treplacement: \"cats\",\n\t\t},\n\t} {\n\t\tt.Run(name, func(t *testing.T) {\n\t\t\tif got := ReplaceFirstFromBack(test.input, test.toReplace, test.replacement); got != test.exp {\n\t\t\t\tt.Fatalf(\"expected %v, got %v\", test.exp, got)\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "verchk.go",
          "type": "blob",
          "size": 2.1513671875,
          "content": "package pilosa\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"io\"\n\t\"net/http\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\n\t\"github.com/google/uuid\"\n)\n\ntype versionChecker struct {\n\tpath   string\n\turl    string\n\tidfile string\n}\n\nfunc newVersionChecker(path, endpoint, idfile string) *versionChecker {\n\tv := versionChecker{\n\t\tpath:   path,\n\t\turl:    endpoint,\n\t\tidfile: idfile,\n\t}\n\treturn &v\n}\n\nfunc (v *versionChecker) checkIn() (*verCheckResponse, error) {\n\tid, err := v.writeClientUUID()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tbody := make(map[string]string, 2)\n\tbody[\"entry_type\"] = \"user\"\n\tbody[\"version\"] = Version\n\tbody[\"client_id\"] = id\n\n\treq, err := json.Marshal(body)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\twReq := bytes.NewReader(req)\n\n\tvar jsonResp verCheckResponse\n\tr, err := http.Post(v.url, \"application/json\", wReq)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tdata, err := io.ReadAll(r.Body)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\terr = json.Unmarshal(data, &jsonResp)\n\tif err != nil {\n\n\t\treturn nil, err\n\t}\n\treturn &jsonResp, nil\n}\n\nfunc (v *versionChecker) generateClientUUID() (string, error) {\n\tclientUUID := uuid.New()\n\tcleanedUUID := strings.Replace(clientUUID.String(), \"-\", \"\", -1)\n\treturn cleanedUUID, nil\n}\n\nfunc (v *versionChecker) writeClientUUID() (string, error) {\n\tvar filename string\n\t// if v.idfile starts with a path separator then it's an absolute path\n\t// otherwise it's a relative path and the file goes in the data directory\n\tif v.idfile[0] == os.PathSeparator {\n\t\tfilename = v.idfile\n\t} else {\n\t\tfilename = filepath.Join(v.path, v.idfile)\n\t}\n\tfh, err := os.OpenFile(filename, os.O_RDWR|os.O_CREATE, 0744)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tdefer fh.Close()\n\n\tbuf, err := os.ReadFile(filename)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\t// this just checks to see if there was anything at all in the file.\n\t// we should probably check to make sure it's a valid UUID\n\tif string(buf) == \"\" {\n\t\tid, err := v.generateClientUUID()\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\t_, err = fh.WriteString(id)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\treturn id, nil\n\t}\n\n\treturn string(buf), nil\n}\n\ntype verCheckResponse struct {\n\tVersion string `json:\"latest_version\"`\n\tError   string `json:\"error\"`\n}\n"
        },
        {
          "name": "version.go",
          "type": "blob",
          "size": 1.16015625,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"runtime\"\n\t\"time\"\n)\n\nvar Version string\nvar Commit string\nvar Variant string\nvar BuildTime string\nvar GoVersion string = runtime.Version()\nvar TrialDeadline string\n\nfunc VersionInfo(rename bool) string {\n\tvar prefix string\n\tif Variant != \"\" {\n\t\tprefix = Variant + \" \"\n\t}\n\tvar suffix string\n\tif Version != \"\" {\n\t\tsuffix = \" \" + Version\n\t} else {\n\t\tsuffix = \" v3.x\"\n\t}\n\tbuildTime := BuildTime\n\tif buildTime != \"\" {\n\t\t// Normalize the build time into a friendly format in the user's time zone.\n\t\tif t, err := time.Parse(\"2006-01-02T15:04:05+0000\", BuildTime); err == nil {\n\t\t\tbuildTime = t.Local().Format(\"Jan _2 2006 3:04PM\")\n\t\t}\n\t}\n\tswitch {\n\tcase Commit != \"\" && buildTime != \"\":\n\t\tsuffix += \" (\" + buildTime + \", \" + Commit + \")\"\n\tcase Commit != \"\":\n\t\tsuffix += \" (\" + Commit + \")\"\n\tcase buildTime != \"\":\n\t\tsuffix += \" (\" + buildTime + \")\"\n\t}\n\tsuffix += \" \" + GoVersion\n\tif TrialDeadline != \"\" {\n\t\tsuffix += \" limited time trial ends on: \" + TrialDeadline\n\t}\n\tproductName := \"Pilosa\"\n\tif rename {\n\t\tproductName = \"FeatureBase\"\n\t}\n\treturn prefix + productName + suffix\n}\n"
        },
        {
          "name": "view.go",
          "type": "blob",
          "size": 16.201171875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"math\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/pql\"\n\t\"github.com/featurebasedb/featurebase/v3/roaring\"\n\t\"github.com/featurebasedb/featurebase/v3/testhook\"\n\t\"github.com/featurebasedb/featurebase/v3/vprint\"\n\t\"github.com/pkg/errors\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\n// View layout modes.\nconst (\n\t// standard view holds regular set/mutex data\n\tviewStandard = \"standard\"\n\t// bsig_X view holds BSI data for X\n\tviewBSIGroupPrefix = \"bsig_\"\n\t// existence view holds existence bits for a specific field\n\tviewExistence = \"existence\"\n)\n\n// view represents a container for field data.\ntype view struct {\n\tmu            sync.RWMutex\n\tpath          string\n\tindex         string\n\tfield         string\n\tname          string\n\tqualifiedName string\n\n\tholder *Holder\n\tidx    *Index\n\tfld    *Field\n\n\tfieldType string\n\tcacheType string\n\tcacheSize uint32\n\n\t// Fragments by shard.\n\tfragments map[uint64]*fragment\n\n\tbroadcaster broadcaster\n\n\tknownShards       *roaring.Bitmap\n\tknownShardsCopied uint32\n\n\tclosing chan struct{}\n}\n\n// newView returns a new instance of View.\nfunc newView(holder *Holder, path, index, field, name string, fieldOptions FieldOptions) *view {\n\tvprint.PanicOn(ValidateName(name))\n\n\treturn &view{\n\t\tpath:          path,\n\t\tindex:         index,\n\t\tfield:         field,\n\t\tname:          name,\n\t\tqualifiedName: FormatQualifiedViewName(index, field, name),\n\n\t\tholder: holder,\n\n\t\tfieldType: fieldOptions.Type,\n\t\tcacheType: fieldOptions.CacheType,\n\t\tcacheSize: fieldOptions.CacheSize,\n\n\t\tfragments: make(map[uint64]*fragment),\n\n\t\tbroadcaster: NopBroadcaster,\n\t\tknownShards: roaring.NewSliceBitmap(),\n\n\t\tclosing: make(chan struct{}),\n\t}\n}\n\n// addKnownShard adds a known shard to v, which you should only do when\n// holding the lock -- but that's probably a given, since you're presumably\n// calling it because you were potentially altering the shard list. Since\n// you have the write lock, availableShards() can't be happening right now.\n// Either it'll get the previous value or the next value of knownShards,\n// and either is probably fine.\n//\n// This means that we only copy the (probably tiny) bitmap if we're\n// modifying it after it's been read. If it never gets read, knownShardsCopied\n// never changes. If it gets read, then we treat that one as immutable --\n// we never modify it again, because the field code might be reading it, so\n// we make a fresh copy. Since shards almost never change, the expected\n// behavior is that we call addKnownShard a lot during initial startup,\n// when knownShardsCopied is 0, and then after that calls to availableShards\n// return that bitmap, and set knownShardsCopied to 1, but we rarely modify\n// the list.\nfunc (v *view) addKnownShard(shard uint64) {\n\tv.notifyIfNewShard(shard)\n\tif atomic.LoadUint32(&v.knownShardsCopied) == 1 {\n\t\tv.knownShards = v.knownShards.Clone()\n\t\tatomic.StoreUint32(&v.knownShardsCopied, 0)\n\t}\n\t_, err := v.knownShards.Add(shard)\n\tvprint.PanicOn(err)\n}\n\n// removeKnownShard removes a known shard from v. See the notes on addKnownShard.\nfunc (v *view) removeKnownShard(shard uint64) {\n\tif atomic.LoadUint32(&v.knownShardsCopied) == 1 {\n\t\tv.knownShards = v.knownShards.Clone()\n\t\tatomic.StoreUint32(&v.knownShardsCopied, 0)\n\t}\n\t_, _ = v.knownShards.Remove(shard)\n}\n\n// openWithShardSet opens the view. Importantly, it\n// only opens the fragments that have data. This saves\n// a ton of time. If you have no data and want a new\n// view, call view.openEmpty().\nfunc (v *view) openWithShardSet(ss *shardSet) error {\n\tif v.knownShards == nil {\n\t\tv.knownShards = roaring.NewSliceBitmap()\n\t}\n\n\t// Never keep a cache for field views.\n\tif strings.HasPrefix(v.name, viewBSIGroupPrefix) {\n\t\tv.cacheType = CacheTypeNone\n\t}\n\n\tshards := ss.CloneMaybe()\n\n\tfrags := make([]*fragment, 0, len(shards))\n\tfor shard := range shards {\n\t\tfrag := v.newFragment(shard)\n\t\tfrags = append(frags, frag)\n\t\tv.fragments[frag.shard] = frag\n\t}\n\n\tnGoro := runtime.NumCPU()\n\tif v.idx.holder.txf.TxType() != \"roaring\" {\n\t\tnGoro = nGoro / 4\n\t}\n\tif nGoro < 4 {\n\t\tnGoro = 4\n\t}\n\tvar eg errgroup.Group\n\tthrottle := make(chan struct{}, nGoro)\n\n\tfor i := range frags {\n\t\t// create a new variable frag on each time through\n\t\t// the loop (instead of i, frag := range frags)\n\t\t// so that the closure run on the\n\t\t// goroutine has its own variable.\n\t\tfrag := frags[i]\n\t\tthrottle <- struct{}{}\n\t\teg.Go(func() error {\n\t\t\tdefer func() {\n\t\t\t\t<-throttle\n\t\t\t}()\n\t\t\tif err := frag.Open(); err != nil {\n\t\t\t\treturn fmt.Errorf(\"open fragment: shard=%d, err=%s\", frag.shard, err)\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t}\n\n\terr := eg.Wait()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// serial, not parallel, because no locking inside addKnownShard at the moment.\n\t// TODO(jea): is this slow on a cluster? can we optimize it\n\t// by running it on a goroutine in the background?\n\tfor shard := range shards {\n\t\tv.addKnownShard(shard)\n\t}\n\n\t_ = testhook.Opened(v.holder.Auditor, v, nil)\n\tv.holder.Logger.Debugf(\"successfully opened index/field/view: %s/%s/%s\", v.index, v.field, v.name)\n\treturn nil\n}\n\n// openEmpty opens and initializes a new view that has no\n// data. If you have data already, then use view.openWithShardSet()\nfunc (v *view) openEmpty() error {\n\tif v.knownShards == nil {\n\t\tv.knownShards = roaring.NewSliceBitmap()\n\t}\n\n\t// Never keep a cache for field views.\n\tif strings.HasPrefix(v.name, viewBSIGroupPrefix) {\n\t\tv.cacheType = CacheTypeNone\n\t}\n\n\tif err := func() error {\n\t\t// Ensure the view's path exists.\n\t\tv.holder.Logger.Debugf(\"ensure view path exists: %s\", v.path)\n\t\terr := os.MkdirAll(v.path, 0750)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"creating view directory\")\n\t\t}\n\t\terr = os.MkdirAll(filepath.Join(v.path, \"fragments\"), 0750)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"creating fragments directory\")\n\t\t}\n\n\t\treturn nil\n\t}(); err != nil {\n\t\tv.close()\n\t\treturn err\n\t}\n\n\t_ = testhook.Opened(v.holder.Auditor, v, nil)\n\treturn nil\n}\n\nvar workQueue = make(chan struct{}, runtime.NumCPU()*2)\n\n// close closes the view and its fragments.\nfunc (v *view) close() error {\n\tv.mu.Lock()\n\tdefer v.mu.Unlock()\n\tclose(v.closing)\n\tdefer func() {\n\t\t_ = testhook.Closed(v.holder.Auditor, v, nil)\n\t}()\n\n\t// Close all fragments.\n\teg, ctx := errgroup.WithContext(context.Background())\nfragLoop:\n\tfor _, loopFrag := range v.fragments {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\tbreak fragLoop\n\t\tdefault:\n\t\t\tfrag := loopFrag\n\t\t\tworkQueue <- struct{}{}\n\t\t\teg.Go(func() error {\n\t\t\t\tdefer func() {\n\t\t\t\t\t<-workQueue\n\t\t\t\t}()\n\n\t\t\t\tif err := frag.Close(); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, \"closing fragment\")\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t}\n\t}\n\terr := eg.Wait()\n\tv.fragments = make(map[uint64]*fragment)\n\tv.knownShards = nil\n\treturn err\n}\n\nfunc (v *view) flushCaches() {\n\t// we don't have a lock/cache of the closing mutex here, because\n\t// individual view objects never get reopened, just discarded and recreated.\n\tfor _, f := range v.allFragments() {\n\t\tselect {\n\t\tcase <-v.closing:\n\t\t\treturn\n\t\tdefault:\n\t\t\tif err := f.FlushCache(); err != nil {\n\t\t\t\tv.holder.Logger.Errorf(\"flushing cache: err=%s, path=%s\", err, f.cachePath())\n\t\t\t}\n\t\t}\n\t}\n}\n\n// availableShards returns a bitmap of shards which contain data.\nfunc (v *view) availableShards() *roaring.Bitmap {\n\t// A read lock prevents anything with the write lock from being\n\t// active, so anything that's calling add/removeKnownShard won't\n\t// be doing it here. But we do need to indicate that we came\n\t// through, but we don't want to block on a write lock. So we\n\t// use an atomic for that.\n\tv.mu.RLock()\n\tdefer v.mu.RUnlock()\n\tatomic.StoreUint32(&v.knownShardsCopied, 1)\n\treturn v.knownShards\n}\n\n// Fragment returns a fragment in the view by shard.\nfunc (v *view) Fragment(shard uint64) *fragment {\n\tv.mu.RLock()\n\tdefer v.mu.RUnlock()\n\treturn v.fragments[shard]\n}\n\n// allFragments returns a list of all fragments in the view.\nfunc (v *view) allFragments() []*fragment {\n\tv.mu.RLock()\n\tdefer v.mu.RUnlock()\n\n\tother := make([]*fragment, 0, len(v.fragments))\n\tfor _, fragment := range v.fragments {\n\t\tother = append(other, fragment)\n\t}\n\treturn other\n}\n\n// recalculateCaches recalculates the cache on every fragment in the view.\nfunc (v *view) recalculateCaches() {\n\tfor _, fragment := range v.allFragments() {\n\t\tfragment.RecalculateCache()\n\t}\n}\n\nfunc (v *view) Name() string {\n\treturn v.name\n}\n\nfunc (v *view) isClosing() bool {\n\tselect {\n\tcase <-v.closing:\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// CreateFragmentIfNotExists returns a fragment in the view by shard.\nfunc (v *view) CreateFragmentIfNotExists(shard uint64) (*fragment, error) {\n\tv.mu.Lock()\n\tdefer v.mu.Unlock()\n\n\tif v.isClosing() {\n\t\treturn nil, fmt.Errorf(\"cannot create fragment, view is closed\")\n\t}\n\n\t// Find fragment in cache first.\n\tif frag := v.fragments[shard]; frag != nil {\n\t\treturn frag, nil\n\t}\n\n\t// Initialize and open fragment.\n\tfrag := v.newFragment(shard)\n\tif err := frag.Open(); err != nil {\n\t\treturn nil, errors.Wrap(err, \"opening fragment\")\n\t}\n\n\tv.fragments[shard] = frag\n\tv.addKnownShard(shard)\n\treturn frag, nil\n}\n\nfunc (v *view) notifyIfNewShard(shard uint64) {\n\t// if single node, don't bother serializing only to drop it b/c\n\t// we won't send to ourselves.\n\tsrv, ok := v.broadcaster.(*Server)\n\tif ok && len(srv.cluster.Nodes()) == 1 {\n\t\treturn\n\t}\n\n\tif v.knownShards.Contains(shard) { //checks the fields remoteShards bitmap to see if broadcast needed\n\t\treturn\n\t}\n\n\tbroadcastChan := make(chan struct{})\n\n\tgo func() {\n\t\terr := v.holder.sendOrSpool(&CreateShardMessage{\n\t\t\tIndex: v.index,\n\t\t\tField: v.field,\n\t\t\tShard: shard,\n\t\t})\n\t\tif err != nil {\n\t\t\tv.holder.Logger.Errorf(\"broadcasting create shard: %v\", err)\n\t\t}\n\t\tclose(broadcastChan)\n\t}()\n\n\ttimer := time.NewTimer(50 * time.Millisecond)\n\tselect {\n\tcase <-broadcastChan:\n\t\ttimer.Stop()\n\tcase <-timer.C:\n\t\tv.holder.Logger.Debugf(\"broadcasting create shard took >50ms\")\n\t}\n}\n\nfunc (v *view) newFragment(shard uint64) *fragment {\n\tfrag := newFragment(v.holder, v.idx, v.fld, v, shard)\n\tfrag.CacheType = v.cacheType\n\tfrag.CacheSize = v.cacheSize\n\tif v.fieldType == FieldTypeMutex {\n\t\tfrag.mutexVector = newRowsVector(frag)\n\t} else if v.fieldType == FieldTypeBool {\n\t\tfrag.mutexVector = newBoolVector(frag)\n\t}\n\treturn frag\n}\n\n// deleteFragment removes the fragment from the view.\nfunc (v *view) deleteFragment(shard uint64) error {\n\tv.mu.Lock()\n\tdefer v.mu.Unlock()\n\tf := v.fragments[shard]\n\tif f == nil {\n\t\treturn ErrFragmentNotFound\n\t}\n\n\tv.holder.Logger.Infof(\"delete fragment: (%s/%s/%s) %d\", v.index, v.field, v.name, shard)\n\n\tidx := f.holder.Index(v.index)\n\tf.Close()\n\tif err := idx.holder.txf.DeleteFragmentFromStore(f.index(), f.field(), f.view(), f.shard, f); err != nil {\n\t\treturn errors.Wrap(err, \"DeleteFragment\")\n\t}\n\tdelete(v.fragments, shard)\n\tv.removeKnownShard(shard)\n\n\treturn nil\n}\n\n// row returns a row for a shard of the view.\nfunc (v *view) row(qcx *Qcx, rowID uint64) (*Row, error) {\n\trow := NewRow()\n\tfor _, frag := range v.allFragments() {\n\t\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Index: v.idx, Fragment: frag, Shard: frag.shard})\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tdefer finisher(&err)\n\t\tfr, err := frag.row(tx, rowID)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t} else if fr == nil {\n\t\t\tcontinue\n\t\t}\n\t\trow.Merge(fr)\n\t}\n\treturn row, nil\n\n}\n\n// mutexCheck checks all available fragments for duplicate values. The return\n// is map[column]map[shard][]values for collisions only.\nfunc (v *view) mutexCheck(ctx context.Context, qcx *Qcx, details bool, limit int) (map[uint64]map[uint64][]uint64, error) {\n\t// We don't need the context, we just want the context-awareness on the error groups.\n\t// It would be nice if the inner functions could use this too...\n\teg, _ := errgroup.WithContext(ctx)\n\tthrottle := make(chan struct{}, runtime.NumCPU())\n\tfrags := v.allFragments()\n\tresults := make([]map[uint64][]uint64, len(frags))\n\tfor i, frag := range frags {\n\t\t// local copies for the goroutine to use\n\t\ti, frag := i, frag\n\t\teg.Go(func() error {\n\t\t\t// limit simultaneous parallel goroutines associated with this\n\t\t\tthrottle <- struct{}{}\n\t\t\tdefer func() {\n\t\t\t\t<-throttle\n\t\t\t}()\n\t\t\ttx, finisher, err := qcx.GetTx(Txo{Index: v.idx, Shard: frag.shard})\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tdefer finisher(&err)\n\t\t\tresults[i], err = frag.mutexCheck(tx, details, limit)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t}\n\terr := eg.Wait()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tout := map[uint64]map[uint64][]uint64{}\n\t// We would use MaxInt here, but it's new with go 1.17. In practice if\n\t// you have 2 billion duplicates you're sorta screwed anyway.\n\tif limit == 0 {\n\t\tlimit = math.MaxInt32\n\t}\n\tcount := 0\n\tfor i, result := range results {\n\t\tif len(result) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tout[frags[i].shard] = result\n\t\tcount += len(result)\n\t\t// if we have enough, stop\n\t\tif count > limit {\n\t\t\tbreak\n\t\t}\n\t}\n\treturn out, nil\n}\n\n// setBit sets a bit within the view.\nfunc (v *view) setBit(qcx *Qcx, rowID, columnID uint64) (changed bool, err error) {\n\tshard := columnID / ShardWidth\n\ttx, finisher, err := qcx.GetTx(Txo{Write: true, Index: v.idx, Shard: shard})\n\tdefer finisher(&err)\n\tvar frag *fragment\n\tfrag, err = v.CreateFragmentIfNotExists(shard)\n\tif err != nil {\n\t\treturn changed, err\n\t}\n\n\treturn frag.setBit(tx, rowID, columnID)\n}\n\n// clearBit clears a bit within the view.\nfunc (v *view) clearBit(qcx *Qcx, rowID, columnID uint64) (changed bool, err error) {\n\tshard := columnID / ShardWidth\n\ttx, finisher, err := qcx.GetTx(Txo{Write: true, Index: v.idx, Shard: shard})\n\tdefer finisher(&err)\n\tfrag := v.Fragment(shard)\n\tif frag == nil {\n\t\treturn false, nil\n\t}\n\n\treturn frag.clearBit(tx, rowID, columnID)\n}\n\n// value uses a column of bits to read a multi-bit value.\nfunc (v *view) value(qcx *Qcx, columnID uint64, bitDepth uint64) (value int64, exists bool, err error) {\n\tshard := columnID / ShardWidth\n\ttx, finisher, err := qcx.GetTx(Txo{Write: false, Index: v.idx, Shard: shard})\n\tdefer finisher(&err)\n\tfrag, err := v.CreateFragmentIfNotExists(shard)\n\tif err != nil {\n\t\treturn value, exists, err\n\t}\n\n\treturn frag.value(tx, columnID, bitDepth)\n}\n\n// setValue uses a column of bits to set a multi-bit value.\nfunc (v *view) setValue(qcx *Qcx, columnID uint64, bitDepth uint64, value int64) (changed bool, err error) {\n\tshard := columnID / ShardWidth\n\ttx, finisher, err := qcx.GetTx(Txo{Write: true, Index: v.idx, Shard: shard})\n\tdefer finisher(&err)\n\tfrag, err := v.CreateFragmentIfNotExists(shard)\n\tif err != nil {\n\t\treturn changed, err\n\t}\n\n\treturn frag.setValue(tx, columnID, bitDepth, value)\n}\n\n// clearValue removes a specific value assigned to columnID\nfunc (v *view) clearValue(qcx *Qcx, columnID uint64, bitDepth uint64, value int64) (changed bool, err error) {\n\tshard := columnID / ShardWidth\n\ttx, finisher, err := qcx.GetTx(Txo{Write: true, Index: v.idx, Shard: shard})\n\tdefer finisher(&err)\n\tfrag := v.Fragment(shard)\n\tif frag == nil {\n\t\treturn false, nil\n\t}\n\n\treturn frag.clearValue(tx, columnID, bitDepth, value)\n}\n\n// rangeOp returns rows with a field value encoding matching the predicate.\nfunc (v *view) rangeOp(qcx *Qcx, op pql.Token, bitDepth uint64, predicate int64) (_ *Row, err0 error) {\n\tr := NewRow()\n\tfor _, frag := range v.allFragments() {\n\n\t\ttx, finisher, err := qcx.GetTx(Txo{Write: !writable, Index: v.idx, Shard: frag.shard})\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tdefer finisher(&err0)\n\n\t\tother, err := frag.rangeOp(tx, op, bitDepth, predicate)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tr = r.Union(other)\n\t}\n\treturn r, nil\n}\n\nfunc (v *view) bitDepth(shards []uint64) (uint64, error) {\n\tvar maxBitDepth uint64\n\n\tfor _, shard := range shards {\n\t\tv.mu.RLock()\n\t\tfrag, ok := v.fragments[shard]\n\t\tv.mu.RUnlock()\n\t\tif !ok || frag == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tbd, err := frag.bitDepth()\n\t\tif err != nil {\n\t\t\treturn 0, errors.Wrapf(err, \"getting fragment(%d) bit depth\", shard)\n\t\t}\n\n\t\tif bd > maxBitDepth {\n\t\t\tmaxBitDepth = bd\n\t\t}\n\t}\n\n\treturn maxBitDepth, nil\n}\n\n// ViewInfo represents schema information for a view.\ntype ViewInfo struct {\n\tName string `json:\"name\"`\n}\n\ntype viewInfoSlice []*ViewInfo\n\nfunc (p viewInfoSlice) Swap(i, j int)      { p[i], p[j] = p[j], p[i] }\nfunc (p viewInfoSlice) Len() int           { return len(p) }\nfunc (p viewInfoSlice) Less(i, j int) bool { return p[i].Name < p[j].Name }\n\n// FormatQualifiedViewName generates a qualified name for the view to be used with Tx operations.\nfunc FormatQualifiedViewName(index, field, view string) string {\n\treturn fmt.Sprintf(\"%s\\x00%s\\x00%s\\x00\", index, field, view)\n}\n"
        },
        {
          "name": "view_internal_test.go",
          "type": "blob",
          "size": 2.044921875,
          "content": "// Copyright 2022 Molecula Corp. (DBA FeatureBase).\n// SPDX-License-Identifier: Apache-2.0\npackage pilosa\n\nimport (\n\t\"testing\"\n\t\"time\"\n\n\t\"golang.org/x/sync/errgroup\"\n)\n\n// mustOpenView returns a new instance of View with a temporary path.\nfunc mustOpenView(tb testing.TB) *view {\n\t_, _, _, v := newTestView(tb)\n\tif err := v.openEmpty(); err != nil {\n\t\ttb.Fatalf(\"opening empty test view: %v\", err)\n\t}\n\treturn v\n}\n\n// Ensure view can open and retrieve a fragment.\nfunc TestView_DeleteFragment(t *testing.T) {\n\tv := mustOpenView(t)\n\n\tshard := uint64(9)\n\n\t// Create fragment.\n\tfragment, err := v.CreateFragmentIfNotExists(shard)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t} else if fragment == nil {\n\t\tt.Fatal(\"expected fragment\")\n\t}\n\n\terr = v.deleteFragment(shard)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif v.Fragment(shard) != nil {\n\t\tt.Fatal(\"fragment still exists in view\")\n\t}\n\n\t// Recreate fragment with same shard, verify that the old fragment was not reused.\n\tfragment2, err := v.CreateFragmentIfNotExists(shard)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t} else if fragment == fragment2 {\n\t\tt.Fatal(\"failed to create new fragment\")\n\t}\n}\n\n// Ensure that simultaneous attempts to grab a new fragment don't clash even\n// if the broadcast operation takes a bit of time.\nfunc TestView_CreateFragmentRace(t *testing.T) {\n\tvar creates errgroup.Group\n\tv := mustOpenView(t)\n\n\t// Use a broadcaster which intentionally fails.\n\tv.broadcaster = delayBroadcaster{delay: 10 * time.Millisecond}\n\n\tshard := uint64(0)\n\n\tcreates.Go(func() error {\n\t\t_, err := v.CreateFragmentIfNotExists(shard)\n\t\treturn err\n\t})\n\tcreates.Go(func() error {\n\t\t_, err := v.CreateFragmentIfNotExists(shard)\n\t\treturn err\n\t})\n\terr := creates.Wait()\n\tif err != nil {\n\t\tt.Fatalf(\"unexpected error: %v\", err)\n\t}\n}\n\n// delayBroadcaster is a nopBroadcaster with a configurable delay.\ntype delayBroadcaster struct {\n\tnopBroadcaster\n\tdelay time.Duration\n}\n\n// SendSync is an implementation of Broadcaster SendSync which delays for a\n// specified interval before succeeding.\nfunc (d delayBroadcaster) SendSync(Message) error {\n\ttime.Sleep(d.delay)\n\treturn nil\n}\n"
        },
        {
          "name": "vprint",
          "type": "tree",
          "content": null
        },
        {
          "name": "wire_response.go",
          "type": "blob",
          "size": 7.93359375,
          "content": "package pilosa\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"log\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/featurebasedb/featurebase/v3/dax\"\n\t\"github.com/featurebasedb/featurebase/v3/pql\"\n\t\"github.com/pkg/errors\"\n)\n\n// WireQueryResponse is the standard featurebase response type which can be\n// serialized and sent over the wire.\ntype WireQueryResponse struct {\n\tSchema        WireQuerySchema        `json:\"schema\"`\n\tData          [][]interface{}        `json:\"data\"`\n\tError         string                 `json:\"error\"`\n\tWarnings      []string               `json:\"warnings\"`\n\tQueryPlan     map[string]interface{} `json:\"query-plan\"`\n\tExecutionTime int64                  `json:\"execution-time\"`\n}\n\n// WireQuerySchema is a list of Fields which map to the data columns in the\n// Response.\ntype WireQuerySchema struct {\n\tFields []*WireQueryField `json:\"fields\"`\n}\n\n// WireQueryField is a field name along with a supported BaseType and type\n// information.\ntype WireQueryField struct {\n\tName     dax.FieldName          `json:\"name\"`\n\tType     string                 `json:\"type\"`      // human readable display (e.g. \"decimal(2)\")\n\tBaseType dax.BaseType           `json:\"base-type\"` // for programmatic switching on type (e.g. \"decimal\")\n\tTypeInfo map[string]interface{} `json:\"type-info\"` // type modifiers (like scale), but not constraints (like min/max)\n}\n\n// UnmarshalJSON is a custom unmarshaller for the SQLResponse that converts the\n// value types in `Data` based on the types in `Schema`.\nfunc (s *WireQueryResponse) UnmarshalJSON(in []byte) error {\n\treturn s.UnmarshalJSONTyped(in, false)\n}\n\n// UnmarshalJSONTyped is a temporary until we send typed values back in sql\n// responses. At that point, we can get rid of the typed=false path. In order to\n// do that, we need sql3 to return typed values, and we need the sql3/test/defs\n// to define results as typed values (like `IDSet`) instead of (for example)\n// `[]int64`.\nfunc (s *WireQueryResponse) UnmarshalJSONTyped(in []byte, typed bool) error {\n\ttype Alias WireQueryResponse\n\tvar aux Alias\n\n\tdec := json.NewDecoder(bytes.NewReader(in))\n\tdec.UseNumber()\n\terr := dec.Decode(&aux)\n\tif err != nil {\n\t\treturn err\n\t}\n\t*s = WireQueryResponse(aux)\n\n\t// If the SQLResponse contains an error, don't bother doing any conversions\n\t// on the data.\n\tif s.Error != \"\" {\n\t\treturn nil\n\t}\n\n\t// Try to convert the types in the TypeInfo map for each field in the\n\t// schema.\n\tfor _, fld := range s.Schema.Fields {\n\t\t// TODO(tlt): we can remove these two \"ToLower\" calls once sql3 is\n\t\t// returning dax.FieldType (i.e. lowercase).\n\t\tfld.Type = strings.ToLower(fld.Type)\n\t\tfld.BaseType = dax.BaseType(strings.ToLower(string(fld.BaseType)))\n\t\tfor k, v := range fld.TypeInfo {\n\t\t\tswitch k {\n\t\t\tcase \"scale\":\n\t\t\t\tswitch n := v.(type) {\n\t\t\t\tcase float64:\n\t\t\t\t\tfld.TypeInfo[k] = int64(n)\n\t\t\t\tcase json.Number:\n\t\t\t\t\tfld.TypeInfo[k], _ = n.Int64()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Try to convert the data types based on the headers.\n\tfor i := range s.Data {\n\t\tfor j, hdr := range s.Schema.Fields {\n\t\t\tswitch hdr.BaseType {\n\t\t\tcase dax.BaseTypeID, dax.BaseTypeInt:\n\t\t\t\tjn := s.Data[i][j]\n\t\t\t\tif v, ok := jn.(json.Number); ok {\n\t\t\t\t\tif x, err := v.Int64(); err == nil {\n\t\t\t\t\t\ts.Data[i][j] = x\n\t\t\t\t\t} else {\n\t\t\t\t\t\treturn errors.Wrap(err, \"can't be decoded as int64\")\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\tcase dax.BaseTypeIDSet:\n\t\t\t\tif src, ok := s.Data[i][j].([]interface{}); ok {\n\t\t\t\t\tif typed {\n\t\t\t\t\t\tval := make(IDSet, len(src))\n\t\t\t\t\t\tfor k := range src {\n\t\t\t\t\t\t\tv := src[k].(json.Number)\n\t\t\t\t\t\t\tif x, err := v.Int64(); err == nil {\n\t\t\t\t\t\t\t\tval[k] = x\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\treturn errors.Wrap(err, \"can't be decoded as int64\")\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\ts.Data[i][j] = val\n\t\t\t\t\t} else {\n\t\t\t\t\t\tval := make([]int64, len(src))\n\t\t\t\t\t\tfor k := range src {\n\t\t\t\t\t\t\tv := src[k].(json.Number)\n\t\t\t\t\t\t\tif x, err := v.Int64(); err == nil {\n\t\t\t\t\t\t\t\tval[k] = x\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\treturn errors.Wrap(err, \"can't be decoded as int64\")\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\ts.Data[i][j] = val\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\tcase dax.BaseTypeDecimal:\n\t\t\t\tif jn, ok := s.Data[i][j].(json.Number); ok {\n\t\t\t\t\tvar scale int64\n\t\t\t\t\tif scaleVal, ok := hdr.TypeInfo[\"scale\"]; !ok {\n\t\t\t\t\t\treturn errors.New(\"decimal does not have a scale\")\n\t\t\t\t\t} else if scaleInt64, ok := scaleVal.(int64); !ok {\n\t\t\t\t\t\treturn errors.New(\"scale can't be cast to int64\")\n\t\t\t\t\t} else {\n\t\t\t\t\t\tscale = scaleInt64\n\t\t\t\t\t}\n\n\t\t\t\t\tformat := fmt.Sprintf(\"%%.%df\", scale)\n\t\t\t\t\tf, err := jn.Float64()\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn errors.Wrap(err, \"parsing decimal\")\n\t\t\t\t\t}\n\t\t\t\t\tdec, err := pql.ParseDecimal(fmt.Sprintf(format, f))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn errors.Wrap(err, \"parsing decimal\")\n\t\t\t\t\t}\n\t\t\t\t\tif dec.Scale != scale {\n\t\t\t\t\t\tdec = pql.NewDecimal(dec.ToInt64(scale), scale)\n\t\t\t\t\t}\n\t\t\t\t\ts.Data[i][j] = dec\n\t\t\t\t}\n\n\t\t\tcase dax.BaseTypeStringSet:\n\t\t\t\tif src, ok := s.Data[i][j].([]interface{}); ok {\n\t\t\t\t\tif typed {\n\t\t\t\t\t\tval := make(StringSet, len(src))\n\t\t\t\t\t\tfor k := range src {\n\t\t\t\t\t\t\tval[k] = src[k].(string)\n\t\t\t\t\t\t}\n\t\t\t\t\t\ts.Data[i][j] = val\n\t\t\t\t\t} else {\n\t\t\t\t\t\tval := make([]string, len(src))\n\t\t\t\t\t\tfor k := range src {\n\t\t\t\t\t\t\tval[k] = src[k].(string)\n\t\t\t\t\t\t}\n\t\t\t\t\t\ts.Data[i][j] = val\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\tcase dax.BaseTypeTimestamp:\n\t\t\t\tif src, ok := s.Data[i][j].(string); ok && src != \"\" {\n\t\t\t\t\tval, err := time.ParseInLocation(time.RFC3339Nano, src, time.UTC)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn errors.Wrap(err, \"parsing timestamp\")\n\t\t\t\t\t}\n\t\t\t\t\ts.Data[i][j] = val\n\t\t\t\t}\n\n\t\t\tcase dax.BaseTypeBool, dax.BaseTypeString:\n\t\t\t\t// no need to convert\n\n\t\t\tdefault:\n\t\t\t\tlog.Printf(\"WARNING: unimplemented: %s\", hdr.BaseType)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// IDSet is a return type specific to SQLResponse types.\ntype IDSet []int64\n\nfunc (ii IDSet) String() string {\n\tvar sb strings.Builder\n\tsb.WriteString(\"[\")\n\tfor i := range ii {\n\t\tif i > 0 {\n\t\t\tsb.WriteString(\", \")\n\t\t}\n\t\tsb.WriteString(fmt.Sprintf(\"%d\", ii[i]))\n\t}\n\tsb.WriteString(\"]\")\n\n\treturn sb.String()\n}\n\n// StringSet is a return type specific to SQLResponse types.\ntype StringSet []string\n\nfunc (ss StringSet) String() string {\n\tvar sb strings.Builder\n\tsb.WriteString(\"[\")\n\tfor i := range ss {\n\t\tif i > 0 {\n\t\t\tsb.WriteString(\", \")\n\t\t}\n\t\tsb.WriteString(\"'\" + ss[i] + \"'\")\n\t}\n\tsb.WriteString(\"]\")\n\n\treturn sb.String()\n}\n\n// ShowColumnsResponse returns a structure which is specific to a `SHOW COLUMNS`\n// statement, derived from the results in the WireQueryResponse. This is kind of\n// a crude way to unmarshal a WireQueryResponse into a type which is specific to\n// the sql operation.\n// TODO(tlt): see if we can standardize on this logic, because it would be useful to\n// have the same thing for SHOW DATABASES and SHOW TABLES.\n// TODO(tlt): the fields unmarshalled in this method are a subset of the actual\n// columns available; at the moment, we only handled the ones we need in the\n// CLI.\nfunc (s *WireQueryResponse) ShowColumnsResponse() (*ShowColumnsResponse, error) {\n\t// Make a map of header names to index position. We do this to avoid\n\t// breaking things if for some reason the format of the SHOW COLUMNS\n\t// response defined in the sql3 package changes.\n\tm := make(map[string]int)\n\tfor i, fld := range s.Schema.Fields {\n\t\tm[string(fld.Name)] = i\n\t}\n\n\tflds := make([]*dax.Field, 0, len(s.Data))\n\tfor _, row := range s.Data {\n\t\t// name\n\t\tvar name dax.FieldName\n\t\tif val, ok := row[m[\"name\"]].(string); ok {\n\t\t\tname = dax.FieldName(val)\n\t\t}\n\n\t\t// type\n\t\tvar typ dax.BaseType\n\t\tif val, ok := row[m[\"type\"]].(string); ok {\n\t\t\ttyp = dax.BaseType(val)\n\t\t}\n\n\t\t// scale\n\t\tvar scale int64\n\t\tif val, ok := row[m[\"scale\"]].(int64); ok {\n\t\t\tscale = val\n\t\t}\n\n\t\tflds = append(flds, &dax.Field{\n\t\t\tName: name,\n\t\t\tType: typ,\n\t\t\tOptions: dax.FieldOptions{\n\t\t\t\tScale: scale,\n\t\t\t},\n\t\t})\n\t}\n\treturn &ShowColumnsResponse{\n\t\tFields: flds,\n\t}, nil\n}\n\n// ShowColumnsResponse is a type used to marshal the results of a `SHOW COLUMNS`\n// statement.\ntype ShowColumnsResponse struct {\n\tFields []*dax.Field\n}\n\n// Field returns the field by name. If the field is not found, nil is returned.\nfunc (s *ShowColumnsResponse) Field(name dax.FieldName) *dax.Field {\n\tfor i := range s.Fields {\n\t\tif s.Fields[i].Name == name {\n\t\t\treturn s.Fields[i]\n\t\t}\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "wireprotocol",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}