{
  "metadata": {
    "timestamp": 1736566898605,
    "page": 468,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "charmbracelet/mods",
      "stars": 3189,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0400390625,
          "content": "mods\n.envrc\ncompletions/\nmanpages/\ndist/\n"
        },
        {
          "name": ".golangci-soft.yml",
          "type": "blob",
          "size": 0.568359375,
          "content": "run:\n  tests: false\n\nissues:\n  include:\n    - EXC0001\n    - EXC0005\n    - EXC0011\n    - EXC0012\n    - EXC0013\n\n  max-issues-per-linter: 0\n  max-same-issues: 0\n\nlinters:\n  enable:\n    - exhaustive\n    - goconst\n    - godot\n    - godox\n    - mnd\n    - gomoddirectives\n    - goprintffuncname\n    - misspell\n    - nakedret\n    - nestif\n    - noctx\n    - nolintlint\n    - prealloc\n    - wrapcheck\n\n  # disable default linters, they are already enabled in .golangci.yml\n  disable:\n    - errcheck\n    - gosimple\n    - govet\n    - ineffassign\n    - staticcheck\n    - typecheck\n    - unused\n"
        },
        {
          "name": ".golangci.yml",
          "type": "blob",
          "size": 0.3603515625,
          "content": "run:\n  tests: false\n\nissues:\n  include:\n    - EXC0001\n    - EXC0005\n    - EXC0011\n    - EXC0012\n    - EXC0013\n\n  max-issues-per-linter: 0\n  max-same-issues: 0\n\nlinters:\n  enable:\n    - bodyclose\n    - goimports\n    - gosec\n    - nilerr\n    - predeclared\n    - revive\n    - rowserrcheck\n    - sqlclosecheck\n    - tparallel\n    - unconvert\n    - unparam\n    - whitespace\n"
        },
        {
          "name": ".goreleaser.yml",
          "type": "blob",
          "size": 0.322265625,
          "content": "# The lines beneath this are called `modelines`. See `:help modeline`\n# Feel free to remove those if you don't want/use them.\n# yaml-language-server: $schema=https://goreleaser.com/static/schema-pro.json\n# vim: set ts=2 sw=2 tw=0 fo=cnqoj\nversion: 2\nincludes:\n  - from_url:\n      url: charmbracelet/meta/main/goreleaser-mods.yaml\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0498046875,
          "content": "MIT License\n\nCopyright (c) 2023 Charmbracelet, Inc\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.4853515625,
          "content": "# Mods\n\n<p>\n    <img src=\"https://github.com/charmbracelet/mods/assets/25087/5442bf46-b908-47af-bf4e-60f7c38951c4\" width=\"630\" alt=\"Mods product art and type treatment\"/>\n    <br>\n    <a href=\"https://github.com/charmbracelet/mods/releases\"><img src=\"https://img.shields.io/github/release/charmbracelet/mods.svg\" alt=\"Latest Release\"></a>\n    <a href=\"https://github.com/charmbracelet/mods/actions\"><img src=\"https://github.com/charmbracelet/mods/workflows/build/badge.svg\" alt=\"Build Status\"></a>\n</p>\n\nAI for the command line, built for pipelines.\n\n<p><img src=\"https://vhs.charm.sh/vhs-5Uyj0U6Hlqi1LVIIRyYKM5.gif\" width=\"900\" alt=\"a GIF of mods running\"></p>\n\nLarge Language Models (LLM) based AI is useful to ingest command output and\nformat results in Markdown, JSON, and other text based formats. Mods is a\ntool to add a sprinkle of AI in your command line and make your pipelines\nartificially intelligent.\n\nIt works great with LLMs running locally through [LocalAI]. You can also use\n[OpenAI], [Cohere], [Groq], or [Azure OpenAI].\n\n[LocalAI]: https://github.com/go-skynet/LocalAI\n[OpenAI]: https://platform.openai.com/account/api-keys\n[Cohere]: https://dashboard.cohere.com/api-keys\n[Groq]: https://console.groq.com/keys\n[Azure OpenAI]: https://azure.microsoft.com/en-us/products/cognitive-services/openai-service\n\n### Installation\n\nUse a package manager:\n\n```bash\n# macOS or Linux\nbrew install charmbracelet/tap/mods\n\n# Windows (with Winget)\nwinget install charmbracelet.mods\n\n# Arch Linux (btw)\nyay -S mods\n\n# Nix\nnix-shell -p mods\n```\n\n<details>\n<summary>Debian/Ubuntu</summary>\n\n```bash\nsudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://repo.charm.sh/apt/gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/charm.gpg\necho \"deb [signed-by=/etc/apt/keyrings/charm.gpg] https://repo.charm.sh/apt/ * *\" | sudo tee /etc/apt/sources.list.d/charm.list\nsudo apt update && sudo apt install mods\n```\n\n</details>\n\n<details>\n<summary>Fedora/RHEL</summary>\n\n```bash\necho '[charm]\nname=Charm\nbaseurl=https://repo.charm.sh/yum/\nenabled=1\ngpgcheck=1\ngpgkey=https://repo.charm.sh/yum/gpg.key' | sudo tee /etc/yum.repos.d/charm.repo\nsudo yum install mods\n```\n\n</details>\n\nOr, download it:\n\n- [Packages][releases] are available in Debian and RPM formats\n- [Binaries][releases] are available for Linux, macOS, and Windows\n\n[releases]: https://github.com/charmbracelet/mods/releases\n\nOr, just install it with `go`:\n\n```sh\ngo install github.com/charmbracelet/mods@latest\n```\n\n<details>\n<summary>Shell Completions</summary>\n\nAll the packages and archives come with pre-generated completion files for Bash,\nZSH, Fish, and PowerShell.\n\nIf you built it from source, you can generate them with:\n\n```bash\nmods completion bash -h\nmods completion zsh -h\nmods completion fish -h\nmods completion powershell -h\n```\n\nIf you use a package (like Homebrew, Debs, etc), the completions should be set\nup automatically, given your shell is configured properly.\n\n</details>\n\n## What Can It Do?\n\nMods works by reading standard in and prefacing it with a prompt supplied in\nthe `mods` arguments. It sends the input text to an LLM and prints out the\nresult, optionally asking the LLM to format the response as Markdown. This\ngives you a way to \"question\" the output of a command. Mods will also work on\nstandard in or an argument supplied prompt individually.\n\nBe sure to check out the [examples](examples.md) and a list of all the\n[features](features.md).\n\nMods works with OpenAI compatible endpoints. By default, Mods is configured to\nsupport OpenAI's official API and a LocalAI installation running on port 8080.\nYou can configure additional endpoints in your settings file by running\n`mods --settings`.\n\n## Saved Conversations\n\nConversations are saved locally by default. Each conversation has a SHA-1\nidentifier and a title (like `git`!).\n\n<p>\n  <img src=\"https://vhs.charm.sh/vhs-6MMscpZwgzohYYMfTrHErF.gif\" width=\"900\" alt=\"a GIF listing and showing saved conversations.\">\n</p>\n\nCheck the [`./features.md`](./features.md) for more details.\n\n## Usage\n\n- `-m`, `--model`: Specify Large Language Model to use.\n- `-f`, `--format`: Ask the LLM to format the response in a given format.\n- `--format-as`: Specify the format for the output (used with `--format`).\n- `-P`, `--prompt`: Prompt should include stdin and args.\n- `-p`, `--prompt-args`: Prompt should only include args.\n- `-q`, `--quiet`: Only output errors to standard err.\n- `-r`, `--raw`: Print raw response without syntax highlighting.\n- `--settings`: Open settings.\n- `-x`, `--http-proxy`: Use HTTP proxy to connect to the API endpoints.\n- `--max-retries`: Maximum number of retries.\n- `--max-tokens`: Specify maximum tokens with which to respond.\n- `--no-limit`: Do not limit the response tokens.\n- `--role`: Specify the role to use (See [custom roles](#custom-roles)).\n- `--word-wrap`: Wrap output at width (defaults to 80)\n- `--reset-settings`: Restore settings to default.\n\n#### Conversations\n\n- `-t`, `--title`: Set the title for the conversation.\n- `-l`, `--list`: List saved conversations.\n- `-c`, `--continue`: Continue from last response or specific title or SHA-1.\n- `-C`, `--continue-last`: Continue the last conversation.\n- `-s`, `--show`: Show saved conversation for the given title or SHA-1.\n- `-S`, `--show-last`: Show previous conversation.\n- `--delete-older-than=<duration>`: Deletes conversations older than given duration (`10d`, `1mo`).\n- `--delete`: Deletes the saved conversation for the given title or SHA-1.\n- `--no-cache`: Do not save conversations.\n\n#### Advanced\n\n- `--fanciness`: Level of fanciness.\n- `--temp`: Sampling temperature.\n- `--topp`: Top P value.\n- `--topk`: Top K value.\n\n## Custom Roles\n\nRoles allow you to set system prompts. Here is an example of a `shell` role:\n\n```yaml\nroles:\n  shell:\n    - you are a shell expert\n    - you do not explain anything\n    - you simply output one liners to solve the problems you're asked\n    - you do not provide any explanation whatsoever, ONLY the command\n```\n\nThen, use the custom role in `mods`:\n\n```sh\nmods --role shell list files in the current directory\n```\n\n## Setup\n\n### Open AI\n\nMods uses GPT-4 by default. It will fall back to GPT-3.5 Turbo.\n\nSet the `OPENAI_API_KEY` environment variable. If you don't have one yet, you\ncan grab it the [OpenAI website](https://platform.openai.com/account/api-keys).\n\nAlternatively, set the [`AZURE_OPENAI_KEY`] environment variable to use Azure\nOpenAI. Grab a key from [Azure](https://azure.microsoft.com/en-us/products/cognitive-services/openai-service).\n\n### Cohere\n\nCohere provides enterprise optimized models.\n\nSet the `COHERE_API_KEY` environment variable. If you don't have one yet, you can\nget it from the [Cohere dashboard](https://dashboard.cohere.com/api-keys).\n\n### Local AI\n\nLocal AI allows you to run models locally. Mods works with the GPT4ALL-J model\nas setup in [this tutorial](https://github.com/go-skynet/LocalAI#example-use-gpt4all-j-model).\n\n### Groq\n\nGroq provides models powered by their LPU inference engine.\n\nSet the `GROQ_API_KEY` environment variable. If you don't have one yet, you can\nget it from the [Groq console](https://console.groq.com/keys).\n\n## Whatcha Think?\n\nWe’d love to hear your thoughts on this project. Feel free to drop us a note.\n\n- [Twitter](https://twitter.com/charmcli)\n- [The Fediverse](https://mastodon.social/@charmcli)\n- [Discord](https://charm.sh/chat)\n\n## License\n\n[MIT](https://github.com/charmbracelet/mods/raw/main/LICENSE)\n\n---\n\nPart of [Charm](https://charm.sh).\n\n<a href=\"https://charm.sh/\"><img alt=\"The Charm logo\" width=\"400\" src=\"https://stuff.charm.sh/charm-badge.jpg\" /></a>\n\n<!--prettier-ignore-->\nCharm热爱开源 • Charm loves open source\n"
        },
        {
          "name": "anim.go",
          "type": "blob",
          "size": 5.982421875,
          "content": "package main\n\nimport (\n\t\"math/rand\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/charmbracelet/bubbles/spinner\"\n\ttea \"github.com/charmbracelet/bubbletea\"\n\t\"github.com/charmbracelet/lipgloss\"\n\t\"github.com/lucasb-eyer/go-colorful\"\n\t\"github.com/muesli/termenv\"\n)\n\nconst (\n\tcharCyclingFPS  = time.Second / 22\n\tcolorCycleFPS   = time.Second / 5\n\tmaxCyclingChars = 120\n)\n\nvar charRunes = []rune(\"0123456789abcdefABCDEF~!@#$£€%^&*()+=_\")\n\ntype charState int\n\nconst (\n\tcharInitialState charState = iota\n\tcharCyclingState\n\tcharEndOfLifeState\n)\n\n// cyclingChar is a single animated character.\ntype cyclingChar struct {\n\tfinalValue   rune // if < 0 cycle forever\n\tcurrentValue rune\n\tinitialDelay time.Duration\n\tlifetime     time.Duration\n}\n\nfunc (c cyclingChar) randomRune() rune {\n\treturn (charRunes)[rand.Intn(len(charRunes))] //nolint:gosec\n}\n\nfunc (c cyclingChar) state(start time.Time) charState {\n\tnow := time.Now()\n\tif now.Before(start.Add(c.initialDelay)) {\n\t\treturn charInitialState\n\t}\n\tif c.finalValue > 0 && now.After(start.Add(c.initialDelay)) {\n\t\treturn charEndOfLifeState\n\t}\n\treturn charCyclingState\n}\n\ntype stepCharsMsg struct{}\n\nfunc stepChars() tea.Cmd {\n\treturn tea.Tick(charCyclingFPS, func(time.Time) tea.Msg {\n\t\treturn stepCharsMsg{}\n\t})\n}\n\ntype colorCycleMsg struct{}\n\nfunc cycleColors() tea.Cmd {\n\treturn tea.Tick(colorCycleFPS, func(time.Time) tea.Msg {\n\t\treturn colorCycleMsg{}\n\t})\n}\n\n// anim is the model that manages the animation that displays while the\n// output is being generated.\ntype anim struct {\n\tstart           time.Time\n\tcyclingChars    []cyclingChar\n\tlabelChars      []cyclingChar\n\tramp            []lipgloss.Style\n\tlabel           []rune\n\tellipsis        spinner.Model\n\tellipsisStarted bool\n\tstyles          styles\n}\n\nfunc newAnim(cyclingCharsSize uint, label string, r *lipgloss.Renderer, s styles) anim {\n\t// #nosec G115\n\tn := int(cyclingCharsSize)\n\tif n > maxCyclingChars {\n\t\tn = maxCyclingChars\n\t}\n\n\tgap := \" \"\n\tif n == 0 {\n\t\tgap = \"\"\n\t}\n\n\tc := anim{\n\t\tstart:    time.Now(),\n\t\tlabel:    []rune(gap + label),\n\t\tellipsis: spinner.New(spinner.WithSpinner(spinner.Ellipsis)),\n\t\tstyles:   s,\n\t}\n\n\t// If we're in truecolor mode (and there are enough cycling characters)\n\t// color the cycling characters with a gradient ramp.\n\tconst minRampSize = 3\n\tif n >= minRampSize && r.ColorProfile() == termenv.TrueColor {\n\t\t// Note: double capacity for color cycling as we'll need to reverse and\n\t\t// append the ramp for seamless transitions.\n\t\tc.ramp = make([]lipgloss.Style, n, n*2) //nolint:mnd\n\t\tramp := makeGradientRamp(n)\n\t\tfor i, color := range ramp {\n\t\t\tc.ramp[i] = r.NewStyle().Foreground(color)\n\t\t}\n\t\tc.ramp = append(c.ramp, reverse(c.ramp)...) // reverse and append for color cycling\n\t}\n\n\tmakeDelay := func(a int32, b time.Duration) time.Duration {\n\t\treturn time.Duration(rand.Int31n(a)) * (time.Millisecond * b) //nolint:gosec\n\t}\n\n\tmakeInitialDelay := func() time.Duration {\n\t\treturn makeDelay(8, 60) //nolint:mnd\n\t}\n\n\t// Initial characters that cycle forever.\n\tc.cyclingChars = make([]cyclingChar, n)\n\n\tfor i := 0; i < n; i++ {\n\t\tc.cyclingChars[i] = cyclingChar{\n\t\t\tfinalValue:   -1, // cycle forever\n\t\t\tinitialDelay: makeInitialDelay(),\n\t\t}\n\t}\n\n\t// Label text that only cycles for a little while.\n\tc.labelChars = make([]cyclingChar, len(c.label))\n\n\tfor i, r := range c.label {\n\t\tc.labelChars[i] = cyclingChar{\n\t\t\tfinalValue:   r,\n\t\t\tinitialDelay: makeInitialDelay(),\n\t\t\tlifetime:     makeDelay(5, 180), //nolint:mnd\n\t\t}\n\t}\n\n\treturn c\n}\n\n// Init initializes the animation.\nfunc (anim) Init() tea.Cmd {\n\treturn tea.Batch(stepChars(), cycleColors())\n}\n\n// Update handles messages.\nfunc (a anim) Update(msg tea.Msg) (tea.Model, tea.Cmd) {\n\tvar cmd tea.Cmd\n\tswitch msg.(type) {\n\tcase stepCharsMsg:\n\t\ta.updateChars(&a.cyclingChars)\n\t\ta.updateChars(&a.labelChars)\n\n\t\tif !a.ellipsisStarted {\n\t\t\tvar eol int\n\t\t\tfor _, c := range a.labelChars {\n\t\t\t\tif c.state(a.start) == charEndOfLifeState {\n\t\t\t\t\teol++\n\t\t\t\t}\n\t\t\t}\n\t\t\tif eol == len(a.label) {\n\t\t\t\t// If our entire label has reached end of life, start the\n\t\t\t\t// ellipsis \"spinner\" after a short pause.\n\t\t\t\ta.ellipsisStarted = true\n\t\t\t\tcmd = tea.Tick(time.Millisecond*220, func(time.Time) tea.Msg { //nolint:mnd\n\t\t\t\t\treturn a.ellipsis.Tick()\n\t\t\t\t})\n\t\t\t}\n\t\t}\n\n\t\treturn a, tea.Batch(stepChars(), cmd)\n\tcase colorCycleMsg:\n\t\tconst minColorCycleSize = 2\n\t\tif len(a.ramp) < minColorCycleSize {\n\t\t\treturn a, nil\n\t\t}\n\t\ta.ramp = append(a.ramp[1:], a.ramp[0])\n\t\treturn a, cycleColors()\n\tcase spinner.TickMsg:\n\t\tvar cmd tea.Cmd\n\t\ta.ellipsis, cmd = a.ellipsis.Update(msg)\n\t\treturn a, cmd\n\tdefault:\n\t\treturn a, nil\n\t}\n}\n\nfunc (a *anim) updateChars(chars *[]cyclingChar) {\n\tfor i, c := range *chars {\n\t\tswitch c.state(a.start) {\n\t\tcase charInitialState:\n\t\t\t(*chars)[i].currentValue = '.'\n\t\tcase charCyclingState:\n\t\t\t(*chars)[i].currentValue = c.randomRune()\n\t\tcase charEndOfLifeState:\n\t\t\t(*chars)[i].currentValue = c.finalValue\n\t\t}\n\t}\n}\n\n// View renders the animation.\nfunc (a anim) View() string {\n\tvar b strings.Builder\n\n\tfor i, c := range a.cyclingChars {\n\t\tif len(a.ramp) > i {\n\t\t\tb.WriteString(a.ramp[i].Render(string(c.currentValue)))\n\t\t\tcontinue\n\t\t}\n\t\tb.WriteRune(c.currentValue)\n\t}\n\n\tfor _, c := range a.labelChars {\n\t\tb.WriteRune(c.currentValue)\n\t}\n\n\treturn b.String() + a.ellipsis.View()\n}\n\nfunc makeGradientRamp(length int) []lipgloss.Color {\n\tconst startColor = \"#F967DC\"\n\tconst endColor = \"#6B50FF\"\n\tvar (\n\t\tc        = make([]lipgloss.Color, length)\n\t\tstart, _ = colorful.Hex(startColor)\n\t\tend, _   = colorful.Hex(endColor)\n\t)\n\tfor i := 0; i < length; i++ {\n\t\tstep := start.BlendLuv(end, float64(i)/float64(length))\n\t\tc[i] = lipgloss.Color(step.Hex())\n\t}\n\treturn c\n}\n\nfunc makeGradientText(baseStyle lipgloss.Style, str string) string {\n\tconst minSize = 3\n\tif len(str) < minSize {\n\t\treturn str\n\t}\n\tb := strings.Builder{}\n\trunes := []rune(str)\n\tfor i, c := range makeGradientRamp(len(str)) {\n\t\tb.WriteString(baseStyle.Foreground(c).Render(string(runes[i])))\n\t}\n\treturn b.String()\n}\n\nfunc reverse[T any](in []T) []T {\n\tout := make([]T, len(in))\n\tcopy(out, in[:])\n\tfor i, j := 0, len(out)-1; i < j; i, j = i+1, j-1 {\n\t\tout[i], out[j] = out[j], out[i]\n\t}\n\treturn out\n}\n"
        },
        {
          "name": "anthropic.go",
          "type": "blob",
          "size": 10.1044921875,
          "content": "package main\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\n\topenai \"github.com/sashabaranov/go-openai\"\n)\n\ntype (\n\t// AnthropicAPIVersion represents the version of the Anthropic API.\n\tAnthropicAPIVersion string\n\t// AnthropicAPIBeta represents the beta version of the Anthropic API.\n\tAnthropicAPIBeta string\n)\n\nconst (\n\t// AnthropicV20230601 represents the version of the Anthropic API.\n\tAnthropicV20230601 AnthropicAPIVersion = \"2023-06-01\"\n\t// AnthropicBeta represents the beta version of the Anthropic API.\n\tAnthropicBeta AnthropicAPIBeta = \"messages-2023-12-15\"\n)\n\nvar (\n\theaderData  = []byte(\"data: \")\n\terrorPrefix = []byte(`event: error`)\n)\n\n// AnthropicClientConfig represents the configuration for the Anthropic API client.\ntype AnthropicClientConfig struct {\n\tAuthToken          string\n\tBaseURL            string\n\tHTTPClient         *http.Client\n\tVersion            AnthropicAPIVersion\n\tBeta               AnthropicAPIBeta\n\tEmptyMessagesLimit uint\n}\n\n// DefaultAnthropicConfig returns the default configuration for the Anthropic API client.\nfunc DefaultAnthropicConfig(authToken string) AnthropicClientConfig {\n\treturn AnthropicClientConfig{\n\t\tAuthToken:          authToken,\n\t\tBaseURL:            \"https://api.anthropic.com/v1\",\n\t\tVersion:            AnthropicV20230601,\n\t\tBeta:               AnthropicBeta,\n\t\tHTTPClient:         &http.Client{},\n\t\tEmptyMessagesLimit: defaultEmptyMessagesLimit,\n\t}\n}\n\n// AnthropicMessageCompletionRequest represents the request body for the chat completion API.\ntype AnthropicMessageCompletionRequest struct {\n\tModel         string                         `json:\"model\"`\n\tSystem        string                         `json:\"system\"`\n\tMessages      []openai.ChatCompletionMessage `json:\"messages\"`\n\tMaxTokens     int                            `json:\"max_tokens\"`\n\tTemperature   float32                        `json:\"temperature,omitempty\"`\n\tTopP          float32                        `json:\"top_p,omitempty\"`\n\tTopK          int                            `json:\"top_k,omitempty\"`\n\tStream        bool                           `json:\"stream,omitempty\"`\n\tStopSequences []string                       `json:\"stop_sequences,omitempty\"`\n}\n\n// AnthropicRequestBuilder is an interface for building HTTP requests for the Anthropic API.\ntype AnthropicRequestBuilder interface {\n\tBuild(ctx context.Context, method, url string, body any, header http.Header) (*http.Request, error)\n}\n\n// NewAnthropicRequestBuilder creates a new HTTPRequestBuilder.\nfunc NewAnthropicRequestBuilder() *HTTPRequestBuilder {\n\treturn &HTTPRequestBuilder{\n\t\tmarshaller: &JSONMarshaller{},\n\t}\n}\n\n// AnthropicClient is a client for the Anthropic API.\ntype AnthropicClient struct {\n\tconfig AnthropicClientConfig\n\n\trequestBuilder AnthropicRequestBuilder\n}\n\n// NewAnthropicClient creates a new AnthropicClient with the given configuration.\nfunc NewAnthropicClientWithConfig(config AnthropicClientConfig) *AnthropicClient {\n\treturn &AnthropicClient{\n\t\tconfig:         config,\n\t\trequestBuilder: NewAnthropicRequestBuilder(),\n\t}\n}\n\nconst anthropicChatCompletionsSuffix = \"/messages\"\n\nfunc (c *AnthropicClient) setCommonHeaders(req *http.Request) {\n\treq.Header.Set(\"anthropic-version\", string(c.config.Version))\n\treq.Header.Set(\"x-api-key\", c.config.AuthToken)\n}\n\nfunc (c *AnthropicClient) newRequest(ctx context.Context, method, url string, setters ...requestOption) (*http.Request, error) {\n\t// Default Options\n\targs := &requestOptions{\n\t\tbody:   nil,\n\t\theader: make(http.Header),\n\t}\n\tfor _, setter := range setters {\n\t\tsetter(args)\n\t}\n\treq, err := c.requestBuilder.Build(ctx, method, url, args.body, args.header)\n\tif err != nil {\n\t\treturn new(http.Request), err\n\t}\n\tc.setCommonHeaders(req)\n\treturn req, nil\n}\n\nfunc (c *AnthropicClient) handleErrorResp(resp *http.Response) error {\n\t// Print the response text\n\tvar errRes openai.ErrorResponse\n\terr := json.NewDecoder(resp.Body).Decode(&errRes)\n\tif err != nil || errRes.Error == nil {\n\t\treqErr := &openai.RequestError{\n\t\t\tHTTPStatusCode: resp.StatusCode,\n\t\t\tErr:            err,\n\t\t}\n\t\tif errRes.Error != nil {\n\t\t\treqErr.Err = errRes.Error\n\t\t}\n\t\treturn reqErr\n\t}\n\n\terrRes.Error.HTTPStatusCode = resp.StatusCode\n\treturn errRes.Error\n}\n\n// AnthropicMessageUsage represents the usage of an Anthropic message.\ntype AnthropicMessageUsage struct {\n\tInputTokens  int `json:\"input_tokens,omitempty\"`\n\tOutputTokens int `json:\"output_tokens,omitempty\"`\n}\n\n// AnthropicMessage represents an Anthropic message.\ntype AnthropicMessage struct {\n\tUsage        *AnthropicMessageUsage `json:\"usage,omitempty\"`\n\tStopReason   *string                `json:\"stop_reason,omitempty\"`\n\tStopSequence *string                `json:\"stop_sequence,omitempty\"`\n\tID           string                 `json:\"id,omitempty\"`\n\tType         string                 `json:\"type\"`\n\tRole         string                 `json:\"role,omitempty\"`\n\tModel        string                 `json:\"model,omitempty\"`\n\tContent      []string               `json:\"content,omitempty\"`\n}\n\n// AnthropicMessageContentBlock represents a content block in an Anthropic message.\ntype AnthropicMessageContentBlock struct {\n\tType string `json:\"type\"`\n\tText string `json:\"text,omitempty\"`\n}\n\n// AnthropicMessageTextDelta represents a text delta in an Anthropic message.\ntype AnthropicMessageTextDelta struct {\n\tType string `json:\"type\"`\n\tText string `json:\"text,omitempty\"`\n}\n\n// AnthropicCompletionMessageResponse represents a response to an Anthropic completion message.\ntype AnthropicCompletionMessageResponse struct {\n\tType         string                        `json:\"type\"`\n\tMessage      *AnthropicMessage             `json:\"message,omitempty\"`\n\tIndex        int                           `json:\"index,omitempty\"`\n\tContentBlock *AnthropicMessageContentBlock `json:\"content_block,omitempty\"`\n\tDelta        *AnthropicMessageTextDelta    `json:\"delta,omitempty\"`\n}\n\n// AnthropicChatCompletionStream represents a stream for chat completion.\ntype AnthropicChatCompletionStream struct {\n\t*anthropicStreamReader\n}\n\ntype anthropicStreamReader struct {\n\temptyMessagesLimit uint\n\tisFinished         bool\n\n\treader         *bufio.Reader\n\tresponse       *http.Response\n\terrAccumulator ErrorAccumulator\n\tunmarshaler    Unmarshaler\n\n\thttpHeader\n}\n\n// Recv reads the next response from the stream.\nfunc (stream *anthropicStreamReader) Recv() (response openai.ChatCompletionStreamResponse, err error) {\n\tif stream.isFinished {\n\t\terr = io.EOF\n\t\treturn\n\t}\n\n\tresponse, err = stream.processLines()\n\treturn\n}\n\n// Close closes the stream.\nfunc (stream *anthropicStreamReader) Close() error {\n\treturn stream.response.Body.Close() //nolint:wrapcheck\n}\n\n//nolint:gocognit\nfunc (stream *anthropicStreamReader) processLines() (openai.ChatCompletionStreamResponse, error) {\n\tvar (\n\t\temptyMessagesCount uint\n\t\thasError           bool\n\t)\n\n\tfor {\n\t\trawLine, readErr := stream.reader.ReadBytes('\\n')\n\n\t\tif readErr != nil {\n\t\t\treturn *new(openai.ChatCompletionStreamResponse), fmt.Errorf(\"anthropicStreamReader.processLines: %w\", readErr)\n\t\t}\n\n\t\tnoSpaceLine := bytes.TrimSpace(rawLine)\n\n\t\tif bytes.HasPrefix(noSpaceLine, errorPrefix) {\n\t\t\thasError = true\n\t\t\t// NOTE: Continue to the next event to get the error data.\n\t\t\tcontinue\n\t\t}\n\n\t\tif !bytes.HasPrefix(noSpaceLine, headerData) || hasError {\n\t\t\tif hasError {\n\t\t\t\tnoSpaceLine = bytes.TrimPrefix(noSpaceLine, headerData)\n\t\t\t}\n\t\t\twriteErr := stream.errAccumulator.Write(noSpaceLine)\n\t\t\tif writeErr != nil {\n\t\t\t\treturn *new(openai.ChatCompletionStreamResponse), fmt.Errorf(\"ollamaStreamReader.processLines: %w\", writeErr)\n\t\t\t}\n\t\t\temptyMessagesCount++\n\t\t\tif emptyMessagesCount > stream.emptyMessagesLimit {\n\t\t\t\treturn *new(openai.ChatCompletionStreamResponse), ErrTooManyEmptyStreamMessages\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\tnoPrefixLine := bytes.TrimPrefix(noSpaceLine, headerData)\n\t\tif string(noPrefixLine) == \"event: message_stop\" {\n\t\t\tstream.isFinished = true\n\t\t\treturn *new(openai.ChatCompletionStreamResponse), io.EOF\n\t\t}\n\n\t\tvar chunk AnthropicCompletionMessageResponse\n\t\tunmarshalErr := stream.unmarshaler.Unmarshal(noPrefixLine, &chunk)\n\t\tif unmarshalErr != nil {\n\t\t\treturn *new(openai.ChatCompletionStreamResponse), fmt.Errorf(\"anthropicStreamReader.processLines: %w\", unmarshalErr)\n\t\t}\n\n\t\tif chunk.Type != \"content_block_delta\" {\n\t\t\tcontinue\n\t\t}\n\n\t\t// NOTE: Leverage the existing logic based on OpenAI ChatCompletionStreamResponse by\n\t\t//       converting the Anthropic events into them.\n\t\tresponse := openai.ChatCompletionStreamResponse{\n\t\t\tChoices: []openai.ChatCompletionStreamChoice{\n\t\t\t\t{\n\t\t\t\t\tIndex: 0,\n\t\t\t\t\tDelta: openai.ChatCompletionStreamChoiceDelta{\n\t\t\t\t\t\tContent: chunk.Delta.Text,\n\t\t\t\t\t\tRole:    \"assistant\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\n\t\treturn response, nil\n\t}\n}\n\nfunc anthropicSendRequestStream(client *AnthropicClient, req *http.Request) (*anthropicStreamReader, error) {\n\treq.Header.Set(\"content-type\", \"application/json\")\n\treq.Header.Set(\"anthropic-beta\", string(client.config.Beta))\n\n\tresp, err := client.config.HTTPClient.Do(req) //nolint:bodyclose // body is closed in stream.Close()\n\tif err != nil {\n\t\treturn new(anthropicStreamReader), err\n\t}\n\tif isFailureStatusCode(resp) {\n\t\treturn new(anthropicStreamReader), client.handleErrorResp(resp)\n\t}\n\treturn &anthropicStreamReader{\n\t\temptyMessagesLimit: client.config.EmptyMessagesLimit,\n\t\treader:             bufio.NewReader(resp.Body),\n\t\tresponse:           resp,\n\t\terrAccumulator:     NewErrorAccumulator(),\n\t\tunmarshaler:        &JSONUnmarshaler{},\n\t\thttpHeader:         httpHeader(resp.Header),\n\t}, nil\n}\n\n// CreateChatCompletionStream — API call to create a chat completion w/ streaming\n// support. It sets whether to stream back partial progress. If set, tokens will be\n// sent as data-only server-sent events as they become available, with the\n// stream terminated by a data: [DONE] message.\nfunc (c *AnthropicClient) CreateChatCompletionStream(\n\tctx context.Context,\n\trequest AnthropicMessageCompletionRequest,\n) (stream *AnthropicChatCompletionStream, err error) {\n\turlSuffix := anthropicChatCompletionsSuffix\n\n\trequest.Stream = true\n\treq, err := c.newRequest(ctx, http.MethodPost, c.config.BaseURL+urlSuffix, withBody(request))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tresp, err := anthropicSendRequestStream(c, req)\n\tif err != nil {\n\t\treturn\n\t}\n\tstream = &AnthropicChatCompletionStream{\n\t\tanthropicStreamReader: resp,\n\t}\n\treturn\n}\n"
        },
        {
          "name": "cache.go",
          "type": "blob",
          "size": 2.40234375,
          "content": "package main\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sync\"\n\n\topenai \"github.com/sashabaranov/go-openai\"\n)\n\nconst cacheExt = \".gob\"\n\nvar errInvalidID = errors.New(\"invalid id\")\n\ntype convoCache struct {\n\tdir string\n}\n\nfunc newCache(dir string) *convoCache {\n\treturn &convoCache{dir}\n}\n\nfunc (c *convoCache) read(id string, messages *[]openai.ChatCompletionMessage) error {\n\tif id == \"\" {\n\t\treturn fmt.Errorf(\"read: %w\", errInvalidID)\n\t}\n\tfile, err := os.Open(filepath.Join(c.dir, id+cacheExt))\n\tif err != nil {\n\t\treturn fmt.Errorf(\"read: %w\", err)\n\t}\n\tdefer file.Close() //nolint:errcheck\n\n\tif err := decode(file, messages); err != nil {\n\t\treturn fmt.Errorf(\"read: %w\", err)\n\t}\n\treturn nil\n}\n\nfunc (c *convoCache) write(id string, messages *[]openai.ChatCompletionMessage) error {\n\tif id == \"\" {\n\t\treturn fmt.Errorf(\"write: %w\", errInvalidID)\n\t}\n\n\tfile, err := os.Create(filepath.Join(c.dir, id+cacheExt))\n\tif err != nil {\n\t\treturn fmt.Errorf(\"write: %w\", err)\n\t}\n\tdefer file.Close() //nolint:errcheck\n\n\tif err := encode(file, messages); err != nil {\n\t\treturn fmt.Errorf(\"write: %w\", err)\n\t}\n\n\treturn nil\n}\n\nfunc (c *convoCache) delete(id string) error {\n\tif id == \"\" {\n\t\treturn fmt.Errorf(\"delete: %w\", errInvalidID)\n\t}\n\tif err := os.Remove(filepath.Join(c.dir, id+cacheExt)); err != nil {\n\t\treturn fmt.Errorf(\"delete: %w\", err)\n\t}\n\treturn nil\n}\n\nvar _ chatCompletionReceiver = &cachedCompletionStream{}\n\ntype cachedCompletionStream struct {\n\tmessages []openai.ChatCompletionMessage\n\tread     int\n\tm        sync.Mutex\n}\n\nfunc (c *cachedCompletionStream) Close() error { return nil }\nfunc (c *cachedCompletionStream) Recv() (openai.ChatCompletionStreamResponse, error) {\n\tc.m.Lock()\n\tdefer c.m.Unlock()\n\n\tif c.read == len(c.messages) {\n\t\treturn openai.ChatCompletionStreamResponse{}, io.EOF\n\t}\n\n\tmsg := c.messages[c.read]\n\tprefix := \"\"\n\n\tswitch msg.Role {\n\tcase openai.ChatMessageRoleSystem:\n\t\tprefix += \"\\n**System**: \"\n\tcase openai.ChatMessageRoleUser:\n\t\tprefix += \"\\n**Prompt**: \"\n\tcase openai.ChatMessageRoleAssistant:\n\t\tprefix += \"\\n**Assistant**: \"\n\tcase openai.ChatMessageRoleFunction:\n\t\tprefix += \"\\n**Function**: \"\n\tcase openai.ChatMessageRoleTool:\n\t\tprefix += \"\\n**Tool**: \"\n\t}\n\n\tc.read++\n\treturn openai.ChatCompletionStreamResponse{\n\t\tChoices: []openai.ChatCompletionStreamChoice{\n\t\t\t{\n\t\t\t\tDelta: openai.ChatCompletionStreamChoiceDelta{\n\t\t\t\t\tContent: prefix + msg.Content + \"\\n\",\n\t\t\t\t\tRole:    msg.Role,\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}, nil\n}\n"
        },
        {
          "name": "cache_test.go",
          "type": "blob",
          "size": 2.9443359375,
          "content": "package main\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"flag\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/sashabaranov/go-openai\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nvar update = flag.Bool(\"update\", false, \"update .golden files\")\n\nfunc TestCache(t *testing.T) {\n\tt.Run(\"read non-existent\", func(t *testing.T) {\n\t\tcache := newCache(t.TempDir())\n\t\terr := cache.read(\"super-fake\", &[]openai.ChatCompletionMessage{})\n\t\trequire.ErrorIs(t, err, os.ErrNotExist)\n\t})\n\n\tt.Run(\"write\", func(t *testing.T) {\n\t\tcache := newCache(t.TempDir())\n\t\tmessages := []openai.ChatCompletionMessage{\n\t\t\t{\n\t\t\t\tRole:    openai.ChatMessageRoleUser,\n\t\t\t\tContent: \"first 4 natural numbers\",\n\t\t\t},\n\t\t\t{\n\t\t\t\tRole:    openai.ChatMessageRoleAssistant,\n\t\t\t\tContent: \"1, 2, 3, 4\",\n\t\t\t},\n\t\t}\n\t\trequire.NoError(t, cache.write(\"fake\", &messages))\n\n\t\tresult := []openai.ChatCompletionMessage{}\n\t\trequire.NoError(t, cache.read(\"fake\", &result))\n\n\t\trequire.ElementsMatch(t, messages, result)\n\t})\n\n\tt.Run(\"delete\", func(t *testing.T) {\n\t\tcache := newCache(t.TempDir())\n\t\tcache.write(\"fake\", &[]openai.ChatCompletionMessage{})\n\t\trequire.NoError(t, cache.delete(\"fake\"))\n\t\trequire.ErrorIs(t, cache.read(\"fake\", nil), os.ErrNotExist)\n\t})\n\n\tt.Run(\"invalid id\", func(t *testing.T) {\n\t\tt.Run(\"write\", func(t *testing.T) {\n\t\t\tcache := newCache(t.TempDir())\n\t\t\trequire.ErrorIs(t, cache.write(\"\", nil), errInvalidID)\n\t\t})\n\t\tt.Run(\"delete\", func(t *testing.T) {\n\t\t\tcache := newCache(t.TempDir())\n\t\t\trequire.ErrorIs(t, cache.delete(\"\"), errInvalidID)\n\t\t})\n\t\tt.Run(\"read\", func(t *testing.T) {\n\t\t\tcache := newCache(t.TempDir())\n\t\t\trequire.ErrorIs(t, cache.read(\"\", nil), errInvalidID)\n\t\t})\n\t})\n}\n\nfunc TestCachedCompletionStream(t *testing.T) {\n\tstream := cachedCompletionStream{\n\t\tmessages: []openai.ChatCompletionMessage{\n\t\t\t{\n\t\t\t\tRole:    openai.ChatMessageRoleSystem,\n\t\t\t\tContent: \"you are a medieval king\",\n\t\t\t},\n\t\t\t{\n\t\t\t\tRole:    openai.ChatMessageRoleUser,\n\t\t\t\tContent: \"first 4 natural numbers\",\n\t\t\t},\n\t\t\t{\n\t\t\t\tRole:    openai.ChatMessageRoleAssistant,\n\t\t\t\tContent: \"1, 2, 3, 4\",\n\t\t\t},\n\n\t\t\t{\n\t\t\t\tRole:    openai.ChatMessageRoleUser,\n\t\t\t\tContent: \"as a json array\",\n\t\t\t},\n\t\t\t{\n\t\t\t\tRole:    openai.ChatMessageRoleAssistant,\n\t\t\t\tContent: \"[ 1, 2, 3, 4 ]\",\n\t\t\t},\n\t\t\t{\n\t\t\t\tRole:    openai.ChatMessageRoleAssistant,\n\t\t\t\tContent: \"something from an assistant\",\n\t\t\t},\n\t\t\t{\n\t\t\t\tRole:    openai.ChatMessageRoleFunction,\n\t\t\t\tContent: \"something from a function\",\n\t\t\t},\n\t\t},\n\t}\n\tt.Cleanup(func() { require.NoError(t, stream.Close()) })\n\n\tvar output []string\n\n\tfor {\n\t\tresp, err := stream.Recv()\n\t\tif errors.Is(err, io.EOF) {\n\t\t\tbreak\n\t\t}\n\t\trequire.NoError(t, err)\n\t\toutput = append(output, resp.Choices[0].Delta.Content)\n\t}\n\n\tgolden := filepath.Join(\"testdata\", t.Name()+\".md.golden\")\n\tcontent := strings.Join(output, \"\\n\")\n\tif *update {\n\t\trequire.NoError(t, os.WriteFile(golden, []byte(content), 0o644))\n\t}\n\n\tbts, err := os.ReadFile(golden)\n\trequire.NoError(t, err)\n\n\trequire.Equal(t, string(bytes.ReplaceAll(bts, []byte(\"\\r\\n\"), []byte(\"\\n\"))), content)\n}\n"
        },
        {
          "name": "cohere.go",
          "type": "blob",
          "size": 3.7431640625,
          "content": "package main\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\n\tcohere \"github.com/cohere-ai/cohere-go/v2\"\n\t\"github.com/cohere-ai/cohere-go/v2/client\"\n\t\"github.com/cohere-ai/cohere-go/v2/core\"\n\t\"github.com/cohere-ai/cohere-go/v2/option\"\n\topenai \"github.com/sashabaranov/go-openai\"\n)\n\n// CohereClientConfig represents the configuration for the Cohere API client.\ntype CohereClientConfig struct {\n\tAuthToken          string\n\tBaseURL            string\n\tHTTPClient         *http.Client\n\tEmptyMessagesLimit uint\n}\n\n// DefaultCohereConfig returns the default configuration for the Cohere API client.\nfunc DefaultCohereConfig(authToken string) CohereClientConfig {\n\treturn CohereClientConfig{\n\t\tAuthToken:  authToken,\n\t\tBaseURL:    \"\",\n\t\tHTTPClient: &http.Client{},\n\t}\n}\n\n// CohereClient is a client for the Cohere API.\ntype CohereClient struct {\n\t*client.Client\n}\n\n// NewCohereClient creates a new [client.Client] with the given configuration.\nfunc NewCohereClientWithConfig(config CohereClientConfig) *CohereClient {\n\topts := []option.RequestOption{\n\t\tclient.WithToken(config.AuthToken),\n\t\tclient.WithHTTPClient(config.HTTPClient),\n\t}\n\n\tif config.BaseURL != \"\" {\n\t\topts = append(opts, client.WithBaseURL(config.BaseURL))\n\t}\n\n\treturn &CohereClient{\n\t\tClient: client.NewClient(opts...),\n\t}\n}\n\n// CohereChatCompletionStream represents a stream for chat completion.\ntype CohereChatCompletionStream struct {\n\t*cohereStreamReader\n}\n\ntype cohereStreamReader struct {\n\t*core.Stream[cohere.StreamedChatResponse]\n}\n\n// Recv reads the next response from the stream.\nfunc (stream *cohereStreamReader) Recv() (response openai.ChatCompletionStreamResponse, err error) {\n\treturn stream.processMessages()\n}\n\n// Close closes the stream.\nfunc (stream *cohereStreamReader) Close() error {\n\tif err := stream.Stream.Close(); err != nil {\n\t\treturn fmt.Errorf(\"cohere: %w\", err)\n\t}\n\treturn nil\n}\n\nfunc (stream *cohereStreamReader) processMessages() (openai.ChatCompletionStreamResponse, error) {\n\tfor {\n\t\tmessage, err := stream.Stream.Recv()\n\t\tif err != nil {\n\t\t\tif errors.Is(err, io.EOF) {\n\t\t\t\treturn *new(openai.ChatCompletionStreamResponse), io.EOF\n\t\t\t}\n\t\t\treturn *new(openai.ChatCompletionStreamResponse), fmt.Errorf(\"cohere: %w\", err)\n\t\t}\n\n\t\tif message.EventType != \"text-generation\" {\n\t\t\tcontinue\n\t\t}\n\n\t\t// NOTE: Leverage the existing logic based on OpenAI ChatCompletionStreamResponse by\n\t\t//       converting the Cohere events into them.\n\t\tresponse := openai.ChatCompletionStreamResponse{\n\t\t\tChoices: []openai.ChatCompletionStreamChoice{\n\t\t\t\t{\n\t\t\t\t\tIndex: 0,\n\t\t\t\t\tDelta: openai.ChatCompletionStreamChoiceDelta{\n\t\t\t\t\t\tContent: message.TextGeneration.Text,\n\t\t\t\t\t\tRole:    \"assistant\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\n\t\treturn response, nil\n\t}\n}\n\n// CreateChatCompletionStream — API call to create a chat completion w/ streaming\n// support.\nfunc (c *CohereClient) CreateChatCompletionStream(\n\tctx context.Context,\n\trequest *cohere.ChatStreamRequest,\n) (stream *CohereChatCompletionStream, err error) {\n\tresp, err := c.ChatStream(ctx, request)\n\tif err != nil {\n\t\treturn\n\t}\n\n\tstream = &CohereChatCompletionStream{\n\t\tcohereStreamReader: &cohereStreamReader{\n\t\t\tStream: resp,\n\t\t},\n\t}\n\treturn\n}\n\n// CohereToOpenAIAPIError attempts to convert a Cohere API error into\n// an OpenAI API error to later reuse the existing error handling logic.\nfunc CohereToOpenAIAPIError(err error) error {\n\tce := &core.APIError{}\n\tif !errors.As(err, &ce) {\n\t\treturn err\n\t}\n\n\tunwrapped := ce.Unwrap()\n\tif unwrapped == nil {\n\t\tunwrapped = err\n\t}\n\n\tvar message string\n\tvar body map[string]interface{}\n\tif err := json.Unmarshal([]byte(unwrapped.Error()), &body); err == nil {\n\t\tmessage, _ = body[\"message\"].(string)\n\t}\n\n\tif message == \"\" {\n\t\tmessage = unwrapped.Error()\n\t}\n\n\treturn &openai.APIError{\n\t\tHTTPStatusCode: ce.StatusCode,\n\t\tMessage:        message,\n\t}\n}\n"
        },
        {
          "name": "config.go",
          "type": "blob",
          "size": 10.8310546875,
          "content": "package main\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"text/template\"\n\t\"time\"\n\n\t_ \"embed\"\n\n\t\"github.com/adrg/xdg\"\n\t\"github.com/caarlos0/duration\"\n\t\"github.com/caarlos0/env/v9\"\n\t\"github.com/charmbracelet/x/exp/strings\"\n\t\"github.com/muesli/termenv\"\n\t\"github.com/spf13/cobra\"\n\tflag \"github.com/spf13/pflag\"\n\t\"gopkg.in/yaml.v3\"\n)\n\n//go:embed config_template.yml\nvar configTemplate string\n\nconst (\n\tdefaultMarkdownFormatText = \"Format the response as markdown without enclosing backticks.\"\n\tdefaultJSONFormatText     = \"Format the response as json without enclosing backticks.\"\n)\n\nvar help = map[string]string{\n\t\"api\":               \"OpenAI compatible REST API (openai, localai).\",\n\t\"apis\":              \"Aliases and endpoints for OpenAI compatible REST API.\",\n\t\"http-proxy\":        \"HTTP proxy to use for API requests.\",\n\t\"model\":             \"Default model (gpt-3.5-turbo, gpt-4, ggml-gpt4all-j...).\",\n\t\"ask-model\":         \"Ask which model to use with an interactive prompt.\",\n\t\"max-input-chars\":   \"Default character limit on input to model.\",\n\t\"format\":            \"Ask for the response to be formatted as markdown unless otherwise set.\",\n\t\"format-text\":       \"Text to append when using the -f flag.\",\n\t\"role\":              \"System role to use.\",\n\t\"roles\":             \"List of predefined system messages that can be used as roles.\",\n\t\"list-roles\":        \"List the roles defined in your configuration file\",\n\t\"prompt\":            \"Include the prompt from the arguments and stdin, truncate stdin to specified number of lines.\",\n\t\"prompt-args\":       \"Include the prompt from the arguments in the response.\",\n\t\"raw\":               \"Render output as raw text when connected to a TTY.\",\n\t\"quiet\":             \"Quiet mode (hide the spinner while loading and stderr messages for success).\",\n\t\"help\":              \"Show help and exit.\",\n\t\"version\":           \"Show version and exit.\",\n\t\"max-retries\":       \"Maximum number of times to retry API calls.\",\n\t\"no-limit\":          \"Turn off the client-side limit on the size of the input into the model.\",\n\t\"word-wrap\":         \"Wrap formatted output at specific width (default is 80)\",\n\t\"max-tokens\":        \"Maximum number of tokens in response.\",\n\t\"temp\":              \"Temperature (randomness) of results, from 0.0 to 2.0.\",\n\t\"stop\":              \"Up to 4 sequences where the API will stop generating further tokens.\",\n\t\"topp\":              \"TopP, an alternative to temperature that narrows response, from 0.0 to 1.0.\",\n\t\"topk\":              \"TopK, only sample from the top K options for each subsequent token.\",\n\t\"fanciness\":         \"Your desired level of fanciness.\",\n\t\"status-text\":       \"Text to show while generating.\",\n\t\"settings\":          \"Open settings in your $EDITOR.\",\n\t\"dirs\":              \"Print the directories in which mods store its data.\",\n\t\"reset-settings\":    \"Backup your old settings file and reset everything to the defaults.\",\n\t\"continue\":          \"Continue from the last response or a given save title.\",\n\t\"continue-last\":     \"Continue from the last response.\",\n\t\"no-cache\":          \"Disables caching of the prompt/response.\",\n\t\"title\":             \"Saves the current conversation with the given title.\",\n\t\"list\":              \"Lists saved conversations.\",\n\t\"delete\":            \"Deletes a saved conversation with the given title or ID.\",\n\t\"delete-older-than\": \"Deletes all saved conversations older than the specified duration. Valid units are: \" + strings.EnglishJoin(duration.ValidUnits(), true) + \".\",\n\t\"show\":              \"Show a saved conversation with the given title or ID.\",\n\t\"theme\":             \"Theme to use in the forms. Valid units are: 'charm', 'catppuccin', 'dracula', and 'base16'\",\n\t\"show-last\":         \"Show the last saved conversation.\",\n}\n\n// Model represents the LLM model used in the API call.\ntype Model struct {\n\tName     string\n\tAPI      string\n\tMaxChars int      `yaml:\"max-input-chars\"`\n\tAliases  []string `yaml:\"aliases\"`\n\tFallback string   `yaml:\"fallback\"`\n}\n\n// API represents an API endpoint and its models.\ntype API struct {\n\tName      string\n\tAPIKey    string           `yaml:\"api-key\"`\n\tAPIKeyEnv string           `yaml:\"api-key-env\"`\n\tAPIKeyCmd string           `yaml:\"api-key-cmd\"`\n\tVersion   string           `yaml:\"version\"`\n\tBaseURL   string           `yaml:\"base-url\"`\n\tModels    map[string]Model `yaml:\"models\"`\n\tUser      string           `yaml:\"user\"`\n}\n\n// APIs is a type alias to allow custom YAML decoding.\ntype APIs []API\n\n// UnmarshalYAML implements sorted API YAML decoding.\nfunc (apis *APIs) UnmarshalYAML(node *yaml.Node) error {\n\tfor i := 0; i < len(node.Content); i += 2 {\n\t\tvar api API\n\t\tif err := node.Content[i+1].Decode(&api); err != nil {\n\t\t\treturn fmt.Errorf(\"error decoding YAML file: %s\", err)\n\t\t}\n\t\tapi.Name = node.Content[i].Value\n\t\t*apis = append(*apis, api)\n\t}\n\treturn nil\n}\n\n// FormatText is a map[format]formatting_text.\ntype FormatText map[string]string\n\n// UnmarshalYAML conforms with yaml.Unmarshaler.\nfunc (ft *FormatText) UnmarshalYAML(unmarshal func(interface{}) error) error {\n\tvar text string\n\tif err := unmarshal(&text); err != nil {\n\t\tvar formats map[string]string\n\t\tif err := unmarshal(&formats); err != nil {\n\t\t\treturn err\n\t\t}\n\t\t*ft = (FormatText)(formats)\n\t\treturn nil\n\t}\n\n\t*ft = map[string]string{\n\t\t\"markdown\": text,\n\t}\n\treturn nil\n}\n\n// Config holds the main configuration and is mapped to the YAML settings file.\ntype Config struct {\n\tModel             string     `yaml:\"default-model\" env:\"MODEL\"`\n\tFormat            bool       `yaml:\"format\" env:\"FORMAT\"`\n\tFormatText        FormatText `yaml:\"format-text\"`\n\tFormatAs          string     `yaml:\"format-as\" env:\"FORMAT_AS\"`\n\tRaw               bool       `yaml:\"raw\" env:\"RAW\"`\n\tQuiet             bool       `yaml:\"quiet\" env:\"QUIET\"`\n\tMaxTokens         int        `yaml:\"max-tokens\" env:\"MAX_TOKENS\"`\n\tMaxInputChars     int        `yaml:\"max-input-chars\" env:\"MAX_INPUT_CHARS\"`\n\tTemperature       float32    `yaml:\"temp\" env:\"TEMP\"`\n\tStop              []string   `yaml:\"stop\" env:\"STOP\"`\n\tTopP              float32    `yaml:\"topp\" env:\"TOPP\"`\n\tTopK              int        `yaml:\"topk\" env:\"TOPK\"`\n\tNoLimit           bool       `yaml:\"no-limit\" env:\"NO_LIMIT\"`\n\tCachePath         string     `yaml:\"cache-path\" env:\"CACHE_PATH\"`\n\tNoCache           bool       `yaml:\"no-cache\" env:\"NO_CACHE\"`\n\tIncludePromptArgs bool       `yaml:\"include-prompt-args\" env:\"INCLUDE_PROMPT_ARGS\"`\n\tIncludePrompt     int        `yaml:\"include-prompt\" env:\"INCLUDE_PROMPT\"`\n\tMaxRetries        int        `yaml:\"max-retries\" env:\"MAX_RETRIES\"`\n\tWordWrap          int        `yaml:\"word-wrap\" env:\"WORD_WRAP\"`\n\tFanciness         uint       `yaml:\"fanciness\" env:\"FANCINESS\"`\n\tStatusText        string     `yaml:\"status-text\" env:\"STATUS_TEXT\"`\n\tHTTPProxy         string     `yaml:\"http-proxy\" env:\"HTTP_PROXY\"`\n\tAPIs              APIs       `yaml:\"apis\"`\n\tSystem            string     `yaml:\"system\"`\n\tRole              string     `yaml:\"role\" env:\"ROLE\"`\n\tAskModel          bool\n\tAPI               string\n\tModels            map[string]Model\n\tRoles             map[string][]string\n\tShowHelp          bool\n\tResetSettings     bool\n\tPrefix            string\n\tVersion           bool\n\tSettings          bool\n\tDirs              bool\n\tTheme             string\n\tSettingsPath      string\n\tContinueLast      bool\n\tContinue          string\n\tTitle             string\n\tShowLast          bool\n\tShow              string\n\tList              bool\n\tListRoles         bool\n\tDelete            string\n\tDeleteOlderThan   time.Duration\n\tUser              string\n\n\tcacheReadFromID, cacheWriteToID, cacheWriteToTitle string\n}\n\nfunc ensureConfig() (Config, error) {\n\tvar c Config\n\tsp, err := xdg.ConfigFile(filepath.Join(\"mods\", \"mods.yml\"))\n\tif err != nil {\n\t\treturn c, modsError{err, \"Could not find settings path.\"}\n\t}\n\tc.SettingsPath = sp\n\n\tdir := filepath.Dir(sp)\n\tif dirErr := os.MkdirAll(dir, 0o700); dirErr != nil { //nolint:mnd\n\t\treturn c, modsError{dirErr, \"Could not create cache directory.\"}\n\t}\n\n\tif dirErr := writeConfigFile(sp); dirErr != nil {\n\t\treturn c, dirErr\n\t}\n\tcontent, err := os.ReadFile(sp)\n\tif err != nil {\n\t\treturn c, modsError{err, \"Could not read settings file.\"}\n\t}\n\tif err := yaml.Unmarshal(content, &c); err != nil {\n\t\treturn c, modsError{err, \"Could not parse settings file.\"}\n\t}\n\tms := make(map[string]Model)\n\tfor _, api := range c.APIs {\n\t\tfor mk, mv := range api.Models {\n\t\t\tmv.Name = mk\n\t\t\tmv.API = api.Name\n\t\t\t// only set the model key and aliases if they haven't already been used\n\t\t\t_, ok := ms[mk]\n\t\t\tif !ok {\n\t\t\t\tms[mk] = mv\n\t\t\t}\n\t\t\tfor _, a := range mv.Aliases {\n\t\t\t\t_, ok := ms[a]\n\t\t\t\tif !ok {\n\t\t\t\t\tms[a] = mv\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tc.Models = ms\n\n\tif err := env.ParseWithOptions(&c, env.Options{Prefix: \"MODS_\"}); err != nil {\n\t\treturn c, modsError{err, \"Could not parse environment into settings file.\"}\n\t}\n\n\tif c.CachePath == \"\" {\n\t\tc.CachePath = filepath.Join(xdg.DataHome, \"mods\", \"conversations\")\n\t}\n\n\tif err := os.MkdirAll(c.CachePath, 0o700); err != nil { //nolint:mnd\n\t\treturn c, modsError{err, \"Could not create cache directory.\"}\n\t}\n\n\tif c.WordWrap == 0 {\n\t\tc.WordWrap = 80\n\t}\n\n\treturn c, nil\n}\n\nfunc writeConfigFile(path string) error {\n\tif _, err := os.Stat(path); errors.Is(err, os.ErrNotExist) {\n\t\treturn createConfigFile(path)\n\t} else if err != nil {\n\t\treturn modsError{err, \"Could not stat path.\"}\n\t}\n\treturn nil\n}\n\nfunc createConfigFile(path string) error {\n\ttmpl := template.Must(template.New(\"config\").Parse(configTemplate))\n\n\tf, err := os.Create(path)\n\tif err != nil {\n\t\treturn modsError{err, \"Could not create configuration file.\"}\n\t}\n\tdefer func() { _ = f.Close() }()\n\n\tm := struct {\n\t\tConfig Config\n\t\tHelp   map[string]string\n\t}{\n\t\tConfig: defaultConfig(),\n\t\tHelp:   help,\n\t}\n\tif err := tmpl.Execute(f, m); err != nil {\n\t\treturn modsError{err, \"Could not render template.\"}\n\t}\n\treturn nil\n}\n\nfunc defaultConfig() Config {\n\treturn Config{\n\t\tFormatAs: \"markdown\",\n\t\tFormatText: FormatText{\n\t\t\t\"markdown\": defaultMarkdownFormatText,\n\t\t\t\"json\":     defaultJSONFormatText,\n\t\t},\n\t}\n}\n\nfunc useLine() string {\n\tappName := filepath.Base(os.Args[0])\n\n\tif stdoutRenderer().ColorProfile() == termenv.TrueColor {\n\t\tappName = makeGradientText(stdoutStyles().AppName, appName)\n\t}\n\n\treturn fmt.Sprintf(\n\t\t\"%s %s\",\n\t\tappName,\n\t\tstdoutStyles().CliArgs.Render(\"[OPTIONS] [PREFIX TERM]\"),\n\t)\n}\n\nfunc usageFunc(cmd *cobra.Command) error {\n\tfmt.Printf(\n\t\t\"Usage:\\n  %s\\n\\n\",\n\t\tuseLine(),\n\t)\n\tfmt.Println(\"Options:\")\n\tcmd.Flags().VisitAll(func(f *flag.Flag) {\n\t\tif f.Hidden {\n\t\t\treturn\n\t\t}\n\t\tif f.Shorthand == \"\" {\n\t\t\tfmt.Printf(\n\t\t\t\t\"  %-44s %s\\n\",\n\t\t\t\tstdoutStyles().Flag.Render(\"--\"+f.Name),\n\t\t\t\tstdoutStyles().FlagDesc.Render(f.Usage),\n\t\t\t)\n\t\t} else {\n\t\t\tfmt.Printf(\n\t\t\t\t\"  %s%s %-40s %s\\n\",\n\t\t\t\tstdoutStyles().Flag.Render(\"-\"+f.Shorthand),\n\t\t\t\tstdoutStyles().FlagComma,\n\t\t\t\tstdoutStyles().Flag.Render(\"--\"+f.Name),\n\t\t\t\tstdoutStyles().FlagDesc.Render(f.Usage),\n\t\t\t)\n\t\t}\n\t})\n\tif cmd.HasExample() {\n\t\tfmt.Printf(\n\t\t\t\"\\nExample:\\n  %s\\n  %s\\n\",\n\t\t\tstdoutStyles().Comment.Render(\"# \"+cmd.Example),\n\t\t\tcheapHighlighting(stdoutStyles(), examples[cmd.Example]),\n\t\t)\n\t}\n\n\treturn nil\n}\n"
        },
        {
          "name": "config_template.yml",
          "type": "blob",
          "size": 9.248046875,
          "content": "# {{ index .Help \"model\" }}\ndefault-model: gpt-4o\n# {{ index .Help \"format-text\" }}\nformat-text:\n  markdown: '{{ index .Config.FormatText \"markdown\" }}'\n  json: '{{ index .Config.FormatText \"json\" }}'\n# {{ index .Help \"roles\" }}\nroles:\n  \"default\": []\n  # Example, a role called `shell`:\n  # shell:\n  #   - you are a shell expert\n  #   - you do not explain anything\n  #   - you simply output one liners to solve the problems you're asked\n  #   - you do not provide any explanation whatsoever, ONLY the command\n# {{ index .Help \"format\" }}\nformat: false\n# {{ index .Help \"role\" }}\nrole: \"default\"\n# {{ index .Help \"raw\" }}\nraw: false\n# {{ index .Help \"quiet\" }}\nquiet: false\n# {{ index .Help \"temp\" }}\ntemp: 1.0\n# {{ index .Help \"topp\" }}\ntopp: 1.0\n# {{ index .Help \"topk\" }}\ntopk: 50\n# {{ index .Help \"no-limit\" }}\nno-limit: false\n# {{ index .Help \"word-wrap\" }}\nword-wrap: 80\n# {{ index .Help \"prompt-args\" }}\ninclude-prompt-args: false\n# {{ index .Help \"prompt\" }}\ninclude-prompt: 0\n# {{ index .Help \"max-retries\" }}\nmax-retries: 5\n# {{ index .Help \"fanciness\" }}\nfanciness: 10\n# {{ index .Help \"status-text\" }}\nstatus-text: Generating\n# {{ index .Help \"theme\" }}\ntheme: charm\n# {{ index .Help \"max-input-chars\" }}\nmax-input-chars: 12250\n# {{ index .Help \"max-tokens\" }}\n# max-tokens: 100\n# {{ index .Help \"apis\" }}\napis:\n  openai:\n    base-url: https://api.openai.com/v1\n    api-key:\n    api-key-env: OPENAI_API_KEY\n    # api-key-cmd: rbw get -f OPENAI_API_KEY chat.openai.com\n    models: # https://platform.openai.com/docs/models\n      gpt-4o-mini:\n        aliases: [\"4o-mini\"]\n        max-input-chars: 392000\n        fallback: gpt-4o\n      gpt-4o:\n        aliases: [\"4o\"]\n        max-input-chars: 392000\n        fallback: gpt-4\n      gpt-4:\n        aliases: [\"4\"]\n        max-input-chars: 24500\n        fallback: gpt-3.5-turbo\n      gpt-4-1106-preview:\n        aliases: [\"128k\"]\n        max-input-chars: 392000\n        fallback: gpt-4\n      gpt-4-32k:\n        aliases: [\"32k\"]\n        max-input-chars: 98000\n        fallback: gpt-4\n      gpt-3.5-turbo:\n        aliases: [\"35t\"]\n        max-input-chars: 12250\n        fallback: gpt-3.5\n      gpt-3.5-turbo-1106:\n        aliases: [\"35t-1106\"]\n        max-input-chars: 12250\n        fallback: gpt-3.5-turbo\n      gpt-3.5-turbo-16k:\n        aliases: [\"35t16k\"]\n        max-input-chars: 44500\n        fallback: gpt-3.5\n      gpt-3.5:\n        aliases: [\"35\"]\n        max-input-chars: 12250\n        fallback:\n  copilot:\n    base-url: https://api.githubcopilot.com\n    models:\n      gpt-4o-2024-05-13:\n        aliases: [\"4o-2024\", \"4o\", \"gpt-4o\"]\n        max-input-chars: 392000\n      gpt-4:\n        aliases: [\"4\"]\n        max-input-chars: 24500\n      gpt-3.5-turbo:\n        aliases: [\"35t\"]\n        max-input-chars: 12250\n      o1-preview-2024-09-12:\n        aliases: [\"o1-preview\", \"o1p\"]\n        max-input-chars: 128000\n      o1-mini-2024-09-12:\n        aliases: [\"o1-mini\", \"o1m\"]\n        max-input-chars: 128000\n      claude-3.5-sonnet:\n        aliases: [\"claude3.5-sonnet\", \"sonnet-3.5\", \"claude-3-5-sonnet\"]\n        max-input-chars: 680000\n  anthropic:\n    base-url: https://api.anthropic.com/v1\n    api-key:\n    api-key-env: ANTHROPIC_API_KEY\n    models: # https://docs.anthropic.com/en/docs/about-claude/models\n      claude-3-5-sonnet-latest:\n        aliases: [\"claude3.5-sonnet\", \"claude-3-5-sonnet\", \"sonnet-3.5\"]\n        max-input-chars: 680000\n      claude-3-5-sonnet-20241022:\n        max-input-chars: 680000\n      claude-3-5-sonnet-20240620:\n        max-input-chars: 680000\n      claude-3-opus-20240229:\n        aliases: [\"claude3-opus\", \"opus\"]\n        max-input-chars: 680000\n  cohere:\n    base-url: https://api.cohere.com/v1\n    models:\n      command-r-plus:\n        max-input-chars: 128000\n      command-r:\n        max-input-chars: 128000\n  google:\n    models:\n      gemini-1.5-pro-latest:\n        aliases: [\"gemini\"]\n        max-input-chars: 392000\n      gemini-1.5-flash-latest:\n        aliases: [\"flash\"]\n        max-input-chars: 392000\n  ollama:\n    base-url: http://localhost:11434/api\n    models: # https://ollama.com/library\n      \"llama3.2:3b\":\n        aliases: [\"llama3.2\"]\n        max-input-chars: 650000\n      \"llama3.2:1b\":\n        aliases: [\"llama3.2_1b\"]\n        max-input-chars: 650000\n      \"llama3:70b\":\n        aliases: [\"llama3\"]\n        max-input-chars: 650000\n  perplexity:\n    base-url: https://api.perplexity.ai\n    api-key:\n    api-key-env: PERPLEXITY_API_KEY\n    models: # https://docs.perplexity.ai/guides/model-cards                         \n      llama-3.1-sonar-small-128k-online:\n        aliases: [\"llam31-small\"]\n        max-input-chars: 127072\n      llama-3.1-sonar-large-128k-online:\n        aliases: [\"llam31-large\"]\n        max-input-chars: 127072\n      llama-3.1-sonar-huge-128k-online:\n        aliases: [\"llam31-huge\"]\n        max-input-chars: 127072\n  groq:\n    base-url: https://api.groq.com/openai/v1\n    api-key:\n    api-key-env: GROQ_API_KEY\n    models: # https://console.groq.com/docs/models\n      gemma-7b-it:\n        aliases: [\"gemma\"]\n        max-input-chars: 24500\n      gemma2-9b-it:\n        aliases: [\"gemma2\"]\n        max-input-chars: 24500\n      llama3-groq-70b-8192-tool-use-preview:\n        aliases: [\"llama3-tool\"]\n        max-input-chars: 24500\n      llama3-groq-8b-8192-tool-use-preview:\n        aliases: [\"llama3-8b-tool\"]\n        max-input-chars: 24500\n      llama-3.1-70b-versatile:\n        aliases: [\"llama3.1\", \"llama3.1-70b\", \"llama3.1-versatile\"]\n        max-input-chars: 392000\n      llama-3.1-8b-instant:\n        aliases: [\"llama3.1-8b\", \"llama3.1-instant\"]\n        max-input-chars: 392000\n      llama-guard-3-8b:\n        aliases: [\"llama-guard\"]\n        max-input-chars: 24500\n      llama3-70b-8192:\n        aliases: [\"llama3\", \"llama3-70b\"]\n        max-input-chars: 24500\n        fallback: llama3-8b-8192\n      llama3-8b-8192:\n        aliases: [\"llama3-8b\"]\n        max-input-chars: 24500\n      mixtral-8x7b-32768:\n        aliases: [\"mixtral\"]\n        max-input-chars: 98000\n  cerebras:\n    base-url: https://api.cerebras.ai/v1\n    api-key:\n    api-key-env: CEREBRAS_API_KEY\n    models: # https://inference-docs.cerebras.ai/introduction\n      llama3.1-8b:\n        aliases: [\"llama3.1-8b-cerebras\"]\n        max-input-chars: 24500\n      llama3.1-70b:\n        aliases: [\"llama3.1-cerebras\", \"llama3.1-70b-cerebras\"]\n        max-input-chars: 24500\n  sambanova:\n    base-url: https://api.sambanova.ai/v1\n    api-key:\n    api-key-env: SAMBANOVA_API_KEY\n    models: # https://community.sambanova.ai/t/supported-models/193\n      Meta-Llama-3.1-8B-Instruct:\n        aliases: [\"llama3.1-8b-sambanova-4k\", \"llama3.1-instruct-8b-sambanova-4k\"]\n        max-input-chars: 12250\n      Meta-Llama-3.1-70B-Instruct:\n        aliases: [\"llama3.1-70b-sambanova-4k\", \"llama3.1-instruct-70b-sambanova-4k\"]\n        max-input-chars: 12250\n      Meta-Llama-3.1-405B-Instruct:\n        aliases: [\"llama3.1-405b-sambanova-4k\", \"llama3.1-instruct-405b-sambanova-4k\"]\n        max-input-chars: 12250\n      Meta-Llama-3.1-8B-Instruct-8k:\n        aliases: [\"llama3.1-8b-sambanova\", \"llama3.1-instruct-8b-sambanova\", \"llama3.1-8b-sambanova-8k\", \"llama3.1-instruct-8b-sambanova-8k\"]\n        max-input-chars: 24500\n      Meta-Llama-3.1-70B-Instruct-8k:\n        aliases: [\"llama3.1-70b-sambanova\", \"llama3.1-instruct-70b-sambanova\", \"llama3.1-70b-sambanova-8k\", \"llama3.1-instruct-70b-sambanova-8k\"]\n        max-input-chars: 24500\n      Meta-Llama-3.1-405B-Instruct-8k:\n        aliases: [\"llama3.1-405b-sambanova\", \"llama3.1-instruct-405b-sambanova\", \"llama3.1-405b-sambanova-8k\", \"llama3.1-instruct-405b-sambanova-8k\"]\n        max-input-chars: 24500\n  localai:\n    # LocalAI setup instructions: https://github.com/go-skynet/LocalAI#example-use-gpt4all-j-model\n    base-url: http://localhost:8080\n    models:\n      ggml-gpt4all-j:\n        aliases: [\"local\", \"4all\"]\n        max-input-chars: 12250\n        fallback:\n  azure:\n    # Set to 'azure-ad' to use Active Directory\n    # Azure OpenAI setup: https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource\n    base-url: https://YOUR_RESOURCE_NAME.openai.azure.com\n    api-key:\n    api-key-env: AZURE_OPENAI_KEY\n    models:\n      gpt-4:\n        aliases: [\"az4\"]\n        max-input-chars: 24500\n        fallback: gpt-35-turbo\n      gpt-35-turbo:\n        aliases: [\"az35t\"]\n        max-input-chars: 12250\n        fallback: gpt-35\n      gpt-35:\n        aliases: [\"az35\"]\n        max-input-chars: 12250\n        fallback:\n  runpod:\n    # https://docs.runpod.io/serverless/workers/vllm/openai-compatibility\n    base-url: https://api.runpod.ai/v2/${YOUR_ENDPOINT}/openai/v1\n    api-key:\n    api-key-env: RUNPOD_API_KEY\n    models:\n      openchat/openchat-3.5-1210:\n        aliases: [\"openchat\"]\n        max-input-chars: 8192\n  mistral:\n    base-url: https://api.mistral.ai/v1\n    api-key:\n    api-key-env: MISTRAL_API_KEY\n    models: # https://docs.mistral.ai/getting-started/models/\n      mistral-large-latest:\n        aliases: [\"mistral-large\"]\n        max-input-chars: 384000\n      open-mistral-nemo:\n        aliases: [\"mistral-nemo\"]\n        max-input-chars: 384000\n  deepseek:\n    base-url: https://api.deepseek.com/\n    api-key:\n    api-key-env: DEEPSEEK_API_KEY\n    models: # https://platform.deepseek.com/api-docs/api/list-models/\n      deepseek-chat:\n        aliases: [\"ds-chat\"]\n        max-input-chars: 384000\n      deepseek-code:\n        aliases: [\"ds-code\"]\n        max-input-chars: 384000\n"
        },
        {
          "name": "config_test.go",
          "type": "blob",
          "size": 0.6669921875,
          "content": "package main\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n\t\"gopkg.in/yaml.v3\"\n)\n\nfunc TestConfig(t *testing.T) {\n\tt.Run(\"old format text\", func(t *testing.T) {\n\t\tvar cfg Config\n\t\trequire.NoError(t, yaml.Unmarshal([]byte(\"format-text: as markdown\"), &cfg))\n\t\trequire.Equal(t, FormatText(map[string]string{\n\t\t\t\"markdown\": \"as markdown\",\n\t\t}), cfg.FormatText)\n\t})\n\tt.Run(\"new format text\", func(t *testing.T) {\n\t\tvar cfg Config\n\t\trequire.NoError(t, yaml.Unmarshal([]byte(\"format-text:\\n  markdown: as markdown\\n  json: as json\"), &cfg))\n\t\trequire.Equal(t, FormatText(map[string]string{\n\t\t\t\"markdown\": \"as markdown\",\n\t\t\t\"json\":     \"as json\",\n\t\t}), cfg.FormatText)\n\t})\n}\n"
        },
        {
          "name": "copilot.go",
          "type": "blob",
          "size": 4.3408203125,
          "content": "package main\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"net/http\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"strings\"\n\t\"time\"\n)\n\nconst (\n\tcopilotChatAuthURL   = \"https://api.github.com/copilot_internal/v2/token\"\n\tcopilotEditorVersion = \"vscode/1.95.3\"\n\tcopilotUserAgent     = \"curl/7.81.0\" // Necessay to bypass the user-agent check\n)\n\n// Authentication response from GitHub Copilot's token endpoint\ntype CopilotAccessToken struct {\n\tToken     string `json:\"token\"`\n\tExpiresAt int64  `json:\"expires_at\"`\n\tEndpoints struct {\n\t\tAPI           string `json:\"api\"` // Can change in Github Enterprise instances\n\t\tOriginTracker string `json:\"origin-tracker\"`\n\t\tProxy         string `json:\"proxy\"`\n\t\tTelemetry     string `json:\"telemetry\"`\n\t} `json:\"endpoints\"`\n\tErrorDetails *struct {\n\t\tURL            string `json:\"url,omitempty\"`\n\t\tMessage        string `json:\"message,omitempty\"`\n\t\tTitle          string `json:\"title,omitempty\"`\n\t\tNotificationID string `json:\"notification_id,omitempty\"`\n\t} `json:\"error_details,omitempty\"`\n}\n\ntype copilotHTTPClient struct {\n\tclient      *http.Client\n\tAccessToken *CopilotAccessToken\n}\n\nfunc newCopilotHTTPClient() *copilotHTTPClient {\n\treturn &copilotHTTPClient{\n\t\tclient: &http.Client{},\n\t}\n}\n\nfunc (c *copilotHTTPClient) Do(req *http.Request) (*http.Response, error) {\n\treq.Header.Set(\"Accept\", \"application/json\")\n\treq.Header.Set(\"Editor-Version\", copilotEditorVersion)\n\treq.Header.Set(\"User-Agent\", copilotUserAgent)\n\n\tvar isTokenExpired = c.AccessToken != nil && c.AccessToken.ExpiresAt < time.Now().Unix()\n\n\tif c.AccessToken == nil || isTokenExpired {\n\t\t// Use the base http.Client for token requests to avoid recursion\n\t\taccessToken, err := getCopilotAccessToken(c.client)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to get access token: %w\", err)\n\t\t}\n\n\t\tc.AccessToken = &accessToken\n\t}\n\n\treturn c.client.Do(req)\n}\n\nfunc getCopilotRefreshToken() (string, error) {\n\tconfigPath := filepath.Join(os.Getenv(\"HOME\"), \".config/github-copilot\")\n\tif runtime.GOOS == \"windows\" {\n\t\tconfigPath = filepath.Join(os.Getenv(\"LOCALAPPDATA\"), \"github-copilot\")\n\t}\n\n\t// Check both possible config file locations\n\tconfigFiles := []string{\n\t\tfilepath.Join(configPath, \"hosts.json\"),\n\t\tfilepath.Join(configPath, \"apps.json\"),\n\t}\n\n\t// Try to get token from config files\n\tfor _, path := range configFiles {\n\t\ttoken, err := extractCopilotTokenFromFile(path)\n\t\tif err == nil && token != \"\" {\n\t\t\treturn token, nil\n\t\t}\n\t}\n\n\treturn \"\", fmt.Errorf(\"no token found in %s\", strings.Join(configFiles, \", \"))\n}\n\nfunc extractCopilotTokenFromFile(path string) (string, error) {\n\tbytes, err := os.ReadFile(path)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tvar config map[string]json.RawMessage\n\tif err := json.Unmarshal(bytes, &config); err != nil {\n\t\treturn \"\", err\n\t}\n\n\tfor key, value := range config {\n\t\tif key == \"github.com\" || strings.HasPrefix(key, \"github.com:\") {\n\t\t\tvar tokenData map[string]string\n\t\t\tif err := json.Unmarshal(value, &tokenData); err != nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif token, exists := tokenData[\"oauth_token\"]; exists {\n\t\t\t\treturn token, nil\n\t\t\t}\n\t\t}\n\t}\n\n\treturn \"\", fmt.Errorf(\"no token found in %s\", path)\n}\n\nfunc getCopilotAccessToken(client *http.Client) (CopilotAccessToken, error) {\n\trefreshToken, err := getCopilotRefreshToken()\n\tif err != nil {\n\t\treturn CopilotAccessToken{}, fmt.Errorf(\"failed to get refresh token: %w\", err)\n\t}\n\n\ttokenReq, err := http.NewRequest(http.MethodGet, copilotChatAuthURL, nil)\n\tif err != nil {\n\t\treturn CopilotAccessToken{}, fmt.Errorf(\"failed to create token request: %w\", err)\n\t}\n\n\ttokenReq.Header.Set(\"Authorization\", \"token \"+refreshToken)\n\ttokenReq.Header.Set(\"Accept\", \"application/json\")\n\ttokenReq.Header.Set(\"Editor-Version\", copilotEditorVersion)\n\ttokenReq.Header.Set(\"User-Agent\", copilotUserAgent)\n\n\ttokenResp, err := client.Do(tokenReq)\n\tif err != nil {\n\t\treturn CopilotAccessToken{}, fmt.Errorf(\"failed to get access token: %w\", err)\n\t}\n\tdefer func() {\n\t\tif closeErr := tokenResp.Body.Close(); closeErr != nil && err == nil {\n\t\t\terr = fmt.Errorf(\"error closing response body: %w\", closeErr)\n\t\t}\n\t}()\n\n\tvar tokenResponse CopilotAccessToken\n\tif err := json.NewDecoder(tokenResp.Body).Decode(&tokenResponse); err != nil {\n\t\treturn CopilotAccessToken{}, fmt.Errorf(\"failed to decode token response: %w\", err)\n\t}\n\n\tif tokenResponse.ErrorDetails != nil {\n\t\treturn CopilotAccessToken{}, fmt.Errorf(\"token error: %s\", tokenResponse.ErrorDetails.Message)\n\t}\n\n\treturn tokenResponse, err\n}\n"
        },
        {
          "name": "db.go",
          "type": "blob",
          "size": 5.4423828125,
          "content": "package main\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/jmoiron/sqlx\"\n\t\"modernc.org/sqlite\"\n)\n\nvar (\n\terrNoMatches   = errors.New(\"no conversations found\")\n\terrManyMatches = errors.New(\"multiple conversations matched the input\")\n)\n\nfunc handleSqliteErr(err error) error {\n\tsqerr := &sqlite.Error{}\n\tif errors.As(err, &sqerr) {\n\t\treturn fmt.Errorf(\n\t\t\t\"%w: %s\",\n\t\t\tsqerr,\n\t\t\tsqlite.ErrorCodeString[sqerr.Code()],\n\t\t)\n\t}\n\treturn err\n}\n\nfunc openDB(ds string) (*convoDB, error) {\n\tdb, err := sqlx.Open(\"sqlite\", ds)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\n\t\t\t\"could not create db: %w\",\n\t\t\thandleSqliteErr(err),\n\t\t)\n\t}\n\tif err := db.Ping(); err != nil {\n\t\treturn nil, fmt.Errorf(\n\t\t\t\"could not ping db: %w\",\n\t\t\thandleSqliteErr(err),\n\t\t)\n\t}\n\tif _, err := db.Exec(`\n\t\tCREATE TABLE\n\t\t  IF NOT EXISTS conversations (\n\t\t    id string NOT NULL PRIMARY KEY,\n\t\t    title string NOT NULL,\n\t\t    updated_at datetime NOT NULL DEFAULT (strftime ('%Y-%m-%d %H:%M:%f', 'now')),\n\t\t    CHECK (id <> ''),\n\t\t    CHECK (title <> '')\n\t\t  )\n\t`); err != nil {\n\t\treturn nil, fmt.Errorf(\"could not migrate db: %w\", err)\n\t}\n\tif _, err := db.Exec(`\n\t\tCREATE INDEX IF NOT EXISTS idx_conv_id ON conversations (id)\n\t`); err != nil {\n\t\treturn nil, fmt.Errorf(\"could not migrate db: %w\", err)\n\t}\n\tif _, err := db.Exec(`\n\t\tCREATE INDEX IF NOT EXISTS idx_conv_title ON conversations (title)\n\t`); err != nil {\n\t\treturn nil, fmt.Errorf(\"could not migrate db: %w\", err)\n\t}\n\n\tif !hasColumn(db, \"model\") {\n\t\tif _, err := db.Exec(`\n\t\t\tALTER TABLE conversations ADD COLUMN model string\n\t\t`); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"could not migrate db: %w\", err)\n\t\t}\n\t}\n\n\treturn &convoDB{db: db}, nil\n}\n\nfunc hasColumn(db *sqlx.DB, col string) bool {\n\tvar count int\n\tif err := db.Get(&count, `\n\t\tSELECT count(*)\n\t\tFROM pragma_table_info('conversations') c\n\t\tWHERE c.name = $1\n\t`, col); err != nil {\n\t\treturn false\n\t}\n\treturn count > 0\n}\n\ntype convoDB struct {\n\tdb *sqlx.DB\n}\n\n// Conversation in the database.\ntype Conversation struct {\n\tID        string    `db:\"id\"`\n\tTitle     string    `db:\"title\"`\n\tUpdatedAt time.Time `db:\"updated_at\"`\n\tModel     *string   `db:\"model\"`\n}\n\nfunc (c *convoDB) Close() error {\n\treturn c.db.Close() //nolint: wrapcheck\n}\n\nfunc (c *convoDB) Save(id, title, model string) error {\n\tres, err := c.db.Exec(c.db.Rebind(`\n\t\tUPDATE conversations\n\t\tSET\n\t\t  title = ?,\n\t\t  model = ?,\n\t\t  updated_at = CURRENT_TIMESTAMP\n\t\tWHERE\n\t\t  id = ?\n\t`), title, model, id)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Save: %w\", err)\n\t}\n\n\trows, err := res.RowsAffected()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Save: %w\", err)\n\t}\n\n\tif rows > 0 {\n\t\treturn nil\n\t}\n\n\tif _, err := c.db.Exec(c.db.Rebind(`\n\t\tINSERT INTO\n\t\t  conversations (id, title, model)\n\t\tVALUES\n\t\t  (?, ?, ?)\n\t`), id, title, model); err != nil {\n\t\treturn fmt.Errorf(\"Save: %w\", err)\n\t}\n\n\treturn nil\n}\n\nfunc (c *convoDB) Delete(id string) error {\n\tif _, err := c.db.Exec(c.db.Rebind(`\n\t\tDELETE FROM conversations\n\t\tWHERE\n\t\t  id = ?\n\t`), id); err != nil {\n\t\treturn fmt.Errorf(\"Delete: %w\", err)\n\t}\n\treturn nil\n}\n\nfunc (c *convoDB) ListOlderThan(t time.Duration) ([]Conversation, error) {\n\tvar convos []Conversation\n\tif err := c.db.Select(&convos, c.db.Rebind(`\n\t\tSELECT\n\t\t  *\n\t\tFROM\n\t\t  conversations\n\t\tWHERE\n\t\t  updated_at < ?\n\t\t`), time.Now().Add(-t)); err != nil {\n\t\treturn nil, fmt.Errorf(\"ListOlderThan: %w\", err)\n\t}\n\treturn convos, nil\n}\n\nfunc (c *convoDB) FindHEAD() (*Conversation, error) {\n\tvar convo Conversation\n\tif err := c.db.Get(&convo, `\n\t\tSELECT\n\t\t  *\n\t\tFROM\n\t\t  conversations\n\t\tORDER BY\n\t\t  updated_at DESC\n\t\tLIMIT\n\t\t  1\n\t`); err != nil {\n\t\treturn nil, fmt.Errorf(\"FindHead: %w\", err)\n\t}\n\treturn &convo, nil\n}\n\nfunc (c *convoDB) findByExactTitle(result *[]Conversation, in string) error {\n\tif err := c.db.Select(result, c.db.Rebind(`\n\t\tSELECT\n\t\t  *\n\t\tFROM\n\t\t  conversations\n\t\tWHERE\n\t\t  title = ?\n\t`), in); err != nil {\n\t\treturn fmt.Errorf(\"findByExactTitle: %w\", err)\n\t}\n\treturn nil\n}\n\nfunc (c *convoDB) findByIDOrTitle(result *[]Conversation, in string) error {\n\tif err := c.db.Select(result, c.db.Rebind(`\n\t\tSELECT\n\t\t  *\n\t\tFROM\n\t\t  conversations\n\t\tWHERE\n\t\t  id glob ?\n\t\t  OR title = ?\n\t`), in+\"*\", in); err != nil {\n\t\treturn fmt.Errorf(\"findByIDOrTitle: %w\", err)\n\t}\n\treturn nil\n}\n\nfunc (c *convoDB) Completions(in string) ([]string, error) {\n\tvar result []string\n\tif err := c.db.Select(&result, c.db.Rebind(`\n\t\tSELECT\n\t\t  printf (\n\t\t    '%s%c%s',\n\t\t    CASE\n\t\t      WHEN length (?) < ? THEN substr (id, 1, ?)\n\t\t      ELSE id\n\t\t    END,\n\t\t    char(9),\n\t\t    title\n\t\t  )\n\t\tFROM\n\t\t  conversations\n\t\tWHERE\n\t\t  id glob ?\n\t\tUNION\n\t\tSELECT\n\t\t  printf (\"%s%c%s\", title, char(9), substr (id, 1, ?))\n\t\tFROM\n\t\t  conversations\n\t\tWHERE\n\t\t  title glob ?\n\t`), in, sha1short, sha1short, in+\"*\", sha1short, in+\"*\"); err != nil {\n\t\treturn result, fmt.Errorf(\"Completions: %w\", err)\n\t}\n\treturn result, nil\n}\n\nfunc (c *convoDB) Find(in string) (*Conversation, error) {\n\tvar conversations []Conversation\n\tvar err error\n\n\tif len(in) < sha1minLen {\n\t\terr = c.findByExactTitle(&conversations, in)\n\t} else {\n\t\terr = c.findByIDOrTitle(&conversations, in)\n\t}\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Find: %w\", err)\n\t}\n\n\tif len(conversations) > 1 {\n\t\treturn nil, errManyMatches\n\t}\n\tif len(conversations) == 1 {\n\t\treturn &conversations[0], nil\n\t}\n\treturn nil, errNoMatches\n}\n\nfunc (c *convoDB) List() ([]Conversation, error) {\n\tvar convos []Conversation\n\tif err := c.db.Select(&convos, `\n\t\tSELECT\n\t\t  *\n\t\tFROM\n\t\t  conversations\n\t\tORDER BY\n\t\t  updated_at DESC\n\t`); err != nil {\n\t\treturn convos, fmt.Errorf(\"List: %w\", err)\n\t}\n\treturn convos, nil\n}\n"
        },
        {
          "name": "db_test.go",
          "type": "blob",
          "size": 4.228515625,
          "content": "package main\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc testDB(tb testing.TB) *convoDB {\n\tdb, err := openDB(\":memory:\")\n\trequire.NoError(tb, err)\n\ttb.Cleanup(func() {\n\t\trequire.NoError(tb, db.Close())\n\t})\n\treturn db\n}\n\nfunc TestConvoDB(t *testing.T) {\n\tconst testid = \"df31ae23ab8b75b5643c2f846c570997edc71333\"\n\n\tt.Run(\"list-empty\", func(t *testing.T) {\n\t\tdb := testDB(t)\n\t\tlist, err := db.List()\n\t\trequire.NoError(t, err)\n\t\trequire.Empty(t, list)\n\t})\n\n\tt.Run(\"save\", func(t *testing.T) {\n\t\tdb := testDB(t)\n\n\t\trequire.NoError(t, db.Save(testid, \"message 1\", \"gpt-4o\"))\n\n\t\tconvo, err := db.Find(\"df31\")\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, testid, convo.ID)\n\t\trequire.Equal(t, \"message 1\", convo.Title)\n\n\t\tlist, err := db.List()\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, list, 1)\n\t})\n\n\tt.Run(\"save no id\", func(t *testing.T) {\n\t\tdb := testDB(t)\n\t\trequire.Error(t, db.Save(\"\", \"message 1\", \"gpt-4o\"))\n\t})\n\n\tt.Run(\"save no message\", func(t *testing.T) {\n\t\tdb := testDB(t)\n\t\trequire.Error(t, db.Save(newConversationID(), \"\", \"gpt-4o\"))\n\t})\n\n\tt.Run(\"update\", func(t *testing.T) {\n\t\tdb := testDB(t)\n\n\t\trequire.NoError(t, db.Save(testid, \"message 1\", \"gpt-4o\"))\n\t\ttime.Sleep(100 * time.Millisecond)\n\t\trequire.NoError(t, db.Save(testid, \"message 2\", \"gpt-4o\"))\n\n\t\tconvo, err := db.Find(\"df31\")\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, testid, convo.ID)\n\t\trequire.Equal(t, \"message 2\", convo.Title)\n\n\t\tlist, err := db.List()\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, list, 1)\n\t})\n\n\tt.Run(\"find head single\", func(t *testing.T) {\n\t\tdb := testDB(t)\n\n\t\trequire.NoError(t, db.Save(testid, \"message 2\", \"gpt-4o\"))\n\n\t\thead, err := db.FindHEAD()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, testid, head.ID)\n\t\trequire.Equal(t, \"message 2\", head.Title)\n\t})\n\n\tt.Run(\"find head multiple\", func(t *testing.T) {\n\t\tdb := testDB(t)\n\n\t\trequire.NoError(t, db.Save(testid, \"message 2\", \"gpt-4o\"))\n\t\ttime.Sleep(time.Millisecond * 100)\n\t\tnextConvo := newConversationID()\n\t\trequire.NoError(t, db.Save(nextConvo, \"another message\", \"gpt-4o\"))\n\n\t\thead, err := db.FindHEAD()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, nextConvo, head.ID)\n\t\trequire.Equal(t, \"another message\", head.Title)\n\n\t\tlist, err := db.List()\n\t\trequire.NoError(t, err)\n\t\trequire.Len(t, list, 2)\n\t})\n\n\tt.Run(\"find by title\", func(t *testing.T) {\n\t\tdb := testDB(t)\n\n\t\trequire.NoError(t, db.Save(newConversationID(), \"message 1\", \"gpt-4o\"))\n\t\trequire.NoError(t, db.Save(testid, \"message 2\", \"gpt-4o\"))\n\n\t\tconvo, err := db.Find(\"message 2\")\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, testid, convo.ID)\n\t\trequire.Equal(t, \"message 2\", convo.Title)\n\t})\n\n\tt.Run(\"find match nothing\", func(t *testing.T) {\n\t\tdb := testDB(t)\n\t\trequire.NoError(t, db.Save(testid, \"message 1\", \"gpt-4o\"))\n\t\t_, err := db.Find(\"message\")\n\t\trequire.ErrorIs(t, err, errNoMatches)\n\t})\n\n\tt.Run(\"find match many\", func(t *testing.T) {\n\t\tdb := testDB(t)\n\t\tconst testid2 = \"df31ae23ab9b75b5641c2f846c571000edc71315\"\n\t\trequire.NoError(t, db.Save(testid, \"message 1\", \"gpt-4o\"))\n\t\trequire.NoError(t, db.Save(testid2, \"message 2\", \"gpt-4o\"))\n\t\t_, err := db.Find(\"df31ae\")\n\t\trequire.ErrorIs(t, err, errManyMatches)\n\t})\n\n\tt.Run(\"delete\", func(t *testing.T) {\n\t\tdb := testDB(t)\n\n\t\trequire.NoError(t, db.Save(testid, \"message 1\", \"gpt-4o\"))\n\t\trequire.NoError(t, db.Delete(newConversationID()))\n\n\t\tlist, err := db.List()\n\t\trequire.NoError(t, err)\n\t\trequire.NotEmpty(t, list)\n\n\t\tfor _, item := range list {\n\t\t\trequire.NoError(t, db.Delete(item.ID))\n\t\t}\n\n\t\tlist, err = db.List()\n\t\trequire.NoError(t, err)\n\t\trequire.Empty(t, list)\n\t})\n\n\tt.Run(\"completions\", func(t *testing.T) {\n\t\tdb := testDB(t)\n\n\t\tconst testid1 = \"fc5012d8c67073ea0a46a3c05488a0e1d87df74b\"\n\t\tconst title1 = \"some title\"\n\t\tconst testid2 = \"6c33f71694bf41a18c844a96d1f62f153e5f6f44\"\n\t\tconst title2 = \"football teams\"\n\t\trequire.NoError(t, db.Save(testid1, title1, \"gpt-4o\"))\n\t\trequire.NoError(t, db.Save(testid2, title2, \"gpt-4o\"))\n\n\t\tresults, err := db.Completions(\"f\")\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []string{\n\t\t\tfmt.Sprintf(\"%s\\t%s\", testid1[:sha1short], title1),\n\t\t\tfmt.Sprintf(\"%s\\t%s\", title2, testid2[:sha1short]),\n\t\t}, results)\n\n\t\tresults, err = db.Completions(testid1[:8])\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []string{\n\t\t\tfmt.Sprintf(\"%s\\t%s\", testid1, title1),\n\t\t}, results)\n\t})\n}\n"
        },
        {
          "name": "error.go",
          "type": "blob",
          "size": 0.4921875,
          "content": "package main\n\nimport \"fmt\"\n\n// newUserErrorf is a user-facing error.\n// this function is mostly to avoid linters complain about errors starting with a capitalized letter.\nfunc newUserErrorf(format string, a ...any) error {\n\treturn fmt.Errorf(format, a...)\n}\n\n// modsError is a wrapper around an error that adds additional context.\ntype modsError struct {\n\terr    error\n\treason string\n}\n\nfunc (m modsError) Error() string {\n\treturn m.err.Error()\n}\n\nfunc (m modsError) Reason() string {\n\treturn m.reason\n}\n"
        },
        {
          "name": "examples.go",
          "type": "blob",
          "size": 0.974609375,
          "content": "package main\n\nimport (\n\t\"math/rand\"\n\t\"regexp\"\n)\n\nvar examples = map[string]string{\n\t\"Write new sections for a readme\": `cat README.md | mods \"write a new section to this README documenting a pdf sharing feature\"`,\n\t\"Editorialize your video files\":   `ls ~/vids | mods -f \"summarize each of these titles, group them by decade\" | glow`,\n\t\"Let GPT pick something to watch\": `ls ~/vids | mods \"Pick 5 action packed shows from the 80s from this list\" | gum choose | xargs vlc`,\n}\n\nfunc randomExample() string {\n\tkeys := make([]string, 0, len(examples))\n\tfor k := range examples {\n\t\tkeys = append(keys, k)\n\t}\n\tdesc := keys[rand.Intn(len(keys))] //nolint:gosec\n\treturn desc\n}\n\nfunc cheapHighlighting(s styles, code string) string {\n\tcode = regexp.\n\t\tMustCompile(`\"([^\"\\\\]|\\\\.)*\"`).\n\t\tReplaceAllStringFunc(code, func(x string) string {\n\t\t\treturn s.Quote.Render(x)\n\t\t})\n\tcode = regexp.\n\t\tMustCompile(`\\|`).\n\t\tReplaceAllStringFunc(code, func(x string) string {\n\t\t\treturn s.Pipe.Render(x)\n\t\t})\n\treturn code\n}\n"
        },
        {
          "name": "examples.md",
          "type": "blob",
          "size": 3.724609375,
          "content": "# Mods Examples\n\n### Improve Your Code\n\nPiping source code to Mods and giving it an instruction on what to do with it\ngives you a lot of options for refactoring, enhancing or debugging code.\n\n`mods -f \"what are your thoughts on improving this code?\" < main.go | glow`\n\n<p><img src=\"https://github.com/charmbracelet/mods/assets/25087/738fe969-1c9f-4849-af8a-cde38156ce92\" width=\"900\" alt=\"a GIF of mods offering code refactoring suggestions\"></p>\n\n### Come Up With Product Features\n\nMods can also come up with entirely new features based on source code (or a\nREADME file).\n\n`mods -f \"come up with 10 new features for this tool.\" < main.go | glow`\n\n<p><img src=\"https://github.com/charmbracelet/mods/assets/25087/025de860-798a-4ab2-b1cf-a0b32dbdbe4d\" width=\"900\" alt=\"a GIF of mods suggesting feature improvements\"></p>\n\n### Help Write Docs\n\nMods can quickly give you a first draft for new documentation.\n\n`mods \"write a new section to this readme for a feature that sends you a free rabbit if you hit r\" < README.md | glow`\n\n<p><img src=\"https://github.com/charmbracelet/mods/assets/25087/c26a17a9-c772-40cc-b3f1-9189ac682730\" width=\"900\" alt=\"a GIF of mods contributing to a product README\"></p>\n\n### Organize Your Videos\n\nThe file system can be an amazing source of input for Mods. If you have music\nor video files, Mods can parse the output of `ls` and offer really good\neditorialization of your content.\n\n`ls ~/vids | mods -f \"organize these by decade and summarize each\" | glow`\n\n<p><img src=\"https://github.com/charmbracelet/mods/assets/25087/8204d06a-8cf1-401d-802f-2b94345dec5d\" width=\"900\" alt=\"a GIF of mods oraganizing and summarizing video from a shell ls statement\"></p>\n\n### Make Recommendations\n\nMods is really good at generating recommendations based on what you have as\nwell, both for similar content but also content in an entirely different media\n(like getting music recommendations based on movies you have).\n\n`ls ~/vids | mods -f \"recommend me 10 shows based on these, make them obscure\" | glow`\n\n`ls ~/vids | mods -f \"recommend me 10 albums based on these shows, do not include any soundtrack music or music from the show\" | glow`\n\n<p><img src=\"https://github.com/charmbracelet/mods/assets/25087/48159b19-5cae-413b-9677-dce8c6dfb6b8\" width=\"900\" alt=\"a GIF of mods generating television show recommendations based on a file listing from a directory of videos\"></p>\n\n### Read Your Fortune\n\nIt's easy to let your downloads folder grow into a chaotic never-ending pit of\nfiles, but with Mods you can use that to your advantage!\n\n`ls ~/Downloads | mods -f \"tell my fortune based on these files\" | glow`\n\n<p><img src=\"https://github.com/charmbracelet/mods/assets/25087/da2206a8-799f-4c92-b75e-bac66c56ea88\" width=\"900\" alt=\"a GIF of mods generating a fortune from the contents of a downloads directory\"></p>\n\n### Understand APIs\n\nMods can parse and understand the output of an API call with `curl` and convert\nit to something human readable.\n\n`curl \"https://api.open-meteo.com/v1/forecast?latitude=29.00&longitude=-90.00&current_weather=true&hourly=temperature_2m,relativehumidity_2m,windspeed_10m\" 2>/dev/null | mods -f \"summarize this weather data for a human.\" | glow`\n\n<p><img src=\"https://github.com/charmbracelet/mods/assets/25087/3af13876-46a3-4bab-986e-50d9f54d2921\" width=\"900\" alt=\"a GIF of mods summarizing the weather from JSON API output\"></p>\n\n### Read The Comments (so you don't have to)\n\nJust like with APIs, Mods can read through raw HTML and summarize the contents.\n\n`curl \"https://news.ycombinator.com/item?id=30048332\" 2>/dev/null | mods -f \"what are the authors of these comments saying?\" | glow`\n\n<p><img src=\"https://github.com/charmbracelet/mods/assets/25087/e4d94ef8-43aa-45ea-9be5-fe13e53d5203\" width=\"900\" alt=\"a GIF of mods summarizing the comments on hacker news\"></p>\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "features.md",
          "type": "blob",
          "size": 2.369140625,
          "content": "# Mods Features\n\n## Regular usage\n\nBy default:\n\n- all messages go to `STDERR`\n- all prompts are saved with the first line of the prompt as the title\n- glamour is used by default if `STDOUT` is a TTY\n\n### Basic\n\nThe most basic usage is:\n\n```bash\nmods 'first 2 primes'\n```\n\n### Pipe from\n\nYou can also pipe to it, in which case `STDIN` will not be a TTY:\n\n```bash\necho 'as json' | mods 'first 2 primes'\n```\n\nIn this case, `mods` should read `STDIN` and append it to the prompt.\n\n### Pipe to\n\nYou may also pipe the output to another program, in which case `STDOUT` will not\nbe a TTY:\n\n```bash\necho 'as json' | mods 'first 2 primes' | jq .\n```\n\nIn this case, the \"Generating\" animation will go to `STDERR`, but the response\nwill be streamed to `STDOUT`.\n\n### Custom title\n\nYou can set a custom title:\n\n```bash\nmods --title='title' 'first 2 primes'\n```\n\n### Continue latest\n\nYou can continue the latest conversation and save it with a new title using\n`--continue=title`:\n\n```bash\nmods 'first 2 primes'\nmods --continue='primes as json' 'format as json'\n```\n\n### Untitled continue latest\n\n```bash\nmods 'first 2 primes'\nmods --continue-last 'format as json'\n```\n\n### Continue from specific conversation, save with a new title\n\n```bash\nmods --title='naturals' 'first 5 natural numbers'\nmods --continue='naturals' --title='naturals.json' 'format as json'\n```\n\n### Conversation branching\n\nYou can use the `--continue` and `--title` to branch out conversations, for\ninstance:\n\n```bash\nmods --title='naturals' 'first 5 natural numbers'\nmods --continue='naturals' --title='naturals.json' 'format as json'\nmods --continue='naturals' --title='naturals.yaml' 'format as yaml'\n```\n\nWith this you'll end up with 3 conversations: `naturals`, `naturals.json`, and\n`naturals.yaml`.\n\n## List conversations\n\nYou can list your previous conversations with:\n\n```bash\nmods --list\n# or\nmods -l\n```\n\n## Show a previous conversation\n\nYou can also show a previous conversation by ID or title, e.g.:\n\n```bash\nmods --show='naturals'\nmods -s='a2e2'\n```\n\nFor titles, the match should be exact.\nFor IDs, only the first 4 chars are needed. If it matches multiple\nconversations, you can add more chars until it matches a single one again.\n\n## Delete a conversation\n\nYou can also delete conversations by title or ID, same as `--show`, different\nflag:\n\n```bash\nmods --delete='naturals'\nmods --delete='a2e2'\n```\n\nKeep in mind that these operations are not reversible.\n"
        },
        {
          "name": "flag.go",
          "type": "blob",
          "size": 1.5732421875,
          "content": "package main\n\nimport (\n\t\"regexp\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/caarlos0/duration\"\n)\n\nfunc newFlagParseError(err error) flagParseError {\n\tvar reason, flag string\n\ts := err.Error()\n\tswitch {\n\tcase strings.HasPrefix(s, \"flag needs an argument:\"):\n\t\treason = \"Flag %s needs an argument.\"\n\t\tps := strings.Split(s, \"-\")\n\t\tswitch len(ps) {\n\t\tcase 2: //nolint:mnd\n\t\t\tflag = \"-\" + ps[len(ps)-1]\n\t\tcase 3: //nolint:mnd\n\t\t\tflag = \"--\" + ps[len(ps)-1]\n\t\t}\n\tcase strings.HasPrefix(s, \"unknown flag:\"):\n\t\treason = \"Flag %s is missing.\"\n\t\tflag = strings.TrimPrefix(s, \"unknown flag: \")\n\tcase strings.HasPrefix(s, \"invalid argument\"):\n\t\treason = \"Flag %s have an invalid argument.\"\n\t\tre := regexp.MustCompile(`invalid argument \".*\" for \"(.*)\" flag: .*`)\n\t\tparts := re.FindStringSubmatch(s)\n\t\tif len(parts) > 1 {\n\t\t\tflag = parts[1]\n\t\t}\n\tdefault:\n\t\treason = s\n\t}\n\treturn flagParseError{\n\t\terr:    err,\n\t\treason: reason,\n\t\tflag:   flag,\n\t}\n}\n\ntype flagParseError struct {\n\terr    error\n\treason string\n\tflag   string\n}\n\nfunc (f flagParseError) Error() string {\n\treturn f.err.Error()\n}\n\nfunc (f flagParseError) ReasonFormat() string {\n\treturn f.reason\n}\n\nfunc (f flagParseError) Flag() string {\n\treturn f.flag\n}\n\nfunc newDurationFlag(val time.Duration, p *time.Duration) *durationFlag {\n\t*p = val\n\treturn (*durationFlag)(p)\n}\n\ntype durationFlag time.Duration\n\nfunc (d *durationFlag) Set(s string) error {\n\tv, err := duration.Parse(s)\n\t*d = durationFlag(v)\n\t//nolint: wrapcheck\n\treturn err\n}\n\nfunc (d *durationFlag) String() string {\n\treturn time.Duration(*d).String()\n}\n\nfunc (*durationFlag) Type() string {\n\treturn \"duration\"\n}\n"
        },
        {
          "name": "flag_test.go",
          "type": "blob",
          "size": 1.208984375,
          "content": "package main\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\nvar flagParseErrorTests = []struct {\n\tin     string\n\tflag   string\n\treason string\n}{\n\t{\n\t\t\"unknown flag: --nope\",\n\t\t\"--nope\",\n\t\t\"Flag %s is missing.\",\n\t},\n\t{\n\t\t\"flag needs an argument: --delete\",\n\t\t\"--delete\",\n\t\t\"Flag %s needs an argument.\",\n\t},\n\t{\n\t\t\"flag needs an argument: 'd' in -d\",\n\t\t\"-d\",\n\t\t\"Flag %s needs an argument.\",\n\t},\n\t{\n\t\t`invalid argument \"20dd\" for \"--delete-older-than\" flag: time: unknown unit \"dd\" in duration \"20dd\"`,\n\t\t\"--delete-older-than\",\n\t\t\"Flag %s have an invalid argument.\",\n\t},\n\t{\n\t\t`invalid argument \"sdfjasdl\" for \"--max-tokens\" flag: strconv.ParseInt: parsing \"sdfjasdl\": invalid syntax`,\n\t\t\"--max-tokens\",\n\t\t\"Flag %s have an invalid argument.\",\n\t},\n\t{\n\t\t`invalid argument \"nope\" for \"-r, --raw\" flag: strconv.ParseBool: parsing \"nope\": invalid syntax`,\n\t\t\"-r, --raw\",\n\t\t\"Flag %s have an invalid argument.\",\n\t},\n}\n\nfunc TestFlagParseError(t *testing.T) {\n\tfor _, tf := range flagParseErrorTests {\n\t\tt.Run(tf.in, func(t *testing.T) {\n\t\t\terr := newFlagParseError(fmt.Errorf(tf.in))\n\t\t\trequire.Equal(t, tf.flag, err.Flag())\n\t\t\trequire.Equal(t, tf.reason, err.ReasonFormat())\n\t\t\trequire.Equal(t, tf.in, err.Error())\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "format.go",
          "type": "blob",
          "size": 0.4716796875,
          "content": "package main\n\nimport (\n\t\"encoding/gob\"\n\t\"fmt\"\n\t\"io\"\n\n\topenai \"github.com/sashabaranov/go-openai\"\n)\n\nfunc encode(w io.Writer, messages *[]openai.ChatCompletionMessage) error {\n\tif err := gob.NewEncoder(w).Encode(messages); err != nil {\n\t\treturn fmt.Errorf(\"encode: %w\", err)\n\t}\n\treturn nil\n}\n\nfunc decode(r io.Reader, messages *[]openai.ChatCompletionMessage) error {\n\tif err := gob.NewDecoder(r).Decode(messages); err != nil {\n\t\treturn fmt.Errorf(\"decode: %w\", err)\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 3.169921875,
          "content": "module github.com/charmbracelet/mods\n\ngo 1.21\n\nrequire (\n\tgithub.com/adrg/xdg v0.5.3\n\tgithub.com/atotto/clipboard v0.1.4\n\tgithub.com/caarlos0/duration v0.0.0-20240108180406-5d492514f3c7\n\tgithub.com/caarlos0/env/v9 v9.0.0\n\tgithub.com/caarlos0/go-shellwords v1.0.12\n\tgithub.com/caarlos0/timea.go v1.2.0\n\tgithub.com/charmbracelet/bubbles v0.20.0\n\tgithub.com/charmbracelet/bubbletea v1.2.4\n\tgithub.com/charmbracelet/glamour v0.8.0\n\tgithub.com/charmbracelet/huh v0.6.0\n\tgithub.com/charmbracelet/lipgloss v1.0.0\n\tgithub.com/charmbracelet/x/editor v0.1.0\n\tgithub.com/charmbracelet/x/exp/ordered v0.1.0\n\tgithub.com/charmbracelet/x/exp/strings v0.0.0-20240722160745-212f7b056ed0\n\tgithub.com/cohere-ai/cohere-go/v2 v2.12.4\n\tgithub.com/jmoiron/sqlx v1.4.0\n\tgithub.com/lucasb-eyer/go-colorful v1.2.0\n\tgithub.com/mattn/go-isatty v0.0.20\n\tgithub.com/muesli/mango-cobra v1.2.0\n\tgithub.com/muesli/roff v0.1.0\n\tgithub.com/muesli/termenv v0.15.3-0.20240618155329-98d742f6907a\n\tgithub.com/sashabaranov/go-openai v1.36.1\n\tgithub.com/spf13/cobra v1.8.1\n\tgithub.com/spf13/pflag v1.0.5\n\tgithub.com/stretchr/testify v1.10.0\n\tgopkg.in/yaml.v3 v3.0.1\n\tmodernc.org/sqlite v1.34.4\n)\n\nrequire (\n\tgithub.com/alecthomas/chroma/v2 v2.14.0 // indirect\n\tgithub.com/aws/aws-sdk-go-v2 v1.30.3 // indirect\n\tgithub.com/aws/smithy-go v1.20.3 // indirect\n\tgithub.com/aymanbagabas/go-osc52/v2 v2.0.1 // indirect\n\tgithub.com/aymerick/douceur v0.2.0 // indirect\n\tgithub.com/catppuccin/go v0.2.0 // indirect\n\tgithub.com/charmbracelet/x/ansi v0.4.5 // indirect\n\tgithub.com/charmbracelet/x/term v0.2.1 // indirect\n\tgithub.com/davecgh/go-spew v1.1.1 // indirect\n\tgithub.com/dlclark/regexp2 v1.11.0 // indirect\n\tgithub.com/dustin/go-humanize v1.0.1 // indirect\n\tgithub.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f // indirect\n\tgithub.com/google/uuid v1.6.0 // indirect\n\tgithub.com/gorilla/css v1.0.1 // indirect\n\tgithub.com/hashicorp/golang-lru/v2 v2.0.7 // indirect\n\tgithub.com/inconshreveable/mousetrap v1.1.0 // indirect\n\tgithub.com/mattn/go-localereader v0.0.1 // indirect\n\tgithub.com/mattn/go-runewidth v0.0.16 // indirect\n\tgithub.com/microcosm-cc/bluemonday v1.0.27 // indirect\n\tgithub.com/mitchellh/hashstructure/v2 v2.0.2 // indirect\n\tgithub.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6 // indirect\n\tgithub.com/muesli/cancelreader v0.2.2 // indirect\n\tgithub.com/muesli/mango v0.1.0 // indirect\n\tgithub.com/muesli/mango-pflag v0.1.0 // indirect\n\tgithub.com/muesli/reflow v0.3.0 // indirect\n\tgithub.com/ncruces/go-strftime v0.1.9 // indirect\n\tgithub.com/pmezard/go-difflib v1.0.0 // indirect\n\tgithub.com/remyoudompheng/bigfft v0.0.0-20230129092748-24d4a6f8daec // indirect\n\tgithub.com/rivo/uniseg v0.4.7 // indirect\n\tgithub.com/yuin/goldmark v1.7.4 // indirect\n\tgithub.com/yuin/goldmark-emoji v1.0.3 // indirect\n\tgolang.org/x/net v0.27.0 // indirect\n\tgolang.org/x/sync v0.9.0 // indirect\n\tgolang.org/x/sys v0.27.0 // indirect\n\tgolang.org/x/term v0.22.0 // indirect\n\tgolang.org/x/text v0.18.0 // indirect\n\tmodernc.org/gc/v3 v3.0.0-20240107210532-573471604cb6 // indirect\n\tmodernc.org/libc v1.55.3 // indirect\n\tmodernc.org/mathutil v1.6.0 // indirect\n\tmodernc.org/memory v1.8.0 // indirect\n\tmodernc.org/strutil v1.2.0 // indirect\n\tmodernc.org/token v1.1.0 // indirect\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 16.1962890625,
          "content": "filippo.io/edwards25519 v1.1.0 h1:FNf4tywRC1HmFuKW5xopWpigGjJKiJSV0Cqo0cJWDaA=\nfilippo.io/edwards25519 v1.1.0/go.mod h1:BxyFTGdWcka3PhytdK4V28tE5sGfRvvvRV7EaN4VDT4=\ngithub.com/MakeNowJust/heredoc v1.0.0 h1:cXCdzVdstXyiTqTvfqk9SDHpKNjxuom+DOlyEeQ4pzQ=\ngithub.com/MakeNowJust/heredoc v1.0.0/go.mod h1:mG5amYoWBHf8vpLOuehzbGGw0EHxpZZ6lCpQ4fNJ8LE=\ngithub.com/adrg/xdg v0.5.3 h1:xRnxJXne7+oWDatRhR1JLnvuccuIeCoBu2rtuLqQB78=\ngithub.com/adrg/xdg v0.5.3/go.mod h1:nlTsY+NNiCBGCK2tpm09vRqfVzrc2fLmXGpBLF0zlTQ=\ngithub.com/alecthomas/assert/v2 v2.7.0 h1:QtqSACNS3tF7oasA8CU6A6sXZSBDqnm7RfpLl9bZqbE=\ngithub.com/alecthomas/assert/v2 v2.7.0/go.mod h1:Bze95FyfUr7x34QZrjL+XP+0qgp/zg8yS+TtBj1WA3k=\ngithub.com/alecthomas/chroma/v2 v2.14.0 h1:R3+wzpnUArGcQz7fCETQBzO5n9IMNi13iIs46aU4V9E=\ngithub.com/alecthomas/chroma/v2 v2.14.0/go.mod h1:QolEbTfmUHIMVpBqxeDnNBj2uoeI4EbYP4i6n68SG4I=\ngithub.com/alecthomas/repr v0.4.0 h1:GhI2A8MACjfegCPVq9f1FLvIBS+DrQ2KQBFZP1iFzXc=\ngithub.com/alecthomas/repr v0.4.0/go.mod h1:Fr0507jx4eOXV7AlPV6AVZLYrLIuIeSOWtW57eE/O/4=\ngithub.com/atotto/clipboard v0.1.4 h1:EH0zSVneZPSuFR11BlR9YppQTVDbh5+16AmcJi4g1z4=\ngithub.com/atotto/clipboard v0.1.4/go.mod h1:ZY9tmq7sm5xIbd9bOK4onWV4S6X0u6GY7Vn0Yu86PYI=\ngithub.com/aws/aws-sdk-go-v2 v1.30.3 h1:jUeBtG0Ih+ZIFH0F4UkmL9w3cSpaMv9tYYDbzILP8dY=\ngithub.com/aws/aws-sdk-go-v2 v1.30.3/go.mod h1:nIQjQVp5sfpQcTc9mPSr1B0PaWK5ByX9MOoDadSN4lc=\ngithub.com/aws/smithy-go v1.20.3 h1:ryHwveWzPV5BIof6fyDvor6V3iUL7nTfiTKXHiW05nE=\ngithub.com/aws/smithy-go v1.20.3/go.mod h1:krry+ya/rV9RDcV/Q16kpu6ypI4K2czasz0NC3qS14E=\ngithub.com/aymanbagabas/go-osc52/v2 v2.0.1 h1:HwpRHbFMcZLEVr42D4p7XBqjyuxQH5SMiErDT4WkJ2k=\ngithub.com/aymanbagabas/go-osc52/v2 v2.0.1/go.mod h1:uYgXzlJ7ZpABp8OJ+exZzJJhRNQ2ASbcXHWsFqH8hp8=\ngithub.com/aymanbagabas/go-udiff v0.2.0 h1:TK0fH4MteXUDspT88n8CKzvK0X9O2xu9yQjWpi6yML8=\ngithub.com/aymanbagabas/go-udiff v0.2.0/go.mod h1:RE4Ex0qsGkTAJoQdQQCA0uG+nAzJO/pI/QwceO5fgrA=\ngithub.com/aymerick/douceur v0.2.0 h1:Mv+mAeH1Q+n9Fr+oyamOlAkUNPWPlA8PPGR0QAaYuPk=\ngithub.com/aymerick/douceur v0.2.0/go.mod h1:wlT5vV2O3h55X9m7iVYN0TBM0NH/MmbLnd30/FjWUq4=\ngithub.com/caarlos0/duration v0.0.0-20240108180406-5d492514f3c7 h1:kJP/C2eL9DCKrCOlX6lPVmAUAb6U4u9xllgws1kP9ds=\ngithub.com/caarlos0/duration v0.0.0-20240108180406-5d492514f3c7/go.mod h1:mSkwb/eZEwOJJJ4tqAKiuhLIPe0e9+FKhlU0oMCpbf8=\ngithub.com/caarlos0/env/v9 v9.0.0 h1:SI6JNsOA+y5gj9njpgybykATIylrRMklbs5ch6wO6pc=\ngithub.com/caarlos0/env/v9 v9.0.0/go.mod h1:ye5mlCVMYh6tZ+vCgrs/B95sj88cg5Tlnc0XIzgZ020=\ngithub.com/caarlos0/go-shellwords v1.0.12 h1:HWrUnu6lGbWfrDcFiHcZiwOLzHWjjrPVehULaTFgPp8=\ngithub.com/caarlos0/go-shellwords v1.0.12/go.mod h1:bYeeX1GrTLPl5cAMYEzdm272qdsQAZiaHgeF0KTk1Gw=\ngithub.com/caarlos0/timea.go v1.2.0 h1:JkjyWSUheN4nGO/OmYVGKbEv4ozHP/zuTZWD5Ih3Gog=\ngithub.com/caarlos0/timea.go v1.2.0/go.mod h1:p4uopjR7K+y0Oxh7j0vLh3vSo58jjzOgXHKcyKwQjuY=\ngithub.com/catppuccin/go v0.2.0 h1:ktBeIrIP42b/8FGiScP9sgrWOss3lw0Z5SktRoithGA=\ngithub.com/catppuccin/go v0.2.0/go.mod h1:8IHJuMGaUUjQM82qBrGNBv7LFq6JI3NnQCF6MOlZjpc=\ngithub.com/charmbracelet/bubbles v0.20.0 h1:jSZu6qD8cRQ6k9OMfR1WlM+ruM8fkPWkHvQWD9LIutE=\ngithub.com/charmbracelet/bubbles v0.20.0/go.mod h1:39slydyswPy+uVOHZ5x/GjwVAFkCsV8IIVy+4MhzwwU=\ngithub.com/charmbracelet/bubbletea v1.2.4 h1:KN8aCViA0eps9SCOThb2/XPIlea3ANJLUkv3KnQRNCE=\ngithub.com/charmbracelet/bubbletea v1.2.4/go.mod h1:Qr6fVQw+wX7JkWWkVyXYk/ZUQ92a6XNekLXa3rR18MM=\ngithub.com/charmbracelet/glamour v0.8.0 h1:tPrjL3aRcQbn++7t18wOpgLyl8wrOHUEDS7IZ68QtZs=\ngithub.com/charmbracelet/glamour v0.8.0/go.mod h1:ViRgmKkf3u5S7uakt2czJ272WSg2ZenlYEZXT2x7Bjw=\ngithub.com/charmbracelet/huh v0.6.0 h1:mZM8VvZGuE0hoDXq6XLxRtgfWyTI3b2jZNKh0xWmax8=\ngithub.com/charmbracelet/huh v0.6.0/go.mod h1:GGNKeWCeNzKpEOh/OJD8WBwTQjV3prFAtQPpLv+AVwU=\ngithub.com/charmbracelet/lipgloss v1.0.0 h1:O7VkGDvqEdGi93X+DeqsQ7PKHDgtQfF8j8/O2qFMQNg=\ngithub.com/charmbracelet/lipgloss v1.0.0/go.mod h1:U5fy9Z+C38obMs+T+tJqst9VGzlOYGj4ri9reL3qUlo=\ngithub.com/charmbracelet/x/ansi v0.4.5 h1:LqK4vwBNaXw2AyGIICa5/29Sbdq58GbGdFngSexTdRM=\ngithub.com/charmbracelet/x/ansi v0.4.5/go.mod h1:dk73KoMTT5AX5BsX0KrqhsTqAnhZZoCBjs7dGWp4Ktw=\ngithub.com/charmbracelet/x/editor v0.1.0 h1:p69/dpvlwRTs9uYiPeAWruwsHqTFzHhTvQOd/WVSX98=\ngithub.com/charmbracelet/x/editor v0.1.0/go.mod h1:oivrEbcP/AYt/Hpvk5pwDXXrQ933gQS6UzL6fxqAGSA=\ngithub.com/charmbracelet/x/exp/golden v0.0.0-20240815200342-61de596daa2b h1:MnAMdlwSltxJyULnrYbkZpp4k58Co7Tah3ciKhSNo0Q=\ngithub.com/charmbracelet/x/exp/golden v0.0.0-20240815200342-61de596daa2b/go.mod h1:wDlXFlCrmJ8J+swcL/MnGUuYnqgQdW9rhSD61oNMb6U=\ngithub.com/charmbracelet/x/exp/ordered v0.1.0 h1:55/qLwjIh0gL0Vni+QAWk7T/qRVP6sBf+2agPBgnOFE=\ngithub.com/charmbracelet/x/exp/ordered v0.1.0/go.mod h1:5UHwmG+is5THxMyCJHNPCn2/ecI07aKNrW+LcResjJ8=\ngithub.com/charmbracelet/x/exp/strings v0.0.0-20240722160745-212f7b056ed0 h1:qko3AQ4gK1MTS/de7F5hPGx6/k1u0w4TeYmBFwzYVP4=\ngithub.com/charmbracelet/x/exp/strings v0.0.0-20240722160745-212f7b056ed0/go.mod h1:pBhA0ybfXv6hDjQUZ7hk1lVxBiUbupdw5R31yPUViVQ=\ngithub.com/charmbracelet/x/term v0.2.1 h1:AQeHeLZ1OqSXhrAWpYUtZyX1T3zVxfpZuEQMIQaGIAQ=\ngithub.com/charmbracelet/x/term v0.2.1/go.mod h1:oQ4enTYFV7QN4m0i9mzHrViD7TQKvNEEkHUMCmsxdUg=\ngithub.com/cohere-ai/cohere-go/v2 v2.12.4 h1:hWiOc7LkwJ21S3hh3Ogh9Fe5s9ZDsVu11qoaMGfYZRQ=\ngithub.com/cohere-ai/cohere-go/v2 v2.12.4/go.mod h1:MuiJkCxlR18BDV2qQPbz2Yb/OCVphT1y6nD2zYaKeR0=\ngithub.com/cpuguy83/go-md2man/v2 v2.0.4/go.mod h1:tgQtvFlXSQOSOSIRvRPT7W67SCa46tRHOmNcaadrF8o=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/dlclark/regexp2 v1.11.0 h1:G/nrcoOa7ZXlpoa/91N3X7mM3r8eIlMBBJZvsz/mxKI=\ngithub.com/dlclark/regexp2 v1.11.0/go.mod h1:DHkYz0B9wPfa6wondMfaivmHpzrQ3v9q8cnmRbL6yW8=\ngithub.com/dustin/go-humanize v1.0.1 h1:GzkhY7T5VNhEkwH0PVJgjz+fX1rhBrR7pRT3mDkpeCY=\ngithub.com/dustin/go-humanize v1.0.1/go.mod h1:Mu1zIs6XwVuF/gI1OepvI0qD18qycQx+mFykh5fBlto=\ngithub.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f h1:Y/CXytFA4m6baUTXGLOoWe4PQhGxaX0KpnayAqC48p4=\ngithub.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f/go.mod h1:vw97MGsxSvLiUE2X8qFplwetxpGLQrlU1Q9AUEIzCaM=\ngithub.com/go-sql-driver/mysql v1.8.1 h1:LedoTUt/eveggdHS9qUFC1EFSa8bU2+1pZjSRpvNJ1Y=\ngithub.com/go-sql-driver/mysql v1.8.1/go.mod h1:wEBSXgmK//2ZFJyE+qWnIsVGmvmEKlqwuVSjsCm7DZg=\ngithub.com/google/pprof v0.0.0-20240409012703-83162a5b38cd h1:gbpYu9NMq8jhDVbvlGkMFWCjLFlqqEZjEmObmhUy6Vo=\ngithub.com/google/pprof v0.0.0-20240409012703-83162a5b38cd/go.mod h1:kf6iHlnVGwgKolg33glAes7Yg/8iWP8ukqeldJSO7jw=\ngithub.com/google/uuid v1.6.0 h1:NIvaJDMOsjHA8n1jAhLSgzrAzy1Hgr+hNrb57e+94F0=\ngithub.com/google/uuid v1.6.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/gorilla/css v1.0.1 h1:ntNaBIghp6JmvWnxbZKANoLyuXTPZ4cAMlo6RyhlbO8=\ngithub.com/gorilla/css v1.0.1/go.mod h1:BvnYkspnSzMmwRK+b8/xgNPLiIuNZr6vbZBTPQ2A3b0=\ngithub.com/hashicorp/golang-lru/v2 v2.0.7 h1:a+bsQ5rvGLjzHuww6tVxozPZFVghXaHOwFs4luLUK2k=\ngithub.com/hashicorp/golang-lru/v2 v2.0.7/go.mod h1:QeFd9opnmA6QUJc5vARoKUSoFhyfM2/ZepoAG6RGpeM=\ngithub.com/hexops/gotextdiff v1.0.3 h1:gitA9+qJrrTCsiCl7+kh75nPqQt1cx4ZkudSTLoUqJM=\ngithub.com/hexops/gotextdiff v1.0.3/go.mod h1:pSWU5MAI3yDq+fZBTazCSJysOMbxWL1BSow5/V2vxeg=\ngithub.com/inconshreveable/mousetrap v1.1.0 h1:wN+x4NVGpMsO7ErUn/mUI3vEoE6Jt13X2s0bqwp9tc8=\ngithub.com/inconshreveable/mousetrap v1.1.0/go.mod h1:vpF70FUmC8bwa3OWnCshd2FqLfsEA9PFc4w1p2J65bw=\ngithub.com/jmoiron/sqlx v1.4.0 h1:1PLqN7S1UYp5t4SrVVnt4nUVNemrDAtxlulVe+Qgm3o=\ngithub.com/jmoiron/sqlx v1.4.0/go.mod h1:ZrZ7UsYB/weZdl2Bxg6jCRO9c3YHl8r3ahlKmRT4JLY=\ngithub.com/lib/pq v1.10.9 h1:YXG7RB+JIjhP29X+OtkiDnYaXQwpS4JEWq7dtCCRUEw=\ngithub.com/lib/pq v1.10.9/go.mod h1:AlVN5x4E4T544tWzH6hKfbfQvm3HdbOxrmggDNAPY9o=\ngithub.com/lucasb-eyer/go-colorful v1.2.0 h1:1nnpGOrhyZZuNyfu1QjKiUICQ74+3FNCN69Aj6K7nkY=\ngithub.com/lucasb-eyer/go-colorful v1.2.0/go.mod h1:R4dSotOR9KMtayYi1e77YzuveK+i7ruzyGqttikkLy0=\ngithub.com/mattn/go-isatty v0.0.20 h1:xfD0iDuEKnDkl03q4limB+vH+GxLEtL/jb4xVJSWWEY=\ngithub.com/mattn/go-isatty v0.0.20/go.mod h1:W+V8PltTTMOvKvAeJH7IuucS94S2C6jfK/D7dTCTo3Y=\ngithub.com/mattn/go-localereader v0.0.1 h1:ygSAOl7ZXTx4RdPYinUpg6W99U8jWvWi9Ye2JC/oIi4=\ngithub.com/mattn/go-localereader v0.0.1/go.mod h1:8fBrzywKY7BI3czFoHkuzRoWE9C+EiG4R1k4Cjx5p88=\ngithub.com/mattn/go-runewidth v0.0.12/go.mod h1:RAqKPSqVFrSLVXbA8x7dzmKdmGzieGRCM46jaSJTDAk=\ngithub.com/mattn/go-runewidth v0.0.16 h1:E5ScNMtiwvlvB5paMFdw9p4kSQzbXFikJ5SQO6TULQc=\ngithub.com/mattn/go-runewidth v0.0.16/go.mod h1:Jdepj2loyihRzMpdS35Xk/zdY8IAYHsh153qUoGf23w=\ngithub.com/mattn/go-sqlite3 v1.14.22 h1:2gZY6PC6kBnID23Tichd1K+Z0oS6nE/XwU+Vz/5o4kU=\ngithub.com/mattn/go-sqlite3 v1.14.22/go.mod h1:Uh1q+B4BYcTPb+yiD3kU8Ct7aC0hY9fxUwlHK0RXw+Y=\ngithub.com/microcosm-cc/bluemonday v1.0.27 h1:MpEUotklkwCSLeH+Qdx1VJgNqLlpY2KXwXFM08ygZfk=\ngithub.com/microcosm-cc/bluemonday v1.0.27/go.mod h1:jFi9vgW+H7c3V0lb6nR74Ib/DIB5OBs92Dimizgw2cA=\ngithub.com/mitchellh/hashstructure/v2 v2.0.2 h1:vGKWl0YJqUNxE8d+h8f6NJLcCJrgbhC4NcD46KavDd4=\ngithub.com/mitchellh/hashstructure/v2 v2.0.2/go.mod h1:MG3aRVU/N29oo/V/IhBX8GR/zz4kQkprJgF2EVszyDE=\ngithub.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6 h1:ZK8zHtRHOkbHy6Mmr5D264iyp3TiX5OmNcI5cIARiQI=\ngithub.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6/go.mod h1:CJlz5H+gyd6CUWT45Oy4q24RdLyn7Md9Vj2/ldJBSIo=\ngithub.com/muesli/cancelreader v0.2.2 h1:3I4Kt4BQjOR54NavqnDogx/MIoWBFa0StPA8ELUXHmA=\ngithub.com/muesli/cancelreader v0.2.2/go.mod h1:3XuTXfFS2VjM+HTLZY9Ak0l6eUKfijIfMUZ4EgX0QYo=\ngithub.com/muesli/mango v0.1.0 h1:DZQK45d2gGbql1arsYA4vfg4d7I9Hfx5rX/GCmzsAvI=\ngithub.com/muesli/mango v0.1.0/go.mod h1:5XFpbC8jY5UUv89YQciiXNlbi+iJgt29VDC5xbzrLL4=\ngithub.com/muesli/mango-cobra v1.2.0 h1:DQvjzAM0PMZr85Iv9LIMaYISpTOliMEg+uMFtNbYvWg=\ngithub.com/muesli/mango-cobra v1.2.0/go.mod h1:vMJL54QytZAJhCT13LPVDfkvCUJ5/4jNUKF/8NC2UjA=\ngithub.com/muesli/mango-pflag v0.1.0 h1:UADqbYgpUyRoBja3g6LUL+3LErjpsOwaC9ywvBWe7Sg=\ngithub.com/muesli/mango-pflag v0.1.0/go.mod h1:YEQomTxaCUp8PrbhFh10UfbhbQrM/xJ4i2PB8VTLLW0=\ngithub.com/muesli/reflow v0.3.0 h1:IFsN6K9NfGtjeggFP+68I4chLZV2yIKsXJFNZ+eWh6s=\ngithub.com/muesli/reflow v0.3.0/go.mod h1:pbwTDkVPibjO2kyvBQRBxTWEEGDGq0FlB1BIKtnHY/8=\ngithub.com/muesli/roff v0.1.0 h1:YD0lalCotmYuF5HhZliKWlIx7IEhiXeSfq7hNjFqGF8=\ngithub.com/muesli/roff v0.1.0/go.mod h1:pjAHQM9hdUUwm/krAfrLGgJkXJ+YuhtsfZ42kieB2Ig=\ngithub.com/muesli/termenv v0.15.3-0.20240618155329-98d742f6907a h1:2MaM6YC3mGu54x+RKAA6JiFFHlHDY1UbkxqppT7wYOg=\ngithub.com/muesli/termenv v0.15.3-0.20240618155329-98d742f6907a/go.mod h1:hxSnBBYLK21Vtq/PHd0S2FYCxBXzBua8ov5s1RobyRQ=\ngithub.com/ncruces/go-strftime v0.1.9 h1:bY0MQC28UADQmHmaF5dgpLmImcShSi2kHU9XLdhx/f4=\ngithub.com/ncruces/go-strftime v0.1.9/go.mod h1:Fwc5htZGVVkseilnfgOVb9mKy6w1naJmn9CehxcKcls=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/remyoudompheng/bigfft v0.0.0-20230129092748-24d4a6f8daec h1:W09IVJc94icq4NjY3clb7Lk8O1qJ8BdBEF8z0ibU0rE=\ngithub.com/remyoudompheng/bigfft v0.0.0-20230129092748-24d4a6f8daec/go.mod h1:qqbHyh8v60DhA7CoWK5oRCqLrMHRGoxYCSS9EjAz6Eo=\ngithub.com/rivo/uniseg v0.1.0/go.mod h1:J6wj4VEh+S6ZtnVlnTBMWIodfgj8LQOQFoIToxlJtxc=\ngithub.com/rivo/uniseg v0.2.0/go.mod h1:J6wj4VEh+S6ZtnVlnTBMWIodfgj8LQOQFoIToxlJtxc=\ngithub.com/rivo/uniseg v0.4.7 h1:WUdvkW8uEhrYfLC4ZzdpI2ztxP1I582+49Oc5Mq64VQ=\ngithub.com/rivo/uniseg v0.4.7/go.mod h1:FN3SvrM+Zdj16jyLfmOkMNblXMcoc8DfTHruCPUcx88=\ngithub.com/russross/blackfriday/v2 v2.1.0/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=\ngithub.com/sashabaranov/go-openai v1.36.1 h1:EVfRXwIlW2rUzpx6vR+aeIKCK/xylSrVYAx1TMTSX3g=\ngithub.com/sashabaranov/go-openai v1.36.1/go.mod h1:lj5b/K+zjTSFxVLijLSTDZuP7adOgerWeFyZLUhAKRg=\ngithub.com/spf13/cobra v1.8.1 h1:e5/vxKd/rZsfSJMUX1agtjeTDf+qv1/JdBF8gg5k9ZM=\ngithub.com/spf13/cobra v1.8.1/go.mod h1:wHxEcudfqmLYa8iTfL+OuZPbBZkmvliBWKIezN3kD9Y=\ngithub.com/spf13/pflag v1.0.5 h1:iy+VFUOCP1a+8yFto/drg2CJ5u0yRoB7fZw3DKv/JXA=\ngithub.com/spf13/pflag v1.0.5/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=\ngithub.com/stretchr/testify v1.10.0 h1:Xv5erBjTwe/5IxqUQTdXv5kgmIvbHo3QQyRwhJsOfJA=\ngithub.com/stretchr/testify v1.10.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=\ngithub.com/yuin/goldmark v1.7.1/go.mod h1:uzxRWxtg69N339t3louHJ7+O03ezfj6PlliRlaOzY1E=\ngithub.com/yuin/goldmark v1.7.4 h1:BDXOHExt+A7gwPCJgPIIq7ENvceR7we7rOS9TNoLZeg=\ngithub.com/yuin/goldmark v1.7.4/go.mod h1:uzxRWxtg69N339t3louHJ7+O03ezfj6PlliRlaOzY1E=\ngithub.com/yuin/goldmark-emoji v1.0.3 h1:aLRkLHOuBR2czCY4R8olwMjID+tENfhyFDMCRhbIQY4=\ngithub.com/yuin/goldmark-emoji v1.0.3/go.mod h1:tTkZEbwu5wkPmgTcitqddVxY9osFZiavD+r4AzQrh1U=\ngolang.org/x/mod v0.17.0 h1:zY54UmvipHiNd+pm+m0x9KhZ9hl1/7QNMyxXbc6ICqA=\ngolang.org/x/mod v0.17.0/go.mod h1:hTbmBsO62+eylJbnUtE2MGJUyE7QWk4xUqPFrRgJ+7c=\ngolang.org/x/net v0.27.0 h1:5K3Njcw06/l2y9vpGCSdcxWOYHOUk3dVNGDXN+FvAys=\ngolang.org/x/net v0.27.0/go.mod h1:dDi0PyhWNoiUOrAS8uXv/vnScO4wnHQO4mj9fn/RytE=\ngolang.org/x/sync v0.9.0 h1:fEo0HyrW1GIgZdpbhCRO0PkJajUS5H9IFUztCgEo2jQ=\ngolang.org/x/sync v0.9.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=\ngolang.org/x/sys v0.0.0-20210809222454-d867a43fc93e/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.6.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.27.0 h1:wBqf8DvsY9Y/2P8gAfPDEYNuS30J4lPHJxXSb/nJZ+s=\ngolang.org/x/sys v0.27.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\ngolang.org/x/term v0.22.0 h1:BbsgPEJULsl2fV/AT3v15Mjva5yXKQDyKf+TbDz7QJk=\ngolang.org/x/term v0.22.0/go.mod h1:F3qCibpT5AMpCRfhfT53vVJwhLtIVHhB9XDjfFvnMI4=\ngolang.org/x/text v0.18.0 h1:XvMDiNzPAl0jr17s6W9lcaIhGUfUORdGCNsuLmPG224=\ngolang.org/x/text v0.18.0/go.mod h1:BuEKDfySbSR4drPmRPG/7iBdf8hvFMuRexcpahXilzY=\ngolang.org/x/tools v0.21.1-0.20240508182429-e35e4ccd0d2d h1:vU5i/LfpvrRCpgM/VPfJLg5KjxD3E+hfT1SH+d9zLwg=\ngolang.org/x/tools v0.21.1-0.20240508182429-e35e4ccd0d2d/go.mod h1:aiJjzUbINMkxbQROHiO6hDPo2LHcIPhhQsa9DLh0yGk=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405 h1:yhCVgyC4o1eVCa2tZl7eS0r+SDo693bJlVdllGtEeKM=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\nmodernc.org/cc/v4 v4.21.4 h1:3Be/Rdo1fpr8GrQ7IVw9OHtplU4gWbb+wNgeoBMmGLQ=\nmodernc.org/cc/v4 v4.21.4/go.mod h1:HM7VJTZbUCR3rV8EYBi9wxnJ0ZBRiGE5OeGXNA0IsLQ=\nmodernc.org/ccgo/v4 v4.19.2 h1:lwQZgvboKD0jBwdaeVCTouxhxAyN6iawF3STraAal8Y=\nmodernc.org/ccgo/v4 v4.19.2/go.mod h1:ysS3mxiMV38XGRTTcgo0DQTeTmAO4oCmJl1nX9VFI3s=\nmodernc.org/fileutil v1.3.0 h1:gQ5SIzK3H9kdfai/5x41oQiKValumqNTDXMvKo62HvE=\nmodernc.org/fileutil v1.3.0/go.mod h1:XatxS8fZi3pS8/hKG2GH/ArUogfxjpEKs3Ku3aK4JyQ=\nmodernc.org/gc/v2 v2.4.1 h1:9cNzOqPyMJBvrUipmynX0ZohMhcxPtMccYgGOJdOiBw=\nmodernc.org/gc/v2 v2.4.1/go.mod h1:wzN5dK1AzVGoH6XOzc3YZ+ey/jPgYHLuVckd62P0GYU=\nmodernc.org/gc/v3 v3.0.0-20240107210532-573471604cb6 h1:5D53IMaUuA5InSeMu9eJtlQXS2NxAhyWQvkKEgXZhHI=\nmodernc.org/gc/v3 v3.0.0-20240107210532-573471604cb6/go.mod h1:Qz0X07sNOR1jWYCrJMEnbW/X55x206Q7Vt4mz6/wHp4=\nmodernc.org/libc v1.55.3 h1:AzcW1mhlPNrRtjS5sS+eW2ISCgSOLLNyFzRh/V3Qj/U=\nmodernc.org/libc v1.55.3/go.mod h1:qFXepLhz+JjFThQ4kzwzOjA/y/artDeg+pcYnY+Q83w=\nmodernc.org/mathutil v1.6.0 h1:fRe9+AmYlaej+64JsEEhoWuAYBkOtQiMEU7n/XgfYi4=\nmodernc.org/mathutil v1.6.0/go.mod h1:Ui5Q9q1TR2gFm0AQRqQUaBWFLAhQpCwNcuhBOSedWPo=\nmodernc.org/memory v1.8.0 h1:IqGTL6eFMaDZZhEWwcREgeMXYwmW83LYW8cROZYkg+E=\nmodernc.org/memory v1.8.0/go.mod h1:XPZ936zp5OMKGWPqbD3JShgd/ZoQ7899TUuQqxY+peU=\nmodernc.org/opt v0.1.3 h1:3XOZf2yznlhC+ibLltsDGzABUGVx8J6pnFMS3E4dcq4=\nmodernc.org/opt v0.1.3/go.mod h1:WdSiB5evDcignE70guQKxYUl14mgWtbClRi5wmkkTX0=\nmodernc.org/sortutil v1.2.0 h1:jQiD3PfS2REGJNzNCMMaLSp/wdMNieTbKX920Cqdgqc=\nmodernc.org/sortutil v1.2.0/go.mod h1:TKU2s7kJMf1AE84OoiGppNHJwvB753OYfNl2WRb++Ss=\nmodernc.org/sqlite v1.34.4 h1:sjdARozcL5KJBvYQvLlZEmctRgW9xqIZc2ncN7PU0P8=\nmodernc.org/sqlite v1.34.4/go.mod h1:3QQFCG2SEMtc2nv+Wq4cQCH7Hjcg+p/RMlS1XK+zwbk=\nmodernc.org/strutil v1.2.0 h1:agBi9dp1I+eOnxXeiZawM8F4LawKv4NzGWSaLfyeNZA=\nmodernc.org/strutil v1.2.0/go.mod h1:/mdcBmfOibveCTBxUl5B5l6W+TTH1FXPLHZE6bTosX0=\nmodernc.org/token v1.1.0 h1:Xl7Ap9dKaEs5kLoOQeQmPWevfnk/DM5qcLcYlA8ys6Y=\nmodernc.org/token v1.1.0/go.mod h1:UGzOrNV1mAFSEB63lOFHIpNRUVMvYTc6yu1SMY/XTDM=\n"
        },
        {
          "name": "google.go",
          "type": "blob",
          "size": 8.322265625,
          "content": "package main\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\n\topenai \"github.com/sashabaranov/go-openai\"\n)\n\nvar googleHeaderData = []byte(\"data: \")\n\n// GoogleClientConfig represents the configuration for the Google API client.\ntype GoogleClientConfig struct {\n\tBaseURL            string\n\tHTTPClient         *http.Client\n\tEmptyMessagesLimit uint\n}\n\n// DefaultGoogleConfig returns the default configuration for the Google API client.\nfunc DefaultGoogleConfig(model, authToken string) GoogleClientConfig {\n\treturn GoogleClientConfig{\n\t\tBaseURL:            fmt.Sprintf(\"https://generativelanguage.googleapis.com/v1beta/models/%s:streamGenerateContent?alt=sse&key=%s\", model, authToken),\n\t\tHTTPClient:         &http.Client{},\n\t\tEmptyMessagesLimit: defaultEmptyMessagesLimit,\n\t}\n}\n\n// GoogleParts is a datatype containing media that is part of a multi-part Content message.\ntype GoogleParts struct {\n\tText string `json:\"text,omitempty\"`\n}\n\n// GoogleContent is the base structured datatype containing multi-part content of a message.\ntype GoogleContent struct {\n\tParts []GoogleParts `json:\"parts,omitempty\"`\n\tRole  string        `json:\"role,omitempty\"`\n}\n\n// GoogleGenerationConfig are the options for model generation and outputs. Not all parameters are configurable for every model.\ntype GoogleGenerationConfig struct {\n\tStopSequences    []string `json:\"stopSequences,omitempty\"`\n\tResponseMimeType string   `json:\"responseMimeType,omitempty\"`\n\tCandidateCount   uint     `json:\"candidateCount,omitempty\"`\n\tMaxOutputTokens  uint     `json:\"maxOutputTokens,omitempty\"`\n\tTemperature      float32  `json:\"temperature,omitempty\"`\n\tTopP             float32  `json:\"topP,omitempty\"`\n\tTopK             int      `json:\"topK,omitempty\"`\n}\n\n// GoogleMessageCompletionRequestOptions represents the valid parameters and value options for the request.\ntype GoogleMessageCompletionRequest struct {\n\tContents         []GoogleContent        `json:\"contents,omitempty\"`\n\tGenerationConfig GoogleGenerationConfig `json:\"generationConfig,omitempty\"`\n}\n\n// GoogleRequestBuilder is an interface for building HTTP requests for the Google API.\ntype GoogleRequestBuilder interface {\n\tBuild(ctx context.Context, method, url string, body any, header http.Header) (*http.Request, error)\n}\n\n// NewGoogleRequestBuilder creates a new HTTPRequestBuilder.\nfunc NewGoogleRequestBuilder() *HTTPRequestBuilder {\n\treturn &HTTPRequestBuilder{\n\t\tmarshaller: &JSONMarshaller{},\n\t}\n}\n\n// GoogleClient is a client for the Anthropic API.\ntype GoogleClient struct {\n\tconfig GoogleClientConfig\n\n\trequestBuilder GoogleRequestBuilder\n}\n\n// NewGoogleClient creates a new AnthropicClient with the given configuration.\nfunc NewGoogleClientWithConfig(config GoogleClientConfig) *GoogleClient {\n\treturn &GoogleClient{\n\t\tconfig:         config,\n\t\trequestBuilder: NewGoogleRequestBuilder(),\n\t}\n}\n\nfunc (c *GoogleClient) newRequest(ctx context.Context, method, url string, setters ...requestOption) (*http.Request, error) {\n\t// Default Options\n\targs := &requestOptions{\n\t\tbody:   nil,\n\t\theader: make(http.Header),\n\t}\n\tfor _, setter := range setters {\n\t\tsetter(args)\n\t}\n\treq, err := c.requestBuilder.Build(ctx, method, url, args.body, args.header)\n\tif err != nil {\n\t\treturn new(http.Request), err\n\t}\n\treturn req, nil\n}\n\nfunc (c *GoogleClient) handleErrorResp(resp *http.Response) error {\n\t// Print the response text\n\tvar errRes openai.ErrorResponse\n\terr := json.NewDecoder(resp.Body).Decode(&errRes)\n\tif err != nil || errRes.Error == nil {\n\t\treqErr := &openai.RequestError{\n\t\t\tHTTPStatusCode: resp.StatusCode,\n\t\t\tErr:            err,\n\t\t}\n\t\tif errRes.Error != nil {\n\t\t\treqErr.Err = errRes.Error\n\t\t}\n\t\treturn reqErr\n\t}\n\n\terrRes.Error.HTTPStatusCode = resp.StatusCode\n\treturn errRes.Error\n}\n\n// GoogleCandidates represents a response candidate generated from the model.\ntype GoogleCandidate struct {\n\tContent      GoogleContent `json:\"content,omitempty\"`\n\tFinishReason string        `json:\"finishReason,omitempty\"`\n\tTokenCount   uint          `json:\"tokenCount,omitempty\"`\n\tIndex        uint          `json:\"index,omitempty\"`\n}\n\n// GoogleCompletionMessageResponse represents a response to an Google completion message.\ntype GoogleCompletionMessageResponse struct {\n\tCandidates []GoogleCandidate `json:\"candidates,omitempty\"`\n}\n\n// GoogleChatCompletionStream represents a stream for chat completion.\ntype GoogleChatCompletionStream struct {\n\t*googleStreamReader\n}\n\ntype googleStreamReader struct {\n\temptyMessagesLimit uint\n\tisFinished         bool\n\n\treader         *bufio.Reader\n\tresponse       *http.Response\n\terrAccumulator ErrorAccumulator\n\tunmarshaler    Unmarshaler\n\n\thttpHeader\n}\n\n// Recv reads the next response from the stream.\nfunc (stream *googleStreamReader) Recv() (response openai.ChatCompletionStreamResponse, err error) {\n\tif stream.isFinished {\n\t\terr = io.EOF\n\t\treturn\n\t}\n\n\tresponse, err = stream.processLines()\n\treturn\n}\n\n// Close closes the stream.\nfunc (stream *googleStreamReader) Close() error {\n\treturn stream.response.Body.Close() //nolint:wrapcheck\n}\n\n//nolint:gocognit\nfunc (stream *googleStreamReader) processLines() (openai.ChatCompletionStreamResponse, error) {\n\tvar (\n\t\temptyMessagesCount uint\n\t\thasError           bool\n\t)\n\n\tfor {\n\t\trawLine, readErr := stream.reader.ReadBytes('\\n')\n\n\t\tif readErr != nil {\n\t\t\treturn *new(openai.ChatCompletionStreamResponse), fmt.Errorf(\"googleStreamReader.processLines: %w\", readErr)\n\t\t}\n\n\t\tnoSpaceLine := bytes.TrimSpace(rawLine)\n\n\t\tif bytes.HasPrefix(noSpaceLine, errorPrefix) {\n\t\t\thasError = true\n\t\t\t// NOTE: Continue to the next event to get the error data.\n\t\t\tcontinue\n\t\t}\n\n\t\tif !bytes.HasPrefix(noSpaceLine, googleHeaderData) || hasError {\n\t\t\tif hasError {\n\t\t\t\tnoSpaceLine = bytes.TrimPrefix(noSpaceLine, googleHeaderData)\n\t\t\t}\n\t\t\twriteErr := stream.errAccumulator.Write(noSpaceLine)\n\t\t\tif writeErr != nil {\n\t\t\t\treturn *new(openai.ChatCompletionStreamResponse), fmt.Errorf(\"ollamaStreamReader.processLines: %w\", writeErr)\n\t\t\t}\n\t\t\temptyMessagesCount++\n\t\t\tif emptyMessagesCount > stream.emptyMessagesLimit {\n\t\t\t\treturn *new(openai.ChatCompletionStreamResponse), ErrTooManyEmptyStreamMessages\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\tnoPrefixLine := bytes.TrimPrefix(noSpaceLine, googleHeaderData)\n\n\t\tvar chunk GoogleCompletionMessageResponse\n\t\tunmarshalErr := stream.unmarshaler.Unmarshal(noPrefixLine, &chunk)\n\t\tif unmarshalErr != nil {\n\t\t\treturn *new(openai.ChatCompletionStreamResponse), fmt.Errorf(\"googleStreamReader.processLines: %w\", unmarshalErr)\n\t\t}\n\n\t\t// NOTE: Leverage the existing logic based on OpenAI ChatCompletionStreamResponse by\n\t\t//       converting the Anthropic events into them.\n\t\tif len(chunk.Candidates) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tparts := chunk.Candidates[0].Content.Parts\n\t\tif len(parts) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tresponse := openai.ChatCompletionStreamResponse{\n\t\t\tChoices: []openai.ChatCompletionStreamChoice{\n\t\t\t\t{\n\t\t\t\t\tIndex: 0,\n\t\t\t\t\tDelta: openai.ChatCompletionStreamChoiceDelta{\n\t\t\t\t\t\tContent: chunk.Candidates[0].Content.Parts[0].Text,\n\t\t\t\t\t\tRole:    \"assistant\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\n\t\treturn response, nil\n\t}\n}\n\nfunc googleSendRequestStream(client *GoogleClient, req *http.Request) (*googleStreamReader, error) {\n\treq.Header.Set(\"content-type\", \"application/json\")\n\n\tresp, err := client.config.HTTPClient.Do(req) //nolint:bodyclose // body is closed in stream.Close()\n\tif err != nil {\n\t\treturn new(googleStreamReader), err\n\t}\n\tif isFailureStatusCode(resp) {\n\t\treturn new(googleStreamReader), client.handleErrorResp(resp)\n\t}\n\treturn &googleStreamReader{\n\t\temptyMessagesLimit: client.config.EmptyMessagesLimit,\n\t\treader:             bufio.NewReader(resp.Body),\n\t\tresponse:           resp,\n\t\terrAccumulator:     NewErrorAccumulator(),\n\t\tunmarshaler:        &JSONUnmarshaler{},\n\t\thttpHeader:         httpHeader(resp.Header),\n\t}, nil\n}\n\n// CreateChatCompletionStream — API call to create a chat completion w/ streaming\n// support. It sets whether to stream back partial progress. If set, tokens will be\n// sent as data-only server-sent events as they become available, with the\n// stream terminated by a data: [DONE] message.\nfunc (c *GoogleClient) CreateChatCompletionStream(\n\tctx context.Context,\n\trequest GoogleMessageCompletionRequest,\n) (stream *GoogleChatCompletionStream, err error) {\n\treq, err := c.newRequest(ctx, http.MethodPost, c.config.BaseURL, withBody(request))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tresp, err := googleSendRequestStream(c, req)\n\tif err != nil {\n\t\treturn\n\t}\n\tstream = &GoogleChatCompletionStream{\n\t\tgoogleStreamReader: resp,\n\t}\n\treturn\n}\n"
        },
        {
          "name": "load.go",
          "type": "blob",
          "size": 0.6533203125,
          "content": "package main\n\nimport (\n\t\"io\"\n\t\"net/http\"\n\t\"os\"\n\t\"strings\"\n)\n\nfunc loadMsg(msg string) (string, error) {\n\tif strings.HasPrefix(msg, \"https://\") || strings.HasPrefix(msg, \"http://\") {\n\t\tresp, err := http.Get(msg) //nolint:gosec,noctx\n\t\tif err != nil {\n\t\t\treturn \"\", err //nolint:wrapcheck\n\t\t}\n\t\tdefer func() { _ = resp.Body.Close() }()\n\t\tbts, err := io.ReadAll(resp.Body)\n\t\tif err != nil {\n\t\t\treturn \"\", err //nolint:wrapcheck\n\t\t}\n\t\treturn string(bts), nil\n\t}\n\n\tif strings.HasPrefix(msg, \"file://\") {\n\t\tbts, err := os.ReadFile(strings.TrimPrefix(msg, \"file://\"))\n\t\tif err != nil {\n\t\t\treturn \"\", err //nolint:wrapcheck\n\t\t}\n\t\treturn string(bts), nil\n\t}\n\n\treturn msg, nil\n}\n"
        },
        {
          "name": "load_test.go",
          "type": "blob",
          "size": 0.9365234375,
          "content": "package main\n\nimport (\n\t\"os\"\n\t\"path/filepath\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestLoad(t *testing.T) {\n\tconst content = \"just text\"\n\tt.Run(\"normal msg\", func(t *testing.T) {\n\t\tmsg, err := loadMsg(content)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, content, msg)\n\t})\n\n\tt.Run(\"file\", func(t *testing.T) {\n\t\tpath := filepath.Join(t.TempDir(), \"foo.txt\")\n\t\trequire.NoError(t, os.WriteFile(path, []byte(content), 0o644))\n\n\t\tmsg, err := loadMsg(\"file://\" + path)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, content, msg)\n\t})\n\n\tt.Run(\"http url\", func(t *testing.T) {\n\t\tmsg, err := loadMsg(\"http://raw.githubusercontent.com/charmbracelet/mods/main/LICENSE\")\n\t\trequire.NoError(t, err)\n\t\trequire.Contains(t, msg, \"MIT License\")\n\t})\n\n\tt.Run(\"https url\", func(t *testing.T) {\n\t\tmsg, err := loadMsg(\"https://raw.githubusercontent.com/charmbracelet/mods/main/LICENSE\")\n\t\trequire.NoError(t, err)\n\t\trequire.Contains(t, msg, \"MIT License\")\n\t})\n}\n"
        },
        {
          "name": "main.go",
          "type": "blob",
          "size": 22.923828125,
          "content": "package main\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime/debug\"\n\t\"runtime/pprof\"\n\t\"slices\"\n\t\"strings\"\n\n\t\"github.com/atotto/clipboard\"\n\ttimeago \"github.com/caarlos0/timea.go\"\n\ttea \"github.com/charmbracelet/bubbletea\"\n\tglamour \"github.com/charmbracelet/glamour/styles\"\n\t\"github.com/charmbracelet/huh\"\n\t\"github.com/charmbracelet/x/editor\"\n\tmcobra \"github.com/muesli/mango-cobra\"\n\t\"github.com/muesli/roff\"\n\t\"github.com/muesli/termenv\"\n\t\"github.com/spf13/cobra\"\n)\n\n// Build vars.\nvar (\n\t//nolint: gochecknoglobals\n\tVersion   = \"\"\n\tCommitSHA = \"\"\n)\n\nfunc buildVersion() {\n\tif len(CommitSHA) >= sha1short {\n\t\tvt := rootCmd.VersionTemplate()\n\t\trootCmd.SetVersionTemplate(vt[:len(vt)-1] + \" (\" + CommitSHA[0:7] + \")\\n\")\n\t}\n\tif Version == \"\" {\n\t\tif info, ok := debug.ReadBuildInfo(); ok && info.Main.Sum != \"\" {\n\t\t\tVersion = info.Main.Version\n\t\t} else {\n\t\t\tVersion = \"unknown (built from source)\"\n\t\t}\n\t}\n\trootCmd.Version = Version\n}\n\nfunc init() {\n\t// XXX: unset error styles in Glamour dark and light styles.\n\t// On the glamour side, we should probably add constructors for generating\n\t// default styles so they can be essentially copied and altered without\n\t// mutating the definitions in Glamour itself (or relying on any deep\n\t// copying).\n\tglamour.DarkStyleConfig.CodeBlock.Chroma.Error.BackgroundColor = new(string)\n\tglamour.LightStyleConfig.CodeBlock.Chroma.Error.BackgroundColor = new(string)\n\n\tbuildVersion()\n\trootCmd.SetUsageFunc(usageFunc)\n\trootCmd.SetFlagErrorFunc(func(_ *cobra.Command, err error) error {\n\t\treturn newFlagParseError(err)\n\t})\n\n\trootCmd.CompletionOptions.HiddenDefaultCmd = true\n\trootCmd.SetHelpCommand(&cobra.Command{Hidden: true})\n}\n\nvar (\n\tconfig = defaultConfig()\n\tdb     *convoDB\n\tcache  *convoCache\n\n\trootCmd = &cobra.Command{\n\t\tUse:           \"mods\",\n\t\tShort:         \"GPT on the command line. Built for pipelines.\",\n\t\tSilenceUsage:  true,\n\t\tSilenceErrors: true,\n\t\tExample:       randomExample(),\n\t\tRunE: func(cmd *cobra.Command, args []string) error {\n\t\t\tconfig.Prefix = removeWhitespace(strings.Join(args, \" \"))\n\n\t\t\topts := []tea.ProgramOption{}\n\n\t\t\tif !isInputTTY() || config.Raw {\n\t\t\t\topts = append(opts, tea.WithInput(nil))\n\t\t\t}\n\t\t\tif isOutputTTY() && !config.Raw {\n\t\t\t\topts = append(opts, tea.WithOutput(os.Stderr))\n\t\t\t} else {\n\t\t\t\topts = append(opts, tea.WithoutRenderer())\n\t\t\t}\n\t\t\tif os.Getenv(\"VIMRUNTIME\") != \"\" {\n\t\t\t\tconfig.Quiet = true\n\t\t\t}\n\n\t\t\tif (isNoArgs() || config.AskModel) && isInputTTY() {\n\t\t\t\tif err := askInfo(); err != nil && err == huh.ErrUserAborted {\n\t\t\t\t\treturn modsError{\n\t\t\t\t\t\terr:    err,\n\t\t\t\t\t\treason: \"User canceled.\",\n\t\t\t\t\t}\n\t\t\t\t} else if err != nil {\n\t\t\t\t\treturn modsError{\n\t\t\t\t\t\terr:    err,\n\t\t\t\t\t\treason: \"Prompt failed.\",\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tmods := newMods(stderrRenderer(), &config, db, cache)\n\t\t\tp := tea.NewProgram(mods, opts...)\n\t\t\tm, err := p.Run()\n\t\t\tif err != nil {\n\t\t\t\treturn modsError{err, \"Couldn't start Bubble Tea program.\"}\n\t\t\t}\n\n\t\t\tmods = m.(*Mods)\n\t\t\tif mods.Error != nil {\n\t\t\t\treturn *mods.Error\n\t\t\t}\n\n\t\t\tif config.Dirs {\n\t\t\t\tif len(args) > 0 {\n\t\t\t\t\tswitch args[0] {\n\t\t\t\t\tcase \"config\":\n\t\t\t\t\t\tfmt.Println(filepath.Dir(config.SettingsPath))\n\t\t\t\t\t\treturn nil\n\t\t\t\t\tcase \"cache\":\n\t\t\t\t\t\tfmt.Println(filepath.Dir(config.CachePath))\n\t\t\t\t\t\treturn nil\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfmt.Printf(\"Configuration: %s\\n\", filepath.Dir(config.SettingsPath))\n\t\t\t\t//nolint:mnd\n\t\t\t\tfmt.Printf(\"%*sCache: %s\\n\", 8, \" \", filepath.Dir(config.CachePath))\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\tif config.Settings {\n\t\t\t\tc, err := editor.Cmd(\"mods\", config.SettingsPath)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn modsError{\n\t\t\t\t\t\terr:    err,\n\t\t\t\t\t\treason: \"Could not edit your settings file.\",\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tc.Stdin = os.Stdin\n\t\t\t\tc.Stdout = os.Stdout\n\t\t\t\tc.Stderr = os.Stderr\n\t\t\t\tif err := c.Run(); err != nil {\n\t\t\t\t\treturn modsError{err, fmt.Sprintf(\n\t\t\t\t\t\t\"Missing %s.\",\n\t\t\t\t\t\tstderrStyles().InlineCode.Render(\"$EDITOR\"),\n\t\t\t\t\t)}\n\t\t\t\t}\n\n\t\t\t\tif !config.Quiet {\n\t\t\t\t\tfmt.Fprintln(os.Stderr, \"Wrote config file to:\", config.SettingsPath)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\tif config.ResetSettings {\n\t\t\t\treturn resetSettings()\n\t\t\t}\n\n\t\t\tif mods.Input == \"\" && isNoArgs() {\n\t\t\t\treturn modsError{\n\t\t\t\t\treason: \"You haven't provided any prompt input.\",\n\t\t\t\t\terr: newUserErrorf(\n\t\t\t\t\t\t\"You can give your prompt as arguments and/or pipe it from STDIN.\\nExample: %s\",\n\t\t\t\t\t\tstdoutStyles().InlineCode.Render(\"mods [prompt]\"),\n\t\t\t\t\t),\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif config.ShowHelp {\n\t\t\t\treturn cmd.Usage()\n\t\t\t}\n\n\t\t\tif config.ListRoles {\n\t\t\t\treturn listRoles()\n\t\t\t}\n\t\t\tif config.List {\n\t\t\t\treturn listConversations()\n\t\t\t}\n\n\t\t\tif config.Delete != \"\" {\n\t\t\t\treturn deleteConversation()\n\t\t\t}\n\n\t\t\tif config.DeleteOlderThan > 0 {\n\t\t\t\treturn deleteConversationOlderThan()\n\t\t\t}\n\n\t\t\tif isOutputTTY() {\n\t\t\t\tswitch {\n\t\t\t\tcase mods.glamOutput != \"\":\n\t\t\t\t\tfmt.Print(mods.glamOutput)\n\t\t\t\tcase mods.Output != \"\":\n\t\t\t\t\tfmt.Print(mods.Output)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif config.Show != \"\" || config.ShowLast {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\tif config.cacheWriteToID != \"\" {\n\t\t\t\treturn saveConversation(mods)\n\t\t\t}\n\n\t\t\treturn nil\n\t\t},\n\t}\n)\n\nvar memprofile bool\n\nfunc initFlags() {\n\tflags := rootCmd.Flags()\n\tflags.StringVarP(&config.Model, \"model\", \"m\", config.Model, stdoutStyles().FlagDesc.Render(help[\"model\"]))\n\tflags.BoolVarP(&config.AskModel, \"ask-model\", \"M\", config.AskModel, stdoutStyles().FlagDesc.Render(help[\"ask-model\"]))\n\tflags.StringVarP(&config.API, \"api\", \"a\", config.API, stdoutStyles().FlagDesc.Render(help[\"api\"]))\n\tflags.StringVarP(&config.HTTPProxy, \"http-proxy\", \"x\", config.HTTPProxy, stdoutStyles().FlagDesc.Render(help[\"http-proxy\"]))\n\tflags.BoolVarP(&config.Format, \"format\", \"f\", config.Format, stdoutStyles().FlagDesc.Render(help[\"format\"]))\n\tflags.StringVar(&config.FormatAs, \"format-as\", config.FormatAs, stdoutStyles().FlagDesc.Render(help[\"format-as\"]))\n\tflags.BoolVarP(&config.Raw, \"raw\", \"r\", config.Raw, stdoutStyles().FlagDesc.Render(help[\"raw\"]))\n\tflags.IntVarP(&config.IncludePrompt, \"prompt\", \"P\", config.IncludePrompt, stdoutStyles().FlagDesc.Render(help[\"prompt\"]))\n\tflags.BoolVarP(&config.IncludePromptArgs, \"prompt-args\", \"p\", config.IncludePromptArgs, stdoutStyles().FlagDesc.Render(help[\"prompt-args\"]))\n\tflags.StringVarP(&config.Continue, \"continue\", \"c\", \"\", stdoutStyles().FlagDesc.Render(help[\"continue\"]))\n\tflags.BoolVarP(&config.ContinueLast, \"continue-last\", \"C\", false, stdoutStyles().FlagDesc.Render(help[\"continue-last\"]))\n\tflags.BoolVarP(&config.List, \"list\", \"l\", config.List, stdoutStyles().FlagDesc.Render(help[\"list\"]))\n\tflags.StringVarP(&config.Title, \"title\", \"t\", config.Title, stdoutStyles().FlagDesc.Render(help[\"title\"]))\n\tflags.StringVarP(&config.Delete, \"delete\", \"d\", config.Delete, stdoutStyles().FlagDesc.Render(help[\"delete\"]))\n\tflags.Var(newDurationFlag(config.DeleteOlderThan, &config.DeleteOlderThan), \"delete-older-than\", stdoutStyles().FlagDesc.Render(help[\"delete-older-than\"]))\n\tflags.StringVarP(&config.Show, \"show\", \"s\", config.Show, stdoutStyles().FlagDesc.Render(help[\"show\"]))\n\tflags.BoolVarP(&config.ShowLast, \"show-last\", \"S\", false, stdoutStyles().FlagDesc.Render(help[\"show-last\"]))\n\tflags.BoolVarP(&config.Quiet, \"quiet\", \"q\", config.Quiet, stdoutStyles().FlagDesc.Render(help[\"quiet\"]))\n\tflags.BoolVarP(&config.ShowHelp, \"help\", \"h\", false, stdoutStyles().FlagDesc.Render(help[\"help\"]))\n\tflags.BoolVarP(&config.Version, \"version\", \"v\", false, stdoutStyles().FlagDesc.Render(help[\"version\"]))\n\tflags.IntVar(&config.MaxRetries, \"max-retries\", config.MaxRetries, stdoutStyles().FlagDesc.Render(help[\"max-retries\"]))\n\tflags.BoolVar(&config.NoLimit, \"no-limit\", config.NoLimit, stdoutStyles().FlagDesc.Render(help[\"no-limit\"]))\n\tflags.IntVar(&config.MaxTokens, \"max-tokens\", config.MaxTokens, stdoutStyles().FlagDesc.Render(help[\"max-tokens\"]))\n\tflags.IntVar(&config.WordWrap, \"word-wrap\", config.WordWrap, stdoutStyles().FlagDesc.Render(help[\"word-wrap\"]))\n\tflags.Float32Var(&config.Temperature, \"temp\", config.Temperature, stdoutStyles().FlagDesc.Render(help[\"temp\"]))\n\tflags.StringArrayVar(&config.Stop, \"stop\", config.Stop, stdoutStyles().FlagDesc.Render(help[\"stop\"]))\n\tflags.Float32Var(&config.TopP, \"topp\", config.TopP, stdoutStyles().FlagDesc.Render(help[\"topp\"]))\n\tflags.IntVar(&config.TopK, \"topk\", config.TopK, stdoutStyles().FlagDesc.Render(help[\"topk\"]))\n\tflags.UintVar(&config.Fanciness, \"fanciness\", config.Fanciness, stdoutStyles().FlagDesc.Render(help[\"fanciness\"]))\n\tflags.StringVar(&config.StatusText, \"status-text\", config.StatusText, stdoutStyles().FlagDesc.Render(help[\"status-text\"]))\n\tflags.BoolVar(&config.NoCache, \"no-cache\", config.NoCache, stdoutStyles().FlagDesc.Render(help[\"no-cache\"]))\n\tflags.BoolVar(&config.ResetSettings, \"reset-settings\", config.ResetSettings, stdoutStyles().FlagDesc.Render(help[\"reset-settings\"]))\n\tflags.BoolVar(&config.Settings, \"settings\", false, stdoutStyles().FlagDesc.Render(help[\"settings\"]))\n\tflags.BoolVar(&config.Dirs, \"dirs\", false, stdoutStyles().FlagDesc.Render(help[\"dirs\"]))\n\tflags.StringVarP(&config.Role, \"role\", \"R\", config.Role, stdoutStyles().FlagDesc.Render(help[\"role\"]))\n\tflags.BoolVar(&config.ListRoles, \"list-roles\", config.ListRoles, stdoutStyles().FlagDesc.Render(help[\"list-roles\"]))\n\tflags.StringVar(&config.Theme, \"theme\", \"charm\", stdoutStyles().FlagDesc.Render(help[\"theme\"]))\n\tflags.Lookup(\"prompt\").NoOptDefVal = \"-1\"\n\tflags.SortFlags = false\n\n\tflags.BoolVar(&memprofile, \"memprofile\", false, \"Write memory profiles to CWD\")\n\t_ = flags.MarkHidden(\"memprofile\")\n\n\tfor _, name := range []string{\"show\", \"delete\", \"continue\"} {\n\t\t_ = rootCmd.RegisterFlagCompletionFunc(name, func(_ *cobra.Command, _ []string, toComplete string) ([]string, cobra.ShellCompDirective) {\n\t\t\tresults, _ := db.Completions(toComplete)\n\t\t\treturn results, cobra.ShellCompDirectiveDefault\n\t\t})\n\t}\n\t_ = rootCmd.RegisterFlagCompletionFunc(\"role\", func(_ *cobra.Command, _ []string, toComplete string) ([]string, cobra.ShellCompDirective) {\n\t\treturn roleNames(toComplete), cobra.ShellCompDirectiveDefault\n\t})\n\n\tif config.FormatText == nil {\n\t\tconfig.FormatText = defaultConfig().FormatText\n\t}\n\n\tif config.Format && config.FormatAs == \"\" {\n\t\tconfig.FormatAs = \"markdown\"\n\t}\n\n\tif config.Format && config.FormatAs != \"\" && config.FormatText[config.FormatAs] == \"\" {\n\t\tconfig.FormatText[config.FormatAs] = defaultConfig().FormatText[config.FormatAs]\n\t}\n\n\trootCmd.MarkFlagsMutuallyExclusive(\n\t\t\"settings\",\n\t\t\"show\",\n\t\t\"show-last\",\n\t\t\"delete\",\n\t\t\"delete-older-than\",\n\t\t\"list\",\n\t\t\"continue\",\n\t\t\"continue-last\",\n\t\t\"reset-settings\",\n\t)\n}\n\nfunc main() {\n\tdefer maybeWriteMemProfile()\n\tvar err error\n\tconfig, err = ensureConfig()\n\tif err != nil {\n\t\thandleError(modsError{err, \"Could not load your configuration file.\"})\n\t\t// if user is editing the settings, only print out the error, but do\n\t\t// not exit.\n\t\tif !slices.Contains(os.Args, \"--settings\") {\n\t\t\tos.Exit(1)\n\t\t}\n\t}\n\n\tcache = newCache(config.CachePath)\n\tdb, err = openDB(filepath.Join(config.CachePath, \"mods.db\"))\n\tif err != nil {\n\t\thandleError(modsError{err, \"Could not open database.\"})\n\t\tos.Exit(1)\n\t}\n\tdefer db.Close() //nolint:errcheck\n\n\t// XXX: this must come after creating the config.\n\tinitFlags()\n\n\tif isCompletionCmd(os.Args) {\n\t\t// XXX: since mods doesn't have any sub-commands, Cobra won't create\n\t\t// the default `completion` command. Forcefully create the completion\n\t\t// related sub-commands by adding a fake command when completions are\n\t\t// being used.\n\t\trootCmd.AddCommand(&cobra.Command{\n\t\t\tUse:    \"____fake_command_to_enable_completions\",\n\t\t\tHidden: true,\n\t\t})\n\t\trootCmd.InitDefaultCompletionCmd()\n\t}\n\n\tif isManCmd(os.Args) {\n\t\trootCmd.AddCommand(&cobra.Command{\n\t\t\tUse:                   \"man\",\n\t\t\tShort:                 \"Generates manpages\",\n\t\t\tSilenceUsage:          true,\n\t\t\tDisableFlagsInUseLine: true,\n\t\t\tHidden:                true,\n\t\t\tArgs:                  cobra.NoArgs,\n\t\t\tRunE: func(*cobra.Command, []string) error {\n\t\t\t\tmanPage, err := mcobra.NewManPage(1, rootCmd)\n\t\t\t\tif err != nil {\n\t\t\t\t\t//nolint:wrapcheck\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\t_, err = fmt.Fprint(os.Stdout, manPage.Build(roff.NewDocument()))\n\t\t\t\t//nolint:wrapcheck\n\t\t\t\treturn err\n\t\t\t},\n\t\t})\n\t}\n\n\tif err := rootCmd.Execute(); err != nil {\n\t\thandleError(err)\n\t\t_ = db.Close()\n\t\tos.Exit(1)\n\t}\n}\n\nfunc maybeWriteMemProfile() {\n\tif !memprofile {\n\t\treturn\n\t}\n\n\tclosers := []func() error{db.Close}\n\tdefer func() {\n\t\tfor _, cl := range closers {\n\t\t\t_ = cl()\n\t\t}\n\t}()\n\n\thandle := func(err error) {\n\t\tfmt.Println(err)\n\t\tfor _, cl := range closers {\n\t\t\t_ = cl()\n\t\t}\n\t\tos.Exit(1)\n\t}\n\n\theap, err := os.Create(\"mods_heap.profile\")\n\tif err != nil {\n\t\thandle(err)\n\t}\n\tclosers = append(closers, heap.Close)\n\tallocs, err := os.Create(\"mods_allocs.profile\")\n\tif err != nil {\n\t\thandle(err)\n\t}\n\tclosers = append(closers, allocs.Close)\n\n\tif err := pprof.Lookup(\"heap\").WriteTo(heap, 0); err != nil {\n\t\thandle(err)\n\t}\n\tif err := pprof.Lookup(\"allocs\").WriteTo(allocs, 0); err != nil {\n\t\thandle(err)\n\t}\n}\n\nfunc handleError(err error) {\n\tmaybeWriteMemProfile()\n\t// exhaust stdin\n\tif !isInputTTY() {\n\t\t_, _ = io.ReadAll(os.Stdin)\n\t}\n\n\tformat := \"\\n%s\\n\\n\"\n\n\tvar args []interface{}\n\tvar ferr flagParseError\n\tvar merr modsError\n\tif errors.As(err, &ferr) {\n\t\tformat += \"%s\\n\\n\"\n\t\targs = []interface{}{\n\t\t\tfmt.Sprintf(\n\t\t\t\t\"Check out %s %s\",\n\t\t\t\tstderrStyles().InlineCode.Render(\"mods -h\"),\n\t\t\t\tstderrStyles().Comment.Render(\"for help.\"),\n\t\t\t),\n\t\t\tfmt.Sprintf(\n\t\t\t\tferr.ReasonFormat(),\n\t\t\t\tstderrStyles().InlineCode.Render(ferr.Flag()),\n\t\t\t),\n\t\t}\n\t} else if errors.As(err, &merr) {\n\t\targs = []interface{}{\n\t\t\tstderrStyles().ErrPadding.Render(stderrStyles().ErrorHeader.String(), merr.reason),\n\t\t}\n\n\t\t// Skip the error details if the user simply canceled out of huh.\n\t\tif merr.err != huh.ErrUserAborted {\n\t\t\tformat += \"%s\\n\\n\"\n\t\t\targs = append(args, stderrStyles().ErrPadding.Render(stderrStyles().ErrorDetails.Render(err.Error())))\n\t\t}\n\t} else {\n\t\targs = []interface{}{\n\t\t\tstderrStyles().ErrPadding.Render(stderrStyles().ErrorDetails.Render(err.Error())),\n\t\t}\n\t}\n\n\tfmt.Fprintf(os.Stderr, format, args...)\n}\n\nfunc resetSettings() error {\n\t_, err := os.Stat(config.SettingsPath)\n\tif err != nil {\n\t\treturn modsError{err, \"Couldn't read config file.\"}\n\t}\n\tinputFile, err := os.Open(config.SettingsPath)\n\tif err != nil {\n\t\treturn modsError{err, \"Couldn't open config file.\"}\n\t}\n\tdefer inputFile.Close() //nolint:errcheck\n\toutputFile, err := os.Create(config.SettingsPath + \".bak\")\n\tif err != nil {\n\t\treturn modsError{err, \"Couldn't backup config file.\"}\n\t}\n\tdefer outputFile.Close() //nolint:errcheck\n\t_, err = io.Copy(outputFile, inputFile)\n\tif err != nil {\n\t\treturn modsError{err, \"Couldn't write config file.\"}\n\t}\n\t// The copy was successful, so now delete the original file\n\terr = os.Remove(config.SettingsPath)\n\tif err != nil {\n\t\treturn modsError{err, \"Couldn't remove config file.\"}\n\t}\n\terr = writeConfigFile(config.SettingsPath)\n\tif err != nil {\n\t\treturn modsError{err, \"Couldn't write new config file.\"}\n\t}\n\tif !config.Quiet {\n\t\tfmt.Fprintln(os.Stderr, \"\\nSettings restored to defaults!\")\n\t\tfmt.Fprintf(os.Stderr,\n\t\t\t\"\\n  %s %s\\n\\n\",\n\t\t\tstderrStyles().Comment.Render(\"Your old settings have been saved to:\"),\n\t\t\tstderrStyles().Link.Render(config.SettingsPath+\".bak\"),\n\t\t)\n\t}\n\treturn nil\n}\n\nfunc deleteConversationOlderThan() error {\n\tconversations, err := db.ListOlderThan(config.DeleteOlderThan)\n\tif err != nil {\n\t\treturn modsError{err, \"Couldn't find conversation to delete.\"}\n\t}\n\n\tif len(conversations) == 0 {\n\t\tif !config.Quiet {\n\t\t\tfmt.Fprintln(os.Stderr, \"No conversations found.\")\n\t\t\treturn nil\n\t\t}\n\t\treturn nil\n\t}\n\n\tif !config.Quiet {\n\t\tprintList(conversations)\n\n\t\tif !isOutputTTY() || !isInputTTY() {\n\t\t\tfmt.Fprintln(os.Stderr)\n\t\t\treturn newUserErrorf(\n\t\t\t\t\"To delete the conversations above, run: %s\",\n\t\t\t\tstrings.Join(append(os.Args, \"--quiet\"), \" \"),\n\t\t\t)\n\t\t}\n\t\tvar confirm bool\n\t\tif err := huh.Run(\n\t\t\thuh.NewConfirm().\n\t\t\t\tTitle(fmt.Sprintf(\"Delete conversations older than %s?\", config.DeleteOlderThan)).\n\t\t\t\tDescription(fmt.Sprintf(\"This will delete all the %d conversations listed above.\", len(conversations))).\n\t\t\t\tValue(&confirm),\n\t\t); err != nil {\n\t\t\treturn modsError{err, \"Couldn't delete old conversations.\"}\n\t\t}\n\t\tif !confirm {\n\t\t\treturn newUserErrorf(\"Aborted by user\")\n\t\t}\n\t}\n\n\tfor _, c := range conversations {\n\t\tif err := db.Delete(c.ID); err != nil {\n\t\t\treturn modsError{err, \"Couldn't delete conversation.\"}\n\t\t}\n\n\t\tif err := cache.delete(c.ID); err != nil {\n\t\t\treturn modsError{err, \"Couldn't delete conversation.\"}\n\t\t}\n\n\t\tif !config.Quiet {\n\t\t\tfmt.Fprintln(os.Stderr, \"Conversation deleted:\", c.ID[:sha1minLen])\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc deleteConversation() error {\n\tconvo, err := db.Find(config.Delete)\n\tif err != nil {\n\t\treturn modsError{err, \"Couldn't find conversation to delete.\"}\n\t}\n\n\tif err := db.Delete(convo.ID); err != nil {\n\t\treturn modsError{err, \"Couldn't delete conversation.\"}\n\t}\n\n\tif err := cache.delete(convo.ID); err != nil {\n\t\treturn modsError{err, \"Couldn't delete conversation.\"}\n\t}\n\n\tif !config.Quiet {\n\t\tfmt.Fprintln(os.Stderr, \"Conversation deleted:\", convo.ID[:sha1minLen])\n\t}\n\treturn nil\n}\n\nfunc listConversations() error {\n\tconversations, err := db.List()\n\tif err != nil {\n\t\treturn modsError{err, \"Couldn't list saves.\"}\n\t}\n\n\tif len(conversations) == 0 {\n\t\tfmt.Fprintln(os.Stderr, \"No conversations found.\")\n\t\treturn nil\n\t}\n\n\tif isInputTTY() && isOutputTTY() {\n\t\tselectFromList(conversations)\n\t\treturn nil\n\t}\n\tprintList(conversations)\n\treturn nil\n}\n\nfunc roleNames(prefix string) []string {\n\troles := make([]string, 0, len(config.Roles))\n\tfor role := range config.Roles {\n\t\tif prefix != \"\" && !strings.HasPrefix(role, prefix) {\n\t\t\tcontinue\n\t\t}\n\t\troles = append(roles, role)\n\t}\n\tslices.Sort(roles)\n\treturn roles\n}\n\nfunc listRoles() error {\n\tfor _, role := range roleNames(\"\") {\n\t\ts := role\n\t\tif role == config.Role {\n\t\t\ts = role + stdoutStyles().Timeago.Render(\" (default)\")\n\t\t}\n\t\tfmt.Println(s)\n\t}\n\treturn nil\n}\n\nfunc makeOptions(conversations []Conversation) []huh.Option[string] {\n\topts := make([]huh.Option[string], 0, len(conversations))\n\tfor _, c := range conversations {\n\t\ttimea := stdoutStyles().Timeago.Render(timeago.Of(c.UpdatedAt))\n\t\tleft := stdoutStyles().SHA1.Render(c.ID[:sha1short])\n\t\tright := stdoutStyles().ConversationList.Render(c.Title, timea)\n\t\tif c.Model != nil {\n\t\t\tright += stdoutStyles().Comment.Render(*c.Model)\n\t\t}\n\t\topts = append(opts, huh.NewOption(left+\" \"+right, c.ID))\n\t}\n\treturn opts\n}\n\nfunc selectFromList(conversations []Conversation) {\n\tvar selected string\n\tif err := huh.NewForm(\n\t\thuh.NewGroup(\n\t\t\thuh.NewSelect[string]().\n\t\t\t\tTitle(\"Conversations\").\n\t\t\t\tValue(&selected).\n\t\t\t\tOptions(makeOptions(conversations)...),\n\t\t),\n\t).Run(); err != nil {\n\t\tif !errors.Is(err, huh.ErrUserAborted) {\n\t\t\tfmt.Fprintln(os.Stderr, err.Error())\n\t\t}\n\t\treturn\n\t}\n\n\t_ = clipboard.WriteAll(selected)\n\ttermenv.Copy(selected)\n\tprintConfirmation(\"COPIED\", selected)\n\t// suggest actions to use this conversation ID\n\tfmt.Println(stdoutStyles().Comment.Render(\n\t\t\"You can use this conversation ID with the following commands:\",\n\t))\n\tsuggestions := []string{\"show\", \"continue\", \"delete\"}\n\tfor _, flag := range suggestions {\n\t\tfmt.Printf(\n\t\t\t\"  %-44s %s\\n\",\n\t\t\tstdoutStyles().Flag.Render(\"--\"+flag),\n\t\t\tstdoutStyles().FlagDesc.Render(help[flag]),\n\t\t)\n\t}\n}\n\nfunc printList(conversations []Conversation) {\n\tfor _, conversation := range conversations {\n\t\t_, _ = fmt.Fprintf(\n\t\t\tos.Stdout,\n\t\t\t\"%s\\t%s\\t%s\\n\",\n\t\t\tstdoutStyles().SHA1.Render(conversation.ID[:sha1short]),\n\t\t\tconversation.Title,\n\t\t\tstdoutStyles().Timeago.Render(timeago.Of(conversation.UpdatedAt)),\n\t\t)\n\t}\n}\n\nfunc saveConversation(mods *Mods) error {\n\tif config.NoCache {\n\t\tif !config.Quiet {\n\t\t\tfmt.Fprintf(\n\t\t\t\tos.Stderr,\n\t\t\t\t\"\\nConversation was not saved because %s or %s is set.\\n\",\n\t\t\t\tstderrStyles().InlineCode.Render(\"--no-cache\"),\n\t\t\t\tstderrStyles().InlineCode.Render(\"NO_CACHE\"),\n\t\t\t)\n\t\t}\n\t\treturn nil\n\t}\n\n\t// if message is a sha1, use the last prompt instead.\n\tid := config.cacheWriteToID\n\ttitle := strings.TrimSpace(config.cacheWriteToTitle)\n\n\tif sha1reg.MatchString(title) || title == \"\" {\n\t\ttitle = firstLine(lastPrompt(mods.messages))\n\t}\n\n\tif err := cache.write(id, &mods.messages); err != nil {\n\t\treturn modsError{err, fmt.Sprintf(\n\t\t\t\"There was a problem writing %s to the cache. Use %s / %s to disable it.\",\n\t\t\tconfig.cacheWriteToID,\n\t\t\tstderrStyles().InlineCode.Render(\"--no-cache\"),\n\t\t\tstderrStyles().InlineCode.Render(\"NO_CACHE\"),\n\t\t)}\n\t}\n\tif err := db.Save(id, title, config.Model); err != nil {\n\t\t_ = cache.delete(id) // remove leftovers\n\t\treturn modsError{err, fmt.Sprintf(\n\t\t\t\"There was a problem writing %s to the cache. Use %s / %s to disable it.\",\n\t\t\tconfig.cacheWriteToID,\n\t\t\tstderrStyles().InlineCode.Render(\"--no-cache\"),\n\t\t\tstderrStyles().InlineCode.Render(\"NO_CACHE\"),\n\t\t)}\n\t}\n\n\tif !config.Quiet {\n\t\tfmt.Fprintln(\n\t\t\tos.Stderr,\n\t\t\t\"\\nConversation saved:\",\n\t\t\tstderrStyles().InlineCode.Render(config.cacheWriteToID[:sha1short]),\n\t\t\tstderrStyles().Comment.Render(title),\n\t\t)\n\t}\n\treturn nil\n}\n\nfunc isNoArgs() bool {\n\treturn config.Prefix == \"\" &&\n\t\tconfig.Show == \"\" &&\n\t\t!config.ShowLast &&\n\t\tconfig.Delete == \"\" &&\n\t\tconfig.DeleteOlderThan == 0 &&\n\t\t!config.ShowHelp &&\n\t\t!config.List &&\n\t\t!config.ListRoles &&\n\t\t!config.Dirs &&\n\t\t!config.Settings &&\n\t\t!config.ResetSettings\n}\n\nfunc askInfo() error {\n\tapis := make([]huh.Option[string], 0, len(config.APIs))\n\topts := map[string][]huh.Option[string]{}\n\tfor _, api := range config.APIs {\n\t\tapis = append(apis, huh.NewOption(api.Name, api.Name))\n\t\tfor model := range api.Models {\n\t\t\topts[api.Name] = append(opts[api.Name], huh.NewOption(model, model))\n\t\t}\n\t}\n\n\tif config.ContinueLast {\n\t\tfound, err := db.FindHEAD()\n\t\tif err == nil && found != nil && found.Model != nil {\n\t\t\tconfig.Model = *found.Model\n\t\t}\n\t}\n\n\t// wrapping is done by the caller\n\t//nolint:wrapcheck\n\treturn huh.NewForm(\n\t\thuh.NewGroup(\n\t\t\thuh.NewSelect[string]().\n\t\t\t\tTitle(\"Choose the API:\").\n\t\t\t\tOptions(apis...).\n\t\t\t\tValue(&config.API),\n\t\t\thuh.NewSelect[string]().\n\t\t\t\tTitleFunc(func() string {\n\t\t\t\t\treturn fmt.Sprintf(\"Choose the model for '%s':\", config.API)\n\t\t\t\t}, &config.API).\n\t\t\t\tOptionsFunc(func() []huh.Option[string] {\n\t\t\t\t\treturn opts[config.API]\n\t\t\t\t}, &config.API).\n\t\t\t\tValue(&config.Model),\n\t\t).WithHideFunc(func() bool {\n\t\t\treturn !config.AskModel\n\t\t}),\n\t\thuh.NewGroup(\n\t\t\thuh.NewText().\n\t\t\t\tTitleFunc(func() string {\n\t\t\t\t\treturn fmt.Sprintf(\"Enter a prompt for %s:\", config.Model)\n\t\t\t\t}, &config.Model).\n\t\t\t\tValue(&config.Prefix),\n\t\t).WithHideFunc(func() bool {\n\t\t\treturn config.Prefix != \"\"\n\t\t}),\n\t).\n\t\tWithTheme(themeFrom(config.Theme)).\n\t\tRun()\n}\n\n//nolint:mnd\nfunc isManCmd(args []string) bool {\n\tif len(args) == 2 {\n\t\treturn args[1] == \"man\"\n\t}\n\tif len(args) == 3 && args[1] == \"man\" {\n\t\treturn args[2] == \"-h\" || args[2] == \"--help\"\n\t}\n\treturn false\n}\n\n//nolint:mnd\nfunc isCompletionCmd(args []string) bool {\n\tif len(args) <= 1 {\n\t\treturn false\n\t}\n\tif args[1] == \"__complete\" {\n\t\treturn true\n\t}\n\tif args[1] != \"completion\" {\n\t\treturn false\n\t}\n\tif len(args) == 3 {\n\t\t_, ok := map[string]any{\n\t\t\t\"bash\":       nil,\n\t\t\t\"fish\":       nil,\n\t\t\t\"zsh\":        nil,\n\t\t\t\"powershell\": nil,\n\t\t\t\"-h\":         nil,\n\t\t\t\"--help\":     nil,\n\t\t\t\"help\":       nil,\n\t\t}[args[2]]\n\t\treturn ok\n\t}\n\tif len(args) == 4 {\n\t\t_, ok := map[string]any{\n\t\t\t\"-h\":     nil,\n\t\t\t\"--help\": nil,\n\t\t}[args[3]]\n\t\treturn ok\n\t}\n\treturn false\n}\n\nfunc themeFrom(theme string) *huh.Theme {\n\tswitch theme {\n\tcase \"dracula\":\n\t\treturn huh.ThemeDracula()\n\tcase \"catppuccin\":\n\t\treturn huh.ThemeCatppuccin()\n\tcase \"base16\":\n\t\treturn huh.ThemeBase16()\n\tdefault:\n\t\treturn huh.ThemeCharm()\n\t}\n}\n"
        },
        {
          "name": "main_test.go",
          "type": "blob",
          "size": 1.9765625,
          "content": "package main\n\nimport (\n\t\"strings\"\n\t\"testing\"\n)\n\nfunc TestIsCompletionCmd(t *testing.T) {\n\tfor args, is := range map[string]bool{\n\t\t\"\":                                     false,\n\t\t\"something\":                            false,\n\t\t\"something something\":                  false,\n\t\t\"completion for my bash script how to\": false,\n\t\t\"completion bash how to\":               false,\n\t\t\"completion\":                           false,\n\t\t\"completion -h\":                        true,\n\t\t\"completion --help\":                    true,\n\t\t\"completion help\":                      true,\n\t\t\"completion bash\":                      true,\n\t\t\"completion fish\":                      true,\n\t\t\"completion zsh\":                       true,\n\t\t\"completion powershell\":                true,\n\t\t\"completion bash -h\":                   true,\n\t\t\"completion fish -h\":                   true,\n\t\t\"completion zsh -h\":                    true,\n\t\t\"completion powershell -h\":             true,\n\t\t\"completion bash --help\":               true,\n\t\t\"completion fish --help\":               true,\n\t\t\"completion zsh --help\":                true,\n\t\t\"completion powershell --help\":         true,\n\t\t\"__complete\":                           true,\n\t\t\"__complete blah blah blah\":            true,\n\t} {\n\t\tt.Run(args, func(t *testing.T) {\n\t\t\tvargs := append([]string{\"mods\"}, strings.Fields(args)...)\n\t\t\tif b := isCompletionCmd(vargs); b != is {\n\t\t\t\tt.Errorf(\"%v: expected %v, got %v\", vargs, is, b)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestIsManCmd(t *testing.T) {\n\tfor args, is := range map[string]bool{\n\t\t\"\":                    false,\n\t\t\"something\":           false,\n\t\t\"something something\": false,\n\t\t\"man is no more\":      false,\n\t\t\"mans\":                false,\n\t\t\"man foo\":             false,\n\t\t\"man\":                 true,\n\t\t\"man -h\":              true,\n\t\t\"man --help\":          true,\n\t} {\n\t\tt.Run(args, func(t *testing.T) {\n\t\t\tvargs := append([]string{\"mods\"}, strings.Fields(args)...)\n\t\t\tif b := isManCmd(vargs); b != is {\n\t\t\t\tt.Errorf(\"%v: expected %v, got %v\", vargs, is, b)\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "messages.go",
          "type": "blob",
          "size": 0.3828125,
          "content": "package main\n\nimport (\n\t\"strings\"\n\n\topenai \"github.com/sashabaranov/go-openai\"\n)\n\nfunc lastPrompt(messages []openai.ChatCompletionMessage) string {\n\tvar result string\n\tfor _, msg := range messages {\n\t\tif msg.Role != openai.ChatMessageRoleUser {\n\t\t\tcontinue\n\t\t}\n\t\tresult = msg.Content\n\t}\n\treturn result\n}\n\nfunc firstLine(s string) string {\n\tfirst, _, _ := strings.Cut(s, \"\\n\")\n\treturn first\n}\n"
        },
        {
          "name": "messages_test.go",
          "type": "blob",
          "size": 1.3076171875,
          "content": "package main\n\nimport (\n\t\"testing\"\n\n\t\"github.com/sashabaranov/go-openai\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestLastPrompt(t *testing.T) {\n\tt.Run(\"no prompt\", func(t *testing.T) {\n\t\trequire.Equal(t, \"\", lastPrompt(nil))\n\t})\n\n\tt.Run(\"single prompt\", func(t *testing.T) {\n\t\trequire.Equal(t, \"single\", lastPrompt([]openai.ChatCompletionMessage{\n\t\t\t{\n\t\t\t\tRole:    openai.ChatMessageRoleUser,\n\t\t\t\tContent: \"single\",\n\t\t\t},\n\t\t}))\n\t})\n\n\tt.Run(\"multiple prompts\", func(t *testing.T) {\n\t\trequire.Equal(t, \"last\", lastPrompt([]openai.ChatCompletionMessage{\n\t\t\t{\n\t\t\t\tRole:    openai.ChatMessageRoleUser,\n\t\t\t\tContent: \"first\",\n\t\t\t},\n\t\t\t{\n\t\t\t\tRole:    openai.ChatMessageRoleAssistant,\n\t\t\t\tContent: \"hallo\",\n\t\t\t},\n\t\t\t{\n\t\t\t\tRole:    openai.ChatMessageRoleUser,\n\t\t\t\tContent: \"middle 1\",\n\t\t\t},\n\t\t\t{\n\t\t\t\tRole:    openai.ChatMessageRoleUser,\n\t\t\t\tContent: \"middle 2\",\n\t\t\t},\n\t\t\t{\n\t\t\t\tRole:    openai.ChatMessageRoleUser,\n\t\t\t\tContent: \"last\",\n\t\t\t},\n\t\t}))\n\t})\n}\n\nfunc TestFirstLine(t *testing.T) {\n\tt.Run(\"single line\", func(t *testing.T) {\n\t\trequire.Equal(t, \"line\", firstLine(\"line\"))\n\t})\n\tt.Run(\"single line ending with \\n\", func(t *testing.T) {\n\t\trequire.Equal(t, \"line\", firstLine(\"line\\n\"))\n\t})\n\tt.Run(\"multiple lines\", func(t *testing.T) {\n\t\trequire.Equal(t, \"line\", firstLine(\"line\\nsomething else\\nline3\\nfoo\\nends with a double \\n\\n\"))\n\t})\n}\n"
        },
        {
          "name": "mods.go",
          "type": "blob",
          "size": 18.41796875,
          "content": "package main\n\nimport (\n\t\"bufio\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"os\"\n\t\"os/exec\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\t\"unicode\"\n\n\t\"github.com/caarlos0/go-shellwords\"\n\t\"github.com/charmbracelet/bubbles/viewport\"\n\ttea \"github.com/charmbracelet/bubbletea\"\n\t\"github.com/charmbracelet/glamour\"\n\t\"github.com/charmbracelet/lipgloss\"\n\t\"github.com/charmbracelet/x/exp/ordered\"\n\topenai \"github.com/sashabaranov/go-openai\"\n)\n\ntype state int\n\nconst (\n\tstartState state = iota\n\tconfigLoadedState\n\trequestState\n\tresponseState\n\tdoneState\n\terrorState\n)\n\n// Mods is the Bubble Tea model that manages reading stdin and querying the\n// OpenAI API.\ntype Mods struct {\n\tOutput        string\n\tInput         string\n\tStyles        styles\n\tError         *modsError\n\tstate         state\n\tretries       int\n\tsystem        string\n\trenderer      *lipgloss.Renderer\n\tglam          *glamour.TermRenderer\n\tglamViewport  viewport.Model\n\tglamOutput    string\n\tglamHeight    int\n\tmessages      []openai.ChatCompletionMessage\n\tcancelRequest context.CancelFunc\n\tanim          tea.Model\n\twidth         int\n\theight        int\n\n\tdb     *convoDB\n\tcache  *convoCache\n\tConfig *Config\n\n\tcontent      []string\n\tcontentMutex *sync.Mutex\n}\n\nfunc newMods(r *lipgloss.Renderer, cfg *Config, db *convoDB, cache *convoCache) *Mods {\n\tgr, _ := glamour.NewTermRenderer(glamour.WithEnvironmentConfig(), glamour.WithWordWrap(cfg.WordWrap))\n\tvp := viewport.New(0, 0)\n\tvp.GotoBottom()\n\treturn &Mods{\n\t\tStyles:       makeStyles(r),\n\t\tglam:         gr,\n\t\tstate:        startState,\n\t\trenderer:     r,\n\t\tglamViewport: vp,\n\t\tcontentMutex: &sync.Mutex{},\n\t\tdb:           db,\n\t\tcache:        cache,\n\t\tConfig:       cfg,\n\t}\n}\n\n// completionInput is a tea.Msg that wraps the content read from stdin.\ntype completionInput struct {\n\tcontent string\n}\n\n// completionOutput a tea.Msg that wraps the content returned from openai.\ntype completionOutput struct {\n\tcontent string\n\tstream  chatCompletionReceiver\n}\n\ntype chatCompletionReceiver interface {\n\tRecv() (openai.ChatCompletionStreamResponse, error)\n\tClose() error\n}\n\n// Init implements tea.Model.\nfunc (m *Mods) Init() tea.Cmd {\n\treturn m.findCacheOpsDetails()\n}\n\n// Update implements tea.Model.\nfunc (m *Mods) Update(msg tea.Msg) (tea.Model, tea.Cmd) {\n\tvar cmds []tea.Cmd\n\tswitch msg := msg.(type) {\n\tcase cacheDetailsMsg:\n\t\tm.Config.cacheWriteToID = msg.WriteID\n\t\tm.Config.cacheWriteToTitle = msg.Title\n\t\tm.Config.cacheReadFromID = msg.ReadID\n\t\tm.Config.Model = msg.Model\n\n\t\tif !m.Config.Quiet {\n\t\t\tm.anim = newAnim(m.Config.Fanciness, m.Config.StatusText, m.renderer, m.Styles)\n\t\t\tcmds = append(cmds, m.anim.Init())\n\t\t}\n\t\tm.state = configLoadedState\n\t\tcmds = append(cmds, m.readStdinCmd)\n\n\tcase completionInput:\n\t\tif msg.content != \"\" {\n\t\t\tm.Input = removeWhitespace(msg.content)\n\t\t}\n\t\tif m.Input == \"\" && m.Config.Prefix == \"\" && m.Config.Show == \"\" && !m.Config.ShowLast {\n\t\t\treturn m, m.quit\n\t\t}\n\t\tif m.Config.Dirs ||\n\t\t\tm.Config.Delete != \"\" ||\n\t\t\tm.Config.DeleteOlderThan != 0 ||\n\t\t\tm.Config.ShowHelp ||\n\t\t\tm.Config.List ||\n\t\t\tm.Config.ListRoles ||\n\t\t\tm.Config.Settings ||\n\t\t\tm.Config.ResetSettings {\n\t\t\treturn m, m.quit\n\t\t}\n\n\t\tif m.Config.IncludePromptArgs {\n\t\t\tm.appendToOutput(m.Config.Prefix + \"\\n\\n\")\n\t\t}\n\n\t\tif m.Config.IncludePrompt > 0 {\n\t\t\tparts := strings.Split(m.Input, \"\\n\")\n\t\t\tif len(parts) > m.Config.IncludePrompt {\n\t\t\t\tparts = parts[0:m.Config.IncludePrompt]\n\t\t\t}\n\t\t\tm.appendToOutput(strings.Join(parts, \"\\n\") + \"\\n\")\n\t\t}\n\t\tm.state = requestState\n\t\tcmds = append(cmds, m.startCompletionCmd(msg.content))\n\tcase completionOutput:\n\t\tif msg.stream == nil {\n\t\t\tm.state = doneState\n\t\t\treturn m, m.quit\n\t\t}\n\t\tif msg.content != \"\" {\n\t\t\tm.appendToOutput(msg.content)\n\t\t\tm.state = responseState\n\t\t}\n\t\tcmds = append(cmds, m.receiveCompletionStreamCmd(msg))\n\tcase modsError:\n\t\tm.Error = &msg\n\t\tm.state = errorState\n\t\treturn m, m.quit\n\tcase tea.WindowSizeMsg:\n\t\tm.width, m.height = msg.Width, msg.Height\n\t\tm.glamViewport.Width = m.width\n\t\tm.glamViewport.Height = m.height\n\t\treturn m, nil\n\tcase tea.KeyMsg:\n\t\tswitch msg.String() {\n\t\tcase \"q\", \"ctrl+c\":\n\t\t\tm.state = doneState\n\t\t\treturn m, m.quit\n\t\t}\n\t}\n\tif !m.Config.Quiet && (m.state == configLoadedState || m.state == requestState) {\n\t\tvar cmd tea.Cmd\n\t\tm.anim, cmd = m.anim.Update(msg)\n\t\tcmds = append(cmds, cmd)\n\t}\n\tif m.viewportNeeded() {\n\t\t// Only respond to keypresses when the viewport (i.e. the content) is\n\t\t// taller than the window.\n\t\tvar cmd tea.Cmd\n\t\tm.glamViewport, cmd = m.glamViewport.Update(msg)\n\t\tcmds = append(cmds, cmd)\n\t}\n\treturn m, tea.Batch(cmds...)\n}\n\nfunc (m Mods) viewportNeeded() bool {\n\treturn m.glamHeight > m.height\n}\n\n// View implements tea.Model.\nfunc (m *Mods) View() string {\n\t//nolint:exhaustive\n\tswitch m.state {\n\tcase errorState:\n\t\treturn \"\"\n\tcase requestState:\n\t\tif !m.Config.Quiet {\n\t\t\treturn m.anim.View()\n\t\t}\n\tcase responseState:\n\t\tif !m.Config.Raw && isOutputTTY() {\n\t\t\tif m.viewportNeeded() {\n\t\t\t\treturn m.glamViewport.View()\n\t\t\t}\n\t\t\t// We don't need the viewport yet.\n\t\t\treturn m.glamOutput\n\t\t}\n\n\t\tif isOutputTTY() && !m.Config.Raw {\n\t\t\treturn m.Output\n\t\t}\n\n\t\tm.contentMutex.Lock()\n\t\tfor _, c := range m.content {\n\t\t\tfmt.Print(c)\n\t\t}\n\t\tm.content = []string{}\n\t\tm.contentMutex.Unlock()\n\tcase doneState:\n\t\tif !isOutputTTY() {\n\t\t\tfmt.Printf(\"\\n\")\n\t\t}\n\t\treturn \"\"\n\t}\n\treturn \"\"\n}\n\nfunc (m *Mods) quit() tea.Msg {\n\tif m.cancelRequest != nil {\n\t\tm.cancelRequest()\n\t}\n\treturn tea.Quit()\n}\n\nfunc (m *Mods) retry(content string, err modsError) tea.Msg {\n\tm.retries++\n\tif m.retries >= m.Config.MaxRetries {\n\t\treturn err\n\t}\n\twait := time.Millisecond * 100 * time.Duration(math.Pow(2, float64(m.retries))) //nolint:mnd\n\ttime.Sleep(wait)\n\treturn completionInput{content}\n}\n\nfunc (m *Mods) startCompletionCmd(content string) tea.Cmd {\n\tif m.Config.Show != \"\" || m.Config.ShowLast {\n\t\treturn m.readFromCache()\n\t}\n\n\treturn func() tea.Msg {\n\t\tvar ok bool\n\t\tvar mod Model\n\t\tvar api API\n\t\tvar ccfg openai.ClientConfig\n\t\tvar accfg AnthropicClientConfig\n\t\tvar cccfg CohereClientConfig\n\t\tvar occfg OllamaClientConfig\n\t\tvar gccfg GoogleClientConfig\n\n\t\tcfg := m.Config\n\t\tmod, ok = cfg.Models[cfg.Model]\n\t\tif !ok {\n\t\t\tif cfg.API == \"\" {\n\t\t\t\treturn modsError{\n\t\t\t\t\treason: fmt.Sprintf(\n\t\t\t\t\t\t\"Model %s is not in the settings file.\",\n\t\t\t\t\t\tm.Styles.InlineCode.Render(cfg.Model),\n\t\t\t\t\t),\n\t\t\t\t\terr: newUserErrorf(\n\t\t\t\t\t\t\"Please specify an API endpoint with %s or configure the model in the settings: %s\",\n\t\t\t\t\t\tm.Styles.InlineCode.Render(\"--api\"),\n\t\t\t\t\t\tm.Styles.InlineCode.Render(\"mods -s\"),\n\t\t\t\t\t),\n\t\t\t\t}\n\t\t\t}\n\t\t\tmod.Name = cfg.Model\n\t\t\tmod.API = cfg.API\n\t\t\tmod.MaxChars = cfg.MaxInputChars\n\t\t}\n\t\tfor _, a := range cfg.APIs {\n\t\t\tif mod.API == a.Name {\n\t\t\t\tapi = a\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif api.Name == \"\" {\n\t\t\teps := make([]string, 0)\n\t\t\tfor _, a := range cfg.APIs {\n\t\t\t\teps = append(eps, m.Styles.InlineCode.Render(a.Name))\n\t\t\t}\n\t\t\treturn modsError{\n\t\t\t\terr: newUserErrorf(\n\t\t\t\t\t\"Your configured API endpoints are: %s\",\n\t\t\t\t\teps,\n\t\t\t\t),\n\t\t\t\treason: fmt.Sprintf(\n\t\t\t\t\t\"The API endpoint %s is not configured.\",\n\t\t\t\t\tm.Styles.InlineCode.Render(cfg.API),\n\t\t\t\t),\n\t\t\t}\n\t\t}\n\n\t\tswitch mod.API {\n\t\tcase \"ollama\":\n\t\t\toccfg = DefaultOllamaConfig()\n\t\t\tif api.BaseURL != \"\" {\n\t\t\t\toccfg.BaseURL = api.BaseURL\n\t\t\t}\n\t\tcase \"anthropic\":\n\t\t\tkey, err := m.ensureKey(api, \"ANTHROPIC_API_KEY\", \"https://console.anthropic.com/settings/keys\")\n\t\t\tif err != nil {\n\t\t\t\treturn modsError{err, \"Anthropic authentication failed\"}\n\t\t\t}\n\t\t\taccfg = DefaultAnthropicConfig(key)\n\t\t\tif api.BaseURL != \"\" {\n\t\t\t\taccfg.BaseURL = api.BaseURL\n\t\t\t}\n\t\t\tif api.Version != \"\" {\n\t\t\t\taccfg.Version = AnthropicAPIVersion(api.Version)\n\t\t\t}\n\t\tcase \"google\":\n\t\t\tkey, err := m.ensureKey(api, \"GOOGLE_API_KEY\", \"https://aistudio.google.com/app/apikey\")\n\t\t\tif err != nil {\n\t\t\t\treturn modsError{err, \"Google authentication failed\"}\n\t\t\t}\n\t\t\tgccfg = DefaultGoogleConfig(mod.Name, key)\n\t\tcase \"cohere\":\n\t\t\tkey, err := m.ensureKey(api, \"COHERE_API_KEY\", \"https://dashboard.cohere.com/api-keys\")\n\t\t\tif err != nil {\n\t\t\t\treturn modsError{err, \"Cohere authentication failed\"}\n\t\t\t}\n\t\t\tcccfg = DefaultCohereConfig(key)\n\t\t\tif api.BaseURL != \"\" {\n\t\t\t\tccfg.BaseURL = api.BaseURL\n\t\t\t}\n\t\tcase \"azure\", \"azure-ad\":\n\t\t\tkey, err := m.ensureKey(api, \"AZURE_OPENAI_KEY\", \"https://aka.ms/oai/access\")\n\t\t\tif err != nil {\n\t\t\t\treturn modsError{err, \"Azure authentication failed\"}\n\t\t\t}\n\t\t\tccfg = openai.DefaultAzureConfig(key, api.BaseURL)\n\t\t\tif mod.API == \"azure-ad\" {\n\t\t\t\tccfg.APIType = openai.APITypeAzureAD\n\t\t\t}\n\t\t\tif api.User != \"\" {\n\t\t\t\tcfg.User = api.User\n\t\t\t}\n\t\tcase \"copilot\":\n\t\t\tghCopilotHTTPClient := newCopilotHTTPClient()\n\t\t\taccessToken, err := getCopilotAccessToken(ghCopilotHTTPClient.client)\n\t\t\tif err != nil {\n\t\t\t\treturn modsError{err, \"Copilot authentication failed\"}\n\t\t\t}\n\n\t\t\tccfg = openai.DefaultConfig(accessToken.Token)\n\t\t\tccfg.HTTPClient = ghCopilotHTTPClient\n\t\t\tccfg.HTTPClient.(*copilotHTTPClient).AccessToken = &accessToken\n\t\t\tccfg.BaseURL = ordered.First(api.BaseURL, accessToken.Endpoints.API)\n\n\t\tdefault:\n\t\t\tkey, err := m.ensureKey(api, \"OPENAI_API_KEY\", \"https://platform.openai.com/account/api-keys\")\n\t\t\tif err != nil {\n\t\t\t\treturn modsError{err, \"OpenAI authentication failed\"}\n\t\t\t}\n\t\t\tccfg = openai.DefaultConfig(key)\n\t\t\tif api.BaseURL != \"\" {\n\t\t\t\tccfg.BaseURL = api.BaseURL\n\t\t\t}\n\t\t}\n\n\t\tif cfg.HTTPProxy != \"\" {\n\t\t\tproxyURL, err := url.Parse(cfg.HTTPProxy)\n\t\t\tif err != nil {\n\t\t\t\treturn modsError{err, \"There was an error parsing your proxy URL.\"}\n\t\t\t}\n\t\t\thttpClient := &http.Client{Transport: &http.Transport{Proxy: http.ProxyURL(proxyURL)}}\n\t\t\tccfg.HTTPClient = httpClient\n\t\t\taccfg.HTTPClient = httpClient\n\t\t\tcccfg.HTTPClient = httpClient\n\t\t\toccfg.HTTPClient = httpClient\n\t\t}\n\n\t\tif mod.MaxChars == 0 {\n\t\t\tmod.MaxChars = cfg.MaxInputChars\n\t\t}\n\n\t\tswitch mod.API {\n\t\tcase \"anthropic\":\n\t\t\treturn m.createAnthropicStream(content, accfg, mod)\n\t\tcase \"google\":\n\t\t\treturn m.createGoogleStream(content, gccfg, mod)\n\t\tcase \"cohere\":\n\t\t\treturn m.createCohereStream(content, cccfg, mod)\n\t\tcase \"ollama\":\n\t\t\treturn m.createOllamaStream(content, occfg, mod)\n\t\tdefault:\n\t\t\treturn m.createOpenAIStream(content, ccfg, mod)\n\t\t}\n\t}\n}\n\nfunc (m Mods) ensureKey(api API, defaultEnv, docsURL string) (string, error) {\n\tkey := api.APIKey\n\tif key == \"\" && api.APIKeyEnv != \"\" && api.APIKeyCmd == \"\" {\n\t\tkey = os.Getenv(api.APIKeyEnv)\n\t}\n\tif key == \"\" && api.APIKeyCmd != \"\" {\n\t\targs, err := shellwords.Parse(api.APIKeyCmd)\n\t\tif err != nil {\n\t\t\treturn \"\", modsError{err, \"Failed to parse api-key-cmd\"}\n\t\t}\n\t\tout, err := exec.Command(args[0], args[1:]...).CombinedOutput() //nolint:gosec\n\t\tif err != nil {\n\t\t\treturn \"\", modsError{err, \"Cannot exec api-key-cmd\"}\n\t\t}\n\t\tkey = strings.TrimSpace(string(out))\n\t}\n\tif key == \"\" {\n\t\tkey = os.Getenv(defaultEnv)\n\t}\n\tif key != \"\" {\n\t\treturn key, nil\n\t}\n\treturn \"\", modsError{\n\t\treason: fmt.Sprintf(\n\t\t\t\"%[1]s required; set the environment variable %[1]s or update %[2]s through %[3]s.\",\n\t\t\tm.Styles.InlineCode.Render(defaultEnv),\n\t\t\tm.Styles.InlineCode.Render(\"mods.yaml\"),\n\t\t\tm.Styles.InlineCode.Render(\"mods --settings\"),\n\t\t),\n\t\terr: newUserErrorf(\n\t\t\t\"You can grab one at %s.\",\n\t\t\tm.Styles.Link.Render(docsURL),\n\t\t),\n\t}\n}\n\nfunc (m *Mods) handleRequestError(err error, mod Model, content string) tea.Msg {\n\tae := &openai.APIError{}\n\tif errors.As(err, &ae) {\n\t\treturn m.handleAPIError(ae, mod, content)\n\t}\n\treturn modsError{err, fmt.Sprintf(\n\t\t\"There was a problem with the %s API request.\",\n\t\tmod.API,\n\t)}\n}\n\nfunc (m *Mods) handleAPIError(err *openai.APIError, mod Model, content string) tea.Msg {\n\tcfg := m.Config\n\tswitch err.HTTPStatusCode {\n\tcase http.StatusNotFound:\n\t\tif mod.Fallback != \"\" {\n\t\t\tm.Config.Model = mod.Fallback\n\t\t\treturn m.retry(content, modsError{\n\t\t\t\terr:    err,\n\t\t\t\treason: fmt.Sprintf(\"%s API server error.\", mod.API),\n\t\t\t})\n\t\t}\n\t\treturn modsError{err: err, reason: fmt.Sprintf(\n\t\t\t\"Missing model '%s' for API '%s'.\",\n\t\t\tcfg.Model,\n\t\t\tcfg.API,\n\t\t)}\n\tcase http.StatusBadRequest:\n\t\tif err.Code == \"context_length_exceeded\" {\n\t\t\tpe := modsError{err: err, reason: \"Maximum prompt size exceeded.\"}\n\t\t\tif cfg.NoLimit {\n\t\t\t\treturn pe\n\t\t\t}\n\n\t\t\treturn m.retry(cutPrompt(err.Message, content), pe)\n\t\t}\n\t\t// bad request (do not retry)\n\t\treturn modsError{err: err, reason: fmt.Sprintf(\"%s API request error.\", mod.API)}\n\tcase http.StatusUnauthorized:\n\t\t// invalid auth or key (do not retry)\n\t\treturn modsError{err: err, reason: fmt.Sprintf(\"Invalid %s API key.\", mod.API)}\n\tcase http.StatusTooManyRequests:\n\t\t// rate limiting or engine overload (wait and retry)\n\t\treturn m.retry(content, modsError{\n\t\t\terr: err, reason: fmt.Sprintf(\"You’ve hit your %s API rate limit.\", mod.API),\n\t\t})\n\tcase http.StatusInternalServerError:\n\t\tif mod.API == \"openai\" {\n\t\t\treturn m.retry(content, modsError{err: err, reason: \"OpenAI API server error.\"})\n\t\t}\n\t\treturn modsError{err: err, reason: fmt.Sprintf(\n\t\t\t\"Error loading model '%s' for API '%s'.\",\n\t\t\tmod.Name,\n\t\t\tmod.API,\n\t\t)}\n\tdefault:\n\t\treturn m.retry(content, modsError{err: err, reason: \"Unknown API error.\"})\n\t}\n}\n\nfunc (m *Mods) receiveCompletionStreamCmd(msg completionOutput) tea.Cmd {\n\treturn func() tea.Msg {\n\t\tresp, err := msg.stream.Recv()\n\t\tif errors.Is(err, io.EOF) {\n\t\t\t_ = msg.stream.Close()\n\t\t\tm.messages = append(m.messages, openai.ChatCompletionMessage{\n\t\t\t\tRole:    openai.ChatMessageRoleAssistant,\n\t\t\t\tContent: m.Output,\n\t\t\t})\n\t\t\treturn completionOutput{}\n\t\t}\n\t\tif err != nil {\n\t\t\t_ = msg.stream.Close()\n\t\t\treturn modsError{err, \"There was an error when streaming the API response.\"}\n\t\t}\n\t\tif len(resp.Choices) > 0 {\n\t\t\tmsg.content = resp.Choices[0].Delta.Content\n\t\t}\n\t\treturn msg\n\t}\n}\n\ntype cacheDetailsMsg struct {\n\tWriteID, Title, ReadID, Model string\n}\n\nfunc (m *Mods) findCacheOpsDetails() tea.Cmd {\n\treturn func() tea.Msg {\n\t\tcontinueLast := m.Config.ContinueLast || (m.Config.Continue != \"\" && m.Config.Title == \"\")\n\t\treadID := ordered.First(m.Config.Continue, m.Config.Show)\n\t\twriteID := ordered.First(m.Config.Title, m.Config.Continue)\n\t\ttitle := writeID\n\t\tmodel := config.Model\n\n\t\tif readID != \"\" || continueLast || m.Config.ShowLast {\n\t\t\tfound, err := m.findReadID(readID)\n\t\t\tif err != nil {\n\t\t\t\treturn modsError{\n\t\t\t\t\terr:    err,\n\t\t\t\t\treason: \"Could not find the conversation.\",\n\t\t\t\t}\n\t\t\t}\n\t\t\tif found != nil {\n\t\t\t\treadID = found.ID\n\t\t\t\tif found.Model != nil {\n\t\t\t\t\tmodel = *found.Model\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// if we are continuing last, update the existing conversation\n\t\tif continueLast {\n\t\t\twriteID = readID\n\t\t}\n\n\t\tif writeID == \"\" {\n\t\t\twriteID = newConversationID()\n\t\t}\n\n\t\tif !sha1reg.MatchString(writeID) {\n\t\t\tconvo, err := m.db.Find(writeID)\n\t\t\tif err != nil {\n\t\t\t\t// its a new conversation with a title\n\t\t\t\twriteID = newConversationID()\n\t\t\t} else {\n\t\t\t\twriteID = convo.ID\n\t\t\t}\n\t\t}\n\n\t\treturn cacheDetailsMsg{\n\t\t\tWriteID: writeID,\n\t\t\tTitle:   title,\n\t\t\tReadID:  readID,\n\t\t\tModel:   model,\n\t\t}\n\t}\n}\n\nfunc (m *Mods) findReadID(in string) (*Conversation, error) {\n\tconvo, err := m.db.Find(in)\n\tif err == nil {\n\t\treturn convo, nil\n\t}\n\tif errors.Is(err, errNoMatches) && m.Config.Show == \"\" {\n\t\tconvo, err := m.db.FindHEAD()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn convo, nil\n\t}\n\treturn nil, err\n}\n\nfunc (m *Mods) readStdinCmd() tea.Msg {\n\tif !isInputTTY() {\n\t\treader := bufio.NewReader(os.Stdin)\n\t\tstdinBytes, err := io.ReadAll(reader)\n\t\tif err != nil {\n\t\t\treturn modsError{err, \"Unable to read stdin.\"}\n\t\t}\n\n\t\treturn completionInput{increaseIndent(string(stdinBytes))}\n\t}\n\treturn completionInput{\"\"}\n}\n\n// noOmitFloat converts a 0.0 value to a float usable by the OpenAI client\n// library, which currently uses Float32 fields in the request struct with the\n// omitempty tag. This means we need to use math.SmallestNonzeroFloat32 instead\n// of 0.0 so it doesn't get stripped from the request and replaced server side\n// with the default values.\n// Issue: https://github.com/sashabaranov/go-openai/issues/9\nfunc noOmitFloat(f float32) float32 {\n\tif f == 0.0 {\n\t\treturn math.SmallestNonzeroFloat32\n\t}\n\treturn f\n}\n\nfunc (m *Mods) readFromCache() tea.Cmd {\n\treturn func() tea.Msg {\n\t\tvar messages []openai.ChatCompletionMessage\n\t\tif err := m.cache.read(m.Config.cacheReadFromID, &messages); err != nil {\n\t\t\treturn modsError{err, \"There was an error loading the conversation.\"}\n\t\t}\n\n\t\treturn m.receiveCompletionStreamCmd(completionOutput{\n\t\t\tstream: &cachedCompletionStream{\n\t\t\t\tmessages: messages,\n\t\t\t},\n\t\t})()\n\t}\n}\n\nconst tabWidth = 4\n\nfunc (m *Mods) appendToOutput(s string) {\n\tm.Output += s\n\tif !isOutputTTY() || m.Config.Raw {\n\t\tm.contentMutex.Lock()\n\t\tm.content = append(m.content, s)\n\t\tm.contentMutex.Unlock()\n\t\treturn\n\t}\n\n\twasAtBottom := m.glamViewport.ScrollPercent() == 1.0\n\toldHeight := m.glamHeight\n\tm.glamOutput, _ = m.glam.Render(m.Output)\n\tm.glamOutput = strings.TrimRightFunc(m.glamOutput, unicode.IsSpace)\n\tm.glamOutput = strings.ReplaceAll(m.glamOutput, \"\\t\", strings.Repeat(\" \", tabWidth))\n\tm.glamHeight = lipgloss.Height(m.glamOutput)\n\tm.glamOutput += \"\\n\"\n\ttruncatedGlamOutput := m.renderer.NewStyle().MaxWidth(m.width).Render(m.glamOutput)\n\tm.glamViewport.SetContent(truncatedGlamOutput)\n\tif oldHeight < m.glamHeight && wasAtBottom {\n\t\t// If the viewport's at the bottom and we've received a new\n\t\t// line of content, follow the output by auto scrolling to\n\t\t// the bottom.\n\t\tm.glamViewport.GotoBottom()\n\t}\n}\n\n// if the input is whitespace only, make it empty.\nfunc removeWhitespace(s string) string {\n\tif strings.TrimSpace(s) == \"\" {\n\t\treturn \"\"\n\t}\n\treturn s\n}\n\nfunc responseFormat(cfg *Config) *openai.ChatCompletionResponseFormat {\n\tif cfg.API != \"openai\" {\n\t\t// only openai's api supports ChatCompletionResponseFormat\n\t\treturn nil\n\t}\n\treturn &openai.ChatCompletionResponseFormat{\n\t\tType: responseType(cfg),\n\t}\n}\n\nfunc responseType(cfg *Config) openai.ChatCompletionResponseFormatType {\n\tif !cfg.Format {\n\t\treturn openai.ChatCompletionResponseFormatTypeText\n\t}\n\t// only these two models support json\n\tif cfg.Model != \"gpt-4-1106-preview\" && cfg.Model != \"gpt-3.5-turbo-1106\" {\n\t\treturn openai.ChatCompletionResponseFormatTypeText\n\t}\n\tswitch cfg.FormatAs {\n\tcase \"json\":\n\t\treturn openai.ChatCompletionResponseFormatTypeJSONObject\n\tdefault:\n\t\treturn openai.ChatCompletionResponseFormatTypeText\n\t}\n}\n\nvar tokenErrRe = regexp.MustCompile(`This model's maximum context length is (\\d+) tokens. However, your messages resulted in (\\d+) tokens`)\n\nfunc cutPrompt(msg, prompt string) string {\n\tfound := tokenErrRe.FindStringSubmatch(msg)\n\tif len(found) != 3 { //nolint:mnd\n\t\treturn prompt\n\t}\n\n\tmaxt, _ := strconv.Atoi(found[1])\n\tcurrent, _ := strconv.Atoi(found[2])\n\n\tif maxt > current {\n\t\treturn prompt\n\t}\n\n\t// 1 token =~ 4 chars\n\t// cut 10 extra chars 'just in case'\n\treduceBy := 10 + (current-maxt)*4 //nolint:mnd\n\tif len(prompt) > reduceBy {\n\t\treturn prompt[:len(prompt)-reduceBy]\n\t}\n\n\treturn prompt\n}\n\nfunc increaseIndent(s string) string {\n\tlines := strings.Split(s, \"\\n\")\n\tfor i := 0; i < len(lines); i++ {\n\t\tlines[i] = \"\\t\" + lines[i]\n\t}\n\treturn strings.Join(lines, \"\\n\")\n}\n"
        },
        {
          "name": "mods_test.go",
          "type": "blob",
          "size": 7.232421875,
          "content": "package main\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/sashabaranov/go-openai\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestFindCacheOpsDetails(t *testing.T) {\n\tnewMods := func(t *testing.T) *Mods {\n\t\tdb := testDB(t)\n\t\treturn &Mods{\n\t\t\tdb:     db,\n\t\t\tConfig: &Config{},\n\t\t}\n\t}\n\n\tt.Run(\"all empty\", func(t *testing.T) {\n\t\tmsg := newMods(t).findCacheOpsDetails()()\n\t\tdets := msg.(cacheDetailsMsg)\n\t\trequire.Empty(t, dets.ReadID)\n\t\trequire.NotEmpty(t, dets.WriteID)\n\t\trequire.Empty(t, dets.Title)\n\t})\n\n\tt.Run(\"show id\", func(t *testing.T) {\n\t\tmods := newMods(t)\n\t\tid := newConversationID()\n\t\trequire.NoError(t, mods.db.Save(id, \"message\", \"gpt-4\"))\n\t\tmods.Config.Show = id[:8]\n\t\tmsg := mods.findCacheOpsDetails()()\n\t\tdets := msg.(cacheDetailsMsg)\n\t\trequire.Equal(t, id, dets.ReadID)\n\t})\n\n\tt.Run(\"show title\", func(t *testing.T) {\n\t\tmods := newMods(t)\n\t\tid := newConversationID()\n\t\trequire.NoError(t, mods.db.Save(id, \"message 1\", \"gpt-4\"))\n\t\tmods.Config.Show = \"message 1\"\n\t\tmsg := mods.findCacheOpsDetails()()\n\t\tdets := msg.(cacheDetailsMsg)\n\t\trequire.Equal(t, id, dets.ReadID)\n\t})\n\n\tt.Run(\"continue id\", func(t *testing.T) {\n\t\tmods := newMods(t)\n\t\tid := newConversationID()\n\t\trequire.NoError(t, mods.db.Save(id, \"message\", \"gpt-4\"))\n\t\tmods.Config.Continue = id[:5]\n\t\tmods.Config.Prefix = \"prompt\"\n\t\tmsg := mods.findCacheOpsDetails()()\n\t\tdets := msg.(cacheDetailsMsg)\n\t\trequire.Equal(t, id, dets.ReadID)\n\t\trequire.Equal(t, id, dets.WriteID)\n\t})\n\n\tt.Run(\"continue with no prompt\", func(t *testing.T) {\n\t\tmods := newMods(t)\n\t\tid := newConversationID()\n\t\trequire.NoError(t, mods.db.Save(id, \"message 1\", \"gpt-4\"))\n\t\tmods.Config.ContinueLast = true\n\t\tmsg := mods.findCacheOpsDetails()()\n\t\tdets := msg.(cacheDetailsMsg)\n\t\trequire.Equal(t, id, dets.ReadID)\n\t\trequire.Equal(t, id, dets.WriteID)\n\t\trequire.Empty(t, dets.Title)\n\t})\n\n\tt.Run(\"continue title\", func(t *testing.T) {\n\t\tmods := newMods(t)\n\t\tid := newConversationID()\n\t\trequire.NoError(t, mods.db.Save(id, \"message 1\", \"gpt-4\"))\n\t\tmods.Config.Continue = \"message 1\"\n\t\tmods.Config.Prefix = \"prompt\"\n\t\tmsg := mods.findCacheOpsDetails()()\n\t\tdets := msg.(cacheDetailsMsg)\n\t\trequire.Equal(t, id, dets.ReadID)\n\t\trequire.Equal(t, id, dets.WriteID)\n\t})\n\n\tt.Run(\"continue last\", func(t *testing.T) {\n\t\tmods := newMods(t)\n\t\tid := newConversationID()\n\t\trequire.NoError(t, mods.db.Save(id, \"message 1\", \"gpt-4\"))\n\t\tmods.Config.ContinueLast = true\n\t\tmods.Config.Prefix = \"prompt\"\n\t\tmsg := mods.findCacheOpsDetails()()\n\t\tdets := msg.(cacheDetailsMsg)\n\t\trequire.Equal(t, id, dets.ReadID)\n\t\trequire.Equal(t, id, dets.WriteID)\n\t\trequire.Empty(t, dets.Title)\n\t})\n\n\tt.Run(\"continue last with name\", func(t *testing.T) {\n\t\tmods := newMods(t)\n\t\tid := newConversationID()\n\t\trequire.NoError(t, mods.db.Save(id, \"message 1\", \"gpt-4\"))\n\t\tmods.Config.Continue = \"message 2\"\n\t\tmods.Config.Prefix = \"prompt\"\n\t\tmsg := mods.findCacheOpsDetails()()\n\t\tdets := msg.(cacheDetailsMsg)\n\t\trequire.Equal(t, id, dets.ReadID)\n\t\trequire.Equal(t, \"message 2\", dets.Title)\n\t\trequire.NotEmpty(t, dets.WriteID)\n\t\trequire.Equal(t, id, dets.WriteID)\n\t})\n\n\tt.Run(\"write\", func(t *testing.T) {\n\t\tmods := newMods(t)\n\t\tmods.Config.Title = \"some title\"\n\t\tmsg := mods.findCacheOpsDetails()()\n\t\tdets := msg.(cacheDetailsMsg)\n\t\trequire.Empty(t, dets.ReadID)\n\t\trequire.NotEmpty(t, dets.WriteID)\n\t\trequire.NotEqual(t, \"some title\", dets.WriteID)\n\t\trequire.Equal(t, \"some title\", dets.Title)\n\t})\n\n\tt.Run(\"continue id and write with title\", func(t *testing.T) {\n\t\tmods := newMods(t)\n\t\tid := newConversationID()\n\t\trequire.NoError(t, mods.db.Save(id, \"message 1\", \"gpt-4\"))\n\t\tmods.Config.Title = \"some title\"\n\t\tmods.Config.Continue = id[:10]\n\t\tmsg := mods.findCacheOpsDetails()()\n\t\tdets := msg.(cacheDetailsMsg)\n\t\trequire.Equal(t, id, dets.ReadID)\n\t\trequire.NotEmpty(t, dets.WriteID)\n\t\trequire.NotEqual(t, id, dets.WriteID)\n\t\trequire.NotEqual(t, \"some title\", dets.WriteID)\n\t\trequire.Equal(t, \"some title\", dets.Title)\n\t})\n\n\tt.Run(\"continue title and write with title\", func(t *testing.T) {\n\t\tmods := newMods(t)\n\t\tid := newConversationID()\n\t\trequire.NoError(t, mods.db.Save(id, \"message 1\", \"gpt-4\"))\n\t\tmods.Config.Title = \"some title\"\n\t\tmods.Config.Continue = \"message 1\"\n\t\tmsg := mods.findCacheOpsDetails()()\n\t\tdets := msg.(cacheDetailsMsg)\n\t\trequire.Equal(t, id, dets.ReadID)\n\t\trequire.NotEmpty(t, dets.WriteID)\n\t\trequire.NotEqual(t, id, dets.WriteID)\n\t\trequire.NotEqual(t, \"some title\", dets.WriteID)\n\t\trequire.Equal(t, \"some title\", dets.Title)\n\t})\n\n\tt.Run(\"show invalid\", func(t *testing.T) {\n\t\tmods := newMods(t)\n\t\tmods.Config.Show = \"aaa\"\n\t\tmsg := mods.findCacheOpsDetails()()\n\t\terr := msg.(modsError)\n\t\trequire.Equal(t, \"Could not find the conversation.\", err.reason)\n\t\trequire.EqualError(t, err, errNoMatches.Error())\n\t})\n}\n\nfunc TestRemoveWhitespace(t *testing.T) {\n\tt.Run(\"only whitespaces\", func(t *testing.T) {\n\t\trequire.Equal(t, \"\", removeWhitespace(\" \\n\"))\n\t})\n\n\tt.Run(\"regular text\", func(t *testing.T) {\n\t\trequire.Equal(t, \" regular\\n \", removeWhitespace(\" regular\\n \"))\n\t})\n}\n\nvar responseTypeCases = map[string]struct {\n\tconfig Config\n\texpect openai.ChatCompletionResponseFormatType\n}{\n\t\"no format\": {\n\t\tConfig{},\n\t\topenai.ChatCompletionResponseFormatTypeText,\n\t},\n\t\"format markdown\": {\n\t\tConfig{\n\t\t\tFormat:   true,\n\t\t\tFormatAs: \"markdown\",\n\t\t\tModel:    \"gpt-4\",\n\t\t},\n\t\topenai.ChatCompletionResponseFormatTypeText,\n\t},\n\t\"format json with unsupported model\": {\n\t\tConfig{\n\t\t\tFormat:   true,\n\t\t\tFormatAs: \"json\",\n\t\t\tModel:    \"gpt-4\",\n\t\t},\n\t\topenai.ChatCompletionResponseFormatTypeText,\n\t},\n\t\"format markdown with gpt-4-1106-preview\": {\n\t\tConfig{\n\t\t\tFormat:   true,\n\t\t\tFormatAs: \"markdown\",\n\t\t\tModel:    \"gpt-4-1106-preview\",\n\t\t},\n\t\topenai.ChatCompletionResponseFormatTypeText,\n\t},\n\t\"format markdown with gpt-3.5-turbo-1106\": {\n\t\tConfig{\n\t\t\tFormat:   true,\n\t\t\tFormatAs: \"markdown\",\n\t\t\tModel:    \"gpt-3.5-turbo-1106\",\n\t\t},\n\t\topenai.ChatCompletionResponseFormatTypeText,\n\t},\n\t\"format json with gpt-4-1106-preview\": {\n\t\tConfig{\n\t\t\tFormat:   true,\n\t\t\tFormatAs: \"json\",\n\t\t\tModel:    \"gpt-4-1106-preview\",\n\t\t},\n\t\topenai.ChatCompletionResponseFormatTypeJSONObject,\n\t},\n\t\"format json with gpt-3.5-turbo-1106\": {\n\t\tConfig{\n\t\t\tFormat:   true,\n\t\t\tFormatAs: \"json\",\n\t\t\tModel:    \"gpt-3.5-turbo-1106\",\n\t\t},\n\t\topenai.ChatCompletionResponseFormatTypeJSONObject,\n\t},\n}\n\nfunc TestResponseType(t *testing.T) {\n\tfor k, tc := range responseTypeCases {\n\t\tt.Run(k, func(t *testing.T) {\n\t\t\trequire.Equal(t, tc.expect, responseType(&tc.config))\n\t\t})\n\t}\n}\n\nvar cutPromptTests = map[string]struct {\n\tmsg      string\n\tprompt   string\n\texpected string\n}{\n\t\"bad error\": {\n\t\tmsg:      \"nope\",\n\t\tprompt:   \"the prompt\",\n\t\texpected: \"the prompt\",\n\t},\n\t\"crazy error\": {\n\t\tmsg:      tokenErrMsg(10, 93),\n\t\tprompt:   \"the prompt\",\n\t\texpected: \"the prompt\",\n\t},\n\t\"cut prompt\": {\n\t\tmsg:      tokenErrMsg(10, 3),\n\t\tprompt:   \"this is a long prompt I have no idea if its really 10 tokens\",\n\t\texpected: \"this is a long prompt \",\n\t},\n\t\"missmatch of token estimation vs api result\": {\n\t\tmsg:      tokenErrMsg(30000, 100),\n\t\tprompt:   \"tell me a joke\",\n\t\texpected: \"tell me a joke\",\n\t},\n}\n\nfunc tokenErrMsg(l, ml int) string {\n\treturn fmt.Sprintf(`This model's maximum context length is %d tokens. However, your messages resulted in %d tokens`, ml, l)\n}\n\nfunc TestCutPrompt(t *testing.T) {\n\tfor name, tc := range cutPromptTests {\n\t\tt.Run(name, func(t *testing.T) {\n\t\t\trequire.Equal(t, tc.expected, cutPrompt(tc.msg, tc.prompt))\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "ollama.go",
          "type": "blob",
          "size": 8.951171875,
          "content": "package main\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\n\topenai \"github.com/sashabaranov/go-openai\"\n)\n\n// OllamaClientConfig represents the configuration for the Ollama API client.\ntype OllamaClientConfig struct {\n\tBaseURL            string\n\tHTTPClient         *http.Client\n\tEmptyMessagesLimit uint\n}\n\n// DefaultOllamaConfig returns the default configuration for the Ollama API client.\nfunc DefaultOllamaConfig() OllamaClientConfig {\n\treturn OllamaClientConfig{\n\t\tBaseURL:            \"http://localhost:11434/api\",\n\t\tHTTPClient:         &http.Client{},\n\t\tEmptyMessagesLimit: defaultEmptyMessagesLimit,\n\t}\n}\n\n// OllamaMessageCompletionRequestOptions represents the valid parameters and values options for the request.\ntype OllamaMessageCompletionRequestOptions struct {\n\tMirostat      int     `json:\"mirostat,omitempty\"`\n\tMirostatEta   int     `json:\"mirostat_eta,omitempty\"`\n\tMirostatTau   int     `json:\"mirostat_tau,omitempty\"`\n\tNumCtx        int     `json:\"num_ctx,omitempty\"`\n\tRepeatLastN   int     `json:\"repeat_last_n,omitempty\"`\n\tRepeatPenalty float32 `json:\"repeat_penalty,omitempty\"`\n\tTemperature   float32 `json:\"temperature,omitempty\"`\n\tSeed          int     `json:\"seed,omitempty\"`\n\tStop          string  `json:\"stop,omitempty\"`\n\tTfsZ          float32 `json:\"tfs_z,omitempty\"`\n\tNumPredict    int     `json:\"num_predict,omitempty\"`\n\tTopP          float32 `json:\"top_p,omitempty\"`\n\tTopK          int     `json:\"top_k,omitempty\"`\n}\n\n// OllamaMessageCompletionRequest represents the request body for the generate completion API.\ntype OllamaMessageCompletionRequest struct {\n\tModel     string                                `json:\"model\"`\n\tMessages  []openai.ChatCompletionMessage        `json:\"messages\"`\n\tOptions   OllamaMessageCompletionRequestOptions `json:\"options,omitempty\"`\n\tStream    bool                                  `json:\"stream,omitempty\"`\n\tKeepAlive string                                `json:\"keep_alive,omitempty\"`\n}\n\n// OllamaRequestBuilder is an interface for building HTTP requests for the Ollama API.\ntype OllamaRequestBuilder interface {\n\tBuild(ctx context.Context, method, url string, body any, header http.Header) (*http.Request, error)\n}\n\n// NewOllamaRequestBuilder creates a new HTTPRequestBuilder.\nfunc NewOllamaRequestBuilder() *HTTPRequestBuilder {\n\treturn &HTTPRequestBuilder{\n\t\tmarshaller: &JSONMarshaller{},\n\t}\n}\n\n// OllamaClient is a client for the Ollama API.\ntype OllamaClient struct {\n\tconfig OllamaClientConfig\n\n\trequestBuilder OllamaRequestBuilder\n}\n\n// NewOllamaClient creates a new OllamaClient with the given configuration.\nfunc NewOllamaClientWithConfig(config OllamaClientConfig) *OllamaClient {\n\treturn &OllamaClient{\n\t\tconfig:         config,\n\t\trequestBuilder: NewOllamaRequestBuilder(),\n\t}\n}\n\nconst ollamaChatCompletionsSuffix = \"/chat\"\n\nfunc (c *OllamaClient) newRequest(ctx context.Context, method, url string, setters ...requestOption) (*http.Request, error) {\n\t// Default Options\n\targs := &requestOptions{\n\t\tbody:   nil,\n\t\theader: make(http.Header),\n\t}\n\tfor _, setter := range setters {\n\t\tsetter(args)\n\t}\n\treq, err := c.requestBuilder.Build(ctx, method, url, args.body, args.header)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"OllamaClient.newRequest: %w\", err)\n\t}\n\treturn req, nil\n}\n\nfunc (c *OllamaClient) handleErrorResp(resp *http.Response) error {\n\t// Print the response text\n\tvar errRes openai.ErrorResponse\n\terr := json.NewDecoder(resp.Body).Decode(&errRes)\n\tif err != nil || errRes.Error == nil {\n\t\treqErr := &openai.RequestError{\n\t\t\tHTTPStatusCode: resp.StatusCode,\n\t\t\tErr:            err,\n\t\t}\n\t\tif errRes.Error != nil {\n\t\t\treqErr.Err = errRes.Error\n\t\t}\n\t\treturn reqErr\n\t}\n\n\terrRes.Error.HTTPStatusCode = resp.StatusCode\n\treturn errRes.Error\n}\n\n// OllamaMessageUsage represents the usage of an Ollama message.\ntype OllamaMessageUsage struct {\n\tInputTokens  int `json:\"input_tokens,omitempty\"`\n\tOutputTokens int `json:\"output_tokens,omitempty\"`\n}\n\n// OllamaMessage represents an Ollama message.\ntype OllamaMessage struct {\n\tUsage        *OllamaMessageUsage `json:\"usage,omitempty\"`\n\tStopReason   *string             `json:\"stop_reason,omitempty\"`\n\tStopSequence *string             `json:\"stop_sequence,omitempty\"`\n\tID           string              `json:\"id,omitempty\"`\n\tType         string              `json:\"type\"`\n\tRole         string              `json:\"role,omitempty\"`\n\tModel        string              `json:\"model,omitempty\"`\n\tContent      []string            `json:\"content,omitempty\"`\n}\n\n// OllamaMessageContentBlock represents a content block in an Ollama message.\ntype OllamaMessageContentBlock struct {\n\tType string `json:\"type\"`\n\tText string `json:\"text,omitempty\"`\n}\n\n// OllamaMessageTextDelta represents a text delta in an Ollama message.\ntype OllamaMessageTextDelta struct {\n\tType string `json:\"type\"`\n\tText string `json:\"text,omitempty\"`\n}\n\n// OllamaMessageCompletionRequest represents the response body for the generate completion API.\ntype OllamaCompletionMessageResponse struct {\n\tModel              string                       `json:\"model\"`\n\tCreatedAt          string                       `json:\"created_at\"`\n\tMessage            openai.ChatCompletionMessage `json:\"message\"`\n\tDone               bool                         `json:\"done\"`\n\tTotalDuration      int                          `json:\"total_duration\"`\n\tLoadDuration       int                          `json:\"load_duration\"`\n\tPromptEvalCount    int                          `json:\"prompt_eval_count\"`\n\tPromptEvalDuration int                          `json:\"prompt_eval_duration\"`\n\tEvalCount          int                          `json:\"eval_count\"`\n\tEvalDuration       int                          `json:\"eval_duration\"`\n}\n\n// ChatCompletionStream represents a stream for chat completion.\ntype OllamaChatCompletionStream struct {\n\t*ollamaStreamReader\n}\n\ntype ollamaStreamReader struct {\n\temptyMessagesLimit uint\n\tisFinished         bool\n\n\treader         *bufio.Reader\n\tresponse       *http.Response\n\terrAccumulator ErrorAccumulator\n\tunmarshaler    Unmarshaler\n\n\thttpHeader\n}\n\n// Recv reads the next response from the stream.\nfunc (stream *ollamaStreamReader) Recv() (response openai.ChatCompletionStreamResponse, err error) {\n\tif stream.isFinished {\n\t\terr = io.EOF\n\t\treturn\n\t}\n\n\tresponse, err = stream.processLines()\n\treturn\n}\n\n// Close closes the stream.\nfunc (stream *ollamaStreamReader) Close() error {\n\treturn stream.response.Body.Close() //nolint:wrapcheck\n}\n\n//nolint:gocognit\nfunc (stream *ollamaStreamReader) processLines() (openai.ChatCompletionStreamResponse, error) {\n\tfor {\n\t\trawLine, readErr := stream.reader.ReadBytes('\\n')\n\n\t\tif readErr != nil {\n\t\t\treturn *new(openai.ChatCompletionStreamResponse), fmt.Errorf(\"ollamaStreamReader.processLines: %w\", readErr)\n\t\t}\n\n\t\tnoSpaceLine := bytes.TrimSpace(rawLine)\n\n\t\tvar chunk OllamaCompletionMessageResponse\n\t\tunmarshalErr := stream.unmarshaler.Unmarshal(noSpaceLine, &chunk)\n\t\tif unmarshalErr != nil {\n\t\t\treturn openai.ChatCompletionStreamResponse{}, fmt.Errorf(\"ollamaStreamReader.processLines: %w\", unmarshalErr)\n\t\t}\n\n\t\tif chunk.Done {\n\t\t\treturn openai.ChatCompletionStreamResponse{}, nil\n\t\t}\n\n\t\tif chunk.Message.Content == \"\" {\n\t\t\tcontinue\n\t\t}\n\n\t\t// NOTE: Leverage the existing logic based on OpenAI ChatCompletionStreamResponse by\n\t\t//       converting the Ollama events into them.\n\t\tresponse := openai.ChatCompletionStreamResponse{\n\t\t\tChoices: []openai.ChatCompletionStreamChoice{\n\t\t\t\t{\n\t\t\t\t\tIndex: 0,\n\t\t\t\t\tDelta: openai.ChatCompletionStreamChoiceDelta{\n\t\t\t\t\t\tContent: chunk.Message.Content,\n\t\t\t\t\t\tRole:    \"assistant\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\n\t\treturn response, nil\n\t}\n}\n\nfunc ollamaSendRequestStream(client *OllamaClient, req *http.Request) (*ollamaStreamReader, error) {\n\treq.Header.Set(\"content-type\", \"application/json\")\n\n\tresp, err := client.config.HTTPClient.Do(req) //nolint:bodyclose // body is closed in stream.Close()\n\tif err != nil {\n\t\treturn new(ollamaStreamReader), err\n\t}\n\tif isFailureStatusCode(resp) {\n\t\treturn new(ollamaStreamReader), client.handleErrorResp(resp)\n\t}\n\n\treturn &ollamaStreamReader{\n\t\temptyMessagesLimit: client.config.EmptyMessagesLimit,\n\t\treader:             bufio.NewReader(resp.Body),\n\t\tresponse:           resp,\n\t\terrAccumulator:     NewErrorAccumulator(),\n\t\tunmarshaler:        &JSONUnmarshaler{},\n\t\thttpHeader:         httpHeader(resp.Header),\n\t}, nil\n}\n\n// CreateChatCompletionStream — API call to create a generate completion w/ streaming\n// support. It sets whether to stream back partial progress. If set, tokens will be\n// sent as data-only server-sent events as they become available, with the\n// stream terminated by a data: [DONE] message.\nfunc (c *OllamaClient) CreateChatCompletionStream(\n\tctx context.Context,\n\trequest OllamaMessageCompletionRequest,\n) (stream *OllamaChatCompletionStream, err error) {\n\turlSuffix := ollamaChatCompletionsSuffix\n\n\trequest.Stream = true\n\treq, err := c.newRequest(ctx, http.MethodPost, c.config.BaseURL+urlSuffix, withBody(request))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tresp, err := ollamaSendRequestStream(c, req)\n\tif err != nil {\n\t\treturn\n\t}\n\n\tstream = &OllamaChatCompletionStream{\n\t\tollamaStreamReader: resp,\n\t}\n\treturn\n}\n"
        },
        {
          "name": "sha.go",
          "type": "blob",
          "size": 0.3671875,
          "content": "package main\n\nimport (\n\t\"crypto/rand\"\n\t\"crypto/sha1\" //nolint: gosec\n\t\"fmt\"\n\t\"regexp\"\n)\n\nconst (\n\tsha1short         = 7\n\tsha1minLen        = 4\n\tsha1ReadBlockSize = 4096\n)\n\nvar sha1reg = regexp.MustCompile(`\\b[0-9a-f]{40}\\b`)\n\nfunc newConversationID() string {\n\tb := make([]byte, sha1ReadBlockSize)\n\t_, _ = rand.Read(b)\n\treturn fmt.Sprintf(\"%x\", sha1.Sum(b)) //nolint: gosec\n}\n"
        },
        {
          "name": "shared.go",
          "type": "blob",
          "size": 3.234375,
          "content": "package main\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n)\n\ntype httpHeader http.Header\n\nconst (\n\tdefaultEmptyMessagesLimit uint = 300\n)\n\n// ErrTooManyEmptyStreamMessages represents an error when a stream has sent too many empty messages.\nvar ErrTooManyEmptyStreamMessages = errors.New(\"stream has sent too many empty messages\")\n\n// Marshaller is an interface for marshalling values to bytes.\ntype Marshaller interface {\n\tMarshal(value any) ([]byte, error)\n}\n\n// JSONMarshaller is a marshaller that marshals values to JSON.\ntype JSONMarshaller struct{}\n\n// Marshal marshals a value to JSON.\nfunc (jm *JSONMarshaller) Marshal(value any) ([]byte, error) {\n\tresult, err := json.Marshal(value)\n\tif err != nil {\n\t\treturn result, fmt.Errorf(\"JSONMarshaller.Marshal: %w\", err)\n\t}\n\treturn result, nil\n}\n\n// HTTPRequestBuilder is an implementation of OllamaRequestBuilder that builds HTTP requests.\ntype HTTPRequestBuilder struct {\n\tmarshaller Marshaller\n}\n\n// Build builds an HTTP request.\nfunc (b *HTTPRequestBuilder) Build(\n\tctx context.Context,\n\tmethod string,\n\turl string,\n\tbody any,\n\theader http.Header,\n) (req *http.Request, err error) {\n\tvar bodyReader io.Reader\n\tif body != nil {\n\t\tif v, ok := body.(io.Reader); ok {\n\t\t\tbodyReader = v\n\t\t} else {\n\t\t\tvar reqBytes []byte\n\t\t\treqBytes, err = b.marshaller.Marshal(body)\n\t\t\tif err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tbodyReader = bytes.NewBuffer(reqBytes)\n\t\t}\n\t}\n\treq, err = http.NewRequestWithContext(ctx, method, url, bodyReader)\n\tif err != nil {\n\t\treturn\n\t}\n\tif header != nil {\n\t\treq.Header = header\n\t}\n\treturn\n}\n\ntype requestOptions struct {\n\tbody   any\n\theader http.Header\n}\n\ntype requestOption func(*requestOptions)\n\nfunc withBody(body any) requestOption {\n\treturn func(args *requestOptions) {\n\t\targs.body = body\n\t}\n}\n\n// ErrorAccumulator is an interface for accumulating errors.\ntype ErrorAccumulator interface {\n\tWrite(p []byte) error\n\tBytes() []byte\n}\n\n// Unmarshaler is an interface for unmarshalling bytes.\ntype Unmarshaler interface {\n\tUnmarshal(data []byte, v any) error\n}\n\ntype errorBuffer interface {\n\tio.Writer\n\tLen() int\n\tBytes() []byte\n}\n\n// DefaultErrorAccumulator is a default implementation of ErrorAccumulator.\ntype DefaultErrorAccumulator struct {\n\tBuffer errorBuffer\n}\n\n// NewErrorAccumulator creates a new ErrorAccumulator.\nfunc NewErrorAccumulator() ErrorAccumulator {\n\treturn &DefaultErrorAccumulator{\n\t\tBuffer: &bytes.Buffer{},\n\t}\n}\n\n// Write writes data to the error accumulator.\nfunc (e *DefaultErrorAccumulator) Write(p []byte) error {\n\t_, err := e.Buffer.Write(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error accumulator write error, %w\", err)\n\t}\n\treturn nil\n}\n\n// Bytes returns the accumulated error bytes.\nfunc (e *DefaultErrorAccumulator) Bytes() (errBytes []byte) {\n\tif e.Buffer.Len() == 0 {\n\t\treturn\n\t}\n\terrBytes = e.Buffer.Bytes()\n\treturn\n}\n\nfunc isFailureStatusCode(resp *http.Response) bool {\n\treturn resp.StatusCode < http.StatusOK || resp.StatusCode >= http.StatusBadRequest\n}\n\n// JSONUnmarshaler is an unmarshaler that unmarshals JSON data.\ntype JSONUnmarshaler struct{}\n\n// Unmarshal unmarshals JSON data.\nfunc (jm *JSONUnmarshaler) Unmarshal(data []byte, v any) error {\n\terr := json.Unmarshal(data, v)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"JSONUnmarshaler.Unmarshal: %w\", err)\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "stream.go",
          "type": "blob",
          "size": 8.1181640625,
          "content": "package main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strings\"\n\n\ttea \"github.com/charmbracelet/bubbletea\"\n\tcohere \"github.com/cohere-ai/cohere-go/v2\"\n\topenai \"github.com/sashabaranov/go-openai\"\n)\n\nfunc (m *Mods) createOpenAIStream(content string, ccfg openai.ClientConfig, mod Model) tea.Msg {\n\tcfg := m.Config\n\n\tclient := openai.NewClientWithConfig(ccfg)\n\tctx, cancel := context.WithCancel(context.Background())\n\tm.cancelRequest = cancel\n\n\tif err := m.setupStreamContext(content, mod); err != nil {\n\t\treturn err\n\t}\n\n\treq := openai.ChatCompletionRequest{\n\t\tModel:    mod.Name,\n\t\tMessages: m.messages,\n\t\tStream:   true,\n\t\tUser:     cfg.User,\n\t}\n\n\tif mod.API != \"perplexity\" || !strings.Contains(mod.Name, \"online\") {\n\t\treq.Temperature = noOmitFloat(cfg.Temperature)\n\t\treq.TopP = noOmitFloat(cfg.TopP)\n\t\treq.Stop = cfg.Stop\n\t\treq.MaxTokens = cfg.MaxTokens\n\t\treq.ResponseFormat = responseFormat(cfg)\n\t}\n\n\tstream, err := client.CreateChatCompletionStream(ctx, req)\n\tif err != nil {\n\t\treturn m.handleRequestError(err, mod, content)\n\t}\n\n\treturn m.receiveCompletionStreamCmd(completionOutput{stream: stream})()\n}\n\nfunc (m *Mods) createOllamaStream(content string, occfg OllamaClientConfig, mod Model) tea.Msg {\n\tcfg := m.Config\n\n\tclient := NewOllamaClientWithConfig(occfg)\n\tctx, cancel := context.WithCancel(context.Background())\n\tm.cancelRequest = cancel\n\n\tif err := m.setupStreamContext(content, mod); err != nil {\n\t\treturn err\n\t}\n\n\treq := OllamaMessageCompletionRequest{\n\t\tModel:    mod.Name,\n\t\tMessages: m.messages,\n\t\tStream:   true,\n\t\tOptions: OllamaMessageCompletionRequestOptions{\n\t\t\tTemperature: noOmitFloat(cfg.Temperature),\n\t\t\tTopP:        noOmitFloat(cfg.TopP),\n\t\t},\n\t}\n\n\tif len(cfg.Stop) > 0 {\n\t\treq.Options.Stop = cfg.Stop[0]\n\t}\n\n\tif cfg.MaxTokens > 0 {\n\t\treq.Options.NumCtx = cfg.MaxTokens\n\t}\n\n\tstream, err := client.CreateChatCompletionStream(ctx, req)\n\tif err != nil {\n\t\treturn m.handleRequestError(err, mod, content)\n\t}\n\n\treturn m.receiveCompletionStreamCmd(completionOutput{stream: stream})()\n}\n\nfunc (m *Mods) createGoogleStream(content string, gccfg GoogleClientConfig, mod Model) tea.Msg {\n\tcfg := m.Config\n\n\tclient := NewGoogleClientWithConfig(gccfg)\n\tctx, cancel := context.WithCancel(context.Background())\n\tm.cancelRequest = cancel\n\n\tif err := m.setupStreamContext(content, mod); err != nil {\n\t\treturn err\n\t}\n\n\t// Google doesn't support the System role so we need to remove those message\n\t// and, instead, store their content on the `System` request value.\n\t//\n\t// Also, the shape of Google messages is slightly different, so we make the\n\t// conversion here.\n\tmessages := []GoogleContent{}\n\n\tfor _, message := range m.messages {\n\t\tif message.Role == openai.ChatMessageRoleSystem {\n\t\t\tparts := []GoogleParts{\n\t\t\t\t{Text: fmt.Sprintf(\"%s\\n\", message.Content)},\n\t\t\t}\n\t\t\tmessages = append(messages, GoogleContent{\n\t\t\t\tRole:  \"user\",\n\t\t\t\tParts: parts,\n\t\t\t})\n\t\t} else {\n\t\t\trole := \"user\"\n\t\t\tif message.Role == openai.ChatMessageRoleAssistant {\n\t\t\t\trole = \"model\"\n\t\t\t}\n\t\t\tparts := []GoogleParts{\n\t\t\t\t{Text: message.Content},\n\t\t\t}\n\t\t\tmessages = append(messages, GoogleContent{\n\t\t\t\tRole:  role,\n\t\t\t\tParts: parts,\n\t\t\t})\n\t\t}\n\t}\n\n\tgenerationConfig := GoogleGenerationConfig{\n\t\tStopSequences:  cfg.Stop,\n\t\tTemperature:    cfg.Temperature,\n\t\tTopP:           cfg.TopP,\n\t\tTopK:           cfg.TopK,\n\t\tCandidateCount: 1,\n\t}\n\n\tif cfg.MaxTokens > 0 {\n\t\tgenerationConfig.MaxOutputTokens = uint(cfg.MaxTokens) //nolint: gosec\n\t} else {\n\t\tgenerationConfig.MaxOutputTokens = 4096\n\t}\n\n\treq := GoogleMessageCompletionRequest{\n\t\tContents:         messages,\n\t\tGenerationConfig: generationConfig,\n\t}\n\n\tstream, err := client.CreateChatCompletionStream(ctx, req)\n\tif err != nil {\n\t\treturn m.handleRequestError(err, mod, content)\n\t}\n\n\treturn m.receiveCompletionStreamCmd(completionOutput{stream: stream})()\n}\n\nfunc (m *Mods) createAnthropicStream(content string, accfg AnthropicClientConfig, mod Model) tea.Msg {\n\tcfg := m.Config\n\n\tclient := NewAnthropicClientWithConfig(accfg)\n\tctx, cancel := context.WithCancel(context.Background())\n\tm.cancelRequest = cancel\n\n\tif err := m.setupStreamContext(content, mod); err != nil {\n\t\treturn err\n\t}\n\n\t// Anthropic doesn't support the System role so we need to remove those message\n\t// and, instead, store their content on the `System` request value.\n\tmessages := []openai.ChatCompletionMessage{}\n\n\tfor _, message := range m.messages {\n\t\tif message.Role == openai.ChatMessageRoleSystem {\n\t\t\tm.system += message.Content + \"\\n\"\n\t\t} else {\n\t\t\tmessages = append(messages, message)\n\t\t}\n\t}\n\n\treq := AnthropicMessageCompletionRequest{\n\t\tModel:         mod.Name,\n\t\tMessages:      messages,\n\t\tSystem:        m.system,\n\t\tStream:        true,\n\t\tTemperature:   noOmitFloat(cfg.Temperature),\n\t\tTopP:          noOmitFloat(cfg.TopP),\n\t\tTopK:          cfg.TopK,\n\t\tStopSequences: cfg.Stop,\n\t}\n\n\tif cfg.MaxTokens > 0 {\n\t\treq.MaxTokens = cfg.MaxTokens\n\t} else {\n\t\treq.MaxTokens = 4096\n\t}\n\n\tstream, err := client.CreateChatCompletionStream(ctx, req)\n\tif err != nil {\n\t\treturn m.handleRequestError(err, mod, content)\n\t}\n\n\treturn m.receiveCompletionStreamCmd(completionOutput{stream: stream})()\n}\n\nfunc (m *Mods) createCohereStream(content string, cccfg CohereClientConfig, mod Model) tea.Msg {\n\tcfg := m.Config\n\n\tclient := NewCohereClientWithConfig(cccfg)\n\tctx, cancel := context.WithCancel(context.Background())\n\tm.cancelRequest = cancel\n\n\tif err := m.setupStreamContext(content, mod); err != nil {\n\t\treturn err\n\t}\n\n\tvar messages []*cohere.Message\n\tfor _, message := range m.messages {\n\t\tswitch message.Role {\n\t\tcase openai.ChatMessageRoleSystem:\n\t\t\t// For system, it is recommended to use the `preamble` field\n\t\t\t// rather than a \"SYSTEM\" role message\n\t\t\tm.system += message.Content + \"\\n\"\n\t\tcase openai.ChatMessageRoleAssistant:\n\t\t\tmessages = append(messages, &cohere.Message{\n\t\t\t\tRole: \"CHATBOT\",\n\t\t\t\tChatbot: &cohere.ChatMessage{\n\t\t\t\t\tMessage: message.Content,\n\t\t\t\t},\n\t\t\t})\n\t\tcase openai.ChatMessageRoleUser:\n\t\t\tmessages = append(messages, &cohere.Message{\n\t\t\t\tRole: \"USER\",\n\t\t\t\tUser: &cohere.ChatMessage{\n\t\t\t\t\tMessage: message.Content,\n\t\t\t\t},\n\t\t\t})\n\t\t}\n\t}\n\n\tvar history []*cohere.Message\n\tif len(messages) > 1 {\n\t\thistory = messages[:len(messages)-1]\n\t}\n\n\treq := &cohere.ChatStreamRequest{\n\t\tModel:         cohere.String(mod.Name),\n\t\tChatHistory:   history,\n\t\tMessage:       messages[len(messages)-1].User.Message,\n\t\tPreamble:      cohere.String(m.system),\n\t\tTemperature:   cohere.Float64(float64(cfg.Temperature)),\n\t\tP:             cohere.Float64(float64(cfg.TopP)),\n\t\tStopSequences: cfg.Stop,\n\t}\n\n\tif cfg.MaxTokens > 0 {\n\t\treq.MaxTokens = cohere.Int(cfg.MaxTokens)\n\t}\n\n\tstream, err := client.CreateChatCompletionStream(ctx, req)\n\tif err != nil {\n\t\treturn m.handleRequestError(CohereToOpenAIAPIError(err), mod, content)\n\t}\n\n\treturn m.receiveCompletionStreamCmd(completionOutput{stream: stream})()\n}\n\nfunc (m *Mods) setupStreamContext(content string, mod Model) error {\n\tcfg := m.Config\n\tm.messages = []openai.ChatCompletionMessage{}\n\tif cfg.Format {\n\t\tm.messages = append(m.messages, openai.ChatCompletionMessage{\n\t\t\tRole:    openai.ChatMessageRoleSystem,\n\t\t\tContent: cfg.FormatText[cfg.FormatAs],\n\t\t})\n\t}\n\n\tif cfg.Role != \"\" {\n\t\troleSetup, ok := cfg.Roles[cfg.Role]\n\t\tif !ok {\n\t\t\treturn modsError{\n\t\t\t\terr:    fmt.Errorf(\"role %q does not exist\", cfg.Role),\n\t\t\t\treason: \"Could not use role\",\n\t\t\t}\n\t\t}\n\t\tfor _, msg := range roleSetup {\n\t\t\tcontent, err := loadMsg(msg)\n\t\t\tif err != nil {\n\t\t\t\treturn modsError{\n\t\t\t\t\terr:    err,\n\t\t\t\t\treason: \"Could not use role\",\n\t\t\t\t}\n\t\t\t}\n\t\t\tm.messages = append(m.messages, openai.ChatCompletionMessage{\n\t\t\t\tRole:    openai.ChatMessageRoleSystem,\n\t\t\t\tContent: content,\n\t\t\t})\n\t\t}\n\t}\n\n\tif prefix := cfg.Prefix; prefix != \"\" {\n\t\tcontent = strings.TrimSpace(prefix + \"\\n\\n\" + content)\n\t}\n\n\tif !cfg.NoLimit && len(content) > mod.MaxChars {\n\t\tcontent = content[:mod.MaxChars]\n\t}\n\n\tif !cfg.NoCache && cfg.cacheReadFromID != \"\" {\n\t\tif err := m.cache.read(cfg.cacheReadFromID, &m.messages); err != nil {\n\t\t\treturn modsError{\n\t\t\t\terr: err,\n\t\t\t\treason: fmt.Sprintf(\n\t\t\t\t\t\"There was a problem reading the cache. Use %s / %s to disable it.\",\n\t\t\t\t\tm.Styles.InlineCode.Render(\"--no-cache\"),\n\t\t\t\t\tm.Styles.InlineCode.Render(\"NO_CACHE\"),\n\t\t\t\t),\n\t\t\t}\n\t\t}\n\t}\n\n\tm.messages = append(m.messages, openai.ChatCompletionMessage{\n\t\tRole:    openai.ChatMessageRoleUser,\n\t\tContent: content,\n\t})\n\n\treturn nil\n}\n"
        },
        {
          "name": "styles.go",
          "type": "blob",
          "size": 2.06640625,
          "content": "package main\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n\n\t\"github.com/charmbracelet/lipgloss\"\n)\n\ntype styles struct {\n\tAppName,\n\tCliArgs,\n\tComment,\n\tCyclingChars,\n\tErrorHeader,\n\tErrorDetails,\n\tErrPadding,\n\tFlag,\n\tFlagComma,\n\tFlagDesc,\n\tInlineCode,\n\tLink,\n\tPipe,\n\tQuote,\n\tConversationList,\n\tSHA1,\n\tTimeago lipgloss.Style\n}\n\nfunc makeStyles(r *lipgloss.Renderer) (s styles) {\n\tconst horizontalEdgePadding = 2\n\ts.AppName = r.NewStyle().Bold(true)\n\ts.CliArgs = r.NewStyle().Foreground(lipgloss.Color(\"#585858\"))\n\ts.Comment = r.NewStyle().Foreground(lipgloss.Color(\"#757575\"))\n\ts.CyclingChars = r.NewStyle().Foreground(lipgloss.Color(\"#FF87D7\"))\n\ts.ErrorHeader = r.NewStyle().Foreground(lipgloss.Color(\"#F1F1F1\")).Background(lipgloss.Color(\"#FF5F87\")).Bold(true).Padding(0, 1).SetString(\"ERROR\")\n\ts.ErrorDetails = s.Comment\n\ts.ErrPadding = r.NewStyle().Padding(0, horizontalEdgePadding)\n\ts.Flag = r.NewStyle().Foreground(lipgloss.AdaptiveColor{Light: \"#00B594\", Dark: \"#3EEFCF\"}).Bold(true)\n\ts.FlagComma = r.NewStyle().Foreground(lipgloss.AdaptiveColor{Light: \"#5DD6C0\", Dark: \"#427C72\"}).SetString(\",\")\n\ts.FlagDesc = s.Comment\n\ts.InlineCode = r.NewStyle().Foreground(lipgloss.Color(\"#FF5F87\")).Background(lipgloss.Color(\"#3A3A3A\")).Padding(0, 1)\n\ts.Link = r.NewStyle().Foreground(lipgloss.Color(\"#00AF87\")).Underline(true)\n\ts.Quote = r.NewStyle().Foreground(lipgloss.AdaptiveColor{Light: \"#FF71D0\", Dark: \"#FF78D2\"})\n\ts.Pipe = r.NewStyle().Foreground(lipgloss.AdaptiveColor{Light: \"#8470FF\", Dark: \"#745CFF\"})\n\ts.ConversationList = r.NewStyle().Padding(0, 1)\n\ts.SHA1 = s.Flag\n\ts.Timeago = r.NewStyle().Foreground(lipgloss.AdaptiveColor{Light: \"#999\", Dark: \"#555\"})\n\treturn s\n}\n\n// action messages\n\nconst defaultAction = \"WROTE\"\n\nvar outputHeader = lipgloss.NewStyle().Foreground(lipgloss.Color(\"#F1F1F1\")).Background(lipgloss.Color(\"#6C50FF\")).Bold(true).Padding(0, 1).MarginRight(1)\n\nfunc printConfirmation(action, content string) {\n\tif action == \"\" {\n\t\taction = defaultAction\n\t}\n\toutputHeader = outputHeader.SetString(strings.ToUpper(action))\n\tfmt.Println(lipgloss.JoinHorizontal(lipgloss.Center, outputHeader.String(), content))\n}\n"
        },
        {
          "name": "sync.go",
          "type": "blob",
          "size": 0.6689453125,
          "content": "package main\n\nimport \"sync\"\n\n// copied from go1.21, remove and use sync.OnceValue once go 1.21 is widely\n// available in package managers.\n\n// OnceValue returns a function that invokes f only once and returns the value\n// returned by f. The returned function may be called concurrently.\n//\n// If f panics, the returned function will panic with the same value on every call.\nfunc OnceValue[T any](f func() T) func() T {\n\tvar once sync.Once\n\tvar valid bool\n\tvar p any\n\tvar result T\n\treturn func() T {\n\t\tonce.Do(func() {\n\t\t\tdefer func() {\n\t\t\t\tp = recover()\n\t\t\t\tif !valid {\n\t\t\t\t\tpanic(p)\n\t\t\t\t}\n\t\t\t}()\n\t\t\tresult = f()\n\t\t\tvalid = true\n\t\t})\n\t\tif !valid {\n\t\t\tpanic(p)\n\t\t}\n\t\treturn result\n\t}\n}\n"
        },
        {
          "name": "term.go",
          "type": "blob",
          "size": 0.6884765625,
          "content": "package main\n\nimport (\n\t\"os\"\n\n\t\"github.com/charmbracelet/lipgloss\"\n\t\"github.com/mattn/go-isatty\"\n\t\"github.com/muesli/termenv\"\n)\n\nvar isInputTTY = OnceValue(func() bool {\n\treturn isatty.IsTerminal(os.Stdin.Fd())\n})\n\nvar isOutputTTY = OnceValue(func() bool {\n\treturn isatty.IsTerminal(os.Stdout.Fd())\n})\n\nvar stdoutRenderer = OnceValue(func() *lipgloss.Renderer {\n\treturn lipgloss.DefaultRenderer()\n})\n\nvar stdoutStyles = OnceValue(func() styles {\n\treturn makeStyles(stdoutRenderer())\n})\n\nvar stderrRenderer = OnceValue(func() *lipgloss.Renderer {\n\treturn lipgloss.NewRenderer(os.Stderr, termenv.WithColorCache(true))\n})\n\nvar stderrStyles = OnceValue(func() styles {\n\treturn makeStyles(stderrRenderer())\n})\n"
        },
        {
          "name": "testdata",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}