{
  "metadata": {
    "timestamp": 1736566779295,
    "page": 338,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "mholt/timeliner",
      "stars": 3562,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1181640625,
          "content": "_gitignore/\n_storage/\noauth2client/oauth2proxy/cmd/oauth2proxy/credentials.toml\n**/timeliner.toml\ncmc/timeliner/timeliner"
        },
        {
          "name": ".sqliterc",
          "type": "blob",
          "size": 0.025390625,
          "content": "PRAGMA foreign_keys = ON;\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 33.7138671875,
          "content": "                    GNU AFFERO GENERAL PUBLIC LICENSE\n                       Version 3, 19 November 2007\n\n Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>\n Everyone is permitted to copy and distribute verbatim copies\n of this license document, but changing it is not allowed.\n\n                            Preamble\n\n  The GNU Affero General Public License is a free, copyleft license for\nsoftware and other kinds of works, specifically designed to ensure\ncooperation with the community in the case of network server software.\n\n  The licenses for most software and other practical works are designed\nto take away your freedom to share and change the works.  By contrast,\nour General Public Licenses are intended to guarantee your freedom to\nshare and change all versions of a program--to make sure it remains free\nsoftware for all its users.\n\n  When we speak of free software, we are referring to freedom, not\nprice.  Our General Public Licenses are designed to make sure that you\nhave the freedom to distribute copies of free software (and charge for\nthem if you wish), that you receive source code or can get it if you\nwant it, that you can change the software or use pieces of it in new\nfree programs, and that you know you can do these things.\n\n  Developers that use our General Public Licenses protect your rights\nwith two steps: (1) assert copyright on the software, and (2) offer\nyou this License which gives you legal permission to copy, distribute\nand/or modify the software.\n\n  A secondary benefit of defending all users' freedom is that\nimprovements made in alternate versions of the program, if they\nreceive widespread use, become available for other developers to\nincorporate.  Many developers of free software are heartened and\nencouraged by the resulting cooperation.  However, in the case of\nsoftware used on network servers, this result may fail to come about.\nThe GNU General Public License permits making a modified version and\nletting the public access it on a server without ever releasing its\nsource code to the public.\n\n  The GNU Affero General Public License is designed specifically to\nensure that, in such cases, the modified source code becomes available\nto the community.  It requires the operator of a network server to\nprovide the source code of the modified version running there to the\nusers of that server.  Therefore, public use of a modified version, on\na publicly accessible server, gives the public access to the source\ncode of the modified version.\n\n  An older license, called the Affero General Public License and\npublished by Affero, was designed to accomplish similar goals.  This is\na different license, not a version of the Affero GPL, but Affero has\nreleased a new version of the Affero GPL which permits relicensing under\nthis license.\n\n  The precise terms and conditions for copying, distribution and\nmodification follow.\n\n                       TERMS AND CONDITIONS\n\n  0. Definitions.\n\n  \"This License\" refers to version 3 of the GNU Affero General Public License.\n\n  \"Copyright\" also means copyright-like laws that apply to other kinds of\nworks, such as semiconductor masks.\n\n  \"The Program\" refers to any copyrightable work licensed under this\nLicense.  Each licensee is addressed as \"you\".  \"Licensees\" and\n\"recipients\" may be individuals or organizations.\n\n  To \"modify\" a work means to copy from or adapt all or part of the work\nin a fashion requiring copyright permission, other than the making of an\nexact copy.  The resulting work is called a \"modified version\" of the\nearlier work or a work \"based on\" the earlier work.\n\n  A \"covered work\" means either the unmodified Program or a work based\non the Program.\n\n  To \"propagate\" a work means to do anything with it that, without\npermission, would make you directly or secondarily liable for\ninfringement under applicable copyright law, except executing it on a\ncomputer or modifying a private copy.  Propagation includes copying,\ndistribution (with or without modification), making available to the\npublic, and in some countries other activities as well.\n\n  To \"convey\" a work means any kind of propagation that enables other\nparties to make or receive copies.  Mere interaction with a user through\na computer network, with no transfer of a copy, is not conveying.\n\n  An interactive user interface displays \"Appropriate Legal Notices\"\nto the extent that it includes a convenient and prominently visible\nfeature that (1) displays an appropriate copyright notice, and (2)\ntells the user that there is no warranty for the work (except to the\nextent that warranties are provided), that licensees may convey the\nwork under this License, and how to view a copy of this License.  If\nthe interface presents a list of user commands or options, such as a\nmenu, a prominent item in the list meets this criterion.\n\n  1. Source Code.\n\n  The \"source code\" for a work means the preferred form of the work\nfor making modifications to it.  \"Object code\" means any non-source\nform of a work.\n\n  A \"Standard Interface\" means an interface that either is an official\nstandard defined by a recognized standards body, or, in the case of\ninterfaces specified for a particular programming language, one that\nis widely used among developers working in that language.\n\n  The \"System Libraries\" of an executable work include anything, other\nthan the work as a whole, that (a) is included in the normal form of\npackaging a Major Component, but which is not part of that Major\nComponent, and (b) serves only to enable use of the work with that\nMajor Component, or to implement a Standard Interface for which an\nimplementation is available to the public in source code form.  A\n\"Major Component\", in this context, means a major essential component\n(kernel, window system, and so on) of the specific operating system\n(if any) on which the executable work runs, or a compiler used to\nproduce the work, or an object code interpreter used to run it.\n\n  The \"Corresponding Source\" for a work in object code form means all\nthe source code needed to generate, install, and (for an executable\nwork) run the object code and to modify the work, including scripts to\ncontrol those activities.  However, it does not include the work's\nSystem Libraries, or general-purpose tools or generally available free\nprograms which are used unmodified in performing those activities but\nwhich are not part of the work.  For example, Corresponding Source\nincludes interface definition files associated with source files for\nthe work, and the source code for shared libraries and dynamically\nlinked subprograms that the work is specifically designed to require,\nsuch as by intimate data communication or control flow between those\nsubprograms and other parts of the work.\n\n  The Corresponding Source need not include anything that users\ncan regenerate automatically from other parts of the Corresponding\nSource.\n\n  The Corresponding Source for a work in source code form is that\nsame work.\n\n  2. Basic Permissions.\n\n  All rights granted under this License are granted for the term of\ncopyright on the Program, and are irrevocable provided the stated\nconditions are met.  This License explicitly affirms your unlimited\npermission to run the unmodified Program.  The output from running a\ncovered work is covered by this License only if the output, given its\ncontent, constitutes a covered work.  This License acknowledges your\nrights of fair use or other equivalent, as provided by copyright law.\n\n  You may make, run and propagate covered works that you do not\nconvey, without conditions so long as your license otherwise remains\nin force.  You may convey covered works to others for the sole purpose\nof having them make modifications exclusively for you, or provide you\nwith facilities for running those works, provided that you comply with\nthe terms of this License in conveying all material for which you do\nnot control copyright.  Those thus making or running the covered works\nfor you must do so exclusively on your behalf, under your direction\nand control, on terms that prohibit them from making any copies of\nyour copyrighted material outside their relationship with you.\n\n  Conveying under any other circumstances is permitted solely under\nthe conditions stated below.  Sublicensing is not allowed; section 10\nmakes it unnecessary.\n\n  3. Protecting Users' Legal Rights From Anti-Circumvention Law.\n\n  No covered work shall be deemed part of an effective technological\nmeasure under any applicable law fulfilling obligations under article\n11 of the WIPO copyright treaty adopted on 20 December 1996, or\nsimilar laws prohibiting or restricting circumvention of such\nmeasures.\n\n  When you convey a covered work, you waive any legal power to forbid\ncircumvention of technological measures to the extent such circumvention\nis effected by exercising rights under this License with respect to\nthe covered work, and you disclaim any intention to limit operation or\nmodification of the work as a means of enforcing, against the work's\nusers, your or third parties' legal rights to forbid circumvention of\ntechnological measures.\n\n  4. Conveying Verbatim Copies.\n\n  You may convey verbatim copies of the Program's source code as you\nreceive it, in any medium, provided that you conspicuously and\nappropriately publish on each copy an appropriate copyright notice;\nkeep intact all notices stating that this License and any\nnon-permissive terms added in accord with section 7 apply to the code;\nkeep intact all notices of the absence of any warranty; and give all\nrecipients a copy of this License along with the Program.\n\n  You may charge any price or no price for each copy that you convey,\nand you may offer support or warranty protection for a fee.\n\n  5. Conveying Modified Source Versions.\n\n  You may convey a work based on the Program, or the modifications to\nproduce it from the Program, in the form of source code under the\nterms of section 4, provided that you also meet all of these conditions:\n\n    a) The work must carry prominent notices stating that you modified\n    it, and giving a relevant date.\n\n    b) The work must carry prominent notices stating that it is\n    released under this License and any conditions added under section\n    7.  This requirement modifies the requirement in section 4 to\n    \"keep intact all notices\".\n\n    c) You must license the entire work, as a whole, under this\n    License to anyone who comes into possession of a copy.  This\n    License will therefore apply, along with any applicable section 7\n    additional terms, to the whole of the work, and all its parts,\n    regardless of how they are packaged.  This License gives no\n    permission to license the work in any other way, but it does not\n    invalidate such permission if you have separately received it.\n\n    d) If the work has interactive user interfaces, each must display\n    Appropriate Legal Notices; however, if the Program has interactive\n    interfaces that do not display Appropriate Legal Notices, your\n    work need not make them do so.\n\n  A compilation of a covered work with other separate and independent\nworks, which are not by their nature extensions of the covered work,\nand which are not combined with it such as to form a larger program,\nin or on a volume of a storage or distribution medium, is called an\n\"aggregate\" if the compilation and its resulting copyright are not\nused to limit the access or legal rights of the compilation's users\nbeyond what the individual works permit.  Inclusion of a covered work\nin an aggregate does not cause this License to apply to the other\nparts of the aggregate.\n\n  6. Conveying Non-Source Forms.\n\n  You may convey a covered work in object code form under the terms\nof sections 4 and 5, provided that you also convey the\nmachine-readable Corresponding Source under the terms of this License,\nin one of these ways:\n\n    a) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by the\n    Corresponding Source fixed on a durable physical medium\n    customarily used for software interchange.\n\n    b) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by a\n    written offer, valid for at least three years and valid for as\n    long as you offer spare parts or customer support for that product\n    model, to give anyone who possesses the object code either (1) a\n    copy of the Corresponding Source for all the software in the\n    product that is covered by this License, on a durable physical\n    medium customarily used for software interchange, for a price no\n    more than your reasonable cost of physically performing this\n    conveying of source, or (2) access to copy the\n    Corresponding Source from a network server at no charge.\n\n    c) Convey individual copies of the object code with a copy of the\n    written offer to provide the Corresponding Source.  This\n    alternative is allowed only occasionally and noncommercially, and\n    only if you received the object code with such an offer, in accord\n    with subsection 6b.\n\n    d) Convey the object code by offering access from a designated\n    place (gratis or for a charge), and offer equivalent access to the\n    Corresponding Source in the same way through the same place at no\n    further charge.  You need not require recipients to copy the\n    Corresponding Source along with the object code.  If the place to\n    copy the object code is a network server, the Corresponding Source\n    may be on a different server (operated by you or a third party)\n    that supports equivalent copying facilities, provided you maintain\n    clear directions next to the object code saying where to find the\n    Corresponding Source.  Regardless of what server hosts the\n    Corresponding Source, you remain obligated to ensure that it is\n    available for as long as needed to satisfy these requirements.\n\n    e) Convey the object code using peer-to-peer transmission, provided\n    you inform other peers where the object code and Corresponding\n    Source of the work are being offered to the general public at no\n    charge under subsection 6d.\n\n  A separable portion of the object code, whose source code is excluded\nfrom the Corresponding Source as a System Library, need not be\nincluded in conveying the object code work.\n\n  A \"User Product\" is either (1) a \"consumer product\", which means any\ntangible personal property which is normally used for personal, family,\nor household purposes, or (2) anything designed or sold for incorporation\ninto a dwelling.  In determining whether a product is a consumer product,\ndoubtful cases shall be resolved in favor of coverage.  For a particular\nproduct received by a particular user, \"normally used\" refers to a\ntypical or common use of that class of product, regardless of the status\nof the particular user or of the way in which the particular user\nactually uses, or expects or is expected to use, the product.  A product\nis a consumer product regardless of whether the product has substantial\ncommercial, industrial or non-consumer uses, unless such uses represent\nthe only significant mode of use of the product.\n\n  \"Installation Information\" for a User Product means any methods,\nprocedures, authorization keys, or other information required to install\nand execute modified versions of a covered work in that User Product from\na modified version of its Corresponding Source.  The information must\nsuffice to ensure that the continued functioning of the modified object\ncode is in no case prevented or interfered with solely because\nmodification has been made.\n\n  If you convey an object code work under this section in, or with, or\nspecifically for use in, a User Product, and the conveying occurs as\npart of a transaction in which the right of possession and use of the\nUser Product is transferred to the recipient in perpetuity or for a\nfixed term (regardless of how the transaction is characterized), the\nCorresponding Source conveyed under this section must be accompanied\nby the Installation Information.  But this requirement does not apply\nif neither you nor any third party retains the ability to install\nmodified object code on the User Product (for example, the work has\nbeen installed in ROM).\n\n  The requirement to provide Installation Information does not include a\nrequirement to continue to provide support service, warranty, or updates\nfor a work that has been modified or installed by the recipient, or for\nthe User Product in which it has been modified or installed.  Access to a\nnetwork may be denied when the modification itself materially and\nadversely affects the operation of the network or violates the rules and\nprotocols for communication across the network.\n\n  Corresponding Source conveyed, and Installation Information provided,\nin accord with this section must be in a format that is publicly\ndocumented (and with an implementation available to the public in\nsource code form), and must require no special password or key for\nunpacking, reading or copying.\n\n  7. Additional Terms.\n\n  \"Additional permissions\" are terms that supplement the terms of this\nLicense by making exceptions from one or more of its conditions.\nAdditional permissions that are applicable to the entire Program shall\nbe treated as though they were included in this License, to the extent\nthat they are valid under applicable law.  If additional permissions\napply only to part of the Program, that part may be used separately\nunder those permissions, but the entire Program remains governed by\nthis License without regard to the additional permissions.\n\n  When you convey a copy of a covered work, you may at your option\nremove any additional permissions from that copy, or from any part of\nit.  (Additional permissions may be written to require their own\nremoval in certain cases when you modify the work.)  You may place\nadditional permissions on material, added by you to a covered work,\nfor which you have or can give appropriate copyright permission.\n\n  Notwithstanding any other provision of this License, for material you\nadd to a covered work, you may (if authorized by the copyright holders of\nthat material) supplement the terms of this License with terms:\n\n    a) Disclaiming warranty or limiting liability differently from the\n    terms of sections 15 and 16 of this License; or\n\n    b) Requiring preservation of specified reasonable legal notices or\n    author attributions in that material or in the Appropriate Legal\n    Notices displayed by works containing it; or\n\n    c) Prohibiting misrepresentation of the origin of that material, or\n    requiring that modified versions of such material be marked in\n    reasonable ways as different from the original version; or\n\n    d) Limiting the use for publicity purposes of names of licensors or\n    authors of the material; or\n\n    e) Declining to grant rights under trademark law for use of some\n    trade names, trademarks, or service marks; or\n\n    f) Requiring indemnification of licensors and authors of that\n    material by anyone who conveys the material (or modified versions of\n    it) with contractual assumptions of liability to the recipient, for\n    any liability that these contractual assumptions directly impose on\n    those licensors and authors.\n\n  All other non-permissive additional terms are considered \"further\nrestrictions\" within the meaning of section 10.  If the Program as you\nreceived it, or any part of it, contains a notice stating that it is\ngoverned by this License along with a term that is a further\nrestriction, you may remove that term.  If a license document contains\na further restriction but permits relicensing or conveying under this\nLicense, you may add to a covered work material governed by the terms\nof that license document, provided that the further restriction does\nnot survive such relicensing or conveying.\n\n  If you add terms to a covered work in accord with this section, you\nmust place, in the relevant source files, a statement of the\nadditional terms that apply to those files, or a notice indicating\nwhere to find the applicable terms.\n\n  Additional terms, permissive or non-permissive, may be stated in the\nform of a separately written license, or stated as exceptions;\nthe above requirements apply either way.\n\n  8. Termination.\n\n  You may not propagate or modify a covered work except as expressly\nprovided under this License.  Any attempt otherwise to propagate or\nmodify it is void, and will automatically terminate your rights under\nthis License (including any patent licenses granted under the third\nparagraph of section 11).\n\n  However, if you cease all violation of this License, then your\nlicense from a particular copyright holder is reinstated (a)\nprovisionally, unless and until the copyright holder explicitly and\nfinally terminates your license, and (b) permanently, if the copyright\nholder fails to notify you of the violation by some reasonable means\nprior to 60 days after the cessation.\n\n  Moreover, your license from a particular copyright holder is\nreinstated permanently if the copyright holder notifies you of the\nviolation by some reasonable means, this is the first time you have\nreceived notice of violation of this License (for any work) from that\ncopyright holder, and you cure the violation prior to 30 days after\nyour receipt of the notice.\n\n  Termination of your rights under this section does not terminate the\nlicenses of parties who have received copies or rights from you under\nthis License.  If your rights have been terminated and not permanently\nreinstated, you do not qualify to receive new licenses for the same\nmaterial under section 10.\n\n  9. Acceptance Not Required for Having Copies.\n\n  You are not required to accept this License in order to receive or\nrun a copy of the Program.  Ancillary propagation of a covered work\noccurring solely as a consequence of using peer-to-peer transmission\nto receive a copy likewise does not require acceptance.  However,\nnothing other than this License grants you permission to propagate or\nmodify any covered work.  These actions infringe copyright if you do\nnot accept this License.  Therefore, by modifying or propagating a\ncovered work, you indicate your acceptance of this License to do so.\n\n  10. Automatic Licensing of Downstream Recipients.\n\n  Each time you convey a covered work, the recipient automatically\nreceives a license from the original licensors, to run, modify and\npropagate that work, subject to this License.  You are not responsible\nfor enforcing compliance by third parties with this License.\n\n  An \"entity transaction\" is a transaction transferring control of an\norganization, or substantially all assets of one, or subdividing an\norganization, or merging organizations.  If propagation of a covered\nwork results from an entity transaction, each party to that\ntransaction who receives a copy of the work also receives whatever\nlicenses to the work the party's predecessor in interest had or could\ngive under the previous paragraph, plus a right to possession of the\nCorresponding Source of the work from the predecessor in interest, if\nthe predecessor has it or can get it with reasonable efforts.\n\n  You may not impose any further restrictions on the exercise of the\nrights granted or affirmed under this License.  For example, you may\nnot impose a license fee, royalty, or other charge for exercise of\nrights granted under this License, and you may not initiate litigation\n(including a cross-claim or counterclaim in a lawsuit) alleging that\nany patent claim is infringed by making, using, selling, offering for\nsale, or importing the Program or any portion of it.\n\n  11. Patents.\n\n  A \"contributor\" is a copyright holder who authorizes use under this\nLicense of the Program or a work on which the Program is based.  The\nwork thus licensed is called the contributor's \"contributor version\".\n\n  A contributor's \"essential patent claims\" are all patent claims\nowned or controlled by the contributor, whether already acquired or\nhereafter acquired, that would be infringed by some manner, permitted\nby this License, of making, using, or selling its contributor version,\nbut do not include claims that would be infringed only as a\nconsequence of further modification of the contributor version.  For\npurposes of this definition, \"control\" includes the right to grant\npatent sublicenses in a manner consistent with the requirements of\nthis License.\n\n  Each contributor grants you a non-exclusive, worldwide, royalty-free\npatent license under the contributor's essential patent claims, to\nmake, use, sell, offer for sale, import and otherwise run, modify and\npropagate the contents of its contributor version.\n\n  In the following three paragraphs, a \"patent license\" is any express\nagreement or commitment, however denominated, not to enforce a patent\n(such as an express permission to practice a patent or covenant not to\nsue for patent infringement).  To \"grant\" such a patent license to a\nparty means to make such an agreement or commitment not to enforce a\npatent against the party.\n\n  If you convey a covered work, knowingly relying on a patent license,\nand the Corresponding Source of the work is not available for anyone\nto copy, free of charge and under the terms of this License, through a\npublicly available network server or other readily accessible means,\nthen you must either (1) cause the Corresponding Source to be so\navailable, or (2) arrange to deprive yourself of the benefit of the\npatent license for this particular work, or (3) arrange, in a manner\nconsistent with the requirements of this License, to extend the patent\nlicense to downstream recipients.  \"Knowingly relying\" means you have\nactual knowledge that, but for the patent license, your conveying the\ncovered work in a country, or your recipient's use of the covered work\nin a country, would infringe one or more identifiable patents in that\ncountry that you have reason to believe are valid.\n\n  If, pursuant to or in connection with a single transaction or\narrangement, you convey, or propagate by procuring conveyance of, a\ncovered work, and grant a patent license to some of the parties\nreceiving the covered work authorizing them to use, propagate, modify\nor convey a specific copy of the covered work, then the patent license\nyou grant is automatically extended to all recipients of the covered\nwork and works based on it.\n\n  A patent license is \"discriminatory\" if it does not include within\nthe scope of its coverage, prohibits the exercise of, or is\nconditioned on the non-exercise of one or more of the rights that are\nspecifically granted under this License.  You may not convey a covered\nwork if you are a party to an arrangement with a third party that is\nin the business of distributing software, under which you make payment\nto the third party based on the extent of your activity of conveying\nthe work, and under which the third party grants, to any of the\nparties who would receive the covered work from you, a discriminatory\npatent license (a) in connection with copies of the covered work\nconveyed by you (or copies made from those copies), or (b) primarily\nfor and in connection with specific products or compilations that\ncontain the covered work, unless you entered into that arrangement,\nor that patent license was granted, prior to 28 March 2007.\n\n  Nothing in this License shall be construed as excluding or limiting\nany implied license or other defenses to infringement that may\notherwise be available to you under applicable patent law.\n\n  12. No Surrender of Others' Freedom.\n\n  If conditions are imposed on you (whether by court order, agreement or\notherwise) that contradict the conditions of this License, they do not\nexcuse you from the conditions of this License.  If you cannot convey a\ncovered work so as to satisfy simultaneously your obligations under this\nLicense and any other pertinent obligations, then as a consequence you may\nnot convey it at all.  For example, if you agree to terms that obligate you\nto collect a royalty for further conveying from those to whom you convey\nthe Program, the only way you could satisfy both those terms and this\nLicense would be to refrain entirely from conveying the Program.\n\n  13. Remote Network Interaction; Use with the GNU General Public License.\n\n  Notwithstanding any other provision of this License, if you modify the\nProgram, your modified version must prominently offer all users\ninteracting with it remotely through a computer network (if your version\nsupports such interaction) an opportunity to receive the Corresponding\nSource of your version by providing access to the Corresponding Source\nfrom a network server at no charge, through some standard or customary\nmeans of facilitating copying of software.  This Corresponding Source\nshall include the Corresponding Source for any work covered by version 3\nof the GNU General Public License that is incorporated pursuant to the\nfollowing paragraph.\n\n  Notwithstanding any other provision of this License, you have\npermission to link or combine any covered work with a work licensed\nunder version 3 of the GNU General Public License into a single\ncombined work, and to convey the resulting work.  The terms of this\nLicense will continue to apply to the part which is the covered work,\nbut the work with which it is combined will remain governed by version\n3 of the GNU General Public License.\n\n  14. Revised Versions of this License.\n\n  The Free Software Foundation may publish revised and/or new versions of\nthe GNU Affero General Public License from time to time.  Such new versions\nwill be similar in spirit to the present version, but may differ in detail to\naddress new problems or concerns.\n\n  Each version is given a distinguishing version number.  If the\nProgram specifies that a certain numbered version of the GNU Affero General\nPublic License \"or any later version\" applies to it, you have the\noption of following the terms and conditions either of that numbered\nversion or of any later version published by the Free Software\nFoundation.  If the Program does not specify a version number of the\nGNU Affero General Public License, you may choose any version ever published\nby the Free Software Foundation.\n\n  If the Program specifies that a proxy can decide which future\nversions of the GNU Affero General Public License can be used, that proxy's\npublic statement of acceptance of a version permanently authorizes you\nto choose that version for the Program.\n\n  Later license versions may give you additional or different\npermissions.  However, no additional obligations are imposed on any\nauthor or copyright holder as a result of your choosing to follow a\nlater version.\n\n  15. Disclaimer of Warranty.\n\n  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY\nAPPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT\nHOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY\nOF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,\nTHE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\nPURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM\nIS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF\nALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n  16. Limitation of Liability.\n\n  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\nWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS\nTHE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY\nGENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE\nUSE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF\nDATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD\nPARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),\nEVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF\nSUCH DAMAGES.\n\n  17. Interpretation of Sections 15 and 16.\n\n  If the disclaimer of warranty and limitation of liability provided\nabove cannot be given local legal effect according to their terms,\nreviewing courts shall apply local law that most closely approximates\nan absolute waiver of all civil liability in connection with the\nProgram, unless a warranty or assumption of liability accompanies a\ncopy of the Program in return for a fee.\n\n                     END OF TERMS AND CONDITIONS\n\n            How to Apply These Terms to Your New Programs\n\n  If you develop a new program, and you want it to be of the greatest\npossible use to the public, the best way to achieve this is to make it\nfree software which everyone can redistribute and change under these terms.\n\n  To do so, attach the following notices to the program.  It is safest\nto attach them to the start of each source file to most effectively\nstate the exclusion of warranty; and each file should have at least\nthe \"copyright\" line and a pointer to where the full notice is found.\n\n    <one line to give the program's name and a brief idea of what it does.>\n    Copyright (C) <year>  <name of author>\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU Affero General Public License as published\n    by the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU Affero General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\nAlso add information on how to contact you by electronic and paper mail.\n\n  If your software can interact with users remotely through a computer\nnetwork, you should also make sure that it provides a way for users to\nget its source.  For example, if your program is a web application, its\ninterface could display a \"Source\" link that leads users to an archive\nof the code.  There are many ways you could offer source, and different\nsolutions will be better for different programs; see section 13 for the\nspecific requirements.\n\n  You should also get your employer (if you work as a programmer) or school,\nif any, to sign a \"copyright disclaimer\" for the program, if necessary.\nFor more information on this, and how to apply and follow the GNU AGPL, see\n<https://www.gnu.org/licenses/>.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 30.7734375,
          "content": "> [!NOTE]  \n> # Timeliner is being deprecated in favor of its successor, [Timelinize](https://timelinize.com). New development on this repo has stopped.\n>\n> Don't worry though&mdash;Timelinize is a huge improvement:\n> - **A web UI for viewing your data, finally!**\n> - CLI and JSON HTTP API for integrations, scripting, and automations.\n> - Same overall simple architecture: SQLite DB alongside a folder with your files.\n> - Open schema and self-hosted! You always have complete control over and access to your data. No DRM.\n> - Many more data sources.\n> - Major performance improvements.\n> - Bug fixes.\n>\n> **If you are interested in alpha testing Timelinize during the early developer previews, please let me know! I just ask that you be actively involved with our community on Discord during development and offer your feedback/ideas.** You can find my email address on the Timelinize website.\n> \n> ## Timeliner :arrow_right: Timelinize FAQ\n>\n> ### Where is it? What is the status of Timelinize?\n> Timelinize is still undergoing heavy development and is not yet release-worthy, but I have alpha versions that developers can preview if you wish. You can request access in an issue or via email. Timelinize has [a basic website](https://timelinize.com) now. The email address to reach me at is on that site.\n>\n> **You may wonder, why are there not more screenshots!?** A valid question. I would LOVE to show you but all the data I have to develop with is my own personal data, of course! It's hard to find parts of it that are not sensitive. I'm working on ways to either generate fake data or obfuscate real data for demo purposes. Then there will be more screenshots.\n> \n> ### Why a new project?\n> Timeliner was originally a project called Photobak, for backing up your photos from Picasa (then Google Photos). Then I realized that I wanted to back up all the photos I put on Facebook, Twitter, and Instagram. And while I was at it, why not my Google Location History? The project evolved enough to warrant a new name, hence Timeliner. Well, the next evolution is here, and I feel like it finally starts to live up to the vision I originally had after Photobak: a complete, highly detailed archive of my digital content and life and that of my family's. The overall scope changed enough that it warranted a new name.\n>\n> Perhaps the biggest reason is the development of a UI, which hopefully makes the project accessible to many more people like my own family members. There are also technical reasons. Over the years I've found that Timeliner's method of ingesting data via API downloads is brittle, as services cut off free API access (see Twitter/X), or strip critical data when using APIs (see Google Photos). This necessitated hacking in alternate ways of augmenting one's timeline, e.g. importing a Google Takeout to replace the missing data from Google Photos' API. This is tedious and inelegant. Learning from lessons like this, I've redesigned Timelinize to be more flexible going forward.\n> \n> ### Will Timelinize be open source?\n> Undecided. The schema is for sure going to be open. The project has taken an _exceptional_ amount of my time over the last decade, and that cost may be too high for me if everyone uses it for free. Because self-hosting is one of the primary values of this project, and I don't even _want_ to host your private data, selling a hosted version of the app as a way to make money isn't a favorable option. If Timelinize doesn't end up open source, it will likely have a very generous free tier.\n>\n> ### Will Timelinize send any of my data to someone's servers or store it anywhere else?\n> NO. Just like Timeliner, all your data is stored on your own computer and it doesn't go through any remote servers when you use Timelinize. You can verify this with tools like Wireshark. In the future, we have plans to implement sharing features so you can securely choose parts of your data/timeline to share with peers like your friends and family. I'm hoping that through technologies like Wireguard, Tailscale, or OpenZiti/zrok, we can implement those sharing features directly P2P, and the only interactions with our servers would be to coordinate the sharing and permissions.\n> \n> ### Can Timelinize do everything Timeliner does?\n> Fundamentally, yes. The result is achieved differently though; in particular, API downloads have been shelved for now. To explain: Timeliner downloads data from APIs. Unfortunately, we've seen in the last several years that API offerings are brittle. Twitter/X cut off all free access, even to your own data; Google Photos strips location data from your own photos when downloaded via the API; etc. So while Timelinize has the ability to do API downloads, its primary method of ingesting data for now is importing files on disk, usually obtained from services' \"takeout\" or \"export\" services such as like Google Takeout or \"Download your account data\" pages, which has to be done manually. So, Timelinize is a little less \"set it and forget it,\" but you will likely only need to do major imports about once every year or so. And I'm experimenting with ways to automate those, too.\n>\n> ### How different is Timelinize?\n> Well, the fact that it has a graphical UI makes it hugely different. But it still feels familiar. It has the same basic architecture: you import data that gets stored in a SQLite DB on your computer, except for binary data that gets stored in a folder adjacent to it. Everything is indexed and searchable. It's organized primarily by time. The main difference is it's much more capable.\n>\n> ### What are some improvements that Timelinize makes over Timeliner?\n> The schema has been greatly improved. Instead of focusing only on items as data (with an associated \"person\" for each item), Timelinize is more fully \"entity-aware.\" An entity is a person, place, pet/animal, organization, etc; and each entity can have 1 or more defining attributes. An attribute may be like an email or phone number, account ID or username. Some attributes are identifying. This enables us to represent the same entity using multiple identifiers, which is useful when crossing data sources. You'll find that we can automatically connect the Sally Jane on your email, for example, as the same Sally Jane in your text messages, even if her display name is different (and if we don't, merging two duplicate entities is a cinch)! Attributes can also map to multiple entities, for example if two people share an email address, we can represent that.\n>\n> Another schema improvement is the representation of items. They now have multiple time fields: timestamp, timespan, timeframe, time_offset, and time_uncertainty. Combining these allows us to represent items that span time, have an uncertain time, have different time zones, or take place at an unknown time within some timeframe. (This is useful for location data when traveling, importing scans of old photos, etc.) We also store the original path or location of each item as well as its intermediate path or location. For example, items imported from iPhones have a location as originally found on the actual iPhone; they have a different path in the iPhone backup the data was imported from. We preserve both now so you can better trace back an item's origins.\n>\n> Timelinize imports data much faster as well.\n>\n> ### How can I view my data using Timelinize?\n> The web UI will launch with at least 6 ways to view your data:\n> 1. Timeline view\n> 2. Map view\n> 3. Conversation view\n> 4. Gallery view\n> 5. Raw item list\n> 6. Raw entity list\n>\n> The first four are \"projections\" of your data into a certain paradigm. A possible future projection may be a calendar. I'm sure we'll think of more, too.\n>\n> ### Is Timelinize a rewrite?\n> Technically yes, as I started with an empty `main()`. However, I brought over a lot of the code from Timeliner file-by-file. However... I ended up changing a lot of it, and completely rewrote the import logic into an all-new pipeline to improve performance and correctness. So the fundamental code concepts are still mostly intact (ItemGraph, Person, PersonAttribute, etc. -- though they have names like Graph, Entity, and Attribute now). I would say, \"Much of it has been rewritten.\"\n> \n> ### What is different about the CLI?\n> The Timelinize web UI uses a JSON HTTP API for its functionality. That same API is available for you to use, and from that, we also auto-generate a CLI. That means you can completely operate Timelinize through its CLI as much as its GUI and its API. Pretty cool! But yes, Timelinize is very much a breaking change over Timeliner.\n>\n> ### Will I be able to port my Timeliner repo to a Timelinize repo?\n> No, but that's actually a good thing, since Timelinize timelines are much more capable and detailed. Even if I did write code to port Timeliner data to Timelinize, you'd lose the magic of what Timelinize can offer.\n>\n> ### Do you have an ETA?\n> I do not have a timeline. The irony of this is not lost on me.\n\n\nOriginal docs (DEPRECATED) [![timeliner godoc](https://pkg.go.dev/badge/github.com/mholt/timeliner)](https://pkg.go.dev/github.com/mholt/timeliner)\n==========================\n\n\n\nTimeliner is a personal data aggregation utility. It collects all your digital things from pretty much anywhere and stores them on your own computer, indexes them, and projects them onto a single, unified timeline.\n\nThe intended purpose of this tool is to help preserve personal and family history.\n\nThings that are stored by Timeliner are easily accessible in a SQLite database or, for media items like photos and videos, are simply plopped onto your disk as regular files, organized in folders by date and data source.\n\n**WIP Notice:** This project works as documented for the most part, but is still very young. Please consider this experimental until stable releases. The documentation needs a lot of work too, I know... so feel free to contribute!\n\n\n## About\n\nIn general, Timeliner obtains _items_ from _data sources_ and stores them in a _timeline_.\n\n- **Items** are anything that has content: text, image, video, etc. For example: photos, tweets, social media posts, even locations.\n- **Data sources** are anything that can provide a list of items. For example: social media sites, online services, archive files, etc.\n- **Timelines** are repositories that store the data. Typically, you will have one timeline that is your own, but timelines can support multiple people and multiple accounts per person if you desire to share it.\n\nTechnically speaking:\n\n- An **Item** implements [this interface](https://pkg.go.dev/github.com/mholt/timeliner#Item) and provides access to its content and metadata.\n- A **DataSource** is defined by [this struct](https://pkg.go.dev/github.com/mholt/timeliner#DataSource) which configures a [Client](https://pkg.go.dev/github.com/mholt/timeliner#Client) to access it (by its `NewClient` field). Clients are the types that do the actual work of listing of items.\n- A **Timeline** is opened when being used. It consists of an underlying SQLite database and an adjacent data folder where larger/media items are stored as files. Timelines are essentially the folder that contains them. They are portable, so you can move them around and won't break things. However, don't change the contents of the folder directly! Don't add, remove, or modify items in the folder; you will break something. This does not mean timelines are read-only: they just have to be modified through the program in order to stay consistent.\n\nTimeliner can pull data in from local or remote sources. It provides integrated support for OAuth2 and rate limiting where that is needed. It can also import data from local files. For example, some online services let you download all your data as an archive file. Timeliner can read those and index your data.\n\nTimeliner data sources are strictly _read-only_ meaning that no write permissions are needed and Timeliner will never change or delete from the source.\n\n## Features\n\n- Supported data sources\n\t- [Facebook](https://github.com/mholt/timeliner/wiki/Data-Source:-Facebook)\n\t- [Google Location History](https://github.com/mholt/timeliner/wiki/Data-Source:-Google-Location-History)\n\t- [Google Photos](https://github.com/mholt/timeliner/wiki/Data-Source:-Google-Photos)\n\t- [Twitter](https://github.com/mholt/timeliner/wiki/Data-Source:-Twitter)\n\t- [Instagram](https://github.com/mholt/timeliner/wiki/Data-Source:-Instagram)\n\t- [SMS Backup & Restore](https://github.com/mholt/timeliner/wiki/Data-Source:-SMS-Backup-&-Restore)\n\t- **[Learn how to add more](https://github.com/mholt/timeliner/wiki/Writing-a-Data-Source)** - please contribute!\n- Checkpointing (resume interrupted downloads)\n- Pruning\n- Integrity checks\n- Deduplication\n- Timeframing\n- Differential reprocessing (only re-process items that have changed on the source)\n- Construct graph-like relationships between items and people\n- Memory-efficient for high-volume data processing\n- Built-in rate limiting for API clients\n- Built-in OAuth2 facilities for API clients\n- Configurable data merging behavior for similar/identical items\n- Ability to get and organize data from... almost anything, really, including export files\n\nSome features are dependent upon the actual implementation of each data source. For example, differential reprocessing requires that the data source provide some sort of checksum or \"ETag\" for the item, but if that is not available, there's no way to know if an item has changed remotely without downloading the whole thing and reprocessing it.\n\n\n## Install\n\n**Minimum Go version required:** Go 1.13\n\nClone this repository, then from the project folder, run:\n\n```\n$ cd cmd/timeliner\n$ go build\n```\n\nThen move the resulting executable into your PATH.\n\n\n\n## Command line interface\n\nThis is a quick reference only. Be sure to read the tutorial below to learn how to use the program!\n\n```\n$ timeliner [<flags...>] <command> <args...>\n```\n\nUse `timeliner -h` to see available flags.\n\n### Commands\n\n- **`add-account`** adds a new account to the timeline and, if relevant, authenticates with the data source so that items can be obtained from an API. This only has to be done once per account per data source:\n\t```\n\t$ timeliner add-account <data_source>/<username>...\n\t```\n\tIf the data source requires authentication (for example with OAuth), be sure the config file is properly created first.\n- **`reauth`** re-authenticates with a data source. This is only necessary on some data sources that expire auth leases after some time:\n\t```\n\t$ timeliner reauth <data_source>/<username>...\n\t```\n- **`import`** adds items from a local file:\n\t```\n\t$ timeliner import <filename> <data_source>/<username>\n\t```\n- **`get-all`** adds items from the service's API.\n\t```\n\t$ timeliner get-all <data_source>/<username>...\n\t```\n- **`get-latest`** adds only the latest items from the service's API (since the last checkpoint):\n\t```\n\t$ timeliner get-latest <data_source>/<username>...\n\t```\n\nFlags can be used to constrain or customize the behavior of commands (`timeliner -h` to list flags).\n\nSee the [wiki page for your data sources](https://github.com/mholt/timeliner/wiki) to know how to use the various data sources.\n\n\n## Tutorial\n\n_After you've read this tutorial, [the Timeliner wiki](https://github.com/mholt/timeliner/wiki/) has all the information you'll need for using each data source._\n\nThese are the basic steps for getting set up:\n\n1. Create a `timeliner.toml` config file (if any data sources require authentication)\n2. Add your data source accounts\n3. Fill your timeline\n\nAll items are associated with an account from whence they come. Even if a data source doesn't have the concept of accounts, Timeliner still has to think there is one.\n\nAccounts are designated in the form `<data source ID>/<user ID>`, for example: `twitter/mholt6`. The data source ID is shown on each data source's [wiki page](https://github.com/mholt/timeliner/wiki/). With some data sources (like the Twitter API), the user ID matters; so where possible, give the actual username or email address you use with that service. For data sources that don't have the concept of accounts or a login, choose a user ID you will recognize such that the data source ID + user ID are unique.\n\nIf we want to use accounts that require OAuth2, we need to configure Timeliner with OAuth2 app credentials. You can learn which data sources need OAuth2 and what their configuration looks like by reading their [wiki page](https://github.com/mholt/timeliner/wiki/). By default, Timeliner will try to load `timeliner.toml` from the current directory, but you can use the `-config` flag to change that. Here's a sample `timeliner.toml` file for authenticating with Google:\n\n```\n[oauth2.providers.google]\nclient_id = \"YOUR_APP_ID\"\nclient_secret = \"YOUR_APP_SECRET\"\nauth_url = \"https://accounts.google.com/o/oauth2/auth\"\ntoken_url = \"https://accounts.google.com/o/oauth2/token\"\n```\n\nWith that file in place, let's create an account to store our Google Photos:\n\n```\n$ timeliner add-account google_photos/you@gmail.com\n```\n\nThis will open your browser window to authenticate with OAuth2.\n\nYou will notice that a folder called `timeliner_repo` was created in the current directory. This is your timeline. You can move it around if you want, and then use the `-repo` flag to work with that timeline.\n\nNow let's get all our stuff from Google Photos. And I mean, _all_ of it. It's ours, after all:\n\n```\n$ timeliner get-all google_photos/you@gmail.com\n```\n\n(You can list multiple accounts on a single command, except `import` commands.)\n\nThis process can take weeks if you have a large library. Even if you have a fast Internet connection, the client is carefully rate-limited to be a good API citizen, so the process will be slow.\n\nIf you open your timeline folder in a file browser, you will see it start to fill up with your photos from Google Photos. To see more verbose logging, use the `-v` flag (NOTE: this will drastically slow down processing that isn't bottlenecked by the network).\n\nData sources may create checkpoints as they go. If so, `get-all` or `get-latest` will automatically resume the last listing if it was interrupted, but only if the same command is repeated (you can't resume a `get-latest` with `get-all`, for example, or with different timeframe parameters). In the case of Google Photos, each page of API results is checkpointed. Checkpoints are not intended for long-term pauses. In other words, a resume should happen fairly shortly after being interrupted, and should be resumed using the same command as before. (A checkpoint will be automatically resumed only if the command parameters are identical.)\n\nItem processing is idempotent, so as long as items have faithfully-unique IDs from their account, items that already exist in the timeline will be skipped and/or processed much faster.\n\n\n### Constraining within a timeframe\n\nYou can use the `-start` and `-end` flags to specify either absolute dates within which to constrain data collection, or with [duration values](https://golang.org/pkg/time/#ParseDuration) to specify a date relative to the current timestamp. These flags appear before the subcommand.\n\nTo get all the items newer than a certain date:\n\n\n```\n$ timeliner -start=2019/07/1 get-all ...\n```\n\nThis will get all items dated July 1, 2019 or newer.\n\nTo get all items older than certain date:\n\n```\n$ timeliner -end=2020/02/29 get-all ...\n```\n\nThis processes all items before February 29, 2020.\n\nTo create a bounded window, use both:\n\n```\n$ timeliner -start=2019/07/01 -end=2020/02/29 get-all ...\n```\n\nDurations can be used for relative dates. To get all items up to 30 days old:\n\n```\n$ timeliner -end=-720h get-all ...\n```\n\nNotice how the duration value is negative; this is because you want the end date to be 720 hours (30 days) in the past, not in the future.\n\n\n### Pulling the latest\n\nOnce your initial download completes, you can run Timeliner so that only the latest items are retrieved:\n\n```\n$ timeliner get-latest google_photos/you@gmail.com\n```\n\nThis will get only the items timestamped newer than the newest item in your timeline (from the last successful run).\n\nIf `get-latest` is interrupted after adding some newer items to the timeline, the next run of `get-latest` will not stop at the first new item added last time; it is smart enough to know that it was interrupted and needs to keep getting items all the way until the beginning of the last _successful_ run, as long as the command's parameters are the same. For example, re-running the last command will automatically resume where it left off; but changing the `-end` flag, for example, won't be able to resume.\n\nThis subcommand supports the `-end` flag, but not the `-start` flag (since the start is determined from the last downloaded item). One thing I like to do is use `-end=-720h` with my Google Photos to only download the latest photos that are at least 30 days old. This gives me a month to delete unwanted/duplicate photos from my cloud library before I store them on my computer permanently.\n\n\n### Duplicate items\n\nTimeliner often encounters the same items multiple times. By default, it skips items with the same ID as one already stored in the timeline because it is faster and more efficient, but you can also configure it to \"reprocess\" or \"merge\" duplicate items. These two concepts are distinct and important.\n\n**Reprocessing** is when Timeliner completely replaces an existing item with a new one.\n\n**Merging** is when Timeliner combines a new item's data with an existing item.\n\nNeither happen by default because they can be less efficient or cause undesired results. In other words: by default, Timeliner will only download and process and item once. This makes its `get-all`, `get-latest`, and `import` commands idempotent.\n\n#### Reprocessing\n\nReprocessing replaces items with the same ID. This happens if one of the following conditions is met:\n\n- You run with the `-integrity` flag which enables integrity checks, and an item's data file fails the integrity check. In that case, the item will be reprocessed to restore its correct data.\n\n- The item has changed on the data source _and the data source indicates this change somehow_. However, very few (if any?) data sources actually provide a hash or ETag to help us compare whether a resource has changed.\n\n- You run with the `-reprocess` flag. This does a \"full reprocess\" (or \"forced reprocess\") which indiscriminately reprocesses every item, just in case it changed. In other words, a forced reprocess will update your local copy with the source's latest for every item. This is often used because a data source might not provide enough information to automatically determine whether an item has changed. If you know you have changed your items on the data source, you could specify this flag to force Timeliner to update everything.\n\n\n#### Merging\n\nMerging combines two items without completely replacing the old item. Merges are additive: they'll never replace a field with a null value. By default, merges only add data that was missing and will not overwrite existing data (but this is configurable).\n\nIn theory, any two items can be merged, even if they don't have the same ID. Currently, the only way to trigger a merge is to enable \"soft merging\" which allows Timeliner to treat two items with different IDs as identical if ALL of these are true:\n\n- They have the same account (same data source)\n- They have the same timestamp\n- They have either the same text data OR the same data file name\n\nMerging can be enabled and customized with the `-merge` flag. This flag accepts a comma-separated list of merge options:\n\n- `soft` (required): Enables soft merging. Currently, this is the only way to enable merging at all.\n- `id`: Prefer new item's ID\n- `text`: Prefer new item's text data\n- `file`: Prefer new item's data file\n- `meta`: Prefer new item's metadata\n\n**Soft merging** simply updates the ID of either the existing, stored item or the new, incoming item to be the same as the other. (As with other fields, the ID of the existing item will be preferred by default, meaning the ID of the new item will be adjusted to match it.)\n\n**Example:** I often use soft merging with Google Photos. Because the Google Photos API strips location data (grrr), I also use Google Takeout to import an archive of my photos. This adds the location data. However, although the archive has coordinate data, it does NOT have IDs like the Google Photos API provides. Thus, soft merging prevents a duplication of my photo library in my timeline.\n\nTo illustrate, I schedule this command to run regularly:\n\n```\n$ timeliner -merge=soft,id,meta -end=-720h get-latest google_photos/me\n```\n\nThis uses the API to pull the latest photos up to 30 days old so I have time to delete unwanted photos from my library first. Notably, I enable soft merging and prefer the IDs and metadata given by the Google Photos API because they are richer and more precise.\n\nOccasionally I will use Takeout to download an archive to add location data to my timeline, which I import like this:\n\n```\n$ timeliner -merge=soft import takeout.tgz google_photos/me\n```\n\nNote that soft merging is still enabled, but I always prefer existing data when doing this because all I want to do is fill in the missing location data.\n\nThis pattern takes advantage of soft merging and allows me to completely back up my Photos library locally, complete with location data, using both the API and Google Takeout.\n\n\n### Pruning your timeline\n\nSuppose you downloaded a bunch of photos with Timeliner that you later deleted from Google Photos. Timeliner can remove those items from your local timeline, too, to save disk space and keep things clean.\n\nTo schedule a prune, just run with the `-prune` flag: \n\n```\n$ timeliner -prune get-all ...\n```\n\nHowever, this involves doing a complete listing of all the items. Pruning happens at the end. Any items not seen in the listing will be deleted. This also means that a full, uninterrupted listing is required, since resuming from a checkpoint yields an incomplete file listing. Pruning after a resumed listing will result in an error. (There's a TODO to improve this situation -- feel free to contribute! We just need to preserve the item listing along with the checkpoint.)\n\nBeware! If your timeline has extra items added from auxillary sources (for example, using `import` with an archive file in addition to the regular API pulls), the prune operation may not see those extra items and thus delete them. Always back up your timeline before doing a prune.\n\n\n### Reauthenticating with a data source\n\nSome data sources (Facebook) expire tokens that don't have recent user interactions. Every 2-3 months, you may need to reauthenticate:\n\n```\n$ timeliner reauth facebook/you\n```\n\nSee the [wiki](https://github.com/mholt/timeliner/wiki) for each data source to know if you need to reauthenticate and how to do so. Sometimes you have to go to the data source itself and authorize a reauthentication first.\n\n\n### More information about each data source\n\nCongratulations, you've [graduated to the wiki pages](https://github.com/mholt/timeliner/wiki) to learn more about how to set up and use each data source.\n\n\n\n## Motivation and long-term vision\n\nThe motivation for this project is two-fold. Both press upon me with a sense of urgency, which is why I dedicated some nights and weekends to work on this.\n\n1) Connecting with my family -- both living and deceased -- is important to me and my close relatives. But I wish we had more insights into the lives and values of those who came before us. What better time than right now to start collecting personal histories from all available sources and develop a rich timeline of our life for our family, and maybe even for our own reference or nostalgia.\n\n2) Our lives are better-documented than any before us, but the documentation is more ephemeral than any before us, too. We lose control of our data by relying on centralized, proprietary cloud services which are useful today, and gone tomorrow. I wrote Timeliner because now is the time to liberate my data from corporations who don't own it, yet who have the only copy of it. This reality has made me feel uneasy for years, and it's not going away soon. Timeliner makes it bearable.\n\nImagine being able to pull up a single screen with your data from any and all of your online accounts and services -- while offline. And there you see so many aspects of your life at a glance: your photos and videos, social media posts, locations on a map and how you got there, emails and letters, documents, health and physical activities, and even your GitHub projects (if you're like me), for any given day. You can \"zoom out\" and get the big picture. Machine learning algorithms could suggest major clusters based on your content to summarize your days, months, or years, and from that, even recommend printing physical memorabilia. It's like a highly-detailed, automated journal, fully in your control, which you can add to in the app: augment it with your own thoughts like a regular journal.\n\nThen cross-reference your own timeline with a global public timeline: see how locations you went to changed over time, or what major news events may have affected you, or what the political/social climate was like at the time.\n\nOr translate the projection sideways, and instead of looking at time cross-sections, look at cross-sections of your timeline by media type: photos, posts, location, sentiment. Look at plots, charts, graphs, of your physical activity.\n\nAnd all of this runs on your own computer: no one else has access to it, no one else owns it, but you.\n\n\n## Viewing your Timeline\n\n**UPDATE:** [Timelinize](https://timelinize.com) is the successor to this project, and it has a fully-featured graphical web UI to view and manage your timeline data!\n\n~~There is not yet a viewer for the timeline. For now, I've just been using [Table Plus](https://tableplus.io) to browse the SQLite database, and my file browser to look at the files in it. The important thing is that you have them, at least.~~\n\n~~However, a viewer would be really cool. It's something I've been wanting to do but don't have time for right now. Contributions are welcomed along these lines, but this feature _must_ be thoroughly discussed before any pull requests will be accepted to implement a timeline viewer. Thanks!~~\n\n\n## Notes\n\nYeah, I know this is very similar to what [Perkeep](https://perkeep.org/) does. Perkeep is a way cooler project in my opinion. However, Perkeep is more about storage and sync, whereas Timeliner is more focused on constructing relationships between items and projecting your digital life onto a single timeline. If Perkeep is my unified personal data storage, then Timeliner is my automatic journal. (Believe me, my heart sank after I realized that I was almost rewriting parts of Perkeep, until I decided that the two are different enough to warrant a separate project.)\n\n\n## License\n\nThis project is licensed with AGPL. I chose this license because I do not want others to make proprietary software using this package. The point of this project is liberation of and control over one's own, personal data, and I want to ensure that this project won't be used in anything that would perpetuate the walled garden dilemma we already face today. Even if this project's official successor has proprietary source code, I can ensure it will stay aligned with my values and the project's original goals.\n"
        },
        {
          "name": "account.go",
          "type": "blob",
          "size": 5.2802734375,
          "content": "package timeliner\n\nimport (\n\t\"bytes\"\n\t\"encoding/gob\"\n\t\"fmt\"\n\t\"net/http\"\n\t\"sync\"\n\t\"time\"\n)\n\n// Account represents an account with a service.\ntype Account struct {\n\tID            int64\n\tDataSourceID  string\n\tUserID        string\n\tperson        Person\n\tauthorization []byte\n\tcheckpoint    []byte\n\tlastItemID    *int64\n\n\tt  *Timeline\n\tds DataSource\n\tcp *checkpointWrapper\n}\n\n// NewHTTPClient returns an HTTP client that is suitable for use\n// with an API associated with the account's data source. If\n// OAuth2 is configured for the data source, the client has OAuth2\n// credentials. If a rate limit is configured, this client is\n// rate limited. A sane default timeout is set, and any fields\n// on the returned Client valule can be modified as needed.\nfunc (acc Account) NewHTTPClient() (*http.Client, error) {\n\thttpClient := new(http.Client)\n\tif acc.ds.OAuth2.ProviderID != \"\" {\n\t\tvar err error\n\t\thttpClient, err = acc.NewOAuth2HTTPClient()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tif acc.ds.RateLimit.RequestsPerHour > 0 {\n\t\thttpClient.Transport = acc.NewRateLimitedRoundTripper(httpClient.Transport)\n\t}\n\thttpClient.Timeout = 60 * time.Second\n\treturn httpClient, nil\n}\n\nfunc (acc Account) String() string {\n\treturn acc.DataSourceID + \"/\" + acc.UserID\n}\n\n// AddAccount authenticates userID with the service identified\n// within the application by dataSourceID, and then stores it in the\n// database. The account must not yet exist.\nfunc (t *Timeline) AddAccount(dataSourceID, userID string) error {\n\t// ensure account is not already stored in our system\n\tvar count int\n\terr := t.db.QueryRow(`SELECT COUNT(*) FROM accounts WHERE data_source_id=? AND user_id=? LIMIT 1`,\n\t\tdataSourceID, userID).Scan(&count)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"checking if account is already stored: %v\", err)\n\t}\n\tif count > 0 {\n\t\treturn fmt.Errorf(\"account already stored in database: %s/%s\", dataSourceID, userID)\n\t}\n\n\treturn t.Authenticate(dataSourceID, userID)\n}\n\n// Authenticate gets authentication for userID with dataSourceID. If the\n// account already exists in the database, it will be updated with the\n// latest authorization.\nfunc (t *Timeline) Authenticate(dataSourceID, userID string) error {\n\tds, ok := dataSources[dataSourceID]\n\tif !ok {\n\t\treturn fmt.Errorf(\"data source not registered: %s\", dataSourceID)\n\t}\n\n\t// authenticate with the data source (if necessary)\n\tvar credsBytes []byte\n\tvar err error\n\tif authFn := ds.authFunc(); authFn != nil {\n\t\tcredsBytes, err = authFn(userID)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"authenticating %s for %s: %v\", userID, dataSourceID, err)\n\t\t}\n\t}\n\n\t// make sure the data source is registered in the DB\n\t_, err = t.db.Exec(`INSERT OR IGNORE INTO data_sources (id, name) VALUES (?, ?)`,\n\t\tdataSourceID, ds.Name)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"saving data source record: %v\", err)\n\t}\n\n\t// store the account along with our authorization to access it\n\t_, err = t.db.Exec(`INSERT INTO accounts\n\t\t(data_source_id, user_id, authorization)\n\t\tVALUES (?, ?, ?)\n\t\tON CONFLICT (data_source_id, user_id)\n\t\tDO UPDATE SET authorization=?`,\n\t\tdataSourceID, userID, credsBytes,\n\t\tcredsBytes)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"inserting into or updating DB: %v\", err)\n\t}\n\n\treturn nil\n}\n\n// NewClient returns a new Client that is ready to interact with\n// the data source for the account uniquely specified by the data\n// source ID and the user ID for that data source. The Client is\n// actually wrapped by a type with unexported fields that are\n// necessary for internal use.\nfunc (t *Timeline) NewClient(dataSourceID, userID string) (WrappedClient, error) {\n\tds, ok := dataSources[dataSourceID]\n\tif !ok {\n\t\treturn WrappedClient{}, fmt.Errorf(\"data source not registered: %s\", dataSourceID)\n\t}\n\tif ds.NewClient == nil {\n\t\treturn WrappedClient{}, fmt.Errorf(\"impossible to make client for data source: %s\", dataSourceID)\n\t}\n\n\tacc, err := t.getAccount(dataSourceID, userID)\n\tif err != nil {\n\t\treturn WrappedClient{}, fmt.Errorf(\"getting account: %v\", err)\n\t}\n\n\tcl, err := ds.NewClient(acc)\n\tif err != nil {\n\t\treturn WrappedClient{}, fmt.Errorf(\"making client from data source: %v\", err)\n\t}\n\n\treturn WrappedClient{\n\t\tClient:     cl,\n\t\ttl:         t,\n\t\tacc:        acc,\n\t\tds:         ds,\n\t\tlastItemMu: new(sync.Mutex),\n\t}, nil\n}\n\nfunc (t *Timeline) getAccount(dsID, userID string) (Account, error) {\n\tds, ok := dataSources[dsID]\n\tif !ok {\n\t\treturn Account{}, fmt.Errorf(\"data source not registered: %s\", dsID)\n\t}\n\tacc := Account{\n\t\tds: ds,\n\t\tt:  t,\n\t}\n\terr := t.db.QueryRow(`SELECT\n\t\tid, data_source_id, user_id, authorization, checkpoint, last_item_id\n\t\tFROM accounts WHERE data_source_id=? AND user_id=? LIMIT 1`,\n\t\tdsID, userID).Scan(&acc.ID, &acc.DataSourceID, &acc.UserID, &acc.authorization, &acc.checkpoint, &acc.lastItemID)\n\tif err != nil {\n\t\treturn acc, fmt.Errorf(\"querying account %s/%s from DB: %v\", dsID, userID, err)\n\t}\n\tif acc.checkpoint != nil {\n\t\terr = UnmarshalGob(acc.checkpoint, &acc.cp)\n\t\tif err != nil {\n\t\t\treturn acc, fmt.Errorf(\"decoding checkpoint wrapper: %v\", err)\n\t\t}\n\t}\n\treturn acc, nil\n}\n\n// MarshalGob is a convenient way to gob-encode v.\nfunc MarshalGob(v interface{}) ([]byte, error) {\n\tb := new(bytes.Buffer)\n\terr := gob.NewEncoder(b).Encode(v)\n\treturn b.Bytes(), err\n}\n\n// UnmarshalGob is a convenient way to gob-decode data into v.\nfunc UnmarshalGob(data []byte, v interface{}) error {\n\treturn gob.NewDecoder(bytes.NewReader(data)).Decode(v)\n}\n"
        },
        {
          "name": "cmd",
          "type": "tree",
          "content": null
        },
        {
          "name": "datasource.go",
          "type": "blob",
          "size": 7.0849609375,
          "content": "package timeliner\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/gob\"\n\t\"fmt\"\n\t\"log\"\n\t\"time\"\n)\n\nfunc init() {\n\ttdBuf := new(bytes.Buffer)\n\terr := gob.NewEncoder(tdBuf).Encode(Metadata{})\n\tif err != nil {\n\t\tlog.Fatalf(\"[FATAL] Unable to gob-encode metadata struct: %v\", err)\n\t}\n\tmetadataGobPrefix = tdBuf.Bytes()\n}\n\n// RegisterDataSource registers ds as a data source.\nfunc RegisterDataSource(ds DataSource) error {\n\tif ds.ID == \"\" {\n\t\treturn fmt.Errorf(\"missing ID\")\n\t}\n\tif ds.Name == \"\" {\n\t\treturn fmt.Errorf(\"missing Name\")\n\t}\n\tif ds.OAuth2.ProviderID != \"\" && ds.Authenticate != nil {\n\t\treturn fmt.Errorf(\"conflicting ways of obtaining authorization\")\n\t}\n\n\t// register the data source\n\tif _, ok := dataSources[ds.ID]; ok {\n\t\treturn fmt.Errorf(\"data source already registered: %s\", ds.ID)\n\t}\n\tdataSources[ds.ID] = ds\n\n\treturn nil\n}\n\nfunc saveAllDataSources(db *sql.DB) error {\n\tif len(dataSources) == 0 {\n\t\treturn nil\n\t}\n\n\tquery := `INSERT INTO \"data_sources\" (\"id\", \"name\") VALUES`\n\tvar vals []interface{}\n\tvar count int\n\n\tfor _, ds := range dataSources {\n\t\tif count > 0 {\n\t\t\tquery += \",\"\n\t\t}\n\t\tquery += \" (?, ?)\"\n\t\tvals = append(vals, ds.ID, ds.Name)\n\t\tcount++\n\t}\n\n\tquery += \" ON CONFLICT DO NOTHING\"\n\n\t_, err := db.Exec(query, vals...)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"writing data sources to DB: %v\", err)\n\t}\n\n\treturn nil\n}\n\n// DataSource has information about a\n// data source that can be registered.\ntype DataSource struct {\n\t// A snake_cased name of the service\n\t// that uniquely identifies it from\n\t// all others.\n\tID string\n\n\t// The human-readable or brand name of\n\t// the service.\n\tName string\n\n\t// If the service authenticates with\n\t// OAuth2, fill out this field.\n\tOAuth2 OAuth2\n\n\t// Otherwise, if the service uses some\n\t// other form of authentication,\n\t// Authenticate is a function which\n\t// returns the credentials needed to\n\t// access an account on the service.\n\tAuthenticate AuthenticateFn\n\n\t// If the service enforces a rate limit,\n\t// specify it here. You can abide it by\n\t// getting an http.Client from the\n\t// Account passed into NewClient.\n\tRateLimit RateLimit\n\n\t// NewClient is a function which takes\n\t// information about the account and\n\t// returns a type which can facilitate\n\t// transactions with the service.\n\tNewClient NewClientFn\n}\n\n// authFunc gets the authentication function for this\n// service. If s.Authenticate is set, it returns that;\n// if s.OAuth2 is set, it uses a standard OAuth2 func.\nfunc (ds DataSource) authFunc() AuthenticateFn {\n\tif ds.Authenticate != nil {\n\t\treturn ds.Authenticate\n\t} else if ds.OAuth2.ProviderID != \"\" {\n\t\treturn func(userID string) ([]byte, error) {\n\t\t\treturn authorizeWithOAuth2(ds.OAuth2)\n\t\t}\n\t}\n\treturn nil\n}\n\n// OAuth2 defines which OAuth2 provider a service\n// uses and which scopes it requires.\ntype OAuth2 struct {\n\t// The ID of the service must be recognized\n\t// by the OAuth2 app configuration.\n\tProviderID string\n\n\t// The list of scopes to ask for during auth.\n\tScopes []string\n}\n\n// AuthenticateFn is a function that authenticates userID with a service.\n// It returns the authorization or credentials needed to operate. The return\n// value should be byte-encoded so it can be stored in the DB to be reused.\n// To store arbitrary types, encode the value as a gob, for example.\ntype AuthenticateFn func(userID string) ([]byte, error)\n\n// NewClientFn is a function that returns a client which, given\n// the account passed in, can interact with a service provider.\ntype NewClientFn func(acc Account) (Client, error)\n\n// Client is a type that can interact with a data source.\ntype Client interface {\n\t// ListItems lists the items on the account. Items should be\n\t// sent on itemChan as they are discovered, but related items\n\t// should be combined onto a single ItemGraph so that their\n\t// relationships can be stored. If the relationships are not\n\t// discovered until later, that's OK: item processing is\n\t// idempotent, so repeating an item from earlier will have no\n\t// adverse effects (this is possible because a unique ID is\n\t// required for each item).\n\t//\n\t// Implementations must honor the context's cancellation. If\n\t// ctx.Done() is closed, the function should return. Typically,\n\t// this is done by having an outer loop select over ctx.Done()\n\t// and default, where the next page or set of items is handled\n\t// in the default case.\n\t//\n\t// ListItems MUST close itemChan when returning. A\n\t// `defer close(itemChan)` will usually suffice. Closing\n\t// this channel signals to the processing goroutine that\n\t// no more items are coming.\n\t//\n\t// Further options for listing items may be passed in opt.\n\t//\n\t// If opt.Filename is specified, the implementation is expected\n\t// to open and list items from that file. If this is not\n\t// supported, an error should be returned. Conversely, if a\n\t// filename is not specified but required, an error should be\n\t// returned.\n\t//\n\t// opt.Timeframe consists of two optional timestamp and/or item\n\t// ID values. If set, item listings should be bounded in the\n\t// respective direction by that timestamp / item ID. (Items\n\t// are assumed to be part of a chronology; both timestamp and\n\t// item ID *may be* provided, when possible, to accommodate\n\t// data sources which do not constrain by timestamp but which\n\t// do by item ID instead.) The respective time and item ID\n\t// fields, if set, will not be in conflict, so either may be\n\t// used if both are present. While it should be documented if\n\t// timeframes are not supported, an error need not be returned\n\t// if they cannot be honored.\n\t//\n\t// opt.Checkpoint consists of the last checkpoint for this\n\t// account if the last call to ListItems did not finish and\n\t// if a checkpoint was saved. If not nil, the checkpoint\n\t// should be used to resume the listing instead of starting\n\t// over from the beginning. Checkpoint values usually consist\n\t// of page tokens or whatever state is required to resume. Call\n\t// timeliner.Checkpoint to set a checkpoint. Checkpoints are not\n\t// required, but if the implementation sets checkpoints, it\n\t// should be able to resume from one, too.\n\tListItems(ctx context.Context, itemChan chan<- *ItemGraph, opt ListingOptions) error\n}\n\n// Timeframe represents a start and end time and/or\n// a start and end item, where either value could be\n// nil which means unbounded in that direction.\n// When items are used as the timeframe boundaries,\n// the ItemID fields will be populated. It is not\n// guaranteed that any particular field will be set\n// or unset just because other fields are set or unset.\n// However, if both Since or both Until fields are\n// set, that means the timestamp and items are\n// correlated; i.e. the Since timestamp is (approx.)\n// that of the item ID. Or, put another way: there\n// will never be conflicts among the fields which\n// are non-nil.\ntype Timeframe struct {\n\tSince, Until             *time.Time\n\tSinceItemID, UntilItemID *string\n}\n\nfunc (tf Timeframe) String() string {\n\tvar sinceItemID, untilItemID string\n\tif tf.SinceItemID != nil {\n\t\tsinceItemID = *tf.SinceItemID\n\t}\n\tif tf.UntilItemID != nil {\n\t\tuntilItemID = *tf.UntilItemID\n\t}\n\treturn fmt.Sprintf(\"{Since:%s Until:%s SinceItemID:%s UntilItemID:%s}\",\n\t\ttf.Since, tf.Until, sinceItemID, untilItemID)\n}\n\nvar dataSources = make(map[string]DataSource)\n"
        },
        {
          "name": "datasources",
          "type": "tree",
          "content": null
        },
        {
          "name": "db.go",
          "type": "blob",
          "size": 5.291015625,
          "content": "package timeliner\n\nimport (\n\t\"database/sql\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\n\t// register the sqlite3 driver\n\t_ \"github.com/mattn/go-sqlite3\"\n)\n\nfunc openDB(dataDir string) (*sql.DB, error) {\n\tvar db *sql.DB\n\tvar err error\n\tdefer func() {\n\t\tif err != nil && db != nil {\n\t\t\tdb.Close()\n\t\t}\n\t}()\n\n\terr = os.MkdirAll(dataDir, 0755)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"making data directory: %v\", err)\n\t}\n\n\tdbPath := filepath.Join(dataDir, \"index.db\")\n\n\tdb, err = sql.Open(\"sqlite3\", dbPath+\"?_foreign_keys=true\")\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"opening database: %v\", err)\n\t}\n\n\t// ensure DB is provisioned\n\t_, err = db.Exec(createDB)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"setting up database: %v\", err)\n\t}\n\n\t// add all registered data sources\n\terr = saveAllDataSources(db)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"saving registered data sources to database: %v\", err)\n\t}\n\n\treturn db, nil\n}\n\nconst createDB = `\n-- A data source is a content provider, like a cloud photo service, social media site, or exported archive format.\nCREATE TABLE IF NOT EXISTS \"data_sources\" (\n\t\"id\" TEXT PRIMARY KEY,\n\t\"name\" TEXT NOT NULL\n);\n\n-- An account contains credentials necessary for accessing a data source.\nCREATE TABLE IF NOT EXISTS \"accounts\" (\n\t\"id\" INTEGER PRIMARY KEY,\n\t\"data_source_id\" TEXT NOT NULL,\n\t\"user_id\" TEXT NOT NULL,\n\t\"authorization\" BLOB,\n\t\"checkpoint\" BLOB,\n\t\"last_item_id\" INTEGER, -- row ID of item having highest timestamp processed during the last run\n\tFOREIGN KEY (\"data_source_id\") REFERENCES \"data_sources\"(\"id\") ON DELETE CASCADE,\n\tFOREIGN KEY (\"last_item_id\") REFERENCES \"items\"(\"id\") ON DELETE SET NULL,\n\tUNIQUE (\"data_source_id\", \"user_id\")\n);\n\nCREATE TABLE IF NOT EXISTS \"persons\" (\n\t\"id\" INTEGER PRIMARY KEY,\n\t\"name\" TEXT\n);\n\n-- This table specifies identities (user IDs, etc.) of a person across data_sources.\nCREATE TABLE IF NOT EXISTS \"person_identities\" (\n\t\"id\" INTEGER PRIMARY KEY,\n\t\"person_id\" INTEGER NOT NULL,\n\t\"data_source_id\" TEXT NOT NULL,\n\t\"user_id\" TEXT NOT NULL, -- whatever identifier a person takes on at the data source\n\tFOREIGN KEY (\"person_id\") REFERENCES \"persons\"(\"id\") ON DELETE CASCADE,\n\tFOREIGN KEY (\"data_source_id\") REFERENCES \"data_sources\"(\"id\") ON DELETE CASCADE,\n\tUNIQUE (\"person_id\", \"data_source_id\", \"user_id\")\n);\n\n-- An item is something downloaded from a specific account on a specific data source.\nCREATE TABLE IF NOT EXISTS \"items\" (\n\t\"id\" INTEGER PRIMARY KEY,\n\t\"account_id\" INTEGER NOT NULL,\n\t\"original_id\" TEXT NOT NULL, -- ID provided by the data source\n\t\"person_id\" INTEGER NOT NULL,\n\t\"timestamp\" INTEGER, -- timestamp when item content was originally created (NOT when the database row was created)\n\t\"stored\" INTEGER NOT NULL DEFAULT (strftime('%s', CURRENT_TIME)), -- timestamp row was created or last updated from source\n\t\"modified\" INTEGER, -- timestamp when item was locally modified; if not null, then item is \"not clean\"\n\t\"class\" INTEGER,\n\t\"mime_type\" TEXT,\n\t\"data_text\" TEXT COLLATE NOCASE,  -- item content, if text-encoded\n\t\"data_file\" TEXT, -- item filename, if non-text or not suitable for storage in DB (usually media items)\n\t\"data_hash\" TEXT, -- base64 encoding of SHA-256 checksum of contents of data file, if any\n\t\"metadata\" BLOB,  -- optional extra information\n\t\"latitude\" REAL,\n\t\"longitude\" REAL,\n\tFOREIGN KEY (\"account_id\") REFERENCES \"accounts\"(\"id\") ON DELETE CASCADE,\n\tFOREIGN KEY (\"person_id\") REFERENCES \"persons\"(\"id\") ON DELETE CASCADE,\n\tUNIQUE (\"original_id\", \"account_id\")\n);\n\nCREATE INDEX IF NOT EXISTS \"idx_items_timestamp\" ON \"items\"(\"timestamp\");\nCREATE INDEX IF NOT EXISTS \"idx_items_data_text\" ON \"items\"(\"data_text\");\nCREATE INDEX IF NOT EXISTS \"idx_items_data_file\" ON \"items\"(\"data_file\");\nCREATE INDEX IF NOT EXISTS \"idx_items_data_hash\" ON \"items\"(\"data_hash\");\n\n-- Relationships draws relationships between and across items and persons.\nCREATE TABLE IF NOT EXISTS \"relationships\" (\n\t\"id\" INTEGER PRIMARY KEY,\n\t\"from_person_id\" INTEGER,\n\t\"from_item_id\" INTEGER,\n\t\"to_person_id\" INTEGER,\n\t\"to_item_id\" INTEGER,\n\t\"directed\" BOOLEAN, -- if false, the edge goes both ways\n \t\"label\" TEXT NOT NULL,\n\tFOREIGN KEY (\"from_item_id\") REFERENCES \"items\"(\"id\") ON DELETE CASCADE,\n\tFOREIGN KEY (\"to_item_id\") REFERENCES \"items\"(\"id\") ON DELETE CASCADE,\n\tFOREIGN KEY (\"from_person_id\") REFERENCES \"persons\"(\"id\") ON DELETE CASCADE,\n\tFOREIGN KEY (\"to_person_id\") REFERENCES \"persons\"(\"id\") ON DELETE CASCADE,\n\tUNIQUE (\"from_item_id\", \"to_item_id\", \"label\"),\n\tUNIQUE (\"from_person_id\", \"to_person_id\", \"label\"),\n\tUNIQUE (\"from_item_id\", \"to_person_id\", \"label\"),\n\tUNIQUE (\"from_person_id\", \"to_item_id\", \"label\")\n);\n\nCREATE TABLE IF NOT EXISTS \"collections\" (\n\t\"id\" INTEGER PRIMARY KEY,\n\t\"account_id\" INTEGER NOT NULL,\n\t\"original_id\" TEXT,\n\t\"name\" TEXT,\n\t\"description\" TEXT,\n\t\"modified\" INTEGER, -- timestamp when collection or any of its items/ordering were modified locally; if not null, then collection is \"not clean\"\n\tFOREIGN KEY (\"account_id\") REFERENCES \"accounts\"(\"id\") ON DELETE CASCADE,\n\tUNIQUE(\"account_id\", \"original_id\")\n);\n\nCREATE TABLE IF NOT EXISTS \"collection_items\" (\n\t\"id\" INTEGER PRIMARY KEY,\n\t\"item_id\" INTEGER NOT NULL,\n\t\"collection_id\" INTEGER NOT NULL,\n\t\"position\" INTEGER NOT NULL DEFAULT 0,\n\tFOREIGN KEY (\"item_id\") REFERENCES \"items\"(\"id\") ON DELETE CASCADE,\n\tFOREIGN KEY (\"collection_id\") REFERENCES \"collections\"(\"id\") ON DELETE CASCADE,\n\tUNIQUE(\"item_id\", \"collection_id\", \"position\")\n);\n`\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 0.470703125,
          "content": "module github.com/mholt/timeliner\n\ngo 1.13\n\nrequire (\n\tgithub.com/BurntSushi/toml v0.3.1\n\tgithub.com/dgryski/go-metro v0.0.0-20180109044635-280f6062b5bc // indirect\n\tgithub.com/mattn/go-sqlite3 v1.10.0\n\tgithub.com/mholt/archiver/v3 v3.3.0\n\tgithub.com/seiflotfy/cuckoofilter v0.0.0-20200323075608-c8f23b6b6cef\n\tgithub.com/ttacon/builder v0.0.0-20170518171403-c099f663e1c2 // indirect\n\tgithub.com/ttacon/libphonenumber v1.1.0\n\tgolang.org/x/oauth2 v0.0.0-20200107190931-bf48bf16ab8d\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 4.8603515625,
          "content": "cloud.google.com/go v0.34.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\ngithub.com/BurntSushi/toml v0.3.1 h1:WXkYYl6Yr3qBf1K79EBnL4mak0OimBfB0XUf9Vl28OQ=\ngithub.com/BurntSushi/toml v0.3.1/go.mod h1:xHWCNGjB5oqiDr8zfno3MHue2Ht5sIBksp03qcyfWMU=\ngithub.com/andybalholm/brotli v0.0.0-20190621154722-5f990b63d2d6 h1:bZ28Hqta7TFAK3Q08CMvv8y3/8ATaEqv2nGoc6yff6c=\ngithub.com/andybalholm/brotli v0.0.0-20190621154722-5f990b63d2d6/go.mod h1:+lx6/Aqd1kLJ1GQfkvOnaZ1WGmLpMpbprPuIOOZX30U=\ngithub.com/dgryski/go-metro v0.0.0-20180109044635-280f6062b5bc h1:8WFBn63wegobsYAX0YjD+8suexZDga5CctH4CCTx2+8=\ngithub.com/dgryski/go-metro v0.0.0-20180109044635-280f6062b5bc/go.mod h1:c9O8+fpSOX1DM8cPNSkX/qsBWdkD4yd2dpciOWQjpBw=\ngithub.com/dsnet/compress v0.0.1 h1:PlZu0n3Tuv04TzpfPbrnI0HW/YwodEXDS+oPKahKF0Q=\ngithub.com/dsnet/compress v0.0.1/go.mod h1:Aw8dCMJ7RioblQeTqt88akK31OvO8Dhf5JflhBbQEHo=\ngithub.com/dsnet/golib v0.0.0-20171103203638-1ea166775780/go.mod h1:Lj+Z9rebOhdfkVLjJ8T6VcRQv3SXugXy999NBtR9aFY=\ngithub.com/golang/gddo v0.0.0-20190419222130-af0f2af80721 h1:KRMr9A3qfbVM7iV/WcLY/rL5LICqwMHLhwRXKu99fXw=\ngithub.com/golang/gddo v0.0.0-20190419222130-af0f2af80721/go.mod h1:xEhNfoBDX1hzLm2Nf80qUvZ2sVwoMZ8d6IE2SrsQfh4=\ngithub.com/golang/protobuf v1.2.0 h1:P3YflyNX/ehuJFLhxviNdFxQPkGK5cDcApsge1SqnvM=\ngithub.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/snappy v0.0.1 h1:Qgr9rKW7uDUkrbSmQeiDsGa8SjGyCOGtuasMWwvp2P4=\ngithub.com/golang/snappy v0.0.1/go.mod h1:/XxbfmMg8lxefKM7IXC3fBNl/7bRcc72aCRzEWrmP2Q=\ngithub.com/google/go-cmp v0.3.0 h1:crn/baboCvb5fXaQ0IJ1SGTsTVrWpDsCWC8EGETZijY=\ngithub.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/klauspost/compress v1.4.1/go.mod h1:RyIbtBH6LamlWaDj8nUwkbUhJ87Yi3uG0guNDohfE1A=\ngithub.com/klauspost/compress v1.9.2 h1:LfVyl+ZlLlLDeQ/d2AqfGIIH4qEDu0Ed2S5GyhCWIWY=\ngithub.com/klauspost/compress v1.9.2/go.mod h1:RyIbtBH6LamlWaDj8nUwkbUhJ87Yi3uG0guNDohfE1A=\ngithub.com/klauspost/cpuid v1.2.0/go.mod h1:Pj4uuM528wm8OyEC2QMXAi2YiTZ96dNQPGgoMS4s3ek=\ngithub.com/klauspost/pgzip v1.2.1 h1:oIPZROsWuPHpOdMVWLuJZXwgjhrW8r1yEX8UqMyeNHM=\ngithub.com/klauspost/pgzip v1.2.1/go.mod h1:Ch1tH69qFZu15pkjo5kYi6mth2Zzwzt50oCQKQE9RUs=\ngithub.com/mattn/go-sqlite3 v1.10.0 h1:jbhqpg7tQe4SupckyijYiy0mJJ/pRyHvXf7JdWK860o=\ngithub.com/mattn/go-sqlite3 v1.10.0/go.mod h1:FPy6KqzDD04eiIsT53CuJW3U88zkxoIYsOqkbpncsNc=\ngithub.com/mholt/archiver/v3 v3.3.0 h1:vWjhY8SQp5yzM9P6OJ/eZEkmi3UAbRrxCq48MxjAzig=\ngithub.com/mholt/archiver/v3 v3.3.0/go.mod h1:YnQtqsp+94Rwd0D/rk5cnLrxusUBUXg+08Ebtr1Mqao=\ngithub.com/nwaples/rardecode v1.0.0 h1:r7vGuS5akxOnR4JQSkko62RJ1ReCMXxQRPtxsiFMBOs=\ngithub.com/nwaples/rardecode v1.0.0/go.mod h1:5DzqNKiOdpKKBH87u8VlvAnPZMXcGRhxWkRpHbbfGS0=\ngithub.com/pierrec/lz4 v2.0.5+incompatible h1:2xWsjqPFWcplujydGg4WmhC/6fZqK42wMM8aXeqhl0I=\ngithub.com/pierrec/lz4 v2.0.5+incompatible/go.mod h1:pdkljMzZIN41W+lC3N2tnIh5sFi+IEE17M5jbnwPHcY=\ngithub.com/seiflotfy/cuckoofilter v0.0.0-20200323075608-c8f23b6b6cef h1:PokWhuPtXrgwLeUZzanj6iMZpUnFBCc6g2tDeheBLrE=\ngithub.com/seiflotfy/cuckoofilter v0.0.0-20200323075608-c8f23b6b6cef/go.mod h1:ET5mVvNjwaGXRgZxO9UZr7X+8eAf87AfIYNwRSp9s4Y=\ngithub.com/ttacon/builder v0.0.0-20170518171403-c099f663e1c2 h1:5u+EJUQiosu3JFX0XS0qTf5FznsMOzTjGqavBGuCbo0=\ngithub.com/ttacon/builder v0.0.0-20170518171403-c099f663e1c2/go.mod h1:4kyMkleCiLkgY6z8gK5BkI01ChBtxR0ro3I1ZDcGM3w=\ngithub.com/ttacon/libphonenumber v1.1.0 h1:tC6kE4t8UI4OqQVQjW5q8gSWhG2wnY5moEpSEORdYm4=\ngithub.com/ttacon/libphonenumber v1.1.0/go.mod h1:E0TpmdVMq5dyVlQ7oenAkhsLu86OkUl+yR4OAxyEg/M=\ngithub.com/ulikunitz/xz v0.5.6 h1:jGHAfXawEGZQ3blwU5wnWKQJvAraT7Ftq9EXjnXYgt8=\ngithub.com/ulikunitz/xz v0.5.6/go.mod h1:2bypXElzHzzJZwzH67Y6wb67pO62Rzfn7BSiF4ABRW8=\ngithub.com/xi2/xz v0.0.0-20171230120015-48954b6210f8 h1:nIPpBwaJSVYIxUFsDv3M8ofmx9yWTog9BfvIu0q41lo=\ngithub.com/xi2/xz v0.0.0-20171230120015-48954b6210f8/go.mod h1:HUYIGzjTL3rfEspMxjDjgmT5uz5wzYJKVo23qUhYTos=\ngolang.org/x/net v0.0.0-20180724234803-3673e40ba225/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190108225652-1e06a53dbb7e h1:bRhVy7zSSasaqNksaRZiA5EEI+Ei4I1nO5Jh72wfHlg=\ngolang.org/x/net v0.0.0-20190108225652-1e06a53dbb7e/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/oauth2 v0.0.0-20200107190931-bf48bf16ab8d h1:TzXSXBo42m9gQenoE3b9BGiEpg5IG2JkU5FkPIawgtw=\ngolang.org/x/oauth2 v0.0.0-20200107190931-bf48bf16ab8d/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4 h1:YUO/7uOKsKeq9UokNS62b8FYywz3ker1l1vDZRCRefw=\ngolang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngoogle.golang.org/appengine v1.4.0 h1:/wp5JvzpHIxhs/dumFmF7BXTf3Z+dd4uXta4kVyO508=\ngoogle.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\n"
        },
        {
          "name": "itemfiles.go",
          "type": "blob",
          "size": 7.9970703125,
          "content": "package timeliner\n\nimport (\n\t\"crypto/sha256\"\n\t\"database/sql\"\n\t\"encoding/base64\"\n\t\"fmt\"\n\t\"hash\"\n\t\"io\"\n\t\"log\"\n\tmathrand \"math/rand\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"regexp\"\n\t\"strings\"\n\t\"time\"\n)\n\n// downloadItemFile ... TODO: finish godoc.\nfunc (t *Timeline) downloadItemFile(src io.ReadCloser, dest *os.File, h hash.Hash) (int64, error) {\n\tif src == nil {\n\t\treturn 0, fmt.Errorf(\"missing reader with which to download file\")\n\t}\n\tif dest == nil {\n\t\treturn 0, fmt.Errorf(\"missing file to download into\")\n\t}\n\n\t// TODO: What if file already exists on disk (byte-for-byte)? - i.e. data_hash in DB has a duplicate\n\n\t// give the hasher a copy of the file bytes\n\ttr := io.TeeReader(src, h)\n\n\tn, err := io.Copy(dest, tr)\n\tif err != nil {\n\t\tos.Remove(dest.Name())\n\t\treturn n, fmt.Errorf(\"copying contents: %v\", err)\n\t}\n\tif err := dest.Sync(); err != nil {\n\t\tos.Remove(dest.Name())\n\t\treturn n, fmt.Errorf(\"syncing file: %v\", err)\n\t}\n\n\t// TODO: If mime type is photo or video, extract most important EXIF data and return it for storage in DB?\n\n\treturn n, nil\n}\n\n// makeUniqueCanonicalItemDataFileName returns an available\n// (non-overwriting) filename for the item's data file, starting\n// with its plain, canonical data file name, then improvising\n// and making unique if necessary. If there is no error, the\n// return value is always a usable data file name.\nfunc (t *Timeline) openUniqueCanonicalItemDataFile(it Item, dataSourceID string) (*os.File, *string, error) {\n\tif dataSourceID == \"\" {\n\t\treturn nil, nil, fmt.Errorf(\"missing service ID\")\n\t}\n\n\tdir := t.canonicalItemDataFileDir(it, dataSourceID)\n\n\terr := os.MkdirAll(t.fullpath(dir), 0700)\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"making directory for data file: %v\", err)\n\t}\n\n\ttryPath := path.Join(dir, t.canonicalItemDataFileName(it, dataSourceID))\n\tlastAppend := path.Ext(tryPath)\n\n\tfor i := 0; i < 100; i++ {\n\t\tfullFilePath := t.fullpath(filepath.FromSlash(tryPath))\n\n\t\tf, err := os.OpenFile(fullFilePath, os.O_CREATE|os.O_RDWR|os.O_EXCL, 0600)\n\t\tif os.IsExist(err) {\n\t\t\text := path.Ext(tryPath)\n\t\t\ttryPath = strings.TrimSuffix(tryPath, lastAppend)\n\t\t\tlastAppend = fmt.Sprintf(\"_%d%s\", i+1, ext) // start at 1, but actually 2 because existing file is \"1\"\n\t\t\ttryPath += lastAppend\n\t\t\tcontinue\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, nil, fmt.Errorf(\"creating data file: %v\", err)\n\t\t}\n\n\t\treturn f, &tryPath, nil\n\t}\n\n\treturn nil, nil, fmt.Errorf(\"unable to find available filename for item: %s\", tryPath)\n}\n\n// canonicalItemDataFileName returns the plain, canonical name of the\n// data file for the item. Canonical data file names are relative to\n// the base storage (repo) path (i.e. the folder of the DB file). This\n// function does no improvising in case of a name missing from the item,\n// nor does it do uniqueness checks. If the item does not have enough\n// information to generate a deterministic file name, the returned path\n// will end with a trailing slash (i.e. the path's last component empty).\n// Things considered deterministic for filename construction include the\n// item's filename, the item's original ID, and its timestamp.\n// TODO: fix godoc (this returns only the name now, not the whole dir)\nfunc (t *Timeline) canonicalItemDataFileName(it Item, dataSourceID string) string {\n\t// ideally, the filename is simply the one provided with the item\n\tvar filename string\n\tif fname := it.DataFileName(); fname != nil {\n\t\tfilename = t.safePathComponent(*fname)\n\t}\n\n\t// otherwise, try a filename based on the item's ID\n\tif filename == \"\" {\n\t\tif itemOriginalID := it.ID(); itemOriginalID != \"\" {\n\t\t\tfilename = fmt.Sprintf(\"item_%s\", itemOriginalID)\n\t\t}\n\t}\n\n\t// otherwise, try a filename based on the item's timestamp\n\tts := it.Timestamp()\n\tif filename == \"\" && !ts.IsZero() {\n\t\tfilename = ts.Format(\"2006_01_02_150405\")\n\t}\n\n\t// otherwise, out of options; revert to a random string\n\t// since no deterministic filename is available\n\tif filename == \"\" {\n\t\tfilename = randomString(24, false)\n\t}\n\n\t// shorten the name if needed (thanks for everything, Windows)\n\treturn t.ensureDataFileNameShortEnough(filename)\n}\n\nfunc (t *Timeline) canonicalItemDataFileDir(it Item, dataSourceID string) string {\n\tts := it.Timestamp()\n\tif ts.IsZero() {\n\t\tts = time.Now()\n\t}\n\n\tif dataSourceID == \"\" {\n\t\tdataSourceID = \"unknown_service\"\n\t}\n\n\t// use \"/\" separators and adjust for the OS\n\t// path separator when accessing disk\n\treturn path.Join(\"data\",\n\t\tfmt.Sprintf(\"%04d\", ts.Year()),\n\t\tfmt.Sprintf(\"%02d\", ts.Month()),\n\t\tt.safePathComponent(dataSourceID))\n}\n\nfunc (t *Timeline) ensureDataFileNameShortEnough(filename string) string {\n\t// thanks for nothing, Windows\n\tif len(filename) > 250 {\n\t\text := path.Ext(filename)\n\t\tif len(ext) > 20 { // arbitrary and unlikely, but just in case\n\t\t\text = ext[:20]\n\t\t}\n\t\tfilename = filename[:250-len(ext)]\n\t\tfilename += ext\n\t}\n\treturn filename\n}\n\n// TODO:/NOTE: If changing a file name, all items with same data_hash must also be updated to use same file name\nfunc (t *Timeline) replaceWithExisting(canonical *string, checksumBase64 string, itemRowID int64) error {\n\tif canonical == nil || *canonical == \"\" || checksumBase64 == \"\" {\n\t\treturn fmt.Errorf(\"missing data filename and/or hash of contents\")\n\t}\n\n\tvar existingDatafile *string\n\terr := t.db.QueryRow(`SELECT data_file FROM items\n\t\tWHERE data_hash = ? AND id != ? LIMIT 1`,\n\t\tchecksumBase64, itemRowID).Scan(&existingDatafile)\n\tif err == sql.ErrNoRows {\n\t\treturn nil // file is unique; carry on\n\t}\n\tif err != nil {\n\t\treturn fmt.Errorf(\"querying DB: %v\", err)\n\t}\n\n\t// file is a duplicate!\n\n\tif existingDatafile == nil {\n\t\t// ... that's weird, how's this possible? it has a hash but no file name recorded\n\t\treturn fmt.Errorf(\"item with matching hash is missing data file name; hash: %s\", checksumBase64)\n\t}\n\n\t// ensure the existing file is still the same\n\th := sha256.New()\n\tf, err := os.Open(t.fullpath(*existingDatafile))\n\tif err != nil {\n\t\treturn fmt.Errorf(\"opening existing file: %v\", err)\n\t}\n\tdefer f.Close()\n\n\t_, err = io.Copy(h, f)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"checking file integrity: %v\", err)\n\t}\n\n\texistingFileHash := h.Sum(nil)\n\tb64ExistingFileHash := base64.StdEncoding.EncodeToString(existingFileHash)\n\n\t// if the existing file was modified; restore it with\n\t// what we just downloaded, which presumably succeeded\n\tif checksumBase64 != b64ExistingFileHash {\n\t\tlog.Printf(\"[INFO] Restoring modified data file: %s was '%s' but is now '%s'\",\n\t\t\t*existingDatafile, checksumBase64, existingFileHash)\n\t\terr := os.Rename(t.fullpath(*canonical), t.fullpath(*existingDatafile))\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"replacing modified data file: %v\", err)\n\t\t}\n\t}\n\n\t// everything checks out; delete the newly-downloaded file\n\t// and use the existing file instead of duplicating it\n\terr = os.Remove(t.fullpath(*canonical))\n\tif err != nil {\n\t\treturn fmt.Errorf(\"removing duplicate data file: %v\", err)\n\t}\n\n\tcanonical = existingDatafile\n\n\treturn nil\n}\n\n// randomString returns a string of n random characters.\n// It is not even remotely secure or a proper distribution.\n// But it's good enough for some things. It excludes certain\n// confusing characters like I, l, 1, 0, O, etc. If sameCase\n// is true, then uppercase letters are excluded.\nfunc randomString(n int, sameCase bool) string {\n\tif n <= 0 {\n\t\treturn \"\"\n\t}\n\tdict := []byte(\"abcdefghijkmnopqrstuvwxyzABCDEFGHJKLMNPQRTUVWXY23456789\")\n\tif sameCase {\n\t\tdict = []byte(\"abcdefghijkmnpqrstuvwxyz0123456789\")\n\t}\n\tb := make([]byte, n)\n\tfor i := range b {\n\t\tb[i] = dict[mathrand.Int63()%int64(len(dict))]\n\t}\n\treturn string(b)\n}\n\nfunc (t *Timeline) fullpath(canonicalDatafileName string) string {\n\treturn filepath.Join(t.repoDir, filepath.FromSlash(canonicalDatafileName))\n}\n\nfunc (t *Timeline) datafileExists(canonicalDatafileName string) bool {\n\t_, err := os.Stat(t.fullpath(canonicalDatafileName))\n\treturn !os.IsNotExist(err)\n}\n\nfunc (t *Timeline) safePathComponent(s string) string {\n\ts = safePathRE.ReplaceAllLiteralString(s, \"\")\n\ts = strings.Replace(s, \"..\", \"\", -1)\n\tif s == \".\" {\n\t\ts = \"\"\n\t}\n\treturn s\n}\n\n// safePathRER matches any undesirable characters in a filepath.\n// Note that this allows dots, so you'll have to strip \"..\" manually.\nvar safePathRE = regexp.MustCompile(`[^\\w.-]`)\n"
        },
        {
          "name": "itemgraph.go",
          "type": "blob",
          "size": 12.6962890625,
          "content": "package timeliner\n\nimport (\n\t\"bytes\"\n\t\"encoding/gob\"\n\t\"io\"\n\t\"time\"\n)\n\n// Item is the central concept of a piece of content\n// from a service or data source. Take note of which\n// methods are required to return non-empty values.\n//\n// The actual content of an item is stored either in\n// the database or on disk as a file. Generally,\n// content that is text-encoded can and should be\n// stored in the database where it will be indexed.\n// However, if the item's content (for example, the\n// bytes of a photo or video) are not text or if the\n// text is too large to store well in a database (for\n// example, an entire novel), it should be stored\n// on disk, and this interface has methods to\n// accommodate both. Note that an item may have both\n// text and non-text content, too: for example, photos\n// and videos may have descriptions that are as much\n// \"content\" as the media iteself. One part of an item\n// is not mutually exclusive with any other.\ntype Item interface {\n\t// The unique ID of the item assigned by the service.\n\t// If the service does not assign one, then invent\n\t// one such that the ID is unique to the content or\n\t// substance of the item (for example, an ID derived\n\t// from timestamp or from the actual content of the\n\t// item -- whatever makes it unique). The ID need\n\t// only be unique for the account it is associated\n\t// with, although more unique is, of course, acceptable.\n\t//\n\t// REQUIRED.\n\tID() string\n\n\t// The originating timestamp of the item, which\n\t// may be different from when the item was posted\n\t// or created. For example, a photo may be taken\n\t// one day but uploaded a week later. Prefer the\n\t// time when the original item content was captured.\n\t//\n\t// REQUIRED.\n\tTimestamp() time.Time\n\n\t// A classification of the item's kind.\n\t//\n\t// REQUIRED.\n\tClass() ItemClass\n\n\t// The user/account ID of the owner or\n\t// originator of the content, along with their\n\t// username or real name. The ID is used to\n\t// relate the item with the person behind it;\n\t// the name is used to make the person\n\t// recognizable to the human reader. If the\n\t// ID is nil, the current account owner will\n\t// be assumed. (Use the ID as given by the\n\t// data source.) If the data source only\n\t// provides a name but no ID, you may return\n\t// the name as the ID with the understanding\n\t// that a different name will be counted as a\n\t// different person. You may also return the\n\t// name as the name and leave the ID nil and\n\t// have correct results if it is safe to assume\n\t// the name belongs to the current account owner.\n\tOwner() (id *string, name *string)\n\n\t// Returns the text of the item, if any.\n\t// This field is indexed in the DB, so don't\n\t// use for unimportant metadata or huge\n\t// swaths of text; if there is a large\n\t// amount of text, use an item file instead.\n\tDataText() (*string, error)\n\n\t// For primary content which is not text or\n\t// which is too large to be stored well in a\n\t// database, the content can be downloaded\n\t// into a file. If so, the following methods\n\t// should return the necessary information,\n\t// if available from the service, so that a\n\t// data file can be obtained, stored, and\n\t// later read successfully.\n\t//\n\t// DataFileName returns the filename (NOT full\n\t// path or URL) of the file; prefer the original\n\t// filename if it originated as a file. If the\n\t// filename is not unique on disk when downloaded,\n\t// it will be made unique by modifying it. If\n\t// this value is nil/empty, a filename will be\n\t// generated from the item's other data.\n\t//\n\t// DataFileReader returns a way to read the data.\n\t// It will be closed when the read is completed.\n\t//\n\t// DataFileHash returns the checksum of the\n\t// content as provided by the service. If the\n\t// service (or data source) does not provide a\n\t// hash, leave this field empty, but note that\n\t// later it will be impossible to efficiently\n\t// know whether the content has changed on the\n\t// service from what is stored locally.\n\t//\n\t// DataFileMIMEType returns the MIME type of\n\t// the data file, if known.\n\tDataFileName() *string\n\tDataFileReader() (io.ReadCloser, error)\n\tDataFileHash() []byte\n\tDataFileMIMEType() *string\n\n\t// Metadata returns any optional metadata.\n\t// Feel free to leave as many fields empty\n\t// as you'd like: the less fields that are\n\t// filled out, the smaller the storage size.\n\t// Metadata is not indexed by the DB but is\n\t// rendered in projections and queries\n\t// according to the item's classification.\n\tMetadata() (*Metadata, error)\n\n\t// Location returns an item's location,\n\t// if known. For now, only Earth\n\t// coordinates are accepted, but we can\n\t// improve this later.\n\tLocation() (*Location, error)\n}\n\n// ItemClass classifies an item.\ntype ItemClass int\n\n// Various classes of items.\nconst (\n\tClassUnknown ItemClass = iota\n\tClassImage\n\tClassVideo\n\tClassAudio\n\tClassPost\n\tClassLocation\n\tClassEmail\n\tClassPrivateMessage\n\tClassMessage\n)\n\n// These are the standard relationships that Timeliner\n// recognizes. Using these known relationships is not\n// required, but it makes it easier to translate them to\n// human-friendly phrases when visualizing the timeline.\nvar (\n\tRelReplyTo  = Relation{Label: \"reply_to\", Bidirectional: false}      // \"<from> is in reply to <to>\"\n\tRelAttached = Relation{Label: \"attached\", Bidirectional: true}       // \"<to|from> is attached to <from|to>\"\n\tRelQuotes   = Relation{Label: \"quotes\", Bidirectional: false}        // \"<from> quotes <to>\"\n\tRelCCed     = Relation{Label: \"carbon_copied\", Bidirectional: false} // \"<from_item> is carbon-copied to <to_person>\"\n)\n\n// ItemRow has the structure of an item's row in our DB.\ntype ItemRow struct {\n\tID         int64\n\tAccountID  int64\n\tOriginalID string\n\tPersonID   int64\n\tTimestamp  time.Time\n\tStored     time.Time\n\tModified   *time.Time\n\tClass      ItemClass\n\tMIMEType   *string\n\tDataText   *string\n\tDataFile   *string\n\tDataHash   *string // base64-encoded SHA-256\n\tMetadata   *Metadata\n\tLocation\n\n\tmetaGob []byte // use Metadata.(encode/decode)\n\titem    Item\n}\n\n// Location contains location information.\ntype Location struct {\n\tLatitude  *float64\n\tLongitude *float64\n}\n\n// ItemGraph is an item with optional connections to other items.\n// All ItemGraph values should be pointers to ensure consistency.\n// The usual weird/fun thing about representing graph data structures\n// in memory is that a graph is a node, and a node is a graph. \ntype ItemGraph struct {\n\t// The node item. This can be nil, but note that\n\t// Edges will not be traversed if Node is nil,\n\t// because there must be a node on both ends of\n\t// an edge.\n\t//\n\t// Optional.\n\tNode Item\n\n\t// Edges are represented as 1:many relations\n\t// to other \"graphs\" (nodes in the graph).\n\t// Fill this out to add multiple items to the\n\t// timeline at once, while drawing the\n\t// designated relationships between them.\n\t// Useful when processing related items in\n\t// batches.\n\t//\n\t// Directional relationships go from Node to\n\t// the map key.\n\t//\n\t// If the items involved in a relationship are\n\t// not efficiently available at the same time\n\t// (i.e. if loading both items involved in the\n\t// relationship would take a non-trivial amount\n\t// of time or API calls), you can use the\n\t// Relations field instead, but only after the\n\t// items have been added to the timeline.\n\t//\n\t// Optional.\n\tEdges map[*ItemGraph][]Relation\n\n\t// If items in the graph belong to a collection,\n\t// specify them here. If the collection does not\n\t// exist (by row ID or AccountID+OriginalID), it\n\t// will be created. If it already exists, the\n\t// collection in the DB will be unioned with the\n\t// collection specified here. Collections are\n\t// processed regardless of Node and Edges.\n\t//\n\t// Optional.\n\tCollections []Collection\n\n\t// Relationships between existing items in the\n\t// timeline can be represented here in a list\n\t// of item IDs that are connected by a label.\n\t// This field is useful when relationships and\n\t// the items involved in them are not discovered\n\t// at the same time. Relations in this list will\n\t// be added to the timeline, joined by the item\n\t// IDs described in the RawRelations, only if\n\t// the items having those IDs (as provided by\n\t// the data source; we're not talking about DB\n\t// row IDs here) already exist in the timeline.\n\t// In other words, this is a best-effort field;\n\t// useful for forming relationships of existing\n\t// items, but without access to the actual items\n\t// themselves. If you have the items involved in\n\t// the relationships, use Edges instead.\n\t//\n\t// Optional.\n\tRelations []RawRelation\n}\n\n// NewItemGraph returns a new node/graph.\nfunc NewItemGraph(node Item) *ItemGraph {\n\treturn &ItemGraph{\n\t\tNode:  node,\n\t\tEdges: make(map[*ItemGraph][]Relation),\n\t}\n}\n\n// Add adds item to the graph ig by making an edge described\n// by rel from the node ig to a new node for item.\n//\n// This method is for simple inserts, where the only thing to add\n// to the graph at this moment is a single item, since the graph\n// it inserts contains only a single node populated by item. To\n// add a full graph with multiple items (i.e. a graph with edges),\n// call ig.Connect directly.\nfunc (ig *ItemGraph) Add(item Item, rel Relation) {\n\tig.Connect(NewItemGraph(item), rel)\n}\n\n// Connect is a simple convenience function that adds a graph (node)\n// to ig by an edge described by rel.\nfunc (ig *ItemGraph) Connect(node *ItemGraph, rel Relation) {\n\tif ig.Edges == nil {\n\t\tig.Edges = make(map[*ItemGraph][]Relation)\n\t}\n\tig.Edges[node] = append(ig.Edges[node], rel)\n}\n\n// RawRelation represents a relationship between\n// two items or people (or both) from the same\n// data source (but not necessarily the same\n// accounts; we assume that a data source's item\n// IDs are globally unique across accounts).\n// The item IDs should be those which are\n// assigned/provided by the data source, NOT a\n// database row ID. Likewise, the persons' user\n// IDs should be the IDs of the user as associated\n// with the data source, NOT their row IDs.\ntype RawRelation struct {\n\tFromItemID       string\n\tToItemID         string\n\tFromPersonUserID string\n\tToPersonUserID   string\n\tRelation\n}\n\n// Relation describes how two nodes in a graph are related.\n// It's essentially an edge on a graph.\ntype Relation struct {\n\tLabel         string\n\tBidirectional bool\n}\n\n// Collection represents a group of items, like an album.\ntype Collection struct {\n\t// The ID of the collection as given\n\t// by the service; for example, the\n\t// album ID. If the service does not\n\t// provide an ID for the collection,\n\t// invent one such that the next time\n\t// the collection is encountered and\n\t// processed, its ID will be the same.\n\t// An ID is necessary here to ensure\n\t// uniqueness.\n\t//\n\t// REQUIRED.\n\tOriginalID string\n\n\t// The name of the collection as\n\t// given by the service; for example,\n\t// the album title.\n\t//\n\t// Optional.\n\tName *string\n\n\t// The description, caption, or any\n\t// other relevant text describing\n\t// the collection.\n\t//\n\t// Optional.\n\tDescription *string\n\n\t// The items for the collection;\n\t// if ordering is significant,\n\t// specify each item's Position\n\t// field; the order of elememts\n\t// of this slice will not be\n\t// considered important.\n\tItems []CollectionItem\n}\n\n// CollectionItem represents an item\n// stored in a collection.\ntype CollectionItem struct {\n\t// The item to add to the collection.\n\tItem Item\n\n\t// Specify if ordering is important.\n\tPosition int\n\n\t// Used when processing; this will\n\t// store the row ID of the item\n\t// after the item has been inserted\n\t// into the DB.\n\titemRowID int64\n}\n\n// Metadata is a unified structure for storing\n// item metadata in the DB.\ntype Metadata struct {\n\t// A hash or etag provided by the service to\n\t// make it easy to know if it has changed\n\tServiceHash []byte\n\n\t// Locations\n\tLocationAccuracy int\n\tAltitude         int // meters\n\tAltitudeAccuracy int\n\tHeading          int // degrees\n\tVelocity         int\n\n\tGeneralArea string // natural language description of a location\n\n\t// Photos and videos\n\tEXIF map[string]interface{}\n\t// TODO: Should we have some of the \"most important\" EXIF fields explicitly here?\n\n\tWidth  int\n\tHeight int\n\n\t// TODO: Google Photos (how many of these belong in EXIF?)\n\tCameraMake      string\n\tCameraModel     string\n\tFocalLength     float64\n\tApertureFNumber float64\n\tISOEquivalent   int\n\tExposureTime    time.Duration\n\n\tFPS float64 // Frames Per Second\n\n\t// Posts (Facebook so far)\n\tLink        string\n\tDescription string\n\tName        string\n\tParentID    string\n\tStatusType  string\n\tType        string\n\n\tShares int // aka \"Retweets\" or \"Reshares\"\n\tLikes  int\n}\n\nfunc (m *Metadata) encode() ([]byte, error) {\n\t// then encode the actual data, and trim off\n\t// schema from the beginning\n\tbuf := new(bytes.Buffer)\n\terr := gob.NewEncoder(buf).Encode(m)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn buf.Bytes()[len(metadataGobPrefix):], nil\n}\n\nfunc (m *Metadata) decode(b []byte) error {\n\tif b == nil {\n\t\treturn nil\n\t}\n\tfullGob := append(metadataGobPrefix, b...)\n\treturn gob.NewDecoder(bytes.NewReader(fullGob)).Decode(m)\n}\n\nvar metadataGobPrefix []byte\n"
        },
        {
          "name": "mapmutex.go",
          "type": "blob",
          "size": 0.7216796875,
          "content": "package timeliner\n\nimport \"sync\"\n\n// Modified from https://medium.com/@petrlozhkin/kmutex-lock-mutex-by-unique-id-408467659c24\n\ntype mapMutex struct {\n\tcond *sync.Cond\n\tset  map[interface{}]struct{}\n}\n\nfunc newMapMutex() *mapMutex {\n\treturn &mapMutex{\n\t\tcond: sync.NewCond(new(sync.Mutex)),\n\t\tset:  make(map[interface{}]struct{}),\n\t}\n}\n\nfunc (mmu *mapMutex) Lock(key interface{}) {\n\tmmu.cond.L.Lock()\n\tdefer mmu.cond.L.Unlock()\n\tfor mmu.locked(key) {\n\t\tmmu.cond.Wait()\n\t}\n\tmmu.set[key] = struct{}{}\n\treturn\n}\n\nfunc (mmu *mapMutex) Unlock(key interface{}) {\n\tmmu.cond.L.Lock()\n\tdefer mmu.cond.L.Unlock()\n\tdelete(mmu.set, key)\n\tmmu.cond.Broadcast()\n}\n\nfunc (mmu *mapMutex) locked(key interface{}) (ok bool) {\n\t_, ok = mmu.set[key]\n\treturn\n}\n"
        },
        {
          "name": "oauth2.go",
          "type": "blob",
          "size": 3.0244140625,
          "content": "package timeliner\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net/http\"\n\n\t\"github.com/mholt/timeliner/oauth2client\"\n\t\"golang.org/x/oauth2\"\n)\n\n// OAuth2AppSource returns an oauth2client.App for the OAuth2 provider\n// with the given ID. Programs using data sources that authenticate\n// with OAuth2 MUST set this variable, or the program will panic.\nvar OAuth2AppSource func(providerID string, scopes []string) (oauth2client.App, error)\n\n// NewOAuth2HTTPClient returns a new HTTP client which performs\n// HTTP requests that are authenticated with an oauth2.Token\n// stored with the account acc.\nfunc (acc Account) NewOAuth2HTTPClient() (*http.Client, error) {\n\t// load the existing token for this account from the database\n\tvar tkn *oauth2.Token\n\terr := UnmarshalGob(acc.authorization, &tkn)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"gob-decoding OAuth2 token: %v\", err)\n\t}\n\tif tkn == nil || tkn.AccessToken == \"\" {\n\t\treturn nil, fmt.Errorf(\"OAuth2 token is empty: %+v\", tkn)\n\t}\n\n\t// load the service's \"oauth app\", which can provide both tokens and\n\t// oauth configs -- in this case, we need the oauth config; we should\n\t// already have a token\n\toapp, err := OAuth2AppSource(acc.ds.OAuth2.ProviderID, acc.ds.OAuth2.Scopes)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"getting token source for %s: %v\", acc.DataSourceID, err)\n\t}\n\n\t// obtain a token source from the oauth's config so that it can keep\n\t// the token refreshed if it expires\n\tsrc := oapp.TokenSource(context.Background(), tkn)\n\n\t// finally, create an HTTP client that authenticates using the token,\n\t// but wrapping the underlying token source so we can persist any\n\t// changes to the database\n\treturn oauth2.NewClient(context.Background(), &persistedTokenSource{\n\t\ttl:        acc.t,\n\t\tts:        src,\n\t\taccountID: acc.ID,\n\t\ttoken:     tkn,\n\t}), nil\n}\n\n// authorizeWithOAuth2 gets an initial OAuth2 token from the user.\n// It requires OAuth2AppSource to be set or it will panic.\nfunc authorizeWithOAuth2(oc OAuth2) ([]byte, error) {\n\tsrc, err := OAuth2AppSource(oc.ProviderID, oc.Scopes)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"getting token source: %v\", err)\n\t}\n\ttkn, err := src.InitialToken()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"getting token from source: %v\", err)\n\t}\n\treturn MarshalGob(tkn)\n}\n\n// persistedTokenSource wraps a TokenSource for\n// a particular account and persists any changes\n// to the account's token to the database.\ntype persistedTokenSource struct {\n\ttl        *Timeline\n\tts        oauth2.TokenSource\n\taccountID int64\n\ttoken     *oauth2.Token\n}\n\nfunc (ps *persistedTokenSource) Token() (*oauth2.Token, error) {\n\ttkn, err := ps.ts.Token()\n\tif err != nil {\n\t\treturn tkn, err\n\t}\n\n\t// store an updated token in the DB\n\tif tkn.AccessToken != ps.token.AccessToken {\n\t\tps.token = tkn\n\n\t\tauthBytes, err := MarshalGob(tkn)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"gob-encoding new OAuth2 token: %v\", err)\n\t\t}\n\n\t\t_, err = ps.tl.db.Exec(`UPDATE accounts SET authorization=? WHERE id=?`, authBytes, ps.accountID)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"storing refreshed OAuth2 token: %v\", err)\n\t\t}\n\t}\n\n\treturn tkn, nil\n}\n"
        },
        {
          "name": "oauth2client",
          "type": "tree",
          "content": null
        },
        {
          "name": "persons.go",
          "type": "blob",
          "size": 2.2265625,
          "content": "package timeliner\n\nimport (\n\t\"database/sql\"\n\t\"fmt\"\n)\n\n// getPerson returns the person mapped to userID on service.\n// If the person does not exist, it is created.\nfunc (t *Timeline) getPerson(dataSourceID, userID, name string) (Person, error) {\n\t// first, load the person\n\tvar p Person\n\terr := t.db.QueryRow(`SELECT persons.id, persons.name\n\t\tFROM persons, person_identities\n\t\tWHERE person_identities.data_source_id=?\n\t\t\tAND person_identities.user_id=?\n\t\t\tAND persons.id = person_identities.person_id\n\t\tLIMIT 1`, dataSourceID, userID).Scan(&p.ID, &p.Name)\n\tif err == sql.ErrNoRows {\n\t\t// person does not exist; create this mapping - TODO: do in a transaction\n\t\tp = Person{Name: name}\n\t\tres, err := t.db.Exec(`INSERT INTO persons (name) VALUES (?)`, p.Name)\n\t\tif err != nil {\n\t\t\treturn Person{}, fmt.Errorf(\"adding new person: %v\", err)\n\t\t}\n\t\tp.ID, err = res.LastInsertId()\n\t\tif err != nil {\n\t\t\treturn Person{}, fmt.Errorf(\"getting person ID: %v\", err)\n\t\t}\n\t\t_, err = t.db.Exec(`INSERT OR IGNORE INTO person_identities\n\t\t\t(person_id, data_source_id, user_id) VALUES (?, ?, ?)`,\n\t\t\tp.ID, dataSourceID, userID)\n\t\tif err != nil {\n\t\t\treturn Person{}, fmt.Errorf(\"adding new person identity mapping: %v\", err)\n\t\t}\n\t} else if err != nil {\n\t\treturn Person{}, fmt.Errorf(\"selecting person identity: %v\", err)\n\t}\n\n\t// now get all the person's identities\n\trows, err := t.db.Query(`SELECT id, person_id, data_source_id, user_id\n\t\tFROM person_identities WHERE person_id=?`, p.ID)\n\tif err != nil {\n\t\treturn Person{}, fmt.Errorf(\"selecting person's known identities: %v\", err)\n\t}\n\tdefer rows.Close()\n\tfor rows.Next() {\n\t\tvar ident PersonIdentity\n\t\terr := rows.Scan(&ident.ID, &ident.PersonID, &ident.DataSourceID, &ident.UserID)\n\t\tif err != nil {\n\t\t\treturn Person{}, fmt.Errorf(\"loading person's identity: %v\", err)\n\t\t}\n\t\tp.Identities = append(p.Identities, ident)\n\t}\n\tif err = rows.Err(); err != nil {\n\t\treturn Person{}, fmt.Errorf(\"scanning identity rows: %v\", err)\n\t}\n\n\treturn p, nil\n}\n\n// Person represents a person.\ntype Person struct {\n\tID         int64\n\tName       string\n\tIdentities []PersonIdentity\n}\n\n// PersonIdentity is a way to map a user ID on a service to a person.\ntype PersonIdentity struct {\n\tID           int64\n\tPersonID     string\n\tDataSourceID string\n\tUserID       string\n}\n"
        },
        {
          "name": "processing.go",
          "type": "blob",
          "size": 28.0869140625,
          "content": "package timeliner\n\nimport (\n\t\"bytes\"\n\t\"crypto/sha256\"\n\t\"database/sql\"\n\t\"encoding/base64\"\n\t\"fmt\"\n\t\"io\"\n\t\"log\"\n\t\"os\"\n\t\"sync\"\n\t\"time\"\n)\n\n// beginProcessing starts workers to process items that are\n// obtained from ac. It returns a WaitGroup which blocks until\n// all workers have finished, and a channel into which the\n// service should pipe its items.\nfunc (wc *WrappedClient) beginProcessing(cc concurrentCuckoo, po ProcessingOptions) (*sync.WaitGroup, chan<- *ItemGraph) {\n\twg := new(sync.WaitGroup)\n\tch := make(chan *ItemGraph)\n\n\tconst workers = 2 // TODO: Make configurable?\n\tfor i := 0; i < workers; i++ {\n\t\twg.Add(1)\n\t\tgo func(i int) {\n\t\t\tdefer wg.Done()\n\t\t\tfor ig := range ch {\n\t\t\t\t_, err := wc.processItemGraph(ig, &recursiveState{\n\t\t\t\t\ttimestamp: time.Now(),\n\t\t\t\t\tprocOpt:   po,\n\t\t\t\t\tseen:      make(map[*ItemGraph]int64),\n\t\t\t\t\tidmap:     make(map[string]int64),\n\t\t\t\t\tcuckoo:    cc,\n\t\t\t\t})\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Printf(\"[ERROR] %s: processing item graph: %v\", wc.acc, err)\n\t\t\t\t}\n\t\t\t}\n\t\t}(i)\n\t}\n\n\treturn wg, ch\n}\n\ntype recursiveState struct {\n\ttimestamp time.Time\n\tprocOpt   ProcessingOptions\n\tseen      map[*ItemGraph]int64 // value is the item's row ID\n\tidmap     map[string]int64     // map an item's service ID to the row ID -- TODO: I don't love this... any better way?\n\n\t// the cuckoo filter pointer lives for\n\t// the duration of the entire operation;\n\t// it is often nil, but if it is set,\n\t// then the service-produced ID of each\n\t// item should be added to the filter so\n\t// that a prune can take place when the\n\t// entire operation is complete\n\tcuckoo concurrentCuckoo\n}\n\nfunc (wc *WrappedClient) processItemGraph(ig *ItemGraph, state *recursiveState) (int64, error) {\n\tif ig == nil {\n\t\treturn 0, nil\n\t}\n\n\t// don't visit a node twice\n\tif igID, ok := state.seen[ig]; ok {\n\t\tif state.procOpt.Verbose {\n\t\t\tlog.Printf(\"[DEBUG] %s: item graph already visited: %p\", wc.acc, ig)\n\t\t}\n\t\treturn igID, nil\n\t}\n\n\tif state.procOpt.Verbose {\n\t\tvar nodeItemID string\n\t\tif ig.Node != nil {\n\t\t\tnodeItemID = ig.Node.ID()\n\t\t}\n\t\tlog.Printf(\"[DEBUG] %s: visiting item graph %p (node_item_id=%s edges=%d collections=%d relations=%d)\",\n\t\t\twc.acc, ig, nodeItemID, len(ig.Edges), len(ig.Collections), len(ig.Relations))\n\t}\n\n\tvar igRowID int64\n\n\tif ig.Node == nil {\n\t\t// mark this node as visited (no ID)\n\t\tstate.seen[ig] = 0\n\t} else {\n\t\t// process root node\n\t\tvar err error\n\t\tigRowID, err = wc.processSingleItemGraphNode(ig.Node, state)\n\t\tif err != nil {\n\t\t\treturn 0, fmt.Errorf(\"processing node of item graph: %v\", err)\n\t\t}\n\n\t\t// mark this node as visited\n\t\tstate.seen[ig] = igRowID\n\n\t\t// map individual items to their row IDs\n\t\tstate.idmap[ig.Node.ID()] = igRowID\n\n\t\t// process all connected nodes\n\t\tif ig.Edges != nil {\n\t\t\tfor connectedIG, relations := range ig.Edges {\n\t\t\t\t// if node not yet visited, process it now\n\t\t\t\tconnectedIGRowID, visited := state.seen[connectedIG]\n\t\t\t\tif !visited {\n\t\t\t\t\tconnectedIGRowID, err = wc.processItemGraph(connectedIG, state)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn igRowID, fmt.Errorf(\"processing node of item graph: %v\", err)\n\t\t\t\t\t}\n\t\t\t\t\tstate.seen[connectedIG] = connectedIGRowID\n\t\t\t\t}\n\n\t\t\t\t// store this item's ID for later\n\t\t\t\tstate.idmap[connectedIG.Node.ID()] = connectedIGRowID\n\n\t\t\t\t// insert relations to this connected node into DB\n\t\t\t\tfor _, rel := range relations {\n\t\t\t\t\t_, err = wc.tl.db.Exec(`INSERT OR IGNORE INTO relationships\n\t\t\t\t\t(from_item_id, to_item_id, directed, label)\n\t\t\t\t\tVALUES (?, ?, ?, ?)`,\n\t\t\t\t\t\tigRowID, connectedIGRowID, !rel.Bidirectional, rel.Label)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn igRowID, fmt.Errorf(\"storing item relationship: %v (from_item=%d to_item=%d directed=%t label=%v)\",\n\t\t\t\t\t\t\terr, igRowID, connectedIGRowID, !rel.Bidirectional, rel.Label)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// process collections, if any\n\tfor _, coll := range ig.Collections {\n\t\t// attach the item's row ID to each item in the collection\n\t\t// to speed up processing; we won't have to query the database\n\t\t// again for items that were already processed from the graph\n\t\tfor i, it := range coll.Items {\n\t\t\tcoll.Items[i].itemRowID = state.idmap[it.Item.ID()]\n\t\t}\n\n\t\terr := wc.processCollection(coll, state.timestamp, state.procOpt)\n\t\tif err != nil {\n\t\t\treturn 0, fmt.Errorf(\"processing collection: %v (original_id=%s)\", err, coll.OriginalID)\n\t\t}\n\t}\n\n\t// process raw relations, if any\n\tfor _, rr := range ig.Relations {\n\t\tvar fromItemRowID, toItemRowID, fromPersonRowID, toPersonRowID *int64\n\t\tvar err error\n\t\tif rr.FromItemID != \"\" {\n\t\t\t// get each item's row ID from their data source item ID\n\t\t\tfromItemRowID, err = wc.itemRowIDFromOriginalID(rr.FromItemID)\n\t\t\tif err == sql.ErrNoRows {\n\t\t\t\tcontinue // item does not exist in timeline; skip this relation\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\treturn 0, fmt.Errorf(\"querying 'from' item row ID: %v\", err)\n\t\t\t}\n\t\t}\n\t\tif rr.ToItemID != \"\" {\n\t\t\ttoItemRowID, err = wc.itemRowIDFromOriginalID(rr.ToItemID)\n\t\t\tif err == sql.ErrNoRows {\n\t\t\t\tcontinue // item does not exist in timeline; skip this relation\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\treturn 0, fmt.Errorf(\"querying 'to' item row ID: %v\", err)\n\t\t\t}\n\t\t}\n\t\tif rr.FromPersonUserID != \"\" {\n\t\t\tfromPersonRowID, err = wc.personRowIDFromUserID(rr.FromPersonUserID)\n\t\t\tif err == sql.ErrNoRows {\n\t\t\t\tcontinue // person does not exist in timeline; skip this relation\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\treturn 0, fmt.Errorf(\"querying 'from' person row ID: %v\", err)\n\t\t\t}\n\t\t}\n\t\tif rr.ToPersonUserID != \"\" {\n\t\t\ttoPersonRowID, err = wc.personRowIDFromUserID(rr.ToPersonUserID)\n\t\t\tif err == sql.ErrNoRows {\n\t\t\t\tcontinue // person does not exist in timeline; skip this relation\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\treturn 0, fmt.Errorf(\"querying 'to' person row ID: %v\", err)\n\t\t\t}\n\t\t}\n\n\t\t// store the relation\n\t\t_, err = wc.tl.db.Exec(`INSERT OR IGNORE INTO relationships\n\t\t\t\t\t(from_person_id, from_item_id, to_person_id, to_item_id, directed, label)\n\t\t\t\t\tVALUES (?, ?, ?, ?, ?, ?)`,\n\t\t\tfromPersonRowID, fromItemRowID, toPersonRowID, toItemRowID, !rr.Bidirectional, rr.Label)\n\t\tif err != nil {\n\t\t\treturn 0, fmt.Errorf(\"storing raw item relationship: %v (from_person=%d from_item=%d to_person=%d to_item=%d directed=%t label=%v)\",\n\t\t\t\terr, fromPersonRowID, fromItemRowID, toPersonRowID, toItemRowID, !rr.Bidirectional, rr.Label)\n\t\t}\n\t}\n\n\treturn igRowID, nil\n}\n\nfunc (wc *WrappedClient) processSingleItemGraphNode(it Item, state *recursiveState) (int64, error) {\n\tif itemID := it.ID(); itemID != \"\" && state.cuckoo.Filter != nil {\n\t\tstate.cuckoo.Lock()\n\t\tstate.cuckoo.InsertUnique([]byte(itemID))\n\t\tstate.cuckoo.Unlock()\n\t}\n\n\titemRowID, err := wc.storeItemFromService(it, state.timestamp, state.procOpt)\n\tif err != nil {\n\t\treturn itemRowID, err\n\t}\n\n\t// item was stored successfully, so now keep track of the item with the highest\n\t// (latest, last, etc.) timestamp, so that get-latest operations can be resumed\n\t// after interruption without creating gaps in the data that would never be\n\t// filled in otherwise except with a get-all...\n\titemTS := it.Timestamp()\n\twc.lastItemMu.Lock()\n\tif wc.lastItemTimestamp.IsZero() || wc.lastItemTimestamp.Before(itemTS) {\n\t\twc.lastItemRowID = itemRowID\n\t\twc.lastItemTimestamp = itemTS\n\t}\n\twc.lastItemMu.Unlock()\n\n\treturn itemRowID, nil\n}\n\nfunc (wc *WrappedClient) storeItemFromService(it Item, timestamp time.Time, procOpt ProcessingOptions) (int64, error) {\n\tif it == nil {\n\t\treturn 0, nil\n\t}\n\n\titemOriginalID := it.ID()\n\n\t// if enabled, prepare a \"soft merge\" - this operation finds an existing row that\n\t// matches properties of an item that are LIKELY unique if they are, in fact, the\n\t// same item, without relying on the item's original_id alone (which might not be\n\t// consistent depending on the method used, for example importing from Google\n\t// Takeout for Google Photos has different IDs than using their API) -- and sets\n\t// the original_id field of the candidate row to that of the incoming row; this\n\t// will trigger a conflict in the query below that will cause a graceful update\n\tvar doingSoftMerge bool\n\tif procOpt.Merge.SoftMerge {\n\t\tvar err error\n\t\titemOriginalID, doingSoftMerge, err = wc.softMerge(it, procOpt)\n\t\tif err != nil {\n\t\t\treturn 0, fmt.Errorf(\"soft merge: %v\", err)\n\t\t}\n\t}\n\n\t// process this item only one at a time\n\titemLockID := fmt.Sprintf(\"%s_%d_%s\", wc.ds.ID, wc.acc.ID, itemOriginalID)\n\titemLocks.Lock(itemLockID)\n\tdefer itemLocks.Unlock(itemLockID)\n\n\t// if there is a data file, prepare to download it\n\t// and get its file name; but don't actually begin\n\t// downloading it until after it is in the DB, since\n\t// we need to know, if we encounter this item later,\n\t// whether it was downloaded successfully; if not,\n\t// like if the download was interrupted and we didn't\n\t// have a chance to clean up, we can overwrite any\n\t// existing file by that name.\n\trc, err := it.DataFileReader()\n\tif err != nil {\n\t\treturn 0, fmt.Errorf(\"getting item's data file content stream: %v\", err)\n\t}\n\tif rc != nil {\n\t\tdefer rc.Close()\n\t}\n\n\t// as we go, we'll decide whether we are to process and store this item's data\n\t// file or not, as it depends on a few factors; our default behavior is that\n\t// data in a new item is preferred and thus assimilated into the timeline,\n\t// so we begin by deciding simply based on whether this item has a file reader\n\tprocessDataFile := rc != nil\n\n\t// if the item is already in our DB, load it\n\tvar ir ItemRow\n\tif itemOriginalID != \"\" {\n\t\tir, err = wc.loadItemRow(wc.acc.ID, itemOriginalID)\n\t\tif err != nil {\n\t\t\treturn 0, fmt.Errorf(\"checking for item in database: %v\", err)\n\t\t}\n\t\tif ir.ID > 0 {\n\t\t\t// already have it\n\n\t\t\tif !wc.shouldProcessExistingItem(it, ir, doingSoftMerge, procOpt) {\n\t\t\t\tif procOpt.Verbose {\n\t\t\t\t\tlog.Printf(\"[DEBUG] %s: skipping processing of existing item (item_id=%s item_row_id=%d soft_merge=%t)\",\n\t\t\t\t\t\twc.acc, itemOriginalID, ir.ID, doingSoftMerge)\n\t\t\t\t}\n\t\t\t\treturn ir.ID, nil\n\t\t\t}\n\n\t\t\t// update our decision to process the data file; we know we already have this item, so only keep the\n\t\t\t// new data file if this item has a data file and either: the old item does not have one; we are\n\t\t\t// reprocessing (meaning replacing old items, not merging); or we are merging and the user prefers\n\t\t\t// new data files over old ones\n\t\t\tprocessDataFile = processDataFile && (ir.DataFile == nil || !doingSoftMerge || procOpt.Merge.PreferNewDataFile)\n\n\t\t\t// if we are in fact processing this data file, move any old one out of the way temporarily\n\t\t\t// as a safe measure, and also because our filename-generator will not allow a file to be\n\t\t\t// overwritten, but we want to replace the existing file in this case...\n\t\t\tif processDataFile {\n\t\t\t\torigFile := wc.tl.fullpath(*ir.DataFile)\n\t\t\t\tbakFile := wc.tl.fullpath(*ir.DataFile + \".bak\")\n\t\t\t\terr = os.Rename(origFile, bakFile)\n\t\t\t\tif err != nil && !os.IsNotExist(err) {\n\t\t\t\t\treturn 0, fmt.Errorf(\"temporarily moving data file: %v\", err)\n\t\t\t\t}\n\n\t\t\t\t// if this function returns with an error,\n\t\t\t\t// restore the original file in case it was\n\t\t\t\t// partially written or something; otherwise\n\t\t\t\t// delete the old file altogether\n\t\t\t\tdefer func() {\n\t\t\t\t\tif err == nil {\n\t\t\t\t\t\terr := os.Remove(bakFile)\n\t\t\t\t\t\tif err != nil && !os.IsNotExist(err) {\n\t\t\t\t\t\t\tlog.Printf(\"[ERROR] deleting data file backup: %v\", err)\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\terr := os.Rename(bakFile, origFile)\n\t\t\t\t\t\tif err != nil && !os.IsNotExist(err) {\n\t\t\t\t\t\t\tlog.Printf(\"[ERROR] restoring original data file from backup: %v\", err)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}()\n\t\t\t}\n\t\t}\n\t}\n\n\t// get the filename for the data file if we are processing it\n\tvar dataFileName *string\n\tvar datafile *os.File\n\tif processDataFile {\n\t\tdatafile, dataFileName, err = wc.tl.openUniqueCanonicalItemDataFile(it, wc.ds.ID)\n\t\tif err != nil {\n\t\t\treturn 0, fmt.Errorf(\"opening output data file: %v\", err)\n\t\t}\n\t\tdefer datafile.Close()\n\t}\n\n\t// prepare the item's DB row values\n\terr = wc.fillItemRow(&ir, it, itemOriginalID, timestamp, dataFileName)\n\tif err != nil {\n\t\treturn 0, fmt.Errorf(\"assembling item for storage: %v\", err)\n\t}\n\n\t// run the database query to insert or update the item\n\terr = wc.insertOrUpdateItem(ir, doingSoftMerge, procOpt)\n\tif err != nil {\n\t\treturn 0, fmt.Errorf(\"storing item in database: %v (item_id=%v)\", err, ir.OriginalID)\n\t}\n\n\t// get the item's row ID (this works regardless of whether the last query was an insert or an update)\n\tvar itemRowID int64\n\terr = wc.tl.db.QueryRow(`SELECT id FROM items\n\t\tWHERE account_id=? AND original_id=? LIMIT 1`,\n\t\tir.AccountID, ir.OriginalID).Scan(&itemRowID)\n\tif err != nil && err != sql.ErrNoRows {\n\t\treturn 0, fmt.Errorf(\"getting item row ID: %v\", err)\n\t}\n\n\tif procOpt.Verbose {\n\t\tlog.Printf(\"[DEBUG] %s: stored or updated item in database (item_id=%s item_row_id=%d soft_merge=%t)\",\n\t\t\twc.acc, itemOriginalID, itemRowID, doingSoftMerge)\n\t}\n\n\t// if there is a data file, download it and compute its checksum;\n\t// then update the item's row in the DB with its name and checksum\n\tif processDataFile {\n\t\th := sha256.New()\n\t\tdataFileSize, err := wc.tl.downloadItemFile(rc, datafile, h)\n\t\tif err != nil {\n\t\t\treturn 0, fmt.Errorf(\"downloading data file: %v (item_id=%v)\", err, itemRowID)\n\t\t}\n\n\t\t// now that download is complete, compute its hash\n\t\tdfHash := h.Sum(nil)\n\t\tb64hash := base64.StdEncoding.EncodeToString(dfHash)\n\n\t\t// if the exact same file (byte-for-byte) already exists,\n\t\t// delete this copy and reuse the existing one\n\t\terr = wc.tl.replaceWithExisting(dataFileName, b64hash, itemRowID)\n\t\tif err != nil {\n\t\t\treturn 0, fmt.Errorf(\"replacing data file with identical existing file: %v\", err)\n\t\t}\n\n\t\t// save the file's name and hash to confirm it was downloaded successfully\n\t\t_, err = wc.tl.db.Exec(`UPDATE items SET data_hash=? WHERE id=?`, // TODO: LIMIT 1... (see https://github.com/mattn/go-sqlite3/pull/802)\n\t\t\tb64hash, itemRowID)\n\t\tif err != nil {\n\t\t\tlog.Printf(\"[ERROR] %s: updating item's data file hash in DB: %v; cleaning up data file: %s (item_id=%d)\",\n\t\t\t\twc.acc, err, datafile.Name(), itemRowID)\n\t\t\tos.Remove(wc.tl.fullpath(*dataFileName))\n\t\t}\n\n\t\tif procOpt.Verbose {\n\t\t\tlog.Printf(\"[DEBUG] %s: downloaded data file (item_id=%s filename=%s size=%d)\",\n\t\t\t\twc.acc, itemOriginalID, *dataFileName, dataFileSize)\n\t\t}\n\t}\n\n\treturn itemRowID, nil\n}\n\nfunc (wc *WrappedClient) shouldProcessExistingItem(it Item, dbItem ItemRow, doingSoftMerge bool, procOpt ProcessingOptions) bool {\n\t// if integrity check is enabled and checksum mismatches, always reprocess\n\tif procOpt.Integrity && dbItem.DataFile != nil && dbItem.DataHash != nil {\n\t\tdatafile, err := os.Open(wc.tl.fullpath(*dbItem.DataFile))\n\t\tif err != nil {\n\t\t\tlog.Printf(\"[ERROR] %s: integrity check: opening existing data file: %v; reprocessing (item_id=%d)\",\n\t\t\t\twc.acc, err, dbItem.ID)\n\t\t\treturn true\n\t\t}\n\t\tdefer datafile.Close()\n\t\th := sha256.New()\n\t\t_, err = io.Copy(h, datafile)\n\t\tif err != nil {\n\t\t\tlog.Printf(\"[ERROR] %s: integrity check: reading existing data file: %v; reprocessing (item_id=%d)\",\n\t\t\t\twc.acc, err, dbItem.ID)\n\t\t\treturn true\n\t\t}\n\t\tb64hash := base64.StdEncoding.EncodeToString(h.Sum(nil))\n\t\tif b64hash != *dbItem.DataHash {\n\t\t\tlog.Printf(\"[ERROR] %s: integrity check: checksum mismatch: expected %s, got %s; reprocessing (item_id=%d)\",\n\t\t\t\twc.acc, *dbItem.DataHash, b64hash, dbItem.ID)\n\t\t\treturn true\n\t\t}\n\t}\n\n\t// if modified locally, do not overwrite changes\n\tif dbItem.Modified != nil {\n\t\treturn false\n\t}\n\n\t// if a data file is expected, but no completed file exists\n\t// (i.e. its hash is missing), then reprocess to allow download\n\t// to complete successfully this time\n\tif dbItem.DataFile != nil && dbItem.DataHash == nil {\n\t\treturn true\n\t}\n\n\t// if service reports hashes/etags and we see that it\n\t// has changed, reprocess\n\tif serviceHash := it.DataFileHash(); serviceHash != nil &&\n\t\tdbItem.Metadata != nil &&\n\t\tdbItem.Metadata.ServiceHash != nil &&\n\t\t!bytes.Equal(serviceHash, dbItem.Metadata.ServiceHash) {\n\t\treturn true\n\t}\n\n\t// finally, if the user wants to reprocess anyway, then do so;\n\t// or if we are doing a soft merge (merging one identical item\n\t// into an existing one), always return true so that the merge\n\t// will actually take place\n\treturn procOpt.Reprocess || doingSoftMerge\n}\n\nfunc (wc *WrappedClient) fillItemRow(ir *ItemRow, it Item, itemOriginalID string, timestamp time.Time, canonicalDataFileName *string) error {\n\t// unpack the item's information into values to use in the row\n\n\townerID, ownerName := it.Owner()\n\tif ownerID == nil {\n\t\townerID = &wc.acc.UserID // assume current account\n\t}\n\tif ownerName == nil {\n\t\tempty := \"\"\n\t\townerName = &empty\n\t}\n\tperson, err := wc.tl.getPerson(wc.ds.ID, *ownerID, *ownerName)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"getting person associated with item: %v\", err)\n\t}\n\n\ttxt, err := it.DataText()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"getting item text: %v\", err)\n\t}\n\n\tloc, err := it.Location()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"getting item location data: %v\", err)\n\t}\n\tif loc == nil {\n\t\tloc = new(Location) // avoid nil pointer dereference below\n\t}\n\n\t// metadata (optional) needs to be gob-encoded\n\tmetadata, err := it.Metadata()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"getting item metadata: %v\", err)\n\t}\n\tif serviceHash := it.DataFileHash(); serviceHash != nil {\n\t\tmetadata.ServiceHash = serviceHash\n\t}\n\tvar metaGob []byte\n\tif metadata != nil {\n\t\tmetaGob, err = metadata.encode() // use special encoding method for massive space savings\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"gob-encoding metadata: %v\", err)\n\t\t}\n\t}\n\n\tir.AccountID = wc.acc.ID\n\tir.OriginalID = itemOriginalID\n\tir.PersonID = person.ID\n\tir.Timestamp = it.Timestamp()\n\tir.Stored = timestamp\n\tir.Class = it.Class()\n\tir.MIMEType = it.DataFileMIMEType()\n\tir.DataText = txt\n\tir.DataFile = canonicalDataFileName\n\tir.Metadata = metadata\n\tir.metaGob = metaGob\n\tir.Location = *loc\n\n\t// not used in the DB, but if we need to get the item's\n\t// original file name, for example, rather than the\n\t// unique filename to be used on disk...\n\tir.item = it\n\n\treturn nil\n}\n\n// softMerge finds a candidate row that already exists in the DB that is likely to be identical\n// to it, even if the original_id does not match, and updates the original_id field to that of it\n// if there is exactly 1 matching row and if procOpt permits. This will allow the existing row\n// to be merged with the incoming row even though their original_ids do not match. This is a\n// best-guess effort based on timestamp and data_text/data_file/data_hash (the hash must be one\n// that is offered by the data source, as the post-download hashing is not known until after\n// downloading the file, obviously; since most data sources don't offer one, in practice soft\n// merges happen over timestamp plus filename or text data only). It returns the ID that must\n// be used when processing the item, and whether a soft merge is being performed or not.\nfunc (wc *WrappedClient) softMerge(it Item, procOpt ProcessingOptions) (string, bool, error) {\n\tvar filenameLikePattern *string\n\tif dataFileName := it.DataFileName(); dataFileName != nil {\n\t\ttemp := \"%/\" + *dataFileName\n\t\tfilenameLikePattern = &temp\n\t}\n\n\tnewOriginalID := it.ID()\n\tdataText, err := it.DataText()\n\tif err != nil {\n\t\treturn newOriginalID, false, fmt.Errorf(\"getting item text: %v\", err)\n\t}\n\tdataHash := it.DataFileHash()\n\tif err != nil {\n\t\treturn newOriginalID, false, fmt.Errorf(\"getting item data hash: %v\", err)\n\t}\n\n\t// make sure there is exactly 1 matching row; any more is ambiguous and too risky to merge\n\t// (also make sure the existing original_id does not match the new one; that would be a regular merge)\n\tvar numMatches int\n\tvar rowID *int\n\tvar oldOriginalID *string\n\terr = wc.tl.db.QueryRow(`SELECT COUNT(1), id, original_id\n\t\t\tFROM items\n\t\t\tWHERE account_id=? AND timestamp=? AND (data_text=? OR data_file LIKE ? OR data_hash=?) AND original_id != ?\n\t\t\tLIMIT 1`,\n\t\twc.acc.ID, it.Timestamp().Unix(), dataText, filenameLikePattern, dataHash, newOriginalID).Scan(&numMatches, &rowID, &oldOriginalID)\n\tif err == sql.ErrNoRows || numMatches == 0 {\n\t\treturn newOriginalID, false, nil\n\t}\n\tif err != nil {\n\t\treturn newOriginalID, false, fmt.Errorf(\"querying for candidate row: %v\", err)\n\t}\n\tif numMatches > 1 {\n\t\treturn newOriginalID, false, fmt.Errorf(\"ambiguous match with %d existing items (account_id=%d timestamp=%d data_text=%p data_file=%p) - unable to merge, skipping item with ID: %s\",\n\t\t\tnumMatches, wc.acc.ID, it.Timestamp().Unix(), dataText, filenameLikePattern, newOriginalID)\n\t}\n\n\t// now we know there is exactly one match, so we are to perform a soft merge;\n\t// we must honor the configured merge preferences especially regarding ID\n\n\t// if configured to keep existing ID, make sure the caller knows to use the\n\t// existing/old ID rather than the ID associated with the current/new item\n\tif !procOpt.Merge.PreferNewID {\n\t\tlog.Printf(\"[INFO] Soft merging new item with id=%s into row %d with existing item id=%s (using existing item ID)\", newOriginalID, rowID, *oldOriginalID)\n\t\treturn *oldOriginalID, true, nil\n\t}\n\n\t// now we know there is exactly 1 match and we are to use the new item's ID; set up merge by\n\t// updating the item's original_id to the incoming item's ID value; this will cause the\n\t// imminent INSERT query to find a conflict and perform a graceful merge with the incoming data\n\t_, err = wc.tl.db.Exec(`UPDATE items SET original_id=? WHERE id=?`, newOriginalID, rowID) // TODO: limit 1 (see https://github.com/mattn/go-sqlite3/pull/802)\n\tif err != nil && err != sql.ErrNoRows {\n\t\treturn newOriginalID, false, fmt.Errorf(\"updating candidate row's original_id in DB: %v (id=%d old_original_id=%s new_original_id=%s)\",\n\t\t\terr, rowID, *oldOriginalID, newOriginalID)\n\t}\n\n\tlog.Printf(\"[INFO] Soft merging new item with id=%s into row %d with existing item id=%s (changed item ID)\", newOriginalID, rowID, *oldOriginalID)\n\n\treturn newOriginalID, true, nil\n}\n\nfunc (wc *WrappedClient) processCollection(coll Collection, timestamp time.Time, procOpt ProcessingOptions) error {\n\t// never reprocess or check integrity when storing items in collections since the main processing handles that\n\tprocOpt.Reprocess = false\n\tprocOpt.Integrity = false\n\n\t// TODO: support soft merge (based on name, I guess)\n\t_, err := wc.tl.db.Exec(`INSERT INTO collections\n\t\t(account_id, original_id, name) VALUES (?, ?, ?)\n\t\tON CONFLICT (account_id, original_id)\n\t\tDO UPDATE SET name=?`,\n\t\twc.acc.ID, coll.OriginalID, coll.Name,\n\t\tcoll.Name)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"inserting collection: %v\", err)\n\t}\n\n\t// get the collection's row ID, regardless of whether it was inserted or updated\n\tvar collID int64\n\terr = wc.tl.db.QueryRow(`SELECT id FROM collections\n\t\t\tWHERE account_id=? AND original_id=? LIMIT 1`,\n\t\twc.acc.ID, coll.OriginalID).Scan(&collID)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"getting existing collection's row ID: %v\", err)\n\t}\n\n\t// now add all the items\n\t// (TODO: could batch this for faster inserts)\n\tfor _, cit := range coll.Items {\n\t\tif cit.itemRowID == 0 {\n\t\t\titID, err := wc.storeItemFromService(cit.Item, timestamp, procOpt)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"adding item from collection to storage: %v\", err)\n\t\t\t}\n\t\t\tcit.itemRowID = itID\n\t\t}\n\n\t\t_, err = wc.tl.db.Exec(`INSERT OR IGNORE INTO collection_items\n\t\t\t(item_id, collection_id, position)\n\t\t\tVALUES (?, ?, ?)`,\n\t\t\tcit.itemRowID, collID, cit.Position, cit.Position)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"adding item to collection: %v\", err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (wc *WrappedClient) loadItemRow(accountID int64, originalID string) (ItemRow, error) {\n\tvar ir ItemRow\n\tvar metadataGob []byte\n\tvar ts, stored int64 // will convert from Unix timestamp\n\tvar modified *int64\n\terr := wc.tl.db.QueryRow(`SELECT\n\t\t\tid, account_id, original_id, person_id, timestamp, stored,\n\t\t\tmodified, class, mime_type, data_text, data_file, data_hash,\n\t\t\tmetadata, latitude, longitude\n\t\tFROM items WHERE account_id=? AND original_id=? LIMIT 1`, accountID, originalID).Scan(\n\t\t&ir.ID, &ir.AccountID, &ir.OriginalID, &ir.PersonID, &ts, &stored,\n\t\t&modified, &ir.Class, &ir.MIMEType, &ir.DataText, &ir.DataFile, &ir.DataHash,\n\t\t&metadataGob, &ir.Latitude, &ir.Longitude)\n\tif err == sql.ErrNoRows {\n\t\treturn ItemRow{}, nil\n\t}\n\tif err != nil {\n\t\treturn ItemRow{}, fmt.Errorf(\"loading item: %v\", err)\n\t}\n\n\t// the metadata is gob-encoded; decode it into the struct\n\tir.Metadata = new(Metadata)\n\terr = ir.Metadata.decode(metadataGob)\n\tif err != nil {\n\t\treturn ItemRow{}, fmt.Errorf(\"gob-decoding metadata: %v\", err)\n\t}\n\n\tir.Timestamp = time.Unix(ts, 0)\n\tir.Stored = time.Unix(stored, 0)\n\tif modified != nil {\n\t\tmodTime := time.Unix(*modified, 0)\n\t\tir.Modified = &modTime\n\t}\n\n\treturn ir, nil\n}\n\n// insertOrUpdateItem inserts the fully-populated ir into the database or, if there is a conflict on\n// the item's account_id and original_id, it updates the existing row. If softMerge is true, the\n// update is an additive merge defined by procOpt; otherwise, updates always replace the old values.\nfunc (wc *WrappedClient) insertOrUpdateItem(ir ItemRow, softMerge bool, procOpt ProcessingOptions) error {\n\tfieldPersonID, fieldTimestamp, fieldStored, fieldClass,\n\t\tfieldMimeType, fieldDataText, fieldDataFile, fieldDataHash,\n\t\tfieldMetadata, fieldLatitude, fieldLongitude := \"?\", \"?\", \"?\", \"?\", \"?\", \"?\", \"?\", \"?\", \"?\", \"?\", \"?\"\n\n\tif softMerge {\n\t\t// when merging, prefer existing value by default (i.e. by\n\t\t// default, merging is only additive with new values and does\n\t\t// not replace existing fields when there are conflicts);\n\t\t// this seems safer (user must opt-in to overwrite data)\n\t\tfieldPersonID = \"COALESCE(person_id, ?)\"\n\t\tfieldTimestamp = \"COALESCE(timestamp, ?)\"\n\t\tfieldClass = \"COALESCE(class, ?)\"\n\t\tfieldMimeType = \"COALESCE(mime_type, ?)\"\n\t\tfieldDataText = \"COALESCE(data_text, ?)\"\n\t\tfieldDataFile = \"COALESCE(data_file, ?)\"\n\t\tfieldDataHash = \"COALESCE(data_hash, ?)\"\n\t\tfieldMetadata = \"COALESCE(metadata, ?)\"\n\t\tfieldLatitude = \"COALESCE(latitude, ?)\"\n\t\tfieldLongitude = \"COALESCE(longitude, ?)\"\n\n\t\tif procOpt.Merge.PreferNewDataText {\n\t\t\tfieldDataText = \"COALESCE(?, data_text)\"\n\t\t}\n\t\tif procOpt.Merge.PreferNewMetadata {\n\t\t\tfieldMetadata = \"COALESCE(?, metadata)\"\n\t\t}\n\t}\n\n\t// insert into the DB if it does not exist, and if it does, we update the existing\n\t// row such that we usually prefer the new value, but if the new value is nil, keep\n\t// the existing value (this is mostly an additive \"merge\" of the stored row with\n\t// the incoming row, except that if both values are not null, we overwrite existing\n\t// value with the new one); 'coalesce(?, field)' means \"store new value if not null,\n\t// otherwise keep existing value\"; i.e. the incoming data is authoritative unless it\n\t// is missing, in which case we keep what we have\n\t_, err := wc.tl.db.Exec(`INSERT INTO items\n\t\t\t(account_id, original_id, person_id, timestamp, stored,\n\t\t\t\tclass, mime_type, data_text, data_file, data_hash, metadata,\n\t\t\t\tlatitude, longitude)\n\t\t\tVALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n\t\t\tON CONFLICT (account_id, original_id) DO UPDATE\n\t\t\tSET person_id=`+fieldPersonID+`,\n\t\t\t\ttimestamp=`+fieldTimestamp+`,\n\t\t\t\tstored=`+fieldStored+`,\n\t\t\t\tclass=`+fieldClass+`,\n\t\t\t\tmime_type=`+fieldMimeType+`,\n\t\t\t\tdata_text=`+fieldDataText+`,\n\t\t\t\tdata_file=`+fieldDataFile+`,\n\t\t\t\tdata_hash=`+fieldDataHash+`,\n\t\t\t\tmetadata=`+fieldMetadata+`,\n\t\t\t\tlatitude=`+fieldLatitude+`,\n\t\t\t\tlongitude=`+fieldLongitude,\n\t\tir.AccountID, ir.OriginalID, ir.PersonID, ir.Timestamp.Unix(), ir.Stored.Unix(),\n\t\tir.Class, ir.MIMEType, ir.DataText, ir.DataFile, ir.DataHash, ir.metaGob,\n\t\tir.Latitude, ir.Longitude,\n\t\tir.PersonID, ir.Timestamp.Unix(), ir.Stored.Unix(), ir.Class, ir.MIMEType, ir.DataText,\n\t\tir.DataFile, ir.DataHash, ir.metaGob, ir.Latitude, ir.Longitude)\n\n\treturn err\n}\n\n// itemRowIDFromOriginalID returns an item's row ID from the ID\n// associated with the data source of wc, along with its original\n// item ID from that data source. If the item does not exist,\n// sql.ErrNoRows will be returned. A pointer is returned because\n// the column is nullable in the DB.\nfunc (wc *WrappedClient) itemRowIDFromOriginalID(originalID string) (*int64, error) {\n\tvar rowID int64\n\terr := wc.tl.db.QueryRow(`SELECT items.id\n\t\t\tFROM items, accounts\n\t\t\tWHERE items.original_id=?\n\t\t\t\tAND accounts.data_source_id=?\n\t\t\t\tAND items.account_id = accounts.id\n\t\t\tLIMIT 1`, originalID, wc.ds.ID).Scan(&rowID)\n\treturn &rowID, err\n}\n\n// personRowIDFromUserID returns a person's row ID from the user ID\n// associated with the data source of wc. If the person does not exist,\n// sql.ErrNoRows will be returned. A pointer is returned because the\n// column is nullable in the DB.\nfunc (wc *WrappedClient) personRowIDFromUserID(userID string) (*int64, error) {\n\tvar rowID int64\n\terr := wc.tl.db.QueryRow(`SELECT person_id FROM person_identities\n\t\tWHERE data_source_id=? AND user_id=? LIMIT 1`,\n\t\twc.ds.ID, userID).Scan(&rowID)\n\treturn &rowID, err\n}\n\n// itemLocks is used to ensure that an item\n// is not processed twice at the same time.\nvar itemLocks = newMapMutex()\n"
        },
        {
          "name": "ratelimit.go",
          "type": "blob",
          "size": 1.4453125,
          "content": "package timeliner\n\nimport (\n\t\"net/http\"\n\t\"time\"\n)\n\n// RateLimit describes a rate limit.\ntype RateLimit struct {\n\tRequestsPerHour int\n\tBurstSize       int\n\n\tticker *time.Ticker\n\ttoken  chan struct{}\n}\n\n// NewRateLimitedRoundTripper adds rate limiting to rt based on the rate\n// limiting policy registered by the data source associated with acc.\nfunc (acc Account) NewRateLimitedRoundTripper(rt http.RoundTripper) http.RoundTripper {\n\trl, ok := acc.t.rateLimiters[acc.String()]\n\n\tif !ok && acc.ds.RateLimit.RequestsPerHour > 0 {\n\t\tsecondsBetweenReqs := 60.0 / (float64(acc.ds.RateLimit.RequestsPerHour) / 60.0)\n\t\tmillisBetweenReqs := secondsBetweenReqs * 1000.0\n\t\treqInterval := time.Duration(millisBetweenReqs) * time.Millisecond\n\t\tif reqInterval < minInterval {\n\t\t\treqInterval = minInterval\n\t\t}\n\n\t\trl.ticker = time.NewTicker(reqInterval)\n\t\trl.token = make(chan struct{}, rl.BurstSize)\n\n\t\tfor i := 0; i < cap(rl.token); i++ {\n\t\t\trl.token <- struct{}{}\n\t\t}\n\t\tgo func() {\n\t\t\tfor range rl.ticker.C {\n\t\t\t\trl.token <- struct{}{}\n\t\t\t}\n\t\t}()\n\n\t\tacc.t.rateLimiters[acc.String()] = rl\n\t}\n\n\treturn rateLimitedRoundTripper{\n\t\tRoundTripper: rt,\n\t\ttoken:        rl.token,\n\t}\n}\n\ntype rateLimitedRoundTripper struct {\n\thttp.RoundTripper\n\ttoken <-chan struct{}\n}\n\nfunc (rt rateLimitedRoundTripper) RoundTrip(req *http.Request) (*http.Response, error) {\n\t<-rt.token\n\treturn rt.RoundTripper.RoundTrip(req)\n}\n\nvar rateLimiters = make(map[string]RateLimit)\n\nconst minInterval = 100 * time.Millisecond\n"
        },
        {
          "name": "timeliner.go",
          "type": "blob",
          "size": 6.498046875,
          "content": "// Timeliner - A personal data aggregation utility\n// Copyright (C) 2019 Matthew Holt\n//\n// This program is free software: you can redistribute it and/or modify\n// it under the terms of the GNU Affero General Public License as published\n// by the Free Software Foundation, either version 3 of the License, or\n// (at your option) any later version.\n//\n// This program is distributed in the hope that it will be useful,\n// but WITHOUT ANY WARRANTY; without even the implied warranty of\n// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n// GNU Affero General Public License for more details.\n//\n// You should have received a copy of the GNU Affero General Public License\n// along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\n// TODO: Apply license to all files\n\npackage timeliner\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"fmt\"\n\t\"io\"\n\t\"log\"\n\tmathrand \"math/rand\"\n\t\"sync\"\n\t\"time\"\n\n\tcuckoo \"github.com/seiflotfy/cuckoofilter\"\n)\n\nfunc init() {\n\tmathrand.Seed(time.Now().UnixNano())\n}\n\n// Timeline represents an opened timeline repository.\n// The zero value is NOT valid; use Open() to obtain\n// a valid value.\ntype Timeline struct {\n\tdb           *sql.DB\n\trepoDir      string\n\trateLimiters map[string]RateLimit\n}\n\n// Open creates/opens a timeline at the given\n// repository directory. Timelines should always\n// be Close()'d for a clean shutdown when done.\nfunc Open(repo string) (*Timeline, error) {\n\tdb, err := openDB(repo)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"opening database: %v\", err)\n\t}\n\treturn &Timeline{\n\t\tdb:           db,\n\t\trepoDir:      repo,\n\t\trateLimiters: make(map[string]RateLimit),\n\t}, nil\n}\n\n// Close frees up resources allocated from Open.\nfunc (t *Timeline) Close() error {\n\tfor key, rl := range t.rateLimiters {\n\t\tif rl.ticker != nil {\n\t\t\trl.ticker.Stop()\n\t\t\trl.ticker = nil\n\t\t}\n\t\tdelete(t.rateLimiters, key)\n\t}\n\tif t.db != nil {\n\t\treturn t.db.Close()\n\t}\n\treturn nil\n}\n\ntype concurrentCuckoo struct {\n\t*cuckoo.Filter\n\t*sync.Mutex\n}\n\n// FakeCloser turns an io.Reader into an io.ReadCloser\n// where the Close() method does nothing.\nfunc FakeCloser(r io.Reader) io.ReadCloser {\n\treturn fakeCloser{r}\n}\n\ntype fakeCloser struct {\n\tio.Reader\n}\n\n// Close does nothing except satisfy io.Closer.\nfunc (fc fakeCloser) Close() error { return nil }\n\n// ctxKey is used for contexts, as recommended by\n// https://golang.org/pkg/context/#WithValue. It\n// is unexported so values stored by this package\n// can only be accessed by this package.\ntype ctxKey string\n\n// wrappedClientCtxKey is how the context value is accessed.\nvar wrappedClientCtxKey ctxKey = \"wrapped_client\"\n\n// CheckpointFn is a function that saves a checkpoint.\ntype CheckpointFn func(checkpoint []byte) error\n\n// Checkpoint saves a checkpoint for the processing associated\n// with the provided context. It overwrites any previous\n// checkpoint. Any errors are logged.\nfunc Checkpoint(ctx context.Context, checkpoint []byte) {\n\twc, ok := ctx.Value(wrappedClientCtxKey).(*WrappedClient)\n\n\tif !ok {\n\t\tlog.Printf(\"[ERROR][%s/%s] Checkpoint function not available; got type %T (%#v)\",\n\t\t\twc.ds.ID, wc.acc.UserID, wc, wc)\n\t\treturn\n\t}\n\n\tchkpt, err := MarshalGob(checkpointWrapper{wc.commandParams, checkpoint})\n\tif err != nil {\n\t\tlog.Printf(\"[ERROR][%s/%s] Encoding checkpoint wrapper: %v\", wc.ds.ID, wc.acc.UserID, err)\n\t\treturn\n\t}\n\n\t_, err = wc.tl.db.Exec(`UPDATE accounts SET checkpoint=? WHERE id=?`, // TODO: LIMIT 1 (see https://github.com/mattn/go-sqlite3/pull/564)\n\t\tchkpt, wc.acc.ID)\n\tif err != nil {\n\t\tlog.Printf(\"[ERROR][%s/%s] Checkpoint: %v\", wc.ds.ID, wc.acc.UserID, err)\n\t\treturn\n\t}\n}\n\n// checkpointWrapper stores a provider's checkpoint along with the\n// parameters of the command that initiated the process; the checkpoint\n// will only be loaded and restored to the provider on next run if\n// the parameters match, because it doesn't make sense to restore a\n// process that has different, potentially conflicting, parameters,\n// such as timeframe.\ntype checkpointWrapper struct {\n\tParams string\n\tData   []byte\n}\n\n// ProcessingOptions configures how item processing is carried out.\ntype ProcessingOptions struct {\n\tReprocess bool\n\tPrune     bool\n\tIntegrity bool\n\tTimeframe Timeframe\n\tMerge     MergeOptions\n\tVerbose   bool\n}\n\n// MergeOptions configures how items are merged. By\n// default, items are not merged; if an item with a\n// duplicate ID is encountered, it will be replaced\n// with the new item (see the \"reprocess\" flag).\n// Merging has to be explicitly enabled.\n//\n// Currently, the only way to perform a merge is to\n// enable \"soft\" merging: finding an item with the\n// same timestamp and either text data or filename.\n// Then, one of the item's IDs is updated to match\n// the other. These merge options configure how\n// the items are then combined.\n//\n// As it is possible and likely for both items to\n// have non-empty values for the same fields, these\n// \"conflicts\" must be resolved non-interactively.\n// By default, a merge conflict prefers existing\n// values (old item's field) over the new one, and\n// the new one only fills in missing values. (This\n// seems safest.) However, these merge options allow\n// you to customize that behavior and overwrite\n// existing values with the new item's fields (only\n// happens if new item's field is non-empty, i.e.\n// a merge will never delete existing data).\ntype MergeOptions struct {\n\t// Enables \"soft\" merging.\n\t//\n\t// If true, an item may be merged if it is likely\n\t// to be the same as an existing item, even if the\n\t// item IDs are different. For example, if a\n\t// service has multiple ways of listing items, but\n\t// does not provide a consistent ID for the same\n\t// item across listings, a soft merge will allow the\n\t// processing to treat them as the same as long as\n\t// other fields match: timestamp, and either data text\n\t// or data filename.\n\tSoftMerge bool\n\n\t// Overwrite existing (old) item's ID with the ID\n\t// provided by the current (new) item.\n\tPreferNewID bool\n\n\t// Overwrite existing item's text data.\n\tPreferNewDataText bool\n\n\t// Overwrite existing item's data file.\n\tPreferNewDataFile bool\n\n\t// Overwrite existing item's metadata.\n\tPreferNewMetadata bool\n}\n\n// ListingOptions specifies parameters for listing items\n// from a data source. Some data sources might not be\n// able to honor all fields.\ntype ListingOptions struct {\n\t// A file from which to read the data.\n\tFilename string\n\n\t// Time bounds on which data to retrieve.\n\t// The respective time and item ID fields\n\t// which are set must never conflict.\n\tTimeframe Timeframe\n\n\t// A checkpoint from which to resume\n\t// item retrieval.\n\tCheckpoint []byte\n\n\t// Enable verbose output (logs).\n\tVerbose bool\n}\n"
        },
        {
          "name": "wrappedclient.go",
          "type": "blob",
          "size": 10.3349609375,
          "content": "package timeliner\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\t\"sync\"\n\t\"time\"\n\n\tcuckoo \"github.com/seiflotfy/cuckoofilter\"\n)\n\n// WrappedClient wraps a Client instance with unexported\n// fields that contain necessary state for performing\n// data collection operations. Do not craft this type\n// manually; use Timeline.NewClient() to obtain one.\ntype WrappedClient struct {\n\tClient\n\ttl  *Timeline\n\tacc Account\n\tds  DataSource\n\n\tlastItemRowID     int64\n\tlastItemTimestamp time.Time\n\tlastItemMu        *sync.Mutex\n\n\t// used with checkpoints; it only makes sense to resume a checkpoint\n\t// if the process has the same operational parameters as before;\n\t// some providers (like Google Photos) even return errors if you\n\t// query a \"next page\" with different parameters\n\tcommandParams string\n}\n\n// GetLatest gets the most recent items from wc. It does not prune or\n// reprocess; only meant for a quick pull (error will be returned if\n// procOpt is not compatible). If there are no items pulled yet, all\n// items will be pulled. If procOpt.Timeframe.Until is not nil, the\n// latest only up to that timestamp will be pulled, and if until is\n// after the latest item, no items will be pulled.\nfunc (wc *WrappedClient) GetLatest(ctx context.Context, procOpt ProcessingOptions) error {\n\tif ctx == nil {\n\t\tctx = context.Background()\n\t}\n\tctx = context.WithValue(ctx, wrappedClientCtxKey, wc)\n\n\tif procOpt.Reprocess || procOpt.Prune || procOpt.Integrity || procOpt.Timeframe.Since != nil {\n\t\treturn fmt.Errorf(\"get-latest does not support -reprocess, -prune, -integrity, or -start\")\n\t}\n\n\t// get date and original ID of the most recent item for this\n\t// account from the last successful run\n\tvar mostRecentTimestamp int64\n\tvar mostRecentOriginalID string\n\tif wc.acc.lastItemID != nil {\n\t\terr := wc.tl.db.QueryRow(`SELECT timestamp, original_id\n\t\tFROM items WHERE id=? LIMIT 1`, *wc.acc.lastItemID).Scan(&mostRecentTimestamp, &mostRecentOriginalID)\n\t\tif err != nil && err != sql.ErrNoRows {\n\t\t\treturn fmt.Errorf(\"getting most recent item: %v\", err)\n\t\t}\n\t}\n\n\t// constrain the pull to the recent timeframe\n\ttimeframe := Timeframe{Until: procOpt.Timeframe.Until}\n\tif mostRecentTimestamp > 0 {\n\t\tts := time.Unix(mostRecentTimestamp, 0)\n\t\ttimeframe.Since = &ts\n\t\tif timeframe.Until != nil && timeframe.Until.Before(ts) {\n\t\t\t// most recent item is already after \"until\"/end date; nothing to do\n\t\t\treturn nil\n\t\t}\n\t}\n\tif mostRecentOriginalID != \"\" {\n\t\ttimeframe.SinceItemID = &mostRecentOriginalID\n\t}\n\n\tcheckpoint := wc.prepareCheckpoint(timeframe)\n\n\twg, ch := wc.beginProcessing(concurrentCuckoo{}, procOpt)\n\n\terr := wc.Client.ListItems(ctx, ch, ListingOptions{\n\t\tTimeframe:  timeframe,\n\t\tCheckpoint: checkpoint,\n\t\tVerbose:    procOpt.Verbose,\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"getting items from service: %v\", err)\n\t}\n\n\t// wait for processing to complete\n\twg.Wait()\n\n\terr = wc.successCleanup()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"processing completed, but error cleaning up: %v\", err)\n\t}\n\n\treturn nil\n}\n\n// GetAll gets all the items using wc. If procOpt.Reprocess is true, items that\n// are already in the timeline will be re-processed. If procOpt.Prune is true,\n// items that are not listed on the data source by wc will be removed\n// from the timeline at the end of the listing. If procOpt.Integrity is true,\n// all items that are listed by wc that exist in the timeline and which\n// consist of a data file will be opened and checked for integrity; if\n// the file has changed, it will be reprocessed.\nfunc (wc *WrappedClient) GetAll(ctx context.Context, procOpt ProcessingOptions) error {\n\tif wc.Client == nil {\n\t\treturn fmt.Errorf(\"no client\")\n\t}\n\tif ctx == nil {\n\t\tctx = context.Background()\n\t}\n\tctx = context.WithValue(ctx, wrappedClientCtxKey, wc)\n\n\tvar cc concurrentCuckoo\n\tif procOpt.Prune {\n\t\tcc.Filter = cuckoo.NewFilter(10000000) // 10mil = ~16 MB on 64-bit\n\t\tcc.Mutex = new(sync.Mutex)\n\t}\n\n\tcheckpoint := wc.prepareCheckpoint(procOpt.Timeframe)\n\n\twg, ch := wc.beginProcessing(cc, procOpt)\n\n\terr := wc.Client.ListItems(ctx, ch, ListingOptions{\n\t\tCheckpoint: checkpoint,\n\t\tTimeframe:  procOpt.Timeframe,\n\t\tVerbose:    procOpt.Verbose,\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"getting items from service: %v\", err)\n\t}\n\n\t// wait for processing to complete\n\twg.Wait()\n\n\terr = wc.successCleanup()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"processing completed, but error cleaning up: %v\", err)\n\t}\n\n\t// commence prune, if requested\n\tif procOpt.Prune {\n\t\terr := wc.doPrune(cc)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"processing completed, but error pruning: %v\", err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// prepareCheckpoint sets the current command parameters on wc for\n// checkpoints to be saved later on, and then returns the last\n// checkpoint data only if its parameters match the new/current ones.\n// This prevents trying to resume a process with different parameters\n// which can cause errors.\nfunc (wc *WrappedClient) prepareCheckpoint(tf Timeframe) []byte {\n\twc.commandParams = tf.String()\n\tif wc.acc.cp == nil || wc.acc.cp.Params != wc.commandParams {\n\t\treturn nil\n\t}\n\treturn wc.acc.cp.Data\n}\n\nfunc (wc *WrappedClient) successCleanup() error {\n\t// clear checkpoint\n\t_, err := wc.tl.db.Exec(`UPDATE accounts SET checkpoint=NULL WHERE id=?`, wc.acc.ID) // TODO: limit 1 (see https://github.com/mattn/go-sqlite3/pull/802)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"clearing checkpoint: %v\", err)\n\t}\n\twc.acc.checkpoint = nil\n\n\t// update the last item ID, to advance the window for future get-latest operations\n\twc.lastItemMu.Lock()\n\tlastItemID := wc.lastItemRowID\n\twc.lastItemMu.Unlock()\n\tif lastItemID > 0 {\n\t\t_, err = wc.tl.db.Exec(`UPDATE accounts SET last_item_id=? WHERE id=?`, lastItemID, wc.acc.ID) // TODO: limit 1\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"advancing most recent item ID: %v\", err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Import is like GetAll but for a locally-stored archive or export file that can\n// simply be opened and processed, rather than needing to run over a network. See\n// the godoc for GetAll. This is only for data sources that support Import.\nfunc (wc *WrappedClient) Import(ctx context.Context, filename string, procOpt ProcessingOptions) error {\n\tif wc.Client == nil {\n\t\treturn fmt.Errorf(\"no client\")\n\t}\n\n\tvar cc concurrentCuckoo\n\tif procOpt.Prune {\n\t\tcc.Filter = cuckoo.NewFilter(10000000) // 10mil = ~16 MB on 64-bit\n\t\tcc.Mutex = new(sync.Mutex)\n\t}\n\n\twg, ch := wc.beginProcessing(cc, procOpt)\n\n\terr := wc.Client.ListItems(ctx, ch, ListingOptions{\n\t\tFilename:   filename,\n\t\tCheckpoint: wc.acc.checkpoint,\n\t\tTimeframe:  procOpt.Timeframe,\n\t\tVerbose:    procOpt.Verbose,\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"importing: %v\", err)\n\t}\n\n\t// wait for processing to complete\n\twg.Wait()\n\n\terr = wc.successCleanup()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"processing completed, but error cleaning up: %v\", err)\n\t}\n\n\t// commence prune, if requested\n\tif procOpt.Prune {\n\t\terr := wc.doPrune(cc)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"processing completed, but error pruning: %v\", err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (wc *WrappedClient) doPrune(cuckoo concurrentCuckoo) error {\n\t// absolutely do not allow a prune to happen if the account\n\t// has a checkpoint; this is because we don't store the cuckoo\n\t// filter with checkpoints, meaning that the list of items\n\t// that have been seen is INCOMPLETE, and pruning on that\n\t// would lead to data loss. TODO: Find a way to store the\n\t// cuckoo filter with a checkpoint...\n\tvar ckpt []byte\n\terr := wc.tl.db.QueryRow(`SELECT checkpoint FROM accounts WHERE id=? LIMIT 1`,\n\t\twc.acc.ID).Scan(&ckpt)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"querying checkpoint: %v\", err)\n\t}\n\tif len(ckpt) > 0 {\n\t\treturn fmt.Errorf(\"checkpoint exists; refusing to prune for fear of incomplete item listing\")\n\t}\n\n\t// deleting items can't happen while iterating the rows\n\t// since the database table locks; i.e. those two operations\n\t// are in conflict, so we can't do the delete until we\n\t// close the result rows; hence, we have to load each\n\t// item to delete into memory (sigh) and then delete after\n\t// the listing is complete\n\titemsToDelete, err := wc.listItemsToDelete(cuckoo)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"listing items to delete: %v\", err)\n\t}\n\n\tfor _, rowID := range itemsToDelete {\n\t\terr := wc.deleteItem(rowID)\n\t\tif err != nil {\n\t\t\tlog.Printf(\"[ERROR][%s/%s] Deleting item: %v (item_id=%d)\",\n\t\t\t\twc.ds.ID, wc.acc.UserID, err, rowID)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (wc *WrappedClient) listItemsToDelete(cuckoo concurrentCuckoo) ([]int64, error) {\n\trows, err := wc.tl.db.Query(`SELECT id, original_id FROM items WHERE account_id=?`, wc.acc.ID)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"selecting all items from account: %v (account_id=%d)\", err, wc.acc.ID)\n\t}\n\tdefer rows.Close()\n\n\tvar itemsToDelete []int64\n\tfor rows.Next() {\n\t\tvar rowID int64\n\t\tvar originalID string\n\t\terr := rows.Scan(&rowID, &originalID)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"scanning item: %v\", err)\n\t\t}\n\t\tif originalID == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\tcuckoo.Lock()\n\t\texistsOnService := cuckoo.Lookup([]byte(originalID))\n\t\tcuckoo.Unlock()\n\t\tif !existsOnService {\n\t\t\titemsToDelete = append(itemsToDelete, rowID)\n\t\t}\n\t}\n\tif err = rows.Err(); err != nil {\n\t\treturn nil, fmt.Errorf(\"iterating item rows: %v\", err)\n\t}\n\n\treturn itemsToDelete, nil\n}\n\nfunc (wc *WrappedClient) deleteItem(rowID int64) error {\n\t// before deleting the row, find out whether this item\n\t// has a data file and is the only one referencing it\n\tvar count int\n\tvar dataFile string\n\terr := wc.tl.db.QueryRow(`SELECT COUNT(*), data_file FROM items\n\t\tWHERE data_file = (SELECT data_file FROM items\n\t\t\t\t\t\t\tWHERE id=? AND data_file IS NOT NULL\n\t\t\t\t\t\t\tAND data_file != \"\" LIMIT 1)`,\n\t\trowID).Scan(&count, &dataFile)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"querying count of rows sharing data file: %v\", err)\n\t}\n\n\t_, err = wc.tl.db.Exec(`DELETE FROM items WHERE id=?`, rowID) // TODO: limit 1 (see https://github.com/mattn/go-sqlite3/pull/802)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"deleting item from DB: %v\", err)\n\t}\n\n\tif count == 1 {\n\t\terr := os.Remove(wc.tl.fullpath(dataFile))\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"deleting associated data file from disk: %v\", err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// DataSourceName returns the name of the data source wc was created from.\nfunc (wc *WrappedClient) DataSourceName() string { return wc.ds.Name }\n\n// DataSourceID returns the ID of the data source wc was created from.\nfunc (wc *WrappedClient) DataSourceID() string { return wc.ds.ID }\n\n// UserID returns the ID of the user associated with this client.\nfunc (wc *WrappedClient) UserID() string { return wc.acc.UserID }\n"
        }
      ]
    }
  ]
}