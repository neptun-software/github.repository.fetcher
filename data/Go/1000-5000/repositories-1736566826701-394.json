{
  "metadata": {
    "timestamp": 1736566826701,
    "page": 394,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "nutsdb/nutsdb",
      "stars": 3435,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.029296875,
          "content": ".idea/\ntestdata/\n\n*/*.DS_Store"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.2666015625,
          "content": "language: go\ngo:\n  - 1.11.x\n  - tip\nbefore_install:\n  - go get golang.org/x/tools/cmd/cover\n  - go get github.com/mattn/goveralls\nscript:\n  - go test -v -covermode=count -coverprofile=coverage.out\n  - $HOME/gopath/bin/goveralls -coverprofile=coverage.out -service=travis-ci"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 22.1630859375,
          "content": "## v0.1.0 ï¼ˆ2019-2-28ï¼‰\n* [New feature] Support Put/Get/Delete Operations\n* [New feature] Support TTL\n* [New feature] Support Range/Prefix Scanning\n* [New feature] Support Merge Operation\n* [New feature] Support BackUp Operation\n* [New feature] Support Bucket\n\n## v0.2.0 ï¼ˆ2019-3-05ï¼‰\n* [New feature] Support list\n* [New feature] Support set\n* [New feature] Support sorted set\n* [Bug Fix] Fix error when batch put operations\n* [Change] Update README && CHANGELOG\n\n## v0.3.0ï¼ˆ2019-3-11ï¼‰\n* [New feature] Support persistence\n* [Bug Fix] Fix when fn is nil\n* [Change] Discard mmap package\n* [Change] Discard EntryIdxMode options: HintAndRAMIdxMode and HintAndMemoryMapIdxMode\n* [Change] Add new EntryIdxMode options: HintKeyValAndRAMIdxMode and HintKeyAndRAMIdxMode\n\n## v0.4.0ï¼ˆ2019-3-15ï¼‰\n* [New feature] Support mmap loading file\n* [Bug Fix] Fix tx bug when a tx commits\n* [Change] Add rwmanager interface\n* [Change] Add new options: RWMode, SyncEnable and StartFileLoadingMode\n* [Change] Clean up some codes\n* [Change] Update README && CHANGELOG\n\n## v0.5.0 (2019-11-28)\n* [New feature] Support EntryIdxMode: HintBPTSparseIdxMode\n* [New feature] Support GetAll() function for all models\n* [Bug Fix] Fix error too many open files in system\n* [Bug Fix] Fix constant 2147483648 overflows int\n* [Bug Fix] Fix when the number of files waiting to be merged not at least 2\n* [Bug Fix] Fix data pollution when executing the merge method\n* [Change] Modify Records type && Entries type\n* [Change] Refactor for tx Commit function\n* [Change] Update Iterating over keys about README\n* [Change] Fix some grammatical mistakes about README\n* [Change] Rename variable for func ReadBPTreeRootIdxAt\n* [Change] Add issue templates\n* [Change] Update README && CHANGELOG\n\n## v0.6.0 (2021-03-21)\n* [New Feature] Add PrefixSearchScan() with regexp search abilityï¼ˆ#53ï¼‰\n* [New Feature] Allow put with timestamp (#88 )\n* [Bug Fix] Fix ZMembers bug (#58 )\n* [Bug Fix] Repeated key merge fix (#83 )\n* [Bug Fix] The LRem implementation is not consistent with the description (#92 )\n* [Refactor] Improve buildBPTreeRootIdxes file reading (#67)\n* [Docs] Update README && CHANGELOG\n\n## v0.7.0 (2022-03-06)\n* [New Feature] support im memory db (#109)\n* [New Feature] Add backup with tar+gz (#111)\n* [New Feature] Add IterateBuckets() and DeleteBucket()\n* [Refactor] refactor error (#112)\n* [Bug Fix] Windows The process cannot access the file because it is being used by another process. (#110)\n* [Docs] Update README && CHANGELOG\n\n## v0.7.1 (2022-03-06)\n* [Bug Fix] Delete buckets without persistence.(#115)\n\n## v0.8.0 (2022-04-01)\n* [Perf] optimize tx commit for batch write (#132)\n* [Bug Fix] fix: open file by variable (#118)\n* [Bug Fix] fix close file before error checkï¼ˆ#122ï¼‰\n* [Bug Fix] fix rwManager.Close orderï¼ˆ#133ï¼‰\n* [Bug Fix] fix last entry status error ï¼ˆ#134ï¼‰\n* [Bug Fix] fix: read ErrIndexOutOfBound err\n* [CI] add unit-test action ï¼ˆ#120ï¼‰\n* [Chore] add constant ErrKeyNotFound and ErrKeyNotExist (#125)\n* [Chore] chore: remove unnecessary conversion  (#126)\n* [Chore] chore(ds/list): replace for-loop with append  (#127)\n* [Chore] add push check for lint, typo  (#131)\n* [Style] style: fix typo and ineffectual assignment  (#130)\n\n## v0.9.0 (2022-06-17)\n* [Bug Fix] close file before error check &remove redundant judgments ï¼ˆ#137ï¼‰ @xujiajun\n* [Bug Fix] update golang.org/x/sys to support go1.18 build ï¼ˆ#139ï¼‰@ag9920\n* [Bug Fix] when use merge, error: The process cannot access the file because it is being used by another process (#166) @xujiajun\n* [Bug Fix] fix code example. (#143) @gphper\n* [Bug Fix] merge error after delete bucket (#153) @xujiajun\n* [Perf] add fd cache(#164) @elliotchenzichang\n* [Perf] optimize sadd function inserting duplicate data leads to datafile growth (#146) @gphper\n* [Refactor] rewrite managed to support panic rollback ï¼ˆ#136ï¼‰@ag9920\n* [Refactor] errors: optimize error management (#163) @xpzouying\n* [Test] Update testcase: use testify test tools (#138) @xpzouying\n* [Test] change list and set test with table driven test and testify (#145ï¼‰ @bigdaronlee163\n* [Test] refactor db_test for string use testify (#147) @Rand01ph\n* [Test] add [bucket_meat/entry] unit test (#148) @gphper\n* [Test] update bptree unittest (#149) @xpzouying\n* [Test] Update tx bptree testcase (#155) @xpzouying\n* [Test] complete zset tests with testify (#151) @bigdaronlee163\n* [Test] optimization tx_bucket_test and bucket_meta_test  (#156) @gphper\n* [Test] test:complete tx_zset tests with testify (#162) @bigdaronlee163\n* [Chore] remove unused member (#157) @xpzouying\n* [Style]  format code comments etc. (#140) @moyrne\n\n## v0.10.0ï¼ˆ2022-08-13ï¼‰\n* [Bug Fix]List data structure with count parameter negative, lack of boundary judgment (#183) @andrewhzy\n* [New Feature] add LRemByIndex (#174) @Nailcui\n* [New Feature] add LKeys SKeys ZKeys API (#175) @Nailcui\n* [New Feature] add Iterator API (HintKeyAndRAMIdxMode and HintKeyValAndRAMIdxMode)(#191) @zeina1i\n* [Refactor] graceful options parameters (#185) @Nailcui\n* [Test] Add rwmanager fileio test (#170) @zeina1i\n* [Test] Improve code coverage about list  (#183) @andrewhzy\n* [Test] Test coverage improvement for inmemory  (#187) @andrewhzy\n* [Docs] A few corrections in ReadME file (#171) @kwakubiney\n\n## v0.11.0ï¼ˆ2022-10-31ï¼‰\n* [Bug Fix] In BPTSparse when combination of bucket and key is repeated (#207) @ShiMaRing\n* [Bug Fix] MInInt function compatible with 32-bit operating systems (#208) @xujiajun\n* [Bug Fix] Index EOF issue#213 (#214) @xujiajun\n* [Perf] Optimize concurrent read performance (#205) @elliotchenzichang\n* [Perf] Use biobuf optimaze startspeed (#212) @elliotchenzichang\n* [New Feature] Support reverse iterator (EntryIdxMode: HintKeyAndRAMIdxMode and HintKeyValAndRAMIdxMode) (#202) @zeina1i\n* [New Feature] Add support for IterateBuckets regularized matching (#198) @Nailcui\n* [New Feature] list all key of bucket in memory mode (#206) @Nailcui\n* [New Feature] Add PrefixScan in memory mode  (#211) @Nailcui\n* [Refactor] make default options to be created in a factory method (#196) @elliotchenzichang\n* [Refactor] use size constant value (#204) @elliotchenzichang\n* [Chore] add iterator example (#209) @xujiajun\n* [Chore] remove option StartFileLoadingMode (#218) @xujiajun\n\n## v0.11.1ï¼ˆ2022-11-13ï¼‰\n* [Bug Fix] avoid nil of it.current (#233) @mindon\n* [Bug Fix] it.current may be nil when options.Reverse is false (#234) @xujiajun\n* [Refactor] changing the lock to be one of property of the structure can make the code more readable.(#228) @elliotchenzichang\n* [New Feature] add buffer size of recovery reader as param (#230) @elliotchenzichang\n\n## v0.12.0ï¼ˆ2023-02-26ï¼‰\n* [New Feature] feat:support ttl function for list (feat:support ttl function for list #263) @xuyukeviki\n* [Bug Fix] fix: panic: db.buildIndexes error: unexpected EOF issue (panic: db.buildIndexes error: unexpected EOF #244) @xujiajun\n* [Bug Fix] issue NewIterator with Reverse=true stop at 28 records #250 :: Andrew :: bug fixed (issue #250 :: Andrew :: bug fixed #266) @andrewhzy\n* [Bug Fix] Fix issues LIST \"start or end error\" after deleting the last item from list  #280: LIST \"start or end error\" after deleting the last item Fix issues #280: LIST \"start or end error\" after deleting the last item #282 @ShawnHXH\n* [Performance] Use file recovery in merge (Use file recovery in merge #259) @elliotchenzichang\n* [Performance] perf(read): reduce one read I/O perf(read): reduce one read I/O #271 @lugosix\n* [Refactor] add options function of BufferSizeOfRecovery (add options function of BufferSizeOfRecovery #236) @elliotchenzichang\n* [Refactor] add fd release logic to file recovery reader (add fd release logic to file recovery reader #242 ) @elliotchenzichang\n* [Refactor] rebuild parse meta func (rebuild parse meta func #243 ) @elliotchenzichang\n* [Refactor] change the xujiajun/nutsdb -> nutsdb/nutsdb change the xujiajun/nutsdb -> nutsdb/nutsdb #281 @elliotchenzichang\n* [Refactor] Update doc Update doc #285 @elliotchenzichang\n* [Refactor] Fix verify logic Fix verify logic #286 @elliotchenzichang\n* [Refactor] fix: issue (Error description problem about IsPrefixSearchScan #288) fix: issue (#288) #289 @CodePrometheus\n* [Refactor] docs: fix typo( docs: fix typo #252) @icpd\n* [Refactor] fix a typo in code fix a typo #291 @elliotchenzichang\n* [UnitTest] Adding a test of readEntry() and refacting readEntry() (Adding a test of readEntry() and refacting readEntry() #237 ) @wangxuanni\n* [UnitTest] test coverage improvement (fix: issue (#213) #214 ) @andrewhzy\n* [UnitTest] adding a test for IsKeyEmpty func in github.com/xujiajun/nutsdb/errors.go:26 (adding a test for IsKeyEmpty func in github.com/xujiajun/nutsdb/errors.go:26: #265) @lyr-2000\n* [UnitTest] Add test for SetKeyPosMap in nutsdb/bptree.go (Add test for SetKeyPosMap in nutsdb/bptree.go #268) @ShawnHXH\n* [UnitTest] Add test for ToBinary in bptree.go Add test for ToBinary in bptree.go #272 @ShawnHXH\n* [UnitTest] Add test for bucket in errors.go Add test for bucket in errors.go #278 @ShawnHXH\n* [UnitTest] add test for rwmanger_mmap.Release add test for rwmanger_mmap.Release #283 @vegetabledogdog\n* [UnitTest] test: add tests for IsDBClosed, IsPrefixScan and IsPrefixSearchScan test: add tests for IsDBClosed, IsPrefixScan and IsPrefixSearchScan #290 @CodePrometheus\n\n## v0.12.1ï¼ˆ2023-05-19ï¼‰\n* [Bug Fix] fix delete non exist will not raise error bug by @elliotchenzichang in #331\n* [Bug Fix] issue #306 - added a MAX_SIZE const that fits 32 and 64bit arch by @hivenet-philippe in #308\n* [New Feature] add GetListTTL by @wangxuanni in #316\n* [Refactor] delete a repeat error validation logic by @elliotchenzichang in #324\n* [Refactor] make bucket value as a property in entry by @elliotchenzichang in #323\n* [Refactor] move crc property into meta struct by @elliotchenzichang in #325\n* [Refactor] delete the position property of entry struct by @elliotchenzichang in #326\n* [Refactor] Refactor-actor method isFilterEntry by @elliotchenzichang in #327\n* [Refactor] add function return the status of DB by @elliotchenzichang in #329\n* [Refactor] add status management by @elliotchenzichang in #330\n* [Refactor] rebuild the status management code by @elliotchenzichang in #332\n* [Refactor] delete the param of writelen by @elliotchenzichang in #333\n* [Refactor] Refactor entry length check by @elliotchenzichang in #334\n* [Test] Add test for PutWithTimestamp in tx.go by @rayz in #307\n* [Docs] docs(readme): format code by @rfyiamcool in #319\n\n## v0.12.2ï¼ˆ2023-05-21ï¼‰\n* [Bug Fix] fix ignore bucket when db recovering by @elliotchenzichang in #336\n\n## v0.12.3ï¼ˆ2023-06-23ï¼‰\n* [Bug Fix] fix the bucket issue by @elliotchenzichang in #337\n* [Bug Fix] fix: err desc for ErrWhenBuildListIdx by @xujiajun in #338\n* [Bug Fix] fix: r.E.Bucket err by @xujiajun in #341\n\n## v0.12.4ï¼ˆ2023-07-25ï¼‰\n* [Bug Fix] fix: remove unnecessary null checks when writing to a list. by @bigboss2063 in #353\n* [Bug Fix] fix the bug of nil entry by @elliotchenzichang in #377\n* [Bug Fix] bug fix, add defer function to release lock to avoid deadlock by @elliotchenzichang in #356\n* [Bug Fix] Fix the bug of nil entry by @elliotchenzichang in #380\n* [Bug Fix] fix: deadlock caused by error by @lyl156 in #371\n* [New Feature] feature: make all objects set to nil after the user calls the close fâ€¦ by @tobehardest in #374\n* [New Feature] feat: implement file lock by @bigboss2063 in #372\n* [Refactor] just move the index structure in a rightful space by @elliotchenzichang in #343\n* [Refactor] fix some issue in go mod config by @elliotchenzichang in #345\n* [Refactor] rebuild the index module for index struct by @elliotchenzichang in #346\n* [Refactor] rebuild the add function in list index by @elliotchenzichang in #350\n* [Refactor] use bytes.Equal instead bytes.Compare by @testwill in #355\n* [Refactor] rebuild the way to create hint object by @elliotchenzichang in #357\n* [Refactor] rebuild isFilter function by @elliotchenzichang in #358\n* [Refactor] rebuild the way to new Entry Object by @elliotchenzichang in #359\n* [Refactor] add comments in entry file by @elliotchenzichang in #362\n* [Refactor] rebuilt the way to new Record object by @elliotchenzichang in #365\n* [Refactor] rebuild part of recovery logic by @elliotchenzichang in #366\n* [Refactor] refactor: use a const to replace magic string by @bigboss2063 in #376\n* [Test] Add test WithNodeNum in nutsdb/options.go by @dongzhiwei-git in #361\n* [Test] test: test WithRWMode by @JingchenZhang in #368\n* [Test] test: test rw manager mmap by @lyl156 in #363\n* [Test] add test case for ErrWhenBuildListIdx func in db.go and optimize enqueue func in bptree.go and add test case by @damotiansheng in #370\n* [Test] test with ReadNode ,WithCleanFdsCacheThreshold,WithMaxFdNumsInCache,WithSyncEnable by @JingchenZhang in #369\n* [Test] test: rebuild unit tests in db_test.go(issue#374, task 4) by @bigboss2063 in #375\n* [Chore] chore: remove element in cache on closing fd manager by @lyl156 in #364\n\n## v0.12.6ï¼ˆ2023-07-26ï¼‰\n* [Refactor] refactor: refactoring the initialization way of the MetaData by @bigboss2063 in #381\n* [Refactor] Small scope refactoring code:move some util func to utils.go from db.go by @damotiansheng in #379\n\n## v0.13.0ï¼ˆ2023-08-01ï¼‰\n* [Bug Fix] fix: fix the bug that allows deletion of a non-existent bucket. by @bigboss2063 in #388\n* [Perf] pref: remove sync.Pool and prev allocate buffer for small tx by @bigboss2063 in #384\n* [New Feature] feat: implement automatic merging by @bigboss2063 in #393\n* [New Feature] feat: add custom comparator by @lyl156 in #389\n* [New Feature] feat: refactor list data structure by using doubly linked list and support HintKeyAndRAMIdxMode by @bigboss2063 in #390\n* [Docs] doc: update options doc by @lyl156 in #391\n* [Docs] doc: add a new option and update the default option by @bigboss2063 in #396\n* [Chore] chore: add error handler for error occurred during transaction by @lyl156 in #383\n## v0.14.0 (2023-9-1)\n### Features\n- feat: make Set support HintKeyAndRAMIdxMode by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/397\n- add batch write by @damotiansheng in https://github.com/nutsdb/nutsdb/pull/398\n- fix: fixed the issue where deletion did not actually remove the record from memory. by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/407\n- feat: make sorted set to support HintKeyAndRAMIdxMode by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/426\n- pref: refactor index item to save memory usage by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/434\n- feat: implemented configurable automatic expiration deletion, optional time heap or time wheel by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/435\n### BugFix\n- repair tx.SMembers bug by @damotiansheng in https://github.com/nutsdb/nutsdb/pull/404\n- fix: fix dead lock during merging by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/417\n- [bugFix]Add tx data handle logic in recovery by @elliotchenzichang in https://github.com/nutsdb/nutsdb/pull/418\n- [bug fix]add skip entry zero error when recovery by @elliotchenzichang in https://github.com/nutsdb/nutsdb/pull/420\n- fix: fix the problem that an error occurs when switching between the first and the second mode at startup by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/423\n- fix: recover panic when executing tx by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/429\n- fix: fix the problem of allowing delete the non-exist member of a set by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/432\n- fix: fix the bugs of ttl effective time and IsExpired method  by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/437\n- [bugFix]fix the bug in prase data in an tx by @elliotchenzichang in https://github.com/nutsdb/nutsdb/pull/441\n- fix: fix bug in `txZCard` by @G-XD in https://github.com/nutsdb/nutsdb/pull/444\n- [bugFix]fix the bug in data in tx logic by @elliotchenzichang in https://github.com/nutsdb/nutsdb/pull/453\n- fix: merge not allowed when using list type (temporary fix) by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/457\n### Other\n- create dir if not exist by @moyrne in https://github.com/nutsdb/nutsdb/pull/399\n- optimize parseDataFiles error check by @moyrne in https://github.com/nutsdb/nutsdb/pull/401\n- [Test] Restart the database using three modes by @RuiHuaLiu2023 in https://github.com/nutsdb/nutsdb/pull/406\n- chore: update ci.yml by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/409\n- style: remove useless code by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/411\n- chore: add change log by @xujiajun in https://github.com/nutsdb/nutsdb/pull/412\n- ref: use goto refactor recovery logic by @elliotchenzichang in https://github.com/nutsdb/nutsdb/pull/414\n- [ref]delete the map db.committedTxIds, it will not needed any more by @elliotchenzichang in https://github.com/nutsdb/nutsdb/pull/421\n- [ref]rebuild the recovery logic, and delete the unconfirmedRecordList by @elliotchenzichang in https://github.com/nutsdb/nutsdb/pull/422\n- doc: update README.md and add comments for tx_zset.go by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/427\n- delete getActiveFileWriteOff func by @damotiansheng in https://github.com/nutsdb/nutsdb/pull/428\n- [ref] rebuild the error handling logic in recovery by @elliotchenzichang in https://github.com/nutsdb/nutsdb/pull/430\n- refactor: unified delete bucket in buildNotDSIdxes by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/433\n- doc: update README.md and README-CN.md by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/436\n- refactor: use chained calls instead of direct initialization by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/439\n- [ref] move the errors in db file to another separated file. by @elliotchenzichang in https://github.com/nutsdb/nutsdb/pull/440\n- [refactor] Refactoring commit  by @RuiHuaLiu2023 in https://github.com/nutsdb/nutsdb/pull/438\n- test: refactor `tx_set_test` by @G-XD in https://github.com/nutsdb/nutsdb/pull/445\n- test: refactor `tx_bucket_test` by @G-XD in https://github.com/nutsdb/nutsdb/pull/446\n- [ref]move the tx error to separated file by @elliotchenzichang in https://github.com/nutsdb/nutsdb/pull/447\n- refactor: remove func ErrWhenBuildListIdx by @TremblingV5 in https://github.com/nutsdb/nutsdb/pull/443\n\n## v0.14.1 (2023-9-16)\n\n### Features\n\n- add max write records control by [@damotiansheng](https://github.com/damotiansheng) in [#459](https://github.com/nutsdb/nutsdb/pull/459)\n- feat(doc): organize README.md by [@hanxuanliang](https://github.com/hanxuanliang) in [#461](https://github.com/nutsdb/nutsdb/pull/461)\n\n### BugFix\n\n- fix: deadlock when test failed by [@G-XD](https://github.com/G-XD) in [#464](https://github.com/nutsdb/nutsdb/pull/464)\n- fix: fix the bug of ttl when restart by [@bigboss2063](https://github.com/bigboss2063) in [#466](https://github.com/nutsdb/nutsdb/pull/466)\n\n### Other\n\n- refactor: Remove the 3rd mode by [@bigboss2063](https://github.com/bigboss2063) in [#456](https://github.com/nutsdb/nutsdb/pull/456)\n- style: reduce duplicate code by [@bigboss2063](https://github.com/bigboss2063) in [#458](https://github.com/nutsdb/nutsdb/pull/458)\n- refactor: refactor index by generics by [@G-XD](https://github.com/G-XD) in [#462](https://github.com/nutsdb/nutsdb/pull/462)\n\n## v0.14.2 (2023-11-06)\n\n### Features\n\n- finish auto merge by [@damotiansheng](https://github.com/damotiansheng) in [#471](https://github.com/nutsdb/nutsdb/pull/471)\n\n### BugFix\n\n- fix: return ErrListNotFound when calling tx.LRemByIndex with non-existent bucket name by [@TremblingV5](https://github.com/TremblingV5) in [#470](https://github.com/nutsdb/nutsdb/pull/470)\n\n### Other\n\n- list index refractor by [@damotiansheng](https://github.com/damotiansheng) in [#467](https://github.com/nutsdb/nutsdb/pull/467)\n- refactor: refactor unit test of tx_list by [@TremblingV5](https://github.com/TremblingV5) in [#468](https://github.com/nutsdb/nutsdb/pull/468)\n- [ref]add a function that can automatically get the disk size of meta header by [@elliotchenzichang](https://github.com/elliotchenzichang) in [#478](https://github.com/nutsdb/nutsdb/pull/478)\n- refactor: Optimize the code structure so that DB and Tx use a set of indexing processes by [@bigboss2063](https://github.com/bigboss2063) in [#479](https://github.com/nutsdb/nutsdb/pull/479)\n- style: rename or remove some code by [@bigboss2063](https://github.com/bigboss2063) in [#480](https://github.com/nutsdb/nutsdb/pull/480)\n- [ref]add a common function to get the special size buffer object by [@elliotchenzichang](https://github.com/elliotchenzichang) in [#482](https://github.com/nutsdb/nutsdb/pull/482)\n- ref: refactor `buildBTreeIdx` method by [@bigboss2063](https://github.com/bigboss2063) in [#483](https://github.com/nutsdb/nutsdb/pull/483)\n- ref: use errors.Is to remove useless code by [@bigboss2063](https://github.com/bigboss2063) in [#485](https://github.com/nutsdb/nutsdb/pull/485)\n- decrease batch write unit test time by [@damotiansheng](https://github.com/damotiansheng) in [#487](https://github.com/nutsdb/nutsdb/pull/487)\n\n## v0.14.3 (2023-11-21)\n\n### BugFix\n\n- fix: use constants instead of iota to prevent forward compatibility. by [@bigboss2063](https://github.com/bigboss2063) in [#490](https://github.com/nutsdb/nutsdb/pull/490)\n\n## v1.0.0 (2023-11-27)\n\n### Features\n\n- [feat]Serperate bucket from entry and build the bucket management system by [@elliotchenzichang](https://github.com/elliotchenzichang) in [#484](https://github.com/nutsdb/nutsdb/pull/484)\n- add lru cache for second index mode of HintKeyAndRAMIdxMode by [@damotiansheng](https://github.com/damotiansheng) in [#495](https://github.com/nutsdb/nutsdb/pull/495)\n- add more api for bucket by [@elliotchenzichang](https://github.com/elliotchenzichang) in [#502](https://github.com/nutsdb/nutsdb/pull/502)\n- feat: use variable length storage to implement storage protocols to save disk space by [@bigboss2063](https://github.com/bigboss2063) in [#501](https://github.com/nutsdb/nutsdb/pull/501)\n- pref: reduce unnecessary fields in Record and save memory by [@bigboss2063](https://github.com/bigboss2063) in [#506](https://github.com/nutsdb/nutsdb/pull/506)\n\n### BugFix\n\n- Only overwrite the managed error when a rollback error occurs by [@xy3](https://github.com/xy3) in [#493](https://github.com/nutsdb/nutsdb/pull/493)\n- add lru cache when commit only for HintKeyAndRAMIdxMode by [@damotiansheng](https://github.com/damotiansheng) in [#504](https://github.com/nutsdb/nutsdb/pull/504)\n- fix: use sync.Map to avoid data race by [@bigboss2063](https://github.com/bigboss2063) in [#509](https://github.com/nutsdb/nutsdb/pull/509)\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.267578125,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at me@xujiajun.cn. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 2.0478515625,
          "content": "# Contributing to NutsDB\n\n:+1::tada: First off, thanks for taking the time to contribute! :tada::+1:\n\nWe love your input! We want to make contributing to this project as easy and transparent as possible, whether it's:\n\n- Reporting a bug\n- Discussing the current state of the code\n- Submitting a fix\n- Proposing new features\n- Contributing Documentation\n- Contributing a Performance Improvement\n\nBy contributing to NutsDB, you agree to abide by the [code of conduct](https://github.com/xujiajun/nutsdb/blob/master/CODE_OF_CONDUCT.md).\n \n## Getting Started\nIf you are looking to contribute to the NutsDB, the best place to start is the [GitHub \"issues\"](https://github.com/xujiajun/nutsdb/issues) tab. This is also a great place for filing bug reports and making suggestions for ways in which we can improve the code and documentation.\n\n## Reporting bugs\n\nSee [reporting bugs](https://github.com/xujiajun/nutsdb/blob/master/.github/ISSUE_TEMPLATE/bug_report.md) for details about reporting any issues.\n\n## Feature request\n\nsee [feature request](https://github.com/xujiajun/nutsdb/blob/master/.github/ISSUE_TEMPLATE/feature_request.md) for details about reporting any feature requests.\n\n## Contribution flow\n\nPull requests are the best way to propose changes to the codebase (we use [Github Flow](https://guides.github.com/introduction/flow/index.html)). We actively welcome your pull requests:\n\n1. Fork the repo and create your branch from `master`.\n2. If you've added code that should be tested, add tests.\n3. If you've changed APIs, update the documentation.\n4. Ensure the test suite passes.\n5. Make sure your code lints.\n6. Issue that pull request!\n\n## Use a Consistent Coding Style\n\nThe coding style suggested by the Golang community is used in NutsDB. See the style [doc](https://github.com/golang/go/wiki/CodeReviewComments) for details.\n\nPlease follow this style to make NutsDB easy to review, maintain and develop.\n\n## License\nBy contributing, you agree that your contributions will be licensed under its [Apache License 2.0](https://github.com/xujiajun/nutsdb/blob/master/LICENSE) License.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README-CN.md",
          "type": "blob",
          "size": 75.6376953125,
          "content": "<p align=\"center\">\r\n    <img src=\"https://user-images.githubusercontent.com/6065007/141310364-62d7eebb-2cbb-4949-80ed-5cd20f705405.png\">\r\n</p>\r\n\r\n# NutsDB [![GoDoc](https://godoc.org/github.com/nutsdb/nutsdb?status.svg)](https://godoc.org/github.com/nutsdb/nutsdb)  [![Go Report Card](https://goreportcard.com/badge/github.com/nutsdb/nutsdb)](https://goreportcard.com/report/github.com/nutsdb/nutsdb)  [![Go](https://github.com/nutsdb/nutsdb/workflows/Go/badge.svg?branch=master)](https://github.com/nutsdb/nutsdb/actions) [![codecov](https://codecov.io/gh/nutsdb/nutsdb/branch/master/graph/badge.svg?token=CupujOXpbe)](https://codecov.io/gh/nutsdb/nutsdb) [![License](http://img.shields.io/badge/license-Apache_2-blue.svg?style=flat-square)](https://raw.githubusercontent.com/nutsdb/nutsdb/master/LICENSE) [![Mentioned in Awesome Go](https://awesome.re/mentioned-badge.svg)](https://github.com/avelino/awesome-go#database)  \r\n\r\n[English](https://github.com/nutsdb/nutsdb/blob/master/README.md) | ç®€ä½“ä¸­æ–‡\r\n\r\nNutsDB æ˜¯ä¸€ä¸ªç”¨çº¯ Go ç¼–å†™çš„ç®€å•ã€å¿«é€Ÿã€å¯åµŒå…¥ä¸”æŒä¹…çš„é”®/å€¼å­˜å‚¨ã€‚\r\n\r\nå®ƒæ”¯æŒå®Œå…¨å¯åºåˆ—åŒ–çš„äº‹åŠ¡ä»¥åŠ Listã€Setã€SortedSet ç­‰å¤šç§æ•°æ®ç»“æ„ã€‚ æ‰€æœ‰æ“ä½œéƒ½å‘ç”Ÿåœ¨ Tx å†…éƒ¨ã€‚ Tx ä»£è¡¨ä¸€ä¸ªäº‹åŠ¡ï¼Œå¯ä»¥æ˜¯åªè¯»çš„ï¼Œä¹Ÿå¯ä»¥æ˜¯è¯»å†™çš„ã€‚ åªè¯»äº‹åŠ¡å¯ä»¥è¯»å–ç»™å®šå­˜å‚¨æ¡¶å’Œç»™å®šé”®çš„å€¼æˆ–è¿­ä»£ä¸€ç»„é”®å€¼å¯¹ã€‚ è¯»å†™äº‹åŠ¡å¯ä»¥ä»æ•°æ®åº“ä¸­è¯»å–ã€æ›´æ–°å’Œåˆ é™¤é”®ã€‚\r\n\r\næˆ‘ä»¬å¯ä»¥åœ¨NutsDBçš„æ–‡æ¡£ç½‘ç«™äº†è§£æ›´å¤šï¼š[NutsDB Documents](https://nutsdb.github.io/nutsdb-docs/)\r\n\r\næ¬¢è¿å¯¹NutsDBæ„Ÿå…´è¶£çš„åŠ ç¾¤ã€ä¸€èµ·å¼€å‘ï¼Œå…·ä½“çœ‹è¿™ä¸ªissueï¼šhttps://github.com/nutsdb/nutsdb/issues/116ã€‚\r\n\r\n### å…³æ³¨nutsdbå…¬ä¼—å·\r\n <img src=\"https://user-images.githubusercontent.com/6065007/221391600-4f53e966-c376-426e-9dec-27364a0704d1.png\"   height = \"300\" alt=\"nutsdbå…¬ä¼—å·\" align=center />\r\n\r\n\r\n### å…¬å‘Š\r\n* v1.0.0 å‘å¸ƒï¼Œè¯¦æƒ…è§ï¼š [https://github.com/nutsdb/nutsdb/releases/tag/v1.0.0](https://github.com/nutsdb/nutsdb/releases/tag/v1.0.0)\r\n* v0.14.3 å‘å¸ƒï¼Œè¯¦æƒ…è§ï¼š [https://github.com/nutsdb/nutsdb/releases/tag/v0.14.3](https://github.com/nutsdb/nutsdb/releases/tag/v0.14.3)\r\n* v0.14.2 å‘å¸ƒï¼Œè¯¦æƒ…è§ï¼š[https://github.com/nutsdb/nutsdb/releases/tag/v0.14.2](https://github.com/nutsdb/nutsdb/releases/tag/v0.14.2)\r\n* v0.14.1 å‘å¸ƒï¼Œè¯¦æƒ…è§ï¼š[https://github.com/nutsdb/nutsdb/releases/tag/v0.14.1](https://github.com/nutsdb/nutsdb/releases/tag/v0.14.1)\r\n\r\nğŸ“¢ æ³¨æ„ï¼šä»v0.9.0å¼€å§‹ï¼Œ**DefaultOptions** é‡Œé¢çš„ **defaultSegmentSize** åšäº†è°ƒæ•´ä»åŸæ¥çš„ **8MB** å˜æˆäº† **256MB**ï¼Œå¦‚æœä½ åŸæ¥è®¾ç½® 256MB ä¸ç”¨æ”¹ï¼Œå¦‚æœåŸæ¥ä½¿ç”¨çš„æ˜¯é»˜è®¤å€¼çš„ï¼Œéœ€è¦æ‰‹åŠ¨æ”¹æˆ 8MBï¼Œä¸ç„¶åŸæ¥çš„æ•°æ®ä¸ä¼šè§£æã€‚è¿™è¾¹çš„å¤§å°è°ƒæ•´åŸå› æ˜¯ä» v0.9.0 å¼€å§‹æœ‰å¯¹æ–‡ä»¶æè¿°ç¬¦çš„ç¼“å­˜ï¼ˆè¯¦è§£è§PR https://github.com/nutsdb/nutsdb/issues/164 ï¼‰ï¼Œæ‰€ä»¥éœ€è¦ç”¨æˆ·çœ‹ä¸‹è‡ªå·±çš„æ–‡ä»¶æè¿°ç¬¦æ•°é‡ï¼Œæœ‰ä¸æ¸…æ¥šå¯ä»¥æissueæˆ–è€…ç¾¤é‡Œé—®ã€‚\r\n\r\n**nutsdb v1.0.0 **ä¹‹åï¼Œç”±äºåº•å±‚æ•°æ®å­˜å‚¨åè®®çš„å˜åŒ–ï¼Œ**æ—§ç‰ˆæœ¬çš„æ•°æ®ä¸å…¼å®¹**ã€‚ è¯·åœ¨ä½¿ç”¨æ–°ç‰ˆæœ¬ä¹‹å‰é‡å†™æ•°æ®ã€‚ å¹¶ä¸”å½“å‰çš„ Bucket éœ€è¦æ‰‹åŠ¨åˆ›å»ºã€‚ è¯¦ç»†è¯·å‚è§Bucketä½¿ç”¨[æ–‡æ¡£](./docs/user_guides/use-buckets.md)ã€‚\r\n\r\n### å­¦ä¹ èµ„æ–™\r\n\r\nhttps://www.bilibili.com/video/BV1T34y1x7AS/\r\n\r\n## æ¶æ„è®¾è®¡\r\n![nutsdb-æ¶æ„å›¾](./docs/img/nutsdb-æ¶æ„å›¾.png)\r\n\r\n\r\n\r\n\r\n## ç›®å½•\r\n\r\n- [NutsDB        ](#nutsdb--------)\r\n    - [å…³æ³¨nutsdbå…¬ä¼—å·](#å…³æ³¨nutsdbå…¬ä¼—å·)\r\n    - [å…¬å‘Š](#å…¬å‘Š)\r\n    - [å­¦ä¹ èµ„æ–™](#å­¦ä¹ èµ„æ–™)\r\n  - [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)\r\n  - [ç›®å½•](#ç›®å½•)\r\n  - [å…¥é—¨æŒ‡å—](#å…¥é—¨æŒ‡å—)\r\n    - [å®‰è£…](#å®‰è£…)\r\n    - [å¼€å¯æ•°æ®åº“](#å¼€å¯æ•°æ®åº“)\r\n    - [é€‰é¡¹é…ç½®](#é€‰é¡¹é…ç½®)\r\n      - [é»˜è®¤é€‰é¡¹](#é»˜è®¤é€‰é¡¹)\r\n    - [ä½¿ç”¨äº‹åŠ¡](#ä½¿ç”¨äº‹åŠ¡)\r\n      - [è¯»å†™äº‹åŠ¡](#è¯»å†™äº‹åŠ¡)\r\n      - [åªè¯»äº‹åŠ¡](#åªè¯»äº‹åŠ¡)\r\n      - [æ‰‹åŠ¨ç®¡ç†äº‹åŠ¡](#æ‰‹åŠ¨ç®¡ç†äº‹åŠ¡)\r\n    - [ä½¿ç”¨buckets](#ä½¿ç”¨buckets)\r\n      - [è¿­ä»£buckets](#è¿­ä»£buckets)\r\n      - [åˆ é™¤bucket](#åˆ é™¤bucket)\r\n    - [ä½¿ç”¨é”®å€¼å¯¹](#ä½¿ç”¨é”®å€¼å¯¹)\r\n      - [åŸºæœ¬æ“ä½œ](#åŸºæœ¬æ“ä½œ)\r\n      - [å¯¹å€¼çš„ä½æ“ä½œ](#å¯¹å€¼çš„ä½æ“ä½œ)\r\n      - [å¯¹å€¼çš„è‡ªå¢å’Œè‡ªå‡æ“ä½œ](#å¯¹å€¼çš„è‡ªå¢å’Œè‡ªå‡æ“ä½œ)\r\n      - [å¯¹å€¼çš„è¿ç»­å¤šæ¬¡Setå’ŒGet](#å¯¹å€¼çš„è¿ç»­å¤šæ¬¡setå’Œget)\r\n      - [å¯¹å€¼çš„å¢è¡¥æ“ä½œ](#å¯¹å€¼çš„å¢è¡¥æ“ä½œ)\r\n      - [è·å–å€¼çš„ä¸€éƒ¨åˆ†](#è·å–å€¼çš„ä¸€éƒ¨åˆ†)\r\n    - [ä½¿ç”¨TTL](#ä½¿ç”¨ttl)\r\n    - [å¯¹keysçš„æ‰«ææ“ä½œ](#å¯¹keysçš„æ‰«ææ“ä½œ)\r\n      - [å‰ç¼€æ‰«æ](#å‰ç¼€æ‰«æ)\r\n      - [å‰ç¼€åçš„æ­£åˆ™æ‰«æ](#å‰ç¼€åçš„æ­£åˆ™æ‰«æ)\r\n      - [èŒƒå›´æ‰«æ](#èŒƒå›´æ‰«æ)\r\n    - [è·å–å…¨éƒ¨çš„keyå’Œvalue](#è·å–å…¨éƒ¨çš„keyå’Œvalue)\r\n    - [è¿­ä»£å™¨](#è¿­ä»£å™¨)\r\n      - [æ­£å‘çš„è¿­ä»£å™¨](#æ­£å‘çš„è¿­ä»£å™¨)\r\n      - [åå‘çš„è¿­ä»£å™¨](#åå‘çš„è¿­ä»£å™¨)\r\n    - [åˆå¹¶æ“ä½œ](#åˆå¹¶æ“ä½œ)\r\n    - [æ•°æ®åº“å¤‡ä»½](#æ•°æ®åº“å¤‡ä»½)\r\n    - [ä½¿ç”¨å†…å­˜æ¨¡å¼](#ä½¿ç”¨å†…å­˜æ¨¡å¼)\r\n    - [ä½¿ç”¨å…¶ä»–æ•°æ®ç»“æ„](#ä½¿ç”¨å…¶ä»–æ•°æ®ç»“æ„)\r\n      - [List](#list)\r\n        - [RPush](#rpush)\r\n        - [LPush](#lpush)\r\n        - [LPop](#lpop)\r\n        - [LPeek](#lpeek)\r\n        - [RPop](#rpop)\r\n        - [RPeek](#rpeek)\r\n        - [LRange](#lrange)\r\n        - [LRem](#lrem)\r\n        - [LRemByIndex](#lrembyindex)\r\n        - [LTrim](#ltrim)\r\n        - [LSize](#lsize)\r\n        - [LKeys](#lkeys)\r\n      - [Set](#set)\r\n        - [SAdd](#sadd)\r\n        - [SAreMembers](#saremembers)\r\n        - [SCard](#scard)\r\n        - [SDiffByOneBucket](#sdiffbyonebucket)\r\n        - [SDiffByTwoBuckets](#sdiffbytwobuckets)\r\n        - [SHasKey](#shaskey)\r\n        - [SIsMember](#sismember)\r\n        - [SMembers](#smembers)\r\n        - [SMoveByOneBucket](#smovebyonebucket)\r\n        - [SMoveByTwoBuckets](#smovebytwobuckets)\r\n        - [SPop](#spop)\r\n        - [SRem](#srem)\r\n        - [SUnionByOneBucket](#sunionbyonebucket)\r\n        - [SUnionByTwoBuckets](#sunionbytwobuckets)\r\n        - [SKeys](#skeys)\r\n      - [Sorted Set](#sorted-set)\r\n        - [ZAdd](#zadd)\r\n        - [ZCard](#zcard)\r\n        - [ZCount](#zcount)\r\n        - [ZGetByKey](#zgetbykey)\r\n        - [ZMembers](#zmembers)\r\n        - [ZPeekMax](#zpeekmax)\r\n        - [ZPeekMin](#zpeekmin)\r\n        - [ZPopMax](#zpopmax)\r\n        - [ZPopMin](#zpopmin)\r\n        - [ZRangeByRank](#zrangebyrank)\r\n        - [ZRangeByScore](#zrangebyscore)\r\n        - [ZRank](#zrank)\r\n      - [ZRevRank](#zrevrank)\r\n        - [ZRem](#zrem)\r\n        - [ZRemRangeByRank](#zremrangebyrank)\r\n        - [ZScore](#zscore)\r\n        - [ZKeys](#zkeys)\r\n    - [ä¸å…¶ä»–æ•°æ®åº“çš„æ¯”è¾ƒ](#ä¸å…¶ä»–æ•°æ®åº“çš„æ¯”è¾ƒ)\r\n      - [BoltDB](#boltdb)\r\n      - [LevelDB, RocksDB](#leveldb-rocksdb)\r\n      - [Badger](#badger)\r\n    - [Benchmarks](#benchmarks)\r\n  - [æµ‹è¯•çš„ç¯å¢ƒ:](#æµ‹è¯•çš„ç¯å¢ƒ)\r\n  - [Benchmarkçš„ç»“æœ:](#benchmarkçš„ç»“æœ)\r\n  - [ç»“è®º:](#ç»“è®º)\r\n    - [å†™æ€§èƒ½:](#å†™æ€§èƒ½)\r\n    - [è¯»æ€§èƒ½:](#è¯»æ€§èƒ½)\r\n    - [è­¦å‘Šå’Œé™åˆ¶](#è­¦å‘Šå’Œé™åˆ¶)\r\n      - [éš”ç¦»çº§åˆ«ä½åˆ°é«˜ï¼š](#éš”ç¦»çº§åˆ«ä½åˆ°é«˜)\r\n        - [1ï¼‰æœªæäº¤è¯»ï¼ˆREAD UNCOMMITTEDï¼‰](#1æœªæäº¤è¯»read-uncommitted)\r\n        - [2ï¼‰åœ¨æäº¤è¯»ï¼ˆREAD COMMITTEDï¼‰](#2åœ¨æäº¤è¯»read-committed)\r\n        - [3ï¼‰åœ¨å¯é‡å¤è¯»ï¼ˆREPEATABLE READSï¼‰](#3åœ¨å¯é‡å¤è¯»repeatable-reads)\r\n        - [4ï¼‰å¯ä¸²è¡ŒåŒ– ï¼ˆSerializableï¼‰](#4å¯ä¸²è¡ŒåŒ–-serializable)\r\n    - [è”ç³»ä½œè€…](#è”ç³»ä½œè€…)\r\n    - [å‚ä¸è´¡çŒ®](#å‚ä¸è´¡çŒ®)\r\n      - [ä»£ç é£æ ¼æŒ‡å—å‚è€ƒ](#ä»£ç é£æ ¼æŒ‡å—å‚è€ƒ)\r\n      - [git commit è§„èŒƒå‚è€ƒ](#git-commit-è§„èŒƒå‚è€ƒ)\r\n      - [typeçš„å‚è€ƒ](#typeçš„å‚è€ƒ)\r\n    - [è‡´è°¢](#è‡´è°¢)\r\n    - [License](#license)\r\n\r\n## å…¥é—¨æŒ‡å—\r\n\r\n### å®‰è£…\r\n\r\nNutsDBçš„å®‰è£…å¾ˆç®€å•ï¼Œé¦–å…ˆä¿è¯ [Golang](https://golang.org/dl/) å·²ç»å®‰è£…å¥½ (ç‰ˆæœ¬è¦æ±‚1.11ä»¥ä¸Š). ç„¶ååœ¨ç»ˆç«¯æ‰§è¡Œå‘½ä»¤:\r\n\r\n```\r\ngo get -u github.com/nutsdb/nutsdb\r\n```\r\n\r\n### å¼€å¯æ•°æ®åº“\r\n\r\nè¦æ‰“å¼€æ•°æ®åº“éœ€è¦ä½¿ç”¨` nutsdb.Open()`è¿™ä¸ªæ–¹æ³•ã€‚å…¶ä¸­ç”¨åˆ°çš„é€‰é¡¹(options)åŒ…æ‹¬ `Dir` , `EntryIdxMode`å’Œ `SegmentSize`ï¼Œåœ¨è°ƒç”¨çš„æ—¶å€™è¿™äº›å‚æ•°å¿…é¡»è®¾ç½®ã€‚å®˜æ–¹æä¾›äº†`DefaultOptions`çš„é€‰é¡¹ï¼Œç›´æ¥ä½¿ç”¨`nutsdb.DefaultOptions`å³å¯ã€‚å½“ç„¶ä½ ä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦è‡ªå·±å®šä¹‰ã€‚\r\n\r\nä¾‹å­ï¼š \r\n\r\n```golang\r\npackage main\r\n\r\nimport (\r\n    \"log\"\r\n\r\n    \"github.com/nutsdb/nutsdb\"\r\n)\r\n\r\nfunc main() {\r\n    db, err := nutsdb.Open(\r\n        nutsdb.DefaultOptions,\r\n        nutsdb.WithDir(\"/tmp/nutsdb\"), // æ•°æ®åº“ä¼šè‡ªåŠ¨åˆ›å»ºè¿™ä¸ªç›®å½•æ–‡ä»¶\r\n    )\r\n    if err != nil {\r\n        log.Fatal(err)\r\n    }\r\n    defer db.Close()\r\n\r\n    ...\r\n}\r\n```\r\n\r\n### é€‰é¡¹é…ç½®\r\n\r\n* Dir                  string  \r\n\r\n`Dir` ä»£è¡¨æ•°æ®åº“å­˜æ”¾æ•°æ®çš„ç›®å½•\r\n\r\n* EntryIdxMode         EntryIdxMode \r\n\r\n`EntryIdxMode` ä»£è¡¨ç´¢å¼•entryçš„æ¨¡å¼. \r\n`EntryIdxMode` åŒ…æ‹¬é€‰é¡¹: `HintKeyValAndRAMIdxMode` ã€ `HintKeyAndRAMIdxMode`å’Œ `HintBPTSparseIdxMode`ã€‚\r\n\r\nå…¶ä¸­`HintKeyValAndRAMIdxMode` ä»£è¡¨çº¯å†…å­˜ç´¢å¼•æ¨¡å¼ï¼ˆkeyå’Œvalueéƒ½ä¼šè¢«cacheï¼‰ã€‚\r\n`HintKeyAndRAMIdxMode` ä»£è¡¨å†…å­˜+ç£ç›˜çš„ç´¢å¼•æ¨¡å¼ï¼ˆåªæœ‰keyè¢«cacheï¼‰ã€‚\r\n`HintBPTSparseIdxMode`ï¼ˆv0.4.0ä¹‹åçš„ç‰ˆæœ¬æ”¯æŒï¼‰ æ˜¯ä¸“é—¨èŠ‚çº¦å†…å­˜çš„è®¾è®¡æ–¹æ¡ˆï¼Œå•æœº10äº¿æ¡æ•°æ®ï¼Œåªè¦80å‡ Må†…å­˜ã€‚ä½†æ˜¯è¯»æ€§èƒ½ä¸é«˜ï¼Œéœ€è¦è‡ªå·±åŠ ç¼“å­˜æ¥åŠ é€Ÿã€‚\r\n\r\n* RWMode               RWMode  \r\n\r\n`RWMode` ä»£è¡¨è¯»å†™æ¨¡å¼. `RWMode` åŒ…æ‹¬ä¸¤ç§é€‰é¡¹: `FileIO` and `MMap`.\r\n`FileIO` ç”¨æ ‡å‡†çš„ I/Oè¯»å†™ã€‚ `MMap` ä»£è¡¨ä½¿ç”¨mmapè¿›è¡Œè¯»å†™ã€‚\r\n\r\n* SegmentSize          int64 \r\n\r\n `SegmentSize` ä»£è¡¨æ•°æ®åº“çš„æ•°æ®å•å…ƒï¼Œæ¯ä¸ªæ•°æ®å•å…ƒï¼ˆæ–‡ä»¶ï¼‰ä¸º`SegmentSize`ï¼Œç°åœ¨é»˜è®¤æ˜¯8ã€‚**æ³¨æ„ï¼šä»å¤§äº0.8.0ç‰ˆæœ¬å¼€å§‹ï¼Œé»˜è®¤SegmentSizeå˜æˆ256MB**\r\nMBï¼Œè¿™ä¸ªå¯ä»¥è‡ªå·±é…ç½®ã€‚ä½†æ˜¯ä¸€æ—¦è¢«è®¾ç½®ï¼Œä¸‹æ¬¡å¯åŠ¨æ•°æ®åº“ä¹Ÿè¦ç”¨è¿™ä¸ªé…ç½®ï¼Œä¸ç„¶ä¼šæŠ¥é”™ã€‚è¯¦æƒ…è§ [é™åˆ¶å’Œè­¦å‘Š](https://github.com/nutsdb/nutsdb/blob/master/README-CN.md#%E8%AD%A6%E5%91%8A%E5%92%8C%E9%99%90%E5%88%B6)ã€‚\r\n\r\n* NodeNum              int64\r\n\r\n`NodeNum` ä»£è¡¨èŠ‚ç‚¹çš„å·ç .é»˜è®¤ NodeNumæ˜¯ 1. `NodeNum` å–å€¼èŒƒå›´ [1,1023] ã€‚\r\n\r\n* SyncEnable           bool\r\n\r\n`SyncEnable` ä»£è¡¨è°ƒç”¨äº† Sync() æ–¹æ³•.\r\nå¦‚æœ `SyncEnable` ä¸º falseï¼Œ å†™æ€§èƒ½ä¼šå¾ˆé«˜ï¼Œä½†æ˜¯å¦‚æœé‡åˆ°æ–­ç”µæˆ–è€…ç³»ç»Ÿå¥”æºƒï¼Œä¼šæœ‰æ•°æ®ä¸¢å¤±çš„é£é™©ã€‚\r\nå¦‚æœ  `SyncEnable` ä¸º trueï¼Œå†™æ€§èƒ½ä¼šç›¸æ¯”falseçš„æƒ…å†µæ…¢å¾ˆå¤šï¼Œä½†æ˜¯æ•°æ®æ›´æœ‰ä¿éšœï¼Œæ¯æ¬¡äº‹åŠ¡æäº¤æˆåŠŸéƒ½ä¼šè½ç›˜ã€‚\r\n\r\n* StartFileLoadingMode RWMode\r\n\r\n`StartFileLoadingMode` ä»£è¡¨å¯åŠ¨æ•°æ®åº“çš„è½½å…¥æ–‡ä»¶çš„æ–¹å¼ã€‚å‚æ•°é€‰é¡¹åŒ`RWMode`ã€‚\r\n\r\n* GCWhenClose bool\r\n\r\n`GCWhenClose` è¡¨ç¤ºè°ƒç”¨ ```db.Close()``` æ—¶ä¸»åŠ¨ GCã€‚Nutsdb é¢„è®¾ä¸ä¼šç«‹å³åœ¨ ```db.Close()``` æ—¶è§¦å‘ GC.\r\n\r\n* CommitBufferSize int64\r\n\r\n`CommitBufferSize` è¡¨ç¤ºä¸ºäº‹åŠ¡é¢„åˆ†é…çš„å†…å­˜å¤§å°ã€‚Nutsdb å°†é¢„åˆ†é…å†…å­˜ä»¥å‡å°‘å†…å­˜åˆ†é…çš„æ¬¡æ•°ã€‚\r\n\r\n* ErrorHandler ErrorHandler\r\n\r\n`ErrorHandler` å¤„ç†äº‹åŠ¡æ‰§è¡ŒæœŸé—´å‘ç”Ÿçš„é”™è¯¯ã€‚\r\n\r\n* LessFunc LessFunc\r\n\r\n`LessFunc` è¡¨ç¤ºå¯¹ key è¿›è¡Œæ’åºçš„å‡½æ•°ã€‚Nutsdb é»˜è®¤æŒ‰å­—å…¸åºå¯¹ key è¿›è¡Œæ’åºã€‚\r\n\r\n* MergeInterval time.Duration\r\n\r\n`MergeInterval` è¡¨ç¤ºè‡ªåŠ¨åŒ– Merge çš„é—´éš”ï¼Œ0 è¡¨ç¤ºä¸è§¦å‘è‡ªåŠ¨åŒ– Mergeï¼Œé»˜è®¤é—´éš”ä¸º 2 å°æ—¶ã€‚\r\n\r\n- MaxBatchCount int64\r\n\r\n`MaxBatchCount` è¡¨ç¤ºæ‰¹é‡å†™å…¥çš„æœ€å¤§æ¡æ•°ã€‚\r\n\r\n- MaxBatchSize int64\r\n\r\n`MaxBatchSize` è¡¨ç¤ºæ‰¹é‡å†™å…¥çš„æœ€å¤§å­—èŠ‚æ•°ã€‚\r\n\r\n- ExpiredDeleteType ExpiredDeleteType\r\n\r\n`ExpiredDeleteType ` è¡¨ç¤ºç”¨äºè‡ªåŠ¨è¿‡æœŸåˆ é™¤çš„æ•°æ®ç»“æ„ã€‚TimeWheel æ„å‘³ç€ä½¿ç”¨æ—¶é—´è½®ï¼Œä½ å¯ä»¥åœ¨éœ€è¦é«˜æ€§èƒ½æˆ–è€…å†…å­˜ä¼šå……è¶³çš„æ—¶å€™ä½¿ç”¨ã€‚TimeHeap æ„å‘³ç€ä½¿ç”¨æ—¶é—´è½®ï¼Œä½ å¯ä»¥åœ¨éœ€è¦é«˜ç²¾åº¦åˆ é™¤æˆ–è€…å†…å­˜å°†åƒç´§çš„æ—¶å€™ä½¿ç”¨ã€‚\r\n\r\n\r\n\r\n#### é»˜è®¤é€‰é¡¹\r\n\r\næ¨èä½¿ç”¨é»˜è®¤é€‰é¡¹çš„æ–¹å¼ã€‚å…¼é¡¾äº†æŒä¹…åŒ–+å¿«é€Ÿçš„å¯åŠ¨æ•°æ®åº“ã€‚å½“ç„¶å…·ä½“è¿˜è¦çœ‹ä½ åœºæ™¯çš„è¦æ±‚ã€‚\r\n\r\n> ä»¥ä¸‹é…ç½®æ˜¯æ¯”è¾ƒä¿å®ˆçš„æ–¹å¼ã€‚\r\n> å¦‚æœä½ å¯¹å†™æ€§èƒ½è¦æ±‚æ¯”è¾ƒé«˜ï¼Œå¯ä»¥è®¾ç½®SyncEnableç­‰äºfalseï¼ŒRWModeæ”¹æˆMMapï¼Œå†™æ€§èƒ½ä¼šå¾—åˆ°æå¤§æå‡ï¼Œç¼ºç‚¹æ˜¯å¯èƒ½ä¼šä¸¢æ•°æ®ï¼ˆä¾‹å¦‚é‡åˆ°æ–­ç”µæˆ–è€…ç³»ç»Ÿå¥”æºƒï¼‰\r\n\r\n```\r\nvar DefaultOptions = func() Options {\r\n\treturn Options{\r\n\t\tEntryIdxMode:      HintKeyValAndRAMIdxMode,\r\n\t\tSegmentSize:       defaultSegmentSize,\r\n\t\tNodeNum:           1,\r\n\t\tRWMode:            FileIO,\r\n\t\tSyncEnable:        true,\r\n\t\tCommitBufferSize:  4 * MB,\r\n\t\tMergeInterval:     2 * time.Hour,\r\n\t\tMaxBatchSize:      (15 * defaultSegmentSize / 4) / 100,\r\n\t\tMaxBatchCount:     (15 * defaultSegmentSize / 4) / 100 / 100,\r\n\t\tExpiredDeleteType: TimeWheel,\r\n\t}\r\n}()\r\n```\r\n\r\n### ä½¿ç”¨äº‹åŠ¡\r\n\r\nNutsDBä¸ºäº†ä¿è¯éš”ç¦»æ€§ï¼Œé˜²æ­¢å¹¶å‘è¯»å†™äº‹åŠ¡æ—¶å€™æ•°æ®çš„ä¸ä¸€è‡´æ€§ï¼ŒåŒä¸€æ—¶é—´åªèƒ½æ‰§è¡Œä¸€ä¸ªè¯»å†™äº‹åŠ¡ï¼Œä½†æ˜¯å…è®¸åŒä¸€æ—¶é—´æ‰§è¡Œå¤šä¸ªåªè¯»äº‹åŠ¡ã€‚\r\nä»v0.3.0ç‰ˆæœ¬å¼€å§‹ï¼ŒNutsDBéµå¾ªæ ‡å‡†çš„ACIDåŸåˆ™ã€‚ï¼ˆå‚è§[é™åˆ¶å’Œè­¦å‘Š](https://github.com/nutsdb/nutsdb/blob/master/README-CN.md#%E8%AD%A6%E5%91%8A%E5%92%8C%E9%99%90%E5%88%B6)ï¼‰\r\n\r\n\r\n#### è¯»å†™äº‹åŠ¡\r\n\r\n```golang\r\nerr := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n    ...\r\n    return nil\r\n})\r\n\r\n```\r\n\r\n#### åªè¯»äº‹åŠ¡\r\n\r\n```golang\r\nerr := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n    ...\r\n    return nil\r\n})\r\n\r\n```\r\n\r\n#### æ‰‹åŠ¨ç®¡ç†äº‹åŠ¡\r\n\r\nä»ä¸Šé¢çš„ä¾‹å­çœ‹åˆ° `DB.View()` å’Œ`DB.Update()` è¿™ä¸¤ä¸ªæ˜¯æ•°æ®åº“è°ƒç”¨äº‹åŠ¡çš„ä¸»è¦æ–¹æ³•ã€‚ä»–ä»¬æœ¬è´¨ä¸Šæ˜¯åŸºäº `DB.Begin()`æ–¹æ³•è¿›è¡Œçš„åŒ…è£…ã€‚ä»–ä»¬å¯ä»¥å¸®ä½ è‡ªåŠ¨ç®¡ç†äº‹åŠ¡çš„ç”Ÿå‘½å‘¨æœŸï¼Œä»äº‹åŠ¡çš„å¼€å§‹ã€äº‹åŠ¡çš„æ‰§è¡Œã€äº‹åŠ¡æäº¤æˆ–è€…å›æ»šä¸€ç›´åˆ°äº‹åŠ¡çš„å®‰å…¨çš„å…³é—­ä¸ºæ­¢ï¼Œå¦‚æœä¸­é—´æœ‰é”™è¯¯ä¼šè¿”å›ã€‚æ‰€ä»¥**ä¸€èˆ¬æƒ…å†µä¸‹æ¨èç”¨è¿™ç§æ–¹å¼å»è°ƒç”¨äº‹åŠ¡**ã€‚\r\n\r\nè¿™å¥½æ¯”å¼€è½¦æœ‰æ‰‹åŠ¨æŒ¡å’Œè‡ªåŠ¨æŒ¡ä¸€æ ·ï¼Œ `DB.View()` å’Œ`DB.Update()`ç­‰äºæä¾›äº†è‡ªåŠ¨æ¡£çš„æ•ˆæœã€‚\r\n\r\nå¦‚æœä½ éœ€è¦æ‰‹åŠ¨å»å¼€å¯ã€æ‰§è¡Œã€å…³é—­äº‹åŠ¡ï¼Œä½ ä¼šç”¨åˆ°`DB.Begin()`æ–¹æ³•å¼€å¯ä¸€ä¸ªäº‹åŠ¡ï¼Œ`tx.Commit()` æ–¹æ³•ç”¨æ¥æäº¤äº‹åŠ¡ã€`tx.Rollback()`æ–¹æ³•ç”¨æ¥å›æ»šäº‹åŠ¡\r\n\r\nä¾‹å­ï¼š\r\n\r\n```golang\r\n//å¼€å§‹äº‹åŠ¡\r\ntx, err := db.Begin(true)\r\nif err != nil {\r\n    return err\r\n}\r\n\r\nbucket := \"bucket1\"\r\nkey := []byte(\"foo\")\r\nval := []byte(\"bar\")\r\n\r\n// ä½¿ç”¨äº‹åŠ¡\r\nif err = tx.Put(bucket, key, val, nutsdb.Persistent); err != nil {\r\n    // å›æ»šäº‹åŠ¡\r\n    tx.Rollback()\r\n} else {\r\n    // æäº¤äº‹åŠ¡\r\n    if err = tx.Commit(); err != nil {\r\n        tx.Rollback()\r\n        return err\r\n    }\r\n}\r\n```\r\n\r\n### ä½¿ç”¨buckets\r\n\r\nbucketsä¸­æ–‡ç¿»è¯‘è¿‡æ¥æ˜¯æ¡¶çš„æ„æ€ï¼Œä½ å¯ä»¥ç†è§£æˆç±»ä¼¼mysqlçš„tableè¡¨çš„æ¦‚å¿µï¼Œä¹Ÿå¯ä»¥ç†è§£æˆå‘½åç©ºé—´ï¼Œæˆ–è€…å¤šç§Ÿæˆ·çš„æ¦‚å¿µã€‚\r\næ‰€ä»¥ä½ å¯ä»¥ç”¨ä»–å­˜ä¸åŒçš„keyçš„é”®å€¼å¯¹ï¼Œä¹Ÿå¯ä»¥å­˜ç›¸åŒçš„keyçš„é”®å€¼å¯¹ã€‚æ‰€æœ‰çš„keyåœ¨ä¸€ä¸ªbucketé‡Œé¢ä¸èƒ½é‡å¤ã€‚\r\n\r\nä¾‹å­ï¼š\r\n\r\n```golang\r\n\r\nkey := []byte(\"key001\")\r\nval := []byte(\"val001\")\r\n\r\nbucket001 := \"bucket001\"\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        if err := tx.Put(bucket001, key, val, 0); err != nil {\r\n            return err\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nbucket002 := \"bucket002\"\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        if err := tx.Put(bucket002, key, val, 0); err != nil {\r\n            return err\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n```\r\nè¿™è¾¹æ³¨æ„ä¸‹ï¼Œè¿™ä¸ªbucketå’Œä½ ä½¿ç”¨æ•°æ®ç»“æ„æœ‰å…³ï¼Œä¸åŒæ•°æ®ç´¢å¼•ç»“æ„ï¼Œç”¨åŒä¸€ä¸ªbucketï¼Œä¹Ÿæ˜¯ä¸åŒçš„ã€‚æ¯”å¦‚ä½ å®šä¹‰äº†ä¸€ä¸ªbucketï¼Œå‘½åä¸º`bucket_foo`ï¼Œæ¯”å¦‚ä½ è¦ç”¨`list`è¿™ä¸ªæ•°æ®ç»“æ„ï¼Œä½¿ç”¨ `tx.RPush`åŠ æ•°æ®ï¼Œå¿…é¡»å¯¹åº”ä»–çš„æ•°æ®ç»“æ„å»ä»è¿™ä¸ª`bucket_foo`æŸ¥è¯¢æˆ–è€…å–å‡ºï¼Œæ¯”å¦‚ç”¨ `tx.RPop`ï¼Œ`tx.LRange` ç­‰ï¼Œä¸èƒ½ç”¨`tx.Get`ï¼ˆå’ŒGetAllã€Putã€Deleteã€RangeScanç­‰åŒä¸€ç´¢å¼•ç±»å‹ï¼‰å»è¯»å–è¿™ä¸ª`bucket_foo`é‡Œé¢çš„æ•°æ®ï¼Œå› ä¸ºç´¢å¼•ç»“æ„ä¸åŒã€‚å…¶ä»–æ•°æ®ç»“æ„å¦‚`Set`ã€`Sorted Set`åŒç†ã€‚\r\n\r\nä¸‹é¢è¯´æ˜ä¸‹è¿­ä»£buckets å’Œ åˆ é™¤bucketã€‚å®ƒä»¬éƒ½ç”¨åˆ°äº†`ds`ã€‚\r\n\r\ndsè¡¨ç¤ºæ•°æ®ç»“æ„ï¼Œæ”¯æŒå¦‚ä¸‹ï¼š\r\n* DataStructureSet\r\n* DataStructureSortedSet\r\n* DataStructureBPTree\r\n* DataStructureList\r\n\r\nç›®å‰æ”¯æŒçš„`EntryIdxMode`å¦‚ä¸‹ï¼š\r\n\r\n* HintKeyValAndRAMIdxMode \r\n* HintKeyAndRAMIdxMode \r\n\r\n#### è¿­ä»£buckets\r\n\r\nIterateBucketsæ”¯æŒè¿­ä»£æŒ‡å®šdsçš„è¿­ä»£ã€‚\r\n\r\n```go\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.IterateBuckets(nutsdb.DataStructureBPTree, \"*\", func(bucket string) bool {\r\n            fmt.Println(\"bucket: \", bucket)\r\n            // true: continue, false: break\r\n            return true\r\n        })\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n#### åˆ é™¤bucket\r\n\r\nDeleteBucketæ”¯æŒåˆ é™¤æŒ‡å®šçš„bucketï¼Œéœ€è¦ä¸¤ä¸ªå‚æ•°`ds`å’Œ`bucket`ã€‚\r\n\r\n```go\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.DeleteBucket(nutsdb.DataStructureBPTree, bucket)\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n\r\n### ä½¿ç”¨é”®å€¼å¯¹\r\n\r\n#### åŸºæœ¬æ“ä½œ\r\n\r\nå°†key-valueé”®å€¼å¯¹ä¿å­˜åœ¨ä¸€ä¸ªbucket, ä½ å¯ä»¥ä½¿ç”¨ `tx.Put` è¿™ä¸ªæ–¹æ³•:\r\n\r\n* æ·»åŠ æ•°æ®\r\n\r\n```golang\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n    key := []byte(\"name1\")\r\n    val := []byte(\"val1\")\r\n    bucket := \"bucket1\"\r\n    if err := tx.Put(bucket, key, val, 0); err != nil {\r\n        return err\r\n    }\r\n    return nil\r\n}); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n```\r\n\r\n* æ›´æ–°æ•°æ®\r\n\r\nä¸Šé¢çš„ä»£ç æ‰§è¡Œä¹‹åkeyä¸º\"name1\"å’Œvalueå€¼\"val1\"è¢«ä¿å­˜åœ¨å‘½åä¸ºbucket1çš„bucketé‡Œé¢ã€‚\r\n\r\nå¦‚æœä½ è¦åšæ›´æ–°æ“ä½œï¼Œä½ å¯ä»¥ä»ç„¶ç”¨`tx.Put`æ–¹æ³•å»æ‰§è¡Œï¼Œæ¯”å¦‚ä¸‹é¢çš„ä¾‹å­æŠŠvalueçš„å€¼æ”¹æˆ\"val1-modify\"ï¼š\r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n    key := []byte(\"name1\")\r\n    val := []byte(\"val1-modify\") // æ›´æ–°å€¼\r\n    bucket := \"bucket1\"\r\n    if err := tx.Put(bucket, key, val, 0); err != nil {\r\n        return err\r\n    }\r\n    return nil\r\n}); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n```\r\n\r\n* è·å–æ•°æ®\r\n\r\nè·å–å€¼å¯ä»¥ç”¨`tx.Get` è¿™ä¸ªæ–¹æ³•:\r\n\r\n```golang\r\nif err := db.View(\r\nfunc(tx *nutsdb.Tx) error {\r\n    key := []byte(\"name1\")\r\n    bucket := \"bucket1\"\r\n    if value, err := tx.Get(bucket, key); err != nil {\r\n        return err\r\n    } else {\r\n        fmt.Println(string(value)) // \"val1-modify\"\r\n    }\r\n    return nil\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n* åˆ é™¤æ•°æ®\r\n\r\nåˆ é™¤ä½¿ç”¨`tx.Delete()` æ–¹æ³•ï¼š\r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n    key := []byte(\"name1\")\r\n    bucket := \"bucket1\"\r\n    if err := tx.Delete(bucket, key); err != nil {\r\n        return err\r\n    }\r\n    return nil\r\n}); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n#### å¯¹å€¼çš„ä½æ“ä½œ\r\n\r\n* ä½¿ç”¨`tx.GetBit()`æ–¹æ³•è·å–æŸä¸€é”®æ‰€å¯¹åº”çš„å€¼åœ¨æŸä¸€åç§»é‡ä¸Šçš„å€¼ã€‚å½“å¯¹åº”çš„é”®å­˜åœ¨æ—¶ï¼Œè¿”å›å‚æ•°ä¸­åç§»é‡æ‰€å¯¹åº”ä½ç½®çš„ä¸Šçš„å€¼ï¼Œå½“åç§»é‡è¶…å‡ºåŸæœ‰çš„æ•°æ®èŒƒå›´æ—¶ï¼Œå°†è¿”å›0ä¸”ä¸æŠ¥é”™ï¼›å½“å¯¹åº”çš„é”®ä¸å­˜åœ¨æ—¶ï¼Œå°†æŠ¥é”™æç¤ºé”®ä¸å­˜åœ¨ã€‚\r\n\r\n```golang\r\nif err := db.View(func(tx *nutsdb.Tx) error {\r\n\tbucket := \"bucket\"\r\n\tkey := []byte(\"key\")\r\n\toffset := 2\r\n    bit, err := tx.GetBit(bucket, key, offset)\r\n    if err != nil {\r\n        return err\r\n    }\r\n    log.Println(\"get bit:\", bit)\r\n    return nil\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n* ä½¿ç”¨`tx.SetBit()`æ–¹æ³•æ·»åŠ æŸä¸€é”®æ‰€å¯¹åº”çš„å€¼åœ¨æŸä¸€åç§»é‡ä¸Šçš„å€¼ã€‚å½“å¯¹åº”çš„é”®å­˜åœ¨æ—¶ï¼Œå°†ä¼šä¿®æ”¹åç§»é‡æ‰€å¯¹åº”çš„ä½ä¸Šçš„å€¼ï¼›å½“å¯¹åº”çš„é”®ä¸å­˜åœ¨æˆ–è€…åç§»é‡è¶…å‡ºåŸæœ‰çš„æ•°æ®èŒƒå›´æ—¶ï¼Œå°†ä¼šå¯¹åŸæœ‰å€¼è¿›è¡Œæ‰©å®¹ç›´åˆ°èƒ½å¤Ÿåœ¨åç§»é‡å¯¹åº”ä½ç½®ä¸Šä¿®æ”¹ã€‚é™¤åç§»é‡å¯¹åº”ä½ç½®ä¹‹å¤–ï¼Œè‡ªåŠ¨æ‰©å®¹äº§ç”Ÿçš„ä½çš„å€¼å‡ä¸º0ã€‚\r\n\r\n```golang\r\nif err := db.Update(func(tx *nutsdb.Tx) error {\r\n\tbucket := \"bucket\"\r\n\tkey := []byte(\"key\")\r\n\toffset := 2\r\n\tbit := 1\r\n\treturn tx.SetBit(bucket, key, offset, bit)\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n#### å¯¹å€¼çš„è‡ªå¢å’Œè‡ªå‡æ“ä½œ\r\n\r\nåœ¨å¯¹å€¼è¿›è¡Œè‡ªå¢å’Œè‡ªå‡æ“ä½œæ—¶éœ€è¦é”®å­˜åœ¨ï¼Œå¦åˆ™å°†æŠ¥é”™æç¤ºé”®ä¸å­˜åœ¨ã€‚å½“å€¼çš„è‡ªå¢å’Œè‡ªå‡ç»“æœå°†è¶…å‡º`int64`çš„èŒƒå›´æ—¶ï¼Œå°†ä½¿ç”¨åŸºäºå­—ç¬¦ä¸²çš„å¤§æ•°è®¡ç®—ï¼Œæ‰€ä»¥ä¸å¿…æ‹…å¿ƒå€¼çš„èŒƒå›´è¿‡å¤§ã€‚\r\n\r\n* ä½¿ç”¨`tx.Incr()`æ–¹æ³•è®©æŸä¸€é”®æ‰€å¯¹åº”çš„å€¼è‡ªå¢1\r\n\r\n```golang\r\nif err := db.Update(func(tx *nutsdb.Tx) error {\r\n\tbucket := \"bucket\"\r\n\tkey := []byte(\"key\")\r\n    return tx.Incr(bucket, key)\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n* ä½¿ç”¨`tx.IncrBy()`æ–¹æ³•è®©æŸä¸€é”®æ‰€å¯¹åº”çš„å€¼è‡ªå¢æŒ‡å®šçš„å€¼\r\n\r\n```golang\r\nif err := db.Update(func(tx *nutsdb.Tx) error {\r\n    bucket := \"bucket\"\r\n    key := []byte(\"key\")\r\n    return tx.IncrBy(bucket, key, 10)\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n* ä½¿ç”¨`tx.Decr()`æ–¹æ³•è®©æŸä¸€é”®æ‰€å¯¹åº”çš„å€¼è‡ªå‡1\r\n\r\n```golang\r\nif err := db.Update(func(tx *nutsdb.Tx) error {\r\n\tbucket := \"bucket\"\r\n\tkey := []byte(\"key\")\r\n    return tx.Decr(bucket, key)\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n* ä½¿ç”¨`tx.DecrBy()`æ–¹æ³•è®©æŸä¸€é”®æ‰€å¯¹åº”çš„å€¼è‡ªå‡æŒ‡å®šçš„å€¼\r\n\r\n```golang\r\nif err := db.Update(func(tx *nutsdb.Tx) error {\r\n    bucket := \"bucket\"\r\n    key := []byte(\"key\")\r\n    return tx.DecrBy(bucket, key, 10)\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n#### å¯¹å€¼çš„è¿ç»­å¤šæ¬¡Setå’ŒGet\r\n\r\n* ä½¿ç”¨`tx.MSet()`æ–¹æ³•è¿ç»­å¤šæ¬¡è®¾ç½®é”®å€¼å¯¹ã€‚å½“ä½¿ç”¨`tx.MSet()`éœ€è¦ä»¥`...[]byte`ç±»å‹ä¼ å…¥è‹¥å¹²ä¸ªé”®å€¼å¯¹ã€‚æ­¤å¤„è¦æ±‚å‚æ•°çš„æ€»æ•°ä¸ºå¶æ•°ä¸ªï¼Œè®¾iä¸ºä»0å¼€å§‹çš„å¶æ•°ï¼Œåˆ™ç¬¬iä¸ªå‚æ•°å’Œç¬¬i+1ä¸ªå‚æ•°å°†åˆ†åˆ«æˆä¸ºä¸€ä¸ªé”®å€¼å¯¹çš„é”®å’Œå€¼ã€‚\r\n\r\n```golang\r\nif err := db.Update(func(tx *nutsdb.Tx) error {\r\n\tbucekt := \"bucket\"\r\n\targs := [][]byte{\r\n        []byte(\"1\"), []byte(\"2\"), []byte(\"3\"), []byte(\"4\"),\r\n    }\r\n    return tx.MSet(bucket, nutsdb.Persistent, args...)\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n* ä½¿ç”¨`tx.MGet()`æ–¹æ³•è¿ç»­å¤šæ¬¡å–å€¼ã€‚å½“ä½¿ç”¨`tx.MGet()`éœ€è¦ä»¥`...[]byte`ç±»å‹ä¼ å…¥è‹¥å¹²ä¸ªé”®ï¼Œè‹¥å…¶ä¸­ä»»ä½•ä¸€ä¸ªé”®ä¸å­˜åœ¨éƒ½ä¼šè¿”å›`key not found`é”™è¯¯ã€‚è¿”å›å€¼æ˜¯ä¸€ä¸ªåˆ‡ç‰‡ï¼Œé•¿åº¦ä¸ä¼ å…¥çš„å‚æ•°ç›¸åŒï¼Œå¹¶ä¸”æ ¹æ®åˆ‡ç‰‡ç´¢å¼•ä¸€ä¸€å¯¹åº”ã€‚\r\n\r\n```golang\r\nif err := db.View(func(tx *nutsdb.Tx) error {\r\n\tbucket := \"bucket\"\r\n\tkey := [][]byte{\r\n\t\t[]byte(\"1\"), []byte(\"2\"), []byte(\"3\"), []byte(\"4\"),\r\n    }\r\n    values, err := tx.MGet(bucket, key...)\r\n    if err != nil {\r\n        return err\r\n    }\r\n    for i, value := range values {\r\n        log.Printf(\"get value by MGet, the %d value is '%s'\", i, string(value))\r\n    }\r\n    return nil\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n#### å¯¹å€¼çš„å¢è¡¥æ“ä½œ\r\n\r\n* ä½¿ç”¨`tx.Append()`æ–¹æ³•å¯¹å€¼è¿›è¡Œå¢è¡¥ã€‚\r\n\r\n```golang\r\nif err := db.Update(func(tx *nutsdb.Tx) error {\r\n\tbucket := \"bucket\"\r\n\tkey := \"key\"\r\n\tappendage := \"appendage\"\r\n    return tx.Append(bucket, []byte(key), []byte(appendage))\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n#### è·å–å€¼çš„ä¸€éƒ¨åˆ†\r\n\r\n* ä½¿ç”¨`tx.GetRange()`æ–¹æ³•å¯ä»¥æ ¹æ®ç»™å®šçš„ç´¢å¼•è·å–å€¼çš„ä¸€éƒ¨åˆ†ã€‚é€šè¿‡ä¸¤ä¸ª`int`ç±»å‹çš„å‚æ•°ç¡®å®šä¸€ä¸ªé—­åŒºé—´ï¼Œè¿”å›é—­åŒºé—´æ‰€å¯¹åº”éƒ¨åˆ†çš„å€¼ã€‚\r\n\r\n```golang\r\nif err := db.View(func(tx *nutsdb.Tx) error {\r\n\tbucket := \"bucket\"\r\n\tkey := \"key\"\r\n\tstart := 0\r\n\tend := 2\r\n    value, err := tx.GetRange(bucket, []byte(key), start, end)\r\n    if err != nil {\r\n        return err\r\n    }\r\n    log.Printf(\"got value: '%s'\", string(value))\r\n    return nil\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n### ä½¿ç”¨TTL\r\n\r\nNusDBæ”¯æŒTTL(å­˜æ´»æ—¶é—´)çš„åŠŸèƒ½ï¼Œå¯ä»¥å¯¹æŒ‡å®šçš„bucketé‡Œçš„keyè¿‡æœŸæ—¶é—´çš„è®¾ç½®ã€‚ä½¿ç”¨`tx.Put`è¿™ä¸ªæ–¹æ³•çš„ä½¿ç”¨`ttl`å‚æ•°å°±å¯ä»¥äº†ã€‚\r\nå¦‚æœè®¾ç½® ttl = 0 æˆ–è€… Persistent, è¿™ä¸ªkeyå°±ä¼šæ°¸ä¹…å­˜åœ¨ã€‚ä¸‹é¢ä¾‹å­ä¸­ttlè®¾ç½®æˆ 60 , 60sä¹‹åkeyå°±ä¼šè¿‡æœŸï¼Œåœ¨æŸ¥è¯¢çš„æ—¶å€™å°†ä¸ä¼šè¢«æœåˆ°ã€‚\r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n    key := []byte(\"name1\")\r\n    val := []byte(\"val1\")\r\n    bucket := \"bucket1\"\r\n    \r\n    // å¦‚æœè®¾ç½® ttl = 0 or Persistent, è¿™ä¸ªkeyå°±ä¼šæ°¸ä¹…ä¸åˆ é™¤\r\n    // è¿™è¾¹ ttl = 60 , 60sä¹‹åå°±ä¼šè¿‡æœŸã€‚\r\n    if err := tx.Put(bucket, key, val, 60); err != nil {\r\n        return err\r\n    }\r\n    return nil\r\n}); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n### å¯¹keysçš„æ‰«ææ“ä½œ\r\n\r\nkeyåœ¨ä¸€ä¸ªbucketé‡Œé¢æŒ‰ç…§byte-sortedæœ‰åºæ’åºçš„ï¼Œæ‰€ä»¥å¯¹äºkeysçš„æ‰«ææ“ä½œï¼Œåœ¨NutsDBé‡Œæ˜¯å¾ˆé«˜æ•ˆçš„ã€‚\r\n\r\n\r\n#### å‰ç¼€æ‰«æ\r\n\r\nå¯¹äºå‰ç¼€çš„æ‰«æï¼Œæˆ‘ä»¬å¯ä»¥ç”¨`PrefixScan` æ–¹æ³•, ä½¿ç”¨å‚æ•° `offSet`å’Œ`limitNum` æ¥é™åˆ¶è¿”å›çš„ç»“æœçš„æ•°é‡ï¼Œæ¯”æ–¹ä¸‹é¢ä¾‹å­é™åˆ¶100ä¸ªentries:\r\n\r\n```golang\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        prefix := []byte(\"user_\")\r\n        bucket := \"user_list\"\r\n        // ä»offset=0å¼€å§‹ ï¼Œé™åˆ¶ 100 entries è¿”å› \r\n        if entries, err := tx.PrefixScan(bucket, prefix, 0, 100); err != nil {\r\n            return err\r\n        } else {\r\n            for _, entry := range entries {\r\n                fmt.Println(string(entry.Key), string(entry.Value))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n        log.Fatal(err)\r\n}\r\n\r\n```\r\n\r\n#### å‰ç¼€åçš„æ­£åˆ™æ‰«æ\r\n\r\nå¯¹äºå‰ç¼€åçš„æ‰«æï¼Œå¯ä»¥é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼å¯¹é”®çš„ç¬¬äºŒéƒ¨åˆ†è¿›è¡Œæœç´¢æ¥éå†ä¸€ä¸ªé”®å‰ç¼€ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`PrefixSearchScan`æ–¹æ³•ï¼Œç”¨å‚æ•°`reg`æ¥ç¼–å†™æ­£åˆ™è¡¨è¾¾å¼ï¼Œä½¿ç”¨å‚æ•°`offsetNum`ã€`limitNum` æ¥çº¦æŸè¿”å›çš„æ¡ç›®çš„æ•°é‡:\r\n\r\n```go\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        prefix := []byte(\"user_\") // å®šä¹‰å‰ç¼€\r\n        reg := \"99\"  // å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼\r\n        bucket := \"user_list\"\r\n        // ä»offset=25å¼€å§‹ï¼Œé™åˆ¶ 100 entries è¿”å› \r\n        if entries, _, err := tx.PrefixSearchScan(bucket, prefix, reg, 25, 100); err != nil {\r\n            return err\r\n        } else {\r\n            for _, entry := range entries {\r\n                fmt.Println(string(entry.Key), string(entry.Value))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n        log.Fatal(err)\r\n}\r\n```\r\n\r\n#### èŒƒå›´æ‰«æ\r\n\r\nå¯¹äºèŒƒå›´çš„æ‰«æï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ `RangeScan` æ–¹æ³•ã€‚\r\n\r\nä¾‹å­ï¼š\r\n\r\n```golang\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        // å‡è®¾ç”¨æˆ·keyä» user_0000000 to user_9999999.\r\n        // æ‰§è¡ŒåŒºé—´æ‰«æç±»ä¼¼è¿™æ ·ä¸€ä¸ªstartå’Œendä½œä¸ºä¸»è¦å‚æ•°.\r\n        start := []byte(\"user_0010001\")\r\n        end := []byte(\"user_0010010\")\r\n        bucket := \"user_list\"\r\n        if entries, err := tx.RangeScan(bucket, start, end); err != nil {\r\n            return err\r\n        } else {\r\n            for _, entry := range entries {\r\n                fmt.Println(string(entry.Key), string(entry.Value))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n### è·å–å…¨éƒ¨çš„keyå’Œvalue\r\n\r\nå¯¹äºè·å–ä¸€ä¸ªbucketçš„æ‰€æœ‰keyå’Œvalueï¼Œå¯ä»¥ä½¿ç”¨`GetAll`æ–¹æ³•ã€‚\r\n\r\nä¾‹å­ï¼š\r\n\r\n```go\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"user_list\"\r\n        entries, err := tx.GetAll(bucket)\r\n        if err != nil {\r\n            return err\r\n        }\r\n\r\n        for _, entry := range entries {\r\n            fmt.Println(string(entry.Key),string(entry.Value))\r\n        }\r\n\r\n        return nil\r\n    }); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n### è¿­ä»£å™¨\r\n\r\nä¸»è¦æ˜¯è¿­ä»£å™¨çš„é€‰é¡¹å‚æ•°`Reverse`çš„å€¼æ¥å†³å®šæ­£å‘è¿˜æ˜¯åå‘è¿­ä»£å™¨, å½“å‰ç‰ˆæœ¬è¿˜ä¸æ”¯æŒHintBPTSparseIdxModeçš„è¿­ä»£å™¨\r\n\r\n\r\n#### æ­£å‘çš„è¿­ä»£å™¨\r\n\r\n```go\r\ntx, err := db.Begin(false)\r\niterator := nutsdb.NewIterator(tx, bucket, nutsdb.IteratorOptions{Reverse: false})\r\ni := 0\r\nfor i < 10 {\r\n    ok, err := iterator.SetNext()\r\n    fmt.Println(\"ok, err\", ok, err)\r\n    fmt.Println(\"Key: \", string(iterator.Entry().Key))\r\n    fmt.Println(\"Value: \", string(iterator.Entry().Value))\r\n    fmt.Println()\r\n    i++\r\n}\r\nerr = tx.Commit()\r\nif err != nil {\r\n    panic(err)\r\n}\r\n```\r\n\r\n#### åå‘çš„è¿­ä»£å™¨\r\n\r\n```go\r\ntx, err := db.Begin(false)\r\niterator := nutsdb.NewIterator(tx, bucket, nutsdb.IteratorOptions{Reverse: true})\r\ni := 0\r\nfor i < 10 {\r\n    ok, err := iterator.SetNext()\r\n    fmt.Println(\"ok, err\", ok, err)\r\n    fmt.Println(\"Key: \", string(iterator.Entry().Key))\r\n    fmt.Println(\"Value: \", string(iterator.Entry().Value))\r\n    fmt.Println()\r\n    i++\r\n}\r\nerr = tx.Commit()\r\nif err != nil {\r\n    panic(err)\r\n}\r\n```\r\n\r\n\r\n\r\n### åˆå¹¶æ“ä½œ\r\n\r\nnutsdbä¸ºäº†ä¿æŒé«˜æ€§èƒ½å†™ï¼ŒåŒä¸€ä¸ªkeyä¼šå†™å¤šä»½ï¼Œå¦‚æœä½ çš„æœåŠ¡ï¼Œæœ‰å¯¹åŒä¸€ä¸ªkeyå¤šæ¬¡çš„æ›´æ–°æˆ–è€…åˆ é™¤ï¼Œä½ å¸Œæœ›å¯¹åŒä¸€ä¸ªkeyåšåˆå¹¶ï¼Œå¯ä»¥ä½¿ç”¨NutsDBæä¾›äº†`db.Merge()`æ–¹æ³•ã€‚\r\nè¿™ä¸ªæ–¹æ³•éœ€è¦è‡ªå·±æ ¹æ®å®é™…æƒ…å†µç¼–å†™åˆå¹¶ç­–ç•¥ã€‚ä¸€æ—¦æ‰§è¡Œä¼šå½±å“åˆ°æ­£å¸¸çš„å†™è¯·æ±‚ï¼Œæ‰€ä»¥æœ€å¥½é¿å¼€é«˜å³°æœŸï¼Œæ¯”å¦‚åŠå¤œå®šæ—¶æ‰§è¡Œç­‰ã€‚\r\n\r\nå½“ç„¶ï¼Œå¦‚æœä½ æ²¡æœ‰å¯¹åŒä¸€ä¸ªkeyæœ‰å¤ªå¤šçš„æ›´æ–°æˆ–è€…åˆ é™¤ï¼Œå¯ä»¥ä¸ç”¨Merge()å‡½æ•°ã€‚\r\n\r\n```golang\r\nerr := db.Merge()\r\nif err != nil {\r\n    ...\r\n}\r\n```\r\n\r\næ³¨æ„ï¼šå½“å‰ç‰ˆæœ¬ä¸æ”¯æŒ`HintBPTSparseIdxMode`æ¨¡å¼çš„åˆå¹¶æ“ä½œ\r\n\r\n### æ•°æ®åº“å¤‡ä»½\r\n\r\nå¯¹äºæ•°æ®åº“çš„å¤‡ä»½ï¼Œä½ å¯ä»¥è°ƒç”¨ `db.Backup()`æ–¹æ³•ï¼Œåªè¦æä¾›ä¸€ä¸ªå¤‡ä»½çš„æ–‡ä»¶ç›®å½•åœ°å€å³å¯ã€‚è¿™ä¸ªæ–¹æ³•æ‰§è¡Œçš„æ˜¯ä¸€ä¸ªçƒ­å¤‡ä»½ï¼Œä¸ä¼šé˜»å¡åˆ°æ•°æ®åº“å…¶ä»–çš„åªè¯»äº‹åŠ¡æ“ä½œï¼Œå¯¹å†™ï¼ˆè¯»å†™ï¼‰äº‹åŠ¡ä¼šæœ‰å½±å“ã€‚\r\n\r\n```golang\r\nerr = db.Backup(dir)\r\nif err != nil {\r\n   ...\r\n}\r\n```\r\n\r\nNutsDBè¿˜æä¾›gzipçš„å‹ç¼©å¤‡ä»½ï¼š\r\n\r\n```golang\r\nf, _ := os.Create(path)\r\ndefer f.Close()\r\nerr = db.BackupTarGZ(f)\r\nif err != nil {\r\n   ...\r\n}\r\n\r\n```\r\n\r\nå¥½äº†ï¼Œå…¥é—¨æŒ‡å—å·²ç»å®Œç»“ã€‚ æ•£èŠ±~ï¼Œåˆ°ç›®å‰ä¸ºæ­¢éƒ½æ˜¯Stringç±»å‹çš„æ•°æ®çš„crudæ“ä½œï¼Œä¸‹é¢å°†å­¦ä¹ å…¶ä»–æ›´å¤šçš„æ•°æ®ç»“æ„çš„æ“ä½œã€‚\r\n\r\n### ä½¿ç”¨å†…å­˜æ¨¡å¼\r\n\r\nNutsDBä»0.7.0ç‰ˆæœ¬å¼€å§‹æ”¯æŒå†…å­˜æ¨¡å¼ï¼Œè¿™ä¸ªæ¨¡å¼ä¸‹ï¼Œé‡å¯æ•°æ®åº“ï¼Œæ•°æ®ä¼šä¸¢å¤±çš„ã€‚å¦å¤–ï¼Œå†…å­˜æ¨¡å¼æœ‰ä¸€äº›APIï¼Œç›¸å¯¹éå†…å­˜æ¨¡å‹ï¼Œå¹¶æ²¡æœ‰å®ç°ï¼Œå¦‚æœä½ å‘ç°æœ‰ç¼ºå°‘çš„ï¼Œå¹¶å¸Œæœ›å®ç°ï¼Œè¯·æäº¤issueè¯´æ˜æƒ…å†µã€‚\r\n\r\nä¾‹å­ï¼š\r\n\r\n```go\r\n\r\n    opts := inmemory.DefaultOptions\r\n    db, err := inmemory.Open(opts)\r\n    if err != nil {\r\n        panic(err)\r\n    }\r\n    bucket := \"bucket1\"\r\n    key := []byte(\"key1\")\r\n    val := []byte(\"val1\")\r\n    err = db.Put(bucket, key, val, 0)\r\n    if err != nil {\r\n        fmt.Println(\"err\", err)\r\n    }\r\n\r\n    entry, err := db.Get(bucket, key)\r\n    if err != nil {\r\n        fmt.Println(\"err\", err)\r\n    }\r\n\r\n    fmt.Println(\"entry.Key\", string(entry.Key))     // entry.Key key1\r\n    fmt.Println(\"entry.Value\", string(entry.Value)) // entry.Value val1\r\n    \r\n```\r\n\r\n### ä½¿ç”¨å…¶ä»–æ•°æ®ç»“æ„\r\n\r\nçœ‹åˆ°è¿™è¾¹æˆ‘ä»¬å°†å­¦ä¹ å…¶ä»–æ•°æ®ç»“æ„ï¼ŒApiå‘½åé£æ ¼æ¨¡ä»¿ [Redis å‘½ä»¤](https://redis.io/commands)ã€‚æ‰€ä»¥å¦‚æœä½ ç†Ÿæ‚‰Redisï¼Œå°†ä¼šå¾ˆå¿«æŒæ¡ä½¿ç”¨ã€‚\r\nå…¶ä»–æ–¹é¢ç»§æ‰¿äº†ä¸Šé¢çš„bucket/key/valueæ¨¡å‹ï¼Œæ‰€ä»¥ä½ ä¼šçœ‹åˆ°å’ŒRedisçš„Apiä½¿ç”¨ä¸Šç¨å¾®æœ‰äº›ä¸åŒï¼Œä¼šå¤šä¸€ä¸ªbucketã€‚\r\n\r\n#### List\r\n\r\n##### RPush\r\n\r\nä»æŒ‡å®šbucketé‡Œé¢çš„æŒ‡å®šé˜Ÿåˆ—keyçš„å³è¾¹å…¥é˜Ÿä¸€ä¸ªæˆ–è€…å¤šä¸ªå…ƒç´ valã€‚\r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        val := []byte(\"val1\")\r\n        return tx.RPush(bucket, key, val)\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### LPush \r\n\r\nä»æŒ‡å®šbucketé‡Œé¢çš„æŒ‡å®šé˜Ÿåˆ—keyçš„å·¦è¾¹å…¥é˜Ÿä¸€ä¸ªæˆ–è€…å¤šä¸ªå…ƒç´ valã€‚\r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n            bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        val := []byte(\"val2\")\r\n        return tx.LPush(bucket, key, val)\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### LPop \r\n\r\nä»æŒ‡å®šbucketé‡Œé¢çš„æŒ‡å®šé˜Ÿåˆ—keyçš„å·¦è¾¹å‡ºé˜Ÿä¸€ä¸ªå…ƒç´ ï¼Œåˆ é™¤å¹¶è¿”å›ã€‚\r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n            bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        if item, err := tx.LPop(bucket, key); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"LPop item:\", string(item))\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### LPeek\r\n\r\nä»æŒ‡å®šbucketé‡Œé¢çš„æŒ‡å®šé˜Ÿåˆ—keyçš„å·¦è¾¹å‡ºé˜Ÿä¸€ä¸ªå…ƒç´ è¿”å›ä¸åˆ é™¤ã€‚\r\n\r\n```golang\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n            bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        if item, err := tx.LPeek(bucket, key); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"LPeek item:\", string(item)) //val11\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### RPop \r\n\r\nä»æŒ‡å®šbucketé‡Œé¢çš„æŒ‡å®šé˜Ÿåˆ—keyçš„å³è¾¹å‡ºé˜Ÿä¸€ä¸ªå…ƒç´ ï¼Œåˆ é™¤å¹¶è¿”å›ã€‚\r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n            bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        if item, err := tx.RPop(bucket, key); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"RPop item:\", string(item))\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### RPeek\r\n\r\nä»æŒ‡å®šbucketé‡Œé¢çš„æŒ‡å®šé˜Ÿåˆ—keyçš„å³è¾¹å‡ºé˜Ÿä¸€ä¸ªå…ƒç´ è¿”å›ä¸åˆ é™¤ã€‚\r\n\r\n```golang\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n            bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        if item, err := tx.RPeek(bucket, key); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"RPeek item:\", string(item))\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### LRange \r\n\r\nè¿”å›æŒ‡å®šbucketé‡Œé¢çš„æŒ‡å®šé˜Ÿåˆ—keyåˆ—è¡¨é‡ŒæŒ‡å®šèŒƒå›´å†…çš„å…ƒç´ ã€‚ start å’Œ end åç§»é‡éƒ½æ˜¯åŸºäº0çš„ä¸‹æ ‡ï¼Œå³listçš„ç¬¬ä¸€ä¸ªå…ƒç´ ä¸‹æ ‡æ˜¯0ï¼ˆlistçš„è¡¨å¤´ï¼‰ï¼Œç¬¬äºŒä¸ªå…ƒç´ ä¸‹æ ‡æ˜¯1ï¼Œä»¥æ­¤ç±»æ¨ã€‚\r\nåç§»é‡ä¹Ÿå¯ä»¥æ˜¯è´Ÿæ•°ï¼Œè¡¨ç¤ºåç§»é‡æ˜¯ä»listå°¾éƒ¨å¼€å§‹è®¡æ•°ã€‚ ä¾‹å¦‚ï¼š-1 è¡¨ç¤ºåˆ—è¡¨çš„æœ€åä¸€ä¸ªå…ƒç´ ï¼Œ-2 æ˜¯å€’æ•°ç¬¬äºŒä¸ªï¼Œä»¥æ­¤ç±»æ¨ã€‚\r\n\r\n```golang\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n            bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        if items, err := tx.LRange(bucket, key, 0, -1); err != nil {\r\n            return err\r\n        } else {\r\n            //fmt.Println(items)\r\n            for _, item := range items {\r\n                fmt.Println(string(item))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### LRem \r\n\r\næ³¨æ„: è¿™ä¸ªæ–¹æ³•åœ¨ v0.6.0ç‰ˆæœ¬å¼€å§‹æ”¯æŒï¼Œä¹‹å‰çš„ç‰ˆæœ¬å®ç°å’Œæè¿°æœ‰é—®é¢˜ã€‚\r\n\r\nä»æŒ‡å®šbucketé‡Œé¢çš„æŒ‡å®šçš„keyçš„åˆ—è¡¨é‡Œç§»é™¤å‰ count æ¬¡å‡ºç°çš„å€¼ä¸º value çš„å…ƒç´ ã€‚ è¿™ä¸ª count å‚æ•°é€šè¿‡ä¸‹é¢å‡ ç§æ–¹å¼å½±å“è¿™ä¸ªæ“ä½œï¼š\r\n\r\ncount > 0: ä»å¤´å¾€å°¾ç§»é™¤å€¼ä¸º value çš„å…ƒç´ ã€‚\r\ncount < 0: ä»å°¾å¾€å¤´ç§»é™¤å€¼ä¸º value çš„å…ƒç´ ã€‚\r\ncount = 0: ç§»é™¤æ‰€æœ‰å€¼ä¸º value çš„å…ƒç´ ã€‚\r\n\r\nä¸‹é¢çš„ä¾‹å­count=1ï¼š\r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n            bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        return tx.LRem(bucket, key, 1, []byte(\"val11\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### LRemByIndex\r\n\r\næ³¨æ„: è¿™ä¸ªæ–¹æ³•åœ¨ v0.10.0ç‰ˆæœ¬å¼€å§‹æ”¯æŒ\r\n\r\nç§»é™¤åˆ—è¡¨ä¸­æŒ‡å®šä½ç½®ï¼ˆå•ä¸ªæˆ–å¤šä¸ªä½ç½®ï¼‰çš„å…ƒç´ \r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        removedNum, err := tx.LRemByIndex(bucket, key, 0, 1)\r\n        fmt.Printf(\"removed num %d\\n\", removedNum)\r\n        return err\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### LTrim \r\n\r\nä¿®å‰ªä¸€ä¸ªå·²å­˜åœ¨çš„ listï¼Œè¿™æ · list å°±ä¼šåªåŒ…å«æŒ‡å®šèŒƒå›´çš„æŒ‡å®šå…ƒç´ ã€‚start å’Œ stop éƒ½æ˜¯ç”±0å¼€å§‹è®¡æ•°çš„ï¼Œ è¿™é‡Œçš„ 0 æ˜¯åˆ—è¡¨é‡Œçš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆè¡¨å¤´ï¼‰ï¼Œ1 æ˜¯ç¬¬äºŒä¸ªå…ƒç´ ï¼Œä»¥æ­¤ç±»æ¨ã€‚\r\n\r\nä¾‹å¦‚ï¼š LTRIM foobar 0 2 å°†ä¼šå¯¹å­˜å‚¨åœ¨ foobar çš„åˆ—è¡¨è¿›è¡Œä¿®å‰ªï¼Œåªä¿ç•™åˆ—è¡¨é‡Œçš„å‰3ä¸ªå…ƒç´ ã€‚\r\n\r\nstart å’Œ end ä¹Ÿå¯ä»¥ç”¨è´Ÿæ•°æ¥è¡¨ç¤ºä¸è¡¨å°¾çš„åç§»é‡ï¼Œæ¯”å¦‚ -1 è¡¨ç¤ºåˆ—è¡¨é‡Œçš„æœ€åä¸€ä¸ªå…ƒç´ ï¼Œ -2 è¡¨ç¤ºå€’æ•°ç¬¬äºŒä¸ªï¼Œç­‰ç­‰ã€‚\r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n            bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        return tx.LTrim(bucket, key, 0, 1)\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### LSize \r\n\r\nè¿”å›æŒ‡å®šbucketä¸‹æŒ‡å®škeyåˆ—è¡¨çš„sizeå¤§å°\r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n            bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        if size,err := tx.LSize(bucket, key); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"myList size is \",size)\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### LKeys\r\n\r\næŸ¥æ‰¾`List`ç±»å‹çš„æ‰€æœ‰åŒ¹é…æŒ‡å®šæ¨¡å¼`pattern`çš„`key`ï¼Œç±»ä¼¼äºRediså‘½ä»¤: [KEYS](https://redis.io/commands/keys/)\r\n\r\næ³¨æ„ï¼šæ¨¡å¼åŒ¹é…ä½¿ç”¨ Go æ ‡å‡†åº“çš„`filepath.Match`ï¼Œéƒ¨åˆ†ç»†èŠ‚ä¸Šå’Œredisçš„è¡Œä¸ºæœ‰åŒºåˆ«ï¼Œæ¯”å¦‚å¯¹äº `[` çš„å¤„ç†ã€‚\r\n\r\n```golang\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        var keys []string\r\n        err := tx.LKeys(bucket, \"*\", func(key string) bool {\r\n            keys = append(keys, key)\r\n            // true: continue, false: break\r\n            return true\r\n        })\r\n        fmt.Printf(\"keys: %v\\n\", keys)\r\n        return err\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n#### Set\r\n\r\n##### SAdd\r\n\r\næ·»åŠ ä¸€ä¸ªæŒ‡å®šçš„memberå…ƒç´ åˆ°æŒ‡å®šbucketçš„é‡Œçš„æŒ‡å®šé›†åˆkeyä¸­ã€‚\r\n\r\n```go\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n            bucket := \"bucketForSet\"\r\n        key := []byte(\"mySet\")\r\n        return tx.SAdd(bucket, key, []byte(\"a\"), []byte(\"b\"), []byte(\"c\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### SAreMembers \r\n\r\nè¿”å›å¤šä¸ªæˆå‘˜memberæ˜¯å¦æ˜¯æŒ‡å®šbucketçš„é‡Œçš„æŒ‡å®šé›†åˆkeyçš„æˆå‘˜ã€‚\r\n\r\n```go\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"bucketForSet\"\r\n        key := []byte(\"mySet\")\r\n        if ok, err := tx.SAreMembers(bucket, key, []byte(\"a\"), []byte(\"b\"), []byte(\"c\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SAreMembers:\", ok)\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### SCard \r\n\r\nè¿”å›æŒ‡å®šbucketçš„æŒ‡å®šçš„é›†åˆkeyçš„åŸºæ•° (é›†åˆå…ƒç´ çš„æ•°é‡)ã€‚\r\n\r\n```go\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"bucketForSet\"\r\n        key := []byte(\"mySet\")\r\n        if num, err := tx.SCard(bucket, key); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SCard:\", num)\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n```\r\n##### SDiffByOneBucket \r\n\r\nè¿”å›ä¸€ä¸ªé›†åˆä¸ç»™å®šé›†åˆçš„å·®é›†çš„å…ƒç´ ã€‚è¿™ä¸¤ä¸ªé›†åˆéƒ½åœ¨ä¸€ä¸ªbucketä¸­ã€‚\r\n\r\n```go\r\n\r\nkey1 := []byte(\"mySet1\") // é›†åˆ1\r\nkey2 := []byte(\"mySet2\") // é›†åˆ2\r\nbucket := \"bucketForSet\"\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket, key1, []byte(\"a\"), []byte(\"b\"), []byte(\"c\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket, key2, []byte(\"c\"), []byte(\"d\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if items, err := tx.SDiffByOneBucket(bucket, key1, key2); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SDiffByOneBucket:\", items)\r\n            for _, item := range items {\r\n                fmt.Println(\"item\", string(item))\r\n            }\r\n            //item a\r\n            //item b\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n```\r\n\r\n##### SDiffByTwoBuckets \r\n\r\nè¿”å›ä¸€ä¸ªé›†åˆä¸ç»™å®šé›†åˆçš„å·®é›†çš„å…ƒç´ ã€‚è¿™ä¸¤ä¸ªé›†åˆåˆ†åˆ«åœ¨ä¸åŒbucketä¸­ã€‚\r\n\r\n```go\r\nbucket1 := \"bucket1\"\r\nkey1 := []byte(\"mySet1\")\r\n\r\nbucket2 := \"bucket2\"\r\nkey2 := []byte(\"mySet2\")\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket1, key1, []byte(\"a\"), []byte(\"b\"), []byte(\"c\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket2, key2, []byte(\"c\"), []byte(\"d\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if items, err := tx.SDiffByTwoBuckets(bucket1, key1, bucket2, key2); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SDiffByTwoBuckets:\", items)\r\n            for _, item := range items {\r\n                fmt.Println(\"item\", string(item))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n```\r\n##### SHasKey \r\n\r\nåˆ¤æ–­æ˜¯å¦æŒ‡å®šçš„é›†åˆåœ¨æŒ‡å®šçš„bucketä¸­ã€‚\r\n\r\n```go\r\n\r\nbucket := \"bucketForSet\"\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if ok, err := tx.SHasKey(bucket, []byte(\"mySet\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SHasKey\", ok)\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n```\r\n##### SIsMember \r\n\r\nè¿”å›æˆå‘˜memberæ˜¯å¦æ˜¯æŒ‡å®šbucketçš„å­˜æŒ‡å®škeyé›†åˆçš„æˆå‘˜ã€‚\r\n\r\n```go\r\nbucket := \"bucketForSet\"\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if ok, err := tx.SIsMember(bucket, []byte(\"mySet\"), []byte(\"a\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SIsMember\", ok)\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### SMembers \r\n\r\nè¿”å›æŒ‡å®šbucketçš„æŒ‡å®škeyé›†åˆæ‰€æœ‰çš„å…ƒç´ ã€‚\r\n\r\n```go\r\nbucket := \"bucketForSet\"\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if items, err := tx.SMembers(bucket, []byte(\"mySet\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SMembers\", items)\r\n            for _, item := range items {\r\n                fmt.Println(\"item\", string(item))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### SMoveByOneBucket \r\n\r\nå°†memberä»sourceé›†åˆç§»åŠ¨åˆ°destinationé›†åˆä¸­ï¼Œå…¶ä¸­sourceé›†åˆå’Œdestinationé›†åˆå‡åœ¨ä¸€ä¸ªbucketä¸­ã€‚\r\n\r\n```go\r\nbucket3 := \"bucket3\"\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket3, []byte(\"mySet1\"), []byte(\"a\"), []byte(\"b\"), []byte(\"c\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket3, []byte(\"mySet2\"), []byte(\"c\"), []byte(\"d\"), []byte(\"e\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        if ok, err := tx.SMoveByOneBucket(bucket3, []byte(\"mySet1\"), []byte(\"mySet2\"), []byte(\"a\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SMoveByOneBucket\", ok)\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if items, err := tx.SMembers(bucket3, []byte(\"mySet1\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"after SMoveByOneBucket bucket3 mySet1 SMembers\", items)\r\n            for _, item := range items {\r\n                fmt.Println(\"item\", string(item))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if items, err := tx.SMembers(bucket3, []byte(\"mySet2\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"after SMoveByOneBucket bucket3 mySet2 SMembers\", items)\r\n            for _, item := range items {\r\n                fmt.Println(\"item\", string(item))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### SMoveByTwoBuckets \r\n\r\nå°†memberä»sourceé›†åˆç§»åŠ¨åˆ°destinationé›†åˆä¸­ã€‚å…¶ä¸­sourceé›†åˆå’Œdestinationé›†åˆåœ¨ä¸¤ä¸ªä¸åŒçš„bucketä¸­ã€‚\r\n\r\n```go\r\nbucket4 := \"bucket4\"\r\nbucket5 := \"bucket5\"\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket4, []byte(\"mySet1\"), []byte(\"a\"), []byte(\"b\"), []byte(\"c\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket5, []byte(\"mySet2\"), []byte(\"c\"), []byte(\"d\"), []byte(\"e\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        if ok, err := tx.SMoveByTwoBuckets(bucket4, []byte(\"mySet1\"), bucket5, []byte(\"mySet2\"), []byte(\"a\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SMoveByTwoBuckets\", ok)\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if items, err := tx.SMembers(bucket4, []byte(\"mySet1\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"after SMoveByTwoBuckets bucket4 mySet1 SMembers\", items)\r\n            for _, item := range items {\r\n                fmt.Println(\"item\", string(item))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if items, err := tx.SMembers(bucket5, []byte(\"mySet2\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"after SMoveByTwoBuckets bucket5 mySet2 SMembers\", items)\r\n            for _, item := range items {\r\n                fmt.Println(\"item\", string(item))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### SPop \r\n\r\nä»æŒ‡å®šbucketé‡Œçš„æŒ‡å®škeyçš„é›†åˆä¸­ç§»é™¤å¹¶è¿”å›ä¸€ä¸ªæˆ–å¤šä¸ªéšæœºå…ƒç´ ã€‚\r\n\r\n```go\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        key := []byte(\"mySet\")\r\n        if item, err := tx.SPop(bucket, key); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SPop item from mySet:\", string(item))\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### SRem \r\n\r\nåœ¨æŒ‡å®šbucketé‡Œé¢ç§»é™¤æŒ‡å®šçš„keyé›†åˆä¸­ç§»é™¤æŒ‡å®šçš„ä¸€ä¸ªæˆ–è€…å¤šä¸ªå…ƒç´ ã€‚\r\n\r\n```go\r\nbucket6:=\"bucket6\"\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket6, []byte(\"mySet\"), []byte(\"a\"), []byte(\"b\"), []byte(\"c\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        if err := tx.SRem(bucket6, []byte(\"mySet\"), []byte(\"a\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SRem ok\")\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if items, err := tx.SMembers(bucket6, []byte(\"mySet\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SMembers items:\", items)\r\n            for _, item := range items {\r\n                fmt.Println(\"item:\", string(item))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### SUnionByOneBucket \r\n\r\nè¿”å›æŒ‡å®šä¸€ä¸ªbucketé‡Œé¢çš„ç»™å®šçš„ä¸¤ä¸ªé›†åˆçš„å¹¶é›†ä¸­çš„æ‰€æœ‰æˆå‘˜ã€‚\r\n\r\n```go\r\nbucket7 := \"bucket1\"\r\nkey1 := []byte(\"mySet1\")\r\nkey2 := []byte(\"mySet2\")\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket7, key1, []byte(\"a\"), []byte(\"b\"), []byte(\"c\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket7, key2, []byte(\"c\"), []byte(\"d\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if items, err := tx.SUnionByOneBucket(bucket7, key1, key2); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SUnionByOneBucket:\", items)\r\n            for _, item := range items {\r\n                fmt.Println(\"item\", string(item))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### SUnionByTwoBuckets \r\n\r\nè¿”å›æŒ‡å®šä¸¤ä¸ªbucketé‡Œé¢çš„ç»™å®šçš„ä¸¤ä¸ªé›†åˆçš„å¹¶é›†ä¸­çš„æ‰€æœ‰æˆå‘˜ã€‚\r\n\r\n```go\r\nbucket8 := \"bucket1\"\r\nkey1 := []byte(\"mySet1\")\r\n\r\nbucket9 := \"bucket2\"\r\nkey2 := []byte(\"mySet2\")\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket8, key1, []byte(\"a\"), []byte(\"b\"), []byte(\"c\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket9, key2, []byte(\"c\"), []byte(\"d\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if items, err := tx.SUnionByTwoBuckets(bucket8, key1, bucket9, key2); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SUnionByTwoBucket:\", items)\r\n            for _, item := range items {\r\n                fmt.Println(\"item\", string(item))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### SKeys\r\n\r\næŸ¥æ‰¾`Set`ç±»å‹çš„æ‰€æœ‰åŒ¹é…æŒ‡å®šæ¨¡å¼`pattern`çš„`key`ï¼Œç±»ä¼¼äºRediså‘½ä»¤: [KEYS](https://redis.io/commands/keys/)\r\n\r\næ³¨æ„ï¼šæ¨¡å¼åŒ¹é…ä½¿ç”¨ Go æ ‡å‡†åº“çš„`filepath.Match`ï¼Œéƒ¨åˆ†ç»†èŠ‚ä¸Šå’Œredisçš„è¡Œä¸ºæœ‰åŒºåˆ«ï¼Œæ¯”å¦‚å¯¹äº `[` çš„å¤„ç†ã€‚\r\n\r\n```golang\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        var keys []string\r\n        err := tx.SKeys(bucket, \"*\", func(key string) bool {\r\n            keys = append(keys, key)\r\n            // true: continue, false: break\r\n            return true\r\n        })\r\n        fmt.Printf(\"keys: %v\\n\", keys)\r\n        return err\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n#### Sorted Set\r\n\r\n> æ³¨æ„ï¼šè¿™è¾¹çš„bucketæ˜¯æœ‰åºé›†åˆåã€‚\r\n\r\n##### ZAdd\r\n\r\nå°†æŒ‡å®šæˆå‘˜ï¼ˆåŒ…æ‹¬keyã€scoreã€valueï¼‰æ·»åŠ åˆ°æŒ‡å®šbucketçš„æœ‰åºé›†åˆï¼ˆsorted setï¼‰é‡Œé¢ã€‚\r\n\r\n\r\n```go\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet1\" // æ³¨æ„ï¼šè¿™è¾¹çš„bucketæ˜¯æœ‰åºé›†åˆå\r\n        key := []byte(\"key1\")\r\n        return tx.ZAdd(bucket, key, 1, []byte(\"val1\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### ZCard \r\n\r\nè¿”å›æŒ‡å®šbucketçš„çš„æœ‰åºé›†å…ƒç´ ä¸ªæ•°ã€‚\r\n\r\n```go\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet1\"\r\n        if num, err := tx.ZCard(bucket); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZCard num\", num)\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### ZCount \r\n\r\nè¿”å›æŒ‡å®šbucketçš„æœ‰åºé›†ï¼Œscoreå€¼åœ¨minå’Œmaxä¹‹é—´(é»˜è®¤åŒ…æ‹¬scoreå€¼ç­‰äºstartæˆ–end)çš„æˆå‘˜ã€‚\r\n\r\nOptsåŒ…å«çš„å‚æ•°ï¼š\r\n\r\n* Limit        int  // é™åˆ¶è¿”å›çš„nodeæ•°ç›®\r\n* ExcludeStart bool // æ’é™¤start\r\n* ExcludeEnd   bool // æ’é™¤end\r\n\r\n```go\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet1\"\r\n        if num, err := tx.ZCount(bucket, 0, 1, nil); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZCount num\", num)\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### ZGetByKey \r\n\r\nè¿”å›ä¸€ä¸ªèŠ‚ç‚¹é€šè¿‡æŒ‡å®šçš„bucketæœ‰åºé›†åˆå’ŒæŒ‡å®šçš„keyæ¥è·å–ã€‚\r\n\r\n```go\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet1\"\r\n        key := []byte(\"key2\")\r\n        if node, err := tx.ZGetByKey(bucket, key); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZGetByKey key2 val:\", string(node.Value))\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### ZMembers \r\n\r\nè¿”å›æ‰€æœ‰æˆå‘˜é€šè¿‡åœ¨æŒ‡å®šçš„bucketã€‚\r\n\r\n```go\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet1\"\r\n        if nodes, err := tx.ZMembers(bucket); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZMembers:\", nodes)\r\n\r\n            for _, node := range nodes {\r\n                fmt.Println(\"member:\", node.Key(), string(node.Value))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### ZPeekMax \r\n\r\nè¿”å›æŒ‡å®šbucketæœ‰åºé›†åˆä¸­çš„å…·æœ‰æœ€é«˜å¾—åˆ†çš„æˆå‘˜ã€‚\r\n\r\n```go\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet1\"\r\n        if node, err := tx.ZPeekMax(bucket); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZPeekMax:\", string(node.Value))\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### ZPeekMin \r\n\r\nè¿”å›æŒ‡å®šbucketæœ‰åºé›†åˆä¸­çš„å…·æœ‰æœ€ä½å¾—åˆ†çš„æˆå‘˜ã€‚\r\n\r\n```go\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet1\"\r\n        if node, err := tx.ZPeekMin(bucket); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZPeekMin:\", string(node.Value))\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### ZPopMax \r\n\r\nåˆ é™¤å¹¶è¿”å›æŒ‡å®šbucketæœ‰åºé›†åˆä¸­çš„å…·æœ‰æœ€é«˜å¾—åˆ†çš„æˆå‘˜ã€‚\r\n\r\n```go\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet1\"\r\n        if node, err := tx.ZPopMax(bucket); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZPopMax:\", string(node.Value)) //val3\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### ZPopMin \r\n\r\nåˆ é™¤å¹¶è¿”å›æŒ‡å®šbucketæœ‰åºé›†åˆä¸­çš„å…·æœ‰æœ€ä½å¾—åˆ†çš„æˆå‘˜ã€‚\r\n\r\n```go\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet1\"\r\n        if node, err := tx.ZPopMin(bucket); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZPopMin:\", string(node.Value)) //val1\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### ZRangeByRank \r\n\r\nè¿”å›æŒ‡å®šbucketæœ‰åºé›†åˆçš„æ’åstartåˆ°endçš„èŒƒå›´ï¼ˆåŒ…æ‹¬startå’Œendï¼‰çš„æ‰€æœ‰å…ƒç´ ã€‚\r\n\r\n```go\r\n// ZAdd add items\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet2\"\r\n        key1 := []byte(\"key1\")\r\n        return tx.ZAdd(bucket, key1, 1, []byte(\"val1\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet2\"\r\n        key2 := []byte(\"key2\")\r\n        return tx.ZAdd(bucket, key2, 2, []byte(\"val2\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet2\"\r\n        key3 := []byte(\"key3\")\r\n        return tx.ZAdd(bucket, key3, 3, []byte(\"val3\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n// ZRangeByRank\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet2\"\r\n        if nodes, err := tx.ZRangeByRank(bucket, 1, 2); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZRangeByRank nodes :\", nodes)\r\n            for _, node := range nodes {\r\n                fmt.Println(\"item:\", node.Key(), node.Score())\r\n            }\r\n            \r\n            //item: key1 1\r\n            //item: key2 2\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### ZRangeByScore \r\n\r\nè¿”å›æŒ‡å®šbucketæœ‰åºé›†åˆçš„åˆ†æ•°startåˆ°endçš„èŒƒå›´ï¼ˆåŒ…æ‹¬startå’Œendï¼‰çš„æ‰€æœ‰å…ƒç´ ã€‚å…¶ä¸­æœ‰ä¸ª`Opts`å‚æ•°ç”¨æ³•å‚è€ƒ`ZCount`ã€‚\r\n\r\n```go\r\n// ZAdd\r\nif err := db.Update(\r\n        func(tx *nutsdb.Tx) error {\r\n            bucket := \"myZSet3\"\r\n            key1 := []byte(\"key1\")\r\n            return tx.ZAdd(bucket, key1, 70, []byte(\"val1\"))\r\n        }); err != nil {\r\n        log.Fatal(err)\r\n    }\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet3\"\r\n        key2 := []byte(\"key2\")\r\n        return tx.ZAdd(bucket, key2, 90, []byte(\"val2\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet3\"\r\n        key3 := []byte(\"key3\")\r\n        return tx.ZAdd(bucket, key3, 86, []byte(\"val3\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n// ZRangeByScore\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet3\"\r\n        if nodes, err := tx.ZRangeByScore(bucket, 80, 100,nil); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZRangeByScore nodes :\", nodes)\r\n            for _, node := range nodes {\r\n                fmt.Println(\"item:\", node.Key(), node.Score())\r\n            }\r\n            //item: key3 86\r\n            //item: key2 90\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}   \r\n```\r\n##### ZRank\r\n\r\nè¿”å›æœ‰åºé›†bucketä¸­æˆå‘˜æŒ‡å®šæˆå‘˜keyçš„æ’åã€‚å…¶ä¸­æœ‰åºé›†æˆå‘˜æŒ‰scoreå€¼é€’å¢(ä»å°åˆ°å¤§)é¡ºåºæ’åˆ—ã€‚æ³¨æ„æ’åä»¥1ä¸ºåº•ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œscoreå€¼æœ€å°çš„æˆå‘˜æ’åä¸º1ã€‚\r\nè¿™ç‚¹å’ŒRedisä¸åŒï¼ŒRedisæ˜¯ä»0å¼€å§‹çš„ã€‚\r\n\r\n```go\r\n\r\n// ZAdd\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet4\"\r\n        key1 := []byte(\"key1\")\r\n        return tx.ZAdd(bucket, key1, 70, []byte(\"val1\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet4\"\r\n        key2 := []byte(\"key2\")\r\n        return tx.ZAdd(bucket, key2, 90, []byte(\"val2\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet4\"\r\n        key3 := []byte(\"key3\")\r\n        return tx.ZAdd(bucket, key3, 86, []byte(\"val3\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n// ZRank\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet4\"\r\n        key1 := []byte(\"key1\")\r\n        if rank, err := tx.ZRank(bucket, key1); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"key1 ZRank :\", rank) // key1 ZRank : 1\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n#### ZRevRank\r\n\r\nè¿”å›æœ‰åºé›†bucketä¸­æˆå‘˜æŒ‡å®šæˆå‘˜keyçš„åå‘æ’åã€‚å…¶ä¸­æœ‰åºé›†æˆå‘˜è¿˜æ˜¯æŒ‰scoreå€¼é€’å¢(ä»å°åˆ°å¤§)é¡ºåºæ’åˆ—ã€‚ä½†æ˜¯è·å–åå‘æ’åï¼Œæ³¨æ„æ’åè¿˜æ˜¯ä»¥1ä¸ºå¼€å§‹ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œä½†æ˜¯è¿™ä¸ªæ—¶å€™scoreå€¼æœ€å¤§çš„æˆå‘˜æ’åä¸º1ã€‚\r\n\r\n```go\r\n// ZAdd\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet8\"\r\n        key1 := []byte(\"key1\")\r\n        return tx.ZAdd(bucket, key1, 10, []byte(\"val1\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet8\"\r\n        key2 := []byte(\"key2\")\r\n        return tx.ZAdd(bucket, key2, 20, []byte(\"val2\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet8\"\r\n        key3 := []byte(\"key3\")\r\n        return tx.ZAdd(bucket, key3, 30, []byte(\"val3\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n// ZRevRank\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet8\"\r\n        if rank, err := tx.ZRevRank(bucket, []byte(\"key3\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZRevRank key1 rank:\", rank) //ZRevRank key3 rank: 1\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### ZRem \r\n\r\nåˆ é™¤æŒ‡å®šæˆå‘˜keyåœ¨ä¸€ä¸ªæŒ‡å®šçš„æœ‰åºé›†åˆbucketä¸­ã€‚\r\n\r\n```go\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet5\"\r\n        key1 := []byte(\"key1\")\r\n        return tx.ZAdd(bucket, key1, 10, []byte(\"val1\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet5\"\r\n        key2 := []byte(\"key2\")\r\n        return tx.ZAdd(bucket, key2, 20, []byte(\"val2\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet5\"\r\n        if nodes,err := tx.ZMembers(bucket); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"before ZRem key1, ZMembers nodes\",nodes)\r\n            for _,node:=range nodes {\r\n                fmt.Println(\"item:\",node.Key(),node.Score())\r\n            }\r\n        }\r\n        // before ZRem key1, ZMembers nodes map[key1:0xc00008cfa0 key2:0xc00008d090]\r\n        // item: key1 10\r\n        // item: key2 20\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet5\"\r\n        if err := tx.ZRem(bucket, \"key1\"); err != nil {\r\n            return err\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet5\"\r\n        if nodes,err := tx.ZMembers(bucket); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"after ZRem key1, ZMembers nodes\",nodes)\r\n            for _,node:=range nodes {\r\n                fmt.Println(\"item:\",node.Key(),node.Score())\r\n            }\r\n            // after ZRem key1, ZMembers nodes map[key2:0xc00008d090]\r\n            // item: key2 20\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n```\r\n\r\n##### ZRemRangeByRank \r\n\r\nåˆ é™¤æ‰€æœ‰æˆå‘˜æ»¡è¶³æ’åstartåˆ°endï¼ˆåŒ…æ‹¬startå’Œendï¼‰åœ¨ä¸€ä¸ªæŒ‡å®šçš„æœ‰åºé›†åˆbucketä¸­ã€‚å…¶ä¸­æ’åä»¥1å¼€å§‹ï¼Œæ’å1è¡¨ç¤ºç¬¬ä¸€ä¸ªèŠ‚ç‚¹å…ƒç´ ï¼Œæ’å-1è¡¨ç¤ºæœ€åçš„èŠ‚ç‚¹å…ƒç´ ã€‚\r\n\r\n```go\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet6\"\r\n        key1 := []byte(\"key1\")\r\n        return tx.ZAdd(bucket, key1, 10, []byte(\"val1\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet6\"\r\n        key2 := []byte(\"key2\")\r\n        return tx.ZAdd(bucket, key2, 20, []byte(\"val2\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet6\"\r\n        key3 := []byte(\"key3\")\r\n        return tx.ZAdd(bucket, key3, 30, []byte(\"val2\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet6\"\r\n        if nodes,err := tx.ZMembers(bucket); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"before ZRemRangeByRank, ZMembers nodes\",nodes)\r\n            for _,node:=range nodes {\r\n                fmt.Println(\"item:\",node.Key(),node.Score())\r\n            }\r\n            // before ZRemRangeByRank, ZMembers nodes map[key3:0xc00008d450 key1:0xc00008d270 key2:0xc00008d360]\r\n            // item: key1 10\r\n            // item: key2 20\r\n            // item: key3 30\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet6\"\r\n        if err := tx.ZRemRangeByRank(bucket, 1,2); err != nil {\r\n            return err\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet6\"\r\n        if nodes,err := tx.ZMembers(bucket); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"after ZRemRangeByRank, ZMembers nodes\",nodes)\r\n            for _,node:=range nodes {\r\n                fmt.Println(\"item:\",node.Key(),node.Score())\r\n            }\r\n            // after ZRemRangeByRank, ZMembers nodes map[key3:0xc00008d450]\r\n            // item: key3 30\r\n            // key1 ZScore 10\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### ZScore\r\n\r\nè¿”å›æŒ‡å®šæœ‰åºé›†bucketä¸­ï¼Œæˆå‘˜keyçš„scoreå€¼ã€‚\r\n\r\n```go\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet7\"\r\n        if score,err := tx.ZScore(bucket, []byte(\"key1\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZScore key1 score:\",score)\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### ZKeys\r\n\r\næŸ¥æ‰¾`Sorted Set`ç±»å‹çš„æ‰€æœ‰åŒ¹é…æŒ‡å®šæ¨¡å¼`pattern`çš„`key`ï¼Œç±»ä¼¼äºRediså‘½ä»¤: [KEYS](https://redis.io/commands/keys/)\r\n\r\næ³¨æ„ï¼šæ¨¡å¼åŒ¹é…ä½¿ç”¨ Go æ ‡å‡†åº“çš„`filepath.Match`ï¼Œéƒ¨åˆ†ç»†èŠ‚ä¸Šå’Œredisçš„è¡Œä¸ºæœ‰åŒºåˆ«ï¼Œæ¯”å¦‚å¯¹äº `[` çš„å¤„ç†ã€‚\r\n\r\n```golang\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        var keys []string\r\n        err := tx.ZKeys(bucket, \"*\", func(key string) bool {\r\n            keys = append(keys, key)\r\n            // true: continue, false: break\r\n            return true\r\n        })\r\n        fmt.Printf(\"keys: %v\\n\", keys)\r\n        return err\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n### ä¸å…¶ä»–æ•°æ®åº“çš„æ¯”è¾ƒ\r\n\r\n#### BoltDB\r\n\r\nBoltDBå’ŒNutsDBå¾ˆç›¸ä¼¼éƒ½æ˜¯å†…åµŒå‹çš„key-valueæ•°æ®åº“ï¼ŒåŒæ—¶æ”¯æŒäº‹åŠ¡ã€‚BoltåŸºäºB+treeå¼•æ“æ¨¡å‹ï¼Œåªæœ‰ä¸€ä¸ªæ–‡ä»¶ï¼ŒNutsDBåŸºäºbitcaskå¼•æ“æ¨¡å‹ï¼Œä¼šç”Ÿæˆå¤šä¸ªæ–‡ä»¶ã€‚å½“ç„¶ä»–ä»¬éƒ½æ”¯æŒèŒƒå›´æ‰«æå’Œå‰ç¼€æ‰«æè¿™ä¸¤ä¸ªå®ç”¨çš„ç‰¹æ€§ã€‚\r\n\r\n#### LevelDB, RocksDB\r\n\r\nLevelDB å’Œ RocksDB éƒ½æ˜¯åŸºäºLSM treeæ¨¡å‹ã€‚ä¸æ”¯æŒbucketã€‚ å…¶ä¸­RocksDBç›®å‰è¿˜æ²¡çœ‹åˆ°golangå®ç°çš„ç‰ˆæœ¬ã€‚\r\n\r\n#### Badger\r\n\r\nBadgerä¹Ÿæ˜¯åŸºäºLSM treeæ¨¡å‹ã€‚ä½†æ˜¯å†™æ€§èƒ½æ²¡æœ‰æˆ‘æƒ³è±¡ä¸­é«˜ã€‚ä¸æ”¯æŒbucketã€‚\r\n\r\nå¦å¤–ï¼Œä»¥ä¸Šæ•°æ®åº“å‡ä¸æ”¯æŒå¤šç§æ•°æ®ç»“æ„å¦‚listã€setã€sorted setï¼Œè€ŒNutsDBä»0.2.0ç‰ˆæœ¬å¼€å§‹æ”¯æŒè¿™äº›æ•°æ®ç»“æ„ã€‚\r\n\r\n### Benchmarks\r\n\r\nä¸ºäº†ä¿è¯å°½å¯èƒ½å…¬å¹³ï¼Œæ‰¾äº†2æ¬¾å…³æ³¨åº¦å¾ˆé«˜çš„å†…åµŒå‹çš„kvstoreæ¥åšå¯¹æ¯”ï¼Œä»–ä»¬éƒ½æ”¯æŒäº‹åŠ¡ã€æ”¯æŒæŒä¹…åŒ–ã€‚\r\n\r\n* [BadgerDB](https://github.com/dgraph-io/badger) (masteråˆ†æ”¯å’Œé»˜è®¤é…ç½®)\r\n* [BoltDB](https://github.com/boltdb/bolt) (masteråˆ†æ”¯å’Œé»˜è®¤é…ç½®)\r\n* [NutsDB](https://github.com/nutsdb/nutsdb) (masteråˆ†æ”¯å’Œé»˜è®¤é…ç½®+è‡ªå®šä¹‰é…ç½®)\r\n\r\n## æµ‹è¯•çš„ç¯å¢ƒ:\r\n\r\n* Go Version : go1.11.4 darwin/amd64\r\n* OS: Mac OS X 10.13.6\r\n* Architecture: x86_64\r\n* 16 GB 2133 MHz LPDDR3\r\n* CPU: 3.1 GHz Intel Core i7\r\n\r\n\r\n##  Benchmarkçš„ç»“æœ:\r\n\r\n```\r\nbadger 2019/03/11 18:06:05 INFO: All 0 tables opened in 0s\r\ngoos: darwin\r\ngoarch: amd64\r\npkg: github.com/nutsdb/kvstore-bench\r\nBenchmarkBadgerDBPutValue64B-8         10000        112382 ns/op        2374 B/op         74 allocs/op\r\nBenchmarkBadgerDBPutValue128B-8        20000         94110 ns/op        2503 B/op         74 allocs/op\r\nBenchmarkBadgerDBPutValue256B-8        20000         93480 ns/op        2759 B/op         74 allocs/op\r\nBenchmarkBadgerDBPutValue512B-8        10000        101407 ns/op        3271 B/op         74 allocs/op\r\nBenchmarkBadgerDBGet-8               1000000          1552 ns/op         416 B/op          9 allocs/op\r\nBenchmarkBoltDBPutValue64B-8           10000        203128 ns/op       21231 B/op         62 allocs/op\r\nBenchmarkBoltDBPutValue128B-8           5000        229568 ns/op       13716 B/op         64 allocs/op\r\nBenchmarkBoltDBPutValue256B-8          10000        196513 ns/op       17974 B/op         64 allocs/op\r\nBenchmarkBoltDBPutValue512B-8          10000        199805 ns/op       17064 B/op         64 allocs/op\r\nBenchmarkBoltDBGet-8                 1000000          1122 ns/op         592 B/op         10 allocs/op\r\nBenchmarkNutsDBPutValue64B-8           30000         53614 ns/op         626 B/op         14 allocs/op\r\nBenchmarkNutsDBPutValue128B-8          30000         51998 ns/op         664 B/op         13 allocs/op\r\nBenchmarkNutsDBPutValue256B-8          30000         53958 ns/op         920 B/op         13 allocs/op\r\nBenchmarkNutsDBPutValue512B-8          30000         55787 ns/op        1432 B/op         13 allocs/op\r\nBenchmarkNutsDBGet-8                 2000000           661 ns/op          88 B/op          3 allocs/op\r\nBenchmarkNutsDBGetByHintKey-8          50000         27255 ns/op         840 B/op         16 allocs/op\r\nPASS\r\nok      github.com/nutsdb/kvstore-bench   83.856s\r\n```\r\n\r\n## ç»“è®º:\r\n\r\n### å†™æ€§èƒ½: \r\n\r\nNutsDBæœ€å¿«ã€‚ NutsDBæ¯”BoltDBå¿«2-5å€ , æ¯”BadgerDBå¿«0.5-2å€ã€‚\r\nBadgerDBæ¬¡ä¹‹ï¼Œä»–æ¯”BoltDBå¿«1-3å€ã€‚\r\nBoltDBæœ€æ…¢ã€‚\r\n\r\n### è¯»æ€§èƒ½: \r\n\r\né»˜è®¤æ¨¡å¼ä¸‹ï¼Œè¯»éƒ½å¾ˆå¿«ã€‚å…¶ä¸­NutsDBåœ¨é»˜è®¤é…ç½®ä¸‹æ¯”å…¶ä»–æ•°æ®åº“å¿«ä¸€å€ã€‚ä½†æ˜¯å¦‚æœä½¿ç”¨`HintKeyAndRAMIdxMode`çš„é€‰é¡¹ï¼Œè¯»é€Ÿåº¦æ¯”é»˜è®¤é…ç½®ä½å¾ˆå¤šã€‚é“ç†å¾ˆç®€å•ï¼Œé»˜è®¤é…ç½®æ˜¯å…¨å†…å­˜ç´¢å¼•ï¼Œä½†æ˜¯`HintKeyAndRAMIdxMode`çš„æ¨¡å¼ï¼Œæ˜¯å†…å­˜ç´¢å¼•+ç£ç›˜æ··åˆçš„æ–¹å¼ï¼Œä½†æ˜¯è¿™ä¸ªé€‰é¡¹æ¨¡å¼å¯ä»¥ä¿å­˜è¿œå¤§äºå†…å­˜çš„æ•°æ®ã€‚ç‰¹åˆ«æ˜¯valueè¿œå¤§äºkeyçš„åœºæ™¯æ•ˆæœæ›´æ˜æ˜¾ã€‚\r\n\r\n\r\n### è­¦å‘Šå’Œé™åˆ¶\r\n\r\n* å¯åŠ¨ç´¢å¼•æ¨¡å¼\r\n\r\nå½“å‰ç‰ˆæœ¬ä½¿ç”¨`HintKeyValAndRAMIdxMode`ã€ `HintKeyAndRAMIdxMode`å’Œ`HintBPTSparseIdxMode` è¿™ä¸‰ç§ä½œä¸ºdbå¯åŠ¨çš„æ—¶å€™ç´¢å¼•æ¨¡å¼ã€‚\r\né»˜è®¤ä½¿ç”¨`HintKeyValAndRAMIdxMode`ã€‚åœ¨åŸºæœ¬çš„åŠŸèƒ½çš„stringæ•°æ®ç±»å‹ï¼ˆputã€getã€deleteã€rangeScanã€PrefixScanï¼‰è¿™ä¸‰ç§æ¨¡å¼éƒ½æ”¯æŒã€‚`HintKeyValAndRAMIdxMode`ï¼Œä½œä¸ºæ•°æ®åº“é»˜è®¤é€‰é¡¹ï¼Œä»–æ˜¯å…¨å†…å­˜ç´¢å¼•ï¼Œè¯»å†™æ€§èƒ½éƒ½å¾ˆé«˜ã€‚ä»–çš„ç“¶é¢ˆåœ¨äºå†…å­˜ã€‚å¦‚æœä½ å†…å­˜å¤Ÿçš„è¯ï¼Œè¿™ç§é»˜è®¤æ˜¯é€‚åˆçš„ã€‚å¦ä¸€ç§æ¨¡å¼`HintKeyAndRAMIdxMode`ï¼Œä»–ä¼šæŠŠvalueå­˜ç£ç›˜ï¼Œé€šè¿‡ç´¢å¼•å»æ‰¾offsetï¼Œè¿™ç§æ¨¡å¼ç‰¹åˆ«é€‚åˆvalueè¿œå¤§äºkeyçš„åœºæ™¯ï¼Œä»–çš„è¯»æ€§èƒ½è¦æ¯”èµ·é»˜è®¤æ¨¡å¼è¦é™ä½ä¸å°‘ã€‚`HintBPTSparseIdxMode`è¿™ä¸ªæ¨¡å¼ï¼ˆv0.4.0ä¹‹åæ”¯æŒï¼‰è¿™ä¸ªæ¨¡å¼éå¸¸çœå†…å­˜ï¼Œä½¿ç”¨å¤šçº§ç´¢å¼•ï¼Œæµ‹è¯•10äº¿æ•°æ®ï¼Œåªå ç”¨80å‡ MBçš„å†…å­˜ï¼Œä½†æ˜¯è¯»æ€§èƒ½æ¯”è¾ƒå·®ï¼Œéœ€è¦è‡ªå·±åŠ ç¼“å­˜åŠ é€Ÿã€‚å…·ä½“çœ‹è‡ªå·±çš„è¦æ±‚é€‰æ‹©æ¨¡å¼ã€‚\r\n\r\nå…³äº**å…¶ä»–çš„æ•°æ®ç»“æ„ï¼ˆlist\\set\\sorted setï¼‰åªæ”¯æŒé»˜è®¤çš„HintKeyValAndRAMIdxModeã€‚è¯·æ ¹æ®éœ€è¦é€‰æ¨¡å¼**ã€‚**è¿˜æœ‰ä¸€ä¸ªå¯åŠ¨ç´¢å¼•æ¨¡å¼ä¸€æ—¦å¼€å¯ä¸è¦æ¥å›åˆ‡æ¢åˆ°å…¶ä»–æ¨¡å¼ï¼Œå› ä¸ºç´¢å¼•ç»“æ„ä¸ä¸€æ ·ï¼Œå¯èƒ½å¯¼è‡´æ•°æ®è¯»ä¸å‡ºæ¥**ã€‚ \r\n\r\n* Segmenté…ç½®é—®é¢˜\r\n\r\nNutsDBä¼šè‡ªåŠ¨åˆ‡å‰²åˆ†æˆä¸€ä¸ªä¸ªå—ï¼ˆSegmentï¼‰ï¼Œé»˜è®¤`SegmentSize`æ˜¯8MBï¼Œè¿™ä¸ªå‚æ•°å¯ä»¥è‡ªå·±éœ€è¦é…ç½®ï¼ˆæ¯”å¦‚16MBã€32MBã€64MBã€128MBã€512MBç­‰ï¼‰ï¼Œä½†æ˜¯**ä¸€æ—¦é…ç½®ä¸èƒ½ä¿®æ”¹**ã€‚\r\n\r\n* keyå’Œvalueçš„å¤§å°é™åˆ¶é—®é¢˜\r\n\r\nå…³äºkeyå’Œvalueçš„å¤§å°å—åˆ°SegmentSizeçš„å¤§å°çš„å½±å“ï¼Œæ¯”å¦‚SegmentSizeä¸º8Mï¼Œkeyå’Œvalueçš„å¤§å°è‚¯å®šæ˜¯å°äº8Mçš„ï¼Œä¸ç„¶ä¼šè¿”å›é”™è¯¯ã€‚\r\nåœ¨NutsDBé‡Œé¢entryæ˜¯æœ€å°å•ä½ï¼Œåªè¦ä¿è¯entryä¸å¤§äº`SegmentSize`å°±å¯ä»¥äº†ã€‚\r\n\r\n* entryçš„å¤§å°é—®é¢˜\r\n\r\nentryçš„çš„å¤§å°=EntryHeaderçš„å¤§å°+keyçš„å¤§å°+valueçš„å¤§å°+bucketçš„å¤§å°\r\n\r\n* å…³äºæ”¯æŒçš„æ“ä½œç³»ç»Ÿ\r\n\r\næ”¯æŒ Mac OS ã€Linux ã€Windows ä¸‰å¤§å¹³å°ã€‚\r\n\r\n* å…³äºåˆå¹¶æ“ä½œ\r\n\r\n`HintBPTSparseIdxMode` è¿™ä¸ªæ¨¡å¼åœ¨å½“å‰ç‰ˆæœ¬è¿˜æ²¡æœ‰æ”¯æŒã€‚\r\n\r\n* å…³äºäº‹åŠ¡è¯´æ˜\r\n\r\nåœ¨ä¼ ç»Ÿçš„å…³ç³»å¼æ•°æ®åº“ä¸­ï¼Œå¸¸å¸¸ç”¨ ACID æ€§è´¨æ¥æ£€éªŒäº‹åŠ¡åŠŸèƒ½çš„å®‰å…¨æ€§ï¼Œ~~NutsDBç›®å‰çš„ç‰ˆæœ¬å¹¶æ²¡æœ‰å®Œå…¨æ”¯æŒACIDã€‚~~ NutsDBä»v0.2.0ä¹‹åçš„ç‰ˆæœ¬å¼€å§‹å®Œå…¨æ”¯æŒACIDã€‚\r\n\r\nè¿™è¿™ç‰¹åˆ«æ„Ÿè°¢ @damnever ç»™æˆ‘æçš„[issue](https://github.com/nutsdb/nutsdb/issues/10)ç»™æˆ‘æŒ‡å‡ºï¼Œç‰¹åˆ«åœ¨è¿™è¯´æ˜ä¸‹ï¼Œå…å¾—è¯¯å¯¼å¤§å®¶ã€‚\r\n\r\nä»v0.3.0ç‰ˆæœ¬èµ·ï¼ŒNutsDBæ”¯æŒï¼ˆAï¼‰åŸå­æ€§ã€Cï¼ˆä¸€è‡´æ€§ï¼‰ã€Iï¼ˆéš”ç¦»æ€§ï¼‰ï¼Œå¹¶ä¿è¯ï¼ˆDï¼‰æŒä¹…åŒ–ã€‚ä»¥ä¸‹å‚è€ƒ[wikiç™¾ç§‘](https://zh.wikipedia.org/wiki/ACID)çš„å¯¹ACIDå®šä¹‰åˆ†åˆ«è®²ä¸€ä¸‹ã€‚å¦‚è®²çš„æœ‰è¯¯ï¼Œæ¬¢è¿å¸®æˆ‘æŒ‡æ­£ã€‚\r\n\r\n1ã€ï¼ˆAï¼‰åŸå­æ€§\r\n\r\næ‰€è°“åŸå­æ€§ï¼Œä¸€ä¸ªäº‹åŠ¡ï¼ˆtransactionï¼‰ä¸­çš„æ‰€æœ‰æ“ä½œï¼Œæˆ–è€…å…¨éƒ¨å®Œæˆï¼Œæˆ–è€…å…¨éƒ¨ä¸å®Œæˆï¼Œä¸ä¼šç»“æŸåœ¨ä¸­é—´æŸä¸ªç¯èŠ‚ã€‚å®ç°äº‹åŠ¡çš„åŸå­æ€§ï¼Œè¦æ”¯æŒå›æ»šæ“ä½œï¼Œåœ¨æŸä¸ªæ“ä½œå¤±è´¥åï¼Œå›æ»šåˆ°äº‹åŠ¡æ‰§è¡Œä¹‹å‰çš„çŠ¶æ€ã€‚ä¸€èˆ¬çš„åšæ³•æ˜¯ç±»ä¼¼æ•°æ®å¿«ç…§çš„æ–¹æ¡ˆã€‚å…³äºè¿™ä¸€ç‚¹ï¼ŒNutsDBæ”¯æŒå›æ»šæ“ä½œã€‚NutsDBçš„ä½œæ³•æ˜¯å…ˆå®é™…é¢„æ¼”ä¸€è¾¹æ‰€æœ‰è¦æ‰§è¡Œçš„æ“ä½œï¼Œè¿™ä¸ªæ—¶å€™æ•°æ®å…¶å®è¿˜æ˜¯uncommittedçŠ¶æ€ï¼Œä¸€ç›´åˆ°æ‰€æœ‰ç¯èŠ‚éƒ½æ²¡æœ‰é—®é¢˜ï¼Œæ‰ä¼šä½œcommitæ“ä½œï¼Œå¦‚æœä¸­é—´ä»»ä½•ç¯èŠ‚ä¸€æ—¦å‘ç”Ÿé”™è¯¯ï¼Œç›´æ¥ä½œrollbackå›æ»šæ“ä½œï¼Œä¿è¯åŸå­æ€§ã€‚ å°±ç®—å‘ç”Ÿé”™è¯¯çš„æ—¶å€™å·²ç»æœ‰æ•°æ®è¿›ç£ç›˜ï¼Œä¸‹æ¬¡å¯åŠ¨ä¹Ÿä¸ä¼šè¢«ç´¢å¼•åˆ°è¿™äº›æ•°æ®ã€‚\r\n\r\n2ã€ï¼ˆCï¼‰ä¸€è‡´æ€§\r\n\r\nåœ¨äº‹åŠ¡å¼€å§‹ä¹‹å‰å’Œäº‹åŠ¡ç»“æŸä»¥åï¼Œæ•°æ®åº“çš„å®Œæ•´æ€§æ²¡æœ‰è¢«ç ´åã€‚è¿™è¡¨ç¤ºå†™å…¥çš„æ•°æ®å¿…é¡»å®Œå…¨ç¬¦åˆé¢„æœŸçš„ã€‚NutsDBåŸºäºè¯»å†™é”å®ç°é”æœºåˆ¶ï¼Œåœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹ï¼Œä¸€ä¸ªè¯»å†™äº‹åŠ¡å…·æœ‰æ’ä»–æ€§çš„ï¼Œæ¯”å¦‚ä¸€ä¸ªgoroutineéœ€è¦æ‰§è¡Œä¸€ä¸ªè¯»å†™äº‹åŠ¡ï¼Œå…¶ä»–ä¸ç®¡æƒ³è¦è¯»å†™çš„äº‹åŠ¡æˆ–è€…åªè¯»çš„åªèƒ½ç­‰å¾…ï¼Œç›´åˆ°è¿™ä¸ªé”é‡Šæ”¾ä¸ºæ­¢ã€‚ä¿è¯äº†æ•°æ®çš„ä¸€è‡´æ€§ã€‚æ‰€ä»¥è¿™ä¸€ç‚¹NutsDBæ»¡è¶³ä¸€è‡´æ€§ã€‚\r\n\r\n3ã€ï¼ˆIï¼‰éš”ç¦»æ€§\r\n\r\næ•°æ®åº“å…è®¸å¤šä¸ªå¹¶å‘äº‹åŠ¡åŒæ—¶å¯¹å…¶æ•°æ®è¿›è¡Œè¯»å†™å’Œä¿®æ”¹çš„èƒ½åŠ›ï¼Œéš”ç¦»æ€§å¯ä»¥é˜²æ­¢å¤šä¸ªäº‹åŠ¡å¹¶å‘æ‰§è¡Œæ—¶ç”±äºäº¤å‰æ‰§è¡Œè€Œå¯¼è‡´æ•°æ®çš„ä¸ä¸€è‡´ã€‚å¦‚ä¸Šé¢çš„ä¸€è‡´æ€§æ‰€è¯´ï¼ŒNutsDBåŸºäºè¯»å†™é”å®ç°é”æœºåˆ¶ã€‚ä¸ä¼šå‡ºç°æ•°æ®ä¸²çš„æƒ…å†µã€‚æ‰€ä»¥ä¹Ÿæ˜¯æ»¡è¶³éš”ç¦»æ€§çš„ã€‚\r\n\r\nå…³äºäº‹åŠ¡çš„éš”ç¦»çº§åˆ«ï¼Œæˆ‘ä»¬ä¹Ÿæ¥å¯¹ç…§[wikiç™¾ç§‘](https://zh.wikipedia.org/wiki/%E4%BA%8B%E5%8B%99%E9%9A%94%E9%9B%A2)ï¼Œæ¥çœ‹ä¸‹NutsDBå±äºå“ªä¸€ä¸ªçº§åˆ«ï¼š\r\n\r\n#### éš”ç¦»çº§åˆ«ä½åˆ°é«˜ï¼š\r\n\r\n##### 1ï¼‰æœªæäº¤è¯»ï¼ˆREAD UNCOMMITTEDï¼‰\r\n\r\nè¿™ä¸ªæ˜¯æœ€ä½çš„éš”ç¦»çº§åˆ«ã€‚å…è®¸â€œè„è¯»â€ï¼ˆdirty readsï¼‰ï¼Œäº‹åŠ¡å¯ä»¥çœ‹åˆ°å…¶ä»–äº‹åŠ¡â€œå°šæœªæäº¤â€çš„ä¿®æ”¹ã€‚å¾ˆæ˜æ˜¾nutsDBæ˜¯é¿å…è„è¯»çš„ã€‚\r\n\r\n##### 2ï¼‰åœ¨æäº¤è¯»ï¼ˆREAD COMMITTEDï¼‰\r\n\r\nå®šä¹‰ï¼šè¿™ä¸ªéš”ç¦»çº§åˆ«ä¸­ï¼ŒåŸºäºé”æœºåˆ¶å¹¶å‘æ§åˆ¶çš„DBMSéœ€è¦å¯¹é€‰å®šå¯¹è±¡çš„å†™é”ä¸€ç›´ä¿æŒåˆ°äº‹åŠ¡ç»“æŸï¼Œä½†æ˜¯è¯»é”åœ¨SELECTæ“ä½œå®Œæˆåé©¬ä¸Šé‡Šæ”¾ï¼ˆå› æ­¤â€œä¸å¯é‡å¤è¯»â€ç°è±¡å¯èƒ½ä¼šå‘ç”Ÿï¼‰ã€‚\r\nçœ‹ä¸‹â€œä¸å¯é‡å¤è¯»â€çš„å®šä¹‰ï¼šåœ¨ä¸€æ¬¡äº‹åŠ¡ä¸­ï¼Œå½“ä¸€è¡Œæ•°æ®è·å–ä¸¤éå¾—åˆ°ä¸åŒçš„ç»“æœè¡¨ç¤ºå‘ç”Ÿäº†â€œä¸å¯é‡å¤è¯»â€ã€‚\r\n\r\nnutsDBä¸ä¼šå‡ºç°â€œä¸å¯é‡å¤è¯»â€è¿™ç§æƒ…å†µï¼Œå½“é«˜å¹¶å‘çš„æ—¶å€™ï¼Œæ­£åœ¨è¿›è¡Œè¯»å†™æ“ä½œï¼Œä¸€ä¸ªgoroutineåˆšå¥½å…ˆæ‹¿åˆ°åªè¯»é”ï¼Œè¿™ä¸ªæ—¶å€™è¦å®Œæˆä¸€ä¸ªè¯»å†™äº‹åŠ¡æ“ä½œçš„é‚£ä¸ªgoroutineè¦é˜»å¡ç­‰åˆ°åªè¯»é”é‡Šæ”¾ä¸ºæ­¢ã€‚ä¹Ÿå°±é¿å…ä¸Šé¢çš„é—®é¢˜ã€‚\r\n\r\n##### 3ï¼‰åœ¨å¯é‡å¤è¯»ï¼ˆREPEATABLE READSï¼‰\r\n\r\nå®šä¹‰ï¼šè¿™ä¸ªéš”ç¦»çº§åˆ«ä¸­ï¼ŒåŸºäºé”æœºåˆ¶å¹¶å‘æ§åˆ¶çš„DBMSéœ€è¦å¯¹é€‰å®šå¯¹è±¡çš„è¯»é”ï¼ˆread locksï¼‰å’Œå†™é”ï¼ˆwrite locksï¼‰ä¸€ç›´ä¿æŒåˆ°äº‹åŠ¡ç»“æŸï¼Œä½†ä¸è¦æ±‚â€œèŒƒå›´é”â€ï¼Œå› æ­¤å¯èƒ½ä¼šå‘ç”Ÿâ€œå¹»å½±è¯»â€ã€‚\r\n\r\nå…³äºå¹»å½±è¯»å®šä¹‰ï¼ŒæŒ‡åœ¨äº‹åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œå½“ä¸¤ä¸ªå®Œå…¨ç›¸åŒçš„æŸ¥è¯¢è¯­å¥æ‰§è¡Œå¾—åˆ°ä¸åŒçš„ç»“æœé›†ã€‚è¿™ç§ç°è±¡ç§°ä¸ºâ€œå¹»å½±è¯»ï¼ˆphantom readï¼‰â€ï¼Œæœ‰äº›äººä¹Ÿå«ä»–å¹»è¯»ï¼Œæ­£å¦‚ä¸Šé¢æ‰€è¯´ï¼Œåœ¨nutsDBä¸­ï¼Œå½“è¿›è¡Œåªè¯»æ“ä½œçš„æ—¶å€™ï¼ŒåŒä¸€æ—¶é—´åªèƒ½å¹¶å‘åªè¯»æ“ä½œï¼Œå…¶ä»–æœ‰å…³â€œå†™â€çš„äº‹åŠ¡æ˜¯è¢«é˜»å¡çš„ï¼Œç›´åˆ°è¿™äº›åªè¯»é”é‡Šæ”¾ä¸ºæ­¢ï¼Œå› æ­¤ä¸ä¼šå‡ºç°â€œå¹»å½±è¯»â€çš„æƒ…å†µã€‚\r\n\r\n##### 4ï¼‰å¯ä¸²è¡ŒåŒ– ï¼ˆSerializableï¼‰\r\n\r\nå®šä¹‰ï¼šè¿™ä¸ªéš”ç¦»çº§åˆ«æ˜¯æœ€é«˜çš„ã€‚é¿å…äº†æ‰€æœ‰ä¸Šé¢çš„â€œè„è¯»â€ã€ä¸å¯é‡å¤è¯»â€ã€â€œå¹»å½±è¯»â€ç°è±¡ã€‚\r\n\r\nåœ¨nutsDBä¸­ï¼Œä¸€ä¸ªåªè¯»äº‹åŠ¡å’Œä¸€ä¸ªå†™ï¼ˆè¯»å†™ï¼‰äº‹åŠ¡ï¼Œæ˜¯äº’æ–¥çš„ï¼Œéœ€è¦ä¸²è¡Œæ‰§è¡Œï¼Œä¸ä¼šå‡ºç°å¹¶å‘æ‰§è¡Œã€‚nutsDBå±äºè¿™ä¸ªå¯ä¸²è¡ŒåŒ–çº§åˆ«ã€‚\r\nè¿™ä¸ªçº§åˆ«çš„éš”ç¦»ä¸€èˆ¬æ¥è¯´åœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹æ€§èƒ½ä¼šå—åˆ°å½±å“ã€‚ä½†æ˜¯å¦‚æœé”æœ¬èº«æ€§èƒ½è¿˜å¯ä»¥ï¼Œä¹Ÿä¸å¤±ä¸ºä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ–¹æ³•ã€‚å½“å‰ç‰ˆæœ¬nutsDBåŸºäºè¯»å†™é”ï¼Œåœ¨å¹¶å‘è¯»å¤šå†™å°‘çš„åœºæ™¯ä¸‹ï¼Œæ€§èƒ½ä¼šå¥½ä¸€ç‚¹ã€‚\r\n\r\n\r\n4ã€ï¼ˆDï¼‰æŒä¹…åŒ–\r\n\r\näº‹åŠ¡å¤„ç†ç»“æŸåï¼Œå¯¹æ•°æ®çš„ä¿®æ”¹å°±æ˜¯æ°¸ä¹…çš„ï¼Œå³ä¾¿ç³»ç»Ÿæ•…éšœä¹Ÿä¸ä¼šä¸¢å¤±ã€‚v0.3.0ä¹‹å‰ç‰ˆæœ¬çš„nutsdbä¸ºäº†æä¾›é«˜æ€§èƒ½çš„å†™å…¥ï¼Œå¹¶æ²¡æœ‰å®æ—¶çš„åšsyncæ“ä½œã€‚ä»v0.3.0å¼€å§‹ä½¿ç”¨syncæ“ä½œä½œå¼ºåˆ¶åŒæ­¥ï¼Œå¼€å§‹æ”¯æŒæŒä¹…åŒ–ï¼Œå»ºè®®ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬ã€‚\r\n\r\n\r\nå…³ä¸å…¶ä»–ä¿¡æ¯å¾…è¡¥å……ã€‚æœ‰é”™è¯¯è¯·å¸®å¿™æŒ‡å‡ºï¼Œæç»™æˆ‘issueï¼Œè°¢è°¢ã€‚\r\n\r\n### è”ç³»ä½œè€…\r\n\r\n* [nutsdb](https://github.com/nutsdb)\r\n\r\n### å‚ä¸è´¡çŒ®\r\n\r\n:+1::tada: é¦–å…ˆæ„Ÿè°¢ä½ èƒ½çœ‹åˆ°è¿™é‡Œï¼Œå‚ä¸è´¡çŒ® :tada::+1:\r\n\r\nå‚ä¸è´¡çŒ®æ–¹å¼ä¸é™äºï¼š\r\n\r\n* æå„ç§issuesï¼ˆåŒ…æ‹¬è¯¢é—®é—®é¢˜ã€æåŠŸèƒ½å»ºè®®ã€æ€§èƒ½å»ºè®®ç­‰ï¼‰\r\n* æäº¤bug\r\n* æpull requests\r\n* ä¼˜åŒ–ä¿®æ”¹READMEæ–‡æ¡£\r\n\r\næ„Ÿè°¢ä»¥ä¸‹è´¡çŒ®è€…ï¼Œæ„Ÿè°¢ä½ ä»¬çš„ä»˜å‡ºï¼\r\n\r\n<a href=\"https://github.com/nutsdb/nutsdb/graphs/contributors\">\r\n<img src=\"https://contrib.rocks/image?repo=nutsdb/nutsdb\" />\r\n</a>\r\n\r\n#### ä»£ç é£æ ¼æŒ‡å—å‚è€ƒ\r\n\r\nhttps://github.com/golang/go/wiki/CodeReviewComments\r\n\r\n#### git commit è§„èŒƒå‚è€ƒ\r\n\r\n commit messageæ ¼å¼\r\n\r\n ```\r\n <type>(<scope>): <subject>\r\n ```\r\n\r\n\r\n####  typeçš„å‚è€ƒ\r\n\r\n![image](https://user-images.githubusercontent.com/6065007/162549766-58f164df-3794-4a5a-ab25-dd47962de74e.png)\r\n\r\n \r\n\r\nè¯¦æƒ…å‚è€ƒè‹±æ–‡ç‰ˆçš„ [CONTRIBUTING](https://github.com/nutsdb/nutsdb/blob/master/CONTRIBUTING.md) ã€‚\r\n\r\n### è‡´è°¢\r\n\r\nè¿™ä¸ªé¡¹ç›®å—åˆ°ä»¥ä¸‹é¡¹ç›®æˆ–å¤šæˆ–å°‘çš„çµæ„Ÿå’Œå¸®åŠ©ï¼š\r\n\r\n* [Bitcask-intro](https://github.com/basho/bitcask/blob/develop/doc/bitcask-intro.pdf)\r\n* [BoltDB](https://github.com/boltdb)\r\n* [BuntDB](https://github.com/tidwall/buntdb)\r\n* [Redis](https://redis.io)\r\n* [Sorted Set](https://github.com/wangjia184/sortedset)\r\n\r\n### License\r\n\r\nThe NutsDB is open-sourced software licensed under the [Apache 2.0 license](https://github.com/nutsdb/nutsdb/blob/master/LICENSE).\r\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.0244140625,
          "content": "<p align=\"center\">\n    <img src=\"https://user-images.githubusercontent.com/6065007/141310364-62d7eebb-2cbb-4949-80ed-5cd20f705405.png\">\n</p>\n\n<div class=\"column\" align=\"middle\">\n  <a href=\"https://godoc.org/github.com/nutsdb/nutsdb\"><img src=\"https://godoc.org/github.com/nutsdb/nutsdb?status.svg\" /></a>\n  <a href=\"https://goreportcard.com/report/github.com/nutsdb/nutsdb\"><img src=\"https://goreportcard.com/badge/github.com/nutsdb/nutsdb\" /></a>\n  <a href=\"https://goreportcard.com/report/github.com/nutsdb/nutsdb\"><img src=\"https://github.com/nutsdb/nutsdb/workflows/Go/badge.svg?branch=master\"/></a>\n  <a href=\"https://codecov.io/gh/nutsdb/nutsdb\"><img src=\"https://codecov.io/gh/nutsdb/nutsdb/branch/master/graph/badge.svg?token=CupujOXpbe\"/></a>\n  <a href=\"https://raw.githubusercontent.com/nutsdb/nutsdb/master/LICENSE\"><img src=\"http://img.shields.io/badge/license-Apache_2-blue.svg?style=flat-square\"/></a>\n  <a href=\"https://github.com/avelino/awesome-go#database\"><img src=\"https://awesome.re/mentioned-badge.svg\"/></a>\n</div>\n\n## What is NutsDB?\n\nEnglish | [ç®€ä½“ä¸­æ–‡](https://github.com/nutsdb/nutsdb/blob/master/README-CN.md)\n\nNutsDB is a simple, fast, embeddable and persistent key/value store written in pure Go.\n\nIt supports fully serializable transactions and many data structures such as listã€setã€sorted set. All operations happen inside a Tx. Tx represents a transaction, which can be read-only or read-write. Read-only transactions can read values for a given bucket and a given key or iterate over a set of key-value pairs. Read-write transactions can read, update and delete keys from the DB.\n\nWe can learn more about NutsDB in details on the documents site of NutsDB: [NutsDB Documents](https://nutsdb.github.io/nutsdb-docs/)\n\n## Announcement\n\n* v1.0.0 release, see for details: [https://github.com/nutsdb/nutsdb/releases/tag/v1.0.0](https://github.com/nutsdb/nutsdb/releases/tag/v1.0.0)\n* v0.14.3 release, see for details: [https://github.com/nutsdb/nutsdb/releases/tag/v0.14.3](https://github.com/nutsdb/nutsdb/releases/tag/v0.14.3)\n* v0.14.2 release, see for details: [https://github.com/nutsdb/nutsdb/releases/tag/v0.14.2](https://github.com/nutsdb/nutsdb/releases/tag/v0.14.2)\n* v0.14.1 release, see for details: [https://github.com/nutsdb/nutsdb/releases/tag/v0.14.1](https://github.com/nutsdb/nutsdb/releases/tag/v0.14.1)\n\nğŸ“¢ Note: Starting from v0.9.0, **defaultSegmentSize** in **DefaultOptions** has been adjusted from **8MB** to **256MB**. The original value is the default value, which needs to be manually changed to 8MB, otherwise the original data will not be parsed. The reason for the size adjustment here is that there is a cache for file descriptors starting from v0.9.0 (detail see https://github.com/nutsdb/nutsdb/pull/164 ), so users need to look at the number of fds they use on the server, which can be set manually. If you have any questions, you can open an issue.\n\nAfter **nutsdb v1.0.0**, due to changes in the underlying data storage protocol, **the data of the old version is not compatible**. Please rewrite it before using the new version. And the current Bucket needs to be created manually. Please see the Bucket usage [documentation](./docs/user_guides/use-buckets.md) for details.\n\n## Architecture\n![nutsdb-æ¶æ„å›¾](./docs/img/nutsdb-æ¶æ„å›¾.png)\n\n\n Welcome [contributions to NutsDB](https://github.com/nutsdb/nutsdb#contributing).\n\n## Quick start\n\n### Install NutsDB\n\nTo start using NutsDB, first needs [Go](https://golang.org/dl/) installed (version 1.18+ is required).  and run go get:\n\n```\ngo get -u github.com/nutsdb/nutsdb\n```\n\n### Opening a database\n\nTo open your database, use the nutsdb.Open() function,with the appropriate options.The `Dir` , `EntryIdxMode`  and  `SegmentSize`  options are must be specified by the client. About options see [here](https://github.com/nutsdb/nutsdb#options) for detail.\n\n```go\npackage main\n\nimport (\n    \"log\"\n\n    \"github.com/nutsdb/nutsdb\"\n)\n\nfunc main() {\n    // Open the database located in the /tmp/nutsdb directory.\n    // It will be created if it doesn't exist.\n    db, err := nutsdb.Open(\n        nutsdb.DefaultOptions,\n        nutsdb.WithDir(\"/tmp/nutsdb\"),\n    )\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer db.Close()\n\n    ...\n}\n```\n\n## Documentation\n\n<details>\n  <summary><b>Buckets</b></summary>\n\n- [Using buckets](./docs/user_guides/use-buckets.md)\n</details>\n\n<details>\n  <summary><b>Pairs</b></summary>\n\n- [Using key/value pairs](./docs/user_guides/use-kv-pair.md)\n</details>\n\n<details>\n  <summary><b>Iterator</b></summary>\n\n- [Iterating over keys](./docs/user_guides/iterator.md)\n</details>\n\n<details>\n  <summary><b>Data Structures</b></summary>\n\n- [List](./docs/user_guides/data-structure.md#list)\n- [Set](./docs/user_guides/data-structure.md#set)\n- [Sorted Set](./docs/user_guides/data-structure.md#sorted-set)\n</details>\n\n<details>\n  <summary><b>Database Options</b></summary>\n\n- [Options](./docs/user_guides/options.md)\n</details>\n\n<details>\n  <summary><b>More Operation</b></summary>\n\n- [More Operation](./docs/user_guides/others.md)\n</details>\n\n<details>\n  <summary><b>Comparison</b></summary>\n\n- [Comparison](./docs/user_guides/comparison.md)\n</details>\n\n<details>\n  <summary><b>Benchmark</b></summary>\n\n- [Benchmark](./docs/user_guides/benchmarks.md)\n</details>\n\n## Contributors\n\nThank you for considering contributing to NutsDB! The contribution guide can be found in the [CONTRIBUTING](https://github.com/nutsdb/nutsdb/blob/master/CONTRIBUTING.md) for details on submitting patches and the contribution workflow.\n\n<a href=\"https://github.com/nutsdb/nutsdb/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=nutsdb/nutsdb\" />\n</a>\n\n## Acknowledgements\n\nThis package is inspired by the following:\n\n- [Bitcask-intro](https://github.com/basho/bitcask/blob/develop/doc/bitcask-intro.pdf)\n- [BoltDB](https://github.com/boltdb)\n- [BuntDB](https://github.com/tidwall/buntdb)\n- [Redis](https://redis.io/)\n- [Sorted Set](https://github.com/wangjia184/sortedset)\n\n## License\n\nThe NutsDB is open-sourced software licensed under the [Apache 2.0 license](https://github.com/nutsdb/nutsdb/blob/master/LICENSE).\n"
        },
        {
          "name": "_config.yml",
          "type": "blob",
          "size": 0.025390625,
          "content": "theme: jekyll-theme-cayman"
        },
        {
          "name": "_typos.toml",
          "type": "blob",
          "size": 0.1826171875,
          "content": "# Typo check: https://github.com/crate-ci/typos\n\n[files]\nextend-exclude = [\"*.json\", \"*.js\", \"static/*\", \"layouts/*\", \"assets/*\", \"i18n/*\", \"go.*\"]\n\n[default.extend-words]\ndatas = \"datas\""
        },
        {
          "name": "batch.go",
          "type": "blob",
          "size": 3.36328125,
          "content": "package nutsdb\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"sync\"\n\t\"sync/atomic\"\n)\n\n// ErrCommitAfterFinish indicates that write batch commit was called after\nvar ErrCommitAfterFinish = errors.New(\"Batch commit not permitted after finish\")\n\nconst (\n\tDefaultThrottleSize = 16\n)\n\n// WriteBatch holds the necessary info to perform batched writes.\ntype WriteBatch struct {\n\tsync.Mutex\n\ttx       *Tx\n\tdb       *DB\n\tthrottle *Throttle\n\terr      atomic.Value\n\tfinished bool\n}\n\nfunc (db *DB) NewWriteBatch() (*WriteBatch, error) {\n\twb := &WriteBatch{\n\t\tdb:       db,\n\t\tthrottle: NewThrottle(DefaultThrottleSize),\n\t}\n\n\tvar err error\n\twb.tx, err = newTx(db, true)\n\treturn wb, err\n}\n\n// SetMaxPendingTxns sets a limit on maximum number of pending transactions while writing batches.\n// This function should be called before using WriteBatch. Default value of MaxPendingTxns is\n// 16 to minimise memory usage.\nfunc (wb *WriteBatch) SetMaxPendingTxns(max int) {\n\twb.throttle = NewThrottle(max)\n}\n\nfunc (wb *WriteBatch) Cancel() error {\n\twb.Lock()\n\twb.finished = true\n\twb.tx.setStatusClosed()\n\twb.Unlock()\n\n\tif err := wb.throttle.Finish(); err != nil {\n\t\treturn fmt.Errorf(\"WatchBatch.Cancel error while finishing: %v\", err)\n\t}\n\n\treturn nil\n}\n\nfunc (wb *WriteBatch) Put(bucket string, key, value []byte, ttl uint32) error {\n\twb.Lock()\n\tdefer wb.Unlock()\n\n\twb.tx.lock()\n\terr := wb.tx.Put(bucket, key, value, ttl)\n\tif err != nil {\n\t\twb.tx.unlock()\n\t\treturn err\n\t}\n\n\tif nil == wb.tx.checkSize() {\n\t\twb.tx.unlock()\n\t\treturn err\n\t}\n\n\twb.tx.unlock()\n\tif cerr := wb.commit(); cerr != nil {\n\t\treturn cerr\n\t}\n\n\treturn err\n}\n\n// func (tx *Tx) Delete(bucket string, key []byte) error\nfunc (wb *WriteBatch) Delete(bucket string, key []byte) error {\n\twb.Lock()\n\tdefer wb.Unlock()\n\n\twb.tx.lock()\n\terr := wb.tx.Delete(bucket, key)\n\tif err != nil {\n\t\twb.tx.unlock()\n\t\treturn err\n\t}\n\n\tif nil == wb.tx.checkSize() {\n\t\twb.tx.unlock()\n\t\treturn err\n\t}\n\n\twb.tx.unlock()\n\tif cerr := wb.commit(); cerr != nil {\n\t\treturn cerr\n\t}\n\n\treturn err\n}\n\nfunc (wb *WriteBatch) commit() error {\n\tif err := wb.Error(); err != nil {\n\t\treturn err\n\t}\n\n\tif wb.finished {\n\t\treturn ErrCommitAfterFinish\n\t}\n\n\tif err := wb.throttle.Do(); err != nil {\n\t\twb.err.Store(err)\n\t\treturn err\n\t}\n\n\twb.tx.CommitWith(wb.callback)\n\n\t// new a new tx\n\tvar err error\n\twb.tx, err = newTx(wb.db, true)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn wb.Error()\n}\n\nfunc (wb *WriteBatch) callback(err error) {\n\t// sync.WaitGroup is thread-safe, so it doesn't need to be run inside wb.Lock.\n\tdefer wb.throttle.Done(err)\n\tif err == nil {\n\t\treturn\n\t}\n\tif err := wb.Error(); err != nil {\n\t\treturn\n\t}\n\n\twb.err.Store(err)\n}\n\nfunc (wb *WriteBatch) Flush() error {\n\twb.Lock()\n\terr := wb.commit()\n\tif err != nil {\n\t\twb.Unlock()\n\t\treturn err\n\t}\n\twb.finished = true\n\twb.tx.setStatusClosed()\n\twb.Unlock()\n\tif err := wb.throttle.Finish(); err != nil {\n\t\tif wb.Error() != nil {\n\t\t\treturn fmt.Errorf(\"wb.err: %s err: %s\", wb.Error(), err)\n\t\t}\n\t\treturn err\n\t}\n\n\treturn wb.Error()\n}\n\nfunc (wb *WriteBatch) Reset() error {\n\twb.Lock()\n\tdefer wb.Unlock()\n\tvar err error\n\twb.finished = false\n\twb.tx, err = newTx(wb.db, true)\n\tif err != nil {\n\t\treturn err\n\t}\n\twb.throttle = NewThrottle(DefaultThrottleSize)\n\treturn err\n}\n\n// Error returns any errors encountered so far. No commits would be run once an error is detected.\nfunc (wb *WriteBatch) Error() error {\n\t// If the interface conversion fails, the err will be nil.\n\terr, _ := wb.err.Load().(error)\n\treturn err\n}\n"
        },
        {
          "name": "batch_test.go",
          "type": "blob",
          "size": 4.470703125,
          "content": "package nutsdb\n\nimport (\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"log\"\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n\t\"github.com/xujiajun/utils/time2\"\n)\n\nvar bucket string\n\nconst (\n\tfileDir  = \"/tmp/nutsdb/\"\n\tfileDir1 = \"/tmp/nutsdb/nutsdb_batch_write1\"\n\tfileDir2 = \"/tmp/nutsdb/nutsdb_batch_write2\"\n\tfileDir3 = \"/tmp/nutsdb/nutsdb_batch_write3\"\n\tfileDir4 = \"/tmp/nutsdb/nutsdb_batch_write4\"\n\tfileDir5 = \"/tmp/nutsdb/nutsdb_batch_write5\"\n\tfileDir6 = \"/tmp/nutsdb/nutsdb_batch_write6\"\n\tfileDir7 = \"/tmp/nutsdb/nutsdb_batch_write7\"\n\tfileDir8 = \"/tmp/nutsdb/nutsdb_batch_write8\"\n\tfileDir9 = \"/tmp/nutsdb/nutsdb_batch_write9\"\n\tN        = 100\n)\n\nfunc init() {\n\tbucket = \"bucketForBatchWrite\"\n\tfiles, err := ioutil.ReadDir(fileDir)\n\tif err != nil {\n\t\treturn\n\t}\n\tfor _, f := range files {\n\t\tname := f.Name()\n\t\tif name != \"\" {\n\t\t\tfilePath := fmt.Sprintf(\"%s/%s\", fileDir, name)\n\t\t\terr := os.RemoveAll(filePath)\n\t\t\tif err != nil {\n\t\t\t\tlog.Fatal(err)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestBatchWrite(t *testing.T) {\n\tTestFlushPanic := func(t *testing.T, db *DB) {\n\t\twb, err := db.NewWriteBatch()\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, wb.Flush())\n\t\trequire.Error(t, ErrCommitAfterFinish, wb.Flush())\n\t\twb, err = db.NewWriteBatch()\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, wb.Cancel())\n\t\trequire.Error(t, ErrCommitAfterFinish, wb.Flush())\n\t}\n\n\ttestEmptyWrite := func(t *testing.T, db *DB) {\n\t\twb, err := db.NewWriteBatch()\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, wb.Flush())\n\t\twb, err = db.NewWriteBatch()\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, wb.Flush())\n\t\twb, err = db.NewWriteBatch()\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, wb.Flush())\n\t}\n\n\ttestWrite := func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\tkey := func(i int) []byte {\n\t\t\treturn []byte(fmt.Sprintf(\"%10d\", i))\n\t\t}\n\t\tval := func(i int) []byte {\n\t\t\treturn []byte(fmt.Sprintf(\"%128d\", i))\n\t\t}\n\t\twb, err := db.NewWriteBatch()\n\t\trequire.NoError(t, err)\n\t\ttime2.Start()\n\t\tfor i := 0; i < N; i++ {\n\t\t\trequire.NoError(t, wb.Put(bucket, key(i), val(i), 0))\n\t\t}\n\t\trequire.NoError(t, wb.Flush())\n\t\t// fmt.Printf(\"Time taken via batch write %v keys: %v\\n\", N, time2.End())\n\n\t\ttime2.Start()\n\t\tif err := db.View(\n\t\t\tfunc(tx *Tx) error {\n\t\t\t\tfor i := 0; i < N; i++ {\n\t\t\t\t\tkey := key(i)\n\t\t\t\t\tvalue, err := tx.Get(bucket, key)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\t\trequire.Equal(t, val(i), value)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t}); err != nil {\n\t\t\tlog.Println(err)\n\t\t}\n\n\t\t// fmt.Printf(\"Time taken via db.View %v keys: %v\\n\", N, time2.End())\n\t\t// err = wb.Reset()\n\t\twb, err = db.NewWriteBatch()\n\t\trequire.NoError(t, err)\n\t\ttime2.Start()\n\t\tfor i := 0; i < N; i++ {\n\t\t\trequire.NoError(t, wb.Delete(bucket, key(i)))\n\t\t}\n\t\trequire.NoError(t, wb.Flush())\n\t\t// fmt.Printf(\"Time taken via batch delete %v keys: %v\\n\", N, time2.End())\n\n\t\tif err := db.View(\n\t\t\tfunc(tx *Tx) error {\n\t\t\t\tfor i := 0; i < N; i++ {\n\t\t\t\t\tkey := key(i)\n\t\t\t\t\t_, err := tx.Get(bucket, key)\n\t\t\t\t\trequire.Error(t, ErrNotFoundKey, err)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t}); err != nil {\n\t\t\tlog.Println(err)\n\t\t}\n\t}\n\n\tdbs := make([]*DB, 6)\n\tdbs[0], _ = Open(\n\t\tDefaultOptions,\n\t\tWithDir(fileDir1),\n\t\tWithEntryIdxMode(HintKeyValAndRAMIdxMode),\n\t)\n\tdbs[1], _ = Open(\n\t\tDefaultOptions,\n\t\tWithDir(fileDir2),\n\t\tWithEntryIdxMode(HintKeyAndRAMIdxMode),\n\t)\n\tdbs[2], _ = Open(\n\t\tDefaultOptions,\n\t\tWithDir(fileDir4),\n\t\tWithEntryIdxMode(HintKeyValAndRAMIdxMode),\n\t\tWithMaxBatchCount(35),\n\t)\n\tdbs[3], _ = Open(\n\t\tDefaultOptions,\n\t\tWithDir(fileDir5),\n\t\tWithEntryIdxMode(HintKeyAndRAMIdxMode),\n\t\tWithMaxBatchCount(35),\n\t)\n\tdbs[4], _ = Open(\n\t\tDefaultOptions,\n\t\tWithDir(fileDir7),\n\t\tWithEntryIdxMode(HintKeyValAndRAMIdxMode),\n\t\tWithMaxBatchSize(20), // change to 1000, unit test is not ok, 1000000 is ok\n\t)\n\tdbs[5], _ = Open(\n\t\tDefaultOptions,\n\t\tWithDir(fileDir8),\n\t\tWithEntryIdxMode(HintKeyAndRAMIdxMode),\n\t\tWithMaxBatchSize(20),\n\t)\n\n\tfor _, db := range dbs {\n\t\trequire.NotEqual(t, db, nil)\n\t\ttestWrite(t, db)\n\t\ttestEmptyWrite(t, db)\n\t\tTestFlushPanic(t, db)\n\t}\n}\n\nfunc TestWriteBatch_SetMaxPendingTxns(t *testing.T) {\n\tmax := 10\n\tdb, err := Open(\n\t\tDefaultOptions,\n\t\tWithDir(\"/tmp\"),\n\t)\n\trequire.NoError(t, err)\n\twb, err := db.NewWriteBatch()\n\trequire.NoError(t, err)\n\twb.SetMaxPendingTxns(max)\n\tif wb.throttle == nil {\n\t\tt.Error(\"Expected throttle to be initialized, but it was nil\")\n\t}\n\tif cap(wb.throttle.ch) != max {\n\t\tt.Errorf(\"Expected channel length to be %d, but got %d\", max, len(wb.throttle.ch))\n\t}\n\tif cap(wb.throttle.errCh) != max {\n\t\tt.Errorf(\"Expected error channel length to be %d, but got %d\", max, len(wb.throttle.errCh))\n\t}\n}\n"
        },
        {
          "name": "btree.go",
          "type": "blob",
          "size": 3.3125,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"regexp\"\n\n\t\"github.com/tidwall/btree\"\n)\n\n// ErrKeyNotFound is returned when the key is not in the b tree.\nvar ErrKeyNotFound = errors.New(\"key not found\")\n\ntype Item struct {\n\tkey    []byte\n\trecord *Record\n}\n\ntype BTree struct {\n\tbtree *btree.BTreeG[*Item]\n}\n\nfunc NewBTree() *BTree {\n\treturn &BTree{\n\t\tbtree: btree.NewBTreeG[*Item](func(a, b *Item) bool {\n\t\t\treturn bytes.Compare(a.key, b.key) == -1\n\t\t}),\n\t}\n}\n\nfunc (bt *BTree) Find(key []byte) (*Record, bool) {\n\titem, ok := bt.btree.Get(&Item{key: key})\n\tif ok {\n\t\treturn item.record, ok\n\t}\n\treturn nil, ok\n}\n\nfunc (bt *BTree) Insert(record *Record) bool {\n\t_, replaced := bt.btree.Set(&Item{key: record.Key, record: record})\n\treturn replaced\n}\n\nfunc (bt *BTree) InsertRecord(key []byte, record *Record) bool {\n\t_, replaced := bt.btree.Set(&Item{key: key, record: record})\n\treturn replaced\n}\n\nfunc (bt *BTree) Delete(key []byte) bool {\n\t_, deleted := bt.btree.Delete(&Item{key: key})\n\treturn deleted\n}\n\nfunc (bt *BTree) All() []*Record {\n\titems := bt.btree.Items()\n\n\trecords := make([]*Record, len(items))\n\tfor i, item := range items {\n\t\trecords[i] = item.record\n\t}\n\n\treturn records\n}\n\nfunc (bt *BTree) AllItems() []*Item {\n\titems := bt.btree.Items()\n\treturn items\n}\n\nfunc (bt *BTree) Range(start, end []byte) []*Record {\n\trecords := make([]*Record, 0)\n\n\tbt.btree.Ascend(&Item{key: start}, func(item *Item) bool {\n\t\tif bytes.Compare(item.key, end) > 0 {\n\t\t\treturn false\n\t\t}\n\t\trecords = append(records, item.record)\n\t\treturn true\n\t})\n\n\treturn records\n}\n\nfunc (bt *BTree) PrefixScan(prefix []byte, offset, limitNum int) []*Record {\n\trecords := make([]*Record, 0)\n\n\tbt.btree.Ascend(&Item{key: prefix}, func(item *Item) bool {\n\t\tif !bytes.HasPrefix(item.key, prefix) {\n\t\t\treturn false\n\t\t}\n\n\t\tif offset > 0 {\n\t\t\toffset--\n\t\t\treturn true\n\t\t}\n\n\t\trecords = append(records, item.record)\n\n\t\tlimitNum--\n\t\treturn limitNum != 0\n\t})\n\n\treturn records\n}\n\nfunc (bt *BTree) PrefixSearchScan(prefix []byte, reg string, offset, limitNum int) []*Record {\n\trecords := make([]*Record, 0)\n\n\trgx := regexp.MustCompile(reg)\n\n\tbt.btree.Ascend(&Item{key: prefix}, func(item *Item) bool {\n\t\tif !bytes.HasPrefix(item.key, prefix) {\n\t\t\treturn false\n\t\t}\n\n\t\tif offset > 0 {\n\t\t\toffset--\n\t\t\treturn true\n\t\t}\n\n\t\tif !rgx.Match(bytes.TrimPrefix(item.key, prefix)) {\n\t\t\treturn true\n\t\t}\n\n\t\trecords = append(records, item.record)\n\n\t\tlimitNum--\n\t\treturn limitNum != 0\n\t})\n\n\treturn records\n}\n\nfunc (bt *BTree) Count() int {\n\treturn bt.btree.Len()\n}\n\nfunc (bt *BTree) PopMin() (*Item, bool) {\n\treturn bt.btree.PopMin()\n}\n\nfunc (bt *BTree) PopMax() (*Item, bool) {\n\treturn bt.btree.PopMax()\n}\n\nfunc (bt *BTree) Min() (*Item, bool) {\n\treturn bt.btree.Min()\n}\n\nfunc (bt *BTree) Max() (*Item, bool) {\n\treturn bt.btree.Max()\n}\n"
        },
        {
          "name": "btree_test.go",
          "type": "blob",
          "size": 5.6875,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"fmt\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n\t\"testing\"\n)\n\nvar (\n\tkeyFormat = \"key_%03d\"\n\tvalFormat = \"val_%03d\"\n)\n\nfunc runBTreeTest(t *testing.T, test func(t *testing.T, btree *BTree)) {\n\tbtree := NewBTree()\n\n\tfor i := 0; i < 100; i++ {\n\t\tkey := []byte(fmt.Sprintf(keyFormat, i))\n\t\tval := []byte(fmt.Sprintf(valFormat, i))\n\n\t\t_ = btree.Insert(NewRecord().WithKey(key).WithValue(val))\n\t}\n\n\ttest(t, btree)\n}\n\nfunc TestBTree_Find(t *testing.T) {\n\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\t\tr, ok := btree.Find([]byte(fmt.Sprintf(keyFormat, 0)))\n\t\trequire.Equal(t, []byte(fmt.Sprintf(keyFormat, 0)), r.Key)\n\t\trequire.True(t, ok)\n\t})\n}\n\nfunc TestBTree_Delete(t *testing.T) {\n\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\t\trequire.True(t, btree.Delete([]byte(fmt.Sprintf(keyFormat, 0))))\n\t\trequire.False(t, btree.Delete([]byte(fmt.Sprintf(keyFormat, 100))))\n\n\t\t_, ok := btree.Find([]byte(fmt.Sprintf(keyFormat, 0)))\n\t\trequire.False(t, ok)\n\t})\n}\n\nfunc TestBTree_PrefixScan(t *testing.T) {\n\tt.Run(\"prefix scan from beginning\", func(t *testing.T) {\n\t\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\t\t\tlimit := 10\n\n\t\t\trecords := btree.PrefixScan([]byte(\"key_\"), 0, limit)\n\n\t\t\tfor i, r := range records {\n\t\t\t\twantKey := []byte(fmt.Sprintf(keyFormat, i))\n\t\t\t\twantValue := []byte(fmt.Sprintf(valFormat, i))\n\n\t\t\t\tassert.Equal(t, wantKey, r.Key)\n\t\t\t\tassert.Equal(t, wantValue, r.Value)\n\t\t\t}\n\t\t})\n\t})\n\n\tt.Run(\"prefix scan for not exists pre key\", func(t *testing.T) {\n\t\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\t\t\trecords := btree.PrefixScan([]byte(\"key_xx\"), 0, 10)\n\t\t\tassert.Len(t, records, 0)\n\t\t})\n\t})\n}\n\nfunc TestBTree_PrefixSearchScan(t *testing.T) {\n\tt.Run(\"prefix search scan right email\", func(t *testing.T) {\n\t\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\n\t\t\tkey := []byte(\"nutsdb-123456789@outlook.com\")\n\t\t\tval := GetRandomBytes(24)\n\n\t\t\t_ = btree.Insert(NewRecord().WithKey(key).WithValue(val))\n\n\t\t\trecord, ok := btree.Find(key)\n\t\t\trequire.True(t, ok)\n\t\t\trequire.Equal(t, key, record.Key)\n\n\t\t\trecords := btree.PrefixSearchScan([]byte(\"nutsdb-\"),\n\t\t\t\t\"[a-z\\\\d]+(\\\\.[a-z\\\\d]+)*@([\\\\da-z](-[\\\\da-z])?)+(\\\\.{1,2}[a-z]+)+$\", 0, 1)\n\t\t\trequire.Equal(t, key, records[0].Key)\n\t\t})\n\t})\n\n\tt.Run(\"prefix search scan wrong email\", func(t *testing.T) {\n\t\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\n\t\t\tkey := []byte(\"nutsdb-123456789@outlook\")\n\t\t\tval := GetRandomBytes(24)\n\n\t\t\t_ = btree.Insert(NewRecord().WithKey(key).WithValue(val))\n\n\t\t\trecord, ok := btree.Find(key)\n\t\t\trequire.True(t, ok)\n\t\t\trequire.Equal(t, key, record.Key)\n\n\t\t\trecords := btree.PrefixSearchScan([]byte(\"nutsdb-\"),\n\t\t\t\t\"[a-z\\\\d]+(\\\\.[a-z\\\\d]+)*@([\\\\da-z](-[\\\\da-z])?)+(\\\\.{1,2}[a-z]+)+$\", 0, 1)\n\t\t\trequire.Len(t, records, 0)\n\t\t})\n\t})\n}\n\nfunc TestBTree_All(t *testing.T) {\n\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\t\texpectRecords := make([]*Record, 100)\n\n\t\tfor i := 0; i < 100; i++ {\n\t\t\tkey := []byte(fmt.Sprintf(keyFormat, i))\n\t\t\tval := []byte(fmt.Sprintf(valFormat, i))\n\n\t\t\texpectRecords[i] = NewRecord().WithKey(key).WithValue(val)\n\t\t}\n\n\t\trequire.ElementsMatch(t, expectRecords, btree.All())\n\t})\n}\n\nfunc TestBTree_Range(t *testing.T) {\n\tt.Run(\"btree range at begin\", func(t *testing.T) {\n\t\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\t\t\texpectRecords := make([]*Record, 10)\n\n\t\t\tfor i := 0; i < 10; i++ {\n\t\t\t\tkey := []byte(fmt.Sprintf(keyFormat, i))\n\t\t\t\tval := []byte(fmt.Sprintf(valFormat, i))\n\n\t\t\t\texpectRecords[i] = NewRecord().WithKey(key).WithValue(val)\n\t\t\t}\n\n\t\t\trecords := btree.Range([]byte(fmt.Sprintf(keyFormat, 0)), []byte(fmt.Sprintf(keyFormat, 9)))\n\n\t\t\trequire.ElementsMatch(t, records, expectRecords)\n\t\t})\n\t})\n\n\tt.Run(\"btree range at middle\", func(t *testing.T) {\n\t\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\t\t\texpectRecords := make([]*Record, 10)\n\n\t\t\tfor i := 40; i < 50; i++ {\n\t\t\t\tkey := []byte(fmt.Sprintf(keyFormat, i))\n\t\t\t\tval := []byte(fmt.Sprintf(valFormat, i))\n\n\t\t\t\texpectRecords[i-40] = NewRecord().WithKey(key).WithValue(val)\n\t\t\t}\n\n\t\t\trecords := btree.Range([]byte(fmt.Sprintf(keyFormat, 40)), []byte(fmt.Sprintf(keyFormat, 49)))\n\n\t\t\trequire.ElementsMatch(t, records, expectRecords)\n\t\t})\n\t})\n\n\tt.Run(\"btree range at end\", func(t *testing.T) {\n\t\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\t\t\texpectRecords := make([]*Record, 10)\n\n\t\t\tfor i := 90; i < 100; i++ {\n\t\t\t\tkey := []byte(fmt.Sprintf(keyFormat, i))\n\t\t\t\tval := []byte(fmt.Sprintf(valFormat, i))\n\n\t\t\t\texpectRecords[i-90] = NewRecord().WithKey(key).WithValue(val)\n\t\t\t}\n\n\t\t\trecords := btree.Range([]byte(fmt.Sprintf(keyFormat, 90)), []byte(fmt.Sprintf(keyFormat, 99)))\n\n\t\t\trequire.ElementsMatch(t, records, expectRecords)\n\t\t})\n\t})\n}\n\nfunc TestBTree_Update(t *testing.T) {\n\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\t\tfor i := 40; i < 50; i++ {\n\t\t\tkey := []byte(fmt.Sprintf(keyFormat, i))\n\t\t\tval := []byte(fmt.Sprintf(\"val_%03d_modify\", i))\n\n\t\t\tbtree.Insert(NewRecord().WithKey(key).WithValue(val))\n\t\t}\n\n\t\trecords := btree.Range([]byte(fmt.Sprintf(keyFormat, 40)), []byte(fmt.Sprintf(keyFormat, 49)))\n\n\t\tfor i := 40; i < 50; i++ {\n\t\t\trequire.Equal(t, []byte(fmt.Sprintf(\"val_%03d_modify\", i)), records[i-40].Value)\n\t\t}\n\t})\n}\n"
        },
        {
          "name": "bucket.go",
          "type": "blob",
          "size": 2.818359375,
          "content": "package nutsdb\n\nimport (\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"hash/crc32\"\n)\n\nvar BucketMetaSize int64\n\nconst (\n\tIdSize = 8\n\tDsSize = 2\n)\n\ntype BucketOperation uint16\n\nconst (\n\tBucketInsertOperation BucketOperation = 1\n\tBucketUpdateOperation BucketOperation = 2\n\tBucketDeleteOperation BucketOperation = 3\n)\n\nvar ErrBucketCrcInvalid = errors.New(\"bucket crc invalid\")\n\nfunc init() {\n\tBucketMetaSize = GetDiskSizeFromSingleObject(BucketMeta{})\n}\n\n// BucketMeta stores the Meta info of a Bucket. E.g. the size of bucket it store in disk.\ntype BucketMeta struct {\n\tCrc uint32\n\t// Op: Mark the latest operation (e.g. delete, insert, update) for this bucket.\n\tOp BucketOperation\n\t// Size: the size of payload.\n\tSize uint32\n}\n\n// Bucket is the disk structure of bucket\ntype Bucket struct {\n\t// Meta: the metadata for this bucket\n\tMeta *BucketMeta\n\t// Id: is the marker for this bucket, every bucket creation activity will generate a new Id for it.\n\t// for example. If you have a bucket called \"bucket_1\", and you just delete bucket and create it again.\n\t// the last bucket will have a different Id from the previous one.\n\tId BucketId\n\t// Ds: the data structure for this bucket. (List, Set, SortSet, String)\n\tDs Ds\n\t// Name: the name of this bucket.\n\tName string\n}\n\n// Decode : CRC | op | size\nfunc (meta *BucketMeta) Decode(bytes []byte) {\n\t_ = bytes[BucketMetaSize-1]\n\tcrc := binary.LittleEndian.Uint32(bytes[:4])\n\top := binary.LittleEndian.Uint16(bytes[4:6])\n\tsize := binary.LittleEndian.Uint32(bytes[6:10])\n\tmeta.Crc = crc\n\tmeta.Size = size\n\tmeta.Op = BucketOperation(op)\n}\n\n// Encode : Meta | BucketId | Ds | BucketName\nfunc (b *Bucket) Encode() []byte {\n\tentrySize := b.GetEntrySize()\n\tbuf := make([]byte, entrySize)\n\tb.Meta.Size = uint32(b.GetPayloadSize())\n\tbinary.LittleEndian.PutUint16(buf[4:6], uint16(b.Meta.Op))\n\tbinary.LittleEndian.PutUint32(buf[6:10], b.Meta.Size)\n\tbinary.LittleEndian.PutUint64(buf[BucketMetaSize:BucketMetaSize+IdSize], uint64(b.Id))\n\tbinary.LittleEndian.PutUint16(buf[BucketMetaSize+IdSize:BucketMetaSize+IdSize+DsSize], uint16(b.Ds))\n\tcopy(buf[BucketMetaSize+IdSize+DsSize:], b.Name)\n\tc32 := crc32.ChecksumIEEE(buf[4:])\n\tb.Meta.Crc = c32\n\tbinary.LittleEndian.PutUint32(buf[0:4], c32)\n\n\treturn buf\n}\n\n// Decode : Meta | BucketId | Ds | BucketName\nfunc (b *Bucket) Decode(bytes []byte) error {\n\t// parse the payload\n\tid := binary.LittleEndian.Uint64(bytes[:IdSize])\n\tds := binary.LittleEndian.Uint16(bytes[IdSize : IdSize+DsSize])\n\tname := bytes[IdSize+DsSize:]\n\tb.Id = id\n\tb.Name = string(name)\n\tb.Ds = ds\n\treturn nil\n}\n\nfunc (b *Bucket) GetEntrySize() int {\n\treturn int(BucketMetaSize) + b.GetPayloadSize()\n}\n\nfunc (b *Bucket) GetCRC(headerBuf []byte, dataBuf []byte) uint32 {\n\tcrc := crc32.ChecksumIEEE(headerBuf[4:])\n\tcrc = crc32.Update(crc, crc32.IEEETable, dataBuf)\n\treturn crc\n}\n\nfunc (b *Bucket) GetPayloadSize() int {\n\treturn IdSize + DsSize + len(b.Name)\n}\n"
        },
        {
          "name": "bucket_manager.go",
          "type": "blob",
          "size": 3.15234375,
          "content": "package nutsdb\n\nimport (\n\t\"errors\"\n\t\"os\"\n)\n\nvar ErrBucketNotExist = errors.New(\"bucket not exist\")\n\nconst BucketStoreFileName = \"bucket.Meta\"\n\ntype Ds = uint16\ntype BucketId = uint64\ntype BucketName = string\ntype IDMarkerInBucket map[BucketName]map[Ds]BucketId\ntype InfoMapperInBucket map[BucketId]*Bucket\n\ntype BucketManager struct {\n\tfd *os.File\n\t// BucketInfoMapper BucketID => Bucket itself\n\tBucketInfoMapper InfoMapperInBucket\n\n\tBucketIDMarker IDMarkerInBucket\n\n\t// IDGenerator helps generates an ID for every single bucket\n\tGen *IDGenerator\n}\n\nfunc NewBucketManager(dir string) (*BucketManager, error) {\n\tbm := &BucketManager{\n\t\tBucketInfoMapper: map[BucketId]*Bucket{},\n\t\tBucketIDMarker:   map[BucketName]map[Ds]BucketId{},\n\t}\n\tbucketFilePath := dir + \"/\" + BucketStoreFileName\n\t_, err := os.Stat(bucketFilePath)\n\tmode := os.O_RDWR\n\tif err != nil {\n\t\tmode |= os.O_CREATE\n\t}\n\tfd, err := os.OpenFile(bucketFilePath, mode, os.ModePerm)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbm.fd = fd\n\tbm.Gen = &IDGenerator{currentMaxId: 0}\n\treturn bm, nil\n}\n\ntype bucketSubmitRequest struct {\n\tds     Ds\n\tname   BucketName\n\tbucket *Bucket\n}\n\nfunc (bm *BucketManager) SubmitPendingBucketChange(reqs []*bucketSubmitRequest) error {\n\tbytes := make([]byte, 0)\n\tfor _, req := range reqs {\n\t\tbs := req.bucket.Encode()\n\t\tbytes = append(bytes, bs...)\n\t\t// update the marker info\n\t\tif _, exist := bm.BucketIDMarker[req.name]; !exist {\n\t\t\tbm.BucketIDMarker[req.name] = map[Ds]BucketId{}\n\t\t}\n\t\t// recover maxid otherwise new bucket start from 1 again\n\t\tbm.Gen.CompareAndSetMaxId(req.bucket.Id)\n\t\tswitch req.bucket.Meta.Op {\n\t\tcase BucketInsertOperation:\n\t\t\tbm.BucketInfoMapper[req.bucket.Id] = req.bucket\n\t\t\tbm.BucketIDMarker[req.name][req.bucket.Ds] = req.bucket.Id\n\t\tcase BucketDeleteOperation:\n\t\t\tif len(bm.BucketIDMarker[req.name]) == 1 {\n\t\t\t\tdelete(bm.BucketIDMarker, req.name)\n\t\t\t} else {\n\t\t\t\tdelete(bm.BucketIDMarker[req.name], req.bucket.Ds)\n\t\t\t}\n\t\t\tdelete(bm.BucketInfoMapper, req.bucket.Id)\n\t\t}\n\t}\n\t_, err := bm.fd.Write(bytes)\n\treturn err\n}\n\ntype IDGenerator struct {\n\tcurrentMaxId uint64\n}\n\nfunc (g *IDGenerator) GenId() uint64 {\n\tg.currentMaxId++\n\treturn g.currentMaxId\n}\n\nfunc (g *IDGenerator) CompareAndSetMaxId(id uint64) {\n\tif id > g.currentMaxId {\n\t\tg.currentMaxId = id\n\t}\n}\n\nfunc (bm *BucketManager) ExistBucket(ds Ds, name BucketName) bool {\n\tbucket, err := bm.GetBucket(ds, name)\n\tif bucket != nil && err == nil {\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (bm *BucketManager) GetBucket(ds Ds, name BucketName) (b *Bucket, err error) {\n\tds2IdMapper := bm.BucketIDMarker[name]\n\tif ds2IdMapper == nil {\n\t\treturn nil, ErrBucketNotExist\n\t}\n\n\tif id, exist := ds2IdMapper[ds]; exist {\n\t\tif bucket, ok := bm.BucketInfoMapper[id]; ok {\n\t\t\treturn bucket, nil\n\t\t} else {\n\t\t\treturn nil, ErrBucketNotExist\n\t\t}\n\t} else {\n\t\treturn nil, ErrBucketNotExist\n\t}\n}\n\nfunc (bm *BucketManager) GetBucketById(id BucketId) (*Bucket, error) {\n\tif bucket, exist := bm.BucketInfoMapper[id]; exist {\n\t\treturn bucket, nil\n\t} else {\n\t\treturn nil, ErrBucketNotExist\n\t}\n}\n\nfunc (bm *BucketManager) GetBucketID(ds Ds, name BucketName) (BucketId, error) {\n\tif bucket, err := bm.GetBucket(ds, name); err != nil {\n\t\treturn 0, err\n\t} else {\n\t\treturn bucket.Id, nil\n\t}\n}\n"
        },
        {
          "name": "bucket_manager_test.go",
          "type": "blob",
          "size": 3.361328125,
          "content": "package nutsdb\n\nimport (\n\t\"github.com/stretchr/testify/assert\"\n\t\"testing\"\n)\n\nfunc TestBucketManager_NewBucketAndDeleteBucket(t *testing.T) {\n\tbucket1 := \"bucket_1\"\n\tbucket2 := \"bucket_2\"\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxNewBucket(t, db, bucket1, DataStructureBTree, nil, nil)\n\t\texist := db.bm.ExistBucket(DataStructureBTree, bucket1)\n\t\tassert.Equal(t, true, exist)\n\t\ttxNewBucket(t, db, bucket2, DataStructureBTree, nil, nil)\n\t\texist = db.bm.ExistBucket(DataStructureBTree, bucket2)\n\t\tassert.Equal(t, true, exist)\n\t})\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxNewBucket(t, db, bucket1, DataStructureBTree, nil, nil)\n\t\texist := db.bm.ExistBucket(DataStructureBTree, bucket1)\n\t\tassert.Equal(t, true, exist)\n\t\ttxDeleteBucketFunc(t, db, bucket1, DataStructureBTree, nil, nil)\n\t\texist = db.bm.ExistBucket(DataStructureBTree, bucket1)\n\t\tassert.Equal(t, false, exist)\n\t})\n}\n\nfunc TestBucketManager_ExistBucket(t *testing.T) {\n\tbucket1 := \"bucket_1\"\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\texist := db.bm.ExistBucket(DataStructureBTree, bucket1)\n\t\tassert.Equal(t, false, exist)\n\n\t\ttxNewBucket(t, db, bucket1, DataStructureBTree, nil, nil)\n\t\texist = db.bm.ExistBucket(DataStructureBTree, bucket1)\n\t\tassert.Equal(t, true, exist)\n\t})\n}\n\nfunc TestBucketManager_Recovery(t *testing.T) {\n\tdir := \"/tmp/nutsdb_test_data\"\n\tconst bucket1 = \"bucket_1\"\n\tconst bucket2 = \"bucket_2\"\n\tdb, err := Open(DefaultOptions, WithDir(dir))\n\tdefer removeDir(dir)\n\tassert.NotNil(t, db)\n\tassert.Nil(t, err)\n\ttxNewBucket(t, db, bucket1, DataStructureBTree, nil, nil)\n\ttxNewBucket(t, db, bucket2, DataStructureBTree, nil, nil)\n\ttxDeleteBucketFunc(t, db, bucket1, DataStructureBTree, nil, nil)\n\tdb.Close()\n\n\tdb, err = Open(DefaultOptions, WithDir(dir))\n\tassert.Nil(t, err)\n\tassert.NotNil(t, db)\n\n\terr = db.View(func(tx *Tx) error {\n\t\texist := tx.ExistBucket(DataStructureBTree, bucket2)\n\t\tassert.Equal(t, true, exist)\n\t\texist = tx.ExistBucket(DataStructureBTree, bucket1)\n\t\tassert.Equal(t, false, exist)\n\t\treturn nil\n\t})\n\tassert.Nil(t, err)\n}\n\nfunc TestBucketManager_DataStructureIsolation(t *testing.T) {\n\tconst bucket1 = \"bucket_1\"\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket1, nil)\n\n\t\tassert.Equal(t, false, db.bm.ExistBucket(DataStructureList, bucket1))\n\t\tassert.Equal(t, false, db.bm.ExistBucket(DataStructureSortedSet, bucket1))\n\t\tassert.Equal(t, false, db.bm.ExistBucket(DataStructureSet, bucket1))\n\t})\n}\n\nfunc TestBucketManager_DeleteBucketIsolation(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tconst bucket1 = \"bucket_1\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket1, nil)\n\t\ttxPut(t, db, bucket1, []byte(\"key_1\"), []byte(\"value_1\"), Persistent, nil, nil)\n\t\ttxDeleteBucket(t, db, DataStructureBTree, bucket1, nil)\n\t\ttxGet(t, db, bucket1, []byte(\"key_1\"), nil, ErrBucketNotExist)\n\t})\n}\n\nfunc txNewBucket(t *testing.T, db *DB, bucket string, ds uint16, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr2 := tx.NewBucket(ds, bucket)\n\t\tassertErr(t, expectErr, err2)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n\n}\n\nfunc txDeleteBucketFunc(t *testing.T, db *DB, bucket string, ds uint16, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr2 := tx.DeleteBucket(ds, bucket)\n\t\tassertErr(t, expectErr, err2)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n"
        },
        {
          "name": "bucket_test.go",
          "type": "blob",
          "size": 0.7880859375,
          "content": "package nutsdb\n\nimport (\n\t\"github.com/stretchr/testify/assert\"\n\t\"testing\"\n)\n\nfunc TestBucket_DecodeAndDecode(t *testing.T) {\n\tbucket := &Bucket{\n\t\tMeta: &BucketMeta{\n\t\t\tOp: BucketInsertOperation,\n\t\t},\n\t\tId:   1,\n\t\tDs:   DataStructureBTree,\n\t\tName: \"bucket_1\",\n\t}\n\tbytes := bucket.Encode()\n\n\tbucketMeta := &BucketMeta{}\n\tbucketMeta.Decode(bytes[:BucketMetaSize])\n\tassert.Equal(t, bucketMeta.Op, BucketInsertOperation)\n\tassert.Equal(t, int64(8+2+8), int64(bucketMeta.Size))\n\tdecodeBucket := &Bucket{Meta: bucketMeta}\n\n\terr := decodeBucket.Decode(bytes[BucketMetaSize:])\n\tassert.Nil(t, err)\n\tassert.Equal(t, BucketId(1), decodeBucket.Id)\n\tassert.Equal(t, decodeBucket.Name, \"bucket_1\")\n\n\tcrc := decodeBucket.GetCRC(bytes[:BucketMetaSize], bytes[BucketMetaSize:])\n\tassert.Equal(t, decodeBucket.Meta.Crc, crc)\n}\n"
        },
        {
          "name": "const.go",
          "type": "blob",
          "size": 0.6884765625,
          "content": "//go:build !386 && !arm\n\n// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport \"math\"\n\nconst MAX_SIZE = math.MaxUint32\n"
        },
        {
          "name": "const_32bits.go",
          "type": "blob",
          "size": 0.685546875,
          "content": "//go:build 386 || arm\n\n// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport \"math\"\n\nconst MAX_SIZE = math.MaxInt32\n"
        },
        {
          "name": "datafile.go",
          "type": "blob",
          "size": 3.1572265625,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"errors\"\n)\n\nvar (\n\t// ErrCrc is returned when crc is error\n\tErrCrc = errors.New(\"crc error\")\n\n\t// ErrCapacity is returned when capacity is error.\n\tErrCapacity = errors.New(\"capacity error\")\n\n\tErrEntryZero = errors.New(\"entry is zero \")\n)\n\nconst (\n\t// DataSuffix returns the data suffix\n\tDataSuffix = \".dat\"\n)\n\n// DataFile records about data file information.\ntype DataFile struct {\n\tpath       string\n\tfileID     int64\n\twriteOff   int64\n\tActualSize int64\n\trwManager  RWManager\n}\n\n// NewDataFile will return a new DataFile Object.\nfunc NewDataFile(path string, rwManager RWManager) *DataFile {\n\tdataFile := &DataFile{\n\t\tpath:      path,\n\t\trwManager: rwManager,\n\t}\n\treturn dataFile\n}\n\n// ReadEntry returns entry at the given off(offset).\n// payloadSize = bucketSize + keySize + valueSize\nfunc (df *DataFile) ReadEntry(off int, payloadSize int64) (e *Entry, err error) {\n\tsize := MaxEntryHeaderSize + payloadSize\n\t// Since MaxEntryHeaderSize + payloadSize may be larger than the actual entry size, it needs to be calculated\n\tif int64(off)+size > df.rwManager.Size() {\n\t\tsize = df.rwManager.Size() - int64(off)\n\t}\n\tbuf := make([]byte, size)\n\n\tif _, err := df.rwManager.ReadAt(buf, int64(off)); err != nil {\n\t\treturn nil, err\n\t}\n\n\te = new(Entry)\n\theaderSize, err := e.ParseMeta(buf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Remove the content after the Header\n\tbuf = buf[:int(headerSize+payloadSize)]\n\n\tif e.IsZero() {\n\t\treturn nil, ErrEntryZero\n\t}\n\n\tif err := e.checkPayloadSize(payloadSize); err != nil {\n\t\treturn nil, err\n\t}\n\n\terr = e.ParsePayload(buf[headerSize:])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcrc := e.GetCrc(buf[:headerSize])\n\tif crc != e.Meta.Crc {\n\t\treturn nil, ErrCrc\n\t}\n\n\treturn\n}\n\n// WriteAt copies data to mapped region from the b slice starting at\n// given off and returns number of bytes copied to the mapped region.\nfunc (df *DataFile) WriteAt(b []byte, off int64) (n int, err error) {\n\treturn df.rwManager.WriteAt(b, off)\n}\n\n// Sync commits the current contents of the file to stable storage.\n// Typically, this means flushing the file system's in-memory copy\n// of recently written data to disk.\nfunc (df *DataFile) Sync() (err error) {\n\treturn df.rwManager.Sync()\n}\n\n// Close closes the RWManager.\n// If RWManager is FileRWManager represents closes the File,\n// rendering it unusable for I/O.\n// If RWManager is a MMapRWManager represents Unmap deletes the memory mapped region,\n// flushes any remaining changes.\nfunc (df *DataFile) Close() (err error) {\n\treturn df.rwManager.Close()\n}\n\nfunc (df *DataFile) Release() (err error) {\n\treturn df.rwManager.Release()\n}\n"
        },
        {
          "name": "datafile_test.go",
          "type": "blob",
          "size": 4.828125,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nvar (\n\tfilePath string\n\tentry    Entry\n)\n\nfunc init() {\n\tfilePath = \"/tmp/foo\"\n\tentry = Entry{\n\t\tKey:   []byte(\"key_0001\"),\n\t\tValue: []byte(\"val_0001\"),\n\t\tMeta: NewMetaData().WithKeySize(uint32(len(\"key_0001\"))).\n\t\t\tWithValueSize(uint32(len(\"val_0001\"))).WithTimeStamp(1547707905).\n\t\t\tWithTTL(Persistent).WithFlag(DataSetFlag).WithBucketId(1),\n\t}\n}\n\nfunc TestDataFile_Err(t *testing.T) {\n\tfm := newFileManager(MMap, 1024, 0.5, 256*MB)\n\tdefer fm.close()\n\t_, err := fm.getDataFile(filePath, -1)\n\tdefer func() {\n\t\tos.Remove(filePath)\n\t}()\n\n\tassert.NotNil(t, err)\n}\n\nfunc TestDataFile1(t *testing.T) {\n\tfm := newFileManager(MMap, 1024, 0.5, 256*MB)\n\tdefer fm.close()\n\tdf, err := fm.getDataFile(filePath, 1024)\n\tdefer os.Remove(filePath)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tn, err := df.WriteAt(entry.Encode(), 0)\n\tif err != nil {\n\t\tt.Error(\"err TestDataFile_All WriteAt\")\n\t}\n\n\tpayloadSize := entry.Meta.PayloadSize()\n\te, err := df.ReadEntry(n, payloadSize)\n\tassert.Nil(t, e)\n\tassert.Error(t, err, ErrEntryZero)\n\n\te, err = df.ReadEntry(0, payloadSize)\n\tif err != nil || string(e.Key) != \"key_0001\" || string(e.Value) != \"val_0001\" || e.Meta.Timestamp != 1547707905 {\n\t\tt.Error(\"err TestDataFile_All ReadAt\")\n\t}\n\n\te, err = df.ReadEntry(1, payloadSize)\n\tif err == nil || e != nil {\n\t\tt.Error(\"err TestDataFile_All ReadAt\")\n\t}\n}\n\nfunc TestDataFile2(t *testing.T) {\n\tfm := newFileManager(FileIO, 1024, 0.5, 256*MB)\n\n\tfilePath2 := \"/tmp/foo2\"\n\tdf, err := fm.getDataFile(filePath2, 64)\n\tassert.Nil(t, err)\n\tdefer os.Remove(filePath2)\n\theaderSize := entry.Meta.Size()\n\tcontent := entry.Encode()[0 : headerSize-1]\n\t_, err = df.WriteAt(content, 0)\n\tif err != nil {\n\t\tt.Error(\"err TestDataFile_All WriteAt\")\n\t}\n\n\tpayloadSize := entry.Meta.PayloadSize()\n\te, err := df.ReadEntry(0, payloadSize)\n\tif err == nil || e != nil {\n\t\tt.Error(\"err TestDataFile_All ReadAt\")\n\t}\n\n\tfilePath3 := \"/tmp/foo3\"\n\n\tdf2, err := fm.getDataFile(filePath3, 64)\n\tdefer os.Remove(filePath3)\n\tassert.Nil(t, err)\n\n\theaderSize = entry.Meta.Size()\n\tcontent = entry.Encode()[0 : headerSize+1]\n\t_, err = df2.WriteAt(content, 0)\n\tassert.Nil(t, err)\n\n\te, err = df2.ReadEntry(0, payloadSize)\n\tif err == nil || e != nil {\n\t\tt.Error(\"err TestDataFile_All ReadAt\")\n\t}\n\n\terr = df.Release()\n\tassert.Nil(t, err)\n\terr = df2.Release()\n\tassert.Nil(t, err)\n\terr = fm.close()\n\tassert.Nil(t, err)\n}\n\nfunc TestDataFile_ReadRecord(t *testing.T) {\n\tfm := newFileManager(FileIO, 1024, 0.5, 256*MB)\n\tfilePath4 := \"/tmp/foo4\"\n\tdf, err := fm.getDataFile(filePath4, 1024)\n\tdefer func() {\n\t\terr = df.Release()\n\t\tassert.Nil(t, err)\n\t\terr = fm.close()\n\t\tassert.Nil(t, err)\n\t}()\n\tassert.Nil(t, err)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tpayloadSize := entry.Meta.PayloadSize()\n\te, err := df.ReadEntry(0, payloadSize)\n\tif err != nil && e != nil {\n\t\tt.Error(\"err ReadAt\")\n\t}\n\n\te, err = df.ReadEntry(1025, payloadSize)\n\tif err == nil && e != nil {\n\t\tt.Error(\"err ReadAt\")\n\t}\n}\n\nfunc TestDataFile_Err_Path(t *testing.T) {\n\tfm := newFileManager(FileIO, 1024, 0.5, 256*MB)\n\tdefer fm.close()\n\tfilePath5 := \":/tmp/foo5\"\n\tdf, err := fm.getDataFile(filePath5, entry.Size())\n\tif err == nil && df != nil {\n\t\tt.Error(\"err TestDataFile_All open\")\n\t}\n}\n\nfunc TestDataFile_Crc_Err(t *testing.T) {\n\tfm := newFileManager(FileIO, 1024, 0.5, 256*MB)\n\tfilePath4 := \"/tmp/foo6\"\n\n\tdf, err := fm.getDataFile(filePath4, entry.Size())\n\tassert.Nil(t, err)\n\tassert.NotNil(t, df)\n\tdefer func() {\n\t\terr = df.Release()\n\t\tassert.Nil(t, err)\n\t\terr = fm.close()\n\t\tassert.Nil(t, err)\n\t\terr = os.Remove(filePath4)\n\t\tassert.Nil(t, err)\n\t}()\n\n\tvar errContent []byte\n\terrContent = append(errContent, entry.Encode()[0:4]...)\n\terrContent = append(errContent, entry.Encode()[4:entry.Size()-1]...)\n\terrContent = append(errContent, 0)\n\t_, err = df.WriteAt(errContent, 0)\n\tassert.Nil(t, err)\n\n\tpayloadSize := entry.Meta.PayloadSize()\n\te, err := df.ReadEntry(0, payloadSize)\n\tif err == nil || e != nil {\n\t\tt.Error(\"err TestDataFile_All ReadAt\")\n\t}\n}\n\nfunc TestFileManager1(t *testing.T) {\n\tfm := newFileManager(FileIO, 1024, 0.5, 256*MB)\n\tfilePath4 := \"/tmp/foo6\"\n\tdf, err := fm.getDataFile(filePath4, entry.Size())\n\tassert.Nil(t, err)\n\tdefer func() {\n\t\terr = df.Release()\n\t\tassert.Nil(t, err)\n\t\terr = fm.close()\n\t\tassert.Nil(t, err)\n\t\tos.Remove(filePath)\n\t}()\n}\n"
        },
        {
          "name": "db.go",
          "type": "blob",
          "size": 20.822265625,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/gofrs/flock\"\n\t\"github.com/xujiajun/utils/filesystem\"\n\t\"github.com/xujiajun/utils/strconv2\"\n)\n\n// ScanNoLimit represents the data scan no limit flag\nconst ScanNoLimit int = -1\nconst KvWriteChCapacity = 1000\nconst FLockName = \"nutsdb-flock\"\n\ntype (\n\t// DB represents a collection of buckets that persist on disk.\n\tDB struct {\n\t\topt                     Options // the database options\n\t\tIndex                   *index\n\t\tActiveFile              *DataFile\n\t\tMaxFileID               int64\n\t\tmu                      sync.RWMutex\n\t\tKeyCount                int // total key number ,include expired, deleted, repeated.\n\t\tclosed                  bool\n\t\tisMerging               bool\n\t\tfm                      *fileManager\n\t\tflock                   *flock.Flock\n\t\tcommitBuffer            *bytes.Buffer\n\t\tmergeStartCh            chan struct{}\n\t\tmergeEndCh              chan error\n\t\tmergeWorkCloseCh        chan struct{}\n\t\twriteCh                 chan *request\n\t\ttm                      *ttlManager\n\t\tRecordCount             int64 // current valid record count, exclude deleted, repeated\n\t\tbm                      *BucketManager\n\t\thintKeyAndRAMIdxModeLru *LRUCache // lru cache for HintKeyAndRAMIdxMode\n\t}\n)\n\n// open returns a newly initialized DB object.\nfunc open(opt Options) (*DB, error) {\n\tdb := &DB{\n\t\tMaxFileID:               0,\n\t\topt:                     opt,\n\t\tKeyCount:                0,\n\t\tclosed:                  false,\n\t\tIndex:                   newIndex(),\n\t\tfm:                      newFileManager(opt.RWMode, opt.MaxFdNumsInCache, opt.CleanFdsCacheThreshold, opt.SegmentSize),\n\t\tmergeStartCh:            make(chan struct{}),\n\t\tmergeEndCh:              make(chan error),\n\t\tmergeWorkCloseCh:        make(chan struct{}),\n\t\twriteCh:                 make(chan *request, KvWriteChCapacity),\n\t\ttm:                      newTTLManager(opt.ExpiredDeleteType),\n\t\thintKeyAndRAMIdxModeLru: NewLruCache(opt.HintKeyAndRAMIdxCacheSize),\n\t}\n\n\tdb.commitBuffer = createNewBufferWithSize(int(db.opt.CommitBufferSize))\n\n\tif ok := filesystem.PathIsExist(db.opt.Dir); !ok {\n\t\tif err := os.MkdirAll(db.opt.Dir, os.ModePerm); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tfileLock := flock.New(filepath.Join(opt.Dir, FLockName))\n\tif ok, err := fileLock.TryLock(); err != nil {\n\t\treturn nil, err\n\t} else if !ok {\n\t\treturn nil, ErrDirLocked\n\t}\n\n\tdb.flock = fileLock\n\n\tif bm, err := NewBucketManager(opt.Dir); err == nil {\n\t\tdb.bm = bm\n\t} else {\n\t\treturn nil, err\n\t}\n\n\tif err := db.rebuildBucketManager(); err != nil {\n\t\treturn nil, fmt.Errorf(\"db.rebuildBucketManager err:%s\", err)\n\t}\n\n\tif err := db.buildIndexes(); err != nil {\n\t\treturn nil, fmt.Errorf(\"db.buildIndexes error: %s\", err)\n\t}\n\n\tgo db.mergeWorker()\n\tgo db.doWrites()\n\tgo db.tm.run()\n\n\treturn db, nil\n}\n\n// Open returns a newly initialized DB object with Option.\nfunc Open(options Options, ops ...Option) (*DB, error) {\n\topts := &options\n\tfor _, do := range ops {\n\t\tdo(opts)\n\t}\n\treturn open(*opts)\n}\n\n// Update executes a function within a managed read/write transaction.\nfunc (db *DB) Update(fn func(tx *Tx) error) error {\n\tif fn == nil {\n\t\treturn ErrFn\n\t}\n\n\treturn db.managed(true, fn)\n}\n\n// View executes a function within a managed read-only transaction.\nfunc (db *DB) View(fn func(tx *Tx) error) error {\n\tif fn == nil {\n\t\treturn ErrFn\n\t}\n\n\treturn db.managed(false, fn)\n}\n\n// Backup copies the database to file directory at the given dir.\nfunc (db *DB) Backup(dir string) error {\n\treturn db.View(func(tx *Tx) error {\n\t\treturn filesystem.CopyDir(db.opt.Dir, dir)\n\t})\n}\n\n// BackupTarGZ Backup copy the database to writer.\nfunc (db *DB) BackupTarGZ(w io.Writer) error {\n\treturn db.View(func(tx *Tx) error {\n\t\treturn tarGZCompress(w, db.opt.Dir)\n\t})\n}\n\n// Close releases all db resources.\nfunc (db *DB) Close() error {\n\tdb.mu.Lock()\n\tdefer db.mu.Unlock()\n\n\tif db.closed {\n\t\treturn ErrDBClosed\n\t}\n\n\tdb.closed = true\n\n\terr := db.release()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// release set all obj in the db instance to nil\nfunc (db *DB) release() error {\n\tGCEnable := db.opt.GCWhenClose\n\n\terr := db.ActiveFile.rwManager.Release()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdb.Index = nil\n\n\tdb.ActiveFile = nil\n\n\terr = db.fm.close()\n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdb.mergeWorkCloseCh <- struct{}{}\n\n\tif !db.flock.Locked() {\n\t\treturn ErrDirUnlocked\n\t}\n\n\terr = db.flock.Unlock()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdb.fm = nil\n\n\tdb.tm.close()\n\n\tif GCEnable {\n\t\truntime.GC()\n\t}\n\n\treturn nil\n}\n\nfunc (db *DB) getValueByRecord(record *Record) ([]byte, error) {\n\tif record == nil {\n\t\treturn nil, ErrRecordIsNil\n\t}\n\n\tif record.Value != nil {\n\t\treturn record.Value, nil\n\t}\n\n\t// firstly we find data in cache\n\tif db.getHintKeyAndRAMIdxCacheSize() > 0 {\n\t\tif value := db.hintKeyAndRAMIdxModeLru.Get(record); value != nil {\n\t\t\treturn value.(*Entry).Value, nil\n\t\t}\n\t}\n\n\tdirPath := getDataPath(record.FileID, db.opt.Dir)\n\tdf, err := db.fm.getDataFile(dirPath, db.opt.SegmentSize)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func(rwManager RWManager) {\n\t\terr := rwManager.Release()\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t}(df.rwManager)\n\n\tpayloadSize := int64(len(record.Key)) + int64(record.ValueSize)\n\titem, err := df.ReadEntry(int(record.DataPos), payloadSize)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"read err. pos %d, key %s, err %s\", record.DataPos, record.Key, err)\n\t}\n\n\t// saved in cache\n\tif db.getHintKeyAndRAMIdxCacheSize() > 0 {\n\t\tdb.hintKeyAndRAMIdxModeLru.Add(record, item)\n\t}\n\n\treturn item.Value, nil\n}\n\nfunc (db *DB) commitTransaction(tx *Tx) error {\n\tvar err error\n\tdefer func() {\n\t\tvar panicked bool\n\t\tif r := recover(); r != nil {\n\t\t\t// resume normal execution\n\t\t\tpanicked = true\n\t\t}\n\t\tif panicked || err != nil {\n\t\t\t// log.Fatal(\"panicked=\", panicked, \", err=\", err)\n\t\t\tif errRollback := tx.Rollback(); errRollback != nil {\n\t\t\t\terr = errRollback\n\t\t\t}\n\t\t}\n\t}()\n\n\t// commit current tx\n\ttx.lock()\n\ttx.setStatusRunning()\n\terr = tx.Commit()\n\tif err != nil {\n\t\t// log.Fatal(\"txCommit fail,err=\", err)\n\t\treturn err\n\t}\n\n\treturn err\n}\n\nfunc (db *DB) writeRequests(reqs []*request) error {\n\tvar err error\n\tif len(reqs) == 0 {\n\t\treturn nil\n\t}\n\n\tdone := func(err error) {\n\t\tfor _, r := range reqs {\n\t\t\tr.Err = err\n\t\t\tr.Wg.Done()\n\t\t}\n\t}\n\n\tfor _, req := range reqs {\n\t\ttx := req.tx\n\t\tcerr := db.commitTransaction(tx)\n\t\tif cerr != nil {\n\t\t\terr = cerr\n\t\t}\n\t}\n\n\tdone(err)\n\treturn err\n}\n\n// MaxBatchCount returns max possible entries in batch\nfunc (db *DB) getMaxBatchCount() int64 {\n\treturn db.opt.MaxBatchCount\n}\n\n// MaxBatchSize returns max possible batch size\nfunc (db *DB) getMaxBatchSize() int64 {\n\treturn db.opt.MaxBatchSize\n}\n\nfunc (db *DB) getMaxWriteRecordCount() int64 {\n\treturn db.opt.MaxWriteRecordCount\n}\n\nfunc (db *DB) getHintKeyAndRAMIdxCacheSize() int {\n\treturn db.opt.HintKeyAndRAMIdxCacheSize\n}\n\nfunc (db *DB) doWrites() {\n\tpendingCh := make(chan struct{}, 1)\n\twriteRequests := func(reqs []*request) {\n\t\tif err := db.writeRequests(reqs); err != nil {\n\t\t\tlog.Fatal(\"writeRequests fail, err=\", err)\n\t\t}\n\t\t<-pendingCh\n\t}\n\n\treqs := make([]*request, 0, 10)\n\tvar r *request\n\tvar ok bool\n\tfor {\n\t\tr, ok = <-db.writeCh\n\t\tif !ok {\n\t\t\tgoto closedCase\n\t\t}\n\n\t\tfor {\n\t\t\treqs = append(reqs, r)\n\n\t\t\tif len(reqs) >= 3*KvWriteChCapacity {\n\t\t\t\tpendingCh <- struct{}{} // blocking.\n\t\t\t\tgoto writeCase\n\t\t\t}\n\n\t\t\tselect {\n\t\t\t// Either push to pending, or continue to pick from writeCh.\n\t\t\tcase r, ok = <-db.writeCh:\n\t\t\t\tif !ok {\n\t\t\t\t\tgoto closedCase\n\t\t\t\t}\n\t\t\tcase pendingCh <- struct{}{}:\n\t\t\t\tgoto writeCase\n\t\t\t}\n\t\t}\n\n\tclosedCase:\n\t\t// All the pending request are drained.\n\t\t// Don't close the writeCh, because it has be used in several places.\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase r = <-db.writeCh:\n\t\t\t\treqs = append(reqs, r)\n\t\t\tdefault:\n\t\t\t\tpendingCh <- struct{}{} // Push to pending before doing write.\n\t\t\t\twriteRequests(reqs)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\twriteCase:\n\t\tgo writeRequests(reqs)\n\t\treqs = make([]*request, 0, 10)\n\t}\n}\n\n// setActiveFile sets the ActiveFile (DataFile object).\nfunc (db *DB) setActiveFile() (err error) {\n\tactiveFilePath := getDataPath(db.MaxFileID, db.opt.Dir)\n\tdb.ActiveFile, err = db.fm.getDataFile(activeFilePath, db.opt.SegmentSize)\n\tif err != nil {\n\t\treturn\n\t}\n\n\tdb.ActiveFile.fileID = db.MaxFileID\n\n\treturn nil\n}\n\n// getMaxFileIDAndFileIds returns max fileId and fileIds.\nfunc (db *DB) getMaxFileIDAndFileIDs() (maxFileID int64, dataFileIds []int) {\n\tfiles, _ := os.ReadDir(db.opt.Dir)\n\n\tif len(files) == 0 {\n\t\treturn 0, nil\n\t}\n\n\tfor _, file := range files {\n\t\tfilename := file.Name()\n\t\tfileSuffix := path.Ext(path.Base(filename))\n\t\tif fileSuffix != DataSuffix {\n\t\t\tcontinue\n\t\t}\n\n\t\tfilename = strings.TrimSuffix(filename, DataSuffix)\n\t\tid, _ := strconv2.StrToInt(filename)\n\t\tdataFileIds = append(dataFileIds, id)\n\t}\n\n\tif len(dataFileIds) == 0 {\n\t\treturn 0, nil\n\t}\n\n\tsort.Ints(dataFileIds)\n\tmaxFileID = int64(dataFileIds[len(dataFileIds)-1])\n\n\treturn\n}\n\nfunc (db *DB) parseDataFiles(dataFileIds []int) (err error) {\n\tvar (\n\t\toff      int64\n\t\tf        *fileRecovery\n\t\tfID      int64\n\t\tdataInTx dataInTx\n\t)\n\n\tparseDataInTx := func() error {\n\t\tfor _, entry := range dataInTx.es {\n\t\t\t// if this bucket is not existed in bucket manager right now\n\t\t\t// its because it already deleted in the feature WAL log.\n\t\t\t// so we can just ignore here.\n\t\t\tbucketId := entry.Meta.BucketId\n\t\t\tif _, err := db.bm.GetBucketById(bucketId); errors.Is(err, ErrBucketNotExist) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\trecord := db.createRecordByModeWithFidAndOff(entry.fid, uint64(entry.off), &entry.Entry)\n\n\t\t\tif err = db.buildIdxes(record, &entry.Entry); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tdb.KeyCount++\n\n\t\t}\n\t\treturn nil\n\t}\n\n\treadEntriesFromFile := func() error {\n\t\tfor {\n\t\t\tentry, err := f.readEntry(off)\n\t\t\tif err != nil {\n\t\t\t\t// whatever which logic branch it will choose, we will release the fd.\n\t\t\t\t_ = f.release()\n\t\t\t\tif errors.Is(err, io.EOF) || errors.Is(err, ErrIndexOutOfBound) || errors.Is(err, io.ErrUnexpectedEOF) || errors.Is(err, ErrEntryZero) || errors.Is(err, ErrHeaderSizeOutOfBounds) {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tif off >= db.opt.SegmentSize {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tif entry == nil {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tentryWhenRecovery := &EntryWhenRecovery{\n\t\t\t\tEntry: *entry,\n\t\t\t\tfid:   fID,\n\t\t\t\toff:   off,\n\t\t\t}\n\t\t\tif dataInTx.txId == 0 {\n\t\t\t\tdataInTx.appendEntry(entryWhenRecovery)\n\t\t\t\tdataInTx.txId = entry.Meta.TxID\n\t\t\t\tdataInTx.startOff = off\n\t\t\t} else if dataInTx.isSameTx(entryWhenRecovery) {\n\t\t\t\tdataInTx.appendEntry(entryWhenRecovery)\n\t\t\t}\n\n\t\t\tif entry.Meta.Status == Committed {\n\t\t\t\terr := parseDataInTx()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tdataInTx.reset()\n\t\t\t\tdataInTx.startOff = off\n\t\t\t}\n\n\t\t\tif !dataInTx.isSameTx(entryWhenRecovery) {\n\t\t\t\tdataInTx.reset()\n\t\t\t\tdataInTx.startOff = off\n\t\t\t}\n\n\t\t\toff += entry.Size()\n\t\t}\n\n\t\tif fID == db.MaxFileID {\n\t\t\tdb.ActiveFile.ActualSize = off\n\t\t\tdb.ActiveFile.writeOff = off\n\t\t}\n\n\t\treturn nil\n\t}\n\n\tfor _, dataID := range dataFileIds {\n\t\toff = 0\n\t\tfID = int64(dataID)\n\t\tdataPath := getDataPath(fID, db.opt.Dir)\n\t\tf, err = newFileRecovery(dataPath, db.opt.BufferSizeOfRecovery)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\terr := readEntriesFromFile()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// compute the valid record count and save it in db.RecordCount\n\tdb.RecordCount, err = db.getRecordCount()\n\treturn\n}\n\nfunc (db *DB) getRecordCount() (int64, error) {\n\tvar res int64\n\n\t// Iterate through the BTree indices\n\tfor _, btree := range db.Index.bTree.idx {\n\t\tres += int64(btree.Count())\n\t}\n\n\t// Iterate through the List indices\n\tfor _, listItem := range db.Index.list.idx {\n\t\tfor key := range listItem.Items {\n\t\t\tcurLen, err := listItem.Size(key)\n\t\t\tif err != nil {\n\t\t\t\treturn res, err\n\t\t\t}\n\t\t\tres += int64(curLen)\n\t\t}\n\t}\n\n\t// Iterate through the Set indices\n\tfor _, setItem := range db.Index.set.idx {\n\t\tfor key := range setItem.M {\n\t\t\tres += int64(setItem.SCard(key))\n\t\t}\n\t}\n\n\t// Iterate through the SortedSet indices\n\tfor _, zsetItem := range db.Index.sortedSet.idx {\n\t\tfor key := range zsetItem.M {\n\t\t\tcurLen, err := zsetItem.ZCard(key)\n\t\t\tif err != nil {\n\t\t\t\treturn res, err\n\t\t\t}\n\t\t\tres += int64(curLen)\n\t\t}\n\t}\n\n\treturn res, nil\n}\n\nfunc (db *DB) buildBTreeIdx(record *Record, entry *Entry) error {\n\tkey, meta := entry.Key, entry.Meta\n\n\tbucket, err := db.bm.GetBucketById(meta.BucketId)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbucketId := bucket.Id\n\n\tbTree := db.Index.bTree.getWithDefault(bucketId)\n\n\tif record.IsExpired() || meta.Flag == DataDeleteFlag {\n\t\tdb.tm.del(bucketId, string(key))\n\t\tbTree.Delete(key)\n\t} else {\n\t\tif meta.TTL != Persistent {\n\t\t\tdb.tm.add(bucketId, string(key), db.expireTime(meta.Timestamp, meta.TTL), db.buildExpireCallback(bucket.Name, key))\n\t\t} else {\n\t\t\tdb.tm.del(bucketId, string(key))\n\t\t}\n\t\tbTree.Insert(record)\n\t}\n\treturn nil\n}\n\nfunc (db *DB) expireTime(timestamp uint64, ttl uint32) time.Duration {\n\tnow := time.UnixMilli(time.Now().UnixMilli())\n\texpireTime := time.UnixMilli(int64(timestamp))\n\texpireTime = expireTime.Add(time.Duration(int64(ttl)) * time.Second)\n\treturn expireTime.Sub(now)\n}\n\nfunc (db *DB) buildIdxes(record *Record, entry *Entry) error {\n\tmeta := entry.Meta\n\tswitch meta.Ds {\n\tcase DataStructureBTree:\n\t\treturn db.buildBTreeIdx(record, entry)\n\tcase DataStructureList:\n\t\tif err := db.buildListIdx(record, entry); err != nil {\n\t\t\treturn err\n\t\t}\n\tcase DataStructureSet:\n\t\tif err := db.buildSetIdx(record, entry); err != nil {\n\t\t\treturn err\n\t\t}\n\tcase DataStructureSortedSet:\n\t\tif err := db.buildSortedSetIdx(record, entry); err != nil {\n\t\t\treturn err\n\t\t}\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"there is an unexpected data structure that is unimplemented in our database.:%d\", meta.Ds))\n\t}\n\treturn nil\n}\n\nfunc (db *DB) deleteBucket(ds uint16, bucket BucketId) {\n\tif ds == DataStructureSet {\n\t\tdb.Index.set.delete(bucket)\n\t}\n\tif ds == DataStructureSortedSet {\n\t\tdb.Index.sortedSet.delete(bucket)\n\t}\n\tif ds == DataStructureBTree {\n\t\tdb.Index.bTree.delete(bucket)\n\t}\n\tif ds == DataStructureList {\n\t\tdb.Index.list.delete(bucket)\n\t}\n}\n\n// buildSetIdx builds set index when opening the DB.\nfunc (db *DB) buildSetIdx(record *Record, entry *Entry) error {\n\tkey, val, meta := entry.Key, entry.Value, entry.Meta\n\n\tbucket, err := db.bm.GetBucketById(entry.Meta.BucketId)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbucketId := bucket.Id\n\n\ts := db.Index.set.getWithDefault(bucketId)\n\n\tswitch meta.Flag {\n\tcase DataSetFlag:\n\t\tif err := s.SAdd(string(key), [][]byte{val}, []*Record{record}); err != nil {\n\t\t\treturn fmt.Errorf(\"when build SetIdx SAdd index err: %s\", err)\n\t\t}\n\tcase DataDeleteFlag:\n\t\tif err := s.SRem(string(key), val); err != nil {\n\t\t\treturn fmt.Errorf(\"when build SetIdx SRem index err: %s\", err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// buildSortedSetIdx builds sorted set index when opening the DB.\nfunc (db *DB) buildSortedSetIdx(record *Record, entry *Entry) error {\n\tkey, val, meta := entry.Key, entry.Value, entry.Meta\n\n\tbucket, err := db.bm.GetBucketById(entry.Meta.BucketId)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbucketId := bucket.Id\n\n\tss := db.Index.sortedSet.getWithDefault(bucketId, db)\n\n\tswitch meta.Flag {\n\tcase DataZAddFlag:\n\t\tkeyAndScore := strings.Split(string(key), SeparatorForZSetKey)\n\t\tif len(keyAndScore) == 2 {\n\t\t\tkey := keyAndScore[0]\n\t\t\tscore, _ := strconv2.StrToFloat64(keyAndScore[1])\n\t\t\terr = ss.ZAdd(key, SCORE(score), val, record)\n\t\t}\n\tcase DataZRemFlag:\n\t\t_, err = ss.ZRem(string(key), val)\n\tcase DataZRemRangeByRankFlag:\n\t\tstart, end := splitIntIntStr(string(val), SeparatorForZSetKey)\n\t\terr = ss.ZRemRangeByRank(string(key), start, end)\n\tcase DataZPopMaxFlag:\n\t\t_, _, err = ss.ZPopMax(string(key))\n\tcase DataZPopMinFlag:\n\t\t_, _, err = ss.ZPopMin(string(key))\n\t}\n\n\t// We don't need to panic if sorted set is not found.\n\tif err != nil && !errors.Is(err, ErrSortedSetNotFound) {\n\t\treturn fmt.Errorf(\"when build sortedSetIdx err: %s\", err)\n\t}\n\n\treturn nil\n}\n\n// buildListIdx builds List index when opening the DB.\nfunc (db *DB) buildListIdx(record *Record, entry *Entry) error {\n\tkey, val, meta := entry.Key, entry.Value, entry.Meta\n\n\tbucket, err := db.bm.GetBucketById(entry.Meta.BucketId)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbucketId := bucket.Id\n\n\tl := db.Index.list.getWithDefault(bucketId)\n\n\tif IsExpired(meta.TTL, meta.Timestamp) {\n\t\treturn nil\n\t}\n\n\tswitch meta.Flag {\n\tcase DataExpireListFlag:\n\t\tt, _ := strconv2.StrToInt64(string(val))\n\t\tttl := uint32(t)\n\t\tl.TTL[string(key)] = ttl\n\t\tl.TimeStamp[string(key)] = meta.Timestamp\n\tcase DataLPushFlag:\n\t\terr = l.LPush(string(key), record)\n\tcase DataRPushFlag:\n\t\terr = l.RPush(string(key), record)\n\tcase DataLRemFlag:\n\t\terr = db.buildListLRemIdx(val, l, key)\n\tcase DataLPopFlag:\n\t\t_, err = l.LPop(string(key))\n\tcase DataRPopFlag:\n\t\t_, err = l.RPop(string(key))\n\tcase DataLTrimFlag:\n\t\tnewKey, start := splitStringIntStr(string(key), SeparatorForListKey)\n\t\tend, _ := strconv2.StrToInt(string(val))\n\t\terr = l.LTrim(newKey, start, end)\n\tcase DataLRemByIndex:\n\t\tindexes, _ := UnmarshalInts(val)\n\t\terr = l.LRemByIndex(string(key), indexes)\n\t}\n\n\tif err != nil {\n\t\treturn fmt.Errorf(\"when build listIdx err: %s\", err)\n\t}\n\n\treturn nil\n}\n\nfunc (db *DB) buildListLRemIdx(value []byte, l *List, key []byte) error {\n\tcount, newValue := splitIntStringStr(string(value), SeparatorForListKey)\n\n\treturn l.LRem(string(key), count, func(r *Record) (bool, error) {\n\t\tv, err := db.getValueByRecord(r)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\t\treturn bytes.Equal([]byte(newValue), v), nil\n\t})\n}\n\n// buildIndexes builds indexes when db initialize resource.\nfunc (db *DB) buildIndexes() (err error) {\n\tvar (\n\t\tmaxFileID   int64\n\t\tdataFileIds []int\n\t)\n\n\tmaxFileID, dataFileIds = db.getMaxFileIDAndFileIDs()\n\n\t// init db.ActiveFile\n\tdb.MaxFileID = maxFileID\n\n\t// set ActiveFile\n\tif err = db.setActiveFile(); err != nil {\n\t\treturn\n\t}\n\n\tif dataFileIds == nil && maxFileID == 0 {\n\t\treturn\n\t}\n\n\t// build hint index\n\treturn db.parseDataFiles(dataFileIds)\n}\n\nfunc (db *DB) createRecordByModeWithFidAndOff(fid int64, off uint64, entry *Entry) *Record {\n\trecord := NewRecord()\n\n\trecord.WithKey(entry.Key).\n\t\tWithTimestamp(entry.Meta.Timestamp).\n\t\tWithTTL(entry.Meta.TTL).\n\t\tWithTxID(entry.Meta.TxID)\n\n\tif db.opt.EntryIdxMode == HintKeyValAndRAMIdxMode {\n\t\trecord.WithValue(entry.Value)\n\t}\n\n\tif db.opt.EntryIdxMode == HintKeyAndRAMIdxMode {\n\t\trecord.WithFileId(fid).\n\t\t\tWithDataPos(off).\n\t\t\tWithValueSize(uint32(len(entry.Value)))\n\t}\n\n\treturn record\n}\n\n// managed calls a block of code that is fully contained in a transaction.\nfunc (db *DB) managed(writable bool, fn func(tx *Tx) error) (err error) {\n\tvar tx *Tx\n\n\ttx, err = db.Begin(writable)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr = fmt.Errorf(\"panic when executing tx, err is %+v\", r)\n\t\t}\n\t}()\n\n\tif err = fn(tx); err == nil {\n\t\terr = tx.Commit()\n\t} else {\n\t\tif db.opt.ErrorHandler != nil {\n\t\t\tdb.opt.ErrorHandler.HandleError(err)\n\t\t}\n\n\t\tif errRollback := tx.Rollback(); errRollback != nil {\n\t\t\terr = fmt.Errorf(\"%v. Rollback err: %v\", err, errRollback)\n\t\t}\n\t}\n\n\treturn err\n}\n\nfunc (db *DB) sendToWriteCh(tx *Tx) (*request, error) {\n\treq := requestPool.Get().(*request)\n\treq.reset()\n\treq.Wg.Add(1)\n\treq.tx = tx\n\treq.IncrRef()     // for db write\n\tdb.writeCh <- req // Handled in doWrites.\n\treturn req, nil\n}\n\nfunc (db *DB) checkListExpired() {\n\tdb.Index.list.rangeIdx(func(l *List) {\n\t\tfor key := range l.TTL {\n\t\t\tl.IsExpire(key)\n\t\t}\n\t})\n}\n\n// IsClose return the value that represents the status of DB\nfunc (db *DB) IsClose() bool {\n\treturn db.closed\n}\n\nfunc (db *DB) buildExpireCallback(bucket string, key []byte) func() {\n\treturn func() {\n\t\terr := db.Update(func(tx *Tx) error {\n\t\t\tb, err := tx.db.bm.GetBucket(DataStructureBTree, bucket)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tbucketId := b.Id\n\t\t\tif db.tm.exist(bucketId, string(key)) {\n\t\t\t\treturn tx.Delete(bucket, key)\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t\tif err != nil {\n\t\t\tlog.Printf(\"occur error when expired deletion, error: %v\", err.Error())\n\t\t}\n\t}\n}\n\nfunc (db *DB) rebuildBucketManager() error {\n\tbucketFilePath := db.opt.Dir + \"/\" + BucketStoreFileName\n\tf, err := newFileRecovery(bucketFilePath, db.opt.BufferSizeOfRecovery)\n\tif err != nil {\n\t\treturn nil\n\t}\n\tbucketRequest := make([]*bucketSubmitRequest, 0)\n\n\tfor {\n\t\tbucket, err := f.readBucket()\n\t\tif err != nil {\n\t\t\t// whatever which logic branch it will choose, we will release the fd.\n\t\t\t_ = f.release()\n\t\t\tif errors.Is(err, io.EOF) || errors.Is(err, io.ErrUnexpectedEOF) {\n\t\t\t\tbreak\n\t\t\t} else {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tbucketRequest = append(bucketRequest, &bucketSubmitRequest{\n\t\t\tds:     bucket.Ds,\n\t\t\tname:   BucketName(bucket.Name),\n\t\t\tbucket: bucket,\n\t\t})\n\t}\n\n\tif len(bucketRequest) > 0 {\n\t\terr = db.bm.SubmitPendingBucketChange(bucketRequest)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "db_error.go",
          "type": "blob",
          "size": 1.19921875,
          "content": "package nutsdb\n\nimport \"errors\"\n\nvar (\n\t// ErrDBClosed is returned when db is closed.\n\tErrDBClosed = errors.New(\"db is closed\")\n\n\t// ErrBucket is returned when bucket is not in the HintIdx.\n\tErrBucket = errors.New(\"err bucket\")\n\n\t// ErrFn is returned when fn is nil.\n\tErrFn = errors.New(\"err fn\")\n\n\t// ErrBucketNotFound is returned when looking for bucket that does not exist\n\tErrBucketNotFound = errors.New(\"bucket not found\")\n\n\t// ErrDataStructureNotSupported is returned when pass a not supported data structure\n\tErrDataStructureNotSupported = errors.New(\"this data structure is not supported for now\")\n\n\t// ErrDirLocked is returned when can't get the file lock of dir\n\tErrDirLocked = errors.New(\"the dir of db is locked\")\n\n\t// ErrDirUnlocked is returned when the file lock already unlocked\n\tErrDirUnlocked = errors.New(\"the dir of db is unlocked\")\n\n\t// ErrIsMerging is returned when merge in progress\n\tErrIsMerging = errors.New(\"merge in progress\")\n\n\t// ErrNotSupportMergeWhenUsingList is returned calling 'Merge' when using list\n\tErrNotSupportMergeWhenUsingList = errors.New(\"not support merge when using list for now\")\n\n\t// ErrRecordIsNil is returned when Record is nil\n\tErrRecordIsNil = errors.New(\"the record is nil\")\n)\n"
        },
        {
          "name": "db_test.go",
          "type": "blob",
          "size": 47.2490234375,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"math\"\n\t\"os\"\n\t\"strconv\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nvar (\n\tdb  *DB\n\topt Options\n\terr error\n)\n\nconst NutsDBTestDirPath = \"/tmp/nutsdb-test\"\n\nfunc assertErr(t *testing.T, err error, expectErr error) {\n\tif expectErr != nil {\n\t\trequire.Equal(t, expectErr, err)\n\t} else {\n\t\trequire.NoError(t, err)\n\t}\n}\n\nfunc removeDir(dir string) {\n\tif err := os.RemoveAll(dir); err != nil {\n\t\tpanic(err)\n\t}\n}\n\nfunc runNutsDBTest(t *testing.T, opts *Options, test func(t *testing.T, db *DB)) {\n\tif opts == nil {\n\t\topts = &DefaultOptions\n\t}\n\tif opts.Dir == \"\" {\n\t\topts.Dir = NutsDBTestDirPath\n\t}\n\tdefer removeDir(opts.Dir)\n\tdb, err := Open(*opts)\n\trequire.NoError(t, err)\n\n\ttest(t, db)\n\tt.Cleanup(func() {\n\t\tif !db.IsClose() {\n\t\t\trequire.NoError(t, db.Close())\n\t\t}\n\t})\n}\n\nfunc txPut(t *testing.T, db *DB, bucket string, key, value []byte, ttl uint32, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr = tx.Put(bucket, key, value, ttl)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txGet(t *testing.T, db *DB, bucket string, key []byte, expectVal []byte, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tvalue, err := tx.Get(bucket, key)\n\t\tif expectErr != nil {\n\t\t\trequire.Equal(t, expectErr, err)\n\t\t} else {\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.EqualValuesf(t, expectVal, value, \"err Tx Get. got %s want %s\", string(value), string(expectVal))\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txGetAll(t *testing.T, db *DB, bucket string, expectKeys [][]byte, expectValues [][]byte, expectErr error) {\n\trequire.NoError(t, db.View(func(tx *Tx) error {\n\t\tkeys, values, err := tx.GetAll(bucket)\n\t\tif expectErr != nil {\n\t\t\trequire.Equal(t, expectErr, err)\n\t\t} else {\n\t\t\trequire.NoError(t, err)\n\t\t\tn := len(keys)\n\t\t\tfor i := 0; i < n; i++ {\n\t\t\t\trequire.Equal(t, expectKeys[i], keys[i])\n\t\t\t\trequire.Equal(t, expectValues[i], values[i])\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}))\n}\n\nfunc txDel(t *testing.T, db *DB, bucket string, key []byte, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.Delete(bucket, key)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txGetMaxOrMinKey(t *testing.T, db *DB, bucket string, isMax bool, expectVal []byte, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tvalue, err := tx.getMaxOrMinKey(bucket, isMax)\n\t\tif expectErr != nil {\n\t\t\trequire.Equal(t, expectErr, err)\n\t\t} else {\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.EqualValuesf(t, expectVal, value, \"err Tx Get. got %s want %s\", string(value), string(expectVal))\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txDeleteBucket(t *testing.T, db *DB, ds uint16, bucket string, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.DeleteBucket(ds, bucket)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txCreateBucket(t *testing.T, db *DB, ds uint16, bucket string, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.NewBucket(ds, bucket)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc InitOpt(fileDir string, isRemoveFiles bool) {\n\tif fileDir == \"\" {\n\t\tfileDir = \"/tmp/nutsdbtest\"\n\t}\n\tif isRemoveFiles {\n\t\tfiles, _ := ioutil.ReadDir(fileDir)\n\t\tfor _, f := range files {\n\t\t\tname := f.Name()\n\t\t\tif name != \"\" {\n\t\t\t\terr := os.RemoveAll(fileDir + \"/\" + name)\n\t\t\t\tif err != nil {\n\t\t\t\t\tpanic(err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\topt = DefaultOptions\n\topt.Dir = fileDir\n\topt.SegmentSize = 8 * 1024\n\topt.CleanFdsCacheThreshold = 0.5\n\topt.MaxFdNumsInCache = 1024\n}\n\nfunc TestDB_Basic(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tbucket := \"bucket\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\tkey0 := GetTestBytes(0)\n\t\tval0 := GetRandomBytes(24)\n\n\t\t// put\n\t\ttxPut(t, db, bucket, key0, val0, Persistent, nil, nil)\n\t\ttxGet(t, db, bucket, key0, val0, nil)\n\n\t\tval1 := GetRandomBytes(24)\n\n\t\t// update\n\t\ttxPut(t, db, bucket, key0, val1, Persistent, nil, nil)\n\t\ttxGet(t, db, bucket, key0, val1, nil)\n\n\t\t// del\n\t\ttxDel(t, db, bucket, key0, nil)\n\t\ttxGet(t, db, bucket, key0, val1, ErrKeyNotFound)\n\t})\n}\n\nfunc TestDB_ReopenWithDelete(t *testing.T) {\n\tvar opts *Options\n\tif opts == nil {\n\t\topts = &DefaultOptions\n\t}\n\tif opts.Dir == \"\" {\n\t\topts.Dir = NutsDBTestDirPath\n\t}\n\tdb, err := Open(*opts)\n\trequire.NoError(t, err)\n\tdefer removeDir(opts.Dir)\n\n\tbucket := \"bucket\"\n\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\ttxPush(t, db, bucket, GetTestBytes(5), GetTestBytes(0), true, nil, nil)\n\ttxPush(t, db, bucket, GetTestBytes(5), GetTestBytes(1), true, nil, nil)\n\ttxDeleteBucket(t, db, DataStructureList, bucket, nil)\n\n\tif !db.IsClose() {\n\t\trequire.NoError(t, db.Close())\n\t}\n\n\tdb, err = Open(*opts)\n\trequire.NoError(t, err)\n\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\ttxDeleteBucket(t, db, DataStructureList, bucket, nil)\n\tif !db.IsClose() {\n\t\trequire.NoError(t, db.Close())\n\t}\n}\n\nfunc TestDB_Flock(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tdb2, err := Open(db.opt)\n\t\trequire.Nil(t, db2)\n\t\trequire.Equal(t, ErrDirLocked, err)\n\n\t\terr = db.Close()\n\t\trequire.NoError(t, err)\n\n\t\tdb2, err = Open(db.opt)\n\t\trequire.NoError(t, err)\n\t\trequire.NotNil(t, db2)\n\n\t\terr = db2.flock.Unlock()\n\t\trequire.NoError(t, err)\n\t\trequire.False(t, db2.flock.Locked())\n\n\t\terr = db2.Close()\n\t\trequire.Error(t, err)\n\t\trequire.Equal(t, ErrDirUnlocked, err)\n\t})\n}\n\nfunc TestDB_DeleteANonExistKey(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttestBucket := \"test_bucket\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, testBucket, nil)\n\n\t\ttxDel(t, db, testBucket, GetTestBytes(0), ErrKeyNotFound)\n\t\ttxPut(t, db, testBucket, GetTestBytes(1), GetRandomBytes(24), Persistent, nil, nil)\n\t\ttxDel(t, db, testBucket, GetTestBytes(0), ErrKeyNotFound)\n\t})\n}\n\nfunc TestDB_CheckListExpired(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttestBucket := \"test_bucket\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, testBucket, nil)\n\n\t\ttxPut(t, db, testBucket, GetTestBytes(0), GetTestBytes(1), Persistent, nil, nil)\n\t\ttxPut(t, db, testBucket, GetTestBytes(1), GetRandomBytes(24), 1, nil, nil)\n\n\t\ttime.Sleep(1100 * time.Millisecond)\n\n\t\tdb.checkListExpired()\n\n\t\t// this entry still alive\n\t\ttxGet(t, db, testBucket, GetTestBytes(0), GetTestBytes(1), nil)\n\t\t// this entry will be deleted\n\t\ttxGet(t, db, testBucket, GetTestBytes(1), nil, ErrKeyNotFound)\n\t})\n}\n\nfunc txLRem(t *testing.T, db *DB, bucket string, key []byte, count int, value []byte, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.LRem(bucket, key, count, value)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txLRemByIndex(t *testing.T, db *DB, bucket string, key []byte, expectErr error, indexes ...int) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.LRemByIndex(bucket, key, indexes...)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSAdd(t *testing.T, db *DB, bucket string, key, value []byte, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.SAdd(bucket, key, value)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txSKeys(t *testing.T, db *DB, bucket, pattern string, f func(key string) bool, expectVal int, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tpatternMatchNum := 0\n\t\terr := tx.SKeys(bucket, pattern, func(key string) bool {\n\t\t\tpatternMatchNum += 1\n\t\t\treturn f(key)\n\t\t})\n\t\tif expectErr != nil {\n\t\t\tassert.ErrorIs(t, expectErr, err)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.Equal(t, expectVal, patternMatchNum)\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSIsMember(t *testing.T, db *DB, bucket string, key, value []byte, expect bool) {\n\terr := db.View(func(tx *Tx) error {\n\t\tok, _ := tx.SIsMember(bucket, key, value)\n\t\trequire.Equal(t, expect, ok)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSAreMembers(t *testing.T, db *DB, bucket string, key []byte, expect bool, value ...[]byte) {\n\terr := db.View(func(tx *Tx) error {\n\t\tok, _ := tx.SAreMembers(bucket, key, value...)\n\t\trequire.Equal(t, expect, ok)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSHasKey(t *testing.T, db *DB, bucket string, key []byte, expect bool) {\n\terr := db.View(func(tx *Tx) error {\n\t\tok, _ := tx.SHasKey(bucket, key)\n\t\trequire.Equal(t, expect, ok)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSMembers(t *testing.T, db *DB, bucket string, key []byte, expectLength int, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tmembers, err := tx.SMembers(bucket, key)\n\t\tif expectErr != nil {\n\t\t\tassert.ErrorIs(t, expectErr, err)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.Equal(t, expectLength, len(members))\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSCard(t *testing.T, db *DB, bucket string, key []byte, expectLength int, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tlength, err := tx.SCard(bucket, key)\n\t\tif expectErr != nil {\n\t\t\tassert.ErrorIs(t, expectErr, err)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.Equal(t, expectLength, length)\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSDiffByOneBucket(t *testing.T, db *DB, bucket string, key1, key2 []byte, expectVal [][]byte, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tdiff, err := tx.SDiffByOneBucket(bucket, key1, key2)\n\t\tif expectErr != nil {\n\t\t\tassert.ErrorIs(t, expectErr, err)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.ElementsMatch(t, expectVal, diff)\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSDiffByTwoBucket(t *testing.T, db *DB, bucket1 string, key1 []byte, bucket2 string, key2 []byte, expectVal [][]byte, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tdiff, err := tx.SDiffByTwoBuckets(bucket1, key1, bucket2, key2)\n\t\tif expectErr != nil {\n\t\t\tassert.ErrorIs(t, err, expectErr)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.ElementsMatch(t, expectVal, diff)\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSPop(t *testing.T, db *DB, bucket string, key []byte, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\t_, err := tx.SPop(bucket, key)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSMoveByOneBucket(t *testing.T, db *DB, bucket1 string, key1, key2, val []byte, expectVal bool, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tok, err := tx.SMoveByOneBucket(bucket1, key1, key2, val)\n\t\tif expectErr != nil {\n\t\t\tassert.ErrorIs(t, err, expectErr)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.Equal(t, expectVal, ok)\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSMoveByTwoBuckets(t *testing.T, db *DB, bucket1 string, key1 []byte, bucket2 string, key2 []byte, val []byte, expectVal bool, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tok, err := tx.SMoveByTwoBuckets(bucket1, key1, bucket2, key2, val)\n\t\tif expectErr != nil {\n\t\t\tassert.ErrorIs(t, err, expectErr)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.Equal(t, expectVal, ok)\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSUnionByOneBucket(t *testing.T, db *DB, bucket1 string, key1, key2 []byte, expectVal [][]byte, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tunion, err := tx.SUnionByOneBucket(bucket1, key1, key2)\n\t\tif expectErr != nil {\n\t\t\tassert.ErrorIs(t, err, expectErr)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.ElementsMatch(t, expectVal, union)\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSUnionByTwoBuckets(t *testing.T, db *DB, bucket1 string, key1 []byte, bucket2 string, key2 []byte, expectVal [][]byte, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tunion, err := tx.SUnionByTwoBuckets(bucket1, key1, bucket2, key2)\n\t\tif expectErr != nil {\n\t\t\tassert.ErrorIs(t, err, expectErr)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.ElementsMatch(t, expectVal, union)\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSRem(t *testing.T, db *DB, bucket string, key, value []byte, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.SRem(bucket, key, value)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txZAdd(t *testing.T, db *DB, bucket string, key, value []byte, score float64, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.ZAdd(bucket, key, score, value)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txZRem(t *testing.T, db *DB, bucket string, key, value []byte, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.ZRem(bucket, key, value)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassert.NoError(t, err)\n}\n\nfunc txZCard(t *testing.T, db *DB, bucket string, key []byte, expectLength int, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tlength, err := tx.ZCard(bucket, key)\n\t\tif expectErr != nil {\n\t\t\tassert.Equal(t, expectErr, err)\n\t\t} else {\n\t\t\tassert.Equal(t, expectLength, length)\n\t\t}\n\t\treturn nil\n\t})\n\tassert.NoError(t, err)\n}\n\nfunc txZScore(t *testing.T, db *DB, bucket string, key, value []byte, expectScore float64, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tscore, err := tx.ZScore(bucket, key, value)\n\t\tif err != nil {\n\t\t\tassert.Equal(t, expectErr, err)\n\t\t} else {\n\t\t\tassert.Equal(t, expectScore, score)\n\t\t}\n\t\treturn nil\n\t})\n\tassert.NoError(t, err)\n}\n\nfunc txZRank(t *testing.T, db *DB, bucket string, key, value []byte, isRev bool, expectRank int, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tvar (\n\t\t\trank int\n\t\t\terr  error\n\t\t)\n\t\tif isRev {\n\t\t\trank, err = tx.ZRevRank(bucket, key, value)\n\t\t} else {\n\t\t\trank, err = tx.ZRank(bucket, key, value)\n\t\t}\n\t\tif expectErr != nil {\n\t\t\tassert.Equal(t, expectErr, err)\n\t\t} else {\n\t\t\tassert.Equal(t, expectRank, rank)\n\t\t}\n\t\treturn nil\n\t})\n\tassert.NoError(t, err)\n}\n\nfunc txZPop(t *testing.T, db *DB, bucket string, key []byte, isMax bool, expectVal []byte, expectScore float64, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\tvar (\n\t\t\tmember *SortedSetMember\n\t\t\terr    error\n\t\t)\n\t\tif isMax {\n\t\t\tmember, err = tx.ZPopMax(bucket, key)\n\t\t} else {\n\t\t\tmember, err = tx.ZPopMin(bucket, key)\n\t\t}\n\n\t\tif expectErr != nil {\n\t\t\tassert.Equal(t, expectErr, err)\n\t\t} else {\n\t\t\tassert.Equal(t, expectVal, member.Value)\n\t\t\tassert.Equal(t, expectScore, member.Score)\n\t\t}\n\t\treturn nil\n\t})\n\tassert.NoError(t, err)\n}\n\nfunc txZPeekMin(t *testing.T, db *DB, bucket string, key, expectVal []byte, expectScore float64, expectErr, finalExpectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tminMem, err1 := tx.ZPeekMin(bucket, key)\n\t\tassertErr(t, err1, finalExpectErr)\n\n\t\tif expectErr == nil {\n\t\t\trequire.Equal(t, &SortedSetMember{\n\t\t\t\tValue: expectVal,\n\t\t\t\tScore: expectScore,\n\t\t\t}, minMem)\n\t\t}\n\t\treturn err1\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txZKeys(t *testing.T, db *DB, bucket, pattern string, f func(key string) bool, expectVal int, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tpatternMatchNum := 0\n\t\terr := tx.ZKeys(bucket, pattern, func(key string) bool {\n\t\t\tpatternMatchNum += 1\n\t\t\treturn f(key)\n\t\t})\n\t\tif expectErr != nil {\n\t\t\tassert.ErrorIs(t, expectErr, err)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.Equal(t, expectVal, patternMatchNum)\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txPop(t *testing.T, db *DB, bucket string, key, expectVal []byte, expectErr error, isLeft bool) {\n\terr := db.Update(func(tx *Tx) error {\n\t\tvar item []byte\n\t\tvar err error\n\n\t\tif isLeft {\n\t\t\titem, err = tx.LPop(bucket, key)\n\t\t} else {\n\t\t\titem, err = tx.RPop(bucket, key)\n\t\t}\n\n\t\tif expectErr != nil {\n\t\t\trequire.Equal(t, expectErr, err)\n\t\t} else {\n\t\t\trequire.Equal(t, expectVal, item)\n\t\t}\n\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txPush(t *testing.T, db *DB, bucket string, key, val []byte, isLeft bool, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\tvar err error\n\n\t\tif isLeft {\n\t\t\terr = tx.LPush(bucket, key, val)\n\t\t} else {\n\t\t\terr = tx.RPush(bucket, key, val)\n\t\t}\n\n\t\tassertErr(t, err, expectErr)\n\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txMPush(t *testing.T, db *DB, bucket string, key []byte, vals [][]byte, isLeft bool, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\tvar err error\n\n\t\tif isLeft {\n\t\t\terr = tx.LPush(bucket, key, vals...)\n\t\t} else {\n\t\t\terr = tx.RPush(bucket, key, vals...)\n\t\t}\n\n\t\tassertErr(t, err, expectErr)\n\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txPushRaw(t *testing.T, db *DB, bucket string, key, val []byte, isLeft bool, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\tvar err error\n\n\t\tif isLeft {\n\t\t\terr = tx.LPushRaw(bucket, key, val)\n\t\t} else {\n\t\t\terr = tx.RPushRaw(bucket, key, val)\n\t\t}\n\n\t\tassertErr(t, err, expectErr)\n\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txExpireList(t *testing.T, db *DB, bucket string, key []byte, ttl uint32, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.ExpireList(bucket, key, ttl)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txGetListTTL(t *testing.T, db *DB, bucket string, key []byte, expectVal uint32, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tttl, err := tx.GetListTTL(bucket, key)\n\t\tassertErr(t, err, expectErr)\n\t\trequire.Equal(t, ttl, expectVal)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txLKeys(t *testing.T, db *DB, bucket, pattern string, expectLen int, expectErr error, keysOperation func(keys []string) bool) {\n\terr := db.View(func(tx *Tx) error {\n\t\tvar keys []string\n\t\terr := tx.LKeys(bucket, pattern, func(key string) bool {\n\t\t\tkeys = append(keys, key)\n\t\t\treturn keysOperation(keys)\n\t\t})\n\t\tassertErr(t, err, expectErr)\n\t\trequire.Equal(t, expectLen, len(keys))\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txLRange(t *testing.T, db *DB, bucket string, key []byte, start, end, expectLen int, expectVal [][]byte, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tlist, err := tx.LRange(bucket, key, start, end)\n\t\tassertErr(t, err, expectErr)\n\n\t\trequire.Equal(t, expectLen, len(list))\n\n\t\tif len(expectVal) > 0 {\n\t\t\tfor i, val := range list {\n\t\t\t\tassert.Equal(t, expectVal[i], val)\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txLSize(t *testing.T, db *DB, bucket string, key []byte, expectVal int, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tsize, err := tx.LSize(bucket, key)\n\t\tassertErr(t, err, expectErr)\n\n\t\trequire.Equal(t, expectVal, size)\n\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txLTrim(t *testing.T, db *DB, bucket string, key []byte, start int, end int, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.LTrim(bucket, key, start, end)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txIterateBuckets(t *testing.T, db *DB, ds uint16, pattern string, f func(key string) bool, expectErr error, containsKey ...string) {\n\terr := db.View(func(tx *Tx) error {\n\t\tvar elements []string\n\t\terr := tx.IterateBuckets(ds, pattern, func(key string) bool {\n\t\t\tif f != nil && !f(key) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\telements = append(elements, key)\n\t\t\treturn true\n\t\t})\n\t\tif err != nil {\n\t\t\tassert.Equal(t, expectErr, err)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tfor _, key := range containsKey {\n\t\t\t\tassert.Contains(t, elements, key)\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc TestDB_GetKeyNotFound(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tbucket := \"bucket\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\ttxGet(t, db, bucket, GetTestBytes(0), nil, ErrKeyNotFound)\n\t\ttxPut(t, db, bucket, GetTestBytes(1), GetRandomBytes(24), Persistent, nil, nil)\n\t\ttxGet(t, db, bucket, GetTestBytes(0), nil, ErrKeyNotFound)\n\t})\n}\n\nfunc TestDB_Backup(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tbackUpDir := \"/tmp/nutsdb-backup\"\n\t\trequire.NoError(t, db.Backup(backUpDir))\n\t})\n}\n\nfunc TestDB_BackupTarGZ(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tbackUpFile := \"/tmp/nutsdb-backup/backup.tar.gz\"\n\t\tf, err := os.Create(backUpFile)\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, db.BackupTarGZ(f))\n\t})\n}\n\nfunc TestDB_Close(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\trequire.NoError(t, db.Close())\n\t\trequire.Equal(t, ErrDBClosed, db.Close())\n\t})\n}\n\nfunc TestDB_ErrThenReadWrite(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tbucket := \"testForDeadLock\"\n\t\terr = db.View(\n\t\t\tfunc(tx *Tx) error {\n\t\t\t\treturn fmt.Errorf(\"err happened\")\n\t\t\t})\n\t\trequire.NotNil(t, err)\n\n\t\terr = db.View(\n\t\t\tfunc(tx *Tx) error {\n\t\t\t\tkey := []byte(\"key1\")\n\t\t\t\t_, err := tx.Get(bucket, key)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\treturn nil\n\t\t\t})\n\t\trequire.NotNil(t, err)\n\n\t\tnotice := make(chan struct{})\n\t\tgo func() {\n\t\t\terr = db.Update(\n\t\t\t\tfunc(tx *Tx) error {\n\t\t\t\t\tnotice <- struct{}{}\n\n\t\t\t\t\treturn nil\n\t\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t}()\n\n\t\tselect {\n\t\tcase <-notice:\n\t\tcase <-time.After(1 * time.Second):\n\t\t\tt.Fatalf(\"exist deadlock\")\n\t\t}\n\t})\n}\n\nfunc TestDB_ErrorHandler(t *testing.T) {\n\topts := DefaultOptions\n\thandleErrCalled := false\n\topts.ErrorHandler = ErrorHandlerFunc(func(err error) {\n\t\thandleErrCalled = true\n\t})\n\n\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\terr = db.View(\n\t\t\tfunc(tx *Tx) error {\n\t\t\t\treturn fmt.Errorf(\"err happened\")\n\t\t\t})\n\t\trequire.NotNil(t, err)\n\t\trequire.Equal(t, handleErrCalled, true)\n\t})\n}\n\nfunc TestDB_CommitBuffer(t *testing.T) {\n\tbucket := \"bucket\"\n\n\topts := DefaultOptions\n\topts.CommitBufferSize = 8 * MB\n\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\trequire.Equal(t, int64(8*MB), db.opt.CommitBufferSize)\n\t\t// When the database starts, the commit buffer should be allocated with the size of CommitBufferSize.\n\t\trequire.Equal(t, 0, db.commitBuffer.Len())\n\t\trequire.Equal(t, db.opt.CommitBufferSize, int64(db.commitBuffer.Cap()))\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\ttxPut(t, db, bucket, GetTestBytes(0), GetRandomBytes(24), Persistent, nil, nil)\n\n\t\t// When tx is committed, content of commit buffer should be empty, but do not release memory\n\t\trequire.Equal(t, 0, db.commitBuffer.Len())\n\t\trequire.Equal(t, db.opt.CommitBufferSize, int64(db.commitBuffer.Cap()))\n\t})\n\n\topts = DefaultOptions\n\topts.CommitBufferSize = 1 * KB\n\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\trequire.Equal(t, int64(1*KB), db.opt.CommitBufferSize)\n\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\terr := db.Update(func(tx *Tx) error {\n\t\t\t// making this tx big enough, it should not use the commit buffer\n\t\t\tfor i := 0; i < 1000; i++ {\n\t\t\t\terr := tx.Put(bucket, GetTestBytes(i), GetRandomBytes(1024), Persistent)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\trequire.Equal(t, 0, db.commitBuffer.Len())\n\t\trequire.Equal(t, db.opt.CommitBufferSize, int64(db.commitBuffer.Cap()))\n\t})\n}\n\nfunc TestDB_DeleteBucket(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tbucket := \"bucket\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\tkey := GetTestBytes(0)\n\t\tval := GetTestBytes(0)\n\t\ttxPut(t, db, bucket, key, val, Persistent, nil, nil)\n\t\ttxGet(t, db, bucket, key, val, nil)\n\n\t\ttxDeleteBucket(t, db, DataStructureBTree, bucket, nil)\n\t\ttxPut(t, db, bucket, key, val, Persistent, ErrorBucketNotExist, nil)\n\t})\n}\n\nfunc withDBOption(t *testing.T, opt Options, fn func(t *testing.T, db *DB)) {\n\tdb, err := Open(opt)\n\trequire.NoError(t, err)\n\n\tdefer func() {\n\t\tos.RemoveAll(db.opt.Dir)\n\t\tdb.Close()\n\t}()\n\n\tfn(t, db)\n}\n\nfunc withDefaultDB(t *testing.T, fn func(t *testing.T, db *DB)) {\n\ttmpdir, _ := os.MkdirTemp(\"\", \"nutsdb\")\n\topt := DefaultOptions\n\topt.Dir = tmpdir\n\topt.SegmentSize = 8 * 1024\n\n\twithDBOption(t, opt, fn)\n}\n\nfunc withRAMIdxDB(t *testing.T, fn func(t *testing.T, db *DB)) {\n\ttmpdir, _ := os.MkdirTemp(\"\", \"nutsdb\")\n\topt := DefaultOptions\n\topt.Dir = tmpdir\n\topt.EntryIdxMode = HintKeyAndRAMIdxMode\n\n\twithDBOption(t, opt, fn)\n}\n\nfunc TestDB_HintKeyValAndRAMIdxMode_RestartDB(t *testing.T) {\n\topts := DefaultOptions\n\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\tbucket := \"bucket\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\tkey := GetTestBytes(0)\n\t\tval := GetTestBytes(0)\n\n\t\ttxPut(t, db, bucket, key, val, Persistent, nil, nil)\n\t\ttxGet(t, db, bucket, key, val, nil)\n\n\t\tdb.Close()\n\t\t// restart db with HintKeyValAndRAMIdxMode EntryIdxMode\n\t\tdb, err := Open(db.opt)\n\t\trequire.NoError(t, err)\n\t\ttxGet(t, db, bucket, key, val, nil)\n\t})\n}\n\nfunc TestDB_HintKeyAndRAMIdxMode_RestartDB(t *testing.T) {\n\topts := DefaultOptions\n\topts.EntryIdxMode = HintKeyAndRAMIdxMode\n\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\tbucket := \"bucket\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\tkey := GetTestBytes(0)\n\t\tval := GetTestBytes(0)\n\n\t\ttxPut(t, db, bucket, key, val, Persistent, nil, nil)\n\t\ttxGet(t, db, bucket, key, val, nil)\n\t\tdb.Close()\n\n\t\t// restart db with HintKeyAndRAMIdxMode EntryIdxMode\n\t\tdb, err := Open(db.opt)\n\t\trequire.NoError(t, err)\n\t\ttxGet(t, db, bucket, key, val, nil)\n\t})\n}\n\nfunc TestDB_HintKeyAndRAMIdxMode_LruCache(t *testing.T) {\n\topts := DefaultOptions\n\topts.EntryIdxMode = HintKeyAndRAMIdxMode\n\tlruCacheSizes := []int{0, 5000, 10000, 20000}\n\n\tfor _, lruCacheSize := range lruCacheSizes {\n\t\topts.HintKeyAndRAMIdxCacheSize = lruCacheSize\n\t\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\t\tbucket := \"bucket\"\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\tfor i := 0; i < 10000; i++ {\n\t\t\t\tkey := []byte(fmt.Sprintf(\"%10d\", i))\n\t\t\t\tval := []byte(fmt.Sprintf(\"%10d\", i))\n\t\t\t\ttxPut(t, db, bucket, key, val, Persistent, nil, nil)\n\t\t\t\ttxGet(t, db, bucket, key, val, nil)\n\t\t\t\ttxGet(t, db, bucket, key, val, nil)\n\t\t\t}\n\t\t\tdb.Close()\n\t\t})\n\t}\n}\n\nfunc TestDB_ChangeMode_RestartDB(t *testing.T) {\n\tchangeModeRestart := func(firstMode EntryIdxMode, secondMode EntryIdxMode) {\n\t\topts := DefaultOptions\n\t\topts.EntryIdxMode = firstMode\n\t\tvar err error\n\n\t\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\t\tbucket := \"bucket\"\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\n\t\t\t// k-v\n\t\t\tfor i := 0; i < 10; i++ {\n\t\t\t\ttxPut(t, db, bucket, GetTestBytes(i), GetTestBytes(i), Persistent, nil, nil)\n\t\t\t}\n\n\t\t\t// list\n\t\t\tfor i := 0; i < 10; i++ {\n\t\t\t\ttxPush(t, db, bucket, GetTestBytes(0), GetTestBytes(i), true, nil, nil)\n\t\t\t}\n\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\treturn tx.LRem(bucket, GetTestBytes(0), 1, GetTestBytes(5))\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\n\t\t\tfor i := 0; i < 2; i++ {\n\t\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(9-i), nil, true)\n\t\t\t}\n\n\t\t\tfor i := 0; i < 2; i++ {\n\t\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(i), nil, false)\n\t\t\t}\n\n\t\t\t// set\n\t\t\tfor i := 0; i < 10; i++ {\n\t\t\t\ttxSAdd(t, db, bucket, GetTestBytes(0), GetTestBytes(i), nil, nil)\n\t\t\t}\n\n\t\t\tfor i := 0; i < 3; i++ {\n\t\t\t\ttxSRem(t, db, bucket, GetTestBytes(0), GetTestBytes(i), nil)\n\t\t\t}\n\n\t\t\t// zset\n\t\t\tfor i := 0; i < 10; i++ {\n\t\t\t\ttxZAdd(t, db, bucket, GetTestBytes(0), GetTestBytes(i), float64(i), nil, nil)\n\t\t\t}\n\n\t\t\tfor i := 0; i < 3; i++ {\n\t\t\t\ttxZRem(t, db, bucket, GetTestBytes(0), GetTestBytes(i), nil)\n\t\t\t}\n\n\t\t\trequire.NoError(t, db.Close())\n\n\t\t\topts.EntryIdxMode = secondMode\n\t\t\tdb, err = Open(opts)\n\t\t\trequire.NoError(t, err)\n\n\t\t\t// k-v\n\t\t\tfor i := 0; i < 10; i++ {\n\t\t\t\ttxGet(t, db, bucket, GetTestBytes(i), GetTestBytes(i), nil)\n\t\t\t}\n\n\t\t\t// list\n\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(7), nil, true)\n\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(6), nil, true)\n\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(4), nil, true)\n\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(2), nil, false)\n\n\t\t\terr = db.View(func(tx *Tx) error {\n\t\t\t\tsize, err := tx.LSize(bucket, GetTestBytes(0))\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\trequire.Equal(t, 1, size)\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\n\t\t\t// set\n\t\t\tfor i := 0; i < 3; i++ {\n\t\t\t\ttxSIsMember(t, db, bucket, GetTestBytes(0), GetTestBytes(i), false)\n\t\t\t}\n\n\t\t\tfor i := 3; i < 10; i++ {\n\t\t\t\ttxSIsMember(t, db, bucket, GetTestBytes(0), GetTestBytes(i), true)\n\t\t\t}\n\n\t\t\t// zset\n\t\t\tfor i := 0; i < 3; i++ {\n\t\t\t\ttxZScore(t, db, bucket, GetTestBytes(0), GetTestBytes(i), float64(i), ErrSortedSetMemberNotExist)\n\t\t\t}\n\n\t\t\tfor i := 3; i < 10; i++ {\n\t\t\t\ttxZScore(t, db, bucket, GetTestBytes(0), GetTestBytes(i), float64(i), nil)\n\t\t\t}\n\t\t})\n\t}\n\n\t// HintKeyValAndRAMIdxMode to HintKeyAndRAMIdxMode\n\tchangeModeRestart(HintKeyValAndRAMIdxMode, HintKeyAndRAMIdxMode)\n\t// HintKeyAndRAMIdxMode to HintKeyValAndRAMIdxMode\n\tchangeModeRestart(HintKeyAndRAMIdxMode, HintKeyValAndRAMIdxMode)\n}\n\nfunc TestTx_SmallFile(t *testing.T) {\n\topts := DefaultOptions\n\topts.SegmentSize = 100\n\topts.EntryIdxMode = HintKeyAndRAMIdxMode\n\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\tbucket := \"bucket\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\terr := db.Update(func(tx *Tx) error {\n\t\t\tfor i := 0; i < 100; i++ {\n\t\t\t\terr := tx.Put(bucket, GetTestBytes(i), GetTestBytes(i), Persistent)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t\trequire.Nil(t, err)\n\t\trequire.NoError(t, db.Close())\n\t\tdb, _ = Open(opts)\n\n\t\ttxGet(t, db, bucket, GetTestBytes(10), GetTestBytes(10), nil)\n\t})\n}\n\nfunc TestDB_DataStructureBTreeWriteRecordLimit(t *testing.T) {\n\topts := DefaultOptions\n\tlimitCount := int64(1000)\n\topts.MaxWriteRecordCount = limitCount\n\tbucket1 := \"bucket1\"\n\tbucket2 := \"bucket2\"\n\t// Iterate over different EntryIdxModes\n\tfor _, idxMode := range []EntryIdxMode{HintKeyValAndRAMIdxMode, HintKeyAndRAMIdxMode} {\n\t\topts.EntryIdxMode = idxMode\n\t\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket1, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket2, nil)\n\n\t\t\t// Add limitCount records\n\t\t\terr := db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < int(limitCount); i++ {\n\t\t\t\t\tkey := []byte(strconv.Itoa(i))\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr = tx.Put(bucket1, key, value, Persistent)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\t// Trigger the limit\n\t\t\ttxPut(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), Persistent, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Add a key that is within the limit\n\t\t\ttxPut(t, db, bucket1, []byte(\"0\"), []byte(\"000\"), Persistent, nil, nil)\n\t\t\t// Delete and add one item\n\t\t\ttxDel(t, db, bucket1, []byte(\"0\"), nil)\n\t\t\ttxPut(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), Persistent, nil, nil)\n\t\t\t// Add an item to another bucket\n\t\t\ttxPut(t, db, bucket2, []byte(\"key2\"), []byte(\"value2\"), Persistent, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Delete bucket1\n\t\t\ttxDeleteBucket(t, db, DataStructureBTree, bucket1, nil)\n\t\t\t// Add data to bucket2\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < (int(limitCount) - 1); i++ {\n\t\t\t\t\tkey := []byte(strconv.Itoa(i))\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr = tx.Put(bucket2, key, value, Persistent)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\t// Add items to bucket2\n\t\t\ttxPut(t, db, bucket2, []byte(\"key1\"), []byte(\"value1\"), Persistent, nil, nil)\n\t\t\ttxPut(t, db, bucket2, []byte(\"key2\"), []byte(\"value2\"), Persistent, nil, ErrTxnExceedWriteLimit)\n\t\t})\n\t}\n}\n\nfunc TestDB_DataStructureListWriteRecordLimit(t *testing.T) {\n\t// Set options\n\topts := DefaultOptions\n\tlimitCount := int64(1000)\n\topts.MaxWriteRecordCount = limitCount\n\t// Define bucket names\n\tbucket1 := \"bucket1\"\n\tbucket2 := \"bucket2\"\n\t// Iterate over EntryIdxMode options\n\tfor _, idxMode := range []EntryIdxMode{HintKeyValAndRAMIdxMode, HintKeyAndRAMIdxMode} {\n\n\t\topts.EntryIdxMode = idxMode\n\t\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket1, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket2, nil)\n\t\t\t// Add limitCount records\n\t\t\terr := db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < int(limitCount); i++ {\n\t\t\t\t\tkey := []byte(\"0\")\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr = tx.LPush(bucket1, key, value)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\t// Trigger the limit\n\t\t\ttxPush(t, db, bucket1, []byte(\"0\"), []byte(\"value1\"), false, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Test LRem\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\terr := tx.LRem(bucket1, []byte(\"0\"), 1, []byte(\"0\"))\n\t\t\t\tassertErr(t, err, nil)\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\ttxPush(t, db, bucket1, []byte(\"0\"), []byte(\"value1\"), true, nil, nil)\n\t\t\ttxPush(t, db, bucket1, []byte(\"0\"), []byte(\"value1\"), false, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Test for DataLPopFlag\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\t_, err := tx.LPop(bucket1, []byte(\"0\"))\n\t\t\t\tassertErr(t, err, nil)\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\ttxPush(t, db, bucket1, []byte(\"0\"), []byte(\"value1\"), false, nil, nil)\n\t\t\ttxPush(t, db, bucket1, []byte(\"0\"), []byte(\"value1\"), false, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Test for DataLTrimFlag\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\terr := tx.LTrim(bucket1, []byte(\"0\"), 0, 0)\n\t\t\t\tassertErr(t, err, nil)\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < int(limitCount)-2; i++ {\n\t\t\t\t\tkey := []byte(\"0\")\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr = tx.RPush(bucket1, key, value)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\ttxPush(t, db, bucket1, []byte(\"0\"), []byte(\"value11\"), false, nil, nil)\n\t\t\ttxPush(t, db, bucket1, []byte(\"0\"), []byte(\"value11\"), false, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Test for LRemByIndex\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\terr := tx.LRemByIndex(bucket1, []byte(\"0\"), 0, 1, 2)\n\t\t\t\tassertErr(t, err, nil)\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < 2; i++ {\n\t\t\t\t\tkey := []byte(\"0\")\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr = tx.RPush(bucket1, key, value)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\ttxPush(t, db, bucket2, []byte(\"0\"), []byte(\"value11\"), false, nil, nil)\n\t\t\ttxPush(t, db, bucket1, []byte(\"0\"), []byte(\"value11\"), false, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Delete bucket\n\t\t\ttxDeleteBucket(t, db, DataStructureList, bucket1, nil)\n\t\t\t// Add data to another bucket\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < int(limitCount)-1; i++ {\n\t\t\t\t\tkey := []byte(strconv.Itoa(i))\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr = tx.RPush(bucket2, key, value)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\ttxPush(t, db, bucket2, []byte(\"key1\"), []byte(\"value1\"), false, nil, ErrTxnExceedWriteLimit)\n\t\t})\n\t}\n}\n\nfunc TestDB_DataStructureSetWriteRecordLimit(t *testing.T) {\n\t// Set default options and limitCount.\n\topts := DefaultOptions\n\tlimitCount := int64(1000)\n\topts.MaxWriteRecordCount = limitCount\n\t// Define bucket names.\n\tbucket1 := \"bucket1\"\n\tbucket2 := \"bucket2\"\n\t// Loop through EntryIdxModes.\n\tfor _, idxMode := range []EntryIdxMode{HintKeyValAndRAMIdxMode, HintKeyAndRAMIdxMode} {\n\t\topts.EntryIdxMode = idxMode\n\t\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureSet, bucket1, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureSet, bucket2, nil)\n\n\t\t\t// Add limitCount records to bucket1.\n\t\t\terr := db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < int(limitCount); i++ {\n\t\t\t\t\tkey := []byte(\"0\")\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr := tx.SAdd(bucket1, key, value)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\t// Try to add one more item to bucket1 and check for ErrTxnExceedWriteLimit.\n\t\t\ttxSAdd(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), nil, ErrTxnExceedWriteLimit)\n\t\t\t// Remove one item and add another item to bucket1.\n\t\t\ttxSRem(t, db, bucket1, []byte(\"0\"), []byte(\"0\"), nil)\n\t\t\ttxSAdd(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), nil, nil)\n\t\t\t// Add two more items to bucket1 and check for ErrTxnExceedWriteLimit.\n\t\t\ttxSAdd(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), nil, nil)\n\t\t\ttxSAdd(t, db, bucket1, []byte(\"key11\"), []byte(\"value11\"), nil, ErrTxnExceedWriteLimit)\n\t\t\t// Test for SPOP, SPOP two items from bucket1.\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\t_, err := tx.SPop(bucket1, []byte(\"0\"))\n\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t_, err = tx.SPop(bucket1, []byte(\"key1\"))\n\t\t\t\tassertErr(t, err, nil)\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\t// Add two items to bucket1 and check for ErrTxnExceedWriteLimit.\n\t\t\ttxSAdd(t, db, bucket1, []byte(\"1\"), []byte(\"value1\"), nil, nil)\n\t\t\ttxSAdd(t, db, bucket1, []byte(\"1\"), []byte(\"value2\"), nil, nil)\n\t\t\ttxSAdd(t, db, bucket1, []byte(\"1\"), []byte(\"value3\"), nil, ErrTxnExceedWriteLimit)\n\t\t\t// Delete bucket1.\n\t\t\ttxDeleteBucket(t, db, DataStructureSet, bucket1, nil)\n\t\t\t// Add data to bucket2.\n\t\t\ttxSAdd(t, db, bucket2, []byte(\"key1\"), []byte(\"value1\"), nil, nil)\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < int(limitCount)-1; i++ {\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr = tx.SAdd(bucket2, []byte(\"2\"), value)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\t// Try to add one more item to bucket2 and check for ErrTxnExceedWriteLimit.\n\t\t\ttxSAdd(t, db, bucket2, []byte(\"key2\"), []byte(\"value2\"), nil, ErrTxnExceedWriteLimit)\n\t\t})\n\t}\n}\n\nfunc TestDB_DataStructureSortedSetWriteRecordLimit(t *testing.T) {\n\t// Set up options\n\topts := DefaultOptions\n\tlimitCount := int64(1000)\n\topts.MaxWriteRecordCount = limitCount\n\t// Set up bucket names and score\n\tbucket1 := \"bucket1\"\n\tbucket2 := \"bucket2\"\n\tscore := 1.0\n\t// Iterate over EntryIdxMode options\n\tfor _, idxMode := range []EntryIdxMode{HintKeyValAndRAMIdxMode, HintKeyAndRAMIdxMode} {\n\t\topts.EntryIdxMode = idxMode\n\t\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket1, nil)\n\t\t\t// Add limitCount records\n\t\t\terr := db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < int(limitCount); i++ {\n\t\t\t\t\tkey := []byte(\"0\")\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr := tx.ZAdd(bucket1, key, score+float64(i), value)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\t// Trigger the limit\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), score, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Delete and add one item\n\t\t\ttxZRem(t, db, bucket1, []byte(\"0\"), []byte(\"0\"), nil)\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), score, nil, nil)\n\t\t\t// Add some data is ok\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), score, nil, nil)\n\t\t\t// Trigger the limit\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"key2\"), []byte(\"value2\"), score, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Test for ZRemRangeByRank\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\terr := tx.ZRemRangeByRank(bucket1, []byte(\"0\"), 1, 3)\n\t\t\t\tassert.NoError(t, err)\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\tassert.NoError(t, err)\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"0\"), []byte(\"value1\"), score, nil, nil)\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"0\"), []byte(\"value2\"), score, nil, nil)\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"0\"), []byte(\"value3\"), score+float64(1000), nil, nil)\n\t\t\t// Trigger the limit\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"0\"), []byte(\"value4\"), score, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Test for ZPop\n\t\t\ttxZPop(t, db, bucket1, []byte(\"0\"), true, []byte(\"value3\"), score+float64(1000), nil)\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"key3\"), []byte(\"value3\"), score, nil, nil)\n\t\t\t// Delete bucket\n\t\t\ttxDeleteBucket(t, db, DataStructureSortedSet, bucket1, nil)\n\t\t\t// Add data to another bucket\n\t\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket1, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket2, nil)\n\t\t\ttxZAdd(t, db, bucket2, []byte(\"key1\"), []byte(\"value1\"), score, nil, nil)\n\t\t\t// Add data to bucket1\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < int(limitCount)-1; i++ {\n\t\t\t\t\tkey := []byte(strconv.Itoa(i))\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr = tx.ZAdd(bucket1, key, score, value)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\t// Trigger the limit\n\t\t\ttxZAdd(t, db, bucket2, []byte(\"key1\"), []byte(\"value2\"), score, nil, ErrTxnExceedWriteLimit)\n\t\t})\n\t}\n}\n\nfunc TestDB_AllDsWriteRecordLimit(t *testing.T) {\n\t// Set up options\n\topts := DefaultOptions\n\tlimitCount := int64(1000)\n\topts.MaxWriteRecordCount = limitCount\n\t// Set up bucket names and score\n\tbucket1 := \"bucket1\"\n\tbucket2 := \"bucket2\"\n\tscore := 1.0\n\t// Iterate over EntryIdxMode options\n\tfor _, idxMode := range []EntryIdxMode{HintKeyValAndRAMIdxMode, HintKeyAndRAMIdxMode} {\n\t\topts.EntryIdxMode = idxMode\n\t\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket1, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket1, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureSet, bucket1, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket1, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket2, nil)\n\n\t\t\t// Add limitCount records\n\t\t\terr := db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < int(limitCount); i++ {\n\t\t\t\t\tkey := []byte(strconv.Itoa(i))\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr = tx.Put(bucket1, key, value, Persistent)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\t// Trigger the limit\n\t\t\ttxPush(t, db, bucket1, []byte(\"0\"), []byte(\"value1\"), false, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Delete item and add one\n\t\t\ttxDel(t, db, bucket1, []byte(\"0\"), nil)\n\t\t\ttxPush(t, db, bucket1, []byte(\"0\"), []byte(\"value1\"), false, nil, nil)\n\t\t\t// Trigger the limit\n\t\t\ttxSAdd(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), nil, ErrTxnExceedWriteLimit)\n\t\t\t// Delete item and add one\n\t\t\ttxDel(t, db, bucket1, []byte(\"1\"), nil)\n\t\t\ttxSAdd(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), nil, nil)\n\t\t\t// Trigger the limit\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), score, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Delete item and add one\n\t\t\ttxDel(t, db, bucket1, []byte(\"2\"), nil)\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), score, nil, nil)\n\t\t\t// Delete bucket\n\t\t\ttxDeleteBucket(t, db, DataStructureSortedSet, bucket1, nil)\n\t\t\t// Add data to another bucket\n\t\t\ttxPush(t, db, bucket2, []byte(\"key1\"), []byte(\"value1\"), false, nil, nil)\n\t\t\t// Trigger the limit\n\t\t\ttxPush(t, db, bucket2, []byte(\"key2\"), []byte(\"value2\"), false, nil, ErrTxnExceedWriteLimit)\n\t\t})\n\t}\n}\n\nfunc txIncrement(t *testing.T, db *DB, bucket string, key []byte, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.Incr(bucket, key)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txDecrement(t *testing.T, db *DB, bucket string, key []byte, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.Decr(bucket, key)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txIncrementBy(t *testing.T, db *DB, bucket string, key []byte, value int64, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.IncrBy(bucket, key, value)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txDecrementBy(t *testing.T, db *DB, bucket string, key []byte, value int64, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.DecrBy(bucket, key, value)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txPutIfNotExists(t *testing.T, db *DB, bucket string, key, value []byte, expectedErr, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.PutIfNotExists(bucket, key, value, Persistent)\n\t\tassertErr(t, err, expectedErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txPutIfExists(t *testing.T, db *DB, bucket string, key, value []byte, expectedErr, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.PutIfExists(bucket, key, value, Persistent)\n\t\tassertErr(t, err, expectedErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txValueLen(t *testing.T, db *DB, bucket string, key []byte, expectLength int, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tlength, err := tx.ValueLen(bucket, key)\n\t\tif expectErr != nil {\n\t\t\trequire.Equal(t, expectErr, err)\n\t\t} else {\n\t\t\trequire.NoError(t, err)\n\t\t}\n\t\trequire.EqualValuesf(t, expectLength, length, \"err Tx ValueLen. got %s want %s\", length, expectLength)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txGetSet(t *testing.T, db *DB, bucket string, key, value []byte, expectOldValue []byte, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\toldValue, err := tx.GetSet(bucket, key, value)\n\t\tassertErr(t, err, expectErr)\n\t\trequire.EqualValuesf(t, oldValue, expectOldValue, \"err Tx GetSet. got %s want %s\", string(oldValue), string(expectOldValue))\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txGetBit(t *testing.T, db *DB, bucket string, key []byte, offset int, expectVal byte, expectErr error, finalExpectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tvalue, err := tx.GetBit(bucket, key, offset)\n\t\tassertErr(t, err, expectErr)\n\t\trequire.Equal(t, expectVal, value)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txSetBit(t *testing.T, db *DB, bucket string, key []byte, offset int, value byte, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.SetBit(bucket, key, offset, value)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txGetTTL(t *testing.T, db *DB, bucket string, key []byte, expectedTTL int64, expectedErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tttl, err := tx.GetTTL(bucket, key)\n\t\tassertErr(t, err, expectedErr)\n\n\t\t// If diff between expectedTTL and realTTL lesser than 1s, We'll consider as equal\n\t\tdiff := int(math.Abs(float64(ttl - expectedTTL)))\n\t\tassert.LessOrEqual(t, diff, 1)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txPersist(t *testing.T, db *DB, bucket string, key []byte, expectedErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.Persist(bucket, key)\n\t\tassertErr(t, err, expectedErr)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txMSet(t *testing.T, db *DB, bucket string, args [][]byte, ttl uint32, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.MSet(bucket, ttl, args...)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txMGet(t *testing.T, db *DB, bucket string, keys [][]byte, expectValues [][]byte, expectErr error, finalExpectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tvalues, err := tx.MGet(bucket, keys...)\n\t\tassertErr(t, err, expectErr)\n\t\trequire.EqualValues(t, expectValues, values)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txAppend(t *testing.T, db *DB, bucket string, key, appendage []byte, expectErr error, expectFinalErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.Append(bucket, key, appendage)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, expectFinalErr)\n}\n\nfunc txGetRange(t *testing.T, db *DB, bucket string, key []byte, start, end int, expectVal []byte, expectErr error, expectFinalErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tvalue, err := tx.GetRange(bucket, key, start, end)\n\t\tassertErr(t, err, expectErr)\n\t\trequire.EqualValues(t, expectVal, value)\n\t\treturn nil\n\t})\n\tassertErr(t, err, expectFinalErr)\n}\n"
        },
        {
          "name": "doc.go",
          "type": "blob",
          "size": 0.80859375,
          "content": "/*\nPackage nutsdb implements a simple, fast, embeddable and persistent key/value store\nwritten in pure Go. It supports fully serializable transactions.\nAnd it also supports data structure such as listã€setã€sorted set etc.\n\nNutsDB currently works on Mac OS, Linux and Windows.\n\nUsage\n\nNutsDB has the following main types: DB, BPTree, Entry, DataFile And Tx. and NutsDB supports bucket, A bucket is\na collection of unique keys that are associated with values.\n\nAll operations happen inside a Tx. Tx represents a transaction, which can\nbe read-only or read-write. Read-only transactions can read values for a\ngiven key , or iterate over a set of key-value pairs (prefix scanning or range scanning).\nread-write transactions can also update and delete keys from the DB.\n\nSee the examples for more usage details.\n*/\npackage nutsdb\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "entity_utils.go",
          "type": "blob",
          "size": 0.6689453125,
          "content": "package nutsdb\n\nimport \"reflect\"\n\nfunc GetDiskSizeFromSingleObject(obj interface{}) int64 {\n\ttyp := reflect.TypeOf(obj)\n\tfields := reflect.VisibleFields(typ)\n\tif len(fields) == 0 {\n\t\treturn 0\n\t}\n\tvar size int64 = 0\n\tfor _, field := range fields {\n\t\t// Currently, we only use the unsigned value type for our metadata.go. That's reasonable for us.\n\t\t// Because it's not possible to use negative value mark the size of data.\n\t\t// But if you want to make it more flexible, please help yourself.\n\t\tswitch field.Type.Kind() {\n\t\tcase reflect.Uint8:\n\t\t\tsize += 1\n\t\tcase reflect.Uint16:\n\t\t\tsize += 2\n\t\tcase reflect.Uint32:\n\t\t\tsize += 4\n\t\tcase reflect.Uint64:\n\t\t\tsize += 8\n\t\t}\n\t}\n\treturn size\n}\n"
        },
        {
          "name": "entity_utils_test.go",
          "type": "blob",
          "size": 0.537109375,
          "content": "package nutsdb\n\nimport (\n\t\"github.com/stretchr/testify/assert\"\n\t\"testing\"\n)\n\nfunc TestGetDiskSizeFromSingleObject(t *testing.T) {\n\ttype args struct {\n\t\tobj interface{}\n\t}\n\ttests := []struct {\n\t\tname string\n\t\targs args\n\t\twant int64\n\t}{\n\t\t{\n\t\t\tname: \"happy path for getting entry header size\",\n\t\t\targs: args{\n\t\t\t\tobj: MetaData{},\n\t\t\t},\n\t\t\twant: 50,\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tassert.Equalf(t, tt.want, GetDiskSizeFromSingleObject(tt.args.obj), \"GetDiskSizeFromSingleObject(%v)\", tt.args.obj)\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "entry.go",
          "type": "blob",
          "size": 8.33203125,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"hash/crc32\"\n\t\"sort\"\n\t\"strings\"\n\n\t\"github.com/xujiajun/utils/strconv2\"\n)\n\nvar (\n\tErrPayLoadSizeMismatch   = errors.New(\"the payload size in Meta mismatch with the payload size needed\")\n\tErrHeaderSizeOutOfBounds = errors.New(\"the header size is out of bounds\")\n)\n\nconst (\n\tMaxEntryHeaderSize = 4 + binary.MaxVarintLen32*3 + binary.MaxVarintLen64*3 + binary.MaxVarintLen16*3\n\tMinEntryHeaderSize = 4 + 9\n)\n\ntype (\n\t// Entry represents the data item.\n\tEntry struct {\n\t\tKey   []byte\n\t\tValue []byte\n\t\tMeta  *MetaData\n\t}\n)\n\n// Size returns the size of the entry.\nfunc (e *Entry) Size() int64 {\n\treturn e.Meta.Size() + int64(e.Meta.KeySize+e.Meta.ValueSize)\n}\n\n// Encode returns the slice after the entry be encoded.\n//\n//\tthe entry stored format:\n//\t|----------------------------------------------------------------------------------------------------------|\n//\t|  crc  | timestamp | ksz | valueSize | flag  | TTL  | status | ds   | txId |  bucketId |  key  | value    |\n//\t|----------------------------------------------------------------------------------------------------------|\n//\t| uint32| uint64  |uint32 |  uint32 | uint16  | uint32| uint16 | uint16 |uint64 | uint64 | []byte | []byte |\n//\t|----------------------------------------------------------------------------------------------------------|\nfunc (e *Entry) Encode() []byte {\n\tkeySize := e.Meta.KeySize\n\tvalueSize := e.Meta.ValueSize\n\n\tbuf := make([]byte, MaxEntryHeaderSize+keySize+valueSize)\n\n\tindex := e.setEntryHeaderBuf(buf)\n\tcopy(buf[index:], e.Key)\n\tindex += int(keySize)\n\tcopy(buf[index:], e.Value)\n\tindex += int(valueSize)\n\n\tbuf = buf[:index]\n\n\tc32 := crc32.ChecksumIEEE(buf[4:])\n\tbinary.LittleEndian.PutUint32(buf[0:4], c32)\n\n\treturn buf\n}\n\n// setEntryHeaderBuf sets the entry header buff.\nfunc (e *Entry) setEntryHeaderBuf(buf []byte) int {\n\tindex := 4\n\n\tindex += binary.PutUvarint(buf[index:], e.Meta.Timestamp)\n\tindex += binary.PutUvarint(buf[index:], uint64(e.Meta.KeySize))\n\tindex += binary.PutUvarint(buf[index:], uint64(e.Meta.ValueSize))\n\tindex += binary.PutUvarint(buf[index:], uint64(e.Meta.Flag))\n\tindex += binary.PutUvarint(buf[index:], uint64(e.Meta.TTL))\n\tindex += binary.PutUvarint(buf[index:], uint64(e.Meta.Status))\n\tindex += binary.PutUvarint(buf[index:], uint64(e.Meta.Ds))\n\tindex += binary.PutUvarint(buf[index:], e.Meta.TxID)\n\tindex += binary.PutUvarint(buf[index:], e.Meta.BucketId)\n\n\treturn index\n}\n\n// IsZero checks if the entry is zero or not.\nfunc (e *Entry) IsZero() bool {\n\tif e.Meta.Crc == 0 && e.Meta.KeySize == 0 && e.Meta.ValueSize == 0 && e.Meta.Timestamp == 0 {\n\t\treturn true\n\t}\n\treturn false\n}\n\n// GetCrc returns the crc at given buf slice.\nfunc (e *Entry) GetCrc(buf []byte) uint32 {\n\tcrc := crc32.ChecksumIEEE(buf[4:])\n\tcrc = crc32.Update(crc, crc32.IEEETable, e.Key)\n\tcrc = crc32.Update(crc, crc32.IEEETable, e.Value)\n\n\treturn crc\n}\n\n// ParsePayload means this function will parse a byte array to bucket, key, size of an entry\nfunc (e *Entry) ParsePayload(data []byte) error {\n\tmeta := e.Meta\n\tkeyLowBound := 0\n\tkeyHighBound := meta.KeySize\n\tvalueLowBound := keyHighBound\n\tvalueHighBound := meta.KeySize + meta.ValueSize\n\n\t// parse key\n\te.Key = data[keyLowBound:keyHighBound]\n\t// parse value\n\te.Value = data[valueLowBound:valueHighBound]\n\treturn nil\n}\n\n// checkPayloadSize checks the payload size\nfunc (e *Entry) checkPayloadSize(size int64) error {\n\tif e.Meta.PayloadSize() != size {\n\t\treturn ErrPayLoadSizeMismatch\n\t}\n\treturn nil\n}\n\n// ParseMeta parse Meta object to entry\nfunc (e *Entry) ParseMeta(buf []byte) (int64, error) {\n\t// If the length of the header is less than MinEntryHeaderSize,\n\t// it means that the final remaining capacity of the file is not enough to write a record,\n\t// and an error needs to be returned.\n\tif len(buf) < MinEntryHeaderSize {\n\t\treturn 0, ErrHeaderSizeOutOfBounds\n\t}\n\n\te.Meta = NewMetaData()\n\n\te.Meta.WithCrc(binary.LittleEndian.Uint32(buf[0:4]))\n\n\tindex := 4\n\n\ttimestamp, n := binary.Uvarint(buf[index:])\n\tindex += n\n\tkeySize, n := binary.Uvarint(buf[index:])\n\tindex += n\n\tvalueSize, n := binary.Uvarint(buf[index:])\n\tindex += n\n\tflag, n := binary.Uvarint(buf[index:])\n\tindex += n\n\tttl, n := binary.Uvarint(buf[index:])\n\tindex += n\n\tstatus, n := binary.Uvarint(buf[index:])\n\tindex += n\n\tds, n := binary.Uvarint(buf[index:])\n\tindex += n\n\ttxId, n := binary.Uvarint(buf[index:])\n\tindex += n\n\tbucketId, n := binary.Uvarint(buf[index:])\n\tindex += n\n\n\te.Meta.\n\t\tWithTimeStamp(timestamp).\n\t\tWithKeySize(uint32(keySize)).\n\t\tWithValueSize(uint32(valueSize)).\n\t\tWithFlag(uint16(flag)).\n\t\tWithTTL(uint32(ttl)).\n\t\tWithStatus(uint16(status)).\n\t\tWithDs(uint16(ds)).\n\t\tWithTxID(txId).\n\t\tWithBucketId(bucketId)\n\n\treturn int64(index), nil\n}\n\n// isFilter to confirm if this entry is can be filtered\nfunc (e *Entry) isFilter() bool {\n\tmeta := e.Meta\n\tvar filterDataSet = []uint16{\n\t\tDataDeleteFlag,\n\t\tDataRPopFlag,\n\t\tDataLPopFlag,\n\t\tDataLRemFlag,\n\t\tDataLTrimFlag,\n\t\tDataZRemFlag,\n\t\tDataZRemRangeByRankFlag,\n\t\tDataZPopMaxFlag,\n\t\tDataZPopMinFlag,\n\t\tDataLRemByIndex,\n\t}\n\treturn OneOfUint16Array(meta.Flag, filterDataSet)\n}\n\n// valid check the entry fields valid or not\nfunc (e *Entry) valid() error {\n\tif len(e.Key) == 0 {\n\t\treturn ErrKeyEmpty\n\t}\n\tif len(e.Key) > MAX_SIZE || len(e.Value) > MAX_SIZE {\n\t\treturn ErrDataSizeExceed\n\t}\n\treturn nil\n}\n\n// NewEntry new Entry Object\nfunc NewEntry() *Entry {\n\treturn new(Entry)\n}\n\n// WithKey set key to Entry\nfunc (e *Entry) WithKey(key []byte) *Entry {\n\te.Key = key\n\treturn e\n}\n\n// WithValue set value to Entry\nfunc (e *Entry) WithValue(value []byte) *Entry {\n\te.Value = value\n\treturn e\n}\n\n// WithMeta set Meta to Entry\nfunc (e *Entry) WithMeta(meta *MetaData) *Entry {\n\te.Meta = meta\n\treturn e\n}\n\n// GetTxIDBytes return the bytes of TxID\nfunc (e *Entry) GetTxIDBytes() []byte {\n\treturn []byte(strconv2.Int64ToStr(int64(e.Meta.TxID)))\n}\n\nfunc (e *Entry) IsBelongsToBPlusTree() bool {\n\treturn e.Meta.IsBPlusTree()\n}\n\nfunc (e *Entry) IsBelongsToList() bool {\n\treturn e.Meta.IsList()\n}\n\nfunc (e *Entry) IsBelongsToSet() bool {\n\treturn e.Meta.IsSet()\n}\n\nfunc (e *Entry) IsBelongsToSortSet() bool {\n\treturn e.Meta.IsSortSet()\n}\n\n// Entries represents entries\ntype Entries []*Entry\n\nfunc (e Entries) Len() int { return len(e) }\n\nfunc (e Entries) Less(i, j int) bool {\n\tl := string(e[i].Key)\n\tr := string(e[j].Key)\n\n\treturn strings.Compare(l, r) == -1\n}\n\nfunc (e Entries) Swap(i, j int) { e[i], e[j] = e[j], e[i] }\n\nfunc (e Entries) processEntriesScanOnDisk() (result []*Entry) {\n\tsort.Sort(e)\n\tfor _, ele := range e {\n\t\tcurE := ele\n\t\tif !IsExpired(curE.Meta.TTL, curE.Meta.Timestamp) && curE.Meta.Flag != DataDeleteFlag {\n\t\t\tresult = append(result, curE)\n\t\t}\n\t}\n\n\treturn result\n}\n\nfunc (e Entries) ToCEntries(lFunc func(l, r string) bool) CEntries {\n\treturn CEntries{\n\t\tEntries:  e,\n\t\tLessFunc: lFunc,\n\t}\n}\n\ntype CEntries struct {\n\tEntries\n\tLessFunc func(l, r string) bool\n}\n\nfunc (c CEntries) Len() int { return len(c.Entries) }\n\nfunc (c CEntries) Less(i, j int) bool {\n\tl := string(c.Entries[i].Key)\n\tr := string(c.Entries[j].Key)\n\tif c.LessFunc != nil {\n\t\treturn c.LessFunc(l, r)\n\t}\n\n\treturn c.Entries.Less(i, j)\n}\n\nfunc (c CEntries) Swap(i, j int) { c.Entries[i], c.Entries[j] = c.Entries[j], c.Entries[i] }\n\nfunc (c CEntries) processEntriesScanOnDisk() (result []*Entry) {\n\tsort.Sort(c)\n\tfor _, ele := range c.Entries {\n\t\tcurE := ele\n\t\tif !IsExpired(curE.Meta.TTL, curE.Meta.Timestamp) && curE.Meta.Flag != DataDeleteFlag {\n\t\t\tresult = append(result, curE)\n\t\t}\n\t}\n\n\treturn result\n}\n\ntype EntryWhenRecovery struct {\n\tEntry\n\tfid int64\n\toff int64\n}\n\ntype dataInTx struct {\n\tes       []*EntryWhenRecovery\n\ttxId     uint64\n\tstartOff int64\n}\n\nfunc (dt *dataInTx) isSameTx(e *EntryWhenRecovery) bool {\n\treturn dt.txId == e.Meta.TxID\n}\n\nfunc (dt *dataInTx) appendEntry(e *EntryWhenRecovery) {\n\tdt.es = append(dt.es, e)\n}\n\nfunc (dt *dataInTx) reset() {\n\tdt.es = make([]*EntryWhenRecovery, 0)\n\tdt.txId = 0\n}\n"
        },
        {
          "name": "entry_test.go",
          "type": "blob",
          "size": 3.2880859375,
          "content": "// Copyright 2019 The nutsdb Authors. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"encoding/binary\"\n\t\"reflect\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/suite\"\n)\n\ntype EntryTestSuite struct {\n\tsuite.Suite\n\tentry          Entry\n\texpectedEncode []byte\n}\n\nfunc (suite *EntryTestSuite) SetupSuite() {\n\tsuite.entry = Entry{\n\t\tKey:   []byte(\"key_0001\"),\n\t\tValue: []byte(\"val_0001\"),\n\t\tMeta: NewMetaData().WithKeySize(uint32(len(\"key_0001\"))).\n\t\t\tWithValueSize(uint32(len(\"val_0001\"))).WithTimeStamp(1547707905).WithTTL(Persistent).WithFlag(DataSetFlag).WithBucketId(1),\n\t}\n\tsuite.expectedEncode = []byte{168, 1, 59, 122, 129, 204, 128, 226, 5, 8, 8, 1, 0, 0, 0, 0, 1, 107, 101, 121, 95, 48, 48, 48, 49, 118, 97, 108, 95, 48, 48, 48, 49}\n}\n\nfunc (suite *EntryTestSuite) TestEncode() {\n\tok := reflect.DeepEqual(suite.entry.Encode(), suite.expectedEncode)\n\tassert.True(suite.T(), ok, \"entry's encode test fail\")\n}\n\nfunc (suite *EntryTestSuite) TestIsZero() {\n\n\tif ok := suite.entry.IsZero(); ok {\n\t\tassert.Fail(suite.T(), \"entry's IsZero test fail\")\n\t}\n\n}\n\nfunc (suite *EntryTestSuite) TestGetCrc() {\n\n\theaderSize := suite.entry.Meta.Size()\n\tcrc1 := suite.entry.GetCrc(suite.expectedEncode[:headerSize])\n\tcrc2 := binary.LittleEndian.Uint32(suite.expectedEncode[:4])\n\n\tif crc1 != crc2 {\n\t\tassert.Fail(suite.T(), \"entry's GetCrc test fail\")\n\t}\n}\n\nfunc TestEntrySuit(t *testing.T) {\n\tsuite.Run(t, new(EntryTestSuite))\n}\n\nfunc TestEntries_processEntriesScanOnDisk(t *testing.T) {\n\ttests := []struct {\n\t\tname       string\n\t\te          Entries\n\t\twantResult []*Entry\n\t}{\n\t\t{\n\t\t\t\"sort\",\n\t\t\tEntries{\n\t\t\t\t{\n\t\t\t\t\tKey:  []byte(\"abc\"),\n\t\t\t\t\tMeta: NewMetaData().WithTTL(0).WithFlag(DataSetFlag),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tKey:  []byte(\"z\"),\n\t\t\t\t\tMeta: NewMetaData().WithTTL(0).WithFlag(DataSetFlag),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tKey:  []byte(\"abcd\"),\n\t\t\t\t\tMeta: NewMetaData().WithTTL(0).WithFlag(DataSetFlag),\n\t\t\t\t},\n\t\t\t},\n\t\t\t[]*Entry{\n\t\t\t\t{\n\t\t\t\t\tKey:  []byte(\"abc\"),\n\t\t\t\t\tMeta: NewMetaData().WithTTL(0).WithFlag(DataSetFlag),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tKey:  []byte(\"abcd\"),\n\t\t\t\t\tMeta: NewMetaData().WithTTL(0).WithFlag(DataSetFlag),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tKey:  []byte(\"z\"),\n\t\t\t\t\tMeta: NewMetaData().WithTTL(0).WithFlag(DataSetFlag),\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"expired\",\n\t\t\tEntries{\n\t\t\t\t{\n\t\t\t\t\tKey:  []byte(\"abc\"),\n\t\t\t\t\tMeta: NewMetaData().WithTTL(1),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tKey:  []byte(\"abc\"),\n\t\t\t\t\tMeta: NewMetaData().WithTTL(0).WithFlag(DataDeleteFlag),\n\t\t\t\t},\n\t\t\t},\n\t\t\tnil,\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tassert.Equalf(t, tt.wantResult, tt.e.processEntriesScanOnDisk(), \"processEntriesScanOnDisk()\")\n\t\t})\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tassert.Equalf(t, tt.wantResult, tt.e.ToCEntries(nil).processEntriesScanOnDisk(), \"CEntries.processEntriesScanOnDisk()\")\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "errors.go",
          "type": "blob",
          "size": 1.017578125,
          "content": "package nutsdb\n\nimport (\n\t\"errors\"\n)\n\n// IsDBClosed is true if the error indicates the db was closed.\nfunc IsDBClosed(err error) bool {\n\treturn errors.Is(err, ErrDBClosed)\n}\n\n// IsKeyNotFound is true if the error indicates the key is not found.\nfunc IsKeyNotFound(err error) bool {\n\treturn errors.Is(err, ErrKeyNotFound)\n}\n\n// IsBucketNotFound is true if the error indicates the bucket is not exists.\nfunc IsBucketNotFound(err error) bool {\n\treturn errors.Is(err, ErrBucketNotFound)\n}\n\n// IsBucketEmpty is true if the bucket is empty.\nfunc IsBucketEmpty(err error) bool {\n\treturn errors.Is(err, ErrBucketEmpty)\n}\n\n// IsKeyEmpty is true if the key is empty.\nfunc IsKeyEmpty(err error) bool {\n\treturn errors.Is(err, ErrKeyEmpty)\n}\n\n// IsPrefixScan is true if prefix scanning not found the result.\nfunc IsPrefixScan(err error) bool {\n\treturn errors.Is(err, ErrPrefixScan)\n}\n\n// IsPrefixSearchScan is true if prefix and search scanning not found the result.\nfunc IsPrefixSearchScan(err error) bool {\n\treturn errors.Is(err, ErrPrefixSearchScan)\n}\n"
        },
        {
          "name": "errors_test.go",
          "type": "blob",
          "size": 4.86328125,
          "content": "package nutsdb\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/pkg/errors\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestIsKeyNotFound(t *testing.T) {\n\tts := []struct {\n\t\terr error\n\n\t\twant bool\n\t}{\n\t\t{\n\t\t\tErrKeyNotFound,\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\terrors.Wrap(ErrKeyNotFound, \"foobar\"),\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\terrors.New(\"foo bar\"),\n\t\t\tfalse,\n\t\t},\n\t}\n\n\tfor _, tc := range ts {\n\t\tgot := IsKeyNotFound(tc.err)\n\n\t\tassert.Equal(t, tc.want, got)\n\t}\n}\n\nfunc TestIsKeyEmpty(t *testing.T) {\n\ttype args struct {\n\t\terr error\n\t}\n\ttests := []struct {\n\t\tname string\n\t\targs args\n\t\twant bool\n\t}{\n\t\t{\"case1\", args{fmt.Errorf(\"foo error\")}, false},\n\t\t{\"case2\", args{fmt.Errorf(\"foo error,%w\", errors.New(\"sourceErr\"))}, false},\n\t\t{\"case3\", args{fmt.Errorf(\"foo error,%w\", ErrKeyEmpty)}, true},\n\t\t{\"case4_errors.Wrap\", args{errors.Wrap(ErrKeyEmpty, \"foo Err\")}, true},\n\t\t{\"case5_errors.Wrapf\", args{errors.Wrapf(ErrKeyEmpty, \"foo Err %s\", ErrKeyEmpty.Error())}, true},\n\t\t{\"case6_errors.rawError\", args{ErrKeyEmpty}, true},\n\t\t{\"case6_errors.keynotfoundErr\", args{ErrKeyNotFound}, false},\n\t\t// {\"case5\", args{errors.Wrap())}, true},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tif got := IsKeyEmpty(tt.args.err); got != tt.want {\n\t\t\t\tt.Errorf(\"IsKeyEmpty() = %v, want %v\", got, tt.want)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestIsBucketNotFound(t *testing.T) {\n\tts := []struct {\n\t\terr  error\n\t\twant bool\n\t}{\n\t\t{\n\t\t\tErrBucketNotFound,\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\terrors.Wrap(ErrBucketNotFound, \"foobar\"),\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\terrors.New(\"foobar\"),\n\t\t\tfalse,\n\t\t},\n\t}\n\n\tfor _, tc := range ts {\n\t\tgot := IsBucketNotFound(tc.err)\n\n\t\tassert.Equal(t, tc.want, got)\n\t}\n}\n\nfunc TestIsBucketEmpty(t *testing.T) {\n\tts := []struct {\n\t\terr  error\n\t\twant bool\n\t}{\n\t\t{\n\t\t\tErrBucketEmpty,\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\terrors.Wrap(ErrBucketEmpty, \"foobar\"),\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\terrors.New(\"foobar\"),\n\t\t\tfalse,\n\t\t},\n\t}\n\n\tfor _, tc := range ts {\n\t\tgot := IsBucketEmpty(tc.err)\n\n\t\tassert.Equal(t, tc.want, got)\n\t}\n}\n\nfunc TestIsDBClosed(t *testing.T) {\n\tInitOpt(\"\", true)\n\tbucket := \"test_closed\"\n\tkey := []byte(\"foo\")\n\tval := []byte(\"bar\")\n\tdb, err = Open(opt)\n\n\tt.Run(\"db can be used before closed\", func(t *testing.T) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\terr = db.Update(\n\t\t\tfunc(tx *Tx) error {\n\t\t\t\treturn tx.Put(bucket, key, val, Persistent)\n\t\t\t})\n\t\trequire.NoError(t, err)\n\t})\n\tassert.NoError(t, db.Close())\n\n\tt.Run(\"db can't be used after closed\", func(t *testing.T) {\n\t\terr = db.Update(\n\t\t\tfunc(tx *Tx) error {\n\t\t\t\treturn tx.Put(bucket, key, val, Persistent)\n\t\t\t})\n\t\tgot := IsDBClosed(err)\n\t\tassert.Equal(t, true, got)\n\t})\n}\n\nfunc TestIsPrefixScan(t *testing.T) {\n\tbucket := \"test_prefix_scan\"\n\tt.Run(\"if prefix scanning not found the result return true\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\t{\n\t\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\t\ttx, err := db.Begin(true)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tfor i := 0; i <= 10; i++ {\n\t\t\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\t\tval := []byte(\"val\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\t\t\tassert.NoError(t, err)\n\t\t\t\t}\n\t\t\t\tassert.NoError(t, tx.Commit())\n\t\t\t}\n\t\t\t{\n\t\t\t\ttx, err = db.Begin(false)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tprefix := []byte(\"key_\")\n\t\t\t\tes, err := tx.PrefixScan(bucket, prefix, 0, 10)\n\t\t\t\tassert.NoError(t, tx.Commit())\n\t\t\t\tassert.NotEmpty(t, es)\n\t\t\t\tgot := IsPrefixScan(err)\n\t\t\t\tassert.Equal(t, false, got)\n\t\t\t}\n\t\t\t{\n\t\t\t\ttx, err = db.Begin(false)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tprefix := []byte(\"foo_\")\n\t\t\t\tes, err := tx.PrefixScan(bucket, prefix, 0, 10)\n\t\t\t\tassert.NoError(t, tx.Commit())\n\t\t\t\tassert.Empty(t, es)\n\t\t\t\tgot := IsPrefixScan(err)\n\t\t\t\tassert.Equal(t, true, got)\n\t\t\t}\n\t\t})\n\t})\n}\n\nfunc TestIsPrefixSearchScan(t *testing.T) {\n\tregs := \"(.+)\"\n\tbucket := \"test_prefix_search_scan\"\n\tt.Run(\"if prefix and search scanning not found the result return true\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\t{\n\t\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\t\ttx, err := db.Begin(true)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tfor i := 0; i <= 10; i++ {\n\t\t\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\t\tval := []byte(\"val\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\t\t\tassert.NoError(t, err)\n\t\t\t\t}\n\t\t\t\tassert.NoError(t, tx.Commit())\n\t\t\t}\n\t\t\t{\n\t\t\t\ttx, err = db.Begin(false)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tprefix := []byte(\"key_\")\n\t\t\t\tes, err := tx.PrefixSearchScan(bucket, prefix, regs, 0, 10)\n\t\t\t\tassert.NoError(t, tx.Commit())\n\t\t\t\tassert.NotEmpty(t, es)\n\t\t\t\tgot := IsPrefixSearchScan(err)\n\t\t\t\tassert.Equal(t, false, got)\n\t\t\t}\n\t\t\t{\n\t\t\t\ttx, err = db.Begin(false)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tprefix := []byte(\"foo_\")\n\t\t\t\tes, err := tx.PrefixSearchScan(bucket, prefix, regs, 0, 10)\n\t\t\t\tassert.NoError(t, tx.Commit())\n\t\t\t\tassert.Empty(t, es)\n\t\t\t\tgot := IsPrefixSearchScan(err)\n\t\t\t\tassert.Equal(t, true, got)\n\t\t\t}\n\t\t})\n\t})\n}\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "fd_manager.go",
          "type": "blob",
          "size": 4.9228515625,
          "content": "package nutsdb\n\nimport (\n\t\"math\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"sync\"\n)\n\nconst (\n\tDefaultMaxFileNums = 256\n)\n\nconst (\n\tTooManyFileOpenErrSuffix = \"too many open files\"\n)\n\n// fdManager hold a fd cache in memory, it lru based cache.\ntype fdManager struct {\n\tlock               sync.Mutex\n\tcache              map[string]*FdInfo\n\tfdList             *doubleLinkedList\n\tsize               int\n\tcleanThresholdNums int\n\tmaxFdNums          int\n}\n\n// newFdm will return a fdManager object\nfunc newFdm(maxFdNums int, cleanThreshold float64) (fdm *fdManager) {\n\tfdm = &fdManager{\n\t\tcache:     map[string]*FdInfo{},\n\t\tfdList:    initDoubleLinkedList(),\n\t\tsize:      0,\n\t\tmaxFdNums: DefaultMaxFileNums,\n\t}\n\tfdm.cleanThresholdNums = int(math.Floor(0.5 * float64(fdm.maxFdNums)))\n\tif maxFdNums > 0 {\n\t\tfdm.maxFdNums = maxFdNums\n\t}\n\n\tif cleanThreshold > 0.0 && cleanThreshold < 1.0 {\n\t\tfdm.cleanThresholdNums = int(math.Floor(cleanThreshold * float64(fdm.maxFdNums)))\n\t}\n\treturn fdm\n}\n\n// FdInfo holds base fd info\ntype FdInfo struct {\n\tfd    *os.File\n\tpath  string\n\tusing uint\n\tnext  *FdInfo\n\tprev  *FdInfo\n}\n\n// getFd go through this method to get fd.\nfunc (fdm *fdManager) getFd(path string) (fd *os.File, err error) {\n\tfdm.lock.Lock()\n\tdefer fdm.lock.Unlock()\n\tcleanPath := filepath.Clean(path)\n\tif fdInfo := fdm.cache[cleanPath]; fdInfo == nil {\n\t\tfd, err = os.OpenFile(cleanPath, os.O_CREATE|os.O_RDWR, 0o644)\n\t\tif err == nil {\n\t\t\t// if the numbers of fd in cache larger than the cleanThreshold in config, we will clean useless fd in cache\n\t\t\tif fdm.size >= fdm.cleanThresholdNums {\n\t\t\t\terr = fdm.cleanUselessFd()\n\t\t\t}\n\t\t\t// if the numbers of fd in cache larger than the max numbers of fd in config, we will not add this fd to cache\n\t\t\tif fdm.size >= fdm.maxFdNums {\n\t\t\t\treturn fd, nil\n\t\t\t}\n\t\t\t// add this fd to cache\n\t\t\tfdm.addToCache(fd, cleanPath)\n\t\t\treturn fd, nil\n\t\t} else {\n\t\t\t// determine if there are too many open files, we will first clean useless fd in cache and try open this file again\n\t\t\tif strings.HasSuffix(err.Error(), TooManyFileOpenErrSuffix) {\n\t\t\t\tcleanErr := fdm.cleanUselessFd()\n\t\t\t\t// if something wrong in cleanUselessFd, we will return \"open too many files\" err, because we want user not the main err is that\n\t\t\t\tif cleanErr != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\t// try open this file againï¼Œif it still returns err, we will show this error to user\n\t\t\t\tfd, err = os.OpenFile(cleanPath, os.O_CREATE|os.O_RDWR, 0o644)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\t// add to cache if open this file successfully\n\t\t\t\tfdm.addToCache(fd, cleanPath)\n\t\t\t}\n\t\t\treturn fd, err\n\t\t}\n\t} else {\n\t\tfdInfo.using++\n\t\tfdm.fdList.moveNodeToFront(fdInfo)\n\t\treturn fdInfo.fd, nil\n\t}\n}\n\n// addToCache add fd to cache\nfunc (fdm *fdManager) addToCache(fd *os.File, cleanPath string) {\n\tfdInfo := &FdInfo{\n\t\tfd:    fd,\n\t\tusing: 1,\n\t\tpath:  cleanPath,\n\t}\n\tfdm.fdList.addNode(fdInfo)\n\tfdm.size++\n\tfdm.cache[cleanPath] = fdInfo\n}\n\n// reduceUsing when RWManager object close, it will go through this method let fdm know it return the fd to cache\nfunc (fdm *fdManager) reduceUsing(path string) {\n\tfdm.lock.Lock()\n\tdefer fdm.lock.Unlock()\n\tcleanPath := filepath.Clean(path)\n\tnode, isExist := fdm.cache[cleanPath]\n\tif !isExist {\n\t\tpanic(\"unexpected the node is not in cache\")\n\t}\n\tnode.using--\n}\n\n// close means the cache.\nfunc (fdm *fdManager) close() error {\n\tfdm.lock.Lock()\n\tdefer fdm.lock.Unlock()\n\tnode := fdm.fdList.tail.prev\n\tfor node != fdm.fdList.head {\n\t\terr := node.fd.Close()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdelete(fdm.cache, node.path)\n\t\tfdm.size--\n\t\tnode = node.prev\n\t}\n\tfdm.fdList.head.next = fdm.fdList.tail\n\tfdm.fdList.tail.prev = fdm.fdList.head\n\treturn nil\n}\n\ntype doubleLinkedList struct {\n\thead *FdInfo\n\ttail *FdInfo\n\tsize int\n}\n\nfunc initDoubleLinkedList() *doubleLinkedList {\n\tlist := &doubleLinkedList{\n\t\thead: &FdInfo{},\n\t\ttail: &FdInfo{},\n\t\tsize: 0,\n\t}\n\tlist.head.next = list.tail\n\tlist.tail.prev = list.head\n\treturn list\n}\n\nfunc (list *doubleLinkedList) addNode(node *FdInfo) {\n\tlist.head.next.prev = node\n\tnode.next = list.head.next\n\tlist.head.next = node\n\tnode.prev = list.head\n\tlist.size++\n}\n\nfunc (list *doubleLinkedList) removeNode(node *FdInfo) {\n\tnode.prev.next = node.next\n\tnode.next.prev = node.prev\n\tnode.prev = nil\n\tnode.next = nil\n}\n\nfunc (list *doubleLinkedList) moveNodeToFront(node *FdInfo) {\n\tlist.removeNode(node)\n\tlist.addNode(node)\n}\n\nfunc (fdm *fdManager) cleanUselessFd() error {\n\tcleanNums := fdm.cleanThresholdNums\n\tnode := fdm.fdList.tail.prev\n\tfor node != nil && node != fdm.fdList.head && cleanNums > 0 {\n\t\tnextItem := node.prev\n\t\tif node.using == 0 {\n\t\t\tfdm.fdList.removeNode(node)\n\t\t\terr := node.fd.Close()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tfdm.size--\n\t\t\tdelete(fdm.cache, node.path)\n\t\t\tcleanNums--\n\t\t}\n\t\tnode = nextItem\n\t}\n\treturn nil\n}\n\nfunc (fdm *fdManager) closeByPath(path string) error {\n\tfdm.lock.Lock()\n\tdefer fdm.lock.Unlock()\n\tfdInfo, ok := fdm.cache[path]\n\tif !ok {\n\t\treturn nil\n\t}\n\tdelete(fdm.cache, path)\n\n\tfdm.fdList.removeNode(fdInfo)\n\treturn fdInfo.fd.Close()\n}\n"
        },
        {
          "name": "fd_manager_test.go",
          "type": "blob",
          "size": 7.6181640625,
          "content": "package nutsdb\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestFdManager_All(t *testing.T) {\n\tdir := \"test-data\"\n\ttestBasePath := dir + \"/data-\"\n\terr := os.Mkdir(dir, os.ModePerm)\n\tassert.Nil(t, err)\n\tdefer os.RemoveAll(dir)\n\n\tstartFdNums := 1\n\tmaxFdNums := 20\n\tcreateFileLimit := 11\n\tcleanThreshold := 0.5\n\n\tdefer func() {\n\t\tif panicErr := recover(); panicErr != nil {\n\t\t\tt.Logf(\"panic is %s\", panicErr)\n\t\t}\n\t\terr := os.RemoveAll(testBasePath)\n\t\tassert.Nil(t, err)\n\t}()\n\n\tvar fdm *fdManager\n\tt.Run(\"test init fdm\", func(t *testing.T) {\n\t\tfdm = newFdm(maxFdNums, cleanThreshold)\n\t\tassert.NotNil(t, fdm)\n\t\tassert.Equal(t, maxFdNums, fdm.maxFdNums)\n\t\tassert.Equal(t, int(math.Floor(cleanThreshold*float64(fdm.maxFdNums))), fdm.cleanThresholdNums)\n\t})\n\n\tt.Run(\"create fd to cache\", func(t *testing.T) {\n\t\tfor i := startFdNums; i < createFileLimit; i++ {\n\t\t\tpath := testBasePath + fmt.Sprint(i)\n\t\t\tfd, err := fdm.getFd(path)\n\t\t\tassert.Nil(t, err)\n\t\t\tassert.NotNil(t, fd)\n\t\t}\n\t\tpositiveFdsSeq := []int{10, 9, 8, 7, 6, 5, 4, 3, 2, 1}\n\n\t\tassertChainFromTailAndHead(t, fdm, testBasePath, positiveFdsSeq)\n\t})\n\n\tt.Run(\"test get fd in cache\", func(t *testing.T) {\n\t\tt.Run(\"test get head item in cache\", func(t *testing.T) {\n\t\t\tfd, err := fdm.getFd(fdm.fdList.head.next.path)\n\t\t\tassert.Nil(t, err)\n\t\t\tassert.NotNil(t, fd)\n\t\t\tassert.Equal(t, fdm.fdList.head.next.fd, fd)\n\t\t\tpositiveFdsSeq := []int{10, 9, 8, 7, 6, 5, 4, 3, 2, 1}\n\t\t\tassertChainFromTailAndHead(t, fdm, testBasePath, positiveFdsSeq)\n\t\t})\n\n\t\tt.Run(\"test get tail item in cache\", func(t *testing.T) {\n\t\t\tfd, err := fdm.getFd(fdm.fdList.tail.prev.path)\n\t\t\tassert.Nil(t, err)\n\t\t\tassert.NotNil(t, fd)\n\t\t\tassert.Equal(t, fdm.fdList.head.next.fd, fd)\n\t\t\tpositiveFdsSeq := []int{1, 10, 9, 8, 7, 6, 5, 4, 3, 2}\n\t\t\tassertChainFromTailAndHead(t, fdm, testBasePath, positiveFdsSeq)\n\t\t})\n\n\t\tt.Run(\"test get middle item in cache\", func(t *testing.T) {\n\t\t\tpath := testBasePath + fmt.Sprint(5)\n\t\t\tfd, err := fdm.getFd(path)\n\t\t\tassert.Nil(t, err)\n\t\t\tassert.NotNil(t, fd)\n\t\t\tassert.Equal(t, fdm.fdList.head.next.fd, fd)\n\t\t\tpositiveFdsSeq := []int{5, 1, 10, 9, 8, 7, 6, 4, 3, 2}\n\t\t\tassertChainFromTailAndHead(t, fdm, testBasePath, positiveFdsSeq)\n\t\t})\n\t})\n\n\tt.Run(\"test reduce using\", func(t *testing.T) {\n\t\tpath := testBasePath + fmt.Sprint(5)\n\t\t_, err := fdm.getFd(path)\n\t\tassert.Nil(t, err)\n\t\tusing := fdm.fdList.head.next.using\n\t\t_, err = fdm.getFd(path)\n\t\tassert.Nil(t, err)\n\t\tassert.Equal(t, using+1, fdm.fdList.head.next.using)\n\t\tfdm.reduceUsing(path)\n\t\tassert.Nil(t, err)\n\t\tassert.Equal(t, using, fdm.fdList.head.next.using)\n\t})\n\n\tt.Run(\"test clean fd in cache\", func(t *testing.T) {\n\t\tpreReducePath := []int{2, 3, 4, 6, 7, 8}\n\t\tfor _, pathNum := range preReducePath {\n\t\t\tpath := testBasePath + fmt.Sprint(pathNum)\n\t\t\tfdm.reduceUsing(path)\n\t\t\tassert.Nil(t, err)\n\t\t}\n\t\tpath := testBasePath + fmt.Sprint(11)\n\t\tfd, err := fdm.getFd(path)\n\t\tassert.Nil(t, err)\n\t\tassert.NotNil(t, fd)\n\t\tpositiveFdsSeq := []int{11, 5, 1, 10, 9}\n\t\tassertChainFromTailAndHead(t, fdm, testBasePath, positiveFdsSeq)\n\t})\n\n\tt.Run(\"test close fdm\", func(t *testing.T) {\n\t\terr := fdm.close()\n\t\tif err != nil {\n\t\t\tt.Logf(\"err during close is:%s\", err)\n\t\t}\n\t\tassert.Nil(t, err)\n\t\tassert.Equal(t, 0, len(fdm.cache))\n\t\tassert.Equal(t, 0, fdm.size)\n\t})\n}\n\nfunc TestDoubleLinkedList_All(t *testing.T) {\n\tlist := initDoubleLinkedList()\n\tnodeMap := make(map[int]*FdInfo)\n\tt.Run(\"test add node\", func(t *testing.T) {\n\t\tfor i := 1; i <= 10; i++ {\n\t\t\tfd := &FdInfo{\n\t\t\t\tpath: fmt.Sprint(i),\n\t\t\t}\n\t\t\tlist.addNode(fd)\n\t\t\tnodeMap[i] = fd\n\t\t}\n\t\tassert.Equal(t, `[10 9 8 7 6 5 4 3 2 1 ]`, fmt.Sprintf(\"%+v\", getAllNodePathFromHead(list)))\n\t\tassert.Equal(t, `[1 2 3 4 5 6 7 8 9 10 ]`, fmt.Sprintf(\"%+v\", getAllNodePathFromTail(list)))\n\t})\n\n\tt.Run(\"test remove node\", func(t *testing.T) {\n\t\tt.Run(\"test remove first node\", func(t *testing.T) {\n\t\t\tlist.removeNode(nodeMap[10])\n\t\t\tassert.Equal(t, \"[9 8 7 6 5 4 3 2 1 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromHead(list)))\n\t\t\tassert.Equal(t, \"[1 2 3 4 5 6 7 8 9 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromTail(list)))\n\t\t})\n\t\tt.Run(\"test remove last node\", func(t *testing.T) {\n\t\t\tlist.removeNode(nodeMap[1])\n\t\t\tassert.Equal(t, \"[9 8 7 6 5 4 3 2 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromHead(list)))\n\t\t\tassert.Equal(t, \"[2 3 4 5 6 7 8 9 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromTail(list)))\n\t\t})\n\t\tt.Run(\"test remove middle node\", func(t *testing.T) {\n\t\t\tlist.removeNode(nodeMap[5])\n\t\t\tassert.Equal(t, \"[9 8 7 6 4 3 2 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromHead(list)))\n\t\t\tassert.Equal(t, \"[2 3 4 6 7 8 9 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromTail(list)))\n\t\t})\n\t})\n\n\tt.Run(\"test move node to head\", func(t *testing.T) {\n\t\tt.Run(\"test move first node\", func(t *testing.T) {\n\t\t\tlist.moveNodeToFront(nodeMap[9])\n\t\t\tassert.Equal(t, \"[9 8 7 6 4 3 2 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromHead(list)))\n\t\t\tassert.Equal(t, \"[2 3 4 6 7 8 9 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromTail(list)))\n\t\t})\n\t\tt.Run(\"test move last node\", func(t *testing.T) {\n\t\t\tlist.moveNodeToFront(nodeMap[2])\n\t\t\tassert.Equal(t, \"[2 9 8 7 6 4 3 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromHead(list)))\n\t\t\tassert.Equal(t, \"[3 4 6 7 8 9 2 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromTail(list)))\n\t\t})\n\t\tt.Run(\"test move middle node\", func(t *testing.T) {\n\t\t\tlist.moveNodeToFront(nodeMap[6])\n\t\t\tassert.Equal(t, \"[6 2 9 8 7 4 3 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromHead(list)))\n\t\t\tassert.Equal(t, \"[3 4 7 8 9 2 6 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromTail(list)))\n\t\t})\n\t})\n}\n\nfunc getAllNodePathFromHead(list *doubleLinkedList) (res []string) {\n\tnode := list.head.next\n\tfor node != nil {\n\t\tres = append(res, node.path)\n\t\tnode = node.next\n\t}\n\treturn res\n}\n\nfunc getAllNodePathFromTail(list *doubleLinkedList) (res []string) {\n\tnode := list.tail.prev\n\tfor node != nil {\n\t\tres = append(res, node.path)\n\t\tnode = node.prev\n\t}\n\treturn res\n}\n\nfunc assertChainFromTailAndHead(t *testing.T, fdm *fdManager, testBasePath string, positiveFdsSeq []int) {\n\tassertChainFromHead(t, fdm, testBasePath, positiveFdsSeq)\n\tassertChainFromTail(t, fdm, testBasePath, positiveFdsSeq)\n}\n\nfunc assertChainFromHead(t *testing.T, fdm *fdManager, testBasePath string, positiveFdsSeq []int) {\n\tnode := fdm.fdList.head.next\n\tindex := 0\n\tnums := 0\n\tfor node != fdm.fdList.tail {\n\t\texpectedPath := testBasePath + fmt.Sprint(positiveFdsSeq[index])\n\t\tassert.NotNil(t, node.fd)\n\t\tassert.Equal(t, expectedPath, node.path)\n\t\tnode = node.next\n\t\tindex++\n\t\tnums++\n\t}\n\tassert.Equal(t, fdm.size, nums)\n}\n\nfunc assertChainFromTail(t *testing.T, fdm *fdManager, testBasePath string, positiveFdsSeq []int) {\n\tindex := len(positiveFdsSeq) - 1\n\tnode := fdm.fdList.tail.prev\n\tnums := 0\n\tfor node != fdm.fdList.head {\n\t\texpectedPath := testBasePath + fmt.Sprint(positiveFdsSeq[index])\n\t\tassert.NotNil(t, node.fd)\n\t\tassert.Equal(t, expectedPath, node.path)\n\t\tnode = node.prev\n\t\tindex--\n\t\tnums++\n\t}\n\tassert.Equal(t, fdm.size, nums)\n}\n\n//func TestGetMaxNums(t *testing.T) {\n//\tmaxNums := 30000\n//\tbasePath := \"test-path/\"\n//\terr := os.RemoveAll(basePath)\n//\tassert.Nil(t, err)\n//\terr = os.Mkdir(basePath, os.ModePerm)\n//\tassert.Nil(t, err)\n//\tdefer func() {\n//\t\terr := os.RemoveAll(basePath)\n//\t\tif err != nil {\n//\t\t\tt.Logf(\"err is %s\", err)\n//\t\t}\n//\t}()\n//\tvar fdList []*os.File\n//\tfor i := 1; i <= maxNums; i++ {\n//\t\tpath := basePath + fmt.Sprintf(\"%d\", i)\n//\t\tfd, err := os.OpenFile(path, os.O_CREATE|os.O_RDWR, 0644)\n//\t\tif err != nil {\n//\t\t\tif strings.HasSuffix(err.Error(), TooManyFileOpenErrSuffix) {\n//\t\t\t\tt.Logf(\"file num is %d, err is %s, and it had handle\", i, err)\n//\t\t\t}\n//\t\t\tfor _, fd := range fdList {\n//\t\t\t\terr := fd.Close()\n//\t\t\t\tif err != nil {\n//\t\t\t\t\tt.Logf(\"err is %s, and it had handle\", err)\n//\t\t\t\t}\n//\t\t\t}\n//\t\t\treturn\n//\t\t} else {\n//\t\t\tfdList = append(fdList, fd)\n//\t\t}\n//\t}\n//}\n"
        },
        {
          "name": "file_manager.go",
          "type": "blob",
          "size": 2.0712890625,
          "content": "package nutsdb\n\nimport (\n\t\"github.com/xujiajun/mmap-go\"\n)\n\n// fileManager holds the fd cache and file-related operations go through the manager to obtain the file processing object\ntype fileManager struct {\n\trwMode      RWMode\n\tfdm         *fdManager\n\tsegmentSize int64\n}\n\n// newFileManager will create a newFileManager object\nfunc newFileManager(rwMode RWMode, maxFdNums int, cleanThreshold float64, segmentSize int64) (fm *fileManager) {\n\tfm = &fileManager{\n\t\trwMode:      rwMode,\n\t\tfdm:         newFdm(maxFdNums, cleanThreshold),\n\t\tsegmentSize: segmentSize,\n\t}\n\treturn fm\n}\n\n// getDataFile will return a DataFile Object\nfunc (fm *fileManager) getDataFile(path string, capacity int64) (datafile *DataFile, err error) {\n\tif capacity <= 0 {\n\t\treturn nil, ErrCapacity\n\t}\n\n\tvar rwManager RWManager\n\n\tif fm.rwMode == FileIO {\n\t\trwManager, err = fm.getFileRWManager(path, capacity, fm.segmentSize)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tif fm.rwMode == MMap {\n\t\trwManager, err = fm.getMMapRWManager(path, capacity, fm.segmentSize)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn NewDataFile(path, rwManager), nil\n}\n\n// getFileRWManager will return a FileIORWManager Object\nfunc (fm *fileManager) getFileRWManager(path string, capacity int64, segmentSize int64) (*FileIORWManager, error) {\n\tfd, err := fm.fdm.getFd(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\terr = Truncate(path, capacity, fd)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &FileIORWManager{fd: fd, path: path, fdm: fm.fdm, segmentSize: segmentSize}, nil\n}\n\n// getMMapRWManager will return a MMapRWManager Object\nfunc (fm *fileManager) getMMapRWManager(path string, capacity int64, segmentSize int64) (*MMapRWManager, error) {\n\tfd, err := fm.fdm.getFd(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\terr = Truncate(path, capacity, fd)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tm, err := mmap.Map(fd, mmap.RDWR, 0)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &MMapRWManager{m: m, path: path, fdm: fm.fdm, segmentSize: segmentSize}, nil\n}\n\n// close will close fdm resource\nfunc (fm *fileManager) close() error {\n\terr := fm.fdm.close()\n\treturn err\n}\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 0.6044921875,
          "content": "module github.com/nutsdb/nutsdb\n\ngo 1.18\n\nrequire (\n\tgithub.com/antlabs/timer v0.0.11\n\tgithub.com/bwmarrin/snowflake v0.3.0\n\tgithub.com/gofrs/flock v0.8.1\n\tgithub.com/pkg/errors v0.9.1\n\tgithub.com/stretchr/testify v1.7.1\n\tgithub.com/tidwall/btree v1.6.0\n\tgithub.com/xujiajun/gorouter v1.2.0\n\tgithub.com/xujiajun/mmap-go v1.0.1\n\tgithub.com/xujiajun/utils v0.0.0-20220904132955-5f7c5b914235\n)\n\nrequire (\n\tgithub.com/antlabs/stl v0.0.1 // indirect\n\tgithub.com/davecgh/go-spew v1.1.1 // indirect\n\tgithub.com/pmezard/go-difflib v1.0.0 // indirect\n\tgolang.org/x/sys v0.15.0 // indirect\n\tgopkg.in/yaml.v3 v3.0.1 // indirect\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 3.1005859375,
          "content": "github.com/antlabs/stl v0.0.1 h1:TRD3csCrjREeLhLoQ/supaoCvFhNLBTNIwuRGrDIs6Q=\ngithub.com/antlabs/stl v0.0.1/go.mod h1:wvVwP1loadLG3cRjxUxK8RL4Co5xujGaZlhbztmUEqQ=\ngithub.com/antlabs/timer v0.0.11 h1:z75oGFLeTqJHMOcWzUPBKsBbQAz4Ske3AfqJ7bsdcwU=\ngithub.com/antlabs/timer v0.0.11/go.mod h1:JNV8J3yGvMKhCavGXgj9HXrVZkfdQyKCcqXBT8RdyuU=\ngithub.com/bwmarrin/snowflake v0.3.0 h1:xm67bEhkKh6ij1790JB83OujPR5CzNe8QuQqAgISZN0=\ngithub.com/bwmarrin/snowflake v0.3.0/go.mod h1:NdZxfVWX+oR6y2K0o6qAYv6gIOP9rjG0/E9WsDpxqwE=\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/gofrs/flock v0.8.1 h1:+gYjHKf32LDeiEEFhQaotPbLuUXjY5ZqxKgXy7n59aw=\ngithub.com/gofrs/flock v0.8.1/go.mod h1:F1TvTiK9OcQqauNUHlbJvyl9Qa1QvF/gOUDKA14jxHU=\ngithub.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=\ngithub.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/testify v1.6.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.7.1 h1:5TQK59W5E3v0r2duFAb7P95B6hEeOyEnHRa8MjYSMTY=\ngithub.com/stretchr/testify v1.7.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/tidwall/btree v1.6.0 h1:LDZfKfQIBHGHWSwckhXI0RPSXzlo+KYdjK7FWSqOzzg=\ngithub.com/tidwall/btree v1.6.0/go.mod h1:twD9XRA5jj9VUQGELzDO4HPQTNJsoWWfYEL+EUQ2cKY=\ngithub.com/xujiajun/gorouter v1.2.0 h1:aPKfkzLHxPYRgr+irEE00SEOf78LHnxH/v4m8QiV51Y=\ngithub.com/xujiajun/gorouter v1.2.0/go.mod h1:yJrIta+bTNpBM/2UT8hLOaEAFckO+m/qmR3luMIQygM=\ngithub.com/xujiajun/mmap-go v1.0.1 h1:7Se7ss1fLPPRW+ePgqGpCkfGIZzJV6JPq9Wq9iv/WHc=\ngithub.com/xujiajun/mmap-go v1.0.1/go.mod h1:CNN6Sw4SL69Sui00p0zEzcZKbt+5HtEnYUsc6BKKRMg=\ngithub.com/xujiajun/utils v0.0.0-20220904132955-5f7c5b914235 h1:w0si+uee0iAaCJO9q86T6yrhdadgcsoNuh47LrUykzg=\ngithub.com/xujiajun/utils v0.0.0-20220904132955-5f7c5b914235/go.mod h1:MR4+0R6A9NS5IABnIM3384FfOq8QFVnm7WDrBOhIaMU=\ngolang.org/x/sys v0.0.0-20181221143128-b4a75ba826a6/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.15.0 h1:h48lPFYpsTvQJZF4EKyI4aLHaev3CxivZmv7yZig9pc=\ngolang.org/x/sys v0.15.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405 h1:yhCVgyC4o1eVCa2tZl7eS0r+SDo693bJlVdllGtEeKM=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/go-playground/assert.v1 v1.2.1 h1:xoYuJVE7KT85PYWrN730RguIQO0ePzVRfFMXadIrXTM=\ngopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\n"
        },
        {
          "name": "index.go",
          "type": "blob",
          "size": 2.3837890625,
          "content": "// Copyright 2022 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\ntype IdxType interface {\n\tBTree | Set | SortedSet | List\n}\n\ntype defaultOp[T IdxType] struct {\n\tidx map[BucketId]*T\n}\n\nfunc (op *defaultOp[T]) computeIfAbsent(id BucketId, f func() *T) *T {\n\tif i, isExist := op.idx[id]; isExist {\n\t\treturn i\n\t}\n\ti := f()\n\top.idx[id] = i\n\treturn i\n}\n\nfunc (op *defaultOp[T]) delete(id BucketId) {\n\tdelete(op.idx, id)\n}\n\nfunc (op *defaultOp[T]) exist(id BucketId) (*T, bool) {\n\ti, isExist := op.idx[id]\n\treturn i, isExist\n}\n\nfunc (op *defaultOp[T]) getIdxLen() int {\n\treturn len(op.idx)\n}\n\nfunc (op *defaultOp[T]) rangeIdx(f func(elem *T)) {\n\tfor _, t := range op.idx {\n\t\tf(t)\n\t}\n}\n\ntype ListIdx struct {\n\t*defaultOp[List]\n}\n\nfunc (idx ListIdx) getWithDefault(id BucketId) *List {\n\treturn idx.defaultOp.computeIfAbsent(id, func() *List {\n\t\treturn NewList()\n\t})\n}\n\ntype BTreeIdx struct {\n\t*defaultOp[BTree]\n}\n\nfunc (idx BTreeIdx) getWithDefault(id BucketId) *BTree {\n\treturn idx.defaultOp.computeIfAbsent(id, func() *BTree {\n\t\treturn NewBTree()\n\t})\n}\n\ntype SetIdx struct {\n\t*defaultOp[Set]\n}\n\nfunc (idx SetIdx) getWithDefault(id BucketId) *Set {\n\treturn idx.defaultOp.computeIfAbsent(id, func() *Set {\n\t\treturn NewSet()\n\t})\n}\n\ntype SortedSetIdx struct {\n\t*defaultOp[SortedSet]\n}\n\nfunc (idx SortedSetIdx) getWithDefault(id BucketId, db *DB) *SortedSet {\n\treturn idx.defaultOp.computeIfAbsent(id, func() *SortedSet {\n\t\treturn NewSortedSet(db)\n\t})\n}\n\ntype index struct {\n\tlist      ListIdx\n\tbTree     BTreeIdx\n\tset       SetIdx\n\tsortedSet SortedSetIdx\n}\n\nfunc newIndex() *index {\n\ti := new(index)\n\ti.list = ListIdx{&defaultOp[List]{idx: map[BucketId]*List{}}}\n\ti.bTree = BTreeIdx{&defaultOp[BTree]{idx: map[BucketId]*BTree{}}}\n\ti.set = SetIdx{&defaultOp[Set]{idx: map[BucketId]*Set{}}}\n\ti.sortedSet = SortedSetIdx{&defaultOp[SortedSet]{idx: map[BucketId]*SortedSet{}}}\n\treturn i\n}\n"
        },
        {
          "name": "iterator.go",
          "type": "blob",
          "size": 1.740234375,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"github.com/tidwall/btree\"\n)\n\ntype Iterator struct {\n\ttx      *Tx\n\toptions IteratorOptions\n\titer    btree.IterG[*Item]\n}\n\ntype IteratorOptions struct {\n\tReverse bool\n}\n\nfunc NewIterator(tx *Tx, bucket string, options IteratorOptions) *Iterator {\n\tb, err := tx.db.bm.GetBucket(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn nil\n\t}\n\titerator := &Iterator{\n\t\ttx:      tx,\n\t\toptions: options,\n\t\titer:    tx.db.Index.bTree.getWithDefault(b.Id).btree.Iter(),\n\t}\n\n\tif options.Reverse {\n\t\titerator.iter.Last()\n\t} else {\n\t\titerator.iter.First()\n\t}\n\n\treturn iterator\n}\n\nfunc (it *Iterator) Rewind() bool {\n\tif it.options.Reverse {\n\t\treturn it.iter.Last()\n\t} else {\n\t\treturn it.iter.First()\n\t}\n}\n\nfunc (it *Iterator) Seek(key []byte) bool {\n\treturn it.iter.Seek(&Item{key: key})\n}\n\nfunc (it *Iterator) Next() bool {\n\tif it.options.Reverse {\n\t\treturn it.iter.Prev()\n\t} else {\n\t\treturn it.iter.Next()\n\t}\n}\n\nfunc (it *Iterator) Valid() bool {\n\treturn it.iter.Item() != nil\n}\n\nfunc (it *Iterator) Key() []byte {\n\treturn it.iter.Item().key\n}\n\nfunc (it *Iterator) Value() ([]byte, error) {\n\treturn it.tx.db.getValueByRecord(it.iter.Item().record)\n}\n"
        },
        {
          "name": "iterator_test.go",
          "type": "blob",
          "size": 2.3994140625,
          "content": "// Copyright 2022 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"github.com/stretchr/testify/require\"\n\t\"testing\"\n)\n\nfunc TestIterator(t *testing.T) {\n\tbucket := \"bucket\"\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\tfor i := 0; i < 100; i++ {\n\t\t\ttxPut(t, db, bucket, GetTestBytes(i), GetTestBytes(i), Persistent, nil, nil)\n\t\t}\n\n\t\t_ = db.View(func(tx *Tx) error {\n\t\t\titerator := NewIterator(tx, bucket, IteratorOptions{Reverse: false})\n\n\t\t\ti := 0\n\n\t\t\tfor {\n\t\t\t\tvalue, err := iterator.Value()\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\trequire.Equal(t, GetTestBytes(i), value)\n\t\t\t\tif !iterator.Next() {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\ti++\n\t\t\t}\n\n\t\t\treturn nil\n\t\t})\n\t})\n}\n\nfunc TestIterator_Reverse(t *testing.T) {\n\tbucket := \"bucket\"\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\tfor i := 0; i < 100; i++ {\n\t\t\ttxPut(t, db, bucket, GetTestBytes(i), GetTestBytes(i), Persistent, nil, nil)\n\t\t}\n\n\t\t_ = db.View(func(tx *Tx) error {\n\t\t\titerator := NewIterator(tx, bucket, IteratorOptions{Reverse: true})\n\n\t\t\ti := 99\n\t\t\tfor {\n\t\t\t\tvalue, err := iterator.Value()\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\trequire.Equal(t, GetTestBytes(i), value)\n\t\t\t\tif !iterator.Next() {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\ti--\n\t\t\t}\n\n\t\t\treturn nil\n\t\t})\n\t})\n}\n\nfunc TestIterator_Seek(t *testing.T) {\n\tbucket := \"bucket\"\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\tfor i := 0; i < 100; i++ {\n\t\t\ttxPut(t, db, bucket, GetTestBytes(i), GetTestBytes(i), Persistent, nil, nil)\n\t\t}\n\n\t\t_ = db.View(func(tx *Tx) error {\n\t\t\titerator := NewIterator(tx, bucket, IteratorOptions{Reverse: true})\n\n\t\t\titerator.Seek(GetTestBytes(40))\n\n\t\t\tvalue, err := iterator.Value()\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Equal(t, GetTestBytes(40), value)\n\n\t\t\treturn nil\n\t\t})\n\t})\n}\n"
        },
        {
          "name": "list.go",
          "type": "blob",
          "size": 8.041015625,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"errors\"\n\t\"math\"\n\t\"time\"\n)\n\nvar (\n\t// ErrListNotFound is returned when the list not found.\n\tErrListNotFound = errors.New(\"the list not found\")\n\n\t// ErrCount is returned when count is error.\n\tErrCount = errors.New(\"err count\")\n\n\t// ErrEmptyList is returned when the list is empty.\n\tErrEmptyList = errors.New(\"the list is empty\")\n\n\t// ErrStartOrEnd is returned when start > end\n\tErrStartOrEnd = errors.New(\"start or end error\")\n)\n\nconst (\n\tinitialListSeq = math.MaxUint64 / 2\n)\n\n// BTree represents the btree.\n\n// HeadTailSeq list head and tail seq num\ntype HeadTailSeq struct {\n\tHead uint64\n\tTail uint64\n}\n\n// List represents the list.\ntype List struct {\n\tItems     map[string]*BTree\n\tTTL       map[string]uint32\n\tTimeStamp map[string]uint64\n\tSeq       map[string]*HeadTailSeq\n}\n\nfunc NewList() *List {\n\treturn &List{\n\t\tItems:     make(map[string]*BTree),\n\t\tTTL:       make(map[string]uint32),\n\t\tTimeStamp: make(map[string]uint64),\n\t\tSeq:       make(map[string]*HeadTailSeq),\n\t}\n}\n\nfunc (l *List) LPush(key string, r *Record) error {\n\treturn l.push(key, r, true)\n}\n\nfunc (l *List) RPush(key string, r *Record) error {\n\treturn l.push(key, r, false)\n}\n\nfunc (l *List) push(key string, r *Record, isLeft bool) error {\n\t// key is seq + user_key\n\tuserKey, curSeq := decodeListKey([]byte(key))\n\tuserKeyStr := string(userKey)\n\tif l.IsExpire(userKeyStr) {\n\t\treturn ErrListNotFound\n\t}\n\n\tlist, ok := l.Items[userKeyStr]\n\tif !ok {\n\t\tl.Items[userKeyStr] = NewBTree()\n\t\tlist = l.Items[userKeyStr]\n\t}\n\n\tseq, ok := l.Seq[userKeyStr]\n\tif !ok {\n\t\tl.Seq[userKeyStr] = &HeadTailSeq{Head: initialListSeq, Tail: initialListSeq + 1}\n\t\tseq = l.Seq[userKeyStr]\n\t}\n\n\tlist.InsertRecord(ConvertUint64ToBigEndianBytes(curSeq), r)\n\tif isLeft {\n\t\tif seq.Head > curSeq-1 {\n\t\t\tseq.Head = curSeq - 1\n\t\t}\n\t} else {\n\t\tif seq.Tail < curSeq+1 {\n\t\t\tseq.Tail = curSeq + 1\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (l *List) LPop(key string) (*Record, error) {\n\titem, err := l.LPeek(key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tl.Items[key].Delete(item.key)\n\tl.Seq[key].Head = ConvertBigEndianBytesToUint64(item.key)\n\treturn item.record, nil\n}\n\n// RPop removes and returns the last element of the list stored at key.\nfunc (l *List) RPop(key string) (*Record, error) {\n\titem, err := l.RPeek(key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tl.Items[key].Delete(item.key)\n\tl.Seq[key].Tail = ConvertBigEndianBytesToUint64(item.key)\n\treturn item.record, nil\n}\n\nfunc (l *List) LPeek(key string) (*Item, error) {\n\treturn l.peek(key, true)\n}\n\nfunc (l *List) RPeek(key string) (*Item, error) {\n\treturn l.peek(key, false)\n}\n\nfunc (l *List) peek(key string, isLeft bool) (*Item, error) {\n\tif l.IsExpire(key) {\n\t\treturn nil, ErrListNotFound\n\t}\n\tlist, ok := l.Items[key]\n\tif !ok {\n\t\treturn nil, ErrListNotFound\n\t}\n\n\tif isLeft {\n\t\titem, ok := list.Min()\n\t\tif ok {\n\t\t\treturn item, nil\n\t\t}\n\t} else {\n\t\titem, ok := list.Max()\n\t\tif ok {\n\t\t\treturn item, nil\n\t\t}\n\t}\n\n\treturn nil, ErrEmptyList\n}\n\n// LRange returns the specified elements of the list stored at key [start,end]\nfunc (l *List) LRange(key string, start, end int) ([]*Record, error) {\n\tsize, err := l.Size(key)\n\tif err != nil || size == 0 {\n\t\treturn nil, err\n\t}\n\n\tstart, end, err = checkBounds(start, end, size)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar res []*Record\n\tallRecords := l.Items[key].All()\n\tfor i, item := range allRecords {\n\t\tif i >= start && i <= end {\n\t\t\tres = append(res, item)\n\t\t}\n\t}\n\n\treturn res, nil\n}\n\n// getRemoveIndexes returns a slice of indices to be removed from the list based on the count\nfunc (l *List) getRemoveIndexes(key string, count int, cmp func(r *Record) (bool, error)) ([][]byte, error) {\n\tif l.IsExpire(key) {\n\t\treturn nil, ErrListNotFound\n\t}\n\n\tlist, ok := l.Items[key]\n\n\tif !ok {\n\t\treturn nil, ErrListNotFound\n\t}\n\n\tvar res [][]byte\n\tvar allItems []*Item\n\tif 0 == count {\n\t\tcount = list.Count()\n\t}\n\n\tallItems = l.Items[key].AllItems()\n\tif count > 0 {\n\t\tfor _, item := range allItems {\n\t\t\tif count <= 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tr := item.record\n\t\t\tok, err := cmp(r)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tif ok {\n\t\t\t\tres = append(res, item.key)\n\t\t\t\tcount--\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfor i := len(allItems) - 1; i >= 0; i-- {\n\t\t\tif count >= 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tr := allItems[i].record\n\t\t\tok, err := cmp(r)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tif ok {\n\t\t\t\tres = append(res, allItems[i].key)\n\t\t\t\tcount++\n\t\t\t}\n\t\t}\n\t}\n\n\treturn res, nil\n}\n\n// LRem removes the first count occurrences of elements equal to value from the list stored at key.\n// The count argument influences the operation in the following ways:\n// count > 0: Remove elements equal to value moving from head to tail.\n// count < 0: Remove elements equal to value moving from tail to head.\n// count = 0: Remove all elements equal to value.\nfunc (l *List) LRem(key string, count int, cmp func(r *Record) (bool, error)) error {\n\tremoveIndexes, err := l.getRemoveIndexes(key, count, cmp)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tlist := l.Items[key]\n\tfor _, idx := range removeIndexes {\n\t\tlist.Delete(idx)\n\t}\n\n\treturn nil\n}\n\n// LTrim trim an existing list so that it will contain only the specified range of elements specified.\nfunc (l *List) LTrim(key string, start, end int) error {\n\tif l.IsExpire(key) {\n\t\treturn ErrListNotFound\n\t}\n\tif _, ok := l.Items[key]; !ok {\n\t\treturn ErrListNotFound\n\t}\n\n\tlist := l.Items[key]\n\tallItems := list.AllItems()\n\tfor i, item := range allItems {\n\t\tif i < start || i > end {\n\t\t\tlist.Delete(item.key)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// LRemByIndex remove the list element at specified index\nfunc (l *List) LRemByIndex(key string, indexes []int) error {\n\tif l.IsExpire(key) {\n\t\treturn ErrListNotFound\n\t}\n\n\tidxes := l.getValidIndexes(key, indexes)\n\tif len(idxes) == 0 {\n\t\treturn nil\n\t}\n\n\tlist := l.Items[key]\n\tallItems := list.AllItems()\n\tfor i, item := range allItems {\n\t\tif _, ok := idxes[i]; ok {\n\t\t\tlist.Delete(item.key)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (l *List) getValidIndexes(key string, indexes []int) map[int]struct{} {\n\tidxes := make(map[int]struct{})\n\tlistLen, err := l.Size(key)\n\tif err != nil || 0 == listLen {\n\t\treturn idxes\n\t}\n\n\tfor _, idx := range indexes {\n\t\tif idx < 0 || idx >= listLen {\n\t\t\tcontinue\n\t\t}\n\t\tidxes[idx] = struct{}{}\n\t}\n\n\treturn idxes\n}\n\nfunc (l *List) IsExpire(key string) bool {\n\tif l == nil {\n\t\treturn false\n\t}\n\n\t_, ok := l.TTL[key]\n\tif !ok {\n\t\treturn false\n\t}\n\n\tnow := time.Now().Unix()\n\ttimestamp := l.TimeStamp[key]\n\tif l.TTL[key] > 0 && uint64(l.TTL[key])+timestamp > uint64(now) || l.TTL[key] == uint32(0) {\n\t\treturn false\n\t}\n\n\tdelete(l.Items, key)\n\tdelete(l.TTL, key)\n\tdelete(l.TimeStamp, key)\n\tdelete(l.Seq, key)\n\n\treturn true\n}\n\nfunc (l *List) Size(key string) (int, error) {\n\tif l.IsExpire(key) {\n\t\treturn 0, ErrListNotFound\n\t}\n\tif _, ok := l.Items[key]; !ok {\n\t\treturn 0, ErrListNotFound\n\t}\n\n\treturn l.Items[key].Count(), nil\n}\n\nfunc (l *List) IsEmpty(key string) (bool, error) {\n\tsize, err := l.Size(key)\n\tif err != nil || size > 0 {\n\t\treturn false, err\n\t}\n\treturn true, nil\n}\n\nfunc (l *List) GetListTTL(key string) (uint32, error) {\n\tif l.IsExpire(key) {\n\t\treturn 0, ErrListNotFound\n\t}\n\n\tttl := l.TTL[key]\n\ttimestamp := l.TimeStamp[key]\n\tif ttl == 0 || timestamp == 0 {\n\t\treturn 0, nil\n\t}\n\n\tnow := time.Now().Unix()\n\tremain := timestamp + uint64(ttl) - uint64(now)\n\n\treturn uint32(remain), nil\n}\n\nfunc checkBounds(start, end int, size int) (int, int, error) {\n\tif start >= 0 && end < 0 {\n\t\tend = size + end\n\t}\n\n\tif start < 0 && end > 0 {\n\t\tstart = size + start\n\t}\n\n\tif start < 0 && end < 0 {\n\t\tstart, end = size+start, size+end\n\t}\n\n\tif end >= size {\n\t\tend = size - 1\n\t}\n\n\tif start > end {\n\t\treturn 0, 0, ErrStartOrEnd\n\t}\n\n\treturn start, end, nil\n}\n"
        },
        {
          "name": "list_test.go",
          "type": "blob",
          "size": 7.01171875,
          "content": "// Copyright 2023 The PromiseDB Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file expect in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"bytes\"\n\t\"math/rand\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc ListPush(t *testing.T, list *List, key string, r *Record, isLeft bool, expectError error) {\n\tvar e error\n\tif isLeft {\n\t\te = list.LPush(key, r)\n\t} else {\n\t\te = list.RPush(key, r)\n\t}\n\tassertErr(t, e, expectError)\n}\n\nfunc ListPop(t *testing.T, list *List, key string, isLeft bool, expectVal *Record, expectError error) {\n\tvar (\n\t\te error\n\t\tr *Record\n\t)\n\n\tif isLeft {\n\t\tr, e = list.LPop(key)\n\t} else {\n\t\tr, e = list.RPop(key)\n\t}\n\tif expectError != nil {\n\t\trequire.Equal(t, expectError, e)\n\t} else {\n\t\trequire.NoError(t, e)\n\t\trequire.Equal(t, expectVal, r)\n\t}\n}\n\nfunc ListCmp(t *testing.T, list *List, key string, expectRecords []*Record, isReverse bool) {\n\trecords, err := list.LRange(key, 0, -1)\n\trequire.NoError(t, err)\n\n\tif isReverse {\n\t\tfor i := len(expectRecords) - 1; i >= 0; i-- {\n\t\t\trequire.Equal(t, expectRecords[i], records[len(expectRecords)-1-i])\n\t\t}\n\t} else {\n\t\tfor i := 0; i < len(expectRecords); i++ {\n\t\t\trequire.Equal(t, expectRecords[i], records[i])\n\t\t}\n\t}\n}\n\nfunc TestList_LPush(t *testing.T) {\n\tlist := NewList()\n\t// æµ‹è¯• LPush\n\tkey := string(GetTestBytes(0))\n\texpectRecords := generateRecords(5)\n\tseqInfo := HeadTailSeq{Head: initialListSeq, Tail: initialListSeq + 1}\n\n\tfor i := 0; i < len(expectRecords); i++ {\n\t\tseq := generateSeq(&seqInfo, true)\n\t\tnewKey := encodeListKey([]byte(key), seq)\n\t\texpectRecords[i].Key = newKey\n\t\tListPush(t, list, string(newKey), expectRecords[i], true, nil)\n\t}\n\n\tListCmp(t, list, key, expectRecords, true)\n}\n\nfunc TestList_RPush(t *testing.T) {\n\tlist := NewList()\n\texpectRecords := generateRecords(5)\n\tkey := string(GetTestBytes(0))\n\tseqInfo := HeadTailSeq{Head: initialListSeq, Tail: initialListSeq + 1}\n\n\tfor i := 0; i < len(expectRecords); i++ {\n\t\tseq := generateSeq(&seqInfo, false)\n\t\tnewKey := encodeListKey([]byte(key), seq)\n\t\texpectRecords[i].Key = newKey\n\t\tListPush(t, list, string(newKey), expectRecords[i], false, nil)\n\t}\n\n\tListCmp(t, list, key, expectRecords, false)\n}\n\nfunc TestList_Pop(t *testing.T) {\n\tlist := NewList()\n\texpectRecords := generateRecords(5)\n\tkey := string(GetTestBytes(0))\n\n\tListPop(t, list, key, true, nil, ErrListNotFound)\n\tseqInfo := HeadTailSeq{Head: initialListSeq, Tail: initialListSeq + 1}\n\n\tfor i := 0; i < len(expectRecords); i++ {\n\t\tseq := generateSeq(&seqInfo, false)\n\t\tnewKey := encodeListKey([]byte(key), seq)\n\t\texpectRecords[i].Key = newKey\n\t\tListPush(t, list, string(newKey), expectRecords[i], false, nil)\n\t}\n\n\tListPop(t, list, key, true, expectRecords[0], nil)\n\texpectRecords = expectRecords[1:]\n\n\tListPop(t, list, key, false, expectRecords[len(expectRecords)-1], nil)\n\texpectRecords = expectRecords[:len(expectRecords)-1]\n\n\tListCmp(t, list, key, expectRecords, false)\n}\n\nfunc TestList_LRem(t *testing.T) {\n\tlist := NewList()\n\trecords := generateRecords(2)\n\tkey := string(GetTestBytes(0))\n\tseqInfo := HeadTailSeq{Head: initialListSeq, Tail: initialListSeq + 1}\n\n\tfor i := 0; i < 3; i++ {\n\t\tseq := generateSeq(&seqInfo, false)\n\t\tnewKey := encodeListKey([]byte(key), seq)\n\t\trecords[0].Key = newKey\n\t\tListPush(t, list, string(newKey), records[0], false, nil)\n\t}\n\n\tseq := generateSeq(&seqInfo, false)\n\tnewKey := encodeListKey([]byte(key), seq)\n\trecords[1].Key = newKey\n\tListPush(t, list, string(newKey), records[1], false, nil)\n\n\tseq = generateSeq(&seqInfo, false)\n\tnewKey = encodeListKey([]byte(key), seq)\n\trecords[0].Key = newKey\n\tListPush(t, list, string(newKey), records[0], false, nil)\n\n\tseq = generateSeq(&seqInfo, false)\n\tnewKey = encodeListKey([]byte(key), seq)\n\trecords[1].Key = newKey\n\tListPush(t, list, string(newKey), records[1], false, nil)\n\n\t// r1 r1 r1 r2 r1 r2\n\texpectRecords := []*Record{records[0], records[0], records[0], records[1], records[0], records[1]}\n\n\tcmp := func(r *Record) (bool, error) {\n\t\treturn bytes.Equal(r.Value, records[0].Value), nil\n\t}\n\n\t// r1 r1 r1 r2 r2\n\terr := list.LRem(key, -1, cmp)\n\trequire.NoError(t, err)\n\texpectRecords = append(expectRecords[0:4], expectRecords[5:]...)\n\tListCmp(t, list, key, expectRecords, false)\n\n\t// r1 r2 r2\n\terr = list.LRem(key, 2, cmp)\n\trequire.NoError(t, err)\n\texpectRecords = expectRecords[2:]\n\tListCmp(t, list, key, expectRecords, false)\n\n\tcmp = func(r *Record) (bool, error) {\n\t\treturn bytes.Equal(r.Value, records[1].Value), nil\n\t}\n\n\t// r1\n\terr = list.LRem(key, 0, cmp)\n\trequire.NoError(t, err)\n\texpectRecords = expectRecords[0:1]\n\tListCmp(t, list, key, expectRecords, false)\n}\n\nfunc TestList_LTrim(t *testing.T) {\n\tlist := NewList()\n\texpectRecords := generateRecords(5)\n\tkey := string(GetTestBytes(0))\n\tseqInfo := HeadTailSeq{Head: initialListSeq, Tail: initialListSeq + 1}\n\n\tfor i := 0; i < len(expectRecords); i++ {\n\t\tseq := generateSeq(&seqInfo, false)\n\t\tnewKey := encodeListKey([]byte(key), seq)\n\t\texpectRecords[i].Key = newKey\n\t\tListPush(t, list, string(newKey), expectRecords[i], false, nil)\n\t}\n\n\terr := list.LTrim(key, 1, 3)\n\trequire.NoError(t, err)\n\texpectRecords = expectRecords[1 : len(expectRecords)-1]\n\tListCmp(t, list, key, expectRecords, false)\n}\n\nfunc TestList_LRemByIndex(t *testing.T) {\n\tlist := NewList()\n\texpectRecords := generateRecords(8)\n\tkey := string(GetTestBytes(0))\n\tseqInfo := HeadTailSeq{Head: initialListSeq, Tail: initialListSeq + 1}\n\n\t// r1 r2 r3 r4 r5 r6 r7 r8\n\tfor i := 0; i < 8; i++ {\n\t\tseq := generateSeq(&seqInfo, false)\n\t\tnewKey := encodeListKey([]byte(key), seq)\n\t\texpectRecords[i].Key = newKey\n\t\tListPush(t, list, string(newKey), expectRecords[i], false, nil)\n\t}\n\n\t// r1 r2 r4 r5 r6 r7 r8\n\terr := list.LRemByIndex(key, []int{2})\n\trequire.NoError(t, err)\n\texpectRecords = append(expectRecords[0:2], expectRecords[3:]...)\n\tListCmp(t, list, key, expectRecords, false)\n\n\t// r2 r6 r7 r8\n\terr = list.LRemByIndex(key, []int{0, 2, 3})\n\trequire.NoError(t, err)\n\texpectRecords = expectRecords[1:]\n\texpectRecords = append(expectRecords[0:1], expectRecords[3:]...)\n\tListCmp(t, list, key, expectRecords, false)\n\n\terr = list.LRemByIndex(key, []int{0, 0, 0})\n\trequire.NoError(t, err)\n\texpectRecords = expectRecords[1:]\n\tListCmp(t, list, key, expectRecords, false)\n}\n\nfunc generateRecords(count int) []*Record {\n\trand.Seed(time.Now().UnixNano())\n\trecords := make([]*Record, count)\n\tfor i := 0; i < count; i++ {\n\t\tkey := GetTestBytes(i)\n\t\tval := GetRandomBytes(24)\n\n\t\trecord := &Record{\n\t\t\tKey:       key,\n\t\t\tValue:     val,\n\t\t\tFileID:    int64(i),\n\t\t\tDataPos:   uint64(rand.Uint32()),\n\t\t\tValueSize: uint32(len(val)),\n\t\t\tTimestamp: uint64(time.Now().Unix()),\n\t\t\tTTL:       uint32(rand.Intn(3600)),\n\t\t\tTxID:      uint64(rand.Intn(1000)),\n\t\t}\n\t\trecords[i] = record\n\t}\n\treturn records\n}\n"
        },
        {
          "name": "lru.go",
          "type": "blob",
          "size": 1.876953125,
          "content": "package nutsdb\n\nimport (\n\t\"container/list\"\n\t\"sync\"\n)\n\n// LRUCache is a least recently used (LRU) cache.\ntype LRUCache struct {\n\tm   map[interface{}]*list.Element\n\tl   *list.List\n\tcap int\n\tmu  *sync.RWMutex\n}\n\n// New creates a new LRUCache with the specified capacity.\nfunc NewLruCache(cap int) *LRUCache {\n\treturn &LRUCache{\n\t\tm:   make(map[interface{}]*list.Element),\n\t\tl:   list.New(),\n\t\tcap: cap,\n\t\tmu:  &sync.RWMutex{},\n\t}\n}\n\n// Add adds a new entry to the cache.\nfunc (c *LRUCache) Add(key interface{}, value interface{}) {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tif c.cap <= 0 {\n\t\treturn\n\t}\n\n\tif c.l.Len() >= c.cap {\n\t\tc.removeOldest()\n\t}\n\n\te := &LruEntry{\n\t\tKey:   key,\n\t\tValue: value,\n\t}\n\tentry := c.l.PushFront(e)\n\n\tc.m[key] = entry\n}\n\n// Get returns the entry associated with the given key, or nil if the key is not in the cache.\nfunc (c *LRUCache) Get(key interface{}) interface{} {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tentry, ok := c.m[key]\n\tif !ok {\n\t\treturn nil\n\t}\n\n\tc.l.MoveToFront(entry)\n\treturn entry.Value.(*LruEntry).Value\n}\n\n// Remove removes the entry associated with the given key from the cache.\nfunc (c *LRUCache) Remove(key interface{}) {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tentry, ok := c.m[key]\n\tif !ok {\n\t\treturn\n\t}\n\n\tc.l.Remove(entry)\n\tdelete(c.m, key)\n}\n\n// Len returns the number of entries in the cache.\nfunc (c *LRUCache) Len() int {\n\tc.mu.RLock()\n\tdefer c.mu.RUnlock()\n\n\treturn c.l.Len()\n}\n\n// Clear clears the cache.\nfunc (c *LRUCache) Clear() {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tc.l.Init()\n\tc.m = make(map[interface{}]*list.Element)\n}\n\n// removeOldest removes the oldest entry from the cache.\nfunc (c *LRUCache) removeOldest() {\n\tentry := c.l.Back()\n\tif entry == nil {\n\t\treturn\n\t}\n\n\tkey := entry.Value.(*LruEntry).Key\n\tdelete(c.m, key)\n\n\tc.l.Remove(entry)\n}\n\n// LruEntry is a struct that represents an entry in the LRU cache.\ntype LruEntry struct {\n\tKey   interface{}\n\tValue interface{}\n}\n"
        },
        {
          "name": "lru_test.go",
          "type": "blob",
          "size": 1.576171875,
          "content": "package nutsdb\n\nimport (\n\t\"testing\"\n)\n\nfunc TestLRUCache(t *testing.T) {\n\tcache := NewLruCache(3)\n\n\t// Add some entries to the cache\n\tcache.Add(1, \"one\")\n\tcache.Add(2, \"two\")\n\tcache.Add(3, \"three\")\n\n\t// Check if cache length is correct\n\tif cache.Len() != 3 {\n\t\tt.Errorf(\"Expected cache length to be 3, got %d\", cache.Len())\n\t}\n\n\t// Test getting values from cache\n\tval := cache.Get(1)\n\tif val != \"one\" {\n\t\tt.Errorf(\"Expected value for key 1 to be 'one', got %v\", val)\n\t}\n\n\t// Test LRU behavior: recently used item should be retained in cache, least recently used should be evicted\n\tcache.Add(4, \"four\")\n\tval = cache.Get(2) // Should return nil as key 2 is the least recently used item\n\tif val != nil {\n\t\tt.Errorf(\"Expected value for key 2 to be nil, got %v\", val)\n\t}\n\n\t// Test removing an entry from cache\n\tcache.Remove(3)\n\tif cache.Len() != 2 {\n\t\tt.Errorf(\"Expected cache length after removal to be 2, got %d\", cache.Len())\n\t}\n\n\t// Test clearing the cache\n\tcache.Clear()\n\tif cache.Len() != 0 {\n\t\tt.Errorf(\"Expected cache length after clearing to be 0, got %d\", cache.Len())\n\t}\n}\n\nfunc TestLRUCache_RemoveOldest(t *testing.T) {\n\tcache := NewLruCache(2)\n\n\t// Add two entries to the cache\n\tcache.Add(1, \"one\")\n\tcache.Add(2, \"two\")\n\n\t// Remove the oldest entry\n\tcache.removeOldest()\n\n\t// Check if cache length is correct\n\tif cache.Len() != 1 {\n\t\tt.Errorf(\"Expected cache length after removing oldest to be 1, got %d\", cache.Len())\n\t}\n\n\t// Check if the oldest entry has been evicted properly\n\tval := cache.Get(1)\n\tif val != nil {\n\t\tt.Errorf(\"Expected value for key 1 to be nil after removing oldest, got %v\", val)\n\t}\n}\n"
        },
        {
          "name": "merge.go",
          "type": "blob",
          "size": 7.708984375,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"os\"\n\t\"time\"\n\n\t\"github.com/xujiajun/utils/strconv2\"\n)\n\nvar ErrDontNeedMerge = errors.New(\"the number of files waiting to be merged is at least 2\")\n\nfunc (db *DB) Merge() error {\n\tdb.mergeStartCh <- struct{}{}\n\treturn <-db.mergeEndCh\n}\n\n// Merge removes dirty data and reduce data redundancy,following these steps:\n//\n// 1. Filter delete or expired entry.\n//\n// 2. Write entry to activeFile if the key not existï¼Œif exist miss this write operation.\n//\n// 3. Filter the entry which is committed.\n//\n// 4. At last remove the merged files.\n//\n// Caveat: merge is Called means starting multiple write transactions, and it\n// will affect the other write request. so execute it at the appropriate time.\nfunc (db *DB) merge() error {\n\tvar (\n\t\toff              int64\n\t\tpendingMergeFIds []int\n\t)\n\n\t// to prevent the initiation of multiple merges simultaneously.\n\tdb.mu.Lock()\n\n\tif db.isMerging {\n\t\tdb.mu.Unlock()\n\t\treturn ErrIsMerging\n\t}\n\n\tdb.isMerging = true\n\tdefer func() {\n\t\tdb.isMerging = false\n\t}()\n\n\t_, pendingMergeFIds = db.getMaxFileIDAndFileIDs()\n\tif len(pendingMergeFIds) < 2 {\n\t\tdb.mu.Unlock()\n\t\treturn ErrDontNeedMerge\n\t}\n\n\tdb.MaxFileID++\n\n\tif !db.opt.SyncEnable && db.opt.RWMode == MMap {\n\t\tif err := db.ActiveFile.rwManager.Sync(); err != nil {\n\t\t\tdb.mu.Unlock()\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif err := db.ActiveFile.rwManager.Release(); err != nil {\n\t\tdb.mu.Unlock()\n\t\treturn err\n\t}\n\n\tvar err error\n\tpath := getDataPath(db.MaxFileID, db.opt.Dir)\n\tdb.ActiveFile, err = db.fm.getDataFile(path, db.opt.SegmentSize)\n\tif err != nil {\n\t\tdb.mu.Unlock()\n\t\treturn err\n\t}\n\n\tdb.ActiveFile.fileID = db.MaxFileID\n\n\tdb.mu.Unlock()\n\n\tmergingPath := make([]string, len(pendingMergeFIds))\n\n\tfor i, pendingMergeFId := range pendingMergeFIds {\n\t\toff = 0\n\t\tpath := getDataPath(int64(pendingMergeFId), db.opt.Dir)\n\t\tfr, err := newFileRecovery(path, db.opt.BufferSizeOfRecovery)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tfor {\n\t\t\tif entry, err := fr.readEntry(off); err == nil {\n\t\t\t\tif entry == nil {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\tif entry.isFilter() {\n\t\t\t\t\toff += entry.Size()\n\t\t\t\t\tif off >= db.opt.SegmentSize {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t// Due to the lack of concurrency safety in the index,\n\t\t\t\t// there is a possibility that a race condition might occur when the merge goroutine reads the index,\n\t\t\t\t// while a transaction is being committed, causing modifications to the index.\n\t\t\t\t// To address this issue, we need to use a transaction to perform this operation.\n\t\t\t\terr := db.Update(func(tx *Tx) error {\n\t\t\t\t\t// check if we have a new entry with same key and bucket\n\t\t\t\t\tif ok := db.isPendingMergeEntry(entry); ok {\n\t\t\t\t\t\tbucket, err := db.bm.GetBucketById(entry.Meta.BucketId)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn err\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbucketName := bucket.Name\n\t\t\t\t\t\tif entry.Meta.Flag == DataLPushFlag {\n\t\t\t\t\t\t\treturn tx.LPushRaw(bucketName, entry.Key, entry.Value)\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif entry.Meta.Flag == DataRPushFlag {\n\t\t\t\t\t\t\treturn tx.RPushRaw(bucketName, entry.Key, entry.Value)\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\treturn tx.put(\n\t\t\t\t\t\t\tbucketName,\n\t\t\t\t\t\t\tentry.Key,\n\t\t\t\t\t\t\tentry.Value,\n\t\t\t\t\t\t\tentry.Meta.TTL,\n\t\t\t\t\t\t\tentry.Meta.Flag,\n\t\t\t\t\t\t\tentry.Meta.Timestamp,\n\t\t\t\t\t\t\tentry.Meta.Ds,\n\t\t\t\t\t\t)\n\t\t\t\t\t}\n\n\t\t\t\t\treturn nil\n\t\t\t\t})\n\t\t\t\tif err != nil {\n\t\t\t\t\t_ = fr.release()\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\toff += entry.Size()\n\t\t\t\tif off >= db.opt.SegmentSize {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t} else {\n\t\t\t\tif errors.Is(err, io.EOF) || errors.Is(err, ErrIndexOutOfBound) || errors.Is(err, io.ErrUnexpectedEOF) || errors.Is(err, ErrHeaderSizeOutOfBounds) {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\treturn fmt.Errorf(\"when merge operation build hintIndex readAt err: %s\", err)\n\t\t\t}\n\t\t}\n\n\t\terr = fr.release()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmergingPath[i] = path\n\t}\n\n\tdb.mu.Lock()\n\tdefer db.mu.Unlock()\n\n\tfor i := 0; i < len(mergingPath); i++ {\n\t\tif err := os.Remove(mergingPath[i]); err != nil {\n\t\t\treturn fmt.Errorf(\"when merge err: %s\", err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (db *DB) mergeWorker() {\n\tvar ticker *time.Ticker\n\n\tif db.opt.MergeInterval != 0 {\n\t\tticker = time.NewTicker(db.opt.MergeInterval)\n\t} else {\n\t\tticker = time.NewTicker(math.MaxInt)\n\t\tticker.Stop()\n\t}\n\n\tfor {\n\t\tselect {\n\t\tcase <-db.mergeStartCh:\n\t\t\tdb.mergeEndCh <- db.merge()\n\t\t\t// if automatic merging is enabled, then after a manual merge\n\t\t\t// the t needs to be reset.\n\t\t\tif db.opt.MergeInterval != 0 {\n\t\t\t\tticker.Reset(db.opt.MergeInterval)\n\t\t\t}\n\t\tcase <-ticker.C:\n\t\t\t_ = db.merge()\n\t\tcase <-db.mergeWorkCloseCh:\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (db *DB) isPendingMergeEntry(entry *Entry) bool {\n\tbucket, err := db.bm.GetBucketById(entry.Meta.BucketId)\n\tif err != nil {\n\t\treturn false\n\t}\n\tbucketId := bucket.Id\n\tswitch {\n\tcase entry.IsBelongsToBPlusTree():\n\t\treturn db.isPendingBtreeEntry(bucketId, entry)\n\tcase entry.IsBelongsToList():\n\t\treturn db.isPendingListEntry(bucketId, entry)\n\tcase entry.IsBelongsToSet():\n\t\treturn db.isPendingSetEntry(bucketId, entry)\n\tcase entry.IsBelongsToSortSet():\n\t\treturn db.isPendingZSetEntry(bucketId, entry)\n\t}\n\treturn false\n}\n\nfunc (db *DB) isPendingBtreeEntry(bucketId BucketId, entry *Entry) bool {\n\tidx, exist := db.Index.bTree.exist(bucketId)\n\tif !exist {\n\t\treturn false\n\t}\n\n\tr, ok := idx.Find(entry.Key)\n\tif !ok {\n\t\treturn false\n\t}\n\n\tif r.IsExpired() {\n\t\tdb.tm.del(bucketId, string(entry.Key))\n\t\tidx.Delete(entry.Key)\n\t\treturn false\n\t}\n\n\tif r.TxID != entry.Meta.TxID || r.Timestamp != entry.Meta.Timestamp {\n\t\treturn false\n\t}\n\n\treturn true\n}\n\nfunc (db *DB) isPendingSetEntry(bucketId BucketId, entry *Entry) bool {\n\tsetIdx, exist := db.Index.set.exist(bucketId)\n\tif !exist {\n\t\treturn false\n\t}\n\n\tisMember, err := setIdx.SIsMember(string(entry.Key), entry.Value)\n\tif err != nil || !isMember {\n\t\treturn false\n\t}\n\n\treturn true\n}\n\nfunc (db *DB) isPendingZSetEntry(bucketId BucketId, entry *Entry) bool {\n\tkey, score := splitStringFloat64Str(string(entry.Key), SeparatorForZSetKey)\n\tsortedSetIdx, exist := db.Index.sortedSet.exist(bucketId)\n\tif !exist {\n\t\treturn false\n\t}\n\ts, err := sortedSetIdx.ZScore(key, entry.Value)\n\tif err != nil || s != score {\n\t\treturn false\n\t}\n\n\treturn true\n}\n\nfunc (db *DB) isPendingListEntry(bucketId BucketId, entry *Entry) bool {\n\tvar userKeyStr string\n\tvar curSeq uint64\n\tvar userKey []byte\n\n\tif entry.Meta.Flag == DataExpireListFlag {\n\t\tuserKeyStr = string(entry.Key)\n\t\tlist, exist := db.Index.list.exist(bucketId)\n\t\tif !exist {\n\t\t\treturn false\n\t\t}\n\n\t\tif _, ok := list.Items[userKeyStr]; !ok {\n\t\t\treturn false\n\t\t}\n\n\t\tt, _ := strconv2.StrToInt64(string(entry.Value))\n\t\tttl := uint32(t)\n\t\tif _, ok := list.TTL[userKeyStr]; !ok {\n\t\t\treturn false\n\t\t}\n\n\t\tif list.TTL[userKeyStr] != ttl || list.TimeStamp[userKeyStr] != entry.Meta.Timestamp {\n\t\t\treturn false\n\t\t}\n\n\t\treturn true\n\t}\n\n\tif entry.Meta.Flag == DataLPushFlag || entry.Meta.Flag == DataRPushFlag {\n\t\tuserKey, curSeq = decodeListKey(entry.Key)\n\t\tuserKeyStr = string(userKey)\n\n\t\tlist, exist := db.Index.list.exist(bucketId)\n\t\tif !exist {\n\t\t\treturn false\n\t\t}\n\n\t\tif _, ok := list.Items[userKeyStr]; !ok {\n\t\t\treturn false\n\t\t}\n\n\t\tr, ok := list.Items[userKeyStr].Find(ConvertUint64ToBigEndianBytes(curSeq))\n\t\tif !ok {\n\t\t\treturn false\n\t\t}\n\n\t\tif !bytes.Equal(r.Key, entry.Key) || r.TxID != entry.Meta.TxID || r.Timestamp != entry.Meta.Timestamp {\n\t\t\treturn false\n\t\t}\n\n\t\treturn true\n\t}\n\n\treturn false\n}\n"
        },
        {
          "name": "merge_test.go",
          "type": "blob",
          "size": 9.8369140625,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"bytes\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n\t\"github.com/xujiajun/utils/strconv2\"\n)\n\nfunc TestDB_MergeForString(t *testing.T) {\n\tbucket := \"bucket\"\n\topts := DefaultOptions\n\topts.SegmentSize = KB\n\topts.Dir = \"/tmp/test-string-merge/\"\n\n\tfor _, idxMode := range []EntryIdxMode{HintKeyValAndRAMIdxMode, HintKeyAndRAMIdxMode} {\n\t\topts.EntryIdxMode = idxMode\n\t\tdb, err := Open(opts)\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\trequire.NoError(t, err)\n\n\t\t// Merge is not needed\n\t\terr = db.Merge()\n\t\trequire.Equal(t, ErrDontNeedMerge, err)\n\n\t\t// Add some data\n\t\tn := 1000\n\t\tfor i := 0; i < n; i++ {\n\t\t\ttxPut(t, db, bucket, GetTestBytes(i), GetTestBytes(i), Persistent, nil, nil)\n\t\t}\n\n\t\t// Delete some data\n\t\tfor i := 0; i < n/2; i++ {\n\t\t\ttxDel(t, db, bucket, GetTestBytes(i), nil)\n\t\t}\n\n\t\t// Merge and check the result\n\t\trequire.NoError(t, db.Merge())\n\n\t\tdbCnt, err := db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(n/2), dbCnt)\n\n\t\t// Check the deleted data is deleted\n\t\tfor i := 0; i < n/2; i++ {\n\t\t\ttxGet(t, db, bucket, GetTestBytes(i), GetTestBytes(i), ErrKeyNotFound)\n\t\t}\n\n\t\t// Check the added data is added\n\t\tfor i := n / 2; i < n; i++ {\n\t\t\ttxGet(t, db, bucket, GetTestBytes(i), GetTestBytes(i), nil)\n\t\t}\n\n\t\t// Close and reopen the db\n\t\trequire.NoError(t, db.Close())\n\n\t\tdb, err = Open(opts)\n\t\trequire.NoError(t, err)\n\n\t\tdbCnt, err = db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(n/2), dbCnt)\n\n\t\t// Check the deleted data is deleted\n\t\tfor i := 0; i < n/2; i++ {\n\t\t\ttxGet(t, db, bucket, GetTestBytes(i), GetTestBytes(i), ErrKeyNotFound)\n\t\t}\n\n\t\t// Check the added data is added\n\t\tfor i := n / 2; i < n; i++ {\n\t\t\ttxGet(t, db, bucket, GetTestBytes(i), GetTestBytes(i), nil)\n\t\t}\n\n\t\trequire.NoError(t, db.Close())\n\t\tremoveDir(opts.Dir)\n\t}\n}\n\nfunc TestDB_MergeForSet(t *testing.T) {\n\tbucket := \"bucket\"\n\topts := DefaultOptions\n\topts.SegmentSize = KB\n\topts.Dir = \"/tmp/test-set-merge/\"\n\n\tfor _, idxMode := range []EntryIdxMode{HintKeyValAndRAMIdxMode, HintKeyAndRAMIdxMode} {\n\t\topts.EntryIdxMode = idxMode\n\t\tdb, err := Open(opts)\n\t\tif exist := db.bm.ExistBucket(DataStructureSet, bucket); !exist {\n\t\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\t}\n\n\t\trequire.NoError(t, err)\n\n\t\t// Merge is not needed\n\t\terr = db.Merge()\n\t\trequire.Equal(t, ErrDontNeedMerge, err)\n\n\t\t// Add some data\n\t\tn := 1000\n\t\tkey := GetTestBytes(0)\n\t\tfor i := 0; i < n; i++ {\n\t\t\ttxSAdd(t, db, bucket, key, GetTestBytes(i), nil, nil)\n\t\t}\n\n\t\t// Delete some data\n\t\tfor i := 0; i < n/2; i++ {\n\t\t\ttxSRem(t, db, bucket, key, GetTestBytes(i), nil)\n\t\t}\n\n\t\t// Pop a random value\n\t\tvar spopValue []byte\n\t\terr = db.Update(func(tx *Tx) error {\n\t\t\tvar err error\n\t\t\tspopValue, err = tx.SPop(bucket, key)\n\t\t\tassertErr(t, err, nil)\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\t// Check the random value is popped\n\t\ttxSIsMember(t, db, bucket, key, spopValue, false)\n\n\t\t// txSPop(t, db, bucket, key,nil)\n\t\tdbCnt, err := db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(n/2-1), dbCnt)\n\n\t\t// Merge and check the result\n\t\trequire.NoError(t, db.Merge())\n\n\t\tdbCnt, err = db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(n/2-1), dbCnt)\n\n\t\t// Check the random value is popped\n\t\ttxSIsMember(t, db, bucket, key, spopValue, false)\n\t\tfor i := n / 2; i < n; i++ {\n\t\t\tv := GetTestBytes(i)\n\t\t\tif bytes.Equal(v, spopValue) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttxSIsMember(t, db, bucket, key, v, true)\n\t\t}\n\n\t\t// Close and reopen the db\n\t\trequire.NoError(t, db.Close())\n\n\t\t// reopen db\n\t\tdb, err = Open(opts)\n\t\trequire.NoError(t, err)\n\n\t\tdbCnt, err = db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(n/2-1), dbCnt)\n\n\t\t// Check the random value is popped\n\t\ttxSIsMember(t, db, bucket, key, spopValue, false)\n\t\tfor i := n / 2; i < n; i++ {\n\t\t\tv := GetTestBytes(i)\n\t\t\tif bytes.Equal(v, spopValue) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttxSIsMember(t, db, bucket, key, v, true)\n\t\t}\n\t\trequire.NoError(t, db.Close())\n\t\tremoveDir(opts.Dir)\n\t}\n}\n\n// TestDB_MergeForZSet is a test function to check the Merge() function of the DB struct\n// It creates a DB with two different EntryIdxMode, then adds and scores each item in the DB\n// It then removes half of the items from the DB, then checks that the items that are left are the same as the ones that were removed\n// It then closes the DB, reopens it, and checks that the items that were removed are now not present\nfunc TestDB_MergeForZSet(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\tn := 1000\n\topts := DefaultOptions\n\topts.SegmentSize = KB\n\topts.Dir = \"/tmp/test-zset-merge/\"\n\n\t// test different EntryIdxMode\n\tfor _, idxMode := range []EntryIdxMode{HintKeyValAndRAMIdxMode, HintKeyAndRAMIdxMode} {\n\n\t\topts.EntryIdxMode = idxMode\n\t\tdb, err := Open(opts)\n\t\tif exist := db.bm.ExistBucket(DataStructureSortedSet, bucket); !exist {\n\t\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\t\t}\n\t\trequire.NoError(t, err)\n\n\t\t// add items\n\t\terr = db.Merge()\n\t\trequire.Equal(t, ErrDontNeedMerge, err)\n\n\t\tfor i := 0; i < n; i++ {\n\t\t\tscore, _ := strconv2.IntToFloat64(i)\n\t\t\ttxZAdd(t, db, bucket, key, GetTestBytes(i), score, nil, nil)\n\t\t}\n\n\t\tfor i := 0; i < n; i++ {\n\t\t\tscore, _ := strconv2.IntToFloat64(i)\n\t\t\ttxZScore(t, db, bucket, key, GetTestBytes(i), score, nil)\n\t\t}\n\n\t\t// remove half of the items\n\t\tfor i := 0; i < n/2; i++ {\n\t\t\ttxZRem(t, db, bucket, key, GetTestBytes(i), nil)\n\t\t}\n\n\t\t// check that the items that are left are the same as the ones that were removed\n\t\tfor i := 0; i < n/2; i++ {\n\t\t\tscore, _ := strconv2.IntToFloat64(i)\n\t\t\ttxZScore(t, db, bucket, key, GetTestBytes(i), score, ErrSortedSetMemberNotExist)\n\t\t}\n\n\t\t// check that the items that are left are the same as the ones that were removed\n\t\tfor i := n / 2; i < n; i++ {\n\t\t\tscore, _ := strconv2.IntToFloat64(i)\n\t\t\ttxZScore(t, db, bucket, key, GetTestBytes(i), score, nil)\n\t\t}\n\n\t\t// check that the number of items in the DB is correct\n\t\tdbCnt, err := db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(n/2), dbCnt)\n\n\t\t// merge\n\t\trequire.NoError(t, db.Merge())\n\n\t\t// check that the number of items in the DB is correct\n\t\tdbCnt, err = db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(n/2), dbCnt)\n\n\t\t// check that the items that were removed are now not present\n\t\tfor i := 0; i < n/2; i++ {\n\t\t\tscore, _ := strconv2.IntToFloat64(i)\n\t\t\ttxZScore(t, db, bucket, key, GetTestBytes(i), score, ErrSortedSetMemberNotExist)\n\t\t}\n\n\t\t// check that the items that are left are the same as the ones that were removed\n\t\tfor i := n / 2; i < n; i++ {\n\t\t\tscore, _ := strconv2.IntToFloat64(i)\n\t\t\ttxZScore(t, db, bucket, key, GetTestBytes(i), score, nil)\n\t\t}\n\n\t\t// close db\n\t\trequire.NoError(t, db.Close())\n\n\t\t// reopen db\n\t\tdb, err = Open(opts)\n\t\trequire.NoError(t, err)\n\t\tdbCnt, err = db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(n/2), dbCnt)\n\n\t\t// check that the items that were removed are now not present\n\t\tfor i := 0; i < n/2; i++ {\n\t\t\tscore, _ := strconv2.IntToFloat64(i)\n\t\t\ttxZScore(t, db, bucket, key, GetTestBytes(i), score, ErrSortedSetMemberNotExist)\n\t\t}\n\n\t\t// check that the items that are left are the same as the ones that were removed\n\t\tfor i := n / 2; i < n; i++ {\n\t\t\tscore, _ := strconv2.IntToFloat64(i)\n\t\t\ttxZScore(t, db, bucket, key, GetTestBytes(i), score, nil)\n\t\t}\n\n\t\trequire.NoError(t, db.Close())\n\t\tremoveDir(opts.Dir)\n\t}\n}\n\n// TestDB_MergeForList tests the Merge() function of the DB struct.\n// It creates a DB with two different EntryIdxMode, pushes and pops data, and then merges the DB.\n// It then reopens the DB and checks that the data is still there.\nfunc TestDB_MergeForList(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\topts := DefaultOptions\n\topts.SegmentSize = KB\n\topts.Dir = \"/tmp/test-list-merge/\"\n\n\t// test different EntryIdxMode\n\tfor _, idxMode := range []EntryIdxMode{HintKeyValAndRAMIdxMode, HintKeyAndRAMIdxMode} {\n\t\topts.EntryIdxMode = idxMode\n\t\tdb, err := Open(opts)\n\t\tif exist := db.bm.ExistBucket(DataStructureList, bucket); !exist {\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\t}\n\n\t\trequire.NoError(t, err)\n\n\t\t// check that we don't need merge\n\t\terr = db.Merge()\n\t\trequire.Equal(t, ErrDontNeedMerge, err)\n\n\t\t// push data\n\t\tn := 1000\n\t\tfor i := 0; i < n; i++ {\n\t\t\ttxPush(t, db, bucket, key, GetTestBytes(i), true, nil, nil)\n\t\t}\n\n\t\tfor i := n; i < 2*n; i++ {\n\t\t\ttxPush(t, db, bucket, key, GetTestBytes(i), false, nil, nil)\n\t\t}\n\n\t\t// pop data\n\t\tfor i := n - 1; i >= n/2; i-- {\n\t\t\ttxPop(t, db, bucket, key, GetTestBytes(i), nil, true)\n\t\t}\n\n\t\tfor i := 2*n - 1; i >= 3*n/2; i-- {\n\t\t\ttxPop(t, db, bucket, key, GetTestBytes(i), nil, false)\n\t\t}\n\n\t\t// trim and remove data\n\t\ttxLTrim(t, db, bucket, key, 0, 9, nil)\n\t\ttxLRem(t, db, bucket, key, 0, GetTestBytes(100), nil)\n\t\ttxLRemByIndex(t, db, bucket, key, nil, []int{7, 8, 9}...)\n\n\t\tdbCnt, err := db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(7), dbCnt)\n\n\t\t// merge\n\t\trequire.NoError(t, db.Merge())\n\n\t\tdbCnt, err = db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(7), dbCnt)\n\n\t\trequire.NoError(t, db.Close())\n\n\t\t// reopen db\n\t\tdb, err = Open(opts)\n\t\trequire.NoError(t, err)\n\n\t\tdbCnt, err = db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(7), dbCnt)\n\n\t\t// pop data\n\t\tfor i := n/2 - 1; i < n/2-8; i-- {\n\t\t\ttxPop(t, db, bucket, key, GetTestBytes(i), nil, true)\n\t\t}\n\n\t\trequire.NoError(t, db.Close())\n\t\tremoveDir(opts.Dir)\n\t}\n}\n"
        },
        {
          "name": "metadata.go",
          "type": "blob",
          "size": 4.828125,
          "content": "package nutsdb\n\n// DataStructure represents the data structure we have already supported\ntype DataStructure = uint16\n\n// DataFlag means the data operations have done by users.\ntype DataFlag = uint16\n\n// DataStatus means the status of data\ntype DataStatus = uint16\n\nconst (\n\t// DataStructureSet represents the data structure set flag\n\tDataStructureSet DataStructure = 0\n\n\t// DataStructureSortedSet represents the data structure sorted set flag\n\tDataStructureSortedSet DataStructure = 1\n\n\t// DataStructureBTree represents the data structure b tree flag\n\tDataStructureBTree DataStructure = 2\n\n\t// DataStructureList represents the data structure list flag\n\tDataStructureList DataStructure = 3\n)\n\nconst (\n\t// DataDeleteFlag represents the data delete flag\n\tDataDeleteFlag DataFlag = 0\n\n\t// DataSetFlag represents the data set flag\n\tDataSetFlag DataFlag = 1\n\n\t// DataLPushFlag represents the data LPush flag\n\tDataLPushFlag DataFlag = 2\n\n\t// DataRPushFlag represents the data RPush flag\n\tDataRPushFlag DataFlag = 3\n\n\t// DataLRemFlag represents the data LRem flag\n\tDataLRemFlag DataFlag = 4\n\n\t// DataLPopFlag represents the data LPop flag\n\tDataLPopFlag DataFlag = 5\n\n\t// DataRPopFlag represents the data RPop flag\n\tDataRPopFlag DataFlag = 6\n\n\t// DataLTrimFlag represents the data LTrim flag\n\tDataLTrimFlag DataFlag = 8\n\n\t// DataZAddFlag represents the data ZAdd flag\n\tDataZAddFlag DataFlag = 9\n\n\t// DataZRemFlag represents the data ZRem flag\n\tDataZRemFlag DataFlag = 10\n\n\t// DataZRemRangeByRankFlag represents the data ZRemRangeByRank flag\n\tDataZRemRangeByRankFlag DataFlag = 11\n\n\t// DataZPopMaxFlag represents the data ZPopMax flag\n\tDataZPopMaxFlag DataFlag = 12\n\n\t// DataZPopMinFlag represents the data aZPopMin flag\n\tDataZPopMinFlag DataFlag = 13\n\n\t// DataSetBucketDeleteFlag represents the delete Set bucket flag\n\tDataSetBucketDeleteFlag DataFlag = 14\n\n\t// DataSortedSetBucketDeleteFlag represents the delete Sorted Set bucket flag\n\tDataSortedSetBucketDeleteFlag DataFlag = 15\n\n\t// DataBTreeBucketDeleteFlag represents the delete BTree bucket flag\n\tDataBTreeBucketDeleteFlag DataFlag = 16\n\n\t// DataListBucketDeleteFlag represents the delete List bucket flag\n\tDataListBucketDeleteFlag DataFlag = 17\n\n\t// DataLRemByIndex represents the data LRemByIndex flag\n\tDataLRemByIndex DataFlag = 18\n\n\t// DataExpireListFlag represents that set ttl for the list\n\tDataExpireListFlag DataFlag = 19\n)\n\nconst (\n\t// UnCommitted represents the tx unCommitted status\n\tUnCommitted uint16 = 0\n\n\t// Committed represents the tx committed status\n\tCommitted uint16 = 1\n)\n\n// Persistent represents the data persistent flag\nconst Persistent uint32 = 0\n\ntype MetaData struct {\n\tKeySize    uint32\n\tValueSize  uint32\n\tTimestamp  uint64\n\tTTL        uint32\n\tFlag       DataFlag // delete / set\n\tBucketSize uint32\n\tTxID       uint64\n\tStatus     DataStatus    // committed / uncommitted\n\tDs         DataStructure // data structure\n\tCrc        uint32\n\tBucketId   BucketId\n}\n\nfunc (meta *MetaData) Size() int64 {\n\t// CRC\n\tsize := 4\n\n\tsize += UvarintSize(uint64(meta.KeySize))\n\tsize += UvarintSize(uint64(meta.ValueSize))\n\tsize += UvarintSize(meta.Timestamp)\n\tsize += UvarintSize(uint64(meta.TTL))\n\tsize += UvarintSize(uint64(meta.Flag))\n\tsize += UvarintSize(meta.TxID)\n\tsize += UvarintSize(uint64(meta.Status))\n\tsize += UvarintSize(uint64(meta.Ds))\n\tsize += UvarintSize(meta.BucketId)\n\n\treturn int64(size)\n}\n\nfunc (meta *MetaData) PayloadSize() int64 {\n\treturn int64(meta.BucketSize) + int64(meta.KeySize) + int64(meta.ValueSize)\n}\n\nfunc NewMetaData() *MetaData {\n\treturn new(MetaData)\n}\n\nfunc (meta *MetaData) WithKeySize(keySize uint32) *MetaData {\n\tmeta.KeySize = keySize\n\treturn meta\n}\n\nfunc (meta *MetaData) WithValueSize(valueSize uint32) *MetaData {\n\tmeta.ValueSize = valueSize\n\treturn meta\n}\n\nfunc (meta *MetaData) WithTimeStamp(timestamp uint64) *MetaData {\n\tmeta.Timestamp = timestamp\n\treturn meta\n}\n\nfunc (meta *MetaData) WithTTL(ttl uint32) *MetaData {\n\tmeta.TTL = ttl\n\treturn meta\n}\n\nfunc (meta *MetaData) WithFlag(flag uint16) *MetaData {\n\tmeta.Flag = flag\n\treturn meta\n}\n\nfunc (meta *MetaData) WithBucketSize(bucketSize uint32) *MetaData {\n\tmeta.BucketSize = bucketSize\n\treturn meta\n}\n\nfunc (meta *MetaData) WithTxID(txID uint64) *MetaData {\n\tmeta.TxID = txID\n\treturn meta\n}\n\nfunc (meta *MetaData) WithStatus(status uint16) *MetaData {\n\tmeta.Status = status\n\treturn meta\n}\n\nfunc (meta *MetaData) WithDs(ds uint16) *MetaData {\n\tmeta.Ds = ds\n\treturn meta\n}\n\nfunc (meta *MetaData) WithCrc(crc uint32) *MetaData {\n\tmeta.Crc = crc\n\treturn meta\n}\n\nfunc (meta *MetaData) WithBucketId(bucketID uint64) *MetaData {\n\tmeta.BucketId = bucketID\n\treturn meta\n}\n\nfunc (meta *MetaData) IsBPlusTree() bool {\n\treturn meta.Ds == DataStructureBTree\n}\n\nfunc (meta *MetaData) IsSet() bool {\n\treturn meta.Ds == DataStructureSet\n}\n\nfunc (meta *MetaData) IsSortSet() bool {\n\treturn meta.Ds == DataStructureSortedSet\n}\n\nfunc (meta *MetaData) IsList() bool {\n\treturn meta.Ds == DataStructureList\n}\n"
        },
        {
          "name": "options.go",
          "type": "blob",
          "size": 6.34765625,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport \"time\"\n\n// EntryIdxMode represents entry index mode.\ntype EntryIdxMode int\n\nconst (\n\t// HintKeyValAndRAMIdxMode represents ram index (key and value) mode.\n\tHintKeyValAndRAMIdxMode EntryIdxMode = iota\n\n\t// HintKeyAndRAMIdxMode represents ram index (only key) mode.\n\tHintKeyAndRAMIdxMode\n)\n\ntype ExpiredDeleteType uint8\n\nconst (\n\t// TimeWheel represents use time wheel to do expired deletion\n\tTimeWheel ExpiredDeleteType = iota\n\n\t// TimeHeap represents use time heap to do expired deletion\n\tTimeHeap\n)\n\n// An ErrorHandler handles an error occurred during transaction.\ntype ErrorHandler interface {\n\tHandleError(err error)\n}\n\n// The ErrorHandlerFunc type is an adapter to ErrorHandler.\ntype ErrorHandlerFunc func(err error)\n\nfunc (fn ErrorHandlerFunc) HandleError(err error) {\n\tfn(err)\n}\n\ntype LessFunc func(l, r string) bool\n\n// Options records params for creating DB object.\ntype Options struct {\n\t// Dir represents Open the database located in which dir.\n\tDir string\n\n\t// EntryIdxMode represents using which mode to index the entries.\n\tEntryIdxMode EntryIdxMode\n\n\t// RWMode represents the read and write mode.\n\t// RWMode includes two options: FileIO and MMap.\n\t// FileIO represents the read and write mode using standard I/O.\n\t// MMap represents the read and write mode using mmap.\n\tRWMode      RWMode\n\tSegmentSize int64\n\n\t// NodeNum represents the node number.\n\t// Default NodeNum is 1. NodeNum range [1,1023].\n\tNodeNum int64\n\n\t// SyncEnable represents if call Sync() function.\n\t// if SyncEnable is false, high write performance but potential data loss likely.\n\t// if SyncEnable is true, slower but persistent.\n\tSyncEnable bool\n\n\t// MaxFdNumsInCache represents the max numbers of fd in cache.\n\tMaxFdNumsInCache int\n\n\t// CleanFdsCacheThreshold represents the maximum threshold for recycling fd, it should be between 0 and 1.\n\tCleanFdsCacheThreshold float64\n\n\t// BufferSizeOfRecovery represents the buffer size of recoveryReader buffer Size\n\tBufferSizeOfRecovery int\n\n\t// CcWhenClose represent initiative GC when calling db.Close()\n\tGCWhenClose bool\n\n\t// CommitBufferSize represent allocated memory for tx\n\tCommitBufferSize int64\n\n\t// ErrorHandler handles an error occurred during transaction.\n\t// Example:\n\t//     func triggerAlertError(err error) {\n\t//     \t   if errors.Is(err, targetErr) {\n\t//         \t\talertManager.TriggerAlert()\n\t//     \t   }\n\t//     })\n\tErrorHandler ErrorHandler\n\n\t// LessFunc is a function that sorts keys.\n\tLessFunc LessFunc\n\n\t// MergeInterval represent the interval for automatic merges, with 0 meaning automatic merging is disabled.\n\tMergeInterval time.Duration\n\n\t// MaxBatchCount represents max entries in batch\n\tMaxBatchCount int64\n\n\t// MaxBatchSize represents max batch size in bytes\n\tMaxBatchSize int64\n\n\t// ExpiredDeleteType represents the data structure used for expired deletion\n\t// TimeWheel means use the time wheel, You can use it when you need high performance or low memory usage\n\t// TimeHeap means use the time heap, You can use it when you need to delete precisely or memory usage will be high\n\tExpiredDeleteType ExpiredDeleteType\n\n\t// max write record num\n\tMaxWriteRecordCount int64\n\n\t// cache size for HintKeyAndRAMIdxMode\n\tHintKeyAndRAMIdxCacheSize int\n}\n\nconst (\n\tB = 1\n\n\tKB = 1024 * B\n\n\tMB = 1024 * KB\n\n\tGB = 1024 * MB\n)\n\n// defaultSegmentSize is default data file size.\nvar defaultSegmentSize int64 = 256 * MB\n\n// DefaultOptions represents the default options.\nvar DefaultOptions = func() Options {\n\treturn Options{\n\t\tEntryIdxMode:      HintKeyValAndRAMIdxMode,\n\t\tSegmentSize:       defaultSegmentSize,\n\t\tNodeNum:           1,\n\t\tRWMode:            FileIO,\n\t\tSyncEnable:        true,\n\t\tCommitBufferSize:  4 * MB,\n\t\tMergeInterval:     2 * time.Hour,\n\t\tMaxBatchSize:      (15 * defaultSegmentSize / 4) / 100,\n\t\tMaxBatchCount:     (15 * defaultSegmentSize / 4) / 100 / 100,\n\t\tHintKeyAndRAMIdxCacheSize: 0,\n\t\tExpiredDeleteType: TimeWheel,\n\t}\n}()\n\ntype Option func(*Options)\n\nfunc WithDir(dir string) Option {\n\treturn func(opt *Options) {\n\t\topt.Dir = dir\n\t}\n}\n\nfunc WithEntryIdxMode(entryIdxMode EntryIdxMode) Option {\n\treturn func(opt *Options) {\n\t\topt.EntryIdxMode = entryIdxMode\n\t}\n}\n\nfunc WithRWMode(rwMode RWMode) Option {\n\treturn func(opt *Options) {\n\t\topt.RWMode = rwMode\n\t}\n}\n\nfunc WithSegmentSize(size int64) Option {\n\treturn func(opt *Options) {\n\t\topt.SegmentSize = size\n\t}\n}\n\nfunc WithMaxBatchCount(count int64) Option {\n\treturn func(opt *Options) {\n\t\topt.MaxBatchCount = count\n\t}\n}\n\nfunc WithHintKeyAndRAMIdxCacheSize(size int) Option {\n    return func(opt *Options) {\n        opt.HintKeyAndRAMIdxCacheSize = size\n    }\n}\n\nfunc WithMaxBatchSize(size int64) Option {\n\treturn func(opt *Options) {\n\t\topt.MaxBatchSize = size\n\t}\n}\n\nfunc WithNodeNum(num int64) Option {\n\treturn func(opt *Options) {\n\t\topt.NodeNum = num\n\t}\n}\n\nfunc WithSyncEnable(enable bool) Option {\n\treturn func(opt *Options) {\n\t\topt.SyncEnable = enable\n\t}\n}\n\nfunc WithMaxFdNumsInCache(num int) Option {\n\treturn func(opt *Options) {\n\t\topt.MaxFdNumsInCache = num\n\t}\n}\n\nfunc WithCleanFdsCacheThreshold(threshold float64) Option {\n\treturn func(opt *Options) {\n\t\topt.CleanFdsCacheThreshold = threshold\n\t}\n}\n\nfunc WithBufferSizeOfRecovery(size int) Option {\n\treturn func(opt *Options) {\n\t\topt.BufferSizeOfRecovery = size\n\t}\n}\n\nfunc WithGCWhenClose(enable bool) Option {\n\treturn func(opt *Options) {\n\t\topt.GCWhenClose = enable\n\t}\n}\n\nfunc WithErrorHandler(errorHandler ErrorHandler) Option {\n\treturn func(opt *Options) {\n\t\topt.ErrorHandler = errorHandler\n\t}\n}\n\nfunc WithCommitBufferSize(commitBufferSize int64) Option {\n\treturn func(opt *Options) {\n\t\topt.CommitBufferSize = commitBufferSize\n\t}\n}\n\nfunc WithLessFunc(lessFunc LessFunc) Option {\n\treturn func(opt *Options) {\n\t\topt.LessFunc = lessFunc\n\t}\n}\n\nfunc WithMaxWriteRecordCount(maxWriteRecordCount int64) Option {\n\treturn func(opt *Options) {\n\t\topt.MaxWriteRecordCount = maxWriteRecordCount\n\t}\n}\n"
        },
        {
          "name": "options_test.go",
          "type": "blob",
          "size": 2.7529296875,
          "content": "package nutsdb\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestWithNodeNum(t *testing.T) {\n\tInitOpt(\"\", true)\n\tdb, err := Open(\n\t\topt,\n\t\tWithNodeNum(1011),\n\t)\n\tassert.NoError(t, err)\n\tassert.Equal(t, int64(1011), db.opt.NodeNum)\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n\nfunc TestWithMaxBatchCount(t *testing.T) {\n\tInitOpt(\"\", true)\n\tdb, err := Open(\n\t\topt,\n\t\tWithMaxBatchCount(10),\n\t)\n\tassert.NoError(t, err)\n\tassert.Equal(t, int64(10), db.getMaxBatchCount())\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n\nfunc TestWithMaxBatchSize(t *testing.T) {\n\tInitOpt(\"\", true)\n\tdb, err := Open(\n\t\topt,\n\t\tWithMaxBatchSize(100),\n\t)\n\tassert.NoError(t, err)\n\tassert.Equal(t, int64(100), db.getMaxBatchSize())\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n\nfunc TestWithHintKeyAndRAMIdxCacheSize(t *testing.T) {\n\tInitOpt(\"\", true)\n\tdb, err := Open(\n\t\topt,\n\t\tWithHintKeyAndRAMIdxCacheSize(100),\n\t)\n\tassert.NoError(t, err)\n\tassert.Equal(t, 100, db.getHintKeyAndRAMIdxCacheSize())\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n\nfunc TestWithMaxWriteRecordCount(t *testing.T) {\n\tInitOpt(\"\", true)\n\tdb, err := Open(\n\t\topt,\n\t\tWithMaxWriteRecordCount(100),\n\t)\n\tassert.NoError(t, err)\n\tassert.Equal(t, int64(100), db.getMaxWriteRecordCount())\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n\nfunc TestWithRWMode(t *testing.T) {\n\tdb, err = Open(DefaultOptions,\n\t\tWithDir(\"/tmp/nutsdb\"),\n\t\tWithRWMode(MMap),\n\t)\n\tassert.NoError(t, err)\n\tassert.Equal(t, db.opt.RWMode, MMap)\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n\nfunc TestWithSyncEnable(t *testing.T) {\n\tdb, err = Open(DefaultOptions,\n\t\tWithDir(\"/tmp/nutsdb\"),\n\t\tWithSyncEnable(false),\n\t)\n\n\tassert.NoError(t, err)\n\tassert.False(t, db.opt.SyncEnable)\n\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n\nfunc TestWithMaxFdNumsInCache(t *testing.T) {\n\tdb, err = Open(DefaultOptions,\n\t\tWithDir(\"/tmp/nutsdb\"),\n\t\tWithMaxFdNumsInCache(100),\n\t)\n\n\tassert.NoError(t, err)\n\tassert.Equal(t, db.opt.MaxFdNumsInCache, 100)\n\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n\nfunc TestWithCleanFdsCacheThreshold(t *testing.T) {\n\tdb, err = Open(DefaultOptions,\n\t\tWithDir(\"/tmp/nutsdb\"),\n\t\tWithCleanFdsCacheThreshold(0.5),\n\t)\n\n\tassert.NoError(t, err)\n\tassert.Equal(t, db.opt.CleanFdsCacheThreshold, 0.5)\n\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n\nfunc TestWithErrorHandler(t *testing.T) {\n\tdb, err = Open(DefaultOptions,\n\t\tWithDir(\"/tmp/nutsdb\"),\n\t\tWithErrorHandler(ErrorHandlerFunc(func(err error) {\n\t\t})),\n\t)\n\tassert.NoError(t, err)\n\tassert.NotNil(t, db.opt.ErrorHandler)\n\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n\nfunc TestWithLessFunc(t *testing.T) {\n\tdb, err = Open(DefaultOptions,\n\t\tWithDir(\"/tmp/nutsdb\"),\n\t\tWithLessFunc(func(l, r string) bool {\n\t\t\treturn len(l) < len(r)\n\t\t}),\n\t)\n\tassert.NoError(t, err)\n\tassert.NotNil(t, db.opt.LessFunc)\n\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n"
        },
        {
          "name": "pending.go",
          "type": "blob",
          "size": 3.1337890625,
          "content": "package nutsdb\n\n// EntryStatus represents the Entry status in the current Tx\ntype EntryStatus = uint8\n\nconst (\n\t// NotFoundEntry means there is no changes for this entry in current Tx\n\tNotFoundEntry EntryStatus = 0\n\t// EntryDeleted means this Entry has been deleted in the current Tx\n\tEntryDeleted EntryStatus = 1\n\t// EntryUpdated means this Entry has been updated in the current Tx\n\tEntryUpdated EntryStatus = 2\n)\n\n// BucketStatus represents the current status of bucket in current Tx\ntype BucketStatus = uint8\n\nconst (\n\t// BucketStatusExistAlready means this bucket already exists\n\tBucketStatusExistAlready = 1\n\t// BucketStatusDeleted means this bucket is already deleted\n\tBucketStatusDeleted = 2\n\t// BucketStatusNew means this bucket is created in current Tx\n\tBucketStatusNew = 3\n\t// BucketStatusUpdated means this bucket is updated in current Tx\n\tBucketStatusUpdated = 4\n\t// BucketStatusUnknown means this bucket doesn't exist\n\tBucketStatusUnknown = 5\n)\n\n// pendingBucketList the uncommitted bucket changes in this Tx\ntype pendingBucketList map[Ds]map[BucketName]*Bucket\n\n// pendingEntriesInBTree means the changes Entries in DataStructureBTree in the Tx\ntype pendingEntriesInBTree map[BucketName]map[string]*Entry\n\n// pendingEntryList the uncommitted Entry changes in this Tx\ntype pendingEntryList struct {\n\tentriesInBTree pendingEntriesInBTree\n\tentries        map[Ds]map[BucketName][]*Entry\n\tsize           int\n}\n\n// newPendingEntriesList create a new pendingEntryList object for a Tx\nfunc newPendingEntriesList() *pendingEntryList {\n\tpending := &pendingEntryList{\n\t\tentriesInBTree: map[BucketName]map[string]*Entry{},\n\t\tentries:        map[Ds]map[BucketName][]*Entry{},\n\t\tsize:           0,\n\t}\n\treturn pending\n}\n\n// submitEntry submit an entry into pendingEntryList\nfunc (pending *pendingEntryList) submitEntry(ds Ds, bucket string, e *Entry) {\n\tswitch ds {\n\tcase DataStructureBTree:\n\t\tif _, exist := pending.entriesInBTree[bucket]; !exist {\n\t\t\tpending.entriesInBTree[bucket] = map[string]*Entry{}\n\t\t}\n\t\tif _, exist := pending.entriesInBTree[bucket][string(e.Key)]; !exist {\n\t\t\tpending.entriesInBTree[bucket][string(e.Key)] = e\n\t\t\tpending.size++\n\t\t}\n\tdefault:\n\t\tif _, exist := pending.entries[ds]; !exist {\n\t\t\tpending.entries[ds] = map[BucketName][]*Entry{}\n\t\t}\n\t\tentries := pending.entries[ds][bucket]\n\t\tentries = append(entries, e)\n\t\tpending.entries[ds][bucket] = entries\n\t\tpending.size++\n\t}\n}\n\n// rangeBucket input a range handler function f and call it with every bucket in pendingBucketList\nfunc (p pendingBucketList) rangeBucket(f func(bucket *Bucket) error) error {\n\tfor _, bucketsInDs := range p {\n\t\tfor _, bucket := range bucketsInDs {\n\t\t\terr := f(bucket)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// toList collect all the entries in pendingEntryList to a list.\nfunc (pending *pendingEntryList) toList() []*Entry {\n\tlist := make([]*Entry, 0, pending.size)\n\tfor _, entriesInBucket := range pending.entriesInBTree {\n\t\tfor _, entry := range entriesInBucket {\n\t\t\tlist = append(list, entry)\n\t\t}\n\t}\n\tfor _, entriesInDS := range pending.entries {\n\t\tfor _, entries := range entriesInDS {\n\t\t\tfor _, entry := range entries {\n\t\t\t\tlist = append(list, entry)\n\t\t\t}\n\t\t}\n\t}\n\treturn list\n}\n"
        },
        {
          "name": "record.go",
          "type": "blob",
          "size": 2.1123046875,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"time\"\n)\n\n// Record means item of indexes in memory\ntype Record struct {\n\tKey       []byte\n\tValue     []byte\n\tFileID    int64\n\tDataPos   uint64\n\tValueSize uint32\n\tTimestamp uint64\n\tTTL       uint32\n\tTxID      uint64\n}\n\n// IsExpired returns the record if expired or not.\nfunc (r *Record) IsExpired() bool {\n\treturn IsExpired(r.TTL, r.Timestamp)\n}\n\n// IsExpired checks the ttl if expired or not.\nfunc IsExpired(ttl uint32, timestamp uint64) bool {\n\tif ttl == Persistent {\n\t\treturn false\n\t}\n\n\tnow := time.UnixMilli(time.Now().UnixMilli())\n\texpireTime := time.UnixMilli(int64(timestamp))\n\texpireTime = expireTime.Add(time.Duration(ttl) * time.Second)\n\n\treturn expireTime.Before(now)\n}\n\n// NewRecord generate a record Obj\nfunc NewRecord() *Record {\n\treturn new(Record)\n}\n\nfunc (r *Record) WithKey(k []byte) *Record {\n\tr.Key = k\n\treturn r\n}\n\n// WithValue set the Value to Record\nfunc (r *Record) WithValue(v []byte) *Record {\n\tr.Value = v\n\treturn r\n}\n\n// WithFileId set FileID to Record\nfunc (r *Record) WithFileId(fid int64) *Record {\n\tr.FileID = fid\n\treturn r\n}\n\n// WithDataPos set DataPos to Record\nfunc (r *Record) WithDataPos(pos uint64) *Record {\n\tr.DataPos = pos\n\treturn r\n}\n\nfunc (r *Record) WithValueSize(valueSize uint32) *Record {\n\tr.ValueSize = valueSize\n\treturn r\n}\n\nfunc (r *Record) WithTimestamp(timestamp uint64) *Record {\n\tr.Timestamp = timestamp\n\treturn r\n}\n\nfunc (r *Record) WithTTL(ttl uint32) *Record {\n\tr.TTL = ttl\n\treturn r\n}\n\nfunc (r *Record) WithTxID(txID uint64) *Record {\n\tr.TxID = txID\n\treturn r\n}\n"
        },
        {
          "name": "recovery_reader.go",
          "type": "blob",
          "size": 2.7001953125,
          "content": "package nutsdb\n\nimport (\n\t\"bufio\"\n\t\"io\"\n\t\"os\"\n)\n\n// fileRecovery use bufio.Reader to read entry\ntype fileRecovery struct {\n\tfd     *os.File\n\treader *bufio.Reader\n\tsize   int64\n}\n\nfunc newFileRecovery(path string, bufSize int) (fr *fileRecovery, err error) {\n\tfd, err := os.OpenFile(path, os.O_RDWR, os.ModePerm)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbufSize = calBufferSize(bufSize)\n\tfileInfo, err := fd.Stat()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &fileRecovery{\n\t\tfd:     fd,\n\t\treader: bufio.NewReaderSize(fd, bufSize),\n\t\tsize:   fileInfo.Size(),\n\t}, nil\n}\n\n// readEntry will read an Entry from disk.\nfunc (fr *fileRecovery) readEntry(off int64) (e *Entry, err error) {\n\tvar size int64 = MaxEntryHeaderSize\n\t// Since MaxEntryHeaderSize may be larger than the actual Header, it needs to be calculated\n\tif off+size > fr.size {\n\t\tsize = fr.size - off\n\t}\n\n\tbuf := make([]byte, size)\n\t_, err = fr.fd.Seek(off, 0)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t_, err = fr.fd.Read(buf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\te = new(Entry)\n\theaderSize, err := e.ParseMeta(buf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif e.IsZero() {\n\t\treturn nil, nil\n\t}\n\n\theaderBuf := buf[:headerSize]\n\tremainingBuf := buf[headerSize:]\n\n\tpayloadSize := e.Meta.PayloadSize()\n\tdataBuf := make([]byte, payloadSize)\n\texcessSize := size - headerSize\n\n\tif payloadSize <= excessSize {\n\t\tcopy(dataBuf, remainingBuf[:payloadSize])\n\t} else {\n\t\tcopy(dataBuf, remainingBuf)\n\t\t_, err := fr.fd.Read(dataBuf[excessSize:])\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\terr = e.ParsePayload(dataBuf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcrc := e.GetCrc(headerBuf)\n\tif crc != e.Meta.Crc {\n\t\treturn nil, ErrCrc\n\t}\n\n\treturn e, nil\n}\n\nfunc (fr *fileRecovery) readBucket() (b *Bucket, err error) {\n\tbuf := make([]byte, BucketMetaSize)\n\t_, err = io.ReadFull(fr.reader, buf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tmeta := new(BucketMeta)\n\tmeta.Decode(buf)\n\tbucket := new(Bucket)\n\tbucket.Meta = meta\n\tdataBuf := make([]byte, meta.Size)\n\t_, err = io.ReadFull(fr.reader, dataBuf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\terr = bucket.Decode(dataBuf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif bucket.GetCRC(buf, dataBuf) != bucket.Meta.Crc {\n\t\treturn nil, ErrBucketCrcInvalid\n\t}\n\n\treturn bucket, nil\n}\n\n// calBufferSize calculates the buffer size of bufio.Reader\n// if the size < 4 * KB, use 4 * KB as the size of buffer in bufio.Reader\n// if the size > 4 * KB, use the nearly blockSize buffer as the size of buffer in bufio.Reader\nfunc calBufferSize(size int) int {\n\tblockSize := 4 * KB\n\tif size < blockSize {\n\t\treturn blockSize\n\t}\n\thasRest := (size%blockSize == 0)\n\tif hasRest {\n\t\treturn (size/blockSize + 1) * blockSize\n\t}\n\treturn size\n}\n\nfunc (fr *fileRecovery) release() error {\n\treturn fr.fd.Close()\n}\n"
        },
        {
          "name": "recovery_reader_test.go",
          "type": "blob",
          "size": 1.6279296875,
          "content": "package nutsdb\n\nimport (\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n\t\"os\"\n\t\"testing\"\n)\n\nfunc Test_readEntry(t *testing.T) {\n\tpath := \"/tmp/test_read_entry\"\n\n\tfd, err := os.OpenFile(path, os.O_TRUNC|os.O_CREATE|os.O_RDWR, os.ModePerm)\n\trequire.NoError(t, err)\n\tmeta := NewMetaData().WithKeySize(uint32(len(\"key\"))).\n\t\tWithValueSize(uint32(len(\"val\"))).WithTimeStamp(1547707905).\n\t\tWithTTL(Persistent).WithFlag(DataSetFlag).WithBucketId(1)\n\n\texpect := NewEntry().WithKey([]byte(\"key\")).WithMeta(meta).WithValue([]byte(\"val\"))\n\n\t_, err = fd.Write(expect.Encode())\n\trequire.NoError(t, err)\n\n\tf, err := newFileRecovery(path, 4096)\n\trequire.NoError(t, err)\n\n\tentry, err := f.readEntry(0)\n\trequire.NoError(t, err)\n\n\tassert.Equal(t, expect.Encode(), entry.Encode())\n\n\terr = fd.Close()\n\trequire.NoError(t, err)\n\n}\n\nfunc Test_fileRecovery_readBucket(t *testing.T) {\n\tfilePath := \"bucket_test_data\"\n\tbucket := &Bucket{\n\t\tMeta: &BucketMeta{\n\t\t\tOp: BucketInsertOperation,\n\t\t},\n\t\tId:   1,\n\t\tDs:   DataStructureBTree,\n\t\tName: \"bucket_1\",\n\t}\n\tbytes := bucket.Encode()\n\n\tfd, err := os.OpenFile(filePath, os.O_RDWR|os.O_CREATE, os.ModePerm)\n\tdefer func() {\n\t\terr = fd.Close()\n\t\tassert.Nil(t, err)\n\t\terr = os.Remove(filePath)\n\t\tassert.Nil(t, nil)\n\t}()\n\tassert.Nil(t, err)\n\t_, err = fd.Write(bytes)\n\tassert.Nil(t, err)\n\n\tfr, err := newFileRecovery(filePath, 4*MB)\n\tassert.Nil(t, err)\n\treadBucket, err := fr.readBucket()\n\tassert.Nil(t, err)\n\tassert.Equal(t, readBucket.Meta.Op, BucketInsertOperation)\n\tassert.Equal(t, int64(8+2+8), int64(readBucket.Meta.Size))\n\tassert.Equal(t, BucketId(1), readBucket.Id)\n\tassert.Equal(t, readBucket.Name, \"bucket_1\")\n}\n"
        },
        {
          "name": "rwmanager.go",
          "type": "blob",
          "size": 1.0888671875,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\n// RWMode represents the read and write mode.\ntype RWMode int\n\nconst (\n\t// FileIO represents the read and write mode using standard I/O.\n\tFileIO RWMode = iota\n\n\t// MMap represents the read and write mode using mmap.\n\tMMap\n)\n\n// RWManager represents an interface to a RWManager.\ntype RWManager interface {\n\tWriteAt(b []byte, off int64) (n int, err error)\n\tReadAt(b []byte, off int64) (n int, err error)\n\tSync() (err error)\n\tRelease() (err error)\n\tSize() int64\n\tClose() (err error)\n}\n"
        },
        {
          "name": "rwmanager_fileio_test.go",
          "type": "blob",
          "size": 0.8349609375,
          "content": "package nutsdb\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestRWManager_FileIO_All(t *testing.T) {\n\n\tfilePath := \"/tmp/foo_rw_fileio\"\n\tmaxFdNums := 20\n\tcleanThreshold := 0.5\n\tvar fdm *fdManager\n\n\tt.Run(\"test write read\", func(t *testing.T) {\n\t\tfdm = newFdm(maxFdNums, cleanThreshold)\n\t\tfd, err := fdm.getFd(filePath)\n\t\tif err != nil {\n\t\t\trequire.NoError(t, err)\n\t\t}\n\n\t\trwManager := &FileIORWManager{fd, filePath, fdm, 256 * MB}\n\t\tb := []byte(\"hello\")\n\t\toff := int64(3)\n\t\t_, err = rwManager.WriteAt(b, off)\n\t\tif err != nil {\n\t\t\trequire.NoError(t, err)\n\t\t}\n\n\t\tbucketBufLen := len(b)\n\t\tbucketBuf := make([]byte, bucketBufLen)\n\t\tn, err := rwManager.ReadAt(bucketBuf, off)\n\t\tif err != nil {\n\t\t\trequire.NoError(t, err)\n\t\t}\n\n\t\tassert.Equal(t, bucketBufLen, n)\n\t\tassert.Equal(t, b, bucketBuf)\n\t})\n}\n"
        },
        {
          "name": "rwmanager_mmap_test.go",
          "type": "blob",
          "size": 3.689453125,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/xujiajun/mmap-go\"\n)\n\nfunc TestRWManager_MMap_Release(t *testing.T) {\n\tfilePath := \"/tmp/foo_rw_MMap\"\n\tfdm := newFileManager(MMap, 1024, 0.5, 256*MB)\n\trwmanager, err := fdm.getMMapRWManager(filePath, 1024, 256*MB)\n\tif err != nil {\n\t\tt.Error(\"err TestRWManager_MMap_Release getMMapRWManager\")\n\t}\n\n\tb := []byte(\"hello\")\n\toff := int64(0)\n\t_, err = rwmanager.WriteAt(b, off)\n\tif err != nil {\n\t\tt.Error(\"err TestRWManager_MMap_Release WriteAt\")\n\t}\n\n\tif !rwmanager.IsActive() {\n\t\tt.Error(\"err TestRWManager_MMap_Release FdInfo:using not correct\")\n\t}\n\n\terr = rwmanager.Release()\n\tif err != nil {\n\t\tt.Error(\"err TestRWManager_MMap_Release Release\")\n\t}\n\n\tif rwmanager.IsActive() {\n\t\tt.Error(\"err TestRWManager_MMap_Release Release Failed\")\n\t}\n}\n\nfunc (rwmanager *MMapRWManager) IsActive() bool {\n\treturn rwmanager.fdm.cache[rwmanager.path].using != 0\n}\n\nfunc TestRWManager_MMap_WriteAt(t *testing.T) {\n\tfilePath := \"/tmp/foo_rw_filemmap\"\n\tmaxFdNums := 1024\n\tcleanThreshold := 0.5\n\tvar fdm = newFdm(maxFdNums, cleanThreshold)\n\n\tfd, err := fdm.getFd(filePath)\n\tif err != nil {\n\t\trequire.NoError(t, err)\n\t}\n\tdefer os.Remove(fd.Name())\n\n\terr = Truncate(filePath, 1024, fd)\n\tif err != nil {\n\t\trequire.NoError(t, err)\n\n\t}\n\tm, err := mmap.Map(fd, mmap.RDWR, 0)\n\tif err != nil {\n\t\trequire.NoError(t, err)\n\t}\n\n\tmmManager := &MMapRWManager{filePath, fdm, m, 256 * MB}\n\tb := []byte(\"test write at\")\n\toff := int64(3)\n\tn, err := mmManager.WriteAt(b, off)\n\tif err != nil {\n\t\trequire.NoError(t, err)\n\t}\n\trequire.Equal(t, len(b), n)\n\trequire.Equal(t, append([]byte{0, 0, 0}, b...), []byte(m[:off+int64(len(b))]))\n}\n\nfunc TestRWManager_MMap_Sync(t *testing.T) {\n\tfilePath := \"/tmp/foo_rw_filemmap\"\n\tmaxFdNums := 1024\n\tcleanThreshold := 0.5\n\tvar fdm = newFdm(maxFdNums, cleanThreshold)\n\n\tfd, err := fdm.getFd(filePath)\n\tif err != nil {\n\t\trequire.NoError(t, err)\n\t}\n\tdefer os.Remove(fd.Name())\n\n\terr = Truncate(filePath, 1024, fd)\n\tif err != nil {\n\t\trequire.NoError(t, err)\n\n\t}\n\tm, err := mmap.Map(fd, mmap.RDWR, 0)\n\tif err != nil {\n\t\trequire.NoError(t, err)\n\t}\n\n\tmmManager := &MMapRWManager{filePath, fdm, m, 256 * MB}\n\tm[1] = 'z'\n\terr = mmManager.Sync()\n\trequire.NoError(t, err)\n\tfileContents, err := os.ReadFile(filePath)\n\trequire.NoError(t, err)\n\trequire.Equal(t, fileContents, []byte(m[:]))\n}\n\nfunc TestRWManager_MMap_Close(t *testing.T) {\n\tfilePath := \"/tmp/foo_rw_filemmap\"\n\tmaxFdNums := 1024\n\tcleanThreshold := 0.5\n\tvar fdm = newFdm(maxFdNums, cleanThreshold)\n\n\tfd, err := fdm.getFd(filePath)\n\tif err != nil {\n\t\trequire.NoError(t, err)\n\t}\n\tdefer os.Remove(fd.Name())\n\n\terr = Truncate(filePath, 1024, fd)\n\tif err != nil {\n\t\trequire.NoError(t, err)\n\n\t}\n\tm, err := mmap.Map(fd, mmap.RDWR, 0)\n\tif err != nil {\n\t\trequire.NoError(t, err)\n\t}\n\n\tmmManager := &MMapRWManager{filePath, fdm, m, 256 * MB}\n\terr = mmManager.Close()\n\terr = isFileDescriptorClosed(fd.Fd())\n\tif err == nil {\n\t\tt.Error(\"expected file descriptor to be closed, but it's still open\")\n\t}\n}\n\nfunc isFileDescriptorClosed(fd uintptr) error {\n\tfile := os.NewFile(fd, \"\")\n\tdefer file.Close()\n\t_, err := file.Stat()\n\n\treturn err\n}\n"
        },
        {
          "name": "rwmanger_fileio.go",
          "type": "blob",
          "size": 1.951171875,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"os\"\n)\n\n// FileIORWManager represents the RWManager which using standard I/O.\ntype FileIORWManager struct {\n\tfd          *os.File\n\tpath        string\n\tfdm         *fdManager\n\tsegmentSize int64\n}\n\n// WriteAt writes len(b) bytes to the File starting at byte offset off.\n// `WriteAt` is a wrapper of the *File.WriteAt.\nfunc (fm *FileIORWManager) WriteAt(b []byte, off int64) (n int, err error) {\n\treturn fm.fd.WriteAt(b, off)\n}\n\n// ReadAt reads len(b) bytes from the File starting at byte offset off.\n// `ReadAt` is a wrapper of the *File.ReadAt.\nfunc (fm *FileIORWManager) ReadAt(b []byte, off int64) (n int, err error) {\n\treturn fm.fd.ReadAt(b, off)\n}\n\n// Sync commits the current contents of the file to stable storage.\n// Typically, this means flushing the file system's in-memory copy\n// of recently written data to disk.\n// `Sync` is a wrapper of the *File.Sync.\nfunc (fm *FileIORWManager) Sync() (err error) {\n\treturn fm.fd.Sync()\n}\n\n// Release is a wrapper around the reduceUsing method\nfunc (fm *FileIORWManager) Release() (err error) {\n\tfm.fdm.reduceUsing(fm.path)\n\treturn nil\n}\n\nfunc (fm *FileIORWManager) Size() int64 {\n\treturn fm.segmentSize\n}\n\n// Close will remove the cache in the fdm of the specified path, and call the close method of the os of the file\nfunc (fm *FileIORWManager) Close() (err error) {\n\treturn fm.fdm.closeByPath(fm.path)\n}\n"
        },
        {
          "name": "rwmanger_mmap.go",
          "type": "blob",
          "size": 2.42578125,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"errors\"\n\n\tmmap \"github.com/xujiajun/mmap-go\"\n)\n\n// MMapRWManager represents the RWManager which using mmap.\ntype MMapRWManager struct {\n\tpath        string\n\tfdm         *fdManager\n\tm           mmap.MMap\n\tsegmentSize int64\n}\n\nvar (\n\t// ErrUnmappedMemory is returned when a function is called on unmapped memory\n\tErrUnmappedMemory = errors.New(\"unmapped memory\")\n\n\t// ErrIndexOutOfBound is returned when given offset out of mapped region\n\tErrIndexOutOfBound = errors.New(\"offset out of mapped region\")\n)\n\n// WriteAt copies data to mapped region from the b slice starting at\n// given off and returns number of bytes copied to the mapped region.\nfunc (mm *MMapRWManager) WriteAt(b []byte, off int64) (n int, err error) {\n\tif mm.m == nil {\n\t\treturn 0, ErrUnmappedMemory\n\t} else if off >= int64(len(mm.m)) || off < 0 {\n\t\treturn 0, ErrIndexOutOfBound\n\t}\n\n\treturn copy(mm.m[off:], b), nil\n}\n\n// ReadAt copies data to b slice from mapped region starting at\n// given off and returns number of bytes copied to the b slice.\nfunc (mm *MMapRWManager) ReadAt(b []byte, off int64) (n int, err error) {\n\tif mm.m == nil {\n\t\treturn 0, ErrUnmappedMemory\n\t} else if off >= int64(len(mm.m)) || off < 0 {\n\t\treturn 0, ErrIndexOutOfBound\n\t}\n\n\treturn copy(b, mm.m[off:]), nil\n}\n\n// Sync synchronizes the mapping's contents to the file's contents on disk.\nfunc (mm *MMapRWManager) Sync() (err error) {\n\treturn mm.m.Flush()\n}\n\n// Release deletes the memory mapped region, flushes any remaining changes\nfunc (mm *MMapRWManager) Release() (err error) {\n\tmm.fdm.reduceUsing(mm.path)\n\treturn mm.m.Unmap()\n}\n\nfunc (mm *MMapRWManager) Size() int64 {\n\treturn mm.segmentSize\n}\n\n// Close will remove the cache in the fdm of the specified path, and call the close method of the os of the file\nfunc (mm *MMapRWManager) Close() (err error) {\n\treturn mm.fdm.closeByPath(mm.path)\n}\n"
        },
        {
          "name": "set.go",
          "type": "blob",
          "size": 5.5048828125,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"errors\"\n\t\"hash/fnv\"\n)\n\nvar (\n\t// ErrSetNotExist is returned when the key does not exist.\n\tErrSetNotExist = errors.New(\"set not exist\")\n\n\t// ErrSetMemberNotExist is returned when the member of set does not exist\n\tErrSetMemberNotExist = errors.New(\"set member not exist\")\n\n\t// ErrMemberEmpty is returned when the item received is nil\n\tErrMemberEmpty = errors.New(\"item empty\")\n)\n\nvar fnvHash = fnv.New32a()\n\ntype Set struct {\n\tM map[string]map[uint32]*Record\n}\n\nfunc NewSet() *Set {\n\treturn &Set{\n\t\tM: map[string]map[uint32]*Record{},\n\t}\n}\n\n// SAdd adds the specified members to the set stored at key.\nfunc (s *Set) SAdd(key string, values [][]byte, records []*Record) error {\n\tset, ok := s.M[key]\n\tif !ok {\n\t\ts.M[key] = map[uint32]*Record{}\n\t\tset = s.M[key]\n\t}\n\n\tfor i, value := range values {\n\t\thash, err := getFnv32(value)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tset[hash] = records[i]\n\t}\n\n\treturn nil\n}\n\n// SRem removes the specified members from the set stored at key.\nfunc (s *Set) SRem(key string, values ...[]byte) error {\n\tset, ok := s.M[key]\n\tif !ok {\n\t\treturn ErrSetNotExist\n\t}\n\n\tif len(values) == 0 || values[0] == nil {\n\t\treturn ErrMemberEmpty\n\t}\n\n\tfor _, value := range values {\n\t\thash, err := getFnv32(value)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdelete(set, hash)\n\t}\n\n\treturn nil\n}\n\n// SHasKey returns whether it has the set at given key.\nfunc (s *Set) SHasKey(key string) bool {\n\tif _, ok := s.M[key]; ok {\n\t\treturn true\n\t}\n\treturn false\n}\n\n// SPop removes and returns one or more random elements from the set value store at key.\nfunc (s *Set) SPop(key string) *Record {\n\tif !s.SHasKey(key) {\n\t\treturn nil\n\t}\n\n\tfor hash, record := range s.M[key] {\n\t\tdelete(s.M[key], hash)\n\t\treturn record\n\t}\n\n\treturn nil\n}\n\n// SCard Returns the set cardinality (number of elements) of the set stored at key.\nfunc (s *Set) SCard(key string) int {\n\tif !s.SHasKey(key) {\n\t\treturn 0\n\t}\n\n\treturn len(s.M[key])\n}\n\n// SDiff Returns the members of the set resulting from the difference between the first set and all the successive sets.\nfunc (s *Set) SDiff(key1, key2 string) ([]*Record, error) {\n\tif !s.SHasKey(key1) || !s.SHasKey(key2) {\n\t\treturn nil, ErrSetNotExist\n\t}\n\n\trecords := make([]*Record, 0)\n\n\tfor hash, record := range s.M[key1] {\n\t\tif _, ok := s.M[key2][hash]; !ok {\n\t\t\trecords = append(records, record)\n\t\t}\n\t}\n\treturn records, nil\n}\n\n// SInter Returns the members of the set resulting from the intersection of all the given sets.\nfunc (s *Set) SInter(key1, key2 string) ([]*Record, error) {\n\tif !s.SHasKey(key1) || !s.SHasKey(key2) {\n\t\treturn nil, ErrSetNotExist\n\t}\n\n\trecords := make([]*Record, 0)\n\n\tfor hash, record := range s.M[key1] {\n\t\tif _, ok := s.M[key2][hash]; ok {\n\t\t\trecords = append(records, record)\n\t\t}\n\t}\n\treturn records, nil\n}\n\n// SIsMember Returns if member is a member of the set stored at key.\nfunc (s *Set) SIsMember(key string, value []byte) (bool, error) {\n\tif _, ok := s.M[key]; !ok {\n\t\treturn false, ErrSetNotExist\n\t}\n\n\thash, err := getFnv32(value)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\n\tif _, ok := s.M[key][hash]; ok {\n\t\treturn true, nil\n\t}\n\n\treturn false, nil\n}\n\n// SAreMembers Returns if members are members of the set stored at key.\n// For multiple items it returns true only if all the items exist.\nfunc (s *Set) SAreMembers(key string, values ...[]byte) (bool, error) {\n\tif _, ok := s.M[key]; !ok {\n\t\treturn false, ErrSetNotExist\n\t}\n\n\tfor _, value := range values {\n\n\t\thash, err := getFnv32(value)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\n\t\tif _, ok := s.M[key][hash]; !ok {\n\t\t\treturn false, nil\n\t\t}\n\t}\n\n\treturn true, nil\n}\n\n// SMembers returns all the members of the set value stored at key.\nfunc (s *Set) SMembers(key string) ([]*Record, error) {\n\tif _, ok := s.M[key]; !ok {\n\t\treturn nil, ErrSetNotExist\n\t}\n\n\trecords := make([]*Record, 0)\n\n\tfor _, record := range s.M[key] {\n\t\trecords = append(records, record)\n\t}\n\n\treturn records, nil\n}\n\n// SMove moves member from the set at source to the set at destination.\nfunc (s *Set) SMove(key1, key2 string, value []byte) (bool, error) {\n\tif !s.SHasKey(key1) || !s.SHasKey(key2) {\n\t\treturn false, ErrSetNotExist\n\t}\n\n\tset1, set2 := s.M[key1], s.M[key2]\n\n\thash, err := getFnv32(value)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\n\tvar (\n\t\tmember *Record\n\t\tok     bool\n\t)\n\n\tif member, ok = set1[hash]; !ok {\n\t\treturn false, ErrSetMemberNotExist\n\t}\n\n\tif _, ok = set2[hash]; !ok {\n\t\terr = s.SAdd(key2, [][]byte{value}, []*Record{member})\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\t}\n\n\terr = s.SRem(key1, value)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\n\treturn true, nil\n}\n\n// SUnion returns the members of the set resulting from the union of all the given sets.\nfunc (s *Set) SUnion(key1, key2 string) ([]*Record, error) {\n\tif !s.SHasKey(key1) || !s.SHasKey(key2) {\n\t\treturn nil, ErrSetNotExist\n\t}\n\n\trecords, err := s.SMembers(key1)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor hash, record := range s.M[key2] {\n\t\tif _, ok := s.M[key1][hash]; !ok {\n\t\t\trecords = append(records, record)\n\t\t}\n\t}\n\n\treturn records, nil\n}\n"
        },
        {
          "name": "set_test.go",
          "type": "blob",
          "size": 11.33984375,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"github.com/stretchr/testify/require\"\n\t\"testing\"\n)\n\nfunc TestSet_SAdd(t *testing.T) {\n\tset := NewSet()\n\tkey := \"key\"\n\texpectRecords := generateRecords(3)\n\n\tvalues := make([][]byte, 3)\n\tfor i := range expectRecords {\n\t\tvalues[i] = expectRecords[i].Value\n\t}\n\n\trequire.NoError(t, set.SAdd(key, values, expectRecords))\n\n\tok, err := set.SAreMembers(key, values...)\n\trequire.True(t, ok)\n\trequire.NoError(t, err)\n}\n\nfunc TestSet_SRem(t *testing.T) {\n\tset := NewSet()\n\tkey := \"key\"\n\texpectRecords := generateRecords(4)\n\tvalues := make([][]byte, 4)\n\tfor i := range expectRecords {\n\t\tvalues[i] = expectRecords[i].Value\n\t}\n\n\trequire.NoError(t, set.SAdd(key, values, expectRecords))\n\n\trequire.NoError(t, set.SRem(key, values[0]))\n\trequire.NoError(t, set.SRem(key, values[1:]...))\n\n\trequire.NoError(t, set.SAdd(key, values, expectRecords))\n\trequire.NoError(t, set.SRem(key, values...))\n\n\trequire.Error(t, set.SRem(\"fake key\", values...))\n\trequire.Error(t, set.SRem(key, nil))\n}\n\nfunc TestSet_SDiff(t *testing.T) {\n\tset := NewSet()\n\n\tkey1 := \"set1\"\n\tkey2 := \"set2\"\n\tkey3 := \"set3\"\n\tkey4 := \"set4\"\n\n\texpectRecords := generateRecords(10)\n\n\tvalues := make([][]byte, 10)\n\tfor i := range expectRecords {\n\t\tvalues[i] = expectRecords[i].Value\n\t}\n\n\trequire.NoError(t, set.SAdd(key1, values[:5], expectRecords[:5]))\n\trequire.NoError(t, set.SAdd(key2, values[2:6], expectRecords[2:6]))\n\trequire.NoError(t, set.SAdd(key3, values[2:7], expectRecords[2:7]))\n\trequire.NoError(t, set.SAdd(key4, values[7:], expectRecords[7:]))\n\n\ttype args struct {\n\t\tkey1 string\n\t\tkey2 string\n\t}\n\n\ttests := []struct {\n\t\tname    string\n\t\targs    args\n\t\tset     *Set\n\t\twant    []*Record\n\t\twantErr bool\n\t}{\n\t\t{\"normal set diff1\", args{key1, key2}, set, expectRecords[:2], false},\n\t\t{\"normal set diff2\", args{key1, key4}, set, expectRecords[:5], false},\n\t\t{\"normal set diff3\", args{key3, key2}, set, expectRecords[6:7], false},\n\t\t{\"first fake set\", args{\"fake_key1\", key2}, set, nil, true},\n\t\t{\"second fake set\", args{key1, \"fake_key2\"}, set, nil, true},\n\t\t{\"two fake set\", args{\"fake_key1\", \"fake_key2\"}, set, nil, true},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tgot, err := tt.set.SDiff(tt.args.key1, tt.args.key2)\n\t\t\tif (err != nil) != tt.wantErr {\n\t\t\t\trequire.Errorf(t, err, \"Get() error = %v, wantErr %v\", tt.wantErr)\n\t\t\t}\n\t\t\trequire.ElementsMatchf(t, tt.want, got, \"Get() got = %v, want %v\", got, tt.want)\n\t\t})\n\t}\n}\n\nfunc TestSet_SCard(t *testing.T) {\n\tset := NewSet()\n\n\tkey1 := \"set1\"\n\tkey2 := \"set2\"\n\tkey3 := \"set3\"\n\tkey4 := \"set4\"\n\n\texpectRecords := generateRecords(10)\n\n\tvalues := make([][]byte, 10)\n\tfor i := range expectRecords {\n\t\tvalues[i] = expectRecords[i].Value\n\t}\n\n\trequire.NoError(t, set.SAdd(key1, values[:5], expectRecords[:5]))\n\trequire.NoError(t, set.SAdd(key2, values[2:6], expectRecords[2:6]))\n\trequire.NoError(t, set.SAdd(key3, values[2:7], expectRecords[2:7]))\n\trequire.NoError(t, set.SAdd(key4, values[7:], expectRecords[7:]))\n\n\ttests := []struct {\n\t\tname string\n\t\tkey  string\n\t\tset  *Set\n\t\twant int\n\t}{\n\t\t{\"normal set\", key1, set, 5},\n\t\t{\"normal set\", key2, set, 4},\n\t\t{\"normal set\", key3, set, 5},\n\t\t{\"fake key\", \"key_fake\", set, 0},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tgot := tt.set.SCard(tt.key)\n\t\t\trequire.Equalf(t, tt.want, got, \"TestSet_SCard err\")\n\t\t})\n\t}\n}\n\nfunc TestSet_SInter(t *testing.T) {\n\tset := NewSet()\n\n\tkey1 := \"set1\"\n\tkey2 := \"set2\"\n\tkey3 := \"set3\"\n\tkey4 := \"set4\"\n\n\texpectRecords := generateRecords(10)\n\n\tvalues := make([][]byte, 10)\n\tfor i := range expectRecords {\n\t\tvalues[i] = expectRecords[i].Value\n\t}\n\n\trequire.NoError(t, set.SAdd(key1, values[:5], expectRecords[:5]))\n\trequire.NoError(t, set.SAdd(key2, values[2:6], expectRecords[2:6]))\n\trequire.NoError(t, set.SAdd(key3, values[2:7], expectRecords[2:7]))\n\trequire.NoError(t, set.SAdd(key4, values[7:], expectRecords[7:]))\n\n\ttype args struct {\n\t\tkey1 string\n\t\tkey2 string\n\t}\n\n\ttests := []struct {\n\t\tname    string\n\t\targs    args\n\t\tset     *Set\n\t\twant    []*Record\n\t\twantErr bool\n\t}{\n\t\t{\"normal set inter1\", args{key1, key2}, set, []*Record{expectRecords[2], expectRecords[3], expectRecords[4]}, false},\n\t\t{\"normal set inter1\", args{key2, key3}, set, []*Record{expectRecords[2], expectRecords[3], expectRecords[4], expectRecords[5]}, false},\n\t\t{\"normal set inter2\", args{key1, key4}, set, nil, false},\n\t\t{\"first fake set\", args{\"fake_key1\", key2}, set, nil, true},\n\t\t{\"second fake set\", args{key1, \"fake_key2\"}, set, nil, true},\n\t\t{\"two fake set\", args{\"fake_key1\", \"fake_key2\"}, set, nil, true},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tgot, err := tt.set.SInter(tt.args.key1, tt.args.key2)\n\t\t\tif (err != nil) != tt.wantErr {\n\t\t\t\trequire.Errorf(t, err, \"Get() error = %v, wantErr %v\", tt.wantErr)\n\t\t\t}\n\t\t\trequire.ElementsMatchf(t, tt.want, got, \"Get() got = %v, want %v\", got, tt.want)\n\t\t})\n\t}\n}\n\nfunc TestSet_SMembers(t *testing.T) {\n\tset := NewSet()\n\n\tkey := \"set\"\n\n\texpectRecords := generateRecords(3)\n\n\tvalues := make([][]byte, 3)\n\tfor i := range expectRecords {\n\t\tvalues[i] = expectRecords[i].Value\n\t}\n\n\trequire.NoError(t, set.SAdd(key, values, expectRecords))\n\n\ttests := []struct {\n\t\tname    string\n\t\tkey     string\n\t\tset     *Set\n\t\twant    []*Record\n\t\twantErr bool\n\t}{\n\t\t{\"normal SMembers\", key, set, expectRecords[0:1], false},\n\t\t{\"normal SMembers\", key, set, expectRecords[1:], false},\n\t\t{\"normal SMembers\", key, set, expectRecords, false},\n\t\t{\"fake key\", \"fake_key\", set, nil, true},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tgot, err := tt.set.SMembers(tt.key)\n\t\t\tif (err != nil) != tt.wantErr {\n\t\t\t\trequire.Errorf(t, err, \"Get() error = %v, wantErr %v\", tt.wantErr)\n\t\t\t}\n\t\t\trequire.Subsetf(t, got, tt.want, \"SInter() got = %v, want = %v\", got, tt.want)\n\t\t})\n\t}\n}\n\nfunc TestSet_SMove(t *testing.T) {\n\tset := NewSet()\n\n\tkey1 := \"set1\"\n\tkey2 := \"set2\"\n\n\texpectRecords := generateRecords(3)\n\tvalues := make([][]byte, 3)\n\tfor i := range expectRecords {\n\t\tvalues[i] = expectRecords[i].Value\n\t}\n\n\trequire.NoError(t, set.SAdd(key1, values[:2], expectRecords[:2]))\n\trequire.NoError(t, set.SAdd(key2, values[2:], expectRecords[2:]))\n\n\ttype args struct {\n\t\tkey1 string\n\t\tkey2 string\n\t\titem []byte\n\t}\n\n\ttests := []struct {\n\t\tname      string\n\t\targs      args\n\t\tset       *Set\n\t\twant1     []*Record\n\t\twant2     []*Record\n\t\texpectErr error\n\t}{\n\t\t{\"normal SMove\", args{key1, key2, values[1]}, set, expectRecords[0:1], expectRecords[1:], nil},\n\t\t{\"not exist member SMove\", args{key1, key2, values[2]}, set, nil, nil, ErrSetMemberNotExist},\n\t\t{\"fake key SMove1\", args{\"fake key\", key2, values[2]}, set, nil, nil, ErrSetNotExist},\n\t\t{\"fake key SMove\", args{key1, \"fake key\", values[2]}, set, nil, nil, ErrSetNotExist},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\t_, err := tt.set.SMove(tt.args.key1, tt.args.key2, tt.args.item)\n\t\t\tif tt.expectErr != nil {\n\t\t\t\trequire.Error(t, err)\n\t\t\t\trequire.Equal(t, tt.expectErr, err)\n\t\t\t} else {\n\t\t\t\tgot1, _ := tt.set.SMembers(tt.args.key1)\n\t\t\t\tgot2, _ := tt.set.SMembers(tt.args.key2)\n\t\t\t\trequire.ElementsMatchf(t, got1, tt.want1, \"SMove() got = %v, want = %v\", got1, tt.want1)\n\t\t\t\trequire.ElementsMatchf(t, got2, tt.want2, \"SMove() got = %v, want = %v\", got2, tt.want2)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestSet_SPop(t *testing.T) {\n\tset := NewSet()\n\n\tkey := \"set\"\n\n\texpectRecords := generateRecords(2)\n\tvalues := make([][]byte, 2)\n\tfor i := range expectRecords {\n\t\tvalues[i] = expectRecords[i].Value\n\t}\n\tm := map[*Record]struct{}{}\n\tfor _, expectRecord := range expectRecords {\n\t\tm[expectRecord] = struct{}{}\n\t}\n\n\trequire.NoError(t, set.SAdd(key, values, expectRecords))\n\n\ttests := []struct {\n\t\tname string\n\t\tkey  string\n\t\tset  *Set\n\t\tok   bool\n\t}{\n\t\t{\"normal set SPop\", key, set, true},\n\t\t{\"normal set SPop\", key, set, true},\n\t\t{\"normal set SPop\", key, set, false},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\trecord := tt.set.SPop(tt.key)\n\t\t\t_, ok := m[record]\n\t\t\trequire.Equal(t, tt.ok, ok)\n\t\t})\n\t}\n}\n\nfunc TestSet_SIsMember(t *testing.T) {\n\tset := NewSet()\n\n\tkey := \"key\"\n\n\texpectRecords := generateRecords(1)\n\tvalues := [][]byte{expectRecords[0].Value}\n\n\trequire.NoError(t, set.SAdd(key, values, expectRecords))\n\ttests := []struct {\n\t\tkey       string\n\t\tval       []byte\n\t\tok        bool\n\t\texpectErr error\n\t}{\n\t\t{key, values[0], true, nil},\n\t\t{key, GetRandomBytes(24), false, nil},\n\t\t{\"fake key\", GetRandomBytes(24), false, ErrSetNotExist},\n\t}\n\tfor _, tt := range tests {\n\t\tok, err := set.SIsMember(tt.key, tt.val)\n\t\tif tt.expectErr != nil {\n\t\t\trequire.Equal(t, tt.expectErr, err)\n\t\t} else {\n\t\t\trequire.Equal(t, tt.ok, ok)\n\t\t}\n\t}\n}\n\nfunc TestSet_SAreMembers(t *testing.T) {\n\tset := NewSet()\n\n\tkey := \"set\"\n\n\texpectRecords := generateRecords(4)\n\tvalues := make([][]byte, 4)\n\tfor i := range expectRecords {\n\t\tvalues[i] = expectRecords[i].Value\n\t}\n\n\trequire.NoError(t, set.SAdd(key, values, expectRecords))\n\ttests := []struct {\n\t\tkey       string\n\t\tval       [][]byte\n\t\tok        bool\n\t\texpectErr error\n\t}{\n\t\t{key, values[0:2], true, nil},\n\t\t{key, values[2:], true, nil},\n\t\t{key, values, true, nil},\n\t\t{key, [][]byte{GetRandomBytes(24)}, false, nil},\n\t\t{\"fake key\", values, true, ErrSetNotExist},\n\t}\n\tfor _, tt := range tests {\n\t\tok, err := set.SAreMembers(tt.key, tt.val...)\n\t\tif tt.expectErr != nil {\n\t\t\trequire.Equal(t, tt.expectErr, err)\n\t\t} else {\n\t\t\trequire.Equal(t, tt.ok, ok)\n\t\t}\n\t}\n}\n\nfunc TestSet_SUnion(t *testing.T) {\n\tset := NewSet()\n\n\tkey1 := \"set1\"\n\tkey2 := \"set2\"\n\tkey3 := \"set3\"\n\tkey4 := \"set4\"\n\n\texpectRecords := generateRecords(10)\n\n\tvalues := make([][]byte, 10)\n\tfor i := range expectRecords {\n\t\tvalues[i] = expectRecords[i].Value\n\t}\n\n\trequire.NoError(t, set.SAdd(key1, values[:5], expectRecords[:5]))\n\trequire.NoError(t, set.SAdd(key2, values[2:6], expectRecords[2:6]))\n\trequire.NoError(t, set.SAdd(key3, values[2:7], expectRecords[2:7]))\n\trequire.NoError(t, set.SAdd(key4, values[7:], expectRecords[7:]))\n\n\ttype args struct {\n\t\tkey1 string\n\t\tkey2 string\n\t}\n\n\ttests := []struct {\n\t\tname    string\n\t\targs    args\n\t\tset     *Set\n\t\twant    []*Record\n\t\twantErr bool\n\t}{\n\t\t{\"normal set Union1\", args{key1, key4}, set,\n\t\t\t[]*Record{expectRecords[0], expectRecords[1], expectRecords[2], expectRecords[3], expectRecords[4], expectRecords[7], expectRecords[8], expectRecords[9]},\n\t\t\tfalse},\n\t\t{\n\t\t\t\"normal set Union2\", args{key2, key3}, set,\n\t\t\t[]*Record{expectRecords[2], expectRecords[3], expectRecords[4], expectRecords[5], expectRecords[6]},\n\t\t\tfalse,\n\t\t},\n\t\t{\"first fake set\", args{\"fake_key1\", key2}, set, nil, true},\n\t\t{\"second fake set\", args{key1, \"fake_key2\"}, set, nil, true},\n\t\t{\"two fake set\", args{\"fake_key1\", \"fake_key2\"}, set, nil, true},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tgot, err := tt.set.SUnion(tt.args.key1, tt.args.key2)\n\t\t\tif (err != nil) != tt.wantErr {\n\t\t\t\trequire.Errorf(t, err, \"Get() error = %v, wantErr %v\", tt.wantErr)\n\t\t\t}\n\t\t\trequire.ElementsMatchf(t, tt.want, got, \"Get() got = %v, want %v\", got, tt.want)\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "sorted_set.go",
          "type": "blob",
          "size": 19.2705078125,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"math/rand\"\n)\n\nvar (\n\tErrSortedSetNotFound = errors.New(\"the sortedSet does not exist\")\n\n\tErrSortedSetMemberNotExist = errors.New(\"the member of sortedSet does not exist\")\n\n\tErrSortedSetIsEmpty = errors.New(\"the sortedSet if empty\")\n)\n\nconst (\n\t// SkipListMaxLevel represents the skipList max level number.\n\tSkipListMaxLevel = 32\n\n\t// SkipListP represents the p parameter of the skipList.\n\tSkipListP = 0.25\n)\n\ntype SortedSet struct {\n\tdb *DB\n\tM  map[string]*SkipList\n}\n\nfunc NewSortedSet(db *DB) *SortedSet {\n\treturn &SortedSet{\n\t\tdb: db,\n\t\tM:  map[string]*SkipList{},\n\t}\n}\n\nfunc (z *SortedSet) ZAdd(key string, score SCORE, value []byte, record *Record) error {\n\tsortedSet, ok := z.M[key]\n\tif !ok {\n\t\tz.M[key] = newSkipList(z.db)\n\t\tsortedSet = z.M[key]\n\t}\n\n\treturn sortedSet.Put(score, value, record)\n}\n\nfunc (z *SortedSet) ZMembers(key string) (map[*Record]SCORE, error) {\n\tsortedSet, ok := z.M[key]\n\n\tif !ok {\n\t\treturn nil, ErrSortedSetNotFound\n\t}\n\n\tnodes := sortedSet.dict\n\n\tmembers := make(map[*Record]SCORE, len(nodes))\n\tfor _, node := range nodes {\n\t\tmembers[node.record] = node.score\n\t}\n\n\treturn members, nil\n}\n\nfunc (z *SortedSet) ZCard(key string) (int, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\treturn int(sortedSet.length), nil\n\t}\n\n\treturn 0, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZCount(key string, start SCORE, end SCORE, opts *GetByScoreRangeOptions) (int, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\treturn len(sortedSet.GetByScoreRange(start, end, opts)), nil\n\t}\n\treturn 0, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZPeekMax(key string) (*Record, SCORE, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\tnode := sortedSet.PeekMax()\n\t\tif node != nil {\n\t\t\treturn node.record, node.score, nil\n\t\t}\n\t\treturn nil, 0, ErrSortedSetIsEmpty\n\t}\n\n\treturn nil, 0, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZPopMax(key string) (*Record, SCORE, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\tnode := sortedSet.PopMax()\n\t\tif node != nil {\n\t\t\treturn node.record, node.score, nil\n\t\t}\n\t\treturn nil, 0, ErrSortedSetIsEmpty\n\t}\n\n\treturn nil, 0, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZPeekMin(key string) (*Record, SCORE, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\tnode := sortedSet.PeekMin()\n\t\tif node != nil {\n\t\t\treturn node.record, node.score, nil\n\t\t}\n\t\treturn nil, 0, ErrSortedSetIsEmpty\n\t}\n\n\treturn nil, 0, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZPopMin(key string) (*Record, SCORE, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\tnode := sortedSet.PopMin()\n\t\tif node != nil {\n\t\t\treturn node.record, node.score, nil\n\t\t}\n\t\treturn nil, 0, ErrSortedSetIsEmpty\n\t}\n\n\treturn nil, 0, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZRangeByScore(key string, start SCORE, end SCORE, opts *GetByScoreRangeOptions) ([]*Record, []float64, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\n\t\tnodes := sortedSet.GetByScoreRange(start, end, opts)\n\n\t\trecords := make([]*Record, len(nodes))\n\t\tscores := make([]float64, len(nodes))\n\n\t\tfor i, node := range nodes {\n\t\t\trecords[i] = node.record\n\t\t\tscores[i] = float64(node.score)\n\t\t}\n\n\t\treturn records, scores, nil\n\t}\n\n\treturn nil, nil, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZRangeByRank(key string, start int, end int) ([]*Record, []float64, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\n\t\tnodes := sortedSet.GetByRankRange(start, end, false)\n\n\t\trecords := make([]*Record, len(nodes))\n\t\tscores := make([]float64, len(nodes))\n\n\t\tfor i, node := range nodes {\n\t\t\trecords[i] = node.record\n\t\t\tscores[i] = float64(node.score)\n\t\t}\n\n\t\treturn records, scores, nil\n\t}\n\n\treturn nil, nil, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZRem(key string, value []byte) (*Record, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\thash, err := getFnv32(value)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tnode := sortedSet.Remove(hash)\n\t\tif node != nil {\n\t\t\treturn node.record, nil\n\t\t}\n\t\treturn nil, ErrSortedSetMemberNotExist\n\t}\n\n\treturn nil, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZRemRangeByRank(key string, start int, end int) error {\n\tif sortedSet, ok := z.M[key]; ok {\n\n\t\t_ = sortedSet.GetByRankRange(start, end, true)\n\t\treturn nil\n\t}\n\n\treturn ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) getZRemRangeByRankNodes(key string, start int, end int) ([]*SkipListNode, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\treturn sortedSet.GetByRankRange(start, end, false), nil\n\t}\n\n\treturn []*SkipListNode{}, nil\n}\n\nfunc (z *SortedSet) ZRank(key string, value []byte) (int, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\thash, err := getFnv32(value)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\trank := sortedSet.FindRank(hash)\n\t\tif rank == 0 {\n\t\t\treturn 0, ErrSortedSetMemberNotExist\n\t\t}\n\t\treturn rank, nil\n\t}\n\treturn 0, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZRevRank(key string, value []byte) (int, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\thash, err := getFnv32(value)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\trank := sortedSet.FindRevRank(hash)\n\t\tif rank == 0 {\n\t\t\treturn 0, ErrSortedSetMemberNotExist\n\t\t}\n\t\treturn rank, nil\n\t}\n\treturn 0, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZScore(key string, value []byte) (float64, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\tnode := sortedSet.GetByValue(value)\n\t\tif node != nil {\n\t\t\treturn float64(sortedSet.GetByValue(value).score), nil\n\t\t}\n\t\treturn 0, ErrSortedSetMemberNotExist\n\t}\n\treturn 0, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZExist(key string, value []byte) (bool, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\thash, err := getFnv32(value)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\t\t_, ok := sortedSet.dict[hash]\n\t\treturn ok, nil\n\t}\n\treturn false, ErrSortedSetNotFound\n}\n\n// SCORE represents the score type.\ntype SCORE float64\n\n// SkipListLevel records forward and span.\ntype SkipListLevel struct {\n\tforward *SkipListNode\n\tspan    int64\n}\n\n// The SkipList represents the sorted set.\ntype SkipList struct {\n\tdb     *DB\n\theader *SkipListNode\n\ttail   *SkipListNode\n\tlength int64\n\tlevel  int\n\tdict   map[uint32]*SkipListNode\n}\n\n// SkipListNode represents a node in the SkipList.\ntype SkipListNode struct {\n\thash     uint32  // unique key of this node\n\trecord   *Record // associated data\n\tscore    SCORE   // score to determine the order of this node in the set\n\tbackward *SkipListNode\n\tlevel    []SkipListLevel\n}\n\n// Hash returns the key of the node.\nfunc (sln *SkipListNode) Hash() uint32 {\n\treturn sln.hash\n}\n\n// Score returns the score of the node.\nfunc (sln *SkipListNode) Score() SCORE {\n\treturn sln.score\n}\n\n// createNode returns a newly initialized SkipListNode Object that implements the SkipListNode.\nfunc createNode(level int, score SCORE, hash uint32, record *Record) *SkipListNode {\n\tnode := SkipListNode{\n\t\thash:   hash,\n\t\trecord: record,\n\t\tscore:  score,\n\t\tlevel:  make([]SkipListLevel, level),\n\t}\n\treturn &node\n}\n\n// randomLevel returns a random level for the new skiplist node we are going to create.\n// The return value of this function is between 1 and SkipListMaxLevel\n// (both inclusive), with a powerlaw-alike distribution where higher\n// levels are lesl likely to be returned.\nfunc randomLevel() int {\n\tlevel := 1\n\n\tfor float64(rand.Int31()&0xFFFF) < SkipListP*0xFFFF {\n\t\tlevel += 1\n\t}\n\tif level < SkipListMaxLevel {\n\t\treturn level\n\t}\n\n\treturn SkipListMaxLevel\n}\n\nfunc newSkipList(db *DB) *SkipList {\n\tskipList := &SkipList{\n\t\tdb:    db,\n\t\tlevel: 1,\n\t\tdict:  make(map[uint32]*SkipListNode),\n\t}\n\thash, _ := getFnv32([]byte(\"\"))\n\tskipList.header = createNode(SkipListMaxLevel, 0, hash, nil)\n\treturn skipList\n}\n\nfunc (sl *SkipList) cmp(r1 *Record, r2 *Record) int {\n\tval1, _ := sl.db.getValueByRecord(r1)\n\tval2, _ := sl.db.getValueByRecord(r2)\n\treturn bytes.Compare(val1, val2)\n}\n\nfunc (sl *SkipList) insertNode(score SCORE, hash uint32, record *Record) *SkipListNode {\n\tvar update [SkipListMaxLevel]*SkipListNode\n\tvar rank [SkipListMaxLevel]int64\n\n\tx := sl.header\n\tfor i := sl.level - 1; i >= 0; i-- {\n\t\t// store rank that is crosled to reach the insert position\n\t\tif sl.level-1 == i {\n\t\t\trank[i] = 0\n\t\t} else {\n\t\t\trank[i] = rank[i+1]\n\t\t}\n\n\t\tfor x.level[i].forward != nil &&\n\t\t\t(x.level[i].forward.score < score ||\n\t\t\t\t(x.level[i].forward.score == score && // score is the same but the key is different\n\t\t\t\t\tsl.cmp(x.level[i].forward.record, record) < 0)) {\n\t\t\trank[i] += x.level[i].span\n\t\t\tx = x.level[i].forward\n\t\t}\n\n\t\tupdate[i] = x\n\t}\n\n\t/* we assume the key is not already inside, since we allow duplicated\n\t * scores, and the re-insertion of score and redis object should never\n\t * happen since the caller of Insert() should test in the hash table\n\t * if the element is already inside or not. */\n\tlevel := randomLevel()\n\n\tif level > sl.level { // add a new level\n\t\tfor i := sl.level; i < level; i++ {\n\t\t\trank[i] = 0\n\t\t\tupdate[i] = sl.header\n\t\t\tupdate[i].level[i].span = sl.length\n\t\t}\n\t\tsl.level = level\n\t}\n\n\tx = createNode(level, score, hash, record)\n\tfor i := 0; i < level; i++ {\n\t\tx.level[i].forward = update[i].level[i].forward\n\t\tupdate[i].level[i].forward = x\n\n\t\t/* update span covered by update[i] as x is inserted here */\n\t\tx.level[i].span = update[i].level[i].span - (rank[0] - rank[i])\n\n\t\tupdate[i].level[i].span = (rank[0] - rank[i]) + 1\n\t}\n\n\t// increment span for untouched levels\n\tfor i := level; i < sl.level; i++ {\n\t\tupdate[i].level[i].span++\n\t}\n\n\tif update[0] == sl.header {\n\t\tx.backward = nil\n\t} else {\n\t\tx.backward = update[0]\n\t}\n\n\tif x.level[0].forward != nil {\n\t\tx.level[0].forward.backward = x\n\t} else {\n\t\tsl.tail = x\n\t}\n\n\tsl.length++\n\n\treturn x\n}\n\n// deleteNode represents internal function used by delete, DeleteByScore and DeleteByRank.\nfunc (sl *SkipList) deleteNode(x *SkipListNode, update [SkipListMaxLevel]*SkipListNode) {\n\tfor i := 0; i < sl.level; i++ {\n\t\tif update[i].level[i].forward == x {\n\t\t\tupdate[i].level[i].span += x.level[i].span - 1\n\t\t\tupdate[i].level[i].forward = x.level[i].forward\n\t\t} else {\n\t\t\tupdate[i].level[i].span -= 1\n\t\t}\n\t}\n\tif x.level[0].forward != nil {\n\t\tx.level[0].forward.backward = x.backward\n\t} else {\n\t\tsl.tail = x.backward\n\t}\n\tfor sl.level > 1 && sl.header.level[sl.level-1].forward == nil {\n\t\tsl.level--\n\t}\n\tsl.length--\n\tdelete(sl.dict, x.hash)\n}\n\n// delete removes an element with matching score/key from the skiplist.\nfunc (sl *SkipList) delete(score SCORE, hash uint32) bool {\n\tvar update [SkipListMaxLevel]*SkipListNode\n\n\ttargetNode := sl.dict[hash]\n\n\tx := sl.header\n\tfor i := sl.level - 1; i >= 0; i-- {\n\t\tfor x.level[i].forward != nil &&\n\t\t\t(x.level[i].forward.score < score ||\n\t\t\t\t(x.level[i].forward.score == score &&\n\t\t\t\t\tsl.cmp(x.level[i].forward.record, targetNode.record) < 0)) {\n\t\t\tx = x.level[i].forward\n\t\t}\n\t\tupdate[i] = x\n\t}\n\t/* We may have multiple elements with the same score, what we need\n\t * is to find the element with both the right score and object. */\n\tx = x.level[0].forward\n\tif x != nil && score == x.score && sl.cmp(x.record, targetNode.record) == 0 {\n\t\tsl.deleteNode(x, update)\n\t\t// free x\n\t\treturn true\n\t}\n\treturn false /* not found */\n}\n\n// Size returns the number of elements in the SkipList.\nfunc (sl *SkipList) Size() int {\n\treturn int(sl.length)\n}\n\n// PeekMin returns the element with minimum score, nil if the set is empty.\n//\n// Time complexity of this method is : O(log(N)).\nfunc (sl *SkipList) PeekMin() *SkipListNode {\n\treturn sl.header.level[0].forward\n}\n\n// PopMin returns and remove the element with minimal score, nil if the set is empty.\n//\n// Time complexity of this method is : O(log(N)).\nfunc (sl *SkipList) PopMin() *SkipListNode {\n\tx := sl.header.level[0].forward\n\tif x != nil {\n\t\tsl.Remove(x.hash)\n\t}\n\treturn x\n}\n\n// PeekMax returns the element with maximum score, nil if the set is empty.\n//\n// Time Complexity : O(1).\nfunc (sl *SkipList) PeekMax() *SkipListNode {\n\treturn sl.tail\n}\n\n// PopMax returns and remove the element with maximum score, nil if the set is empty.\n//\n// Time complexity of this method is : O(log(N)).\nfunc (sl *SkipList) PopMax() *SkipListNode {\n\tx := sl.tail\n\tif x != nil {\n\t\tsl.Remove(x.hash)\n\t}\n\treturn x\n}\n\n// Put puts an element into the sorted set with specific key / value / score.\n//\n// Time complexity of this method is : O(log(N)).\nfunc (sl *SkipList) Put(score SCORE, value []byte, record *Record) error {\n\tvar newNode *SkipListNode\n\n\thash, _ := getFnv32(value)\n\n\tif n, ok := sl.dict[hash]; ok {\n\t\t// score does not change, only update value\n\t\tif n.score != score { // score changes, delete and re-insert\n\t\t\tsl.delete(n.score, n.hash)\n\t\t\tnewNode = sl.insertNode(score, hash, record)\n\t\t}\n\t} else {\n\t\tnewNode = sl.insertNode(score, hash, record)\n\t}\n\n\tif newNode != nil {\n\t\tsl.dict[hash] = newNode\n\t}\n\n\treturn nil\n}\n\n// Remove removes element specified at given key.\n//\n// Time complexity of this method is : O(log(N)).\nfunc (sl *SkipList) Remove(hash uint32) *SkipListNode {\n\tfound := sl.dict[hash]\n\tif found != nil {\n\t\tsl.delete(found.score, hash)\n\t\treturn found\n\t}\n\treturn nil\n}\n\n// GetByScoreRangeOptions represents the options of the GetByScoreRange function.\ntype GetByScoreRangeOptions struct {\n\tLimit        int  // limit the max nodes to return\n\tExcludeStart bool // exclude start value, so it search in interval (start, end] or (start, end)\n\tExcludeEnd   bool // exclude end value, so it search in interval [start, end) or (start, end)\n}\n\n// GetByScoreRange returns the nodes whose score within the specific range.\n// If options is nil, it searches in interval [start, end] without any limit by default.\n//\n// Time complexity of this method is : O(log(N)).\nfunc (sl *SkipList) GetByScoreRange(start SCORE, end SCORE, options *GetByScoreRangeOptions) []*SkipListNode {\n\tlimit := 1<<31 - 1\n\tif options != nil && options.Limit > 0 {\n\t\tlimit = options.Limit\n\t}\n\n\texcludeStart := options != nil && options.ExcludeStart\n\texcludeEnd := options != nil && options.ExcludeEnd\n\treverse := start > end\n\tif reverse {\n\t\tstart, end = end, start\n\t\texcludeStart, excludeEnd = excludeEnd, excludeStart\n\t}\n\n\tvar nodes []*SkipListNode\n\n\t// determine if out of range\n\tif sl.length == 0 {\n\t\treturn nodes\n\t}\n\n\tif reverse {\n\t\t// search from end to start\n\t\treturn sl.searchReverse(nodes, excludeStart, excludeEnd, start, end, limit)\n\t}\n\t// search from start to end\n\treturn sl.searchForward(nodes, excludeStart, excludeEnd, start, end, limit)\n}\n\nfunc (sl *SkipList) searchForward(nodes []*SkipListNode, excludeStart, excludeEnd bool, start, end SCORE, limit int) []*SkipListNode {\n\t// search from start to end\n\tx := sl.header\n\tif excludeStart {\n\t\tfor i := sl.level - 1; i >= 0; i-- {\n\t\t\tfor x.level[i].forward != nil &&\n\t\t\t\tx.level[i].forward.score <= start {\n\t\t\t\tx = x.level[i].forward\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfor i := sl.level - 1; i >= 0; i-- {\n\t\t\tfor x.level[i].forward != nil &&\n\t\t\t\tx.level[i].forward.score < start {\n\t\t\t\tx = x.level[i].forward\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Current node is the last with score < or <= start. */\n\tx = x.level[0].forward\n\n\tfor x != nil && limit > 0 {\n\t\tif excludeEnd {\n\t\t\tif x.score >= end {\n\t\t\t\tbreak\n\t\t\t}\n\t\t} else {\n\t\t\tif x.score > end {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\tnext := x.level[0].forward\n\n\t\tnodes = append(nodes, x)\n\t\tlimit--\n\n\t\tx = next\n\t}\n\n\treturn nodes\n}\n\nfunc (sl *SkipList) searchReverse(nodes []*SkipListNode, excludeStart, excludeEnd bool, start, end SCORE, limit int) []*SkipListNode {\n\tx := sl.header\n\n\tif excludeEnd {\n\t\tfor i := sl.level - 1; i >= 0; i-- {\n\t\t\tfor x.level[i].forward != nil &&\n\t\t\t\tx.level[i].forward.score < end {\n\t\t\t\tx = x.level[i].forward\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfor i := sl.level - 1; i >= 0; i-- {\n\t\t\tfor x.level[i].forward != nil &&\n\t\t\t\tx.level[i].forward.score <= end {\n\t\t\t\tx = x.level[i].forward\n\t\t\t}\n\t\t}\n\t}\n\n\tfor x != nil && limit > 0 {\n\t\tif excludeStart {\n\t\t\tif x.score <= start {\n\t\t\t\tbreak\n\t\t\t}\n\t\t} else {\n\t\t\tif x.score < start {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\tnext := x.backward\n\n\t\tnodes = append(nodes, x)\n\t\tlimit--\n\n\t\tx = next\n\t}\n\n\treturn nodes\n}\n\n// GetByRankRange returns nodes within specific rank range [start, end].\n// Note that the rank is 1-based integer. Rank 1 means the first node; Rank -1 means the last node\n// If start is greater than end, the returned array is in reserved order\n// If remove is true, the returned nodes are removed.\n//\n// Time complexity of this method is : O(log(N)).\nfunc (sl *SkipList) GetByRankRange(start, end int, remove bool) []*SkipListNode {\n\tvar (\n\t\tupdate    [SkipListMaxLevel]*SkipListNode\n\t\tnodes     []*SkipListNode\n\t\ttraversed int\n\t)\n\n\tstart, end = sl.sanitizeIndexes(start, end)\n\n\treverse := start > end\n\tif reverse { // swap start and end\n\t\tstart, end = end, start\n\t}\n\n\ttraversed = 0\n\tx := sl.header\n\tfor i := sl.level - 1; i >= 0; i-- {\n\t\tfor x.level[i].forward != nil &&\n\t\t\ttraversed+int(x.level[i].span) < start {\n\t\t\ttraversed += int(x.level[i].span)\n\t\t\tx = x.level[i].forward\n\t\t}\n\t\tif remove {\n\t\t\tupdate[i] = x\n\t\t} else {\n\t\t\tif traversed+1 == start {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\ttraversed++\n\tx = x.level[0].forward\n\tfor x != nil && traversed <= end {\n\t\tnext := x.level[0].forward\n\n\t\tnodes = append(nodes, x)\n\n\t\tif remove {\n\t\t\tsl.deleteNode(x, update)\n\t\t}\n\n\t\ttraversed++\n\t\tx = next\n\t}\n\n\tif reverse {\n\t\tfor i, j := 0, len(nodes)-1; i < j; i, j = i+1, j-1 {\n\t\t\tnodes[i], nodes[j] = nodes[j], nodes[i]\n\t\t}\n\t}\n\treturn nodes\n}\n\nfunc (sl *SkipList) sanitizeIndexes(start, end int) (newStart, newEnd int) {\n\tif start < 0 {\n\t\tstart = int(sl.length) + start + 1\n\t}\n\tif end < 0 {\n\t\tend = int(sl.length) + end + 1\n\t}\n\tif start <= 0 {\n\t\tstart = 1\n\t}\n\tif end <= 0 {\n\t\tend = 1\n\t}\n\n\treturn start, end\n}\n\n// GetByRank returns the node at given rank.\n// Note that the rank is 1-based integer. Rank 1 means the first node; Rank -1 means the last node.\n// If remove is true, the returned nodes are removed\n// If node is not found at specific rank, nil is returned.\n//\n// Time complexity of this method is : O(log(N)).\nfunc (sl *SkipList) GetByRank(rank int, remove bool) *SkipListNode {\n\tnodes := sl.GetByRankRange(rank, rank, remove)\n\tif len(nodes) == 1 {\n\t\treturn nodes[0]\n\t}\n\treturn nil\n}\n\n// GetByValue returns the node at given key.\n// If node is not found, nil is returned\n//\n// Time complexity : O(1).\nfunc (sl *SkipList) GetByValue(value []byte) *SkipListNode {\n\thash, _ := getFnv32(value)\n\treturn sl.dict[hash]\n}\n\n// FindRank Returns the rank of member in the sorted set stored at key, with the scores ordered from low to high.\n// Note that the rank is 1-based integer. Rank 1 means the first node\n// If the node is not found, 0 is returned. Otherwise rank(> 0) is returned.\n//\n// Time complexity of this method is : O(log(N)).\nfunc (sl *SkipList) FindRank(hash uint32) int {\n\trank := 0\n\ttargetNode := sl.dict[hash]\n\tif targetNode != nil {\n\t\tx := sl.header\n\t\tfor i := sl.level - 1; i >= 0; i-- {\n\t\t\tfor x.level[i].forward != nil &&\n\t\t\t\t(x.level[i].forward.score < targetNode.score ||\n\t\t\t\t\t(x.level[i].forward.score == targetNode.score &&\n\t\t\t\t\t\tsl.cmp(x.level[i].forward.record, targetNode.record) <= 0)) {\n\t\t\t\trank += int(x.level[i].span)\n\t\t\t\tx = x.level[i].forward\n\t\t\t}\n\n\t\t\tif x.hash == hash {\n\t\t\t\treturn rank\n\t\t\t}\n\t\t}\n\t}\n\treturn 0\n}\n\n// FindRevRank Returns the rank of member in the sorted set stored at key, with the scores ordered from high to low.\nfunc (sl *SkipList) FindRevRank(hash uint32) int {\n\tif sl.length == 0 {\n\t\treturn 0\n\t}\n\n\tif _, ok := sl.dict[hash]; !ok {\n\t\treturn 0\n\t}\n\n\treturn sl.Size() - sl.FindRank(hash) + 1\n}\n"
        },
        {
          "name": "tar.go",
          "type": "blob",
          "size": 2.333984375,
          "content": "// Copyright 2021 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"archive/tar\"\n\t\"compress/gzip\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n)\n\nfunc tarGZCompress(dst io.Writer, src string) error {\n\tgz := gzip.NewWriter(dst)\n\tdefer gz.Close()\n\treturn tarCompress(gz, src)\n}\n\n// https://blog.ralch.com/articles/golang-working-with-tar-and-gzip\nfunc tarCompress(dst io.Writer, src string) error {\n\ttarball := tar.NewWriter(dst)\n\tdefer tarball.Close()\n\n\tinfo, err := os.Stat(src)\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\tvar baseDir string\n\tif info.IsDir() {\n\t\tbaseDir = filepath.Base(src)\n\t}\n\n\treturn filepath.Walk(src,\n\t\tfunc(path string, info os.FileInfo, err error) error {\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\theader, err := tar.FileInfoHeader(info, info.Name())\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tif baseDir != \"\" {\n\t\t\t\theader.Name = filepath.Join(baseDir, strings.TrimPrefix(path, src))\n\t\t\t}\n\n\t\t\tif err := tarball.WriteHeader(header); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tif info.IsDir() {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\tfile, err := os.Open(filepath.Clean(path))\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tdefer file.Close()\n\t\t\t_, err = io.Copy(tarball, file)\n\t\t\treturn err\n\t\t})\n}\n\nfunc tarDecompress(dst string, src io.Reader) error {\n\ttarReader := tar.NewReader(src)\n\n\tfor {\n\t\theader, err := tarReader.Next()\n\t\tif err == io.EOF {\n\t\t\tbreak\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tpath := filepath.Join(dst, header.Name)\n\t\tinfo := header.FileInfo()\n\t\tif info.IsDir() {\n\t\t\tif err = os.MkdirAll(path, info.Mode()); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\tfile, err := os.OpenFile(filepath.Clean(path), os.O_CREATE|os.O_TRUNC|os.O_WRONLY, info.Mode())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer file.Close()\n\t\t_, err = io.Copy(file, tarReader)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n"
        },
        {
          "name": "test_utils.go",
          "type": "blob",
          "size": 0.927734375,
          "content": "// Copyright 2023 The PromiseDB Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"fmt\"\n\t\"math/rand\"\n)\n\nconst charset = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n\nfunc GetTestBytes(i int) []byte {\n\treturn []byte(fmt.Sprintf(\"nutsdb-%09d\", i))\n}\n\nfunc GetRandomBytes(length int) []byte {\n\tb := make([]byte, length)\n\tfor i := range b {\n\t\tb[i] = charset[rand.Intn(len(charset))]\n\t}\n\treturn b\n}\n"
        },
        {
          "name": "throttle.go",
          "type": "blob",
          "size": 1.6513671875,
          "content": "package nutsdb\n\nimport (\n\t\"sync\"\n)\n\n// Throttle allows a limited number of workers to run at a time. It also\n// provides a mechanism to check for errors encountered by workers and wait for\n// them to finish.\ntype Throttle struct {\n\tonce      sync.Once\n\twg        sync.WaitGroup\n\tch        chan struct{}\n\terrCh     chan error\n\tfinishErr error\n}\n\n// NewThrottle creates a new throttle with a max number of workers.\nfunc NewThrottle(max int) *Throttle {\n\treturn &Throttle{\n\t\tch:    make(chan struct{}, max),\n\t\terrCh: make(chan error, max),\n\t}\n}\n\n// Do should be called by workers before they start working. It blocks if there\n// are already maximum number of workers working. If it detects an error from\n// previously Done workers, it would return it.\nfunc (t *Throttle) Do() error {\n\tfor {\n\t\tselect {\n\t\tcase t.ch <- struct{}{}:\n\t\t\tt.wg.Add(1)\n\t\t\treturn nil\n\t\tcase err := <-t.errCh:\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Done should be called by workers when they finish working. They can also\n// pass the error status of work done.\nfunc (t *Throttle) Done(err error) {\n\tif err != nil {\n\t\tt.errCh <- err\n\t}\n\tselect {\n\tcase <-t.ch:\n\tdefault:\n\t\tpanic(\"Throttle Do Done mismatch\")\n\t}\n\tt.wg.Done()\n}\n\n// Finish waits until all workers have finished working. It would return any error passed by Done.\n// If Finish is called multiple time, it will wait for workers to finish only once(first time).\n// From next calls, it will return same error as found on first call.\nfunc (t *Throttle) Finish() error {\n\tt.once.Do(func() {\n\t\tt.wg.Wait()\n\t\tclose(t.ch)\n\t\tclose(t.errCh)\n\t\tfor err := range t.errCh {\n\t\t\tif err != nil {\n\t\t\t\tt.finishErr = err\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t})\n\n\treturn t.finishErr\n}\n"
        },
        {
          "name": "ttl_manager.go",
          "type": "blob",
          "size": 2.3427734375,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"time\"\n\n\t\"github.com/antlabs/timer\"\n)\n\ntype nodesInBucket map[string]timer.TimeNoder // key to timer node\n\nfunc newNodesInBucket() nodesInBucket {\n\treturn make(map[string]timer.TimeNoder)\n}\n\ntype nodes map[BucketId]nodesInBucket // bucket to nodes that in a bucket\n\nfunc (n nodes) getNode(bucketId BucketId, key string) (timer.TimeNoder, bool) {\n\tnib, ok := n[bucketId]\n\tif !ok {\n\t\treturn nil, false\n\t}\n\tnode, ok := nib[key]\n\treturn node, ok\n}\n\nfunc (n nodes) addNode(bucketId BucketId, key string, node timer.TimeNoder) {\n\tnib, ok := n[bucketId]\n\tif !ok {\n\t\tnib = newNodesInBucket()\n\t\tn[bucketId] = nib\n\t}\n\tnib[key] = node\n}\n\nfunc (n nodes) delNode(bucketId BucketId, key string) {\n\tif nib, ok := n[bucketId]; ok {\n\t\tdelete(nib, key)\n\t}\n}\n\ntype ttlManager struct {\n\tt          timer.Timer\n\ttimerNodes nodes\n}\n\nfunc newTTLManager(expiredDeleteType ExpiredDeleteType) *ttlManager {\n\tvar t timer.Timer\n\n\tswitch expiredDeleteType {\n\tcase TimeWheel:\n\t\tt = timer.NewTimer(timer.WithTimeWheel())\n\tcase TimeHeap:\n\t\tt = timer.NewTimer(timer.WithMinHeap())\n\tdefault:\n\t\tt = timer.NewTimer()\n\t}\n\n\treturn &ttlManager{\n\t\tt:          t,\n\t\ttimerNodes: make(nodes),\n\t}\n}\n\nfunc (tm *ttlManager) run() {\n\ttm.t.Run()\n}\n\nfunc (tm *ttlManager) exist(bucketId BucketId, key string) bool {\n\t_, ok := tm.timerNodes.getNode(bucketId, key)\n\treturn ok\n}\n\nfunc (tm *ttlManager) add(bucketId BucketId, key string, expire time.Duration, callback func()) {\n\tif node, ok := tm.timerNodes.getNode(bucketId, key); ok {\n\t\tnode.Stop()\n\t}\n\n\tnode := tm.t.AfterFunc(expire, callback)\n\ttm.timerNodes.addNode(bucketId, key, node)\n}\n\nfunc (tm *ttlManager) del(bucket BucketId, key string) {\n\ttm.timerNodes.delNode(bucket, key)\n}\n\nfunc (tm *ttlManager) close() {\n\ttm.timerNodes = nil\n\ttm.t.Stop()\n}\n"
        },
        {
          "name": "tx.go",
          "type": "blob",
          "size": 20.5634765625,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\t\"sync/atomic\"\n\n\t\"github.com/bwmarrin/snowflake\"\n\t\"github.com/xujiajun/utils/strconv2\"\n)\n\nconst (\n\t// txStatusRunning means the tx is running\n\ttxStatusRunning = 1\n\t// txStatusCommitting means the tx is committing\n\ttxStatusCommitting = 2\n\t// txStatusClosed means the tx is closed, ether committed or rollback\n\ttxStatusClosed = 3\n)\n\n// Tx represents a transaction.\ntype Tx struct {\n\tid                uint64\n\tdb                *DB\n\twritable          bool\n\tstatus            atomic.Value\n\tpendingWrites     *pendingEntryList\n\tsize              int64\n\tpendingBucketList pendingBucketList\n}\n\ntype txnCb struct {\n\tcommit func() error\n\tuser   func(error)\n\terr    error\n}\n\nfunc (tx *Tx) submitEntry(ds uint16, bucket string, e *Entry) {\n\ttx.pendingWrites.submitEntry(ds, bucket, e)\n}\n\nfunc runTxnCallback(cb *txnCb) {\n\tswitch {\n\tcase cb == nil:\n\t\tpanic(\"tx callback is nil\")\n\tcase cb.user == nil:\n\t\tpanic(\"Must have caught a nil callback for tx.CommitWith\")\n\tcase cb.err != nil:\n\t\tcb.user(cb.err)\n\tcase cb.commit != nil:\n\t\terr := cb.commit()\n\t\tcb.user(err)\n\tdefault:\n\t\tcb.user(nil)\n\t}\n}\n\n// Begin opens a new transaction.\n// Multiple read-only transactions can be opened at the same time but there can\n// only be one read/write transaction at a time. Attempting to open a read/write\n// transactions while another one is in progress will result in blocking until\n// the current read/write transaction is completed.\n// All transactions must be closed by calling Commit() or Rollback() when done.\nfunc (db *DB) Begin(writable bool) (tx *Tx, err error) {\n\ttx, err = newTx(db, writable)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ttx.lock()\n\ttx.setStatusRunning()\n\tif db.closed {\n\t\ttx.unlock()\n\t\ttx.setStatusClosed()\n\t\treturn nil, ErrDBClosed\n\t}\n\n\treturn\n}\n\n// newTx returns a newly initialized Tx object at given writable.\nfunc newTx(db *DB, writable bool) (tx *Tx, err error) {\n\tvar txID uint64\n\n\ttx = &Tx{\n\t\tdb:                db,\n\t\twritable:          writable,\n\t\tpendingWrites:     newPendingEntriesList(),\n\t\tpendingBucketList: make(map[Ds]map[BucketName]*Bucket),\n\t}\n\n\ttxID, err = tx.getTxID()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ttx.id = txID\n\n\treturn\n}\n\nfunc (tx *Tx) CommitWith(cb func(error)) {\n\tif cb == nil {\n\t\tpanic(\"Nil callback provided to CommitWith\")\n\t}\n\n\tif tx.pendingWrites.size == 0 {\n\t\t// Do not run these callbacks from here, because the CommitWith and the\n\t\t// callback might be acquiring the same locks. Instead run the callback\n\t\t// from another goroutine.\n\t\tgo runTxnCallback(&txnCb{user: cb, err: nil})\n\t\treturn\n\t}\n\t// defer tx.setStatusClosed()  //must not add this code because another process is also accessing tx\n\tcommitCb, err := tx.commitAndSend()\n\tif err != nil {\n\t\tgo runTxnCallback(&txnCb{user: cb, err: err})\n\t\treturn\n\t}\n\n\tgo runTxnCallback(&txnCb{user: cb, commit: commitCb})\n}\n\nfunc (tx *Tx) commitAndSend() (func() error, error) {\n\treq, err := tx.db.sendToWriteCh(tx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tret := func() error {\n\t\terr := req.Wait()\n\t\treturn err\n\t}\n\n\treturn ret, nil\n}\n\nfunc (tx *Tx) checkSize() error {\n\tcount := tx.pendingWrites.size\n\tif int64(count) >= tx.db.getMaxBatchCount() || tx.size >= tx.db.getMaxBatchSize() {\n\t\treturn ErrTxnTooBig\n\t}\n\n\treturn nil\n}\n\n// getTxID returns the tx id.\nfunc (tx *Tx) getTxID() (id uint64, err error) {\n\tnode, err := snowflake.NewNode(tx.db.opt.NodeNum)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tid = uint64(node.Generate().Int64())\n\n\treturn\n}\n\n// Commit commits the transaction, following these steps:\n//\n// 1. check the length of pendingWrites.If there are no writes, return immediately.\n//\n// 2. check if the ActiveFile has not enough space to store entry. if not, call rotateActiveFile function.\n//\n// 3. write pendingWrites to disk, if a non-nil error,return the error.\n//\n// 4. build Hint index.\n//\n// 5. Unlock the database and clear the db field.\nfunc (tx *Tx) Commit() (err error) {\n\tdefer func() {\n\t\tif err != nil {\n\t\t\ttx.handleErr(err)\n\t\t}\n\t\ttx.unlock()\n\t\ttx.db = nil\n\n\t\ttx.pendingWrites = nil\n\t}()\n\tif tx.isClosed() {\n\t\treturn ErrCannotCommitAClosedTx\n\t}\n\n\tif tx.db == nil {\n\t\ttx.setStatusClosed()\n\t\treturn ErrDBClosed\n\t}\n\n\tvar curWriteCount int64\n\tif tx.db.opt.MaxWriteRecordCount > 0 {\n\t\tcurWriteCount, err = tx.getNewAddRecordCount()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// judge all write records is whether more than the MaxWriteRecordCount\n\t\tif tx.db.RecordCount+curWriteCount > tx.db.opt.MaxWriteRecordCount {\n\t\t\treturn ErrTxnExceedWriteLimit\n\t\t}\n\t}\n\n\ttx.setStatusCommitting()\n\tdefer tx.setStatusClosed()\n\n\twritesBucketLen := len(tx.pendingBucketList)\n\tif tx.pendingWrites.size == 0 && writesBucketLen == 0 {\n\t\treturn nil\n\t}\n\n\tbuff := tx.allocCommitBuffer()\n\tdefer tx.db.commitBuffer.Reset()\n\n\tvar records []*Record\n\n\tpendingWriteList := tx.pendingWrites.toList()\n\tlastIndex := len(pendingWriteList) - 1\n\tfor i := 0; i < len(pendingWriteList); i++ {\n\t\tentry := pendingWriteList[i]\n\t\tentrySize := entry.Size()\n\t\tif entrySize > tx.db.opt.SegmentSize {\n\t\t\treturn ErrDataSizeExceed\n\t\t}\n\n\t\tif tx.db.ActiveFile.ActualSize+int64(buff.Len())+entrySize > tx.db.opt.SegmentSize {\n\t\t\tif _, err := tx.writeData(buff.Bytes()); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tbuff.Reset()\n\n\t\t\tif err := tx.rotateActiveFile(); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\toffset := tx.db.ActiveFile.writeOff + int64(buff.Len())\n\n\t\tif i == lastIndex {\n\t\t\tentry.Meta.Status = Committed\n\t\t}\n\n\t\tif _, err := buff.Write(entry.Encode()); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif i == lastIndex {\n\t\t\tif _, err := tx.writeData(buff.Bytes()); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\trecord := tx.db.createRecordByModeWithFidAndOff(tx.db.ActiveFile.fileID, uint64(offset), entry)\n\n\t\t// add to cache\n\t\tif tx.db.getHintKeyAndRAMIdxCacheSize() > 0 && tx.db.opt.EntryIdxMode == HintKeyAndRAMIdxMode {\n\t\t\ttx.db.hintKeyAndRAMIdxModeLru.Add(record, entry)\n\t\t}\n\n\t\trecords = append(records, record)\n\t}\n\n\tif err := tx.SubmitBucket(); err != nil {\n\t\treturn err\n\t}\n\n\tif err := tx.buildIdxes(records, pendingWriteList); err != nil {\n\t\treturn err\n\t}\n\ttx.db.RecordCount += curWriteCount\n\n\tif err := tx.buildBucketInIndex(); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\nfunc (tx *Tx) getNewAddRecordCount() (int64, error) {\n\tvar res int64\n\tchangeCountInEntries, err := tx.getChangeCountInEntriesChanges()\n\tchangeCountInBucket := tx.getChangeCountInBucketChanges()\n\tres += changeCountInEntries\n\tres += changeCountInBucket\n\treturn res, err\n}\n\nfunc (tx *Tx) getListHeadTailSeq(bucketId BucketId, key string) *HeadTailSeq {\n\tres := HeadTailSeq{Head: initialListSeq, Tail: initialListSeq + 1}\n\tif _, ok := tx.db.Index.list.idx[bucketId]; ok {\n\t\tif _, ok := tx.db.Index.list.idx[bucketId].Seq[key]; ok {\n\t\t\tres = *tx.db.Index.list.idx[bucketId].Seq[key]\n\t\t}\n\t}\n\n\treturn &res\n}\n\nfunc (tx *Tx) getListEntryNewAddRecordCount(bucketId BucketId, entry *Entry) (int64, error) {\n\tif entry.Meta.Flag == DataExpireListFlag {\n\t\treturn 0, nil\n\t}\n\n\tvar res int64\n\tkey := string(entry.Key)\n\tvalue := string(entry.Value)\n\tl := tx.db.Index.list.getWithDefault(bucketId)\n\n\tswitch entry.Meta.Flag {\n\tcase DataLPushFlag, DataRPushFlag:\n\t\tres++\n\tcase DataLPopFlag, DataRPopFlag:\n\t\tres--\n\tcase DataLRemByIndex:\n\t\tindexes, _ := UnmarshalInts([]byte(value))\n\t\tres -= int64(len(l.getValidIndexes(key, indexes)))\n\tcase DataLRemFlag:\n\t\tcount, newValue := splitIntStringStr(value, SeparatorForListKey)\n\t\tremoveIndices, err := l.getRemoveIndexes(key, count, func(r *Record) (bool, error) {\n\t\t\tv, err := tx.db.getValueByRecord(r)\n\t\t\tif err != nil {\n\t\t\t\treturn false, err\n\t\t\t}\n\t\t\treturn bytes.Equal([]byte(newValue), v), nil\n\t\t})\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\tres -= int64(len(removeIndices))\n\tcase DataLTrimFlag:\n\t\tnewKey, start := splitStringIntStr(key, SeparatorForListKey)\n\t\tend, _ := strconv2.StrToInt(value)\n\n\t\tif l.IsExpire(newKey) {\n\t\t\treturn 0, nil\n\t\t}\n\n\t\tif _, ok := l.Items[newKey]; !ok {\n\t\t\treturn 0, nil\n\t\t}\n\n\t\titems, err := l.LRange(newKey, start, end)\n\t\tif err != nil {\n\t\t\treturn res, err\n\t\t}\n\n\t\tlist := l.Items[newKey]\n\t\tres -= int64(list.Count() - len(items))\n\t}\n\n\treturn res, nil\n}\n\nfunc (tx *Tx) getKvEntryNewAddRecordCount(bucketId BucketId, entry *Entry) (int64, error) {\n\tvar res int64\n\n\tswitch entry.Meta.Flag {\n\tcase DataDeleteFlag:\n\t\tres--\n\tcase DataSetFlag:\n\t\tif idx, ok := tx.db.Index.bTree.exist(bucketId); ok {\n\t\t\t_, found := idx.Find(entry.Key)\n\t\t\tif !found {\n\t\t\t\tres++\n\t\t\t}\n\t\t} else {\n\t\t\tres++\n\t\t}\n\t}\n\n\treturn res, nil\n}\n\nfunc (tx *Tx) getSetEntryNewAddRecordCount(bucketId BucketId, entry *Entry) (int64, error) {\n\tvar res int64\n\n\tif entry.Meta.Flag == DataDeleteFlag {\n\t\tres--\n\t}\n\n\tif entry.Meta.Flag == DataSetFlag {\n\t\tres++\n\t}\n\n\treturn res, nil\n}\n\nfunc (tx *Tx) getSortedSetEntryNewAddRecordCount(bucketId BucketId, entry *Entry) (int64, error) {\n\tvar res int64\n\tkey := string(entry.Key)\n\tvalue := string(entry.Value)\n\n\tswitch entry.Meta.Flag {\n\tcase DataZAddFlag:\n\t\tif !tx.keyExistsInSortedSet(bucketId, key, value) {\n\t\t\tres++\n\t\t}\n\tcase DataZRemFlag:\n\t\tres--\n\tcase DataZRemRangeByRankFlag:\n\t\tstart, end := splitIntIntStr(value, SeparatorForZSetKey)\n\t\tdelNodes, err := tx.db.Index.sortedSet.getWithDefault(bucketId, tx.db).getZRemRangeByRankNodes(key, start, end)\n\t\tif err != nil {\n\t\t\treturn res, err\n\t\t}\n\t\tres -= int64(len(delNodes))\n\tcase DataZPopMaxFlag, DataZPopMinFlag:\n\t\tres--\n\t}\n\n\treturn res, nil\n}\n\nfunc (tx *Tx) keyExistsInSortedSet(bucketId BucketId, key, value string) bool {\n\tif _, exist := tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn false\n\t}\n\tnewKey := key\n\tif strings.Contains(key, SeparatorForZSetKey) {\n\t\tnewKey, _ = splitStringFloat64Str(key, SeparatorForZSetKey)\n\t}\n\texists, _ := tx.db.Index.sortedSet.idx[bucketId].ZExist(newKey, []byte(value))\n\treturn exists\n}\n\nfunc (tx *Tx) getEntryNewAddRecordCount(entry *Entry) (int64, error) {\n\tvar res int64\n\tvar err error\n\n\tbucket, err := tx.db.bm.GetBucketById(entry.Meta.BucketId)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tbucketId := bucket.Id\n\n\tif entry.Meta.Ds == DataStructureBTree {\n\t\tres, err = tx.getKvEntryNewAddRecordCount(bucketId, entry)\n\t}\n\n\tif entry.Meta.Ds == DataStructureList {\n\t\tres, err = tx.getListEntryNewAddRecordCount(bucketId, entry)\n\t}\n\n\tif entry.Meta.Ds == DataStructureSet {\n\t\tres, err = tx.getSetEntryNewAddRecordCount(bucketId, entry)\n\t}\n\n\tif entry.Meta.Ds == DataStructureSortedSet {\n\t\tres, err = tx.getSortedSetEntryNewAddRecordCount(bucketId, entry)\n\t}\n\n\treturn res, err\n}\n\nfunc (tx *Tx) allocCommitBuffer() *bytes.Buffer {\n\tvar buff *bytes.Buffer\n\n\tif tx.size < tx.db.opt.CommitBufferSize {\n\t\tbuff = tx.db.commitBuffer\n\t} else {\n\t\tbuff = new(bytes.Buffer)\n\t\t// avoid grow\n\t\tbuff.Grow(int(tx.size))\n\t}\n\n\treturn buff\n}\n\n// rotateActiveFile rotates log file when active file is not enough space to store the entry.\nfunc (tx *Tx) rotateActiveFile() error {\n\tvar err error\n\ttx.db.MaxFileID++\n\n\tif !tx.db.opt.SyncEnable && tx.db.opt.RWMode == MMap {\n\t\tif err := tx.db.ActiveFile.rwManager.Sync(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif err := tx.db.ActiveFile.rwManager.Release(); err != nil {\n\t\treturn err\n\t}\n\n\t// reset ActiveFile\n\tpath := getDataPath(tx.db.MaxFileID, tx.db.opt.Dir)\n\ttx.db.ActiveFile, err = tx.db.fm.getDataFile(path, tx.db.opt.SegmentSize)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\ttx.db.ActiveFile.fileID = tx.db.MaxFileID\n\treturn nil\n}\n\nfunc (tx *Tx) writeData(data []byte) (n int, err error) {\n\tif len(data) == 0 {\n\t\treturn\n\t}\n\n\twriteOffset := tx.db.ActiveFile.ActualSize\n\n\tl := len(data)\n\tif writeOffset+int64(l) > tx.db.opt.SegmentSize {\n\t\treturn 0, errors.New(\"not enough file space\")\n\t}\n\n\tif n, err = tx.db.ActiveFile.WriteAt(data, writeOffset); err != nil {\n\t\treturn\n\t}\n\n\ttx.db.ActiveFile.writeOff += int64(l)\n\ttx.db.ActiveFile.ActualSize += int64(l)\n\n\tif tx.db.opt.SyncEnable {\n\t\tif err := tx.db.ActiveFile.rwManager.Sync(); err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t}\n\n\treturn\n}\n\n// Rollback closes the transaction.\nfunc (tx *Tx) Rollback() error {\n\tif tx.db == nil {\n\t\ttx.setStatusClosed()\n\t\treturn ErrDBClosed\n\t}\n\tif tx.isCommitting() {\n\t\treturn ErrCannotRollbackACommittingTx\n\t}\n\n\tif tx.isClosed() {\n\t\treturn ErrCannotRollbackAClosedTx\n\t}\n\n\ttx.setStatusClosed()\n\ttx.unlock()\n\n\ttx.db = nil\n\ttx.pendingWrites = nil\n\n\treturn nil\n}\n\n// lock locks the database based on the transaction type.\nfunc (tx *Tx) lock() {\n\tif tx.writable {\n\t\ttx.db.mu.Lock()\n\t} else {\n\t\ttx.db.mu.RLock()\n\t}\n}\n\n// unlock unlocks the database based on the transaction type.\nfunc (tx *Tx) unlock() {\n\tif tx.writable {\n\t\ttx.db.mu.Unlock()\n\t} else {\n\t\ttx.db.mu.RUnlock()\n\t}\n}\n\nfunc (tx *Tx) handleErr(err error) {\n\tif tx.db.opt.ErrorHandler != nil {\n\t\ttx.db.opt.ErrorHandler.HandleError(err)\n\t}\n}\n\nfunc (tx *Tx) checkTxIsClosed() error {\n\tif tx.db == nil {\n\t\treturn ErrTxClosed\n\t}\n\treturn nil\n}\n\n// put sets the value for a key in the bucket.\n// Returns an error if tx is closed, if performing a write operation on a read-only transaction, if the key is empty.\nfunc (tx *Tx) put(bucket string, key, value []byte, ttl uint32, flag uint16, timestamp uint64, ds uint16) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\n\tbucketStatus := tx.getBucketStatus(DataStructureBTree, bucket)\n\tif bucketStatus == BucketStatusDeleted {\n\t\treturn ErrBucketNotFound\n\t}\n\n\tif !tx.db.bm.ExistBucket(ds, bucket) {\n\t\treturn ErrorBucketNotExist\n\t}\n\n\tif !tx.writable {\n\t\treturn ErrTxNotWritable\n\t}\n\n\tbucketId, err := tx.db.bm.GetBucketID(ds, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tmeta := NewMetaData().WithTimeStamp(timestamp).WithKeySize(uint32(len(key))).WithValueSize(uint32(len(value))).WithFlag(flag).\n\t\tWithTTL(ttl).WithStatus(UnCommitted).WithDs(ds).WithTxID(tx.id).WithBucketId(bucketId)\n\n\te := NewEntry().WithKey(key).WithMeta(meta).WithValue(value)\n\n\terr = e.valid()\n\tif err != nil {\n\t\treturn err\n\t}\n\ttx.submitEntry(ds, bucket, e)\n\ttx.size += e.Size()\n\n\treturn nil\n}\n\nfunc (tx *Tx) putDeleteLog(bucketId BucketId, key, value []byte, ttl uint32, flag uint16, timestamp uint64, ds uint16) {\n\tbucket, err := tx.db.bm.GetBucketById(bucketId)\n\tif err != nil {\n\t\treturn\n\t}\n\tmeta := NewMetaData().WithTimeStamp(timestamp).WithKeySize(uint32(len(key))).WithValueSize(uint32(len(value))).WithFlag(flag).\n\t\tWithTTL(ttl).WithStatus(UnCommitted).WithDs(ds).WithTxID(tx.id).WithBucketId(bucket.Id)\n\n\te := NewEntry().WithKey(key).WithMeta(meta).WithValue(value)\n\ttx.submitEntry(ds, bucket.Name, e)\n\ttx.size += e.Size()\n}\n\n// setStatusCommitting will change the tx status to txStatusCommitting\nfunc (tx *Tx) setStatusCommitting() {\n\tstatus := txStatusCommitting\n\ttx.status.Store(status)\n}\n\n// setStatusClosed will change the tx status to txStatusClosed\nfunc (tx *Tx) setStatusClosed() {\n\tstatus := txStatusClosed\n\ttx.status.Store(status)\n}\n\n// setStatusRunning will change the tx status to txStatusRunning\nfunc (tx *Tx) setStatusRunning() {\n\tstatus := txStatusRunning\n\ttx.status.Store(status)\n}\n\n// isRunning will check if the tx status is txStatusRunning\nfunc (tx *Tx) isRunning() bool {\n\tstatus := tx.status.Load().(int)\n\treturn status == txStatusRunning\n}\n\n// isCommitting will check if the tx status is txStatusCommitting\nfunc (tx *Tx) isCommitting() bool {\n\tstatus := tx.status.Load().(int)\n\treturn status == txStatusCommitting\n}\n\n// isClosed will check if the tx status is txStatusClosed\nfunc (tx *Tx) isClosed() bool {\n\tstatus := tx.status.Load().(int)\n\treturn status == txStatusClosed\n}\n\nfunc (tx *Tx) buildIdxes(records []*Record, entries []*Entry) error {\n\tfor i, entry := range entries {\n\t\tmeta := entry.Meta\n\t\tvar err error\n\t\tswitch meta.Ds {\n\t\tcase DataStructureBTree:\n\t\t\terr = tx.db.buildBTreeIdx(records[i], entry)\n\t\tcase DataStructureList:\n\t\t\terr = tx.db.buildListIdx(records[i], entry)\n\t\tcase DataStructureSet:\n\t\t\terr = tx.db.buildSetIdx(records[i], entry)\n\t\tcase DataStructureSortedSet:\n\t\t\terr = tx.db.buildSortedSetIdx(records[i], entry)\n\t\t}\n\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\ttx.db.KeyCount++\n\t}\n\treturn nil\n}\n\nfunc (tx *Tx) putBucket(b *Bucket) error {\n\tif _, exist := tx.pendingBucketList[b.Ds]; !exist {\n\t\ttx.pendingBucketList[b.Ds] = map[BucketName]*Bucket{}\n\t}\n\tbucketInDs := tx.pendingBucketList[b.Ds]\n\tbucketInDs[b.Name] = b\n\treturn nil\n}\n\nfunc (tx *Tx) SubmitBucket() error {\n\tbucketReqs := make([]*bucketSubmitRequest, 0)\n\tfor ds, mapper := range tx.pendingBucketList {\n\t\tfor name, bucket := range mapper {\n\t\t\treq := &bucketSubmitRequest{\n\t\t\t\tds:     ds,\n\t\t\t\tname:   name,\n\t\t\t\tbucket: bucket,\n\t\t\t}\n\t\t\tbucketReqs = append(bucketReqs, req)\n\t\t}\n\t}\n\treturn tx.db.bm.SubmitPendingBucketChange(bucketReqs)\n}\n\n// buildBucketInIndex build indexes on creation and deletion of buckets\nfunc (tx *Tx) buildBucketInIndex() error {\n\tfor _, mapper := range tx.pendingBucketList {\n\t\tfor _, bucket := range mapper {\n\t\t\tif bucket.Meta.Op == BucketInsertOperation {\n\t\t\t\tswitch bucket.Ds {\n\t\t\t\tcase DataStructureBTree:\n\t\t\t\t\ttx.db.Index.bTree.getWithDefault(bucket.Id)\n\t\t\t\tcase DataStructureList:\n\t\t\t\t\ttx.db.Index.list.getWithDefault(bucket.Id)\n\t\t\t\tcase DataStructureSet:\n\t\t\t\t\ttx.db.Index.set.getWithDefault(bucket.Id)\n\t\t\t\tcase DataStructureSortedSet:\n\t\t\t\t\ttx.db.Index.sortedSet.getWithDefault(bucket.Id, tx.db)\n\t\t\t\tdefault:\n\t\t\t\t\treturn ErrDataStructureNotSupported\n\t\t\t\t}\n\t\t\t} else if bucket.Meta.Op == BucketDeleteOperation {\n\t\t\t\tswitch bucket.Ds {\n\t\t\t\tcase DataStructureBTree:\n\t\t\t\t\ttx.db.Index.bTree.delete(bucket.Id)\n\t\t\t\tcase DataStructureList:\n\t\t\t\t\ttx.db.Index.list.delete(bucket.Id)\n\t\t\t\tcase DataStructureSet:\n\t\t\t\t\ttx.db.Index.set.delete(bucket.Id)\n\t\t\t\tcase DataStructureSortedSet:\n\t\t\t\t\ttx.db.Index.sortedSet.delete(bucket.Id)\n\t\t\t\tdefault:\n\t\t\t\t\treturn ErrDataStructureNotSupported\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (tx *Tx) getChangeCountInEntriesChanges() (int64, error) {\n\tvar res int64\n\tfor _, entriesInDS := range tx.pendingWrites.entriesInBTree {\n\t\tfor _, entry := range entriesInDS {\n\t\t\tcurRecordCnt, err := tx.getEntryNewAddRecordCount(entry)\n\t\t\tif err != nil {\n\t\t\t\treturn res, nil\n\t\t\t}\n\t\t\tres += curRecordCnt\n\t\t}\n\t}\n\tfor _, entriesInDS := range tx.pendingWrites.entries {\n\t\tfor _, entries := range entriesInDS {\n\t\t\tfor _, entry := range entries {\n\t\t\t\tcurRecordCnt, err := tx.getEntryNewAddRecordCount(entry)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn res, err\n\t\t\t\t}\n\t\t\t\tres += curRecordCnt\n\t\t\t}\n\t\t}\n\t}\n\treturn res, nil\n}\n\nfunc (tx *Tx) getChangeCountInBucketChanges() int64 {\n\tvar res int64\n\tf := func(bucket *Bucket) error {\n\t\tbucketId := bucket.Id\n\t\tif bucket.Meta.Op == BucketDeleteOperation {\n\t\t\tswitch bucket.Ds {\n\t\t\tcase DataStructureBTree:\n\t\t\t\tif bTree, ok := tx.db.Index.bTree.idx[bucketId]; ok {\n\t\t\t\t\tres -= int64(bTree.Count())\n\t\t\t\t}\n\t\t\tcase DataStructureSet:\n\t\t\t\tif set, ok := tx.db.Index.set.idx[bucketId]; ok {\n\t\t\t\t\tfor key := range set.M {\n\t\t\t\t\t\tres -= int64(set.SCard(key))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tcase DataStructureSortedSet:\n\t\t\t\tif sortedSet, ok := tx.db.Index.sortedSet.idx[bucketId]; ok {\n\t\t\t\t\tfor key := range sortedSet.M {\n\t\t\t\t\t\tcurLen, _ := sortedSet.ZCard(key)\n\t\t\t\t\t\tres -= int64(curLen)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tcase DataStructureList:\n\t\t\t\tif list, ok := tx.db.Index.list.idx[bucketId]; ok {\n\t\t\t\t\tfor key := range list.Items {\n\t\t\t\t\t\tcurLen, _ := list.Size(key)\n\t\t\t\t\t\tres -= int64(curLen)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\tpanic(fmt.Sprintf(\"there is an unexpected data structure that is unimplemented in our database.:%d\", bucket.Ds))\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\t_ = tx.pendingBucketList.rangeBucket(f)\n\treturn res\n}\n\nfunc (tx *Tx) getBucketStatus(ds Ds, name BucketName) BucketStatus {\n\tif len(tx.pendingBucketList) > 0 {\n\t\tif bucketInDs, exist := tx.pendingBucketList[ds]; exist {\n\t\t\tif bucket, exist := bucketInDs[name]; exist {\n\t\t\t\tswitch bucket.Meta.Op {\n\t\t\t\tcase BucketInsertOperation:\n\t\t\t\t\treturn BucketStatusNew\n\t\t\t\tcase BucketDeleteOperation:\n\t\t\t\t\treturn BucketStatusDeleted\n\t\t\t\tcase BucketUpdateOperation:\n\t\t\t\t\treturn BucketStatusUpdated\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif tx.db.bm.ExistBucket(ds, name) {\n\t\treturn BucketStatusExistAlready\n\t}\n\treturn BucketStatusUnknown\n}\n\n// findEntryStatus finds the latest status for the certain Entry in Tx\nfunc (tx *Tx) findEntryAndItsStatus(ds Ds, bucket BucketName, key string) (EntryStatus, *Entry) {\n\tif tx.pendingWrites.size == 0 {\n\t\treturn NotFoundEntry, nil\n\t}\n\tpendingWriteEntries := tx.pendingWrites.entriesInBTree\n\tif pendingWriteEntries == nil {\n\t\treturn NotFoundEntry, nil\n\t}\n\tif pendingWriteEntries[bucket] == nil {\n\t\treturn NotFoundEntry, nil\n\t}\n\tentries := pendingWriteEntries[bucket]\n\tif entry, exist := entries[key]; exist {\n\t\tswitch entry.Meta.Flag {\n\t\tcase DataDeleteFlag:\n\t\t\treturn EntryDeleted, nil\n\t\tdefault:\n\t\t\treturn EntryUpdated, entry\n\t\t}\n\t}\n\treturn NotFoundEntry, nil\n}\n"
        },
        {
          "name": "tx_btree.go",
          "type": "blob",
          "size": 16.25,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"errors\"\n\t\"math\"\n\t\"math/big\"\n\t\"strconv\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/xujiajun/utils/strconv2\"\n)\n\nconst (\n\tgetAllType    uint8 = 0\n\tgetKeysType   uint8 = 1\n\tgetValuesType uint8 = 2\n)\n\nfunc (tx *Tx) PutWithTimestamp(bucket string, key, value []byte, ttl uint32, timestamp uint64) error {\n\treturn tx.put(bucket, key, value, ttl, DataSetFlag, timestamp, DataStructureBTree)\n}\n\n// Put sets the value for a key in the bucket.\n// a wrapper of the function put.\nfunc (tx *Tx) Put(bucket string, key, value []byte, ttl uint32) error {\n\treturn tx.put(bucket, key, value, ttl, DataSetFlag, uint64(time.Now().UnixMilli()), DataStructureBTree)\n}\n\n// PutIfNotExists set the value for a key in the bucket only if the key doesn't exist already.\nfunc (tx *Tx) PutIfNotExists(bucket string, key, value []byte, ttl uint32) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbucketId := b.Id\n\n\tidx, bucketExists := tx.db.Index.bTree.exist(bucketId)\n\tif !bucketExists {\n\t\treturn ErrNotFoundBucket\n\t}\n\trecord, recordExists := idx.Find(key)\n\n\tif recordExists && !record.IsExpired() {\n\t\treturn nil\n\t}\n\n\treturn tx.put(bucket, key, value, ttl, DataSetFlag, uint64(time.Now().UnixMilli()), DataStructureBTree)\n}\n\n// PutIfExits set the value for a key in the bucket only if the key already exits.\nfunc (tx *Tx) PutIfExists(bucket string, key, value []byte, ttl uint32) error {\n\treturn tx.update(bucket, key, func(_ []byte) ([]byte, error) {\n\t\treturn value, nil\n\t}, func(_ uint32) (uint32, error) {\n\t\treturn ttl, nil\n\t})\n}\n\n// Get retrieves the value for a key in the bucket.\n// The returned value is only valid for the life of the transaction.\nfunc (tx *Tx) Get(bucket string, key []byte) (value []byte, err error) {\n\treturn tx.get(bucket, key)\n}\n\nfunc (tx *Tx) get(bucket string, key []byte) (value []byte, err error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId := b.Id\n\n\tbucketStatus := tx.getBucketStatus(DataStructureBTree, bucket)\n\tif bucketStatus == BucketStatusDeleted {\n\t\treturn nil, ErrBucketNotFound\n\t}\n\n\tstatus, entry := tx.findEntryAndItsStatus(DataStructureBTree, bucket, string(key))\n\tif status != NotFoundEntry && entry != nil {\n\t\tif status == EntryDeleted {\n\t\t\treturn nil, ErrKeyNotFound\n\t\t} else {\n\t\t\treturn entry.Value, nil\n\t\t}\n\t}\n\n\tif idx, ok := tx.db.Index.bTree.exist(bucketId); ok {\n\t\trecord, found := idx.Find(key)\n\t\tif !found {\n\t\t\treturn nil, ErrKeyNotFound\n\t\t}\n\n\t\tif record.IsExpired() {\n\t\t\ttx.putDeleteLog(bucketId, key, nil, Persistent, DataDeleteFlag, uint64(time.Now().Unix()), DataStructureBTree)\n\t\t\treturn nil, ErrNotFoundKey\n\t\t}\n\n\t\tvalue, err = tx.db.getValueByRecord(record)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn value, nil\n\n\t} else {\n\t\treturn nil, ErrNotFoundBucket\n\t}\n}\n\nfunc (tx *Tx) ValueLen(bucket string, key []byte) (int, error) {\n\tvalue, err := tx.get(bucket, key)\n\treturn len(value), err\n}\n\nfunc (tx *Tx) GetMaxKey(bucket string) ([]byte, error) {\n\treturn tx.getMaxOrMinKey(bucket, true)\n}\n\nfunc (tx *Tx) GetMinKey(bucket string) ([]byte, error) {\n\treturn tx.getMaxOrMinKey(bucket, false)\n}\n\nfunc (tx *Tx) getMaxOrMinKey(bucket string, isMax bool) ([]byte, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId := b.Id\n\n\tif idx, ok := tx.db.Index.bTree.exist(bucketId); ok {\n\t\tvar (\n\t\t\titem  *Item\n\t\t\tfound bool\n\t\t)\n\n\t\tif isMax {\n\t\t\titem, found = idx.Max()\n\t\t} else {\n\t\t\titem, found = idx.Min()\n\t\t}\n\n\t\tif !found {\n\t\t\treturn nil, ErrKeyNotFound\n\t\t}\n\n\t\tif item.record.IsExpired() {\n\t\t\ttx.putDeleteLog(bucketId, item.key, nil, Persistent, DataDeleteFlag, uint64(time.Now().Unix()), DataStructureBTree)\n\t\t\treturn nil, ErrNotFoundKey\n\t\t}\n\n\t\treturn item.key, nil\n\t} else {\n\t\treturn nil, ErrNotFoundBucket\n\t}\n}\n\n// GetAll returns all keys and values of the bucket stored at given bucket.\nfunc (tx *Tx) GetAll(bucket string) ([][]byte, [][]byte, error) {\n\treturn tx.getAllOrKeysOrValues(bucket, getAllType)\n}\n\n// GetKeys returns all keys of the bucket stored at given bucket.\nfunc (tx *Tx) GetKeys(bucket string) ([][]byte, error) {\n\tkeys, _, err := tx.getAllOrKeysOrValues(bucket, getKeysType)\n\treturn keys, err\n}\n\n// GetValues returns all values of the bucket stored at given bucket.\nfunc (tx *Tx) GetValues(bucket string) ([][]byte, error) {\n\t_, values, err := tx.getAllOrKeysOrValues(bucket, getValuesType)\n\treturn values, err\n}\n\nfunc (tx *Tx) getAllOrKeysOrValues(bucket string, typ uint8) ([][]byte, [][]byte, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tbucketId, err := tx.db.bm.GetBucketID(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tif index, ok := tx.db.Index.bTree.exist(bucketId); ok {\n\t\trecords := index.All()\n\n\t\tvar (\n\t\t\tkeys   [][]byte\n\t\t\tvalues [][]byte\n\t\t)\n\n\t\tswitch typ {\n\t\tcase getAllType:\n\t\t\tkeys, values, err = tx.getHintIdxDataItemsWrapper(records, ScanNoLimit, bucketId, true, true)\n\t\tcase getKeysType:\n\t\t\tkeys, _, err = tx.getHintIdxDataItemsWrapper(records, ScanNoLimit, bucketId, true, false)\n\t\tcase getValuesType:\n\t\t\t_, values, err = tx.getHintIdxDataItemsWrapper(records, ScanNoLimit, bucketId, false, true)\n\t\t}\n\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n\t\treturn keys, values, nil\n\t}\n\n\treturn nil, nil, nil\n}\n\nfunc (tx *Tx) GetSet(bucket string, key, value []byte) (oldValue []byte, err error) {\n\treturn oldValue, tx.update(bucket, key, func(b []byte) ([]byte, error) {\n\t\toldValue = b\n\t\treturn value, nil\n\t}, func(oldTTL uint32) (uint32, error) {\n\t\treturn oldTTL, nil\n\t})\n}\n\n// RangeScan query a range at given bucket, start and end slice.\nfunc (tx *Tx) RangeScan(bucket string, start, end []byte) (values [][]byte, err error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId := b.Id\n\n\tif index, ok := tx.db.Index.bTree.exist(bucketId); ok {\n\t\trecords := index.Range(start, end)\n\n\t\t_, values, err = tx.getHintIdxDataItemsWrapper(records, ScanNoLimit, bucketId, false, true)\n\t\tif err != nil {\n\t\t\treturn nil, ErrRangeScan\n\t\t}\n\t}\n\n\tif len(values) == 0 {\n\t\treturn nil, ErrRangeScan\n\t}\n\n\treturn\n}\n\n// PrefixScan iterates over a key prefix at given bucket, prefix and limitNum.\n// LimitNum will limit the number of entries return.\nfunc (tx *Tx) PrefixScan(bucket string, prefix []byte, offsetNum int, limitNum int) (values [][]byte, err error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId := b.Id\n\n\tif idx, ok := tx.db.Index.bTree.exist(bucketId); ok {\n\t\trecords := idx.PrefixScan(prefix, offsetNum, limitNum)\n\t\t_, values, err = tx.getHintIdxDataItemsWrapper(records, limitNum, bucketId, false, true)\n\t\tif err != nil {\n\t\t\treturn nil, ErrPrefixScan\n\t\t}\n\t}\n\n\tif len(values) == 0 {\n\t\treturn nil, ErrPrefixScan\n\t}\n\n\treturn\n}\n\n// PrefixSearchScan iterates over a key prefix at given bucket, prefix, match regular expression and limitNum.\n// LimitNum will limit the number of entries return.\nfunc (tx *Tx) PrefixSearchScan(bucket string, prefix []byte, reg string, offsetNum int, limitNum int) (values [][]byte, err error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId := b.Id\n\n\tif idx, ok := tx.db.Index.bTree.exist(bucketId); ok {\n\t\trecords := idx.PrefixSearchScan(prefix, reg, offsetNum, limitNum)\n\t\t_, values, err = tx.getHintIdxDataItemsWrapper(records, limitNum, bucketId, false, true)\n\t\tif err != nil {\n\t\t\treturn nil, ErrPrefixSearchScan\n\t\t}\n\t}\n\n\tif len(values) == 0 {\n\t\treturn nil, ErrPrefixSearchScan\n\t}\n\n\treturn\n}\n\n// Delete removes a key from the bucket at given bucket and key.\nfunc (tx *Tx) Delete(bucket string, key []byte) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbucketId := b.Id\n\n\tif idx, ok := tx.db.Index.bTree.exist(bucketId); ok {\n\t\tif _, found := idx.Find(key); !found {\n\t\t\treturn ErrKeyNotFound\n\t\t}\n\t} else {\n\t\treturn ErrNotFoundBucket\n\t}\n\n\treturn tx.put(bucket, key, nil, Persistent, DataDeleteFlag, uint64(time.Now().Unix()), DataStructureBTree)\n}\n\n// getHintIdxDataItemsWrapper returns keys and values when prefix scanning or range scanning.\nfunc (tx *Tx) getHintIdxDataItemsWrapper(records []*Record, limitNum int, bucketId BucketId, needKeys bool, needValues bool) (keys [][]byte, values [][]byte, err error) {\n\tfor _, record := range records {\n\t\tif record.IsExpired() {\n\t\t\ttx.putDeleteLog(bucketId, record.Key, nil, Persistent, DataDeleteFlag, uint64(time.Now().Unix()), DataStructureBTree)\n\t\t\tcontinue\n\t\t}\n\n\t\tif limitNum > 0 && len(values) < limitNum || limitNum == ScanNoLimit {\n\t\t\tif needKeys {\n\t\t\t\tkeys = append(keys, record.Key)\n\t\t\t}\n\n\t\t\tif needValues {\n\t\t\t\tvalue, err := tx.db.getValueByRecord(record)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, nil, err\n\t\t\t\t}\n\t\t\t\tvalues = append(values, value)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn keys, values, nil\n}\n\nfunc (tx *Tx) tryGet(bucket string, key []byte, solveRecord func(record *Record, found bool, bucketId BucketId) error) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\n\tbucketId, err := tx.db.bm.GetBucketID(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif idx, ok := tx.db.Index.bTree.exist(bucketId); ok {\n\t\trecord, found := idx.Find(key)\n\t\treturn solveRecord(record, found, bucketId)\n\t} else {\n\t\treturn ErrBucketNotFound\n\t}\n}\n\nfunc (tx *Tx) update(bucket string, key []byte, getNewValue func([]byte) ([]byte, error), getNewTTL func(uint32) (uint32, error)) error {\n\treturn tx.tryGet(bucket, key, func(record *Record, found bool, bucketId BucketId) error {\n\t\tif !found {\n\t\t\treturn ErrKeyNotFound\n\t\t}\n\n\t\tif record.IsExpired() {\n\t\t\ttx.putDeleteLog(bucketId, key, nil, Persistent, DataDeleteFlag, uint64(time.Now().Unix()), DataStructureBTree)\n\t\t\treturn ErrNotFoundKey\n\t\t}\n\n\t\tvalue, err := tx.db.getValueByRecord(record)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tnewValue, err := getNewValue(value)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tnewTTL, err := getNewTTL(record.TTL)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\treturn tx.put(bucket, key, newValue, newTTL, DataSetFlag, uint64(time.Now().Unix()), DataStructureBTree)\n\t})\n}\n\nfunc (tx *Tx) updateOrPut(bucket string, key, value []byte, getUpdatedValue func([]byte) ([]byte, error)) error {\n\treturn tx.tryGet(bucket, key, func(record *Record, found bool, bucketId BucketId) error {\n\t\tif !found {\n\t\t\treturn tx.put(bucket, key, value, Persistent, DataSetFlag, uint64(time.Now().Unix()), DataStructureBTree)\n\t\t}\n\n\t\tvalue, err := tx.db.getValueByRecord(record)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tnewValue, err := getUpdatedValue(value)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\treturn tx.put(bucket, key, newValue, record.TTL, DataSetFlag, uint64(time.Now().Unix()), DataStructureBTree)\n\t})\n}\n\nfunc bigIntIncr(a string, b string) string {\n\tbigIntA, _ := new(big.Int).SetString(a, 10)\n\tbigIntB, _ := new(big.Int).SetString(b, 10)\n\tbigIntA.Add(bigIntA, bigIntB)\n\treturn bigIntA.String()\n}\n\nfunc (tx *Tx) integerIncr(bucket string, key []byte, increment int64) error {\n\treturn tx.update(bucket, key, func(value []byte) ([]byte, error) {\n\t\tintValue, err := strconv2.StrToInt64(string(value))\n\n\t\tif err != nil && errors.Is(err, strconv.ErrRange) {\n\t\t\treturn []byte(bigIntIncr(string(value), strconv2.Int64ToStr(increment))), nil\n\t\t}\n\n\t\tif err != nil {\n\t\t\treturn nil, ErrValueNotInteger\n\t\t}\n\n\t\tif (increment > 0 && math.MaxInt64-increment < intValue) || (increment < 0 && math.MinInt64-increment > intValue) {\n\t\t\treturn []byte(bigIntIncr(string(value), strconv2.Int64ToStr(increment))), nil\n\t\t}\n\n\t\tatomic.AddInt64(&intValue, increment)\n\t\treturn []byte(strconv2.Int64ToStr(intValue)), nil\n\t}, func(oldTTL uint32) (uint32, error) {\n\t\treturn oldTTL, nil\n\t})\n}\n\nfunc (tx *Tx) Incr(bucket string, key []byte) error {\n\treturn tx.integerIncr(bucket, key, 1)\n}\n\nfunc (tx *Tx) Decr(bucket string, key []byte) error {\n\treturn tx.integerIncr(bucket, key, -1)\n}\n\nfunc (tx *Tx) IncrBy(bucket string, key []byte, increment int64) error {\n\treturn tx.integerIncr(bucket, key, increment)\n}\n\nfunc (tx *Tx) DecrBy(bucket string, key []byte, decrement int64) error {\n\treturn tx.integerIncr(bucket, key, -1*decrement)\n}\n\nfunc (tx *Tx) GetBit(bucket string, key []byte, offset int) (byte, error) {\n\tif offset >= math.MaxInt || offset < 0 {\n\t\treturn 0, ErrOffsetInvalid\n\t}\n\n\tvalue, err := tx.Get(bucket, key)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tif len(value) <= offset {\n\t\treturn 0, nil\n\t}\n\n\treturn value[offset], nil\n}\n\nfunc (tx *Tx) SetBit(bucket string, key []byte, offset int, bit byte) error {\n\tif offset >= math.MaxInt || offset < 0 {\n\t\treturn ErrOffsetInvalid\n\t}\n\n\tvalueIfKeyNotFound := make([]byte, offset+1)\n\tvalueIfKeyNotFound[offset] = bit\n\n\treturn tx.updateOrPut(bucket, key, valueIfKeyNotFound, func(value []byte) ([]byte, error) {\n\t\tif len(value) <= offset {\n\t\t\tvalue = append(value, make([]byte, offset-len(value)+1)...)\n\t\t\tvalue[offset] = bit\n\t\t\treturn value, nil\n\t\t} else {\n\t\t\tvalue[offset] = bit\n\t\t\treturn value, nil\n\t\t}\n\t})\n}\n\n// GetTTL returns remaining TTL of a value by key.\n// It returns\n// (-1, nil) If TTL is Persistent\n// (0, ErrBucketNotFound|ErrKeyNotFound) If expired or not found\n// (TTL, nil) If the record exists with a TTL\n// Note: The returned remaining TTL will be in seconds. For example,\n// remainingTTL is 500ms, It'll return 0.\nfunc (tx *Tx) GetTTL(bucket string, key []byte) (int64, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn 0, err\n\t}\n\n\tbucketId, err := tx.db.bm.GetBucketID(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tidx, bucketExists := tx.db.Index.bTree.exist(bucketId)\n\n\tif !bucketExists {\n\t\treturn 0, ErrBucketNotFound\n\t}\n\n\trecord, recordFound := idx.Find(key)\n\n\tif !recordFound || record.IsExpired() {\n\t\treturn 0, ErrKeyNotFound\n\t}\n\n\tif record.TTL == Persistent {\n\t\treturn -1, nil\n\t}\n\n\tremTTL := tx.db.expireTime(record.Timestamp, record.TTL)\n\tif remTTL >= 0 {\n\t\treturn int64(remTTL.Seconds()), nil\n\t} else {\n\t\treturn 0, ErrKeyNotFound\n\t}\n}\n\n// Persist updates record's TTL as Persistent if the record exits.\nfunc (tx *Tx) Persist(bucket string, key []byte) error {\n\treturn tx.update(bucket, key, func(oldValue []byte) ([]byte, error) {\n\t\treturn oldValue, nil\n\t}, func(_ uint32) (uint32, error) {\n\t\treturn Persistent, nil\n\t})\n}\n\nfunc (tx *Tx) MSet(bucket string, ttl uint32, args ...[]byte) error {\n\tif len(args) == 0 {\n\t\treturn nil\n\t}\n\n\tif len(args)%2 != 0 {\n\t\treturn ErrKVArgsLenNotEven\n\t}\n\n\tfor i := 0; i < len(args); i += 2 {\n\t\tif err := tx.put(bucket, args[i], args[i+1], ttl, DataSetFlag, uint64(time.Now().Unix()), DataStructureBTree); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (tx *Tx) MGet(bucket string, keys ...[]byte) ([][]byte, error) {\n\tif len(keys) == 0 {\n\t\treturn nil, nil\n\t}\n\n\tvalues := make([][]byte, len(keys))\n\tfor i, key := range keys {\n\t\tvalue, err := tx.Get(bucket, key)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tvalues[i] = value\n\t}\n\n\treturn values, nil\n}\n\nfunc (tx *Tx) Append(bucket string, key, appendage []byte) error {\n\tif len(appendage) == 0 {\n\t\treturn nil\n\t}\n\n\treturn tx.updateOrPut(bucket, key, appendage, func(value []byte) ([]byte, error) {\n\t\treturn append(value, appendage...), nil\n\t})\n}\n\nfunc (tx *Tx) GetRange(bucket string, key []byte, start, end int) ([]byte, error) {\n\tif start > end {\n\t\treturn nil, ErrStartGreaterThanEnd\n\t}\n\n\tvalue, err := tx.get(bucket, key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif start >= len(value) {\n\t\treturn nil, nil\n\t}\n\n\tif end >= len(value) {\n\t\treturn value[start:], nil\n\t}\n\n\treturn value[start : end+1], nil\n}\n"
        },
        {
          "name": "tx_btree_test.go",
          "type": "blob",
          "size": 36.736328125,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n\t\"github.com/xujiajun/utils/strconv2\"\n)\n\nfunc TestTx_PutAndGet(t *testing.T) {\n\n\tvar (\n\t\tbucket = \"bucket1\"\n\t\tkey    = []byte(\"key1\")\n\t\tval    = []byte(\"val1\")\n\t)\n\n\tt.Run(\"put_and_get\", func(t *testing.T) {\n\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\n\t\t\t{\n\t\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\t\ttx, err := db.Begin(true)\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\t\tassert.NoError(t, err)\n\n\t\t\t\tassert.NoError(t, tx.Commit())\n\t\t\t}\n\n\t\t\t{\n\t\t\t\ttx, err = db.Begin(false)\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\tvalue, err := tx.Get(bucket, key)\n\t\t\t\tassert.NoError(t, err)\n\t\t\t\tassert.NoError(t, tx.Commit())\n\n\t\t\t\tassert.Equal(t, val, value)\n\t\t\t}\n\n\t\t})\n\t})\n\n\tt.Run(\"get by closed tx\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\ttx, err := db.Begin(false)\n\t\t\trequire.NoError(t, err)\n\t\t\tassert.NoError(t, tx.Commit())\n\n\t\t\t_, err = tx.Get(bucket, key) // use closed tx\n\t\t\tassert.Error(t, err)\n\t\t})\n\t})\n\n}\n\nfunc TestTx_GetAll_GetKeys_GetValues(t *testing.T) {\n\tbucket := \"bucket\"\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\ttxGetAll(t, db, bucket, nil, nil, nil)\n\n\t\tn := 10\n\t\tkeys := make([][]byte, n)\n\t\tvalues := make([][]byte, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\tkeys[i] = GetTestBytes(i)\n\t\t\tvalues[i] = GetRandomBytes(10)\n\t\t\ttxPut(t, db, bucket, keys[i], values[i], Persistent, nil, nil)\n\t\t}\n\n\t\ttxGetAll(t, db, bucket, keys, values, nil)\n\n\t\tkeys = append(keys, GetTestBytes(10))\n\t\tvalues = append(values, GetRandomBytes(10))\n\t\ttxPut(t, db, bucket, keys[10], values[10], Persistent, nil, nil)\n\t\ttxGetAll(t, db, bucket, keys, values, nil)\n\n\t\ttxDel(t, db, bucket, keys[0], nil)\n\t\tkeys = keys[1:]\n\t\tvalues = values[1:]\n\t\ttxGetAll(t, db, bucket, keys, values, nil)\n\n\t\ttxPut(t, db, bucket, GetTestBytes(11), GetRandomBytes(10), 1, nil, nil)\n\t\ttime.Sleep(1100 * time.Millisecond)\n\t\ttxGetAll(t, db, bucket, keys, values, nil)\n\n\t\trequire.NoError(t, db.View(func(tx *Tx) error {\n\t\t\tkeysInBucket, err := tx.GetKeys(bucket)\n\t\t\trequire.NoError(t, err)\n\t\t\tvaluesInBucket, err := tx.GetValues(bucket)\n\t\t\trequire.NoError(t, err)\n\t\t\tfor i := 0; i < n; i++ {\n\t\t\t\trequire.Equal(t, keys[i], keysInBucket[i])\n\t\t\t\trequire.Equal(t, values[i], valuesInBucket[i])\n\t\t\t}\n\t\t\treturn nil\n\t\t}))\n\t})\n}\n\nfunc TestTx_RangeScan_Err(t *testing.T) {\n\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\n\t\tbucket := \"bucket_for_rangeScan\"\n\n\t\t{\n\t\t\t// setup the data\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttx, err := db.Begin(true)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tfor i := 0; i < 10; i++ {\n\t\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\tval := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\t\tassert.NoError(t, err)\n\t\t\t}\n\n\t\t\tassert.NoError(t, tx.Commit()) // tx commit\n\t\t}\n\n\t\t{\n\t\t\t// verify\n\n\t\t\ttx, err := db.Begin(false)\n\t\t\tassert.NoError(t, err)\n\n\t\t\tstart := []byte(\"key_0010001\")\n\t\t\tend := []byte(\"key_0010010\")\n\t\t\t_, err = tx.RangeScan(bucket, start, end)\n\t\t\tassert.Error(t, err)\n\n\t\t\tassert.NoError(t, tx.Rollback())\n\t\t}\n\t})\n}\n\nfunc TestTx_RangeScan(t *testing.T) {\n\tbucket := \"bucket_for_range\"\n\n\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\n\t\t{\n\t\t\t// setup the data\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttx, err := db.Begin(true)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tfor i := 0; i < 10; i++ {\n\t\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\tval := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\t\tassert.NoError(t, err)\n\t\t\t}\n\n\t\t\tassert.NoError(t, tx.Commit()) // tx commit\n\t\t}\n\n\t\t{\n\n\t\t\ttx, err = db.Begin(false)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tvar (\n\t\t\t\ts = 5\n\t\t\t\te = 8\n\t\t\t)\n\n\t\t\tstart := []byte(fmt.Sprintf(\"key_%07d\", s))\n\t\t\tend := []byte(fmt.Sprintf(\"key_%07d\", e))\n\n\t\t\tvalues, err := tx.RangeScan(bucket, start, end)\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.NoError(t, tx.Commit()) // tx commit\n\n\t\t\twantCount := (e - s) + 1 // the range: [5, 8]\n\t\t\tassert.Equal(t, wantCount, len(values))\n\n\t\t\tfor i, value := range values {\n\n\t\t\t\twantValue := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", i+s))\n\t\t\t\tassert.Equal(t, wantValue, value)\n\t\t\t}\n\n\t\t\tvalues, err = tx.RangeScan(bucket, start, end)\n\t\t\tassert.Error(t, err)\n\t\t\tassert.Empty(t, values)\n\t\t}\n\n\t})\n\n}\n\nfunc TestTx_PrefixScan(t *testing.T) {\n\n\tbucket := \"bucket_for_prefix_scan\"\n\n\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\n\t\t{\n\t\t\t// setup the data\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttx, err := db.Begin(true)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tfor i, prefix := range []string{\"key1_\", \"key2_\"} {\n\t\t\t\tfor j := 0; j < 10; j++ {\n\t\t\t\t\tkey := []byte(prefix + fmt.Sprintf(\"%07d\", j))\n\t\t\t\t\tval := []byte(\"foobar\" + fmt.Sprintf(\"%d%07d\", i+1, j))\n\t\t\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\t\t\tassert.NoError(t, err)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassert.NoError(t, tx.Commit()) // tx commit\n\t\t}\n\n\t\t{\n\t\t\ttx, err = db.Begin(false)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tvar (\n\t\t\t\toffset = 5\n\t\t\t\tlimit  = 3\n\t\t\t)\n\n\t\t\tprefix := []byte(\"key1_\")\n\t\t\tvalues, err := tx.PrefixScan(bucket, prefix, offset, limit)\n\t\t\tassert.NoError(t, err)\n\n\t\t\tassert.NoError(t, tx.Commit())\n\n\t\t\tassert.Equal(t, limit, len(values))\n\n\t\t\tfor i := 0; i < limit; i++ {\n\t\t\t\tvalIndex := offset + i\n\n\t\t\t\twantVal := []byte(\"foobar\" + fmt.Sprintf(\"%d%07d\", 1, valIndex))\n\t\t\t\tassert.Equal(t, wantVal, values[i])\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc TestTx_PrefixSearchScan(t *testing.T) {\n\tbucket := \"bucket_for_prefix_search_scan\"\n\n\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\tregs := \"1\"\n\n\t\ttx, err := db.Begin(true)\n\t\trequire.NoError(t, err)\n\n\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", 0))\n\t\tval := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", 0))\n\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\tassert.NoError(t, err)\n\n\t\tassert.NoError(t, tx.Commit()) // tx commit\n\n\t\ttx, err = db.Begin(true)\n\t\trequire.NoError(t, err)\n\n\t\tkey = []byte(\"key_\" + fmt.Sprintf(\"%07d\", 1))\n\t\tval = []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", 1))\n\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\tassert.NoError(t, err)\n\n\t\tassert.NoError(t, tx.Commit()) // tx commit\n\n\t\ttx, err = db.Begin(false)\n\t\trequire.NoError(t, err)\n\n\t\tprefix := []byte(\"key_\")\n\t\tvalues, err := tx.PrefixSearchScan(bucket, prefix, regs, 0, 1)\n\t\tassert.NoError(t, err)\n\n\t\tassert.NoError(t, tx.Commit()) // tx commit\n\n\t\tc := 0\n\t\tfor _, value := range values {\n\t\t\twantVal := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", 1))\n\n\t\t\tassert.Equal(t, wantVal, value)\n\t\t\tc++\n\t\t}\n\n\t\tassert.Equal(t, 1, c)\n\t})\n}\n\nfunc TestTx_DeleteAndGet(t *testing.T) {\n\n\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\n\t\tbucket := \"bucket_delete_test\"\n\n\t\t{\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttx, err := db.Begin(true)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tfor i := 0; i <= 10; i++ {\n\t\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\tval := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\terr := tx.Put(bucket, key, val, Persistent)\n\t\t\t\tassert.NoError(t, err)\n\t\t\t}\n\n\t\t\t// tx commit\n\t\t\tassert.NoError(t, tx.Commit())\n\t\t}\n\n\t\t{\n\t\t\ttx, err := db.Begin(true)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tfor i := 0; i <= 5; i++ {\n\t\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\terr := tx.Delete(bucket, key)\n\t\t\t\tassert.NoError(t, err)\n\t\t\t}\n\n\t\t\t// tx commit\n\t\t\tassert.NoError(t, tx.Commit())\n\n\t\t\terr = tx.Delete(bucket, []byte(\"key_\"+fmt.Sprintf(\"%07d\", 1)))\n\t\t\tassert.Error(t, err)\n\t\t}\n\t})\n\n}\n\nfunc TestTx_DeleteFromMemory(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tbucket := \"bucket\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxPut(t, db, bucket, GetTestBytes(i), GetTestBytes(i), Persistent, nil, nil)\n\t\t}\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxGet(t, db, bucket, GetTestBytes(i), GetTestBytes(i), nil)\n\t\t}\n\n\t\ttxDel(t, db, bucket, GetTestBytes(3), nil)\n\n\t\terr := db.View(func(tx *Tx) error {\n\t\t\tr, ok := db.Index.bTree.getWithDefault(1).Find(GetTestBytes(3))\n\t\t\trequire.Nil(t, r)\n\t\t\trequire.False(t, ok)\n\n\t\t\treturn nil\n\t\t})\n\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc TestTx_GetAndScansFromHintKey(t *testing.T) {\n\n\tbucket := \"bucket_get_test\"\n\twithRAMIdxDB(t, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t// write tx begin\n\t\ttx, err := db.Begin(true)\n\t\trequire.NoError(t, err)\n\n\t\tfor i := 0; i <= 10; i++ {\n\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", i))\n\t\t\tval := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", i))\n\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\tassert.NoError(t, err)\n\t\t}\n\t\tassert.NoError(t, tx.Commit()) // tx commit\n\n\t\tfor i := 0; i <= 10; i++ {\n\t\t\ttx, err = db.Begin(false)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", i))\n\t\t\tvalue, err := tx.Get(bucket, key)\n\t\t\tassert.NoError(t, err)\n\n\t\t\twantValue := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", i))\n\t\t\tassert.Equal(t, wantValue, value)\n\n\t\t\t// tx commit\n\t\t\tassert.NoError(t, tx.Commit())\n\t\t}\n\n\t\ttx, err = db.Begin(false)\n\t\trequire.NoError(t, err)\n\n\t\tstart := []byte(\"key_0000001\")\n\t\tend := []byte(\"key_0000010\")\n\t\tvalues, err := tx.RangeScan(bucket, start, end)\n\t\tassert.NoError(t, err)\n\n\t\tj := 0\n\t\tfor i := 1; i <= 10; i++ {\n\t\t\twantVal := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", i))\n\t\t\tassert.Equal(t, wantVal, values[j])\n\n\t\t\tj++\n\t\t}\n\n\t\t// tx commit\n\t\tassert.NoError(t, tx.Commit())\n\t})\n\n}\n\nfunc TestTx_Put_Err(t *testing.T) {\n\n\tbucket := \"bucket_tx_put\"\n\n\tt.Run(\"write with read only tx\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\t// write tx begin err setting here\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttx, err := db.Begin(false) // tx not writable\n\t\t\trequire.NoError(t, err)\n\n\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", 0))\n\t\t\tval := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", 0))\n\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\tassert.Error(t, err)\n\n\t\t\tassert.NoError(t, tx.Rollback())\n\t\t})\n\t})\n\n\tt.Run(\"write with empty key\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttx, err := db.Begin(true)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tkey := []byte(\"\") // key cannot be empty\n\t\t\tval := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", 0))\n\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\tassert.Error(t, err)\n\n\t\t\tassert.NoError(t, tx.Rollback())\n\t\t})\n\t})\n\n\tt.Run(\"write with TOO big size\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttx, err := db.Begin(true)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tkey := []byte(\"key_bigone\")\n\t\t\tvar bigVal string // too big size\n\t\t\tfor i := 1; i <= 9*1024; i++ {\n\t\t\t\tbigVal += \"val\" + strconv2.IntToStr(i)\n\t\t\t}\n\n\t\t\terr = tx.Put(bucket, key, []byte(bigVal), Persistent)\n\t\t\tassert.NoError(t, err)\n\n\t\t\tassert.Error(t, tx.Commit()) // too big cannot commit by tx\n\t\t})\n\t})\n}\n\nfunc TestTx_PrefixScan_NotFound(t *testing.T) {\n\tt.Run(\"prefix scan in empty bucket\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttx, err := db.Begin(false)\n\t\t\tassert.NoError(t, err)\n\n\t\t\tprefix := []byte(\"key_\")\n\t\t\tentries, err := tx.PrefixScan(\"foobucket\", prefix, 0, 10)\n\t\t\tassert.Error(t, err)\n\t\t\tassert.Empty(t, entries)\n\n\t\t\tassert.NoError(t, tx.Commit()) // tx commit\n\t\t})\n\t})\n\n\tt.Run(\"prefix scan\", func(t *testing.T) {\n\t\tbucket := \"bucket_prefix_scan_test\"\n\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\t{ // write tx begin\n\t\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\t\ttx, err := db.Begin(true)\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\tfor i := 0; i <= 10; i++ {\n\t\t\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\t\tval := []byte(\"val\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\t\t\tassert.NoError(t, err)\n\t\t\t\t}\n\n\t\t\t\tassert.NoError(t, tx.Commit()) // tx commit\n\t\t\t}\n\n\t\t\t{\n\t\t\t\ttx, err = db.Begin(false)\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\tprefix := []byte(\"key_foo\")\n\t\t\t\tentries, err := tx.PrefixScan(bucket, prefix, 0, 10)\n\t\t\t\tassert.Error(t, err)\n\t\t\t\tassert.NoError(t, tx.Commit())\n\n\t\t\t\tassert.Empty(t, entries)\n\t\t\t}\n\n\t\t\t{\n\t\t\t\ttx, err = db.Begin(false)\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\tentries, err := tx.PrefixScan(bucket, []byte(\"key_\"), 0, 10)\n\t\t\t\tassert.NoError(t, err)\n\t\t\t\tassert.NoError(t, tx.Commit())\n\n\t\t\t\tassert.NotEmpty(t, entries)\n\n\t\t\t}\n\n\t\t\t{ // scan by closed tx\n\t\t\t\tentries, err := tx.PrefixScan(bucket, []byte(\"key_\"), 0, 10)\n\t\t\t\tassert.Error(t, err)\n\t\t\t\tif len(entries) > 0 || err == nil {\n\t\t\t\t\tt.Error(\"err TestTx_PrefixScan_NotFound\")\n\t\t\t\t}\n\t\t\t}\n\n\t\t})\n\t})\n}\n\nfunc TestTx_PrefixSearchScan_NotFound(t *testing.T) {\n\tregs := \"(.+)\"\n\tbucket := \"bucket_prefix_search_scan_test\"\n\n\tt.Run(\"prefix search in empty bucket\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttx, err := db.Begin(false)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tprefix := []byte(\"key_\")\n\t\t\t_, err = tx.PrefixSearchScan(\"foobucket\", prefix, regs, 0, 10)\n\t\t\tassert.Error(t, err)\n\n\t\t\tassert.NoError(t, tx.Commit())\n\t\t})\n\t})\n\n\tt.Run(\"prefix search scan\", func(t *testing.T) {\n\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\t{ // set up the data\n\t\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\t\ttx, err = db.Begin(true) // write tx begin\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\tfor i := 0; i <= 10; i++ {\n\t\t\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\t\tval := []byte(\"val\" + fmt.Sprintf(\"%07d\", i))\n\n\t\t\t\t\terr := tx.Put(bucket, key, val, Persistent)\n\t\t\t\t\tassert.NoError(t, err)\n\t\t\t\t}\n\t\t\t\t// tx commit\n\t\t\t\tassert.NoError(t, tx.Commit())\n\t\t\t}\n\n\t\t\t{ // no key exists in bucket\n\t\t\t\ttx, err := db.Begin(false)\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\tprefix := []byte(\"key_foo\")\n\t\t\t\t_, err = tx.PrefixSearchScan(bucket, prefix, regs, 0, 10)\n\t\t\t\tassert.Error(t, err)\n\n\t\t\t\tassert.NoError(t, tx.Rollback())\n\t\t\t}\n\n\t\t\t{ // scan by closed tx\n\t\t\t\ttx, err := db.Begin(false)\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\tassert.NoError(t, tx.Commit())\n\n\t\t\t\t_, err = tx.PrefixSearchScan(bucket, []byte(\"key_\"), regs, 0, 10)\n\t\t\t\tassert.Error(t, err)\n\t\t\t}\n\t\t})\n\t})\n}\n\nfunc TestTx_RangeScan_NotFound(t *testing.T) {\n\n\tbucket := \"bucket_range_scan_test\"\n\n\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\ttx, err := db.Begin(true) // write tx begin\n\t\trequire.NoError(t, err)\n\n\t\tfor i := 0; i <= 10; i++ {\n\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%03d\", i))\n\t\t\tval := []byte(\"val\" + fmt.Sprintf(\"%03d\", i))\n\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\tassert.NoError(t, err)\n\t\t}\n\t\tassert.NoError(t, tx.Commit()) // tx commit\n\n\t\ttx, err = db.Begin(false)\n\t\trequire.NoError(t, err)\n\n\t\tstart := []byte(\"key_011\")\n\t\tend := []byte(\"key_012\")\n\t\t_, err = tx.RangeScan(bucket, start, end)\n\t\tassert.Error(t, err)\n\n\t\tassert.NoError(t, tx.Commit())\n\t})\n}\n\nfunc TestTx_ExpiredDeletion(t *testing.T) {\n\tbucket := \"bucket\"\n\n\tt.Run(\"expired deletion\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, GetTestBytes(0), GetTestBytes(0), 1, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(1), GetTestBytes(1), 2, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(2), GetTestBytes(2), 3, nil, nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(0), GetTestBytes(0), nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), GetTestBytes(1), nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(2), GetTestBytes(2), nil)\n\n\t\t\ttxDel(t, db, bucket, GetTestBytes(2), nil)\n\n\t\t\ttime.Sleep(1100 * time.Millisecond)\n\n\t\t\t// this entry will be deleted\n\t\t\ttxGet(t, db, bucket, GetTestBytes(0), nil, ErrKeyNotFound)\n\t\t\t// this entry still alive\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), GetTestBytes(1), nil)\n\n\t\t\ttime.Sleep(1 * time.Second)\n\n\t\t\t// this entry will be deleted\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), nil, ErrKeyNotFound)\n\n\t\t\tr, ok := db.Index.bTree.getWithDefault(1).Find(GetTestBytes(0))\n\t\t\trequire.Nil(t, r)\n\t\t\trequire.False(t, ok)\n\n\t\t\tr, ok = db.Index.bTree.getWithDefault(1).Find(GetTestBytes(1))\n\t\t\trequire.Nil(t, r)\n\t\t\trequire.False(t, ok)\n\t\t})\n\t})\n\n\tt.Run(\"update expire time\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, GetTestBytes(0), GetTestBytes(0), 1, nil, nil)\n\t\t\ttime.Sleep(500 * time.Millisecond)\n\n\t\t\t// reset expire time\n\t\t\ttxPut(t, db, bucket, GetTestBytes(0), GetTestBytes(0), 3, nil, nil)\n\t\t\ttime.Sleep(1 * time.Second)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(0), GetTestBytes(0), nil)\n\n\t\t\ttime.Sleep(3 * time.Second)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(0), nil, ErrKeyNotFound)\n\t\t})\n\t})\n\n\tt.Run(\"persist expire time\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, GetTestBytes(0), GetTestBytes(0), 1, nil, nil)\n\t\t\ttime.Sleep(500 * time.Millisecond)\n\n\t\t\t// persist expire time\n\t\t\ttxPut(t, db, bucket, GetTestBytes(0), GetTestBytes(0), Persistent, nil, nil)\n\t\t\ttime.Sleep(1 * time.Second)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(0), GetTestBytes(0), nil)\n\n\t\t\ttime.Sleep(3 * time.Second)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(0), GetTestBytes(0), nil)\n\t\t})\n\t})\n\n\tt.Run(\"expired deletion when open\", func(t *testing.T) {\n\t\topts := DefaultOptions\n\t\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, GetTestBytes(0), GetTestBytes(0), 1, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(1), GetTestBytes(1), 3, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(2), GetTestBytes(2), 3, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(3), GetTestBytes(3), Persistent, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(4), GetTestBytes(4), Persistent, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(5), GetTestBytes(5), 5, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(1), GetTestBytes(1), Persistent, nil, nil)\n\t\t\ttxDel(t, db, bucket, GetTestBytes(5), nil)\n\n\t\t\trequire.NoError(t, db.Close())\n\n\t\t\ttime.Sleep(1100 * time.Millisecond)\n\n\t\t\tdb, err := Open(opts)\n\t\t\trequire.NoError(t, err)\n\n\t\t\ttxGet(t, db, bucket, GetTestBytes(0), nil, ErrKeyNotFound)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), GetTestBytes(1), nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(2), GetTestBytes(2), nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(5), nil, ErrKeyNotFound)\n\n\t\t\ttime.Sleep(2 * time.Second)\n\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), GetTestBytes(1), nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(2), GetTestBytes(2), ErrKeyNotFound)\n\n\t\t\ttime.Sleep(2 * time.Second)\n\t\t\t// this entry should be persistent\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), GetTestBytes(1), nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(3), GetTestBytes(3), nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(4), GetTestBytes(4), nil)\n\t\t})\n\t})\n\n\tt.Run(\"expire deletion when merge\", func(t *testing.T) {\n\t\topts := DefaultOptions\n\t\topts.SegmentSize = 1 * 100\n\t\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\t\tbucket := \"bucket\"\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, GetTestBytes(0), GetTestBytes(0), Persistent, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(1), GetTestBytes(1), Persistent, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(2), GetTestBytes(2), 1, nil, nil)\n\n\t\t\ttime.Sleep(1100 * time.Millisecond)\n\n\t\t\trequire.NoError(t, db.Merge())\n\n\t\t\ttxGet(t, db, bucket, GetTestBytes(0), GetTestBytes(0), nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), GetTestBytes(1), nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(2), nil, ErrKeyNotFound)\n\n\t\t\tr, ok := db.Index.bTree.getWithDefault(1).Find(GetTestBytes(2))\n\t\t\trequire.Nil(t, r)\n\t\t\trequire.False(t, ok)\n\t\t})\n\t})\n\n\tt.Run(\"expire deletion use time heap\", func(t *testing.T) {\n\t\topts := DefaultOptions\n\t\topts.ExpiredDeleteType = TimeHeap\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(0), GetTestBytes(0), 1, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(1), GetTestBytes(1), 2, nil, nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(0), GetTestBytes(0), nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), GetTestBytes(1), nil)\n\n\t\t\ttime.Sleep(1100 * time.Millisecond)\n\n\t\t\t// this entry will be deleted\n\t\t\ttxGet(t, db, bucket, GetTestBytes(0), nil, ErrKeyNotFound)\n\t\t\t// this entry still alive\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), GetTestBytes(1), nil)\n\n\t\t\ttime.Sleep(1 * time.Second)\n\n\t\t\t// this entry will be deleted\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), nil, ErrKeyNotFound)\n\n\t\t\tr, ok := db.Index.bTree.getWithDefault(1).Find(GetTestBytes(0))\n\t\t\trequire.Nil(t, r)\n\t\t\trequire.False(t, ok)\n\n\t\t\tr, ok = db.Index.bTree.getWithDefault(1).Find(GetTestBytes(1))\n\t\t\trequire.Nil(t, r)\n\t\t\trequire.False(t, ok)\n\t\t})\n\t})\n}\n\nfunc TestTx_GetMaxOrMinKey(t *testing.T) {\n\tbucket := \"bucket\"\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxPut(t, db, bucket, GetTestBytes(i), GetRandomBytes(24), Persistent, nil, nil)\n\t\t}\n\n\t\ttxGetMaxOrMinKey(t, db, bucket, true, GetTestBytes(9), nil)\n\t\ttxGetMaxOrMinKey(t, db, bucket, false, GetTestBytes(0), nil)\n\n\t\ttxDel(t, db, bucket, GetTestBytes(9), nil)\n\t\ttxDel(t, db, bucket, GetTestBytes(0), nil)\n\n\t\ttxGetMaxOrMinKey(t, db, bucket, true, GetTestBytes(8), nil)\n\t\ttxGetMaxOrMinKey(t, db, bucket, false, GetTestBytes(1), nil)\n\n\t\ttxPut(t, db, bucket, GetTestBytes(-1), GetRandomBytes(24), Persistent, nil, nil)\n\t\ttxPut(t, db, bucket, GetTestBytes(100), GetRandomBytes(24), Persistent, nil, nil)\n\n\t\ttxGetMaxOrMinKey(t, db, bucket, false, GetTestBytes(-1), nil)\n\t\ttxGetMaxOrMinKey(t, db, bucket, true, GetTestBytes(100), nil)\n\t})\n}\n\nfunc TestTx_updateOrPut(t *testing.T) {\n\tbucket := \"bucket\"\n\tt.Run(\"updateOrPut will return the expected error when it do update\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttxPut(t, db, bucket, []byte(\"any\"), []byte(\"any\"), Persistent, nil, nil)\n\n\t\t\ttx, err := db.Begin(true)\n\t\t\trequire.NoError(t, err)\n\t\t\texpectedErr := errors.New(\"some error\")\n\t\t\trequire.Equal(t, expectedErr, tx.updateOrPut(\"bucket\", []byte(\"any\"), []byte(\"any\"), func(bytes []byte) ([]byte, error) {\n\t\t\t\treturn nil, expectedErr\n\t\t\t}))\n\t\t\trequire.Equal(t, nil, tx.Commit())\n\t\t})\n\t})\n}\n\nfunc TestTx_IncrementAndDecrement(t *testing.T) {\n\tbucket := \"bucket\"\n\tgetIntegerValue := func(value int64) []byte {\n\t\treturn []byte(fmt.Sprintf(\"%d\", value))\n\t}\n\n\tkey := GetTestBytes(0)\n\n\tt.Run(\"increments and decrements on a non-exist key\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxIncrement(t, db, bucket, key, ErrKeyNotFound, nil)\n\t\t\ttxIncrementBy(t, db, bucket, key, 12, ErrKeyNotFound, nil)\n\t\t\ttxDecrement(t, db, bucket, key, ErrKeyNotFound, nil)\n\t\t\ttxDecrementBy(t, db, bucket, key, 12, ErrKeyNotFound, nil)\n\t\t})\n\t})\n\n\tt.Run(\"increments and decrements on a non-integer value\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, key, []byte(\"foo\"), Persistent, nil, nil)\n\n\t\t\ttxIncrement(t, db, bucket, key, ErrValueNotInteger, nil)\n\t\t\ttxIncrementBy(t, db, bucket, key, 12, ErrValueNotInteger, nil)\n\t\t\ttxDecrement(t, db, bucket, key, ErrValueNotInteger, nil)\n\t\t\ttxDecrementBy(t, db, bucket, key, 12, ErrValueNotInteger, nil)\n\t\t})\n\t})\n\n\tt.Run(\"increments and decrements normally\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, key, getIntegerValue(12), Persistent, nil, nil)\n\t\t\ttxIncrement(t, db, bucket, key, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, getIntegerValue(13), nil)\n\n\t\t\ttxDecrement(t, db, bucket, key, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, getIntegerValue(12), nil)\n\n\t\t\ttxIncrementBy(t, db, bucket, key, 12, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, getIntegerValue(24), nil)\n\n\t\t\ttxDecrementBy(t, db, bucket, key, 25, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, getIntegerValue(-1), nil)\n\t\t})\n\t})\n\n\tt.Run(\"increments and decrements over range of int64\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, key, []byte(\"9223372036854775818\"), Persistent, nil, nil)\n\t\t\ttxIncrement(t, db, bucket, key, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"9223372036854775819\"), nil)\n\n\t\t\ttxIncrementBy(t, db, bucket, key, 200, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"9223372036854776019\"), nil)\n\n\t\t\ttxDecrement(t, db, bucket, key, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"9223372036854776018\"), nil)\n\n\t\t\ttxDecrementBy(t, db, bucket, key, math.MaxInt64, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"211\"), nil)\n\n\t\t\ttxDecrementBy(t, db, bucket, key, math.MaxInt64, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"-9223372036854775596\"), nil)\n\t\t})\n\t})\n\n\tt.Run(\"increments and decrements on value which will overflow after operation\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, key, []byte(\"9223372036854775800\"), Persistent, nil, nil)\n\t\t\ttxIncrementBy(t, db, bucket, key, 100, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"9223372036854775900\"), nil)\n\n\t\t\ttxDecrementBy(t, db, bucket, key, math.MaxInt64, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"93\"), nil)\n\n\t\t\ttxDecrementBy(t, db, bucket, key, math.MaxInt64, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"-9223372036854775714\"), nil)\n\n\t\t\ttxDecrementBy(t, db, bucket, key, math.MaxInt64, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"-18446744073709551521\"), nil)\n\t\t})\n\t})\n\n\tt.Run(\"operations on expired key\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, key, []byte(\"1\"), 1, nil, nil)\n\n\t\t\ttime.Sleep(2 * time.Second)\n\n\t\t\ttxIncrement(t, db, bucket, key, ErrKeyNotFound, nil)\n\t\t\ttxIncrementBy(t, db, bucket, key, 12, ErrKeyNotFound, nil)\n\t\t\ttxDecrement(t, db, bucket, key, ErrKeyNotFound, nil)\n\t\t\ttxDecrementBy(t, db, bucket, key, 12, ErrKeyNotFound, nil)\n\t\t})\n\t})\n}\n\nfunc TestTx_PutIfNotExistsAndPutIfExists(t *testing.T) {\n\tbucket := \"bucket\"\n\tval := []byte(\"value\")\n\tupdated_val := []byte(\"updated_value\")\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxPutIfNotExists(t, db, bucket, GetTestBytes(i), val, nil, nil)\n\t\t}\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxGet(t, db, bucket, GetTestBytes(i), val, nil)\n\t\t}\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxPutIfExists(t, db, bucket, GetTestBytes(i), updated_val, nil, nil)\n\t\t}\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxGet(t, db, bucket, GetTestBytes(i), updated_val, nil)\n\t\t}\n\t})\n}\n\nfunc TestTx_GetAndSetBit(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\tt.Run(\"get bit on a non-exist key\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxGetBit(t, db, bucket, key, 0, 0, ErrKeyNotFound, nil)\n\t\t})\n\t})\n\n\tt.Run(\"set bit on a non-exist key\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxSetBit(t, db, bucket, key, 0, 1, nil, nil)\n\t\t\ttxGetBit(t, db, bucket, key, 0, 1, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"\\x01\"), nil)\n\n\t\t\ttxDel(t, db, bucket, key, nil)\n\t\t\ttxSetBit(t, db, bucket, key, 2, 1, nil, nil)\n\t\t\ttxGetBit(t, db, bucket, key, 2, 1, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"\\x00\\x00\\x01\"), nil)\n\n\t\t\ttxDel(t, db, bucket, key, nil)\n\t\t\ttxSetBit(t, db, bucket, key, 2, '1', nil, nil)\n\t\t\ttxGetBit(t, db, bucket, key, 2, '1', nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"\\x00\\x001\"), nil)\n\t\t})\n\t})\n\n\tt.Run(\"get and set bit on a exist string\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, key, []byte(\"foober\"), Persistent, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"foober\"), nil)\n\t\t\ttxGetBit(t, db, bucket, key, 0, 'f', nil, nil)\n\n\t\t\ttxSetBit(t, db, bucket, key, 0, 'F', nil, nil)\n\t\t\ttxGetBit(t, db, bucket, key, 0, 'F', nil, nil)\n\n\t\t\ttxSetBit(t, db, bucket, key, 3, 'B', nil, nil)\n\t\t\ttxGetBit(t, db, bucket, key, 3, 'B', nil, nil)\n\n\t\t\ttxSetBit(t, db, bucket, key, 5, 'R', nil, nil)\n\t\t\ttxGetBit(t, db, bucket, key, 5, 'R', nil, nil)\n\n\t\t\ttxSetBit(t, db, bucket, key, 6, 'A', nil, nil)\n\t\t\ttxGetBit(t, db, bucket, key, 6, 'A', nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"FooBeRA\"), nil)\n\n\t\t\ttxSetBit(t, db, bucket, key, 12, 'C', nil, nil)\n\t\t\ttxGetBit(t, db, bucket, key, 12, 'C', nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"FooBeRA\\x00\\x00\\x00\\x00\\x00C\"), nil)\n\t\t})\n\t})\n\n\tt.Run(\"give a invalid offset\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxSetBit(t, db, bucket, key, math.MaxInt64, 0, ErrOffsetInvalid, nil)\n\t\t\ttxGetBit(t, db, bucket, key, math.MaxInt64, 0, ErrOffsetInvalid, nil)\n\n\t\t\ttxSetBit(t, db, bucket, key, 0, 1, nil, nil)\n\t\t\ttxGetBit(t, db, bucket, key, 1, 0, nil, nil)\n\t\t})\n\t})\n}\n\nfunc TestTx_ValueLen(t *testing.T) {\n\tbucket := \"bucket\"\n\tval := []byte(\"value\")\n\n\tt.Run(\"match length\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(1), val, Persistent, nil, nil)\n\t\t\ttxValueLen(t, db, bucket, GetTestBytes(1), 5, nil)\n\t\t})\n\t})\n\n\tt.Run(\"bucket not exist\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxValueLen(t, db, bucket, GetTestBytes(1), 0, ErrBucketNotExist)\n\t\t})\n\t})\n\n\tt.Run(\"key not found\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(1), val, Persistent, nil, nil)\n\t\t\ttxValueLen(t, db, bucket, GetTestBytes(2), 0, ErrKeyNotFound)\n\t\t})\n\t})\n\n\tt.Run(\"expired test\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(1), val, 1, nil, nil)\n\t\t\ttime.Sleep(3 * time.Second)\n\t\t\ttxValueLen(t, db, bucket, GetTestBytes(1), 0, ErrKeyNotFound)\n\t\t})\n\t})\n}\n\nfunc TestTx_GetSet(t *testing.T) {\n\tbucket := \"bucket\"\n\tval := []byte(\"value\")\n\tnewVal := []byte(\"new_value\")\n\n\tt.Run(\"match value\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(1), val, Persistent, nil, nil)\n\t\t\ttxGetSet(t, db, bucket, GetTestBytes(1), newVal, val, nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), newVal, nil)\n\t\t})\n\t})\n\n\tt.Run(\"bucket not exist\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxGetSet(t, db, bucket, GetTestBytes(1), newVal, nil, ErrBucketNotExist)\n\t\t})\n\t})\n\n\tt.Run(\"expired test\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(1), val, 1, nil, nil)\n\t\t\ttime.Sleep(3 * time.Second)\n\t\t\ttxGetSet(t, db, bucket, GetTestBytes(1), newVal, nil, ErrKeyNotFound)\n\t\t})\n\t})\n}\n\nfunc TestTx_GetTTLAndPersist(t *testing.T) {\n\tbucket := \"bucket1\"\n\tkey := []byte(\"key1\")\n\tvalue := []byte(\"value1\")\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxGetTTL(t, db, bucket, key, 0, ErrBucketNotExist)\n\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\ttxGetTTL(t, db, bucket, key, 0, ErrKeyNotFound)\n\t\ttxPersist(t, db, bucket, key, ErrKeyNotFound)\n\n\t\ttxPut(t, db, bucket, key, value, 1, nil, nil)\n\t\ttime.Sleep(2 * time.Second) // Wait till value to expire\n\t\ttxGetTTL(t, db, bucket, key, 0, ErrKeyNotFound)\n\n\t\ttxPut(t, db, bucket, key, value, 100, nil, nil)\n\t\ttxGetTTL(t, db, bucket, key, 100, nil)\n\n\t\ttxPersist(t, db, bucket, key, nil)\n\t\ttxGetTTL(t, db, bucket, key, -1, nil)\n\t})\n}\n\nfunc TestTx_MSetMGet(t *testing.T) {\n\tbucket := \"bucket\"\n\n\tt.Run(\"use MSet and MGet with 0 args\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxMSet(t, db, bucket, nil, Persistent, nil, nil)\n\t\t\ttxMGet(t, db, bucket, nil, nil, nil, nil)\n\t\t})\n\t})\n\n\tt.Run(\"use MSet by using odd number of args \", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxMSet(t, db, bucket, [][]byte{\n\t\t\t\tGetTestBytes(0), GetTestBytes(1), GetTestBytes(2),\n\t\t\t}, Persistent, ErrKVArgsLenNotEven, nil)\n\t\t})\n\t})\n\n\tt.Run(\"use MSet and MGet normally\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxMSet(t, db, bucket, [][]byte{\n\t\t\t\tGetTestBytes(0), GetTestBytes(1), GetTestBytes(2), GetTestBytes(3),\n\t\t\t}, Persistent, nil, nil)\n\t\t\ttxMGet(t, db, bucket, [][]byte{\n\t\t\t\tGetTestBytes(0), GetTestBytes(2),\n\t\t\t}, [][]byte{\n\t\t\t\tGetTestBytes(1), GetTestBytes(3),\n\t\t\t}, nil, nil)\n\t\t})\n\t})\n}\n\nfunc TestTx_Append(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\tt.Run(\"use Append with a nil appendage\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxAppend(t, db, bucket, key, []byte(\"\"), nil, nil)\n\t\t\ttxAppend(t, db, bucket, key, nil, nil, nil)\n\t\t})\n\t})\n\n\tt.Run(\"use Append on a non-exist key\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxAppend(t, db, bucket, key, GetTestBytes(1), nil, nil)\n\t\t\ttxGet(t, db, bucket, key, GetTestBytes(1), nil)\n\t\t})\n\t})\n\n\tt.Run(\"use append on an exist key\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, key, []byte(\"test\"), Persistent, nil, nil)\n\t\t\ttxAppend(t, db, bucket, key, []byte(\"test\"), nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"testtest\"), nil)\n\t\t})\n\t})\n}\n\nfunc TestTx_GetRange(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\tt.Run(\"use GetRange with start greater than less\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxGetRange(t, db, bucket, key, 5, 1, nil, ErrStartGreaterThanEnd, nil)\n\t\t})\n\t})\n\n\tt.Run(\"use GetRange with start greater than size of value\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, key, []byte(\"test\"), Persistent, nil, nil)\n\t\t\ttxGetRange(t, db, bucket, key, 5, 10, nil, nil, nil)\n\t\t})\n\t})\n\n\tt.Run(\"use GetRange with end greater than size of value\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, key, []byte(\"test\"), Persistent, nil, nil)\n\t\t\ttxGetRange(t, db, bucket, key, 2, 10, []byte(\"st\"), nil, nil)\n\t\t})\n\t})\n\n\tt.Run(\"use GetRange normally\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, key, []byte(\"test\"), Persistent, nil, nil)\n\t\t\ttxGetRange(t, db, bucket, key, 1, 2, []byte(\"es\"), nil, nil)\n\t\t\ttxGetRange(t, db, bucket, key, 1, 3, []byte(\"est\"), nil, nil)\n\t\t\ttxGetRange(t, db, bucket, key, 1, 4, []byte(\"est\"), nil, nil)\n\t\t})\n\t})\n}\n\nfunc TestBTreeInternalVisibility(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\ttxPut(t, db, bucket, key, []byte(\"test\"), Persistent, nil, nil)\n\t\terr := db.Update(func(tx *Tx) error {\n\t\t\terr := tx.Put(bucket, key, []byte(\"test-updated\"), Persistent)\n\t\t\tassert.Nil(t, err)\n\t\t\tvalue, err := tx.Get(bucket, key)\n\t\t\tassert.Nil(t, err)\n\t\t\tassert.Equal(t, \"test-updated\", string(value))\n\t\t\terr = tx.DeleteBucket(DataStructureBTree, bucket)\n\t\t\tassert.Nil(t, err)\n\t\t\terr = tx.Put(bucket, key, []byte(\"test-updated\"), Persistent)\n\t\t\tassert.Equal(t, ErrBucketNotFound, err)\n\t\t\treturn nil\n\t\t})\n\t\tassert.Equal(t, ErrBucketNotExist, err)\n\t})\n}\n"
        },
        {
          "name": "tx_bucket.go",
          "type": "blob",
          "size": 3.326171875,
          "content": "// Copyright 2022 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\n// IterateBuckets iterate over all the bucket depends on ds (represents the data structure)\nfunc (tx *Tx) IterateBuckets(ds uint16, pattern string, f func(bucket string) bool) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\n\tif ds == DataStructureSet {\n\t\tfor bucketId := range tx.db.Index.set.idx {\n\t\t\tbucket, err := tx.db.bm.GetBucketById(uint64(bucketId))\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif end, err := MatchForRange(pattern, bucket.Name, f); end || err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\tif ds == DataStructureSortedSet {\n\t\tfor bucketId := range tx.db.Index.sortedSet.idx {\n\t\t\tbucket, err := tx.db.bm.GetBucketById(uint64(bucketId))\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif end, err := MatchForRange(pattern, bucket.Name, f); end || err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\tif ds == DataStructureList {\n\t\tfor bucketId := range tx.db.Index.list.idx {\n\t\t\tbucket, err := tx.db.bm.GetBucketById(uint64(bucketId))\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif end, err := MatchForRange(pattern, bucket.Name, f); end || err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\tif ds == DataStructureBTree {\n\t\tfor bucketId := range tx.db.Index.bTree.idx {\n\t\t\tbucket, err := tx.db.bm.GetBucketById(uint64(bucketId))\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif end, err := MatchForRange(pattern, bucket.Name, f); end || err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (tx *Tx) NewKVBucket(name string) error {\n\treturn tx.NewBucket(DataStructureBTree, name)\n}\n\nfunc (tx *Tx) NewListBucket(name string) error {\n\treturn tx.NewBucket(DataStructureList, name)\n}\n\nfunc (tx *Tx) NewSetBucket(name string) error {\n\treturn tx.NewBucket(DataStructureSet, name)\n}\n\nfunc (tx *Tx) NewSortSetBucket(name string) error {\n\treturn tx.NewBucket(DataStructureSortedSet, name)\n}\n\nfunc (tx *Tx) NewBucket(ds uint16, name string) (err error) {\n\tif tx.ExistBucket(ds, name) {\n\t\treturn ErrBucketAlreadyExist\n\t}\n\tbucket := &Bucket{\n\t\tMeta: &BucketMeta{\n\t\t\tOp: BucketInsertOperation,\n\t\t},\n\t\tId:   tx.db.bm.Gen.GenId(),\n\t\tDs:   ds,\n\t\tName: name,\n\t}\n\tif _, exist := tx.pendingBucketList[ds]; !exist {\n\t\ttx.pendingBucketList[ds] = map[BucketName]*Bucket{}\n\t}\n\ttx.pendingBucketList[ds][name] = bucket\n\treturn nil\n}\n\n// DeleteBucket delete bucket depends on ds (represents the data structure)\nfunc (tx *Tx) DeleteBucket(ds uint16, bucket string) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(ds, bucket)\n\tif err != nil {\n\t\treturn ErrBucketNotFound\n\t}\n\n\tdeleteBucket := &Bucket{\n\t\tMeta: &BucketMeta{\n\t\t\tOp: BucketDeleteOperation,\n\t\t},\n\t\tId:   b.Id,\n\t\tDs:   ds,\n\t\tName: bucket,\n\t}\n\n\treturn tx.putBucket(deleteBucket)\n}\n\nfunc (tx *Tx) ExistBucket(ds uint16, bucket string) bool {\n\treturn tx.db.bm.ExistBucket(ds, bucket)\n}\n"
        },
        {
          "name": "tx_bucket_test.go",
          "type": "blob",
          "size": 2.685546875,
          "content": "// Copyright 2022 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nvar (\n\tsetBucketName    = \"set_bucket\"\n\tzSetBucketName   = \"zset_bucket\"\n\tlistBucketName   = \"list_bucket\"\n\tstringBucketName = \"string_bucket\"\n)\n\nfunc setupBucket(t *testing.T, db *DB) {\n\tkey := GetTestBytes(0)\n\tval := GetTestBytes(1)\n\ttxCreateBucket(t, db, DataStructureBTree, stringBucketName, nil)\n\ttxCreateBucket(t, db, DataStructureSet, setBucketName, nil)\n\ttxCreateBucket(t, db, DataStructureSortedSet, zSetBucketName, nil)\n\ttxCreateBucket(t, db, DataStructureList, listBucketName, nil)\n\n\ttxSAdd(t, db, setBucketName, key, val, nil, nil)\n\ttxZAdd(t, db, zSetBucketName, key, val, 80, nil, nil)\n\ttxPush(t, db, listBucketName, key, val, true, nil, nil)\n\ttxPut(t, db, stringBucketName, key, val, Persistent, nil, nil)\n}\n\nfunc TestBucket_IterateBuckets(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tsetupBucket(t, db)\n\n\t\ttxIterateBuckets(t, db, DataStructureSet, \"*\", func(bucket string) bool {\n\t\t\treturn true\n\t\t}, nil, setBucketName)\n\n\t\ttxIterateBuckets(t, db, DataStructureSortedSet, \"*\", func(bucket string) bool {\n\t\t\treturn true\n\t\t}, nil, zSetBucketName)\n\n\t\ttxIterateBuckets(t, db, DataStructureList, \"*\", func(bucket string) bool {\n\t\t\treturn true\n\t\t}, nil, listBucketName)\n\n\t\ttxIterateBuckets(t, db, DataStructureBTree, \"*\", func(bucket string) bool {\n\t\t\treturn true\n\t\t}, nil, stringBucketName)\n\n\t\tmatched := false\n\t\ttxIterateBuckets(t, db, DataStructureBTree, \"str*\", func(bucket string) bool {\n\t\t\tmatched = true\n\t\t\treturn true\n\t\t}, nil)\n\t\tassert.Equal(t, true, matched)\n\n\t\tmatched = false\n\t\ttxIterateBuckets(t, db, DataStructureList, \"str*\", func(bucket string) bool {\n\t\t\treturn true\n\t\t}, nil)\n\t\tassert.Equal(t, false, matched)\n\t})\n}\n\nfunc TestBucket_DeleteBucket(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tsetupBucket(t, db)\n\n\t\ttxDeleteBucket(t, db, DataStructureSet, setBucketName, nil)\n\t\ttxDeleteBucket(t, db, DataStructureSortedSet, zSetBucketName, nil)\n\t\ttxDeleteBucket(t, db, DataStructureList, listBucketName, nil)\n\t\ttxDeleteBucket(t, db, DataStructureBTree, stringBucketName, nil)\n\n\t})\n}\n"
        },
        {
          "name": "tx_error.go",
          "type": "blob",
          "size": 2.537109375,
          "content": "package nutsdb\n\nimport \"errors\"\n\nvar (\n\t// ErrDataSizeExceed is returned when given key and value size is too big.\n\tErrDataSizeExceed = errors.New(\"data size too big\")\n\n\t// ErrTxClosed is returned when committing or rolling back a transaction\n\t// that has already been committed or rolled back.\n\tErrTxClosed = errors.New(\"tx is closed\")\n\n\t// ErrTxNotWritable is returned when performing a write operation on\n\t// a read-only transaction.\n\tErrTxNotWritable = errors.New(\"tx not writable\")\n\n\t// ErrKeyEmpty is returned if an empty key is passed on an update function.\n\tErrKeyEmpty = errors.New(\"key cannot be empty\")\n\n\t// ErrBucketEmpty is returned if bucket is empty.\n\tErrBucketEmpty = errors.New(\"bucket is empty\")\n\n\t// ErrRangeScan is returned when range scanning not found the result\n\tErrRangeScan = errors.New(\"range scans not found\")\n\n\t// ErrPrefixScan is returned when prefix scanning not found the result\n\tErrPrefixScan = errors.New(\"prefix scans not found\")\n\n\t// ErrPrefixSearchScan is returned when prefix and search scanning not found the result\n\tErrPrefixSearchScan = errors.New(\"prefix and search scans not found\")\n\n\t// ErrNotFoundKey is returned when key not found int the bucket on an view function.\n\tErrNotFoundKey = errors.New(\"key not found in the bucket\")\n\n\t// ErrCannotCommitAClosedTx is returned when the tx committing a closed tx\n\tErrCannotCommitAClosedTx = errors.New(\"can not commit a closed tx\")\n\n\t// ErrCannotRollbackACommittingTx is returned when the tx rollback a committing tx\n\tErrCannotRollbackACommittingTx = errors.New(\"can not rollback a committing tx\")\n\n\tErrCannotRollbackAClosedTx = errors.New(\"can not rollback a closed tx\")\n\n\t// ErrNotFoundBucket is returned when key not found int the bucket on an view function.\n\tErrNotFoundBucket = errors.New(\"bucket not found\")\n\n\t// ErrTxnTooBig is returned if too many writes are fit into a single transaction.\n\tErrTxnTooBig = errors.New(\"Txn is too big to fit into one request\")\n\n\t// ErrTxnExceedWriteLimit is returned when this tx's write is exceed max write record\n\tErrTxnExceedWriteLimit = errors.New(\"txn is exceed max write record count\")\n\n\tErrBucketAlreadyExist = errors.New(\"bucket is already exist\")\n\n\tErrorBucketNotExist = errors.New(\"bucket is not exist yet, please use NewBucket function to create this bucket first\")\n\n\tErrValueNotInteger = errors.New(\"value is not an integer\")\n\n\tErrOffsetInvalid = errors.New(\"offset is invalid\")\n\n\tErrKVArgsLenNotEven = errors.New(\"parameters is used to represent key value pairs and cannot be odd numbers\")\n\n\tErrStartGreaterThanEnd = errors.New(\"start is greater than end\")\n)\n"
        },
        {
          "name": "tx_list.go",
          "type": "blob",
          "size": 11.962890625,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"bytes\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/pkg/errors\"\n\t\"github.com/xujiajun/utils/strconv2\"\n)\n\nvar bucketKeySeqMap map[string]*HeadTailSeq\n\n// ErrSeparatorForListKey returns when list key contains the SeparatorForListKey.\nvar ErrSeparatorForListKey = errors.Errorf(\"contain separator (%s) for List key\", SeparatorForListKey)\n\n// SeparatorForListKey represents separator for listKey\nconst SeparatorForListKey = \"|\"\n\n// RPop removes and returns the last element of the list stored in the bucket at given bucket and key.\nfunc (tx *Tx) RPop(bucket string, key []byte) (item []byte, err error) {\n\titem, err = tx.RPeek(bucket, key)\n\tif err != nil {\n\t\treturn\n\t}\n\n\treturn item, tx.push(bucket, key, DataRPopFlag, item)\n}\n\n// RPeek returns the last element of the list stored in the bucket at given bucket and key.\nfunc (tx *Tx) RPeek(bucket string, key []byte) ([]byte, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar (\n\t\tbucketId = b.Id\n\t\tl        *List\n\t\texist    bool\n\t)\n\n\tif l, exist = tx.db.Index.list.exist(bucketId); !exist {\n\t\treturn nil, ErrBucket\n\t}\n\n\tif tx.CheckExpire(bucket, key) {\n\t\treturn nil, ErrListNotFound\n\t}\n\n\titem, err := l.RPeek(string(key))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tv, err := tx.db.getValueByRecord(item.record)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn v, nil\n}\n\n// push sets values for list stored in the bucket at given bucket, key, flag and values.\nfunc (tx *Tx) push(bucket string, key []byte, flag uint16, values ...[]byte) error {\n\tfor _, value := range values {\n\t\terr := tx.put(bucket, key, value, Persistent, flag, uint64(time.Now().Unix()), DataStructureList)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (tx *Tx) getListNewKey(bucket string, key []byte, isLeft bool) []byte {\n\tif bucketKeySeqMap == nil {\n\t\tbucketKeySeqMap = make(map[string]*HeadTailSeq)\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn nil\n\t}\n\tbucketId := b.Id\n\n\tbucketKey := bucket + string(key)\n\tif _, ok := bucketKeySeqMap[bucketKey]; !ok {\n\t\tbucketKeySeqMap[bucketKey] = tx.getListHeadTailSeq(bucketId, string(key))\n\t}\n\n\tseq := generateSeq(bucketKeySeqMap[bucketKey], isLeft)\n\treturn encodeListKey(key, seq)\n}\n\n// RPush inserts the values at the tail of the list stored in the bucket at given bucket,key and values.\nfunc (tx *Tx) RPush(bucket string, key []byte, values ...[]byte) error {\n\tif err := tx.isKeyValid(bucket, key); err != nil {\n\t\treturn err\n\t}\n\n\tif strings.Contains(string(key), SeparatorForListKey) {\n\t\treturn ErrSeparatorForListKey\n\t}\n\n\tfor _, value := range values {\n\t\tnewKey := tx.getListNewKey(bucket, key, false)\n\t\terr := tx.push(bucket, newKey, DataLPushFlag, value)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// LPush inserts the values at the head of the list stored in the bucket at given bucket,key and values.\nfunc (tx *Tx) LPush(bucket string, key []byte, values ...[]byte) error {\n\tif err := tx.isKeyValid(bucket, key); err != nil {\n\t\treturn err\n\t}\n\n\tif strings.Contains(string(key), SeparatorForListKey) {\n\t\treturn ErrSeparatorForListKey\n\t}\n\n\tfor _, value := range values {\n\t\tnewKey := tx.getListNewKey(bucket, key, true)\n\t\terr := tx.push(bucket, newKey, DataLPushFlag, value)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (tx *Tx) isKeyValid(bucket string, key []byte) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\n\tif tx.CheckExpire(bucket, key) {\n\t\treturn ErrListNotFound\n\t}\n\n\treturn nil\n}\n\nfunc (tx *Tx) LPushRaw(bucket string, key []byte, values ...[]byte) error {\n\tif err := tx.isKeyValid(bucket, key); err != nil {\n\t\treturn err\n\t}\n\n\treturn tx.push(bucket, key, DataLPushFlag, values...)\n}\n\nfunc (tx *Tx) RPushRaw(bucket string, key []byte, values ...[]byte) error {\n\tif err := tx.isKeyValid(bucket, key); err != nil {\n\t\treturn err\n\t}\n\n\treturn tx.push(bucket, key, DataRPushFlag, values...)\n}\n\n// LPop removes and returns the first element of the list stored in the bucket at given bucket and key.\nfunc (tx *Tx) LPop(bucket string, key []byte) (item []byte, err error) {\n\titem, err = tx.LPeek(bucket, key)\n\tif err != nil {\n\t\treturn\n\t}\n\n\treturn item, tx.push(bucket, key, DataLPopFlag, item)\n}\n\n// LPeek returns the first element of the list stored in the bucket at given bucket and key.\nfunc (tx *Tx) LPeek(bucket string, key []byte) (item []byte, err error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar (\n\t\tbucketId = b.Id\n\t\tl        *List\n\t\texist    bool\n\t)\n\n\tif l, exist = tx.db.Index.list.exist(bucketId); !exist {\n\t\treturn nil, ErrBucket\n\t}\n\tif tx.CheckExpire(bucket, key) {\n\t\treturn nil, ErrListNotFound\n\t}\n\tr, err := l.LPeek(string(key))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tv, err := tx.db.getValueByRecord(r.record)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn v, nil\n}\n\n// LSize returns the size of key in the bucket in the bucket at given bucket and key.\nfunc (tx *Tx) LSize(bucket string, key []byte) (int, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn 0, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tvar (\n\t\tbucketId = b.Id\n\t\tl        *List\n\t\texist    bool\n\t)\n\n\tif l, exist = tx.db.Index.list.exist(bucketId); !exist {\n\t\treturn 0, ErrBucket\n\t}\n\tif tx.CheckExpire(bucket, key) {\n\t\treturn 0, ErrListNotFound\n\t}\n\treturn l.Size(string(key))\n}\n\n// LRange returns the specified elements of the list stored in the bucket at given bucket,key, start and end.\n// The offsets start and stop are zero-based indexes 0 being the first element of the list (the head of the list),\n// 1 being the next element and so on.\n// Start and end can also be negative numbers indicating offsets from the end of the list,\n// where -1 is the last element of the list, -2 the penultimate element and so on.\nfunc (tx *Tx) LRange(bucket string, key []byte, start, end int) ([][]byte, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar (\n\t\tbucketId = b.Id\n\t\tl        *List\n\t\texist    bool\n\t)\n\n\tif l, exist = tx.db.Index.list.exist(bucketId); !exist {\n\t\treturn nil, ErrBucket\n\t}\n\tif tx.CheckExpire(bucket, key) {\n\t\treturn nil, ErrListNotFound\n\t}\n\n\trecords, err := l.LRange(string(key), start, end)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvalues := make([][]byte, len(records))\n\n\tfor i, r := range records {\n\t\tvalue, err := tx.db.getValueByRecord(r)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tvalues[i] = value\n\t}\n\n\treturn values, nil\n}\n\n// LRem removes the first count occurrences of elements equal to value from the list stored in the bucket at given bucket,key,count.\n// The count argument influences the operation in the following ways:\n// count > 0: Remove elements equal to value moving from head to tail.\n// count < 0: Remove elements equal to value moving from tail to head.\n// count = 0: Remove all elements equal to value.\nfunc (tx *Tx) LRem(bucket string, key []byte, count int, value []byte) error {\n\tvar (\n\t\tbuffer bytes.Buffer\n\t\tsize   int\n\t)\n\tsize, err := tx.LSize(bucket, key)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif count > size || count < -size {\n\t\treturn ErrCount\n\t}\n\n\tbuffer.Write([]byte(strconv2.IntToStr(count)))\n\tbuffer.Write([]byte(SeparatorForListKey))\n\tbuffer.Write(value)\n\tnewValue := buffer.Bytes()\n\n\terr = tx.push(bucket, key, DataLRemFlag, newValue)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// LTrim trims an existing list so that it will contain only the specified range of elements specified.\n// the offsets start and stop are zero-based indexes 0 being the first element of the list (the head of the list),\n// 1 being the next element and so on.\n// start and end can also be negative numbers indicating offsets from the end of the list,\n// where -1 is the last element of the list, -2 the penultimate element and so on.\nfunc (tx *Tx) LTrim(bucket string, key []byte, start, end int) error {\n\tvar (\n\t\terr    error\n\t\tbuffer bytes.Buffer\n\t)\n\n\tif err = tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvar (\n\t\tbucketId = b.Id\n\t\tl        *List\n\t\texist    bool\n\t)\n\n\tif l, exist = tx.db.Index.list.exist(bucketId); !exist {\n\t\treturn ErrBucket\n\t}\n\n\tif tx.CheckExpire(bucket, key) {\n\t\treturn ErrListNotFound\n\t}\n\tif _, ok := l.Items[string(key)]; !ok {\n\t\treturn ErrListNotFound\n\t}\n\n\tif _, err := tx.LRange(bucket, key, start, end); err != nil {\n\t\treturn err\n\t}\n\n\tbuffer.Write(key)\n\tbuffer.Write([]byte(SeparatorForListKey))\n\tbuffer.Write([]byte(strconv2.IntToStr(start)))\n\tnewKey := buffer.Bytes()\n\n\treturn tx.push(bucket, newKey, DataLTrimFlag, []byte(strconv2.IntToStr(end)))\n}\n\n// LRemByIndex remove the list element at specified index\nfunc (tx *Tx) LRemByIndex(bucket string, key []byte, indexes ...int) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbucketId := b.Id\n\tif _, ok := tx.db.Index.list.exist(bucketId); !ok {\n\t\treturn ErrListNotFound\n\t}\n\n\tif tx.CheckExpire(bucket, key) {\n\t\treturn ErrListNotFound\n\t}\n\n\tif len(indexes) == 0 {\n\t\treturn nil\n\t}\n\n\tsort.Ints(indexes)\n\tdata, err := MarshalInts(indexes)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = tx.push(bucket, key, DataLRemByIndex, data)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// LKeys find all keys matching a given pattern\nfunc (tx *Tx) LKeys(bucket, pattern string, f func(key string) bool) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\tvar (\n\t\tbucketId = b.Id\n\t\tl        *List\n\t\texist    bool\n\t)\n\tif l, exist = tx.db.Index.list.exist(bucketId); !exist {\n\t\treturn ErrBucket\n\t}\n\n\tfor key := range l.Items {\n\t\tif tx.CheckExpire(bucket, []byte(key)) {\n\t\t\tcontinue\n\t\t}\n\t\tif end, err := MatchForRange(pattern, key, f); end || err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (tx *Tx) ExpireList(bucket string, key []byte, ttl uint32) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvar (\n\t\tbucketId = b.Id\n\t\tl        *List\n\t\texist    bool\n\t)\n\n\tif l, exist = tx.db.Index.list.exist(bucketId); !exist {\n\t\treturn ErrBucket\n\t}\n\n\tl.TTL[string(key)] = ttl\n\tl.TimeStamp[string(key)] = uint64(time.Now().Unix())\n\tttls := strconv2.Int64ToStr(int64(ttl))\n\terr = tx.push(bucket, key, DataExpireListFlag, []byte(ttls))\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc (tx *Tx) CheckExpire(bucket string, key []byte) bool {\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn false\n\t}\n\n\tvar (\n\t\tbucketId = b.Id\n\t\tl        *List\n\t\texist    bool\n\t)\n\n\tif l, exist = tx.db.Index.list.exist(bucketId); !exist {\n\t\treturn false\n\t}\n\n\tif l.IsExpire(string(key)) {\n\t\t_ = tx.push(bucket, key, DataDeleteFlag)\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (tx *Tx) GetListTTL(bucket string, key []byte) (uint32, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn 0, err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tvar (\n\t\tbucketId = b.Id\n\t\tl        *List\n\t\texist    bool\n\t)\n\tif l, exist = tx.db.Index.list.exist(bucketId); !exist {\n\t\treturn 0, ErrBucket\n\t}\n\treturn l.GetListTTL(string(key))\n}\n"
        },
        {
          "name": "tx_list_test.go",
          "type": "blob",
          "size": 16.78125,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc pushDataByStartEnd(t *testing.T, db *DB, bucket string, key int, start, end int, isLeft bool) {\n\tfor i := start; i <= end; i++ {\n\t\ttxPush(t, db, bucket, GetTestBytes(key), GetTestBytes(i), isLeft, nil, nil)\n\t}\n}\n\nfunc pushDataByValues(t *testing.T, db *DB, bucket string, key int, isLeft bool, values ...int) {\n\tfor _, v := range values {\n\t\ttxPush(t, db, bucket, GetTestBytes(key), GetTestBytes(v), isLeft, nil, nil)\n\t}\n}\n\nfunc TestTx_RPush(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// 1. Insert values for some keys by using RPush\n\t// 2. Validate values for these keys by using RPop\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 9, false)\n\t\tpushDataByStartEnd(t, db, bucket, 1, 10, 19, false)\n\t\tpushDataByStartEnd(t, db, bucket, 2, 20, 29, false)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(9-i), nil, false)\n\t\t}\n\t\tfor i := 10; i < 20; i++ {\n\t\t\ttxPop(t, db, bucket, GetTestBytes(1), GetTestBytes(29-i), nil, false)\n\t\t}\n\t\tfor i := 20; i < 30; i++ {\n\t\t\ttxPop(t, db, bucket, GetTestBytes(2), GetTestBytes(49-i), nil, false)\n\t\t}\n\t})\n}\n\nfunc TestTx_MPush(t *testing.T) {\n\tbucket := \"bucket\"\n\tt.Run(\"Test Multiple LPush \", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\tbbs := make([][]byte, 0)\n\t\t\tbbs = append(bbs, GetTestBytes(2))\n\t\t\tbbs = append(bbs, GetTestBytes(3))\n\t\t\tbbs = append(bbs, GetTestBytes(4))\n\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\t\ttxMPush(t, db, bucket, GetTestBytes(1), bbs, true, nil, nil)\n\n\t\t\texpect := make([][]byte, 0)\n\t\t\tfor i := len(bbs) - 1; i >= 0; i-- {\n\t\t\t\texpect = append(expect, bbs[i])\n\t\t\t}\n\n\t\t\ttxLRange(t, db, bucket, GetTestBytes(1), 0, 2, 3, expect, nil)\n\t\t})\n\t})\n\n\tt.Run(\"Test Error LPush \", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\tbbs := make([][]byte, 0)\n\t\t\tbbs = append(bbs, GetTestBytes(2))\n\t\t\tbbs = append(bbs, GetTestBytes(3))\n\t\t\tbbs = append(bbs, GetTestBytes(4))\n\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\t\ttxMPush(t, db, \"test1\", GetTestBytes(1), bbs, true, ErrorBucketNotExist, nil)\n\t\t})\n\t})\n\n\tt.Run(\"Test Multiple RPush \", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\tbbs := make([][]byte, 0)\n\t\t\tbbs = append(bbs, GetTestBytes(2))\n\t\t\tbbs = append(bbs, GetTestBytes(3))\n\t\t\tbbs = append(bbs, GetTestBytes(4))\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\t\ttxMPush(t, db, bucket, GetTestBytes(1), bbs, false, nil, nil)\n\t\t\ttxLRange(t, db, bucket, GetTestBytes(1), 0, 2, 3, bbs, nil)\n\t\t})\n\t})\n\n\tt.Run(\"Test Error RPush \", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\tbbs := make([][]byte, 0)\n\t\t\tbbs = append(bbs, GetTestBytes(2))\n\t\t\tbbs = append(bbs, GetTestBytes(3))\n\t\t\tbbs = append(bbs, GetTestBytes(4))\n\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\t\ttxMPush(t, db, \"test1\", GetTestBytes(1), bbs, false, ErrorBucketNotExist, nil)\n\t\t})\n\t})\n}\n\nfunc TestTx_LPush(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// 1. Insert values for some keys by using LPush\n\t// 2. Validate values for these keys by using LPop\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 9, true)\n\t\tpushDataByStartEnd(t, db, bucket, 1, 10, 19, true)\n\t\tpushDataByStartEnd(t, db, bucket, 2, 20, 29, true)\n\n\t\ttxPush(t, db, bucket, []byte(\"012|sas\"), GetTestBytes(0), true, ErrSeparatorForListKey, nil)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(9-i), nil, true)\n\t\t}\n\t\tfor i := 10; i < 20; i++ {\n\t\t\ttxPop(t, db, bucket, GetTestBytes(1), GetTestBytes(29-i), nil, true)\n\t\t}\n\t\tfor i := 20; i < 30; i++ {\n\t\t\ttxPop(t, db, bucket, GetTestBytes(2), GetTestBytes(49-i), nil, true)\n\t\t}\n\t})\n}\n\nfunc TestTx_LPushRaw(t *testing.T) {\n\tbucket := \"bucket\"\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\n\t\tseq := uint64(100000)\n\t\tfor i := 0; i <= 100; i++ {\n\t\t\tkey := encodeListKey([]byte(\"0\"), seq)\n\t\t\tseq--\n\t\t\ttxPushRaw(t, db, bucket, key, GetTestBytes(i), true, nil, nil)\n\t\t}\n\n\t\tfor i := 0; i <= 100; i++ {\n\t\t\tv := GetTestBytes(100 - i)\n\t\t\ttxPop(t, db, bucket, []byte(\"0\"), v, nil, true)\n\t\t}\n\t})\n}\n\nfunc TestTx_RPushRaw(t *testing.T) {\n\tbucket := \"bucket\"\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tseq := uint64(100000)\n\t\tfor i := 0; i <= 100; i++ {\n\t\t\tkey := encodeListKey([]byte(\"0\"), seq)\n\t\t\tseq++\n\t\t\ttxPushRaw(t, db, bucket, key, GetTestBytes(i), false, nil, nil)\n\t\t}\n\n\t\ttxPush(t, db, bucket, []byte(\"012|sas\"), GetTestBytes(0), false, ErrSeparatorForListKey, nil)\n\n\t\tfor i := 0; i <= 100; i++ {\n\t\t\tv := GetTestBytes(100 - i)\n\t\t\ttxPop(t, db, bucket, []byte(\"0\"), v, nil, false)\n\t\t}\n\t})\n}\n\nfunc TestTx_LPop(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// Calling LPop on a non-existent list\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\ttxPop(t, db, bucket, GetTestBytes(0), nil, ErrListNotFound, true)\n\t})\n\n\t// Insert some values for a key and validate them by using LPop\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 2, true)\n\t\tfor i := 0; i < 3; i++ {\n\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(2-i), nil, true)\n\t\t}\n\t})\n}\n\nfunc TestTx_RPop(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// Calling RPop on a non-existent list\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\ttxPop(t, db, bucket, GetTestBytes(0), nil, ErrListNotFound, false)\n\t})\n\n\t// Calling RPop on a list with added data\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 2, false)\n\n\t\ttxPop(t, db, \"fake_bucket\", GetTestBytes(0), nil, ErrBucketNotExist, false)\n\n\t\tfor i := 0; i < 3; i++ {\n\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(2-i), nil, false)\n\t\t}\n\n\t\ttxPop(t, db, bucket, GetTestBytes(0), nil, ErrEmptyList, false)\n\t})\n}\n\nfunc TestTx_LRange(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// Calling LRange on a non-existent list\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\n\t\ttxLRange(t, db, bucket, GetTestBytes(0), 0, -1, 0, nil, ErrListNotFound)\n\t})\n\n\t// Calling LRange on a list with added data\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 2, true)\n\n\t\ttxLRange(t, db, bucket, GetTestBytes(0), 0, -1, 3, [][]byte{\n\t\t\tGetTestBytes(2), GetTestBytes(1), GetTestBytes(0),\n\t\t}, nil)\n\n\t\tfor i := 0; i < 3; i++ {\n\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(2-i), nil, true)\n\t\t}\n\n\t\ttxLRange(t, db, bucket, GetTestBytes(0), 0, -1, 0, nil, nil)\n\t})\n}\n\nfunc TestTx_LRem(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// Calling LRem on a non-existent list\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\ttxLRem(t, db, bucket, GetTestBytes(0), 1, GetTestBytes(0), ErrListNotFound)\n\t})\n\n\t// A basic calling for LRem\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 3, true)\n\n\t\ttxLRem(t, db, bucket, GetTestBytes(0), 1, GetTestBytes(0), nil)\n\t\ttxLRange(t, db, bucket, GetTestBytes(0), 0, -1, 3, [][]byte{\n\t\t\tGetTestBytes(3), GetTestBytes(2), GetTestBytes(1),\n\t\t}, nil)\n\t\ttxLRem(t, db, bucket, GetTestBytes(0), 4, GetTestBytes(0), ErrCount)\n\t\ttxLRem(t, db, bucket, GetTestBytes(0), 1, GetTestBytes(1), nil)\n\t})\n\n\t// Calling LRem with count > 0\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\n\t\tcount := 3\n\n\t\tpushDataByValues(t, db, bucket, 1, true, 0, 1, 0, 1, 0, 1, 0, 1)\n\n\t\ttxLRange(t, db, bucket, GetTestBytes(1), 0, -1, 8, [][]byte{\n\t\t\tGetTestBytes(1), GetTestBytes(0), GetTestBytes(1), GetTestBytes(0),\n\t\t\tGetTestBytes(1), GetTestBytes(0), GetTestBytes(1), GetTestBytes(0),\n\t\t}, nil)\n\t\ttxLRem(t, db, bucket, GetTestBytes(1), count, GetTestBytes(0), nil)\n\t\ttxLRange(t, db, bucket, GetTestBytes(1), 0, -1, 5, [][]byte{\n\t\t\tGetTestBytes(1), GetTestBytes(1), GetTestBytes(1), GetTestBytes(1), GetTestBytes(0),\n\t\t}, nil)\n\t})\n\n\t// Calling LRem with count == 0\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tcount := 0\n\n\t\tpushDataByValues(t, db, bucket, 1, true, 0, 1, 0, 1, 0, 1, 0, 1)\n\n\t\ttxLRange(t, db, bucket, GetTestBytes(1), 0, -1, 8, [][]byte{\n\t\t\tGetTestBytes(1), GetTestBytes(0), GetTestBytes(1), GetTestBytes(0),\n\t\t\tGetTestBytes(1), GetTestBytes(0), GetTestBytes(1), GetTestBytes(0),\n\t\t}, nil)\n\t\ttxLRem(t, db, bucket, GetTestBytes(1), count, GetTestBytes(0), nil)\n\t\ttxLRange(t, db, bucket, GetTestBytes(1), 0, -1, 4, [][]byte{\n\t\t\tGetTestBytes(1), GetTestBytes(1), GetTestBytes(1), GetTestBytes(1),\n\t\t}, nil)\n\t})\n\n\t// Calling LRem with count < 0\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\n\t\tcount := -3\n\n\t\tpushDataByValues(t, db, bucket, 1, true, 0, 1, 0, 1, 0, 1, 0, 1)\n\n\t\ttxLRange(t, db, bucket, GetTestBytes(1), 0, -1, 8, [][]byte{\n\t\t\tGetTestBytes(1), GetTestBytes(0), GetTestBytes(1), GetTestBytes(0),\n\t\t\tGetTestBytes(1), GetTestBytes(0), GetTestBytes(1), GetTestBytes(0),\n\t\t}, nil)\n\t\ttxLRem(t, db, bucket, GetTestBytes(1), count, GetTestBytes(0), nil)\n\t\ttxLRange(t, db, bucket, GetTestBytes(1), 0, -1, 5, [][]byte{\n\t\t\tGetTestBytes(1), GetTestBytes(0), GetTestBytes(1), GetTestBytes(1), GetTestBytes(1),\n\t\t}, nil)\n\t})\n}\n\nfunc TestTx_LTrim(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// Calling LTrim on a non-existent list\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\ttxLTrim(t, db, bucket, GetTestBytes(0), 0, 1, ErrListNotFound)\n\t})\n\n\t// Calling LTrim on a list with added data and use LRange to validate it\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 2, true)\n\t\ttxLTrim(t, db, bucket, GetTestBytes(0), 0, 1, nil)\n\n\t\ttxLRange(t, db, bucket, GetTestBytes(0), 0, -1, 2, [][]byte{\n\t\t\tGetTestBytes(2), GetTestBytes(1),\n\t\t}, nil)\n\t})\n\n\t// Calling LTrim with incorrect start and end\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\n\t\tfor i := 0; i < 3; i++ {\n\t\t\ttxPush(t, db, bucket, GetTestBytes(2), GetTestBytes(i), true, nil, nil)\n\t\t}\n\t\ttxLTrim(t, db, bucket, GetTestBytes(2), 0, -10, ErrStartOrEnd)\n\t})\n}\n\nfunc TestTx_LSize(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// Calling LSize on a non-existent list\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\ttxLSize(t, db, bucket, GetTestBytes(0), 0, ErrListNotFound)\n\t})\n\n\t// Calling LSize after adding some values\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 2, false)\n\t\ttxLSize(t, db, bucket, GetTestBytes(0), 3, nil)\n\t})\n}\n\nfunc TestTx_LRemByIndex(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// Calling LRemByIndex on a newly created empty list\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\ttxLRemByIndex(t, db, bucket, GetTestBytes(0), nil)\n\t})\n\n\t// Calling LRemByIndex with len(indexes) == 0\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByValues(t, db, bucket, 0, true, 0)\n\t\ttxLRemByIndex(t, db, bucket, GetTestBytes(0), nil)\n\t})\n\n\t// Calling LRemByIndex with a expired bucket name\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByValues(t, db, bucket, 0, true, 0)\n\t\ttxExpireList(t, db, bucket, GetTestBytes(0), 1, nil)\n\t\ttime.Sleep(3 * time.Second)\n\t\ttxLRemByIndex(t, db, bucket, GetTestBytes(0), ErrListNotFound)\n\t})\n\n\t// Calling LRemByIndex on a list with added data and use LRange to validate it\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 2, false)\n\t\ttxLRemByIndex(t, db, bucket, GetTestBytes(0), nil, 1, 0, 8, -8, 88, -88)\n\t\ttxLRange(t, db, bucket, GetTestBytes(0), 0, -1, 1, [][]byte{\n\t\t\tGetTestBytes(2),\n\t\t}, nil)\n\t})\n}\n\nfunc TestTx_ExpireList(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// Verify that the list with expiration time expires normally\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 3, false)\n\t\ttxLRange(t, db, bucket, GetTestBytes(0), 0, -1, 4, [][]byte{\n\t\t\tGetTestBytes(0), GetTestBytes(1), GetTestBytes(2), GetTestBytes(3),\n\t\t}, nil)\n\n\t\ttxExpireList(t, db, bucket, GetTestBytes(0), 1, nil)\n\t\ttime.Sleep(time.Second)\n\t\ttxLRange(t, db, bucket, GetTestBytes(0), 0, -1, 0, nil, ErrListNotFound)\n\t})\n\n\t// Verify that the list with persistent time\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 3, false)\n\t\ttxExpireList(t, db, bucket, GetTestBytes(0), Persistent, nil)\n\t\ttime.Sleep(time.Second)\n\t\ttxLRange(t, db, bucket, GetTestBytes(0), 0, -1, 4, [][]byte{\n\t\t\tGetTestBytes(0), GetTestBytes(1), GetTestBytes(2), GetTestBytes(3),\n\t\t}, nil)\n\t})\n}\n\nfunc TestTx_LKeys(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// Calling LKeys after adding some keys\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByValues(t, db, bucket, 10, false, 0)\n\t\tpushDataByValues(t, db, bucket, 11, false, 1)\n\t\tpushDataByValues(t, db, bucket, 12, false, 2)\n\t\tpushDataByValues(t, db, bucket, 23, false, 3)\n\n\t\ttxLKeys(t, db, bucket, \"*\", 4, nil, func(keys []string) bool {\n\t\t\treturn true\n\t\t})\n\n\t\ttxLKeys(t, db, bucket, \"*\", 2, nil, func(keys []string) bool {\n\t\t\treturn len(keys) != 2\n\t\t})\n\n\t\ttxLKeys(t, db, bucket, \"nutsdb-00000001*\", 3, nil, func(keys []string) bool {\n\t\t\treturn true\n\t\t})\n\t})\n}\n\nfunc TestTx_GetListTTL(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// Verify TTL of list\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 3, false)\n\n\t\ttxGetListTTL(t, db, bucket, GetTestBytes(0), uint32(0), nil)\n\t\ttxExpireList(t, db, bucket, GetTestBytes(0), uint32(1), nil)\n\t\ttxGetListTTL(t, db, bucket, GetTestBytes(0), uint32(1), nil)\n\n\t\ttime.Sleep(3 * time.Second)\n\t\ttxGetListTTL(t, db, bucket, GetTestBytes(0), uint32(0), ErrListNotFound)\n\t})\n}\n\nfunc TestTx_ListEntryIdxMode_HintKeyValAndRAMIdxMode(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\topts := DefaultOptions\n\topts.EntryIdxMode = HintKeyValAndRAMIdxMode\n\n\t// HintKeyValAndRAMIdxMode\n\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\terr := db.Update(func(tx *Tx) error {\n\t\t\terr := tx.LPush(bucket, key, []byte(\"d\"), []byte(\"c\"), []byte(\"b\"), []byte(\"a\"))\n\t\t\trequire.NoError(t, err)\n\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\tlistIdx := db.Index.list.getWithDefault(1)\n\t\titem, ok := listIdx.Items[string(key)].PopMin()\n\t\tr := item.record\n\t\trequire.True(t, ok)\n\t\trequire.NotNil(t, r.Value)\n\t\trequire.Equal(t, []byte(\"a\"), r.Value)\n\t})\n}\n\nfunc TestTx_ListEntryIdxMode_HintKeyAndRAMIdxMode(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\topts := &DefaultOptions\n\topts.EntryIdxMode = HintKeyAndRAMIdxMode\n\n\t// HintKeyAndRAMIdxMode\n\trunNutsDBTest(t, opts, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\terr := db.Update(func(tx *Tx) error {\n\t\t\terr := tx.LPush(bucket, key, []byte(\"d\"), []byte(\"c\"), []byte(\"b\"), []byte(\"a\"))\n\t\t\trequire.NoError(t, err)\n\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\tlistIdx := db.Index.list.getWithDefault(1)\n\t\titem, ok := listIdx.Items[string(key)].PopMin()\n\t\tr := item.record\n\t\trequire.True(t, ok)\n\t\trequire.Nil(t, r.Value)\n\n\t\tval, err := db.getValueByRecord(r)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []byte(\"a\"), val)\n\t})\n}\n"
        },
        {
          "name": "tx_set.go",
          "type": "blob",
          "size": 12.517578125,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"time\"\n\n\t\"github.com/pkg/errors\"\n)\n\nfunc (tx *Tx) sPut(bucket string, key []byte, dataFlag uint16, values ...[]byte) error {\n\n\tif dataFlag == DataSetFlag {\n\n\t\tfilter := make(map[uint32]struct{})\n\n\t\tb1, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tbucketId := b1.Id\n\n\t\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\n\t\t\tif _, ok := set.M[string(key)]; ok {\n\t\t\t\tfor hash := range set.M[string(key)] {\n\t\t\t\t\tfilter[hash] = struct{}{}\n\t\t\t\t}\n\t\t\t}\n\n\t\t}\n\n\t\tfor _, value := range values {\n\t\t\thash, err := getFnv32(value)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif _, ok := filter[hash]; !ok {\n\t\t\t\tfilter[hash] = struct{}{}\n\t\t\t\terr := tx.put(bucket, key, value, Persistent, dataFlag, uint64(time.Now().Unix()), DataStructureSet)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t} else {\n\t\tfor _, value := range values {\n\n\t\t\terr := tx.put(bucket, key, value, Persistent, dataFlag, uint64(time.Now().Unix()), DataStructureSet)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// SAdd adds the specified members to the set stored int the bucket at given bucket,key and items.\nfunc (tx *Tx) SAdd(bucket string, key []byte, items ...[]byte) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\treturn tx.sPut(bucket, key, DataSetFlag, items...)\n}\n\n// SRem removes the specified members from the set stored int the bucket at given bucket,key and items.\nfunc (tx *Tx) SRem(bucket string, key []byte, items ...[]byte) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbucketId := b.Id\n\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\t\tok, err := set.SAreMembers(string(key), items...)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif !ok {\n\t\t\treturn ErrSetMemberNotExist\n\t\t}\n\t\treturn tx.sPut(bucket, key, DataDeleteFlag, items...)\n\t}\n\treturn ErrBucketNotFound\n}\n\n// SAreMembers returns if the specified members are the member of the set int the bucket at given bucket,key and items.\nfunc (tx *Tx) SAreMembers(bucket string, key []byte, items ...[]byte) (bool, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn false, err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tbucketId := b.Id\n\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\t\treturn set.SAreMembers(string(key), items...)\n\t}\n\n\treturn false, ErrBucketNotFound\n}\n\n// SIsMember returns if member is a member of the set stored int the bucket at given bucket,key and item.\nfunc (tx *Tx) SIsMember(bucket string, key, item []byte) (bool, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn false, err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tbucketId := b.Id\n\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\t\tisMember, err := set.SIsMember(string(key), item)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\t\treturn isMember, nil\n\t}\n\n\treturn false, ErrBucketNotFound\n}\n\n// SMembers returns all the members of the set value stored int the bucket at given bucket and key.\nfunc (tx *Tx) SMembers(bucket string, key []byte) ([][]byte, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId := b.Id\n\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\t\titems, err := set.SMembers(string(key))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tvalues := make([][]byte, len(items))\n\t\tfor i, item := range items {\n\t\t\tvalue, err := tx.db.getValueByRecord(item)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tvalues[i] = value\n\t\t}\n\n\t\treturn values, nil\n\t}\n\n\treturn nil, ErrBucketNotFound\n\n}\n\n// SHasKey returns if the set in the bucket at given bucket and key.\nfunc (tx *Tx) SHasKey(bucket string, key []byte) (bool, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn false, err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tbucketId := b.Id\n\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\t\treturn set.SHasKey(string(key)), nil\n\t}\n\n\treturn false, ErrBucketNotFound\n}\n\n// SPop removes and returns one or more random elements from the set value store in the bucket at given bucket and key.\nfunc (tx *Tx) SPop(bucket string, key []byte) ([]byte, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId := b.Id\n\n\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\t\tfor _, items := range set.M[string(key)] {\n\t\t\tvalue, err := tx.db.getValueByRecord(items)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\terr = tx.sPut(bucket, key, DataDeleteFlag, value)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\treturn value, err\n\t\t}\n\t}\n\n\treturn nil, ErrBucketNotFound\n}\n\n// SCard returns the set cardinality (number of elements) of the set stored in the bucket at given bucket and key.\nfunc (tx *Tx) SCard(bucket string, key []byte) (int, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn 0, err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tbucketId := b.Id\n\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\t\treturn set.SCard(string(key)), nil\n\t}\n\n\treturn 0, ErrBucketNotFound\n}\n\n// SDiffByOneBucket returns the members of the set resulting from the difference\n// between the first set and all the successive sets in one bucket.\nfunc (tx *Tx) SDiffByOneBucket(bucket string, key1, key2 []byte) ([][]byte, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId := b.Id\n\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\t\titems, err := set.SDiff(string(key1), string(key2))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tvalues := make([][]byte, len(items))\n\t\tfor i, item := range items {\n\t\t\tvalue, err := tx.db.getValueByRecord(item)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tvalues[i] = value\n\t\t}\n\t\treturn values, nil\n\t}\n\n\treturn nil, ErrBucketNotFound\n}\n\n// SDiffByTwoBuckets returns the members of the set resulting from the difference\n// between the first set and all the successive sets in two buckets.\nfunc (tx *Tx) SDiffByTwoBuckets(bucket1 string, key1 []byte, bucket2 string, key2 []byte) ([][]byte, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tset1, set2 *Set\n\t\tok         bool\n\t)\n\n\tb1, err := tx.db.bm.GetBucket(DataStructureSet, bucket1)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId1 := b1.Id\n\n\tb2, err := tx.db.bm.GetBucket(DataStructureSet, bucket2)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId2 := b2.Id\n\n\tif set1, ok = tx.db.Index.set.exist(bucketId1); !ok {\n\t\treturn nil, ErrBucketAndKey(bucket1, key1)\n\t}\n\n\tif set2, ok = tx.db.Index.set.exist(bucketId2); !ok {\n\t\treturn nil, ErrBucketAndKey(bucket2, key2)\n\t}\n\n\tvalues := make([][]byte, 0)\n\n\tfor hash, item := range set1.M[string(key1)] {\n\t\tif _, ok := set2.M[string(key2)][hash]; !ok {\n\t\t\tvalue, err := tx.db.getValueByRecord(item)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tvalues = append(values, value)\n\t\t}\n\t}\n\n\treturn values, nil\n}\n\n// SMoveByOneBucket moves member from the set at source to the set at destination in one bucket.\nfunc (tx *Tx) SMoveByOneBucket(bucket string, key1, key2, item []byte) (bool, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn false, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tbucketId := b.Id\n\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\t\treturn set.SMove(string(key1), string(key2), item)\n\t}\n\n\treturn false, ErrBucket\n}\n\n// SMoveByTwoBuckets moves member from the set at source to the set at destination in two buckets.\nfunc (tx *Tx) SMoveByTwoBuckets(bucket1 string, key1 []byte, bucket2 string, key2, item []byte) (bool, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn false, err\n\t}\n\n\tvar (\n\t\tset1, set2 *Set\n\t\tok         bool\n\t)\n\n\tb1, err := tx.db.bm.GetBucket(DataStructureSet, bucket1)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tbucketId1 := b1.Id\n\n\tb2, err := tx.db.bm.GetBucket(DataStructureSet, bucket2)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tbucketId2 := b2.Id\n\n\tif set1, ok = tx.db.Index.set.exist(bucketId1); !ok {\n\t\treturn false, ErrBucketAndKey(bucket1, key1)\n\t}\n\n\tif set2, ok = tx.db.Index.set.exist(bucketId2); !ok {\n\t\treturn false, ErrBucketAndKey(bucket2, key1)\n\t}\n\n\tif !set1.SHasKey(string(key1)) {\n\t\treturn false, ErrNotFoundKeyInBucket(bucket1, key1)\n\t}\n\n\tif !set2.SHasKey(string(key2)) {\n\t\treturn false, ErrNotFoundKeyInBucket(bucket2, key2)\n\t}\n\n\thash, err := getFnv32(item)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\n\tif r, ok := set2.M[string(key2)][hash]; !ok {\n\t\terr := set2.SAdd(string(key2), [][]byte{item}, []*Record{r})\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\t}\n\n\terr = set1.SRem(string(key1), item)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\n\treturn true, nil\n}\n\n// SUnionByOneBucket the members of the set resulting from the union of all the given sets in one bucket.\nfunc (tx *Tx) SUnionByOneBucket(bucket string, key1, key2 []byte) ([][]byte, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\tb1, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId := b1.Id\n\n\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\t\titems, err := set.SUnion(string(key1), string(key2))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tvalues := make([][]byte, len(items))\n\n\t\tfor i, item := range items {\n\t\t\tvalue, err := tx.db.getValueByRecord(item)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tvalues[i] = value\n\t\t}\n\t\treturn values, nil\n\t}\n\n\treturn nil, ErrBucket\n}\n\n// SUnionByTwoBuckets the members of the set resulting from the union of all the given sets in two buckets.\nfunc (tx *Tx) SUnionByTwoBuckets(bucket1 string, key1 []byte, bucket2 string, key2 []byte) ([][]byte, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tset1, set2 *Set\n\t\tok         bool\n\t)\n\tb1, err := tx.db.bm.GetBucket(DataStructureSet, bucket1)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId1 := b1.Id\n\n\tb2, err := tx.db.bm.GetBucket(DataStructureSet, bucket2)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId2 := b2.Id\n\n\tif set1, ok = tx.db.Index.set.exist(bucketId1); !ok {\n\t\treturn nil, ErrBucketAndKey(bucket1, key1)\n\t}\n\n\tif set2, ok = tx.db.Index.set.exist(bucketId2); !ok {\n\t\treturn nil, ErrBucketAndKey(bucket2, key2)\n\t}\n\n\tif !set1.SHasKey(string(key1)) {\n\t\treturn nil, ErrNotFoundKeyInBucket(bucket1, key1)\n\t}\n\n\tif !set2.SHasKey(string(key2)) {\n\t\treturn nil, ErrNotFoundKeyInBucket(bucket2, key2)\n\t}\n\n\tvalues := make([][]byte, 0)\n\n\tfor _, r := range set1.M[string(key1)] {\n\t\tvalue, err := tx.db.getValueByRecord(r)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tvalues = append(values, value)\n\t}\n\n\tfor hash, r := range set2.M[string(key2)] {\n\t\tif _, ok := set1.M[string(key1)][hash]; !ok {\n\t\t\tvalue, err := tx.db.getValueByRecord(r)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tvalues = append(values, value)\n\t\t}\n\t}\n\n\treturn values, nil\n}\n\n// SKeys find all keys matching a given pattern\nfunc (tx *Tx) SKeys(bucket, pattern string, f func(key string) bool) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbucketId := b.Id\n\tif set, ok := tx.db.Index.set.exist(bucketId); !ok {\n\t\treturn ErrBucket\n\t} else {\n\t\tfor key := range set.M {\n\t\t\tif end, err := MatchForRange(pattern, key, f); end || err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// ErrBucketAndKey returns when bucket or key not found.\nfunc ErrBucketAndKey(bucket string, key []byte) error {\n\treturn errors.Wrapf(ErrBucketNotFound, \"bucket:%s, key:%s\", bucket, key)\n}\n\n// ErrNotFoundKeyInBucket returns when key not in the bucket.\nfunc ErrNotFoundKeyInBucket(bucket string, key []byte) error {\n\treturn errors.Wrapf(ErrNotFoundKey, \"%s is not found in %s\", key, bucket)\n}\n"
        },
        {
          "name": "tx_set_test.go",
          "type": "blob",
          "size": 12.853515625,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestTx_SAdd(t *testing.T) {\n\tbucket := \"bucket\"\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, []byte(\"\"), []byte(\"val1\"), ErrKeyEmpty, nil)\n\n\t\tkey := GetTestBytes(0)\n\t\tnum := 10\n\t\tfor i := 0; i < num; i++ {\n\t\t\ttxSAdd(t, db, bucket, key, GetTestBytes(i), nil, nil)\n\t\t}\n\n\t\tfor i := 0; i < num; i++ {\n\t\t\ttxSIsMember(t, db, bucket, key, GetTestBytes(i), true)\n\t\t}\n\n\t\ttxSIsMember(t, db, bucket, key, GetTestBytes(num), false)\n\t})\n}\n\nfunc TestTx_SRem(t *testing.T) {\n\tbucket := \"bucket\"\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\tkey := []byte(\"key1\")\n\t\tval1 := []byte(\"one\")\n\t\tval2 := []byte(\"two\")\n\t\tval3 := []byte(\"three\")\n\n\t\ttxSAdd(t, db, bucket, key, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key, val2, nil, nil)\n\t\ttxSAdd(t, db, bucket, key, val3, nil, nil)\n\n\t\ttxSRem(t, db, bucket, key, val3, nil)\n\n\t\ttxSIsMember(t, db, bucket, key, val1, true)\n\t\ttxSIsMember(t, db, bucket, key, val2, true)\n\t\ttxSIsMember(t, db, bucket, key, val3, false)\n\t})\n}\n\nfunc TestTx_SRem2(t *testing.T) {\n\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\tval1 := GetTestBytes(0)\n\tval2 := GetTestBytes(1)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, key, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key, val2, nil, nil)\n\n\t\ttxSRem(t, db, bucket, key, val1, nil)\n\t\ttxSRem(t, db, bucket, key, val1, ErrSetMemberNotExist)\n\n\t\ttxSRem(t, db, bucket, key, val2, nil)\n\n\t\ttxSAreMembers(t, db, bucket, key, false, val1, val2)\n\t})\n}\n\nfunc TestTx_SMembers(t *testing.T) {\n\tbucket := \"bucket\"\n\tfakeBucket := \"fake_bucket\"\n\tkey := GetTestBytes(0)\n\tval1 := GetTestBytes(0)\n\tval2 := GetTestBytes(1)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, key, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key, val2, nil, nil)\n\n\t\ttxSMembers(t, db, bucket, key, 2, nil)\n\n\t\ttxSIsMember(t, db, bucket, key, val1, true)\n\t\ttxSIsMember(t, db, bucket, key, val1, true)\n\n\t\ttxSMembers(t, db, fakeBucket, key, 0, ErrBucketNotExist)\n\t})\n}\n\nfunc TestTx_SCard(t *testing.T) {\n\tbucket := \"bucket\"\n\tfakeBucket := \"fake_bucket\"\n\tkey := GetTestBytes(0)\n\tval1 := GetTestBytes(1)\n\tval2 := GetTestBytes(2)\n\tval3 := GetTestBytes(3)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, key, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key, val2, nil, nil)\n\t\ttxSAdd(t, db, bucket, key, val3, nil, nil)\n\n\t\ttxSCard(t, db, bucket, key, 3, nil)\n\n\t\ttxSCard(t, db, fakeBucket, key, 0, ErrBucketNotExist)\n\t})\n}\n\nfunc TestTx_SDiffByOneBucket(t *testing.T) {\n\tbucket := \"bucket\"\n\tfakeBucket := \"fake_bucket\"\n\tkey1 := GetTestBytes(0)\n\tkey2 := GetTestBytes(1)\n\tkey3 := GetTestBytes(2)\n\tval1 := GetTestBytes(1)\n\tval2 := GetTestBytes(2)\n\tval3 := GetTestBytes(3)\n\tval4 := GetTestBytes(4)\n\tval5 := GetTestBytes(5)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, key1, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key1, val2, nil, nil)\n\t\ttxSAdd(t, db, bucket, key1, val3, nil, nil)\n\n\t\ttxSAdd(t, db, bucket, key2, val3, nil, nil)\n\t\ttxSAdd(t, db, bucket, key2, val4, nil, nil)\n\t\ttxSAdd(t, db, bucket, key2, val5, nil, nil)\n\n\t\tdiff := [][]byte{val1, val2}\n\t\ttxSDiffByOneBucket(t, db, bucket, key1, key2, diff, nil)\n\t\ttxSDiffByOneBucket(t, db, fakeBucket, key2, key1, nil, ErrBucketNotExist)\n\n\t\ttxSAdd(t, db, bucket, key3, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key3, val2, nil, nil)\n\n\t\tfor _, val := range diff {\n\t\t\ttxSIsMember(t, db, bucket, key3, val, true)\n\t\t}\n\t})\n}\n\nfunc TestTx_SDiffByTwoBuckets(t *testing.T) {\n\tbucket1 := \"bucket1\"\n\tbucket2 := \"bucket2\"\n\tbucket3 := \"bucket3\"\n\tfakeBucket := \"fake_bucket_%d\"\n\tkey1 := GetTestBytes(0)\n\tkey2 := GetTestBytes(1)\n\tkey3 := GetTestBytes(2)\n\tval1 := GetTestBytes(1)\n\tval2 := GetTestBytes(2)\n\tval3 := GetTestBytes(3)\n\tval4 := GetTestBytes(4)\n\tval5 := GetTestBytes(5)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket1, nil)\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket2, nil)\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket3, nil)\n\n\t\ttxSAdd(t, db, bucket1, key1, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket1, key1, val2, nil, nil)\n\t\ttxSAdd(t, db, bucket1, key1, val3, nil, nil)\n\n\t\ttxSAdd(t, db, bucket2, key2, val3, nil, nil)\n\t\ttxSAdd(t, db, bucket2, key2, val4, nil, nil)\n\t\ttxSAdd(t, db, bucket2, key2, val5, nil, nil)\n\n\t\tdiff := [][]byte{val1, val2}\n\t\ttxSDiffByTwoBucket(t, db, bucket1, key1, bucket2, key2, diff, nil)\n\n\t\ttxSDiffByTwoBucket(t, db, fmt.Sprintf(fakeBucket, 1), key1, bucket2, key2, nil, ErrBucketNotExist)\n\t\ttxSDiffByTwoBucket(t, db, bucket1, key1, fmt.Sprintf(fakeBucket, 2), key2, nil, ErrBucketNotExist)\n\n\t\ttxSAdd(t, db, bucket3, key3, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket3, key3, val2, nil, nil)\n\n\t\tfor _, val := range diff {\n\t\t\ttxSIsMember(t, db, bucket3, key3, val, true)\n\t\t}\n\t})\n}\n\nfunc TestTx_SPop(t *testing.T) {\n\tbucket := \"bucket\"\n\tfakeBucket := \"fake_bucket\"\n\tkey := GetTestBytes(0)\n\tval1 := GetTestBytes(1)\n\tval2 := GetTestBytes(2)\n\tval3 := GetTestBytes(3)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, key, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key, val2, nil, nil)\n\t\ttxSAdd(t, db, bucket, key, val3, nil, nil)\n\n\t\ttxSCard(t, db, bucket, key, 3, nil)\n\t\ttxSPop(t, db, bucket, key, nil)\n\t\ttxSCard(t, db, bucket, key, 2, nil)\n\n\t\ttxSPop(t, db, fakeBucket, key, ErrBucketNotExist)\n\t})\n\n}\n\nfunc TestTx_SMoveByOneBucket(t *testing.T) {\n\tbucket := \"bucket\"\n\tfakeBucket := \"fake_bucket\"\n\tkey1 := GetTestBytes(0)\n\tkey2 := GetTestBytes(1)\n\tval1 := GetTestBytes(1)\n\tval2 := GetTestBytes(2)\n\tval3 := GetTestBytes(3)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, key1, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key1, val2, nil, nil)\n\n\t\ttxSAdd(t, db, bucket, key2, val3, nil, nil)\n\n\t\ttxSMoveByOneBucket(t, db, bucket, key1, key2, val2, true, nil)\n\t\ttxSIsMember(t, db, bucket, key1, val2, false)\n\t\ttxSIsMember(t, db, bucket, key2, val2, true)\n\n\t\ttxSMoveByOneBucket(t, db, fakeBucket, key1, key2, val2, false, ErrBucketNotExist)\n\t})\n}\n\nfunc TestTx_SMoveByTwoBuckets(t *testing.T) {\n\tbucket1 := \"bucket1\"\n\tbucket2 := \"bucket2\"\n\tfakeBucket := \"fake_bucket_%d\"\n\tkey1 := GetTestBytes(0)\n\tkey2 := GetTestBytes(1)\n\tfakeKey1 := GetTestBytes(2)\n\tfakeKey2 := GetTestBytes(3)\n\tval1 := GetTestBytes(1)\n\tval2 := GetTestBytes(2)\n\tval3 := GetTestBytes(3)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket1, nil)\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket2, nil)\n\t\ttxSAdd(t, db, bucket1, key1, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket1, key1, val2, nil, nil)\n\n\t\ttxSAdd(t, db, bucket2, key2, val3, nil, nil)\n\n\t\ttxSMoveByTwoBuckets(t, db, bucket1, key1, bucket2, key2, val2, true, nil)\n\t\ttxSIsMember(t, db, bucket1, key1, val2, false)\n\t\ttxSIsMember(t, db, bucket2, key2, val2, true)\n\n\t\ttxSMoveByTwoBuckets(t, db, bucket1, fakeKey1, bucket2, key2, val2, false, ErrNotFoundKey)\n\t\ttxSMoveByTwoBuckets(t, db, bucket1, key1, bucket2, fakeKey2, val2, false, ErrNotFoundKey)\n\t\ttxSMoveByTwoBuckets(t, db, fmt.Sprintf(fakeBucket, 1), key1, bucket2, key2, val2, false, ErrBucketNotExist)\n\t\ttxSMoveByTwoBuckets(t, db, bucket1, key1, fmt.Sprintf(fakeBucket, 2), key2, val2, false, ErrBucketNotExist)\n\t\ttxSMoveByTwoBuckets(t, db, fmt.Sprintf(fakeBucket, 1), key1, fmt.Sprintf(fakeBucket, 2), key2, val2, false, ErrBucketNotExist)\n\t})\n}\n\nfunc TestTx_SUnionByOneBucket(t *testing.T) {\n\tbucket := \"bucket\"\n\tfakeBucket := \"fake_bucket\"\n\tkey1 := GetTestBytes(0)\n\tkey2 := GetTestBytes(1)\n\tkey3 := GetTestBytes(2)\n\n\tval1 := GetTestBytes(1)\n\tval2 := GetTestBytes(2)\n\tval3 := GetTestBytes(3)\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, key1, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key1, val2, nil, nil)\n\t\ttxSAdd(t, db, bucket, key2, val3, nil, nil)\n\t\ttxSAdd(t, db, bucket, key3, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key3, val2, nil, nil)\n\t\ttxSAdd(t, db, bucket, key3, val3, nil, nil)\n\n\t\tall := [][]byte{val1, val2, val3}\n\t\ttxSUnionByOneBucket(t, db, bucket, key1, key2, all, nil)\n\t\tfor _, item := range all {\n\t\t\ttxSIsMember(t, db, bucket, key3, item, true)\n\t\t}\n\n\t\ttxSUnionByOneBucket(t, db, fakeBucket, key1, key2, nil, ErrBucketNotExist)\n\t})\n}\n\nfunc TestTx_SUnionByTwoBuckets(t *testing.T) {\n\tbucket1 := \"bucket1\"\n\tbucket2 := \"bucket2\"\n\tfakeBucket := \"fake_bucket_%d\"\n\tkey1 := GetTestBytes(0)\n\tkey2 := GetTestBytes(1)\n\tfakeKey1 := GetTestBytes(2)\n\tfakeKey2 := GetTestBytes(3)\n\tval1 := GetTestBytes(1)\n\tval2 := GetTestBytes(2)\n\tval3 := GetTestBytes(3)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket1, nil)\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket2, nil)\n\t\ttxSAdd(t, db, bucket1, key1, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket1, key1, val2, nil, nil)\n\t\ttxSAdd(t, db, bucket2, key2, val3, nil, nil)\n\n\t\tall := [][]byte{val1, val2, val3}\n\t\ttxSUnionByTwoBuckets(t, db, bucket1, key1, bucket2, key2, all, nil)\n\n\t\ttxSUnionByTwoBuckets(t, db, fmt.Sprintf(fakeBucket, 1), key1, bucket2, key2, nil, ErrBucketNotExist)\n\t\ttxSUnionByTwoBuckets(t, db, bucket1, key1, fmt.Sprintf(fakeBucket, 2), key2, nil, ErrBucketNotExist)\n\t\ttxSUnionByTwoBuckets(t, db, bucket1, fakeKey1, bucket2, key2, nil, ErrNotFoundKey)\n\t\ttxSUnionByTwoBuckets(t, db, bucket1, key1, bucket2, fakeKey2, nil, ErrNotFoundKey)\n\t})\n}\n\nfunc TestTx_SHasKey(t *testing.T) {\n\tbucket := \"bucket\"\n\tfakeBucket := \"fake_bucket\"\n\tkey := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, key, GetTestBytes(1), nil, nil)\n\n\t\ttxSHasKey(t, db, bucket, key, true)\n\t\ttxSHasKey(t, db, fakeBucket, key, false)\n\t})\n}\n\nfunc TestTx_SIsMember(t *testing.T) {\n\tbucket := \"bucket\"\n\tfakeBucket := \"fake_bucket\"\n\tkey := GetTestBytes(0)\n\tfakeKey := GetTestBytes(1)\n\tval := GetTestBytes(0)\n\tfakeVal := GetTestBytes(1)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, key, val, nil, nil)\n\n\t\ttxSIsMember(t, db, bucket, key, val, true)\n\t\ttxSIsMember(t, db, bucket, key, fakeVal, false)\n\t\ttxSIsMember(t, db, bucket, fakeKey, val, false)\n\t\ttxSIsMember(t, db, fakeBucket, fakeKey, val, false)\n\t})\n}\n\nfunc TestTx_SAreMembers(t *testing.T) {\n\tbucket := \"bucket\"\n\tfakeBucket := \"fake_bucket\"\n\tkey := GetTestBytes(0)\n\tfakeKey := GetTestBytes(1)\n\tval1 := GetTestBytes(0)\n\tval2 := GetTestBytes(1)\n\tfakeVal := GetTestBytes(2)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, key, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key, val2, nil, nil)\n\n\t\ttxSAreMembers(t, db, bucket, key, true)\n\t\ttxSAreMembers(t, db, bucket, key, true, val1)\n\t\ttxSAreMembers(t, db, bucket, key, true, val2)\n\t\ttxSAreMembers(t, db, bucket, key, true, val1, val2)\n\t\ttxSAreMembers(t, db, bucket, key, false, fakeVal)\n\t\ttxSAreMembers(t, db, bucket, fakeKey, false, val1)\n\t\ttxSAreMembers(t, db, fakeBucket, key, false, val1)\n\t})\n}\n\nfunc TestTx_SKeys(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := \"key_%d\"\n\tval := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\tnum := 3\n\t\tfor i := 0; i < num; i++ {\n\t\t\ttxSAdd(t, db, bucket, []byte(fmt.Sprintf(key, i)), val, nil, nil)\n\t\t}\n\n\t\tvar keys []string\n\t\ttxSKeys(t, db, bucket, \"*\", func(key string) bool {\n\t\t\tkeys = append(keys, key)\n\t\t\treturn true\n\t\t}, num, nil)\n\n\t\tkeys = []string{}\n\t\ttxSKeys(t, db, bucket, \"*\", func(key string) bool {\n\t\t\tkeys = append(keys, key)\n\t\t\treturn len(keys) != num-1\n\t\t}, num-1, nil)\n\n\t\tkeys = []string{}\n\t\ttxSKeys(t, db, bucket, \"fake_key*\", func(key string) bool {\n\t\t\tkeys = append(keys, key)\n\t\t\treturn true\n\t\t}, 0, nil)\n\t})\n}\n\nfunc TestErrBucketAndKey(t *testing.T) {\n\n\tgot := ErrBucketAndKey(\"foo\", []byte(\"bar\"))\n\n\tassert.True(t,\n\t\terrors.Is(got, ErrBucketNotFound))\n}\n\nfunc TestErrNotFoundKeyInBucket(t *testing.T) {\n\n\tgot := ErrNotFoundKeyInBucket(\"foo\", []byte(\"bar\"))\n\n\tassert.True(t,\n\t\terrors.Is(got, ErrNotFoundKey))\n}\n"
        },
        {
          "name": "tx_test.go",
          "type": "blob",
          "size": 5.5859375,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\n// todo to check is there any deadlock here?\nfunc TestTx_Rollback(t *testing.T) {\n\n\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\tbucket := \"bucket_rollback_test\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\ttx, err := db.Begin(true)\n\t\tassert.NoError(t, err)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%03d\", i))\n\t\t\tval := []byte(\"val_\" + fmt.Sprintf(\"%03d\", i))\n\t\t\tif i == 7 {\n\t\t\t\tkey = []byte(\"\") // set error key to make tx rollback\n\t\t\t}\n\t\t\tif err = tx.Put(bucket, key, val, Persistent); err != nil {\n\t\t\t\t// tx rollback\n\t\t\t\ttx.Rollback()\n\n\t\t\t\tif i < 7 {\n\t\t\t\t\tt.Fatal(\"err TestTx_Rollback\")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// no one found\n\t\tfor i := 0; i <= 10; i++ {\n\t\t\ttx, err = db.Begin(false)\n\t\t\tassert.NoError(t, err)\n\n\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%03d\", i))\n\t\t\tif _, err := tx.Get(bucket, key); err != nil {\n\t\t\t\t// tx rollback\n\t\t\t\ttx.Rollback()\n\t\t\t} else {\n\t\t\t\tt.Fatal(\"err TestTx_Rollback\")\n\t\t\t}\n\t\t}\n\n\t})\n}\n\nfunc TestTx_Begin(t *testing.T) {\n\tt.Run(\"Begin with default options, with only read\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\ttx, err := db.Begin(false)\n\t\t\tassert.NoError(t, err)\n\n\t\t\terr = tx.Rollback()\n\t\t\tassert.NoError(t, err)\n\n\t\t\terr = db.Close()\n\t\t\tassert.NoError(t, err)\n\t\t})\n\t})\n\n\tt.Run(\"Begin with default options, with writable\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\ttx, err := db.Begin(true)\n\t\t\tassert.NoError(t, err)\n\n\t\t\ttx.Rollback()\n\n\t\t\terr = db.Close()\n\t\t\tassert.NoError(t, err)\n\t\t})\n\t})\n\n\tt.Run(\"Begin with error: error options\", func(t *testing.T) {\n\t\topt := DefaultOptions\n\t\topt.Dir = \"/tmp/nutsdbtesttx\"\n\t\topt.NodeNum = -1\n\n\t\twithDBOption(t, opt, func(t *testing.T, db *DB) {\n\t\t\t_, err := db.Begin(false)\n\t\t\tassert.Error(t, err)\n\t\t})\n\t})\n\n\tt.Run(\"Begin with error: begin the closed db\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\ttx, err := db.Begin(true)\n\t\t\tassert.NoError(t, err)\n\n\t\t\ttx.Rollback() // for unlock mutex\n\n\t\t\terr = db.Close()\n\t\t\tassert.NoError(t, err)\n\n\t\t\t_, err = db.Begin(false)\n\t\t\tassert.Error(t, err)\n\t\t})\n\t})\n}\n\nfunc TestTx_Close(t *testing.T) {\n\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\ttx, err := db.Begin(false)\n\t\tassert.NoError(t, err)\n\n\t\terr = tx.Rollback()\n\t\tassert.NoError(t, err)\n\n\t\tbucket := \"bucket_tx_close_test\"\n\n\t\t_, err = tx.Get(bucket, []byte(\"foo\"))\n\t\tassert.Errorf(t, err, \"err TestTx_Close\")\n\n\t\t_, err = tx.RangeScan(bucket, []byte(\"foo0\"), []byte(\"foo1\"))\n\t\tassert.Errorf(t, err, \"err TestTx_Close\")\n\n\t\t_, err = tx.PrefixScan(bucket, []byte(\"foo\"), 0, 1)\n\t\tassert.Errorf(t, err, \"err TestTx_Close\")\n\n\t\t_, err = tx.PrefixSearchScan(bucket, []byte(\"f\"), \"oo\", 0, 1)\n\t\tassert.Errorf(t, err, \"err TestTx_Close\")\n\t})\n}\n\nfunc TestTx_CommittedStatus(t *testing.T) {\n\n\twithRAMIdxDB(t, func(t *testing.T, db *DB) {\n\n\t\tbucket := \"bucket_committed_status\"\n\n\t\t{ // setup data\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttx, err := db.Begin(true)\n\t\t\tassert.NoError(t, err)\n\n\t\t\terr = tx.Put(bucket, []byte(\"key1\"), []byte(\"value1\"), 0)\n\t\t\tassert.NoError(t, err)\n\n\t\t\terr = tx.Put(bucket, []byte(\"key2\"), []byte(\"value2\"), 0)\n\t\t\tassert.NoError(t, err)\n\n\t\t\terr = tx.Commit()\n\t\t\tassert.NoError(t, err)\n\t\t}\n\t})\n}\n\nfunc TestTx_PutWithTimestamp(t *testing.T) {\n\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\tbucket := \"bucket_put_with_timestamp\"\n\n\t\ttimestamps := []uint64{1547707905, 1547707910, uint64(time.Now().Unix())}\n\n\t\t{ // put with timestamp\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttx, err := db.Begin(true)\n\t\t\tassert.NoError(t, err)\n\t\t\tfor i, timestamp := range timestamps {\n\t\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%03d\", i))\n\t\t\t\tval := []byte(\"val_\" + fmt.Sprintf(\"%03d\", i))\n\n\t\t\t\terr = tx.PutWithTimestamp(bucket, key, val, 0, timestamp)\n\t\t\t\tassert.NoError(t, err)\n\n\t\t\t}\n\t\t\terr = tx.Commit()\n\t\t\tassert.NoError(t, err)\n\t\t}\n\t})\n}\n\nfunc TestTx_Commit(t *testing.T) {\n\tt.Run(\"build_bucket_indexes_after_commit\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\tbucket := \"bucket1\"\n\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\tassert.Equal(t, 1, db.Index.bTree.defaultOp.getIdxLen())\n\t\t\ttxDeleteBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\tassert.Equal(t, 0, db.Index.bTree.defaultOp.getIdxLen())\n\n\t\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\t\t\tassert.Equal(t, 1, db.Index.sortedSet.defaultOp.getIdxLen())\n\t\t\ttxDeleteBucket(t, db, DataStructureSortedSet, bucket, nil)\n\t\t\tassert.Equal(t, 0, db.Index.sortedSet.defaultOp.getIdxLen())\n\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\t\tassert.Equal(t, 1, db.Index.list.defaultOp.getIdxLen())\n\t\t\ttxDeleteBucket(t, db, DataStructureList, bucket, nil)\n\t\t\tassert.Equal(t, 0, db.Index.list.defaultOp.getIdxLen())\n\n\t\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\t\tassert.Equal(t, 1, db.Index.set.defaultOp.getIdxLen())\n\t\t\ttxDeleteBucket(t, db, DataStructureSet, bucket, nil)\n\t\t\tassert.Equal(t, 0, db.Index.set.defaultOp.getIdxLen())\n\n\t\t})\n\t})\n\n}\n"
        },
        {
          "name": "tx_zset.go",
          "type": "blob",
          "size": 13.033203125,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/xujiajun/utils/strconv2\"\n)\n\n// SeparatorForZSetKey represents separator for zSet key.\nconst SeparatorForZSetKey = \"|\"\n\ntype SortedSetMember struct {\n\tValue []byte\n\tScore float64\n}\n\n// ZAdd Adds the specified member with the specified score into the sorted set specified by key in a bucket.\nfunc (tx *Tx) ZAdd(bucket string, key []byte, score float64, val []byte) error {\n\tvar buffer bytes.Buffer\n\n\tif strings.Contains(string(key), SeparatorForZSetKey) {\n\t\treturn ErrSeparatorForZSetKey()\n\t}\n\n\tbuffer.Write(key)\n\tbuffer.Write([]byte(SeparatorForZSetKey))\n\tscoreBytes := []byte(strconv.FormatFloat(score, 'f', -1, 64))\n\tbuffer.Write(scoreBytes)\n\tnewKey := buffer.Bytes()\n\n\treturn tx.put(bucket, newKey, val, Persistent, DataZAddFlag, uint64(time.Now().Unix()), DataStructureSortedSet)\n}\n\n// ZMembers Returns all the members and scores of members of the set specified by key in a bucket.\nfunc (tx *Tx) ZMembers(bucket string, key []byte) (map[*SortedSetMember]struct{}, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn nil, ErrBucket\n\t}\n\n\tmembers, err := sortedSet.ZMembers(string(key))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tres := make(map[*SortedSetMember]struct{})\n\tfor record, score := range members {\n\t\tvalue, err := tx.db.getValueByRecord(record)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tres[&SortedSetMember{\n\t\t\tValue: value,\n\t\t\tScore: float64(score),\n\t\t}] = struct{}{}\n\t}\n\n\treturn res, nil\n}\n\n// ZCard Returns the sorted set cardinality (number of elements) of the sorted set specified by key in a bucket.\nfunc (tx *Tx) ZCard(bucket string, key []byte) (int, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn 0, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn 0, ErrBucket\n\t}\n\n\treturn sortedSet.ZCard(string(key))\n}\n\n// ZCount Returns the number of elements in the sorted set specified by key in a bucket with a score between min and max and opts.\n// Opt includes the following parameters:\n// Limit        int  // limit the max nodes to return\n// ExcludeStart bool // exclude start value, so it search in interval (start, end] or (start, end)\n// ExcludeEnd   bool // exclude end value, so it search in interval [start, end) or (start, end)\nfunc (tx *Tx) ZCount(bucket string, key []byte, start, end float64, opts *GetByScoreRangeOptions) (int, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn 0, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn 0, ErrBucket\n\t}\n\n\treturn sortedSet.ZCount(string(key), SCORE(start), SCORE(end), opts)\n}\n\n// ZPopMax Removes and returns the member with the highest score in the sorted set specified by key in a bucket.\nfunc (tx *Tx) ZPopMax(bucket string, key []byte) (*SortedSetMember, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn nil, ErrBucket\n\t}\n\n\trecord, score, err := sortedSet.ZPeekMax(string(key))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvalue, err := tx.db.getValueByRecord(record)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &SortedSetMember{Value: value, Score: float64(score)}, tx.put(bucket, key, []byte(\"\"), Persistent, DataZPopMaxFlag, uint64(time.Now().Unix()), DataStructureSortedSet)\n}\n\n// ZPopMin Removes and returns the member with the lowest score in the sorted set specified by key in a bucket.\nfunc (tx *Tx) ZPopMin(bucket string, key []byte) (*SortedSetMember, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn nil, ErrBucket\n\t}\n\n\trecord, score, err := sortedSet.ZPeekMin(string(key))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvalue, err := tx.db.getValueByRecord(record)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &SortedSetMember{Value: value, Score: float64(score)}, tx.put(bucket, key, []byte(\"\"), Persistent, DataZPopMinFlag, uint64(time.Now().Unix()), DataStructureSortedSet)\n}\n\n// ZPeekMax Returns the member with the highest score in the sorted set specified by key in a bucket.\nfunc (tx *Tx) ZPeekMax(bucket string, key []byte) (*SortedSetMember, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn nil, ErrBucket\n\t}\n\n\trecord, score, err := sortedSet.ZPeekMax(string(key))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvalue, err := tx.db.getValueByRecord(record)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &SortedSetMember{Value: value, Score: float64(score)}, nil\n}\n\n// ZPeekMin Returns the member with the lowest score in the sorted set specified by key in a bucket.\nfunc (tx *Tx) ZPeekMin(bucket string, key []byte) (*SortedSetMember, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn nil, ErrBucket\n\t}\n\n\trecord, score, err := sortedSet.ZPeekMin(string(key))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvalue, err := tx.db.getValueByRecord(record)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &SortedSetMember{Value: value, Score: float64(score)}, nil\n}\n\n// ZRangeByScore Returns all the elements in the sorted set specified by key in a bucket with a score between min and max.\n// And the parameter `Opts` is the same as ZCount's.\nfunc (tx *Tx) ZRangeByScore(bucket string, key []byte, start, end float64, opts *GetByScoreRangeOptions) ([]*SortedSetMember, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn nil, ErrBucket\n\t}\n\n\trecords, scores, err := sortedSet.ZRangeByScore(string(key), SCORE(start), SCORE(end), opts)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tmembers := make([]*SortedSetMember, len(records))\n\tfor i, record := range records {\n\t\tvalue, err := tx.db.getValueByRecord(record)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tmembers[i] = &SortedSetMember{Value: value, Score: scores[i]}\n\t}\n\n\treturn members, nil\n}\n\n// ZRangeByRank Returns all the elements in the sorted set specified by key in a bucket\n// with a rank between start and end (including elements with rank equal to start or end).\nfunc (tx *Tx) ZRangeByRank(bucket string, key []byte, start, end int) ([]*SortedSetMember, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn nil, ErrBucket\n\t}\n\n\trecords, scores, err := sortedSet.ZRangeByRank(string(key), start, end)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tmembers := make([]*SortedSetMember, len(records))\n\tfor i, record := range records {\n\t\tvalue, err := tx.db.getValueByRecord(record)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tmembers[i] = &SortedSetMember{Value: value, Score: scores[i]}\n\t}\n\n\treturn members, nil\n}\n\n// ZRem removes the specified members from the sorted set stored in one bucket at given bucket and key.\nfunc (tx *Tx) ZRem(bucket string, key []byte, value []byte) error {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn ErrBucket\n\t}\n\n\texist, err = sortedSet.ZExist(string(key), value)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif !exist {\n\t\treturn ErrSortedSetMemberNotExist\n\t}\n\n\treturn tx.put(bucket, key, value, Persistent, DataZRemFlag, uint64(time.Now().Unix()), DataStructureSortedSet)\n}\n\n// ZRemRangeByRank removes all elements in the sorted set stored in one bucket at given bucket with rank between start and end.\n// the rank is 1-based integer. Rank 1 means the first node; Rank -1 means the last node.\nfunc (tx *Tx) ZRemRangeByRank(bucket string, key []byte, start, end int) error {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn err\n\t}\n\n\tstartStr := strconv2.IntToStr(start)\n\tendStr := strconv2.IntToStr(end)\n\treturn tx.put(bucket, key, []byte(startStr+SeparatorForZSetKey+endStr), Persistent, DataZRemRangeByRankFlag, uint64(time.Now().Unix()), DataStructureSortedSet)\n}\n\n// ZRank Returns the rank of member in the sorted set specified by key in a bucket, with the scores ordered from low to high.\nfunc (tx *Tx) ZRank(bucket string, key, value []byte) (int, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn 0, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn 0, ErrBucket\n\t}\n\n\treturn sortedSet.ZRank(string(key), value)\n}\n\n// ZRevRank Returns the rank of member in the sorted set specified by key in a bucket, with the scores ordered from high to low.\nfunc (tx *Tx) ZRevRank(bucket string, key, value []byte) (int, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn 0, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn 0, ErrBucket\n\t}\n\n\treturn sortedSet.ZRevRank(string(key), value)\n}\n\n// ZScore Returns the score of members in a sorted set specified by key in a bucket.\nfunc (tx *Tx) ZScore(bucket string, key, value []byte) (float64, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn 0, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn 0.0, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn 0, ErrBucket\n\t}\n\n\treturn sortedSet.ZScore(string(key), value)\n}\n\n// ZKeys find all keys matching a given pattern in a bucket\nfunc (tx *Tx) ZKeys(bucket, pattern string, f func(key string) bool) error {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn ErrBucket\n\t}\n\n\tfor key := range sortedSet.M {\n\t\tif end, err := MatchForRange(pattern, key, f); end || err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (tx *Tx) ZCheck(bucket string) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbucketId := b.Id\n\tif _, ok := tx.db.Index.sortedSet.exist(bucketId); !ok {\n\t\treturn ErrBucket\n\t}\n\treturn nil\n}\n\n// ErrSeparatorForZSetKey returns when zSet key contains the SeparatorForZSetKey flag.\nfunc ErrSeparatorForZSetKey() error {\n\treturn errors.New(\"contain separator (\" + SeparatorForZSetKey + \") for SortedSetIdx key\")\n}\n"
        },
        {
          "name": "tx_zset_test.go",
          "type": "blob",
          "size": 14.5791015625,
          "content": "// Copyright 2023 The nutsdb Authors. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nvar tx *Tx\n\nfunc TestTx_ZCheck(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\terr := db.View(func(tx *Tx) error {\n\t\t\trequire.Equal(t, ErrBucketNotExist, tx.ZCheck(\"fake bucket\"))\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc TestTx_ZAdd(t *testing.T) {\n\n\tbucket := \"bucket\"\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxZAdd(t, db, bucket, GetTestBytes(0), GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\ttxZCard(t, db, bucket, GetTestBytes(0), 10, nil)\n\t})\n}\n\nfunc TestTx_ZScore(t *testing.T) {\n\tbucket := \"bucket\"\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxZAdd(t, db, bucket, GetTestBytes(0), GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxZScore(t, db, bucket, GetTestBytes(0), GetTestBytes(i), float64(i), nil)\n\t\t}\n\n\t\ttxZScore(t, db, bucket, GetTestBytes(0), GetTestBytes(10), float64(10), ErrSortedSetMemberNotExist)\n\t\ttxZScore(t, db, bucket, GetTestBytes(1), GetTestBytes(0), float64(0), ErrSortedSetNotFound)\n\n\t\t// update the score of member\n\t\ttxZAdd(t, db, bucket, GetTestBytes(0), GetTestBytes(5), float64(999), nil, nil)\n\t\ttxZScore(t, db, bucket, GetTestBytes(0), GetTestBytes(5), 999, nil)\n\t})\n}\n\nfunc TestTx_ZRem(t *testing.T) {\n\tbucket := \"bucket\"\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxZAdd(t, db, bucket, GetTestBytes(0), GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\ttxZScore(t, db, bucket, GetTestBytes(0), GetTestBytes(3), float64(3), nil)\n\n\t\t// normal remove\n\t\ttxZRem(t, db, bucket, GetTestBytes(0), GetTestBytes(3), nil)\n\t\ttxZScore(t, db, bucket, GetTestBytes(0), GetTestBytes(3), float64(3), ErrSortedSetMemberNotExist)\n\n\t\ttxZCard(t, db, bucket, GetTestBytes(0), 9, nil)\n\n\t\t// remove a fake member\n\t\ttxZRem(t, db, bucket, GetTestBytes(0), GetTestBytes(10), ErrSortedSetMemberNotExist)\n\n\t\t// remove from a fake zset\n\t\ttxZRem(t, db, bucket, GetTestBytes(1), GetTestBytes(0), ErrSortedSetNotFound)\n\t})\n}\n\nfunc TestTx_ZMembers(t *testing.T) {\n\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxZAdd(t, db, bucket, key, GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\terr := db.View(func(tx *Tx) error {\n\t\t\tmembers, err := tx.ZMembers(bucket, key)\n\t\t\trequire.NoError(t, err)\n\n\t\t\trequire.Len(t, members, 10)\n\n\t\t\tfor member := range members {\n\t\t\t\trequire.Equal(t, GetTestBytes(int(member.Score)), member.Value)\n\t\t\t}\n\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc TestTx_ZCount(t *testing.T) {\n\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\n\t\tfor i := 0; i < 30; i++ {\n\t\t\ttxZAdd(t, db, bucket, key, GetRandomBytes(24), float64(i), nil, nil)\n\t\t}\n\n\t\terr := db.View(func(tx *Tx) error {\n\n\t\t\tcount, err := tx.ZCount(bucket, key, 10, 20, nil)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Equal(t, 11, count)\n\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc TestTx_ZPop(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\n\t\ttxZPop(t, db, bucket, key, true, nil, 0, ErrSortedSetNotFound)\n\t\ttxZPop(t, db, bucket, key, false, nil, 0, ErrSortedSetNotFound)\n\n\t\ttxZAdd(t, db, bucket, key, GetTestBytes(0), float64(0), nil, nil)\n\t\ttxZRem(t, db, bucket, key, GetTestBytes(0), nil)\n\n\t\ttxZPop(t, db, bucket, key, true, nil, 0, ErrSortedSetIsEmpty)\n\t\ttxZPop(t, db, bucket, key, false, nil, 0, ErrSortedSetIsEmpty)\n\n\t\tfor i := 0; i < 30; i++ {\n\t\t\ttxZAdd(t, db, bucket, key, GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\ttxZPop(t, db, bucket, key, true, GetTestBytes(29), float64(29), nil)\n\t\ttxZPop(t, db, bucket, key, false, GetTestBytes(0), 0, nil)\n\n\t\ttxZPop(t, db, bucket, key, true, GetTestBytes(28), float64(28), nil)\n\t\ttxZPop(t, db, bucket, key, false, GetTestBytes(1), 1, nil)\n\t})\n}\n\nfunc TestTx_ZRangeByScore(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\t\terr := db.View((func(tx *Tx) error {\n\t\t\t_, err := tx.ZRangeByScore(bucket, key, 1, 10, nil)\n\t\t\trequire.Error(t, err)\n\t\t\treturn nil\n\t\t}))\n\t\trequire.NoError(t, err)\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxZAdd(t, db, bucket, key, GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\terr = db.View(func(tx *Tx) error {\n\t\t\tmembers, err := tx.ZRangeByScore(bucket, key, 0, 11, nil)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Len(t, members, 10)\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\t\terr = db.View(func(tx *Tx) error {\n\t\t\tmembers, err := tx.ZRangeByScore(bucket, key, -1, 2, nil)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Len(t, members, 3)\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\t\terr = db.View(func(tx *Tx) error {\n\t\t\tmembers, err := tx.ZRangeByScore(bucket, key, 8, 12, nil)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Len(t, members, 2)\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\t\terr = db.View(func(tx *Tx) error {\n\t\t\tmembers, err := tx.ZRangeByScore(bucket, key, 5, 2, nil)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Len(t, members, 4)\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\t})\n}\nfunc TestTx_ZPeekMin(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\n\t\tfor i := 0; i < 30; i++ {\n\t\t\ttxZAdd(t, db, bucket, key, GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\t// get minimum node\n\t\ttxZPeekMin(t, db, bucket, key, GetTestBytes(0), float64(0), nil, nil)\n\n\t\t//  bucket not exists\n\t\ttxZPeekMin(t, db, \"non-exists-bucket\", key, []byte{}, float64(0), ErrBucketNotExist, ErrBucketNotExist)\n\n\t\t// key not exists\n\t\ttxZPeekMin(t, db, bucket, []byte(\"non-exists-key\"), []byte{}, float64(0), ErrSortedSetNotFound, ErrSortedSetNotFound)\n\n\t\t// add nodes\n\n\t\t// add node that will not affect the minimum node\n\t\ttxZAdd(t, db, bucket, key, []byte(\"new-mem\"), float64(3), nil, nil)\n\t\ttxZPeekMin(t, db, bucket, key, GetTestBytes(0), float64(0), nil, nil)\n\t\t// add a new minimum value\n\t\ttxZAdd(t, db, bucket, key, []byte(\"new-min\"), float64(0), nil, nil)\n\t\ttxZPeekMin(t, db, bucket, key, []byte(\"new-min\"), float64(0), nil, nil)\n\n\t\t// remove nodes\n\n\t\t// remove minimum node\n\t\ttxZRem(t, db, bucket, key, []byte(\"new-min\"), nil)\n\t\ttxZPeekMin(t, db, bucket, key, GetTestBytes(0), float64(0), nil, nil)\n\n\t\t// remove non-minimum node\n\t\ttxZRem(t, db, bucket, key, GetTestBytes(5), nil)\n\t\ttxZPeekMin(t, db, bucket, key, GetTestBytes(0), float64(0), nil, nil)\n\n\t\t// remove range by rank\n\t\terr := db.Update(func(tx *Tx) error {\n\t\t\terr := tx.ZRemRangeByRank(bucket, key, 1, 10)\n\t\t\tassert.NoError(t, err)\n\t\t\treturn nil\n\t\t})\n\t\tassert.NoError(t, err)\n\t\ttxZPeekMin(t, db, bucket, key, GetTestBytes(10), float64(10), nil, nil)\n\n\t\t// pop\n\n\t\t// pop min\n\t\ttxZPop(t, db, bucket, key, false, GetTestBytes(10), float64(10), nil)\n\t\ttxZPeekMin(t, db, bucket, key, GetTestBytes(11), float64(11), nil, nil)\n\n\t\t// pop max\n\t\ttxZPop(t, db, bucket, key, true, GetTestBytes(29), float64(29), nil)\n\t\ttxZPeekMin(t, db, bucket, key, GetTestBytes(11), float64(11), nil, nil)\n\t})\n}\n\nfunc TestTx_ZRangeByRank(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\n\t\terr := db.View(func(tx *Tx) error {\n\t\t\t_, err := tx.ZRangeByRank(bucket, key, 1, 10)\n\t\t\trequire.Error(t, err)\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxZAdd(t, db, bucket, key, GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\terr = db.View(func(tx *Tx) error {\n\t\t\tmembers, err := tx.ZRangeByRank(bucket, key, 1, 10)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Len(t, members, 10)\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\terr = db.View(func(tx *Tx) error {\n\t\t\tmembers, err := tx.ZRangeByRank(bucket, key, 3, 6)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Len(t, members, 4)\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\terr = db.View(func(tx *Tx) error {\n\t\t\tmembers, err := tx.ZRangeByRank(bucket, key, -1, 11)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Len(t, members, 1)\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\terr = db.View(func(tx *Tx) error {\n\t\t\tmembers, err := tx.ZRangeByRank(bucket, key, 8, 4)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Len(t, members, 5)\n\n\t\t\tfor i, member := range members {\n\t\t\t\trequire.Equal(t, member.Score, float64(7-i))\n\t\t\t\trequire.Equal(t, member.Value, GetTestBytes(7-i))\n\t\t\t}\n\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc TestTx_ZRemRangeByRank(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\n\t\terr := db.Update(func(tx *Tx) error {\n\t\t\terr := tx.ZRemRangeByRank(bucket, key, 1, 10)\n\t\t\tassert.NoError(t, err)\n\t\t\treturn nil\n\t\t})\n\t\tassert.NoError(t, err)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxZAdd(t, db, bucket, key, GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\terr = db.Update(func(tx *Tx) error {\n\t\t\terr := tx.ZRemRangeByRank(bucket, key, 1, 10)\n\t\t\tassert.NoError(t, err)\n\t\t\treturn nil\n\t\t})\n\t\tassert.NoError(t, err)\n\n\t\ttxZCard(t, db, bucket, key, 0, nil)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxZAdd(t, db, bucket, key, GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\terr = db.Update(func(tx *Tx) error {\n\t\t\terr := tx.ZRemRangeByRank(bucket, key, 1, 2)\n\t\t\tassert.NoError(t, err)\n\t\t\treturn nil\n\t\t})\n\n\t\tfor i := 0; i < 2; i++ {\n\t\t\ttxZScore(t, db, bucket, key, GetTestBytes(0), 0, ErrSortedSetMemberNotExist)\n\t\t}\n\n\t\terr = db.Update(func(tx *Tx) error {\n\t\t\tcard, err := tx.ZCard(bucket, key)\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.Equal(t, 8, card)\n\n\t\t\terr = tx.ZRemRangeByRank(bucket, key, 6, 8)\n\t\t\tassert.NoError(t, err)\n\t\t\treturn nil\n\t\t})\n\n\t\tfor i := 5; i < 8; i++ {\n\t\t\ttxZScore(t, db, bucket, key, GetTestBytes(0), 0, ErrSortedSetMemberNotExist)\n\t\t}\n\n\t\terr = db.Update(func(tx *Tx) error {\n\t\t\tcard, err := tx.ZCard(bucket, key)\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.Equal(t, 5, card)\n\n\t\t\terr = tx.ZRemRangeByRank(bucket, key, 4, 3)\n\t\t\tassert.NoError(t, err)\n\t\t\treturn nil\n\t\t})\n\n\t\tfor i := 2; i < 4; i++ {\n\t\t\ttxZScore(t, db, bucket, key, GetTestBytes(0), 0, ErrSortedSetMemberNotExist)\n\t\t}\n\n\t\tassert.NoError(t, err)\n\t})\n}\n\nfunc TestTx_ZRank(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\n\t\ttxZRank(t, db, bucket, key, GetTestBytes(0), true, 0, ErrSortedSetNotFound)\n\t\ttxZRank(t, db, bucket, key, GetTestBytes(0), false, 0, ErrSortedSetNotFound)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxZAdd(t, db, bucket, key, GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\ttxZRank(t, db, bucket, key, GetTestBytes(0), true, 10, nil)\n\t\ttxZRank(t, db, bucket, key, GetTestBytes(0), false, 1, nil)\n\n\t\ttxZRem(t, db, bucket, key, GetTestBytes(0), nil)\n\n\t\ttxZRank(t, db, bucket, key, GetTestBytes(0), true, 10, ErrSortedSetMemberNotExist)\n\t\ttxZRank(t, db, bucket, key, GetTestBytes(0), false, 1, ErrSortedSetMemberNotExist)\n\n\t\ttxZRem(t, db, bucket, key, GetTestBytes(3), nil)\n\n\t\ttxZRank(t, db, bucket, key, GetTestBytes(4), true, 6, nil)\n\t\ttxZRank(t, db, bucket, key, GetTestBytes(4), false, 3, nil)\n\t})\n}\n\nfunc TestTx_ZKeys(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := \"key_%d\"\n\tval := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\t\tfor i := 0; i < 3; i++ {\n\t\t\ttxZAdd(t, db, bucket, []byte(fmt.Sprintf(key, i)), val, float64(i), nil, nil)\n\t\t}\n\t\ttxZAdd(t, db, bucket, []byte(\"foo\"), val, 1, nil, nil)\n\n\t\ttests := []struct {\n\t\t\tpattern         string\n\t\t\texpectedMatches int\n\t\t\texpectedError   error\n\t\t}{\n\t\t\t{\"*\", 4, nil},         // find all keys\n\t\t\t{\"key_*\", 3, nil},     // find keys with 'key_' prefix\n\t\t\t{\"fake_key*\", 0, nil}, // find non-existing keys\n\t\t}\n\n\t\tfor _, test := range tests {\n\t\t\ttxZKeys(t, db, bucket, test.pattern, func(key string) bool { return true }, test.expectedMatches, test.expectedError)\n\t\t}\n\n\t\t// stop after finding the expected number of keys.\n\t\texpectNum := 2\n\t\tvar foundKeys []string\n\t\ttxZKeys(t, db, bucket, \"*\", func(key string) bool {\n\t\t\tfoundKeys = append(foundKeys, key)\n\t\t\treturn len(foundKeys) < expectNum\n\t\t}, expectNum, nil)\n\t\tassert.Equal(t, expectNum, len(foundKeys))\n\t})\n}\n\nfunc TestTx_ZSetEntryIdxMode_HintKeyValAndRAMIdxMode(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\tvalue := GetRandomBytes(24)\n\n\topts := DefaultOptions\n\topts.EntryIdxMode = HintKeyValAndRAMIdxMode\n\n\t// HintKeyValAndRAMIdxMode\n\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\n\t\terr := db.Update(func(tx *Tx) error {\n\t\t\terr := tx.ZAdd(bucket, key, float64(0), value)\n\t\t\trequire.NoError(t, err)\n\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\tzset := db.Index.sortedSet.getWithDefault(1, db).M[string(key)]\n\t\thash, _ := getFnv32(value)\n\t\tnode := zset.dict[hash]\n\n\t\trequire.NotNil(t, node.record.Value)\n\t\trequire.Equal(t, value, node.record.Value)\n\t})\n}\n\nfunc TestTx_ZSetEntryIdxMode_HintKeyAndRAMIdxMode(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\tvalue := GetRandomBytes(24)\n\n\topts := DefaultOptions\n\topts.EntryIdxMode = HintKeyAndRAMIdxMode\n\n\t// HintKeyValAndRAMIdxMode\n\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\t\terr := db.Update(func(tx *Tx) error {\n\t\t\terr := tx.ZAdd(bucket, key, float64(0), value)\n\t\t\trequire.NoError(t, err)\n\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\tzset := db.Index.sortedSet.getWithDefault(1, db).M[string(key)]\n\t\thash, _ := getFnv32(value)\n\t\tnode := zset.dict[hash]\n\n\t\trequire.Nil(t, node.record.Value)\n\n\t\tv, err := db.getValueByRecord(node.record)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, value, v)\n\t})\n}\n"
        },
        {
          "name": "utils.go",
          "type": "blob",
          "size": 4.083984375,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"bytes\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\n\t\"github.com/xujiajun/utils/strconv2\"\n)\n\n// Truncate changes the size of the file.\nfunc Truncate(path string, capacity int64, f *os.File) error {\n\tfileInfo, _ := os.Stat(path)\n\tif fileInfo.Size() < capacity {\n\t\tif err := f.Truncate(capacity); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc ConvertBigEndianBytesToUint64(data []byte) uint64 {\n\treturn binary.BigEndian.Uint64(data)\n}\n\nfunc ConvertUint64ToBigEndianBytes(value uint64) []byte {\n\tb := make([]byte, 8)\n\tbinary.BigEndian.PutUint64(b, value)\n\treturn b\n}\n\nfunc MarshalInts(ints []int) ([]byte, error) {\n\tbuffer := bytes.NewBuffer([]byte{})\n\tfor _, x := range ints {\n\t\tif err := binary.Write(buffer, binary.LittleEndian, int64(x)); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\treturn buffer.Bytes(), nil\n}\n\nfunc UnmarshalInts(data []byte) ([]int, error) {\n\tvar ints []int\n\tbuffer := bytes.NewBuffer(data)\n\tfor {\n\t\tvar i int64\n\t\terr := binary.Read(buffer, binary.LittleEndian, &i)\n\t\tif errors.Is(err, io.EOF) {\n\t\t\tbreak\n\t\t} else if err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tints = append(ints, int(i))\n\t}\n\treturn ints, nil\n}\n\nfunc MatchForRange(pattern, bucket string, f func(bucket string) bool) (end bool, err error) {\n\tmatch, err := filepath.Match(pattern, bucket)\n\tif err != nil {\n\t\treturn true, err\n\t}\n\tif match && !f(bucket) {\n\t\treturn true, nil\n\t}\n\treturn false, nil\n}\n\n// getDataPath returns the data path for the given file ID.\nfunc getDataPath(fID int64, dir string) string {\n\tseparator := string(filepath.Separator)\n\treturn dir + separator + strconv2.Int64ToStr(fID) + DataSuffix\n}\n\nfunc OneOfUint16Array(value uint16, array []uint16) bool {\n\tfor _, v := range array {\n\t\tif v == value {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc splitIntStringStr(str, separator string) (int, string) {\n\tstrList := strings.Split(str, separator)\n\tfirstItem, _ := strconv2.StrToInt(strList[0])\n\tsecondItem := strList[1]\n\treturn firstItem, secondItem\n}\n\nfunc splitStringIntStr(str, separator string) (string, int) {\n\tstrList := strings.Split(str, separator)\n\tfirstItem := strList[0]\n\tsecondItem, _ := strconv2.StrToInt(strList[1])\n\treturn firstItem, secondItem\n}\n\nfunc splitIntIntStr(str, separator string) (int, int) {\n\tstrList := strings.Split(str, separator)\n\tfirstItem, _ := strconv2.StrToInt(strList[0])\n\tsecondItem, _ := strconv2.StrToInt(strList[1])\n\treturn firstItem, secondItem\n}\n\nfunc encodeListKey(key []byte, seq uint64) []byte {\n\tbuf := make([]byte, len(key)+8)\n\tbinary.LittleEndian.PutUint64(buf[:8], seq)\n\tcopy(buf[8:], key[:])\n\treturn buf\n}\n\nfunc decodeListKey(buf []byte) ([]byte, uint64) {\n\tseq := binary.LittleEndian.Uint64(buf[:8])\n\tkey := make([]byte, len(buf[8:]))\n\tcopy(key[:], buf[8:])\n\treturn key, seq\n}\n\nfunc splitStringFloat64Str(str, separator string) (string, float64) {\n\tstrList := strings.Split(str, separator)\n\tfirstItem := strList[0]\n\tsecondItem, _ := strconv2.StrToFloat64(strList[1])\n\treturn firstItem, secondItem\n}\n\nfunc getFnv32(value []byte) (uint32, error) {\n\t_, err := fnvHash.Write(value)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\thash := fnvHash.Sum32()\n\tfnvHash.Reset()\n\treturn hash, nil\n}\n\nfunc generateSeq(seq *HeadTailSeq, isLeft bool) uint64 {\n\tvar res uint64\n\tif isLeft {\n\t\tres = seq.Head\n\t\tseq.Head--\n\t} else {\n\t\tres = seq.Tail\n\t\tseq.Tail++\n\t}\n\n\treturn res\n}\n\nfunc createNewBufferWithSize(size int) *bytes.Buffer {\n\tbuf := new(bytes.Buffer)\n\tbuf.Grow(int(size))\n\treturn buf\n}\n\nfunc UvarintSize(x uint64) int {\n\ti := 0\n\tfor x >= 0x80 {\n\t\tx >>= 7\n\t\ti++\n\t}\n\treturn i + 1\n}\n"
        },
        {
          "name": "utils_test.go",
          "type": "blob",
          "size": 1.7822265625,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestMarshalInts(t *testing.T) {\n\tassertions := assert.New(t)\n\tdata, err := MarshalInts([]int{})\n\tassertions.NoError(err, \"TestMarshalInts\")\n\n\tints, err := UnmarshalInts(data)\n\tassertions.NoError(err, \"TestMarshalInts\")\n\tassertions.Equal(0, len(ints), \"TestMarshalInts\")\n\n\tdata, err = MarshalInts([]int{1, 3})\n\tassertions.NoError(err, \"TestMarshalInts\")\n\n\tints, err = UnmarshalInts(data)\n\tassertions.NoError(err, \"TestMarshalInts\")\n\tassertions.Equal(2, len(ints), \"TestMarshalInts\")\n\tassertions.Equal(1, ints[0], \"TestMarshalInts\")\n\tassertions.Equal(3, ints[1], \"TestMarshalInts\")\n}\n\nfunc TestMatchForRange(t *testing.T) {\n\tassertions := assert.New(t)\n\n\tend, err := MatchForRange(\"*\", \"hello\", func(key string) bool {\n\t\treturn true\n\t})\n\tassertions.NoError(err, \"TestMatchForRange\")\n\tassertions.False(end, \"TestMatchForRange\")\n\n\t_, err = MatchForRange(\"[\", \"hello\", func(key string) bool {\n\t\treturn true\n\t})\n\tassertions.Error(err, \"TestMatchForRange\")\n\n\tend, err = MatchForRange(\"*\", \"hello\", func(key string) bool {\n\t\treturn false\n\t})\n\tassertions.NoError(err, \"TestMatchForRange\")\n\tassertions.True(end, \"TestMatchForRange\")\n}\n"
        },
        {
          "name": "value.go",
          "type": "blob",
          "size": 0.6474609375,
          "content": "package nutsdb\n\nimport (\n\t\"sync\"\n\t\"sync/atomic\"\n)\n\ntype request struct {\n\ttx  *Tx\n\tWg  sync.WaitGroup\n\tErr error\n\tref int32\n}\n\nvar requestPool = sync.Pool{\n\tNew: func() interface{} {\n\t\treturn new(request)\n\t},\n}\n\nfunc (req *request) reset() {\n\treq.tx = nil\n\treq.Wg = sync.WaitGroup{}\n\treq.Err = nil\n\n\tatomic.StoreInt32(&req.ref, 0)\n}\n\nfunc (req *request) IncrRef() {\n\tatomic.AddInt32(&req.ref, 1)\n}\n\nfunc (req *request) DecrRef() {\n\tnRef := atomic.AddInt32(&req.ref, -1)\n\tif nRef > 0 {\n\t\treturn\n\t}\n\treq.tx = nil\n\trequestPool.Put(req)\n}\n\nfunc (req *request) Wait() error {\n\treq.Wg.Wait()\n\terr := req.Err\n\treq.DecrRef() // DecrRef after writing to DB.\n\treturn err\n}\n"
        }
      ]
    }
  ]
}