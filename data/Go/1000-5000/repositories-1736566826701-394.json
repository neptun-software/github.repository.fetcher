{
  "metadata": {
    "timestamp": 1736566826701,
    "page": 394,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "nutsdb/nutsdb",
      "stars": 3435,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.029296875,
          "content": ".idea/\ntestdata/\n\n*/*.DS_Store"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.2666015625,
          "content": "language: go\ngo:\n  - 1.11.x\n  - tip\nbefore_install:\n  - go get golang.org/x/tools/cmd/cover\n  - go get github.com/mattn/goveralls\nscript:\n  - go test -v -covermode=count -coverprofile=coverage.out\n  - $HOME/gopath/bin/goveralls -coverprofile=coverage.out -service=travis-ci"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 22.1630859375,
          "content": "## v0.1.0 （2019-2-28）\n* [New feature] Support Put/Get/Delete Operations\n* [New feature] Support TTL\n* [New feature] Support Range/Prefix Scanning\n* [New feature] Support Merge Operation\n* [New feature] Support BackUp Operation\n* [New feature] Support Bucket\n\n## v0.2.0 （2019-3-05）\n* [New feature] Support list\n* [New feature] Support set\n* [New feature] Support sorted set\n* [Bug Fix] Fix error when batch put operations\n* [Change] Update README && CHANGELOG\n\n## v0.3.0（2019-3-11）\n* [New feature] Support persistence\n* [Bug Fix] Fix when fn is nil\n* [Change] Discard mmap package\n* [Change] Discard EntryIdxMode options: HintAndRAMIdxMode and HintAndMemoryMapIdxMode\n* [Change] Add new EntryIdxMode options: HintKeyValAndRAMIdxMode and HintKeyAndRAMIdxMode\n\n## v0.4.0（2019-3-15）\n* [New feature] Support mmap loading file\n* [Bug Fix] Fix tx bug when a tx commits\n* [Change] Add rwmanager interface\n* [Change] Add new options: RWMode, SyncEnable and StartFileLoadingMode\n* [Change] Clean up some codes\n* [Change] Update README && CHANGELOG\n\n## v0.5.0 (2019-11-28)\n* [New feature] Support EntryIdxMode: HintBPTSparseIdxMode\n* [New feature] Support GetAll() function for all models\n* [Bug Fix] Fix error too many open files in system\n* [Bug Fix] Fix constant 2147483648 overflows int\n* [Bug Fix] Fix when the number of files waiting to be merged not at least 2\n* [Bug Fix] Fix data pollution when executing the merge method\n* [Change] Modify Records type && Entries type\n* [Change] Refactor for tx Commit function\n* [Change] Update Iterating over keys about README\n* [Change] Fix some grammatical mistakes about README\n* [Change] Rename variable for func ReadBPTreeRootIdxAt\n* [Change] Add issue templates\n* [Change] Update README && CHANGELOG\n\n## v0.6.0 (2021-03-21)\n* [New Feature] Add PrefixSearchScan() with regexp search ability（#53）\n* [New Feature] Allow put with timestamp (#88 )\n* [Bug Fix] Fix ZMembers bug (#58 )\n* [Bug Fix] Repeated key merge fix (#83 )\n* [Bug Fix] The LRem implementation is not consistent with the description (#92 )\n* [Refactor] Improve buildBPTreeRootIdxes file reading (#67)\n* [Docs] Update README && CHANGELOG\n\n## v0.7.0 (2022-03-06)\n* [New Feature] support im memory db (#109)\n* [New Feature] Add backup with tar+gz (#111)\n* [New Feature] Add IterateBuckets() and DeleteBucket()\n* [Refactor] refactor error (#112)\n* [Bug Fix] Windows The process cannot access the file because it is being used by another process. (#110)\n* [Docs] Update README && CHANGELOG\n\n## v0.7.1 (2022-03-06)\n* [Bug Fix] Delete buckets without persistence.(#115)\n\n## v0.8.0 (2022-04-01)\n* [Perf] optimize tx commit for batch write (#132)\n* [Bug Fix] fix: open file by variable (#118)\n* [Bug Fix] fix close file before error check（#122）\n* [Bug Fix] fix rwManager.Close order（#133）\n* [Bug Fix] fix last entry status error （#134）\n* [Bug Fix] fix: read ErrIndexOutOfBound err\n* [CI] add unit-test action （#120）\n* [Chore] add constant ErrKeyNotFound and ErrKeyNotExist (#125)\n* [Chore] chore: remove unnecessary conversion  (#126)\n* [Chore] chore(ds/list): replace for-loop with append  (#127)\n* [Chore] add push check for lint, typo  (#131)\n* [Style] style: fix typo and ineffectual assignment  (#130)\n\n## v0.9.0 (2022-06-17)\n* [Bug Fix] close file before error check &remove redundant judgments （#137） @xujiajun\n* [Bug Fix] update golang.org/x/sys to support go1.18 build （#139）@ag9920\n* [Bug Fix] when use merge, error: The process cannot access the file because it is being used by another process (#166) @xujiajun\n* [Bug Fix] fix code example. (#143) @gphper\n* [Bug Fix] merge error after delete bucket (#153) @xujiajun\n* [Perf] add fd cache(#164) @elliotchenzichang\n* [Perf] optimize sadd function inserting duplicate data leads to datafile growth (#146) @gphper\n* [Refactor] rewrite managed to support panic rollback （#136）@ag9920\n* [Refactor] errors: optimize error management (#163) @xpzouying\n* [Test] Update testcase: use testify test tools (#138) @xpzouying\n* [Test] change list and set test with table driven test and testify (#145） @bigdaronlee163\n* [Test] refactor db_test for string use testify (#147) @Rand01ph\n* [Test] add [bucket_meat/entry] unit test (#148) @gphper\n* [Test] update bptree unittest (#149) @xpzouying\n* [Test] Update tx bptree testcase (#155) @xpzouying\n* [Test] complete zset tests with testify (#151) @bigdaronlee163\n* [Test] optimization tx_bucket_test and bucket_meta_test  (#156) @gphper\n* [Test] test:complete tx_zset tests with testify (#162) @bigdaronlee163\n* [Chore] remove unused member (#157) @xpzouying\n* [Style]  format code comments etc. (#140) @moyrne\n\n## v0.10.0（2022-08-13）\n* [Bug Fix]List data structure with count parameter negative, lack of boundary judgment (#183) @andrewhzy\n* [New Feature] add LRemByIndex (#174) @Nailcui\n* [New Feature] add LKeys SKeys ZKeys API (#175) @Nailcui\n* [New Feature] add Iterator API (HintKeyAndRAMIdxMode and HintKeyValAndRAMIdxMode)(#191) @zeina1i\n* [Refactor] graceful options parameters (#185) @Nailcui\n* [Test] Add rwmanager fileio test (#170) @zeina1i\n* [Test] Improve code coverage about list  (#183) @andrewhzy\n* [Test] Test coverage improvement for inmemory  (#187) @andrewhzy\n* [Docs] A few corrections in ReadME file (#171) @kwakubiney\n\n## v0.11.0（2022-10-31）\n* [Bug Fix] In BPTSparse when combination of bucket and key is repeated (#207) @ShiMaRing\n* [Bug Fix] MInInt function compatible with 32-bit operating systems (#208) @xujiajun\n* [Bug Fix] Index EOF issue#213 (#214) @xujiajun\n* [Perf] Optimize concurrent read performance (#205) @elliotchenzichang\n* [Perf] Use biobuf optimaze startspeed (#212) @elliotchenzichang\n* [New Feature] Support reverse iterator (EntryIdxMode: HintKeyAndRAMIdxMode and HintKeyValAndRAMIdxMode) (#202) @zeina1i\n* [New Feature] Add support for IterateBuckets regularized matching (#198) @Nailcui\n* [New Feature] list all key of bucket in memory mode (#206) @Nailcui\n* [New Feature] Add PrefixScan in memory mode  (#211) @Nailcui\n* [Refactor] make default options to be created in a factory method (#196) @elliotchenzichang\n* [Refactor] use size constant value (#204) @elliotchenzichang\n* [Chore] add iterator example (#209) @xujiajun\n* [Chore] remove option StartFileLoadingMode (#218) @xujiajun\n\n## v0.11.1（2022-11-13）\n* [Bug Fix] avoid nil of it.current (#233) @mindon\n* [Bug Fix] it.current may be nil when options.Reverse is false (#234) @xujiajun\n* [Refactor] changing the lock to be one of property of the structure can make the code more readable.(#228) @elliotchenzichang\n* [New Feature] add buffer size of recovery reader as param (#230) @elliotchenzichang\n\n## v0.12.0（2023-02-26）\n* [New Feature] feat:support ttl function for list (feat:support ttl function for list #263) @xuyukeviki\n* [Bug Fix] fix: panic: db.buildIndexes error: unexpected EOF issue (panic: db.buildIndexes error: unexpected EOF #244) @xujiajun\n* [Bug Fix] issue NewIterator with Reverse=true stop at 28 records #250 :: Andrew :: bug fixed (issue #250 :: Andrew :: bug fixed #266) @andrewhzy\n* [Bug Fix] Fix issues LIST \"start or end error\" after deleting the last item from list  #280: LIST \"start or end error\" after deleting the last item Fix issues #280: LIST \"start or end error\" after deleting the last item #282 @ShawnHXH\n* [Performance] Use file recovery in merge (Use file recovery in merge #259) @elliotchenzichang\n* [Performance] perf(read): reduce one read I/O perf(read): reduce one read I/O #271 @lugosix\n* [Refactor] add options function of BufferSizeOfRecovery (add options function of BufferSizeOfRecovery #236) @elliotchenzichang\n* [Refactor] add fd release logic to file recovery reader (add fd release logic to file recovery reader #242 ) @elliotchenzichang\n* [Refactor] rebuild parse meta func (rebuild parse meta func #243 ) @elliotchenzichang\n* [Refactor] change the xujiajun/nutsdb -> nutsdb/nutsdb change the xujiajun/nutsdb -> nutsdb/nutsdb #281 @elliotchenzichang\n* [Refactor] Update doc Update doc #285 @elliotchenzichang\n* [Refactor] Fix verify logic Fix verify logic #286 @elliotchenzichang\n* [Refactor] fix: issue (Error description problem about IsPrefixSearchScan #288) fix: issue (#288) #289 @CodePrometheus\n* [Refactor] docs: fix typo( docs: fix typo #252) @icpd\n* [Refactor] fix a typo in code fix a typo #291 @elliotchenzichang\n* [UnitTest] Adding a test of readEntry() and refacting readEntry() (Adding a test of readEntry() and refacting readEntry() #237 ) @wangxuanni\n* [UnitTest] test coverage improvement (fix: issue (#213) #214 ) @andrewhzy\n* [UnitTest] adding a test for IsKeyEmpty func in github.com/xujiajun/nutsdb/errors.go:26 (adding a test for IsKeyEmpty func in github.com/xujiajun/nutsdb/errors.go:26: #265) @lyr-2000\n* [UnitTest] Add test for SetKeyPosMap in nutsdb/bptree.go (Add test for SetKeyPosMap in nutsdb/bptree.go #268) @ShawnHXH\n* [UnitTest] Add test for ToBinary in bptree.go Add test for ToBinary in bptree.go #272 @ShawnHXH\n* [UnitTest] Add test for bucket in errors.go Add test for bucket in errors.go #278 @ShawnHXH\n* [UnitTest] add test for rwmanger_mmap.Release add test for rwmanger_mmap.Release #283 @vegetabledogdog\n* [UnitTest] test: add tests for IsDBClosed, IsPrefixScan and IsPrefixSearchScan test: add tests for IsDBClosed, IsPrefixScan and IsPrefixSearchScan #290 @CodePrometheus\n\n## v0.12.1（2023-05-19）\n* [Bug Fix] fix delete non exist will not raise error bug by @elliotchenzichang in #331\n* [Bug Fix] issue #306 - added a MAX_SIZE const that fits 32 and 64bit arch by @hivenet-philippe in #308\n* [New Feature] add GetListTTL by @wangxuanni in #316\n* [Refactor] delete a repeat error validation logic by @elliotchenzichang in #324\n* [Refactor] make bucket value as a property in entry by @elliotchenzichang in #323\n* [Refactor] move crc property into meta struct by @elliotchenzichang in #325\n* [Refactor] delete the position property of entry struct by @elliotchenzichang in #326\n* [Refactor] Refactor-actor method isFilterEntry by @elliotchenzichang in #327\n* [Refactor] add function return the status of DB by @elliotchenzichang in #329\n* [Refactor] add status management by @elliotchenzichang in #330\n* [Refactor] rebuild the status management code by @elliotchenzichang in #332\n* [Refactor] delete the param of writelen by @elliotchenzichang in #333\n* [Refactor] Refactor entry length check by @elliotchenzichang in #334\n* [Test] Add test for PutWithTimestamp in tx.go by @rayz in #307\n* [Docs] docs(readme): format code by @rfyiamcool in #319\n\n## v0.12.2（2023-05-21）\n* [Bug Fix] fix ignore bucket when db recovering by @elliotchenzichang in #336\n\n## v0.12.3（2023-06-23）\n* [Bug Fix] fix the bucket issue by @elliotchenzichang in #337\n* [Bug Fix] fix: err desc for ErrWhenBuildListIdx by @xujiajun in #338\n* [Bug Fix] fix: r.E.Bucket err by @xujiajun in #341\n\n## v0.12.4（2023-07-25）\n* [Bug Fix] fix: remove unnecessary null checks when writing to a list. by @bigboss2063 in #353\n* [Bug Fix] fix the bug of nil entry by @elliotchenzichang in #377\n* [Bug Fix] bug fix, add defer function to release lock to avoid deadlock by @elliotchenzichang in #356\n* [Bug Fix] Fix the bug of nil entry by @elliotchenzichang in #380\n* [Bug Fix] fix: deadlock caused by error by @lyl156 in #371\n* [New Feature] feature: make all objects set to nil after the user calls the close f… by @tobehardest in #374\n* [New Feature] feat: implement file lock by @bigboss2063 in #372\n* [Refactor] just move the index structure in a rightful space by @elliotchenzichang in #343\n* [Refactor] fix some issue in go mod config by @elliotchenzichang in #345\n* [Refactor] rebuild the index module for index struct by @elliotchenzichang in #346\n* [Refactor] rebuild the add function in list index by @elliotchenzichang in #350\n* [Refactor] use bytes.Equal instead bytes.Compare by @testwill in #355\n* [Refactor] rebuild the way to create hint object by @elliotchenzichang in #357\n* [Refactor] rebuild isFilter function by @elliotchenzichang in #358\n* [Refactor] rebuild the way to new Entry Object by @elliotchenzichang in #359\n* [Refactor] add comments in entry file by @elliotchenzichang in #362\n* [Refactor] rebuilt the way to new Record object by @elliotchenzichang in #365\n* [Refactor] rebuild part of recovery logic by @elliotchenzichang in #366\n* [Refactor] refactor: use a const to replace magic string by @bigboss2063 in #376\n* [Test] Add test WithNodeNum in nutsdb/options.go by @dongzhiwei-git in #361\n* [Test] test: test WithRWMode by @JingchenZhang in #368\n* [Test] test: test rw manager mmap by @lyl156 in #363\n* [Test] add test case for ErrWhenBuildListIdx func in db.go and optimize enqueue func in bptree.go and add test case by @damotiansheng in #370\n* [Test] test with ReadNode ,WithCleanFdsCacheThreshold,WithMaxFdNumsInCache,WithSyncEnable by @JingchenZhang in #369\n* [Test] test: rebuild unit tests in db_test.go(issue#374, task 4) by @bigboss2063 in #375\n* [Chore] chore: remove element in cache on closing fd manager by @lyl156 in #364\n\n## v0.12.6（2023-07-26）\n* [Refactor] refactor: refactoring the initialization way of the MetaData by @bigboss2063 in #381\n* [Refactor] Small scope refactoring code:move some util func to utils.go from db.go by @damotiansheng in #379\n\n## v0.13.0（2023-08-01）\n* [Bug Fix] fix: fix the bug that allows deletion of a non-existent bucket. by @bigboss2063 in #388\n* [Perf] pref: remove sync.Pool and prev allocate buffer for small tx by @bigboss2063 in #384\n* [New Feature] feat: implement automatic merging by @bigboss2063 in #393\n* [New Feature] feat: add custom comparator by @lyl156 in #389\n* [New Feature] feat: refactor list data structure by using doubly linked list and support HintKeyAndRAMIdxMode by @bigboss2063 in #390\n* [Docs] doc: update options doc by @lyl156 in #391\n* [Docs] doc: add a new option and update the default option by @bigboss2063 in #396\n* [Chore] chore: add error handler for error occurred during transaction by @lyl156 in #383\n## v0.14.0 (2023-9-1)\n### Features\n- feat: make Set support HintKeyAndRAMIdxMode by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/397\n- add batch write by @damotiansheng in https://github.com/nutsdb/nutsdb/pull/398\n- fix: fixed the issue where deletion did not actually remove the record from memory. by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/407\n- feat: make sorted set to support HintKeyAndRAMIdxMode by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/426\n- pref: refactor index item to save memory usage by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/434\n- feat: implemented configurable automatic expiration deletion, optional time heap or time wheel by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/435\n### BugFix\n- repair tx.SMembers bug by @damotiansheng in https://github.com/nutsdb/nutsdb/pull/404\n- fix: fix dead lock during merging by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/417\n- [bugFix]Add tx data handle logic in recovery by @elliotchenzichang in https://github.com/nutsdb/nutsdb/pull/418\n- [bug fix]add skip entry zero error when recovery by @elliotchenzichang in https://github.com/nutsdb/nutsdb/pull/420\n- fix: fix the problem that an error occurs when switching between the first and the second mode at startup by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/423\n- fix: recover panic when executing tx by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/429\n- fix: fix the problem of allowing delete the non-exist member of a set by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/432\n- fix: fix the bugs of ttl effective time and IsExpired method  by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/437\n- [bugFix]fix the bug in prase data in an tx by @elliotchenzichang in https://github.com/nutsdb/nutsdb/pull/441\n- fix: fix bug in `txZCard` by @G-XD in https://github.com/nutsdb/nutsdb/pull/444\n- [bugFix]fix the bug in data in tx logic by @elliotchenzichang in https://github.com/nutsdb/nutsdb/pull/453\n- fix: merge not allowed when using list type (temporary fix) by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/457\n### Other\n- create dir if not exist by @moyrne in https://github.com/nutsdb/nutsdb/pull/399\n- optimize parseDataFiles error check by @moyrne in https://github.com/nutsdb/nutsdb/pull/401\n- [Test] Restart the database using three modes by @RuiHuaLiu2023 in https://github.com/nutsdb/nutsdb/pull/406\n- chore: update ci.yml by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/409\n- style: remove useless code by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/411\n- chore: add change log by @xujiajun in https://github.com/nutsdb/nutsdb/pull/412\n- ref: use goto refactor recovery logic by @elliotchenzichang in https://github.com/nutsdb/nutsdb/pull/414\n- [ref]delete the map db.committedTxIds, it will not needed any more by @elliotchenzichang in https://github.com/nutsdb/nutsdb/pull/421\n- [ref]rebuild the recovery logic, and delete the unconfirmedRecordList by @elliotchenzichang in https://github.com/nutsdb/nutsdb/pull/422\n- doc: update README.md and add comments for tx_zset.go by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/427\n- delete getActiveFileWriteOff func by @damotiansheng in https://github.com/nutsdb/nutsdb/pull/428\n- [ref] rebuild the error handling logic in recovery by @elliotchenzichang in https://github.com/nutsdb/nutsdb/pull/430\n- refactor: unified delete bucket in buildNotDSIdxes by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/433\n- doc: update README.md and README-CN.md by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/436\n- refactor: use chained calls instead of direct initialization by @bigboss2063 in https://github.com/nutsdb/nutsdb/pull/439\n- [ref] move the errors in db file to another separated file. by @elliotchenzichang in https://github.com/nutsdb/nutsdb/pull/440\n- [refactor] Refactoring commit  by @RuiHuaLiu2023 in https://github.com/nutsdb/nutsdb/pull/438\n- test: refactor `tx_set_test` by @G-XD in https://github.com/nutsdb/nutsdb/pull/445\n- test: refactor `tx_bucket_test` by @G-XD in https://github.com/nutsdb/nutsdb/pull/446\n- [ref]move the tx error to separated file by @elliotchenzichang in https://github.com/nutsdb/nutsdb/pull/447\n- refactor: remove func ErrWhenBuildListIdx by @TremblingV5 in https://github.com/nutsdb/nutsdb/pull/443\n\n## v0.14.1 (2023-9-16)\n\n### Features\n\n- add max write records control by [@damotiansheng](https://github.com/damotiansheng) in [#459](https://github.com/nutsdb/nutsdb/pull/459)\n- feat(doc): organize README.md by [@hanxuanliang](https://github.com/hanxuanliang) in [#461](https://github.com/nutsdb/nutsdb/pull/461)\n\n### BugFix\n\n- fix: deadlock when test failed by [@G-XD](https://github.com/G-XD) in [#464](https://github.com/nutsdb/nutsdb/pull/464)\n- fix: fix the bug of ttl when restart by [@bigboss2063](https://github.com/bigboss2063) in [#466](https://github.com/nutsdb/nutsdb/pull/466)\n\n### Other\n\n- refactor: Remove the 3rd mode by [@bigboss2063](https://github.com/bigboss2063) in [#456](https://github.com/nutsdb/nutsdb/pull/456)\n- style: reduce duplicate code by [@bigboss2063](https://github.com/bigboss2063) in [#458](https://github.com/nutsdb/nutsdb/pull/458)\n- refactor: refactor index by generics by [@G-XD](https://github.com/G-XD) in [#462](https://github.com/nutsdb/nutsdb/pull/462)\n\n## v0.14.2 (2023-11-06)\n\n### Features\n\n- finish auto merge by [@damotiansheng](https://github.com/damotiansheng) in [#471](https://github.com/nutsdb/nutsdb/pull/471)\n\n### BugFix\n\n- fix: return ErrListNotFound when calling tx.LRemByIndex with non-existent bucket name by [@TremblingV5](https://github.com/TremblingV5) in [#470](https://github.com/nutsdb/nutsdb/pull/470)\n\n### Other\n\n- list index refractor by [@damotiansheng](https://github.com/damotiansheng) in [#467](https://github.com/nutsdb/nutsdb/pull/467)\n- refactor: refactor unit test of tx_list by [@TremblingV5](https://github.com/TremblingV5) in [#468](https://github.com/nutsdb/nutsdb/pull/468)\n- [ref]add a function that can automatically get the disk size of meta header by [@elliotchenzichang](https://github.com/elliotchenzichang) in [#478](https://github.com/nutsdb/nutsdb/pull/478)\n- refactor: Optimize the code structure so that DB and Tx use a set of indexing processes by [@bigboss2063](https://github.com/bigboss2063) in [#479](https://github.com/nutsdb/nutsdb/pull/479)\n- style: rename or remove some code by [@bigboss2063](https://github.com/bigboss2063) in [#480](https://github.com/nutsdb/nutsdb/pull/480)\n- [ref]add a common function to get the special size buffer object by [@elliotchenzichang](https://github.com/elliotchenzichang) in [#482](https://github.com/nutsdb/nutsdb/pull/482)\n- ref: refactor `buildBTreeIdx` method by [@bigboss2063](https://github.com/bigboss2063) in [#483](https://github.com/nutsdb/nutsdb/pull/483)\n- ref: use errors.Is to remove useless code by [@bigboss2063](https://github.com/bigboss2063) in [#485](https://github.com/nutsdb/nutsdb/pull/485)\n- decrease batch write unit test time by [@damotiansheng](https://github.com/damotiansheng) in [#487](https://github.com/nutsdb/nutsdb/pull/487)\n\n## v0.14.3 (2023-11-21)\n\n### BugFix\n\n- fix: use constants instead of iota to prevent forward compatibility. by [@bigboss2063](https://github.com/bigboss2063) in [#490](https://github.com/nutsdb/nutsdb/pull/490)\n\n## v1.0.0 (2023-11-27)\n\n### Features\n\n- [feat]Serperate bucket from entry and build the bucket management system by [@elliotchenzichang](https://github.com/elliotchenzichang) in [#484](https://github.com/nutsdb/nutsdb/pull/484)\n- add lru cache for second index mode of HintKeyAndRAMIdxMode by [@damotiansheng](https://github.com/damotiansheng) in [#495](https://github.com/nutsdb/nutsdb/pull/495)\n- add more api for bucket by [@elliotchenzichang](https://github.com/elliotchenzichang) in [#502](https://github.com/nutsdb/nutsdb/pull/502)\n- feat: use variable length storage to implement storage protocols to save disk space by [@bigboss2063](https://github.com/bigboss2063) in [#501](https://github.com/nutsdb/nutsdb/pull/501)\n- pref: reduce unnecessary fields in Record and save memory by [@bigboss2063](https://github.com/bigboss2063) in [#506](https://github.com/nutsdb/nutsdb/pull/506)\n\n### BugFix\n\n- Only overwrite the managed error when a rollback error occurs by [@xy3](https://github.com/xy3) in [#493](https://github.com/nutsdb/nutsdb/pull/493)\n- add lru cache when commit only for HintKeyAndRAMIdxMode by [@damotiansheng](https://github.com/damotiansheng) in [#504](https://github.com/nutsdb/nutsdb/pull/504)\n- fix: use sync.Map to avoid data race by [@bigboss2063](https://github.com/bigboss2063) in [#509](https://github.com/nutsdb/nutsdb/pull/509)\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.267578125,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at me@xujiajun.cn. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 2.0478515625,
          "content": "# Contributing to NutsDB\n\n:+1::tada: First off, thanks for taking the time to contribute! :tada::+1:\n\nWe love your input! We want to make contributing to this project as easy and transparent as possible, whether it's:\n\n- Reporting a bug\n- Discussing the current state of the code\n- Submitting a fix\n- Proposing new features\n- Contributing Documentation\n- Contributing a Performance Improvement\n\nBy contributing to NutsDB, you agree to abide by the [code of conduct](https://github.com/xujiajun/nutsdb/blob/master/CODE_OF_CONDUCT.md).\n \n## Getting Started\nIf you are looking to contribute to the NutsDB, the best place to start is the [GitHub \"issues\"](https://github.com/xujiajun/nutsdb/issues) tab. This is also a great place for filing bug reports and making suggestions for ways in which we can improve the code and documentation.\n\n## Reporting bugs\n\nSee [reporting bugs](https://github.com/xujiajun/nutsdb/blob/master/.github/ISSUE_TEMPLATE/bug_report.md) for details about reporting any issues.\n\n## Feature request\n\nsee [feature request](https://github.com/xujiajun/nutsdb/blob/master/.github/ISSUE_TEMPLATE/feature_request.md) for details about reporting any feature requests.\n\n## Contribution flow\n\nPull requests are the best way to propose changes to the codebase (we use [Github Flow](https://guides.github.com/introduction/flow/index.html)). We actively welcome your pull requests:\n\n1. Fork the repo and create your branch from `master`.\n2. If you've added code that should be tested, add tests.\n3. If you've changed APIs, update the documentation.\n4. Ensure the test suite passes.\n5. Make sure your code lints.\n6. Issue that pull request!\n\n## Use a Consistent Coding Style\n\nThe coding style suggested by the Golang community is used in NutsDB. See the style [doc](https://github.com/golang/go/wiki/CodeReviewComments) for details.\n\nPlease follow this style to make NutsDB easy to review, maintain and develop.\n\n## License\nBy contributing, you agree that your contributions will be licensed under its [Apache License 2.0](https://github.com/xujiajun/nutsdb/blob/master/LICENSE) License.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README-CN.md",
          "type": "blob",
          "size": 75.6376953125,
          "content": "<p align=\"center\">\r\n    <img src=\"https://user-images.githubusercontent.com/6065007/141310364-62d7eebb-2cbb-4949-80ed-5cd20f705405.png\">\r\n</p>\r\n\r\n# NutsDB [![GoDoc](https://godoc.org/github.com/nutsdb/nutsdb?status.svg)](https://godoc.org/github.com/nutsdb/nutsdb)  [![Go Report Card](https://goreportcard.com/badge/github.com/nutsdb/nutsdb)](https://goreportcard.com/report/github.com/nutsdb/nutsdb)  [![Go](https://github.com/nutsdb/nutsdb/workflows/Go/badge.svg?branch=master)](https://github.com/nutsdb/nutsdb/actions) [![codecov](https://codecov.io/gh/nutsdb/nutsdb/branch/master/graph/badge.svg?token=CupujOXpbe)](https://codecov.io/gh/nutsdb/nutsdb) [![License](http://img.shields.io/badge/license-Apache_2-blue.svg?style=flat-square)](https://raw.githubusercontent.com/nutsdb/nutsdb/master/LICENSE) [![Mentioned in Awesome Go](https://awesome.re/mentioned-badge.svg)](https://github.com/avelino/awesome-go#database)  \r\n\r\n[English](https://github.com/nutsdb/nutsdb/blob/master/README.md) | 简体中文\r\n\r\nNutsDB 是一个用纯 Go 编写的简单、快速、可嵌入且持久的键/值存储。\r\n\r\n它支持完全可序列化的事务以及 List、Set、SortedSet 等多种数据结构。 所有操作都发生在 Tx 内部。 Tx 代表一个事务，可以是只读的，也可以是读写的。 只读事务可以读取给定存储桶和给定键的值或迭代一组键值对。 读写事务可以从数据库中读取、更新和删除键。\r\n\r\n我们可以在NutsDB的文档网站了解更多：[NutsDB Documents](https://nutsdb.github.io/nutsdb-docs/)\r\n\r\n欢迎对NutsDB感兴趣的加群、一起开发，具体看这个issue：https://github.com/nutsdb/nutsdb/issues/116。\r\n\r\n### 关注nutsdb公众号\r\n <img src=\"https://user-images.githubusercontent.com/6065007/221391600-4f53e966-c376-426e-9dec-27364a0704d1.png\"   height = \"300\" alt=\"nutsdb公众号\" align=center />\r\n\r\n\r\n### 公告\r\n* v1.0.0 发布，详情见： [https://github.com/nutsdb/nutsdb/releases/tag/v1.0.0](https://github.com/nutsdb/nutsdb/releases/tag/v1.0.0)\r\n* v0.14.3 发布，详情见： [https://github.com/nutsdb/nutsdb/releases/tag/v0.14.3](https://github.com/nutsdb/nutsdb/releases/tag/v0.14.3)\r\n* v0.14.2 发布，详情见：[https://github.com/nutsdb/nutsdb/releases/tag/v0.14.2](https://github.com/nutsdb/nutsdb/releases/tag/v0.14.2)\r\n* v0.14.1 发布，详情见：[https://github.com/nutsdb/nutsdb/releases/tag/v0.14.1](https://github.com/nutsdb/nutsdb/releases/tag/v0.14.1)\r\n\r\n📢 注意：从v0.9.0开始，**DefaultOptions** 里面的 **defaultSegmentSize** 做了调整从原来的 **8MB** 变成了 **256MB**，如果你原来设置 256MB 不用改，如果原来使用的是默认值的，需要手动改成 8MB，不然原来的数据不会解析。这边的大小调整原因是从 v0.9.0 开始有对文件描述符的缓存（详解见PR https://github.com/nutsdb/nutsdb/issues/164 ），所以需要用户看下自己的文件描述符数量，有不清楚可以提issue或者群里问。\r\n\r\n**nutsdb v1.0.0 **之后，由于底层数据存储协议的变化，**旧版本的数据不兼容**。 请在使用新版本之前重写数据。 并且当前的 Bucket 需要手动创建。 详细请参见Bucket使用[文档](./docs/user_guides/use-buckets.md)。\r\n\r\n### 学习资料\r\n\r\nhttps://www.bilibili.com/video/BV1T34y1x7AS/\r\n\r\n## 架构设计\r\n![nutsdb-架构图](./docs/img/nutsdb-架构图.png)\r\n\r\n\r\n\r\n\r\n## 目录\r\n\r\n- [NutsDB        ](#nutsdb--------)\r\n    - [关注nutsdb公众号](#关注nutsdb公众号)\r\n    - [公告](#公告)\r\n    - [学习资料](#学习资料)\r\n  - [架构设计](#架构设计)\r\n  - [目录](#目录)\r\n  - [入门指南](#入门指南)\r\n    - [安装](#安装)\r\n    - [开启数据库](#开启数据库)\r\n    - [选项配置](#选项配置)\r\n      - [默认选项](#默认选项)\r\n    - [使用事务](#使用事务)\r\n      - [读写事务](#读写事务)\r\n      - [只读事务](#只读事务)\r\n      - [手动管理事务](#手动管理事务)\r\n    - [使用buckets](#使用buckets)\r\n      - [迭代buckets](#迭代buckets)\r\n      - [删除bucket](#删除bucket)\r\n    - [使用键值对](#使用键值对)\r\n      - [基本操作](#基本操作)\r\n      - [对值的位操作](#对值的位操作)\r\n      - [对值的自增和自减操作](#对值的自增和自减操作)\r\n      - [对值的连续多次Set和Get](#对值的连续多次set和get)\r\n      - [对值的增补操作](#对值的增补操作)\r\n      - [获取值的一部分](#获取值的一部分)\r\n    - [使用TTL](#使用ttl)\r\n    - [对keys的扫描操作](#对keys的扫描操作)\r\n      - [前缀扫描](#前缀扫描)\r\n      - [前缀后的正则扫描](#前缀后的正则扫描)\r\n      - [范围扫描](#范围扫描)\r\n    - [获取全部的key和value](#获取全部的key和value)\r\n    - [迭代器](#迭代器)\r\n      - [正向的迭代器](#正向的迭代器)\r\n      - [反向的迭代器](#反向的迭代器)\r\n    - [合并操作](#合并操作)\r\n    - [数据库备份](#数据库备份)\r\n    - [使用内存模式](#使用内存模式)\r\n    - [使用其他数据结构](#使用其他数据结构)\r\n      - [List](#list)\r\n        - [RPush](#rpush)\r\n        - [LPush](#lpush)\r\n        - [LPop](#lpop)\r\n        - [LPeek](#lpeek)\r\n        - [RPop](#rpop)\r\n        - [RPeek](#rpeek)\r\n        - [LRange](#lrange)\r\n        - [LRem](#lrem)\r\n        - [LRemByIndex](#lrembyindex)\r\n        - [LTrim](#ltrim)\r\n        - [LSize](#lsize)\r\n        - [LKeys](#lkeys)\r\n      - [Set](#set)\r\n        - [SAdd](#sadd)\r\n        - [SAreMembers](#saremembers)\r\n        - [SCard](#scard)\r\n        - [SDiffByOneBucket](#sdiffbyonebucket)\r\n        - [SDiffByTwoBuckets](#sdiffbytwobuckets)\r\n        - [SHasKey](#shaskey)\r\n        - [SIsMember](#sismember)\r\n        - [SMembers](#smembers)\r\n        - [SMoveByOneBucket](#smovebyonebucket)\r\n        - [SMoveByTwoBuckets](#smovebytwobuckets)\r\n        - [SPop](#spop)\r\n        - [SRem](#srem)\r\n        - [SUnionByOneBucket](#sunionbyonebucket)\r\n        - [SUnionByTwoBuckets](#sunionbytwobuckets)\r\n        - [SKeys](#skeys)\r\n      - [Sorted Set](#sorted-set)\r\n        - [ZAdd](#zadd)\r\n        - [ZCard](#zcard)\r\n        - [ZCount](#zcount)\r\n        - [ZGetByKey](#zgetbykey)\r\n        - [ZMembers](#zmembers)\r\n        - [ZPeekMax](#zpeekmax)\r\n        - [ZPeekMin](#zpeekmin)\r\n        - [ZPopMax](#zpopmax)\r\n        - [ZPopMin](#zpopmin)\r\n        - [ZRangeByRank](#zrangebyrank)\r\n        - [ZRangeByScore](#zrangebyscore)\r\n        - [ZRank](#zrank)\r\n      - [ZRevRank](#zrevrank)\r\n        - [ZRem](#zrem)\r\n        - [ZRemRangeByRank](#zremrangebyrank)\r\n        - [ZScore](#zscore)\r\n        - [ZKeys](#zkeys)\r\n    - [与其他数据库的比较](#与其他数据库的比较)\r\n      - [BoltDB](#boltdb)\r\n      - [LevelDB, RocksDB](#leveldb-rocksdb)\r\n      - [Badger](#badger)\r\n    - [Benchmarks](#benchmarks)\r\n  - [测试的环境:](#测试的环境)\r\n  - [Benchmark的结果:](#benchmark的结果)\r\n  - [结论:](#结论)\r\n    - [写性能:](#写性能)\r\n    - [读性能:](#读性能)\r\n    - [警告和限制](#警告和限制)\r\n      - [隔离级别低到高：](#隔离级别低到高)\r\n        - [1）未提交读（READ UNCOMMITTED）](#1未提交读read-uncommitted)\r\n        - [2）在提交读（READ COMMITTED）](#2在提交读read-committed)\r\n        - [3）在可重复读（REPEATABLE READS）](#3在可重复读repeatable-reads)\r\n        - [4）可串行化 （Serializable）](#4可串行化-serializable)\r\n    - [联系作者](#联系作者)\r\n    - [参与贡献](#参与贡献)\r\n      - [代码风格指南参考](#代码风格指南参考)\r\n      - [git commit 规范参考](#git-commit-规范参考)\r\n      - [type的参考](#type的参考)\r\n    - [致谢](#致谢)\r\n    - [License](#license)\r\n\r\n## 入门指南\r\n\r\n### 安装\r\n\r\nNutsDB的安装很简单，首先保证 [Golang](https://golang.org/dl/) 已经安装好 (版本要求1.11以上). 然后在终端执行命令:\r\n\r\n```\r\ngo get -u github.com/nutsdb/nutsdb\r\n```\r\n\r\n### 开启数据库\r\n\r\n要打开数据库需要使用` nutsdb.Open()`这个方法。其中用到的选项(options)包括 `Dir` , `EntryIdxMode`和 `SegmentSize`，在调用的时候这些参数必须设置。官方提供了`DefaultOptions`的选项，直接使用`nutsdb.DefaultOptions`即可。当然你也可以根据需要自己定义。\r\n\r\n例子： \r\n\r\n```golang\r\npackage main\r\n\r\nimport (\r\n    \"log\"\r\n\r\n    \"github.com/nutsdb/nutsdb\"\r\n)\r\n\r\nfunc main() {\r\n    db, err := nutsdb.Open(\r\n        nutsdb.DefaultOptions,\r\n        nutsdb.WithDir(\"/tmp/nutsdb\"), // 数据库会自动创建这个目录文件\r\n    )\r\n    if err != nil {\r\n        log.Fatal(err)\r\n    }\r\n    defer db.Close()\r\n\r\n    ...\r\n}\r\n```\r\n\r\n### 选项配置\r\n\r\n* Dir                  string  \r\n\r\n`Dir` 代表数据库存放数据的目录\r\n\r\n* EntryIdxMode         EntryIdxMode \r\n\r\n`EntryIdxMode` 代表索引entry的模式. \r\n`EntryIdxMode` 包括选项: `HintKeyValAndRAMIdxMode` 、 `HintKeyAndRAMIdxMode`和 `HintBPTSparseIdxMode`。\r\n\r\n其中`HintKeyValAndRAMIdxMode` 代表纯内存索引模式（key和value都会被cache）。\r\n`HintKeyAndRAMIdxMode` 代表内存+磁盘的索引模式（只有key被cache）。\r\n`HintBPTSparseIdxMode`（v0.4.0之后的版本支持） 是专门节约内存的设计方案，单机10亿条数据，只要80几M内存。但是读性能不高，需要自己加缓存来加速。\r\n\r\n* RWMode               RWMode  \r\n\r\n`RWMode` 代表读写模式. `RWMode` 包括两种选项: `FileIO` and `MMap`.\r\n`FileIO` 用标准的 I/O读写。 `MMap` 代表使用mmap进行读写。\r\n\r\n* SegmentSize          int64 \r\n\r\n `SegmentSize` 代表数据库的数据单元，每个数据单元（文件）为`SegmentSize`，现在默认是8。**注意：从大于0.8.0版本开始，默认SegmentSize变成256MB**\r\nMB，这个可以自己配置。但是一旦被设置，下次启动数据库也要用这个配置，不然会报错。详情见 [限制和警告](https://github.com/nutsdb/nutsdb/blob/master/README-CN.md#%E8%AD%A6%E5%91%8A%E5%92%8C%E9%99%90%E5%88%B6)。\r\n\r\n* NodeNum              int64\r\n\r\n`NodeNum` 代表节点的号码.默认 NodeNum是 1. `NodeNum` 取值范围 [1,1023] 。\r\n\r\n* SyncEnable           bool\r\n\r\n`SyncEnable` 代表调用了 Sync() 方法.\r\n如果 `SyncEnable` 为 false， 写性能会很高，但是如果遇到断电或者系统奔溃，会有数据丢失的风险。\r\n如果  `SyncEnable` 为 true，写性能会相比false的情况慢很多，但是数据更有保障，每次事务提交成功都会落盘。\r\n\r\n* StartFileLoadingMode RWMode\r\n\r\n`StartFileLoadingMode` 代表启动数据库的载入文件的方式。参数选项同`RWMode`。\r\n\r\n* GCWhenClose bool\r\n\r\n`GCWhenClose` 表示调用 ```db.Close()``` 时主动 GC。Nutsdb 预设不会立即在 ```db.Close()``` 时触发 GC.\r\n\r\n* CommitBufferSize int64\r\n\r\n`CommitBufferSize` 表示为事务预分配的内存大小。Nutsdb 将预分配内存以减少内存分配的次数。\r\n\r\n* ErrorHandler ErrorHandler\r\n\r\n`ErrorHandler` 处理事务执行期间发生的错误。\r\n\r\n* LessFunc LessFunc\r\n\r\n`LessFunc` 表示对 key 进行排序的函数。Nutsdb 默认按字典序对 key 进行排序。\r\n\r\n* MergeInterval time.Duration\r\n\r\n`MergeInterval` 表示自动化 Merge 的间隔，0 表示不触发自动化 Merge，默认间隔为 2 小时。\r\n\r\n- MaxBatchCount int64\r\n\r\n`MaxBatchCount` 表示批量写入的最大条数。\r\n\r\n- MaxBatchSize int64\r\n\r\n`MaxBatchSize` 表示批量写入的最大字节数。\r\n\r\n- ExpiredDeleteType ExpiredDeleteType\r\n\r\n`ExpiredDeleteType ` 表示用于自动过期删除的数据结构。TimeWheel 意味着使用时间轮，你可以在需要高性能或者内存会充足的时候使用。TimeHeap 意味着使用时间轮，你可以在需要高精度删除或者内存将吃紧的时候使用。\r\n\r\n\r\n\r\n#### 默认选项\r\n\r\n推荐使用默认选项的方式。兼顾了持久化+快速的启动数据库。当然具体还要看你场景的要求。\r\n\r\n> 以下配置是比较保守的方式。\r\n> 如果你对写性能要求比较高，可以设置SyncEnable等于false，RWMode改成MMap，写性能会得到极大提升，缺点是可能会丢数据（例如遇到断电或者系统奔溃）\r\n\r\n```\r\nvar DefaultOptions = func() Options {\r\n\treturn Options{\r\n\t\tEntryIdxMode:      HintKeyValAndRAMIdxMode,\r\n\t\tSegmentSize:       defaultSegmentSize,\r\n\t\tNodeNum:           1,\r\n\t\tRWMode:            FileIO,\r\n\t\tSyncEnable:        true,\r\n\t\tCommitBufferSize:  4 * MB,\r\n\t\tMergeInterval:     2 * time.Hour,\r\n\t\tMaxBatchSize:      (15 * defaultSegmentSize / 4) / 100,\r\n\t\tMaxBatchCount:     (15 * defaultSegmentSize / 4) / 100 / 100,\r\n\t\tExpiredDeleteType: TimeWheel,\r\n\t}\r\n}()\r\n```\r\n\r\n### 使用事务\r\n\r\nNutsDB为了保证隔离性，防止并发读写事务时候数据的不一致性，同一时间只能执行一个读写事务，但是允许同一时间执行多个只读事务。\r\n从v0.3.0版本开始，NutsDB遵循标准的ACID原则。（参见[限制和警告](https://github.com/nutsdb/nutsdb/blob/master/README-CN.md#%E8%AD%A6%E5%91%8A%E5%92%8C%E9%99%90%E5%88%B6)）\r\n\r\n\r\n#### 读写事务\r\n\r\n```golang\r\nerr := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n    ...\r\n    return nil\r\n})\r\n\r\n```\r\n\r\n#### 只读事务\r\n\r\n```golang\r\nerr := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n    ...\r\n    return nil\r\n})\r\n\r\n```\r\n\r\n#### 手动管理事务\r\n\r\n从上面的例子看到 `DB.View()` 和`DB.Update()` 这两个是数据库调用事务的主要方法。他们本质上是基于 `DB.Begin()`方法进行的包装。他们可以帮你自动管理事务的生命周期，从事务的开始、事务的执行、事务提交或者回滚一直到事务的安全的关闭为止，如果中间有错误会返回。所以**一般情况下推荐用这种方式去调用事务**。\r\n\r\n这好比开车有手动挡和自动挡一样， `DB.View()` 和`DB.Update()`等于提供了自动档的效果。\r\n\r\n如果你需要手动去开启、执行、关闭事务，你会用到`DB.Begin()`方法开启一个事务，`tx.Commit()` 方法用来提交事务、`tx.Rollback()`方法用来回滚事务\r\n\r\n例子：\r\n\r\n```golang\r\n//开始事务\r\ntx, err := db.Begin(true)\r\nif err != nil {\r\n    return err\r\n}\r\n\r\nbucket := \"bucket1\"\r\nkey := []byte(\"foo\")\r\nval := []byte(\"bar\")\r\n\r\n// 使用事务\r\nif err = tx.Put(bucket, key, val, nutsdb.Persistent); err != nil {\r\n    // 回滚事务\r\n    tx.Rollback()\r\n} else {\r\n    // 提交事务\r\n    if err = tx.Commit(); err != nil {\r\n        tx.Rollback()\r\n        return err\r\n    }\r\n}\r\n```\r\n\r\n### 使用buckets\r\n\r\nbuckets中文翻译过来是桶的意思，你可以理解成类似mysql的table表的概念，也可以理解成命名空间，或者多租户的概念。\r\n所以你可以用他存不同的key的键值对，也可以存相同的key的键值对。所有的key在一个bucket里面不能重复。\r\n\r\n例子：\r\n\r\n```golang\r\n\r\nkey := []byte(\"key001\")\r\nval := []byte(\"val001\")\r\n\r\nbucket001 := \"bucket001\"\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        if err := tx.Put(bucket001, key, val, 0); err != nil {\r\n            return err\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nbucket002 := \"bucket002\"\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        if err := tx.Put(bucket002, key, val, 0); err != nil {\r\n            return err\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n```\r\n这边注意下，这个bucket和你使用数据结构有关，不同数据索引结构，用同一个bucket，也是不同的。比如你定义了一个bucket，命名为`bucket_foo`，比如你要用`list`这个数据结构，使用 `tx.RPush`加数据，必须对应他的数据结构去从这个`bucket_foo`查询或者取出，比如用 `tx.RPop`，`tx.LRange` 等，不能用`tx.Get`（和GetAll、Put、Delete、RangeScan等同一索引类型）去读取这个`bucket_foo`里面的数据，因为索引结构不同。其他数据结构如`Set`、`Sorted Set`同理。\r\n\r\n下面说明下迭代buckets 和 删除bucket。它们都用到了`ds`。\r\n\r\nds表示数据结构，支持如下：\r\n* DataStructureSet\r\n* DataStructureSortedSet\r\n* DataStructureBPTree\r\n* DataStructureList\r\n\r\n目前支持的`EntryIdxMode`如下：\r\n\r\n* HintKeyValAndRAMIdxMode \r\n* HintKeyAndRAMIdxMode \r\n\r\n#### 迭代buckets\r\n\r\nIterateBuckets支持迭代指定ds的迭代。\r\n\r\n```go\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.IterateBuckets(nutsdb.DataStructureBPTree, \"*\", func(bucket string) bool {\r\n            fmt.Println(\"bucket: \", bucket)\r\n            // true: continue, false: break\r\n            return true\r\n        })\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n#### 删除bucket\r\n\r\nDeleteBucket支持删除指定的bucket，需要两个参数`ds`和`bucket`。\r\n\r\n```go\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.DeleteBucket(nutsdb.DataStructureBPTree, bucket)\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n\r\n### 使用键值对\r\n\r\n#### 基本操作\r\n\r\n将key-value键值对保存在一个bucket, 你可以使用 `tx.Put` 这个方法:\r\n\r\n* 添加数据\r\n\r\n```golang\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n    key := []byte(\"name1\")\r\n    val := []byte(\"val1\")\r\n    bucket := \"bucket1\"\r\n    if err := tx.Put(bucket, key, val, 0); err != nil {\r\n        return err\r\n    }\r\n    return nil\r\n}); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n```\r\n\r\n* 更新数据\r\n\r\n上面的代码执行之后key为\"name1\"和value值\"val1\"被保存在命名为bucket1的bucket里面。\r\n\r\n如果你要做更新操作，你可以仍然用`tx.Put`方法去执行，比如下面的例子把value的值改成\"val1-modify\"：\r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n    key := []byte(\"name1\")\r\n    val := []byte(\"val1-modify\") // 更新值\r\n    bucket := \"bucket1\"\r\n    if err := tx.Put(bucket, key, val, 0); err != nil {\r\n        return err\r\n    }\r\n    return nil\r\n}); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n```\r\n\r\n* 获取数据\r\n\r\n获取值可以用`tx.Get` 这个方法:\r\n\r\n```golang\r\nif err := db.View(\r\nfunc(tx *nutsdb.Tx) error {\r\n    key := []byte(\"name1\")\r\n    bucket := \"bucket1\"\r\n    if value, err := tx.Get(bucket, key); err != nil {\r\n        return err\r\n    } else {\r\n        fmt.Println(string(value)) // \"val1-modify\"\r\n    }\r\n    return nil\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n* 删除数据\r\n\r\n删除使用`tx.Delete()` 方法：\r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n    key := []byte(\"name1\")\r\n    bucket := \"bucket1\"\r\n    if err := tx.Delete(bucket, key); err != nil {\r\n        return err\r\n    }\r\n    return nil\r\n}); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n#### 对值的位操作\r\n\r\n* 使用`tx.GetBit()`方法获取某一键所对应的值在某一偏移量上的值。当对应的键存在时，返回参数中偏移量所对应位置的上的值，当偏移量超出原有的数据范围时，将返回0且不报错；当对应的键不存在时，将报错提示键不存在。\r\n\r\n```golang\r\nif err := db.View(func(tx *nutsdb.Tx) error {\r\n\tbucket := \"bucket\"\r\n\tkey := []byte(\"key\")\r\n\toffset := 2\r\n    bit, err := tx.GetBit(bucket, key, offset)\r\n    if err != nil {\r\n        return err\r\n    }\r\n    log.Println(\"get bit:\", bit)\r\n    return nil\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n* 使用`tx.SetBit()`方法添加某一键所对应的值在某一偏移量上的值。当对应的键存在时，将会修改偏移量所对应的位上的值；当对应的键不存在或者偏移量超出原有的数据范围时，将会对原有值进行扩容直到能够在偏移量对应位置上修改。除偏移量对应位置之外，自动扩容产生的位的值均为0。\r\n\r\n```golang\r\nif err := db.Update(func(tx *nutsdb.Tx) error {\r\n\tbucket := \"bucket\"\r\n\tkey := []byte(\"key\")\r\n\toffset := 2\r\n\tbit := 1\r\n\treturn tx.SetBit(bucket, key, offset, bit)\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n#### 对值的自增和自减操作\r\n\r\n在对值进行自增和自减操作时需要键存在，否则将报错提示键不存在。当值的自增和自减结果将超出`int64`的范围时，将使用基于字符串的大数计算，所以不必担心值的范围过大。\r\n\r\n* 使用`tx.Incr()`方法让某一键所对应的值自增1\r\n\r\n```golang\r\nif err := db.Update(func(tx *nutsdb.Tx) error {\r\n\tbucket := \"bucket\"\r\n\tkey := []byte(\"key\")\r\n    return tx.Incr(bucket, key)\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n* 使用`tx.IncrBy()`方法让某一键所对应的值自增指定的值\r\n\r\n```golang\r\nif err := db.Update(func(tx *nutsdb.Tx) error {\r\n    bucket := \"bucket\"\r\n    key := []byte(\"key\")\r\n    return tx.IncrBy(bucket, key, 10)\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n* 使用`tx.Decr()`方法让某一键所对应的值自减1\r\n\r\n```golang\r\nif err := db.Update(func(tx *nutsdb.Tx) error {\r\n\tbucket := \"bucket\"\r\n\tkey := []byte(\"key\")\r\n    return tx.Decr(bucket, key)\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n* 使用`tx.DecrBy()`方法让某一键所对应的值自减指定的值\r\n\r\n```golang\r\nif err := db.Update(func(tx *nutsdb.Tx) error {\r\n    bucket := \"bucket\"\r\n    key := []byte(\"key\")\r\n    return tx.DecrBy(bucket, key, 10)\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n#### 对值的连续多次Set和Get\r\n\r\n* 使用`tx.MSet()`方法连续多次设置键值对。当使用`tx.MSet()`需要以`...[]byte`类型传入若干个键值对。此处要求参数的总数为偶数个，设i为从0开始的偶数，则第i个参数和第i+1个参数将分别成为一个键值对的键和值。\r\n\r\n```golang\r\nif err := db.Update(func(tx *nutsdb.Tx) error {\r\n\tbucekt := \"bucket\"\r\n\targs := [][]byte{\r\n        []byte(\"1\"), []byte(\"2\"), []byte(\"3\"), []byte(\"4\"),\r\n    }\r\n    return tx.MSet(bucket, nutsdb.Persistent, args...)\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n* 使用`tx.MGet()`方法连续多次取值。当使用`tx.MGet()`需要以`...[]byte`类型传入若干个键，若其中任何一个键不存在都会返回`key not found`错误。返回值是一个切片，长度与传入的参数相同，并且根据切片索引一一对应。\r\n\r\n```golang\r\nif err := db.View(func(tx *nutsdb.Tx) error {\r\n\tbucket := \"bucket\"\r\n\tkey := [][]byte{\r\n\t\t[]byte(\"1\"), []byte(\"2\"), []byte(\"3\"), []byte(\"4\"),\r\n    }\r\n    values, err := tx.MGet(bucket, key...)\r\n    if err != nil {\r\n        return err\r\n    }\r\n    for i, value := range values {\r\n        log.Printf(\"get value by MGet, the %d value is '%s'\", i, string(value))\r\n    }\r\n    return nil\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n#### 对值的增补操作\r\n\r\n* 使用`tx.Append()`方法对值进行增补。\r\n\r\n```golang\r\nif err := db.Update(func(tx *nutsdb.Tx) error {\r\n\tbucket := \"bucket\"\r\n\tkey := \"key\"\r\n\tappendage := \"appendage\"\r\n    return tx.Append(bucket, []byte(key), []byte(appendage))\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n#### 获取值的一部分\r\n\r\n* 使用`tx.GetRange()`方法可以根据给定的索引获取值的一部分。通过两个`int`类型的参数确定一个闭区间，返回闭区间所对应部分的值。\r\n\r\n```golang\r\nif err := db.View(func(tx *nutsdb.Tx) error {\r\n\tbucket := \"bucket\"\r\n\tkey := \"key\"\r\n\tstart := 0\r\n\tend := 2\r\n    value, err := tx.GetRange(bucket, []byte(key), start, end)\r\n    if err != nil {\r\n        return err\r\n    }\r\n    log.Printf(\"got value: '%s'\", string(value))\r\n    return nil\r\n}); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n### 使用TTL\r\n\r\nNusDB支持TTL(存活时间)的功能，可以对指定的bucket里的key过期时间的设置。使用`tx.Put`这个方法的使用`ttl`参数就可以了。\r\n如果设置 ttl = 0 或者 Persistent, 这个key就会永久存在。下面例子中ttl设置成 60 , 60s之后key就会过期，在查询的时候将不会被搜到。\r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n    key := []byte(\"name1\")\r\n    val := []byte(\"val1\")\r\n    bucket := \"bucket1\"\r\n    \r\n    // 如果设置 ttl = 0 or Persistent, 这个key就会永久不删除\r\n    // 这边 ttl = 60 , 60s之后就会过期。\r\n    if err := tx.Put(bucket, key, val, 60); err != nil {\r\n        return err\r\n    }\r\n    return nil\r\n}); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n### 对keys的扫描操作\r\n\r\nkey在一个bucket里面按照byte-sorted有序排序的，所以对于keys的扫描操作，在NutsDB里是很高效的。\r\n\r\n\r\n#### 前缀扫描\r\n\r\n对于前缀的扫描，我们可以用`PrefixScan` 方法, 使用参数 `offSet`和`limitNum` 来限制返回的结果的数量，比方下面例子限制100个entries:\r\n\r\n```golang\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        prefix := []byte(\"user_\")\r\n        bucket := \"user_list\"\r\n        // 从offset=0开始 ，限制 100 entries 返回 \r\n        if entries, err := tx.PrefixScan(bucket, prefix, 0, 100); err != nil {\r\n            return err\r\n        } else {\r\n            for _, entry := range entries {\r\n                fmt.Println(string(entry.Key), string(entry.Value))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n        log.Fatal(err)\r\n}\r\n\r\n```\r\n\r\n#### 前缀后的正则扫描\r\n\r\n对于前缀后的扫描，可以通过正则表达式对键的第二部分进行搜索来遍历一个键前缀，我们可以使用`PrefixSearchScan`方法，用参数`reg`来编写正则表达式，使用参数`offsetNum`、`limitNum` 来约束返回的条目的数量:\r\n\r\n```go\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        prefix := []byte(\"user_\") // 定义前缀\r\n        reg := \"99\"  // 定义正则表达式\r\n        bucket := \"user_list\"\r\n        // 从offset=25开始，限制 100 entries 返回 \r\n        if entries, _, err := tx.PrefixSearchScan(bucket, prefix, reg, 25, 100); err != nil {\r\n            return err\r\n        } else {\r\n            for _, entry := range entries {\r\n                fmt.Println(string(entry.Key), string(entry.Value))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n        log.Fatal(err)\r\n}\r\n```\r\n\r\n#### 范围扫描\r\n\r\n对于范围的扫描，我们可以用 `RangeScan` 方法。\r\n\r\n例子：\r\n\r\n```golang\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        // 假设用户key从 user_0000000 to user_9999999.\r\n        // 执行区间扫描类似这样一个start和end作为主要参数.\r\n        start := []byte(\"user_0010001\")\r\n        end := []byte(\"user_0010010\")\r\n        bucket := \"user_list\"\r\n        if entries, err := tx.RangeScan(bucket, start, end); err != nil {\r\n            return err\r\n        } else {\r\n            for _, entry := range entries {\r\n                fmt.Println(string(entry.Key), string(entry.Value))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n### 获取全部的key和value\r\n\r\n对于获取一个bucket的所有key和value，可以使用`GetAll`方法。\r\n\r\n例子：\r\n\r\n```go\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"user_list\"\r\n        entries, err := tx.GetAll(bucket)\r\n        if err != nil {\r\n            return err\r\n        }\r\n\r\n        for _, entry := range entries {\r\n            fmt.Println(string(entry.Key),string(entry.Value))\r\n        }\r\n\r\n        return nil\r\n    }); err != nil {\r\n    log.Println(err)\r\n}\r\n```\r\n\r\n### 迭代器\r\n\r\n主要是迭代器的选项参数`Reverse`的值来决定正向还是反向迭代器, 当前版本还不支持HintBPTSparseIdxMode的迭代器\r\n\r\n\r\n#### 正向的迭代器\r\n\r\n```go\r\ntx, err := db.Begin(false)\r\niterator := nutsdb.NewIterator(tx, bucket, nutsdb.IteratorOptions{Reverse: false})\r\ni := 0\r\nfor i < 10 {\r\n    ok, err := iterator.SetNext()\r\n    fmt.Println(\"ok, err\", ok, err)\r\n    fmt.Println(\"Key: \", string(iterator.Entry().Key))\r\n    fmt.Println(\"Value: \", string(iterator.Entry().Value))\r\n    fmt.Println()\r\n    i++\r\n}\r\nerr = tx.Commit()\r\nif err != nil {\r\n    panic(err)\r\n}\r\n```\r\n\r\n#### 反向的迭代器\r\n\r\n```go\r\ntx, err := db.Begin(false)\r\niterator := nutsdb.NewIterator(tx, bucket, nutsdb.IteratorOptions{Reverse: true})\r\ni := 0\r\nfor i < 10 {\r\n    ok, err := iterator.SetNext()\r\n    fmt.Println(\"ok, err\", ok, err)\r\n    fmt.Println(\"Key: \", string(iterator.Entry().Key))\r\n    fmt.Println(\"Value: \", string(iterator.Entry().Value))\r\n    fmt.Println()\r\n    i++\r\n}\r\nerr = tx.Commit()\r\nif err != nil {\r\n    panic(err)\r\n}\r\n```\r\n\r\n\r\n\r\n### 合并操作\r\n\r\nnutsdb为了保持高性能写，同一个key会写多份，如果你的服务，有对同一个key多次的更新或者删除，你希望对同一个key做合并，可以使用NutsDB提供了`db.Merge()`方法。\r\n这个方法需要自己根据实际情况编写合并策略。一旦执行会影响到正常的写请求，所以最好避开高峰期，比如半夜定时执行等。\r\n\r\n当然，如果你没有对同一个key有太多的更新或者删除，可以不用Merge()函数。\r\n\r\n```golang\r\nerr := db.Merge()\r\nif err != nil {\r\n    ...\r\n}\r\n```\r\n\r\n注意：当前版本不支持`HintBPTSparseIdxMode`模式的合并操作\r\n\r\n### 数据库备份\r\n\r\n对于数据库的备份，你可以调用 `db.Backup()`方法，只要提供一个备份的文件目录地址即可。这个方法执行的是一个热备份，不会阻塞到数据库其他的只读事务操作，对写（读写）事务会有影响。\r\n\r\n```golang\r\nerr = db.Backup(dir)\r\nif err != nil {\r\n   ...\r\n}\r\n```\r\n\r\nNutsDB还提供gzip的压缩备份：\r\n\r\n```golang\r\nf, _ := os.Create(path)\r\ndefer f.Close()\r\nerr = db.BackupTarGZ(f)\r\nif err != nil {\r\n   ...\r\n}\r\n\r\n```\r\n\r\n好了，入门指南已经完结。 散花~，到目前为止都是String类型的数据的crud操作，下面将学习其他更多的数据结构的操作。\r\n\r\n### 使用内存模式\r\n\r\nNutsDB从0.7.0版本开始支持内存模式，这个模式下，重启数据库，数据会丢失的。另外，内存模式有一些API，相对非内存模型，并没有实现，如果你发现有缺少的，并希望实现，请提交issue说明情况。\r\n\r\n例子：\r\n\r\n```go\r\n\r\n    opts := inmemory.DefaultOptions\r\n    db, err := inmemory.Open(opts)\r\n    if err != nil {\r\n        panic(err)\r\n    }\r\n    bucket := \"bucket1\"\r\n    key := []byte(\"key1\")\r\n    val := []byte(\"val1\")\r\n    err = db.Put(bucket, key, val, 0)\r\n    if err != nil {\r\n        fmt.Println(\"err\", err)\r\n    }\r\n\r\n    entry, err := db.Get(bucket, key)\r\n    if err != nil {\r\n        fmt.Println(\"err\", err)\r\n    }\r\n\r\n    fmt.Println(\"entry.Key\", string(entry.Key))     // entry.Key key1\r\n    fmt.Println(\"entry.Value\", string(entry.Value)) // entry.Value val1\r\n    \r\n```\r\n\r\n### 使用其他数据结构\r\n\r\n看到这边我们将学习其他数据结构，Api命名风格模仿 [Redis 命令](https://redis.io/commands)。所以如果你熟悉Redis，将会很快掌握使用。\r\n其他方面继承了上面的bucket/key/value模型，所以你会看到和Redis的Api使用上稍微有些不同，会多一个bucket。\r\n\r\n#### List\r\n\r\n##### RPush\r\n\r\n从指定bucket里面的指定队列key的右边入队一个或者多个元素val。\r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        val := []byte(\"val1\")\r\n        return tx.RPush(bucket, key, val)\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### LPush \r\n\r\n从指定bucket里面的指定队列key的左边入队一个或者多个元素val。\r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n            bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        val := []byte(\"val2\")\r\n        return tx.LPush(bucket, key, val)\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### LPop \r\n\r\n从指定bucket里面的指定队列key的左边出队一个元素，删除并返回。\r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n            bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        if item, err := tx.LPop(bucket, key); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"LPop item:\", string(item))\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### LPeek\r\n\r\n从指定bucket里面的指定队列key的左边出队一个元素返回不删除。\r\n\r\n```golang\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n            bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        if item, err := tx.LPeek(bucket, key); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"LPeek item:\", string(item)) //val11\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### RPop \r\n\r\n从指定bucket里面的指定队列key的右边出队一个元素，删除并返回。\r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n            bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        if item, err := tx.RPop(bucket, key); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"RPop item:\", string(item))\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### RPeek\r\n\r\n从指定bucket里面的指定队列key的右边出队一个元素返回不删除。\r\n\r\n```golang\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n            bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        if item, err := tx.RPeek(bucket, key); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"RPeek item:\", string(item))\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### LRange \r\n\r\n返回指定bucket里面的指定队列key列表里指定范围内的元素。 start 和 end 偏移量都是基于0的下标，即list的第一个元素下标是0（list的表头），第二个元素下标是1，以此类推。\r\n偏移量也可以是负数，表示偏移量是从list尾部开始计数。 例如：-1 表示列表的最后一个元素，-2 是倒数第二个，以此类推。\r\n\r\n```golang\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n            bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        if items, err := tx.LRange(bucket, key, 0, -1); err != nil {\r\n            return err\r\n        } else {\r\n            //fmt.Println(items)\r\n            for _, item := range items {\r\n                fmt.Println(string(item))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### LRem \r\n\r\n注意: 这个方法在 v0.6.0版本开始支持，之前的版本实现和描述有问题。\r\n\r\n从指定bucket里面的指定的key的列表里移除前 count 次出现的值为 value 的元素。 这个 count 参数通过下面几种方式影响这个操作：\r\n\r\ncount > 0: 从头往尾移除值为 value 的元素。\r\ncount < 0: 从尾往头移除值为 value 的元素。\r\ncount = 0: 移除所有值为 value 的元素。\r\n\r\n下面的例子count=1：\r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n            bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        return tx.LRem(bucket, key, 1, []byte(\"val11\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### LRemByIndex\r\n\r\n注意: 这个方法在 v0.10.0版本开始支持\r\n\r\n移除列表中指定位置（单个或多个位置）的元素\r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        removedNum, err := tx.LRemByIndex(bucket, key, 0, 1)\r\n        fmt.Printf(\"removed num %d\\n\", removedNum)\r\n        return err\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### LTrim \r\n\r\n修剪一个已存在的 list，这样 list 就会只包含指定范围的指定元素。start 和 stop 都是由0开始计数的， 这里的 0 是列表里的第一个元素（表头），1 是第二个元素，以此类推。\r\n\r\n例如： LTRIM foobar 0 2 将会对存储在 foobar 的列表进行修剪，只保留列表里的前3个元素。\r\n\r\nstart 和 end 也可以用负数来表示与表尾的偏移量，比如 -1 表示列表里的最后一个元素， -2 表示倒数第二个，等等。\r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n            bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        return tx.LTrim(bucket, key, 0, 1)\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### LSize \r\n\r\n返回指定bucket下指定key列表的size大小\r\n\r\n```golang\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n            bucket := \"bucketForList\"\r\n        key := []byte(\"myList\")\r\n        if size,err := tx.LSize(bucket, key); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"myList size is \",size)\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### LKeys\r\n\r\n查找`List`类型的所有匹配指定模式`pattern`的`key`，类似于Redis命令: [KEYS](https://redis.io/commands/keys/)\r\n\r\n注意：模式匹配使用 Go 标准库的`filepath.Match`，部分细节上和redis的行为有区别，比如对于 `[` 的处理。\r\n\r\n```golang\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        var keys []string\r\n        err := tx.LKeys(bucket, \"*\", func(key string) bool {\r\n            keys = append(keys, key)\r\n            // true: continue, false: break\r\n            return true\r\n        })\r\n        fmt.Printf(\"keys: %v\\n\", keys)\r\n        return err\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n#### Set\r\n\r\n##### SAdd\r\n\r\n添加一个指定的member元素到指定bucket的里的指定集合key中。\r\n\r\n```go\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n            bucket := \"bucketForSet\"\r\n        key := []byte(\"mySet\")\r\n        return tx.SAdd(bucket, key, []byte(\"a\"), []byte(\"b\"), []byte(\"c\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### SAreMembers \r\n\r\n返回多个成员member是否是指定bucket的里的指定集合key的成员。\r\n\r\n```go\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"bucketForSet\"\r\n        key := []byte(\"mySet\")\r\n        if ok, err := tx.SAreMembers(bucket, key, []byte(\"a\"), []byte(\"b\"), []byte(\"c\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SAreMembers:\", ok)\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### SCard \r\n\r\n返回指定bucket的指定的集合key的基数 (集合元素的数量)。\r\n\r\n```go\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"bucketForSet\"\r\n        key := []byte(\"mySet\")\r\n        if num, err := tx.SCard(bucket, key); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SCard:\", num)\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n```\r\n##### SDiffByOneBucket \r\n\r\n返回一个集合与给定集合的差集的元素。这两个集合都在一个bucket中。\r\n\r\n```go\r\n\r\nkey1 := []byte(\"mySet1\") // 集合1\r\nkey2 := []byte(\"mySet2\") // 集合2\r\nbucket := \"bucketForSet\"\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket, key1, []byte(\"a\"), []byte(\"b\"), []byte(\"c\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket, key2, []byte(\"c\"), []byte(\"d\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if items, err := tx.SDiffByOneBucket(bucket, key1, key2); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SDiffByOneBucket:\", items)\r\n            for _, item := range items {\r\n                fmt.Println(\"item\", string(item))\r\n            }\r\n            //item a\r\n            //item b\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n```\r\n\r\n##### SDiffByTwoBuckets \r\n\r\n返回一个集合与给定集合的差集的元素。这两个集合分别在不同bucket中。\r\n\r\n```go\r\nbucket1 := \"bucket1\"\r\nkey1 := []byte(\"mySet1\")\r\n\r\nbucket2 := \"bucket2\"\r\nkey2 := []byte(\"mySet2\")\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket1, key1, []byte(\"a\"), []byte(\"b\"), []byte(\"c\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket2, key2, []byte(\"c\"), []byte(\"d\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if items, err := tx.SDiffByTwoBuckets(bucket1, key1, bucket2, key2); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SDiffByTwoBuckets:\", items)\r\n            for _, item := range items {\r\n                fmt.Println(\"item\", string(item))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n```\r\n##### SHasKey \r\n\r\n判断是否指定的集合在指定的bucket中。\r\n\r\n```go\r\n\r\nbucket := \"bucketForSet\"\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if ok, err := tx.SHasKey(bucket, []byte(\"mySet\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SHasKey\", ok)\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n```\r\n##### SIsMember \r\n\r\n返回成员member是否是指定bucket的存指定key集合的成员。\r\n\r\n```go\r\nbucket := \"bucketForSet\"\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if ok, err := tx.SIsMember(bucket, []byte(\"mySet\"), []byte(\"a\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SIsMember\", ok)\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### SMembers \r\n\r\n返回指定bucket的指定key集合所有的元素。\r\n\r\n```go\r\nbucket := \"bucketForSet\"\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if items, err := tx.SMembers(bucket, []byte(\"mySet\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SMembers\", items)\r\n            for _, item := range items {\r\n                fmt.Println(\"item\", string(item))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### SMoveByOneBucket \r\n\r\n将member从source集合移动到destination集合中，其中source集合和destination集合均在一个bucket中。\r\n\r\n```go\r\nbucket3 := \"bucket3\"\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket3, []byte(\"mySet1\"), []byte(\"a\"), []byte(\"b\"), []byte(\"c\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket3, []byte(\"mySet2\"), []byte(\"c\"), []byte(\"d\"), []byte(\"e\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        if ok, err := tx.SMoveByOneBucket(bucket3, []byte(\"mySet1\"), []byte(\"mySet2\"), []byte(\"a\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SMoveByOneBucket\", ok)\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if items, err := tx.SMembers(bucket3, []byte(\"mySet1\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"after SMoveByOneBucket bucket3 mySet1 SMembers\", items)\r\n            for _, item := range items {\r\n                fmt.Println(\"item\", string(item))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if items, err := tx.SMembers(bucket3, []byte(\"mySet2\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"after SMoveByOneBucket bucket3 mySet2 SMembers\", items)\r\n            for _, item := range items {\r\n                fmt.Println(\"item\", string(item))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### SMoveByTwoBuckets \r\n\r\n将member从source集合移动到destination集合中。其中source集合和destination集合在两个不同的bucket中。\r\n\r\n```go\r\nbucket4 := \"bucket4\"\r\nbucket5 := \"bucket5\"\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket4, []byte(\"mySet1\"), []byte(\"a\"), []byte(\"b\"), []byte(\"c\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket5, []byte(\"mySet2\"), []byte(\"c\"), []byte(\"d\"), []byte(\"e\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        if ok, err := tx.SMoveByTwoBuckets(bucket4, []byte(\"mySet1\"), bucket5, []byte(\"mySet2\"), []byte(\"a\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SMoveByTwoBuckets\", ok)\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if items, err := tx.SMembers(bucket4, []byte(\"mySet1\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"after SMoveByTwoBuckets bucket4 mySet1 SMembers\", items)\r\n            for _, item := range items {\r\n                fmt.Println(\"item\", string(item))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if items, err := tx.SMembers(bucket5, []byte(\"mySet2\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"after SMoveByTwoBuckets bucket5 mySet2 SMembers\", items)\r\n            for _, item := range items {\r\n                fmt.Println(\"item\", string(item))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### SPop \r\n\r\n从指定bucket里的指定key的集合中移除并返回一个或多个随机元素。\r\n\r\n```go\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        key := []byte(\"mySet\")\r\n        if item, err := tx.SPop(bucket, key); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SPop item from mySet:\", string(item))\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### SRem \r\n\r\n在指定bucket里面移除指定的key集合中移除指定的一个或者多个元素。\r\n\r\n```go\r\nbucket6:=\"bucket6\"\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket6, []byte(\"mySet\"), []byte(\"a\"), []byte(\"b\"), []byte(\"c\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        if err := tx.SRem(bucket6, []byte(\"mySet\"), []byte(\"a\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SRem ok\")\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if items, err := tx.SMembers(bucket6, []byte(\"mySet\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SMembers items:\", items)\r\n            for _, item := range items {\r\n                fmt.Println(\"item:\", string(item))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### SUnionByOneBucket \r\n\r\n返回指定一个bucket里面的给定的两个集合的并集中的所有成员。\r\n\r\n```go\r\nbucket7 := \"bucket1\"\r\nkey1 := []byte(\"mySet1\")\r\nkey2 := []byte(\"mySet2\")\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket7, key1, []byte(\"a\"), []byte(\"b\"), []byte(\"c\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket7, key2, []byte(\"c\"), []byte(\"d\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if items, err := tx.SUnionByOneBucket(bucket7, key1, key2); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SUnionByOneBucket:\", items)\r\n            for _, item := range items {\r\n                fmt.Println(\"item\", string(item))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### SUnionByTwoBuckets \r\n\r\n返回指定两个bucket里面的给定的两个集合的并集中的所有成员。\r\n\r\n```go\r\nbucket8 := \"bucket1\"\r\nkey1 := []byte(\"mySet1\")\r\n\r\nbucket9 := \"bucket2\"\r\nkey2 := []byte(\"mySet2\")\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket8, key1, []byte(\"a\"), []byte(\"b\"), []byte(\"c\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        return tx.SAdd(bucket9, key2, []byte(\"c\"), []byte(\"d\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        if items, err := tx.SUnionByTwoBuckets(bucket8, key1, bucket9, key2); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"SUnionByTwoBucket:\", items)\r\n            for _, item := range items {\r\n                fmt.Println(\"item\", string(item))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### SKeys\r\n\r\n查找`Set`类型的所有匹配指定模式`pattern`的`key`，类似于Redis命令: [KEYS](https://redis.io/commands/keys/)\r\n\r\n注意：模式匹配使用 Go 标准库的`filepath.Match`，部分细节上和redis的行为有区别，比如对于 `[` 的处理。\r\n\r\n```golang\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        var keys []string\r\n        err := tx.SKeys(bucket, \"*\", func(key string) bool {\r\n            keys = append(keys, key)\r\n            // true: continue, false: break\r\n            return true\r\n        })\r\n        fmt.Printf(\"keys: %v\\n\", keys)\r\n        return err\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n#### Sorted Set\r\n\r\n> 注意：这边的bucket是有序集合名。\r\n\r\n##### ZAdd\r\n\r\n将指定成员（包括key、score、value）添加到指定bucket的有序集合（sorted set）里面。\r\n\r\n\r\n```go\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet1\" // 注意：这边的bucket是有序集合名\r\n        key := []byte(\"key1\")\r\n        return tx.ZAdd(bucket, key, 1, []byte(\"val1\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### ZCard \r\n\r\n返回指定bucket的的有序集元素个数。\r\n\r\n```go\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet1\"\r\n        if num, err := tx.ZCard(bucket); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZCard num\", num)\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### ZCount \r\n\r\n返回指定bucket的有序集，score值在min和max之间(默认包括score值等于start或end)的成员。\r\n\r\nOpts包含的参数：\r\n\r\n* Limit        int  // 限制返回的node数目\r\n* ExcludeStart bool // 排除start\r\n* ExcludeEnd   bool // 排除end\r\n\r\n```go\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet1\"\r\n        if num, err := tx.ZCount(bucket, 0, 1, nil); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZCount num\", num)\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### ZGetByKey \r\n\r\n返回一个节点通过指定的bucket有序集合和指定的key来获取。\r\n\r\n```go\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet1\"\r\n        key := []byte(\"key2\")\r\n        if node, err := tx.ZGetByKey(bucket, key); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZGetByKey key2 val:\", string(node.Value))\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### ZMembers \r\n\r\n返回所有成员通过在指定的bucket。\r\n\r\n```go\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet1\"\r\n        if nodes, err := tx.ZMembers(bucket); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZMembers:\", nodes)\r\n\r\n            for _, node := range nodes {\r\n                fmt.Println(\"member:\", node.Key(), string(node.Value))\r\n            }\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### ZPeekMax \r\n\r\n返回指定bucket有序集合中的具有最高得分的成员。\r\n\r\n```go\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet1\"\r\n        if node, err := tx.ZPeekMax(bucket); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZPeekMax:\", string(node.Value))\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### ZPeekMin \r\n\r\n返回指定bucket有序集合中的具有最低得分的成员。\r\n\r\n```go\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet1\"\r\n        if node, err := tx.ZPeekMin(bucket); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZPeekMin:\", string(node.Value))\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### ZPopMax \r\n\r\n删除并返回指定bucket有序集合中的具有最高得分的成员。\r\n\r\n```go\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet1\"\r\n        if node, err := tx.ZPopMax(bucket); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZPopMax:\", string(node.Value)) //val3\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### ZPopMin \r\n\r\n删除并返回指定bucket有序集合中的具有最低得分的成员。\r\n\r\n```go\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet1\"\r\n        if node, err := tx.ZPopMin(bucket); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZPopMin:\", string(node.Value)) //val1\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### ZRangeByRank \r\n\r\n返回指定bucket有序集合的排名start到end的范围（包括start和end）的所有元素。\r\n\r\n```go\r\n// ZAdd add items\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet2\"\r\n        key1 := []byte(\"key1\")\r\n        return tx.ZAdd(bucket, key1, 1, []byte(\"val1\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet2\"\r\n        key2 := []byte(\"key2\")\r\n        return tx.ZAdd(bucket, key2, 2, []byte(\"val2\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet2\"\r\n        key3 := []byte(\"key3\")\r\n        return tx.ZAdd(bucket, key3, 3, []byte(\"val3\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n// ZRangeByRank\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet2\"\r\n        if nodes, err := tx.ZRangeByRank(bucket, 1, 2); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZRangeByRank nodes :\", nodes)\r\n            for _, node := range nodes {\r\n                fmt.Println(\"item:\", node.Key(), node.Score())\r\n            }\r\n            \r\n            //item: key1 1\r\n            //item: key2 2\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### ZRangeByScore \r\n\r\n返回指定bucket有序集合的分数start到end的范围（包括start和end）的所有元素。其中有个`Opts`参数用法参考`ZCount`。\r\n\r\n```go\r\n// ZAdd\r\nif err := db.Update(\r\n        func(tx *nutsdb.Tx) error {\r\n            bucket := \"myZSet3\"\r\n            key1 := []byte(\"key1\")\r\n            return tx.ZAdd(bucket, key1, 70, []byte(\"val1\"))\r\n        }); err != nil {\r\n        log.Fatal(err)\r\n    }\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet3\"\r\n        key2 := []byte(\"key2\")\r\n        return tx.ZAdd(bucket, key2, 90, []byte(\"val2\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet3\"\r\n        key3 := []byte(\"key3\")\r\n        return tx.ZAdd(bucket, key3, 86, []byte(\"val3\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n// ZRangeByScore\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet3\"\r\n        if nodes, err := tx.ZRangeByScore(bucket, 80, 100,nil); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZRangeByScore nodes :\", nodes)\r\n            for _, node := range nodes {\r\n                fmt.Println(\"item:\", node.Key(), node.Score())\r\n            }\r\n            //item: key3 86\r\n            //item: key2 90\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}   \r\n```\r\n##### ZRank\r\n\r\n返回有序集bucket中成员指定成员key的排名。其中有序集成员按score值递增(从小到大)顺序排列。注意排名以1为底，也就是说，score值最小的成员排名为1。\r\n这点和Redis不同，Redis是从0开始的。\r\n\r\n```go\r\n\r\n// ZAdd\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet4\"\r\n        key1 := []byte(\"key1\")\r\n        return tx.ZAdd(bucket, key1, 70, []byte(\"val1\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet4\"\r\n        key2 := []byte(\"key2\")\r\n        return tx.ZAdd(bucket, key2, 90, []byte(\"val2\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet4\"\r\n        key3 := []byte(\"key3\")\r\n        return tx.ZAdd(bucket, key3, 86, []byte(\"val3\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n// ZRank\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet4\"\r\n        key1 := []byte(\"key1\")\r\n        if rank, err := tx.ZRank(bucket, key1); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"key1 ZRank :\", rank) // key1 ZRank : 1\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n#### ZRevRank\r\n\r\n返回有序集bucket中成员指定成员key的反向排名。其中有序集成员还是按score值递增(从小到大)顺序排列。但是获取反向排名，注意排名还是以1为开始，也就是说，但是这个时候score值最大的成员排名为1。\r\n\r\n```go\r\n// ZAdd\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet8\"\r\n        key1 := []byte(\"key1\")\r\n        return tx.ZAdd(bucket, key1, 10, []byte(\"val1\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet8\"\r\n        key2 := []byte(\"key2\")\r\n        return tx.ZAdd(bucket, key2, 20, []byte(\"val2\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet8\"\r\n        key3 := []byte(\"key3\")\r\n        return tx.ZAdd(bucket, key3, 30, []byte(\"val3\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n// ZRevRank\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet8\"\r\n        if rank, err := tx.ZRevRank(bucket, []byte(\"key3\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZRevRank key1 rank:\", rank) //ZRevRank key3 rank: 1\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### ZRem \r\n\r\n删除指定成员key在一个指定的有序集合bucket中。\r\n\r\n```go\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet5\"\r\n        key1 := []byte(\"key1\")\r\n        return tx.ZAdd(bucket, key1, 10, []byte(\"val1\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet5\"\r\n        key2 := []byte(\"key2\")\r\n        return tx.ZAdd(bucket, key2, 20, []byte(\"val2\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet5\"\r\n        if nodes,err := tx.ZMembers(bucket); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"before ZRem key1, ZMembers nodes\",nodes)\r\n            for _,node:=range nodes {\r\n                fmt.Println(\"item:\",node.Key(),node.Score())\r\n            }\r\n        }\r\n        // before ZRem key1, ZMembers nodes map[key1:0xc00008cfa0 key2:0xc00008d090]\r\n        // item: key1 10\r\n        // item: key2 20\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet5\"\r\n        if err := tx.ZRem(bucket, \"key1\"); err != nil {\r\n            return err\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet5\"\r\n        if nodes,err := tx.ZMembers(bucket); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"after ZRem key1, ZMembers nodes\",nodes)\r\n            for _,node:=range nodes {\r\n                fmt.Println(\"item:\",node.Key(),node.Score())\r\n            }\r\n            // after ZRem key1, ZMembers nodes map[key2:0xc00008d090]\r\n            // item: key2 20\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\n```\r\n\r\n##### ZRemRangeByRank \r\n\r\n删除所有成员满足排名start到end（包括start和end）在一个指定的有序集合bucket中。其中排名以1开始，排名1表示第一个节点元素，排名-1表示最后的节点元素。\r\n\r\n```go\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet6\"\r\n        key1 := []byte(\"key1\")\r\n        return tx.ZAdd(bucket, key1, 10, []byte(\"val1\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet6\"\r\n        key2 := []byte(\"key2\")\r\n        return tx.ZAdd(bucket, key2, 20, []byte(\"val2\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet6\"\r\n        key3 := []byte(\"key3\")\r\n        return tx.ZAdd(bucket, key3, 30, []byte(\"val2\"))\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet6\"\r\n        if nodes,err := tx.ZMembers(bucket); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"before ZRemRangeByRank, ZMembers nodes\",nodes)\r\n            for _,node:=range nodes {\r\n                fmt.Println(\"item:\",node.Key(),node.Score())\r\n            }\r\n            // before ZRemRangeByRank, ZMembers nodes map[key3:0xc00008d450 key1:0xc00008d270 key2:0xc00008d360]\r\n            // item: key1 10\r\n            // item: key2 20\r\n            // item: key3 30\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.Update(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet6\"\r\n        if err := tx.ZRemRangeByRank(bucket, 1,2); err != nil {\r\n            return err\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet6\"\r\n        if nodes,err := tx.ZMembers(bucket); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"after ZRemRangeByRank, ZMembers nodes\",nodes)\r\n            for _,node:=range nodes {\r\n                fmt.Println(\"item:\",node.Key(),node.Score())\r\n            }\r\n            // after ZRemRangeByRank, ZMembers nodes map[key3:0xc00008d450]\r\n            // item: key3 30\r\n            // key1 ZScore 10\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n##### ZScore\r\n\r\n返回指定有序集bucket中，成员key的score值。\r\n\r\n```go\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        bucket := \"myZSet7\"\r\n        if score,err := tx.ZScore(bucket, []byte(\"key1\")); err != nil {\r\n            return err\r\n        } else {\r\n            fmt.Println(\"ZScore key1 score:\",score)\r\n        }\r\n        return nil\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n##### ZKeys\r\n\r\n查找`Sorted Set`类型的所有匹配指定模式`pattern`的`key`，类似于Redis命令: [KEYS](https://redis.io/commands/keys/)\r\n\r\n注意：模式匹配使用 Go 标准库的`filepath.Match`，部分细节上和redis的行为有区别，比如对于 `[` 的处理。\r\n\r\n```golang\r\nif err := db.View(\r\n    func(tx *nutsdb.Tx) error {\r\n        var keys []string\r\n        err := tx.ZKeys(bucket, \"*\", func(key string) bool {\r\n            keys = append(keys, key)\r\n            // true: continue, false: break\r\n            return true\r\n        })\r\n        fmt.Printf(\"keys: %v\\n\", keys)\r\n        return err\r\n    }); err != nil {\r\n    log.Fatal(err)\r\n}\r\n```\r\n\r\n### 与其他数据库的比较\r\n\r\n#### BoltDB\r\n\r\nBoltDB和NutsDB很相似都是内嵌型的key-value数据库，同时支持事务。Bolt基于B+tree引擎模型，只有一个文件，NutsDB基于bitcask引擎模型，会生成多个文件。当然他们都支持范围扫描和前缀扫描这两个实用的特性。\r\n\r\n#### LevelDB, RocksDB\r\n\r\nLevelDB 和 RocksDB 都是基于LSM tree模型。不支持bucket。 其中RocksDB目前还没看到golang实现的版本。\r\n\r\n#### Badger\r\n\r\nBadger也是基于LSM tree模型。但是写性能没有我想象中高。不支持bucket。\r\n\r\n另外，以上数据库均不支持多种数据结构如list、set、sorted set，而NutsDB从0.2.0版本开始支持这些数据结构。\r\n\r\n### Benchmarks\r\n\r\n为了保证尽可能公平，找了2款关注度很高的内嵌型的kvstore来做对比，他们都支持事务、支持持久化。\r\n\r\n* [BadgerDB](https://github.com/dgraph-io/badger) (master分支和默认配置)\r\n* [BoltDB](https://github.com/boltdb/bolt) (master分支和默认配置)\r\n* [NutsDB](https://github.com/nutsdb/nutsdb) (master分支和默认配置+自定义配置)\r\n\r\n## 测试的环境:\r\n\r\n* Go Version : go1.11.4 darwin/amd64\r\n* OS: Mac OS X 10.13.6\r\n* Architecture: x86_64\r\n* 16 GB 2133 MHz LPDDR3\r\n* CPU: 3.1 GHz Intel Core i7\r\n\r\n\r\n##  Benchmark的结果:\r\n\r\n```\r\nbadger 2019/03/11 18:06:05 INFO: All 0 tables opened in 0s\r\ngoos: darwin\r\ngoarch: amd64\r\npkg: github.com/nutsdb/kvstore-bench\r\nBenchmarkBadgerDBPutValue64B-8         10000        112382 ns/op        2374 B/op         74 allocs/op\r\nBenchmarkBadgerDBPutValue128B-8        20000         94110 ns/op        2503 B/op         74 allocs/op\r\nBenchmarkBadgerDBPutValue256B-8        20000         93480 ns/op        2759 B/op         74 allocs/op\r\nBenchmarkBadgerDBPutValue512B-8        10000        101407 ns/op        3271 B/op         74 allocs/op\r\nBenchmarkBadgerDBGet-8               1000000          1552 ns/op         416 B/op          9 allocs/op\r\nBenchmarkBoltDBPutValue64B-8           10000        203128 ns/op       21231 B/op         62 allocs/op\r\nBenchmarkBoltDBPutValue128B-8           5000        229568 ns/op       13716 B/op         64 allocs/op\r\nBenchmarkBoltDBPutValue256B-8          10000        196513 ns/op       17974 B/op         64 allocs/op\r\nBenchmarkBoltDBPutValue512B-8          10000        199805 ns/op       17064 B/op         64 allocs/op\r\nBenchmarkBoltDBGet-8                 1000000          1122 ns/op         592 B/op         10 allocs/op\r\nBenchmarkNutsDBPutValue64B-8           30000         53614 ns/op         626 B/op         14 allocs/op\r\nBenchmarkNutsDBPutValue128B-8          30000         51998 ns/op         664 B/op         13 allocs/op\r\nBenchmarkNutsDBPutValue256B-8          30000         53958 ns/op         920 B/op         13 allocs/op\r\nBenchmarkNutsDBPutValue512B-8          30000         55787 ns/op        1432 B/op         13 allocs/op\r\nBenchmarkNutsDBGet-8                 2000000           661 ns/op          88 B/op          3 allocs/op\r\nBenchmarkNutsDBGetByHintKey-8          50000         27255 ns/op         840 B/op         16 allocs/op\r\nPASS\r\nok      github.com/nutsdb/kvstore-bench   83.856s\r\n```\r\n\r\n## 结论:\r\n\r\n### 写性能: \r\n\r\nNutsDB最快。 NutsDB比BoltDB快2-5倍 , 比BadgerDB快0.5-2倍。\r\nBadgerDB次之，他比BoltDB快1-3倍。\r\nBoltDB最慢。\r\n\r\n### 读性能: \r\n\r\n默认模式下，读都很快。其中NutsDB在默认配置下比其他数据库快一倍。但是如果使用`HintKeyAndRAMIdxMode`的选项，读速度比默认配置低很多。道理很简单，默认配置是全内存索引，但是`HintKeyAndRAMIdxMode`的模式，是内存索引+磁盘混合的方式，但是这个选项模式可以保存远大于内存的数据。特别是value远大于key的场景效果更明显。\r\n\r\n\r\n### 警告和限制\r\n\r\n* 启动索引模式\r\n\r\n当前版本使用`HintKeyValAndRAMIdxMode`、 `HintKeyAndRAMIdxMode`和`HintBPTSparseIdxMode` 这三种作为db启动的时候索引模式。\r\n默认使用`HintKeyValAndRAMIdxMode`。在基本的功能的string数据类型（put、get、delete、rangeScan、PrefixScan）这三种模式都支持。`HintKeyValAndRAMIdxMode`，作为数据库默认选项，他是全内存索引，读写性能都很高。他的瓶颈在于内存。如果你内存够的话，这种默认是适合的。另一种模式`HintKeyAndRAMIdxMode`，他会把value存磁盘，通过索引去找offset，这种模式特别适合value远大于key的场景，他的读性能要比起默认模式要降低不少。`HintBPTSparseIdxMode`这个模式（v0.4.0之后支持）这个模式非常省内存，使用多级索引，测试10亿数据，只占用80几MB的内存，但是读性能比较差，需要自己加缓存加速。具体看自己的要求选择模式。\r\n\r\n关于**其他的数据结构（list\\set\\sorted set）只支持默认的HintKeyValAndRAMIdxMode。请根据需要选模式**。**还有一个启动索引模式一旦开启不要来回切换到其他模式，因为索引结构不一样，可能导致数据读不出来**。 \r\n\r\n* Segment配置问题\r\n\r\nNutsDB会自动切割分成一个个块（Segment），默认`SegmentSize`是8MB，这个参数可以自己需要配置（比如16MB、32MB、64MB、128MB、512MB等），但是**一旦配置不能修改**。\r\n\r\n* key和value的大小限制问题\r\n\r\n关于key和value的大小受到SegmentSize的大小的影响，比如SegmentSize为8M，key和value的大小肯定是小于8M的，不然会返回错误。\r\n在NutsDB里面entry是最小单位，只要保证entry不大于`SegmentSize`就可以了。\r\n\r\n* entry的大小问题\r\n\r\nentry的的大小=EntryHeader的大小+key的大小+value的大小+bucket的大小\r\n\r\n* 关于支持的操作系统\r\n\r\n支持 Mac OS 、Linux 、Windows 三大平台。\r\n\r\n* 关于合并操作\r\n\r\n`HintBPTSparseIdxMode` 这个模式在当前版本还没有支持。\r\n\r\n* 关于事务说明\r\n\r\n在传统的关系式数据库中，常常用 ACID 性质来检验事务功能的安全性，~~NutsDB目前的版本并没有完全支持ACID。~~ NutsDB从v0.2.0之后的版本开始完全支持ACID。\r\n\r\n这这特别感谢 @damnever 给我提的[issue](https://github.com/nutsdb/nutsdb/issues/10)给我指出，特别在这说明下，免得误导大家。\r\n\r\n从v0.3.0版本起，NutsDB支持（A）原子性、C（一致性）、I（隔离性），并保证（D）持久化。以下参考[wiki百科](https://zh.wikipedia.org/wiki/ACID)的对ACID定义分别讲一下。如讲的有误，欢迎帮我指正。\r\n\r\n1、（A）原子性\r\n\r\n所谓原子性，一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。实现事务的原子性，要支持回滚操作，在某个操作失败后，回滚到事务执行之前的状态。一般的做法是类似数据快照的方案。关于这一点，NutsDB支持回滚操作。NutsDB的作法是先实际预演一边所有要执行的操作，这个时候数据其实还是uncommitted状态，一直到所有环节都没有问题，才会作commit操作，如果中间任何环节一旦发生错误，直接作rollback回滚操作，保证原子性。 就算发生错误的时候已经有数据进磁盘，下次启动也不会被索引到这些数据。\r\n\r\n2、（C）一致性\r\n\r\n在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的数据必须完全符合预期的。NutsDB基于读写锁实现锁机制，在高并发场景下，一个读写事务具有排他性的，比如一个goroutine需要执行一个读写事务，其他不管想要读写的事务或者只读的只能等待，直到这个锁释放为止。保证了数据的一致性。所以这一点NutsDB满足一致性。\r\n\r\n3、（I）隔离性\r\n\r\n数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。如上面的一致性所说，NutsDB基于读写锁实现锁机制。不会出现数据串的情况。所以也是满足隔离性的。\r\n\r\n关于事务的隔离级别，我们也来对照[wiki百科](https://zh.wikipedia.org/wiki/%E4%BA%8B%E5%8B%99%E9%9A%94%E9%9B%A2)，来看下NutsDB属于哪一个级别：\r\n\r\n#### 隔离级别低到高：\r\n\r\n##### 1）未提交读（READ UNCOMMITTED）\r\n\r\n这个是最低的隔离级别。允许“脏读”（dirty reads），事务可以看到其他事务“尚未提交”的修改。很明显nutsDB是避免脏读的。\r\n\r\n##### 2）在提交读（READ COMMITTED）\r\n\r\n定义：这个隔离级别中，基于锁机制并发控制的DBMS需要对选定对象的写锁一直保持到事务结束，但是读锁在SELECT操作完成后马上释放（因此“不可重复读”现象可能会发生）。\r\n看下“不可重复读”的定义：在一次事务中，当一行数据获取两遍得到不同的结果表示发生了“不可重复读”。\r\n\r\nnutsDB不会出现“不可重复读”这种情况，当高并发的时候，正在进行读写操作，一个goroutine刚好先拿到只读锁，这个时候要完成一个读写事务操作的那个goroutine要阻塞等到只读锁释放为止。也就避免上面的问题。\r\n\r\n##### 3）在可重复读（REPEATABLE READS）\r\n\r\n定义：这个隔离级别中，基于锁机制并发控制的DBMS需要对选定对象的读锁（read locks）和写锁（write locks）一直保持到事务结束，但不要求“范围锁”，因此可能会发生“幻影读”。\r\n\r\n关于幻影读定义，指在事务执行过程中，当两个完全相同的查询语句执行得到不同的结果集。这种现象称为“幻影读（phantom read）”，有些人也叫他幻读，正如上面所说，在nutsDB中，当进行只读操作的时候，同一时间只能并发只读操作，其他有关“写”的事务是被阻塞的，直到这些只读锁释放为止，因此不会出现“幻影读”的情况。\r\n\r\n##### 4）可串行化 （Serializable）\r\n\r\n定义：这个隔离级别是最高的。避免了所有上面的“脏读”、不可重复读”、“幻影读”现象。\r\n\r\n在nutsDB中，一个只读事务和一个写（读写）事务，是互斥的，需要串行执行，不会出现并发执行。nutsDB属于这个可串行化级别。\r\n这个级别的隔离一般来说在高并发场景下性能会受到影响。但是如果锁本身性能还可以，也不失为一个简单有效的方法。当前版本nutsDB基于读写锁，在并发读多写少的场景下，性能会好一点。\r\n\r\n\r\n4、（D）持久化\r\n\r\n事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。v0.3.0之前版本的nutsdb为了提供高性能的写入，并没有实时的做sync操作。从v0.3.0开始使用sync操作作强制同步，开始支持持久化，建议使用最新版本。\r\n\r\n\r\n关与其他信息待补充。有错误请帮忙指出，提给我issue，谢谢。\r\n\r\n### 联系作者\r\n\r\n* [nutsdb](https://github.com/nutsdb)\r\n\r\n### 参与贡献\r\n\r\n:+1::tada: 首先感谢你能看到这里，参与贡献 :tada::+1:\r\n\r\n参与贡献方式不限于：\r\n\r\n* 提各种issues（包括询问问题、提功能建议、性能建议等）\r\n* 提交bug\r\n* 提pull requests\r\n* 优化修改README文档\r\n\r\n感谢以下贡献者，感谢你们的付出！\r\n\r\n<a href=\"https://github.com/nutsdb/nutsdb/graphs/contributors\">\r\n<img src=\"https://contrib.rocks/image?repo=nutsdb/nutsdb\" />\r\n</a>\r\n\r\n#### 代码风格指南参考\r\n\r\nhttps://github.com/golang/go/wiki/CodeReviewComments\r\n\r\n#### git commit 规范参考\r\n\r\n commit message格式\r\n\r\n ```\r\n <type>(<scope>): <subject>\r\n ```\r\n\r\n\r\n####  type的参考\r\n\r\n![image](https://user-images.githubusercontent.com/6065007/162549766-58f164df-3794-4a5a-ab25-dd47962de74e.png)\r\n\r\n \r\n\r\n详情参考英文版的 [CONTRIBUTING](https://github.com/nutsdb/nutsdb/blob/master/CONTRIBUTING.md) 。\r\n\r\n### 致谢\r\n\r\n这个项目受到以下项目或多或少的灵感和帮助：\r\n\r\n* [Bitcask-intro](https://github.com/basho/bitcask/blob/develop/doc/bitcask-intro.pdf)\r\n* [BoltDB](https://github.com/boltdb)\r\n* [BuntDB](https://github.com/tidwall/buntdb)\r\n* [Redis](https://redis.io)\r\n* [Sorted Set](https://github.com/wangjia184/sortedset)\r\n\r\n### License\r\n\r\nThe NutsDB is open-sourced software licensed under the [Apache 2.0 license](https://github.com/nutsdb/nutsdb/blob/master/LICENSE).\r\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.0244140625,
          "content": "<p align=\"center\">\n    <img src=\"https://user-images.githubusercontent.com/6065007/141310364-62d7eebb-2cbb-4949-80ed-5cd20f705405.png\">\n</p>\n\n<div class=\"column\" align=\"middle\">\n  <a href=\"https://godoc.org/github.com/nutsdb/nutsdb\"><img src=\"https://godoc.org/github.com/nutsdb/nutsdb?status.svg\" /></a>\n  <a href=\"https://goreportcard.com/report/github.com/nutsdb/nutsdb\"><img src=\"https://goreportcard.com/badge/github.com/nutsdb/nutsdb\" /></a>\n  <a href=\"https://goreportcard.com/report/github.com/nutsdb/nutsdb\"><img src=\"https://github.com/nutsdb/nutsdb/workflows/Go/badge.svg?branch=master\"/></a>\n  <a href=\"https://codecov.io/gh/nutsdb/nutsdb\"><img src=\"https://codecov.io/gh/nutsdb/nutsdb/branch/master/graph/badge.svg?token=CupujOXpbe\"/></a>\n  <a href=\"https://raw.githubusercontent.com/nutsdb/nutsdb/master/LICENSE\"><img src=\"http://img.shields.io/badge/license-Apache_2-blue.svg?style=flat-square\"/></a>\n  <a href=\"https://github.com/avelino/awesome-go#database\"><img src=\"https://awesome.re/mentioned-badge.svg\"/></a>\n</div>\n\n## What is NutsDB?\n\nEnglish | [简体中文](https://github.com/nutsdb/nutsdb/blob/master/README-CN.md)\n\nNutsDB is a simple, fast, embeddable and persistent key/value store written in pure Go.\n\nIt supports fully serializable transactions and many data structures such as list、set、sorted set. All operations happen inside a Tx. Tx represents a transaction, which can be read-only or read-write. Read-only transactions can read values for a given bucket and a given key or iterate over a set of key-value pairs. Read-write transactions can read, update and delete keys from the DB.\n\nWe can learn more about NutsDB in details on the documents site of NutsDB: [NutsDB Documents](https://nutsdb.github.io/nutsdb-docs/)\n\n## Announcement\n\n* v1.0.0 release, see for details: [https://github.com/nutsdb/nutsdb/releases/tag/v1.0.0](https://github.com/nutsdb/nutsdb/releases/tag/v1.0.0)\n* v0.14.3 release, see for details: [https://github.com/nutsdb/nutsdb/releases/tag/v0.14.3](https://github.com/nutsdb/nutsdb/releases/tag/v0.14.3)\n* v0.14.2 release, see for details: [https://github.com/nutsdb/nutsdb/releases/tag/v0.14.2](https://github.com/nutsdb/nutsdb/releases/tag/v0.14.2)\n* v0.14.1 release, see for details: [https://github.com/nutsdb/nutsdb/releases/tag/v0.14.1](https://github.com/nutsdb/nutsdb/releases/tag/v0.14.1)\n\n📢 Note: Starting from v0.9.0, **defaultSegmentSize** in **DefaultOptions** has been adjusted from **8MB** to **256MB**. The original value is the default value, which needs to be manually changed to 8MB, otherwise the original data will not be parsed. The reason for the size adjustment here is that there is a cache for file descriptors starting from v0.9.0 (detail see https://github.com/nutsdb/nutsdb/pull/164 ), so users need to look at the number of fds they use on the server, which can be set manually. If you have any questions, you can open an issue.\n\nAfter **nutsdb v1.0.0**, due to changes in the underlying data storage protocol, **the data of the old version is not compatible**. Please rewrite it before using the new version. And the current Bucket needs to be created manually. Please see the Bucket usage [documentation](./docs/user_guides/use-buckets.md) for details.\n\n## Architecture\n![nutsdb-架构图](./docs/img/nutsdb-架构图.png)\n\n\n Welcome [contributions to NutsDB](https://github.com/nutsdb/nutsdb#contributing).\n\n## Quick start\n\n### Install NutsDB\n\nTo start using NutsDB, first needs [Go](https://golang.org/dl/) installed (version 1.18+ is required).  and run go get:\n\n```\ngo get -u github.com/nutsdb/nutsdb\n```\n\n### Opening a database\n\nTo open your database, use the nutsdb.Open() function,with the appropriate options.The `Dir` , `EntryIdxMode`  and  `SegmentSize`  options are must be specified by the client. About options see [here](https://github.com/nutsdb/nutsdb#options) for detail.\n\n```go\npackage main\n\nimport (\n    \"log\"\n\n    \"github.com/nutsdb/nutsdb\"\n)\n\nfunc main() {\n    // Open the database located in the /tmp/nutsdb directory.\n    // It will be created if it doesn't exist.\n    db, err := nutsdb.Open(\n        nutsdb.DefaultOptions,\n        nutsdb.WithDir(\"/tmp/nutsdb\"),\n    )\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer db.Close()\n\n    ...\n}\n```\n\n## Documentation\n\n<details>\n  <summary><b>Buckets</b></summary>\n\n- [Using buckets](./docs/user_guides/use-buckets.md)\n</details>\n\n<details>\n  <summary><b>Pairs</b></summary>\n\n- [Using key/value pairs](./docs/user_guides/use-kv-pair.md)\n</details>\n\n<details>\n  <summary><b>Iterator</b></summary>\n\n- [Iterating over keys](./docs/user_guides/iterator.md)\n</details>\n\n<details>\n  <summary><b>Data Structures</b></summary>\n\n- [List](./docs/user_guides/data-structure.md#list)\n- [Set](./docs/user_guides/data-structure.md#set)\n- [Sorted Set](./docs/user_guides/data-structure.md#sorted-set)\n</details>\n\n<details>\n  <summary><b>Database Options</b></summary>\n\n- [Options](./docs/user_guides/options.md)\n</details>\n\n<details>\n  <summary><b>More Operation</b></summary>\n\n- [More Operation](./docs/user_guides/others.md)\n</details>\n\n<details>\n  <summary><b>Comparison</b></summary>\n\n- [Comparison](./docs/user_guides/comparison.md)\n</details>\n\n<details>\n  <summary><b>Benchmark</b></summary>\n\n- [Benchmark](./docs/user_guides/benchmarks.md)\n</details>\n\n## Contributors\n\nThank you for considering contributing to NutsDB! The contribution guide can be found in the [CONTRIBUTING](https://github.com/nutsdb/nutsdb/blob/master/CONTRIBUTING.md) for details on submitting patches and the contribution workflow.\n\n<a href=\"https://github.com/nutsdb/nutsdb/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=nutsdb/nutsdb\" />\n</a>\n\n## Acknowledgements\n\nThis package is inspired by the following:\n\n- [Bitcask-intro](https://github.com/basho/bitcask/blob/develop/doc/bitcask-intro.pdf)\n- [BoltDB](https://github.com/boltdb)\n- [BuntDB](https://github.com/tidwall/buntdb)\n- [Redis](https://redis.io/)\n- [Sorted Set](https://github.com/wangjia184/sortedset)\n\n## License\n\nThe NutsDB is open-sourced software licensed under the [Apache 2.0 license](https://github.com/nutsdb/nutsdb/blob/master/LICENSE).\n"
        },
        {
          "name": "_config.yml",
          "type": "blob",
          "size": 0.025390625,
          "content": "theme: jekyll-theme-cayman"
        },
        {
          "name": "_typos.toml",
          "type": "blob",
          "size": 0.1826171875,
          "content": "# Typo check: https://github.com/crate-ci/typos\n\n[files]\nextend-exclude = [\"*.json\", \"*.js\", \"static/*\", \"layouts/*\", \"assets/*\", \"i18n/*\", \"go.*\"]\n\n[default.extend-words]\ndatas = \"datas\""
        },
        {
          "name": "batch.go",
          "type": "blob",
          "size": 3.36328125,
          "content": "package nutsdb\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"sync\"\n\t\"sync/atomic\"\n)\n\n// ErrCommitAfterFinish indicates that write batch commit was called after\nvar ErrCommitAfterFinish = errors.New(\"Batch commit not permitted after finish\")\n\nconst (\n\tDefaultThrottleSize = 16\n)\n\n// WriteBatch holds the necessary info to perform batched writes.\ntype WriteBatch struct {\n\tsync.Mutex\n\ttx       *Tx\n\tdb       *DB\n\tthrottle *Throttle\n\terr      atomic.Value\n\tfinished bool\n}\n\nfunc (db *DB) NewWriteBatch() (*WriteBatch, error) {\n\twb := &WriteBatch{\n\t\tdb:       db,\n\t\tthrottle: NewThrottle(DefaultThrottleSize),\n\t}\n\n\tvar err error\n\twb.tx, err = newTx(db, true)\n\treturn wb, err\n}\n\n// SetMaxPendingTxns sets a limit on maximum number of pending transactions while writing batches.\n// This function should be called before using WriteBatch. Default value of MaxPendingTxns is\n// 16 to minimise memory usage.\nfunc (wb *WriteBatch) SetMaxPendingTxns(max int) {\n\twb.throttle = NewThrottle(max)\n}\n\nfunc (wb *WriteBatch) Cancel() error {\n\twb.Lock()\n\twb.finished = true\n\twb.tx.setStatusClosed()\n\twb.Unlock()\n\n\tif err := wb.throttle.Finish(); err != nil {\n\t\treturn fmt.Errorf(\"WatchBatch.Cancel error while finishing: %v\", err)\n\t}\n\n\treturn nil\n}\n\nfunc (wb *WriteBatch) Put(bucket string, key, value []byte, ttl uint32) error {\n\twb.Lock()\n\tdefer wb.Unlock()\n\n\twb.tx.lock()\n\terr := wb.tx.Put(bucket, key, value, ttl)\n\tif err != nil {\n\t\twb.tx.unlock()\n\t\treturn err\n\t}\n\n\tif nil == wb.tx.checkSize() {\n\t\twb.tx.unlock()\n\t\treturn err\n\t}\n\n\twb.tx.unlock()\n\tif cerr := wb.commit(); cerr != nil {\n\t\treturn cerr\n\t}\n\n\treturn err\n}\n\n// func (tx *Tx) Delete(bucket string, key []byte) error\nfunc (wb *WriteBatch) Delete(bucket string, key []byte) error {\n\twb.Lock()\n\tdefer wb.Unlock()\n\n\twb.tx.lock()\n\terr := wb.tx.Delete(bucket, key)\n\tif err != nil {\n\t\twb.tx.unlock()\n\t\treturn err\n\t}\n\n\tif nil == wb.tx.checkSize() {\n\t\twb.tx.unlock()\n\t\treturn err\n\t}\n\n\twb.tx.unlock()\n\tif cerr := wb.commit(); cerr != nil {\n\t\treturn cerr\n\t}\n\n\treturn err\n}\n\nfunc (wb *WriteBatch) commit() error {\n\tif err := wb.Error(); err != nil {\n\t\treturn err\n\t}\n\n\tif wb.finished {\n\t\treturn ErrCommitAfterFinish\n\t}\n\n\tif err := wb.throttle.Do(); err != nil {\n\t\twb.err.Store(err)\n\t\treturn err\n\t}\n\n\twb.tx.CommitWith(wb.callback)\n\n\t// new a new tx\n\tvar err error\n\twb.tx, err = newTx(wb.db, true)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn wb.Error()\n}\n\nfunc (wb *WriteBatch) callback(err error) {\n\t// sync.WaitGroup is thread-safe, so it doesn't need to be run inside wb.Lock.\n\tdefer wb.throttle.Done(err)\n\tif err == nil {\n\t\treturn\n\t}\n\tif err := wb.Error(); err != nil {\n\t\treturn\n\t}\n\n\twb.err.Store(err)\n}\n\nfunc (wb *WriteBatch) Flush() error {\n\twb.Lock()\n\terr := wb.commit()\n\tif err != nil {\n\t\twb.Unlock()\n\t\treturn err\n\t}\n\twb.finished = true\n\twb.tx.setStatusClosed()\n\twb.Unlock()\n\tif err := wb.throttle.Finish(); err != nil {\n\t\tif wb.Error() != nil {\n\t\t\treturn fmt.Errorf(\"wb.err: %s err: %s\", wb.Error(), err)\n\t\t}\n\t\treturn err\n\t}\n\n\treturn wb.Error()\n}\n\nfunc (wb *WriteBatch) Reset() error {\n\twb.Lock()\n\tdefer wb.Unlock()\n\tvar err error\n\twb.finished = false\n\twb.tx, err = newTx(wb.db, true)\n\tif err != nil {\n\t\treturn err\n\t}\n\twb.throttle = NewThrottle(DefaultThrottleSize)\n\treturn err\n}\n\n// Error returns any errors encountered so far. No commits would be run once an error is detected.\nfunc (wb *WriteBatch) Error() error {\n\t// If the interface conversion fails, the err will be nil.\n\terr, _ := wb.err.Load().(error)\n\treturn err\n}\n"
        },
        {
          "name": "batch_test.go",
          "type": "blob",
          "size": 4.470703125,
          "content": "package nutsdb\n\nimport (\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"log\"\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n\t\"github.com/xujiajun/utils/time2\"\n)\n\nvar bucket string\n\nconst (\n\tfileDir  = \"/tmp/nutsdb/\"\n\tfileDir1 = \"/tmp/nutsdb/nutsdb_batch_write1\"\n\tfileDir2 = \"/tmp/nutsdb/nutsdb_batch_write2\"\n\tfileDir3 = \"/tmp/nutsdb/nutsdb_batch_write3\"\n\tfileDir4 = \"/tmp/nutsdb/nutsdb_batch_write4\"\n\tfileDir5 = \"/tmp/nutsdb/nutsdb_batch_write5\"\n\tfileDir6 = \"/tmp/nutsdb/nutsdb_batch_write6\"\n\tfileDir7 = \"/tmp/nutsdb/nutsdb_batch_write7\"\n\tfileDir8 = \"/tmp/nutsdb/nutsdb_batch_write8\"\n\tfileDir9 = \"/tmp/nutsdb/nutsdb_batch_write9\"\n\tN        = 100\n)\n\nfunc init() {\n\tbucket = \"bucketForBatchWrite\"\n\tfiles, err := ioutil.ReadDir(fileDir)\n\tif err != nil {\n\t\treturn\n\t}\n\tfor _, f := range files {\n\t\tname := f.Name()\n\t\tif name != \"\" {\n\t\t\tfilePath := fmt.Sprintf(\"%s/%s\", fileDir, name)\n\t\t\terr := os.RemoveAll(filePath)\n\t\t\tif err != nil {\n\t\t\t\tlog.Fatal(err)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestBatchWrite(t *testing.T) {\n\tTestFlushPanic := func(t *testing.T, db *DB) {\n\t\twb, err := db.NewWriteBatch()\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, wb.Flush())\n\t\trequire.Error(t, ErrCommitAfterFinish, wb.Flush())\n\t\twb, err = db.NewWriteBatch()\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, wb.Cancel())\n\t\trequire.Error(t, ErrCommitAfterFinish, wb.Flush())\n\t}\n\n\ttestEmptyWrite := func(t *testing.T, db *DB) {\n\t\twb, err := db.NewWriteBatch()\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, wb.Flush())\n\t\twb, err = db.NewWriteBatch()\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, wb.Flush())\n\t\twb, err = db.NewWriteBatch()\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, wb.Flush())\n\t}\n\n\ttestWrite := func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\tkey := func(i int) []byte {\n\t\t\treturn []byte(fmt.Sprintf(\"%10d\", i))\n\t\t}\n\t\tval := func(i int) []byte {\n\t\t\treturn []byte(fmt.Sprintf(\"%128d\", i))\n\t\t}\n\t\twb, err := db.NewWriteBatch()\n\t\trequire.NoError(t, err)\n\t\ttime2.Start()\n\t\tfor i := 0; i < N; i++ {\n\t\t\trequire.NoError(t, wb.Put(bucket, key(i), val(i), 0))\n\t\t}\n\t\trequire.NoError(t, wb.Flush())\n\t\t// fmt.Printf(\"Time taken via batch write %v keys: %v\\n\", N, time2.End())\n\n\t\ttime2.Start()\n\t\tif err := db.View(\n\t\t\tfunc(tx *Tx) error {\n\t\t\t\tfor i := 0; i < N; i++ {\n\t\t\t\t\tkey := key(i)\n\t\t\t\t\tvalue, err := tx.Get(bucket, key)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\t\trequire.Equal(t, val(i), value)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t}); err != nil {\n\t\t\tlog.Println(err)\n\t\t}\n\n\t\t// fmt.Printf(\"Time taken via db.View %v keys: %v\\n\", N, time2.End())\n\t\t// err = wb.Reset()\n\t\twb, err = db.NewWriteBatch()\n\t\trequire.NoError(t, err)\n\t\ttime2.Start()\n\t\tfor i := 0; i < N; i++ {\n\t\t\trequire.NoError(t, wb.Delete(bucket, key(i)))\n\t\t}\n\t\trequire.NoError(t, wb.Flush())\n\t\t// fmt.Printf(\"Time taken via batch delete %v keys: %v\\n\", N, time2.End())\n\n\t\tif err := db.View(\n\t\t\tfunc(tx *Tx) error {\n\t\t\t\tfor i := 0; i < N; i++ {\n\t\t\t\t\tkey := key(i)\n\t\t\t\t\t_, err := tx.Get(bucket, key)\n\t\t\t\t\trequire.Error(t, ErrNotFoundKey, err)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t}); err != nil {\n\t\t\tlog.Println(err)\n\t\t}\n\t}\n\n\tdbs := make([]*DB, 6)\n\tdbs[0], _ = Open(\n\t\tDefaultOptions,\n\t\tWithDir(fileDir1),\n\t\tWithEntryIdxMode(HintKeyValAndRAMIdxMode),\n\t)\n\tdbs[1], _ = Open(\n\t\tDefaultOptions,\n\t\tWithDir(fileDir2),\n\t\tWithEntryIdxMode(HintKeyAndRAMIdxMode),\n\t)\n\tdbs[2], _ = Open(\n\t\tDefaultOptions,\n\t\tWithDir(fileDir4),\n\t\tWithEntryIdxMode(HintKeyValAndRAMIdxMode),\n\t\tWithMaxBatchCount(35),\n\t)\n\tdbs[3], _ = Open(\n\t\tDefaultOptions,\n\t\tWithDir(fileDir5),\n\t\tWithEntryIdxMode(HintKeyAndRAMIdxMode),\n\t\tWithMaxBatchCount(35),\n\t)\n\tdbs[4], _ = Open(\n\t\tDefaultOptions,\n\t\tWithDir(fileDir7),\n\t\tWithEntryIdxMode(HintKeyValAndRAMIdxMode),\n\t\tWithMaxBatchSize(20), // change to 1000, unit test is not ok, 1000000 is ok\n\t)\n\tdbs[5], _ = Open(\n\t\tDefaultOptions,\n\t\tWithDir(fileDir8),\n\t\tWithEntryIdxMode(HintKeyAndRAMIdxMode),\n\t\tWithMaxBatchSize(20),\n\t)\n\n\tfor _, db := range dbs {\n\t\trequire.NotEqual(t, db, nil)\n\t\ttestWrite(t, db)\n\t\ttestEmptyWrite(t, db)\n\t\tTestFlushPanic(t, db)\n\t}\n}\n\nfunc TestWriteBatch_SetMaxPendingTxns(t *testing.T) {\n\tmax := 10\n\tdb, err := Open(\n\t\tDefaultOptions,\n\t\tWithDir(\"/tmp\"),\n\t)\n\trequire.NoError(t, err)\n\twb, err := db.NewWriteBatch()\n\trequire.NoError(t, err)\n\twb.SetMaxPendingTxns(max)\n\tif wb.throttle == nil {\n\t\tt.Error(\"Expected throttle to be initialized, but it was nil\")\n\t}\n\tif cap(wb.throttle.ch) != max {\n\t\tt.Errorf(\"Expected channel length to be %d, but got %d\", max, len(wb.throttle.ch))\n\t}\n\tif cap(wb.throttle.errCh) != max {\n\t\tt.Errorf(\"Expected error channel length to be %d, but got %d\", max, len(wb.throttle.errCh))\n\t}\n}\n"
        },
        {
          "name": "btree.go",
          "type": "blob",
          "size": 3.3125,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"regexp\"\n\n\t\"github.com/tidwall/btree\"\n)\n\n// ErrKeyNotFound is returned when the key is not in the b tree.\nvar ErrKeyNotFound = errors.New(\"key not found\")\n\ntype Item struct {\n\tkey    []byte\n\trecord *Record\n}\n\ntype BTree struct {\n\tbtree *btree.BTreeG[*Item]\n}\n\nfunc NewBTree() *BTree {\n\treturn &BTree{\n\t\tbtree: btree.NewBTreeG[*Item](func(a, b *Item) bool {\n\t\t\treturn bytes.Compare(a.key, b.key) == -1\n\t\t}),\n\t}\n}\n\nfunc (bt *BTree) Find(key []byte) (*Record, bool) {\n\titem, ok := bt.btree.Get(&Item{key: key})\n\tif ok {\n\t\treturn item.record, ok\n\t}\n\treturn nil, ok\n}\n\nfunc (bt *BTree) Insert(record *Record) bool {\n\t_, replaced := bt.btree.Set(&Item{key: record.Key, record: record})\n\treturn replaced\n}\n\nfunc (bt *BTree) InsertRecord(key []byte, record *Record) bool {\n\t_, replaced := bt.btree.Set(&Item{key: key, record: record})\n\treturn replaced\n}\n\nfunc (bt *BTree) Delete(key []byte) bool {\n\t_, deleted := bt.btree.Delete(&Item{key: key})\n\treturn deleted\n}\n\nfunc (bt *BTree) All() []*Record {\n\titems := bt.btree.Items()\n\n\trecords := make([]*Record, len(items))\n\tfor i, item := range items {\n\t\trecords[i] = item.record\n\t}\n\n\treturn records\n}\n\nfunc (bt *BTree) AllItems() []*Item {\n\titems := bt.btree.Items()\n\treturn items\n}\n\nfunc (bt *BTree) Range(start, end []byte) []*Record {\n\trecords := make([]*Record, 0)\n\n\tbt.btree.Ascend(&Item{key: start}, func(item *Item) bool {\n\t\tif bytes.Compare(item.key, end) > 0 {\n\t\t\treturn false\n\t\t}\n\t\trecords = append(records, item.record)\n\t\treturn true\n\t})\n\n\treturn records\n}\n\nfunc (bt *BTree) PrefixScan(prefix []byte, offset, limitNum int) []*Record {\n\trecords := make([]*Record, 0)\n\n\tbt.btree.Ascend(&Item{key: prefix}, func(item *Item) bool {\n\t\tif !bytes.HasPrefix(item.key, prefix) {\n\t\t\treturn false\n\t\t}\n\n\t\tif offset > 0 {\n\t\t\toffset--\n\t\t\treturn true\n\t\t}\n\n\t\trecords = append(records, item.record)\n\n\t\tlimitNum--\n\t\treturn limitNum != 0\n\t})\n\n\treturn records\n}\n\nfunc (bt *BTree) PrefixSearchScan(prefix []byte, reg string, offset, limitNum int) []*Record {\n\trecords := make([]*Record, 0)\n\n\trgx := regexp.MustCompile(reg)\n\n\tbt.btree.Ascend(&Item{key: prefix}, func(item *Item) bool {\n\t\tif !bytes.HasPrefix(item.key, prefix) {\n\t\t\treturn false\n\t\t}\n\n\t\tif offset > 0 {\n\t\t\toffset--\n\t\t\treturn true\n\t\t}\n\n\t\tif !rgx.Match(bytes.TrimPrefix(item.key, prefix)) {\n\t\t\treturn true\n\t\t}\n\n\t\trecords = append(records, item.record)\n\n\t\tlimitNum--\n\t\treturn limitNum != 0\n\t})\n\n\treturn records\n}\n\nfunc (bt *BTree) Count() int {\n\treturn bt.btree.Len()\n}\n\nfunc (bt *BTree) PopMin() (*Item, bool) {\n\treturn bt.btree.PopMin()\n}\n\nfunc (bt *BTree) PopMax() (*Item, bool) {\n\treturn bt.btree.PopMax()\n}\n\nfunc (bt *BTree) Min() (*Item, bool) {\n\treturn bt.btree.Min()\n}\n\nfunc (bt *BTree) Max() (*Item, bool) {\n\treturn bt.btree.Max()\n}\n"
        },
        {
          "name": "btree_test.go",
          "type": "blob",
          "size": 5.6875,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"fmt\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n\t\"testing\"\n)\n\nvar (\n\tkeyFormat = \"key_%03d\"\n\tvalFormat = \"val_%03d\"\n)\n\nfunc runBTreeTest(t *testing.T, test func(t *testing.T, btree *BTree)) {\n\tbtree := NewBTree()\n\n\tfor i := 0; i < 100; i++ {\n\t\tkey := []byte(fmt.Sprintf(keyFormat, i))\n\t\tval := []byte(fmt.Sprintf(valFormat, i))\n\n\t\t_ = btree.Insert(NewRecord().WithKey(key).WithValue(val))\n\t}\n\n\ttest(t, btree)\n}\n\nfunc TestBTree_Find(t *testing.T) {\n\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\t\tr, ok := btree.Find([]byte(fmt.Sprintf(keyFormat, 0)))\n\t\trequire.Equal(t, []byte(fmt.Sprintf(keyFormat, 0)), r.Key)\n\t\trequire.True(t, ok)\n\t})\n}\n\nfunc TestBTree_Delete(t *testing.T) {\n\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\t\trequire.True(t, btree.Delete([]byte(fmt.Sprintf(keyFormat, 0))))\n\t\trequire.False(t, btree.Delete([]byte(fmt.Sprintf(keyFormat, 100))))\n\n\t\t_, ok := btree.Find([]byte(fmt.Sprintf(keyFormat, 0)))\n\t\trequire.False(t, ok)\n\t})\n}\n\nfunc TestBTree_PrefixScan(t *testing.T) {\n\tt.Run(\"prefix scan from beginning\", func(t *testing.T) {\n\t\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\t\t\tlimit := 10\n\n\t\t\trecords := btree.PrefixScan([]byte(\"key_\"), 0, limit)\n\n\t\t\tfor i, r := range records {\n\t\t\t\twantKey := []byte(fmt.Sprintf(keyFormat, i))\n\t\t\t\twantValue := []byte(fmt.Sprintf(valFormat, i))\n\n\t\t\t\tassert.Equal(t, wantKey, r.Key)\n\t\t\t\tassert.Equal(t, wantValue, r.Value)\n\t\t\t}\n\t\t})\n\t})\n\n\tt.Run(\"prefix scan for not exists pre key\", func(t *testing.T) {\n\t\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\t\t\trecords := btree.PrefixScan([]byte(\"key_xx\"), 0, 10)\n\t\t\tassert.Len(t, records, 0)\n\t\t})\n\t})\n}\n\nfunc TestBTree_PrefixSearchScan(t *testing.T) {\n\tt.Run(\"prefix search scan right email\", func(t *testing.T) {\n\t\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\n\t\t\tkey := []byte(\"nutsdb-123456789@outlook.com\")\n\t\t\tval := GetRandomBytes(24)\n\n\t\t\t_ = btree.Insert(NewRecord().WithKey(key).WithValue(val))\n\n\t\t\trecord, ok := btree.Find(key)\n\t\t\trequire.True(t, ok)\n\t\t\trequire.Equal(t, key, record.Key)\n\n\t\t\trecords := btree.PrefixSearchScan([]byte(\"nutsdb-\"),\n\t\t\t\t\"[a-z\\\\d]+(\\\\.[a-z\\\\d]+)*@([\\\\da-z](-[\\\\da-z])?)+(\\\\.{1,2}[a-z]+)+$\", 0, 1)\n\t\t\trequire.Equal(t, key, records[0].Key)\n\t\t})\n\t})\n\n\tt.Run(\"prefix search scan wrong email\", func(t *testing.T) {\n\t\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\n\t\t\tkey := []byte(\"nutsdb-123456789@outlook\")\n\t\t\tval := GetRandomBytes(24)\n\n\t\t\t_ = btree.Insert(NewRecord().WithKey(key).WithValue(val))\n\n\t\t\trecord, ok := btree.Find(key)\n\t\t\trequire.True(t, ok)\n\t\t\trequire.Equal(t, key, record.Key)\n\n\t\t\trecords := btree.PrefixSearchScan([]byte(\"nutsdb-\"),\n\t\t\t\t\"[a-z\\\\d]+(\\\\.[a-z\\\\d]+)*@([\\\\da-z](-[\\\\da-z])?)+(\\\\.{1,2}[a-z]+)+$\", 0, 1)\n\t\t\trequire.Len(t, records, 0)\n\t\t})\n\t})\n}\n\nfunc TestBTree_All(t *testing.T) {\n\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\t\texpectRecords := make([]*Record, 100)\n\n\t\tfor i := 0; i < 100; i++ {\n\t\t\tkey := []byte(fmt.Sprintf(keyFormat, i))\n\t\t\tval := []byte(fmt.Sprintf(valFormat, i))\n\n\t\t\texpectRecords[i] = NewRecord().WithKey(key).WithValue(val)\n\t\t}\n\n\t\trequire.ElementsMatch(t, expectRecords, btree.All())\n\t})\n}\n\nfunc TestBTree_Range(t *testing.T) {\n\tt.Run(\"btree range at begin\", func(t *testing.T) {\n\t\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\t\t\texpectRecords := make([]*Record, 10)\n\n\t\t\tfor i := 0; i < 10; i++ {\n\t\t\t\tkey := []byte(fmt.Sprintf(keyFormat, i))\n\t\t\t\tval := []byte(fmt.Sprintf(valFormat, i))\n\n\t\t\t\texpectRecords[i] = NewRecord().WithKey(key).WithValue(val)\n\t\t\t}\n\n\t\t\trecords := btree.Range([]byte(fmt.Sprintf(keyFormat, 0)), []byte(fmt.Sprintf(keyFormat, 9)))\n\n\t\t\trequire.ElementsMatch(t, records, expectRecords)\n\t\t})\n\t})\n\n\tt.Run(\"btree range at middle\", func(t *testing.T) {\n\t\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\t\t\texpectRecords := make([]*Record, 10)\n\n\t\t\tfor i := 40; i < 50; i++ {\n\t\t\t\tkey := []byte(fmt.Sprintf(keyFormat, i))\n\t\t\t\tval := []byte(fmt.Sprintf(valFormat, i))\n\n\t\t\t\texpectRecords[i-40] = NewRecord().WithKey(key).WithValue(val)\n\t\t\t}\n\n\t\t\trecords := btree.Range([]byte(fmt.Sprintf(keyFormat, 40)), []byte(fmt.Sprintf(keyFormat, 49)))\n\n\t\t\trequire.ElementsMatch(t, records, expectRecords)\n\t\t})\n\t})\n\n\tt.Run(\"btree range at end\", func(t *testing.T) {\n\t\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\t\t\texpectRecords := make([]*Record, 10)\n\n\t\t\tfor i := 90; i < 100; i++ {\n\t\t\t\tkey := []byte(fmt.Sprintf(keyFormat, i))\n\t\t\t\tval := []byte(fmt.Sprintf(valFormat, i))\n\n\t\t\t\texpectRecords[i-90] = NewRecord().WithKey(key).WithValue(val)\n\t\t\t}\n\n\t\t\trecords := btree.Range([]byte(fmt.Sprintf(keyFormat, 90)), []byte(fmt.Sprintf(keyFormat, 99)))\n\n\t\t\trequire.ElementsMatch(t, records, expectRecords)\n\t\t})\n\t})\n}\n\nfunc TestBTree_Update(t *testing.T) {\n\trunBTreeTest(t, func(t *testing.T, btree *BTree) {\n\t\tfor i := 40; i < 50; i++ {\n\t\t\tkey := []byte(fmt.Sprintf(keyFormat, i))\n\t\t\tval := []byte(fmt.Sprintf(\"val_%03d_modify\", i))\n\n\t\t\tbtree.Insert(NewRecord().WithKey(key).WithValue(val))\n\t\t}\n\n\t\trecords := btree.Range([]byte(fmt.Sprintf(keyFormat, 40)), []byte(fmt.Sprintf(keyFormat, 49)))\n\n\t\tfor i := 40; i < 50; i++ {\n\t\t\trequire.Equal(t, []byte(fmt.Sprintf(\"val_%03d_modify\", i)), records[i-40].Value)\n\t\t}\n\t})\n}\n"
        },
        {
          "name": "bucket.go",
          "type": "blob",
          "size": 2.818359375,
          "content": "package nutsdb\n\nimport (\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"hash/crc32\"\n)\n\nvar BucketMetaSize int64\n\nconst (\n\tIdSize = 8\n\tDsSize = 2\n)\n\ntype BucketOperation uint16\n\nconst (\n\tBucketInsertOperation BucketOperation = 1\n\tBucketUpdateOperation BucketOperation = 2\n\tBucketDeleteOperation BucketOperation = 3\n)\n\nvar ErrBucketCrcInvalid = errors.New(\"bucket crc invalid\")\n\nfunc init() {\n\tBucketMetaSize = GetDiskSizeFromSingleObject(BucketMeta{})\n}\n\n// BucketMeta stores the Meta info of a Bucket. E.g. the size of bucket it store in disk.\ntype BucketMeta struct {\n\tCrc uint32\n\t// Op: Mark the latest operation (e.g. delete, insert, update) for this bucket.\n\tOp BucketOperation\n\t// Size: the size of payload.\n\tSize uint32\n}\n\n// Bucket is the disk structure of bucket\ntype Bucket struct {\n\t// Meta: the metadata for this bucket\n\tMeta *BucketMeta\n\t// Id: is the marker for this bucket, every bucket creation activity will generate a new Id for it.\n\t// for example. If you have a bucket called \"bucket_1\", and you just delete bucket and create it again.\n\t// the last bucket will have a different Id from the previous one.\n\tId BucketId\n\t// Ds: the data structure for this bucket. (List, Set, SortSet, String)\n\tDs Ds\n\t// Name: the name of this bucket.\n\tName string\n}\n\n// Decode : CRC | op | size\nfunc (meta *BucketMeta) Decode(bytes []byte) {\n\t_ = bytes[BucketMetaSize-1]\n\tcrc := binary.LittleEndian.Uint32(bytes[:4])\n\top := binary.LittleEndian.Uint16(bytes[4:6])\n\tsize := binary.LittleEndian.Uint32(bytes[6:10])\n\tmeta.Crc = crc\n\tmeta.Size = size\n\tmeta.Op = BucketOperation(op)\n}\n\n// Encode : Meta | BucketId | Ds | BucketName\nfunc (b *Bucket) Encode() []byte {\n\tentrySize := b.GetEntrySize()\n\tbuf := make([]byte, entrySize)\n\tb.Meta.Size = uint32(b.GetPayloadSize())\n\tbinary.LittleEndian.PutUint16(buf[4:6], uint16(b.Meta.Op))\n\tbinary.LittleEndian.PutUint32(buf[6:10], b.Meta.Size)\n\tbinary.LittleEndian.PutUint64(buf[BucketMetaSize:BucketMetaSize+IdSize], uint64(b.Id))\n\tbinary.LittleEndian.PutUint16(buf[BucketMetaSize+IdSize:BucketMetaSize+IdSize+DsSize], uint16(b.Ds))\n\tcopy(buf[BucketMetaSize+IdSize+DsSize:], b.Name)\n\tc32 := crc32.ChecksumIEEE(buf[4:])\n\tb.Meta.Crc = c32\n\tbinary.LittleEndian.PutUint32(buf[0:4], c32)\n\n\treturn buf\n}\n\n// Decode : Meta | BucketId | Ds | BucketName\nfunc (b *Bucket) Decode(bytes []byte) error {\n\t// parse the payload\n\tid := binary.LittleEndian.Uint64(bytes[:IdSize])\n\tds := binary.LittleEndian.Uint16(bytes[IdSize : IdSize+DsSize])\n\tname := bytes[IdSize+DsSize:]\n\tb.Id = id\n\tb.Name = string(name)\n\tb.Ds = ds\n\treturn nil\n}\n\nfunc (b *Bucket) GetEntrySize() int {\n\treturn int(BucketMetaSize) + b.GetPayloadSize()\n}\n\nfunc (b *Bucket) GetCRC(headerBuf []byte, dataBuf []byte) uint32 {\n\tcrc := crc32.ChecksumIEEE(headerBuf[4:])\n\tcrc = crc32.Update(crc, crc32.IEEETable, dataBuf)\n\treturn crc\n}\n\nfunc (b *Bucket) GetPayloadSize() int {\n\treturn IdSize + DsSize + len(b.Name)\n}\n"
        },
        {
          "name": "bucket_manager.go",
          "type": "blob",
          "size": 3.15234375,
          "content": "package nutsdb\n\nimport (\n\t\"errors\"\n\t\"os\"\n)\n\nvar ErrBucketNotExist = errors.New(\"bucket not exist\")\n\nconst BucketStoreFileName = \"bucket.Meta\"\n\ntype Ds = uint16\ntype BucketId = uint64\ntype BucketName = string\ntype IDMarkerInBucket map[BucketName]map[Ds]BucketId\ntype InfoMapperInBucket map[BucketId]*Bucket\n\ntype BucketManager struct {\n\tfd *os.File\n\t// BucketInfoMapper BucketID => Bucket itself\n\tBucketInfoMapper InfoMapperInBucket\n\n\tBucketIDMarker IDMarkerInBucket\n\n\t// IDGenerator helps generates an ID for every single bucket\n\tGen *IDGenerator\n}\n\nfunc NewBucketManager(dir string) (*BucketManager, error) {\n\tbm := &BucketManager{\n\t\tBucketInfoMapper: map[BucketId]*Bucket{},\n\t\tBucketIDMarker:   map[BucketName]map[Ds]BucketId{},\n\t}\n\tbucketFilePath := dir + \"/\" + BucketStoreFileName\n\t_, err := os.Stat(bucketFilePath)\n\tmode := os.O_RDWR\n\tif err != nil {\n\t\tmode |= os.O_CREATE\n\t}\n\tfd, err := os.OpenFile(bucketFilePath, mode, os.ModePerm)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbm.fd = fd\n\tbm.Gen = &IDGenerator{currentMaxId: 0}\n\treturn bm, nil\n}\n\ntype bucketSubmitRequest struct {\n\tds     Ds\n\tname   BucketName\n\tbucket *Bucket\n}\n\nfunc (bm *BucketManager) SubmitPendingBucketChange(reqs []*bucketSubmitRequest) error {\n\tbytes := make([]byte, 0)\n\tfor _, req := range reqs {\n\t\tbs := req.bucket.Encode()\n\t\tbytes = append(bytes, bs...)\n\t\t// update the marker info\n\t\tif _, exist := bm.BucketIDMarker[req.name]; !exist {\n\t\t\tbm.BucketIDMarker[req.name] = map[Ds]BucketId{}\n\t\t}\n\t\t// recover maxid otherwise new bucket start from 1 again\n\t\tbm.Gen.CompareAndSetMaxId(req.bucket.Id)\n\t\tswitch req.bucket.Meta.Op {\n\t\tcase BucketInsertOperation:\n\t\t\tbm.BucketInfoMapper[req.bucket.Id] = req.bucket\n\t\t\tbm.BucketIDMarker[req.name][req.bucket.Ds] = req.bucket.Id\n\t\tcase BucketDeleteOperation:\n\t\t\tif len(bm.BucketIDMarker[req.name]) == 1 {\n\t\t\t\tdelete(bm.BucketIDMarker, req.name)\n\t\t\t} else {\n\t\t\t\tdelete(bm.BucketIDMarker[req.name], req.bucket.Ds)\n\t\t\t}\n\t\t\tdelete(bm.BucketInfoMapper, req.bucket.Id)\n\t\t}\n\t}\n\t_, err := bm.fd.Write(bytes)\n\treturn err\n}\n\ntype IDGenerator struct {\n\tcurrentMaxId uint64\n}\n\nfunc (g *IDGenerator) GenId() uint64 {\n\tg.currentMaxId++\n\treturn g.currentMaxId\n}\n\nfunc (g *IDGenerator) CompareAndSetMaxId(id uint64) {\n\tif id > g.currentMaxId {\n\t\tg.currentMaxId = id\n\t}\n}\n\nfunc (bm *BucketManager) ExistBucket(ds Ds, name BucketName) bool {\n\tbucket, err := bm.GetBucket(ds, name)\n\tif bucket != nil && err == nil {\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (bm *BucketManager) GetBucket(ds Ds, name BucketName) (b *Bucket, err error) {\n\tds2IdMapper := bm.BucketIDMarker[name]\n\tif ds2IdMapper == nil {\n\t\treturn nil, ErrBucketNotExist\n\t}\n\n\tif id, exist := ds2IdMapper[ds]; exist {\n\t\tif bucket, ok := bm.BucketInfoMapper[id]; ok {\n\t\t\treturn bucket, nil\n\t\t} else {\n\t\t\treturn nil, ErrBucketNotExist\n\t\t}\n\t} else {\n\t\treturn nil, ErrBucketNotExist\n\t}\n}\n\nfunc (bm *BucketManager) GetBucketById(id BucketId) (*Bucket, error) {\n\tif bucket, exist := bm.BucketInfoMapper[id]; exist {\n\t\treturn bucket, nil\n\t} else {\n\t\treturn nil, ErrBucketNotExist\n\t}\n}\n\nfunc (bm *BucketManager) GetBucketID(ds Ds, name BucketName) (BucketId, error) {\n\tif bucket, err := bm.GetBucket(ds, name); err != nil {\n\t\treturn 0, err\n\t} else {\n\t\treturn bucket.Id, nil\n\t}\n}\n"
        },
        {
          "name": "bucket_manager_test.go",
          "type": "blob",
          "size": 3.361328125,
          "content": "package nutsdb\n\nimport (\n\t\"github.com/stretchr/testify/assert\"\n\t\"testing\"\n)\n\nfunc TestBucketManager_NewBucketAndDeleteBucket(t *testing.T) {\n\tbucket1 := \"bucket_1\"\n\tbucket2 := \"bucket_2\"\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxNewBucket(t, db, bucket1, DataStructureBTree, nil, nil)\n\t\texist := db.bm.ExistBucket(DataStructureBTree, bucket1)\n\t\tassert.Equal(t, true, exist)\n\t\ttxNewBucket(t, db, bucket2, DataStructureBTree, nil, nil)\n\t\texist = db.bm.ExistBucket(DataStructureBTree, bucket2)\n\t\tassert.Equal(t, true, exist)\n\t})\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxNewBucket(t, db, bucket1, DataStructureBTree, nil, nil)\n\t\texist := db.bm.ExistBucket(DataStructureBTree, bucket1)\n\t\tassert.Equal(t, true, exist)\n\t\ttxDeleteBucketFunc(t, db, bucket1, DataStructureBTree, nil, nil)\n\t\texist = db.bm.ExistBucket(DataStructureBTree, bucket1)\n\t\tassert.Equal(t, false, exist)\n\t})\n}\n\nfunc TestBucketManager_ExistBucket(t *testing.T) {\n\tbucket1 := \"bucket_1\"\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\texist := db.bm.ExistBucket(DataStructureBTree, bucket1)\n\t\tassert.Equal(t, false, exist)\n\n\t\ttxNewBucket(t, db, bucket1, DataStructureBTree, nil, nil)\n\t\texist = db.bm.ExistBucket(DataStructureBTree, bucket1)\n\t\tassert.Equal(t, true, exist)\n\t})\n}\n\nfunc TestBucketManager_Recovery(t *testing.T) {\n\tdir := \"/tmp/nutsdb_test_data\"\n\tconst bucket1 = \"bucket_1\"\n\tconst bucket2 = \"bucket_2\"\n\tdb, err := Open(DefaultOptions, WithDir(dir))\n\tdefer removeDir(dir)\n\tassert.NotNil(t, db)\n\tassert.Nil(t, err)\n\ttxNewBucket(t, db, bucket1, DataStructureBTree, nil, nil)\n\ttxNewBucket(t, db, bucket2, DataStructureBTree, nil, nil)\n\ttxDeleteBucketFunc(t, db, bucket1, DataStructureBTree, nil, nil)\n\tdb.Close()\n\n\tdb, err = Open(DefaultOptions, WithDir(dir))\n\tassert.Nil(t, err)\n\tassert.NotNil(t, db)\n\n\terr = db.View(func(tx *Tx) error {\n\t\texist := tx.ExistBucket(DataStructureBTree, bucket2)\n\t\tassert.Equal(t, true, exist)\n\t\texist = tx.ExistBucket(DataStructureBTree, bucket1)\n\t\tassert.Equal(t, false, exist)\n\t\treturn nil\n\t})\n\tassert.Nil(t, err)\n}\n\nfunc TestBucketManager_DataStructureIsolation(t *testing.T) {\n\tconst bucket1 = \"bucket_1\"\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket1, nil)\n\n\t\tassert.Equal(t, false, db.bm.ExistBucket(DataStructureList, bucket1))\n\t\tassert.Equal(t, false, db.bm.ExistBucket(DataStructureSortedSet, bucket1))\n\t\tassert.Equal(t, false, db.bm.ExistBucket(DataStructureSet, bucket1))\n\t})\n}\n\nfunc TestBucketManager_DeleteBucketIsolation(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tconst bucket1 = \"bucket_1\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket1, nil)\n\t\ttxPut(t, db, bucket1, []byte(\"key_1\"), []byte(\"value_1\"), Persistent, nil, nil)\n\t\ttxDeleteBucket(t, db, DataStructureBTree, bucket1, nil)\n\t\ttxGet(t, db, bucket1, []byte(\"key_1\"), nil, ErrBucketNotExist)\n\t})\n}\n\nfunc txNewBucket(t *testing.T, db *DB, bucket string, ds uint16, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr2 := tx.NewBucket(ds, bucket)\n\t\tassertErr(t, expectErr, err2)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n\n}\n\nfunc txDeleteBucketFunc(t *testing.T, db *DB, bucket string, ds uint16, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr2 := tx.DeleteBucket(ds, bucket)\n\t\tassertErr(t, expectErr, err2)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n"
        },
        {
          "name": "bucket_test.go",
          "type": "blob",
          "size": 0.7880859375,
          "content": "package nutsdb\n\nimport (\n\t\"github.com/stretchr/testify/assert\"\n\t\"testing\"\n)\n\nfunc TestBucket_DecodeAndDecode(t *testing.T) {\n\tbucket := &Bucket{\n\t\tMeta: &BucketMeta{\n\t\t\tOp: BucketInsertOperation,\n\t\t},\n\t\tId:   1,\n\t\tDs:   DataStructureBTree,\n\t\tName: \"bucket_1\",\n\t}\n\tbytes := bucket.Encode()\n\n\tbucketMeta := &BucketMeta{}\n\tbucketMeta.Decode(bytes[:BucketMetaSize])\n\tassert.Equal(t, bucketMeta.Op, BucketInsertOperation)\n\tassert.Equal(t, int64(8+2+8), int64(bucketMeta.Size))\n\tdecodeBucket := &Bucket{Meta: bucketMeta}\n\n\terr := decodeBucket.Decode(bytes[BucketMetaSize:])\n\tassert.Nil(t, err)\n\tassert.Equal(t, BucketId(1), decodeBucket.Id)\n\tassert.Equal(t, decodeBucket.Name, \"bucket_1\")\n\n\tcrc := decodeBucket.GetCRC(bytes[:BucketMetaSize], bytes[BucketMetaSize:])\n\tassert.Equal(t, decodeBucket.Meta.Crc, crc)\n}\n"
        },
        {
          "name": "const.go",
          "type": "blob",
          "size": 0.6884765625,
          "content": "//go:build !386 && !arm\n\n// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport \"math\"\n\nconst MAX_SIZE = math.MaxUint32\n"
        },
        {
          "name": "const_32bits.go",
          "type": "blob",
          "size": 0.685546875,
          "content": "//go:build 386 || arm\n\n// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport \"math\"\n\nconst MAX_SIZE = math.MaxInt32\n"
        },
        {
          "name": "datafile.go",
          "type": "blob",
          "size": 3.1572265625,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"errors\"\n)\n\nvar (\n\t// ErrCrc is returned when crc is error\n\tErrCrc = errors.New(\"crc error\")\n\n\t// ErrCapacity is returned when capacity is error.\n\tErrCapacity = errors.New(\"capacity error\")\n\n\tErrEntryZero = errors.New(\"entry is zero \")\n)\n\nconst (\n\t// DataSuffix returns the data suffix\n\tDataSuffix = \".dat\"\n)\n\n// DataFile records about data file information.\ntype DataFile struct {\n\tpath       string\n\tfileID     int64\n\twriteOff   int64\n\tActualSize int64\n\trwManager  RWManager\n}\n\n// NewDataFile will return a new DataFile Object.\nfunc NewDataFile(path string, rwManager RWManager) *DataFile {\n\tdataFile := &DataFile{\n\t\tpath:      path,\n\t\trwManager: rwManager,\n\t}\n\treturn dataFile\n}\n\n// ReadEntry returns entry at the given off(offset).\n// payloadSize = bucketSize + keySize + valueSize\nfunc (df *DataFile) ReadEntry(off int, payloadSize int64) (e *Entry, err error) {\n\tsize := MaxEntryHeaderSize + payloadSize\n\t// Since MaxEntryHeaderSize + payloadSize may be larger than the actual entry size, it needs to be calculated\n\tif int64(off)+size > df.rwManager.Size() {\n\t\tsize = df.rwManager.Size() - int64(off)\n\t}\n\tbuf := make([]byte, size)\n\n\tif _, err := df.rwManager.ReadAt(buf, int64(off)); err != nil {\n\t\treturn nil, err\n\t}\n\n\te = new(Entry)\n\theaderSize, err := e.ParseMeta(buf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Remove the content after the Header\n\tbuf = buf[:int(headerSize+payloadSize)]\n\n\tif e.IsZero() {\n\t\treturn nil, ErrEntryZero\n\t}\n\n\tif err := e.checkPayloadSize(payloadSize); err != nil {\n\t\treturn nil, err\n\t}\n\n\terr = e.ParsePayload(buf[headerSize:])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcrc := e.GetCrc(buf[:headerSize])\n\tif crc != e.Meta.Crc {\n\t\treturn nil, ErrCrc\n\t}\n\n\treturn\n}\n\n// WriteAt copies data to mapped region from the b slice starting at\n// given off and returns number of bytes copied to the mapped region.\nfunc (df *DataFile) WriteAt(b []byte, off int64) (n int, err error) {\n\treturn df.rwManager.WriteAt(b, off)\n}\n\n// Sync commits the current contents of the file to stable storage.\n// Typically, this means flushing the file system's in-memory copy\n// of recently written data to disk.\nfunc (df *DataFile) Sync() (err error) {\n\treturn df.rwManager.Sync()\n}\n\n// Close closes the RWManager.\n// If RWManager is FileRWManager represents closes the File,\n// rendering it unusable for I/O.\n// If RWManager is a MMapRWManager represents Unmap deletes the memory mapped region,\n// flushes any remaining changes.\nfunc (df *DataFile) Close() (err error) {\n\treturn df.rwManager.Close()\n}\n\nfunc (df *DataFile) Release() (err error) {\n\treturn df.rwManager.Release()\n}\n"
        },
        {
          "name": "datafile_test.go",
          "type": "blob",
          "size": 4.828125,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nvar (\n\tfilePath string\n\tentry    Entry\n)\n\nfunc init() {\n\tfilePath = \"/tmp/foo\"\n\tentry = Entry{\n\t\tKey:   []byte(\"key_0001\"),\n\t\tValue: []byte(\"val_0001\"),\n\t\tMeta: NewMetaData().WithKeySize(uint32(len(\"key_0001\"))).\n\t\t\tWithValueSize(uint32(len(\"val_0001\"))).WithTimeStamp(1547707905).\n\t\t\tWithTTL(Persistent).WithFlag(DataSetFlag).WithBucketId(1),\n\t}\n}\n\nfunc TestDataFile_Err(t *testing.T) {\n\tfm := newFileManager(MMap, 1024, 0.5, 256*MB)\n\tdefer fm.close()\n\t_, err := fm.getDataFile(filePath, -1)\n\tdefer func() {\n\t\tos.Remove(filePath)\n\t}()\n\n\tassert.NotNil(t, err)\n}\n\nfunc TestDataFile1(t *testing.T) {\n\tfm := newFileManager(MMap, 1024, 0.5, 256*MB)\n\tdefer fm.close()\n\tdf, err := fm.getDataFile(filePath, 1024)\n\tdefer os.Remove(filePath)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tn, err := df.WriteAt(entry.Encode(), 0)\n\tif err != nil {\n\t\tt.Error(\"err TestDataFile_All WriteAt\")\n\t}\n\n\tpayloadSize := entry.Meta.PayloadSize()\n\te, err := df.ReadEntry(n, payloadSize)\n\tassert.Nil(t, e)\n\tassert.Error(t, err, ErrEntryZero)\n\n\te, err = df.ReadEntry(0, payloadSize)\n\tif err != nil || string(e.Key) != \"key_0001\" || string(e.Value) != \"val_0001\" || e.Meta.Timestamp != 1547707905 {\n\t\tt.Error(\"err TestDataFile_All ReadAt\")\n\t}\n\n\te, err = df.ReadEntry(1, payloadSize)\n\tif err == nil || e != nil {\n\t\tt.Error(\"err TestDataFile_All ReadAt\")\n\t}\n}\n\nfunc TestDataFile2(t *testing.T) {\n\tfm := newFileManager(FileIO, 1024, 0.5, 256*MB)\n\n\tfilePath2 := \"/tmp/foo2\"\n\tdf, err := fm.getDataFile(filePath2, 64)\n\tassert.Nil(t, err)\n\tdefer os.Remove(filePath2)\n\theaderSize := entry.Meta.Size()\n\tcontent := entry.Encode()[0 : headerSize-1]\n\t_, err = df.WriteAt(content, 0)\n\tif err != nil {\n\t\tt.Error(\"err TestDataFile_All WriteAt\")\n\t}\n\n\tpayloadSize := entry.Meta.PayloadSize()\n\te, err := df.ReadEntry(0, payloadSize)\n\tif err == nil || e != nil {\n\t\tt.Error(\"err TestDataFile_All ReadAt\")\n\t}\n\n\tfilePath3 := \"/tmp/foo3\"\n\n\tdf2, err := fm.getDataFile(filePath3, 64)\n\tdefer os.Remove(filePath3)\n\tassert.Nil(t, err)\n\n\theaderSize = entry.Meta.Size()\n\tcontent = entry.Encode()[0 : headerSize+1]\n\t_, err = df2.WriteAt(content, 0)\n\tassert.Nil(t, err)\n\n\te, err = df2.ReadEntry(0, payloadSize)\n\tif err == nil || e != nil {\n\t\tt.Error(\"err TestDataFile_All ReadAt\")\n\t}\n\n\terr = df.Release()\n\tassert.Nil(t, err)\n\terr = df2.Release()\n\tassert.Nil(t, err)\n\terr = fm.close()\n\tassert.Nil(t, err)\n}\n\nfunc TestDataFile_ReadRecord(t *testing.T) {\n\tfm := newFileManager(FileIO, 1024, 0.5, 256*MB)\n\tfilePath4 := \"/tmp/foo4\"\n\tdf, err := fm.getDataFile(filePath4, 1024)\n\tdefer func() {\n\t\terr = df.Release()\n\t\tassert.Nil(t, err)\n\t\terr = fm.close()\n\t\tassert.Nil(t, err)\n\t}()\n\tassert.Nil(t, err)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tpayloadSize := entry.Meta.PayloadSize()\n\te, err := df.ReadEntry(0, payloadSize)\n\tif err != nil && e != nil {\n\t\tt.Error(\"err ReadAt\")\n\t}\n\n\te, err = df.ReadEntry(1025, payloadSize)\n\tif err == nil && e != nil {\n\t\tt.Error(\"err ReadAt\")\n\t}\n}\n\nfunc TestDataFile_Err_Path(t *testing.T) {\n\tfm := newFileManager(FileIO, 1024, 0.5, 256*MB)\n\tdefer fm.close()\n\tfilePath5 := \":/tmp/foo5\"\n\tdf, err := fm.getDataFile(filePath5, entry.Size())\n\tif err == nil && df != nil {\n\t\tt.Error(\"err TestDataFile_All open\")\n\t}\n}\n\nfunc TestDataFile_Crc_Err(t *testing.T) {\n\tfm := newFileManager(FileIO, 1024, 0.5, 256*MB)\n\tfilePath4 := \"/tmp/foo6\"\n\n\tdf, err := fm.getDataFile(filePath4, entry.Size())\n\tassert.Nil(t, err)\n\tassert.NotNil(t, df)\n\tdefer func() {\n\t\terr = df.Release()\n\t\tassert.Nil(t, err)\n\t\terr = fm.close()\n\t\tassert.Nil(t, err)\n\t\terr = os.Remove(filePath4)\n\t\tassert.Nil(t, err)\n\t}()\n\n\tvar errContent []byte\n\terrContent = append(errContent, entry.Encode()[0:4]...)\n\terrContent = append(errContent, entry.Encode()[4:entry.Size()-1]...)\n\terrContent = append(errContent, 0)\n\t_, err = df.WriteAt(errContent, 0)\n\tassert.Nil(t, err)\n\n\tpayloadSize := entry.Meta.PayloadSize()\n\te, err := df.ReadEntry(0, payloadSize)\n\tif err == nil || e != nil {\n\t\tt.Error(\"err TestDataFile_All ReadAt\")\n\t}\n}\n\nfunc TestFileManager1(t *testing.T) {\n\tfm := newFileManager(FileIO, 1024, 0.5, 256*MB)\n\tfilePath4 := \"/tmp/foo6\"\n\tdf, err := fm.getDataFile(filePath4, entry.Size())\n\tassert.Nil(t, err)\n\tdefer func() {\n\t\terr = df.Release()\n\t\tassert.Nil(t, err)\n\t\terr = fm.close()\n\t\tassert.Nil(t, err)\n\t\tos.Remove(filePath)\n\t}()\n}\n"
        },
        {
          "name": "db.go",
          "type": "blob",
          "size": 20.822265625,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/gofrs/flock\"\n\t\"github.com/xujiajun/utils/filesystem\"\n\t\"github.com/xujiajun/utils/strconv2\"\n)\n\n// ScanNoLimit represents the data scan no limit flag\nconst ScanNoLimit int = -1\nconst KvWriteChCapacity = 1000\nconst FLockName = \"nutsdb-flock\"\n\ntype (\n\t// DB represents a collection of buckets that persist on disk.\n\tDB struct {\n\t\topt                     Options // the database options\n\t\tIndex                   *index\n\t\tActiveFile              *DataFile\n\t\tMaxFileID               int64\n\t\tmu                      sync.RWMutex\n\t\tKeyCount                int // total key number ,include expired, deleted, repeated.\n\t\tclosed                  bool\n\t\tisMerging               bool\n\t\tfm                      *fileManager\n\t\tflock                   *flock.Flock\n\t\tcommitBuffer            *bytes.Buffer\n\t\tmergeStartCh            chan struct{}\n\t\tmergeEndCh              chan error\n\t\tmergeWorkCloseCh        chan struct{}\n\t\twriteCh                 chan *request\n\t\ttm                      *ttlManager\n\t\tRecordCount             int64 // current valid record count, exclude deleted, repeated\n\t\tbm                      *BucketManager\n\t\thintKeyAndRAMIdxModeLru *LRUCache // lru cache for HintKeyAndRAMIdxMode\n\t}\n)\n\n// open returns a newly initialized DB object.\nfunc open(opt Options) (*DB, error) {\n\tdb := &DB{\n\t\tMaxFileID:               0,\n\t\topt:                     opt,\n\t\tKeyCount:                0,\n\t\tclosed:                  false,\n\t\tIndex:                   newIndex(),\n\t\tfm:                      newFileManager(opt.RWMode, opt.MaxFdNumsInCache, opt.CleanFdsCacheThreshold, opt.SegmentSize),\n\t\tmergeStartCh:            make(chan struct{}),\n\t\tmergeEndCh:              make(chan error),\n\t\tmergeWorkCloseCh:        make(chan struct{}),\n\t\twriteCh:                 make(chan *request, KvWriteChCapacity),\n\t\ttm:                      newTTLManager(opt.ExpiredDeleteType),\n\t\thintKeyAndRAMIdxModeLru: NewLruCache(opt.HintKeyAndRAMIdxCacheSize),\n\t}\n\n\tdb.commitBuffer = createNewBufferWithSize(int(db.opt.CommitBufferSize))\n\n\tif ok := filesystem.PathIsExist(db.opt.Dir); !ok {\n\t\tif err := os.MkdirAll(db.opt.Dir, os.ModePerm); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tfileLock := flock.New(filepath.Join(opt.Dir, FLockName))\n\tif ok, err := fileLock.TryLock(); err != nil {\n\t\treturn nil, err\n\t} else if !ok {\n\t\treturn nil, ErrDirLocked\n\t}\n\n\tdb.flock = fileLock\n\n\tif bm, err := NewBucketManager(opt.Dir); err == nil {\n\t\tdb.bm = bm\n\t} else {\n\t\treturn nil, err\n\t}\n\n\tif err := db.rebuildBucketManager(); err != nil {\n\t\treturn nil, fmt.Errorf(\"db.rebuildBucketManager err:%s\", err)\n\t}\n\n\tif err := db.buildIndexes(); err != nil {\n\t\treturn nil, fmt.Errorf(\"db.buildIndexes error: %s\", err)\n\t}\n\n\tgo db.mergeWorker()\n\tgo db.doWrites()\n\tgo db.tm.run()\n\n\treturn db, nil\n}\n\n// Open returns a newly initialized DB object with Option.\nfunc Open(options Options, ops ...Option) (*DB, error) {\n\topts := &options\n\tfor _, do := range ops {\n\t\tdo(opts)\n\t}\n\treturn open(*opts)\n}\n\n// Update executes a function within a managed read/write transaction.\nfunc (db *DB) Update(fn func(tx *Tx) error) error {\n\tif fn == nil {\n\t\treturn ErrFn\n\t}\n\n\treturn db.managed(true, fn)\n}\n\n// View executes a function within a managed read-only transaction.\nfunc (db *DB) View(fn func(tx *Tx) error) error {\n\tif fn == nil {\n\t\treturn ErrFn\n\t}\n\n\treturn db.managed(false, fn)\n}\n\n// Backup copies the database to file directory at the given dir.\nfunc (db *DB) Backup(dir string) error {\n\treturn db.View(func(tx *Tx) error {\n\t\treturn filesystem.CopyDir(db.opt.Dir, dir)\n\t})\n}\n\n// BackupTarGZ Backup copy the database to writer.\nfunc (db *DB) BackupTarGZ(w io.Writer) error {\n\treturn db.View(func(tx *Tx) error {\n\t\treturn tarGZCompress(w, db.opt.Dir)\n\t})\n}\n\n// Close releases all db resources.\nfunc (db *DB) Close() error {\n\tdb.mu.Lock()\n\tdefer db.mu.Unlock()\n\n\tif db.closed {\n\t\treturn ErrDBClosed\n\t}\n\n\tdb.closed = true\n\n\terr := db.release()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// release set all obj in the db instance to nil\nfunc (db *DB) release() error {\n\tGCEnable := db.opt.GCWhenClose\n\n\terr := db.ActiveFile.rwManager.Release()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdb.Index = nil\n\n\tdb.ActiveFile = nil\n\n\terr = db.fm.close()\n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdb.mergeWorkCloseCh <- struct{}{}\n\n\tif !db.flock.Locked() {\n\t\treturn ErrDirUnlocked\n\t}\n\n\terr = db.flock.Unlock()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdb.fm = nil\n\n\tdb.tm.close()\n\n\tif GCEnable {\n\t\truntime.GC()\n\t}\n\n\treturn nil\n}\n\nfunc (db *DB) getValueByRecord(record *Record) ([]byte, error) {\n\tif record == nil {\n\t\treturn nil, ErrRecordIsNil\n\t}\n\n\tif record.Value != nil {\n\t\treturn record.Value, nil\n\t}\n\n\t// firstly we find data in cache\n\tif db.getHintKeyAndRAMIdxCacheSize() > 0 {\n\t\tif value := db.hintKeyAndRAMIdxModeLru.Get(record); value != nil {\n\t\t\treturn value.(*Entry).Value, nil\n\t\t}\n\t}\n\n\tdirPath := getDataPath(record.FileID, db.opt.Dir)\n\tdf, err := db.fm.getDataFile(dirPath, db.opt.SegmentSize)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func(rwManager RWManager) {\n\t\terr := rwManager.Release()\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t}(df.rwManager)\n\n\tpayloadSize := int64(len(record.Key)) + int64(record.ValueSize)\n\titem, err := df.ReadEntry(int(record.DataPos), payloadSize)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"read err. pos %d, key %s, err %s\", record.DataPos, record.Key, err)\n\t}\n\n\t// saved in cache\n\tif db.getHintKeyAndRAMIdxCacheSize() > 0 {\n\t\tdb.hintKeyAndRAMIdxModeLru.Add(record, item)\n\t}\n\n\treturn item.Value, nil\n}\n\nfunc (db *DB) commitTransaction(tx *Tx) error {\n\tvar err error\n\tdefer func() {\n\t\tvar panicked bool\n\t\tif r := recover(); r != nil {\n\t\t\t// resume normal execution\n\t\t\tpanicked = true\n\t\t}\n\t\tif panicked || err != nil {\n\t\t\t// log.Fatal(\"panicked=\", panicked, \", err=\", err)\n\t\t\tif errRollback := tx.Rollback(); errRollback != nil {\n\t\t\t\terr = errRollback\n\t\t\t}\n\t\t}\n\t}()\n\n\t// commit current tx\n\ttx.lock()\n\ttx.setStatusRunning()\n\terr = tx.Commit()\n\tif err != nil {\n\t\t// log.Fatal(\"txCommit fail,err=\", err)\n\t\treturn err\n\t}\n\n\treturn err\n}\n\nfunc (db *DB) writeRequests(reqs []*request) error {\n\tvar err error\n\tif len(reqs) == 0 {\n\t\treturn nil\n\t}\n\n\tdone := func(err error) {\n\t\tfor _, r := range reqs {\n\t\t\tr.Err = err\n\t\t\tr.Wg.Done()\n\t\t}\n\t}\n\n\tfor _, req := range reqs {\n\t\ttx := req.tx\n\t\tcerr := db.commitTransaction(tx)\n\t\tif cerr != nil {\n\t\t\terr = cerr\n\t\t}\n\t}\n\n\tdone(err)\n\treturn err\n}\n\n// MaxBatchCount returns max possible entries in batch\nfunc (db *DB) getMaxBatchCount() int64 {\n\treturn db.opt.MaxBatchCount\n}\n\n// MaxBatchSize returns max possible batch size\nfunc (db *DB) getMaxBatchSize() int64 {\n\treturn db.opt.MaxBatchSize\n}\n\nfunc (db *DB) getMaxWriteRecordCount() int64 {\n\treturn db.opt.MaxWriteRecordCount\n}\n\nfunc (db *DB) getHintKeyAndRAMIdxCacheSize() int {\n\treturn db.opt.HintKeyAndRAMIdxCacheSize\n}\n\nfunc (db *DB) doWrites() {\n\tpendingCh := make(chan struct{}, 1)\n\twriteRequests := func(reqs []*request) {\n\t\tif err := db.writeRequests(reqs); err != nil {\n\t\t\tlog.Fatal(\"writeRequests fail, err=\", err)\n\t\t}\n\t\t<-pendingCh\n\t}\n\n\treqs := make([]*request, 0, 10)\n\tvar r *request\n\tvar ok bool\n\tfor {\n\t\tr, ok = <-db.writeCh\n\t\tif !ok {\n\t\t\tgoto closedCase\n\t\t}\n\n\t\tfor {\n\t\t\treqs = append(reqs, r)\n\n\t\t\tif len(reqs) >= 3*KvWriteChCapacity {\n\t\t\t\tpendingCh <- struct{}{} // blocking.\n\t\t\t\tgoto writeCase\n\t\t\t}\n\n\t\t\tselect {\n\t\t\t// Either push to pending, or continue to pick from writeCh.\n\t\t\tcase r, ok = <-db.writeCh:\n\t\t\t\tif !ok {\n\t\t\t\t\tgoto closedCase\n\t\t\t\t}\n\t\t\tcase pendingCh <- struct{}{}:\n\t\t\t\tgoto writeCase\n\t\t\t}\n\t\t}\n\n\tclosedCase:\n\t\t// All the pending request are drained.\n\t\t// Don't close the writeCh, because it has be used in several places.\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase r = <-db.writeCh:\n\t\t\t\treqs = append(reqs, r)\n\t\t\tdefault:\n\t\t\t\tpendingCh <- struct{}{} // Push to pending before doing write.\n\t\t\t\twriteRequests(reqs)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\twriteCase:\n\t\tgo writeRequests(reqs)\n\t\treqs = make([]*request, 0, 10)\n\t}\n}\n\n// setActiveFile sets the ActiveFile (DataFile object).\nfunc (db *DB) setActiveFile() (err error) {\n\tactiveFilePath := getDataPath(db.MaxFileID, db.opt.Dir)\n\tdb.ActiveFile, err = db.fm.getDataFile(activeFilePath, db.opt.SegmentSize)\n\tif err != nil {\n\t\treturn\n\t}\n\n\tdb.ActiveFile.fileID = db.MaxFileID\n\n\treturn nil\n}\n\n// getMaxFileIDAndFileIds returns max fileId and fileIds.\nfunc (db *DB) getMaxFileIDAndFileIDs() (maxFileID int64, dataFileIds []int) {\n\tfiles, _ := os.ReadDir(db.opt.Dir)\n\n\tif len(files) == 0 {\n\t\treturn 0, nil\n\t}\n\n\tfor _, file := range files {\n\t\tfilename := file.Name()\n\t\tfileSuffix := path.Ext(path.Base(filename))\n\t\tif fileSuffix != DataSuffix {\n\t\t\tcontinue\n\t\t}\n\n\t\tfilename = strings.TrimSuffix(filename, DataSuffix)\n\t\tid, _ := strconv2.StrToInt(filename)\n\t\tdataFileIds = append(dataFileIds, id)\n\t}\n\n\tif len(dataFileIds) == 0 {\n\t\treturn 0, nil\n\t}\n\n\tsort.Ints(dataFileIds)\n\tmaxFileID = int64(dataFileIds[len(dataFileIds)-1])\n\n\treturn\n}\n\nfunc (db *DB) parseDataFiles(dataFileIds []int) (err error) {\n\tvar (\n\t\toff      int64\n\t\tf        *fileRecovery\n\t\tfID      int64\n\t\tdataInTx dataInTx\n\t)\n\n\tparseDataInTx := func() error {\n\t\tfor _, entry := range dataInTx.es {\n\t\t\t// if this bucket is not existed in bucket manager right now\n\t\t\t// its because it already deleted in the feature WAL log.\n\t\t\t// so we can just ignore here.\n\t\t\tbucketId := entry.Meta.BucketId\n\t\t\tif _, err := db.bm.GetBucketById(bucketId); errors.Is(err, ErrBucketNotExist) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\trecord := db.createRecordByModeWithFidAndOff(entry.fid, uint64(entry.off), &entry.Entry)\n\n\t\t\tif err = db.buildIdxes(record, &entry.Entry); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tdb.KeyCount++\n\n\t\t}\n\t\treturn nil\n\t}\n\n\treadEntriesFromFile := func() error {\n\t\tfor {\n\t\t\tentry, err := f.readEntry(off)\n\t\t\tif err != nil {\n\t\t\t\t// whatever which logic branch it will choose, we will release the fd.\n\t\t\t\t_ = f.release()\n\t\t\t\tif errors.Is(err, io.EOF) || errors.Is(err, ErrIndexOutOfBound) || errors.Is(err, io.ErrUnexpectedEOF) || errors.Is(err, ErrEntryZero) || errors.Is(err, ErrHeaderSizeOutOfBounds) {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tif off >= db.opt.SegmentSize {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tif entry == nil {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tentryWhenRecovery := &EntryWhenRecovery{\n\t\t\t\tEntry: *entry,\n\t\t\t\tfid:   fID,\n\t\t\t\toff:   off,\n\t\t\t}\n\t\t\tif dataInTx.txId == 0 {\n\t\t\t\tdataInTx.appendEntry(entryWhenRecovery)\n\t\t\t\tdataInTx.txId = entry.Meta.TxID\n\t\t\t\tdataInTx.startOff = off\n\t\t\t} else if dataInTx.isSameTx(entryWhenRecovery) {\n\t\t\t\tdataInTx.appendEntry(entryWhenRecovery)\n\t\t\t}\n\n\t\t\tif entry.Meta.Status == Committed {\n\t\t\t\terr := parseDataInTx()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tdataInTx.reset()\n\t\t\t\tdataInTx.startOff = off\n\t\t\t}\n\n\t\t\tif !dataInTx.isSameTx(entryWhenRecovery) {\n\t\t\t\tdataInTx.reset()\n\t\t\t\tdataInTx.startOff = off\n\t\t\t}\n\n\t\t\toff += entry.Size()\n\t\t}\n\n\t\tif fID == db.MaxFileID {\n\t\t\tdb.ActiveFile.ActualSize = off\n\t\t\tdb.ActiveFile.writeOff = off\n\t\t}\n\n\t\treturn nil\n\t}\n\n\tfor _, dataID := range dataFileIds {\n\t\toff = 0\n\t\tfID = int64(dataID)\n\t\tdataPath := getDataPath(fID, db.opt.Dir)\n\t\tf, err = newFileRecovery(dataPath, db.opt.BufferSizeOfRecovery)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\terr := readEntriesFromFile()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// compute the valid record count and save it in db.RecordCount\n\tdb.RecordCount, err = db.getRecordCount()\n\treturn\n}\n\nfunc (db *DB) getRecordCount() (int64, error) {\n\tvar res int64\n\n\t// Iterate through the BTree indices\n\tfor _, btree := range db.Index.bTree.idx {\n\t\tres += int64(btree.Count())\n\t}\n\n\t// Iterate through the List indices\n\tfor _, listItem := range db.Index.list.idx {\n\t\tfor key := range listItem.Items {\n\t\t\tcurLen, err := listItem.Size(key)\n\t\t\tif err != nil {\n\t\t\t\treturn res, err\n\t\t\t}\n\t\t\tres += int64(curLen)\n\t\t}\n\t}\n\n\t// Iterate through the Set indices\n\tfor _, setItem := range db.Index.set.idx {\n\t\tfor key := range setItem.M {\n\t\t\tres += int64(setItem.SCard(key))\n\t\t}\n\t}\n\n\t// Iterate through the SortedSet indices\n\tfor _, zsetItem := range db.Index.sortedSet.idx {\n\t\tfor key := range zsetItem.M {\n\t\t\tcurLen, err := zsetItem.ZCard(key)\n\t\t\tif err != nil {\n\t\t\t\treturn res, err\n\t\t\t}\n\t\t\tres += int64(curLen)\n\t\t}\n\t}\n\n\treturn res, nil\n}\n\nfunc (db *DB) buildBTreeIdx(record *Record, entry *Entry) error {\n\tkey, meta := entry.Key, entry.Meta\n\n\tbucket, err := db.bm.GetBucketById(meta.BucketId)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbucketId := bucket.Id\n\n\tbTree := db.Index.bTree.getWithDefault(bucketId)\n\n\tif record.IsExpired() || meta.Flag == DataDeleteFlag {\n\t\tdb.tm.del(bucketId, string(key))\n\t\tbTree.Delete(key)\n\t} else {\n\t\tif meta.TTL != Persistent {\n\t\t\tdb.tm.add(bucketId, string(key), db.expireTime(meta.Timestamp, meta.TTL), db.buildExpireCallback(bucket.Name, key))\n\t\t} else {\n\t\t\tdb.tm.del(bucketId, string(key))\n\t\t}\n\t\tbTree.Insert(record)\n\t}\n\treturn nil\n}\n\nfunc (db *DB) expireTime(timestamp uint64, ttl uint32) time.Duration {\n\tnow := time.UnixMilli(time.Now().UnixMilli())\n\texpireTime := time.UnixMilli(int64(timestamp))\n\texpireTime = expireTime.Add(time.Duration(int64(ttl)) * time.Second)\n\treturn expireTime.Sub(now)\n}\n\nfunc (db *DB) buildIdxes(record *Record, entry *Entry) error {\n\tmeta := entry.Meta\n\tswitch meta.Ds {\n\tcase DataStructureBTree:\n\t\treturn db.buildBTreeIdx(record, entry)\n\tcase DataStructureList:\n\t\tif err := db.buildListIdx(record, entry); err != nil {\n\t\t\treturn err\n\t\t}\n\tcase DataStructureSet:\n\t\tif err := db.buildSetIdx(record, entry); err != nil {\n\t\t\treturn err\n\t\t}\n\tcase DataStructureSortedSet:\n\t\tif err := db.buildSortedSetIdx(record, entry); err != nil {\n\t\t\treturn err\n\t\t}\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"there is an unexpected data structure that is unimplemented in our database.:%d\", meta.Ds))\n\t}\n\treturn nil\n}\n\nfunc (db *DB) deleteBucket(ds uint16, bucket BucketId) {\n\tif ds == DataStructureSet {\n\t\tdb.Index.set.delete(bucket)\n\t}\n\tif ds == DataStructureSortedSet {\n\t\tdb.Index.sortedSet.delete(bucket)\n\t}\n\tif ds == DataStructureBTree {\n\t\tdb.Index.bTree.delete(bucket)\n\t}\n\tif ds == DataStructureList {\n\t\tdb.Index.list.delete(bucket)\n\t}\n}\n\n// buildSetIdx builds set index when opening the DB.\nfunc (db *DB) buildSetIdx(record *Record, entry *Entry) error {\n\tkey, val, meta := entry.Key, entry.Value, entry.Meta\n\n\tbucket, err := db.bm.GetBucketById(entry.Meta.BucketId)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbucketId := bucket.Id\n\n\ts := db.Index.set.getWithDefault(bucketId)\n\n\tswitch meta.Flag {\n\tcase DataSetFlag:\n\t\tif err := s.SAdd(string(key), [][]byte{val}, []*Record{record}); err != nil {\n\t\t\treturn fmt.Errorf(\"when build SetIdx SAdd index err: %s\", err)\n\t\t}\n\tcase DataDeleteFlag:\n\t\tif err := s.SRem(string(key), val); err != nil {\n\t\t\treturn fmt.Errorf(\"when build SetIdx SRem index err: %s\", err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// buildSortedSetIdx builds sorted set index when opening the DB.\nfunc (db *DB) buildSortedSetIdx(record *Record, entry *Entry) error {\n\tkey, val, meta := entry.Key, entry.Value, entry.Meta\n\n\tbucket, err := db.bm.GetBucketById(entry.Meta.BucketId)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbucketId := bucket.Id\n\n\tss := db.Index.sortedSet.getWithDefault(bucketId, db)\n\n\tswitch meta.Flag {\n\tcase DataZAddFlag:\n\t\tkeyAndScore := strings.Split(string(key), SeparatorForZSetKey)\n\t\tif len(keyAndScore) == 2 {\n\t\t\tkey := keyAndScore[0]\n\t\t\tscore, _ := strconv2.StrToFloat64(keyAndScore[1])\n\t\t\terr = ss.ZAdd(key, SCORE(score), val, record)\n\t\t}\n\tcase DataZRemFlag:\n\t\t_, err = ss.ZRem(string(key), val)\n\tcase DataZRemRangeByRankFlag:\n\t\tstart, end := splitIntIntStr(string(val), SeparatorForZSetKey)\n\t\terr = ss.ZRemRangeByRank(string(key), start, end)\n\tcase DataZPopMaxFlag:\n\t\t_, _, err = ss.ZPopMax(string(key))\n\tcase DataZPopMinFlag:\n\t\t_, _, err = ss.ZPopMin(string(key))\n\t}\n\n\t// We don't need to panic if sorted set is not found.\n\tif err != nil && !errors.Is(err, ErrSortedSetNotFound) {\n\t\treturn fmt.Errorf(\"when build sortedSetIdx err: %s\", err)\n\t}\n\n\treturn nil\n}\n\n// buildListIdx builds List index when opening the DB.\nfunc (db *DB) buildListIdx(record *Record, entry *Entry) error {\n\tkey, val, meta := entry.Key, entry.Value, entry.Meta\n\n\tbucket, err := db.bm.GetBucketById(entry.Meta.BucketId)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbucketId := bucket.Id\n\n\tl := db.Index.list.getWithDefault(bucketId)\n\n\tif IsExpired(meta.TTL, meta.Timestamp) {\n\t\treturn nil\n\t}\n\n\tswitch meta.Flag {\n\tcase DataExpireListFlag:\n\t\tt, _ := strconv2.StrToInt64(string(val))\n\t\tttl := uint32(t)\n\t\tl.TTL[string(key)] = ttl\n\t\tl.TimeStamp[string(key)] = meta.Timestamp\n\tcase DataLPushFlag:\n\t\terr = l.LPush(string(key), record)\n\tcase DataRPushFlag:\n\t\terr = l.RPush(string(key), record)\n\tcase DataLRemFlag:\n\t\terr = db.buildListLRemIdx(val, l, key)\n\tcase DataLPopFlag:\n\t\t_, err = l.LPop(string(key))\n\tcase DataRPopFlag:\n\t\t_, err = l.RPop(string(key))\n\tcase DataLTrimFlag:\n\t\tnewKey, start := splitStringIntStr(string(key), SeparatorForListKey)\n\t\tend, _ := strconv2.StrToInt(string(val))\n\t\terr = l.LTrim(newKey, start, end)\n\tcase DataLRemByIndex:\n\t\tindexes, _ := UnmarshalInts(val)\n\t\terr = l.LRemByIndex(string(key), indexes)\n\t}\n\n\tif err != nil {\n\t\treturn fmt.Errorf(\"when build listIdx err: %s\", err)\n\t}\n\n\treturn nil\n}\n\nfunc (db *DB) buildListLRemIdx(value []byte, l *List, key []byte) error {\n\tcount, newValue := splitIntStringStr(string(value), SeparatorForListKey)\n\n\treturn l.LRem(string(key), count, func(r *Record) (bool, error) {\n\t\tv, err := db.getValueByRecord(r)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\t\treturn bytes.Equal([]byte(newValue), v), nil\n\t})\n}\n\n// buildIndexes builds indexes when db initialize resource.\nfunc (db *DB) buildIndexes() (err error) {\n\tvar (\n\t\tmaxFileID   int64\n\t\tdataFileIds []int\n\t)\n\n\tmaxFileID, dataFileIds = db.getMaxFileIDAndFileIDs()\n\n\t// init db.ActiveFile\n\tdb.MaxFileID = maxFileID\n\n\t// set ActiveFile\n\tif err = db.setActiveFile(); err != nil {\n\t\treturn\n\t}\n\n\tif dataFileIds == nil && maxFileID == 0 {\n\t\treturn\n\t}\n\n\t// build hint index\n\treturn db.parseDataFiles(dataFileIds)\n}\n\nfunc (db *DB) createRecordByModeWithFidAndOff(fid int64, off uint64, entry *Entry) *Record {\n\trecord := NewRecord()\n\n\trecord.WithKey(entry.Key).\n\t\tWithTimestamp(entry.Meta.Timestamp).\n\t\tWithTTL(entry.Meta.TTL).\n\t\tWithTxID(entry.Meta.TxID)\n\n\tif db.opt.EntryIdxMode == HintKeyValAndRAMIdxMode {\n\t\trecord.WithValue(entry.Value)\n\t}\n\n\tif db.opt.EntryIdxMode == HintKeyAndRAMIdxMode {\n\t\trecord.WithFileId(fid).\n\t\t\tWithDataPos(off).\n\t\t\tWithValueSize(uint32(len(entry.Value)))\n\t}\n\n\treturn record\n}\n\n// managed calls a block of code that is fully contained in a transaction.\nfunc (db *DB) managed(writable bool, fn func(tx *Tx) error) (err error) {\n\tvar tx *Tx\n\n\ttx, err = db.Begin(writable)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr = fmt.Errorf(\"panic when executing tx, err is %+v\", r)\n\t\t}\n\t}()\n\n\tif err = fn(tx); err == nil {\n\t\terr = tx.Commit()\n\t} else {\n\t\tif db.opt.ErrorHandler != nil {\n\t\t\tdb.opt.ErrorHandler.HandleError(err)\n\t\t}\n\n\t\tif errRollback := tx.Rollback(); errRollback != nil {\n\t\t\terr = fmt.Errorf(\"%v. Rollback err: %v\", err, errRollback)\n\t\t}\n\t}\n\n\treturn err\n}\n\nfunc (db *DB) sendToWriteCh(tx *Tx) (*request, error) {\n\treq := requestPool.Get().(*request)\n\treq.reset()\n\treq.Wg.Add(1)\n\treq.tx = tx\n\treq.IncrRef()     // for db write\n\tdb.writeCh <- req // Handled in doWrites.\n\treturn req, nil\n}\n\nfunc (db *DB) checkListExpired() {\n\tdb.Index.list.rangeIdx(func(l *List) {\n\t\tfor key := range l.TTL {\n\t\t\tl.IsExpire(key)\n\t\t}\n\t})\n}\n\n// IsClose return the value that represents the status of DB\nfunc (db *DB) IsClose() bool {\n\treturn db.closed\n}\n\nfunc (db *DB) buildExpireCallback(bucket string, key []byte) func() {\n\treturn func() {\n\t\terr := db.Update(func(tx *Tx) error {\n\t\t\tb, err := tx.db.bm.GetBucket(DataStructureBTree, bucket)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tbucketId := b.Id\n\t\t\tif db.tm.exist(bucketId, string(key)) {\n\t\t\t\treturn tx.Delete(bucket, key)\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t\tif err != nil {\n\t\t\tlog.Printf(\"occur error when expired deletion, error: %v\", err.Error())\n\t\t}\n\t}\n}\n\nfunc (db *DB) rebuildBucketManager() error {\n\tbucketFilePath := db.opt.Dir + \"/\" + BucketStoreFileName\n\tf, err := newFileRecovery(bucketFilePath, db.opt.BufferSizeOfRecovery)\n\tif err != nil {\n\t\treturn nil\n\t}\n\tbucketRequest := make([]*bucketSubmitRequest, 0)\n\n\tfor {\n\t\tbucket, err := f.readBucket()\n\t\tif err != nil {\n\t\t\t// whatever which logic branch it will choose, we will release the fd.\n\t\t\t_ = f.release()\n\t\t\tif errors.Is(err, io.EOF) || errors.Is(err, io.ErrUnexpectedEOF) {\n\t\t\t\tbreak\n\t\t\t} else {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tbucketRequest = append(bucketRequest, &bucketSubmitRequest{\n\t\t\tds:     bucket.Ds,\n\t\t\tname:   BucketName(bucket.Name),\n\t\t\tbucket: bucket,\n\t\t})\n\t}\n\n\tif len(bucketRequest) > 0 {\n\t\terr = db.bm.SubmitPendingBucketChange(bucketRequest)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "db_error.go",
          "type": "blob",
          "size": 1.19921875,
          "content": "package nutsdb\n\nimport \"errors\"\n\nvar (\n\t// ErrDBClosed is returned when db is closed.\n\tErrDBClosed = errors.New(\"db is closed\")\n\n\t// ErrBucket is returned when bucket is not in the HintIdx.\n\tErrBucket = errors.New(\"err bucket\")\n\n\t// ErrFn is returned when fn is nil.\n\tErrFn = errors.New(\"err fn\")\n\n\t// ErrBucketNotFound is returned when looking for bucket that does not exist\n\tErrBucketNotFound = errors.New(\"bucket not found\")\n\n\t// ErrDataStructureNotSupported is returned when pass a not supported data structure\n\tErrDataStructureNotSupported = errors.New(\"this data structure is not supported for now\")\n\n\t// ErrDirLocked is returned when can't get the file lock of dir\n\tErrDirLocked = errors.New(\"the dir of db is locked\")\n\n\t// ErrDirUnlocked is returned when the file lock already unlocked\n\tErrDirUnlocked = errors.New(\"the dir of db is unlocked\")\n\n\t// ErrIsMerging is returned when merge in progress\n\tErrIsMerging = errors.New(\"merge in progress\")\n\n\t// ErrNotSupportMergeWhenUsingList is returned calling 'Merge' when using list\n\tErrNotSupportMergeWhenUsingList = errors.New(\"not support merge when using list for now\")\n\n\t// ErrRecordIsNil is returned when Record is nil\n\tErrRecordIsNil = errors.New(\"the record is nil\")\n)\n"
        },
        {
          "name": "db_test.go",
          "type": "blob",
          "size": 47.2490234375,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"math\"\n\t\"os\"\n\t\"strconv\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nvar (\n\tdb  *DB\n\topt Options\n\terr error\n)\n\nconst NutsDBTestDirPath = \"/tmp/nutsdb-test\"\n\nfunc assertErr(t *testing.T, err error, expectErr error) {\n\tif expectErr != nil {\n\t\trequire.Equal(t, expectErr, err)\n\t} else {\n\t\trequire.NoError(t, err)\n\t}\n}\n\nfunc removeDir(dir string) {\n\tif err := os.RemoveAll(dir); err != nil {\n\t\tpanic(err)\n\t}\n}\n\nfunc runNutsDBTest(t *testing.T, opts *Options, test func(t *testing.T, db *DB)) {\n\tif opts == nil {\n\t\topts = &DefaultOptions\n\t}\n\tif opts.Dir == \"\" {\n\t\topts.Dir = NutsDBTestDirPath\n\t}\n\tdefer removeDir(opts.Dir)\n\tdb, err := Open(*opts)\n\trequire.NoError(t, err)\n\n\ttest(t, db)\n\tt.Cleanup(func() {\n\t\tif !db.IsClose() {\n\t\t\trequire.NoError(t, db.Close())\n\t\t}\n\t})\n}\n\nfunc txPut(t *testing.T, db *DB, bucket string, key, value []byte, ttl uint32, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr = tx.Put(bucket, key, value, ttl)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txGet(t *testing.T, db *DB, bucket string, key []byte, expectVal []byte, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tvalue, err := tx.Get(bucket, key)\n\t\tif expectErr != nil {\n\t\t\trequire.Equal(t, expectErr, err)\n\t\t} else {\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.EqualValuesf(t, expectVal, value, \"err Tx Get. got %s want %s\", string(value), string(expectVal))\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txGetAll(t *testing.T, db *DB, bucket string, expectKeys [][]byte, expectValues [][]byte, expectErr error) {\n\trequire.NoError(t, db.View(func(tx *Tx) error {\n\t\tkeys, values, err := tx.GetAll(bucket)\n\t\tif expectErr != nil {\n\t\t\trequire.Equal(t, expectErr, err)\n\t\t} else {\n\t\t\trequire.NoError(t, err)\n\t\t\tn := len(keys)\n\t\t\tfor i := 0; i < n; i++ {\n\t\t\t\trequire.Equal(t, expectKeys[i], keys[i])\n\t\t\t\trequire.Equal(t, expectValues[i], values[i])\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}))\n}\n\nfunc txDel(t *testing.T, db *DB, bucket string, key []byte, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.Delete(bucket, key)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txGetMaxOrMinKey(t *testing.T, db *DB, bucket string, isMax bool, expectVal []byte, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tvalue, err := tx.getMaxOrMinKey(bucket, isMax)\n\t\tif expectErr != nil {\n\t\t\trequire.Equal(t, expectErr, err)\n\t\t} else {\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.EqualValuesf(t, expectVal, value, \"err Tx Get. got %s want %s\", string(value), string(expectVal))\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txDeleteBucket(t *testing.T, db *DB, ds uint16, bucket string, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.DeleteBucket(ds, bucket)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txCreateBucket(t *testing.T, db *DB, ds uint16, bucket string, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.NewBucket(ds, bucket)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc InitOpt(fileDir string, isRemoveFiles bool) {\n\tif fileDir == \"\" {\n\t\tfileDir = \"/tmp/nutsdbtest\"\n\t}\n\tif isRemoveFiles {\n\t\tfiles, _ := ioutil.ReadDir(fileDir)\n\t\tfor _, f := range files {\n\t\t\tname := f.Name()\n\t\t\tif name != \"\" {\n\t\t\t\terr := os.RemoveAll(fileDir + \"/\" + name)\n\t\t\t\tif err != nil {\n\t\t\t\t\tpanic(err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\topt = DefaultOptions\n\topt.Dir = fileDir\n\topt.SegmentSize = 8 * 1024\n\topt.CleanFdsCacheThreshold = 0.5\n\topt.MaxFdNumsInCache = 1024\n}\n\nfunc TestDB_Basic(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tbucket := \"bucket\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\tkey0 := GetTestBytes(0)\n\t\tval0 := GetRandomBytes(24)\n\n\t\t// put\n\t\ttxPut(t, db, bucket, key0, val0, Persistent, nil, nil)\n\t\ttxGet(t, db, bucket, key0, val0, nil)\n\n\t\tval1 := GetRandomBytes(24)\n\n\t\t// update\n\t\ttxPut(t, db, bucket, key0, val1, Persistent, nil, nil)\n\t\ttxGet(t, db, bucket, key0, val1, nil)\n\n\t\t// del\n\t\ttxDel(t, db, bucket, key0, nil)\n\t\ttxGet(t, db, bucket, key0, val1, ErrKeyNotFound)\n\t})\n}\n\nfunc TestDB_ReopenWithDelete(t *testing.T) {\n\tvar opts *Options\n\tif opts == nil {\n\t\topts = &DefaultOptions\n\t}\n\tif opts.Dir == \"\" {\n\t\topts.Dir = NutsDBTestDirPath\n\t}\n\tdb, err := Open(*opts)\n\trequire.NoError(t, err)\n\tdefer removeDir(opts.Dir)\n\n\tbucket := \"bucket\"\n\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\ttxPush(t, db, bucket, GetTestBytes(5), GetTestBytes(0), true, nil, nil)\n\ttxPush(t, db, bucket, GetTestBytes(5), GetTestBytes(1), true, nil, nil)\n\ttxDeleteBucket(t, db, DataStructureList, bucket, nil)\n\n\tif !db.IsClose() {\n\t\trequire.NoError(t, db.Close())\n\t}\n\n\tdb, err = Open(*opts)\n\trequire.NoError(t, err)\n\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\ttxDeleteBucket(t, db, DataStructureList, bucket, nil)\n\tif !db.IsClose() {\n\t\trequire.NoError(t, db.Close())\n\t}\n}\n\nfunc TestDB_Flock(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tdb2, err := Open(db.opt)\n\t\trequire.Nil(t, db2)\n\t\trequire.Equal(t, ErrDirLocked, err)\n\n\t\terr = db.Close()\n\t\trequire.NoError(t, err)\n\n\t\tdb2, err = Open(db.opt)\n\t\trequire.NoError(t, err)\n\t\trequire.NotNil(t, db2)\n\n\t\terr = db2.flock.Unlock()\n\t\trequire.NoError(t, err)\n\t\trequire.False(t, db2.flock.Locked())\n\n\t\terr = db2.Close()\n\t\trequire.Error(t, err)\n\t\trequire.Equal(t, ErrDirUnlocked, err)\n\t})\n}\n\nfunc TestDB_DeleteANonExistKey(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttestBucket := \"test_bucket\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, testBucket, nil)\n\n\t\ttxDel(t, db, testBucket, GetTestBytes(0), ErrKeyNotFound)\n\t\ttxPut(t, db, testBucket, GetTestBytes(1), GetRandomBytes(24), Persistent, nil, nil)\n\t\ttxDel(t, db, testBucket, GetTestBytes(0), ErrKeyNotFound)\n\t})\n}\n\nfunc TestDB_CheckListExpired(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttestBucket := \"test_bucket\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, testBucket, nil)\n\n\t\ttxPut(t, db, testBucket, GetTestBytes(0), GetTestBytes(1), Persistent, nil, nil)\n\t\ttxPut(t, db, testBucket, GetTestBytes(1), GetRandomBytes(24), 1, nil, nil)\n\n\t\ttime.Sleep(1100 * time.Millisecond)\n\n\t\tdb.checkListExpired()\n\n\t\t// this entry still alive\n\t\ttxGet(t, db, testBucket, GetTestBytes(0), GetTestBytes(1), nil)\n\t\t// this entry will be deleted\n\t\ttxGet(t, db, testBucket, GetTestBytes(1), nil, ErrKeyNotFound)\n\t})\n}\n\nfunc txLRem(t *testing.T, db *DB, bucket string, key []byte, count int, value []byte, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.LRem(bucket, key, count, value)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txLRemByIndex(t *testing.T, db *DB, bucket string, key []byte, expectErr error, indexes ...int) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.LRemByIndex(bucket, key, indexes...)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSAdd(t *testing.T, db *DB, bucket string, key, value []byte, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.SAdd(bucket, key, value)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txSKeys(t *testing.T, db *DB, bucket, pattern string, f func(key string) bool, expectVal int, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tpatternMatchNum := 0\n\t\terr := tx.SKeys(bucket, pattern, func(key string) bool {\n\t\t\tpatternMatchNum += 1\n\t\t\treturn f(key)\n\t\t})\n\t\tif expectErr != nil {\n\t\t\tassert.ErrorIs(t, expectErr, err)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.Equal(t, expectVal, patternMatchNum)\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSIsMember(t *testing.T, db *DB, bucket string, key, value []byte, expect bool) {\n\terr := db.View(func(tx *Tx) error {\n\t\tok, _ := tx.SIsMember(bucket, key, value)\n\t\trequire.Equal(t, expect, ok)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSAreMembers(t *testing.T, db *DB, bucket string, key []byte, expect bool, value ...[]byte) {\n\terr := db.View(func(tx *Tx) error {\n\t\tok, _ := tx.SAreMembers(bucket, key, value...)\n\t\trequire.Equal(t, expect, ok)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSHasKey(t *testing.T, db *DB, bucket string, key []byte, expect bool) {\n\terr := db.View(func(tx *Tx) error {\n\t\tok, _ := tx.SHasKey(bucket, key)\n\t\trequire.Equal(t, expect, ok)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSMembers(t *testing.T, db *DB, bucket string, key []byte, expectLength int, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tmembers, err := tx.SMembers(bucket, key)\n\t\tif expectErr != nil {\n\t\t\tassert.ErrorIs(t, expectErr, err)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.Equal(t, expectLength, len(members))\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSCard(t *testing.T, db *DB, bucket string, key []byte, expectLength int, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tlength, err := tx.SCard(bucket, key)\n\t\tif expectErr != nil {\n\t\t\tassert.ErrorIs(t, expectErr, err)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.Equal(t, expectLength, length)\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSDiffByOneBucket(t *testing.T, db *DB, bucket string, key1, key2 []byte, expectVal [][]byte, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tdiff, err := tx.SDiffByOneBucket(bucket, key1, key2)\n\t\tif expectErr != nil {\n\t\t\tassert.ErrorIs(t, expectErr, err)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.ElementsMatch(t, expectVal, diff)\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSDiffByTwoBucket(t *testing.T, db *DB, bucket1 string, key1 []byte, bucket2 string, key2 []byte, expectVal [][]byte, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tdiff, err := tx.SDiffByTwoBuckets(bucket1, key1, bucket2, key2)\n\t\tif expectErr != nil {\n\t\t\tassert.ErrorIs(t, err, expectErr)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.ElementsMatch(t, expectVal, diff)\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSPop(t *testing.T, db *DB, bucket string, key []byte, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\t_, err := tx.SPop(bucket, key)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSMoveByOneBucket(t *testing.T, db *DB, bucket1 string, key1, key2, val []byte, expectVal bool, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tok, err := tx.SMoveByOneBucket(bucket1, key1, key2, val)\n\t\tif expectErr != nil {\n\t\t\tassert.ErrorIs(t, err, expectErr)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.Equal(t, expectVal, ok)\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSMoveByTwoBuckets(t *testing.T, db *DB, bucket1 string, key1 []byte, bucket2 string, key2 []byte, val []byte, expectVal bool, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tok, err := tx.SMoveByTwoBuckets(bucket1, key1, bucket2, key2, val)\n\t\tif expectErr != nil {\n\t\t\tassert.ErrorIs(t, err, expectErr)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.Equal(t, expectVal, ok)\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSUnionByOneBucket(t *testing.T, db *DB, bucket1 string, key1, key2 []byte, expectVal [][]byte, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tunion, err := tx.SUnionByOneBucket(bucket1, key1, key2)\n\t\tif expectErr != nil {\n\t\t\tassert.ErrorIs(t, err, expectErr)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.ElementsMatch(t, expectVal, union)\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSUnionByTwoBuckets(t *testing.T, db *DB, bucket1 string, key1 []byte, bucket2 string, key2 []byte, expectVal [][]byte, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tunion, err := tx.SUnionByTwoBuckets(bucket1, key1, bucket2, key2)\n\t\tif expectErr != nil {\n\t\t\tassert.ErrorIs(t, err, expectErr)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.ElementsMatch(t, expectVal, union)\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txSRem(t *testing.T, db *DB, bucket string, key, value []byte, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.SRem(bucket, key, value)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txZAdd(t *testing.T, db *DB, bucket string, key, value []byte, score float64, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.ZAdd(bucket, key, score, value)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txZRem(t *testing.T, db *DB, bucket string, key, value []byte, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.ZRem(bucket, key, value)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassert.NoError(t, err)\n}\n\nfunc txZCard(t *testing.T, db *DB, bucket string, key []byte, expectLength int, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tlength, err := tx.ZCard(bucket, key)\n\t\tif expectErr != nil {\n\t\t\tassert.Equal(t, expectErr, err)\n\t\t} else {\n\t\t\tassert.Equal(t, expectLength, length)\n\t\t}\n\t\treturn nil\n\t})\n\tassert.NoError(t, err)\n}\n\nfunc txZScore(t *testing.T, db *DB, bucket string, key, value []byte, expectScore float64, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tscore, err := tx.ZScore(bucket, key, value)\n\t\tif err != nil {\n\t\t\tassert.Equal(t, expectErr, err)\n\t\t} else {\n\t\t\tassert.Equal(t, expectScore, score)\n\t\t}\n\t\treturn nil\n\t})\n\tassert.NoError(t, err)\n}\n\nfunc txZRank(t *testing.T, db *DB, bucket string, key, value []byte, isRev bool, expectRank int, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tvar (\n\t\t\trank int\n\t\t\terr  error\n\t\t)\n\t\tif isRev {\n\t\t\trank, err = tx.ZRevRank(bucket, key, value)\n\t\t} else {\n\t\t\trank, err = tx.ZRank(bucket, key, value)\n\t\t}\n\t\tif expectErr != nil {\n\t\t\tassert.Equal(t, expectErr, err)\n\t\t} else {\n\t\t\tassert.Equal(t, expectRank, rank)\n\t\t}\n\t\treturn nil\n\t})\n\tassert.NoError(t, err)\n}\n\nfunc txZPop(t *testing.T, db *DB, bucket string, key []byte, isMax bool, expectVal []byte, expectScore float64, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\tvar (\n\t\t\tmember *SortedSetMember\n\t\t\terr    error\n\t\t)\n\t\tif isMax {\n\t\t\tmember, err = tx.ZPopMax(bucket, key)\n\t\t} else {\n\t\t\tmember, err = tx.ZPopMin(bucket, key)\n\t\t}\n\n\t\tif expectErr != nil {\n\t\t\tassert.Equal(t, expectErr, err)\n\t\t} else {\n\t\t\tassert.Equal(t, expectVal, member.Value)\n\t\t\tassert.Equal(t, expectScore, member.Score)\n\t\t}\n\t\treturn nil\n\t})\n\tassert.NoError(t, err)\n}\n\nfunc txZPeekMin(t *testing.T, db *DB, bucket string, key, expectVal []byte, expectScore float64, expectErr, finalExpectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tminMem, err1 := tx.ZPeekMin(bucket, key)\n\t\tassertErr(t, err1, finalExpectErr)\n\n\t\tif expectErr == nil {\n\t\t\trequire.Equal(t, &SortedSetMember{\n\t\t\t\tValue: expectVal,\n\t\t\t\tScore: expectScore,\n\t\t\t}, minMem)\n\t\t}\n\t\treturn err1\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txZKeys(t *testing.T, db *DB, bucket, pattern string, f func(key string) bool, expectVal int, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tpatternMatchNum := 0\n\t\terr := tx.ZKeys(bucket, pattern, func(key string) bool {\n\t\t\tpatternMatchNum += 1\n\t\t\treturn f(key)\n\t\t})\n\t\tif expectErr != nil {\n\t\t\tassert.ErrorIs(t, expectErr, err)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.Equal(t, expectVal, patternMatchNum)\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txPop(t *testing.T, db *DB, bucket string, key, expectVal []byte, expectErr error, isLeft bool) {\n\terr := db.Update(func(tx *Tx) error {\n\t\tvar item []byte\n\t\tvar err error\n\n\t\tif isLeft {\n\t\t\titem, err = tx.LPop(bucket, key)\n\t\t} else {\n\t\t\titem, err = tx.RPop(bucket, key)\n\t\t}\n\n\t\tif expectErr != nil {\n\t\t\trequire.Equal(t, expectErr, err)\n\t\t} else {\n\t\t\trequire.Equal(t, expectVal, item)\n\t\t}\n\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txPush(t *testing.T, db *DB, bucket string, key, val []byte, isLeft bool, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\tvar err error\n\n\t\tif isLeft {\n\t\t\terr = tx.LPush(bucket, key, val)\n\t\t} else {\n\t\t\terr = tx.RPush(bucket, key, val)\n\t\t}\n\n\t\tassertErr(t, err, expectErr)\n\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txMPush(t *testing.T, db *DB, bucket string, key []byte, vals [][]byte, isLeft bool, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\tvar err error\n\n\t\tif isLeft {\n\t\t\terr = tx.LPush(bucket, key, vals...)\n\t\t} else {\n\t\t\terr = tx.RPush(bucket, key, vals...)\n\t\t}\n\n\t\tassertErr(t, err, expectErr)\n\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txPushRaw(t *testing.T, db *DB, bucket string, key, val []byte, isLeft bool, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\tvar err error\n\n\t\tif isLeft {\n\t\t\terr = tx.LPushRaw(bucket, key, val)\n\t\t} else {\n\t\t\terr = tx.RPushRaw(bucket, key, val)\n\t\t}\n\n\t\tassertErr(t, err, expectErr)\n\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txExpireList(t *testing.T, db *DB, bucket string, key []byte, ttl uint32, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.ExpireList(bucket, key, ttl)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txGetListTTL(t *testing.T, db *DB, bucket string, key []byte, expectVal uint32, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tttl, err := tx.GetListTTL(bucket, key)\n\t\tassertErr(t, err, expectErr)\n\t\trequire.Equal(t, ttl, expectVal)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txLKeys(t *testing.T, db *DB, bucket, pattern string, expectLen int, expectErr error, keysOperation func(keys []string) bool) {\n\terr := db.View(func(tx *Tx) error {\n\t\tvar keys []string\n\t\terr := tx.LKeys(bucket, pattern, func(key string) bool {\n\t\t\tkeys = append(keys, key)\n\t\t\treturn keysOperation(keys)\n\t\t})\n\t\tassertErr(t, err, expectErr)\n\t\trequire.Equal(t, expectLen, len(keys))\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txLRange(t *testing.T, db *DB, bucket string, key []byte, start, end, expectLen int, expectVal [][]byte, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tlist, err := tx.LRange(bucket, key, start, end)\n\t\tassertErr(t, err, expectErr)\n\n\t\trequire.Equal(t, expectLen, len(list))\n\n\t\tif len(expectVal) > 0 {\n\t\t\tfor i, val := range list {\n\t\t\t\tassert.Equal(t, expectVal[i], val)\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txLSize(t *testing.T, db *DB, bucket string, key []byte, expectVal int, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tsize, err := tx.LSize(bucket, key)\n\t\tassertErr(t, err, expectErr)\n\n\t\trequire.Equal(t, expectVal, size)\n\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txLTrim(t *testing.T, db *DB, bucket string, key []byte, start int, end int, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.LTrim(bucket, key, start, end)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txIterateBuckets(t *testing.T, db *DB, ds uint16, pattern string, f func(key string) bool, expectErr error, containsKey ...string) {\n\terr := db.View(func(tx *Tx) error {\n\t\tvar elements []string\n\t\terr := tx.IterateBuckets(ds, pattern, func(key string) bool {\n\t\t\tif f != nil && !f(key) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\telements = append(elements, key)\n\t\t\treturn true\n\t\t})\n\t\tif err != nil {\n\t\t\tassert.Equal(t, expectErr, err)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tfor _, key := range containsKey {\n\t\t\t\tassert.Contains(t, elements, key)\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc TestDB_GetKeyNotFound(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tbucket := \"bucket\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\ttxGet(t, db, bucket, GetTestBytes(0), nil, ErrKeyNotFound)\n\t\ttxPut(t, db, bucket, GetTestBytes(1), GetRandomBytes(24), Persistent, nil, nil)\n\t\ttxGet(t, db, bucket, GetTestBytes(0), nil, ErrKeyNotFound)\n\t})\n}\n\nfunc TestDB_Backup(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tbackUpDir := \"/tmp/nutsdb-backup\"\n\t\trequire.NoError(t, db.Backup(backUpDir))\n\t})\n}\n\nfunc TestDB_BackupTarGZ(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tbackUpFile := \"/tmp/nutsdb-backup/backup.tar.gz\"\n\t\tf, err := os.Create(backUpFile)\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, db.BackupTarGZ(f))\n\t})\n}\n\nfunc TestDB_Close(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\trequire.NoError(t, db.Close())\n\t\trequire.Equal(t, ErrDBClosed, db.Close())\n\t})\n}\n\nfunc TestDB_ErrThenReadWrite(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tbucket := \"testForDeadLock\"\n\t\terr = db.View(\n\t\t\tfunc(tx *Tx) error {\n\t\t\t\treturn fmt.Errorf(\"err happened\")\n\t\t\t})\n\t\trequire.NotNil(t, err)\n\n\t\terr = db.View(\n\t\t\tfunc(tx *Tx) error {\n\t\t\t\tkey := []byte(\"key1\")\n\t\t\t\t_, err := tx.Get(bucket, key)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\treturn nil\n\t\t\t})\n\t\trequire.NotNil(t, err)\n\n\t\tnotice := make(chan struct{})\n\t\tgo func() {\n\t\t\terr = db.Update(\n\t\t\t\tfunc(tx *Tx) error {\n\t\t\t\t\tnotice <- struct{}{}\n\n\t\t\t\t\treturn nil\n\t\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t}()\n\n\t\tselect {\n\t\tcase <-notice:\n\t\tcase <-time.After(1 * time.Second):\n\t\t\tt.Fatalf(\"exist deadlock\")\n\t\t}\n\t})\n}\n\nfunc TestDB_ErrorHandler(t *testing.T) {\n\topts := DefaultOptions\n\thandleErrCalled := false\n\topts.ErrorHandler = ErrorHandlerFunc(func(err error) {\n\t\thandleErrCalled = true\n\t})\n\n\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\terr = db.View(\n\t\t\tfunc(tx *Tx) error {\n\t\t\t\treturn fmt.Errorf(\"err happened\")\n\t\t\t})\n\t\trequire.NotNil(t, err)\n\t\trequire.Equal(t, handleErrCalled, true)\n\t})\n}\n\nfunc TestDB_CommitBuffer(t *testing.T) {\n\tbucket := \"bucket\"\n\n\topts := DefaultOptions\n\topts.CommitBufferSize = 8 * MB\n\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\trequire.Equal(t, int64(8*MB), db.opt.CommitBufferSize)\n\t\t// When the database starts, the commit buffer should be allocated with the size of CommitBufferSize.\n\t\trequire.Equal(t, 0, db.commitBuffer.Len())\n\t\trequire.Equal(t, db.opt.CommitBufferSize, int64(db.commitBuffer.Cap()))\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\ttxPut(t, db, bucket, GetTestBytes(0), GetRandomBytes(24), Persistent, nil, nil)\n\n\t\t// When tx is committed, content of commit buffer should be empty, but do not release memory\n\t\trequire.Equal(t, 0, db.commitBuffer.Len())\n\t\trequire.Equal(t, db.opt.CommitBufferSize, int64(db.commitBuffer.Cap()))\n\t})\n\n\topts = DefaultOptions\n\topts.CommitBufferSize = 1 * KB\n\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\trequire.Equal(t, int64(1*KB), db.opt.CommitBufferSize)\n\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\terr := db.Update(func(tx *Tx) error {\n\t\t\t// making this tx big enough, it should not use the commit buffer\n\t\t\tfor i := 0; i < 1000; i++ {\n\t\t\t\terr := tx.Put(bucket, GetTestBytes(i), GetRandomBytes(1024), Persistent)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\trequire.Equal(t, 0, db.commitBuffer.Len())\n\t\trequire.Equal(t, db.opt.CommitBufferSize, int64(db.commitBuffer.Cap()))\n\t})\n}\n\nfunc TestDB_DeleteBucket(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tbucket := \"bucket\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\tkey := GetTestBytes(0)\n\t\tval := GetTestBytes(0)\n\t\ttxPut(t, db, bucket, key, val, Persistent, nil, nil)\n\t\ttxGet(t, db, bucket, key, val, nil)\n\n\t\ttxDeleteBucket(t, db, DataStructureBTree, bucket, nil)\n\t\ttxPut(t, db, bucket, key, val, Persistent, ErrorBucketNotExist, nil)\n\t})\n}\n\nfunc withDBOption(t *testing.T, opt Options, fn func(t *testing.T, db *DB)) {\n\tdb, err := Open(opt)\n\trequire.NoError(t, err)\n\n\tdefer func() {\n\t\tos.RemoveAll(db.opt.Dir)\n\t\tdb.Close()\n\t}()\n\n\tfn(t, db)\n}\n\nfunc withDefaultDB(t *testing.T, fn func(t *testing.T, db *DB)) {\n\ttmpdir, _ := os.MkdirTemp(\"\", \"nutsdb\")\n\topt := DefaultOptions\n\topt.Dir = tmpdir\n\topt.SegmentSize = 8 * 1024\n\n\twithDBOption(t, opt, fn)\n}\n\nfunc withRAMIdxDB(t *testing.T, fn func(t *testing.T, db *DB)) {\n\ttmpdir, _ := os.MkdirTemp(\"\", \"nutsdb\")\n\topt := DefaultOptions\n\topt.Dir = tmpdir\n\topt.EntryIdxMode = HintKeyAndRAMIdxMode\n\n\twithDBOption(t, opt, fn)\n}\n\nfunc TestDB_HintKeyValAndRAMIdxMode_RestartDB(t *testing.T) {\n\topts := DefaultOptions\n\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\tbucket := \"bucket\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\tkey := GetTestBytes(0)\n\t\tval := GetTestBytes(0)\n\n\t\ttxPut(t, db, bucket, key, val, Persistent, nil, nil)\n\t\ttxGet(t, db, bucket, key, val, nil)\n\n\t\tdb.Close()\n\t\t// restart db with HintKeyValAndRAMIdxMode EntryIdxMode\n\t\tdb, err := Open(db.opt)\n\t\trequire.NoError(t, err)\n\t\ttxGet(t, db, bucket, key, val, nil)\n\t})\n}\n\nfunc TestDB_HintKeyAndRAMIdxMode_RestartDB(t *testing.T) {\n\topts := DefaultOptions\n\topts.EntryIdxMode = HintKeyAndRAMIdxMode\n\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\tbucket := \"bucket\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\tkey := GetTestBytes(0)\n\t\tval := GetTestBytes(0)\n\n\t\ttxPut(t, db, bucket, key, val, Persistent, nil, nil)\n\t\ttxGet(t, db, bucket, key, val, nil)\n\t\tdb.Close()\n\n\t\t// restart db with HintKeyAndRAMIdxMode EntryIdxMode\n\t\tdb, err := Open(db.opt)\n\t\trequire.NoError(t, err)\n\t\ttxGet(t, db, bucket, key, val, nil)\n\t})\n}\n\nfunc TestDB_HintKeyAndRAMIdxMode_LruCache(t *testing.T) {\n\topts := DefaultOptions\n\topts.EntryIdxMode = HintKeyAndRAMIdxMode\n\tlruCacheSizes := []int{0, 5000, 10000, 20000}\n\n\tfor _, lruCacheSize := range lruCacheSizes {\n\t\topts.HintKeyAndRAMIdxCacheSize = lruCacheSize\n\t\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\t\tbucket := \"bucket\"\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\tfor i := 0; i < 10000; i++ {\n\t\t\t\tkey := []byte(fmt.Sprintf(\"%10d\", i))\n\t\t\t\tval := []byte(fmt.Sprintf(\"%10d\", i))\n\t\t\t\ttxPut(t, db, bucket, key, val, Persistent, nil, nil)\n\t\t\t\ttxGet(t, db, bucket, key, val, nil)\n\t\t\t\ttxGet(t, db, bucket, key, val, nil)\n\t\t\t}\n\t\t\tdb.Close()\n\t\t})\n\t}\n}\n\nfunc TestDB_ChangeMode_RestartDB(t *testing.T) {\n\tchangeModeRestart := func(firstMode EntryIdxMode, secondMode EntryIdxMode) {\n\t\topts := DefaultOptions\n\t\topts.EntryIdxMode = firstMode\n\t\tvar err error\n\n\t\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\t\tbucket := \"bucket\"\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\n\t\t\t// k-v\n\t\t\tfor i := 0; i < 10; i++ {\n\t\t\t\ttxPut(t, db, bucket, GetTestBytes(i), GetTestBytes(i), Persistent, nil, nil)\n\t\t\t}\n\n\t\t\t// list\n\t\t\tfor i := 0; i < 10; i++ {\n\t\t\t\ttxPush(t, db, bucket, GetTestBytes(0), GetTestBytes(i), true, nil, nil)\n\t\t\t}\n\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\treturn tx.LRem(bucket, GetTestBytes(0), 1, GetTestBytes(5))\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\n\t\t\tfor i := 0; i < 2; i++ {\n\t\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(9-i), nil, true)\n\t\t\t}\n\n\t\t\tfor i := 0; i < 2; i++ {\n\t\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(i), nil, false)\n\t\t\t}\n\n\t\t\t// set\n\t\t\tfor i := 0; i < 10; i++ {\n\t\t\t\ttxSAdd(t, db, bucket, GetTestBytes(0), GetTestBytes(i), nil, nil)\n\t\t\t}\n\n\t\t\tfor i := 0; i < 3; i++ {\n\t\t\t\ttxSRem(t, db, bucket, GetTestBytes(0), GetTestBytes(i), nil)\n\t\t\t}\n\n\t\t\t// zset\n\t\t\tfor i := 0; i < 10; i++ {\n\t\t\t\ttxZAdd(t, db, bucket, GetTestBytes(0), GetTestBytes(i), float64(i), nil, nil)\n\t\t\t}\n\n\t\t\tfor i := 0; i < 3; i++ {\n\t\t\t\ttxZRem(t, db, bucket, GetTestBytes(0), GetTestBytes(i), nil)\n\t\t\t}\n\n\t\t\trequire.NoError(t, db.Close())\n\n\t\t\topts.EntryIdxMode = secondMode\n\t\t\tdb, err = Open(opts)\n\t\t\trequire.NoError(t, err)\n\n\t\t\t// k-v\n\t\t\tfor i := 0; i < 10; i++ {\n\t\t\t\ttxGet(t, db, bucket, GetTestBytes(i), GetTestBytes(i), nil)\n\t\t\t}\n\n\t\t\t// list\n\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(7), nil, true)\n\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(6), nil, true)\n\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(4), nil, true)\n\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(2), nil, false)\n\n\t\t\terr = db.View(func(tx *Tx) error {\n\t\t\t\tsize, err := tx.LSize(bucket, GetTestBytes(0))\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\trequire.Equal(t, 1, size)\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\n\t\t\t// set\n\t\t\tfor i := 0; i < 3; i++ {\n\t\t\t\ttxSIsMember(t, db, bucket, GetTestBytes(0), GetTestBytes(i), false)\n\t\t\t}\n\n\t\t\tfor i := 3; i < 10; i++ {\n\t\t\t\ttxSIsMember(t, db, bucket, GetTestBytes(0), GetTestBytes(i), true)\n\t\t\t}\n\n\t\t\t// zset\n\t\t\tfor i := 0; i < 3; i++ {\n\t\t\t\ttxZScore(t, db, bucket, GetTestBytes(0), GetTestBytes(i), float64(i), ErrSortedSetMemberNotExist)\n\t\t\t}\n\n\t\t\tfor i := 3; i < 10; i++ {\n\t\t\t\ttxZScore(t, db, bucket, GetTestBytes(0), GetTestBytes(i), float64(i), nil)\n\t\t\t}\n\t\t})\n\t}\n\n\t// HintKeyValAndRAMIdxMode to HintKeyAndRAMIdxMode\n\tchangeModeRestart(HintKeyValAndRAMIdxMode, HintKeyAndRAMIdxMode)\n\t// HintKeyAndRAMIdxMode to HintKeyValAndRAMIdxMode\n\tchangeModeRestart(HintKeyAndRAMIdxMode, HintKeyValAndRAMIdxMode)\n}\n\nfunc TestTx_SmallFile(t *testing.T) {\n\topts := DefaultOptions\n\topts.SegmentSize = 100\n\topts.EntryIdxMode = HintKeyAndRAMIdxMode\n\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\tbucket := \"bucket\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\terr := db.Update(func(tx *Tx) error {\n\t\t\tfor i := 0; i < 100; i++ {\n\t\t\t\terr := tx.Put(bucket, GetTestBytes(i), GetTestBytes(i), Persistent)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t\trequire.Nil(t, err)\n\t\trequire.NoError(t, db.Close())\n\t\tdb, _ = Open(opts)\n\n\t\ttxGet(t, db, bucket, GetTestBytes(10), GetTestBytes(10), nil)\n\t})\n}\n\nfunc TestDB_DataStructureBTreeWriteRecordLimit(t *testing.T) {\n\topts := DefaultOptions\n\tlimitCount := int64(1000)\n\topts.MaxWriteRecordCount = limitCount\n\tbucket1 := \"bucket1\"\n\tbucket2 := \"bucket2\"\n\t// Iterate over different EntryIdxModes\n\tfor _, idxMode := range []EntryIdxMode{HintKeyValAndRAMIdxMode, HintKeyAndRAMIdxMode} {\n\t\topts.EntryIdxMode = idxMode\n\t\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket1, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket2, nil)\n\n\t\t\t// Add limitCount records\n\t\t\terr := db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < int(limitCount); i++ {\n\t\t\t\t\tkey := []byte(strconv.Itoa(i))\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr = tx.Put(bucket1, key, value, Persistent)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\t// Trigger the limit\n\t\t\ttxPut(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), Persistent, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Add a key that is within the limit\n\t\t\ttxPut(t, db, bucket1, []byte(\"0\"), []byte(\"000\"), Persistent, nil, nil)\n\t\t\t// Delete and add one item\n\t\t\ttxDel(t, db, bucket1, []byte(\"0\"), nil)\n\t\t\ttxPut(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), Persistent, nil, nil)\n\t\t\t// Add an item to another bucket\n\t\t\ttxPut(t, db, bucket2, []byte(\"key2\"), []byte(\"value2\"), Persistent, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Delete bucket1\n\t\t\ttxDeleteBucket(t, db, DataStructureBTree, bucket1, nil)\n\t\t\t// Add data to bucket2\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < (int(limitCount) - 1); i++ {\n\t\t\t\t\tkey := []byte(strconv.Itoa(i))\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr = tx.Put(bucket2, key, value, Persistent)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\t// Add items to bucket2\n\t\t\ttxPut(t, db, bucket2, []byte(\"key1\"), []byte(\"value1\"), Persistent, nil, nil)\n\t\t\ttxPut(t, db, bucket2, []byte(\"key2\"), []byte(\"value2\"), Persistent, nil, ErrTxnExceedWriteLimit)\n\t\t})\n\t}\n}\n\nfunc TestDB_DataStructureListWriteRecordLimit(t *testing.T) {\n\t// Set options\n\topts := DefaultOptions\n\tlimitCount := int64(1000)\n\topts.MaxWriteRecordCount = limitCount\n\t// Define bucket names\n\tbucket1 := \"bucket1\"\n\tbucket2 := \"bucket2\"\n\t// Iterate over EntryIdxMode options\n\tfor _, idxMode := range []EntryIdxMode{HintKeyValAndRAMIdxMode, HintKeyAndRAMIdxMode} {\n\n\t\topts.EntryIdxMode = idxMode\n\t\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket1, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket2, nil)\n\t\t\t// Add limitCount records\n\t\t\terr := db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < int(limitCount); i++ {\n\t\t\t\t\tkey := []byte(\"0\")\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr = tx.LPush(bucket1, key, value)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\t// Trigger the limit\n\t\t\ttxPush(t, db, bucket1, []byte(\"0\"), []byte(\"value1\"), false, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Test LRem\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\terr := tx.LRem(bucket1, []byte(\"0\"), 1, []byte(\"0\"))\n\t\t\t\tassertErr(t, err, nil)\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\ttxPush(t, db, bucket1, []byte(\"0\"), []byte(\"value1\"), true, nil, nil)\n\t\t\ttxPush(t, db, bucket1, []byte(\"0\"), []byte(\"value1\"), false, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Test for DataLPopFlag\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\t_, err := tx.LPop(bucket1, []byte(\"0\"))\n\t\t\t\tassertErr(t, err, nil)\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\ttxPush(t, db, bucket1, []byte(\"0\"), []byte(\"value1\"), false, nil, nil)\n\t\t\ttxPush(t, db, bucket1, []byte(\"0\"), []byte(\"value1\"), false, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Test for DataLTrimFlag\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\terr := tx.LTrim(bucket1, []byte(\"0\"), 0, 0)\n\t\t\t\tassertErr(t, err, nil)\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < int(limitCount)-2; i++ {\n\t\t\t\t\tkey := []byte(\"0\")\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr = tx.RPush(bucket1, key, value)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\ttxPush(t, db, bucket1, []byte(\"0\"), []byte(\"value11\"), false, nil, nil)\n\t\t\ttxPush(t, db, bucket1, []byte(\"0\"), []byte(\"value11\"), false, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Test for LRemByIndex\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\terr := tx.LRemByIndex(bucket1, []byte(\"0\"), 0, 1, 2)\n\t\t\t\tassertErr(t, err, nil)\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < 2; i++ {\n\t\t\t\t\tkey := []byte(\"0\")\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr = tx.RPush(bucket1, key, value)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\ttxPush(t, db, bucket2, []byte(\"0\"), []byte(\"value11\"), false, nil, nil)\n\t\t\ttxPush(t, db, bucket1, []byte(\"0\"), []byte(\"value11\"), false, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Delete bucket\n\t\t\ttxDeleteBucket(t, db, DataStructureList, bucket1, nil)\n\t\t\t// Add data to another bucket\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < int(limitCount)-1; i++ {\n\t\t\t\t\tkey := []byte(strconv.Itoa(i))\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr = tx.RPush(bucket2, key, value)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\ttxPush(t, db, bucket2, []byte(\"key1\"), []byte(\"value1\"), false, nil, ErrTxnExceedWriteLimit)\n\t\t})\n\t}\n}\n\nfunc TestDB_DataStructureSetWriteRecordLimit(t *testing.T) {\n\t// Set default options and limitCount.\n\topts := DefaultOptions\n\tlimitCount := int64(1000)\n\topts.MaxWriteRecordCount = limitCount\n\t// Define bucket names.\n\tbucket1 := \"bucket1\"\n\tbucket2 := \"bucket2\"\n\t// Loop through EntryIdxModes.\n\tfor _, idxMode := range []EntryIdxMode{HintKeyValAndRAMIdxMode, HintKeyAndRAMIdxMode} {\n\t\topts.EntryIdxMode = idxMode\n\t\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureSet, bucket1, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureSet, bucket2, nil)\n\n\t\t\t// Add limitCount records to bucket1.\n\t\t\terr := db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < int(limitCount); i++ {\n\t\t\t\t\tkey := []byte(\"0\")\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr := tx.SAdd(bucket1, key, value)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\t// Try to add one more item to bucket1 and check for ErrTxnExceedWriteLimit.\n\t\t\ttxSAdd(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), nil, ErrTxnExceedWriteLimit)\n\t\t\t// Remove one item and add another item to bucket1.\n\t\t\ttxSRem(t, db, bucket1, []byte(\"0\"), []byte(\"0\"), nil)\n\t\t\ttxSAdd(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), nil, nil)\n\t\t\t// Add two more items to bucket1 and check for ErrTxnExceedWriteLimit.\n\t\t\ttxSAdd(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), nil, nil)\n\t\t\ttxSAdd(t, db, bucket1, []byte(\"key11\"), []byte(\"value11\"), nil, ErrTxnExceedWriteLimit)\n\t\t\t// Test for SPOP, SPOP two items from bucket1.\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\t_, err := tx.SPop(bucket1, []byte(\"0\"))\n\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t_, err = tx.SPop(bucket1, []byte(\"key1\"))\n\t\t\t\tassertErr(t, err, nil)\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\t// Add two items to bucket1 and check for ErrTxnExceedWriteLimit.\n\t\t\ttxSAdd(t, db, bucket1, []byte(\"1\"), []byte(\"value1\"), nil, nil)\n\t\t\ttxSAdd(t, db, bucket1, []byte(\"1\"), []byte(\"value2\"), nil, nil)\n\t\t\ttxSAdd(t, db, bucket1, []byte(\"1\"), []byte(\"value3\"), nil, ErrTxnExceedWriteLimit)\n\t\t\t// Delete bucket1.\n\t\t\ttxDeleteBucket(t, db, DataStructureSet, bucket1, nil)\n\t\t\t// Add data to bucket2.\n\t\t\ttxSAdd(t, db, bucket2, []byte(\"key1\"), []byte(\"value1\"), nil, nil)\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < int(limitCount)-1; i++ {\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr = tx.SAdd(bucket2, []byte(\"2\"), value)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\t// Try to add one more item to bucket2 and check for ErrTxnExceedWriteLimit.\n\t\t\ttxSAdd(t, db, bucket2, []byte(\"key2\"), []byte(\"value2\"), nil, ErrTxnExceedWriteLimit)\n\t\t})\n\t}\n}\n\nfunc TestDB_DataStructureSortedSetWriteRecordLimit(t *testing.T) {\n\t// Set up options\n\topts := DefaultOptions\n\tlimitCount := int64(1000)\n\topts.MaxWriteRecordCount = limitCount\n\t// Set up bucket names and score\n\tbucket1 := \"bucket1\"\n\tbucket2 := \"bucket2\"\n\tscore := 1.0\n\t// Iterate over EntryIdxMode options\n\tfor _, idxMode := range []EntryIdxMode{HintKeyValAndRAMIdxMode, HintKeyAndRAMIdxMode} {\n\t\topts.EntryIdxMode = idxMode\n\t\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket1, nil)\n\t\t\t// Add limitCount records\n\t\t\terr := db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < int(limitCount); i++ {\n\t\t\t\t\tkey := []byte(\"0\")\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr := tx.ZAdd(bucket1, key, score+float64(i), value)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\t// Trigger the limit\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), score, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Delete and add one item\n\t\t\ttxZRem(t, db, bucket1, []byte(\"0\"), []byte(\"0\"), nil)\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), score, nil, nil)\n\t\t\t// Add some data is ok\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), score, nil, nil)\n\t\t\t// Trigger the limit\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"key2\"), []byte(\"value2\"), score, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Test for ZRemRangeByRank\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\terr := tx.ZRemRangeByRank(bucket1, []byte(\"0\"), 1, 3)\n\t\t\t\tassert.NoError(t, err)\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\tassert.NoError(t, err)\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"0\"), []byte(\"value1\"), score, nil, nil)\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"0\"), []byte(\"value2\"), score, nil, nil)\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"0\"), []byte(\"value3\"), score+float64(1000), nil, nil)\n\t\t\t// Trigger the limit\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"0\"), []byte(\"value4\"), score, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Test for ZPop\n\t\t\ttxZPop(t, db, bucket1, []byte(\"0\"), true, []byte(\"value3\"), score+float64(1000), nil)\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"key3\"), []byte(\"value3\"), score, nil, nil)\n\t\t\t// Delete bucket\n\t\t\ttxDeleteBucket(t, db, DataStructureSortedSet, bucket1, nil)\n\t\t\t// Add data to another bucket\n\t\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket1, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket2, nil)\n\t\t\ttxZAdd(t, db, bucket2, []byte(\"key1\"), []byte(\"value1\"), score, nil, nil)\n\t\t\t// Add data to bucket1\n\t\t\terr = db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < int(limitCount)-1; i++ {\n\t\t\t\t\tkey := []byte(strconv.Itoa(i))\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr = tx.ZAdd(bucket1, key, score, value)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\t// Trigger the limit\n\t\t\ttxZAdd(t, db, bucket2, []byte(\"key1\"), []byte(\"value2\"), score, nil, ErrTxnExceedWriteLimit)\n\t\t})\n\t}\n}\n\nfunc TestDB_AllDsWriteRecordLimit(t *testing.T) {\n\t// Set up options\n\topts := DefaultOptions\n\tlimitCount := int64(1000)\n\topts.MaxWriteRecordCount = limitCount\n\t// Set up bucket names and score\n\tbucket1 := \"bucket1\"\n\tbucket2 := \"bucket2\"\n\tscore := 1.0\n\t// Iterate over EntryIdxMode options\n\tfor _, idxMode := range []EntryIdxMode{HintKeyValAndRAMIdxMode, HintKeyAndRAMIdxMode} {\n\t\topts.EntryIdxMode = idxMode\n\t\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket1, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket1, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureSet, bucket1, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket1, nil)\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket2, nil)\n\n\t\t\t// Add limitCount records\n\t\t\terr := db.Update(func(tx *Tx) error {\n\t\t\t\tfor i := 0; i < int(limitCount); i++ {\n\t\t\t\t\tkey := []byte(strconv.Itoa(i))\n\t\t\t\t\tvalue := []byte(strconv.Itoa(i))\n\t\t\t\t\terr = tx.Put(bucket1, key, value, Persistent)\n\t\t\t\t\tassertErr(t, err, nil)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\t// Trigger the limit\n\t\t\ttxPush(t, db, bucket1, []byte(\"0\"), []byte(\"value1\"), false, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Delete item and add one\n\t\t\ttxDel(t, db, bucket1, []byte(\"0\"), nil)\n\t\t\ttxPush(t, db, bucket1, []byte(\"0\"), []byte(\"value1\"), false, nil, nil)\n\t\t\t// Trigger the limit\n\t\t\ttxSAdd(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), nil, ErrTxnExceedWriteLimit)\n\t\t\t// Delete item and add one\n\t\t\ttxDel(t, db, bucket1, []byte(\"1\"), nil)\n\t\t\ttxSAdd(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), nil, nil)\n\t\t\t// Trigger the limit\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), score, nil, ErrTxnExceedWriteLimit)\n\t\t\t// Delete item and add one\n\t\t\ttxDel(t, db, bucket1, []byte(\"2\"), nil)\n\t\t\ttxZAdd(t, db, bucket1, []byte(\"key1\"), []byte(\"value1\"), score, nil, nil)\n\t\t\t// Delete bucket\n\t\t\ttxDeleteBucket(t, db, DataStructureSortedSet, bucket1, nil)\n\t\t\t// Add data to another bucket\n\t\t\ttxPush(t, db, bucket2, []byte(\"key1\"), []byte(\"value1\"), false, nil, nil)\n\t\t\t// Trigger the limit\n\t\t\ttxPush(t, db, bucket2, []byte(\"key2\"), []byte(\"value2\"), false, nil, ErrTxnExceedWriteLimit)\n\t\t})\n\t}\n}\n\nfunc txIncrement(t *testing.T, db *DB, bucket string, key []byte, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.Incr(bucket, key)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txDecrement(t *testing.T, db *DB, bucket string, key []byte, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.Decr(bucket, key)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txIncrementBy(t *testing.T, db *DB, bucket string, key []byte, value int64, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.IncrBy(bucket, key, value)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txDecrementBy(t *testing.T, db *DB, bucket string, key []byte, value int64, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.DecrBy(bucket, key, value)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txPutIfNotExists(t *testing.T, db *DB, bucket string, key, value []byte, expectedErr, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.PutIfNotExists(bucket, key, value, Persistent)\n\t\tassertErr(t, err, expectedErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txPutIfExists(t *testing.T, db *DB, bucket string, key, value []byte, expectedErr, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.PutIfExists(bucket, key, value, Persistent)\n\t\tassertErr(t, err, expectedErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txValueLen(t *testing.T, db *DB, bucket string, key []byte, expectLength int, expectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tlength, err := tx.ValueLen(bucket, key)\n\t\tif expectErr != nil {\n\t\t\trequire.Equal(t, expectErr, err)\n\t\t} else {\n\t\t\trequire.NoError(t, err)\n\t\t}\n\t\trequire.EqualValuesf(t, expectLength, length, \"err Tx ValueLen. got %s want %s\", length, expectLength)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txGetSet(t *testing.T, db *DB, bucket string, key, value []byte, expectOldValue []byte, expectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\toldValue, err := tx.GetSet(bucket, key, value)\n\t\tassertErr(t, err, expectErr)\n\t\trequire.EqualValuesf(t, oldValue, expectOldValue, \"err Tx GetSet. got %s want %s\", string(oldValue), string(expectOldValue))\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txGetBit(t *testing.T, db *DB, bucket string, key []byte, offset int, expectVal byte, expectErr error, finalExpectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tvalue, err := tx.GetBit(bucket, key, offset)\n\t\tassertErr(t, err, expectErr)\n\t\trequire.Equal(t, expectVal, value)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txSetBit(t *testing.T, db *DB, bucket string, key []byte, offset int, value byte, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.SetBit(bucket, key, offset, value)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txGetTTL(t *testing.T, db *DB, bucket string, key []byte, expectedTTL int64, expectedErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tttl, err := tx.GetTTL(bucket, key)\n\t\tassertErr(t, err, expectedErr)\n\n\t\t// If diff between expectedTTL and realTTL lesser than 1s, We'll consider as equal\n\t\tdiff := int(math.Abs(float64(ttl - expectedTTL)))\n\t\tassert.LessOrEqual(t, diff, 1)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txPersist(t *testing.T, db *DB, bucket string, key []byte, expectedErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.Persist(bucket, key)\n\t\tassertErr(t, err, expectedErr)\n\t\treturn nil\n\t})\n\trequire.NoError(t, err)\n}\n\nfunc txMSet(t *testing.T, db *DB, bucket string, args [][]byte, ttl uint32, expectErr error, finalExpectErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.MSet(bucket, ttl, args...)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txMGet(t *testing.T, db *DB, bucket string, keys [][]byte, expectValues [][]byte, expectErr error, finalExpectErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tvalues, err := tx.MGet(bucket, keys...)\n\t\tassertErr(t, err, expectErr)\n\t\trequire.EqualValues(t, expectValues, values)\n\t\treturn nil\n\t})\n\tassertErr(t, err, finalExpectErr)\n}\n\nfunc txAppend(t *testing.T, db *DB, bucket string, key, appendage []byte, expectErr error, expectFinalErr error) {\n\terr := db.Update(func(tx *Tx) error {\n\t\terr := tx.Append(bucket, key, appendage)\n\t\tassertErr(t, err, expectErr)\n\t\treturn nil\n\t})\n\tassertErr(t, err, expectFinalErr)\n}\n\nfunc txGetRange(t *testing.T, db *DB, bucket string, key []byte, start, end int, expectVal []byte, expectErr error, expectFinalErr error) {\n\terr := db.View(func(tx *Tx) error {\n\t\tvalue, err := tx.GetRange(bucket, key, start, end)\n\t\tassertErr(t, err, expectErr)\n\t\trequire.EqualValues(t, expectVal, value)\n\t\treturn nil\n\t})\n\tassertErr(t, err, expectFinalErr)\n}\n"
        },
        {
          "name": "doc.go",
          "type": "blob",
          "size": 0.80859375,
          "content": "/*\nPackage nutsdb implements a simple, fast, embeddable and persistent key/value store\nwritten in pure Go. It supports fully serializable transactions.\nAnd it also supports data structure such as list、set、sorted set etc.\n\nNutsDB currently works on Mac OS, Linux and Windows.\n\nUsage\n\nNutsDB has the following main types: DB, BPTree, Entry, DataFile And Tx. and NutsDB supports bucket, A bucket is\na collection of unique keys that are associated with values.\n\nAll operations happen inside a Tx. Tx represents a transaction, which can\nbe read-only or read-write. Read-only transactions can read values for a\ngiven key , or iterate over a set of key-value pairs (prefix scanning or range scanning).\nread-write transactions can also update and delete keys from the DB.\n\nSee the examples for more usage details.\n*/\npackage nutsdb\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "entity_utils.go",
          "type": "blob",
          "size": 0.6689453125,
          "content": "package nutsdb\n\nimport \"reflect\"\n\nfunc GetDiskSizeFromSingleObject(obj interface{}) int64 {\n\ttyp := reflect.TypeOf(obj)\n\tfields := reflect.VisibleFields(typ)\n\tif len(fields) == 0 {\n\t\treturn 0\n\t}\n\tvar size int64 = 0\n\tfor _, field := range fields {\n\t\t// Currently, we only use the unsigned value type for our metadata.go. That's reasonable for us.\n\t\t// Because it's not possible to use negative value mark the size of data.\n\t\t// But if you want to make it more flexible, please help yourself.\n\t\tswitch field.Type.Kind() {\n\t\tcase reflect.Uint8:\n\t\t\tsize += 1\n\t\tcase reflect.Uint16:\n\t\t\tsize += 2\n\t\tcase reflect.Uint32:\n\t\t\tsize += 4\n\t\tcase reflect.Uint64:\n\t\t\tsize += 8\n\t\t}\n\t}\n\treturn size\n}\n"
        },
        {
          "name": "entity_utils_test.go",
          "type": "blob",
          "size": 0.537109375,
          "content": "package nutsdb\n\nimport (\n\t\"github.com/stretchr/testify/assert\"\n\t\"testing\"\n)\n\nfunc TestGetDiskSizeFromSingleObject(t *testing.T) {\n\ttype args struct {\n\t\tobj interface{}\n\t}\n\ttests := []struct {\n\t\tname string\n\t\targs args\n\t\twant int64\n\t}{\n\t\t{\n\t\t\tname: \"happy path for getting entry header size\",\n\t\t\targs: args{\n\t\t\t\tobj: MetaData{},\n\t\t\t},\n\t\t\twant: 50,\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tassert.Equalf(t, tt.want, GetDiskSizeFromSingleObject(tt.args.obj), \"GetDiskSizeFromSingleObject(%v)\", tt.args.obj)\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "entry.go",
          "type": "blob",
          "size": 8.33203125,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"hash/crc32\"\n\t\"sort\"\n\t\"strings\"\n\n\t\"github.com/xujiajun/utils/strconv2\"\n)\n\nvar (\n\tErrPayLoadSizeMismatch   = errors.New(\"the payload size in Meta mismatch with the payload size needed\")\n\tErrHeaderSizeOutOfBounds = errors.New(\"the header size is out of bounds\")\n)\n\nconst (\n\tMaxEntryHeaderSize = 4 + binary.MaxVarintLen32*3 + binary.MaxVarintLen64*3 + binary.MaxVarintLen16*3\n\tMinEntryHeaderSize = 4 + 9\n)\n\ntype (\n\t// Entry represents the data item.\n\tEntry struct {\n\t\tKey   []byte\n\t\tValue []byte\n\t\tMeta  *MetaData\n\t}\n)\n\n// Size returns the size of the entry.\nfunc (e *Entry) Size() int64 {\n\treturn e.Meta.Size() + int64(e.Meta.KeySize+e.Meta.ValueSize)\n}\n\n// Encode returns the slice after the entry be encoded.\n//\n//\tthe entry stored format:\n//\t|----------------------------------------------------------------------------------------------------------|\n//\t|  crc  | timestamp | ksz | valueSize | flag  | TTL  | status | ds   | txId |  bucketId |  key  | value    |\n//\t|----------------------------------------------------------------------------------------------------------|\n//\t| uint32| uint64  |uint32 |  uint32 | uint16  | uint32| uint16 | uint16 |uint64 | uint64 | []byte | []byte |\n//\t|----------------------------------------------------------------------------------------------------------|\nfunc (e *Entry) Encode() []byte {\n\tkeySize := e.Meta.KeySize\n\tvalueSize := e.Meta.ValueSize\n\n\tbuf := make([]byte, MaxEntryHeaderSize+keySize+valueSize)\n\n\tindex := e.setEntryHeaderBuf(buf)\n\tcopy(buf[index:], e.Key)\n\tindex += int(keySize)\n\tcopy(buf[index:], e.Value)\n\tindex += int(valueSize)\n\n\tbuf = buf[:index]\n\n\tc32 := crc32.ChecksumIEEE(buf[4:])\n\tbinary.LittleEndian.PutUint32(buf[0:4], c32)\n\n\treturn buf\n}\n\n// setEntryHeaderBuf sets the entry header buff.\nfunc (e *Entry) setEntryHeaderBuf(buf []byte) int {\n\tindex := 4\n\n\tindex += binary.PutUvarint(buf[index:], e.Meta.Timestamp)\n\tindex += binary.PutUvarint(buf[index:], uint64(e.Meta.KeySize))\n\tindex += binary.PutUvarint(buf[index:], uint64(e.Meta.ValueSize))\n\tindex += binary.PutUvarint(buf[index:], uint64(e.Meta.Flag))\n\tindex += binary.PutUvarint(buf[index:], uint64(e.Meta.TTL))\n\tindex += binary.PutUvarint(buf[index:], uint64(e.Meta.Status))\n\tindex += binary.PutUvarint(buf[index:], uint64(e.Meta.Ds))\n\tindex += binary.PutUvarint(buf[index:], e.Meta.TxID)\n\tindex += binary.PutUvarint(buf[index:], e.Meta.BucketId)\n\n\treturn index\n}\n\n// IsZero checks if the entry is zero or not.\nfunc (e *Entry) IsZero() bool {\n\tif e.Meta.Crc == 0 && e.Meta.KeySize == 0 && e.Meta.ValueSize == 0 && e.Meta.Timestamp == 0 {\n\t\treturn true\n\t}\n\treturn false\n}\n\n// GetCrc returns the crc at given buf slice.\nfunc (e *Entry) GetCrc(buf []byte) uint32 {\n\tcrc := crc32.ChecksumIEEE(buf[4:])\n\tcrc = crc32.Update(crc, crc32.IEEETable, e.Key)\n\tcrc = crc32.Update(crc, crc32.IEEETable, e.Value)\n\n\treturn crc\n}\n\n// ParsePayload means this function will parse a byte array to bucket, key, size of an entry\nfunc (e *Entry) ParsePayload(data []byte) error {\n\tmeta := e.Meta\n\tkeyLowBound := 0\n\tkeyHighBound := meta.KeySize\n\tvalueLowBound := keyHighBound\n\tvalueHighBound := meta.KeySize + meta.ValueSize\n\n\t// parse key\n\te.Key = data[keyLowBound:keyHighBound]\n\t// parse value\n\te.Value = data[valueLowBound:valueHighBound]\n\treturn nil\n}\n\n// checkPayloadSize checks the payload size\nfunc (e *Entry) checkPayloadSize(size int64) error {\n\tif e.Meta.PayloadSize() != size {\n\t\treturn ErrPayLoadSizeMismatch\n\t}\n\treturn nil\n}\n\n// ParseMeta parse Meta object to entry\nfunc (e *Entry) ParseMeta(buf []byte) (int64, error) {\n\t// If the length of the header is less than MinEntryHeaderSize,\n\t// it means that the final remaining capacity of the file is not enough to write a record,\n\t// and an error needs to be returned.\n\tif len(buf) < MinEntryHeaderSize {\n\t\treturn 0, ErrHeaderSizeOutOfBounds\n\t}\n\n\te.Meta = NewMetaData()\n\n\te.Meta.WithCrc(binary.LittleEndian.Uint32(buf[0:4]))\n\n\tindex := 4\n\n\ttimestamp, n := binary.Uvarint(buf[index:])\n\tindex += n\n\tkeySize, n := binary.Uvarint(buf[index:])\n\tindex += n\n\tvalueSize, n := binary.Uvarint(buf[index:])\n\tindex += n\n\tflag, n := binary.Uvarint(buf[index:])\n\tindex += n\n\tttl, n := binary.Uvarint(buf[index:])\n\tindex += n\n\tstatus, n := binary.Uvarint(buf[index:])\n\tindex += n\n\tds, n := binary.Uvarint(buf[index:])\n\tindex += n\n\ttxId, n := binary.Uvarint(buf[index:])\n\tindex += n\n\tbucketId, n := binary.Uvarint(buf[index:])\n\tindex += n\n\n\te.Meta.\n\t\tWithTimeStamp(timestamp).\n\t\tWithKeySize(uint32(keySize)).\n\t\tWithValueSize(uint32(valueSize)).\n\t\tWithFlag(uint16(flag)).\n\t\tWithTTL(uint32(ttl)).\n\t\tWithStatus(uint16(status)).\n\t\tWithDs(uint16(ds)).\n\t\tWithTxID(txId).\n\t\tWithBucketId(bucketId)\n\n\treturn int64(index), nil\n}\n\n// isFilter to confirm if this entry is can be filtered\nfunc (e *Entry) isFilter() bool {\n\tmeta := e.Meta\n\tvar filterDataSet = []uint16{\n\t\tDataDeleteFlag,\n\t\tDataRPopFlag,\n\t\tDataLPopFlag,\n\t\tDataLRemFlag,\n\t\tDataLTrimFlag,\n\t\tDataZRemFlag,\n\t\tDataZRemRangeByRankFlag,\n\t\tDataZPopMaxFlag,\n\t\tDataZPopMinFlag,\n\t\tDataLRemByIndex,\n\t}\n\treturn OneOfUint16Array(meta.Flag, filterDataSet)\n}\n\n// valid check the entry fields valid or not\nfunc (e *Entry) valid() error {\n\tif len(e.Key) == 0 {\n\t\treturn ErrKeyEmpty\n\t}\n\tif len(e.Key) > MAX_SIZE || len(e.Value) > MAX_SIZE {\n\t\treturn ErrDataSizeExceed\n\t}\n\treturn nil\n}\n\n// NewEntry new Entry Object\nfunc NewEntry() *Entry {\n\treturn new(Entry)\n}\n\n// WithKey set key to Entry\nfunc (e *Entry) WithKey(key []byte) *Entry {\n\te.Key = key\n\treturn e\n}\n\n// WithValue set value to Entry\nfunc (e *Entry) WithValue(value []byte) *Entry {\n\te.Value = value\n\treturn e\n}\n\n// WithMeta set Meta to Entry\nfunc (e *Entry) WithMeta(meta *MetaData) *Entry {\n\te.Meta = meta\n\treturn e\n}\n\n// GetTxIDBytes return the bytes of TxID\nfunc (e *Entry) GetTxIDBytes() []byte {\n\treturn []byte(strconv2.Int64ToStr(int64(e.Meta.TxID)))\n}\n\nfunc (e *Entry) IsBelongsToBPlusTree() bool {\n\treturn e.Meta.IsBPlusTree()\n}\n\nfunc (e *Entry) IsBelongsToList() bool {\n\treturn e.Meta.IsList()\n}\n\nfunc (e *Entry) IsBelongsToSet() bool {\n\treturn e.Meta.IsSet()\n}\n\nfunc (e *Entry) IsBelongsToSortSet() bool {\n\treturn e.Meta.IsSortSet()\n}\n\n// Entries represents entries\ntype Entries []*Entry\n\nfunc (e Entries) Len() int { return len(e) }\n\nfunc (e Entries) Less(i, j int) bool {\n\tl := string(e[i].Key)\n\tr := string(e[j].Key)\n\n\treturn strings.Compare(l, r) == -1\n}\n\nfunc (e Entries) Swap(i, j int) { e[i], e[j] = e[j], e[i] }\n\nfunc (e Entries) processEntriesScanOnDisk() (result []*Entry) {\n\tsort.Sort(e)\n\tfor _, ele := range e {\n\t\tcurE := ele\n\t\tif !IsExpired(curE.Meta.TTL, curE.Meta.Timestamp) && curE.Meta.Flag != DataDeleteFlag {\n\t\t\tresult = append(result, curE)\n\t\t}\n\t}\n\n\treturn result\n}\n\nfunc (e Entries) ToCEntries(lFunc func(l, r string) bool) CEntries {\n\treturn CEntries{\n\t\tEntries:  e,\n\t\tLessFunc: lFunc,\n\t}\n}\n\ntype CEntries struct {\n\tEntries\n\tLessFunc func(l, r string) bool\n}\n\nfunc (c CEntries) Len() int { return len(c.Entries) }\n\nfunc (c CEntries) Less(i, j int) bool {\n\tl := string(c.Entries[i].Key)\n\tr := string(c.Entries[j].Key)\n\tif c.LessFunc != nil {\n\t\treturn c.LessFunc(l, r)\n\t}\n\n\treturn c.Entries.Less(i, j)\n}\n\nfunc (c CEntries) Swap(i, j int) { c.Entries[i], c.Entries[j] = c.Entries[j], c.Entries[i] }\n\nfunc (c CEntries) processEntriesScanOnDisk() (result []*Entry) {\n\tsort.Sort(c)\n\tfor _, ele := range c.Entries {\n\t\tcurE := ele\n\t\tif !IsExpired(curE.Meta.TTL, curE.Meta.Timestamp) && curE.Meta.Flag != DataDeleteFlag {\n\t\t\tresult = append(result, curE)\n\t\t}\n\t}\n\n\treturn result\n}\n\ntype EntryWhenRecovery struct {\n\tEntry\n\tfid int64\n\toff int64\n}\n\ntype dataInTx struct {\n\tes       []*EntryWhenRecovery\n\ttxId     uint64\n\tstartOff int64\n}\n\nfunc (dt *dataInTx) isSameTx(e *EntryWhenRecovery) bool {\n\treturn dt.txId == e.Meta.TxID\n}\n\nfunc (dt *dataInTx) appendEntry(e *EntryWhenRecovery) {\n\tdt.es = append(dt.es, e)\n}\n\nfunc (dt *dataInTx) reset() {\n\tdt.es = make([]*EntryWhenRecovery, 0)\n\tdt.txId = 0\n}\n"
        },
        {
          "name": "entry_test.go",
          "type": "blob",
          "size": 3.2880859375,
          "content": "// Copyright 2019 The nutsdb Authors. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"encoding/binary\"\n\t\"reflect\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/suite\"\n)\n\ntype EntryTestSuite struct {\n\tsuite.Suite\n\tentry          Entry\n\texpectedEncode []byte\n}\n\nfunc (suite *EntryTestSuite) SetupSuite() {\n\tsuite.entry = Entry{\n\t\tKey:   []byte(\"key_0001\"),\n\t\tValue: []byte(\"val_0001\"),\n\t\tMeta: NewMetaData().WithKeySize(uint32(len(\"key_0001\"))).\n\t\t\tWithValueSize(uint32(len(\"val_0001\"))).WithTimeStamp(1547707905).WithTTL(Persistent).WithFlag(DataSetFlag).WithBucketId(1),\n\t}\n\tsuite.expectedEncode = []byte{168, 1, 59, 122, 129, 204, 128, 226, 5, 8, 8, 1, 0, 0, 0, 0, 1, 107, 101, 121, 95, 48, 48, 48, 49, 118, 97, 108, 95, 48, 48, 48, 49}\n}\n\nfunc (suite *EntryTestSuite) TestEncode() {\n\tok := reflect.DeepEqual(suite.entry.Encode(), suite.expectedEncode)\n\tassert.True(suite.T(), ok, \"entry's encode test fail\")\n}\n\nfunc (suite *EntryTestSuite) TestIsZero() {\n\n\tif ok := suite.entry.IsZero(); ok {\n\t\tassert.Fail(suite.T(), \"entry's IsZero test fail\")\n\t}\n\n}\n\nfunc (suite *EntryTestSuite) TestGetCrc() {\n\n\theaderSize := suite.entry.Meta.Size()\n\tcrc1 := suite.entry.GetCrc(suite.expectedEncode[:headerSize])\n\tcrc2 := binary.LittleEndian.Uint32(suite.expectedEncode[:4])\n\n\tif crc1 != crc2 {\n\t\tassert.Fail(suite.T(), \"entry's GetCrc test fail\")\n\t}\n}\n\nfunc TestEntrySuit(t *testing.T) {\n\tsuite.Run(t, new(EntryTestSuite))\n}\n\nfunc TestEntries_processEntriesScanOnDisk(t *testing.T) {\n\ttests := []struct {\n\t\tname       string\n\t\te          Entries\n\t\twantResult []*Entry\n\t}{\n\t\t{\n\t\t\t\"sort\",\n\t\t\tEntries{\n\t\t\t\t{\n\t\t\t\t\tKey:  []byte(\"abc\"),\n\t\t\t\t\tMeta: NewMetaData().WithTTL(0).WithFlag(DataSetFlag),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tKey:  []byte(\"z\"),\n\t\t\t\t\tMeta: NewMetaData().WithTTL(0).WithFlag(DataSetFlag),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tKey:  []byte(\"abcd\"),\n\t\t\t\t\tMeta: NewMetaData().WithTTL(0).WithFlag(DataSetFlag),\n\t\t\t\t},\n\t\t\t},\n\t\t\t[]*Entry{\n\t\t\t\t{\n\t\t\t\t\tKey:  []byte(\"abc\"),\n\t\t\t\t\tMeta: NewMetaData().WithTTL(0).WithFlag(DataSetFlag),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tKey:  []byte(\"abcd\"),\n\t\t\t\t\tMeta: NewMetaData().WithTTL(0).WithFlag(DataSetFlag),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tKey:  []byte(\"z\"),\n\t\t\t\t\tMeta: NewMetaData().WithTTL(0).WithFlag(DataSetFlag),\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"expired\",\n\t\t\tEntries{\n\t\t\t\t{\n\t\t\t\t\tKey:  []byte(\"abc\"),\n\t\t\t\t\tMeta: NewMetaData().WithTTL(1),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tKey:  []byte(\"abc\"),\n\t\t\t\t\tMeta: NewMetaData().WithTTL(0).WithFlag(DataDeleteFlag),\n\t\t\t\t},\n\t\t\t},\n\t\t\tnil,\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tassert.Equalf(t, tt.wantResult, tt.e.processEntriesScanOnDisk(), \"processEntriesScanOnDisk()\")\n\t\t})\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tassert.Equalf(t, tt.wantResult, tt.e.ToCEntries(nil).processEntriesScanOnDisk(), \"CEntries.processEntriesScanOnDisk()\")\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "errors.go",
          "type": "blob",
          "size": 1.017578125,
          "content": "package nutsdb\n\nimport (\n\t\"errors\"\n)\n\n// IsDBClosed is true if the error indicates the db was closed.\nfunc IsDBClosed(err error) bool {\n\treturn errors.Is(err, ErrDBClosed)\n}\n\n// IsKeyNotFound is true if the error indicates the key is not found.\nfunc IsKeyNotFound(err error) bool {\n\treturn errors.Is(err, ErrKeyNotFound)\n}\n\n// IsBucketNotFound is true if the error indicates the bucket is not exists.\nfunc IsBucketNotFound(err error) bool {\n\treturn errors.Is(err, ErrBucketNotFound)\n}\n\n// IsBucketEmpty is true if the bucket is empty.\nfunc IsBucketEmpty(err error) bool {\n\treturn errors.Is(err, ErrBucketEmpty)\n}\n\n// IsKeyEmpty is true if the key is empty.\nfunc IsKeyEmpty(err error) bool {\n\treturn errors.Is(err, ErrKeyEmpty)\n}\n\n// IsPrefixScan is true if prefix scanning not found the result.\nfunc IsPrefixScan(err error) bool {\n\treturn errors.Is(err, ErrPrefixScan)\n}\n\n// IsPrefixSearchScan is true if prefix and search scanning not found the result.\nfunc IsPrefixSearchScan(err error) bool {\n\treturn errors.Is(err, ErrPrefixSearchScan)\n}\n"
        },
        {
          "name": "errors_test.go",
          "type": "blob",
          "size": 4.86328125,
          "content": "package nutsdb\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/pkg/errors\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestIsKeyNotFound(t *testing.T) {\n\tts := []struct {\n\t\terr error\n\n\t\twant bool\n\t}{\n\t\t{\n\t\t\tErrKeyNotFound,\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\terrors.Wrap(ErrKeyNotFound, \"foobar\"),\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\terrors.New(\"foo bar\"),\n\t\t\tfalse,\n\t\t},\n\t}\n\n\tfor _, tc := range ts {\n\t\tgot := IsKeyNotFound(tc.err)\n\n\t\tassert.Equal(t, tc.want, got)\n\t}\n}\n\nfunc TestIsKeyEmpty(t *testing.T) {\n\ttype args struct {\n\t\terr error\n\t}\n\ttests := []struct {\n\t\tname string\n\t\targs args\n\t\twant bool\n\t}{\n\t\t{\"case1\", args{fmt.Errorf(\"foo error\")}, false},\n\t\t{\"case2\", args{fmt.Errorf(\"foo error,%w\", errors.New(\"sourceErr\"))}, false},\n\t\t{\"case3\", args{fmt.Errorf(\"foo error,%w\", ErrKeyEmpty)}, true},\n\t\t{\"case4_errors.Wrap\", args{errors.Wrap(ErrKeyEmpty, \"foo Err\")}, true},\n\t\t{\"case5_errors.Wrapf\", args{errors.Wrapf(ErrKeyEmpty, \"foo Err %s\", ErrKeyEmpty.Error())}, true},\n\t\t{\"case6_errors.rawError\", args{ErrKeyEmpty}, true},\n\t\t{\"case6_errors.keynotfoundErr\", args{ErrKeyNotFound}, false},\n\t\t// {\"case5\", args{errors.Wrap())}, true},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tif got := IsKeyEmpty(tt.args.err); got != tt.want {\n\t\t\t\tt.Errorf(\"IsKeyEmpty() = %v, want %v\", got, tt.want)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestIsBucketNotFound(t *testing.T) {\n\tts := []struct {\n\t\terr  error\n\t\twant bool\n\t}{\n\t\t{\n\t\t\tErrBucketNotFound,\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\terrors.Wrap(ErrBucketNotFound, \"foobar\"),\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\terrors.New(\"foobar\"),\n\t\t\tfalse,\n\t\t},\n\t}\n\n\tfor _, tc := range ts {\n\t\tgot := IsBucketNotFound(tc.err)\n\n\t\tassert.Equal(t, tc.want, got)\n\t}\n}\n\nfunc TestIsBucketEmpty(t *testing.T) {\n\tts := []struct {\n\t\terr  error\n\t\twant bool\n\t}{\n\t\t{\n\t\t\tErrBucketEmpty,\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\terrors.Wrap(ErrBucketEmpty, \"foobar\"),\n\t\t\ttrue,\n\t\t},\n\t\t{\n\t\t\terrors.New(\"foobar\"),\n\t\t\tfalse,\n\t\t},\n\t}\n\n\tfor _, tc := range ts {\n\t\tgot := IsBucketEmpty(tc.err)\n\n\t\tassert.Equal(t, tc.want, got)\n\t}\n}\n\nfunc TestIsDBClosed(t *testing.T) {\n\tInitOpt(\"\", true)\n\tbucket := \"test_closed\"\n\tkey := []byte(\"foo\")\n\tval := []byte(\"bar\")\n\tdb, err = Open(opt)\n\n\tt.Run(\"db can be used before closed\", func(t *testing.T) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\terr = db.Update(\n\t\t\tfunc(tx *Tx) error {\n\t\t\t\treturn tx.Put(bucket, key, val, Persistent)\n\t\t\t})\n\t\trequire.NoError(t, err)\n\t})\n\tassert.NoError(t, db.Close())\n\n\tt.Run(\"db can't be used after closed\", func(t *testing.T) {\n\t\terr = db.Update(\n\t\t\tfunc(tx *Tx) error {\n\t\t\t\treturn tx.Put(bucket, key, val, Persistent)\n\t\t\t})\n\t\tgot := IsDBClosed(err)\n\t\tassert.Equal(t, true, got)\n\t})\n}\n\nfunc TestIsPrefixScan(t *testing.T) {\n\tbucket := \"test_prefix_scan\"\n\tt.Run(\"if prefix scanning not found the result return true\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\t{\n\t\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\t\ttx, err := db.Begin(true)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tfor i := 0; i <= 10; i++ {\n\t\t\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\t\tval := []byte(\"val\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\t\t\tassert.NoError(t, err)\n\t\t\t\t}\n\t\t\t\tassert.NoError(t, tx.Commit())\n\t\t\t}\n\t\t\t{\n\t\t\t\ttx, err = db.Begin(false)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tprefix := []byte(\"key_\")\n\t\t\t\tes, err := tx.PrefixScan(bucket, prefix, 0, 10)\n\t\t\t\tassert.NoError(t, tx.Commit())\n\t\t\t\tassert.NotEmpty(t, es)\n\t\t\t\tgot := IsPrefixScan(err)\n\t\t\t\tassert.Equal(t, false, got)\n\t\t\t}\n\t\t\t{\n\t\t\t\ttx, err = db.Begin(false)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tprefix := []byte(\"foo_\")\n\t\t\t\tes, err := tx.PrefixScan(bucket, prefix, 0, 10)\n\t\t\t\tassert.NoError(t, tx.Commit())\n\t\t\t\tassert.Empty(t, es)\n\t\t\t\tgot := IsPrefixScan(err)\n\t\t\t\tassert.Equal(t, true, got)\n\t\t\t}\n\t\t})\n\t})\n}\n\nfunc TestIsPrefixSearchScan(t *testing.T) {\n\tregs := \"(.+)\"\n\tbucket := \"test_prefix_search_scan\"\n\tt.Run(\"if prefix and search scanning not found the result return true\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\t{\n\t\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\t\ttx, err := db.Begin(true)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tfor i := 0; i <= 10; i++ {\n\t\t\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\t\tval := []byte(\"val\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\t\t\tassert.NoError(t, err)\n\t\t\t\t}\n\t\t\t\tassert.NoError(t, tx.Commit())\n\t\t\t}\n\t\t\t{\n\t\t\t\ttx, err = db.Begin(false)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tprefix := []byte(\"key_\")\n\t\t\t\tes, err := tx.PrefixSearchScan(bucket, prefix, regs, 0, 10)\n\t\t\t\tassert.NoError(t, tx.Commit())\n\t\t\t\tassert.NotEmpty(t, es)\n\t\t\t\tgot := IsPrefixSearchScan(err)\n\t\t\t\tassert.Equal(t, false, got)\n\t\t\t}\n\t\t\t{\n\t\t\t\ttx, err = db.Begin(false)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tprefix := []byte(\"foo_\")\n\t\t\t\tes, err := tx.PrefixSearchScan(bucket, prefix, regs, 0, 10)\n\t\t\t\tassert.NoError(t, tx.Commit())\n\t\t\t\tassert.Empty(t, es)\n\t\t\t\tgot := IsPrefixSearchScan(err)\n\t\t\t\tassert.Equal(t, true, got)\n\t\t\t}\n\t\t})\n\t})\n}\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "fd_manager.go",
          "type": "blob",
          "size": 4.9228515625,
          "content": "package nutsdb\n\nimport (\n\t\"math\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"sync\"\n)\n\nconst (\n\tDefaultMaxFileNums = 256\n)\n\nconst (\n\tTooManyFileOpenErrSuffix = \"too many open files\"\n)\n\n// fdManager hold a fd cache in memory, it lru based cache.\ntype fdManager struct {\n\tlock               sync.Mutex\n\tcache              map[string]*FdInfo\n\tfdList             *doubleLinkedList\n\tsize               int\n\tcleanThresholdNums int\n\tmaxFdNums          int\n}\n\n// newFdm will return a fdManager object\nfunc newFdm(maxFdNums int, cleanThreshold float64) (fdm *fdManager) {\n\tfdm = &fdManager{\n\t\tcache:     map[string]*FdInfo{},\n\t\tfdList:    initDoubleLinkedList(),\n\t\tsize:      0,\n\t\tmaxFdNums: DefaultMaxFileNums,\n\t}\n\tfdm.cleanThresholdNums = int(math.Floor(0.5 * float64(fdm.maxFdNums)))\n\tif maxFdNums > 0 {\n\t\tfdm.maxFdNums = maxFdNums\n\t}\n\n\tif cleanThreshold > 0.0 && cleanThreshold < 1.0 {\n\t\tfdm.cleanThresholdNums = int(math.Floor(cleanThreshold * float64(fdm.maxFdNums)))\n\t}\n\treturn fdm\n}\n\n// FdInfo holds base fd info\ntype FdInfo struct {\n\tfd    *os.File\n\tpath  string\n\tusing uint\n\tnext  *FdInfo\n\tprev  *FdInfo\n}\n\n// getFd go through this method to get fd.\nfunc (fdm *fdManager) getFd(path string) (fd *os.File, err error) {\n\tfdm.lock.Lock()\n\tdefer fdm.lock.Unlock()\n\tcleanPath := filepath.Clean(path)\n\tif fdInfo := fdm.cache[cleanPath]; fdInfo == nil {\n\t\tfd, err = os.OpenFile(cleanPath, os.O_CREATE|os.O_RDWR, 0o644)\n\t\tif err == nil {\n\t\t\t// if the numbers of fd in cache larger than the cleanThreshold in config, we will clean useless fd in cache\n\t\t\tif fdm.size >= fdm.cleanThresholdNums {\n\t\t\t\terr = fdm.cleanUselessFd()\n\t\t\t}\n\t\t\t// if the numbers of fd in cache larger than the max numbers of fd in config, we will not add this fd to cache\n\t\t\tif fdm.size >= fdm.maxFdNums {\n\t\t\t\treturn fd, nil\n\t\t\t}\n\t\t\t// add this fd to cache\n\t\t\tfdm.addToCache(fd, cleanPath)\n\t\t\treturn fd, nil\n\t\t} else {\n\t\t\t// determine if there are too many open files, we will first clean useless fd in cache and try open this file again\n\t\t\tif strings.HasSuffix(err.Error(), TooManyFileOpenErrSuffix) {\n\t\t\t\tcleanErr := fdm.cleanUselessFd()\n\t\t\t\t// if something wrong in cleanUselessFd, we will return \"open too many files\" err, because we want user not the main err is that\n\t\t\t\tif cleanErr != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\t// try open this file again，if it still returns err, we will show this error to user\n\t\t\t\tfd, err = os.OpenFile(cleanPath, os.O_CREATE|os.O_RDWR, 0o644)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\t// add to cache if open this file successfully\n\t\t\t\tfdm.addToCache(fd, cleanPath)\n\t\t\t}\n\t\t\treturn fd, err\n\t\t}\n\t} else {\n\t\tfdInfo.using++\n\t\tfdm.fdList.moveNodeToFront(fdInfo)\n\t\treturn fdInfo.fd, nil\n\t}\n}\n\n// addToCache add fd to cache\nfunc (fdm *fdManager) addToCache(fd *os.File, cleanPath string) {\n\tfdInfo := &FdInfo{\n\t\tfd:    fd,\n\t\tusing: 1,\n\t\tpath:  cleanPath,\n\t}\n\tfdm.fdList.addNode(fdInfo)\n\tfdm.size++\n\tfdm.cache[cleanPath] = fdInfo\n}\n\n// reduceUsing when RWManager object close, it will go through this method let fdm know it return the fd to cache\nfunc (fdm *fdManager) reduceUsing(path string) {\n\tfdm.lock.Lock()\n\tdefer fdm.lock.Unlock()\n\tcleanPath := filepath.Clean(path)\n\tnode, isExist := fdm.cache[cleanPath]\n\tif !isExist {\n\t\tpanic(\"unexpected the node is not in cache\")\n\t}\n\tnode.using--\n}\n\n// close means the cache.\nfunc (fdm *fdManager) close() error {\n\tfdm.lock.Lock()\n\tdefer fdm.lock.Unlock()\n\tnode := fdm.fdList.tail.prev\n\tfor node != fdm.fdList.head {\n\t\terr := node.fd.Close()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdelete(fdm.cache, node.path)\n\t\tfdm.size--\n\t\tnode = node.prev\n\t}\n\tfdm.fdList.head.next = fdm.fdList.tail\n\tfdm.fdList.tail.prev = fdm.fdList.head\n\treturn nil\n}\n\ntype doubleLinkedList struct {\n\thead *FdInfo\n\ttail *FdInfo\n\tsize int\n}\n\nfunc initDoubleLinkedList() *doubleLinkedList {\n\tlist := &doubleLinkedList{\n\t\thead: &FdInfo{},\n\t\ttail: &FdInfo{},\n\t\tsize: 0,\n\t}\n\tlist.head.next = list.tail\n\tlist.tail.prev = list.head\n\treturn list\n}\n\nfunc (list *doubleLinkedList) addNode(node *FdInfo) {\n\tlist.head.next.prev = node\n\tnode.next = list.head.next\n\tlist.head.next = node\n\tnode.prev = list.head\n\tlist.size++\n}\n\nfunc (list *doubleLinkedList) removeNode(node *FdInfo) {\n\tnode.prev.next = node.next\n\tnode.next.prev = node.prev\n\tnode.prev = nil\n\tnode.next = nil\n}\n\nfunc (list *doubleLinkedList) moveNodeToFront(node *FdInfo) {\n\tlist.removeNode(node)\n\tlist.addNode(node)\n}\n\nfunc (fdm *fdManager) cleanUselessFd() error {\n\tcleanNums := fdm.cleanThresholdNums\n\tnode := fdm.fdList.tail.prev\n\tfor node != nil && node != fdm.fdList.head && cleanNums > 0 {\n\t\tnextItem := node.prev\n\t\tif node.using == 0 {\n\t\t\tfdm.fdList.removeNode(node)\n\t\t\terr := node.fd.Close()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tfdm.size--\n\t\t\tdelete(fdm.cache, node.path)\n\t\t\tcleanNums--\n\t\t}\n\t\tnode = nextItem\n\t}\n\treturn nil\n}\n\nfunc (fdm *fdManager) closeByPath(path string) error {\n\tfdm.lock.Lock()\n\tdefer fdm.lock.Unlock()\n\tfdInfo, ok := fdm.cache[path]\n\tif !ok {\n\t\treturn nil\n\t}\n\tdelete(fdm.cache, path)\n\n\tfdm.fdList.removeNode(fdInfo)\n\treturn fdInfo.fd.Close()\n}\n"
        },
        {
          "name": "fd_manager_test.go",
          "type": "blob",
          "size": 7.6181640625,
          "content": "package nutsdb\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestFdManager_All(t *testing.T) {\n\tdir := \"test-data\"\n\ttestBasePath := dir + \"/data-\"\n\terr := os.Mkdir(dir, os.ModePerm)\n\tassert.Nil(t, err)\n\tdefer os.RemoveAll(dir)\n\n\tstartFdNums := 1\n\tmaxFdNums := 20\n\tcreateFileLimit := 11\n\tcleanThreshold := 0.5\n\n\tdefer func() {\n\t\tif panicErr := recover(); panicErr != nil {\n\t\t\tt.Logf(\"panic is %s\", panicErr)\n\t\t}\n\t\terr := os.RemoveAll(testBasePath)\n\t\tassert.Nil(t, err)\n\t}()\n\n\tvar fdm *fdManager\n\tt.Run(\"test init fdm\", func(t *testing.T) {\n\t\tfdm = newFdm(maxFdNums, cleanThreshold)\n\t\tassert.NotNil(t, fdm)\n\t\tassert.Equal(t, maxFdNums, fdm.maxFdNums)\n\t\tassert.Equal(t, int(math.Floor(cleanThreshold*float64(fdm.maxFdNums))), fdm.cleanThresholdNums)\n\t})\n\n\tt.Run(\"create fd to cache\", func(t *testing.T) {\n\t\tfor i := startFdNums; i < createFileLimit; i++ {\n\t\t\tpath := testBasePath + fmt.Sprint(i)\n\t\t\tfd, err := fdm.getFd(path)\n\t\t\tassert.Nil(t, err)\n\t\t\tassert.NotNil(t, fd)\n\t\t}\n\t\tpositiveFdsSeq := []int{10, 9, 8, 7, 6, 5, 4, 3, 2, 1}\n\n\t\tassertChainFromTailAndHead(t, fdm, testBasePath, positiveFdsSeq)\n\t})\n\n\tt.Run(\"test get fd in cache\", func(t *testing.T) {\n\t\tt.Run(\"test get head item in cache\", func(t *testing.T) {\n\t\t\tfd, err := fdm.getFd(fdm.fdList.head.next.path)\n\t\t\tassert.Nil(t, err)\n\t\t\tassert.NotNil(t, fd)\n\t\t\tassert.Equal(t, fdm.fdList.head.next.fd, fd)\n\t\t\tpositiveFdsSeq := []int{10, 9, 8, 7, 6, 5, 4, 3, 2, 1}\n\t\t\tassertChainFromTailAndHead(t, fdm, testBasePath, positiveFdsSeq)\n\t\t})\n\n\t\tt.Run(\"test get tail item in cache\", func(t *testing.T) {\n\t\t\tfd, err := fdm.getFd(fdm.fdList.tail.prev.path)\n\t\t\tassert.Nil(t, err)\n\t\t\tassert.NotNil(t, fd)\n\t\t\tassert.Equal(t, fdm.fdList.head.next.fd, fd)\n\t\t\tpositiveFdsSeq := []int{1, 10, 9, 8, 7, 6, 5, 4, 3, 2}\n\t\t\tassertChainFromTailAndHead(t, fdm, testBasePath, positiveFdsSeq)\n\t\t})\n\n\t\tt.Run(\"test get middle item in cache\", func(t *testing.T) {\n\t\t\tpath := testBasePath + fmt.Sprint(5)\n\t\t\tfd, err := fdm.getFd(path)\n\t\t\tassert.Nil(t, err)\n\t\t\tassert.NotNil(t, fd)\n\t\t\tassert.Equal(t, fdm.fdList.head.next.fd, fd)\n\t\t\tpositiveFdsSeq := []int{5, 1, 10, 9, 8, 7, 6, 4, 3, 2}\n\t\t\tassertChainFromTailAndHead(t, fdm, testBasePath, positiveFdsSeq)\n\t\t})\n\t})\n\n\tt.Run(\"test reduce using\", func(t *testing.T) {\n\t\tpath := testBasePath + fmt.Sprint(5)\n\t\t_, err := fdm.getFd(path)\n\t\tassert.Nil(t, err)\n\t\tusing := fdm.fdList.head.next.using\n\t\t_, err = fdm.getFd(path)\n\t\tassert.Nil(t, err)\n\t\tassert.Equal(t, using+1, fdm.fdList.head.next.using)\n\t\tfdm.reduceUsing(path)\n\t\tassert.Nil(t, err)\n\t\tassert.Equal(t, using, fdm.fdList.head.next.using)\n\t})\n\n\tt.Run(\"test clean fd in cache\", func(t *testing.T) {\n\t\tpreReducePath := []int{2, 3, 4, 6, 7, 8}\n\t\tfor _, pathNum := range preReducePath {\n\t\t\tpath := testBasePath + fmt.Sprint(pathNum)\n\t\t\tfdm.reduceUsing(path)\n\t\t\tassert.Nil(t, err)\n\t\t}\n\t\tpath := testBasePath + fmt.Sprint(11)\n\t\tfd, err := fdm.getFd(path)\n\t\tassert.Nil(t, err)\n\t\tassert.NotNil(t, fd)\n\t\tpositiveFdsSeq := []int{11, 5, 1, 10, 9}\n\t\tassertChainFromTailAndHead(t, fdm, testBasePath, positiveFdsSeq)\n\t})\n\n\tt.Run(\"test close fdm\", func(t *testing.T) {\n\t\terr := fdm.close()\n\t\tif err != nil {\n\t\t\tt.Logf(\"err during close is:%s\", err)\n\t\t}\n\t\tassert.Nil(t, err)\n\t\tassert.Equal(t, 0, len(fdm.cache))\n\t\tassert.Equal(t, 0, fdm.size)\n\t})\n}\n\nfunc TestDoubleLinkedList_All(t *testing.T) {\n\tlist := initDoubleLinkedList()\n\tnodeMap := make(map[int]*FdInfo)\n\tt.Run(\"test add node\", func(t *testing.T) {\n\t\tfor i := 1; i <= 10; i++ {\n\t\t\tfd := &FdInfo{\n\t\t\t\tpath: fmt.Sprint(i),\n\t\t\t}\n\t\t\tlist.addNode(fd)\n\t\t\tnodeMap[i] = fd\n\t\t}\n\t\tassert.Equal(t, `[10 9 8 7 6 5 4 3 2 1 ]`, fmt.Sprintf(\"%+v\", getAllNodePathFromHead(list)))\n\t\tassert.Equal(t, `[1 2 3 4 5 6 7 8 9 10 ]`, fmt.Sprintf(\"%+v\", getAllNodePathFromTail(list)))\n\t})\n\n\tt.Run(\"test remove node\", func(t *testing.T) {\n\t\tt.Run(\"test remove first node\", func(t *testing.T) {\n\t\t\tlist.removeNode(nodeMap[10])\n\t\t\tassert.Equal(t, \"[9 8 7 6 5 4 3 2 1 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromHead(list)))\n\t\t\tassert.Equal(t, \"[1 2 3 4 5 6 7 8 9 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromTail(list)))\n\t\t})\n\t\tt.Run(\"test remove last node\", func(t *testing.T) {\n\t\t\tlist.removeNode(nodeMap[1])\n\t\t\tassert.Equal(t, \"[9 8 7 6 5 4 3 2 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromHead(list)))\n\t\t\tassert.Equal(t, \"[2 3 4 5 6 7 8 9 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromTail(list)))\n\t\t})\n\t\tt.Run(\"test remove middle node\", func(t *testing.T) {\n\t\t\tlist.removeNode(nodeMap[5])\n\t\t\tassert.Equal(t, \"[9 8 7 6 4 3 2 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromHead(list)))\n\t\t\tassert.Equal(t, \"[2 3 4 6 7 8 9 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromTail(list)))\n\t\t})\n\t})\n\n\tt.Run(\"test move node to head\", func(t *testing.T) {\n\t\tt.Run(\"test move first node\", func(t *testing.T) {\n\t\t\tlist.moveNodeToFront(nodeMap[9])\n\t\t\tassert.Equal(t, \"[9 8 7 6 4 3 2 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromHead(list)))\n\t\t\tassert.Equal(t, \"[2 3 4 6 7 8 9 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromTail(list)))\n\t\t})\n\t\tt.Run(\"test move last node\", func(t *testing.T) {\n\t\t\tlist.moveNodeToFront(nodeMap[2])\n\t\t\tassert.Equal(t, \"[2 9 8 7 6 4 3 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromHead(list)))\n\t\t\tassert.Equal(t, \"[3 4 6 7 8 9 2 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromTail(list)))\n\t\t})\n\t\tt.Run(\"test move middle node\", func(t *testing.T) {\n\t\t\tlist.moveNodeToFront(nodeMap[6])\n\t\t\tassert.Equal(t, \"[6 2 9 8 7 4 3 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromHead(list)))\n\t\t\tassert.Equal(t, \"[3 4 7 8 9 2 6 ]\", fmt.Sprintf(\"%+v\", getAllNodePathFromTail(list)))\n\t\t})\n\t})\n}\n\nfunc getAllNodePathFromHead(list *doubleLinkedList) (res []string) {\n\tnode := list.head.next\n\tfor node != nil {\n\t\tres = append(res, node.path)\n\t\tnode = node.next\n\t}\n\treturn res\n}\n\nfunc getAllNodePathFromTail(list *doubleLinkedList) (res []string) {\n\tnode := list.tail.prev\n\tfor node != nil {\n\t\tres = append(res, node.path)\n\t\tnode = node.prev\n\t}\n\treturn res\n}\n\nfunc assertChainFromTailAndHead(t *testing.T, fdm *fdManager, testBasePath string, positiveFdsSeq []int) {\n\tassertChainFromHead(t, fdm, testBasePath, positiveFdsSeq)\n\tassertChainFromTail(t, fdm, testBasePath, positiveFdsSeq)\n}\n\nfunc assertChainFromHead(t *testing.T, fdm *fdManager, testBasePath string, positiveFdsSeq []int) {\n\tnode := fdm.fdList.head.next\n\tindex := 0\n\tnums := 0\n\tfor node != fdm.fdList.tail {\n\t\texpectedPath := testBasePath + fmt.Sprint(positiveFdsSeq[index])\n\t\tassert.NotNil(t, node.fd)\n\t\tassert.Equal(t, expectedPath, node.path)\n\t\tnode = node.next\n\t\tindex++\n\t\tnums++\n\t}\n\tassert.Equal(t, fdm.size, nums)\n}\n\nfunc assertChainFromTail(t *testing.T, fdm *fdManager, testBasePath string, positiveFdsSeq []int) {\n\tindex := len(positiveFdsSeq) - 1\n\tnode := fdm.fdList.tail.prev\n\tnums := 0\n\tfor node != fdm.fdList.head {\n\t\texpectedPath := testBasePath + fmt.Sprint(positiveFdsSeq[index])\n\t\tassert.NotNil(t, node.fd)\n\t\tassert.Equal(t, expectedPath, node.path)\n\t\tnode = node.prev\n\t\tindex--\n\t\tnums++\n\t}\n\tassert.Equal(t, fdm.size, nums)\n}\n\n//func TestGetMaxNums(t *testing.T) {\n//\tmaxNums := 30000\n//\tbasePath := \"test-path/\"\n//\terr := os.RemoveAll(basePath)\n//\tassert.Nil(t, err)\n//\terr = os.Mkdir(basePath, os.ModePerm)\n//\tassert.Nil(t, err)\n//\tdefer func() {\n//\t\terr := os.RemoveAll(basePath)\n//\t\tif err != nil {\n//\t\t\tt.Logf(\"err is %s\", err)\n//\t\t}\n//\t}()\n//\tvar fdList []*os.File\n//\tfor i := 1; i <= maxNums; i++ {\n//\t\tpath := basePath + fmt.Sprintf(\"%d\", i)\n//\t\tfd, err := os.OpenFile(path, os.O_CREATE|os.O_RDWR, 0644)\n//\t\tif err != nil {\n//\t\t\tif strings.HasSuffix(err.Error(), TooManyFileOpenErrSuffix) {\n//\t\t\t\tt.Logf(\"file num is %d, err is %s, and it had handle\", i, err)\n//\t\t\t}\n//\t\t\tfor _, fd := range fdList {\n//\t\t\t\terr := fd.Close()\n//\t\t\t\tif err != nil {\n//\t\t\t\t\tt.Logf(\"err is %s, and it had handle\", err)\n//\t\t\t\t}\n//\t\t\t}\n//\t\t\treturn\n//\t\t} else {\n//\t\t\tfdList = append(fdList, fd)\n//\t\t}\n//\t}\n//}\n"
        },
        {
          "name": "file_manager.go",
          "type": "blob",
          "size": 2.0712890625,
          "content": "package nutsdb\n\nimport (\n\t\"github.com/xujiajun/mmap-go\"\n)\n\n// fileManager holds the fd cache and file-related operations go through the manager to obtain the file processing object\ntype fileManager struct {\n\trwMode      RWMode\n\tfdm         *fdManager\n\tsegmentSize int64\n}\n\n// newFileManager will create a newFileManager object\nfunc newFileManager(rwMode RWMode, maxFdNums int, cleanThreshold float64, segmentSize int64) (fm *fileManager) {\n\tfm = &fileManager{\n\t\trwMode:      rwMode,\n\t\tfdm:         newFdm(maxFdNums, cleanThreshold),\n\t\tsegmentSize: segmentSize,\n\t}\n\treturn fm\n}\n\n// getDataFile will return a DataFile Object\nfunc (fm *fileManager) getDataFile(path string, capacity int64) (datafile *DataFile, err error) {\n\tif capacity <= 0 {\n\t\treturn nil, ErrCapacity\n\t}\n\n\tvar rwManager RWManager\n\n\tif fm.rwMode == FileIO {\n\t\trwManager, err = fm.getFileRWManager(path, capacity, fm.segmentSize)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tif fm.rwMode == MMap {\n\t\trwManager, err = fm.getMMapRWManager(path, capacity, fm.segmentSize)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn NewDataFile(path, rwManager), nil\n}\n\n// getFileRWManager will return a FileIORWManager Object\nfunc (fm *fileManager) getFileRWManager(path string, capacity int64, segmentSize int64) (*FileIORWManager, error) {\n\tfd, err := fm.fdm.getFd(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\terr = Truncate(path, capacity, fd)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &FileIORWManager{fd: fd, path: path, fdm: fm.fdm, segmentSize: segmentSize}, nil\n}\n\n// getMMapRWManager will return a MMapRWManager Object\nfunc (fm *fileManager) getMMapRWManager(path string, capacity int64, segmentSize int64) (*MMapRWManager, error) {\n\tfd, err := fm.fdm.getFd(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\terr = Truncate(path, capacity, fd)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tm, err := mmap.Map(fd, mmap.RDWR, 0)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &MMapRWManager{m: m, path: path, fdm: fm.fdm, segmentSize: segmentSize}, nil\n}\n\n// close will close fdm resource\nfunc (fm *fileManager) close() error {\n\terr := fm.fdm.close()\n\treturn err\n}\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 0.6044921875,
          "content": "module github.com/nutsdb/nutsdb\n\ngo 1.18\n\nrequire (\n\tgithub.com/antlabs/timer v0.0.11\n\tgithub.com/bwmarrin/snowflake v0.3.0\n\tgithub.com/gofrs/flock v0.8.1\n\tgithub.com/pkg/errors v0.9.1\n\tgithub.com/stretchr/testify v1.7.1\n\tgithub.com/tidwall/btree v1.6.0\n\tgithub.com/xujiajun/gorouter v1.2.0\n\tgithub.com/xujiajun/mmap-go v1.0.1\n\tgithub.com/xujiajun/utils v0.0.0-20220904132955-5f7c5b914235\n)\n\nrequire (\n\tgithub.com/antlabs/stl v0.0.1 // indirect\n\tgithub.com/davecgh/go-spew v1.1.1 // indirect\n\tgithub.com/pmezard/go-difflib v1.0.0 // indirect\n\tgolang.org/x/sys v0.15.0 // indirect\n\tgopkg.in/yaml.v3 v3.0.1 // indirect\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 3.1005859375,
          "content": "github.com/antlabs/stl v0.0.1 h1:TRD3csCrjREeLhLoQ/supaoCvFhNLBTNIwuRGrDIs6Q=\ngithub.com/antlabs/stl v0.0.1/go.mod h1:wvVwP1loadLG3cRjxUxK8RL4Co5xujGaZlhbztmUEqQ=\ngithub.com/antlabs/timer v0.0.11 h1:z75oGFLeTqJHMOcWzUPBKsBbQAz4Ske3AfqJ7bsdcwU=\ngithub.com/antlabs/timer v0.0.11/go.mod h1:JNV8J3yGvMKhCavGXgj9HXrVZkfdQyKCcqXBT8RdyuU=\ngithub.com/bwmarrin/snowflake v0.3.0 h1:xm67bEhkKh6ij1790JB83OujPR5CzNe8QuQqAgISZN0=\ngithub.com/bwmarrin/snowflake v0.3.0/go.mod h1:NdZxfVWX+oR6y2K0o6qAYv6gIOP9rjG0/E9WsDpxqwE=\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/gofrs/flock v0.8.1 h1:+gYjHKf32LDeiEEFhQaotPbLuUXjY5ZqxKgXy7n59aw=\ngithub.com/gofrs/flock v0.8.1/go.mod h1:F1TvTiK9OcQqauNUHlbJvyl9Qa1QvF/gOUDKA14jxHU=\ngithub.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=\ngithub.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/testify v1.6.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.7.1 h1:5TQK59W5E3v0r2duFAb7P95B6hEeOyEnHRa8MjYSMTY=\ngithub.com/stretchr/testify v1.7.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/tidwall/btree v1.6.0 h1:LDZfKfQIBHGHWSwckhXI0RPSXzlo+KYdjK7FWSqOzzg=\ngithub.com/tidwall/btree v1.6.0/go.mod h1:twD9XRA5jj9VUQGELzDO4HPQTNJsoWWfYEL+EUQ2cKY=\ngithub.com/xujiajun/gorouter v1.2.0 h1:aPKfkzLHxPYRgr+irEE00SEOf78LHnxH/v4m8QiV51Y=\ngithub.com/xujiajun/gorouter v1.2.0/go.mod h1:yJrIta+bTNpBM/2UT8hLOaEAFckO+m/qmR3luMIQygM=\ngithub.com/xujiajun/mmap-go v1.0.1 h1:7Se7ss1fLPPRW+ePgqGpCkfGIZzJV6JPq9Wq9iv/WHc=\ngithub.com/xujiajun/mmap-go v1.0.1/go.mod h1:CNN6Sw4SL69Sui00p0zEzcZKbt+5HtEnYUsc6BKKRMg=\ngithub.com/xujiajun/utils v0.0.0-20220904132955-5f7c5b914235 h1:w0si+uee0iAaCJO9q86T6yrhdadgcsoNuh47LrUykzg=\ngithub.com/xujiajun/utils v0.0.0-20220904132955-5f7c5b914235/go.mod h1:MR4+0R6A9NS5IABnIM3384FfOq8QFVnm7WDrBOhIaMU=\ngolang.org/x/sys v0.0.0-20181221143128-b4a75ba826a6/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.15.0 h1:h48lPFYpsTvQJZF4EKyI4aLHaev3CxivZmv7yZig9pc=\ngolang.org/x/sys v0.15.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405 h1:yhCVgyC4o1eVCa2tZl7eS0r+SDo693bJlVdllGtEeKM=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/go-playground/assert.v1 v1.2.1 h1:xoYuJVE7KT85PYWrN730RguIQO0ePzVRfFMXadIrXTM=\ngopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\n"
        },
        {
          "name": "index.go",
          "type": "blob",
          "size": 2.3837890625,
          "content": "// Copyright 2022 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\ntype IdxType interface {\n\tBTree | Set | SortedSet | List\n}\n\ntype defaultOp[T IdxType] struct {\n\tidx map[BucketId]*T\n}\n\nfunc (op *defaultOp[T]) computeIfAbsent(id BucketId, f func() *T) *T {\n\tif i, isExist := op.idx[id]; isExist {\n\t\treturn i\n\t}\n\ti := f()\n\top.idx[id] = i\n\treturn i\n}\n\nfunc (op *defaultOp[T]) delete(id BucketId) {\n\tdelete(op.idx, id)\n}\n\nfunc (op *defaultOp[T]) exist(id BucketId) (*T, bool) {\n\ti, isExist := op.idx[id]\n\treturn i, isExist\n}\n\nfunc (op *defaultOp[T]) getIdxLen() int {\n\treturn len(op.idx)\n}\n\nfunc (op *defaultOp[T]) rangeIdx(f func(elem *T)) {\n\tfor _, t := range op.idx {\n\t\tf(t)\n\t}\n}\n\ntype ListIdx struct {\n\t*defaultOp[List]\n}\n\nfunc (idx ListIdx) getWithDefault(id BucketId) *List {\n\treturn idx.defaultOp.computeIfAbsent(id, func() *List {\n\t\treturn NewList()\n\t})\n}\n\ntype BTreeIdx struct {\n\t*defaultOp[BTree]\n}\n\nfunc (idx BTreeIdx) getWithDefault(id BucketId) *BTree {\n\treturn idx.defaultOp.computeIfAbsent(id, func() *BTree {\n\t\treturn NewBTree()\n\t})\n}\n\ntype SetIdx struct {\n\t*defaultOp[Set]\n}\n\nfunc (idx SetIdx) getWithDefault(id BucketId) *Set {\n\treturn idx.defaultOp.computeIfAbsent(id, func() *Set {\n\t\treturn NewSet()\n\t})\n}\n\ntype SortedSetIdx struct {\n\t*defaultOp[SortedSet]\n}\n\nfunc (idx SortedSetIdx) getWithDefault(id BucketId, db *DB) *SortedSet {\n\treturn idx.defaultOp.computeIfAbsent(id, func() *SortedSet {\n\t\treturn NewSortedSet(db)\n\t})\n}\n\ntype index struct {\n\tlist      ListIdx\n\tbTree     BTreeIdx\n\tset       SetIdx\n\tsortedSet SortedSetIdx\n}\n\nfunc newIndex() *index {\n\ti := new(index)\n\ti.list = ListIdx{&defaultOp[List]{idx: map[BucketId]*List{}}}\n\ti.bTree = BTreeIdx{&defaultOp[BTree]{idx: map[BucketId]*BTree{}}}\n\ti.set = SetIdx{&defaultOp[Set]{idx: map[BucketId]*Set{}}}\n\ti.sortedSet = SortedSetIdx{&defaultOp[SortedSet]{idx: map[BucketId]*SortedSet{}}}\n\treturn i\n}\n"
        },
        {
          "name": "iterator.go",
          "type": "blob",
          "size": 1.740234375,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"github.com/tidwall/btree\"\n)\n\ntype Iterator struct {\n\ttx      *Tx\n\toptions IteratorOptions\n\titer    btree.IterG[*Item]\n}\n\ntype IteratorOptions struct {\n\tReverse bool\n}\n\nfunc NewIterator(tx *Tx, bucket string, options IteratorOptions) *Iterator {\n\tb, err := tx.db.bm.GetBucket(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn nil\n\t}\n\titerator := &Iterator{\n\t\ttx:      tx,\n\t\toptions: options,\n\t\titer:    tx.db.Index.bTree.getWithDefault(b.Id).btree.Iter(),\n\t}\n\n\tif options.Reverse {\n\t\titerator.iter.Last()\n\t} else {\n\t\titerator.iter.First()\n\t}\n\n\treturn iterator\n}\n\nfunc (it *Iterator) Rewind() bool {\n\tif it.options.Reverse {\n\t\treturn it.iter.Last()\n\t} else {\n\t\treturn it.iter.First()\n\t}\n}\n\nfunc (it *Iterator) Seek(key []byte) bool {\n\treturn it.iter.Seek(&Item{key: key})\n}\n\nfunc (it *Iterator) Next() bool {\n\tif it.options.Reverse {\n\t\treturn it.iter.Prev()\n\t} else {\n\t\treturn it.iter.Next()\n\t}\n}\n\nfunc (it *Iterator) Valid() bool {\n\treturn it.iter.Item() != nil\n}\n\nfunc (it *Iterator) Key() []byte {\n\treturn it.iter.Item().key\n}\n\nfunc (it *Iterator) Value() ([]byte, error) {\n\treturn it.tx.db.getValueByRecord(it.iter.Item().record)\n}\n"
        },
        {
          "name": "iterator_test.go",
          "type": "blob",
          "size": 2.3994140625,
          "content": "// Copyright 2022 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"github.com/stretchr/testify/require\"\n\t\"testing\"\n)\n\nfunc TestIterator(t *testing.T) {\n\tbucket := \"bucket\"\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\tfor i := 0; i < 100; i++ {\n\t\t\ttxPut(t, db, bucket, GetTestBytes(i), GetTestBytes(i), Persistent, nil, nil)\n\t\t}\n\n\t\t_ = db.View(func(tx *Tx) error {\n\t\t\titerator := NewIterator(tx, bucket, IteratorOptions{Reverse: false})\n\n\t\t\ti := 0\n\n\t\t\tfor {\n\t\t\t\tvalue, err := iterator.Value()\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\trequire.Equal(t, GetTestBytes(i), value)\n\t\t\t\tif !iterator.Next() {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\ti++\n\t\t\t}\n\n\t\t\treturn nil\n\t\t})\n\t})\n}\n\nfunc TestIterator_Reverse(t *testing.T) {\n\tbucket := \"bucket\"\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\tfor i := 0; i < 100; i++ {\n\t\t\ttxPut(t, db, bucket, GetTestBytes(i), GetTestBytes(i), Persistent, nil, nil)\n\t\t}\n\n\t\t_ = db.View(func(tx *Tx) error {\n\t\t\titerator := NewIterator(tx, bucket, IteratorOptions{Reverse: true})\n\n\t\t\ti := 99\n\t\t\tfor {\n\t\t\t\tvalue, err := iterator.Value()\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\trequire.Equal(t, GetTestBytes(i), value)\n\t\t\t\tif !iterator.Next() {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\ti--\n\t\t\t}\n\n\t\t\treturn nil\n\t\t})\n\t})\n}\n\nfunc TestIterator_Seek(t *testing.T) {\n\tbucket := \"bucket\"\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\tfor i := 0; i < 100; i++ {\n\t\t\ttxPut(t, db, bucket, GetTestBytes(i), GetTestBytes(i), Persistent, nil, nil)\n\t\t}\n\n\t\t_ = db.View(func(tx *Tx) error {\n\t\t\titerator := NewIterator(tx, bucket, IteratorOptions{Reverse: true})\n\n\t\t\titerator.Seek(GetTestBytes(40))\n\n\t\t\tvalue, err := iterator.Value()\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Equal(t, GetTestBytes(40), value)\n\n\t\t\treturn nil\n\t\t})\n\t})\n}\n"
        },
        {
          "name": "list.go",
          "type": "blob",
          "size": 8.041015625,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"errors\"\n\t\"math\"\n\t\"time\"\n)\n\nvar (\n\t// ErrListNotFound is returned when the list not found.\n\tErrListNotFound = errors.New(\"the list not found\")\n\n\t// ErrCount is returned when count is error.\n\tErrCount = errors.New(\"err count\")\n\n\t// ErrEmptyList is returned when the list is empty.\n\tErrEmptyList = errors.New(\"the list is empty\")\n\n\t// ErrStartOrEnd is returned when start > end\n\tErrStartOrEnd = errors.New(\"start or end error\")\n)\n\nconst (\n\tinitialListSeq = math.MaxUint64 / 2\n)\n\n// BTree represents the btree.\n\n// HeadTailSeq list head and tail seq num\ntype HeadTailSeq struct {\n\tHead uint64\n\tTail uint64\n}\n\n// List represents the list.\ntype List struct {\n\tItems     map[string]*BTree\n\tTTL       map[string]uint32\n\tTimeStamp map[string]uint64\n\tSeq       map[string]*HeadTailSeq\n}\n\nfunc NewList() *List {\n\treturn &List{\n\t\tItems:     make(map[string]*BTree),\n\t\tTTL:       make(map[string]uint32),\n\t\tTimeStamp: make(map[string]uint64),\n\t\tSeq:       make(map[string]*HeadTailSeq),\n\t}\n}\n\nfunc (l *List) LPush(key string, r *Record) error {\n\treturn l.push(key, r, true)\n}\n\nfunc (l *List) RPush(key string, r *Record) error {\n\treturn l.push(key, r, false)\n}\n\nfunc (l *List) push(key string, r *Record, isLeft bool) error {\n\t// key is seq + user_key\n\tuserKey, curSeq := decodeListKey([]byte(key))\n\tuserKeyStr := string(userKey)\n\tif l.IsExpire(userKeyStr) {\n\t\treturn ErrListNotFound\n\t}\n\n\tlist, ok := l.Items[userKeyStr]\n\tif !ok {\n\t\tl.Items[userKeyStr] = NewBTree()\n\t\tlist = l.Items[userKeyStr]\n\t}\n\n\tseq, ok := l.Seq[userKeyStr]\n\tif !ok {\n\t\tl.Seq[userKeyStr] = &HeadTailSeq{Head: initialListSeq, Tail: initialListSeq + 1}\n\t\tseq = l.Seq[userKeyStr]\n\t}\n\n\tlist.InsertRecord(ConvertUint64ToBigEndianBytes(curSeq), r)\n\tif isLeft {\n\t\tif seq.Head > curSeq-1 {\n\t\t\tseq.Head = curSeq - 1\n\t\t}\n\t} else {\n\t\tif seq.Tail < curSeq+1 {\n\t\t\tseq.Tail = curSeq + 1\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (l *List) LPop(key string) (*Record, error) {\n\titem, err := l.LPeek(key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tl.Items[key].Delete(item.key)\n\tl.Seq[key].Head = ConvertBigEndianBytesToUint64(item.key)\n\treturn item.record, nil\n}\n\n// RPop removes and returns the last element of the list stored at key.\nfunc (l *List) RPop(key string) (*Record, error) {\n\titem, err := l.RPeek(key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tl.Items[key].Delete(item.key)\n\tl.Seq[key].Tail = ConvertBigEndianBytesToUint64(item.key)\n\treturn item.record, nil\n}\n\nfunc (l *List) LPeek(key string) (*Item, error) {\n\treturn l.peek(key, true)\n}\n\nfunc (l *List) RPeek(key string) (*Item, error) {\n\treturn l.peek(key, false)\n}\n\nfunc (l *List) peek(key string, isLeft bool) (*Item, error) {\n\tif l.IsExpire(key) {\n\t\treturn nil, ErrListNotFound\n\t}\n\tlist, ok := l.Items[key]\n\tif !ok {\n\t\treturn nil, ErrListNotFound\n\t}\n\n\tif isLeft {\n\t\titem, ok := list.Min()\n\t\tif ok {\n\t\t\treturn item, nil\n\t\t}\n\t} else {\n\t\titem, ok := list.Max()\n\t\tif ok {\n\t\t\treturn item, nil\n\t\t}\n\t}\n\n\treturn nil, ErrEmptyList\n}\n\n// LRange returns the specified elements of the list stored at key [start,end]\nfunc (l *List) LRange(key string, start, end int) ([]*Record, error) {\n\tsize, err := l.Size(key)\n\tif err != nil || size == 0 {\n\t\treturn nil, err\n\t}\n\n\tstart, end, err = checkBounds(start, end, size)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar res []*Record\n\tallRecords := l.Items[key].All()\n\tfor i, item := range allRecords {\n\t\tif i >= start && i <= end {\n\t\t\tres = append(res, item)\n\t\t}\n\t}\n\n\treturn res, nil\n}\n\n// getRemoveIndexes returns a slice of indices to be removed from the list based on the count\nfunc (l *List) getRemoveIndexes(key string, count int, cmp func(r *Record) (bool, error)) ([][]byte, error) {\n\tif l.IsExpire(key) {\n\t\treturn nil, ErrListNotFound\n\t}\n\n\tlist, ok := l.Items[key]\n\n\tif !ok {\n\t\treturn nil, ErrListNotFound\n\t}\n\n\tvar res [][]byte\n\tvar allItems []*Item\n\tif 0 == count {\n\t\tcount = list.Count()\n\t}\n\n\tallItems = l.Items[key].AllItems()\n\tif count > 0 {\n\t\tfor _, item := range allItems {\n\t\t\tif count <= 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tr := item.record\n\t\t\tok, err := cmp(r)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tif ok {\n\t\t\t\tres = append(res, item.key)\n\t\t\t\tcount--\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfor i := len(allItems) - 1; i >= 0; i-- {\n\t\t\tif count >= 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tr := allItems[i].record\n\t\t\tok, err := cmp(r)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tif ok {\n\t\t\t\tres = append(res, allItems[i].key)\n\t\t\t\tcount++\n\t\t\t}\n\t\t}\n\t}\n\n\treturn res, nil\n}\n\n// LRem removes the first count occurrences of elements equal to value from the list stored at key.\n// The count argument influences the operation in the following ways:\n// count > 0: Remove elements equal to value moving from head to tail.\n// count < 0: Remove elements equal to value moving from tail to head.\n// count = 0: Remove all elements equal to value.\nfunc (l *List) LRem(key string, count int, cmp func(r *Record) (bool, error)) error {\n\tremoveIndexes, err := l.getRemoveIndexes(key, count, cmp)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tlist := l.Items[key]\n\tfor _, idx := range removeIndexes {\n\t\tlist.Delete(idx)\n\t}\n\n\treturn nil\n}\n\n// LTrim trim an existing list so that it will contain only the specified range of elements specified.\nfunc (l *List) LTrim(key string, start, end int) error {\n\tif l.IsExpire(key) {\n\t\treturn ErrListNotFound\n\t}\n\tif _, ok := l.Items[key]; !ok {\n\t\treturn ErrListNotFound\n\t}\n\n\tlist := l.Items[key]\n\tallItems := list.AllItems()\n\tfor i, item := range allItems {\n\t\tif i < start || i > end {\n\t\t\tlist.Delete(item.key)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// LRemByIndex remove the list element at specified index\nfunc (l *List) LRemByIndex(key string, indexes []int) error {\n\tif l.IsExpire(key) {\n\t\treturn ErrListNotFound\n\t}\n\n\tidxes := l.getValidIndexes(key, indexes)\n\tif len(idxes) == 0 {\n\t\treturn nil\n\t}\n\n\tlist := l.Items[key]\n\tallItems := list.AllItems()\n\tfor i, item := range allItems {\n\t\tif _, ok := idxes[i]; ok {\n\t\t\tlist.Delete(item.key)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (l *List) getValidIndexes(key string, indexes []int) map[int]struct{} {\n\tidxes := make(map[int]struct{})\n\tlistLen, err := l.Size(key)\n\tif err != nil || 0 == listLen {\n\t\treturn idxes\n\t}\n\n\tfor _, idx := range indexes {\n\t\tif idx < 0 || idx >= listLen {\n\t\t\tcontinue\n\t\t}\n\t\tidxes[idx] = struct{}{}\n\t}\n\n\treturn idxes\n}\n\nfunc (l *List) IsExpire(key string) bool {\n\tif l == nil {\n\t\treturn false\n\t}\n\n\t_, ok := l.TTL[key]\n\tif !ok {\n\t\treturn false\n\t}\n\n\tnow := time.Now().Unix()\n\ttimestamp := l.TimeStamp[key]\n\tif l.TTL[key] > 0 && uint64(l.TTL[key])+timestamp > uint64(now) || l.TTL[key] == uint32(0) {\n\t\treturn false\n\t}\n\n\tdelete(l.Items, key)\n\tdelete(l.TTL, key)\n\tdelete(l.TimeStamp, key)\n\tdelete(l.Seq, key)\n\n\treturn true\n}\n\nfunc (l *List) Size(key string) (int, error) {\n\tif l.IsExpire(key) {\n\t\treturn 0, ErrListNotFound\n\t}\n\tif _, ok := l.Items[key]; !ok {\n\t\treturn 0, ErrListNotFound\n\t}\n\n\treturn l.Items[key].Count(), nil\n}\n\nfunc (l *List) IsEmpty(key string) (bool, error) {\n\tsize, err := l.Size(key)\n\tif err != nil || size > 0 {\n\t\treturn false, err\n\t}\n\treturn true, nil\n}\n\nfunc (l *List) GetListTTL(key string) (uint32, error) {\n\tif l.IsExpire(key) {\n\t\treturn 0, ErrListNotFound\n\t}\n\n\tttl := l.TTL[key]\n\ttimestamp := l.TimeStamp[key]\n\tif ttl == 0 || timestamp == 0 {\n\t\treturn 0, nil\n\t}\n\n\tnow := time.Now().Unix()\n\tremain := timestamp + uint64(ttl) - uint64(now)\n\n\treturn uint32(remain), nil\n}\n\nfunc checkBounds(start, end int, size int) (int, int, error) {\n\tif start >= 0 && end < 0 {\n\t\tend = size + end\n\t}\n\n\tif start < 0 && end > 0 {\n\t\tstart = size + start\n\t}\n\n\tif start < 0 && end < 0 {\n\t\tstart, end = size+start, size+end\n\t}\n\n\tif end >= size {\n\t\tend = size - 1\n\t}\n\n\tif start > end {\n\t\treturn 0, 0, ErrStartOrEnd\n\t}\n\n\treturn start, end, nil\n}\n"
        },
        {
          "name": "list_test.go",
          "type": "blob",
          "size": 7.01171875,
          "content": "// Copyright 2023 The PromiseDB Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file expect in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"bytes\"\n\t\"math/rand\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc ListPush(t *testing.T, list *List, key string, r *Record, isLeft bool, expectError error) {\n\tvar e error\n\tif isLeft {\n\t\te = list.LPush(key, r)\n\t} else {\n\t\te = list.RPush(key, r)\n\t}\n\tassertErr(t, e, expectError)\n}\n\nfunc ListPop(t *testing.T, list *List, key string, isLeft bool, expectVal *Record, expectError error) {\n\tvar (\n\t\te error\n\t\tr *Record\n\t)\n\n\tif isLeft {\n\t\tr, e = list.LPop(key)\n\t} else {\n\t\tr, e = list.RPop(key)\n\t}\n\tif expectError != nil {\n\t\trequire.Equal(t, expectError, e)\n\t} else {\n\t\trequire.NoError(t, e)\n\t\trequire.Equal(t, expectVal, r)\n\t}\n}\n\nfunc ListCmp(t *testing.T, list *List, key string, expectRecords []*Record, isReverse bool) {\n\trecords, err := list.LRange(key, 0, -1)\n\trequire.NoError(t, err)\n\n\tif isReverse {\n\t\tfor i := len(expectRecords) - 1; i >= 0; i-- {\n\t\t\trequire.Equal(t, expectRecords[i], records[len(expectRecords)-1-i])\n\t\t}\n\t} else {\n\t\tfor i := 0; i < len(expectRecords); i++ {\n\t\t\trequire.Equal(t, expectRecords[i], records[i])\n\t\t}\n\t}\n}\n\nfunc TestList_LPush(t *testing.T) {\n\tlist := NewList()\n\t// 测试 LPush\n\tkey := string(GetTestBytes(0))\n\texpectRecords := generateRecords(5)\n\tseqInfo := HeadTailSeq{Head: initialListSeq, Tail: initialListSeq + 1}\n\n\tfor i := 0; i < len(expectRecords); i++ {\n\t\tseq := generateSeq(&seqInfo, true)\n\t\tnewKey := encodeListKey([]byte(key), seq)\n\t\texpectRecords[i].Key = newKey\n\t\tListPush(t, list, string(newKey), expectRecords[i], true, nil)\n\t}\n\n\tListCmp(t, list, key, expectRecords, true)\n}\n\nfunc TestList_RPush(t *testing.T) {\n\tlist := NewList()\n\texpectRecords := generateRecords(5)\n\tkey := string(GetTestBytes(0))\n\tseqInfo := HeadTailSeq{Head: initialListSeq, Tail: initialListSeq + 1}\n\n\tfor i := 0; i < len(expectRecords); i++ {\n\t\tseq := generateSeq(&seqInfo, false)\n\t\tnewKey := encodeListKey([]byte(key), seq)\n\t\texpectRecords[i].Key = newKey\n\t\tListPush(t, list, string(newKey), expectRecords[i], false, nil)\n\t}\n\n\tListCmp(t, list, key, expectRecords, false)\n}\n\nfunc TestList_Pop(t *testing.T) {\n\tlist := NewList()\n\texpectRecords := generateRecords(5)\n\tkey := string(GetTestBytes(0))\n\n\tListPop(t, list, key, true, nil, ErrListNotFound)\n\tseqInfo := HeadTailSeq{Head: initialListSeq, Tail: initialListSeq + 1}\n\n\tfor i := 0; i < len(expectRecords); i++ {\n\t\tseq := generateSeq(&seqInfo, false)\n\t\tnewKey := encodeListKey([]byte(key), seq)\n\t\texpectRecords[i].Key = newKey\n\t\tListPush(t, list, string(newKey), expectRecords[i], false, nil)\n\t}\n\n\tListPop(t, list, key, true, expectRecords[0], nil)\n\texpectRecords = expectRecords[1:]\n\n\tListPop(t, list, key, false, expectRecords[len(expectRecords)-1], nil)\n\texpectRecords = expectRecords[:len(expectRecords)-1]\n\n\tListCmp(t, list, key, expectRecords, false)\n}\n\nfunc TestList_LRem(t *testing.T) {\n\tlist := NewList()\n\trecords := generateRecords(2)\n\tkey := string(GetTestBytes(0))\n\tseqInfo := HeadTailSeq{Head: initialListSeq, Tail: initialListSeq + 1}\n\n\tfor i := 0; i < 3; i++ {\n\t\tseq := generateSeq(&seqInfo, false)\n\t\tnewKey := encodeListKey([]byte(key), seq)\n\t\trecords[0].Key = newKey\n\t\tListPush(t, list, string(newKey), records[0], false, nil)\n\t}\n\n\tseq := generateSeq(&seqInfo, false)\n\tnewKey := encodeListKey([]byte(key), seq)\n\trecords[1].Key = newKey\n\tListPush(t, list, string(newKey), records[1], false, nil)\n\n\tseq = generateSeq(&seqInfo, false)\n\tnewKey = encodeListKey([]byte(key), seq)\n\trecords[0].Key = newKey\n\tListPush(t, list, string(newKey), records[0], false, nil)\n\n\tseq = generateSeq(&seqInfo, false)\n\tnewKey = encodeListKey([]byte(key), seq)\n\trecords[1].Key = newKey\n\tListPush(t, list, string(newKey), records[1], false, nil)\n\n\t// r1 r1 r1 r2 r1 r2\n\texpectRecords := []*Record{records[0], records[0], records[0], records[1], records[0], records[1]}\n\n\tcmp := func(r *Record) (bool, error) {\n\t\treturn bytes.Equal(r.Value, records[0].Value), nil\n\t}\n\n\t// r1 r1 r1 r2 r2\n\terr := list.LRem(key, -1, cmp)\n\trequire.NoError(t, err)\n\texpectRecords = append(expectRecords[0:4], expectRecords[5:]...)\n\tListCmp(t, list, key, expectRecords, false)\n\n\t// r1 r2 r2\n\terr = list.LRem(key, 2, cmp)\n\trequire.NoError(t, err)\n\texpectRecords = expectRecords[2:]\n\tListCmp(t, list, key, expectRecords, false)\n\n\tcmp = func(r *Record) (bool, error) {\n\t\treturn bytes.Equal(r.Value, records[1].Value), nil\n\t}\n\n\t// r1\n\terr = list.LRem(key, 0, cmp)\n\trequire.NoError(t, err)\n\texpectRecords = expectRecords[0:1]\n\tListCmp(t, list, key, expectRecords, false)\n}\n\nfunc TestList_LTrim(t *testing.T) {\n\tlist := NewList()\n\texpectRecords := generateRecords(5)\n\tkey := string(GetTestBytes(0))\n\tseqInfo := HeadTailSeq{Head: initialListSeq, Tail: initialListSeq + 1}\n\n\tfor i := 0; i < len(expectRecords); i++ {\n\t\tseq := generateSeq(&seqInfo, false)\n\t\tnewKey := encodeListKey([]byte(key), seq)\n\t\texpectRecords[i].Key = newKey\n\t\tListPush(t, list, string(newKey), expectRecords[i], false, nil)\n\t}\n\n\terr := list.LTrim(key, 1, 3)\n\trequire.NoError(t, err)\n\texpectRecords = expectRecords[1 : len(expectRecords)-1]\n\tListCmp(t, list, key, expectRecords, false)\n}\n\nfunc TestList_LRemByIndex(t *testing.T) {\n\tlist := NewList()\n\texpectRecords := generateRecords(8)\n\tkey := string(GetTestBytes(0))\n\tseqInfo := HeadTailSeq{Head: initialListSeq, Tail: initialListSeq + 1}\n\n\t// r1 r2 r3 r4 r5 r6 r7 r8\n\tfor i := 0; i < 8; i++ {\n\t\tseq := generateSeq(&seqInfo, false)\n\t\tnewKey := encodeListKey([]byte(key), seq)\n\t\texpectRecords[i].Key = newKey\n\t\tListPush(t, list, string(newKey), expectRecords[i], false, nil)\n\t}\n\n\t// r1 r2 r4 r5 r6 r7 r8\n\terr := list.LRemByIndex(key, []int{2})\n\trequire.NoError(t, err)\n\texpectRecords = append(expectRecords[0:2], expectRecords[3:]...)\n\tListCmp(t, list, key, expectRecords, false)\n\n\t// r2 r6 r7 r8\n\terr = list.LRemByIndex(key, []int{0, 2, 3})\n\trequire.NoError(t, err)\n\texpectRecords = expectRecords[1:]\n\texpectRecords = append(expectRecords[0:1], expectRecords[3:]...)\n\tListCmp(t, list, key, expectRecords, false)\n\n\terr = list.LRemByIndex(key, []int{0, 0, 0})\n\trequire.NoError(t, err)\n\texpectRecords = expectRecords[1:]\n\tListCmp(t, list, key, expectRecords, false)\n}\n\nfunc generateRecords(count int) []*Record {\n\trand.Seed(time.Now().UnixNano())\n\trecords := make([]*Record, count)\n\tfor i := 0; i < count; i++ {\n\t\tkey := GetTestBytes(i)\n\t\tval := GetRandomBytes(24)\n\n\t\trecord := &Record{\n\t\t\tKey:       key,\n\t\t\tValue:     val,\n\t\t\tFileID:    int64(i),\n\t\t\tDataPos:   uint64(rand.Uint32()),\n\t\t\tValueSize: uint32(len(val)),\n\t\t\tTimestamp: uint64(time.Now().Unix()),\n\t\t\tTTL:       uint32(rand.Intn(3600)),\n\t\t\tTxID:      uint64(rand.Intn(1000)),\n\t\t}\n\t\trecords[i] = record\n\t}\n\treturn records\n}\n"
        },
        {
          "name": "lru.go",
          "type": "blob",
          "size": 1.876953125,
          "content": "package nutsdb\n\nimport (\n\t\"container/list\"\n\t\"sync\"\n)\n\n// LRUCache is a least recently used (LRU) cache.\ntype LRUCache struct {\n\tm   map[interface{}]*list.Element\n\tl   *list.List\n\tcap int\n\tmu  *sync.RWMutex\n}\n\n// New creates a new LRUCache with the specified capacity.\nfunc NewLruCache(cap int) *LRUCache {\n\treturn &LRUCache{\n\t\tm:   make(map[interface{}]*list.Element),\n\t\tl:   list.New(),\n\t\tcap: cap,\n\t\tmu:  &sync.RWMutex{},\n\t}\n}\n\n// Add adds a new entry to the cache.\nfunc (c *LRUCache) Add(key interface{}, value interface{}) {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tif c.cap <= 0 {\n\t\treturn\n\t}\n\n\tif c.l.Len() >= c.cap {\n\t\tc.removeOldest()\n\t}\n\n\te := &LruEntry{\n\t\tKey:   key,\n\t\tValue: value,\n\t}\n\tentry := c.l.PushFront(e)\n\n\tc.m[key] = entry\n}\n\n// Get returns the entry associated with the given key, or nil if the key is not in the cache.\nfunc (c *LRUCache) Get(key interface{}) interface{} {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tentry, ok := c.m[key]\n\tif !ok {\n\t\treturn nil\n\t}\n\n\tc.l.MoveToFront(entry)\n\treturn entry.Value.(*LruEntry).Value\n}\n\n// Remove removes the entry associated with the given key from the cache.\nfunc (c *LRUCache) Remove(key interface{}) {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tentry, ok := c.m[key]\n\tif !ok {\n\t\treturn\n\t}\n\n\tc.l.Remove(entry)\n\tdelete(c.m, key)\n}\n\n// Len returns the number of entries in the cache.\nfunc (c *LRUCache) Len() int {\n\tc.mu.RLock()\n\tdefer c.mu.RUnlock()\n\n\treturn c.l.Len()\n}\n\n// Clear clears the cache.\nfunc (c *LRUCache) Clear() {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tc.l.Init()\n\tc.m = make(map[interface{}]*list.Element)\n}\n\n// removeOldest removes the oldest entry from the cache.\nfunc (c *LRUCache) removeOldest() {\n\tentry := c.l.Back()\n\tif entry == nil {\n\t\treturn\n\t}\n\n\tkey := entry.Value.(*LruEntry).Key\n\tdelete(c.m, key)\n\n\tc.l.Remove(entry)\n}\n\n// LruEntry is a struct that represents an entry in the LRU cache.\ntype LruEntry struct {\n\tKey   interface{}\n\tValue interface{}\n}\n"
        },
        {
          "name": "lru_test.go",
          "type": "blob",
          "size": 1.576171875,
          "content": "package nutsdb\n\nimport (\n\t\"testing\"\n)\n\nfunc TestLRUCache(t *testing.T) {\n\tcache := NewLruCache(3)\n\n\t// Add some entries to the cache\n\tcache.Add(1, \"one\")\n\tcache.Add(2, \"two\")\n\tcache.Add(3, \"three\")\n\n\t// Check if cache length is correct\n\tif cache.Len() != 3 {\n\t\tt.Errorf(\"Expected cache length to be 3, got %d\", cache.Len())\n\t}\n\n\t// Test getting values from cache\n\tval := cache.Get(1)\n\tif val != \"one\" {\n\t\tt.Errorf(\"Expected value for key 1 to be 'one', got %v\", val)\n\t}\n\n\t// Test LRU behavior: recently used item should be retained in cache, least recently used should be evicted\n\tcache.Add(4, \"four\")\n\tval = cache.Get(2) // Should return nil as key 2 is the least recently used item\n\tif val != nil {\n\t\tt.Errorf(\"Expected value for key 2 to be nil, got %v\", val)\n\t}\n\n\t// Test removing an entry from cache\n\tcache.Remove(3)\n\tif cache.Len() != 2 {\n\t\tt.Errorf(\"Expected cache length after removal to be 2, got %d\", cache.Len())\n\t}\n\n\t// Test clearing the cache\n\tcache.Clear()\n\tif cache.Len() != 0 {\n\t\tt.Errorf(\"Expected cache length after clearing to be 0, got %d\", cache.Len())\n\t}\n}\n\nfunc TestLRUCache_RemoveOldest(t *testing.T) {\n\tcache := NewLruCache(2)\n\n\t// Add two entries to the cache\n\tcache.Add(1, \"one\")\n\tcache.Add(2, \"two\")\n\n\t// Remove the oldest entry\n\tcache.removeOldest()\n\n\t// Check if cache length is correct\n\tif cache.Len() != 1 {\n\t\tt.Errorf(\"Expected cache length after removing oldest to be 1, got %d\", cache.Len())\n\t}\n\n\t// Check if the oldest entry has been evicted properly\n\tval := cache.Get(1)\n\tif val != nil {\n\t\tt.Errorf(\"Expected value for key 1 to be nil after removing oldest, got %v\", val)\n\t}\n}\n"
        },
        {
          "name": "merge.go",
          "type": "blob",
          "size": 7.708984375,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"os\"\n\t\"time\"\n\n\t\"github.com/xujiajun/utils/strconv2\"\n)\n\nvar ErrDontNeedMerge = errors.New(\"the number of files waiting to be merged is at least 2\")\n\nfunc (db *DB) Merge() error {\n\tdb.mergeStartCh <- struct{}{}\n\treturn <-db.mergeEndCh\n}\n\n// Merge removes dirty data and reduce data redundancy,following these steps:\n//\n// 1. Filter delete or expired entry.\n//\n// 2. Write entry to activeFile if the key not exist，if exist miss this write operation.\n//\n// 3. Filter the entry which is committed.\n//\n// 4. At last remove the merged files.\n//\n// Caveat: merge is Called means starting multiple write transactions, and it\n// will affect the other write request. so execute it at the appropriate time.\nfunc (db *DB) merge() error {\n\tvar (\n\t\toff              int64\n\t\tpendingMergeFIds []int\n\t)\n\n\t// to prevent the initiation of multiple merges simultaneously.\n\tdb.mu.Lock()\n\n\tif db.isMerging {\n\t\tdb.mu.Unlock()\n\t\treturn ErrIsMerging\n\t}\n\n\tdb.isMerging = true\n\tdefer func() {\n\t\tdb.isMerging = false\n\t}()\n\n\t_, pendingMergeFIds = db.getMaxFileIDAndFileIDs()\n\tif len(pendingMergeFIds) < 2 {\n\t\tdb.mu.Unlock()\n\t\treturn ErrDontNeedMerge\n\t}\n\n\tdb.MaxFileID++\n\n\tif !db.opt.SyncEnable && db.opt.RWMode == MMap {\n\t\tif err := db.ActiveFile.rwManager.Sync(); err != nil {\n\t\t\tdb.mu.Unlock()\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif err := db.ActiveFile.rwManager.Release(); err != nil {\n\t\tdb.mu.Unlock()\n\t\treturn err\n\t}\n\n\tvar err error\n\tpath := getDataPath(db.MaxFileID, db.opt.Dir)\n\tdb.ActiveFile, err = db.fm.getDataFile(path, db.opt.SegmentSize)\n\tif err != nil {\n\t\tdb.mu.Unlock()\n\t\treturn err\n\t}\n\n\tdb.ActiveFile.fileID = db.MaxFileID\n\n\tdb.mu.Unlock()\n\n\tmergingPath := make([]string, len(pendingMergeFIds))\n\n\tfor i, pendingMergeFId := range pendingMergeFIds {\n\t\toff = 0\n\t\tpath := getDataPath(int64(pendingMergeFId), db.opt.Dir)\n\t\tfr, err := newFileRecovery(path, db.opt.BufferSizeOfRecovery)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tfor {\n\t\t\tif entry, err := fr.readEntry(off); err == nil {\n\t\t\t\tif entry == nil {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\tif entry.isFilter() {\n\t\t\t\t\toff += entry.Size()\n\t\t\t\t\tif off >= db.opt.SegmentSize {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t// Due to the lack of concurrency safety in the index,\n\t\t\t\t// there is a possibility that a race condition might occur when the merge goroutine reads the index,\n\t\t\t\t// while a transaction is being committed, causing modifications to the index.\n\t\t\t\t// To address this issue, we need to use a transaction to perform this operation.\n\t\t\t\terr := db.Update(func(tx *Tx) error {\n\t\t\t\t\t// check if we have a new entry with same key and bucket\n\t\t\t\t\tif ok := db.isPendingMergeEntry(entry); ok {\n\t\t\t\t\t\tbucket, err := db.bm.GetBucketById(entry.Meta.BucketId)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn err\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbucketName := bucket.Name\n\t\t\t\t\t\tif entry.Meta.Flag == DataLPushFlag {\n\t\t\t\t\t\t\treturn tx.LPushRaw(bucketName, entry.Key, entry.Value)\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif entry.Meta.Flag == DataRPushFlag {\n\t\t\t\t\t\t\treturn tx.RPushRaw(bucketName, entry.Key, entry.Value)\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\treturn tx.put(\n\t\t\t\t\t\t\tbucketName,\n\t\t\t\t\t\t\tentry.Key,\n\t\t\t\t\t\t\tentry.Value,\n\t\t\t\t\t\t\tentry.Meta.TTL,\n\t\t\t\t\t\t\tentry.Meta.Flag,\n\t\t\t\t\t\t\tentry.Meta.Timestamp,\n\t\t\t\t\t\t\tentry.Meta.Ds,\n\t\t\t\t\t\t)\n\t\t\t\t\t}\n\n\t\t\t\t\treturn nil\n\t\t\t\t})\n\t\t\t\tif err != nil {\n\t\t\t\t\t_ = fr.release()\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\toff += entry.Size()\n\t\t\t\tif off >= db.opt.SegmentSize {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t} else {\n\t\t\t\tif errors.Is(err, io.EOF) || errors.Is(err, ErrIndexOutOfBound) || errors.Is(err, io.ErrUnexpectedEOF) || errors.Is(err, ErrHeaderSizeOutOfBounds) {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\treturn fmt.Errorf(\"when merge operation build hintIndex readAt err: %s\", err)\n\t\t\t}\n\t\t}\n\n\t\terr = fr.release()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmergingPath[i] = path\n\t}\n\n\tdb.mu.Lock()\n\tdefer db.mu.Unlock()\n\n\tfor i := 0; i < len(mergingPath); i++ {\n\t\tif err := os.Remove(mergingPath[i]); err != nil {\n\t\t\treturn fmt.Errorf(\"when merge err: %s\", err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (db *DB) mergeWorker() {\n\tvar ticker *time.Ticker\n\n\tif db.opt.MergeInterval != 0 {\n\t\tticker = time.NewTicker(db.opt.MergeInterval)\n\t} else {\n\t\tticker = time.NewTicker(math.MaxInt)\n\t\tticker.Stop()\n\t}\n\n\tfor {\n\t\tselect {\n\t\tcase <-db.mergeStartCh:\n\t\t\tdb.mergeEndCh <- db.merge()\n\t\t\t// if automatic merging is enabled, then after a manual merge\n\t\t\t// the t needs to be reset.\n\t\t\tif db.opt.MergeInterval != 0 {\n\t\t\t\tticker.Reset(db.opt.MergeInterval)\n\t\t\t}\n\t\tcase <-ticker.C:\n\t\t\t_ = db.merge()\n\t\tcase <-db.mergeWorkCloseCh:\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (db *DB) isPendingMergeEntry(entry *Entry) bool {\n\tbucket, err := db.bm.GetBucketById(entry.Meta.BucketId)\n\tif err != nil {\n\t\treturn false\n\t}\n\tbucketId := bucket.Id\n\tswitch {\n\tcase entry.IsBelongsToBPlusTree():\n\t\treturn db.isPendingBtreeEntry(bucketId, entry)\n\tcase entry.IsBelongsToList():\n\t\treturn db.isPendingListEntry(bucketId, entry)\n\tcase entry.IsBelongsToSet():\n\t\treturn db.isPendingSetEntry(bucketId, entry)\n\tcase entry.IsBelongsToSortSet():\n\t\treturn db.isPendingZSetEntry(bucketId, entry)\n\t}\n\treturn false\n}\n\nfunc (db *DB) isPendingBtreeEntry(bucketId BucketId, entry *Entry) bool {\n\tidx, exist := db.Index.bTree.exist(bucketId)\n\tif !exist {\n\t\treturn false\n\t}\n\n\tr, ok := idx.Find(entry.Key)\n\tif !ok {\n\t\treturn false\n\t}\n\n\tif r.IsExpired() {\n\t\tdb.tm.del(bucketId, string(entry.Key))\n\t\tidx.Delete(entry.Key)\n\t\treturn false\n\t}\n\n\tif r.TxID != entry.Meta.TxID || r.Timestamp != entry.Meta.Timestamp {\n\t\treturn false\n\t}\n\n\treturn true\n}\n\nfunc (db *DB) isPendingSetEntry(bucketId BucketId, entry *Entry) bool {\n\tsetIdx, exist := db.Index.set.exist(bucketId)\n\tif !exist {\n\t\treturn false\n\t}\n\n\tisMember, err := setIdx.SIsMember(string(entry.Key), entry.Value)\n\tif err != nil || !isMember {\n\t\treturn false\n\t}\n\n\treturn true\n}\n\nfunc (db *DB) isPendingZSetEntry(bucketId BucketId, entry *Entry) bool {\n\tkey, score := splitStringFloat64Str(string(entry.Key), SeparatorForZSetKey)\n\tsortedSetIdx, exist := db.Index.sortedSet.exist(bucketId)\n\tif !exist {\n\t\treturn false\n\t}\n\ts, err := sortedSetIdx.ZScore(key, entry.Value)\n\tif err != nil || s != score {\n\t\treturn false\n\t}\n\n\treturn true\n}\n\nfunc (db *DB) isPendingListEntry(bucketId BucketId, entry *Entry) bool {\n\tvar userKeyStr string\n\tvar curSeq uint64\n\tvar userKey []byte\n\n\tif entry.Meta.Flag == DataExpireListFlag {\n\t\tuserKeyStr = string(entry.Key)\n\t\tlist, exist := db.Index.list.exist(bucketId)\n\t\tif !exist {\n\t\t\treturn false\n\t\t}\n\n\t\tif _, ok := list.Items[userKeyStr]; !ok {\n\t\t\treturn false\n\t\t}\n\n\t\tt, _ := strconv2.StrToInt64(string(entry.Value))\n\t\tttl := uint32(t)\n\t\tif _, ok := list.TTL[userKeyStr]; !ok {\n\t\t\treturn false\n\t\t}\n\n\t\tif list.TTL[userKeyStr] != ttl || list.TimeStamp[userKeyStr] != entry.Meta.Timestamp {\n\t\t\treturn false\n\t\t}\n\n\t\treturn true\n\t}\n\n\tif entry.Meta.Flag == DataLPushFlag || entry.Meta.Flag == DataRPushFlag {\n\t\tuserKey, curSeq = decodeListKey(entry.Key)\n\t\tuserKeyStr = string(userKey)\n\n\t\tlist, exist := db.Index.list.exist(bucketId)\n\t\tif !exist {\n\t\t\treturn false\n\t\t}\n\n\t\tif _, ok := list.Items[userKeyStr]; !ok {\n\t\t\treturn false\n\t\t}\n\n\t\tr, ok := list.Items[userKeyStr].Find(ConvertUint64ToBigEndianBytes(curSeq))\n\t\tif !ok {\n\t\t\treturn false\n\t\t}\n\n\t\tif !bytes.Equal(r.Key, entry.Key) || r.TxID != entry.Meta.TxID || r.Timestamp != entry.Meta.Timestamp {\n\t\t\treturn false\n\t\t}\n\n\t\treturn true\n\t}\n\n\treturn false\n}\n"
        },
        {
          "name": "merge_test.go",
          "type": "blob",
          "size": 9.8369140625,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"bytes\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n\t\"github.com/xujiajun/utils/strconv2\"\n)\n\nfunc TestDB_MergeForString(t *testing.T) {\n\tbucket := \"bucket\"\n\topts := DefaultOptions\n\topts.SegmentSize = KB\n\topts.Dir = \"/tmp/test-string-merge/\"\n\n\tfor _, idxMode := range []EntryIdxMode{HintKeyValAndRAMIdxMode, HintKeyAndRAMIdxMode} {\n\t\topts.EntryIdxMode = idxMode\n\t\tdb, err := Open(opts)\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\trequire.NoError(t, err)\n\n\t\t// Merge is not needed\n\t\terr = db.Merge()\n\t\trequire.Equal(t, ErrDontNeedMerge, err)\n\n\t\t// Add some data\n\t\tn := 1000\n\t\tfor i := 0; i < n; i++ {\n\t\t\ttxPut(t, db, bucket, GetTestBytes(i), GetTestBytes(i), Persistent, nil, nil)\n\t\t}\n\n\t\t// Delete some data\n\t\tfor i := 0; i < n/2; i++ {\n\t\t\ttxDel(t, db, bucket, GetTestBytes(i), nil)\n\t\t}\n\n\t\t// Merge and check the result\n\t\trequire.NoError(t, db.Merge())\n\n\t\tdbCnt, err := db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(n/2), dbCnt)\n\n\t\t// Check the deleted data is deleted\n\t\tfor i := 0; i < n/2; i++ {\n\t\t\ttxGet(t, db, bucket, GetTestBytes(i), GetTestBytes(i), ErrKeyNotFound)\n\t\t}\n\n\t\t// Check the added data is added\n\t\tfor i := n / 2; i < n; i++ {\n\t\t\ttxGet(t, db, bucket, GetTestBytes(i), GetTestBytes(i), nil)\n\t\t}\n\n\t\t// Close and reopen the db\n\t\trequire.NoError(t, db.Close())\n\n\t\tdb, err = Open(opts)\n\t\trequire.NoError(t, err)\n\n\t\tdbCnt, err = db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(n/2), dbCnt)\n\n\t\t// Check the deleted data is deleted\n\t\tfor i := 0; i < n/2; i++ {\n\t\t\ttxGet(t, db, bucket, GetTestBytes(i), GetTestBytes(i), ErrKeyNotFound)\n\t\t}\n\n\t\t// Check the added data is added\n\t\tfor i := n / 2; i < n; i++ {\n\t\t\ttxGet(t, db, bucket, GetTestBytes(i), GetTestBytes(i), nil)\n\t\t}\n\n\t\trequire.NoError(t, db.Close())\n\t\tremoveDir(opts.Dir)\n\t}\n}\n\nfunc TestDB_MergeForSet(t *testing.T) {\n\tbucket := \"bucket\"\n\topts := DefaultOptions\n\topts.SegmentSize = KB\n\topts.Dir = \"/tmp/test-set-merge/\"\n\n\tfor _, idxMode := range []EntryIdxMode{HintKeyValAndRAMIdxMode, HintKeyAndRAMIdxMode} {\n\t\topts.EntryIdxMode = idxMode\n\t\tdb, err := Open(opts)\n\t\tif exist := db.bm.ExistBucket(DataStructureSet, bucket); !exist {\n\t\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\t}\n\n\t\trequire.NoError(t, err)\n\n\t\t// Merge is not needed\n\t\terr = db.Merge()\n\t\trequire.Equal(t, ErrDontNeedMerge, err)\n\n\t\t// Add some data\n\t\tn := 1000\n\t\tkey := GetTestBytes(0)\n\t\tfor i := 0; i < n; i++ {\n\t\t\ttxSAdd(t, db, bucket, key, GetTestBytes(i), nil, nil)\n\t\t}\n\n\t\t// Delete some data\n\t\tfor i := 0; i < n/2; i++ {\n\t\t\ttxSRem(t, db, bucket, key, GetTestBytes(i), nil)\n\t\t}\n\n\t\t// Pop a random value\n\t\tvar spopValue []byte\n\t\terr = db.Update(func(tx *Tx) error {\n\t\t\tvar err error\n\t\t\tspopValue, err = tx.SPop(bucket, key)\n\t\t\tassertErr(t, err, nil)\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\t// Check the random value is popped\n\t\ttxSIsMember(t, db, bucket, key, spopValue, false)\n\n\t\t// txSPop(t, db, bucket, key,nil)\n\t\tdbCnt, err := db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(n/2-1), dbCnt)\n\n\t\t// Merge and check the result\n\t\trequire.NoError(t, db.Merge())\n\n\t\tdbCnt, err = db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(n/2-1), dbCnt)\n\n\t\t// Check the random value is popped\n\t\ttxSIsMember(t, db, bucket, key, spopValue, false)\n\t\tfor i := n / 2; i < n; i++ {\n\t\t\tv := GetTestBytes(i)\n\t\t\tif bytes.Equal(v, spopValue) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttxSIsMember(t, db, bucket, key, v, true)\n\t\t}\n\n\t\t// Close and reopen the db\n\t\trequire.NoError(t, db.Close())\n\n\t\t// reopen db\n\t\tdb, err = Open(opts)\n\t\trequire.NoError(t, err)\n\n\t\tdbCnt, err = db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(n/2-1), dbCnt)\n\n\t\t// Check the random value is popped\n\t\ttxSIsMember(t, db, bucket, key, spopValue, false)\n\t\tfor i := n / 2; i < n; i++ {\n\t\t\tv := GetTestBytes(i)\n\t\t\tif bytes.Equal(v, spopValue) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttxSIsMember(t, db, bucket, key, v, true)\n\t\t}\n\t\trequire.NoError(t, db.Close())\n\t\tremoveDir(opts.Dir)\n\t}\n}\n\n// TestDB_MergeForZSet is a test function to check the Merge() function of the DB struct\n// It creates a DB with two different EntryIdxMode, then adds and scores each item in the DB\n// It then removes half of the items from the DB, then checks that the items that are left are the same as the ones that were removed\n// It then closes the DB, reopens it, and checks that the items that were removed are now not present\nfunc TestDB_MergeForZSet(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\tn := 1000\n\topts := DefaultOptions\n\topts.SegmentSize = KB\n\topts.Dir = \"/tmp/test-zset-merge/\"\n\n\t// test different EntryIdxMode\n\tfor _, idxMode := range []EntryIdxMode{HintKeyValAndRAMIdxMode, HintKeyAndRAMIdxMode} {\n\n\t\topts.EntryIdxMode = idxMode\n\t\tdb, err := Open(opts)\n\t\tif exist := db.bm.ExistBucket(DataStructureSortedSet, bucket); !exist {\n\t\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\t\t}\n\t\trequire.NoError(t, err)\n\n\t\t// add items\n\t\terr = db.Merge()\n\t\trequire.Equal(t, ErrDontNeedMerge, err)\n\n\t\tfor i := 0; i < n; i++ {\n\t\t\tscore, _ := strconv2.IntToFloat64(i)\n\t\t\ttxZAdd(t, db, bucket, key, GetTestBytes(i), score, nil, nil)\n\t\t}\n\n\t\tfor i := 0; i < n; i++ {\n\t\t\tscore, _ := strconv2.IntToFloat64(i)\n\t\t\ttxZScore(t, db, bucket, key, GetTestBytes(i), score, nil)\n\t\t}\n\n\t\t// remove half of the items\n\t\tfor i := 0; i < n/2; i++ {\n\t\t\ttxZRem(t, db, bucket, key, GetTestBytes(i), nil)\n\t\t}\n\n\t\t// check that the items that are left are the same as the ones that were removed\n\t\tfor i := 0; i < n/2; i++ {\n\t\t\tscore, _ := strconv2.IntToFloat64(i)\n\t\t\ttxZScore(t, db, bucket, key, GetTestBytes(i), score, ErrSortedSetMemberNotExist)\n\t\t}\n\n\t\t// check that the items that are left are the same as the ones that were removed\n\t\tfor i := n / 2; i < n; i++ {\n\t\t\tscore, _ := strconv2.IntToFloat64(i)\n\t\t\ttxZScore(t, db, bucket, key, GetTestBytes(i), score, nil)\n\t\t}\n\n\t\t// check that the number of items in the DB is correct\n\t\tdbCnt, err := db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(n/2), dbCnt)\n\n\t\t// merge\n\t\trequire.NoError(t, db.Merge())\n\n\t\t// check that the number of items in the DB is correct\n\t\tdbCnt, err = db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(n/2), dbCnt)\n\n\t\t// check that the items that were removed are now not present\n\t\tfor i := 0; i < n/2; i++ {\n\t\t\tscore, _ := strconv2.IntToFloat64(i)\n\t\t\ttxZScore(t, db, bucket, key, GetTestBytes(i), score, ErrSortedSetMemberNotExist)\n\t\t}\n\n\t\t// check that the items that are left are the same as the ones that were removed\n\t\tfor i := n / 2; i < n; i++ {\n\t\t\tscore, _ := strconv2.IntToFloat64(i)\n\t\t\ttxZScore(t, db, bucket, key, GetTestBytes(i), score, nil)\n\t\t}\n\n\t\t// close db\n\t\trequire.NoError(t, db.Close())\n\n\t\t// reopen db\n\t\tdb, err = Open(opts)\n\t\trequire.NoError(t, err)\n\t\tdbCnt, err = db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(n/2), dbCnt)\n\n\t\t// check that the items that were removed are now not present\n\t\tfor i := 0; i < n/2; i++ {\n\t\t\tscore, _ := strconv2.IntToFloat64(i)\n\t\t\ttxZScore(t, db, bucket, key, GetTestBytes(i), score, ErrSortedSetMemberNotExist)\n\t\t}\n\n\t\t// check that the items that are left are the same as the ones that were removed\n\t\tfor i := n / 2; i < n; i++ {\n\t\t\tscore, _ := strconv2.IntToFloat64(i)\n\t\t\ttxZScore(t, db, bucket, key, GetTestBytes(i), score, nil)\n\t\t}\n\n\t\trequire.NoError(t, db.Close())\n\t\tremoveDir(opts.Dir)\n\t}\n}\n\n// TestDB_MergeForList tests the Merge() function of the DB struct.\n// It creates a DB with two different EntryIdxMode, pushes and pops data, and then merges the DB.\n// It then reopens the DB and checks that the data is still there.\nfunc TestDB_MergeForList(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\topts := DefaultOptions\n\topts.SegmentSize = KB\n\topts.Dir = \"/tmp/test-list-merge/\"\n\n\t// test different EntryIdxMode\n\tfor _, idxMode := range []EntryIdxMode{HintKeyValAndRAMIdxMode, HintKeyAndRAMIdxMode} {\n\t\topts.EntryIdxMode = idxMode\n\t\tdb, err := Open(opts)\n\t\tif exist := db.bm.ExistBucket(DataStructureList, bucket); !exist {\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\t}\n\n\t\trequire.NoError(t, err)\n\n\t\t// check that we don't need merge\n\t\terr = db.Merge()\n\t\trequire.Equal(t, ErrDontNeedMerge, err)\n\n\t\t// push data\n\t\tn := 1000\n\t\tfor i := 0; i < n; i++ {\n\t\t\ttxPush(t, db, bucket, key, GetTestBytes(i), true, nil, nil)\n\t\t}\n\n\t\tfor i := n; i < 2*n; i++ {\n\t\t\ttxPush(t, db, bucket, key, GetTestBytes(i), false, nil, nil)\n\t\t}\n\n\t\t// pop data\n\t\tfor i := n - 1; i >= n/2; i-- {\n\t\t\ttxPop(t, db, bucket, key, GetTestBytes(i), nil, true)\n\t\t}\n\n\t\tfor i := 2*n - 1; i >= 3*n/2; i-- {\n\t\t\ttxPop(t, db, bucket, key, GetTestBytes(i), nil, false)\n\t\t}\n\n\t\t// trim and remove data\n\t\ttxLTrim(t, db, bucket, key, 0, 9, nil)\n\t\ttxLRem(t, db, bucket, key, 0, GetTestBytes(100), nil)\n\t\ttxLRemByIndex(t, db, bucket, key, nil, []int{7, 8, 9}...)\n\n\t\tdbCnt, err := db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(7), dbCnt)\n\n\t\t// merge\n\t\trequire.NoError(t, db.Merge())\n\n\t\tdbCnt, err = db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(7), dbCnt)\n\n\t\trequire.NoError(t, db.Close())\n\n\t\t// reopen db\n\t\tdb, err = Open(opts)\n\t\trequire.NoError(t, err)\n\n\t\tdbCnt, err = db.getRecordCount()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(7), dbCnt)\n\n\t\t// pop data\n\t\tfor i := n/2 - 1; i < n/2-8; i-- {\n\t\t\ttxPop(t, db, bucket, key, GetTestBytes(i), nil, true)\n\t\t}\n\n\t\trequire.NoError(t, db.Close())\n\t\tremoveDir(opts.Dir)\n\t}\n}\n"
        },
        {
          "name": "metadata.go",
          "type": "blob",
          "size": 4.828125,
          "content": "package nutsdb\n\n// DataStructure represents the data structure we have already supported\ntype DataStructure = uint16\n\n// DataFlag means the data operations have done by users.\ntype DataFlag = uint16\n\n// DataStatus means the status of data\ntype DataStatus = uint16\n\nconst (\n\t// DataStructureSet represents the data structure set flag\n\tDataStructureSet DataStructure = 0\n\n\t// DataStructureSortedSet represents the data structure sorted set flag\n\tDataStructureSortedSet DataStructure = 1\n\n\t// DataStructureBTree represents the data structure b tree flag\n\tDataStructureBTree DataStructure = 2\n\n\t// DataStructureList represents the data structure list flag\n\tDataStructureList DataStructure = 3\n)\n\nconst (\n\t// DataDeleteFlag represents the data delete flag\n\tDataDeleteFlag DataFlag = 0\n\n\t// DataSetFlag represents the data set flag\n\tDataSetFlag DataFlag = 1\n\n\t// DataLPushFlag represents the data LPush flag\n\tDataLPushFlag DataFlag = 2\n\n\t// DataRPushFlag represents the data RPush flag\n\tDataRPushFlag DataFlag = 3\n\n\t// DataLRemFlag represents the data LRem flag\n\tDataLRemFlag DataFlag = 4\n\n\t// DataLPopFlag represents the data LPop flag\n\tDataLPopFlag DataFlag = 5\n\n\t// DataRPopFlag represents the data RPop flag\n\tDataRPopFlag DataFlag = 6\n\n\t// DataLTrimFlag represents the data LTrim flag\n\tDataLTrimFlag DataFlag = 8\n\n\t// DataZAddFlag represents the data ZAdd flag\n\tDataZAddFlag DataFlag = 9\n\n\t// DataZRemFlag represents the data ZRem flag\n\tDataZRemFlag DataFlag = 10\n\n\t// DataZRemRangeByRankFlag represents the data ZRemRangeByRank flag\n\tDataZRemRangeByRankFlag DataFlag = 11\n\n\t// DataZPopMaxFlag represents the data ZPopMax flag\n\tDataZPopMaxFlag DataFlag = 12\n\n\t// DataZPopMinFlag represents the data aZPopMin flag\n\tDataZPopMinFlag DataFlag = 13\n\n\t// DataSetBucketDeleteFlag represents the delete Set bucket flag\n\tDataSetBucketDeleteFlag DataFlag = 14\n\n\t// DataSortedSetBucketDeleteFlag represents the delete Sorted Set bucket flag\n\tDataSortedSetBucketDeleteFlag DataFlag = 15\n\n\t// DataBTreeBucketDeleteFlag represents the delete BTree bucket flag\n\tDataBTreeBucketDeleteFlag DataFlag = 16\n\n\t// DataListBucketDeleteFlag represents the delete List bucket flag\n\tDataListBucketDeleteFlag DataFlag = 17\n\n\t// DataLRemByIndex represents the data LRemByIndex flag\n\tDataLRemByIndex DataFlag = 18\n\n\t// DataExpireListFlag represents that set ttl for the list\n\tDataExpireListFlag DataFlag = 19\n)\n\nconst (\n\t// UnCommitted represents the tx unCommitted status\n\tUnCommitted uint16 = 0\n\n\t// Committed represents the tx committed status\n\tCommitted uint16 = 1\n)\n\n// Persistent represents the data persistent flag\nconst Persistent uint32 = 0\n\ntype MetaData struct {\n\tKeySize    uint32\n\tValueSize  uint32\n\tTimestamp  uint64\n\tTTL        uint32\n\tFlag       DataFlag // delete / set\n\tBucketSize uint32\n\tTxID       uint64\n\tStatus     DataStatus    // committed / uncommitted\n\tDs         DataStructure // data structure\n\tCrc        uint32\n\tBucketId   BucketId\n}\n\nfunc (meta *MetaData) Size() int64 {\n\t// CRC\n\tsize := 4\n\n\tsize += UvarintSize(uint64(meta.KeySize))\n\tsize += UvarintSize(uint64(meta.ValueSize))\n\tsize += UvarintSize(meta.Timestamp)\n\tsize += UvarintSize(uint64(meta.TTL))\n\tsize += UvarintSize(uint64(meta.Flag))\n\tsize += UvarintSize(meta.TxID)\n\tsize += UvarintSize(uint64(meta.Status))\n\tsize += UvarintSize(uint64(meta.Ds))\n\tsize += UvarintSize(meta.BucketId)\n\n\treturn int64(size)\n}\n\nfunc (meta *MetaData) PayloadSize() int64 {\n\treturn int64(meta.BucketSize) + int64(meta.KeySize) + int64(meta.ValueSize)\n}\n\nfunc NewMetaData() *MetaData {\n\treturn new(MetaData)\n}\n\nfunc (meta *MetaData) WithKeySize(keySize uint32) *MetaData {\n\tmeta.KeySize = keySize\n\treturn meta\n}\n\nfunc (meta *MetaData) WithValueSize(valueSize uint32) *MetaData {\n\tmeta.ValueSize = valueSize\n\treturn meta\n}\n\nfunc (meta *MetaData) WithTimeStamp(timestamp uint64) *MetaData {\n\tmeta.Timestamp = timestamp\n\treturn meta\n}\n\nfunc (meta *MetaData) WithTTL(ttl uint32) *MetaData {\n\tmeta.TTL = ttl\n\treturn meta\n}\n\nfunc (meta *MetaData) WithFlag(flag uint16) *MetaData {\n\tmeta.Flag = flag\n\treturn meta\n}\n\nfunc (meta *MetaData) WithBucketSize(bucketSize uint32) *MetaData {\n\tmeta.BucketSize = bucketSize\n\treturn meta\n}\n\nfunc (meta *MetaData) WithTxID(txID uint64) *MetaData {\n\tmeta.TxID = txID\n\treturn meta\n}\n\nfunc (meta *MetaData) WithStatus(status uint16) *MetaData {\n\tmeta.Status = status\n\treturn meta\n}\n\nfunc (meta *MetaData) WithDs(ds uint16) *MetaData {\n\tmeta.Ds = ds\n\treturn meta\n}\n\nfunc (meta *MetaData) WithCrc(crc uint32) *MetaData {\n\tmeta.Crc = crc\n\treturn meta\n}\n\nfunc (meta *MetaData) WithBucketId(bucketID uint64) *MetaData {\n\tmeta.BucketId = bucketID\n\treturn meta\n}\n\nfunc (meta *MetaData) IsBPlusTree() bool {\n\treturn meta.Ds == DataStructureBTree\n}\n\nfunc (meta *MetaData) IsSet() bool {\n\treturn meta.Ds == DataStructureSet\n}\n\nfunc (meta *MetaData) IsSortSet() bool {\n\treturn meta.Ds == DataStructureSortedSet\n}\n\nfunc (meta *MetaData) IsList() bool {\n\treturn meta.Ds == DataStructureList\n}\n"
        },
        {
          "name": "options.go",
          "type": "blob",
          "size": 6.34765625,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport \"time\"\n\n// EntryIdxMode represents entry index mode.\ntype EntryIdxMode int\n\nconst (\n\t// HintKeyValAndRAMIdxMode represents ram index (key and value) mode.\n\tHintKeyValAndRAMIdxMode EntryIdxMode = iota\n\n\t// HintKeyAndRAMIdxMode represents ram index (only key) mode.\n\tHintKeyAndRAMIdxMode\n)\n\ntype ExpiredDeleteType uint8\n\nconst (\n\t// TimeWheel represents use time wheel to do expired deletion\n\tTimeWheel ExpiredDeleteType = iota\n\n\t// TimeHeap represents use time heap to do expired deletion\n\tTimeHeap\n)\n\n// An ErrorHandler handles an error occurred during transaction.\ntype ErrorHandler interface {\n\tHandleError(err error)\n}\n\n// The ErrorHandlerFunc type is an adapter to ErrorHandler.\ntype ErrorHandlerFunc func(err error)\n\nfunc (fn ErrorHandlerFunc) HandleError(err error) {\n\tfn(err)\n}\n\ntype LessFunc func(l, r string) bool\n\n// Options records params for creating DB object.\ntype Options struct {\n\t// Dir represents Open the database located in which dir.\n\tDir string\n\n\t// EntryIdxMode represents using which mode to index the entries.\n\tEntryIdxMode EntryIdxMode\n\n\t// RWMode represents the read and write mode.\n\t// RWMode includes two options: FileIO and MMap.\n\t// FileIO represents the read and write mode using standard I/O.\n\t// MMap represents the read and write mode using mmap.\n\tRWMode      RWMode\n\tSegmentSize int64\n\n\t// NodeNum represents the node number.\n\t// Default NodeNum is 1. NodeNum range [1,1023].\n\tNodeNum int64\n\n\t// SyncEnable represents if call Sync() function.\n\t// if SyncEnable is false, high write performance but potential data loss likely.\n\t// if SyncEnable is true, slower but persistent.\n\tSyncEnable bool\n\n\t// MaxFdNumsInCache represents the max numbers of fd in cache.\n\tMaxFdNumsInCache int\n\n\t// CleanFdsCacheThreshold represents the maximum threshold for recycling fd, it should be between 0 and 1.\n\tCleanFdsCacheThreshold float64\n\n\t// BufferSizeOfRecovery represents the buffer size of recoveryReader buffer Size\n\tBufferSizeOfRecovery int\n\n\t// CcWhenClose represent initiative GC when calling db.Close()\n\tGCWhenClose bool\n\n\t// CommitBufferSize represent allocated memory for tx\n\tCommitBufferSize int64\n\n\t// ErrorHandler handles an error occurred during transaction.\n\t// Example:\n\t//     func triggerAlertError(err error) {\n\t//     \t   if errors.Is(err, targetErr) {\n\t//         \t\talertManager.TriggerAlert()\n\t//     \t   }\n\t//     })\n\tErrorHandler ErrorHandler\n\n\t// LessFunc is a function that sorts keys.\n\tLessFunc LessFunc\n\n\t// MergeInterval represent the interval for automatic merges, with 0 meaning automatic merging is disabled.\n\tMergeInterval time.Duration\n\n\t// MaxBatchCount represents max entries in batch\n\tMaxBatchCount int64\n\n\t// MaxBatchSize represents max batch size in bytes\n\tMaxBatchSize int64\n\n\t// ExpiredDeleteType represents the data structure used for expired deletion\n\t// TimeWheel means use the time wheel, You can use it when you need high performance or low memory usage\n\t// TimeHeap means use the time heap, You can use it when you need to delete precisely or memory usage will be high\n\tExpiredDeleteType ExpiredDeleteType\n\n\t// max write record num\n\tMaxWriteRecordCount int64\n\n\t// cache size for HintKeyAndRAMIdxMode\n\tHintKeyAndRAMIdxCacheSize int\n}\n\nconst (\n\tB = 1\n\n\tKB = 1024 * B\n\n\tMB = 1024 * KB\n\n\tGB = 1024 * MB\n)\n\n// defaultSegmentSize is default data file size.\nvar defaultSegmentSize int64 = 256 * MB\n\n// DefaultOptions represents the default options.\nvar DefaultOptions = func() Options {\n\treturn Options{\n\t\tEntryIdxMode:      HintKeyValAndRAMIdxMode,\n\t\tSegmentSize:       defaultSegmentSize,\n\t\tNodeNum:           1,\n\t\tRWMode:            FileIO,\n\t\tSyncEnable:        true,\n\t\tCommitBufferSize:  4 * MB,\n\t\tMergeInterval:     2 * time.Hour,\n\t\tMaxBatchSize:      (15 * defaultSegmentSize / 4) / 100,\n\t\tMaxBatchCount:     (15 * defaultSegmentSize / 4) / 100 / 100,\n\t\tHintKeyAndRAMIdxCacheSize: 0,\n\t\tExpiredDeleteType: TimeWheel,\n\t}\n}()\n\ntype Option func(*Options)\n\nfunc WithDir(dir string) Option {\n\treturn func(opt *Options) {\n\t\topt.Dir = dir\n\t}\n}\n\nfunc WithEntryIdxMode(entryIdxMode EntryIdxMode) Option {\n\treturn func(opt *Options) {\n\t\topt.EntryIdxMode = entryIdxMode\n\t}\n}\n\nfunc WithRWMode(rwMode RWMode) Option {\n\treturn func(opt *Options) {\n\t\topt.RWMode = rwMode\n\t}\n}\n\nfunc WithSegmentSize(size int64) Option {\n\treturn func(opt *Options) {\n\t\topt.SegmentSize = size\n\t}\n}\n\nfunc WithMaxBatchCount(count int64) Option {\n\treturn func(opt *Options) {\n\t\topt.MaxBatchCount = count\n\t}\n}\n\nfunc WithHintKeyAndRAMIdxCacheSize(size int) Option {\n    return func(opt *Options) {\n        opt.HintKeyAndRAMIdxCacheSize = size\n    }\n}\n\nfunc WithMaxBatchSize(size int64) Option {\n\treturn func(opt *Options) {\n\t\topt.MaxBatchSize = size\n\t}\n}\n\nfunc WithNodeNum(num int64) Option {\n\treturn func(opt *Options) {\n\t\topt.NodeNum = num\n\t}\n}\n\nfunc WithSyncEnable(enable bool) Option {\n\treturn func(opt *Options) {\n\t\topt.SyncEnable = enable\n\t}\n}\n\nfunc WithMaxFdNumsInCache(num int) Option {\n\treturn func(opt *Options) {\n\t\topt.MaxFdNumsInCache = num\n\t}\n}\n\nfunc WithCleanFdsCacheThreshold(threshold float64) Option {\n\treturn func(opt *Options) {\n\t\topt.CleanFdsCacheThreshold = threshold\n\t}\n}\n\nfunc WithBufferSizeOfRecovery(size int) Option {\n\treturn func(opt *Options) {\n\t\topt.BufferSizeOfRecovery = size\n\t}\n}\n\nfunc WithGCWhenClose(enable bool) Option {\n\treturn func(opt *Options) {\n\t\topt.GCWhenClose = enable\n\t}\n}\n\nfunc WithErrorHandler(errorHandler ErrorHandler) Option {\n\treturn func(opt *Options) {\n\t\topt.ErrorHandler = errorHandler\n\t}\n}\n\nfunc WithCommitBufferSize(commitBufferSize int64) Option {\n\treturn func(opt *Options) {\n\t\topt.CommitBufferSize = commitBufferSize\n\t}\n}\n\nfunc WithLessFunc(lessFunc LessFunc) Option {\n\treturn func(opt *Options) {\n\t\topt.LessFunc = lessFunc\n\t}\n}\n\nfunc WithMaxWriteRecordCount(maxWriteRecordCount int64) Option {\n\treturn func(opt *Options) {\n\t\topt.MaxWriteRecordCount = maxWriteRecordCount\n\t}\n}\n"
        },
        {
          "name": "options_test.go",
          "type": "blob",
          "size": 2.7529296875,
          "content": "package nutsdb\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestWithNodeNum(t *testing.T) {\n\tInitOpt(\"\", true)\n\tdb, err := Open(\n\t\topt,\n\t\tWithNodeNum(1011),\n\t)\n\tassert.NoError(t, err)\n\tassert.Equal(t, int64(1011), db.opt.NodeNum)\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n\nfunc TestWithMaxBatchCount(t *testing.T) {\n\tInitOpt(\"\", true)\n\tdb, err := Open(\n\t\topt,\n\t\tWithMaxBatchCount(10),\n\t)\n\tassert.NoError(t, err)\n\tassert.Equal(t, int64(10), db.getMaxBatchCount())\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n\nfunc TestWithMaxBatchSize(t *testing.T) {\n\tInitOpt(\"\", true)\n\tdb, err := Open(\n\t\topt,\n\t\tWithMaxBatchSize(100),\n\t)\n\tassert.NoError(t, err)\n\tassert.Equal(t, int64(100), db.getMaxBatchSize())\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n\nfunc TestWithHintKeyAndRAMIdxCacheSize(t *testing.T) {\n\tInitOpt(\"\", true)\n\tdb, err := Open(\n\t\topt,\n\t\tWithHintKeyAndRAMIdxCacheSize(100),\n\t)\n\tassert.NoError(t, err)\n\tassert.Equal(t, 100, db.getHintKeyAndRAMIdxCacheSize())\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n\nfunc TestWithMaxWriteRecordCount(t *testing.T) {\n\tInitOpt(\"\", true)\n\tdb, err := Open(\n\t\topt,\n\t\tWithMaxWriteRecordCount(100),\n\t)\n\tassert.NoError(t, err)\n\tassert.Equal(t, int64(100), db.getMaxWriteRecordCount())\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n\nfunc TestWithRWMode(t *testing.T) {\n\tdb, err = Open(DefaultOptions,\n\t\tWithDir(\"/tmp/nutsdb\"),\n\t\tWithRWMode(MMap),\n\t)\n\tassert.NoError(t, err)\n\tassert.Equal(t, db.opt.RWMode, MMap)\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n\nfunc TestWithSyncEnable(t *testing.T) {\n\tdb, err = Open(DefaultOptions,\n\t\tWithDir(\"/tmp/nutsdb\"),\n\t\tWithSyncEnable(false),\n\t)\n\n\tassert.NoError(t, err)\n\tassert.False(t, db.opt.SyncEnable)\n\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n\nfunc TestWithMaxFdNumsInCache(t *testing.T) {\n\tdb, err = Open(DefaultOptions,\n\t\tWithDir(\"/tmp/nutsdb\"),\n\t\tWithMaxFdNumsInCache(100),\n\t)\n\n\tassert.NoError(t, err)\n\tassert.Equal(t, db.opt.MaxFdNumsInCache, 100)\n\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n\nfunc TestWithCleanFdsCacheThreshold(t *testing.T) {\n\tdb, err = Open(DefaultOptions,\n\t\tWithDir(\"/tmp/nutsdb\"),\n\t\tWithCleanFdsCacheThreshold(0.5),\n\t)\n\n\tassert.NoError(t, err)\n\tassert.Equal(t, db.opt.CleanFdsCacheThreshold, 0.5)\n\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n\nfunc TestWithErrorHandler(t *testing.T) {\n\tdb, err = Open(DefaultOptions,\n\t\tWithDir(\"/tmp/nutsdb\"),\n\t\tWithErrorHandler(ErrorHandlerFunc(func(err error) {\n\t\t})),\n\t)\n\tassert.NoError(t, err)\n\tassert.NotNil(t, db.opt.ErrorHandler)\n\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n\nfunc TestWithLessFunc(t *testing.T) {\n\tdb, err = Open(DefaultOptions,\n\t\tWithDir(\"/tmp/nutsdb\"),\n\t\tWithLessFunc(func(l, r string) bool {\n\t\t\treturn len(l) < len(r)\n\t\t}),\n\t)\n\tassert.NoError(t, err)\n\tassert.NotNil(t, db.opt.LessFunc)\n\n\terr = db.Close()\n\tassert.NoError(t, err)\n}\n"
        },
        {
          "name": "pending.go",
          "type": "blob",
          "size": 3.1337890625,
          "content": "package nutsdb\n\n// EntryStatus represents the Entry status in the current Tx\ntype EntryStatus = uint8\n\nconst (\n\t// NotFoundEntry means there is no changes for this entry in current Tx\n\tNotFoundEntry EntryStatus = 0\n\t// EntryDeleted means this Entry has been deleted in the current Tx\n\tEntryDeleted EntryStatus = 1\n\t// EntryUpdated means this Entry has been updated in the current Tx\n\tEntryUpdated EntryStatus = 2\n)\n\n// BucketStatus represents the current status of bucket in current Tx\ntype BucketStatus = uint8\n\nconst (\n\t// BucketStatusExistAlready means this bucket already exists\n\tBucketStatusExistAlready = 1\n\t// BucketStatusDeleted means this bucket is already deleted\n\tBucketStatusDeleted = 2\n\t// BucketStatusNew means this bucket is created in current Tx\n\tBucketStatusNew = 3\n\t// BucketStatusUpdated means this bucket is updated in current Tx\n\tBucketStatusUpdated = 4\n\t// BucketStatusUnknown means this bucket doesn't exist\n\tBucketStatusUnknown = 5\n)\n\n// pendingBucketList the uncommitted bucket changes in this Tx\ntype pendingBucketList map[Ds]map[BucketName]*Bucket\n\n// pendingEntriesInBTree means the changes Entries in DataStructureBTree in the Tx\ntype pendingEntriesInBTree map[BucketName]map[string]*Entry\n\n// pendingEntryList the uncommitted Entry changes in this Tx\ntype pendingEntryList struct {\n\tentriesInBTree pendingEntriesInBTree\n\tentries        map[Ds]map[BucketName][]*Entry\n\tsize           int\n}\n\n// newPendingEntriesList create a new pendingEntryList object for a Tx\nfunc newPendingEntriesList() *pendingEntryList {\n\tpending := &pendingEntryList{\n\t\tentriesInBTree: map[BucketName]map[string]*Entry{},\n\t\tentries:        map[Ds]map[BucketName][]*Entry{},\n\t\tsize:           0,\n\t}\n\treturn pending\n}\n\n// submitEntry submit an entry into pendingEntryList\nfunc (pending *pendingEntryList) submitEntry(ds Ds, bucket string, e *Entry) {\n\tswitch ds {\n\tcase DataStructureBTree:\n\t\tif _, exist := pending.entriesInBTree[bucket]; !exist {\n\t\t\tpending.entriesInBTree[bucket] = map[string]*Entry{}\n\t\t}\n\t\tif _, exist := pending.entriesInBTree[bucket][string(e.Key)]; !exist {\n\t\t\tpending.entriesInBTree[bucket][string(e.Key)] = e\n\t\t\tpending.size++\n\t\t}\n\tdefault:\n\t\tif _, exist := pending.entries[ds]; !exist {\n\t\t\tpending.entries[ds] = map[BucketName][]*Entry{}\n\t\t}\n\t\tentries := pending.entries[ds][bucket]\n\t\tentries = append(entries, e)\n\t\tpending.entries[ds][bucket] = entries\n\t\tpending.size++\n\t}\n}\n\n// rangeBucket input a range handler function f and call it with every bucket in pendingBucketList\nfunc (p pendingBucketList) rangeBucket(f func(bucket *Bucket) error) error {\n\tfor _, bucketsInDs := range p {\n\t\tfor _, bucket := range bucketsInDs {\n\t\t\terr := f(bucket)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// toList collect all the entries in pendingEntryList to a list.\nfunc (pending *pendingEntryList) toList() []*Entry {\n\tlist := make([]*Entry, 0, pending.size)\n\tfor _, entriesInBucket := range pending.entriesInBTree {\n\t\tfor _, entry := range entriesInBucket {\n\t\t\tlist = append(list, entry)\n\t\t}\n\t}\n\tfor _, entriesInDS := range pending.entries {\n\t\tfor _, entries := range entriesInDS {\n\t\t\tfor _, entry := range entries {\n\t\t\t\tlist = append(list, entry)\n\t\t\t}\n\t\t}\n\t}\n\treturn list\n}\n"
        },
        {
          "name": "record.go",
          "type": "blob",
          "size": 2.1123046875,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"time\"\n)\n\n// Record means item of indexes in memory\ntype Record struct {\n\tKey       []byte\n\tValue     []byte\n\tFileID    int64\n\tDataPos   uint64\n\tValueSize uint32\n\tTimestamp uint64\n\tTTL       uint32\n\tTxID      uint64\n}\n\n// IsExpired returns the record if expired or not.\nfunc (r *Record) IsExpired() bool {\n\treturn IsExpired(r.TTL, r.Timestamp)\n}\n\n// IsExpired checks the ttl if expired or not.\nfunc IsExpired(ttl uint32, timestamp uint64) bool {\n\tif ttl == Persistent {\n\t\treturn false\n\t}\n\n\tnow := time.UnixMilli(time.Now().UnixMilli())\n\texpireTime := time.UnixMilli(int64(timestamp))\n\texpireTime = expireTime.Add(time.Duration(ttl) * time.Second)\n\n\treturn expireTime.Before(now)\n}\n\n// NewRecord generate a record Obj\nfunc NewRecord() *Record {\n\treturn new(Record)\n}\n\nfunc (r *Record) WithKey(k []byte) *Record {\n\tr.Key = k\n\treturn r\n}\n\n// WithValue set the Value to Record\nfunc (r *Record) WithValue(v []byte) *Record {\n\tr.Value = v\n\treturn r\n}\n\n// WithFileId set FileID to Record\nfunc (r *Record) WithFileId(fid int64) *Record {\n\tr.FileID = fid\n\treturn r\n}\n\n// WithDataPos set DataPos to Record\nfunc (r *Record) WithDataPos(pos uint64) *Record {\n\tr.DataPos = pos\n\treturn r\n}\n\nfunc (r *Record) WithValueSize(valueSize uint32) *Record {\n\tr.ValueSize = valueSize\n\treturn r\n}\n\nfunc (r *Record) WithTimestamp(timestamp uint64) *Record {\n\tr.Timestamp = timestamp\n\treturn r\n}\n\nfunc (r *Record) WithTTL(ttl uint32) *Record {\n\tr.TTL = ttl\n\treturn r\n}\n\nfunc (r *Record) WithTxID(txID uint64) *Record {\n\tr.TxID = txID\n\treturn r\n}\n"
        },
        {
          "name": "recovery_reader.go",
          "type": "blob",
          "size": 2.7001953125,
          "content": "package nutsdb\n\nimport (\n\t\"bufio\"\n\t\"io\"\n\t\"os\"\n)\n\n// fileRecovery use bufio.Reader to read entry\ntype fileRecovery struct {\n\tfd     *os.File\n\treader *bufio.Reader\n\tsize   int64\n}\n\nfunc newFileRecovery(path string, bufSize int) (fr *fileRecovery, err error) {\n\tfd, err := os.OpenFile(path, os.O_RDWR, os.ModePerm)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbufSize = calBufferSize(bufSize)\n\tfileInfo, err := fd.Stat()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &fileRecovery{\n\t\tfd:     fd,\n\t\treader: bufio.NewReaderSize(fd, bufSize),\n\t\tsize:   fileInfo.Size(),\n\t}, nil\n}\n\n// readEntry will read an Entry from disk.\nfunc (fr *fileRecovery) readEntry(off int64) (e *Entry, err error) {\n\tvar size int64 = MaxEntryHeaderSize\n\t// Since MaxEntryHeaderSize may be larger than the actual Header, it needs to be calculated\n\tif off+size > fr.size {\n\t\tsize = fr.size - off\n\t}\n\n\tbuf := make([]byte, size)\n\t_, err = fr.fd.Seek(off, 0)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t_, err = fr.fd.Read(buf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\te = new(Entry)\n\theaderSize, err := e.ParseMeta(buf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif e.IsZero() {\n\t\treturn nil, nil\n\t}\n\n\theaderBuf := buf[:headerSize]\n\tremainingBuf := buf[headerSize:]\n\n\tpayloadSize := e.Meta.PayloadSize()\n\tdataBuf := make([]byte, payloadSize)\n\texcessSize := size - headerSize\n\n\tif payloadSize <= excessSize {\n\t\tcopy(dataBuf, remainingBuf[:payloadSize])\n\t} else {\n\t\tcopy(dataBuf, remainingBuf)\n\t\t_, err := fr.fd.Read(dataBuf[excessSize:])\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\terr = e.ParsePayload(dataBuf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcrc := e.GetCrc(headerBuf)\n\tif crc != e.Meta.Crc {\n\t\treturn nil, ErrCrc\n\t}\n\n\treturn e, nil\n}\n\nfunc (fr *fileRecovery) readBucket() (b *Bucket, err error) {\n\tbuf := make([]byte, BucketMetaSize)\n\t_, err = io.ReadFull(fr.reader, buf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tmeta := new(BucketMeta)\n\tmeta.Decode(buf)\n\tbucket := new(Bucket)\n\tbucket.Meta = meta\n\tdataBuf := make([]byte, meta.Size)\n\t_, err = io.ReadFull(fr.reader, dataBuf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\terr = bucket.Decode(dataBuf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif bucket.GetCRC(buf, dataBuf) != bucket.Meta.Crc {\n\t\treturn nil, ErrBucketCrcInvalid\n\t}\n\n\treturn bucket, nil\n}\n\n// calBufferSize calculates the buffer size of bufio.Reader\n// if the size < 4 * KB, use 4 * KB as the size of buffer in bufio.Reader\n// if the size > 4 * KB, use the nearly blockSize buffer as the size of buffer in bufio.Reader\nfunc calBufferSize(size int) int {\n\tblockSize := 4 * KB\n\tif size < blockSize {\n\t\treturn blockSize\n\t}\n\thasRest := (size%blockSize == 0)\n\tif hasRest {\n\t\treturn (size/blockSize + 1) * blockSize\n\t}\n\treturn size\n}\n\nfunc (fr *fileRecovery) release() error {\n\treturn fr.fd.Close()\n}\n"
        },
        {
          "name": "recovery_reader_test.go",
          "type": "blob",
          "size": 1.6279296875,
          "content": "package nutsdb\n\nimport (\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n\t\"os\"\n\t\"testing\"\n)\n\nfunc Test_readEntry(t *testing.T) {\n\tpath := \"/tmp/test_read_entry\"\n\n\tfd, err := os.OpenFile(path, os.O_TRUNC|os.O_CREATE|os.O_RDWR, os.ModePerm)\n\trequire.NoError(t, err)\n\tmeta := NewMetaData().WithKeySize(uint32(len(\"key\"))).\n\t\tWithValueSize(uint32(len(\"val\"))).WithTimeStamp(1547707905).\n\t\tWithTTL(Persistent).WithFlag(DataSetFlag).WithBucketId(1)\n\n\texpect := NewEntry().WithKey([]byte(\"key\")).WithMeta(meta).WithValue([]byte(\"val\"))\n\n\t_, err = fd.Write(expect.Encode())\n\trequire.NoError(t, err)\n\n\tf, err := newFileRecovery(path, 4096)\n\trequire.NoError(t, err)\n\n\tentry, err := f.readEntry(0)\n\trequire.NoError(t, err)\n\n\tassert.Equal(t, expect.Encode(), entry.Encode())\n\n\terr = fd.Close()\n\trequire.NoError(t, err)\n\n}\n\nfunc Test_fileRecovery_readBucket(t *testing.T) {\n\tfilePath := \"bucket_test_data\"\n\tbucket := &Bucket{\n\t\tMeta: &BucketMeta{\n\t\t\tOp: BucketInsertOperation,\n\t\t},\n\t\tId:   1,\n\t\tDs:   DataStructureBTree,\n\t\tName: \"bucket_1\",\n\t}\n\tbytes := bucket.Encode()\n\n\tfd, err := os.OpenFile(filePath, os.O_RDWR|os.O_CREATE, os.ModePerm)\n\tdefer func() {\n\t\terr = fd.Close()\n\t\tassert.Nil(t, err)\n\t\terr = os.Remove(filePath)\n\t\tassert.Nil(t, nil)\n\t}()\n\tassert.Nil(t, err)\n\t_, err = fd.Write(bytes)\n\tassert.Nil(t, err)\n\n\tfr, err := newFileRecovery(filePath, 4*MB)\n\tassert.Nil(t, err)\n\treadBucket, err := fr.readBucket()\n\tassert.Nil(t, err)\n\tassert.Equal(t, readBucket.Meta.Op, BucketInsertOperation)\n\tassert.Equal(t, int64(8+2+8), int64(readBucket.Meta.Size))\n\tassert.Equal(t, BucketId(1), readBucket.Id)\n\tassert.Equal(t, readBucket.Name, \"bucket_1\")\n}\n"
        },
        {
          "name": "rwmanager.go",
          "type": "blob",
          "size": 1.0888671875,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\n// RWMode represents the read and write mode.\ntype RWMode int\n\nconst (\n\t// FileIO represents the read and write mode using standard I/O.\n\tFileIO RWMode = iota\n\n\t// MMap represents the read and write mode using mmap.\n\tMMap\n)\n\n// RWManager represents an interface to a RWManager.\ntype RWManager interface {\n\tWriteAt(b []byte, off int64) (n int, err error)\n\tReadAt(b []byte, off int64) (n int, err error)\n\tSync() (err error)\n\tRelease() (err error)\n\tSize() int64\n\tClose() (err error)\n}\n"
        },
        {
          "name": "rwmanager_fileio_test.go",
          "type": "blob",
          "size": 0.8349609375,
          "content": "package nutsdb\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestRWManager_FileIO_All(t *testing.T) {\n\n\tfilePath := \"/tmp/foo_rw_fileio\"\n\tmaxFdNums := 20\n\tcleanThreshold := 0.5\n\tvar fdm *fdManager\n\n\tt.Run(\"test write read\", func(t *testing.T) {\n\t\tfdm = newFdm(maxFdNums, cleanThreshold)\n\t\tfd, err := fdm.getFd(filePath)\n\t\tif err != nil {\n\t\t\trequire.NoError(t, err)\n\t\t}\n\n\t\trwManager := &FileIORWManager{fd, filePath, fdm, 256 * MB}\n\t\tb := []byte(\"hello\")\n\t\toff := int64(3)\n\t\t_, err = rwManager.WriteAt(b, off)\n\t\tif err != nil {\n\t\t\trequire.NoError(t, err)\n\t\t}\n\n\t\tbucketBufLen := len(b)\n\t\tbucketBuf := make([]byte, bucketBufLen)\n\t\tn, err := rwManager.ReadAt(bucketBuf, off)\n\t\tif err != nil {\n\t\t\trequire.NoError(t, err)\n\t\t}\n\n\t\tassert.Equal(t, bucketBufLen, n)\n\t\tassert.Equal(t, b, bucketBuf)\n\t})\n}\n"
        },
        {
          "name": "rwmanager_mmap_test.go",
          "type": "blob",
          "size": 3.689453125,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/xujiajun/mmap-go\"\n)\n\nfunc TestRWManager_MMap_Release(t *testing.T) {\n\tfilePath := \"/tmp/foo_rw_MMap\"\n\tfdm := newFileManager(MMap, 1024, 0.5, 256*MB)\n\trwmanager, err := fdm.getMMapRWManager(filePath, 1024, 256*MB)\n\tif err != nil {\n\t\tt.Error(\"err TestRWManager_MMap_Release getMMapRWManager\")\n\t}\n\n\tb := []byte(\"hello\")\n\toff := int64(0)\n\t_, err = rwmanager.WriteAt(b, off)\n\tif err != nil {\n\t\tt.Error(\"err TestRWManager_MMap_Release WriteAt\")\n\t}\n\n\tif !rwmanager.IsActive() {\n\t\tt.Error(\"err TestRWManager_MMap_Release FdInfo:using not correct\")\n\t}\n\n\terr = rwmanager.Release()\n\tif err != nil {\n\t\tt.Error(\"err TestRWManager_MMap_Release Release\")\n\t}\n\n\tif rwmanager.IsActive() {\n\t\tt.Error(\"err TestRWManager_MMap_Release Release Failed\")\n\t}\n}\n\nfunc (rwmanager *MMapRWManager) IsActive() bool {\n\treturn rwmanager.fdm.cache[rwmanager.path].using != 0\n}\n\nfunc TestRWManager_MMap_WriteAt(t *testing.T) {\n\tfilePath := \"/tmp/foo_rw_filemmap\"\n\tmaxFdNums := 1024\n\tcleanThreshold := 0.5\n\tvar fdm = newFdm(maxFdNums, cleanThreshold)\n\n\tfd, err := fdm.getFd(filePath)\n\tif err != nil {\n\t\trequire.NoError(t, err)\n\t}\n\tdefer os.Remove(fd.Name())\n\n\terr = Truncate(filePath, 1024, fd)\n\tif err != nil {\n\t\trequire.NoError(t, err)\n\n\t}\n\tm, err := mmap.Map(fd, mmap.RDWR, 0)\n\tif err != nil {\n\t\trequire.NoError(t, err)\n\t}\n\n\tmmManager := &MMapRWManager{filePath, fdm, m, 256 * MB}\n\tb := []byte(\"test write at\")\n\toff := int64(3)\n\tn, err := mmManager.WriteAt(b, off)\n\tif err != nil {\n\t\trequire.NoError(t, err)\n\t}\n\trequire.Equal(t, len(b), n)\n\trequire.Equal(t, append([]byte{0, 0, 0}, b...), []byte(m[:off+int64(len(b))]))\n}\n\nfunc TestRWManager_MMap_Sync(t *testing.T) {\n\tfilePath := \"/tmp/foo_rw_filemmap\"\n\tmaxFdNums := 1024\n\tcleanThreshold := 0.5\n\tvar fdm = newFdm(maxFdNums, cleanThreshold)\n\n\tfd, err := fdm.getFd(filePath)\n\tif err != nil {\n\t\trequire.NoError(t, err)\n\t}\n\tdefer os.Remove(fd.Name())\n\n\terr = Truncate(filePath, 1024, fd)\n\tif err != nil {\n\t\trequire.NoError(t, err)\n\n\t}\n\tm, err := mmap.Map(fd, mmap.RDWR, 0)\n\tif err != nil {\n\t\trequire.NoError(t, err)\n\t}\n\n\tmmManager := &MMapRWManager{filePath, fdm, m, 256 * MB}\n\tm[1] = 'z'\n\terr = mmManager.Sync()\n\trequire.NoError(t, err)\n\tfileContents, err := os.ReadFile(filePath)\n\trequire.NoError(t, err)\n\trequire.Equal(t, fileContents, []byte(m[:]))\n}\n\nfunc TestRWManager_MMap_Close(t *testing.T) {\n\tfilePath := \"/tmp/foo_rw_filemmap\"\n\tmaxFdNums := 1024\n\tcleanThreshold := 0.5\n\tvar fdm = newFdm(maxFdNums, cleanThreshold)\n\n\tfd, err := fdm.getFd(filePath)\n\tif err != nil {\n\t\trequire.NoError(t, err)\n\t}\n\tdefer os.Remove(fd.Name())\n\n\terr = Truncate(filePath, 1024, fd)\n\tif err != nil {\n\t\trequire.NoError(t, err)\n\n\t}\n\tm, err := mmap.Map(fd, mmap.RDWR, 0)\n\tif err != nil {\n\t\trequire.NoError(t, err)\n\t}\n\n\tmmManager := &MMapRWManager{filePath, fdm, m, 256 * MB}\n\terr = mmManager.Close()\n\terr = isFileDescriptorClosed(fd.Fd())\n\tif err == nil {\n\t\tt.Error(\"expected file descriptor to be closed, but it's still open\")\n\t}\n}\n\nfunc isFileDescriptorClosed(fd uintptr) error {\n\tfile := os.NewFile(fd, \"\")\n\tdefer file.Close()\n\t_, err := file.Stat()\n\n\treturn err\n}\n"
        },
        {
          "name": "rwmanger_fileio.go",
          "type": "blob",
          "size": 1.951171875,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"os\"\n)\n\n// FileIORWManager represents the RWManager which using standard I/O.\ntype FileIORWManager struct {\n\tfd          *os.File\n\tpath        string\n\tfdm         *fdManager\n\tsegmentSize int64\n}\n\n// WriteAt writes len(b) bytes to the File starting at byte offset off.\n// `WriteAt` is a wrapper of the *File.WriteAt.\nfunc (fm *FileIORWManager) WriteAt(b []byte, off int64) (n int, err error) {\n\treturn fm.fd.WriteAt(b, off)\n}\n\n// ReadAt reads len(b) bytes from the File starting at byte offset off.\n// `ReadAt` is a wrapper of the *File.ReadAt.\nfunc (fm *FileIORWManager) ReadAt(b []byte, off int64) (n int, err error) {\n\treturn fm.fd.ReadAt(b, off)\n}\n\n// Sync commits the current contents of the file to stable storage.\n// Typically, this means flushing the file system's in-memory copy\n// of recently written data to disk.\n// `Sync` is a wrapper of the *File.Sync.\nfunc (fm *FileIORWManager) Sync() (err error) {\n\treturn fm.fd.Sync()\n}\n\n// Release is a wrapper around the reduceUsing method\nfunc (fm *FileIORWManager) Release() (err error) {\n\tfm.fdm.reduceUsing(fm.path)\n\treturn nil\n}\n\nfunc (fm *FileIORWManager) Size() int64 {\n\treturn fm.segmentSize\n}\n\n// Close will remove the cache in the fdm of the specified path, and call the close method of the os of the file\nfunc (fm *FileIORWManager) Close() (err error) {\n\treturn fm.fdm.closeByPath(fm.path)\n}\n"
        },
        {
          "name": "rwmanger_mmap.go",
          "type": "blob",
          "size": 2.42578125,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"errors\"\n\n\tmmap \"github.com/xujiajun/mmap-go\"\n)\n\n// MMapRWManager represents the RWManager which using mmap.\ntype MMapRWManager struct {\n\tpath        string\n\tfdm         *fdManager\n\tm           mmap.MMap\n\tsegmentSize int64\n}\n\nvar (\n\t// ErrUnmappedMemory is returned when a function is called on unmapped memory\n\tErrUnmappedMemory = errors.New(\"unmapped memory\")\n\n\t// ErrIndexOutOfBound is returned when given offset out of mapped region\n\tErrIndexOutOfBound = errors.New(\"offset out of mapped region\")\n)\n\n// WriteAt copies data to mapped region from the b slice starting at\n// given off and returns number of bytes copied to the mapped region.\nfunc (mm *MMapRWManager) WriteAt(b []byte, off int64) (n int, err error) {\n\tif mm.m == nil {\n\t\treturn 0, ErrUnmappedMemory\n\t} else if off >= int64(len(mm.m)) || off < 0 {\n\t\treturn 0, ErrIndexOutOfBound\n\t}\n\n\treturn copy(mm.m[off:], b), nil\n}\n\n// ReadAt copies data to b slice from mapped region starting at\n// given off and returns number of bytes copied to the b slice.\nfunc (mm *MMapRWManager) ReadAt(b []byte, off int64) (n int, err error) {\n\tif mm.m == nil {\n\t\treturn 0, ErrUnmappedMemory\n\t} else if off >= int64(len(mm.m)) || off < 0 {\n\t\treturn 0, ErrIndexOutOfBound\n\t}\n\n\treturn copy(b, mm.m[off:]), nil\n}\n\n// Sync synchronizes the mapping's contents to the file's contents on disk.\nfunc (mm *MMapRWManager) Sync() (err error) {\n\treturn mm.m.Flush()\n}\n\n// Release deletes the memory mapped region, flushes any remaining changes\nfunc (mm *MMapRWManager) Release() (err error) {\n\tmm.fdm.reduceUsing(mm.path)\n\treturn mm.m.Unmap()\n}\n\nfunc (mm *MMapRWManager) Size() int64 {\n\treturn mm.segmentSize\n}\n\n// Close will remove the cache in the fdm of the specified path, and call the close method of the os of the file\nfunc (mm *MMapRWManager) Close() (err error) {\n\treturn mm.fdm.closeByPath(mm.path)\n}\n"
        },
        {
          "name": "set.go",
          "type": "blob",
          "size": 5.5048828125,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"errors\"\n\t\"hash/fnv\"\n)\n\nvar (\n\t// ErrSetNotExist is returned when the key does not exist.\n\tErrSetNotExist = errors.New(\"set not exist\")\n\n\t// ErrSetMemberNotExist is returned when the member of set does not exist\n\tErrSetMemberNotExist = errors.New(\"set member not exist\")\n\n\t// ErrMemberEmpty is returned when the item received is nil\n\tErrMemberEmpty = errors.New(\"item empty\")\n)\n\nvar fnvHash = fnv.New32a()\n\ntype Set struct {\n\tM map[string]map[uint32]*Record\n}\n\nfunc NewSet() *Set {\n\treturn &Set{\n\t\tM: map[string]map[uint32]*Record{},\n\t}\n}\n\n// SAdd adds the specified members to the set stored at key.\nfunc (s *Set) SAdd(key string, values [][]byte, records []*Record) error {\n\tset, ok := s.M[key]\n\tif !ok {\n\t\ts.M[key] = map[uint32]*Record{}\n\t\tset = s.M[key]\n\t}\n\n\tfor i, value := range values {\n\t\thash, err := getFnv32(value)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tset[hash] = records[i]\n\t}\n\n\treturn nil\n}\n\n// SRem removes the specified members from the set stored at key.\nfunc (s *Set) SRem(key string, values ...[]byte) error {\n\tset, ok := s.M[key]\n\tif !ok {\n\t\treturn ErrSetNotExist\n\t}\n\n\tif len(values) == 0 || values[0] == nil {\n\t\treturn ErrMemberEmpty\n\t}\n\n\tfor _, value := range values {\n\t\thash, err := getFnv32(value)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdelete(set, hash)\n\t}\n\n\treturn nil\n}\n\n// SHasKey returns whether it has the set at given key.\nfunc (s *Set) SHasKey(key string) bool {\n\tif _, ok := s.M[key]; ok {\n\t\treturn true\n\t}\n\treturn false\n}\n\n// SPop removes and returns one or more random elements from the set value store at key.\nfunc (s *Set) SPop(key string) *Record {\n\tif !s.SHasKey(key) {\n\t\treturn nil\n\t}\n\n\tfor hash, record := range s.M[key] {\n\t\tdelete(s.M[key], hash)\n\t\treturn record\n\t}\n\n\treturn nil\n}\n\n// SCard Returns the set cardinality (number of elements) of the set stored at key.\nfunc (s *Set) SCard(key string) int {\n\tif !s.SHasKey(key) {\n\t\treturn 0\n\t}\n\n\treturn len(s.M[key])\n}\n\n// SDiff Returns the members of the set resulting from the difference between the first set and all the successive sets.\nfunc (s *Set) SDiff(key1, key2 string) ([]*Record, error) {\n\tif !s.SHasKey(key1) || !s.SHasKey(key2) {\n\t\treturn nil, ErrSetNotExist\n\t}\n\n\trecords := make([]*Record, 0)\n\n\tfor hash, record := range s.M[key1] {\n\t\tif _, ok := s.M[key2][hash]; !ok {\n\t\t\trecords = append(records, record)\n\t\t}\n\t}\n\treturn records, nil\n}\n\n// SInter Returns the members of the set resulting from the intersection of all the given sets.\nfunc (s *Set) SInter(key1, key2 string) ([]*Record, error) {\n\tif !s.SHasKey(key1) || !s.SHasKey(key2) {\n\t\treturn nil, ErrSetNotExist\n\t}\n\n\trecords := make([]*Record, 0)\n\n\tfor hash, record := range s.M[key1] {\n\t\tif _, ok := s.M[key2][hash]; ok {\n\t\t\trecords = append(records, record)\n\t\t}\n\t}\n\treturn records, nil\n}\n\n// SIsMember Returns if member is a member of the set stored at key.\nfunc (s *Set) SIsMember(key string, value []byte) (bool, error) {\n\tif _, ok := s.M[key]; !ok {\n\t\treturn false, ErrSetNotExist\n\t}\n\n\thash, err := getFnv32(value)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\n\tif _, ok := s.M[key][hash]; ok {\n\t\treturn true, nil\n\t}\n\n\treturn false, nil\n}\n\n// SAreMembers Returns if members are members of the set stored at key.\n// For multiple items it returns true only if all the items exist.\nfunc (s *Set) SAreMembers(key string, values ...[]byte) (bool, error) {\n\tif _, ok := s.M[key]; !ok {\n\t\treturn false, ErrSetNotExist\n\t}\n\n\tfor _, value := range values {\n\n\t\thash, err := getFnv32(value)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\n\t\tif _, ok := s.M[key][hash]; !ok {\n\t\t\treturn false, nil\n\t\t}\n\t}\n\n\treturn true, nil\n}\n\n// SMembers returns all the members of the set value stored at key.\nfunc (s *Set) SMembers(key string) ([]*Record, error) {\n\tif _, ok := s.M[key]; !ok {\n\t\treturn nil, ErrSetNotExist\n\t}\n\n\trecords := make([]*Record, 0)\n\n\tfor _, record := range s.M[key] {\n\t\trecords = append(records, record)\n\t}\n\n\treturn records, nil\n}\n\n// SMove moves member from the set at source to the set at destination.\nfunc (s *Set) SMove(key1, key2 string, value []byte) (bool, error) {\n\tif !s.SHasKey(key1) || !s.SHasKey(key2) {\n\t\treturn false, ErrSetNotExist\n\t}\n\n\tset1, set2 := s.M[key1], s.M[key2]\n\n\thash, err := getFnv32(value)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\n\tvar (\n\t\tmember *Record\n\t\tok     bool\n\t)\n\n\tif member, ok = set1[hash]; !ok {\n\t\treturn false, ErrSetMemberNotExist\n\t}\n\n\tif _, ok = set2[hash]; !ok {\n\t\terr = s.SAdd(key2, [][]byte{value}, []*Record{member})\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\t}\n\n\terr = s.SRem(key1, value)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\n\treturn true, nil\n}\n\n// SUnion returns the members of the set resulting from the union of all the given sets.\nfunc (s *Set) SUnion(key1, key2 string) ([]*Record, error) {\n\tif !s.SHasKey(key1) || !s.SHasKey(key2) {\n\t\treturn nil, ErrSetNotExist\n\t}\n\n\trecords, err := s.SMembers(key1)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor hash, record := range s.M[key2] {\n\t\tif _, ok := s.M[key1][hash]; !ok {\n\t\t\trecords = append(records, record)\n\t\t}\n\t}\n\n\treturn records, nil\n}\n"
        },
        {
          "name": "set_test.go",
          "type": "blob",
          "size": 11.33984375,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"github.com/stretchr/testify/require\"\n\t\"testing\"\n)\n\nfunc TestSet_SAdd(t *testing.T) {\n\tset := NewSet()\n\tkey := \"key\"\n\texpectRecords := generateRecords(3)\n\n\tvalues := make([][]byte, 3)\n\tfor i := range expectRecords {\n\t\tvalues[i] = expectRecords[i].Value\n\t}\n\n\trequire.NoError(t, set.SAdd(key, values, expectRecords))\n\n\tok, err := set.SAreMembers(key, values...)\n\trequire.True(t, ok)\n\trequire.NoError(t, err)\n}\n\nfunc TestSet_SRem(t *testing.T) {\n\tset := NewSet()\n\tkey := \"key\"\n\texpectRecords := generateRecords(4)\n\tvalues := make([][]byte, 4)\n\tfor i := range expectRecords {\n\t\tvalues[i] = expectRecords[i].Value\n\t}\n\n\trequire.NoError(t, set.SAdd(key, values, expectRecords))\n\n\trequire.NoError(t, set.SRem(key, values[0]))\n\trequire.NoError(t, set.SRem(key, values[1:]...))\n\n\trequire.NoError(t, set.SAdd(key, values, expectRecords))\n\trequire.NoError(t, set.SRem(key, values...))\n\n\trequire.Error(t, set.SRem(\"fake key\", values...))\n\trequire.Error(t, set.SRem(key, nil))\n}\n\nfunc TestSet_SDiff(t *testing.T) {\n\tset := NewSet()\n\n\tkey1 := \"set1\"\n\tkey2 := \"set2\"\n\tkey3 := \"set3\"\n\tkey4 := \"set4\"\n\n\texpectRecords := generateRecords(10)\n\n\tvalues := make([][]byte, 10)\n\tfor i := range expectRecords {\n\t\tvalues[i] = expectRecords[i].Value\n\t}\n\n\trequire.NoError(t, set.SAdd(key1, values[:5], expectRecords[:5]))\n\trequire.NoError(t, set.SAdd(key2, values[2:6], expectRecords[2:6]))\n\trequire.NoError(t, set.SAdd(key3, values[2:7], expectRecords[2:7]))\n\trequire.NoError(t, set.SAdd(key4, values[7:], expectRecords[7:]))\n\n\ttype args struct {\n\t\tkey1 string\n\t\tkey2 string\n\t}\n\n\ttests := []struct {\n\t\tname    string\n\t\targs    args\n\t\tset     *Set\n\t\twant    []*Record\n\t\twantErr bool\n\t}{\n\t\t{\"normal set diff1\", args{key1, key2}, set, expectRecords[:2], false},\n\t\t{\"normal set diff2\", args{key1, key4}, set, expectRecords[:5], false},\n\t\t{\"normal set diff3\", args{key3, key2}, set, expectRecords[6:7], false},\n\t\t{\"first fake set\", args{\"fake_key1\", key2}, set, nil, true},\n\t\t{\"second fake set\", args{key1, \"fake_key2\"}, set, nil, true},\n\t\t{\"two fake set\", args{\"fake_key1\", \"fake_key2\"}, set, nil, true},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tgot, err := tt.set.SDiff(tt.args.key1, tt.args.key2)\n\t\t\tif (err != nil) != tt.wantErr {\n\t\t\t\trequire.Errorf(t, err, \"Get() error = %v, wantErr %v\", tt.wantErr)\n\t\t\t}\n\t\t\trequire.ElementsMatchf(t, tt.want, got, \"Get() got = %v, want %v\", got, tt.want)\n\t\t})\n\t}\n}\n\nfunc TestSet_SCard(t *testing.T) {\n\tset := NewSet()\n\n\tkey1 := \"set1\"\n\tkey2 := \"set2\"\n\tkey3 := \"set3\"\n\tkey4 := \"set4\"\n\n\texpectRecords := generateRecords(10)\n\n\tvalues := make([][]byte, 10)\n\tfor i := range expectRecords {\n\t\tvalues[i] = expectRecords[i].Value\n\t}\n\n\trequire.NoError(t, set.SAdd(key1, values[:5], expectRecords[:5]))\n\trequire.NoError(t, set.SAdd(key2, values[2:6], expectRecords[2:6]))\n\trequire.NoError(t, set.SAdd(key3, values[2:7], expectRecords[2:7]))\n\trequire.NoError(t, set.SAdd(key4, values[7:], expectRecords[7:]))\n\n\ttests := []struct {\n\t\tname string\n\t\tkey  string\n\t\tset  *Set\n\t\twant int\n\t}{\n\t\t{\"normal set\", key1, set, 5},\n\t\t{\"normal set\", key2, set, 4},\n\t\t{\"normal set\", key3, set, 5},\n\t\t{\"fake key\", \"key_fake\", set, 0},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tgot := tt.set.SCard(tt.key)\n\t\t\trequire.Equalf(t, tt.want, got, \"TestSet_SCard err\")\n\t\t})\n\t}\n}\n\nfunc TestSet_SInter(t *testing.T) {\n\tset := NewSet()\n\n\tkey1 := \"set1\"\n\tkey2 := \"set2\"\n\tkey3 := \"set3\"\n\tkey4 := \"set4\"\n\n\texpectRecords := generateRecords(10)\n\n\tvalues := make([][]byte, 10)\n\tfor i := range expectRecords {\n\t\tvalues[i] = expectRecords[i].Value\n\t}\n\n\trequire.NoError(t, set.SAdd(key1, values[:5], expectRecords[:5]))\n\trequire.NoError(t, set.SAdd(key2, values[2:6], expectRecords[2:6]))\n\trequire.NoError(t, set.SAdd(key3, values[2:7], expectRecords[2:7]))\n\trequire.NoError(t, set.SAdd(key4, values[7:], expectRecords[7:]))\n\n\ttype args struct {\n\t\tkey1 string\n\t\tkey2 string\n\t}\n\n\ttests := []struct {\n\t\tname    string\n\t\targs    args\n\t\tset     *Set\n\t\twant    []*Record\n\t\twantErr bool\n\t}{\n\t\t{\"normal set inter1\", args{key1, key2}, set, []*Record{expectRecords[2], expectRecords[3], expectRecords[4]}, false},\n\t\t{\"normal set inter1\", args{key2, key3}, set, []*Record{expectRecords[2], expectRecords[3], expectRecords[4], expectRecords[5]}, false},\n\t\t{\"normal set inter2\", args{key1, key4}, set, nil, false},\n\t\t{\"first fake set\", args{\"fake_key1\", key2}, set, nil, true},\n\t\t{\"second fake set\", args{key1, \"fake_key2\"}, set, nil, true},\n\t\t{\"two fake set\", args{\"fake_key1\", \"fake_key2\"}, set, nil, true},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tgot, err := tt.set.SInter(tt.args.key1, tt.args.key2)\n\t\t\tif (err != nil) != tt.wantErr {\n\t\t\t\trequire.Errorf(t, err, \"Get() error = %v, wantErr %v\", tt.wantErr)\n\t\t\t}\n\t\t\trequire.ElementsMatchf(t, tt.want, got, \"Get() got = %v, want %v\", got, tt.want)\n\t\t})\n\t}\n}\n\nfunc TestSet_SMembers(t *testing.T) {\n\tset := NewSet()\n\n\tkey := \"set\"\n\n\texpectRecords := generateRecords(3)\n\n\tvalues := make([][]byte, 3)\n\tfor i := range expectRecords {\n\t\tvalues[i] = expectRecords[i].Value\n\t}\n\n\trequire.NoError(t, set.SAdd(key, values, expectRecords))\n\n\ttests := []struct {\n\t\tname    string\n\t\tkey     string\n\t\tset     *Set\n\t\twant    []*Record\n\t\twantErr bool\n\t}{\n\t\t{\"normal SMembers\", key, set, expectRecords[0:1], false},\n\t\t{\"normal SMembers\", key, set, expectRecords[1:], false},\n\t\t{\"normal SMembers\", key, set, expectRecords, false},\n\t\t{\"fake key\", \"fake_key\", set, nil, true},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tgot, err := tt.set.SMembers(tt.key)\n\t\t\tif (err != nil) != tt.wantErr {\n\t\t\t\trequire.Errorf(t, err, \"Get() error = %v, wantErr %v\", tt.wantErr)\n\t\t\t}\n\t\t\trequire.Subsetf(t, got, tt.want, \"SInter() got = %v, want = %v\", got, tt.want)\n\t\t})\n\t}\n}\n\nfunc TestSet_SMove(t *testing.T) {\n\tset := NewSet()\n\n\tkey1 := \"set1\"\n\tkey2 := \"set2\"\n\n\texpectRecords := generateRecords(3)\n\tvalues := make([][]byte, 3)\n\tfor i := range expectRecords {\n\t\tvalues[i] = expectRecords[i].Value\n\t}\n\n\trequire.NoError(t, set.SAdd(key1, values[:2], expectRecords[:2]))\n\trequire.NoError(t, set.SAdd(key2, values[2:], expectRecords[2:]))\n\n\ttype args struct {\n\t\tkey1 string\n\t\tkey2 string\n\t\titem []byte\n\t}\n\n\ttests := []struct {\n\t\tname      string\n\t\targs      args\n\t\tset       *Set\n\t\twant1     []*Record\n\t\twant2     []*Record\n\t\texpectErr error\n\t}{\n\t\t{\"normal SMove\", args{key1, key2, values[1]}, set, expectRecords[0:1], expectRecords[1:], nil},\n\t\t{\"not exist member SMove\", args{key1, key2, values[2]}, set, nil, nil, ErrSetMemberNotExist},\n\t\t{\"fake key SMove1\", args{\"fake key\", key2, values[2]}, set, nil, nil, ErrSetNotExist},\n\t\t{\"fake key SMove\", args{key1, \"fake key\", values[2]}, set, nil, nil, ErrSetNotExist},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\t_, err := tt.set.SMove(tt.args.key1, tt.args.key2, tt.args.item)\n\t\t\tif tt.expectErr != nil {\n\t\t\t\trequire.Error(t, err)\n\t\t\t\trequire.Equal(t, tt.expectErr, err)\n\t\t\t} else {\n\t\t\t\tgot1, _ := tt.set.SMembers(tt.args.key1)\n\t\t\t\tgot2, _ := tt.set.SMembers(tt.args.key2)\n\t\t\t\trequire.ElementsMatchf(t, got1, tt.want1, \"SMove() got = %v, want = %v\", got1, tt.want1)\n\t\t\t\trequire.ElementsMatchf(t, got2, tt.want2, \"SMove() got = %v, want = %v\", got2, tt.want2)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestSet_SPop(t *testing.T) {\n\tset := NewSet()\n\n\tkey := \"set\"\n\n\texpectRecords := generateRecords(2)\n\tvalues := make([][]byte, 2)\n\tfor i := range expectRecords {\n\t\tvalues[i] = expectRecords[i].Value\n\t}\n\tm := map[*Record]struct{}{}\n\tfor _, expectRecord := range expectRecords {\n\t\tm[expectRecord] = struct{}{}\n\t}\n\n\trequire.NoError(t, set.SAdd(key, values, expectRecords))\n\n\ttests := []struct {\n\t\tname string\n\t\tkey  string\n\t\tset  *Set\n\t\tok   bool\n\t}{\n\t\t{\"normal set SPop\", key, set, true},\n\t\t{\"normal set SPop\", key, set, true},\n\t\t{\"normal set SPop\", key, set, false},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\trecord := tt.set.SPop(tt.key)\n\t\t\t_, ok := m[record]\n\t\t\trequire.Equal(t, tt.ok, ok)\n\t\t})\n\t}\n}\n\nfunc TestSet_SIsMember(t *testing.T) {\n\tset := NewSet()\n\n\tkey := \"key\"\n\n\texpectRecords := generateRecords(1)\n\tvalues := [][]byte{expectRecords[0].Value}\n\n\trequire.NoError(t, set.SAdd(key, values, expectRecords))\n\ttests := []struct {\n\t\tkey       string\n\t\tval       []byte\n\t\tok        bool\n\t\texpectErr error\n\t}{\n\t\t{key, values[0], true, nil},\n\t\t{key, GetRandomBytes(24), false, nil},\n\t\t{\"fake key\", GetRandomBytes(24), false, ErrSetNotExist},\n\t}\n\tfor _, tt := range tests {\n\t\tok, err := set.SIsMember(tt.key, tt.val)\n\t\tif tt.expectErr != nil {\n\t\t\trequire.Equal(t, tt.expectErr, err)\n\t\t} else {\n\t\t\trequire.Equal(t, tt.ok, ok)\n\t\t}\n\t}\n}\n\nfunc TestSet_SAreMembers(t *testing.T) {\n\tset := NewSet()\n\n\tkey := \"set\"\n\n\texpectRecords := generateRecords(4)\n\tvalues := make([][]byte, 4)\n\tfor i := range expectRecords {\n\t\tvalues[i] = expectRecords[i].Value\n\t}\n\n\trequire.NoError(t, set.SAdd(key, values, expectRecords))\n\ttests := []struct {\n\t\tkey       string\n\t\tval       [][]byte\n\t\tok        bool\n\t\texpectErr error\n\t}{\n\t\t{key, values[0:2], true, nil},\n\t\t{key, values[2:], true, nil},\n\t\t{key, values, true, nil},\n\t\t{key, [][]byte{GetRandomBytes(24)}, false, nil},\n\t\t{\"fake key\", values, true, ErrSetNotExist},\n\t}\n\tfor _, tt := range tests {\n\t\tok, err := set.SAreMembers(tt.key, tt.val...)\n\t\tif tt.expectErr != nil {\n\t\t\trequire.Equal(t, tt.expectErr, err)\n\t\t} else {\n\t\t\trequire.Equal(t, tt.ok, ok)\n\t\t}\n\t}\n}\n\nfunc TestSet_SUnion(t *testing.T) {\n\tset := NewSet()\n\n\tkey1 := \"set1\"\n\tkey2 := \"set2\"\n\tkey3 := \"set3\"\n\tkey4 := \"set4\"\n\n\texpectRecords := generateRecords(10)\n\n\tvalues := make([][]byte, 10)\n\tfor i := range expectRecords {\n\t\tvalues[i] = expectRecords[i].Value\n\t}\n\n\trequire.NoError(t, set.SAdd(key1, values[:5], expectRecords[:5]))\n\trequire.NoError(t, set.SAdd(key2, values[2:6], expectRecords[2:6]))\n\trequire.NoError(t, set.SAdd(key3, values[2:7], expectRecords[2:7]))\n\trequire.NoError(t, set.SAdd(key4, values[7:], expectRecords[7:]))\n\n\ttype args struct {\n\t\tkey1 string\n\t\tkey2 string\n\t}\n\n\ttests := []struct {\n\t\tname    string\n\t\targs    args\n\t\tset     *Set\n\t\twant    []*Record\n\t\twantErr bool\n\t}{\n\t\t{\"normal set Union1\", args{key1, key4}, set,\n\t\t\t[]*Record{expectRecords[0], expectRecords[1], expectRecords[2], expectRecords[3], expectRecords[4], expectRecords[7], expectRecords[8], expectRecords[9]},\n\t\t\tfalse},\n\t\t{\n\t\t\t\"normal set Union2\", args{key2, key3}, set,\n\t\t\t[]*Record{expectRecords[2], expectRecords[3], expectRecords[4], expectRecords[5], expectRecords[6]},\n\t\t\tfalse,\n\t\t},\n\t\t{\"first fake set\", args{\"fake_key1\", key2}, set, nil, true},\n\t\t{\"second fake set\", args{key1, \"fake_key2\"}, set, nil, true},\n\t\t{\"two fake set\", args{\"fake_key1\", \"fake_key2\"}, set, nil, true},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tgot, err := tt.set.SUnion(tt.args.key1, tt.args.key2)\n\t\t\tif (err != nil) != tt.wantErr {\n\t\t\t\trequire.Errorf(t, err, \"Get() error = %v, wantErr %v\", tt.wantErr)\n\t\t\t}\n\t\t\trequire.ElementsMatchf(t, tt.want, got, \"Get() got = %v, want %v\", got, tt.want)\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "sorted_set.go",
          "type": "blob",
          "size": 19.2705078125,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"math/rand\"\n)\n\nvar (\n\tErrSortedSetNotFound = errors.New(\"the sortedSet does not exist\")\n\n\tErrSortedSetMemberNotExist = errors.New(\"the member of sortedSet does not exist\")\n\n\tErrSortedSetIsEmpty = errors.New(\"the sortedSet if empty\")\n)\n\nconst (\n\t// SkipListMaxLevel represents the skipList max level number.\n\tSkipListMaxLevel = 32\n\n\t// SkipListP represents the p parameter of the skipList.\n\tSkipListP = 0.25\n)\n\ntype SortedSet struct {\n\tdb *DB\n\tM  map[string]*SkipList\n}\n\nfunc NewSortedSet(db *DB) *SortedSet {\n\treturn &SortedSet{\n\t\tdb: db,\n\t\tM:  map[string]*SkipList{},\n\t}\n}\n\nfunc (z *SortedSet) ZAdd(key string, score SCORE, value []byte, record *Record) error {\n\tsortedSet, ok := z.M[key]\n\tif !ok {\n\t\tz.M[key] = newSkipList(z.db)\n\t\tsortedSet = z.M[key]\n\t}\n\n\treturn sortedSet.Put(score, value, record)\n}\n\nfunc (z *SortedSet) ZMembers(key string) (map[*Record]SCORE, error) {\n\tsortedSet, ok := z.M[key]\n\n\tif !ok {\n\t\treturn nil, ErrSortedSetNotFound\n\t}\n\n\tnodes := sortedSet.dict\n\n\tmembers := make(map[*Record]SCORE, len(nodes))\n\tfor _, node := range nodes {\n\t\tmembers[node.record] = node.score\n\t}\n\n\treturn members, nil\n}\n\nfunc (z *SortedSet) ZCard(key string) (int, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\treturn int(sortedSet.length), nil\n\t}\n\n\treturn 0, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZCount(key string, start SCORE, end SCORE, opts *GetByScoreRangeOptions) (int, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\treturn len(sortedSet.GetByScoreRange(start, end, opts)), nil\n\t}\n\treturn 0, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZPeekMax(key string) (*Record, SCORE, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\tnode := sortedSet.PeekMax()\n\t\tif node != nil {\n\t\t\treturn node.record, node.score, nil\n\t\t}\n\t\treturn nil, 0, ErrSortedSetIsEmpty\n\t}\n\n\treturn nil, 0, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZPopMax(key string) (*Record, SCORE, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\tnode := sortedSet.PopMax()\n\t\tif node != nil {\n\t\t\treturn node.record, node.score, nil\n\t\t}\n\t\treturn nil, 0, ErrSortedSetIsEmpty\n\t}\n\n\treturn nil, 0, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZPeekMin(key string) (*Record, SCORE, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\tnode := sortedSet.PeekMin()\n\t\tif node != nil {\n\t\t\treturn node.record, node.score, nil\n\t\t}\n\t\treturn nil, 0, ErrSortedSetIsEmpty\n\t}\n\n\treturn nil, 0, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZPopMin(key string) (*Record, SCORE, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\tnode := sortedSet.PopMin()\n\t\tif node != nil {\n\t\t\treturn node.record, node.score, nil\n\t\t}\n\t\treturn nil, 0, ErrSortedSetIsEmpty\n\t}\n\n\treturn nil, 0, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZRangeByScore(key string, start SCORE, end SCORE, opts *GetByScoreRangeOptions) ([]*Record, []float64, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\n\t\tnodes := sortedSet.GetByScoreRange(start, end, opts)\n\n\t\trecords := make([]*Record, len(nodes))\n\t\tscores := make([]float64, len(nodes))\n\n\t\tfor i, node := range nodes {\n\t\t\trecords[i] = node.record\n\t\t\tscores[i] = float64(node.score)\n\t\t}\n\n\t\treturn records, scores, nil\n\t}\n\n\treturn nil, nil, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZRangeByRank(key string, start int, end int) ([]*Record, []float64, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\n\t\tnodes := sortedSet.GetByRankRange(start, end, false)\n\n\t\trecords := make([]*Record, len(nodes))\n\t\tscores := make([]float64, len(nodes))\n\n\t\tfor i, node := range nodes {\n\t\t\trecords[i] = node.record\n\t\t\tscores[i] = float64(node.score)\n\t\t}\n\n\t\treturn records, scores, nil\n\t}\n\n\treturn nil, nil, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZRem(key string, value []byte) (*Record, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\thash, err := getFnv32(value)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tnode := sortedSet.Remove(hash)\n\t\tif node != nil {\n\t\t\treturn node.record, nil\n\t\t}\n\t\treturn nil, ErrSortedSetMemberNotExist\n\t}\n\n\treturn nil, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZRemRangeByRank(key string, start int, end int) error {\n\tif sortedSet, ok := z.M[key]; ok {\n\n\t\t_ = sortedSet.GetByRankRange(start, end, true)\n\t\treturn nil\n\t}\n\n\treturn ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) getZRemRangeByRankNodes(key string, start int, end int) ([]*SkipListNode, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\treturn sortedSet.GetByRankRange(start, end, false), nil\n\t}\n\n\treturn []*SkipListNode{}, nil\n}\n\nfunc (z *SortedSet) ZRank(key string, value []byte) (int, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\thash, err := getFnv32(value)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\trank := sortedSet.FindRank(hash)\n\t\tif rank == 0 {\n\t\t\treturn 0, ErrSortedSetMemberNotExist\n\t\t}\n\t\treturn rank, nil\n\t}\n\treturn 0, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZRevRank(key string, value []byte) (int, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\thash, err := getFnv32(value)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\trank := sortedSet.FindRevRank(hash)\n\t\tif rank == 0 {\n\t\t\treturn 0, ErrSortedSetMemberNotExist\n\t\t}\n\t\treturn rank, nil\n\t}\n\treturn 0, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZScore(key string, value []byte) (float64, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\tnode := sortedSet.GetByValue(value)\n\t\tif node != nil {\n\t\t\treturn float64(sortedSet.GetByValue(value).score), nil\n\t\t}\n\t\treturn 0, ErrSortedSetMemberNotExist\n\t}\n\treturn 0, ErrSortedSetNotFound\n}\n\nfunc (z *SortedSet) ZExist(key string, value []byte) (bool, error) {\n\tif sortedSet, ok := z.M[key]; ok {\n\t\thash, err := getFnv32(value)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\t\t_, ok := sortedSet.dict[hash]\n\t\treturn ok, nil\n\t}\n\treturn false, ErrSortedSetNotFound\n}\n\n// SCORE represents the score type.\ntype SCORE float64\n\n// SkipListLevel records forward and span.\ntype SkipListLevel struct {\n\tforward *SkipListNode\n\tspan    int64\n}\n\n// The SkipList represents the sorted set.\ntype SkipList struct {\n\tdb     *DB\n\theader *SkipListNode\n\ttail   *SkipListNode\n\tlength int64\n\tlevel  int\n\tdict   map[uint32]*SkipListNode\n}\n\n// SkipListNode represents a node in the SkipList.\ntype SkipListNode struct {\n\thash     uint32  // unique key of this node\n\trecord   *Record // associated data\n\tscore    SCORE   // score to determine the order of this node in the set\n\tbackward *SkipListNode\n\tlevel    []SkipListLevel\n}\n\n// Hash returns the key of the node.\nfunc (sln *SkipListNode) Hash() uint32 {\n\treturn sln.hash\n}\n\n// Score returns the score of the node.\nfunc (sln *SkipListNode) Score() SCORE {\n\treturn sln.score\n}\n\n// createNode returns a newly initialized SkipListNode Object that implements the SkipListNode.\nfunc createNode(level int, score SCORE, hash uint32, record *Record) *SkipListNode {\n\tnode := SkipListNode{\n\t\thash:   hash,\n\t\trecord: record,\n\t\tscore:  score,\n\t\tlevel:  make([]SkipListLevel, level),\n\t}\n\treturn &node\n}\n\n// randomLevel returns a random level for the new skiplist node we are going to create.\n// The return value of this function is between 1 and SkipListMaxLevel\n// (both inclusive), with a powerlaw-alike distribution where higher\n// levels are lesl likely to be returned.\nfunc randomLevel() int {\n\tlevel := 1\n\n\tfor float64(rand.Int31()&0xFFFF) < SkipListP*0xFFFF {\n\t\tlevel += 1\n\t}\n\tif level < SkipListMaxLevel {\n\t\treturn level\n\t}\n\n\treturn SkipListMaxLevel\n}\n\nfunc newSkipList(db *DB) *SkipList {\n\tskipList := &SkipList{\n\t\tdb:    db,\n\t\tlevel: 1,\n\t\tdict:  make(map[uint32]*SkipListNode),\n\t}\n\thash, _ := getFnv32([]byte(\"\"))\n\tskipList.header = createNode(SkipListMaxLevel, 0, hash, nil)\n\treturn skipList\n}\n\nfunc (sl *SkipList) cmp(r1 *Record, r2 *Record) int {\n\tval1, _ := sl.db.getValueByRecord(r1)\n\tval2, _ := sl.db.getValueByRecord(r2)\n\treturn bytes.Compare(val1, val2)\n}\n\nfunc (sl *SkipList) insertNode(score SCORE, hash uint32, record *Record) *SkipListNode {\n\tvar update [SkipListMaxLevel]*SkipListNode\n\tvar rank [SkipListMaxLevel]int64\n\n\tx := sl.header\n\tfor i := sl.level - 1; i >= 0; i-- {\n\t\t// store rank that is crosled to reach the insert position\n\t\tif sl.level-1 == i {\n\t\t\trank[i] = 0\n\t\t} else {\n\t\t\trank[i] = rank[i+1]\n\t\t}\n\n\t\tfor x.level[i].forward != nil &&\n\t\t\t(x.level[i].forward.score < score ||\n\t\t\t\t(x.level[i].forward.score == score && // score is the same but the key is different\n\t\t\t\t\tsl.cmp(x.level[i].forward.record, record) < 0)) {\n\t\t\trank[i] += x.level[i].span\n\t\t\tx = x.level[i].forward\n\t\t}\n\n\t\tupdate[i] = x\n\t}\n\n\t/* we assume the key is not already inside, since we allow duplicated\n\t * scores, and the re-insertion of score and redis object should never\n\t * happen since the caller of Insert() should test in the hash table\n\t * if the element is already inside or not. */\n\tlevel := randomLevel()\n\n\tif level > sl.level { // add a new level\n\t\tfor i := sl.level; i < level; i++ {\n\t\t\trank[i] = 0\n\t\t\tupdate[i] = sl.header\n\t\t\tupdate[i].level[i].span = sl.length\n\t\t}\n\t\tsl.level = level\n\t}\n\n\tx = createNode(level, score, hash, record)\n\tfor i := 0; i < level; i++ {\n\t\tx.level[i].forward = update[i].level[i].forward\n\t\tupdate[i].level[i].forward = x\n\n\t\t/* update span covered by update[i] as x is inserted here */\n\t\tx.level[i].span = update[i].level[i].span - (rank[0] - rank[i])\n\n\t\tupdate[i].level[i].span = (rank[0] - rank[i]) + 1\n\t}\n\n\t// increment span for untouched levels\n\tfor i := level; i < sl.level; i++ {\n\t\tupdate[i].level[i].span++\n\t}\n\n\tif update[0] == sl.header {\n\t\tx.backward = nil\n\t} else {\n\t\tx.backward = update[0]\n\t}\n\n\tif x.level[0].forward != nil {\n\t\tx.level[0].forward.backward = x\n\t} else {\n\t\tsl.tail = x\n\t}\n\n\tsl.length++\n\n\treturn x\n}\n\n// deleteNode represents internal function used by delete, DeleteByScore and DeleteByRank.\nfunc (sl *SkipList) deleteNode(x *SkipListNode, update [SkipListMaxLevel]*SkipListNode) {\n\tfor i := 0; i < sl.level; i++ {\n\t\tif update[i].level[i].forward == x {\n\t\t\tupdate[i].level[i].span += x.level[i].span - 1\n\t\t\tupdate[i].level[i].forward = x.level[i].forward\n\t\t} else {\n\t\t\tupdate[i].level[i].span -= 1\n\t\t}\n\t}\n\tif x.level[0].forward != nil {\n\t\tx.level[0].forward.backward = x.backward\n\t} else {\n\t\tsl.tail = x.backward\n\t}\n\tfor sl.level > 1 && sl.header.level[sl.level-1].forward == nil {\n\t\tsl.level--\n\t}\n\tsl.length--\n\tdelete(sl.dict, x.hash)\n}\n\n// delete removes an element with matching score/key from the skiplist.\nfunc (sl *SkipList) delete(score SCORE, hash uint32) bool {\n\tvar update [SkipListMaxLevel]*SkipListNode\n\n\ttargetNode := sl.dict[hash]\n\n\tx := sl.header\n\tfor i := sl.level - 1; i >= 0; i-- {\n\t\tfor x.level[i].forward != nil &&\n\t\t\t(x.level[i].forward.score < score ||\n\t\t\t\t(x.level[i].forward.score == score &&\n\t\t\t\t\tsl.cmp(x.level[i].forward.record, targetNode.record) < 0)) {\n\t\t\tx = x.level[i].forward\n\t\t}\n\t\tupdate[i] = x\n\t}\n\t/* We may have multiple elements with the same score, what we need\n\t * is to find the element with both the right score and object. */\n\tx = x.level[0].forward\n\tif x != nil && score == x.score && sl.cmp(x.record, targetNode.record) == 0 {\n\t\tsl.deleteNode(x, update)\n\t\t// free x\n\t\treturn true\n\t}\n\treturn false /* not found */\n}\n\n// Size returns the number of elements in the SkipList.\nfunc (sl *SkipList) Size() int {\n\treturn int(sl.length)\n}\n\n// PeekMin returns the element with minimum score, nil if the set is empty.\n//\n// Time complexity of this method is : O(log(N)).\nfunc (sl *SkipList) PeekMin() *SkipListNode {\n\treturn sl.header.level[0].forward\n}\n\n// PopMin returns and remove the element with minimal score, nil if the set is empty.\n//\n// Time complexity of this method is : O(log(N)).\nfunc (sl *SkipList) PopMin() *SkipListNode {\n\tx := sl.header.level[0].forward\n\tif x != nil {\n\t\tsl.Remove(x.hash)\n\t}\n\treturn x\n}\n\n// PeekMax returns the element with maximum score, nil if the set is empty.\n//\n// Time Complexity : O(1).\nfunc (sl *SkipList) PeekMax() *SkipListNode {\n\treturn sl.tail\n}\n\n// PopMax returns and remove the element with maximum score, nil if the set is empty.\n//\n// Time complexity of this method is : O(log(N)).\nfunc (sl *SkipList) PopMax() *SkipListNode {\n\tx := sl.tail\n\tif x != nil {\n\t\tsl.Remove(x.hash)\n\t}\n\treturn x\n}\n\n// Put puts an element into the sorted set with specific key / value / score.\n//\n// Time complexity of this method is : O(log(N)).\nfunc (sl *SkipList) Put(score SCORE, value []byte, record *Record) error {\n\tvar newNode *SkipListNode\n\n\thash, _ := getFnv32(value)\n\n\tif n, ok := sl.dict[hash]; ok {\n\t\t// score does not change, only update value\n\t\tif n.score != score { // score changes, delete and re-insert\n\t\t\tsl.delete(n.score, n.hash)\n\t\t\tnewNode = sl.insertNode(score, hash, record)\n\t\t}\n\t} else {\n\t\tnewNode = sl.insertNode(score, hash, record)\n\t}\n\n\tif newNode != nil {\n\t\tsl.dict[hash] = newNode\n\t}\n\n\treturn nil\n}\n\n// Remove removes element specified at given key.\n//\n// Time complexity of this method is : O(log(N)).\nfunc (sl *SkipList) Remove(hash uint32) *SkipListNode {\n\tfound := sl.dict[hash]\n\tif found != nil {\n\t\tsl.delete(found.score, hash)\n\t\treturn found\n\t}\n\treturn nil\n}\n\n// GetByScoreRangeOptions represents the options of the GetByScoreRange function.\ntype GetByScoreRangeOptions struct {\n\tLimit        int  // limit the max nodes to return\n\tExcludeStart bool // exclude start value, so it search in interval (start, end] or (start, end)\n\tExcludeEnd   bool // exclude end value, so it search in interval [start, end) or (start, end)\n}\n\n// GetByScoreRange returns the nodes whose score within the specific range.\n// If options is nil, it searches in interval [start, end] without any limit by default.\n//\n// Time complexity of this method is : O(log(N)).\nfunc (sl *SkipList) GetByScoreRange(start SCORE, end SCORE, options *GetByScoreRangeOptions) []*SkipListNode {\n\tlimit := 1<<31 - 1\n\tif options != nil && options.Limit > 0 {\n\t\tlimit = options.Limit\n\t}\n\n\texcludeStart := options != nil && options.ExcludeStart\n\texcludeEnd := options != nil && options.ExcludeEnd\n\treverse := start > end\n\tif reverse {\n\t\tstart, end = end, start\n\t\texcludeStart, excludeEnd = excludeEnd, excludeStart\n\t}\n\n\tvar nodes []*SkipListNode\n\n\t// determine if out of range\n\tif sl.length == 0 {\n\t\treturn nodes\n\t}\n\n\tif reverse {\n\t\t// search from end to start\n\t\treturn sl.searchReverse(nodes, excludeStart, excludeEnd, start, end, limit)\n\t}\n\t// search from start to end\n\treturn sl.searchForward(nodes, excludeStart, excludeEnd, start, end, limit)\n}\n\nfunc (sl *SkipList) searchForward(nodes []*SkipListNode, excludeStart, excludeEnd bool, start, end SCORE, limit int) []*SkipListNode {\n\t// search from start to end\n\tx := sl.header\n\tif excludeStart {\n\t\tfor i := sl.level - 1; i >= 0; i-- {\n\t\t\tfor x.level[i].forward != nil &&\n\t\t\t\tx.level[i].forward.score <= start {\n\t\t\t\tx = x.level[i].forward\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfor i := sl.level - 1; i >= 0; i-- {\n\t\t\tfor x.level[i].forward != nil &&\n\t\t\t\tx.level[i].forward.score < start {\n\t\t\t\tx = x.level[i].forward\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Current node is the last with score < or <= start. */\n\tx = x.level[0].forward\n\n\tfor x != nil && limit > 0 {\n\t\tif excludeEnd {\n\t\t\tif x.score >= end {\n\t\t\t\tbreak\n\t\t\t}\n\t\t} else {\n\t\t\tif x.score > end {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\tnext := x.level[0].forward\n\n\t\tnodes = append(nodes, x)\n\t\tlimit--\n\n\t\tx = next\n\t}\n\n\treturn nodes\n}\n\nfunc (sl *SkipList) searchReverse(nodes []*SkipListNode, excludeStart, excludeEnd bool, start, end SCORE, limit int) []*SkipListNode {\n\tx := sl.header\n\n\tif excludeEnd {\n\t\tfor i := sl.level - 1; i >= 0; i-- {\n\t\t\tfor x.level[i].forward != nil &&\n\t\t\t\tx.level[i].forward.score < end {\n\t\t\t\tx = x.level[i].forward\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfor i := sl.level - 1; i >= 0; i-- {\n\t\t\tfor x.level[i].forward != nil &&\n\t\t\t\tx.level[i].forward.score <= end {\n\t\t\t\tx = x.level[i].forward\n\t\t\t}\n\t\t}\n\t}\n\n\tfor x != nil && limit > 0 {\n\t\tif excludeStart {\n\t\t\tif x.score <= start {\n\t\t\t\tbreak\n\t\t\t}\n\t\t} else {\n\t\t\tif x.score < start {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\tnext := x.backward\n\n\t\tnodes = append(nodes, x)\n\t\tlimit--\n\n\t\tx = next\n\t}\n\n\treturn nodes\n}\n\n// GetByRankRange returns nodes within specific rank range [start, end].\n// Note that the rank is 1-based integer. Rank 1 means the first node; Rank -1 means the last node\n// If start is greater than end, the returned array is in reserved order\n// If remove is true, the returned nodes are removed.\n//\n// Time complexity of this method is : O(log(N)).\nfunc (sl *SkipList) GetByRankRange(start, end int, remove bool) []*SkipListNode {\n\tvar (\n\t\tupdate    [SkipListMaxLevel]*SkipListNode\n\t\tnodes     []*SkipListNode\n\t\ttraversed int\n\t)\n\n\tstart, end = sl.sanitizeIndexes(start, end)\n\n\treverse := start > end\n\tif reverse { // swap start and end\n\t\tstart, end = end, start\n\t}\n\n\ttraversed = 0\n\tx := sl.header\n\tfor i := sl.level - 1; i >= 0; i-- {\n\t\tfor x.level[i].forward != nil &&\n\t\t\ttraversed+int(x.level[i].span) < start {\n\t\t\ttraversed += int(x.level[i].span)\n\t\t\tx = x.level[i].forward\n\t\t}\n\t\tif remove {\n\t\t\tupdate[i] = x\n\t\t} else {\n\t\t\tif traversed+1 == start {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\ttraversed++\n\tx = x.level[0].forward\n\tfor x != nil && traversed <= end {\n\t\tnext := x.level[0].forward\n\n\t\tnodes = append(nodes, x)\n\n\t\tif remove {\n\t\t\tsl.deleteNode(x, update)\n\t\t}\n\n\t\ttraversed++\n\t\tx = next\n\t}\n\n\tif reverse {\n\t\tfor i, j := 0, len(nodes)-1; i < j; i, j = i+1, j-1 {\n\t\t\tnodes[i], nodes[j] = nodes[j], nodes[i]\n\t\t}\n\t}\n\treturn nodes\n}\n\nfunc (sl *SkipList) sanitizeIndexes(start, end int) (newStart, newEnd int) {\n\tif start < 0 {\n\t\tstart = int(sl.length) + start + 1\n\t}\n\tif end < 0 {\n\t\tend = int(sl.length) + end + 1\n\t}\n\tif start <= 0 {\n\t\tstart = 1\n\t}\n\tif end <= 0 {\n\t\tend = 1\n\t}\n\n\treturn start, end\n}\n\n// GetByRank returns the node at given rank.\n// Note that the rank is 1-based integer. Rank 1 means the first node; Rank -1 means the last node.\n// If remove is true, the returned nodes are removed\n// If node is not found at specific rank, nil is returned.\n//\n// Time complexity of this method is : O(log(N)).\nfunc (sl *SkipList) GetByRank(rank int, remove bool) *SkipListNode {\n\tnodes := sl.GetByRankRange(rank, rank, remove)\n\tif len(nodes) == 1 {\n\t\treturn nodes[0]\n\t}\n\treturn nil\n}\n\n// GetByValue returns the node at given key.\n// If node is not found, nil is returned\n//\n// Time complexity : O(1).\nfunc (sl *SkipList) GetByValue(value []byte) *SkipListNode {\n\thash, _ := getFnv32(value)\n\treturn sl.dict[hash]\n}\n\n// FindRank Returns the rank of member in the sorted set stored at key, with the scores ordered from low to high.\n// Note that the rank is 1-based integer. Rank 1 means the first node\n// If the node is not found, 0 is returned. Otherwise rank(> 0) is returned.\n//\n// Time complexity of this method is : O(log(N)).\nfunc (sl *SkipList) FindRank(hash uint32) int {\n\trank := 0\n\ttargetNode := sl.dict[hash]\n\tif targetNode != nil {\n\t\tx := sl.header\n\t\tfor i := sl.level - 1; i >= 0; i-- {\n\t\t\tfor x.level[i].forward != nil &&\n\t\t\t\t(x.level[i].forward.score < targetNode.score ||\n\t\t\t\t\t(x.level[i].forward.score == targetNode.score &&\n\t\t\t\t\t\tsl.cmp(x.level[i].forward.record, targetNode.record) <= 0)) {\n\t\t\t\trank += int(x.level[i].span)\n\t\t\t\tx = x.level[i].forward\n\t\t\t}\n\n\t\t\tif x.hash == hash {\n\t\t\t\treturn rank\n\t\t\t}\n\t\t}\n\t}\n\treturn 0\n}\n\n// FindRevRank Returns the rank of member in the sorted set stored at key, with the scores ordered from high to low.\nfunc (sl *SkipList) FindRevRank(hash uint32) int {\n\tif sl.length == 0 {\n\t\treturn 0\n\t}\n\n\tif _, ok := sl.dict[hash]; !ok {\n\t\treturn 0\n\t}\n\n\treturn sl.Size() - sl.FindRank(hash) + 1\n}\n"
        },
        {
          "name": "tar.go",
          "type": "blob",
          "size": 2.333984375,
          "content": "// Copyright 2021 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"archive/tar\"\n\t\"compress/gzip\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n)\n\nfunc tarGZCompress(dst io.Writer, src string) error {\n\tgz := gzip.NewWriter(dst)\n\tdefer gz.Close()\n\treturn tarCompress(gz, src)\n}\n\n// https://blog.ralch.com/articles/golang-working-with-tar-and-gzip\nfunc tarCompress(dst io.Writer, src string) error {\n\ttarball := tar.NewWriter(dst)\n\tdefer tarball.Close()\n\n\tinfo, err := os.Stat(src)\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\tvar baseDir string\n\tif info.IsDir() {\n\t\tbaseDir = filepath.Base(src)\n\t}\n\n\treturn filepath.Walk(src,\n\t\tfunc(path string, info os.FileInfo, err error) error {\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\theader, err := tar.FileInfoHeader(info, info.Name())\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tif baseDir != \"\" {\n\t\t\t\theader.Name = filepath.Join(baseDir, strings.TrimPrefix(path, src))\n\t\t\t}\n\n\t\t\tif err := tarball.WriteHeader(header); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tif info.IsDir() {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\tfile, err := os.Open(filepath.Clean(path))\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tdefer file.Close()\n\t\t\t_, err = io.Copy(tarball, file)\n\t\t\treturn err\n\t\t})\n}\n\nfunc tarDecompress(dst string, src io.Reader) error {\n\ttarReader := tar.NewReader(src)\n\n\tfor {\n\t\theader, err := tarReader.Next()\n\t\tif err == io.EOF {\n\t\t\tbreak\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tpath := filepath.Join(dst, header.Name)\n\t\tinfo := header.FileInfo()\n\t\tif info.IsDir() {\n\t\t\tif err = os.MkdirAll(path, info.Mode()); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\tfile, err := os.OpenFile(filepath.Clean(path), os.O_CREATE|os.O_TRUNC|os.O_WRONLY, info.Mode())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer file.Close()\n\t\t_, err = io.Copy(file, tarReader)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n"
        },
        {
          "name": "test_utils.go",
          "type": "blob",
          "size": 0.927734375,
          "content": "// Copyright 2023 The PromiseDB Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"fmt\"\n\t\"math/rand\"\n)\n\nconst charset = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n\nfunc GetTestBytes(i int) []byte {\n\treturn []byte(fmt.Sprintf(\"nutsdb-%09d\", i))\n}\n\nfunc GetRandomBytes(length int) []byte {\n\tb := make([]byte, length)\n\tfor i := range b {\n\t\tb[i] = charset[rand.Intn(len(charset))]\n\t}\n\treturn b\n}\n"
        },
        {
          "name": "throttle.go",
          "type": "blob",
          "size": 1.6513671875,
          "content": "package nutsdb\n\nimport (\n\t\"sync\"\n)\n\n// Throttle allows a limited number of workers to run at a time. It also\n// provides a mechanism to check for errors encountered by workers and wait for\n// them to finish.\ntype Throttle struct {\n\tonce      sync.Once\n\twg        sync.WaitGroup\n\tch        chan struct{}\n\terrCh     chan error\n\tfinishErr error\n}\n\n// NewThrottle creates a new throttle with a max number of workers.\nfunc NewThrottle(max int) *Throttle {\n\treturn &Throttle{\n\t\tch:    make(chan struct{}, max),\n\t\terrCh: make(chan error, max),\n\t}\n}\n\n// Do should be called by workers before they start working. It blocks if there\n// are already maximum number of workers working. If it detects an error from\n// previously Done workers, it would return it.\nfunc (t *Throttle) Do() error {\n\tfor {\n\t\tselect {\n\t\tcase t.ch <- struct{}{}:\n\t\t\tt.wg.Add(1)\n\t\t\treturn nil\n\t\tcase err := <-t.errCh:\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Done should be called by workers when they finish working. They can also\n// pass the error status of work done.\nfunc (t *Throttle) Done(err error) {\n\tif err != nil {\n\t\tt.errCh <- err\n\t}\n\tselect {\n\tcase <-t.ch:\n\tdefault:\n\t\tpanic(\"Throttle Do Done mismatch\")\n\t}\n\tt.wg.Done()\n}\n\n// Finish waits until all workers have finished working. It would return any error passed by Done.\n// If Finish is called multiple time, it will wait for workers to finish only once(first time).\n// From next calls, it will return same error as found on first call.\nfunc (t *Throttle) Finish() error {\n\tt.once.Do(func() {\n\t\tt.wg.Wait()\n\t\tclose(t.ch)\n\t\tclose(t.errCh)\n\t\tfor err := range t.errCh {\n\t\t\tif err != nil {\n\t\t\t\tt.finishErr = err\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t})\n\n\treturn t.finishErr\n}\n"
        },
        {
          "name": "ttl_manager.go",
          "type": "blob",
          "size": 2.3427734375,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"time\"\n\n\t\"github.com/antlabs/timer\"\n)\n\ntype nodesInBucket map[string]timer.TimeNoder // key to timer node\n\nfunc newNodesInBucket() nodesInBucket {\n\treturn make(map[string]timer.TimeNoder)\n}\n\ntype nodes map[BucketId]nodesInBucket // bucket to nodes that in a bucket\n\nfunc (n nodes) getNode(bucketId BucketId, key string) (timer.TimeNoder, bool) {\n\tnib, ok := n[bucketId]\n\tif !ok {\n\t\treturn nil, false\n\t}\n\tnode, ok := nib[key]\n\treturn node, ok\n}\n\nfunc (n nodes) addNode(bucketId BucketId, key string, node timer.TimeNoder) {\n\tnib, ok := n[bucketId]\n\tif !ok {\n\t\tnib = newNodesInBucket()\n\t\tn[bucketId] = nib\n\t}\n\tnib[key] = node\n}\n\nfunc (n nodes) delNode(bucketId BucketId, key string) {\n\tif nib, ok := n[bucketId]; ok {\n\t\tdelete(nib, key)\n\t}\n}\n\ntype ttlManager struct {\n\tt          timer.Timer\n\ttimerNodes nodes\n}\n\nfunc newTTLManager(expiredDeleteType ExpiredDeleteType) *ttlManager {\n\tvar t timer.Timer\n\n\tswitch expiredDeleteType {\n\tcase TimeWheel:\n\t\tt = timer.NewTimer(timer.WithTimeWheel())\n\tcase TimeHeap:\n\t\tt = timer.NewTimer(timer.WithMinHeap())\n\tdefault:\n\t\tt = timer.NewTimer()\n\t}\n\n\treturn &ttlManager{\n\t\tt:          t,\n\t\ttimerNodes: make(nodes),\n\t}\n}\n\nfunc (tm *ttlManager) run() {\n\ttm.t.Run()\n}\n\nfunc (tm *ttlManager) exist(bucketId BucketId, key string) bool {\n\t_, ok := tm.timerNodes.getNode(bucketId, key)\n\treturn ok\n}\n\nfunc (tm *ttlManager) add(bucketId BucketId, key string, expire time.Duration, callback func()) {\n\tif node, ok := tm.timerNodes.getNode(bucketId, key); ok {\n\t\tnode.Stop()\n\t}\n\n\tnode := tm.t.AfterFunc(expire, callback)\n\ttm.timerNodes.addNode(bucketId, key, node)\n}\n\nfunc (tm *ttlManager) del(bucket BucketId, key string) {\n\ttm.timerNodes.delNode(bucket, key)\n}\n\nfunc (tm *ttlManager) close() {\n\ttm.timerNodes = nil\n\ttm.t.Stop()\n}\n"
        },
        {
          "name": "tx.go",
          "type": "blob",
          "size": 20.5634765625,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\t\"sync/atomic\"\n\n\t\"github.com/bwmarrin/snowflake\"\n\t\"github.com/xujiajun/utils/strconv2\"\n)\n\nconst (\n\t// txStatusRunning means the tx is running\n\ttxStatusRunning = 1\n\t// txStatusCommitting means the tx is committing\n\ttxStatusCommitting = 2\n\t// txStatusClosed means the tx is closed, ether committed or rollback\n\ttxStatusClosed = 3\n)\n\n// Tx represents a transaction.\ntype Tx struct {\n\tid                uint64\n\tdb                *DB\n\twritable          bool\n\tstatus            atomic.Value\n\tpendingWrites     *pendingEntryList\n\tsize              int64\n\tpendingBucketList pendingBucketList\n}\n\ntype txnCb struct {\n\tcommit func() error\n\tuser   func(error)\n\terr    error\n}\n\nfunc (tx *Tx) submitEntry(ds uint16, bucket string, e *Entry) {\n\ttx.pendingWrites.submitEntry(ds, bucket, e)\n}\n\nfunc runTxnCallback(cb *txnCb) {\n\tswitch {\n\tcase cb == nil:\n\t\tpanic(\"tx callback is nil\")\n\tcase cb.user == nil:\n\t\tpanic(\"Must have caught a nil callback for tx.CommitWith\")\n\tcase cb.err != nil:\n\t\tcb.user(cb.err)\n\tcase cb.commit != nil:\n\t\terr := cb.commit()\n\t\tcb.user(err)\n\tdefault:\n\t\tcb.user(nil)\n\t}\n}\n\n// Begin opens a new transaction.\n// Multiple read-only transactions can be opened at the same time but there can\n// only be one read/write transaction at a time. Attempting to open a read/write\n// transactions while another one is in progress will result in blocking until\n// the current read/write transaction is completed.\n// All transactions must be closed by calling Commit() or Rollback() when done.\nfunc (db *DB) Begin(writable bool) (tx *Tx, err error) {\n\ttx, err = newTx(db, writable)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ttx.lock()\n\ttx.setStatusRunning()\n\tif db.closed {\n\t\ttx.unlock()\n\t\ttx.setStatusClosed()\n\t\treturn nil, ErrDBClosed\n\t}\n\n\treturn\n}\n\n// newTx returns a newly initialized Tx object at given writable.\nfunc newTx(db *DB, writable bool) (tx *Tx, err error) {\n\tvar txID uint64\n\n\ttx = &Tx{\n\t\tdb:                db,\n\t\twritable:          writable,\n\t\tpendingWrites:     newPendingEntriesList(),\n\t\tpendingBucketList: make(map[Ds]map[BucketName]*Bucket),\n\t}\n\n\ttxID, err = tx.getTxID()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ttx.id = txID\n\n\treturn\n}\n\nfunc (tx *Tx) CommitWith(cb func(error)) {\n\tif cb == nil {\n\t\tpanic(\"Nil callback provided to CommitWith\")\n\t}\n\n\tif tx.pendingWrites.size == 0 {\n\t\t// Do not run these callbacks from here, because the CommitWith and the\n\t\t// callback might be acquiring the same locks. Instead run the callback\n\t\t// from another goroutine.\n\t\tgo runTxnCallback(&txnCb{user: cb, err: nil})\n\t\treturn\n\t}\n\t// defer tx.setStatusClosed()  //must not add this code because another process is also accessing tx\n\tcommitCb, err := tx.commitAndSend()\n\tif err != nil {\n\t\tgo runTxnCallback(&txnCb{user: cb, err: err})\n\t\treturn\n\t}\n\n\tgo runTxnCallback(&txnCb{user: cb, commit: commitCb})\n}\n\nfunc (tx *Tx) commitAndSend() (func() error, error) {\n\treq, err := tx.db.sendToWriteCh(tx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tret := func() error {\n\t\terr := req.Wait()\n\t\treturn err\n\t}\n\n\treturn ret, nil\n}\n\nfunc (tx *Tx) checkSize() error {\n\tcount := tx.pendingWrites.size\n\tif int64(count) >= tx.db.getMaxBatchCount() || tx.size >= tx.db.getMaxBatchSize() {\n\t\treturn ErrTxnTooBig\n\t}\n\n\treturn nil\n}\n\n// getTxID returns the tx id.\nfunc (tx *Tx) getTxID() (id uint64, err error) {\n\tnode, err := snowflake.NewNode(tx.db.opt.NodeNum)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tid = uint64(node.Generate().Int64())\n\n\treturn\n}\n\n// Commit commits the transaction, following these steps:\n//\n// 1. check the length of pendingWrites.If there are no writes, return immediately.\n//\n// 2. check if the ActiveFile has not enough space to store entry. if not, call rotateActiveFile function.\n//\n// 3. write pendingWrites to disk, if a non-nil error,return the error.\n//\n// 4. build Hint index.\n//\n// 5. Unlock the database and clear the db field.\nfunc (tx *Tx) Commit() (err error) {\n\tdefer func() {\n\t\tif err != nil {\n\t\t\ttx.handleErr(err)\n\t\t}\n\t\ttx.unlock()\n\t\ttx.db = nil\n\n\t\ttx.pendingWrites = nil\n\t}()\n\tif tx.isClosed() {\n\t\treturn ErrCannotCommitAClosedTx\n\t}\n\n\tif tx.db == nil {\n\t\ttx.setStatusClosed()\n\t\treturn ErrDBClosed\n\t}\n\n\tvar curWriteCount int64\n\tif tx.db.opt.MaxWriteRecordCount > 0 {\n\t\tcurWriteCount, err = tx.getNewAddRecordCount()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// judge all write records is whether more than the MaxWriteRecordCount\n\t\tif tx.db.RecordCount+curWriteCount > tx.db.opt.MaxWriteRecordCount {\n\t\t\treturn ErrTxnExceedWriteLimit\n\t\t}\n\t}\n\n\ttx.setStatusCommitting()\n\tdefer tx.setStatusClosed()\n\n\twritesBucketLen := len(tx.pendingBucketList)\n\tif tx.pendingWrites.size == 0 && writesBucketLen == 0 {\n\t\treturn nil\n\t}\n\n\tbuff := tx.allocCommitBuffer()\n\tdefer tx.db.commitBuffer.Reset()\n\n\tvar records []*Record\n\n\tpendingWriteList := tx.pendingWrites.toList()\n\tlastIndex := len(pendingWriteList) - 1\n\tfor i := 0; i < len(pendingWriteList); i++ {\n\t\tentry := pendingWriteList[i]\n\t\tentrySize := entry.Size()\n\t\tif entrySize > tx.db.opt.SegmentSize {\n\t\t\treturn ErrDataSizeExceed\n\t\t}\n\n\t\tif tx.db.ActiveFile.ActualSize+int64(buff.Len())+entrySize > tx.db.opt.SegmentSize {\n\t\t\tif _, err := tx.writeData(buff.Bytes()); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tbuff.Reset()\n\n\t\t\tif err := tx.rotateActiveFile(); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\toffset := tx.db.ActiveFile.writeOff + int64(buff.Len())\n\n\t\tif i == lastIndex {\n\t\t\tentry.Meta.Status = Committed\n\t\t}\n\n\t\tif _, err := buff.Write(entry.Encode()); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif i == lastIndex {\n\t\t\tif _, err := tx.writeData(buff.Bytes()); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\trecord := tx.db.createRecordByModeWithFidAndOff(tx.db.ActiveFile.fileID, uint64(offset), entry)\n\n\t\t// add to cache\n\t\tif tx.db.getHintKeyAndRAMIdxCacheSize() > 0 && tx.db.opt.EntryIdxMode == HintKeyAndRAMIdxMode {\n\t\t\ttx.db.hintKeyAndRAMIdxModeLru.Add(record, entry)\n\t\t}\n\n\t\trecords = append(records, record)\n\t}\n\n\tif err := tx.SubmitBucket(); err != nil {\n\t\treturn err\n\t}\n\n\tif err := tx.buildIdxes(records, pendingWriteList); err != nil {\n\t\treturn err\n\t}\n\ttx.db.RecordCount += curWriteCount\n\n\tif err := tx.buildBucketInIndex(); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\nfunc (tx *Tx) getNewAddRecordCount() (int64, error) {\n\tvar res int64\n\tchangeCountInEntries, err := tx.getChangeCountInEntriesChanges()\n\tchangeCountInBucket := tx.getChangeCountInBucketChanges()\n\tres += changeCountInEntries\n\tres += changeCountInBucket\n\treturn res, err\n}\n\nfunc (tx *Tx) getListHeadTailSeq(bucketId BucketId, key string) *HeadTailSeq {\n\tres := HeadTailSeq{Head: initialListSeq, Tail: initialListSeq + 1}\n\tif _, ok := tx.db.Index.list.idx[bucketId]; ok {\n\t\tif _, ok := tx.db.Index.list.idx[bucketId].Seq[key]; ok {\n\t\t\tres = *tx.db.Index.list.idx[bucketId].Seq[key]\n\t\t}\n\t}\n\n\treturn &res\n}\n\nfunc (tx *Tx) getListEntryNewAddRecordCount(bucketId BucketId, entry *Entry) (int64, error) {\n\tif entry.Meta.Flag == DataExpireListFlag {\n\t\treturn 0, nil\n\t}\n\n\tvar res int64\n\tkey := string(entry.Key)\n\tvalue := string(entry.Value)\n\tl := tx.db.Index.list.getWithDefault(bucketId)\n\n\tswitch entry.Meta.Flag {\n\tcase DataLPushFlag, DataRPushFlag:\n\t\tres++\n\tcase DataLPopFlag, DataRPopFlag:\n\t\tres--\n\tcase DataLRemByIndex:\n\t\tindexes, _ := UnmarshalInts([]byte(value))\n\t\tres -= int64(len(l.getValidIndexes(key, indexes)))\n\tcase DataLRemFlag:\n\t\tcount, newValue := splitIntStringStr(value, SeparatorForListKey)\n\t\tremoveIndices, err := l.getRemoveIndexes(key, count, func(r *Record) (bool, error) {\n\t\t\tv, err := tx.db.getValueByRecord(r)\n\t\t\tif err != nil {\n\t\t\t\treturn false, err\n\t\t\t}\n\t\t\treturn bytes.Equal([]byte(newValue), v), nil\n\t\t})\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\tres -= int64(len(removeIndices))\n\tcase DataLTrimFlag:\n\t\tnewKey, start := splitStringIntStr(key, SeparatorForListKey)\n\t\tend, _ := strconv2.StrToInt(value)\n\n\t\tif l.IsExpire(newKey) {\n\t\t\treturn 0, nil\n\t\t}\n\n\t\tif _, ok := l.Items[newKey]; !ok {\n\t\t\treturn 0, nil\n\t\t}\n\n\t\titems, err := l.LRange(newKey, start, end)\n\t\tif err != nil {\n\t\t\treturn res, err\n\t\t}\n\n\t\tlist := l.Items[newKey]\n\t\tres -= int64(list.Count() - len(items))\n\t}\n\n\treturn res, nil\n}\n\nfunc (tx *Tx) getKvEntryNewAddRecordCount(bucketId BucketId, entry *Entry) (int64, error) {\n\tvar res int64\n\n\tswitch entry.Meta.Flag {\n\tcase DataDeleteFlag:\n\t\tres--\n\tcase DataSetFlag:\n\t\tif idx, ok := tx.db.Index.bTree.exist(bucketId); ok {\n\t\t\t_, found := idx.Find(entry.Key)\n\t\t\tif !found {\n\t\t\t\tres++\n\t\t\t}\n\t\t} else {\n\t\t\tres++\n\t\t}\n\t}\n\n\treturn res, nil\n}\n\nfunc (tx *Tx) getSetEntryNewAddRecordCount(bucketId BucketId, entry *Entry) (int64, error) {\n\tvar res int64\n\n\tif entry.Meta.Flag == DataDeleteFlag {\n\t\tres--\n\t}\n\n\tif entry.Meta.Flag == DataSetFlag {\n\t\tres++\n\t}\n\n\treturn res, nil\n}\n\nfunc (tx *Tx) getSortedSetEntryNewAddRecordCount(bucketId BucketId, entry *Entry) (int64, error) {\n\tvar res int64\n\tkey := string(entry.Key)\n\tvalue := string(entry.Value)\n\n\tswitch entry.Meta.Flag {\n\tcase DataZAddFlag:\n\t\tif !tx.keyExistsInSortedSet(bucketId, key, value) {\n\t\t\tres++\n\t\t}\n\tcase DataZRemFlag:\n\t\tres--\n\tcase DataZRemRangeByRankFlag:\n\t\tstart, end := splitIntIntStr(value, SeparatorForZSetKey)\n\t\tdelNodes, err := tx.db.Index.sortedSet.getWithDefault(bucketId, tx.db).getZRemRangeByRankNodes(key, start, end)\n\t\tif err != nil {\n\t\t\treturn res, err\n\t\t}\n\t\tres -= int64(len(delNodes))\n\tcase DataZPopMaxFlag, DataZPopMinFlag:\n\t\tres--\n\t}\n\n\treturn res, nil\n}\n\nfunc (tx *Tx) keyExistsInSortedSet(bucketId BucketId, key, value string) bool {\n\tif _, exist := tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn false\n\t}\n\tnewKey := key\n\tif strings.Contains(key, SeparatorForZSetKey) {\n\t\tnewKey, _ = splitStringFloat64Str(key, SeparatorForZSetKey)\n\t}\n\texists, _ := tx.db.Index.sortedSet.idx[bucketId].ZExist(newKey, []byte(value))\n\treturn exists\n}\n\nfunc (tx *Tx) getEntryNewAddRecordCount(entry *Entry) (int64, error) {\n\tvar res int64\n\tvar err error\n\n\tbucket, err := tx.db.bm.GetBucketById(entry.Meta.BucketId)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tbucketId := bucket.Id\n\n\tif entry.Meta.Ds == DataStructureBTree {\n\t\tres, err = tx.getKvEntryNewAddRecordCount(bucketId, entry)\n\t}\n\n\tif entry.Meta.Ds == DataStructureList {\n\t\tres, err = tx.getListEntryNewAddRecordCount(bucketId, entry)\n\t}\n\n\tif entry.Meta.Ds == DataStructureSet {\n\t\tres, err = tx.getSetEntryNewAddRecordCount(bucketId, entry)\n\t}\n\n\tif entry.Meta.Ds == DataStructureSortedSet {\n\t\tres, err = tx.getSortedSetEntryNewAddRecordCount(bucketId, entry)\n\t}\n\n\treturn res, err\n}\n\nfunc (tx *Tx) allocCommitBuffer() *bytes.Buffer {\n\tvar buff *bytes.Buffer\n\n\tif tx.size < tx.db.opt.CommitBufferSize {\n\t\tbuff = tx.db.commitBuffer\n\t} else {\n\t\tbuff = new(bytes.Buffer)\n\t\t// avoid grow\n\t\tbuff.Grow(int(tx.size))\n\t}\n\n\treturn buff\n}\n\n// rotateActiveFile rotates log file when active file is not enough space to store the entry.\nfunc (tx *Tx) rotateActiveFile() error {\n\tvar err error\n\ttx.db.MaxFileID++\n\n\tif !tx.db.opt.SyncEnable && tx.db.opt.RWMode == MMap {\n\t\tif err := tx.db.ActiveFile.rwManager.Sync(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif err := tx.db.ActiveFile.rwManager.Release(); err != nil {\n\t\treturn err\n\t}\n\n\t// reset ActiveFile\n\tpath := getDataPath(tx.db.MaxFileID, tx.db.opt.Dir)\n\ttx.db.ActiveFile, err = tx.db.fm.getDataFile(path, tx.db.opt.SegmentSize)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\ttx.db.ActiveFile.fileID = tx.db.MaxFileID\n\treturn nil\n}\n\nfunc (tx *Tx) writeData(data []byte) (n int, err error) {\n\tif len(data) == 0 {\n\t\treturn\n\t}\n\n\twriteOffset := tx.db.ActiveFile.ActualSize\n\n\tl := len(data)\n\tif writeOffset+int64(l) > tx.db.opt.SegmentSize {\n\t\treturn 0, errors.New(\"not enough file space\")\n\t}\n\n\tif n, err = tx.db.ActiveFile.WriteAt(data, writeOffset); err != nil {\n\t\treturn\n\t}\n\n\ttx.db.ActiveFile.writeOff += int64(l)\n\ttx.db.ActiveFile.ActualSize += int64(l)\n\n\tif tx.db.opt.SyncEnable {\n\t\tif err := tx.db.ActiveFile.rwManager.Sync(); err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t}\n\n\treturn\n}\n\n// Rollback closes the transaction.\nfunc (tx *Tx) Rollback() error {\n\tif tx.db == nil {\n\t\ttx.setStatusClosed()\n\t\treturn ErrDBClosed\n\t}\n\tif tx.isCommitting() {\n\t\treturn ErrCannotRollbackACommittingTx\n\t}\n\n\tif tx.isClosed() {\n\t\treturn ErrCannotRollbackAClosedTx\n\t}\n\n\ttx.setStatusClosed()\n\ttx.unlock()\n\n\ttx.db = nil\n\ttx.pendingWrites = nil\n\n\treturn nil\n}\n\n// lock locks the database based on the transaction type.\nfunc (tx *Tx) lock() {\n\tif tx.writable {\n\t\ttx.db.mu.Lock()\n\t} else {\n\t\ttx.db.mu.RLock()\n\t}\n}\n\n// unlock unlocks the database based on the transaction type.\nfunc (tx *Tx) unlock() {\n\tif tx.writable {\n\t\ttx.db.mu.Unlock()\n\t} else {\n\t\ttx.db.mu.RUnlock()\n\t}\n}\n\nfunc (tx *Tx) handleErr(err error) {\n\tif tx.db.opt.ErrorHandler != nil {\n\t\ttx.db.opt.ErrorHandler.HandleError(err)\n\t}\n}\n\nfunc (tx *Tx) checkTxIsClosed() error {\n\tif tx.db == nil {\n\t\treturn ErrTxClosed\n\t}\n\treturn nil\n}\n\n// put sets the value for a key in the bucket.\n// Returns an error if tx is closed, if performing a write operation on a read-only transaction, if the key is empty.\nfunc (tx *Tx) put(bucket string, key, value []byte, ttl uint32, flag uint16, timestamp uint64, ds uint16) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\n\tbucketStatus := tx.getBucketStatus(DataStructureBTree, bucket)\n\tif bucketStatus == BucketStatusDeleted {\n\t\treturn ErrBucketNotFound\n\t}\n\n\tif !tx.db.bm.ExistBucket(ds, bucket) {\n\t\treturn ErrorBucketNotExist\n\t}\n\n\tif !tx.writable {\n\t\treturn ErrTxNotWritable\n\t}\n\n\tbucketId, err := tx.db.bm.GetBucketID(ds, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tmeta := NewMetaData().WithTimeStamp(timestamp).WithKeySize(uint32(len(key))).WithValueSize(uint32(len(value))).WithFlag(flag).\n\t\tWithTTL(ttl).WithStatus(UnCommitted).WithDs(ds).WithTxID(tx.id).WithBucketId(bucketId)\n\n\te := NewEntry().WithKey(key).WithMeta(meta).WithValue(value)\n\n\terr = e.valid()\n\tif err != nil {\n\t\treturn err\n\t}\n\ttx.submitEntry(ds, bucket, e)\n\ttx.size += e.Size()\n\n\treturn nil\n}\n\nfunc (tx *Tx) putDeleteLog(bucketId BucketId, key, value []byte, ttl uint32, flag uint16, timestamp uint64, ds uint16) {\n\tbucket, err := tx.db.bm.GetBucketById(bucketId)\n\tif err != nil {\n\t\treturn\n\t}\n\tmeta := NewMetaData().WithTimeStamp(timestamp).WithKeySize(uint32(len(key))).WithValueSize(uint32(len(value))).WithFlag(flag).\n\t\tWithTTL(ttl).WithStatus(UnCommitted).WithDs(ds).WithTxID(tx.id).WithBucketId(bucket.Id)\n\n\te := NewEntry().WithKey(key).WithMeta(meta).WithValue(value)\n\ttx.submitEntry(ds, bucket.Name, e)\n\ttx.size += e.Size()\n}\n\n// setStatusCommitting will change the tx status to txStatusCommitting\nfunc (tx *Tx) setStatusCommitting() {\n\tstatus := txStatusCommitting\n\ttx.status.Store(status)\n}\n\n// setStatusClosed will change the tx status to txStatusClosed\nfunc (tx *Tx) setStatusClosed() {\n\tstatus := txStatusClosed\n\ttx.status.Store(status)\n}\n\n// setStatusRunning will change the tx status to txStatusRunning\nfunc (tx *Tx) setStatusRunning() {\n\tstatus := txStatusRunning\n\ttx.status.Store(status)\n}\n\n// isRunning will check if the tx status is txStatusRunning\nfunc (tx *Tx) isRunning() bool {\n\tstatus := tx.status.Load().(int)\n\treturn status == txStatusRunning\n}\n\n// isCommitting will check if the tx status is txStatusCommitting\nfunc (tx *Tx) isCommitting() bool {\n\tstatus := tx.status.Load().(int)\n\treturn status == txStatusCommitting\n}\n\n// isClosed will check if the tx status is txStatusClosed\nfunc (tx *Tx) isClosed() bool {\n\tstatus := tx.status.Load().(int)\n\treturn status == txStatusClosed\n}\n\nfunc (tx *Tx) buildIdxes(records []*Record, entries []*Entry) error {\n\tfor i, entry := range entries {\n\t\tmeta := entry.Meta\n\t\tvar err error\n\t\tswitch meta.Ds {\n\t\tcase DataStructureBTree:\n\t\t\terr = tx.db.buildBTreeIdx(records[i], entry)\n\t\tcase DataStructureList:\n\t\t\terr = tx.db.buildListIdx(records[i], entry)\n\t\tcase DataStructureSet:\n\t\t\terr = tx.db.buildSetIdx(records[i], entry)\n\t\tcase DataStructureSortedSet:\n\t\t\terr = tx.db.buildSortedSetIdx(records[i], entry)\n\t\t}\n\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\ttx.db.KeyCount++\n\t}\n\treturn nil\n}\n\nfunc (tx *Tx) putBucket(b *Bucket) error {\n\tif _, exist := tx.pendingBucketList[b.Ds]; !exist {\n\t\ttx.pendingBucketList[b.Ds] = map[BucketName]*Bucket{}\n\t}\n\tbucketInDs := tx.pendingBucketList[b.Ds]\n\tbucketInDs[b.Name] = b\n\treturn nil\n}\n\nfunc (tx *Tx) SubmitBucket() error {\n\tbucketReqs := make([]*bucketSubmitRequest, 0)\n\tfor ds, mapper := range tx.pendingBucketList {\n\t\tfor name, bucket := range mapper {\n\t\t\treq := &bucketSubmitRequest{\n\t\t\t\tds:     ds,\n\t\t\t\tname:   name,\n\t\t\t\tbucket: bucket,\n\t\t\t}\n\t\t\tbucketReqs = append(bucketReqs, req)\n\t\t}\n\t}\n\treturn tx.db.bm.SubmitPendingBucketChange(bucketReqs)\n}\n\n// buildBucketInIndex build indexes on creation and deletion of buckets\nfunc (tx *Tx) buildBucketInIndex() error {\n\tfor _, mapper := range tx.pendingBucketList {\n\t\tfor _, bucket := range mapper {\n\t\t\tif bucket.Meta.Op == BucketInsertOperation {\n\t\t\t\tswitch bucket.Ds {\n\t\t\t\tcase DataStructureBTree:\n\t\t\t\t\ttx.db.Index.bTree.getWithDefault(bucket.Id)\n\t\t\t\tcase DataStructureList:\n\t\t\t\t\ttx.db.Index.list.getWithDefault(bucket.Id)\n\t\t\t\tcase DataStructureSet:\n\t\t\t\t\ttx.db.Index.set.getWithDefault(bucket.Id)\n\t\t\t\tcase DataStructureSortedSet:\n\t\t\t\t\ttx.db.Index.sortedSet.getWithDefault(bucket.Id, tx.db)\n\t\t\t\tdefault:\n\t\t\t\t\treturn ErrDataStructureNotSupported\n\t\t\t\t}\n\t\t\t} else if bucket.Meta.Op == BucketDeleteOperation {\n\t\t\t\tswitch bucket.Ds {\n\t\t\t\tcase DataStructureBTree:\n\t\t\t\t\ttx.db.Index.bTree.delete(bucket.Id)\n\t\t\t\tcase DataStructureList:\n\t\t\t\t\ttx.db.Index.list.delete(bucket.Id)\n\t\t\t\tcase DataStructureSet:\n\t\t\t\t\ttx.db.Index.set.delete(bucket.Id)\n\t\t\t\tcase DataStructureSortedSet:\n\t\t\t\t\ttx.db.Index.sortedSet.delete(bucket.Id)\n\t\t\t\tdefault:\n\t\t\t\t\treturn ErrDataStructureNotSupported\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (tx *Tx) getChangeCountInEntriesChanges() (int64, error) {\n\tvar res int64\n\tfor _, entriesInDS := range tx.pendingWrites.entriesInBTree {\n\t\tfor _, entry := range entriesInDS {\n\t\t\tcurRecordCnt, err := tx.getEntryNewAddRecordCount(entry)\n\t\t\tif err != nil {\n\t\t\t\treturn res, nil\n\t\t\t}\n\t\t\tres += curRecordCnt\n\t\t}\n\t}\n\tfor _, entriesInDS := range tx.pendingWrites.entries {\n\t\tfor _, entries := range entriesInDS {\n\t\t\tfor _, entry := range entries {\n\t\t\t\tcurRecordCnt, err := tx.getEntryNewAddRecordCount(entry)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn res, err\n\t\t\t\t}\n\t\t\t\tres += curRecordCnt\n\t\t\t}\n\t\t}\n\t}\n\treturn res, nil\n}\n\nfunc (tx *Tx) getChangeCountInBucketChanges() int64 {\n\tvar res int64\n\tf := func(bucket *Bucket) error {\n\t\tbucketId := bucket.Id\n\t\tif bucket.Meta.Op == BucketDeleteOperation {\n\t\t\tswitch bucket.Ds {\n\t\t\tcase DataStructureBTree:\n\t\t\t\tif bTree, ok := tx.db.Index.bTree.idx[bucketId]; ok {\n\t\t\t\t\tres -= int64(bTree.Count())\n\t\t\t\t}\n\t\t\tcase DataStructureSet:\n\t\t\t\tif set, ok := tx.db.Index.set.idx[bucketId]; ok {\n\t\t\t\t\tfor key := range set.M {\n\t\t\t\t\t\tres -= int64(set.SCard(key))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tcase DataStructureSortedSet:\n\t\t\t\tif sortedSet, ok := tx.db.Index.sortedSet.idx[bucketId]; ok {\n\t\t\t\t\tfor key := range sortedSet.M {\n\t\t\t\t\t\tcurLen, _ := sortedSet.ZCard(key)\n\t\t\t\t\t\tres -= int64(curLen)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tcase DataStructureList:\n\t\t\t\tif list, ok := tx.db.Index.list.idx[bucketId]; ok {\n\t\t\t\t\tfor key := range list.Items {\n\t\t\t\t\t\tcurLen, _ := list.Size(key)\n\t\t\t\t\t\tres -= int64(curLen)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\tpanic(fmt.Sprintf(\"there is an unexpected data structure that is unimplemented in our database.:%d\", bucket.Ds))\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\t_ = tx.pendingBucketList.rangeBucket(f)\n\treturn res\n}\n\nfunc (tx *Tx) getBucketStatus(ds Ds, name BucketName) BucketStatus {\n\tif len(tx.pendingBucketList) > 0 {\n\t\tif bucketInDs, exist := tx.pendingBucketList[ds]; exist {\n\t\t\tif bucket, exist := bucketInDs[name]; exist {\n\t\t\t\tswitch bucket.Meta.Op {\n\t\t\t\tcase BucketInsertOperation:\n\t\t\t\t\treturn BucketStatusNew\n\t\t\t\tcase BucketDeleteOperation:\n\t\t\t\t\treturn BucketStatusDeleted\n\t\t\t\tcase BucketUpdateOperation:\n\t\t\t\t\treturn BucketStatusUpdated\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif tx.db.bm.ExistBucket(ds, name) {\n\t\treturn BucketStatusExistAlready\n\t}\n\treturn BucketStatusUnknown\n}\n\n// findEntryStatus finds the latest status for the certain Entry in Tx\nfunc (tx *Tx) findEntryAndItsStatus(ds Ds, bucket BucketName, key string) (EntryStatus, *Entry) {\n\tif tx.pendingWrites.size == 0 {\n\t\treturn NotFoundEntry, nil\n\t}\n\tpendingWriteEntries := tx.pendingWrites.entriesInBTree\n\tif pendingWriteEntries == nil {\n\t\treturn NotFoundEntry, nil\n\t}\n\tif pendingWriteEntries[bucket] == nil {\n\t\treturn NotFoundEntry, nil\n\t}\n\tentries := pendingWriteEntries[bucket]\n\tif entry, exist := entries[key]; exist {\n\t\tswitch entry.Meta.Flag {\n\t\tcase DataDeleteFlag:\n\t\t\treturn EntryDeleted, nil\n\t\tdefault:\n\t\t\treturn EntryUpdated, entry\n\t\t}\n\t}\n\treturn NotFoundEntry, nil\n}\n"
        },
        {
          "name": "tx_btree.go",
          "type": "blob",
          "size": 16.25,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"errors\"\n\t\"math\"\n\t\"math/big\"\n\t\"strconv\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/xujiajun/utils/strconv2\"\n)\n\nconst (\n\tgetAllType    uint8 = 0\n\tgetKeysType   uint8 = 1\n\tgetValuesType uint8 = 2\n)\n\nfunc (tx *Tx) PutWithTimestamp(bucket string, key, value []byte, ttl uint32, timestamp uint64) error {\n\treturn tx.put(bucket, key, value, ttl, DataSetFlag, timestamp, DataStructureBTree)\n}\n\n// Put sets the value for a key in the bucket.\n// a wrapper of the function put.\nfunc (tx *Tx) Put(bucket string, key, value []byte, ttl uint32) error {\n\treturn tx.put(bucket, key, value, ttl, DataSetFlag, uint64(time.Now().UnixMilli()), DataStructureBTree)\n}\n\n// PutIfNotExists set the value for a key in the bucket only if the key doesn't exist already.\nfunc (tx *Tx) PutIfNotExists(bucket string, key, value []byte, ttl uint32) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbucketId := b.Id\n\n\tidx, bucketExists := tx.db.Index.bTree.exist(bucketId)\n\tif !bucketExists {\n\t\treturn ErrNotFoundBucket\n\t}\n\trecord, recordExists := idx.Find(key)\n\n\tif recordExists && !record.IsExpired() {\n\t\treturn nil\n\t}\n\n\treturn tx.put(bucket, key, value, ttl, DataSetFlag, uint64(time.Now().UnixMilli()), DataStructureBTree)\n}\n\n// PutIfExits set the value for a key in the bucket only if the key already exits.\nfunc (tx *Tx) PutIfExists(bucket string, key, value []byte, ttl uint32) error {\n\treturn tx.update(bucket, key, func(_ []byte) ([]byte, error) {\n\t\treturn value, nil\n\t}, func(_ uint32) (uint32, error) {\n\t\treturn ttl, nil\n\t})\n}\n\n// Get retrieves the value for a key in the bucket.\n// The returned value is only valid for the life of the transaction.\nfunc (tx *Tx) Get(bucket string, key []byte) (value []byte, err error) {\n\treturn tx.get(bucket, key)\n}\n\nfunc (tx *Tx) get(bucket string, key []byte) (value []byte, err error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId := b.Id\n\n\tbucketStatus := tx.getBucketStatus(DataStructureBTree, bucket)\n\tif bucketStatus == BucketStatusDeleted {\n\t\treturn nil, ErrBucketNotFound\n\t}\n\n\tstatus, entry := tx.findEntryAndItsStatus(DataStructureBTree, bucket, string(key))\n\tif status != NotFoundEntry && entry != nil {\n\t\tif status == EntryDeleted {\n\t\t\treturn nil, ErrKeyNotFound\n\t\t} else {\n\t\t\treturn entry.Value, nil\n\t\t}\n\t}\n\n\tif idx, ok := tx.db.Index.bTree.exist(bucketId); ok {\n\t\trecord, found := idx.Find(key)\n\t\tif !found {\n\t\t\treturn nil, ErrKeyNotFound\n\t\t}\n\n\t\tif record.IsExpired() {\n\t\t\ttx.putDeleteLog(bucketId, key, nil, Persistent, DataDeleteFlag, uint64(time.Now().Unix()), DataStructureBTree)\n\t\t\treturn nil, ErrNotFoundKey\n\t\t}\n\n\t\tvalue, err = tx.db.getValueByRecord(record)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn value, nil\n\n\t} else {\n\t\treturn nil, ErrNotFoundBucket\n\t}\n}\n\nfunc (tx *Tx) ValueLen(bucket string, key []byte) (int, error) {\n\tvalue, err := tx.get(bucket, key)\n\treturn len(value), err\n}\n\nfunc (tx *Tx) GetMaxKey(bucket string) ([]byte, error) {\n\treturn tx.getMaxOrMinKey(bucket, true)\n}\n\nfunc (tx *Tx) GetMinKey(bucket string) ([]byte, error) {\n\treturn tx.getMaxOrMinKey(bucket, false)\n}\n\nfunc (tx *Tx) getMaxOrMinKey(bucket string, isMax bool) ([]byte, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId := b.Id\n\n\tif idx, ok := tx.db.Index.bTree.exist(bucketId); ok {\n\t\tvar (\n\t\t\titem  *Item\n\t\t\tfound bool\n\t\t)\n\n\t\tif isMax {\n\t\t\titem, found = idx.Max()\n\t\t} else {\n\t\t\titem, found = idx.Min()\n\t\t}\n\n\t\tif !found {\n\t\t\treturn nil, ErrKeyNotFound\n\t\t}\n\n\t\tif item.record.IsExpired() {\n\t\t\ttx.putDeleteLog(bucketId, item.key, nil, Persistent, DataDeleteFlag, uint64(time.Now().Unix()), DataStructureBTree)\n\t\t\treturn nil, ErrNotFoundKey\n\t\t}\n\n\t\treturn item.key, nil\n\t} else {\n\t\treturn nil, ErrNotFoundBucket\n\t}\n}\n\n// GetAll returns all keys and values of the bucket stored at given bucket.\nfunc (tx *Tx) GetAll(bucket string) ([][]byte, [][]byte, error) {\n\treturn tx.getAllOrKeysOrValues(bucket, getAllType)\n}\n\n// GetKeys returns all keys of the bucket stored at given bucket.\nfunc (tx *Tx) GetKeys(bucket string) ([][]byte, error) {\n\tkeys, _, err := tx.getAllOrKeysOrValues(bucket, getKeysType)\n\treturn keys, err\n}\n\n// GetValues returns all values of the bucket stored at given bucket.\nfunc (tx *Tx) GetValues(bucket string) ([][]byte, error) {\n\t_, values, err := tx.getAllOrKeysOrValues(bucket, getValuesType)\n\treturn values, err\n}\n\nfunc (tx *Tx) getAllOrKeysOrValues(bucket string, typ uint8) ([][]byte, [][]byte, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tbucketId, err := tx.db.bm.GetBucketID(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tif index, ok := tx.db.Index.bTree.exist(bucketId); ok {\n\t\trecords := index.All()\n\n\t\tvar (\n\t\t\tkeys   [][]byte\n\t\t\tvalues [][]byte\n\t\t)\n\n\t\tswitch typ {\n\t\tcase getAllType:\n\t\t\tkeys, values, err = tx.getHintIdxDataItemsWrapper(records, ScanNoLimit, bucketId, true, true)\n\t\tcase getKeysType:\n\t\t\tkeys, _, err = tx.getHintIdxDataItemsWrapper(records, ScanNoLimit, bucketId, true, false)\n\t\tcase getValuesType:\n\t\t\t_, values, err = tx.getHintIdxDataItemsWrapper(records, ScanNoLimit, bucketId, false, true)\n\t\t}\n\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n\t\treturn keys, values, nil\n\t}\n\n\treturn nil, nil, nil\n}\n\nfunc (tx *Tx) GetSet(bucket string, key, value []byte) (oldValue []byte, err error) {\n\treturn oldValue, tx.update(bucket, key, func(b []byte) ([]byte, error) {\n\t\toldValue = b\n\t\treturn value, nil\n\t}, func(oldTTL uint32) (uint32, error) {\n\t\treturn oldTTL, nil\n\t})\n}\n\n// RangeScan query a range at given bucket, start and end slice.\nfunc (tx *Tx) RangeScan(bucket string, start, end []byte) (values [][]byte, err error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId := b.Id\n\n\tif index, ok := tx.db.Index.bTree.exist(bucketId); ok {\n\t\trecords := index.Range(start, end)\n\n\t\t_, values, err = tx.getHintIdxDataItemsWrapper(records, ScanNoLimit, bucketId, false, true)\n\t\tif err != nil {\n\t\t\treturn nil, ErrRangeScan\n\t\t}\n\t}\n\n\tif len(values) == 0 {\n\t\treturn nil, ErrRangeScan\n\t}\n\n\treturn\n}\n\n// PrefixScan iterates over a key prefix at given bucket, prefix and limitNum.\n// LimitNum will limit the number of entries return.\nfunc (tx *Tx) PrefixScan(bucket string, prefix []byte, offsetNum int, limitNum int) (values [][]byte, err error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId := b.Id\n\n\tif idx, ok := tx.db.Index.bTree.exist(bucketId); ok {\n\t\trecords := idx.PrefixScan(prefix, offsetNum, limitNum)\n\t\t_, values, err = tx.getHintIdxDataItemsWrapper(records, limitNum, bucketId, false, true)\n\t\tif err != nil {\n\t\t\treturn nil, ErrPrefixScan\n\t\t}\n\t}\n\n\tif len(values) == 0 {\n\t\treturn nil, ErrPrefixScan\n\t}\n\n\treturn\n}\n\n// PrefixSearchScan iterates over a key prefix at given bucket, prefix, match regular expression and limitNum.\n// LimitNum will limit the number of entries return.\nfunc (tx *Tx) PrefixSearchScan(bucket string, prefix []byte, reg string, offsetNum int, limitNum int) (values [][]byte, err error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId := b.Id\n\n\tif idx, ok := tx.db.Index.bTree.exist(bucketId); ok {\n\t\trecords := idx.PrefixSearchScan(prefix, reg, offsetNum, limitNum)\n\t\t_, values, err = tx.getHintIdxDataItemsWrapper(records, limitNum, bucketId, false, true)\n\t\tif err != nil {\n\t\t\treturn nil, ErrPrefixSearchScan\n\t\t}\n\t}\n\n\tif len(values) == 0 {\n\t\treturn nil, ErrPrefixSearchScan\n\t}\n\n\treturn\n}\n\n// Delete removes a key from the bucket at given bucket and key.\nfunc (tx *Tx) Delete(bucket string, key []byte) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbucketId := b.Id\n\n\tif idx, ok := tx.db.Index.bTree.exist(bucketId); ok {\n\t\tif _, found := idx.Find(key); !found {\n\t\t\treturn ErrKeyNotFound\n\t\t}\n\t} else {\n\t\treturn ErrNotFoundBucket\n\t}\n\n\treturn tx.put(bucket, key, nil, Persistent, DataDeleteFlag, uint64(time.Now().Unix()), DataStructureBTree)\n}\n\n// getHintIdxDataItemsWrapper returns keys and values when prefix scanning or range scanning.\nfunc (tx *Tx) getHintIdxDataItemsWrapper(records []*Record, limitNum int, bucketId BucketId, needKeys bool, needValues bool) (keys [][]byte, values [][]byte, err error) {\n\tfor _, record := range records {\n\t\tif record.IsExpired() {\n\t\t\ttx.putDeleteLog(bucketId, record.Key, nil, Persistent, DataDeleteFlag, uint64(time.Now().Unix()), DataStructureBTree)\n\t\t\tcontinue\n\t\t}\n\n\t\tif limitNum > 0 && len(values) < limitNum || limitNum == ScanNoLimit {\n\t\t\tif needKeys {\n\t\t\t\tkeys = append(keys, record.Key)\n\t\t\t}\n\n\t\t\tif needValues {\n\t\t\t\tvalue, err := tx.db.getValueByRecord(record)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, nil, err\n\t\t\t\t}\n\t\t\t\tvalues = append(values, value)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn keys, values, nil\n}\n\nfunc (tx *Tx) tryGet(bucket string, key []byte, solveRecord func(record *Record, found bool, bucketId BucketId) error) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\n\tbucketId, err := tx.db.bm.GetBucketID(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif idx, ok := tx.db.Index.bTree.exist(bucketId); ok {\n\t\trecord, found := idx.Find(key)\n\t\treturn solveRecord(record, found, bucketId)\n\t} else {\n\t\treturn ErrBucketNotFound\n\t}\n}\n\nfunc (tx *Tx) update(bucket string, key []byte, getNewValue func([]byte) ([]byte, error), getNewTTL func(uint32) (uint32, error)) error {\n\treturn tx.tryGet(bucket, key, func(record *Record, found bool, bucketId BucketId) error {\n\t\tif !found {\n\t\t\treturn ErrKeyNotFound\n\t\t}\n\n\t\tif record.IsExpired() {\n\t\t\ttx.putDeleteLog(bucketId, key, nil, Persistent, DataDeleteFlag, uint64(time.Now().Unix()), DataStructureBTree)\n\t\t\treturn ErrNotFoundKey\n\t\t}\n\n\t\tvalue, err := tx.db.getValueByRecord(record)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tnewValue, err := getNewValue(value)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tnewTTL, err := getNewTTL(record.TTL)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\treturn tx.put(bucket, key, newValue, newTTL, DataSetFlag, uint64(time.Now().Unix()), DataStructureBTree)\n\t})\n}\n\nfunc (tx *Tx) updateOrPut(bucket string, key, value []byte, getUpdatedValue func([]byte) ([]byte, error)) error {\n\treturn tx.tryGet(bucket, key, func(record *Record, found bool, bucketId BucketId) error {\n\t\tif !found {\n\t\t\treturn tx.put(bucket, key, value, Persistent, DataSetFlag, uint64(time.Now().Unix()), DataStructureBTree)\n\t\t}\n\n\t\tvalue, err := tx.db.getValueByRecord(record)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tnewValue, err := getUpdatedValue(value)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\treturn tx.put(bucket, key, newValue, record.TTL, DataSetFlag, uint64(time.Now().Unix()), DataStructureBTree)\n\t})\n}\n\nfunc bigIntIncr(a string, b string) string {\n\tbigIntA, _ := new(big.Int).SetString(a, 10)\n\tbigIntB, _ := new(big.Int).SetString(b, 10)\n\tbigIntA.Add(bigIntA, bigIntB)\n\treturn bigIntA.String()\n}\n\nfunc (tx *Tx) integerIncr(bucket string, key []byte, increment int64) error {\n\treturn tx.update(bucket, key, func(value []byte) ([]byte, error) {\n\t\tintValue, err := strconv2.StrToInt64(string(value))\n\n\t\tif err != nil && errors.Is(err, strconv.ErrRange) {\n\t\t\treturn []byte(bigIntIncr(string(value), strconv2.Int64ToStr(increment))), nil\n\t\t}\n\n\t\tif err != nil {\n\t\t\treturn nil, ErrValueNotInteger\n\t\t}\n\n\t\tif (increment > 0 && math.MaxInt64-increment < intValue) || (increment < 0 && math.MinInt64-increment > intValue) {\n\t\t\treturn []byte(bigIntIncr(string(value), strconv2.Int64ToStr(increment))), nil\n\t\t}\n\n\t\tatomic.AddInt64(&intValue, increment)\n\t\treturn []byte(strconv2.Int64ToStr(intValue)), nil\n\t}, func(oldTTL uint32) (uint32, error) {\n\t\treturn oldTTL, nil\n\t})\n}\n\nfunc (tx *Tx) Incr(bucket string, key []byte) error {\n\treturn tx.integerIncr(bucket, key, 1)\n}\n\nfunc (tx *Tx) Decr(bucket string, key []byte) error {\n\treturn tx.integerIncr(bucket, key, -1)\n}\n\nfunc (tx *Tx) IncrBy(bucket string, key []byte, increment int64) error {\n\treturn tx.integerIncr(bucket, key, increment)\n}\n\nfunc (tx *Tx) DecrBy(bucket string, key []byte, decrement int64) error {\n\treturn tx.integerIncr(bucket, key, -1*decrement)\n}\n\nfunc (tx *Tx) GetBit(bucket string, key []byte, offset int) (byte, error) {\n\tif offset >= math.MaxInt || offset < 0 {\n\t\treturn 0, ErrOffsetInvalid\n\t}\n\n\tvalue, err := tx.Get(bucket, key)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tif len(value) <= offset {\n\t\treturn 0, nil\n\t}\n\n\treturn value[offset], nil\n}\n\nfunc (tx *Tx) SetBit(bucket string, key []byte, offset int, bit byte) error {\n\tif offset >= math.MaxInt || offset < 0 {\n\t\treturn ErrOffsetInvalid\n\t}\n\n\tvalueIfKeyNotFound := make([]byte, offset+1)\n\tvalueIfKeyNotFound[offset] = bit\n\n\treturn tx.updateOrPut(bucket, key, valueIfKeyNotFound, func(value []byte) ([]byte, error) {\n\t\tif len(value) <= offset {\n\t\t\tvalue = append(value, make([]byte, offset-len(value)+1)...)\n\t\t\tvalue[offset] = bit\n\t\t\treturn value, nil\n\t\t} else {\n\t\t\tvalue[offset] = bit\n\t\t\treturn value, nil\n\t\t}\n\t})\n}\n\n// GetTTL returns remaining TTL of a value by key.\n// It returns\n// (-1, nil) If TTL is Persistent\n// (0, ErrBucketNotFound|ErrKeyNotFound) If expired or not found\n// (TTL, nil) If the record exists with a TTL\n// Note: The returned remaining TTL will be in seconds. For example,\n// remainingTTL is 500ms, It'll return 0.\nfunc (tx *Tx) GetTTL(bucket string, key []byte) (int64, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn 0, err\n\t}\n\n\tbucketId, err := tx.db.bm.GetBucketID(DataStructureBTree, bucket)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tidx, bucketExists := tx.db.Index.bTree.exist(bucketId)\n\n\tif !bucketExists {\n\t\treturn 0, ErrBucketNotFound\n\t}\n\n\trecord, recordFound := idx.Find(key)\n\n\tif !recordFound || record.IsExpired() {\n\t\treturn 0, ErrKeyNotFound\n\t}\n\n\tif record.TTL == Persistent {\n\t\treturn -1, nil\n\t}\n\n\tremTTL := tx.db.expireTime(record.Timestamp, record.TTL)\n\tif remTTL >= 0 {\n\t\treturn int64(remTTL.Seconds()), nil\n\t} else {\n\t\treturn 0, ErrKeyNotFound\n\t}\n}\n\n// Persist updates record's TTL as Persistent if the record exits.\nfunc (tx *Tx) Persist(bucket string, key []byte) error {\n\treturn tx.update(bucket, key, func(oldValue []byte) ([]byte, error) {\n\t\treturn oldValue, nil\n\t}, func(_ uint32) (uint32, error) {\n\t\treturn Persistent, nil\n\t})\n}\n\nfunc (tx *Tx) MSet(bucket string, ttl uint32, args ...[]byte) error {\n\tif len(args) == 0 {\n\t\treturn nil\n\t}\n\n\tif len(args)%2 != 0 {\n\t\treturn ErrKVArgsLenNotEven\n\t}\n\n\tfor i := 0; i < len(args); i += 2 {\n\t\tif err := tx.put(bucket, args[i], args[i+1], ttl, DataSetFlag, uint64(time.Now().Unix()), DataStructureBTree); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (tx *Tx) MGet(bucket string, keys ...[]byte) ([][]byte, error) {\n\tif len(keys) == 0 {\n\t\treturn nil, nil\n\t}\n\n\tvalues := make([][]byte, len(keys))\n\tfor i, key := range keys {\n\t\tvalue, err := tx.Get(bucket, key)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tvalues[i] = value\n\t}\n\n\treturn values, nil\n}\n\nfunc (tx *Tx) Append(bucket string, key, appendage []byte) error {\n\tif len(appendage) == 0 {\n\t\treturn nil\n\t}\n\n\treturn tx.updateOrPut(bucket, key, appendage, func(value []byte) ([]byte, error) {\n\t\treturn append(value, appendage...), nil\n\t})\n}\n\nfunc (tx *Tx) GetRange(bucket string, key []byte, start, end int) ([]byte, error) {\n\tif start > end {\n\t\treturn nil, ErrStartGreaterThanEnd\n\t}\n\n\tvalue, err := tx.get(bucket, key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif start >= len(value) {\n\t\treturn nil, nil\n\t}\n\n\tif end >= len(value) {\n\t\treturn value[start:], nil\n\t}\n\n\treturn value[start : end+1], nil\n}\n"
        },
        {
          "name": "tx_btree_test.go",
          "type": "blob",
          "size": 36.736328125,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n\t\"github.com/xujiajun/utils/strconv2\"\n)\n\nfunc TestTx_PutAndGet(t *testing.T) {\n\n\tvar (\n\t\tbucket = \"bucket1\"\n\t\tkey    = []byte(\"key1\")\n\t\tval    = []byte(\"val1\")\n\t)\n\n\tt.Run(\"put_and_get\", func(t *testing.T) {\n\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\n\t\t\t{\n\t\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\t\ttx, err := db.Begin(true)\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\t\tassert.NoError(t, err)\n\n\t\t\t\tassert.NoError(t, tx.Commit())\n\t\t\t}\n\n\t\t\t{\n\t\t\t\ttx, err = db.Begin(false)\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\tvalue, err := tx.Get(bucket, key)\n\t\t\t\tassert.NoError(t, err)\n\t\t\t\tassert.NoError(t, tx.Commit())\n\n\t\t\t\tassert.Equal(t, val, value)\n\t\t\t}\n\n\t\t})\n\t})\n\n\tt.Run(\"get by closed tx\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\ttx, err := db.Begin(false)\n\t\t\trequire.NoError(t, err)\n\t\t\tassert.NoError(t, tx.Commit())\n\n\t\t\t_, err = tx.Get(bucket, key) // use closed tx\n\t\t\tassert.Error(t, err)\n\t\t})\n\t})\n\n}\n\nfunc TestTx_GetAll_GetKeys_GetValues(t *testing.T) {\n\tbucket := \"bucket\"\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\ttxGetAll(t, db, bucket, nil, nil, nil)\n\n\t\tn := 10\n\t\tkeys := make([][]byte, n)\n\t\tvalues := make([][]byte, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\tkeys[i] = GetTestBytes(i)\n\t\t\tvalues[i] = GetRandomBytes(10)\n\t\t\ttxPut(t, db, bucket, keys[i], values[i], Persistent, nil, nil)\n\t\t}\n\n\t\ttxGetAll(t, db, bucket, keys, values, nil)\n\n\t\tkeys = append(keys, GetTestBytes(10))\n\t\tvalues = append(values, GetRandomBytes(10))\n\t\ttxPut(t, db, bucket, keys[10], values[10], Persistent, nil, nil)\n\t\ttxGetAll(t, db, bucket, keys, values, nil)\n\n\t\ttxDel(t, db, bucket, keys[0], nil)\n\t\tkeys = keys[1:]\n\t\tvalues = values[1:]\n\t\ttxGetAll(t, db, bucket, keys, values, nil)\n\n\t\ttxPut(t, db, bucket, GetTestBytes(11), GetRandomBytes(10), 1, nil, nil)\n\t\ttime.Sleep(1100 * time.Millisecond)\n\t\ttxGetAll(t, db, bucket, keys, values, nil)\n\n\t\trequire.NoError(t, db.View(func(tx *Tx) error {\n\t\t\tkeysInBucket, err := tx.GetKeys(bucket)\n\t\t\trequire.NoError(t, err)\n\t\t\tvaluesInBucket, err := tx.GetValues(bucket)\n\t\t\trequire.NoError(t, err)\n\t\t\tfor i := 0; i < n; i++ {\n\t\t\t\trequire.Equal(t, keys[i], keysInBucket[i])\n\t\t\t\trequire.Equal(t, values[i], valuesInBucket[i])\n\t\t\t}\n\t\t\treturn nil\n\t\t}))\n\t})\n}\n\nfunc TestTx_RangeScan_Err(t *testing.T) {\n\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\n\t\tbucket := \"bucket_for_rangeScan\"\n\n\t\t{\n\t\t\t// setup the data\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttx, err := db.Begin(true)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tfor i := 0; i < 10; i++ {\n\t\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\tval := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\t\tassert.NoError(t, err)\n\t\t\t}\n\n\t\t\tassert.NoError(t, tx.Commit()) // tx commit\n\t\t}\n\n\t\t{\n\t\t\t// verify\n\n\t\t\ttx, err := db.Begin(false)\n\t\t\tassert.NoError(t, err)\n\n\t\t\tstart := []byte(\"key_0010001\")\n\t\t\tend := []byte(\"key_0010010\")\n\t\t\t_, err = tx.RangeScan(bucket, start, end)\n\t\t\tassert.Error(t, err)\n\n\t\t\tassert.NoError(t, tx.Rollback())\n\t\t}\n\t})\n}\n\nfunc TestTx_RangeScan(t *testing.T) {\n\tbucket := \"bucket_for_range\"\n\n\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\n\t\t{\n\t\t\t// setup the data\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttx, err := db.Begin(true)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tfor i := 0; i < 10; i++ {\n\t\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\tval := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\t\tassert.NoError(t, err)\n\t\t\t}\n\n\t\t\tassert.NoError(t, tx.Commit()) // tx commit\n\t\t}\n\n\t\t{\n\n\t\t\ttx, err = db.Begin(false)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tvar (\n\t\t\t\ts = 5\n\t\t\t\te = 8\n\t\t\t)\n\n\t\t\tstart := []byte(fmt.Sprintf(\"key_%07d\", s))\n\t\t\tend := []byte(fmt.Sprintf(\"key_%07d\", e))\n\n\t\t\tvalues, err := tx.RangeScan(bucket, start, end)\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.NoError(t, tx.Commit()) // tx commit\n\n\t\t\twantCount := (e - s) + 1 // the range: [5, 8]\n\t\t\tassert.Equal(t, wantCount, len(values))\n\n\t\t\tfor i, value := range values {\n\n\t\t\t\twantValue := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", i+s))\n\t\t\t\tassert.Equal(t, wantValue, value)\n\t\t\t}\n\n\t\t\tvalues, err = tx.RangeScan(bucket, start, end)\n\t\t\tassert.Error(t, err)\n\t\t\tassert.Empty(t, values)\n\t\t}\n\n\t})\n\n}\n\nfunc TestTx_PrefixScan(t *testing.T) {\n\n\tbucket := \"bucket_for_prefix_scan\"\n\n\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\n\t\t{\n\t\t\t// setup the data\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttx, err := db.Begin(true)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tfor i, prefix := range []string{\"key1_\", \"key2_\"} {\n\t\t\t\tfor j := 0; j < 10; j++ {\n\t\t\t\t\tkey := []byte(prefix + fmt.Sprintf(\"%07d\", j))\n\t\t\t\t\tval := []byte(\"foobar\" + fmt.Sprintf(\"%d%07d\", i+1, j))\n\t\t\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\t\t\tassert.NoError(t, err)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tassert.NoError(t, tx.Commit()) // tx commit\n\t\t}\n\n\t\t{\n\t\t\ttx, err = db.Begin(false)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tvar (\n\t\t\t\toffset = 5\n\t\t\t\tlimit  = 3\n\t\t\t)\n\n\t\t\tprefix := []byte(\"key1_\")\n\t\t\tvalues, err := tx.PrefixScan(bucket, prefix, offset, limit)\n\t\t\tassert.NoError(t, err)\n\n\t\t\tassert.NoError(t, tx.Commit())\n\n\t\t\tassert.Equal(t, limit, len(values))\n\n\t\t\tfor i := 0; i < limit; i++ {\n\t\t\t\tvalIndex := offset + i\n\n\t\t\t\twantVal := []byte(\"foobar\" + fmt.Sprintf(\"%d%07d\", 1, valIndex))\n\t\t\t\tassert.Equal(t, wantVal, values[i])\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc TestTx_PrefixSearchScan(t *testing.T) {\n\tbucket := \"bucket_for_prefix_search_scan\"\n\n\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\tregs := \"1\"\n\n\t\ttx, err := db.Begin(true)\n\t\trequire.NoError(t, err)\n\n\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", 0))\n\t\tval := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", 0))\n\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\tassert.NoError(t, err)\n\n\t\tassert.NoError(t, tx.Commit()) // tx commit\n\n\t\ttx, err = db.Begin(true)\n\t\trequire.NoError(t, err)\n\n\t\tkey = []byte(\"key_\" + fmt.Sprintf(\"%07d\", 1))\n\t\tval = []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", 1))\n\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\tassert.NoError(t, err)\n\n\t\tassert.NoError(t, tx.Commit()) // tx commit\n\n\t\ttx, err = db.Begin(false)\n\t\trequire.NoError(t, err)\n\n\t\tprefix := []byte(\"key_\")\n\t\tvalues, err := tx.PrefixSearchScan(bucket, prefix, regs, 0, 1)\n\t\tassert.NoError(t, err)\n\n\t\tassert.NoError(t, tx.Commit()) // tx commit\n\n\t\tc := 0\n\t\tfor _, value := range values {\n\t\t\twantVal := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", 1))\n\n\t\t\tassert.Equal(t, wantVal, value)\n\t\t\tc++\n\t\t}\n\n\t\tassert.Equal(t, 1, c)\n\t})\n}\n\nfunc TestTx_DeleteAndGet(t *testing.T) {\n\n\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\n\t\tbucket := \"bucket_delete_test\"\n\n\t\t{\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttx, err := db.Begin(true)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tfor i := 0; i <= 10; i++ {\n\t\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\tval := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\terr := tx.Put(bucket, key, val, Persistent)\n\t\t\t\tassert.NoError(t, err)\n\t\t\t}\n\n\t\t\t// tx commit\n\t\t\tassert.NoError(t, tx.Commit())\n\t\t}\n\n\t\t{\n\t\t\ttx, err := db.Begin(true)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tfor i := 0; i <= 5; i++ {\n\t\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\terr := tx.Delete(bucket, key)\n\t\t\t\tassert.NoError(t, err)\n\t\t\t}\n\n\t\t\t// tx commit\n\t\t\tassert.NoError(t, tx.Commit())\n\n\t\t\terr = tx.Delete(bucket, []byte(\"key_\"+fmt.Sprintf(\"%07d\", 1)))\n\t\t\tassert.Error(t, err)\n\t\t}\n\t})\n\n}\n\nfunc TestTx_DeleteFromMemory(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tbucket := \"bucket\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxPut(t, db, bucket, GetTestBytes(i), GetTestBytes(i), Persistent, nil, nil)\n\t\t}\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxGet(t, db, bucket, GetTestBytes(i), GetTestBytes(i), nil)\n\t\t}\n\n\t\ttxDel(t, db, bucket, GetTestBytes(3), nil)\n\n\t\terr := db.View(func(tx *Tx) error {\n\t\t\tr, ok := db.Index.bTree.getWithDefault(1).Find(GetTestBytes(3))\n\t\t\trequire.Nil(t, r)\n\t\t\trequire.False(t, ok)\n\n\t\t\treturn nil\n\t\t})\n\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc TestTx_GetAndScansFromHintKey(t *testing.T) {\n\n\tbucket := \"bucket_get_test\"\n\twithRAMIdxDB(t, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t// write tx begin\n\t\ttx, err := db.Begin(true)\n\t\trequire.NoError(t, err)\n\n\t\tfor i := 0; i <= 10; i++ {\n\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", i))\n\t\t\tval := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", i))\n\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\tassert.NoError(t, err)\n\t\t}\n\t\tassert.NoError(t, tx.Commit()) // tx commit\n\n\t\tfor i := 0; i <= 10; i++ {\n\t\t\ttx, err = db.Begin(false)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", i))\n\t\t\tvalue, err := tx.Get(bucket, key)\n\t\t\tassert.NoError(t, err)\n\n\t\t\twantValue := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", i))\n\t\t\tassert.Equal(t, wantValue, value)\n\n\t\t\t// tx commit\n\t\t\tassert.NoError(t, tx.Commit())\n\t\t}\n\n\t\ttx, err = db.Begin(false)\n\t\trequire.NoError(t, err)\n\n\t\tstart := []byte(\"key_0000001\")\n\t\tend := []byte(\"key_0000010\")\n\t\tvalues, err := tx.RangeScan(bucket, start, end)\n\t\tassert.NoError(t, err)\n\n\t\tj := 0\n\t\tfor i := 1; i <= 10; i++ {\n\t\t\twantVal := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", i))\n\t\t\tassert.Equal(t, wantVal, values[j])\n\n\t\t\tj++\n\t\t}\n\n\t\t// tx commit\n\t\tassert.NoError(t, tx.Commit())\n\t})\n\n}\n\nfunc TestTx_Put_Err(t *testing.T) {\n\n\tbucket := \"bucket_tx_put\"\n\n\tt.Run(\"write with read only tx\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\t// write tx begin err setting here\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttx, err := db.Begin(false) // tx not writable\n\t\t\trequire.NoError(t, err)\n\n\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", 0))\n\t\t\tval := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", 0))\n\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\tassert.Error(t, err)\n\n\t\t\tassert.NoError(t, tx.Rollback())\n\t\t})\n\t})\n\n\tt.Run(\"write with empty key\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttx, err := db.Begin(true)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tkey := []byte(\"\") // key cannot be empty\n\t\t\tval := []byte(\"valvalvalvalvalvalvalvalval\" + fmt.Sprintf(\"%07d\", 0))\n\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\tassert.Error(t, err)\n\n\t\t\tassert.NoError(t, tx.Rollback())\n\t\t})\n\t})\n\n\tt.Run(\"write with TOO big size\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttx, err := db.Begin(true)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tkey := []byte(\"key_bigone\")\n\t\t\tvar bigVal string // too big size\n\t\t\tfor i := 1; i <= 9*1024; i++ {\n\t\t\t\tbigVal += \"val\" + strconv2.IntToStr(i)\n\t\t\t}\n\n\t\t\terr = tx.Put(bucket, key, []byte(bigVal), Persistent)\n\t\t\tassert.NoError(t, err)\n\n\t\t\tassert.Error(t, tx.Commit()) // too big cannot commit by tx\n\t\t})\n\t})\n}\n\nfunc TestTx_PrefixScan_NotFound(t *testing.T) {\n\tt.Run(\"prefix scan in empty bucket\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttx, err := db.Begin(false)\n\t\t\tassert.NoError(t, err)\n\n\t\t\tprefix := []byte(\"key_\")\n\t\t\tentries, err := tx.PrefixScan(\"foobucket\", prefix, 0, 10)\n\t\t\tassert.Error(t, err)\n\t\t\tassert.Empty(t, entries)\n\n\t\t\tassert.NoError(t, tx.Commit()) // tx commit\n\t\t})\n\t})\n\n\tt.Run(\"prefix scan\", func(t *testing.T) {\n\t\tbucket := \"bucket_prefix_scan_test\"\n\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\t{ // write tx begin\n\t\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\t\ttx, err := db.Begin(true)\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\tfor i := 0; i <= 10; i++ {\n\t\t\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\t\tval := []byte(\"val\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\t\t\tassert.NoError(t, err)\n\t\t\t\t}\n\n\t\t\t\tassert.NoError(t, tx.Commit()) // tx commit\n\t\t\t}\n\n\t\t\t{\n\t\t\t\ttx, err = db.Begin(false)\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\tprefix := []byte(\"key_foo\")\n\t\t\t\tentries, err := tx.PrefixScan(bucket, prefix, 0, 10)\n\t\t\t\tassert.Error(t, err)\n\t\t\t\tassert.NoError(t, tx.Commit())\n\n\t\t\t\tassert.Empty(t, entries)\n\t\t\t}\n\n\t\t\t{\n\t\t\t\ttx, err = db.Begin(false)\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\tentries, err := tx.PrefixScan(bucket, []byte(\"key_\"), 0, 10)\n\t\t\t\tassert.NoError(t, err)\n\t\t\t\tassert.NoError(t, tx.Commit())\n\n\t\t\t\tassert.NotEmpty(t, entries)\n\n\t\t\t}\n\n\t\t\t{ // scan by closed tx\n\t\t\t\tentries, err := tx.PrefixScan(bucket, []byte(\"key_\"), 0, 10)\n\t\t\t\tassert.Error(t, err)\n\t\t\t\tif len(entries) > 0 || err == nil {\n\t\t\t\t\tt.Error(\"err TestTx_PrefixScan_NotFound\")\n\t\t\t\t}\n\t\t\t}\n\n\t\t})\n\t})\n}\n\nfunc TestTx_PrefixSearchScan_NotFound(t *testing.T) {\n\tregs := \"(.+)\"\n\tbucket := \"bucket_prefix_search_scan_test\"\n\n\tt.Run(\"prefix search in empty bucket\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttx, err := db.Begin(false)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tprefix := []byte(\"key_\")\n\t\t\t_, err = tx.PrefixSearchScan(\"foobucket\", prefix, regs, 0, 10)\n\t\t\tassert.Error(t, err)\n\n\t\t\tassert.NoError(t, tx.Commit())\n\t\t})\n\t})\n\n\tt.Run(\"prefix search scan\", func(t *testing.T) {\n\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\t{ // set up the data\n\t\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\t\ttx, err = db.Begin(true) // write tx begin\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\tfor i := 0; i <= 10; i++ {\n\t\t\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%07d\", i))\n\t\t\t\t\tval := []byte(\"val\" + fmt.Sprintf(\"%07d\", i))\n\n\t\t\t\t\terr := tx.Put(bucket, key, val, Persistent)\n\t\t\t\t\tassert.NoError(t, err)\n\t\t\t\t}\n\t\t\t\t// tx commit\n\t\t\t\tassert.NoError(t, tx.Commit())\n\t\t\t}\n\n\t\t\t{ // no key exists in bucket\n\t\t\t\ttx, err := db.Begin(false)\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\tprefix := []byte(\"key_foo\")\n\t\t\t\t_, err = tx.PrefixSearchScan(bucket, prefix, regs, 0, 10)\n\t\t\t\tassert.Error(t, err)\n\n\t\t\t\tassert.NoError(t, tx.Rollback())\n\t\t\t}\n\n\t\t\t{ // scan by closed tx\n\t\t\t\ttx, err := db.Begin(false)\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\tassert.NoError(t, tx.Commit())\n\n\t\t\t\t_, err = tx.PrefixSearchScan(bucket, []byte(\"key_\"), regs, 0, 10)\n\t\t\t\tassert.Error(t, err)\n\t\t\t}\n\t\t})\n\t})\n}\n\nfunc TestTx_RangeScan_NotFound(t *testing.T) {\n\n\tbucket := \"bucket_range_scan_test\"\n\n\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\ttx, err := db.Begin(true) // write tx begin\n\t\trequire.NoError(t, err)\n\n\t\tfor i := 0; i <= 10; i++ {\n\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%03d\", i))\n\t\t\tval := []byte(\"val\" + fmt.Sprintf(\"%03d\", i))\n\t\t\terr = tx.Put(bucket, key, val, Persistent)\n\t\t\tassert.NoError(t, err)\n\t\t}\n\t\tassert.NoError(t, tx.Commit()) // tx commit\n\n\t\ttx, err = db.Begin(false)\n\t\trequire.NoError(t, err)\n\n\t\tstart := []byte(\"key_011\")\n\t\tend := []byte(\"key_012\")\n\t\t_, err = tx.RangeScan(bucket, start, end)\n\t\tassert.Error(t, err)\n\n\t\tassert.NoError(t, tx.Commit())\n\t})\n}\n\nfunc TestTx_ExpiredDeletion(t *testing.T) {\n\tbucket := \"bucket\"\n\n\tt.Run(\"expired deletion\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, GetTestBytes(0), GetTestBytes(0), 1, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(1), GetTestBytes(1), 2, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(2), GetTestBytes(2), 3, nil, nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(0), GetTestBytes(0), nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), GetTestBytes(1), nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(2), GetTestBytes(2), nil)\n\n\t\t\ttxDel(t, db, bucket, GetTestBytes(2), nil)\n\n\t\t\ttime.Sleep(1100 * time.Millisecond)\n\n\t\t\t// this entry will be deleted\n\t\t\ttxGet(t, db, bucket, GetTestBytes(0), nil, ErrKeyNotFound)\n\t\t\t// this entry still alive\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), GetTestBytes(1), nil)\n\n\t\t\ttime.Sleep(1 * time.Second)\n\n\t\t\t// this entry will be deleted\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), nil, ErrKeyNotFound)\n\n\t\t\tr, ok := db.Index.bTree.getWithDefault(1).Find(GetTestBytes(0))\n\t\t\trequire.Nil(t, r)\n\t\t\trequire.False(t, ok)\n\n\t\t\tr, ok = db.Index.bTree.getWithDefault(1).Find(GetTestBytes(1))\n\t\t\trequire.Nil(t, r)\n\t\t\trequire.False(t, ok)\n\t\t})\n\t})\n\n\tt.Run(\"update expire time\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, GetTestBytes(0), GetTestBytes(0), 1, nil, nil)\n\t\t\ttime.Sleep(500 * time.Millisecond)\n\n\t\t\t// reset expire time\n\t\t\ttxPut(t, db, bucket, GetTestBytes(0), GetTestBytes(0), 3, nil, nil)\n\t\t\ttime.Sleep(1 * time.Second)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(0), GetTestBytes(0), nil)\n\n\t\t\ttime.Sleep(3 * time.Second)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(0), nil, ErrKeyNotFound)\n\t\t})\n\t})\n\n\tt.Run(\"persist expire time\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, GetTestBytes(0), GetTestBytes(0), 1, nil, nil)\n\t\t\ttime.Sleep(500 * time.Millisecond)\n\n\t\t\t// persist expire time\n\t\t\ttxPut(t, db, bucket, GetTestBytes(0), GetTestBytes(0), Persistent, nil, nil)\n\t\t\ttime.Sleep(1 * time.Second)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(0), GetTestBytes(0), nil)\n\n\t\t\ttime.Sleep(3 * time.Second)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(0), GetTestBytes(0), nil)\n\t\t})\n\t})\n\n\tt.Run(\"expired deletion when open\", func(t *testing.T) {\n\t\topts := DefaultOptions\n\t\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, GetTestBytes(0), GetTestBytes(0), 1, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(1), GetTestBytes(1), 3, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(2), GetTestBytes(2), 3, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(3), GetTestBytes(3), Persistent, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(4), GetTestBytes(4), Persistent, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(5), GetTestBytes(5), 5, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(1), GetTestBytes(1), Persistent, nil, nil)\n\t\t\ttxDel(t, db, bucket, GetTestBytes(5), nil)\n\n\t\t\trequire.NoError(t, db.Close())\n\n\t\t\ttime.Sleep(1100 * time.Millisecond)\n\n\t\t\tdb, err := Open(opts)\n\t\t\trequire.NoError(t, err)\n\n\t\t\ttxGet(t, db, bucket, GetTestBytes(0), nil, ErrKeyNotFound)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), GetTestBytes(1), nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(2), GetTestBytes(2), nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(5), nil, ErrKeyNotFound)\n\n\t\t\ttime.Sleep(2 * time.Second)\n\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), GetTestBytes(1), nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(2), GetTestBytes(2), ErrKeyNotFound)\n\n\t\t\ttime.Sleep(2 * time.Second)\n\t\t\t// this entry should be persistent\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), GetTestBytes(1), nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(3), GetTestBytes(3), nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(4), GetTestBytes(4), nil)\n\t\t})\n\t})\n\n\tt.Run(\"expire deletion when merge\", func(t *testing.T) {\n\t\topts := DefaultOptions\n\t\topts.SegmentSize = 1 * 100\n\t\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\t\tbucket := \"bucket\"\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, GetTestBytes(0), GetTestBytes(0), Persistent, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(1), GetTestBytes(1), Persistent, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(2), GetTestBytes(2), 1, nil, nil)\n\n\t\t\ttime.Sleep(1100 * time.Millisecond)\n\n\t\t\trequire.NoError(t, db.Merge())\n\n\t\t\ttxGet(t, db, bucket, GetTestBytes(0), GetTestBytes(0), nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), GetTestBytes(1), nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(2), nil, ErrKeyNotFound)\n\n\t\t\tr, ok := db.Index.bTree.getWithDefault(1).Find(GetTestBytes(2))\n\t\t\trequire.Nil(t, r)\n\t\t\trequire.False(t, ok)\n\t\t})\n\t})\n\n\tt.Run(\"expire deletion use time heap\", func(t *testing.T) {\n\t\topts := DefaultOptions\n\t\topts.ExpiredDeleteType = TimeHeap\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(0), GetTestBytes(0), 1, nil, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(1), GetTestBytes(1), 2, nil, nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(0), GetTestBytes(0), nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), GetTestBytes(1), nil)\n\n\t\t\ttime.Sleep(1100 * time.Millisecond)\n\n\t\t\t// this entry will be deleted\n\t\t\ttxGet(t, db, bucket, GetTestBytes(0), nil, ErrKeyNotFound)\n\t\t\t// this entry still alive\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), GetTestBytes(1), nil)\n\n\t\t\ttime.Sleep(1 * time.Second)\n\n\t\t\t// this entry will be deleted\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), nil, ErrKeyNotFound)\n\n\t\t\tr, ok := db.Index.bTree.getWithDefault(1).Find(GetTestBytes(0))\n\t\t\trequire.Nil(t, r)\n\t\t\trequire.False(t, ok)\n\n\t\t\tr, ok = db.Index.bTree.getWithDefault(1).Find(GetTestBytes(1))\n\t\t\trequire.Nil(t, r)\n\t\t\trequire.False(t, ok)\n\t\t})\n\t})\n}\n\nfunc TestTx_GetMaxOrMinKey(t *testing.T) {\n\tbucket := \"bucket\"\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxPut(t, db, bucket, GetTestBytes(i), GetRandomBytes(24), Persistent, nil, nil)\n\t\t}\n\n\t\ttxGetMaxOrMinKey(t, db, bucket, true, GetTestBytes(9), nil)\n\t\ttxGetMaxOrMinKey(t, db, bucket, false, GetTestBytes(0), nil)\n\n\t\ttxDel(t, db, bucket, GetTestBytes(9), nil)\n\t\ttxDel(t, db, bucket, GetTestBytes(0), nil)\n\n\t\ttxGetMaxOrMinKey(t, db, bucket, true, GetTestBytes(8), nil)\n\t\ttxGetMaxOrMinKey(t, db, bucket, false, GetTestBytes(1), nil)\n\n\t\ttxPut(t, db, bucket, GetTestBytes(-1), GetRandomBytes(24), Persistent, nil, nil)\n\t\ttxPut(t, db, bucket, GetTestBytes(100), GetRandomBytes(24), Persistent, nil, nil)\n\n\t\ttxGetMaxOrMinKey(t, db, bucket, false, GetTestBytes(-1), nil)\n\t\ttxGetMaxOrMinKey(t, db, bucket, true, GetTestBytes(100), nil)\n\t})\n}\n\nfunc TestTx_updateOrPut(t *testing.T) {\n\tbucket := \"bucket\"\n\tt.Run(\"updateOrPut will return the expected error when it do update\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttxPut(t, db, bucket, []byte(\"any\"), []byte(\"any\"), Persistent, nil, nil)\n\n\t\t\ttx, err := db.Begin(true)\n\t\t\trequire.NoError(t, err)\n\t\t\texpectedErr := errors.New(\"some error\")\n\t\t\trequire.Equal(t, expectedErr, tx.updateOrPut(\"bucket\", []byte(\"any\"), []byte(\"any\"), func(bytes []byte) ([]byte, error) {\n\t\t\t\treturn nil, expectedErr\n\t\t\t}))\n\t\t\trequire.Equal(t, nil, tx.Commit())\n\t\t})\n\t})\n}\n\nfunc TestTx_IncrementAndDecrement(t *testing.T) {\n\tbucket := \"bucket\"\n\tgetIntegerValue := func(value int64) []byte {\n\t\treturn []byte(fmt.Sprintf(\"%d\", value))\n\t}\n\n\tkey := GetTestBytes(0)\n\n\tt.Run(\"increments and decrements on a non-exist key\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxIncrement(t, db, bucket, key, ErrKeyNotFound, nil)\n\t\t\ttxIncrementBy(t, db, bucket, key, 12, ErrKeyNotFound, nil)\n\t\t\ttxDecrement(t, db, bucket, key, ErrKeyNotFound, nil)\n\t\t\ttxDecrementBy(t, db, bucket, key, 12, ErrKeyNotFound, nil)\n\t\t})\n\t})\n\n\tt.Run(\"increments and decrements on a non-integer value\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, key, []byte(\"foo\"), Persistent, nil, nil)\n\n\t\t\ttxIncrement(t, db, bucket, key, ErrValueNotInteger, nil)\n\t\t\ttxIncrementBy(t, db, bucket, key, 12, ErrValueNotInteger, nil)\n\t\t\ttxDecrement(t, db, bucket, key, ErrValueNotInteger, nil)\n\t\t\ttxDecrementBy(t, db, bucket, key, 12, ErrValueNotInteger, nil)\n\t\t})\n\t})\n\n\tt.Run(\"increments and decrements normally\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, key, getIntegerValue(12), Persistent, nil, nil)\n\t\t\ttxIncrement(t, db, bucket, key, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, getIntegerValue(13), nil)\n\n\t\t\ttxDecrement(t, db, bucket, key, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, getIntegerValue(12), nil)\n\n\t\t\ttxIncrementBy(t, db, bucket, key, 12, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, getIntegerValue(24), nil)\n\n\t\t\ttxDecrementBy(t, db, bucket, key, 25, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, getIntegerValue(-1), nil)\n\t\t})\n\t})\n\n\tt.Run(\"increments and decrements over range of int64\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, key, []byte(\"9223372036854775818\"), Persistent, nil, nil)\n\t\t\ttxIncrement(t, db, bucket, key, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"9223372036854775819\"), nil)\n\n\t\t\ttxIncrementBy(t, db, bucket, key, 200, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"9223372036854776019\"), nil)\n\n\t\t\ttxDecrement(t, db, bucket, key, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"9223372036854776018\"), nil)\n\n\t\t\ttxDecrementBy(t, db, bucket, key, math.MaxInt64, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"211\"), nil)\n\n\t\t\ttxDecrementBy(t, db, bucket, key, math.MaxInt64, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"-9223372036854775596\"), nil)\n\t\t})\n\t})\n\n\tt.Run(\"increments and decrements on value which will overflow after operation\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, key, []byte(\"9223372036854775800\"), Persistent, nil, nil)\n\t\t\ttxIncrementBy(t, db, bucket, key, 100, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"9223372036854775900\"), nil)\n\n\t\t\ttxDecrementBy(t, db, bucket, key, math.MaxInt64, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"93\"), nil)\n\n\t\t\ttxDecrementBy(t, db, bucket, key, math.MaxInt64, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"-9223372036854775714\"), nil)\n\n\t\t\ttxDecrementBy(t, db, bucket, key, math.MaxInt64, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"-18446744073709551521\"), nil)\n\t\t})\n\t})\n\n\tt.Run(\"operations on expired key\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, key, []byte(\"1\"), 1, nil, nil)\n\n\t\t\ttime.Sleep(2 * time.Second)\n\n\t\t\ttxIncrement(t, db, bucket, key, ErrKeyNotFound, nil)\n\t\t\ttxIncrementBy(t, db, bucket, key, 12, ErrKeyNotFound, nil)\n\t\t\ttxDecrement(t, db, bucket, key, ErrKeyNotFound, nil)\n\t\t\ttxDecrementBy(t, db, bucket, key, 12, ErrKeyNotFound, nil)\n\t\t})\n\t})\n}\n\nfunc TestTx_PutIfNotExistsAndPutIfExists(t *testing.T) {\n\tbucket := \"bucket\"\n\tval := []byte(\"value\")\n\tupdated_val := []byte(\"updated_value\")\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxPutIfNotExists(t, db, bucket, GetTestBytes(i), val, nil, nil)\n\t\t}\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxGet(t, db, bucket, GetTestBytes(i), val, nil)\n\t\t}\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxPutIfExists(t, db, bucket, GetTestBytes(i), updated_val, nil, nil)\n\t\t}\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxGet(t, db, bucket, GetTestBytes(i), updated_val, nil)\n\t\t}\n\t})\n}\n\nfunc TestTx_GetAndSetBit(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\tt.Run(\"get bit on a non-exist key\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxGetBit(t, db, bucket, key, 0, 0, ErrKeyNotFound, nil)\n\t\t})\n\t})\n\n\tt.Run(\"set bit on a non-exist key\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxSetBit(t, db, bucket, key, 0, 1, nil, nil)\n\t\t\ttxGetBit(t, db, bucket, key, 0, 1, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"\\x01\"), nil)\n\n\t\t\ttxDel(t, db, bucket, key, nil)\n\t\t\ttxSetBit(t, db, bucket, key, 2, 1, nil, nil)\n\t\t\ttxGetBit(t, db, bucket, key, 2, 1, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"\\x00\\x00\\x01\"), nil)\n\n\t\t\ttxDel(t, db, bucket, key, nil)\n\t\t\ttxSetBit(t, db, bucket, key, 2, '1', nil, nil)\n\t\t\ttxGetBit(t, db, bucket, key, 2, '1', nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"\\x00\\x001\"), nil)\n\t\t})\n\t})\n\n\tt.Run(\"get and set bit on a exist string\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, key, []byte(\"foober\"), Persistent, nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"foober\"), nil)\n\t\t\ttxGetBit(t, db, bucket, key, 0, 'f', nil, nil)\n\n\t\t\ttxSetBit(t, db, bucket, key, 0, 'F', nil, nil)\n\t\t\ttxGetBit(t, db, bucket, key, 0, 'F', nil, nil)\n\n\t\t\ttxSetBit(t, db, bucket, key, 3, 'B', nil, nil)\n\t\t\ttxGetBit(t, db, bucket, key, 3, 'B', nil, nil)\n\n\t\t\ttxSetBit(t, db, bucket, key, 5, 'R', nil, nil)\n\t\t\ttxGetBit(t, db, bucket, key, 5, 'R', nil, nil)\n\n\t\t\ttxSetBit(t, db, bucket, key, 6, 'A', nil, nil)\n\t\t\ttxGetBit(t, db, bucket, key, 6, 'A', nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"FooBeRA\"), nil)\n\n\t\t\ttxSetBit(t, db, bucket, key, 12, 'C', nil, nil)\n\t\t\ttxGetBit(t, db, bucket, key, 12, 'C', nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"FooBeRA\\x00\\x00\\x00\\x00\\x00C\"), nil)\n\t\t})\n\t})\n\n\tt.Run(\"give a invalid offset\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxSetBit(t, db, bucket, key, math.MaxInt64, 0, ErrOffsetInvalid, nil)\n\t\t\ttxGetBit(t, db, bucket, key, math.MaxInt64, 0, ErrOffsetInvalid, nil)\n\n\t\t\ttxSetBit(t, db, bucket, key, 0, 1, nil, nil)\n\t\t\ttxGetBit(t, db, bucket, key, 1, 0, nil, nil)\n\t\t})\n\t})\n}\n\nfunc TestTx_ValueLen(t *testing.T) {\n\tbucket := \"bucket\"\n\tval := []byte(\"value\")\n\n\tt.Run(\"match length\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(1), val, Persistent, nil, nil)\n\t\t\ttxValueLen(t, db, bucket, GetTestBytes(1), 5, nil)\n\t\t})\n\t})\n\n\tt.Run(\"bucket not exist\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxValueLen(t, db, bucket, GetTestBytes(1), 0, ErrBucketNotExist)\n\t\t})\n\t})\n\n\tt.Run(\"key not found\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(1), val, Persistent, nil, nil)\n\t\t\ttxValueLen(t, db, bucket, GetTestBytes(2), 0, ErrKeyNotFound)\n\t\t})\n\t})\n\n\tt.Run(\"expired test\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(1), val, 1, nil, nil)\n\t\t\ttime.Sleep(3 * time.Second)\n\t\t\ttxValueLen(t, db, bucket, GetTestBytes(1), 0, ErrKeyNotFound)\n\t\t})\n\t})\n}\n\nfunc TestTx_GetSet(t *testing.T) {\n\tbucket := \"bucket\"\n\tval := []byte(\"value\")\n\tnewVal := []byte(\"new_value\")\n\n\tt.Run(\"match value\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(1), val, Persistent, nil, nil)\n\t\t\ttxGetSet(t, db, bucket, GetTestBytes(1), newVal, val, nil)\n\t\t\ttxGet(t, db, bucket, GetTestBytes(1), newVal, nil)\n\t\t})\n\t})\n\n\tt.Run(\"bucket not exist\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxGetSet(t, db, bucket, GetTestBytes(1), newVal, nil, ErrBucketNotExist)\n\t\t})\n\t})\n\n\tt.Run(\"expired test\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttxPut(t, db, bucket, GetTestBytes(1), val, 1, nil, nil)\n\t\t\ttime.Sleep(3 * time.Second)\n\t\t\ttxGetSet(t, db, bucket, GetTestBytes(1), newVal, nil, ErrKeyNotFound)\n\t\t})\n\t})\n}\n\nfunc TestTx_GetTTLAndPersist(t *testing.T) {\n\tbucket := \"bucket1\"\n\tkey := []byte(\"key1\")\n\tvalue := []byte(\"value1\")\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxGetTTL(t, db, bucket, key, 0, ErrBucketNotExist)\n\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\ttxGetTTL(t, db, bucket, key, 0, ErrKeyNotFound)\n\t\ttxPersist(t, db, bucket, key, ErrKeyNotFound)\n\n\t\ttxPut(t, db, bucket, key, value, 1, nil, nil)\n\t\ttime.Sleep(2 * time.Second) // Wait till value to expire\n\t\ttxGetTTL(t, db, bucket, key, 0, ErrKeyNotFound)\n\n\t\ttxPut(t, db, bucket, key, value, 100, nil, nil)\n\t\ttxGetTTL(t, db, bucket, key, 100, nil)\n\n\t\ttxPersist(t, db, bucket, key, nil)\n\t\ttxGetTTL(t, db, bucket, key, -1, nil)\n\t})\n}\n\nfunc TestTx_MSetMGet(t *testing.T) {\n\tbucket := \"bucket\"\n\n\tt.Run(\"use MSet and MGet with 0 args\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxMSet(t, db, bucket, nil, Persistent, nil, nil)\n\t\t\ttxMGet(t, db, bucket, nil, nil, nil, nil)\n\t\t})\n\t})\n\n\tt.Run(\"use MSet by using odd number of args \", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxMSet(t, db, bucket, [][]byte{\n\t\t\t\tGetTestBytes(0), GetTestBytes(1), GetTestBytes(2),\n\t\t\t}, Persistent, ErrKVArgsLenNotEven, nil)\n\t\t})\n\t})\n\n\tt.Run(\"use MSet and MGet normally\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxMSet(t, db, bucket, [][]byte{\n\t\t\t\tGetTestBytes(0), GetTestBytes(1), GetTestBytes(2), GetTestBytes(3),\n\t\t\t}, Persistent, nil, nil)\n\t\t\ttxMGet(t, db, bucket, [][]byte{\n\t\t\t\tGetTestBytes(0), GetTestBytes(2),\n\t\t\t}, [][]byte{\n\t\t\t\tGetTestBytes(1), GetTestBytes(3),\n\t\t\t}, nil, nil)\n\t\t})\n\t})\n}\n\nfunc TestTx_Append(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\tt.Run(\"use Append with a nil appendage\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxAppend(t, db, bucket, key, []byte(\"\"), nil, nil)\n\t\t\ttxAppend(t, db, bucket, key, nil, nil, nil)\n\t\t})\n\t})\n\n\tt.Run(\"use Append on a non-exist key\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxAppend(t, db, bucket, key, GetTestBytes(1), nil, nil)\n\t\t\ttxGet(t, db, bucket, key, GetTestBytes(1), nil)\n\t\t})\n\t})\n\n\tt.Run(\"use append on an exist key\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, key, []byte(\"test\"), Persistent, nil, nil)\n\t\t\ttxAppend(t, db, bucket, key, []byte(\"test\"), nil, nil)\n\t\t\ttxGet(t, db, bucket, key, []byte(\"testtest\"), nil)\n\t\t})\n\t})\n}\n\nfunc TestTx_GetRange(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\tt.Run(\"use GetRange with start greater than less\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxGetRange(t, db, bucket, key, 5, 1, nil, ErrStartGreaterThanEnd, nil)\n\t\t})\n\t})\n\n\tt.Run(\"use GetRange with start greater than size of value\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, key, []byte(\"test\"), Persistent, nil, nil)\n\t\t\ttxGetRange(t, db, bucket, key, 5, 10, nil, nil, nil)\n\t\t})\n\t})\n\n\tt.Run(\"use GetRange with end greater than size of value\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, key, []byte(\"test\"), Persistent, nil, nil)\n\t\t\ttxGetRange(t, db, bucket, key, 2, 10, []byte(\"st\"), nil, nil)\n\t\t})\n\t})\n\n\tt.Run(\"use GetRange normally\", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttxPut(t, db, bucket, key, []byte(\"test\"), Persistent, nil, nil)\n\t\t\ttxGetRange(t, db, bucket, key, 1, 2, []byte(\"es\"), nil, nil)\n\t\t\ttxGetRange(t, db, bucket, key, 1, 3, []byte(\"est\"), nil, nil)\n\t\t\ttxGetRange(t, db, bucket, key, 1, 4, []byte(\"est\"), nil, nil)\n\t\t})\n\t})\n}\n\nfunc TestBTreeInternalVisibility(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\ttxPut(t, db, bucket, key, []byte(\"test\"), Persistent, nil, nil)\n\t\terr := db.Update(func(tx *Tx) error {\n\t\t\terr := tx.Put(bucket, key, []byte(\"test-updated\"), Persistent)\n\t\t\tassert.Nil(t, err)\n\t\t\tvalue, err := tx.Get(bucket, key)\n\t\t\tassert.Nil(t, err)\n\t\t\tassert.Equal(t, \"test-updated\", string(value))\n\t\t\terr = tx.DeleteBucket(DataStructureBTree, bucket)\n\t\t\tassert.Nil(t, err)\n\t\t\terr = tx.Put(bucket, key, []byte(\"test-updated\"), Persistent)\n\t\t\tassert.Equal(t, ErrBucketNotFound, err)\n\t\t\treturn nil\n\t\t})\n\t\tassert.Equal(t, ErrBucketNotExist, err)\n\t})\n}\n"
        },
        {
          "name": "tx_bucket.go",
          "type": "blob",
          "size": 3.326171875,
          "content": "// Copyright 2022 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\n// IterateBuckets iterate over all the bucket depends on ds (represents the data structure)\nfunc (tx *Tx) IterateBuckets(ds uint16, pattern string, f func(bucket string) bool) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\n\tif ds == DataStructureSet {\n\t\tfor bucketId := range tx.db.Index.set.idx {\n\t\t\tbucket, err := tx.db.bm.GetBucketById(uint64(bucketId))\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif end, err := MatchForRange(pattern, bucket.Name, f); end || err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\tif ds == DataStructureSortedSet {\n\t\tfor bucketId := range tx.db.Index.sortedSet.idx {\n\t\t\tbucket, err := tx.db.bm.GetBucketById(uint64(bucketId))\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif end, err := MatchForRange(pattern, bucket.Name, f); end || err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\tif ds == DataStructureList {\n\t\tfor bucketId := range tx.db.Index.list.idx {\n\t\t\tbucket, err := tx.db.bm.GetBucketById(uint64(bucketId))\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif end, err := MatchForRange(pattern, bucket.Name, f); end || err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\tif ds == DataStructureBTree {\n\t\tfor bucketId := range tx.db.Index.bTree.idx {\n\t\t\tbucket, err := tx.db.bm.GetBucketById(uint64(bucketId))\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif end, err := MatchForRange(pattern, bucket.Name, f); end || err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (tx *Tx) NewKVBucket(name string) error {\n\treturn tx.NewBucket(DataStructureBTree, name)\n}\n\nfunc (tx *Tx) NewListBucket(name string) error {\n\treturn tx.NewBucket(DataStructureList, name)\n}\n\nfunc (tx *Tx) NewSetBucket(name string) error {\n\treturn tx.NewBucket(DataStructureSet, name)\n}\n\nfunc (tx *Tx) NewSortSetBucket(name string) error {\n\treturn tx.NewBucket(DataStructureSortedSet, name)\n}\n\nfunc (tx *Tx) NewBucket(ds uint16, name string) (err error) {\n\tif tx.ExistBucket(ds, name) {\n\t\treturn ErrBucketAlreadyExist\n\t}\n\tbucket := &Bucket{\n\t\tMeta: &BucketMeta{\n\t\t\tOp: BucketInsertOperation,\n\t\t},\n\t\tId:   tx.db.bm.Gen.GenId(),\n\t\tDs:   ds,\n\t\tName: name,\n\t}\n\tif _, exist := tx.pendingBucketList[ds]; !exist {\n\t\ttx.pendingBucketList[ds] = map[BucketName]*Bucket{}\n\t}\n\ttx.pendingBucketList[ds][name] = bucket\n\treturn nil\n}\n\n// DeleteBucket delete bucket depends on ds (represents the data structure)\nfunc (tx *Tx) DeleteBucket(ds uint16, bucket string) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(ds, bucket)\n\tif err != nil {\n\t\treturn ErrBucketNotFound\n\t}\n\n\tdeleteBucket := &Bucket{\n\t\tMeta: &BucketMeta{\n\t\t\tOp: BucketDeleteOperation,\n\t\t},\n\t\tId:   b.Id,\n\t\tDs:   ds,\n\t\tName: bucket,\n\t}\n\n\treturn tx.putBucket(deleteBucket)\n}\n\nfunc (tx *Tx) ExistBucket(ds uint16, bucket string) bool {\n\treturn tx.db.bm.ExistBucket(ds, bucket)\n}\n"
        },
        {
          "name": "tx_bucket_test.go",
          "type": "blob",
          "size": 2.685546875,
          "content": "// Copyright 2022 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nvar (\n\tsetBucketName    = \"set_bucket\"\n\tzSetBucketName   = \"zset_bucket\"\n\tlistBucketName   = \"list_bucket\"\n\tstringBucketName = \"string_bucket\"\n)\n\nfunc setupBucket(t *testing.T, db *DB) {\n\tkey := GetTestBytes(0)\n\tval := GetTestBytes(1)\n\ttxCreateBucket(t, db, DataStructureBTree, stringBucketName, nil)\n\ttxCreateBucket(t, db, DataStructureSet, setBucketName, nil)\n\ttxCreateBucket(t, db, DataStructureSortedSet, zSetBucketName, nil)\n\ttxCreateBucket(t, db, DataStructureList, listBucketName, nil)\n\n\ttxSAdd(t, db, setBucketName, key, val, nil, nil)\n\ttxZAdd(t, db, zSetBucketName, key, val, 80, nil, nil)\n\ttxPush(t, db, listBucketName, key, val, true, nil, nil)\n\ttxPut(t, db, stringBucketName, key, val, Persistent, nil, nil)\n}\n\nfunc TestBucket_IterateBuckets(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tsetupBucket(t, db)\n\n\t\ttxIterateBuckets(t, db, DataStructureSet, \"*\", func(bucket string) bool {\n\t\t\treturn true\n\t\t}, nil, setBucketName)\n\n\t\ttxIterateBuckets(t, db, DataStructureSortedSet, \"*\", func(bucket string) bool {\n\t\t\treturn true\n\t\t}, nil, zSetBucketName)\n\n\t\ttxIterateBuckets(t, db, DataStructureList, \"*\", func(bucket string) bool {\n\t\t\treturn true\n\t\t}, nil, listBucketName)\n\n\t\ttxIterateBuckets(t, db, DataStructureBTree, \"*\", func(bucket string) bool {\n\t\t\treturn true\n\t\t}, nil, stringBucketName)\n\n\t\tmatched := false\n\t\ttxIterateBuckets(t, db, DataStructureBTree, \"str*\", func(bucket string) bool {\n\t\t\tmatched = true\n\t\t\treturn true\n\t\t}, nil)\n\t\tassert.Equal(t, true, matched)\n\n\t\tmatched = false\n\t\ttxIterateBuckets(t, db, DataStructureList, \"str*\", func(bucket string) bool {\n\t\t\treturn true\n\t\t}, nil)\n\t\tassert.Equal(t, false, matched)\n\t})\n}\n\nfunc TestBucket_DeleteBucket(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\tsetupBucket(t, db)\n\n\t\ttxDeleteBucket(t, db, DataStructureSet, setBucketName, nil)\n\t\ttxDeleteBucket(t, db, DataStructureSortedSet, zSetBucketName, nil)\n\t\ttxDeleteBucket(t, db, DataStructureList, listBucketName, nil)\n\t\ttxDeleteBucket(t, db, DataStructureBTree, stringBucketName, nil)\n\n\t})\n}\n"
        },
        {
          "name": "tx_error.go",
          "type": "blob",
          "size": 2.537109375,
          "content": "package nutsdb\n\nimport \"errors\"\n\nvar (\n\t// ErrDataSizeExceed is returned when given key and value size is too big.\n\tErrDataSizeExceed = errors.New(\"data size too big\")\n\n\t// ErrTxClosed is returned when committing or rolling back a transaction\n\t// that has already been committed or rolled back.\n\tErrTxClosed = errors.New(\"tx is closed\")\n\n\t// ErrTxNotWritable is returned when performing a write operation on\n\t// a read-only transaction.\n\tErrTxNotWritable = errors.New(\"tx not writable\")\n\n\t// ErrKeyEmpty is returned if an empty key is passed on an update function.\n\tErrKeyEmpty = errors.New(\"key cannot be empty\")\n\n\t// ErrBucketEmpty is returned if bucket is empty.\n\tErrBucketEmpty = errors.New(\"bucket is empty\")\n\n\t// ErrRangeScan is returned when range scanning not found the result\n\tErrRangeScan = errors.New(\"range scans not found\")\n\n\t// ErrPrefixScan is returned when prefix scanning not found the result\n\tErrPrefixScan = errors.New(\"prefix scans not found\")\n\n\t// ErrPrefixSearchScan is returned when prefix and search scanning not found the result\n\tErrPrefixSearchScan = errors.New(\"prefix and search scans not found\")\n\n\t// ErrNotFoundKey is returned when key not found int the bucket on an view function.\n\tErrNotFoundKey = errors.New(\"key not found in the bucket\")\n\n\t// ErrCannotCommitAClosedTx is returned when the tx committing a closed tx\n\tErrCannotCommitAClosedTx = errors.New(\"can not commit a closed tx\")\n\n\t// ErrCannotRollbackACommittingTx is returned when the tx rollback a committing tx\n\tErrCannotRollbackACommittingTx = errors.New(\"can not rollback a committing tx\")\n\n\tErrCannotRollbackAClosedTx = errors.New(\"can not rollback a closed tx\")\n\n\t// ErrNotFoundBucket is returned when key not found int the bucket on an view function.\n\tErrNotFoundBucket = errors.New(\"bucket not found\")\n\n\t// ErrTxnTooBig is returned if too many writes are fit into a single transaction.\n\tErrTxnTooBig = errors.New(\"Txn is too big to fit into one request\")\n\n\t// ErrTxnExceedWriteLimit is returned when this tx's write is exceed max write record\n\tErrTxnExceedWriteLimit = errors.New(\"txn is exceed max write record count\")\n\n\tErrBucketAlreadyExist = errors.New(\"bucket is already exist\")\n\n\tErrorBucketNotExist = errors.New(\"bucket is not exist yet, please use NewBucket function to create this bucket first\")\n\n\tErrValueNotInteger = errors.New(\"value is not an integer\")\n\n\tErrOffsetInvalid = errors.New(\"offset is invalid\")\n\n\tErrKVArgsLenNotEven = errors.New(\"parameters is used to represent key value pairs and cannot be odd numbers\")\n\n\tErrStartGreaterThanEnd = errors.New(\"start is greater than end\")\n)\n"
        },
        {
          "name": "tx_list.go",
          "type": "blob",
          "size": 11.962890625,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"bytes\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/pkg/errors\"\n\t\"github.com/xujiajun/utils/strconv2\"\n)\n\nvar bucketKeySeqMap map[string]*HeadTailSeq\n\n// ErrSeparatorForListKey returns when list key contains the SeparatorForListKey.\nvar ErrSeparatorForListKey = errors.Errorf(\"contain separator (%s) for List key\", SeparatorForListKey)\n\n// SeparatorForListKey represents separator for listKey\nconst SeparatorForListKey = \"|\"\n\n// RPop removes and returns the last element of the list stored in the bucket at given bucket and key.\nfunc (tx *Tx) RPop(bucket string, key []byte) (item []byte, err error) {\n\titem, err = tx.RPeek(bucket, key)\n\tif err != nil {\n\t\treturn\n\t}\n\n\treturn item, tx.push(bucket, key, DataRPopFlag, item)\n}\n\n// RPeek returns the last element of the list stored in the bucket at given bucket and key.\nfunc (tx *Tx) RPeek(bucket string, key []byte) ([]byte, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar (\n\t\tbucketId = b.Id\n\t\tl        *List\n\t\texist    bool\n\t)\n\n\tif l, exist = tx.db.Index.list.exist(bucketId); !exist {\n\t\treturn nil, ErrBucket\n\t}\n\n\tif tx.CheckExpire(bucket, key) {\n\t\treturn nil, ErrListNotFound\n\t}\n\n\titem, err := l.RPeek(string(key))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tv, err := tx.db.getValueByRecord(item.record)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn v, nil\n}\n\n// push sets values for list stored in the bucket at given bucket, key, flag and values.\nfunc (tx *Tx) push(bucket string, key []byte, flag uint16, values ...[]byte) error {\n\tfor _, value := range values {\n\t\terr := tx.put(bucket, key, value, Persistent, flag, uint64(time.Now().Unix()), DataStructureList)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (tx *Tx) getListNewKey(bucket string, key []byte, isLeft bool) []byte {\n\tif bucketKeySeqMap == nil {\n\t\tbucketKeySeqMap = make(map[string]*HeadTailSeq)\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn nil\n\t}\n\tbucketId := b.Id\n\n\tbucketKey := bucket + string(key)\n\tif _, ok := bucketKeySeqMap[bucketKey]; !ok {\n\t\tbucketKeySeqMap[bucketKey] = tx.getListHeadTailSeq(bucketId, string(key))\n\t}\n\n\tseq := generateSeq(bucketKeySeqMap[bucketKey], isLeft)\n\treturn encodeListKey(key, seq)\n}\n\n// RPush inserts the values at the tail of the list stored in the bucket at given bucket,key and values.\nfunc (tx *Tx) RPush(bucket string, key []byte, values ...[]byte) error {\n\tif err := tx.isKeyValid(bucket, key); err != nil {\n\t\treturn err\n\t}\n\n\tif strings.Contains(string(key), SeparatorForListKey) {\n\t\treturn ErrSeparatorForListKey\n\t}\n\n\tfor _, value := range values {\n\t\tnewKey := tx.getListNewKey(bucket, key, false)\n\t\terr := tx.push(bucket, newKey, DataLPushFlag, value)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// LPush inserts the values at the head of the list stored in the bucket at given bucket,key and values.\nfunc (tx *Tx) LPush(bucket string, key []byte, values ...[]byte) error {\n\tif err := tx.isKeyValid(bucket, key); err != nil {\n\t\treturn err\n\t}\n\n\tif strings.Contains(string(key), SeparatorForListKey) {\n\t\treturn ErrSeparatorForListKey\n\t}\n\n\tfor _, value := range values {\n\t\tnewKey := tx.getListNewKey(bucket, key, true)\n\t\terr := tx.push(bucket, newKey, DataLPushFlag, value)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (tx *Tx) isKeyValid(bucket string, key []byte) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\n\tif tx.CheckExpire(bucket, key) {\n\t\treturn ErrListNotFound\n\t}\n\n\treturn nil\n}\n\nfunc (tx *Tx) LPushRaw(bucket string, key []byte, values ...[]byte) error {\n\tif err := tx.isKeyValid(bucket, key); err != nil {\n\t\treturn err\n\t}\n\n\treturn tx.push(bucket, key, DataLPushFlag, values...)\n}\n\nfunc (tx *Tx) RPushRaw(bucket string, key []byte, values ...[]byte) error {\n\tif err := tx.isKeyValid(bucket, key); err != nil {\n\t\treturn err\n\t}\n\n\treturn tx.push(bucket, key, DataRPushFlag, values...)\n}\n\n// LPop removes and returns the first element of the list stored in the bucket at given bucket and key.\nfunc (tx *Tx) LPop(bucket string, key []byte) (item []byte, err error) {\n\titem, err = tx.LPeek(bucket, key)\n\tif err != nil {\n\t\treturn\n\t}\n\n\treturn item, tx.push(bucket, key, DataLPopFlag, item)\n}\n\n// LPeek returns the first element of the list stored in the bucket at given bucket and key.\nfunc (tx *Tx) LPeek(bucket string, key []byte) (item []byte, err error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar (\n\t\tbucketId = b.Id\n\t\tl        *List\n\t\texist    bool\n\t)\n\n\tif l, exist = tx.db.Index.list.exist(bucketId); !exist {\n\t\treturn nil, ErrBucket\n\t}\n\tif tx.CheckExpire(bucket, key) {\n\t\treturn nil, ErrListNotFound\n\t}\n\tr, err := l.LPeek(string(key))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tv, err := tx.db.getValueByRecord(r.record)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn v, nil\n}\n\n// LSize returns the size of key in the bucket in the bucket at given bucket and key.\nfunc (tx *Tx) LSize(bucket string, key []byte) (int, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn 0, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tvar (\n\t\tbucketId = b.Id\n\t\tl        *List\n\t\texist    bool\n\t)\n\n\tif l, exist = tx.db.Index.list.exist(bucketId); !exist {\n\t\treturn 0, ErrBucket\n\t}\n\tif tx.CheckExpire(bucket, key) {\n\t\treturn 0, ErrListNotFound\n\t}\n\treturn l.Size(string(key))\n}\n\n// LRange returns the specified elements of the list stored in the bucket at given bucket,key, start and end.\n// The offsets start and stop are zero-based indexes 0 being the first element of the list (the head of the list),\n// 1 being the next element and so on.\n// Start and end can also be negative numbers indicating offsets from the end of the list,\n// where -1 is the last element of the list, -2 the penultimate element and so on.\nfunc (tx *Tx) LRange(bucket string, key []byte, start, end int) ([][]byte, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar (\n\t\tbucketId = b.Id\n\t\tl        *List\n\t\texist    bool\n\t)\n\n\tif l, exist = tx.db.Index.list.exist(bucketId); !exist {\n\t\treturn nil, ErrBucket\n\t}\n\tif tx.CheckExpire(bucket, key) {\n\t\treturn nil, ErrListNotFound\n\t}\n\n\trecords, err := l.LRange(string(key), start, end)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvalues := make([][]byte, len(records))\n\n\tfor i, r := range records {\n\t\tvalue, err := tx.db.getValueByRecord(r)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tvalues[i] = value\n\t}\n\n\treturn values, nil\n}\n\n// LRem removes the first count occurrences of elements equal to value from the list stored in the bucket at given bucket,key,count.\n// The count argument influences the operation in the following ways:\n// count > 0: Remove elements equal to value moving from head to tail.\n// count < 0: Remove elements equal to value moving from tail to head.\n// count = 0: Remove all elements equal to value.\nfunc (tx *Tx) LRem(bucket string, key []byte, count int, value []byte) error {\n\tvar (\n\t\tbuffer bytes.Buffer\n\t\tsize   int\n\t)\n\tsize, err := tx.LSize(bucket, key)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif count > size || count < -size {\n\t\treturn ErrCount\n\t}\n\n\tbuffer.Write([]byte(strconv2.IntToStr(count)))\n\tbuffer.Write([]byte(SeparatorForListKey))\n\tbuffer.Write(value)\n\tnewValue := buffer.Bytes()\n\n\terr = tx.push(bucket, key, DataLRemFlag, newValue)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// LTrim trims an existing list so that it will contain only the specified range of elements specified.\n// the offsets start and stop are zero-based indexes 0 being the first element of the list (the head of the list),\n// 1 being the next element and so on.\n// start and end can also be negative numbers indicating offsets from the end of the list,\n// where -1 is the last element of the list, -2 the penultimate element and so on.\nfunc (tx *Tx) LTrim(bucket string, key []byte, start, end int) error {\n\tvar (\n\t\terr    error\n\t\tbuffer bytes.Buffer\n\t)\n\n\tif err = tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvar (\n\t\tbucketId = b.Id\n\t\tl        *List\n\t\texist    bool\n\t)\n\n\tif l, exist = tx.db.Index.list.exist(bucketId); !exist {\n\t\treturn ErrBucket\n\t}\n\n\tif tx.CheckExpire(bucket, key) {\n\t\treturn ErrListNotFound\n\t}\n\tif _, ok := l.Items[string(key)]; !ok {\n\t\treturn ErrListNotFound\n\t}\n\n\tif _, err := tx.LRange(bucket, key, start, end); err != nil {\n\t\treturn err\n\t}\n\n\tbuffer.Write(key)\n\tbuffer.Write([]byte(SeparatorForListKey))\n\tbuffer.Write([]byte(strconv2.IntToStr(start)))\n\tnewKey := buffer.Bytes()\n\n\treturn tx.push(bucket, newKey, DataLTrimFlag, []byte(strconv2.IntToStr(end)))\n}\n\n// LRemByIndex remove the list element at specified index\nfunc (tx *Tx) LRemByIndex(bucket string, key []byte, indexes ...int) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbucketId := b.Id\n\tif _, ok := tx.db.Index.list.exist(bucketId); !ok {\n\t\treturn ErrListNotFound\n\t}\n\n\tif tx.CheckExpire(bucket, key) {\n\t\treturn ErrListNotFound\n\t}\n\n\tif len(indexes) == 0 {\n\t\treturn nil\n\t}\n\n\tsort.Ints(indexes)\n\tdata, err := MarshalInts(indexes)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = tx.push(bucket, key, DataLRemByIndex, data)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// LKeys find all keys matching a given pattern\nfunc (tx *Tx) LKeys(bucket, pattern string, f func(key string) bool) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\tvar (\n\t\tbucketId = b.Id\n\t\tl        *List\n\t\texist    bool\n\t)\n\tif l, exist = tx.db.Index.list.exist(bucketId); !exist {\n\t\treturn ErrBucket\n\t}\n\n\tfor key := range l.Items {\n\t\tif tx.CheckExpire(bucket, []byte(key)) {\n\t\t\tcontinue\n\t\t}\n\t\tif end, err := MatchForRange(pattern, key, f); end || err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (tx *Tx) ExpireList(bucket string, key []byte, ttl uint32) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvar (\n\t\tbucketId = b.Id\n\t\tl        *List\n\t\texist    bool\n\t)\n\n\tif l, exist = tx.db.Index.list.exist(bucketId); !exist {\n\t\treturn ErrBucket\n\t}\n\n\tl.TTL[string(key)] = ttl\n\tl.TimeStamp[string(key)] = uint64(time.Now().Unix())\n\tttls := strconv2.Int64ToStr(int64(ttl))\n\terr = tx.push(bucket, key, DataExpireListFlag, []byte(ttls))\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc (tx *Tx) CheckExpire(bucket string, key []byte) bool {\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn false\n\t}\n\n\tvar (\n\t\tbucketId = b.Id\n\t\tl        *List\n\t\texist    bool\n\t)\n\n\tif l, exist = tx.db.Index.list.exist(bucketId); !exist {\n\t\treturn false\n\t}\n\n\tif l.IsExpire(string(key)) {\n\t\t_ = tx.push(bucket, key, DataDeleteFlag)\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (tx *Tx) GetListTTL(bucket string, key []byte) (uint32, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn 0, err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureList, bucket)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tvar (\n\t\tbucketId = b.Id\n\t\tl        *List\n\t\texist    bool\n\t)\n\tif l, exist = tx.db.Index.list.exist(bucketId); !exist {\n\t\treturn 0, ErrBucket\n\t}\n\treturn l.GetListTTL(string(key))\n}\n"
        },
        {
          "name": "tx_list_test.go",
          "type": "blob",
          "size": 16.78125,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc pushDataByStartEnd(t *testing.T, db *DB, bucket string, key int, start, end int, isLeft bool) {\n\tfor i := start; i <= end; i++ {\n\t\ttxPush(t, db, bucket, GetTestBytes(key), GetTestBytes(i), isLeft, nil, nil)\n\t}\n}\n\nfunc pushDataByValues(t *testing.T, db *DB, bucket string, key int, isLeft bool, values ...int) {\n\tfor _, v := range values {\n\t\ttxPush(t, db, bucket, GetTestBytes(key), GetTestBytes(v), isLeft, nil, nil)\n\t}\n}\n\nfunc TestTx_RPush(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// 1. Insert values for some keys by using RPush\n\t// 2. Validate values for these keys by using RPop\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 9, false)\n\t\tpushDataByStartEnd(t, db, bucket, 1, 10, 19, false)\n\t\tpushDataByStartEnd(t, db, bucket, 2, 20, 29, false)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(9-i), nil, false)\n\t\t}\n\t\tfor i := 10; i < 20; i++ {\n\t\t\ttxPop(t, db, bucket, GetTestBytes(1), GetTestBytes(29-i), nil, false)\n\t\t}\n\t\tfor i := 20; i < 30; i++ {\n\t\t\ttxPop(t, db, bucket, GetTestBytes(2), GetTestBytes(49-i), nil, false)\n\t\t}\n\t})\n}\n\nfunc TestTx_MPush(t *testing.T) {\n\tbucket := \"bucket\"\n\tt.Run(\"Test Multiple LPush \", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\tbbs := make([][]byte, 0)\n\t\t\tbbs = append(bbs, GetTestBytes(2))\n\t\t\tbbs = append(bbs, GetTestBytes(3))\n\t\t\tbbs = append(bbs, GetTestBytes(4))\n\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\t\ttxMPush(t, db, bucket, GetTestBytes(1), bbs, true, nil, nil)\n\n\t\t\texpect := make([][]byte, 0)\n\t\t\tfor i := len(bbs) - 1; i >= 0; i-- {\n\t\t\t\texpect = append(expect, bbs[i])\n\t\t\t}\n\n\t\t\ttxLRange(t, db, bucket, GetTestBytes(1), 0, 2, 3, expect, nil)\n\t\t})\n\t})\n\n\tt.Run(\"Test Error LPush \", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\tbbs := make([][]byte, 0)\n\t\t\tbbs = append(bbs, GetTestBytes(2))\n\t\t\tbbs = append(bbs, GetTestBytes(3))\n\t\t\tbbs = append(bbs, GetTestBytes(4))\n\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\t\ttxMPush(t, db, \"test1\", GetTestBytes(1), bbs, true, ErrorBucketNotExist, nil)\n\t\t})\n\t})\n\n\tt.Run(\"Test Multiple RPush \", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\tbbs := make([][]byte, 0)\n\t\t\tbbs = append(bbs, GetTestBytes(2))\n\t\t\tbbs = append(bbs, GetTestBytes(3))\n\t\t\tbbs = append(bbs, GetTestBytes(4))\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\t\ttxMPush(t, db, bucket, GetTestBytes(1), bbs, false, nil, nil)\n\t\t\ttxLRange(t, db, bucket, GetTestBytes(1), 0, 2, 3, bbs, nil)\n\t\t})\n\t})\n\n\tt.Run(\"Test Error RPush \", func(t *testing.T) {\n\t\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\t\tbbs := make([][]byte, 0)\n\t\t\tbbs = append(bbs, GetTestBytes(2))\n\t\t\tbbs = append(bbs, GetTestBytes(3))\n\t\t\tbbs = append(bbs, GetTestBytes(4))\n\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\t\ttxMPush(t, db, \"test1\", GetTestBytes(1), bbs, false, ErrorBucketNotExist, nil)\n\t\t})\n\t})\n}\n\nfunc TestTx_LPush(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// 1. Insert values for some keys by using LPush\n\t// 2. Validate values for these keys by using LPop\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 9, true)\n\t\tpushDataByStartEnd(t, db, bucket, 1, 10, 19, true)\n\t\tpushDataByStartEnd(t, db, bucket, 2, 20, 29, true)\n\n\t\ttxPush(t, db, bucket, []byte(\"012|sas\"), GetTestBytes(0), true, ErrSeparatorForListKey, nil)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(9-i), nil, true)\n\t\t}\n\t\tfor i := 10; i < 20; i++ {\n\t\t\ttxPop(t, db, bucket, GetTestBytes(1), GetTestBytes(29-i), nil, true)\n\t\t}\n\t\tfor i := 20; i < 30; i++ {\n\t\t\ttxPop(t, db, bucket, GetTestBytes(2), GetTestBytes(49-i), nil, true)\n\t\t}\n\t})\n}\n\nfunc TestTx_LPushRaw(t *testing.T) {\n\tbucket := \"bucket\"\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\n\t\tseq := uint64(100000)\n\t\tfor i := 0; i <= 100; i++ {\n\t\t\tkey := encodeListKey([]byte(\"0\"), seq)\n\t\t\tseq--\n\t\t\ttxPushRaw(t, db, bucket, key, GetTestBytes(i), true, nil, nil)\n\t\t}\n\n\t\tfor i := 0; i <= 100; i++ {\n\t\t\tv := GetTestBytes(100 - i)\n\t\t\ttxPop(t, db, bucket, []byte(\"0\"), v, nil, true)\n\t\t}\n\t})\n}\n\nfunc TestTx_RPushRaw(t *testing.T) {\n\tbucket := \"bucket\"\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tseq := uint64(100000)\n\t\tfor i := 0; i <= 100; i++ {\n\t\t\tkey := encodeListKey([]byte(\"0\"), seq)\n\t\t\tseq++\n\t\t\ttxPushRaw(t, db, bucket, key, GetTestBytes(i), false, nil, nil)\n\t\t}\n\n\t\ttxPush(t, db, bucket, []byte(\"012|sas\"), GetTestBytes(0), false, ErrSeparatorForListKey, nil)\n\n\t\tfor i := 0; i <= 100; i++ {\n\t\t\tv := GetTestBytes(100 - i)\n\t\t\ttxPop(t, db, bucket, []byte(\"0\"), v, nil, false)\n\t\t}\n\t})\n}\n\nfunc TestTx_LPop(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// Calling LPop on a non-existent list\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\ttxPop(t, db, bucket, GetTestBytes(0), nil, ErrListNotFound, true)\n\t})\n\n\t// Insert some values for a key and validate them by using LPop\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 2, true)\n\t\tfor i := 0; i < 3; i++ {\n\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(2-i), nil, true)\n\t\t}\n\t})\n}\n\nfunc TestTx_RPop(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// Calling RPop on a non-existent list\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\ttxPop(t, db, bucket, GetTestBytes(0), nil, ErrListNotFound, false)\n\t})\n\n\t// Calling RPop on a list with added data\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 2, false)\n\n\t\ttxPop(t, db, \"fake_bucket\", GetTestBytes(0), nil, ErrBucketNotExist, false)\n\n\t\tfor i := 0; i < 3; i++ {\n\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(2-i), nil, false)\n\t\t}\n\n\t\ttxPop(t, db, bucket, GetTestBytes(0), nil, ErrEmptyList, false)\n\t})\n}\n\nfunc TestTx_LRange(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// Calling LRange on a non-existent list\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\n\t\ttxLRange(t, db, bucket, GetTestBytes(0), 0, -1, 0, nil, ErrListNotFound)\n\t})\n\n\t// Calling LRange on a list with added data\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 2, true)\n\n\t\ttxLRange(t, db, bucket, GetTestBytes(0), 0, -1, 3, [][]byte{\n\t\t\tGetTestBytes(2), GetTestBytes(1), GetTestBytes(0),\n\t\t}, nil)\n\n\t\tfor i := 0; i < 3; i++ {\n\t\t\ttxPop(t, db, bucket, GetTestBytes(0), GetTestBytes(2-i), nil, true)\n\t\t}\n\n\t\ttxLRange(t, db, bucket, GetTestBytes(0), 0, -1, 0, nil, nil)\n\t})\n}\n\nfunc TestTx_LRem(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// Calling LRem on a non-existent list\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\ttxLRem(t, db, bucket, GetTestBytes(0), 1, GetTestBytes(0), ErrListNotFound)\n\t})\n\n\t// A basic calling for LRem\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 3, true)\n\n\t\ttxLRem(t, db, bucket, GetTestBytes(0), 1, GetTestBytes(0), nil)\n\t\ttxLRange(t, db, bucket, GetTestBytes(0), 0, -1, 3, [][]byte{\n\t\t\tGetTestBytes(3), GetTestBytes(2), GetTestBytes(1),\n\t\t}, nil)\n\t\ttxLRem(t, db, bucket, GetTestBytes(0), 4, GetTestBytes(0), ErrCount)\n\t\ttxLRem(t, db, bucket, GetTestBytes(0), 1, GetTestBytes(1), nil)\n\t})\n\n\t// Calling LRem with count > 0\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\n\t\tcount := 3\n\n\t\tpushDataByValues(t, db, bucket, 1, true, 0, 1, 0, 1, 0, 1, 0, 1)\n\n\t\ttxLRange(t, db, bucket, GetTestBytes(1), 0, -1, 8, [][]byte{\n\t\t\tGetTestBytes(1), GetTestBytes(0), GetTestBytes(1), GetTestBytes(0),\n\t\t\tGetTestBytes(1), GetTestBytes(0), GetTestBytes(1), GetTestBytes(0),\n\t\t}, nil)\n\t\ttxLRem(t, db, bucket, GetTestBytes(1), count, GetTestBytes(0), nil)\n\t\ttxLRange(t, db, bucket, GetTestBytes(1), 0, -1, 5, [][]byte{\n\t\t\tGetTestBytes(1), GetTestBytes(1), GetTestBytes(1), GetTestBytes(1), GetTestBytes(0),\n\t\t}, nil)\n\t})\n\n\t// Calling LRem with count == 0\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tcount := 0\n\n\t\tpushDataByValues(t, db, bucket, 1, true, 0, 1, 0, 1, 0, 1, 0, 1)\n\n\t\ttxLRange(t, db, bucket, GetTestBytes(1), 0, -1, 8, [][]byte{\n\t\t\tGetTestBytes(1), GetTestBytes(0), GetTestBytes(1), GetTestBytes(0),\n\t\t\tGetTestBytes(1), GetTestBytes(0), GetTestBytes(1), GetTestBytes(0),\n\t\t}, nil)\n\t\ttxLRem(t, db, bucket, GetTestBytes(1), count, GetTestBytes(0), nil)\n\t\ttxLRange(t, db, bucket, GetTestBytes(1), 0, -1, 4, [][]byte{\n\t\t\tGetTestBytes(1), GetTestBytes(1), GetTestBytes(1), GetTestBytes(1),\n\t\t}, nil)\n\t})\n\n\t// Calling LRem with count < 0\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\n\t\tcount := -3\n\n\t\tpushDataByValues(t, db, bucket, 1, true, 0, 1, 0, 1, 0, 1, 0, 1)\n\n\t\ttxLRange(t, db, bucket, GetTestBytes(1), 0, -1, 8, [][]byte{\n\t\t\tGetTestBytes(1), GetTestBytes(0), GetTestBytes(1), GetTestBytes(0),\n\t\t\tGetTestBytes(1), GetTestBytes(0), GetTestBytes(1), GetTestBytes(0),\n\t\t}, nil)\n\t\ttxLRem(t, db, bucket, GetTestBytes(1), count, GetTestBytes(0), nil)\n\t\ttxLRange(t, db, bucket, GetTestBytes(1), 0, -1, 5, [][]byte{\n\t\t\tGetTestBytes(1), GetTestBytes(0), GetTestBytes(1), GetTestBytes(1), GetTestBytes(1),\n\t\t}, nil)\n\t})\n}\n\nfunc TestTx_LTrim(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// Calling LTrim on a non-existent list\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\ttxLTrim(t, db, bucket, GetTestBytes(0), 0, 1, ErrListNotFound)\n\t})\n\n\t// Calling LTrim on a list with added data and use LRange to validate it\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 2, true)\n\t\ttxLTrim(t, db, bucket, GetTestBytes(0), 0, 1, nil)\n\n\t\ttxLRange(t, db, bucket, GetTestBytes(0), 0, -1, 2, [][]byte{\n\t\t\tGetTestBytes(2), GetTestBytes(1),\n\t\t}, nil)\n\t})\n\n\t// Calling LTrim with incorrect start and end\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\n\t\tfor i := 0; i < 3; i++ {\n\t\t\ttxPush(t, db, bucket, GetTestBytes(2), GetTestBytes(i), true, nil, nil)\n\t\t}\n\t\ttxLTrim(t, db, bucket, GetTestBytes(2), 0, -10, ErrStartOrEnd)\n\t})\n}\n\nfunc TestTx_LSize(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// Calling LSize on a non-existent list\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\ttxLSize(t, db, bucket, GetTestBytes(0), 0, ErrListNotFound)\n\t})\n\n\t// Calling LSize after adding some values\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 2, false)\n\t\ttxLSize(t, db, bucket, GetTestBytes(0), 3, nil)\n\t})\n}\n\nfunc TestTx_LRemByIndex(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// Calling LRemByIndex on a newly created empty list\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\ttxLRemByIndex(t, db, bucket, GetTestBytes(0), nil)\n\t})\n\n\t// Calling LRemByIndex with len(indexes) == 0\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByValues(t, db, bucket, 0, true, 0)\n\t\ttxLRemByIndex(t, db, bucket, GetTestBytes(0), nil)\n\t})\n\n\t// Calling LRemByIndex with a expired bucket name\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByValues(t, db, bucket, 0, true, 0)\n\t\ttxExpireList(t, db, bucket, GetTestBytes(0), 1, nil)\n\t\ttime.Sleep(3 * time.Second)\n\t\ttxLRemByIndex(t, db, bucket, GetTestBytes(0), ErrListNotFound)\n\t})\n\n\t// Calling LRemByIndex on a list with added data and use LRange to validate it\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 2, false)\n\t\ttxLRemByIndex(t, db, bucket, GetTestBytes(0), nil, 1, 0, 8, -8, 88, -88)\n\t\ttxLRange(t, db, bucket, GetTestBytes(0), 0, -1, 1, [][]byte{\n\t\t\tGetTestBytes(2),\n\t\t}, nil)\n\t})\n}\n\nfunc TestTx_ExpireList(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// Verify that the list with expiration time expires normally\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 3, false)\n\t\ttxLRange(t, db, bucket, GetTestBytes(0), 0, -1, 4, [][]byte{\n\t\t\tGetTestBytes(0), GetTestBytes(1), GetTestBytes(2), GetTestBytes(3),\n\t\t}, nil)\n\n\t\ttxExpireList(t, db, bucket, GetTestBytes(0), 1, nil)\n\t\ttime.Sleep(time.Second)\n\t\ttxLRange(t, db, bucket, GetTestBytes(0), 0, -1, 0, nil, ErrListNotFound)\n\t})\n\n\t// Verify that the list with persistent time\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 3, false)\n\t\ttxExpireList(t, db, bucket, GetTestBytes(0), Persistent, nil)\n\t\ttime.Sleep(time.Second)\n\t\ttxLRange(t, db, bucket, GetTestBytes(0), 0, -1, 4, [][]byte{\n\t\t\tGetTestBytes(0), GetTestBytes(1), GetTestBytes(2), GetTestBytes(3),\n\t\t}, nil)\n\t})\n}\n\nfunc TestTx_LKeys(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// Calling LKeys after adding some keys\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByValues(t, db, bucket, 10, false, 0)\n\t\tpushDataByValues(t, db, bucket, 11, false, 1)\n\t\tpushDataByValues(t, db, bucket, 12, false, 2)\n\t\tpushDataByValues(t, db, bucket, 23, false, 3)\n\n\t\ttxLKeys(t, db, bucket, \"*\", 4, nil, func(keys []string) bool {\n\t\t\treturn true\n\t\t})\n\n\t\ttxLKeys(t, db, bucket, \"*\", 2, nil, func(keys []string) bool {\n\t\t\treturn len(keys) != 2\n\t\t})\n\n\t\ttxLKeys(t, db, bucket, \"nutsdb-00000001*\", 3, nil, func(keys []string) bool {\n\t\t\treturn true\n\t\t})\n\t})\n}\n\nfunc TestTx_GetListTTL(t *testing.T) {\n\tbucket := \"bucket\"\n\n\t// Verify TTL of list\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\tpushDataByStartEnd(t, db, bucket, 0, 0, 3, false)\n\n\t\ttxGetListTTL(t, db, bucket, GetTestBytes(0), uint32(0), nil)\n\t\ttxExpireList(t, db, bucket, GetTestBytes(0), uint32(1), nil)\n\t\ttxGetListTTL(t, db, bucket, GetTestBytes(0), uint32(1), nil)\n\n\t\ttime.Sleep(3 * time.Second)\n\t\ttxGetListTTL(t, db, bucket, GetTestBytes(0), uint32(0), ErrListNotFound)\n\t})\n}\n\nfunc TestTx_ListEntryIdxMode_HintKeyValAndRAMIdxMode(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\topts := DefaultOptions\n\topts.EntryIdxMode = HintKeyValAndRAMIdxMode\n\n\t// HintKeyValAndRAMIdxMode\n\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\terr := db.Update(func(tx *Tx) error {\n\t\t\terr := tx.LPush(bucket, key, []byte(\"d\"), []byte(\"c\"), []byte(\"b\"), []byte(\"a\"))\n\t\t\trequire.NoError(t, err)\n\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\tlistIdx := db.Index.list.getWithDefault(1)\n\t\titem, ok := listIdx.Items[string(key)].PopMin()\n\t\tr := item.record\n\t\trequire.True(t, ok)\n\t\trequire.NotNil(t, r.Value)\n\t\trequire.Equal(t, []byte(\"a\"), r.Value)\n\t})\n}\n\nfunc TestTx_ListEntryIdxMode_HintKeyAndRAMIdxMode(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\topts := &DefaultOptions\n\topts.EntryIdxMode = HintKeyAndRAMIdxMode\n\n\t// HintKeyAndRAMIdxMode\n\trunNutsDBTest(t, opts, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\terr := db.Update(func(tx *Tx) error {\n\t\t\terr := tx.LPush(bucket, key, []byte(\"d\"), []byte(\"c\"), []byte(\"b\"), []byte(\"a\"))\n\t\t\trequire.NoError(t, err)\n\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\tlistIdx := db.Index.list.getWithDefault(1)\n\t\titem, ok := listIdx.Items[string(key)].PopMin()\n\t\tr := item.record\n\t\trequire.True(t, ok)\n\t\trequire.Nil(t, r.Value)\n\n\t\tval, err := db.getValueByRecord(r)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []byte(\"a\"), val)\n\t})\n}\n"
        },
        {
          "name": "tx_set.go",
          "type": "blob",
          "size": 12.517578125,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"time\"\n\n\t\"github.com/pkg/errors\"\n)\n\nfunc (tx *Tx) sPut(bucket string, key []byte, dataFlag uint16, values ...[]byte) error {\n\n\tif dataFlag == DataSetFlag {\n\n\t\tfilter := make(map[uint32]struct{})\n\n\t\tb1, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tbucketId := b1.Id\n\n\t\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\n\t\t\tif _, ok := set.M[string(key)]; ok {\n\t\t\t\tfor hash := range set.M[string(key)] {\n\t\t\t\t\tfilter[hash] = struct{}{}\n\t\t\t\t}\n\t\t\t}\n\n\t\t}\n\n\t\tfor _, value := range values {\n\t\t\thash, err := getFnv32(value)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif _, ok := filter[hash]; !ok {\n\t\t\t\tfilter[hash] = struct{}{}\n\t\t\t\terr := tx.put(bucket, key, value, Persistent, dataFlag, uint64(time.Now().Unix()), DataStructureSet)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t} else {\n\t\tfor _, value := range values {\n\n\t\t\terr := tx.put(bucket, key, value, Persistent, dataFlag, uint64(time.Now().Unix()), DataStructureSet)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// SAdd adds the specified members to the set stored int the bucket at given bucket,key and items.\nfunc (tx *Tx) SAdd(bucket string, key []byte, items ...[]byte) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\treturn tx.sPut(bucket, key, DataSetFlag, items...)\n}\n\n// SRem removes the specified members from the set stored int the bucket at given bucket,key and items.\nfunc (tx *Tx) SRem(bucket string, key []byte, items ...[]byte) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbucketId := b.Id\n\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\t\tok, err := set.SAreMembers(string(key), items...)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif !ok {\n\t\t\treturn ErrSetMemberNotExist\n\t\t}\n\t\treturn tx.sPut(bucket, key, DataDeleteFlag, items...)\n\t}\n\treturn ErrBucketNotFound\n}\n\n// SAreMembers returns if the specified members are the member of the set int the bucket at given bucket,key and items.\nfunc (tx *Tx) SAreMembers(bucket string, key []byte, items ...[]byte) (bool, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn false, err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tbucketId := b.Id\n\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\t\treturn set.SAreMembers(string(key), items...)\n\t}\n\n\treturn false, ErrBucketNotFound\n}\n\n// SIsMember returns if member is a member of the set stored int the bucket at given bucket,key and item.\nfunc (tx *Tx) SIsMember(bucket string, key, item []byte) (bool, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn false, err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tbucketId := b.Id\n\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\t\tisMember, err := set.SIsMember(string(key), item)\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\t\treturn isMember, nil\n\t}\n\n\treturn false, ErrBucketNotFound\n}\n\n// SMembers returns all the members of the set value stored int the bucket at given bucket and key.\nfunc (tx *Tx) SMembers(bucket string, key []byte) ([][]byte, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId := b.Id\n\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\t\titems, err := set.SMembers(string(key))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tvalues := make([][]byte, len(items))\n\t\tfor i, item := range items {\n\t\t\tvalue, err := tx.db.getValueByRecord(item)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tvalues[i] = value\n\t\t}\n\n\t\treturn values, nil\n\t}\n\n\treturn nil, ErrBucketNotFound\n\n}\n\n// SHasKey returns if the set in the bucket at given bucket and key.\nfunc (tx *Tx) SHasKey(bucket string, key []byte) (bool, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn false, err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tbucketId := b.Id\n\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\t\treturn set.SHasKey(string(key)), nil\n\t}\n\n\treturn false, ErrBucketNotFound\n}\n\n// SPop removes and returns one or more random elements from the set value store in the bucket at given bucket and key.\nfunc (tx *Tx) SPop(bucket string, key []byte) ([]byte, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId := b.Id\n\n\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\t\tfor _, items := range set.M[string(key)] {\n\t\t\tvalue, err := tx.db.getValueByRecord(items)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\terr = tx.sPut(bucket, key, DataDeleteFlag, value)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\treturn value, err\n\t\t}\n\t}\n\n\treturn nil, ErrBucketNotFound\n}\n\n// SCard returns the set cardinality (number of elements) of the set stored in the bucket at given bucket and key.\nfunc (tx *Tx) SCard(bucket string, key []byte) (int, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn 0, err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tbucketId := b.Id\n\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\t\treturn set.SCard(string(key)), nil\n\t}\n\n\treturn 0, ErrBucketNotFound\n}\n\n// SDiffByOneBucket returns the members of the set resulting from the difference\n// between the first set and all the successive sets in one bucket.\nfunc (tx *Tx) SDiffByOneBucket(bucket string, key1, key2 []byte) ([][]byte, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId := b.Id\n\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\t\titems, err := set.SDiff(string(key1), string(key2))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tvalues := make([][]byte, len(items))\n\t\tfor i, item := range items {\n\t\t\tvalue, err := tx.db.getValueByRecord(item)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tvalues[i] = value\n\t\t}\n\t\treturn values, nil\n\t}\n\n\treturn nil, ErrBucketNotFound\n}\n\n// SDiffByTwoBuckets returns the members of the set resulting from the difference\n// between the first set and all the successive sets in two buckets.\nfunc (tx *Tx) SDiffByTwoBuckets(bucket1 string, key1 []byte, bucket2 string, key2 []byte) ([][]byte, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tset1, set2 *Set\n\t\tok         bool\n\t)\n\n\tb1, err := tx.db.bm.GetBucket(DataStructureSet, bucket1)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId1 := b1.Id\n\n\tb2, err := tx.db.bm.GetBucket(DataStructureSet, bucket2)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId2 := b2.Id\n\n\tif set1, ok = tx.db.Index.set.exist(bucketId1); !ok {\n\t\treturn nil, ErrBucketAndKey(bucket1, key1)\n\t}\n\n\tif set2, ok = tx.db.Index.set.exist(bucketId2); !ok {\n\t\treturn nil, ErrBucketAndKey(bucket2, key2)\n\t}\n\n\tvalues := make([][]byte, 0)\n\n\tfor hash, item := range set1.M[string(key1)] {\n\t\tif _, ok := set2.M[string(key2)][hash]; !ok {\n\t\t\tvalue, err := tx.db.getValueByRecord(item)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tvalues = append(values, value)\n\t\t}\n\t}\n\n\treturn values, nil\n}\n\n// SMoveByOneBucket moves member from the set at source to the set at destination in one bucket.\nfunc (tx *Tx) SMoveByOneBucket(bucket string, key1, key2, item []byte) (bool, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn false, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tbucketId := b.Id\n\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\t\treturn set.SMove(string(key1), string(key2), item)\n\t}\n\n\treturn false, ErrBucket\n}\n\n// SMoveByTwoBuckets moves member from the set at source to the set at destination in two buckets.\nfunc (tx *Tx) SMoveByTwoBuckets(bucket1 string, key1 []byte, bucket2 string, key2, item []byte) (bool, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn false, err\n\t}\n\n\tvar (\n\t\tset1, set2 *Set\n\t\tok         bool\n\t)\n\n\tb1, err := tx.db.bm.GetBucket(DataStructureSet, bucket1)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tbucketId1 := b1.Id\n\n\tb2, err := tx.db.bm.GetBucket(DataStructureSet, bucket2)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tbucketId2 := b2.Id\n\n\tif set1, ok = tx.db.Index.set.exist(bucketId1); !ok {\n\t\treturn false, ErrBucketAndKey(bucket1, key1)\n\t}\n\n\tif set2, ok = tx.db.Index.set.exist(bucketId2); !ok {\n\t\treturn false, ErrBucketAndKey(bucket2, key1)\n\t}\n\n\tif !set1.SHasKey(string(key1)) {\n\t\treturn false, ErrNotFoundKeyInBucket(bucket1, key1)\n\t}\n\n\tif !set2.SHasKey(string(key2)) {\n\t\treturn false, ErrNotFoundKeyInBucket(bucket2, key2)\n\t}\n\n\thash, err := getFnv32(item)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\n\tif r, ok := set2.M[string(key2)][hash]; !ok {\n\t\terr := set2.SAdd(string(key2), [][]byte{item}, []*Record{r})\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\t}\n\n\terr = set1.SRem(string(key1), item)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\n\treturn true, nil\n}\n\n// SUnionByOneBucket the members of the set resulting from the union of all the given sets in one bucket.\nfunc (tx *Tx) SUnionByOneBucket(bucket string, key1, key2 []byte) ([][]byte, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\tb1, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId := b1.Id\n\n\tif set, ok := tx.db.Index.set.exist(bucketId); ok {\n\t\titems, err := set.SUnion(string(key1), string(key2))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tvalues := make([][]byte, len(items))\n\n\t\tfor i, item := range items {\n\t\t\tvalue, err := tx.db.getValueByRecord(item)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tvalues[i] = value\n\t\t}\n\t\treturn values, nil\n\t}\n\n\treturn nil, ErrBucket\n}\n\n// SUnionByTwoBuckets the members of the set resulting from the union of all the given sets in two buckets.\nfunc (tx *Tx) SUnionByTwoBuckets(bucket1 string, key1 []byte, bucket2 string, key2 []byte) ([][]byte, error) {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tset1, set2 *Set\n\t\tok         bool\n\t)\n\tb1, err := tx.db.bm.GetBucket(DataStructureSet, bucket1)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId1 := b1.Id\n\n\tb2, err := tx.db.bm.GetBucket(DataStructureSet, bucket2)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbucketId2 := b2.Id\n\n\tif set1, ok = tx.db.Index.set.exist(bucketId1); !ok {\n\t\treturn nil, ErrBucketAndKey(bucket1, key1)\n\t}\n\n\tif set2, ok = tx.db.Index.set.exist(bucketId2); !ok {\n\t\treturn nil, ErrBucketAndKey(bucket2, key2)\n\t}\n\n\tif !set1.SHasKey(string(key1)) {\n\t\treturn nil, ErrNotFoundKeyInBucket(bucket1, key1)\n\t}\n\n\tif !set2.SHasKey(string(key2)) {\n\t\treturn nil, ErrNotFoundKeyInBucket(bucket2, key2)\n\t}\n\n\tvalues := make([][]byte, 0)\n\n\tfor _, r := range set1.M[string(key1)] {\n\t\tvalue, err := tx.db.getValueByRecord(r)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tvalues = append(values, value)\n\t}\n\n\tfor hash, r := range set2.M[string(key2)] {\n\t\tif _, ok := set1.M[string(key1)][hash]; !ok {\n\t\t\tvalue, err := tx.db.getValueByRecord(r)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tvalues = append(values, value)\n\t\t}\n\t}\n\n\treturn values, nil\n}\n\n// SKeys find all keys matching a given pattern\nfunc (tx *Tx) SKeys(bucket, pattern string, f func(key string) bool) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\tb, err := tx.db.bm.GetBucket(DataStructureSet, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbucketId := b.Id\n\tif set, ok := tx.db.Index.set.exist(bucketId); !ok {\n\t\treturn ErrBucket\n\t} else {\n\t\tfor key := range set.M {\n\t\t\tif end, err := MatchForRange(pattern, key, f); end || err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// ErrBucketAndKey returns when bucket or key not found.\nfunc ErrBucketAndKey(bucket string, key []byte) error {\n\treturn errors.Wrapf(ErrBucketNotFound, \"bucket:%s, key:%s\", bucket, key)\n}\n\n// ErrNotFoundKeyInBucket returns when key not in the bucket.\nfunc ErrNotFoundKeyInBucket(bucket string, key []byte) error {\n\treturn errors.Wrapf(ErrNotFoundKey, \"%s is not found in %s\", key, bucket)\n}\n"
        },
        {
          "name": "tx_set_test.go",
          "type": "blob",
          "size": 12.853515625,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestTx_SAdd(t *testing.T) {\n\tbucket := \"bucket\"\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, []byte(\"\"), []byte(\"val1\"), ErrKeyEmpty, nil)\n\n\t\tkey := GetTestBytes(0)\n\t\tnum := 10\n\t\tfor i := 0; i < num; i++ {\n\t\t\ttxSAdd(t, db, bucket, key, GetTestBytes(i), nil, nil)\n\t\t}\n\n\t\tfor i := 0; i < num; i++ {\n\t\t\ttxSIsMember(t, db, bucket, key, GetTestBytes(i), true)\n\t\t}\n\n\t\ttxSIsMember(t, db, bucket, key, GetTestBytes(num), false)\n\t})\n}\n\nfunc TestTx_SRem(t *testing.T) {\n\tbucket := \"bucket\"\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\tkey := []byte(\"key1\")\n\t\tval1 := []byte(\"one\")\n\t\tval2 := []byte(\"two\")\n\t\tval3 := []byte(\"three\")\n\n\t\ttxSAdd(t, db, bucket, key, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key, val2, nil, nil)\n\t\ttxSAdd(t, db, bucket, key, val3, nil, nil)\n\n\t\ttxSRem(t, db, bucket, key, val3, nil)\n\n\t\ttxSIsMember(t, db, bucket, key, val1, true)\n\t\ttxSIsMember(t, db, bucket, key, val2, true)\n\t\ttxSIsMember(t, db, bucket, key, val3, false)\n\t})\n}\n\nfunc TestTx_SRem2(t *testing.T) {\n\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\tval1 := GetTestBytes(0)\n\tval2 := GetTestBytes(1)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, key, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key, val2, nil, nil)\n\n\t\ttxSRem(t, db, bucket, key, val1, nil)\n\t\ttxSRem(t, db, bucket, key, val1, ErrSetMemberNotExist)\n\n\t\ttxSRem(t, db, bucket, key, val2, nil)\n\n\t\ttxSAreMembers(t, db, bucket, key, false, val1, val2)\n\t})\n}\n\nfunc TestTx_SMembers(t *testing.T) {\n\tbucket := \"bucket\"\n\tfakeBucket := \"fake_bucket\"\n\tkey := GetTestBytes(0)\n\tval1 := GetTestBytes(0)\n\tval2 := GetTestBytes(1)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, key, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key, val2, nil, nil)\n\n\t\ttxSMembers(t, db, bucket, key, 2, nil)\n\n\t\ttxSIsMember(t, db, bucket, key, val1, true)\n\t\ttxSIsMember(t, db, bucket, key, val1, true)\n\n\t\ttxSMembers(t, db, fakeBucket, key, 0, ErrBucketNotExist)\n\t})\n}\n\nfunc TestTx_SCard(t *testing.T) {\n\tbucket := \"bucket\"\n\tfakeBucket := \"fake_bucket\"\n\tkey := GetTestBytes(0)\n\tval1 := GetTestBytes(1)\n\tval2 := GetTestBytes(2)\n\tval3 := GetTestBytes(3)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, key, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key, val2, nil, nil)\n\t\ttxSAdd(t, db, bucket, key, val3, nil, nil)\n\n\t\ttxSCard(t, db, bucket, key, 3, nil)\n\n\t\ttxSCard(t, db, fakeBucket, key, 0, ErrBucketNotExist)\n\t})\n}\n\nfunc TestTx_SDiffByOneBucket(t *testing.T) {\n\tbucket := \"bucket\"\n\tfakeBucket := \"fake_bucket\"\n\tkey1 := GetTestBytes(0)\n\tkey2 := GetTestBytes(1)\n\tkey3 := GetTestBytes(2)\n\tval1 := GetTestBytes(1)\n\tval2 := GetTestBytes(2)\n\tval3 := GetTestBytes(3)\n\tval4 := GetTestBytes(4)\n\tval5 := GetTestBytes(5)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, key1, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key1, val2, nil, nil)\n\t\ttxSAdd(t, db, bucket, key1, val3, nil, nil)\n\n\t\ttxSAdd(t, db, bucket, key2, val3, nil, nil)\n\t\ttxSAdd(t, db, bucket, key2, val4, nil, nil)\n\t\ttxSAdd(t, db, bucket, key2, val5, nil, nil)\n\n\t\tdiff := [][]byte{val1, val2}\n\t\ttxSDiffByOneBucket(t, db, bucket, key1, key2, diff, nil)\n\t\ttxSDiffByOneBucket(t, db, fakeBucket, key2, key1, nil, ErrBucketNotExist)\n\n\t\ttxSAdd(t, db, bucket, key3, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key3, val2, nil, nil)\n\n\t\tfor _, val := range diff {\n\t\t\ttxSIsMember(t, db, bucket, key3, val, true)\n\t\t}\n\t})\n}\n\nfunc TestTx_SDiffByTwoBuckets(t *testing.T) {\n\tbucket1 := \"bucket1\"\n\tbucket2 := \"bucket2\"\n\tbucket3 := \"bucket3\"\n\tfakeBucket := \"fake_bucket_%d\"\n\tkey1 := GetTestBytes(0)\n\tkey2 := GetTestBytes(1)\n\tkey3 := GetTestBytes(2)\n\tval1 := GetTestBytes(1)\n\tval2 := GetTestBytes(2)\n\tval3 := GetTestBytes(3)\n\tval4 := GetTestBytes(4)\n\tval5 := GetTestBytes(5)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket1, nil)\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket2, nil)\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket3, nil)\n\n\t\ttxSAdd(t, db, bucket1, key1, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket1, key1, val2, nil, nil)\n\t\ttxSAdd(t, db, bucket1, key1, val3, nil, nil)\n\n\t\ttxSAdd(t, db, bucket2, key2, val3, nil, nil)\n\t\ttxSAdd(t, db, bucket2, key2, val4, nil, nil)\n\t\ttxSAdd(t, db, bucket2, key2, val5, nil, nil)\n\n\t\tdiff := [][]byte{val1, val2}\n\t\ttxSDiffByTwoBucket(t, db, bucket1, key1, bucket2, key2, diff, nil)\n\n\t\ttxSDiffByTwoBucket(t, db, fmt.Sprintf(fakeBucket, 1), key1, bucket2, key2, nil, ErrBucketNotExist)\n\t\ttxSDiffByTwoBucket(t, db, bucket1, key1, fmt.Sprintf(fakeBucket, 2), key2, nil, ErrBucketNotExist)\n\n\t\ttxSAdd(t, db, bucket3, key3, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket3, key3, val2, nil, nil)\n\n\t\tfor _, val := range diff {\n\t\t\ttxSIsMember(t, db, bucket3, key3, val, true)\n\t\t}\n\t})\n}\n\nfunc TestTx_SPop(t *testing.T) {\n\tbucket := \"bucket\"\n\tfakeBucket := \"fake_bucket\"\n\tkey := GetTestBytes(0)\n\tval1 := GetTestBytes(1)\n\tval2 := GetTestBytes(2)\n\tval3 := GetTestBytes(3)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, key, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key, val2, nil, nil)\n\t\ttxSAdd(t, db, bucket, key, val3, nil, nil)\n\n\t\ttxSCard(t, db, bucket, key, 3, nil)\n\t\ttxSPop(t, db, bucket, key, nil)\n\t\ttxSCard(t, db, bucket, key, 2, nil)\n\n\t\ttxSPop(t, db, fakeBucket, key, ErrBucketNotExist)\n\t})\n\n}\n\nfunc TestTx_SMoveByOneBucket(t *testing.T) {\n\tbucket := \"bucket\"\n\tfakeBucket := \"fake_bucket\"\n\tkey1 := GetTestBytes(0)\n\tkey2 := GetTestBytes(1)\n\tval1 := GetTestBytes(1)\n\tval2 := GetTestBytes(2)\n\tval3 := GetTestBytes(3)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, key1, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key1, val2, nil, nil)\n\n\t\ttxSAdd(t, db, bucket, key2, val3, nil, nil)\n\n\t\ttxSMoveByOneBucket(t, db, bucket, key1, key2, val2, true, nil)\n\t\ttxSIsMember(t, db, bucket, key1, val2, false)\n\t\ttxSIsMember(t, db, bucket, key2, val2, true)\n\n\t\ttxSMoveByOneBucket(t, db, fakeBucket, key1, key2, val2, false, ErrBucketNotExist)\n\t})\n}\n\nfunc TestTx_SMoveByTwoBuckets(t *testing.T) {\n\tbucket1 := \"bucket1\"\n\tbucket2 := \"bucket2\"\n\tfakeBucket := \"fake_bucket_%d\"\n\tkey1 := GetTestBytes(0)\n\tkey2 := GetTestBytes(1)\n\tfakeKey1 := GetTestBytes(2)\n\tfakeKey2 := GetTestBytes(3)\n\tval1 := GetTestBytes(1)\n\tval2 := GetTestBytes(2)\n\tval3 := GetTestBytes(3)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket1, nil)\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket2, nil)\n\t\ttxSAdd(t, db, bucket1, key1, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket1, key1, val2, nil, nil)\n\n\t\ttxSAdd(t, db, bucket2, key2, val3, nil, nil)\n\n\t\ttxSMoveByTwoBuckets(t, db, bucket1, key1, bucket2, key2, val2, true, nil)\n\t\ttxSIsMember(t, db, bucket1, key1, val2, false)\n\t\ttxSIsMember(t, db, bucket2, key2, val2, true)\n\n\t\ttxSMoveByTwoBuckets(t, db, bucket1, fakeKey1, bucket2, key2, val2, false, ErrNotFoundKey)\n\t\ttxSMoveByTwoBuckets(t, db, bucket1, key1, bucket2, fakeKey2, val2, false, ErrNotFoundKey)\n\t\ttxSMoveByTwoBuckets(t, db, fmt.Sprintf(fakeBucket, 1), key1, bucket2, key2, val2, false, ErrBucketNotExist)\n\t\ttxSMoveByTwoBuckets(t, db, bucket1, key1, fmt.Sprintf(fakeBucket, 2), key2, val2, false, ErrBucketNotExist)\n\t\ttxSMoveByTwoBuckets(t, db, fmt.Sprintf(fakeBucket, 1), key1, fmt.Sprintf(fakeBucket, 2), key2, val2, false, ErrBucketNotExist)\n\t})\n}\n\nfunc TestTx_SUnionByOneBucket(t *testing.T) {\n\tbucket := \"bucket\"\n\tfakeBucket := \"fake_bucket\"\n\tkey1 := GetTestBytes(0)\n\tkey2 := GetTestBytes(1)\n\tkey3 := GetTestBytes(2)\n\n\tval1 := GetTestBytes(1)\n\tval2 := GetTestBytes(2)\n\tval3 := GetTestBytes(3)\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, key1, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key1, val2, nil, nil)\n\t\ttxSAdd(t, db, bucket, key2, val3, nil, nil)\n\t\ttxSAdd(t, db, bucket, key3, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key3, val2, nil, nil)\n\t\ttxSAdd(t, db, bucket, key3, val3, nil, nil)\n\n\t\tall := [][]byte{val1, val2, val3}\n\t\ttxSUnionByOneBucket(t, db, bucket, key1, key2, all, nil)\n\t\tfor _, item := range all {\n\t\t\ttxSIsMember(t, db, bucket, key3, item, true)\n\t\t}\n\n\t\ttxSUnionByOneBucket(t, db, fakeBucket, key1, key2, nil, ErrBucketNotExist)\n\t})\n}\n\nfunc TestTx_SUnionByTwoBuckets(t *testing.T) {\n\tbucket1 := \"bucket1\"\n\tbucket2 := \"bucket2\"\n\tfakeBucket := \"fake_bucket_%d\"\n\tkey1 := GetTestBytes(0)\n\tkey2 := GetTestBytes(1)\n\tfakeKey1 := GetTestBytes(2)\n\tfakeKey2 := GetTestBytes(3)\n\tval1 := GetTestBytes(1)\n\tval2 := GetTestBytes(2)\n\tval3 := GetTestBytes(3)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket1, nil)\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket2, nil)\n\t\ttxSAdd(t, db, bucket1, key1, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket1, key1, val2, nil, nil)\n\t\ttxSAdd(t, db, bucket2, key2, val3, nil, nil)\n\n\t\tall := [][]byte{val1, val2, val3}\n\t\ttxSUnionByTwoBuckets(t, db, bucket1, key1, bucket2, key2, all, nil)\n\n\t\ttxSUnionByTwoBuckets(t, db, fmt.Sprintf(fakeBucket, 1), key1, bucket2, key2, nil, ErrBucketNotExist)\n\t\ttxSUnionByTwoBuckets(t, db, bucket1, key1, fmt.Sprintf(fakeBucket, 2), key2, nil, ErrBucketNotExist)\n\t\ttxSUnionByTwoBuckets(t, db, bucket1, fakeKey1, bucket2, key2, nil, ErrNotFoundKey)\n\t\ttxSUnionByTwoBuckets(t, db, bucket1, key1, bucket2, fakeKey2, nil, ErrNotFoundKey)\n\t})\n}\n\nfunc TestTx_SHasKey(t *testing.T) {\n\tbucket := \"bucket\"\n\tfakeBucket := \"fake_bucket\"\n\tkey := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, key, GetTestBytes(1), nil, nil)\n\n\t\ttxSHasKey(t, db, bucket, key, true)\n\t\ttxSHasKey(t, db, fakeBucket, key, false)\n\t})\n}\n\nfunc TestTx_SIsMember(t *testing.T) {\n\tbucket := \"bucket\"\n\tfakeBucket := \"fake_bucket\"\n\tkey := GetTestBytes(0)\n\tfakeKey := GetTestBytes(1)\n\tval := GetTestBytes(0)\n\tfakeVal := GetTestBytes(1)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, key, val, nil, nil)\n\n\t\ttxSIsMember(t, db, bucket, key, val, true)\n\t\ttxSIsMember(t, db, bucket, key, fakeVal, false)\n\t\ttxSIsMember(t, db, bucket, fakeKey, val, false)\n\t\ttxSIsMember(t, db, fakeBucket, fakeKey, val, false)\n\t})\n}\n\nfunc TestTx_SAreMembers(t *testing.T) {\n\tbucket := \"bucket\"\n\tfakeBucket := \"fake_bucket\"\n\tkey := GetTestBytes(0)\n\tfakeKey := GetTestBytes(1)\n\tval1 := GetTestBytes(0)\n\tval2 := GetTestBytes(1)\n\tfakeVal := GetTestBytes(2)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\ttxSAdd(t, db, bucket, key, val1, nil, nil)\n\t\ttxSAdd(t, db, bucket, key, val2, nil, nil)\n\n\t\ttxSAreMembers(t, db, bucket, key, true)\n\t\ttxSAreMembers(t, db, bucket, key, true, val1)\n\t\ttxSAreMembers(t, db, bucket, key, true, val2)\n\t\ttxSAreMembers(t, db, bucket, key, true, val1, val2)\n\t\ttxSAreMembers(t, db, bucket, key, false, fakeVal)\n\t\ttxSAreMembers(t, db, bucket, fakeKey, false, val1)\n\t\ttxSAreMembers(t, db, fakeBucket, key, false, val1)\n\t})\n}\n\nfunc TestTx_SKeys(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := \"key_%d\"\n\tval := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\tnum := 3\n\t\tfor i := 0; i < num; i++ {\n\t\t\ttxSAdd(t, db, bucket, []byte(fmt.Sprintf(key, i)), val, nil, nil)\n\t\t}\n\n\t\tvar keys []string\n\t\ttxSKeys(t, db, bucket, \"*\", func(key string) bool {\n\t\t\tkeys = append(keys, key)\n\t\t\treturn true\n\t\t}, num, nil)\n\n\t\tkeys = []string{}\n\t\ttxSKeys(t, db, bucket, \"*\", func(key string) bool {\n\t\t\tkeys = append(keys, key)\n\t\t\treturn len(keys) != num-1\n\t\t}, num-1, nil)\n\n\t\tkeys = []string{}\n\t\ttxSKeys(t, db, bucket, \"fake_key*\", func(key string) bool {\n\t\t\tkeys = append(keys, key)\n\t\t\treturn true\n\t\t}, 0, nil)\n\t})\n}\n\nfunc TestErrBucketAndKey(t *testing.T) {\n\n\tgot := ErrBucketAndKey(\"foo\", []byte(\"bar\"))\n\n\tassert.True(t,\n\t\terrors.Is(got, ErrBucketNotFound))\n}\n\nfunc TestErrNotFoundKeyInBucket(t *testing.T) {\n\n\tgot := ErrNotFoundKeyInBucket(\"foo\", []byte(\"bar\"))\n\n\tassert.True(t,\n\t\terrors.Is(got, ErrNotFoundKey))\n}\n"
        },
        {
          "name": "tx_test.go",
          "type": "blob",
          "size": 5.5859375,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\n// todo to check is there any deadlock here?\nfunc TestTx_Rollback(t *testing.T) {\n\n\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\tbucket := \"bucket_rollback_test\"\n\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\ttx, err := db.Begin(true)\n\t\tassert.NoError(t, err)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%03d\", i))\n\t\t\tval := []byte(\"val_\" + fmt.Sprintf(\"%03d\", i))\n\t\t\tif i == 7 {\n\t\t\t\tkey = []byte(\"\") // set error key to make tx rollback\n\t\t\t}\n\t\t\tif err = tx.Put(bucket, key, val, Persistent); err != nil {\n\t\t\t\t// tx rollback\n\t\t\t\ttx.Rollback()\n\n\t\t\t\tif i < 7 {\n\t\t\t\t\tt.Fatal(\"err TestTx_Rollback\")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// no one found\n\t\tfor i := 0; i <= 10; i++ {\n\t\t\ttx, err = db.Begin(false)\n\t\t\tassert.NoError(t, err)\n\n\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%03d\", i))\n\t\t\tif _, err := tx.Get(bucket, key); err != nil {\n\t\t\t\t// tx rollback\n\t\t\t\ttx.Rollback()\n\t\t\t} else {\n\t\t\t\tt.Fatal(\"err TestTx_Rollback\")\n\t\t\t}\n\t\t}\n\n\t})\n}\n\nfunc TestTx_Begin(t *testing.T) {\n\tt.Run(\"Begin with default options, with only read\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\ttx, err := db.Begin(false)\n\t\t\tassert.NoError(t, err)\n\n\t\t\terr = tx.Rollback()\n\t\t\tassert.NoError(t, err)\n\n\t\t\terr = db.Close()\n\t\t\tassert.NoError(t, err)\n\t\t})\n\t})\n\n\tt.Run(\"Begin with default options, with writable\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\ttx, err := db.Begin(true)\n\t\t\tassert.NoError(t, err)\n\n\t\t\ttx.Rollback()\n\n\t\t\terr = db.Close()\n\t\t\tassert.NoError(t, err)\n\t\t})\n\t})\n\n\tt.Run(\"Begin with error: error options\", func(t *testing.T) {\n\t\topt := DefaultOptions\n\t\topt.Dir = \"/tmp/nutsdbtesttx\"\n\t\topt.NodeNum = -1\n\n\t\twithDBOption(t, opt, func(t *testing.T, db *DB) {\n\t\t\t_, err := db.Begin(false)\n\t\t\tassert.Error(t, err)\n\t\t})\n\t})\n\n\tt.Run(\"Begin with error: begin the closed db\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\ttx, err := db.Begin(true)\n\t\t\tassert.NoError(t, err)\n\n\t\t\ttx.Rollback() // for unlock mutex\n\n\t\t\terr = db.Close()\n\t\t\tassert.NoError(t, err)\n\n\t\t\t_, err = db.Begin(false)\n\t\t\tassert.Error(t, err)\n\t\t})\n\t})\n}\n\nfunc TestTx_Close(t *testing.T) {\n\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\ttx, err := db.Begin(false)\n\t\tassert.NoError(t, err)\n\n\t\terr = tx.Rollback()\n\t\tassert.NoError(t, err)\n\n\t\tbucket := \"bucket_tx_close_test\"\n\n\t\t_, err = tx.Get(bucket, []byte(\"foo\"))\n\t\tassert.Errorf(t, err, \"err TestTx_Close\")\n\n\t\t_, err = tx.RangeScan(bucket, []byte(\"foo0\"), []byte(\"foo1\"))\n\t\tassert.Errorf(t, err, \"err TestTx_Close\")\n\n\t\t_, err = tx.PrefixScan(bucket, []byte(\"foo\"), 0, 1)\n\t\tassert.Errorf(t, err, \"err TestTx_Close\")\n\n\t\t_, err = tx.PrefixSearchScan(bucket, []byte(\"f\"), \"oo\", 0, 1)\n\t\tassert.Errorf(t, err, \"err TestTx_Close\")\n\t})\n}\n\nfunc TestTx_CommittedStatus(t *testing.T) {\n\n\twithRAMIdxDB(t, func(t *testing.T, db *DB) {\n\n\t\tbucket := \"bucket_committed_status\"\n\n\t\t{ // setup data\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\ttx, err := db.Begin(true)\n\t\t\tassert.NoError(t, err)\n\n\t\t\terr = tx.Put(bucket, []byte(\"key1\"), []byte(\"value1\"), 0)\n\t\t\tassert.NoError(t, err)\n\n\t\t\terr = tx.Put(bucket, []byte(\"key2\"), []byte(\"value2\"), 0)\n\t\t\tassert.NoError(t, err)\n\n\t\t\terr = tx.Commit()\n\t\t\tassert.NoError(t, err)\n\t\t}\n\t})\n}\n\nfunc TestTx_PutWithTimestamp(t *testing.T) {\n\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\tbucket := \"bucket_put_with_timestamp\"\n\n\t\ttimestamps := []uint64{1547707905, 1547707910, uint64(time.Now().Unix())}\n\n\t\t{ // put with timestamp\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\n\t\t\ttx, err := db.Begin(true)\n\t\t\tassert.NoError(t, err)\n\t\t\tfor i, timestamp := range timestamps {\n\t\t\t\tkey := []byte(\"key_\" + fmt.Sprintf(\"%03d\", i))\n\t\t\t\tval := []byte(\"val_\" + fmt.Sprintf(\"%03d\", i))\n\n\t\t\t\terr = tx.PutWithTimestamp(bucket, key, val, 0, timestamp)\n\t\t\t\tassert.NoError(t, err)\n\n\t\t\t}\n\t\t\terr = tx.Commit()\n\t\t\tassert.NoError(t, err)\n\t\t}\n\t})\n}\n\nfunc TestTx_Commit(t *testing.T) {\n\tt.Run(\"build_bucket_indexes_after_commit\", func(t *testing.T) {\n\t\twithDefaultDB(t, func(t *testing.T, db *DB) {\n\t\t\tbucket := \"bucket1\"\n\n\t\t\ttxCreateBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\tassert.Equal(t, 1, db.Index.bTree.defaultOp.getIdxLen())\n\t\t\ttxDeleteBucket(t, db, DataStructureBTree, bucket, nil)\n\t\t\tassert.Equal(t, 0, db.Index.bTree.defaultOp.getIdxLen())\n\n\t\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\t\t\tassert.Equal(t, 1, db.Index.sortedSet.defaultOp.getIdxLen())\n\t\t\ttxDeleteBucket(t, db, DataStructureSortedSet, bucket, nil)\n\t\t\tassert.Equal(t, 0, db.Index.sortedSet.defaultOp.getIdxLen())\n\n\t\t\ttxCreateBucket(t, db, DataStructureList, bucket, nil)\n\t\t\tassert.Equal(t, 1, db.Index.list.defaultOp.getIdxLen())\n\t\t\ttxDeleteBucket(t, db, DataStructureList, bucket, nil)\n\t\t\tassert.Equal(t, 0, db.Index.list.defaultOp.getIdxLen())\n\n\t\t\ttxCreateBucket(t, db, DataStructureSet, bucket, nil)\n\t\t\tassert.Equal(t, 1, db.Index.set.defaultOp.getIdxLen())\n\t\t\ttxDeleteBucket(t, db, DataStructureSet, bucket, nil)\n\t\t\tassert.Equal(t, 0, db.Index.set.defaultOp.getIdxLen())\n\n\t\t})\n\t})\n\n}\n"
        },
        {
          "name": "tx_zset.go",
          "type": "blob",
          "size": 13.033203125,
          "content": "// Copyright 2023 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/xujiajun/utils/strconv2\"\n)\n\n// SeparatorForZSetKey represents separator for zSet key.\nconst SeparatorForZSetKey = \"|\"\n\ntype SortedSetMember struct {\n\tValue []byte\n\tScore float64\n}\n\n// ZAdd Adds the specified member with the specified score into the sorted set specified by key in a bucket.\nfunc (tx *Tx) ZAdd(bucket string, key []byte, score float64, val []byte) error {\n\tvar buffer bytes.Buffer\n\n\tif strings.Contains(string(key), SeparatorForZSetKey) {\n\t\treturn ErrSeparatorForZSetKey()\n\t}\n\n\tbuffer.Write(key)\n\tbuffer.Write([]byte(SeparatorForZSetKey))\n\tscoreBytes := []byte(strconv.FormatFloat(score, 'f', -1, 64))\n\tbuffer.Write(scoreBytes)\n\tnewKey := buffer.Bytes()\n\n\treturn tx.put(bucket, newKey, val, Persistent, DataZAddFlag, uint64(time.Now().Unix()), DataStructureSortedSet)\n}\n\n// ZMembers Returns all the members and scores of members of the set specified by key in a bucket.\nfunc (tx *Tx) ZMembers(bucket string, key []byte) (map[*SortedSetMember]struct{}, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn nil, ErrBucket\n\t}\n\n\tmembers, err := sortedSet.ZMembers(string(key))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tres := make(map[*SortedSetMember]struct{})\n\tfor record, score := range members {\n\t\tvalue, err := tx.db.getValueByRecord(record)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tres[&SortedSetMember{\n\t\t\tValue: value,\n\t\t\tScore: float64(score),\n\t\t}] = struct{}{}\n\t}\n\n\treturn res, nil\n}\n\n// ZCard Returns the sorted set cardinality (number of elements) of the sorted set specified by key in a bucket.\nfunc (tx *Tx) ZCard(bucket string, key []byte) (int, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn 0, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn 0, ErrBucket\n\t}\n\n\treturn sortedSet.ZCard(string(key))\n}\n\n// ZCount Returns the number of elements in the sorted set specified by key in a bucket with a score between min and max and opts.\n// Opt includes the following parameters:\n// Limit        int  // limit the max nodes to return\n// ExcludeStart bool // exclude start value, so it search in interval (start, end] or (start, end)\n// ExcludeEnd   bool // exclude end value, so it search in interval [start, end) or (start, end)\nfunc (tx *Tx) ZCount(bucket string, key []byte, start, end float64, opts *GetByScoreRangeOptions) (int, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn 0, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn 0, ErrBucket\n\t}\n\n\treturn sortedSet.ZCount(string(key), SCORE(start), SCORE(end), opts)\n}\n\n// ZPopMax Removes and returns the member with the highest score in the sorted set specified by key in a bucket.\nfunc (tx *Tx) ZPopMax(bucket string, key []byte) (*SortedSetMember, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn nil, ErrBucket\n\t}\n\n\trecord, score, err := sortedSet.ZPeekMax(string(key))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvalue, err := tx.db.getValueByRecord(record)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &SortedSetMember{Value: value, Score: float64(score)}, tx.put(bucket, key, []byte(\"\"), Persistent, DataZPopMaxFlag, uint64(time.Now().Unix()), DataStructureSortedSet)\n}\n\n// ZPopMin Removes and returns the member with the lowest score in the sorted set specified by key in a bucket.\nfunc (tx *Tx) ZPopMin(bucket string, key []byte) (*SortedSetMember, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn nil, ErrBucket\n\t}\n\n\trecord, score, err := sortedSet.ZPeekMin(string(key))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvalue, err := tx.db.getValueByRecord(record)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &SortedSetMember{Value: value, Score: float64(score)}, tx.put(bucket, key, []byte(\"\"), Persistent, DataZPopMinFlag, uint64(time.Now().Unix()), DataStructureSortedSet)\n}\n\n// ZPeekMax Returns the member with the highest score in the sorted set specified by key in a bucket.\nfunc (tx *Tx) ZPeekMax(bucket string, key []byte) (*SortedSetMember, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn nil, ErrBucket\n\t}\n\n\trecord, score, err := sortedSet.ZPeekMax(string(key))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvalue, err := tx.db.getValueByRecord(record)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &SortedSetMember{Value: value, Score: float64(score)}, nil\n}\n\n// ZPeekMin Returns the member with the lowest score in the sorted set specified by key in a bucket.\nfunc (tx *Tx) ZPeekMin(bucket string, key []byte) (*SortedSetMember, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn nil, ErrBucket\n\t}\n\n\trecord, score, err := sortedSet.ZPeekMin(string(key))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvalue, err := tx.db.getValueByRecord(record)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &SortedSetMember{Value: value, Score: float64(score)}, nil\n}\n\n// ZRangeByScore Returns all the elements in the sorted set specified by key in a bucket with a score between min and max.\n// And the parameter `Opts` is the same as ZCount's.\nfunc (tx *Tx) ZRangeByScore(bucket string, key []byte, start, end float64, opts *GetByScoreRangeOptions) ([]*SortedSetMember, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn nil, ErrBucket\n\t}\n\n\trecords, scores, err := sortedSet.ZRangeByScore(string(key), SCORE(start), SCORE(end), opts)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tmembers := make([]*SortedSetMember, len(records))\n\tfor i, record := range records {\n\t\tvalue, err := tx.db.getValueByRecord(record)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tmembers[i] = &SortedSetMember{Value: value, Score: scores[i]}\n\t}\n\n\treturn members, nil\n}\n\n// ZRangeByRank Returns all the elements in the sorted set specified by key in a bucket\n// with a rank between start and end (including elements with rank equal to start or end).\nfunc (tx *Tx) ZRangeByRank(bucket string, key []byte, start, end int) ([]*SortedSetMember, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn nil, ErrBucket\n\t}\n\n\trecords, scores, err := sortedSet.ZRangeByRank(string(key), start, end)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tmembers := make([]*SortedSetMember, len(records))\n\tfor i, record := range records {\n\t\tvalue, err := tx.db.getValueByRecord(record)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tmembers[i] = &SortedSetMember{Value: value, Score: scores[i]}\n\t}\n\n\treturn members, nil\n}\n\n// ZRem removes the specified members from the sorted set stored in one bucket at given bucket and key.\nfunc (tx *Tx) ZRem(bucket string, key []byte, value []byte) error {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn ErrBucket\n\t}\n\n\texist, err = sortedSet.ZExist(string(key), value)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif !exist {\n\t\treturn ErrSortedSetMemberNotExist\n\t}\n\n\treturn tx.put(bucket, key, value, Persistent, DataZRemFlag, uint64(time.Now().Unix()), DataStructureSortedSet)\n}\n\n// ZRemRangeByRank removes all elements in the sorted set stored in one bucket at given bucket with rank between start and end.\n// the rank is 1-based integer. Rank 1 means the first node; Rank -1 means the last node.\nfunc (tx *Tx) ZRemRangeByRank(bucket string, key []byte, start, end int) error {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn err\n\t}\n\n\tstartStr := strconv2.IntToStr(start)\n\tendStr := strconv2.IntToStr(end)\n\treturn tx.put(bucket, key, []byte(startStr+SeparatorForZSetKey+endStr), Persistent, DataZRemRangeByRankFlag, uint64(time.Now().Unix()), DataStructureSortedSet)\n}\n\n// ZRank Returns the rank of member in the sorted set specified by key in a bucket, with the scores ordered from low to high.\nfunc (tx *Tx) ZRank(bucket string, key, value []byte) (int, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn 0, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn 0, ErrBucket\n\t}\n\n\treturn sortedSet.ZRank(string(key), value)\n}\n\n// ZRevRank Returns the rank of member in the sorted set specified by key in a bucket, with the scores ordered from high to low.\nfunc (tx *Tx) ZRevRank(bucket string, key, value []byte) (int, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn 0, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn 0, ErrBucket\n\t}\n\n\treturn sortedSet.ZRevRank(string(key), value)\n}\n\n// ZScore Returns the score of members in a sorted set specified by key in a bucket.\nfunc (tx *Tx) ZScore(bucket string, key, value []byte) (float64, error) {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn 0, err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn 0.0, err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn 0, ErrBucket\n\t}\n\n\treturn sortedSet.ZScore(string(key), value)\n}\n\n// ZKeys find all keys matching a given pattern in a bucket\nfunc (tx *Tx) ZKeys(bucket, pattern string, f func(key string) bool) error {\n\tif err := tx.ZCheck(bucket); err != nil {\n\t\treturn err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvar (\n\t\tbucketId  = b.Id\n\t\tsortedSet *SortedSet\n\t\texist     bool\n\t)\n\n\tif sortedSet, exist = tx.db.Index.sortedSet.exist(bucketId); !exist {\n\t\treturn ErrBucket\n\t}\n\n\tfor key := range sortedSet.M {\n\t\tif end, err := MatchForRange(pattern, key, f); end || err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (tx *Tx) ZCheck(bucket string) error {\n\tif err := tx.checkTxIsClosed(); err != nil {\n\t\treturn err\n\t}\n\n\tb, err := tx.db.bm.GetBucket(DataStructureSortedSet, bucket)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbucketId := b.Id\n\tif _, ok := tx.db.Index.sortedSet.exist(bucketId); !ok {\n\t\treturn ErrBucket\n\t}\n\treturn nil\n}\n\n// ErrSeparatorForZSetKey returns when zSet key contains the SeparatorForZSetKey flag.\nfunc ErrSeparatorForZSetKey() error {\n\treturn errors.New(\"contain separator (\" + SeparatorForZSetKey + \") for SortedSetIdx key\")\n}\n"
        },
        {
          "name": "tx_zset_test.go",
          "type": "blob",
          "size": 14.5791015625,
          "content": "// Copyright 2023 The nutsdb Authors. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nvar tx *Tx\n\nfunc TestTx_ZCheck(t *testing.T) {\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\terr := db.View(func(tx *Tx) error {\n\t\t\trequire.Equal(t, ErrBucketNotExist, tx.ZCheck(\"fake bucket\"))\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc TestTx_ZAdd(t *testing.T) {\n\n\tbucket := \"bucket\"\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxZAdd(t, db, bucket, GetTestBytes(0), GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\ttxZCard(t, db, bucket, GetTestBytes(0), 10, nil)\n\t})\n}\n\nfunc TestTx_ZScore(t *testing.T) {\n\tbucket := \"bucket\"\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxZAdd(t, db, bucket, GetTestBytes(0), GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxZScore(t, db, bucket, GetTestBytes(0), GetTestBytes(i), float64(i), nil)\n\t\t}\n\n\t\ttxZScore(t, db, bucket, GetTestBytes(0), GetTestBytes(10), float64(10), ErrSortedSetMemberNotExist)\n\t\ttxZScore(t, db, bucket, GetTestBytes(1), GetTestBytes(0), float64(0), ErrSortedSetNotFound)\n\n\t\t// update the score of member\n\t\ttxZAdd(t, db, bucket, GetTestBytes(0), GetTestBytes(5), float64(999), nil, nil)\n\t\ttxZScore(t, db, bucket, GetTestBytes(0), GetTestBytes(5), 999, nil)\n\t})\n}\n\nfunc TestTx_ZRem(t *testing.T) {\n\tbucket := \"bucket\"\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxZAdd(t, db, bucket, GetTestBytes(0), GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\ttxZScore(t, db, bucket, GetTestBytes(0), GetTestBytes(3), float64(3), nil)\n\n\t\t// normal remove\n\t\ttxZRem(t, db, bucket, GetTestBytes(0), GetTestBytes(3), nil)\n\t\ttxZScore(t, db, bucket, GetTestBytes(0), GetTestBytes(3), float64(3), ErrSortedSetMemberNotExist)\n\n\t\ttxZCard(t, db, bucket, GetTestBytes(0), 9, nil)\n\n\t\t// remove a fake member\n\t\ttxZRem(t, db, bucket, GetTestBytes(0), GetTestBytes(10), ErrSortedSetMemberNotExist)\n\n\t\t// remove from a fake zset\n\t\ttxZRem(t, db, bucket, GetTestBytes(1), GetTestBytes(0), ErrSortedSetNotFound)\n\t})\n}\n\nfunc TestTx_ZMembers(t *testing.T) {\n\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxZAdd(t, db, bucket, key, GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\terr := db.View(func(tx *Tx) error {\n\t\t\tmembers, err := tx.ZMembers(bucket, key)\n\t\t\trequire.NoError(t, err)\n\n\t\t\trequire.Len(t, members, 10)\n\n\t\t\tfor member := range members {\n\t\t\t\trequire.Equal(t, GetTestBytes(int(member.Score)), member.Value)\n\t\t\t}\n\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc TestTx_ZCount(t *testing.T) {\n\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\n\t\tfor i := 0; i < 30; i++ {\n\t\t\ttxZAdd(t, db, bucket, key, GetRandomBytes(24), float64(i), nil, nil)\n\t\t}\n\n\t\terr := db.View(func(tx *Tx) error {\n\n\t\t\tcount, err := tx.ZCount(bucket, key, 10, 20, nil)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Equal(t, 11, count)\n\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc TestTx_ZPop(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\n\t\ttxZPop(t, db, bucket, key, true, nil, 0, ErrSortedSetNotFound)\n\t\ttxZPop(t, db, bucket, key, false, nil, 0, ErrSortedSetNotFound)\n\n\t\ttxZAdd(t, db, bucket, key, GetTestBytes(0), float64(0), nil, nil)\n\t\ttxZRem(t, db, bucket, key, GetTestBytes(0), nil)\n\n\t\ttxZPop(t, db, bucket, key, true, nil, 0, ErrSortedSetIsEmpty)\n\t\ttxZPop(t, db, bucket, key, false, nil, 0, ErrSortedSetIsEmpty)\n\n\t\tfor i := 0; i < 30; i++ {\n\t\t\ttxZAdd(t, db, bucket, key, GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\ttxZPop(t, db, bucket, key, true, GetTestBytes(29), float64(29), nil)\n\t\ttxZPop(t, db, bucket, key, false, GetTestBytes(0), 0, nil)\n\n\t\ttxZPop(t, db, bucket, key, true, GetTestBytes(28), float64(28), nil)\n\t\ttxZPop(t, db, bucket, key, false, GetTestBytes(1), 1, nil)\n\t})\n}\n\nfunc TestTx_ZRangeByScore(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\t\terr := db.View((func(tx *Tx) error {\n\t\t\t_, err := tx.ZRangeByScore(bucket, key, 1, 10, nil)\n\t\t\trequire.Error(t, err)\n\t\t\treturn nil\n\t\t}))\n\t\trequire.NoError(t, err)\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxZAdd(t, db, bucket, key, GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\terr = db.View(func(tx *Tx) error {\n\t\t\tmembers, err := tx.ZRangeByScore(bucket, key, 0, 11, nil)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Len(t, members, 10)\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\t\terr = db.View(func(tx *Tx) error {\n\t\t\tmembers, err := tx.ZRangeByScore(bucket, key, -1, 2, nil)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Len(t, members, 3)\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\t\terr = db.View(func(tx *Tx) error {\n\t\t\tmembers, err := tx.ZRangeByScore(bucket, key, 8, 12, nil)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Len(t, members, 2)\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\t\terr = db.View(func(tx *Tx) error {\n\t\t\tmembers, err := tx.ZRangeByScore(bucket, key, 5, 2, nil)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Len(t, members, 4)\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\t})\n}\nfunc TestTx_ZPeekMin(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\n\t\tfor i := 0; i < 30; i++ {\n\t\t\ttxZAdd(t, db, bucket, key, GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\t// get minimum node\n\t\ttxZPeekMin(t, db, bucket, key, GetTestBytes(0), float64(0), nil, nil)\n\n\t\t//  bucket not exists\n\t\ttxZPeekMin(t, db, \"non-exists-bucket\", key, []byte{}, float64(0), ErrBucketNotExist, ErrBucketNotExist)\n\n\t\t// key not exists\n\t\ttxZPeekMin(t, db, bucket, []byte(\"non-exists-key\"), []byte{}, float64(0), ErrSortedSetNotFound, ErrSortedSetNotFound)\n\n\t\t// add nodes\n\n\t\t// add node that will not affect the minimum node\n\t\ttxZAdd(t, db, bucket, key, []byte(\"new-mem\"), float64(3), nil, nil)\n\t\ttxZPeekMin(t, db, bucket, key, GetTestBytes(0), float64(0), nil, nil)\n\t\t// add a new minimum value\n\t\ttxZAdd(t, db, bucket, key, []byte(\"new-min\"), float64(0), nil, nil)\n\t\ttxZPeekMin(t, db, bucket, key, []byte(\"new-min\"), float64(0), nil, nil)\n\n\t\t// remove nodes\n\n\t\t// remove minimum node\n\t\ttxZRem(t, db, bucket, key, []byte(\"new-min\"), nil)\n\t\ttxZPeekMin(t, db, bucket, key, GetTestBytes(0), float64(0), nil, nil)\n\n\t\t// remove non-minimum node\n\t\ttxZRem(t, db, bucket, key, GetTestBytes(5), nil)\n\t\ttxZPeekMin(t, db, bucket, key, GetTestBytes(0), float64(0), nil, nil)\n\n\t\t// remove range by rank\n\t\terr := db.Update(func(tx *Tx) error {\n\t\t\terr := tx.ZRemRangeByRank(bucket, key, 1, 10)\n\t\t\tassert.NoError(t, err)\n\t\t\treturn nil\n\t\t})\n\t\tassert.NoError(t, err)\n\t\ttxZPeekMin(t, db, bucket, key, GetTestBytes(10), float64(10), nil, nil)\n\n\t\t// pop\n\n\t\t// pop min\n\t\ttxZPop(t, db, bucket, key, false, GetTestBytes(10), float64(10), nil)\n\t\ttxZPeekMin(t, db, bucket, key, GetTestBytes(11), float64(11), nil, nil)\n\n\t\t// pop max\n\t\ttxZPop(t, db, bucket, key, true, GetTestBytes(29), float64(29), nil)\n\t\ttxZPeekMin(t, db, bucket, key, GetTestBytes(11), float64(11), nil, nil)\n\t})\n}\n\nfunc TestTx_ZRangeByRank(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\n\t\terr := db.View(func(tx *Tx) error {\n\t\t\t_, err := tx.ZRangeByRank(bucket, key, 1, 10)\n\t\t\trequire.Error(t, err)\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxZAdd(t, db, bucket, key, GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\terr = db.View(func(tx *Tx) error {\n\t\t\tmembers, err := tx.ZRangeByRank(bucket, key, 1, 10)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Len(t, members, 10)\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\terr = db.View(func(tx *Tx) error {\n\t\t\tmembers, err := tx.ZRangeByRank(bucket, key, 3, 6)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Len(t, members, 4)\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\terr = db.View(func(tx *Tx) error {\n\t\t\tmembers, err := tx.ZRangeByRank(bucket, key, -1, 11)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Len(t, members, 1)\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\terr = db.View(func(tx *Tx) error {\n\t\t\tmembers, err := tx.ZRangeByRank(bucket, key, 8, 4)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.Len(t, members, 5)\n\n\t\t\tfor i, member := range members {\n\t\t\t\trequire.Equal(t, member.Score, float64(7-i))\n\t\t\t\trequire.Equal(t, member.Value, GetTestBytes(7-i))\n\t\t\t}\n\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc TestTx_ZRemRangeByRank(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\n\t\terr := db.Update(func(tx *Tx) error {\n\t\t\terr := tx.ZRemRangeByRank(bucket, key, 1, 10)\n\t\t\tassert.NoError(t, err)\n\t\t\treturn nil\n\t\t})\n\t\tassert.NoError(t, err)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxZAdd(t, db, bucket, key, GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\terr = db.Update(func(tx *Tx) error {\n\t\t\terr := tx.ZRemRangeByRank(bucket, key, 1, 10)\n\t\t\tassert.NoError(t, err)\n\t\t\treturn nil\n\t\t})\n\t\tassert.NoError(t, err)\n\n\t\ttxZCard(t, db, bucket, key, 0, nil)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxZAdd(t, db, bucket, key, GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\terr = db.Update(func(tx *Tx) error {\n\t\t\terr := tx.ZRemRangeByRank(bucket, key, 1, 2)\n\t\t\tassert.NoError(t, err)\n\t\t\treturn nil\n\t\t})\n\n\t\tfor i := 0; i < 2; i++ {\n\t\t\ttxZScore(t, db, bucket, key, GetTestBytes(0), 0, ErrSortedSetMemberNotExist)\n\t\t}\n\n\t\terr = db.Update(func(tx *Tx) error {\n\t\t\tcard, err := tx.ZCard(bucket, key)\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.Equal(t, 8, card)\n\n\t\t\terr = tx.ZRemRangeByRank(bucket, key, 6, 8)\n\t\t\tassert.NoError(t, err)\n\t\t\treturn nil\n\t\t})\n\n\t\tfor i := 5; i < 8; i++ {\n\t\t\ttxZScore(t, db, bucket, key, GetTestBytes(0), 0, ErrSortedSetMemberNotExist)\n\t\t}\n\n\t\terr = db.Update(func(tx *Tx) error {\n\t\t\tcard, err := tx.ZCard(bucket, key)\n\t\t\tassert.NoError(t, err)\n\t\t\tassert.Equal(t, 5, card)\n\n\t\t\terr = tx.ZRemRangeByRank(bucket, key, 4, 3)\n\t\t\tassert.NoError(t, err)\n\t\t\treturn nil\n\t\t})\n\n\t\tfor i := 2; i < 4; i++ {\n\t\t\ttxZScore(t, db, bucket, key, GetTestBytes(0), 0, ErrSortedSetMemberNotExist)\n\t\t}\n\n\t\tassert.NoError(t, err)\n\t})\n}\n\nfunc TestTx_ZRank(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\n\t\ttxZRank(t, db, bucket, key, GetTestBytes(0), true, 0, ErrSortedSetNotFound)\n\t\ttxZRank(t, db, bucket, key, GetTestBytes(0), false, 0, ErrSortedSetNotFound)\n\n\t\tfor i := 0; i < 10; i++ {\n\t\t\ttxZAdd(t, db, bucket, key, GetTestBytes(i), float64(i), nil, nil)\n\t\t}\n\n\t\ttxZRank(t, db, bucket, key, GetTestBytes(0), true, 10, nil)\n\t\ttxZRank(t, db, bucket, key, GetTestBytes(0), false, 1, nil)\n\n\t\ttxZRem(t, db, bucket, key, GetTestBytes(0), nil)\n\n\t\ttxZRank(t, db, bucket, key, GetTestBytes(0), true, 10, ErrSortedSetMemberNotExist)\n\t\ttxZRank(t, db, bucket, key, GetTestBytes(0), false, 1, ErrSortedSetMemberNotExist)\n\n\t\ttxZRem(t, db, bucket, key, GetTestBytes(3), nil)\n\n\t\ttxZRank(t, db, bucket, key, GetTestBytes(4), true, 6, nil)\n\t\ttxZRank(t, db, bucket, key, GetTestBytes(4), false, 3, nil)\n\t})\n}\n\nfunc TestTx_ZKeys(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := \"key_%d\"\n\tval := GetTestBytes(0)\n\n\trunNutsDBTest(t, nil, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\t\tfor i := 0; i < 3; i++ {\n\t\t\ttxZAdd(t, db, bucket, []byte(fmt.Sprintf(key, i)), val, float64(i), nil, nil)\n\t\t}\n\t\ttxZAdd(t, db, bucket, []byte(\"foo\"), val, 1, nil, nil)\n\n\t\ttests := []struct {\n\t\t\tpattern         string\n\t\t\texpectedMatches int\n\t\t\texpectedError   error\n\t\t}{\n\t\t\t{\"*\", 4, nil},         // find all keys\n\t\t\t{\"key_*\", 3, nil},     // find keys with 'key_' prefix\n\t\t\t{\"fake_key*\", 0, nil}, // find non-existing keys\n\t\t}\n\n\t\tfor _, test := range tests {\n\t\t\ttxZKeys(t, db, bucket, test.pattern, func(key string) bool { return true }, test.expectedMatches, test.expectedError)\n\t\t}\n\n\t\t// stop after finding the expected number of keys.\n\t\texpectNum := 2\n\t\tvar foundKeys []string\n\t\ttxZKeys(t, db, bucket, \"*\", func(key string) bool {\n\t\t\tfoundKeys = append(foundKeys, key)\n\t\t\treturn len(foundKeys) < expectNum\n\t\t}, expectNum, nil)\n\t\tassert.Equal(t, expectNum, len(foundKeys))\n\t})\n}\n\nfunc TestTx_ZSetEntryIdxMode_HintKeyValAndRAMIdxMode(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\tvalue := GetRandomBytes(24)\n\n\topts := DefaultOptions\n\topts.EntryIdxMode = HintKeyValAndRAMIdxMode\n\n\t// HintKeyValAndRAMIdxMode\n\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\n\t\terr := db.Update(func(tx *Tx) error {\n\t\t\terr := tx.ZAdd(bucket, key, float64(0), value)\n\t\t\trequire.NoError(t, err)\n\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\tzset := db.Index.sortedSet.getWithDefault(1, db).M[string(key)]\n\t\thash, _ := getFnv32(value)\n\t\tnode := zset.dict[hash]\n\n\t\trequire.NotNil(t, node.record.Value)\n\t\trequire.Equal(t, value, node.record.Value)\n\t})\n}\n\nfunc TestTx_ZSetEntryIdxMode_HintKeyAndRAMIdxMode(t *testing.T) {\n\tbucket := \"bucket\"\n\tkey := GetTestBytes(0)\n\tvalue := GetRandomBytes(24)\n\n\topts := DefaultOptions\n\topts.EntryIdxMode = HintKeyAndRAMIdxMode\n\n\t// HintKeyValAndRAMIdxMode\n\trunNutsDBTest(t, &opts, func(t *testing.T, db *DB) {\n\t\ttxCreateBucket(t, db, DataStructureSortedSet, bucket, nil)\n\t\terr := db.Update(func(tx *Tx) error {\n\t\t\terr := tx.ZAdd(bucket, key, float64(0), value)\n\t\t\trequire.NoError(t, err)\n\n\t\t\treturn nil\n\t\t})\n\t\trequire.NoError(t, err)\n\n\t\tzset := db.Index.sortedSet.getWithDefault(1, db).M[string(key)]\n\t\thash, _ := getFnv32(value)\n\t\tnode := zset.dict[hash]\n\n\t\trequire.Nil(t, node.record.Value)\n\n\t\tv, err := db.getValueByRecord(node.record)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, value, v)\n\t})\n}\n"
        },
        {
          "name": "utils.go",
          "type": "blob",
          "size": 4.083984375,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"bytes\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\n\t\"github.com/xujiajun/utils/strconv2\"\n)\n\n// Truncate changes the size of the file.\nfunc Truncate(path string, capacity int64, f *os.File) error {\n\tfileInfo, _ := os.Stat(path)\n\tif fileInfo.Size() < capacity {\n\t\tif err := f.Truncate(capacity); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc ConvertBigEndianBytesToUint64(data []byte) uint64 {\n\treturn binary.BigEndian.Uint64(data)\n}\n\nfunc ConvertUint64ToBigEndianBytes(value uint64) []byte {\n\tb := make([]byte, 8)\n\tbinary.BigEndian.PutUint64(b, value)\n\treturn b\n}\n\nfunc MarshalInts(ints []int) ([]byte, error) {\n\tbuffer := bytes.NewBuffer([]byte{})\n\tfor _, x := range ints {\n\t\tif err := binary.Write(buffer, binary.LittleEndian, int64(x)); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\treturn buffer.Bytes(), nil\n}\n\nfunc UnmarshalInts(data []byte) ([]int, error) {\n\tvar ints []int\n\tbuffer := bytes.NewBuffer(data)\n\tfor {\n\t\tvar i int64\n\t\terr := binary.Read(buffer, binary.LittleEndian, &i)\n\t\tif errors.Is(err, io.EOF) {\n\t\t\tbreak\n\t\t} else if err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tints = append(ints, int(i))\n\t}\n\treturn ints, nil\n}\n\nfunc MatchForRange(pattern, bucket string, f func(bucket string) bool) (end bool, err error) {\n\tmatch, err := filepath.Match(pattern, bucket)\n\tif err != nil {\n\t\treturn true, err\n\t}\n\tif match && !f(bucket) {\n\t\treturn true, nil\n\t}\n\treturn false, nil\n}\n\n// getDataPath returns the data path for the given file ID.\nfunc getDataPath(fID int64, dir string) string {\n\tseparator := string(filepath.Separator)\n\treturn dir + separator + strconv2.Int64ToStr(fID) + DataSuffix\n}\n\nfunc OneOfUint16Array(value uint16, array []uint16) bool {\n\tfor _, v := range array {\n\t\tif v == value {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc splitIntStringStr(str, separator string) (int, string) {\n\tstrList := strings.Split(str, separator)\n\tfirstItem, _ := strconv2.StrToInt(strList[0])\n\tsecondItem := strList[1]\n\treturn firstItem, secondItem\n}\n\nfunc splitStringIntStr(str, separator string) (string, int) {\n\tstrList := strings.Split(str, separator)\n\tfirstItem := strList[0]\n\tsecondItem, _ := strconv2.StrToInt(strList[1])\n\treturn firstItem, secondItem\n}\n\nfunc splitIntIntStr(str, separator string) (int, int) {\n\tstrList := strings.Split(str, separator)\n\tfirstItem, _ := strconv2.StrToInt(strList[0])\n\tsecondItem, _ := strconv2.StrToInt(strList[1])\n\treturn firstItem, secondItem\n}\n\nfunc encodeListKey(key []byte, seq uint64) []byte {\n\tbuf := make([]byte, len(key)+8)\n\tbinary.LittleEndian.PutUint64(buf[:8], seq)\n\tcopy(buf[8:], key[:])\n\treturn buf\n}\n\nfunc decodeListKey(buf []byte) ([]byte, uint64) {\n\tseq := binary.LittleEndian.Uint64(buf[:8])\n\tkey := make([]byte, len(buf[8:]))\n\tcopy(key[:], buf[8:])\n\treturn key, seq\n}\n\nfunc splitStringFloat64Str(str, separator string) (string, float64) {\n\tstrList := strings.Split(str, separator)\n\tfirstItem := strList[0]\n\tsecondItem, _ := strconv2.StrToFloat64(strList[1])\n\treturn firstItem, secondItem\n}\n\nfunc getFnv32(value []byte) (uint32, error) {\n\t_, err := fnvHash.Write(value)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\thash := fnvHash.Sum32()\n\tfnvHash.Reset()\n\treturn hash, nil\n}\n\nfunc generateSeq(seq *HeadTailSeq, isLeft bool) uint64 {\n\tvar res uint64\n\tif isLeft {\n\t\tres = seq.Head\n\t\tseq.Head--\n\t} else {\n\t\tres = seq.Tail\n\t\tseq.Tail++\n\t}\n\n\treturn res\n}\n\nfunc createNewBufferWithSize(size int) *bytes.Buffer {\n\tbuf := new(bytes.Buffer)\n\tbuf.Grow(int(size))\n\treturn buf\n}\n\nfunc UvarintSize(x uint64) int {\n\ti := 0\n\tfor x >= 0x80 {\n\t\tx >>= 7\n\t\ti++\n\t}\n\treturn i + 1\n}\n"
        },
        {
          "name": "utils_test.go",
          "type": "blob",
          "size": 1.7822265625,
          "content": "// Copyright 2019 The nutsdb Author. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nutsdb\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestMarshalInts(t *testing.T) {\n\tassertions := assert.New(t)\n\tdata, err := MarshalInts([]int{})\n\tassertions.NoError(err, \"TestMarshalInts\")\n\n\tints, err := UnmarshalInts(data)\n\tassertions.NoError(err, \"TestMarshalInts\")\n\tassertions.Equal(0, len(ints), \"TestMarshalInts\")\n\n\tdata, err = MarshalInts([]int{1, 3})\n\tassertions.NoError(err, \"TestMarshalInts\")\n\n\tints, err = UnmarshalInts(data)\n\tassertions.NoError(err, \"TestMarshalInts\")\n\tassertions.Equal(2, len(ints), \"TestMarshalInts\")\n\tassertions.Equal(1, ints[0], \"TestMarshalInts\")\n\tassertions.Equal(3, ints[1], \"TestMarshalInts\")\n}\n\nfunc TestMatchForRange(t *testing.T) {\n\tassertions := assert.New(t)\n\n\tend, err := MatchForRange(\"*\", \"hello\", func(key string) bool {\n\t\treturn true\n\t})\n\tassertions.NoError(err, \"TestMatchForRange\")\n\tassertions.False(end, \"TestMatchForRange\")\n\n\t_, err = MatchForRange(\"[\", \"hello\", func(key string) bool {\n\t\treturn true\n\t})\n\tassertions.Error(err, \"TestMatchForRange\")\n\n\tend, err = MatchForRange(\"*\", \"hello\", func(key string) bool {\n\t\treturn false\n\t})\n\tassertions.NoError(err, \"TestMatchForRange\")\n\tassertions.True(end, \"TestMatchForRange\")\n}\n"
        },
        {
          "name": "value.go",
          "type": "blob",
          "size": 0.6474609375,
          "content": "package nutsdb\n\nimport (\n\t\"sync\"\n\t\"sync/atomic\"\n)\n\ntype request struct {\n\ttx  *Tx\n\tWg  sync.WaitGroup\n\tErr error\n\tref int32\n}\n\nvar requestPool = sync.Pool{\n\tNew: func() interface{} {\n\t\treturn new(request)\n\t},\n}\n\nfunc (req *request) reset() {\n\treq.tx = nil\n\treq.Wg = sync.WaitGroup{}\n\treq.Err = nil\n\n\tatomic.StoreInt32(&req.ref, 0)\n}\n\nfunc (req *request) IncrRef() {\n\tatomic.AddInt32(&req.ref, 1)\n}\n\nfunc (req *request) DecrRef() {\n\tnRef := atomic.AddInt32(&req.ref, -1)\n\tif nRef > 0 {\n\t\treturn\n\t}\n\treq.tx = nil\n\trequestPool.Put(req)\n}\n\nfunc (req *request) Wait() error {\n\treq.Wg.Wait()\n\terr := req.Err\n\treq.DecrRef() // DecrRef after writing to DB.\n\treturn err\n}\n"
        }
      ]
    }
  ]
}