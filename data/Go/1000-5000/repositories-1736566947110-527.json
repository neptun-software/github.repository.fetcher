{
  "metadata": {
    "timestamp": 1736566947110,
    "page": 527,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "jdkato/prose",
      "stars": 3061,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.294921875,
          "content": "bin\n\n# Compiled Object files, Static and Dynamic libs (Shared Objects)\n*.o\n*.a\n*.so\n\n# Folders\n_obj\n_test\ntag/*.gob\nfuzz/\n\n# Architecture specific extensions/prefixes\n*.[568vq]\n[568vq].out\n\n*.cgo1.go\n*.cgo2.c\n_cgo_defun.c\n_cgo_gotypes.go\n_cgo_export.*\n\n_testmain.go\n\n*.exe\n*.test\n*.prof\n\ntestdata/temp\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.2197265625,
          "content": "dist: bionic\nlanguage: go\ngo:\n  - \"1.13.x\"\ninstall:\n  - curl -sfL https://install.goreleaser.com/github.com/golangci/golangci-lint.sh | bash\n  - go get github.com/mattn/goveralls\nscript:\n  - make ci\n  - bash scripts/cover.sh\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.048828125,
          "content": "MIT License\n\nCopyright (c) 2017 -2018 Joseph Kato\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.470703125,
          "content": "BASE_DIR=$(shell echo $$GOPATH)/src/github.com/jdkato/prose\nBUILD_DIR=./builds\n\nLDFLAGS=-ldflags \"-s -w\"\n\n.PHONY: clean test lint ci cross install bump model setup\n\nall: build\n\nbuild:\n\tgo build ${LDFLAGS} -o bin/prose ./cmd/prose\n\nbuild-win:\n\tgo build ${LDFLAGS} -o bin/prose.exe ./cmd/prose\n\nbench:\n\tgo test -bench=. -run=^$$ -benchmem\n\ntest:\n\tgo test -v\n\nci: lint test\n\nlint:\n\t./bin/golangci-lint run\n\nmodel:\n\tgo-bindata -ignore=\\\\.DS_Store -pkg=\"prose\" -o data.go model/**/*.gob\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.1220703125,
          "content": "# prose [![Build Status](https://travis-ci.org/jdkato/prose.svg?branch=master)](https://travis-ci.org/jdkato/prose) [![GoDoc](https://godoc.org/github.com/golang/gddo?status.svg)](https://pkg.go.dev/github.com/jdkato/prose/v2@v2.0.0?tab=doc) [![Coverage Status](https://coveralls.io/repos/github/jdkato/prose/badge.svg?branch=master)](https://coveralls.io/github/jdkato/prose?branch=master) [![Go Report Card](https://goreportcard.com/badge/github.com/jdkato/prose)](https://goreportcard.com/report/github.com/jdkato/prose) [![codebeat badge](https://codebeat.co/badges/a867ec38-c025-4f65-85f9-89a9188cc458)](https://codebeat.co/projects/github-com-jdkato-prose-master) [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/avelino/awesome-go#natural-language-processing)\n\n`prose` is a natural language processing library (English only, at the moment) in *pure Go*. It supports tokenization, segmentation, part-of-speech tagging, and named-entity extraction.\n\nYou can find a more detailed summary on the library's performance here: [Introducing `prose` v2.0.0: Bringing NLP *to Go*](https://medium.com/@errata.ai/introducing-prose-v2-0-0-bringing-nlp-to-go-a1f0c121e4a5).\n\n## Installation\n\n```console\n$ go get github.com/jdkato/prose/v2\n```\n\n## Usage\n\n### Contents\n\n* [Overview](#overview)\n* [Tokenizing](#tokenizing)\n* [Segmenting](#segmenting)\n* [Tagging](#tagging)\n* [NER](#ner)\n\n### Overview\n\n\n```go\npackage main\n\nimport (\n    \"fmt\"\n    \"log\"\n\n    \"github.com/jdkato/prose/v2\"\n)\n\nfunc main() {\n    // Create a new document with the default configuration:\n    doc, err := prose.NewDocument(\"Go is an open-source programming language created at Google.\")\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    // Iterate over the doc's tokens:\n    for _, tok := range doc.Tokens() {\n        fmt.Println(tok.Text, tok.Tag, tok.Label)\n        // Go NNP B-GPE\n        // is VBZ O\n        // an DT O\n        // ...\n    }\n\n    // Iterate over the doc's named-entities:\n    for _, ent := range doc.Entities() {\n        fmt.Println(ent.Text, ent.Label)\n        // Go GPE\n        // Google GPE\n    }\n\n    // Iterate over the doc's sentences:\n    for _, sent := range doc.Sentences() {\n        fmt.Println(sent.Text)\n        // Go is an open-source programming language created at Google.\n    }\n}\n```\n\nThe document-creation process adheres to the following sequence of steps:\n\n```text\ntokenization -> POS tagging -> NE extraction\n            \\\n             segmentation\n```\n\nEach step may be disabled (assuming later steps aren't required) by passing the appropriate [*functional option*](https://dave.cheney.net/2014/10/17/functional-options-for-friendly-apis). To disable named-entity extraction, for example, you'd do the following:\n\n```go\ndoc, err := prose.NewDocument(\n        \"Go is an open-source programming language created at Google.\",\n        prose.WithExtraction(false))\n```\n\n### Tokenizing\n\n`prose` includes a tokenizer capable of processing modern text, including the non-word character spans shown below.\n\n| Type            | Example                           |\n|-----------------|-----------------------------------|\n| Email addresses | `Jane.Doe@example.com`            |\n| Hashtags        | `#trending`                       |\n| Mentions        | `@jdkato`                         |\n| URLs            | `https://github.com/jdkato/prose` |\n| Emoticons       | `:-)`, `>:(`, `o_0`, etc.         |\n\n\n```go\npackage main\n\nimport (\n    \"fmt\"\n    \"log\"\n\n    \"github.com/jdkato/prose/v2\"\n)\n\nfunc main() {\n    // Create a new document with the default configuration:\n    doc, err := prose.NewDocument(\"@jdkato, go to http://example.com thanks :).\")\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    // Iterate over the doc's tokens:\n    for _, tok := range doc.Tokens() {\n        fmt.Println(tok.Text, tok.Tag)\n        // @jdkato NN\n        // , ,\n        // go VB\n        // to TO\n        // http://example.com NN\n        // thanks NNS\n        // :) SYM\n        // . .\n    }\n}\n```\n\n### Segmenting\n\n`prose` includes one of the most accurate sentence segmenters available, according to the [Golden Rules](https://github.com/diasks2/pragmatic_segmenter#comparison-of-segmentation-tools-libraries-and-algorithms) created by the developers of the `pragmatic_segmenter`.\n\n| Name                | Language | License   | GRS (English)  | GRS (Other) | Speed†   |\n|---------------------|----------|-----------|----------------|-------------|----------|\n| Pragmatic Segmenter | Ruby     | MIT       | 98.08% (51/52) | 100.00%     | 3.84 s   |\n| prose               | Go       | MIT       | 75.00% (39/52) | N/A         | 0.96 s   |\n| TactfulTokenizer    | Ruby     | GNU GPLv3 | 65.38% (34/52) | 48.57%      | 46.32 s  |\n| OpenNLP             | Java     | APLv2     | 59.62% (31/52) | 45.71%      | 1.27 s   |\n| Standford CoreNLP   | Java     | GNU GPLv3 | 59.62% (31/52) | 31.43%      | 0.92 s   |\n| Splitta             | Python   | APLv2     | 55.77% (29/52) | 37.14%      | N/A      |\n| Punkt               | Python   | APLv2     | 46.15% (24/52) | 48.57%      | 1.79 s   |\n| SRX English         | Ruby     | GNU GPLv3 | 30.77% (16/52) | 28.57%      | 6.19 s   |\n| Scapel              | Ruby     | GNU GPLv3 | 28.85% (15/52) | 20.00%      | 0.13 s   |\n\n> † The original tests were performed using a *MacBook Pro 3.7 GHz Quad-Core Intel Xeon E5 running 10.9.5*, while `prose` was timed using a *MacBook Pro 2.9 GHz Intel Core i7 running 10.13.3*.\n\n```go\npackage main\n\nimport (\n    \"fmt\"\n    \"strings\"\n\n    \"github.com/jdkato/prose/v2\"\n)\n\nfunc main() {\n    // Create a new document with the default configuration:\n    doc, _ := prose.NewDocument(strings.Join([]string{\n        \"I can see Mt. Fuji from here.\",\n        \"St. Michael's Church is on 5th st. near the light.\"}, \" \"))\n\n    // Iterate over the doc's sentences:\n    sents := doc.Sentences()\n    fmt.Println(len(sents)) // 2\n    for _, sent := range sents {\n        fmt.Println(sent.Text)\n        // I can see Mt. Fuji from here.\n        // St. Michael's Church is on 5th st. near the light.\n    }\n}\n```\n\n### Tagging\n\n`prose` includes a tagger based on Textblob's [\"fast and accurate\" POS tagger](https://github.com/sloria/textblob-aptagger). Below is a comparison of its performance against [NLTK](http://www.nltk.org/)'s implementation of the same tagger on the Treebank corpus:\n\n| Library | Accuracy | 5-Run Average (sec) |\n|:--------|---------:|--------------------:|\n| NLTK    |    0.893 |               7.224 |\n| `prose` |    0.961 |               2.538 |\n\n(See [`scripts/test_model.py`](https://github.com/jdkato/aptag/blob/master/scripts/test_model.py) for more information.)\n\nThe full list of supported POS tags is given below.\n\n| TAG        | DESCRIPTION                               |\n|------------|-------------------------------------------|\n| `(`        | left round bracket                        |\n| `)`        | right round bracket                       |\n| `,`        | comma                                     |\n| `:`        | colon                                     |\n| `.`        | period                                    |\n| `''`       | closing quotation mark                    |\n| ``` `` ``` | opening quotation mark                    |\n| `#`        | number sign                               |\n| `$`        | currency                                  |\n| `CC`       | conjunction, coordinating                 |\n| `CD`       | cardinal number                           |\n| `DT`       | determiner                                |\n| `EX`       | existential there                         |\n| `FW`       | foreign word                              |\n| `IN`       | conjunction, subordinating or preposition |\n| `JJ`       | adjective                                 |\n| `JJR`      | adjective, comparative                    |\n| `JJS`      | adjective, superlative                    |\n| `LS`       | list item marker                          |\n| `MD`       | verb, modal auxiliary                     |\n| `NN`       | noun, singular or mass                    |\n| `NNP`      | noun, proper singular                     |\n| `NNPS`     | noun, proper plural                       |\n| `NNS`      | noun, plural                              |\n| `PDT`      | predeterminer                             |\n| `POS`      | possessive ending                         |\n| `PRP`      | pronoun, personal                         |\n| `PRP$`     | pronoun, possessive                       |\n| `RB`       | adverb                                    |\n| `RBR`      | adverb, comparative                       |\n| `RBS`      | adverb, superlative                       |\n| `RP`       | adverb, particle                          |\n| `SYM`      | symbol                                    |\n| `TO`       | infinitival to                            |\n| `UH`       | interjection                              |\n| `VB`       | verb, base form                           |\n| `VBD`      | verb, past tense                          |\n| `VBG`      | verb, gerund or present participle        |\n| `VBN`      | verb, past participle                     |\n| `VBP`      | verb, non-3rd person singular present     |\n| `VBZ`      | verb, 3rd person singular present         |\n| `WDT`      | wh-determiner                             |\n| `WP`       | wh-pronoun, personal                      |\n| `WP$`      | wh-pronoun, possessive                    |\n| `WRB`      | wh-adverb                                 |\n\n### NER\n\n`prose` v2.0.0 includes a much improved version of v1.0.0's chunk package, which can identify people (`PERSON`) and geographical/political Entities (`GPE`) by default.\n\n```go\npackage main\n\nimport (\n    \"github.com/jdkato/prose/v2\"\n)\n\nfunc main() {\n    doc, _ := prose.NewDocument(\"Lebron James plays basketball in Los Angeles.\")\n    for _, ent := range doc.Entities() {\n        fmt.Println(ent.Text, ent.Label)\n        // Lebron James PERSON\n        // Los Angeles GPE\n    }\n}\n```\n\nHowever, in an attempt to make this feature more useful, we've made it straightforward to train your own models for specific use cases. See [Prodigy + `prose`: Radically efficient machine teaching *in Go*](https://medium.com/@errata.ai/prodigy-prose-radically-efficient-machine-teaching-in-go-93389bf2d772) for a tutorial.\n"
        },
        {
          "name": "appveyor.yml",
          "type": "blob",
          "size": 0.3505859375,
          "content": "version: \"{build}\"\nos: Windows Server 2012 R2\nclone_folder: c:\\GOPATH\\src\\github.com\\jdkato\\prose\nbuild: off\nenvironment:\n  GOPATH: c:\\GOPATH\ninit:\n  - cmd: set PATH=C:\\MinGW\\bin;%PATH%\n  - cmd: copy c:\\MinGW\\bin\\mingw32-make.exe c:\\MinGW\\bin\\make.exe\ninstall:\n  - set PATH=%GOPATH%\\bin;c:\\go\\bin;C:\\Ruby22\\bin;%cd%\\bin;%PATH%\ntest_script:\n  - cmd: make test\n"
        },
        {
          "name": "data.go",
          "type": "blob",
          "size": 17219.2978515625,
          "content": ""
        },
        {
          "name": "doc.go",
          "type": "blob",
          "size": 0.1630859375,
          "content": "// Package prose is a repository of packages related to text processing,\n// including tokenization, part-of-speech tagging, and named-entity extraction.\npackage prose\n"
        },
        {
          "name": "document.go",
          "type": "blob",
          "size": 3.34765625,
          "content": "package prose\n\n// A DocOpt represents a setting that changes the document creation process.\n//\n// For example, it might disable named-entity extraction:\n//\n//    doc := prose.NewDocument(\"...\", prose.WithExtraction(false))\ntype DocOpt func(doc *Document, opts *DocOpts)\n\n// DocOpts controls the Document creation process:\ntype DocOpts struct {\n\tExtract  bool // If true, include named-entity extraction\n\tSegment  bool // If true, include segmentation\n\tTag      bool // If true, include POS tagging\n\tTokenizer Tokenizer // If true, include tokenization\n}\n\n// UsingTokenizer specifies the Tokenizer to use.\nfunc UsingTokenizer(include Tokenizer) DocOpt {\n\treturn func(doc *Document, opts *DocOpts) {\n\t\t// Tagging and entity extraction both require tokenization.\n\t\topts.Tokenizer = include\n\t}\n}\n\n// WithTokenization can enable (the default) or disable tokenization.\n// Deprecated: use UsingTokenizer instead.\nfunc WithTokenization(include bool) DocOpt {\n\treturn func(doc *Document, opts *DocOpts) {\n\t\tif !include {\n\t\t\topts.Tokenizer = nil\n\t\t}\n\t}\n}\n\n// WithTagging can enable (the default) or disable POS tagging.\nfunc WithTagging(include bool) DocOpt {\n\treturn func(doc *Document, opts *DocOpts) {\n\t\topts.Tag = include\n\t}\n}\n\n// WithSegmentation can enable (the default) or disable sentence segmentation.\nfunc WithSegmentation(include bool) DocOpt {\n\treturn func(doc *Document, opts *DocOpts) {\n\t\topts.Segment = include\n\t}\n}\n\n// WithExtraction can enable (the default) or disable named-entity extraction.\nfunc WithExtraction(include bool) DocOpt {\n\treturn func(doc *Document, opts *DocOpts) {\n\t\topts.Extract = include\n\t}\n}\n\n// UsingModel can enable (the default) or disable named-entity extraction.\nfunc UsingModel(model *Model) DocOpt {\n\treturn func(doc *Document, opts *DocOpts) {\n\t\tdoc.Model = model\n\t}\n}\n\n// A Document represents a parsed body of text.\ntype Document struct {\n\tModel *Model\n\tText  string\n\n\t// TODO: Store offsets (begin, end) instead of `text` field.\n\tentities  []Entity\n\tsentences []Sentence\n\ttokens    []*Token\n}\n\n// Tokens returns `doc`'s tokens.\nfunc (doc *Document) Tokens() []Token {\n\ttokens := make([]Token, 0, len(doc.tokens))\n\tfor _, tok := range doc.tokens {\n\t\ttokens = append(tokens, *tok)\n\t}\n\treturn tokens\n}\n\n// Sentences returns `doc`'s sentences.\nfunc (doc *Document) Sentences() []Sentence {\n\treturn doc.sentences\n}\n\n// Entities returns `doc`'s entities.\nfunc (doc *Document) Entities() []Entity {\n\treturn doc.entities\n}\n\nvar defaultOpts = DocOpts{\n\tTokenizer: NewIterTokenizer(),\n\tSegment:  true,\n\tTag:      true,\n\tExtract:  true,\n}\n\n// NewDocument creates a Document according to the user-specified options.\n//\n// For example,\n//\n//    doc := prose.NewDocument(\"...\")\nfunc NewDocument(text string, opts ...DocOpt) (*Document, error) {\n\tvar pipeError error\n\n\tdoc := Document{Text: text}\n\tbase := defaultOpts\n\tfor _, applyOpt := range opts {\n\t\tapplyOpt(&doc, &base)\n\t}\n\n\tif doc.Model == nil {\n\t\tdoc.Model = defaultModel(base.Tag, base.Extract)\n\t}\n\n\tif base.Segment {\n\t\tsegmenter := newPunktSentenceTokenizer()\n\t\tdoc.sentences = segmenter.segment(text)\n\t}\n\tif base.Tokenizer != nil {\n\t\tdoc.tokens = append(doc.tokens, base.Tokenizer.Tokenize(text)...)\n\t}\n\tif base.Tag || base.Extract {\n\t\tdoc.tokens = doc.Model.tagger.tag(doc.tokens)\n\t}\n\tif base.Extract {\n\t\tdoc.tokens = doc.Model.extracter.classify(doc.tokens)\n\t\tdoc.entities = doc.Model.extracter.chunk(doc.tokens)\n\t}\n\n\treturn &doc, pipeError\n}\n"
        },
        {
          "name": "document_test.go",
          "type": "blob",
          "size": 0.7451171875,
          "content": "package prose\n\nimport (\n\t\"path/filepath\"\n\t\"strings\"\n\t\"testing\"\n)\n\nfunc BenchmarkDoc(b *testing.B) {\n\tcontent := readDataFile(filepath.Join(testdata, \"sherlock.txt\"))\n\ttext := string(content)\n\tfor n := 0; n < b.N; n++ {\n\t\t_, err := NewDocument(text)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n}\n\nfunc BenchmarkCustomTokenizer(b *testing.B) {\n\tcontent := readDataFile(filepath.Join(testdata, \"sherlock.txt\"))\n\ttok := NewIterTokenizer(\n\t\tUsingSanitizer(strings.NewReplacer()), // Disable sanitizer\n\t\tUsingPrefixes([]string{\"(\", `\"`, \"[\", \"'\"}),\n\t\tUsingSuffixes([]string{\",\", \")\", `\"`, \"]\", \"!\", \";\", \".\", \"?\", \":\", \"'\"}),\n\t)\n\ttext := string(content)\n\tfor n := 0; n < b.N; n++ {\n\t\t_, err := NewDocument(text, UsingTokenizer(tok))\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "extract.go",
          "type": "blob",
          "size": 12.853515625,
          "content": "package prose\n\nimport (\n\t\"encoding/gob\"\n\t\"math\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"strings\"\n\t\"unicode\"\n\n\t\"gonum.org/v1/gonum/mat\"\n)\n\nvar maxLogDiff = math.Log2(1e-30)\n\ntype mappedProbDist struct {\n\tdict map[string]float64\n\tlog  bool\n}\n\nfunc (m *mappedProbDist) prob(label string) float64 {\n\tif p, found := m.dict[label]; found {\n\t\treturn math.Pow(2, p)\n\t}\n\treturn 0.0\n}\n\nfunc newMappedProbDist(dict map[string]float64, normalize bool) *mappedProbDist {\n\tif normalize {\n\t\tvalues := make([]float64, len(dict))\n\t\ti := 0\n\t\tfor _, v := range dict {\n\t\t\tvalues[i] = v\n\t\t\ti++\n\t\t}\n\t\tsum := sumLogs(values)\n\t\tif sum <= math.Inf(-1) {\n\t\t\tp := math.Log2(1.0 / float64(len(dict)))\n\t\t\tfor k := range dict {\n\t\t\t\tdict[k] = p\n\t\t\t}\n\t\t} else {\n\t\t\tfor k := range dict {\n\t\t\t\tdict[k] -= sum\n\t\t\t}\n\t\t}\n\t}\n\treturn &mappedProbDist{dict: dict, log: true}\n}\n\ntype encodedValue struct {\n\tkey   int\n\tvalue int\n}\n\ntype feature struct {\n\tlabel    string\n\tfeatures map[string]string\n}\n\ntype featureSet []feature\n\nvar featureOrder = []string{\n\t\"bias\", \"en-wordlist\", \"nextpos\", \"nextword\", \"pos\", \"pos+prevtag\",\n\t\"prefix3\", \"prevpos\", \"prevtag\", \"prevword\", \"shape\", \"shape+prevtag\",\n\t\"suffix3\", \"word\", \"word+nextpos\", \"word.lower\", \"wordlen\"}\n\n// binaryMaxentClassifier is a feature encoding that generates vectors\n// containing binary joint-features of the form:\n//\n//    |  joint_feat(fs, l) = { 1 if (fs[fname] == fval) and (l == label)\n//    |                      {\n//    |                      { 0 otherwise\n//\n// where `fname` is the name of an input-feature, `fval` is a value for that\n// input-feature, and `label` is a label.\n//\n// See https://www.nltk.org/_modules/nltk/classify/maxent.html for more\n// information.\ntype binaryMaxentClassifier struct {\n\tcardinality int\n\tlabels      []string\n\tmapping     map[string]int\n\tweights     []float64\n}\n\n// newMaxentClassifier creates a new binaryMaxentClassifier from the provided\n// input values.\nfunc newMaxentClassifier(\n\tweights []float64,\n\tmapping map[string]int,\n\tlabels []string) *binaryMaxentClassifier {\n\n\tset := make(map[string]struct{})\n\tfor label := range mapping {\n\t\tk := strings.Split(label, \"-\")[0]\n\t\tset[k] = struct{}{}\n\t}\n\n\treturn &binaryMaxentClassifier{\n\t\tlen(set) + 1,\n\t\tlabels,\n\t\tmapping,\n\t\tweights}\n}\n\n// marshal saves the model to disk.\nfunc (m *binaryMaxentClassifier) marshal(path string) error {\n\tfolder := filepath.Join(path, \"Maxent\")\n\terr := os.Mkdir(folder, os.ModePerm)\n\tfor i, entry := range []string{\"labels\", \"mapping\", \"weights\"} {\n\t\tcomponent, _ := os.Create(filepath.Join(folder, entry+\".gob\"))\n\t\tencoder := gob.NewEncoder(component)\n\t\tif i == 0 {\n\t\t\tcheckError(encoder.Encode(m.labels))\n\t\t} else if i == 1 {\n\t\t\tcheckError(encoder.Encode(m.mapping))\n\t\t} else {\n\t\t\tcheckError(encoder.Encode(m.weights))\n\t\t}\n\t}\n\treturn err\n}\n\n// entityExtracter is a maximum entropy classifier.\n//\n// See https://www.nltk.org/_modules/nltk/classify/maxent.html for more\n// information.\ntype entityExtracter struct {\n\tmodel *binaryMaxentClassifier\n}\n\n// newEntityExtracter creates a new entityExtracter using the default model.\nfunc newEntityExtracter() *entityExtracter {\n\tvar mapping map[string]int\n\tvar weights []float64\n\tvar labels []string\n\n\tdec := getAsset(\"Maxent\", \"mapping.gob\")\n\tcheckError(dec.Decode(&mapping))\n\n\tdec = getAsset(\"Maxent\", \"weights.gob\")\n\tcheckError(dec.Decode(&weights))\n\n\tdec = getAsset(\"Maxent\", \"labels.gob\")\n\tcheckError(dec.Decode(&labels))\n\n\treturn &entityExtracter{model: newMaxentClassifier(weights, mapping, labels)}\n}\n\n// newTrainedEntityExtracter creates a new EntityExtracter using the given\n// model.\nfunc newTrainedEntityExtracter(model *binaryMaxentClassifier) *entityExtracter {\n\treturn &entityExtracter{model: model}\n}\n\n// chunk finds named-entity \"chunks\" from the given, pre-labeled tokens.\nfunc (e *entityExtracter) chunk(tokens []*Token) []Entity {\n\tentities := []Entity{}\n\tend := \"\"\n\n\tparts := []*Token{}\n\tidx := 0\n\n\tfor _, tok := range tokens {\n\t\tlabel := tok.Label\n\t\tif (label != \"O\" && label != end) ||\n\t\t\t(idx > 0 && tok.Tag == parts[idx-1].Tag) ||\n\t\t\t(idx > 0 && tok.Tag == \"CD\" && parts[idx-1].Label != \"O\") {\n\t\t\tend = strings.Replace(label, \"B\", \"I\", 1)\n\t\t\tparts = append(parts, tok)\n\t\t\tidx++\n\t\t} else if (label == \"O\" && end != \"\") || label == end {\n\t\t\t// We've found the end of an entity.\n\t\t\tif label != \"O\" {\n\t\t\t\tparts = append(parts, tok)\n\t\t\t}\n\t\t\tentities = append(entities, coalesce(parts))\n\n\t\t\tend = \"\"\n\t\t\tparts = []*Token{}\n\t\t\tidx = 0\n\t\t}\n\t}\n\n\treturn entities\n}\n\nfunc (m *binaryMaxentClassifier) encode(features map[string]string, label string) []encodedValue {\n\tencoding := []encodedValue{}\n\tfor _, key := range featureOrder {\n\t\tval := features[key]\n\t\tentry := strings.Join([]string{key, val, label}, \"-\")\n\t\tif ret, found := m.mapping[entry]; found {\n\t\t\tencoding = append(encoding, encodedValue{\n\t\t\t\tkey:   ret,\n\t\t\t\tvalue: 1})\n\t\t}\n\t}\n\treturn encoding\n}\n\nfunc (m *binaryMaxentClassifier) encodeGIS(features map[string]string, label string) []encodedValue {\n\tencoding := m.encode(features, label)\n\tlength := len(m.mapping)\n\n\ttotal := 0\n\tfor _, v := range encoding {\n\t\ttotal += v.value\n\t}\n\tencoding = append(encoding, encodedValue{\n\t\tkey:   length,\n\t\tvalue: m.cardinality - total})\n\n\treturn encoding\n}\n\nfunc adjustPos(text string, start, end int) (int, int) {\n\tindex, left, right := -1, 0, 0\n\t_ = strings.Map(func(r rune) rune {\n\t\tindex++\n\t\tif unicode.IsSpace(r) {\n\t\t\tif index < start {\n\t\t\t\tleft++\n\t\t\t}\n\t\t\tif index < end {\n\t\t\t\tright++\n\t\t\t}\n\t\t\treturn -1\n\t\t}\n\t\treturn r\n\t}, text)\n\treturn start - left, end - right\n}\n\nfunc extractFeatures(tokens []*Token, history []string) []feature {\n\tfeatures := make([]feature, len(tokens))\n\tfor i := range tokens {\n\t\tfeatures[i] = feature{\n\t\t\tlabel:    history[i],\n\t\t\tfeatures: extract(i, tokens, history)}\n\t}\n\treturn features\n}\n\nfunc assignLabels(tokens []*Token, entity *EntityContext) []string {\n\thistory := make([]string, len(tokens))\n\tfor i := range tokens {\n\t\thistory[i] = \"O\"\n\t}\n\n\tif entity.Accept {\n\t\tfor _, span := range entity.Spans {\n\t\t\tstart, end := adjustPos(entity.Text, span.Start, span.End)\n\t\t\tindex := 0\n\t\t\tfor i, tok := range tokens {\n\t\t\t\tif index == start {\n\t\t\t\t\thistory[i] = \"B-\" + span.Label\n\t\t\t\t} else if index > start && index < end {\n\t\t\t\t\thistory[i] = \"I-\" + span.Label\n\t\t\t\t}\n\t\t\t\tindex += len(tok.Text)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn history\n}\n\nfunc makeCorpus(data []EntityContext, tagger *perceptronTagger, tokenizer Tokenizer) featureSet {\n\tcorpus := featureSet{}\n\tfor i := range data {\n\t\tentry := &data[i]\n\t\ttokens := tagger.tag(tokenizer.Tokenize(entry.Text))\n\t\thistory := assignLabels(tokens, entry)\n\t\tfor _, element := range extractFeatures(tokens, history) {\n\t\t\tcorpus = append(corpus, element)\n\t\t}\n\t}\n\treturn corpus\n}\n\nfunc extracterFromData(corpus featureSet) *entityExtracter {\n\tencoding := encode(corpus)\n\tcInv := 1.0 / float64(encoding.cardinality)\n\n\tempfreq := empiricalCount(corpus, encoding)\n\trows, _ := empfreq.Dims()\n\n\tunattested := []int{}\n\tfor index := 0; index < rows; index++ {\n\t\tif empfreq.At(index, 0) == 0.0 {\n\t\t\tunattested = append(unattested, index)\n\t\t}\n\t\tempfreq.SetVec(index, math.Log2(empfreq.At(index, 0)))\n\t}\n\n\tweights := make([]float64, rows)\n\tfor _, idx := range unattested {\n\t\tweights[idx] = math.Inf(-1)\n\t}\n\tencoding.weights = weights\n\n\tclassifier := newTrainedEntityExtracter(encoding)\n\tfor index := 0; index < 100; index++ {\n\t\test := estCount(classifier, corpus, encoding)\n\t\tfor _, idx := range unattested {\n\t\t\test.SetVec(idx, est.AtVec(idx)+1)\n\t\t}\n\t\trows, _ := est.Dims()\n\t\tfor index := 0; index < rows; index++ {\n\t\t\test.SetVec(index, math.Log2(est.At(index, 0)))\n\t\t}\n\t\tweights = classifier.model.weights\n\n\t\test.SubVec(empfreq, est)\n\t\test.ScaleVec(cInv, est)\n\n\t\tfor index := 0; index < len(weights); index++ {\n\t\t\tweights[index] += est.AtVec(index)\n\t\t}\n\n\t\tclassifier.model.weights = weights\n\t}\n\n\treturn classifier\n}\n\nfunc estCount(\n\tclassifier *entityExtracter,\n\tcorpus featureSet,\n\tencoder *binaryMaxentClassifier,\n) *mat.VecDense {\n\tcount := mat.NewVecDense(len(encoder.mapping)+1, nil)\n\tfor _, entry := range corpus {\n\t\tpdist := classifier.probClassify(entry.features)\n\t\tfor label := range pdist.dict {\n\t\t\tprob := pdist.prob(label)\n\t\t\tfor _, enc := range encoder.encodeGIS(entry.features, label) {\n\t\t\t\tout := count.AtVec(enc.key) + (prob * float64(enc.value))\n\t\t\t\tcount.SetVec(enc.key, out)\n\t\t\t}\n\t\t}\n\t}\n\treturn count\n}\n\nfunc (e *entityExtracter) classify(tokens []*Token) []*Token {\n\tlength := len(tokens)\n\thistory := make([]string, 0, length)\n\tfor i := 0; i < length; i++ {\n\t\tscores := make(map[string]float64)\n\t\tfeatures := extract(i, tokens, history)\n\t\tfor _, label := range e.model.labels {\n\t\t\ttotal := 0.0\n\t\t\tfor _, encoded := range e.model.encode(features, label) {\n\t\t\t\ttotal += e.model.weights[encoded.key] * float64(encoded.value)\n\t\t\t}\n\t\t\tscores[label] = total\n\t\t}\n\t\tlabel := max(scores)\n\t\ttokens[i].Label = label\n\t\thistory = append(history, simplePOS(label))\n\t}\n\treturn tokens\n}\n\nfunc (e *entityExtracter) probClassify(features map[string]string) *mappedProbDist {\n\tscores := make(map[string]float64)\n\tfor _, label := range e.model.labels {\n\t\tvec := e.model.encodeGIS(features, label)\n\t\ttotal := 0.0\n\t\tfor _, entry := range vec {\n\t\t\ttotal += e.model.weights[entry.key] * float64(entry.value)\n\t\t}\n\t\tscores[label] = total\n\t}\n\n\t//&mappedProbDist{dict: scores, log: true}\n\treturn newMappedProbDist(scores, true)\n}\n\nfunc parseEntities(ents []string) string {\n\tif stringInSlice(\"B-PERSON\", ents) && len(ents) == 2 {\n\t\t// PERSON takes precedence because it's hard to identify.\n\t\treturn \"PERSON\"\n\t}\n\treturn strings.Split(ents[0], \"-\")[1]\n}\n\nfunc coalesce(parts []*Token) Entity {\n\tlength := len(parts)\n\tlabels := make([]string, length)\n\ttokens := make([]string, length)\n\tfor i, tok := range parts {\n\t\ttokens[i] = tok.Text\n\t\tlabels[i] = tok.Label\n\t}\n\treturn Entity{\n\t\tLabel: parseEntities(labels),\n\t\tText:  strings.Join(tokens, \" \"),\n\t}\n}\n\nfunc extract(i int, ctx []*Token, history []string) map[string]string {\n\tfeats := make(map[string]string)\n\n\tword := ctx[i].Text\n\tprevShape := \"None\"\n\n\tfeats[\"bias\"] = \"True\"\n\tfeats[\"word\"] = word\n\tfeats[\"pos\"] = ctx[i].Tag\n\tfeats[\"en-wordlist\"] = isBasic(word)\n\tfeats[\"word.lower\"] = strings.ToLower(word)\n\tfeats[\"suffix3\"] = nSuffix(word, 3)\n\tfeats[\"prefix3\"] = nPrefix(word, 3)\n\tfeats[\"shape\"] = shape(word)\n\tfeats[\"wordlen\"] = strconv.Itoa(len(word))\n\n\tif i == 0 {\n\t\tfeats[\"prevtag\"] = \"None\"\n\t\tfeats[\"prevword\"], feats[\"prevpos\"] = \"None\", \"None\"\n\t} else if i == 1 {\n\t\tfeats[\"prevword\"] = strings.ToLower(ctx[i-1].Text)\n\t\tfeats[\"prevpos\"] = ctx[i-1].Tag\n\t\tfeats[\"prevtag\"] = history[i-1]\n\t} else {\n\t\tfeats[\"prevword\"] = strings.ToLower(ctx[i-1].Text)\n\t\tfeats[\"prevpos\"] = ctx[i-1].Tag\n\t\tfeats[\"prevtag\"] = history[i-1]\n\t\tprevShape = shape(ctx[i-1].Text)\n\t}\n\n\tif i == len(ctx)-1 {\n\t\tfeats[\"nextword\"], feats[\"nextpos\"] = \"None\", \"None\"\n\t} else {\n\t\tfeats[\"nextword\"] = strings.ToLower(ctx[i+1].Text)\n\t\tfeats[\"nextpos\"] = strings.ToLower(ctx[i+1].Tag)\n\t}\n\n\tfeats[\"word+nextpos\"] = strings.Join(\n\t\t[]string{feats[\"word.lower\"], feats[\"nextpos\"]}, \"+\")\n\tfeats[\"pos+prevtag\"] = strings.Join(\n\t\t[]string{feats[\"pos\"], feats[\"prevtag\"]}, \"+\")\n\tfeats[\"shape+prevtag\"] = strings.Join(\n\t\t[]string{prevShape, feats[\"prevtag\"]}, \"+\")\n\n\treturn feats\n}\n\nfunc shape(word string) string {\n\tif isNumeric(word) {\n\t\treturn \"number\"\n\t} else if match, _ := regexp.MatchString(`\\W+$`, word); match {\n\t\treturn \"punct\"\n\t} else if match, _ := regexp.MatchString(`\\w+$`, word); match {\n\t\tif strings.ToLower(word) == word {\n\t\t\treturn \"downcase\"\n\t\t} else if strings.Title(word) == word {\n\t\t\treturn \"upcase\"\n\t\t} else {\n\t\t\treturn \"mixedcase\"\n\t\t}\n\t}\n\treturn \"other\"\n}\n\nfunc simplePOS(pos string) string {\n\tif strings.HasPrefix(pos, \"V\") {\n\t\treturn \"v\"\n\t}\n\treturn strings.Split(pos, \"-\")[0]\n}\n\nfunc encode(corpus featureSet) *binaryMaxentClassifier {\n\tmapping := make(map[string]int) // maps (fname-fval-label) -> fid\n\tcount := make(map[string]int)   // maps (fname, fval) -> count\n\tweights := []float64{}\n\n\tlabels := []string{}\n\tfor _, entry := range corpus {\n\t\tlabel := entry.label\n\t\tif !stringInSlice(label, labels) {\n\t\t\tlabels = append(labels, label)\n\t\t}\n\n\t\tfor _, fname := range featureOrder {\n\t\t\tfval := entry.features[fname]\n\t\t\tkey := strings.Join([]string{fname, fval}, \"-\")\n\t\t\tcount[key]++\n\t\t\tentry := strings.Join([]string{fname, fval, label}, \"-\")\n\t\t\tif _, found := mapping[entry]; !found {\n\t\t\t\tmapping[entry] = len(mapping)\n\t\t\t}\n\n\t\t}\n\t}\n\treturn newMaxentClassifier(weights, mapping, labels)\n}\n\nfunc empiricalCount(corpus featureSet, encoding *binaryMaxentClassifier) *mat.VecDense {\n\tcount := mat.NewVecDense(len(encoding.mapping)+1, nil)\n\tfor _, entry := range corpus {\n\t\tfor _, encoded := range encoding.encodeGIS(entry.features, entry.label) {\n\t\t\tidx := encoded.key\n\t\t\tcount.SetVec(idx, count.AtVec(idx)+float64(encoded.value))\n\t\t}\n\t}\n\treturn count\n}\n\nfunc addLogs(x, y float64) float64 {\n\tif x < y+maxLogDiff {\n\t\treturn y\n\t} else if y < x+maxLogDiff {\n\t\treturn x\n\t}\n\tbase := math.Min(x, y)\n\treturn base + math.Log2(math.Pow(2, x-base)+math.Pow(2, y-base))\n}\n\nfunc sumLogs(logs []float64) float64 {\n\tif len(logs) == 0 {\n\t\treturn math.Inf(-1)\n\t}\n\tsum := logs[0]\n\tfor _, log := range logs[1:] {\n\t\tsum = addLogs(sum, log)\n\t}\n\treturn sum\n}\n"
        },
        {
          "name": "extract_test.go",
          "type": "blob",
          "size": 1.9501953125,
          "content": "package prose\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"math\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"testing\"\n)\n\nfunc makeNER(text string, model *Model) (*Document, error) {\n\treturn NewDocument(text,\n\t\tWithSegmentation(false),\n\t\tUsingModel(model))\n}\n\ntype prodigyOuput struct {\n\tText   string\n\tSpans  []LabeledEntity\n\tAnswer string\n}\n\nfunc readProdigy(jsonLines []byte) []prodigyOuput {\n\tdec := json.NewDecoder(bytes.NewReader(jsonLines))\n\tentries := []prodigyOuput{}\n\tfor {\n\t\tent := prodigyOuput{}\n\t\terr := dec.Decode(&ent)\n\t\tif err != nil {\n\t\t\tif err == io.EOF {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tpanic(err)\n\t\t}\n\t\tentries = append(entries, ent)\n\t}\n\treturn entries\n}\n\nfunc split(data []prodigyOuput) ([]EntityContext, []prodigyOuput) {\n\tcutoff := int(float64(len(data)) * 0.8)\n\n\ttrain, test := []EntityContext{}, []prodigyOuput{}\n\tfor i := range data {\n\t\tif i < cutoff {\n\t\t\ttrain = append(train, EntityContext{\n\t\t\t\tText:   data[i].Text,\n\t\t\t\tSpans:  data[i].Spans,\n\t\t\t\tAccept: data[i].Answer == \"accept\"})\n\t\t} else {\n\t\t\ttest = append(test, data[i])\n\t\t}\n\t}\n\n\treturn train, test\n}\n\nfunc TestSumLogs(t *testing.T) {\n\ts := sumLogs([]float64{math.Log2(3), math.Log2(5)})\n\tif s != 3.0 {\n\t\tt.Errorf(\"sumLogs() expected = %v, got = %v\", 3.0, s)\n\t}\n}\n\nfunc TestNERProdigy(t *testing.T) {\n\tdata := filepath.Join(testdata, \"reddit_product.jsonl\")\n\n\tfile, e := ioutil.ReadFile(data)\n\tif e != nil {\n\t\tpanic(e)\n\t}\n\n\ttrain, test := split(readProdigy(file))\n\tcorrect := 0.0\n\n\tmodel := ModelFromData(\"PRODUCT\", UsingEntities(train))\n\tfor _, entry := range test {\n\t\tdoc, _ := makeNER(entry.Text, model)\n\t\tents := doc.Entities()\n\t\tif entry.Answer != \"accept\" && len(ents) == 0 {\n\t\t\tcorrect++\n\t\t} else {\n\t\t\texpected := []string{}\n\t\t\tfor _, span := range entry.Spans {\n\t\t\t\texpected = append(expected, entry.Text[span.Start:span.End])\n\t\t\t}\n\t\t\tif reflect.DeepEqual(expected, ents) {\n\t\t\t\tcorrect++\n\t\t\t}\n\t\t}\n\t}\n\n\tr := correct / float64(len(test))\n\tif r < 0.819444 {\n\t\tt.Errorf(\"NERProdigy() expected >= 0.819444, got = %v\", r)\n\t}\n}\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 0.169921875,
          "content": "module github.com/jdkato/prose/v3\n\ngo 1.16\n\nrequire (\n\tgithub.com/neurosnap/sentences v1.0.6 // indirect\n\tgonum.org/v1/gonum v0.7.0\n\tgopkg.in/neurosnap/sentences.v1 v1.0.6\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 2.1806640625,
          "content": "github.com/ajstarks/svgo v0.0.0-20180226025133-644b8db467af/go.mod h1:K08gAheRH3/J6wwsYMMT4xOr94bZjxIelGM0+d/wbFw=\ngithub.com/fogleman/gg v1.2.1-0.20190220221249-0403632d5b90/go.mod h1:R/bRT+9gY/C5z7JzPU0zXsXHKM4/ayA+zqcVNZzPa1k=\ngithub.com/golang/freetype v0.0.0-20170609003504-e2365dfdc4a0/go.mod h1:E/TSTwGwJL78qG/PmXZO1EjYhfJinVAhrmmHX6Z8B9k=\ngithub.com/jung-kurt/gofpdf v1.0.3-0.20190309125859-24315acbbda5/go.mod h1:7Id9E/uU8ce6rXgefFLlgrJj/GYY22cpxn+r32jIOes=\ngithub.com/neurosnap/sentences v1.0.6 h1:iBVUivNtlwGkYsJblWV8GGVFmXzZzak907Ci8aA0VTE=\ngithub.com/neurosnap/sentences v1.0.6/go.mod h1:pg1IapvYpWCJJm/Etxeh0+gtMf1rI1STY9S7eUCPbDc=\ngolang.org/x/exp v0.0.0-20180321215751-8460e604b9de/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20180807140117-3d87b88a115f/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20190125153040-c74c464bbbf2 h1:y102fOLFqhV41b+4GPiJoa0k/x+pJcEi2/HB1Y5T6fU=\ngolang.org/x/exp v0.0.0-20190125153040-c74c464bbbf2/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/image v0.0.0-20180708004352-c73c2afc3b81/go.mod h1:ux5Hcp/YLpHSI86hEcLt0YII63i6oz57MZXIpbrjZUs=\ngolang.org/x/tools v0.0.0-20180525024113-a5b4c53f6e8b/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20190206041539-40960b6deb8e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngonum.org/v1/gonum v0.0.0-20180816165407-929014505bf4/go.mod h1:Y+Yx5eoAFn32cQvJDxZx5Dpnq+c3wtXuadVZAcxbbBo=\ngonum.org/v1/gonum v0.7.0 h1:Hdks0L0hgznZLG9nzXb8vZ0rRvqNvAcgAp84y7Mwkgw=\ngonum.org/v1/gonum v0.7.0/go.mod h1:L02bwd0sqlsvRv41G7wGWFCsVNZFv/k1xzGIxeANHGM=\ngonum.org/v1/netlib v0.0.0-20190313105609-8cb42192e0e0 h1:OE9mWmgKkjJyEmDAAtGMPjXu+YNeGvK9VTSHY6+Qihc=\ngonum.org/v1/netlib v0.0.0-20190313105609-8cb42192e0e0/go.mod h1:wa6Ws7BG/ESfp6dHfk7C6KdzKA7wR7u/rKwOGE66zvw=\ngonum.org/v1/plot v0.0.0-20190515093506-e2840ee46a6b/go.mod h1:Wt8AAjI+ypCyYX3nZBvf6cAIx93T+c/OS2HFAYskSZc=\ngopkg.in/neurosnap/sentences.v1 v1.0.6 h1:v7ElyP020iEZQONyLld3fHILHWOPs+ntzuQTNPkul8E=\ngopkg.in/neurosnap/sentences.v1 v1.0.6/go.mod h1:YlK+SN+fLQZj+kY3r8DkGDhDr91+S3JmTb5LSxFRQo0=\nrsc.io/pdf v0.1.1/go.mod h1:n8OzWcQ6Sp37PL01nO98y4iUCRdTGarVfzxY20ICaU4=\n"
        },
        {
          "name": "model.go",
          "type": "blob",
          "size": 4.0791015625,
          "content": "package prose\n\nimport (\n\t\"io\"\n\t\"io/fs\"\n\t\"os\"\n\t\"path/filepath\"\n)\n\n// A Model holds the structures and data used internally by prose.\ntype Model struct {\n\tName string\n\n\ttagger    *perceptronTagger\n\textracter *entityExtracter\n}\n\n// DataSource provides training data to a Model.\ntype DataSource func(model *Model)\n\n// UsingEntities creates a NER from labeled data.\nfunc UsingEntities(data []EntityContext) DataSource {\n\treturn UsingEntitiesAndTokenizer(data, NewIterTokenizer())\n}\n\n// UsingEntities creates a NER from labeled data and custom tokenizer.\nfunc UsingEntitiesAndTokenizer(data []EntityContext, tokenizer Tokenizer) DataSource {\n\treturn func(model *Model) {\n\t\tcorpus := makeCorpus(data, model.tagger, tokenizer)\n\t\tmodel.extracter = extracterFromData(corpus)\n\t}\n}\n\n// LabeledEntity represents an externally-labeled named-entity.\ntype LabeledEntity struct {\n\tStart int\n\tEnd   int\n\tLabel string\n}\n\n// EntityContext represents text containing named-entities.\ntype EntityContext struct {\n\t// Is this is a correct entity?\n\t//\n\t// Some annotation software, e.g. Prodigy, include entities \"rejected\" by\n\t// its user. This allows us to handle those cases.\n\tAccept bool\n\n\tSpans []LabeledEntity // The entity locations relative to `Text`.\n\tText  string          // The sentence containing the entities.\n}\n\n// ModelFromData creates a new Model from user-provided training data.\nfunc ModelFromData(name string, sources ...DataSource) *Model {\n\tmodel := defaultModel(true, true)\n\tmodel.Name = name\n\tfor _, source := range sources {\n\t\tsource(model)\n\t}\n\treturn model\n}\n\n// ModelFromDisk loads a Model from the user-provided location.\nfunc ModelFromDisk(path string) *Model {\n\tfilesys := os.DirFS(path)\n\treturn &Model{\n\t\tName: filepath.Base(path),\n\n\t\textracter: loadClassifier(filesys),\n\t\ttagger:    newPerceptronTagger(),\n\t}\n}\n\n// ModelFromFS loads a model from the\nfunc ModelFromFS(name string, filesys fs.FS) *Model {\n\t// Locate a folder matching name within filesys\n\tvar modelFS fs.FS\n\terr := fs.WalkDir(filesys, \".\", func(path string, d fs.DirEntry, err error) error {\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Model located. Exit tree traversal\n\t\tif d.Name() == name {\n\t\t\tmodelFS, err = fs.Sub(filesys, path)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\treturn io.EOF\n\t\t}\n\n\t\treturn nil\n\t})\n\tif err != io.EOF {\n\t\tcheckError(err)\n\t}\n\n\treturn &Model{\n\t\tName: name,\n\n\t\textracter: loadClassifier(modelFS),\n\t\ttagger:    newPerceptronTagger(),\n\t}\n}\n\n// Write saves a Model to the user-provided location.\nfunc (m *Model) Write(path string) error {\n\terr := os.MkdirAll(path, os.ModePerm)\n\t// m.Tagger.model.Marshal(path)\n\tcheckError(m.extracter.model.marshal(path))\n\treturn err\n}\n\n/* TODO: External taggers\nfunc loadTagger(path string) *perceptronTagger {\n\tvar wts map[string]map[string]float64\n\tvar tags map[string]string\n\tvar classes []string\n\n\tloc := filepath.Join(path, \"AveragedPerceptron\")\n\tdec := getDiskAsset(filepath.Join(loc, \"weights.gob\"))\n\tcheckError(dec.Decode(&wts))\n\n\tdec = getDiskAsset(filepath.Join(loc, \"tags.gob\"))\n\tcheckError(dec.Decode(&tags))\n\n\tdec = getDiskAsset(filepath.Join(loc, \"classes.gob\"))\n\tcheckError(dec.Decode(&classes))\n\n\tmodel := newAveragedPerceptron(wts, tags, classes)\n\treturn newTrainedPerceptronTagger(model)\n}*/\n\nfunc loadClassifier(filesys fs.FS) *entityExtracter {\n\tvar mapping map[string]int\n\tvar weights []float64\n\tvar labels []string\n\n\tmaxent, err := fs.Sub(filesys, \"Maxent\")\n\tcheckError(err)\n\n\tfile, err := maxent.Open(\"mapping.gob\")\n\tcheckError(err)\n\tcheckError(getDiskAsset(file).Decode(&mapping))\n\n\tfile, err = maxent.Open(\"weights.gob\")\n\tcheckError(err)\n\tcheckError(getDiskAsset(file).Decode(&weights))\n\n\tfile, err = maxent.Open(\"labels.gob\")\n\tcheckError(err)\n\tcheckError(getDiskAsset(file).Decode(&labels))\n\n\tmodel := newMaxentClassifier(weights, mapping, labels)\n\treturn newTrainedEntityExtracter(model)\n}\n\nfunc defaultModel(tagging, classifying bool) *Model {\n\tvar tagger *perceptronTagger\n\tvar classifier *entityExtracter\n\n\tif tagging || classifying {\n\t\ttagger = newPerceptronTagger()\n\t}\n\tif classifying {\n\t\tclassifier = newEntityExtracter()\n\t}\n\n\treturn &Model{\n\t\tName: \"en-v2.0.0\",\n\n\t\ttagger:    tagger,\n\t\textracter: classifier,\n\t}\n}\n"
        },
        {
          "name": "model",
          "type": "tree",
          "content": null
        },
        {
          "name": "model_test.go",
          "type": "blob",
          "size": 1.4580078125,
          "content": "package prose\n\nimport (\n\t\"embed\"\n\t\"io/fs\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"testing\"\n)\n\nfunc TestModelFromDisk(t *testing.T) {\n\tdata := filepath.Join(testdata, \"PRODUCT\")\n\n\tmodel := ModelFromDisk(data)\n\tif model.Name != \"PRODUCT\" {\n\t\tt.Errorf(\"ModelFromDisk() expected = PRODUCT, got = %v\", model.Name)\n\t}\n\n\ttemp := filepath.Join(testdata, \"temp\")\n\t_ = os.RemoveAll(temp)\n\n\terr := model.Write(temp)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tmodel = ModelFromDisk(temp)\n\tif model.Name != \"temp\" {\n\t\tt.Errorf(\"ModelFromDisk() expected = temp, got = %v\", model.Name)\n\t}\n}\n\n//go:embed testdata/PRODUCT\nvar embeddedModel embed.FS\n\nfunc TestModelFromFS(t *testing.T) {\n\terr := fs.WalkDir(embeddedModel, \".\", func(path string, d fs.DirEntry, err error) error {\n\t\t//fmt.Printf(\"Walking dir %s, err %s\\n\", path, err)\n\t\treturn nil\n\t})\n\n\t// Load the embedded PRODUCT model\n\tmodel := ModelFromFS(\"PRODUCT\", embeddedModel)\n\tif model.Name != \"PRODUCT\" {\n\t\tt.Errorf(\"ModelFromFS() expected = PRODUCT, got = %v\", model.Name)\n\t}\n\n\tdoc, err := NewDocument(\"Windows 10 is an operating system\",\n\t\tUsingModel(model))\n\n\tif err != nil {\n\t\tt.Errorf(\"Failed to create doc with ModelFromFS\")\n\t}\n\n\tents := doc.Entities()\n\n\tif len(ents) != 1 {\n\t\tt.Fatalf(\"Expected 1 entity, got %v\", ents)\n\t}\n\n\tif ents[0].Text != \"Windows 10\" {\n\t\tt.Errorf(\"Expected to find entity 'Windows 10' with ModelFromFS, got = %v\", ents[0].Text)\n\t}\n\n\tif ents[0].Label != \"PRODUCT\" {\n\t\tt.Errorf(\"Expected to tab entity with PRODUCT, got = %v\", ents[0].Label)\n\t}\n}\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "segment.go",
          "type": "blob",
          "size": 7.365234375,
          "content": "// Copyright (c) 2015 Eric Bower\n//\n// Permission is hereby granted, free of charge, to any person obtaining a copy\n// of this software and associated documentation files (the \"Software\"), to deal\n// in the Software without restriction, including without limitation the rights\n// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n// copies of the Software, and to permit persons to whom the Software is\n// furnished to do so, subject to the following conditions:\n//\n// The above copyright notice and this permission notice shall be included in all\n// copies or substantial portions of the Software.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n// SOFTWARE.\n\npackage prose\n\nimport (\n\t\"regexp\"\n\t\"strings\"\n\n\t\"gopkg.in/neurosnap/sentences.v1\"\n\t\"gopkg.in/neurosnap/sentences.v1/data\"\n)\n\n// punktSentenceTokenizer is an extension of the Go implementation of the Punkt\n// sentence tokenizer (https://github.com/neurosnap/sentences), with a few\n// minor improvements (see https://github.com/neurosnap/sentences/pull/18).\ntype punktSentenceTokenizer struct {\n\ttokenizer *sentences.DefaultSentenceTokenizer\n}\n\n// newPunktSentenceTokenizer creates a new PunktSentenceTokenizer and loads\n// its English model.\nfunc newPunktSentenceTokenizer() *punktSentenceTokenizer {\n\tvar pt punktSentenceTokenizer\n\tvar err error\n\n\tpt.tokenizer, err = newSentenceTokenizer(nil)\n\tcheckError(err)\n\n\treturn &pt\n}\n\n// segment splits text into sentences.\nfunc (p punktSentenceTokenizer) segment(text string) []Sentence {\n\ttokens := p.tokenizer.Tokenize(text)\n\tsents := make([]Sentence, len(tokens))\n\tfor i := range tokens {\n\t\tsents[i] = Sentence{Text: strings.TrimSpace(tokens[i].Text)}\n\t}\n\treturn sents\n}\n\ntype wordTokenizer struct {\n\tsentences.DefaultWordTokenizer\n}\n\nvar reAbbr = regexp.MustCompile(`((?:[\\w]\\.)+[\\w]*\\.)`)\nvar reLooksLikeEllipsis = regexp.MustCompile(`(?:\\.\\s?){2,}\\.`)\nvar reEntities = regexp.MustCompile(`Yahoo!`)\n\n// English customized sentence tokenizer.\nfunc newSentenceTokenizer(s *sentences.Storage) (*sentences.DefaultSentenceTokenizer, error) {\n\ttraining := s\n\n\tif training == nil {\n\t\tb, err := data.Asset(\"data/english.json\")\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\ttraining, err = sentences.LoadTraining(b)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// supervisor abbreviations\n\tabbrevs := []string{\"sgt\", \"gov\", \"no\", \"mt\"}\n\tfor _, abbr := range abbrevs {\n\t\ttraining.AbbrevTypes.Add(abbr)\n\t}\n\n\tlang := sentences.NewPunctStrings()\n\tword := newWordTokenizer(lang)\n\tannotations := sentences.NewAnnotations(training, lang, word)\n\n\tortho := &sentences.OrthoContext{\n\t\tStorage:      training,\n\t\tPunctStrings: lang,\n\t\tTokenType:    word,\n\t\tTokenFirst:   word,\n\t}\n\n\tmultiPunct := &multiPunctWordAnnotation{\n\t\tStorage:      training,\n\t\tTokenParser:  word,\n\t\tTokenGrouper: &sentences.DefaultTokenGrouper{},\n\t\tOrtho:        ortho,\n\t}\n\n\tannotations = append(annotations, multiPunct)\n\n\ttokenizer := &sentences.DefaultSentenceTokenizer{\n\t\tStorage:       training,\n\t\tPunctStrings:  lang,\n\t\tWordTokenizer: word,\n\t\tAnnotations:   annotations,\n\t}\n\n\treturn tokenizer, nil\n}\n\nfunc newWordTokenizer(p sentences.PunctStrings) *wordTokenizer {\n\tword := &wordTokenizer{}\n\tword.PunctStrings = p\n\n\treturn word\n}\n\nfunc (e *wordTokenizer) HasSentEndChars(t *sentences.Token) bool {\n\tenders := []string{\n\t\t`.\"`, `.)`, `.’`, `.”`,\n\t\t`?`, `?\"`, `?'`, `?)`, `?’`, `?”`,\n\t\t`!`, `!\"`, `!'`, `!)`, `!’`, `!”`,\n\t}\n\n\tfor _, ender := range enders {\n\t\tif strings.HasSuffix(t.Tok, ender) && !reEntities.MatchString(t.Tok) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\tparens := []string{\n\t\t`.[`, `.(`, `.\"`,\n\t\t`?[`, `?(`,\n\t\t`![`, `!(`,\n\t}\n\n\tfor _, paren := range parens {\n\t\tif strings.Contains(t.Tok, paren) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\n// MultiPunctWordAnnotation attempts to tease out custom Abbreviations such as\n// \"F.B.I.\"\ntype multiPunctWordAnnotation struct {\n\t*sentences.Storage\n\tsentences.TokenParser\n\tsentences.TokenGrouper\n\tsentences.Ortho\n}\n\nfunc (a *multiPunctWordAnnotation) Annotate(tokens []*sentences.Token) []*sentences.Token {\n\tfor _, tokPair := range a.TokenGrouper.Group(tokens) {\n\t\tif len(tokPair) < 2 || tokPair[1] == nil {\n\t\t\ttok := tokPair[0].Tok\n\t\t\tif strings.Contains(tok, \"\\n\") && strings.Contains(tok, \" \") {\n\t\t\t\t// We've mislabeled due to an errant newline.\n\t\t\t\ttokPair[0].SentBreak = false\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\ta.tokenAnnotation(tokPair[0], tokPair[1])\n\t}\n\n\treturn tokens\n}\n\n// looksInternal determines if tok's punctuation could appear\n// sentence-internally (i.e., parentheses or quotations).\nfunc looksInternal(tok string) bool {\n\tinternal := []string{\")\", `’`, `”`, `\"`, `'`}\n\tfor _, punc := range internal {\n\t\tif strings.HasSuffix(tok, punc) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc (a *multiPunctWordAnnotation) tokenAnnotation(tokOne, tokTwo *sentences.Token) {\n\t// This is an expensive calculation, so we only want to do it once.\n\tvar nextTyp string\n\n\t// If both tokOne and tokTwo and periods, we're probably in an ellipsis\n\t// that wasn't properly tokenized by `WordTokenizer`.\n\tif strings.HasSuffix(tokOne.Tok, \".\") && tokTwo.Tok == \".\" {\n\t\ttokOne.SentBreak = false\n\t\ttokTwo.SentBreak = false\n\t\treturn\n\t}\n\n\tisNonBreak := strings.HasSuffix(tokOne.Tok, \".\") && !tokOne.SentBreak\n\tisEllipsis := reLooksLikeEllipsis.MatchString(tokOne.Tok)\n\tisInternal := tokOne.SentBreak && looksInternal(tokOne.Tok)\n\n\tif isNonBreak || isEllipsis || isInternal {\n\t\tnextTyp = a.TokenParser.TypeNoSentPeriod(tokTwo)\n\t\tisStarter := a.SentStarters[nextTyp]\n\n\t\t// If the tokOne looks like an ellipsis and tokTwo is either\n\t\t// capitalized or a frequent sentence starter, break the sentence.\n\t\tif isEllipsis {\n\t\t\tif a.TokenParser.FirstUpper(tokTwo) || isStarter != 0 {\n\t\t\t\ttokOne.SentBreak = true\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\t// If the tokOne's sentence-breaking punctuation looks like it could\n\t\t// occur sentence-internally, ensure that the following word is either\n\t\t// capitalized or a frequent sentence starter.\n\t\tif isInternal {\n\t\t\tif a.TokenParser.FirstLower(tokTwo) && isStarter == 0 {\n\t\t\t\ttokOne.SentBreak = false\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\t// If the tokOne ends with a period but isn't marked as a sentence\n\t\t// break, mark it if tokTwo is capitalized and can occur in _ORTHO_LC.\n\t\tif isNonBreak && a.TokenParser.FirstUpper(tokTwo) {\n\t\t\tif a.Storage.OrthoContext[nextTyp]&112 != 0 {\n\t\t\t\ttokOne.SentBreak = true\n\t\t\t}\n\t\t}\n\t}\n\n\tif len(reAbbr.FindAllString(tokOne.Tok, 1)) == 0 {\n\t\treturn\n\t}\n\n\tif a.IsInitial(tokOne) {\n\t\treturn\n\t}\n\n\ttokOne.Abbr = true\n\ttokOne.SentBreak = false\n\n\t// [4.1.1. Orthographic Heuristic] Check if there's\n\t// orthogrpahic evidence about whether the next word\n\t// starts a sentence or not.\n\tisSentStarter := a.Ortho.Heuristic(tokTwo)\n\tif isSentStarter == 1 {\n\t\ttokOne.SentBreak = true\n\t\treturn\n\t}\n\n\tif nextTyp == \"\" {\n\t\tnextTyp = a.TokenParser.TypeNoSentPeriod(tokTwo)\n\t}\n\n\t// [4.1.3. Frequent Sentence Starter Heruistic] If the\n\t// next word is capitalized, and is a member of the\n\t// frequent-sentence-starters list, then label tok as a\n\t// sentence break.\n\tif a.TokenParser.FirstUpper(tokTwo) && a.SentStarters[nextTyp] != 0 {\n\t\ttokOne.SentBreak = true\n\t\treturn\n\t}\n}\n"
        },
        {
          "name": "segment_test.go",
          "type": "blob",
          "size": 15.205078125,
          "content": "package prose\n\nimport (\n\t\"encoding/json\"\n\t\"io/ioutil\"\n\t\"path/filepath\"\n\t\"testing\"\n)\n\ntype goldenRule struct {\n\tName   string\n\tInput  string\n\tOutput []string\n}\n\nfunc readDataFile(path string) []byte {\n\tp, err := filepath.Abs(path)\n\tcheckError(err)\n\n\tdata, ferr := ioutil.ReadFile(p)\n\tcheckError(ferr)\n\n\treturn data\n}\n\nfunc makeSegmenter(text string) (*Document, error) {\n\treturn NewDocument(\n\t\ttext,\n\t\tUsingTokenizer(nil),\n\t\tWithTagging(false),\n\t\tWithExtraction(false))\n}\n\nfunc BenchmarkPunkt(b *testing.B) {\n\ttests := make([]goldenRule, 0)\n\tcases := readDataFile(filepath.Join(testdata, \"golden_rules_en.json\"))\n\n\tcheckError(json.Unmarshal(cases, &tests))\n\tfor n := 0; n < b.N; n++ {\n\t\tfor i := range tests {\n\t\t\t_, err := makeSegmenter(tests[i].Input)\n\t\t\tif err != nil {\n\t\t\t\tpanic(err)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestEnglishSmartQuotes(t *testing.T) {\n\tactualText := \"Here is a quote, ”a smart one.” Will this break properly?\"\n\tdoc, _ := makeSegmenter(actualText)\n\tactual := doc.Sentences()\n\n\texpected := []string{\n\t\t\"Here is a quote, ”a smart one.”\",\n\t\t\"Will this break properly?\",\n\t}\n\n\tif len(actual) != len(expected) {\n\t\tt.Fatalf(\"Actual: %d, Expected: %d\", len(actual), len(expected))\n\t}\n\n\tfor index, sent := range actual {\n\t\tif sent.Text != expected[index] {\n\t\t\tt.Fatalf(\"Actual: %s\\nExpected: %s\", sent, expected[index])\n\t\t}\n\t}\n}\n\nfunc TestEnglishCustomAbbrev(t *testing.T) {\n\tactualText := \"One custom abbreviation is F.B.I.  The abbreviation, F.B.I. should properly break.\"\n\tdoc, _ := makeSegmenter(actualText)\n\tactual := doc.Sentences()\n\n\texpected := []string{\n\t\t\"One custom abbreviation is F.B.I.\",\n\t\t\"The abbreviation, F.B.I. should properly break.\",\n\t}\n\n\tif len(actual) != len(expected) {\n\t\tt.Fatalf(\"Actual: %d, Expected: %d\", len(actual), len(expected))\n\t}\n\n\tfor index, sent := range actual {\n\t\tif sent.Text != expected[index] {\n\t\t\tt.Fatalf(\"Actual: %s\\nExpected: %s\", sent, expected[index])\n\t\t}\n\t}\n\n\tactualText = \"An abbreviation near the end of a G.D. sentence.  J.G. Wentworth was cool.\"\n\tdoc, _ = makeSegmenter(actualText)\n\tactual = doc.Sentences()\n\n\texpected = []string{\n\t\t\"An abbreviation near the end of a G.D. sentence.\",\n\t\t\"J.G. Wentworth was cool.\",\n\t}\n\n\tif len(actual) != len(expected) {\n\t\tt.Fatalf(\"Actual: %d, Expected: %d\", len(actual), len(expected))\n\t}\n\n\tfor index, sent := range actual {\n\t\tif sent.Text != expected[index] {\n\t\t\tt.Fatalf(\"Actual: %s\\nExpected: %s\", sent, expected[index])\n\t\t}\n\t}\n}\n\nfunc TestEnglishSupervisedAbbrev(t *testing.T) {\n\tactualText := \"I am a Sgt. in the army.  I am a No. 1 student.  The Gov. of Michigan is a dick.\"\n\tdoc, _ := makeSegmenter(actualText)\n\tactual := doc.Sentences()\n\n\texpected := []string{\n\t\t\"I am a Sgt. in the army.\",\n\t\t\"I am a No. 1 student.\",\n\t\t\"The Gov. of Michigan is a dick.\",\n\t}\n\n\tif len(actual) != len(expected) {\n\t\tt.Fatalf(\"Actual: %d, Expected: %d\", len(actual), len(expected))\n\t}\n\n\tfor index, sent := range actual {\n\t\tif sent.Text != expected[index] {\n\t\t\tt.Fatalf(\"Actual: %s\\nExpected: %s\", sent, expected[index])\n\t\t}\n\t}\n}\n\nfunc TestEnglishSemicolon(t *testing.T) {\n\tactualText := \"I am here; you are over there.  Will the tokenizer output two complete sentences?\"\n\tdoc, _ := makeSegmenter(actualText)\n\tactual := doc.Sentences()\n\n\texpected := []string{\n\t\t\"I am here; you are over there.\",\n\t\t\"Will the tokenizer output two complete sentences?\",\n\t}\n\n\tif len(actual) != len(expected) {\n\t\tt.Fatalf(\"Actual: %d, Expected: %d\", len(actual), len(expected))\n\t}\n\n\tfor index, sent := range actual {\n\t\tif sent.Text != expected[index] {\n\t\t\tt.Fatalf(\"Actual: %s\\nExpected: %s\", sent, expected[index])\n\t\t}\n\t}\n}\n\nfunc compareSentences(t *testing.T, actualText string, expected []string, test string) bool {\n\tdoc, _ := makeSegmenter(actualText)\n\tactual := doc.Sentences()\n\n\tif len(actual) != len(expected) {\n\t\tt.Log(test)\n\t\tt.Logf(\"Actual: %v\\n\", actual)\n\t\tt.Errorf(\"Actual: %d, Expected: %d\\n\", len(actual), len(expected))\n\t\tt.Log(\"===\")\n\t\treturn false\n\t}\n\n\tfor index, sent := range actual {\n\t\tif sent.Text != expected[index] {\n\t\t\tt.Log(test)\n\t\t\tt.Errorf(\"Actual: [%s] Expected: [%s]\\n\", sent, expected[index])\n\t\t\tt.Log(\"===\")\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// Based on https://github.com/diasks2/pragmatic_segmenter#the-golden-rules.\nfunc TestGoldenRules(t *testing.T) {\n\tvar actualText string\n\tvar expected []string\n\n\ttest := \"1. Simple period to end sentence\"\n\tactualText = \"Hello World. My name is Jonas.\"\n\texpected = []string{\n\t\t\"Hello World.\",\n\t\t\"My name is Jonas.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"2. Question mark to end sentence\"\n\tactualText = \"What is your name? My name is Jonas.\"\n\texpected = []string{\n\t\t\"What is your name?\",\n\t\t\"My name is Jonas.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"3. Exclamation point to end sentence\"\n\tactualText = \"There it is! I found it.\"\n\texpected = []string{\n\t\t\"There it is!\",\n\t\t\"I found it.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"4. One letter upper case abbreviations\"\n\tactualText = \"My name is Jonas E. Smith.\"\n\texpected = []string{\n\t\t\"My name is Jonas E. Smith.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"5. One letter lower case abbreviations\"\n\tactualText = \"Please turn to p. 55.\"\n\texpected = []string{\n\t\t\"Please turn to p. 55.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"6. Two letter lower case abbreviations in the middle of a sentence\"\n\tactualText = \"Were Jane and co. at the party?\"\n\texpected = []string{\n\t\t\"Were Jane and co. at the party?\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"7. Two letter upper case abbreviations in the middle of a sentence\"\n\tactualText = \"They closed the deal with Pitt, Briggs & Co. at noon.\"\n\texpected = []string{\n\t\t\"They closed the deal with Pitt, Briggs & Co. at noon.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"8. Two letter lower case abbreviations at the end of a sentence\"\n\tactualText = \"Let's ask Jane and co. They should know.\"\n\texpected = []string{\n\t\t\"Let's ask Jane and co.\",\n\t\t\"They should know.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"9. Two letter upper case abbreviations at the end of a sentence\"\n\tactualText = \"They closed the deal with Pitt, Briggs & Co. It closed yesterday.\"\n\texpected = []string{\n\t\t\"They closed the deal with Pitt, Briggs & Co.\",\n\t\t\"It closed yesterday.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"10. Two letter (prepositive) abbreviations\"\n\tactualText = \"I can see Mt. Fuji from here.\"\n\texpected = []string{\n\t\t\"I can see Mt. Fuji from here.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"11. Two letter (prepositive & postpositive) abbreviations\"\n\tactualText = \"St. Michael's Church is on 5th st. near the light.\"\n\texpected = []string{\n\t\t\"St. Michael's Church is on 5th st. near the light.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"12. Possesive two letter abbreviations\"\n\tactualText = \"That is JFK Jr.'s book.\"\n\texpected = []string{\n\t\t\"That is JFK Jr.'s book.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"13. Multi-period abbreviations in the middle of a sentence\"\n\tactualText = \"I visited the U.S.A. last year.\"\n\texpected = []string{\n\t\t\"I visited the U.S.A. last year.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\t/* FIXME\n\ttest = \"14. Multi-period abbreviations at the end of a sentence\"\n\tactualText = \"I live in the E.U. How about you?\"\n\texpected = []string{\n\t\t\"I live in the E.U.\",\n\t\t\"How about you?\",\n\t}\n\tcompareSentences(t, actualText, expected, test)*/\n\n\t/* FIXME\n\ttest = \"15. U.S. as sentence boundary\"\n\tactualText = \"I live in the U.S. How about you?\"\n\texpected = []string{\n\t\t\"I live in the U.S.\",\n\t\t\"How about you?\",\n\t}\n\tcompareSentences(t, actualText, expected, test) */\n\n\ttest = \"16. U.S. as non sentence boundary with next word capitalized\"\n\tactualText = \"I work for the U.S. Government in Virginia.\"\n\texpected = []string{\n\t\t\"I work for the U.S. Government in Virginia.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"17. U.S. as non sentence boundary\"\n\tactualText = \"I have lived in the U.S. for 20 years.\"\n\texpected = []string{\n\t\t\"I have lived in the U.S. for 20 years.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\t/* FIXME\n\ttest = \"18. A.M. / P.M. as non sentence boundary and sentence boundary\"\n\tactualText = \"At 5 a.m. Mr. Smith went to the bank. He left the bank at 6 P.M. Mr. Smith then went to the store.\"\n\texpected = []string{\n\t\t\"At 5 a.m. Mr. Smith went to the bank.\",\n\t\t\"He left the bank at 6 P.M.\",\n\t\t\"Mr. Smith then went to the store.\",\n\t}\n\tcompareSentences(t, actualText, expected, test) */\n\n\ttest = \"19. Number as non sentence boundary\"\n\tactualText = \"She has $100.00 in her bag.\"\n\texpected = []string{\n\t\t\"She has $100.00 in her bag.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"20. Number as sentence boundary\"\n\tactualText = \"She has $100.00. It is in her bag.\"\n\texpected = []string{\n\t\t\"She has $100.00.\",\n\t\t\"It is in her bag.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"21. Parenthetical inside sentence\"\n\tactualText = \"He teaches science (He previously worked for 5 years as an engineer.) at the local University.\"\n\texpected = []string{\n\t\t\"He teaches science (He previously worked for 5 years as an engineer.) at the local University.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"22. Email addresses\"\n\tactualText = \"Her email is Jane.Doe@example.com. I sent her an email.\"\n\texpected = []string{\n\t\t\"Her email is Jane.Doe@example.com.\",\n\t\t\"I sent her an email.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"23. Web addresses\"\n\tactualText = \"The site is: https://www.example.50.com/new-site/awesome_content.html. Please check it out.\"\n\texpected = []string{\n\t\t\"The site is: https://www.example.50.com/new-site/awesome_content.html.\",\n\t\t\"Please check it out.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"24. Single quotations inside sentence\"\n\tactualText = \"She turned to him, 'This is great.' she said.\"\n\texpected = []string{\n\t\t\"She turned to him, 'This is great.' she said.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"25. Double quotations inside sentence\"\n\tactualText = \"She turned to him, \\\"This is great.\\\" she said.\"\n\texpected = []string{\n\t\t\"She turned to him, \\\"This is great.\\\" she said.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"26. Double quotations at the end of a sentence\"\n\tactualText = \"She turned to him, \\\"This is great.\\\" She held the book out to show him.\"\n\texpected = []string{\n\t\t\"She turned to him, \\\"This is great.\\\"\",\n\t\t\"She held the book out to show him.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"27. Double punctuation (exclamation point)\"\n\tactualText = \"Hello!! Long time no see.\"\n\texpected = []string{\n\t\t\"Hello!!\",\n\t\t\"Long time no see.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"28. Double punctuation (question mark)\"\n\tactualText = \"Hello?? Who is there?\"\n\texpected = []string{\n\t\t\"Hello??\",\n\t\t\"Who is there?\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"29. Double punctuation (exclamation point / question mark)\"\n\tactualText = \"Hello!? Is that you?\"\n\texpected = []string{\n\t\t\"Hello!?\",\n\t\t\"Is that you?\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"30. Double punctuation (question mark / exclamation point)\"\n\tactualText = \"Hello?! Is that you?\"\n\texpected = []string{\n\t\t\"Hello?!\",\n\t\t\"Is that you?\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"40. Errant newlines in the middle of sentences (PDF)\"\n\tactualText = \"This is a sentence\\ncut off in the middle because pdf.\"\n\texpected = []string{\n\t\t\"This is a sentence\\ncut off in the middle because pdf.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"41. Errant newlines in the middle of sentences\"\n\tactualText = \"It was a cold \\nnight in the city.\"\n\texpected = []string{\n\t\t\"It was a cold \\nnight in the city.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\t/* FIXME\n\ttest = \"43. Geo Coordinates\"\n\tactualText = \"You can find it at N°. 1026.253.553. That is where the treasure is.\"\n\texpected = []string{\n\t\t\"You can find it at N°. 1026.253.553.\",\n\t\t\"That is where the treasure is.\",\n\t}\n\tcompareSentences(t, actualText, expected, test) */\n\n\ttest = \"44. Named entities with an exclamation point\"\n\tactualText = \"She works at Yahoo! in the accounting department.\"\n\texpected = []string{\n\t\t\"She works at Yahoo! in the accounting department.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"45. I as a sentence boundary and I as an abbreviation\"\n\tactualText = \"We make a good team, you and I. Did you see Albert I. Jones yesterday?\"\n\texpected = []string{\n\t\t\"We make a good team, you and I.\",\n\t\t\"Did you see Albert I. Jones yesterday?\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"46. Ellipsis at end of quotation\"\n\tactualText = \"Thoreau argues that by simplifying one’s life, “the laws of the universe will appear less complex. . . .”\"\n\texpected = []string{\n\t\t\"Thoreau argues that by simplifying one’s life, “the laws of the universe will appear less complex. . . .”\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"47. Ellipsis with square brackets\"\n\tactualText = \"\\\"Bohr [...] used the analogy of parallel stairways [...]\\\" (Smith 55).\"\n\texpected = []string{\n\t\t\"\\\"Bohr [...] used the analogy of parallel stairways [...]\\\" (Smith 55).\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"48. Ellipsis as sentence boundary (standard ellipsis rules)\"\n\tactualText = \"If words are left off at the end of a sentence, and that is all that is omitted, indicate the omission with ellipsis marks (preceded and followed by a space) and then indicate the end of the sentence with a period . . . . Next sentence.\"\n\texpected = []string{\n\t\t\"If words are left off at the end of a sentence, and that is all that is omitted, indicate the omission with ellipsis marks (preceded and followed by a space) and then indicate the end of the sentence with a period . . . .\",\n\t\t\"Next sentence.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"49. Ellipsis as sentence boundary (non-standard ellipsis rules)\"\n\tactualText = \"I never meant that.... She left the store.\"\n\texpected = []string{\n\t\t\"I never meant that....\",\n\t\t\"She left the store.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"50. Ellipsis as non sentence boundary\"\n\tactualText = \"I wasn’t really ... well, what I mean...see . . . what I'm saying, the thing is . . . I didn’t mean it.\"\n\texpected = []string{\n\t\t\"I wasn’t really ... well, what I mean...see . . . what I'm saying, the thing is . . . I didn’t mean it.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\ttest = \"51. 4-dot ellipsis\"\n\tactualText = \"One further habit which was somewhat weakened . . . was that of combining words into self-interpreting compounds. . . . The practice was not abandoned. . . .\"\n\texpected = []string{\n\t\t\"One further habit which was somewhat weakened . . . was that of combining words into self-interpreting compounds. . . .\",\n\t\t\"The practice was not abandoned. . . .\",\n\t}\n\tcompareSentences(t, actualText, expected, test)\n\n\t/* FIXME:\n\ttest = \"52. No whitespace in between sentences\"\n\tactualText = \"Hello world.Today is Tuesday.Mr. Smith went to the store and bought 1,000.That is a lot.\"\n\texpected = []string{\n\t\t\"Hello world.\",\n\t\t\"Today is Tuesday.\",\n\t\t\"Mr. Smith went to the store and bought 1,000.\",\n\t\t\"That is a lot.\",\n\t}\n\tcompareSentences(t, actualText, expected, test)*/\n}\n"
        },
        {
          "name": "tag.go",
          "type": "blob",
          "size": 10.103515625,
          "content": "// Copyright 2013 Matthew Honnibal\n//\n// Permission is hereby granted, free of charge, to any person obtaining a copy\n// of this software and associated documentation files (the \"Software\"), to deal\n// in the Software without restriction, including without limitation the rights\n// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n// copies of the Software, and to permit persons to whom the Software is\n// furnished to do so, subject to the following conditions:\n//\n// The above copyright notice and this permission notice shall be included in\n// all copies or substantial portions of the Software.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n// THE SOFTWARE.\n\npackage prose\n\nimport (\n\t\"math\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"strings\"\n)\n\n// TupleSlice is a slice of tuples in the form (words, tags).\ntype TupleSlice [][][]string\n\n// Len returns the length of a Tuple.\nfunc (t TupleSlice) Len() int { return len(t) }\n\n// Swap switches the ith and jth elements in a Tuple.\nfunc (t TupleSlice) Swap(i, j int) { t[i], t[j] = t[j], t[i] }\n\n// ReadTagged converts pre-tagged input into a TupleSlice suitable for training.\nfunc ReadTagged(text, sep string) TupleSlice {\n\tlines := strings.Split(text, \"\\n\")\n\tlength := len(lines)\n\tt := make(TupleSlice, length)\n\tfor i, sent := range lines {\n\t\tset := strings.Split(sent, \" \")\n\t\tlength = len(set)\n\t\ttokens := make([]string, length)\n\t\ttags := make([]string, length)\n\t\tfor j, token := range set {\n\t\t\tparts := strings.Split(token, sep)\n\t\t\ttokens[j] = parts[0]\n\t\t\ttags[j] = parts[1]\n\t\t}\n\t\tt[i] = [][]string{tokens, tags}\n\t}\n\treturn t\n}\n\nvar none = regexp.MustCompile(`^(?:0|\\*[\\w?]\\*|\\*\\-\\d{1,3}|\\*[A-Z]+\\*\\-\\d{1,3}|\\*)$`)\nvar keep = regexp.MustCompile(`^\\-[A-Z]{3}\\-$`)\n\n// averagedPerceptron is a Averaged Perceptron classifier.\ntype averagedPerceptron struct {\n\tclasses []string\n\tstamps  map[string]float64\n\ttotals  map[string]float64\n\ttagMap  map[string]string\n\tweights map[string]map[string]float64\n\n\t// TODO: Training\n\t//\n\t// instances float64\n}\n\n// newAveragedPerceptron creates a new AveragedPerceptron model.\nfunc newAveragedPerceptron(weights map[string]map[string]float64,\n\ttags map[string]string, classes []string) *averagedPerceptron {\n\treturn &averagedPerceptron{\n\t\ttotals: make(map[string]float64), stamps: make(map[string]float64),\n\t\tclasses: classes, tagMap: tags, weights: weights}\n}\n\n/* TODO: Training API\n\n\"github.com/shogo82148/go-shuffle\"\n\n// marshal saves the model to disk.\nfunc (m *averagedPerceptron) marshal(path string) error {\n\tfolder := filepath.Join(path, \"AveragedPerceptron\")\n\terr := os.Mkdir(folder, os.ModePerm)\n\tfor i, entry := range []string{\"weights\", \"tags\", \"classes\"} {\n\t\tcomponent, _ := os.Create(filepath.Join(folder, entry+\".gob\"))\n\t\tencoder := gob.NewEncoder(component)\n\t\tif i == 0 {\n\t\t\tcheckError(encoder.Encode(m.weights))\n\t\t} else if i == 1 {\n\t\t\tcheckError(encoder.Encode(m.tagMap))\n\t\t} else {\n\t\t\tcheckError(encoder.Encode(m.classes))\n\t\t}\n\t}\n\treturn err\n}\n\n// train an Averaged Perceptron model based on sentences.\nfunc (pt *perceptronTagger) train(sentences TupleSlice, iterations int) {\n\tvar guess string\n\tvar found bool\n\n\tpt.makeTagMap(sentences)\n\tfor i := 0; i < iterations; i++ {\n\t\tfor _, tuple := range sentences {\n\t\t\twords, tags := tuple[0], tuple[1]\n\t\t\tp1, p2 := \"-START-\", \"-START2-\"\n\t\t\tcontext := []string{p1, p2}\n\t\t\tfor _, w := range words {\n\t\t\t\tif w == \"\" {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tcontext = append(context, normalize(w))\n\t\t\t}\n\t\t\tcontext = append(context, []string{\"-END-\", \"-END2-\"}...)\n\t\t\tfor i, word := range words {\n\t\t\t\tif guess, found = pt.tagMap[word]; !found {\n\t\t\t\t\tfeats := featurize(i, context, word, p1, p2)\n\t\t\t\t\tguess = pt.model.predict(feats)\n\t\t\t\t\tpt.model.update(tags[i], guess, feats)\n\t\t\t\t}\n\t\t\t\tp2 = p1\n\t\t\t\tp1 = guess\n\t\t\t}\n\t\t}\n\t\tshuffle.Shuffle(sentences)\n\t}\n\tpt.model.averageWeights()\n}\n\nfunc (m *averagedPerceptron) averageWeights() {\n\tfor feat, weights := range m.weights {\n\t\tnewWeights := make(map[string]float64)\n\t\tfor class, weight := range weights {\n\t\t\tkey := feat + \"-\" + class\n\t\t\ttotal := m.totals[key]\n\t\t\ttotal += (m.instances - m.stamps[key]) * weight\n\t\t\taveraged, _ := stats.Round(total/m.instances, 3)\n\t\t\tif averaged != 0.0 {\n\t\t\t\tnewWeights[class] = averaged\n\t\t\t}\n\t\t}\n\t\tm.weights[feat] = newWeights\n\t}\n}\n\n// newTrainedPerceptronTagger creates a new PerceptronTagger using the given\n// model.\nfunc newTrainedPerceptronTagger(model *averagedPerceptron) *perceptronTagger {\n\treturn &perceptronTagger{model: model}\n}\n\nfunc (pt *perceptronTagger) makeTagMap(sentences TupleSlice) {\n\tcounts := make(map[string]map[string]int)\n\tfor _, tuple := range sentences {\n\t\twords, tags := tuple[0], tuple[1]\n\t\tfor i, word := range words {\n\t\t\ttag := tags[i]\n\t\t\tif counts[word] == nil {\n\t\t\t\tcounts[word] = make(map[string]int)\n\t\t\t}\n\t\t\tcounts[word][tag]++\n\t\t\tpt.model.addClass(tag)\n\t\t}\n\t}\n\tfor word, tagFreqs := range counts {\n\t\ttag, mode := maxValue(tagFreqs)\n\t\tn := float64(sumValues(tagFreqs))\n\t\tif n >= 20 && (float64(mode)/n) >= 0.97 {\n\t\t\tpt.tagMap[word] = tag\n\t\t}\n\t}\n}\n\nfunc sumValues(m map[string]int) int {\n\tsum := 0\n\tfor _, v := range m {\n\t\tsum += v\n\t}\n\treturn sum\n}\n\nfunc maxValue(m map[string]int) (string, int) {\n\tmaxValue := 0\n\tkey := \"\"\n\tfor k, v := range m {\n\t\tif v >= maxValue {\n\t\t\tmaxValue = v\n\t\t\tkey = k\n\t\t}\n\t}\n\treturn key, maxValue\n}\n\nfunc get(k string, m map[string]float64) float64 {\n\tif v, ok := m[k]; ok {\n\t\treturn v\n\t}\n\treturn 0.0\n}\n\nfunc (m *averagedPerceptron) update(truth, guess string, feats map[string]float64) {\n\tm.instances++\n\tif truth == guess {\n\t\treturn\n\t}\n\tfor f := range feats {\n\t\tweights := make(map[string]float64)\n\t\tif val, ok := m.weights[f]; ok {\n\t\t\tweights = val\n\t\t} else {\n\t\t\tm.weights[f] = weights\n\t\t}\n\t\tm.updateFeat(truth, f, get(truth, weights), 1.0)\n\t\tm.updateFeat(guess, f, get(guess, weights), -1.0)\n\t}\n}\n\nfunc (m *averagedPerceptron) updateFeat(c, f string, v, w float64) {\n\tkey := f + \"-\" + c\n\tm.totals[key] = (m.instances - m.stamps[key]) * w\n\tm.stamps[key] = m.instances\n\tm.weights[f][c] = w + v\n}\n\nfunc (m *averagedPerceptron) addClass(class string) {\n\tif !stringInSlice(class, m.classes) {\n\t\tm.classes = append(m.classes, class)\n\t}\n}*/\n\n// perceptronTagger is a port of Textblob's \"fast and accurate\" POS tagger.\n// See https://github.com/sloria/textblob-aptagger for details.\ntype perceptronTagger struct {\n\tmodel *averagedPerceptron\n}\n\n// newPerceptronTagger creates a new PerceptronTagger and loads the built-in\n// AveragedPerceptron model.\nfunc newPerceptronTagger() *perceptronTagger {\n\tvar wts map[string]map[string]float64\n\tvar tags map[string]string\n\tvar classes []string\n\n\tdec := getAsset(\"AveragedPerceptron\", \"classes.gob\")\n\tcheckError(dec.Decode(&classes))\n\n\tdec = getAsset(\"AveragedPerceptron\", \"tags.gob\")\n\tcheckError(dec.Decode(&tags))\n\n\tdec = getAsset(\"AveragedPerceptron\", \"weights.gob\")\n\tcheckError(dec.Decode(&wts))\n\n\treturn &perceptronTagger{model: newAveragedPerceptron(wts, tags, classes)}\n}\n\n// tag takes a slice of words and returns a slice of tagged tokens.\nfunc (pt *perceptronTagger) tag(tokens []*Token) []*Token {\n\tvar tag string\n\tvar found bool\n\n\tp1, p2 := \"-START-\", \"-START2-\"\n\tlength := len(tokens) + 4\n\tcontext := make([]string, length)\n\tcontext[0] = p1\n\tcontext[1] = p2\n\tfor i, t := range tokens {\n\t\tcontext[i+2] = normalize(t.Text)\n\t}\n\tcontext[length-2] = \"-END-\"\n\tcontext[length-1] = \"-END2-\"\n\tfor i := 0; i < len(tokens); i++ {\n\t\tword := tokens[i].Text\n\t\tif word == \"-\" {\n\t\t\ttag = \"-\"\n\t\t} else if _, ok := emoticons[word]; ok {\n\t\t\ttag = \"SYM\"\n\t\t} else if strings.HasPrefix(word, \"@\") {\n\t\t\t// TODO: URLs and emails?\n\t\t\ttag = \"NN\"\n\t\t} else if none.MatchString(word) {\n\t\t\ttag = \"-NONE-\"\n\t\t} else if keep.MatchString(word) {\n\t\t\ttag = word\n\t\t} else if tag, found = pt.model.tagMap[word]; !found {\n\t\t\ttag = pt.model.predict(featurize(i, context, word, p1, p2))\n\t\t}\n\t\ttokens[i].Tag = tag\n\t\tp2 = p1\n\t\tp1 = tag\n\t}\n\n\treturn tokens\n}\n\nfunc (m *averagedPerceptron) predict(features map[string]float64) string {\n\tvar weights map[string]float64\n\tvar found bool\n\n\tscores := make(map[string]float64)\n\tfor feat, value := range features {\n\t\tif weights, found = m.weights[feat]; !found || value == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tfor label, weight := range weights {\n\t\t\tscores[label] += value * weight\n\t\t}\n\t}\n\treturn max(scores)\n}\n\nfunc max(scores map[string]float64) string {\n\tvar class string\n\tmax := math.Inf(-1)\n\tfor label, value := range scores {\n\t\tif value > max {\n\t\t\tmax = value\n\t\t\tclass = label\n\t\t}\n\t}\n\treturn class\n}\n\nfunc featurize(i int, ctx []string, w, p1, p2 string) map[string]float64 {\n\tfeats := make(map[string]float64)\n\tsuf := min(len(w), 3)\n\ti = min(len(ctx)-2, i+2)\n\timinus := min(len(ctx[i-1]), 3)\n\tiplus := min(len(ctx[i+1]), 3)\n\tfeats = add([]string{\"bias\"}, feats)\n\tfeats = add([]string{\"i suffix\", w[len(w)-suf:]}, feats)\n\tfeats = add([]string{\"i pref1\", string(w[0])}, feats)\n\tfeats = add([]string{\"i-1 tag\", p1}, feats)\n\tfeats = add([]string{\"i-2 tag\", p2}, feats)\n\tfeats = add([]string{\"i tag+i-2 tag\", p1, p2}, feats)\n\tfeats = add([]string{\"i word\", ctx[i]}, feats)\n\tfeats = add([]string{\"i-1 tag+i word\", p1, ctx[i]}, feats)\n\tfeats = add([]string{\"i-1 word\", ctx[i-1]}, feats)\n\tfeats = add([]string{\"i-1 suffix\", ctx[i-1][len(ctx[i-1])-iminus:]}, feats)\n\tfeats = add([]string{\"i-2 word\", ctx[i-2]}, feats)\n\tfeats = add([]string{\"i+1 word\", ctx[i+1]}, feats)\n\tfeats = add([]string{\"i+1 suffix\", ctx[i+1][len(ctx[i+1])-iplus:]}, feats)\n\tfeats = add([]string{\"i+2 word\", ctx[i+2]}, feats)\n\treturn feats\n}\n\nfunc add(args []string, features map[string]float64) map[string]float64 {\n\tkey := strings.Join(args, \" \")\n\tfeatures[key]++\n\treturn features\n}\n\nfunc normalize(word string) string {\n\tif word == \"\" {\n\t\treturn word\n\t}\n\tfirst := string(word[0])\n\tif strings.Contains(word, \"-\") && first != \"-\" {\n\t\treturn \"!HYPHEN\"\n\t} else if _, err := strconv.Atoi(word); err == nil && len(word) == 4 {\n\t\treturn \"!YEAR\"\n\t} else if _, err := strconv.Atoi(first); err == nil {\n\t\treturn \"!DIGITS\"\n\t}\n\treturn strings.ToLower(word)\n}\n"
        },
        {
          "name": "tag_test.go",
          "type": "blob",
          "size": 3.13671875,
          "content": "package prose\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"testing\"\n)\n\nfunc makeTagger(text string) (*Document, error) {\n\treturn NewDocument(\n\t\ttext,\n\t\tWithSegmentation(false),\n\t\tWithExtraction(false))\n}\n\nfunc ExampleReadTagged() {\n\ttagged := \"Pierre|NNP Vinken|NNP ,|, 61|CD years|NNS\"\n\tfmt.Println(ReadTagged(tagged, \"|\"))\n\t// Output: [[[Pierre Vinken , 61 years] [NNP NNP , CD NNS]]]\n}\n\nfunc TestTagSimple(t *testing.T) {\n\tdoc, err := makeTagger(\"Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.\")\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\ttags := []string{}\n\tfor _, tok := range doc.Tokens() {\n\t\ttags = append(tags, tok.Tag)\n\t}\n\tif !reflect.DeepEqual([]string{\n\t\t\"NNP\", \"NNP\", \",\", \"CD\", \"NNS\", \"JJ\", \",\", \"MD\", \"VB\", \"DT\", \"NN\",\n\t\t\"IN\", \"DT\", \"JJ\", \"NN\", \"NNP\", \"CD\", \".\"}, tags) {\n\t\tt.Errorf(\"TagSimple() got = %v\", tags)\n\t}\n}\n\nfunc TestTagTreebank(t *testing.T) {\n\ttagger := newPerceptronTagger()\n\ttokens, expected := []*Token{}, []string{}\n\n\ttags := readDataFile(filepath.Join(testdata, \"treebank_tags.json\"))\n\tcheckError(json.Unmarshal(tags, &expected))\n\n\ttreebank := readDataFile(filepath.Join(testdata, \"treebank_tokens.json\"))\n\tcheckError(json.Unmarshal(treebank, &tokens))\n\n\tcorrect := 0.0\n\tfor i, tok := range tagger.tag(tokens) {\n\t\tif expected[i] == tok.Tag {\n\t\t\tcorrect++\n\t\t}\n\t}\n\n\tv := correct / float64(len(expected))\n\tif v < 0.957477 {\n\t\tt.Errorf(\"TagTreebank() expected >= 0.957477, got = %v\", v)\n\t}\n}\n\nfunc BenchmarkTag(b *testing.B) {\n\ttagger := newPerceptronTagger()\n\ttokens := []*Token{}\n\n\ttreebank := readDataFile(filepath.Join(testdata, \"treebank_tokens.json\"))\n\tcheckError(json.Unmarshal(treebank, &tokens))\n\tfor n := 0; n < b.N; n++ {\n\t\t_ = tagger.tag(tokens)\n\t}\n}\n\n/* TODO: POS training API\n\nvar wsj = \"Pierre|NNP Vinken|NNP ,|, 61|CD years|NNS old|JJ ,|, will|MD \" +\n\t\"join|VB the|DT board|NN as|IN a|DT nonexecutive|JJ director|NN \" +\n\t\"Nov.|NNP 29|CD .|.\\nMr.|NNP Vinken|NNP is|VBZ chairman|NN of|IN \" +\n\t\"Elsevier|NNP N.V.|NNP ,|, the|DT Dutch|NNP publishing|VBG \" +\n\t\"group|NN .|. Rudolph|NNP Agnew|NNP ,|, 55|CD years|NNS old|JJ \" +\n\t\"and|CC former|JJ chairman|NN of|IN Consolidated|NNP Gold|NNP \" +\n\t\"Fields|NNP PLC|NNP ,|, was|VBD named|VBN a|DT nonexecutive|JJ \" +\n\t\"director|NN of|IN this|DT British|JJ industrial|JJ conglomerate|NN \" +\n\t\".|.\\nA|DT form|NN of|IN asbestos|NN once|RB used|VBN to|TO make|VB \" +\n\t\"Kent|NNP cigarette|NN filters|NNS has|VBZ caused|VBN a|DT high|JJ \" +\n\t\"percentage|NN of|IN cancer|NN deaths|NNS among|IN a|DT group|NN \" +\n\t\"of|IN workers|NNS exposed|VBN to|TO it|PRP more|RBR than|IN \" +\n\t\"30|CD years|NNS ago|IN ,|, researchers|NNS reported|VBD .|.\"\n\nfunc TestTrain(t *testing.T) {\n\tsentences := ReadTagged(wsj, \"|\")\n\titer := random(5, 20)\n\ttagger.Train(sentences, iter)\n\n\ttagSet := []string{}\n\tnrWords := 0\n\tfor _, tuple := range sentences {\n\t\tnrWords += len(tuple[0])\n\t\tfor _, tag := range tuple[1] {\n\t\t\tif !util.StringInSlice(tag, tagSet) {\n\t\t\t\ttagSet = append(tagSet, tag)\n\t\t\t}\n\t\t}\n\t}\n\n\tassert.Equal(t, nrWords*iter, int(tagger.model.instances))\n\tassert.Subset(t, tagger.Classes(), tagSet)\n}\n\nfunc random(min, max int) int {\n\trand.Seed(time.Now().Unix())\n\treturn rand.Intn(max-min) + min\n}*/\n"
        },
        {
          "name": "testdata",
          "type": "tree",
          "content": null
        },
        {
          "name": "tokenize.go",
          "type": "blob",
          "size": 6.2470703125,
          "content": "package prose\n\nimport (\n\t\"regexp\"\n\t\"strings\"\n\t\"unicode\"\n\t\"unicode/utf8\"\n)\n\ntype TokenTester func(string) bool\n\ntype Tokenizer interface {\n\tTokenize(string) []*Token\n}\n\n// iterTokenizer splits a sentence into words.\ntype iterTokenizer struct {\n\tspecialRE      *regexp.Regexp\n\tsanitizer      *strings.Replacer\n\tcontractions   []string\n\tsplitCases     []string\n\tsuffixes       []string\n\tprefixes       []string\n\temoticons      map[string]int\n\tisUnsplittable TokenTester\n}\n\ntype TokenizerOptFunc func(*iterTokenizer)\n\n// UsingIsUnsplittableFN gives a function that tests whether a token is splittable or not.\nfunc UsingIsUnsplittable(x TokenTester) TokenizerOptFunc {\n\treturn func(tokenizer *iterTokenizer) {\n\t\ttokenizer.isUnsplittable = x\n\t}\n}\n\n// Use the provided special regex for unsplittable tokens.\nfunc UsingSpecialRE(x *regexp.Regexp) TokenizerOptFunc {\n\treturn func(tokenizer *iterTokenizer) {\n\t\ttokenizer.specialRE = x\n\t}\n}\n\n// Use the provided sanitizer.\nfunc UsingSanitizer(x *strings.Replacer) TokenizerOptFunc {\n\treturn func(tokenizer *iterTokenizer) {\n\t\ttokenizer.sanitizer = x\n\t}\n}\n\n// Use the provided suffixes.\nfunc UsingSuffixes(x []string) TokenizerOptFunc {\n\treturn func(tokenizer *iterTokenizer) {\n\t\ttokenizer.suffixes = x\n\t}\n}\n\n// Use the provided prefixes.\nfunc UsingPrefixes(x []string) TokenizerOptFunc {\n\treturn func(tokenizer *iterTokenizer) {\n\t\ttokenizer.prefixes = x\n\t}\n}\n\n// Use the provided map of emoticons.\nfunc UsingEmoticons(x map[string]int) TokenizerOptFunc {\n\treturn func(tokenizer *iterTokenizer) {\n\t\ttokenizer.emoticons = x\n\t}\n}\n\n// Use the provided contractions.\nfunc UsingContractions(x []string) TokenizerOptFunc {\n\treturn func(tokenizer *iterTokenizer) {\n\t\ttokenizer.contractions = x\n\t}\n}\n\n// Use the provided splitCases.\nfunc UsingSplitCases(x []string) TokenizerOptFunc {\n\treturn func(tokenizer *iterTokenizer) {\n\t\ttokenizer.splitCases = x\n\t}\n}\n\n// Constructor for default iterTokenizer\nfunc NewIterTokenizer(opts ...TokenizerOptFunc) *iterTokenizer {\n\ttok := new(iterTokenizer)\n\n\t// Set default parameters\n\ttok.contractions = contractions\n\ttok.emoticons = emoticons\n\ttok.isUnsplittable = func(_ string) bool { return false }\n\ttok.prefixes = prefixes\n\ttok.sanitizer = sanitizer\n\ttok.specialRE = internalRE\n\ttok.suffixes = suffixes\n\n\t// Apply options if provided\n\tfor _, applyOpt := range opts {\n\t\tapplyOpt(tok)\n\t}\n\n\ttok.splitCases = append(tok.splitCases, tok.contractions...)\n\n\treturn tok\n}\n\nfunc addToken(s string, toks []*Token) []*Token {\n\tif strings.TrimSpace(s) != \"\" {\n\t\ttoks = append(toks, &Token{Text: s})\n\t}\n\treturn toks\n}\n\nfunc (t *iterTokenizer) isSpecial(token string) bool {\n\t_, found := t.emoticons[token]\n\treturn found || t.specialRE.MatchString(token) || t.isUnsplittable(token)\n}\n\nfunc (t *iterTokenizer) doSplit(token string) []*Token {\n\ttokens := []*Token{}\n\tsuffs := []*Token{}\n\n\tlast := 0\n\tfor token != \"\" && utf8.RuneCountInString(token) != last {\n\t\tif t.isSpecial(token) {\n\t\t\t// We've found a special case (e.g., an emoticon) -- so, we add it as a token without\n\t\t\t// any further processing.\n\t\t\ttokens = addToken(token, tokens)\n\t\t\tbreak\n\t\t}\n\t\tlast = utf8.RuneCountInString(token)\n\t\tlower := strings.ToLower(token)\n\t\tif hasAnyPrefix(token, t.prefixes) {\n\t\t\t// Remove prefixes -- e.g., $100 -> [$, 100].\n\t\t\ttokens = addToken(string(token[0]), tokens)\n\t\t\ttoken = token[1:]\n\t\t} else if idx := hasAnyIndex(lower, t.splitCases); idx > -1 {\n\t\t\t// Handle \"they'll\", \"I'll\", \"Don't\", \"won't\", amount($).\n\t\t\t//\n\t\t\t// they'll -> [they, 'll].\n\t\t\t// don't -> [do, n't].\n\t\t\t// amount($) -> [amount, (, $, )].\n\t\t\ttokens = addToken(token[:idx], tokens)\n\t\t\ttoken = token[idx:]\n\t\t} else if hasAnySuffix(token, t.suffixes) {\n\t\t\t// Remove suffixes -- e.g., Well) -> [Well, )].\n\t\t\tsuffs = append([]*Token{\n\t\t\t\t{Text: string(token[len(token)-1])}},\n\t\t\t\tsuffs...)\n\t\t\ttoken = token[:len(token)-1]\n\t\t} else {\n\t\t\ttokens = addToken(token, tokens)\n\t\t}\n\t}\n\n\treturn append(tokens, suffs...)\n}\n\n// tokenize splits a sentence into a slice of words.\nfunc (t *iterTokenizer) Tokenize(text string) []*Token {\n\tvar tokens []*Token\n\n\tclean, white := t.sanitizer.Replace(text), false\n\tlength := len(clean)\n\n\tstart, index := 0, 0\n\tcache := map[string][]*Token{}\n\tfor index <= length {\n\t\tuc, size := utf8.DecodeRuneInString(clean[index:])\n\t\tif size == 0 {\n\t\t\tbreak\n\t\t} else if index == 0 {\n\t\t\twhite = unicode.IsSpace(uc)\n\t\t}\n\t\tif unicode.IsSpace(uc) != white {\n\t\t\tif start < index {\n\t\t\t\tspan := clean[start:index]\n\t\t\t\tif toks, found := cache[span]; found {\n\t\t\t\t\ttokens = append(tokens, toks...)\n\t\t\t\t} else {\n\t\t\t\t\ttoks := t.doSplit(span)\n\t\t\t\t\tcache[span] = toks\n\t\t\t\t\ttokens = append(tokens, toks...)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif uc == ' ' {\n\t\t\t\tstart = index + 1\n\t\t\t} else {\n\t\t\t\tstart = index\n\t\t\t}\n\t\t\twhite = !white\n\t\t}\n\t\tindex += size\n\t}\n\n\tif start < index {\n\t\ttokens = append(tokens, t.doSplit(clean[start:index])...)\n\t}\n\n\treturn tokens\n}\n\nvar internalRE = regexp.MustCompile(`^(?:[A-Za-z]\\.){2,}$|^[A-Z][a-z]{1,2}\\.$`)\nvar sanitizer = strings.NewReplacer(\n\t\"\\u201c\", `\"`,\n\t\"\\u201d\", `\"`,\n\t\"\\u2018\", \"'\",\n\t\"\\u2019\", \"'\",\n\t\"&rsquo;\", \"'\")\nvar contractions = []string{\"'ll\", \"'s\", \"'re\", \"'m\", \"n't\"}\nvar suffixes = []string{\",\", \")\", `\"`, \"]\", \"!\", \";\", \".\", \"?\", \":\", \"'\"}\nvar prefixes = []string{\"$\", \"(\", `\"`, \"[\"}\nvar emoticons = map[string]int{\n\t\"(-8\":         1,\n\t\"(-;\":         1,\n\t\"(-_-)\":       1,\n\t\"(._.)\":       1,\n\t\"(:\":          1,\n\t\"(=\":          1,\n\t\"(o:\":         1,\n\t\"(¬_¬)\":       1,\n\t\"(ಠ_ಠ)\":       1,\n\t\"(╯°□°）╯︵┻━┻\": 1,\n\t\"-__-\":        1,\n\t\"8-)\":         1,\n\t\"8-D\":         1,\n\t\"8D\":          1,\n\t\":(\":          1,\n\t\":((\":         1,\n\t\":(((\":        1,\n\t\":()\":         1,\n\t\":)))\":        1,\n\t\":-)\":         1,\n\t\":-))\":        1,\n\t\":-)))\":       1,\n\t\":-*\":         1,\n\t\":-/\":         1,\n\t\":-X\":         1,\n\t\":-]\":         1,\n\t\":-o\":         1,\n\t\":-p\":         1,\n\t\":-x\":         1,\n\t\":-|\":         1,\n\t\":-}\":         1,\n\t\":0\":          1,\n\t\":3\":          1,\n\t\":P\":          1,\n\t\":]\":          1,\n\t\":`(\":         1,\n\t\":`)\":         1,\n\t\":`-(\":        1,\n\t\":o\":          1,\n\t\":o)\":         1,\n\t\"=(\":          1,\n\t\"=)\":          1,\n\t\"=D\":          1,\n\t\"=|\":          1,\n\t\"@_@\":         1,\n\t\"O.o\":         1,\n\t\"O_o\":         1,\n\t\"V_V\":         1,\n\t\"XDD\":         1,\n\t\"[-:\":         1,\n\t\"^___^\":       1,\n\t\"o_0\":         1,\n\t\"o_O\":         1,\n\t\"o_o\":         1,\n\t\"v_v\":         1,\n\t\"xD\":          1,\n\t\"xDD\":         1,\n\t\"¯\\\\(ツ)/¯\":    1,\n}\n"
        },
        {
          "name": "tokenize_test.go",
          "type": "blob",
          "size": 9.5908203125,
          "content": "package prose\n\nimport (\n\t\"encoding/json\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"testing\"\n)\n\nvar testdata = \"testdata\"\n\nfunc checkTokens(t *testing.T, tokens []*Token, expected []string, name string) {\n\tobserved := []string{}\n\tfor i := range tokens {\n\t\tobserved = append(observed, tokens[i].Text)\n\t}\n\tif !reflect.DeepEqual(observed, expected) {\n\t\tt.Errorf(\"%v: unexpected tokens\", name)\n\t}\n}\n\nfunc checkCase(t *testing.T, doc *Document, expected []string, name string) {\n\ttokens := getTokenText(doc)\n\tif !reflect.DeepEqual(tokens, expected) {\n\t\tt.Errorf(\"%v: unexpected tokens\", name)\n\t}\n}\n\nfunc makeDoc(text string) (*Document, error) {\n\treturn NewDocument(\n\t\ttext,\n\t\tWithSegmentation(false),\n\t\tWithTagging(false),\n\t\tWithExtraction(false))\n}\n\nfunc getTokenText(doc *Document) []string {\n\tobserved := []string{}\n\ttokens := doc.Tokens()\n\tfor i := range tokens {\n\t\tobserved = append(observed, tokens[i].Text)\n\t}\n\treturn observed\n}\n\nfunc getWordData(file string) ([]string, [][]string) {\n\tin := readDataFile(filepath.Join(testdata, \"treebank_sents.json\"))\n\tout := readDataFile(filepath.Join(testdata, file))\n\n\tinput := []string{}\n\toutput := [][]string{}\n\n\tcheckError(json.Unmarshal(in, &input))\n\tcheckError(json.Unmarshal(out, &output))\n\n\treturn input, output\n}\n\nfunc getWordBenchData() []string {\n\tin := readDataFile(filepath.Join(testdata, \"treebank_sents.json\"))\n\tinput := []string{}\n\tcheckError(json.Unmarshal(in, &input))\n\treturn input\n}\n\nfunc TestTokenizationEmpty(t *testing.T) {\n\tdoc, _ := makeDoc(\"\")\n\n\tl := len(getTokenText(doc))\n\tif l != 0 {\n\t\tt.Errorf(\"TokenizationEmpty() expected = 0, got = %v\", l)\n\t}\n}\n\nfunc TestTokenizationSimple(t *testing.T) {\n\tdoc, _ := makeDoc(\"Vale is a natural language linter that supports plain text, markup (Markdown, reStructuredText, AsciiDoc, and HTML), and source code comments. Vale doesn't attempt to offer a one-size-fits-all collection of rules—instead, it strives to make customization as easy as possible.\")\n\texpected := []string{\n\t\t\"Vale\", \"is\", \"a\", \"natural\", \"language\", \"linter\", \"that\", \"supports\",\n\t\t\"plain\", \"text\", \",\", \"markup\", \"(\", \"Markdown\", \",\", \"reStructuredText\",\n\t\t\",\", \"AsciiDoc\", \",\", \"and\", \"HTML\", \")\", \",\", \"and\", \"source\",\n\t\t\"code\", \"comments\", \".\", \"Vale\", \"does\", \"n't\", \"attempt\", \"to\",\n\t\t\"offer\", \"a\", \"one-size-fits-all\", \"collection\", \"of\", \"rules—instead\",\n\t\t\",\", \"it\", \"strives\", \"to\", \"make\", \"customization\", \"as\", \"easy\", \"as\",\n\t\t\"possible\", \".\"}\n\tcheckCase(t, doc, expected, \"TokenizationSimple()\")\n}\n\nfunc TestTokenizationTreebank(t *testing.T) {\n\tinput, output := getWordData(\"treebank_words.json\")\n\tfor i, s := range input {\n\t\tdoc, _ := makeDoc(s)\n\t\ttokens := getTokenText(doc)\n\t\tif !reflect.DeepEqual(tokens, output[i]) {\n\t\t\tt.Errorf(\"TokenizationTreebank(): unexpected tokens\")\n\t\t}\n\t}\n}\n\nfunc TestTokenizationWeb(t *testing.T) {\n\tweb := `Independent of current body composition, IGF-I levels at 5 yr were significantly\n            associated with rate of weight gain between 0-2 yr (beta=0.19; P&lt;0.0005);\n            and children who showed postnatal catch-up growth (i.e. those who showed gains in\n            weight or length between 0-2 yr by >0.67 SD score) had higher IGF-I levels than other\n\t\t\tchildren (P=0.02; http://univ.edu.es/study.html) [20-22].`\n\texpected := []string{\"Independent\", \"of\", \"current\", \"body\", \"composition\", \",\", \"IGF-I\",\n\t\t\"levels\", \"at\", \"5\", \"yr\", \"were\", \"significantly\", \"associated\", \"with\", \"rate\", \"of\",\n\t\t\"weight\", \"gain\", \"between\", \"0-2\", \"yr\", \"(\", \"beta=0.19\", \";\", \"P&lt;0.0005\", \")\", \";\",\n\t\t\"and\", \"children\", \"who\", \"showed\", \"postnatal\", \"catch-up\", \"growth\", \"(\", \"i.e.\", \"those\",\n\t\t\"who\", \"showed\", \"gains\", \"in\", \"weight\", \"or\", \"length\", \"between\", \"0-2\", \"yr\", \"by\",\n\t\t\">0.67\", \"SD\", \"score\", \")\", \"had\", \"higher\", \"IGF-I\", \"levels\", \"than\", \"other\", \"children\",\n\t\t\"(\", \"P=0.02\", \";\", \"http://univ.edu.es/study.html\", \")\", \"[\", \"20-22\", \"]\", \".\"}\n\tdoc, _ := makeDoc(web)\n\tcheckCase(t, doc, expected, \"TokenizationWeb()\")\n}\n\nfunc TestTokenizationWebParagraph(t *testing.T) {\n\tweb := `Independent of current body composition, IGF-I levels at 5 yr were significantly\n            associated with rate of weight gain between 0-2 yr (beta=0.19; P&lt;0.0005);\n            and children who showed postnatal catch-up growth (i.e. those who showed gains in\n            weight or length between 0-2 yr by >0.67 SD score) had higher IGF-I levels than other\n\t\t\tchildren (P=0.02; http://univ.edu.es/study.html) [20-22].\n\n\t\t\tIndependent of current body composition, IGF-I levels at 5 yr were significantly\n            associated with rate of weight gain between 0-2 yr (beta=0.19; P&lt;0.0005);\n            and children who showed postnatal catch-up growth (i.e. those who showed gains in\n            weight or length between 0-2 yr by >0.67 SD score) had higher IGF-I levels than other\n\t\t\tchildren (P=0.02; http://univ.edu.es/study.html) [20-22].\n\n\t\t\tIndependent of current body composition, IGF-I levels at 5 yr were significantly\n            associated with rate of weight gain between 0-2 yr (beta=0.19; P&lt;0.0005);\n            and children who showed postnatal catch-up growth (i.e. those who showed gains in\n            weight or length between 0-2 yr by >0.67 SD score) had higher IGF-I levels than other\n\t\t\tchildren (P=0.02; http://univ.edu.es/study.html) [20-22].`\n\n\texpected := []string{\"Independent\", \"of\", \"current\", \"body\", \"composition\", \",\", \"IGF-I\",\n\t\t\"levels\", \"at\", \"5\", \"yr\", \"were\", \"significantly\", \"associated\", \"with\", \"rate\", \"of\",\n\t\t\"weight\", \"gain\", \"between\", \"0-2\", \"yr\", \"(\", \"beta=0.19\", \";\", \"P&lt;0.0005\", \")\", \";\",\n\t\t\"and\", \"children\", \"who\", \"showed\", \"postnatal\", \"catch-up\", \"growth\", \"(\", \"i.e.\", \"those\",\n\t\t\"who\", \"showed\", \"gains\", \"in\", \"weight\", \"or\", \"length\", \"between\", \"0-2\", \"yr\", \"by\",\n\t\t\">0.67\", \"SD\", \"score\", \")\", \"had\", \"higher\", \"IGF-I\", \"levels\", \"than\", \"other\", \"children\",\n\t\t\"(\", \"P=0.02\", \";\", \"http://univ.edu.es/study.html\", \")\", \"[\", \"20-22\", \"]\", \".\", \"Independent\", \"of\", \"current\", \"body\", \"composition\", \",\", \"IGF-I\",\n\t\t\"levels\", \"at\", \"5\", \"yr\", \"were\", \"significantly\", \"associated\", \"with\", \"rate\", \"of\",\n\t\t\"weight\", \"gain\", \"between\", \"0-2\", \"yr\", \"(\", \"beta=0.19\", \";\", \"P&lt;0.0005\", \")\", \";\",\n\t\t\"and\", \"children\", \"who\", \"showed\", \"postnatal\", \"catch-up\", \"growth\", \"(\", \"i.e.\", \"those\",\n\t\t\"who\", \"showed\", \"gains\", \"in\", \"weight\", \"or\", \"length\", \"between\", \"0-2\", \"yr\", \"by\",\n\t\t\">0.67\", \"SD\", \"score\", \")\", \"had\", \"higher\", \"IGF-I\", \"levels\", \"than\", \"other\", \"children\",\n\t\t\"(\", \"P=0.02\", \";\", \"http://univ.edu.es/study.html\", \")\", \"[\", \"20-22\", \"]\", \".\", \"Independent\", \"of\", \"current\", \"body\", \"composition\", \",\", \"IGF-I\",\n\t\t\"levels\", \"at\", \"5\", \"yr\", \"were\", \"significantly\", \"associated\", \"with\", \"rate\", \"of\",\n\t\t\"weight\", \"gain\", \"between\", \"0-2\", \"yr\", \"(\", \"beta=0.19\", \";\", \"P&lt;0.0005\", \")\", \";\",\n\t\t\"and\", \"children\", \"who\", \"showed\", \"postnatal\", \"catch-up\", \"growth\", \"(\", \"i.e.\", \"those\",\n\t\t\"who\", \"showed\", \"gains\", \"in\", \"weight\", \"or\", \"length\", \"between\", \"0-2\", \"yr\", \"by\",\n\t\t\">0.67\", \"SD\", \"score\", \")\", \"had\", \"higher\", \"IGF-I\", \"levels\", \"than\", \"other\", \"children\",\n\t\t\"(\", \"P=0.02\", \";\", \"http://univ.edu.es/study.html\", \")\", \"[\", \"20-22\", \"]\", \".\"}\n\n\tdoc, _ := makeDoc(web)\n\tcheckCase(t, doc, expected, \"TokenizationWebParagraph()\")\n}\n\nfunc TestTokenizationTwitter(t *testing.T) {\n\tdoc, _ := makeDoc(\"@twitter, what time does it start :-)\")\n\texpected := []string{\"@twitter\", \",\", \"what\", \"time\", \"does\", \"it\", \"start\", \":-)\"}\n\tcheckCase(t, doc, expected, \"TokenizationWebParagraph(1)\")\n\n\tdoc, _ = makeDoc(\"Mr. James plays basketball in the N.B.A., do you?\")\n\texpected = []string{\n\t\t\"Mr.\", \"James\", \"plays\", \"basketball\", \"in\", \"the\", \"N.B.A.\", \",\",\n\t\t\"do\", \"you\", \"?\"}\n\tcheckCase(t, doc, expected, \"TokenizationWebParagraph(2)\")\n\n\tdoc, _ = makeDoc(\"ˌˌ kill the last letter\")\n\texpected = []string{\"ˌˌ\", \"kill\", \"the\", \"last\", \"letter\"}\n\tcheckCase(t, doc, expected, \"TokenizationWebParagraph(3)\")\n\n\tdoc, _ = makeDoc(\"ˌˌˌ kill the last letter\")\n\texpected = []string{\"ˌˌˌ\", \"kill\", \"the\", \"last\", \"letter\"}\n\tcheckCase(t, doc, expected, \"TokenizationWebParagraph(4)\")\n\n\tdoc, _ = makeDoc(\"March. July. March. June. January.\")\n\texpected = []string{\n\t\t\"March\", \".\", \"July\", \".\", \"March\", \".\", \"June\", \".\", \"January\", \".\"}\n\tcheckCase(t, doc, expected, \"TokenizationWebParagraph(5)\")\n}\n\nfunc TestTokenizationSplitCases(t *testing.T) {\n\ttokenizer := NewIterTokenizer(UsingSplitCases([]string{\"(\"}))\n\ttokens := tokenizer.Tokenize(\"amount($)\")\n\texpected := []string{\"amount\", \"(\", \"$\", \")\"}\n\tcheckTokens(t, tokens, expected, \"TokenizationSplitCases(custom-found)\")\n}\n\nfunc TestTokenizationContractions(t *testing.T) {\n\ttokenizer := NewIterTokenizer()\n\ttokens := tokenizer.Tokenize(\"He's happy\")\n\texpected := []string{\"He\", \"'s\", \"happy\"}\n\tcheckTokens(t, tokens, expected, \"TokenizationContraction(default-found)\")\n\n\ttokens = tokenizer.Tokenize(\"I've been better\")\n\texpected = []string{\"I've\", \"been\", \"better\"}\n\tcheckTokens(t, tokens, expected, \"TokenizationContraction(default-missing)\")\n\n\ttokenizer = NewIterTokenizer(UsingContractions([]string{\"'ve\"}))\n\ttokens = tokenizer.Tokenize(\"I've been better\")\n\texpected = []string{\"I\", \"'ve\", \"been\", \"better\"}\n\tcheckTokens(t, tokens, expected, \"TokenizationContraction(custom-found)\")\n\n\ttokens = tokenizer.Tokenize(\"He's happy\")\n\texpected = []string{\"He's\", \"happy\"}\n\tcheckTokens(t, tokens, expected, \"TokenizationContraction(custom-missing)\")\n}\n\nfunc BenchmarkTokenization(b *testing.B) {\n\tin := readDataFile(filepath.Join(testdata, \"sherlock.txt\"))\n\ttext := string(in)\n\tfor n := 0; n < b.N; n++ {\n\t\t_, err := makeDoc(text)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n}\n\nfunc BenchmarkTokenizationSimple(b *testing.B) {\n\tfor n := 0; n < b.N; n++ {\n\t\tfor _, s := range getWordBenchData() {\n\t\t\t_, err := makeDoc(s)\n\t\t\tif err != nil {\n\t\t\t\tpanic(err)\n\t\t\t}\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "types.go",
          "type": "blob",
          "size": 0.525390625,
          "content": "package prose\n\n// A Token represents an individual token of text such as a word or punctuation\n// symbol.\ntype Token struct {\n\tTag   string // The token's part-of-speech tag.\n\tText  string // The token's actual content.\n\tLabel string // The token's IOB label.\n}\n\n// An Entity represents an individual named-entity.\ntype Entity struct {\n\tText  string // The entity's actual content.\n\tLabel string // The entity's label.\n}\n\n// A Sentence represents a segmented portion of text.\ntype Sentence struct {\n\tText string // The sentence's text.\n}\n"
        },
        {
          "name": "utilities.go",
          "type": "blob",
          "size": 1.77734375,
          "content": "package prose\n\nimport (\n\t\"bytes\"\n\t\"encoding/gob\"\n\t\"io/fs\"\n\t\"path\"\n\t\"strconv\"\n\t\"strings\"\n)\n\n// checkError panics if `err` is not `nil`.\nfunc checkError(err error) {\n\tif err != nil {\n\t\tpanic(err)\n\t}\n}\n\n// min returns the minimum of `a` and `b`.\nfunc min(a, b int) int {\n\tif a < b {\n\t\treturn a\n\t}\n\treturn b\n}\n\n// isPunct determines if the string represents a number.\nfunc isNumeric(s string) bool {\n\t_, err := strconv.ParseFloat(s, 64)\n\treturn err == nil\n}\n\n// stringInSlice determines if `slice` contains the string `a`.\nfunc stringInSlice(a string, slice []string) bool {\n\tfor _, b := range slice {\n\t\tif a == b {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc getAsset(folder, name string) *gob.Decoder {\n\tb, err := Asset(path.Join(\"model\", folder, name))\n\tcheckError(err)\n\treturn gob.NewDecoder(bytes.NewReader(b))\n}\n\nfunc getDiskAsset(file fs.File) *gob.Decoder {\n\treturn gob.NewDecoder(file)\n}\n\nfunc hasAnyPrefix(s string, prefixes []string) bool {\n\tn := len(s)\n\tfor _, prefix := range prefixes {\n\t\tif n > len(prefix) && strings.HasPrefix(s, prefix) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc hasAnySuffix(s string, suffixes []string) bool {\n\tn := len(s)\n\tfor _, suffix := range suffixes {\n\t\tif n > len(suffix) && strings.HasSuffix(s, suffix) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc hasAnyIndex(s string, suffixes []string) int {\n\tn := len(s)\n\tfor _, suffix := range suffixes {\n\t\tidx := strings.Index(s, suffix)\n\t\tif idx >= 0 && n > len(suffix) {\n\t\t\treturn idx\n\t\t}\n\t}\n\treturn -1\n}\n\nfunc nSuffix(word string, length int) string {\n\treturn strings.ToLower(word[len(word)-min(len(word), length):])\n}\n\nfunc nPrefix(word string, length int) string {\n\treturn strings.ToLower(word[:min(len(word), length)])\n}\n\nfunc isBasic(word string) string {\n\tif stringInSlice(word, enWordList) {\n\t\treturn \"True\"\n\t}\n\treturn \"False\"\n}\n"
        },
        {
          "name": "words.go",
          "type": "blob",
          "size": 7.77734375,
          "content": "package prose\n\nvar enWordList = []string{\n\t\"open\", \"shame\", \"balance\", \"distance\", \"fish\", \"tray\", \"or\", \"verse\",\n\t\"look\", \"twist\", \"teaching\", \"even\", \"hate\", \"pot\", \"left\", \"hole\", \"ice\",\n\t\"sex\", \"leaf\", \"ready\", \"strong\", \"tree\", \"public\", \"parcel\", \"market\",\n\t\"ink\", \"near\", \"neck\", \"drain\", \"necessary\", \"automatic\", \"art\", \"hook\",\n\t\"suggestion\", \"off\", \"condition\", \"distribution\", \"any\", \"solid\",\n\t\"increase\", \"political\", \"approval\", \"bright\", \"yellow\", \"effect\", \"give\",\n\t\"current\", \"cushion\", \"stick\", \"he\", \"spoon\", \"put\", \"friend\", \"grey\",\n\t\"back\", \"chest\", \"snow\", \"stretch\", \"hospital\", \"hair\", \"across\", \"horn\",\n\t\"pig\", \"knowledge\", \"poison\", \"kind\", \"burn\", \"lead\", \"roll\", \"paste\",\n\t\"history\", \"representative\", \"thumb\", \"use\", \"elastic\", \"awake\", \"here\",\n\t\"wet\", \"fiction\", \"land\", \"bent\", \"regular\", \"but\", \"blade\", \"answer\",\n\t\"sweet\", \"business\", \"moon\", \"owner\", \"surprise\", \"hanging\", \"wool\",\n\t\"form\", \"thing\", \"porter\", \"dust\", \"fold\", \"nation\", \"vessel\", \"edge\",\n\t\"poor\", \"science\", \"earth\", \"guide\", \"song\", \"manager\", \"authority\",\n\t\"system\", \"adjustment\", \"belief\", \"protest\", \"detail\", \"burst\", \"metal\",\n\t\"such\", \"brother\", \"news\", \"food\", \"change\", \"heat\", \"tired\", \"where\",\n\t\"yesterday\", \"apple\", \"wire\", \"safe\", \"engine\", \"company\", \"connection\",\n\t\"match\", \"soap\", \"story\", \"about\", \"there\", \"book\", \"wound\", \"false\",\n\t\"hammer\", \"receipt\", \"ever\", \"toe\", \"than\", \"mine\", \"loose\", \"true\", \"ant\",\n\t\"rail\", \"insurance\", \"frequent\", \"blow\", \"goat\", \"damage\", \"while\",\n\t\"record\", \"bitter\", \"after\", \"curve\", \"skirt\", \"stomach\", \"all\", \"wash\",\n\t\"almost\", \"of\", \"night\", \"winter\", \"memory\", \"lock\", \"high\", \"exchange\",\n\t\"degree\", \"growth\", \"basin\", \"basket\", \"living\", \"family\", \"quick\",\n\t\"chemical\", \"money\", \"space\", \"stiff\", \"school\", \"cold\", \"little\",\n\t\"orange\", \"middle\", \"south\", \"brush\", \"control\", \"strange\", \"nail\",\n\t\"competition\", \"flower\", \"start\", \"window\", \"statement\", \"observation\",\n\t\"curtain\", \"coat\", \"fixed\", \"pipe\", \"sister\", \"send\", \"prose\", \"leg\",\n\t\"water\", \"bag\", \"face\", \"list\", \"amusement\", \"stitch\", \"structure\", \"milk\",\n\t\"board\", \"iron\", \"so\", \"loud\", \"not\", \"hour\", \"square\", \"view\", \"sense\",\n\t\"stop\", \"that\", \"farm\", \"ball\", \"education\", \"fruit\", \"war\", \"full\",\n\t\"quite\", \"danger\", \"green\", \"soft\", \"talk\", \"profit\", \"if\", \"from\", \"wing\",\n\t\"right\", \"bath\", \"government\", \"a\", \"instrument\", \"stamp\", \"parallel\",\n\t\"disgust\", \"knot\", \"bird\", \"comfort\", \"key\", \"digestion\", \"hard\", \"wide\",\n\t\"steam\", \"test\", \"rat\", \"late\", \"bridge\", \"salt\", \"rest\", \"band\", \"make\",\n\t\"general\", \"animal\", \"physical\", \"paper\", \"error\", \"to\", \"natural\",\n\t\"harbour\", \"help\", \"dependent\", \"hollow\", \"bulb\", \"circle\", \"country\",\n\t\"like\", \"writing\", \"impulse\", \"year\", \"copper\", \"this\", \"picture\", \"voice\",\n\t\"society\", \"shock\", \"month\", \"road\", \"girl\", \"healthy\", \"potato\", \"cry\",\n\t\"tax\", \"sheep\", \"size\", \"swim\", \"fear\", \"boot\", \"silk\", \"because\",\n\t\"common\", \"island\", \"language\", \"some\", \"debt\", \"room\", \"order\", \"well\",\n\t\"jump\", \"soup\", \"sleep\", \"ornament\", \"comb\", \"feather\", \"cart\", \"cause\",\n\t\"every\", \"push\", \"between\", \"question\", \"tooth\", \"trick\", \"rate\", \"egg\",\n\t\"quality\", \"sharp\", \"canvas\", \"bad\", \"servant\", \"happy\", \"judge\", \"pin\",\n\t\"dead\", \"noise\", \"theory\", \"garden\", \"idea\", \"pain\", \"driving\", \"meat\",\n\t\"name\", \"low\", \"star\", \"wood\", \"light\", \"base\", \"cheap\", \"opinion\",\n\t\"relation\", \"stone\", \"flight\", \"pen\", \"map\", \"sea\", \"rhythm\", \"and\", \"air\",\n\t\"wheel\", \"able\", \"time\", \"bite\", \"certain\", \"at\", \"wise\", \"death\", \"jewel\",\n\t\"head\", \"last\", \"mouth\", \"spade\", \"breath\", \"sail\", \"lip\", \"bone\", \"only\",\n\t\"simple\", \"private\", \"net\", \"body\", \"powder\", \"kiss\", \"sky\", \"meeting\",\n\t\"limit\", \"cheese\", \"sign\", \"come\", \"blue\", \"measure\", \"rub\", \"slip\", \"sad\",\n\t\"cork\", \"drawer\", \"cake\", \"level\", \"self\", \"tin\", \"chalk\", \"button\",\n\t\"electric\", \"finger\", \"serious\", \"plate\", \"through\", \"west\", \"weight\",\n\t\"payment\", \"frame\", \"hat\", \"short\", \"acid\", \"arch\", \"hand\", \"reaction\",\n\t\"smile\", \"account\", \"experience\", \"get\", \"attention\", \"fly\", \"blood\",\n\t\"dress\", \"field\", \"steel\", \"fork\", \"prison\", \"thin\", \"trade\", \"pull\",\n\t\"event\", \"seem\", \"cover\", \"see\", \"music\", \"ray\", \"brass\", \"hope\", \"top\",\n\t\"industry\", \"taste\", \"whistle\", \"berry\", \"worm\", \"sponge\", \"beautiful\",\n\t\"monkey\", \"as\", \"week\", \"drink\", \"flat\", \"cow\", \"mist\", \"throat\", \"colour\",\n\t\"great\", \"nerve\", \"reason\", \"waste\", \"amount\", \"army\", \"man\", \"house\",\n\t\"umbrella\", \"for\", \"watch\", \"floor\", \"cord\", \"step\", \"invention\",\n\t\"machine\", \"different\", \"against\", \"jelly\", \"go\", \"bucket\", \"mark\",\n\t\"shelf\", \"kick\", \"price\", \"library\", \"range\", \"knee\", \"white\", \"regret\",\n\t\"sun\", \"may\", \"smell\", \"violent\", \"shade\", \"summer\", \"cook\", \"position\",\n\t\"purpose\", \"cloud\", \"brake\", \"kettle\", \"division\", \"discovery\", \"tongue\",\n\t\"again\", \"touch\", \"probable\", \"drop\", \"copy\", \"letter\", \"dry\", \"gun\",\n\t\"learning\", \"need\", \"why\", \"polish\", \"father\", \"walk\", \"how\", \"boat\",\n\t\"ill\", \"way\", \"keep\", \"train\", \"much\", \"plough\", \"waiting\", \"wind\",\n\t\"expansion\", \"committee\", \"present\", \"tomorrow\", \"shake\", \"unit\",\n\t\"forward\", \"enough\", \"trousers\", \"north\", \"fight\", \"cloth\", \"crime\",\n\t\"insect\", \"rain\", \"place\", \"responsible\", \"sand\", \"second\", \"attempt\", \"decision\", \"leather\", \"married\", \"other\", \"seed\", \"dear\", \"building\", \"tall\", \"value\", \"with\", \"disease\", \"flag\", \"pump\", \"by\", \"front\", \"collar\", \"join\", \"advertisement\", \"fowl\", \"in\", \"now\", \"fat\", \"wall\", \"past\", \"morning\", \"broken\", \"thick\", \"bell\", \"then\", \"harmony\", \"knife\", \"together\", \"fall\", \"feeling\", \"print\", \"street\", \"day\", \"have\", \"expert\", \"important\", \"woman\", \"thread\", \"nose\", \"screw\", \"complete\", \"crush\", \"carriage\", \"smoke\", \"till\", \"clear\", \"plant\", \"process\", \"up\", \"angle\", \"transport\", \"stocking\", \"on\", \"ear\", \"will\", \"bed\", \"table\", \"cut\", \"foolish\", \"mountain\", \"motion\", \"substance\", \"spring\", \"scissors\", \"station\", \"eye\", \"horse\", \"boiling\", \"play\", \"grip\", \"root\", \"down\", \"property\", \"comparison\", \"I\", \"dog\", \"respect\", \"card\", \"do\", \"reading\", \"normal\", \"butter\", \"shut\", \"delicate\", \"cough\", \"who\", \"mass\", \"example\", \"sock\", \"brown\", \"no\", \"cup\", \"minute\", \"seat\", \"liquid\", \"smooth\", \"end\", \"humour\", \"discussion\", \"flame\", \"before\", \"over\", \"fact\", \"shirt\", \"still\", \"sneeze\", \"slow\", \"free\", \"request\", \"church\", \"act\", \"arm\", \"tendency\", \"bee\", \"foot\", \"operation\", \"reward\", \"agreement\", \"mother\", \"sound\", \"secretary\", \"note\", \"stage\", \"tight\", \"please\", \"town\", \"yes\", \"round\", \"under\", \"you\", \"doubt\", \"new\", \"lift\", \"special\", \"laugh\", \"door\", \"roof\", \"narrow\", \"rough\", \"turn\", \"feeble\", \"far\", \"nut\", \"gold\", \"shoe\", \"wax\", \"ring\", \"dirty\", \"equal\", \"number\", \"pencil\", \"grain\", \"ship\", \"fertile\", \"side\", \"selection\", \"loss\", \"say\", \"ticket\", \"move\", \"cruel\", \"mixed\", \"interest\", \"though\", \"glove\", \"point\", \"sort\", \"care\", \"young\", \"material\", \"oven\", \"sugar\", \"complex\", \"fire\", \"small\", \"separate\", \"son\", \"crack\", \"first\", \"office\", \"let\", \"brain\", \"straight\", \"argument\", \"line\", \"slope\", \"military\", \"same\", \"thunder\", \"baby\", \"chin\", \"word\", \"smash\", \"run\", \"rice\", \"bread\", \"heart\", \"the\", \"snake\", \"skin\", \"offer\", \"good\", \"oil\", \"behaviour\", \"work\", \"wave\", \"angry\", \"design\", \"very\", \"credit\", \"whip\", \"deep\", \"chain\", \"pleasure\", \"sticky\", \"wrong\", \"store\", \"cotton\", \"destruction\", \"rod\", \"glass\", \"needle\", \"possible\", \"red\", \"law\", \"journey\", \"black\", \"camera\", \"trouble\", \"peace\", \"birth\", \"force\", \"meal\", \"secret\", \"conscious\", \"attraction\", \"addition\", \"group\", \"stem\", \"clean\", \"thought\", \"love\", \"old\", \"weather\", \"rule\", \"page\", \"desire\", \"bottle\", \"scale\", \"river\", \"tail\", \"chief\", \"paint\", \"east\", \"grass\", \"medical\", \"direction\", \"early\", \"pocket\", \"sudden\", \"future\", \"be\", \"religion\", \"muscle\", \"warm\", \"dark\", \"linen\", \"support\", \"bit\", \"box\", \"mind\", \"take\", \"female\", \"opposite\", \"among\", \"punishment\", \"apparatus\", \"development\", \"produce\", \"hearing\", \"male\", \"coal\", \"when\", \"cat\", \"part\", \"silver\", \"brick\", \"long\", \"branch\", \"attack\", \"quiet\", \"existence\", \"plane\", \"clock\", \"out\", \"person\", \"boy\", \"daughter\", \"chance\", \"power\", \"wine\", \"organization\"}\n"
        }
      ]
    }
  ]
}