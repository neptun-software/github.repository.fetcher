{
  "metadata": {
    "timestamp": 1736566793521,
    "page": 356,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "alecthomas/participle",
      "stars": 3530,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".golangci.yml",
          "type": "blob",
          "size": 1.7138671875,
          "content": "run:\n  tests: true\n  skip-dirs:\n    - _examples\n\noutput:\n  print-issued-lines: false\n\nlinters:\n  enable-all: true\n  disable:\n    - maligned\n    - lll\n    - gocyclo\n    - gochecknoglobals\n    - wsl\n    - whitespace\n    - godox\n    - funlen\n    - gocognit\n    - gomnd\n    - goerr113\n    - godot\n    - nestif\n    - testpackage\n    - nolintlint\n    - exhaustivestruct\n    - wrapcheck\n    - gci\n    - gofumpt\n    - gocritic\n    - nlreturn\n    - errorlint\n    - nakedret\n    - forbidigo\n    - revive\n    - cyclop\n    - ifshort\n    - paralleltest\n    - interfacer\n    - scopelint\n    - golint\n    - wastedassign\n    - forcetypeassert\n    - gomoddirectives\n    - varnamelen\n    - exhaustruct\n    - ireturn\n    - nonamedreturns\n    - errname\n    - nilnil\n    - maintidx\n    - unused # Does not work with type parameters\n    - dupword\n    - depguard\n    - mnd\n    - recvcheck\n    - perfsprint\n    - predeclared\n\nlinters-settings:\n  govet:\n    check-shadowing: true\n  gocyclo:\n    min-complexity: 10\n  dupl:\n    threshold: 100\n  goconst:\n    min-len: 8\n    min-occurrences: 3\n  exhaustive:\n    default-signifies-exhaustive: true\n\nissues:\n  max-per-linter: 0\n  max-same: 0\n  exclude-use-default: false\n  exclude:\n    # Captured by errcheck.\n    - '^(G104|G204|G307):'\n    # Very commonly not checked.\n    - 'Error return value of .(.*\\.Help|.*\\.MarkFlagRequired|(os\\.)?std(out|err)\\..*|.*Close|.*Flush|os\\.Remove(All)?|.*printf?|os\\.(Un)?Setenv). is not checked'\n    - 'exported method `(.*\\.MarshalJSON|.*\\.UnmarshalJSON|.*\\.EntityURN|.*\\.GoString|.*\\.Pos)` should have comment or be unexported'\n    - 'uses unkeyed fields'\n    - 'declaration of \"err\" shadows declaration'\n    - 'bad syntax for struct tag key'\n    - 'bad syntax for struct tag pair'\n    - '^ST1012'\n"
        },
        {
          "name": ".goreleaser.yml",
          "type": "blob",
          "size": 0.67578125,
          "content": "project_name: participle\nrelease:\n  github:\n    owner: alecthomas\n    name: participle\nbrews:\n  -\n    install: bin.install \"participle\"\nenv:\n  - CGO_ENABLED=0\nbuilds:\n- goos:\n    - linux\n    - darwin\n    - windows\n  goarch:\n    - arm64\n    - amd64\n    - \"386\"\n  goarm:\n    - \"6\"\n  dir: ./cmd/participle\n  main: .\n  ldflags: -s -w -X main.version={{.Version}}\n  binary: participle\narchives:\n  -\n    format: tar.gz\n    name_template: '{{ .Binary }}-{{ .Version }}-{{ .Os }}-{{ .Arch }}{{ if .Arm }}v{{\n    .Arm }}{{ end }}'\n    files:\n      - COPYING\n      - README*\nsnapshot:\n  name_template: SNAPSHOT-{{ .Commit }}\nchecksum:\n  name_template: '{{ .ProjectName }}-{{ .Version }}-checksums.txt'\n"
        },
        {
          "name": "CHANGES.md",
          "type": "blob",
          "size": 1.3671875,
          "content": "<!-- TOC depthFrom:2 insertAnchor:true updateOnSave:true -->\n\n- [v2](#v2)\n\n<!-- /TOC -->\n\n<a id=\"markdown-v2\" name=\"v2\"></a>\n## v2\n\nv2 was released in November 2020. It contains the following changes, some of\nwhich are backwards-incompatible:\n\n- Added optional `LexString()` and `LexBytes()` methods that lexer\n  definitions can implement to fast-path lexing of bytes and strings.\n- A new stateful lexer has been added.\n- A `filename` must now be passed to all `Parse*()` and `Lex*()` methods.\n- The `text/scanner` lexer no longer automatically unquotes strings or\n  supports arbitary length single quoted strings. The tokens it produces are\n  identical to that of the `text/scanner` package. Use `Unquote()` to remove\n  quotes.\n- `Tok` and `EndTok` will no longer be populated.\n- If a field named `Token []lexer.Token` exists it will be populated with the\n  raw tokens that the node parsed from the lexer.\n- Support capturing directly into lexer.Token fields. eg.\n\n      type ast struct {\n          Head lexer.Token   `@Ident`\n          Tail []lexer.Token `@(Ident*)`\n      }\n- Add an `experimental/codegen` for stateful lexers. This provides ~10x\n  performance improvement with zero garbage when lexing strings.\n- The `regex` lexer has been removed.\n- The `ebnf` lexer has been removed.\n- All future work on lexing will be put into the stateful lexer.\n- The need for `DropToken` has been removed.\n\n"
        },
        {
          "name": "COPYING",
          "type": "blob",
          "size": 1.03515625,
          "content": "Copyright (C) 2017-2022 Alec Thomas\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\nof the Software, and to permit persons to whom the Software is furnished to do\nso, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 24.25,
          "content": "# A dead simple parser package for Go\n<a id=\"markdown-a-dead-simple-parser-package-for-go\" name=\"a-dead-simple-parser-package-for-go\"></a>\n\n[![PkgGoDev](https://pkg.go.dev/badge/github.com/alecthomas/participle/v2)](https://pkg.go.dev/github.com/alecthomas/participle/v2) [![GHA Build](https://github.com/alecthomas/participle/actions/workflows/ci.yml/badge.svg)](https://github.com/alecthomas/participle/actions)\n [![Go Report Card](https://goreportcard.com/badge/github.com/alecthomas/participle/v2)](https://goreportcard.com/report/github.com/alecthomas/participle/v2) [![Slack chat](https://img.shields.io/static/v1?logo=slack&style=flat&label=slack&color=green&message=gophers)](https://gophers.slack.com/messages/CN9DS8YF3)\n\n<!-- MarkdownTOC autolink=\"true\" lowercase=\"only_ascii\" -->\n\n- [V2](#v2)\n- [Introduction](#introduction)\n- [Tutorial](#tutorial)\n- [Tag syntax](#tag-syntax)\n- [Overview](#overview)\n- [Grammar syntax](#grammar-syntax)\n- [Capturing](#capturing)\n\t- [Capturing boolean value](#capturing-boolean-value)\n- [\"Union\" types](#union-types)\n- [Custom parsing](#custom-parsing)\n- [Lexing](#lexing)\n\t- [Stateful lexer](#stateful-lexer)\n\t- [Example stateful lexer](#example-stateful-lexer)\n\t- [Example simple/non-stateful lexer](#example-simplenon-stateful-lexer)\n\t- [Experimental - code generation](#experimental---code-generation)\n- [Options](#options)\n- [Examples](#examples)\n- [Performance](#performance)\n- [Concurrency](#concurrency)\n- [Error reporting](#error-reporting)\n- [Comments](#comments)\n- [Limitations](#limitations)\n- [EBNF](#ebnf)\n- [Syntax/Railroad Diagrams](#syntaxrailroad-diagrams)\n\n<!-- /MarkdownTOC -->\n\n## V2\n\nThis is version 2 of Participle.\n\nIt can be installed with:\n\n```shell\n$ go get github.com/alecthomas/participle/v2@latest\n```\n\nThe latest version from v0 can be installed via:\n\n```shell\n$ go get github.com/alecthomas/participle@latest\n```\n\n## <a name='Introduction'></a>Introduction\n\nThe goal of this package is to provide a simple, idiomatic and elegant way of\ndefining parsers in Go.\n\nParticiple's method of defining grammars should be familiar to any Go\nprogrammer who has used the `encoding/json` package: struct field tags define\nwhat and how input is mapped to those same fields. This is not unusual for Go\nencoders, but is unusual for a parser.\n\n## Tutorial\n\nA [tutorial](TUTORIAL.md) is available, walking through the creation of an .ini parser.\n\n## Tag syntax\n\nParticiple supports two forms of struct tag grammar syntax.\n\nThe easiest to read is when the grammar uses the entire struct tag content, eg.\n\n```go\nField string `@Ident @(\",\" Ident)*`\n```\n\nHowever, this does not coexist well with other tags such as JSON, etc. and\nmay cause issues with linters. If this is an issue then you can use the\n`parser:\"\"` tag format. In this case single quotes can be used to quote\nliterals making the tags somewhat easier to write, eg.\n\n```go\nField string `parser:\"@ident (',' Ident)*\" json:\"field\"`\n```\n\n\n\n## Overview\n\nA grammar is an annotated Go structure used to both define the parser grammar,\nand be the AST output by the parser. As an example, following is the final INI\nparser from the tutorial.\n\n ```go\n type INI struct {\n   Properties []*Property `@@*`\n   Sections   []*Section  `@@*`\n }\n\n type Section struct {\n   Identifier string      `\"[\" @Ident \"]\"`\n   Properties []*Property `@@*`\n }\n\n type Property struct {\n   Key   string `@Ident \"=\"`\n   Value *Value `@@`\n }\n\n type Value struct {\n   String *string  `  @String`\n   Float *float64  `| @Float`\n   Int    *int     `| @Int`\n }\n ```\n\n> **Note:** Participle also supports named struct tags (eg. <code>Hello string &#96;parser:\"@Ident\"&#96;</code>).\n\nA parser is constructed from a grammar and a lexer:\n\n```go\nparser, err := participle.Build[INI]()\n```\n\nOnce constructed, the parser is applied to input to produce an AST:\n\n```go\nast, err := parser.ParseString(\"\", \"size = 10\")\n// ast == &INI{\n//   Properties: []*Property{\n//     {Key: \"size\", Value: &Value{Int: &10}},\n//   },\n// }\n```\n\n## Grammar syntax\n\nParticiple grammars are defined as tagged Go structures. Participle will\nfirst look for tags in the form `parser:\"...\"`. It will then fall back to\nusing the entire tag body.\n\nThe grammar format is:\n\n- `@<expr>` Capture expression into the field.\n- `@@` Recursively capture using the fields own type.\n- `<identifier>` Match named lexer token.\n- `( ... )` Group.\n- `\"...\"` or `'...'` Match the literal (note that the lexer must emit tokens matching this literal exactly).\n- `\"...\":<identifier>` Match the literal, specifying the exact lexer token type to match.\n- `<expr> <expr> ...` Match expressions.\n- `<expr> | <expr> | ...` Match one of the alternatives. Each alternative is tried in order, with backtracking.\n- `~<expr>` Match any token that is _not_ the start of the expression (eg: `@~\";\"` matches anything but the `;` character into the field).\n- `(?= ... )` Positive lookahead group - requires the contents to match further input, without consuming it.\n- `(?! ... )` Negative lookahead group - requires the contents not to match further input, without consuming it.\n\nThe following modifiers can be used after any expression:\n\n- `*` Expression can match zero or more times.\n- `+` Expression must match one or more times.\n- `?` Expression can match zero or once.\n- `!` Require a non-empty match (this is useful with a sequence of optional matches eg. `(\"a\"? \"b\"? \"c\"?)!`).\n\nNotes:\n\n- Each struct is a single production, with each field applied in sequence.\n- `@<expr>` is the mechanism for capturing matches into the field.\n- if a struct field is not keyed with \"parser\", the entire struct tag\n  will be used as the grammar fragment. This allows the grammar syntax to remain\n  clear and simple to maintain.\n\n## Capturing\n\nPrefixing any expression in the grammar with `@` will capture matching values\nfor that expression into the corresponding field.\n\nFor example:\n\n```go\n// The grammar definition.\ntype Grammar struct {\n  Hello string `@Ident`\n}\n\n// The source text to parse.\nsource := \"world\"\n\n// After parsing, the resulting AST.\nresult == &Grammar{\n  Hello: \"world\",\n}\n```\n\nFor slice and string fields, each instance of `@` will accumulate into the\nfield (including repeated patterns). Accumulation into other types is not\nsupported.\n\nFor integer and floating point types, a successful capture will be parsed\nwith `strconv.ParseInt()` and `strconv.ParseFloat()` respectively.\n\nA successful capture match into a `bool` field will set the field to true.\n\nTokens can also be captured directly into fields of type `lexer.Token` and\n`[]lexer.Token`.\n\nCustom control of how values are captured into fields can be achieved by a\nfield type implementing the `Capture` interface (`Capture(values []string)\nerror`).\n\nAdditionally, any field implementing the `encoding.TextUnmarshaler` interface\nwill be capturable too. One caveat is that `UnmarshalText()` will be called once\nfor each captured token, so eg. `@(Ident Ident Ident)` will be called three times.\n\n### Capturing boolean value\n\nBy default, a boolean field is used to indicate that a match occurred, which\nturns out to be much more useful and common in Participle than parsing true\nor false literals. For example, parsing a variable declaration with a\ntrailing optional syntax:\n\n```go\ntype Var struct {\n  Name string `\"var\" @Ident`\n  Type string `\":\" @Ident`\n  Optional bool `@\"?\"?`\n}\n```\n\nIn practice this gives more useful ASTs. If bool were to be parsed literally\nthen you'd need to have some alternate type for Optional such as string or a\ncustom type.\n\nTo capture literal boolean values such as `true` or `false`, implement the\nCapture interface like so:\n\n```go\ntype Boolean bool\n\nfunc (b *Boolean) Capture(values []string) error {\n\t*b = values[0] == \"true\"\n\treturn nil\n}\n\ntype Value struct {\n\tFloat  *float64 `  @Float`\n\tInt    *int     `| @Int`\n\tString *string  `| @String`\n\tBool   *Boolean `| @(\"true\" | \"false\")`\n}\n```\n\n## \"Union\" types\n\nA very common pattern in parsers is \"union\" types, an example of which is\nshown above in the `Value` type. A common way of expressing this in Go is via\na sealed interface, with each member of the union implementing this\ninterface.\n\neg. this is how the `Value` type could be expressed in this way:\n\n```go\ntype Value interface { value() }\n\ntype Float struct { Value float64 `@Float` }\nfunc (f Float) value() {}\n\ntype Int struct { Value int `@Int` }\nfunc (f Int) value() {}\n\ntype String struct { Value string `@String` }\nfunc (f String) value() {}\n\ntype Bool struct { Value Boolean `@(\"true\" | \"false\")` }\nfunc (f Bool) value() {}\n```\n\nThanks to the efforts of [Jacob Ryan McCollum](https://github.com/mccolljr), Participle\nnow supports this pattern. Simply construct your parser with the `Union[T](member...T)`\noption, eg.\n\n```go\nparser := participle.MustBuild[AST](participle.Union[Value](Float{}, Int{}, String{}, Bool{}))\n```\n\nCustom parsers may also be defined for union types with the [ParseTypeWith](https://pkg.go.dev/github.com/alecthomas/participle/v2#ParseTypeWith) option.\n\n## Custom parsing\n\nThere are three ways of defining custom parsers for nodes in the grammar:\n\n1. Implement the [Capture](https://pkg.go.dev/github.com/alecthomas/participle/v2#Capture) interface.\n2. Implement the [Parseable](https://pkg.go.dev/github.com/alecthomas/participle/v2#Parseable) interface.\n3. Use the [ParseTypeWith](https://pkg.go.dev/github.com/alecthomas/participle/v2#ParseTypeWith) option to specify a custom parser for union interface types.\n\n\n## Lexing\n\nParticiple relies on distinct lexing and parsing phases. The lexer takes raw\nbytes and produces tokens which the parser consumes. The parser transforms\nthese tokens into Go values.\n\nThe default lexer, if one is not explicitly configured, is based on the Go\n`text/scanner` package and thus produces tokens for C/Go-like source code. This\nis surprisingly useful, but if you do require more control over lexing the\nincluded stateful [`participle/lexer`](#markdown-stateful-lexer) lexer should\ncover most other cases. If that in turn is not flexible enough, you can\nimplement your own lexer.\n\nConfigure your parser with a lexer using the `participle.Lexer()` option.\n\nTo use your own Lexer you will need to implement two interfaces:\n[Definition](https://pkg.go.dev/github.com/alecthomas/participle/v2/lexer#Definition)\n(and optionally [StringsDefinition](https://pkg.go.dev/github.com/alecthomas/participle/v2/lexer#StringDefinition) and [BytesDefinition](https://pkg.go.dev/github.com/alecthomas/participle/v2/lexer#BytesDefinition)) and [Lexer](https://pkg.go.dev/github.com/alecthomas/participle/v2/lexer#Lexer).\n\n### Stateful lexer\n\nIn addition to the default lexer, Participle includes an optional\nstateful/modal lexer which provides powerful yet convenient\nconstruction of most lexers.  (Notably, indentation based lexers cannot\nbe expressed using the `stateful` lexer -- for discussion of how these\nlexers can be implemented, see [#20](https://github.com/alecthomas/participle/issues/20)).\n\nIt is sometimes the case that a simple lexer cannot fully express the tokens\nrequired by a parser. The canonical example of this is interpolated strings\nwithin a larger language. eg.\n\n```go\nlet a = \"hello ${name + \", ${last + \"!\"}\"}\"\n```\n\nThis is impossible to tokenise with a normal lexer due to the arbitrarily\ndeep nesting of expressions. To support this case Participle's lexer is now\nstateful by default.\n\nThe lexer is a state machine defined by a map of rules keyed by the state\nname. Each rule within the state includes the name of the produced token, the\nregex to match, and an optional operation to apply when the rule matches.\n\nAs a convenience, any `Rule` starting with a lowercase letter will be elided\nfrom output, though it is recommended to use `participle.Elide()` instead, as it\nbetter integrates with the parser.\n\nLexing starts in the `Root` group. Each rule is matched in order, with the first\nsuccessful match producing a lexeme. If the matching rule has an associated Action\nit will be executed.\n\nA state change can be introduced with the Action `Push(state)`. `Pop()` will\nreturn to the previous state.\n\nTo reuse rules from another state, use `Include(state)`.\n\nA special named rule `Return()` can also be used as the final rule in a state\nto always return to the previous state.\n\nAs a special case, regexes containing backrefs in the form `\\N` (where `N` is\na digit) will match the corresponding capture group from the immediate parent\ngroup. This can be used to parse, among other things, heredocs. See the\n[tests](https://github.com/alecthomas/participle/blob/master/lexer/stateful_test.go#L59)\nfor an example of this, among others.\n\n### Example stateful lexer\n\nHere's a cut down example of the string interpolation described above. Refer to\nthe [stateful example](https://github.com/alecthomas/participle/tree/master/_examples/stateful)\nfor the corresponding parser.\n\n```go\nvar lexer = lexer.Must(Rules{\n\t\"Root\": {\n\t\t{`String`, `\"`, Push(\"String\")},\n\t},\n\t\"String\": {\n\t\t{\"Escaped\", `\\\\.`, nil},\n\t\t{\"StringEnd\", `\"`, Pop()},\n\t\t{\"Expr\", `\\${`, Push(\"Expr\")},\n\t\t{\"Char\", `[^$\"\\\\]+`, nil},\n\t},\n\t\"Expr\": {\n\t\tInclude(\"Root\"),\n\t\t{`whitespace`, `\\s+`, nil},\n\t\t{`Oper`, `[-+/*%]`, nil},\n\t\t{\"Ident\", `\\w+`, nil},\n\t\t{\"ExprEnd\", `}`, Pop()},\n\t},\n})\n```\n\n### Example simple/non-stateful lexer\n\nOther than the default and stateful lexers, it's easy to define your\nown _stateless_ lexer using the `lexer.MustSimple()` and\n`lexer.NewSimple()` functions.  These functions accept a slice of\n`lexer.SimpleRule{}` objects consisting of a key and a regex-style pattern.\n\n> **Note:** The stateful lexer replaces the old regex lexer.\n\nFor example, the lexer for a form of BASIC:\n\n```go\nvar basicLexer = lexer.MustSimple([]lexer.SimpleRule{\n    {\"Comment\", `(?i)rem[^\\n]*`},\n    {\"String\", `\"(\\\\\"|[^\"])*\"`},\n    {\"Number\", `[-+]?(\\d*\\.)?\\d+`},\n    {\"Ident\", `[a-zA-Z_]\\w*`},\n    {\"Punct\", `[-[!@#$%^&*()+_={}\\|:;\"'<,>.?/]|]`},\n    {\"EOL\", `[\\n\\r]+`},\n    {\"whitespace\", `[ \\t]+`},\n})\n```\n\n### Experimental - code generation\n\nParticiple v2 now has experimental support for generating code to perform\nlexing.\n\nThis will generally provide around a 10x improvement in lexing performance\nwhile producing O(1) garbage.\n\nTo use:\n1. Serialize the `stateful` lexer definition to a JSON file (pass to `json.Marshal`).\n2. Run the `participle` command (see `scripts/participle`) to generate go code from the lexer JSON definition. For example:\n```\nparticiple gen lexer <package name> [--name SomeCustomName] < mylexer.json | gofmt > mypackage/mylexer.go\n```\n(see `genLexer` in `conformance_test.go` for a more detailed example)\n\n3. When constructing your parser, use the generated lexer for your lexer definition, such as:\n```\nvar ParserDef = participle.MustBuild[someGrammer](participle.Lexer(mylexer.SomeCustomnameLexer))\n```\n\nConsider contributing to the tests in `conformance_test.go` if they do not\nappear to cover the types of expressions you are using the generated\nlexer.\n\n**Known limitations of the code generated lexer:**\n\n* The lexer is always greedy. e.g., the regex `\"[A-Z][A-Z][A-Z]?T\"` will not match `\"EST\"` in the generated lexer because the quest operator is a greedy match and does not \"give back\" to try other possibilities; you can overcome by using `|` if you have a non-greedy match, e.g., `\"[A-Z][A-Z]|(?:[A-Z]T|T)\"` will produce correct results in both lexers (see [#276](https://github.com/alecthomas/participle/issues/276) for more detail); this limitation allows the generated lexer to be very fast and memory efficient\n* Backreferences in regular expressions are not currently supported\n\n## Options\n\nThe Parser's behaviour can be configured via [Options](https://pkg.go.dev/github.com/alecthomas/participle/v2#Option).\n\n## Examples\n\nThere are several [examples included](https://github.com/alecthomas/participle/tree/master/_examples),\nsome of which are linked directly here. These examples should be run from the\n`_examples` subdirectory within a cloned copy of this repository.\n\nExample | Description\n--------|---------------\n[BASIC](https://github.com/alecthomas/participle/tree/master/_examples/basic) | A lexer, parser and interpreter for a [rudimentary dialect](https://caml.inria.fr/pub/docs/oreilly-book/html/book-ora058.html) of BASIC.\n[EBNF](https://github.com/alecthomas/participle/tree/master/_examples/ebnf) | Parser for the form of EBNF used by Go.\n[Expr](https://github.com/alecthomas/participle/tree/master/_examples/expr) | A basic mathematical expression parser and evaluator.\n[GraphQL](https://github.com/alecthomas/participle/tree/master/_examples/graphql) | Lexer+parser for GraphQL schemas\n[HCL](https://github.com/alecthomas/participle/tree/master/_examples/hcl) | A parser for the [HashiCorp Configuration Language](https://github.com/hashicorp/hcl).\n[INI](https://github.com/alecthomas/participle/tree/master/_examples/ini) | An INI file parser.\n[Protobuf](https://github.com/alecthomas/participle/tree/master/_examples/protobuf) | A full [Protobuf](https://developers.google.com/protocol-buffers/) version 2 and 3 parser.\n[SQL](https://github.com/alecthomas/participle/tree/master/_examples/sql) | A *very* rudimentary SQL SELECT parser.\n[Stateful](https://github.com/alecthomas/participle/tree/master/_examples/stateful) | A basic example of a stateful lexer and corresponding parser.\n[Thrift](https://github.com/alecthomas/participle/tree/master/_examples/thrift) | A full [Thrift](https://thrift.apache.org/docs/idl) parser.\n[TOML](https://github.com/alecthomas/participle/tree/master/_examples/toml) | A [TOML](https://github.com/toml-lang/toml) parser.\n\nIncluded below is a full GraphQL lexer and parser:\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\n\t\"github.com/alecthomas/kong\"\n\t\"github.com/alecthomas/repr\"\n\n\t\"github.com/alecthomas/participle/v2\"\n\t\"github.com/alecthomas/participle/v2/lexer\"\n)\n\ntype File struct {\n\tEntries []*Entry `@@*`\n}\n\ntype Entry struct {\n\tType   *Type   `  @@`\n\tSchema *Schema `| @@`\n\tEnum   *Enum   `| @@`\n\tScalar string  `| \"scalar\" @Ident`\n}\n\ntype Enum struct {\n\tName  string   `\"enum\" @Ident`\n\tCases []string `\"{\" @Ident* \"}\"`\n}\n\ntype Schema struct {\n\tFields []*Field `\"schema\" \"{\" @@* \"}\"`\n}\n\ntype Type struct {\n\tName       string   `\"type\" @Ident`\n\tImplements string   `( \"implements\" @Ident )?`\n\tFields     []*Field `\"{\" @@* \"}\"`\n}\n\ntype Field struct {\n\tName       string      `@Ident`\n\tArguments  []*Argument `( \"(\" ( @@ ( \",\" @@ )* )? \")\" )?`\n\tType       *TypeRef    `\":\" @@`\n\tAnnotation string      `( \"@\" @Ident )?`\n}\n\ntype Argument struct {\n\tName    string   `@Ident`\n\tType    *TypeRef `\":\" @@`\n\tDefault *Value   `( \"=\" @@ )`\n}\n\ntype TypeRef struct {\n\tArray       *TypeRef `(   \"[\" @@ \"]\"`\n\tType        string   `  | @Ident )`\n\tNonNullable bool     `( @\"!\" )?`\n}\n\ntype Value struct {\n\tSymbol string `@Ident`\n}\n\nvar (\n\tgraphQLLexer = lexer.MustSimple([]lexer.SimpleRule{\n\t\t{\"Comment\", `(?:#|//)[^\\n]*\\n?`},\n\t\t{\"Ident\", `[a-zA-Z]\\w*`},\n\t\t{\"Number\", `(?:\\d*\\.)?\\d+`},\n\t\t{\"Punct\", `[-[!@#$%^&*()+_={}\\|:;\"'<,>.?/]|]`},\n\t\t{\"Whitespace\", `[ \\t\\n\\r]+`},\n\t})\n\tparser = participle.MustBuild[File](\n\t\tparticiple.Lexer(graphQLLexer),\n\t\tparticiple.Elide(\"Comment\", \"Whitespace\"),\n\t\tparticiple.UseLookahead(2),\n\t)\n)\n\nvar cli struct {\n\tEBNF  bool     `help\"Dump EBNF.\"`\n\tFiles []string `arg:\"\" optional:\"\" type:\"existingfile\" help:\"GraphQL schema files to parse.\"`\n}\n\nfunc main() {\n\tctx := kong.Parse(&cli)\n\tif cli.EBNF {\n\t\tfmt.Println(parser.String())\n\t\tctx.Exit(0)\n\t}\n\tfor _, file := range cli.Files {\n\t\tr, err := os.Open(file)\n\t\tctx.FatalIfErrorf(err)\n\t\tast, err := parser.Parse(file, r)\n\t\tr.Close()\n\t\trepr.Println(ast)\n\t\tctx.FatalIfErrorf(err)\n\t}\n}\n```\n\n## Performance\n\nOne of the included examples is a complete Thrift parser\n(shell-style comments are not supported). This gives\na convenient baseline for comparing to the PEG based\n[pigeon](https://github.com/PuerkitoBio/pigeon), which is the parser used by\n[go-thrift](https://github.com/samuel/go-thrift). Additionally, the pigeon\nparser is utilising a generated parser, while the participle parser is built at\nrun time.\n\nYou can run the benchmarks yourself, but here's the output on my machine:\n\n    BenchmarkParticipleThrift-12    \t   5941\t   201242 ns/op\t 178088 B/op\t   2390 allocs/op\n    BenchmarkGoThriftParser-12      \t   3196\t   379226 ns/op\t 157560 B/op\t   2644 allocs/op\n\nOn a real life codebase of 47K lines of Thrift, Participle takes 200ms and go-\nthrift takes 630ms, which aligns quite closely with the benchmarks.\n\n## Concurrency\n\nA compiled `Parser` instance can be used concurrently. A `LexerDefinition` can be used concurrently. A `Lexer` instance cannot be used concurrently.\n\n## Error reporting\n\nThere are a few areas where Participle can provide useful feedback to users of your parser.\n\n1. Errors returned by [Parser.Parse*()](https://pkg.go.dev/github.com/alecthomas/participle/v2#Parser.Parse) will be:\n\t1. Of type [Error](https://pkg.go.dev/github.com/alecthomas/participle/v2#Error). This will contain positional information where available.\n\t2. May either be [ParseError](https://pkg.go.dev/github.com/alecthomas/participle/v2#ParseError) or [lexer.Error](https://pkg.go.dev/github.com/alecthomas/participle/v2/lexer#Error)\n2. Participle will make a best effort to return as much of the AST up to the error location as possible.\n3. Any node in the AST containing a field `Pos lexer.Position` [^1] will be automatically\n   populated from the nearest matching token.\n4. Any node in the AST containing a field `EndPos lexer.Position` [^1] will be\n   automatically populated from the token at the end of the node.\n5. Any node in the AST containing a field `Tokens []lexer.Token` will be automatically\n   populated with _all_ tokens captured by the node, _including_ elided tokens.\n\n[^1]: Either the concrete type or a type convertible to it, allowing user defined types to be used.\n\nThese related pieces of information can be combined to provide fairly comprehensive error reporting.\n\n## Comments\n\nComments can be difficult to capture as in most languages they may appear almost\nanywhere. There are three ways of capturing comments, with decreasing fidelity.\n\nThe first is to elide tokens in the parser, then add `Tokens []lexer.Token` as a\nfield to each AST node. Comments will be included. This has the downside that\nthere's no straightforward way to know where the comments are relative to\nnon-comment tokens in that node.\n\nThe second way is to _not_ elide comment tokens, and explicitly capture them at\nevery location in the AST where they might occur. This has the downside that\nunless you place these captures in every possible valid location, users might\ninsert valid comments that then fail to parse.\n\nThe third way is to elide comment tokens and capture them where they're\nsemantically meaningful, such as for documentation comments. Participle supports\nexplicitly matching elided tokens for this purpose.\n\n## Limitations\n\nInternally, Participle is a recursive descent parser with backtracking (see\n`UseLookahead(K)`).\n\nAmong other things, this means that Participle grammars do not support left\nrecursion. Left recursion must be eliminated by restructuring your grammar.\n\n## EBNF\n\nThe old `EBNF` lexer was removed in a major refactoring at\n[362b26](https://github.com/alecthomas/participle/commit/362b26640fa3dc406aa60960f7d9a5b9a909414e)\n-- if you have an EBNF grammar you need to implement, you can either translate\nit into regex-style `lexer.Rule{}` syntax or implement your own EBNF lexer\nyou might be able to use [the old EBNF lexer](https://github.com/alecthomas/participle/blob/2403858c8b2068b4b0cf96a6b36dd7069674039b/lexer/ebnf/ebnf.go)\n-- as a starting point.\n\nParticiple supports outputting an EBNF grammar from a Participle parser. Once\nthe parser is constructed simply call `String()`.\n\nParticiple also [includes a parser](https://pkg.go.dev/github.com/alecthomas/participle/v2/ebnf) for this form of EBNF (naturally).\n\neg. The [GraphQL example](https://github.com/alecthomas/participle/blob/master/_examples/graphql/main.go#L15-L62)\ngives in the following EBNF:\n\n```ebnf\nFile = Entry* .\nEntry = Type | Schema | Enum | \"scalar\" ident .\nType = \"type\" ident (\"implements\" ident)? \"{\" Field* \"}\" .\nField = ident (\"(\" (Argument (\",\" Argument)*)? \")\")? \":\" TypeRef (\"@\" ident)? .\nArgument = ident \":\" TypeRef (\"=\" Value)? .\nTypeRef = \"[\" TypeRef \"]\" | ident \"!\"? .\nValue = ident .\nSchema = \"schema\" \"{\" Field* \"}\" .\nEnum = \"enum\" ident \"{\" ident* \"}\" .\n```\n\n## Syntax/Railroad Diagrams\n\nParticiple includes a [command-line utility](https://github.com/alecthomas/participle/tree/master/cmd/railroad) to take an EBNF representation of a Participle grammar\n(as returned by `Parser.String()`) and produce a Railroad Diagram using\n[tabatkins/railroad-diagrams](https://github.com/tabatkins/railroad-diagrams).\n\nHere's what the GraphQL grammar looks like:\n\n![EBNF Railroad Diagram](railroad.png)\n"
        },
        {
          "name": "TUTORIAL.md",
          "type": "blob",
          "size": 7.5732421875,
          "content": "# Participle parser tutorial\n\n<!-- TOC depthFrom:2 insertAnchor:true updateOnSave:true -->\n\n- [Introduction](#introduction)\n- [The complete grammar](#the-complete-grammar)\n- [Root of the .ini AST (structure, fields)](#root-of-the-ini-ast-structure-fields)\n- [.ini properties (named tokens, capturing, literals)](#ini-properties-named-tokens-capturing-literals)\n- [.ini property values (alternates, recursive structs, sequences)](#ini-property-values-alternates-recursive-structs-sequences)\n- [Complete, but limited, .ini grammar (top-level properties only)](#complete-but-limited-ini-grammar-top-level-properties-only)\n- [Extending our grammar to support sections](#extending-our-grammar-to-support-sections)\n- [(Optional) Source positional information](#optional-source-positional-information)\n- [Parsing using our grammar](#parsing-using-our-grammar)\n\n<!-- /TOC -->\n\n<a id=\"markdown-introduction\" name=\"introduction\"></a>\n## Introduction\n\nWriting a parser in Participle typically involves starting from the \"root\" of\nthe AST, annotating fields with the grammar, then recursively expanding until\nit is complete. The AST is expressed via Go data types and the grammar is\nexpressed through struct field tags, as a form of EBNF.\n\nThe parser we're going to create for this tutorial parses .ini files\nlike this:\n\n```ini\nage = 21\nname = \"Bob Smith\"\n\n[address]\ncity = \"Beverly Hills\"\npostal_code = 90210\n```\n\n<a id=\"markdown-the-complete-grammar\" name=\"the-complete-grammar\"></a>\n## The complete grammar\n\nI think it's useful to see the complete grammar first, to see what we're\nworking towards. Read on below for details.\n\n ```go\n type INI struct {\n   Properties []*Property `@@*`\n   Sections   []*Section  `@@*`\n }\n\n type Section struct {\n   Identifier string      `\"[\" @Ident \"]\"`\n   Properties []*Property `@@*`\n }\n\n type Property struct {\n   Key   string `@Ident \"=\"`\n   Value Value `@@`\n }\n\ntype Value interface{ value() }\n\ntype String struct {\n\tString string `@String`\n}\n\nfunc (String) value() {}\n\ntype Number struct {\n\tNumber float64 `@Float | @Int`\n}\n\nfunc (Number) value() {}\n ```\n\n<a id=\"markdown-root-of-the-ini-ast-structure-fields\" name=\"root-of-the-ini-ast-structure-fields\"></a>\n## Root of the .ini AST (structure, fields)\n\nThe first step is to create a root struct for our grammar. In the case of our\n.ini parser, this struct will contain a sequence of properties:\n\n```go\ntype INI struct {\n  Properties []*Property\n}\n\ntype Property struct {\n}\n```\n\n<a id=\"markdown-ini-properties-named-tokens-capturing-literals\" name=\"ini-properties-named-tokens-capturing-literals\"></a>\n## .ini properties (named tokens, capturing, literals)\n\nEach property in an .ini file has an identifier key:\n\n```go\ntype Property struct {\n  Key string\n}\n```\n\nThe default lexer tokenises Go source code, and includes an `Ident` token type\nthat matches identifiers. To match this token we simply use the token type\nname:\n\n```go\ntype Property struct {\n  Key string `Ident`\n}\n```\n\nThis will *match* identifiers, but not *capture* them into the `Key` field. To\ncapture input tokens into AST fields, prefix any grammar node with `@`:\n\n```go\ntype Property struct {\n  Key string `@Ident`\n}\n```\n\nIn .ini files, each key is separated from its value with a literal `=`. To\nmatch a literal, enclose the literal in double quotes:\n\n```go\ntype Property struct {\n  Key string `@Ident \"=\"`\n}\n```\n\n> Note: literals in the grammar must match tokens from the lexer *exactly*. In\n> this example if the lexer does not output `=` as a distinct token the\n> grammar will not match.\n\n<a id=\"markdown-ini-property-values-alternates-recursive-structs-sequences\" name=\"ini-property-values-alternates-recursive-structs-sequences\"></a>\n## .ini property values (alternates, recursive structs, sequences)\n\nFor the purposes of our example we are only going to support quoted string\nand numeric property values. As each value can be *either* a string or a float\nwe'll need something akin to a sum type. Participle supports this via the \n`Union[T any](members...T) Option` parser option. This tells the parser that\nwhen a field of interface type `T` is encountered, it should try to match each\nof the `members` in turn, and return the first successful match.\n\n```go\ntype Value interface{ value() }\n\ntype String struct {\n\tString string `@String`\n}\n\nfunc (String) value() {}\n\ntype Number struct {\n\tNumber float64 `@Float`\n}\n\nfunc (Number) value() {}\n```\n\nSince we want to also parse integers and the default lexer differentiates\nbetween floats and integers, we need to explicitly match either. To express\nmatching a set of alternatives such as this, we use the `|` operator:\n\n```go\ntype Number struct {\n\tNumber float64 `@Float | @Int`\n}\n```\n\n> Note: the grammar can cross fields.\n\nNext, we'll match values and capture them into the `Property`. To recursively\ncapture structs use `@@` (capture self):\n\n```go\ntype Property struct {\n  Key   string `@Ident \"=\"`\n  Value Value `@@`\n}\n```\n\nNow that we can parse a `Property` we need to go back to the root of the\ngrammar. We want to parse 0 or more properties. To do this, we use `<expr>*`.\nParticiple will accumulate each match into the slice until matching fails,\nthen move to the next node in the grammar.\n\n```go\ntype INI struct {\n  Properties []*Property `@@*`\n}\n```\n\n> Note: tokens can also be accumulated into strings, appending each match.\n\n<a id=\"markdown-complete-but-limited-ini-grammar-top-level-properties-only\" name=\"complete-but-limited-ini-grammar-top-level-properties-only\"></a>\n## Complete, but limited, .ini grammar (top-level properties only)\n\nWe now have a functional, but limited, .ini parser!\n\n```go\ntype INI struct {\n  Properties []*Property `@@*`\n}\n\ntype Property struct {\n  Key   string   `@Ident \"=\"`\n  Value Value    `@@`\n}\n\ntype Value interface{ value() }\n\ntype String struct {\n\tString string `@String`\n}\n\nfunc (String) value() {}\n\ntype Number struct {\n\tNumber float64 `@Float | @Int`\n}\n\nfunc (Number) value() {}\n```\n\n<a id=\"markdown-extending-our-grammar-to-support-sections\" name=\"extending-our-grammar-to-support-sections\"></a>\n## Extending our grammar to support sections\n\nAdding support for sections is simply a matter of utilising the constructs\nwe've just learnt. A section consists of a header identifier, and a sequence\nof properties:\n\n```go\ntype Section struct {\n  Identifier string      `\"[\" @Ident \"]\"`\n  Properties []*Property `@@*`\n}\n```\n\nSimple!\n\nNow we just add a sequence of `Section`s to our root node:\n\n```go\ntype INI struct {\n  Properties []*Property `@@*`\n  Sections   []*Section  `@@*`\n}\n```\n\nAnd we're done!\n\n<a id=\"markdown-optional-source-positional-information\" name=\"optional-source-positional-information\"></a>\n## (Optional) Source positional information\n\nIf a grammar node includes a field with the name `Pos` and type `lexer.Position`, it will be automatically populated by positional information. eg.\n\n```go\ntype String struct {\n  Pos lexer.Position\n\n\tString string `@String`\n}\n\ntype Number struct {\n  Pos lexer.Position\n\n\tNumber float64 `@Float | @Int`\n}\n```\n\nThis is useful for error reporting.\n\n<a id=\"markdown-parsing-using-our-grammar\" name=\"parsing-using-our-grammar\"></a>\n## Parsing using our grammar\n\nTo parse with this grammar we first construct the parser (we'll use the\ndefault lexer for now):\n\n```go\nparser, err := participle.Build[INI](\n  participle.Unquote(\"String\"),\n  participle.Union[Value](String{}, Number{}),\n)\n```\n\nThen parse a new INI file with `parser.Parse{,String,Bytes}()`:\n\n```go\nini, err := parser.ParseString(\"\", `\nage = 21\nname = \"Bob Smith\"\n\n[address]\ncity = \"Beverly Hills\"\npostal_code = 90210\n`)\n```\n\nYou can find the full example [here](_examples/ini/main.go), alongside\nother examples including an SQL `SELECT` parser and a full\n[Thrift](https://thrift.apache.org/) parser.\n"
        },
        {
          "name": "_examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "api.go",
          "type": "blob",
          "size": 0.5556640625,
          "content": "package participle\n\nimport (\n\t\"github.com/alecthomas/participle/v2/lexer\"\n)\n\n// Capture can be implemented by fields in order to transform captured tokens into field values.\ntype Capture interface {\n\tCapture(values []string) error\n}\n\n// The Parseable interface can be implemented by any element in the grammar to provide custom parsing.\ntype Parseable interface {\n\t// Parse into the receiver.\n\t//\n\t// Should return NextMatch if no tokens matched and parsing should continue.\n\t// Nil should be returned if parsing was successful.\n\tParse(lex *lexer.PeekingLexer) error\n}\n"
        },
        {
          "name": "bin",
          "type": "tree",
          "content": null
        },
        {
          "name": "cmd",
          "type": "tree",
          "content": null
        },
        {
          "name": "context.go",
          "type": "blob",
          "size": 3.4921875,
          "content": "package participle\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"reflect\"\n\t\"strings\"\n\n\t\"github.com/alecthomas/participle/v2/lexer\"\n)\n\ntype contextFieldSet struct {\n\ttokens     []lexer.Token\n\tstrct      reflect.Value\n\tfield      structLexerField\n\tfieldValue []reflect.Value\n}\n\n// Context for a single parse.\ntype parseContext struct {\n\tlexer.PeekingLexer\n\tdepth             int\n\ttrace             io.Writer\n\tdeepestError      error\n\tdeepestErrorDepth int\n\tlookahead         int\n\tcaseInsensitive   map[lexer.TokenType]bool\n\tapply             []*contextFieldSet\n\tallowTrailing     bool\n}\n\nfunc newParseContext(lex *lexer.PeekingLexer, lookahead int, caseInsensitive map[lexer.TokenType]bool) parseContext {\n\treturn parseContext{\n\t\tPeekingLexer:    *lex,\n\t\tcaseInsensitive: caseInsensitive,\n\t\tlookahead:       lookahead,\n\t}\n}\n\nfunc (p *parseContext) DeepestError(err error) error {\n\tif p.PeekingLexer.Cursor() >= p.deepestErrorDepth {\n\t\treturn err\n\t}\n\tif p.deepestError != nil {\n\t\treturn p.deepestError\n\t}\n\treturn err\n}\n\n// Defer adds a function to be applied once a branch has been picked.\nfunc (p *parseContext) Defer(tokens []lexer.Token, strct reflect.Value, field structLexerField, fieldValue []reflect.Value) {\n\tp.apply = append(p.apply, &contextFieldSet{tokens, strct, field, fieldValue})\n}\n\n// Apply deferred functions.\nfunc (p *parseContext) Apply() error {\n\tfor _, apply := range p.apply {\n\t\tif err := setField(apply.tokens, apply.strct, apply.field, apply.fieldValue); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tp.apply = nil\n\treturn nil\n}\n\n// Branch accepts the branch as the correct branch.\nfunc (p *parseContext) Accept(branch *parseContext) {\n\tp.apply = append(p.apply, branch.apply...)\n\tp.PeekingLexer = branch.PeekingLexer\n\tif branch.deepestErrorDepth >= p.deepestErrorDepth {\n\t\tp.deepestErrorDepth = branch.deepestErrorDepth\n\t\tp.deepestError = branch.deepestError\n\t}\n}\n\n// Branch starts a new lookahead branch.\nfunc (p *parseContext) Branch() *parseContext {\n\tbranch := &parseContext{}\n\t*branch = *p\n\tbranch.apply = nil\n\treturn branch\n}\n\nfunc (p *parseContext) MaybeUpdateError(err error) {\n\tif p.PeekingLexer.Cursor() >= p.deepestErrorDepth {\n\t\tp.deepestError = err\n\t\tp.deepestErrorDepth = p.PeekingLexer.Cursor()\n\t}\n}\n\n// Stop returns true if parsing should terminate after the given \"branch\" failed to match.\n//\n// Additionally, track the deepest error in the branch - the deeper the error, the more useful it usually is.\n// It could already be the deepest error in the branch (only if deeper than current parent context deepest),\n// or it could be \"err\", the latest error on the branch (even if same depth; the lexer holds the position).\nfunc (p *parseContext) Stop(err error, branch *parseContext) bool {\n\tif branch.deepestErrorDepth > p.deepestErrorDepth {\n\t\tp.deepestError = branch.deepestError\n\t\tp.deepestErrorDepth = branch.deepestErrorDepth\n\t} else if branch.PeekingLexer.Cursor() >= p.deepestErrorDepth {\n\t\tp.deepestError = err\n\t\tp.deepestErrorDepth = maxInt(branch.PeekingLexer.Cursor(), branch.deepestErrorDepth)\n\t}\n\tif !p.hasInfiniteLookahead() && branch.PeekingLexer.Cursor() > p.PeekingLexer.Cursor()+p.lookahead {\n\t\tp.Accept(branch)\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (p *parseContext) hasInfiniteLookahead() bool { return p.lookahead < 0 }\n\nfunc (p *parseContext) printTrace(n node) func() {\n\tif p.trace != nil {\n\t\ttok := p.PeekingLexer.Peek()\n\t\tfmt.Fprintf(p.trace, \"%s%q %s\\n\", strings.Repeat(\" \", p.depth*2), tok, n.GoString())\n\t\tp.depth += 1\n\t\treturn func() { p.depth -= 1 }\n\t}\n\treturn func() {}\n}\n\nfunc maxInt(a, b int) int {\n\tif a > b {\n\t\treturn a\n\t}\n\treturn b\n}\n"
        },
        {
          "name": "doc.go",
          "type": "blob",
          "size": 2.08203125,
          "content": "// Package participle constructs parsers from definitions in struct tags and parses directly into\n// those structs. The approach is philosophically similar to how other marshallers work in Go,\n// \"unmarshalling\" an instance of a grammar into a struct.\n//\n// The supported annotation syntax is:\n//\n//   - `@<expr>` Capture expression into the field.\n//   - `@@` Recursively capture using the fields own type.\n//   - `<identifier>` Match named lexer token.\n//   - `( ... )` Group.\n//   - `\"...\"` Match the literal (note that the lexer must emit tokens matching this literal exactly).\n//   - `\"...\":<identifier>` Match the literal, specifying the exact lexer token type to match.\n//   - `<expr> <expr> ...` Match expressions.\n//   - `<expr> | <expr>` Match one of the alternatives.\n//\n// The following modifiers can be used after any expression:\n//\n//   - `*` Expression can match zero or more times.\n//   - `+` Expression must match one or more times.\n//   - `?` Expression can match zero or once.\n//   - `!` Require a non-empty match (this is useful with a sequence of optional matches eg. `(\"a\"? \"b\"? \"c\"?)!`).\n//\n// Here's an example of an EBNF grammar.\n//\n//\ttype Group struct {\n//\t    Expression *Expression `\"(\" @@ \")\"`\n//\t}\n//\n//\ttype Option struct {\n//\t    Expression *Expression `\"[\" @@ \"]\"`\n//\t}\n//\n//\ttype Repetition struct {\n//\t    Expression *Expression `\"{\" @@ \"}\"`\n//\t}\n//\n//\ttype Literal struct {\n//\t    Start string `@String` // lexer.Lexer token \"String\"\n//\t    End   string `(\"â€¦\" @String)?`\n//\t}\n//\n//\ttype Term struct {\n//\t    Name       string      `  @Ident`\n//\t    Literal    *Literal    `| @@`\n//\t    Group      *Group      `| @@`\n//\t    Option     *Option     `| @@`\n//\t    Repetition *Expression `| \"(\" @@ \")\"`\n//\t}\n//\n//\ttype Sequence struct {\n//\t    Terms []*Term `@@+`\n//\t}\n//\n//\ttype Expression struct {\n//\t    Alternatives []*Sequence `@@ (\"|\" @@)*`\n//\t}\n//\n//\ttype Expressions []*Expression\n//\n//\ttype Production struct {\n//\t    Name        string      `@Ident \"=\"`\n//\t    Expressions Expressions `@@+ \".\"`\n//\t}\n//\n//\ttype EBNF struct {\n//\t    Productions []*Production `@@*`\n//\t}\npackage participle\n"
        },
        {
          "name": "ebnf.go",
          "type": "blob",
          "size": 3.01953125,
          "content": "package participle\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n)\n\n// String returns the EBNF for the grammar.\n//\n// Productions are always upper cased. Lexer tokens are always lower case.\nfunc (p *Parser[G]) String() string {\n\treturn ebnf(p.typeNodes[p.rootType])\n}\n\ntype ebnfp struct {\n\tname string\n\tout  string\n}\n\nfunc ebnf(n node) string {\n\toutp := []*ebnfp{}\n\tswitch n.(type) {\n\tcase *strct:\n\t\tbuildEBNF(true, n, map[node]bool{}, nil, &outp)\n\t\tout := []string{}\n\t\tfor _, p := range outp {\n\t\t\tout = append(out, fmt.Sprintf(\"%s = %s .\", p.name, p.out))\n\t\t}\n\t\treturn strings.Join(out, \"\\n\")\n\n\tdefault:\n\t\tout := &ebnfp{}\n\t\tbuildEBNF(true, n, map[node]bool{}, out, &outp)\n\t\treturn out.out\n\t}\n}\n\nfunc buildEBNF(root bool, n node, seen map[node]bool, p *ebnfp, outp *[]*ebnfp) {\n\tswitch n := n.(type) {\n\tcase *disjunction:\n\t\tif !root {\n\t\t\tp.out += \"(\"\n\t\t}\n\t\tfor i, next := range n.nodes {\n\t\t\tif i > 0 {\n\t\t\t\tp.out += \" | \"\n\t\t\t}\n\t\t\tbuildEBNF(false, next, seen, p, outp)\n\t\t}\n\t\tif !root {\n\t\t\tp.out += \")\"\n\t\t}\n\n\tcase *union:\n\t\tname := strings.ToUpper(n.typ.Name()[:1]) + n.typ.Name()[1:]\n\t\tif p != nil {\n\t\t\tp.out += name\n\t\t}\n\t\tif seen[n] {\n\t\t\treturn\n\t\t}\n\t\tp = &ebnfp{name: name}\n\t\t*outp = append(*outp, p)\n\t\tseen[n] = true\n\t\tfor i, next := range n.disjunction.nodes {\n\t\t\tif i > 0 {\n\t\t\t\tp.out += \" | \"\n\t\t\t}\n\t\t\tbuildEBNF(false, next, seen, p, outp)\n\t\t}\n\n\tcase *custom:\n\t\tname := strings.ToUpper(n.typ.Name()[:1]) + n.typ.Name()[1:]\n\t\tp.out += name\n\n\tcase *strct:\n\t\tname := strings.ToUpper(n.typ.Name()[:1]) + n.typ.Name()[1:]\n\t\tif p != nil {\n\t\t\tp.out += name\n\t\t}\n\t\tif seen[n] {\n\t\t\treturn\n\t\t}\n\t\tseen[n] = true\n\t\tp = &ebnfp{name: name}\n\t\t*outp = append(*outp, p)\n\t\tbuildEBNF(true, n.expr, seen, p, outp)\n\n\tcase *sequence:\n\t\tgroup := n.next != nil && !root\n\t\tif group {\n\t\t\tp.out += \"(\"\n\t\t}\n\t\tfor n != nil {\n\t\t\tbuildEBNF(false, n.node, seen, p, outp)\n\t\t\tn = n.next\n\t\t\tif n != nil {\n\t\t\t\tp.out += \" \"\n\t\t\t}\n\t\t}\n\t\tif group {\n\t\t\tp.out += \")\"\n\t\t}\n\n\tcase *parseable:\n\t\tp.out += n.t.Name()\n\n\tcase *capture:\n\t\tbuildEBNF(false, n.node, seen, p, outp)\n\n\tcase *reference:\n\t\tp.out += \"<\" + strings.ToLower(n.identifier) + \">\"\n\n\tcase *negation:\n\t\tp.out += \"~\"\n\t\tbuildEBNF(false, n.node, seen, p, outp)\n\n\tcase *literal:\n\t\tp.out += fmt.Sprintf(\"%q\", n.s)\n\n\tcase *group:\n\t\tif child, ok := n.expr.(*group); ok && child.mode == groupMatchOnce {\n\t\t\tbuildEBNF(false, child.expr, seen, p, outp)\n\t\t} else if child, ok := n.expr.(*capture); ok {\n\t\t\tif grandchild, ok := child.node.(*group); ok && grandchild.mode == groupMatchOnce {\n\t\t\t\tbuildEBNF(false, grandchild.expr, seen, p, outp)\n\t\t\t} else {\n\t\t\t\tbuildEBNF(false, n.expr, seen, p, outp)\n\t\t\t}\n\t\t} else {\n\t\t\tbuildEBNF(false, n.expr, seen, p, outp)\n\t\t}\n\t\tswitch n.mode {\n\t\tcase groupMatchNonEmpty:\n\t\t\tp.out += \"!\"\n\t\tcase groupMatchZeroOrOne:\n\t\t\tp.out += \"?\"\n\t\tcase groupMatchZeroOrMore:\n\t\t\tp.out += \"*\"\n\t\tcase groupMatchOneOrMore:\n\t\t\tp.out += \"+\"\n\t\tcase groupMatchOnce:\n\t\t}\n\n\tcase *lookaheadGroup:\n\t\tif !n.negative {\n\t\t\tp.out += \"(?= \"\n\t\t} else {\n\t\t\tp.out += \"(?! \"\n\t\t}\n\t\tbuildEBNF(true, n.expr, seen, p, outp)\n\t\tp.out += \")\"\n\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"unsupported node type %T\", n))\n\t}\n}\n"
        },
        {
          "name": "ebnf",
          "type": "tree",
          "content": null
        },
        {
          "name": "ebnf_test.go",
          "type": "blob",
          "size": 1.806640625,
          "content": "package participle_test\n\nimport (\n\t\"strings\"\n\t\"testing\"\n\n\trequire \"github.com/alecthomas/assert/v2\"\n\t\"github.com/alecthomas/participle/v2\"\n)\n\nfunc TestEBNF(t *testing.T) {\n\tparser := mustTestParser[EBNF](t)\n\texpected := `\nEBNF = Production* .\nProduction = <ident> \"=\" Expression+ \".\" .\nExpression = Sequence (\"|\" Sequence)* .\nSequence = Term+ .\nTerm = <ident> | Literal | Range | Group | LookaheadGroup | EBNFOption | Repetition | Negation .\nLiteral = <string> .\nRange = <string> \"â€¦\" <string> .\nGroup = \"(\" Expression \")\" .\nLookaheadGroup = \"(\" \"?\" (\"=\" | \"!\") Expression \")\" .\nEBNFOption = \"[\" Expression \"]\" .\nRepetition = \"{\" Expression \"}\" .\nNegation = \"!\" Expression .\n`\n\trequire.Equal(t, strings.TrimSpace(expected), parser.String())\n}\n\nfunc TestEBNF_Other(t *testing.T) {\n\ttype Grammar struct {\n\t\tPositiveLookahead string `  (?= 'good') @Ident`\n\t\tNegativeLookahead string `| (?! 'bad' | \"worse\") @Ident`\n\t\tNegation          string `| !(\"anything\" | 'but')`\n\t}\n\n\tparser := mustTestParser[Grammar](t)\n\texpected := `Grammar = ((?= \"good\") <ident>) | ((?! \"bad\" | \"worse\") <ident>) | ~(\"anything\" | \"but\") .`\n\trequire.Equal(t, expected, parser.String())\n}\n\ntype (\n\tEBNFUnion interface{ ebnfUnion() }\n\n\tEBNFUnionA struct {\n\t\tA string `@Ident`\n\t}\n\n\tEBNFUnionB struct {\n\t\tB string `@String`\n\t}\n\n\tEBNFUnionC struct {\n\t\tC string `@Float`\n\t}\n)\n\nfunc (EBNFUnionA) ebnfUnion() {}\nfunc (EBNFUnionB) ebnfUnion() {}\nfunc (EBNFUnionC) ebnfUnion() {}\n\nfunc TestEBNF_Union(t *testing.T) {\n\ttype Grammar struct {\n\t\tTheUnion EBNFUnion `@@`\n\t}\n\n\tparser := mustTestParser[Grammar](t, participle.Union[EBNFUnion](EBNFUnionA{}, EBNFUnionB{}, EBNFUnionC{}))\n\trequire.Equal(t,\n\t\tstrings.TrimSpace(`\nGrammar = EBNFUnion .\nEBNFUnion = EBNFUnionA | EBNFUnionB | EBNFUnionC .\nEBNFUnionA = <ident> .\nEBNFUnionB = <string> .\nEBNFUnionC = <float> .\n`),\n\t\tparser.String())\n}\n"
        },
        {
          "name": "error.go",
          "type": "blob",
          "size": 3.0302734375,
          "content": "package participle\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/alecthomas/participle/v2/lexer\"\n)\n\n// Error represents an error while parsing.\n//\n// The format of an Error is in the form \"[<filename>:][<line>:<pos>:] <message>\".\n//\n// The error will contain positional information if available.\ntype Error interface {\n\terror\n\t// Unadorned message.\n\tMessage() string\n\t// Closest position to error location.\n\tPosition() lexer.Position\n}\n\n// FormatError formats an error in the form \"[<filename>:][<line>:<pos>:] <message>\"\nfunc FormatError(err Error) string {\n\tmsg := \"\"\n\tpos := err.Position()\n\tif pos.Filename != \"\" {\n\t\tmsg += pos.Filename + \":\"\n\t}\n\tif pos.Line != 0 || pos.Column != 0 {\n\t\tmsg += fmt.Sprintf(\"%d:%d:\", pos.Line, pos.Column)\n\t}\n\tif msg != \"\" {\n\t\tmsg += \" \" + err.Message()\n\t} else {\n\t\tmsg = err.Message()\n\t}\n\treturn msg\n}\n\n// UnexpectedTokenError is returned by Parse when an unexpected token is encountered.\n//\n// This is useful for composing parsers in order to detect when a sub-parser has terminated.\ntype UnexpectedTokenError struct {\n\tUnexpected lexer.Token\n\tExpect     string\n\texpectNode node // Usable instead of Expect, delays creating the string representation until necessary\n}\n\nfunc (u *UnexpectedTokenError) Error() string { return FormatError(u) }\n\nfunc (u *UnexpectedTokenError) Message() string { // nolint: golint\n\tvar expected string\n\tif u.expectNode != nil {\n\t\texpected = fmt.Sprintf(\" (expected %s)\", u.expectNode)\n\t} else if u.Expect != \"\" {\n\t\texpected = fmt.Sprintf(\" (expected %s)\", u.Expect)\n\t}\n\treturn fmt.Sprintf(\"unexpected token %q%s\", u.Unexpected, expected)\n}\nfunc (u *UnexpectedTokenError) Position() lexer.Position { return u.Unexpected.Pos } // nolint: golint\n\n// ParseError is returned when a parse error occurs.\n//\n// It is useful for differentiating between parse errors and other errors such\n// as lexing and IO errors.\ntype ParseError struct {\n\tMsg string\n\tPos lexer.Position\n}\n\nfunc (p *ParseError) Error() string            { return FormatError(p) }\nfunc (p *ParseError) Message() string          { return p.Msg }\nfunc (p *ParseError) Position() lexer.Position { return p.Pos }\n\n// Errorf creates a new Error at the given position.\nfunc Errorf(pos lexer.Position, format string, args ...interface{}) Error {\n\treturn &ParseError{Msg: fmt.Sprintf(format, args...), Pos: pos}\n}\n\ntype wrappingParseError struct {\n\terr error\n\tParseError\n}\n\nfunc (w *wrappingParseError) Unwrap() error { return w.err }\n\n// Wrapf attempts to wrap an existing error in a new message.\n//\n// If \"err\" is a participle.Error, its positional information will be used and\n// \"pos\" will be ignored.\n//\n// The returned error implements the Unwrap() method supported by the errors package.\nfunc Wrapf(pos lexer.Position, err error, format string, args ...interface{}) Error {\n\tvar msg string\n\tif perr, ok := err.(Error); ok {\n\t\tpos = perr.Position()\n\t\tmsg = fmt.Sprintf(\"%s: %s\", fmt.Sprintf(format, args...), perr.Message())\n\t} else {\n\t\tmsg = fmt.Sprintf(\"%s: %s\", fmt.Sprintf(format, args...), err.Error())\n\t}\n\treturn &wrappingParseError{err: err, ParseError: ParseError{Msg: msg, Pos: pos}}\n}\n"
        },
        {
          "name": "error_test.go",
          "type": "blob",
          "size": 2.5234375,
          "content": "package participle_test\n\nimport (\n\t\"errors\"\n\t\"testing\"\n\n\trequire \"github.com/alecthomas/assert/v2\"\n\n\t\"github.com/alecthomas/participle/v2\"\n\t\"github.com/alecthomas/participle/v2/lexer\"\n)\n\nfunc TestErrorReporting(t *testing.T) {\n\ttype cls struct {\n\t\tVisibility string   `@\"public\"?`\n\t\tClass      string   `\"class\" @Ident`\n\t\tBases      []string `('(' @Ident (',' @Ident)+ ')')?`\n\t}\n\ttype union struct {\n\t\tVisibility string `@\"public\"?`\n\t\tUnion      string `\"union\" @Ident`\n\t}\n\ttype decl struct {\n\t\tClass *cls   `(  @@`\n\t\tUnion *union ` | @@ )`\n\t}\n\ttype grammar struct {\n\t\tDecls []*decl `( @@ \";\" )*`\n\t}\n\tp := mustTestParser[grammar](t, participle.UseLookahead(5))\n\n\tast, err := p.ParseString(\"\", `public class A(B, C); class D; public union A;`)\n\trequire.NoError(t, err)\n\trequire.Equal(t, &grammar{Decls: []*decl{\n\t\t{Class: &cls{Visibility: \"public\", Class: \"A\", Bases: []string{\"B\", \"C\"}}},\n\t\t{Class: &cls{Class: \"D\"}},\n\t\t{Union: &union{Visibility: \"public\", Union: \"A\"}},\n\t}}, ast)\n\n\t_, err = p.ParseString(\"\", `public struct Bar;`)\n\trequire.EqualError(t, err, `1:8: unexpected token \"struct\" (expected \"union\" <ident>)`)\n\t_, err = p.ParseString(\"\", `public class 1;`)\n\trequire.EqualError(t, err, `1:14: unexpected token \"1\" (expected <ident> (\"(\" <ident> (\",\" <ident>)+ \")\")?)`)\n\t_, err = p.ParseString(\"\", `public class A(B,C,);`)\n\trequire.EqualError(t, err, `1:20: unexpected token \")\" (expected <ident>)`)\n}\n\nfunc TestMoreThanOneErrors(t *testing.T) {\n\ttype unionMatchAtLeastOnce struct {\n\t\tIdent  string  `( @Ident `\n\t\tString string  `| @String+ `\n\t\tFloat  float64 `| @Float )`\n\t}\n\ttype union struct {\n\t\tIdent  string  `( @Ident `\n\t\tString string  `| @String `\n\t\tFloat  float64 `| @Float )`\n\t}\n\n\tpAtLeastOnce := mustTestParser[unionMatchAtLeastOnce](t, participle.Unquote(\"String\"))\n\tp := mustTestParser[union](t, participle.Unquote(\"String\"))\n\n\tast, err := pAtLeastOnce.ParseString(\"\", `\"a string\" \"two strings\"`)\n\trequire.NoError(t, err)\n\trequire.Equal(t, &unionMatchAtLeastOnce{String: \"a stringtwo strings\"}, ast)\n\n\t_, err = p.ParseString(\"\", `102`)\n\trequire.EqualError(t, err, `1:1: unexpected token \"102\"`)\n\n\t_, err = pAtLeastOnce.ParseString(\"\", `102`)\n\t// ensure we don't get a \"+1:1: sub-expression <string>+ must match at least once\" error\n\trequire.EqualError(t, err, `1:1: unexpected token \"102\"`)\n}\n\nfunc TestErrorWrap(t *testing.T) {\n\texpected := errors.New(\"badbad\")\n\terr := participle.Wrapf(lexer.Position{Line: 1, Column: 1}, expected, \"bad: %s\", \"thing\")\n\trequire.Equal(t, expected, errors.Unwrap(err))\n\trequire.Equal(t, \"1:1: bad: thing: badbad\", err.Error())\n}\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 0.1923828125,
          "content": "module github.com/alecthomas/participle/v2\n\ngo 1.18\n\nrequire (\n\tgithub.com/alecthomas/assert/v2 v2.6.0\n\tgithub.com/alecthomas/repr v0.4.0\n)\n\nrequire github.com/hexops/gotextdiff v1.0.3 // indirect\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 0.5146484375,
          "content": "github.com/alecthomas/assert/v2 v2.6.0 h1:o3WJwILtexrEUk3cUVal3oiQY2tfgr/FHWiz/v2n4FU=\ngithub.com/alecthomas/assert/v2 v2.6.0/go.mod h1:Bze95FyfUr7x34QZrjL+XP+0qgp/zg8yS+TtBj1WA3k=\ngithub.com/alecthomas/repr v0.4.0 h1:GhI2A8MACjfegCPVq9f1FLvIBS+DrQ2KQBFZP1iFzXc=\ngithub.com/alecthomas/repr v0.4.0/go.mod h1:Fr0507jx4eOXV7AlPV6AVZLYrLIuIeSOWtW57eE/O/4=\ngithub.com/hexops/gotextdiff v1.0.3 h1:gitA9+qJrrTCsiCl7+kh75nPqQt1cx4ZkudSTLoUqJM=\ngithub.com/hexops/gotextdiff v1.0.3/go.mod h1:pSWU5MAI3yDq+fZBTazCSJysOMbxWL1BSow5/V2vxeg=\n"
        },
        {
          "name": "grammar.go",
          "type": "blob",
          "size": 10.677734375,
          "content": "package participle\n\nimport (\n\t\"fmt\"\n\t\"reflect\"\n\t\"text/scanner\"\n\n\t\"github.com/alecthomas/participle/v2/lexer\"\n)\n\ntype generatorContext struct {\n\tlexer.Definition\n\ttypeNodes    map[reflect.Type]node\n\tsymbolsToIDs map[lexer.TokenType]string\n}\n\nfunc newGeneratorContext(lex lexer.Definition) *generatorContext {\n\treturn &generatorContext{\n\t\tDefinition:   lex,\n\t\ttypeNodes:    map[reflect.Type]node{},\n\t\tsymbolsToIDs: lexer.SymbolsByRune(lex),\n\t}\n}\n\nfunc (g *generatorContext) addUnionDefs(defs []unionDef) error {\n\tunionNodes := make([]*union, len(defs))\n\tfor i, def := range defs {\n\t\tif _, exists := g.typeNodes[def.typ]; exists {\n\t\t\treturn fmt.Errorf(\"duplicate definition for interface or union type %s\", def.typ)\n\t\t}\n\t\tunionNode := &union{\n\t\t\tunionDef:    def,\n\t\t\tdisjunction: disjunction{nodes: make([]node, 0, len(def.members))},\n\t\t}\n\t\tg.typeNodes[def.typ], unionNodes[i] = unionNode, unionNode\n\t}\n\tfor i, def := range defs {\n\t\tunionNode := unionNodes[i]\n\t\tfor _, memberType := range def.members {\n\t\t\tmemberNode, err := g.parseType(memberType)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tunionNode.disjunction.nodes = append(unionNode.disjunction.nodes, memberNode)\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (g *generatorContext) addCustomDefs(defs []customDef) error {\n\tfor _, def := range defs {\n\t\tif _, exists := g.typeNodes[def.typ]; exists {\n\t\t\treturn fmt.Errorf(\"duplicate definition for interface or union type %s\", def.typ)\n\t\t}\n\t\tg.typeNodes[def.typ] = &custom{typ: def.typ, parseFn: def.parseFn}\n\t}\n\treturn nil\n}\n\n// Takes a type and builds a tree of nodes out of it.\nfunc (g *generatorContext) parseType(t reflect.Type) (_ node, returnedError error) {\n\tt = indirectType(t)\n\tif n, ok := g.typeNodes[t]; ok {\n\t\tif s, ok := n.(*strct); ok {\n\t\t\ts.usages++\n\t\t}\n\t\treturn n, nil\n\t}\n\tif t.Implements(parseableType) {\n\t\treturn &parseable{t.Elem()}, nil\n\t}\n\tif reflect.PtrTo(t).Implements(parseableType) {\n\t\treturn &parseable{t}, nil\n\t}\n\tswitch t.Kind() { // nolint: exhaustive\n\tcase reflect.Slice, reflect.Ptr:\n\t\tt = indirectType(t.Elem())\n\t\tif t.Kind() != reflect.Struct {\n\t\t\treturn nil, fmt.Errorf(\"expected a struct but got %T\", t)\n\t\t}\n\t\tfallthrough\n\n\tcase reflect.Struct:\n\t\tslexer, err := lexStruct(t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tout := newStrct(t)\n\t\tg.typeNodes[t] = out // Ensure we avoid infinite recursion.\n\t\tif slexer.NumField() == 0 {\n\t\t\treturn nil, fmt.Errorf(\"can not parse into empty struct %s\", t)\n\t\t}\n\t\tdefer decorate(&returnedError, func() string { return slexer.Field().Name })\n\t\te, err := g.parseDisjunction(slexer)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif e == nil {\n\t\t\treturn nil, fmt.Errorf(\"no grammar found in %s\", t)\n\t\t}\n\t\tif token, _ := slexer.Peek(); !token.EOF() {\n\t\t\treturn nil, fmt.Errorf(\"unexpected input %q\", token.Value)\n\t\t}\n\t\tout.expr = e\n\t\treturn out, nil\n\t}\n\treturn nil, fmt.Errorf(\"%s should be a struct or should implement the Parseable interface\", t)\n}\n\nfunc (g *generatorContext) parseDisjunction(slexer *structLexer) (node, error) {\n\tout := &disjunction{}\n\tfor {\n\t\tn, err := g.parseSequence(slexer)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif n == nil {\n\t\t\treturn nil, fmt.Errorf(\"alternative expression %d cannot be empty\", len(out.nodes)+1)\n\t\t}\n\t\tout.nodes = append(out.nodes, n)\n\t\tif token, _ := slexer.Peek(); token.Type != '|' {\n\t\t\tbreak\n\t\t}\n\t\t_, err = slexer.Next() // |\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tif len(out.nodes) == 1 {\n\t\treturn out.nodes[0], nil\n\t}\n\treturn out, nil\n}\n\nfunc (g *generatorContext) parseSequence(slexer *structLexer) (node, error) {\n\thead := &sequence{}\n\tcursor := head\nloop:\n\tfor {\n\t\tif token, err := slexer.Peek(); err != nil {\n\t\t\treturn nil, err\n\t\t} else if token.Type == lexer.EOF {\n\t\t\tbreak loop\n\t\t}\n\t\tterm, err := g.parseTerm(slexer, true)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif term == nil {\n\t\t\tbreak loop\n\t\t}\n\t\tif cursor.node == nil {\n\t\t\tcursor.head = true\n\t\t\tcursor.node = term\n\t\t} else {\n\t\t\tcursor.next = &sequence{node: term}\n\t\t\tcursor = cursor.next\n\t\t}\n\t}\n\tif head.node == nil {\n\t\treturn nil, nil\n\t}\n\tif head.next == nil {\n\t\treturn head.node, nil\n\t}\n\treturn head, nil\n}\n\nfunc (g *generatorContext) parseTermNoModifiers(slexer *structLexer, allowUnknown bool) (node, error) {\n\tt, err := slexer.Peek()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tswitch t.Type {\n\tcase '@':\n\t\treturn g.parseCapture(slexer)\n\tcase scanner.String, scanner.RawString, scanner.Char:\n\t\treturn g.parseLiteral(slexer)\n\tcase '!', '~':\n\t\treturn g.parseNegation(slexer)\n\tcase '[':\n\t\treturn g.parseOptional(slexer)\n\tcase '{':\n\t\treturn g.parseRepetition(slexer)\n\tcase '(':\n\t\t// Also handles (? used for lookahead groups\n\t\treturn g.parseGroup(slexer)\n\tcase scanner.Ident:\n\t\treturn g.parseReference(slexer)\n\tcase lexer.EOF:\n\t\t_, _ = slexer.Next()\n\t\treturn nil, nil\n\tdefault:\n\t\tif allowUnknown {\n\t\t\treturn nil, nil\n\t\t}\n\t\treturn nil, fmt.Errorf(\"unexpected token %v\", t)\n\t}\n}\n\nfunc (g *generatorContext) parseTerm(slexer *structLexer, allowUnknown bool) (node, error) {\n\tout, err := g.parseTermNoModifiers(slexer, allowUnknown)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn g.parseModifier(slexer, out)\n}\n\n// Parse modifiers: ?, *, + and/or !\nfunc (g *generatorContext) parseModifier(slexer *structLexer, expr node) (node, error) {\n\tout := &group{expr: expr}\n\tt, err := slexer.Peek()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tswitch t.Type {\n\tcase '!':\n\t\tout.mode = groupMatchNonEmpty\n\tcase '+':\n\t\tout.mode = groupMatchOneOrMore\n\tcase '*':\n\t\tout.mode = groupMatchZeroOrMore\n\tcase '?':\n\t\tout.mode = groupMatchZeroOrOne\n\tdefault:\n\t\treturn expr, nil\n\t}\n\t_, _ = slexer.Next()\n\treturn out, nil\n}\n\n// @<expression> captures <expression> into the current field.\nfunc (g *generatorContext) parseCapture(slexer *structLexer) (node, error) {\n\t_, _ = slexer.Next()\n\ttoken, err := slexer.Peek()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfield := slexer.Field()\n\tif token.Type == '@' {\n\t\t_, _ = slexer.Next()\n\t\tn, err := g.parseType(field.Type)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn &capture{field, n}, nil\n\t}\n\tft := indirectType(field.Type)\n\tif ft.Kind() == reflect.Struct && ft != tokenType && ft != tokensType && !implements(ft, captureType) && !implements(ft, textUnmarshalerType) {\n\t\treturn nil, fmt.Errorf(\"%s: structs can only be parsed with @@ or by implementing the Capture or encoding.TextUnmarshaler interfaces\", ft)\n\t}\n\tn, err := g.parseTermNoModifiers(slexer, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &capture{field, n}, nil\n}\n\n// A reference in the form <identifier> refers to a named token from the lexer.\nfunc (g *generatorContext) parseReference(slexer *structLexer) (node, error) { // nolint: interfacer\n\ttoken, err := slexer.Next()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif token.Type != scanner.Ident {\n\t\treturn nil, fmt.Errorf(\"expected identifier but got %q\", token)\n\t}\n\ttyp, ok := g.Symbols()[token.Value]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"unknown token type %q\", token)\n\t}\n\treturn &reference{typ: typ, identifier: token.Value}, nil\n}\n\n// [ <expression> ] optionally matches <expression>.\nfunc (g *generatorContext) parseOptional(slexer *structLexer) (node, error) {\n\t_, _ = slexer.Next() // [\n\tdisj, err := g.parseDisjunction(slexer)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tn := &group{expr: disj, mode: groupMatchZeroOrOne}\n\tnext, err := slexer.Next()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif next.Type != ']' {\n\t\treturn nil, fmt.Errorf(\"expected ] but got %q\", next)\n\t}\n\treturn n, nil\n}\n\n// { <expression> } matches 0 or more repititions of <expression>\nfunc (g *generatorContext) parseRepetition(slexer *structLexer) (node, error) {\n\t_, _ = slexer.Next() // {\n\tdisj, err := g.parseDisjunction(slexer)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tn := &group{expr: disj, mode: groupMatchZeroOrMore}\n\tnext, err := slexer.Next()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif next.Type != '}' {\n\t\treturn nil, fmt.Errorf(\"expected } but got %q\", next)\n\t}\n\treturn n, nil\n}\n\n// ( <expression> ) groups a sub-expression\nfunc (g *generatorContext) parseGroup(slexer *structLexer) (node, error) {\n\t_, _ = slexer.Next() // (\n\tpeek, err := slexer.Peek()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif peek.Type == '?' {\n\t\treturn g.subparseLookaheadGroup(slexer) // If there was an error peeking, code below will handle it\n\t}\n\texpr, err := g.subparseGroup(slexer)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &group{expr: expr}, nil\n}\n\n// (?[!=] <expression> ) requires a grouped sub-expression either matches or doesn't match, without consuming it\nfunc (g *generatorContext) subparseLookaheadGroup(slexer *structLexer) (node, error) {\n\t_, _ = slexer.Next() // ? - the opening ( was already consumed in parseGroup\n\tvar negative bool\n\tnext, err := slexer.Next()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tswitch next.Type {\n\tcase '=':\n\t\tnegative = false\n\tcase '!':\n\t\tnegative = true\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"expected = or ! but got %q\", next)\n\t}\n\texpr, err := g.subparseGroup(slexer)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &lookaheadGroup{expr: expr, negative: negative}, nil\n}\n\n// helper parsing <expression> ) to finish parsing groups or lookahead groups\nfunc (g *generatorContext) subparseGroup(slexer *structLexer) (node, error) {\n\tdisj, err := g.parseDisjunction(slexer)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tnext, err := slexer.Next() // )\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif next.Type != ')' {\n\t\treturn nil, fmt.Errorf(\"expected ) but got %q\", next)\n\t}\n\treturn disj, nil\n}\n\n// A token negation\n//\n// Accepts both the form !\"some-literal\" and !SomeNamedToken\nfunc (g *generatorContext) parseNegation(slexer *structLexer) (node, error) {\n\t_, _ = slexer.Next() // advance the parser since we have '!' right now.\n\tnext, err := g.parseTermNoModifiers(slexer, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &negation{next}, nil\n}\n\n// A literal string.\n//\n// Note that for this to match, the tokeniser must be able to produce this string. For example,\n// if the tokeniser only produces individual characters but the literal is \"hello\", or vice versa.\nfunc (g *generatorContext) parseLiteral(lex *structLexer) (node, error) { // nolint: interfacer\n\ttoken, err := lex.Next()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ts := token.Value\n\tt := lexer.TokenType(-1)\n\ttoken, err = lex.Peek()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif token.Type == ':' {\n\t\t_, _ = lex.Next()\n\t\ttoken, err = lex.Next()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif token.Type != scanner.Ident {\n\t\t\treturn nil, fmt.Errorf(\"expected identifier for literal type constraint but got %q\", token)\n\t\t}\n\t\tvar ok bool\n\t\tt, ok = g.Symbols()[token.Value]\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"unknown token type %q in literal type constraint\", token)\n\t\t}\n\t}\n\treturn &literal{s: s, t: t, tt: g.symbolsToIDs[t]}, nil\n}\n\nfunc indirectType(t reflect.Type) reflect.Type {\n\tif t.Kind() == reflect.Ptr || t.Kind() == reflect.Slice {\n\t\treturn indirectType(t.Elem())\n\t}\n\treturn t\n}\n\nfunc implements(t, i reflect.Type) bool {\n\treturn t.Implements(i) || reflect.PtrTo(t).Implements(i)\n}\n"
        },
        {
          "name": "grammar_test.go",
          "type": "blob",
          "size": 1.7470703125,
          "content": "package participle_test\n\nimport (\n\t\"testing\"\n\n\trequire \"github.com/alecthomas/assert/v2\"\n\t\"github.com/alecthomas/participle/v2\"\n)\n\nfunc TestBuild_Errors_Negation(t *testing.T) {\n\ttype grammar struct {\n\t\tWhatever string `'a' | ! | 'b'`\n\t}\n\t_, err := participle.Build[grammar]()\n\trequire.EqualError(t, err, \"Whatever: unexpected token |\")\n}\n\nfunc TestBuild_Errors_Capture(t *testing.T) {\n\ttype grammar struct {\n\t\tWhatever string `'a' | @ | 'b'`\n\t}\n\t_, err := participle.Build[grammar]()\n\trequire.EqualError(t, err, \"Whatever: unexpected token |\")\n}\n\nfunc TestBuild_Errors_UnclosedGroup(t *testing.T) {\n\ttype grammar struct {\n\t\tWhatever string `'a' | ('b' | 'c'`\n\t}\n\t_, err := participle.Build[grammar]()\n\trequire.EqualError(t, err, `Whatever: expected ) but got \"<EOF>\"`)\n}\n\nfunc TestBuild_Errors_LookaheadGroup(t *testing.T) {\n\ttype grammar struct {\n\t\tWhatever string `'a' | (?? 'what') | 'b'`\n\t}\n\t_, err := participle.Build[grammar]()\n\trequire.EqualError(t, err, `Whatever: expected = or ! but got \"?\"`)\n}\n\nfunc TestBuild_Colon_OK(t *testing.T) {\n\ttype grammar struct {\n\t\tTokenTypeTest bool   `  'TokenTypeTest'  :   Ident`\n\t\tDoubleCapture string `| 'DoubleCapture' \":\" @Ident`\n\t\tSinglePresent bool   `| 'SinglePresent' ':'  Ident`\n\t\tSingleCapture string `| 'SingleCapture' ':' @Ident`\n\t}\n\tparser, err := participle.Build[grammar]()\n\trequire.NoError(t, err)\n\trequire.Equal(t, `Grammar = \"TokenTypeTest\"`+\n\t\t` | (\"DoubleCapture\" \":\" <ident>)`+\n\t\t` | (\"SinglePresent\" \":\" <ident>)`+\n\t\t` | (\"SingleCapture\" \":\" <ident>) .`, parser.String())\n}\n\nfunc TestBuild_Colon_MissingTokenType(t *testing.T) {\n\ttype grammar struct {\n\t\tKey string `'name' : @Ident`\n\t}\n\t_, err := participle.Build[grammar]()\n\trequire.EqualError(t, err, `Key: expected identifier for literal type constraint but got \"@\"`)\n}\n"
        },
        {
          "name": "lexer",
          "type": "tree",
          "content": null
        },
        {
          "name": "lookahead_test.go",
          "type": "blob",
          "size": 8.7158203125,
          "content": "package participle_test\n\nimport (\n\t\"testing\"\n\n\trequire \"github.com/alecthomas/assert/v2\"\n\t\"github.com/alecthomas/participle/v2\"\n)\n\nfunc TestIssue3Example1(t *testing.T) {\n\ttype LAT1Decl struct {\n\t\tSourceFilename string `  \"source_filename\" \"=\" @String`\n\t\tDataLayout     string `| \"target\" \"datalayout\" \"=\" @String`\n\t\tTargetTriple   string `| \"target\" \"triple\" \"=\" @String`\n\t}\n\n\ttype LAT1Module struct {\n\t\tDecls []*LAT1Decl `@@*`\n\t}\n\n\tp := mustTestParser[LAT1Module](t, participle.UseLookahead(5), participle.Unquote())\n\tg, err := p.ParseString(\"\", `\n\t\tsource_filename = \"foo.c\"\n\t\ttarget datalayout = \"bar\"\n\t\ttarget triple = \"baz\"\n\t`)\n\trequire.NoError(t, err)\n\trequire.Equal(t,\n\t\t&LAT1Module{\n\t\t\tDecls: []*LAT1Decl{\n\t\t\t\t{SourceFilename: \"foo.c\"},\n\t\t\t\t{DataLayout: \"bar\"},\n\t\t\t\t{TargetTriple: \"baz\"},\n\t\t\t},\n\t\t}, g)\n}\n\ntype LAT2Config struct {\n\tEntries []*LAT2Entry `@@+`\n}\n\ntype LAT2Entry struct {\n\tAttribute *LAT2Attribute `@@`\n\tGroup     *LAT2Group     `| @@`\n}\n\ntype LAT2Attribute struct {\n\tKey   string `@Ident \"=\"`\n\tValue string `@String`\n}\n\ntype LAT2Group struct {\n\tName    string       `@Ident \"{\"`\n\tEntries []*LAT2Entry `@@+ \"}\"`\n}\n\nfunc TestIssue3Example2(t *testing.T) {\n\tp := mustTestParser[LAT2Config](t, participle.UseLookahead(2), participle.Unquote())\n\tg, err := p.ParseString(\"\", `\n\t\tkey = \"value\"\n\t\tblock {\n\t\t\tinner_key = \"inner_value\"\n\t\t}\n\t`)\n\trequire.NoError(t, err)\n\trequire.Equal(t,\n\t\t&LAT2Config{\n\t\t\tEntries: []*LAT2Entry{\n\t\t\t\t{Attribute: &LAT2Attribute{Key: \"key\", Value: \"value\"}},\n\t\t\t\t{\n\t\t\t\t\tGroup: &LAT2Group{\n\t\t\t\t\t\tName: \"block\",\n\t\t\t\t\t\tEntries: []*LAT2Entry{\n\t\t\t\t\t\t\t{Attribute: &LAT2Attribute{Key: \"inner_key\", Value: \"inner_value\"}},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\tg,\n\t)\n}\n\ntype LAT3Grammar struct {\n\tExpenses []*LAT3Expense `{ @@ }`\n}\n\ntype LAT3Expense struct {\n\tName   string     `@Ident \"paid\"`\n\tAmount *LAT3Value `@@ Ident* \".\"`\n}\n\ntype LAT3Value struct {\n\tFloat   float64 `  \"$\" @Float`\n\tInteger int     `| \"$\" @Int`\n}\n\nfunc TestIssue11(t *testing.T) {\n\tp := mustTestParser[LAT3Grammar](t, participle.UseLookahead(5))\n\tg, err := p.ParseString(\"\", `\n\t\tA paid $30.80 for snacks.\n\t\tB paid $70 for housecleaning.\n\t\tC paid $63.50 for utilities.\n\t`)\n\trequire.NoError(t, err)\n\trequire.Equal(t,\n\t\tg,\n\t\t&LAT3Grammar{\n\t\t\tExpenses: []*LAT3Expense{\n\t\t\t\t{Name: \"A\", Amount: &LAT3Value{Float: 30.8}},\n\t\t\t\t{Name: \"B\", Amount: &LAT3Value{Integer: 70}},\n\t\t\t\t{Name: \"C\", Amount: &LAT3Value{Float: 63.5}},\n\t\t\t},\n\t\t},\n\t)\n}\n\nfunc TestLookaheadOptional(t *testing.T) {\n\ttype grammar struct {\n\t\tKey   string `[ @Ident \"=\" ]`\n\t\tValue string `@Ident`\n\t}\n\tp := mustTestParser[grammar](t, participle.UseLookahead(5))\n\tactual, err := p.ParseString(\"\", `value`)\n\trequire.NoError(t, err)\n\trequire.Equal(t, &grammar{Value: \"value\"}, actual)\n\tactual, err = p.ParseString(\"\", `key = value`)\n\trequire.NoError(t, err)\n\trequire.Equal(t, &grammar{Key: \"key\", Value: \"value\"}, actual)\n}\n\nfunc TestLookaheadOptionalNoTail(t *testing.T) {\n\ttype grammar struct {\n\t\tKey   string `@Ident`\n\t\tValue string `[ \"=\" @Int ]`\n\t}\n\tp := mustTestParser[grammar](t, participle.UseLookahead(5))\n\t_, err := p.ParseString(\"\", `key`)\n\trequire.NoError(t, err)\n}\n\nfunc TestLookaheadDisjunction(t *testing.T) {\n\ttype grammar struct {\n\t\tB string `  \"hello\" @Ident \"world\"`\n\t\tC string `| \"hello\" \"world\" @Ident`\n\t\tA string `| \"hello\" @Ident`\n\t}\n\tp := mustTestParser[grammar](t, participle.UseLookahead(5))\n\n\tg, err := p.ParseString(\"\", `hello moo`)\n\trequire.NoError(t, err)\n\trequire.Equal(t, &grammar{A: \"moo\"}, g)\n\n\tg, err = p.ParseString(\"\", `hello moo world`)\n\trequire.NoError(t, err)\n\trequire.Equal(t, &grammar{B: \"moo\"}, g)\n}\n\nfunc TestLookaheadNestedDisjunctions(t *testing.T) {\n\ttype grammar struct {\n\t\tA string `  \"hello\" ( \"foo\" @Ident | \"bar\" \"waz\" @Ident)`\n\t\tB string `| \"hello\" @\"world\"`\n\t}\n\tp := mustTestParser[grammar](t, participle.UseLookahead(5))\n\n\tg, err := p.ParseString(\"\", `hello foo FOO`)\n\trequire.NoError(t, err)\n\trequire.Equal(t, g.A, \"FOO\")\n\n\tg, err = p.ParseString(\"\", `hello world`)\n\trequire.NoError(t, err)\n\trequire.Equal(t, g.B, \"world\")\n}\n\nfunc TestLookaheadTerm(t *testing.T) {\n\ttype grammar struct {\n\t\tA string `  @Ident`\n\t\tB struct {\n\t\t\tA string `@String`\n\t\t} `| @@`\n\t\tC struct {\n\t\t\tA string `@String`\n\t\t\tB string `\"â€¦\" @String`\n\t\t} `| @@`\n\t\tD struct {\n\t\t\tA string `\"[\" @Ident \"]\"`\n\t\t} `| @@`\n\t\tE struct {\n\t\t\tA string `\"(\" @Ident \")\"`\n\t\t} `| @@`\n\t}\n\tmustTestParser[grammar](t, participle.UseLookahead(5))\n}\n\nfunc TestIssue28(t *testing.T) {\n\t// Key holds the possible key types for a kv\n\ttype issue28Key struct {\n\t\tIdent *string `@Ident \":\"`\n\t\tStr   *string `| @String \":\"`\n\t}\n\n\t// Value holds the possible values for a kv\n\ttype issue28Value struct {\n\t\tBool  *bool    `(@\"true\" | \"false\")`\n\t\tStr   *string  `| @String`\n\t\tIdent *string  `| @Ident`\n\t\tInt   *int64   `| @Int`\n\t\tFloat *float64 `| @Float`\n\t}\n\n\t// KV represents a json kv\n\ttype issue28KV struct {\n\t\tKey   *issue28Key   `@@`\n\t\tValue *issue28Value `@@`\n\t}\n\n\t// Term holds the different possible terms\n\ttype issue28Term struct {\n\t\tKV   *issue28KV ` @@ `\n\t\tText *string    `| @String `\n\t}\n\n\tp := mustTestParser[issue28Term](t, participle.UseLookahead(5), participle.Unquote())\n\n\tactual, err := p.ParseString(\"\", `\"key\": \"value\"`)\n\trequire.NoError(t, err)\n\tkey := \"key\"\n\tvalue := \"value\"\n\texpected := &issue28Term{\n\t\tKV: &issue28KV{\n\t\t\tKey: &issue28Key{\n\t\t\t\tStr: &key,\n\t\t\t},\n\t\t\tValue: &issue28Value{\n\t\t\t\tStr: &value,\n\t\t\t},\n\t\t},\n\t}\n\trequire.Equal(t, expected, actual)\n\n\tactual, err = p.ParseString(\"\", `\"some text string\"`)\n\trequire.NoError(t, err)\n\ttext := \"some text string\"\n\texpected = &issue28Term{\n\t\tText: &text,\n\t}\n\trequire.Equal(t, expected, actual)\n}\n\n// This test used to fail because the lookahead table only tracks (root, depth, token) for each root. In this case there\n// are two roots that have the same second token (0, 1, \"=\") and (2, 1, \"=\"). As (depth, token) is the uniqueness\n// constraint, this never disambiguates.\n//\n// To solve this, each ambiguous group will need to track the history of tokens.\n//\n// eg.\n//\n//  0. groups = [\n//     {history: [\">\"] roots: [0, 1]},\n//     {history: [\"<\"], roots: [2, 3]},\n//     ]\n//  1. groups = [\n//     {history: [\">\", \"=\"], roots: [0]},\n//     {history: [\">\"], roots: [1]},\n//     {history: [\"<\", \"=\"], roots: [2]},\n//     {history: [\"<\"], roots: [3]},\n//     ]\nfunc TestLookaheadWithConvergingTokens(t *testing.T) {\n\ttype grammar struct {\n\t\tLeft string   `@Ident`\n\t\tOp   string   `[ @( \">\" \"=\" | \">\" | \"<\" \"=\" | \"<\" )`\n\t\tNext *grammar `  @@ ]`\n\t}\n\tp := mustTestParser[grammar](t, participle.UseLookahead(5))\n\t_, err := p.ParseString(\"\", \"a >= b\")\n\trequire.NoError(t, err)\n}\n\nfunc TestIssue27(t *testing.T) {\n\ttype grammar struct {\n\t\tNumber int    `  @([\"-\"] Int)`\n\t\tString string `| @String`\n\t}\n\tp := mustTestParser[grammar](t)\n\tactual, err := p.ParseString(\"\", `- 100`)\n\trequire.NoError(t, err)\n\trequire.Equal(t, &grammar{Number: -100}, actual)\n\n\tactual, err = p.ParseString(\"\", `100`)\n\trequire.NoError(t, err)\n\trequire.Equal(t, &grammar{Number: 100}, actual)\n}\n\nfunc TestLookaheadDisambiguateByType(t *testing.T) {\n\ttype grammar struct {\n\t\tInt   int     `  @([\"-\"] Int)`\n\t\tFloat float64 `| @([\"-\"] Float)`\n\t}\n\n\tp := mustTestParser[grammar](t, participle.UseLookahead(5))\n\n\tactual, err := p.ParseString(\"\", `- 100`)\n\trequire.NoError(t, err)\n\trequire.Equal(t, &grammar{Int: -100}, actual)\n\n\tactual, err = p.ParseString(\"\", `- 100.5`)\n\trequire.NoError(t, err)\n\trequire.Equal(t, &grammar{Float: -100.5}, actual)\n}\n\nfunc TestShowNearestError(t *testing.T) {\n\ttype grammar struct {\n\t\tA string `  @\"a\" @\"b\" @\"c\"`\n\t\tB string `| @\"a\" @\"z\"`\n\t}\n\tp := mustTestParser[grammar](t, participle.UseLookahead(10))\n\t_, err := p.ParseString(\"\", `a b d`)\n\trequire.EqualError(t, err, `1:5: unexpected token \"d\" (expected \"c\")`)\n}\n\nfunc TestRewindDisjunction(t *testing.T) {\n\ttype grammar struct {\n\t\tFunction string `  @Ident \"(\" \")\"`\n\t\tIdent    string `| @Ident`\n\t}\n\tp := mustTestParser[grammar](t, participle.UseLookahead(2))\n\tast, err := p.ParseString(\"\", `name`)\n\trequire.NoError(t, err)\n\trequire.Equal(t, &grammar{Ident: \"name\"}, ast)\n}\n\nfunc TestRewindOptional(t *testing.T) {\n\ttype grammar struct {\n\t\tVar string `  [ \"int\" \"int\" ] @Ident`\n\t}\n\tp := mustTestParser[grammar](t, participle.UseLookahead(3))\n\n\tast, err := p.ParseString(\"\", `one`)\n\trequire.NoError(t, err)\n\trequire.Equal(t, &grammar{Var: \"one\"}, ast)\n\n\tast, err = p.ParseString(\"\", `int int one`)\n\trequire.NoError(t, err)\n\trequire.Equal(t, &grammar{Var: \"one\"}, ast)\n}\n\nfunc TestRewindRepetition(t *testing.T) {\n\ttype grammar struct {\n\t\tInts  []string `(@\"int\")*`\n\t\tIdent string   `@Ident`\n\t}\n\tp := mustTestParser[grammar](t, participle.UseLookahead(3))\n\n\tast, err := p.ParseString(\"\", `int int one`)\n\trequire.NoError(t, err)\n\trequire.Equal(t, &grammar{Ints: []string{\"int\", \"int\"}, Ident: \"one\"}, ast)\n\n\tast, err = p.ParseString(\"\", `int int one`)\n\trequire.NoError(t, err)\n\trequire.Equal(t, &grammar{Ints: []string{\"int\", \"int\"}, Ident: \"one\"}, ast)\n}\n"
        },
        {
          "name": "map.go",
          "type": "blob",
          "size": 2.603515625,
          "content": "package participle\n\nimport (\n\t\"io\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/alecthomas/participle/v2/lexer\"\n)\n\ntype mapperByToken struct {\n\tsymbols []string\n\tmapper  Mapper\n}\n\n// Mapper function for mutating tokens before being applied to the AST.\ntype Mapper func(token lexer.Token) (lexer.Token, error)\n\n// Map is an Option that configures the Parser to apply a mapping function to each Token from the lexer.\n//\n// This can be useful to eg. upper-case all tokens of a certain type, or dequote strings.\n//\n// \"symbols\" specifies the token symbols that the Mapper will be applied to. If empty, all tokens will be mapped.\nfunc Map(mapper Mapper, symbols ...string) Option {\n\treturn func(p *parserOptions) error {\n\t\tp.mappers = append(p.mappers, mapperByToken{\n\t\t\tmapper:  mapper,\n\t\t\tsymbols: symbols,\n\t\t})\n\t\treturn nil\n\t}\n}\n\n// Unquote applies strconv.Unquote() to tokens of the given types.\n//\n// Tokens of type \"String\" will be unquoted if no other types are provided.\nfunc Unquote(types ...string) Option {\n\tif len(types) == 0 {\n\t\ttypes = []string{\"String\"}\n\t}\n\treturn Map(func(t lexer.Token) (lexer.Token, error) {\n\t\tvalue, err := unquote(t.Value)\n\t\tif err != nil {\n\t\t\treturn t, Errorf(t.Pos, \"invalid quoted string %q: %s\", t.Value, err.Error())\n\t\t}\n\t\tt.Value = value\n\t\treturn t, nil\n\t}, types...)\n}\n\nfunc unquote(s string) (string, error) {\n\tquote := s[0]\n\ts = s[1 : len(s)-1]\n\tout := \"\"\n\tfor s != \"\" {\n\t\tvalue, _, tail, err := strconv.UnquoteChar(s, quote)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\ts = tail\n\t\tout += string(value)\n\t}\n\treturn out, nil\n}\n\n// Upper is an Option that upper-cases all tokens of the given type. Useful for case normalisation.\nfunc Upper(types ...string) Option {\n\treturn Map(func(token lexer.Token) (lexer.Token, error) {\n\t\ttoken.Value = strings.ToUpper(token.Value)\n\t\treturn token, nil\n\t}, types...)\n}\n\n// Elide drops tokens of the specified types.\nfunc Elide(types ...string) Option {\n\treturn func(p *parserOptions) error {\n\t\tp.elide = append(p.elide, types...)\n\t\treturn nil\n\t}\n}\n\n// Apply a Mapping to all tokens coming out of a Lexer.\ntype mappingLexerDef struct {\n\tl      lexer.Definition\n\tmapper Mapper\n}\n\nvar _ lexer.Definition = &mappingLexerDef{}\n\nfunc (m *mappingLexerDef) Symbols() map[string]lexer.TokenType { return m.l.Symbols() }\n\nfunc (m *mappingLexerDef) Lex(filename string, r io.Reader) (lexer.Lexer, error) {\n\tl, err := m.l.Lex(filename, r)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &mappingLexer{l, m.mapper}, nil\n}\n\ntype mappingLexer struct {\n\tlexer.Lexer\n\tmapper Mapper\n}\n\nfunc (m *mappingLexer) Next() (lexer.Token, error) {\n\tt, err := m.Lexer.Next()\n\tif err != nil {\n\t\treturn t, err\n\t}\n\treturn m.mapper(t)\n}\n"
        },
        {
          "name": "map_test.go",
          "type": "blob",
          "size": 1.916015625,
          "content": "package participle_test\n\nimport (\n\t\"strings\"\n\t\"testing\"\n\n\trequire \"github.com/alecthomas/assert/v2\"\n\t\"github.com/alecthomas/participle/v2\"\n\t\"github.com/alecthomas/participle/v2/lexer\"\n)\n\nfunc TestUpper(t *testing.T) {\n\ttype grammar struct {\n\t\tText string `@Ident`\n\t}\n\tdef := lexer.MustSimple([]lexer.SimpleRule{\n\t\t{\"Whitespace\", `\\s+`},\n\t\t{\"Ident\", `\\w+`},\n\t})\n\tparser := mustTestParser[grammar](t, participle.Lexer(def), participle.Upper(\"Ident\"))\n\tactual, err := parser.Lex(\"\", strings.NewReader(\"hello world\"))\n\trequire.NoError(t, err)\n\n\texpected := []lexer.Token{\n\t\t{Type: -3, Value: \"HELLO\", Pos: lexer.Position{Filename: \"\", Offset: 0, Line: 1, Column: 1}},\n\t\t{Type: -2, Value: \" \", Pos: lexer.Position{Filename: \"\", Offset: 5, Line: 1, Column: 6}},\n\t\t{Type: -3, Value: \"WORLD\", Pos: lexer.Position{Filename: \"\", Offset: 6, Line: 1, Column: 7}},\n\t\t{Type: lexer.EOF, Value: \"\", Pos: lexer.Position{Filename: \"\", Offset: 11, Line: 1, Column: 12}},\n\t}\n\n\trequire.Equal(t, expected, actual)\n}\n\nfunc TestUnquote(t *testing.T) {\n\ttype grammar struct {\n\t\tText string `@Ident`\n\t}\n\tlex := lexer.MustSimple([]lexer.SimpleRule{\n\t\t{\"whitespace\", `\\s+`},\n\t\t{\"Ident\", `\\w+`},\n\t\t{\"String\", `\\\"(?:[^\\\"]|\\\\.)*\\\"`},\n\t\t{\"RawString\", \"`[^`]*`\"},\n\t})\n\tparser := mustTestParser[grammar](t, participle.Lexer(lex), participle.Unquote(\"String\", \"RawString\"))\n\tactual, err := parser.Lex(\"\", strings.NewReader(\"hello world \\\"quoted\\\\tstring\\\" `backtick quotes`\"))\n\trequire.NoError(t, err)\n\texpected := []lexer.Token{\n\t\t{Type: -3, Value: \"hello\", Pos: lexer.Position{Line: 1, Column: 1}},\n\t\t{Type: -3, Value: \"world\", Pos: lexer.Position{Offset: 6, Line: 1, Column: 7}},\n\t\t{Type: -4, Value: \"quoted\\tstring\", Pos: lexer.Position{Offset: 12, Line: 1, Column: 13}},\n\t\t{Type: -5, Value: \"backtick quotes\", Pos: lexer.Position{Offset: 29, Line: 1, Column: 30}},\n\t\t{Type: lexer.EOF, Value: \"\", Pos: lexer.Position{Offset: 46, Line: 1, Column: 47}},\n\t}\n\trequire.Equal(t, expected, actual)\n}\n"
        },
        {
          "name": "nodes.go",
          "type": "blob",
          "size": 19.6005859375,
          "content": "package participle\n\nimport (\n\t\"encoding\"\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/alecthomas/participle/v2/lexer\"\n)\n\nvar (\n\t// MaxIterations limits the number of elements capturable by {}.\n\tMaxIterations = 1000000\n\n\tpositionType        = reflect.TypeOf(lexer.Position{})\n\ttokenType           = reflect.TypeOf(lexer.Token{})\n\ttokensType          = reflect.TypeOf([]lexer.Token{})\n\tcaptureType         = reflect.TypeOf((*Capture)(nil)).Elem()\n\ttextUnmarshalerType = reflect.TypeOf((*encoding.TextUnmarshaler)(nil)).Elem()\n\tparseableType       = reflect.TypeOf((*Parseable)(nil)).Elem()\n\n\t// NextMatch should be returned by Parseable.Parse() method implementations to indicate\n\t// that the node did not match and that other matches should be attempted, if appropriate.\n\tNextMatch = errors.New(\"no match\") // nolint: golint\n)\n\n// A node in the grammar.\ntype node interface {\n\t// Parse from scanner into value.\n\t//\n\t// Returned slice will be nil if the node does not match.\n\tParse(ctx *parseContext, parent reflect.Value) ([]reflect.Value, error)\n\n\t// Return a decent string representation of the Node.\n\tfmt.Stringer\n\n\tfmt.GoStringer\n}\n\nfunc decorate(err *error, name func() string) {\n\tif *err == nil {\n\t\treturn\n\t}\n\tif perr, ok := (*err).(Error); ok {\n\t\t*err = Errorf(perr.Position(), \"%s: %s\", name(), perr.Message())\n\t} else {\n\t\t*err = &ParseError{Msg: fmt.Sprintf(\"%s: %s\", name(), *err)}\n\t}\n}\n\n// A node that proxies to an implementation that implements the Parseable interface.\ntype parseable struct {\n\tt reflect.Type\n}\n\nfunc (p *parseable) String() string   { return ebnf(p) }\nfunc (p *parseable) GoString() string { return p.t.String() }\n\nfunc (p *parseable) Parse(ctx *parseContext, parent reflect.Value) (out []reflect.Value, err error) {\n\tdefer ctx.printTrace(p)()\n\trv := reflect.New(p.t)\n\tv := rv.Interface().(Parseable)\n\terr = v.Parse(&ctx.PeekingLexer)\n\tif err != nil {\n\t\tif err == NextMatch {\n\t\t\treturn nil, nil\n\t\t}\n\t\treturn nil, err\n\t}\n\treturn []reflect.Value{rv.Elem()}, nil\n}\n\n// @@ (but for a custom production)\ntype custom struct {\n\ttyp     reflect.Type\n\tparseFn reflect.Value\n}\n\nfunc (c *custom) String() string   { return ebnf(c) }\nfunc (c *custom) GoString() string { return c.typ.Name() }\n\nfunc (c *custom) Parse(ctx *parseContext, parent reflect.Value) (out []reflect.Value, err error) {\n\tdefer ctx.printTrace(c)()\n\tresults := c.parseFn.Call([]reflect.Value{reflect.ValueOf(&ctx.PeekingLexer)})\n\tif err, _ := results[1].Interface().(error); err != nil {\n\t\tif err == NextMatch {\n\t\t\treturn nil, nil\n\t\t}\n\t\treturn nil, err\n\t}\n\treturn []reflect.Value{results[0]}, nil\n}\n\n// @@ (for a union)\ntype union struct {\n\tunionDef\n\tdisjunction disjunction\n}\n\nfunc (u *union) String() string   { return ebnf(u) }\nfunc (u *union) GoString() string { return u.typ.Name() }\n\nfunc (u *union) Parse(ctx *parseContext, parent reflect.Value) (out []reflect.Value, err error) {\n\tdefer ctx.printTrace(u)()\n\tvals, err := u.disjunction.Parse(ctx, parent)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor i := range vals {\n\t\tvals[i] = maybeRef(u.members[i], vals[i]).Convert(u.typ)\n\t}\n\treturn vals, nil\n}\n\n// @@\ntype strct struct {\n\ttyp              reflect.Type\n\texpr             node\n\ttokensFieldIndex []int\n\tposFieldIndex    []int\n\tendPosFieldIndex []int\n\tusages           int\n}\n\nfunc newStrct(typ reflect.Type) *strct {\n\ts := &strct{\n\t\ttyp:    typ,\n\t\tusages: 1,\n\t}\n\tfield, ok := typ.FieldByName(\"Pos\")\n\tif ok && positionType.ConvertibleTo(field.Type) {\n\t\ts.posFieldIndex = field.Index\n\t}\n\tfield, ok = typ.FieldByName(\"EndPos\")\n\tif ok && positionType.ConvertibleTo(field.Type) {\n\t\ts.endPosFieldIndex = field.Index\n\t}\n\tfield, ok = typ.FieldByName(\"Tokens\")\n\tif ok && field.Type == tokensType {\n\t\ts.tokensFieldIndex = field.Index\n\t}\n\treturn s\n}\n\nfunc (s *strct) String() string   { return ebnf(s) }\nfunc (s *strct) GoString() string { return s.typ.Name() }\n\nfunc (s *strct) Parse(ctx *parseContext, parent reflect.Value) (out []reflect.Value, err error) {\n\tdefer ctx.printTrace(s)()\n\tsv := reflect.New(s.typ).Elem()\n\tstart := ctx.RawCursor()\n\tt := ctx.Peek()\n\ts.maybeInjectStartToken(t, sv)\n\tif out, err = s.expr.Parse(ctx, sv); err != nil {\n\t\t_ = ctx.Apply() // Best effort to give partial AST.\n\t\tctx.MaybeUpdateError(err)\n\t\treturn []reflect.Value{sv}, err\n\t} else if out == nil {\n\t\treturn nil, nil\n\t}\n\tend := ctx.RawCursor()\n\tt = ctx.RawPeek()\n\ts.maybeInjectEndToken(t, sv)\n\ts.maybeInjectTokens(ctx.Range(start, end), sv)\n\treturn []reflect.Value{sv}, ctx.Apply()\n}\n\nfunc (s *strct) maybeInjectStartToken(token *lexer.Token, v reflect.Value) {\n\tif s.posFieldIndex == nil {\n\t\treturn\n\t}\n\tf := v.FieldByIndex(s.posFieldIndex)\n\tf.Set(reflect.ValueOf(token.Pos).Convert(f.Type()))\n}\n\nfunc (s *strct) maybeInjectEndToken(token *lexer.Token, v reflect.Value) {\n\tif s.endPosFieldIndex == nil {\n\t\treturn\n\t}\n\tf := v.FieldByIndex(s.endPosFieldIndex)\n\tf.Set(reflect.ValueOf(token.Pos).Convert(f.Type()))\n}\n\nfunc (s *strct) maybeInjectTokens(tokens []lexer.Token, v reflect.Value) {\n\tif s.tokensFieldIndex == nil {\n\t\treturn\n\t}\n\tv.FieldByIndex(s.tokensFieldIndex).Set(reflect.ValueOf(tokens))\n}\n\ntype groupMatchMode int\n\nfunc (g groupMatchMode) String() string {\n\tswitch g {\n\tcase groupMatchOnce:\n\t\treturn \"n\"\n\tcase groupMatchZeroOrOne:\n\t\treturn \"n?\"\n\tcase groupMatchZeroOrMore:\n\t\treturn \"n*\"\n\tcase groupMatchOneOrMore:\n\t\treturn \"n+\"\n\tcase groupMatchNonEmpty:\n\t\treturn \"n!\"\n\t}\n\tpanic(\"??\")\n}\n\nconst (\n\tgroupMatchOnce       groupMatchMode = iota\n\tgroupMatchZeroOrOne                 = iota\n\tgroupMatchZeroOrMore                = iota\n\tgroupMatchOneOrMore                 = iota\n\tgroupMatchNonEmpty                  = iota\n)\n\n// ( <expr> ) - match once\n// ( <expr> )* - match zero or more times\n// ( <expr> )+ - match one or more times\n// ( <expr> )? - match zero or once\n// ( <expr> )! - must be a non-empty match\n//\n// The additional modifier \"!\" forces the content of the group to be non-empty if it does match.\ntype group struct {\n\texpr node\n\tmode groupMatchMode\n}\n\nfunc (g *group) String() string   { return ebnf(g) }\nfunc (g *group) GoString() string { return fmt.Sprintf(\"group{%s}\", g.mode) }\nfunc (g *group) Parse(ctx *parseContext, parent reflect.Value) (out []reflect.Value, err error) {\n\tdefer ctx.printTrace(g)()\n\t// Configure min/max matches.\n\tmin := 1\n\tmax := 1\n\tswitch g.mode {\n\tcase groupMatchNonEmpty:\n\t\tout, err = g.expr.Parse(ctx, parent)\n\t\tif err != nil {\n\t\t\treturn out, err\n\t\t}\n\t\tif len(out) == 0 {\n\t\t\tt := ctx.Peek()\n\t\t\treturn out, Errorf(t.Pos, \"sub-expression %s cannot be empty\", g)\n\t\t}\n\t\treturn out, nil\n\tcase groupMatchOnce:\n\t\treturn g.expr.Parse(ctx, parent)\n\tcase groupMatchZeroOrOne:\n\t\tmin = 0\n\tcase groupMatchZeroOrMore:\n\t\tmin = 0\n\t\tmax = MaxIterations\n\tcase groupMatchOneOrMore:\n\t\tmin = 1\n\t\tmax = MaxIterations\n\t}\n\tmatches := 0\n\tfor ; matches < max; matches++ {\n\t\tbranch := ctx.Branch()\n\t\tv, err := g.expr.Parse(branch, parent)\n\t\tif err != nil {\n\t\t\tctx.MaybeUpdateError(err)\n\t\t\t// Optional part failed to match.\n\t\t\tif ctx.Stop(err, branch) {\n\t\t\t\tout = append(out, v...) // Try to return as much of the parse tree as possible\n\t\t\t\treturn out, err\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\t\tout = append(out, v...)\n\t\tctx.Accept(branch)\n\t\tif v == nil {\n\t\t\tbreak\n\t\t}\n\t}\n\t// fmt.Printf(\"%d < %d < %d: out == nil? %v\\n\", min, matches, max, out == nil)\n\tt := ctx.Peek()\n\tif matches >= MaxIterations {\n\t\treturn nil, Errorf(t.Pos, \"too many iterations of %s (> %d)\", g, MaxIterations)\n\t}\n\t// avoid returning errors in parent nodes if the group is optional\n\tif matches > 0 && matches < min {\n\t\treturn out, Errorf(t.Pos, \"sub-expression %s must match at least once\", g)\n\t}\n\t// The idea here is that something like \"a\"? is a successful match and that parsing should proceed.\n\tif min == 0 && out == nil {\n\t\tout = []reflect.Value{}\n\t}\n\treturn out, nil\n}\n\n// (?= <expr> ) for positive lookahead, (?! <expr> ) for negative lookahead; neither consumes input\ntype lookaheadGroup struct {\n\texpr     node\n\tnegative bool\n}\n\nfunc (l *lookaheadGroup) String() string   { return ebnf(l) }\nfunc (l *lookaheadGroup) GoString() string { return \"lookaheadGroup{}\" }\n\nfunc (l *lookaheadGroup) Parse(ctx *parseContext, parent reflect.Value) (out []reflect.Value, err error) {\n\tdefer ctx.printTrace(l)()\n\t// Create a branch to avoid advancing the parser as any match will be discarded\n\tbranch := ctx.Branch()\n\tout, err = l.expr.Parse(branch, parent)\n\tmatchedLookahead := err == nil && out != nil\n\texpectingMatch := !l.negative\n\tif matchedLookahead != expectingMatch {\n\t\treturn nil, &UnexpectedTokenError{Unexpected: *ctx.Peek()}\n\t}\n\treturn []reflect.Value{}, nil // Empty match slice means a match, unlike nil\n}\n\n// <expr> {\"|\" <expr>}\ntype disjunction struct {\n\tnodes []node\n}\n\nfunc (d *disjunction) String() string   { return ebnf(d) }\nfunc (d *disjunction) GoString() string { return \"disjunction{}\" }\n\nfunc (d *disjunction) Parse(ctx *parseContext, parent reflect.Value) (out []reflect.Value, err error) {\n\tdefer ctx.printTrace(d)()\n\tvar (\n\t\tdeepestError = 0\n\t\tfirstError   error\n\t\tfirstValues  []reflect.Value\n\t)\n\tfor _, a := range d.nodes {\n\t\tbranch := ctx.Branch()\n\t\tif value, err := a.Parse(branch, parent); err != nil {\n\t\t\t// If this branch progressed too far and still didn't match, error out.\n\t\t\tif ctx.Stop(err, branch) {\n\t\t\t\treturn value, err\n\t\t\t}\n\t\t\t// Show the closest error returned. The idea here is that the further the parser progresses\n\t\t\t// without error, the more difficult it is to trace the error back to its root.\n\t\t\tif branch.Cursor() >= deepestError {\n\t\t\t\tfirstError = err\n\t\t\t\tfirstValues = value\n\t\t\t\tdeepestError = branch.Cursor()\n\t\t\t}\n\t\t} else if value != nil {\n\t\t\tbt := branch.RawPeek()\n\t\t\tct := ctx.RawPeek()\n\t\t\tif bt == ct && bt.Type != lexer.EOF {\n\t\t\t\tpanic(Errorf(bt.Pos, \"branch %s was accepted but did not progress the lexer at %s (%q)\", a, bt.Pos, bt.Value))\n\t\t\t}\n\t\t\tctx.Accept(branch)\n\t\t\treturn value, nil\n\t\t}\n\t}\n\tif firstError != nil {\n\t\tctx.MaybeUpdateError(firstError)\n\t\treturn firstValues, firstError\n\t}\n\treturn nil, nil\n}\n\n// <node> ...\ntype sequence struct {\n\thead bool // True if this is the head node.\n\tnode node\n\tnext *sequence\n}\n\nfunc (s *sequence) String() string   { return ebnf(s) }\nfunc (s *sequence) GoString() string { return \"sequence{}\" }\n\nfunc (s *sequence) Parse(ctx *parseContext, parent reflect.Value) (out []reflect.Value, err error) {\n\tdefer ctx.printTrace(s)()\n\tfor n := s; n != nil; n = n.next {\n\t\tchild, err := n.node.Parse(ctx, parent)\n\t\tout = append(out, child...)\n\t\tif err != nil {\n\t\t\treturn out, err\n\t\t}\n\t\tif child == nil {\n\t\t\t// Early exit if first value doesn't match, otherwise all values must match.\n\t\t\tif n == s {\n\t\t\t\treturn nil, nil\n\t\t\t}\n\t\t\ttoken := ctx.Peek()\n\t\t\treturn out, &UnexpectedTokenError{Unexpected: *token, expectNode: n}\n\t\t}\n\t\t// Special-case for when children return an empty match.\n\t\t// Appending an empty, non-nil slice to a nil slice returns a nil slice.\n\t\t// https://go.dev/play/p/lV1Xk-IP6Ta\n\t\tif out == nil {\n\t\t\tout = []reflect.Value{}\n\t\t}\n\t}\n\treturn out, nil\n}\n\n// @<expr>\ntype capture struct {\n\tfield structLexerField\n\tnode  node\n}\n\nfunc (c *capture) String() string   { return ebnf(c) }\nfunc (c *capture) GoString() string { return \"capture{}\" }\n\nfunc (c *capture) Parse(ctx *parseContext, parent reflect.Value) (out []reflect.Value, err error) {\n\tdefer ctx.printTrace(c)()\n\tstart := ctx.RawCursor()\n\tv, err := c.node.Parse(ctx, parent)\n\tif v != nil {\n\t\tctx.Defer(ctx.Range(start, ctx.RawCursor()), parent, c.field, v)\n\t}\n\tif err != nil {\n\t\treturn []reflect.Value{parent}, err\n\t}\n\tif v == nil {\n\t\treturn nil, nil\n\t}\n\treturn []reflect.Value{parent}, nil\n}\n\n// <identifier> - named lexer token reference\ntype reference struct {\n\ttyp        lexer.TokenType\n\tidentifier string // Used for informational purposes.\n}\n\nfunc (r *reference) String() string   { return ebnf(r) }\nfunc (r *reference) GoString() string { return fmt.Sprintf(\"reference{%s}\", r.identifier) }\n\nfunc (r *reference) Parse(ctx *parseContext, parent reflect.Value) (out []reflect.Value, err error) {\n\tdefer ctx.printTrace(r)()\n\ttoken, cursor := ctx.PeekAny(func(t lexer.Token) bool {\n\t\treturn t.Type == r.typ\n\t})\n\tif token.Type != r.typ {\n\t\treturn nil, nil\n\t}\n\tctx.FastForward(cursor)\n\treturn []reflect.Value{reflect.ValueOf(token.Value)}, nil\n}\n\n// Match a token literal exactly \"...\"[:<type>].\ntype literal struct {\n\ts  string\n\tt  lexer.TokenType\n\ttt string // Used for display purposes - symbolic name of t.\n}\n\nfunc (l *literal) String() string   { return ebnf(l) }\nfunc (l *literal) GoString() string { return fmt.Sprintf(\"literal{%q, %q}\", l.s, l.tt) }\n\nfunc (l *literal) Parse(ctx *parseContext, parent reflect.Value) (out []reflect.Value, err error) {\n\tdefer ctx.printTrace(l)()\n\tmatch := func(t lexer.Token) bool {\n\t\tvar equal bool\n\t\tif ctx.caseInsensitive[t.Type] {\n\t\t\tequal = l.s == \"\" || strings.EqualFold(t.Value, l.s)\n\t\t} else {\n\t\t\tequal = l.s == \"\" || t.Value == l.s\n\t\t}\n\t\treturn (l.t == lexer.EOF || l.t == t.Type) && equal\n\t}\n\ttoken, cursor := ctx.PeekAny(match)\n\tif match(token) {\n\t\tctx.FastForward(cursor)\n\t\treturn []reflect.Value{reflect.ValueOf(token.Value)}, nil\n\t}\n\treturn nil, nil\n}\n\ntype negation struct {\n\tnode node\n}\n\nfunc (n *negation) String() string   { return ebnf(n) }\nfunc (n *negation) GoString() string { return \"negation{}\" }\n\nfunc (n *negation) Parse(ctx *parseContext, parent reflect.Value) (out []reflect.Value, err error) {\n\tdefer ctx.printTrace(n)()\n\t// Create a branch to avoid advancing the parser, but call neither Stop nor Accept on it\n\t// since we will discard a match.\n\tbranch := ctx.Branch()\n\tnotEOF := ctx.Peek()\n\tif notEOF.EOF() {\n\t\t// EOF cannot match a negation, which expects something\n\t\treturn nil, nil\n\t}\n\n\tout, err = n.node.Parse(branch, parent)\n\tif out != nil && err == nil {\n\t\t// out being non-nil means that what we don't want is actually here, so we report nomatch\n\t\treturn nil, &UnexpectedTokenError{Unexpected: *notEOF}\n\t}\n\n\t// Just give the next token\n\tnext := ctx.Next()\n\treturn []reflect.Value{reflect.ValueOf(next.Value)}, nil\n}\n\n// Attempt to transform values to given type.\n//\n// This will dereference pointers, and attempt to parse strings into integer values, floats, etc.\nfunc conform(t reflect.Type, values []reflect.Value) (out []reflect.Value, err error) {\n\tfor _, v := range values {\n\t\tfor t != v.Type() && t.Kind() == reflect.Ptr && v.Kind() != reflect.Ptr {\n\t\t\t// This can occur during partial failure.\n\t\t\tif !v.CanAddr() {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tv = v.Addr()\n\t\t}\n\n\t\t// Already of the right kind, don't bother converting.\n\t\tif v.Kind() == t.Kind() {\n\t\t\tif v.Type() != t {\n\t\t\t\tv = v.Convert(t)\n\t\t\t}\n\t\t\tout = append(out, v)\n\t\t\tcontinue\n\t\t}\n\n\t\tkind := t.Kind()\n\t\tswitch kind { // nolint: exhaustive\n\t\tcase reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64:\n\t\t\tn, err := strconv.ParseInt(v.String(), 0, sizeOfKind(kind))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tv = reflect.New(t).Elem()\n\t\t\tv.SetInt(n)\n\n\t\tcase reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64:\n\t\t\tn, err := strconv.ParseUint(v.String(), 0, sizeOfKind(kind))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tv = reflect.New(t).Elem()\n\t\t\tv.SetUint(n)\n\n\t\tcase reflect.Bool:\n\t\t\tv = reflect.ValueOf(true)\n\n\t\tcase reflect.Float32, reflect.Float64:\n\t\t\tn, err := strconv.ParseFloat(v.String(), sizeOfKind(kind))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tv = reflect.New(t).Elem()\n\t\t\tv.SetFloat(n)\n\t\t}\n\n\t\tout = append(out, v)\n\t}\n\treturn out, nil\n}\n\nfunc sizeOfKind(kind reflect.Kind) int {\n\tswitch kind { // nolint: exhaustive\n\tcase reflect.Int8, reflect.Uint8:\n\t\treturn 8\n\tcase reflect.Int16, reflect.Uint16:\n\t\treturn 16\n\tcase reflect.Int32, reflect.Uint32, reflect.Float32:\n\t\treturn 32\n\tcase reflect.Int64, reflect.Uint64, reflect.Float64:\n\t\treturn 64\n\tcase reflect.Int, reflect.Uint:\n\t\treturn strconv.IntSize\n\t}\n\tpanic(\"unsupported kind \" + kind.String())\n}\n\nfunc maybeRef(tmpl reflect.Type, strct reflect.Value) reflect.Value {\n\tif strct.Type() == tmpl {\n\t\treturn strct\n\t}\n\tif tmpl.Kind() == reflect.Ptr {\n\t\tif strct.CanAddr() {\n\t\t\treturn strct.Addr()\n\t\t}\n\t\tptr := reflect.New(tmpl)\n\t\tptr.Set(strct)\n\t\treturn ptr\n\t}\n\treturn strct\n}\n\n// Set field.\n//\n// If field is a pointer the pointer will be set to the value. If field is a string, value will be\n// appended. If field is a slice, value will be appended to slice.\n//\n// For all other types, an attempt will be made to convert the string to the corresponding\n// type (int, float32, etc.).\nfunc setField(tokens []lexer.Token, strct reflect.Value, field structLexerField, fieldValue []reflect.Value) (err error) { // nolint: gocognit\n\tdefer decorate(&err, func() string { return strct.Type().Name() + \".\" + field.Name })\n\n\tf := strct.FieldByIndex(field.Index)\n\n\t// Any kind of pointer, hydrate it first.\n\tif f.Kind() == reflect.Ptr {\n\t\tif f.IsNil() {\n\t\t\tfv := reflect.New(f.Type().Elem()).Elem()\n\t\t\tf.Set(fv.Addr())\n\t\t\tf = fv\n\t\t} else {\n\t\t\tf = f.Elem()\n\t\t}\n\t}\n\n\tif f.Type() == tokenType {\n\t\tf.Set(reflect.ValueOf(tokens[0]))\n\t\treturn nil\n\t}\n\n\tif f.Type() == tokensType {\n\t\tf.Set(reflect.ValueOf(tokens))\n\t\treturn nil\n\t}\n\n\tif f.CanAddr() {\n\t\tif d, ok := f.Addr().Interface().(Capture); ok {\n\t\t\tifv := make([]string, 0, len(fieldValue))\n\t\t\tfor _, v := range fieldValue {\n\t\t\t\tifv = append(ifv, v.Interface().(string))\n\t\t\t}\n\t\t\treturn d.Capture(ifv)\n\t\t} else if d, ok := f.Addr().Interface().(encoding.TextUnmarshaler); ok {\n\t\t\tfor _, v := range fieldValue {\n\t\t\t\tif err := d.UnmarshalText([]byte(v.Interface().(string))); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t}\n\n\tif f.Kind() == reflect.Slice {\n\t\tsliceElemType := f.Type().Elem()\n\t\tif sliceElemType.Implements(captureType) || reflect.PtrTo(sliceElemType).Implements(captureType) {\n\t\t\tif sliceElemType.Kind() == reflect.Ptr {\n\t\t\t\tsliceElemType = sliceElemType.Elem()\n\t\t\t}\n\t\t\tfor _, v := range fieldValue {\n\t\t\t\td := reflect.New(sliceElemType).Interface().(Capture)\n\t\t\t\tif err := d.Capture([]string{v.Interface().(string)}); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\teltValue := reflect.ValueOf(d)\n\t\t\t\tif f.Type().Elem().Kind() != reflect.Ptr {\n\t\t\t\t\teltValue = eltValue.Elem()\n\t\t\t\t}\n\t\t\t\tf.Set(reflect.Append(f, eltValue))\n\t\t\t}\n\t\t} else {\n\t\t\tfieldValue, err = conform(sliceElemType, fieldValue)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tf.Set(reflect.Append(f, fieldValue...))\n\t\t}\n\t\treturn nil\n\t}\n\n\t// Strings concatenate all captured tokens.\n\tif f.Kind() == reflect.String {\n\t\tfieldValue, err = conform(f.Type(), fieldValue)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif len(fieldValue) == 0 {\n\t\t\treturn nil\n\t\t}\n\t\taccumulated := f.String()\n\t\tfor _, v := range fieldValue {\n\t\t\taccumulated += v.String()\n\t\t}\n\t\tf.SetString(accumulated)\n\t\treturn nil\n\t}\n\n\t// Coalesce multiple tokens into one. This allows eg. [\"-\", \"10\"] to be captured as separate tokens but\n\t// parsed as a single string \"-10\".\n\tif len(fieldValue) > 1 {\n\t\tout := []string{}\n\t\tfor _, v := range fieldValue {\n\t\t\tout = append(out, v.String())\n\t\t}\n\t\tfieldValue = []reflect.Value{reflect.ValueOf(strings.Join(out, \"\"))}\n\t}\n\n\tfieldValue, err = conform(f.Type(), fieldValue)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif len(fieldValue) == 0 {\n\t\treturn nil // Nothing to capture, can happen when trying to get a partial parse tree\n\t}\n\n\tfv := fieldValue[0]\n\n\tswitch f.Kind() { // nolint: exhaustive\n\t// Numeric types will increment if the token can not be coerced.\n\tcase reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64:\n\t\tif fv.Type() != f.Type() {\n\t\t\tf.SetInt(f.Int() + 1)\n\t\t} else {\n\t\t\tf.Set(fv)\n\t\t}\n\tcase reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64:\n\t\tif fv.Type() != f.Type() {\n\t\t\tf.SetUint(f.Uint() + 1)\n\t\t} else {\n\t\t\tf.Set(fv)\n\t\t}\n\n\tcase reflect.Float32, reflect.Float64:\n\t\tif fv.Type() != f.Type() {\n\t\t\tf.SetFloat(f.Float() + 1)\n\t\t} else {\n\t\t\tf.Set(fv)\n\t\t}\n\n\tcase reflect.Bool, reflect.Struct, reflect.Interface:\n\t\tif f.Kind() == reflect.Bool && fv.Kind() == reflect.Bool {\n\t\t\tf.SetBool(fv.Bool())\n\t\t\tbreak\n\t\t}\n\t\tif fv.Type() != f.Type() {\n\t\t\treturn fmt.Errorf(\"value %q is not correct type %s\", fv, f.Type())\n\t\t}\n\t\tf.Set(fv)\n\n\tdefault:\n\t\treturn fmt.Errorf(\"unsupported field type %s for field %s\", f.Type(), field.Name)\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "options.go",
          "type": "blob",
          "size": 4.5771484375,
          "content": "package participle\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"reflect\"\n\n\t\"github.com/alecthomas/participle/v2/lexer\"\n)\n\n// MaxLookahead can be used with UseLookahead to get pseudo-infinite\n// lookahead without the risk of pathological cases causing a stack\n// overflow.\nconst MaxLookahead = 99999\n\n// An Option to modify the behaviour of the Parser.\ntype Option func(p *parserOptions) error\n\n// Lexer is an Option that sets the lexer to use with the given grammar.\nfunc Lexer(def lexer.Definition) Option {\n\treturn func(p *parserOptions) error {\n\t\tp.lex = def\n\t\treturn nil\n\t}\n}\n\n// UseLookahead allows branch lookahead up to \"n\" tokens.\n//\n// If parsing cannot be disambiguated before \"n\" tokens of lookahead, parsing will fail.\n//\n// Note that increasing lookahead has a minor performance impact, but also\n// reduces the accuracy of error reporting.\n//\n// If \"n\" is negative, it will be treated as \"infinite\" lookahead.\n// This can have a large impact on performance, and does not provide any\n// protection against stack overflow during parsing.\n// In most cases, using MaxLookahead will achieve the same results in practice,\n// but with a concrete upper bound to prevent pathological behavior in the parser.\n// Using infinite lookahead can be useful for testing, or for parsing especially\n// ambiguous grammars. Use at your own risk!\nfunc UseLookahead(n int) Option {\n\treturn func(p *parserOptions) error {\n\t\tp.useLookahead = n\n\t\treturn nil\n\t}\n}\n\n// CaseInsensitive allows the specified token types to be matched case-insensitively.\n//\n// Note that the lexer itself will also have to be case-insensitive; this option\n// just controls whether literals in the grammar are matched case insensitively.\nfunc CaseInsensitive(tokens ...string) Option {\n\treturn func(p *parserOptions) error {\n\t\tfor _, token := range tokens {\n\t\t\tp.caseInsensitive[token] = true\n\t\t}\n\t\treturn nil\n\t}\n}\n\n// ParseTypeWith associates a custom parsing function with some interface type T.\n// When the parser encounters a value of type T, it will use the given parse function to\n// parse a value from the input.\n//\n// The parse function may return anything it wishes as long as that value satisfies the interface T.\n// However, only a single function can be defined for any type T.\n// If you want to have multiple parse functions returning types that satisfy the same interface, you'll\n// need to define new wrapper types for each one.\n//\n// This can be useful if you want to parse a DSL within the larger grammar, or if you want\n// to implement an optimized parsing scheme for some portion of the grammar.\nfunc ParseTypeWith[T any](parseFn func(*lexer.PeekingLexer) (T, error)) Option {\n\treturn func(p *parserOptions) error {\n\t\tparseFnVal := reflect.ValueOf(parseFn)\n\t\tparseFnType := parseFnVal.Type()\n\t\tif parseFnType.Out(0).Kind() != reflect.Interface {\n\t\t\treturn fmt.Errorf(\"ParseTypeWith: T must be an interface type (got %s)\", parseFnType.Out(0))\n\t\t}\n\t\tprodType := parseFnType.Out(0)\n\t\tp.customDefs = append(p.customDefs, customDef{prodType, parseFnVal})\n\t\treturn nil\n\t}\n}\n\n// Union associates several member productions with some interface type T.\n// Given members X, Y, Z, and W for a union type U, then the EBNF rule is:\n//\n//\tU = X | Y | Z | W .\n//\n// When the parser encounters a field of type T, it will attempt to parse each member\n// in sequence and take the first match. Because of this, the order in which the\n// members are defined is important. You must be careful to order your members appropriately.\n//\n// An example of a bad parse that can happen if members are out of order:\n//\n// If the first member matches A, and the second member matches A B,\n// and the source string is \"AB\", then the parser will only match A, and will not\n// try to parse the second member at all.\nfunc Union[T any](members ...T) Option {\n\treturn func(p *parserOptions) error {\n\t\tvar t T\n\t\tunionType := reflect.TypeOf(&t).Elem()\n\t\tif unionType.Kind() != reflect.Interface {\n\t\t\treturn fmt.Errorf(\"union: union type must be an interface (got %s)\", unionType)\n\t\t}\n\t\tmemberTypes := make([]reflect.Type, 0, len(members))\n\t\tfor _, m := range members {\n\t\t\tmemberTypes = append(memberTypes, reflect.TypeOf(m))\n\t\t}\n\t\tp.unionDefs = append(p.unionDefs, unionDef{unionType, memberTypes})\n\t\treturn nil\n\t}\n}\n\n// ParseOption modifies how an individual parse is applied.\ntype ParseOption func(p *parseContext)\n\n// Trace the parse to \"w\".\nfunc Trace(w io.Writer) ParseOption {\n\treturn func(p *parseContext) {\n\t\tp.trace = w\n\t}\n}\n\n// AllowTrailing tokens without erroring.\n//\n// That is, do not error if a full parse completes but additional tokens remain.\nfunc AllowTrailing(ok bool) ParseOption {\n\treturn func(p *parseContext) {\n\t\tp.allowTrailing = ok\n\t}\n}\n"
        },
        {
          "name": "parser.go",
          "type": "blob",
          "size": 8.6767578125,
          "content": "package participle\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"reflect\"\n\t\"strings\"\n\n\t\"github.com/alecthomas/participle/v2/lexer\"\n)\n\ntype unionDef struct {\n\ttyp     reflect.Type\n\tmembers []reflect.Type\n}\n\ntype customDef struct {\n\ttyp     reflect.Type\n\tparseFn reflect.Value\n}\n\ntype parserOptions struct {\n\tlex                   lexer.Definition\n\trootType              reflect.Type\n\ttypeNodes             map[reflect.Type]node\n\tuseLookahead          int\n\tcaseInsensitive       map[string]bool\n\tcaseInsensitiveTokens map[lexer.TokenType]bool\n\tmappers               []mapperByToken\n\tunionDefs             []unionDef\n\tcustomDefs            []customDef\n\telide                 []string\n}\n\n// A Parser for a particular grammar and lexer.\ntype Parser[G any] struct {\n\tparserOptions\n}\n\n// ParserForProduction returns a new parser for the given production in grammar G.\nfunc ParserForProduction[P, G any](parser *Parser[G]) (*Parser[P], error) {\n\tt := reflect.TypeOf(*new(P))\n\t_, ok := parser.typeNodes[t]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"parser does not contain a production of type %s\", t)\n\t}\n\treturn (*Parser[P])(parser), nil\n}\n\n// MustBuild calls Build[G](options...) and panics if an error occurs.\nfunc MustBuild[G any](options ...Option) *Parser[G] {\n\tparser, err := Build[G](options...)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn parser\n}\n\n// Build constructs a parser for the given grammar.\n//\n// If \"Lexer()\" is not provided as an option, a default lexer based on text/scanner will be used. This scans typical Go-\n// like tokens.\n//\n// See documentation for details.\nfunc Build[G any](options ...Option) (parser *Parser[G], err error) {\n\t// Configure Parser[G] struct with defaults + options.\n\tp := &Parser[G]{\n\t\tparserOptions: parserOptions{\n\t\t\tlex:             lexer.TextScannerLexer,\n\t\t\tcaseInsensitive: map[string]bool{},\n\t\t\tuseLookahead:    1,\n\t\t},\n\t}\n\tfor _, option := range options {\n\t\tif err = option(&p.parserOptions); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tsymbols := p.lex.Symbols()\n\tif len(p.mappers) > 0 {\n\t\tmappers := map[lexer.TokenType][]Mapper{}\n\t\tfor _, mapper := range p.mappers {\n\t\t\tif len(mapper.symbols) == 0 {\n\t\t\t\tmappers[lexer.EOF] = append(mappers[lexer.EOF], mapper.mapper)\n\t\t\t} else {\n\t\t\t\tfor _, symbol := range mapper.symbols {\n\t\t\t\t\tif rn, ok := symbols[symbol]; !ok {\n\t\t\t\t\t\treturn nil, fmt.Errorf(\"mapper %#v uses unknown token %q\", mapper, symbol)\n\t\t\t\t\t} else { // nolint: golint\n\t\t\t\t\t\tmappers[rn] = append(mappers[rn], mapper.mapper)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tp.lex = &mappingLexerDef{p.lex, func(t lexer.Token) (lexer.Token, error) {\n\t\t\tcombined := make([]Mapper, 0, len(mappers[t.Type])+len(mappers[lexer.EOF]))\n\t\t\tcombined = append(combined, mappers[lexer.EOF]...)\n\t\t\tcombined = append(combined, mappers[t.Type]...)\n\n\t\t\tvar err error\n\t\t\tfor _, m := range combined {\n\t\t\t\tt, err = m(t)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn t, err\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn t, nil\n\t\t}}\n\t}\n\n\tcontext := newGeneratorContext(p.lex)\n\tif err := context.addCustomDefs(p.customDefs); err != nil {\n\t\treturn nil, err\n\t}\n\tif err := context.addUnionDefs(p.unionDefs); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar grammar G\n\tv := reflect.ValueOf(&grammar)\n\tif v.Kind() == reflect.Interface {\n\t\tv = v.Elem()\n\t}\n\tp.rootType = v.Type()\n\trootNode, err := context.parseType(p.rootType)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := validate(rootNode); err != nil {\n\t\treturn nil, err\n\t}\n\tp.typeNodes = context.typeNodes\n\tp.typeNodes[p.rootType] = rootNode\n\tp.setCaseInsensitiveTokens()\n\treturn p, nil\n}\n\n// Lexer returns the parser's builtin lexer.\nfunc (p *Parser[G]) Lexer() lexer.Definition {\n\treturn p.lex\n}\n\n// Lex uses the parser's lexer to tokenise input.\n// Parameter filename is used as an opaque prefix in error messages.\nfunc (p *Parser[G]) Lex(filename string, r io.Reader) ([]lexer.Token, error) {\n\tlex, err := p.lex.Lex(filename, r)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ttokens, err := lexer.ConsumeAll(lex)\n\treturn tokens, err\n}\n\n// ParseFromLexer into grammar v which must be of the same type as the grammar passed to\n// Build().\n//\n// This may return a Error.\nfunc (p *Parser[G]) ParseFromLexer(lex *lexer.PeekingLexer, options ...ParseOption) (*G, error) {\n\tv := new(G)\n\trv := reflect.ValueOf(v)\n\tparseNode, err := p.parseNodeFor(rv)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tctx := newParseContext(lex, p.useLookahead, p.caseInsensitiveTokens)\n\tdefer func() { *lex = ctx.PeekingLexer }()\n\tfor _, option := range options {\n\t\toption(&ctx)\n\t}\n\t// If the grammar implements Parseable, use it.\n\tif parseable, ok := any(v).(Parseable); ok {\n\t\treturn v, p.rootParseable(&ctx, parseable)\n\t}\n\treturn v, p.parseOne(&ctx, parseNode, rv)\n}\n\nfunc (p *Parser[G]) setCaseInsensitiveTokens() {\n\tp.caseInsensitiveTokens = map[lexer.TokenType]bool{}\n\tfor sym, tt := range p.lex.Symbols() {\n\t\tif p.caseInsensitive[sym] {\n\t\t\tp.caseInsensitiveTokens[tt] = true\n\t\t}\n\t}\n}\n\nfunc (p *Parser[G]) parse(lex lexer.Lexer, options ...ParseOption) (v *G, err error) {\n\tpeeker, err := lexer.Upgrade(lex, p.getElidedTypes()...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn p.ParseFromLexer(peeker, options...)\n}\n\n// Parse from r into grammar v which must be of the same type as the grammar passed to\n// Build(). Parameter filename is used as an opaque prefix in error messages.\n//\n// This may return an Error.\nfunc (p *Parser[G]) Parse(filename string, r io.Reader, options ...ParseOption) (v *G, err error) {\n\tif filename == \"\" {\n\t\tfilename = lexer.NameOfReader(r)\n\t}\n\tlex, err := p.lex.Lex(filename, r)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn p.parse(lex, options...)\n}\n\n// ParseString from s into grammar v which must be of the same type as the grammar passed to\n// Build(). Parameter filename is used as an opaque prefix in error messages.\n//\n// This may return an Error.\nfunc (p *Parser[G]) ParseString(filename string, s string, options ...ParseOption) (v *G, err error) {\n\tvar lex lexer.Lexer\n\tif sl, ok := p.lex.(lexer.StringDefinition); ok {\n\t\tlex, err = sl.LexString(filename, s)\n\t} else {\n\t\tlex, err = p.lex.Lex(filename, strings.NewReader(s))\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn p.parse(lex, options...)\n}\n\n// ParseBytes from b into grammar v which must be of the same type as the grammar passed to\n// Build(). Parameter filename is used as an opaque prefix in error messages.\n//\n// This may return an Error.\nfunc (p *Parser[G]) ParseBytes(filename string, b []byte, options ...ParseOption) (v *G, err error) {\n\tvar lex lexer.Lexer\n\tif sl, ok := p.lex.(lexer.BytesDefinition); ok {\n\t\tlex, err = sl.LexBytes(filename, b)\n\t} else {\n\t\tlex, err = p.lex.Lex(filename, bytes.NewReader(b))\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn p.parse(lex, options...)\n}\n\nfunc (p *Parser[G]) parseOne(ctx *parseContext, parseNode node, rv reflect.Value) error {\n\terr := p.parseInto(ctx, parseNode, rv)\n\tif err != nil {\n\t\treturn err\n\t}\n\ttoken := ctx.Peek()\n\tif !token.EOF() && !ctx.allowTrailing {\n\t\treturn ctx.DeepestError(&UnexpectedTokenError{Unexpected: *token})\n\t}\n\treturn nil\n}\n\nfunc (p *Parser[G]) parseInto(ctx *parseContext, parseNode node, rv reflect.Value) error {\n\tif rv.IsNil() {\n\t\treturn fmt.Errorf(\"target must be a non-nil pointer to a struct or interface, but is a nil %s\", rv.Type())\n\t}\n\tpv, err := p.typeNodes[rv.Type().Elem()].Parse(ctx, rv.Elem())\n\tif len(pv) > 0 && pv[0].Type() == rv.Elem().Type() {\n\t\trv.Elem().Set(reflect.Indirect(pv[0]))\n\t}\n\tif err != nil {\n\t\treturn err\n\t}\n\tif pv == nil {\n\t\ttoken := ctx.Peek()\n\t\treturn ctx.DeepestError(&UnexpectedTokenError{Unexpected: *token})\n\t}\n\treturn nil\n}\n\nfunc (p *Parser[G]) rootParseable(ctx *parseContext, parseable Parseable) error {\n\tif err := parseable.Parse(&ctx.PeekingLexer); err != nil {\n\t\tif err == NextMatch {\n\t\t\terr = &UnexpectedTokenError{Unexpected: *ctx.Peek()}\n\t\t} else {\n\t\t\terr = &ParseError{Msg: err.Error(), Pos: ctx.Peek().Pos}\n\t\t}\n\t\treturn ctx.DeepestError(err)\n\t}\n\tpeek := ctx.Peek()\n\tif !peek.EOF() && !ctx.allowTrailing {\n\t\treturn ctx.DeepestError(&UnexpectedTokenError{Unexpected: *peek})\n\t}\n\treturn nil\n}\n\nfunc (p *Parser[G]) getElidedTypes() []lexer.TokenType {\n\tsymbols := p.lex.Symbols()\n\telideTypes := make([]lexer.TokenType, 0, len(p.elide))\n\tfor _, elide := range p.elide {\n\t\trn, ok := symbols[elide]\n\t\tif !ok {\n\t\t\tpanic(fmt.Errorf(\"Elide() uses unknown token %q\", elide))\n\t\t}\n\t\telideTypes = append(elideTypes, rn)\n\t}\n\treturn elideTypes\n}\n\nfunc (p *Parser[G]) parseNodeFor(v reflect.Value) (node, error) {\n\tt := v.Type()\n\tif t.Kind() == reflect.Interface {\n\t\tt = t.Elem()\n\t}\n\tif t.Kind() != reflect.Ptr || (t.Elem().Kind() != reflect.Struct && t.Elem().Kind() != reflect.Interface) {\n\t\treturn nil, fmt.Errorf(\"expected a pointer to a struct or interface, but got %s\", t)\n\t}\n\tparseNode := p.typeNodes[t]\n\tif parseNode == nil {\n\t\tt = t.Elem()\n\t\tparseNode = p.typeNodes[t]\n\t}\n\tif parseNode == nil {\n\t\treturn nil, fmt.Errorf(\"parser does not know how to parse values of type %s\", t)\n\t}\n\treturn parseNode, nil\n}\n"
        },
        {
          "name": "parser_test.go",
          "type": "blob",
          "size": 45.244140625,
          "content": "package participle_test\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"net\"\n\t\"reflect\"\n\t\"strconv\"\n\t\"strings\"\n\t\"testing\"\n\t\"text/scanner\"\n\n\t\"github.com/alecthomas/assert/v2\"\n\n\t\"github.com/alecthomas/participle/v2\"\n\t\"github.com/alecthomas/participle/v2/lexer\"\n)\n\nfunc TestProductionCapture(t *testing.T) {\n\ttype testCapture struct {\n\t\tA string `@Test`\n\t}\n\n\t_, err := participle.Build[testCapture]()\n\tassert.Error(t, err)\n}\n\nfunc TestTermCapture(t *testing.T) {\n\ttype grammar struct {\n\t\tA string `@\".\"*`\n\t}\n\n\tparser := mustTestParser[grammar](t)\n\n\texpected := &grammar{\"...\"}\n\n\tactual, err := parser.ParseString(\"\", \"...\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\nfunc TestParseScalar(t *testing.T) {\n\ttype grammar struct {\n\t\tA string `@\"one\"`\n\t}\n\n\tparser := mustTestParser[grammar](t)\n\n\tactual, err := parser.ParseString(\"\", \"one\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, &grammar{\"one\"}, actual)\n}\n\nfunc TestParseGroup(t *testing.T) {\n\ttype grammar struct {\n\t\tA string `@(\"one\" | \"two\")`\n\t}\n\n\tparser := mustTestParser[grammar](t)\n\n\tactual, err := parser.ParseString(\"\", \"one\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, &grammar{\"one\"}, actual)\n\n\tactual, err = parser.ParseString(\"\", \"two\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, &grammar{\"two\"}, actual)\n}\n\nfunc TestParseAlternative(t *testing.T) {\n\ttype grammar struct {\n\t\tA string `@\"one\" |`\n\t\tB string `@\"two\"`\n\t}\n\n\tparser := mustTestParser[grammar](t)\n\n\tactual, err := parser.ParseString(\"\", \"one\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, &grammar{A: \"one\"}, actual)\n\n\tactual, err = parser.ParseString(\"\", \"two\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, &grammar{B: \"two\"}, actual)\n}\n\nfunc TestParseSequence(t *testing.T) {\n\ttype grammar struct {\n\t\tA string `@\"one\"`\n\t\tB string `@\"two\"`\n\t\tC string `@\"three\"`\n\t}\n\n\tparser := mustTestParser[grammar](t)\n\n\texpected := &grammar{\"one\", \"two\", \"three\"}\n\tactual, err := parser.ParseString(\"\", \"one two three\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n\n\texpected = &grammar{}\n\tactual, err = parser.ParseString(\"\", \"moo\")\n\tassert.Error(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\nfunc TestNested(t *testing.T) {\n\ttype nestedInner struct {\n\t\tB string `@\"one\"`\n\t\tC string `@\"two\"`\n\t}\n\n\ttype testNested struct {\n\t\tA *nestedInner `@@`\n\t}\n\n\tparser := mustTestParser[testNested](t)\n\n\texpected := &testNested{A: &nestedInner{B: \"one\", C: \"two\"}}\n\tactual, err := parser.ParseString(\"\", \"one two\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\nfunc TestAccumulateNested(t *testing.T) {\n\ttype nestedInner struct {\n\t\tB string `@\"one\"`\n\t\tC string `@\"two\"`\n\t}\n\ttype testAccumulateNested struct {\n\t\tA []*nestedInner `@@+`\n\t}\n\n\tparser := mustTestParser[testAccumulateNested](t)\n\n\texpected := &testAccumulateNested{A: []*nestedInner{{B: \"one\", C: \"two\"}, {B: \"one\", C: \"two\"}}}\n\tactual, err := parser.ParseString(\"\", \"one two one two\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\nfunc TestRepetitionNoMatch(t *testing.T) {\n\ttype grammar struct {\n\t\tA []string `@\".\"*`\n\t}\n\tparser := mustTestParser[grammar](t)\n\n\texpected := &grammar{}\n\tactual, err := parser.ParseString(\"\", ``)\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\nfunc TestRepetition(t *testing.T) {\n\ttype grammar struct {\n\t\tA []string `@\".\"*`\n\t}\n\tparser := mustTestParser[grammar](t)\n\n\texpected := &grammar{A: []string{\".\", \".\", \".\"}}\n\tactual, err := parser.ParseString(\"\", `...`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\nfunc TestRepetitionAcrossFields(t *testing.T) {\n\ttype testRepetition struct {\n\t\tA []string `@\".\"*`\n\t\tB *string  `(@\"b\" |`\n\t\tC *string  ` @\"c\")`\n\t}\n\n\tparser := mustTestParser[testRepetition](t)\n\n\tb := \"b\"\n\tc := \"c\"\n\n\texpected := &testRepetition{\n\t\tA: []string{\".\", \".\", \".\"},\n\t\tB: &b,\n\t}\n\tactual, err := parser.ParseString(\"\", \"...b\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n\n\texpected = &testRepetition{\n\t\tA: []string{\".\", \".\", \".\"},\n\t\tC: &c,\n\t}\n\tactual, err = parser.ParseString(\"\", \"...c\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n\n\texpected = &testRepetition{\n\t\tB: &b,\n\t}\n\tactual, err = parser.ParseString(\"\", \"b\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\nfunc TestAccumulateString(t *testing.T) {\n\ttype customString string\n\ttype testAccumulateString struct {\n\t\tA customString `@\".\"+`\n\t}\n\n\tparser := mustTestParser[testAccumulateString](t)\n\n\texpected := &testAccumulateString{\n\t\tA: \"...\",\n\t}\n\tactual, err := parser.ParseString(\"\", \"...\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\ntype Group struct {\n\tExpression *Expression `\"(\" @@ \")\"`\n}\n\ntype LookaheadGroup struct {\n\tExpression *Expression `\"(\" \"?\" (\"=\" | \"!\") @@ \")\"`\n}\n\ntype EBNFOption struct {\n\tExpression *Expression `\"[\" @@ \"]\"`\n}\n\ntype Repetition struct {\n\tExpression *Expression `\"{\" @@ \"}\"`\n}\n\ntype Negation struct {\n\tExpression *Expression `\"!\" @@`\n}\n\ntype Literal struct {\n\tStart string `@String`\n}\n\ntype Range struct {\n\tStart string `@String`\n\tEnd   string `\"â€¦\" @String`\n}\n\ntype Term struct {\n\tName           string          `@Ident |`\n\tLiteral        *Literal        `@@ |`\n\tRange          *Range          `@@ |`\n\tGroup          *Group          `@@ |`\n\tLookaheadGroup *LookaheadGroup `@@ |`\n\tOption         *EBNFOption     `@@ |`\n\tRepetition     *Repetition     `@@ |`\n\tNegation       *Negation       `@@`\n}\n\ntype Sequence struct {\n\tTerms []*Term `@@+`\n}\n\ntype Expression struct {\n\tAlternatives []*Sequence `@@ ( \"|\" @@ )*`\n}\n\ntype Production struct {\n\tName       string        `@Ident \"=\"`\n\tExpression []*Expression `@@+ \".\"`\n}\n\ntype EBNF struct {\n\tProductions []*Production `@@*`\n}\n\nfunc TestEBNFParser(t *testing.T) {\n\tparser := mustTestParser[EBNF](t, participle.Unquote())\n\n\texpected := &EBNF{\n\t\tProductions: []*Production{\n\t\t\t{\n\t\t\t\tName: \"Production\",\n\t\t\t\tExpression: []*Expression{\n\t\t\t\t\t{\n\t\t\t\t\t\tAlternatives: []*Sequence{\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tTerms: []*Term{\n\t\t\t\t\t\t\t\t\t{Name: \"name\"},\n\t\t\t\t\t\t\t\t\t{Literal: &Literal{Start: \"=\"}},\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tOption: &EBNFOption{\n\t\t\t\t\t\t\t\t\t\t\tExpression: &Expression{\n\t\t\t\t\t\t\t\t\t\t\t\tAlternatives: []*Sequence{\n\t\t\t\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tTerms: []*Term{\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{Name: \"Expression\"},\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t{Literal: &Literal{Start: \".\"}},\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\tName: \"Expression\",\n\t\t\t\tExpression: []*Expression{\n\t\t\t\t\t{\n\t\t\t\t\t\tAlternatives: []*Sequence{\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tTerms: []*Term{\n\t\t\t\t\t\t\t\t\t{Name: \"Alternative\"},\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tRepetition: &Repetition{\n\t\t\t\t\t\t\t\t\t\t\tExpression: &Expression{\n\t\t\t\t\t\t\t\t\t\t\t\tAlternatives: []*Sequence{\n\t\t\t\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tTerms: []*Term{\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{Literal: &Literal{Start: \"|\"}},\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{Name: \"Alternative\"},\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\tName: \"Alternative\",\n\t\t\t\tExpression: []*Expression{\n\t\t\t\t\t{\n\t\t\t\t\t\tAlternatives: []*Sequence{\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tTerms: []*Term{\n\t\t\t\t\t\t\t\t\t{Name: \"Term\"},\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tRepetition: &Repetition{\n\t\t\t\t\t\t\t\t\t\t\tExpression: &Expression{\n\t\t\t\t\t\t\t\t\t\t\t\tAlternatives: []*Sequence{\n\t\t\t\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tTerms: []*Term{\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{Name: \"Term\"},\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\tName: \"Term\",\n\t\t\t\tExpression: []*Expression{\n\t\t\t\t\t{\n\t\t\t\t\t\tAlternatives: []*Sequence{\n\t\t\t\t\t\t\t{Terms: []*Term{{Name: \"name\"}}},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tTerms: []*Term{\n\t\t\t\t\t\t\t\t\t{Name: \"token\"},\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tOption: &EBNFOption{\n\t\t\t\t\t\t\t\t\t\t\tExpression: &Expression{\n\t\t\t\t\t\t\t\t\t\t\t\tAlternatives: []*Sequence{\n\t\t\t\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tTerms: []*Term{\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{Literal: &Literal{Start: \"â€¦\"}},\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{Name: \"token\"},\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{Terms: []*Term{{Literal: &Literal{Start: \"@@\"}}}},\n\t\t\t\t\t\t\t{Terms: []*Term{{Name: \"Group\"}}},\n\t\t\t\t\t\t\t{Terms: []*Term{{Name: \"EBNFOption\"}}},\n\t\t\t\t\t\t\t{Terms: []*Term{{Name: \"Repetition\"}}},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\tName: \"Group\",\n\t\t\t\tExpression: []*Expression{\n\t\t\t\t\t{\n\t\t\t\t\t\tAlternatives: []*Sequence{\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tTerms: []*Term{\n\t\t\t\t\t\t\t\t\t{Literal: &Literal{Start: \"(\"}},\n\t\t\t\t\t\t\t\t\t{Name: \"Expression\"},\n\t\t\t\t\t\t\t\t\t{Literal: &Literal{Start: \")\"}},\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\tName: \"EBNFOption\",\n\t\t\t\tExpression: []*Expression{\n\t\t\t\t\t{\n\t\t\t\t\t\tAlternatives: []*Sequence{\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tTerms: []*Term{\n\t\t\t\t\t\t\t\t\t{Literal: &Literal{Start: \"[\"}},\n\t\t\t\t\t\t\t\t\t{Name: \"Expression\"},\n\t\t\t\t\t\t\t\t\t{Literal: &Literal{Start: \"]\"}},\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\t{\n\t\t\t\tName: \"Repetition\",\n\t\t\t\tExpression: []*Expression{\n\t\t\t\t\t{\n\t\t\t\t\t\tAlternatives: []*Sequence{\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tTerms: []*Term{\n\t\t\t\t\t\t\t\t\t{Literal: &Literal{Start: \"{\"}},\n\t\t\t\t\t\t\t\t\t{Name: \"Expression\"},\n\t\t\t\t\t\t\t\t\t{Literal: &Literal{Start: \"}\"}},\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\tactual, err := parser.ParseString(\"\", strings.TrimSpace(`\nProduction  = name \"=\" [ Expression ] \".\" .\nExpression  = Alternative { \"|\" Alternative } .\nAlternative = Term { Term } .\nTerm        = name | token [ \"â€¦\" token ] | \"@@\" | Group | EBNFOption | Repetition .\nGroup       = \"(\" Expression \")\" .\nEBNFOption      = \"[\" Expression \"]\" .\nRepetition  = \"{\" Expression \"}\" .\n`))\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\nfunc TestParseExpression(t *testing.T) {\n\ttype testNestA struct {\n\t\tA string `\":\" @\"a\"*`\n\t}\n\ttype testNestB struct {\n\t\tB string `\";\" @\"b\"*`\n\t}\n\ttype testExpression struct {\n\t\tA *testNestA `@@ |`\n\t\tB *testNestB `@@`\n\t}\n\n\tparser := mustTestParser[testExpression](t)\n\n\texpected := &testExpression{\n\t\tB: &testNestB{\n\t\t\tB: \"b\",\n\t\t},\n\t}\n\tactual, err := parser.ParseString(\"\", \";b\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\nfunc TestParseOptional(t *testing.T) {\n\ttype testOptional struct {\n\t\tA string `( @\"a\" @\"b\" )?`\n\t\tB string `@\"c\"`\n\t}\n\n\tparser := mustTestParser[testOptional](t)\n\n\texpected := &testOptional{B: \"c\"}\n\tactual, err := parser.ParseString(\"\", `c`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\nfunc TestHello(t *testing.T) {\n\ttype testHello struct {\n\t\tHello string `@\"hello\"`\n\t\tTo    string `@String`\n\t}\n\n\tparser := mustTestParser[testHello](t, participle.Unquote())\n\n\texpected := &testHello{\"hello\", `Bobby Brown`}\n\tactual, err := parser.ParseString(\"\", `hello \"Bobby Brown\"`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\nfunc mustTestParser[G any](t *testing.T, options ...participle.Option) *participle.Parser[G] {\n\tt.Helper()\n\tparser, err := participle.Build[G](options...)\n\tassert.NoError(t, err)\n\treturn parser\n}\n\nfunc BenchmarkEBNFParser(b *testing.B) {\n\tparser, err := participle.Build[EBNF]()\n\tassert.NoError(b, err)\n\tb.ResetTimer()\n\tsource := strings.TrimSpace(`\nProduction  = name \"=\" [ Expression ] \".\" .\nExpression  = Alternative { \"|\" Alternative } .\nAlternative = Term { Term } .\nTerm        = name | token [ \"â€¦\" token ] | \"@@\" | Group | EBNFOption | Repetition .\nGroup       = \"(\" Expression \")\" .\nEBNFOption      = \"[\" Expression \"]\" .\nRepetition  = \"{\" Expression \"}\" .\n\n`)\n\tfor i := 0; i < b.N; i++ {\n\t\t_, _ = parser.ParseString(\"\", source)\n\t}\n}\n\nfunc TestRepeatAcrossFields(t *testing.T) {\n\ttype grammar struct {\n\t\tA string `( @(\".\" \">\") |`\n\t\tB string `  @(\",\" \"<\") )*`\n\t}\n\n\tparser := mustTestParser[grammar](t)\n\n\texpected := &grammar{A: \".>.>.>.>\", B: \",<,<,<\"}\n\n\tactual, err := parser.ParseString(\"\", \".>,<.>.>,<.>,<\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\nfunc TestPosInjection(t *testing.T) {\n\ttype subgrammar struct {\n\t\tPos    lexer.Position\n\t\tB      string `@\",\"*`\n\t\tEndPos lexer.Position\n\t}\n\ttype grammar struct {\n\t\tPos    lexer.Position\n\t\tA      string      `@\".\"*`\n\t\tB      *subgrammar `@@`\n\t\tC      string      `@\".\"`\n\t\tEndPos lexer.Position\n\t}\n\n\tparser := mustTestParser[grammar](t)\n\n\texpected := &grammar{\n\t\tPos: lexer.Position{\n\t\t\tOffset: 3,\n\t\t\tLine:   1,\n\t\t\tColumn: 4,\n\t\t},\n\t\tA: \"...\",\n\t\tB: &subgrammar{\n\t\t\tB: \",,,\",\n\t\t\tPos: lexer.Position{\n\t\t\t\tOffset: 6,\n\t\t\t\tLine:   1,\n\t\t\t\tColumn: 7,\n\t\t\t},\n\t\t\tEndPos: lexer.Position{\n\t\t\t\tOffset: 9,\n\t\t\t\tLine:   1,\n\t\t\t\tColumn: 10,\n\t\t\t},\n\t\t},\n\t\tC: \".\",\n\t\tEndPos: lexer.Position{\n\t\t\tOffset: 10,\n\t\t\tLine:   1,\n\t\t\tColumn: 11,\n\t\t},\n\t}\n\n\tactual, err := parser.ParseString(\"\", \"   ...,,,.\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\nfunc TestPosInjectionCustomPosition(t *testing.T) {\n\ttype Position struct {\n\t\tFilename string\n\t\tOffset   int\n\t\tLine     int\n\t\tColumn   int\n\t}\n\ttype grammar struct {\n\t\tPos    Position\n\t\tEndPos Position\n\n\t\tName string `@Ident`\n\t}\n\n\tparser := mustTestParser[grammar](t)\n\tg, err := parser.ParseString(\"\", \"  foo\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, Position{Offset: 2, Line: 1, Column: 3}, g.Pos)\n\tassert.Equal(t, Position{Offset: 5, Line: 1, Column: 6}, g.EndPos)\n}\n\ntype parseableCount int\n\nfunc (c *parseableCount) Capture(values []string) error {\n\t*c += parseableCount(len(values))\n\treturn nil\n}\n\nfunc TestCaptureInterface(t *testing.T) {\n\ttype grammar struct {\n\t\tCount parseableCount `@\"a\"*`\n\t}\n\n\tparser := mustTestParser[grammar](t)\n\texpected := &grammar{Count: 3}\n\tactual, err := parser.ParseString(\"\", \"a a a\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\ntype unmarshallableCount int\n\nfunc (u *unmarshallableCount) UnmarshalText(text []byte) error {\n\t*u += unmarshallableCount(len(text))\n\treturn nil\n}\n\nfunc TestTextUnmarshalerInterface(t *testing.T) {\n\ttype grammar struct {\n\t\tCount unmarshallableCount `{ @\"a\" }`\n\t}\n\n\tparser := mustTestParser[grammar](t)\n\texpected := &grammar{Count: 3}\n\tactual, err := parser.ParseString(\"\", \"a a a\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\nfunc TestLiteralTypeConstraint(t *testing.T) {\n\ttype grammar struct {\n\t\tLiteral string `@\"123456\":String`\n\t}\n\n\tparser := mustTestParser[grammar](t, participle.Unquote())\n\n\texpected := &grammar{Literal: \"123456\"}\n\tactual, err := parser.ParseString(\"\", `\"123456\"`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n\n\t_, err = parser.ParseString(\"\", `123456`)\n\tassert.Error(t, err)\n}\n\ntype nestedCapture struct {\n\tTokens []string\n}\n\nfunc (n *nestedCapture) Capture(tokens []string) error {\n\tn.Tokens = tokens\n\treturn nil\n}\n\nfunc TestStructCaptureInterface(t *testing.T) {\n\ttype grammar struct {\n\t\tCapture *nestedCapture `@String`\n\t}\n\n\tparser, err := participle.Build[grammar](participle.Unquote())\n\tassert.NoError(t, err)\n\n\texpected := &grammar{Capture: &nestedCapture{Tokens: []string{\"hello\"}}}\n\tactual, err := parser.ParseString(\"\", `\"hello\"`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\ntype parseableStruct struct {\n\tTokens []string\n}\n\nfunc (p *parseableStruct) Parse(lex *lexer.PeekingLexer) error {\n\tfor {\n\t\ttok := lex.Next()\n\t\tif tok.EOF() {\n\t\t\tbreak\n\t\t}\n\t\tp.Tokens = append(p.Tokens, tok.Value)\n\t}\n\treturn nil\n}\n\nfunc TestParseable(t *testing.T) {\n\ttype grammar struct {\n\t\tInner *parseableStruct `@@`\n\t}\n\n\tparser, err := participle.Build[grammar](participle.Unquote())\n\tassert.NoError(t, err)\n\n\texpected := &grammar{Inner: &parseableStruct{Tokens: []string{\"hello\", \"123\", \"world\"}}}\n\tactual, err := parser.ParseString(\"\", `hello 123 \"world\"`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\nfunc TestStringConcat(t *testing.T) {\n\ttype grammar struct {\n\t\tField string `@\".\"+`\n\t}\n\n\tparser, err := participle.Build[grammar]()\n\tassert.NoError(t, err)\n\n\texpected := &grammar{\"....\"}\n\tactual, err := parser.ParseString(\"\", `. . . .`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\nfunc TestParseIntSlice(t *testing.T) {\n\ttype grammar struct {\n\t\tField []int `@Int+`\n\t}\n\n\tparser := mustTestParser[grammar](t)\n\n\texpected := &grammar{[]int{1, 2, 3, 4}}\n\tactual, err := parser.ParseString(\"\", `1 2 3 4`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\nfunc TestEmptyStructErrorsNotPanicsIssue21(t *testing.T) {\n\ttype grammar struct {\n\t\tFoo struct{} `@@`\n\t}\n\t_, err := participle.Build[grammar]()\n\tassert.Error(t, err)\n}\n\nfunc TestMultipleTokensIntoScalar(t *testing.T) {\n\ttype grammar struct {\n\t\tField int `@(\"-\"? Int)`\n\t}\n\tp, err := participle.Build[grammar]()\n\tassert.NoError(t, err)\n\tactual, err := p.ParseString(\"\", `- 10`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, -10, actual.Field)\n\n\t_, err = p.ParseString(\"\", `x`)\n\tassert.EqualError(t, err, `1:1: unexpected token \"x\" (expected <int>)`)\n\t_, err = p.ParseString(\"\", ` `)\n\tassert.EqualError(t, err, `1:2: unexpected token \"<EOF>\" (expected <int>)`)\n}\n\ntype posMixin struct {\n\tPos lexer.Position\n}\n\nfunc TestMixinPosIsPopulated(t *testing.T) {\n\ttype grammar struct {\n\t\tposMixin\n\n\t\tInt int `@Int`\n\t}\n\n\tp := mustTestParser[grammar](t)\n\tactual, err := p.ParseString(\"\", \"10\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, 10, actual.Int)\n\tassert.Equal(t, 1, actual.Pos.Column)\n\tassert.Equal(t, 1, actual.Pos.Line)\n}\n\ntype testParserMixin struct {\n\tA string `@Ident`\n\tB string `@Ident`\n}\n\nfunc TestMixinFieldsAreParsed(t *testing.T) {\n\ttype grammar struct {\n\t\ttestParserMixin\n\t\tC string `@Ident`\n\t}\n\tp := mustTestParser[grammar](t)\n\tactual, err := p.ParseString(\"\", \"one two three\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, \"one\", actual.A)\n\tassert.Equal(t, \"two\", actual.B)\n\tassert.Equal(t, \"three\", actual.C)\n}\n\nfunc TestNestedOptional(t *testing.T) {\n\ttype grammar struct {\n\t\tArgs []string `\"(\" [ @Ident ( \",\" @Ident )* ] \")\"`\n\t}\n\tp := mustTestParser[grammar](t)\n\t_, err := p.ParseString(\"\", `()`)\n\tassert.NoError(t, err)\n\t_, err = p.ParseString(\"\", `(a)`)\n\tassert.NoError(t, err)\n\t_, err = p.ParseString(\"\", `(a, b, c)`)\n\tassert.NoError(t, err)\n\t_, err = p.ParseString(\"\", `(1)`)\n\tassert.Error(t, err)\n}\n\nfunc TestInvalidNumbers(t *testing.T) {\n\ttype grammar struct {\n\t\tInt8    int8    `  \"int8\" @Int`\n\t\tInt16   int16   `| \"int16\" @Int`\n\t\tInt32   int32   `| \"int32\" @Int`\n\t\tInt64   int64   `| \"int64\" @Int`\n\t\tUint8   uint8   `| \"uint8\" @Int`\n\t\tUint16  uint16  `| \"uint16\" @Int`\n\t\tUint32  uint32  `| \"uint32\" @Int`\n\t\tUint64  uint64  `| \"uint64\" @Int`\n\t\tFloat32 float32 `| \"float32\" @Float`\n\t\tFloat64 float64 `| \"float64\" @Float`\n\t}\n\n\tp := mustTestParser[grammar](t)\n\n\ttests := []struct {\n\t\tname     string\n\t\tinput    string\n\t\texpected *grammar\n\t\terr      bool\n\t}{\n\t\t{name: \"ValidInt8\", input: \"int8 127\", expected: &grammar{Int8: 127}},\n\t\t{name: \"InvalidInt8\", input: \"int8 129\", err: true},\n\t\t{name: \"ValidInt16\", input: \"int16 32767\", expected: &grammar{Int16: 32767}},\n\t\t{name: \"InvalidInt16\", input: \"int16 32768\", err: true},\n\t\t{name: \"ValidInt32\", input: fmt.Sprintf(\"int32 %d\", math.MaxInt32), expected: &grammar{Int32: math.MaxInt32}},\n\t\t{name: \"InvalidInt32\", input: fmt.Sprintf(\"int32 %d\", int64(math.MaxInt32+1)), err: true},\n\t\t{name: \"ValidInt64\", input: fmt.Sprintf(\"int64 %d\", int64(math.MaxInt64)), expected: &grammar{Int64: math.MaxInt64}},\n\t\t{name: \"InvalidInt64\", input: \"int64 9223372036854775808\", err: true},\n\t\t{name: \"ValidFloat64\", input: \"float64 1234.5\", expected: &grammar{Float64: 1234.5}},\n\t\t{name: \"InvalidFloat64\", input: \"float64 asdf\", err: true},\n\t}\n\tfor _, test := range tests {\n\t\t// nolint: scopelint\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tactual, err := p.ParseString(\"\", test.input)\n\t\t\tif test.err {\n\t\t\t\tassert.Error(t, err, fmt.Sprintf(\"%#v\", actual))\n\t\t\t} else {\n\t\t\t\tassert.NoError(t, err)\n\t\t\t\tassert.Equal(t, test.expected, actual)\n\t\t\t}\n\t\t})\n\t}\n}\n\n// We'd like this to work, but it can wait.\n\nfunc TestPartialAST(t *testing.T) {\n\ttype grammar struct {\n\t\tSucceed string `@Ident`\n\t\tFail    string `@\"foo\"`\n\t}\n\tp := mustTestParser[grammar](t)\n\tactual, err := p.ParseString(\"\", `foo bar`)\n\tassert.Error(t, err)\n\texpected := &grammar{Succeed: \"foo\"}\n\tassert.Equal(t, expected, actual)\n}\n\nfunc TestCaseInsensitive(t *testing.T) {\n\ttype grammar struct {\n\t\tSelect string `\"select\":Keyword @Ident`\n\t}\n\n\t// lex := lexer.MustStateful(lexer.Regexp(\n\t// \t`(?i)(?P<Keyword>SELECT)` +\n\t// \t\t`|(?P<Ident>\\w+)` +\n\t// \t\t`|(\\s+)`,\n\t// ))\n\tlex := lexer.MustSimple([]lexer.SimpleRule{\n\t\t{\"Keyword\", `(?i)SELECT`},\n\t\t{\"Ident\", `\\w+`},\n\t\t{\"whitespace\", `\\s+`},\n\t})\n\n\tp := mustTestParser[grammar](t, participle.Lexer(lex), participle.CaseInsensitive(\"Keyword\"))\n\tactual, err := p.ParseString(\"\", `SELECT foo`)\n\texpected := &grammar{\"foo\"}\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n\n\tactual, err = p.ParseString(\"\", `select foo`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\nfunc TestTokenAfterRepeatErrors(t *testing.T) {\n\ttype grammar struct {\n\t\tText string `@Ident* \"foo\"`\n\t}\n\tp := mustTestParser[grammar](t)\n\t_, err := p.ParseString(\"\", ``)\n\tassert.Error(t, err)\n}\n\nfunc TestEOFAfterRepeat(t *testing.T) {\n\ttype grammar struct {\n\t\tText string `@Ident*`\n\t}\n\tp := mustTestParser[grammar](t)\n\t_, err := p.ParseString(\"\", ``)\n\tassert.NoError(t, err)\n}\n\nfunc TestTrailing(t *testing.T) {\n\ttype grammar struct {\n\t\tText string `@Ident`\n\t}\n\tp := mustTestParser[grammar](t)\n\t_, err := p.ParseString(\"\", `foo bar`)\n\tassert.Error(t, err)\n}\n\ntype modifierTest[G any] struct {\n\tname     string\n\tinput    string\n\texpected string\n\tfail     bool\n}\n\nfunc (test modifierTest[G]) test(t *testing.T) {\n\tt.Helper()\n\tt.Run(test.name, func(t *testing.T) {\n\t\tp := mustTestParser[G](t)\n\t\tgrammar, err := p.ParseString(\"\", test.input)\n\t\tif test.fail {\n\t\t\tassert.Error(t, err)\n\t\t} else {\n\t\t\tassert.NoError(t, err)\n\t\t\tactual := reflect.ValueOf(grammar).Elem().FieldByName(\"A\").String()\n\t\t\tassert.Equal(t, test.expected, actual)\n\t\t}\n\t})\n}\n\nfunc TestModifiers(t *testing.T) {\n\ttype nonEmptyGrammar struct {\n\t\tA string `@( (\"x\"? \"y\"? \"z\"?)! \"b\" )`\n\t}\n\ttests := []interface{ test(t *testing.T) }{\n\t\tmodifierTest[nonEmptyGrammar]{name: \"NonMatchingOptionalNonEmpty\",\n\t\t\tinput: \"b\",\n\t\t\tfail:  true,\n\t\t},\n\t\tmodifierTest[nonEmptyGrammar]{name: \"NonEmptyMatch\",\n\t\t\tinput:    \"x b\",\n\t\t\texpected: \"xb\",\n\t\t},\n\t\tmodifierTest[nonEmptyGrammar]{name: \"NonEmptyMatchAll\",\n\t\t\tinput:    \"x y z b\",\n\t\t\texpected: \"xyzb\",\n\t\t},\n\t\tmodifierTest[nonEmptyGrammar]{name: \"NonEmptyMatchSome\",\n\t\t\tinput:    \"x z b\",\n\t\t\texpected: \"xzb\",\n\t\t},\n\t\tmodifierTest[struct {\n\t\t\tA string `@( \"a\"? \"b\" )`\n\t\t}]{name: \"MatchingOptional\",\n\t\t\tinput:    \"a b\",\n\t\t\texpected: \"ab\",\n\t\t},\n\t\tmodifierTest[struct {\n\t\t\tA string `@( \"a\"? \"b\" )`\n\t\t}]{name: \"NonMatchingOptionalIsSkipped\",\n\t\t\tinput:    \"b\",\n\t\t\texpected: \"b\",\n\t\t},\n\t\tmodifierTest[struct {\n\t\t\tA string `@( \"a\"+ )`\n\t\t}]{name: \"MatchingOneOrMore\",\n\t\t\tinput:    \"a a a a a\",\n\t\t\texpected: \"aaaaa\",\n\t\t},\n\t\tmodifierTest[struct {\n\t\t\tA string `@( \"a\"+ )`\n\t\t}]{name: \"NonMatchingOneOrMore\",\n\t\t\tinput: \"\",\n\t\t\tfail:  true,\n\t\t},\n\t\tmodifierTest[struct {\n\t\t\tA string `@( \"a\"* )`\n\t\t}]{name: \"MatchingZeroOrMore\",\n\t\t\tinput: \"aaaaaaa\",\n\t\t\tfail:  true,\n\t\t},\n\t\tmodifierTest[struct {\n\t\t\tA string `@( \"a\"* )`\n\t\t}]{name: \"NonMatchingZeroOrMore\",\n\t\t\tinput: \"\",\n\t\t},\n\t}\n\tfor _, test := range tests {\n\t\ttest.test(t)\n\t}\n}\n\nfunc TestNonEmptyMatchWithOptionalGroup(t *testing.T) {\n\ttype term struct {\n\t\tMinus bool   `@'-'?`\n\t\tName  string `@Ident`\n\t}\n\ttype grammar struct {\n\t\tStart term `parser:\"'[' (@@?\"`\n\t\tEnd   term `parser:\"     (':' @@)?)! ']'\"`\n\t}\n\n\tp := mustTestParser[grammar](t)\n\n\tresult, err := p.ParseString(\"\", \"[-x]\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, &grammar{Start: term{Minus: true, Name: \"x\"}}, result)\n\n\tresult, err = p.ParseString(\"\", \"[a:-b]\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, &grammar{Start: term{Name: \"a\"}, End: term{Minus: true, Name: \"b\"}}, result)\n\n\tresult, err = p.ParseString(\"\", \"[:end]\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, &grammar{End: term{Name: \"end\"}}, result)\n\n\t_, err = p.ParseString(\"\", \"[]\")\n\tassert.EqualError(t, err, `1:2: sub-expression (Term? (\":\" Term)?)! cannot be empty`)\n}\n\nfunc TestIssue60(t *testing.T) {\n\ttype grammar struct {\n\t\tA string `@(\"one\" | | \"two\")`\n\t}\n\t_, err := participle.Build[grammar]()\n\tassert.Error(t, err)\n}\n\ntype Issue62Bar struct {\n\tA int\n}\n\nfunc (x *Issue62Bar) Parse(lex *lexer.PeekingLexer) error {\n\ttoken := lex.Next()\n\tvar err error\n\tx.A, err = strconv.Atoi(token.Value)\n\treturn err\n}\n\ntype Issue62Foo struct {\n\tBars []Issue62Bar `parser:\"@@+\"`\n}\n\nfunc TestIssue62(t *testing.T) {\n\t_, err := participle.Build[Issue62Foo]()\n\tassert.NoError(t, err)\n}\n\n// nolint: structcheck, unused\nfunc TestIssue71(t *testing.T) {\n\ttype Sub struct {\n\t\tname string `@Ident`\n\t}\n\ttype grammar struct {\n\t\tpattern *Sub `@@`\n\t}\n\n\t_, err := participle.Build[grammar]()\n\tassert.Error(t, err)\n}\n\nfunc TestAllowTrailing(t *testing.T) {\n\ttype G struct {\n\t\tName string `@Ident`\n\t}\n\n\tp, err := participle.Build[G]()\n\tassert.NoError(t, err)\n\n\t_, err = p.ParseString(\"\", `hello world`)\n\tassert.Error(t, err)\n\tg, err := p.ParseString(\"\", `hello world`, participle.AllowTrailing(true))\n\tassert.NoError(t, err)\n\tassert.Equal(t, &G{\"hello\"}, g)\n}\n\nfunc TestDisjunctionErrorReporting(t *testing.T) {\n\ttype statement struct {\n\t\tAdd    bool `  @\"add\"`\n\t\tRemove bool `| @\"remove\"`\n\t}\n\ttype grammar struct {\n\t\tStatements []*statement `\"{\" ( @@ )* \"}\"`\n\t}\n\tp := mustTestParser[grammar](t)\n\t_, err := p.ParseString(\"\", `{ add foo }`)\n\t// TODO: This should produce a more useful error. This is returned by sequence.Parse().\n\tassert.EqualError(t, err, `1:7: unexpected token \"foo\" (expected \"}\")`)\n}\n\nfunc TestCustomInt(t *testing.T) {\n\ttype MyInt int\n\ttype G struct {\n\t\tValue MyInt `@Int`\n\t}\n\n\tp, err := participle.Build[G]()\n\tassert.NoError(t, err)\n\n\tg, err := p.ParseString(\"\", `42`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, &G{42}, g)\n}\n\nfunc TestBoolIfSet(t *testing.T) {\n\ttype G struct {\n\t\tValue bool `@\"true\"?`\n\t}\n\n\tp, err := participle.Build[G]()\n\tassert.NoError(t, err)\n\n\tg, err := p.ParseString(\"\", `true`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, &G{true}, g)\n\tg, err = p.ParseString(\"\", ``)\n\tassert.NoError(t, err)\n\tassert.Equal(t, &G{false}, g)\n}\n\nfunc TestCustomBoolIfSet(t *testing.T) {\n\ttype MyBool bool\n\ttype G struct {\n\t\tValue MyBool `@\"true\"?`\n\t}\n\n\tp, err := participle.Build[G]()\n\tassert.NoError(t, err)\n\n\tg, err := p.ParseString(\"\", `true`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, &G{true}, g)\n\tg, err = p.ParseString(\"\", ``)\n\tassert.NoError(t, err)\n\tassert.Equal(t, &G{false}, g)\n}\n\nfunc TestPointerToList(t *testing.T) {\n\ttype grammar struct {\n\t\tList *[]string `@Ident*`\n\t}\n\tp := mustTestParser[grammar](t)\n\tast, err := p.ParseString(\"\", `foo bar`)\n\tassert.NoError(t, err)\n\tl := []string{\"foo\", \"bar\"}\n\tassert.Equal(t, &grammar{List: &l}, ast)\n}\n\n// I'm not sure if this is a problem that should be solved like this.\n\n// func TestMatchHydratesNullFields(t *testing.T) {\n// \ttype grammar struct {\n// \t\tList []string `\"{\" @Ident* \"}\"`\n// \t}\n// \tp := mustTestParser[grammar](t)\n// \tast := &grammar{}\n// \terr := p.ParseString(`{}`, ast)\n// \tassert.NoError(t, err)\n// \tassert.NotNil(t, ast.List)\n// }\n\nfunc TestNegation(t *testing.T) {\n\ttype grammar struct {\n\t\tEverythingUntilSemicolon *[]string `@!';'* @';'`\n\t}\n\tp := mustTestParser[grammar](t)\n\tast, err := p.ParseString(\"\", `hello world ;`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, &[]string{\"hello\", \"world\", \";\"}, ast.EverythingUntilSemicolon)\n\n\t_, err = p.ParseString(\"\", `hello world`)\n\tassert.Error(t, err)\n}\n\nfunc TestNegationWithPattern(t *testing.T) {\n\ttype grammar struct {\n\t\tEverythingMoreComplex *[]string `@!(';' String)* @';' @String`\n\t}\n\n\tp := mustTestParser[grammar](t, participle.Unquote())\n\t// j, err := json.MarshalIndent(p.root, \"\", \"  \")\n\t// log.Print(j)\n\t// log.Print(ebnf(p.root))\n\tast, err := p.ParseString(\"\", `hello world ; \"some-str\"`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, &[]string{\"hello\", \"world\", \";\", `some-str`}, ast.EverythingMoreComplex)\n\n\tast, err = p.ParseString(\"\", `hello ; world ; \"hey\"`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, &[]string{\"hello\", \";\", \"world\", \";\", `hey`}, ast.EverythingMoreComplex)\n\n\t_, err = p.ParseString(\"\", `hello ; world ;`)\n\tassert.Error(t, err)\n}\n\nfunc TestNegationWithDisjunction(t *testing.T) {\n\ttype grammar struct {\n\t\tEverythingMoreComplex *[]string `@!(';' | ',')* @(';' | ',')`\n\t}\n\n\t// Note: we need more lookahead since (';' String) needs some before failing to match\n\tp := mustTestParser[grammar](t)\n\tast, err := p.ParseString(\"\", `hello world ;`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, &[]string{\"hello\", \"world\", \";\"}, ast.EverythingMoreComplex)\n\n\tast, err = p.ParseString(\"\", `hello world , `)\n\tassert.NoError(t, err)\n\tassert.Equal(t, &[]string{\"hello\", \"world\", \",\"}, ast.EverythingMoreComplex)\n}\n\nfunc TestNegationLookaheadError(t *testing.T) {\n\ttype grammar struct {\n\t\tStuff []string `@Ident @!('.' | '#') @Ident`\n\t}\n\tp := mustTestParser[grammar](t)\n\n\tast, err := p.ParseString(\"\", `hello, world`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, []string{\"hello\", \",\", \"world\"}, ast.Stuff)\n\n\t_, err = p.ParseString(\"\", `hello . world`)\n\tassert.EqualError(t, err, `1:7: unexpected token \".\"`)\n}\n\nfunc TestLookaheadGroup_Positive_SingleToken(t *testing.T) {\n\ttype val struct {\n\t\tStr string `  @String`\n\t\tInt int    `| @Int`\n\t}\n\ttype op struct {\n\t\tOp      string `@('+' | '*' (?= @Int))`\n\t\tOperand val    `@@`\n\t}\n\ttype sum struct {\n\t\tLeft val  `@@`\n\t\tOps  []op `@@*`\n\t}\n\tp := mustTestParser[sum](t)\n\n\tast, err := p.ParseString(\"\", `\"x\" + \"y\" + 4`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, &sum{Left: val{Str: `\"x\"`}, Ops: []op{{\"+\", val{Str: `\"y\"`}}, {\"+\", val{Int: 4}}}}, ast)\n\n\tast, err = p.ParseString(\"\", `\"a\" * 4 + \"b\"`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, &sum{Left: val{Str: `\"a\"`}, Ops: []op{{\"*\", val{Int: 4}}, {\"+\", val{Str: `\"b\"`}}}}, ast)\n\n\tast, err = p.ParseString(\"\", `1 * 2 * 3`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, &sum{Left: val{Int: 1}, Ops: []op{{\"*\", val{Int: 2}}, {\"*\", val{Int: 3}}}}, ast)\n\n\t_, err = p.ParseString(\"\", `\"a\" * 'x' + \"b\"`)\n\tassert.EqualError(t, err, `1:7: unexpected token \"'x'\"`)\n\t_, err = p.ParseString(\"\", `4 * 2 + 0 * \"b\"`)\n\tassert.EqualError(t, err, `1:13: unexpected token \"\\\"b\\\"\"`)\n}\n\nfunc TestLookaheadGroup_Negative_SingleToken(t *testing.T) {\n\ttype variable struct {\n\t\tName string `@Ident`\n\t}\n\ttype grammar struct {\n\t\tIdentifiers []variable `((?! 'except'|'end') @@)*`\n\t\tExcept      *variable  `('except' @@)? 'end'`\n\t}\n\tp := mustTestParser[grammar](t)\n\n\tast, err := p.ParseString(\"\", `one two three exception end`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, []variable{{\"one\"}, {\"two\"}, {\"three\"}, {\"exception\"}}, ast.Identifiers)\n\tassert.Zero(t, ast.Except)\n\n\tast, err = p.ParseString(\"\", `anything except this end`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, []variable{{\"anything\"}}, ast.Identifiers)\n\tassert.Equal(t, &variable{\"this\"}, ast.Except)\n\n\tast, err = p.ParseString(\"\", `except the end`)\n\tassert.NoError(t, err)\n\tassert.Zero(t, ast.Identifiers)\n\tassert.Equal(t, &variable{\"the\"}, ast.Except)\n\n\t_, err = p.ParseString(\"\", `no ending`)\n\tassert.EqualError(t, err, `1:10: unexpected token \"<EOF>\" (expected \"end\")`)\n\n\t_, err = p.ParseString(\"\", `no end in sight`)\n\tassert.EqualError(t, err, `1:8: unexpected token \"in\"`)\n}\n\nfunc TestLookaheadGroup_Negative_MultipleTokens(t *testing.T) {\n\ttype grammar struct {\n\t\tParts []string `((?! '.' '.' '.') @(Ident | '.'))*`\n\t}\n\tp := mustTestParser[grammar](t)\n\n\tast, err := p.ParseString(\"\", `x.y.z.`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, []string{\"x\", \".\", \"y\", \".\", \"z\", \".\"}, ast.Parts)\n\n\tast, err = p.ParseString(\"\", `..x..`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, []string{\".\", \".\", \"x\", \".\", \".\"}, ast.Parts)\n\n\tast, err = p.ParseString(\"\", `two.. are fine`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, []string{\"two\", \".\", \".\", \"are\", \"fine\"}, ast.Parts)\n\n\t_, err = p.ParseString(\"\", `but this... is just wrong`)\n\tassert.EqualError(t, err, `1:9: unexpected token \".\"`)\n}\n\nfunc TestASTTokens(t *testing.T) {\n\ttype subject struct {\n\t\tTokens []lexer.Token\n\n\t\tWord string `@Ident`\n\t}\n\n\ttype hello struct {\n\t\tTokens []lexer.Token\n\n\t\tSubject subject `\"hello\" @@`\n\t}\n\n\tp := mustTestParser[hello](t,\n\t\tparticiple.Elide(\"Whitespace\"),\n\t\tparticiple.Lexer(lexer.MustSimple([]lexer.SimpleRule{\n\t\t\t{\"Ident\", `\\w+`},\n\t\t\t{\"Whitespace\", `\\s+`},\n\t\t})))\n\tactual, err := p.ParseString(\"\", \"hello world\")\n\tassert.NoError(t, err)\n\ttokens := []lexer.Token{\n\t\t{-2, \"hello\", lexer.Position{Line: 1, Column: 1}},\n\t\t{-3, \" \", lexer.Position{Offset: 5, Line: 1, Column: 6}},\n\t\t{-2, \"world\", lexer.Position{Offset: 6, Line: 1, Column: 7}},\n\t}\n\texpected := &hello{\n\t\tTokens: tokens,\n\t\tSubject: subject{\n\t\t\tTokens: tokens[1:],\n\t\t\tWord:   \"world\",\n\t\t},\n\t}\n\tassert.Equal(t, expected, actual)\n}\n\nfunc TestCaptureIntoToken(t *testing.T) {\n\ttype ast struct {\n\t\tHead lexer.Token   `@Ident`\n\t\tTail []lexer.Token `@(Ident*)`\n\t}\n\n\tp := mustTestParser[ast](t)\n\tactual, err := p.ParseString(\"\", \"hello waz baz\")\n\tassert.NoError(t, err)\n\texpected := &ast{\n\t\tHead: lexer.Token{-2, \"hello\", lexer.Position{Line: 1, Column: 1}},\n\t\tTail: []lexer.Token{\n\t\t\t{-2, \"waz\", lexer.Position{Offset: 6, Line: 1, Column: 7}},\n\t\t\t{-2, \"baz\", lexer.Position{Offset: 10, Line: 1, Column: 11}},\n\t\t},\n\t}\n\tassert.Equal(t, expected, actual)\n}\n\nfunc TestEndPos(t *testing.T) {\n\ttype Ident struct {\n\t\tPos    lexer.Position\n\t\tEndPos lexer.Position\n\t\tText   string `parser:\"@Ident\"`\n\t}\n\n\ttype AST struct {\n\t\tFirst  *Ident `parser:\"@@\"`\n\t\tSecond *Ident `parser:\"@@\"`\n\t}\n\n\tvar (\n\t\tLexer = lexer.Must(lexer.New(lexer.Rules{\n\t\t\t\"Root\": {\n\t\t\t\t{\"Ident\", `[\\w:]+`, nil},\n\t\t\t\t{\"Whitespace\", `[\\r\\t ]+`, nil},\n\t\t\t},\n\t\t}))\n\n\t\tParser = participle.MustBuild[AST](\n\t\t\tparticiple.Lexer(Lexer),\n\t\t\tparticiple.Elide(\"Whitespace\"),\n\t\t)\n\t)\n\n\tmod, err := Parser.Parse(\"\", strings.NewReader(\"foo bar\"))\n\tassert.NoError(t, err)\n\tassert.Equal(t, 0, mod.First.Pos.Offset)\n\tassert.Equal(t, 3, mod.First.EndPos.Offset)\n}\n\nfunc TestBug(t *testing.T) {\n\ttype A struct {\n\t\tShared string `parser:\"@'1'\"`\n\t\tDiff   string `parser:\"@A\"`\n\t}\n\ttype B struct {\n\t\tShared string `parser:\"@'1'\"`\n\t\tDiff   string `parser:\"@B\"`\n\t}\n\ttype AST struct {\n\t\tBranch string `parser:\"@'branch'\"`\n\t\tA      *A     `parser:\"( @@\"`\n\t\tB      *B     `parser:\"| @@ )\"`\n\t}\n\tvar (\n\t\tlexer = lexer.Must(lexer.New(lexer.Rules{\n\t\t\t\"Root\": {\n\t\t\t\t{\"A\", `@`, nil},\n\t\t\t\t{\"B\", `!`, nil},\n\t\t\t\t{\"Ident\", `[\\w:]+`, nil},\n\t\t\t\t{\"Whitespace\", `[\\r\\t ]+`, nil},\n\t\t\t},\n\t\t}))\n\t\tparser = participle.MustBuild[AST](\n\t\t\tparticiple.Lexer(lexer),\n\t\t\tparticiple.Elide(\"Whitespace\"),\n\t\t)\n\t)\n\texpected := &AST{\n\t\tBranch: \"branch\",\n\t\tB: &B{\n\t\t\tShared: \"1\",\n\t\t\tDiff:   \"!\",\n\t\t},\n\t}\n\tactual, err := parser.Parse(\"name\", strings.NewReader(`branch 1!`))\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\ntype sliceCapture string\n\nfunc (c *sliceCapture) Capture(values []string) error {\n\t*c = sliceCapture(strings.ToUpper(values[0]))\n\treturn nil\n}\n\nfunc TestCaptureOnSliceElements(t *testing.T) { // nolint:dupl\n\ttype capture struct {\n\t\tSingle   *sliceCapture   `@Capture`\n\t\tSlice    []sliceCapture  `@Capture @Capture`\n\t\tSlicePtr []*sliceCapture `@Capture @Capture`\n\t}\n\n\tparser := participle.MustBuild[capture]([]participle.Option{\n\t\tparticiple.Lexer(lexer.MustSimple([]lexer.SimpleRule{\n\t\t\t{Name: \"Capture\", Pattern: `[a-z]{3}`},\n\t\t\t{Name: \"Whitespace\", Pattern: `[\\s|\\n]+`},\n\t\t})),\n\t\tparticiple.Elide(\"Whitespace\"),\n\t}...)\n\n\tcaptured, err := parser.ParseString(\"capture_slice\", `abc def ijk lmn opq`)\n\tassert.NoError(t, err)\n\n\texpectedSingle := sliceCapture(\"ABC\")\n\texpectedSlicePtr1 := sliceCapture(\"LMN\")\n\texpectedSlicePtr2 := sliceCapture(\"OPQ\")\n\texpected := &capture{\n\t\tSingle:   &expectedSingle,\n\t\tSlice:    []sliceCapture{\"DEF\", \"IJK\"},\n\t\tSlicePtr: []*sliceCapture{&expectedSlicePtr1, &expectedSlicePtr2},\n\t}\n\n\tassert.Equal(t, expected, captured)\n}\n\ntype sliceParse string\n\nfunc (s *sliceParse) Parse(lex *lexer.PeekingLexer) error {\n\ttoken := lex.Peek()\n\tif len(token.Value) != 3 {\n\t\treturn participle.NextMatch\n\t}\n\tlex.Next()\n\t*s = sliceParse(strings.Repeat(token.Value, 2))\n\treturn nil\n}\n\nfunc TestParseOnSliceElements(t *testing.T) { // nolint:dupl\n\ttype parse struct {\n\t\tSingle *sliceParse  `@@`\n\t\tSlice  []sliceParse `@@+`\n\t}\n\n\tparser := participle.MustBuild[parse]([]participle.Option{\n\t\tparticiple.Lexer(lexer.MustSimple([]lexer.SimpleRule{\n\t\t\t{Name: \"Element\", Pattern: `[a-z]{3}`},\n\t\t\t{Name: \"Whitespace\", Pattern: `[\\s|\\n]+`},\n\t\t})),\n\t\tparticiple.Elide(\"Whitespace\"),\n\t}...)\n\n\tparsed, err := parser.ParseString(\"parse_slice\", `abc def ijk`)\n\tassert.NoError(t, err)\n\n\texpectedSingle := sliceParse(\"abcabc\")\n\texpected := &parse{\n\t\tSingle: &expectedSingle,\n\t\tSlice:  []sliceParse{\"defdef\", \"ijkijk\"},\n\t}\n\n\tassert.Equal(t, expected, parsed)\n}\n\nfunc TestUnmarshalNetIP(t *testing.T) {\n\ttype grammar struct {\n\t\tIP net.IP `@IP`\n\t}\n\n\tparser := mustTestParser[grammar](t, participle.Lexer(lexer.MustSimple([]lexer.SimpleRule{\n\t\t{\"IP\", `[\\d.]+`},\n\t})))\n\tast, err := parser.ParseString(\"\", \"10.2.3.4\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, \"10.2.3.4\", ast.IP.String())\n}\n\ntype Address net.IP\n\nfunc (a *Address) Capture(values []string) error {\n\tfmt.Println(\"does not run at all\")\n\t*a = Address(net.ParseIP(values[0]))\n\treturn nil\n}\n\nfunc TestCaptureIP(t *testing.T) {\n\ttype grammar struct {\n\t\tIP Address `@IP`\n\t}\n\n\tparser := mustTestParser[grammar](t, participle.Lexer(lexer.MustSimple([]lexer.SimpleRule{\n\t\t{\"IP\", `[\\d.]+`},\n\t})))\n\tast, err := parser.ParseString(\"\", \"10.2.3.4\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, \"10.2.3.4\", (net.IP)(ast.IP).String())\n}\n\nfunc BenchmarkIssue143(b *testing.B) {\n\ttype Disjunction struct {\n\t\tLong1 bool `parser:\"  '<' '1' ' ' 'l' 'o' 'n' 'g' ' ' 'r' 'u' 'l' 'e' ' ' 't' 'o' ' ' 'f' 'o' 'r' 'm' 'a' 't' '>'\"`\n\t\tLong2 bool `parser:\"| '<' '2' ' ' 'l' 'o' 'n' 'g' ' ' 'r' 'u' 'l' 'e' ' ' 't' 'o' ' ' 'f' 'o' 'r' 'm' 'a' 't' '>'\"`\n\t\tLong3 bool `parser:\"| '<' '3' ' ' 'l' 'o' 'n' 'g' ' ' 'r' 'u' 'l' 'e' ' ' 't' 'o' ' ' 'f' 'o' 'r' 'm' 'a' 't' '>'\"`\n\t\tLong4 bool `parser:\"| '<' '4' ' ' 'l' 'o' 'n' 'g' ' ' 'r' 'u' 'l' 'e' ' ' 't' 'o' ' ' 'f' 'o' 'r' 'm' 'a' 't' '>'\"`\n\t\tReal  bool `parser:\"| '<' 'x' '>'\"`\n\t}\n\n\ttype Disjunctions struct {\n\t\tList []Disjunction `parser:\"@@*\"`\n\t}\n\n\tvar disjunctionParser = participle.MustBuild[Disjunctions]()\n\tinput := \"<x> <x> <x> <x> <x> <x> <x> <x> <x> <x> <x> <x> <x> <x> <x> <x> <x> <x> <x> <x>\"\n\tb.ResetTimer()\n\tb.ReportAllocs()\n\tfor i := 0; i < b.N; i++ {\n\t\tif _, err := disjunctionParser.ParseString(\"\", input); err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n}\n\ntype Boxes struct {\n\tPos   lexer.Position\n\tBoxes Box `@Ident`\n}\n\ntype Box struct {\n\tPos lexer.Position\n\tVal string `@Ident`\n}\n\nfunc (b *Box) Capture(values []string) error {\n\tb.Val = values[0]\n\treturn nil\n}\n\nfunc TestBoxedCapture(t *testing.T) {\n\tlex := lexer.MustSimple([]lexer.SimpleRule{\n\t\t{\"Ident\", `[a-zA-Z](\\w|\\.|/|:|-)*`},\n\t\t{\"whitespace\", `\\s+`},\n\t})\n\n\tparser := participle.MustBuild[Boxes](\n\t\tparticiple.Lexer(lex),\n\t\tparticiple.UseLookahead(2),\n\t)\n\n\tif _, err := parser.ParseString(\"test\", \"abc::cdef.abc\"); err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc TestMatchEOF(t *testing.T) {\n\ttype testMatchNewlineOrEOF struct {\n\t\tText []string `@Ident+ (\"\\n\" | EOF)`\n\t}\n\tp := mustTestParser[testMatchNewlineOrEOF](t)\n\t_, err := p.ParseString(\"\", \"hell world\")\n\tassert.NoError(t, err)\n\t_, err = p.ParseString(\"\", \"hell world\\n\")\n\tassert.NoError(t, err)\n}\n\nfunc TestParseExplicitElidedIdent(t *testing.T) { // nolint\n\tlex := lexer.MustSimple([]lexer.SimpleRule{\n\t\t{\"Ident\", `[a-zA-Z](\\w|\\.|/|:|-)*`},\n\t\t{\"Comment\", `/\\*[^*]*\\*/`},\n\t\t{\"whitespace\", `\\s+`},\n\t})\n\ttype grammar struct {\n\t\tComment string `@Comment?`\n\t\tIdent   string `@Ident`\n\t}\n\tp := mustTestParser[grammar](t, participle.Lexer(lex), participle.Elide(\"Comment\"))\n\n\tactual, err := p.ParseString(\"\", `hello`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, &grammar{Ident: \"hello\"}, actual)\n\n\tactual, err = p.ParseString(\"\", `/* Comment */ hello`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, &grammar{Comment: `/* Comment */`, Ident: \"hello\"}, actual)\n}\n\nfunc TestParseExplicitElidedTypedLiteral(t *testing.T) { // nolint\n\tlex := lexer.MustSimple([]lexer.SimpleRule{\n\t\t{\"Ident\", `[a-zA-Z](\\w|\\.|/|:|-)*`},\n\t\t{\"Comment\", `/\\*[^*]*\\*/`},\n\t\t{\"whitespace\", `\\s+`},\n\t})\n\ttype grammar struct {\n\t\tComment string `@\"/* Comment */\":Comment?`\n\t\tIdent   string `@Ident`\n\t}\n\tp := mustTestParser[grammar](t, participle.Lexer(lex), participle.Elide(\"Comment\"))\n\n\tactual, err := p.ParseString(\"\", `hello`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, &grammar{Ident: \"hello\"}, actual)\n\n\tactual, err = p.ParseString(\"\", `/* Comment */ hello`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, &grammar{Comment: `/* Comment */`, Ident: \"hello\"}, actual)\n}\n\nfunc TestEmptySequenceMatches(t *testing.T) {\n\tlex := lexer.MustSimple([]lexer.SimpleRule{\n\t\t{\"Ident\", `[a-zA-Z](\\w|\\.|/|:|-)*`},\n\t\t{\"Comment\", `/\\*[^*]*\\*/`},\n\t\t{\"Whitespace\", `\\s+`},\n\t})\n\ttype grammar struct {\n\t\tIdent    []string `@Ident*`\n\t\tComments []string `@Comment*`\n\t}\n\tp := mustTestParser[grammar](t, participle.Lexer(lex), participle.Elide(\"Whitespace\"))\n\texpected := &grammar{}\n\tactual, err := p.ParseString(\"\", \"\")\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n}\n\ntype RootParseableFail struct{}\n\nfunc (*RootParseableFail) String() string   { return \"\" }\nfunc (*RootParseableFail) GoString() string { return \"\" }\nfunc (*RootParseableFail) Parse(lex *lexer.PeekingLexer) error {\n\treturn errors.New(\"always fail immediately\")\n}\n\nfunc TestRootParseableFail(t *testing.T) {\n\tp := mustTestParser[RootParseableFail](t)\n\t_, err := p.ParseString(\"<test>\", \"blah\")\n\tassert.EqualError(t, err, \"<test>:1:1: always fail immediately\")\n}\n\ntype (\n\tTestCustom interface{ isTestCustom() }\n\n\tCustomIdent   string\n\tCustomNumber  float64\n\tCustomBoolean bool\n)\n\nfunc (CustomIdent) isTestCustom()   {}\nfunc (CustomNumber) isTestCustom()  {}\nfunc (CustomBoolean) isTestCustom() {}\n\nfunc TestParserWithCustomProduction(t *testing.T) {\n\ttype grammar struct {\n\t\tCustom TestCustom `@@`\n\t}\n\n\tp := mustTestParser[grammar](t, participle.ParseTypeWith(func(lex *lexer.PeekingLexer) (TestCustom, error) {\n\t\tswitch peek := lex.Peek(); {\n\t\tcase peek.Type == scanner.Int || peek.Type == scanner.Float:\n\t\t\tv, err := strconv.ParseFloat(lex.Next().Value, 64)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\treturn CustomNumber(v), nil\n\t\tcase peek.Type == scanner.Ident:\n\t\t\tname := lex.Next().Value\n\t\t\tif name == \"true\" || name == \"false\" {\n\t\t\t\treturn CustomBoolean(name == \"true\"), nil\n\t\t\t}\n\t\t\treturn CustomIdent(name), nil\n\t\tdefault:\n\t\t\treturn nil, participle.NextMatch\n\t\t}\n\t}))\n\n\ttype testCase struct {\n\t\tsrc      string\n\t\texpected TestCustom\n\t}\n\n\tfor _, c := range []testCase{\n\t\t{\"a\", CustomIdent(\"a\")},\n\t\t{\"12.5\", CustomNumber(12.5)},\n\t\t{\"true\", CustomBoolean(true)},\n\t\t{\"false\", CustomBoolean(false)},\n\t} {\n\t\tactual, err := p.ParseString(\"\", c.src)\n\t\tassert.NoError(t, err)\n\t\tassert.Equal(t, c.expected, actual.Custom)\n\t}\n\n\tassert.Equal(t, `Grammar = TestCustom .`, p.String())\n}\n\ntype (\n\tTestUnionA interface{ isTestUnionA() }\n\tTestUnionB interface{ isTestUnionB() }\n\n\tAMember1 struct {\n\t\tV string `@Ident`\n\t}\n\n\tAMember2 struct {\n\t\tV TestUnionB `\"[\" @@ \"]\"`\n\t}\n\n\tBMember1 struct {\n\t\tV float64 `@Int | @Float`\n\t}\n\n\tBMember2 struct {\n\t\tV TestUnionA `\"{\" @@ \"}\"`\n\t}\n)\n\nfunc (AMember1) isTestUnionA() {}\nfunc (AMember2) isTestUnionA() {}\n\nfunc (BMember1) isTestUnionB() {}\nfunc (BMember2) isTestUnionB() {}\n\nfunc TestParserWithUnion(t *testing.T) {\n\ttype grammar struct {\n\t\tA TestUnionA `@@`\n\t\tB TestUnionB `| @@`\n\t}\n\n\tparser := mustTestParser[grammar](t, participle.UseLookahead(10),\n\t\tparticiple.Union[TestUnionA](AMember1{}, AMember2{}),\n\t\tparticiple.Union[TestUnionB](BMember1{}, BMember2{}))\n\n\ttype testCase struct {\n\t\tsrc      string\n\t\texpected grammar\n\t}\n\n\tfor _, c := range []testCase{\n\t\t{`a`, grammar{A: AMember1{\"a\"}}},\n\t\t{`1.5`, grammar{B: BMember1{1.5}}},\n\t\t{`[2.5]`, grammar{A: AMember2{BMember1{2.5}}}},\n\t\t{`{x}`, grammar{B: BMember2{AMember1{\"x\"}}}},\n\t\t{`{ [ { [12] } ] }`, grammar{B: BMember2{AMember2{BMember2{AMember2{BMember1{12}}}}}}},\n\t} {\n\t\tvar trace strings.Builder\n\t\tactual, err := parser.ParseString(\"\", c.src, participle.Trace(&trace))\n\t\tassert.NoError(t, err)\n\t\tassert.Equal(t, &c.expected, actual) //nolint:gosec\n\t\tassert.NotEqual(t, \"\", trace.String())\n\t}\n\n\tassert.Equal(t, strings.TrimSpace(`\nGrammar = TestUnionA | TestUnionB .\nTestUnionA = AMember1 | AMember2 .\nAMember1 = <ident> .\nAMember2 = \"[\" TestUnionB \"]\" .\nTestUnionB = BMember1 | BMember2 .\nBMember1 = <int> | <float> .\nBMember2 = \"{\" TestUnionA \"}\" .\n\t`), parser.String())\n}\n\nfunc TestParseSubProduction(t *testing.T) {\n\ttype (\n\t\tListItem struct {\n\t\t\tNumber *float64 `(@Int | @Float)`\n\t\t\tString *string  `| @String`\n\t\t}\n\n\t\tGrammar struct {\n\t\t\tList []ListItem `\"[\" @@ (\",\" @@)* \"]\"`\n\t\t}\n\t)\n\n\tnumberItem := func(n float64) ListItem { return ListItem{Number: &n} }\n\tstringItem := func(s string) ListItem { return ListItem{String: &s} }\n\n\tp := mustTestParser[Grammar](t, participle.Unquote())\n\n\texpected := &Grammar{List: []ListItem{numberItem(1), stringItem(\"test\")}}\n\n\tactual, err := p.ParseString(\"\", `[ 1, \"test\" ]`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, expected, actual)\n\n\texpectedItem := numberItem(1.234e5)\n\n\tip, err := participle.ParserForProduction[ListItem](p)\n\tassert.NoError(t, err)\n\n\tactualItem, err := ip.ParseString(\"\", `1.234e5`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, &expectedItem, actualItem)\n\n\texpectedItem2 := stringItem(\"\\t\\ttest\\t\\t\")\n\n\tactualItem2, err := ip.ParseString(\"\", `\"\\t\\ttest\\t\\t\"`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, &expectedItem2, actualItem2)\n}\n\ntype I255Grammar struct {\n\tUnion I255Union `@@`\n}\n\ntype I255Union interface{ union() }\n\ntype I255String struct {\n\tValue string `@String`\n}\n\nfunc (*I255String) union() {}\n\nfunc TestIssue255(t *testing.T) {\n\tparser, err := participle.Build[I255Grammar](\n\t\tparticiple.Union[I255Union](&I255String{}),\n\t)\n\tassert.NoError(t, err)\n\tg, err := parser.ParseString(\"\", `\"Hello, World!\"`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, &I255Grammar{Union: &I255String{Value: `\"Hello, World!\"`}}, g)\n}\n\nfunc TestParseNumbers(t *testing.T) {\n\ttype grammar struct {\n\t\tInt   int8    `@('-'? Int)`\n\t\tUint  uint16  `@('-'? Int)`\n\t\tFloat float64 `@Ident`\n\t}\n\tparser := participle.MustBuild[grammar]()\n\t_, err := parser.ParseString(\"\", `300 0 x`)\n\tassert.EqualError(t, err, `grammar.Int: strconv.ParseInt: parsing \"300\": value out of range`)\n\t_, err = parser.ParseString(\"\", `-2 -2 x`)\n\tassert.EqualError(t, err, `grammar.Uint: strconv.ParseUint: parsing \"-2\": invalid syntax`)\n\t_, err = parser.ParseString(\"\", `0 0 nope`)\n\tassert.EqualError(t, err, `grammar.Float: strconv.ParseFloat: parsing \"nope\": invalid syntax`)\n\tresult, err := parser.ParseString(\"\", `-30 3000 Inf`)\n\tassert.NoError(t, err)\n\tassert.Equal(t, grammar{Int: -30, Uint: 3000, Float: math.Inf(1)}, *result)\n}\n"
        },
        {
          "name": "railroad.png",
          "type": "blob",
          "size": 86.1474609375,
          "content": null
        },
        {
          "name": "renovate.json5",
          "type": "blob",
          "size": 0.376953125,
          "content": "{\n\t$schema: \"https://docs.renovatebot.com/renovate-schema.json\",\n\textends: [\n\t\t\"config:recommended\",\n\t\t\":semanticCommits\",\n\t\t\":semanticCommitTypeAll(chore)\",\n\t\t\":semanticCommitScope(deps)\",\n\t\t\"group:allNonMajor\",\n\t\t\"schedule:earlyMondays\", // Run once a week.\n\t],\n\tpackageRules: [\n\t\t{\n\t\t\tmatchPackageNames: [\"golangci-lint\"],\n\t\t\tmatchManagers: [\"hermit\"],\n\t\t\tenabled: false,\n\t\t},\n\t],\n}\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "struct.go",
          "type": "blob",
          "size": 4.6650390625,
          "content": "package participle\n\nimport (\n\t\"fmt\"\n\t\"reflect\"\n\t\"strconv\"\n\t\"strings\"\n\t\"text/scanner\"\n\t\"unicode/utf8\"\n\n\t\"github.com/alecthomas/participle/v2/lexer\"\n)\n\n// A structLexer lexes over the tags of struct fields while tracking the current field.\ntype structLexer struct {\n\ts       reflect.Type\n\tfield   int\n\tindexes [][]int\n\tlexer   *lexer.PeekingLexer\n}\n\nfunc lexStruct(s reflect.Type) (*structLexer, error) {\n\tindexes, err := collectFieldIndexes(s)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tslex := &structLexer{\n\t\ts:       s,\n\t\tindexes: indexes,\n\t}\n\tif len(slex.indexes) > 0 {\n\t\ttag := fieldLexerTag(slex.Field().StructField)\n\t\tslex.lexer, err = lexer.Upgrade(newTagLexer(s.Name(), tag))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\treturn slex, nil\n}\n\n// NumField returns the number of fields in the struct associated with this structLexer.\nfunc (s *structLexer) NumField() int {\n\treturn len(s.indexes)\n}\n\ntype structLexerField struct {\n\treflect.StructField\n\tIndex []int\n}\n\n// Field returns the field associated with the current token.\nfunc (s *structLexer) Field() structLexerField {\n\treturn s.GetField(s.field)\n}\n\nfunc (s *structLexer) GetField(field int) structLexerField {\n\tif field >= len(s.indexes) {\n\t\tfield = len(s.indexes) - 1\n\t}\n\treturn structLexerField{\n\t\tStructField: s.s.FieldByIndex(s.indexes[field]),\n\t\tIndex:       s.indexes[field],\n\t}\n}\n\nfunc (s *structLexer) Peek() (*lexer.Token, error) {\n\tfield := s.field\n\tlex := s.lexer\n\tfor {\n\t\ttoken := lex.Peek()\n\t\tif !token.EOF() {\n\t\t\ttoken.Pos.Line = field + 1\n\t\t\treturn token, nil\n\t\t}\n\t\tfield++\n\t\tif field >= s.NumField() {\n\t\t\tt := lexer.EOFToken(token.Pos)\n\t\t\treturn &t, nil\n\t\t}\n\t\tft := s.GetField(field).StructField\n\t\ttag := fieldLexerTag(ft)\n\t\tvar err error\n\t\tlex, err = lexer.Upgrade(newTagLexer(ft.Name, tag))\n\t\tif err != nil {\n\t\t\treturn token, err\n\t\t}\n\t}\n}\n\nfunc (s *structLexer) Next() (*lexer.Token, error) {\n\ttoken := s.lexer.Next()\n\tif !token.EOF() {\n\t\ttoken.Pos.Line = s.field + 1\n\t\treturn token, nil\n\t}\n\tif s.field+1 >= s.NumField() {\n\t\tt := lexer.EOFToken(token.Pos)\n\t\treturn &t, nil\n\t}\n\ts.field++\n\tft := s.Field().StructField\n\ttag := fieldLexerTag(ft)\n\tvar err error\n\ts.lexer, err = lexer.Upgrade(newTagLexer(ft.Name, tag))\n\tif err != nil {\n\t\treturn token, err\n\t}\n\treturn s.Next()\n}\n\nfunc fieldLexerTag(field reflect.StructField) string {\n\tif tag, ok := field.Tag.Lookup(\"parser\"); ok {\n\t\treturn tag\n\t}\n\treturn string(field.Tag)\n}\n\n// Recursively collect flattened indices for top-level fields and embedded fields.\nfunc collectFieldIndexes(s reflect.Type) (out [][]int, err error) {\n\tif s.Kind() != reflect.Struct {\n\t\treturn nil, fmt.Errorf(\"expected a struct but got %q\", s)\n\t}\n\tdefer decorate(&err, s.String)\n\tfor i := 0; i < s.NumField(); i++ {\n\t\tf := s.Field(i)\n\t\tswitch {\n\t\tcase f.Anonymous && f.Type.Kind() == reflect.Struct: // Embedded struct.\n\t\t\tchildren, err := collectFieldIndexes(f.Type)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tfor _, idx := range children {\n\t\t\t\tout = append(out, append(f.Index, idx...))\n\t\t\t}\n\n\t\tcase f.PkgPath != \"\":\n\t\t\tcontinue\n\n\t\tcase fieldLexerTag(f) != \"\":\n\t\t\tout = append(out, f.Index)\n\t\t}\n\t}\n\treturn\n}\n\n// tagLexer is a Lexer based on text/scanner.Scanner\ntype tagLexer struct {\n\tscanner  *scanner.Scanner\n\tfilename string\n\terr      error\n}\n\nfunc newTagLexer(filename string, tag string) *tagLexer {\n\ts := &scanner.Scanner{}\n\ts.Init(strings.NewReader(tag))\n\tlexer := &tagLexer{\n\t\tfilename: filename,\n\t\tscanner:  s,\n\t}\n\tlexer.scanner.Error = func(s *scanner.Scanner, msg string) {\n\t\t// This is to support single quoted strings. Hacky.\n\t\tif !strings.HasSuffix(msg, \"char literal\") {\n\t\t\tlexer.err = fmt.Errorf(\"%s: %s\", lexer.scanner.Pos(), msg)\n\t\t}\n\t}\n\treturn lexer\n}\n\nfunc (t *tagLexer) Next() (lexer.Token, error) {\n\ttyp := t.scanner.Scan()\n\ttext := t.scanner.TokenText()\n\tpos := lexer.Position(t.scanner.Position)\n\tpos.Filename = t.filename\n\tif t.err != nil {\n\t\treturn lexer.Token{}, t.err\n\t}\n\treturn textScannerTransform(lexer.Token{\n\t\tType:  lexer.TokenType(typ),\n\t\tValue: text,\n\t\tPos:   pos,\n\t})\n}\n\nfunc textScannerTransform(token lexer.Token) (lexer.Token, error) {\n\t// Unquote strings.\n\tswitch token.Type {\n\tcase scanner.Char:\n\t\t// FIXME(alec): This is pretty hacky...we convert a single quoted char into a double\n\t\t// quoted string in order to support single quoted strings.\n\t\ttoken.Value = fmt.Sprintf(\"\\\"%s\\\"\", token.Value[1:len(token.Value)-1])\n\t\tfallthrough\n\tcase scanner.String:\n\t\ts, err := strconv.Unquote(token.Value)\n\t\tif err != nil {\n\t\t\treturn lexer.Token{}, Errorf(token.Pos, \"%s: %q\", err.Error(), token.Value)\n\t\t}\n\t\ttoken.Value = s\n\t\tif token.Type == scanner.Char && utf8.RuneCountInString(s) > 1 {\n\t\t\ttoken.Type = scanner.String\n\t\t}\n\tcase scanner.RawString:\n\t\ttoken.Value = token.Value[1 : len(token.Value)-1]\n\n\tdefault:\n\t}\n\treturn token, nil\n}\n"
        },
        {
          "name": "struct_test.go",
          "type": "blob",
          "size": 1.95703125,
          "content": "package participle\n\nimport (\n\t\"reflect\"\n\t\"testing\"\n\t\"text/scanner\"\n\n\trequire \"github.com/alecthomas/assert/v2\"\n\t\"github.com/alecthomas/participle/v2/lexer\"\n)\n\nfunc TestStructLexerTokens(t *testing.T) {\n\ttype testScanner struct {\n\t\tA string `12`\n\t\tB string `34`\n\t}\n\n\tscan, err := lexStruct(reflect.TypeOf(testScanner{}))\n\trequire.NoError(t, err)\n\tt12 := lexer.Token{Type: scanner.Int, Value: \"12\", Pos: lexer.Position{Filename: \"testScanner\", Line: 1, Column: 1}}\n\tt34 := lexer.Token{Type: scanner.Int, Value: \"34\", Pos: lexer.Position{Filename: \"B\", Line: 2, Column: 1}}\n\trequire.Equal(t, t12, *mustPeek(scan))\n\trequire.Equal(t, 0, scan.field)\n\trequire.Equal(t, t12, *mustNext(scan))\n\n\trequire.Equal(t, t34, *mustPeek(scan))\n\trequire.Equal(t, 0, scan.field)\n\trequire.Equal(t, t34, *mustNext(scan))\n\trequire.Equal(t, 1, scan.field)\n\n\trequire.True(t, mustNext(scan).EOF())\n}\n\nfunc TestStructLexer(t *testing.T) {\n\tg := struct {\n\t\tA string `\"a\"|`\n\t\tB string `\"b\"`\n\t}{}\n\n\tgt := reflect.TypeOf(g)\n\tr, err := lexStruct(gt)\n\trequire.NoError(t, err)\n\tf := []structLexerField{}\n\ts := \"\"\n\tfor {\n\t\t_, err := r.Peek()\n\t\trequire.NoError(t, err)\n\t\trn, err := r.Next()\n\t\trequire.NoError(t, err)\n\t\tif rn.EOF() {\n\t\t\tbreak\n\t\t}\n\t\tf = append(f, r.Field())\n\t\ts += rn.String()\n\t}\n\trequire.Equal(t, `a|b`, s)\n\tf0 := r.GetField(0)\n\tf1 := r.GetField(1)\n\trequire.Equal(t, []structLexerField{f0, f0, f1}, f)\n}\n\ntype testEmbeddedIndexes struct {\n\tA string `@String`\n\tB string `@String`\n}\n\nfunc TestCollectFieldIndexes(t *testing.T) {\n\tvar grammar struct {\n\t\ttestEmbeddedIndexes\n\t\tC string `@String`\n\t}\n\ttyp := reflect.TypeOf(grammar)\n\tindexes, err := collectFieldIndexes(typ)\n\trequire.NoError(t, err)\n\trequire.Equal(t, [][]int{{0, 0}, {0, 1}, {1}}, indexes)\n}\n\nfunc mustPeek(scan *structLexer) *lexer.Token {\n\ttoken, err := scan.Peek()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn token\n}\n\nfunc mustNext(scan *structLexer) *lexer.Token { // nolint: interfacer\n\ttoken, err := scan.Next()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn token\n}\n"
        },
        {
          "name": "validate.go",
          "type": "blob",
          "size": 1.0498046875,
          "content": "package participle\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n)\n\n// Perform some post-construction validation. This currently does:\n//\n// Checks for left recursion.\nfunc validate(n node) error {\n\tchecked := map[*strct]bool{}\n\tseen := map[node]bool{}\n\n\treturn visit(n, func(n node, next func() error) error {\n\t\tif n, ok := n.(*strct); ok {\n\t\t\tif !checked[n] && isLeftRecursive(n) {\n\t\t\t\treturn fmt.Errorf(\"left recursion detected on\\n\\n%s\", indent(n.String()))\n\t\t\t}\n\t\t\tchecked[n] = true\n\t\t\tif seen[n] {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t\tseen[n] = true\n\t\treturn next()\n\t})\n}\n\nfunc isLeftRecursive(root *strct) (found bool) {\n\tdefer func() { _ = recover() }()\n\tseen := map[node]bool{}\n\t_ = visit(root.expr, func(n node, next func() error) error {\n\t\tif found {\n\t\t\treturn nil\n\t\t}\n\t\tswitch n := n.(type) {\n\t\tcase *strct:\n\t\t\tif root.typ == n.typ {\n\t\t\t\tfound = true\n\t\t\t}\n\n\t\tcase *sequence:\n\t\t\tif !n.head {\n\t\t\t\tpanic(\"done\")\n\t\t\t}\n\t\t}\n\t\tif seen[n] {\n\t\t\treturn nil\n\t\t}\n\t\tseen[n] = true\n\t\treturn next()\n\t})\n\treturn\n}\n\nfunc indent(s string) string {\n\treturn \"  \" + strings.Join(strings.Split(s, \"\\n\"), \"\\n  \")\n}\n"
        },
        {
          "name": "validate_test.go",
          "type": "blob",
          "size": 1.041015625,
          "content": "package participle_test\n\nimport (\n\t\"testing\"\n\n\trequire \"github.com/alecthomas/assert/v2\"\n\t\"github.com/alecthomas/participle/v2\"\n)\n\ntype leftRecursionSimple struct {\n\tBegin string               `  @Ident`\n\tMore  *leftRecursionSimple `| @@ \"more\"`\n}\n\nfunc TestValidateLeftRecursion(t *testing.T) {\n\t_, err := participle.Build[leftRecursionSimple]()\n\trequire.Error(t, err)\n\trequire.Equal(t, err.Error(), `left recursion detected on\n\n  LeftRecursionSimple = <ident> | (LeftRecursionSimple \"more\") .`)\n}\n\ntype leftRecursionNestedInner struct {\n\tBegin string               `  @Ident`\n\tNext  *leftRecursionNested `| @@`\n}\n\ntype leftRecursionNested struct {\n\tBegin string                    `  @Ident`\n\tMore  *leftRecursionNestedInner `| @@ \"more\"`\n}\n\nfunc TestValidateLeftRecursionNested(t *testing.T) {\n\t_, err := participle.Build[leftRecursionNested]()\n\trequire.Error(t, err)\n\trequire.Equal(t, err.Error(), `left recursion detected on\n\n  LeftRecursionNested = <ident> | (LeftRecursionNestedInner \"more\") .\n  LeftRecursionNestedInner = <ident> | LeftRecursionNested .`)\n}\n"
        },
        {
          "name": "visit.go",
          "type": "blob",
          "size": 1.150390625,
          "content": "package participle\n\nimport \"fmt\"\n\n// Visit all nodes.\n//\n// Cycles are deliberately not detected, it is up to the visitor function to handle this.\nfunc visit(n node, visitor func(n node, next func() error) error) error {\n\treturn visitor(n, func() error {\n\t\tswitch n := n.(type) {\n\t\tcase *disjunction:\n\t\t\tfor _, child := range n.nodes {\n\t\t\t\tif err := visit(child, visitor); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\tcase *strct:\n\t\t\treturn visit(n.expr, visitor)\n\t\tcase *custom:\n\t\t\treturn nil\n\t\tcase *union:\n\t\t\tfor _, member := range n.disjunction.nodes {\n\t\t\t\tif err := visit(member, visitor); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\tcase *sequence:\n\t\t\tif err := visit(n.node, visitor); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif n.next != nil {\n\t\t\t\treturn visit(n.next, visitor)\n\t\t\t}\n\t\t\treturn nil\n\t\tcase *parseable:\n\t\t\treturn nil\n\t\tcase *capture:\n\t\t\treturn visit(n.node, visitor)\n\t\tcase *reference:\n\t\t\treturn nil\n\t\tcase *negation:\n\t\t\treturn visit(n.node, visitor)\n\t\tcase *literal:\n\t\t\treturn nil\n\t\tcase *group:\n\t\t\treturn visit(n.expr, visitor)\n\t\tcase *lookaheadGroup:\n\t\t\treturn visit(n.expr, visitor)\n\t\tdefault:\n\t\t\tpanic(fmt.Sprintf(\"%T\", n))\n\t\t}\n\t})\n}\n"
        }
      ]
    }
  ]
}