{
  "metadata": {
    "timestamp": 1736567219534,
    "page": 815,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "galeone/tfgo",
      "stars": 2430,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.4345703125,
          "content": "# Binaries for programs and plugins\n*.exe\n*.dll\n*.so\n*.dylib\n\n# Test binary, build with `go test -c`\n*.test\n\n# Output of the go coverage tool, specifically when used with LiteIDE\n*.out\n\n# Project-local glide cache, RE: https://github.com/Masterminds/glide/issues/736\n.glide/\n\n# Trained models\ntest_models/log/*\ntest_models/export/*\ntest_models/models/*\ntest_models/output/*\n\n# Coverage HTML\ncover.html\n\n# Editor configuration information\n.idea/\n"
        },
        {
          "name": ".readme",
          "type": "tree",
          "content": null
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 1.3359375,
          "content": "language: go\nos: linux\ndist: focal\nenv:\n    global:\n        - TF_VERSION_MAJOR=2\n        - TF_VERSION_MINOR=9\n        - TF_VERSION_PATCH=1\n\nbefore_install:\n  # Install TensorFlow\n  - sudo pip3 install --progress-bar off tensorflow==\"$TF_VERSION_MAJOR\".\"$TF_VERSION_MINOR\".\"$TF_VERSION_PATCH\"\n  - sudo pip3 install --progress-bar off protobuf\n  - sudo pip3 install --progress-bar off -r test_models/requirements.txt\n  # Install TensorFlow system library\n  - curl -L --silent \"https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-\"\"$TF_VERSION_MAJOR\".\"$TF_VERSION_MINOR\".\"$TF_VERSION_PATCH\"\".tar.gz\" | sudo tar -C /usr/local -xz\n  - sudo ldconfig\n  # Get and install TensorFlow Go bindings: we also need other packages not automatically downloaded (like proto)\n  - go get github.com/golang/protobuf/proto\n  # NOTE: we use our own fork with the Go package fixed and go-gettable and usable\n  - git clone https://github.com/galeone/tensorflow $GOPATH/src/github.com/galeone/tensorflow/\n  - pushd $GOPATH/src/github.com/galeone/tensorflow/tensorflow/go\n  - git checkout r\"$TF_VERSION_MAJOR\".\"$TF_VERSION_MINOR\"-go\n  - go build\n  - popd\n  # Install Goveralls\n  - go get github.com/mattn/goveralls\n  # Build models - used in tests\n  - pushd test_models\n  - sudo python3 create.py\n  - popd\nscript:\n  - $GOPATH/bin/goveralls -service=travis-ci\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"{}\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright {yyyy} {name of copyright owner}\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.15234375,
          "content": "# tfgo: TensorFlow in Go\n[![GoDoc](https://godoc.org/github.com/galeone/tfgo?status.svg)](https://godoc.org/github.com/galeone/tfgo)\n[![Build Status](https://travis-ci.org/galeone/tfgo.svg?branch=master)](https://travis-ci.org/galeone/tfgo)\n---\n\n- [tfgo: TensorFlow in Go](#tfgo-tensorflow-in-go)\n  - [Dependencies](#dependencies)\n  - [Installation](#installation)\n  - [Getting started](#getting-started)\n  - [Computer Vision using data flow graph](#computer-vision-using-data-flow-graph)\n  - [Train in Python, Serve in Go](#train-in-python-serve-in-go)\n    - [Python code](#python-code)\n    - [Go code](#go-code)\n  - [Why?](#why)\n  - [Contribute](#contribute)\n  - [TensorFlow installation](#tensorflow-installation)\n    - [Manual](#manual)\n    - [Docker](#docker)\n\nTensorFlow's Go bindings are [hard to use](https://pgaleone.eu/tensorflow/go/2017/05/29/understanding-tensorflow-using-go/): tfgo makes it easy!\n\nNo more problems like:\n\n- Scoping: each new node will have a new and unique name\n- Typing: attributes are automatically converted to a supported type instead of throwing errors at runtime\n\nAlso, it uses [Method chaining](https://en.wikipedia.org/wiki/Method_chaining) making possible to write pleasant Go code.\n\n## Dependencies\n\n1. TensorFlow-2.9.1 lib. [How to install tensorflow](#tensorflow-installation).\n2. TensorFlow bindings github.com/galeone/tensorflow. In order to correctly work with TensorFlow 2.9.1 in Go, we have to use a fork I created with some fix for the Go bindings. Bindings can be too large for go mod proxy, so you may want to switch off proxy usage by executing `go env -w GONOSUMDB=\"github.com/galeone/tensorflow\"` to pull code directly using system installed git. It changes nothing in the user interface -- you can use go modules as usual.\n\n## Installation\n\n```\ngo get github.com/galeone/tfgo\n```\n\n## Getting started\n\nThe core data structure of the TensorFlow's Go bindings is the `op.Scope` struct. tfgo allows creating new `*op.Scope` that solves the scoping issue mentioned above.\n\nSince we're defining a graph, let's start from its root (empty graph)\n\n```go\nroot := tg.NewRoot()\n```\n\nWe can now place nodes into this graphs and connect them. Let's say we want to multiply a matrix for a column vector and then add another column vector to the result.\n\nHere's the complete source code.\n\n```go\npackage main\n\nimport (\n        \"fmt\"\n        tg \"github.com/galeone/tfgo\"\n        tf \"github.com/galeone/tensorflow/tensorflow/go\"\n)\n\nfunc main() {\n        root := tg.NewRoot()\n        A := tg.NewTensor(root, tg.Const(root, [2][2]int32{{1, 2}, {-1, -2}}))\n        x := tg.NewTensor(root, tg.Const(root, [2][1]int64{{10}, {100}}))\n        b := tg.NewTensor(root, tg.Const(root, [2][1]int32{{-10}, {10}}))\n        Y := A.MatMul(x.Output).Add(b.Output)\n        // Please note that Y is just a pointer to A!\n\n        // If we want to create a different node in the graph, we have to clone Y\n        // or equivalently A\n        Z := A.Clone()\n        results := tg.Exec(root, []tf.Output{Y.Output, Z.Output}, nil, &tf.SessionOptions{})\n        fmt.Println(\"Y: \", results[0].Value(), \"Z: \", results[1].Value())\n        fmt.Println(\"Y == A\", Y == A) // ==> true\n        fmt.Println(\"Z == A\", Z == A) // ==> false\n}\n```\n\nthat produces\n\n```\nY:  [[200] [-200]] Z:  [[200] [-200]]\nY == A true\nZ == A false\n```\n\nThe list of the available methods is available on GoDoc: http://godoc.org/github.com/galeone/tfgo\n\n## Computer Vision using data flow graph\n\nTensorFlow is rich of methods for performing operations on images. tfgo provides the `image` package that allows using the Go bindings to perform computer vision tasks in an elegant way.\n\nFor instance, it's possible to read an image, compute its directional derivative along the horizontal and vertical directions, compute the gradient and save it.\n\nThe code below does that, showing the different results achieved using correlation and convolution operations.\n\n```go\npackage main\n\nimport (\n        tg \"github.com/galeone/tfgo\"\n        \"github.com/galeone/tfgo/image\"\n        \"github.com/galeone/tfgo/image/filter\"\n        \"github.com/galeone/tfgo/image/padding\"\n        tf \"github.com/galeone/tensorflow/tensorflow/go\"\n        \"os\"\n)\n\nfunc main() {\n        root := tg.NewRoot()\n        grayImg := image.Read(root, \"/home/pgaleone/airplane.png\", 1)\n        grayImg = grayImg.Scale(0, 255)\n\n        // Edge detection using sobel filter: convolution\n        Gx := grayImg.Clone().Convolve(filter.SobelX(root), image.Stride{X: 1, Y: 1}, padding.SAME)\n        Gy := grayImg.Clone().Convolve(filter.SobelY(root), image.Stride{X: 1, Y: 1}, padding.SAME)\n        convoluteEdges := image.NewImage(root.SubScope(\"edge\"), Gx.Square().Add(Gy.Square().Value()).Sqrt().Value()).EncodeJPEG()\n\n        Gx = grayImg.Clone().Correlate(filter.SobelX(root), image.Stride{X: 1, Y: 1}, padding.SAME)\n        Gy = grayImg.Clone().Correlate(filter.SobelY(root), image.Stride{X: 1, Y: 1}, padding.SAME)\n        correlateEdges := image.NewImage(root.SubScope(\"edge\"), Gx.Square().Add(Gy.Square().Value()).Sqrt().Value()).EncodeJPEG()\n\n        results := tg.Exec(root, []tf.Output{convoluteEdges, correlateEdges}, nil, &tf.SessionOptions{})\n\n        file, _ := os.Create(\"convolved.png\")\n        file.WriteString(results[0].Value().(string))\n        file.Close()\n\n        file, _ = os.Create(\"correlated.png\")\n        file.WriteString(results[1].Value().(string))\n        file.Close()\n}\n\n```\n\n**airplane.png**\n\n![airplane](https://i.imgur.com/QS6shgc.jpg)\n\n**convolved.png**\n\n![convolved](https://i.imgur.com/zVndo9B.jpg)\n\n**correlated.png**\n\n![correlated](https://i.imgur.com/vhYF7o3.jpg)\n\nthe list of the available methods is available on GoDoc: http://godoc.org/github.com/galeone/tfgo/image\n\n## Train in Python, Serve in Go\n\nTensorFlow 2 comes with a lot of easy way to export a computational graph (e.g. Keras model, or a function decorated with `@tf.function`) to the `SavedModel` serialization format (that's the only one officially supported).\n\n![saved model](.readme/saved_model.png)\n\nUsing TensorFlow 2 (with Keras or tf.function) + tfgo, exporting a trained model (or a generic computational graph) and use it in Go is straightforward.\n\nJust dig into the example to understand how to serve a trained model with `tfgo`.\n\n### Python code\n\n```python\nimport tensorflow as tf\n\nmodel = tf.keras.Sequential(\n    [\n        tf.keras.layers.Conv2D(\n            8,\n            (3, 3),\n            strides=(2, 2),\n            padding=\"valid\",\n            input_shape=(28, 28, 1),\n            activation=tf.nn.relu,\n            name=\"inputs\",\n        ),  # 14x14x8\n        tf.keras.layers.Conv2D(\n            16, (3, 3), strides=(2, 2), padding=\"valid\", activation=tf.nn.relu\n        ),  # 7x716\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(10, name=\"logits\"),  # linear\n    ]\n)\n\ntf.saved_model.save(model, \"output/keras\")\n\n```\n\n### Go code\n\n```go\npackage main\n\nimport (\n        \"fmt\"\n        tg \"github.com/galeone/tfgo\"\n        tf \"github.com/galeone/tensorflow/tensorflow/go\"\n)\n\nfunc main() {\n        // A model exported with tf.saved_model.save()\n        // automatically comes with the \"serve\" tag because the SavedModel\n        // file format is designed for serving.\n        // This tag contains the various functions exported. Among these, there is\n        // always present the \"serving_default\" signature_def. This signature def\n        // works exactly like the TF 1.x graph. Get the input tensor and the output tensor,\n        // and use them as placeholder to feed and output to get, respectively.\n\n        // To get info inside a SavedModel the best tool is saved_model_cli\n        // that comes with the TensorFlow Python package.\n\n        // e.g. saved_model_cli show --all --dir output/keras\n        // gives, among the others, this info:\n\n        // signature_def['serving_default']:\n        // The given SavedModel SignatureDef contains the following input(s):\n        //   inputs['inputs_input'] tensor_info:\n        //       dtype: DT_FLOAT\n        //       shape: (-1, 28, 28, 1)\n        //       name: serving_default_inputs_input:0\n        // The given SavedModel SignatureDef contains the following output(s):\n        //   outputs['logits'] tensor_info:\n        //       dtype: DT_FLOAT\n        //       shape: (-1, 10)\n        //       name: StatefulPartitionedCall:0\n        // Method name is: tensorflow/serving/predict\n\n        model := tg.LoadModel(\"test_models/output/keras\", []string{\"serve\"}, nil)\n\n        fakeInput, _ := tf.NewTensor([1][28][28][1]float32{})\n        results := model.Exec([]tf.Output{\n                model.Op(\"StatefulPartitionedCall\", 0),\n        }, map[tf.Output]*tf.Tensor{\n                model.Op(\"serving_default_inputs_input\", 0): fakeInput,\n        })\n\n        predictions := results[0]\n        fmt.Println(predictions.Value())\n}\n```\n\n## Why?\n\nThinking about computation represented using graphs, describing computing in this way is, in one word, *challenging*.\n\nAlso, tfgo brings GPU computations to Go and allows writing parallel code without worrying about the device that executes it\n(just place the graph into the device you desire: that's it!)\n\n## Contribute\n\nI love contributions. Seriously. Having people that share your same interests and want to face your same challenges it's something awesome.\n\nIf you'd like to contribute, just dig in the code and see what can be added or improved. Start a discussion opening an issue and let's talk about it.\n\nJust follow the same design I use into the `image` package (\"override\" the same `Tensor` methods, document the methods, **test** your changes, ...)\n\nThere are **a lot** of packages that can be added, like the `image` package. Feel free to work on a brand new package: I'd love to see this kind of contributions!\n\n## TensorFlow installation\n\n### Manual\nOn MacOS you can `brew install libtensorflow` (assuming you have brew installed. Brew is a package manager. If you need help installing brew follow instructions here: https://docs.brew.sh/Installation )\n\nDownload and install the C library from https://www.tensorflow.org/install/lang_c\n\n```bash\ncurl -L \"https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.9.1.tar.gz\" | sudo tar -C /usr/local -xz\nsudo ldconfig\n```\n\n### Docker\n\n```bash\ndocker pull tensorflow/tensorflow:2.9.1\n```\n\nOr you can use system package manager.\n"
        },
        {
          "name": "README.zh.md",
          "type": "blob",
          "size": 9.5009765625,
          "content": "# tfgo：Go中的TensorFlow\n[![GoDoc](https://godoc.org/github.com/galeone/tfgo?status.svg)](https://godoc.org/github.com/galeone/tfgo)\n[![Build Status](https://travis-ci.org/galeone/tfgo.svg?branch=master)](https://travis-ci.org/galeone/tfgo)\n---\n\n- [tfgo：Go中的TensorFlow](#tfgo-Go中的TensorFlow)\n  - [依赖关系](#依赖关系)\n  - [安装](#安装)\n  - [入门指南](#入门指南)\n  - [使用数据流图的计算机视觉](#使用数据流图的计算机视觉)\n  - [在Python中训练，在Go中服务](#在Python中训练，在Go中服务)\n    - [Python代码](#Python代码)\n    - [Go代码](#Go代码)\n  - [为什么？](#为什么?)\n  - [贡献](#贡献)\n  - [TensorFlow安装](#TensorFlow安装)\n    - [手册](#手册)\n    - [Docker](#docker)\n\nTensorFlow的Go绑定 [很难使用](https://pgaleone.eu/tensorflow/go/2017/05/29/understanding-tensorflow-using-go/): tfgo让它变得简单！\n\n不再有以下问题：\n\n- 作用域：每个新节点都将有一个新的唯一名称\n- Typing：属性自动转换为支持的类型，而不是在运行时抛出错误\n\n此外，它使用 [方法链](https://en.wikipedia.org/wiki/Method_chaining) 让愉快地编写Go代码成为可能。\n\n## 依赖关系\n\n1. TensorFlow-2.9.1. [如何安装tensorflow](#tensorflow-installation)。\n2. TensorFlow绑定github.com/galeone/tensorflow。为了在Go中正确使用TensorFlow 2.9.1，我们必须使用我为Go绑定创建的分支。对于go mod代理来说，绑定可能太大，因此您可能想通过执行`go env -w GONOSUMDB=\"github.com/galeone/tensorflow\"`来关闭代理使用，以直接使用系统安装的git提取代码。它不会改变用户界面中的内容——您可以像往常一样使用go模块。\n\n## 安装\n\n```\ngo get github.com/galeone/tfgo\n```\n\n## 入门指南\n\nTensorFlow的Go绑定的核心数据结构是`op.Scope`结构。tfgo允许创建新的`*op.Scope`来解决上述范围界定问题。\n\n因为我们定义的是一个图，所以让我们从它的根（空图）开始\n\n```go\nroot := tg.NewRoot()\n```\n\n我们现在可以将节点放置到此图中并连接它们。假设我们想为列向量乘以矩阵，然后在结果中添加另一个列向量。\n\n这是完整的源代码。\n\n```go\npackage main\n\nimport (\n        \"fmt\"\n        tg \"github.com/galeone/tfgo\"\n        tf \"github.com/galeone/tensorflow/tensorflow/go\"\n)\n\nfunc main() {\n        root := tg.NewRoot()\n        A := tg.NewTensor(root, tg.Const(root, [2][2]int32{{1, 2}, {-1, -2}}))\n        x := tg.NewTensor(root, tg.Const(root, [2][1]int64{{10}, {100}}))\n        b := tg.NewTensor(root, tg.Const(root, [2][1]int32{{-10}, {10}}))\n        Y := A.MatMul(x.Output).Add(b.Output)\n        // 请注意，Y只是指向A的指针！\n\n        //如果我们想在图中创建不同的节点，我们必须克隆Y或等效的A\n        Z := A.Clone()\n        results := tg.Exec(root, []tf.Output{Y.Output, Z.Output}, nil, &tf.SessionOptions{})\n        fmt.Println(\"Y: \", results[0].Value(), \"Z: \", results[1].Value())\n        fmt.Println(\"Y == A\", Y == A) // ==> true\n        fmt.Println(\"Z == A\", Z == A) // ==> false\n}\n```\n\n产生\n\n```\nY:  [[200] [-200]] Z:  [[200] [-200]]\nY == A true\nZ == A false\n```\n\n可用方法列表可在GoDoc上找到: http://godoc.org/github.com/galeone/tfgo\n\n## 使用数据流图的计算机视觉\n\nTensorFlow有很多方法可以对图像执行操作。tfgo提供了`image`包，允许使用Go绑定以优雅的方式执行计算机视觉任务。\n\n例如，可以读取图像，计算其沿水平和垂直方向的方向导数，计算梯度并保存。\n\n下面的代码做到了这一点，显示了使用相关性和卷积操作实现的不同结果。\n\n```go\npackage main\n\nimport (\n        tg \"github.com/galeone/tfgo\"\n        \"github.com/galeone/tfgo/image\"\n        \"github.com/galeone/tfgo/image/filter\"\n        \"github.com/galeone/tfgo/image/padding\"\n        tf \"github.com/galeone/tensorflow/tensorflow/go\"\n        \"os\"\n)\n\nfunc main() {\n        root := tg.NewRoot()\n        grayImg := image.Read(root, \"/home/pgaleone/airplane.png\", 1)\n        grayImg = grayImg.Scale(0, 255)\n\n        // 使用sobel滤波器的边缘检测：卷积\n        Gx := grayImg.Clone().Convolve(filter.SobelX(root), image.Stride{X: 1, Y: 1}, padding.SAME)\n        Gy := grayImg.Clone().Convolve(filter.SobelY(root), image.Stride{X: 1, Y: 1}, padding.SAME)\n        convoluteEdges := image.NewImage(root.SubScope(\"edge\"), Gx.Square().Add(Gy.Square().Value()).Sqrt().Value()).EncodeJPEG()\n\n        Gx = grayImg.Clone().Correlate(filter.SobelX(root), image.Stride{X: 1, Y: 1}, padding.SAME)\n        Gy = grayImg.Clone().Correlate(filter.SobelY(root), image.Stride{X: 1, Y: 1}, padding.SAME)\n        correlateEdges := image.NewImage(root.SubScope(\"edge\"), Gx.Square().Add(Gy.Square().Value()).Sqrt().Value()).EncodeJPEG()\n\n        results := tg.Exec(root, []tf.Output{convoluteEdges, correlateEdges}, nil, &tf.SessionOptions{})\n\n        file, _ := os.Create(\"convolved.png\")\n        file.WriteString(results[0].Value().(string))\n        file.Close()\n\n        file, _ = os.Create(\"correlated.png\")\n        file.WriteString(results[1].Value().(string))\n        file.Close()\n}\n\n```\n\n**airplane.png**\n\n![airplane](https://i.imgur.com/QS6shgc.jpg)\n\n**convolved.png**\n\n![convolved](https://i.imgur.com/zVndo9B.jpg)\n\n**correlated.png**\n\n![correlated](https://i.imgur.com/vhYF7o3.jpg)\n\n可用方法列表可在GoDoc上找到: http://godoc.org/github.com/galeone/tfgo/image\n\n## 在Python中训练，在Go中服务\n\nTensorFlow 2提供了很多简单的方法，可以将计算图形（例如Keras模型或用`@tf.function`修饰的函数）导出为`SavedModel`序列化格式（这是唯一官方支持的格式）。\n\n![saved model](.readme/saved_model.png)\n\n使用TensorFlow 2（带有Keras或tf.function）+tfgo，导出一个经过训练的模型（或通用计算图）并在Go中使用它很简单。\n\n只需深入研究示例，了解如何使用`tfgo`为经过训练的模型服务。\n\n### Python代码\n\n```python\nimport tensorflow as tf\n\nmodel = tf.keras.Sequential(\n    [\n        tf.keras.layers.Conv2D(\n            8,\n            (3, 3),\n            strides=(2, 2),\n            padding=\"valid\",\n            input_shape=(28, 28, 1),\n            activation=tf.nn.relu,\n            name=\"inputs\",\n        ),  # 14x14x8\n        tf.keras.layers.Conv2D(\n            16, (3, 3), strides=(2, 2), padding=\"valid\", activation=tf.nn.relu\n        ),  # 7x716\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(10, name=\"logits\"),  # linear\n    ]\n)\n\ntf.saved_model.save(model, \"output/keras\")\n\n```\n\n### Go代码\n\n```go\npackage main\n\nimport (\n        \"fmt\"\n        tg \"github.com/galeone/tfgo\"\n        tf \"github.com/galeone/tensorflow/tensorflow/go\"\n)\n\nfunc main() {\n        // A model exported with tf.saved_model.save()\n        // automatically comes with the \"serve\" tag because the SavedModel\n        // file format is designed for serving.\n        // This tag contains the various functions exported. Among these, there is\n        // always present the \"serving_default\" signature_def. This signature def\n        // works exactly like the TF 1.x graph. Get the input tensor and the output tensor,\n        // and use them as placeholder to feed and output to get, respectively.\n\n        // To get info inside a SavedModel the best tool is saved_model_cli\n        // that comes with the TensorFlow Python package.\n\n        // e.g. saved_model_cli show --all --dir output/keras\n        // gives, among the others, this info:\n\n        // signature_def['serving_default']:\n        // The given SavedModel SignatureDef contains the following input(s):\n        //   inputs['inputs_input'] tensor_info:\n        //       dtype: DT_FLOAT\n        //       shape: (-1, 28, 28, 1)\n        //       name: serving_default_inputs_input:0\n        // The given SavedModel SignatureDef contains the following output(s):\n        //   outputs['logits'] tensor_info:\n        //       dtype: DT_FLOAT\n        //       shape: (-1, 10)\n        //       name: StatefulPartitionedCall:0\n        // Method name is: tensorflow/serving/predict\n\n        model := tg.LoadModel(\"test_models/output/keras\", []string{\"serve\"}, nil)\n\n        fakeInput, _ := tf.NewTensor([1][28][28][1]float32{})\n        results := model.Exec([]tf.Output{\n                model.Op(\"StatefulPartitionedCall\", 0),\n        }, map[tf.Output]*tf.Tensor{\n                model.Op(\"serving_default_inputs_input\", 0): fakeInput,\n        })\n\n        predictions := results[0]\n        fmt.Println(predictions.Value())\n}\n```\n\n## 为什么?\n\n考虑到用图形表示的计算，用这种方式描述计算是很有挑战性的。\n\n此外，tfgo将GPU计算引入Go，并允许编写并行代码，而无需担心执行它的设备（只需将图形放入您想要的设备中：就是这样！）\n\n## 贡献\n\n我喜欢贡献。认真地拥有与你志趣相投、想面对同样挑战的人，真是太棒了。\n\n如果您想贡献，只需深入研究代码，看看可以添加或改进什么。开始讨论一个问题，让我们讨论一下。\n\n只需遵循我在`image`包中使用的相同设计（“覆盖”相同的`Tensor`方法、记录方法、测试更改……）\n\n有很多包可以添加，比如`image`包。请随意编写一个全新的包：我很想看到这种贡献！\n\n## TensorFlow安装\n\n### 手册\n\n从 https://www.tensorflow.org/install/lang_c 下载并安装C库 \n\n```bash\ncurl -L \"https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.9.1.tar.gz\" | sudo tar -C /usr/local -xz\nsudo ldconfig\n```\n\n### Docker\n\n```bash\ndocker pull tensorflow/tensorflow:2.9.1\n```\n\n或者您可以使用系统包管理器。\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 0.1796875,
          "content": "module github.com/galeone/tfgo\n\ngo 1.19\n\nrequire github.com/galeone/tensorflow/tensorflow/go v0.0.0-20221023090153-6b7fa0680c3e\n\nrequire google.golang.org/protobuf v1.28.1 // indirect\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 1.400390625,
          "content": "github.com/galeone/tensorflow/tensorflow/go v0.0.0-20220620094824-6bb01e3a58fa h1:UsqNDZ0Olkhk+9wFchvc1JCQn7Bq6e694uQ+hGBlJcM=\ngithub.com/galeone/tensorflow/tensorflow/go v0.0.0-20220620094824-6bb01e3a58fa/go.mod h1:nHvVZJgJuQ0V2Xe4BqhTeCKQSMWDRI/gDkN8UxAANtU=\ngithub.com/galeone/tensorflow/tensorflow/go v0.0.0-20221023090153-6b7fa0680c3e h1:9+2AEFZymTi25FIIcDwuzcOPH04z9+fV6XeLiGORPDI=\ngithub.com/galeone/tensorflow/tensorflow/go v0.0.0-20221023090153-6b7fa0680c3e/go.mod h1:TelZuq26kz2jysARBwOrTv16629hyUsHmIoj54QqyFo=\ngithub.com/golang/protobuf v1.5.0/go.mod h1:FsONVRAS9T7sI+LIUmWTfcYkHO4aIWwzhcaSAoJOfIk=\ngithub.com/google/go-cmp v0.5.5 h1:Khx7svrCpmxxtHBq5j2mp/xVjsi8hQMfNLvJFAlrGgU=\ngithub.com/google/go-cmp v0.5.5/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngolang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543 h1:E7g+9GITq07hpfrRu66IVDexMakfv52eLZ2CXBWiKr4=\ngolang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngoogle.golang.org/protobuf v1.26.0-rc.1/go.mod h1:jlhhOSvTdKEhbULTjvd4ARK9grFBp09yW+WbY/TyQbw=\ngoogle.golang.org/protobuf v1.28.0 h1:w43yiav+6bVFTBQFZX0r7ipe9JQ1QsbMgHwbBziscLw=\ngoogle.golang.org/protobuf v1.28.0/go.mod h1:HV8QOd/L58Z+nl8r43ehVNZIU/HEI6OcFqwMG9pJV4I=\ngoogle.golang.org/protobuf v1.28.1 h1:d0NfwRgPtno5B1Wa6L2DAG+KivqkdutMf1UhdNx175w=\ngoogle.golang.org/protobuf v1.28.1/go.mod h1:HV8QOd/L58Z+nl8r43ehVNZIU/HEI6OcFqwMG9pJV4I=\n"
        },
        {
          "name": "image",
          "type": "tree",
          "content": null
        },
        {
          "name": "model.go",
          "type": "blob",
          "size": 2.9970703125,
          "content": "/*\nCopyright 2017-2022 Paolo Galeone. All right reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n    http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\n\npackage tfgo\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"runtime\"\n\n\ttf \"github.com/galeone/tensorflow/tensorflow/go\"\n)\n\n// Model represents a trained model\ntype Model struct {\n\tsaved *tf.SavedModel\n}\n\n// LoadModel creates a new *Model, loading it from the exportDir.\n// The graph loaded is identified by the set of tags specified when exporting it.\n// This operation creates a session with specified `options`\n// Panics if the model can't be loaded\nfunc LoadModel(exportDir string, tags []string, options *tf.SessionOptions) (model *Model) {\n\tvar err error\n\tmodel = new(Model)\n\tmodel.saved, err = tf.LoadSavedModel(exportDir, tags, options)\n\n\tif err != nil {\n\t\tpanic(err.Error())\n\t}\n\n\truntime.SetFinalizer(model, func(model *Model) {\n\t\tif model.saved != nil && model.saved.Session != nil {\n\t\t\t_ = model.saved.Session.Close()\n\t\t}\n\t})\n\treturn\n}\n\n// ImportModel creates a new *Model, loading the graph from the serialized representation.\n// This operation creates a session with specified `options`\n// Panics if the model can't be loaded\nfunc ImportModel(serializedModel, prefix string, options *tf.SessionOptions) (model *Model) {\n\tmodel = new(Model)\n\tcontents, err := os.ReadFile(serializedModel)\n\tif err != nil {\n\t\tpanic(err.Error())\n\t}\n\n\tgraph := tf.NewGraph()\n\tif err := graph.Import(contents, prefix); err != nil {\n\t\tpanic(err.Error())\n\t}\n\n\tsession, err := tf.NewSession(graph, options)\n\tif err != nil {\n\t\tpanic(err.Error())\n\t}\n\n\tmodel.saved = &tf.SavedModel{Session: session, Graph: graph}\n\truntime.SetFinalizer(model, func(model *Model) {\n\t\tif model.saved != nil && model.saved.Session != nil {\n\t\t\t_ = model.saved.Session.Close()\n\t\t}\n\t})\n\treturn\n}\n\n// Exec executes the nodes/tensors that must be present in the loaded model\n// feedDict values to feed to placeholders (that must have been saved in the model definition)\n// panics on error\nfunc (model *Model) Exec(tensors []tf.Output, feedDict map[tf.Output]*tf.Tensor) (results []*tf.Tensor) {\n\tvar err error\n\tif results, err = model.saved.Session.Run(feedDict, tensors, nil); err == nil {\n\t\treturn results\n\t}\n\tpanic(err)\n}\n\n// Op extracts the output in position idx of the tensor with the specified name from the model graph\nfunc (model *Model) Op(name string, idx int) tf.Output {\n\top := model.saved.Graph.Operation(name)\n\tif op == nil {\n\t\tpanic(fmt.Errorf(\"op %s not found\", name))\n\t}\n\tnout := op.NumOutputs()\n\tif nout <= idx {\n\t\tpanic(fmt.Errorf(\"op %s has %d outputs. Requested output number %d\", name, nout, idx))\n\t}\n\treturn op.Output(idx)\n}\n"
        },
        {
          "name": "ops.go",
          "type": "blob",
          "size": 3.5810546875,
          "content": "/*\nCopyright 2017-2022 Paolo Galeone. All right reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n    http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\n\n// Package tfgo simplifies the usage of the Tensorflow's go bindings\n// wrapping the most common methods as methods of new and logically separated\n// objects. These objects handle the naming issues (that could happen when\n// describing a tf.Graph) in a transparent way. Also, additional features are added.\n// Why this package is required is explained in this blog post:\n// https://pgaleone.eu/tensorflow/go/2017/05/29/understanding-tensorflow-using-go/\npackage tfgo\n\nimport (\n\ttf \"github.com/galeone/tensorflow/tensorflow/go\"\n\t\"github.com/galeone/tensorflow/tensorflow/go/op\"\n)\n\n// Batchify creates a batch of tensors, concatenating them along the first dimension\nfunc Batchify(scope *op.Scope, tensors []tf.Output) tf.Output {\n\ts := scope.SubScope(\"batchify\")\n\t// Batchify a single value, means add batch dimension and return\n\tif len(tensors) == 1 {\n\t\treturn op.ExpandDims(s.SubScope(\"ExpandDims\"), tensors[0], op.Const(s.SubScope(\"axis\"), []int32{0}))\n\t}\n\tvar tensors4d []tf.Output\n\tfor _, tensor := range tensors {\n\t\ttensors4d = append(tensors4d, op.ExpandDims(s.SubScope(\"ExpandDims\"), tensor, op.Const(s.SubScope(\"axis\"), []int32{0})))\n\t}\n\treturn op.ConcatV2(s.SubScope(\"ConcatV2\"), tensors4d, op.Const(s.SubScope(\"axis\"), int32(0)))\n}\n\n// Cast casts value to the specified dtype\nfunc Cast(scope *op.Scope, value tf.Output, dtype tf.DataType) tf.Output {\n\tif value.DataType() == dtype {\n\t\treturn value\n\t}\n\treturn op.Cast(scope.SubScope(\"Cast\"), value, dtype)\n}\n\n// NewRoot creates a new *op.Scope, empty\nfunc NewRoot() *op.Scope {\n\treturn op.NewScope()\n}\n\n// Const creates a constant value within the specified scope\nfunc Const(scope *op.Scope, value interface{}) tf.Output {\n\treturn op.Const(scope.SubScope(\"Const\"), value)\n}\n\n// IsClose defines the isclose operation between a and b.\n// Returns a conditional node that is true when a is close to b.\n// relTol is the relative tolerance\n// absTol is the absolute tolerance\nfunc IsClose(scope *op.Scope, a, b tf.Output, relTol, absTol tf.Output) tf.Output {\n\ts := scope.SubScope(\"IsClose\")\n\treturn op.LessEqual(s.SubScope(\"LessEqual\"),\n\t\top.Abs(s.SubScope(\"Abs\"),\n\t\t\top.Sub(s.SubScope(\"Sub\"), a, b)),\n\t\top.Maximum(s.SubScope(\"Maximum\"),\n\t\t\top.Mul(s.SubScope(\"Mul\"), relTol,\n\t\t\t\top.Maximum(s.SubScope(\"Maximum\"), op.Abs(s.SubScope(\"Abs\"), a), op.Abs(s.SubScope(\"Abs\"), b))), absTol))\n}\n\n// Exec creates the computation graph from the scope, then executes\n// the operations required to compute each element of tensors.\n// Node in the graph can be overwritten with feedDict.\n// The session options can be specified using the session parameter.\n// Returns the evaluated tensors. Panics on error.\nfunc Exec(scope *op.Scope, tensors []tf.Output, feedDict map[tf.Output]*tf.Tensor, options *tf.SessionOptions) []*tf.Tensor {\n\tgraph, err := scope.Finalize()\n\tif err != nil {\n\t\tpanic(err.Error())\n\t}\n\tvar sess *tf.Session\n\n\tsess, err = tf.NewSession(graph, options)\n\tif err == nil {\n\t\tdefer sess.Close()\n\t\tvar results []*tf.Tensor\n\t\tif results, err = sess.Run(feedDict, tensors, nil); err == nil {\n\t\t\treturn results\n\t\t}\n\t}\n\tpanic(err)\n}\n"
        },
        {
          "name": "scope.go",
          "type": "blob",
          "size": 1.0009765625,
          "content": "/*\nCopyright 2017-2022 Paolo Galeone. All right reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n    http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\n\npackage tfgo\n\nimport (\n\t\"fmt\"\n\t\"sync/atomic\"\n\n\t\"github.com/galeone/tensorflow/tensorflow/go/op\"\n)\n\nvar tensorCounter uint64\n\n// NewScope returns a unique scope in the format\n// input_<suffix> where suffix is a counter\n// This function isthread safe can be called in parallel for DIFFERENT scopes.\nfunc NewScope(root *op.Scope) *op.Scope {\n\tvar scope = atomic.AddUint64(&tensorCounter, 1)\n\treturn root.SubScope(fmt.Sprint(\"input_\", scope))\n}\n"
        },
        {
          "name": "tensor.go",
          "type": "blob",
          "size": 5.0263671875,
          "content": "/*\nCopyright 2017-2022 Paolo Galeone. All right reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n    http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\n\npackage tfgo\n\nimport (\n\ttf \"github.com/galeone/tensorflow/tensorflow/go\"\n\t\"github.com/galeone/tensorflow/tensorflow/go/op\"\n)\n\n// Tensor is an high level abstraction for the\n// tf.Output structure, associating a scope to the Tensor\ntype Tensor struct {\n\t// Root: Each tensor maintains a pointer to the graph root\n\tRoot *op.Scope\n\t// Path is the current Tensor full path\n\tPath *op.Scope\n\t// Output is the Tensor content\n\tOutput tf.Output\n}\n\n// NewTensor creates a *Tensor from a tf.Output\n// Place the cloned tensor within the specified scope\nfunc NewTensor(scope *op.Scope, tfout tf.Output) (tensor *Tensor) {\n\ttensor = new(Tensor)\n\ttensor.Root = scope\n\ttensor.Path = NewScope(scope)\n\t// Copy the tensor to a new node in the graph\n\ttensor.Output = op.Identity(tensor.Path.SubScope(\"Identity\"), tfout)\n\treturn tensor\n}\n\n// Check checks if the previous operation caused an error\n// and thus tensor.Path.Err is not nil.\n// If it's not, panics because we're defining the graph in a wrong way\nfunc (tensor *Tensor) Check() {\n\terr := tensor.Path.Err()\n\tif err != nil {\n\t\tpanic(err.Error())\n\t}\n}\n\n// Scope returns the scope associated to the tensor\nfunc (tensor *Tensor) Scope() *op.Scope {\n\treturn tensor.Path\n}\n\n// Shape64 returns the shape of the tensor as []int64.\n// If firstDimension is true a 4 elements slice is returned.\n// Otherwise a 3 elements slice is returned.\nfunc (tensor *Tensor) Shape64(firstDimension bool) []int64 {\n\tdims64, _ := tensor.Output.Shape().ToSlice()\n\tif firstDimension {\n\t\treturn dims64\n\t}\n\treturn dims64[1:]\n}\n\n// Shape32 returns the shape of the tensor as []int32.\n// If firstDimension is true a 4 elements slice is returned.\n// Otherwise a 3 elements slice is returned.\nfunc (tensor *Tensor) Shape32(firstDimension bool) []int32 {\n\tdims64 := tensor.Shape64(firstDimension)\n\tvar dims32 = make([]int32, len(dims64))\n\tfor idx, dim := range dims64 {\n\t\tdims32[idx] = int32(dim)\n\t}\n\treturn dims32\n}\n\n// Dtype returns the tensor dtype\nfunc (tensor *Tensor) Dtype() tf.DataType {\n\treturn tensor.Output.DataType()\n}\n\n// --------\n// Methods returning *Tensor\n// Structs that embed *Tensor should \"\"override\"\" them\n// Returning their own type\n// --------\n\n// Clone returns a copy of the current tensor in a new scope\n// Clone is used to create a different tensor\n// from the output of an operation.\n// The new node is placed at the same level of the current tensor\n// it can be seen as a twin tensor\nfunc (tensor *Tensor) Clone() *Tensor {\n\tdefer tensor.Check()\n\tscope := NewScope(tensor.Root)\n\treturn NewTensor(scope, tensor.Output)\n}\n\n// Cast casts the current tensor to the requested dtype\nfunc (tensor *Tensor) Cast(dtype tf.DataType) *Tensor {\n\tdefer tensor.Check()\n\ttensor.Output = Cast(tensor.Path, tensor.Output, dtype)\n\treturn tensor\n}\n\n// Add defines the add operation between the tensor and tfout\n// `tfout` dtype is converted to tensor.Dtype() before adding\nfunc (tensor *Tensor) Add(tfout tf.Output) *Tensor {\n\tdefer tensor.Check()\n\ts := tensor.Path.SubScope(\"Add\")\n\ttensor.Output = op.Add(s, tensor.Output, Cast(s, tfout, tensor.Dtype()))\n\treturn tensor\n}\n\n// Mul defines the multiplication operation between the tensor\n// and `tfout`. It's the multiplication element-wise with broadcasting support.\n// `tfout` dtype is converted to tensor.Dtype() before multiplying\nfunc (tensor *Tensor) Mul(tfout tf.Output) *Tensor {\n\tdefer tensor.Check()\n\ts := tensor.Path.SubScope(\"Mul\")\n\ttensor.Output = op.Mul(s, tensor.Output, Cast(s, tfout, tensor.Dtype()))\n\treturn tensor\n}\n\n// MatMul defines the matrix multiplication operation between the tensor\n// and `tfout`.\n// `tfout` dtype is converted to tensor.Dtype() before multiplying\nfunc (tensor *Tensor) MatMul(tfout tf.Output) *Tensor {\n\tdefer tensor.Check()\n\ts := tensor.Path.SubScope(\"MatMul\")\n\ttensor.Output = op.MatMul(s, tensor.Output, Cast(s, tfout, tensor.Dtype()))\n\treturn tensor\n}\n\n// Pow defines the pow operation x^y, where x are the tensor values\n// y dtype is converted to tensor.Dtype() before executing Pow\nfunc (tensor *Tensor) Pow(y tf.Output) *Tensor {\n\tdefer tensor.Check()\n\ts := tensor.Path.SubScope(\"Pow\")\n\ttensor.Output = op.Pow(s, tensor.Output, Cast(s, y, tensor.Dtype()))\n\treturn tensor\n}\n\n// Square defines the square operation for the tensor values\nfunc (tensor *Tensor) Square() *Tensor {\n\tdefer tensor.Check()\n\treturn tensor.Pow(op.Const(tensor.Path.SubScope(\"y\"), 2.))\n}\n\n// Sqrt defines the square root operation for the tensor values\nfunc (tensor *Tensor) Sqrt() *Tensor {\n\tdefer tensor.Check()\n\treturn tensor.Pow(op.Const(tensor.Path.SubScope(\"y\"), 0.5))\n}\n"
        },
        {
          "name": "test_models",
          "type": "tree",
          "content": null
        },
        {
          "name": "tfgo_test.go",
          "type": "blob",
          "size": 13.6728515625,
          "content": "/*\nCopyright 2017-2022 Paolo Galeone. All right reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n    http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\n\npackage tfgo_test\n\nimport (\n\t\"math\"\n\t\"reflect\"\n\t\"testing\"\n\n\ttf \"github.com/galeone/tensorflow/tensorflow/go\"\n\ttg \"github.com/galeone/tfgo\"\n)\n\nfunc TestNewScope(t *testing.T) {\n\troot := tg.NewRoot()\n\tscope := tg.NewScope(root)\n\tif scope == nil {\n\t\tt.Error(\"NewScope shouldn't return nil\")\n\t}\n}\n\nfunc TestTensor(t *testing.T) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tt.Errorf(\"The code panic, but it shouldn't: %v\", r)\n\t\t}\n\t}()\n\n\troot := tg.NewRoot()\n\ttensorA := tg.NewTensor(root, tg.Const(root, [3]int32{1, 2, 3}))\n\tif tensorA == nil {\n\t\tt.Fatal(\"NewTensor shouldn't return nil\")\n\t}\n\t// shouldn't panic\n\ttensorA.Check()\n\ttensorB := tg.NewTensor(root, tg.Const(root, [3]int32{1, 2, 3}))\n\n\t// For not changing the content of A\n\t// Create a new tensor with the same content of A\n\t// on every invocation.\n\t// Change the content on the fly is useful when used chaning the operations\n\n\tadd := tensorA.Clone().Add(tensorB.Output).Output\n\tmul := tensorA.Clone().Mul(tensorB.Output).Output\n\t// types must be always well defined\n\t// never use a number, e.i. 2, but force a type e.i. int32(2)\n\tpow := tensorA.Clone().Pow(tg.Const(root, int32(2))).Output\n\tsqrt := tensorA.Clone().Sqrt().Output\n\tsquare := tensorA.Clone().Square().Output\n\tshape32 := tensorA.Clone().Shape32(true)\n\tshape64 := tensorA.Clone().Shape64(true)\n\tif len(shape32) != len(shape64) {\n\t\tt.Errorf(\"Expected len(shape32) = len(shape64), but got: %v != %v\", len(shape32), len(shape64))\n\t}\n\t// remove first dim\n\tshape32 = tensorA.Clone().Shape32(false)\n\tshape64 = tensorA.Clone().Shape64(false)\n\tif len(shape32) != len(shape64) {\n\t\tt.Errorf(\"Expected len(shape32) = len(shape64), but got: %v != %v\", len(shape32), len(shape64))\n\t}\n\n\tmatA := tg.NewTensor(root, tg.Const(root, [2][2]int32{{1, 2}, {-1, -2}}))\n\tmatB := tg.NewTensor(root, tg.Const(root, [2][1]int32{{10}, {100}}))\n\t// chain op without clone, matA now is matmul result\n\tmatA = matA.MatMul(matB.Output)\n\n\tresult := tg.Exec(root, []tf.Output{add, mul, pow, sqrt, square, matA.Output}, nil, nil)\n\tif result[0].Value().([]int32)[0] != 2 {\n\t\tt.Errorf(\"Expected 2 as first value in sum, but got: %v\", result[0].Value().([]int32)[0])\n\t}\n\n\tif result[1].Value().([]int32)[0] != 1 {\n\t\tt.Errorf(\"Expected 1 as first value in mul, but got: %v\", result[1].Value().([]int32))\n\t}\n\tif result[2].Value().([]int32)[0] != 1 {\n\t\tt.Errorf(\"Expected 1 as first value in pow, but got: %v\", result[2].Value().([]int32)[0])\n\t}\n\n\tif result[3].Value().([]int32)[0] != 1 {\n\t\tt.Errorf(\"Expected 1 as first value in sqrt, but got: %v\", result[3].Value().([]int32)[0])\n\t}\n\tif result[4].Value().([]int32)[0] != result[2].Value().([]int32)[0] {\n\t\tt.Errorf(\"Expected output of square being equal to tensor² but got: %v vs %v\", result[4].Value().([]int32), result[2].Value().([]int32))\n\t}\n\n\tif result[5].Value().([][]int32)[0][0] != 210 {\n\t\tt.Errorf(\"Expected output of matmul in pos 0,0 should be 210, but got: %v\", result[5].Value().([][]int32))\n\t}\n}\n\nfunc TestTensorPanic(t *testing.T) {\n\tdefer func() {\n\t\tif r := recover(); r == nil {\n\t\t\tt.Errorf(\"The code did not panic\")\n\t\t}\n\t}()\n\n\troot := tg.NewRoot()\n\ttensorA := tg.NewTensor(root, tg.Const(root, [3]int32{1, 2, 3}))\n\tif tensorA == nil {\n\t\tt.Fatal(\"NewTensor shouldn't return nil\")\n\t}\n\t// shouldn't panic\n\ttensorA.Check()\n\ttensorB := tg.NewTensor(root, tg.Const(root, [3]int32{1, 2, 3}))\n\n\tadd := tensorA.Add(tensorB.Output)\n\tresult := tg.Exec(root, []tf.Output{add.Output}, nil, nil)\n\tif result[0].Value().([]int32)[0] != 2 {\n\t\tt.Errorf(\"Expected 2 as first value in sum, but got: %v\", result[0].Value().([]int32)[0])\n\t}\n\t// After the Exec operation, everything should panic because the graph has been finalized\n\t// and the graph, thus, has been built and it's unmodifiable\n\ttensorA = tensorA.Cast(tf.Float)\n\tif tensorA == nil {\n\t\tt.Error(\"Cast operation shouldn't return nil\")\n\t}\n\ttensorA.Check()\n\n}\n\nfunc TestBatchify(t *testing.T) {\n\troot := tg.NewRoot()\n\tvar tensors []tf.Output\n\tfor i := 0; i < 10; i++ {\n\t\ttensors = append(tensors, tg.Const(root, [3]int32{1, 2, 3}))\n\t}\n\tbatch := tg.Batchify(root, tensors)\n\n\tif batch.Shape().NumDimensions() != 2 {\n\t\tt.Errorf(\"Expected 2D tensor, but got: %dD tensor\", batch.Shape().NumDimensions())\n\t}\n\n\tshape, _ := batch.Shape().ToSlice()\n\tif shape[0] != 10 || shape[1] != 3 {\n\t\tt.Errorf(\"Expected shape (10,3), got  (%d,%d)\", shape[0], shape[1])\n\t}\n\n\tresult := tg.Exec(root, []tf.Output{batch}, nil, nil)\n\t// Note the cast to [][] and not to [10][3]\n\tmatrixResult := result[0].Value().([][]int32)\n\tvar expectedMatrix [][]int32\n\trow := []int32{1, 2, 3}\n\tfor i := 0; i < 10; i++ {\n\t\texpectedMatrix = append(expectedMatrix, row)\n\t}\n\tif !reflect.DeepEqual(matrixResult, expectedMatrix) {\n\t\tt.Errorf(\"Expected matrix %v\\n Got matrix %v\", expectedMatrix, matrixResult)\n\t}\n}\n\nfunc TestIsClose(t *testing.T) {\n\troot := tg.NewRoot()\n\tA := tg.Const(root, []float32{0.1, 0.2, 0.3, 1e-1, 1e-2, 1e-3, 1e-4, 1e-6, 5e-5})\n\tB := tg.Const(root, []float32{0.11, 0.2, 0.299, 0, 2e-2, 2e-3, 2e-4, 0, 10})\n\trelTol := tg.Const(root, float32(1e-3))\n\tabsTol := tg.Const(root, float32(1e-6))\n\tisClose := tg.IsClose(root, A, B, relTol, absTol)\n\n\texpected := []bool{false, true, false, false, false, false, false, true, false}\n\tresults := tg.Exec(root, []tf.Output{isClose}, nil, nil)\n\tresult := results[0].Value().([]bool)\n\tif !reflect.DeepEqual(result, expected) {\n\t\tt.Errorf(\"Expected  %v\\n Got  %v\", expected, result)\n\t}\n}\n\nfunc TestPanicModelRestore(t *testing.T) {\n\tdefer func() {\n\t\tif r := recover(); r == nil {\n\t\t\tt.Errorf(\"The code did not panic\")\n\t\t}\n\t}()\n\t// Panics because the tag does not exist\n\ttg.LoadModel(\"test_models/keras\", []string{\"tagwat\"}, nil)\n}\n\nfunc TestPanicModelWhenOpNotExists(t *testing.T) {\n\tdefer func() {\n\t\tif r := recover(); r == nil {\n\t\t\tt.Errorf(\"The code did not panic\")\n\t\t}\n\t}()\n\tmodel := tg.LoadModel(\"test_models/keras\", []string{\"tag\"}, nil)\n\tmodel.Op(\"does not exists\", 0)\n}\n\nfunc TestPanicModelWhenOpOutputNotExists(t *testing.T) {\n\tdefer func() {\n\t\tif r := recover(); r == nil {\n\t\t\tt.Errorf(\"The code did not panic\")\n\t\t}\n\t}()\n\tmodel := tg.LoadModel(\"test_models/keras\", []string{\"tag\"}, nil)\n\t// Esists but wroing output number (1 instead of 0)\n\tmodel.Op(\"LeNetDropout/softmax_linear/Identity\", 1)\n}\n\nfunc TestLoadModel(t *testing.T) {\n\t// A model exported with tf.saved_model.save()\n\t// automatically comes with the \"serve\" tag because the SavedModel\n\t// file format is designed for serving.\n\t// This tag contains the various functions exported. Among these, there is\n\t// always present the \"serving_default\" signature_def. This signature def\n\t// works exactly like the TF 1.x graph. Get the input tensor and the output tensor,\n\t// and use them as placeholder to feed and output to get, respectively.\n\n\t// To get info inside a SavedModel the best tool is saved_model_cli\n\t// that comes with the TensorFlow Python package.\n\n\t// e.g. saved_model_cli show --all --dir output/keras\n\t// gives, among the others, this info:\n\n\t// signature_def['serving_default']:\n\t// The given SavedModel SignatureDef contains the following input(s):\n\t//   inputs['inputs_input'] tensor_info:\n\t//       dtype: DT_FLOAT\n\t//       shape: (-1, 28, 28, 1)\n\t//       name: serving_default_inputs_input:0\n\t// The given SavedModel SignatureDef contains the following output(s):\n\t//   outputs['logits'] tensor_info:\n\t//       dtype: DT_FLOAT\n\t//       shape: (-1, 10)\n\t//       name: StatefulPartitionedCall:0\n\t// Method name is: tensorflow/serving/predict\n\n\tmodel := tg.LoadModel(\"test_models/output/keras\", []string{\"serve\"}, nil)\n\n\tfakeInput, _ := tf.NewTensor([1][28][28][1]float32{})\n\tresults := model.Exec([]tf.Output{\n\t\tmodel.Op(\"StatefulPartitionedCall\", 0),\n\t}, map[tf.Output]*tf.Tensor{\n\t\tmodel.Op(\"serving_default_inputs_input\", 0): fakeInput,\n\t})\n\n\tif results[0].Shape()[0] != 1 || results[0].Shape()[1] != 10 {\n\t\tt.Errorf(\"Expected output shape of [1,10], got %v\", results[0].Shape())\n\t}\n}\n\nfunc TestPanicImportModelReadFile(t *testing.T) {\n\tdefer func() {\n\t\tif r := recover(); r == nil {\n\t\t\tt.Errorf(\"The code did not panic\")\n\t\t}\n\t}()\n\t// Panics because the model file does not exists\n\ttg.ImportModel(\"test_models/export/fake\", \"\", nil)\n}\n\nfunc TestPanicModelExec(t *testing.T) {\n\tdefer func() {\n\t\tif r := recover(); r == nil {\n\t\t\tt.Errorf(\"The code did not panic\")\n\t\t}\n\t}()\n\tmodel := tg.LoadModel(\"test_models/export\", []string{\"tag\"}, nil)\n\t// fake input with meaningless type should make the model crash\n\tfakeInput, _ := tf.NewTensor([1][28][28][1]string{})\n\tmodel.Exec([]tf.Output{\n\t\tmodel.Op(\"LeNetDropout/softmax_linear/Identity\", 0),\n\t}, map[tf.Output]*tf.Tensor{\n\t\tmodel.Op(\"input_\", 0): fakeInput,\n\t})\n}\n\nfunc TestIsIntegerFloat(t *testing.T) {\n\troot := tg.NewRoot()\n\tA := tg.Const(root, int64(0))\n\tB := tg.Const(root, []float32{0.11})\n\n\tif tg.IsInteger(B.DataType()) {\n\t\tt.Error(\"Expected a float type, but integer found\")\n\t}\n\tif !tg.IsInteger(A.DataType()) {\n\t\tt.Error(\"A supposed to be integer, but IsInteger said no\")\n\t}\n\n\tif tg.IsFloat(A.DataType()) {\n\t\tt.Error(\"A is integer, but IsFloat returned true\")\n\t}\n\n\tif !tg.IsFloat(B.DataType()) {\n\t\tt.Error(\"Expected a float type, but float32 has been considered not float\")\n\t}\n}\n\nfunc TestMinValue(t *testing.T) {\n\troot := tg.NewRoot()\n\tA := tg.Const(root, int64(0))\n\tB := tg.Const(root, float64(0))\n\n\tif tg.MinValue(A.DataType()) != math.MinInt64 {\n\t\tt.Errorf(\"expected MinValue of dype int64 to be equal to math.MinInt64, but got %v\", tg.MinValue(A.DataType()))\n\t}\n\n\tif tg.MinValue(B.DataType()) != math.SmallestNonzeroFloat64 {\n\t\tt.Errorf(\"expected MinValue of dype float64 to be equal to math.SmallestNonzeroFloat64 but got %v\", tg.MinValue(B.DataType()))\n\t}\n\n\tA = tg.Cast(root, A, tf.Int32)\n\n\tif tg.MinValue(A.DataType()) != math.MinInt32 {\n\t\tt.Errorf(\"expected MinValue of dype int32 to be equal to math.MinInt32, but got %v\", tg.MinValue(A.DataType()))\n\t}\n\n\tA = tg.Cast(root, A, tf.Int16)\n\n\tif tg.MinValue(A.DataType()) != math.MinInt16 {\n\t\tt.Errorf(\"expected MinValue of dype int16 to be equal to math.MinInt16, but got %v\", tg.MinValue(A.DataType()))\n\t}\n\n\tA = tg.Cast(root, A, tf.Int8)\n\n\tif tg.MinValue(A.DataType()) != math.MinInt8 {\n\t\tt.Errorf(\"expected MinValue of dype int8 to be equal to math.MinInt8, but got %v\", tg.MinValue(A.DataType()))\n\t}\n\n\tA = tg.Cast(root, A, tf.Uint8)\n\n\tif tg.MinValue(A.DataType()) != 0 {\n\t\tt.Errorf(\"expected MinValue of dype uint8 to be equal to 0, but got %v\", tg.MinValue(A.DataType()))\n\t}\n\n\tA = tg.Cast(root, A, tf.Uint16)\n\n\tif tg.MinValue(A.DataType()) != 0 {\n\t\tt.Errorf(\"expected MinValue of dype uint16 to be equal to 0, but got %v\", tg.MinValue(A.DataType()))\n\t}\n\n\tB = tg.Cast(root, B, tf.Float)\n\n\tif tg.MinValue(B.DataType()) != math.SmallestNonzeroFloat32 {\n\t\tt.Errorf(\"expected MinValue of dype float32 to be equal to math.SmallestNonzeroFloat32 but got %v\", tg.MinValue(B.DataType()))\n\t}\n\n\tB = tg.Cast(root, B, tf.Half)\n\n\tif tg.MinValue(B.DataType()) != 6.10*math.Pow10(-5) {\n\t\tt.Errorf(\"expected MinValue of dype float32 to be equal to 6.1*10^{-5} but got %v\", tg.MinValue(B.DataType()))\n\t}\n}\n\nfunc TestMaxValuePanic(t *testing.T) {\n\tdefer func() {\n\t\t// Panic on max on unsupported dtype\n\t\tif r := recover(); r == nil {\n\t\t\tt.Errorf(\"The code did not panic\")\n\t\t}\n\t}()\n\troot := tg.NewRoot()\n\ts := tg.Const(root, \"test\")\n\ttg.MaxValue(s.DataType())\n}\n\nfunc TestMinValuePanic(t *testing.T) {\n\tdefer func() {\n\t\t// Panic on max on unsupported dtype\n\t\tif r := recover(); r == nil {\n\t\t\tt.Errorf(\"The code did not panic\")\n\t\t}\n\t}()\n\troot := tg.NewRoot()\n\ts := tg.Const(root, \"test\")\n\ttg.MinValue(s.DataType())\n}\n\nfunc TestMaxValue(t *testing.T) {\n\troot := tg.NewRoot()\n\tA := tg.Const(root, int64(0))\n\tB := tg.Const(root, float64(0))\n\n\tif tg.MaxValue(A.DataType()) != math.MaxInt64 {\n\t\tt.Errorf(\"expected MaxValue of dype int64 to be equal to math.MaxInt64, but got %v\", tg.MaxValue(A.DataType()))\n\t}\n\n\tif tg.MaxValue(B.DataType()) != math.MaxFloat64 {\n\t\tt.Errorf(\"expected MaxValue of dype float64 to be equal to math.MaxFloat64 but got %v\", tg.MaxValue(B.DataType()))\n\t}\n\n\tA = tg.Cast(root, A, tf.Int32)\n\n\tif tg.MaxValue(A.DataType()) != math.MaxInt32 {\n\t\tt.Errorf(\"expected MaxValue of dype int32 to be equal to math.MaxInt32, but got %v\", tg.MaxValue(A.DataType()))\n\t}\n\n\tA = tg.Cast(root, A, tf.Int16)\n\n\tif tg.MaxValue(A.DataType()) != math.MaxInt16 {\n\t\tt.Errorf(\"expected MaxValue of dype int16 to be equal to math.MaxInt16, but got %v\", tg.MaxValue(A.DataType()))\n\t}\n\n\tA = tg.Cast(root, A, tf.Int8)\n\n\tif tg.MaxValue(A.DataType()) != math.MaxInt8 {\n\t\tt.Errorf(\"expected MaxValue of dype int8 to be equal to math.MaxInt8, but got %v\", tg.MaxValue(A.DataType()))\n\t}\n\n\tA = tg.Cast(root, A, tf.Uint8)\n\n\tif tg.MaxValue(A.DataType()) != math.MaxUint8 {\n\t\tt.Errorf(\"expected MaxValue of dype uint8 to be equal to math.MaxUint8, but got %v\", tg.MaxValue(A.DataType()))\n\t}\n\n\tA = tg.Cast(root, A, tf.Uint16)\n\n\tif tg.MaxValue(A.DataType()) != math.MaxUint16 {\n\t\tt.Errorf(\"expected MaxValue of dype uint16 to be equal to math.MaxUint16, but got %v\", tg.MaxValue(A.DataType()))\n\t}\n\n\tB = tg.Cast(root, B, tf.Float)\n\tif tg.MaxValue(B.DataType()) != math.MaxFloat32 {\n\t\tt.Errorf(\"expected MaxValue of dype float32 to be equal to math.MaxFloat32 but got %v\", tg.MaxValue(B.DataType()))\n\t}\n\n\tB = tg.Cast(root, B, tf.Half)\n\tif tg.MaxValue(B.DataType()) != math.MaxFloat32/math.Pow(2, 16) {\n\t\tt.Errorf(\"expected MaxValue of dype float32 to be equal to math.MaxFloat32  / math.Pow(2, 16) but got %v\", tg.MaxValue(B.DataType()))\n\t}\n}\n"
        },
        {
          "name": "types.go",
          "type": "blob",
          "size": 2.3408203125,
          "content": "/*\nCopyright 2017-2022 Paolo Galeone. All right reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n    http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\n\npackage tfgo\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\n\ttf \"github.com/galeone/tensorflow/tensorflow/go\"\n)\n\n// IsInteger returns true if dtype is a tensorflow integer type\nfunc IsInteger(dtype tf.DataType) bool {\n\tswitch dtype {\n\tcase tf.Int8, tf.Int16, tf.Int32, tf.Int64, tf.Uint8, tf.Uint16, tf.Qint8, tf.Qint16, tf.Qint32, tf.Quint8, tf.Quint16:\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// IsFloat returns true if dtype is a tensorfow float type\nfunc IsFloat(dtype tf.DataType) bool {\n\tswitch dtype {\n\tcase tf.Double, tf.Float, tf.Half:\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// MaxValue returns the maximum value accepted for the dtype\nfunc MaxValue(dtype tf.DataType) float64 {\n\tswitch dtype {\n\tcase tf.Double:\n\t\treturn math.MaxFloat64\n\tcase tf.Float:\n\t\treturn math.MaxFloat32\n\tcase tf.Half:\n\t\treturn math.MaxFloat32 / math.Pow(2, 16)\n\tcase tf.Int16:\n\t\treturn math.MaxInt16\n\tcase tf.Int32:\n\t\treturn math.MaxInt32\n\tcase tf.Int64:\n\t\treturn math.MaxInt64\n\tcase tf.Int8:\n\t\treturn math.MaxInt8\n\tcase tf.Uint16:\n\t\treturn math.MaxUint16\n\tcase tf.Uint8:\n\t\treturn math.MaxUint8\n\t\t// No support for Quantized types\n\t}\n\tpanic(fmt.Sprintf(\"dtype %d not supported\", dtype))\n}\n\n// MinValue returns the minimum representable value for the specified dtype\nfunc MinValue(dtype tf.DataType) float64 {\n\tswitch dtype {\n\tcase tf.Double:\n\t\treturn math.SmallestNonzeroFloat64\n\tcase tf.Float:\n\t\treturn math.SmallestNonzeroFloat32\n\tcase tf.Half:\n\t\t// According to: https://www.khronos.org/opengl/wiki/Small_Float_Formats\n\t\treturn 6.10 * math.Pow10(-5)\n\tcase tf.Int16:\n\t\treturn math.MinInt16\n\tcase tf.Int32:\n\t\treturn math.MinInt32\n\tcase tf.Int64:\n\t\treturn math.MinInt64\n\tcase tf.Int8:\n\t\treturn math.MinInt8\n\tcase tf.Uint16, tf.Uint8:\n\t\treturn 0\n\t\t// No support for Quantized types\n\t}\n\tpanic(fmt.Sprintf(\"dtype %d not supported\", dtype))\n}\n"
        }
      ]
    }
  ]
}