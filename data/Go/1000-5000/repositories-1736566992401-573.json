{
  "metadata": {
    "timestamp": 1736566992401,
    "page": 573,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "NVIDIA/k8s-device-plugin",
      "stars": 2933,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".common-ci.yml",
          "type": "blob",
          "size": 6.673828125,
          "content": "# Copyright (c) 2021-2022, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\ndefault:\n  image: docker:24.0.6\n  services:\n    - name: docker:24.0.6-dind\n      command: [\"--experimental\"]\n\nvariables:\n  GIT_SUBMODULE_STRATEGY: recursive\n  BUILD_MULTI_ARCH_IMAGES: \"true\"\n\nstages:\n  - trigger\n  - package-build\n  - image-build\n  - test\n  - scan\n  - release\n  - sign\n\n.pipeline-trigger-rules:\n  rules:\n    # We trigger the pipeline if started manually\n    - if: $CI_PIPELINE_SOURCE == \"web\"\n    # We trigger the pipeline on the main branch\n    - if: $CI_COMMIT_BRANCH == \"main\"\n    # We trigger the pipeline on the release- branches\n    - if: $CI_COMMIT_BRANCH =~ /^release-.*$/\n    # We trigger the pipeline on tags\n    - if: $CI_COMMIT_TAG && $CI_COMMIT_TAG != \"\"\n\nworkflow:\n  rules:\n    # We trigger the pipeline on a merge request\n    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'\n    # We then add all the regular triggers\n    - !reference [.pipeline-trigger-rules, rules]\n\n# The main or manual job is used to filter out distributions or architectures that are not required on\n# every build.\n.main-or-manual:\n  rules:\n    - !reference [.pipeline-trigger-rules, rules]\n    - if: $CI_PIPELINE_SOURCE == \"schedule\"\n      when: manual\n\n# The trigger-pipeline job adds a manualy triggered job to the pipeline on merge requests.\ntrigger-pipeline:\n  stage: trigger\n  script:\n    - echo \"starting pipeline\"\n  rules:\n    - !reference [.main-or-manual, rules]\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n      when: manual\n      allow_failure: false\n    - when: always\n\n# The .dist- dummy steps set the DIST variable for the targeted distribution.\n.dist-ubi9:\n  variables:\n    DIST: \"ubi9\"\n\n# Define the platform targets\n.platform-amd64:\n  variables:\n    PLATFORM: linux/amd64\n\n.platform-arm64:\n  variables:\n    PLATFORM: linux/arm64\n\n# Make buildx available as a docker CLI plugin\n.buildx-setup:\n  before_script:\n    -  export BUILDX_VERSION=v0.6.3\n    -  apk add --no-cache curl\n    -  mkdir -p ~/.docker/cli-plugins\n    -  curl -sSLo ~/.docker/cli-plugins/docker-buildx \"https://github.com/docker/buildx/releases/download/${BUILDX_VERSION}/buildx-${BUILDX_VERSION}.linux-amd64\"\n    -  chmod a+x ~/.docker/cli-plugins/docker-buildx\n\n    -  docker buildx create --use --platform=linux/amd64,linux/arm64\n\n    -  '[[ -n \"${SKIP_QEMU_SETUP}\" ]] || docker run --rm --privileged multiarch/qemu-user-static --reset -p yes'\n\n# The .scan step forms the base of the image scan operation performed before releasing\n# images. We implement a .scan-base job here to allow for the variable checks to be\n# exercised before internal releases.\n.scan-base:\n  stage: scan\n  variables:\n    IMAGE: \"${CI_REGISTRY_IMAGE}/k8s-device-plugin:${CI_COMMIT_SHORT_SHA}-${DIST}\"\n    IMAGE_ARCHIVE: \"k8s-device-plugin.tar\"\n  except:\n    variables:\n      - $SKIP_SCANS && $SKIP_SCANS == \"yes\"\n  before_script:\n    - docker login -u \"${CI_REGISTRY_USER}\" -p \"${CI_REGISTRY_PASSWORD}\" \"${CI_REGISTRY}\"\n    - docker pull --platform=\"${PLATFORM}\" \"${IMAGE}\"\n    - docker save \"${IMAGE}\" -o \"${IMAGE_ARCHIVE}\"\n  script:\n    - >\n      echo \"Skipping scan of ${IMAGE}\"\n\n.scan:\n  extends:\n    - .scan-base\n\n# Define the scan targets\nscan-ubi9-amd64:\n  extends:\n    - .scan\n    - .dist-ubi9\n    - .platform-amd64\n  needs:\n    - image-ubi9\n\nscan-ubi9-arm64:\n  extends:\n    - .scan\n    - .dist-ubi9\n    - .platform-arm64\n  needs:\n    - image-ubi9\n    - scan-ubi9-amd64\n\n# Download the regctl binary for use in the release steps\n.regctl-setup:\n  before_script:\n    - export REGCTL_VERSION=v0.3.10\n    - apk add --no-cache curl\n    - mkdir -p bin\n    - curl -sSLo bin/regctl https://github.com/regclient/regclient/releases/download/${REGCTL_VERSION}/regctl-linux-amd64\n    - chmod a+x bin/regctl\n    - export PATH=$(pwd)/bin:${PATH}\n\n# .release forms the base of the deployment jobs which push images to the CI registry.\n# This is extended with the version to be deployed (e.g. the SHA or TAG) and the\n# target os.\n.release:\n  stage: release\n  variables:\n    # Define the source image for the release\n    IMAGE_NAME: \"${CI_REGISTRY_IMAGE}/k8s-device-plugin\"\n    VERSION: \"${CI_COMMIT_SHORT_SHA}\"\n    # OUT_IMAGE_VERSION is overridden for external releases\n    OUT_IMAGE_VERSION: \"${CI_COMMIT_SHORT_SHA}\"\n  before_script:\n    - !reference [.regctl-setup, before_script]\n\n    # We ensure that the OUT_IMAGE_VERSION is set\n    - 'echo Version: ${OUT_IMAGE_VERSION} ; [[ -n \"${OUT_IMAGE_VERSION}\" ]] || exit 1'\n\n    # In the case where we are deploying a different version to the CI_COMMIT_SHA, we\n    # need to tag the image.\n    # Note: a leading 'v' is stripped from the version if present\n    - apk add --no-cache make bash\n  script:\n    # Log in to the \"output\" registry, tag the image and push the image\n    - 'echo \"Logging in to CI registry ${CI_REGISTRY}\"'\n    - regctl registry login \"${CI_REGISTRY}\" -u \"${CI_REGISTRY_USER}\" -p \"${CI_REGISTRY_PASSWORD}\"\n    - '[ ${CI_REGISTRY} = ${OUT_REGISTRY} ] || echo \"Logging in to output registry ${OUT_REGISTRY}\"'\n    - '[ ${CI_REGISTRY} = ${OUT_REGISTRY} ] || regctl registry login \"${OUT_REGISTRY}\" -u \"${OUT_REGISTRY_USER}\" -p \"${OUT_REGISTRY_TOKEN}\"'\n\n    # Since OUT_IMAGE_NAME and OUT_IMAGE_VERSION are set, this will push the CI image to the\n    # Target\n    - make -f deployments/container/Makefile push-${DIST}\n\n# Define a staging release step that pushes an image to an internal \"staging\" repository\n# This is triggered for all pipelines (i.e. not only tags) to test the pipeline steps\n# outside of the release process.\n.release:staging:\n  extends:\n    - .release\n  variables:\n    OUT_REGISTRY_USER: \"${CI_REGISTRY_USER}\"\n    OUT_REGISTRY_TOKEN: \"${CI_REGISTRY_PASSWORD}\"\n    OUT_REGISTRY: \"${CI_REGISTRY}\"\n    OUT_IMAGE_NAME: \"${CI_REGISTRY_IMAGE}/staging/k8s-device-plugin\"\n\n# Define an external release step that pushes an image to an external repository.\n# This includes a devlopment image off main.\n.release:external:\n  extends:\n    - .release\n  rules:\n    - if: $CI_COMMIT_TAG\n      variables:\n        OUT_IMAGE_VERSION: \"${CI_COMMIT_TAG}\"\n    - if: $CI_COMMIT_BRANCH == $RELEASE_DEVEL_BRANCH\n      variables:\n        OUT_IMAGE_VERSION: \"${DEVEL_RELEASE_IMAGE_VERSION}\"\n\nrelease:staging-ubi9:\n  extends:\n    - .release:staging\n    - .dist-ubi9\n  needs:\n    - image-ubi9\n"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.046875,
          "content": "/releases\nDockerfile\n.git\n/nvidia-device-plugin\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1875,
          "content": "/nvidia-device-plugin\n/config-manager\n/gpu-feature-discovery\n/mps-control-daemon\n/releases\ndeployments/helm/gpu-feature-discovery\ncmd/gpu-feature-discovery/gfd-test-loop\ne2e_logs\n\n*.out\n*.log\n"
        },
        {
          "name": ".golangci.yml",
          "type": "blob",
          "size": 0.7490234375,
          "content": "run:\n  timeout: 10m\n\nlinters:\n  enable:\n    - contextcheck\n    - errcheck\n    - gocritic\n    - gofmt\n    - goimports\n    - gosec\n    - gosimple\n    - govet\n    - ineffassign\n    - misspell\n    - staticcheck\n    - unconvert\n  disable: []\n\nlinters-settings:\n  goimports:\n    local-prefixes: github.com/NVIDIA/k8s-device-plugin\n\nissues:\n  exclude:\n  # A conversion of a uint8 to an int cannot overflow.\n  - \"G115: integer overflow conversion uint8 -> int\"\n  exclude-rules:\n  # We use math/rand instead of crypto/rand for unique names in e2e tests.\n  - path: tests/e2e/\n    linters:\n    - gosec\n    text: \"G404\"\n  # We create world-readable files in tests.\n  - path: \".*_test.go\"\n    linters:\n    - gosec\n    text: \"G306: Expect WriteFile permissions to be 0600 or less\"\n"
        },
        {
          "name": ".nspect-allowlist.toml",
          "type": "blob",
          "size": 0.4912109375,
          "content": "version = \"1.0.0\"\n\n[[pulse-trufflehog.files]]\n\nfile = \"vendor/k8s.io/kubernetes/test/e2e/common/node/runtime.go\"\n\n  # When adding the credential type and specific masked/redacted secret, add them as exactly seen in the result (if you are running local Pulse-Trufflehog then for NVIDIARaptor detector type you need to use \"ExtraData > Key Type\" as the credential type)\n  [[pulse-trufflehog.files.secrets]]\n  type = \"GCP\"\n  values = [ \"image-pulling@authenticated-image-pulling.iam.gserviceaccount.com\" ]\n"
        },
        {
          "name": ".nvidia-ci.yml",
          "type": "blob",
          "size": 5.650390625,
          "content": "# Copyright (c) 2021-2022, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\ninclude:\n  - local: '.common-ci.yml'\n\ndefault:\n  tags:\n    - cnt\n    - container-dev\n    - docker/multi-arch\n    - docker/privileged\n    - os/linux\n    - type/docker\n\nvariables:\n  DOCKER_DRIVER: overlay2\n  DOCKER_TLS_CERTDIR: \"/certs\"\n  # Release \"devel\"-tagged images off the main branch\n  RELEASE_DEVEL_BRANCH: \"main\"\n  DEVEL_RELEASE_IMAGE_VERSION: \"devel\"\n  # On the multi-arch builder we don't need the qemu setup.\n  SKIP_QEMU_SETUP: \"1\"\n  # Define the public staging registry\n  STAGING_REGISTRY: ghcr.io/nvidia\n  STAGING_VERSION: ${CI_COMMIT_SHORT_SHA}\n\n.image-pull:\n  stage: image-build\n  variables:\n    IN_REGISTRY: \"${STAGING_REGISTRY}\"\n    IN_IMAGE_NAME: k8s-device-plugin\n    IN_VERSION: \"${STAGING_VERSION}\"\n    OUT_REGISTRY_USER: \"${CI_REGISTRY_USER}\"\n    OUT_REGISTRY_TOKEN: \"${CI_REGISTRY_PASSWORD}\"\n    OUT_REGISTRY: \"${CI_REGISTRY}\"\n    OUT_IMAGE_NAME: \"${CI_REGISTRY_IMAGE}/k8s-device-plugin\"\n    PUSH_MULTIPLE_TAGS: \"false\"\n  # We delay the job start to allow the public pipeline to generate the required images.\n  when: delayed\n  start_in: 30 minutes\n  timeout: 30 minutes\n  retry:\n    max: 2\n    when:\n      - job_execution_timeout\n      - stuck_or_timeout_failure\n  before_script:\n    - !reference [.regctl-setup, before_script]\n    - apk add --no-cache make bash\n    - >\n      regctl manifest get ${IN_REGISTRY}/${IN_IMAGE_NAME}:${IN_VERSION}-${DIST} --list > /dev/null && echo \"${IN_REGISTRY}/${IN_IMAGE_NAME}:${IN_VERSION}-${DIST}\" || ( echo \"${IN_REGISTRY}/${IN_IMAGE_NAME}:${IN_VERSION}-${DIST} does not exist\" && sleep infinity )\n  script:\n    - regctl registry login \"${OUT_REGISTRY}\" -u \"${OUT_REGISTRY_USER}\" -p \"${OUT_REGISTRY_TOKEN}\"\n    - make -f deployments/container/Makefile IMAGE=${IN_REGISTRY}/${IN_IMAGE_NAME}:${IN_VERSION}-${DIST} OUT_IMAGE=${OUT_IMAGE_NAME}:${CI_COMMIT_SHORT_SHA}-${DIST} push-${DIST}\n\nimage-ubi9:\n  extends:\n    - .image-pull\n    - .dist-ubi9\n\n# We skip the integration tests for the internal CI:\n.integration:\n  stage: test\n  before_script:\n    - echo \"Skipped in internal CI\"\n  script:\n    - echo \"Skipped in internal CI\"\n\n# The .scan step forms the base of the image scan operation performed before releasing\n# images.\n.scan:\n  extends:\n    - .scan-base\n  stage: scan\n  image: \"${PULSE_IMAGE}\"\n  script:\n    - AuthHeader=$(echo -n $SSA_CLIENT_ID:$SSA_CLIENT_SECRET | base64 -w0)\n    - >\n      export SSA_TOKEN=$(curl --request POST --header \"Authorization: Basic $AuthHeader\" --header \"Content-Type: application/x-www-form-urlencoded\" ${SSA_ISSUER_URL} | jq \".access_token\" |  tr -d '\"')\n    - if [ -z \"$SSA_TOKEN\" ]; then exit 1; else echo \"SSA_TOKEN set!\"; fi\n    - pulse-cli -n $NSPECT_ID --ssa $SSA_TOKEN scan -i $IMAGE_ARCHIVE -p $CONTAINER_POLICY -o\n  artifacts:\n    when: always\n    expire_in: 1 week\n    paths:\n      - pulse-cli.logs\n      - licenses.json\n      - sbom.json\n      - vulns.json\n      - policy_evaluation.json\n\n# Define external release helpers\n.release:ngc:\n  extends:\n    - .release:external\n  variables:\n    OUT_REGISTRY_USER: \"${NGC_REGISTRY_USER}\"\n    OUT_REGISTRY_TOKEN: \"${NGC_REGISTRY_TOKEN}\"\n    OUT_REGISTRY: \"${NGC_REGISTRY}\"\n    OUT_IMAGE_NAME: \"${NGC_REGISTRY_IMAGE}\"\n\n# Define the external release targets\n# Release to NGC\nrelease:ngc-ubi9:\n  extends:\n    - .release:ngc\n    - .dist-ubi9\n\n# Define the external image signing steps for NGC\n# Download the ngc cli binary for use in the sign steps\n.ngccli-setup:\n  before_script:\n    - apt-get update && apt-get install -y curl unzip jq\n    - |\n      if [ -z \"${NGCCLI_VERSION}\" ]; then\n        NGC_VERSION_URL=\"https://api.ngc.nvidia.com/v2/resources/nvidia/ngc-apps/ngc_cli/versions\"\n        # Extract the latest version from the JSON data using jq\n        export NGCCLI_VERSION=$(curl -s $NGC_VERSION_URL | jq -r '.recipe.latestVersionIdStr')\n      fi\n      echo \"NGCCLI_VERSION ${NGCCLI_VERSION}\"\n    - curl -sSLo ngccli_linux.zip https://api.ngc.nvidia.com/v2/resources/nvidia/ngc-apps/ngc_cli/versions/${NGCCLI_VERSION}/files/ngccli_linux.zip\n    - unzip ngccli_linux.zip\n    - chmod u+x ngc-cli/ngc\n\n# .sign forms the base of the deployment jobs which signs images in the CI registry.\n# This is extended with the image name and version to be deployed.\n.sign:ngc:\n  image: ubuntu:latest\n  stage: sign\n  rules:\n    - if: $CI_COMMIT_TAG\n  variables:\n    NGC_CLI_API_KEY: \"${NGC_REGISTRY_TOKEN}\"\n    IMAGE_NAME: \"${NGC_REGISTRY_IMAGE}\"\n    IMAGE_TAG: \"${CI_COMMIT_TAG}-${DIST}\"\n  retry:\n    max: 2\n  before_script:\n    - !reference [.ngccli-setup, before_script]\n    # We ensure that the IMAGE_NAME and IMAGE_TAG is set\n    - 'echo Image Name: ${IMAGE_NAME} && [[ -n \"${IMAGE_NAME}\" ]] || exit 1'\n    - 'echo Image Tag: ${IMAGE_TAG} && [[ -n \"${IMAGE_TAG}\" ]] || exit 1'\n  script:\n    - 'echo \"Signing the image ${IMAGE_NAME}:${IMAGE_TAG}\"'\n    - ngc-cli/ngc registry image publish --source ${IMAGE_NAME}:${IMAGE_TAG} ${IMAGE_NAME}:${IMAGE_TAG} --public --discoverable --allow-guest --sign --org nvidia\n\nsign:ngc-short-tag:\n  extends:\n    - .sign:ngc\n  needs:\n    - release:ngc-ubi9\n  variables:\n    IMAGE_TAG: \"${CI_COMMIT_TAG}\"\n\nsign:ngc-ubi9:\n  extends:\n    - .dist-ubi9\n    - .sign:ngc\n  needs:\n    - release:ngc-ubi9\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 15.5908203125,
          "content": "## Changelog\n\n### v0.17.0\n- Promote v0.17.0-rc.1 to GA\n\n### v0.17.0-rc.1\n- Add CAP_SYS_ADMIN if volume-mounts list strategy is included\n- Remove unneeded DEVICE_PLUGIN_MODE envvar\n- Fix applying SELinux label for MPS\n- Use a base image that aligns with the ubi-minimal base image\n- Switch to a ubi9-based base image\n- Remove namespace field from cluster-scoped resources\n- Generate labels for IMEX cligue and domain\n- Add optional injection of the default IMEX channel\n- Allow kubelet-socket to be specified as command line argument\n\n### v0.16.2\n- Add CAP_SYS_ADMIN if volume-mounts list strategy is included (fixes #856)\n- Remove unneeded DEVICE_PLUGIN_MODE envvar\n- Fix applying SELinux label for MPS\n\n### v0.16.1\n- Bump nvidia-container-toolkit to v1.16.1 to fix a bug with CDI spec generation for MIG devices\n\n### v0.16.0\n- Fixed logic of atomic writing of the feature file\n- Replaced `WithDialer` with `WithContextDialer`\n- Fixed SELinux context of MPS pipe directory.\n- Changed behavior for empty MIG devices to issue a warning instead of an error when the mixed strategy is selected\n- Added a a GFD node label for the GPU mode.\n- Update CUDA base image version to 12.5.1\n\n### v0.16.0-rc.1\n- Skip container updates if only CDI is selected\n- Allow cdi hook path to be set\n- Add nvidiaDevRoot config option\n- Detect devRoot for driver installation\n- Changed the automatically created MPS /dev/shm to half of the total memory as obtained from /proc/meminfo\n- Remove redundant version log\n- Remove provenance information from image manifests\n- add ngc image signing job for auto signing\n- fix: target should be binaries\n- Allow device discovery strategy to be specified\n- Refactor cdi handler construction\n- Add addMigMonitorDevices field to nvidia-device-plugin.options helper\n- Fix allPossibleMigStrategiesAreNone helm chart helper\n- use the helm quote function to wrap boolean values in quotes\n- Fix usage of hasConfigMap\n- Make info, nvml, and device lib construction explicit\n- Clean up construction of WSL devices\n- Remove unused function\n- Don't require node-name to be set if not needed\n- Make vgpu failures non-fatal\n- Use HasTegraFiles over IsTegraSystem\n- Raise error for MPS when using MIG\n- Align container driver root envvars\n- Update github.com/NVIDIA/go-nvml to v0.12.0-6\n- Add unit tests cases for sanitise func\n- Improving logic to sanitize GFD generated node labels\n- Add newline to pod logs\n- Adding vfio manager\n- Add prepare-release.sh script\n- Don't require node-name to be set if not needed\n- Remove GitLab pipeline .gitlab.yml\n- E2E test: fix object names\n- strip parentheses from the gpu product name\n- E2E test: instanciate a logger for helm outputs\n- E2E test: enhance logging via ginkgo/gomega\n- E2E test: remove e2elogs helper pkg\n- E2E test: Create HelmClient during Framework init\n- E2E test: Add -ginkgo.v flag to increase verbosity\n- E2E test: Create DiagnosticsCollector\n- Update vendoring\n- Replace go-nvlib/pkg/nvml with go-nvml/pkg/nvml\n- Add dependabot updates for release-0.15\n\n### Version v0.15.0\n- Moved `nvidia-device-plugin.yml` static deployment at the root of the repository to `deployments/static/nvidia-device-plugin.yml`.\n- Simplify PCI device clases in NFD worker configuration.\n- Update CUDA base image version to 12.4.1.\n- Switch to Ubuntu22.04-based CUDA image for default image.\n- Add new CUDA driver and runtime version labels to align with other NFD version labels.\n- Update NFD dependency to v0.15.3.\n\n### Version v0.15.0-rc.2\n- Bump CUDA base image version to 12.3.2\n- Add `cdi-cri` device list strategy. This uses the CDIDevices CRI field to request CDI devices instead of annotations.\n- Set MPS memory limit by device index and not device UUID. This is a workaround for an issue where\n  these limits are not applied for devices if set by UUID.\n- Update MPS sharing to disallow requests for multiple devices if MPS sharing is configured.\n- Set mps device memory limit by index.\n- Explicitly set sharing.mps.failRequestsGreaterThanOne = true.\n- Run tail -f for each MPS daemon to output logs.\n- Enforce replica limits for MPS sharing.\n\n### Version v0.15.0-rc.1\n- Import GPU Feature Discovery into the GPU Device Plugin repo. This means that\n  the same version and container image is used for both components.\n- Add tooling to create a kind cluster for local development and testing.\n- Update `go-gpuallocator` dependency to migrate away from the deprecated `gpu-monitoring-tools` NVML bindings.\n- Remove `legacyDaemonsetAPI` config option. This was only required for k8s versions < 1.16.\n- Add support for MPS sharing.\n- Bump CUDA base image version to 12.3.1\n\n### Version v0.14.5\n\n- Fix bug in CDI spec generation on systems with `lib -> usr/lib` symlinks.\n- Bump CUDA base image version to 12.3.2.\n\n\n### Version v0.14.4\n\n- Update to refactored go-gpuallocator code. This permanently fixes the NVML_NVLINK_MAX_LINKS value addressed in a\n  hotfix in v0.14.3. This also addresses a bug due to uninitialized NVML when calling go-gpuallocator.\n\n### Version v0.14.3\n\n- Patch vendored code for new NVML_NVLINK_MAX_LINKS value\n- Bumped CUDA base images version to 12.3.0\n\n### Version v0.14.2\n\n- Update GFD subchart to v0.8.2\n- Bumped CUDA base images version to 12.2.2\n\n### Version v0.14.1\n\n- Fix parsing of `deviceListStrategy` in config file to correctly support strings as well as slices.\n- Update GFD subchart to v0.8.1\n- Bumped CUDA base images version to 12.2.0\n\n\n### Version v0.14.0\n\n- Promote v0.14.0-rc.3 to v0.14.0\n- Bumped `nvidia-container-toolkit` dependency to latest version for newer CDI spec generation code\n\n### Version v0.14.0-rc.3\n\n- Removed `--cdi-enabled` config option and instead trigger CDI injection based on `cdi-annotation` strategy.\n- Bumped `go-nvlib` dependency to latest version for support of new MIG profiles.\n- Added `cdi-annotation-prefix` config option to control how CDI annotations are generated.\n- Renamed `driver-root-ctr-path` config option added in `v0.14.0-rc.1` to `container-driver-root`.\n- Updated GFD subchart to version 0.8.0-rc.2\n\n### Version v0.14.0-rc.2\n\n- Fix bug from v0.14.0-rc.1 when using cdi-enabled=false\n\n### Version v0.14.0-rc.1\n\n- Added --cdi-enabled flag to GPU Device Plugin. With this enabled, the device plugin will generate CDI specifications for available NVIDIA devices. Allocation will add CDI anntiations (`cdi.k8s.io/*`) to the response. These are read by a CDI-enabled runtime to make the required modifications to a container being created.\n- Updated GFD subchart to version 0.8.0-rc.1\n- Bumped Golang version to 1.20.1\n- Bumped CUDA base images version to 12.1.0\n- Switched to klog for logging\n- Added a static deployment file for Microshift\n\n### Version v0.13.0\n\n- Promote v0.13.0-rc.3 to v0.13.0\n- Fail on startup if no valid resources are detected\n- Ensure that display adapters are skipped when enumerating devices\n- Bump GFD subchart to version 0.7.0\n\n### Version v0.13.0-rc.3\n\n- Use `nodeAffinity` instead of `nodeSelector` by default in daemonsets\n- Mount `/sys` instead of `/sys/class/dmi/id/product_name` in GPU Feature Discovery daemonset\n- Bump GFD subchart to version 0.7.0-rc.3\n\n### Version v0.13.0-rc.2\n\n- Bump cuda base image to 11.8.0\n- Use consistent indentation in YAML manifests\n- Fix bug from v0.13.0-rc.1 when using mig-strategy=\"mixed\"\n- Add logged error message if setting up health checks fails\n- Support MIG devices with 1g.10gb+me profile\n- Distribute replicas evenly across GPUs during allocation\n- Bump GFD subchart to version 0.7.0-rc.2\n\n### Version v0.13.0-rc.1\n\n- Improve health checks to detect errors when waiting on device events\n- Log ECC error events detected during health check\n- Add the GIT sha to version information for the CLI and container images\n- Use NVML interfaces from go-nvlib to query devices\n- Refactor plugin creation from resources\n- Add a CUDA-based resource manager that can be used to expose integrated devices on Tegra-based systems\n- Bump GFD subchart to version 0.7.0-rc.1\n\n### Version v0.12.3\n\n- Bump cuda base image to 11.7.1\n- Remove CUDA compat libs from the device-plugin image in favor of libs installed by the driver\n- Fix securityContext.capabilities indentation\n- Add namespace override for multi-namespace deployments\n\n### Version v0.12.2\n\n- Add an 'empty' config fallback (but don't apply it by default)\n- Make config fallbacks for config-manager a configurable, ordered list\n- Allow an empty config file and default to \"version: v1\"\n- Bump GFD subchart to version 0.6.1\n- Move NFD servicAccount info under 'master' in helm chart\n- Make priorityClassName configurable through helm\n- Fix assertions for panicking on uniformity with migStrategy=single\n- Fix example configmap settings in values.yaml file\n\n### Version v0.12.1\n\n- Exit the plugin and GFD sidecar containers on error instead of logging and continuing\n- Only force restart of daemonsets when using config file and allow overrides\n- Fix bug in calculation for GFD security context in helm chart\n\n### Version v0.12.0\n\n- Promote v0.12.0-rc.6 to v0.12.0\n- Update README.md with all of the v0.12.0 features\n\n### Version v0.12.0-rc.6\n\n- Send SIGHUP from GFD sidecar to GFD main container on config change\n- Reuse main container's securityContext in sidecar containers\n- Update GFD subchart to v0.6.0-rc.1\n- Bump CUDA base image version to 11.7.0\n- Add a flag called FailRequestsGreaterThanOne for TimeSlicing resources\n\n### Version v0.12.0-rc.5\n\n- Allow either an external ConfigMap name or a set of configs in helm\n- Handle cases where no default config is specified to config-manager\n- Update API used to pass config files to helm to use map instead of list\n- Fix bug that wasn't properly stopping plugins across a soft restart\n\n### Version v0.12.0-rc.4\n\n- Make GFD and NFD (optional) subcharts of the device plugin's helm chart\n- Add new config-manager binary to run as sidecar and update the plugin's configuration via a node label\n- Add support to helm to provide multiple config files for the config map\n- Refactor main to allow configs to be reloaded across a (soft) restart\n- Add field for `TimeSlicing.RenameByDefault` to rename all replicated resources to `<resource-name>.shared`\n- Disable support for resource-renaming in the config (will no longer be part of this release)\n\n### Version v0.12.0-rc.3\n\n- Add ability to parse Duration fields from config file\n- Omit either the Plugin or GFD flags from the config when not present\n- Fix bug when falling back to none strategy from single strategy\n\n### Version v0.12.0-rc.2\n\n- Move MigStrategy from Sharing.Mig.Strategy back to Flags.MigStrategy\n- Remove timeSlicing.strategy and any allocation policies built around it\n- Add support for specifying a config file to the helm chart\n\n### Version v0.12.0-rc.1\n\n- Add API for specifying time-slicing parameters to support GPU sharing\n- Add API for specifying explicit resource naming in the config file\n- Update config file to be used across plugin and GFD\n- Stop publishing images to dockerhub (now only published to nvcr.io)\n- Add NVIDIA_MIG_MONITOR_DEVICES=all to daemonset envvars when mig mode is enabled\n- Print the plugin configuration at startup\n- Add the ability to load the plugin configuration from a file\n- Remove deprecated tolerations for critical-pod\n- Drop critical-pod annotation(removed from 1.16+) in favor of priorityClassName\n- Pass all parameters as env in helm chart and example daemonset.yamls files for consistency\n\n### Version v0.11.0\n\n- Update CUDA base image version to 11.6.0\n- Add support for multi-arch images\n\n### Version v0.10.0\n\n- Update CUDA base images to 11.4.2\n- Ignore Xid=13 (Graphics Engine Exception) critical errors in device health-check\n- Ignore Xid=68 (Video processor exception) critical errors in device health-check\n- Build multi-arch container images for linux/amd64 and linux/arm64\n- Use Ubuntu 20.04 for Ubuntu-based container images\n- Remove Centos7 images\n\n### Version v0.9.0\n\n- Fix bug when using CPUManager and the device plugin MIG mode not set to \"none\"\n- Allow passing list of GPUs by device index instead of uuid\n- Move to urfave/cli to build the CLI\n- Support setting command line flags via environment variables\n\n### Version v0.8.2\n\n- Update all dockerhub references to nvcr.io\n\n### Version v0.8.1\n\n- Fix permission error when using NewDevice instead of NewDeviceLite when constructing MIG device map\n\n### Version v0.8.0\n\n- Raise an error if a device has migEnabled=true but has no MIG devices\n- Allow mig.strategy=single on nodes with non-MIG gpus\n\n### Version v0.7.3\n\n- Update vendoring to include bug fix for `nvmlEventSetWait_v2`\n\n### Version v0.7.2\n\n- Fix bug in dockfiles for ubi8 and centos using CMD not ENTRYPOINT\n\n### Version v0.7.1\n\n- Update all Dockerfiles to point to latest cuda-base on nvcr.io\n\n### Version v0.7.0\n\n- Promote v0.7.0-rc.8 to v0.7.0\n\n### Version v0.7.0-rc.8\n\n- Permit configuration of alternative container registry through environment variables.\n- Add an alternate set of gitlab-ci directives under .nvidia-ci.yml\n- Update all k8s dependencies to v1.19.1\n- Update vendoring for NVML Go bindings\n- Move restart loop to force recreate of plugins on SIGHUP\n\n### Version v0.7.0-rc.7\n\n- Fix bug which only allowed running the plugin on machines with CUDA 10.2+ installed\n\n### Version v0.7.0-rc.6\n\n- Add logic to skip / error out when unsupported MIG device encountered\n- Fix bug treating memory as multiple of 1000 instead of 1024\n- Switch to using CUDA base images\n- Add a set of standard tests to the .gitlab-ci.yml file\n\n### Version v0.7.0-rc.5\n\n- Add deviceListStrategyFlag to allow device list passing as volume mounts\n\n### Version v0.7.0-rc.4\n\n- Allow one to override selector.matchLabels in the helm chart\n- Allow one to override the udateStrategy in the helm chart\n\n### Version v0.7.0-rc.3\n\n- Fail the plugin if NVML cannot be loaded\n- Update logging to print to stderr on error\n- Add best effort removal of socket file before serving\n- Add logic to implement GetPreferredAllocation() call from kubelet\n\n### Version v0.7.0-rc.2\n\n- Add the ability to set 'resources' as part of a helm install\n- Add overrides for name and fullname in helm chart\n- Add ability to override image related parameters helm chart\n- Add conditional support for overriding secutiryContext in helm chart\n\n### Version v0.7.0-rc.1\n\n- Added `migStrategy` as a parameter to select the MIG strategy to the helm chart\n- Add support for MIG with different strategies {none, single, mixed}\n- Update vendored NVML bindings to latest (to include MIG APIs)\n- Add license in UBI image\n- Update UBI image with certification requirements\n\n### Version v0.6.0\n\n- Update CI, build system, and vendoring mechanism\n- Change versioning scheme to v0.x.x instead of v1.0.0-betax\n- Introduced helm charts as a mechanism to deploy the plugin\n\n### Version v0.5.0\n\n- Add a new plugin.yml variant that is compatible with the CPUManager\n- Change CMD in Dockerfile to ENTRYPOINT\n- Add flag to optionally return list of device nodes in Allocate() call\n- Refactor device plugin to eventually handle multiple resource types\n- Move plugin error retry to event loop so we can exit with a signal\n- Update all vendored dependencies to their latest versions\n- Fix bug that was inadvertently *always* disabling health checks\n- Update minimal driver version to 384.81\n\n### Version v0.4.0\n\n- Fixes a bug with a nil pointer dereference around `getDevices:CPUAffinity`\n\n### Version v0.3.0\n\n- Manifest is updated for Kubernetes 1.16+ (apps/v1)\n- Adds more logging information\n\n### Version v0.2.0\n\n- Adds the Topology field for Kubernetes 1.16+\n\n### Version v0.1.0\n\n- If gRPC throws an error, the device plugin no longer ends up in a non responsive state.\n\n### Version v0.0.0\n\n- Reversioned to SEMVER as device plugins aren't tied to a specific version of kubernetes anymore.\n\n### Version v1.11\n\n- No change.\n\n### Version v1.10\n\n- The device Plugin API is now v1beta1\n\n### Version v1.9\n\n- The device Plugin API changed and is no longer compatible with 1.8\n- Error messages were added"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 2.2099609375,
          "content": "# Contribute to the NVIDIA Kubernetes Device Plugin\n\nWant to hack on the NVIDIA Kubernetes Device plugin Project? Awesome!\nWe only require you to sign your work, the below section describes this!\n\n## Sign your work\n\nThe sign-off is a simple line at the end of the explanation for the patch. Your\nsignature certifies that you wrote the patch or otherwise have the right to pass\nit on as an open-source patch. The rules are pretty simple: if you can certify\nthe below (from [developercertificate.org](http://developercertificate.org/)):\n\n```\nDeveloper Certificate of Origin\nVersion 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n1 Letterman Drive\nSuite D4700\nSan Francisco, CA, 94129\n\nEveryone is permitted to copy and distribute verbatim copies of this\nlicense document, but changing it is not allowed.\n\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I\n    have the right to submit it under the open source license\n    indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best\n    of my knowledge, is covered under an appropriate open source\n    license and I have the right under that license to submit that\n    work with modifications, whether created in whole or in part\n    by me, under the same open source license (unless I am\n    permitted to submit under a different license), as indicated\n    in the file; or\n\n(c) The contribution was provided directly to me by some other\n    person who certified (a), (b) or (c) and I have not modified\n    it.\n\n(d) I understand and agree that this project and the contribution\n    are public and that a record of the contribution (including all\n    personal information I submit with it, including my sign-off) is\n    maintained indefinitely and may be redistributed consistent with\n    this project or the open source license(s) involved.\n```\n\nThen you just add a line to every git commit message:\n\n    Signed-off-by: Joe Smith <joe.smith@email.com>\n\nUse your real name (sorry, no pseudonyms or anonymous contributions.)\n\nIf you set your `user.name` and `user.email` git configs, you can sign your\ncommit automatically with `git commit -s`.\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.091796875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 4.4482421875,
          "content": "# Copyright (c) 2020-2022, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nDOCKER   ?= docker\nMKDIR    ?= mkdir\nDIST_DIR ?= $(CURDIR)/dist\n\ninclude $(CURDIR)/versions.mk\n\nifeq ($(IMAGE_NAME),)\nREGISTRY ?= nvidia\nIMAGE_NAME = $(REGISTRY)/k8s-device-plugin\nendif\n\nEXAMPLES := $(patsubst ./examples/%/,%,$(sort $(dir $(wildcard ./examples/*/))))\nEXAMPLE_TARGETS := $(patsubst %,example-%, $(EXAMPLES))\n\nCMDS := $(patsubst ./cmd/%/,%,$(sort $(dir $(wildcard ./cmd/*/))))\nCMD_TARGETS := $(patsubst %,cmd-%, $(CMDS))\n\nCHECK_TARGETS := lint\nMAKE_TARGETS := binaries build check fmt lint-internal test examples cmds coverage generate vendor check-modules $(CHECK_TARGETS)\n\nTARGETS := $(MAKE_TARGETS) $(EXAMPLE_TARGETS) $(CMD_TARGETS)\n\nDOCKER_TARGETS := $(patsubst %,docker-%, $(TARGETS))\n.PHONY: $(TARGETS) $(DOCKER_TARGETS)\n\nifeq ($(VERSION),)\nCLI_VERSION = $(LIB_VERSION)$(if $(LIB_TAG),-$(LIB_TAG))\nelse\nCLI_VERSION = $(VERSION)\nendif\nCLI_VERSION_PACKAGE = github.com/NVIDIA/k8s-device-plugin/internal/info\n\nbinaries: cmds\nifneq ($(PREFIX),)\ncmd-%: COMMAND_BUILD_OPTIONS = -o $(PREFIX)/$(*)\nendif\nifneq ($(shell uname),Darwin)\nEXTLDFLAGS = -Wl,--export-dynamic -Wl,--unresolved-symbols=ignore-in-object-files\nelse\nEXTLDFLAGS = -Wl,-undefined,dynamic_lookup\nendif\nBUILDFLAGS = -ldflags \"-s -w '-extldflags=$(EXTLDFLAGS)' -X $(CLI_VERSION_PACKAGE).gitCommit=$(GIT_COMMIT) -X $(CLI_VERSION_PACKAGE).version=$(CLI_VERSION)\"\nbuild:\n\tgo build $(BUILDFLAGS) ./...\n\ncmds: $(CMD_TARGETS)\n$(CMD_TARGETS): cmd-%:\n\tgo build $(BUILDFLAGS) $(COMMAND_BUILD_OPTIONS) $(MODULE)/cmd/$(*)\n\nexamples: $(EXAMPLE_TARGETS)\n$(EXAMPLE_TARGETS): example-%:\n\tgo build $(BUILDFLAGS) $(COMMAND_BUILD_OPTIONS) ./examples/$(*)\n\n# We also add top-level targets for testing make targets:\n.PHONY: $(TEST_TARGETS)\nTESTS_FOLDERS := $(patsubst ./tests/%/,%,$(sort $(dir $(wildcard ./tests/*/))))\nTEST_TARGETS := $(patsubst %,test-%,$(TESTS_FOLDERS))\n$(TEST_TARGETS): test-%:\n\tmake -f tests/$(*)/Makefile test\n\nall: check test build binaries\ncheck: $(CHECK_TARGETS)\n\n# Apply go fmt to the codebase\nfmt:\n\tgo list -f '{{.Dir}}' $(MODULE)/... \\\n\t\t| xargs gofmt -s -l -w\n\ngoimports:\n\tgo list -f {{.Dir}} $(MODULE)/... \\\n\t\t| xargs goimports -local $(MODULE) -w\n\nlint:\n\tgolangci-lint run ./...\n\n\nmod-tidy:\n\t@for mod in $$(find . -name go.mod); do \\\n\t    echo \"Tidying $$mod...\"; ( \\\n\t        cd $$(dirname $$mod) && go mod tidy \\\n            ) || exit 1; \\\n\tdone\n\nmod-verify:\n\t@for mod in $$(find . -name go.mod); do \\\n\t    echo \"Verifying $$mod...\"; ( \\\n\t        cd $$(dirname $$mod) && go mod verify | sed 's/^/  /g' \\\n\t    ) || exit 1; \\\n\tdone\n\nmod-vendor: mod-tidy\n\t@for mod in $$(find . -name go.mod); do \\\n\t    echo \"Verifying $$mod...\"; ( \\\n\t        cd $$(dirname $$mod) && go mod vendor \\\n\t    ) || exit 1; \\\n\tdone\n\nvendor: mod-vendor\n\ncheck-modules: vendor\n\tgit diff --quiet HEAD -- $$(find . -name go.mod -o -name go.sum -o -name vendor)\n\nCOVERAGE_FILE := coverage.out\ntest: build cmds\n\tgo test -coverprofile=$(COVERAGE_FILE) $(MODULE)/cmd/... $(MODULE)/internal/... $(MODULE)/api/...\n\ncoverage: test\n\tcat $(COVERAGE_FILE) | grep -v \"_mock.go\" > $(COVERAGE_FILE).no-mocks\n\tgo tool cover -func=$(COVERAGE_FILE).no-mocks\n\ngenerate:\n\tgo generate $(MODULE)/...\n\n\n# Generate an image for containerized builds\n# Note: This image is local only\n.PHONY: .build-image\n.build-image:\n\tmake -f deployments/devel/Makefile .build-image\n\nifeq ($(BUILD_DEVEL_IMAGE),yes)\n$(DOCKER_TARGETS): .build-image\n.shell: .build-image\nendif\n\n$(DOCKER_TARGETS): docker-%:\n\t@echo \"Running 'make $(*)' in container image $(BUILDIMAGE)\"\n\t$(DOCKER) run \\\n\t\t--rm \\\n\t\t-e GOCACHE=/tmp/.cache/go \\\n\t\t-e GOMODCACHE=/tmp/.cache/gomod \\\n\t\t-v $(PWD):/work \\\n\t\t-w /work \\\n\t\t--user $$(id -u):$$(id -g) \\\n\t\t$(BUILDIMAGE) \\\n\t\t\tmake $(*)\n\n# Start an interactive shell using the development image.\nPHONY: .shell\n.shell:\n\t$(DOCKER) run \\\n\t\t--rm \\\n\t\t-ti \\\n\t\t-e GOCACHE=/tmp/.cache/go \\\n\t\t-e GOMODCACHE=/tmp/.cache/gomod \\\n\t\t-v $(PWD):/work \\\n\t\t-w /work \\\n\t\t--user $$(id -u):$$(id -g) \\\n\t\t$(BUILDIMAGE)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 45.9130859375,
          "content": "# NVIDIA device plugin for Kubernetes\n\n[![End-to-end Tests](https://github.com/NVIDIA/k8s-device-plugin/actions/workflows/e2e.yaml/badge.svg)](https://github.com/NVIDIA/k8s-device-plugin/actions/workflows/e2e.yaml) [![Go Report Card](https://goreportcard.com/badge/github.com/NVIDIA/k8s-device-plugin)](https://goreportcard.com/report/github.com/NVIDIA/k8s-device-plugin) [![Latest Release](https://img.shields.io/github/v/release/NVIDIA/k8s-device-plugin)](https://github.com/NVIDIA/k8s-device-plugin/releases/latest)\n\n## Table of Contents\n\n- [About](#about)\n- [Prerequisites](#prerequisites)\n- [Quick Start](#quick-start)\n  - [Preparing your GPU Nodes](#preparing-your-gpu-nodes)\n    - [Example for debian-based systems with `docker` and `containerd`](#example-for-debian-based-systems-with-docker-and-containerd)\n      - [Install the NVIDIA Container Toolkit](#install-the-nvidia-container-toolkit)\n      - [Notes on `CRI-O` configuration](#notes-on-cri-o-configuration)\n  - [Enabling GPU Support in Kubernetes](#enabling-gpu-support-in-kubernetes)\n  - [Running GPU Jobs](#running-gpu-jobs)\n- [Configuring the NVIDIA device plugin binary](#configuring-the-nvidia-device-plugin-binary)\n  - [As command line flags or envvars](#as-command-line-flags-or-envvars)\n  - [As a configuration file](#as-a-configuration-file)\n  - [Configuration Option Details](#configuration-option-details)\n  - [Shared Access to GPUs](#shared-access-to-gpus)\n    - [With CUDA Time-Slicing](#with-cuda-time-slicing)\n    - [With CUDA MPS](#with-cuda-mps)\n  - [IMEX Support](#imex-support)\n- [Catalog of Labels](#catalog-of-labels)\n- [Deployment via `helm`](#deployment-via-helm)\n  - [Configuring the device plugin's `helm` chart](#configuring-the-device-plugins-helm-chart)\n    - [Passing configuration to the plugin via a `ConfigMap`](#passing-configuration-to-the-plugin-via-a-configmap)\n      - [Single Config File Example](#single-config-file-example)\n      - [Multiple Config File Example](#multiple-config-file-example)\n      - [Updating Per-Node Configuration With a Node Label](#updating-per-node-configuration-with-a-node-label)\n    - [Setting other helm chart values](#setting-other-helm-chart-values)\n    - [Deploying with gpu-feature-discovery for automatic node labels](#deploying-with-gpu-feature-discovery-for-automatic-node-labels)\n    - [Deploying gpu-feature-discovery in standalone mode](#deploying-gpu-feature-discovery-in-standalone-mode)\n  - [Deploying via `helm install` with a direct URL to the `helm` package](#deploying-via-helm-install-with-a-direct-url-to-the-helm-package)\n- [Building and Running Locally](#building-and-running-locally)\n  - [With Docker](#with-docker)\n    - [Build](#build)\n    - [Run](#run)\n  - [Without Docker](#without-docker)\n    - [Build](#build-1)\n    - [Run](#run-1)\n- [Changelog](#changelog)\n- [Issues and Contributing](#issues-and-contributing)\n  - [Versioning](#versioning)\n  - [Upgrading Kubernetes with the Device Plugin](#upgrading-kubernetes-with-the-device-plugin)\n\n## About\n\nThe NVIDIA device plugin for Kubernetes is a Daemonset that allows you to automatically:\n\n- Expose the number of GPUs on each nodes of your cluster\n- Keep track of the health of your GPUs\n- Run GPU enabled containers in your Kubernetes cluster.\n\nThis repository contains NVIDIA's official implementation of the [Kubernetes device plugin](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/).\nAs of v0.15.0 this repository also holds the implementation for GPU Feature Discovery labels,\nfor further information on GPU Feature Discovery see [here](docs/gpu-feature-discovery/README.md).\n\nPlease note that:\n\n- The NVIDIA device plugin API is beta as of Kubernetes v1.10.\n- The NVIDIA device plugin is currently lacking\n  - Comprehensive GPU health checking features\n  - GPU cleanup features\n- Support will only be provided for the official NVIDIA device plugin (and not\n  for forks or other variants of this plugin).\n\n## Prerequisites\n\nThe list of prerequisites for running the NVIDIA device plugin is described below:\n\n- NVIDIA drivers ~= 384.81\n- nvidia-docker >= 2.0 || nvidia-container-toolkit >= 1.7.0 (>= 1.11.0 to use integrated GPUs on Tegra-based systems)\n- nvidia-container-runtime configured as the default low-level runtime\n- Kubernetes version >= 1.10\n\n## Quick Start\n\n### Preparing your GPU Nodes\n\nThe following steps need to be executed on all your GPU nodes.\nThis README assumes that the NVIDIA drivers and the `nvidia-container-toolkit` have been pre-installed.\nIt also assumes that you have configured the `nvidia-container-runtime` as the default low-level runtime to use.\n\nPlease see: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html\n\n#### Example for debian-based systems with `docker` and `containerd`\n\n##### Install the NVIDIA Container Toolkit\n\nFor instructions on installing and getting started with the NVIDIA Container Toolkit, refer to the [installation guide](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#installation-guide).\n\nAlso note the configuration instructions for:\n\n- [`containerd`](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#configuring-containerd-for-kubernetes)\n- [`CRI-O`](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#configuring-cri-o)\n- [`docker` (Deprecated)](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#configuring-docker)\n\nRemembering to restart each runtime after applying the configuration changes.\n\nIf the `nvidia` runtime should be set as the default runtime (required for `docker`), the `--set-as-default` argument\nmust also be included in the commands above. If this is not done, a RuntimeClass needs to be defined.\n\n##### Notes on `CRI-O` configuration\n\nWhen running `kubernetes` with `CRI-O`, add the config file to set the\n`nvidia-container-runtime` as the default low-level OCI runtime under\n`/etc/crio/crio.conf.d/99-nvidia.conf`. This will take priority over the default\n`crun` config file at `/etc/crio/crio.conf.d/10-crun.conf`:\n\n```toml\n[crio]\n\n  [crio.runtime]\n    default_runtime = \"nvidia\"\n\n    [crio.runtime.runtimes]\n\n      [crio.runtime.runtimes.nvidia]\n        runtime_path = \"/usr/bin/nvidia-container-runtime\"\n        runtime_type = \"oci\"\n```\n\nAs stated in the linked documentation, this file can automatically be generated with the nvidia-ctk command:\n\n```shell\nsudo nvidia-ctk runtime configure --runtime=crio --set-as-default --config=/etc/crio/crio.conf.d/99-nvidia.conf\n```\n\n`CRI-O` uses `crun` as default low-level OCI runtime so `crun` needs to be added\nto the runtimes of the `nvidia-container-runtime` in the config file at `/etc/nvidia-container-runtime/config.toml`:\n\n```toml\n[nvidia-container-runtime]\nruntimes = [\"crun\", \"docker-runc\", \"runc\"]\n```\n\nAnd then restart `CRI-O`:\n\n```shell\nsudo systemctl restart crio\n```\n\n### Enabling GPU Support in Kubernetes\n\nOnce you have configured the options above on all the GPU nodes in your\ncluster, you can enable GPU support by deploying the following Daemonset:\n\n```shell\nkubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.17.0/deployments/static/nvidia-device-plugin.yml\n```\n\n**Note:** This is a simple static daemonset meant to demonstrate the basic\nfeatures of the `nvidia-device-plugin`. Please see the instructions below for\n[Deployment via `helm`](#deployment-via-helm) when deploying the plugin in a\nproduction setting.\n\n### Running GPU Jobs\n\nWith the daemonset deployed, NVIDIA GPUs can now be requested by a container\nusing the `nvidia.com/gpu` resource type:\n\n```shell\ncat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-pod\nspec:\n  restartPolicy: Never\n  containers:\n    - name: cuda-container\n      image: nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda12.5.0\n      resources:\n        limits:\n          nvidia.com/gpu: 1 # requesting 1 GPU\n  tolerations:\n  - key: nvidia.com/gpu\n    operator: Exists\n    effect: NoSchedule\nEOF\n```\n\n```shell\n$ kubectl logs gpu-pod\n[Vector addition of 50000 elements]\nCopy input data from the host memory to the CUDA device\nCUDA kernel launch with 196 blocks of 256 threads\nCopy output data from the CUDA device to the host memory\nTest PASSED\nDone\n```\n\n> [!WARNING]\n> If you do not request GPUs when you use the device plugin, the plugin exposes all the GPUs on the machine inside your container.\n\n## Configuring the NVIDIA device plugin binary\n\nThe NVIDIA device plugin has a number of options that can be configured for it.\nThese options can be configured as command line flags, environment variables,\nor via a config file when launching the device plugin. Here we explain what\neach of these options are and how to configure them directly against the plugin\nbinary. The following section explains how to set these configurations when\ndeploying the plugin via `helm`.\n\n### As command line flags or envvars\n\n| Flag                     | Environment Variable    | Default Value   |\n|--------------------------|-------------------------|-----------------|\n| `--mig-strategy`         | `$MIG_STRATEGY`         | `\"none\"`        |\n| `--fail-on-init-error`   | `$FAIL_ON_INIT_ERROR`   | `true`          |\n| `--nvidia-driver-root`   | `$NVIDIA_DRIVER_ROOT`   | `\"/\"`           |\n| `--pass-device-specs`    | `$PASS_DEVICE_SPECS`    | `false`         |\n| `--device-list-strategy` | `$DEVICE_LIST_STRATEGY` | `\"envvar\"`      |\n| `--device-id-strategy`   | `$DEVICE_ID_STRATEGY`   | `\"uuid\"`        |\n| `--config-file`          | `$CONFIG_FILE`          | `\"\"`            |\n\n### As a configuration file\n\n```yaml\nversion: v1\nflags:\n  migStrategy: \"none\"\n  failOnInitError: true\n  nvidiaDriverRoot: \"/\"\n  plugin:\n    passDeviceSpecs: false\n    deviceListStrategy: \"envvar\"\n    deviceIDStrategy: \"uuid\"\n```\n\n**Note:** The configuration file has an explicit `plugin` section because it\nis a shared configuration between the plugin and\n[`gpu-feature-discovery`](https://github.com/NVIDIA/gpu-feature-discovery).\nAll options inside the `plugin` section are specific to the plugin. All\noptions outside of this section are shared.\n\n### Configuration Option Details\n\n**`MIG_STRATEGY`**:\n  the desired strategy for exposing MIG devices on GPUs that support it\n\n  `[none | single | mixed] (default 'none')`\n\n  The `MIG_STRATEGY` option configures the daemonset to be able to expose\n  Multi-Instance GPUs (MIG) on GPUs that support them. More information on what\n  these strategies are and how they should be used can be found in [Supporting\n  Multi-Instance GPUs (MIG) in\n  Kubernetes](https://docs.google.com/document/d/1mdgMQ8g7WmaI_XVVRrCvHPFPOMCm5LQD5JefgAh6N8g).\n\n  **Note:** With a `MIG_STRATEGY` of mixed, you will have additional resources\n  available to you of the form `nvidia.com/mig-<slice_count>g.<memory_size>gb`\n  that you can set in your pod spec to get access to a specific MIG device.\n\n**`FAIL_ON_INIT_ERROR`**:\n  fail the plugin if an error is encountered during initialization, otherwise block indefinitely\n\n  `(default 'true')`\n\n  When set to true, the `FAIL_ON_INIT_ERROR` option fails the plugin if an error is\n  encountered during initialization. When set to false, it prints an error\n  message and blocks the plugin indefinitely instead of failing. Blocking\n  indefinitely follows legacy semantics that allow the plugin to deploy\n  successfully on nodes that don't have GPUs on them (and aren't supposed to have\n  GPUs on them) without throwing an error. In this way, you can blindly deploy a\n  daemonset with the plugin on all nodes in your cluster, whether they have GPUs\n  on them or not, without encountering an error.  However, doing so means that\n  there is no way to detect an actual error on nodes that are supposed to have\n  GPUs on them. Failing if an initialization error is encountered is now the\n  default and should be adopted by all new deployments.\n\n**`NVIDIA_DRIVER_ROOT`**:\n  the root path for the NVIDIA driver installation\n\n  `(default '/')`\n\n  When the NVIDIA drivers are installed directly on the host, this should be\n  set to `'/'`. When installed elsewhere (e.g. via a driver container), this\n  should be set to the root filesystem where the drivers are installed (e.g.\n  `'/run/nvidia/driver'`).\n\n  **Note:** This option is only necessary when used in conjunction with the\n  `$PASS_DEVICE_SPECS` option described below. It tells the plugin what prefix\n  to add to any device file paths passed back as part of the device specs.\n\n**`PASS_DEVICE_SPECS`**:\n  pass the paths and desired device node permissions for any NVIDIA devices\n  being allocated to the container\n\n  `(default 'false')`\n\n  This option exists for the sole purpose of allowing the device plugin to\n  interoperate with the `CPUManager` in Kubernetes. Setting this flag also\n  requires one to deploy the daemonset with elevated privileges, so only do so if\n  you know you need to interoperate with the `CPUManager`.\n\n**`DEVICE_LIST_STRATEGY`**:\n  the desired strategy for passing the device list to the underlying runtime\n\n  `[envvar | volume-mounts | cdi-annotations | cdi-cri ] (default 'envvar')`\n\n  **Note**: Multiple device list strategies can be specified (as a comma-separated list).\n\n  The `DEVICE_LIST_STRATEGY` flag allows one to choose which strategy the plugin\n  will use to advertise the list of GPUs allocated to a container. Possible values are:\n\n  - `envvar` (default): the `NVIDIA_VISIBLE_DEVICES` environment variable\n  as described\n  [here](https://github.com/NVIDIA/nvidia-container-runtime#nvidia_visible_devices)\n  is used to select the devices that are to be injected by the NVIDIA Container Runtime.\n  - `volume-mounts`: the list of devices is passed as a set of volume mounts instead of as an environment variable\n  to instruct the NVIDIA Container Runtime to inject the devices.\n  Details for the\n  rationale behind this strategy can be found\n  [here](https://docs.google.com/document/d/1uXVF-NWZQXgP1MLb87_kMkQvidpnkNWicdpO2l9g-fw/edit#heading=h.b3ti65rojfy5).\n  - `cdi-annotations`: CDI annotations are used to select the devices that are to be injected.\n  Note that this does not require the NVIDIA Container Runtime, but does required a CDI-enabled container engine.\n  - `cdi-cri`: the `CDIDevices` CRI field is used to select the CDI devices that are to be injected.\n  This requires support in Kubernetes to forward these requests in the CRI to a CDI-enabled container engine.\n\n**`DEVICE_ID_STRATEGY`**:\n  the desired strategy for passing device IDs to the underlying runtime\n\n  `[uuid | index] (default 'uuid')`\n\n  The `DEVICE_ID_STRATEGY` flag allows one to choose which strategy the plugin will\n  use to pass the device ID of the GPUs allocated to a container. The device ID\n  has traditionally been passed as the UUID of the GPU. This flag lets a user\n  decide if they would like to use the UUID or the index of the GPU (as seen in\n  the output of `nvidia-smi`) as the identifier passed to the underlying runtime.\n  Passing the index may be desirable in situations where pods that have been\n  allocated GPUs by the plugin get restarted with different physical GPUs\n  attached to them.\n\n**`CONFIG_FILE`**:\n  point the plugin at a configuration file instead of relying on command line\n  flags or environment variables\n\n  `(default '')`\n\n  The order of precedence for setting each option is (1) command line flag, (2)\n  environment variable, (3) configuration file. In this way, one could use a\n  pre-defined configuration file, but then override the values set in it at\n  launch time. As described below, a `ConfigMap` can be used to point the\n  plugin at a desired configuration file when deploying via `helm`.\n\n### Shared Access to GPUs\n\nThe NVIDIA device plugin allows oversubscription of GPUs through a set of\nextended options in its configuration file. There are two flavors of sharing\navailable: Time-Slicing and MPS.\n\n> [!NOTE]\n> Time-slicing and MPS are mutually exclusive.\n\nIn the case of time-slicing, CUDA time-slicing is used to allow workloads sharing a GPU to\ninterleave with each other. However, nothing special is done to isolate workloads that are\ngranted replicas from the same underlying GPU, and each workload has access to\nthe GPU memory and runs in the same fault-domain as of all the others (meaning\nif one workload crashes, they all do).\n\nIn the case of MPS, a control daemon is used to manage access to the shared GPU.\nIn contrast to time-slicing, MPS does space partitioning and allows memory and\ncompute resources to be explicitly partitioned and enforces these limits per\nworkload.\n\nWith both time-slicing and MPS, the same sharing method is applied to all GPUs on\na node. You cannot configure sharing on a per-GPU basis.\n\n#### With CUDA Time-Slicing\n\nThe extended options for sharing using time-slicing can be seen below:\n\n```yaml\nversion: v1\nsharing:\n  timeSlicing:\n    renameByDefault: <bool>\n    failRequestsGreaterThanOne: <bool>\n    resources:\n    - name: <resource-name>\n      replicas: <num-replicas>\n    ...\n```\n\nThat is, for each named resource under `sharing.timeSlicing.resources`, a number\nof replicas can now be specified for that resource type. These replicas\nrepresent the number of shared accesses that will be granted for a GPU\nrepresented by that resource type.\n\nIf `renameByDefault=true`, then each resource will be advertised under the name\n`<resource-name>.shared` instead of simply `<resource-name>`.\n\nIf `failRequestsGreaterThanOne=true`, then the plugin will fail to allocate any\nshared resources to a container if they request more than one. The container’s\npod will fail with an `UnexpectedAdmissionError` and need to be manually deleted,\nupdated, and redeployed.\n\nFor example:\n\n```yaml\nversion: v1\nsharing:\n  timeSlicing:\n    resources:\n    - name: nvidia.com/gpu\n      replicas: 10\n```\n\nIf this configuration were applied to a node with 8 GPUs on it, the plugin\nwould now advertise 80 `nvidia.com/gpu` resources to Kubernetes instead of 8.\n\n```shell\n$ kubectl describe node\n...\nCapacity:\n  nvidia.com/gpu: 80\n...\n```\n\nLikewise, if the following configuration were applied to a node, then 80\n`nvidia.com/gpu.shared` resources would be advertised to Kubernetes instead of 8\n`nvidia.com/gpu` resources.\n\n```yaml\nversion: v1\nsharing:\n  timeSlicing:\n    renameByDefault: true\n    resources:\n    - name: nvidia.com/gpu\n      replicas: 10\n    ...\n```\n\n```shell\n$ kubectl describe node\n...\nCapacity:\n  nvidia.com/gpu.shared: 80\n...\n```\n\nIn both cases, the plugin simply creates 10 references to each GPU and\nindiscriminately hands them out to anyone that asks for them.\n\nIf `failRequestsGreaterThanOne=true` were set in either of these\nconfigurations and a user requested more than one `nvidia.com/gpu` or\n`nvidia.com/gpu.shared` resource in their pod spec, then the container would\nfail with the resulting error:\n\n```shell\n$ kubectl describe pod gpu-pod\n...\nEvents:\n  Type     Reason                    Age   From               Message\n  ----     ------                    ----  ----               -------\n  Warning  UnexpectedAdmissionError  13s   kubelet            Allocate failed due to rpc error: code = Unknown desc = request for 'nvidia.com/gpu: 2' too large: maximum request size for shared resources is 1, which is unexpected\n...\n```\n\n**Note:** Unlike with \"normal\" GPU requests, requesting more than one shared\nGPU does not imply that you will get guaranteed access to a proportional amount\nof compute power. It only implies that you will get access to a GPU that is\nshared by other clients (each of which has the freedom to run as many processes\non the underlying GPU as they want). Under the hood CUDA will simply give an\nequal share of time to all of the GPU processes across all of the clients. The\n`failRequestsGreaterThanOne` flag is meant to help users understand this\nsubtlety, by treating a request of `1` as an access request rather than an\nexclusive resource request. Setting `failRequestsGreaterThanOne=true` is\nrecommended, but it is set to `false` by default to retain backwards\ncompatibility.\n\nAs of now, the only supported resource available for time-slicing are\n`nvidia.com/gpu` as well as any of the resource types that emerge from\nconfiguring a node with the mixed MIG strategy.\n\nFor example, the full set of time-sliceable resources on a T4 card would be:\n\n```\nnvidia.com/gpu\n```\n\nAnd the full set of time-sliceable resources on an A100 40GB card would be:\n\n```\nnvidia.com/gpu\nnvidia.com/mig-1g.5gb\nnvidia.com/mig-2g.10gb\nnvidia.com/mig-3g.20gb\nnvidia.com/mig-7g.40gb\n```\n\nLikewise, on an A100 80GB card, they would be:\n\n```\nnvidia.com/gpu\nnvidia.com/mig-1g.10gb\nnvidia.com/mig-2g.20gb\nnvidia.com/mig-3g.40gb\nnvidia.com/mig-7g.80gb\n```\n\n#### With CUDA MPS\n\n> [!WARNING]\n> As of v0.15.0 of the device plugin, MPS support is considered experimental. Please see the [release notes](https://github.com/NVIDIA/k8s-device-plugin/releases/tag/v0.15.0) for further details.\n\n> [!NOTE]\n> Sharing with MPS is currently not supported on devices with MIG enabled.\n\nThe extended options for sharing using MPS can be seen below:\n\n```yaml\nversion: v1\nsharing:\n  mps:\n    renameByDefault: <bool>\n    resources:\n    - name: <resource-name>\n      replicas: <num-replicas>\n    ...\n```\n\nThat is, for each named resource under `sharing.mps.resources`, a number\nof replicas can be specified for that resource type. As is the case with\ntime-slicing, these replicas represent the number of shared accesses that will\nbe granted for a GPU associated with that resource type. In contrast with\ntime-slicing, the amount of memory allowed per client (i.e. per partition) is\nmanaged by the MPS control daemon and limited to an equal fraction of the total\ndevice memory. In addition to controlling the amount of memory that each client\ncan consume, the MPS control daemon also limits the amount of compute capacity\nthat can be consumed by a client.\n\nIf `renameByDefault=true`, then each resource will be advertised under the name\n`<resource-name>.shared` instead of simply `<resource-name>`.\n\nFor example:\n\n```yaml\nversion: v1\nsharing:\n  mps:\n    resources:\n    - name: nvidia.com/gpu\n      replicas: 10\n```\n\nIf this configuration were applied to a node with 8 GPUs on it, the plugin\nwould now advertise 80 `nvidia.com/gpu` resources to Kubernetes instead of 8.\n\n```shell\n$ kubectl describe node\n...\nCapacity:\n  nvidia.com/gpu: 80\n...\n```\n\nLikewise, if the following configuration were applied to a node, then 80\n`nvidia.com/gpu.shared` resources would be advertised to Kubernetes instead of 8\n`nvidia.com/gpu` resources.\n\n```yaml\nversion: v1\nsharing:\n  mps:\n    renameByDefault: true\n    resources:\n    - name: nvidia.com/gpu\n      replicas: 10\n    ...\n```\n\n```shell\n$ kubectl describe node\n...\nCapacity:\n  nvidia.com/gpu.shared: 80\n...\n```\n\nFurthermore, each of these resources -- either `nvidia.com/gpu` or\n`nvidia.com/gpu.shared` -- would have access to the same fraction (1/10) of the\ntotal memory and compute resources of the GPU.\n\n**Note**: As of now, the only supported resource available for MPS are `nvidia.com/gpu`\nresources and only with full GPUs.\n\n### IMEX Support\n\nThe NVIDIA GPU Device Plugin can be configured to inject IMEX channels into\nworkloads.\n\nThis opt-in behavior is global and affects all workloads and is controlled by\nthe `imex.channelIDs` and `imex.required` configuration options.\n\n| `imex.channelIDs` | `imex.required` | Effect |\n|---|---|---|\n| `[]` | * | (default) No IMEX channels are added to workload requests. Note that the `imex.required` field has no effect in this case |\n| `[0]` | `false` | If the requested IMEX channel (`0`) is discoverable by the NVIDIA GPU Device Plugin, the channel will be added to each workload request. If the channel cannot be discovered no channels are added to workload requests. |\n| `[0]` | `true` | If the requested IMEX channel (`0`) is discoverable by the NVIDIA GPU Device Plugin, the channel will be added to each workload request. If the channel cannot be discovered an error will be raised since the channel was marked as `required`. |\n\n**Note**: At present the only valid `imex.channelIDs` configurations are `[]` and `[0]`.\n\nFor the containerized NVIDIA GPU Device Plugin running to be able to successfully\ndiscover available IMEX channels, the corresponding device nodes must be available\nto the container.\n\n## Catalog of Labels\n\nThe NVIDIA device plugin reads and writes a number of different labels that it uses as either\nconfiguration elements or informational elements. The following table documents and describes each label\nalong with their use. See the related table [here](/docs/gpu-feature-discovery/README.md#generated-labels) for the labels GFD adds.\n\n| Label Name                          | Description                                                                                                                                                                                                                                  | Example        |\n| ----------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------- |\n| nvidia.com/device-plugin.config     | Specifies the configuration to apply to the node. You apply this this label to perform per-node configuration. Refer to [Updating Per-Node Configuration With a Node Label](#updating-per-node-configuration-with-a-node-label) for details. | my-mps-config  |\n| nvidia.com/gpu.sharing-strategy     | Specifies the sharing strategy. The default value, `none`, indicates no sharing.  Other values are `mps` and `time-slicing`.                                                                                                                 | time-slicing   |\n| nvidia.com/mig.capable              | Specifies if any device on the node supports MIG.                                                                                                                                                                                            | false          |\n| nvidia.com/mps.capable              | Specifies if devices on the node are configured for MPS.                                                                                                                                                                                     | false          |\n| nvidia.com/gpu.sharing-strategy     | Specifies the sharing strategy. The default value, `none`, indicates no sharing.  Other values are `mps` and `time-slicing`.                                                                                                                 | time-slicing   |\n| nvidia.com/mig.capable              | Specifies if any device on the node supports MIG.                                                                                                                                                                                            | false          |\n| nvidia.com/mps.capable              | Specifies if devices on the node are configured for MPS.                                                                                                                                                                                     | false          |\n| nvidia.com/vgpu.present             | Specifies if devices on the node use vGPU.                                                                                                                                                                                                   | false          |\n| nvidia.com/vgpu.host-driver-branch  | Specifies the vGPU host driver branch on the underlying hypervisor.                                                                                                                                                                          | r550_40        |\n| nvidia.com/vgpu.host-driver-version | Specifies the vGPU host driver version on the underlying hypervisor.                                                                                                                                                                         | 550.54.16      |\n\n## Deployment via `helm`\n\nThe preferred method to deploy the device plugin is as a daemonset using `helm`.\nInstructions for installing `helm` can be found\n[here](https://helm.sh/docs/intro/install/).\n\nBegin by setting up the plugin's `helm` repository and updating it at follows:\n\n```shell\nhelm repo add nvdp https://nvidia.github.io/k8s-device-plugin\nhelm repo update\n```\n\nThen verify that the latest release (`v0.17.0`) of the plugin is available:\n\n```shell\n$ helm search repo nvdp --devel\nNAME                     \t  CHART VERSION  APP VERSION\tDESCRIPTION\nnvdp/nvidia-device-plugin\t  0.17.0\t 0.17.0\t\tA Helm chart for ...\n```\n\nOnce this repo is updated, you can begin installing packages from it to deploy\nthe `nvidia-device-plugin` helm chart.\n\nThe most basic installation command without any options is then:\n\n```shell\nhelm upgrade -i nvdp nvdp/nvidia-device-plugin \\\n  --namespace nvidia-device-plugin \\\n  --create-namespace \\\n  --version 0.17.0\n```\n\n**Note:** You only need the to pass the `--devel` flag to `helm search repo`\nand the `--version` flag to `helm upgrade -i` if this is a pre-release\nversion (e.g. `<version>-rc.1`). Full releases will be listed without this.\n\n### Configuring the device plugin's `helm` chart\n\nThe `helm` chart for the latest release of the plugin (`v0.17.0`) includes\na number of customizable values.\n\nPrior to `v0.12.0` the most commonly used values were those that had direct\nmappings to the command line options of the plugin binary. As of `v0.12.0`, the\npreferred method to set these options is via a `ConfigMap`. The primary use\ncase of the original values is then to override an option from the `ConfigMap`\nif desired. Both methods are discussed in more detail below.\n\nThe full set of values that can be set are found here:\n[here](https://github.com/NVIDIA/k8s-device-plugin/blob/v0.17.0/deployments/helm/nvidia-device-plugin/values.yaml).\n\n#### Passing configuration to the plugin via a `ConfigMap`\n\nIn general, we provide a mechanism to pass _multiple_ configuration files to\nto the plugin's `helm` chart, with the ability to choose which configuration\nfile should be applied to a node via a node label.\n\nIn this way, a single chart can be used to deploy each component, but custom\nconfigurations can be applied to different nodes throughout the cluster.\n\nThere are two ways to provide a `ConfigMap` for use by the plugin:\n\n  1. Via an external reference to a pre-defined `ConfigMap`\n  1. As a set of named config files to build an integrated `ConfigMap` associated with the chart\n\nThese can be set via the chart values `config.name` and `config.map` respectively.\nIn both cases, the value `config.default` can be set to point to one of the\nnamed configs in the `ConfigMap` and provide a default configuration for nodes\nthat have not been customized via a node label (more on this later).\n\n##### Single Config File Example\n\nAs an example, create a valid config file on your local filesystem, such as the following:\n\n```shell\ncat << EOF > /tmp/dp-example-config0.yaml\nversion: v1\nflags:\n  migStrategy: \"none\"\n  failOnInitError: true\n  nvidiaDriverRoot: \"/\"\n  plugin:\n    passDeviceSpecs: false\n    deviceListStrategy: envvar\n    deviceIDStrategy: uuid\nEOF\n```\n\nAnd deploy the device plugin via helm (pointing it at this config file and giving it a name):\n\n```shell\nhelm upgrade -i nvdp nvdp/nvidia-device-plugin \\\n  --version=0.17.0 \\\n  --namespace nvidia-device-plugin \\\n  --create-namespace \\\n  --set-file config.map.config=/tmp/dp-example-config0.yaml\n```\n\nUnder the hood this will deploy a `ConfigMap` associated with the plugin and put\nthe contents of the `dp-example-config0.yaml` file into it, using the name\n`config` as its key. It will then start the plugin such that this config gets\napplied when the plugin comes online.\n\nIf you don’t want the plugin’s helm chart to create the `ConfigMap` for you, you\ncan also point it at a pre-created `ConfigMap` as follows:\n\n```shell\nkubectl create ns nvidia-device-plugin\n```\n\n```shell\nkubectl create cm -n nvidia-device-plugin nvidia-plugin-configs \\\n  --from-file=config=/tmp/dp-example-config0.yaml\n```\n\n```shell\nhelm upgrade -i nvdp nvdp/nvidia-device-plugin \\\n  --version=0.17.0 \\\n  --namespace nvidia-device-plugin \\\n  --create-namespace \\\n  --set config.name=nvidia-plugin-configs\n```\n\n##### Multiple Config File Example\n\nFor multiple config files, the procedure is similar.\n\nCreate a second `config` file with the following contents:\n\n```shell\ncat << EOF > /tmp/dp-example-config1.yaml\nversion: v1\nflags:\n  migStrategy: \"mixed\" # Only change from config0.yaml\n  failOnInitError: true\n  nvidiaDriverRoot: \"/\"\n  plugin:\n    passDeviceSpecs: false\n    deviceListStrategy: envvar\n    deviceIDStrategy: uuid\nEOF\n```\n\nAnd redeploy the device plugin via helm (pointing it at both configs with a specified default).\n\n```shell\nhelm upgrade -i nvdp nvdp/nvidia-device-plugin \\\n  --version=0.17.0 \\\n  --namespace nvidia-device-plugin \\\n  --create-namespace \\\n  --set config.default=config0 \\\n  --set-file config.map.config0=/tmp/dp-example-config0.yaml \\\n  --set-file config.map.config1=/tmp/dp-example-config1.yaml\n```\n\nAs before, this can also be done with a pre-created `ConfigMap` if desired:\n\n```shell\nkubectl create ns nvidia-device-plugin\n```\n\n```shell\nkubectl create cm -n nvidia-device-plugin nvidia-plugin-configs \\\n  --from-file=config0=/tmp/dp-example-config0.yaml \\\n  --from-file=config1=/tmp/dp-example-config1.yaml\n```\n\n```shell\nhelm upgrade -i nvdp nvdp/nvidia-device-plugin \\\n  --version=0.17.0 \\\n  --namespace nvidia-device-plugin \\\n  --create-namespace \\\n  --set config.default=config0 \\\n  --set config.name=nvidia-plugin-configs\n```\n\n**Note:** If the `config.default` flag is not explicitly set, then a default\nvalue will be inferred from the config if one of the config names is set to\n'`default`'. If neither of these are set, then the deployment will fail unless\nthere is only **_one_** config provided. In the case of just a single config being\nprovided, it will be chosen as the default because there is no other option.\n\n##### Updating Per-Node Configuration With a Node Label\n\nWith this setup, plugins on all nodes will have `config0` configured for them\nby default. However, the following label can be set to change which\nconfiguration is applied:\n\n```shell\nkubectl label nodes <node-name> –-overwrite \\\n  nvidia.com/device-plugin.config=<config-name>\n```\n\nFor example, applying a custom config for all nodes that have T4 GPUs installed\non them might be:\n\n```shell\nkubectl label node \\\n  --overwrite \\\n  --selector=nvidia.com/gpu.product=TESLA-T4 \\\n  nvidia.com/device-plugin.config=t4-config\n```\n\n**Note:** This label can be applied either _before_ or _after_ the plugin is\nstarted to get the desired configuration applied on the node. Anytime it\nchanges value, the plugin will immediately be updated to start serving the\ndesired configuration. If it is set to an unknown value, it will skip\nreconfiguration. If it is ever unset, it will fallback to the default.\n\n#### Setting other helm chart values\n\nAs mentioned previously, the device plugin's helm chart continues to provide\ndirect values to set the configuration options of the plugin without using a\n`ConfigMap`. These should only be used to set globally applicable options\n(which should then never be embedded in the set of config files provided by the\n`ConfigMap`), or used to override these options as desired.\n\nThese values are as follows:\n\n```yaml\n  migStrategy:\n      the desired strategy for exposing MIG devices on GPUs that support it\n      [none | single | mixed] (default \"none\")\n  failOnInitError:\n      fail the plugin if an error is encountered during initialization, otherwise block indefinitely\n      (default 'true')\n  compatWithCPUManager:\n      run with escalated privileges to be compatible with the static CPUManager policy\n      (default 'false')\n  deviceListStrategy:\n      the desired strategy for passing the device list to the underlying runtime\n      [envvar | volume-mounts | cdi-annotations | cdi-cri] (default \"envvar\")\n  deviceIDStrategy:\n      the desired strategy for passing device IDs to the underlying runtime\n      [uuid | index] (default \"uuid\")\n  nvidiaDriverRoot:\n      the root path for the NVIDIA driver installation (typical values are '/' or '/run/nvidia/driver')\n```\n\n**Note:**  There is no value that directly maps to the `PASS_DEVICE_SPECS`\nconfiguration option of the plugin. Instead a value called\n`compatWithCPUManager` is provided which acts as a proxy for this option.\nIt both sets the `PASS_DEVICE_SPECS` option of the plugin to true **AND** makes\nsure that the plugin is started with elevated privileges to ensure proper\ncompatibility with the `CPUManager`.\n\nBesides these custom configuration options for the plugin, other standard helm\nchart values that are commonly overridden are:\n\n```yaml\nruntimeClassName:\n  the runtimeClassName to use, for use with clusters that have multiple runtimes. (typical value is 'nvidia')\n```\n\nPlease take a look in the\n[`values.yaml`](https://github.com/NVIDIA/k8s-device-plugin/blob/v0.17.0/deployments/helm/nvidia-device-plugin/values.yaml)\nfile to see the full set of overridable parameters for the device plugin.\n\nExamples of setting these options include:\n\nEnabling compatibility with the `CPUManager` and running with a request for\n100ms of CPU time and a limit of 512MB of memory.\n\n```shell\nhelm upgrade -i nvdp nvdp/nvidia-device-plugin \\\n  --version=0.17.0 \\\n  --namespace nvidia-device-plugin \\\n  --create-namespace \\\n  --set compatWithCPUManager=true \\\n  --set resources.requests.cpu=100m \\\n  --set resources.limits.memory=512Mi\n```\n\nEnabling compatibility with the `CPUManager` and the `mixed` `migStrategy`.\n\n```shell\nhelm upgrade -i nvdp nvdp/nvidia-device-plugin \\\n  --version=0.17.0 \\\n  --namespace nvidia-device-plugin \\\n  --create-namespace \\\n  --set compatWithCPUManager=true \\\n  --set migStrategy=mixed\n```\n\n#### Deploying with gpu-feature-discovery for automatic node labels\n\nAs of `v0.12.0`, the device plugin's helm chart has integrated support to\ndeploy\n[`gpu-feature-discovery`](https://github.com/NVIDIA/gpu-feature-discovery)\n(GFD). You can use GFD to automatically generate labels for the\nset of GPUs available on a node. Under the hood, it leverages [Node Feature Discovery](https://kubernetes-sigs.github.io/node-feature-discovery/stable/get-started/index.html) to perform this labeling.\n\nTo enable it, simply set `gfd.enabled=true` during helm install.\n\n```shell\nhelm upgrade -i nvdp nvdp/nvidia-device-plugin \\\n  --version=0.17.0 \\\n  --namespace nvidia-device-plugin \\\n  --create-namespace \\\n  --set gfd.enabled=true\n```\n\nUnder the hood this will also deploy\n[`node-feature-discovery`](https://github.com/kubernetes-sigs/node-feature-discovery)\n(NFD) since it is a prerequisite of GFD. If you already have NFD deployed on\nyour cluster and do not wish for it to be pulled in by this installation, you\ncan disable it with `nfd.enabled=false`.\n\nIn addition to the standard node labels applied by GFD, the following label\nwill also be included when deploying the plugin with the time-slicing or MPS extensions\ndescribed [above](#shared-access-to-gpus).\n\n```\nnvidia.com/<resource-name>.replicas = <num-replicas>\n```\n\nAdditionally, the `nvidia.com/<resource-name>.product` will be modified as follows if\n`renameByDefault=false`.\n\n```\nnvidia.com/<resource-name>.product = <product name>-SHARED\n```\n\nUsing these labels, users have a way of selecting a shared vs. non-shared GPU\nin the same way they would traditionally select one GPU model over another.\nThat is, the `SHARED` annotation ensures that a `nodeSelector` can be used to\nattract pods to nodes that have shared GPUs on them.\n\nSince having `renameByDefault=true` already encodes the fact that the resource is\nshared on the resource name, there is no need to annotate the product\nname with `SHARED`. Users can already find the shared resources they need by\nsimply requesting it in their pod spec.\n\nNote: When running with `renameByDefault=false` and `migStrategy=single` both\nthe MIG profile name and the new `SHARED` annotation will be appended to the\nproduct name, e.g.:\n\n```\nnvidia.com/gpu.product = A100-SXM4-40GB-MIG-1g.5gb-SHARED\n```\n\n#### Deploying gpu-feature-discovery in standalone mode\n\nAs of v0.15.0, the device plugin's helm chart has integrated support to deploy\n[`gpu-feature-discovery`](/docs/gpu-feature-discovery/README.md#overview)\n\nWhen gpu-feature-discovery in deploying standalone, begin by setting up the\nplugin's `helm` repository and updating it at follows:\n\n```shell\nhelm repo add nvdp https://nvidia.github.io/k8s-device-plugin\nhelm repo update\n```\n\nThen verify that the latest release (`v0.17.0`) of the plugin is available\n(Note that this includes the GFD chart):\n\n```shell\nhelm search repo nvdp --devel\nNAME                     \t  CHART VERSION  APP VERSION\tDESCRIPTION\nnvdp/nvidia-device-plugin\t  0.17.0\t 0.17.0\t\tA Helm chart for ...\n```\n\nOnce this repo is updated, you can begin installing packages from it to deploy\nthe `gpu-feature-discovery` component in standalone mode.\n\nThe most basic installation command without any options is then:\n\n```shell\nhelm upgrade -i nvdp nvdp/nvidia-device-plugin \\\n  --version 0.17.0 \\\n  --namespace gpu-feature-discovery \\\n  --create-namespace \\\n  --set devicePlugin.enabled=false\n```\n\nDisabling auto-deployment of NFD and running with a MIG strategy of 'mixed' in\nthe default namespace.\n\n```shell\nhelm upgrade -i nvdp nvdp/nvidia-device-plugin \\\n    --version=0.17.0 \\\n    --set allowDefaultNamespace=true \\\n    --set nfd.enabled=false \\\n    --set migStrategy=mixed \\\n    --set devicePlugin.enabled=false\n```\n\n**Note:** You only need the to pass the `--devel` flag to `helm search repo`\nand the `--version` flag to `helm upgrade -i` if this is a pre-release\nversion (e.g. `<version>-rc.1`). Full releases will be listed without this.\n\n### Deploying via `helm install` with a direct URL to the `helm` package\n\nIf you prefer not to install from the `nvidia-device-plugin` `helm` repo, you can\nrun `helm install` directly against the tarball of the plugin's `helm` package.\nThe example below installs the same chart as the method above, except that\nit uses a direct URL to the `helm` chart instead of via the `helm` repo.\n\nUsing the default values for the flags:\n\n```shell\nhelm upgrade -i nvdp \\\n  --namespace nvidia-device-plugin \\\n  --create-namespace \\\n  https://nvidia.github.io/k8s-device-plugin/stable/nvidia-device-plugin-0.17.0.tgz\n```\n\n## Building and Running Locally\n\nThe next sections are focused on building the device plugin locally and running it.\nIt is intended purely for development and testing, and not required by most users.\nIt assumes you are pinning to the latest release tag (i.e. `v0.17.0`), but can\neasily be modified to work with any available tag or branch.\n\n### With Docker\n\n#### Build\n\nOption 1, pull the prebuilt image from [Docker Hub](https://hub.docker.com/r/nvidia/k8s-device-plugin):\n\n```shell\ndocker pull nvcr.io/nvidia/k8s-device-plugin:v0.17.0\ndocker tag nvcr.io/nvidia/k8s-device-plugin:v0.17.0 nvcr.io/nvidia/k8s-device-plugin:devel\n```\n\nOption 2, build without cloning the repository:\n\n```shell\ndocker build \\\n  -t nvcr.io/nvidia/k8s-device-plugin:devel \\\n  -f deployments/container/Dockerfile.ubuntu \\\n  https://github.com/NVIDIA/k8s-device-plugin.git#v0.17.0\n```\n\nOption 3, if you want to modify the code:\n\n```shell\ngit clone https://github.com/NVIDIA/k8s-device-plugin.git && cd k8s-device-plugin\ndocker build \\\n  -t nvcr.io/nvidia/k8s-device-plugin:devel \\\n  -f deployments/container/Dockerfile.ubuntu \\\n  .\n```\n\n#### Run\n\nWithout compatibility for the `CPUManager` static policy:\n\n```shell\ndocker run \\\n  -it \\\n  --security-opt=no-new-privileges \\\n  --cap-drop=ALL \\\n  --network=none \\\n  -v /var/lib/kubelet/device-plugins:/var/lib/kubelet/device-plugins \\\n  nvcr.io/nvidia/k8s-device-plugin:devel\n```\n\nWith compatibility for the `CPUManager` static policy:\n\n```shell\ndocker run \\\n  -it \\\n  --privileged \\\n  --network=none \\\n  -v /var/lib/kubelet/device-plugins:/var/lib/kubelet/device-plugins \\\n  nvcr.io/nvidia/k8s-device-plugin:devel --pass-device-specs\n```\n\n### Without Docker\n\n#### Build\n\n```shell\nC_INCLUDE_PATH=/usr/local/cuda/include LIBRARY_PATH=/usr/local/cuda/lib64 go build\n```\n\n#### Run\n\nWithout compatibility for the `CPUManager` static policy:\n\n```shell\n./k8s-device-plugin\n```\n\nWith compatibility for the `CPUManager` static policy:\n\n```shell\n./k8s-device-plugin --pass-device-specs\n```\n\n## Changelog\n\nSee the [changelog](CHANGELOG.md)\n\n## Issues and Contributing\n\n[Checkout the Contributing document!](CONTRIBUTING.md)\n\n- You can report a bug by [filing a new issue](https://github.com/NVIDIA/k8s-device-plugin/issues/new)\n- You can contribute by opening a [pull request](https://help.github.com/articles/using-pull-requests/)\n\n### Versioning\n\nBefore v1.10 the versioning scheme of the device plugin had to match exactly the version of Kubernetes.\nAfter the promotion of device plugins to beta this condition was was no longer required.\nWe quickly noticed that this versioning scheme was very confusing for users as they still expected to see\na version of the device plugin for each version of Kubernetes.\n\nThis versioning scheme applies to the tags `v1.8`, `v1.9`, `v1.10`, `v1.11`, `v1.12`.\n\nWe have now changed the versioning to follow [SEMVER](https://semver.org/). The\nfirst version following this scheme has been tagged `v0.0.0`.\n\nGoing forward, the major version of the device plugin will only change\nfollowing a change in the device plugin API itself. For example, version\n`v1beta1` of the device plugin API corresponds to version `v0.x.x` of the\ndevice plugin. If a new `v2beta2` version of the device plugin API comes out,\nthen the device plugin will increase its major version to `1.x.x`.\n\nAs of now, the device plugin API for Kubernetes >= v1.10 is `v1beta1`.  If you\nhave a version of Kubernetes >= 1.10 you can deploy any device plugin version >\n`v0.0.0`.\n\n### Upgrading Kubernetes with the Device Plugin\n\nUpgrading Kubernetes when you have a device plugin deployed doesn't require you\nto do any, particular changes to your workflow.  The API is versioned and is\npretty stable (though it is not guaranteed to be non breaking). Starting with\nKubernetes version 1.10, you can use `v0.3.0` of the device plugin to perform\nupgrades, and Kubernetes won't require you to deploy a different version of the\ndevice plugin. Once a node comes back online after the upgrade, you will see\nGPUs re-registering themselves automatically.\n\nUpgrading the device plugin itself is a more complex task. It is recommended to\ndrain GPU tasks as we cannot guarantee that GPU tasks will survive a rolling\nupgrade. However we make best efforts to preserve GPU tasks during an upgrade.\n"
        },
        {
          "name": "RELEASE.md",
          "type": "blob",
          "size": 2.181640625,
          "content": "# Release Process\n\nThe device plugin consists in two artifacts:\n- The Device Plugin container\n- The Device Plugin helm chart\n\n# Release Process Checklist:\n- [ ] Create a release PR:\n    - [ ] Run the `./hack/prepare-release.sh` script to update the version in all the needed files. This also creates a [release issue](https://github.com/NVIDIA/cloud-native-team/issues?q=is%3Aissue+is%3Aopen+label%3Arelease)\n    - [ ] Run the `./hack/generate-changelog.sh` script to generate the a draft changelog and update `CHANGELOG.md` with the changes.\n    - [ ] Create a PR from the created `bump-release-{{ .VERSION }}` branch.\n- [ ] Merge the release PR\n- [ ] Tag the release and push the tag to the `internal` mirror:\n    - [ ] Image release pipeline:\n- [ ] Wait for the image release to complete.\n- [ ] Push the tag to the the upstream GitHub repo.\n- [ ] Wait for the [`Release`](https://github.com/NVIDIA/k8s-device-plugin/actions/workflows/release.yaml) GitHub Action to complete\n- [ ] Publish the [draft release](https://github.com/NVIDIA/k8s-device-plugin/releases) created by the GitHub Action\n- [ ] Wait for the [`Publish Helm Chart`](https://github.com/NVIDIA/k8s-device-plugin/actions/workflows/helm.yaml) GitHub Action to complete\n\n## Troubleshooting\n\n*Note*: This assumes that we have the release tag checked out locally.\n\n- If the `Release` GitHub Action fails:\n    - Check the logs for the error first.\n    - Create the helm packages locally by running:\n      ```bash\n      ./hack/package-helm-charts.sh {{ .VERSION }}\n      ```\n    - Create the draft release by running:\n      ```bash\n      ./hack/create-release.sh {{ .VERSION }}\n      ```\n- If the `Publish Helm Chart` GitHub Action fails:\n    - Check the logs for the error.\n    - Download the release artifacts:\n\n    - Update the Helm package index on the `gh-pages` branch by running:\n      ```\n      ./hack/update-helm-index.sh --version {{ .VERSION }}\n      ```\n      (this pulls the packages from the release created in the previous step)\n    - Push the change to the `gh-pages` branch:\n      ```\n      git -C releases/{{ .VERSION }} remote set-url origin git@github.com:NVIDIA/k8s-device-plugin.git\n      git -C releases/{{ .VERSION }} push origin gh-pages\n      ```\n"
        },
        {
          "name": "api",
          "type": "tree",
          "content": null
        },
        {
          "name": "cmd",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "deployments",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 3.41796875,
          "content": "module github.com/NVIDIA/k8s-device-plugin\n\ngo 1.22.2\n\nrequire (\n\tgithub.com/NVIDIA/go-gpuallocator v0.5.0\n\tgithub.com/NVIDIA/go-nvlib v0.7.0\n\tgithub.com/NVIDIA/go-nvml v0.12.4-0\n\tgithub.com/NVIDIA/nvidia-container-toolkit v1.17.2\n\tgithub.com/fsnotify/fsnotify v1.7.0\n\tgithub.com/google/renameio v1.0.1\n\tgithub.com/google/uuid v1.6.0\n\tgithub.com/opencontainers/selinux v1.11.1\n\tgithub.com/prometheus/procfs v0.15.1\n\tgithub.com/sirupsen/logrus v1.9.3\n\tgithub.com/stretchr/testify v1.10.0\n\tgithub.com/urfave/cli/v2 v2.27.5\n\tgolang.org/x/mod v0.22.0\n\tgoogle.golang.org/grpc v1.65.0\n\tk8s.io/api v0.31.3\n\tk8s.io/apimachinery v0.31.3\n\tk8s.io/client-go v0.31.3\n\tk8s.io/klog/v2 v2.130.1\n\tk8s.io/kubelet v0.31.3\n\tk8s.io/mount-utils v0.31.3\n\tsigs.k8s.io/node-feature-discovery v0.16.6\n\tsigs.k8s.io/node-feature-discovery/api/nfd v0.16.6\n\tsigs.k8s.io/yaml v1.4.0\n\ttags.cncf.io/container-device-interface v0.8.0\n\ttags.cncf.io/container-device-interface/specs-go v0.8.0\n)\n\nrequire (\n\tgithub.com/coreos/go-systemd/v22 v22.5.0 // indirect\n\tgithub.com/cpuguy83/go-md2man/v2 v2.0.5 // indirect\n\tgithub.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc // indirect\n\tgithub.com/emicklei/go-restful/v3 v3.11.3 // indirect\n\tgithub.com/fxamacker/cbor/v2 v2.7.0 // indirect\n\tgithub.com/go-logr/logr v1.4.2 // indirect\n\tgithub.com/go-openapi/jsonpointer v0.20.2 // indirect\n\tgithub.com/go-openapi/jsonreference v0.20.4 // indirect\n\tgithub.com/go-openapi/swag v0.22.9 // indirect\n\tgithub.com/godbus/dbus/v5 v5.1.0 // indirect\n\tgithub.com/gogo/protobuf v1.3.2 // indirect\n\tgithub.com/golang/protobuf v1.5.4 // indirect\n\tgithub.com/google/gnostic-models v0.6.8 // indirect\n\tgithub.com/google/go-cmp v0.6.0 // indirect\n\tgithub.com/google/gofuzz v1.2.0 // indirect\n\tgithub.com/imdario/mergo v0.3.16 // indirect\n\tgithub.com/josharian/intern v1.0.0 // indirect\n\tgithub.com/json-iterator/go v1.1.12 // indirect\n\tgithub.com/mailru/easyjson v0.7.7 // indirect\n\tgithub.com/moby/sys/mountinfo v0.7.1 // indirect\n\tgithub.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd // indirect\n\tgithub.com/modern-go/reflect2 v1.0.2 // indirect\n\tgithub.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect\n\tgithub.com/opencontainers/runc v1.1.14 // indirect\n\tgithub.com/opencontainers/runtime-spec v1.2.0 // indirect\n\tgithub.com/opencontainers/runtime-tools v0.9.1-0.20221107090550-2e043c6bd626 // indirect\n\tgithub.com/pmezard/go-difflib v1.0.1-0.20181226105442-5d4384ee4fb2 // indirect\n\tgithub.com/russross/blackfriday/v2 v2.1.0 // indirect\n\tgithub.com/spf13/pflag v1.0.5 // indirect\n\tgithub.com/syndtr/gocapability v0.0.0-20200815063812-42c35b437635 // indirect\n\tgithub.com/x448/float16 v0.8.4 // indirect\n\tgithub.com/xrash/smetrics v0.0.0-20240521201337-686a1a2994c1 // indirect\n\tgolang.org/x/net v0.34.0 // indirect\n\tgolang.org/x/oauth2 v0.21.0 // indirect\n\tgolang.org/x/sys v0.29.0 // indirect\n\tgolang.org/x/term v0.28.0 // indirect\n\tgolang.org/x/text v0.21.0 // indirect\n\tgolang.org/x/time v0.5.0 // indirect\n\tgoogle.golang.org/genproto/googleapis/rpc v0.0.0-20240701130421-f6361c86f094 // indirect\n\tgoogle.golang.org/protobuf v1.34.2 // indirect\n\tgopkg.in/inf.v0 v0.9.1 // indirect\n\tgopkg.in/yaml.v2 v2.4.0 // indirect\n\tgopkg.in/yaml.v3 v3.0.1 // indirect\n\tk8s.io/kube-openapi v0.0.0-20240228011516-70dd3763d340 // indirect\n\tk8s.io/utils v0.0.0-20240711033017-18e509b52bc8 // indirect\n\tsigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd // indirect\n\tsigs.k8s.io/structured-merge-diff/v4 v4.4.1 // indirect\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 21.1357421875,
          "content": "github.com/NVIDIA/go-gpuallocator v0.5.0 h1:166ICvPv2dU9oZ2J3kJ4y3XdbGCi6LhXgFZJtrqeu3A=\ngithub.com/NVIDIA/go-gpuallocator v0.5.0/go.mod h1:zos5bTIN01hpQioOyu9oRKglrznImMQvm0bZllMmckw=\ngithub.com/NVIDIA/go-nvlib v0.7.0 h1:Z/J7skMdLbTiHvomKVsGYsttfQMZj5FwNYIFXhZ4i/c=\ngithub.com/NVIDIA/go-nvlib v0.7.0/go.mod h1:9UrsLGx/q1OrENygXjOuM5Ey5KCtiZhbvBlbUIxtGWY=\ngithub.com/NVIDIA/go-nvml v0.12.4-0 h1:4tkbB3pT1O77JGr0gQ6uD8FrsUPqP1A/EOEm2wI1TUg=\ngithub.com/NVIDIA/go-nvml v0.12.4-0/go.mod h1:8Llmj+1Rr+9VGGwZuRer5N/aCjxGuR5nPb/9ebBiIEQ=\ngithub.com/NVIDIA/nvidia-container-toolkit v1.17.2 h1:iE6PK9SQH3HyDrOolu27xn3CJgURR3bDtnbfFrxdML8=\ngithub.com/NVIDIA/nvidia-container-toolkit v1.17.2/go.mod h1:R6bNf6ca0IjjACa0ncKGvsrx6zSjsgz8QkFyBDk5szU=\ngithub.com/blang/semver/v4 v4.0.0 h1:1PFHFE6yCCTv8C1TeyNNarDzntLi7wMI5i/pzqYIsAM=\ngithub.com/blang/semver/v4 v4.0.0/go.mod h1:IbckMUScFkM3pff0VJDNKRiT6TG/YpiHIM2yvyW5YoQ=\ngithub.com/coreos/go-systemd/v22 v22.5.0 h1:RrqgGjYQKalulkV8NGVIfkXQf6YYmOyiJKk8iXXhfZs=\ngithub.com/coreos/go-systemd/v22 v22.5.0/go.mod h1:Y58oyj3AT4RCenI/lSvhwexgC+NSVTIJ3seZv2GcEnc=\ngithub.com/cpuguy83/go-md2man/v2 v2.0.5 h1:ZtcqGrnekaHpVLArFSe4HK5DoKx1T0rq2DwVB0alcyc=\ngithub.com/cpuguy83/go-md2man/v2 v2.0.5/go.mod h1:tgQtvFlXSQOSOSIRvRPT7W67SCa46tRHOmNcaadrF8o=\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc h1:U9qPSI2PIWSS1VwoXQT9A3Wy9MM3WgvqSxFWenqJduM=\ngithub.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/emicklei/go-restful/v3 v3.11.3 h1:yagOQz/38xJmcNeZJtrUcKjkHRltIaIFXKWeG1SkWGE=\ngithub.com/emicklei/go-restful/v3 v3.11.3/go.mod h1:6n3XBCmQQb25CM2LCACGz8ukIrRry+4bhvbpWn3mrbc=\ngithub.com/fsnotify/fsnotify v1.7.0 h1:8JEhPFa5W2WU7YfeZzPNqzMP6Lwt7L2715Ggo0nosvA=\ngithub.com/fsnotify/fsnotify v1.7.0/go.mod h1:40Bi/Hjc2AVfZrqy+aj+yEI+/bRxZnMJyTJwOpGvigM=\ngithub.com/fxamacker/cbor/v2 v2.7.0 h1:iM5WgngdRBanHcxugY4JySA0nk1wZorNOpTgCMedv5E=\ngithub.com/fxamacker/cbor/v2 v2.7.0/go.mod h1:pxXPTn3joSm21Gbwsv0w9OSA2y1HFR9qXEeXQVeNoDQ=\ngithub.com/go-logr/logr v1.4.2 h1:6pFjapn8bFcIbiKo3XT4j/BhANplGihG6tvd+8rYgrY=\ngithub.com/go-logr/logr v1.4.2/go.mod h1:9T104GzyrTigFIr8wt5mBrctHMim0Nb2HLGrmQ40KvY=\ngithub.com/go-openapi/jsonpointer v0.20.2 h1:mQc3nmndL8ZBzStEo3JYF8wzmeWffDH4VbXz58sAx6Q=\ngithub.com/go-openapi/jsonpointer v0.20.2/go.mod h1:bHen+N0u1KEO3YlmqOjTT9Adn1RfD91Ar825/PuiRVs=\ngithub.com/go-openapi/jsonreference v0.20.4 h1:bKlDxQxQJgwpUSgOENiMPzCTBVuc7vTdXSSgNeAhojU=\ngithub.com/go-openapi/jsonreference v0.20.4/go.mod h1:5pZJyJP2MnYCpoeoMAql78cCHauHj0V9Lhc506VOpw4=\ngithub.com/go-openapi/swag v0.22.9 h1:XX2DssF+mQKM2DHsbgZK74y/zj4mo9I99+89xUmuZCE=\ngithub.com/go-openapi/swag v0.22.9/go.mod h1:3/OXnFfnMAwBD099SwYRk7GD3xOrr1iL7d/XNLXVVwE=\ngithub.com/go-task/slim-sprig/v3 v3.0.0 h1:sUs3vkvUymDpBKi3qH1YSqBQk9+9D/8M2mN1vB6EwHI=\ngithub.com/go-task/slim-sprig/v3 v3.0.0/go.mod h1:W848ghGpv3Qj3dhTPRyJypKRiqCdHZiAzKg9hl15HA8=\ngithub.com/godbus/dbus/v5 v5.0.4/go.mod h1:xhWf0FNVPg57R7Z0UbKHbJfkEywrmjJnf7w5xrFpKfA=\ngithub.com/godbus/dbus/v5 v5.1.0 h1:4KLkAxT3aOY8Li4FRJe/KvhoNFFxo0m6fNuFUO8QJUk=\ngithub.com/godbus/dbus/v5 v5.1.0/go.mod h1:xhWf0FNVPg57R7Z0UbKHbJfkEywrmjJnf7w5xrFpKfA=\ngithub.com/gogo/protobuf v1.3.2 h1:Ov1cvc58UF3b5XjBnZv7+opcTcQFZebYjWzi34vdm4Q=\ngithub.com/gogo/protobuf v1.3.2/go.mod h1:P1XiOD3dCwIKUDQYPy72D8LYyHL2YPYrpS2s69NZV8Q=\ngithub.com/golang/protobuf v1.5.4 h1:i7eJL8qZTpSEXOPTxNKhASYpMn+8e5Q6AdndVa1dWek=\ngithub.com/golang/protobuf v1.5.4/go.mod h1:lnTiLA8Wa4RWRcIUkrtSVa5nRhsEGBg48fD6rSs7xps=\ngithub.com/google/gnostic-models v0.6.8 h1:yo/ABAfM5IMRsS1VnXjTBvUb61tFIHozhlYvRgGre9I=\ngithub.com/google/gnostic-models v0.6.8/go.mod h1:5n7qKqH0f5wFt+aWF8CW6pZLLNOfYuF5OpfBSENuI8U=\ngithub.com/google/go-cmp v0.5.9/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=\ngithub.com/google/go-cmp v0.6.0 h1:ofyhxvXcZhMsU5ulbFiLKl/XBFqE1GSq7atu8tAmTRI=\ngithub.com/google/go-cmp v0.6.0/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=\ngithub.com/google/gofuzz v1.0.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=\ngithub.com/google/gofuzz v1.2.0 h1:xRy4A+RhZaiKjJ1bPfwQ8sedCA+YS2YcCHW6ec7JMi0=\ngithub.com/google/gofuzz v1.2.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=\ngithub.com/google/pprof v0.0.0-20240525223248-4bfdf5a9a2af h1:kmjWCqn2qkEml422C2Rrd27c3VGxi6a/6HNq8QmHRKM=\ngithub.com/google/pprof v0.0.0-20240525223248-4bfdf5a9a2af/go.mod h1:K1liHPHnj73Fdn/EKuT8nrFqBihUSKXoLYU0BuatOYo=\ngithub.com/google/renameio v1.0.1 h1:Lh/jXZmvZxb0BBeSY5VKEfidcbcbenKjZFzM/q0fSeU=\ngithub.com/google/renameio v1.0.1/go.mod h1:t/HQoYBZSsWSNK35C6CO/TpPLDVWvxOHboWUAweKUpk=\ngithub.com/google/uuid v1.3.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/google/uuid v1.6.0 h1:NIvaJDMOsjHA8n1jAhLSgzrAzy1Hgr+hNrb57e+94F0=\ngithub.com/google/uuid v1.6.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/hashicorp/errwrap v1.0.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=\ngithub.com/hashicorp/errwrap v1.1.0 h1:OxrOeh75EUXMY8TBjag2fzXGZ40LB6IKw45YeGUDY2I=\ngithub.com/hashicorp/errwrap v1.1.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=\ngithub.com/hashicorp/go-multierror v1.1.1 h1:H5DkEtf6CXdFp0N0Em5UCwQpXMWke8IA0+lD48awMYo=\ngithub.com/hashicorp/go-multierror v1.1.1/go.mod h1:iw975J/qwKPdAO1clOe2L8331t/9/fmwbPZ6JB6eMoM=\ngithub.com/imdario/mergo v0.3.16 h1:wwQJbIsHYGMUyLSPrEq1CT16AhnhNJQ51+4fdHUnCl4=\ngithub.com/imdario/mergo v0.3.16/go.mod h1:WBLT9ZmE3lPoWsEzCh9LPo3TiwVN+ZKEjmz+hD27ysY=\ngithub.com/josharian/intern v1.0.0 h1:vlS4z54oSdjm0bgjRigI+G1HpF+tI+9rE5LLzOg8HmY=\ngithub.com/josharian/intern v1.0.0/go.mod h1:5DoeVV0s6jJacbCEi61lwdGj/aVlrQvzHFFd8Hwg//Y=\ngithub.com/json-iterator/go v1.1.12 h1:PV8peI4a0ysnczrg+LtxykD8LfKY9ML6u2jnxaEnrnM=\ngithub.com/json-iterator/go v1.1.12/go.mod h1:e30LSqwooZae/UwlEbR2852Gd8hjQvJoHmT4TnhNGBo=\ngithub.com/kisielk/errcheck v1.5.0/go.mod h1:pFxgyoBC7bSaBwPgfKdkLd5X25qrDl4LWUI2bnpBCr8=\ngithub.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO+MFFWcvkIS/tQcRk01m1F5IRFswLeQ+oQHNcck=\ngithub.com/kr/pretty v0.3.1 h1:flRD4NNwYAUpkphVc1HcthR4KEIFJ65n8Mw5qdRn3LE=\ngithub.com/kr/pretty v0.3.1/go.mod h1:hoEshYVHaxMs3cyo3Yncou5ZscifuDolrwPKZanG3xk=\ngithub.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=\ngithub.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=\ngithub.com/mailru/easyjson v0.7.7 h1:UGYAvKxe3sBsEDzO8ZeWOSlIQfWFlxbzLZe7hwFURr0=\ngithub.com/mailru/easyjson v0.7.7/go.mod h1:xzfreul335JAWq5oZzymOObrkdz5UnU4kGfJJLY9Nlc=\ngithub.com/mndrix/tap-go v0.0.0-20171203230836-629fa407e90b/go.mod h1:pzzDgJWZ34fGzaAZGFW22KVZDfyrYW+QABMrWnJBnSs=\ngithub.com/moby/sys/mountinfo v0.7.1 h1:/tTvQaSJRr2FshkhXiIpux6fQ2Zvc4j7tAhMTStAG2g=\ngithub.com/moby/sys/mountinfo v0.7.1/go.mod h1:IJb6JQeOklcdMU9F5xQ8ZALD+CUr5VlGpwtX+VE0rpI=\ngithub.com/modern-go/concurrent v0.0.0-20180228061459-e0a39a4cb421/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=\ngithub.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd h1:TRLaZ9cD/w8PVh93nsPXa1VrQ6jlwL5oN8l14QlcNfg=\ngithub.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=\ngithub.com/modern-go/reflect2 v1.0.2 h1:xBagoLtFs94CBntxluKeaWgTMpvLxC4ur3nMaC9Gz0M=\ngithub.com/modern-go/reflect2 v1.0.2/go.mod h1:yWuevngMOJpCy52FWWMvUC8ws7m/LJsjYzDa0/r8luk=\ngithub.com/mrunalp/fileutils v0.5.0/go.mod h1:M1WthSahJixYnrXQl/DFQuteStB1weuxD2QJNHXfbSQ=\ngithub.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 h1:C3w9PqII01/Oq1c1nUAm88MOHcQC9l5mIlSMApZMrHA=\ngithub.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822/go.mod h1:+n7T8mK8HuQTcFwEeznm/DIxMOiR9yIdICNftLE1DvQ=\ngithub.com/onsi/ginkgo/v2 v2.19.0 h1:9Cnnf7UHo57Hy3k6/m5k3dRfGTMXGvxhHFvkDTCTpvA=\ngithub.com/onsi/ginkgo/v2 v2.19.0/go.mod h1:rlwLi9PilAFJ8jCg9UE1QP6VBpd6/xj3SRC0d6TU0To=\ngithub.com/onsi/gomega v1.33.1 h1:dsYjIxxSR755MDmKVsaFQTE22ChNBcuuTWgkUDSubOk=\ngithub.com/onsi/gomega v1.33.1/go.mod h1:U4R44UsT+9eLIaYRB2a5qajjtQYn0hauxvRm16AVYg0=\ngithub.com/opencontainers/runc v1.1.14 h1:rgSuzbmgz5DUJjeSnw337TxDbRuqjs6iqQck/2weR6w=\ngithub.com/opencontainers/runc v1.1.14/go.mod h1:E4C2z+7BxR7GHXp0hAY53mek+x49X1LjPNeMTfRGvOA=\ngithub.com/opencontainers/runtime-spec v1.0.3-0.20220825212826-86290f6a00fb/go.mod h1:jwyrGlmzljRJv/Fgzds9SsS/C5hL+LL3ko9hs6T5lQ0=\ngithub.com/opencontainers/runtime-spec v1.2.0 h1:z97+pHb3uELt/yiAWD691HNHQIF07bE7dzrbT927iTk=\ngithub.com/opencontainers/runtime-spec v1.2.0/go.mod h1:jwyrGlmzljRJv/Fgzds9SsS/C5hL+LL3ko9hs6T5lQ0=\ngithub.com/opencontainers/runtime-tools v0.9.1-0.20221107090550-2e043c6bd626 h1:DmNGcqH3WDbV5k8OJ+esPWbqUOX5rMLR2PMvziDMJi0=\ngithub.com/opencontainers/runtime-tools v0.9.1-0.20221107090550-2e043c6bd626/go.mod h1:BRHJJd0E+cx42OybVYSgUvZmU0B8P9gZuRXlZUP7TKI=\ngithub.com/opencontainers/selinux v1.9.1/go.mod h1:2i0OySw99QjzBBQByd1Gr9gSjvuho1lHsJxIJ3gGbJI=\ngithub.com/opencontainers/selinux v1.11.1 h1:nHFvthhM0qY8/m+vfhJylliSshm8G1jJ2jDMcgULaH8=\ngithub.com/opencontainers/selinux v1.11.1/go.mod h1:E5dMC3VPuVvVHDYmi78qvhJp8+M586T4DlDRYpFkyec=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/pmezard/go-difflib v1.0.1-0.20181226105442-5d4384ee4fb2 h1:Jamvg5psRIccs7FGNTlIRMkT8wgtp5eCXdBlqhYGL6U=\ngithub.com/pmezard/go-difflib v1.0.1-0.20181226105442-5d4384ee4fb2/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/prometheus/procfs v0.15.1 h1:YagwOFzUgYfKKHX6Dr+sHT7km/hxC76UB0learggepc=\ngithub.com/prometheus/procfs v0.15.1/go.mod h1:fB45yRUv8NstnjriLhBQLuOUt+WW4BsoGhij/e3PBqk=\ngithub.com/rogpeppe/go-internal v1.12.0 h1:exVL4IDcn6na9z1rAb56Vxr+CgyK3nn3O+epU5NdKM8=\ngithub.com/rogpeppe/go-internal v1.12.0/go.mod h1:E+RYuTGaKKdloAfM02xzb0FW3Paa99yedzYV+kq4uf4=\ngithub.com/russross/blackfriday/v2 v2.1.0 h1:JIOH55/0cWyOuilr9/qlrm0BSXldqnqwMsf35Ld67mk=\ngithub.com/russross/blackfriday/v2 v2.1.0/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=\ngithub.com/sirupsen/logrus v1.8.1/go.mod h1:yWOB1SBYBC5VeMP7gHvWumXLIWorT60ONWic61uBYv0=\ngithub.com/sirupsen/logrus v1.9.3 h1:dueUQJ1C2q9oE3F7wvmSGAaVtTmUizReu6fjN8uqzbQ=\ngithub.com/sirupsen/logrus v1.9.3/go.mod h1:naHLuLoDiP4jHNo9R0sCBMtWGeIprob74mVsIT4qYEQ=\ngithub.com/spf13/pflag v1.0.5 h1:iy+VFUOCP1a+8yFto/drg2CJ5u0yRoB7fZw3DKv/JXA=\ngithub.com/spf13/pflag v1.0.5/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=\ngithub.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\ngithub.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.10.0 h1:Xv5erBjTwe/5IxqUQTdXv5kgmIvbHo3QQyRwhJsOfJA=\ngithub.com/stretchr/testify v1.10.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=\ngithub.com/syndtr/gocapability v0.0.0-20200815063812-42c35b437635 h1:kdXcSzyDtseVEc4yCz2qF8ZrQvIDBJLl4S1c3GCXmoI=\ngithub.com/syndtr/gocapability v0.0.0-20200815063812-42c35b437635/go.mod h1:hkRG7XYTFWNJGYcbNJQlaLq0fg1yr4J4t/NcTQtrfww=\ngithub.com/urfave/cli v1.19.1/go.mod h1:70zkFmudgCuE/ngEzBv17Jvp/497gISqfk5gWijbERA=\ngithub.com/urfave/cli/v2 v2.27.5 h1:WoHEJLdsXr6dDWoJgMq/CboDmyY/8HMMH1fTECbih+w=\ngithub.com/urfave/cli/v2 v2.27.5/go.mod h1:3Sevf16NykTbInEnD0yKkjDAeZDS0A6bzhBH5hrMvTQ=\ngithub.com/x448/float16 v0.8.4 h1:qLwI1I70+NjRFUR3zs1JPUCgaCXSh3SW62uAKT1mSBM=\ngithub.com/x448/float16 v0.8.4/go.mod h1:14CWIYCyZA/cWjXOioeEpHeN/83MdbZDRQHoFcYsOfg=\ngithub.com/xeipuuv/gojsonpointer v0.0.0-20180127040702-4e3ac2762d5f/go.mod h1:N2zxlSyiKSe5eX1tZViRH5QA0qijqEDrYZiPEAiq3wU=\ngithub.com/xeipuuv/gojsonpointer v0.0.0-20190905194746-02993c407bfb h1:zGWFAtiMcyryUHoUjUJX0/lt1H2+i2Ka2n+D3DImSNo=\ngithub.com/xeipuuv/gojsonpointer v0.0.0-20190905194746-02993c407bfb/go.mod h1:N2zxlSyiKSe5eX1tZViRH5QA0qijqEDrYZiPEAiq3wU=\ngithub.com/xeipuuv/gojsonreference v0.0.0-20180127040603-bd5ef7bd5415 h1:EzJWgHovont7NscjpAxXsDA8S8BMYve8Y5+7cuRE7R0=\ngithub.com/xeipuuv/gojsonreference v0.0.0-20180127040603-bd5ef7bd5415/go.mod h1:GwrjFmJcFw6At/Gs6z4yjiIwzuJ1/+UwLxMQDVQXShQ=\ngithub.com/xeipuuv/gojsonschema v1.2.0 h1:LhYJRs+L4fBtjZUfuSZIKGeVu0QRy8e5Xi7D17UxZ74=\ngithub.com/xeipuuv/gojsonschema v1.2.0/go.mod h1:anYRn/JVcOK2ZgGU+IjEV4nwlhoK5sQluxsYJ78Id3Y=\ngithub.com/xrash/smetrics v0.0.0-20240521201337-686a1a2994c1 h1:gEOO8jv9F4OT7lGCjxCBTO/36wtF6j2nSip77qHd4x4=\ngithub.com/xrash/smetrics v0.0.0-20240521201337-686a1a2994c1/go.mod h1:Ohn+xnUBiLI6FVj/9LpzZWtj1/D6lUovWYBkxHVV3aM=\ngithub.com/yuin/goldmark v1.1.27/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\ngithub.com/yuin/goldmark v1.2.1/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\ngolang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\ngolang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\ngolang.org/x/exp v0.0.0-20240112132812-db7319d0e0e3 h1:hNQpMuAJe5CtcUqCXaWga3FHu+kQvCqcsoVaQgSV60o=\ngolang.org/x/exp v0.0.0-20240112132812-db7319d0e0e3/go.mod h1:idGWGoKP1toJGkd5/ig9ZLuPcZBC3ewk7SzmH0uou08=\ngolang.org/x/mod v0.2.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/mod v0.3.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/mod v0.22.0 h1:D4nJWe9zXqHOmWqj4VMOJhvzj7bEZg4wEYa759z1pH4=\ngolang.org/x/mod v0.22.0/go.mod h1:6SkKJ3Xj0I0BrPOZoBy3bdMptDDU9oJrpohJ3eWZ1fY=\ngolang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200226121028-0de0cce0169b/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20201021035429-f5854403a974/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=\ngolang.org/x/net v0.34.0 h1:Mb7Mrk043xzHgnRM88suvJFwzVrRfHEHJEl5/71CKw0=\ngolang.org/x/net v0.34.0/go.mod h1:di0qlW3YNM5oh6GqDGQr92MyTozJPmybPK4Ev/Gm31k=\ngolang.org/x/oauth2 v0.21.0 h1:tsimM75w1tF/uws5rbeHzIWxEqElMehnc+iW793zsZs=\ngolang.org/x/oauth2 v0.21.0/go.mod h1:XYTD2NtWslqkgxebSiOHnXEap4TF09sJSc7H1sXbhtI=\ngolang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190911185100-cd5d95a43a6e/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20201020160332-67f06af15bc9/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191026070338-33540a1f6037/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191115151921-52ab43148777/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200930185726-fdedc70b468f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20220520151302-bc2c85ada10a/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220715151400-c0bba94af5f8/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.29.0 h1:TPYlXGxvx1MGTn2GiZDhnjPA9wZzZeGKHHmKhHYvgaU=\ngolang.org/x/sys v0.29.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\ngolang.org/x/term v0.28.0 h1:/Ts8HFuMR2E6IP/jlo7QVLZHggjKQbhu/7H0LJFr3Gg=\ngolang.org/x/term v0.28.0/go.mod h1:Sw/lC2IAUZ92udQNf3WodGtn4k/XoLyZoh8v/8uiwek=\ngolang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.21.0 h1:zyQAAkrwaneQ066sspRyJaG9VNi/YJ1NfzcGB3hZ/qo=\ngolang.org/x/text v0.21.0/go.mod h1:4IBbMaMmOPCJ8SecivzSH54+73PCFmPWxNTLm+vZkEQ=\ngolang.org/x/time v0.5.0 h1:o7cqy6amK/52YcAKIPlM3a+Fpj35zvRj2TP+e1xFSfk=\ngolang.org/x/time v0.5.0/go.mod h1:3BpzKBy/shNhVucY/MWOyx10tF3SFh9QdLuxbVysPQM=\ngolang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20200619180055-7c47624df98f/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20210106214847-113979e3529a/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=\ngolang.org/x/tools v0.21.1-0.20240508182429-e35e4ccd0d2d h1:vU5i/LfpvrRCpgM/VPfJLg5KjxD3E+hfT1SH+d9zLwg=\ngolang.org/x/tools v0.21.1-0.20240508182429-e35e4ccd0d2d/go.mod h1:aiJjzUbINMkxbQROHiO6hDPo2LHcIPhhQsa9DLh0yGk=\ngolang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191011141410-1b5146add898/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngoogle.golang.org/genproto/googleapis/rpc v0.0.0-20240701130421-f6361c86f094 h1:BwIjyKYGsK9dMCBOorzRri8MQwmi7mT9rGHsCEinZkA=\ngoogle.golang.org/genproto/googleapis/rpc v0.0.0-20240701130421-f6361c86f094/go.mod h1:Ue6ibwXGpU+dqIcODieyLOcgj7z8+IcskoNIgZxtrFY=\ngoogle.golang.org/grpc v1.65.0 h1:bs/cUb4lp1G5iImFFd3u5ixQzweKizoZJAwBNLR42lc=\ngoogle.golang.org/grpc v1.65.0/go.mod h1:WgYC2ypjlB0EiQi6wdKixMqukr6lBc0Vo+oOgjrM5ZQ=\ngoogle.golang.org/protobuf v1.34.2 h1:6xV6lTsCfpGD21XK49h7MhtcApnLqkfYgPcdHftf6hg=\ngoogle.golang.org/protobuf v1.34.2/go.mod h1:qYOHts0dSfpeUzUFpOMr/WGzszTmLH+DiWniOlNbLDw=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=\ngopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=\ngopkg.in/inf.v0 v0.9.1 h1:73M5CoZyi3ZLMOyDlQh031Cx6N9NDJ2Vvfl76EDAgDc=\ngopkg.in/inf.v0 v0.9.1/go.mod h1:cWUDdTG/fYaXco+Dcufb5Vnc6Gp2YChqWtbxRZE0mXw=\ngopkg.in/yaml.v2 v2.2.8/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.4.0 h1:D8xgwECY7CYvx+Y2n4sBz93Jn9JRvxdiyyo8CTfuKaY=\ngopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=\ngopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\nk8s.io/api v0.31.3 h1:umzm5o8lFbdN/hIXbrK9oRpOproJO62CV1zqxXrLgk8=\nk8s.io/api v0.31.3/go.mod h1:UJrkIp9pnMOI9K2nlL6vwpxRzzEX5sWgn8kGQe92kCE=\nk8s.io/apimachinery v0.31.3 h1:6l0WhcYgasZ/wk9ktLq5vLaoXJJr5ts6lkaQzgeYPq4=\nk8s.io/apimachinery v0.31.3/go.mod h1:rsPdaZJfTfLsNJSQzNHQvYoTmxhoOEofxtOsF3rtsMo=\nk8s.io/client-go v0.31.3 h1:CAlZuM+PH2cm+86LOBemaJI/lQ5linJ6UFxKX/SoG+4=\nk8s.io/client-go v0.31.3/go.mod h1:2CgjPUTpv3fE5dNygAr2NcM8nhHzXvxB8KL5gYc3kJs=\nk8s.io/klog/v2 v2.130.1 h1:n9Xl7H1Xvksem4KFG4PYbdQCQxqc/tTUyrgXaOhHSzk=\nk8s.io/klog/v2 v2.130.1/go.mod h1:3Jpz1GvMt720eyJH1ckRHK1EDfpxISzJ7I9OYgaDtPE=\nk8s.io/kube-openapi v0.0.0-20240228011516-70dd3763d340 h1:BZqlfIlq5YbRMFko6/PM7FjZpUb45WallggurYhKGag=\nk8s.io/kube-openapi v0.0.0-20240228011516-70dd3763d340/go.mod h1:yD4MZYeKMBwQKVht279WycxKyM84kkAx2DPrTXaeb98=\nk8s.io/kubelet v0.31.3 h1:DIXRAmvVGp42mV2vpA1GCLU6oO8who0/vp3Oq6kSpbI=\nk8s.io/kubelet v0.31.3/go.mod h1:KSdbEfNy5VzqUlAHlytA/fH12s+sE1u8fb/8JY9sL/8=\nk8s.io/mount-utils v0.31.3 h1:CANy3prUYvvDCc2X7ZKgpjpDhAidx4gjGh/WwDrCPq8=\nk8s.io/mount-utils v0.31.3/go.mod h1:HV/VYBUGqYUj4vt82YltzpWvgv8FPg0G9ItyInT3NPU=\nk8s.io/utils v0.0.0-20240711033017-18e509b52bc8 h1:pUdcCO1Lk/tbT5ztQWOBi5HBgbBP1J8+AsQnQCKsi8A=\nk8s.io/utils v0.0.0-20240711033017-18e509b52bc8/go.mod h1:OLgZIPagt7ERELqWJFomSt595RzquPNLL48iOWgYOg0=\nsigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd h1:EDPBXCAspyGV4jQlpZSudPeMmr1bNJefnuqLsRAsHZo=\nsigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd/go.mod h1:B8JuhiUyNFVKdsE8h686QcCxMaH6HrOAZj4vswFpcB0=\nsigs.k8s.io/node-feature-discovery v0.16.6 h1:G+n0gStHnLojsLVLgP/ArsC2gwP1fkw7TrHr0MJ4x8k=\nsigs.k8s.io/node-feature-discovery v0.16.6/go.mod h1:7qS1l8P6UdfLa/ec8VneMvINbdWbuhTuuQr1wZ7/Jbg=\nsigs.k8s.io/node-feature-discovery/api/nfd v0.16.6 h1:07AwnEocbzPG98VIwkskfac6fVwzumZMGt9ePyAa60I=\nsigs.k8s.io/node-feature-discovery/api/nfd v0.16.6/go.mod h1:p8V8+A+enKGj31HFeBSpuTjfe6WUdIG0nl0Yze/d/+Q=\nsigs.k8s.io/structured-merge-diff/v4 v4.4.1 h1:150L+0vs/8DA78h1u02ooW1/fFq/Lwr+sGiqlzvrtq4=\nsigs.k8s.io/structured-merge-diff/v4 v4.4.1/go.mod h1:N8hJocpFajUSSeSJ9bOZ77VzejKZaXsTtZo4/u7Io08=\nsigs.k8s.io/yaml v1.4.0 h1:Mk1wCc2gy/F0THH0TAp1QYyJNzRm2KCLy3o5ASXVI5E=\nsigs.k8s.io/yaml v1.4.0/go.mod h1:Ejl7/uTz7PSA4eKMyQCUTnhZYNmLIl+5c2lQPGR2BPY=\ntags.cncf.io/container-device-interface v0.8.0 h1:8bCFo/g9WODjWx3m6EYl3GfUG31eKJbaggyBDxEldRc=\ntags.cncf.io/container-device-interface v0.8.0/go.mod h1:Apb7N4VdILW0EVdEMRYXIDVRZfNJZ+kmEUss2kRRQ6Y=\ntags.cncf.io/container-device-interface/specs-go v0.8.0 h1:QYGFzGxvYK/ZLMrjhvY0RjpUavIn4KcmRmVP/JjdBTA=\ntags.cncf.io/container-device-interface/specs-go v0.8.0/go.mod h1:BhJIkjjPh4qpys+qm4DAYtUyryaTDg9zris+AczXyws=\n"
        },
        {
          "name": "hack",
          "type": "tree",
          "content": null
        },
        {
          "name": "internal",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "vendor",
          "type": "tree",
          "content": null
        },
        {
          "name": "versions.mk",
          "type": "blob",
          "size": 1.0546875,
          "content": "# Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nDRIVER_NAME := k8s-device-plugin\nMODULE := github.com/NVIDIA/$(DRIVER_NAME)\n\nREGISTRY ?= nvcr.io/nvidia\n\nVERSION ?= v0.17.0\n\n# vVERSION represents the version with a guaranteed v-prefix\nvVERSION := v$(VERSION:v%=%)\n\nGOLANG_VERSION := $(shell ./hack/golang-version.sh)\n\nBUILDIMAGE_TAG ?= devel-go$(GOLANG_VERSION)\nBUILDIMAGE ?=  $(DRIVER_NAME):$(BUILDIMAGE_TAG)\n\nGIT_COMMIT ?= $(shell git describe --match=\"\" --dirty --long --always --abbrev=40 2> /dev/null || echo \"\")\n"
        }
      ]
    }
  ]
}