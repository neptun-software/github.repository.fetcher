{
  "metadata": {
    "timestamp": 1736567248635,
    "page": 846,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjg1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "lovoo/goka",
      "stars": 2375,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.037109375,
          "content": "tmp*\n*.*~\n.tags*\nvendor\n.vscode\n.idea\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 21.517578125,
          "content": "# Changelog\n\n## [Unreleased](https://github.com/lovoo/goka/tree/HEAD)\n\n[Full Changelog](https://github.com/lovoo/goka/compare/v0.9.0-beta3...HEAD)\n\n**Implemented enhancements:**\n\n- Migration guide [\\#249](https://github.com/lovoo/goka/issues/249)\n- View: expose view and connection state [\\#248](https://github.com/lovoo/goka/issues/248)\n- PartitionTable backoff on errors [\\#247](https://github.com/lovoo/goka/issues/247)\n\n**Closed issues:**\n\n- Examples `simplest` and `clicks` panic with \"non-positive interval for NewTicker\" [\\#258](https://github.com/lovoo/goka/issues/258)\n- panic: non-positive interval for NewTicker [\\#255](https://github.com/lovoo/goka/issues/255)\n\n**Merged pull requests:**\n\n- fixed broken goka blogpost link [\\#257](https://github.com/lovoo/goka/pull/257) ([frairon](https://github.com/frairon))\n\n## [v0.9.0-beta3](https://github.com/lovoo/goka/tree/v0.9.0-beta3) (2020-04-09)\n\n[Full Changelog](https://github.com/lovoo/goka/compare/v0.9.0-beta2...v0.9.0-beta3)\n\n**Implemented enhancements:**\n\n- Add blocking recovered method [\\#80](https://github.com/lovoo/goka/issues/80)\n- Emit messages with headers [\\#77](https://github.com/lovoo/goka/issues/77)\n\n**Closed issues:**\n\n- cannot run multiple consumers in the same group [\\#204](https://github.com/lovoo/goka/issues/204)\n\n**Merged pull requests:**\n\n- stats-tracking improved [\\#245](https://github.com/lovoo/goka/pull/245) ([frairon](https://github.com/frairon))\n- return trackOutput if stats are nil [\\#244](https://github.com/lovoo/goka/pull/244) ([R053NR07](https://github.com/R053NR07))\n- added lots of godoc, fixed many linter errors, added Open call when câ€¦ [\\#243](https://github.com/lovoo/goka/pull/243) ([frairon](https://github.com/frairon))\n- Open Storage in PartitionTable when performing Setup [\\#242](https://github.com/lovoo/goka/pull/242) ([frairon](https://github.com/frairon))\n- updated readme for configuration, added changelog [\\#240](https://github.com/lovoo/goka/pull/240) ([frairon](https://github.com/frairon))\n\n## [v0.9.0-beta2](https://github.com/lovoo/goka/tree/v0.9.0-beta2) (2020-03-20)\n\n[Full Changelog](https://github.com/lovoo/goka/compare/v0.9.0-beta1...v0.9.0-beta2)\n\n## [v0.9.0-beta1](https://github.com/lovoo/goka/tree/v0.9.0-beta1) (2020-03-19)\n\n[Full Changelog](https://github.com/lovoo/goka/compare/v0.1.4...v0.9.0-beta1)\n\n**Implemented enhancements:**\n\n- Update Kafka consumer group library [\\#187](https://github.com/lovoo/goka/issues/187)\n\n**Closed issues:**\n\n- Does goka support kafka 2.2.1 [\\#233](https://github.com/lovoo/goka/issues/233)\n- Go get error [\\#213](https://github.com/lovoo/goka/issues/213)\n- Samara-cluster dependency seems to have an issue [\\#211](https://github.com/lovoo/goka/issues/211)\n- Dropping message while rebalancing [\\#207](https://github.com/lovoo/goka/issues/207)\n- Lost lots of messages due to async produce [\\#192](https://github.com/lovoo/goka/issues/192)\n- processor gets stuck in shutdown after error [\\#176](https://github.com/lovoo/goka/issues/176)\n- Proposal: support static partitioning [\\#163](https://github.com/lovoo/goka/issues/163)\n- Proposal: add tracing support [\\#160](https://github.com/lovoo/goka/issues/160)\n\n## [v0.1.4](https://github.com/lovoo/goka/tree/v0.1.4) (2020-01-22)\n\n[Full Changelog](https://github.com/lovoo/goka/compare/v0.1.3...v0.1.4)\n\n**Closed issues:**\n\n- How can I debug a \"negative WaitGroup counter\" in a processor? [\\#198](https://github.com/lovoo/goka/issues/198)\n- View is empty, if log forced the keys/values come back [\\#197](https://github.com/lovoo/goka/issues/197)\n- Question: Rebalancing callback? [\\#195](https://github.com/lovoo/goka/issues/195)\n\n**Merged pull requests:**\n\n- Remove unused channels [\\#215](https://github.com/lovoo/goka/pull/215) ([heltonmarx](https://github.com/heltonmarx))\n- Adding header on message [\\#214](https://github.com/lovoo/goka/pull/214) ([toninho09](https://github.com/toninho09))\n- repeating messages from input stream after recover [\\#208](https://github.com/lovoo/goka/pull/208) ([WideLee](https://github.com/WideLee))\n- revert in memory locks [\\#203](https://github.com/lovoo/goka/pull/203) ([TheL1ne](https://github.com/TheL1ne))\n- protect storage writes [\\#200](https://github.com/lovoo/goka/pull/200) ([TheL1ne](https://github.com/TheL1ne))\n- Tester tolerate unconsumed queues [\\#199](https://github.com/lovoo/goka/pull/199) ([TheL1ne](https://github.com/TheL1ne))\n\n## [v0.1.3](https://github.com/lovoo/goka/tree/v0.1.3) (2019-06-28)\n\n[Full Changelog](https://github.com/lovoo/goka/compare/v0.1.2...v0.1.3)\n\n**Fixed bugs:**\n\n- Using a tester with an emitter doesn't seem to be working [\\#188](https://github.com/lovoo/goka/issues/188)\n\n**Merged pull requests:**\n\n- feature 195: Add a rebalance callback to the processor. [\\#196](https://github.com/lovoo/goka/pull/196) ([gmather](https://github.com/gmather))\n\n## [v0.1.2](https://github.com/lovoo/goka/tree/v0.1.2) (2019-06-18)\n\n[Full Changelog](https://github.com/lovoo/goka/compare/v0.1.1...v0.1.2)\n\n**Implemented enhancements:**\n\n- Proposal: log progress of recovery [\\#182](https://github.com/lovoo/goka/issues/182)\n- start logging while recovering [\\#183](https://github.com/lovoo/goka/pull/183) ([TheL1ne](https://github.com/TheL1ne))\n\n**Closed issues:**\n\n- Lookup not working with multi-partitioned topics [\\#186](https://github.com/lovoo/goka/issues/186)\n- How can I config multiple consumer for 1 Topic [\\#184](https://github.com/lovoo/goka/issues/184)\n- multi\\_iterator looks broken [\\#178](https://github.com/lovoo/goka/issues/178)\n- Rebalancing error [\\#172](https://github.com/lovoo/goka/issues/172)\n\n**Merged pull requests:**\n\n- bugfix \\#188: tester for emitter [\\#190](https://github.com/lovoo/goka/pull/190) ([frairon](https://github.com/frairon))\n- storage: merge iterator implementation [\\#185](https://github.com/lovoo/goka/pull/185) ([SamiHiltunen](https://github.com/SamiHiltunen))\n- 178 multi\\_iterator looks broken [\\#180](https://github.com/lovoo/goka/pull/180) ([frairon](https://github.com/frairon))\n- Groupgraphhook option [\\#175](https://github.com/lovoo/goka/pull/175) ([frairon](https://github.com/frairon))\n- Tester deadlock workaround [\\#173](https://github.com/lovoo/goka/pull/173) ([frairon](https://github.com/frairon))\n\n## [v0.1.1](https://github.com/lovoo/goka/tree/v0.1.1) (2019-01-29)\n\n[Full Changelog](https://github.com/lovoo/goka/compare/v0.1.0...v0.1.1)\n\n**Implemented enhancements:**\n\n- Improve tester implementation [\\#145](https://github.com/lovoo/goka/issues/145)\n\n**Fixed bugs:**\n\n- Updates broke go get [\\#152](https://github.com/lovoo/goka/issues/152)\n- zookeeper address with trailing \"/\" fails [\\#126](https://github.com/lovoo/goka/issues/126)\n- Running processor without prior topics fails [\\#125](https://github.com/lovoo/goka/issues/125)\n\n**Closed issues:**\n\n- How to improve performance? [\\#164](https://github.com/lovoo/goka/issues/164)\n- Question: Scanning a Joined Topic [\\#159](https://github.com/lovoo/goka/issues/159)\n- Removal of db7/kazoo-go and subsequent repo deletion breaks previous builds [\\#154](https://github.com/lovoo/goka/issues/154)\n- error on recovering view does not restart [\\#141](https://github.com/lovoo/goka/issues/141)\n- review errors to fail on [\\#140](https://github.com/lovoo/goka/issues/140)\n- Processor failures: expected rebalance OK but received rebalance error [\\#131](https://github.com/lovoo/goka/issues/131)\n- unable to get goka running with the following error.  [\\#119](https://github.com/lovoo/goka/issues/119)\n\n**Merged pull requests:**\n\n- fixed bindata for web templates [\\#170](https://github.com/lovoo/goka/pull/170) ([frairon](https://github.com/frairon))\n- add logo [\\#169](https://github.com/lovoo/goka/pull/169) ([frairon](https://github.com/frairon))\n- add Context\\(\\) method to goka.Context [\\#166](https://github.com/lovoo/goka/pull/166) ([db7](https://github.com/db7))\n- terminate tester when one of the queue consumers died [\\#165](https://github.com/lovoo/goka/pull/165) ([frairon](https://github.com/frairon))\n- fix integration tests [\\#162](https://github.com/lovoo/goka/pull/162) ([frairon](https://github.com/frairon))\n- fixes wrong commit for queue tracker [\\#158](https://github.com/lovoo/goka/pull/158) ([frairon](https://github.com/frairon))\n- remove proto and go-metrics direct dependencies [\\#157](https://github.com/lovoo/goka/pull/157) ([db7](https://github.com/db7))\n- add ensure config to check topic [\\#155](https://github.com/lovoo/goka/pull/155) ([db7](https://github.com/db7))\n- clean trailing slashes in zookeeper chroot [\\#153](https://github.com/lovoo/goka/pull/153) ([db7](https://github.com/db7))\n- update mocks to latest sarama [\\#150](https://github.com/lovoo/goka/pull/150) ([db7](https://github.com/db7))\n- ensure topic exists as more flexible topic creation option [\\#149](https://github.com/lovoo/goka/pull/149) ([db7](https://github.com/db7))\n- fix processor tests [\\#148](https://github.com/lovoo/goka/pull/148) ([db7](https://github.com/db7))\n- cleanup processor rebalance [\\#147](https://github.com/lovoo/goka/pull/147) ([db7](https://github.com/db7))\n- improve tester implementation [\\#146](https://github.com/lovoo/goka/pull/146) ([frairon](https://github.com/frairon))\n- use default kazoo-go [\\#144](https://github.com/lovoo/goka/pull/144) ([db7](https://github.com/db7))\n\n## [v0.1.0](https://github.com/lovoo/goka/tree/v0.1.0) (2018-08-03)\n\n[Full Changelog](https://github.com/lovoo/goka/compare/4dd3c74e427f580a08953ad5cb704dc4421a6426...v0.1.0)\n\n**Implemented enhancements:**\n\n- Proposal: replace Start\\(\\) and Stop\\(\\) with Run\\(context.Context\\) [\\#120](https://github.com/lovoo/goka/issues/120)\n- Support badgeDB [\\#55](https://github.com/lovoo/goka/issues/55)\n- Question: Accessing a goka.View concurrently? [\\#53](https://github.com/lovoo/goka/issues/53)\n- Remove snapshot from storage [\\#30](https://github.com/lovoo/goka/issues/30)\n\n**Fixed bugs:**\n\n- Prevent offset commit in case of panic [\\#129](https://github.com/lovoo/goka/issues/129)\n- Failing view was not added [\\#113](https://github.com/lovoo/goka/issues/113)\n- Don't check copartitioning for lookup topics [\\#109](https://github.com/lovoo/goka/issues/109)\n- Getting error from Emitter [\\#103](https://github.com/lovoo/goka/issues/103)\n- Initial offset handling [\\#97](https://github.com/lovoo/goka/issues/97)\n- Allow passing custom hasher for graph dependencies [\\#84](https://github.com/lovoo/goka/issues/84)\n- monitoring panics on stateless processor [\\#62](https://github.com/lovoo/goka/issues/62)\n- a view never becomes ready on an empty topic [\\#35](https://github.com/lovoo/goka/issues/35)\n- access group table without goka.Persist\\(...\\) [\\#18](https://github.com/lovoo/goka/issues/18)\n\n**Closed issues:**\n\n- View should have option to fallback to local-only mode [\\#102](https://github.com/lovoo/goka/issues/102)\n- Question: Same topic as input and join [\\#95](https://github.com/lovoo/goka/issues/95)\n- Proposal: Add joins with watermarks [\\#94](https://github.com/lovoo/goka/issues/94)\n- Goka is production ready? [\\#93](https://github.com/lovoo/goka/issues/93)\n- Issues getting value from view [\\#83](https://github.com/lovoo/goka/issues/83)\n- HowToX : Is it possible to iterate on View with a subset ?  [\\#82](https://github.com/lovoo/goka/issues/82)\n- Proposal: Implement message keys as bytes instead of strings [\\#81](https://github.com/lovoo/goka/issues/81)\n- Provide examples of lookups [\\#79](https://github.com/lovoo/goka/issues/79)\n- goka does not work latest bsm/sarama-cluster [\\#68](https://github.com/lovoo/goka/issues/68)\n- Reduce metrics overhead [\\#57](https://github.com/lovoo/goka/issues/57)\n- panic: runtime error: invalid memory address or nil pointer dereference [\\#49](https://github.com/lovoo/goka/issues/49)\n- Consume nil messages should be an option [\\#41](https://github.com/lovoo/goka/issues/41)\n- Organizing/modeling Views [\\#32](https://github.com/lovoo/goka/issues/32)\n- print error on failure right away [\\#10](https://github.com/lovoo/goka/issues/10)\n- panic on error. like a double-close [\\#2](https://github.com/lovoo/goka/issues/2)\n\n**Merged pull requests:**\n\n- Fix passing opts to LevelDB in BuilderWithOptions [\\#139](https://github.com/lovoo/goka/pull/139) ([mhaitjema](https://github.com/mhaitjema))\n- Expose partition and offset on the callback context [\\#136](https://github.com/lovoo/goka/pull/136) ([burdiyan](https://github.com/burdiyan))\n- typo fix in tester code [\\#135](https://github.com/lovoo/goka/pull/135) ([frairon](https://github.com/frairon))\n- bugfix tester, improve emitter and mock [\\#134](https://github.com/lovoo/goka/pull/134) ([frairon](https://github.com/frairon))\n- make iteration order deterministic [\\#133](https://github.com/lovoo/goka/pull/133) ([db7](https://github.com/db7))\n- if context panics, finish context with error [\\#130](https://github.com/lovoo/goka/pull/130) ([db7](https://github.com/db7))\n- Feature/run context [\\#127](https://github.com/lovoo/goka/pull/127) ([db7](https://github.com/db7))\n- fixing multi-iterator test cases [\\#122](https://github.com/lovoo/goka/pull/122) ([db7](https://github.com/db7))\n- fix only first two iters checked [\\#121](https://github.com/lovoo/goka/pull/121) ([j0hnsmith](https://github.com/j0hnsmith))\n- Feature/move kafkamock [\\#118](https://github.com/lovoo/goka/pull/118) ([db7](https://github.com/db7))\n- trying with go 1.9 [\\#117](https://github.com/lovoo/goka/pull/117) ([db7](https://github.com/db7))\n- Update mocks [\\#116](https://github.com/lovoo/goka/pull/116) ([db7](https://github.com/db7))\n- Adding TopicManagerBuilderWithConfig to allow custom Sarama config [\\#115](https://github.com/lovoo/goka/pull/115) ([andrewmunro](https://github.com/andrewmunro))\n- mostly complying with gometalinter [\\#114](https://github.com/lovoo/goka/pull/114) ([db7](https://github.com/db7))\n- Feature/errgroup [\\#111](https://github.com/lovoo/goka/pull/111) ([db7](https://github.com/db7))\n- Allow non-copartitioned lookup tables [\\#110](https://github.com/lovoo/goka/pull/110) ([db7](https://github.com/db7))\n- guarantee partition goroutines finished before stop\\(\\) returns [\\#107](https://github.com/lovoo/goka/pull/107) ([db7](https://github.com/db7))\n- fix embarrasing dead unlock [\\#106](https://github.com/lovoo/goka/pull/106) ([db7](https://github.com/db7))\n- add restartable view support [\\#105](https://github.com/lovoo/goka/pull/105) ([db7](https://github.com/db7))\n- bugfix \\#103: propagate error in Emitter.EmitSync [\\#104](https://github.com/lovoo/goka/pull/104) ([frairon](https://github.com/frairon))\n- consume streams from newest [\\#100](https://github.com/lovoo/goka/pull/100) ([db7](https://github.com/db7))\n- Bugfix/fix stalled partitions [\\#98](https://github.com/lovoo/goka/pull/98) ([db7](https://github.com/db7))\n- added redis storage option [\\#96](https://github.com/lovoo/goka/pull/96) ([heltonmarx](https://github.com/heltonmarx))\n- Make builders builders [\\#92](https://github.com/lovoo/goka/pull/92) ([db7](https://github.com/db7))\n- forward options to lookup views [\\#91](https://github.com/lovoo/goka/pull/91) ([db7](https://github.com/db7))\n- finished monitoring-example readme [\\#90](https://github.com/lovoo/goka/pull/90) ([frairon](https://github.com/frairon))\n- add testcase for regression testing of already fixed issue 18. Resolvâ€¦ [\\#89](https://github.com/lovoo/goka/pull/89) ([frairon](https://github.com/frairon))\n- messaging example [\\#88](https://github.com/lovoo/goka/pull/88) ([db7](https://github.com/db7))\n- added support for Seek on iterator and IteratorWithRange on storage [\\#85](https://github.com/lovoo/goka/pull/85) ([jbpin](https://github.com/jbpin))\n- Bugfix/stalled partition recovery [\\#78](https://github.com/lovoo/goka/pull/78) ([frairon](https://github.com/frairon))\n- initial time in stats [\\#76](https://github.com/lovoo/goka/pull/76) ([db7](https://github.com/db7))\n- stop partition while marking recovered [\\#75](https://github.com/lovoo/goka/pull/75) ([db7](https://github.com/db7))\n- Confluent kafka go [\\#74](https://github.com/lovoo/goka/pull/74) ([db7](https://github.com/db7))\n- use default channel size from sarama in kafka package [\\#73](https://github.com/lovoo/goka/pull/73) ([db7](https://github.com/db7))\n- Join web views [\\#71](https://github.com/lovoo/goka/pull/71) ([frairon](https://github.com/frairon))\n- support for new notifications in sarama-cluster [\\#70](https://github.com/lovoo/goka/pull/70) ([SamiHiltunen](https://github.com/SamiHiltunen))\n- speed up stats collection [\\#69](https://github.com/lovoo/goka/pull/69) ([db7](https://github.com/db7))\n- use AsyncClose in simple consumer [\\#67](https://github.com/lovoo/goka/pull/67) ([db7](https://github.com/db7))\n- stop-partition bugfix [\\#66](https://github.com/lovoo/goka/pull/66) ([frairon](https://github.com/frairon))\n- return stats immediately when partition marking storage recovered [\\#65](https://github.com/lovoo/goka/pull/65) ([db7](https://github.com/db7))\n- forward partition consumer errors [\\#64](https://github.com/lovoo/goka/pull/64) ([SamiHiltunen](https://github.com/SamiHiltunen))\n- web/monitoring: removed tableName in template which fails on statelesss processors and was unused. Fixes \\#62 [\\#63](https://github.com/lovoo/goka/pull/63) ([frairon](https://github.com/frairon))\n- reset stats after load finished [\\#61](https://github.com/lovoo/goka/pull/61) ([db7](https://github.com/db7))\n- web-components regenerated, refresh-interval reduced [\\#60](https://github.com/lovoo/goka/pull/60) ([frairon](https://github.com/frairon))\n- Webinterface [\\#59](https://github.com/lovoo/goka/pull/59) ([frairon](https://github.com/frairon))\n- Lightweight partition stats [\\#58](https://github.com/lovoo/goka/pull/58) ([db7](https://github.com/db7))\n- change table suffix to -table [\\#54](https://github.com/lovoo/goka/pull/54) ([db7](https://github.com/db7))\n- removing codec from storage [\\#52](https://github.com/lovoo/goka/pull/52) ([db7](https://github.com/db7))\n- deletes partition consumer from map even on errors [\\#51](https://github.com/lovoo/goka/pull/51) ([db7](https://github.com/db7))\n- nil pointer panic when closing producer  [\\#50](https://github.com/lovoo/goka/pull/50) ([db7](https://github.com/db7))\n- disable partition consumption delay once done [\\#47](https://github.com/lovoo/goka/pull/47) ([db7](https://github.com/db7))\n- timestamp passing in simple consumer [\\#46](https://github.com/lovoo/goka/pull/46) ([SamiHiltunen](https://github.com/SamiHiltunen))\n- nil handling options [\\#45](https://github.com/lovoo/goka/pull/45) ([db7](https://github.com/db7))\n- methods for checking recovery status [\\#44](https://github.com/lovoo/goka/pull/44) ([SamiHiltunen](https://github.com/SamiHiltunen))\n- properly stop goroutines on Close\\(\\) [\\#43](https://github.com/lovoo/goka/pull/43) ([db7](https://github.com/db7))\n- Evict method for Views [\\#42](https://github.com/lovoo/goka/pull/42) ([SamiHiltunen](https://github.com/SamiHiltunen))\n- return after deleting in DefaultUpdate [\\#40](https://github.com/lovoo/goka/pull/40) ([SamiHiltunen](https://github.com/SamiHiltunen))\n- Ignore nil message [\\#39](https://github.com/lovoo/goka/pull/39) ([db7](https://github.com/db7))\n- deletion semantics [\\#38](https://github.com/lovoo/goka/pull/38) ([SamiHiltunen](https://github.com/SamiHiltunen))\n- passing client id to builders [\\#37](https://github.com/lovoo/goka/pull/37) ([db7](https://github.com/db7))\n- let producerBuilder set partitioner in sarama.Config [\\#36](https://github.com/lovoo/goka/pull/36) ([db7](https://github.com/db7))\n- simplify context and move set offset to context commit [\\#34](https://github.com/lovoo/goka/pull/34) ([db7](https://github.com/db7))\n- view iterator [\\#33](https://github.com/lovoo/goka/pull/33) ([SamiHiltunen](https://github.com/SamiHiltunen))\n- snapshot and batching removal, recovery transactions [\\#31](https://github.com/lovoo/goka/pull/31) ([SamiHiltunen](https://github.com/SamiHiltunen))\n- iterator offset key skipping [\\#29](https://github.com/lovoo/goka/pull/29) ([SamiHiltunen](https://github.com/SamiHiltunen))\n- Expose message timestamp and metrics improvements [\\#28](https://github.com/lovoo/goka/pull/28) ([SamiHiltunen](https://github.com/SamiHiltunen))\n- option to provide metrics registry [\\#27](https://github.com/lovoo/goka/pull/27) ([SamiHiltunen](https://github.com/SamiHiltunen))\n- Bugfix/shutdown on errors [\\#26](https://github.com/lovoo/goka/pull/26) ([frairon](https://github.com/frairon))\n- logger interface and options [\\#25](https://github.com/lovoo/goka/pull/25) ([SamiHiltunen](https://github.com/SamiHiltunen))\n- examples [\\#24](https://github.com/lovoo/goka/pull/24) ([frairon](https://github.com/frairon))\n- Readme update [\\#23](https://github.com/lovoo/goka/pull/23) ([db7](https://github.com/db7))\n- Dependency fixes [\\#22](https://github.com/lovoo/goka/pull/22) ([SamiHiltunen](https://github.com/SamiHiltunen))\n- Readme [\\#19](https://github.com/lovoo/goka/pull/19) ([frairon](https://github.com/frairon))\n- isolate concurrent gets in snapshot [\\#17](https://github.com/lovoo/goka/pull/17) ([db7](https://github.com/db7))\n- Fail in partition view [\\#16](https://github.com/lovoo/goka/pull/16) ([db7](https://github.com/db7))\n- expect emit in correct order. Added expectall-function [\\#15](https://github.com/lovoo/goka/pull/15) ([frairon](https://github.com/frairon))\n- inputs for multiple topics [\\#14](https://github.com/lovoo/goka/pull/14) ([frairon](https://github.com/frairon))\n- renaming producer to emitter [\\#13](https://github.com/lovoo/goka/pull/13) ([db7](https://github.com/db7))\n- String types for Table, Stream and Group [\\#12](https://github.com/lovoo/goka/pull/12) ([db7](https://github.com/db7))\n- Sync recover [\\#11](https://github.com/lovoo/goka/pull/11) ([db7](https://github.com/db7))\n- adding lookup tables [\\#9](https://github.com/lovoo/goka/pull/9) ([db7](https://github.com/db7))\n\n\n\n\\* *This Changelog was automatically generated by [github_changelog_generator](https://github.com/github-changelog-generator/github-changelog-generator)*\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.4521484375,
          "content": "Copyright (c) 2017 LOVOO GmbH\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "MIGRATION.md",
          "type": "blob",
          "size": 2.1328125,
          "content": "This document sums up issues for migrating between 0.1.4 to 0.9.x\n\n## restartable/auto reconnect view.Run()\n\nIn 0.1.4, if a view was created to be restartable, it returned from `Run` in case of errors, but allowed\nto be restarted calling `Run` again. The view was still usable, even if it was not running and receiving updates.\n\nThe behavior of that option has changed in 0.9.x in a way that the `view.Run` does the reconnect internally using configurable backoff.\nThe Option was also renamed (the old version has been kept for compatibility reasons).\n\n```go\n\n// create a view\nview014, _ := NewView(..., WithViewRestartable())\n\n// reconnect logic in 0.1.4\ngo func(){\n  for running{\n    err:= view014.Run(context.Background())\n    // handle error\n\n    // sleep or simple backoff logic\n    time.Sleep(time.Second)\n  }\n}()\n\n// After migration:\n// create a view\nview09x, _ := NewView(..., WithViewAutoReconnect())\nctx, cancel := context.WithCancel(context.Background())\n\n// no need for reconnect logic, it's handled by the view internally\ngo func(){\n  err:= view09x.Run(ctx)\n  // handle shutdown error\n}()\n\n// stop view\ncancel()\n\n```\n\n\n## Offset bug in local storage\nIn 0.1.4 there was a bug that caused the table offset being stored in the local cache always be +1 compared the actual offset stored in kafka.\nA second bug kind of evened it out so it never was an issue. \n\nFrom 0.9.x, both bugs are fixed. However, if you upgrade goka and restart a processor using the same cache files that were maintained by the old version you'll see a warning like this\n```\nError: local offset is higher than partition offset. topic some-topic, partition 0, hwm 1312, local offset 1314. This can have several reasons: \n(1) The kafka topic storing the table is gone --> delete the local cache and restart! \n(2) the processor crashed last time while writing to disk. \n(3) You found a bug!\n```\nThis is because goka sees an offset that it is not expecting. \nYou should see this error only once per partition and processor. The offset will be fixed automatically. If it appears on every start or regularily, it might actually a bug or some error and should be further investigated\n(or reported to goka :))."
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.138671875,
          "content": "\n\ntest:\n\tgo test -race ./...\n\ntest-systemtest:\n\tGOKA_SYSTEMTEST=y go test -v github.com/lovoo/goka/systemtest\n\ntest-all: test test-systemtest\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 8.0791015625,
          "content": "# Goka \n[![License](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause) [![Unit Tests/System Tests](https://github.com/lovoo/goka/actions/workflows/main.yml/badge.svg)](https://github.com/lovoo/goka/actions/workflows/main.yml)\n [![GoDoc](https://godoc.org/github.com/lovoo/goka?status.svg)](https://godoc.org/github.com/lovoo/goka) [![Go Report Card](https://goreportcard.com/badge/github.com/lovoo/goka)](https://goreportcard.com/report/github.com/lovoo/goka)\n\nGoka is a compact yet powerful distributed stream processing library for [Apache Kafka] written in Go. Goka aims to reduce the complexity of building highly scalable and highly available microservices.\n\nGoka extends the concept of Kafka consumer groups by binding a state table to them and persisting them in Kafka. Goka provides sane defaults and a pluggable architecture.\n\n## Features\n  * **Message Input and Output**\n\n    Goka handles all the message input and output for you. You only have to provide one or more callback functions that handle messages from any of the Kafka topics you are interested in. You only ever have to deal with deserialized messages.\n\n  * **Scaling**\n\n    Goka automatically distributes the processing and state across multiple instances of a service. This enables effortless scaling when the load increases.\n\n  * **Fault Tolerance**\n\n    In case of a failure, Goka will redistribute the failed instance's workload and state across the remaining healthy instances. All state is safely stored in Kafka and messages delivered with *at-least-once* semantics.\n\n  * **Built-in Monitoring and Introspection**\n\n    Goka provides a web interface for monitoring performance and querying values in the state.\n\n  * **Modularity**\n\n    Goka fosters a pluggable architecture which enables you to replace for example the storage layer or the Kafka communication layer.\n\n## Documentation\n\nThis README provides a brief, high level overview of the ideas behind Goka.\n\nPackage API documentation is available at [GoDoc] and the [Wiki](https://github.com/lovoo/goka/wiki/Tips#configuring-log-compaction-for-table-topics) provides several tips for configuring, extending, and deploying Goka applications.\n\n## Installation\n\nYou can install Goka by running the following command:\n\n``$ go get -u github.com/lovoo/goka``\n\n## Configuration\n\nGoka relies on [Sarama](https://github.com/IBM/sarama) to perform the actual communication with Kafka, which offers many configuration settings. The config is documented [here](https://godoc.org/github.com/IBM/sarama#Config).\n\nIn most cases, you need to modify the config, e.g. to set the Kafka Version.\n\n```go\ncfg := goka.DefaultConfig()\ncfg.Version = sarama.V2_4_0_0\ngoka.ReplaceGlobalConfig(cfg)\n```\n\nThis makes all goka components use the updated config.\n\nIf you do need specific configuration for different components, you need to pass customized builders to the \ncomponent's constructor, e.g.\n\n```go\ncfg := goka.DefaultConfig()\n// modify the config with component-specific settings\n\n\n// use the config by creating a builder which allows to override global config\ngoka.NewProcessor(// ...,\n\tgoka.WithConsumerGroupBuilder(\n\t\tgoka.ConsumerGroupBuilderWithConfig(cfg),\n\t),\n\t// ...\n)\n```\n\n## Concepts\n\nGoka relies on Kafka for message passing, fault-tolerant state storage and workload partitioning.\n\n* **Emitters** deliver key-value messages into Kafka. As an example, an emitter could be a database handler emitting the state changes into Kafka for other interested applications to consume.\n\n* **Processor** is a set of callback functions that consume and perform state transformations upon delivery of these emitted messages. *Processor groups* are formed of one or more instances of a processor. Goka distributes the partitions of the input topics across all processor instances in a processor group. This enables effortless scaling and fault-tolerance. If a processor instance fails, its partitions and state are reassigned to the remaining healthy members of the processor group. Processors can also emit further messages into Kafka.\n\n* **Group table** is the state of a processor group. It is a partitioned key-value table stored in Kafka that belongs to a single processor group. If a processor instance fails, the remaining instances will take over the group table partitions of the failed instance recovering them from Kafka.\n\n* **Views** are local caches of a complete group table. Views provide read-only access to the group tables and can be used to provide external services for example through a gRPC interface.\n\n* **Local storage** keeps a local copy of the group table partitions to speedup recovery and reduce memory utilization. By default, the local storage uses [LevelDB](https://github.com/syndtr/goleveldb), but in-memory map and [Redis-based storage](https://github.com/lovoo/goka/tree/master/storage/redis) are also available.\n\n\n## Get Started\n\nAn example Goka application could look like the following.\nAn emitter emits a single message with key \"some-key\" and value \"some-value\" into the \"example-stream\" topic.\nA processor processes the \"example-stream\" topic counting the number of messages delivered for \"some-key\".\nThe counter is persisted in the \"example-group-table\" topic.\nTo locally start a dockerized Zookeeper and Kafka instances, execute `make start` with the `Makefile` in the [examples] folder.\n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"log\"\n\t\"os\"\n\t\"os/signal\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/lovoo/goka\"\n\t\"github.com/lovoo/goka/codec\"\n)\n\nvar (\n\tbrokers             = []string{\"localhost:9092\"}\n\ttopic   goka.Stream = \"example-stream\"\n\tgroup   goka.Group  = \"example-group\"\n)\n\n// Emit messages forever every second\nfunc runEmitter() {\n\temitter, err := goka.NewEmitter(brokers, topic, new(codec.String))\n\tif err != nil {\n\t\tlog.Fatalf(\"error creating emitter: %v\", err)\n\t}\n\tdefer emitter.Finish()\n\tfor {\n\t\ttime.Sleep(1 * time.Second)\n\t\terr = emitter.EmitSync(\"some-key\", \"some-value\")\n\t\tif err != nil {\n\t\t\tlog.Fatalf(\"error emitting message: %v\", err)\n\t\t}\n\t}\n}\n\n// process messages until ctrl-c is pressed\nfunc runProcessor() {\n\t// process callback is invoked for each message delivered from\n\t// \"example-stream\" topic.\n\tcb := func(ctx goka.Context, msg interface{}) {\n\t\tvar counter int64\n\t\t// ctx.Value() gets from the group table the value that is stored for\n\t\t// the message's key.\n\t\tif val := ctx.Value(); val != nil {\n\t\t\tcounter = val.(int64)\n\t\t}\n\t\tcounter++\n\t\t// SetValue stores the incremented counter in the group table for in\n\t\t// the message's key.\n\t\tctx.SetValue(counter)\n\t\tlog.Printf(\"key = %s, counter = %v, msg = %v\", ctx.Key(), counter, msg)\n\t}\n\n\t// Define a new processor group. The group defines all inputs, outputs, and\n\t// serialization formats. The group-table topic is \"example-group-table\".\n\tg := goka.DefineGroup(group,\n\t\tgoka.Input(topic, new(codec.String), cb),\n\t\tgoka.Persist(new(codec.Int64)),\n\t)\n\n\tp, err := goka.NewProcessor(brokers, g)\n\tif err != nil {\n\t\tlog.Fatalf(\"error creating processor: %v\", err)\n\t}\n\tctx, cancel := context.WithCancel(context.Background())\n\tdone := make(chan bool)\n\tgo func() {\n\t\tdefer close(done)\n\t\tif err = p.Run(ctx); err != nil {\n\t\t\tlog.Fatalf(\"error running processor: %v\", err)\n\t\t} else {\n\t\t\tlog.Printf(\"Processor shutdown cleanly\")\n\t\t}\n\t}()\n\n\twait := make(chan os.Signal, 1)\n\tsignal.Notify(wait, syscall.SIGINT, syscall.SIGTERM)\n\t<-wait   // wait for SIGINT/SIGTERM\n\tcancel() // gracefully stop processor\n\t<-done\n}\n\nfunc main() {\n\tgo runEmitter() // emits one message every second forever\n\trunProcessor()  // press ctrl-c to stop\n}\n\n```\nA very similar example is also in *1-simplest*. Just run `go run examples/1-simplest/main.go`.\n\nNote that tables have to be configured in Kafka with log compaction.\nFor details check the [Wiki](https://github.com/lovoo/goka/wiki/Tips#configuring-log-compaction-for-table-topics).\n\n## How to contribute\n\nContributions are always welcome.\nPlease fork the repo, create a pull request against master, and be sure tests pass.\nSee the [GitHub Flow] for details.\n\n[Apache Kafka]: https://kafka.apache.org/\n[GoDoc]: https://godoc.org/github.com/lovoo/goka\n[examples]: https://github.com/lovoo/goka/tree/master/examples\n[GitHub Flow]: https://guides.github.com/introduction/flow\n"
        },
        {
          "name": "assignment.go",
          "type": "blob",
          "size": 0.2529296875,
          "content": "package goka\n\nimport (\n\t\"fmt\"\n)\n\n// Assignment represents a partition:offset assignment for the current connection\ntype Assignment map[int32]int64\n\nfunc (a *Assignment) string() string {\n\tvar am map[int32]int64 = *a\n\treturn fmt.Sprintf(\"Assignment %v\", am)\n}\n"
        },
        {
          "name": "broker.go",
          "type": "blob",
          "size": 0.27734375,
          "content": "package goka\n\nimport \"github.com/IBM/sarama\"\n\n// Broker is an interface for the sarama broker\ntype Broker interface {\n\tAddr() string\n\tConnected() (bool, error)\n\tCreateTopics(request *sarama.CreateTopicsRequest) (*sarama.CreateTopicsResponse, error)\n\tOpen(conf *sarama.Config) error\n}\n"
        },
        {
          "name": "builders.go",
          "type": "blob",
          "size": 3.841796875,
          "content": "package goka\n\nimport (\n\t\"hash\"\n\n\t\"github.com/IBM/sarama\"\n)\n\n// ProducerBuilder create a Kafka producer.\ntype ProducerBuilder func(brokers []string, clientID string, hasher func() hash.Hash32) (Producer, error)\n\n// DefaultProducerBuilder creates a Kafka producer using the Sarama library.\nfunc DefaultProducerBuilder(brokers []string, clientID string, hasher func() hash.Hash32) (Producer, error) {\n\tconfig := globalConfig\n\tconfig.ClientID = clientID\n\tconfig.Producer.Partitioner = sarama.NewCustomHashPartitioner(hasher)\n\treturn NewProducer(brokers, &config)\n}\n\n// ProducerBuilderWithConfig creates a Kafka consumer using the Sarama library.\nfunc ProducerBuilderWithConfig(config *sarama.Config) ProducerBuilder {\n\treturn func(brokers []string, clientID string, hasher func() hash.Hash32) (Producer, error) {\n\t\tconfig.ClientID = clientID\n\t\tconfig.Producer.Partitioner = sarama.NewCustomHashPartitioner(hasher)\n\t\treturn NewProducer(brokers, config)\n\t}\n}\n\n// TopicManagerBuilder creates a TopicManager to check partition counts and\n// create tables.\ntype TopicManagerBuilder func(brokers []string) (TopicManager, error)\n\n// DefaultTopicManagerBuilder creates TopicManager using the Sarama library.\nfunc DefaultTopicManagerBuilder(brokers []string) (TopicManager, error) {\n\tconfig := globalConfig\n\tconfig.ClientID = \"goka-topic-manager\"\n\treturn NewTopicManager(brokers, &config, NewTopicManagerConfig())\n}\n\n// TopicManagerBuilderWithConfig creates TopicManager using the Sarama library.\nfunc TopicManagerBuilderWithConfig(config *sarama.Config, tmConfig *TopicManagerConfig) TopicManagerBuilder {\n\treturn func(brokers []string) (TopicManager, error) {\n\t\treturn NewTopicManager(brokers, config, tmConfig)\n\t}\n}\n\n// TopicManagerBuilderWithTopicManagerConfig creates TopicManager using the Sarama library.\nfunc TopicManagerBuilderWithTopicManagerConfig(tmConfig *TopicManagerConfig) TopicManagerBuilder {\n\treturn func(brokers []string) (TopicManager, error) {\n\t\tconfig := globalConfig\n\t\tconfig.ClientID = \"goka-topic-manager\"\n\t\treturn NewTopicManager(brokers, &config, tmConfig)\n\t}\n}\n\n// ConsumerGroupBuilder creates a `sarama.ConsumerGroup`\ntype ConsumerGroupBuilder func(brokers []string, group, clientID string) (sarama.ConsumerGroup, error)\n\n// DefaultConsumerGroupBuilder creates a Kafka consumer using the Sarama library.\nfunc DefaultConsumerGroupBuilder(brokers []string, group, clientID string) (sarama.ConsumerGroup, error) {\n\tconfig := globalConfig\n\tconfig.ClientID = clientID\n\treturn sarama.NewConsumerGroup(brokers, group, &config)\n}\n\n// ConsumerGroupBuilderWithConfig creates a sarama consumergroup using passed config\nfunc ConsumerGroupBuilderWithConfig(config *sarama.Config) ConsumerGroupBuilder {\n\treturn func(brokers []string, group, clientID string) (sarama.ConsumerGroup, error) {\n\t\tconfig.ClientID = clientID\n\t\treturn sarama.NewConsumerGroup(brokers, group, config)\n\t}\n}\n\n// SaramaConsumerBuilder creates a `sarama.Consumer`\ntype SaramaConsumerBuilder func(brokers []string, clientID string) (sarama.Consumer, error)\n\n// DefaultSaramaConsumerBuilder creates a Kafka consumer using the Sarama library.\nfunc DefaultSaramaConsumerBuilder(brokers []string, clientID string) (sarama.Consumer, error) {\n\tconfig := globalConfig\n\tconfig.ClientID = clientID\n\treturn sarama.NewConsumer(brokers, &config)\n}\n\n// SaramaConsumerBuilderWithConfig creates a sarama consumer using passed config\nfunc SaramaConsumerBuilderWithConfig(config *sarama.Config) SaramaConsumerBuilder {\n\treturn func(brokers []string, clientID string) (sarama.Consumer, error) {\n\t\tconfig.ClientID = clientID\n\t\treturn sarama.NewConsumer(brokers, config)\n\t}\n}\n\n// BackoffBuilder creates a backoff\ntype BackoffBuilder func() (Backoff, error)\n\n// DefaultBackoffBuilder returnes a simpleBackoff with 10 seconds step increase and 2 minutes max wait\nfunc DefaultBackoffBuilder() (Backoff, error) {\n\treturn NewSimpleBackoff(defaultBackoffStep, defaultBackoffMax), nil\n}\n"
        },
        {
          "name": "codec.go",
          "type": "blob",
          "size": 0.1865234375,
          "content": "package goka\n\n// Codec decodes and encodes from and to []byte\ntype Codec interface {\n\tEncode(value interface{}) (data []byte, err error)\n\tDecode(data []byte) (value interface{}, err error)\n}\n"
        },
        {
          "name": "codec",
          "type": "tree",
          "content": null
        },
        {
          "name": "config.go",
          "type": "blob",
          "size": 1.69140625,
          "content": "package goka\n\nimport (\n\t\"time\"\n\n\t\"github.com/IBM/sarama\"\n)\n\nvar (\n\tglobalConfig = *DefaultConfig()\n)\n\nconst (\n\t// size of sarama buffer for consumer and producer\n\tdefaultChannelBufferSize = 256\n\n\t// time sarama-cluster assumes the processing of an event may take\n\tdefaultMaxProcessingTime = 1 * time.Second\n\n\t// producer flush configuration\n\tdefaultFlushFrequency     = 100 * time.Millisecond\n\tdefaultFlushBytes         = 64 * 1024\n\tdefaultProducerMaxRetries = 10\n)\n\n// DefaultConfig creates a new config used by goka per default\n// Use it to modify and pass to `goka.ReplaceGlobalConifg(...)` to modify\n// goka's global config\nfunc DefaultConfig() *sarama.Config {\n\tconfig := sarama.NewConfig()\n\tconfig.Version = sarama.V2_0_0_0\n\n\t// consumer configuration\n\tconfig.Consumer.Return.Errors = true\n\tconfig.Consumer.MaxProcessingTime = defaultMaxProcessingTime\n\t// this configures the initial offset for streams. Tables are always\n\t// consumed from OffsetOldest.\n\tconfig.Consumer.Offsets.Initial = sarama.OffsetNewest\n\tconfig.Consumer.Group.Rebalance.Strategy = CopartitioningStrategy\n\t// producer configuration\n\tconfig.Producer.RequiredAcks = sarama.WaitForLocal\n\tconfig.Producer.Compression = sarama.CompressionSnappy\n\tconfig.Producer.Flush.Frequency = defaultFlushFrequency\n\tconfig.Producer.Flush.Bytes = defaultFlushBytes\n\tconfig.Producer.Return.Successes = true\n\tconfig.Producer.Return.Errors = true\n\tconfig.Producer.Retry.Max = defaultProducerMaxRetries\n\treturn config\n}\n\n// ReplaceGlobalConfig registeres a standard config used during building if no\n// other config is specified\nfunc ReplaceGlobalConfig(config *sarama.Config) {\n\tif config == nil {\n\t\tpanic(\"nil config registered as global config\")\n\t}\n\tglobalConfig = *config\n}\n"
        },
        {
          "name": "config_test.go",
          "type": "blob",
          "size": 1.30859375,
          "content": "package goka\n\nimport (\n\t\"testing\"\n\n\t\"github.com/IBM/sarama\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestConfig_DefaultConfig(t *testing.T) {\n\tt.Run(\"equal\", func(t *testing.T) {\n\t\tcfg := DefaultConfig()\n\t\trequire.True(t, cfg.Version == sarama.V2_0_0_0)\n\t\trequire.True(t, cfg.Consumer.Return.Errors == true)\n\t\trequire.True(t, cfg.Consumer.MaxProcessingTime == defaultMaxProcessingTime)\n\t\trequire.True(t, cfg.Consumer.Offsets.Initial == sarama.OffsetNewest)\n\t\trequire.True(t, cfg.Producer.RequiredAcks == sarama.WaitForLocal)\n\t\trequire.True(t, cfg.Producer.Compression == sarama.CompressionSnappy)\n\t\trequire.True(t, cfg.Producer.Flush.Frequency == defaultFlushFrequency)\n\t\trequire.True(t, cfg.Producer.Flush.Bytes == defaultFlushBytes)\n\t\trequire.True(t, cfg.Producer.Return.Successes == true)\n\t\trequire.True(t, cfg.Producer.Return.Errors == true)\n\t\trequire.True(t, cfg.Producer.Retry.Max == defaultProducerMaxRetries)\n\t})\n}\n\nfunc TestConfig_ReplaceGlobalConfig(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tcustom := DefaultConfig()\n\t\tcustom.Version = sarama.V0_8_2_0\n\t\tReplaceGlobalConfig(custom)\n\t\trequire.Equal(t, custom.Version, globalConfig.Version)\n\t})\n\tt.Run(\"panic\", func(t *testing.T) {\n\t\tdefer func() {\n\t\t\tif r := recover(); r == nil {\n\t\t\t\tt.Fatal(\"there was no panic\")\n\t\t\t}\n\t\t}()\n\t\tReplaceGlobalConfig(nil)\n\t})\n}\n"
        },
        {
          "name": "context.go",
          "type": "blob",
          "size": 14.2275390625,
          "content": "package goka\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/IBM/sarama\"\n\t\"github.com/hashicorp/go-multierror\"\n)\n\ntype emitter func(topic string, key string, value []byte, headers Headers) *Promise\n\n// Context provides access to the processor's table and emit capabilities to\n// arbitrary topics in kafka.\n// Upon arrival of a message from subscribed topics, the respective\n// ConsumeCallback is invoked with a context object along with the input message.\n// The context is only valid within the callback, do not store it or pass it to other goroutines.\n//\n// # Error handling\n//\n// Most methods of the context can fail due to different reasons, which are handled in different ways:\n// Synchronous errors like\n// * wrong codec for topic (a message cannot be marshalled or unmarshalled)\n// * Emit to a topic without the Output definition in the group graph\n// * Value/SetValue without defining Persist in the group graph\n// * Join/Lookup without the definition in the group graph etc..\n// will result in a panic to stop the callback immediately and shutdown the processor.\n// This is necessary to preserve integrity of the processor and avoid further actions.\n// Do not recover from that panic, otherwise the goroutine will deadlock.\n//\n// Retrying synchronous errors must be implemented by restarting the processor.\n// If errors must be tolerated (which is not advisable because they're usually persistent), provide\n// fail-tolerant versions of the producer, storage or codec as needed.\n//\n// Asynchronous errors can occur when the callback has been finished, but e.g. sending a batched\n// message to kafka fails due to connection errors or leader election in the cluster.\n// Those errors still shutdown the processor but will not result in a panic in the callback.\ntype Context interface {\n\t// Topic returns the topic of input message.\n\tTopic() Stream\n\n\t// Key returns the key of the input message.\n\tKey() string\n\n\t// Partition returns the partition of the input message.\n\tPartition() int32\n\n\t// Offset returns the offset of the input message.\n\tOffset() int64\n\n\t// Group returns the group of the input message\n\tGroup() Group\n\n\t// Value returns the value of the key in the group table.\n\t//\n\t// This method might panic to initiate an immediate shutdown of the processor\n\t// to maintain data integrity. Do not recover from that panic or\n\t// the processor might deadlock.\n\tValue() interface{}\n\n\t// Headers returns the headers of the input message\n\tHeaders() Headers\n\n\t// SetValue updates the value of the key in the group table.\n\t// It stores the value in the local cache and sends the\n\t// update to the Kafka topic representing the group table.\n\t//\n\t// This method might panic to initiate an immediate shutdown of the processor\n\t// to maintain data integrity. Do not recover from that panic or\n\t// the processor might deadlock.\n\tSetValue(value interface{}, options ...ContextOption)\n\n\t// Delete deletes a value from the group table. IMPORTANT: this deletes the\n\t// value associated with the key from both the local cache and the persisted\n\t// table in Kafka.\n\t//\n\t// This method might panic to initiate an immediate shutdown of the processor\n\t// to maintain data integrity. Do not recover from that panic or\n\t// the processor might deadlock.\n\tDelete(options ...ContextOption)\n\n\t// Timestamp returns the timestamp of the input message. If the timestamp is\n\t// invalid, a zero time will be returned.\n\tTimestamp() time.Time\n\n\t// Join returns the value of key in the copartitioned table.\n\t//\n\t// This method might panic to initiate an immediate shutdown of the processor\n\t// to maintain data integrity. Do not recover from that panic or\n\t// the processor might deadlock.\n\tJoin(topic Table) interface{}\n\n\t// Lookup returns the value of key in the view of table.\n\t//\n\t// This method might panic to initiate an immediate shutdown of the processor\n\t// to maintain data integrity. Do not recover from that panic or\n\t// the processor might deadlock.\n\tLookup(topic Table, key string) interface{}\n\n\t// Emit asynchronously writes a message into a topic.\n\t//\n\t// This method might panic to initiate an immediate shutdown of the processor\n\t// to maintain data integrity. Do not recover from that panic or\n\t// the processor might deadlock.\n\tEmit(topic Stream, key string, value interface{}, options ...ContextOption)\n\n\t// Loopback asynchronously sends a message to another key of the group\n\t// table. Value passed to loopback is encoded via the codec given in the\n\t// Loop subscription.\n\t//\n\t// This method might panic to initiate an immediate shutdown of the processor\n\t// to maintain data integrity. Do not recover from that panic or\n\t// the processor might deadlock.\n\tLoopback(key string, value interface{}, options ...ContextOption)\n\n\t// Fail stops execution and shuts down the processor\n\t// The callback is stopped immediately by panicking. Do not recover from that panic or\n\t// the processor might deadlock.\n\tFail(err error)\n\n\t// Context returns the underlying context used to start the processor or a\n\t// subcontext. Returned context.Context can safely be passed to asynchronous code and goroutines.\n\tContext() context.Context\n\n\t// DeferCommit makes the callback omit the final commit when the callback returns.\n\t// It returns a function that *must* be called eventually to mark the message processing as finished.\n\t// If the function is not called, the processor might reprocess the message in future.\n\t// Note when calling DeferCommit multiple times, all returned functions must be called.\n\t// *Important*: the context where `DeferCommit` is called, is only safe to use within this callback,\n\t// never pass it into asynchronous code or goroutines.\n\tDeferCommit() func(error)\n}\n\ntype message struct {\n\tkey       string\n\ttimestamp time.Time\n\ttopic     string\n\toffset    int64\n\tpartition int32\n\theaders   []*sarama.RecordHeader\n\tvalue     []byte\n}\n\ntype cbContext struct {\n\tctx   context.Context\n\tgraph *GroupGraph\n\t// commit commits the message in the consumer session\n\tcommit func()\n\n\temitter               emitter\n\temitterDefaultHeaders Headers\n\n\tasyncFailer func(err error)\n\tsyncFailer  func(err error)\n\n\t// Headers as passed from sarama. Note that this field will be filled\n\t// lazily after the first call to Headers\n\theaders Headers\n\n\ttable *PartitionTable\n\t// joins\n\tpviews map[string]*PartitionTable\n\t// lookup tables\n\tviews map[string]*View\n\n\t// helper function that is provided by the partition processor to allow\n\t// tracking statistics for the output topic\n\ttrackOutputStats func(ctx context.Context, topic string, size int)\n\n\tmsg      *message\n\tdone     bool\n\tcounters struct {\n\t\temits  int\n\t\tdones  int\n\t\tstores int\n\t}\n\terrors *multierror.Error\n\tm      sync.Mutex\n\twg     *sync.WaitGroup\n}\n\n// Emit sends a message asynchronously to a topic.\nfunc (ctx *cbContext) Emit(topic Stream, key string, value interface{}, options ...ContextOption) {\n\topts := new(ctxOptions)\n\topts.applyOptions(options...)\n\tif topic == \"\" {\n\t\tctx.Fail(errors.New(\"cannot emit to empty topic\"))\n\t}\n\tif loopName(ctx.graph.Group()) == string(topic) {\n\t\tctx.Fail(errors.New(\"cannot emit to loop topic (use Loopback instead)\"))\n\t}\n\tif tableName(ctx.graph.Group()) == string(topic) {\n\t\tctx.Fail(errors.New(\"cannot emit to table topic (use SetValue instead)\"))\n\t}\n\tif !ctx.graph.isOutputTopic(topic) {\n\t\tctx.Fail(fmt.Errorf(\"topic %s is not configured for output. Did you specify goka.Output(..) when defining the processor?\", topic))\n\t}\n\n\tc := ctx.graph.codec(string(topic))\n\tif c == nil {\n\t\tctx.Fail(fmt.Errorf(\"no codec for topic %s\", topic))\n\t}\n\n\tvar data []byte\n\tif value != nil {\n\t\tvar err error\n\t\tdata, err = c.Encode(value)\n\t\tif err != nil {\n\t\t\tctx.Fail(fmt.Errorf(\"error encoding message for topic %s: %v\", topic, err))\n\t\t}\n\t}\n\n\tctx.emit(string(topic), key, data, opts.emitHeaders)\n}\n\n// Loopback sends a message to another key of the processor.\nfunc (ctx *cbContext) Loopback(key string, value interface{}, options ...ContextOption) {\n\topts := new(ctxOptions)\n\topts.applyOptions(options...)\n\tl := ctx.graph.LoopStream()\n\tif l == nil {\n\t\tctx.Fail(errors.New(\"no loop topic configured\"))\n\t}\n\n\tdata, err := l.Codec().Encode(value)\n\tif err != nil {\n\t\tctx.Fail(fmt.Errorf(\"error encoding message for key %s: %v\", key, err))\n\t}\n\n\tctx.emit(l.Topic(), key, data, opts.emitHeaders)\n}\n\nfunc (ctx *cbContext) emit(topic string, key string, value []byte, headers Headers) {\n\tctx.counters.emits++\n\tctx.emitter(topic, key, value, ctx.emitterDefaultHeaders.Merged(headers)).Then(func(err error) {\n\t\tif err != nil {\n\t\t\terr = fmt.Errorf(\"error emitting to %s: %v\", topic, err)\n\t\t}\n\t\tctx.emitDone(err)\n\t})\n\tctx.trackOutputStats(ctx.ctx, topic, len(value))\n}\n\nfunc (ctx *cbContext) Delete(options ...ContextOption) {\n\topts := new(ctxOptions)\n\topts.applyOptions(options...)\n\tif err := ctx.deleteKey(ctx.Key(), opts.emitHeaders); err != nil {\n\t\tctx.Fail(err)\n\t}\n}\n\n// Value returns the value of the key in the group table.\nfunc (ctx *cbContext) Value() interface{} {\n\tval, err := ctx.valueForKey(ctx.Key())\n\tif err != nil {\n\t\tctx.Fail(err)\n\t}\n\treturn val\n}\n\n// SetValue updates the value of the key in the group table.\nfunc (ctx *cbContext) SetValue(value interface{}, options ...ContextOption) {\n\topts := new(ctxOptions)\n\topts.applyOptions(options...)\n\tif err := ctx.setValueForKey(ctx.Key(), value, opts.emitHeaders); err != nil {\n\t\tctx.Fail(err)\n\t}\n}\n\n// Timestamp returns the timestamp of the input message.\nfunc (ctx *cbContext) Timestamp() time.Time {\n\treturn ctx.msg.timestamp\n}\n\nfunc (ctx *cbContext) Key() string {\n\treturn ctx.msg.key\n}\n\nfunc (ctx *cbContext) Topic() Stream {\n\treturn Stream(ctx.msg.topic)\n}\n\nfunc (ctx *cbContext) Offset() int64 {\n\treturn ctx.msg.offset\n}\n\nfunc (ctx *cbContext) Group() Group {\n\treturn ctx.graph.Group()\n}\n\nfunc (ctx *cbContext) Partition() int32 {\n\treturn ctx.msg.partition\n}\n\nfunc (ctx *cbContext) Headers() Headers {\n\tif ctx.headers == nil {\n\t\tctx.headers = HeadersFromSarama(ctx.msg.headers)\n\t}\n\treturn ctx.headers\n}\n\nfunc (ctx *cbContext) Join(topic Table) interface{} {\n\tif ctx.pviews == nil {\n\t\tctx.Fail(fmt.Errorf(\"table %s not subscribed\", topic))\n\t}\n\tv, ok := ctx.pviews[string(topic)]\n\tif !ok {\n\t\tctx.Fail(fmt.Errorf(\"table %s not subscribed\", topic))\n\t}\n\tdata, err := v.st.Get(ctx.Key())\n\tif err != nil {\n\t\tctx.Fail(fmt.Errorf(\"error getting key %s of table %s: %v\", ctx.Key(), topic, err))\n\t} else if data == nil {\n\t\treturn nil\n\t}\n\n\tvalue, err := ctx.graph.codec(string(topic)).Decode(data)\n\tif err != nil {\n\t\tctx.Fail(fmt.Errorf(\"error decoding value key %s of table %s: %v\", ctx.Key(), topic, err))\n\t}\n\treturn value\n}\n\nfunc (ctx *cbContext) Lookup(topic Table, key string) interface{} {\n\tif ctx.views == nil {\n\t\tctx.Fail(fmt.Errorf(\"topic %s not subscribed\", topic))\n\t}\n\tv, ok := ctx.views[string(topic)]\n\tif !ok {\n\t\tctx.Fail(fmt.Errorf(\"topic %s not subscribed\", topic))\n\t}\n\tval, err := v.Get(key)\n\tif err != nil {\n\t\tctx.Fail(fmt.Errorf(\"error getting key %s of table %s: %v\", key, topic, err))\n\t}\n\treturn val\n}\n\n// valueForKey returns the value of key in the processor state.\nfunc (ctx *cbContext) valueForKey(key string) (interface{}, error) {\n\tif ctx.table == nil {\n\t\treturn nil, fmt.Errorf(\"Cannot access state in stateless processor\")\n\t}\n\n\tdata, err := ctx.table.Get(key)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error reading value: %v\", err)\n\t} else if data == nil {\n\t\treturn nil, nil\n\t}\n\n\tvalue, err := ctx.graph.GroupTable().Codec().Decode(data)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error decoding value: %v\", err)\n\t}\n\treturn value, nil\n}\n\nfunc (ctx *cbContext) deleteKey(key string, headers Headers) error {\n\tif ctx.graph.GroupTable() == nil {\n\t\treturn fmt.Errorf(\"Cannot access state in stateless processor\")\n\t}\n\n\tctx.counters.stores++\n\tif err := ctx.table.Delete(key); err != nil {\n\t\treturn fmt.Errorf(\"error deleting key (%s) from storage: %v\", key, err)\n\t}\n\n\tctx.counters.emits++\n\tctx.emitter(ctx.graph.GroupTable().Topic(), key, nil, headers).Then(func(err error) {\n\t\tctx.emitDone(err)\n\t})\n\n\treturn nil\n}\n\n// setValueForKey sets a value for a key in the processor state.\nfunc (ctx *cbContext) setValueForKey(key string, value interface{}, headers Headers) error {\n\tif ctx.graph.GroupTable() == nil {\n\t\treturn fmt.Errorf(\"Cannot access state in stateless processor\")\n\t}\n\n\tif value == nil {\n\t\treturn fmt.Errorf(\"cannot set nil as value\")\n\t}\n\n\tencodedValue, err := ctx.graph.GroupTable().Codec().Encode(value)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error encoding value: %v\", err)\n\t}\n\n\tctx.counters.stores++\n\tif err = ctx.table.Set(key, encodedValue); err != nil {\n\t\treturn fmt.Errorf(\"error storing value: %v\", err)\n\t}\n\n\ttable := ctx.graph.GroupTable().Topic()\n\tctx.counters.emits++\n\tctx.emitter(table, key, encodedValue, headers).ThenWithMessage(func(msg *sarama.ProducerMessage, err error) {\n\t\tif err == nil && msg != nil && msg.Offset != 0 {\n\t\t\terr = ctx.table.SetOffset(msg.Offset)\n\t\t}\n\t\tctx.emitDone(err)\n\t})\n\n\t// for a table write we're tracking both the diskwrites and the kafka output\n\tctx.trackOutputStats(ctx.ctx, table, len(encodedValue))\n\tctx.table.TrackMessageWrite(ctx.ctx, len(encodedValue))\n\n\treturn nil\n}\n\nfunc (ctx *cbContext) emitDone(err error) {\n\tctx.m.Lock()\n\tdefer ctx.m.Unlock()\n\tctx.counters.dones++\n\tctx.tryCommit(err)\n}\n\n// called after all emits\nfunc (ctx *cbContext) finish(err error) {\n\tctx.m.Lock()\n\tdefer ctx.m.Unlock()\n\tctx.done = true\n\tctx.tryCommit(err)\n}\n\n// called before any emit\nfunc (ctx *cbContext) start() {\n\tctx.wg.Add(1)\n}\n\n// calls ctx.commit once all emits have successfully finished, or fails context\n// if some emit failed.\n// this function must be called from a locked function.\nfunc (ctx *cbContext) tryCommit(err error) {\n\tif err != nil {\n\t\tctx.errors = multierror.Append(ctx.errors, err)\n\t}\n\n\t// not all calls are done yet, do not send the ack upstream.\n\tif !ctx.done || ctx.counters.emits > ctx.counters.dones {\n\t\treturn\n\t}\n\n\t// commit if no errors, otherwise fail context\n\tif ctx.errors.ErrorOrNil() != nil {\n\t\tctx.asyncFailer(fmt.Errorf(\"could not commit message with key '%s': %w\", ctx.Key(), ctx.errors.ErrorOrNil()))\n\t} else {\n\t\tctx.commit()\n\t}\n\n\tctx.markDone()\n}\n\n// markdone marks the context as done\nfunc (ctx *cbContext) markDone() {\n\tctx.wg.Done()\n}\n\n// Fail stops execution and shuts down the processor\nfunc (ctx *cbContext) Fail(err error) {\n\tctx.syncFailer(err)\n}\n\nfunc (ctx *cbContext) Context() context.Context {\n\treturn ctx.ctx\n}\n\nfunc (ctx *cbContext) DeferCommit() func(err error) {\n\tctx.m.Lock()\n\tdefer ctx.m.Unlock()\n\tctx.counters.emits++\n\n\tvar once sync.Once\n\n\treturn func(err error) {\n\t\tonce.Do(func() {\n\t\t\tctx.emitDone(err)\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "context_test.go",
          "type": "blob",
          "size": 14.8583984375,
          "content": "package goka\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/IBM/sarama\"\n\t\"github.com/golang/mock/gomock\"\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/lovoo/goka/codec\"\n)\n\nfunc newEmitter(err error, done func(err error)) emitter {\n\treturn func(topic string, key string, value []byte, headers Headers) *Promise {\n\t\tp := NewPromise()\n\t\tif done != nil {\n\t\t\tp.Then(done)\n\t\t}\n\t\treturn p.finish(nil, err)\n\t}\n}\n\nfunc newEmitterW(wg *sync.WaitGroup, err error, done func(err error)) emitter {\n\treturn func(topic string, key string, value []byte, headers Headers) *Promise {\n\t\twg.Add(1)\n\t\tp := NewPromise()\n\t\tif done != nil {\n\t\t\tp.Then(done)\n\t\t}\n\t\treturn p.finish(nil, err)\n\t}\n}\n\nfunc TestContext_Emit(t *testing.T) {\n\tvar (\n\t\tack           = 0\n\t\temitted       = 0\n\t\tgroup   Group = \"some-group\"\n\t)\n\n\tctx := &cbContext{\n\t\tgraph:            DefineGroup(group),\n\t\tcommit:           func() { ack++ },\n\t\twg:               &sync.WaitGroup{},\n\t\ttrackOutputStats: func(ctx context.Context, topic string, size int) {},\n\t}\n\n\t// after that the message is processed\n\tctx.emitter = newEmitter(nil, func(err error) {\n\t\temitted++\n\t\trequire.NoError(t, err)\n\t})\n\n\tctx.start()\n\tctx.emit(\"emit-topic\", \"key\", []byte(\"value\"), nil)\n\tctx.finish(nil)\n\n\t// we can now for all callbacks -- it should also guarantee a memory fence\n\t// to the emitted variable (which is not being locked)\n\tctx.wg.Wait()\n\n\t// check everything is done\n\trequire.Equal(t, 1, emitted)\n\trequire.Equal(t, 1, ack)\n}\n\nfunc TestContext_DeferCommit(t *testing.T) {\n\tvar (\n\t\tack         = 0\n\t\tgroup Group = \"some-group\"\n\t)\n\n\tctx := &cbContext{\n\t\tgraph:            DefineGroup(group),\n\t\tcommit:           func() { ack++ },\n\t\twg:               &sync.WaitGroup{},\n\t\ttrackOutputStats: func(ctx context.Context, topic string, size int) {},\n\t}\n\n\tctx.start()\n\tdoneFunc := ctx.DeferCommit()\n\tctx.finish(nil)\n\n\t// ack is not done\n\trequire.Equal(t, 0, ack)\n\tdoneFunc(nil)\n\trequire.Equal(t, 1, ack)\n}\n\nfunc TestContext_DeferCommit_witherror(t *testing.T) {\n\tvar (\n\t\tack             = 0\n\t\tgroup     Group = \"some-group\"\n\t\terrToEmit       = errors.New(\"async error\")\n\t)\n\n\tfailer := func(err error) {\n\t\trequire.True(t, strings.Contains(err.Error(), errToEmit.Error()))\n\t}\n\n\tctx := &cbContext{\n\t\tgraph:            DefineGroup(group),\n\t\tcommit:           func() { ack++ },\n\t\twg:               &sync.WaitGroup{},\n\t\ttrackOutputStats: func(ctx context.Context, topic string, size int) {},\n\t\tasyncFailer:      failer,\n\t\tmsg: &message{\n\t\t\tkey: \"testKey\",\n\t\t},\n\t}\n\n\tctx.start()\n\tdoneFunc := ctx.DeferCommit()\n\tctx.finish(nil)\n\n\t// ack is not done\n\trequire.Equal(t, 0, ack)\n\tdoneFunc(fmt.Errorf(\"async error\"))\n\n\t// no commit, no ack, so we'll get the message again.\n\trequire.Equal(t, 0, ack)\n\trequire.Contains(t, ctx.errors.ErrorOrNil().Error(), \"async error\")\n}\n\nfunc TestContext_Timestamp(t *testing.T) {\n\tts := time.Now()\n\n\tctx := &cbContext{\n\t\tmsg: &message{\n\t\t\ttimestamp: ts,\n\t\t},\n\t}\n\n\trequire.Equal(t, ts, ctx.Timestamp())\n}\n\nfunc TestContext_EmitError(t *testing.T) {\n\tvar (\n\t\tack             = 0\n\t\temitted         = 0\n\t\terrToEmit       = errors.New(\"some error\")\n\t\tgroup     Group = \"some-group\"\n\t)\n\n\tfailer := func(err error) {\n\t\trequire.True(t, strings.Contains(err.Error(), errToEmit.Error()))\n\t}\n\n\t// test error case\n\tctx := &cbContext{\n\t\tgraph:            DefineGroup(group, Persist(new(codec.String))),\n\t\twg:               &sync.WaitGroup{},\n\t\ttrackOutputStats: func(ctx context.Context, topic string, size int) {},\n\t\tsyncFailer:       failer,\n\t\tasyncFailer:      failer,\n\t\tmsg: &message{\n\t\t\tkey: \"testKey\",\n\t\t},\n\t}\n\tctx.emitter = newEmitter(errToEmit, func(err error) {\n\t\temitted++\n\t\trequire.Error(t, err)\n\t\trequire.Equal(t, errToEmit, err)\n\t})\n\n\tctx.start()\n\tctx.emit(\"emit-topic\", ctx.Key(), []byte(\"value\"), nil)\n\tctx.finish(nil)\n\n\t// we can now for all callbacks -- it should also guarantee a memory fence\n\t// to the emitted variable (which is not being locked)\n\tctx.wg.Wait()\n\n\t// check everything is done\n\trequire.Equal(t, 1, emitted)\n\n\t// nothing should be committed here\n\trequire.Equal(t, 0, ack)\n}\n\nfunc TestContext_EmitToStateTopic(t *testing.T) {\n\tvar group Group = \"some-group\"\n\n\tctx := &cbContext{\n\t\tgraph:      DefineGroup(group, Persist(c), Loop(c, cb)),\n\t\tsyncFailer: func(err error) { panic(err) },\n\t}\n\trequire.PanicsWithError(t,\n\t\t\"cannot emit to table topic (use SetValue instead)\",\n\t\tfunc() { ctx.Emit(Stream(tableName(group)), \"key\", []byte(\"value\")) },\n\t)\n\trequire.PanicsWithError(t, \"cannot emit to loop topic (use Loopback instead)\",\n\t\tfunc() { ctx.Emit(Stream(loopName(group)), \"key\", []byte(\"value\")) },\n\t)\n\trequire.PanicsWithError(t, \"cannot emit to empty topic\",\n\t\tfunc() { ctx.Emit(\"\", \"key\", []byte(\"value\")) },\n\t)\n}\n\nfunc TestContext_GetSetStateless(t *testing.T) {\n\t// ctx stateless since no storage passed\n\tctx := &cbContext{\n\t\tgraph:      DefineGroup(\"group\"),\n\t\tmsg:        new(message),\n\t\tsyncFailer: func(err error) { panic(err) },\n\t}\n\trequire.PanicsWithError(t, \"Cannot access state in stateless processor\", func() { ctx.Value() })\n\trequire.PanicsWithError(t, \"Cannot access state in stateless processor\", func() { ctx.SetValue(\"whatever\") })\n}\n\nfunc TestContext_Delete(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\n\tvar (\n\t\toffset       = int64(123)\n\t\tkey          = \"key\"\n\t\tack          = 0\n\t\tgroup  Group = \"some-group\"\n\t\tst           = NewMockStorage(ctrl)\n\t\tpt           = &PartitionTable{\n\t\t\tst: &storageProxy{\n\t\t\t\tStorage: st,\n\t\t\t},\n\t\t\tstats: newTableStats(),\n\t\t}\n\t)\n\n\tst.EXPECT().Delete(key).Return(nil)\n\n\tctx := &cbContext{\n\t\tgraph:  DefineGroup(group, Persist(new(codec.String))),\n\t\twg:     new(sync.WaitGroup),\n\t\tcommit: func() { ack++ },\n\t\tmsg:    &message{offset: offset},\n\t\ttable:  pt,\n\t}\n\n\tctx.emitter = newEmitter(nil, nil)\n\n\tctx.start()\n\terr := ctx.deleteKey(key, nil)\n\trequire.NoError(t, err)\n\tctx.finish(nil)\n\n\tctx.wg.Wait()\n\n\trequire.Equal(t, ctx.counters, struct {\n\t\temits  int\n\t\tdones  int\n\t\tstores int\n\t}{1, 1, 1})\n}\n\nfunc TestContext_DeleteStateless(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\n\tvar (\n\t\toffset       = int64(123)\n\t\tkey          = \"key\"\n\t\tgroup  Group = \"some-group\"\n\t)\n\n\tctx := &cbContext{\n\t\tgraph: DefineGroup(group),\n\t\twg:    new(sync.WaitGroup),\n\t\tmsg:   &message{offset: offset},\n\t}\n\tctx.emitter = newEmitter(nil, nil)\n\n\terr := ctx.deleteKey(key, nil)\n\trequire.True(t, strings.Contains(err.Error(), \"Cannot access state in stateless processor\"))\n}\n\nfunc TestContext_DeleteStorageError(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\n\tvar (\n\t\toffset       = int64(123)\n\t\tkey          = \"key\"\n\t\tgroup  Group = \"some-group\"\n\t\tst           = NewMockStorage(ctrl)\n\t\tpt           = &PartitionTable{\n\t\t\tst: &storageProxy{\n\t\t\t\tStorage: st,\n\t\t\t},\n\t\t\tstats: newTableStats(),\n\t\t}\n\t\tretErr = errors.New(\"storage error\")\n\t)\n\n\tst.EXPECT().Delete(key).Return(retErr)\n\n\tctx := &cbContext{\n\t\tgraph: DefineGroup(group, Persist(new(codec.String))),\n\t\twg:    new(sync.WaitGroup),\n\t\tmsg:   &message{offset: offset},\n\t\ttable: pt,\n\t}\n\n\tctx.emitter = newEmitter(nil, nil)\n\n\terr := ctx.deleteKey(key, nil)\n\trequire.True(t, strings.Contains(err.Error(), \"error deleting key (key) from storage: storage error\"))\n}\n\nfunc TestContext_Set(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\n\tvar (\n\t\toffset       = int64(123)\n\t\tack          = 0\n\t\tkey          = \"key\"\n\t\tvalue        = \"value\"\n\t\tgroup  Group = \"some-group\"\n\t\tst           = NewMockStorage(ctrl)\n\t\tpt           = &PartitionTable{\n\t\t\tst: &storageProxy{\n\t\t\t\tStorage: st,\n\t\t\t},\n\t\t\tstats: newTableStats(),\n\t\t}\n\t)\n\tst.EXPECT().Set(key, []byte(value)).Return(nil)\n\n\tctx := &cbContext{\n\t\tgraph:            DefineGroup(group, Persist(new(codec.String))),\n\t\twg:               new(sync.WaitGroup),\n\t\tcommit:           func() { ack++ },\n\t\ttrackOutputStats: func(ctx context.Context, topic string, size int) {},\n\t\tmsg:              &message{key: key, offset: offset},\n\t\ttable:            pt,\n\t\tctx:              context.Background(),\n\t}\n\n\tctx.emitter = newEmitter(nil, nil)\n\n\tctx.start()\n\terr := ctx.setValueForKey(key, value, nil)\n\trequire.NoError(t, err)\n\tctx.finish(nil)\n\n\tctx.wg.Wait()\n\n\trequire.Equal(t, ctx.counters, struct {\n\t\temits  int\n\t\tdones  int\n\t\tstores int\n\t}{1, 1, 1})\n\trequire.Equal(t, 1, ack)\n}\n\nfunc TestContext_GetSetStateful(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\n\tvar (\n\t\tgroup   Group = \"some-group\"\n\t\tkey           = \"key\"\n\t\tvalue         = \"value\"\n\t\theaders       = Headers{\"key\": []byte(\"headerValue\")}\n\t\toffset        = int64(123)\n\t\twg            = new(sync.WaitGroup)\n\t\tst            = NewMockStorage(ctrl)\n\t\tpt            = &PartitionTable{\n\t\t\tst: &storageProxy{\n\t\t\t\tStorage: st,\n\t\t\t},\n\t\t\tstate: newPartitionTableState().SetState(State(PartitionRunning)),\n\t\t\tstats: newTableStats(),\n\t\t}\n\t)\n\n\tst.EXPECT().Get(key).Return(nil, nil)\n\tst.EXPECT().Set(key, []byte(value)).Return(nil)\n\tst.EXPECT().Get(key).Return([]byte(value), nil)\n\n\tgraph := DefineGroup(group, Persist(new(codec.String)))\n\tctx := &cbContext{\n\t\ttable:            pt,\n\t\twg:               wg,\n\t\tgraph:            graph,\n\t\ttrackOutputStats: func(ctx context.Context, topic string, size int) {},\n\t\tmsg:              &message{key: key, offset: offset},\n\t\temitter: func(tp string, k string, v []byte, h Headers) *Promise {\n\t\t\twg.Add(1)\n\t\t\trequire.Equal(t, graph.GroupTable().Topic(), tp)\n\t\t\trequire.Equal(t, key, string(k))\n\t\t\trequire.Equal(t, value, string(v))\n\t\t\trequire.Equal(t, headers, h)\n\t\t\treturn NewPromise().finish(nil, nil)\n\t\t},\n\t\tctx: context.Background(),\n\t}\n\n\tval := ctx.Value()\n\trequire.True(t, val == nil)\n\n\tctx.SetValue(value, WithCtxEmitHeaders(headers))\n\n\tval = ctx.Value()\n\trequire.Equal(t, value, val)\n}\n\nfunc TestContext_SetErrors(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\n\tvar (\n\t\tgroup  Group = \"some-group\"\n\t\tkey          = \"key\"\n\t\tvalue        = \"value\"\n\t\toffset int64 = 123\n\t\twg           = new(sync.WaitGroup)\n\t\tst           = NewMockStorage(ctrl)\n\t\tpt           = &PartitionTable{\n\t\t\tst: &storageProxy{\n\t\t\t\tStorage: st,\n\t\t\t},\n\t\t\tstats: newTableStats(),\n\t\t}\n\t\tfailed error\n\t\t_      = failed // make linter happy\n\t)\n\n\tfailer := func(err error) { failed = err }\n\n\tctx := &cbContext{\n\t\ttable:            pt,\n\t\ttrackOutputStats: func(ctx context.Context, topic string, size int) {},\n\t\twg:               wg,\n\t\tgraph:            DefineGroup(group, Persist(new(codec.String))),\n\t\tmsg:              &message{key: key, offset: offset},\n\t\tsyncFailer:       failer,\n\t\tasyncFailer:      failer,\n\t}\n\n\terr := ctx.setValueForKey(key, nil, nil)\n\trequire.Error(t, err)\n\trequire.True(t, strings.Contains(err.Error(), \"cannot set nil\"))\n\n\terr = ctx.setValueForKey(key, 123, nil) // cannot encode 123 as string\n\trequire.Error(t, err)\n\trequire.True(t, strings.Contains(err.Error(), \"error encoding\"))\n\n\tst.EXPECT().Set(key, []byte(value)).Return(errors.New(\"some-error\"))\n\n\terr = ctx.setValueForKey(key, value, nil)\n\trequire.Error(t, err)\n\trequire.True(t, strings.Contains(err.Error(), \"some-error\"))\n\n\t// TODO(jb): check if still valid\n\t// finish with error\n\t// ctx.emitter = newEmitterW(wg, fmt.Errorf(\"some-error\"), func(err error) {\n\t// \trequire.Error(t, err)\n\t//\trequire.True(t, strings.Contains(err.Error(), \"some-error\"))\n\t// })\n\t// err = ctx.setValueForKey(key, value)\n\t// require.NoError(t, err)\n}\n\nfunc TestContext_LoopbackNoLoop(t *testing.T) {\n\t// ctx has no loop set\n\tctx := &cbContext{\n\t\tgraph:      DefineGroup(\"group\", Persist(c)),\n\t\tmsg:        &message{},\n\t\tsyncFailer: func(err error) { panic(err) },\n\t}\n\trequire.PanicsWithError(t, \"no loop topic configured\", func() {\n\t\tctx.Loopback(\"some-key\", \"whatever\")\n\t})\n}\n\nfunc TestContext_Loopback(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\n\tvar (\n\t\tkey     = \"key\"\n\t\tvalue   = \"value\"\n\t\theaders = Headers{\"key\": []byte(\"headerValue\")}\n\t\tcnt     = 0\n\t)\n\n\tgraph := DefineGroup(\"group\", Persist(c), Loop(c, cb))\n\tctx := &cbContext{\n\t\tgraph:            graph,\n\t\tmsg:              &message{},\n\t\ttrackOutputStats: func(ctx context.Context, topic string, size int) {},\n\t\temitter: func(tp string, k string, v []byte, h Headers) *Promise {\n\t\t\tcnt++\n\t\t\trequire.Equal(t, graph.LoopStream().Topic(), tp)\n\t\t\trequire.Equal(t, key, string(k))\n\t\t\trequire.Equal(t, value, string(v))\n\t\t\trequire.Equal(t, headers, h)\n\t\t\treturn NewPromise()\n\t\t},\n\t}\n\n\tctx.Loopback(key, value, WithCtxEmitHeaders(headers))\n\trequire.True(t, cnt == 1)\n}\n\nfunc TestContext_Join(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\n\tvar (\n\t\tkey           = \"key\"\n\t\tvalue         = \"value\"\n\t\ttable   Table = \"table\"\n\t\terrSome       = errors.New(\"some-error\")\n\t\tst            = NewMockStorage(ctrl)\n\t)\n\n\tctx := &cbContext{\n\t\tgraph: DefineGroup(\"group\", Persist(c), Loop(c, cb), Join(table, c)),\n\t\tmsg:   &message{key: key},\n\t\tpviews: map[string]*PartitionTable{\n\t\t\tstring(table): {\n\t\t\t\tlog: defaultLogger,\n\t\t\t\tst: &storageProxy{\n\t\t\t\t\tStorage: st,\n\t\t\t\t},\n\t\t\t\tstats: newTableStats(),\n\t\t\t},\n\t\t},\n\t\tsyncFailer: func(err error) { panic(err) },\n\t}\n\n\tst.EXPECT().Get(key).Return([]byte(value), nil)\n\tv := ctx.Join(table)\n\trequire.Equal(t, value, v)\n\n\tst.EXPECT().Get(key).Return(nil, errSome)\n\trequire.Panics(t, func() { ctx.Join(table) })\n\n\trequire.Panics(t, func() { ctx.Join(\"other-table\") })\n\n\tctx.pviews = nil\n\trequire.Panics(t, func() { ctx.Join(table) })\n}\n\nfunc TestContext_Lookup(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\n\tvar (\n\t\tkey           = \"key\"\n\t\tvalue         = \"value\"\n\t\ttable   Table = \"table\"\n\t\terrSome       = errors.New(\"some-error\")\n\t\tst            = NewMockStorage(ctrl)\n\t)\n\n\tctx := &cbContext{\n\t\tgraph: DefineGroup(\"group\", Persist(c), Loop(c, cb)),\n\t\tmsg:   &message{key: key},\n\t\tviews: map[string]*View{\n\t\t\tstring(table): {\n\t\t\t\tstate: newViewSignal().SetState(State(ViewStateRunning)),\n\t\t\t\topts: &voptions{\n\t\t\t\t\ttableCodec: c,\n\t\t\t\t\thasher:     DefaultHasher(),\n\t\t\t\t},\n\t\t\t\tpartitions: []*PartitionTable{\n\t\t\t\t\t{\n\t\t\t\t\t\tst: &storageProxy{\n\t\t\t\t\t\t\tStorage: st,\n\t\t\t\t\t\t},\n\t\t\t\t\t\tstate: newPartitionTableState().SetState(State(PartitionRunning)),\n\t\t\t\t\t\tstats: newTableStats(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\tsyncFailer: func(err error) { panic(err) },\n\t}\n\n\tst.EXPECT().Get(key).Return([]byte(value), nil)\n\tv := ctx.Lookup(table, key)\n\trequire.Equal(t, value, v)\n\n\tst.EXPECT().Get(key).Return(nil, errSome)\n\trequire.Panics(t, func() { ctx.Lookup(table, key) })\n\trequire.Panics(t, func() { ctx.Lookup(\"other-table\", key) })\n\n\tctx.views = nil\n\trequire.Panics(t, func() { ctx.Lookup(table, key) })\n}\n\nfunc TestContext_Headers(t *testing.T) {\n\t// context without headers will return empty map\n\tctx := &cbContext{\n\t\tmsg: &message{\n\t\t\tkey: \"key\",\n\t\t},\n\t}\n\theaders := ctx.Headers()\n\trequire.NotNil(t, headers)\n\trequire.Equal(t, 0, len(headers))\n\n\tctx = &cbContext{\n\t\tmsg: &message{\n\t\t\tkey: \"key\",\n\t\t\theaders: []*sarama.RecordHeader{\n\t\t\t\t{\n\t\t\t\t\tKey:   []byte(\"key\"),\n\t\t\t\t\tValue: []byte(\"value\"),\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\theaders = ctx.Headers()\n\trequire.Equal(t, []byte(\"value\"), headers[\"key\"])\n}\n\nfunc TestContext_Fail(t *testing.T) {\n\tctx := &cbContext{\n\t\tsyncFailer: func(err error) {\n\t\t\tpanic(fmt.Errorf(\"%#v\", err))\n\t\t},\n\t}\n\n\trequire.Panics(t, func() { ctx.Fail(errors.New(\"blubb\")) })\n}\n"
        },
        {
          "name": "copartition_strategy.go",
          "type": "blob",
          "size": 4.4697265625,
          "content": "package goka\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"sort\"\n\n\t\"github.com/IBM/sarama\"\n)\n\nvar (\n\t// CopartitioningStrategy is the rebalance strategy necessary to guarantee the copartitioning\n\t// when consuming multiple input topics with multiple processor instances.\n\t// This strategy tolerates different sets of topics per member of consumer group to allow\n\t// rolling upgrades of processors.\n\t//\n\t// Note that the topic inconcistency needs to be only temporarily, otherwise not all topic partitions will be consumed as in\n\t// the following example:\n\t// Assume having topics X and Y, each with partitions [0,1,2]\n\t// MemberA requests topic X\n\t// MemberB requests topics X and Y, because topic Y was newly added to the processor.\n\t//\n\t// Then the strategy will plan as follows:\n\t// MemberA: X: [0,1]\n\t// MemberB: X: [2], Y:[2]\n\t//\n\t// That means partitions [0,1] from topic Y are not being consumed.\n\t// So the assumption is that memberA will be redeployed so that after a second rebalance\n\t// both members consume both topics and all partitions.\n\t//\n\t// If you do not use rolling upgrades, i.e. replace all members of a group simultaneously, it is\n\t// safe to use the StrictCopartitioningStrategy\n\tCopartitioningStrategy = new(copartitioningStrategy)\n\n\t// StrictCopartitioningStrategy behaves like the copartitioning strategy but it will fail if two members of a consumer group\n\t// request a different set of topics, which might indicate a bug or a reused consumer group name.\n\tStrictCopartitioningStrategy = &copartitioningStrategy{\n\t\tfailOnInconsistentTopics: true,\n\t}\n)\n\ntype copartitioningStrategy struct {\n\tfailOnInconsistentTopics bool\n}\n\n// Name implements BalanceStrategy.\nfunc (s *copartitioningStrategy) Name() string {\n\treturn \"copartition\"\n}\n\n// Plan implements BalanceStrategy.\nfunc (s *copartitioningStrategy) Plan(members map[string]sarama.ConsumerGroupMemberMetadata, topics map[string][]int32) (sarama.BalanceStrategyPlan, error) {\n\tvar (\n\t\tallPartitions []int32\n\t\tallTopics     []string\n\t\tallMembers    []string\n\t)\n\t// (1) collect all topics and check they're copartitioned\n\tfor topic, topicPartitions := range topics {\n\t\tallTopics = append(allTopics, topic)\n\t\tif len(allPartitions) == 0 {\n\t\t\tallPartitions = topicPartitions\n\t\t} else {\n\t\t\tif !s.partitionsEqual(allPartitions, topicPartitions) {\n\t\t\t\treturn nil, fmt.Errorf(\"Error balancing. Not all topics are copartitioned. For goka, all topics need to have the same number of partitions: %#v\", topics)\n\t\t\t}\n\t\t}\n\t}\n\n\t// (2) collect all members and check they consume the same topics\n\tfor memberID, meta := range members {\n\t\tif !s.topicsEqual(allTopics, meta.Topics) {\n\t\t\tif s.failOnInconsistentTopics {\n\t\t\t\treturn nil, fmt.Errorf(\"Error balancing. Not all members request the same list of topics. A group-name clash might be the reason: %#v\", members)\n\t\t\t}\n\t\t}\n\t\tallMembers = append(allMembers, memberID)\n\t}\n\n\t// (3) sort the data structures for deterministic distribution\n\tsort.Strings(allMembers)\n\tsort.Strings(allTopics)\n\tsort.Sort(partitionSlice(allPartitions))\n\n\t// (4) create a plan and assign the same set of partitions to the members\n\t// in a range-like configuration (like `sarama.BalanceStrategyRange`)\n\tplan := make(sarama.BalanceStrategyPlan, len(allMembers))\n\tstep := float64(len(allPartitions)) / float64(len(allMembers))\n\tfor idx, memberID := range allMembers {\n\t\tpos := float64(idx)\n\t\tmin := int(math.Floor(pos*step + 0.5))\n\t\tmax := int(math.Floor((pos+1)*step + 0.5))\n\t\tfor _, topic := range members[memberID].Topics {\n\t\t\tplan.Add(memberID, topic, allPartitions[min:max]...)\n\t\t}\n\t}\n\n\treturn plan, nil\n}\n\n// AssignmentData copartitioning strategy does not require data\nfunc (s *copartitioningStrategy) AssignmentData(memberID string, topics map[string][]int32, generationID int32) ([]byte, error) {\n\treturn nil, nil\n}\n\nfunc (s *copartitioningStrategy) partitionsEqual(a, b []int32) bool {\n\tif len(a) != len(b) {\n\t\treturn false\n\t}\n\tpartSet := make(map[int32]bool, len(a))\n\tfor _, p := range a {\n\t\tpartSet[p] = true\n\t}\n\tfor _, p := range b {\n\t\tif !partSet[p] {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\nfunc (s *copartitioningStrategy) topicsEqual(a, b []string) bool {\n\tif len(a) != len(b) {\n\t\treturn false\n\t}\n\ttopicSet := make(map[string]bool, len(a))\n\tfor _, p := range a {\n\t\ttopicSet[p] = true\n\t}\n\tfor _, p := range b {\n\t\tif !topicSet[p] {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\ntype partitionSlice []int32\n\nfunc (p partitionSlice) Len() int           { return len(p) }\nfunc (p partitionSlice) Less(i, j int) bool { return p[i] < p[j] }\nfunc (p partitionSlice) Swap(i, j int)      { p[i], p[j] = p[j], p[i] }\n"
        },
        {
          "name": "copartition_strategy_test.go",
          "type": "blob",
          "size": 3.76171875,
          "content": "package goka\n\nimport (\n\t\"reflect\"\n\t\"testing\"\n\n\t\"github.com/IBM/sarama\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestCopartitioningStrategy(t *testing.T) {\n\tt.Run(\"name\", func(t *testing.T) {\n\t\trequire.Equal(t, \"copartition\", CopartitioningStrategy.Name())\n\t})\n\n\tfor _, ttest := range []struct {\n\t\tname      string\n\t\tmembers   map[string]sarama.ConsumerGroupMemberMetadata\n\t\ttopics    map[string][]int32\n\t\thasError  bool\n\t\tuseStrict bool\n\t\texpected  sarama.BalanceStrategyPlan\n\t}{\n\t\t{\n\t\t\tname: \"inconsistent-topic-members\",\n\t\t\tmembers: map[string]sarama.ConsumerGroupMemberMetadata{\n\t\t\t\t\"M1\": {Topics: []string{\"T1\"}},\n\t\t\t},\n\t\t\t// topics are inconsistent with members\n\t\t\ttopics: map[string][]int32{\n\t\t\t\t\"T2\": {0, 1, 2},\n\t\t\t},\n\t\t\thasError:  true,\n\t\t\tuseStrict: true,\n\t\t},\n\t\t{\n\t\t\tname: \"not-copartitioned\",\n\t\t\tmembers: map[string]sarama.ConsumerGroupMemberMetadata{\n\t\t\t\t\"M1\": {Topics: []string{\"T1\", \"T2\"}},\n\t\t\t},\n\t\t\t// topics are inconsistent with members\n\t\t\ttopics: map[string][]int32{\n\t\t\t\t\"T1\": {0, 1, 2},\n\t\t\t\t\"T2\": {0, 1},\n\t\t\t},\n\t\t\thasError: true,\n\t\t},\n\t\t{\n\t\t\tname: \"inconsistent-members\",\n\t\t\tmembers: map[string]sarama.ConsumerGroupMemberMetadata{\n\t\t\t\t\"M1\": {Topics: []string{\"T1\", \"T2\"}},\n\t\t\t\t\"M2\": {Topics: []string{\"T2\"}},\n\t\t\t},\n\t\t\t// topics are inconsistent with members\n\t\t\ttopics: map[string][]int32{\n\t\t\t\t\"T1\": {0, 1, 2},\n\t\t\t\t\"T2\": {0, 1, 2},\n\t\t\t},\n\t\t\thasError:  true,\n\t\t\tuseStrict: true,\n\t\t},\n\t\t{\n\t\t\tname: \"tolerate-inconsistent-members\",\n\t\t\tmembers: map[string]sarama.ConsumerGroupMemberMetadata{\n\t\t\t\t\"M1\": {Topics: []string{\"T1\", \"T2\"}},\n\t\t\t\t\"M2\": {Topics: []string{\"T2\"}},\n\t\t\t},\n\t\t\t// topics are inconsistent with members\n\t\t\ttopics: map[string][]int32{\n\t\t\t\t\"T1\": {0, 1, 2},\n\t\t\t\t\"T2\": {0, 1, 2},\n\t\t\t},\n\t\t\texpected: sarama.BalanceStrategyPlan{\n\t\t\t\t\"M1\": map[string][]int32{\n\t\t\t\t\t\"T1\": {0, 1},\n\t\t\t\t\t\"T2\": {0, 1},\n\t\t\t\t},\n\t\t\t\t\"M2\": map[string][]int32{\n\t\t\t\t\t\"T2\": {2},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"single-member\",\n\t\t\tmembers: map[string]sarama.ConsumerGroupMemberMetadata{\n\t\t\t\t\"M1\": {Topics: []string{\"T1\"}},\n\t\t\t},\n\t\t\t// topics are inconsistent with members\n\t\t\ttopics: map[string][]int32{\n\t\t\t\t\"T1\": {0, 1, 2},\n\t\t\t},\n\t\t\texpected: sarama.BalanceStrategyPlan{\n\t\t\t\t\"M1\": map[string][]int32{\n\t\t\t\t\t\"T1\": {0, 1, 2},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"multi-member\",\n\t\t\tmembers: map[string]sarama.ConsumerGroupMemberMetadata{\n\t\t\t\t\"M1\": {Topics: []string{\"T1\"}},\n\t\t\t\t\"M2\": {Topics: []string{\"T1\"}},\n\t\t\t},\n\t\t\t// topics are inconsistent with members\n\t\t\ttopics: map[string][]int32{\n\t\t\t\t\"T1\": {0, 1, 2},\n\t\t\t},\n\t\t\texpected: sarama.BalanceStrategyPlan{\n\t\t\t\t\"M1\": map[string][]int32{\n\t\t\t\t\t\"T1\": {0, 1},\n\t\t\t\t},\n\t\t\t\t\"M2\": map[string][]int32{\n\t\t\t\t\t\"T1\": {2},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"multi-member-multitopic\",\n\t\t\tmembers: map[string]sarama.ConsumerGroupMemberMetadata{\n\t\t\t\t\"M1\": {Topics: []string{\"T1\", \"T2\", \"T3\"}},\n\t\t\t\t\"M2\": {Topics: []string{\"T2\", \"T3\", \"T1\"}},\n\t\t\t\t\"M3\": {Topics: []string{\"T2\", \"T3\", \"T1\"}},\n\t\t\t},\n\t\t\t// topics are inconsistent with members\n\t\t\ttopics: map[string][]int32{\n\t\t\t\t\"T1\": {0, 1, 2, 3, 4, 5},\n\t\t\t\t\"T2\": {0, 1, 2, 3, 4, 5},\n\t\t\t\t\"T3\": {0, 1, 2, 3, 4, 5},\n\t\t\t},\n\t\t\texpected: sarama.BalanceStrategyPlan{\n\t\t\t\t\"M1\": map[string][]int32{\n\t\t\t\t\t\"T1\": {0, 1},\n\t\t\t\t\t\"T2\": {0, 1},\n\t\t\t\t\t\"T3\": {0, 1},\n\t\t\t\t},\n\t\t\t\t\"M2\": map[string][]int32{\n\t\t\t\t\t\"T1\": {2, 3},\n\t\t\t\t\t\"T2\": {2, 3},\n\t\t\t\t\t\"T3\": {2, 3},\n\t\t\t\t},\n\t\t\t\t\"M3\": map[string][]int32{\n\t\t\t\t\t\"T1\": {4, 5},\n\t\t\t\t\t\"T2\": {4, 5},\n\t\t\t\t\t\"T3\": {4, 5},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t} {\n\t\tt.Run(ttest.name, func(t *testing.T) {\n\t\t\tvar strategy sarama.BalanceStrategy = CopartitioningStrategy\n\t\t\tif ttest.useStrict {\n\t\t\t\tstrategy = StrictCopartitioningStrategy\n\t\t\t}\n\n\t\t\tplan, err := strategy.Plan(ttest.members, ttest.topics)\n\t\t\trequire.Equal(t, ttest.hasError, err != nil)\n\t\t\tif err == nil {\n\t\t\t\trequire.True(t, reflect.DeepEqual(ttest.expected, plan), \"expected\", ttest.expected, \"actual\", plan)\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "doc.go",
          "type": "blob",
          "size": 1.9111328125,
          "content": "//go:generate mockgen -self_package github.com/lovoo/goka -package goka -destination mockstorage.go github.com/lovoo/goka/storage Storage\n//go:generate mockgen -self_package github.com/lovoo/goka -package goka -destination mocks.go github.com/lovoo/goka TopicManager,Producer,Broker\n//go:generate mockgen -self_package github.com/lovoo/goka -package goka -destination mockssarama.go github.com/IBM/sarama Client,ClusterAdmin\n\n/*\nPackage goka is a stateful stream processing library for Apache Kafka (version 0.9+) that eases\nthe development of microservices.\nGoka extends the concept of consumer group with a group table, which represents the state of the\ngroup.\nA microservice modifies and serves the content of a table employing two complementary object types:\nprocessors and views.\n\n# Processors\n\nA processor is a set of callback functions that modify the group table when messages arrive and may\nalso emit messages into other topics.\nMessages as well as rows in the group table are key-value pairs.\nCallbacks receive the arriving message and the row addressed by the message's key.\n\nIn Kafka, keys are used to partition topics.\nA goka processor consumes from a set of co-partitioned topics (topics with the same number of\npartitions and the same key range).\nA group topic keeps track of the group table updates, allowing for recovery and rebalancing of\nprocessors:\nWhen multiple processor instances start in the same consumer group, the instances split the\nco-partitioned input topics and load the respective group table partitions from the group topic.\nA local disk storage minimizes recovery time by caching partitions of group table.\n\n# Views\n\nA view is a materialized (ie, persistent) cache of a group table.\nA view subscribes for the updates of all partitions of a group table and keeps local disk storage\nin sync with the group topic.\nWith a view, one can easily serve up-to-date content of the group table via, for example, gRPC.\n*/\npackage goka\n"
        },
        {
          "name": "emitter.go",
          "type": "blob",
          "size": 3.2421875,
          "content": "package goka\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"sync\"\n)\n\nvar (\n\t// ErrEmitterAlreadyClosed is returned when Emit is called after the emitter has been finished.\n\tErrEmitterAlreadyClosed error = errors.New(\"emitter already closed\")\n)\n\n// Emitter emits messages into a specific Kafka topic, first encoding the message with the given codec.\ntype Emitter struct {\n\tcodec    Codec\n\tproducer Producer\n\n\ttopic          string\n\tdefaultHeaders Headers\n\n\twg   sync.WaitGroup\n\tmu   sync.RWMutex\n\tdone chan struct{}\n}\n\n// NewEmitter creates a new emitter using passed brokers, topic, codec and possibly options.\nfunc NewEmitter(brokers []string, topic Stream, codec Codec, options ...EmitterOption) (*Emitter, error) {\n\toptions = append(\n\t\t// default options comes first\n\t\t[]EmitterOption{\n\t\t\tWithEmitterClientID(fmt.Sprintf(\"goka-emitter-%s\", topic)),\n\t\t},\n\n\t\t// user-defined options (may overwrite default ones)\n\t\toptions...,\n\t)\n\n\topts := new(eoptions)\n\n\topts.applyOptions(topic, codec, options...)\n\n\tprod, err := opts.builders.producer(brokers, opts.clientID, opts.hasher)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(errBuildProducer, err)\n\t}\n\n\treturn &Emitter{\n\t\tcodec:          codec,\n\t\tproducer:       prod,\n\t\ttopic:          string(topic),\n\t\tdefaultHeaders: opts.defaultHeaders,\n\t\tdone:           make(chan struct{}),\n\t}, nil\n}\n\nfunc (e *Emitter) emitDone(err error) { e.wg.Done() }\n\n// EmitWithHeaders sends a message with the given headers for the passed key using the emitter's codec.\nfunc (e *Emitter) EmitWithHeaders(key string, msg interface{}, headers Headers) (*Promise, error) {\n\tvar (\n\t\terr  error\n\t\tdata []byte\n\t)\n\n\tif msg != nil {\n\t\tdata, err = e.codec.Encode(msg)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"Error encoding value for key %s in topic %s: %v\", key, e.topic, err)\n\t\t}\n\t}\n\n\t// protect e.done channel and e.wg WaitGroup together to reject all new emits after calling e.Finish\n\t// wg.Add must not be called after wg.Wait finished\n\te.mu.RLock()\n\tselect {\n\tcase <-e.done:\n\t\te.mu.RUnlock()\n\t\treturn NewPromise().finish(nil, ErrEmitterAlreadyClosed), nil\n\tdefault:\n\t\te.wg.Add(1)\n\t\te.mu.RUnlock()\n\t}\n\n\tif headers == nil && e.defaultHeaders == nil {\n\t\treturn e.producer.Emit(e.topic, key, data).Then(e.emitDone), nil\n\t}\n\n\treturn e.producer.EmitWithHeaders(e.topic, key, data, e.defaultHeaders.Merged(headers)).Then(e.emitDone), nil\n}\n\n// Emit sends a message for passed key using the emitter's codec.\nfunc (e *Emitter) Emit(key string, msg interface{}) (*Promise, error) {\n\treturn e.EmitWithHeaders(key, msg, nil)\n}\n\n// EmitSyncWithHeaders sends a message with the given headers to passed topic and key.\nfunc (e *Emitter) EmitSyncWithHeaders(key string, msg interface{}, headers Headers) error {\n\tvar (\n\t\terr     error\n\t\tpromise *Promise\n\t)\n\tpromise, err = e.EmitWithHeaders(key, msg, headers)\n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdone := make(chan struct{})\n\tpromise.Then(func(asyncErr error) {\n\t\terr = asyncErr\n\t\tclose(done)\n\t})\n\t<-done\n\treturn err\n}\n\n// EmitSync sends a message to passed topic and key.\nfunc (e *Emitter) EmitSync(key string, msg interface{}) error {\n\treturn e.EmitSyncWithHeaders(key, msg, nil)\n}\n\n// Finish waits until the emitter is finished producing all pending messages.\nfunc (e *Emitter) Finish() error {\n\te.mu.Lock()\n\tclose(e.done)\n\te.mu.Unlock()\n\te.wg.Wait()\n\treturn e.producer.Close()\n}\n"
        },
        {
          "name": "emitter_test.go",
          "type": "blob",
          "size": 5.98046875,
          "content": "package goka\n\nimport (\n\t\"errors\"\n\t\"hash\"\n\t\"strconv\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/golang/mock/gomock\"\n\t\"github.com/lovoo/goka/codec\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nvar (\n\temitterTestClientID = \"161\"\n\temitterTestBrokers  = []string{\"0\"}\n\temitterTestTopic    = Stream(\"emitter-stream\")\n\temitterIntCodec     = new(codec.Int64)\n)\n\nfunc createEmitter(t *testing.T, options ...EmitterOption) (*Emitter, *builderMock, *gomock.Controller) {\n\tctrl := NewMockController(t)\n\tbm := newBuilderMock(ctrl)\n\temitter, _ := NewEmitter(emitterTestBrokers, emitterTestTopic, emitterIntCodec, append([]EmitterOption{\n\t\tWithEmitterClientID(emitterTestClientID),\n\t\tWithEmitterTopicManagerBuilder(bm.getTopicManagerBuilder()),\n\t\tWithEmitterProducerBuilder(bm.getProducerBuilder()),\n\t\tWithEmitterHasher(func() hash.Hash32 { return newConstHasher(0) }),\n\t}, options...)...)\n\treturn emitter, bm, ctrl\n}\n\nfunc TestEmitter_NewEmitter(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tctrl := NewMockController(t)\n\t\tbm := newBuilderMock(ctrl)\n\t\temitter, err := NewEmitter(emitterTestBrokers, emitterTestTopic, emitterIntCodec, []EmitterOption{\n\t\t\tWithEmitterClientID(emitterTestClientID),\n\t\t\tWithEmitterTopicManagerBuilder(bm.getTopicManagerBuilder()),\n\t\t\tWithEmitterProducerBuilder(bm.getProducerBuilder()),\n\t\t\tWithEmitterHasher(func() hash.Hash32 { return newConstHasher(0) }),\n\t\t}...)\n\t\trequire.NoError(t, err)\n\t\trequire.NotNil(t, emitter)\n\t\trequire.True(t, emitter.codec == emitterIntCodec)\n\t\trequire.Equal(t, emitter.producer, bm.producer)\n\t\trequire.True(t, emitter.topic == string(emitterTestTopic))\n\t})\n\tt.Run(\"fail\", func(t *testing.T) {\n\t\tctrl := NewMockController(t)\n\t\tbm := newBuilderMock(ctrl)\n\t\tdefer ctrl.Finish()\n\t\temitter, err := NewEmitter(emitterTestBrokers, emitterTestTopic, emitterIntCodec, WithEmitterProducerBuilder(bm.getErrorProducerBuilder()))\n\t\trequire.Error(t, err)\n\t\trequire.Nil(t, emitter)\n\t})\n}\n\nfunc TestEmitter_Emit(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\temitter, bm, ctrl := createEmitter(t)\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tkey           = \"some-key\"\n\t\t\tintVal int64  = 1312\n\t\t\tdata   []byte = []byte(strconv.FormatInt(intVal, 10))\n\t\t)\n\n\t\tbm.producer.EXPECT().Emit(emitter.topic, key, data).Return(NewPromise().finish(nil, nil))\n\t\tpromise, err := emitter.Emit(key, intVal)\n\t\trequire.NoError(t, err)\n\t\trequire.NotNil(t, promise)\n\t})\n\tt.Run(\"fail_producer_emit\", func(t *testing.T) {\n\t\temitter, bm, ctrl := createEmitter(t)\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tkey           = \"some-key\"\n\t\t\tintVal int64  = 1312\n\t\t\tdata   []byte = []byte(strconv.FormatInt(intVal, 10))\n\t\t\tretErr error  = errors.New(\"some-error\")\n\t\t)\n\n\t\tbm.producer.EXPECT().Emit(emitter.topic, key, data).Return(NewPromise().finish(nil, retErr))\n\t\tpromise, err := emitter.Emit(key, intVal)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, promise.err, retErr)\n\t})\n\tt.Run(\"fail_closed\", func(t *testing.T) {\n\t\temitter, bm, ctrl := createEmitter(t)\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tkey          = \"some-key\"\n\t\t\tintVal int64 = 1312\n\t\t)\n\n\t\tbm.producer.EXPECT().Close().Return(nil)\n\n\t\temitter.Finish()\n\t\tpromise, err := emitter.Emit(key, intVal)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, promise.err, ErrEmitterAlreadyClosed)\n\t})\n\tt.Run(\"fail_encode\", func(t *testing.T) {\n\t\temitter, _, _ := createEmitter(t)\n\n\t\tvar (\n\t\t\tkey    = \"some-key\"\n\t\t\tintVal = \"1312\"\n\t\t)\n\n\t\t_, err := emitter.Emit(key, intVal)\n\t\trequire.Error(t, err)\n\t})\n\tt.Run(\"default_headers\", func(t *testing.T) {\n\t\temitter, bm, ctrl := createEmitter(t)\n\t\temitter.defaultHeaders = Headers{\"header-key\": []byte(\"header-val\")}\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tkey          = \"some-key\"\n\t\t\tintVal int64 = 1312\n\t\t\tdata         = []byte(strconv.FormatInt(intVal, 10))\n\t\t)\n\n\t\tbm.producer.EXPECT().EmitWithHeaders(emitter.topic, key, data, emitter.defaultHeaders).\n\t\t\tReturn(NewPromise().finish(nil, nil))\n\t\tpromise, err := emitter.Emit(key, intVal)\n\t\trequire.NoError(t, err)\n\t\trequire.NotNil(t, promise)\n\t})\n}\n\nfunc TestEmitter_EmitSync(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\temitter, bm, ctrl := createEmitter(t)\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tkey           = \"some-key\"\n\t\t\tintVal int64  = 1312\n\t\t\tdata   []byte = []byte(strconv.FormatInt(intVal, 10))\n\t\t)\n\n\t\tbm.producer.EXPECT().Emit(emitter.topic, key, data).Return(NewPromise().finish(nil, nil))\n\t\terr := emitter.EmitSync(key, intVal)\n\t\trequire.NoError(t, err)\n\t})\n\tt.Run(\"fail_producer_emit\", func(t *testing.T) {\n\t\temitter, bm, ctrl := createEmitter(t)\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tkey           = \"some-key\"\n\t\t\tintVal int64  = 1312\n\t\t\tdata   []byte = []byte(strconv.FormatInt(intVal, 10))\n\t\t\tretErr error  = errors.New(\"some-error\")\n\t\t)\n\n\t\tbm.producer.EXPECT().Emit(emitter.topic, key, data).Return(NewPromise().finish(nil, retErr))\n\t\terr := emitter.EmitSync(key, intVal)\n\t\trequire.Equal(t, err, retErr)\n\t})\n\tt.Run(\"fail_closed\", func(t *testing.T) {\n\t\temitter, bm, ctrl := createEmitter(t)\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tkey          = \"some-key\"\n\t\t\tintVal int64 = 1312\n\t\t)\n\n\t\tbm.producer.EXPECT().Close().Return(nil)\n\n\t\temitter.Finish()\n\t\terr := emitter.EmitSync(key, intVal)\n\t\trequire.Equal(t, err, ErrEmitterAlreadyClosed)\n\t})\n\tt.Run(\"fail_encode\", func(t *testing.T) {\n\t\temitter, _, _ := createEmitter(t)\n\n\t\tvar (\n\t\t\tkey    = \"some-key\"\n\t\t\tintVal = \"1312\"\n\t\t)\n\n\t\terr := emitter.EmitSync(key, intVal)\n\t\trequire.Error(t, err)\n\t})\n}\n\nfunc TestEmitter_Finish(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\temitter, bm, ctrl := createEmitter(t)\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tkey             = \"some-key\"\n\t\t\tintVal   int64  = 1312\n\t\t\tdata     []byte = []byte(strconv.FormatInt(intVal, 10))\n\t\t\tmsgCount        = 200\n\t\t)\n\n\t\tbm.producer.EXPECT().Emit(emitter.topic, key, data).Return(NewPromise().finish(nil, nil)).MaxTimes(msgCount)\n\t\tbm.producer.EXPECT().Close().Return(nil)\n\n\t\tgo func() {\n\t\t\tfor i := 0; i < msgCount; i++ {\n\t\t\t\t_, err := emitter.Emit(key, intVal)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\t// promise errors are not checked here since they are expected\n\t\t\t}\n\t\t}()\n\n\t\ttime.Sleep(time.Nanosecond * 45)\n\t\terr := emitter.Finish()\n\t\trequire.NoError(t, err)\n\t})\n}\n"
        },
        {
          "name": "errors.go",
          "type": "blob",
          "size": 3.470703125,
          "content": "package goka\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\treflect \"reflect\"\n\t\"regexp\"\n\t\"strings\"\n\n\t\"github.com/go-stack/stack\"\n)\n\nvar (\n\terrBuildConsumer = \"error creating Kafka consumer: %v\"\n\terrBuildProducer = \"error creating Kafka producer: %v\"\n\terrApplyOptions  = \"error applying options: %v\"\n\terrTopicNotFound = errors.New(\"requested topic was not found\")\n)\n\n// this regex matches the package name + some hash info, if we're in gomod but not subpackages\n// examples which match\n// * github.com/lovoo/goka/processor.go\n// * github.com/lovoo/goka@v1.0.0/view.go\n// * github.com/some-fork/goka/view.go\n// examples which do not match\n// * github.com/something/else\n// * github.com/lovoo/goka/subpackage/file.go\n// this regex is used to filter out entries from the stack trace that origin\n// from the root-package of go (but not the subpackages, otherwise we would not see the stack in the example-tests)\n// reflect.TypeOf(Processor{}).PkgPath() returns (in the main repo) \"github.com/lovoo/goka\"\nvar gokaPackageRegex = regexp.MustCompile(fmt.Sprintf(`%s(?:@[^/]+)?/[^/]+$`, reflect.TypeOf(Processor{}).PkgPath()))\n\n// ErrVisitAborted indicates a call to VisitAll could not finish due to rebalance or processor shutdown\nvar ErrVisitAborted = errors.New(\"VisitAll aborted due to context cancel or rebalance\")\n\n// type to indicate that some non-transient error occurred while processing\n// the message, e.g. panic, decoding/encoding errors or invalid usage of context.\ntype errProcessing struct {\n\tpartition int32\n\terr       error\n}\n\nfunc (ec *errProcessing) Error() string {\n\treturn fmt.Sprintf(\"error processing message (partition=%d): %v\", ec.partition, ec.err)\n}\n\nfunc newErrProcessing(partition int32, err error) error {\n\treturn &errProcessing{\n\t\tpartition: partition,\n\t\terr:       err,\n\t}\n}\n\nfunc (ec *errProcessing) Unwrap() error {\n\treturn ec.err\n}\n\n// type to indicate that some non-transient error occurred while setting up the partitions on\n// rebalance.\ntype errSetup struct {\n\tpartition int32\n\terr       error\n}\n\nfunc (ec *errSetup) Error() string {\n\treturn fmt.Sprintf(\"error setting up (partition=%d): %v\", ec.partition, ec.err)\n}\n\nfunc (ec *errSetup) Unwrap() error {\n\treturn ec.err\n}\n\nfunc newErrSetup(partition int32, err error) error {\n\treturn &errSetup{\n\t\tpartition: partition,\n\t\terr:       err,\n\t}\n}\n\n// userStacktrace returns a formatted stack trace only containing the stack trace of the user-code\n// This is mainly used to properly format the error message built after a panic happened in a\n// processor-callback.\nfunc userStacktrace() []string {\n\ttrace := stack.Trace()\n\n\t// pop calls from the top that are either from runtime or goka's internal functions\n\tfor len(trace) > 0 {\n\t\tif strings.HasPrefix(fmt.Sprintf(\"%+s\", trace[0]), \"runtime/\") {\n\t\t\ttrace = trace[1:]\n\t\t\tcontinue\n\t\t}\n\t\tif gokaPackageRegex.MatchString(fmt.Sprintf(\"%+s\", trace[0])) {\n\t\t\ttrace = trace[1:]\n\t\t\tcontinue\n\t\t}\n\t\tbreak\n\t}\n\n\tvar lines []string\n\tfor _, frame := range trace {\n\n\t\t// as soon as we hit goka's internal package again we'll stop because from this point on we would\n\t\t// only print library or runtime frames\n\t\tif gokaPackageRegex.MatchString(fmt.Sprintf(\"%+s\", frame)) {\n\t\t\tbreak\n\t\t}\n\t\tlines = append(lines, fmt.Sprintf(\"%n\\n\\t%+s:%d\", frame, frame, frame))\n\t}\n\n\t// if we don't have anything unfiltered, it means there was an error within goka itself, so we should just\n\t// return the whole stack trace.\n\tif len(lines) == 0 {\n\t\tfor _, frame := range stack.Trace() {\n\t\t\tlines = append(lines, fmt.Sprintf(\"%n\\n\\t%+s:%d\", frame, frame, frame))\n\t\t}\n\t}\n\n\treturn lines\n}\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 1.40625,
          "content": "module github.com/lovoo/goka\n\ngo 1.20\n\nrequire (\n\tgithub.com/IBM/sarama v1.41.3\n\tgithub.com/go-stack/stack v1.8.1\n\tgithub.com/golang/mock v1.6.0\n\tgithub.com/gorilla/mux v1.8.0\n\tgithub.com/hashicorp/go-multierror v1.1.1\n\tgithub.com/stretchr/testify v1.8.4\n\tgithub.com/syndtr/goleveldb v1.0.0\n\tgolang.org/x/sync v0.4.0\n\tgopkg.in/redis.v5 v5.2.9\n\tgopkg.in/yaml.v2 v2.4.0\n)\n\nrequire (\n\tgithub.com/davecgh/go-spew v1.1.1 // indirect\n\tgithub.com/eapache/go-resiliency v1.4.0 // indirect\n\tgithub.com/eapache/go-xerial-snappy v0.0.0-20230731223053-c322873962e3 // indirect\n\tgithub.com/eapache/queue v1.1.0 // indirect\n\tgithub.com/golang/snappy v0.0.4 // indirect\n\tgithub.com/hashicorp/errwrap v1.1.0 // indirect\n\tgithub.com/hashicorp/go-uuid v1.0.3 // indirect\n\tgithub.com/jcmturner/aescts/v2 v2.0.0 // indirect\n\tgithub.com/jcmturner/dnsutils/v2 v2.0.0 // indirect\n\tgithub.com/jcmturner/gofork v1.7.6 // indirect\n\tgithub.com/jcmturner/gokrb5/v8 v8.4.4 // indirect\n\tgithub.com/jcmturner/rpc/v2 v2.0.3 // indirect\n\tgithub.com/klauspost/compress v1.16.7 // indirect\n\tgithub.com/kr/text v0.2.0 // indirect\n\tgithub.com/pierrec/lz4/v4 v4.1.18 // indirect\n\tgithub.com/pmezard/go-difflib v1.0.0 // indirect\n\tgithub.com/rcrowley/go-metrics v0.0.0-20201227073835-cf1acfcdf475 // indirect\n\tgithub.com/rogpeppe/go-internal v1.9.0 // indirect\n\tgolang.org/x/crypto v0.21.0 // indirect\n\tgolang.org/x/net v0.23.0 // indirect\n\tgopkg.in/yaml.v3 v3.0.1 // indirect\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 12.923828125,
          "content": "github.com/IBM/sarama v1.41.3 h1:MWBEJ12vHC8coMjdEXFq/6ftO6DUZnQlFYcxtOJFa7c=\ngithub.com/IBM/sarama v1.41.3/go.mod h1:Xxho9HkHd4K/MDUo/T/sOqwtX/17D33++E9Wib6hUdQ=\ngithub.com/creack/pty v1.1.9/go.mod h1:oKZEueFk5CKHvIhNR5MUki03XCEU+Q6VDXinZuGJ33E=\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/eapache/go-resiliency v1.4.0 h1:3OK9bWpPk5q6pbFAaYSEwD9CLUSHG8bnZuqX2yMt3B0=\ngithub.com/eapache/go-resiliency v1.4.0/go.mod h1:5yPzW0MIvSe0JDsv0v+DvcjEv2FyD6iZYSs1ZI+iQho=\ngithub.com/eapache/go-xerial-snappy v0.0.0-20230731223053-c322873962e3 h1:Oy0F4ALJ04o5Qqpdz8XLIpNA3WM/iSIXqxtqo7UGVws=\ngithub.com/eapache/go-xerial-snappy v0.0.0-20230731223053-c322873962e3/go.mod h1:YvSRo5mw33fLEx1+DlK6L2VV43tJt5Eyel9n9XBcR+0=\ngithub.com/eapache/queue v1.1.0 h1:YOEu7KNc61ntiQlcEeUIoDTJ2o8mQznoNvUhiigpIqc=\ngithub.com/eapache/queue v1.1.0/go.mod h1:6eCeP0CKFpHLu8blIFXhExK/dRa7WDZfr6jVFPTqq+I=\ngithub.com/fortytw2/leaktest v1.3.0 h1:u8491cBMTQ8ft8aeV+adlcytMZylmA5nnwwkRZjI8vw=\ngithub.com/fsnotify/fsnotify v1.4.7/go.mod h1:jwhsz4b93w/PPRr/qN1Yymfu8t87LnFCMoQvtojpjFo=\ngithub.com/go-stack/stack v1.8.1 h1:ntEHSVwIt7PNXNpgPmVfMrNhLtgjlmnZha2kOpuRiDw=\ngithub.com/go-stack/stack v1.8.1/go.mod h1:dcoOX6HbPZSZptuspn9bctJ+N/CnF5gGygcUP3XYfe4=\ngithub.com/golang/mock v1.6.0 h1:ErTB+efbowRARo13NNdxyJji2egdxLGQhRaY+DUumQc=\ngithub.com/golang/mock v1.6.0/go.mod h1:p6yTPP+5HYm5mzsMV8JkE6ZKdX+/wYM6Hr+LicevLPs=\ngithub.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/snappy v0.0.0-20180518054509-2e65f85255db/go.mod h1:/XxbfmMg8lxefKM7IXC3fBNl/7bRcc72aCRzEWrmP2Q=\ngithub.com/golang/snappy v0.0.4 h1:yAGX7huGHXlcLOEtBnF4w7FQwA26wojNCwOYAEhLjQM=\ngithub.com/golang/snappy v0.0.4/go.mod h1:/XxbfmMg8lxefKM7IXC3fBNl/7bRcc72aCRzEWrmP2Q=\ngithub.com/gorilla/mux v1.8.0 h1:i40aqfkR1h2SlN9hojwV5ZA91wcXFOvkdNIeFDP5koI=\ngithub.com/gorilla/mux v1.8.0/go.mod h1:DVbg23sWSpFRCP0SfiEN6jmj59UnW/n46BH5rLB71So=\ngithub.com/gorilla/securecookie v1.1.1/go.mod h1:ra0sb63/xPlUeL+yeDciTfxMRAA+MP+HVt/4epWDjd4=\ngithub.com/gorilla/sessions v1.2.1/go.mod h1:dk2InVEVJ0sfLlnXv9EAgkf6ecYs/i80K/zI+bUmuGM=\ngithub.com/hashicorp/errwrap v1.0.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=\ngithub.com/hashicorp/errwrap v1.1.0 h1:OxrOeh75EUXMY8TBjag2fzXGZ40LB6IKw45YeGUDY2I=\ngithub.com/hashicorp/errwrap v1.1.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=\ngithub.com/hashicorp/go-multierror v1.1.1 h1:H5DkEtf6CXdFp0N0Em5UCwQpXMWke8IA0+lD48awMYo=\ngithub.com/hashicorp/go-multierror v1.1.1/go.mod h1:iw975J/qwKPdAO1clOe2L8331t/9/fmwbPZ6JB6eMoM=\ngithub.com/hashicorp/go-uuid v1.0.2/go.mod h1:6SBZvOh/SIDV7/2o3Jml5SYk/TvGqwFJ/bN7x4byOro=\ngithub.com/hashicorp/go-uuid v1.0.3 h1:2gKiV6YVmrJ1i2CKKa9obLvRieoRGviZFL26PcT/Co8=\ngithub.com/hashicorp/go-uuid v1.0.3/go.mod h1:6SBZvOh/SIDV7/2o3Jml5SYk/TvGqwFJ/bN7x4byOro=\ngithub.com/hpcloud/tail v1.0.0 h1:nfCOvKYfkgYP8hkirhJocXT2+zOD8yUNjXaWfTlyFKI=\ngithub.com/hpcloud/tail v1.0.0/go.mod h1:ab1qPbhIpdTxEkNHXyeSf5vhxWSCs/tWer42PpOxQnU=\ngithub.com/jcmturner/aescts/v2 v2.0.0 h1:9YKLH6ey7H4eDBXW8khjYslgyqG2xZikXP0EQFKrle8=\ngithub.com/jcmturner/aescts/v2 v2.0.0/go.mod h1:AiaICIRyfYg35RUkr8yESTqvSy7csK90qZ5xfvvsoNs=\ngithub.com/jcmturner/dnsutils/v2 v2.0.0 h1:lltnkeZGL0wILNvrNiVCR6Ro5PGU/SeBvVO/8c/iPbo=\ngithub.com/jcmturner/dnsutils/v2 v2.0.0/go.mod h1:b0TnjGOvI/n42bZa+hmXL+kFJZsFT7G4t3HTlQ184QM=\ngithub.com/jcmturner/gofork v1.7.6 h1:QH0l3hzAU1tfT3rZCnW5zXl+orbkNMMRGJfdJjHVETg=\ngithub.com/jcmturner/gofork v1.7.6/go.mod h1:1622LH6i/EZqLloHfE7IeZ0uEJwMSUyQ/nDd82IeqRo=\ngithub.com/jcmturner/goidentity/v6 v6.0.1 h1:VKnZd2oEIMorCTsFBnJWbExfNN7yZr3EhJAxwOkZg6o=\ngithub.com/jcmturner/goidentity/v6 v6.0.1/go.mod h1:X1YW3bgtvwAXju7V3LCIMpY0Gbxyjn/mY9zx4tFonSg=\ngithub.com/jcmturner/gokrb5/v8 v8.4.4 h1:x1Sv4HaTpepFkXbt2IkL29DXRf8sOfZXo8eRKh687T8=\ngithub.com/jcmturner/gokrb5/v8 v8.4.4/go.mod h1:1btQEpgT6k+unzCwX1KdWMEwPPkkgBtP+F6aCACiMrs=\ngithub.com/jcmturner/rpc/v2 v2.0.3 h1:7FXXj8Ti1IaVFpSAziCZWNzbNuZmnvw/i6CqLNdWfZY=\ngithub.com/jcmturner/rpc/v2 v2.0.3/go.mod h1:VUJYCIDm3PVOEHw8sgt091/20OJjskO/YJki3ELg/Hc=\ngithub.com/klauspost/compress v1.16.7 h1:2mk3MPGNzKyxErAw8YaohYh69+pa4sIQSC0fPGCFR9I=\ngithub.com/klauspost/compress v1.16.7/go.mod h1:ntbaceVETuRiXiv4DpjP66DpAtAGkEQskQzEyD//IeE=\ngithub.com/kr/pretty v0.3.0 h1:WgNl7dwNpEZ6jJ9k1snq4pZsg7DOEN8hP9Xw0Tsjwk0=\ngithub.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=\ngithub.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=\ngithub.com/onsi/ginkgo v1.6.0/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\ngithub.com/onsi/ginkgo v1.7.0 h1:WSHQ+IS43OoUrWtD1/bbclrwK8TTH5hzp+umCiuxHgs=\ngithub.com/onsi/ginkgo v1.7.0/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\ngithub.com/onsi/gomega v1.4.3 h1:RE1xgDvH7imwFD45h+u2SgIfERHlS2yNG4DObb5BSKU=\ngithub.com/onsi/gomega v1.4.3/go.mod h1:ex+gbHU/CVuBBDIJjb2X0qEXbFg53c61hWP/1CpauHY=\ngithub.com/pierrec/lz4/v4 v4.1.18 h1:xaKrnTkyoqfh1YItXl56+6KJNVYWlEEPuAQW9xsplYQ=\ngithub.com/pierrec/lz4/v4 v4.1.18/go.mod h1:gZWDp/Ze/IJXGXf23ltt2EXimqmTUXEy0GFuRQyBid4=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/rcrowley/go-metrics v0.0.0-20201227073835-cf1acfcdf475 h1:N/ElC8H3+5XpJzTSTfLsJV/mx9Q9g7kxmchpfZyxgzM=\ngithub.com/rcrowley/go-metrics v0.0.0-20201227073835-cf1acfcdf475/go.mod h1:bCqnVzQkZxMG4s8nGwiZ5l3QUCyqpo9Y+/ZMZ9VjZe4=\ngithub.com/rogpeppe/go-internal v1.9.0 h1:73kH8U+JUqXU8lRuOHeVHaa/SZPifC7BkcraZVejAe8=\ngithub.com/rogpeppe/go-internal v1.9.0/go.mod h1:WtVeX8xhTBvf0smdhujwtBcq4Qrzq/fJaraNFVN+nFs=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/objx v0.4.0/go.mod h1:YvHI0jy2hoMjB+UWwv71VJQ9isScKT/TqJzVSSt89Yw=\ngithub.com/stretchr/objx v0.5.0/go.mod h1:Yh+to48EsGEfYuaHDzXPcE3xhTkx73EhmCGUpEOglKo=\ngithub.com/stretchr/testify v1.4.0/go.mod h1:j7eGeouHqKxXV5pUuKE4zz7dFj8WfuZ+81PSLYec5m4=\ngithub.com/stretchr/testify v1.7.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.8.0/go.mod h1:yNjHg4UonilssWZ8iaSj1OCr/vHnekPRkoO+kdMU+MU=\ngithub.com/stretchr/testify v1.8.1/go.mod h1:w2LPCIKwWwSfY2zedu0+kehJoqGctiVI29o6fzry7u4=\ngithub.com/stretchr/testify v1.8.4 h1:CcVxjf3Q8PM0mHUKJCdn+eZZtm5yQwehR5yeSVQQcUk=\ngithub.com/stretchr/testify v1.8.4/go.mod h1:sz/lmYIOXD/1dqDmKjjqLyZ2RngseejIcXlSw2iwfAo=\ngithub.com/syndtr/goleveldb v1.0.0 h1:fBdIW9lB4Iz0n9khmH8w27SJ3QEJ7+IgjPEwGSZiFdE=\ngithub.com/syndtr/goleveldb v1.0.0/go.mod h1:ZVVdQEZoIme9iO1Ch2Jdy24qqXrMMOU6lpPAyBWyWuQ=\ngithub.com/yuin/goldmark v1.3.5/go.mod h1:mwnBkeHKe2W/ZEtQ+71ViKU8L12m81fl3OWwC1Zlc8k=\ngithub.com/yuin/goldmark v1.4.13/go.mod h1:6yULJ656Px+3vBD8DxQVa3kxgyrAnzto9xy5taEt/CY=\ngolang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\ngolang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20210921155107-089bfa567519/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=\ngolang.org/x/crypto v0.6.0/go.mod h1:OFC/31mSvZgRz0V1QTNCzfAI1aIRzbiufJtkMIlEp58=\ngolang.org/x/crypto v0.21.0 h1:X31++rzVUdKhX5sWmSOFZxx8UW/ldWx55cbf08iNAMA=\ngolang.org/x/crypto v0.21.0/go.mod h1:0BP7YvVV9gBbVKyeTG0Gyn+gZm94bibOW5BjDEYAOMs=\ngolang.org/x/mod v0.4.2/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/mod v0.6.0-dev.0.20220419223038-86c51ed26bb4/go.mod h1:jJ57K6gSWd91VN4djpZkiMVwK6gcyfeH4XE8wZrZaV4=\ngolang.org/x/net v0.0.0-20180906233101-161cd47e91fd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200114155413-6afb5195e5aa/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20210226172049-e18ecbb05110/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=\ngolang.org/x/net v0.0.0-20210405180319-a5a99cb37ef4/go.mod h1:p54w0d4576C0XHj96bSt6lcn1PtDYWL6XObtHCRCNQM=\ngolang.org/x/net v0.0.0-20220722155237-a158d28d115b/go.mod h1:XRhObCWvk6IyKnWLug+ECip1KBveYUHfp+8e9klMJ9c=\ngolang.org/x/net v0.6.0/go.mod h1:2Tu9+aMcznHK/AK1HMvgo6xiTLG5rD5rZLDS+rp2Bjs=\ngolang.org/x/net v0.7.0/go.mod h1:2Tu9+aMcznHK/AK1HMvgo6xiTLG5rD5rZLDS+rp2Bjs=\ngolang.org/x/net v0.23.0 h1:7EYJ93RZ9vYSZAIb2x3lnuvqO5zneoD6IvWjuhfxjTs=\ngolang.org/x/net v0.23.0/go.mod h1:JKghWKKOSdJwpW2GEx0Ja7fmaKnMsbu+MWVZTokSYmg=\ngolang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20210220032951-036812b2e83c/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20220722155255-886fb9371eb4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.4.0 h1:zxkM55ReGkDlKSM+Fu41A+zmbZuaPVbGMzvvdUPznYQ=\ngolang.org/x/sync v0.4.0/go.mod h1:FU7BRWz2tNW+3quACPkgCx/L+uEAv1htQ0V83Z9Rj+Y=\ngolang.org/x/sys v0.0.0-20180909124046-d0be0721c37e/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20201119102817-f84b799fce68/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210330210617-4fbd30eecc44/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210510120138-977fb7262007/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20210615035016-665e8c7367d1/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220520151302-bc2c85ada10a/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220722155257-8c9f86f7a55f/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.5.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.18.0 h1:DBdB3niSjOA/O0blCZBqDefyWNYveAYMNF1Wum0DYQ4=\ngolang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=\ngolang.org/x/term v0.0.0-20210927222741-03fcf44c2211/go.mod h1:jbD1KX2456YbFQfuXm/mYQcufACuNUgVhRMnK/tPxf8=\ngolang.org/x/term v0.5.0/go.mod h1:jMB1sMXY+tzblOD4FWmEbocvup2/aLOaQEp7JmGp78k=\ngolang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.7/go.mod h1:u+2+/6zg+i71rQMx5EYifcz6MCKuco9NR6JIITiCfzQ=\ngolang.org/x/text v0.7.0/go.mod h1:mrYo+phRRbMaCq/xk9113O4dZlRixOauAjOtrjsXDZ8=\ngolang.org/x/text v0.14.0 h1:ScX5w1eTa3QqT8oi6+ziP7dTV1S2+ALU0bI+0zXKWiQ=\ngolang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.1.1/go.mod h1:o0xws9oXOQQZyjljx8fwUC0k7L1pTE6eaCbjGeHmOkk=\ngolang.org/x/tools v0.1.12/go.mod h1:hNGJHUnrk76NpqgfD5Aqm5Crs+Hm0VOH/i9J2+nxYbc=\ngolang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191011141410-1b5146add898/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=\ngopkg.in/fsnotify.v1 v1.4.7 h1:xOHLXZwVvI9hhs+cLKq5+I5onOuwQLhQwiu63xxlHs4=\ngopkg.in/fsnotify.v1 v1.4.7/go.mod h1:Tz8NjZHkW78fSQdbUxIjBTcgA1z1m8ZHf0WmKUhAMys=\ngopkg.in/redis.v5 v5.2.9 h1:MNZYOLPomQzZMfpN3ZtD1uyJ2IDonTTlxYiV/pEApiw=\ngopkg.in/redis.v5 v5.2.9/go.mod h1:6gtv0/+A4iM08kdRfocWYB3bLX2tebpNtfKlFT6H4mY=\ngopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7 h1:uRGJdciOHaEIrze2W8Q3AKkepLTh2hOroT7a+7czfdQ=\ngopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7/go.mod h1:dt/ZhP58zS4L8KSrWDmTeBkI65Dw0HsyUHuEVlX15mw=\ngopkg.in/yaml.v2 v2.2.1/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.4.0 h1:D8xgwECY7CYvx+Y2n4sBz93Jn9JRvxdiyyo8CTfuKaY=\ngopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=\ngopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\n"
        },
        {
          "name": "graph.go",
          "type": "blob",
          "size": 13.5537109375,
          "content": "package goka\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n)\n\nvar (\n\tdefaultTableSuffix = \"-table\"\n\tdefaultLoopSuffix  = \"-loop\"\n\n\ttableSuffix = defaultTableSuffix\n\tloopSuffix  = defaultLoopSuffix\n)\n\n// SetTableSuffix changes `tableSuffix` which is a suffix for table topic.\n// Use it to modify table's suffix to otherwise in case you cannot use the default suffix.\nfunc SetTableSuffix(suffix string) {\n\ttableSuffix = suffix\n}\n\n// SetLoopSuffix changes `loopSuffix` which is a suffix for loop topic of group.\n// Use it to modify loop topic's suffix to otherwise in case you cannot use the default suffix.\nfunc SetLoopSuffix(suffix string) {\n\tloopSuffix = suffix\n}\n\n// ResetSuffixes reset both `loopSuffix` and `tableSuffix` to their default value.\n// This function is helpful when there are multiple testcases, so you can do clean-up any change for suffixes.\nfunc ResetSuffixes() {\n\tloopSuffix = defaultLoopSuffix\n\ttableSuffix = defaultTableSuffix\n}\n\n// Stream is the name of an event stream topic in Kafka, ie, a topic with\n// cleanup.policy=delete\ntype Stream string\n\n// Streams is a slice of Stream names.\ntype Streams []Stream\n\n// Table is the name of a table topic in Kafka, ie, a topic with\n// cleanup.policy=compact\ntype Table string\n\n// Group is the name of a consumer group in Kafka and represents a processor\n// group in Goka. A processor group may have a group table and a group loopback\n// stream. By default, the group table is named <group>-table and the loopback\n// stream <group>-loop.\ntype Group string\n\n// GroupGraph is the specification of a processor group. It contains all input,\n// output, and any other topic from which and into which the processor group\n// may consume or produce events. Each of these links to Kafka is called Edge.\ntype GroupGraph struct {\n\t// the group marks multiple processor instances to be long together\n\tgroup string\n\n\t// the edges define the group graph\n\tinputTables   []Edge\n\tcrossTables   []Edge\n\tinputStreams  []Edge\n\toutputStreams []Edge\n\tloopStream    []Edge\n\tgroupTable    []Edge\n\tvisitors      []Edge\n\n\t// those fields cache the info from above edges or are used to avoid naming/codec collisions\n\tcodecs    map[string]Codec\n\tcallbacks map[string]ProcessCallback\n\n\toutputStreamTopics map[Stream]struct{}\n\n\tjoinCheck map[string]bool\n}\n\n// Group returns the group name.\nfunc (gg *GroupGraph) Group() Group {\n\treturn Group(gg.group)\n}\n\n// InputStreams returns all input stream edges of the group.\nfunc (gg *GroupGraph) InputStreams() Edges {\n\treturn gg.inputStreams\n}\n\n// JointTables retuns all joint table edges of the group.\nfunc (gg *GroupGraph) JointTables() Edges {\n\treturn gg.inputTables\n}\n\n// LookupTables retuns all lookup table edges  of the group.\nfunc (gg *GroupGraph) LookupTables() Edges {\n\treturn gg.crossTables\n}\n\n// LoopStream returns the loopback edge of the group.\nfunc (gg *GroupGraph) LoopStream() Edge {\n\t// only 1 loop stream is valid\n\tif len(gg.loopStream) > 0 {\n\t\treturn gg.loopStream[0]\n\t}\n\treturn nil\n}\n\n// GroupTable returns the group table edge of the group.\nfunc (gg *GroupGraph) GroupTable() Edge {\n\t// only 1 group table is valid\n\tif len(gg.groupTable) > 0 {\n\t\treturn gg.groupTable[0]\n\t}\n\treturn nil\n}\n\n// OutputStreams returns the output stream edges of the group.\nfunc (gg *GroupGraph) OutputStreams() Edges {\n\treturn gg.outputStreams\n}\n\n// AllEdges returns a list of all edges for the group graph.\n// This allows to modify a graph by cloning it's edges into a new one.\n//\n//  var existing Graph\n//  edges := existiting.AllEdges()\n//  // modify edges as required\n//  // recreate the modifiedg raph\n//  newGraph := DefineGroup(existing.Groug(), edges...)\nfunc (gg *GroupGraph) AllEdges() Edges {\n\treturn chainEdges(\n\t\tgg.inputTables,\n\t\tgg.crossTables,\n\t\tgg.inputStreams,\n\t\tgg.outputStreams,\n\t\tgg.loopStream,\n\t\tgg.groupTable,\n\t\tgg.visitors)\n}\n\n// returns whether the passed topic is a valid group output topic\nfunc (gg *GroupGraph) isOutputTopic(topic Stream) bool {\n\t_, ok := gg.outputStreamTopics[topic]\n\treturn ok\n}\n\n// inputs returns all input topics (tables and streams)\nfunc (gg *GroupGraph) inputs() Edges {\n\treturn chainEdges(gg.inputStreams, gg.inputTables, gg.crossTables)\n}\n\n// copartitioned returns all copartitioned topics (joint tables and input streams)\nfunc (gg *GroupGraph) copartitioned() Edges {\n\treturn chainEdges(gg.inputStreams, gg.inputTables)\n}\n\nfunc (gg *GroupGraph) codec(topic string) Codec {\n\treturn gg.codecs[topic]\n}\n\nfunc (gg *GroupGraph) callback(topic string) ProcessCallback {\n\treturn gg.callbacks[topic]\n}\n\nfunc (gg *GroupGraph) joint(topic string) bool {\n\treturn gg.joinCheck[topic]\n}\n\n// DefineGroup creates a group graph with a given group name and a list of\n// edges.\nfunc DefineGroup(group Group, edges ...Edge) *GroupGraph {\n\tgg := GroupGraph{\n\t\tgroup:              string(group),\n\t\tcodecs:             make(map[string]Codec),\n\t\tcallbacks:          make(map[string]ProcessCallback),\n\t\tjoinCheck:          make(map[string]bool),\n\t\toutputStreamTopics: make(map[Stream]struct{}),\n\t}\n\n\tfor _, e := range edges {\n\t\tswitch e := e.(type) {\n\t\tcase inputStreams:\n\t\t\tfor _, input := range e {\n\t\t\t\tgg.validateInputTopic(input.Topic())\n\t\t\t\tinputStr := input.(*inputStream)\n\t\t\t\tgg.codecs[input.Topic()] = input.Codec()\n\t\t\t\tgg.callbacks[input.Topic()] = inputStr.cb\n\t\t\t\tgg.inputStreams = append(gg.inputStreams, inputStr)\n\t\t\t}\n\t\tcase *inputStream:\n\t\t\tgg.validateInputTopic(e.Topic())\n\t\t\tgg.codecs[e.Topic()] = e.Codec()\n\t\t\tgg.callbacks[e.Topic()] = e.cb\n\t\t\tgg.inputStreams = append(gg.inputStreams, e)\n\t\tcase *loopStream:\n\t\t\te.setGroup(group)\n\t\t\tgg.codecs[e.Topic()] = e.Codec()\n\t\t\tgg.callbacks[e.Topic()] = e.cb\n\t\t\tgg.loopStream = append(gg.loopStream, e)\n\t\tcase *outputStream:\n\t\t\tgg.codecs[e.Topic()] = e.Codec()\n\t\t\tgg.outputStreams = append(gg.outputStreams, e)\n\t\t\tgg.outputStreamTopics[Stream(e.Topic())] = struct{}{}\n\t\tcase *inputTable:\n\t\t\tgg.codecs[e.Topic()] = e.Codec()\n\t\t\tgg.inputTables = append(gg.inputTables, e)\n\t\t\tgg.joinCheck[e.Topic()] = true\n\t\tcase *crossTable:\n\t\t\tgg.codecs[e.Topic()] = e.Codec()\n\t\t\tgg.crossTables = append(gg.crossTables, e)\n\t\tcase *groupTable:\n\t\t\te.setGroup(group)\n\t\t\tgg.codecs[e.Topic()] = e.Codec()\n\t\t\tgg.groupTable = append(gg.groupTable, e)\n\t\tcase *visitor:\n\t\t\tgg.visitors = append(gg.visitors, e)\n\t\t}\n\t}\n\n\treturn &gg\n}\n\nfunc (gg *GroupGraph) validateInputTopic(topic string) {\n\tif topic == \"\" {\n\t\tpanic(\"Input topic cannot be empty. This will not work.\")\n\t}\n\n\tif _, exists := gg.callbacks[topic]; exists {\n\t\tpanic(fmt.Errorf(\"Callback for topic %s already exists. It is illegal to consume a topic twice\", topic))\n\t}\n}\n\n// Validate validates the group graph and returns an error if invalid.\n// Main validation checks are:\n// - at most one loopback stream edge is allowed\n// - at most one group table edge is allowed\n// - at least one input stream is required\n// - table and loopback topics cannot be used in any other edge.\nfunc (gg *GroupGraph) Validate() error {\n\tif len(gg.loopStream) > 1 {\n\t\treturn errors.New(\"more than one loop stream in group graph\")\n\t}\n\tif len(gg.groupTable) > 1 {\n\t\treturn errors.New(\"more than one group table in group graph\")\n\t}\n\tif len(gg.inputStreams) == 0 {\n\t\treturn errors.New(\"no input stream in group graph\")\n\t}\n\tfor _, t := range chainEdges(gg.outputStreams, gg.inputStreams, gg.inputTables, gg.crossTables) {\n\t\tif t.Topic() == loopName(gg.Group()) {\n\t\t\treturn errors.New(\"should not directly use loop stream\")\n\t\t}\n\t\tif t.Topic() == tableName(gg.Group()) {\n\t\t\treturn errors.New(\"should not directly use group table\")\n\t\t}\n\t}\n\tif len(gg.visitors) > 0 && len(gg.groupTable) == 0 {\n\t\treturn fmt.Errorf(\"visitors cannot be used in a stateless processor\")\n\t}\n\treturn nil\n}\n\n// Edge represents a topic in Kafka and the corresponding codec to encode and\n// decode the messages of that topic.\ntype Edge interface {\n\tString() string\n\tTopic() string\n\tCodec() Codec\n}\n\n// Edges is a slice of edge objects.\ntype Edges []Edge\n\n// chainEdges chains edges together to avoid error-prone\n// append(edges, moreEdges...) constructs in the graph\nfunc chainEdges(edgeList ...Edges) Edges {\n\tvar sum int\n\tfor _, edges := range edgeList {\n\t\tsum += len(edges)\n\t}\n\tchained := make(Edges, 0, sum)\n\n\tfor _, edges := range edgeList {\n\t\tchained = append(chained, edges...)\n\t}\n\treturn chained\n}\n\n// Topics returns the names of the topics of the edges.\nfunc (e Edges) Topics() []string {\n\tvar t []string\n\tfor _, i := range e {\n\t\tt = append(t, i.Topic())\n\t}\n\treturn t\n}\n\ntype topicDef struct {\n\tname  string\n\tcodec Codec\n}\n\nfunc (t *topicDef) Topic() string {\n\treturn t.name\n}\n\nfunc (t *topicDef) String() string {\n\treturn fmt.Sprintf(\"%s/%T\", t.name, t.codec)\n}\n\nfunc (t *topicDef) Codec() Codec {\n\treturn t.codec\n}\n\ntype inputStream struct {\n\t*topicDef\n\tcb ProcessCallback\n}\n\n// Input represents an edge of an input stream topic. The edge\n// specifies the topic name, its codec and the ProcessorCallback used to\n// process it. The topic has to be copartitioned with any other input stream of\n// the group and with the group table.\n// The group starts reading the topic from the newest offset.\nfunc Input(topic Stream, c Codec, cb ProcessCallback) Edge {\n\treturn &inputStream{&topicDef{string(topic), c}, cb}\n}\n\ntype inputStreams Edges\n\nfunc (is inputStreams) String() string {\n\tif is == nil {\n\t\treturn \"empty input streams\"\n\t}\n\n\treturn fmt.Sprintf(\"input streams: %s/%T\", is.Topic(), is.Codec())\n}\n\nfunc (is inputStreams) Topic() string {\n\tif is == nil {\n\t\treturn \"\"\n\t}\n\tvar topics []string\n\n\tfor _, stream := range is {\n\t\ttopics = append(topics, stream.Topic())\n\t}\n\treturn strings.Join(topics, \",\")\n}\n\nfunc (is inputStreams) Codec() Codec {\n\tif is == nil {\n\t\treturn nil\n\t}\n\treturn is[0].Codec()\n}\n\n// Inputs creates edges of multiple input streams sharing the same\n// codec and callback.\nfunc Inputs(topics Streams, c Codec, cb ProcessCallback) Edge {\n\tif len(topics) == 0 {\n\t\treturn nil\n\t}\n\tvar edges Edges\n\tfor _, topic := range topics {\n\t\tedges = append(edges, Input(topic, c, cb))\n\t}\n\treturn inputStreams(edges)\n}\n\ntype visitor struct {\n\tname string\n\tcb   ProcessCallback\n}\n\nfunc (m *visitor) Topic() string {\n\treturn m.name\n}\nfunc (m *visitor) Codec() Codec {\n\treturn nil\n}\nfunc (m *visitor) String() string {\n\treturn fmt.Sprintf(\"visitor %s\", m.name)\n}\n\n// Visitor adds a visitor edge to the processor. This allows to iterate over the whole processor state\n// while running. Note that this can block rebalance or processor shutdown.\n// EXPERIMENTAL! This feature is not fully tested and might trigger unknown bugs. Be careful!\nfunc Visitor(name string, cb ProcessCallback) Edge {\n\treturn &visitor{\n\t\tname: name,\n\t\tcb:   cb,\n\t}\n}\n\ntype loopStream inputStream\n\n// Loop represents the edge of the loopback topic of the group. The edge\n// specifies the codec of the messages in the topic and ProcesCallback to\n// process the messages of the topic. Context.Loopback() is used to write\n// messages into this topic from any callback of the group.\nfunc Loop(c Codec, cb ProcessCallback) Edge {\n\treturn &loopStream{&topicDef{codec: c}, cb}\n}\n\nfunc (s *loopStream) setGroup(group Group) {\n\ts.topicDef.name = loopName(group)\n}\n\ntype inputTable struct {\n\t*topicDef\n}\n\n// Join represents an edge of a copartitioned, log-compacted table topic. The\n// edge specifies the topic name and the codec of the messages of the topic.\n// The group starts reading the topic from the oldest offset.\n// The processing of input streams is blocked until all partitions of the table\n// are recovered.\nfunc Join(topic Table, c Codec) Edge {\n\treturn &inputTable{&topicDef{string(topic), c}}\n}\n\ntype crossTable struct {\n\t*topicDef\n}\n\n// Lookup represents an edge of a non-copartitioned, log-compacted table\n// topic. The edge specifies the topic name and the codec of the messages of\n// the topic.  The group starts reading the topic from the oldest offset.\n// The processing of input streams is blocked until the table is fully\n// recovered.\nfunc Lookup(topic Table, c Codec) Edge {\n\treturn &crossTable{&topicDef{string(topic), c}}\n}\n\ntype groupTable struct {\n\t*topicDef\n}\n\n// Persist represents the edge of the group table, which is log-compacted and\n// copartitioned with the input streams.\n// Without Persist, calls to ctx.Value or ctx.SetValue in the consume callback will\n// fail and lead to shutdown of the processor.\n//\n// This edge specifies the codec of the\n// messages in the topic, ie, the codec of the values of the table.\n// The processing of input streams is blocked until all partitions of the group\n// table are recovered.\n//\n// The topic name is derived from the group name by appending \"-table\".\nfunc Persist(c Codec) Edge {\n\treturn &groupTable{&topicDef{codec: c}}\n}\n\nfunc (t *groupTable) setGroup(group Group) {\n\tt.topicDef.name = string(GroupTable(group))\n}\n\ntype outputStream struct {\n\t*topicDef\n}\n\n// Output represents an edge of an output stream topic. The edge\n// specifies the topic name and the codec of the messages of the topic.\n// Context.Emit() only emits messages into Output edges defined in the group\n// graph.\n// The topic does not have to be copartitioned with the input streams.\nfunc Output(topic Stream, c Codec) Edge {\n\treturn &outputStream{&topicDef{string(topic), c}}\n}\n\n// GroupTable returns the name of the group table of group.\nfunc GroupTable(group Group) Table {\n\treturn Table(tableName(group))\n}\n\nfunc tableName(group Group) string {\n\treturn string(group) + tableSuffix\n}\n\n// loopName returns the name of the loop topic of group.\nfunc loopName(group Group) string {\n\treturn string(group) + loopSuffix\n}\n\n// StringsToStreams is a simple cast/conversion functions that allows to pass a slice\n// of strings as a slice of Stream (Streams)\n// Avoids the boilerplate loop over the string array that would be necessary otherwise.\nfunc StringsToStreams(strings ...string) Streams {\n\tstreams := make(Streams, 0, len(strings))\n\n\tfor _, str := range strings {\n\t\tstreams = append(streams, Stream(str))\n\t}\n\treturn streams\n}\n"
        },
        {
          "name": "graph_test.go",
          "type": "blob",
          "size": 5.015625,
          "content": "package goka\n\nimport (\n\t\"fmt\"\n\t\"reflect\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/lovoo/goka/codec\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nvar (\n\tc  = new(codec.String)\n\tcb = func(ctx Context, msg interface{}) {}\n)\n\nfunc TestGroupGraph_Validate(t *testing.T) {\n\tg := DefineGroup(\"group\")\n\terr := g.Validate()\n\trequire.Contains(t, err.Error(), \"no input\")\n\n\tg = DefineGroup(\"group\",\n\t\tInput(\"input-topic\", c, cb))\n\terr = g.Validate()\n\trequire.NoError(t, err)\n\n\tg = DefineGroup(\"group\",\n\t\tInput(\"input-topic\", c, cb),\n\t\tLoop(c, cb),\n\t\tLoop(c, cb),\n\t)\n\terr = g.Validate()\n\trequire.Contains(t, err.Error(), \"more than one loop\")\n\n\tg = DefineGroup(\"group\",\n\t\tInput(\"input-topic\", c, cb),\n\t\tPersist(c),\n\t\tPersist(c),\n\t)\n\terr = g.Validate()\n\trequire.Contains(t, err.Error(), \"more than one group table\")\n\n\tg = DefineGroup(\"group\",\n\t\tInput(Stream(tableName(\"group\")), c, cb),\n\t\tPersist(c),\n\t)\n\terr = g.Validate()\n\trequire.Contains(t, err.Error(), \"group table\")\n\n\tg = DefineGroup(\"group\",\n\t\tInput(Stream(loopName(\"group\")), c, cb),\n\t\tLoop(c, cb),\n\t)\n\terr = g.Validate()\n\trequire.Contains(t, err.Error(), \"loop stream\")\n\n\tg = DefineGroup(\"group\",\n\t\tInput(\"input-topic\", c, cb),\n\t\tJoin(Table(loopName(\"group\")), c),\n\t)\n\terr = g.Validate()\n\trequire.Contains(t, err.Error(), \"loop stream\")\n\n\tg = DefineGroup(\"group\",\n\t\tInput(\"input-topic\", c, cb),\n\t\tOutput(Stream(loopName(\"group\")), c),\n\t)\n\terr = g.Validate()\n\trequire.Contains(t, err.Error(), \"loop stream\")\n\n\tg = DefineGroup(\"group\",\n\t\tInput(\"input-topic\", c, cb),\n\t\tLookup(Table(loopName(\"group\")), c),\n\t)\n\terr = g.Validate()\n\trequire.Contains(t, err.Error(), \"loop stream\")\n}\n\nfunc TestGroupGraph_chainEdges(t *testing.T) {\n\trequire.Empty(t, chainEdges())\n\trequire.Empty(t, chainEdges(Edges{}, Edges{}))\n\trequire.Equal(t, Edges{Join(\"a\", nil)}, chainEdges(Edges{Join(\"a\", nil)}, Edges{}))\n\trequire.Equal(t, Edges{Join(\"a\", nil), Join(\"a\", nil), Join(\"b\", nil)}, chainEdges(Edges{Join(\"a\", nil)}, Edges{Join(\"a\", nil), Join(\"b\", nil)}))\n}\n\nfunc TestGroupGraph_codec(t *testing.T) {\n\tg := DefineGroup(\"group\",\n\t\tInput(\"input-topic\", c, cb),\n\t\tInputs(Streams{\"input-topic2\", \"input-topic3\"}, c, cb),\n\t)\n\n\tfor _, topic := range []string{\"input-topic\", \"input-topic2\", \"input-topic3\"} {\n\t\tcodec := g.codec(topic)\n\t\trequire.Equal(t, c, codec)\n\t}\n}\n\nfunc TestGroupGraph_callback(t *testing.T) {\n\tg := DefineGroup(\"group\",\n\t\tInput(\"input-topic\", c, cb),\n\t\tInputs(Streams{\"input-topic2\", \"input-topic3\"}, c, cb),\n\t)\n\n\tfor _, topic := range []string{\"input-topic\", \"input-topic2\", \"input-topic3\"} {\n\t\tcallback := g.callback(topic)\n\t\trequire.True(t, reflect.ValueOf(callback).Pointer() == reflect.ValueOf(cb).Pointer())\n\t}\n}\n\nfunc TestGroupGraph_getters(t *testing.T) {\n\tg := DefineGroup(\"group\",\n\t\tInput(\"t1\", c, cb),\n\t\tInput(\"t2\", c, cb),\n\t\tOutput(\"t3\", c),\n\t\tOutput(\"t4\", c),\n\t\tOutput(\"t5\", c),\n\t\tInputs(Streams{\"t6\", \"t7\"}, c, cb),\n\t)\n\trequire.True(t, g.Group() == \"group\")\n\trequire.True(t, len(g.InputStreams()) == 4)\n\trequire.True(t, len(g.OutputStreams()) == 3)\n\trequire.True(t, g.LoopStream() == nil)\n\n\tg = DefineGroup(\"group\",\n\t\tInput(\"t1\", c, cb),\n\t\tInput(\"t2\", c, cb),\n\t\tOutput(\"t3\", c),\n\t\tOutput(\"t4\", c),\n\t\tOutput(\"t5\", c),\n\t\tLoop(c, cb),\n\t)\n\trequire.True(t, len(g.InputStreams()) == 2)\n\trequire.True(t, len(g.OutputStreams()) == 3)\n\trequire.True(t, g.GroupTable() == nil)\n\trequire.Equal(t, loopName(\"group\"), g.LoopStream().Topic())\n\n\tg = DefineGroup(\"group\",\n\t\tInput(\"t1\", c, cb),\n\t\tInput(\"t2\", c, cb),\n\t\tOutput(\"t3\", c),\n\t\tOutput(\"t4\", c),\n\t\tOutput(\"t5\", c),\n\t\tLoop(c, cb),\n\t\tJoin(\"a1\", c),\n\t\tJoin(\"a2\", c),\n\t\tJoin(\"a3\", c),\n\t\tJoin(\"a4\", c),\n\t\tLookup(\"b1\", c),\n\t\tLookup(\"b2\", c),\n\t\tPersist(c),\n\t)\n\trequire.True(t, len(g.InputStreams()) == 2)\n\trequire.True(t, len(g.OutputStreams()) == 3)\n\trequire.True(t, len(g.JointTables()) == 4)\n\trequire.True(t, len(g.LookupTables()) == 2)\n\trequire.Equal(t, tableName(\"group\"), g.GroupTable().Topic())\n}\n\nfunc TestGroupGraph_Inputs(t *testing.T) {\n\ttopics := Inputs(Streams{\"a\", \"b\", \"c\"}, c, cb)\n\trequire.Equal(t, \"a,b,c\", topics.Topic())\n\trequire.True(t, strings.Contains(topics.String(), \"a,b,c/*codec.String\"))\n}\n\nfunc TestGroupGraph_UpdateSuffixes(t *testing.T) {\n\tSetLoopSuffix(\"-loop1\")\n\tSetTableSuffix(\"-table1\")\n\n\tg := DefineGroup(\"group\",\n\t\tInput(\"input-topic\", c, cb),\n\t\tPersist(c),\n\t\tPersist(c),\n\t)\n\n\trequire.Equal(t, \"group-table1\", tableName(g.Group()))\n\trequire.Equal(t, \"group-loop1\", loopName(g.Group()))\n\n\tResetSuffixes()\n\n\trequire.Equal(t, fmt.Sprintf(\"group%s\", defaultTableSuffix), tableName(g.Group()))\n\trequire.Equal(t, fmt.Sprintf(\"group%s\", defaultLoopSuffix), loopName(g.Group()))\n}\n\nfunc TestStringsToStreams(t *testing.T) {\n\tinputTopics := []string{\n\t\t\"input1\",\n\t\t\"input2\",\n\t}\n\n\tstreams := StringsToStreams(inputTopics...)\n\trequire.Equal(t, Stream(\"input1\"), streams[0])\n\trequire.Equal(t, Stream(\"input2\"), streams[1])\n}\n\nfunc ExampleStringsToStreams() {\n\tinputTopics := []string{\n\t\t\"input1\",\n\t\t\"input2\",\n\t\t\"input3\",\n\t}\n\n\t// use it, e.g. in the Inputs-Edge in the group graph\n\tgraph := DefineGroup(\"group\",\n\t\tInputs(StringsToStreams(inputTopics...), new(codec.String), func(ctx Context, msg interface{}) {}),\n\t)\n\t_ = graph\n}\n"
        },
        {
          "name": "header_test.go",
          "type": "blob",
          "size": 0.8544921875,
          "content": "package goka\n\nimport (\n\t\"testing\"\n)\n\nfunc TestHeaders_Merged(t *testing.T) {\n\th1 := Headers{\n\t\t\"key1\": []byte(\"val1\"),\n\t}\n\th2 := Headers{\n\t\t\"key1\": []byte(\"val1b\"),\n\t\t\"key2\": []byte(\"val2\"),\n\t}\n\tmerged := h1.Merged(h2)\n\n\tif len(h1) != 1 || string(h1[\"key1\"]) != \"val1\" {\n\t\tt.Errorf(\"Merged failed: receiver was modified\")\n\t}\n\n\tif len(h2) != 2 || string(h2[\"key1\"]) != \"val1b\" || string(h2[\"key2\"]) != \"val2\" {\n\t\tt.Errorf(\"Merged failed: argument was modified\")\n\t}\n\n\tif len(merged) != 2 {\n\t\tt.Errorf(\"Merged failed: expected %d keys, but found %d\", 2, len(merged))\n\t}\n\n\tif string(merged[\"key1\"]) != \"val1b\" {\n\t\tt.Errorf(\"Merged failed: expected %q for key %q, but found %q\",\n\t\t\t\"val1b\", \"key1\", string(merged[\"key1\"]))\n\t}\n\n\tif string(merged[\"key2\"]) != \"val2\" {\n\t\tt.Errorf(\"Merged failed: expected %q for key %q, but found %q\",\n\t\t\t\"val2\", \"key2\", string(merged[\"key2\"]))\n\t}\n}\n"
        },
        {
          "name": "headers.go",
          "type": "blob",
          "size": 1.990234375,
          "content": "package goka\n\nimport (\n\t\"github.com/IBM/sarama\"\n)\n\n// Headers represents custom message headers with a convenient interface.\ntype Headers map[string][]byte\n\n// HeadersFromSarama converts sarama headers to goka's type.\nfunc HeadersFromSarama(saramaHeaders []*sarama.RecordHeader) Headers {\n\theaders := Headers{}\n\tfor _, rec := range saramaHeaders {\n\t\theaders[string(rec.Key)] = rec.Value\n\t}\n\treturn headers\n}\n\n// Merged returns a new instance with all headers merged. Later keys override earlier ones.\n// Handles a nil receiver and nil arguments without panics.\n// If all headers are empty, nil is returned to allow using directly in emit functions.\nfunc (h Headers) Merged(headersList ...Headers) Headers {\n\t// optimize for headerless processors.\n\tif len(h) == 0 && allEmpty(headersList...) {\n\t\treturn nil\n\t}\n\n\tmerged := Headers{}\n\tfor k, v := range h {\n\t\tmerged[k] = v\n\t}\n\n\tfor _, headers := range headersList {\n\t\tfor k, v := range headers {\n\t\t\tmerged[k] = v\n\t\t}\n\t}\n\n\tif len(merged) == 0 {\n\t\treturn nil\n\t}\n\n\treturn merged\n}\n\n// ToSarama converts the headers to a slice of sarama.RecordHeader.\n// If called on a nil receiver returns nil.\nfunc (h Headers) ToSarama() []sarama.RecordHeader {\n\tif h == nil {\n\t\treturn nil\n\t}\n\n\trecordHeaders := make([]sarama.RecordHeader, 0, len(h))\n\tfor key, value := range h {\n\t\trecordHeaders = append(recordHeaders,\n\t\t\tsarama.RecordHeader{\n\t\t\t\tKey:   []byte(key),\n\t\t\t\tValue: value,\n\t\t\t})\n\t}\n\treturn recordHeaders\n}\n\n// ToSaramaPtr converts the headers to a slice of pointers to sarama.RecordHeader.\n// If called on a nil receiver returns nil.\nfunc (h Headers) ToSaramaPtr() []*sarama.RecordHeader {\n\tif h == nil {\n\t\treturn nil\n\t}\n\n\trecordHeaders := make([]*sarama.RecordHeader, 0, len(h))\n\tfor key, value := range h {\n\t\trecordHeaders = append(recordHeaders,\n\t\t\t&sarama.RecordHeader{\n\t\t\t\tKey:   []byte(key),\n\t\t\t\tValue: value,\n\t\t\t})\n\t}\n\treturn recordHeaders\n}\n\nfunc allEmpty(headersList ...Headers) bool {\n\tfor _, headers := range headersList {\n\t\tif len(headers) != 0 {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n"
        },
        {
          "name": "integrationtest",
          "type": "tree",
          "content": null
        },
        {
          "name": "iterator.go",
          "type": "blob",
          "size": 1.9208984375,
          "content": "package goka\n\nimport (\n\t\"github.com/lovoo/goka/storage\"\n)\n\n// Iterator allows one to iterate over the keys of a view.\ntype Iterator interface {\n\t// Next advances the iterator to the next KV-pair. Err should be called\n\t// after Next returns false to check whether the iteration finished\n\t// from exhaustion or was aborted due to an error.\n\tNext() bool\n\t// Err returns the error that stopped the iteration if any.\n\tErr() error\n\t// Return the key of the current item\n\tKey() string\n\t// Return the value of the current item\n\t// This value is already decoded with the view's codec (or nil, if it's nil)\n\tValue() (interface{}, error)\n\t// Release the iterator. After release, the iterator is not usable anymore\n\tRelease()\n\t// Seek moves the iterator to the begining of a key-value pair sequence that\n\t// is greater or equal to the given key. It returns whether at least one of\n\t// such key-value pairs exist. If true is returned, Key/Value must be called\n\t// immediately to get the first item. Calling Next immediately after a successful\n\t// seek will effectively skip an item in the iterator.\n\tSeek(key string) bool\n}\n\ntype iterator struct {\n\titer  storage.Iterator\n\tcodec Codec\n}\n\n// Next advances the iterator to the next key.\nfunc (i *iterator) Next() bool {\n\treturn i.iter.Next()\n}\n\n// Key returns the current key.\nfunc (i *iterator) Key() string {\n\treturn string(i.iter.Key())\n}\n\n// Value returns the current value decoded by the codec of the storage.\nfunc (i *iterator) Value() (interface{}, error) {\n\tdata, err := i.iter.Value()\n\tif err != nil {\n\t\treturn nil, err\n\t} else if data == nil {\n\t\treturn nil, nil\n\t}\n\treturn i.codec.Decode(data)\n}\n\n// Err returns the possible iteration error.\nfunc (i *iterator) Err() error {\n\treturn i.iter.Err()\n}\n\n// Releases releases the iterator. The iterator is not usable anymore after calling Release.\nfunc (i *iterator) Release() {\n\ti.iter.Release()\n}\n\nfunc (i *iterator) Seek(key string) bool {\n\treturn i.iter.Seek([]byte(key))\n}\n"
        },
        {
          "name": "iterator_test.go",
          "type": "blob",
          "size": 1.296875,
          "content": "package goka\n\nimport (\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com/lovoo/goka/codec\"\n\t\"github.com/lovoo/goka/storage\"\n\t\"github.com/stretchr/testify/require\"\n\t\"github.com/syndtr/goleveldb/leveldb\"\n)\n\nfunc TestIterator(t *testing.T) {\n\ttmpdir, err := os.MkdirTemp(\"\", \"goka_storage_TestIterator\")\n\trequire.NoError(t, err)\n\n\tdb, err := leveldb.OpenFile(tmpdir, nil)\n\trequire.NoError(t, err)\n\n\tst, err := storage.New(db)\n\trequire.NoError(t, err)\n\n\tkv := map[string]string{\n\t\t\"key-1\": \"val-1\",\n\t\t\"key-2\": \"val-2\",\n\t\t\"key-3\": \"val-3\",\n\t}\n\n\tfor k, v := range kv {\n\t\trequire.NoError(t, st.Set(k, []byte(v)))\n\t}\n\n\trequire.NoError(t, st.SetOffset(777))\n\n\titer, err := st.Iterator()\n\trequire.NoError(t, err)\n\n\tit := &iterator{\n\t\titer:  storage.NewMultiIterator([]storage.Iterator{iter}),\n\t\tcodec: new(codec.String),\n\t}\n\tdefer it.Release()\n\tcount := 0\n\n\t// accessing iterator before Next should only return nils\n\tval, err := it.Value()\n\trequire.True(t, val == nil)\n\trequire.NoError(t, err)\n\n\tfor it.Next() {\n\t\tcount++\n\t\tkey := it.Key()\n\t\texpected, ok := kv[key]\n\t\tif !ok {\n\t\t\tt.Fatalf(\"unexpected key from iterator: %s\", key)\n\t\t}\n\n\t\tval, err := it.Value()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, val.(string), expected)\n\t}\n\n\tif err := it.Err(); err != nil {\n\t\tt.Fatalf(\"unexpected iteration error: %v\", err)\n\t}\n\n\trequire.Equal(t, len(kv), count)\n}\n"
        },
        {
          "name": "logger.go",
          "type": "blob",
          "size": 2.669921875,
          "content": "package goka\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\t\"strings\"\n\n\t\"github.com/IBM/sarama\"\n)\n\nvar defaultLogger = &std{\n\tlog: log.New(os.Stderr, \"\", log.LstdFlags),\n}\n\n// Logger is the interface Goka and its subpackages use for logging.\ntype Logger interface {\n\t// Print will simply print the params\n\tPrint(...interface{})\n\n\t// Print will simply print the params\n\tPrintln(...interface{})\n\n\t// Printf will be used for informational messages. These can be thought of\n\t// having an 'Info'-level in a structured logger.\n\tPrintf(string, ...interface{})\n}\n\ntype logger interface {\n\tLogger\n\t// Debugf is used for debugging messages, mostly for debugging goka itself.\n\t// It is turned off unless goka is initialized\n\tDebugf(string, ...interface{})\n\t// PrefixedLogger returns a logger that prefixes all messages with passed prefix\n\tPrefix(string) logger\n\n\tCurrentPrefix() string\n\n\tStackPrefix(prefix string) logger\n}\n\n// std bridges the logger calls to the standard library log.\ntype std struct {\n\tlog        Logger\n\tdebug      bool\n\tprefixPath []string\n\tprefix     string\n}\n\nfunc (s *std) Print(msgs ...interface{}) {\n\ts.log.Print(msgs...)\n}\n\nfunc (s *std) Println(msgs ...interface{}) {\n\ts.log.Print(msgs...)\n}\n\nfunc (s *std) Printf(msg string, args ...interface{}) {\n\ts.log.Printf(fmt.Sprintf(\"%s%s\", s.prefix, msg), args...)\n}\n\nfunc (s *std) Debugf(msg string, args ...interface{}) {\n\tif s.debug {\n\t\ts.log.Printf(fmt.Sprintf(\"%s%s\", s.prefix, msg), args...)\n\t}\n}\n\nfunc (s *std) Prefix(prefix string) logger {\n\treturn s.StackPrefix(prefix).(*std)\n}\n\n// Default returns the standard library logger\nfunc DefaultLogger() Logger {\n\treturn defaultLogger\n}\n\n// Debug enables or disables debug logging using the global logger.\n// The goka debugging setting is applied to any custom loggers in goka components (Processors, Views, Emitters).\nfunc Debug(gokaDebug, saramaDebug bool) {\n\tdefaultLogger.debug = gokaDebug\n\tif saramaDebug {\n\t\tSetSaramaLogger((&std{log: defaultLogger, debug: true}).Prefix(\"Sarama\"))\n\t}\n}\n\nfunc SetSaramaLogger(logger Logger) {\n\tsarama.Logger = logger\n}\n\n// newLogger creates a new goka logger\nfunc wrapLogger(l Logger, debug bool) logger {\n\treturn &std{\n\t\tlog:   l,\n\t\tdebug: debug,\n\t}\n}\n\nfunc (s *std) CurrentPrefix() string {\n\treturn s.prefix\n}\n\nfunc (s *std) StackPrefix(prefix string) logger {\n\tvar prefPath []string\n\t// append existing path\n\tprefPath = append(prefPath, s.prefixPath...)\n\n\t// if new is not empty, append to path\n\tif prefix != \"\" {\n\t\tprefPath = append(prefPath, prefix)\n\t}\n\n\t// make new prefix\n\tnewPrefix := strings.Join(prefPath, \" > \")\n\tif newPrefix != \"\" {\n\t\tnewPrefix = \"[\" + newPrefix + \"] \"\n\t}\n\n\treturn &std{\n\t\tlog:        s.log,\n\t\tprefixPath: prefPath,\n\t\tprefix:     newPrefix,\n\t\tdebug:      s.debug,\n\t}\n}\n"
        },
        {
          "name": "mockautoconsumers.go",
          "type": "blob",
          "size": 19.9951171875,
          "content": "package goka\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"testing\"\n\n\t\"github.com/IBM/sarama\"\n\t\"github.com/hashicorp/go-multierror\"\n\t\"github.com/lovoo/goka/multierr\"\n)\n\nconst (\n\t// TODO: what is this used for?\n\tanyOffset int64 = -1000\n)\n\nvar (\n\terrOutOfExpectations           = fmt.Errorf(\"error out of expectations\")\n\terrPartitionConsumerNotStarted = fmt.Errorf(\"error partition consumer not started\")\n)\n\n// MockAutoConsumer implements sarama's Consumer interface for testing purposes.\n// Before you can start consuming from this consumer, you have to register\n// topic/partitions using ExpectConsumePartition, and set expectations on them.\ntype MockAutoConsumer struct {\n\tl                  sync.Mutex\n\tt                  *testing.T\n\tconfig             *sarama.Config\n\tpartitionConsumers map[string]map[int32]*MockAutoPartitionConsumer\n\tmetadata           map[string][]int32\n}\n\n// NewMockAutoConsumer returns a new mock Consumer instance. The t argument should\n// be the *testing.T instance of your test method. An error will be written to it if\n// an expectation is violated. The config argument can be set to nil.\nfunc NewMockAutoConsumer(t *testing.T, config *sarama.Config) *MockAutoConsumer {\n\tif config == nil {\n\t\tconfig = sarama.NewConfig()\n\t}\n\n\tc := &MockAutoConsumer{\n\t\tt:                  t,\n\t\tconfig:             config,\n\t\tpartitionConsumers: make(map[string]map[int32]*MockAutoPartitionConsumer),\n\t}\n\treturn c\n}\n\n///////////////////////////////////////////////////\n// Consumer interface implementation\n///////////////////////////////////////////////////\n\n// ConsumePartition implements the ConsumePartition method from the sarama.Consumer interface.\n// Before you can start consuming a partition, you have to set expectations on it using\n// ExpectConsumePartition. You can only consume a partition once per consumer.\nfunc (c *MockAutoConsumer) ConsumePartition(topic string, partition int32, offset int64) (sarama.PartitionConsumer, error) {\n\tc.l.Lock()\n\tdefer c.l.Unlock()\n\n\tif c.partitionConsumers[topic] == nil || c.partitionConsumers[topic][partition] == nil {\n\t\tc.t.Errorf(\"No expectations set for %s/%d\", topic, partition)\n\t\treturn nil, errOutOfExpectations\n\t}\n\n\tpc := c.partitionConsumers[topic][partition]\n\tif pc.consumed {\n\t\treturn nil, sarama.ConfigurationError(\"The topic/partition is already being consumed\")\n\t}\n\n\tif pc.offset != anyOffset && pc.offset != offset {\n\t\tc.t.Errorf(\"Unexpected offset when calling ConsumePartition for %s/%d. Expected %d, got %d.\", topic, partition, pc.offset, offset)\n\t}\n\n\tpc.consumed = true\n\treturn pc, nil\n}\n\n// Topics returns a list of topics, as registered with SetTopicMetadata\nfunc (c *MockAutoConsumer) Topics() ([]string, error) {\n\tc.l.Lock()\n\tdefer c.l.Unlock()\n\n\tif c.metadata == nil {\n\t\tc.t.Errorf(\"Unexpected call to Topics. Initialize the mock's topic metadata with SetTopicMetadata.\")\n\t\treturn nil, sarama.ErrOutOfBrokers\n\t}\n\n\tvar result []string\n\tfor topic := range c.metadata {\n\t\tresult = append(result, topic)\n\t}\n\treturn result, nil\n}\n\n// Partitions returns the list of partitions for the given topic, as registered with SetTopicMetadata\nfunc (c *MockAutoConsumer) Partitions(topic string) ([]int32, error) {\n\tc.l.Lock()\n\tdefer c.l.Unlock()\n\n\tif c.metadata == nil {\n\t\tc.t.Errorf(\"Unexpected call to Partitions. Initialize the mock's topic metadata with SetTopicMetadata.\")\n\t\treturn nil, sarama.ErrOutOfBrokers\n\t}\n\tif c.metadata[topic] == nil {\n\t\treturn nil, sarama.ErrUnknownTopicOrPartition\n\t}\n\n\treturn c.metadata[topic], nil\n}\n\n// HighWaterMarks returns a map of high watermarks for each topic/partition\nfunc (c *MockAutoConsumer) HighWaterMarks() map[string]map[int32]int64 {\n\tc.l.Lock()\n\tdefer c.l.Unlock()\n\n\thwms := make(map[string]map[int32]int64, len(c.partitionConsumers))\n\tfor topic, partitionConsumers := range c.partitionConsumers {\n\t\thwm := make(map[int32]int64, len(partitionConsumers))\n\t\tfor partition, pc := range partitionConsumers {\n\t\t\thwm[partition] = pc.HighWaterMarkOffset()\n\t\t}\n\t\thwms[topic] = hwm\n\t}\n\n\treturn hwms\n}\n\n// Close implements the Close method from the sarama.Consumer interface. It will close\n// all registered PartitionConsumer instances.\nfunc (c *MockAutoConsumer) Close() error {\n\tc.l.Lock()\n\tdefer c.l.Unlock()\n\n\tfor _, partitions := range c.partitionConsumers {\n\t\tfor _, partitionConsumer := range partitions {\n\t\t\tpartitionConsumer.Close()\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (c *MockAutoConsumer) Pause(topicPartitions map[string][]int32) {}\n\nfunc (c *MockAutoConsumer) Resume(topicPartitions map[string][]int32) {}\n\nfunc (c *MockAutoConsumer) PauseAll() {}\n\nfunc (c *MockAutoConsumer) ResumeAll() {}\n\n///////////////////////////////////////////////////\n// Expectation API\n///////////////////////////////////////////////////\n\n// SetTopicMetadata sets the clusters topic/partition metadata,\n// which will be returned by Topics() and Partitions().\nfunc (c *MockAutoConsumer) SetTopicMetadata(metadata map[string][]int32) {\n\tc.l.Lock()\n\tdefer c.l.Unlock()\n\n\tc.metadata = metadata\n}\n\n// ExpectConsumePartition will register a topic/partition, so you can set expectations on it.\n// The registered PartitionConsumer will be returned, so you can set expectations\n// on it using method chaining. Once a topic/partition is registered, you are\n// expected to start consuming it using ConsumePartition. If that doesn't happen,\n// an error will be written to the error reporter once the mock consumer is closed. It will\n// also expect that the\nfunc (c *MockAutoConsumer) ExpectConsumePartition(topic string, partition int32, offset int64) *MockAutoPartitionConsumer {\n\tc.l.Lock()\n\tdefer c.l.Unlock()\n\n\tif c.partitionConsumers[topic] == nil {\n\t\tc.partitionConsumers[topic] = make(map[int32]*MockAutoPartitionConsumer)\n\t}\n\n\tif c.partitionConsumers[topic][partition] == nil {\n\t\tc.partitionConsumers[topic][partition] = &MockAutoPartitionConsumer{\n\t\t\tt:         c.t,\n\t\t\ttopic:     topic,\n\t\t\tpartition: partition,\n\t\t\toffset:    offset,\n\t\t\tmessages:  make(chan *sarama.ConsumerMessage, c.config.ChannelBufferSize),\n\t\t\terrors:    make(chan *sarama.ConsumerError, c.config.ChannelBufferSize),\n\t\t}\n\t}\n\n\treturn c.partitionConsumers[topic][partition]\n}\n\n///////////////////////////////////////////////////\n// PartitionConsumer mock type\n///////////////////////////////////////////////////\n\n// MockAutoPartitionConsumer implements sarama's PartitionConsumer interface for testing purposes.\n// It is returned by the mock Consumers ConsumePartitionMethod, but only if it is\n// registered first using the Consumer's ExpectConsumePartition method. Before consuming the\n// Errors and Messages channel, you should specify what values will be provided on these\n// channels using YieldMessage and YieldError.\ntype MockAutoPartitionConsumer struct {\n\thighWaterMarkOffset     int64 // must be at the top of the struct because https://golang.org/pkg/sync/atomic/#pkg-note-BUG\n\tl                       sync.Mutex\n\tt                       *testing.T\n\ttopic                   string\n\tpartition               int32\n\toffset                  int64\n\tmessages                chan *sarama.ConsumerMessage\n\terrors                  chan *sarama.ConsumerError\n\tsingleClose             sync.Once\n\tconsumed                bool\n\terrorsShouldBeDrained   bool\n\tmessagesShouldBeDrained bool\n}\n\n///////////////////////////////////////////////////\n// PartitionConsumer interface implementation\n///////////////////////////////////////////////////\n\n// AsyncClose implements the AsyncClose method from the sarama.PartitionConsumer interface.\nfunc (pc *MockAutoPartitionConsumer) AsyncClose() {\n\tpc.singleClose.Do(func() {\n\t\tclose(pc.messages)\n\t\tclose(pc.errors)\n\t\tpc.consumed = false\n\t})\n}\n\n// Close implements the Close method from the sarama.PartitionConsumer interface. It will\n// verify whether the partition consumer was actually started.\nfunc (pc *MockAutoPartitionConsumer) Close() error {\n\tvar err error\n\tpc.singleClose.Do(func() {\n\t\tif !pc.consumed {\n\t\t\tpc.t.Errorf(\"Expectations set on %s/%d, but no partition consumer was started.\", pc.topic, pc.partition)\n\t\t\terr = errPartitionConsumerNotStarted\n\t\t\treturn\n\t\t}\n\n\t\tif pc.errorsShouldBeDrained && len(pc.errors) > 0 {\n\t\t\tpc.t.Errorf(\"Expected the errors channel for %s/%d to be drained on close, but found %d errors.\", pc.topic, pc.partition, len(pc.errors))\n\t\t}\n\n\t\tif pc.messagesShouldBeDrained && len(pc.messages) > 0 {\n\t\t\tpc.t.Errorf(\"Expected the messages channel for %s/%d to be drained on close, but found %d messages.\", pc.topic, pc.partition, len(pc.messages))\n\t\t}\n\n\t\tclose(pc.messages)\n\t\tclose(pc.errors)\n\t\tpc.consumed = false\n\n\t\tvar (\n\t\t\tcloseErr error\n\t\t\twg       sync.WaitGroup\n\t\t)\n\n\t\twg.Add(1)\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\n\t\t\terrs := make(sarama.ConsumerErrors, 0)\n\t\t\tfor err := range pc.errors {\n\t\t\t\terrs = append(errs, err)\n\t\t\t}\n\n\t\t\tif len(errs) > 0 {\n\t\t\t\tcloseErr = errs\n\t\t\t}\n\t\t}()\n\n\t\twg.Add(1)\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\t\t\tfor range pc.messages {\n\t\t\t\t// drain\n\t\t\t}\n\t\t}()\n\n\t\twg.Wait()\n\t\terr = closeErr\n\t})\n\treturn err\n}\n\n// Errors implements the Errors method from the sarama.PartitionConsumer interface.\nfunc (pc *MockAutoPartitionConsumer) Errors() <-chan *sarama.ConsumerError {\n\treturn pc.errors\n}\n\n// Messages implements the Messages method from the sarama.PartitionConsumer interface.\nfunc (pc *MockAutoPartitionConsumer) Messages() <-chan *sarama.ConsumerMessage {\n\treturn pc.messages\n}\n\n// HighWaterMarkOffset returns the highwatermark for the partition\nfunc (pc *MockAutoPartitionConsumer) HighWaterMarkOffset() int64 {\n\treturn atomic.LoadInt64(&pc.highWaterMarkOffset)\n}\n\n///////////////////////////////////////////////////\n// Expectation API\n///////////////////////////////////////////////////\n\n// YieldMessage will yield a messages Messages channel of this partition consumer\n// when it is consumed. By default, the mock consumer will not verify whether this\n// message was consumed from the Messages channel, because there are legitimate\n// reasons forthis not to happen. ou can call ExpectMessagesDrainedOnClose so it will\n// verify that the channel is empty on close.\nfunc (pc *MockAutoPartitionConsumer) YieldMessage(msg *sarama.ConsumerMessage) {\n\tpc.l.Lock()\n\tdefer pc.l.Unlock()\n\n\tmsg.Topic = pc.topic\n\tmsg.Partition = pc.partition\n\tmsg.Offset = atomic.LoadInt64(&pc.highWaterMarkOffset)\n\tatomic.AddInt64(&pc.highWaterMarkOffset, 1)\n\n\tpc.messages <- msg\n}\n\n// YieldError will yield an error on the Errors channel of this partition consumer\n// when it is consumed. By default, the mock consumer will not verify whether this error was\n// consumed from the Errors channel, because there are legitimate reasons for this\n// not to happen. You can call ExpectErrorsDrainedOnClose so it will verify that\n// the channel is empty on close.\nfunc (pc *MockAutoPartitionConsumer) YieldError(err error) {\n\tpc.errors <- &sarama.ConsumerError{\n\t\tTopic:     pc.topic,\n\t\tPartition: pc.partition,\n\t\tErr:       err,\n\t}\n}\n\n// ExpectMessagesDrainedOnClose sets an expectation on the partition consumer\n// that the messages channel will be fully drained when Close is called. If this\n// expectation is not met, an error is reported to the error reporter.\nfunc (pc *MockAutoPartitionConsumer) ExpectMessagesDrainedOnClose() {\n\tpc.messagesShouldBeDrained = true\n}\n\n// ExpectErrorsDrainedOnClose sets an expectation on the partition consumer\n// that the errors channel will be fully drained when Close is called. If this\n// expectation is not met, an error is reported to the error reporter.\nfunc (pc *MockAutoPartitionConsumer) ExpectErrorsDrainedOnClose() {\n\tpc.errorsShouldBeDrained = true\n}\n\n// Pause suspends fetching from this partition. Future calls to the broker will not return\n// any records from these partition until it have been resumed using Resume().\n// Note that this method does not affect partition subscription.\n// In particular, it does not cause a group rebalance when automatic assignment is used.\nfunc (pc *MockAutoPartitionConsumer) Pause() {\n}\n\n// Resume resumes this partition which have been paused with Pause().\n// New calls to the broker will return records from these partitions if there are any to be fetched.\n// If the partition was not previously paused, this method is a no-op.\nfunc (pc *MockAutoPartitionConsumer) Resume() {\n}\n\n// IsPaused indicates if this partition consumer is paused or not\nfunc (pc *MockAutoPartitionConsumer) IsPaused() bool {\n\treturn false\n}\n\n// MockConsumerGroupSession mocks the consumer group session used for testing\ntype MockConsumerGroupSession struct {\n\tctx        context.Context\n\tgeneration int32\n\ttopics     []string\n\tclaims     map[string]*MockConsumerGroupClaim\n\n\tmu sync.RWMutex\n\n\tconsumerGroup *MockConsumerGroup\n}\n\n// MockConsumerGroupClaim mocks the consumergroupclaim\ntype MockConsumerGroupClaim struct {\n\ttopic     string\n\tpartition int32\n\tmsgs      chan *sarama.ConsumerMessage\n}\n\n// NewMockConsumerGroupClaim creates a new mocksconsumergroupclaim\nfunc NewMockConsumerGroupClaim(topic string, partition int32) *MockConsumerGroupClaim {\n\treturn &MockConsumerGroupClaim{\n\t\ttopic:     topic,\n\t\tpartition: partition,\n\t\tmsgs:      make(chan *sarama.ConsumerMessage),\n\t}\n}\n\n// Topic returns the current topic of the claim\nfunc (cgc *MockConsumerGroupClaim) Topic() string {\n\treturn cgc.topic\n}\n\n// Partition returns the partition\nfunc (cgc *MockConsumerGroupClaim) Partition() int32 {\n\treturn cgc.partition\n}\n\n// InitialOffset returns the initial offset\nfunc (cgc *MockConsumerGroupClaim) InitialOffset() int64 {\n\treturn 0\n}\n\n// HighWaterMarkOffset returns the hwm offset\nfunc (cgc *MockConsumerGroupClaim) HighWaterMarkOffset() int64 {\n\treturn 0\n}\n\n// Messages returns the message channel that must be\nfunc (cgc *MockConsumerGroupClaim) Messages() <-chan *sarama.ConsumerMessage {\n\treturn cgc.msgs\n}\n\nfunc newConsumerGroupSession(ctx context.Context, generation int32, cg *MockConsumerGroup, topics []string) *MockConsumerGroupSession {\n\treturn &MockConsumerGroupSession{\n\t\tctx:           ctx,\n\t\tgeneration:    generation,\n\t\tconsumerGroup: cg,\n\t\ttopics:        topics,\n\t\tclaims:        make(map[string]*MockConsumerGroupClaim),\n\t}\n}\n\n// Claims returns the number of partitions assigned in the group session for each topic\nfunc (cgs *MockConsumerGroupSession) Claims() map[string][]int32 {\n\tclaims := make(map[string][]int32)\n\tfor _, topic := range cgs.topics {\n\t\tclaims[topic] = []int32{0}\n\t}\n\treturn claims\n}\n\nfunc (cgs *MockConsumerGroupSession) createGroupClaim(topic string, partition int32) *MockConsumerGroupClaim {\n\tcgs.mu.Lock()\n\tdefer cgs.mu.Unlock()\n\n\tcgs.claims[topic] = NewMockConsumerGroupClaim(topic, 0)\n\n\treturn cgs.claims[topic]\n}\n\n// SendMessage sends a message to the consumer\nfunc (cgs *MockConsumerGroupSession) SendMessage(msg *sarama.ConsumerMessage) {\n\tcgs.mu.RLock()\n\tdefer cgs.mu.RUnlock()\n\n\tfor topic, claim := range cgs.claims {\n\t\tif topic == msg.Topic {\n\t\t\tclaim.msgs <- msg\n\t\t}\n\t}\n}\n\n// MemberID returns the member ID\n// TODO: clarify what that actually means and whether we need to mock that somehow\nfunc (cgs *MockConsumerGroupSession) MemberID() string {\n\tpanic(\"MemberID not provided by mock\")\n}\n\n// GenerationID returns the generation ID of the group consumer\nfunc (cgs *MockConsumerGroupSession) GenerationID() int32 {\n\treturn cgs.generation\n}\n\n// MarkOffset marks the passed offset consumed in topic/partition\nfunc (cgs *MockConsumerGroupSession) MarkOffset(topic string, partition int32, offset int64, metadata string) {\n\tcgs.consumerGroup.markMessage(topic, partition, offset, metadata)\n}\n\n// Commit the offset to the backend\nfunc (cgs *MockConsumerGroupSession) Commit() {\n\tpanic(\"not implemented\")\n}\n\n// ResetOffset resets the offset to be consumed from\nfunc (cgs *MockConsumerGroupSession) ResetOffset(topic string, partition int32, offset int64, metadata string) {\n\tpanic(\"reset offset is not implemented by the mock\")\n}\n\n// MarkMessage marks the passed message as consumed\nfunc (cgs *MockConsumerGroupSession) MarkMessage(msg *sarama.ConsumerMessage, metadata string) {\n\tpanic(\"not implemented\")\n}\n\n// Context returns the consumer group's context\nfunc (cgs *MockConsumerGroupSession) Context() context.Context {\n\treturn cgs.ctx\n}\n\n// MockConsumerGroup mocks the consumergroup\ntype MockConsumerGroup struct {\n\terrs chan error\n\n\tmu sync.RWMutex\n\n\t// setting this makes the consume call fail with this error for testing\n\tfailOnConsume error\n\n\t// use the same offset counter for all topics\n\toffset            int64\n\tcurrentGeneration int32\n\n\t// messages we sent to the consumergroup and need to wait for\n\tmMessages  sync.Mutex\n\tmessages   map[int64]int64\n\twgMessages sync.WaitGroup\n\n\tsessions map[string]*MockConsumerGroupSession\n}\n\n// NewMockConsumerGroup creates a new consumer group\nfunc NewMockConsumerGroup(t *testing.T) *MockConsumerGroup {\n\treturn &MockConsumerGroup{\n\t\terrs:     make(chan error, 1),\n\t\tsessions: make(map[string]*MockConsumerGroupSession),\n\t\tmessages: make(map[int64]int64),\n\t}\n}\n\n// FailOnConsume marks the consumer to fail on consume\nfunc (cg *MockConsumerGroup) FailOnConsume(err error) {\n\tcg.failOnConsume = err\n}\n\nfunc (cg *MockConsumerGroup) nextOffset() int64 {\n\treturn atomic.AddInt64(&cg.offset, 1)\n}\n\nfunc (cg *MockConsumerGroup) topicKey(topics []string) string {\n\treturn strings.Join(topics, \",\")\n}\n\nfunc (cg *MockConsumerGroup) markMessage(topic string, partition int32, offset int64, metadata string) {\n\tcg.mMessages.Lock()\n\tdefer cg.mMessages.Unlock()\n\n\tcnt := cg.messages[offset-1]\n\n\tif cnt == 0 {\n\t\tpanic(fmt.Errorf(\"Cannot mark message with offset %d, it's not a valid offset or was already marked\", offset))\n\t}\n\n\tcg.messages[offset] = cnt - 1\n\n\tcg.wgMessages.Done()\n}\n\n// Consume starts consuming from the consumergroup\nfunc (cg *MockConsumerGroup) Consume(ctx context.Context, topics []string, handler sarama.ConsumerGroupHandler) error {\n\tif cg.failOnConsume != nil {\n\t\treturn cg.failOnConsume\n\t}\n\n\tkey := cg.topicKey(topics)\n\tfor {\n\t\tcg.currentGeneration++\n\t\tsession := newConsumerGroupSession(ctx, cg.currentGeneration, cg, topics)\n\n\t\tcg.sessions[key] = session\n\n\t\terr := handler.Setup(session)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"Error setting up: %v\", err)\n\t\t}\n\t\terrg, _ := multierr.NewErrGroup(ctx)\n\t\tfor _, topic := range topics {\n\t\t\tclaim := session.createGroupClaim(topic, 0)\n\t\t\terrg.Go(func() error {\n\t\t\t\t<-ctx.Done()\n\t\t\t\tclose(claim.msgs)\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\terrg.Go(func() error {\n\t\t\t\terr := handler.ConsumeClaim(session, claim)\n\t\t\t\treturn err\n\t\t\t})\n\t\t}\n\n\t\terr = multierror.Append(\n\t\t\t// wait for runner errors and collect error\n\t\t\terrg.Wait(),\n\t\t\t// cleanup and collect errors\n\t\t\thandler.Cleanup(session),\n\t\t).ErrorOrNil()\n\n\t\t// remove current sessions\n\t\tdelete(cg.sessions, key)\n\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"Error running or cleaning: %w\", err)\n\t\t}\n\n\t\tselect {\n\t\t// if the session was terminated because of a cancelled context,\n\t\t// stop the loop\n\t\tcase <-ctx.Done():\n\t\t\treturn nil\n\n\t\t\t// otherwise just continue with the next generation\n\t\tdefault:\n\t\t}\n\t}\n}\n\n// SendError sends an error the consumergroup\nfunc (cg *MockConsumerGroup) SendError(err error) {\n\tcg.mu.RLock()\n\tdefer cg.mu.RUnlock()\n\tcg.errs <- err\n}\n\n// SendMessage sends a message to the consumergroup\n// returns a channel that will be closed when the message has been committed\n// by the group\nfunc (cg *MockConsumerGroup) SendMessage(message *sarama.ConsumerMessage) <-chan struct{} {\n\tcg.mMessages.Lock()\n\tdefer cg.mMessages.Unlock()\n\n\tmessage.Offset = cg.nextOffset()\n\n\tvar messages int\n\tfor _, session := range cg.sessions {\n\t\tsession.SendMessage(message)\n\t\tmessages++\n\t}\n\n\tcg.messages[message.Offset] += int64(messages)\n\tcg.wgMessages.Add(messages)\n\n\tdone := make(chan struct{})\n\tgo func() {\n\t\tdefer close(done)\n\t\tcg.wgMessages.Wait()\n\t}()\n\n\treturn done\n}\n\n// SendMessageWait sends a message to the consumergroup waiting for the message for being committed\nfunc (cg *MockConsumerGroup) SendMessageWait(message *sarama.ConsumerMessage) {\n\t<-cg.SendMessage(message)\n}\n\n// Errors returns the errors channel\nfunc (cg *MockConsumerGroup) Errors() <-chan error {\n\tcg.mu.RLock()\n\tdefer cg.mu.RUnlock()\n\treturn cg.errs\n}\n\n// Close closes the consumergroup\nfunc (cg *MockConsumerGroup) Close() error {\n\tcg.mu.Lock()\n\tdefer cg.mu.Unlock()\n\n\tcg.messages = make(map[int64]int64)\n\n\t// close old errs chan and create new one\n\tclose(cg.errs)\n\treturn nil\n}\n\nfunc (cg *MockConsumerGroup) Pause(partitions map[string][]int32) {}\n\nfunc (cg *MockConsumerGroup) Resume(partitions map[string][]int32) {}\n\nfunc (cg *MockConsumerGroup) PauseAll() {}\n\nfunc (cg *MockConsumerGroup) ResumeAll() {}\n"
        },
        {
          "name": "mockbuilder.go",
          "type": "blob",
          "size": 2.248046875,
          "content": "package goka\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"testing\"\n\n\t\"github.com/golang/mock/gomock\"\n\n\t\"github.com/lovoo/goka/storage\"\n)\n\nvar (\n\terrProducerBuilder error = errors.New(\"building producer failed on purpose\")\n)\n\ntype builderMock struct {\n\tctrl          *gomock.Controller\n\tst            storage.Storage\n\tmst           *MockStorage\n\ttmgr          *MockTopicManager\n\tconsumerGroup *MockConsumerGroup\n\tproducer      *MockProducer\n\tclient        *MockClient\n\tadmin         *MockClusterAdmin\n}\n\nfunc newBuilderMock(ctrl *gomock.Controller) *builderMock {\n\treturn &builderMock{\n\t\tctrl:     ctrl,\n\t\tmst:      NewMockStorage(ctrl),\n\t\ttmgr:     NewMockTopicManager(ctrl),\n\t\tproducer: NewMockProducer(ctrl),\n\t\tclient:   NewMockClient(ctrl),\n\t\tadmin:    NewMockClusterAdmin(ctrl),\n\t}\n}\n\nfunc (bm *builderMock) createProcessorOptions(consBuilder SaramaConsumerBuilder, groupBuilder ConsumerGroupBuilder) []ProcessorOption {\n\treturn []ProcessorOption{\n\t\tWithStorageBuilder(bm.getStorageBuilder()),\n\t\tWithTopicManagerBuilder(bm.getTopicManagerBuilder()),\n\t\tWithProducerBuilder(bm.getProducerBuilder()),\n\t\tWithConsumerGroupBuilder(groupBuilder),\n\t\tWithConsumerSaramaBuilder(consBuilder),\n\t}\n}\n\nfunc (bm *builderMock) getStorageBuilder() storage.Builder {\n\treturn func(topic string, partition int32) (storage.Storage, error) {\n\t\tif bm.st != nil {\n\t\t\treturn bm.st, nil\n\t\t}\n\t\treturn bm.mst, nil\n\t}\n}\n\nfunc (bm *builderMock) getTopicManagerBuilder() TopicManagerBuilder {\n\treturn func([]string) (TopicManager, error) {\n\t\treturn bm.tmgr, nil\n\t}\n}\n\nfunc (bm *builderMock) getProducerBuilder() ProducerBuilder {\n\treturn func(brokers []string, clientID string, hasher func() hash.Hash32) (Producer, error) {\n\t\treturn bm.producer, nil\n\t}\n}\n\nfunc (bm *builderMock) getErrorProducerBuilder() ProducerBuilder {\n\treturn func(brokers []string, clientID string, hasher func() hash.Hash32) (Producer, error) {\n\t\treturn nil, errProducerBuilder\n\t}\n}\n\nfunc (bm *builderMock) useMemoryStorage() {\n\tbm.st = storage.NewMemory()\n}\n\nfunc errStorageBuilder() storage.Builder {\n\treturn func(topic string, partition int32) (storage.Storage, error) {\n\t\treturn nil, fmt.Errorf(\"error returned by errStorageBuilder\")\n\t}\n}\n\nfunc defaultSaramaAutoConsumerMock(t *testing.T) *MockAutoConsumer {\n\treturn NewMockAutoConsumer(t, DefaultConfig())\n}\n"
        },
        {
          "name": "mockcontroller.go",
          "type": "blob",
          "size": 0.7236328125,
          "content": "package goka\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/golang/mock/gomock\"\n)\n\ntype gomockPanicker struct {\n\treporter gomock.TestReporter\n}\n\nfunc (gm *gomockPanicker) Errorf(format string, args ...interface{}) {\n\tgm.reporter.Errorf(format, args...)\n}\nfunc (gm *gomockPanicker) Fatalf(format string, args ...interface{}) {\n\tdefer panic(fmt.Sprintf(format, args...))\n\tgm.reporter.Fatalf(format, args...)\n}\n\n// NewMockController returns a *gomock.Controller using a wrapped testing.T (or whatever)\n// which panics on a Fatalf. This is necessary when using a mock in kafkamock.\n// Otherwise it will freeze on an unexpected call.\nfunc NewMockController(t gomock.TestReporter) *gomock.Controller {\n\treturn gomock.NewController(&gomockPanicker{reporter: t})\n}\n"
        },
        {
          "name": "mocks.go",
          "type": "blob",
          "size": 8.77734375,
          "content": "// Code generated by MockGen. DO NOT EDIT.\n// Source: github.com/lovoo/goka (interfaces: TopicManager,Producer,Broker)\n\n// Package goka is a generated GoMock package.\npackage goka\n\nimport (\n\treflect \"reflect\"\n\n\tsarama \"github.com/IBM/sarama\"\n\tgomock \"github.com/golang/mock/gomock\"\n)\n\n// MockTopicManager is a mock of TopicManager interface.\ntype MockTopicManager struct {\n\tctrl     *gomock.Controller\n\trecorder *MockTopicManagerMockRecorder\n}\n\n// MockTopicManagerMockRecorder is the mock recorder for MockTopicManager.\ntype MockTopicManagerMockRecorder struct {\n\tmock *MockTopicManager\n}\n\n// NewMockTopicManager creates a new mock instance.\nfunc NewMockTopicManager(ctrl *gomock.Controller) *MockTopicManager {\n\tmock := &MockTopicManager{ctrl: ctrl}\n\tmock.recorder = &MockTopicManagerMockRecorder{mock}\n\treturn mock\n}\n\n// EXPECT returns an object that allows the caller to indicate expected use.\nfunc (m *MockTopicManager) EXPECT() *MockTopicManagerMockRecorder {\n\treturn m.recorder\n}\n\n// Close mocks base method.\nfunc (m *MockTopicManager) Close() error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Close\")\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// Close indicates an expected call of Close.\nfunc (mr *MockTopicManagerMockRecorder) Close() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Close\", reflect.TypeOf((*MockTopicManager)(nil).Close))\n}\n\n// EnsureStreamExists mocks base method.\nfunc (m *MockTopicManager) EnsureStreamExists(arg0 string, arg1 int) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"EnsureStreamExists\", arg0, arg1)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// EnsureStreamExists indicates an expected call of EnsureStreamExists.\nfunc (mr *MockTopicManagerMockRecorder) EnsureStreamExists(arg0, arg1 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"EnsureStreamExists\", reflect.TypeOf((*MockTopicManager)(nil).EnsureStreamExists), arg0, arg1)\n}\n\n// EnsureTableExists mocks base method.\nfunc (m *MockTopicManager) EnsureTableExists(arg0 string, arg1 int) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"EnsureTableExists\", arg0, arg1)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// EnsureTableExists indicates an expected call of EnsureTableExists.\nfunc (mr *MockTopicManagerMockRecorder) EnsureTableExists(arg0, arg1 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"EnsureTableExists\", reflect.TypeOf((*MockTopicManager)(nil).EnsureTableExists), arg0, arg1)\n}\n\n// EnsureTopicExists mocks base method.\nfunc (m *MockTopicManager) EnsureTopicExists(arg0 string, arg1, arg2 int, arg3 map[string]string) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"EnsureTopicExists\", arg0, arg1, arg2, arg3)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// EnsureTopicExists indicates an expected call of EnsureTopicExists.\nfunc (mr *MockTopicManagerMockRecorder) EnsureTopicExists(arg0, arg1, arg2, arg3 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"EnsureTopicExists\", reflect.TypeOf((*MockTopicManager)(nil).EnsureTopicExists), arg0, arg1, arg2, arg3)\n}\n\n// GetOffset mocks base method.\nfunc (m *MockTopicManager) GetOffset(arg0 string, arg1 int32, arg2 int64) (int64, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"GetOffset\", arg0, arg1, arg2)\n\tret0, _ := ret[0].(int64)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// GetOffset indicates an expected call of GetOffset.\nfunc (mr *MockTopicManagerMockRecorder) GetOffset(arg0, arg1, arg2 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"GetOffset\", reflect.TypeOf((*MockTopicManager)(nil).GetOffset), arg0, arg1, arg2)\n}\n\n// Partitions mocks base method.\nfunc (m *MockTopicManager) Partitions(arg0 string) ([]int32, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Partitions\", arg0)\n\tret0, _ := ret[0].([]int32)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// Partitions indicates an expected call of Partitions.\nfunc (mr *MockTopicManagerMockRecorder) Partitions(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Partitions\", reflect.TypeOf((*MockTopicManager)(nil).Partitions), arg0)\n}\n\n// MockProducer is a mock of Producer interface.\ntype MockProducer struct {\n\tctrl     *gomock.Controller\n\trecorder *MockProducerMockRecorder\n}\n\n// MockProducerMockRecorder is the mock recorder for MockProducer.\ntype MockProducerMockRecorder struct {\n\tmock *MockProducer\n}\n\n// NewMockProducer creates a new mock instance.\nfunc NewMockProducer(ctrl *gomock.Controller) *MockProducer {\n\tmock := &MockProducer{ctrl: ctrl}\n\tmock.recorder = &MockProducerMockRecorder{mock}\n\treturn mock\n}\n\n// EXPECT returns an object that allows the caller to indicate expected use.\nfunc (m *MockProducer) EXPECT() *MockProducerMockRecorder {\n\treturn m.recorder\n}\n\n// Close mocks base method.\nfunc (m *MockProducer) Close() error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Close\")\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// Close indicates an expected call of Close.\nfunc (mr *MockProducerMockRecorder) Close() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Close\", reflect.TypeOf((*MockProducer)(nil).Close))\n}\n\n// Emit mocks base method.\nfunc (m *MockProducer) Emit(arg0, arg1 string, arg2 []byte) *Promise {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Emit\", arg0, arg1, arg2)\n\tret0, _ := ret[0].(*Promise)\n\treturn ret0\n}\n\n// Emit indicates an expected call of Emit.\nfunc (mr *MockProducerMockRecorder) Emit(arg0, arg1, arg2 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Emit\", reflect.TypeOf((*MockProducer)(nil).Emit), arg0, arg1, arg2)\n}\n\n// EmitWithHeaders mocks base method.\nfunc (m *MockProducer) EmitWithHeaders(arg0, arg1 string, arg2 []byte, arg3 Headers) *Promise {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"EmitWithHeaders\", arg0, arg1, arg2, arg3)\n\tret0, _ := ret[0].(*Promise)\n\treturn ret0\n}\n\n// EmitWithHeaders indicates an expected call of EmitWithHeaders.\nfunc (mr *MockProducerMockRecorder) EmitWithHeaders(arg0, arg1, arg2, arg3 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"EmitWithHeaders\", reflect.TypeOf((*MockProducer)(nil).EmitWithHeaders), arg0, arg1, arg2, arg3)\n}\n\n// MockBroker is a mock of Broker interface.\ntype MockBroker struct {\n\tctrl     *gomock.Controller\n\trecorder *MockBrokerMockRecorder\n}\n\n// MockBrokerMockRecorder is the mock recorder for MockBroker.\ntype MockBrokerMockRecorder struct {\n\tmock *MockBroker\n}\n\n// NewMockBroker creates a new mock instance.\nfunc NewMockBroker(ctrl *gomock.Controller) *MockBroker {\n\tmock := &MockBroker{ctrl: ctrl}\n\tmock.recorder = &MockBrokerMockRecorder{mock}\n\treturn mock\n}\n\n// EXPECT returns an object that allows the caller to indicate expected use.\nfunc (m *MockBroker) EXPECT() *MockBrokerMockRecorder {\n\treturn m.recorder\n}\n\n// Addr mocks base method.\nfunc (m *MockBroker) Addr() string {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Addr\")\n\tret0, _ := ret[0].(string)\n\treturn ret0\n}\n\n// Addr indicates an expected call of Addr.\nfunc (mr *MockBrokerMockRecorder) Addr() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Addr\", reflect.TypeOf((*MockBroker)(nil).Addr))\n}\n\n// Connected mocks base method.\nfunc (m *MockBroker) Connected() (bool, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Connected\")\n\tret0, _ := ret[0].(bool)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// Connected indicates an expected call of Connected.\nfunc (mr *MockBrokerMockRecorder) Connected() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Connected\", reflect.TypeOf((*MockBroker)(nil).Connected))\n}\n\n// CreateTopics mocks base method.\nfunc (m *MockBroker) CreateTopics(arg0 *sarama.CreateTopicsRequest) (*sarama.CreateTopicsResponse, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"CreateTopics\", arg0)\n\tret0, _ := ret[0].(*sarama.CreateTopicsResponse)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// CreateTopics indicates an expected call of CreateTopics.\nfunc (mr *MockBrokerMockRecorder) CreateTopics(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"CreateTopics\", reflect.TypeOf((*MockBroker)(nil).CreateTopics), arg0)\n}\n\n// Open mocks base method.\nfunc (m *MockBroker) Open(arg0 *sarama.Config) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Open\", arg0)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// Open indicates an expected call of Open.\nfunc (mr *MockBrokerMockRecorder) Open(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Open\", reflect.TypeOf((*MockBroker)(nil).Open), arg0)\n}\n"
        },
        {
          "name": "mockssarama.go",
          "type": "blob",
          "size": 32.470703125,
          "content": "// Code generated by MockGen. DO NOT EDIT.\n// Source: github.com/IBM/sarama (interfaces: Client,ClusterAdmin)\n\n// Package goka is a generated GoMock package.\npackage goka\n\nimport (\n\treflect \"reflect\"\n\n\tsarama \"github.com/IBM/sarama\"\n\tgomock \"github.com/golang/mock/gomock\"\n)\n\n// MockClient is a mock of Client interface.\ntype MockClient struct {\n\tctrl     *gomock.Controller\n\trecorder *MockClientMockRecorder\n}\n\n// MockClientMockRecorder is the mock recorder for MockClient.\ntype MockClientMockRecorder struct {\n\tmock *MockClient\n}\n\n// NewMockClient creates a new mock instance.\nfunc NewMockClient(ctrl *gomock.Controller) *MockClient {\n\tmock := &MockClient{ctrl: ctrl}\n\tmock.recorder = &MockClientMockRecorder{mock}\n\treturn mock\n}\n\n// EXPECT returns an object that allows the caller to indicate expected use.\nfunc (m *MockClient) EXPECT() *MockClientMockRecorder {\n\treturn m.recorder\n}\n\n// Broker mocks base method.\nfunc (m *MockClient) Broker(arg0 int32) (*sarama.Broker, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Broker\", arg0)\n\tret0, _ := ret[0].(*sarama.Broker)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// Broker indicates an expected call of Broker.\nfunc (mr *MockClientMockRecorder) Broker(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Broker\", reflect.TypeOf((*MockClient)(nil).Broker), arg0)\n}\n\n// Brokers mocks base method.\nfunc (m *MockClient) Brokers() []*sarama.Broker {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Brokers\")\n\tret0, _ := ret[0].([]*sarama.Broker)\n\treturn ret0\n}\n\n// Brokers indicates an expected call of Brokers.\nfunc (mr *MockClientMockRecorder) Brokers() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Brokers\", reflect.TypeOf((*MockClient)(nil).Brokers))\n}\n\n// Close mocks base method.\nfunc (m *MockClient) Close() error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Close\")\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// Close indicates an expected call of Close.\nfunc (mr *MockClientMockRecorder) Close() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Close\", reflect.TypeOf((*MockClient)(nil).Close))\n}\n\n// Closed mocks base method.\nfunc (m *MockClient) Closed() bool {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Closed\")\n\tret0, _ := ret[0].(bool)\n\treturn ret0\n}\n\n// Closed indicates an expected call of Closed.\nfunc (mr *MockClientMockRecorder) Closed() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Closed\", reflect.TypeOf((*MockClient)(nil).Closed))\n}\n\n// Config mocks base method.\nfunc (m *MockClient) Config() *sarama.Config {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Config\")\n\tret0, _ := ret[0].(*sarama.Config)\n\treturn ret0\n}\n\n// Config indicates an expected call of Config.\nfunc (mr *MockClientMockRecorder) Config() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Config\", reflect.TypeOf((*MockClient)(nil).Config))\n}\n\n// Controller mocks base method.\nfunc (m *MockClient) Controller() (*sarama.Broker, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Controller\")\n\tret0, _ := ret[0].(*sarama.Broker)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// Controller indicates an expected call of Controller.\nfunc (mr *MockClientMockRecorder) Controller() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Controller\", reflect.TypeOf((*MockClient)(nil).Controller))\n}\n\n// Coordinator mocks base method.\nfunc (m *MockClient) Coordinator(arg0 string) (*sarama.Broker, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Coordinator\", arg0)\n\tret0, _ := ret[0].(*sarama.Broker)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// Coordinator indicates an expected call of Coordinator.\nfunc (mr *MockClientMockRecorder) Coordinator(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Coordinator\", reflect.TypeOf((*MockClient)(nil).Coordinator), arg0)\n}\n\n// GetOffset mocks base method.\nfunc (m *MockClient) GetOffset(arg0 string, arg1 int32, arg2 int64) (int64, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"GetOffset\", arg0, arg1, arg2)\n\tret0, _ := ret[0].(int64)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// GetOffset indicates an expected call of GetOffset.\nfunc (mr *MockClientMockRecorder) GetOffset(arg0, arg1, arg2 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"GetOffset\", reflect.TypeOf((*MockClient)(nil).GetOffset), arg0, arg1, arg2)\n}\n\n// InSyncReplicas mocks base method.\nfunc (m *MockClient) InSyncReplicas(arg0 string, arg1 int32) ([]int32, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"InSyncReplicas\", arg0, arg1)\n\tret0, _ := ret[0].([]int32)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// InSyncReplicas indicates an expected call of InSyncReplicas.\nfunc (mr *MockClientMockRecorder) InSyncReplicas(arg0, arg1 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"InSyncReplicas\", reflect.TypeOf((*MockClient)(nil).InSyncReplicas), arg0, arg1)\n}\n\n// InitProducerID mocks base method.\nfunc (m *MockClient) InitProducerID() (*sarama.InitProducerIDResponse, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"InitProducerID\")\n\tret0, _ := ret[0].(*sarama.InitProducerIDResponse)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// InitProducerID indicates an expected call of InitProducerID.\nfunc (mr *MockClientMockRecorder) InitProducerID() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"InitProducerID\", reflect.TypeOf((*MockClient)(nil).InitProducerID))\n}\n\n// Leader mocks base method.\nfunc (m *MockClient) Leader(arg0 string, arg1 int32) (*sarama.Broker, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Leader\", arg0, arg1)\n\tret0, _ := ret[0].(*sarama.Broker)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// Leader indicates an expected call of Leader.\nfunc (mr *MockClientMockRecorder) Leader(arg0, arg1 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Leader\", reflect.TypeOf((*MockClient)(nil).Leader), arg0, arg1)\n}\n\n// LeaderAndEpoch mocks base method.\nfunc (m *MockClient) LeaderAndEpoch(arg0 string, arg1 int32) (*sarama.Broker, int32, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"LeaderAndEpoch\", arg0, arg1)\n\tret0, _ := ret[0].(*sarama.Broker)\n\tret1, _ := ret[1].(int32)\n\tret2, _ := ret[2].(error)\n\treturn ret0, ret1, ret2\n}\n\n// LeaderAndEpoch indicates an expected call of LeaderAndEpoch.\nfunc (mr *MockClientMockRecorder) LeaderAndEpoch(arg0, arg1 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"LeaderAndEpoch\", reflect.TypeOf((*MockClient)(nil).LeaderAndEpoch), arg0, arg1)\n}\n\n// LeastLoadedBroker mocks base method.\nfunc (m *MockClient) LeastLoadedBroker() *sarama.Broker {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"LeastLoadedBroker\")\n\tret0, _ := ret[0].(*sarama.Broker)\n\treturn ret0\n}\n\n// LeastLoadedBroker indicates an expected call of LeastLoadedBroker.\nfunc (mr *MockClientMockRecorder) LeastLoadedBroker() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"LeastLoadedBroker\", reflect.TypeOf((*MockClient)(nil).LeastLoadedBroker))\n}\n\n// OfflineReplicas mocks base method.\nfunc (m *MockClient) OfflineReplicas(arg0 string, arg1 int32) ([]int32, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"OfflineReplicas\", arg0, arg1)\n\tret0, _ := ret[0].([]int32)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// OfflineReplicas indicates an expected call of OfflineReplicas.\nfunc (mr *MockClientMockRecorder) OfflineReplicas(arg0, arg1 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"OfflineReplicas\", reflect.TypeOf((*MockClient)(nil).OfflineReplicas), arg0, arg1)\n}\n\n// Partitions mocks base method.\nfunc (m *MockClient) Partitions(arg0 string) ([]int32, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Partitions\", arg0)\n\tret0, _ := ret[0].([]int32)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// Partitions indicates an expected call of Partitions.\nfunc (mr *MockClientMockRecorder) Partitions(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Partitions\", reflect.TypeOf((*MockClient)(nil).Partitions), arg0)\n}\n\n// RefreshBrokers mocks base method.\nfunc (m *MockClient) RefreshBrokers(arg0 []string) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"RefreshBrokers\", arg0)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// RefreshBrokers indicates an expected call of RefreshBrokers.\nfunc (mr *MockClientMockRecorder) RefreshBrokers(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"RefreshBrokers\", reflect.TypeOf((*MockClient)(nil).RefreshBrokers), arg0)\n}\n\n// RefreshController mocks base method.\nfunc (m *MockClient) RefreshController() (*sarama.Broker, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"RefreshController\")\n\tret0, _ := ret[0].(*sarama.Broker)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// RefreshController indicates an expected call of RefreshController.\nfunc (mr *MockClientMockRecorder) RefreshController() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"RefreshController\", reflect.TypeOf((*MockClient)(nil).RefreshController))\n}\n\n// RefreshCoordinator mocks base method.\nfunc (m *MockClient) RefreshCoordinator(arg0 string) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"RefreshCoordinator\", arg0)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// RefreshCoordinator indicates an expected call of RefreshCoordinator.\nfunc (mr *MockClientMockRecorder) RefreshCoordinator(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"RefreshCoordinator\", reflect.TypeOf((*MockClient)(nil).RefreshCoordinator), arg0)\n}\n\n// RefreshMetadata mocks base method.\nfunc (m *MockClient) RefreshMetadata(arg0 ...string) error {\n\tm.ctrl.T.Helper()\n\tvarargs := []interface{}{}\n\tfor _, a := range arg0 {\n\t\tvarargs = append(varargs, a)\n\t}\n\tret := m.ctrl.Call(m, \"RefreshMetadata\", varargs...)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// RefreshMetadata indicates an expected call of RefreshMetadata.\nfunc (mr *MockClientMockRecorder) RefreshMetadata(arg0 ...interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"RefreshMetadata\", reflect.TypeOf((*MockClient)(nil).RefreshMetadata), arg0...)\n}\n\n// RefreshTransactionCoordinator mocks base method.\nfunc (m *MockClient) RefreshTransactionCoordinator(arg0 string) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"RefreshTransactionCoordinator\", arg0)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// RefreshTransactionCoordinator indicates an expected call of RefreshTransactionCoordinator.\nfunc (mr *MockClientMockRecorder) RefreshTransactionCoordinator(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"RefreshTransactionCoordinator\", reflect.TypeOf((*MockClient)(nil).RefreshTransactionCoordinator), arg0)\n}\n\n// Replicas mocks base method.\nfunc (m *MockClient) Replicas(arg0 string, arg1 int32) ([]int32, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Replicas\", arg0, arg1)\n\tret0, _ := ret[0].([]int32)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// Replicas indicates an expected call of Replicas.\nfunc (mr *MockClientMockRecorder) Replicas(arg0, arg1 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Replicas\", reflect.TypeOf((*MockClient)(nil).Replicas), arg0, arg1)\n}\n\n// Topics mocks base method.\nfunc (m *MockClient) Topics() ([]string, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Topics\")\n\tret0, _ := ret[0].([]string)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// Topics indicates an expected call of Topics.\nfunc (mr *MockClientMockRecorder) Topics() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Topics\", reflect.TypeOf((*MockClient)(nil).Topics))\n}\n\n// TransactionCoordinator mocks base method.\nfunc (m *MockClient) TransactionCoordinator(arg0 string) (*sarama.Broker, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"TransactionCoordinator\", arg0)\n\tret0, _ := ret[0].(*sarama.Broker)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// TransactionCoordinator indicates an expected call of TransactionCoordinator.\nfunc (mr *MockClientMockRecorder) TransactionCoordinator(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"TransactionCoordinator\", reflect.TypeOf((*MockClient)(nil).TransactionCoordinator), arg0)\n}\n\n// WritablePartitions mocks base method.\nfunc (m *MockClient) WritablePartitions(arg0 string) ([]int32, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"WritablePartitions\", arg0)\n\tret0, _ := ret[0].([]int32)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// WritablePartitions indicates an expected call of WritablePartitions.\nfunc (mr *MockClientMockRecorder) WritablePartitions(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"WritablePartitions\", reflect.TypeOf((*MockClient)(nil).WritablePartitions), arg0)\n}\n\n// MockClusterAdmin is a mock of ClusterAdmin interface.\ntype MockClusterAdmin struct {\n\tctrl     *gomock.Controller\n\trecorder *MockClusterAdminMockRecorder\n}\n\n// MockClusterAdminMockRecorder is the mock recorder for MockClusterAdmin.\ntype MockClusterAdminMockRecorder struct {\n\tmock *MockClusterAdmin\n}\n\n// NewMockClusterAdmin creates a new mock instance.\nfunc NewMockClusterAdmin(ctrl *gomock.Controller) *MockClusterAdmin {\n\tmock := &MockClusterAdmin{ctrl: ctrl}\n\tmock.recorder = &MockClusterAdminMockRecorder{mock}\n\treturn mock\n}\n\n// EXPECT returns an object that allows the caller to indicate expected use.\nfunc (m *MockClusterAdmin) EXPECT() *MockClusterAdminMockRecorder {\n\treturn m.recorder\n}\n\n// AlterClientQuotas mocks base method.\nfunc (m *MockClusterAdmin) AlterClientQuotas(arg0 []sarama.QuotaEntityComponent, arg1 sarama.ClientQuotasOp, arg2 bool) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"AlterClientQuotas\", arg0, arg1, arg2)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// AlterClientQuotas indicates an expected call of AlterClientQuotas.\nfunc (mr *MockClusterAdminMockRecorder) AlterClientQuotas(arg0, arg1, arg2 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"AlterClientQuotas\", reflect.TypeOf((*MockClusterAdmin)(nil).AlterClientQuotas), arg0, arg1, arg2)\n}\n\n// AlterConfig mocks base method.\nfunc (m *MockClusterAdmin) AlterConfig(arg0 sarama.ConfigResourceType, arg1 string, arg2 map[string]*string, arg3 bool) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"AlterConfig\", arg0, arg1, arg2, arg3)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// AlterConfig indicates an expected call of AlterConfig.\nfunc (mr *MockClusterAdminMockRecorder) AlterConfig(arg0, arg1, arg2, arg3 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"AlterConfig\", reflect.TypeOf((*MockClusterAdmin)(nil).AlterConfig), arg0, arg1, arg2, arg3)\n}\n\n// AlterPartitionReassignments mocks base method.\nfunc (m *MockClusterAdmin) AlterPartitionReassignments(arg0 string, arg1 [][]int32) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"AlterPartitionReassignments\", arg0, arg1)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// AlterPartitionReassignments indicates an expected call of AlterPartitionReassignments.\nfunc (mr *MockClusterAdminMockRecorder) AlterPartitionReassignments(arg0, arg1 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"AlterPartitionReassignments\", reflect.TypeOf((*MockClusterAdmin)(nil).AlterPartitionReassignments), arg0, arg1)\n}\n\n// Close mocks base method.\nfunc (m *MockClusterAdmin) Close() error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Close\")\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// Close indicates an expected call of Close.\nfunc (mr *MockClusterAdminMockRecorder) Close() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Close\", reflect.TypeOf((*MockClusterAdmin)(nil).Close))\n}\n\n// Controller mocks base method.\nfunc (m *MockClusterAdmin) Controller() (*sarama.Broker, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Controller\")\n\tret0, _ := ret[0].(*sarama.Broker)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// Controller indicates an expected call of Controller.\nfunc (mr *MockClusterAdminMockRecorder) Controller() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Controller\", reflect.TypeOf((*MockClusterAdmin)(nil).Controller))\n}\n\n// CreateACL mocks base method.\nfunc (m *MockClusterAdmin) CreateACL(arg0 sarama.Resource, arg1 sarama.Acl) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"CreateACL\", arg0, arg1)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// CreateACL indicates an expected call of CreateACL.\nfunc (mr *MockClusterAdminMockRecorder) CreateACL(arg0, arg1 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"CreateACL\", reflect.TypeOf((*MockClusterAdmin)(nil).CreateACL), arg0, arg1)\n}\n\n// CreateACLs mocks base method.\nfunc (m *MockClusterAdmin) CreateACLs(arg0 []*sarama.ResourceAcls) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"CreateACLs\", arg0)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// CreateACLs indicates an expected call of CreateACLs.\nfunc (mr *MockClusterAdminMockRecorder) CreateACLs(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"CreateACLs\", reflect.TypeOf((*MockClusterAdmin)(nil).CreateACLs), arg0)\n}\n\n// CreatePartitions mocks base method.\nfunc (m *MockClusterAdmin) CreatePartitions(arg0 string, arg1 int32, arg2 [][]int32, arg3 bool) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"CreatePartitions\", arg0, arg1, arg2, arg3)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// CreatePartitions indicates an expected call of CreatePartitions.\nfunc (mr *MockClusterAdminMockRecorder) CreatePartitions(arg0, arg1, arg2, arg3 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"CreatePartitions\", reflect.TypeOf((*MockClusterAdmin)(nil).CreatePartitions), arg0, arg1, arg2, arg3)\n}\n\n// CreateTopic mocks base method.\nfunc (m *MockClusterAdmin) CreateTopic(arg0 string, arg1 *sarama.TopicDetail, arg2 bool) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"CreateTopic\", arg0, arg1, arg2)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// CreateTopic indicates an expected call of CreateTopic.\nfunc (mr *MockClusterAdminMockRecorder) CreateTopic(arg0, arg1, arg2 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"CreateTopic\", reflect.TypeOf((*MockClusterAdmin)(nil).CreateTopic), arg0, arg1, arg2)\n}\n\n// DeleteACL mocks base method.\nfunc (m *MockClusterAdmin) DeleteACL(arg0 sarama.AclFilter, arg1 bool) ([]sarama.MatchingAcl, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"DeleteACL\", arg0, arg1)\n\tret0, _ := ret[0].([]sarama.MatchingAcl)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// DeleteACL indicates an expected call of DeleteACL.\nfunc (mr *MockClusterAdminMockRecorder) DeleteACL(arg0, arg1 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"DeleteACL\", reflect.TypeOf((*MockClusterAdmin)(nil).DeleteACL), arg0, arg1)\n}\n\n// DeleteConsumerGroup mocks base method.\nfunc (m *MockClusterAdmin) DeleteConsumerGroup(arg0 string) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"DeleteConsumerGroup\", arg0)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// DeleteConsumerGroup indicates an expected call of DeleteConsumerGroup.\nfunc (mr *MockClusterAdminMockRecorder) DeleteConsumerGroup(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"DeleteConsumerGroup\", reflect.TypeOf((*MockClusterAdmin)(nil).DeleteConsumerGroup), arg0)\n}\n\n// DeleteConsumerGroupOffset mocks base method.\nfunc (m *MockClusterAdmin) DeleteConsumerGroupOffset(arg0, arg1 string, arg2 int32) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"DeleteConsumerGroupOffset\", arg0, arg1, arg2)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// DeleteConsumerGroupOffset indicates an expected call of DeleteConsumerGroupOffset.\nfunc (mr *MockClusterAdminMockRecorder) DeleteConsumerGroupOffset(arg0, arg1, arg2 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"DeleteConsumerGroupOffset\", reflect.TypeOf((*MockClusterAdmin)(nil).DeleteConsumerGroupOffset), arg0, arg1, arg2)\n}\n\n// DeleteRecords mocks base method.\nfunc (m *MockClusterAdmin) DeleteRecords(arg0 string, arg1 map[int32]int64) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"DeleteRecords\", arg0, arg1)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// DeleteRecords indicates an expected call of DeleteRecords.\nfunc (mr *MockClusterAdminMockRecorder) DeleteRecords(arg0, arg1 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"DeleteRecords\", reflect.TypeOf((*MockClusterAdmin)(nil).DeleteRecords), arg0, arg1)\n}\n\n// DeleteTopic mocks base method.\nfunc (m *MockClusterAdmin) DeleteTopic(arg0 string) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"DeleteTopic\", arg0)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// DeleteTopic indicates an expected call of DeleteTopic.\nfunc (mr *MockClusterAdminMockRecorder) DeleteTopic(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"DeleteTopic\", reflect.TypeOf((*MockClusterAdmin)(nil).DeleteTopic), arg0)\n}\n\n// DeleteUserScramCredentials mocks base method.\nfunc (m *MockClusterAdmin) DeleteUserScramCredentials(arg0 []sarama.AlterUserScramCredentialsDelete) ([]*sarama.AlterUserScramCredentialsResult, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"DeleteUserScramCredentials\", arg0)\n\tret0, _ := ret[0].([]*sarama.AlterUserScramCredentialsResult)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// DeleteUserScramCredentials indicates an expected call of DeleteUserScramCredentials.\nfunc (mr *MockClusterAdminMockRecorder) DeleteUserScramCredentials(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"DeleteUserScramCredentials\", reflect.TypeOf((*MockClusterAdmin)(nil).DeleteUserScramCredentials), arg0)\n}\n\n// DescribeClientQuotas mocks base method.\nfunc (m *MockClusterAdmin) DescribeClientQuotas(arg0 []sarama.QuotaFilterComponent, arg1 bool) ([]sarama.DescribeClientQuotasEntry, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"DescribeClientQuotas\", arg0, arg1)\n\tret0, _ := ret[0].([]sarama.DescribeClientQuotasEntry)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// DescribeClientQuotas indicates an expected call of DescribeClientQuotas.\nfunc (mr *MockClusterAdminMockRecorder) DescribeClientQuotas(arg0, arg1 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"DescribeClientQuotas\", reflect.TypeOf((*MockClusterAdmin)(nil).DescribeClientQuotas), arg0, arg1)\n}\n\n// DescribeCluster mocks base method.\nfunc (m *MockClusterAdmin) DescribeCluster() ([]*sarama.Broker, int32, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"DescribeCluster\")\n\tret0, _ := ret[0].([]*sarama.Broker)\n\tret1, _ := ret[1].(int32)\n\tret2, _ := ret[2].(error)\n\treturn ret0, ret1, ret2\n}\n\n// DescribeCluster indicates an expected call of DescribeCluster.\nfunc (mr *MockClusterAdminMockRecorder) DescribeCluster() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"DescribeCluster\", reflect.TypeOf((*MockClusterAdmin)(nil).DescribeCluster))\n}\n\n// DescribeConfig mocks base method.\nfunc (m *MockClusterAdmin) DescribeConfig(arg0 sarama.ConfigResource) ([]sarama.ConfigEntry, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"DescribeConfig\", arg0)\n\tret0, _ := ret[0].([]sarama.ConfigEntry)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// DescribeConfig indicates an expected call of DescribeConfig.\nfunc (mr *MockClusterAdminMockRecorder) DescribeConfig(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"DescribeConfig\", reflect.TypeOf((*MockClusterAdmin)(nil).DescribeConfig), arg0)\n}\n\n// DescribeConsumerGroups mocks base method.\nfunc (m *MockClusterAdmin) DescribeConsumerGroups(arg0 []string) ([]*sarama.GroupDescription, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"DescribeConsumerGroups\", arg0)\n\tret0, _ := ret[0].([]*sarama.GroupDescription)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// DescribeConsumerGroups indicates an expected call of DescribeConsumerGroups.\nfunc (mr *MockClusterAdminMockRecorder) DescribeConsumerGroups(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"DescribeConsumerGroups\", reflect.TypeOf((*MockClusterAdmin)(nil).DescribeConsumerGroups), arg0)\n}\n\n// DescribeLogDirs mocks base method.\nfunc (m *MockClusterAdmin) DescribeLogDirs(arg0 []int32) (map[int32][]sarama.DescribeLogDirsResponseDirMetadata, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"DescribeLogDirs\", arg0)\n\tret0, _ := ret[0].(map[int32][]sarama.DescribeLogDirsResponseDirMetadata)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// DescribeLogDirs indicates an expected call of DescribeLogDirs.\nfunc (mr *MockClusterAdminMockRecorder) DescribeLogDirs(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"DescribeLogDirs\", reflect.TypeOf((*MockClusterAdmin)(nil).DescribeLogDirs), arg0)\n}\n\n// DescribeTopics mocks base method.\nfunc (m *MockClusterAdmin) DescribeTopics(arg0 []string) ([]*sarama.TopicMetadata, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"DescribeTopics\", arg0)\n\tret0, _ := ret[0].([]*sarama.TopicMetadata)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// DescribeTopics indicates an expected call of DescribeTopics.\nfunc (mr *MockClusterAdminMockRecorder) DescribeTopics(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"DescribeTopics\", reflect.TypeOf((*MockClusterAdmin)(nil).DescribeTopics), arg0)\n}\n\n// DescribeUserScramCredentials mocks base method.\nfunc (m *MockClusterAdmin) DescribeUserScramCredentials(arg0 []string) ([]*sarama.DescribeUserScramCredentialsResult, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"DescribeUserScramCredentials\", arg0)\n\tret0, _ := ret[0].([]*sarama.DescribeUserScramCredentialsResult)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// DescribeUserScramCredentials indicates an expected call of DescribeUserScramCredentials.\nfunc (mr *MockClusterAdminMockRecorder) DescribeUserScramCredentials(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"DescribeUserScramCredentials\", reflect.TypeOf((*MockClusterAdmin)(nil).DescribeUserScramCredentials), arg0)\n}\n\n// IncrementalAlterConfig mocks base method.\nfunc (m *MockClusterAdmin) IncrementalAlterConfig(arg0 sarama.ConfigResourceType, arg1 string, arg2 map[string]sarama.IncrementalAlterConfigsEntry, arg3 bool) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"IncrementalAlterConfig\", arg0, arg1, arg2, arg3)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// IncrementalAlterConfig indicates an expected call of IncrementalAlterConfig.\nfunc (mr *MockClusterAdminMockRecorder) IncrementalAlterConfig(arg0, arg1, arg2, arg3 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"IncrementalAlterConfig\", reflect.TypeOf((*MockClusterAdmin)(nil).IncrementalAlterConfig), arg0, arg1, arg2, arg3)\n}\n\n// ListAcls mocks base method.\nfunc (m *MockClusterAdmin) ListAcls(arg0 sarama.AclFilter) ([]sarama.ResourceAcls, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"ListAcls\", arg0)\n\tret0, _ := ret[0].([]sarama.ResourceAcls)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// ListAcls indicates an expected call of ListAcls.\nfunc (mr *MockClusterAdminMockRecorder) ListAcls(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"ListAcls\", reflect.TypeOf((*MockClusterAdmin)(nil).ListAcls), arg0)\n}\n\n// ListConsumerGroupOffsets mocks base method.\nfunc (m *MockClusterAdmin) ListConsumerGroupOffsets(arg0 string, arg1 map[string][]int32) (*sarama.OffsetFetchResponse, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"ListConsumerGroupOffsets\", arg0, arg1)\n\tret0, _ := ret[0].(*sarama.OffsetFetchResponse)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// ListConsumerGroupOffsets indicates an expected call of ListConsumerGroupOffsets.\nfunc (mr *MockClusterAdminMockRecorder) ListConsumerGroupOffsets(arg0, arg1 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"ListConsumerGroupOffsets\", reflect.TypeOf((*MockClusterAdmin)(nil).ListConsumerGroupOffsets), arg0, arg1)\n}\n\n// ListConsumerGroups mocks base method.\nfunc (m *MockClusterAdmin) ListConsumerGroups() (map[string]string, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"ListConsumerGroups\")\n\tret0, _ := ret[0].(map[string]string)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// ListConsumerGroups indicates an expected call of ListConsumerGroups.\nfunc (mr *MockClusterAdminMockRecorder) ListConsumerGroups() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"ListConsumerGroups\", reflect.TypeOf((*MockClusterAdmin)(nil).ListConsumerGroups))\n}\n\n// ListPartitionReassignments mocks base method.\nfunc (m *MockClusterAdmin) ListPartitionReassignments(arg0 string, arg1 []int32) (map[string]map[int32]*sarama.PartitionReplicaReassignmentsStatus, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"ListPartitionReassignments\", arg0, arg1)\n\tret0, _ := ret[0].(map[string]map[int32]*sarama.PartitionReplicaReassignmentsStatus)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// ListPartitionReassignments indicates an expected call of ListPartitionReassignments.\nfunc (mr *MockClusterAdminMockRecorder) ListPartitionReassignments(arg0, arg1 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"ListPartitionReassignments\", reflect.TypeOf((*MockClusterAdmin)(nil).ListPartitionReassignments), arg0, arg1)\n}\n\n// ListTopics mocks base method.\nfunc (m *MockClusterAdmin) ListTopics() (map[string]sarama.TopicDetail, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"ListTopics\")\n\tret0, _ := ret[0].(map[string]sarama.TopicDetail)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// ListTopics indicates an expected call of ListTopics.\nfunc (mr *MockClusterAdminMockRecorder) ListTopics() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"ListTopics\", reflect.TypeOf((*MockClusterAdmin)(nil).ListTopics))\n}\n\n// RemoveMemberFromConsumerGroup mocks base method.\nfunc (m *MockClusterAdmin) RemoveMemberFromConsumerGroup(arg0 string, arg1 []string) (*sarama.LeaveGroupResponse, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"RemoveMemberFromConsumerGroup\", arg0, arg1)\n\tret0, _ := ret[0].(*sarama.LeaveGroupResponse)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// RemoveMemberFromConsumerGroup indicates an expected call of RemoveMemberFromConsumerGroup.\nfunc (mr *MockClusterAdminMockRecorder) RemoveMemberFromConsumerGroup(arg0, arg1 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"RemoveMemberFromConsumerGroup\", reflect.TypeOf((*MockClusterAdmin)(nil).RemoveMemberFromConsumerGroup), arg0, arg1)\n}\n\n// UpsertUserScramCredentials mocks base method.\nfunc (m *MockClusterAdmin) UpsertUserScramCredentials(arg0 []sarama.AlterUserScramCredentialsUpsert) ([]*sarama.AlterUserScramCredentialsResult, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"UpsertUserScramCredentials\", arg0)\n\tret0, _ := ret[0].([]*sarama.AlterUserScramCredentialsResult)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// UpsertUserScramCredentials indicates an expected call of UpsertUserScramCredentials.\nfunc (mr *MockClusterAdminMockRecorder) UpsertUserScramCredentials(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"UpsertUserScramCredentials\", reflect.TypeOf((*MockClusterAdmin)(nil).UpsertUserScramCredentials), arg0)\n}\n"
        },
        {
          "name": "mockstorage.go",
          "type": "blob",
          "size": 5.93359375,
          "content": "// Code generated by MockGen. DO NOT EDIT.\n// Source: github.com/lovoo/goka/storage (interfaces: Storage)\n\n// Package goka is a generated GoMock package.\npackage goka\n\nimport (\n\treflect \"reflect\"\n\n\tgomock \"github.com/golang/mock/gomock\"\n\tstorage \"github.com/lovoo/goka/storage\"\n)\n\n// MockStorage is a mock of Storage interface.\ntype MockStorage struct {\n\tctrl     *gomock.Controller\n\trecorder *MockStorageMockRecorder\n}\n\n// MockStorageMockRecorder is the mock recorder for MockStorage.\ntype MockStorageMockRecorder struct {\n\tmock *MockStorage\n}\n\n// NewMockStorage creates a new mock instance.\nfunc NewMockStorage(ctrl *gomock.Controller) *MockStorage {\n\tmock := &MockStorage{ctrl: ctrl}\n\tmock.recorder = &MockStorageMockRecorder{mock}\n\treturn mock\n}\n\n// EXPECT returns an object that allows the caller to indicate expected use.\nfunc (m *MockStorage) EXPECT() *MockStorageMockRecorder {\n\treturn m.recorder\n}\n\n// Close mocks base method.\nfunc (m *MockStorage) Close() error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Close\")\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// Close indicates an expected call of Close.\nfunc (mr *MockStorageMockRecorder) Close() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Close\", reflect.TypeOf((*MockStorage)(nil).Close))\n}\n\n// Delete mocks base method.\nfunc (m *MockStorage) Delete(arg0 string) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Delete\", arg0)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// Delete indicates an expected call of Delete.\nfunc (mr *MockStorageMockRecorder) Delete(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Delete\", reflect.TypeOf((*MockStorage)(nil).Delete), arg0)\n}\n\n// Get mocks base method.\nfunc (m *MockStorage) Get(arg0 string) ([]byte, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Get\", arg0)\n\tret0, _ := ret[0].([]byte)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// Get indicates an expected call of Get.\nfunc (mr *MockStorageMockRecorder) Get(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Get\", reflect.TypeOf((*MockStorage)(nil).Get), arg0)\n}\n\n// GetOffset mocks base method.\nfunc (m *MockStorage) GetOffset(arg0 int64) (int64, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"GetOffset\", arg0)\n\tret0, _ := ret[0].(int64)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// GetOffset indicates an expected call of GetOffset.\nfunc (mr *MockStorageMockRecorder) GetOffset(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"GetOffset\", reflect.TypeOf((*MockStorage)(nil).GetOffset), arg0)\n}\n\n// Has mocks base method.\nfunc (m *MockStorage) Has(arg0 string) (bool, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Has\", arg0)\n\tret0, _ := ret[0].(bool)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// Has indicates an expected call of Has.\nfunc (mr *MockStorageMockRecorder) Has(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Has\", reflect.TypeOf((*MockStorage)(nil).Has), arg0)\n}\n\n// Iterator mocks base method.\nfunc (m *MockStorage) Iterator() (storage.Iterator, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Iterator\")\n\tret0, _ := ret[0].(storage.Iterator)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// Iterator indicates an expected call of Iterator.\nfunc (mr *MockStorageMockRecorder) Iterator() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Iterator\", reflect.TypeOf((*MockStorage)(nil).Iterator))\n}\n\n// IteratorWithRange mocks base method.\nfunc (m *MockStorage) IteratorWithRange(arg0, arg1 []byte) (storage.Iterator, error) {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"IteratorWithRange\", arg0, arg1)\n\tret0, _ := ret[0].(storage.Iterator)\n\tret1, _ := ret[1].(error)\n\treturn ret0, ret1\n}\n\n// IteratorWithRange indicates an expected call of IteratorWithRange.\nfunc (mr *MockStorageMockRecorder) IteratorWithRange(arg0, arg1 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"IteratorWithRange\", reflect.TypeOf((*MockStorage)(nil).IteratorWithRange), arg0, arg1)\n}\n\n// MarkRecovered mocks base method.\nfunc (m *MockStorage) MarkRecovered() error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"MarkRecovered\")\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// MarkRecovered indicates an expected call of MarkRecovered.\nfunc (mr *MockStorageMockRecorder) MarkRecovered() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"MarkRecovered\", reflect.TypeOf((*MockStorage)(nil).MarkRecovered))\n}\n\n// Open mocks base method.\nfunc (m *MockStorage) Open() error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Open\")\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// Open indicates an expected call of Open.\nfunc (mr *MockStorageMockRecorder) Open() *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Open\", reflect.TypeOf((*MockStorage)(nil).Open))\n}\n\n// Set mocks base method.\nfunc (m *MockStorage) Set(arg0 string, arg1 []byte) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"Set\", arg0, arg1)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// Set indicates an expected call of Set.\nfunc (mr *MockStorageMockRecorder) Set(arg0, arg1 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"Set\", reflect.TypeOf((*MockStorage)(nil).Set), arg0, arg1)\n}\n\n// SetOffset mocks base method.\nfunc (m *MockStorage) SetOffset(arg0 int64) error {\n\tm.ctrl.T.Helper()\n\tret := m.ctrl.Call(m, \"SetOffset\", arg0)\n\tret0, _ := ret[0].(error)\n\treturn ret0\n}\n\n// SetOffset indicates an expected call of SetOffset.\nfunc (mr *MockStorageMockRecorder) SetOffset(arg0 interface{}) *gomock.Call {\n\tmr.mock.ctrl.T.Helper()\n\treturn mr.mock.ctrl.RecordCallWithMethodType(mr.mock, \"SetOffset\", reflect.TypeOf((*MockStorage)(nil).SetOffset), arg0)\n}\n"
        },
        {
          "name": "multierr",
          "type": "tree",
          "content": null
        },
        {
          "name": "once.go",
          "type": "blob",
          "size": 0.2216796875,
          "content": "package goka\n\nimport \"sync\"\n\ntype once struct {\n\tonce sync.Once\n\terr  error\n}\n\n// Do runs only once and always return the same error.\nfunc (o *once) Do(f func() error) error {\n\to.once.Do(func() { o.err = f() })\n\treturn o.err\n}\n"
        },
        {
          "name": "once_test.go",
          "type": "blob",
          "size": 0.310546875,
          "content": "package goka\n\nimport (\n\t\"errors\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestOnce_Do(t *testing.T) {\n\tvar o once\n\n\terr := o.Do(func() error { return errors.New(\"some error\") })\n\trequire.Error(t, err)\n\n\terr2 := o.Do(func() error { return nil })\n\trequire.Error(t, err2)\n\trequire.Equal(t, err2, err)\n}\n"
        },
        {
          "name": "options.go",
          "type": "blob",
          "size": 20.0498046875,
          "content": "package goka\n\nimport (\n\t\"fmt\"\n\t\"hash\"\n\t\"hash/fnv\"\n\t\"log\"\n\t\"path/filepath\"\n\t\"time\"\n\n\t\"github.com/IBM/sarama\"\n\t\"github.com/lovoo/goka/storage\"\n)\n\n// UpdateContext defines the interface for UpdateCallback arguments.\ntype UpdateContext interface {\n\t// Topic returns the topic of input message.\n\tTopic() Stream\n\n\t// Partition returns the partition of the input message.\n\tPartition() int32\n\n\t// Offset returns the offset of the input message.\n\tOffset() int64\n\n\t// Headers returns the headers of the input message.\n\t//\n\t// It is recommended to lazily evaluate the headers to reduce overhead per message\n\t// when headers are not used.\n\tHeaders() Headers\n\n\t// Timestamp returns the timestamp of the input message.\n\tTimestamp() time.Time\n}\n\n// UpdateCallback is invoked upon arrival of a message for a table partition.\ntype UpdateCallback func(ctx UpdateContext, s storage.Storage, key string, value []byte) error\n\n// RebalanceCallback is invoked when the processor receives a new partition assignment.\ntype RebalanceCallback func(a Assignment)\n\n///////////////////////////////////////////////////////////////////////////////\n// default values\n///////////////////////////////////////////////////////////////////////////////\n\nconst (\n\tdefaultBaseStoragePath = \"/tmp/goka\"\n\tdefaultClientID        = \"goka\"\n\n\t// duration, after which we'll reset the backoff\n\tdefaultBackoffResetTime = 60 * time.Second\n\t// duration to increase each time retrying\n\tdefaultBackoffStep = 10 * time.Second\n\t// maximum duration to wait for the backoff\n\tdefaultBackoffMax = 120 * time.Second\n\n\tdefaultPPVisitChannelSize = 100\n)\n\n// DefaultProcessorStoragePath is the default path where processor state\n// will be stored.\nfunc DefaultProcessorStoragePath(group Group) string {\n\treturn filepath.Join(defaultBaseStoragePath, \"processor\", string(group))\n}\n\n// DefaultViewStoragePath returns the default path where view state will be stored.\nfunc DefaultViewStoragePath() string {\n\treturn filepath.Join(defaultBaseStoragePath, \"view\")\n}\n\n// DefaultUpdate is the default callback used to update the local storage with\n// from the table topic in Kafka. It is called for every message received\n// during recovery of processors and during the normal operation of views.\n// DefaultUpdate can be used in the function passed to WithUpdateCallback and\n// WithViewCallback.\nfunc DefaultUpdate(ctx UpdateContext, s storage.Storage, key string, value []byte) error {\n\tif value == nil {\n\t\treturn s.Delete(key)\n\t}\n\n\treturn s.Set(key, value)\n}\n\n// DefaultRebalance is the default callback when a new partition assignment is received.\n// DefaultRebalance can be used in the function passed to WithRebalanceCallback.\nfunc DefaultRebalance(a Assignment) {}\n\n// DefaultHasher returns an FNV hasher builder to assign keys to partitions.\nfunc DefaultHasher() func() hash.Hash32 {\n\treturn func() hash.Hash32 {\n\t\treturn fnv.New32a()\n\t}\n}\n\n// DefaultUpdateContext implements the UpdateContext interface.\ntype DefaultUpdateContext struct {\n\ttopic     Stream\n\tpartition int32\n\toffset    int64\n\theaders   []*sarama.RecordHeader\n\ttimestamp time.Time\n}\n\n// Topic returns the topic of input message.\nfunc (ctx DefaultUpdateContext) Topic() Stream {\n\treturn ctx.topic\n}\n\n// Partition returns the partition of the input message.\nfunc (ctx DefaultUpdateContext) Partition() int32 {\n\treturn ctx.partition\n}\n\n// Offset returns the offset of the input message.\nfunc (ctx DefaultUpdateContext) Offset() int64 {\n\treturn ctx.offset\n}\n\n// Headers returns the headers of the input message.\nfunc (ctx DefaultUpdateContext) Headers() Headers {\n\treturn HeadersFromSarama(ctx.headers)\n}\n\n// Timestamp returns the timestamp of the input message.\nfunc (ctx DefaultUpdateContext) Timestamp() time.Time {\n\treturn ctx.timestamp\n}\n\n///////////////////////////////////////////////////////////////////////////////\n// processor options\n///////////////////////////////////////////////////////////////////////////////\n\n// ProcessorOption defines a configuration option to be used when creating a processor.\ntype ProcessorOption func(*poptions, *GroupGraph)\n\n// processor options\ntype poptions struct {\n\tlog      logger\n\tclientID string\n\n\tupdateCallback         UpdateCallback\n\trebalanceCallback      RebalanceCallback\n\tcontextWrapper         ContextWrapper\n\tpartitionChannelSize   int\n\thasher                 func() hash.Hash32\n\tnilHandling            NilHandling\n\tbackoffResetTime       time.Duration\n\thotStandby             bool\n\trecoverAhead           bool\n\tproducerDefaultHeaders Headers\n\n\tbuilders struct {\n\t\tstorage        storage.Builder\n\t\tconsumerSarama SaramaConsumerBuilder\n\t\tconsumerGroup  ConsumerGroupBuilder\n\t\tproducer       ProducerBuilder\n\t\ttopicmgr       TopicManagerBuilder\n\t\tbackoff        BackoffBuilder\n\t}\n}\n\n// WithContextWrapper allows to intercept the context passed to each callback invocation.\n// The wrapper function will be called concurrently across all partitions the returned context\n// must not be shared.\nfunc WithContextWrapper(wrapper ContextWrapper) ProcessorOption {\n\treturn func(o *poptions, gg *GroupGraph) {\n\t\to.contextWrapper = wrapper\n\t}\n}\n\n// WithUpdateCallback defines the callback called upon recovering a message\n// from the log.\nfunc WithUpdateCallback(cb UpdateCallback) ProcessorOption {\n\treturn func(o *poptions, gg *GroupGraph) {\n\t\to.updateCallback = cb\n\t}\n}\n\n// WithClientID defines the client ID used to identify with Kafka.\nfunc WithClientID(clientID string) ProcessorOption {\n\treturn func(o *poptions, gg *GroupGraph) {\n\t\to.clientID = clientID\n\t}\n}\n\n// WithStorageBuilder defines a builder for the storage of each partition.\nfunc WithStorageBuilder(sb storage.Builder) ProcessorOption {\n\treturn func(o *poptions, gg *GroupGraph) {\n\t\to.builders.storage = sb\n\t}\n}\n\n// WithTopicManagerBuilder replaces the default topic manager builder.\nfunc WithTopicManagerBuilder(tmb TopicManagerBuilder) ProcessorOption {\n\treturn func(o *poptions, gg *GroupGraph) {\n\t\to.builders.topicmgr = tmb\n\t}\n}\n\n// WithConsumerGroupBuilder replaces the default consumer group builder\nfunc WithConsumerGroupBuilder(cgb ConsumerGroupBuilder) ProcessorOption {\n\treturn func(o *poptions, gg *GroupGraph) {\n\t\to.builders.consumerGroup = cgb\n\t}\n}\n\n// WithConsumerSaramaBuilder replaces the default consumer group builder\nfunc WithConsumerSaramaBuilder(cgb SaramaConsumerBuilder) ProcessorOption {\n\treturn func(o *poptions, gg *GroupGraph) {\n\t\to.builders.consumerSarama = cgb\n\t}\n}\n\n// WithProducerBuilder replaces the default producer builder.\nfunc WithProducerBuilder(pb ProducerBuilder) ProcessorOption {\n\treturn func(o *poptions, gg *GroupGraph) {\n\t\to.builders.producer = pb\n\t}\n}\n\n// WithBackoffBuilder replaced the default backoff.\nfunc WithBackoffBuilder(bb BackoffBuilder) ProcessorOption {\n\treturn func(o *poptions, gg *GroupGraph) {\n\t\to.builders.backoff = bb\n\t}\n}\n\n// WithPartitionChannelSize replaces the default partition channel size.\n// This is mostly used for testing by setting it to 0 to have synchronous behavior\n// of goka.\nfunc WithPartitionChannelSize(size int) ProcessorOption {\n\treturn func(o *poptions, gg *GroupGraph) {\n\t\to.partitionChannelSize = size\n\t}\n}\n\n// WithLogger sets the logger the processor should use. By default, processors\n// use the standard library logger.\nfunc WithLogger(l Logger) ProcessorOption {\n\treturn func(o *poptions, gg *GroupGraph) {\n\t\tif prefixLogger, ok := l.(logger); ok {\n\t\t\to.log = prefixLogger\n\t\t} else {\n\t\t\to.log = wrapLogger(l, defaultLogger.debug)\n\t\t}\n\t}\n}\n\n// WithHasher sets the hash function that assigns keys to partitions.\nfunc WithHasher(hasher func() hash.Hash32) ProcessorOption {\n\treturn func(o *poptions, gg *GroupGraph) {\n\t\to.hasher = hasher\n\t}\n}\n\n// WithHotStandby configures the processor to keep partitions up to date which are not part\n// of the current generation's assignment. This allows fast processor failover since\n// all partitions are hot in other processor instances, but it requires\n// more resources (in particular network and disk).\n// If this option is used, the option `WithRecoverAhead` should also be added to avoid unnecessary delays.\nfunc WithHotStandby() ProcessorOption {\n\treturn func(o *poptions, gg *GroupGraph) {\n\t\to.hotStandby = true\n\t}\n}\n\n// WithRecoverAhead configures the processor to recover joins and the processor table ahead\n// of joining the group. This reduces the processing delay that occurs when adding new instances to\n// groups with high-volume-joins/tables. If the processor does not use joins or a table, it does not have any\n// effect.\nfunc WithRecoverAhead() ProcessorOption {\n\treturn func(o *poptions, gg *GroupGraph) {\n\t\to.recoverAhead = true\n\t}\n}\n\n// WithGroupGraphHook allows a function to obtain the group graph when a processor is started.\nfunc WithGroupGraphHook(hook func(gg *GroupGraph)) ProcessorOption {\n\treturn func(o *poptions, gg *GroupGraph) {\n\t\thook(gg)\n\t}\n}\n\n// WithBackoffResetTimeout defines the timeout when the backoff\n// will be reset.\nfunc WithBackoffResetTimeout(duration time.Duration) ProcessorOption {\n\treturn func(o *poptions, gg *GroupGraph) {\n\t\to.backoffResetTime = duration\n\t}\n}\n\n// WithProducerDefaultHeaders configures the producer with default headers\n// which are included with every emit.\nfunc WithProducerDefaultHeaders(hdr Headers) ProcessorOption {\n\treturn func(p *poptions, graph *GroupGraph) {\n\t\tp.producerDefaultHeaders = hdr\n\t}\n}\n\n// NilHandling defines how nil messages should be handled by the processor.\ntype NilHandling int\n\nconst (\n\t// NilIgnore drops any message with nil value.\n\tNilIgnore NilHandling = 0 + iota\n\t// NilProcess passes the nil value to ProcessCallback.\n\tNilProcess\n\t// NilDecode passes the nil value to decoder before calling ProcessCallback.\n\tNilDecode\n)\n\n// WithNilHandling configures how the processor should handle messages with nil\n// value. By default the processor ignores nil messages.\nfunc WithNilHandling(nh NilHandling) ProcessorOption {\n\treturn func(o *poptions, gg *GroupGraph) {\n\t\to.nilHandling = nh\n\t}\n}\n\n// Tester interface to avoid import cycles when a processor needs to register to\n// the tester.\ntype Tester interface {\n\tStorageBuilder() storage.Builder\n\tProducerBuilder() ProducerBuilder\n\tConsumerGroupBuilder() ConsumerGroupBuilder\n\tConsumerBuilder() SaramaConsumerBuilder\n\tEmitterProducerBuilder() ProducerBuilder\n\tTopicManagerBuilder() TopicManagerBuilder\n\tRegisterGroupGraph(*GroupGraph) string\n\tRegisterEmitter(Stream, Codec)\n\tRegisterView(Table, Codec) string\n}\n\n// WithTester configures all external connections of a processor, ie, storage,\n// consumer and producer\nfunc WithTester(t Tester) ProcessorOption {\n\treturn func(o *poptions, gg *GroupGraph) {\n\t\to.builders.storage = t.StorageBuilder()\n\t\to.builders.producer = t.ProducerBuilder()\n\t\to.builders.topicmgr = t.TopicManagerBuilder()\n\t\to.builders.consumerGroup = t.ConsumerGroupBuilder()\n\t\to.builders.consumerSarama = t.ConsumerBuilder()\n\t\to.partitionChannelSize = 0\n\t\to.clientID = t.RegisterGroupGraph(gg)\n\t}\n}\n\nfunc (opt *poptions) applyOptions(gg *GroupGraph, opts ...ProcessorOption) error {\n\topt.clientID = defaultClientID\n\topt.log = defaultLogger\n\topt.hasher = DefaultHasher()\n\topt.backoffResetTime = defaultBackoffResetTime\n\t// default context wrapper returns the original context\n\topt.contextWrapper = func(ctx Context) Context { return ctx }\n\n\tfor _, o := range opts {\n\t\to(opt, gg)\n\t}\n\n\t// StorageBuilder should always be set as a default option in NewProcessor\n\tif opt.builders.storage == nil {\n\t\treturn fmt.Errorf(\"StorageBuilder not set\")\n\t}\n\n\tif globalConfig.Producer.RequiredAcks == sarama.NoResponse {\n\t\treturn fmt.Errorf(\"Processors do not work with `Config.Producer.RequiredAcks==sarama.NoResponse`, as it uses the response's offset to store the value\")\n\t}\n\n\tif opt.builders.producer == nil {\n\t\topt.builders.producer = DefaultProducerBuilder\n\t}\n\n\tif opt.builders.topicmgr == nil {\n\t\topt.builders.topicmgr = DefaultTopicManagerBuilder\n\t}\n\n\tif opt.builders.consumerGroup == nil {\n\t\topt.builders.consumerGroup = DefaultConsumerGroupBuilder\n\t}\n\n\tif opt.builders.consumerSarama == nil {\n\t\topt.builders.consumerSarama = DefaultSaramaConsumerBuilder\n\t}\n\n\tif opt.builders.backoff == nil {\n\t\topt.builders.backoff = DefaultBackoffBuilder\n\t}\n\n\treturn nil\n}\n\n// WithRebalanceCallback sets the callback for when a new partition assignment\n// is received. By default, this is an empty function.\nfunc WithRebalanceCallback(cb RebalanceCallback) ProcessorOption {\n\treturn func(o *poptions, gg *GroupGraph) {\n\t\to.rebalanceCallback = cb\n\t}\n}\n\n///////////////////////////////////////////////////////////////////////////////\n// view options\n///////////////////////////////////////////////////////////////////////////////\n\n// ViewOption defines a configuration option to be used when creating a view.\ntype ViewOption func(*voptions, Table, Codec)\n\ntype voptions struct {\n\tlog              logger\n\tclientID         string\n\ttableCodec       Codec\n\tupdateCallback   UpdateCallback\n\thasher           func() hash.Hash32\n\tautoreconnect    bool\n\tbackoffResetTime time.Duration\n\n\tbuilders struct {\n\t\tstorage        storage.Builder\n\t\tconsumerSarama SaramaConsumerBuilder\n\t\ttopicmgr       TopicManagerBuilder\n\t\tbackoff        BackoffBuilder\n\t}\n}\n\n// WithViewLogger sets the logger the view should use. By default, views\n// use the standard library logger.\nfunc WithViewLogger(l Logger) ViewOption {\n\treturn func(o *voptions, table Table, codec Codec) {\n\t\tif prefixLogger, ok := l.(logger); ok {\n\t\t\to.log = prefixLogger\n\t\t} else {\n\t\t\to.log = wrapLogger(l, defaultLogger.debug)\n\t\t}\n\t}\n}\n\n// WithViewCallback defines the callback called upon recovering a message\n// from the log.\nfunc WithViewCallback(cb UpdateCallback) ViewOption {\n\treturn func(o *voptions, table Table, codec Codec) {\n\t\to.updateCallback = cb\n\t}\n}\n\n// WithViewStorageBuilder defines a builder for the storage of each partition.\nfunc WithViewStorageBuilder(sb storage.Builder) ViewOption {\n\treturn func(o *voptions, table Table, codec Codec) {\n\t\to.builders.storage = sb\n\t}\n}\n\n// WithViewConsumerSaramaBuilder replaces the default sarama consumer builder\nfunc WithViewConsumerSaramaBuilder(cgb SaramaConsumerBuilder) ViewOption {\n\treturn func(o *voptions, table Table, codec Codec) {\n\t\to.builders.consumerSarama = cgb\n\t}\n}\n\n// WithViewTopicManagerBuilder replaces the default topic manager.\nfunc WithViewTopicManagerBuilder(tmb TopicManagerBuilder) ViewOption {\n\treturn func(o *voptions, table Table, codec Codec) {\n\t\to.builders.topicmgr = tmb\n\t}\n}\n\n// WithViewBackoffBuilder replaced the default backoff.\nfunc WithViewBackoffBuilder(bb BackoffBuilder) ViewOption {\n\treturn func(o *voptions, table Table, codec Codec) {\n\t\to.builders.backoff = bb\n\t}\n}\n\n// WithViewHasher sets the hash function that assigns keys to partitions.\nfunc WithViewHasher(hasher func() hash.Hash32) ViewOption {\n\treturn func(o *voptions, table Table, codec Codec) {\n\t\to.hasher = hasher\n\t}\n}\n\n// WithViewClientID defines the client ID used to identify with Kafka.\nfunc WithViewClientID(clientID string) ViewOption {\n\treturn func(o *voptions, table Table, codec Codec) {\n\t\to.clientID = clientID\n\t}\n}\n\n// WithViewRestartable is kept only for backwards compatibility.\n// DEPRECATED: since the behavior has changed, this name is misleading and should be replaced by\n// WithViewAutoReconnect().\nfunc WithViewRestartable() ViewOption {\n\tlog.Printf(\"Warning: this option is deprecated and will be removed. Replace with WithViewAutoReconnect, which is semantically equivalent\")\n\treturn WithViewAutoReconnect()\n}\n\n// WithViewAutoReconnect defines the view is reconnecting internally, so Run() does not return\n// in case of connection errors. The view must be shutdown by cancelling the context passed to Run()\nfunc WithViewAutoReconnect() ViewOption {\n\treturn func(o *voptions, table Table, codec Codec) {\n\t\to.autoreconnect = true\n\t}\n}\n\n// WithViewBackoffResetTimeout defines the timeout when the backoff\n// will be reset.\nfunc WithViewBackoffResetTimeout(duration time.Duration) ViewOption {\n\treturn func(o *voptions, table Table, codec Codec) {\n\t\to.backoffResetTime = duration\n\t}\n}\n\n// WithViewTester configures all external connections of a processor, ie, storage,\n// consumer and producer\nfunc WithViewTester(t Tester) ViewOption {\n\treturn func(o *voptions, table Table, codec Codec) {\n\t\to.builders.storage = t.StorageBuilder()\n\t\to.builders.topicmgr = t.TopicManagerBuilder()\n\t\to.builders.consumerSarama = t.ConsumerBuilder()\n\t\to.clientID = t.RegisterView(table, codec)\n\t}\n}\n\nfunc (opt *voptions) applyOptions(topic Table, codec Codec, opts ...ViewOption) error {\n\topt.clientID = defaultClientID\n\topt.log = defaultLogger\n\topt.hasher = DefaultHasher()\n\topt.backoffResetTime = defaultBackoffResetTime\n\n\tfor _, o := range opts {\n\t\to(opt, topic, codec)\n\t}\n\n\t// StorageBuilder should always be set as a default option in NewView\n\tif opt.builders.storage == nil {\n\t\treturn fmt.Errorf(\"StorageBuilder not set\")\n\t}\n\n\tif opt.builders.consumerSarama == nil {\n\t\topt.builders.consumerSarama = DefaultSaramaConsumerBuilder\n\t}\n\n\tif opt.builders.topicmgr == nil {\n\t\topt.builders.topicmgr = DefaultTopicManagerBuilder\n\t}\n\n\tif opt.builders.backoff == nil {\n\t\topt.builders.backoff = DefaultBackoffBuilder\n\t}\n\n\treturn nil\n}\n\n///////////////////////////////////////////////////////////////////////////////\n// emitter options\n///////////////////////////////////////////////////////////////////////////////\n\n// EmitterOption defines a configuration option to be used when creating an\n// emitter.\ntype EmitterOption func(*eoptions, Stream, Codec)\n\n// emitter options\ntype eoptions struct {\n\tlog      logger\n\tclientID string\n\n\thasher         func() hash.Hash32\n\tdefaultHeaders Headers\n\n\tbuilders struct {\n\t\ttopicmgr TopicManagerBuilder\n\t\tproducer ProducerBuilder\n\t}\n}\n\n// WithEmitterLogger sets the logger the emitter should use. By default,\n// emitters use the standard library logger.\nfunc WithEmitterLogger(l Logger) EmitterOption {\n\treturn func(o *eoptions, topic Stream, codec Codec) {\n\t\tif prefixLogger, ok := l.(logger); ok {\n\t\t\to.log = prefixLogger\n\t\t} else {\n\t\t\to.log = wrapLogger(l, defaultLogger.debug)\n\t\t}\n\t}\n}\n\n// WithEmitterClientID defines the client ID used to identify with kafka.\nfunc WithEmitterClientID(clientID string) EmitterOption {\n\treturn func(o *eoptions, topic Stream, codec Codec) {\n\t\to.clientID = clientID\n\t}\n}\n\n// WithEmitterTopicManagerBuilder replaces the default topic manager builder.\nfunc WithEmitterTopicManagerBuilder(tmb TopicManagerBuilder) EmitterOption {\n\treturn func(o *eoptions, topic Stream, codec Codec) {\n\t\to.builders.topicmgr = tmb\n\t}\n}\n\n// WithEmitterProducerBuilder replaces the default producer builder.\nfunc WithEmitterProducerBuilder(pb ProducerBuilder) EmitterOption {\n\treturn func(o *eoptions, topic Stream, codec Codec) {\n\t\to.builders.producer = pb\n\t}\n}\n\n// WithEmitterHasher sets the hash function that assigns keys to partitions.\nfunc WithEmitterHasher(hasher func() hash.Hash32) EmitterOption {\n\treturn func(o *eoptions, topic Stream, codec Codec) {\n\t\to.hasher = hasher\n\t}\n}\n\n// WithEmitterTester configures the emitter to use passed tester.\n// This is used for component tests\nfunc WithEmitterTester(t Tester) EmitterOption {\n\treturn func(o *eoptions, topic Stream, codec Codec) {\n\t\to.builders.producer = t.EmitterProducerBuilder()\n\t\to.builders.topicmgr = t.TopicManagerBuilder()\n\t\tt.RegisterEmitter(topic, codec)\n\t}\n}\n\n// WithEmitterDefaultHeaders configures the emitter with default headers\n// which are included with every emit.\nfunc WithEmitterDefaultHeaders(headers Headers) EmitterOption {\n\treturn func(o *eoptions, _ Stream, _ Codec) {\n\t\to.defaultHeaders = headers\n\t}\n}\n\nfunc (opt *eoptions) applyOptions(topic Stream, codec Codec, opts ...EmitterOption) {\n\topt.clientID = defaultClientID\n\topt.log = defaultLogger\n\topt.hasher = DefaultHasher()\n\n\tfor _, o := range opts {\n\t\to(opt, topic, codec)\n\t}\n\n\t// config not set, use default one\n\tif opt.builders.producer == nil {\n\t\topt.builders.producer = DefaultProducerBuilder\n\t}\n\tif opt.builders.topicmgr == nil {\n\t\topt.builders.topicmgr = DefaultTopicManagerBuilder\n\t}\n}\n\ntype ctxOptions struct {\n\temitHeaders Headers\n}\n\n// ContextOption defines a configuration option to be used when performing\n// operations on a context\ntype ContextOption func(*ctxOptions)\n\n// WithCtxEmitHeaders sets kafka headers to use when emitting to kafka\nfunc WithCtxEmitHeaders(headers Headers) ContextOption {\n\treturn func(opts *ctxOptions) {\n\t\t// Accumulate headers rather than discard previous ones.\n\t\topts.emitHeaders = opts.emitHeaders.Merged(headers)\n\t}\n}\n\nfunc (opt *ctxOptions) applyOptions(opts ...ContextOption) {\n\tfor _, o := range opts {\n\t\to(opt)\n\t}\n}\n"
        },
        {
          "name": "options_test.go",
          "type": "blob",
          "size": 0.546875,
          "content": "package goka\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/lovoo/goka/storage\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc nullStorageBuilder() storage.Builder {\n\treturn func(topic string, partition int32) (storage.Storage, error) {\n\t\treturn &storage.Null{}, nil\n\t}\n}\n\nfunc newMockOptions(t *testing.T) *poptions {\n\topts := new(poptions)\n\terr := opts.applyOptions(new(GroupGraph))\n\trequire.Error(t, err)\n\n\topts.builders.storage = nullStorageBuilder()\n\terr = opts.applyOptions(new(GroupGraph))\n\trequire.NoError(t, err)\n\n\tfmt.Printf(\"%+v\\n\", opts)\n\treturn opts\n}\n"
        },
        {
          "name": "partition_processor.go",
          "type": "blob",
          "size": 18.630859375,
          "content": "package goka\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/IBM/sarama\"\n\t\"github.com/hashicorp/go-multierror\"\n\t\"github.com/lovoo/goka/multierr\"\n)\n\nconst (\n\t// PPStateIdle marks the partition processor as idling (not started yet)\n\tPPStateIdle State = iota\n\t// PPStateRecovering indicates a recovering partition processor\n\tPPStateRecovering\n\t// PPStateRunning indicates a running partition processor\n\tPPStateRunning\n\t// PPStateStopping indicates a stopping partition processor\n\tPPStateStopping\n\t// PPStateStopped indicates a stopped partition processor\n\tPPStateStopped\n)\n\n// PPRunMode configures how the partition processor participates as part of the processor\ntype PPRunMode int\n\nconst (\n\t// default mode: the processor recovers once and consumes messages\n\trunModeActive PPRunMode = iota\n\t// the processor keeps recovering. This is used for hot standby.\n\trunModePassive\n\t// the processor only recovers once and then stops. This is used for recover-ahead-option\n\trunModeRecoverOnly\n)\n\ntype visit struct {\n\tkey  string\n\tname string\n\tmeta interface{}\n\tdone func()\n}\n\ntype commitCallback func(msg *message, meta string)\n\ntype ContextWrapper func(ctx Context) Context\n\n// PartitionProcessor handles message processing of one partition by serializing\n// messages from different input topics.\n// It also handles joined tables as well as lookup views (managed by `Processor`).\ntype PartitionProcessor struct {\n\tcallbacks map[string]ProcessCallback\n\n\tlog logger\n\n\ttable   *PartitionTable\n\tjoins   map[string]*PartitionTable\n\tlookups map[string]*View\n\tgraph   *GroupGraph\n\n\tstate *Signal\n\n\tpartition int32\n\n\tinput       chan *message\n\tinputTopics []string\n\n\tvisitInput     chan *visit\n\tvisitCallbacks map[string]ProcessCallback\n\n\trunnerGroup       *multierr.ErrGroup\n\tcancelRunnerGroup func()\n\n\trunMode PPRunMode\n\n\tconsumer sarama.Consumer\n\ttmgr     TopicManager\n\n\tmStats sync.RWMutex\n\tstats  *PartitionProcStats\n\n\tcommit   commitCallback\n\tproducer Producer\n\n\topts *poptions\n}\n\nfunc newPartitionProcessor(partition int32,\n\tgraph *GroupGraph,\n\tcommit commitCallback,\n\tlogger logger,\n\topts *poptions,\n\trunMode PPRunMode,\n\tlookupTables map[string]*View,\n\tconsumer sarama.Consumer,\n\tproducer Producer,\n\ttmgr TopicManager,\n\tbackoff Backoff,\n\tbackoffResetTime time.Duration,\n) *PartitionProcessor {\n\t// collect all topics I am responsible for\n\ttopicMap := make(map[string]bool)\n\tfor _, stream := range graph.InputStreams() {\n\t\ttopicMap[stream.Topic()] = true\n\t}\n\tif loop := graph.LoopStream(); loop != nil {\n\t\ttopicMap[loop.Topic()] = true\n\t}\n\n\tvar (\n\t\ttopicList      []string\n\t\toutputList     []string\n\t\tcallbacks      = make(map[string]ProcessCallback)\n\t\tvisitCallbacks = make(map[string]ProcessCallback)\n\t)\n\tfor t := range topicMap {\n\t\ttopicList = append(topicList, t)\n\t\tcallbacks[t] = graph.callback(t)\n\t}\n\tfor _, output := range graph.OutputStreams() {\n\t\toutputList = append(outputList, output.Topic())\n\t}\n\tif graph.LoopStream() != nil {\n\t\toutputList = append(outputList, graph.LoopStream().Topic())\n\t}\n\n\tif graph.GroupTable() != nil {\n\t\toutputList = append(outputList, graph.GroupTable().Topic())\n\t}\n\n\tlog := logger.Prefix(fmt.Sprintf(\"PartitionProcessor (%d)\", partition))\n\n\tfor _, v := range graph.visitors {\n\t\tvisitCallbacks[v.(*visitor).name] = v.(*visitor).cb\n\t}\n\n\tpartProc := &PartitionProcessor{\n\t\tlog:            log,\n\t\topts:           opts,\n\t\tpartition:      partition,\n\t\tstate:          NewSignal(PPStateIdle, PPStateRecovering, PPStateRunning, PPStateStopping, PPStateStopped).SetState(PPStateIdle),\n\t\tcallbacks:      callbacks,\n\t\tlookups:        lookupTables,\n\t\tconsumer:       consumer,\n\t\tproducer:       producer,\n\t\ttmgr:           tmgr,\n\t\tjoins:          make(map[string]*PartitionTable),\n\t\tinput:          make(chan *message, opts.partitionChannelSize),\n\t\tinputTopics:    topicList,\n\t\tvisitInput:     make(chan *visit, defaultPPVisitChannelSize),\n\t\tvisitCallbacks: visitCallbacks,\n\t\tgraph:          graph,\n\n\t\tstats: newPartitionProcStats(topicList, outputList),\n\n\t\tcommit:  commit,\n\t\trunMode: runMode,\n\t}\n\n\tif graph.GroupTable() != nil {\n\t\tpartProc.table = newPartitionTable(graph.GroupTable().Topic(),\n\t\t\tpartition,\n\t\t\tconsumer,\n\t\t\ttmgr,\n\t\t\topts.updateCallback,\n\t\t\topts.builders.storage,\n\t\t\tlog.Prefix(\"PartTable\"),\n\t\t\tbackoff,\n\t\t\tbackoffResetTime,\n\t\t)\n\t}\n\treturn partProc\n}\n\n// Recovered returns whether the processor is running (i.e. all joins, lookups and the table is recovered and it's consuming messages)\nfunc (pp *PartitionProcessor) Recovered() bool {\n\treturn pp.state.IsState(PPStateRunning)\n}\n\n// Start initializes the partition processor\n// * recover the table\n// * recover all join tables\n// * run the join-tables in catchup mode\n// * start the processor processing loop to receive messages\n// This method takes two contexts, as it does two distinct phases:\n//   - setting up the partition (loading table, joins etc.), after which it returns.\n//     This needs a separate context to allow terminatin the setup phase\n//   - starting the message-processing-loop of the actual processor. This will keep running\n//     after `Start` returns, so it uses the second context.\nfunc (pp *PartitionProcessor) Start(setupCtx, ctx context.Context) error {\n\tif state := pp.state.State(); state != PPStateIdle {\n\t\treturn fmt.Errorf(\"partitionprocessor is not idle (but %v), cannot start\", state)\n\t}\n\n\t// runner context\n\tctx, pp.cancelRunnerGroup = context.WithCancel(ctx)\n\n\tvar runnerCtx context.Context\n\tpp.runnerGroup, runnerCtx = multierr.NewErrGroup(ctx)\n\n\tsetupErrg, setupCtx := multierr.NewErrGroup(setupCtx)\n\n\tpp.state.SetState(PPStateRecovering)\n\tdefer pp.state.SetState(PPStateRunning)\n\n\tif pp.table != nil {\n\t\tsetupErrg.Go(func() error {\n\t\t\tpp.log.Debugf(\"catching up table\")\n\t\t\tdefer pp.log.Debugf(\"catching up table done\")\n\t\t\treturn pp.table.SetupAndRecover(setupCtx, false)\n\t\t})\n\t}\n\n\tfor _, join := range pp.graph.JointTables() {\n\t\ttable := newPartitionTable(join.Topic(),\n\t\t\tpp.partition,\n\t\t\tpp.consumer,\n\t\t\tpp.tmgr,\n\t\t\tpp.opts.updateCallback,\n\t\t\tpp.opts.builders.storage,\n\t\t\tpp.log.Prefix(fmt.Sprintf(\"Join %s\", join.Topic())),\n\t\t\tNewSimpleBackoff(defaultBackoffStep, defaultBackoffMax),\n\t\t\ttime.Minute,\n\t\t)\n\t\tpp.joins[join.Topic()] = table\n\n\t\tsetupErrg.Go(func() error {\n\t\t\treturn table.SetupAndRecover(setupCtx, false)\n\t\t})\n\t}\n\n\t// here we wait for our table and the joins to recover\n\terr := setupErrg.Wait().ErrorOrNil()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Setup failed. Cannot start processor for partition %d: %v\", pp.partition, err)\n\t}\n\n\t// check if one of the contexts might have been closed in the meantime\n\tselect {\n\tcase <-ctx.Done():\n\t\treturn nil\n\tdefault:\n\t}\n\n\t// at this point, we have successfully recovered all joins and the table of the partition-processor.\n\t// If the partition-processor was started to do only that (e.g. for group-recover-ahead), we\n\t// will return here\n\tif pp.runMode == runModeRecoverOnly {\n\t\treturn nil\n\t}\n\n\tfor _, join := range pp.joins {\n\t\tjoin := join\n\t\tpp.runnerGroup.Go(func() error {\n\t\t\tdefer pp.state.SetState(PPStateStopping)\n\t\t\treturn join.CatchupForever(runnerCtx, false)\n\t\t})\n\t}\n\n\t// now run the processor in a runner-group\n\tpp.runnerGroup.Go(func() error {\n\t\tdefer pp.state.SetState(PPStateStopping)\n\n\t\tvar err error\n\t\t// depending on the run mode, we'll do\n\t\tswitch pp.runMode {\n\t\t// (a) start the processor's message run loop so it is ready to receive and process messages\n\t\tcase runModeActive:\n\t\t\terr = pp.run(runnerCtx)\n\t\t\t// (b) run the processor table in catchup mode so it keeps updating it's state.\n\t\tcase runModePassive:\n\t\t\tif pp.table != nil {\n\t\t\t\terr = pp.table.CatchupForever(runnerCtx, false)\n\t\t\t}\n\t\tdefault:\n\t\t\terr = fmt.Errorf(\"processor has invalid run mode\")\n\t\t}\n\t\tif err != nil {\n\t\t\tpp.log.Debugf(\"Run failed with error: %v\", err)\n\t\t}\n\t\treturn err\n\t})\n\treturn nil\n}\n\nfunc (pp *PartitionProcessor) stopping() (<-chan struct{}, func()) {\n\treturn pp.state.WaitForStateMinWithCleanup(PPStateStopping)\n}\n\n// Stop stops the partition processor\nfunc (pp *PartitionProcessor) Stop() error {\n\tpp.log.Debugf(\"Stopping\")\n\tdefer pp.log.Debugf(\"... Stopping done\")\n\tpp.state.SetState(PPStateStopping)\n\tdefer pp.state.SetState(PPStateStopped)\n\n\tclose(pp.input)\n\tclose(pp.visitInput)\n\n\tif pp.cancelRunnerGroup != nil {\n\t\tpp.cancelRunnerGroup()\n\t}\n\n\t// wait for the runner to be done\n\trunningErrs := multierror.Append(pp.runnerGroup.Wait().ErrorOrNil())\n\n\t// close all the tables\n\tstopErrg, _ := multierr.NewErrGroup(context.Background())\n\tfor _, join := range pp.joins {\n\t\tjoin := join\n\t\tstopErrg.Go(func() error {\n\t\t\treturn join.Close()\n\t\t})\n\t}\n\n\t// close processor table, if there is one\n\tif pp.table != nil {\n\t\tstopErrg.Go(func() error {\n\t\t\treturn pp.table.Close()\n\t\t})\n\t}\n\n\t// return stopping errors and running errors\n\treturn multierror.Append(stopErrg.Wait().ErrorOrNil(), runningErrs).ErrorOrNil()\n}\n\nfunc (pp *PartitionProcessor) run(ctx context.Context) (rerr error) {\n\tpp.log.Debugf(\"starting\")\n\tdefer pp.log.Debugf(\"stopped\")\n\n\t// protect the errors-collection\n\tvar mutexErr sync.Mutex\n\n\tdefer func() {\n\t\tmutexErr.Lock()\n\t\tdefer mutexErr.Unlock()\n\t\trerr = multierror.Append(rerr).ErrorOrNil()\n\t}()\n\n\tvar (\n\t\t// syncFailer is called synchronously from the callback within *this*\n\t\t// goroutine\n\t\tsyncFailer = func(err error) {\n\t\t\t// only fail processor if context not already Done\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tmutexErr.Lock()\n\t\t\t\trerr = multierror.Append(rerr,\n\t\t\t\t\tnewErrProcessing(pp.partition, fmt.Errorf(\"synchronous error in callback: %w\", err)),\n\t\t\t\t)\n\t\t\t\tmutexErr.Unlock()\n\t\t\t\treturn\n\t\t\tdefault:\n\t\t\t}\n\t\t\tpanic(err)\n\t\t}\n\n\t\tcloseOnce = new(sync.Once)\n\t\tasyncErrs = make(chan struct{})\n\n\t\t// asyncFailer is called asynchronously from other goroutines, e.g.\n\t\t// when the promise of an Emit (using a producer internally) fails\n\t\tasyncFailer = func(err error) {\n\t\t\tmutexErr.Lock()\n\t\t\trerr = multierror.Append(rerr, newErrProcessing(pp.partition, fmt.Errorf(\"asynchronous error from callback: %w\", err)))\n\t\t\tmutexErr.Unlock()\n\t\t\tcloseOnce.Do(func() { close(asyncErrs) })\n\t\t}\n\n\t\twg sync.WaitGroup\n\t)\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tmutexErr.Lock()\n\t\t\trerr = multierror.Append(rerr,\n\t\t\t\tnewErrProcessing(pp.partition, fmt.Errorf(\"panic in callback: %v\\n%v\", r, strings.Join(userStacktrace(), \"\\n\"))),\n\t\t\t)\n\t\t\tmutexErr.Unlock()\n\t\t\twg.Done()\n\t\t}\n\n\t\tdone := make(chan struct{})\n\t\tgo func() {\n\t\t\tdefer close(done)\n\t\t\twg.Wait()\n\t\t}()\n\n\t\ttimeout := time.NewTimer(60 * time.Second)\n\t\tdefer timeout.Stop()\n\t\tselect {\n\t\tcase <-done:\n\t\tcase <-timeout.C:\n\t\t\tpp.log.Printf(\"partition processor did not shutdown in time. Will stop waiting\")\n\t\t}\n\t}()\n\n\tupdateHwmStatsTicker := time.NewTicker(statsHwmUpdateInterval)\n\tdefer updateHwmStatsTicker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase ev, isOpen := <-pp.input:\n\t\t\t// channel already closed, ev will be nil\n\t\t\tif !isOpen {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\terr := pp.processMessage(ctx, &wg, ev, syncFailer, asyncFailer)\n\t\t\tif err != nil {\n\t\t\t\treturn newErrProcessing(pp.partition, err)\n\t\t\t}\n\n\t\t\tpp.updateStats(func(stats *PartitionProcStats) {\n\t\t\t\tip := stats.Input[ev.topic]\n\t\t\t\tip.Bytes += len(ev.value)\n\t\t\t\tip.LastOffset = ev.offset\n\t\t\t\tif !ev.timestamp.IsZero() {\n\t\t\t\t\tip.Delay = time.Since(ev.timestamp)\n\t\t\t\t}\n\t\t\t\tip.Count++\n\t\t\t})\n\t\tcase <-ctx.Done():\n\t\t\tpp.log.Debugf(\"exiting, context is cancelled\")\n\t\t\treturn\n\t\tcase visit, open := <-pp.visitInput:\n\t\t\tif !open {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\terr := pp.processVisit(ctx, &wg, visit, syncFailer, asyncFailer)\n\t\t\tif err != nil {\n\t\t\t\treturn newErrProcessing(pp.partition, fmt.Errorf(\"Error visiting %s for %s: %v\", visit.name, visit.key, err))\n\t\t\t}\n\t\tcase <-asyncErrs:\n\t\t\tpp.log.Debugf(\"Errors occurred asynchronously. Will exit partition processor\")\n\t\t\treturn\n\t\tcase <-updateHwmStatsTicker.C:\n\t\t\tpp.updateStats(pp.updateHwmStats)\n\t\t}\n\t}\n}\n\nfunc (pp *PartitionProcessor) updateStats(updater func(stats *PartitionProcStats)) {\n\tpp.mStats.Lock()\n\tdefer pp.mStats.Unlock()\n\tupdater(pp.stats)\n}\n\n// updateHwmStats updates the offset lag for all input topics based on the\n// highwatermarks obtained by the consumer.\nfunc (pp *PartitionProcessor) updateHwmStats(stats *PartitionProcStats) {\n\thwms := pp.consumer.HighWaterMarks()\n\tfor input, inputStats := range stats.Input {\n\t\thwm := hwms[input][pp.partition]\n\t\tif hwm != 0 && inputStats.LastOffset != 0 {\n\t\t\tinputStats.OffsetLag = hwm - inputStats.LastOffset\n\t\t}\n\t}\n}\n\nfunc (pp *PartitionProcessor) fetchStats(ctx context.Context) *PartitionProcStats {\n\tpp.mStats.RLock()\n\tstats := pp.stats.clone()\n\tpp.mStats.RUnlock()\n\n\t// mutex for the local stats-clone so the\n\t// error group below doesn't get a concurrent-map-access error\n\tvar m sync.Mutex\n\n\terrg, ctx := multierr.NewErrGroup(ctx)\n\n\t// fetch join table stats\n\tfor topic, join := range pp.joins {\n\t\ttopic, join := topic, join\n\t\terrg.Go(func() error {\n\t\t\tjoinStats := join.fetchStats(ctx)\n\t\t\tif joinStats != nil {\n\t\t\t\tjoinStats.RunMode = pp.runMode\n\t\t\t}\n\t\t\tm.Lock()\n\t\t\tdefer m.Unlock()\n\t\t\tstats.Joined[topic] = joinStats\n\t\t\treturn nil\n\t\t})\n\t}\n\n\t// if we have processor state, get those stats\n\tif pp.table != nil {\n\t\terrg.Go(func() error {\n\t\t\tstats.TableStats = pp.table.fetchStats(ctx)\n\t\t\tif stats.TableStats != nil {\n\t\t\t\tstats.TableStats.RunMode = pp.runMode\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t}\n\n\terr := errg.Wait().ErrorOrNil()\n\tif err != nil {\n\t\tpp.log.Printf(\"Error retrieving stats: %v\", err)\n\t}\n\n\treturn stats\n}\n\nfunc (pp *PartitionProcessor) enqueueTrackOutputStats(ctx context.Context, topic string, size int) {\n\tpp.updateStats(func(stats *PartitionProcStats) {\n\t\tstats.trackOutput(topic, size)\n\t})\n}\n\nfunc (pp *PartitionProcessor) processVisit(ctx context.Context, wg *sync.WaitGroup, v *visit, syncFailer func(err error), asyncFailer func(err error)) (rerr error) {\n\tcb, ok := pp.visitCallbacks[v.name]\n\t// no callback registered for visit\n\tif !ok {\n\t\treturn fmt.Errorf(\"no callback registered for visit named '%s'\", v.name)\n\t}\n\n\tmsgContext := &cbContext{\n\t\tctx:   ctx,\n\t\tgraph: pp.graph,\n\n\t\ttrackOutputStats: pp.enqueueTrackOutputStats,\n\t\tpviews:           pp.joins,\n\t\tviews:            pp.lookups,\n\t\tcommit:           v.done,\n\t\twg:               wg,\n\t\tmsg: &message{\n\t\t\tkey:       v.key,\n\t\t\ttopic:     v.name,\n\t\t\tpartition: pp.partition,\n\t\t\ttimestamp: time.Now(),\n\t\t},\n\t\tsyncFailer:            syncFailer,\n\t\tasyncFailer:           asyncFailer,\n\t\temitter:               pp.producer.EmitWithHeaders,\n\t\temitterDefaultHeaders: pp.opts.producerDefaultHeaders,\n\t\ttable:                 pp.table,\n\t}\n\t// start context and call the ProcessorCallback cb\n\tmsgContext.start()\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\trerr = fmt.Errorf(\"panic in visit: %v\", r)\n\t\t\t// mark the visit done, otherwise the processor gets stuck and the caller of `visit` never returns.\n\t\t\tv.done()\n\n\t\t\t// throw the panic upwards so the caller can handle the wg.Done\n\t\t\tpanic(r)\n\t\t}\n\t}()\n\n\t// now call cb, wrap the context\n\tcb(pp.opts.contextWrapper(msgContext), v.meta)\n\tmsgContext.finish(nil)\n\treturn\n}\n\nfunc (pp *PartitionProcessor) processMessage(ctx context.Context, wg *sync.WaitGroup, msg *message, syncFailer func(err error), asyncFailer func(err error)) error {\n\tmsgContext := &cbContext{\n\t\tctx:   ctx,\n\t\tgraph: pp.graph,\n\n\t\ttrackOutputStats:      pp.enqueueTrackOutputStats,\n\t\tpviews:                pp.joins,\n\t\tviews:                 pp.lookups,\n\t\tcommit:                func() { pp.commit(msg, \"\") },\n\t\twg:                    wg,\n\t\tmsg:                   msg,\n\t\tsyncFailer:            syncFailer,\n\t\tasyncFailer:           asyncFailer,\n\t\temitter:               pp.producer.EmitWithHeaders,\n\t\temitterDefaultHeaders: pp.opts.producerDefaultHeaders,\n\t\ttable:                 pp.table,\n\t}\n\n\tvar (\n\t\tm   interface{}\n\t\terr error\n\t)\n\n\t// decide whether to decode or ignore message\n\tswitch {\n\tcase msg.value == nil && pp.opts.nilHandling == NilIgnore:\n\t\t// mark the message upstream so we don't receive it again.\n\t\t// this is usually only an edge case in unit tests, as kafka probably never sends us nil messages\n\t\tpp.commit(msg, \"\")\n\t\t// otherwise drop it.\n\t\treturn nil\n\tcase msg.value == nil && pp.opts.nilHandling == NilProcess:\n\t\t// process nil messages without decoding them\n\t\tm = nil\n\tdefault:\n\t\t// get stream subcription\n\t\tcodec := pp.graph.codec(msg.topic)\n\t\tif codec == nil {\n\t\t\treturn fmt.Errorf(\"cannot handle topic %s\", msg.topic)\n\t\t}\n\n\t\t// decode message\n\t\tm, err = codec.Decode(msg.value)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"error decoding message for key %s from %s/%d: %v\", msg.key, msg.topic, msg.partition, err)\n\t\t}\n\t}\n\n\tcb := pp.callbacks[msg.topic]\n\tif cb == nil {\n\t\treturn fmt.Errorf(\"error processing message for key %s from %s/%d: %v\", msg.key, msg.topic, msg.partition, err)\n\t}\n\n\t// start context and call the ProcessorCallback cb\n\tmsgContext.start()\n\n\t// now call cb\n\tcb(pp.opts.contextWrapper(msgContext), m)\n\tmsgContext.finish(nil)\n\treturn nil\n}\n\n// VisitValues iterates over all values in the table and calls the \"visit\"-callback for the passed name.\n// Optional parameter value can be set, which will just be forwarded to the visitor-function\n// the function returns after all items of the table have been added to the channel.\nfunc (pp *PartitionProcessor) VisitValues(ctx context.Context, name string, meta interface{}, visited *int64) error {\n\tif _, ok := pp.visitCallbacks[name]; !ok {\n\t\treturn fmt.Errorf(\"unconfigured visit callback. Did you initialize the processor with DefineGroup(..., Visit(%s, ...), ...)?\", name)\n\t}\n\n\tit, err := pp.table.Iterator()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error creating storage iterator\")\n\t}\n\n\tvar wg sync.WaitGroup\n\n\t// drains the channel and drops out when closed.\n\t// This is done when the processor shuts down during visit\n\t// and makes sure the waitgroup is fully counted down.\n\tdrainUntilClose := func() {\n\t\tfor range pp.visitInput {\n\t\t\twg.Done()\n\t\t}\n\t}\n\n\t// drains the input channel until there are no more items.\n\t// does not wait for close, because the channel stays open for the next visit\n\tdrainUntilEmpty := func() {\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase _, ok := <-pp.visitInput:\n\t\t\t\tif !ok {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\twg.Done()\n\t\t\tdefault:\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\tdefer it.Release()\n\n\tstopping, doneWaitingForStop := pp.stopping()\n\tdefer doneWaitingForStop()\n\n\tfor it.Next() {\n\t\t// add one that we were able to be put into the queue.\n\t\t// wg.Done will be called by the visit handler as commit\n\t\twg.Add(1)\n\t\tselect {\n\t\tcase <-stopping:\n\t\t\tdrainUntilClose()\n\t\t\twg.Done()\n\t\t\treturn ErrVisitAborted\n\t\tcase <-ctx.Done():\n\t\t\tdrainUntilEmpty()\n\t\t\twg.Done()\n\t\t\treturn ctx.Err()\n\t\t// enqueue the visit\n\t\tcase pp.visitInput <- &visit{\n\t\t\tkey:  string(it.Key()),\n\t\t\tname: name,\n\t\t\tmeta: meta,\n\t\t\tdone: func() {\n\t\t\t\tatomic.AddInt64(visited, 1)\n\t\t\t\twg.Done()\n\t\t\t},\n\t\t}:\n\t\t}\n\t}\n\n\t// wait for all visits. We have to wrap it into an extra goroutine using a closing\n\t// channel, since on an explicit stop, the waitgroup will never finish, so we would wait\n\t// forever here.\n\twgDone := make(chan struct{})\n\tgo func() {\n\t\tdefer close(wgDone)\n\t\twg.Wait()\n\t}()\n\tselect {\n\tcase <-stopping:\n\t\tdrainUntilClose()\n\t\treturn ErrVisitAborted\n\tcase <-ctx.Done():\n\t\tdrainUntilEmpty()\n\t\treturn ctx.Err()\n\tcase <-wgDone:\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "partition_table.go",
          "type": "blob",
          "size": 16.9462890625,
          "content": "package goka\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/IBM/sarama\"\n\t\"github.com/hashicorp/go-multierror\"\n\t\"github.com/lovoo/goka/multierr\"\n\t\"github.com/lovoo/goka/storage\"\n)\n\nconst (\n\tdefaultPartitionChannelSize = 10\n\tdefaultStallPeriod          = 30 * time.Second\n\tdefaultStalledTimeout       = 2 * time.Minute\n\n\t// internal offset we use to detect if the offset has never been stored locally\n\toffsetNotStored int64 = -3\n)\n\n// Backoff is used for adding backoff capabilities to the restarting\n// of failing partition tables.\ntype Backoff interface {\n\tDuration() time.Duration\n\tReset()\n}\n\n// PartitionTable manages the usage of a table for one partition.\n// It allows to setup and recover/catchup the table contents from kafka,\n// allow updates via Get/Set/Delete accessors\ntype PartitionTable struct {\n\tlog            logger\n\ttopic          string\n\tpartition      int32\n\tstate          *Signal\n\tbuilder        storage.Builder\n\tst             *storageProxy\n\tconsumer       sarama.Consumer\n\ttmgr           TopicManager\n\tupdateCallback UpdateCallback\n\n\tmStats sync.RWMutex\n\tstats  *TableStats\n\n\t// stall config\n\tstallPeriod    time.Duration\n\tstalledTimeout time.Duration\n\n\tbackoff             Backoff\n\tbackoffResetTimeout time.Duration\n}\n\nfunc newPartitionTableState() *Signal {\n\treturn NewSignal(\n\t\tState(PartitionStopped),\n\t\tState(PartitionInitializing),\n\t\tState(PartitionConnecting),\n\t\tState(PartitionRecovering),\n\t\tState(PartitionPreparing),\n\t\tState(PartitionRunning),\n\t).SetState(State(PartitionStopped))\n}\n\nfunc newPartitionTable(topic string,\n\tpartition int32,\n\tconsumer sarama.Consumer,\n\ttmgr TopicManager,\n\tupdateCallback UpdateCallback,\n\tbuilder storage.Builder,\n\tlog logger,\n\tbackoff Backoff,\n\tbackoffResetTimeout time.Duration,\n) *PartitionTable {\n\tpt := &PartitionTable{\n\t\tpartition:      partition,\n\t\tstate:          newPartitionTableState(),\n\t\tconsumer:       consumer,\n\t\ttmgr:           tmgr,\n\t\ttopic:          topic,\n\t\tupdateCallback: updateCallback,\n\t\tbuilder:        builder,\n\t\tlog:            log,\n\t\tstallPeriod:    defaultStallPeriod,\n\t\tstalledTimeout: defaultStalledTimeout,\n\n\t\tstats: newTableStats(),\n\n\t\tbackoff:             backoff,\n\t\tbackoffResetTimeout: backoffResetTimeout,\n\t}\n\n\treturn pt\n}\n\n// SetupAndRecover  sets up the partition storage and recovers to HWM\nfunc (p *PartitionTable) SetupAndRecover(ctx context.Context, restartOnError bool) error {\n\terr := p.setup(ctx)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// do not continue if the context is already cancelled.\n\t// this can happen if the context was closed during opening the storage.\n\t// Since this is no error we have to check it here, otherwise it'll nil-panic later.\n\tselect {\n\tcase <-ctx.Done():\n\t\treturn nil\n\tdefault:\n\t}\n\n\tif restartOnError {\n\t\treturn p.loadRestarting(ctx, true)\n\t}\n\treturn p.load(ctx, true)\n}\n\n// CatchupForever starts catching the partition table forever (until the context is cancelled).\n// Option restartOnError allows the view to stay open/intact even in case of consumer errors\nfunc (p *PartitionTable) CatchupForever(ctx context.Context, restartOnError bool) error {\n\tif restartOnError {\n\t\treturn p.loadRestarting(ctx, false)\n\t}\n\treturn p.load(ctx, false)\n}\n\nfunc (p *PartitionTable) loadRestarting(ctx context.Context, stopAfterCatchup bool) error {\n\tvar (\n\t\tresetTimer *time.Timer\n\t\tretries    int\n\t\tretryMux   sync.Mutex\n\t)\n\n\tfor {\n\t\terr := p.load(ctx, stopAfterCatchup)\n\t\tif err != nil {\n\t\t\tp.log.Printf(\"Error while starting up: %v\", err)\n\n\t\t\tretryMux.Lock()\n\t\t\tretries++\n\t\t\tretryMux.Unlock()\n\t\t\tif resetTimer != nil {\n\t\t\t\tresetTimer.Stop()\n\t\t\t}\n\t\t\tresetTimer = time.AfterFunc(p.backoffResetTimeout, func() {\n\t\t\t\tp.backoff.Reset()\n\t\t\t\tretryMux.Lock()\n\t\t\t\tretries = 0\n\t\t\t\tretryMux.Unlock()\n\t\t\t})\n\t\t} else {\n\t\t\treturn nil\n\t\t}\n\n\t\tretryDuration := p.backoff.Duration()\n\t\tretryMux.Lock()\n\t\tp.log.Printf(\"Will retry in %.0f seconds (retried %d times so far)\", retryDuration.Seconds(), retries)\n\t\tretryMux.Unlock()\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn nil\n\n\t\tcase <-time.After(retryDuration):\n\t\t}\n\t}\n}\n\n// Setup creates the storage for the partition table\nfunc (p *PartitionTable) setup(ctx context.Context) error {\n\tp.state.SetState(State(PartitionInitializing))\n\tstorage, err := p.createStorage(ctx)\n\tif err != nil {\n\t\tp.state.SetState(State(PartitionStopped))\n\t\treturn fmt.Errorf(\"error setting up partition table: %v\", err)\n\t}\n\n\tp.st = storage\n\treturn nil\n}\n\n// Close closes the partition table\nfunc (p *PartitionTable) Close() error {\n\tif p.st != nil {\n\t\treturn p.st.Close()\n\t}\n\treturn nil\n}\n\nfunc (p *PartitionTable) createStorage(ctx context.Context) (*storageProxy, error) {\n\tvar (\n\t\terr   error\n\t\tst    storage.Storage\n\t\tstart = time.Now()\n\t\tdone  = make(chan struct{})\n\t)\n\tdefer close(done)\n\n\tgo func() {\n\t\tticker := time.NewTicker(5 * time.Minute)\n\t\tdefer ticker.Stop()\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-done:\n\t\t\t\treturn\n\t\t\tcase <-ticker.C:\n\t\t\t\tp.log.Printf(\"creating storage for topic %s/%d for %.1f minutes ...\", p.topic, p.partition, time.Since(start).Minutes())\n\t\t\t}\n\t\t}\n\t}()\n\n\tst, err = p.builder(p.topic, p.partition)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error building storage: %v\", err)\n\t}\n\terr = st.Open()\n\tif err != nil {\n\t\treturn nil, multierror.Append(st.Close(), fmt.Errorf(\"error opening storage: %v\", err)).ErrorOrNil()\n\t}\n\n\t// close the db if context was cancelled before the builder returned\n\tselect {\n\tcase <-ctx.Done():\n\t\terr = st.Close()\n\t\t// only collect context error if Close() errored out\n\t\tif err != nil {\n\t\t\treturn nil, multierror.Append(err, ctx.Err()).ErrorOrNil()\n\t\t}\n\t\treturn nil, nil\n\tdefault:\n\t}\n\n\tp.log.Debugf(\"finished building storage for topic %s/%d in %.1f minutes\", p.topic, p.partition, time.Since(start).Minutes())\n\treturn &storageProxy{\n\t\tStorage:   st,\n\t\ttopic:     Stream(p.topic),\n\t\tpartition: p.partition,\n\t\tupdate:    p.updateCallback,\n\t}, nil\n}\n\n// findOffsetToLoad returns the first offset to load and the high watermark.\nfunc (p *PartitionTable) findOffsetToLoad(storedOffset int64) (int64, int64, error) {\n\toldest, err := p.tmgr.GetOffset(p.topic, p.partition, sarama.OffsetOldest)\n\tif err != nil {\n\t\treturn 0, 0, fmt.Errorf(\"Error getting oldest offset for topic/partition %s/%d: %v\", p.topic, p.partition, err)\n\t}\n\thwm, err := p.tmgr.GetOffset(p.topic, p.partition, sarama.OffsetNewest)\n\tif err != nil {\n\t\treturn 0, 0, fmt.Errorf(\"Error getting newest offset for topic/partition %s/%d: %v\", p.topic, p.partition, err)\n\t}\n\tp.log.Debugf(\"topic manager gives us oldest: %d, hwm: %d\", oldest, hwm)\n\n\tvar start int64\n\n\t// if no offset is found in the local storage start with the oldest offset known\n\t// to kafka.\n\t// Otherwise start with the next element not stored locally.\n\tif storedOffset == offsetNotStored {\n\t\tstart = oldest\n\t} else {\n\t\tstart = storedOffset + 1\n\t}\n\n\t// if kafka does not have the offset we're looking for, use the oldest kafka has\n\t// This can happen when the log compaction removes offsets that we stored.\n\tif start < oldest {\n\t\tstart = oldest\n\t}\n\treturn start, hwm, nil\n}\n\nfunc (p *PartitionTable) load(ctx context.Context, stopAfterCatchup bool) (rerr error) {\n\tvar (\n\t\tstoredOffset int64\n\t\tpartConsumer sarama.PartitionConsumer\n\t\terr          error\n\t)\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\n\tp.state.SetState(State(PartitionConnecting))\n\n\t// fetch local offset\n\tstoredOffset, err = p.st.GetOffset(offsetNotStored)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error reading local offset: %v\", err)\n\t}\n\n\tloadOffset, hwm, err := p.findOffsetToLoad(storedOffset)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif storedOffset > 0 && hwm == 0 {\n\t\treturn fmt.Errorf(\"kafka tells us there's no message in the topic, but our cache has one. The table might be gone. Try to delete your local cache! Topic %s, partition %d, hwm %d, local offset %d\", p.topic, p.partition, hwm, storedOffset)\n\t}\n\n\tif storedOffset >= hwm {\n\t\tp.log.Printf(\"Error: local offset is higher than partition offset. topic %s, partition %d, hwm %d, local offset %d. This can have several reasons: \\n(1) The kafka topic storing the table is gone --> delete the local cache and restart! \\n(2) the processor crashed last time while writing to disk. \\n(3) You found a bug!\", p.topic, p.partition, hwm, storedOffset)\n\n\t\t// we'll just pretend we were done so the partition looks recovered\n\t\tloadOffset = hwm\n\t}\n\n\t// initialize recovery stats here, in case we don't do the recovery because\n\t// we're up to date already\n\tif stopAfterCatchup {\n\t\tp.updateStats(func(stats *TableStats) {\n\t\t\tstats.Recovery.StartTime = time.Now()\n\t\t\tstats.Recovery.Hwm = hwm\n\t\t\tstats.Recovery.Offset = loadOffset\n\t\t})\n\t}\n\n\t// we are exactly where we're supposed to be\n\t// AND we're here for catchup, so let's stop here\n\t// and do not attempt to load anything\n\tif stopAfterCatchup && loadOffset >= hwm {\n\t\treturn p.markRecovered(ctx)\n\t}\n\n\tif stopAfterCatchup {\n\t\tp.log.Debugf(\"Recovering from %d to hwm=%d; (local offset is %d)\", loadOffset, hwm, storedOffset)\n\t} else {\n\t\tp.log.Debugf(\"Catching up from %d to hwm=%d; (local offset is %d)\", loadOffset, hwm, storedOffset)\n\t}\n\n\tdefer p.log.Debugf(\"... Loading done\")\n\n\tpartConsumer, err = p.consumer.ConsumePartition(p.topic, p.partition, loadOffset)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Error creating partition consumer for topic %s, partition %d, offset %d: %v\", p.topic, p.partition, storedOffset, err)\n\t}\n\n\t// close the consumer\n\tdefer func() {\n\t\tpartConsumer.AsyncClose()\n\t\trerr = multierror.Append(rerr, p.drainConsumer(partConsumer)).ErrorOrNil()\n\t}()\n\n\tif stopAfterCatchup {\n\t\tp.state.SetState(State(PartitionRecovering))\n\t} else {\n\t\tp.state.SetState(State(PartitionRunning))\n\t}\n\n\t// load messages and stop when you're at HWM\n\tloadErr := p.loadMessages(ctx, partConsumer, hwm, stopAfterCatchup)\n\n\tif loadErr != nil {\n\t\treturn loadErr\n\t}\n\n\tif stopAfterCatchup {\n\t\terr := p.markRecovered(ctx)\n\t\tp.updateStats(func(stats *TableStats) { stats.Recovery.RecoveryTime = time.Now() })\n\t\treturn err\n\t}\n\treturn\n}\n\nfunc (p *PartitionTable) observeStateChanges() *StateChangeObserver {\n\treturn p.state.ObserveStateChange()\n}\n\nfunc (p *PartitionTable) markRecovered(ctx context.Context) error {\n\tvar (\n\t\tstart  = time.Now()\n\t\tticker = time.NewTicker(10 * time.Second)\n\t\tdone   = make(chan error, 1)\n\t)\n\tdefer ticker.Stop()\n\n\tp.state.SetState(State(PartitionPreparing))\n\tnow := time.Now()\n\tp.updateStats(func(stats *TableStats) { stats.Recovery.RecoveryTime = now })\n\n\tgo func() {\n\t\tdefer close(done)\n\t\terr := p.st.MarkRecovered()\n\t\tif err != nil {\n\t\t\tdone <- err\n\t\t}\n\t}()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ticker.C:\n\t\t\tp.log.Printf(\"Committing storage after recovery for topic/partition %s/%d since %0.f seconds\", p.topic, p.partition, time.Since(start).Seconds())\n\t\tcase <-ctx.Done():\n\t\t\treturn nil\n\t\tcase err := <-done:\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tp.state.SetState(State(PartitionRunning))\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\nfunc (p *PartitionTable) drainConsumer(cons sarama.PartitionConsumer) error {\n\terrg, _ := multierr.NewErrGroup(context.Background())\n\n\t// drain errors channel\n\terrg.Go(func() error {\n\t\tvar errs *multierror.Error\n\t\tfor err := range cons.Errors() {\n\t\t\terrs = multierror.Append(errs, err)\n\t\t}\n\t\treturn errs\n\t})\n\n\t// drain message channel\n\terrg.Go(func() error {\n\t\tfor range cons.Messages() {\n\t\t}\n\t\treturn nil\n\t})\n\n\treturn errg.Wait().ErrorOrNil()\n}\n\nfunc (p *PartitionTable) loadMessages(ctx context.Context, cons sarama.PartitionConsumer, partitionHwm int64, stopAfterCatchup bool) error {\n\tstallTicker := time.NewTicker(p.stallPeriod)\n\tdefer stallTicker.Stop()\n\n\tlastMessage := time.Now()\n\n\tmessages := cons.Messages()\n\terrors := cons.Errors()\n\n\tupdateHwmStatsTicker := time.NewTicker(statsHwmUpdateInterval)\n\tdefer updateHwmStatsTicker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase err, ok := <-errors:\n\t\t\tif !ok {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\tcase msg, ok := <-messages:\n\t\t\tif !ok {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\t// This case is for the Tester to achieve synchronity.\n\t\t\t// Nil messages are never generated by the Sarama Consumer\n\t\t\tif msg == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif p.state.IsState(State(PartitionRunning)) && stopAfterCatchup {\n\t\t\t\t// TODO: should we really ignore the message?\n\t\t\t\t// Shouldn't we instead break here to avoid losing messages or fail or just consume it?\n\t\t\t\tp.log.Printf(\"received message in topic %s, partition %s after catchup. Another processor is still producing messages. Ignoring message.\", p.topic, p.partition)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tlastMessage = time.Now()\n\t\t\tif err := p.storeEvent(string(msg.Key), msg.Value, msg.Offset, msg.Headers, msg.Timestamp); err != nil {\n\t\t\t\treturn fmt.Errorf(\"load: error updating storage: %v\", err)\n\t\t\t}\n\n\t\t\tif stopAfterCatchup {\n\t\t\t\tp.updateStats(func(stats *TableStats) { stats.Recovery.Offset = msg.Offset })\n\t\t\t}\n\n\t\t\tp.updateStats(func(stats *TableStats) {\n\t\t\t\tip := stats.Input\n\t\t\t\tip.Bytes += len(msg.Value)\n\t\t\t\tip.LastOffset = msg.Offset\n\t\t\t\tif !msg.Timestamp.IsZero() {\n\t\t\t\t\tip.Delay = time.Since(msg.Timestamp)\n\t\t\t\t}\n\t\t\t\tip.Count++\n\t\t\t\tstats.Stalled = false\n\t\t\t})\n\n\t\t\tif stopAfterCatchup && msg.Offset >= partitionHwm-1 {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\tcase now := <-stallTicker.C:\n\t\t\t// only set to stalled, if the last message was earlier\n\t\t\t// than the stalled timeout\n\t\t\tif now.Sub(lastMessage) > p.stalledTimeout {\n\t\t\t\tp.updateStats(func(stats *TableStats) { stats.Stalled = true })\n\t\t\t}\n\t\tcase <-updateHwmStatsTicker.C:\n\t\t\tp.updateHwmStats()\n\n\t\tcase <-ctx.Done():\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\nfunc (p *PartitionTable) updateStats(updater func(stats *TableStats)) {\n\tp.mStats.Lock()\n\tdefer p.mStats.Unlock()\n\tupdater(p.stats)\n}\n\nfunc (p *PartitionTable) fetchStats(ctx context.Context) *TableStats {\n\tp.mStats.RLock()\n\tdefer p.mStats.RUnlock()\n\n\tstats := p.stats.clone()\n\tstats.Status = PartitionStatus(p.state.State())\n\n\treturn stats\n}\n\nfunc (p *PartitionTable) updateHwmStats() {\n\thwms := p.consumer.HighWaterMarks()\n\thwm := hwms[p.topic][p.partition]\n\tif hwm != 0 {\n\t\tp.updateStats(func(stats *TableStats) {\n\t\t\tstats.Input.OffsetLag = hwm - stats.Input.LastOffset\n\t\t})\n\t}\n}\n\n// TrackMessageWrite updates the write stats to passed length\nfunc (p *PartitionTable) TrackMessageWrite(ctx context.Context, length int) {\n\tp.updateStats(func(stats *TableStats) {\n\t\tstats.Writes.Bytes += length\n\t\tstats.Writes.Count++\n\t})\n}\n\nfunc (p *PartitionTable) storeEvent(key string, value []byte, offset int64, headers []*sarama.RecordHeader, timestamp time.Time) error {\n\terr := p.st.Update(&DefaultUpdateContext{\n\t\ttopic:     p.st.topic,\n\t\tpartition: p.st.partition,\n\t\toffset:    offset,\n\t\theaders:   headers,\n\t\ttimestamp: timestamp,\n\t}, key, value)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Error from the update callback while recovering from the log: %v\", err)\n\t}\n\terr = p.st.SetOffset(offset)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Error updating offset in local storage while recovering from the log: %v\", err)\n\t}\n\treturn nil\n}\n\n// IsRecovered returns whether the partition table is recovered\nfunc (p *PartitionTable) IsRecovered() bool {\n\treturn p.state.IsState(State(PartitionRunning))\n}\n\n// CurrentState returns the partition's current status\nfunc (p *PartitionTable) CurrentState() PartitionStatus {\n\treturn PartitionStatus(p.state.State())\n}\n\n// WaitRecovered returns a channel that closes when the partition table enters state `PartitionRunning`\nfunc (p *PartitionTable) WaitRecovered() <-chan struct{} {\n\treturn p.state.WaitForState(State(PartitionRunning))\n}\n\n// Get returns the value for passed key\nfunc (p *PartitionTable) Get(key string) ([]byte, error) {\n\tif err := p.readyToRead(); err != nil {\n\t\treturn nil, err\n\t}\n\treturn p.st.Get(key)\n}\n\n// Has returns whether the storage contains passed key\nfunc (p *PartitionTable) Has(key string) (bool, error) {\n\tif err := p.readyToRead(); err != nil {\n\t\treturn false, err\n\t}\n\treturn p.st.Has(key)\n}\n\n// Iterator returns an iterator on the table's storage.\n// If the partition_table is not in a running state, it will return an error to prevent serving\n// incomplete data\nfunc (p *PartitionTable) Iterator() (storage.Iterator, error) {\n\tif err := p.readyToRead(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn p.st.Iterator()\n}\n\n// IteratorWithRange returns an iterator on the table's storage for passed range.\n// If the partition_table is not in a running state, it will return an error to prevent serving\n// incomplete data\nfunc (p *PartitionTable) IteratorWithRange(start []byte, limit []byte) (storage.Iterator, error) {\n\tif err := p.readyToRead(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn p.st.IteratorWithRange(start, limit)\n}\n\nfunc (p *PartitionTable) readyToRead() error {\n\tpstate := p.CurrentState()\n\tif pstate < PartitionConnecting {\n\t\treturn fmt.Errorf(\"Partition is not running (but %v) so it's not safe to read values\", pstate)\n\t}\n\treturn nil\n}\n\n// Set sets a key value key in the partition table by modifying the underlying storage\nfunc (p *PartitionTable) Set(key string, value []byte) error {\n\treturn p.st.Set(key, value)\n}\n\n// Delete removes the passed key from the partition table by deleting from the underlying storage\nfunc (p *PartitionTable) Delete(key string) error {\n\treturn p.st.Delete(key)\n}\n\n// SetOffset sets the magic offset value in storage\nfunc (p *PartitionTable) SetOffset(value int64) error {\n\treturn p.st.SetOffset(value)\n}\n\n// GetOffset returns the magic offset value from storage\nfunc (p *PartitionTable) GetOffset(defValue int64) (int64, error) {\n\treturn p.st.GetOffset(defValue)\n}\n"
        },
        {
          "name": "partition_table_test.go",
          "type": "blob",
          "size": 25.6484375,
          "content": "package goka\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/IBM/sarama\"\n\t\"github.com/golang/mock/gomock\"\n\t\"github.com/lovoo/goka/storage\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc defaultPT(\n\tt *testing.T,\n\ttopic string,\n\tpartition int32,\n\tconsumer sarama.Consumer,\n\tupdateCallback UpdateCallback,\n) (*PartitionTable, *builderMock, *gomock.Controller) {\n\tctrl := gomock.NewController(t)\n\tbm := newBuilderMock(ctrl)\n\treturn newPartitionTable(\n\t\ttopic,\n\t\tpartition,\n\t\tconsumer,\n\t\tbm.tmgr,\n\t\tupdateCallback,\n\t\tbm.getStorageBuilder(),\n\t\tdefaultLogger,\n\t\tNewSimpleBackoff(defaultBackoffStep, defaultBackoffMax),\n\t\ttime.Minute,\n\t), bm, ctrl\n}\n\nfunc updateCallbackNoop(ctx UpdateContext, s storage.Storage, key string, value []byte) error {\n\treturn nil\n}\n\nfunc TestPT_createStorage(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tvar (\n\t\t\tpartition int32          = 101\n\t\t\tcallback  UpdateCallback = updateCallbackNoop\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\t\"some-topic\",\n\t\t\tpartition,\n\t\t\tnil,\n\t\t\tcallback,\n\t\t)\n\t\tdefer ctrl.Finish()\n\n\t\tequalSP := &storageProxy{\n\t\t\tStorage:   bm.mst,\n\t\t\tpartition: partition,\n\t\t\tupdate:    callback,\n\t\t}\n\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\n\t\tbm.mst.EXPECT().Open().Return(nil)\n\t\tsp, err := pt.createStorage(ctx)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, sp.Storage, equalSP.Storage)\n\t\trequire.Equal(t, sp.partition, equalSP.partition)\n\t\t// doing manual pointer equality test here, require.Same does not work for some reason\n\t\trequire.Equal(t, reflect.ValueOf(equalSP.Update).Pointer(), reflect.ValueOf(sp.Update).Pointer())\n\t})\n\tt.Run(\"fail_ctx_cancel\", func(t *testing.T) {\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\t\"some-topic\",\n\t\t\t0,\n\t\t\tnil,\n\t\t\tnil,\n\t\t)\n\t\tdefer ctrl.Finish()\n\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tcancel()\n\n\t\tbm.mst.EXPECT().Open().Return(nil)\n\t\tbm.mst.EXPECT().Close().Return(nil)\n\n\t\tsp, err := pt.createStorage(ctx)\n\t\trequire.NoError(t, err)\n\t\trequire.Nil(t, sp)\n\t})\n\tt.Run(\"fail_storage\", func(t *testing.T) {\n\t\tpt, _, ctrl := defaultPT(\n\t\t\tt,\n\t\t\t\"some-topic\",\n\t\t\t0,\n\t\t\tnil,\n\t\t\tnil,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tpt.builder = errStorageBuilder()\n\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second*1)\n\t\tdefer cancel()\n\t\tsp, err := pt.createStorage(ctx)\n\t\trequire.Error(t, err)\n\t\trequire.Nil(t, sp)\n\t})\n}\n\nfunc TestPT_setup(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\t\"some-topic\",\n\t\t\t0,\n\t\t\tnil,\n\t\t\tnil,\n\t\t)\n\t\tdefer ctrl.Finish()\n\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\t\tbm.mst.EXPECT().Open().Return(nil)\n\t\terr := pt.setup(ctx)\n\t\trequire.NoError(t, err)\n\t})\n\tt.Run(\"fail\", func(t *testing.T) {\n\t\tpt, _, ctrl := defaultPT(\n\t\t\tt,\n\t\t\t\"some-topic\",\n\t\t\t0,\n\t\t\tnil,\n\t\t\tnil,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tpt.builder = errStorageBuilder()\n\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second*1)\n\t\tdefer cancel()\n\t\terr := pt.setup(ctx)\n\t\trequire.Error(t, err)\n\t})\n}\n\nfunc TestPT_close(t *testing.T) {\n\tt.Run(\"on_storage\", func(t *testing.T) {\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\t\"some-topic\",\n\t\t\t0,\n\t\t\tnil,\n\t\t\tnil,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tbm.mst.EXPECT().Close().AnyTimes()\n\t\tbm.mst.EXPECT().Open().Return(nil)\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\t\terr := pt.setup(ctx)\n\t\trequire.NoError(t, err)\n\t\terr = pt.Close()\n\t\trequire.NoError(t, err)\n\t})\n\tt.Run(\"on_nil_storage\", func(t *testing.T) {\n\t\tpt, _, ctrl := defaultPT(\n\t\t\tt,\n\t\t\t\"some-topic\",\n\t\t\t0,\n\t\t\tnil,\n\t\t\tnil,\n\t\t)\n\t\tdefer ctrl.Finish()\n\n\t\terr := pt.Close()\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc TestPT_findOffsetToLoad(t *testing.T) {\n\tt.Run(\"old_local\", func(t *testing.T) {\n\t\tvar (\n\t\t\toldest int64 = 161\n\t\t\tnewest int64 = 1312\n\t\t\tlocal  int64 = 15\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\t\"some-topic\",\n\t\t\t0,\n\t\t\tnil,\n\t\t\tnil,\n\t\t)\n\t\tdefer ctrl.Finish()\n\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetOldest).Return(oldest, nil)\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetNewest).Return(newest, nil)\n\n\t\tactualOldest, actualNewest, err := pt.findOffsetToLoad(local)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, actualOldest, oldest)\n\t\trequire.Equal(t, actualNewest, newest)\n\t})\n\tt.Run(\"new_local\", func(t *testing.T) {\n\t\tvar (\n\t\t\toldest int64 = 161\n\t\t\tnewest int64 = 1312\n\t\t\tlocal  int64 = 175\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\t\"some-topic\",\n\t\t\t0,\n\t\t\tnil,\n\t\t\tnil,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetOldest).Return(oldest, nil)\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetNewest).Return(newest, nil)\n\n\t\toffsetToLoad, actualNewest, err := pt.findOffsetToLoad(local)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, offsetToLoad, local+1)\n\t\trequire.Equal(t, actualNewest, newest)\n\t})\n\tt.Run(\"too_new_local\", func(t *testing.T) {\n\t\tvar (\n\t\t\toldest int64 = 161\n\t\t\tnewest int64 = 1312\n\t\t\tlocal  int64 = 161111\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\t\"some-topic\",\n\t\t\t0,\n\t\t\tnil,\n\t\t\tnil,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetOldest).Return(oldest, nil)\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetNewest).Return(newest, nil)\n\n\t\toffsetToLoad, actualNewest, err := pt.findOffsetToLoad(local)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, offsetToLoad, local+1)\n\t\trequire.Equal(t, actualNewest, newest)\n\t})\n\tt.Run(\"sarama_oldest\", func(t *testing.T) {\n\t\tvar (\n\t\t\toldest int64 = 161\n\t\t\tnewest int64 = 1312\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\t\"some-topic\",\n\t\t\t0,\n\t\t\tnil,\n\t\t\tnil,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetOldest).Return(oldest, nil)\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetNewest).Return(newest, nil)\n\n\t\tactualOldest, actualNewest, err := pt.findOffsetToLoad(sarama.OffsetOldest)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, actualOldest, oldest)\n\t\trequire.Equal(t, actualNewest, newest)\n\t})\n\tt.Run(\"fail_getoffset\", func(t *testing.T) {\n\t\tvar expectedErr error = fmt.Errorf(\"some error\")\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\t\"some-topic\",\n\t\t\t0,\n\t\t\tnil,\n\t\t\tnil,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetOldest).Return(int64(0), expectedErr)\n\n\t\t_, _, err := pt.findOffsetToLoad(sarama.OffsetOldest)\n\t\trequire.Error(t, err)\n\t})\n\tt.Run(\"fail_getoffset2\", func(t *testing.T) {\n\t\tvar (\n\t\t\toldest      int64 = 161\n\t\t\texpectedErr error = fmt.Errorf(\"some error\")\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\t\"some-topic\",\n\t\t\t0,\n\t\t\tnil,\n\t\t\tnil,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetOldest).Return(oldest, nil)\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetNewest).Return(int64(0), expectedErr)\n\n\t\t_, _, err := pt.findOffsetToLoad(sarama.OffsetOldest)\n\t\trequire.Error(t, err)\n\t})\n}\n\nfunc TestPT_load(t *testing.T) {\n\tt.Run(\"succeed_no_load_stopAfterCatchup\", func(t *testing.T) {\n\t\tvar (\n\t\t\toldest           int64 = 161\n\t\t\tnewest           int64 = 1312\n\t\t\tlocal            int64 = 1311\n\t\t\tstopAfterCatchup       = true\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\t\"some-topic\",\n\t\t\t0,\n\t\t\tnil,\n\t\t\tnil,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tbm.mst.EXPECT().Open().Return(nil)\n\t\tbm.mst.EXPECT().GetOffset(offsetNotStored).Return(local, nil)\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetOldest).Return(oldest, nil)\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetNewest).Return(newest, nil)\n\t\tbm.mst.EXPECT().MarkRecovered().Return(nil)\n\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\t\terr := pt.setup(ctx)\n\t\trequire.NoError(t, err)\n\t\terr = pt.load(ctx, stopAfterCatchup)\n\t\trequire.NoError(t, err)\n\t\trequire.True(t, pt.state.IsState(State(PartitionRunning)))\n\t})\n\tt.Run(\"local_offset_too_high_stopAfterCatchup_no_error\", func(t *testing.T) {\n\t\tvar (\n\t\t\toldest           int64 = 161\n\t\t\tnewest           int64 = 1312\n\t\t\tlocal            int64 = 1314\n\t\t\tstopAfterCatchup       = true\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\t\"some-topic\",\n\t\t\t0,\n\t\t\tnil,\n\t\t\tnil,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tbm.mst.EXPECT().Open().Return(nil)\n\t\tbm.mst.EXPECT().GetOffset(offsetNotStored).Return(local, nil)\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetOldest).Return(oldest, nil)\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetNewest).Return(newest, nil)\n\t\tbm.mst.EXPECT().MarkRecovered().Return(nil)\n\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\t\terr := pt.setup(ctx)\n\t\trequire.NoError(t, err)\n\t\terr = pt.load(ctx, stopAfterCatchup)\n\t\trequire.NoError(t, err)\n\t})\n\tt.Run(\"consume\", func(t *testing.T) {\n\t\tvar (\n\t\t\toldest           int64 = 161\n\t\t\tnewest           int64 = 1312\n\t\t\tlocal            int64 = sarama.OffsetOldest\n\t\t\tstopAfterCatchup       = false\n\t\t\tconsumer               = defaultSaramaAutoConsumerMock(t)\n\t\t\ttopic                  = \"some-topic\"\n\t\t\tpartition        int32\n\t\t\tcount            int64\n\t\t\tupdateCB         UpdateCallback = func(ctx UpdateContext, s storage.Storage, key string, value []byte) error {\n\t\t\t\tatomic.AddInt64(&count, 1)\n\t\t\t\treturn nil\n\t\t\t}\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\ttopic,\n\t\t\tpartition,\n\t\t\tnil,\n\t\t\tupdateCB,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tpt.consumer = consumer\n\t\tbm.mst.EXPECT().Open().Return(nil)\n\t\tbm.mst.EXPECT().GetOffset(gomock.Any()).Return(local, nil)\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetOldest).Return(oldest, nil)\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetNewest).Return(newest, nil)\n\t\tpartConsumer := consumer.ExpectConsumePartition(topic, partition, anyOffset)\n\t\tpartConsumer.ExpectMessagesDrainedOnClose()\n\t\tfor i := 0; i < 10; i++ {\n\t\t\tpartConsumer.YieldMessage(&sarama.ConsumerMessage{})\n\t\t\tbm.mst.EXPECT().SetOffset(gomock.Any()).Return(nil)\n\t\t}\n\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\t\tgo func() {\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\tdefault:\n\t\t\t\t}\n\t\t\t\tif atomic.LoadInt64(&count) == 10 {\n\t\t\t\t\tcancel()\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\n\t\terr := pt.setup(ctx)\n\t\trequire.NoError(t, err)\n\t\terr = pt.load(ctx, stopAfterCatchup)\n\t\trequire.NoError(t, err)\n\t\trequire.True(t, atomic.LoadInt64(&count) == 10)\n\t})\n}\n\nfunc TestPT_loadMessages(t *testing.T) {\n\tt.Run(\"consume_till_hwm\", func(t *testing.T) {\n\t\tvar (\n\t\t\tlocalOffset      int64 = sarama.OffsetOldest\n\t\t\tpartitionHwm     int64 = 1\n\t\t\tstopAfterCatchup       = true\n\t\t\ttopic                  = \"some-topic\"\n\t\t\tpartition        int32\n\t\t\tconsumer         = defaultSaramaAutoConsumerMock(t)\n\t\t\trecKey           string\n\t\t\trecVal           []byte\n\t\t\tupdateCB         UpdateCallback = func(ctx UpdateContext, s storage.Storage, key string, value []byte) error {\n\t\t\t\trecKey = key\n\t\t\t\trecVal = value\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tkey   = \"some-key\"\n\t\t\tvalue = []byte(\"some-vale\")\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\t\"some-topic\",\n\t\t\t0,\n\t\t\tnil,\n\t\t\tupdateCB,\n\t\t)\n\t\tdefer ctrl.Finish()\n\n\t\tpartConsumer := consumer.ExpectConsumePartition(topic, partition, localOffset)\n\t\tpartConsumer.YieldMessage(&sarama.ConsumerMessage{\n\t\t\tKey:       []byte(key),\n\t\t\tValue:     value,\n\t\t\tTopic:     topic,\n\t\t\tPartition: partition,\n\t\t\tOffset:    partitionHwm,\n\t\t})\n\t\tpartConsumer.ExpectMessagesDrainedOnClose()\n\t\tbm.mst.EXPECT().SetOffset(int64(0)).Return(nil)\n\t\tbm.mst.EXPECT().Open().Return(nil)\n\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\t\terr := pt.setup(ctx)\n\t\trequire.NoError(t, err)\n\t\terr = pt.loadMessages(ctx, partConsumer, partitionHwm, stopAfterCatchup)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, recKey, key)\n\t\trequire.Equal(t, recVal, value)\n\t})\n\tt.Run(\"consume_till_hwm_more_msgs\", func(t *testing.T) {\n\t\tvar (\n\t\t\tlocalOffset      int64\n\t\t\tpartitionHwm     int64 = 2\n\t\t\tstopAfterCatchup       = true\n\t\t\ttopic                  = \"some-topic\"\n\t\t\tpartition        int32\n\t\t\tconsumer                        = defaultSaramaAutoConsumerMock(t)\n\t\t\tupdateCB         UpdateCallback = updateCallbackNoop\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\ttopic,\n\t\t\tpartition,\n\t\t\tnil,\n\t\t\tupdateCB,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tpartConsumer := consumer.ExpectConsumePartition(topic, partition, localOffset)\n\t\tpartConsumer.YieldMessage(&sarama.ConsumerMessage{\n\t\t\tTopic:     topic,\n\t\t\tPartition: partition,\n\t\t\tOffset:    1,\n\t\t})\n\t\tpartConsumer.YieldMessage(&sarama.ConsumerMessage{\n\t\t\tTopic:     topic,\n\t\t\tPartition: partition,\n\t\t\tOffset:    1,\n\t\t})\n\t\tpartConsumer.ExpectMessagesDrainedOnClose()\n\t\tbm.mst.EXPECT().SetOffset(int64(0)).Return(nil)\n\t\tbm.mst.EXPECT().SetOffset(int64(1)).Return(nil)\n\t\tbm.mst.EXPECT().Open().Return(nil)\n\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\t\terr := pt.setup(ctx)\n\t\trequire.NoError(t, err)\n\t\terr = pt.loadMessages(ctx, partConsumer, partitionHwm, stopAfterCatchup)\n\t\trequire.NoError(t, err)\n\t})\n\tt.Run(\"consume_till_cancel\", func(t *testing.T) {\n\t\tvar (\n\t\t\tlocalOffset      int64\n\t\t\tpartitionHwm     int64 = 2\n\t\t\tstopAfterCatchup       = false\n\t\t\ttopic                  = \"some-topic\"\n\t\t\tpartition        int32\n\t\t\tconsumer         = defaultSaramaAutoConsumerMock(t)\n\t\t\tcount            int64\n\t\t\tupdateCB         UpdateCallback = func(ctx UpdateContext, s storage.Storage, key string, value []byte) error {\n\t\t\t\tatomic.AddInt64(&count, 1)\n\t\t\t\treturn nil\n\t\t\t}\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\ttopic,\n\t\t\tpartition,\n\t\t\tnil,\n\t\t\tupdateCB,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tpartConsumer := consumer.ExpectConsumePartition(topic, partition, localOffset)\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\t\tbm.mst.EXPECT().Open().Return(nil)\n\t\terr := pt.setup(ctx)\n\t\trequire.NoError(t, err)\n\t\tgo func(ctx context.Context) {\n\t\t\tfor i := 0; i < 100; i++ {\n\t\t\t\tbm.mst.EXPECT().SetOffset(gomock.Any()).Return(nil)\n\t\t\t\tpartConsumer.YieldMessage(&sarama.ConsumerMessage{})\n\t\t\t}\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\tdefault:\n\t\t\t\t}\n\t\t\t\tif atomic.LoadInt64(&count) == 100 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tcancel()\n\t\t}(ctx)\n\t\terr = pt.loadMessages(ctx, partConsumer, partitionHwm, stopAfterCatchup)\n\t\trequire.NoError(t, err)\n\t\trequire.True(t, atomic.LoadInt64(&count) == 100)\n\t})\n\tt.Run(\"close_msg_chan\", func(t *testing.T) {\n\t\tvar (\n\t\t\tlocalOffset      int64\n\t\t\tpartitionHwm     int64 = 2\n\t\t\tstopAfterCatchup       = false\n\t\t\ttopic                  = \"some-topic\"\n\t\t\tpartition        int32\n\t\t\tconsumer                        = defaultSaramaAutoConsumerMock(t)\n\t\t\tupdateCB         UpdateCallback = updateCallbackNoop\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\ttopic,\n\t\t\tpartition,\n\t\t\tnil,\n\t\t\tupdateCB,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\t// need to make the message-buffer 0-size, to avoid a race-condition in the test.\n\t\t// Closing the partition-consumer while loading will close the error channel, which will stop\n\t\t// message processing. Usually this isn't a problem, because the partition table usually stops by itself\n\t\t// when reaching HWM. For the test though, it would not recover the expected messages, because it'll drain the channels only.\n\t\t// By setting chan-size to 0, we'll ensure the message is processed.\n\t\tconsumer.config.ChannelBufferSize = 0\n\t\tpartConsumer := consumer.ExpectConsumePartition(topic, partition, localOffset)\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tbm.mst.EXPECT().Open().Return(nil)\n\t\terr := pt.setup(ctx)\n\t\trequire.NoError(t, err)\n\t\tgo func() {\n\t\t\tdefer cancel()\n\t\t\terr = pt.loadMessages(ctx, partConsumer, partitionHwm, stopAfterCatchup)\n\t\t\trequire.NoError(t, err)\n\t\t}()\n\t\tgo func(ctx context.Context) {\n\t\t\tvar (\n\t\t\t\tlock sync.Mutex\n\t\t\t\topen = true\n\t\t\t)\n\t\t\tgo func() {\n\t\t\t\tlock.Lock()\n\t\t\t\tdefer lock.Unlock()\n\t\t\t\tpartConsumer.AsyncClose()\n\t\t\t\topen = false\n\t\t\t}()\n\t\t\tfor i := 0; i < 100; i++ {\n\t\t\t\tselect {\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\tdefault:\n\t\t\t\t}\n\t\t\t\tlock.Lock()\n\t\t\t\tif open {\n\t\t\t\t\tbm.mst.EXPECT().SetOffset(gomock.Any()).Return(nil)\n\t\t\t\t\tpartConsumer.YieldMessage(&sarama.ConsumerMessage{})\n\t\t\t\t}\n\t\t\t\tlock.Unlock()\n\t\t\t}\n\t\t}(ctx)\n\t\t<-ctx.Done()\n\t})\n\tt.Run(\"stalled\", func(t *testing.T) {\n\t\tvar (\n\t\t\tlocalOffset      int64\n\t\t\tpartitionHwm     int64 = 2\n\t\t\tstopAfterCatchup       = false\n\t\t\ttopic                  = \"some-topic\"\n\t\t\tpartition        int32\n\t\t\tconsumer         = defaultSaramaAutoConsumerMock(t)\n\t\t\tctx              = context.Background()\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\ttopic,\n\t\t\tpartition,\n\t\t\tnil,\n\t\t\tnil,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tpt.stalledTimeout = time.Duration(0)\n\t\tpt.stallPeriod = time.Nanosecond\n\n\t\tbm.mst.EXPECT().Open().Return(nil)\n\t\tpartConsumer := consumer.ExpectConsumePartition(topic, partition, localOffset)\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\terr := pt.setup(ctx)\n\t\trequire.NoError(t, err)\n\t\tgo func() {\n\t\t\tdefer cancel()\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\tdefault:\n\t\t\t\t}\n\n\t\t\t\tif pt.fetchStats(ctx).Stalled {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t\terr = pt.loadMessages(ctx, partConsumer, partitionHwm, stopAfterCatchup)\n\t\trequire.NoError(t, err)\n\t})\n\tt.Run(\"fail\", func(t *testing.T) {\n\t\tvar (\n\t\t\tlocalOffset      int64\n\t\t\tpartitionHwm     int64 = 2\n\t\t\tstopAfterCatchup       = true\n\t\t\ttopic                  = \"some-topic\"\n\t\t\tpartition        int32\n\t\t\tconsumer                        = defaultSaramaAutoConsumerMock(t)\n\t\t\tretErr           error          = fmt.Errorf(\"update error\")\n\t\t\tupdateCB         UpdateCallback = func(ctx UpdateContext, s storage.Storage, key string, value []byte) error {\n\t\t\t\treturn retErr\n\t\t\t}\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\ttopic,\n\t\t\tpartition,\n\t\t\tnil,\n\t\t\tupdateCB,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tbm.mst.EXPECT().Open().Return(nil)\n\t\tpartConsumer := consumer.ExpectConsumePartition(topic, partition, localOffset)\n\t\tpartConsumer.YieldMessage(&sarama.ConsumerMessage{\n\t\t\tTopic:     topic,\n\t\t\tPartition: partition,\n\t\t\tOffset:    1,\n\t\t})\n\t\tpartConsumer.ExpectMessagesDrainedOnClose()\n\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\t\terr := pt.setup(ctx)\n\t\trequire.NoError(t, err)\n\t\terr = pt.loadMessages(ctx, partConsumer, partitionHwm, stopAfterCatchup)\n\t\trequire.Error(t, err)\n\t})\n}\n\nfunc TestPT_storeEvent(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tvar (\n\t\t\tlocalOffset     int64\n\t\t\tpartition       int32\n\t\t\ttopic           = \"some-topic\"\n\t\t\tkey             = \"some-key\"\n\t\t\tvalue           = []byte(\"some-vale\")\n\t\t\ttimestamp       = time.Now()\n\t\t\tactualKey       string\n\t\t\tactualValue     []byte\n\t\t\tactualTimestamp time.Time\n\t\t\tupdateCB        UpdateCallback = func(ctx UpdateContext, s storage.Storage, k string, v []byte) error {\n\t\t\t\tactualKey = k\n\t\t\t\tactualValue = v\n\t\t\t\tactualTimestamp = ctx.Timestamp()\n\t\t\t\treturn nil\n\t\t\t}\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\ttopic,\n\t\t\tpartition,\n\t\t\tnil,\n\t\t\tupdateCB,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tbm.mst.EXPECT().Open().Return(nil)\n\t\tbm.mst.EXPECT().SetOffset(localOffset).Return(nil)\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\t\terr := pt.setup(ctx)\n\t\trequire.NoError(t, err)\n\t\terr = pt.storeEvent(key, value, localOffset, nil, timestamp)\n\t\trequire.Equal(t, actualKey, key)\n\t\trequire.Equal(t, actualValue, value)\n\t\trequire.Equal(t, actualTimestamp, timestamp)\n\t\trequire.NoError(t, err)\n\t})\n\tt.Run(\"fail\", func(t *testing.T) {\n\t\tvar (\n\t\t\tlocalOffset int64\n\t\t\tpartition   int32\n\t\t\ttopic                      = \"some-topic\"\n\t\t\tkey                        = \"some-key\"\n\t\t\tvalue                      = []byte(\"some-vale\")\n\t\t\ttimestamp                  = time.Now()\n\t\t\tupdateCB    UpdateCallback = updateCallbackNoop\n\t\t\tretErr      error          = fmt.Errorf(\"storage err\")\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\ttopic,\n\t\t\tpartition,\n\t\t\tnil,\n\t\t\tupdateCB,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tbm.mst.EXPECT().Open().Return(nil)\n\t\tbm.mst.EXPECT().SetOffset(localOffset).Return(retErr)\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\t\terr := pt.setup(ctx)\n\t\trequire.NoError(t, err)\n\t\terr = pt.storeEvent(key, value, localOffset, nil, timestamp)\n\t\trequire.Error(t, err)\n\t})\n}\n\nfunc TestPT_Close(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tvar (\n\t\t\tpartition int32\n\t\t\ttopic     = \"some-topic\"\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\ttopic,\n\t\t\tpartition,\n\t\t\tnil,\n\t\t\tnil,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tbm.mst.EXPECT().Close().Return(nil)\n\t\tbm.mst.EXPECT().Open().Return(nil)\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\t\terr := pt.setup(ctx)\n\t\trequire.NoError(t, err)\n\t\terr = pt.Close()\n\t\trequire.NoError(t, err)\n\t})\n\tt.Run(\"succeed2\", func(t *testing.T) {\n\t\tvar (\n\t\t\tpartition int32\n\t\t\ttopic     = \"some-topic\"\n\t\t)\n\t\tpt, _, ctrl := defaultPT(\n\t\t\tt,\n\t\t\ttopic,\n\t\t\tpartition,\n\t\t\tnil,\n\t\t\tnil,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\terr := pt.Close()\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc TestPT_markRecovered(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tvar (\n\t\t\tpartition int32\n\t\t\ttopic     = \"some-topic\"\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\ttopic,\n\t\t\tpartition,\n\t\t\tnil,\n\t\t\tnil,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tbm.mst.EXPECT().Open().Return(nil)\n\t\tbm.mst.EXPECT().MarkRecovered().Return(nil)\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\t\terr := pt.setup(ctx)\n\t\trequire.NoError(t, err)\n\t\trequire.True(t, !pt.state.IsState(State(PartitionRunning)))\n\t\terr = pt.markRecovered(ctx)\n\t\trequire.NoError(t, err)\n\t\trequire.True(t, pt.state.IsState(State(PartitionRunning)))\n\t})\n\tt.Run(\"fail\", func(t *testing.T) {\n\t\tvar (\n\t\t\tpartition int32\n\t\t\ttopic           = \"some-topic\"\n\t\t\tretErr    error = fmt.Errorf(\"store error\")\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\ttopic,\n\t\t\tpartition,\n\t\t\tnil,\n\t\t\tnil,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tbm.mst.EXPECT().Open().Return(nil)\n\t\tbm.mst.EXPECT().MarkRecovered().Return(retErr)\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\t\terr := pt.setup(ctx)\n\t\trequire.NoError(t, err)\n\t\terr = pt.markRecovered(ctx)\n\t\trequire.Error(t, err)\n\t\trequire.True(t, pt.state.IsState(State(PartitionPreparing)))\n\t})\n}\n\nfunc TestPT_SetupAndCatchupToHwm(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tvar (\n\t\t\toldest    int64\n\t\t\tnewest    int64 = 5\n\t\t\tlocal           = oldest\n\t\t\tconsumer        = defaultSaramaAutoConsumerMock(t)\n\t\t\ttopic           = \"some-topic\"\n\t\t\tpartition int32\n\t\t\tcount     int64\n\t\t\tupdateCB  UpdateCallback = func(ctx UpdateContext, s storage.Storage, key string, value []byte) error {\n\t\t\t\tatomic.AddInt64(&count, 1)\n\t\t\t\treturn nil\n\t\t\t}\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\ttopic,\n\t\t\tpartition,\n\t\t\tnil,\n\t\t\tupdateCB,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tpt.consumer = consumer\n\t\tbm.mst.EXPECT().Open().Return(nil)\n\t\tbm.mst.EXPECT().GetOffset(gomock.Any()).Return(local, nil)\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetOldest).Return(oldest, nil)\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetNewest).Return(newest, nil)\n\t\tbm.mst.EXPECT().MarkRecovered().Return(nil)\n\t\tpartConsumer := consumer.ExpectConsumePartition(topic, partition, local+1)\n\t\tpartConsumer.ExpectMessagesDrainedOnClose()\n\n\t\tmsgsToRecover := newest - local\n\t\tfor i := int64(0); i < msgsToRecover; i++ {\n\t\t\tpartConsumer.YieldMessage(&sarama.ConsumerMessage{})\n\t\t\tbm.mst.EXPECT().SetOffset(gomock.Any()).Return(nil)\n\t\t}\n\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second*5)\n\t\tdefer cancel()\n\n\t\terr := pt.SetupAndRecover(ctx, false)\n\t\trequire.NoError(t, err)\n\t\trequire.True(t, atomic.LoadInt64(&count) == msgsToRecover)\n\t})\n\tt.Run(\"fail\", func(t *testing.T) {\n\t\tvar (\n\t\t\tconsumer  = defaultSaramaAutoConsumerMock(t)\n\t\t\ttopic     = \"some-topic\"\n\t\t\tpartition int32\n\t\t\tretErr    = fmt.Errorf(\"offset-error\")\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\ttopic,\n\t\t\tpartition,\n\t\t\tnil,\n\t\t\tnil,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tpt.consumer = consumer\n\t\tbm.mst.EXPECT().Open().Return(nil)\n\t\tbm.mst.EXPECT().GetOffset(gomock.Any()).Return(int64(0), retErr)\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\n\t\terr := pt.SetupAndRecover(ctx, false)\n\t\trequire.Error(t, err)\n\t})\n}\n\nfunc TestPT_SetupAndCatchupForever(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tvar (\n\t\t\toldest    int64\n\t\t\tnewest    int64 = 10\n\t\t\tconsumer        = defaultSaramaAutoConsumerMock(t)\n\t\t\ttopic           = \"some-topic\"\n\t\t\tpartition int32\n\t\t\tcount     int64\n\t\t\tupdateCB  UpdateCallback = func(ctx UpdateContext, s storage.Storage, key string, value []byte) error {\n\t\t\t\tatomic.AddInt64(&count, 1)\n\t\t\t\treturn nil\n\t\t\t}\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\ttopic,\n\t\t\tpartition,\n\t\t\tnil,\n\t\t\tupdateCB,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tbm.useMemoryStorage()\n\t\tpt.consumer = consumer\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetOldest).Return(oldest, nil).AnyTimes()\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetNewest).Return(newest, nil).AnyTimes()\n\t\tpartConsumer := consumer.ExpectConsumePartition(topic, partition, anyOffset)\n\t\tfor i := 0; i < 10; i++ {\n\t\t\tpartConsumer.YieldMessage(&sarama.ConsumerMessage{})\n\t\t}\n\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\t\tgo func() {\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\tdefault:\n\t\t\t\t}\n\t\t\t\tif atomic.LoadInt64(&count) == 10 {\n\t\t\t\t\ttime.Sleep(time.Millisecond * 10)\n\t\t\t\t\tcancel()\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\n\t\terr := pt.SetupAndRecover(ctx, false)\n\t\trequire.NoError(t, err)\n\t\tcancel()\n\t})\n\tt.Run(\"fail\", func(t *testing.T) {\n\t\tvar (\n\t\t\tconsumer  = defaultSaramaAutoConsumerMock(t)\n\t\t\ttopic     = \"some-topic\"\n\t\t\tpartition int32\n\t\t\tretErr    = fmt.Errorf(\"offset-error\")\n\t\t)\n\t\tpt, bm, ctrl := defaultPT(\n\t\t\tt,\n\t\t\ttopic,\n\t\t\tpartition,\n\t\t\tnil,\n\t\t\tnil,\n\t\t)\n\t\tdefer ctrl.Finish()\n\t\tpt.consumer = consumer\n\t\tbm.mst.EXPECT().Open().Return(nil)\n\t\tbm.mst.EXPECT().GetOffset(gomock.Any()).Return(int64(0), retErr)\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\n\t\terr := pt.SetupAndRecover(ctx, false)\n\t\trequire.Error(t, err)\n\t\tcancel()\n\t})\n}\n"
        },
        {
          "name": "processor.go",
          "type": "blob",
          "size": 30.1826171875,
          "content": "package goka\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/IBM/sarama\"\n\t\"github.com/hashicorp/go-multierror\"\n\n\t\"github.com/lovoo/goka/multierr\"\n\t\"github.com/lovoo/goka/storage\"\n)\n\nconst (\n\t// ProcStateIdle indicates an idling partition processor (not started yet)\n\tProcStateIdle State = iota\n\t// ProcStateStarting indicates a starting partition processor, i.e. before rebalance\n\tProcStateStarting\n\t// ProcStateSetup indicates a partition processor during setup of a rebalance round\n\tProcStateSetup\n\t// ProcStateRunning indicates a running partition processor\n\tProcStateRunning\n\t// ProcStateStopping indicates a stopping partition processor\n\tProcStateStopping\n\t// ProcStateStopped indicates a stopped partition processor\n\tProcStateStopped\n)\n\n// ProcessCallback function is called for every message received by the\n// processor.\ntype ProcessCallback func(ctx Context, msg interface{})\n\n// Processor is a set of stateful callback functions that, on the arrival of\n// messages, modify the content of a table (the group table) and emit messages into other\n// topics. Messages as well as rows in the group table are key-value pairs.\n// A group is composed by multiple processor instances.\ntype Processor struct {\n\topts    *poptions\n\tlog     logger\n\tbrokers []string\n\n\t// hook used to be notified whenever the processor has rebalanced to a new assignment\n\trebalanceCallback RebalanceCallback\n\n\t// rwmutex protecting read/write of partitions and lookuptables.\n\tmTables sync.RWMutex\n\t// Partition processors\n\tpartitions map[int32]*PartitionProcessor\n\t// lookup tables\n\tlookupTables map[string]*View\n\n\tpartitionCount int\n\n\tgraph *GroupGraph\n\n\tsaramaConsumer sarama.Consumer\n\tproducer       Producer\n\ttmgr           TopicManager\n\n\tstate *Signal\n\n\terrMux sync.Mutex\n\terr    error\n\tdone   chan struct{}\n\tcancel context.CancelFunc\n}\n\n// NewProcessor creates a processor instance in a group given the address of\n// Kafka brokers, the consumer group name, a list of subscriptions (topics,\n// codecs, and callbacks), and series of options.\nfunc NewProcessor(brokers []string, gg *GroupGraph, options ...ProcessorOption) (*Processor, error) {\n\toptions = append(\n\t\t// default options comes first\n\t\t[]ProcessorOption{\n\t\t\tWithClientID(fmt.Sprintf(\"goka-processor-%s\", gg.Group())),\n\t\t\tWithUpdateCallback(DefaultUpdate),\n\t\t\tWithPartitionChannelSize(defaultPartitionChannelSize),\n\t\t\tWithStorageBuilder(storage.DefaultBuilder(DefaultProcessorStoragePath(gg.Group()))),\n\t\t\tWithRebalanceCallback(DefaultRebalance),\n\t\t},\n\n\t\t// user-defined options (may overwrite default ones)\n\t\toptions...,\n\t)\n\n\tif err := gg.Validate(); err != nil {\n\t\treturn nil, err\n\t}\n\n\topts := new(poptions)\n\terr := opts.applyOptions(gg, options...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(errApplyOptions, err)\n\t}\n\n\tnpar, err := prepareTopics(brokers, gg, opts)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// create views\n\tlookupTables := make(map[string]*View)\n\tfor _, t := range gg.LookupTables() {\n\t\tview, err := NewView(brokers, Table(t.Topic()), t.Codec(),\n\t\t\tWithViewLogger(opts.log),\n\t\t\tWithViewHasher(opts.hasher),\n\t\t\tWithViewClientID(opts.clientID),\n\t\t\tWithViewTopicManagerBuilder(opts.builders.topicmgr),\n\t\t\tWithViewStorageBuilder(opts.builders.storage),\n\t\t\tWithViewConsumerSaramaBuilder(opts.builders.consumerSarama),\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"error creating view: %v\", err)\n\t\t}\n\t\tlookupTables[t.Topic()] = view\n\t}\n\n\t// combine things together\n\tprocessor := &Processor{\n\t\topts:    opts,\n\t\tlog:     opts.log.Prefix(fmt.Sprintf(\"Processor %s\", gg.Group())),\n\t\tbrokers: brokers,\n\n\t\trebalanceCallback: opts.rebalanceCallback,\n\n\t\tpartitions:     make(map[int32]*PartitionProcessor),\n\t\tpartitionCount: npar,\n\t\tlookupTables:   lookupTables,\n\n\t\tgraph: gg,\n\n\t\tstate: NewSignal(ProcStateIdle, ProcStateStarting, ProcStateSetup, ProcStateRunning, ProcStateStopping, ProcStateStopped).SetState(ProcStateIdle),\n\t\tdone:  make(chan struct{}),\n\t}\n\n\treturn processor, nil\n}\n\n// Graph returns the group graph of the processor.\nfunc (g *Processor) Graph() *GroupGraph {\n\treturn g.graph\n}\n\n// isStateless returns whether the processor is a stateless one.\nfunc (g *Processor) isStateless() bool {\n\treturn g.graph.GroupTable() == nil\n}\n\n///////////////////////////////////////////////////////////////////////////////\n// value getter\n///////////////////////////////////////////////////////////////////////////////\n\n// Get returns a read-only copy of a value from the group table if the\n// respective partition is owned by the processor instace.\n// Get can be called by multiple goroutines concurrently.\n// Get can be only used with stateful processors (ie, when group table is\n// enabled) and after Recovered returns true.\nfunc (g *Processor) Get(key string) (interface{}, error) {\n\tif g.isStateless() {\n\t\treturn nil, fmt.Errorf(\"can't get a value from stateless processor\")\n\t}\n\n\t// find partition where key is located\n\ts, err := g.find(key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// get key and return\n\tval, err := s.Get(key)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error getting %s: %v\", key, err)\n\t} else if val == nil {\n\t\t// if the key does not exist the return value is nil\n\t\treturn nil, nil\n\t}\n\n\t// since we don't know what the codec does, make copy of the object\n\tdata := make([]byte, len(val))\n\tcopy(data, val)\n\tvalue, err := g.graph.GroupTable().Codec().Decode(data)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error decoding %s: %v\", key, err)\n\t}\n\treturn value, nil\n}\n\nfunc (g *Processor) find(key string) (storage.Storage, error) {\n\tp, err := g.hash(key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tpproc, ok := g.getPartProc(p)\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"this processor does not contain partition %v\", p)\n\t}\n\treturn pproc.table.st, nil\n}\n\nfunc (g *Processor) getPartProc(partition int32) (*PartitionProcessor, bool) {\n\tg.mTables.RLock()\n\tdefer g.mTables.RUnlock()\n\tpproc, ok := g.partitions[partition]\n\treturn pproc, ok\n}\n\nfunc (g *Processor) setPartProc(partition int32, pproc *PartitionProcessor) {\n\tg.mTables.Lock()\n\tdefer g.mTables.Unlock()\n\tif pproc == nil {\n\t\tdelete(g.partitions, partition)\n\t} else {\n\t\tg.partitions[partition] = pproc\n\t}\n}\n\nfunc (g *Processor) hash(key string) (int32, error) {\n\t// create a new hasher every time. Alternative would be to store the hash in\n\t// view and every time reset the hasher (ie, hasher.Reset()). But that would\n\t// also require us to protect the access of the hasher with a mutex.\n\thasher := g.opts.hasher()\n\n\t_, err := hasher.Write([]byte(key))\n\tif err != nil {\n\t\treturn -1, err\n\t}\n\thash := int32(hasher.Sum32())\n\tif hash < 0 {\n\t\thash = -hash\n\t}\n\tif g.partitionCount == 0 {\n\t\treturn 0, errors.New(\"can't hash with 0 partitions\")\n\t}\n\treturn hash % int32(g.partitionCount), nil\n}\n\n// Run starts the processor using passed context.\n// The processor stops in case of errors or if the context is cancelled\nfunc (g *Processor) Run(ctx context.Context) (rerr error) {\n\tg.log.Debugf(\"starting\")\n\tdefer g.log.Debugf(\"stopped\")\n\n\t// check if the processor was done already\n\tselect {\n\tcase <-g.done:\n\t\treturn fmt.Errorf(\"error running processor: it was already run and terminated. Run can only be called once\")\n\tdefault:\n\t}\n\n\t// create errorgroup\n\tctx, g.cancel = context.WithCancel(ctx)\n\terrg, ctx := multierr.NewErrGroup(ctx)\n\tdefer close(g.done)\n\tdefer g.cancel()\n\n\t// set a starting state. From this point on we know that there's a cancel and a valid context set\n\t// in the processor which we can use for waiting\n\tg.state.SetState(ProcStateStarting)\n\n\t// collect all errors before leaving\n\tvar errs *multierror.Error\n\tdefer func() {\n\t\tg.errMux.Lock()\n\t\tdefer g.errMux.Unlock()\n\t\tg.err = multierror.Append(errs, rerr).ErrorOrNil()\n\t\trerr = g.err\n\t}()\n\n\tvar err error\n\tg.saramaConsumer, err = g.opts.builders.consumerSarama(g.brokers, g.opts.clientID)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Error creating consumer for brokers [%s]: %v\", strings.Join(g.brokers, \",\"), err)\n\t}\n\n\t// close sarama consume after we're done\n\tdefer func() {\n\t\tif err := g.saramaConsumer.Close(); err != nil {\n\t\t\terrs = multierror.Append(errs, fmt.Errorf(\"error closing sarama consumer: %w\", err))\n\t\t}\n\t}()\n\n\tg.tmgr, err = g.opts.builders.topicmgr(g.brokers)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Error creating topic manager for brokers [%s]: %v\", strings.Join(g.brokers, \",\"), err)\n\t}\n\n\t// create kafka producer\n\tg.log.Debugf(\"creating producer\")\n\tproducer, err := g.opts.builders.producer(g.brokers, g.opts.clientID, g.opts.hasher)\n\tif err != nil {\n\t\treturn fmt.Errorf(errBuildProducer, err)\n\t}\n\tg.producer = producer\n\tdefer func() {\n\t\tg.log.Debugf(\"closing producer\")\n\t\tdefer g.log.Debugf(\"producer ... closed\")\n\t\tif err := g.producer.Close(); err != nil {\n\t\t\terrs = multierror.Append(errs, fmt.Errorf(\"error closing producer: %w\", err))\n\t\t}\n\t}()\n\n\t// start all lookup tables\n\tg.mTables.RLock()\n\tfor topic, view := range g.lookupTables {\n\t\tg.log.Debugf(\"Starting lookup table for %s\", topic)\n\t\t// make local copies\n\t\ttopic, view := topic, view\n\t\terrg.Go(func() error {\n\t\t\tif err := view.Run(ctx); err != nil {\n\t\t\t\treturn fmt.Errorf(\"error running lookup table %s: %v\", topic, err)\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t}\n\tg.mTables.RUnlock()\n\n\tif err := g.waitForStartupTables(ctx); err != nil {\n\t\treturn fmt.Errorf(\"error waiting for start up tables: %w\", err)\n\t}\n\n\t// run the main rebalance-consume-loop\n\terrg.Go(func() error {\n\t\treturn g.rebalanceLoop(ctx)\n\t})\n\n\treturn errg.Wait().ErrorOrNil()\n}\n\nfunc (g *Processor) rebalanceLoop(ctx context.Context) (rerr error) {\n\t// create kafka consumer\n\tconsumerGroup, err := g.opts.builders.consumerGroup(g.brokers, string(g.graph.Group()), g.opts.clientID)\n\tif err != nil {\n\t\treturn fmt.Errorf(errBuildConsumer, err)\n\t}\n\n\tvar topics []string\n\tfor _, e := range g.graph.InputStreams() {\n\t\ttopics = append(topics, e.Topic())\n\t}\n\tif g.graph.LoopStream() != nil {\n\t\ttopics = append(topics, g.graph.LoopStream().Topic())\n\t}\n\n\tvar errs *multierror.Error\n\n\tdefer func() {\n\t\trerr = multierror.Append(errs, rerr).ErrorOrNil()\n\t}()\n\n\tdefer func() {\n\t\tg.log.Debugf(\"closing consumer group\")\n\n\t\tcloseErr := consumerGroup.Close()\n\t\tif closeErr != nil {\n\t\t\tg.log.Printf(\"Error closing consumer group: %v\", closeErr)\n\t\t\terrs = multierror.Append(\n\t\t\t\terrs,\n\t\t\t\tfmt.Errorf(\"Error closing consumer group: %w\", closeErr),\n\t\t\t)\n\t\t}\n\n\t\tg.log.Debugf(\"closing consumer group ... done\")\n\t}()\n\n\tfor {\n\t\tsessionCtx, sessionCtxCancel := context.WithCancel(ctx)\n\n\t\tgo func() {\n\t\t\tg.handleSessionErrors(ctx, sessionCtx, sessionCtxCancel, consumerGroup)\n\t\t}()\n\n\t\terr := consumerGroup.Consume(ctx, topics, g)\n\t\tsessionCtxCancel()\n\n\t\t// error consuming, no way to recover so we have to kill the processor\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"error consuming from group consumer: %w\", err)\n\t\t}\n\n\t\tselect {\n\t\tcase <-time.After(5 * time.Second):\n\t\t\tg.log.Printf(\"Consumer group returned, Rebalancing.\")\n\t\tcase <-ctx.Done():\n\t\t\tg.log.Printf(\"Consumer group cancelled. Stopping\")\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\nfunc (g *Processor) handleSessionErrors(ctx, sessionCtx context.Context, sessionCtxCancel context.CancelFunc, consumerGroup sarama.ConsumerGroup) {\n\terrs := consumerGroup.Errors()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase <-sessionCtx.Done():\n\t\t\treturn\n\t\tcase err, ok := <-errs:\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif err != nil {\n\t\t\t\tg.log.Printf(\"error during execution of consumer group: %v\", err)\n\t\t\t}\n\n\t\t\tvar (\n\t\t\t\terrProc  *errProcessing\n\t\t\t\terrSetup *errSetup\n\t\t\t)\n\n\t\t\tif errors.As(err, &errProc) {\n\t\t\t\tg.log.Debugf(\"error processing message (non-transient), shutting down processor: %v\", err)\n\t\t\t\tsessionCtxCancel()\n\t\t\t}\n\t\t\tif errors.As(err, &errSetup) {\n\t\t\t\tg.log.Debugf(\"setup error (non-transient), shutting down processor: %v\", err)\n\t\t\t\tsessionCtxCancel()\n\t\t\t}\n\t\t}\n\t}\n}\n\n// waits for all tables that are supposed to start up\nfunc (g *Processor) waitForStartupTables(ctx context.Context) error {\n\terrg, startupCtx := multierr.NewErrGroup(ctx)\n\n\tvar (\n\t\twaitMap  = make(map[string]struct{})\n\t\tmWaitMap sync.Mutex\n\t)\n\n\t// we'll wait for all lookup tables to have recovered.\n\t// For this we're looping through all tables and start\n\t// a new goroutine that terminates when the table is done (or ctx is closed).\n\t// The extra code adds and removes the table to a map used for logging\n\t// the items that the processor is still waiting to recover before ready to go.\n\tg.mTables.RLock()\n\tfor _, view := range g.lookupTables {\n\t\tview := view\n\n\t\terrg.Go(func() error {\n\t\t\tname := fmt.Sprintf(\"lookup-table-%s\", view.topic)\n\t\t\tmWaitMap.Lock()\n\t\t\twaitMap[name] = struct{}{}\n\t\t\tmWaitMap.Unlock()\n\n\t\t\tdefer func() {\n\t\t\t\tmWaitMap.Lock()\n\t\t\t\tdefer mWaitMap.Unlock()\n\t\t\t\tdelete(waitMap, name)\n\t\t\t}()\n\n\t\t\tselect {\n\t\t\tcase <-startupCtx.Done():\n\t\t\tcase <-view.WaitRunning():\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t}\n\tg.mTables.RUnlock()\n\n\t// If we recover ahead, we'll also start all partition processors once in recover-only-mode\n\t// and do the same boilerplate to keep the waitmap up to date.\n\tif g.opts.recoverAhead {\n\t\tpartitions, err := g.findStatefulPartitions()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"error finding dependent partitions: %w\", err)\n\t\t}\n\t\tfor _, part := range partitions {\n\t\t\tpart := part\n\t\t\tpproc, err := g.createPartitionProcessor(ctx, part, runModeRecoverOnly, func(msg *message, meta string) {\n\t\t\t\tpanic(\"a partition processor in recover-only-mode never commits a message\")\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"Error creating partition processor for recover-ahead %s/%d: %v\", g.Graph().Group(), part, err)\n\t\t\t}\n\t\t\terrg.Go(func() error {\n\t\t\t\tname := fmt.Sprintf(\"partition-processor-%d\", part)\n\t\t\t\tmWaitMap.Lock()\n\t\t\t\twaitMap[name] = struct{}{}\n\t\t\t\tmWaitMap.Unlock()\n\n\t\t\t\tdefer func() {\n\t\t\t\t\tmWaitMap.Lock()\n\t\t\t\t\tdefer mWaitMap.Unlock()\n\t\t\t\t\tdelete(waitMap, name)\n\t\t\t\t}()\n\n\t\t\t\tg.setPartProc(part, pproc)\n\t\t\t\tdefer g.setPartProc(part, nil)\n\n\t\t\t\terr := pproc.Start(ctx, ctx)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\treturn pproc.Stop()\n\t\t\t})\n\t\t}\n\t}\n\n\tvar (\n\t\tstart     = time.Now()\n\t\tlogTicker = time.NewTicker(1 * time.Minute)\n\t)\n\n\t// Now run through\n\tdefer logTicker.Stop()\n\terrgWaiter := errg.WaitChan()\n\tfor {\n\t\tselect {\n\t\t// the context has closed, no point in waiting\n\t\tcase <-ctx.Done():\n\t\t\tg.log.Debugf(\"Stopping to wait for views to get up, context closed\")\n\t\t\treturn fmt.Errorf(\"context closed while waiting for startup tables to become ready\")\n\n\t\t\t// the error group is done, which means\n\t\t\t// * err==nil --> it's done\n\t\t\t// * err!=nil --> it failed, let's return the error\n\t\tcase errs := <-errgWaiter:\n\t\t\terr := errs.ErrorOrNil()\n\t\t\tif err == nil {\n\t\t\t\tg.log.Debugf(\"View catchup finished\")\n\t\t\t}\n\t\t\treturn err\n\n\t\t// log the things we're still waiting for\n\t\tcase <-logTicker.C:\n\t\t\tvar tablesWaiting []string\n\t\t\tmWaitMap.Lock()\n\t\t\tfor table := range waitMap {\n\t\t\t\ttablesWaiting = append(tablesWaiting, table)\n\t\t\t}\n\t\t\tmWaitMap.Unlock()\n\n\t\t\tg.log.Printf(\"Waiting for [%s] to start up since %.2f minutes\",\n\t\t\t\tstrings.Join(tablesWaiting, \", \"),\n\t\t\t\ttime.Since(start).Minutes())\n\t\t}\n\t}\n}\n\n// find partitions that will any type of local state for this processor.\n// This includes joins and the group-table. If neither are present, it returns an empty list, because\n// it means that the processor is stateless and has only streaming-input.\n// It returns the list of partitions as an error, or empty if there are no such partitions.\n//\n// Supports to pass optional map of excluded partitions if the function is used to determine partitions\n// that are not part of the current assignment\nfunc (g *Processor) findStatefulPartitions() ([]int32, error) {\n\tvar (\n\t\terr           error\n\t\tallPartitions []int32\n\t)\n\tfor _, edge := range chainEdges(g.graph.groupTable, g.graph.inputTables) {\n\t\tallPartitions, err = g.tmgr.Partitions(edge.Topic())\n\t\tif err != nil && err != errTopicNotFound {\n\t\t\treturn nil, err\n\t\t}\n\t\tif len(allPartitions) > 0 {\n\t\t\tbreak\n\t\t}\n\t}\n\n\treturn allPartitions, err\n}\n\n// Recovered returns whether the processor is running, i.e. if the processor\n// has recovered all lookups/joins/tables and is running\nfunc (g *Processor) Recovered() bool {\n\treturn g.state.IsState(ProcStateRunning)\n}\n\n// StateReader returns a read only interface of the processors state.\nfunc (g *Processor) StateReader() StateReader {\n\treturn g.state\n}\n\nfunc (g *Processor) assignmentFromSession(session sarama.ConsumerGroupSession) (Assignment, error) {\n\tvar assignment Assignment\n\n\t// get the partitions this processor is assigned to.\n\t// We use that loop to verify copartitioning and fail otherwise\n\tfor _, claim := range session.Claims() {\n\t\t// for first claim, generate the assignment\n\t\tif assignment == nil {\n\t\t\tassignment = Assignment{}\n\t\t\tfor _, part := range claim {\n\t\t\t\tassignment[part] = sarama.OffsetNewest\n\t\t\t}\n\t\t} else {\n\t\t\t// for all others, verify the assignment is the same\n\n\t\t\t// first check length\n\t\t\tif len(claim) != len(assignment) {\n\t\t\t\treturn nil, fmt.Errorf(\"session claims are not copartitioned: %#v\", session.Claims())\n\t\t\t}\n\n\t\t\t// then check the partitions are exactly the same\n\t\t\tfor _, part := range claim {\n\t\t\t\tif _, exists := assignment[part]; !exists {\n\t\t\t\t\treturn nil, fmt.Errorf(\"session claims are not copartitioned: %#v\", session.Claims())\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn assignment, nil\n}\n\n// Setup is run at the beginning of a new session, before ConsumeClaim.\nfunc (g *Processor) Setup(session sarama.ConsumerGroupSession) error {\n\tg.state.SetState(ProcStateSetup)\n\tdefer g.state.SetState(ProcStateRunning)\n\tg.log.Printf(\"setup generation %d, claims=%#v\", session.GenerationID(), session.Claims())\n\tdefer g.log.Debugf(\"setup generation %d ... done\", session.GenerationID())\n\n\tif err := g.createAssignedPartitions(session); err != nil {\n\t\treturn err\n\t}\n\n\tif err := g.createHotStandbyPartitions(session); err != nil {\n\t\treturn err\n\t}\n\n\t// setup all processors\n\tsetupErrg, setupCtx := multierr.NewErrGroup(session.Context())\n\tg.mTables.RLock()\n\tfor partID, partition := range g.partitions {\n\t\tpproc := partition\n\t\tsetupErrg.Go(func() error {\n\t\t\t// the partition processors need two contexts:\n\t\t\t// setupCtx --> for this setup, which we'll wait for\n\t\t\t// the runner ctx, which is active during a session\n\t\t\terr := pproc.Start(setupCtx, session.Context())\n\t\t\tif err != nil {\n\t\t\t\treturn newErrSetup(partID, err)\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t}\n\tg.mTables.RUnlock()\n\n\treturn setupErrg.Wait().ErrorOrNil()\n}\n\nfunc (g *Processor) createAssignedPartitions(session sarama.ConsumerGroupSession) error {\n\tassignment, err := g.assignmentFromSession(session)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Error verifying assignment from session: %v\", err)\n\t}\n\n\tif g.rebalanceCallback != nil {\n\t\tg.rebalanceCallback(assignment)\n\t}\n\n\t// no partitions configured, just print a log but continue\n\t// in case we have configured standby, we should still start the standby-processors\n\tif len(assignment) == 0 {\n\t\tg.log.Printf(\"No partitions assigned. Claims were: %#v. Will probably sleep this generation\", session.Claims())\n\t}\n\t// create partition views for all partitions\n\tfor partition := range assignment {\n\t\t// create partition processor for our partition\n\t\tpproc, err := g.createPartitionProcessor(session.Context(), partition, runModeActive,\n\t\t\tcreateMessageCommitter(session),\n\t\t)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"Error creating partition processor for %s/%d: %v\", g.Graph().Group(), partition, err)\n\t\t}\n\t\tg.setPartProc(partition, pproc)\n\t}\n\treturn nil\n}\n\n// if hot standby is configured, we find the partitions that are missing\n// from the current assignment, and create those processors in standby mode\nfunc (g *Processor) createHotStandbyPartitions(session sarama.ConsumerGroupSession) error {\n\tif !g.opts.hotStandby {\n\t\treturn nil\n\t}\n\tallPartitions, err := g.findStatefulPartitions()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Error finding partitions for standby\")\n\t}\n\tfor _, standby := range allPartitions {\n\t\t// if the partition already exists, it means it is part of the assignment, so we don't\n\t\t// need to keep it on hot-standby and can ignore it.\n\n\t\t// we check if the partition processor exists and if it does, ignore it\n\t\t// since it's part of the assignment\n\t\tif _, exists := g.getPartProc(standby); exists {\n\t\t\tcontinue\n\t\t}\n\n\t\t// otherwise, let's start the partition processor in passive mode\n\t\tpproc, err := g.createPartitionProcessor(session.Context(), standby, runModePassive,\n\t\t\tfunc(msg *message, metadata string) {\n\t\t\t\tpanic(\"a passive partition processor should never receive input messages, thus never commit any messages\")\n\t\t\t})\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"Error creating partition processor for %s/%d: %v\", g.Graph().Group(), standby, err)\n\t\t}\n\t\tg.setPartProc(standby, pproc)\n\t}\n\treturn nil\n}\n\n// Cleanup is run at the end of a session, once all ConsumeClaim goroutines have exited\n// but before the offsets are committed for the very last time.\nfunc (g *Processor) Cleanup(session sarama.ConsumerGroupSession) error {\n\tg.log.Debugf(\"Cleaning up for %d\", session.GenerationID())\n\tdefer g.log.Debugf(\"Cleaning up for %d ... done\", session.GenerationID())\n\n\tg.state.SetState(ProcStateStopping)\n\tdefer g.state.SetState(ProcStateStopped)\n\terrg, _ := multierr.NewErrGroup(session.Context())\n\tg.mTables.RLock()\n\tfor part, partition := range g.partitions {\n\t\tpartID, pproc := part, partition\n\t\terrg.Go(func() error {\n\t\t\terr := pproc.Stop()\n\t\t\tif err != nil {\n\t\t\t\tg.log.Printf(\"Error running/stopping partition processor %d: %v\", partID, err)\n\t\t\t\treturn err\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t}\n\tg.mTables.RUnlock()\n\terr := errg.Wait().ErrorOrNil()\n\n\t// reset the partitions after the session ends\n\tg.mTables.Lock()\n\tdefer g.mTables.Unlock()\n\tg.partitions = make(map[int32]*PartitionProcessor)\n\treturn err\n}\n\n// WaitForReady waits until the processor is ready to consume messages\n// (or is actually consuming messages)\n// i.e., it is done catching up all partition tables, joins and lookup tables\nfunc (g *Processor) WaitForReady() error {\n\treturn g.waitForReady(context.Background())\n}\n\n// WaitForReadyContext is context aware option of WaitForReady.\n// It either waits until the processor is ready or until context is canceled.\n// If the return was caused by context it will return context reported error.\nfunc (g *Processor) WaitForReadyContext(ctx context.Context) error {\n\treturn g.waitForReady(ctx)\n}\n\nfunc (g *Processor) waitForReady(ctx context.Context) error {\n\t// wait for the processor to be started (or stopped)\n\tselect {\n\tcase <-g.state.WaitForStateMin(ProcStateStarting):\n\tcase <-g.done:\n\t\treturn nil\n\tcase <-ctx.Done():\n\t\treturn ctx.Err()\n\t}\n\t// wait that the processor is actually running\n\tselect {\n\tcase <-g.state.WaitForState(ProcStateRunning):\n\tcase <-g.done:\n\tcase <-ctx.Done():\n\t\treturn ctx.Err()\n\t}\n\n\t// wait for all PartitionProcessors to be running\n\n\t// copy them first with the mutex so we don't run into a deadlock\n\n\tg.mTables.RLock()\n\tparts := make([]*PartitionProcessor, 0, len(g.partitions))\n\tfor _, part := range g.partitions {\n\t\tparts = append(parts, part)\n\t}\n\tg.mTables.RUnlock()\n\n\tfor _, part := range parts {\n\t\tselect {\n\t\tcase <-part.state.WaitForState(PPStateRunning):\n\t\tcase <-g.done:\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// ConsumeClaim must start a consumer loop of ConsumerGroupClaim's Messages().\n// Once the Messages() channel is closed, the Handler must finish its processing\n// loop and exit.\nfunc (g *Processor) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {\n\tg.log.Debugf(\"ConsumeClaim for topic/partition %s/%d, initialOffset=%d\", claim.Topic(), claim.Partition(), claim.InitialOffset())\n\tdefer g.log.Debugf(\"ConsumeClaim done for topic/partition %s/%d\", claim.Topic(), claim.Partition())\n\tpart, has := g.getPartProc(claim.Partition())\n\tif !has {\n\t\treturn fmt.Errorf(\"no partition (%d) to handle input in topic %s\", claim.Partition(), claim.Topic())\n\t}\n\n\tmessages := claim.Messages()\n\tstopping, doneWaitingForStop := part.stopping()\n\tdefer doneWaitingForStop()\n\n\tfor {\n\t\tselect {\n\t\tcase msg, ok := <-messages:\n\t\t\tif !ok {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\tselect {\n\t\t\tcase part.input <- &message{\n\t\t\t\tkey:       string(msg.Key),\n\t\t\t\ttopic:     msg.Topic,\n\t\t\t\toffset:    msg.Offset,\n\t\t\t\tpartition: msg.Partition,\n\t\t\t\ttimestamp: msg.Timestamp,\n\t\t\t\theaders:   msg.Headers,\n\t\t\t\tvalue:     msg.Value,\n\t\t\t}:\n\t\t\tcase <-stopping:\n\t\t\t\treturn nil\n\t\t\tcase <-session.Context().Done():\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\tcase <-stopping:\n\t\t\treturn nil\n\t\tcase <-session.Context().Done():\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\n// Stats returns the aggregated stats for the processor including all partitions, tables, lookups and joins\nfunc (g *Processor) Stats() *ProcessorStats {\n\treturn g.StatsWithContext(context.Background())\n}\n\n// StatsWithContext returns stats for the processor, see #Processor.Stats()\nfunc (g *Processor) StatsWithContext(ctx context.Context) *ProcessorStats {\n\tvar (\n\t\tm     sync.Mutex\n\t\tstats *ProcessorStats\n\t)\n\n\terrg, ctx := multierr.NewErrGroup(ctx)\n\n\t// get partition-processor stats\n\n\tg.mTables.RLock()\n\tstats = newProcessorStats(len(g.partitions))\n\n\tfor partID, proc := range g.partitions {\n\t\tpartID, proc := partID, proc\n\t\terrg.Go(func() error {\n\t\t\tpartStats := proc.fetchStats(ctx)\n\n\t\t\tm.Lock()\n\t\t\tdefer m.Unlock()\n\n\t\t\tif partStats != nil {\n\t\t\t\tstats.Group[partID] = partStats\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t}\n\n\t// get the lookup table stats\n\tfor topic, view := range g.lookupTables {\n\t\ttopic, view := topic, view\n\t\terrg.Go(func() error {\n\t\t\tm.Lock()\n\t\t\tdefer m.Unlock()\n\t\t\tviewStats := view.Stats(ctx)\n\n\t\t\tif viewStats != nil {\n\t\t\t\tstats.Lookup[topic] = viewStats\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t}\n\tg.mTables.RUnlock()\n\n\terr := errg.Wait().ErrorOrNil()\n\tif err != nil {\n\t\tg.log.Printf(\"Error retrieving stats: %v\", err)\n\t}\n\treturn stats\n}\n\n// creates the partition that is responsible for the group processor's table\nfunc (g *Processor) createPartitionProcessor(ctx context.Context, partition int32, runMode PPRunMode, commit commitCallback) (*PartitionProcessor, error) {\n\tg.log.Debugf(\"Creating partition processor for partition %d\", partition)\n\tif _, has := g.getPartProc(partition); has {\n\t\treturn nil, fmt.Errorf(\"processor [%s]: partition %d already exists\", g.graph.Group(), partition)\n\t}\n\n\tbackoff, err := g.opts.builders.backoff()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"processor [%s]: could not build backoff handler: %v\", g.graph.Group(), err)\n\t}\n\treturn newPartitionProcessor(partition,\n\t\tg.graph,\n\t\tcommit,\n\t\tg.log,\n\t\tg.opts,\n\t\trunMode,\n\t\tg.lookupTables,\n\t\tg.saramaConsumer,\n\t\tg.producer,\n\t\tg.tmgr,\n\t\tbackoff,\n\t\tg.opts.backoffResetTime), nil\n}\n\n// Stop stops the processor.\n// This is semantically equivalent of closing the Context\n// that was passed to Processor.Run(..).\n// This method will return immediately, errors during running\n// will be returned from Processor.Run(..)\nfunc (g *Processor) Stop() {\n\tg.cancel()\n}\n\n// Done returns a channel that is closed when the processor is stopped.\nfunc (g *Processor) Done() <-chan struct{} {\n\treturn g.done\n}\n\n// Error returns the error that caused the processor to stop.\nfunc (g *Processor) Error() error {\n\tg.errMux.Lock()\n\tdefer g.errMux.Unlock()\n\treturn g.err\n}\n\n// VisitAllWithStats visits all keys in parallel by passing the visit request\n// to all partitions.\n// The optional argument \"meta\" will be forwarded to the visit-function of each key of the table.\n// The function returns when\n// * all values have been visited or\n// * the context is cancelled or\n// * the processor rebalances/shuts down\n// Return parameters:\n// * number of visited items\n// * error\nfunc (g *Processor) VisitAllWithStats(ctx context.Context, name string, meta interface{}) (int64, error) {\n\tg.mTables.RLock()\n\n\tif g.isStateless() {\n\t\treturn 0, fmt.Errorf(\"cannot visit values in stateless processor\")\n\t}\n\n\terrg, visitCtx := multierr.NewErrGroup(ctx)\n\tvar visited int64\n\tfor _, part := range g.partitions {\n\t\t// we'll only do the visit for active mode partitions\n\t\t// because visit essentially provides write access, which passive partitions don't have\n\t\tif part.runMode != runModeActive {\n\t\t\tcontinue\n\t\t}\n\t\tpart := part\n\t\terrg.Go(func() error {\n\t\t\treturn part.VisitValues(visitCtx, name, meta, &visited)\n\t\t})\n\t}\n\n\tg.mTables.RUnlock()\n\n\t// wait for the visitors\n\terr := errg.Wait().ErrorOrNil()\n\treturn visited, err\n}\n\n// VisitAll visits all values from the processor table.\nfunc (g *Processor) VisitAll(ctx context.Context, name string, meta interface{}) error {\n\t_, err := g.VisitAllWithStats(ctx, name, meta)\n\treturn err\n}\n\nfunc prepareTopics(brokers []string, gg *GroupGraph, opts *poptions) (npar int, err error) {\n\t// create topic manager\n\ttm, err := opts.builders.topicmgr(brokers)\n\tif err != nil {\n\t\treturn 0, fmt.Errorf(\"Error creating topic manager: %v\", err)\n\t}\n\tdefer func() {\n\t\te := tm.Close()\n\t\tif e != nil && err == nil {\n\t\t\terr = fmt.Errorf(\"Error closing topic manager: %v\", e)\n\t\t}\n\t}()\n\n\t// check co-partitioned (external) topics have the same number of partitions\n\tnpar, err = ensureCopartitioned(tm, gg.copartitioned().Topics())\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tif ls := gg.LoopStream(); ls != nil {\n\t\tif err = tm.EnsureStreamExists(ls.Topic(), npar); err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t}\n\n\tif gt := gg.GroupTable(); gt != nil {\n\t\tif err = tm.EnsureTableExists(gt.Topic(), npar); err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t}\n\n\treturn\n}\n\n// returns the number of partitions the topics have, and an error if topics are\n// not copartitionea.\nfunc ensureCopartitioned(tm TopicManager, topics []string) (int, error) {\n\tvar npar int\n\tfor _, topic := range topics {\n\t\tpartitions, err := tm.Partitions(topic)\n\t\tif err != nil {\n\t\t\treturn 0, fmt.Errorf(\"Error fetching partitions for topic %s: %v\", topic, err)\n\t\t}\n\n\t\t// check assumption that partitions are gap-less\n\t\tfor i, p := range partitions {\n\t\t\tif i != int(p) {\n\t\t\t\treturn 0, fmt.Errorf(\"Topic %s has partition gap: %v\", topic, partitions)\n\t\t\t}\n\t\t}\n\n\t\tif npar == 0 {\n\t\t\tnpar = len(partitions)\n\t\t}\n\t\tif len(partitions) != npar {\n\t\t\treturn 0, fmt.Errorf(\"topics are not copartitioned! Topic '%s' does not have %d partitions like the rest but %d\", topic, npar, len(partitions))\n\t\t}\n\t}\n\treturn npar, nil\n}\n\n// createMessageCommitter returns a commitCallback that allows to commit a message into the passed\n// sarama.ConsumerGroupSession.\n//\n// Note that the offset to be committed must be the offset that the consumer expects to consume next, not the offset of the message.\n// See documentation at https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html which says:\n//\n// Note: The committed offset should always be the offset of the next message that your application will read. Thus, when calling commitSync(offsets) you should add one to the offset of the last message processed.\n//\n// This has the same semantics as sarama's implementation of session.MarkMessage (which calls MarkOffset with offset+1)\nfunc createMessageCommitter(session sarama.ConsumerGroupSession) commitCallback {\n\treturn func(msg *message, metadata string) {\n\t\tsession.MarkOffset(msg.topic, msg.partition, msg.offset+1, metadata)\n\t}\n}\n"
        },
        {
          "name": "processor_test.go",
          "type": "blob",
          "size": 12.3359375,
          "content": "package goka\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/IBM/sarama\"\n\t\"github.com/golang/mock/gomock\"\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/lovoo/goka/codec\"\n\t\"github.com/lovoo/goka/storage\"\n)\n\nfunc createMockBuilder(t *testing.T) (*gomock.Controller, *builderMock) {\n\tctrl := NewMockController(t)\n\tbm := newBuilderMock(ctrl)\n\tbm.st = storage.NewMemory()\n\treturn ctrl, bm\n}\n\nfunc createTestConsumerGroupBuilder(t *testing.T) (ConsumerGroupBuilder, *MockConsumerGroup) {\n\tmock := NewMockConsumerGroup(t)\n\treturn func(brokers []string, group, clientID string) (sarama.ConsumerGroup, error) {\n\t\treturn mock, nil\n\t}, mock\n}\n\nfunc createTestConsumerBuilder(t *testing.T) (SaramaConsumerBuilder, *MockAutoConsumer) {\n\tcons := NewMockAutoConsumer(t, nil)\n\n\treturn func(brokers []string, clientID string) (sarama.Consumer, error) {\n\t\treturn cons, nil\n\t}, cons\n}\n\nfunc expectCGEmit(bm *builderMock, table string, msgs []*sarama.ConsumerMessage) {\n\tfor _, msg := range msgs {\n\t\tbm.producer.EXPECT().EmitWithHeaders(table, string(msg.Key), msg.Value, nil).Return(NewPromise().finish(nil, nil))\n\t}\n}\n\nfunc expectCGLoop(bm *builderMock, loop string, msgs []*sarama.ConsumerMessage) {\n\tbm.tmgr.EXPECT().EnsureStreamExists(loop, 1).AnyTimes()\n\tfor _, msg := range msgs {\n\t\tbm.producer.EXPECT().EmitWithHeaders(loop, string(msg.Key), gomock.Any(), nil).Return(NewPromise().finish(nil, nil))\n\t}\n}\n\nfunc expectCGConsume(bm *builderMock, table string, msgs []*sarama.ConsumerMessage) {\n\tvar current int64\n\n\tbm.producer.EXPECT().Close().Return(nil).AnyTimes()\n\n\tbm.tmgr.EXPECT().Close().Return(nil).AnyTimes()\n\tbm.tmgr.EXPECT().EnsureTableExists(table, gomock.Any()).Return(nil)\n\tbm.tmgr.EXPECT().Partitions(gomock.Any()).Return([]int32{0}, nil).AnyTimes()\n\tbm.tmgr.EXPECT().GetOffset(table, gomock.Any(), sarama.OffsetNewest).Return(func() int64 {\n\t\tdefer func() {\n\t\t\tcurrent++\n\t\t}()\n\t\treturn current\n\t}(), nil)\n\tbm.tmgr.EXPECT().GetOffset(table, gomock.Any(), sarama.OffsetOldest).Return(func() int64 {\n\t\treturn 0\n\t}(), nil)\n}\n\n// accumulate is a callback that increments the\n// table value by the incoming message.\n// Persist and incoming codecs must be codec.Int64\nfunc accumulate(ctx Context, msg interface{}) {\n\tinc := msg.(int64)\n\tval := ctx.Value()\n\tif val == nil {\n\t\tctx.SetValue(inc)\n\t} else {\n\t\tctx.SetValue(val.(int64) + inc)\n\t}\n}\n\nfunc TestProcessor_Run(t *testing.T) {\n\tt.Run(\"input-persist\", func(t *testing.T) {\n\t\tctrl, bm := createMockBuilder(t)\n\t\tdefer ctrl.Finish()\n\t\tvar (\n\t\t\ttopic  = \"test-table\"\n\t\t\ttoEmit = []*sarama.ConsumerMessage{\n\t\t\t\t{\n\t\t\t\t\tTopic: \"input\",\n\t\t\t\t\tValue: []byte(strconv.FormatInt(3, 10)),\n\t\t\t\t\tKey:   []byte(\"test-key-1\"),\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tTopic: \"input\",\n\t\t\t\t\tValue: []byte(strconv.FormatInt(3, 10)),\n\t\t\t\t\tKey:   []byte(\"test-key-2\"),\n\t\t\t\t},\n\t\t\t}\n\t\t)\n\n\t\texpectCGConsume(bm, topic, toEmit)\n\t\texpectCGEmit(bm, topic, toEmit)\n\n\t\tgroupBuilder, cg := createTestConsumerGroupBuilder(t)\n\t\tconsBuilder, _ := createTestConsumerBuilder(t)\n\n\t\tgraph := DefineGroup(\"test\",\n\t\t\tInput(\"input\", new(codec.Int64), accumulate),\n\t\t\tPersist(new(codec.Int64)),\n\t\t)\n\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second*1000)\n\t\tdefer cancel()\n\n\t\tnewProc, err := NewProcessor([]string{\"localhost:9092\"}, graph,\n\t\t\tbm.createProcessorOptions(consBuilder, groupBuilder)...,\n\t\t)\n\t\trequire.NoError(t, err)\n\n\t\tgo func() {\n\t\t\tnewProc.Run(ctx)\n\t\t}()\n\n\t\tnewProc.WaitForReady()\n\n\t\t// if there was an error during startup, no point in sending messages\n\t\t// and waiting for them to be delivered\n\t\tselect {\n\t\tcase <-newProc.Done():\n\t\t\trequire.NoError(t, newProc.Error())\n\t\tdefault:\n\t\t}\n\n\t\tfor _, msg := range toEmit {\n\t\t\tcg.SendMessageWait(msg)\n\t\t}\n\n\t\tval, err := newProc.Get(\"test-key-1\")\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(3), val.(int64))\n\n\t\tval, err = newProc.Get(\"test-key-2\")\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, int64(3), val.(int64))\n\n\t\t// shutdown\n\t\tnewProc.Stop()\n\t\t<-newProc.Done()\n\t\trequire.NoError(t, newProc.Error())\n\t})\n\tt.Run(\"loopback\", func(t *testing.T) {\n\t\tctrl, bm := createMockBuilder(t)\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\ttopic  = \"test-table\"\n\t\t\tloop   = \"test-loop\"\n\t\t\ttoEmit = []*sarama.ConsumerMessage{\n\t\t\t\t{\n\t\t\t\t\tTopic: \"input\",\n\t\t\t\t\tValue: []byte(strconv.FormatInt(23, 10)),\n\t\t\t\t\tKey:   []byte(\"test-key\"),\n\t\t\t\t},\n\t\t\t}\n\t\t)\n\n\t\texpectCGConsume(bm, topic, toEmit)\n\t\texpectCGLoop(bm, loop, toEmit)\n\n\t\tgroupBuilder, cg := createTestConsumerGroupBuilder(t)\n\t\tconsBuilder, _ := createTestConsumerBuilder(t)\n\n\t\tgraph := DefineGroup(\"test\",\n\t\t\t// input passes to loopback\n\t\t\tInput(\"input\", new(codec.Int64), func(ctx Context, msg interface{}) {\n\t\t\t\tctx.Loopback(ctx.Key(), msg)\n\t\t\t}),\n\t\t\t// this will not be called in the test but we define it, otherwise the context will raise an error\n\t\t\tLoop(new(codec.Int64), accumulate),\n\t\t\tPersist(new(codec.Int64)),\n\t\t)\n\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\n\t\tnewProc, err := NewProcessor([]string{\"localhost:9092\"}, graph,\n\t\t\tbm.createProcessorOptions(consBuilder, groupBuilder)...,\n\t\t)\n\t\trequire.NoError(t, err)\n\n\t\tgo func() {\n\t\t\tnewProc.Run(ctx)\n\t\t}()\n\n\t\tnewProc.WaitForReady()\n\n\t\t// if there was an error during startup, no point in sending messages\n\t\t// and waiting for them to be delivered\n\t\tselect {\n\t\tcase <-newProc.Done():\n\t\t\trequire.NoError(t, newProc.Error())\n\t\tdefault:\n\t\t}\n\n\t\tfor _, msg := range toEmit {\n\t\t\tcg.SendMessageWait(msg)\n\t\t}\n\n\t\t// shutdown\n\t\tnewProc.Stop()\n\t\t<-newProc.Done()\n\t\trequire.NoError(t, newProc.Error())\n\t})\n\tt.Run(\"consume-error\", func(t *testing.T) {\n\t\tctrl, bm := createMockBuilder(t)\n\t\tdefer ctrl.Finish()\n\n\t\tbm.tmgr.EXPECT().Close().Times(1)\n\t\tbm.tmgr.EXPECT().Partitions(gomock.Any()).Return([]int32{0}, nil).Times(1)\n\t\tbm.producer.EXPECT().Close().Times(1)\n\n\t\tgroupBuilder, cg := createTestConsumerGroupBuilder(t)\n\t\tconsBuilder, _ := createTestConsumerBuilder(t)\n\n\t\tgraph := DefineGroup(\"test\",\n\t\t\t// not really used, we're failing anyway\n\t\t\tInput(\"input\", new(codec.Int64), accumulate),\n\t\t)\n\n\t\tctx, cancel := context.WithCancel(context.Background())\n\t\tdefer cancel()\n\n\t\tnewProc, err := NewProcessor([]string{\"localhost:9092\"}, graph,\n\t\t\tbm.createProcessorOptions(consBuilder, groupBuilder)...,\n\t\t)\n\t\trequire.NoError(t, err)\n\n\t\tgo func() {\n\t\t\tnewProc.Run(ctx)\n\t\t}()\n\n\t\tnewProc.WaitForReady()\n\n\t\t// if there was an error during startup, no point in sending messages\n\t\t// and waiting for them to be delivered\n\t\tselect {\n\t\tcase <-newProc.Done():\n\t\t\trequire.NoError(t, newProc.Error())\n\t\tdefault:\n\t\t}\n\t\tcg.SendError(fmt.Errorf(\"test-error\"))\n\t\tcancel()\n\t\t<-newProc.Done()\n\t\t// the errors sent back by the consumergroup do not lead to a failure of the processor\n\t\trequire.NoError(t, newProc.Error())\n\t})\n\tt.Run(\"setup-error\", func(t *testing.T) {\n\t\tctrl, bm := createMockBuilder(t)\n\t\tdefer ctrl.Finish()\n\n\t\tbm.tmgr.EXPECT().Close().Times(1)\n\t\tbm.tmgr.EXPECT().Partitions(gomock.Any()).Return([]int32{0}, nil).Times(1)\n\t\tbm.producer.EXPECT().Close().Times(1)\n\n\t\tgroupBuilder, cg := createTestConsumerGroupBuilder(t)\n\t\tconsBuilder, _ := createTestConsumerBuilder(t)\n\n\t\tgraph := DefineGroup(\"test\",\n\t\t\t// not really used, we're failing anyway\n\t\t\tInput(\"input\", new(codec.Int64), accumulate),\n\t\t)\n\n\t\tctx, cancel := context.WithCancel(context.Background())\n\t\tdefer cancel()\n\n\t\tnewProc, err := NewProcessor([]string{\"localhost:9092\"}, graph,\n\t\t\tbm.createProcessorOptions(consBuilder, groupBuilder)...,\n\t\t)\n\t\trequire.NoError(t, err)\n\n\t\tcg.FailOnConsume(newErrSetup(123, fmt.Errorf(\"setup-error\")))\n\n\t\tgo func() {\n\t\t\tnewProc.Run(ctx)\n\t\t}()\n\n\t\tnewProc.WaitForReady()\n\n\t\t// if there was an error during startup, no point in sending messages\n\t\t// and waiting for them to be delivered\n\t\t<-newProc.Done()\n\t\trequire.True(t, strings.Contains(newProc.Error().Error(), \"setup-error\"))\n\t})\n\tt.Run(\"returns-error-on-failure\", func(t *testing.T) {\n\t\tctrl, bm := createMockBuilder(t)\n\t\tdefer ctrl.Finish()\n\n\t\tbm.tmgr.EXPECT().Close().Times(1)\n\t\tbm.tmgr.EXPECT().Partitions(gomock.Any()).Return([]int32{0}, nil).Times(1)\n\t\tbm.producer.EXPECT().Close().Times(1)\n\n\t\tgroupBuilder, cg := createTestConsumerGroupBuilder(t)\n\t\tconsBuilder, _ := createTestConsumerBuilder(t)\n\n\t\tgraph := DefineGroup(\"test\",\n\t\t\t// not really used, we're failing anyway\n\t\t\tInput(\"input\", new(codec.Int64), accumulate),\n\t\t)\n\n\t\tctx, cancel := context.WithCancel(context.Background())\n\t\tdefer cancel()\n\n\t\tnewProc, err := NewProcessor([]string{\"localhost:9092\"}, graph,\n\t\t\tbm.createProcessorOptions(consBuilder, groupBuilder)...,\n\t\t)\n\t\trequire.NoError(t, err)\n\n\t\tcg.FailOnConsume(newErrSetup(123, fmt.Errorf(\"setup-error\")))\n\n\t\tvar (\n\t\t\tprocErr error\n\t\t\tdone    = make(chan struct{})\n\t\t)\n\n\t\tgo func() {\n\t\t\tdefer close(done)\n\t\t\tprocErr = newProc.Run(ctx)\n\t\t}()\n\n\t\tnewProc.WaitForReady()\n\n\t\t// if there was an error during startup, no point in sending messages\n\t\t// and waiting for them to be delivered\n\t\t<-done\n\t\trequire.True(t, strings.Contains(procErr.Error(), \"setup-error\"))\n\t})\n\tt.Run(\"returns-nil-on-success\", func(t *testing.T) {\n\t\tctrl, bm := createMockBuilder(t)\n\t\tdefer ctrl.Finish()\n\n\t\tbm.tmgr.EXPECT().Close().Times(1)\n\t\tbm.tmgr.EXPECT().Partitions(gomock.Any()).Return([]int32{0}, nil).Times(1)\n\t\tbm.producer.EXPECT().Close().Times(1)\n\n\t\tgroupBuilder, _ := createTestConsumerGroupBuilder(t)\n\t\tconsBuilder, _ := createTestConsumerBuilder(t)\n\n\t\tgraph := DefineGroup(\"test\",\n\t\t\t// not really used, we're stopping before the processor before producing\n\t\t\tInput(\"input\", new(codec.Int64), accumulate),\n\t\t)\n\n\t\tctx, cancel := context.WithCancel(context.Background())\n\t\tdefer cancel()\n\n\t\tnewProc, err := NewProcessor([]string{\"localhost:9092\"}, graph,\n\t\t\tbm.createProcessorOptions(consBuilder, groupBuilder)...,\n\t\t)\n\t\trequire.NoError(t, err)\n\n\t\tvar (\n\t\t\tprocErr error\n\t\t\tdone    = make(chan struct{})\n\t\t)\n\n\t\tgo func() {\n\t\t\tdefer close(done)\n\t\t\tprocErr = newProc.Run(ctx)\n\t\t}()\n\n\t\tnewProc.WaitForReady()\n\t\tnewProc.Stop()\n\n\t\t<-done\n\t\trequire.NoError(t, procErr)\n\t})\n}\n\nfunc TestProcessor_StateReader(t *testing.T) {\n\tstate := NewSignal(ProcStateSetup, ProcStateRunning)\n\tstate.SetState(ProcStateRunning)\n\tp := Processor{state: state}\n\n\trequire.Equal(t, ProcStateRunning, p.StateReader().State())\n}\n\nfunc TestProcessor_Stop(t *testing.T) {\n\tt.Run(\"expected-state\", func(t *testing.T) {\n\t\tctrl, bm := createMockBuilder(t)\n\t\tdefer ctrl.Finish()\n\n\t\tbm.tmgr.EXPECT().Close().Times(1)\n\t\tbm.tmgr.EXPECT().Partitions(gomock.Any()).Return([]int32{0}, nil).Times(1)\n\t\tbm.producer.EXPECT().Close().Times(1)\n\n\t\tgroupBuilder, _ := createTestConsumerGroupBuilder(t)\n\t\tconsBuilder, _ := createTestConsumerBuilder(t)\n\n\t\tgraph := DefineGroup(\"test\",\n\t\t\tInput(\"input\", new(codec.Int64), func(ctx Context, msg interface{}) {\n\t\t\t\t// Do nothing\n\t\t\t}),\n\t\t)\n\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second*1000)\n\t\tdefer cancel()\n\n\t\tnewProc, err := NewProcessor([]string{\"localhost:9092\"}, graph,\n\t\t\tbm.createProcessorOptions(consBuilder, groupBuilder)...,\n\t\t)\n\t\trequire.NoError(t, err)\n\n\t\tgo func() {\n\t\t\tnewProc.Run(ctx)\n\t\t}()\n\n\t\tnewProc.WaitForReady()\n\n\t\t// if there was an error during startup, no point in continuing\n\t\t// and waiting for the processor to be stopped\n\t\tselect {\n\t\tcase <-newProc.Done():\n\t\t\trequire.NoError(t, newProc.Error())\n\t\tdefault:\n\t\t}\n\n\t\trequire.Equal(t, ProcStateRunning, newProc.StateReader().State())\n\n\t\t// shutdown\n\t\tnewProc.Stop()\n\t\t<-newProc.Done()\n\n\t\tselect {\n\t\tcase <-newProc.Done():\n\t\t\trequire.Equal(t, ProcStateStopped, newProc.StateReader().State())\n\t\t\trequire.NoError(t, newProc.Error())\n\t\tcase <-time.After(10 * time.Second):\n\t\t\tt.Errorf(\"processor did not shut down as expected\")\n\t\t}\n\t})\n\n\tt.Run(\"done-closes\", func(t *testing.T) {\n\t\tctrl, bm := createMockBuilder(t)\n\t\tdefer ctrl.Finish()\n\n\t\tbm.tmgr.EXPECT().Close().Times(1)\n\t\tbm.tmgr.EXPECT().Partitions(gomock.Any()).Return([]int32{0}, nil).Times(1)\n\t\tbm.producer.EXPECT().Close().Times(1)\n\n\t\tgroupBuilder, _ := createTestConsumerGroupBuilder(t)\n\t\tconsBuilder, _ := createTestConsumerBuilder(t)\n\n\t\tgraph := DefineGroup(\"test\",\n\t\t\tInput(\"input\", new(codec.Int64), func(ctx Context, msg interface{}) {\n\t\t\t\t// Do nothing\n\t\t\t}),\n\t\t)\n\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second*1000)\n\t\tdefer cancel()\n\n\t\tnewProc, err := NewProcessor([]string{\"localhost:9092\"}, graph,\n\t\t\tbm.createProcessorOptions(consBuilder, groupBuilder)...,\n\t\t)\n\t\trequire.NoError(t, err)\n\n\t\tgo func() {\n\t\t\tnewProc.Run(ctx)\n\t\t}()\n\n\t\tnewProc.WaitForReady()\n\n\t\t// shutdown\n\t\tnewProc.Stop()\n\n\t\tselect {\n\t\tcase <-newProc.Done():\n\t\t\trequire.NoError(t, newProc.Error())\n\t\tcase <-time.After(10 * time.Second):\n\t\t\tt.Errorf(\"processor did not shut down as expected\")\n\t\t}\n\t})\n}\n"
        },
        {
          "name": "producer.go",
          "type": "blob",
          "size": 2.65625,
          "content": "package goka\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/IBM/sarama\"\n)\n\n// Producer abstracts the kafka producer\ntype Producer interface {\n\t// Emit sends a message to topic.\n\tEmit(topic string, key string, value []byte) *Promise\n\tEmitWithHeaders(topic string, key string, value []byte, headers Headers) *Promise\n\tClose() error\n}\n\ntype producer struct {\n\tproducer sarama.AsyncProducer\n\twg       sync.WaitGroup\n}\n\n// NewProducer creates new kafka producer for passed brokers.\nfunc NewProducer(brokers []string, config *sarama.Config) (Producer, error) {\n\taprod, err := sarama.NewAsyncProducer(brokers, config)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Failed to start Sarama producer: %v\", err)\n\t}\n\n\tp := producer{\n\t\tproducer: aprod,\n\t}\n\n\tp.run()\n\n\treturn &p, nil\n}\n\n// Close stops the producer and waits for the Success/Error channels to drain.\n// Emitting to a closing/closed producer results in write-to-closed-channel panic\nfunc (p *producer) Close() error {\n\t// do an async close to get the rest of the success/error messages to avoid\n\t// leaving unfinished promises.\n\tp.producer.AsyncClose()\n\n\t// wait for the channels to drain\n\tdone := make(chan struct{})\n\tgo func() {\n\t\tp.wg.Wait()\n\t\tclose(done)\n\t}()\n\n\tselect {\n\tcase <-done:\n\tcase <-time.NewTimer(60 * time.Second).C:\n\t}\n\n\treturn nil\n}\n\n// Emit emits a key-value pair to topic and returns a Promise that\n// can be checked for errors asynchronously\nfunc (p *producer) Emit(topic string, key string, value []byte) *Promise {\n\tpromise := NewPromise()\n\n\tp.producer.Input() <- &sarama.ProducerMessage{\n\t\tTopic:    topic,\n\t\tKey:      sarama.StringEncoder(key),\n\t\tValue:    sarama.ByteEncoder(value),\n\t\tMetadata: promise,\n\t}\n\treturn promise\n}\n\n// EmitWithHeaders emits a key-value pair with headers to topic and returns a Promise that\n// can be checked for errors asynchronously\nfunc (p *producer) EmitWithHeaders(topic string, key string, value []byte, headers Headers) *Promise {\n\tpromise := NewPromise()\n\n\tp.producer.Input() <- &sarama.ProducerMessage{\n\t\tTopic:    topic,\n\t\tKey:      sarama.StringEncoder(key),\n\t\tValue:    sarama.ByteEncoder(value),\n\t\tMetadata: promise,\n\t\tHeaders:  headers.ToSarama(),\n\t}\n\treturn promise\n}\n\n// resolve or reject a promise in the message's metadata on Success or Error\nfunc (p *producer) run() {\n\tp.wg.Add(2)\n\tgo func() {\n\t\tdefer p.wg.Done()\n\t\tfor {\n\t\t\terr, ok := <-p.producer.Errors()\n\n\t\t\t// channel closed, the producer is stopping\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t\terr.Msg.Metadata.(*Promise).finish(nil, err)\n\t\t}\n\t}()\n\n\tgo func() {\n\t\tdefer p.wg.Done()\n\t\tfor {\n\t\t\tmsg, ok := <-p.producer.Successes()\n\t\t\t// channel closed, the producer is stopping\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tmsg.Metadata.(*Promise).finish(msg, nil)\n\t\t}\n\t}()\n}\n"
        },
        {
          "name": "producer_test.go",
          "type": "blob",
          "size": 1.5205078125,
          "content": "// +build goka\n\npackage goka\n\nimport (\n\t\"log\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n)\n\n// This tests how a producer behaves when being shutdown to make sure,\n// no promises stay unfinished.\n// To run the test, get a local kafka-container running (e.g. go to\n// examples-directory and do `make restart`), then run the tests with\n// `go test -v github.com/lovoo/goka/kafka/ -tags=kafka`\nfunc TestProducerError(t *testing.T) {\n\tcfg := DefaultConfig()\n\tp, err := NewProducer([]string{\"localhost:9092\"}, cfg)\n\n\tif err != nil {\n\t\tt.Fatalf(\"error creating producer: %v\", err)\n\t}\n\n\tvar (\n\t\tpromises     []*Promise\n\t\tdonePromises int64\n\t\temitted      int64\n\t\tdone         = make(chan bool)\n\t)\n\n\tgo func() {\n\t\tdefer func() {\n\t\t\trecover()\n\t\t}()\n\t\tdefer close(done)\n\t\tfor {\n\t\t\tpromise := p.Emit(\"test\", \"test\", []byte{})\n\t\t\tpromise.Then(func(err error) {\n\t\t\t\tatomic.AddInt64(&donePromises, 1)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Printf(\"error producing message: %v\", err)\n\t\t\t\t}\n\t\t\t})\n\t\t\tpromises = append(promises, promise)\n\t\t\temitted++\n\t\t\ttime.Sleep(20 * time.Millisecond)\n\t\t}\n\t}()\n\n\t// let it run for 1 second\n\ttime.Sleep(1000 * time.Millisecond)\n\n\t// close the producer\n\terr = p.Close()\n\tif err != nil {\n\t\tlog.Printf(\"Error closing producer: %v\", err)\n\t}\n\t// wait for the promises to be\n\t<-done\n\n\tif len(promises) != int(emitted) {\n\t\tt.Errorf(\"Promises/Emits do not match: promises: %d, emitted. %d\", len(promises), emitted)\n\t}\n\tif len(promises) != int(donePromises) {\n\t\tt.Errorf(\"Promises/Done promises do not match: promises: %d, done. %d\", len(promises), donePromises)\n\t}\n}\n"
        },
        {
          "name": "promise.go",
          "type": "blob",
          "size": 1.8134765625,
          "content": "package goka\n\nimport (\n\t\"sync\"\n\n\t\"github.com/IBM/sarama\"\n)\n\n// Promise as in https://en.wikipedia.org/wiki/Futures_and_promises\ntype Promise struct {\n\tsync.Mutex\n\terr      error\n\tmsg      *sarama.ProducerMessage\n\tfinished bool\n\n\tcallbacks []func(msg *sarama.ProducerMessage, err error)\n}\n\n// PromiseFinisher finishes a promise\ntype PromiseFinisher func(msg *sarama.ProducerMessage, err error) *Promise\n\n// NewPromise creates a new Promise\nfunc NewPromise() *Promise {\n\treturn new(Promise)\n}\n\n// NewPromiseWithFinisher creates a new Promise and a separate finish method.\n// This is necessary if the promise is used outside of goka package.\nfunc NewPromiseWithFinisher() (*Promise, PromiseFinisher) {\n\tp := new(Promise)\n\treturn p, p.finish\n}\n\n// execute all callbacks conveniently\n// The caller needs to lock!\nfunc (p *Promise) executeCallbacks() {\n\t// already resolved\n\tif p.finished {\n\t\treturn\n\t}\n\tfor _, s := range p.callbacks {\n\t\ts(p.msg, p.err)\n\t}\n\t// mark as finished\n\tp.finished = true\n}\n\n// Then chains a callback to the Promise\nfunc (p *Promise) Then(callback func(err error)) *Promise {\n\treturn p.ThenWithMessage(func(_ *sarama.ProducerMessage, err error) {\n\t\tcallback(err)\n\t})\n}\n\n// ThenWithMessage chains a callback to the Promise\nfunc (p *Promise) ThenWithMessage(callback func(msg *sarama.ProducerMessage, err error)) *Promise {\n\tp.Lock()\n\tdefer p.Unlock()\n\n\t// promise already run, call the callback immediately\n\tif p.finished {\n\t\tcallback(p.msg, p.err)\n\t\t// append it to the subscribers otherwise\n\t} else {\n\t\tp.callbacks = append(p.callbacks, callback)\n\t}\n\treturn p\n}\n\n// Finish finishes the promise by executing all callbacks and saving the message/error for late subscribers\nfunc (p *Promise) finish(msg *sarama.ProducerMessage, err error) *Promise {\n\tp.Lock()\n\tdefer p.Unlock()\n\n\tp.err = err\n\tp.msg = msg\n\n\tp.executeCallbacks()\n\treturn p\n}\n"
        },
        {
          "name": "promise_test.go",
          "type": "blob",
          "size": 0.6552734375,
          "content": "package goka\n\nimport (\n\t\"errors\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestPromise_thenBeforeFinish(t *testing.T) {\n\tp := new(Promise)\n\n\tvar promiseErr error\n\tp.Then(func(err error) {\n\t\tpromiseErr = err\n\t})\n\n\tp.finish(nil, errors.New(\"test\"))\n\n\trequire.Equal(t, \"test\", promiseErr.Error())\n\n\t// repeating finish won't change result\n\tp.finish(nil, errors.New(\"test-whatever\"))\n\n\trequire.Equal(t, \"test\", promiseErr.Error())\n}\n\nfunc TestPromise_thenAfterFinish(t *testing.T) {\n\tp := new(Promise)\n\n\tvar promiseErr error\n\tp.finish(nil, errors.New(\"test\"))\n\tp.Then(func(err error) {\n\t\tpromiseErr = err\n\t})\n\n\trequire.Equal(t, \"test\", promiseErr.Error())\n}\n"
        },
        {
          "name": "proxy.go",
          "type": "blob",
          "size": 0.6865234375,
          "content": "package goka\n\nimport (\n\t\"github.com/lovoo/goka/storage\"\n)\n\ntype storageProxy struct {\n\tstorage.Storage\n\ttopic     Stream\n\tpartition int32\n\tstateless bool\n\tupdate    UpdateCallback\n\n\topenedOnce once\n\tclosedOnce once\n}\n\nfunc (s *storageProxy) Open() error {\n\tif s == nil {\n\t\treturn nil\n\t}\n\treturn s.openedOnce.Do(s.Storage.Open)\n}\n\nfunc (s *storageProxy) Close() error {\n\tif s == nil {\n\t\treturn nil\n\t}\n\treturn s.closedOnce.Do(s.Storage.Close)\n}\n\nfunc (s *storageProxy) Update(ctx UpdateContext, k string, v []byte) error {\n\treturn s.update(ctx, s, k, v)\n}\n\nfunc (s *storageProxy) Stateless() bool {\n\treturn s.stateless\n}\n\nfunc (s *storageProxy) MarkRecovered() error {\n\treturn s.Storage.MarkRecovered()\n}\n"
        },
        {
          "name": "proxy_test.go",
          "type": "blob",
          "size": 0.8955078125,
          "content": "package goka\n\nimport (\n\t\"testing\"\n\n\t\"github.com/IBM/sarama\"\n\t\"github.com/lovoo/goka/storage\"\n)\n\ntype nullProxy struct{}\n\nfunc (p *nullProxy) Add(topic string, offset int64) error { return nil }\nfunc (p *nullProxy) Remove(topic string) error            { return nil }\nfunc (p *nullProxy) AddGroup()                            {}\nfunc (p *nullProxy) Stop()                                {}\n\nfunc TestUpdateWithHeaders(t *testing.T) {\n\ts := storageProxy{\n\t\tupdate: func(ctx UpdateContext, s storage.Storage, key string, value []byte) error {\n\t\t\tif len(ctx.Headers()) == 0 {\n\t\t\t\tt.Errorf(\"Missing headers\")\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tif string(ctx.Headers()[\"key\"]) != \"value\" {\n\t\t\t\tt.Errorf(\"Key missmatch. Expected %q. Found: %q\", \"key\", string(ctx.Headers()[\"key\"]))\n\t\t\t}\n\t\t\treturn nil\n\t\t},\n\t}\n\t_ = s.Update(&DefaultUpdateContext{headers: []*sarama.RecordHeader{{Key: []byte(\"key\"), Value: []byte(\"value\")}}}, \"\", nil)\n}\n"
        },
        {
          "name": "signal.go",
          "type": "blob",
          "size": 5.25390625,
          "content": "package goka\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n)\n\n// State types a state of the Signal\ntype State int\n\ntype waiter struct {\n\tdone     chan struct{}\n\tstate    State\n\tminState bool\n}\n\n// StateReader is a read only abstraction of a Signal to expose the current state.\ntype StateReader interface {\n\tState() State\n\tIsState(State) bool\n\tWaitForStateMin(state State) <-chan struct{}\n\tWaitForState(state State) <-chan struct{}\n\tObserveStateChange() *StateChangeObserver\n}\n\n// Signal allows synchronization on a state, waiting for that state and checking\n// the current state\ntype Signal struct {\n\tm                    sync.RWMutex\n\tstate                State\n\twaiters              map[*waiter]struct{}\n\tstateChangeObservers []*StateChangeObserver\n\tallowedStates        map[State]bool\n}\n\nvar emptyValue struct{}\n\n// NewSignal creates a new Signal based on the states\nfunc NewSignal(states ...State) *Signal {\n\ts := &Signal{\n\t\tallowedStates: make(map[State]bool),\n\t\twaiters:       make(map[*waiter]struct{}),\n\t}\n\tfor _, state := range states {\n\t\ts.allowedStates[state] = true\n\t}\n\n\treturn s\n}\n\n// SetState changes the state of the signal\n// and notifies all goroutines waiting for the new state\nfunc (s *Signal) SetState(state State) *Signal {\n\ts.m.Lock()\n\tdefer s.m.Unlock()\n\tif !s.allowedStates[state] {\n\t\tpanic(fmt.Errorf(\"trying to set illegal state %v\", state))\n\t}\n\n\t// if we're already in the state, do not notify anyone\n\tif s.state == state {\n\t\treturn s\n\t}\n\n\t// set the state and notify all channels waiting for it.\n\ts.state = state\n\n\tfor w, _ := range s.waiters {\n\t\tif w.state == state || (w.minState && state >= w.state) {\n\t\t\tdelete(s.waiters, w)\n\t\t\tclose(w.done)\n\t\t\tcontinue\n\t\t}\n\t\ts.waiters[w] = emptyValue\n\t}\n\n\t// notify the state change observers\n\tfor _, obs := range s.stateChangeObservers {\n\t\tobs.notify(state)\n\t}\n\n\treturn s\n}\n\n// IsState returns if the signal is in the requested state\nfunc (s *Signal) IsState(state State) bool {\n\ts.m.RLock()\n\tdefer s.m.RUnlock()\n\treturn s.state == state\n}\n\n// State returns the current state\nfunc (s *Signal) State() State {\n\ts.m.RLock()\n\tdefer s.m.RUnlock()\n\treturn s.state\n}\n\n// WaitForStateMin returns a channel that will be closed, when the signal enters passed\n// state or higher (states are ints, so we're just comparing ints here)\nfunc (s *Signal) WaitForStateMin(state State) <-chan struct{} {\n\n\tw := &waiter{\n\t\tdone:     make(chan struct{}),\n\t\tstate:    state,\n\t\tminState: true,\n\t}\n\n\treturn s.waitForWaiter(state, w)\n}\n\n// WaitForStateMinWithCleanup functions identically to WaitForStateMin, but returns a cleanup function in addition\n// so that the caller can cleanup resources if it no longer wants to wait\nfunc (s *Signal) WaitForStateMinWithCleanup(state State) (<-chan struct{}, func()) {\n\n\tw := &waiter{\n\t\tdone:     make(chan struct{}),\n\t\tstate:    state,\n\t\tminState: true,\n\t}\n\n\tcleanup := func() {\n\t\ts.m.Lock()\n\t\tdefer s.m.Unlock()\n\n\t\tdelete(s.waiters, w)\n\t}\n\n\treturn s.waitForWaiter(state, w), cleanup\n}\n\n// WaitForState returns a channel that closes when the signal reaches passed\n// state.\nfunc (s *Signal) WaitForState(state State) <-chan struct{} {\n\n\tw := &waiter{\n\t\tdone:  make(chan struct{}),\n\t\tstate: state,\n\t}\n\n\treturn s.waitForWaiter(state, w)\n}\n\nfunc (s *Signal) waitForWaiter(state State, w *waiter) chan struct{} {\n\t// if the signal is currently in that state (or in a higher state if minState is set)\n\t// then close the waiter immediately\n\ts.m.Lock()\n\tdefer s.m.Unlock()\n\tif curState := s.state; state == curState || (w.minState && curState >= state) {\n\t\tclose(w.done)\n\t} else {\n\t\ts.waiters[w] = emptyValue\n\t}\n\n\treturn w.done\n}\n\n// StateChangeObserver wraps a channel that triggers when the signal's state changes\ntype StateChangeObserver struct {\n\t// state notifier channel\n\tc chan State\n\t// closed is closed when the observer is closed to avoid sending to a closed channel\n\tclosed chan struct{}\n\t// stop is a callback to stop the observer\n\tstop func()\n}\n\n// Stop stops the observer. Its update channel will be closed and\nfunc (s *StateChangeObserver) Stop() {\n\ts.stop()\n}\n\n// C returns the channel to observer state changes\nfunc (s *StateChangeObserver) C() <-chan State {\n\treturn s.c\n}\n\nfunc (s *StateChangeObserver) notify(state State) {\n\tselect {\n\tcase <-s.closed:\n\tcase s.c <- state:\n\t}\n}\n\n// ObserveStateChange returns a channel that receives state changes.\n// Note that the caller must take care of consuming that channel, otherwise the Signal\n// will block upon state changes.\nfunc (s *Signal) ObserveStateChange() *StateChangeObserver {\n\ts.m.Lock()\n\tdefer s.m.Unlock()\n\n\tobserver := &StateChangeObserver{\n\t\tc:      make(chan State, 1),\n\t\tclosed: make(chan struct{}),\n\t}\n\n\t// initialize the observer with the current state\n\tobserver.notify(s.state)\n\n\t// the stop funtion stops the observer by closing its channel\n\t// and removing it from the list of observers\n\tobserver.stop = func() {\n\t\tclose(observer.closed)\n\t\ts.m.Lock()\n\t\tdefer s.m.Unlock()\n\n\t\t// iterate over all observers and close *this* one\n\t\tfor idx, obs := range s.stateChangeObservers {\n\t\t\tif obs == observer {\n\t\t\t\tcopy(s.stateChangeObservers[idx:], s.stateChangeObservers[idx+1:])\n\t\t\t\ts.stateChangeObservers[len(s.stateChangeObservers)-1] = nil\n\t\t\t\ts.stateChangeObservers = s.stateChangeObservers[:len(s.stateChangeObservers)-1]\n\t\t\t}\n\t\t}\n\t\tclose(observer.c)\n\t}\n\n\ts.stateChangeObservers = append(s.stateChangeObservers, observer)\n\treturn observer\n}\n"
        },
        {
          "name": "signal_test.go",
          "type": "blob",
          "size": 1.1923828125,
          "content": "package goka\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestSignal_SetState(t *testing.T) {\n\tsig := NewSignal(0, 1, 2)\n\trequire.True(t, sig.IsState(0))\n\trequire.False(t, sig.IsState(1))\n\n\tsig.SetState(1)\n\trequire.True(t, sig.IsState(1))\n\trequire.False(t, sig.IsState(0))\n\n\tdefer func() {\n\t\terr := recover()\n\t\tif err == nil {\n\t\t\tt.Fatalf(\"Expected panic, which didn't occur\")\n\t\t}\n\t}()\n\n\t// set some invalid state, this will panic\n\tsig.SetState(3)\n}\n\nfunc TestSignal_Wait(t *testing.T) {\n\tsig := NewSignal(0, 1, 2)\n\n\t<-sig.WaitForState(0)\n\t// should continue right now since\n\n\tvar (\n\t\tdone     = make(chan struct{})\n\t\thasState bool\n\t)\n\tgo func() {\n\t\tdefer close(done)\n\t\t<-sig.WaitForState(1)\n\t\thasState = true\n\t}()\n\n\trequire.False(t, hasState)\n\tsig.SetState(1)\n\t// wait for the goroutine to catchup with the state\n\t<-done\n\trequire.True(t, hasState)\n}\n\nfunc TestSignalWaitMin(t *testing.T) {\n\tsig := NewSignal(0, 1, 2)\n\n\tvar (\n\t\tdone     = make(chan struct{})\n\t\thasState bool\n\t)\n\tgo func() {\n\t\tdefer close(done)\n\t\t<-sig.WaitForStateMin(1)\n\t\thasState = true\n\t}()\n\n\trequire.False(t, hasState)\n\tsig.SetState(2)\n\t// wait for the goroutine to catchup with the state\n\t<-done\n\trequire.True(t, hasState)\n}\n"
        },
        {
          "name": "simple_backoff.go",
          "type": "blob",
          "size": 0.6533203125,
          "content": "package goka\n\nimport (\n\t\"sync\"\n\t\"time\"\n)\n\n// NewSimpleBackoff returns a simple backoff waiting the\n// specified duration longer each iteration until reset.\nfunc NewSimpleBackoff(step time.Duration, max time.Duration) Backoff {\n\treturn &simpleBackoff{\n\t\tstep: step,\n\t\tmax:  max,\n\t}\n}\n\ntype simpleBackoff struct {\n\tsync.Mutex\n\n\tcurrent time.Duration\n\tstep    time.Duration\n\tmax     time.Duration\n}\n\nfunc (b *simpleBackoff) Reset() {\n\tb.Lock()\n\tdefer b.Unlock()\n\tb.current = time.Duration(0)\n}\n\nfunc (b *simpleBackoff) Duration() time.Duration {\n\tb.Lock()\n\tdefer b.Unlock()\n\tvalue := b.current\n\n\tif (b.current + b.step) <= b.max {\n\t\tb.current += b.step\n\t}\n\treturn value\n}\n"
        },
        {
          "name": "simple_backoff_test.go",
          "type": "blob",
          "size": 0.76953125,
          "content": "package goka\n\nimport (\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestSimpleBackoff(t *testing.T) {\n\tt.Run(\"simple progression\", func(t *testing.T) {\n\t\tbackoff := NewSimpleBackoff(time.Second, 10*time.Second)\n\t\tfor i := 0; i < 10; i++ {\n\t\t\trequire.Equal(t, backoff.Duration(), time.Duration(i)*time.Second)\n\t\t}\n\n\t\t// it doesn't go higher than the max\n\t\trequire.Equal(t, backoff.Duration(), 10*time.Second)\n\t\trequire.Equal(t, backoff.Duration(), 10*time.Second)\n\t})\n\tt.Run(\"reset\", func(t *testing.T) {\n\t\tbackoff := NewSimpleBackoff(time.Second, 10*time.Second)\n\n\t\trequire.Equal(t, time.Duration(0), backoff.Duration())\n\t\tbackoff.Duration()\n\t\trequire.True(t, backoff.Duration() != 0)\n\t\tbackoff.Reset()\n\t\trequire.Equal(t, time.Duration(0), backoff.Duration())\n\t})\n}\n"
        },
        {
          "name": "stats.go",
          "type": "blob",
          "size": 5.095703125,
          "content": "package goka\n\nimport (\n\t\"log\"\n\t\"time\"\n)\n\n// PartitionStatus is the status of the partition of a table (group table or joined table).\ntype PartitionStatus int\n\nconst (\n\t// PartitionStopped indicates the partition stopped and should not be used anymore.\n\tPartitionStopped PartitionStatus = iota\n\t// PartitionInitializing indicates that the underlying storage is initializing (e.g. opening leveldb files),\n\t// and has not actually started working yet.\n\tPartitionInitializing\n\t// PartitionConnecting indicates the partition trying to (re-)connect to Kafka\n\tPartitionConnecting\n\t// PartitionRecovering indicates the partition is recovering and the storage\n\t// is writing updates in bulk-mode (if the storage implementation supports it).\n\tPartitionRecovering\n\t// PartitionPreparing indicates the end of the bulk-mode. Depending on the storage\n\t// implementation, the Preparing phase may take long because the storage compacts its logs.\n\tPartitionPreparing\n\t// PartitionRunning indicates the partition is recovered and processing updates\n\t// in normal operation.\n\tPartitionRunning\n)\n\nconst (\n\tstatsHwmUpdateInterval = 60 * time.Second\n\tfetchStatsTimeout      = 10 * time.Second\n)\n\n// InputStats represents the number of messages and the number of bytes consumed\n// from a stream or table topic since the process started.\ntype InputStats struct {\n\tCount      uint\n\tBytes      int\n\tOffsetLag  int64\n\tLastOffset int64\n\tDelay      time.Duration\n}\n\n// OutputStats represents the number of messages and the number of bytes emitted\n// into a stream or table since the process started.\ntype OutputStats struct {\n\tCount uint\n\tBytes int\n}\n\n// PartitionProcStats represents metrics and measurements of a partition processor\ntype PartitionProcStats struct {\n\tNow time.Time\n\n\tTableStats *TableStats\n\n\tJoined map[string]*TableStats\n\n\tInput  map[string]*InputStats\n\tOutput map[string]*OutputStats\n}\n\n// RecoveryStats groups statistics during recovery\ntype RecoveryStats struct {\n\tStartTime    time.Time\n\tRecoveryTime time.Time\n\n\tOffset int64 // last offset processed or recovered\n\tHwm    int64 // next offset to be written\n}\n\n// TableStats represents stats for a table partition\ntype TableStats struct {\n\tStalled bool\n\n\tStatus PartitionStatus\n\n\tRunMode PPRunMode\n\n\tRecovery *RecoveryStats\n\n\tInput  *InputStats\n\tWrites *OutputStats\n}\n\nfunc newInputStats() *InputStats {\n\treturn &InputStats{}\n}\n\nfunc newOutputStats() *OutputStats {\n\treturn &OutputStats{}\n}\n\nfunc (is *InputStats) clone() *InputStats {\n\tclone := *is\n\treturn &clone\n}\n\nfunc (os *OutputStats) clone() *OutputStats {\n\tclone := *os\n\treturn &clone\n}\n\ntype (\n\tinputStatsMap  map[string]*InputStats\n\toutputStatsMap map[string]*OutputStats\n)\n\nfunc (isp inputStatsMap) clone() map[string]*InputStats {\n\tc := map[string]*InputStats{}\n\tif isp == nil {\n\t\treturn c\n\t}\n\tfor k, v := range isp {\n\t\tc[k] = v.clone()\n\t}\n\treturn c\n}\n\nfunc (osp outputStatsMap) clone() map[string]*OutputStats {\n\tc := map[string]*OutputStats{}\n\tif osp == nil {\n\t\treturn c\n\t}\n\tfor k, v := range osp {\n\t\tc[k] = v.clone()\n\t}\n\treturn c\n}\n\nfunc newRecoveryStats() *RecoveryStats {\n\treturn new(RecoveryStats)\n}\n\nfunc (rs *RecoveryStats) clone() *RecoveryStats {\n\trsCopy := *rs\n\treturn &rsCopy\n}\n\nfunc newPartitionProcStats(inputs []string, outputs []string) *PartitionProcStats {\n\tprocStats := &PartitionProcStats{\n\t\tNow: time.Now(),\n\n\t\tInput:  make(map[string]*InputStats),\n\t\tOutput: make(map[string]*OutputStats),\n\t}\n\n\tfor _, input := range inputs {\n\t\tprocStats.Input[input] = newInputStats()\n\t}\n\n\tfor _, output := range outputs {\n\t\tprocStats.Output[output] = newOutputStats()\n\t}\n\n\treturn procStats\n}\n\nfunc newTableStats() *TableStats {\n\treturn &TableStats{\n\t\tInput:    newInputStats(),\n\t\tWrites:   newOutputStats(),\n\t\tRecovery: newRecoveryStats(),\n\t}\n}\n\nfunc (ts *TableStats) reset() {\n\tts.Input = newInputStats()\n\tts.Writes = newOutputStats()\n}\n\nfunc (ts *TableStats) clone() *TableStats {\n\treturn &TableStats{\n\t\tInput:    ts.Input.clone(),\n\t\tWrites:   ts.Writes.clone(),\n\t\tRecovery: ts.Recovery.clone(),\n\t\tStalled:  ts.Stalled,\n\t}\n}\n\nfunc (s *PartitionProcStats) clone() *PartitionProcStats {\n\tpps := newPartitionProcStats(nil, nil)\n\tpps.Now = time.Now()\n\tpps.Joined = make(map[string]*TableStats)\n\tpps.Input = inputStatsMap(s.Input).clone()\n\tpps.Output = outputStatsMap(s.Output).clone()\n\n\treturn pps\n}\n\nfunc (s *PartitionProcStats) trackOutput(topic string, valueLen int) {\n\toutStats := s.Output[topic]\n\tif outStats == nil {\n\t\tlog.Printf(\"no out stats for topic %s\", topic)\n\t\treturn\n\t}\n\toutStats.Count++\n\toutStats.Bytes += valueLen\n}\n\n// ViewStats represents the metrics of all partitions of a view.\ntype ViewStats struct {\n\tPartitions map[int32]*TableStats\n}\n\nfunc newViewStats() *ViewStats {\n\treturn &ViewStats{\n\t\tPartitions: make(map[int32]*TableStats),\n\t}\n}\n\n// ProcessorStats represents the metrics of all partitions of the processor,\n// including its group, joined tables and lookup tables.\ntype ProcessorStats struct {\n\tGroup  map[int32]*PartitionProcStats\n\tLookup map[string]*ViewStats\n}\n\nfunc newProcessorStats(partitions int) *ProcessorStats {\n\tstats := &ProcessorStats{\n\t\tGroup:  make(map[int32]*PartitionProcStats, partitions),\n\t\tLookup: make(map[string]*ViewStats, partitions),\n\t}\n\n\treturn stats\n}\n"
        },
        {
          "name": "storage",
          "type": "tree",
          "content": null
        },
        {
          "name": "systemtest",
          "type": "tree",
          "content": null
        },
        {
          "name": "tester",
          "type": "tree",
          "content": null
        },
        {
          "name": "topic_manager.go",
          "type": "blob",
          "size": 12.0986328125,
          "content": "package goka\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/IBM/sarama\"\n)\n\n// TopicManager provides an interface to create/check topics and their partitions\ntype TopicManager interface {\n\t// EnsureTableExists checks that a table (log-compacted topic) exists, or create one if possible\n\tEnsureTableExists(topic string, npar int) error\n\t// EnsureStreamExists checks that a stream topic exists, or create one if possible\n\tEnsureStreamExists(topic string, npar int) error\n\t// EnsureTopicExists checks that a topic exists, or create one if possible,\n\t// enforcing the given configuration\n\tEnsureTopicExists(topic string, npar, rfactor int, config map[string]string) error\n\n\t// Partitions returns the number of partitions of a topic, that are assigned to the running\n\t// instance, i.e. it doesn't represent all partitions of a topic.\n\tPartitions(topic string) ([]int32, error)\n\n\tGetOffset(topic string, partitionID int32, time int64) (int64, error)\n\n\t// Close closes the topic manager\n\tClose() error\n}\n\ntype topicManager struct {\n\tadmin              sarama.ClusterAdmin\n\tclient             sarama.Client\n\ttopicManagerConfig *TopicManagerConfig\n}\n\n// NewTopicManager creates a new topic manager using the sarama library\nfunc NewTopicManager(brokers []string, saramaConfig *sarama.Config, topicManagerConfig *TopicManagerConfig) (TopicManager, error) {\n\n\tif !saramaConfig.Version.IsAtLeast(sarama.V0_10_0_0) {\n\t\treturn nil, fmt.Errorf(\"goka's topic manager needs kafka version v0.10.0.0 or higher to function. Version is %s\", saramaConfig.Version.String())\n\t}\n\n\tclient, err := sarama.NewClient(brokers, saramaConfig)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Error creating the kafka client: %v\", err)\n\t}\n\treturn newTopicManager(brokers, saramaConfig, topicManagerConfig, client, checkBroker)\n}\n\nfunc newTopicManager(brokers []string, saramaConfig *sarama.Config, topicManagerConfig *TopicManagerConfig, client sarama.Client, check checkFunc) (*topicManager, error) {\n\tif client == nil {\n\t\treturn nil, errors.New(\"cannot create topic manager with nil client\")\n\t}\n\n\tif topicManagerConfig == nil {\n\t\treturn nil, errors.New(\"cannot create topic manager with nil config\")\n\t}\n\n\tactiveBrokers := client.Brokers()\n\tif len(activeBrokers) == 0 {\n\t\treturn nil, errors.New(\"no brokers active in current client\")\n\t}\n\n\tbroker := activeBrokers[0]\n\terr := check(broker, saramaConfig)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tadmin, err := sarama.NewClusterAdminFromClient(client)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error creating cluster admin: %v\", err)\n\t}\n\n\treturn &topicManager{\n\t\tadmin:              admin,\n\t\tclient:             client,\n\t\ttopicManagerConfig: topicManagerConfig,\n\t}, nil\n}\n\ntype checkFunc func(broker Broker, config *sarama.Config) error\n\nfunc checkBroker(broker Broker, config *sarama.Config) error {\n\tif config == nil {\n\t\tconfig = DefaultConfig()\n\t}\n\n\terr := broker.Open(config)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error opening broker connection: %v\", err)\n\t}\n\tconnected, err := broker.Connected()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"cannot connect to broker %s: %v\", broker.Addr(), err)\n\t}\n\n\tif !connected {\n\t\treturn fmt.Errorf(\"cannot connect to broker %s: not connected\", broker.Addr())\n\t}\n\n\treturn nil\n}\n\nfunc (m *topicManager) Close() error {\n\treturn m.client.Close()\n}\n\nfunc (m *topicManager) Partitions(topic string) ([]int32, error) {\n\t// refresh metadata, otherwise we might get an outdated number of partitions.\n\t// we cannot call it for that specific topic,\n\t// otherwise we'd create it if auto.create.topics.enable==true, which we want to avoid\n\tif err := m.client.RefreshMetadata(); err != nil {\n\t\treturn nil, fmt.Errorf(\"error refreshing metadata %v\", err)\n\t}\n\ttopics, err := m.client.Topics()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, tpc := range topics {\n\t\t// topic exists, let's list the partitions.\n\t\tif tpc == topic {\n\t\t\treturn m.client.Partitions(topic)\n\t\t}\n\t}\n\treturn nil, errTopicNotFound\n}\n\nfunc (m *topicManager) GetOffset(topic string, partitionID int32, time int64) (int64, error) {\n\treturn m.client.GetOffset(topic, partitionID, time)\n}\n\nfunc (m *topicManager) createTopic(topic string, npar, rfactor int, config map[string]string) error {\n\tm.topicManagerConfig.Logger.Debugf(\"creating topic %s with npar=%d, rfactor=%d, config=%#v\", topic, npar, rfactor, config)\n\ttopicDetail := &sarama.TopicDetail{}\n\ttopicDetail.NumPartitions = int32(npar)\n\ttopicDetail.ReplicationFactor = int16(rfactor)\n\ttopicDetail.ConfigEntries = make(map[string]*string)\n\n\tfor k, v := range config {\n\t\t// configEntries is a map to `*string`, so we have to make a copy of the value\n\t\t// here or end up having the same value for all, since `v` has the same address everywhere\n\t\tvalue := v\n\t\ttopicDetail.ConfigEntries[k] = &value\n\t}\n\n\terr := m.admin.CreateTopic(topic, topicDetail, false)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error creating topic %s, npar=%d, rfactor=%d, config=%#v: %v\",\n\t\t\ttopic, npar, rfactor, config, err)\n\t}\n\n\treturn m.waitForCreated(topic)\n}\n\nfunc (m *topicManager) handleConfigMismatch(message string) error {\n\tswitch m.topicManagerConfig.MismatchBehavior {\n\tcase TMConfigMismatchBehaviorWarn:\n\t\tm.topicManagerConfig.Logger.Printf(\"Warning: %s\", message)\n\t\treturn nil\n\tcase TMConfigMismatchBehaviorFail:\n\t\treturn fmt.Errorf(\"%s\", message)\n\t\t// ignores per default\n\tdefault:\n\t\treturn nil\n\t}\n}\n\nfunc (m *topicManager) ensureExists(topic string, npar, rfactor int, config map[string]string) error {\n\n\tpartitions, err := m.Partitions(topic)\n\n\tif err != nil {\n\t\tif err != errTopicNotFound {\n\t\t\treturn fmt.Errorf(\"error checking topic: %v\", err)\n\t\t}\n\t}\n\t// no topic yet, let's create it\n\tif len(partitions) == 0 {\n\n\t\t// (or not)\n\t\tif m.topicManagerConfig.NoCreate {\n\t\t\treturn fmt.Errorf(\"topic %s does not exist but the manager is configured with NoCreate, so it will not attempt to create it\", topic)\n\t\t}\n\n\t\treturn m.createTopic(topic,\n\t\t\tnpar,\n\t\t\trfactor,\n\t\t\tconfig)\n\t}\n\n\t// we have a topic, let's check their values\n\n\t// partitions do not match\n\tif len(partitions) != npar {\n\t\treturn m.handleConfigMismatch(fmt.Sprintf(\"partition count mismatch for topic %s. Need %d, but existing topic has %d\", topic, npar, len(partitions)))\n\t}\n\n\t// check additional config values via the cluster admin if our current version supports it\n\tif m.adminSupported() {\n\t\tcfgMap, err := m.getTopicConfigMap(topic)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// check for all user-passed config values whether they're as expected\n\t\tfor key, value := range config {\n\t\t\tentry, ok := cfgMap[key]\n\t\t\tif !ok {\n\t\t\t\treturn m.handleConfigMismatch(fmt.Sprintf(\"config for topic %s did not contain requested key %s\", topic, key))\n\t\t\t}\n\t\t\tif entry.Value != value {\n\t\t\t\treturn m.handleConfigMismatch(fmt.Sprintf(\"unexpected config value for topic %s. Expected %s=%s. Got %s=%s\", topic, key, value, key, entry.Value))\n\t\t\t}\n\t\t}\n\n\t\t// check if the number of replicas match what we expect\n\t\ttopicMinReplicas, err := m.getTopicMinReplicas(topic)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif topicMinReplicas != rfactor {\n\t\t\treturn m.handleConfigMismatch(fmt.Sprintf(\"unexpected replication factor for topic %s. Expected %d, got %d\",\n\t\t\t\ttopic,\n\t\t\t\trfactor,\n\t\t\t\ttopicMinReplicas))\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (m *topicManager) waitForCreated(topic string) error {\n\t// no timeout defined -> no check\n\tif m.topicManagerConfig.CreateTopicTimeout == 0 {\n\t\treturn nil\n\t}\n\n\tctx, cancel := context.WithTimeout(context.Background(), m.topicManagerConfig.CreateTopicTimeout)\n\tdefer cancel()\n\n\tfor ctx.Err() == nil {\n\t\t_, err := m.Partitions(topic)\n\t\tswitch err {\n\t\tcase nil:\n\t\t\treturn nil\n\t\tcase errTopicNotFound:\n\t\t\ttime.Sleep(time.Second)\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"error checking topic: %w\", err)\n\t\t}\n\t}\n\treturn fmt.Errorf(\"waiting for topic %s to be created timed out\", topic)\n}\n\nfunc (m *topicManager) adminSupported() bool {\n\treturn m.client.Config().Version.IsAtLeast(sarama.V0_11_0_0)\n}\n\nfunc (m *topicManager) getTopicConfigMap(topic string) (map[string]sarama.ConfigEntry, error) {\n\tcfg, err := m.admin.DescribeConfig(sarama.ConfigResource{\n\t\tType: sarama.TopicResource,\n\t\tName: topic,\n\t})\n\n\t// now it does not exist anymore -- this means the cluster is somehow unstable\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Error getting config for topic %s: %w\", topic, err)\n\t}\n\n\t// remap the config values to a map\n\tcfgMap := make(map[string]sarama.ConfigEntry, len(cfg))\n\tfor _, cfgEntry := range cfg {\n\t\tcfgMap[cfgEntry.Name] = cfgEntry\n\t}\n\treturn cfgMap, nil\n}\n\nfunc (m *topicManager) getTopicMinReplicas(topic string) (int, error) {\n\ttopicsMeta, err := m.admin.DescribeTopics([]string{topic})\n\tif err != nil {\n\t\treturn 0, fmt.Errorf(\"Error describing topic %s: %w\", topic, err)\n\t}\n\tif len(topicsMeta) != 1 {\n\t\treturn 0, fmt.Errorf(\"cannot find meta data for topic %s\", topic)\n\t}\n\n\ttopicMeta := topicsMeta[0]\n\tvar replicasMin int\n\tfor _, part := range topicMeta.Partitions {\n\t\tif replicasMin == 0 || len(part.Replicas) < replicasMin {\n\t\t\treplicasMin = len(part.Replicas)\n\t\t}\n\t}\n\treturn replicasMin, nil\n}\n\nfunc (m *topicManager) EnsureStreamExists(topic string, npar int) error {\n\treturn m.ensureExists(\n\t\ttopic,\n\t\tnpar,\n\t\tm.topicManagerConfig.Stream.Replication,\n\t\tmap[string]string{\n\t\t\t\"cleanup.policy\": m.topicManagerConfig.streamCleanupPolicy(),\n\t\t\t\"retention.ms\":   fmt.Sprintf(\"%d\", m.topicManagerConfig.Stream.Retention.Milliseconds()),\n\t\t})\n}\n\nfunc (m *topicManager) EnsureTopicExists(topic string, npar, rfactor int, config map[string]string) error {\n\treturn m.ensureExists(\n\t\ttopic,\n\t\tnpar,\n\t\trfactor,\n\t\tconfig)\n}\n\nfunc (m *topicManager) EnsureTableExists(topic string, npar int) error {\n\n\treturn m.ensureExists(\n\t\ttopic,\n\t\tnpar,\n\t\tm.topicManagerConfig.Table.Replication,\n\t\tmap[string]string{\n\t\t\t\"cleanup.policy\": m.topicManagerConfig.tableCleanupPolicy(),\n\t\t})\n}\n\n// TMConfigMismatchBehavior configures how configuration mismatches of a topic (replication, num partitions, compaction) should be\n// treated\ntype TMConfigMismatchBehavior int\n\nconst (\n\t// TMConfigMismatchBehaviorIgnore ignore wrong config values\n\tTMConfigMismatchBehaviorIgnore TMConfigMismatchBehavior = 0\n\n\t// TMConfigMismatchBehaviorWarn warns if the topic is configured differently than requested\n\tTMConfigMismatchBehaviorWarn TMConfigMismatchBehavior = 1\n\n\t// TMConfigMismatchBehaviorFail makes checking the topic fail, if the configuration different than requested\n\tTMConfigMismatchBehaviorFail TMConfigMismatchBehavior = 2\n)\n\n// TopicManagerConfig contains options of to create tables and stream topics.\ntype TopicManagerConfig struct {\n\tLogger logger\n\tTable  struct {\n\t\tReplication int\n\t\t// CleanupPolicy allows to overwrite the default cleanup policy for streams.\n\t\t// Defaults to 'compact' if not set\n\t\tCleanupPolicy string\n\t}\n\tStream struct {\n\t\tReplication int\n\t\tRetention   time.Duration\n\t\t// CleanupPolicy allows to overwrite the default cleanup policy for streams.\n\t\t// Defaults to 'delete' if not set\n\t\tCleanupPolicy string\n\t}\n\n\t// CreateTopicTimeout timeout for the topic manager to wait for the topic being created.\n\t// Set to 0 to turn off checking topic creation.\n\t// Defaults to 10 seconds\n\tCreateTopicTimeout time.Duration\n\n\t// TMConfigMismatchBehavior configures how configuration mismatches of a topic (replication, num partitions, compaction) should be\n\t// treated\n\tMismatchBehavior TMConfigMismatchBehavior\n\n\t// If set to true, the topic manager will not attempt to create the topic.\n\t// This can be used if topic creation should be done externally.\n\tNoCreate bool\n}\n\nfunc (tmc *TopicManagerConfig) streamCleanupPolicy() string {\n\n\tif tmc.Stream.CleanupPolicy != \"\" {\n\t\treturn tmc.Stream.CleanupPolicy\n\t}\n\treturn \"delete\"\n}\n\nfunc (tmc *TopicManagerConfig) tableCleanupPolicy() string {\n\tif tmc.Table.CleanupPolicy != \"\" {\n\t\treturn tmc.Table.CleanupPolicy\n\t}\n\treturn \"compact\"\n}\n\n// NewTopicManagerConfig provides a default configuration for auto-creation\n// with replication factor of 2 and rentention time of 1 hour.\n// Use this function rather than creating TopicManagerConfig from scratch to\n// initialize the config with reasonable defaults\nfunc NewTopicManagerConfig() *TopicManagerConfig {\n\tcfg := new(TopicManagerConfig)\n\tcfg.Table.Replication = 2\n\tcfg.Stream.Replication = 2\n\tcfg.Stream.Retention = 1 * time.Hour\n\n\tcfg.MismatchBehavior = TMConfigMismatchBehaviorIgnore\n\tcfg.Logger = defaultLogger.Prefix(\"topic_manager\")\n\tcfg.CreateTopicTimeout = 10 * time.Second\n\treturn cfg\n}\n"
        },
        {
          "name": "topic_manager_test.go",
          "type": "blob",
          "size": 16.533203125,
          "content": "package goka\n\nimport (\n\t\"errors\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/IBM/sarama\"\n\t\"github.com/golang/mock/gomock\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nvar tmTestBrokers = []string{\"0\"}\n\nfunc trueCheckFunc(broker Broker, config *sarama.Config) error {\n\treturn nil\n}\n\nfunc falseCheckFunc(broker Broker, config *sarama.Config) error {\n\treturn errors.New(\"broker check error\")\n}\n\nfunc createTopicManager(t *testing.T) (*topicManager, *builderMock, *gomock.Controller) {\n\tctrl := NewMockController(t)\n\tbm := newBuilderMock(ctrl)\n\treturn &topicManager{\n\t\tadmin:              bm.admin,\n\t\tclient:             bm.client,\n\t\ttopicManagerConfig: NewTopicManagerConfig(),\n\t}, bm, ctrl\n}\n\nfunc TestTM_checkBroker(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tctrl := NewMockController(t)\n\t\tbroker := NewMockBroker(ctrl)\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tconfig    = DefaultConfig()\n\t\t\tconnected = true\n\t\t)\n\t\tbroker.EXPECT().Open(config).Return(nil)\n\t\tbroker.EXPECT().Connected().Return(connected, nil)\n\n\t\terr := checkBroker(broker, config)\n\t\trequire.NoError(t, err)\n\t})\n\tt.Run(\"fail_open\", func(t *testing.T) {\n\t\tctrl := NewMockController(t)\n\t\tbroker := NewMockBroker(ctrl)\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tconfig *sarama.Config = DefaultConfig()\n\t\t\terrRet error          = errors.New(\"some-error\")\n\t\t)\n\t\tbroker.EXPECT().Open(config).Return(errRet)\n\n\t\terr := checkBroker(broker, config)\n\t\trequire.Error(t, err)\n\t})\n\tt.Run(\"fail_connected\", func(t *testing.T) {\n\t\tctrl := NewMockController(t)\n\t\tbroker := NewMockBroker(ctrl)\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tconfig    = DefaultConfig()\n\t\t\tconnected = false\n\t\t)\n\t\tbroker.EXPECT().Open(config).Return(nil)\n\t\tbroker.EXPECT().Connected().Return(connected, nil)\n\t\tbroker.EXPECT().Addr().Return(\"127.0.0.1\")\n\n\t\terr := checkBroker(broker, config)\n\t\trequire.Error(t, err)\n\t})\n\tt.Run(\"fail_not_connected\", func(t *testing.T) {\n\t\tctrl := NewMockController(t)\n\t\tbroker := NewMockBroker(ctrl)\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tconfig    = DefaultConfig()\n\t\t\tconnected = false\n\t\t\terrRet    = errors.New(\"some-error\")\n\t\t)\n\t\tbroker.EXPECT().Open(config).Return(nil)\n\t\tbroker.EXPECT().Connected().Return(connected, errRet)\n\t\tbroker.EXPECT().Addr().Return(\"127.0.0.1\")\n\n\t\terr := checkBroker(broker, config)\n\t\trequire.Error(t, err)\n\t})\n}\n\nfunc TestTM_newTopicManager(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tctrl := NewMockController(t)\n\t\tdefer ctrl.Finish()\n\t\tbm := newBuilderMock(ctrl)\n\n\t\t// expect some calls to properly set up the topic manager\n\t\tbm.client.EXPECT().Controller().Return(nil, nil)\n\t\tbm.client.EXPECT().Config().Return(nil)\n\t\tbm.client.EXPECT().Brokers().Return([]*sarama.Broker{\n\t\t\tnew(sarama.Broker),\n\t\t})\n\t\ttm, err := newTopicManager(tmTestBrokers, DefaultConfig(), NewTopicManagerConfig(), bm.client, trueCheckFunc)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, tm.client, bm.client)\n\t\trequire.NotNil(t, tm.admin)\n\t})\n\tt.Run(\"fail_missing_stuff\", func(t *testing.T) {\n\t\tctrl := NewMockController(t)\n\t\tdefer ctrl.Finish()\n\t\tbm := newBuilderMock(ctrl)\n\n\t\t_, err := newTopicManager(tmTestBrokers, nil, nil, bm.client, trueCheckFunc)\n\t\trequire.Error(t, err)\n\n\t\t_, err = newTopicManager(tmTestBrokers, nil, NewTopicManagerConfig(), nil, trueCheckFunc)\n\t\trequire.Error(t, err)\n\t})\n\tt.Run(\"fail_check\", func(t *testing.T) {\n\t\tctrl := NewMockController(t)\n\t\tdefer ctrl.Finish()\n\t\tbm := newBuilderMock(ctrl)\n\n\t\t// expect to return one broker\n\t\tbm.client.EXPECT().Brokers().Return([]*sarama.Broker{\n\t\t\tnew(sarama.Broker),\n\t\t})\n\n\t\t_, err := newTopicManager(tmTestBrokers, DefaultConfig(), NewTopicManagerConfig(), bm.client, falseCheckFunc)\n\t\trequire.Equal(t, err.Error(), \"broker check error\")\n\t})\n}\n\nfunc TestTM_Close(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\ttm, bm, ctrl := createTopicManager(t)\n\t\tdefer ctrl.Finish()\n\t\tbm.client.EXPECT().Close().Return(nil)\n\t\terr := tm.Close()\n\t\trequire.NoError(t, err)\n\t})\n\tt.Run(\"fail\", func(t *testing.T) {\n\t\ttm, bm, ctrl := createTopicManager(t)\n\t\tdefer ctrl.Finish()\n\t\tbm.client.EXPECT().Close().Return(errors.New(\"some-error\"))\n\t\terr := tm.Close()\n\t\trequire.Error(t, err)\n\t})\n}\n\nfunc TestTM_Partitions(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\ttm, bm, ctrl := createTopicManager(t)\n\t\tdefer ctrl.Finish()\n\t\ttopic := \"some-topic\"\n\t\tbm.client.EXPECT().RefreshMetadata().Return(nil)\n\t\tbm.client.EXPECT().Topics().Return([]string{topic}, nil)\n\t\tbm.client.EXPECT().Partitions(topic).Return([]int32{0}, nil)\n\t\t_, err := tm.Partitions(topic)\n\t\trequire.NoError(t, err)\n\t})\n\tt.Run(\"fail\", func(t *testing.T) {\n\t\ttm, bm, ctrl := createTopicManager(t)\n\t\tdefer ctrl.Finish()\n\t\ttopic := \"some-topic\"\n\t\tbm.client.EXPECT().RefreshMetadata().Return(nil)\n\t\tbm.client.EXPECT().Topics().Return([]string{topic}, nil)\n\t\tbm.client.EXPECT().Partitions(topic).Return([]int32{0}, errors.New(\"some-error\"))\n\t\t_, err := tm.Partitions(topic)\n\t\trequire.Error(t, err)\n\t})\n}\n\nfunc TestTM_GetOffset(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\ttm, bm, ctrl := createTopicManager(t)\n\t\tdefer ctrl.Finish()\n\t\tvar (\n\t\t\ttopic     = \"some-topic\"\n\t\t\tpartition int32\n\t\t\toffset    = sarama.OffsetNewest\n\t\t)\n\t\tbm.client.EXPECT().GetOffset(topic, partition, offset).Return(sarama.OffsetNewest, nil)\n\t\t_, err := tm.GetOffset(topic, partition, offset)\n\t\trequire.NoError(t, err)\n\t})\n\tt.Run(\"fail\", func(t *testing.T) {\n\t\ttm, bm, ctrl := createTopicManager(t)\n\t\tdefer ctrl.Finish()\n\t\tvar (\n\t\t\ttopic     = \"some-topic\"\n\t\t\tpartition int32\n\t\t\toffset    = sarama.OffsetNewest\n\t\t)\n\t\tbm.client.EXPECT().GetOffset(topic, partition, offset).Return(sarama.OffsetNewest, errors.New(\"some-error\"))\n\t\t_, err := tm.GetOffset(topic, partition, offset)\n\t\trequire.Error(t, err)\n\t})\n}\n\nfunc TestTM_EnsureStreamExists(t *testing.T) {\n\tt.Run(\"exists\", func(t *testing.T) {\n\t\ttm, bm, ctrl := createTopicManager(t)\n\t\tdefer ctrl.Finish()\n\n\t\ttm.topicManagerConfig.MismatchBehavior = TMConfigMismatchBehaviorFail\n\t\tvar (\n\t\t\ttopic = \"some-topic\"\n\t\t\tnpar  = 1\n\t\t)\n\n\t\tcfg := sarama.NewConfig()\n\t\tcfg.Version = sarama.V0_10_0_0\n\t\tbm.client.EXPECT().Config().Return(cfg)\n\t\tbm.client.EXPECT().RefreshMetadata().Return(nil)\n\t\tbm.client.EXPECT().Topics().Return([]string{topic}, nil)\n\t\tbm.client.EXPECT().Partitions(topic).Return([]int32{0}, nil)\n\n\t\terr := tm.EnsureStreamExists(topic, npar)\n\t\trequire.NoError(t, err)\n\t})\n\tt.Run(\"create\", func(t *testing.T) {\n\t\ttm, bm, ctrl := createTopicManager(t)\n\t\tdefer ctrl.Finish()\n\t\tvar (\n\t\t\ttopic   = \"some-topic\"\n\t\t\tnpar    = 1\n\t\t\trfactor = 1\n\t\t)\n\n\t\ttm.topicManagerConfig.Stream.Replication = rfactor\n\t\ttm.topicManagerConfig.Stream.Retention = time.Second\n\t\tbm.client.EXPECT().RefreshMetadata().Return(nil).AnyTimes()\n\n\t\tgomock.InOrder(\n\t\t\tbm.client.EXPECT().Topics().Return(nil, nil),\n\t\t\tbm.client.EXPECT().Topics().Return([]string{\"some-topic\"}, nil),\n\t\t\tbm.client.EXPECT().Partitions(\"some-topic\").Return([]int32{0}, nil),\n\t\t)\n\t\tbm.admin.EXPECT().CreateTopic(gomock.Any(), gomock.Any(), false).Return(nil)\n\n\t\terr := tm.EnsureStreamExists(topic, npar)\n\t\trequire.NoError(t, err)\n\t})\n\tt.Run(\"no-create\", func(t *testing.T) {\n\t\ttm, bm, ctrl := createTopicManager(t)\n\t\tdefer ctrl.Finish()\n\t\tvar (\n\t\t\ttopic   = \"some-topic\"\n\t\t\tnpar    = 1\n\t\t\trfactor = 1\n\t\t)\n\n\t\ttm.topicManagerConfig.Stream.Replication = rfactor\n\t\ttm.topicManagerConfig.Stream.Retention = time.Second\n\t\ttm.topicManagerConfig.NoCreate = true\n\n\t\tbm.client.EXPECT().RefreshMetadata().Return(nil).AnyTimes()\n\n\t\tgomock.InOrder(\n\t\t\tbm.client.EXPECT().Topics().Return(nil, nil),\n\t\t)\n\n\t\terr := tm.EnsureStreamExists(topic, npar)\n\t\trequire.ErrorContains(t, err, \"will not attempt to create it\")\n\t})\n\tt.Run(\"fail\", func(t *testing.T) {\n\t\ttm, bm, ctrl := createTopicManager(t)\n\t\tdefer ctrl.Finish()\n\t\tvar (\n\t\t\ttopic  = \"some-topic\"\n\t\t\tnpar   = 1\n\t\t\tretErr = errors.New(\"some-error\")\n\t\t)\n\n\t\tbm.client.EXPECT().RefreshMetadata().Return(nil)\n\t\tbm.client.EXPECT().Topics().Return(nil, retErr)\n\n\t\terr := tm.EnsureStreamExists(topic, npar)\n\t\trequire.Error(t, err)\n\t})\n}\n\nfunc TestTM_createTopic(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\ttm, bm, ctrl := createTopicManager(t)\n\t\tdefer ctrl.Finish()\n\t\tvar (\n\t\t\ttopic   = \"some-topic\"\n\t\t\tnpar    = 1\n\t\t\trfactor = 1\n\t\t\tconfig  = map[string]string{\n\t\t\t\t\"a\": \"a\",\n\t\t\t}\n\t\t)\n\n\t\tbm.client.EXPECT().RefreshMetadata().Return(nil).AnyTimes()\n\t\tgomock.InOrder(\n\t\t\tbm.client.EXPECT().Topics().Return(nil, nil),\n\t\t\tbm.client.EXPECT().Topics().Return([]string{\"some-topic\"}, nil),\n\t\t\tbm.client.EXPECT().Partitions(\"some-topic\").Return([]int32{0}, nil),\n\t\t)\n\n\t\tbm.admin.EXPECT().CreateTopic(gomock.Any(), gomock.Any(), false).Return(nil)\n\t\terr := tm.createTopic(topic, npar, rfactor, config)\n\t\trequire.NoError(t, err)\n\t})\n\tt.Run(\"fail\", func(t *testing.T) {\n\t\ttm, bm, ctrl := createTopicManager(t)\n\t\tdefer ctrl.Finish()\n\t\tvar (\n\t\t\ttopic   = \"some-topic\"\n\t\t\tnpar    = 1\n\t\t\trfactor = 1\n\t\t\tconfig  = map[string]string{\n\t\t\t\t\"a\": \"a\",\n\t\t\t}\n\t\t\tretErr error = errors.New(\"some-error\")\n\t\t)\n\t\tbm.admin.EXPECT().CreateTopic(gomock.Any(), gomock.Any(), false).Return(retErr)\n\t\terr := tm.createTopic(topic, npar, rfactor, config)\n\t\trequire.Error(t, err)\n\t})\n}\n\nfunc TestTM_EnsureTopicExists(t *testing.T) {\n\tt.Run(\"exists_nocheck\", func(t *testing.T) {\n\t\ttm, bm, ctrl := createTopicManager(t)\n\t\tdefer ctrl.Finish()\n\t\tvar (\n\t\t\ttopic   = \"some-topic\"\n\t\t\tnpar    = 1\n\t\t\trfactor = 1\n\t\t\tconfig  = map[string]string{\n\t\t\t\t\"a\": \"a\",\n\t\t\t}\n\t\t)\n\n\t\tcfg := sarama.NewConfig()\n\t\tcfg.Version = sarama.V0_10_0_0\n\t\tbm.client.EXPECT().Config().Return(cfg)\n\t\tbm.client.EXPECT().Topics().Return([]string{topic}, nil)\n\t\tbm.client.EXPECT().RefreshMetadata().Return(nil)\n\t\tbm.client.EXPECT().Partitions(topic).Return([]int32{0}, nil)\n\n\t\terr := tm.EnsureTopicExists(topic, npar, rfactor, config)\n\t\trequire.NoError(t, err)\n\t})\n\tt.Run(\"exists_diff_partitions_ignore\", func(t *testing.T) {\n\t\ttm, bm, ctrl := createTopicManager(t)\n\t\tdefer ctrl.Finish()\n\t\tvar (\n\t\t\ttopic = \"some-topic\"\n\t\t\tnpar  = 1\n\t\t)\n\n\t\ttm.topicManagerConfig.MismatchBehavior = TMConfigMismatchBehaviorIgnore\n\n\t\tbm.client.EXPECT().Topics().Return([]string{topic}, nil)\n\t\tbm.client.EXPECT().RefreshMetadata().Return(nil)\n\t\tbm.client.EXPECT().Partitions(topic).Return([]int32{0, 1}, nil)\n\n\t\terr := tm.EnsureStreamExists(topic, npar)\n\t\trequire.NoError(t, err)\n\t})\n\tt.Run(\"exists_diff_partitions_fail\", func(t *testing.T) {\n\t\ttm, bm, ctrl := createTopicManager(t)\n\t\tdefer ctrl.Finish()\n\t\tvar (\n\t\t\ttopic = \"some-topic\"\n\t\t\tnpar  = 1\n\t\t)\n\n\t\ttm.topicManagerConfig.MismatchBehavior = TMConfigMismatchBehaviorFail\n\n\t\tbm.client.EXPECT().Topics().Return([]string{topic}, nil)\n\t\tbm.client.EXPECT().RefreshMetadata().Return(nil)\n\t\tbm.client.EXPECT().Partitions(topic).Return([]int32{0, 1}, nil)\n\n\t\terr := tm.EnsureTopicExists(topic, npar, 1, map[string]string{})\n\t\trequire.Error(t, err)\n\t})\n\tt.Run(\"exists_diff_config\", func(t *testing.T) {\n\t\ttm, bm, ctrl := createTopicManager(t)\n\t\tdefer ctrl.Finish()\n\t\tvar (\n\t\t\ttopic = \"some-topic\"\n\t\t\tnpar  = 1\n\t\t)\n\t\t// make the tm fail on mismatch:\n\t\ttm.topicManagerConfig.MismatchBehavior = TMConfigMismatchBehaviorFail\n\n\t\tcfg := sarama.NewConfig()\n\t\tcfg.Version = sarama.V0_11_0_0\n\t\tbm.client.EXPECT().Config().Return(cfg)\n\t\tbm.client.EXPECT().Topics().Return([]string{topic}, nil)\n\t\tbm.client.EXPECT().RefreshMetadata().Return(nil)\n\t\tbm.client.EXPECT().Partitions(topic).Return([]int32{0}, nil)\n\t\tbm.admin.EXPECT().DescribeConfig(sarama.ConfigResource{\n\t\t\tType: sarama.TopicResource,\n\t\t\tName: topic,\n\t\t}).Return([]sarama.ConfigEntry{{Name: \"a\", Value: \"b\"}}, nil)\n\t\terr := tm.EnsureTopicExists(topic, npar, 1, map[string]string{\"a\": \"diff-value\"})\n\t\trequire.Error(t, err)\n\t})\n\tt.Run(\"exists_diff_rfactor\", func(t *testing.T) {\n\t\ttm, bm, ctrl := createTopicManager(t)\n\t\tdefer ctrl.Finish()\n\t\tvar (\n\t\t\ttopic = \"some-topic\"\n\t\t\tnpar  = 1\n\t\t)\n\t\t// make the tm fail on mismatch:\n\t\ttm.topicManagerConfig.MismatchBehavior = TMConfigMismatchBehaviorFail\n\n\t\tcfg := sarama.NewConfig()\n\t\tcfg.Version = sarama.V0_11_0_0\n\t\tbm.client.EXPECT().Config().Return(cfg)\n\t\tbm.client.EXPECT().Topics().Return([]string{topic}, nil)\n\t\tbm.client.EXPECT().RefreshMetadata().Return(nil)\n\t\tbm.client.EXPECT().Partitions(topic).Return([]int32{0}, nil)\n\t\tbm.admin.EXPECT().DescribeConfig(sarama.ConfigResource{\n\t\t\tType: sarama.TopicResource,\n\t\t\tName: topic,\n\t\t}).Return(nil, nil)\n\t\tbm.admin.EXPECT().DescribeTopics([]string{topic}).Return([]*sarama.TopicMetadata{\n\t\t\t{\n\t\t\t\tName: topic,\n\t\t\t\tPartitions: []*sarama.PartitionMetadata{\n\t\t\t\t\t// two topics with different replicas,\n\t\t\t\t\t// TM will select the one with the fewest replicas and\n\t\t\t\t\t// compare with what is requested\n\t\t\t\t\t{Replicas: []int32{0}},\n\t\t\t\t\t{Replicas: []int32{0, 1}},\n\t\t\t\t},\n\t\t\t},\n\t\t}, nil)\n\n\t\t// fails because rfactor is requested as 2, but the smallest one is 1\n\t\terr := tm.EnsureTopicExists(topic, npar, 2, map[string]string{})\n\t\trequire.Error(t, err)\n\t})\n\tt.Run(\"exists_same\", func(t *testing.T) {\n\t\ttm, bm, ctrl := createTopicManager(t)\n\t\tdefer ctrl.Finish()\n\t\tvar (\n\t\t\ttopic = \"some-topic\"\n\t\t\tnpar  = 1\n\t\t)\n\t\t// make the tm fail on mismatch:\n\t\ttm.topicManagerConfig.MismatchBehavior = TMConfigMismatchBehaviorFail\n\n\t\tcfg := sarama.NewConfig()\n\t\tcfg.Version = sarama.V0_11_0_0\n\t\tbm.client.EXPECT().Config().Return(cfg)\n\t\tbm.client.EXPECT().Topics().Return([]string{topic}, nil)\n\t\tbm.client.EXPECT().RefreshMetadata().Return(nil)\n\t\tbm.client.EXPECT().Partitions(topic).Return([]int32{0}, nil)\n\t\tbm.admin.EXPECT().DescribeConfig(sarama.ConfigResource{\n\t\t\tType: sarama.TopicResource,\n\t\t\tName: topic,\n\t\t}).Return([]sarama.ConfigEntry{{Name: \"a\", Value: \"b\"}}, nil)\n\t\tbm.admin.EXPECT().DescribeTopics([]string{topic}).Return([]*sarama.TopicMetadata{\n\t\t\t{\n\t\t\t\tName: topic,\n\t\t\t\tPartitions: []*sarama.PartitionMetadata{\n\t\t\t\t\t// two topics with different replicas,\n\t\t\t\t\t// TM will select the one with the fewest replicas and\n\t\t\t\t\t// compare with what is requested\n\t\t\t\t\t{Replicas: []int32{0, 1}},\n\t\t\t\t},\n\t\t\t},\n\t\t}, nil)\n\n\t\t// fails because rfactor is requested as 2, but the smallest one is 1\n\t\terr := tm.EnsureTopicExists(topic, npar, 2, map[string]string{\"a\": \"b\"})\n\t\trequire.NoError(t, err)\n\t})\n\tt.Run(\"create\", func(t *testing.T) {\n\t\ttm, bm, ctrl := createTopicManager(t)\n\t\tdefer ctrl.Finish()\n\t\tvar (\n\t\t\ttopic   = \"some-topic\"\n\t\t\tnpar    = 1\n\t\t\trfactor = 1\n\t\t\tconfig  = map[string]string{\n\t\t\t\t\"a\": \"a\",\n\t\t\t}\n\t\t)\n\n\t\tbm.client.EXPECT().RefreshMetadata().Return(nil).AnyTimes()\n\t\tgomock.InOrder(\n\t\t\tbm.client.EXPECT().Topics().Return(nil, nil),\n\t\t\tbm.client.EXPECT().Topics().Return([]string{\"some-topic\"}, nil),\n\t\t\tbm.client.EXPECT().Partitions(\"some-topic\").Return([]int32{0}, nil),\n\t\t)\n\n\t\tbm.admin.EXPECT().CreateTopic(gomock.Any(), gomock.Any(), false).Return(nil)\n\n\t\terr := tm.EnsureTopicExists(topic, npar, rfactor, config)\n\t\trequire.NoError(t, err)\n\t})\n\tt.Run(\"create-nowait\", func(t *testing.T) {\n\t\ttm, bm, ctrl := createTopicManager(t)\n\t\ttm.topicManagerConfig.CreateTopicTimeout = 0\n\t\tdefer ctrl.Finish()\n\t\tvar (\n\t\t\ttopic   = \"some-topic\"\n\t\t\tnpar    = 1\n\t\t\trfactor = 1\n\t\t\tconfig  = map[string]string{\n\t\t\t\t\"a\": \"a\",\n\t\t\t}\n\t\t)\n\n\t\tbm.client.EXPECT().RefreshMetadata().Return(nil)\n\t\tbm.client.EXPECT().Topics().Return(nil, nil)\n\n\t\tbm.admin.EXPECT().CreateTopic(gomock.Any(), gomock.Any(), false).Return(nil)\n\n\t\terr := tm.EnsureTopicExists(topic, npar, rfactor, config)\n\t\trequire.NoError(t, err)\n\t})\n\tt.Run(\"fail\", func(t *testing.T) {\n\t\ttm, bm, ctrl := createTopicManager(t)\n\t\tdefer ctrl.Finish()\n\t\tvar (\n\t\t\ttopic   = \"some-topic\"\n\t\t\tnpar    = 1\n\t\t\trfactor = 1\n\t\t\tconfig  = map[string]string{\n\t\t\t\t\"a\": \"a\",\n\t\t\t}\n\t\t\tretErr error = errors.New(\"some-error\")\n\t\t)\n\n\t\t// client.Topics() fails\n\t\tbm.client.EXPECT().RefreshMetadata().Return(nil)\n\t\tbm.client.EXPECT().Topics().Return(nil, retErr)\n\t\terr := tm.EnsureTopicExists(topic, npar, rfactor, config)\n\t\trequire.Error(t, err)\n\n\t\t// client.Partitions() fails\n\t\tbm.client.EXPECT().Topics().Return([]string{topic}, nil)\n\t\tbm.client.EXPECT().RefreshMetadata().Return(nil)\n\t\tbm.client.EXPECT().Partitions(topic).Return(nil, retErr)\n\t\terr = tm.EnsureTopicExists(topic, npar, rfactor, config)\n\t\trequire.Error(t, err)\n\n\t\t// client.DescribeConfig() fails\n\t\tbm.client.EXPECT().Topics().Return([]string{topic}, nil)\n\t\tbm.client.EXPECT().RefreshMetadata().Return(nil)\n\t\tbm.client.EXPECT().Partitions(topic).Return([]int32{0}, nil)\n\t\tcfg := sarama.NewConfig()\n\t\tcfg.Version = sarama.V0_11_0_0\n\t\tbm.client.EXPECT().Config().Return(cfg)\n\t\tbm.admin.EXPECT().DescribeConfig(gomock.Any()).Return(nil, retErr)\n\t\terr = tm.EnsureTopicExists(topic, npar, rfactor, config)\n\t\trequire.Error(t, err)\n\n\t\t// client.DescribeTopics() fails\n\t\tbm.client.EXPECT().Topics().Return([]string{topic}, nil)\n\t\tbm.client.EXPECT().RefreshMetadata().Return(nil)\n\t\tbm.client.EXPECT().Partitions(topic).Return([]int32{0}, nil)\n\t\tbm.client.EXPECT().Config().Return(cfg)\n\t\tbm.admin.EXPECT().DescribeConfig(gomock.Any()).Return([]sarama.ConfigEntry{{Name: \"a\", Value: \"a\"}}, nil)\n\t\tbm.admin.EXPECT().DescribeTopics(gomock.Any()).Return(nil, retErr)\n\t\terr = tm.EnsureTopicExists(topic, npar, rfactor, config)\n\t\trequire.Error(t, err)\n\t})\n}\n"
        },
        {
          "name": "view.go",
          "type": "blob",
          "size": 12.9228515625,
          "content": "package goka\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"sync\"\n\n\t\"github.com/IBM/sarama\"\n\t\"github.com/hashicorp/go-multierror\"\n\t\"github.com/lovoo/goka/multierr\"\n\t\"github.com/lovoo/goka/storage\"\n)\n\n// ViewState represents the state of the view\ntype ViewState int\n\nconst (\n\t// ViewStateIdle  - the view is not started yet\n\tViewStateIdle ViewState = iota\n\t// ViewStateInitializing - the view (i.e. at least one partition) is initializing\n\tViewStateInitializing\n\t// ViewStateConnecting - the view (i.e. at least one partition) is (re-)connecting\n\tViewStateConnecting\n\t// ViewStateCatchUp - the view (i.e. at least one partition) is still catching up\n\tViewStateCatchUp\n\t// ViewStateRunning - the view (i.e. all partitions) has caught up and is running\n\tViewStateRunning\n)\n\nfunc newViewSignal() *Signal {\n\treturn NewSignal(State(ViewStateIdle),\n\t\tState(ViewStateInitializing),\n\t\tState(ViewStateConnecting),\n\t\tState(ViewStateCatchUp),\n\t\tState(ViewStateRunning)).SetState(State(ViewStateIdle))\n}\n\n// Getter functions return a value for a key or an error. If no value exists for the key, nil is returned without errors.\ntype Getter func(string) (interface{}, error)\n\n// View is a materialized (i.e. persistent) cache of a group table.\ntype View struct {\n\tbrokers    []string\n\ttopic      string\n\topts       *voptions\n\tlog        logger\n\tpartitions []*PartitionTable\n\tconsumer   sarama.Consumer\n\ttmgr       TopicManager\n\tstate      *Signal\n}\n\n// NewView creates a new View object from a group.\nfunc NewView(brokers []string, topic Table, codec Codec, options ...ViewOption) (*View, error) {\n\toptions = append(\n\t\t// default options comes first\n\t\t[]ViewOption{\n\t\t\tWithViewClientID(fmt.Sprintf(\"goka-view-%s\", topic)),\n\t\t\tWithViewCallback(DefaultUpdate),\n\t\t\tWithViewStorageBuilder(storage.DefaultBuilder(DefaultViewStoragePath())),\n\t\t},\n\n\t\t// then the user passed options\n\t\toptions...,\n\t)\n\n\topts := new(voptions)\n\terr := opts.applyOptions(topic, codec, options...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Error applying user-defined options: %v\", err)\n\t}\n\n\tconsumer, err := opts.builders.consumerSarama(brokers, opts.clientID)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Error creating sarama consumer for brokers %+v: %v\", brokers, err)\n\t}\n\topts.tableCodec = codec\n\n\ttmgr, err := opts.builders.topicmgr(brokers)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Error creating topic manager: %v\", err)\n\t}\n\n\tv := &View{\n\t\tbrokers:  brokers,\n\t\ttopic:    string(topic),\n\t\topts:     opts,\n\t\tlog:      opts.log.Prefix(fmt.Sprintf(\"View %s\", topic)),\n\t\tconsumer: consumer,\n\t\ttmgr:     tmgr,\n\t\tstate:    newViewSignal(),\n\t}\n\n\tif err = v.createPartitions(brokers); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn v, err\n}\n\n// WaitRunning returns a channel that will be closed when the view enters the running state\nfunc (v *View) WaitRunning() <-chan struct{} {\n\treturn v.state.WaitForState(State(ViewStateRunning))\n}\n\nfunc (v *View) createPartitions(brokers []string) (rerr error) {\n\ttm, err := v.opts.builders.topicmgr(brokers)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Error creating topic manager: %v\", err)\n\t}\n\tdefer func() {\n\t\te := tm.Close()\n\t\tif e != nil && rerr == nil {\n\t\t\trerr = fmt.Errorf(\"Error closing topic manager: %v\", e)\n\t\t}\n\t}()\n\n\tpartitions, err := tm.Partitions(v.topic)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Error getting partitions for topic %s: %v\", v.topic, err)\n\t}\n\n\t// check assumption that partitions are gap-less\n\tfor i, p := range partitions {\n\t\tif i != int(p) {\n\t\t\treturn fmt.Errorf(\"Partition numbers are not sequential for topic %s\", v.topic)\n\t\t}\n\t}\n\n\tfor partID, p := range partitions {\n\t\tbackoff, err := v.opts.builders.backoff()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"Error creating backoff: %v\", err)\n\t\t}\n\t\tv.partitions = append(v.partitions, newPartitionTable(v.topic,\n\t\t\tp,\n\t\t\tv.consumer,\n\t\t\tv.tmgr,\n\t\t\tv.opts.updateCallback,\n\t\t\tv.opts.builders.storage,\n\t\t\tv.log.Prefix(fmt.Sprintf(\"PartTable-%d\", partID)),\n\t\t\tbackoff,\n\t\t\tv.opts.backoffResetTime,\n\t\t))\n\t}\n\n\treturn nil\n}\n\nfunc (v *View) runStateMerger(ctx context.Context) {\n\tvar (\n\t\tstates = make(map[int]PartitionStatus)\n\t\tm      sync.Mutex\n\t)\n\n\t// internal callback that will be called when the state of any\n\t// partition changes.\n\t// Then the \"lowest\" state of all partitions will be selected and\n\t// translated into the respective ViewState\n\tupdateViewState := func(idx int, state State) {\n\t\tm.Lock()\n\t\tdefer m.Unlock()\n\t\tstates[idx] = PartitionStatus(state)\n\n\t\tlowestState := PartitionStatus(-1)\n\n\t\tfor _, partitionState := range states {\n\t\t\tif lowestState == -1 || partitionState < lowestState {\n\t\t\t\tlowestState = partitionState\n\t\t\t}\n\t\t}\n\t\tnewState := ViewState(-1)\n\t\tswitch lowestState {\n\t\tcase PartitionStopped:\n\t\t\tnewState = ViewStateIdle\n\t\tcase PartitionInitializing:\n\t\t\tnewState = ViewStateInitializing\n\t\tcase PartitionConnecting:\n\t\t\tnewState = ViewStateConnecting\n\t\tcase PartitionRecovering:\n\t\t\tnewState = ViewStateCatchUp\n\t\tcase PartitionPreparing:\n\t\t\tnewState = ViewStateCatchUp\n\t\tcase PartitionRunning:\n\t\t\tnewState = ViewStateRunning\n\t\tdefault:\n\t\t\tv.log.Printf(\"State merger received unknown partition state: %v\", lowestState)\n\t\t}\n\n\t\tif newState != -1 {\n\t\t\tv.state.SetState(State(newState))\n\t\t}\n\t}\n\n\t// get a state change observer for all partitions\n\tfor idx, partition := range v.partitions {\n\t\tidx := idx\n\t\tpartition := partition\n\n\t\tobserver := partition.observeStateChanges()\n\t\t// create a goroutine that updates the view state based all partition states\n\t\tgo func() {\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase newState, ok := <-observer.C():\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\t// something has changed, so update the state\n\t\t\t\t\tupdateViewState(idx, newState)\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\tobserver.Stop()\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t}\n}\n\n// Run starts consuming the view's topic and saving updates in the local persistent cache.\n//\n// The view will shutdown in case of errors or when the context is closed.\n// It can be initialized with autoreconnect\n//\n//\tview := NewView(..., WithViewAutoReconnect())\n//\n// which makes the view internally reconnect in case of errors.\n// Then it will only stop by canceling the context (see example).\nfunc (v *View) Run(ctx context.Context) (rerr error) {\n\tv.log.Debugf(\"starting\")\n\tdefer v.log.Debugf(\"stopped\")\n\n\t// update the view state asynchronously by observing\n\t// the partition's state and translating that to the view\n\tv.runStateMerger(ctx)\n\tdefer v.state.SetState(State(ViewStateIdle))\n\n\t// close the view after running\n\tdefer func() {\n\t\trerr = multierror.Append(\n\t\t\trerr,\n\t\t\tv.close(),\n\t\t).ErrorOrNil()\n\t}()\n\n\trecoverErrg, recoverCtx := multierr.NewErrGroup(ctx)\n\n\tfor _, partition := range v.partitions {\n\t\tpartition := partition\n\t\trecoverErrg.Go(func() error {\n\t\t\treturn partition.SetupAndRecover(recoverCtx, v.opts.autoreconnect)\n\t\t})\n\t}\n\n\terr := recoverErrg.Wait().ErrorOrNil()\n\tif err != nil {\n\t\trerr = fmt.Errorf(\"Error recovering partitions for view %s: %v\", v.Topic(), err)\n\t\treturn\n\t}\n\n\tselect {\n\tcase <-ctx.Done():\n\t\treturn nil\n\tdefault:\n\t}\n\n\tcatchupErrg, catchupCtx := multierr.NewErrGroup(ctx)\n\n\tfor _, partition := range v.partitions {\n\t\tpartition := partition\n\t\tcatchupErrg.Go(func() error {\n\t\t\treturn partition.CatchupForever(catchupCtx, v.opts.autoreconnect)\n\t\t})\n\t}\n\n\terr = catchupErrg.Wait().ErrorOrNil()\n\tif err != nil {\n\t\trerr = fmt.Errorf(\"Error catching up partitions for view %s: %v\", v.Topic(), err)\n\t}\n\treturn\n}\n\n// close closes all storage partitions\nfunc (v *View) close() error {\n\terrg, _ := multierr.NewErrGroup(context.Background())\n\tfor _, p := range v.partitions {\n\t\tp := p\n\t\terrg.Go(func() error {\n\t\t\treturn p.Close()\n\t\t})\n\t}\n\tv.partitions = nil\n\treturn errg.Wait().ErrorOrNil()\n}\n\nfunc (v *View) hash(key string) (int32, error) {\n\t// create a new hasher every time. Alternative would be to store the hash in\n\t// view and every time reset the hasher (ie, hasher.Reset()). But that would\n\t// also require us to protect the access of the hasher with a mutex.\n\thasher := v.opts.hasher()\n\n\t_, err := hasher.Write([]byte(key))\n\tif err != nil {\n\t\treturn -1, err\n\t}\n\thash := int32(hasher.Sum32())\n\tif hash < 0 {\n\t\thash = -hash\n\t}\n\tif len(v.partitions) == 0 {\n\t\treturn 0, errors.New(\"no partitions found\")\n\t}\n\treturn hash % int32(len(v.partitions)), nil\n}\n\nfunc (v *View) find(key string) (*PartitionTable, error) {\n\th, err := v.hash(key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn v.partitions[h], nil\n}\n\n// Topic returns  the view's topic\nfunc (v *View) Topic() string {\n\treturn v.topic\n}\n\n// Get returns the value for the key in the view, if exists. Nil if it doesn't.\n// Get can be called by multiple goroutines concurrently.\n// Get can only be called after Recovered returns true.\nfunc (v *View) Get(key string) (interface{}, error) {\n\tif v.state.IsState(State(ViewStateIdle)) || v.state.IsState(State(ViewStateInitializing)) {\n\t\treturn nil, fmt.Errorf(\"View is either not running, not correctly initialized or stopped again. It's not safe to retrieve values\")\n\t}\n\n\t// find partition where key is located\n\tpartTable, err := v.find(key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// get key and return\n\tdata, err := partTable.Get(key)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error getting value (key %s): %v\", key, err)\n\t} else if data == nil {\n\t\treturn nil, nil\n\t}\n\n\t// decode value\n\tvalue, err := v.opts.tableCodec.Decode(data)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error decoding value (key %s): %v\", key, err)\n\t}\n\n\t// if the key does not exist the return value is nil\n\treturn value, nil\n}\n\n// Has checks whether a value for passed key exists in the view.\nfunc (v *View) Has(key string) (bool, error) {\n\t// find partition where key is located\n\tpartTable, err := v.find(key)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\n\treturn partTable.Has(key)\n}\n\n// Iterator returns an iterator that iterates over the state of the View.\nfunc (v *View) Iterator() (Iterator, error) {\n\titers := make([]storage.Iterator, 0, len(v.partitions))\n\tfor i := range v.partitions {\n\t\titer, err := v.partitions[i].Iterator()\n\t\tif err != nil {\n\t\t\t// release already opened iterators\n\t\t\tfor i := range iters {\n\t\t\t\titers[i].Release()\n\t\t\t}\n\n\t\t\treturn nil, fmt.Errorf(\"error opening partition iterator: %v\", err)\n\t\t}\n\n\t\titers = append(iters, iter)\n\t}\n\n\treturn &iterator{\n\t\titer:  storage.NewMultiIterator(iters),\n\t\tcodec: v.opts.tableCodec,\n\t}, nil\n}\n\n// IteratorWithRange returns an iterator that iterates over the state of the View. This iterator is build using the range.\nfunc (v *View) IteratorWithRange(start, limit string) (Iterator, error) {\n\titers := make([]storage.Iterator, 0, len(v.partitions))\n\tfor i := range v.partitions {\n\t\titer, err := v.partitions[i].IteratorWithRange([]byte(start), []byte(limit))\n\t\tif err != nil {\n\t\t\t// release already opened iterators\n\t\t\tfor i := range iters {\n\t\t\t\titers[i].Release()\n\t\t\t}\n\n\t\t\treturn nil, fmt.Errorf(\"error opening partition iterator: %v\", err)\n\t\t}\n\n\t\titers = append(iters, iter)\n\t}\n\n\treturn &iterator{\n\t\titer:  storage.NewMultiIterator(iters),\n\t\tcodec: v.opts.tableCodec,\n\t}, nil\n}\n\n// Evict removes the given key only from the local cache. In order to delete a\n// key from Kafka and other Views, context.Delete should be used on a Processor.\nfunc (v *View) Evict(key string) error {\n\ts, err := v.find(key)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn s.Delete(key)\n}\n\n// Recovered returns true when the view has caught up with events from kafka.\nfunc (v *View) Recovered() bool {\n\t// no partitions --> never recover\n\t// Otherwise we might mask errors of initializing the view\n\tif len(v.partitions) == 0 {\n\t\treturn false\n\t}\n\n\tfor _, p := range v.partitions {\n\t\tif !p.IsRecovered() {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// CurrentState returns the current ViewState of the view\n// This is useful for polling e.g. when implementing health checks or metrics\nfunc (v *View) CurrentState() ViewState {\n\treturn ViewState(v.state.State())\n}\n\n// ObserveStateChanges returns a StateChangeObserver that allows to handle state changes of the view\n// by reading from a channel.\n// It is crucial to continuously read from that channel, otherwise the View might deadlock upon\n// state changes.\n// If the observer is not needed, the caller must call observer.Stop()\n//\n// Example\n//\n//\tview := goka.NewView(...)\n//\tgo view.Run(ctx)\n//\n//\tgo func(){\n//\t  obs := view.ObserveStateChanges()\n//\t  defer obs.Stop()\n//\t  for {\n//\t    select{\n//\t      case state, ok := <-obs.C:\n//\t        // handle state (or closed channel)\n//\t      case <-ctx.Done():\n//\t    }\n//\t  }\n//\t}()\nfunc (v *View) ObserveStateChanges() *StateChangeObserver {\n\treturn v.state.ObserveStateChange()\n}\n\n// Stats returns a set of performance metrics of the view.\nfunc (v *View) Stats(ctx context.Context) *ViewStats {\n\treturn v.statsWithContext(ctx)\n}\n\nfunc (v *View) statsWithContext(ctx context.Context) *ViewStats {\n\tvar (\n\t\tm     sync.Mutex\n\t\tstats = newViewStats()\n\t)\n\terrg, ctx := multierr.NewErrGroup(ctx)\n\n\tfor _, partTable := range v.partitions {\n\t\tpartTable := partTable\n\n\t\terrg.Go(func() error {\n\t\t\ttableStats := partTable.fetchStats(ctx)\n\t\t\tm.Lock()\n\t\t\tdefer m.Unlock()\n\n\t\t\tif tableStats != nil {\n\t\t\t\tstats.Partitions[partTable.partition] = tableStats\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t}\n\n\terr := errg.Wait().ErrorOrNil()\n\tif err != nil {\n\t\tv.log.Printf(\"Error retrieving stats: %v\", err)\n\t}\n\treturn stats\n}\n"
        },
        {
          "name": "view_test.go",
          "type": "blob",
          "size": 19.4462890625,
          "content": "package goka\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"hash\"\n\t\"log\"\n\t\"strconv\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/IBM/sarama\"\n\t\"github.com/golang/mock/gomock\"\n\t\"github.com/lovoo/goka/codec\"\n\t\"github.com/lovoo/goka/storage\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nvar (\n\tviewTestRecoveredMessages int\n\tviewTestGroup             Group = \"group-name\"\n\tviewTestTopic                   = tableName(viewTestGroup)\n)\n\n// constHasher implements a hasher that will always return the specified\n// partition. Doesn't properly implement the Hash32 interface, use only in\n// tests.\ntype constHasher struct {\n\tpartition uint32\n\treturnErr bool\n}\n\nfunc (ch *constHasher) Sum(b []byte) []byte {\n\treturn nil\n}\n\nfunc (ch *constHasher) Sum32() uint32 {\n\treturn ch.partition\n}\n\nfunc (ch *constHasher) BlockSize() int {\n\treturn 0\n}\n\nfunc (ch *constHasher) Reset() {}\n\nfunc (ch *constHasher) Size() int { return 4 }\n\nfunc (ch *constHasher) Write(p []byte) (int, error) {\n\tif ch.returnErr {\n\t\treturn 0, fmt.Errorf(\"constHasher write error\")\n\t}\n\treturn len(p), nil\n}\n\nfunc (ch *constHasher) ReturnError() {\n\tch.returnErr = true\n}\n\n// newConstHasher creates a constant hasher that hashes any value to 0.\nfunc newConstHasher(part uint32) *constHasher {\n\treturn &constHasher{partition: part}\n}\n\nfunc createTestView(t *testing.T, consumer sarama.Consumer) (*View, *builderMock, *gomock.Controller) {\n\tctrl := gomock.NewController(t)\n\tbm := newBuilderMock(ctrl)\n\tviewTestRecoveredMessages = 0\n\topts := &voptions{\n\t\tlog:        defaultLogger,\n\t\ttableCodec: new(codec.String),\n\t\tupdateCallback: func(ctx UpdateContext, s storage.Storage, key string, value []byte) error {\n\t\t\tif err := DefaultUpdate(ctx, s, key, value); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tviewTestRecoveredMessages++\n\t\t\treturn nil\n\t\t},\n\t\thasher: DefaultHasher(),\n\t}\n\topts.builders.storage = bm.getStorageBuilder()\n\topts.builders.topicmgr = bm.getTopicManagerBuilder()\n\topts.builders.consumerSarama = func(brokers []string, clientID string) (sarama.Consumer, error) {\n\t\treturn consumer, nil\n\t}\n\topts.builders.backoff = DefaultBackoffBuilder\n\n\tview := &View{\n\t\ttopic: viewTestTopic,\n\t\topts:  opts,\n\t\tlog:   opts.log,\n\t\tstate: newViewSignal(),\n\t}\n\treturn view, bm, ctrl\n}\n\nfunc TestView_hash(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tview, _, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tview.partitions = []*PartitionTable{{}}\n\n\t\th, err := view.hash(\"a\")\n\t\trequire.NoError(t, err)\n\t\trequire.True(t, h == 0)\n\t})\n\tt.Run(\"fail_hash\", func(t *testing.T) {\n\t\tview, _, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tview.partitions = []*PartitionTable{{}}\n\t\tview.opts.hasher = func() hash.Hash32 {\n\t\t\thasher := newConstHasher(0)\n\t\t\thasher.ReturnError()\n\t\t\treturn hasher\n\t\t}\n\n\t\t_, err := view.hash(\"a\")\n\t\trequire.Error(t, err)\n\t})\n\tt.Run(\"fail_no_partition\", func(t *testing.T) {\n\t\tview, _, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\t_, err := view.hash(\"a\")\n\t\trequire.Error(t, err)\n\t})\n}\n\nfunc TestView_find(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tview, _, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tkey   = \"some-key\"\n\t\t\tproxy = &storageProxy{}\n\t\t)\n\t\tview.partitions = []*PartitionTable{\n\t\t\t{\n\t\t\t\tst:    proxy,\n\t\t\t\tstate: newPartitionTableState().SetState(State(PartitionRunning)),\n\t\t\t},\n\t\t}\n\t\tview.opts.hasher = func() hash.Hash32 {\n\t\t\thasher := newConstHasher(0)\n\t\t\treturn hasher\n\t\t}\n\n\t\tst, err := view.find(key)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, st, view.partitions[0])\n\t})\n\tt.Run(\"fail\", func(t *testing.T) {\n\t\tview, _, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tview.partitions = []*PartitionTable{{}}\n\t\tview.opts.hasher = func() hash.Hash32 {\n\t\t\thasher := newConstHasher(0)\n\t\t\thasher.ReturnError()\n\t\t\treturn hasher\n\t\t}\n\n\t\t_, err := view.find(\"a\")\n\t\trequire.Error(t, err)\n\t})\n}\n\nfunc TestView_Get(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tview, bm, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tproxy = &storageProxy{\n\t\t\t\tStorage:   bm.mst,\n\t\t\t\tpartition: 0,\n\t\t\t\tupdate:    updateCallbackNoop,\n\t\t\t}\n\t\t\tkey         = \"some-key\"\n\t\t\tvalue int64 = 3\n\t\t)\n\t\tview.partitions = []*PartitionTable{\n\t\t\t{\n\t\t\t\tst:    proxy,\n\t\t\t\tstate: newPartitionTableState().SetState(State(PartitionRunning)),\n\t\t\t},\n\t\t}\n\t\tview.opts.tableCodec = &codec.Int64{}\n\t\tview.state.SetState(State(ViewStateRunning))\n\n\t\tbm.mst.EXPECT().Get(key).Return([]byte(strconv.FormatInt(value, 10)), nil)\n\n\t\tret, err := view.Get(key)\n\t\trequire.NoError(t, err)\n\t\trequire.True(t, ret == value)\n\t})\n\tt.Run(\"succeed_nil\", func(t *testing.T) {\n\t\tview, bm, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tproxy = &storageProxy{\n\t\t\t\tStorage:   bm.mst,\n\t\t\t\tpartition: 0,\n\t\t\t\tupdate:    updateCallbackNoop,\n\t\t\t}\n\t\t\tkey = \"some-key\"\n\t\t)\n\t\tview.partitions = []*PartitionTable{\n\t\t\t{\n\t\t\t\tst:    proxy,\n\t\t\t\tstate: newPartitionTableState().SetState(State(PartitionRunning)),\n\t\t\t},\n\t\t}\n\t\tview.opts.tableCodec = &codec.Int64{}\n\t\tview.state.SetState(State(ViewStateRunning))\n\t\tbm.mst.EXPECT().Get(key).Return(nil, nil)\n\n\t\tret, err := view.Get(key)\n\t\trequire.NoError(t, err)\n\t\trequire.True(t, ret == nil)\n\t})\n\tt.Run(\"fail_get\", func(t *testing.T) {\n\t\tview, bm, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tproxy = &storageProxy{\n\t\t\t\tStorage:   bm.mst,\n\t\t\t\tpartition: 0,\n\t\t\t\tupdate:    updateCallbackNoop,\n\t\t\t}\n\t\t\tkey          = \"some-key\"\n\t\t\terrRet error = fmt.Errorf(\"get failed\")\n\t\t)\n\t\tview.partitions = []*PartitionTable{\n\t\t\t{\n\t\t\t\tst:    proxy,\n\t\t\t\tstate: newPartitionTableState().SetState(State(PartitionRunning)),\n\t\t\t},\n\t\t}\n\t\tview.opts.tableCodec = &codec.Int64{}\n\t\tview.state.SetState(State(ViewStateRunning))\n\t\tbm.mst.EXPECT().Get(key).Return(nil, errRet)\n\n\t\t_, err := view.Get(key)\n\t\trequire.Error(t, err)\n\t})\n}\n\nfunc TestView_Has(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tview, bm, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tproxy = &storageProxy{\n\t\t\t\tStorage: bm.mst,\n\t\t\t}\n\t\t\tkey = \"some-key\"\n\t\t\thas = true\n\t\t)\n\t\tview.partitions = []*PartitionTable{\n\t\t\t{\n\t\t\t\tst:    proxy,\n\t\t\t\tstate: newPartitionTableState().SetState(State(PartitionRunning)),\n\t\t\t},\n\t\t}\n\t\tview.opts.hasher = func() hash.Hash32 {\n\t\t\thasher := newConstHasher(0)\n\t\t\treturn hasher\n\t\t}\n\n\t\tbm.mst.EXPECT().Has(key).Return(has, nil)\n\n\t\tret, err := view.Has(key)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, ret, has)\n\t})\n\tt.Run(\"succeed_false\", func(t *testing.T) {\n\t\tview, bm, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tproxy = &storageProxy{\n\t\t\t\tStorage: bm.mst,\n\t\t\t}\n\t\t\tkey = \"some-key\"\n\t\t\thas = false\n\t\t)\n\t\tview.partitions = []*PartitionTable{\n\t\t\t{\n\t\t\t\tst:    proxy,\n\t\t\t\tstate: newPartitionTableState().SetState(State(PartitionRunning)),\n\t\t\t},\n\t\t}\n\t\tview.opts.hasher = func() hash.Hash32 {\n\t\t\thasher := newConstHasher(0)\n\t\t\treturn hasher\n\t\t}\n\n\t\tbm.mst.EXPECT().Has(key).Return(has, nil)\n\n\t\tret, err := view.Has(key)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, ret, has)\n\t})\n\tt.Run(\"fail_err\", func(t *testing.T) {\n\t\tview, _, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tkey = \"some-key\"\n\t\t\thas = false\n\t\t)\n\t\tview.partitions = []*PartitionTable{{}}\n\t\tview.opts.hasher = func() hash.Hash32 {\n\t\t\thasher := newConstHasher(0)\n\t\t\thasher.ReturnError()\n\t\t\treturn hasher\n\t\t}\n\n\t\tret, err := view.Has(key)\n\t\trequire.Error(t, err)\n\t\trequire.True(t, ret == has)\n\t})\n}\n\nfunc TestView_Evict(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tview, bm, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tkey   = \"some-key\"\n\t\t\tproxy = &storageProxy{\n\t\t\t\tStorage: bm.mst,\n\t\t\t}\n\t\t)\n\t\tview.partitions = []*PartitionTable{\n\t\t\t{st: proxy},\n\t\t}\n\t\tview.opts.hasher = func() hash.Hash32 {\n\t\t\thasher := newConstHasher(0)\n\t\t\treturn hasher\n\t\t}\n\t\tbm.mst.EXPECT().Delete(key).Return(nil)\n\n\t\terr := view.Evict(key)\n\t\trequire.NoError(t, err)\n\t})\n}\n\nfunc TestView_Recovered(t *testing.T) {\n\tt.Run(\"true\", func(t *testing.T) {\n\t\tview, _, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\thasRecovered := true\n\t\tview.partitions = []*PartitionTable{\n\t\t\t{state: NewSignal(State(PartitionRunning)).SetState(State(PartitionRunning))},\n\t\t}\n\t\tret := view.Recovered()\n\t\trequire.True(t, ret == hasRecovered)\n\t})\n\tt.Run(\"true\", func(t *testing.T) {\n\t\tview, _, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\thasRecovered := false\n\t\tview.partitions = []*PartitionTable{\n\t\t\t{state: NewSignal(State(PartitionRunning), State(PartitionRecovering)).SetState(State(PartitionRecovering))},\n\t\t\t{state: NewSignal(State(PartitionRunning)).SetState(State(PartitionRunning))},\n\t\t}\n\t\tret := view.Recovered()\n\t\trequire.True(t, ret == hasRecovered)\n\t})\n}\n\nfunc TestView_Topic(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tview, _, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tret := view.Topic()\n\t\trequire.True(t, ret == viewTestTopic)\n\t})\n}\n\nfunc TestView_close(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tview, bm, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tproxy := &storageProxy{\n\t\t\tStorage: bm.mst,\n\t\t}\n\t\tview.partitions = []*PartitionTable{\n\t\t\t{st: proxy},\n\t\t\t{st: proxy},\n\t\t\t{st: proxy},\n\t\t}\n\t\tbm.mst.EXPECT().Close().Return(nil).AnyTimes()\n\n\t\terr := view.close()\n\t\trequire.NoError(t, err)\n\t\trequire.True(t, len(view.partitions) == 0)\n\t})\n\tt.Run(\"fail\", func(t *testing.T) {\n\t\tview, bm, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tproxy = &storageProxy{\n\t\t\t\tStorage: bm.mst,\n\t\t\t}\n\t\t\tretErr error = fmt.Errorf(\"some-error\")\n\t\t)\n\t\tview.partitions = []*PartitionTable{\n\t\t\t{st: proxy},\n\t\t\t{st: proxy},\n\t\t\t{st: proxy},\n\t\t}\n\t\tbm.mst.EXPECT().Close().Return(retErr).AnyTimes()\n\n\t\terr := view.close()\n\t\trequire.Error(t, err)\n\t\trequire.True(t, len(view.partitions) == 0)\n\t})\n}\n\nfunc TestView_Terminate(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tview, bm, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tproxy = &storageProxy{\n\t\t\t\tStorage: bm.mst,\n\t\t\t}\n\t\t\tisAutoReconnect = true\n\t\t)\n\t\tview.partitions = []*PartitionTable{\n\t\t\t{st: proxy},\n\t\t\t{st: proxy},\n\t\t\t{st: proxy},\n\t\t}\n\t\tview.opts.autoreconnect = isAutoReconnect\n\t\tbm.mst.EXPECT().Close().Return(nil).AnyTimes()\n\n\t\tret := view.close()\n\t\trequire.NoError(t, ret)\n\t\trequire.True(t, len(view.partitions) == 0)\n\t})\n\n\tt.Run(\"succeed_twice\", func(t *testing.T) {\n\t\tview, bm, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tproxy = &storageProxy{\n\t\t\t\tStorage: bm.mst,\n\t\t\t}\n\t\t\tisAutoReconnect = true\n\t\t)\n\t\tview.partitions = []*PartitionTable{\n\t\t\t{st: proxy},\n\t\t\t{st: proxy},\n\t\t\t{st: proxy},\n\t\t}\n\t\tview.opts.autoreconnect = isAutoReconnect\n\t\tbm.mst.EXPECT().Close().Return(nil).AnyTimes()\n\n\t\tret := view.close()\n\t\trequire.NoError(t, ret)\n\t\trequire.True(t, len(view.partitions) == 0)\n\t\tret = view.close()\n\t\trequire.NoError(t, ret)\n\t\trequire.True(t, len(view.partitions) == 0)\n\t})\n\n\tt.Run(\"fail\", func(t *testing.T) {\n\t\tview, bm, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tproxy = &storageProxy{\n\t\t\t\tStorage: bm.mst,\n\t\t\t}\n\t\t\tretErr          error = fmt.Errorf(\"some-error\")\n\t\t\tisAutoReconnect       = true\n\t\t)\n\t\tview.partitions = []*PartitionTable{\n\t\t\t{st: proxy},\n\t\t\t{st: proxy},\n\t\t\t{st: proxy},\n\t\t}\n\t\tview.opts.autoreconnect = isAutoReconnect\n\t\tbm.mst.EXPECT().Close().Return(retErr).AnyTimes()\n\n\t\tret := view.close()\n\t\trequire.Error(t, ret)\n\t\trequire.True(t, len(view.partitions) == 0)\n\t})\n}\n\nfunc TestView_Run(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tview, bm, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\toldest int64\n\t\t\tnewest int64 = 10\n\t\t\t// local     int64          = oldest\n\t\t\tconsumer  = defaultSaramaAutoConsumerMock(t)\n\t\t\tpartition int32\n\t\t\tcount     int64\n\t\t\tupdateCB  UpdateCallback = func(ctx UpdateContext, s storage.Storage, key string, value []byte) error {\n\t\t\t\tatomic.AddInt64(&count, 1)\n\t\t\t\treturn nil\n\t\t\t}\n\t\t)\n\t\tbm.useMemoryStorage()\n\n\t\tpt := newPartitionTable(\n\t\t\tviewTestTopic,\n\t\t\tpartition,\n\t\t\tconsumer,\n\t\t\tbm.tmgr,\n\t\t\tupdateCB,\n\t\t\tbm.getStorageBuilder(),\n\t\t\tdefaultLogger,\n\t\t\tNewSimpleBackoff(defaultBackoffStep, defaultBackoffMax),\n\t\t\ttime.Minute,\n\t\t)\n\n\t\tpt.consumer = consumer\n\t\tview.partitions = []*PartitionTable{pt}\n\t\tview.state = newViewSignal()\n\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetOldest).Return(oldest, nil).AnyTimes()\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetNewest).Return(newest, nil).AnyTimes()\n\t\tpartConsumer := consumer.ExpectConsumePartition(viewTestTopic, partition, anyOffset)\n\t\tfor i := 0; i < 10; i++ {\n\t\t\tpartConsumer.YieldMessage(&sarama.ConsumerMessage{})\n\t\t}\n\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\t\tgo func() {\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\tdefault:\n\t\t\t\t}\n\t\t\t\tif atomic.LoadInt64(&count) == 10 {\n\t\t\t\t\ttime.Sleep(time.Millisecond * 10)\n\t\t\t\t\tcancel()\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\n\t\tret := view.Run(ctx)\n\t\trequire.NoError(t, ret)\n\t})\n\tt.Run(\"fail\", func(t *testing.T) {\n\t\tview, bm, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tvar (\n\t\t\tpartition int32\n\t\t\tconsumer  = defaultSaramaAutoConsumerMock(t)\n\t\t\tupdateCB  UpdateCallback\n\t\t\tretErr    = fmt.Errorf(\"run error\")\n\t\t)\n\t\tbm.useMemoryStorage()\n\n\t\tpt := newPartitionTable(\n\t\t\tviewTestTopic,\n\t\t\tpartition,\n\t\t\tconsumer,\n\t\t\tbm.tmgr,\n\t\t\tupdateCB,\n\t\t\tbm.getStorageBuilder(),\n\t\t\tdefaultLogger,\n\t\t\tNewSimpleBackoff(defaultBackoffStep, defaultBackoffMax),\n\t\t\ttime.Minute,\n\t\t)\n\n\t\tpt.consumer = consumer\n\t\tview.partitions = []*PartitionTable{pt}\n\t\tview.state = newViewSignal()\n\n\t\tbm.mst.EXPECT().GetOffset(gomock.Any()).Return(int64(0), retErr).AnyTimes()\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetNewest).Return(sarama.OffsetNewest, retErr).AnyTimes()\n\t\tbm.tmgr.EXPECT().GetOffset(pt.topic, pt.partition, sarama.OffsetOldest).Return(sarama.OffsetOldest, retErr).AnyTimes()\n\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\n\t\tret := view.Run(ctx)\n\t\trequire.Error(t, ret)\n\t})\n}\n\nfunc TestView_createPartitions(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tview, bm, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tvar partition int32\n\t\tbm.tmgr.EXPECT().Partitions(viewTestTopic).Return([]int32{partition}, nil)\n\t\tbm.tmgr.EXPECT().Close()\n\n\t\tret := view.createPartitions([]string{\"\"})\n\t\trequire.NoError(t, ret)\n\t\trequire.True(t, len(view.partitions) == 1)\n\t})\n\tt.Run(\"fail_tmgr\", func(t *testing.T) {\n\t\tview, bm, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tvar retErr error = fmt.Errorf(\"tmgr-partition-error\")\n\t\tbm.tmgr.EXPECT().Partitions(viewTestTopic).Return(nil, retErr)\n\t\tbm.tmgr.EXPECT().Close()\n\n\t\tret := view.createPartitions([]string{\"\"})\n\t\trequire.Error(t, ret)\n\t\trequire.True(t, len(view.partitions) == 0)\n\t})\n}\n\nfunc TestView_WaitRunning(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tview, _, ctrl := createTestView(t, NewMockAutoConsumer(t, DefaultConfig()))\n\t\tdefer ctrl.Finish()\n\n\t\tview.state = newViewSignal().SetState(State(ViewStateRunning))\n\n\t\tvar isRunning bool\n\t\tselect {\n\t\tcase <-view.WaitRunning():\n\t\t\tisRunning = true\n\t\tcase <-time.After(time.Second):\n\t\t}\n\n\t\trequire.True(t, isRunning == true)\n\t})\n}\n\nfunc TestView_NewView(t *testing.T) {\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tctrl := gomock.NewController(t)\n\t\tdefer ctrl.Finish()\n\t\tbm := newBuilderMock(ctrl)\n\n\t\tvar partition int32\n\t\tbm.tmgr.EXPECT().Partitions(viewTestTopic).Return([]int32{partition}, nil).AnyTimes()\n\t\tbm.tmgr.EXPECT().Close().AnyTimes()\n\n\t\tview, err := NewView([]string{\"\"}, Table(viewTestTopic), &codec.Int64{}, []ViewOption{\n\t\t\tWithViewTopicManagerBuilder(bm.getTopicManagerBuilder()),\n\t\t\tWithViewConsumerSaramaBuilder(func(brokers []string, clientID string) (sarama.Consumer, error) {\n\t\t\t\treturn NewMockAutoConsumer(t, DefaultConfig()), nil\n\t\t\t}),\n\t\t}...)\n\t\trequire.NoError(t, err)\n\t\trequire.NotNil(t, view)\n\t})\n\tt.Run(\"succeed\", func(t *testing.T) {\n\t\tctrl := gomock.NewController(t)\n\t\tdefer ctrl.Finish()\n\t\tbm := newBuilderMock(ctrl)\n\n\t\tvar retErr error = fmt.Errorf(\"tmgr-error\")\n\t\tbm.tmgr.EXPECT().Partitions(viewTestTopic).Return(nil, retErr).AnyTimes()\n\t\tbm.tmgr.EXPECT().Close().AnyTimes()\n\n\t\tview, err := NewView([]string{\"\"}, Table(viewTestTopic), &codec.Int64{}, []ViewOption{\n\t\t\tWithViewTopicManagerBuilder(bm.getTopicManagerBuilder()),\n\t\t\tWithViewConsumerSaramaBuilder(func(brokers []string, clientID string) (sarama.Consumer, error) {\n\t\t\t\treturn NewMockAutoConsumer(t, DefaultConfig()), nil\n\t\t\t}),\n\t\t}...)\n\t\trequire.Error(t, err)\n\t\trequire.Nil(t, view)\n\t})\n}\n\n// This example shows how views are typically created and used\n// in the most basic way.\nfunc ExampleView() {\n\t// create a new view\n\tview, err := NewView([]string{\"localhost:9092\"},\n\t\t\"input-topic\",\n\t\tnew(codec.String))\n\tif err != nil {\n\t\tlog.Fatalf(\"error creating view: %v\", err)\n\t}\n\n\t// provide a cancelable\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\t// start the view\n\tdone := make(chan struct{})\n\tgo func() {\n\t\tdefer close(done)\n\t\terr := view.Run(ctx)\n\t\tif err != nil {\n\t\t\tlog.Fatalf(\"Error running view: %v\", err)\n\t\t}\n\t}()\n\n\t// wait for the view to be recovered\n\n\t// Option A: by polling\n\tfor !view.Recovered() {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase <-time.After(time.Second):\n\t\t}\n\t}\n\n\t// Option B: by waiting for the signal\n\t<-view.WaitRunning()\n\n\t// retrieve a value from the view\n\tval, err := view.Get(\"some-key\")\n\tif err != nil {\n\t\tlog.Fatalf(\"Error getting item from view: %v\", err)\n\t}\n\n\tif val != nil {\n\t\t// cast it to string\n\t\t// no need for type assertion, if it was not that type, the codec would've failed\n\t\tlog.Printf(\"got value %s\", val.(string))\n\t}\n\n\thas, err := view.Has(\"some-key\")\n\tif err != nil {\n\t\tlog.Fatalf(\"Error getting item from view: %v\", err)\n\t}\n\n\t_ = has\n\n\t// stop the view and wait for it to shut down before returning\n\tcancel()\n\t<-done\n}\n\nfunc ExampleView_autoreconnect() {\n\t// create a new view\n\tview, err := NewView([]string{\"localhost:9092\"},\n\t\t\"input-topic\",\n\t\tnew(codec.String),\n\n\t\t// Automatically reconnect in case of errors. This is useful for services where availability\n\t\t// is more important than the data being up to date in case of kafka connection issues.\n\t\tWithViewAutoReconnect(),\n\n\t\t// Reconnect uses a default backoff mechanism, that can be modified by providing\n\t\t// a custom backoff builder using\n\t\t// WithViewBackoffBuilder(customBackoffBuilder),\n\n\t\t// When the view is running successfully for some time, the backoff is reset.\n\t\t// This time range can be modified using\n\t\t// WithViewBackoffResetTimeout(3*time.Second),\n\t)\n\tif err != nil {\n\t\tlog.Fatalf(\"error creating view: %v\", err)\n\t}\n\n\tctx, cancel := context.WithCancel(context.Background())\n\t// start the view\n\tdone := make(chan struct{})\n\tgo func() {\n\t\tdefer close(done)\n\t\terr := view.Run(ctx)\n\t\tif err != nil {\n\t\t\tlog.Fatalf(\"Error running view: %v\", err)\n\t\t}\n\t}()\n\n\t<-view.WaitRunning()\n\t// at this point we can safely use the view with Has/Get/Iterate,\n\t// even if the kafka connection is lost\n\n\t// Stop the view and wait for it to shutdown before returning\n\tcancel()\n\t<-done\n}\n"
        },
        {
          "name": "web",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}