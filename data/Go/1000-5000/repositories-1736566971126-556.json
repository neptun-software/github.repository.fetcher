{
  "metadata": {
    "timestamp": 1736566971126,
    "page": 556,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "oklog/oklog",
      "stars": 2980,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.2802734375,
          "content": "data/\ndist/\nvendor/\n\n# Compiled Object files, Static and Dynamic libs (Shared Objects)\n*.o\n*.a\n*.so\n\n# Folders\n_obj\n_test\n\n# Architecture specific extensions/prefixes\n*.[568vq]\n[568vq].out\n\n*.cgo1.go\n*.cgo2.c\n_cgo_defun.c\n_cgo_gotypes.go\n_cgo_export.*\n\n_testmain.go\n\n*.exe\n*.test\n*.prof\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.2060546875,
          "content": "language: go\nsudo: false\ngo:\n  - 1.x\n  - tip\ninstall:\n  - go get github.com/golang/dep/cmd/dep\n  - dep ensure\n  - go build ./{cmd,pkg}/...\nscript:\n  - go vet ./{cmd,pkg}/...\n  - go test -v -race ./{cmd,pkg}/...\n"
        },
        {
          "name": "DESIGN.md",
          "type": "blob",
          "size": 19.0185546875,
          "content": "# Design\n\nIn this document, we first describe the system at a high level.\nThen, we introduce constraints and invariants to reify the problem domain.\nWe move methodically toward a concrete solution, describing key components and behaviors along the way.\n\n## Producers and consumers\n\nWe have a large and dynamic set of producers emitting a stream of log records.\nThose records should be made available for searching by a consumer.\n\n```\n     +-----------+\nP -> |           |\nP -> |     ?     | -> C\nP -> |           |\n     +-----------+\n```\n\nThe producers are concerned with getting their records consumed and persisted as quickly as possible.\nIn case of failure, some use cases prefer backpressure (e.g. event logs) and others buffering and dropping (e.g. application logs).\nBut in both cases, the immediate receiving component should be optimized for fast sequential writes.\n\nThe consumers are concerned with getting responses to their queries as quickly and accurately as possible.\nBecause we've defined queries to require time bounds, we claim we can solve the problem with grep over time-partitioned data files.\nSo the final format of records on disk should be a global merge of all producer streams, partitioned by time.\n\n```\n     +-------------------+\nP -> | R                 |\nP -> | R     ?     R R R | -> C\nP -> | R                 |\n     +-------------------+\n```\n\n## Operational details\n\nWe will have many producers, order of thousands.\n(A producer for us is the application process, plus a forwarding agent.)\nOur log system will necessarily be smaller than the production system it is serving.\nSo we will have multiple ingesters, and each one will need to handle writes from multiple producers.\n\nAlso, we want to serve production systems that generate a lot of log data.\nTherefore, we won't make reductive assumptions about data volume.\nWe assume even a minimum working set of log data will be too large for a single node's storage.\nTherefore, consumers will necessarily have to query multiple nodes for results.\nThis implies the final, time-partitioned data set will be distributed and replicated.\n\n```\n          +---+           +---+\nP -> F -> | I |           | Q | --.\nP -> F -> |   |           +---+   |\n          +---+           +---+   '->\n          +---+     ?     | Q | ----> C\nP -> F -> | I |           +---+   .->\nP -> F -> |   |           +---+   |\nP -> F -> |   |           | Q | --'\n          +---+           +---+\n```\n\nWe have now introduced distribution, which means we must address coördination.\n\n## Coördination\n\nCoördination is the death of distributed systems.\nOur log system will be coördination-free.\nLet's see what that requires at each stage.\n\nProducers -- or, more accurately, forwarders -- need to be able to connect to any ingester instance and begin emitting records.\nThose records should be persisted directly to that ingester's disk, with as little intermediary processing as possible.\nIf an ingester dies, its forwarders should be able to simply reconnect to other instances and resume.\n(During the transition, they may provide backpressure, buffer, or drop records, according to their configuration.)\nThis is all to say forwarders must not require knowledge about which is the \"correct\" ingester.\nAny ingester must be equally viable.\n\nAs an optimization, highly-loaded ingesters can shed load (connections) to other ingesters.\nIngesters will gossip load information between each other, like number of connections, iops, etc.\nThen, highly-loaded ingesters can refuse new connections, and thus redirect forwarders to more-lightly-loaded peers.\nExtremely highly-loaded ingesters can even terminate existing connections, if necessary.\nThis needs to be carefully managed to prevent inadvertant denial of service.\nFor example, no more than a handful of ingesters should be refusing connections at a given time.\n\nConsumers need to be able to make queries without any prior knowledge about time partitions, replica allocation, etc.\nWithout that knowledge, this implies the queries will always be scattered to each query node, gathered, and deduplicated.\nQuery nodes may die at any time, or start up and be empty, so query operations must manage partial results gracefully.\n\nAs an optimization, consumers can perform read-repair.\nA query should return N copies of each matching record, where N is the replication factor.\nAny records with fewer than N copies returned may be under-replicated.\nA new segment with the suspect records can be created and replicated to the cluster.\nAs a further optimization, a separate process can perform sequential queries of the timespace, for the explicit purpose of read repair.\n\nThe transfer of data between the ingest and query tier also needs care.\nIdeally, any ingest node should be able to transfer segments to any/all query nodes arbitrarily.\nThat transfer should accomplish work that makes forward progress in the system as a whole.\nAnd we must recover gracefully from failure; for example, network partitions at any stage of the transaction.\n\nLet's now look at how to shuffle data safely from the ingest tier to the query tier.\n\n## Ingest segments\n\nIngesters receive N independent streams of records, from N forwarders.\nEach record shall be prefixed by the ingester with a time UUID, a [ULID](https://github.com/oklog/ulid).\nIt's important that each record have a timestamp with reasonable precision, to create some global order.\nBut it's not important that the clocks are globally synced, or that the records are e.g. strictly linearizable.\nAlso, it's fine if records that arrive in the same minimum time window to appear out-of-order, as long as that order is stable.\n\nIncoming records are written to a so-called active segment, which is a file on disk.\n\n```\n          +---+\nP -> F -> | I | -> Active: R R R...\nP -> F -> |   |\nP -> F -> |   |\n          +---+\n```\n\nOnce it has written B bytes, or been active for S seconds, the active segment is flushed to disk.\n\n```\n          +---+\nP -> F -> | I | -> Active:  R R R...\nP -> F -> |   |    Flushed: R R R R R R R R R\nP -> F -> |   |    Flushed: R R R R R R R R\n          +---+\n```\n\nThe ingester consumes records serially from each forwarder connection.\nThe next record is consumed when the current record is successfully written to the active segment.\nAnd the active segment is normally synced once it is flushed.\nThis is the default durability mode, tentative called fast.\n\nProducers can optionally connect to a separate port, whose handler will sync the active segment after each record is written.\nThis provides stronger durability, at the expense of throughput.\nThis is a separate durability mode, tentatively called durable.\n\nThere can be a third, even higher durability mode, tentatively called bulk.\nForwarders may write entire segment files at once to the ingester.\nEach segment file would only be acknowledged when it's been successfully replicated among the storage nodes.\nThen, the forwarder is free to send the next complete segment.\n\nIngesters host an API which serves flushed segments.\n\n- GET /next — returns the oldest flushed segment and marks it pending\n- POST /commit?id=ID — deletes a pending segment\n- POST /failed?id=ID — returns a pending segment to flushed\n\nSegment state is controlled by file extension, and we leverage the filesystem for atomic renames.\nThe states are .active, .flushed, or .pending, and there is only ever a single active segment at a time per connected forwarder.\n\n```\n          +---+                     \nP -> F -> | I | Active              +---+\nP -> F -> |   | Active              | Q | --.\n          |   |  Flushed            +---+   |        \n          +---+                     +---+   '->\n          +---+              ?      | Q | ----> C\nP -> F -> | I | Active              +---+   .->\nP -> F -> |   | Active              +---+   |\nP -> F -> |   | Active              | Q | --'\n          |   |  Flushed            +---+\n          |   |  Flushed\n          +---+\n```\n\nObserve that ingesters are stateful, so they need a graceful shutdown process.\nFirst, they should terminate connections and close listeners.\nThen, they should wait for all flushed segments to be consumed.\nFinally, they may be shut down.\n\n### Consume segments\n\nThe ingesters act as a sort of queue, buffering records to disk in groups called segments.\nWhile it is protected against e.g. power failure, ultimately, we consider that storage ephemeral.\nSegments should be replicated as quickly as possible by the query tier.\nHere, we take a page from the Prometheus playbook.\nRather than the ingesters pushing flushed segments to query nodes, the query nodes pull flushed segments from the ingesters.\nThis enables a coherent model to scale for throughput.\nTo accept a higher ingest rate, add more ingest nodes, with faster disks.\nIf the ingest nodes are backing up, add more query nodes to consume from them.\n\nConsumption is modeled with a 3-stage transaction.\nThe first stage is the read stage.\nEach query node regularly asks each ingest node for its oldest flushed segment, via GET /next.\n(This can be pure random selection, round-robin, or some more sophisticated algorithm. For now it is pure random.)\nReceived segments are read record-by-record, and merged into another composite segment.\nThis process repeats, consuming multiple segments from the ingest tier, and merging them into the same composite segment.\n\nOnce the composite segment has reached B bytes, or been active for S seconds, it is closed, and we enter the replication stage.\nReplication means writing the composite segment to N distinct query nodes, where N is the replication factor.\nAt the moment we just POST the segment to a replication endpoint on N random store nodes.\n\nOnce the segment is confirmed replicated on N nodes, we enter the commit stage.\nThe query node commits the original segments on all of the ingest nodes, via POST /commit.\nIf the composite segment fails to replicate for any reason, the query node fails all of the original segments, via POST /failed.\nIn either case, the transaction is now complete, and the query node can begin its loop again.\n\n```\nQ1        I1  I2  I3\n--        --  --  --\n|-Next--->|   |   |\n|-Next------->|   |\n|-Next----------->|\n|<-S1-----|   |   |\n|<-S2---------|   |\n|<-S3-------------|\n|\n|--.\n|  | S1∪S2∪S3 = S4     Q2  Q3\n|<-'                   --  --\n|-S4------------------>|   |\n|-S4---------------------->|\n|<-OK------------------|   |\n|<-OK----------------------|\n|\n|         I1  I2  I3\n|         --  --  --\n|-Commit->|   |   |\n|-Commit----->|   |\n|-Commit--------->|\n|<-OK-----|   |   |\n|<-OK---------|   |\n|<-OK-------------|\n```\n\nLet's consider failure at each stage.\n\nIf the query node fails during the read stage, pending segments are in limbo until a timeout elapses.\nAfter which they are made available for consumption by another query node.\nIf the original query node is dead forever, no problem.\nIf the original query node comes back, it may still have records that have already been consumed and replicated by other nodes.\nIn which case, duplicate records will be written to the query tier, and one or more final commits will fail.\nIf this happens, it's OK: the records are over-replicated, but they will be deduplicated at read time, and eventually compacted.\nSo commit failures should be noted, but can be safely ignored.\n\nIf the query node fails during the replication stage, it's a similar story.\nAssuming the node doesn't come back, pending ingest segments will timeout and be retried by other nodes.\nAnd if the node does come back, replication will proceed without failure, and one or more final commits will fail.\nSame as before, this is OK: over-replicated records will be deduplicated at read time, and eventually compacted.\n\nIf the query node fails during the commit stage, one or more ingest segments will be stuck in pending and, again, eventually time out back to flushed.\nSame as before, records will become over-replicated, deduplicated at read time, and eventually compacted.\n\n## Node failure\n\nIf an ingest node fails permanently, any records in its segments are lost.\nTo protect against this class of failure, clients should use the bulk ingest mode.\nThat won't allow progress until the segment file is replicated into the storage tier.\n\nIf a store node fails permanently, records are safe on (replication factor - 1) other nodes.\nBut to get the lost records back up to the desired replication factor, read repair is required.\nA special timespace walking process can be run to perform this recovery.\nIt can essentially query all logs from the beginning of time, and perform read repair on the underreplicated records.\n\n## Query index\n\nAll queries are time-bounded, and segments are written in time-order.\nBut an additional index is necessary to map a time range to matching segments.\nWhenever a query node writes a segment for any reason, it shall read the first and last timestamp (time UUID) from that segment.\nIt can then update an in-memory index, associating that segment to that time range.\nAn interval tree is a good data structure here.\n\nAn alternative approach is to name each file as FROM-TO, with both FROM and TO as smallest and largest time UUID in the file.\nThen, given a query time range of T1–T2, segments can be selected with two simple comparisons to detect time range overlap.\nArrange two ranges (A, B) (C, D) so that A <= B, C <= D, and A <= C.\nIn our case, the first range is the range specified by the query, and the second range is for a given segment file.\nThen, the ranges overlap, and we should query the segment file, if B >= C.\n\n```\nA--B         B >= C?\n  C--D           yes \n  \nA--B         B >= C?\n     C--D         no\n  \nA-----B      B >= C?\n  C-D            yes\n  \nA-B          B >= C?\nC----D           yes\n```\n\nThis gives us a way to select segment files for querying.\n\n## Compaction\n\nCompaction serves two purposes: deduplication of records, and de-overlapping of segments.\nDuplication of records can occur during failures e.g. network partitions.\nBut segments will regularly and naturally overlap.\n\nConsider 3 overlapping segment files on a given query node.\n\n```\nt0             t1\n+-------+       |\n|   A   |       |\n+-------+       |\n|  +---------+  |\n|  |    B    |  |\n|  +---------+  |\n|     +---------+\n|     |    C    |\n|     +---------+\n```\n\nCompaction will first merge these overlapping segments into a single aggregate segment in memory.\nDuring the merge, duplicate records can be detected by ULID and dropped.\nThen, compaction will split the aggregate segment to achieve desired file sizes, and produce new, non-overlapping segments.\n\n```\nt0             t1\n+-------+-------+\n|       |       |\n|   D   |   E   |\n|       |       |\n+-------+-------+\n```\n\nCompaction reduces the number of segments necessary to read in order to serve queries for a given time.\nIn the ideal state, each time will map to exactly 1 segment.\nThis helps query performance by reducing the number of reads.\n\nObserve that compaction improves query performance, but doesn't affect either correctness or space utilization.\nCompression can be applied to segments completely orthogonally to the process described here.\nProper compression can dramatically increase retention, at the cost of some CPU burn.\nIt may also prevent processing of segment files with common UNIX tooling like grep, though this may or may not be important.\n\nSince records are individually addressable, read-time deduplication occurs on a per-record basis.\nSo the mapping of record to segment can be optimized completely independently by each node, without coördination.\n\nThe schedule and aggressiveness of compaction is an important performance consideration.\nAt the moment, a single compactor thread (goroutine) performs each of the compaction tasks sequentially and perpetually.\nIt fires at most once per second.\nMuch more performance analysis and real-world study is necessary here.\n\n## Querying\n\nEach query node hosts a query API, GET /query.\nQueries may be sent to any query node API by users.\nUpon receipt, the query is broadcast to every node in the query tier.\nResponses are gathered, results are merged and deduplicated, and then returned to the user.\n\nThe actual grepping work is done by each query node individually.\nFirst, segment files matching the time boundaries of the query are identified.\nThen, a per-segment reader is attached to each file, and filters matching records from the file.\nFinally, a merging reader takes results from each segment file, orders them, and returns them to the originating query node.\nThis pipeline is lazily constructed of io.ReadClosers, and costs paid when reads actually occur.\nThat is, when the HTTP response is written to the originating query node.\n\nNote that the per-segment reader is launched in its own goroutine, and reading/filtering occurs concurrently.\nCurrently there is no fixed limit on the number of active goroutines allowed to read segment files.\nThis should be improved.\n\nThe query request has several fields.\n\n- From, To time.Time — bounds of query\n- Q string — term to grep for, blank is OK and matches all records\n- Regex bool — if true, compile and match Q as a regex\n- StatsOnly bool — if true, just return stats, without actual results\n\nThe query response has several fields.\n\n- NodeCount int — how many query nodes were queried\n- SegmentCount int — how many segments were read\n- Size int — file size of segments read to produce results\n- Results io.Reader — merged and time-ordered results\n\nStatsOnly can be used to \"explore\" and iterate on a query, until it's been narrowed down to a usable result set.\n\n# Component model\n\nThis is a working draft of the components of the system.\n\n## Processes\n\n### forward\n\n- ./my_application | forward ingest.mycorp.local:7651\n- Should accept multiple ingest host:ports\n- Should contain logic to demux a DNS record to individual instances\n- Should contain failover logic in case of connection interruption\n- Can choose between fast, durable, or chunked writes\n- Post-MVP: more sophisticated (HTTP?) forward/ingest protocol; record enhancement\n\n### ingest\n\n- Receives writes from multiple forwarders\n- Each record is defined to be newline \\n delimited\n- Prefixes each record with time UUID upon ingestion\n- Appends to active segment\n- Flushes active segment to disk at size and time bounds\n- Serves segment API to storage tier, polling semantics\n- Gossips (via memberlist) with ingest peer instances to share load stats\n- Post-MVP: load spreading/shedding; streaming transmission of segments to storage tier\n\n### store\n\n- Polls ingest tier for flushed segments\n- Merges ingest segments together\n- Replicates merged segments to other store nodes\n- Serves query API to clients\n- Performs compaction on some interval\n- Post-MVP: streaming collection of segments from ingest tier; more advanced query use-cases\n\n## Libraries\n\n### Ingest Log\n\n- Abstraction for segments in the ingest tier\n- Operations include create new active segment, flush, mark as pending, commit\n- (I've got a reasonable prototype for this one)\n- Note that this is effectively a disk-backed queue, short-term durable storage\n\n### Store Log\n\n- Abstraction for segments in the storage tier\n- Operations include collect segments, merge, replicate, compact\n- Note that this is meant to be long-term durable storage\n\n### Cluster\n\n- Abstraction to get various nodes talking to each other\n- Not much data is necessary to share, just node identity and health\n- HashiCorp's memberlist fits the bill\n"
        },
        {
          "name": "Gopkg.lock",
          "type": "blob",
          "size": 6.056640625,
          "content": "# This file is autogenerated, do not edit; changes may be undone by the next 'dep ensure'.\n\n\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/armon/go-metrics\"\n  packages = [\".\"]\n  revision = \"7aa49fde808223f8dadfdbfd3a20ff6c19e5f9ec\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/beorn7/perks\"\n  packages = [\"quantile\"]\n  revision = \"4c0e84591b9aa9e6dcfdf3e020114cd81f89d5f9\"\n\n[[projects]]\n  name = \"github.com/cespare/xxhash\"\n  packages = [\".\"]\n  revision = \"5c37fe3735342a2e0d01c87a907579987c8936cc\"\n  version = \"v1.0.0\"\n\n[[projects]]\n  name = \"github.com/djherbis/buffer\"\n  packages = [\".\",\"limio\",\"wrapio\"]\n  revision = \"4972e2bf4a27dbd7c43140c959b85e8c1d2f5536\"\n  version = \"v1.0\"\n\n[[projects]]\n  name = \"github.com/djherbis/nio\"\n  packages = [\".\"]\n  revision = \"824ca9017eeb2a422fdda7ae9dba05d3ab019dfa\"\n  version = \"v2.0.3\"\n\n[[projects]]\n  name = \"github.com/go-kit/kit\"\n  packages = [\"log\",\"log/level\"]\n  revision = \"4dc7be5d2d12881735283bcab7352178e190fc71\"\n  version = \"v0.6.0\"\n\n[[projects]]\n  name = \"github.com/go-logfmt/logfmt\"\n  packages = [\".\"]\n  revision = \"390ab7935ee28ec6b286364bba9b4dd6410cb3d5\"\n  version = \"v0.3.0\"\n\n[[projects]]\n  name = \"github.com/go-stack/stack\"\n  packages = [\".\"]\n  revision = \"259ab82a6cad3992b4e21ff5cac294ccb06474bc\"\n  version = \"v1.7.0\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/golang/protobuf\"\n  packages = [\"proto\"]\n  revision = \"1e59b77b52bf8e4b449a57e6f79f21226d571845\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/google/btree\"\n  packages = [\".\"]\n  revision = \"316fb6d3f031ae8f4d457c6c5186b9e3ded70435\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/hashicorp/errwrap\"\n  packages = [\".\"]\n  revision = \"7554cd9344cec97297fa6649b055a8c98c2a1e55\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/hashicorp/go-immutable-radix\"\n  packages = [\".\"]\n  revision = \"8aac2701530899b64bdea735a1de8da899815220\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/hashicorp/go-msgpack\"\n  packages = [\"codec\"]\n  revision = \"fa3f63826f7c23912c15263591e65d54d080b458\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/hashicorp/go-multierror\"\n  packages = [\".\"]\n  revision = \"83588e72410abfbe4df460eeb6f30841ae47d4c4\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/hashicorp/go-sockaddr\"\n  packages = [\".\"]\n  revision = \"9b4c5fa5b10a683339a270d664474b9f4aee62fc\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/hashicorp/golang-lru\"\n  packages = [\"simplelru\"]\n  revision = \"0a025b7e63adc15a622f29b0b2c4c3848243bbf6\"\n\n[[projects]]\n  name = \"github.com/hashicorp/memberlist\"\n  packages = [\".\"]\n  revision = \"ce8abaa0c60c2d6bee7219f5ddf500e0a1457b28\"\n  version = \"v0.1.0\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/kr/logfmt\"\n  packages = [\".\"]\n  revision = \"b84e30acd515aadc4b783ad4ff83aff3299bdfe0\"\n\n[[projects]]\n  name = \"github.com/matttproud/golang_protobuf_extensions\"\n  packages = [\"pbutil\"]\n  revision = \"3247c84500bff8d9fb6d579d800f20b3e091582c\"\n  version = \"v1.0.0\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/miekg/dns\"\n  packages = [\".\",\"internal/socket\"]\n  revision = \"6da3249dfb57fbaa16efafcd8744cee8809d80cd\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/nightlyone/lockfile\"\n  packages = [\".\"]\n  revision = \"6a197d5ea61168f2ac821de2b7f011b250904900\"\n\n[[projects]]\n  name = \"github.com/oklog/run\"\n  packages = [\".\"]\n  revision = \"4dadeb3030eda0273a12382bb2348ffc7c9d1a39\"\n  version = \"v1.0.0\"\n\n[[projects]]\n  name = \"github.com/oklog/ulid\"\n  packages = [\".\"]\n  revision = \"d311cb43c92434ec4072dfbbda3400741d0a6337\"\n  version = \"v0.3.0\"\n\n[[projects]]\n  name = \"github.com/pborman/uuid\"\n  packages = [\".\"]\n  revision = \"e790cca94e6cc75c7064b1332e63811d4aae1a53\"\n  version = \"v1.1\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/petar/GoLLRB\"\n  packages = [\"llrb\"]\n  revision = \"53be0d36a84c2a886ca057d34b6aa4468df9ccb4\"\n\n[[projects]]\n  name = \"github.com/pkg/errors\"\n  packages = [\".\"]\n  revision = \"645ef00459ed84a119197bfb8d8205042c6df63d\"\n  version = \"v0.8.0\"\n\n[[projects]]\n  name = \"github.com/prometheus/client_golang\"\n  packages = [\"prometheus\",\"prometheus/promhttp\"]\n  revision = \"c5b7fccd204277076155f10851dad72b76a49317\"\n  version = \"v0.8.0\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/prometheus/client_model\"\n  packages = [\"go\"]\n  revision = \"99fa1f4be8e564e8a6b613da7fa6f46c9edafc6c\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/prometheus/common\"\n  packages = [\"expfmt\",\"internal/bitbucket.org/ww/goautoneg\",\"model\"]\n  revision = \"2e54d0b93cba2fd133edc32211dcc32c06ef72ca\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/prometheus/procfs\"\n  packages = [\".\",\"xfs\"]\n  revision = \"a6e9df898b1336106c743392c48ee0b71f5c4efa\"\n\n[[projects]]\n  name = \"github.com/prometheus/prometheus\"\n  packages = [\"pkg/labels\",\"storage\",\"storage/tsdb\",\"util/testutil\"]\n  revision = \"85f23d82a045d103ea7f3c89a91fba4a93e6367a\"\n  version = \"v2.1.0\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/prometheus/tsdb\"\n  packages = [\".\",\"chunkenc\",\"chunks\",\"fileutil\",\"index\",\"labels\"]\n  revision = \"494acd307058387ced7646f9996b0f7372eaa558\"\n\n[[projects]]\n  name = \"github.com/rs/cors\"\n  packages = [\".\"]\n  revision = \"7af7a1e09ba336d2ea14b1ce73bf693c6837dbf6\"\n  version = \"v1.2\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/sean-/seed\"\n  packages = [\".\"]\n  revision = \"e2103e2c35297fb7e17febb81e49b312087a2372\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"golang.org/x/crypto\"\n  packages = [\"ed25519\",\"ed25519/internal/edwards25519\"]\n  revision = \"94eea52f7b742c7cbe0b03b22f0c4c8631ece122\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"golang.org/x/net\"\n  packages = [\"context\"]\n  revision = \"cbe0f9307d0156177f9dd5dc85da1a31abc5f2fb\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"golang.org/x/sync\"\n  packages = [\"errgroup\"]\n  revision = \"fd80eb99c8f653c847d294a001bdf2a3a6f768f5\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"golang.org/x/sys\"\n  packages = [\"unix\"]\n  revision = \"37707fdb30a5b38865cfb95e5aab41707daec7fd\"\n\n[solve-meta]\n  analyzer-name = \"dep\"\n  analyzer-version = 1\n  inputs-digest = \"a2bd5e34e33089f07456e9a837d665ba8543d36fd472e57dc298fa1d26d2ad20\"\n  solver-name = \"gps-cdcl\"\n  solver-version = 1\n"
        },
        {
          "name": "Gopkg.toml",
          "type": "blob",
          "size": 0.927734375,
          "content": "\n[[constraint]]\n  name = \"github.com/djherbis/buffer\"\n  version = \"1.0.0\"\n\n[[constraint]]\n  name = \"github.com/djherbis/nio\"\n  version = \"2.0.3\"\n\n[[constraint]]\n  name = \"github.com/go-kit/kit\"\n  version = \"0.6.0\"\n\n[[constraint]]\n  branch = \"master\"\n  name = \"github.com/google/btree\"\n\n[[constraint]]\n  branch = \"master\"\n  name = \"github.com/hashicorp/go-sockaddr\"\n\n[[constraint]]\n  name = \"github.com/hashicorp/memberlist\"\n  version = \"0.1.0\"\n\n[[constraint]]\n  name = \"github.com/oklog/run\"\n  version = \"1.0.0\"\n\n[[constraint]]\n  name = \"github.com/oklog/ulid\"\n  version = \"0.3.0\"\n\n[[constraint]]\n  name = \"github.com/pborman/uuid\"\n  version = \"1.1.0\"\n\n[[constraint]]\n  name = \"github.com/pkg/errors\"\n  version = \"0.8.0\"\n\n[[constraint]]\n  name = \"github.com/prometheus/client_golang\"\n  version = \"0.8.0\"\n\n[[constraint]]\n  name = \"github.com/prometheus/prometheus\"\n  version = \"2.1.0\"\n\n[[constraint]]\n  name = \"github.com/rs/cors\"\n  version = \"1.2.0\"\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"{}\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright {yyyy} {name of copyright owner}\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MODEL.md",
          "type": "blob",
          "size": 3.818359375,
          "content": "# Model\n\nThis is a very preliminary braindump on some ideas for a system model.\nI am no academic, and it's been a long time since I've taken my queueing theory class.\nThis is all unvalidated, and may be wildly off-base.\nI welcome corrections with an open heart and mind.\n\n## Queueing theory\n\nFirst, [an introduction to some queueing theory](http://www.perfdynamics.com/Tools/PDQ.html).\n\n```\n <--------R--------->\n <---W--->  <---S--->\n           +---------+\n   λ       |         |  X\n --> • • • |    •    |--> •\n           |         |\n           +---------+\n <---Q--->\n```\n\n\n- The average arrival rate **λ** into the queue\n- The average service time **S** in the server\n\nFrom λ and S we can deduce other properties of the system.\n\n- The throughput coming out of the server: **X** = λ\n- The average residence time a customer spends getting through the queue: **R** = S / (1 - λS)\n- The utilization of the server: **ρ** = λS\n- The average queue length: **Q** = λR\n- The average waiting time before service: **W** = R - ρ\n\nFor testing we can reasonably fix λ to some constant value.\nIn contrast, S will be a function of the size of the work unit (record).\nFor now let's fix each record at N bytes. Then, S = f(N).\n\n## Our components\n\n- Producer — Forward — Ingest — Store\n\nThe producer establishes the arrival rate of records λr, and the size of each record Nr.\n\n- λr\n- Sr = f(Nr)\n\nWe'll need to run experiments to figure out the service time function Sr.\nFor now let's assume one nanosecond per byte.\n\n- Sr = Nr * 1ns\n\nBetween the producer and the forwarding agent exists a pipe buffer.\nThis has a maximum bandwidth which affects througput.\n\n- TODO\n\nThe first queue exists at the forward agent.\nThe forwarder just writes records to the connection.\nSo the service time at the forwarder is simple.\n\n- Sforward = Per-record consumption overhead + Cost per byte to write to the connection\n- Sforward = Oconsumption(1) + Wconnection(Nr)\n\nAgain, we'll need to experiment to figure out Oconsumption and Wconnection.\nFor now, let's fix them at some guesses.\n\n- Oconsumption = 10ns\n- Wconnection = Nr * 2ns\n\nBetween the forwarder and the ingester exists a network.\nThis has a maximum bandwidth which affects throughput.\n\n- TODO\n\nThen, we reach the ingester.\nFirst, records are taken from connections and written to the active segment.\nThis follows a similar model of the forwarder.\n\n- Singest = Per-record consumption overhead + Cost per byte to write to the segment\n- Singest = Oconsumption(1) + Wsegment(Nr)\n\nAgain, we'll need to experiment to figure out Wsegment.\nFor now, let's fix it with a guess.\n\n- Wsegment = Nr * 1ns\n\nSegments become available for consumption after they reach a defined age A or size S.\nA imposes a maximum age, and S imposes a maximum size.\nThe ingester effectively changes the unit of work, from record to segment.\nSo, we derive a new rate of segments λs, and segment size Nr.\n\n- Ns = f(λr, Nr, A, S)\n- λs = f(λr, Nr, A, S)\n\nLet λr = 100 records per second, Nr = 500 bytes, A = 1 second, and S = 1 megabyte.\n100 records per second times 500 bytes per record yields 50000 bytes per second.\nThis triggers A = 1 second, but doesn't trigger S = 1 megabyte.\nSo Ns will be 50000 bytes, and λs will be 1 segment per second.\n\nLet λr = 10000 records per second, Nr = 500 bytes, A = 1 second, and S = 1 megabyte.\n10000 records per second times 500 bytes per record yields 5000000 bytes per second.\nThis triggers S = 1 megabyte at (1MB / 5000000B) * second = 200ms.\nSo Ns will be 1MB, and λs will be 5 segments per second.\n\nSegments are made available by ingesters but consumed by store nodes.\nStore nodes consume when they have capacity, choosing arbitrarily from ingest nodes.\nWe can model this as a virtual queue.\nAvailable segments across all ingesters form the virtual queue of waiting work.\nAnd all store nodes collectively form a single virtual server.\n\n- TODO\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.8544921875,
          "content": "# OK Log is archived\n\nI hoped to find the opportunity to continue developing OK Log after the spike of its creation.\nUnfortunately, despite effort, no such opportunity presented itself.\nPlease look at OK Log for inspiration, and consider using the (maintained!) projects that came from it,\n[ulid](https://github.com/oklog/ulid) and [run](https://github.com/oklog/run).\n\n---\n\n<p align=\"center\">\n<img src=\"oklog.png\" width=500 height=142 alt=\"OK Log\"/><br/>\n<a href=\"https://github.com/oklog/oklog/releases/latest\"><img src=\"https://img.shields.io/github/release/oklog/oklog.svg?style=flat-square\"/></a>\n<a href=\"https://travis-ci.org/oklog/oklog\"><img src=\"https://img.shields.io/travis/oklog/oklog.svg?style=flat-square\"/></a>\n<a href=\"https://goreportcard.com/report/github.com/oklog/oklog\"><img src=\"https://goreportcard.com/badge/github.com/oklog/oklog?style=flat-square&x=1\"/></a>\n<a href=\"LICENSE\"><img src=\"https://img.shields.io/badge/license-Apache%202.0-blue.svg?style=flat-square\"/></a>\n</p>\n\nOK Log is a distributed and coördination-free log management system for big ol' clusters.\nIt's an on-prem solution that's designed to be a sort of building block: easy to understand, easy to operate, and easy to extend.\n\n- [Article motivating the system](https://peter.bourgon.org/ok-log)\n- [Detailed system design](DESIGN.md)\n- [Architectural Decision Records](doc/arch)\n\n## Is OK Log for me?\n\nYou may consider OK Log if...\n\n- You're tailing your logs manually, find it annoying, and want to aggregate them without a lot of fuss\n- You're using a hosted solution like Loggly, and want to move logs on-prem\n- You're using Elasticsearch, but find it unreliable, difficult to operate, or don't use many of its features\n- You're using a custom log pipeline with e.g. Fluentd or Logstash, and having performance problems\n- You just wanna, like, grep your logs — why is this all so complicated?\n\n## Getting OK Log\n\nOK Log is distributed as a single, statically-linked binary for a variety of target architectures.\nDownload the latest release from [the releases page](https://github.com/oklog/oklog/releases).\n\n## Quickstart\n\n```sh\n$ oklog ingeststore -store.segment-replication-factor 1\n$ ./myservice | oklog forward localhost\n$ oklog query -from 5m -q Hello\n2017-01-01 12:34:56 Hello world!\n```\n\n## Deploying\n\n### Small installations\n\nIf you have relatively small log volume, you can deploy a cluster of identical ingeststore nodes.\nBy default, the replication factor is 2, so you need at least 2 nodes.\nUse the -cluster flag to specify a routable IP address or hostname for each node to advertise itself on.\nAnd let each node know about at least one other node with the -peer flag.\n\n```sh\nfoo$ oklog ingeststore -cluster foo -peer foo -peer bar -peer baz\nbar$ oklog ingeststore -cluster bar -peer foo -peer bar -peer baz\nbaz$ oklog ingeststore -cluster baz -peer foo -peer bar -peer baz\n```\n\nTo grow the cluster, just add a new node, and tell it about at least one other node via the -peer flag.\nOptionally, you can run the rebalance tool (TODO) to redistribute the data over the new topology.\nTo shrink the cluster, just kill nodes fewer than the replication factor,\n and run the repair tool (TODO) to re-replicate lost records.\n\nAll configuration is done via commandline flags.\nYou can change things like the log retention period (default 7d),\n the target segment file size (default 128MB),\n and maximum time (age) of various stages of the logging pipeline.\nMost defaults should be sane, but you should always audit for your environment.\n\n### Large installations\n\nIf you have relatively large log volume, you can split the ingest and store (query) responsibilities.\nIngest nodes make lots of sequential writes, and benefit from fast disks and moderate CPU.\nStore nodes make lots of random reads and writes, and benefit from large disks and lots of memory.\nBoth ingest and store nodes join the same cluster, so provide them with the same set of peers.\n\n```sh\ningest1$ oklog ingest -cluster 10.1.0.1 -peer ...\ningest2$ oklog ingest -cluster 10.1.0.2 -peer ...\n\nstore1$ oklog store -cluster 10.1.9.1 -peer ...\nstore2$ oklog store -cluster 10.1.9.2 -peer ...\nstore3$ oklog store -cluster 10.1.9.3 -peer ...\n```\n\nTo add more raw ingest capacity, add more ingest nodes to the cluster.\nTo add more storage or query capacity, add more store nodes.\nAlso, make sure you have enough store nodes to consume from the ingest nodes without backing up.\n\n## Forwarding\n\nThe forwarder is basically just netcat with some reconnect logic.\nPipe the stdout/stderr of your service to the forwarder, configured to talk to your ingesters.\n\n```sh\n$ ./myservice | oklog forward ingest1 ingest2\n```\n\nOK Log integrates in a straightforward way with runtimes like Docker and Kubernetes.\nSee [the Integrations page](https://github.com/oklog/oklog/wiki/Integrations) for more details.\n\n## Querying\n\nQuerying is an HTTP GET to /query on any of the store nodes.\nOK Log comes with a query tool to make it easier to play with.\nOne good thing is to first use the -stats flag to refine your query.\nWhen you're satisfied it's sufficiently constrained, drop -stats to get results.\n\n```sh\n$ oklog query -from 2h -to 1h -q \"myservice.*(WARN|ERROR)\" -regex\n2016-01-01 10:34:58 [myservice] request_id 187634 -- [WARN] Get /check: HTTP 419 (0B received)\n2016-01-01 10:35:02 [myservice] request_id 288211 -- [ERROR] Post /ok: HTTP 500 (0B received)\n2016-01-01 10:35:09 [myservice] request_id 291014 -- [WARN] Get /next: HTTP 401 (0B received)\n ...\n```\n\nTo query structured logs, combine a basic grep filter expression with a tool like [jq](https://stedolan.github.io/jq/).\n\n```sh\n$ oklog query -from 1h -q /api/v1/login\n{\"remote_addr\":\"10.34.115.3:50032\",\"path\":\"/api/v1/login\",\"method\":\"POST\",\"status_code\":200}\n{\"remote_addr\":\"10.9.101.113:51442\",\"path\":\"/api/v1/login\",\"method\":\"POST\",\"status_code\":500}\n{\"remote_addr\":\"10.9.55.2:55210\",\"path\":\"/api/v1/login\",\"method\":\"POST\",\"status_code\":200}\n{\"remote_addr\":\"10.34.115.1:51610\",\"path\":\"/api/v1/login\",\"method\":\"POST\",\"status_code\":200}\n...\n\n$ oklog query -from 1h -q /api/v1/login | jq '. | select(.status_code == 500)'\n{\n\t\"remote_addr\": \"10.9.55.2:55210\",\n\t\"path\": \"/api/v1/login\",\n\t\"method\": \"POST\",\n\t\"status_code\": 500\n}\n...\n```\n\n## UI\n\nOK Log ships with a basic UI for making queries.\nYou can access it on any store or ingeststore node, on the public API port (default 7650), path `/ui`.\nSo, e.g. http://localhost:7650/ui.\n\n## Further reading\n\n### Integrations\n\n- [Docker](https://github.com/oklog/oklog/wiki/Integrations#docker)\n- [Kubernetes](https://github.com/oklog/oklog/wiki/Integrations#kubernetes)\n\n### Unofficial Docker images\n\n- [OK Log](https://hub.docker.com/r/oklog/oklog/)\n\n### Translation\n\n- [中文介绍](https://github.com/1046102779/OKLog) \n\n---\n\n[OK icon](https://thenounproject.com/term/like/594444/) by\n [Karthik Srinivas](https://thenounproject.com/aathis/) from the\n [Noun Project](https://thenounproject.com).\nDevelopment supported by [DigitalOcean](https://digitalocean.com).\n"
        },
        {
          "name": "cmd",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "oklog.png",
          "type": "blob",
          "size": 37.9814453125,
          "content": null
        },
        {
          "name": "pkg",
          "type": "tree",
          "content": null
        },
        {
          "name": "quickstart-combined.bash",
          "type": "blob",
          "size": 0.671875,
          "content": "#!/usr/bin/env bash\n\ntrap 'kill 0' SIGTERM\n\nfor i in `seq 1 3`\ndo\n  sleep 0.25\n  oklog ingeststore \\\n    -api                       tcp://0.0.0.0:100${i}0 \\\n    -ingest.fast               tcp://0.0.0.0:100${i}1 \\\n    -ingest.durable            tcp://0.0.0.0:100${i}2 \\\n    -ingest.bulk               tcp://0.0.0.0:100${i}3 \\\n    -cluster                   tcp://0.0.0.0:100${i}9 \\\n    -store.segment-target-size 1000000 \\\n    -store.segment-retain      30m \\\n    -store.segment-purge       5m  \\\n    -ingest.path               data/ingest/${i} \\\n    -store.path                data/store/${i} \\\n    $PEERS 2>&1 | sed -e \"s/^/[$i] /\" &\n  PEERS=\"$PEERS -peer 127.0.0.1:100${i}9\"\ndone\n\nwait\n"
        },
        {
          "name": "quickstart-separate.bash",
          "type": "blob",
          "size": 0.8486328125,
          "content": "#!/usr/bin/env bash\n\ntrap 'kill 0' SIGTERM\n\nfor i in `seq 1 3`\ndo\n  sleep 0.25\n  oklog ingest \\\n    -api            tcp://0.0.0.0:100${i}0 \\\n    -ingest.fast    tcp://0.0.0.0:100${i}1 \\\n    -ingest.durable tcp://0.0.0.0:100${i}2 \\\n    -ingest.bulk    tcp://0.0.0.0:100${i}3 \\\n    -cluster        tcp://0.0.0.0:100${i}9 \\\n    -ingest.path    data/ingest/${i} \\\n    $PEERS 2>&1 | sed -e \"s/^/[I$i] /\" &\n  PEERS=\"$PEERS -peer 127.0.0.1:100${i}9\"\ndone\n\nfor i in `seq 1 3`\ndo\n  sleep 0.25\n  oklog store \\\n    -api                       tcp://0.0.0.0:200${i}0 \\\n    -cluster                   tcp://0.0.0.0:200${i}9 \\\n    -store.path                data/store/${i} \\\n    -store.segment-target-size 1000000 \\\n    -store.segment-retain      2m \\\n    -store.segment-purge       10m \\\n    $PEERS 2>&1 | sed -e \"s/^/[S$i] /\" &\n  PEERS=\"$PEERS -peer 127.0.0.1:200${i}9\"\ndone\n\nwait\n"
        },
        {
          "name": "release.fish",
          "type": "blob",
          "size": 0.7939453125,
          "content": "#!/usr/bin/env fish\n\nfunction version_prompt\n\techo \"Semantic version: \"\nend\n\nread --prompt version_prompt VERSION\n\nif test (echo $VERSION | grep '^v')\n\techo Use the raw semantic version, without a v prefix\n\texit\nend\n\nset REV (git rev-parse --short HEAD)\necho Tagging $REV as v$VERSION\ngit tag --annotate v$VERSION -m \"Release v$VERSION\"\necho Be sure to: git push --tags\necho\n\nset DISTDIR dist/v$VERSION\nmkdir -p $DISTDIR\n\nfor pair in linux/386 linux/amd64 linux/arm linux/arm64 darwin/amd64 dragonfly/amd64 freebsd/amd64 netbsd/amd64 openbsd/amd64\n\tset GOOS   (echo $pair | cut -d'/' -f1)\n\tset GOARCH (echo $pair | cut -d'/' -f2)\n\tset BIN    $DISTDIR/oklog-$VERSION-$GOOS-$GOARCH\n\techo $BIN\n\tenv GOOS=$GOOS GOARCH=$GOARCH go build -o $BIN -ldflags=\"-X main.version=$VERSION\" github.com/oklog/oklog/cmd/oklog\nend\n\n"
        },
        {
          "name": "ui",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}