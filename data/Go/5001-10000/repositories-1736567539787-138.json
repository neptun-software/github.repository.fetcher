{
  "metadata": {
    "timestamp": 1736567539787,
    "page": 138,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "lightningnetwork/lnd",
      "stars": 7781,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".custom-gcl.yml",
          "type": "blob",
          "size": 0.1083984375,
          "content": "version: v1.57.0\nplugins:\n  - module: 'github.com/lightningnetwork/lnd/tools/linters'\n    path: ./tools/linters"
        },
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.3203125,
          "content": "# EditorConfig is awesome: https://EditorConfig.org\n\n# Top-most EditorConfig file.\nroot = true\n\n# Unix-style newlines with a newline ending every file.\n[*.md]\nend_of_line = lf\ninsert_final_newline = true\nmax_line_length = 80\n\n# 8 space indentation for Golang code.\n[*.go]\nindent_style = tab\nindent_size = 8\nmax_line_length = 80\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.95703125,
          "content": "# ---> Go\n# Compiled Object files, Static and Dynamic libs (Shared Objects)\n*.o\n*.a\n*.so\n\n# Folders\n_obj\n_test\n\n# Architecture specific extensions/prefixes\n*.[568vq]\n[568vq].out\n\n*.cgo1.go\n*.cgo2.c\n_cgo_defun.c\n_cgo_gotypes.go\n_cgo_export.*\n\n_testmain.go\n\n*.exe\n*.test\n*.prof\n\n/lnd\n/lnd-debug\n/lncli\n/lncli-debug\n/lnd-itest\n/lncli-itest\n\n# Integration test log files\nitest/*.log\nitest/.backendlogs\nitest/.minerlogs\nitest/lnd-itest\nitest/btcd-itest\nitest/.logs-*\nitest/cover\n\ncmd/cmd\n*.key\n*.hex\n\n# Ignore the custom linter binary if it is built.\ncustom-gcl\n\ncmd/lncli/lncli\n\n# Files from mobile build.\nmobile/build\nmobile/*_generated.go\n\n# vim\n*.swp\n\n*.hex\n*.db\n*.bin\n\nvendor\n*.idea\n*.iml\nprofile.cov\nprofile.tmp\n\n.DS_Store\n\n.vscode\n*.code-workspace\n\n# Coverage test\ncoverage.txt\n\n# Visual Studio cache/options directory\n.vs/\n\n# Release build directory (to avoid build.vcs.modified Golang build tag to be\n# set to true by having untracked files in the working directory).\n/lnd-*/\n"
        },
        {
          "name": ".golangci.yml",
          "type": "blob",
          "size": 7.7236328125,
          "content": "run:\n  # If you change this please also update GO_VERSION in Makefile (then run\n  # `make lint` to see where else it needs to be updated as well).\n  go: \"1.22.6\"\n\n  # Abort after 10 minutes.\n  timeout: 10m\n\n  build-tags:\n    - autopilotrpc\n    - chainrpc\n    - dev\n    - invoicesrpc\n    - neutrinorpc\n    - peersrpc\n    - signrpc\n    - walletrpc\n    - watchtowerrpc\n    - kvdb_etcd\n    - kvdb_postgres\n    - kvdb_sqlite\n    - integration\n\nlinters-settings:\n  custom:\n    ll:\n      type: \"module\"\n      description: \"Custom lll linter with 'S' log line exclusion.\"\n      settings:\n        # Max line length, lines longer will be reported.\n        line-length: 80\n        # Tab width in spaces.\n        tab-width: 8\n        # The regex that we will use to detect the start of an `S` log line.\n        log-regex: \"^\\\\s*.*(L|l)og\\\\.(Info|Debug|Trace|Warn|Error|Critical)S\\\\(\"\n\n  errorlint:\n    # Check for incorrect fmt.Errorf error wrapping.\n    errorf: true\n\n  govet:\n    # Don't report about shadowed variables\n    check-shadowing: false\n\n  gofmt:\n    # simplify code: gofmt with `-s` option, true by default\n    simplify: true\n\n  tagliatelle:\n    case:\n      rules:\n        json: snake\n\n  whitespace:\n    multi-func: true\n    multi-if: true\n\n  gosec:\n    excludes:\n      - G402 # Look for bad TLS connection settings.\n      - G306 # Poor file permissions used when writing to a new file.\n      - G601 # Implicit memory aliasing in for loop.\n\n  staticcheck:\n    checks: [\"-SA1019\"]\n\n  funlen:\n    # Checks the number of lines in a function.\n    # If lower than 0, disable the check.\n    lines: 200\n    # Checks the number of statements in a function.\n    statements: 80\n\n  dupl:\n    # Tokens count to trigger issue.\n    threshold: 200\n\n  nestif:\n    # Minimal complexity of if statements to report.\n    min-complexity: 10\n\n  nlreturn:\n    # Size of the block (including return statement that is still \"OK\")\n    # so no return split required.\n    block-size: 3\n\n  gomnd:\n    # List of numbers to exclude from analysis.\n    # The numbers should be written as string.\n    # Values always ignored: \"1\", \"1.0\", \"0\" and \"0.0\"\n    # Default: []\n    ignored-numbers:\n      - '0666'\n      - '0755'\n\n    # List of function patterns to exclude from analysis.\n    # Values always ignored: `time.Date`\n    # Default: []\n    ignored-functions:\n      - 'math.*'\n      - 'strconv.ParseInt'\n      - 'errors.Wrap'\n  \n  gomoddirectives:\n    replace-local: true\n    replace-allow-list:\n      # See go.mod for the explanation why these are needed.\n      - github.com/ulikunitz/xz\n      - github.com/gogo/protobuf\n      - google.golang.org/protobuf\n      - github.com/lightningnetwork/lnd/sqldb\n\n\nlinters:\n  enable-all: true\n  disable:\n    # We instead use our own custom line length linter called `ll` since\n    # then we can ignore log lines.\n    - lll\n\n    # Global variables are used in many places throughout the code base.\n    - gochecknoglobals\n\n    # We want to allow short variable names.\n    - varnamelen\n\n    # We want to allow TODOs.\n    - godox\n\n    # Instances of table driven tests that don't pre-allocate shouldn't trigger\n    # the linter.\n    - prealloc\n\n    # Init functions are used by loggers throughout the codebase.\n    - gochecknoinits\n\n    # Deprecated linters. See https://golangci-lint.run/usage/linters/.\n    - bodyclose\n    - contextcheck\n    - nilerr\n    - noctx\n    - rowserrcheck\n    - sqlclosecheck\n    - tparallel\n    - unparam\n    - wastedassign\n\n    # Disable gofumpt as it has weird behavior regarding formatting multiple\n    # lines for a function which is in conflict with our contribution\n    # guidelines. See https://github.com/mvdan/gofumpt/issues/235.\n    - gofumpt\n\n    # Disable whitespace linter as it has conflict rules against our\n    # contribution guidelines. See https://github.com/bombsimon/wsl/issues/109.\n    #\n    # TODO(yy): bring it back when the above issue is fixed.\n    - wsl\n\n    # Allow using default empty values.\n    - exhaustruct\n\n    # Allow exiting case select faster by putting everything in default.\n    - exhaustive\n\n    # Allow tests to be put in the same package.\n    - testpackage\n\n    # Don't run the cognitive related linters.\n    - gocognit\n    - gocyclo\n    - maintidx\n    - cyclop\n\n    # Allow customized interfaces to be returned from functions.\n    - ireturn\n\n    # Disable too many blank identifiers check. We won't be able to run this\n    # unless a large refactor has been applied to old code.\n    - dogsled\n\n    # We don't wrap errors.\n    - wrapcheck\n\n    # Allow dynamic errors.\n    - goerr113\n\n    # We use ErrXXX instead.\n    - errname\n\n    # Disable nil check to allow returning multiple nil values.\n    - nilnil\n    \n    # We often split tests into separate test functions. If we are forced to\n    # call t.Helper() within those functions, we lose the information where\n    # exactly a test failed in the generated failure stack trace.\n    - thelper\n    \n    # The linter is too aggressive and doesn't add much value since reviewers\n    # will also catch magic numbers that make sense to extract.\n    - gomnd\n\n    # Some of the tests cannot be parallelized. On the other hand, we don't\n    # gain much performance with this check so we disable it for now until\n    # unit tests become our CI bottleneck.\n    - paralleltest\n\n    # New linters that we haven't had time to address yet.\n    - testifylint\n    - perfsprint\n    - inamedparam\n    - copyloopvar\n    - tagalign\n    - protogetter\n    - revive\n    - depguard\n    - gosmopolitan\n    - intrange\n    - goconst\n\n    # Deprecated linters that have been replaced by newer ones.\n    - deadcode\n    - exhaustivestruct\n    - ifshort\n    - golint\n    - interfacer\n    - varcheck\n    - nosnakecase\n    - scopelint\n    - structcheck\n    - maligned\n\nissues:\n  # Only show newly introduced problems.\n  new-from-rev: 03eab4db64540aa5f789c617793e4459f4ba9e78\n\n  # Skip autogenerated files for mobile and gRPC as well as copied code for\n  # internal use.\n  skip-files:\n    - \"mobile\\\\/.*generated\\\\.go\"\n    - \"\\\\.pb\\\\.go$\"\n    - \"\\\\.pb\\\\.gw\\\\.go$\"\n    - \"internal\\\\/musig2v040\"\n\n  skip-dirs:\n    - channeldb/migration_01_to_11\n    - channeldb/migration/lnwire21\n\n  exclude-rules:\n    # Exclude gosec from running for tests so that tests with weak randomness\n    # (math/rand) will pass the linter.\n    - path: _test\\.go\n      linters:\n        - gosec\n        - funlen\n        - revive\n        # Allow duplications in tests so it's easier to follow a single unit\n        # test.\n        - dupl\n\n    - path: mock*\n      linters:\n        - revive\n        # forcetypeassert is skipped for the mock because the test would fail\n        # if the returned value doesn't match the type, so there's no need to\n        # check the convert.\n        - forcetypeassert\n\n    - path: test*\n      linters:\n        - gosec\n        - funlen\n\n    # Allow duplicated code and fmt.Printf() in DB migrations.\n    - path: channeldb/migration*\n      linters:\n        - dupl\n        - forbidigo\n        - godot\n\n    # Allow duplicated code and fmt.Printf() in DB migration tests.\n    - path: channeldb/migtest\n      linters:\n        - dupl\n        - forbidigo\n        - godot\n\n    # Allow fmt.Printf() in commands.\n    - path: cmd/commands/*\n      linters:\n        - forbidigo\n\n    # Allow fmt.Printf() in config parsing.\n    - path: config\\.go\n      linters:\n        - forbidigo\n    - path: lnd\\.go\n      linters:\n        - forbidigo\n\n    - path: lnmock/*\n      linters:\n        # forcetypeassert is skipped for the mock because the test would fail\n        # if the returned value doesn't match the type, so there's no need to\n        # check the convert.\n        - forcetypeassert\n\n    - path: mock*\n      linters:\n        # forcetypeassert is skipped for the mock because the test would fail\n        # if the returned value doesn't match the type, so there's no need to\n        # check the convert.\n        - forcetypeassert\n"
        },
        {
          "name": ".protolint.yaml",
          "type": "blob",
          "size": 2.1640625,
          "content": "# The example configuration file for the protolint is located here:\n# https://github.com/yoheimuta/protolint/blob/master/_example/config/.protolint.yaml\n---\n# Lint directives.\nlint:\n  # Linter rules.\n  # Run `protolint list` to see all available rules.\n  rules:\n    # Determines whether or not to include the default set of linters.\n    no_default: true\n\n    # Set the default to all linters. This option works the other way around as no_default does.\n    # If you want to enable this option, delete the comment out below and no_default.\n    # all_default: true.\n\n    # The specific linters to add.\n    add:\n      - MESSAGE_NAMES_UPPER_CAMEL_CASE\n      - MAX_LINE_LENGTH\n      - INDENT\n      - FILE_NAMES_LOWER_SNAKE_CASE\n      - IMPORTS_SORTED\n      - PACKAGE_NAME_LOWER_CASE\n      - ORDER\n      - SERVICES_HAVE_COMMENT\n      - RPCS_HAVE_COMMENT\n      - PROTO3_FIELDS_AVOID_REQUIRED\n      - PROTO3_GROUPS_AVOID\n      - SYNTAX_CONSISTENT\n      - RPC_NAMES_CASE\n      - QUOTE_CONSISTENT\n\n  # Linter rules option.\n  rules_option:\n    # MAX_LINE_LENGTH rule option.\n    max_line_length:\n      # Enforces a maximum line length.\n      max_chars: 80\n      # Specifies the character count for tab characters.\n      tab_chars: 2\n\n    # INDENT rule option.\n    indent:\n      # Available styles are 4(4-spaces), 2(2-spaces) or tab.\n      style: 4\n      # Specifies if it should stop considering and inserting new lines at the appropriate positions.\n      # when the inner elements are on the same line. Default is false.\n      not_insert_newline: true\n\n    # QUOTE_CONSISTENT rule option.\n    quote_consistent:\n      # Available quote are \"double\" or \"single\".\n      quote: double\n\n    # ENUM_FIELD_NAMES_ZERO_VALUE_END_WITH rule option.\n    enum_field_names_zero_value_end_with:\n      suffix: INVALID\n\n    # SERVICE_NAMES_END_WITH rule option.\n    service_names_end_with:\n      text: Service\n\n    # REPEATED_FIELD_NAMES_PLURALIZED rule option.\n    ## The spec for each rules follows the implementation of https://github.com/gertd/go-pluralize.\n    ## Plus, you can refer to this rule's test code.\n    repeated_field_names_pluralized:\n      uncountable_rules:\n        - paper\n      irregular_rules:\n        Irregular: Regular\n"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.76953125,
          "content": "# If you change this please also update GO_VERSION in Makefile (then run\n# `make lint` to see where else it needs to be updated as well).\nFROM golang:1.22.6-alpine as builder\n\n# Force Go to use the cgo based DNS resolver. This is required to ensure DNS\n# queries required to connect to linked containers succeed.\nENV GODEBUG netdns=cgo\n\n# Pass a tag, branch or a commit using build-arg.  This allows a docker\n# image to be built from a specified Git state.  The default image\n# will use the Git tip of master by default.\nARG checkout=\"master\"\nARG git_url=\"https://github.com/lightningnetwork/lnd\"\n\n# Install dependencies and build the binaries.\nRUN apk add --no-cache --update alpine-sdk \\\n    git \\\n    make \\\n    gcc \\\n&&  git clone $git_url /go/src/github.com/lightningnetwork/lnd \\\n&&  cd /go/src/github.com/lightningnetwork/lnd \\\n&&  git checkout $checkout \\\n&&  make release-install\n\n# Start a new, final image.\nFROM alpine as final\n\n# Define a root volume for data persistence.\nVOLUME /root/.lnd\n\n# Add utilities for quality of life and SSL-related reasons. We also require\n# curl and gpg for the signature verification script.\nRUN apk --no-cache add \\\n    bash \\\n    jq \\\n    ca-certificates \\\n    gnupg \\\n    curl\n\n# Copy the binaries from the builder image.\nCOPY --from=builder /go/bin/lncli /bin/\nCOPY --from=builder /go/bin/lnd /bin/\nCOPY --from=builder /go/src/github.com/lightningnetwork/lnd/scripts/verify-install.sh /\nCOPY --from=builder /go/src/github.com/lightningnetwork/lnd/scripts/keys/* /keys/\n\n# Store the SHA256 hash of the binaries that were just produced for later\n# verification.\nRUN sha256sum /bin/lnd /bin/lncli > /shasums.txt \\\n  && cat /shasums.txt\n\n# Expose lnd ports (p2p, rpc).\nEXPOSE 9735 10009\n\n# Specify the start command and entrypoint as the lnd daemon.\nENTRYPOINT [\"lnd\"]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.07421875,
          "content": "Copyright (C) 2015-2022 Lightning Labs and The Lightning Network Developers\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 17.7724609375,
          "content": "PKG := github.com/lightningnetwork/lnd\nESCPKG := github.com\\/lightningnetwork\\/lnd\nMOBILE_PKG := $(PKG)/mobile\nTOOLS_DIR := tools\n\nPREFIX ?= /usr/local\n\nBTCD_PKG := github.com/btcsuite/btcd\nGOACC_PKG := github.com/ory/go-acc\nGOIMPORTS_PKG := github.com/rinchsan/gosimports/cmd/gosimports\n\nGO_BIN := ${GOPATH}/bin\nBTCD_BIN := $(GO_BIN)/btcd\nGOIMPORTS_BIN := $(GO_BIN)/gosimports\nGOMOBILE_BIN := $(GO_BIN)/gomobile\nGOACC_BIN := $(GO_BIN)/go-acc\n\nMOBILE_BUILD_DIR :=${GOPATH}/src/$(MOBILE_PKG)/build\nIOS_BUILD_DIR := $(MOBILE_BUILD_DIR)/ios\nIOS_BUILD := $(IOS_BUILD_DIR)/Lndmobile.xcframework\nANDROID_BUILD_DIR := $(MOBILE_BUILD_DIR)/android\nANDROID_BUILD := $(ANDROID_BUILD_DIR)/Lndmobile.aar\n\nCOMMIT := $(shell git describe --tags --dirty)\n\n# Determine the minor version of the active Go installation.\nACTIVE_GO_VERSION := $(shell go version | sed -nre 's/^[^0-9]*(([0-9]+\\.)*[0-9]+).*/\\1/p')\nACTIVE_GO_VERSION_MINOR := $(shell echo $(ACTIVE_GO_VERSION) | cut -d. -f2)\n\nLOOPVARFIX :=\nifeq ($(shell expr $(ACTIVE_GO_VERSION_MINOR) \\>= 21), 1)\n\tLOOPVARFIX := GOEXPERIMENT=loopvar\nendif\n\n# GO_VERSION is the Go version used for the release build, docker files, and\n# GitHub Actions. This is the reference version for the project. All other Go\n# versions are checked against this version.\nGO_VERSION = 1.22.6\n\nGOBUILD := $(LOOPVARFIX) go build -v\nGOINSTALL := $(LOOPVARFIX) go install -v\nGOTEST := $(LOOPVARFIX) go test\n\nGOFILES_NOVENDOR = $(shell find . -type f -name '*.go' -not -path \"./vendor/*\" -not -name \"*pb.go\" -not -name \"*pb.gw.go\" -not -name \"*.pb.json.go\")\n\nRM := rm -f\nCP := cp\nMAKE := make\nXARGS := xargs -L 1\n\ninclude make/testing_flags.mk\ninclude make/release_flags.mk\ninclude make/fuzz_flags.mk\n\nDEV_TAGS := $(if ${tags},$(DEV_TAGS) ${tags},$(DEV_TAGS))\n\n# We only return the part inside the double quote here to avoid escape issues\n# when calling the external release script. The second parameter can be used to\n# add additional ldflags if needed (currently only used for the release).\nmake_ldflags = $(1) -X $(PKG)/build.Commit=$(COMMIT)\n\nDEV_GCFLAGS := -gcflags \"all=-N -l\"\nDEV_LDFLAGS := -ldflags \"$(call make_ldflags)\"\n# For the release, we want to remove the symbol table and debug information (-s)\n# and omit the DWARF symbol table (-w). Also we clear the build ID.\nRELEASE_LDFLAGS := $(call make_ldflags, -s -w -buildid=)\n\n# Linting uses a lot of memory, so keep it under control by limiting the number\n# of workers if requested.\nifneq ($(workers),)\nLINT_WORKERS = --concurrency=$(workers)\nendif\n\nDOCKER_TOOLS = docker run \\\n  --rm \\\n  -v $(shell bash -c \"go env GOCACHE || (mkdir -p /tmp/go-cache; echo /tmp/go-cache)\"):/tmp/build/.cache \\\n  -v $(shell bash -c \"go env GOMODCACHE || (mkdir -p /tmp/go-modcache; echo /tmp/go-modcache)\"):/tmp/build/.modcache \\\n  -v $(shell bash -c \"mkdir -p /tmp/go-lint-cache; echo /tmp/go-lint-cache\"):/root/.cache/golangci-lint \\\n  -v $$(pwd):/build lnd-tools\n\nGREEN := \"\\\\033[0;32m\"\nNC := \"\\\\033[0m\"\ndefine print\n\techo $(GREEN)$1$(NC)\nendef\n\ndefault: scratch\n\nall: scratch check install\n\n# ============\n# DEPENDENCIES\n# ============\n$(GOACC_BIN):\n\t@$(call print, \"Installing go-acc.\")\n\tcd $(TOOLS_DIR); go install -trimpath -tags=tools $(GOACC_PKG)\n\n$(BTCD_BIN):\n\t@$(call print, \"Installing btcd.\")\n\tcd $(TOOLS_DIR); go install -trimpath $(BTCD_PKG)\n\n$(GOIMPORTS_BIN):\n\t@$(call print, \"Installing goimports.\")\n\tcd $(TOOLS_DIR); go install -trimpath $(GOIMPORTS_PKG)\n\n# ============\n# INSTALLATION\n# ============\n\n#? build: Build lnd and lncli binaries, place them in project directory\nbuild:\n\t@$(call print, \"Building debug lnd and lncli.\")\n\t$(GOBUILD) -tags=\"$(DEV_TAGS)\" -o lnd-debug $(DEV_GCFLAGS) $(DEV_LDFLAGS) $(PKG)/cmd/lnd\n\t$(GOBUILD) -tags=\"$(DEV_TAGS)\" -o lncli-debug $(DEV_GCFLAGS) $(DEV_LDFLAGS) $(PKG)/cmd/lncli\n\n#? build-itest: Build integration test binaries, place them in itest directory\nbuild-itest:\n\t@$(call print, \"Building itest btcd and lnd.\")\n\tCGO_ENABLED=0 $(GOBUILD) -tags=\"integration\" -o itest/btcd-itest$(EXEC_SUFFIX) $(DEV_LDFLAGS) $(BTCD_PKG)\n\tCGO_ENABLED=0 $(GOBUILD) -tags=\"$(ITEST_TAGS)\" $(ITEST_COVERAGE) -o itest/lnd-itest$(EXEC_SUFFIX) $(DEV_LDFLAGS) $(PKG)/cmd/lnd\n\n\t@$(call print, \"Building itest binary for ${backend} backend.\")\n\tCGO_ENABLED=0 $(GOTEST) -v ./itest -tags=\"$(DEV_TAGS) $(RPC_TAGS) integration $(backend)\" -c -o itest/itest.test$(EXEC_SUFFIX)\n\n#? build-itest-race: Build integration test binaries in race detector mode, place them in itest directory\nbuild-itest-race:\n\t@$(call print, \"Building itest btcd and lnd with race detector.\")\n\tCGO_ENABLED=0 $(GOBUILD) -tags=\"integration\" -o itest/btcd-itest$(EXEC_SUFFIX) $(DEV_LDFLAGS) $(BTCD_PKG)\n\tCGO_ENABLED=1 $(GOBUILD) -race -tags=\"$(ITEST_TAGS)\" -o itest/lnd-itest$(EXEC_SUFFIX) $(DEV_LDFLAGS) $(PKG)/cmd/lnd\n\n\t@$(call print, \"Building itest binary for ${backend} backend.\")\n\tCGO_ENABLED=0 $(GOTEST) -v ./itest -tags=\"$(DEV_TAGS) $(RPC_TAGS) integration $(backend)\" -c -o itest/itest.test$(EXEC_SUFFIX)\n\n#? install-binaries: Build and install lnd and lncli binaries, place them in $GOPATH/bin\ninstall-binaries:\n\t@$(call print, \"Installing lnd and lncli.\")\n\t$(GOINSTALL) -tags=\"${tags}\" -ldflags=\"$(RELEASE_LDFLAGS)\" $(PKG)/cmd/lnd\n\t$(GOINSTALL) -tags=\"${tags}\" -ldflags=\"$(RELEASE_LDFLAGS)\" $(PKG)/cmd/lncli\n\n#? manpages: generate and install man pages\nmanpages:\n\t@$(call print, \"Generating man pages lncli.1 and lnd.1.\")\n\t./scripts/gen_man_pages.sh $(DESTDIR) $(PREFIX)\n\n#? install: Build and install lnd and lncli binaries and place them in $GOPATH/bin.\ninstall: install-binaries\n\n#? install-all: Performs all the same tasks as the install command along with generating and\n# installing the man pages for the lnd and lncli binaries. This command is useful in an\n# environment where a user has root access and so has write access to the man page directory.\ninstall-all: install manpages\n\n#? release-install: Build and install lnd and lncli release binaries, place them in $GOPATH/bin\nrelease-install:\n\t@$(call print, \"Installing release lnd and lncli.\")\n\tenv CGO_ENABLED=0 $(GOINSTALL) -v -trimpath -ldflags=\"$(RELEASE_LDFLAGS)\" -tags=\"$(RELEASE_TAGS)\" $(PKG)/cmd/lnd\n\tenv CGO_ENABLED=0 $(GOINSTALL) -v -trimpath -ldflags=\"$(RELEASE_LDFLAGS)\" -tags=\"$(RELEASE_TAGS)\" $(PKG)/cmd/lncli\n\n#? release: Build the full set of reproducible release binaries for all supported platforms\n# Make sure the generated mobile RPC stubs don't influence our vendor package\n# by removing them first in the clean-mobile target.\nrelease: clean-mobile\n\t@$(call print, \"Releasing lnd and lncli binaries.\")\n\t$(VERSION_CHECK)\n\t./scripts/release.sh build-release \"$(VERSION_TAG)\" \"$(BUILD_SYSTEM)\" \"$(RELEASE_TAGS)\" \"$(RELEASE_LDFLAGS)\" \"$(GO_VERSION)\"\n\n#? docker-release: Same as release but within a docker container to support reproducible builds on BSD/MacOS platforms\ndocker-release:\n\t@$(call print, \"Building release helper docker image.\")\n\tif [ \"$(tag)\" = \"\" ]; then echo \"Must specify tag=<commit_or_tag>!\"; exit 1; fi\n\n\tdocker build -t lnd-release-helper -f make/builder.Dockerfile make/\n\n\t# Run the actual compilation inside the docker image. We pass in all flags\n\t# that we might want to overwrite in manual tests.\n\t$(DOCKER_RELEASE_HELPER) make release tag=\"$(tag)\" sys=\"$(sys)\" COMMIT=\"$(COMMIT)\" \n\ndocker-tools:\n\t@$(call print, \"Building tools docker image.\")\n\tdocker build -q -t lnd-tools $(TOOLS_DIR)\n\nscratch: build\n\n\n# =======\n# TESTING\n# =======\n\n#? check: Run unit and integration tests\ncheck: unit itest\n\ndb-instance:\nifeq ($(dbbackend),postgres)\n\t# Remove a previous postgres instance if it exists.\n\tdocker rm lnd-postgres --force || echo \"Starting new postgres container\"\n\n\t# Start a fresh postgres instance. Allow a maximum of 500 connections so\n\t# that multiple lnd instances with a maximum number of connections of 20\n\t# each can run concurrently. Note that many of the settings here are\n\t# specifically for integration testing and are not fit for running\n\t# production nodes. The increase in max connections ensures that there\n\t# are enough entries allocated for the RWConflictPool to allow multiple\n\t# conflicting transactions to track serialization conflicts. The\n\t# increase in predicate locks and locks per transaction is to allow the\n\t# queries to lock individual rows instead of entire tables, helping\n\t# reduce serialization conflicts. Disabling sequential scan for small\n\t# tables also helps prevent serialization conflicts by ensuring lookups\n\t# lock only relevant rows in the index rather than the entire table.\n\tdocker run --name lnd-postgres -e POSTGRES_PASSWORD=postgres -p 6432:5432 -d postgres:13-alpine -N 1500 -c max_pred_locks_per_transaction=1024 -c max_locks_per_transaction=128 -c enable_seqscan=off\n\tdocker logs -f lnd-postgres >itest/postgres.log 2>&1 &\n\n\t# Wait for the instance to be started.\n\tsleep $(POSTGRES_START_DELAY)\nendif\n\nclean-itest-logs:\n\trm -rf itest/*.log itest/.logs-*\n\n#? itest-only: Only run integration tests without re-building binaries\nitest-only: clean-itest-logs db-instance\n\t@$(call print, \"Running integration tests with ${backend} backend.\")\n\tdate\n\tEXEC_SUFFIX=$(EXEC_SUFFIX) scripts/itest_part.sh 0 1 $(SHUFFLE_SEED) $(TEST_FLAGS) $(ITEST_FLAGS) -test.v\n\t$(COLLECT_ITEST_COVERAGE)\n\n#? itest: Build and run integration tests\nitest: build-itest itest-only\n\n#? itest-race: Build and run integration tests in race detector mode\nitest-race: build-itest-race itest-only\n\n#? itest-parallel: Build and run integration tests in parallel mode, running up to ITEST_PARALLELISM test tranches in parallel (default 4)\nitest-parallel: clean-itest-logs build-itest db-instance\n\t@$(call print, \"Running tests\")\n\tdate\n\tEXEC_SUFFIX=$(EXEC_SUFFIX) scripts/itest_parallel.sh $(ITEST_PARALLELISM) $(NUM_ITEST_TRANCHES) $(SHUFFLE_SEED) $(TEST_FLAGS) $(ITEST_FLAGS)\n\t$(COLLECT_ITEST_COVERAGE)\n\n#? itest-clean: Kill all running itest processes\nitest-clean:\n\t@$(call print, \"Cleaning old itest processes\")\n\tkillall lnd-itest || echo \"no running lnd-itest process found\";\n\n#? unit: Run unit tests\nunit: $(BTCD_BIN)\n\t@$(call print, \"Running unit tests.\")\n\t$(UNIT)\n\n#? unit-module: Run unit tests of all submodules\nunit-module:\n\t@$(call print, \"Running submodule unit tests.\")\n\tscripts/unit_test_modules.sh\n\n#? unit-debug: Run unit tests with debug log output enabled\nunit-debug: $(BTCD_BIN)\n\t@$(call print, \"Running debug unit tests.\")\n\t$(UNIT_DEBUG)\n\n#? unit-cover: Run unit tests in coverage mode\nunit-cover: $(GOACC_BIN)\n\t@$(call print, \"Running unit coverage tests.\")\n\t$(GOACC)\n\n#? unit-race: Run unit tests in race detector mode\nunit-race:\n\t@$(call print, \"Running unit race tests.\")\n\tenv CGO_ENABLED=1 GORACE=\"history_size=7 halt_on_errors=1\" $(UNIT_RACE)\n\n#? unit-bench: Run benchmark tests\nunit-bench: $(BTCD_BIN)\n\t@$(call print, \"Running benchmark tests.\")\n\t$(UNIT_BENCH)\n\n# =============\n# FLAKE HUNTING\n# =============\n\n#? flakehunter: Run the integration tests continuously until one fails\nflakehunter: build-itest\n\t@$(call print, \"Flake hunting ${backend} integration tests.\")\n\twhile [ $$? -eq 0 ]; do make itest-only icase='${icase}' backend='${backend}'; done\n\n#? flake-unit: Run the unit tests continuously until one fails\nflake-unit:\n\t@$(call print, \"Flake hunting unit tests.\")\n\twhile [ $$? -eq 0 ]; do GOTRACEBACK=all $(UNIT) -count=1; done\n\n#? flakehunter-parallel: Run the integration tests continuously until one fails, running up to ITEST_PARALLELISM test tranches in parallel (default 4)\nflakehunter-parallel:\n\t@$(call print, \"Flake hunting ${backend} integration tests in parallel.\")\n\twhile [ $$? -eq 0 ]; do make itest-parallel tranches=1 parallel=${ITEST_PARALLELISM} icase='${icase}' backend='${backend}'; done\n\n# =============\n# FUZZING\n# =============\n\n#? fuzz: Run the fuzzing tests\nfuzz:\n\t@$(call print, \"Fuzzing packages '$(FUZZPKG)'.\")\n\tscripts/fuzz.sh run \"$(FUZZPKG)\" \"$(FUZZ_TEST_RUN_TIME)\" \"$(FUZZ_NUM_PROCESSES)\"\n\n# =========\n# UTILITIES\n# =========\n\n#? fmt: Format source code and fix imports\nfmt: $(GOIMPORTS_BIN)\n\t@$(call print, \"Fixing imports.\")\n\tgosimports -w $(GOFILES_NOVENDOR)\n\t@$(call print, \"Formatting source.\")\n\tgofmt -l -w -s $(GOFILES_NOVENDOR)\n\n#? fmt-check: Make sure source code is formatted and imports are correct\nfmt-check: fmt\n\t@$(call print, \"Checking fmt results.\")\n\tif test -n \"$$(git status --porcelain)\"; then echo \"code not formatted correctly, please run `make fmt` again!\"; git status; git diff; exit 1; fi\n\n#? check-go-version-yaml: Verify that the Go version is correct in all YAML files\ncheck-go-version-yaml:\n\t@$(call print, \"Checking for target Go version (v$(GO_VERSION)) in YAML files (*.yaml, *.yml)\")\n\t./scripts/check-go-version-yaml.sh $(GO_VERSION)\n\n#? check-go-version-dockerfile: Verify that the Go version is correct in all Dockerfile files\ncheck-go-version-dockerfile:\n\t@$(call print, \"Checking for target Go version (v$(GO_VERSION)) in Dockerfile files (*Dockerfile)\")\n\t./scripts/check-go-version-dockerfile.sh $(GO_VERSION)\n\n#? check-go-version: Verify that the Go version is correct in all project files\ncheck-go-version: check-go-version-dockerfile check-go-version-yaml\n\n#? lint-source: Run static code analysis\nlint-source: docker-tools\n\t@$(call print, \"Linting source.\")\n\t$(DOCKER_TOOLS) custom-gcl run -v $(LINT_WORKERS)\n\n#? lint: Run static code analysis\nlint: check-go-version lint-source\n\n#? protolint: Lint proto files using protolint\nprotolint:\n\t@$(call print, \"Linting proto files.\")\n\tdocker run --rm --volume \"$$(pwd):/workspace\" --workdir /workspace yoheimuta/protolint lint lnrpc/\n\n#? tidy-module: Run `go mod` tidy for all modules\ntidy-module:\n\techo \"Running 'go mod tidy' for all modules\"\n\tscripts/tidy_modules.sh\n\n#? tidy-module-check: Make sure all modules are up to date\ntidy-module-check: tidy-module\n\tif test -n \"$$(git status --porcelain)\"; then echo \"modules not updated, please run `make tidy-module` again!\"; git status; exit 1; fi\n\n#? list: List all available make targets\nlist:\n\t@$(call print, \"Listing commands:\")\n\t@$(MAKE) -qp | \\\n\t\tawk -F':' '/^[a-zA-Z0-9][^$$#\\/\\t=]*:([^=]|$$)/ {split($$1,A,/ /);for(i in A)print A[i]}' | \\\n\t\tgrep -v Makefile | \\\n\t\tsort\n\n#? help: List all available make targets with their descriptions\nhelp: Makefile\n\t@$(call print, \"Listing commands:\")\n\t@sed -n 's/^#?//p' $< | column -t -s ':' |  sort | sed -e 's/^/ /'\n\n#? sqlc: Generate sql models and queries in Go\nsqlc:\n\t@$(call print, \"Generating sql models and queries in Go\")\n\t./scripts/gen_sqlc_docker.sh\n\n#? sqlc-check: Make sure sql models and queries are up to date\nsqlc-check: sqlc\n\t@$(call print, \"Verifying sql code generation.\")\n\tif test -n \"$$(git status --porcelain '*.go')\"; then echo \"SQL models not properly generated!\"; git status --porcelain '*.go'; exit 1; fi\n\n#? rpc: Compile protobuf definitions and generate REST proxy stubs\nrpc:\n\t@$(call print, \"Compiling protos.\")\n\tcd ./lnrpc; ./gen_protos_docker.sh\n\n#? rpc-format: Format protobuf definition files\nrpc-format:\n\t@$(call print, \"Formatting protos.\")\n\tcd ./lnrpc; find . -name \"*.proto\" | xargs clang-format --style=file -i\n\n#? rpc-check: Make sure protobuf definitions are up to date\nrpc-check: rpc\n\t@$(call print, \"Verifying protos.\")\n\tcd ./lnrpc; ../scripts/check-rest-annotations.sh\n\tif test -n \"$$(git status --porcelain)\"; then echo \"Protos not properly formatted or not compiled with v3.4.0\"; git status; git diff; exit 1; fi\n\n#? rpc-js-compile: Compile protobuf definitions and generate JSON/WASM stubs\nrpc-js-compile:\n\t@$(call print, \"Compiling JSON/WASM stubs.\")\n\tGOOS=js GOARCH=wasm $(GOBUILD) -tags=\"$(WASM_RELEASE_TAGS)\" $(PKG)/lnrpc/...\n\n#? sample-conf-check: Make sure default values in the sample-lnd.conf file are set correctly\nsample-conf-check:\n\t@$(call print, \"Checking that default values in the sample-lnd.conf file are set correctly\")\n\tscripts/check-sample-lnd-conf.sh \"$(RELEASE_TAGS)\"\n\n#? mobile-rpc: Compile mobile RPC stubs from the protobuf definitions\nmobile-rpc:\n\t@$(call print, \"Creating mobile RPC from protos.\")\n\tcd ./lnrpc; COMPILE_MOBILE=1 SUBSERVER_PREFIX=1 ./gen_protos_docker.sh\n\n#? vendor: Create a vendor directory with all dependencies\nvendor:\n\t@$(call print, \"Re-creating vendor directory.\")\n\trm -r vendor/; go mod vendor\n\n#? apple: Build mobile RPC stubs and project template for iOS and macOS\napple: mobile-rpc\n\t@$(call print, \"Building iOS and macOS cxframework ($(IOS_BUILD)).\")\n\tmkdir -p $(IOS_BUILD_DIR)\n\t$(GOMOBILE_BIN) bind -target=ios,iossimulator,macos -tags=\"mobile $(DEV_TAGS) $(RPC_TAGS)\" -ldflags \"$(RELEASE_LDFLAGS)\" -v -o $(IOS_BUILD) $(MOBILE_PKG)\n\n#? ios: Build mobile RPC stubs and project template for iOS\nios: mobile-rpc\n\t@$(call print, \"Building iOS cxframework ($(IOS_BUILD)).\")\n\tmkdir -p $(IOS_BUILD_DIR)\n\t$(GOMOBILE_BIN) bind -target=ios,iossimulator -tags=\"mobile $(DEV_TAGS) $(RPC_TAGS)\" -ldflags \"$(RELEASE_LDFLAGS)\" -v -o $(IOS_BUILD) $(MOBILE_PKG)\n\n#? macos: Build mobile RPC stubs and project template for macOS\nmacos: mobile-rpc\n\t@$(call print, \"Building macOS cxframework ($(IOS_BUILD)).\")\n\tmkdir -p $(IOS_BUILD_DIR)\n\t$(GOMOBILE_BIN) bind -target=macos -tags=\"mobile $(DEV_TAGS) $(RPC_TAGS)\" -ldflags \"$(RELEASE_LDFLAGS)\" -v -o $(IOS_BUILD) $(MOBILE_PKG)\n\n#? android: Build mobile RPC stubs and project template for Android\nandroid: mobile-rpc\n\t@$(call print, \"Building Android library ($(ANDROID_BUILD)).\")\n\tmkdir -p $(ANDROID_BUILD_DIR)\n\t$(GOMOBILE_BIN) bind -target=android -androidapi 21 -tags=\"mobile $(DEV_TAGS) $(RPC_TAGS)\" -ldflags \"$(RELEASE_LDFLAGS)\" -v -o $(ANDROID_BUILD) $(MOBILE_PKG)\n\n#? mobile: Build mobile RPC stubs and project templates for iOS and Android\nmobile: ios android\n\n#? clean: Remove all generated files\nclean:\n\t@$(call print, \"Cleaning source.$(NC)\")\n\t$(RM) ./lnd-debug ./lncli-debug\n\t$(RM) ./lnd-itest ./lncli-itest\n\t$(RM) -r ./vendor .vendor-new\n\n#? clean-mobile: Remove all generated mobile files\nclean-mobile:\n\t@$(call print, \"Cleaning autogenerated mobile RPC stubs.\")\n\t$(RM) -r mobile/build\n\t$(RM) mobile/*_generated.go\n\n.PHONY: all \\\n\tbtcd \\\n\tdefault \\\n\tbuild \\\n\tinstall \\\n\tscratch \\\n\tcheck \\\n\thelp \\\n\titest-only \\\n\titest \\\n\tunit \\\n\tunit-debug \\\n\tunit-cover \\\n\tunit-race \\\n\tflakehunter \\\n\tflake-unit \\\n\tfmt \\\n\tlint \\\n\tlist \\\n\trpc \\\n\trpc-format \\\n\trpc-check \\\n\trpc-js-compile \\\n\tmobile-rpc \\\n\tvendor \\\n\tios \\\n\tandroid \\\n\tmobile \\\n\tclean\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.1767578125,
          "content": "## Lightning Network Daemon\n\n[![Release build](https://github.com/lightningnetwork/lnd/actions/workflows/release.yaml/badge.svg)](https://github.com/lightningnetwork/lnd/actions/workflows/release.yaml)\n[![MIT licensed](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/lightningnetwork/lnd/blob/master/LICENSE)\n[![Irc](https://img.shields.io/badge/chat-on%20libera-brightgreen.svg)](https://web.libera.chat/#lnd)\n[![Godoc](https://godoc.org/github.com/lightningnetwork/lnd?status.svg)](https://godoc.org/github.com/lightningnetwork/lnd)\n[![Go Report Card](https://goreportcard.com/badge/github.com/lightningnetwork/lnd)](https://goreportcard.com/report/github.com/lightningnetwork/lnd)\n\n<img src=\"logo.png\">\n\nThe Lightning Network Daemon (`lnd`) - is a complete implementation of a\n[Lightning Network](https://lightning.network) node.  `lnd` has several pluggable back-end\nchain services including [`btcd`](https://github.com/btcsuite/btcd) (a\nfull-node), [`bitcoind`](https://github.com/bitcoin/bitcoin), and\n[`neutrino`](https://github.com/lightninglabs/neutrino) (a new experimental light client). The project's codebase uses the\n[btcsuite](https://github.com/btcsuite/) set of Bitcoin libraries, and also\nexports a large set of isolated re-usable Lightning Network related libraries\nwithin it.  In the current state `lnd` is capable of:\n* Creating channels.\n* Closing channels.\n* Completely managing all channel states (including the exceptional ones!).\n* Maintaining a fully authenticated+validated channel graph.\n* Performing path finding within the network, passively forwarding incoming payments.\n* Sending outgoing [onion-encrypted payments](https://github.com/lightningnetwork/lightning-onion)\nthrough the network.\n* Updating advertised fee schedules.\n* Automatic channel management ([`autopilot`](https://github.com/lightningnetwork/lnd/tree/master/autopilot)).\n\n## Lightning Network Specification Compliance\n`lnd` _fully_ conforms to the [Lightning Network specification\n(BOLTs)](https://github.com/lightningnetwork/lightning-rfc). BOLT stands for:\nBasis of Lightning Technology. The specifications are currently being drafted\nby several groups of implementers based around the world including the\ndevelopers of `lnd`. The set of specification documents as well as our\nimplementation of the specification are still a work-in-progress. With that\nsaid, the current status of `lnd`'s BOLT compliance is:\n\n  - [X] BOLT 1: Base Protocol\n  - [X] BOLT 2: Peer Protocol for Channel Management\n  - [X] BOLT 3: Bitcoin Transaction and Script Formats\n  - [X] BOLT 4: Onion Routing Protocol\n  - [X] BOLT 5: Recommendations for On-chain Transaction Handling\n  - [X] BOLT 7: P2P Node and Channel Discovery\n  - [X] BOLT 8: Encrypted and Authenticated Transport\n  - [X] BOLT 9: Assigned Feature Flags\n  - [X] BOLT 10: DNS Bootstrap and Assisted Node Location\n  - [X] BOLT 11: Invoice Protocol for Lightning Payments\n\n## Developer Resources\n\nThe daemon has been designed to be as developer friendly as possible in order\nto facilitate application development on top of `lnd`. Two primary RPC\ninterfaces are exported: an HTTP REST API, and a [gRPC](https://grpc.io/)\nservice. The exported APIs are not yet stable, so be warned: they may change\ndrastically in the near future.\n\nAn automatically generated set of documentation for the RPC APIs can be found\nat [api.lightning.community](https://api.lightning.community). A set of developer\nresources including guides, articles, example applications and community resources can be found at:\n[docs.lightning.engineering](https://docs.lightning.engineering).\n\nFinally, we also have an active\n[Slack](https://lightning.engineering/slack.html) where protocol developers, application developers, testers and users gather to\ndiscuss various aspects of `lnd` and also Lightning in general.\n\n## Installation\n  In order to build from source, please see [the installation\n  instructions](docs/INSTALL.md).\n\n## Docker\n  To run lnd from Docker, please see the main [Docker instructions](docs/DOCKER.md)\n\n## IRC\n  * irc.libera.chat\n  * channel #lnd\n  * [webchat](https://web.libera.chat/#lnd)\n\n## Safety\n\nWhen operating a mainnet `lnd` node, please refer to our [operational safety\nguidelines](docs/safety.md). It is important to note that `lnd` is still\n**beta** software and that ignoring these operational guidelines can lead to\nloss of funds.\n\n## Security\n\nThe developers of `lnd` take security _very_ seriously. The disclosure of\nsecurity vulnerabilities helps us secure the health of `lnd`, privacy of our\nusers, and also the health of the Lightning Network as a whole.  If you find\nany issues regarding security or privacy, please disclose the information\nresponsibly by sending an email to security at lightning dot engineering,\npreferably encrypted using our designated PGP key\n(`91FE464CD75101DA6B6BAB60555C6465E5BCB3AF`) which can be found\n[here](https://gist.githubusercontent.com/Roasbeef/6fb5b52886183239e4aa558f83d085d3/raw/5fa96010af201628bcfa61e9309d9b13d23d220f/security@lightning.engineering).\n\n## Further reading\n* [Step-by-step send payment guide with docker](https://github.com/lightningnetwork/lnd/tree/master/docker)\n* [Contribution guide](https://github.com/lightningnetwork/lnd/blob/master/docs/code_contribution_guidelines.md)\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 0.5302734375,
          "content": "# Security Policy\n\n## Supported Versions\n\nThe last major lnd release is to be considered the current support version. Given an issue severe enough, a backport will be issued either to the prior major release or the set of releases considered utilized enough. \n\n## Reporting a Vulnerability\n\nTo report security issues, send an email to security@lightning.engineering (this list isn't to be used for support). \n\nThe following key can be used to communicate sensitive information: `91FE 464C D751 01DA 6B6B  AB60 555C 6465 E5BC B3AF`. \n"
        },
        {
          "name": "aezeed",
          "type": "tree",
          "content": null
        },
        {
          "name": "aliasmgr",
          "type": "tree",
          "content": null
        },
        {
          "name": "amp",
          "type": "tree",
          "content": null
        },
        {
          "name": "autopilot",
          "type": "tree",
          "content": null
        },
        {
          "name": "batch",
          "type": "tree",
          "content": null
        },
        {
          "name": "blockcache",
          "type": "tree",
          "content": null
        },
        {
          "name": "brontide",
          "type": "tree",
          "content": null
        },
        {
          "name": "buffer",
          "type": "tree",
          "content": null
        },
        {
          "name": "build",
          "type": "tree",
          "content": null
        },
        {
          "name": "cert",
          "type": "tree",
          "content": null
        },
        {
          "name": "chainio",
          "type": "tree",
          "content": null
        },
        {
          "name": "chainntnfs",
          "type": "tree",
          "content": null
        },
        {
          "name": "chainreg",
          "type": "tree",
          "content": null
        },
        {
          "name": "chanacceptor",
          "type": "tree",
          "content": null
        },
        {
          "name": "chanbackup",
          "type": "tree",
          "content": null
        },
        {
          "name": "chanfitness",
          "type": "tree",
          "content": null
        },
        {
          "name": "channel_notifier.go",
          "type": "blob",
          "size": 5.40625,
          "content": "package lnd\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/btcsuite/btcd/wire\"\n\t\"github.com/lightningnetwork/lnd/chanbackup\"\n\t\"github.com/lightningnetwork/lnd/channeldb\"\n\t\"github.com/lightningnetwork/lnd/channelnotifier\"\n)\n\n// channelNotifier is an implementation of the chanbackup.ChannelNotifier\n// interface using the existing channelnotifier.ChannelNotifier struct. This\n// implementation allows us to satisfy all the dependencies of the\n// chanbackup.SubSwapper struct.\ntype channelNotifier struct {\n\t// chanNotifier is the based channel notifier that we'll proxy requests\n\t// from.\n\tchanNotifier *channelnotifier.ChannelNotifier\n\n\t// addrs is an implementation of the addrSource interface that allows\n\t// us to get the latest set of addresses for a given node. We'll need\n\t// this to be able to create an SCB for new channels.\n\taddrs channeldb.AddrSource\n}\n\n// SubscribeChans requests a new channel subscription relative to the initial\n// set of known channels. We use the knownChans as a synchronization point to\n// ensure that the chanbackup.SubSwapper does not miss any channel open or\n// close events in the period between when it's created, and when it requests\n// the channel subscription.\n//\n// NOTE: This is part of the chanbackup.ChannelNotifier interface.\nfunc (c *channelNotifier) SubscribeChans(startingChans map[wire.OutPoint]struct{}) (\n\t*chanbackup.ChannelSubscription, error) {\n\n\tltndLog.Infof(\"Channel backup proxy channel notifier starting\")\n\n\t// TODO(roasbeef): read existing set of chans and diff\n\n\tquit := make(chan struct{})\n\tchanUpdates := make(chan chanbackup.ChannelEvent, 1)\n\n\t// sendChanOpenUpdate is a closure that sends a ChannelEvent to the\n\t// chanUpdates channel to inform subscribers about new pending or\n\t// confirmed channels.\n\tsendChanOpenUpdate := func(newOrPendingChan *channeldb.OpenChannel) {\n\t\t_, nodeAddrs, err := c.addrs.AddrsForNode(\n\t\t\tnewOrPendingChan.IdentityPub,\n\t\t)\n\t\tif err != nil {\n\t\t\tpub := newOrPendingChan.IdentityPub\n\t\t\tltndLog.Errorf(\"unable to fetch addrs for %x: %v\",\n\t\t\t\tpub.SerializeCompressed(), err)\n\t\t}\n\n\t\tchanEvent := chanbackup.ChannelEvent{\n\t\t\tNewChans: []chanbackup.ChannelWithAddrs{\n\t\t\t\t{\n\t\t\t\t\tOpenChannel: newOrPendingChan,\n\t\t\t\t\tAddrs:       nodeAddrs,\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\n\t\tselect {\n\t\tcase chanUpdates <- chanEvent:\n\t\tcase <-quit:\n\t\t\treturn\n\t\t}\n\t}\n\n\t// In order to adhere to the interface, we'll proxy the events from the\n\t// channel notifier to the sub-swapper in a format it understands.\n\tgo func() {\n\t\t// First, we'll subscribe to the primary channel notifier so we can\n\t\t// obtain events for new opened/closed channels.\n\t\tchanSubscription, err := c.chanNotifier.SubscribeChannelEvents()\n\t\tif err != nil {\n\t\t\tpanic(fmt.Sprintf(\"unable to subscribe to chans: %v\",\n\t\t\t\terr))\n\t\t}\n\n\t\tdefer chanSubscription.Cancel()\n\n\t\tfor {\n\t\t\tselect {\n\n\t\t\t// A new event has been sent by the chanNotifier, we'll\n\t\t\t// filter out the events we actually care about and\n\t\t\t// send them to the sub-swapper.\n\t\t\tcase e := <-chanSubscription.Updates():\n\t\t\t\t// TODO(roasbeef): batch dispatch ntnfs\n\n\t\t\t\tswitch event := e.(type) {\n\t\t\t\t// A new channel has been opened and is still\n\t\t\t\t// pending. We can still create a backup, even\n\t\t\t\t// if the final channel ID is not yet available.\n\t\t\t\tcase channelnotifier.PendingOpenChannelEvent:\n\t\t\t\t\tpendingChan := event.PendingChannel\n\t\t\t\t\tsendChanOpenUpdate(pendingChan)\n\n\t\t\t\t// A new channel has been confirmed, we'll\n\t\t\t\t// obtain the node address, then send to the\n\t\t\t\t// sub-swapper.\n\t\t\t\tcase channelnotifier.OpenChannelEvent:\n\t\t\t\t\tsendChanOpenUpdate(event.Channel)\n\n\t\t\t\t// An existing channel has been closed, we'll\n\t\t\t\t// send only the chanPoint of the closed\n\t\t\t\t// channel to the sub-swapper.\n\t\t\t\tcase channelnotifier.ClosedChannelEvent:\n\t\t\t\t\tchanPoint := event.CloseSummary.ChanPoint\n\t\t\t\t\tcloseType := event.CloseSummary.CloseType\n\n\t\t\t\t\t// Because we see the contract as closed\n\t\t\t\t\t// once our local force close TX\n\t\t\t\t\t// confirms, the channel arbitrator\n\t\t\t\t\t// already fires on this event. But\n\t\t\t\t\t// because our funds can be in limbo for\n\t\t\t\t\t// up to 2 weeks worst case we don't\n\t\t\t\t\t// want to remove the crucial info we\n\t\t\t\t\t// need for sweeping that time locked\n\t\t\t\t\t// output before we've actually done so.\n\t\t\t\t\tif closeType == channeldb.LocalForceClose {\n\t\t\t\t\t\tltndLog.Debugf(\"Channel %v \"+\n\t\t\t\t\t\t\t\"was force closed by \"+\n\t\t\t\t\t\t\t\"us, not removing \"+\n\t\t\t\t\t\t\t\"from channel backup \"+\n\t\t\t\t\t\t\t\"until fully resolved\",\n\t\t\t\t\t\t\tchanPoint)\n\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\n\t\t\t\t\tchanEvent := chanbackup.ChannelEvent{\n\t\t\t\t\t\tClosedChans: []wire.OutPoint{\n\t\t\t\t\t\t\tchanPoint,\n\t\t\t\t\t\t},\n\t\t\t\t\t}\n\n\t\t\t\t\tselect {\n\t\t\t\t\tcase chanUpdates <- chanEvent:\n\t\t\t\t\tcase <-quit:\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\n\t\t\t\t// A channel was fully resolved on chain. This\n\t\t\t\t// should only really interest us if it was a\n\t\t\t\t// locally force closed channel where we didn't\n\t\t\t\t// remove the channel already when the close\n\t\t\t\t// event was fired.\n\t\t\t\tcase channelnotifier.FullyResolvedChannelEvent:\n\t\t\t\t\tchanEvent := chanbackup.ChannelEvent{\n\t\t\t\t\t\tClosedChans: []wire.OutPoint{\n\t\t\t\t\t\t\t*event.ChannelPoint,\n\t\t\t\t\t\t},\n\t\t\t\t\t}\n\n\t\t\t\t\tselect {\n\t\t\t\t\tcase chanUpdates <- chanEvent:\n\t\t\t\t\tcase <-quit:\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t// The cancel method has been called, signalling us to\n\t\t\t// exit\n\t\t\tcase <-quit:\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn &chanbackup.ChannelSubscription{\n\t\tChanUpdates: chanUpdates,\n\t\tCancel: func() {\n\t\t\tclose(quit)\n\t\t},\n\t}, nil\n}\n\n// A compile-time constraint to ensure channelNotifier implements\n// chanbackup.ChannelNotifier.\nvar _ chanbackup.ChannelNotifier = (*channelNotifier)(nil)\n"
        },
        {
          "name": "channeldb",
          "type": "tree",
          "content": null
        },
        {
          "name": "channelnotifier",
          "type": "tree",
          "content": null
        },
        {
          "name": "chanrestore.go",
          "type": "blob",
          "size": 12.58203125,
          "content": "package lnd\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"net\"\n\n\t\"github.com/btcsuite/btcd/btcec/v2\"\n\t\"github.com/btcsuite/btcd/chaincfg\"\n\t\"github.com/btcsuite/btcd/chaincfg/chainhash\"\n\t\"github.com/btcsuite/btcd/wire\"\n\t\"github.com/lightningnetwork/lnd/chanbackup\"\n\t\"github.com/lightningnetwork/lnd/channeldb\"\n\t\"github.com/lightningnetwork/lnd/contractcourt\"\n\t\"github.com/lightningnetwork/lnd/keychain\"\n\t\"github.com/lightningnetwork/lnd/lnwire\"\n\t\"github.com/lightningnetwork/lnd/shachain\"\n)\n\nconst (\n\t// mainnetSCBLaunchBlock is the approximate block height of the bitcoin\n\t// mainnet chain of the date when SCBs first were released in lnd\n\t// (v0.6.0-beta). The block date is 4/15/2019, 10:54 PM UTC.\n\tmainnetSCBLaunchBlock = 571800\n\n\t// testnetSCBLaunchBlock is the approximate block height of the bitcoin\n\t// testnet3 chain of the date when SCBs first were released in lnd\n\t// (v0.6.0-beta). The block date is 4/16/2019, 08:04 AM UTC.\n\ttestnetSCBLaunchBlock = 1489300\n)\n\n// chanDBRestorer is an implementation of the chanbackup.ChannelRestorer\n// interface that is able to properly map a Single backup, into a\n// channeldb.ChannelShell which is required to fully restore a channel. We also\n// need the secret key chain in order obtain the prior shachain root so we can\n// verify the DLP protocol as initiated by the remote node.\ntype chanDBRestorer struct {\n\tdb *channeldb.ChannelStateDB\n\n\tsecretKeys keychain.SecretKeyRing\n\n\tchainArb *contractcourt.ChainArbitrator\n}\n\n// openChannelShell maps the static channel back up into an open channel\n// \"shell\". We say shell as this doesn't include all the information required\n// to continue to use the channel, only the minimal amount of information to\n// insert this shell channel back into the database.\nfunc (c *chanDBRestorer) openChannelShell(backup chanbackup.Single) (\n\t*channeldb.ChannelShell, error) {\n\n\tvar err error\n\n\t// Each of the keys in our local channel config only have their\n\t// locators populate, so we'll re-derive the raw key now as we'll need\n\t// it in order to carry out the DLP protocol.\n\tbackup.LocalChanCfg.MultiSigKey, err = c.secretKeys.DeriveKey(\n\t\tbackup.LocalChanCfg.MultiSigKey.KeyLocator,\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to derive multi sig key: %w\",\n\t\t\terr)\n\t}\n\tbackup.LocalChanCfg.RevocationBasePoint, err = c.secretKeys.DeriveKey(\n\t\tbackup.LocalChanCfg.RevocationBasePoint.KeyLocator,\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to derive revocation key: %w\",\n\t\t\terr)\n\t}\n\tbackup.LocalChanCfg.PaymentBasePoint, err = c.secretKeys.DeriveKey(\n\t\tbackup.LocalChanCfg.PaymentBasePoint.KeyLocator,\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to derive payment key: %w\", err)\n\t}\n\tbackup.LocalChanCfg.DelayBasePoint, err = c.secretKeys.DeriveKey(\n\t\tbackup.LocalChanCfg.DelayBasePoint.KeyLocator,\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to derive delay key: %w\", err)\n\t}\n\tbackup.LocalChanCfg.HtlcBasePoint, err = c.secretKeys.DeriveKey(\n\t\tbackup.LocalChanCfg.HtlcBasePoint.KeyLocator,\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to derive htlc key: %w\", err)\n\t}\n\n\t// The shachain root that seeds RevocationProducer for this channel.\n\t// It currently has two possible formats.\n\tvar revRoot *chainhash.Hash\n\n\t// If the PubKey field is non-nil, then this shachain root is using the\n\t// legacy non-ECDH scheme.\n\tif backup.ShaChainRootDesc.PubKey != nil {\n\t\tltndLog.Debugf(\"Using legacy revocation producer format for \"+\n\t\t\t\"channel point %v\", backup.FundingOutpoint)\n\n\t\t// Obtain the private key for the shachain root from the\n\t\t// encoded public key.\n\t\tprivKey, err := c.secretKeys.DerivePrivKey(\n\t\t\tbackup.ShaChainRootDesc,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"could not derive private key \"+\n\t\t\t\t\"for legacy channel revocation root format: \"+\n\t\t\t\t\"%v\", err)\n\t\t}\n\n\t\trevRoot, err = chainhash.NewHash(privKey.Serialize())\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else {\n\t\tltndLog.Debugf(\"Using new ECDH revocation producer format \"+\n\t\t\t\"for channel point %v\", backup.FundingOutpoint)\n\n\t\t// This is the scheme in which the shachain root is derived via\n\t\t// an ECDH operation on the private key of ShaChainRootDesc and\n\t\t// our public multisig key.\n\t\tecdh, err := c.secretKeys.ECDH(\n\t\t\tbackup.ShaChainRootDesc,\n\t\t\tbackup.LocalChanCfg.MultiSigKey.PubKey,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to derive shachain \"+\n\t\t\t\t\"root: %v\", err)\n\t\t}\n\n\t\tch := chainhash.Hash(ecdh)\n\t\trevRoot = &ch\n\t}\n\n\tshaChainProducer := shachain.NewRevocationProducer(*revRoot)\n\n\tvar chanType channeldb.ChannelType\n\tswitch backup.Version {\n\tcase chanbackup.DefaultSingleVersion:\n\t\tchanType = channeldb.SingleFunderBit\n\n\tcase chanbackup.TweaklessCommitVersion:\n\t\tchanType = channeldb.SingleFunderTweaklessBit\n\n\tcase chanbackup.AnchorsCommitVersion:\n\t\tchanType = channeldb.AnchorOutputsBit\n\t\tchanType |= channeldb.SingleFunderTweaklessBit\n\n\tcase chanbackup.AnchorsZeroFeeHtlcTxCommitVersion:\n\t\tchanType = channeldb.ZeroHtlcTxFeeBit\n\t\tchanType |= channeldb.AnchorOutputsBit\n\t\tchanType |= channeldb.SingleFunderTweaklessBit\n\n\tcase chanbackup.ScriptEnforcedLeaseVersion:\n\t\tchanType = channeldb.LeaseExpirationBit\n\t\tchanType |= channeldb.ZeroHtlcTxFeeBit\n\t\tchanType |= channeldb.AnchorOutputsBit\n\t\tchanType |= channeldb.SingleFunderTweaklessBit\n\n\tcase chanbackup.SimpleTaprootVersion:\n\t\tchanType = channeldb.ZeroHtlcTxFeeBit\n\t\tchanType |= channeldb.AnchorOutputsBit\n\t\tchanType |= channeldb.SingleFunderTweaklessBit\n\t\tchanType |= channeldb.SimpleTaprootFeatureBit\n\n\tcase chanbackup.TapscriptRootVersion:\n\t\tchanType = channeldb.ZeroHtlcTxFeeBit\n\t\tchanType |= channeldb.AnchorOutputsBit\n\t\tchanType |= channeldb.SingleFunderTweaklessBit\n\t\tchanType |= channeldb.SimpleTaprootFeatureBit\n\t\tchanType |= channeldb.TapscriptRootBit\n\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown Single version: %w\", err)\n\t}\n\n\tltndLog.Infof(\"SCB Recovery: created channel shell for ChannelPoint\"+\n\t\t\"(%v), chan_type=%v\", backup.FundingOutpoint, chanType)\n\n\tchanShell := channeldb.ChannelShell{\n\t\tNodeAddrs: backup.Addresses,\n\t\tChan: &channeldb.OpenChannel{\n\t\t\tChanType:                chanType,\n\t\t\tChainHash:               backup.ChainHash,\n\t\t\tIsInitiator:             backup.IsInitiator,\n\t\t\tCapacity:                backup.Capacity,\n\t\t\tFundingOutpoint:         backup.FundingOutpoint,\n\t\t\tShortChannelID:          backup.ShortChannelID,\n\t\t\tIdentityPub:             backup.RemoteNodePub,\n\t\t\tIsPending:               false,\n\t\t\tLocalChanCfg:            backup.LocalChanCfg,\n\t\t\tRemoteChanCfg:           backup.RemoteChanCfg,\n\t\t\tRemoteCurrentRevocation: backup.RemoteNodePub,\n\t\t\tRevocationStore:         shachain.NewRevocationStore(),\n\t\t\tRevocationProducer:      shaChainProducer,\n\t\t\tThawHeight:              backup.LeaseExpiry,\n\t\t},\n\t}\n\n\treturn &chanShell, nil\n}\n\n// RestoreChansFromSingles attempts to map the set of single channel backups to\n// channel shells that will be stored persistently. Once these shells have been\n// stored on disk, we'll be able to connect to the channel peer an execute the\n// data loss recovery protocol.\n//\n// NOTE: Part of the chanbackup.ChannelRestorer interface.\nfunc (c *chanDBRestorer) RestoreChansFromSingles(backups ...chanbackup.Single) error {\n\tchannelShells := make([]*channeldb.ChannelShell, 0, len(backups))\n\tfirstChanHeight := uint32(math.MaxUint32)\n\tfor _, backup := range backups {\n\t\tchanShell, err := c.openChannelShell(backup)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Find the block height of the earliest channel in this backup.\n\t\tchanHeight := chanShell.Chan.ShortChanID().BlockHeight\n\t\tif chanHeight != 0 && chanHeight < firstChanHeight {\n\t\t\tfirstChanHeight = chanHeight\n\t\t}\n\n\t\tchannelShells = append(channelShells, chanShell)\n\t}\n\n\t// In case there were only unconfirmed channels, we will have to scan\n\t// the chain beginning from the launch date of SCBs.\n\tif firstChanHeight == math.MaxUint32 {\n\t\tchainHash := channelShells[0].Chan.ChainHash\n\t\tswitch {\n\t\tcase chainHash.IsEqual(chaincfg.MainNetParams.GenesisHash):\n\t\t\tfirstChanHeight = mainnetSCBLaunchBlock\n\n\t\tcase chainHash.IsEqual(chaincfg.TestNet3Params.GenesisHash):\n\t\t\tfirstChanHeight = testnetSCBLaunchBlock\n\n\t\tdefault:\n\t\t\t// Worst case: We have no height hint and start at\n\t\t\t// block 1. Should only happen for SCBs in regtest\n\t\t\t// and simnet.\n\t\t\tfirstChanHeight = 1\n\t\t}\n\t}\n\n\t// If there were channels in the backup that were not confirmed at the\n\t// time of the backup creation, they won't have a block height in the\n\t// ShortChanID which would lead to an error in the chain watcher.\n\t// We want to at least set the funding broadcast height that the chain\n\t// watcher can use instead. We have two possible fallback values for\n\t// the broadcast height that we are going to try here.\n\tfor _, chanShell := range channelShells {\n\t\tchannel := chanShell.Chan\n\n\t\tswitch {\n\t\t// Fallback case 1: This is an unconfirmed channel from an old\n\t\t// backup file where we didn't have any workaround in place and\n\t\t// the short channel ID is 0:0:0. Best we can do here is set the\n\t\t// funding broadcast height to a reasonable value that we\n\t\t// determined earlier.\n\t\tcase channel.ShortChanID().BlockHeight == 0:\n\t\t\tchannel.SetBroadcastHeight(firstChanHeight)\n\n\t\t// Fallback case 2: It is extremely unlikely at this point that\n\t\t// a channel we are trying to restore has a coinbase funding TX.\n\t\t// Therefore we can be quite certain that if the TxIndex is\n\t\t// zero but the block height wasn't, it was an unconfirmed\n\t\t// channel where we used the BlockHeight to encode the funding\n\t\t// TX broadcast height. To not end up with an invalid short\n\t\t// channel ID that looks valid, we restore the \"original\"\n\t\t// unconfirmed one here.\n\t\tcase channel.ShortChannelID.TxIndex == 0:\n\t\t\tbroadcastHeight := channel.ShortChannelID.BlockHeight\n\t\t\tchannel.SetBroadcastHeight(broadcastHeight)\n\t\t\tchannel.ShortChannelID.BlockHeight = 0\n\t\t}\n\t}\n\n\tltndLog.Infof(\"Inserting %v SCB channel shells into DB\",\n\t\tlen(channelShells))\n\n\t// Now that we have all the backups mapped into a series of Singles,\n\t// we'll insert them all into the database.\n\tif err := c.db.RestoreChannelShells(channelShells...); err != nil {\n\t\treturn err\n\t}\n\n\tltndLog.Infof(\"Informing chain watchers of new restored channels\")\n\n\t// Create a slice of channel points.\n\tchanPoints := make([]wire.OutPoint, 0, len(channelShells))\n\n\t// Finally, we'll need to inform the chain arbitrator of these new\n\t// channels so we'll properly watch for their ultimate closure on chain\n\t// and sweep them via the DLP.\n\tfor _, restoredChannel := range channelShells {\n\t\terr := c.chainArb.WatchNewChannel(restoredChannel.Chan)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tchanPoints = append(\n\t\t\tchanPoints, restoredChannel.Chan.FundingOutpoint,\n\t\t)\n\t}\n\n\t// With all the channels restored, we'll now re-send the blockbeat.\n\tc.chainArb.RedispatchBlockbeat(chanPoints)\n\n\treturn nil\n}\n\n// A compile-time constraint to ensure chanDBRestorer implements\n// chanbackup.ChannelRestorer.\nvar _ chanbackup.ChannelRestorer = (*chanDBRestorer)(nil)\n\n// ConnectPeer attempts to connect to the target node at the set of available\n// addresses. Once this method returns with a non-nil error, the connector\n// should attempt to persistently connect to the target peer in the background\n// as a persistent attempt.\n//\n// NOTE: Part of the chanbackup.PeerConnector interface.\nfunc (s *server) ConnectPeer(nodePub *btcec.PublicKey, addrs []net.Addr) error {\n\t// Before we connect to the remote peer, we'll remove any connections\n\t// to ensure the new connection is created after this new link/channel\n\t// is known.\n\tif err := s.DisconnectPeer(nodePub); err != nil {\n\t\tltndLog.Infof(\"Peer(%x) is already connected, proceeding \"+\n\t\t\t\"with chan restore\", nodePub.SerializeCompressed())\n\t}\n\n\t// For each of the known addresses, we'll attempt to launch a\n\t// persistent connection to the (pub, addr) pair. In the event that any\n\t// of them connect, all the other stale requests will be canceled.\n\tfor _, addr := range addrs {\n\t\tnetAddr := &lnwire.NetAddress{\n\t\t\tIdentityKey: nodePub,\n\t\t\tAddress:     addr,\n\t\t}\n\n\t\tltndLog.Infof(\"Attempting to connect to %v for SCB restore \"+\n\t\t\t\"DLP\", netAddr)\n\n\t\t// Attempt to connect to the peer using this full address. If\n\t\t// we're unable to connect to them, then we'll try the next\n\t\t// address in place of it.\n\t\terr := s.ConnectToPeer(netAddr, true, s.cfg.ConnectionTimeout)\n\n\t\t// If we're already connected to this peer, then we don't\n\t\t// consider this an error, so we'll exit here.\n\t\tif _, ok := err.(*errPeerAlreadyConnected); ok {\n\t\t\treturn nil\n\n\t\t} else if err != nil {\n\t\t\t// Otherwise, something else happened, so we'll try the\n\t\t\t// next address.\n\t\t\tltndLog.Errorf(\"unable to connect to %v to \"+\n\t\t\t\t\"complete SCB restore: %v\", netAddr, err)\n\t\t\tcontinue\n\t\t}\n\n\t\t// If we connected no problem, then we can exit early as our\n\t\t// job here is done.\n\t\treturn nil\n\t}\n\n\treturn fmt.Errorf(\"unable to connect to peer %x for SCB restore\",\n\t\tnodePub.SerializeCompressed())\n}\n"
        },
        {
          "name": "clock",
          "type": "tree",
          "content": null
        },
        {
          "name": "cluster",
          "type": "tree",
          "content": null
        },
        {
          "name": "cmd",
          "type": "tree",
          "content": null
        },
        {
          "name": "config.go",
          "type": "blob",
          "size": 92.1484375,
          "content": "// Copyright (c) 2013-2017 The btcsuite developers\n// Copyright (c) 2015-2016 The Decred developers\n// Copyright (C) 2015-2022 The Lightning Network Developers\n\npackage lnd\n\nimport (\n\t\"encoding/hex\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"os\"\n\t\"os/user\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/btcsuite/btcd/btcutil\"\n\t\"github.com/btcsuite/btcd/chaincfg\"\n\tflags \"github.com/jessevdk/go-flags\"\n\t\"github.com/lightninglabs/neutrino\"\n\t\"github.com/lightningnetwork/lnd/autopilot\"\n\t\"github.com/lightningnetwork/lnd/build\"\n\t\"github.com/lightningnetwork/lnd/chainreg\"\n\t\"github.com/lightningnetwork/lnd/chanbackup\"\n\t\"github.com/lightningnetwork/lnd/channeldb\"\n\t\"github.com/lightningnetwork/lnd/discovery\"\n\t\"github.com/lightningnetwork/lnd/funding\"\n\t\"github.com/lightningnetwork/lnd/htlcswitch\"\n\t\"github.com/lightningnetwork/lnd/htlcswitch/hodl\"\n\t\"github.com/lightningnetwork/lnd/input\"\n\t\"github.com/lightningnetwork/lnd/lncfg\"\n\t\"github.com/lightningnetwork/lnd/lnrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/peersrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/routerrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/signrpc\"\n\t\"github.com/lightningnetwork/lnd/lnutils\"\n\t\"github.com/lightningnetwork/lnd/lnwallet\"\n\t\"github.com/lightningnetwork/lnd/lnwire\"\n\t\"github.com/lightningnetwork/lnd/routing\"\n\t\"github.com/lightningnetwork/lnd/signal\"\n\t\"github.com/lightningnetwork/lnd/tor\"\n)\n\nconst (\n\tdefaultDataDirname        = \"data\"\n\tdefaultChainSubDirname    = \"chain\"\n\tdefaultGraphSubDirname    = \"graph\"\n\tdefaultTowerSubDirname    = \"watchtower\"\n\tdefaultTLSCertFilename    = \"tls.cert\"\n\tdefaultTLSKeyFilename     = \"tls.key\"\n\tdefaultAdminMacFilename   = \"admin.macaroon\"\n\tdefaultReadMacFilename    = \"readonly.macaroon\"\n\tdefaultInvoiceMacFilename = \"invoice.macaroon\"\n\tdefaultLogLevel           = \"info\"\n\tdefaultLogDirname         = \"logs\"\n\tdefaultLogFilename        = \"lnd.log\"\n\tdefaultRPCPort            = 10009\n\tdefaultRESTPort           = 8080\n\tdefaultPeerPort           = 9735\n\tdefaultRPCHost            = \"localhost\"\n\n\tdefaultNoSeedBackup                  = false\n\tdefaultPaymentsExpirationGracePeriod = time.Duration(0)\n\tdefaultTrickleDelay                  = 90 * 1000\n\tdefaultChanStatusSampleInterval      = time.Minute\n\tdefaultChanEnableTimeout             = 19 * time.Minute\n\tdefaultChanDisableTimeout            = 20 * time.Minute\n\tdefaultHeightHintCacheQueryDisable   = false\n\tdefaultMinBackoff                    = time.Second\n\tdefaultMaxBackoff                    = time.Hour\n\tdefaultLetsEncryptDirname            = \"letsencrypt\"\n\tdefaultLetsEncryptListen             = \":80\"\n\n\tdefaultTorSOCKSPort            = 9050\n\tdefaultTorDNSHost              = \"soa.nodes.lightning.directory\"\n\tdefaultTorDNSPort              = 53\n\tdefaultTorControlPort          = 9051\n\tdefaultTorV2PrivateKeyFilename = \"v2_onion_private_key\"\n\tdefaultTorV3PrivateKeyFilename = \"v3_onion_private_key\"\n\n\t// defaultZMQReadDeadline is the default read deadline to be used for\n\t// both the block and tx ZMQ subscriptions.\n\tdefaultZMQReadDeadline = 5 * time.Second\n\n\t// DefaultAutogenValidity is the default validity of a self-signed\n\t// certificate. The value corresponds to 14 months\n\t// (14 months * 30 days * 24 hours).\n\tdefaultTLSCertDuration = 14 * 30 * 24 * time.Hour\n\n\t// minTimeLockDelta is the minimum timelock we require for incoming\n\t// HTLCs on our channels.\n\tminTimeLockDelta = routing.MinCLTVDelta\n\n\t// MaxTimeLockDelta is the maximum CLTV delta that can be applied to\n\t// forwarded HTLCs.\n\tMaxTimeLockDelta = routing.MaxCLTVDelta\n\n\t// defaultAcceptorTimeout is the time after which an RPCAcceptor will time\n\t// out and return false if it hasn't yet received a response.\n\tdefaultAcceptorTimeout = 15 * time.Second\n\n\tdefaultAlias = \"\"\n\tdefaultColor = \"#3399FF\"\n\n\t// defaultCoopCloseTargetConfs is the default confirmation target\n\t// that will be used to estimate a fee rate to use during a\n\t// cooperative channel closure initiated by a remote peer. By default\n\t// we'll set this to a lax value since we weren't the ones that\n\t// initiated the channel closure.\n\tdefaultCoopCloseTargetConfs = 6\n\n\t// defaultBlockCacheSize is the size (in bytes) of blocks that will be\n\t// keep in memory if no size is specified.\n\tdefaultBlockCacheSize uint64 = 20 * 1024 * 1024 // 20 MB\n\n\t// defaultHostSampleInterval is the default amount of time that the\n\t// HostAnnouncer will wait between DNS resolutions to check if the\n\t// backing IP of a host has changed.\n\tdefaultHostSampleInterval = time.Minute * 5\n\n\tdefaultChainInterval = time.Minute\n\tdefaultChainTimeout  = time.Second * 30\n\tdefaultChainBackoff  = time.Minute * 2\n\tdefaultChainAttempts = 3\n\n\t// Set defaults for a health check which ensures that we have space\n\t// available on disk. Although this check is off by default so that we\n\t// avoid breaking any existing setups (particularly on mobile), we still\n\t// set the other default values so that the health check can be easily\n\t// enabled with sane defaults.\n\tdefaultRequiredDisk = 0.1\n\tdefaultDiskInterval = time.Hour * 12\n\tdefaultDiskTimeout  = time.Second * 5\n\tdefaultDiskBackoff  = time.Minute\n\tdefaultDiskAttempts = 0\n\n\t// Set defaults for a health check which ensures that the TLS certificate\n\t// is not expired. Although this check is off by default (not all setups\n\t// require it), we still set the other default values so that the health\n\t// check can be easily enabled with sane defaults.\n\tdefaultTLSInterval = time.Minute\n\tdefaultTLSTimeout  = time.Second * 5\n\tdefaultTLSBackoff  = time.Minute\n\tdefaultTLSAttempts = 0\n\n\t// Set defaults for a health check which ensures that the tor\n\t// connection is alive. Although this check is off by default (not all\n\t// setups require it), we still set the other default values so that\n\t// the health check can be easily enabled with sane defaults.\n\tdefaultTCInterval = time.Minute\n\tdefaultTCTimeout  = time.Second * 5\n\tdefaultTCBackoff  = time.Minute\n\tdefaultTCAttempts = 0\n\n\t// Set defaults for a health check which ensures that the remote signer\n\t// RPC connection is alive. Although this check is off by default (only\n\t// active when remote signing is turned on), we still set the other\n\t// default values so that the health check can be easily enabled with\n\t// sane defaults.\n\tdefaultRSInterval = time.Minute\n\tdefaultRSTimeout  = time.Second * 1\n\tdefaultRSBackoff  = time.Second * 30\n\tdefaultRSAttempts = 1\n\n\t// Set defaults for a health check which ensures that the leader\n\t// election is functioning correctly. Although this check is off by\n\t// default (as etcd leader election is only used in a clustered setup),\n\t// we still set the default values so that the health check can be\n\t// easily enabled with sane defaults. Note that by default we only run\n\t// this check once, as it is critical for the node's operation.\n\tdefaultLeaderCheckInterval = time.Minute\n\tdefaultLeaderCheckTimeout  = time.Second * 5\n\tdefaultLeaderCheckBackoff  = time.Second * 5\n\tdefaultLeaderCheckAttempts = 1\n\n\t// defaultRemoteMaxHtlcs specifies the default limit for maximum\n\t// concurrent HTLCs the remote party may add to commitment transactions.\n\t// This value can be overridden with --default-remote-max-htlcs.\n\tdefaultRemoteMaxHtlcs = 483\n\n\t// defaultMaxLocalCSVDelay is the maximum delay we accept on our\n\t// commitment output. The local csv delay maximum is now equal to\n\t// the remote csv delay maximum we require for the remote commitment\n\t// transaction.\n\tdefaultMaxLocalCSVDelay = 2016\n\n\t// defaultChannelCommitInterval is the default maximum time between\n\t// receiving a channel state update and signing a new commitment.\n\tdefaultChannelCommitInterval = 50 * time.Millisecond\n\n\t// maxChannelCommitInterval is the maximum time the commit interval can\n\t// be configured to.\n\tmaxChannelCommitInterval = time.Hour\n\n\t// defaultPendingCommitInterval specifies the default timeout value\n\t// while waiting for the remote party to revoke a locally initiated\n\t// commitment state.\n\tdefaultPendingCommitInterval = 1 * time.Minute\n\n\t// maxPendingCommitInterval specifies the max allowed duration when\n\t// waiting for the remote party to revoke a locally initiated\n\t// commitment state.\n\tmaxPendingCommitInterval = 5 * time.Minute\n\n\t// defaultChannelCommitBatchSize is the default maximum number of\n\t// channel state updates that is accumulated before signing a new\n\t// commitment.\n\tdefaultChannelCommitBatchSize = 10\n\n\t// defaultCoinSelectionStrategy is the coin selection strategy that is\n\t// used by default to fund transactions.\n\tdefaultCoinSelectionStrategy = \"largest\"\n\n\t// defaultKeepFailedPaymentAttempts is the default setting for whether\n\t// to keep failed payments in the database.\n\tdefaultKeepFailedPaymentAttempts = false\n\n\t// defaultGrpcServerPingTime is the default duration for the amount of\n\t// time of no activity after which the server pings the client to see if\n\t// the transport is still alive. If set below 1s, a minimum value of 1s\n\t// will be used instead.\n\tdefaultGrpcServerPingTime = time.Minute\n\n\t// defaultGrpcServerPingTimeout is the default duration the server waits\n\t// after having pinged for keepalive check, and if no activity is seen\n\t// even after that the connection is closed.\n\tdefaultGrpcServerPingTimeout = 20 * time.Second\n\n\t// defaultGrpcClientPingMinWait is the default minimum amount of time a\n\t// client should wait before sending a keepalive ping.\n\tdefaultGrpcClientPingMinWait = 5 * time.Second\n\n\t// defaultHTTPHeaderTimeout is the default timeout for HTTP requests.\n\tDefaultHTTPHeaderTimeout = 5 * time.Second\n\n\t// BitcoinChainName is a string that represents the Bitcoin blockchain.\n\tBitcoinChainName = \"bitcoin\"\n\n\tbitcoindBackendName = \"bitcoind\"\n\tbtcdBackendName     = \"btcd\"\n\tneutrinoBackendName = \"neutrino\"\n)\n\nvar (\n\t// DefaultLndDir is the default directory where lnd tries to find its\n\t// configuration file and store its data. This is a directory in the\n\t// user's application data, for example:\n\t//   C:\\Users\\<username>\\AppData\\Local\\Lnd on Windows\n\t//   ~/.lnd on Linux\n\t//   ~/Library/Application Support/Lnd on MacOS\n\tDefaultLndDir = btcutil.AppDataDir(\"lnd\", false)\n\n\t// DefaultConfigFile is the default full path of lnd's configuration\n\t// file.\n\tDefaultConfigFile = filepath.Join(DefaultLndDir, lncfg.DefaultConfigFilename)\n\n\tdefaultDataDir = filepath.Join(DefaultLndDir, defaultDataDirname)\n\tdefaultLogDir  = filepath.Join(DefaultLndDir, defaultLogDirname)\n\n\tdefaultTowerDir = filepath.Join(defaultDataDir, defaultTowerSubDirname)\n\n\tdefaultTLSCertPath    = filepath.Join(DefaultLndDir, defaultTLSCertFilename)\n\tdefaultTLSKeyPath     = filepath.Join(DefaultLndDir, defaultTLSKeyFilename)\n\tdefaultLetsEncryptDir = filepath.Join(DefaultLndDir, defaultLetsEncryptDirname)\n\n\tdefaultBtcdDir         = btcutil.AppDataDir(btcdBackendName, false)\n\tdefaultBtcdRPCCertFile = filepath.Join(defaultBtcdDir, \"rpc.cert\")\n\n\tdefaultBitcoindDir = btcutil.AppDataDir(BitcoinChainName, false)\n\n\tdefaultTorSOCKS   = net.JoinHostPort(\"localhost\", strconv.Itoa(defaultTorSOCKSPort))\n\tdefaultTorDNS     = net.JoinHostPort(defaultTorDNSHost, strconv.Itoa(defaultTorDNSPort))\n\tdefaultTorControl = net.JoinHostPort(\"localhost\", strconv.Itoa(defaultTorControlPort))\n\n\t// bitcoindEsimateModes defines all the legal values for bitcoind's\n\t// estimatesmartfee RPC call.\n\tdefaultBitcoindEstimateMode = \"CONSERVATIVE\"\n\tbitcoindEstimateModes       = [2]string{\"ECONOMICAL\", defaultBitcoindEstimateMode}\n\n\tdefaultPrunedNodeMaxPeers = 4\n)\n\n// Config defines the configuration options for lnd.\n//\n// See LoadConfig for further details regarding the configuration\n// loading+parsing process.\n//\n//nolint:ll\ntype Config struct {\n\tShowVersion bool `short:\"V\" long:\"version\" description:\"Display version information and exit\"`\n\n\tLndDir       string `long:\"lnddir\" description:\"The base directory that contains lnd's data, logs, configuration file, etc. This option overwrites all other directory options.\"`\n\tConfigFile   string `short:\"C\" long:\"configfile\" description:\"Path to configuration file\"`\n\tDataDir      string `short:\"b\" long:\"datadir\" description:\"The directory to store lnd's data within\"`\n\tSyncFreelist bool   `long:\"sync-freelist\" description:\"Whether the databases used within lnd should sync their freelist to disk. This is disabled by default resulting in improved memory performance during operation, but with an increase in startup time.\"`\n\n\tTLSCertPath        string        `long:\"tlscertpath\" description:\"Path to write the TLS certificate for lnd's RPC and REST services\"`\n\tTLSKeyPath         string        `long:\"tlskeypath\" description:\"Path to write the TLS private key for lnd's RPC and REST services\"`\n\tTLSExtraIPs        []string      `long:\"tlsextraip\" description:\"Adds an extra ip to the generated certificate\"`\n\tTLSExtraDomains    []string      `long:\"tlsextradomain\" description:\"Adds an extra domain to the generated certificate\"`\n\tTLSAutoRefresh     bool          `long:\"tlsautorefresh\" description:\"Re-generate TLS certificate and key if the IPs or domains are changed\"`\n\tTLSDisableAutofill bool          `long:\"tlsdisableautofill\" description:\"Do not include the interface IPs or the system hostname in TLS certificate, use first --tlsextradomain as Common Name instead, if set\"`\n\tTLSCertDuration    time.Duration `long:\"tlscertduration\" description:\"The duration for which the auto-generated TLS certificate will be valid for\"`\n\tTLSEncryptKey      bool          `long:\"tlsencryptkey\" description:\"Automatically encrypts the TLS private key and generates ephemeral TLS key pairs when the wallet is locked or not initialized\"`\n\n\tNoMacaroons     bool          `long:\"no-macaroons\" description:\"Disable macaroon authentication, can only be used if server is not listening on a public interface.\"`\n\tAdminMacPath    string        `long:\"adminmacaroonpath\" description:\"Path to write the admin macaroon for lnd's RPC and REST services if it doesn't exist\"`\n\tReadMacPath     string        `long:\"readonlymacaroonpath\" description:\"Path to write the read-only macaroon for lnd's RPC and REST services if it doesn't exist\"`\n\tInvoiceMacPath  string        `long:\"invoicemacaroonpath\" description:\"Path to the invoice-only macaroon for lnd's RPC and REST services if it doesn't exist\"`\n\tLogDir          string        `long:\"logdir\" description:\"Directory to log output.\"`\n\tMaxLogFiles     int           `long:\"maxlogfiles\" description:\"Maximum logfiles to keep (0 for no rotation). DEPRECATED: use --logging.file.max-files instead\" hidden:\"true\"`\n\tMaxLogFileSize  int           `long:\"maxlogfilesize\" description:\"Maximum logfile size in MB. DEPRECATED: use --logging.file.max-file-size instead\" hidden:\"true\"`\n\tAcceptorTimeout time.Duration `long:\"acceptortimeout\" description:\"Time after which an RPCAcceptor will time out and return false if it hasn't yet received a response\"`\n\n\tLetsEncryptDir    string `long:\"letsencryptdir\" description:\"The directory to store Let's Encrypt certificates within\"`\n\tLetsEncryptListen string `long:\"letsencryptlisten\" description:\"The IP:port on which lnd will listen for Let's Encrypt challenges. Let's Encrypt will always try to contact on port 80. Often non-root processes are not allowed to bind to ports lower than 1024. This configuration option allows a different port to be used, but must be used in combination with port forwarding from port 80. This configuration can also be used to specify another IP address to listen on, for example an IPv6 address.\"`\n\tLetsEncryptDomain string `long:\"letsencryptdomain\" description:\"Request a Let's Encrypt certificate for this domain. Note that the certificate is only requested and stored when the first rpc connection comes in.\"`\n\n\t// We'll parse these 'raw' string arguments into real net.Addrs in the\n\t// loadConfig function. We need to expose the 'raw' strings so the\n\t// command line library can access them.\n\t// Only the parsed net.Addrs should be used!\n\tRawRPCListeners   []string `long:\"rpclisten\" description:\"Add an interface/port/socket to listen for RPC connections\"`\n\tRawRESTListeners  []string `long:\"restlisten\" description:\"Add an interface/port/socket to listen for REST connections\"`\n\tRawListeners      []string `long:\"listen\" description:\"Add an interface/port to listen for peer connections\"`\n\tRawExternalIPs    []string `long:\"externalip\" description:\"Add an ip:port to the list of local addresses we claim to listen on to peers. If a port is not specified, the default (9735) will be used regardless of other parameters\"`\n\tExternalHosts     []string `long:\"externalhosts\" description:\"Add a hostname:port that should be periodically resolved to announce IPs for. If a port is not specified, the default (9735) will be used.\"`\n\tRPCListeners      []net.Addr\n\tRESTListeners     []net.Addr\n\tRestCORS          []string `long:\"restcors\" description:\"Add an ip:port/hostname to allow cross origin access from. To allow all origins, set as \\\"*\\\".\"`\n\tListeners         []net.Addr\n\tExternalIPs       []net.Addr\n\tDisableListen     bool          `long:\"nolisten\" description:\"Disable listening for incoming peer connections\"`\n\tDisableRest       bool          `long:\"norest\" description:\"Disable REST API\"`\n\tDisableRestTLS    bool          `long:\"no-rest-tls\" description:\"Disable TLS for REST connections\"`\n\tWSPingInterval    time.Duration `long:\"ws-ping-interval\" description:\"The ping interval for REST based WebSocket connections, set to 0 to disable sending ping messages from the server side\"`\n\tWSPongWait        time.Duration `long:\"ws-pong-wait\" description:\"The time we wait for a pong response message on REST based WebSocket connections before the connection is closed as inactive\"`\n\tNAT               bool          `long:\"nat\" description:\"Toggle NAT traversal support (using either UPnP or NAT-PMP) to automatically advertise your external IP address to the network -- NOTE this does not support devices behind multiple NATs\"`\n\tAddPeers          []string      `long:\"addpeer\" description:\"Specify peers to connect to first\"`\n\tMinBackoff        time.Duration `long:\"minbackoff\" description:\"Shortest backoff when reconnecting to persistent peers. Valid time units are {s, m, h}.\"`\n\tMaxBackoff        time.Duration `long:\"maxbackoff\" description:\"Longest backoff when reconnecting to persistent peers. Valid time units are {s, m, h}.\"`\n\tConnectionTimeout time.Duration `long:\"connectiontimeout\" description:\"The timeout value for network connections. Valid time units are {ms, s, m, h}.\"`\n\n\tDebugLevel string `short:\"d\" long:\"debuglevel\" description:\"Logging level for all subsystems {trace, debug, info, warn, error, critical} -- You may also specify <global-level>,<subsystem>=<level>,<subsystem2>=<level>,... to set the log level for individual subsystems -- Use show to list available subsystems\"`\n\n\tCPUProfile      string `long:\"cpuprofile\" description:\"DEPRECATED: Use 'pprof.cpuprofile' option. Write CPU profile to the specified file\" hidden:\"true\"`\n\tProfile         string `long:\"profile\" description:\"DEPRECATED: Use 'pprof.profile' option. Enable HTTP profiling on either a port or host:port\" hidden:\"true\"`\n\tBlockingProfile int    `long:\"blockingprofile\" description:\"DEPRECATED: Use 'pprof.blockingprofile' option. Used to enable a blocking profile to be served on the profiling port. This takes a value from 0 to 1, with 1 including every blocking event, and 0 including no events.\" hidden:\"true\"`\n\tMutexProfile    int    `long:\"mutexprofile\" description:\"DEPRECATED: Use 'pprof.mutexprofile' option. Used to Enable a mutex profile to be served on the profiling port. This takes a value from 0 to 1, with 1 including every mutex event, and 0 including no events.\" hidden:\"true\"`\n\n\tPprof *lncfg.Pprof `group:\"Pprof\" namespace:\"pprof\"`\n\n\tUnsafeDisconnect   bool   `long:\"unsafe-disconnect\" description:\"DEPRECATED: Allows the rpcserver to intentionally disconnect from peers with open channels. THIS FLAG WILL BE REMOVED IN 0.10.0\" hidden:\"true\"`\n\tUnsafeReplay       bool   `long:\"unsafe-replay\" description:\"Causes a link to replay the adds on its commitment txn after starting up, this enables testing of the sphinx replay logic.\"`\n\tMaxPendingChannels int    `long:\"maxpendingchannels\" description:\"The maximum number of incoming pending channels permitted per peer.\"`\n\tBackupFilePath     string `long:\"backupfilepath\" description:\"The target location of the channel backup file\"`\n\n\tFeeURL string `long:\"feeurl\" description:\"DEPRECATED: Use 'fee.url' option. Optional URL for external fee estimation. If no URL is specified, the method for fee estimation will depend on the chosen backend and network. Must be set for neutrino on mainnet.\" hidden:\"true\"`\n\n\tBitcoin      *lncfg.Chain    `group:\"Bitcoin\" namespace:\"bitcoin\"`\n\tBtcdMode     *lncfg.Btcd     `group:\"btcd\" namespace:\"btcd\"`\n\tBitcoindMode *lncfg.Bitcoind `group:\"bitcoind\" namespace:\"bitcoind\"`\n\tNeutrinoMode *lncfg.Neutrino `group:\"neutrino\" namespace:\"neutrino\"`\n\n\tBlockCacheSize uint64 `long:\"blockcachesize\" description:\"The maximum capacity of the block cache\"`\n\n\tAutopilot *lncfg.AutoPilot `group:\"Autopilot\" namespace:\"autopilot\"`\n\n\tTor *lncfg.Tor `group:\"Tor\" namespace:\"tor\"`\n\n\tSubRPCServers *subRPCServerConfigs `group:\"subrpc\"`\n\n\tHodl *hodl.Config `group:\"hodl\" namespace:\"hodl\"`\n\n\tNoNetBootstrap bool `long:\"nobootstrap\" description:\"If true, then automatic network bootstrapping will not be attempted.\"`\n\n\tNoSeedBackup             bool   `long:\"noseedbackup\" description:\"If true, NO SEED WILL BE EXPOSED -- EVER, AND THE WALLET WILL BE ENCRYPTED USING THE DEFAULT PASSPHRASE. THIS FLAG IS ONLY FOR TESTING AND SHOULD NEVER BE USED ON MAINNET.\"`\n\tWalletUnlockPasswordFile string `long:\"wallet-unlock-password-file\" description:\"The full path to a file (or pipe/device) that contains the password for unlocking the wallet; if set, no unlocking through RPC is possible and lnd will exit if no wallet exists or the password is incorrect; if wallet-unlock-allow-create is also set then lnd will ignore this flag if no wallet exists and allow a wallet to be created through RPC.\"`\n\tWalletUnlockAllowCreate  bool   `long:\"wallet-unlock-allow-create\" description:\"Don't fail with an error if wallet-unlock-password-file is set but no wallet exists yet.\"`\n\n\tResetWalletTransactions bool `long:\"reset-wallet-transactions\" description:\"Removes all transaction history from the on-chain wallet on startup, forcing a full chain rescan starting at the wallet's birthday. Implements the same functionality as btcwallet's dropwtxmgr command. Should be set to false after successful execution to avoid rescanning on every restart of lnd.\"`\n\n\tCoinSelectionStrategy string `long:\"coin-selection-strategy\" description:\"The strategy to use for selecting coins for wallet transactions.\" choice:\"largest\" choice:\"random\"`\n\n\tPaymentsExpirationGracePeriod time.Duration `long:\"payments-expiration-grace-period\" description:\"A period to wait before force closing channels with outgoing htlcs that have timed-out and are a result of this node initiated payments.\"`\n\tTrickleDelay                  int           `long:\"trickledelay\" description:\"Time in milliseconds between each release of announcements to the network\"`\n\tChanEnableTimeout             time.Duration `long:\"chan-enable-timeout\" description:\"The duration that a peer connection must be stable before attempting to send a channel update to re-enable or cancel a pending disables of the peer's channels on the network.\"`\n\tChanDisableTimeout            time.Duration `long:\"chan-disable-timeout\" description:\"The duration that must elapse after first detecting that an already active channel is actually inactive and sending channel update disabling it to the network. The pending disable can be canceled if the peer reconnects and becomes stable for chan-enable-timeout before the disable update is sent.\"`\n\tChanStatusSampleInterval      time.Duration `long:\"chan-status-sample-interval\" description:\"The polling interval between attempts to detect if an active channel has become inactive due to its peer going offline.\"`\n\tHeightHintCacheQueryDisable   bool          `long:\"height-hint-cache-query-disable\" description:\"Disable queries from the height-hint cache to try to recover channels stuck in the pending close state. Disabling height hint queries may cause longer chain rescans, resulting in a performance hit. Unset this after channels are unstuck so you can get better performance again.\"`\n\tAlias                         string        `long:\"alias\" description:\"The node alias. Used as a moniker by peers and intelligence services\"`\n\tColor                         string        `long:\"color\" description:\"The color of the node in hex format (i.e. '#3399FF'). Used to customize node appearance in intelligence services\"`\n\tMinChanSize                   int64         `long:\"minchansize\" description:\"The smallest channel size (in satoshis) that we should accept. Incoming channels smaller than this will be rejected\"`\n\tMaxChanSize                   int64         `long:\"maxchansize\" description:\"The largest channel size (in satoshis) that we should accept. Incoming channels larger than this will be rejected\"`\n\tCoopCloseTargetConfs          uint32        `long:\"coop-close-target-confs\" description:\"The target number of blocks that a cooperative channel close transaction should confirm in. This is used to estimate the fee to use as the lower bound during fee negotiation for the channel closure.\"`\n\n\tChannelCommitInterval time.Duration `long:\"channel-commit-interval\" description:\"The maximum time that is allowed to pass between receiving a channel state update and signing the next commitment. Setting this to a longer duration allows for more efficient channel operations at the cost of latency.\"`\n\n\tPendingCommitInterval time.Duration `long:\"pending-commit-interval\" description:\"The maximum time that is allowed to pass while waiting for the remote party to revoke a locally initiated commitment state. Setting this to a longer duration if a slow response is expected from the remote party or large number of payments are attempted at the same time.\"`\n\n\tChannelCommitBatchSize uint32 `long:\"channel-commit-batch-size\" description:\"The maximum number of channel state updates that is accumulated before signing a new commitment.\"`\n\n\tKeepFailedPaymentAttempts bool `long:\"keep-failed-payment-attempts\" description:\"Keeps persistent record of all failed payment attempts for successfully settled payments.\"`\n\n\tStoreFinalHtlcResolutions bool `long:\"store-final-htlc-resolutions\" description:\"Persistently store the final resolution of incoming htlcs.\"`\n\n\tDefaultRemoteMaxHtlcs uint16 `long:\"default-remote-max-htlcs\" description:\"The default max_htlc applied when opening or accepting channels. This value limits the number of concurrent HTLCs that the remote party can add to the commitment. The maximum possible value is 483.\"`\n\n\tNumGraphSyncPeers      int           `long:\"numgraphsyncpeers\" description:\"The number of peers that we should receive new graph updates from. This option can be tuned to save bandwidth for light clients or routing nodes.\"`\n\tHistoricalSyncInterval time.Duration `long:\"historicalsyncinterval\" description:\"The polling interval between historical graph sync attempts. Each historical graph sync attempt ensures we reconcile with the remote peer's graph from the genesis block.\"`\n\n\tIgnoreHistoricalGossipFilters bool `long:\"ignore-historical-gossip-filters\" description:\"If true, will not reply with historical data that matches the range specified by a remote peer's gossip_timestamp_filter. Doing so will result in lower memory and bandwidth requirements.\"`\n\n\tRejectPush bool `long:\"rejectpush\" description:\"If true, lnd will not accept channel opening requests with non-zero push amounts. This should prevent accidental pushes to merchant nodes.\"`\n\n\tRejectHTLC bool `long:\"rejecthtlc\" description:\"If true, lnd will not forward any HTLCs that are meant as onward payments. This option will still allow lnd to send HTLCs and receive HTLCs but lnd won't be used as a hop.\"`\n\n\tAcceptPositiveInboundFees bool `long:\"accept-positive-inbound-fees\" description:\"If true, lnd will also allow setting positive inbound fees. By default, lnd only allows to set negative inbound fees (an inbound \\\"discount\\\") to remain backwards compatible with senders whose implementations do not yet support inbound fees.\"`\n\n\t// RequireInterceptor determines whether the HTLC interceptor is\n\t// registered regardless of whether the RPC is called or not.\n\tRequireInterceptor bool `long:\"requireinterceptor\" description:\"Whether to always intercept HTLCs, even if no stream is attached\"`\n\n\tStaggerInitialReconnect bool `long:\"stagger-initial-reconnect\" description:\"If true, will apply a randomized staggering between 0s and 30s when reconnecting to persistent peers on startup. The first 10 reconnections will be attempted instantly, regardless of the flag's value\"`\n\n\tMaxOutgoingCltvExpiry uint32 `long:\"max-cltv-expiry\" description:\"The maximum number of blocks funds could be locked up for when forwarding payments.\"`\n\n\tMaxChannelFeeAllocation float64 `long:\"max-channel-fee-allocation\" description:\"The maximum percentage of total funds that can be allocated to a channel's commitment fee. This only applies for the initiator of the channel. Valid values are within [0.1, 1].\"`\n\n\tMaxCommitFeeRateAnchors uint64 `long:\"max-commit-fee-rate-anchors\" description:\"The maximum fee rate in sat/vbyte that will be used for commitments of channels of the anchors type. Must be large enough to ensure transaction propagation\"`\n\n\tDryRunMigration bool `long:\"dry-run-migration\" description:\"If true, lnd will abort committing a migration if it would otherwise have been successful. This leaves the database unmodified, and still compatible with the previously active version of lnd.\"`\n\n\tnet tor.Net\n\n\tEnableUpfrontShutdown bool `long:\"enable-upfront-shutdown\" description:\"If true, option upfront shutdown script will be enabled. If peers that we open channels with support this feature, we will automatically set the script to which cooperative closes should be paid out to on channel open. This offers the partial protection of a channel peer disconnecting from us if cooperative close is attempted with a different script.\"`\n\n\tAcceptKeySend bool `long:\"accept-keysend\" description:\"If true, spontaneous payments through keysend will be accepted. [experimental]\"`\n\n\tAcceptAMP bool `long:\"accept-amp\" description:\"If true, spontaneous payments via AMP will be accepted.\"`\n\n\tKeysendHoldTime time.Duration `long:\"keysend-hold-time\" description:\"If non-zero, keysend payments are accepted but not immediately settled. If the payment isn't settled manually after the specified time, it is canceled automatically. [experimental]\"`\n\n\tGcCanceledInvoicesOnStartup bool `long:\"gc-canceled-invoices-on-startup\" description:\"If true, we'll attempt to garbage collect canceled invoices upon start.\"`\n\n\tGcCanceledInvoicesOnTheFly bool `long:\"gc-canceled-invoices-on-the-fly\" description:\"If true, we'll delete newly canceled invoices on the fly.\"`\n\n\tDustThreshold uint64 `long:\"dust-threshold\" description:\"DEPRECATED: Sets the max fee exposure in satoshis for a channel after which HTLC's will be failed.\" hidden:\"true\"`\n\n\tMaxFeeExposure uint64 `long:\"channel-max-fee-exposure\" description:\" Limits the maximum fee exposure in satoshis of a channel. This value is enforced for all channels and is independent of the channel initiator.\"`\n\n\tFee *lncfg.Fee `group:\"fee\" namespace:\"fee\"`\n\n\tInvoices *lncfg.Invoices `group:\"invoices\" namespace:\"invoices\"`\n\n\tRouting *lncfg.Routing `group:\"routing\" namespace:\"routing\"`\n\n\tGossip *lncfg.Gossip `group:\"gossip\" namespace:\"gossip\"`\n\n\tWorkers *lncfg.Workers `group:\"workers\" namespace:\"workers\"`\n\n\tCaches *lncfg.Caches `group:\"caches\" namespace:\"caches\"`\n\n\tPrometheus lncfg.Prometheus `group:\"prometheus\" namespace:\"prometheus\"`\n\n\tWtClient *lncfg.WtClient `group:\"wtclient\" namespace:\"wtclient\"`\n\n\tWatchtower *lncfg.Watchtower `group:\"watchtower\" namespace:\"watchtower\"`\n\n\tProtocolOptions *lncfg.ProtocolOptions `group:\"protocol\" namespace:\"protocol\"`\n\n\tAllowCircularRoute bool `long:\"allow-circular-route\" description:\"If true, our node will allow htlc forwards that arrive and depart on the same channel.\"`\n\n\tHealthChecks *lncfg.HealthCheckConfig `group:\"healthcheck\" namespace:\"healthcheck\"`\n\n\tDB *lncfg.DB `group:\"db\" namespace:\"db\"`\n\n\tCluster *lncfg.Cluster `group:\"cluster\" namespace:\"cluster\"`\n\n\tRPCMiddleware *lncfg.RPCMiddleware `group:\"rpcmiddleware\" namespace:\"rpcmiddleware\"`\n\n\tRemoteSigner *lncfg.RemoteSigner `group:\"remotesigner\" namespace:\"remotesigner\"`\n\n\tSweeper *lncfg.Sweeper `group:\"sweeper\" namespace:\"sweeper\"`\n\n\tHtlcswitch *lncfg.Htlcswitch `group:\"htlcswitch\" namespace:\"htlcswitch\"`\n\n\tGRPC *GRPCConfig `group:\"grpc\" namespace:\"grpc\"`\n\n\t// SubLogMgr is the root logger that all the daemon's subloggers are\n\t// hooked up to.\n\tSubLogMgr  *build.SubLoggerManager\n\tLogRotator *build.RotatingLogWriter\n\tLogConfig  *build.LogConfig `group:\"logging\" namespace:\"logging\"`\n\n\t// networkDir is the path to the directory of the currently active\n\t// network. This path will hold the files related to each different\n\t// network.\n\tnetworkDir string\n\n\t// ActiveNetParams contains parameters of the target chain.\n\tActiveNetParams chainreg.BitcoinNetParams\n\n\t// Estimator is used to estimate routing probabilities.\n\tEstimator routing.Estimator\n\n\t// Dev specifies configs used for integration tests, which is always\n\t// empty if not built with `integration` flag.\n\tDev *lncfg.DevConfig `group:\"dev\" namespace:\"dev\"`\n\n\t// HTTPHeaderTimeout is the maximum duration that the server will wait\n\t// before timing out reading the headers of an HTTP request.\n\tHTTPHeaderTimeout time.Duration `long:\"http-header-timeout\" description:\"The maximum duration that the server will wait before timing out reading the headers of an HTTP request.\"`\n}\n\n// GRPCConfig holds the configuration options for the gRPC server.\n// See https://github.com/grpc/grpc-go/blob/v1.41.0/keepalive/keepalive.go#L50\n// for more details. Any value of 0 means we use the gRPC internal default\n// values.\n//\n//nolint:ll\ntype GRPCConfig struct {\n\t// ServerPingTime is a duration for the amount of time of no activity\n\t// after which the server pings the client to see if the transport is\n\t// still alive. If set below 1s, a minimum value of 1s will be used\n\t// instead.\n\tServerPingTime time.Duration `long:\"server-ping-time\" description:\"How long the server waits on a gRPC stream with no activity before pinging the client.\"`\n\n\t// ServerPingTimeout is the duration the server waits after having\n\t// pinged for keepalive check, and if no activity is seen even after\n\t// that the connection is closed.\n\tServerPingTimeout time.Duration `long:\"server-ping-timeout\" description:\"How long the server waits for the response from the client for the keepalive ping response.\"`\n\n\t// ClientPingMinWait is the minimum amount of time a client should wait\n\t// before sending a keepalive ping.\n\tClientPingMinWait time.Duration `long:\"client-ping-min-wait\" description:\"The minimum amount of time the client should wait before sending a keepalive ping.\"`\n\n\t// ClientAllowPingWithoutStream specifies whether pings from the client\n\t// are allowed even if there are no active gRPC streams. This might be\n\t// useful to keep the underlying HTTP/2 connection open for future\n\t// requests.\n\tClientAllowPingWithoutStream bool `long:\"client-allow-ping-without-stream\" description:\"If true, the server allows keepalive pings from the client even when there are no active gRPC streams. This might be useful to keep the underlying HTTP/2 connection open for future requests.\"`\n}\n\n// DefaultConfig returns all default values for the Config struct.\n//\n//nolint:ll\nfunc DefaultConfig() Config {\n\treturn Config{\n\t\tLndDir:            DefaultLndDir,\n\t\tConfigFile:        DefaultConfigFile,\n\t\tDataDir:           defaultDataDir,\n\t\tDebugLevel:        defaultLogLevel,\n\t\tTLSCertPath:       defaultTLSCertPath,\n\t\tTLSKeyPath:        defaultTLSKeyPath,\n\t\tTLSCertDuration:   defaultTLSCertDuration,\n\t\tLetsEncryptDir:    defaultLetsEncryptDir,\n\t\tLetsEncryptListen: defaultLetsEncryptListen,\n\t\tLogDir:            defaultLogDir,\n\t\tAcceptorTimeout:   defaultAcceptorTimeout,\n\t\tWSPingInterval:    lnrpc.DefaultPingInterval,\n\t\tWSPongWait:        lnrpc.DefaultPongWait,\n\t\tBitcoin: &lncfg.Chain{\n\t\t\tMinHTLCIn:     chainreg.DefaultBitcoinMinHTLCInMSat,\n\t\t\tMinHTLCOut:    chainreg.DefaultBitcoinMinHTLCOutMSat,\n\t\t\tBaseFee:       chainreg.DefaultBitcoinBaseFeeMSat,\n\t\t\tFeeRate:       chainreg.DefaultBitcoinFeeRate,\n\t\t\tTimeLockDelta: chainreg.DefaultBitcoinTimeLockDelta,\n\t\t\tMaxLocalDelay: defaultMaxLocalCSVDelay,\n\t\t\tNode:          btcdBackendName,\n\t\t},\n\t\tBtcdMode: &lncfg.Btcd{\n\t\t\tDir:     defaultBtcdDir,\n\t\t\tRPCHost: defaultRPCHost,\n\t\t\tRPCCert: defaultBtcdRPCCertFile,\n\t\t},\n\t\tBitcoindMode: &lncfg.Bitcoind{\n\t\t\tDir:                defaultBitcoindDir,\n\t\t\tRPCHost:            defaultRPCHost,\n\t\t\tEstimateMode:       defaultBitcoindEstimateMode,\n\t\t\tPrunedNodeMaxPeers: defaultPrunedNodeMaxPeers,\n\t\t\tZMQReadDeadline:    defaultZMQReadDeadline,\n\t\t},\n\t\tNeutrinoMode: &lncfg.Neutrino{\n\t\t\tUserAgentName:    neutrino.UserAgentName,\n\t\t\tUserAgentVersion: neutrino.UserAgentVersion,\n\t\t},\n\t\tBlockCacheSize:     defaultBlockCacheSize,\n\t\tMaxPendingChannels: lncfg.DefaultMaxPendingChannels,\n\t\tNoSeedBackup:       defaultNoSeedBackup,\n\t\tMinBackoff:         defaultMinBackoff,\n\t\tMaxBackoff:         defaultMaxBackoff,\n\t\tConnectionTimeout:  tor.DefaultConnTimeout,\n\n\t\tFee: &lncfg.Fee{\n\t\t\tMinUpdateTimeout: lncfg.DefaultMinUpdateTimeout,\n\t\t\tMaxUpdateTimeout: lncfg.DefaultMaxUpdateTimeout,\n\t\t},\n\n\t\tSubRPCServers: &subRPCServerConfigs{\n\t\t\tSignRPC:   &signrpc.Config{},\n\t\t\tRouterRPC: routerrpc.DefaultConfig(),\n\t\t\tPeersRPC:  &peersrpc.Config{},\n\t\t},\n\t\tAutopilot: &lncfg.AutoPilot{\n\t\t\tMaxChannels:    5,\n\t\t\tAllocation:     0.6,\n\t\t\tMinChannelSize: int64(funding.MinChanFundingSize),\n\t\t\tMaxChannelSize: int64(MaxFundingAmount),\n\t\t\tMinConfs:       1,\n\t\t\tConfTarget:     autopilot.DefaultConfTarget,\n\t\t\tHeuristic: map[string]float64{\n\t\t\t\t\"top_centrality\": 1.0,\n\t\t\t},\n\t\t},\n\t\tPaymentsExpirationGracePeriod: defaultPaymentsExpirationGracePeriod,\n\t\tTrickleDelay:                  defaultTrickleDelay,\n\t\tChanStatusSampleInterval:      defaultChanStatusSampleInterval,\n\t\tChanEnableTimeout:             defaultChanEnableTimeout,\n\t\tChanDisableTimeout:            defaultChanDisableTimeout,\n\t\tHeightHintCacheQueryDisable:   defaultHeightHintCacheQueryDisable,\n\t\tAlias:                         defaultAlias,\n\t\tColor:                         defaultColor,\n\t\tMinChanSize:                   int64(funding.MinChanFundingSize),\n\t\tMaxChanSize:                   int64(0),\n\t\tCoopCloseTargetConfs:          defaultCoopCloseTargetConfs,\n\t\tDefaultRemoteMaxHtlcs:         defaultRemoteMaxHtlcs,\n\t\tNumGraphSyncPeers:             defaultMinPeers,\n\t\tHistoricalSyncInterval:        discovery.DefaultHistoricalSyncInterval,\n\t\tTor: &lncfg.Tor{\n\t\t\tSOCKS:   defaultTorSOCKS,\n\t\t\tDNS:     defaultTorDNS,\n\t\t\tControl: defaultTorControl,\n\t\t},\n\t\tnet: &tor.ClearNet{},\n\t\tWorkers: &lncfg.Workers{\n\t\t\tRead:  lncfg.DefaultReadWorkers,\n\t\t\tWrite: lncfg.DefaultWriteWorkers,\n\t\t\tSig:   lncfg.DefaultSigWorkers,\n\t\t},\n\t\tCaches: &lncfg.Caches{\n\t\t\tRejectCacheSize:  channeldb.DefaultRejectCacheSize,\n\t\t\tChannelCacheSize: channeldb.DefaultChannelCacheSize,\n\t\t},\n\t\tPrometheus: lncfg.DefaultPrometheus(),\n\t\tWatchtower: lncfg.DefaultWatchtowerCfg(defaultTowerDir),\n\t\tHealthChecks: &lncfg.HealthCheckConfig{\n\t\t\tChainCheck: &lncfg.CheckConfig{\n\t\t\t\tInterval: defaultChainInterval,\n\t\t\t\tTimeout:  defaultChainTimeout,\n\t\t\t\tAttempts: defaultChainAttempts,\n\t\t\t\tBackoff:  defaultChainBackoff,\n\t\t\t},\n\t\t\tDiskCheck: &lncfg.DiskCheckConfig{\n\t\t\t\tRequiredRemaining: defaultRequiredDisk,\n\t\t\t\tCheckConfig: &lncfg.CheckConfig{\n\t\t\t\t\tInterval: defaultDiskInterval,\n\t\t\t\t\tAttempts: defaultDiskAttempts,\n\t\t\t\t\tTimeout:  defaultDiskTimeout,\n\t\t\t\t\tBackoff:  defaultDiskBackoff,\n\t\t\t\t},\n\t\t\t},\n\t\t\tTLSCheck: &lncfg.CheckConfig{\n\t\t\t\tInterval: defaultTLSInterval,\n\t\t\t\tTimeout:  defaultTLSTimeout,\n\t\t\t\tAttempts: defaultTLSAttempts,\n\t\t\t\tBackoff:  defaultTLSBackoff,\n\t\t\t},\n\t\t\tTorConnection: &lncfg.CheckConfig{\n\t\t\t\tInterval: defaultTCInterval,\n\t\t\t\tTimeout:  defaultTCTimeout,\n\t\t\t\tAttempts: defaultTCAttempts,\n\t\t\t\tBackoff:  defaultTCBackoff,\n\t\t\t},\n\t\t\tRemoteSigner: &lncfg.CheckConfig{\n\t\t\t\tInterval: defaultRSInterval,\n\t\t\t\tTimeout:  defaultRSTimeout,\n\t\t\t\tAttempts: defaultRSAttempts,\n\t\t\t\tBackoff:  defaultRSBackoff,\n\t\t\t},\n\t\t\tLeaderCheck: &lncfg.CheckConfig{\n\t\t\t\tInterval: defaultLeaderCheckInterval,\n\t\t\t\tTimeout:  defaultLeaderCheckTimeout,\n\t\t\t\tAttempts: defaultLeaderCheckAttempts,\n\t\t\t\tBackoff:  defaultLeaderCheckBackoff,\n\t\t\t},\n\t\t},\n\t\tGossip: &lncfg.Gossip{\n\t\t\tMaxChannelUpdateBurst: discovery.DefaultMaxChannelUpdateBurst,\n\t\t\tChannelUpdateInterval: discovery.DefaultChannelUpdateInterval,\n\t\t\tSubBatchDelay:         discovery.DefaultSubBatchDelay,\n\t\t},\n\t\tInvoices: &lncfg.Invoices{\n\t\t\tHoldExpiryDelta: lncfg.DefaultHoldInvoiceExpiryDelta,\n\t\t},\n\t\tRouting: &lncfg.Routing{\n\t\t\tBlindedPaths: lncfg.BlindedPaths{\n\t\t\t\tMinNumRealHops:           lncfg.DefaultMinNumRealBlindedPathHops,\n\t\t\t\tNumHops:                  lncfg.DefaultNumBlindedPathHops,\n\t\t\t\tMaxNumPaths:              lncfg.DefaultMaxNumBlindedPaths,\n\t\t\t\tPolicyIncreaseMultiplier: lncfg.DefaultBlindedPathPolicyIncreaseMultiplier,\n\t\t\t\tPolicyDecreaseMultiplier: lncfg.DefaultBlindedPathPolicyDecreaseMultiplier,\n\t\t\t},\n\t\t},\n\t\tMaxOutgoingCltvExpiry:     htlcswitch.DefaultMaxOutgoingCltvExpiry,\n\t\tMaxChannelFeeAllocation:   htlcswitch.DefaultMaxLinkFeeAllocation,\n\t\tMaxCommitFeeRateAnchors:   lnwallet.DefaultAnchorsCommitMaxFeeRateSatPerVByte,\n\t\tLogRotator:                build.NewRotatingLogWriter(),\n\t\tDB:                        lncfg.DefaultDB(),\n\t\tCluster:                   lncfg.DefaultCluster(),\n\t\tRPCMiddleware:             lncfg.DefaultRPCMiddleware(),\n\t\tActiveNetParams:           chainreg.BitcoinTestNetParams,\n\t\tChannelCommitInterval:     defaultChannelCommitInterval,\n\t\tPendingCommitInterval:     defaultPendingCommitInterval,\n\t\tChannelCommitBatchSize:    defaultChannelCommitBatchSize,\n\t\tCoinSelectionStrategy:     defaultCoinSelectionStrategy,\n\t\tKeepFailedPaymentAttempts: defaultKeepFailedPaymentAttempts,\n\t\tRemoteSigner: &lncfg.RemoteSigner{\n\t\t\tTimeout: lncfg.DefaultRemoteSignerRPCTimeout,\n\t\t},\n\t\tSweeper: lncfg.DefaultSweeperConfig(),\n\t\tHtlcswitch: &lncfg.Htlcswitch{\n\t\t\tMailboxDeliveryTimeout: htlcswitch.DefaultMailboxDeliveryTimeout,\n\t\t},\n\t\tGRPC: &GRPCConfig{\n\t\t\tServerPingTime:    defaultGrpcServerPingTime,\n\t\t\tServerPingTimeout: defaultGrpcServerPingTimeout,\n\t\t\tClientPingMinWait: defaultGrpcClientPingMinWait,\n\t\t},\n\t\tLogConfig:         build.DefaultLogConfig(),\n\t\tWtClient:          lncfg.DefaultWtClientCfg(),\n\t\tHTTPHeaderTimeout: DefaultHTTPHeaderTimeout,\n\t}\n}\n\n// LoadConfig initializes and parses the config using a config file and command\n// line options.\n//\n// The configuration proceeds as follows:\n//  1. Start with a default config with sane settings\n//  2. Pre-parse the command line to check for an alternative config file\n//  3. Load configuration file overwriting defaults with any specified options\n//  4. Parse CLI options and overwrite/add any specified options\nfunc LoadConfig(interceptor signal.Interceptor) (*Config, error) {\n\t// Pre-parse the command line options to pick up an alternative config\n\t// file.\n\tpreCfg := DefaultConfig()\n\tif _, err := flags.Parse(&preCfg); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Show the version and exit if the version flag was specified.\n\tappName := filepath.Base(os.Args[0])\n\tappName = strings.TrimSuffix(appName, filepath.Ext(appName))\n\tusageMessage := fmt.Sprintf(\"Use %s -h to show usage\", appName)\n\tif preCfg.ShowVersion {\n\t\tfmt.Println(appName, \"version\", build.Version(),\n\t\t\t\"commit=\"+build.Commit)\n\t\tos.Exit(0)\n\t}\n\n\t// If the config file path has not been modified by the user, then we'll\n\t// use the default config file path. However, if the user has modified\n\t// their lnddir, then we should assume they intend to use the config\n\t// file within it.\n\tconfigFileDir := CleanAndExpandPath(preCfg.LndDir)\n\tconfigFilePath := CleanAndExpandPath(preCfg.ConfigFile)\n\tswitch {\n\t// User specified --lnddir but no --configfile. Update the config file\n\t// path to the lnd config directory, but don't require it to exist.\n\tcase configFileDir != DefaultLndDir &&\n\t\tconfigFilePath == DefaultConfigFile:\n\n\t\tconfigFilePath = filepath.Join(\n\t\t\tconfigFileDir, lncfg.DefaultConfigFilename,\n\t\t)\n\n\t// User did specify an explicit --configfile, so we check that it does\n\t// exist under that path to avoid surprises.\n\tcase configFilePath != DefaultConfigFile:\n\t\tif !lnrpc.FileExists(configFilePath) {\n\t\t\treturn nil, fmt.Errorf(\"specified config file does \"+\n\t\t\t\t\"not exist in %s\", configFilePath)\n\t\t}\n\t}\n\n\t// Next, load any additional configuration options from the file.\n\tvar configFileError error\n\tcfg := preCfg\n\tfileParser := flags.NewParser(&cfg, flags.Default)\n\terr := flags.NewIniParser(fileParser).ParseFile(configFilePath)\n\tif err != nil {\n\t\t// If it's a parsing related error, then we'll return\n\t\t// immediately, otherwise we can proceed as possibly the config\n\t\t// file doesn't exist which is OK.\n\t\tif lnutils.ErrorAs[*flags.IniError](err) ||\n\t\t\tlnutils.ErrorAs[*flags.Error](err) {\n\n\t\t\treturn nil, err\n\t\t}\n\n\t\tconfigFileError = err\n\t}\n\n\t// Finally, parse the remaining command line options again to ensure\n\t// they take precedence.\n\tflagParser := flags.NewParser(&cfg, flags.Default)\n\tif _, err := flagParser.Parse(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Make sure everything we just loaded makes sense.\n\tcleanCfg, err := ValidateConfig(\n\t\tcfg, interceptor, fileParser, flagParser,\n\t)\n\tvar usageErr *lncfg.UsageError\n\tif errors.As(err, &usageErr) {\n\t\t// The logging system might not yet be initialized, so we also\n\t\t// write to stderr to make sure the error appears somewhere.\n\t\t_, _ = fmt.Fprintln(os.Stderr, usageMessage)\n\t\tltndLog.Warnf(\"Incorrect usage: %v\", usageMessage)\n\n\t\t// The log subsystem might not yet be initialized. But we still\n\t\t// try to log the error there since some packaging solutions\n\t\t// might only look at the log and not stdout/stderr.\n\t\tltndLog.Warnf(\"Error validating config: %v\", err)\n\n\t\treturn nil, err\n\t}\n\tif err != nil {\n\t\t// The log subsystem might not yet be initialized. But we still\n\t\t// try to log the error there since some packaging solutions\n\t\t// might only look at the log and not stdout/stderr.\n\t\tltndLog.Warnf(\"Error validating config: %v\", err)\n\n\t\treturn nil, err\n\t}\n\n\t// Warn about missing config file only after all other configuration is\n\t// done. This prevents the warning on help messages and invalid options.\n\t// Note this should go directly before the return.\n\tif configFileError != nil {\n\t\tltndLog.Warnf(\"%v\", configFileError)\n\t}\n\n\t// Finally, log warnings for deprecated config options if they are set.\n\tlogWarningsForDeprecation(*cleanCfg)\n\n\treturn cleanCfg, nil\n}\n\n// ValidateConfig check the given configuration to be sane. This makes sure no\n// illegal values or combination of values are set. All file system paths are\n// normalized. The cleaned up config is returned on success.\nfunc ValidateConfig(cfg Config, interceptor signal.Interceptor, fileParser,\n\tflagParser *flags.Parser) (*Config, error) {\n\n\t// Special show command to list supported subsystems and exit.\n\tif cfg.DebugLevel == \"show\" {\n\t\tsubLogMgr := build.NewSubLoggerManager()\n\n\t\t// Initialize logging at the default logging level.\n\t\tSetupLoggers(subLogMgr, interceptor)\n\n\t\tfmt.Println(\"Supported subsystems\",\n\t\t\tsubLogMgr.SupportedSubsystems())\n\t\tos.Exit(0)\n\t}\n\n\t// If the provided lnd directory is not the default, we'll modify the\n\t// path to all of the files and directories that will live within it.\n\tlndDir := CleanAndExpandPath(cfg.LndDir)\n\tif lndDir != DefaultLndDir {\n\t\tcfg.DataDir = filepath.Join(lndDir, defaultDataDirname)\n\t\tcfg.LetsEncryptDir = filepath.Join(\n\t\t\tlndDir, defaultLetsEncryptDirname,\n\t\t)\n\t\tcfg.TLSCertPath = filepath.Join(lndDir, defaultTLSCertFilename)\n\t\tcfg.TLSKeyPath = filepath.Join(lndDir, defaultTLSKeyFilename)\n\t\tcfg.LogDir = filepath.Join(lndDir, defaultLogDirname)\n\n\t\t// If the watchtower's directory is set to the default, i.e. the\n\t\t// user has not requested a different location, we'll move the\n\t\t// location to be relative to the specified lnd directory.\n\t\tif cfg.Watchtower.TowerDir == defaultTowerDir {\n\t\t\tcfg.Watchtower.TowerDir = filepath.Join(\n\t\t\t\tcfg.DataDir, defaultTowerSubDirname,\n\t\t\t)\n\t\t}\n\t}\n\n\tfuncName := \"ValidateConfig\"\n\tmkErr := func(format string, args ...interface{}) error {\n\t\treturn fmt.Errorf(funcName+\": \"+format, args...)\n\t}\n\tmakeDirectory := func(dir string) error {\n\t\terr := os.MkdirAll(dir, 0700)\n\t\tif err != nil {\n\t\t\t// Show a nicer error message if it's because a symlink\n\t\t\t// is linked to a directory that does not exist\n\t\t\t// (probably because it's not mounted).\n\t\t\tif e, ok := err.(*os.PathError); ok && os.IsExist(err) {\n\t\t\t\tlink, lerr := os.Readlink(e.Path)\n\t\t\t\tif lerr == nil {\n\t\t\t\t\tstr := \"is symlink %s -> %s mounted?\"\n\t\t\t\t\terr = fmt.Errorf(str, e.Path, link)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tstr := \"Failed to create lnd directory '%s': %v\"\n\t\t\treturn mkErr(str, dir, err)\n\t\t}\n\n\t\treturn nil\n\t}\n\n\t// IsSet returns true if an option has been set in either the config\n\t// file or by a flag.\n\tisSet := func(field string) (bool, error) {\n\t\tfieldName, ok := reflect.TypeOf(Config{}).FieldByName(field)\n\t\tif !ok {\n\t\t\tstr := \"could not find field %s\"\n\t\t\treturn false, mkErr(str, field)\n\t\t}\n\n\t\tlong, ok := fieldName.Tag.Lookup(\"long\")\n\t\tif !ok {\n\t\t\tstr := \"field %s does not have a long tag\"\n\t\t\treturn false, mkErr(str, field)\n\t\t}\n\n\t\t// The user has the option to set the flag in either the config\n\t\t// file or as a command line flag. If any is set, we consider it\n\t\t// to be set, not applying any precedence rules here (since it\n\t\t// is a boolean the default is false anyway which would screw up\n\t\t// any precedence rules). Additionally, we need to also support\n\t\t// the use case where the config struct is embedded _within_\n\t\t// another struct with a prefix (as is the case with\n\t\t// lightning-terminal).\n\t\tfileOption := fileParser.FindOptionByLongName(long)\n\t\tfileOptionNested := fileParser.FindOptionByLongName(\n\t\t\t\"lnd.\" + long,\n\t\t)\n\t\tflagOption := flagParser.FindOptionByLongName(long)\n\t\tflagOptionNested := flagParser.FindOptionByLongName(\n\t\t\t\"lnd.\" + long,\n\t\t)\n\n\t\treturn (fileOption != nil && fileOption.IsSet()) ||\n\t\t\t\t(fileOptionNested != nil && fileOptionNested.IsSet()) ||\n\t\t\t\t(flagOption != nil && flagOption.IsSet()) ||\n\t\t\t\t(flagOptionNested != nil && flagOptionNested.IsSet()),\n\t\t\tnil\n\t}\n\n\t// As soon as we're done parsing configuration options, ensure all paths\n\t// to directories and files are cleaned and expanded before attempting\n\t// to use them later on.\n\tcfg.DataDir = CleanAndExpandPath(cfg.DataDir)\n\tcfg.TLSCertPath = CleanAndExpandPath(cfg.TLSCertPath)\n\tcfg.TLSKeyPath = CleanAndExpandPath(cfg.TLSKeyPath)\n\tcfg.LetsEncryptDir = CleanAndExpandPath(cfg.LetsEncryptDir)\n\tcfg.AdminMacPath = CleanAndExpandPath(cfg.AdminMacPath)\n\tcfg.ReadMacPath = CleanAndExpandPath(cfg.ReadMacPath)\n\tcfg.InvoiceMacPath = CleanAndExpandPath(cfg.InvoiceMacPath)\n\tcfg.LogDir = CleanAndExpandPath(cfg.LogDir)\n\tcfg.BtcdMode.Dir = CleanAndExpandPath(cfg.BtcdMode.Dir)\n\tcfg.BitcoindMode.Dir = CleanAndExpandPath(cfg.BitcoindMode.Dir)\n\tcfg.BitcoindMode.ConfigPath = CleanAndExpandPath(\n\t\tcfg.BitcoindMode.ConfigPath,\n\t)\n\tcfg.BitcoindMode.RPCCookie = CleanAndExpandPath(cfg.BitcoindMode.RPCCookie)\n\tcfg.Tor.PrivateKeyPath = CleanAndExpandPath(cfg.Tor.PrivateKeyPath)\n\tcfg.Tor.WatchtowerKeyPath = CleanAndExpandPath(cfg.Tor.WatchtowerKeyPath)\n\tcfg.Watchtower.TowerDir = CleanAndExpandPath(cfg.Watchtower.TowerDir)\n\tcfg.BackupFilePath = CleanAndExpandPath(cfg.BackupFilePath)\n\tcfg.WalletUnlockPasswordFile = CleanAndExpandPath(\n\t\tcfg.WalletUnlockPasswordFile,\n\t)\n\n\t// Ensure that the user didn't attempt to specify negative values for\n\t// any of the autopilot params.\n\tif cfg.Autopilot.MaxChannels < 0 {\n\t\tstr := \"autopilot.maxchannels must be non-negative\"\n\n\t\treturn nil, mkErr(str)\n\t}\n\tif cfg.Autopilot.Allocation < 0 {\n\t\tstr := \"autopilot.allocation must be non-negative\"\n\n\t\treturn nil, mkErr(str)\n\t}\n\tif cfg.Autopilot.MinChannelSize < 0 {\n\t\tstr := \"autopilot.minchansize must be non-negative\"\n\n\t\treturn nil, mkErr(str)\n\t}\n\tif cfg.Autopilot.MaxChannelSize < 0 {\n\t\tstr := \"autopilot.maxchansize must be non-negative\"\n\n\t\treturn nil, mkErr(str)\n\t}\n\tif cfg.Autopilot.MinConfs < 0 {\n\t\tstr := \"autopilot.minconfs must be non-negative\"\n\n\t\treturn nil, mkErr(str)\n\t}\n\tif cfg.Autopilot.ConfTarget < 1 {\n\t\tstr := \"autopilot.conftarget must be positive\"\n\n\t\treturn nil, mkErr(str)\n\t}\n\n\t// Ensure that the specified values for the min and max channel size\n\t// are within the bounds of the normal chan size constraints.\n\tif cfg.Autopilot.MinChannelSize < int64(funding.MinChanFundingSize) {\n\t\tcfg.Autopilot.MinChannelSize = int64(funding.MinChanFundingSize)\n\t}\n\tif cfg.Autopilot.MaxChannelSize > int64(MaxFundingAmount) {\n\t\tcfg.Autopilot.MaxChannelSize = int64(MaxFundingAmount)\n\t}\n\n\tif _, err := validateAtplCfg(cfg.Autopilot); err != nil {\n\t\treturn nil, mkErr(\"error validating autopilot: %v\", err)\n\t}\n\n\t// Ensure that --maxchansize is properly handled when set by user.\n\t// For non-Wumbo channels this limit remains 16777215 satoshis by default\n\t// as specified in BOLT-02. For wumbo channels this limit is 1,000,000,000.\n\t// satoshis (10 BTC). Always enforce --maxchansize explicitly set by user.\n\t// If unset (marked by 0 value), then enforce proper default.\n\tif cfg.MaxChanSize == 0 {\n\t\tif cfg.ProtocolOptions.Wumbo() {\n\t\t\tcfg.MaxChanSize = int64(funding.MaxBtcFundingAmountWumbo)\n\t\t} else {\n\t\t\tcfg.MaxChanSize = int64(funding.MaxBtcFundingAmount)\n\t\t}\n\t}\n\n\t// Ensure that the user specified values for the min and max channel\n\t// size make sense.\n\tif cfg.MaxChanSize < cfg.MinChanSize {\n\t\treturn nil, mkErr(\"invalid channel size parameters: \"+\n\t\t\t\"max channel size %v, must be no less than min chan \"+\n\t\t\t\"size %v\", cfg.MaxChanSize, cfg.MinChanSize,\n\t\t)\n\t}\n\n\t// Don't allow superfluous --maxchansize greater than\n\t// BOLT 02 soft-limit for non-wumbo channel\n\tif !cfg.ProtocolOptions.Wumbo() &&\n\t\tcfg.MaxChanSize > int64(MaxFundingAmount) {\n\n\t\treturn nil, mkErr(\"invalid channel size parameters: \"+\n\t\t\t\"maximum channel size %v is greater than maximum \"+\n\t\t\t\"non-wumbo channel size %v\", cfg.MaxChanSize,\n\t\t\tMaxFundingAmount,\n\t\t)\n\t}\n\n\t// Ensure that the amount data for revoked commitment transactions is\n\t// stored if the watchtower client is active.\n\tif cfg.DB.NoRevLogAmtData && cfg.WtClient.Active {\n\t\treturn nil, mkErr(\"revocation log amount data must be stored \" +\n\t\t\t\"if the watchtower client is active\")\n\t}\n\n\t// Ensure a valid max channel fee allocation was set.\n\tif cfg.MaxChannelFeeAllocation <= 0 || cfg.MaxChannelFeeAllocation > 1 {\n\t\treturn nil, mkErr(\"invalid max channel fee allocation: %v, \"+\n\t\t\t\"must be within (0, 1]\", cfg.MaxChannelFeeAllocation)\n\t}\n\n\tif cfg.MaxCommitFeeRateAnchors < 1 {\n\t\treturn nil, mkErr(\"invalid max commit fee rate anchors: %v, \"+\n\t\t\t\"must be at least 1 sat/vByte\",\n\t\t\tcfg.MaxCommitFeeRateAnchors)\n\t}\n\n\t// Validate the Tor config parameters.\n\tsocks, err := lncfg.ParseAddressString(\n\t\tcfg.Tor.SOCKS, strconv.Itoa(defaultTorSOCKSPort),\n\t\tcfg.net.ResolveTCPAddr,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tcfg.Tor.SOCKS = socks.String()\n\n\t// We'll only attempt to normalize and resolve the DNS host if it hasn't\n\t// changed, as it doesn't need to be done for the default.\n\tif cfg.Tor.DNS != defaultTorDNS {\n\t\tdns, err := lncfg.ParseAddressString(\n\t\t\tcfg.Tor.DNS, strconv.Itoa(defaultTorDNSPort),\n\t\t\tcfg.net.ResolveTCPAddr,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, mkErr(\"error parsing tor dns: %v\", err)\n\t\t}\n\t\tcfg.Tor.DNS = dns.String()\n\t}\n\n\tcontrol, err := lncfg.ParseAddressString(\n\t\tcfg.Tor.Control, strconv.Itoa(defaultTorControlPort),\n\t\tcfg.net.ResolveTCPAddr,\n\t)\n\tif err != nil {\n\t\treturn nil, mkErr(\"error parsing tor control address: %v\", err)\n\t}\n\tcfg.Tor.Control = control.String()\n\n\t// Ensure that tor socks host:port is not equal to tor control\n\t// host:port. This would lead to lnd not starting up properly.\n\tif cfg.Tor.SOCKS == cfg.Tor.Control {\n\t\tstr := \"tor.socks and tor.control can not us the same host:port\"\n\n\t\treturn nil, mkErr(str)\n\t}\n\n\tswitch {\n\tcase cfg.Tor.V2 && cfg.Tor.V3:\n\t\treturn nil, mkErr(\"either tor.v2 or tor.v3 can be set, \" +\n\t\t\t\"but not both\")\n\tcase cfg.DisableListen && (cfg.Tor.V2 || cfg.Tor.V3):\n\t\treturn nil, mkErr(\"listening must be enabled when enabling \" +\n\t\t\t\"inbound connections over Tor\")\n\t}\n\n\tif cfg.Tor.PrivateKeyPath == \"\" {\n\t\tswitch {\n\t\tcase cfg.Tor.V2:\n\t\t\tcfg.Tor.PrivateKeyPath = filepath.Join(\n\t\t\t\tlndDir, defaultTorV2PrivateKeyFilename,\n\t\t\t)\n\t\tcase cfg.Tor.V3:\n\t\t\tcfg.Tor.PrivateKeyPath = filepath.Join(\n\t\t\t\tlndDir, defaultTorV3PrivateKeyFilename,\n\t\t\t)\n\t\t}\n\t}\n\n\tif cfg.Tor.WatchtowerKeyPath == \"\" {\n\t\tswitch {\n\t\tcase cfg.Tor.V2:\n\t\t\tcfg.Tor.WatchtowerKeyPath = filepath.Join(\n\t\t\t\tcfg.Watchtower.TowerDir,\n\t\t\t\tdefaultTorV2PrivateKeyFilename,\n\t\t\t)\n\t\tcase cfg.Tor.V3:\n\t\t\tcfg.Tor.WatchtowerKeyPath = filepath.Join(\n\t\t\t\tcfg.Watchtower.TowerDir,\n\t\t\t\tdefaultTorV3PrivateKeyFilename,\n\t\t\t)\n\t\t}\n\t}\n\n\t// Set up the network-related functions that will be used throughout\n\t// the daemon. We use the standard Go \"net\" package functions by\n\t// default. If we should be proxying all traffic through Tor, then\n\t// we'll use the Tor proxy specific functions in order to avoid leaking\n\t// our real information.\n\tif cfg.Tor.Active {\n\t\tcfg.net = &tor.ProxyNet{\n\t\t\tSOCKS:                       cfg.Tor.SOCKS,\n\t\t\tDNS:                         cfg.Tor.DNS,\n\t\t\tStreamIsolation:             cfg.Tor.StreamIsolation,\n\t\t\tSkipProxyForClearNetTargets: cfg.Tor.SkipProxyForClearNetTargets,\n\t\t}\n\t}\n\n\tif cfg.DisableListen && cfg.NAT {\n\t\treturn nil, mkErr(\"NAT traversal cannot be used when \" +\n\t\t\t\"listening is disabled\")\n\t}\n\tif cfg.NAT && len(cfg.ExternalHosts) != 0 {\n\t\treturn nil, mkErr(\"NAT support and externalhosts are \" +\n\t\t\t\"mutually exclusive, only one should be selected\")\n\t}\n\n\t// Multiple networks can't be selected simultaneously.  Count\n\t// number of network flags passed; assign active network params\n\t// while we're at it.\n\tnumNets := 0\n\tif cfg.Bitcoin.MainNet {\n\t\tnumNets++\n\t\tcfg.ActiveNetParams = chainreg.BitcoinMainNetParams\n\t}\n\tif cfg.Bitcoin.TestNet3 {\n\t\tnumNets++\n\t\tcfg.ActiveNetParams = chainreg.BitcoinTestNetParams\n\t}\n\tif cfg.Bitcoin.RegTest {\n\t\tnumNets++\n\t\tcfg.ActiveNetParams = chainreg.BitcoinRegTestNetParams\n\t}\n\tif cfg.Bitcoin.SimNet {\n\t\tnumNets++\n\t\tcfg.ActiveNetParams = chainreg.BitcoinSimNetParams\n\n\t\t// For simnet, the btcsuite chain params uses a\n\t\t// cointype of 115. However, we override this in\n\t\t// chainreg/chainparams.go, but the raw ChainParam\n\t\t// field is used elsewhere. To ensure everything is\n\t\t// consistent, we'll also override the cointype within\n\t\t// the raw params.\n\t\ttargetCoinType := chainreg.BitcoinSigNetParams.CoinType\n\t\tcfg.ActiveNetParams.Params.HDCoinType = targetCoinType\n\t}\n\tif cfg.Bitcoin.SigNet {\n\t\tnumNets++\n\t\tcfg.ActiveNetParams = chainreg.BitcoinSigNetParams\n\n\t\t// Let the user overwrite the default signet parameters.\n\t\t// The challenge defines the actual signet network to\n\t\t// join and the seed nodes are needed for network\n\t\t// discovery.\n\t\tsigNetChallenge := chaincfg.DefaultSignetChallenge\n\t\tsigNetSeeds := chaincfg.DefaultSignetDNSSeeds\n\t\tif cfg.Bitcoin.SigNetChallenge != \"\" {\n\t\t\tchallenge, err := hex.DecodeString(\n\t\t\t\tcfg.Bitcoin.SigNetChallenge,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, mkErr(\"Invalid \"+\n\t\t\t\t\t\"signet challenge, hex decode \"+\n\t\t\t\t\t\"failed: %v\", err)\n\t\t\t}\n\t\t\tsigNetChallenge = challenge\n\t\t}\n\n\t\tif len(cfg.Bitcoin.SigNetSeedNode) > 0 {\n\t\t\tsigNetSeeds = make([]chaincfg.DNSSeed, len(\n\t\t\t\tcfg.Bitcoin.SigNetSeedNode,\n\t\t\t))\n\t\t\tfor idx, seed := range cfg.Bitcoin.SigNetSeedNode {\n\t\t\t\tsigNetSeeds[idx] = chaincfg.DNSSeed{\n\t\t\t\t\tHost:         seed,\n\t\t\t\t\tHasFiltering: false,\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tchainParams := chaincfg.CustomSignetParams(\n\t\t\tsigNetChallenge, sigNetSeeds,\n\t\t)\n\t\tcfg.ActiveNetParams.Params = &chainParams\n\t}\n\tif numNets > 1 {\n\t\tstr := \"The mainnet, testnet, regtest, simnet and signet \" +\n\t\t\t\"params can't be used together -- choose one \" +\n\t\t\t\"of the five\"\n\n\t\treturn nil, mkErr(str)\n\t}\n\n\t// The target network must be provided, otherwise, we won't\n\t// know how to initialize the daemon.\n\tif numNets == 0 {\n\t\tstr := \"either --bitcoin.mainnet, or bitcoin.testnet, \" +\n\t\t\t\"bitcoin.simnet, bitcoin.regtest or bitcoin.signet \" +\n\t\t\t\"must be specified\"\n\n\t\treturn nil, mkErr(str)\n\t}\n\n\terr = cfg.Bitcoin.Validate(minTimeLockDelta, funding.MinBtcRemoteDelay)\n\tif err != nil {\n\t\treturn nil, mkErr(\"error validating bitcoin params: %v\", err)\n\t}\n\n\tswitch cfg.Bitcoin.Node {\n\tcase btcdBackendName:\n\t\terr := parseRPCParams(\n\t\t\tcfg.Bitcoin, cfg.BtcdMode, cfg.ActiveNetParams,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, mkErr(\"unable to load RPC \"+\n\t\t\t\t\"credentials for btcd: %v\", err)\n\t\t}\n\tcase bitcoindBackendName:\n\t\tif cfg.Bitcoin.SimNet {\n\t\t\treturn nil, mkErr(\"bitcoind does not \" +\n\t\t\t\t\"support simnet\")\n\t\t}\n\n\t\terr := parseRPCParams(\n\t\t\tcfg.Bitcoin, cfg.BitcoindMode, cfg.ActiveNetParams,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, mkErr(\"unable to load RPC \"+\n\t\t\t\t\"credentials for bitcoind: %v\", err)\n\t\t}\n\tcase neutrinoBackendName:\n\t\t// No need to get RPC parameters.\n\n\tcase \"nochainbackend\":\n\t\t// Nothing to configure, we're running without any chain\n\t\t// backend whatsoever (pure signing mode).\n\n\tdefault:\n\t\tstr := \"only btcd, bitcoind, and neutrino mode \" +\n\t\t\t\"supported for bitcoin at this time\"\n\n\t\treturn nil, mkErr(str)\n\t}\n\n\tcfg.Bitcoin.ChainDir = filepath.Join(\n\t\tcfg.DataDir, defaultChainSubDirname, BitcoinChainName,\n\t)\n\n\t// Ensure that the user didn't attempt to specify negative values for\n\t// any of the autopilot params.\n\tif cfg.Autopilot.MaxChannels < 0 {\n\t\tstr := \"autopilot.maxchannels must be non-negative\"\n\n\t\treturn nil, mkErr(str)\n\t}\n\tif cfg.Autopilot.Allocation < 0 {\n\t\tstr := \"autopilot.allocation must be non-negative\"\n\n\t\treturn nil, mkErr(str)\n\t}\n\tif cfg.Autopilot.MinChannelSize < 0 {\n\t\tstr := \"autopilot.minchansize must be non-negative\"\n\n\t\treturn nil, mkErr(str)\n\t}\n\tif cfg.Autopilot.MaxChannelSize < 0 {\n\t\tstr := \"autopilot.maxchansize must be non-negative\"\n\n\t\treturn nil, mkErr(str)\n\t}\n\n\t// Ensure that the specified values for the min and max channel size\n\t// don't are within the bounds of the normal chan size constraints.\n\tif cfg.Autopilot.MinChannelSize < int64(funding.MinChanFundingSize) {\n\t\tcfg.Autopilot.MinChannelSize = int64(funding.MinChanFundingSize)\n\t}\n\tif cfg.Autopilot.MaxChannelSize > int64(MaxFundingAmount) {\n\t\tcfg.Autopilot.MaxChannelSize = int64(MaxFundingAmount)\n\t}\n\n\t// We'll now construct the network directory which will be where we\n\t// store all the data specific to this chain/network.\n\tcfg.networkDir = filepath.Join(\n\t\tcfg.DataDir, defaultChainSubDirname, BitcoinChainName,\n\t\tlncfg.NormalizeNetwork(cfg.ActiveNetParams.Name),\n\t)\n\n\t// If a custom macaroon directory wasn't specified and the data\n\t// directory has changed from the default path, then we'll also update\n\t// the path for the macaroons to be generated.\n\tif cfg.AdminMacPath == \"\" {\n\t\tcfg.AdminMacPath = filepath.Join(\n\t\t\tcfg.networkDir, defaultAdminMacFilename,\n\t\t)\n\t}\n\tif cfg.ReadMacPath == \"\" {\n\t\tcfg.ReadMacPath = filepath.Join(\n\t\t\tcfg.networkDir, defaultReadMacFilename,\n\t\t)\n\t}\n\tif cfg.InvoiceMacPath == \"\" {\n\t\tcfg.InvoiceMacPath = filepath.Join(\n\t\t\tcfg.networkDir, defaultInvoiceMacFilename,\n\t\t)\n\t}\n\n\ttowerDir := filepath.Join(\n\t\tcfg.Watchtower.TowerDir, BitcoinChainName,\n\t\tlncfg.NormalizeNetwork(cfg.ActiveNetParams.Name),\n\t)\n\n\t// Create the lnd directory and all other sub-directories if they don't\n\t// already exist. This makes sure that directory trees are also created\n\t// for files that point to outside the lnddir.\n\tdirs := []string{\n\t\tlndDir, cfg.DataDir, cfg.networkDir,\n\t\tcfg.LetsEncryptDir, towerDir, cfg.graphDatabaseDir(),\n\t\tfilepath.Dir(cfg.TLSCertPath), filepath.Dir(cfg.TLSKeyPath),\n\t\tfilepath.Dir(cfg.AdminMacPath), filepath.Dir(cfg.ReadMacPath),\n\t\tfilepath.Dir(cfg.InvoiceMacPath),\n\t\tfilepath.Dir(cfg.Tor.PrivateKeyPath),\n\t\tfilepath.Dir(cfg.Tor.WatchtowerKeyPath),\n\t}\n\tfor _, dir := range dirs {\n\t\tif err := makeDirectory(dir); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// Similarly, if a custom back up file path wasn't specified, then\n\t// we'll update the file location to match our set network directory.\n\tif cfg.BackupFilePath == \"\" {\n\t\tcfg.BackupFilePath = filepath.Join(\n\t\t\tcfg.networkDir, chanbackup.DefaultBackupFileName,\n\t\t)\n\t}\n\n\t// Append the network type to the log directory so it is \"namespaced\"\n\t// per network in the same fashion as the data directory.\n\tcfg.LogDir = filepath.Join(\n\t\tcfg.LogDir, BitcoinChainName,\n\t\tlncfg.NormalizeNetwork(cfg.ActiveNetParams.Name),\n\t)\n\n\tif err := cfg.LogConfig.Validate(); err != nil {\n\t\treturn nil, mkErr(\"error validating logging config: %w\", err)\n\t}\n\n\tcfg.SubLogMgr = build.NewSubLoggerManager(build.NewDefaultLogHandlers(\n\t\tcfg.LogConfig, cfg.LogRotator,\n\t)...)\n\n\t// Initialize logging at the default logging level.\n\tSetupLoggers(cfg.SubLogMgr, interceptor)\n\n\tif cfg.MaxLogFiles != 0 {\n\t\tif cfg.LogConfig.File.MaxLogFiles !=\n\t\t\tbuild.DefaultMaxLogFiles {\n\n\t\t\treturn nil, mkErr(\"cannot set both maxlogfiles and \"+\n\t\t\t\t\"logging.file.max-files\", err)\n\t\t}\n\n\t\tcfg.LogConfig.File.MaxLogFiles = cfg.MaxLogFiles\n\t}\n\tif cfg.MaxLogFileSize != 0 {\n\t\tif cfg.LogConfig.File.MaxLogFileSize !=\n\t\t\tbuild.DefaultMaxLogFileSize {\n\n\t\t\treturn nil, mkErr(\"cannot set both maxlogfilesize and \"+\n\t\t\t\t\"logging.file.max-file-size\", err)\n\t\t}\n\n\t\tcfg.LogConfig.File.MaxLogFileSize = cfg.MaxLogFileSize\n\t}\n\n\terr = cfg.LogRotator.InitLogRotator(\n\t\tcfg.LogConfig.File,\n\t\tfilepath.Join(cfg.LogDir, defaultLogFilename),\n\t)\n\tif err != nil {\n\t\tstr := \"log rotation setup failed: %v\"\n\t\treturn nil, mkErr(str, err)\n\t}\n\n\t// Parse, validate, and set debug log level(s).\n\terr = build.ParseAndSetDebugLevels(cfg.DebugLevel, cfg.SubLogMgr)\n\tif err != nil {\n\t\tstr := \"error parsing debug level: %v\"\n\t\treturn nil, &lncfg.UsageError{Err: mkErr(str, err)}\n\t}\n\n\t// At least one RPCListener is required. So listen on localhost per\n\t// default.\n\tif len(cfg.RawRPCListeners) == 0 {\n\t\taddr := fmt.Sprintf(\"localhost:%d\", defaultRPCPort)\n\t\tcfg.RawRPCListeners = append(cfg.RawRPCListeners, addr)\n\t}\n\n\t// Listen on localhost if no REST listeners were specified.\n\tif len(cfg.RawRESTListeners) == 0 {\n\t\taddr := fmt.Sprintf(\"localhost:%d\", defaultRESTPort)\n\t\tcfg.RawRESTListeners = append(cfg.RawRESTListeners, addr)\n\t}\n\n\t// Listen on the default interface/port if no listeners were specified.\n\t// An empty address string means default interface/address, which on\n\t// most unix systems is the same as 0.0.0.0. If Tor is active, we\n\t// default to only listening on localhost for hidden service\n\t// connections.\n\tif len(cfg.RawListeners) == 0 {\n\t\taddr := fmt.Sprintf(\":%d\", defaultPeerPort)\n\t\tif cfg.Tor.Active && !cfg.Tor.SkipProxyForClearNetTargets {\n\t\t\taddr = fmt.Sprintf(\"localhost:%d\", defaultPeerPort)\n\t\t}\n\t\tcfg.RawListeners = append(cfg.RawListeners, addr)\n\t}\n\n\t// Add default port to all RPC listener addresses if needed and remove\n\t// duplicate addresses.\n\tcfg.RPCListeners, err = lncfg.NormalizeAddresses(\n\t\tcfg.RawRPCListeners, strconv.Itoa(defaultRPCPort),\n\t\tcfg.net.ResolveTCPAddr,\n\t)\n\tif err != nil {\n\t\treturn nil, mkErr(\"error normalizing RPC listen addrs: %v\", err)\n\t}\n\n\t// Add default port to all REST listener addresses if needed and remove\n\t// duplicate addresses.\n\tcfg.RESTListeners, err = lncfg.NormalizeAddresses(\n\t\tcfg.RawRESTListeners, strconv.Itoa(defaultRESTPort),\n\t\tcfg.net.ResolveTCPAddr,\n\t)\n\tif err != nil {\n\t\treturn nil, mkErr(\"error normalizing REST listen addrs: %v\", err)\n\t}\n\n\tswitch {\n\t// The no seed backup and auto unlock are mutually exclusive.\n\tcase cfg.NoSeedBackup && cfg.WalletUnlockPasswordFile != \"\":\n\t\treturn nil, mkErr(\"cannot set noseedbackup and \" +\n\t\t\t\"wallet-unlock-password-file at the same time\")\n\n\t// The \"allow-create\" flag cannot be set without the auto unlock file.\n\tcase cfg.WalletUnlockAllowCreate && cfg.WalletUnlockPasswordFile == \"\":\n\t\treturn nil, mkErr(\"cannot set wallet-unlock-allow-create \" +\n\t\t\t\"without wallet-unlock-password-file\")\n\n\t// If a password file was specified, we need it to exist.\n\tcase cfg.WalletUnlockPasswordFile != \"\" &&\n\t\t!lnrpc.FileExists(cfg.WalletUnlockPasswordFile):\n\n\t\treturn nil, mkErr(\"wallet unlock password file %s does \"+\n\t\t\t\"not exist\", cfg.WalletUnlockPasswordFile)\n\t}\n\n\t// For each of the RPC listeners (REST+gRPC), we'll ensure that users\n\t// have specified a safe combo for authentication. If not, we'll bail\n\t// out with an error. Since we don't allow disabling TLS for gRPC\n\t// connections we pass in tlsActive=true.\n\terr = lncfg.EnforceSafeAuthentication(\n\t\tcfg.RPCListeners, !cfg.NoMacaroons, true,\n\t)\n\tif err != nil {\n\t\treturn nil, mkErr(\"error enforcing safe authentication on \"+\n\t\t\t\"RPC ports: %v\", err)\n\t}\n\n\tif cfg.DisableRest {\n\t\tltndLog.Infof(\"REST API is disabled!\")\n\t\tcfg.RESTListeners = nil\n\t} else {\n\t\terr = lncfg.EnforceSafeAuthentication(\n\t\t\tcfg.RESTListeners, !cfg.NoMacaroons, !cfg.DisableRestTLS,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, mkErr(\"error enforcing safe \"+\n\t\t\t\t\"authentication on REST ports: %v\", err)\n\t\t}\n\t}\n\n\t// Remove the listening addresses specified if listening is disabled.\n\tif cfg.DisableListen {\n\t\tltndLog.Infof(\"Listening on the p2p interface is disabled!\")\n\t\tcfg.Listeners = nil\n\t\tcfg.ExternalIPs = nil\n\t} else {\n\n\t\t// Add default port to all listener addresses if needed and remove\n\t\t// duplicate addresses.\n\t\tcfg.Listeners, err = lncfg.NormalizeAddresses(\n\t\t\tcfg.RawListeners, strconv.Itoa(defaultPeerPort),\n\t\t\tcfg.net.ResolveTCPAddr,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, mkErr(\"error normalizing p2p listen \"+\n\t\t\t\t\"addrs: %v\", err)\n\t\t}\n\n\t\t// Add default port to all external IP addresses if needed and remove\n\t\t// duplicate addresses.\n\t\tcfg.ExternalIPs, err = lncfg.NormalizeAddresses(\n\t\t\tcfg.RawExternalIPs, strconv.Itoa(defaultPeerPort),\n\t\t\tcfg.net.ResolveTCPAddr,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// For the p2p port it makes no sense to listen to an Unix socket.\n\t\t// Also, we would need to refactor the brontide listener to support\n\t\t// that.\n\t\tfor _, p2pListener := range cfg.Listeners {\n\t\t\tif lncfg.IsUnix(p2pListener) {\n\t\t\t\treturn nil, mkErr(\"unix socket addresses \"+\n\t\t\t\t\t\"cannot be used for the p2p \"+\n\t\t\t\t\t\"connection listener: %s\", p2pListener)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Ensure that the specified minimum backoff is below or equal to the\n\t// maximum backoff.\n\tif cfg.MinBackoff > cfg.MaxBackoff {\n\t\treturn nil, mkErr(\"maxbackoff must be greater than minbackoff\")\n\t}\n\n\t// Newer versions of lnd added a new sub-config for bolt-specific\n\t// parameters. However, we want to also allow existing users to use the\n\t// value on the top-level config. If the outer config value is set,\n\t// then we'll use that directly.\n\tflagSet, err := isSet(\"SyncFreelist\")\n\tif err != nil {\n\t\treturn nil, mkErr(\"error parsing freelist sync flag: %v\", err)\n\t}\n\tif flagSet {\n\t\tcfg.DB.Bolt.NoFreelistSync = !cfg.SyncFreelist\n\t}\n\n\t// Parse any extra sqlite pragma options that may have been provided\n\t// to determine if they override any of the defaults that we will\n\t// otherwise add.\n\tvar (\n\t\tdefaultSynchronous = true\n\t\tdefaultAutoVacuum  = true\n\t\tdefaultFullfsync   = true\n\t)\n\tfor _, option := range cfg.DB.Sqlite.PragmaOptions {\n\t\tswitch {\n\t\tcase strings.HasPrefix(option, \"synchronous=\"):\n\t\t\tdefaultSynchronous = false\n\n\t\tcase strings.HasPrefix(option, \"auto_vacuum=\"):\n\t\t\tdefaultAutoVacuum = false\n\n\t\tcase strings.HasPrefix(option, \"fullfsync=\"):\n\t\t\tdefaultFullfsync = false\n\n\t\tdefault:\n\t\t}\n\t}\n\n\tif defaultSynchronous {\n\t\tcfg.DB.Sqlite.PragmaOptions = append(\n\t\t\tcfg.DB.Sqlite.PragmaOptions, \"synchronous=full\",\n\t\t)\n\t}\n\n\tif defaultAutoVacuum {\n\t\tcfg.DB.Sqlite.PragmaOptions = append(\n\t\t\tcfg.DB.Sqlite.PragmaOptions, \"auto_vacuum=incremental\",\n\t\t)\n\t}\n\n\tif defaultFullfsync {\n\t\tcfg.DB.Sqlite.PragmaOptions = append(\n\t\t\tcfg.DB.Sqlite.PragmaOptions, \"fullfsync=true\",\n\t\t)\n\t}\n\n\t// Ensure that the user hasn't chosen a remote-max-htlc value greater\n\t// than the protocol maximum.\n\tmaxRemoteHtlcs := uint16(input.MaxHTLCNumber / 2)\n\tif cfg.DefaultRemoteMaxHtlcs > maxRemoteHtlcs {\n\t\treturn nil, mkErr(\"default-remote-max-htlcs (%v) must be \"+\n\t\t\t\"less than %v\", cfg.DefaultRemoteMaxHtlcs,\n\t\t\tmaxRemoteHtlcs)\n\t}\n\n\t// Clamp the ChannelCommitInterval so that commitment updates can still\n\t// happen in a reasonable timeframe.\n\tif cfg.ChannelCommitInterval > maxChannelCommitInterval {\n\t\treturn nil, mkErr(\"channel-commit-interval (%v) must be less \"+\n\t\t\t\"than %v\", cfg.ChannelCommitInterval,\n\t\t\tmaxChannelCommitInterval)\n\t}\n\n\t// Limit PendingCommitInterval so we don't wait too long for the remote\n\t// party to send back a revoke.\n\tif cfg.PendingCommitInterval > maxPendingCommitInterval {\n\t\treturn nil, mkErr(\"pending-commit-interval (%v) must be less \"+\n\t\t\t\"than %v\", cfg.PendingCommitInterval,\n\t\t\tmaxPendingCommitInterval)\n\t}\n\n\tif err := cfg.Gossip.Parse(); err != nil {\n\t\treturn nil, mkErr(\"error parsing gossip syncer: %v\", err)\n\t}\n\n\t// If the experimental protocol options specify any protocol messages\n\t// that we want to handle as custom messages, set them now.\n\tcustomMsg := cfg.ProtocolOptions.CustomMessageOverrides()\n\n\t// We can safely set our custom override values during startup because\n\t// startup is blocked on config parsing.\n\tif err := lnwire.SetCustomOverrides(customMsg); err != nil {\n\t\treturn nil, mkErr(\"custom-message: %v\", err)\n\t}\n\n\t// Map old pprof flags to new pprof group flags.\n\t//\n\t// NOTE: This is a temporary measure to ensure compatibility with old\n\t// flags.\n\tif cfg.CPUProfile != \"\" {\n\t\tif cfg.Pprof.CPUProfile != \"\" {\n\t\t\treturn nil, mkErr(\"cpuprofile and pprof.cpuprofile \" +\n\t\t\t\t\"are mutually exclusive\")\n\t\t}\n\t\tcfg.Pprof.CPUProfile = cfg.CPUProfile\n\t}\n\tif cfg.Profile != \"\" {\n\t\tif cfg.Pprof.Profile != \"\" {\n\t\t\treturn nil, mkErr(\"profile and pprof.profile \" +\n\t\t\t\t\"are mutually exclusive\")\n\t\t}\n\t\tcfg.Pprof.Profile = cfg.Profile\n\t}\n\tif cfg.BlockingProfile != 0 {\n\t\tif cfg.Pprof.BlockingProfile != 0 {\n\t\t\treturn nil, mkErr(\"blockingprofile and \" +\n\t\t\t\t\"pprof.blockingprofile are mutually exclusive\")\n\t\t}\n\t\tcfg.Pprof.BlockingProfile = cfg.BlockingProfile\n\t}\n\tif cfg.MutexProfile != 0 {\n\t\tif cfg.Pprof.MutexProfile != 0 {\n\t\t\treturn nil, mkErr(\"mutexprofile and \" +\n\t\t\t\t\"pprof.mutexprofile are mutually exclusive\")\n\t\t}\n\t\tcfg.Pprof.MutexProfile = cfg.MutexProfile\n\t}\n\n\t// Don't allow both the old dust-threshold and the new\n\t// channel-max-fee-exposure to be set.\n\tif cfg.DustThreshold != 0 && cfg.MaxFeeExposure != 0 {\n\t\treturn nil, mkErr(\"cannot set both dust-threshold and \" +\n\t\t\t\"channel-max-fee-exposure\")\n\t}\n\n\tswitch {\n\t// Use the old dust-threshold as the max fee exposure if it is set and\n\t// the new option is not.\n\tcase cfg.DustThreshold != 0:\n\t\tcfg.MaxFeeExposure = cfg.DustThreshold\n\n\t// Use the default max fee exposure if the new option is not set and\n\t// the old one is not set either.\n\tcase cfg.MaxFeeExposure == 0:\n\t\tcfg.MaxFeeExposure = uint64(\n\t\t\thtlcswitch.DefaultMaxFeeExposure.ToSatoshis(),\n\t\t)\n\t}\n\n\t// Validate the subconfigs for workers, caches, and the tower client.\n\terr = lncfg.Validate(\n\t\tcfg.Workers,\n\t\tcfg.Caches,\n\t\tcfg.WtClient,\n\t\tcfg.DB,\n\t\tcfg.Cluster,\n\t\tcfg.HealthChecks,\n\t\tcfg.RPCMiddleware,\n\t\tcfg.RemoteSigner,\n\t\tcfg.Sweeper,\n\t\tcfg.Htlcswitch,\n\t\tcfg.Invoices,\n\t\tcfg.Routing,\n\t\tcfg.Pprof,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Finally, ensure that the user's color is correctly formatted,\n\t// otherwise the server will not be able to start after the unlocking\n\t// the wallet.\n\t_, err = lncfg.ParseHexColor(cfg.Color)\n\tif err != nil {\n\t\treturn nil, mkErr(\"unable to parse node color: %v\", err)\n\t}\n\n\t// All good, return the sanitized result.\n\treturn &cfg, nil\n}\n\n// graphDatabaseDir returns the default directory where the local bolt graph db\n// files are stored.\nfunc (c *Config) graphDatabaseDir() string {\n\treturn filepath.Join(\n\t\tc.DataDir, defaultGraphSubDirname,\n\t\tlncfg.NormalizeNetwork(c.ActiveNetParams.Name),\n\t)\n}\n\n// ImplementationConfig returns the configuration of what actual implementations\n// should be used when creating the main lnd instance.\nfunc (c *Config) ImplementationConfig(\n\tinterceptor signal.Interceptor) *ImplementationCfg {\n\n\t// If we're using a remote signer, we still need the base wallet as a\n\t// watch-only source of chain and address data. But we don't need any\n\t// private key material in that btcwallet base wallet.\n\tif c.RemoteSigner.Enable {\n\t\trpcImpl := NewRPCSignerWalletImpl(\n\t\t\tc, ltndLog, interceptor,\n\t\t\tc.RemoteSigner.MigrateWatchOnly,\n\t\t)\n\t\treturn &ImplementationCfg{\n\t\t\tGrpcRegistrar:     rpcImpl,\n\t\t\tRestRegistrar:     rpcImpl,\n\t\t\tExternalValidator: rpcImpl,\n\t\t\tDatabaseBuilder: NewDefaultDatabaseBuilder(\n\t\t\t\tc, ltndLog,\n\t\t\t),\n\t\t\tWalletConfigBuilder: rpcImpl,\n\t\t\tChainControlBuilder: rpcImpl,\n\t\t}\n\t}\n\n\tdefaultImpl := NewDefaultWalletImpl(c, ltndLog, interceptor, false)\n\treturn &ImplementationCfg{\n\t\tGrpcRegistrar:       defaultImpl,\n\t\tRestRegistrar:       defaultImpl,\n\t\tExternalValidator:   defaultImpl,\n\t\tDatabaseBuilder:     NewDefaultDatabaseBuilder(c, ltndLog),\n\t\tWalletConfigBuilder: defaultImpl,\n\t\tChainControlBuilder: defaultImpl,\n\t}\n}\n\n// CleanAndExpandPath expands environment variables and leading ~ in the\n// passed path, cleans the result, and returns it.\n// This function is taken from https://github.com/btcsuite/btcd\nfunc CleanAndExpandPath(path string) string {\n\tif path == \"\" {\n\t\treturn \"\"\n\t}\n\n\t// Expand initial ~ to OS specific home directory.\n\tif strings.HasPrefix(path, \"~\") {\n\t\tvar homeDir string\n\t\tu, err := user.Current()\n\t\tif err == nil {\n\t\t\thomeDir = u.HomeDir\n\t\t} else {\n\t\t\thomeDir = os.Getenv(\"HOME\")\n\t\t}\n\n\t\tpath = strings.Replace(path, \"~\", homeDir, 1)\n\t}\n\n\t// NOTE: The os.ExpandEnv doesn't work with Windows-style %VARIABLE%,\n\t// but the variables can still be expanded via POSIX-style $VARIABLE.\n\treturn filepath.Clean(os.ExpandEnv(path))\n}\n\nfunc parseRPCParams(cConfig *lncfg.Chain, nodeConfig interface{},\n\tnetParams chainreg.BitcoinNetParams) error {\n\n\t// First, we'll check our node config to make sure the RPC parameters\n\t// were set correctly. We'll also determine the path to the conf file\n\t// depending on the backend node.\n\tvar daemonName, confDir, confFile, confFileBase string\n\tswitch conf := nodeConfig.(type) {\n\tcase *lncfg.Btcd:\n\t\t// Resolves environment variable references in RPCUser and\n\t\t// RPCPass fields.\n\t\tconf.RPCUser = supplyEnvValue(conf.RPCUser)\n\t\tconf.RPCPass = supplyEnvValue(conf.RPCPass)\n\n\t\t// If both RPCUser and RPCPass are set, we assume those\n\t\t// credentials are good to use.\n\t\tif conf.RPCUser != \"\" && conf.RPCPass != \"\" {\n\t\t\treturn nil\n\t\t}\n\n\t\t// Set the daemon name for displaying proper errors.\n\t\tdaemonName = btcdBackendName\n\t\tconfDir = conf.Dir\n\t\tconfFileBase = btcdBackendName\n\n\t\t// If only ONE of RPCUser or RPCPass is set, we assume the\n\t\t// user did that unintentionally.\n\t\tif conf.RPCUser != \"\" || conf.RPCPass != \"\" {\n\t\t\treturn fmt.Errorf(\"please set both or neither of \"+\n\t\t\t\t\"%[1]v.rpcuser, %[1]v.rpcpass\", daemonName)\n\t\t}\n\n\tcase *lncfg.Bitcoind:\n\t\t// Ensure that if the ZMQ options are set, that they are not\n\t\t// equal.\n\t\tif conf.ZMQPubRawBlock != \"\" && conf.ZMQPubRawTx != \"\" {\n\t\t\terr := checkZMQOptions(\n\t\t\t\tconf.ZMQPubRawBlock, conf.ZMQPubRawTx,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\t// Ensure that if the estimate mode is set, that it is a legal\n\t\t// value.\n\t\tif conf.EstimateMode != \"\" {\n\t\t\terr := checkEstimateMode(conf.EstimateMode)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\t// Set the daemon name for displaying proper errors.\n\t\tdaemonName = bitcoindBackendName\n\t\tconfDir = conf.Dir\n\t\tconfFile = conf.ConfigPath\n\t\tconfFileBase = BitcoinChainName\n\n\t\t// Resolves environment variable references in RPCUser\n\t\t// and RPCPass fields.\n\t\tconf.RPCUser = supplyEnvValue(conf.RPCUser)\n\t\tconf.RPCPass = supplyEnvValue(conf.RPCPass)\n\n\t\t// Check that cookie and credentials don't contradict each\n\t\t// other.\n\t\tif (conf.RPCUser != \"\" || conf.RPCPass != \"\") &&\n\t\t\tconf.RPCCookie != \"\" {\n\n\t\t\treturn fmt.Errorf(\"please only provide either \"+\n\t\t\t\t\"%[1]v.rpccookie or %[1]v.rpcuser and \"+\n\t\t\t\t\"%[1]v.rpcpass\", daemonName)\n\t\t}\n\n\t\t// We convert the cookie into a user name and password.\n\t\tif conf.RPCCookie != \"\" {\n\t\t\tcookie, err := os.ReadFile(conf.RPCCookie)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"cannot read cookie file: %w\",\n\t\t\t\t\terr)\n\t\t\t}\n\n\t\t\tsplitCookie := strings.Split(string(cookie), \":\")\n\t\t\tif len(splitCookie) != 2 {\n\t\t\t\treturn fmt.Errorf(\"cookie file has a wrong \" +\n\t\t\t\t\t\"format\")\n\t\t\t}\n\t\t\tconf.RPCUser = splitCookie[0]\n\t\t\tconf.RPCPass = splitCookie[1]\n\t\t}\n\n\t\tif conf.RPCUser != \"\" && conf.RPCPass != \"\" {\n\t\t\t// If all of RPCUser, RPCPass, ZMQBlockHost, and\n\t\t\t// ZMQTxHost are set, we assume those parameters are\n\t\t\t// good to use.\n\t\t\tif conf.ZMQPubRawBlock != \"\" && conf.ZMQPubRawTx != \"\" {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\t// If RPCUser and RPCPass are set and RPCPolling is\n\t\t\t// enabled, we assume the parameters are good to use.\n\t\t\tif conf.RPCPolling {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\n\t\t// If not all of the parameters are set, we'll assume the user\n\t\t// did this unintentionally.\n\t\tif conf.RPCUser != \"\" || conf.RPCPass != \"\" ||\n\t\t\tconf.ZMQPubRawBlock != \"\" || conf.ZMQPubRawTx != \"\" {\n\n\t\t\treturn fmt.Errorf(\"please set %[1]v.rpcuser and \"+\n\t\t\t\t\"%[1]v.rpcpass (or %[1]v.rpccookie) together \"+\n\t\t\t\t\"with %[1]v.zmqpubrawblock, %[1]v.zmqpubrawtx\",\n\t\t\t\tdaemonName)\n\t\t}\n\t}\n\n\t// If we're in simnet mode, then the running btcd instance won't read\n\t// the RPC credentials from the configuration. So if lnd wasn't\n\t// specified the parameters, then we won't be able to start.\n\tif cConfig.SimNet {\n\t\treturn fmt.Errorf(\"rpcuser and rpcpass must be set to your \" +\n\t\t\t\"btcd node's RPC parameters for simnet mode\")\n\t}\n\n\tfmt.Println(\"Attempting automatic RPC configuration to \" + daemonName)\n\n\tif confFile == \"\" {\n\t\tconfFile = filepath.Join(confDir, fmt.Sprintf(\"%v.conf\",\n\t\t\tconfFileBase))\n\t}\n\tswitch cConfig.Node {\n\tcase btcdBackendName:\n\t\tnConf := nodeConfig.(*lncfg.Btcd)\n\t\trpcUser, rpcPass, err := extractBtcdRPCParams(confFile)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to extract RPC credentials: \"+\n\t\t\t\t\"%v, cannot start w/o RPC connection\", err)\n\t\t}\n\t\tnConf.RPCUser, nConf.RPCPass = rpcUser, rpcPass\n\n\tcase bitcoindBackendName:\n\t\tnConf := nodeConfig.(*lncfg.Bitcoind)\n\t\trpcUser, rpcPass, zmqBlockHost, zmqTxHost, err :=\n\t\t\textractBitcoindRPCParams(netParams.Params.Name,\n\t\t\t\tnConf.Dir, confFile, nConf.RPCCookie)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to extract RPC credentials: \"+\n\t\t\t\t\"%v, cannot start w/o RPC connection\", err)\n\t\t}\n\t\tnConf.RPCUser, nConf.RPCPass = rpcUser, rpcPass\n\t\tnConf.ZMQPubRawBlock, nConf.ZMQPubRawTx = zmqBlockHost, zmqTxHost\n\t}\n\n\tfmt.Printf(\"Automatically obtained %v's RPC credentials\\n\", daemonName)\n\treturn nil\n}\n\n// supplyEnvValue supplies the value of an environment variable from a string.\n// It supports the following formats:\n// 1) $ENV_VAR\n// 2) ${ENV_VAR}\n// 3) ${ENV_VAR:-DEFAULT}\n//\n// Standard environment variable naming conventions:\n// - ENV_VAR contains letters, digits, and underscores, and does\n// not start with a digit.\n// - DEFAULT follows the rule that it can contain any characters except\n// whitespace.\n//\n// Parameters:\n// - value: The input string containing references to environment variables\n// (if any).\n//\n// Returns:\n// - string: The value of the specified environment variable, the default\n// value if provided, or the original input string if no matching variable is\n// found or set.\nfunc supplyEnvValue(value string) string {\n\t// Regex for $ENV_VAR format.\n\tvar reEnvVar = regexp.MustCompile(`^\\$([a-zA-Z_][a-zA-Z0-9_]*)$`)\n\n\t// Regex for ${ENV_VAR} format.\n\tvar reEnvVarWithBrackets = regexp.MustCompile(\n\t\t`^\\$\\{([a-zA-Z_][a-zA-Z0-9_]*)\\}$`,\n\t)\n\n\t// Regex for ${ENV_VAR:-DEFAULT} format.\n\tvar reEnvVarWithDefault = regexp.MustCompile(\n\t\t`^\\$\\{([a-zA-Z_][a-zA-Z0-9_]*):-([\\S]+)\\}$`,\n\t)\n\n\t// Match against supported formats.\n\tswitch {\n\tcase reEnvVarWithDefault.MatchString(value):\n\t\tmatches := reEnvVarWithDefault.FindStringSubmatch(value)\n\t\tenvVariable := matches[1]\n\t\tdefaultValue := matches[2]\n\t\tif envValue := os.Getenv(envVariable); envValue != \"\" {\n\t\t\treturn envValue\n\t\t}\n\n\t\treturn defaultValue\n\n\tcase reEnvVarWithBrackets.MatchString(value):\n\t\tmatches := reEnvVarWithBrackets.FindStringSubmatch(value)\n\t\tenvVariable := matches[1]\n\t\tenvValue := os.Getenv(envVariable)\n\n\t\treturn envValue\n\n\tcase reEnvVar.MatchString(value):\n\t\tmatches := reEnvVar.FindStringSubmatch(value)\n\t\tenvVariable := matches[1]\n\t\tenvValue := os.Getenv(envVariable)\n\n\t\treturn envValue\n\t}\n\n\treturn value\n}\n\n// extractBtcdRPCParams attempts to extract the RPC credentials for an existing\n// btcd instance. The passed path is expected to be the location of btcd's\n// application data directory on the target system.\nfunc extractBtcdRPCParams(btcdConfigPath string) (string, string, error) {\n\t// First, we'll open up the btcd configuration file found at the target\n\t// destination.\n\tbtcdConfigFile, err := os.Open(btcdConfigPath)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\tdefer func() { _ = btcdConfigFile.Close() }()\n\n\t// With the file open extract the contents of the configuration file so\n\t// we can attempt to locate the RPC credentials.\n\tconfigContents, err := io.ReadAll(btcdConfigFile)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\n\t// Attempt to locate the RPC user using a regular expression. If we\n\t// don't have a match for our regular expression then we'll exit with\n\t// an error.\n\trpcUserRegexp, err := regexp.Compile(`(?m)^\\s*rpcuser\\s*=\\s*([^\\s]+)`)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\tuserSubmatches := rpcUserRegexp.FindSubmatch(configContents)\n\tif userSubmatches == nil {\n\t\treturn \"\", \"\", fmt.Errorf(\"unable to find rpcuser in config\")\n\t}\n\n\t// Similarly, we'll use another regular expression to find the set\n\t// rpcpass (if any). If we can't find the pass, then we'll exit with an\n\t// error.\n\trpcPassRegexp, err := regexp.Compile(`(?m)^\\s*rpcpass\\s*=\\s*([^\\s]+)`)\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\tpassSubmatches := rpcPassRegexp.FindSubmatch(configContents)\n\tif passSubmatches == nil {\n\t\treturn \"\", \"\", fmt.Errorf(\"unable to find rpcuser in config\")\n\t}\n\n\treturn supplyEnvValue(string(userSubmatches[1])),\n\t\tsupplyEnvValue(string(passSubmatches[1])), nil\n}\n\n// extractBitcoindRPCParams attempts to extract the RPC credentials for an\n// existing bitcoind node instance. The routine looks for a cookie first,\n// optionally following the datadir configuration option in the bitcoin.conf. If\n// it doesn't find one, it looks for rpcuser/rpcpassword.\nfunc extractBitcoindRPCParams(networkName, bitcoindDataDir, bitcoindConfigPath,\n\trpcCookiePath string) (string, string, string, string, error) {\n\n\t// First, we'll open up the bitcoind configuration file found at the\n\t// target destination.\n\tbitcoindConfigFile, err := os.Open(bitcoindConfigPath)\n\tif err != nil {\n\t\treturn \"\", \"\", \"\", \"\", err\n\t}\n\tdefer func() { _ = bitcoindConfigFile.Close() }()\n\n\t// With the file open extract the contents of the configuration file so\n\t// we can attempt to locate the RPC credentials.\n\tconfigContents, err := io.ReadAll(bitcoindConfigFile)\n\tif err != nil {\n\t\treturn \"\", \"\", \"\", \"\", err\n\t}\n\n\t// First, we'll look for the ZMQ hosts providing raw block and raw\n\t// transaction notifications.\n\tzmqBlockHostRE, err := regexp.Compile(\n\t\t`(?m)^\\s*zmqpubrawblock\\s*=\\s*([^\\s]+)`,\n\t)\n\tif err != nil {\n\t\treturn \"\", \"\", \"\", \"\", err\n\t}\n\tzmqBlockHostSubmatches := zmqBlockHostRE.FindSubmatch(configContents)\n\tif len(zmqBlockHostSubmatches) < 2 {\n\t\treturn \"\", \"\", \"\", \"\", fmt.Errorf(\"unable to find \" +\n\t\t\t\"zmqpubrawblock in config\")\n\t}\n\tzmqTxHostRE, err := regexp.Compile(`(?m)^\\s*zmqpubrawtx\\s*=\\s*([^\\s]+)`)\n\tif err != nil {\n\t\treturn \"\", \"\", \"\", \"\", err\n\t}\n\tzmqTxHostSubmatches := zmqTxHostRE.FindSubmatch(configContents)\n\tif len(zmqTxHostSubmatches) < 2 {\n\t\treturn \"\", \"\", \"\", \"\", errors.New(\"unable to find zmqpubrawtx \" +\n\t\t\t\"in config\")\n\t}\n\tzmqBlockHost := string(zmqBlockHostSubmatches[1])\n\tzmqTxHost := string(zmqTxHostSubmatches[1])\n\tif err := checkZMQOptions(zmqBlockHost, zmqTxHost); err != nil {\n\t\treturn \"\", \"\", \"\", \"\", err\n\t}\n\n\t// Next, we'll try to find an auth cookie. We need to detect the chain\n\t// by seeing if one is specified in the configuration file.\n\tdataDir := filepath.Dir(bitcoindConfigPath)\n\tif bitcoindDataDir != \"\" {\n\t\tdataDir = bitcoindDataDir\n\t}\n\tdataDirRE, err := regexp.Compile(`(?m)^\\s*datadir\\s*=\\s*([^\\s]+)`)\n\tif err != nil {\n\t\treturn \"\", \"\", \"\", \"\", err\n\t}\n\tdataDirSubmatches := dataDirRE.FindSubmatch(configContents)\n\tif dataDirSubmatches != nil {\n\t\tdataDir = string(dataDirSubmatches[1])\n\t}\n\n\tvar chainDir string\n\tswitch networkName {\n\tcase \"mainnet\":\n\t\tchainDir = \"\"\n\tcase \"regtest\", \"testnet3\", \"signet\":\n\t\tchainDir = networkName\n\tdefault:\n\t\treturn \"\", \"\", \"\", \"\", fmt.Errorf(\"unexpected networkname %v\", networkName)\n\t}\n\n\tcookiePath := filepath.Join(dataDir, chainDir, \".cookie\")\n\tif rpcCookiePath != \"\" {\n\t\tcookiePath = rpcCookiePath\n\t}\n\tcookie, err := os.ReadFile(cookiePath)\n\tif err == nil {\n\t\tsplitCookie := strings.Split(string(cookie), \":\")\n\t\tif len(splitCookie) == 2 {\n\t\t\treturn splitCookie[0], splitCookie[1], zmqBlockHost,\n\t\t\t\tzmqTxHost, nil\n\t\t}\n\t}\n\n\t// We didn't find a cookie, so we attempt to locate the RPC user using\n\t// a regular expression. If we  don't have a match for our regular\n\t// expression then we'll exit with an error.\n\trpcUserRegexp, err := regexp.Compile(`(?m)^\\s*rpcuser\\s*=\\s*([^\\s]+)`)\n\tif err != nil {\n\t\treturn \"\", \"\", \"\", \"\", err\n\t}\n\tuserSubmatches := rpcUserRegexp.FindSubmatch(configContents)\n\n\t// Similarly, we'll use another regular expression to find the set\n\t// rpcpass (if any). If we can't find the pass, then we'll exit with an\n\t// error.\n\trpcPassRegexp, err := regexp.Compile(`(?m)^\\s*rpcpassword\\s*=\\s*([^\\s]+)`)\n\tif err != nil {\n\t\treturn \"\", \"\", \"\", \"\", err\n\t}\n\tpassSubmatches := rpcPassRegexp.FindSubmatch(configContents)\n\n\t// Exit with an error if the cookie file, is defined in config, and\n\t// can not be found, with both rpcuser and rpcpassword undefined.\n\tif rpcCookiePath != \"\" && userSubmatches == nil && passSubmatches == nil {\n\t\treturn \"\", \"\", \"\", \"\", fmt.Errorf(\"unable to open cookie file (%v)\",\n\t\t\trpcCookiePath)\n\t}\n\n\tif userSubmatches == nil {\n\t\treturn \"\", \"\", \"\", \"\", fmt.Errorf(\"unable to find rpcuser in \" +\n\t\t\t\"config\")\n\t}\n\tif passSubmatches == nil {\n\t\treturn \"\", \"\", \"\", \"\", fmt.Errorf(\"unable to find rpcpassword \" +\n\t\t\t\"in config\")\n\t}\n\n\treturn supplyEnvValue(string(userSubmatches[1])),\n\t\tsupplyEnvValue(string(passSubmatches[1])),\n\t\tzmqBlockHost, zmqTxHost, nil\n}\n\n// checkZMQOptions ensures that the provided addresses to use as the hosts for\n// ZMQ rawblock and rawtx notifications are different.\nfunc checkZMQOptions(zmqBlockHost, zmqTxHost string) error {\n\tif zmqBlockHost == zmqTxHost {\n\t\treturn errors.New(\"zmqpubrawblock and zmqpubrawtx must be set \" +\n\t\t\t\"to different addresses\")\n\t}\n\n\treturn nil\n}\n\n// checkEstimateMode ensures that the provided estimate mode is legal.\nfunc checkEstimateMode(estimateMode string) error {\n\tfor _, mode := range bitcoindEstimateModes {\n\t\tif estimateMode == mode {\n\t\t\treturn nil\n\t\t}\n\t}\n\n\treturn fmt.Errorf(\"estimatemode must be one of the following: %v\",\n\t\tbitcoindEstimateModes[:])\n}\n\n// configToFlatMap converts the given config struct into a flat map of\n// key/value pairs using the dot notation we are used to from the config file\n// or command line flags. It also returns a map containing deprecated config\n// options.\nfunc configToFlatMap(cfg Config) (map[string]string,\n\tmap[string]struct{}, error) {\n\n\tresult := make(map[string]string)\n\n\t// deprecated stores a map of deprecated options found in the config\n\t// that are set by the users. A config option is considered as\n\t// deprecated if it has a `hidden` flag.\n\tdeprecated := make(map[string]struct{})\n\n\t// redact is the helper function that redacts sensitive values like\n\t// passwords.\n\tredact := func(key, value string) string {\n\t\tsensitiveKeySuffixes := []string{\n\t\t\t\"pass\",\n\t\t\t\"password\",\n\t\t\t\"dsn\",\n\t\t}\n\t\tfor _, suffix := range sensitiveKeySuffixes {\n\t\t\tif strings.HasSuffix(key, suffix) {\n\t\t\t\treturn \"[redacted]\"\n\t\t\t}\n\t\t}\n\n\t\treturn value\n\t}\n\n\t// printConfig is the helper function that goes into nested structs\n\t// recursively. Because we call it recursively, we need to declare it\n\t// before we define it.\n\tvar printConfig func(reflect.Value, string)\n\tprintConfig = func(obj reflect.Value, prefix string) {\n\t\t// Turn struct pointers into the actual struct, so we can\n\t\t// iterate over the fields as we would with a struct value.\n\t\tif obj.Kind() == reflect.Ptr {\n\t\t\tobj = obj.Elem()\n\t\t}\n\n\t\t// Abort on nil values.\n\t\tif !obj.IsValid() {\n\t\t\treturn\n\t\t}\n\n\t\t// Loop over all fields of the struct and inspect the type.\n\t\tfor i := 0; i < obj.NumField(); i++ {\n\t\t\tfield := obj.Field(i)\n\t\t\tfieldType := obj.Type().Field(i)\n\n\t\t\tlongName := fieldType.Tag.Get(\"long\")\n\t\t\tnamespace := fieldType.Tag.Get(\"namespace\")\n\t\t\tgroup := fieldType.Tag.Get(\"group\")\n\t\t\thidden := fieldType.Tag.Get(\"hidden\")\n\n\t\t\tswitch {\n\t\t\t// We have a long name defined, this is a config value.\n\t\t\tcase longName != \"\":\n\t\t\t\tkey := longName\n\t\t\t\tif prefix != \"\" {\n\t\t\t\t\tkey = prefix + \".\" + key\n\t\t\t\t}\n\n\t\t\t\t// Add the value directly to the flattened map.\n\t\t\t\tresult[key] = redact(key, fmt.Sprintf(\n\t\t\t\t\t\"%v\", field.Interface(),\n\t\t\t\t))\n\n\t\t\t\t// If there's a hidden flag, it's deprecated.\n\t\t\t\tif hidden == \"true\" && !field.IsZero() {\n\t\t\t\t\tdeprecated[key] = struct{}{}\n\t\t\t\t}\n\n\t\t\t// We have no long name but a namespace, this is a\n\t\t\t// nested struct.\n\t\t\tcase longName == \"\" && namespace != \"\":\n\t\t\t\tkey := namespace\n\t\t\t\tif prefix != \"\" {\n\t\t\t\t\tkey = prefix + \".\" + key\n\t\t\t\t}\n\n\t\t\t\tprintConfig(field, key)\n\n\t\t\t// Just a group means this is a dummy struct to house\n\t\t\t// multiple config values, the group name doesn't go\n\t\t\t// into the final field name.\n\t\t\tcase longName == \"\" && group != \"\":\n\t\t\t\tprintConfig(field, prefix)\n\n\t\t\t// Anonymous means embedded struct. We need to recurse\n\t\t\t// into it but without adding anything to the prefix.\n\t\t\tcase fieldType.Anonymous:\n\t\t\t\tprintConfig(field, prefix)\n\n\t\t\tdefault:\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t}\n\n\t// Turn the whole config struct into a flat map.\n\tprintConfig(reflect.ValueOf(cfg), \"\")\n\n\treturn result, deprecated, nil\n}\n\n// logWarningsForDeprecation logs a warning if a deprecated config option is\n// set.\nfunc logWarningsForDeprecation(cfg Config) {\n\t_, deprecated, err := configToFlatMap(cfg)\n\tif err != nil {\n\t\tltndLog.Errorf(\"Convert configs to map: %v\", err)\n\t}\n\n\tfor k := range deprecated {\n\t\tltndLog.Warnf(\"Config '%s' is deprecated, please remove it\", k)\n\t}\n}\n"
        },
        {
          "name": "config_builder.go",
          "type": "blob",
          "size": 50.119140625,
          "content": "package lnd\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"database/sql\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/btcsuite/btcd/chaincfg\"\n\t\"github.com/btcsuite/btcd/chaincfg/chainhash\"\n\t\"github.com/btcsuite/btcd/wire\"\n\t\"github.com/btcsuite/btclog/v2\"\n\t\"github.com/btcsuite/btcwallet/chain\"\n\t\"github.com/btcsuite/btcwallet/waddrmgr\"\n\t\"github.com/btcsuite/btcwallet/wallet\"\n\t\"github.com/btcsuite/btcwallet/walletdb\"\n\tproxy \"github.com/grpc-ecosystem/grpc-gateway/v2/runtime\"\n\t\"github.com/lightninglabs/neutrino\"\n\t\"github.com/lightninglabs/neutrino/blockntfns\"\n\t\"github.com/lightninglabs/neutrino/headerfs\"\n\t\"github.com/lightninglabs/neutrino/pushtx\"\n\t\"github.com/lightningnetwork/lnd/blockcache\"\n\t\"github.com/lightningnetwork/lnd/chainntnfs\"\n\t\"github.com/lightningnetwork/lnd/chainreg\"\n\t\"github.com/lightningnetwork/lnd/channeldb\"\n\t\"github.com/lightningnetwork/lnd/clock\"\n\t\"github.com/lightningnetwork/lnd/fn/v2\"\n\t\"github.com/lightningnetwork/lnd/funding\"\n\tgraphdb \"github.com/lightningnetwork/lnd/graph/db\"\n\t\"github.com/lightningnetwork/lnd/htlcswitch\"\n\t\"github.com/lightningnetwork/lnd/invoices\"\n\t\"github.com/lightningnetwork/lnd/keychain\"\n\t\"github.com/lightningnetwork/lnd/kvdb\"\n\t\"github.com/lightningnetwork/lnd/lncfg\"\n\t\"github.com/lightningnetwork/lnd/lnrpc\"\n\t\"github.com/lightningnetwork/lnd/lnwallet\"\n\t\"github.com/lightningnetwork/lnd/lnwallet/btcwallet\"\n\t\"github.com/lightningnetwork/lnd/lnwallet/chancloser\"\n\t\"github.com/lightningnetwork/lnd/lnwallet/rpcwallet\"\n\t\"github.com/lightningnetwork/lnd/macaroons\"\n\t\"github.com/lightningnetwork/lnd/msgmux\"\n\t\"github.com/lightningnetwork/lnd/rpcperms\"\n\t\"github.com/lightningnetwork/lnd/signal\"\n\t\"github.com/lightningnetwork/lnd/sqldb\"\n\t\"github.com/lightningnetwork/lnd/sweep\"\n\t\"github.com/lightningnetwork/lnd/walletunlocker\"\n\t\"github.com/lightningnetwork/lnd/watchtower\"\n\t\"github.com/lightningnetwork/lnd/watchtower/wtclient\"\n\t\"github.com/lightningnetwork/lnd/watchtower/wtdb\"\n\t\"google.golang.org/grpc\"\n\t\"gopkg.in/macaroon-bakery.v2/bakery\"\n)\n\n// GrpcRegistrar is an interface that must be satisfied by an external subserver\n// that wants to be able to register its own gRPC server onto lnd's main\n// grpc.Server instance.\ntype GrpcRegistrar interface {\n\t// RegisterGrpcSubserver is called for each net.Listener on which lnd\n\t// creates a grpc.Server instance. External subservers implementing this\n\t// method can then register their own gRPC server structs to the main\n\t// server instance.\n\tRegisterGrpcSubserver(*grpc.Server) error\n}\n\n// RestRegistrar is an interface that must be satisfied by an external subserver\n// that wants to be able to register its own REST mux onto lnd's main\n// proxy.ServeMux instance.\ntype RestRegistrar interface {\n\t// RegisterRestSubserver is called after lnd creates the main\n\t// proxy.ServeMux instance. External subservers implementing this method\n\t// can then register their own REST proxy stubs to the main server\n\t// instance.\n\tRegisterRestSubserver(context.Context, *proxy.ServeMux, string,\n\t\t[]grpc.DialOption) error\n}\n\n// ExternalValidator is an interface that must be satisfied by an external\n// macaroon validator.\ntype ExternalValidator interface {\n\tmacaroons.MacaroonValidator\n\n\t// Permissions returns the permissions that the external validator is\n\t// validating. It is a map between the full HTTP URI of each RPC and its\n\t// required macaroon permissions. If multiple action/entity tuples are\n\t// specified per URI, they are all required. See rpcserver.go for a list\n\t// of valid action and entity values.\n\tPermissions() map[string][]bakery.Op\n}\n\n// DatabaseBuilder is an interface that must be satisfied by the implementation\n// that provides lnd's main database backend instances.\ntype DatabaseBuilder interface {\n\t// BuildDatabase extracts the current databases that we'll use for\n\t// normal operation in the daemon. A function closure that closes all\n\t// opened databases is also returned.\n\tBuildDatabase(ctx context.Context) (*DatabaseInstances, func(), error)\n}\n\n// WalletConfigBuilder is an interface that must be satisfied by a custom wallet\n// implementation.\ntype WalletConfigBuilder interface {\n\t// BuildWalletConfig is responsible for creating or unlocking and then\n\t// fully initializing a wallet.\n\tBuildWalletConfig(context.Context, *DatabaseInstances, *AuxComponents,\n\t\t*rpcperms.InterceptorChain,\n\t\t[]*ListenerWithSignal) (*chainreg.PartialChainControl,\n\t\t*btcwallet.Config, func(), error)\n}\n\n// ChainControlBuilder is an interface that must be satisfied by a custom wallet\n// implementation.\ntype ChainControlBuilder interface {\n\t// BuildChainControl is responsible for creating a fully populated chain\n\t// control instance from a wallet.\n\tBuildChainControl(*chainreg.PartialChainControl,\n\t\t*btcwallet.Config) (*chainreg.ChainControl, func(), error)\n}\n\n// ImplementationCfg is a struct that holds all configuration items for\n// components that can be implemented outside lnd itself.\ntype ImplementationCfg struct {\n\t// GrpcRegistrar is a type that can register additional gRPC subservers\n\t// before the main gRPC server is started.\n\tGrpcRegistrar\n\n\t// RestRegistrar is a type that can register additional REST subservers\n\t// before the main REST proxy is started.\n\tRestRegistrar\n\n\t// ExternalValidator is a type that can provide external macaroon\n\t// validation.\n\tExternalValidator\n\n\t// DatabaseBuilder is a type that can provide lnd's main database\n\t// backend instances.\n\tDatabaseBuilder\n\n\t// WalletConfigBuilder is a type that can provide a wallet configuration\n\t// with a fully loaded and unlocked wallet.\n\tWalletConfigBuilder\n\n\t// ChainControlBuilder is a type that can provide a custom wallet\n\t// implementation.\n\tChainControlBuilder\n\n\t// AuxComponents is a set of auxiliary components that can be used by\n\t// lnd for certain custom channel types.\n\tAuxComponents\n}\n\n// AuxComponents is a set of auxiliary components that can be used by lnd for\n// certain custom channel types.\ntype AuxComponents struct {\n\t// AuxLeafStore is an optional data source that can be used by custom\n\t// channels to fetch+store various data.\n\tAuxLeafStore fn.Option[lnwallet.AuxLeafStore]\n\n\t// TrafficShaper is an optional traffic shaper that can be used to\n\t// control the outgoing channel of a payment.\n\tTrafficShaper fn.Option[htlcswitch.AuxTrafficShaper]\n\n\t// MsgRouter is an optional message router that if set will be used in\n\t// place of a new blank default message router.\n\tMsgRouter fn.Option[msgmux.Router]\n\n\t// AuxFundingController is an optional controller that can be used to\n\t// modify the way we handle certain custom channel types. It's also\n\t// able to automatically handle new custom protocol messages related to\n\t// the funding process.\n\tAuxFundingController fn.Option[funding.AuxFundingController]\n\n\t// AuxSigner is an optional signer that can be used to sign auxiliary\n\t// leaves for certain custom channel types.\n\tAuxSigner fn.Option[lnwallet.AuxSigner]\n\n\t// AuxDataParser is an optional data parser that can be used to parse\n\t// auxiliary data for certain custom channel types.\n\tAuxDataParser fn.Option[AuxDataParser]\n\n\t// AuxChanCloser is an optional channel closer that can be used to\n\t// modify the way a coop-close transaction is constructed.\n\tAuxChanCloser fn.Option[chancloser.AuxChanCloser]\n\n\t// AuxSweeper is an optional interface that can be used to modify the\n\t// way sweep transaction are generated.\n\tAuxSweeper fn.Option[sweep.AuxSweeper]\n\n\t// AuxContractResolver is an optional interface that can be used to\n\t// modify the way contracts are resolved.\n\tAuxContractResolver fn.Option[lnwallet.AuxContractResolver]\n}\n\n// DefaultWalletImpl is the default implementation of our normal, btcwallet\n// backed configuration.\ntype DefaultWalletImpl struct {\n\tcfg         *Config\n\tlogger      btclog.Logger\n\tinterceptor signal.Interceptor\n\n\twatchOnly        bool\n\tmigrateWatchOnly bool\n\tpwService        *walletunlocker.UnlockerService\n}\n\n// NewDefaultWalletImpl creates a new default wallet implementation.\nfunc NewDefaultWalletImpl(cfg *Config, logger btclog.Logger,\n\tinterceptor signal.Interceptor, watchOnly bool) *DefaultWalletImpl {\n\n\treturn &DefaultWalletImpl{\n\t\tcfg:         cfg,\n\t\tlogger:      logger,\n\t\tinterceptor: interceptor,\n\t\twatchOnly:   watchOnly,\n\t\tpwService:   createWalletUnlockerService(cfg),\n\t}\n}\n\n// RegisterRestSubserver is called after lnd creates the main proxy.ServeMux\n// instance. External subservers implementing this method can then register\n// their own REST proxy stubs to the main server instance.\n//\n// NOTE: This is part of the GrpcRegistrar interface.\nfunc (d *DefaultWalletImpl) RegisterRestSubserver(ctx context.Context,\n\tmux *proxy.ServeMux, restProxyDest string,\n\trestDialOpts []grpc.DialOption) error {\n\n\treturn lnrpc.RegisterWalletUnlockerHandlerFromEndpoint(\n\t\tctx, mux, restProxyDest, restDialOpts,\n\t)\n}\n\n// RegisterGrpcSubserver is called for each net.Listener on which lnd creates a\n// grpc.Server instance. External subservers implementing this method can then\n// register their own gRPC server structs to the main server instance.\n//\n// NOTE: This is part of the GrpcRegistrar interface.\nfunc (d *DefaultWalletImpl) RegisterGrpcSubserver(s *grpc.Server) error {\n\tlnrpc.RegisterWalletUnlockerServer(s, d.pwService)\n\n\treturn nil\n}\n\n// ValidateMacaroon extracts the macaroon from the context's gRPC metadata,\n// checks its signature, makes sure all specified permissions for the called\n// method are contained within and finally ensures all caveat conditions are\n// met. A non-nil error is returned if any of the checks fail.\n//\n// NOTE: This is part of the ExternalValidator interface.\nfunc (d *DefaultWalletImpl) ValidateMacaroon(ctx context.Context,\n\trequiredPermissions []bakery.Op, fullMethod string) error {\n\n\t// Because the default implementation does not return any permissions,\n\t// we shouldn't be registered as an external validator at all and this\n\t// should never be invoked.\n\treturn fmt.Errorf(\"default implementation does not support external \" +\n\t\t\"macaroon validation\")\n}\n\n// Permissions returns the permissions that the external validator is\n// validating. It is a map between the full HTTP URI of each RPC and its\n// required macaroon permissions. If multiple action/entity tuples are specified\n// per URI, they are all required. See rpcserver.go for a list of valid action\n// and entity values.\n//\n// NOTE: This is part of the ExternalValidator interface.\nfunc (d *DefaultWalletImpl) Permissions() map[string][]bakery.Op {\n\treturn nil\n}\n\n// BuildWalletConfig is responsible for creating or unlocking and then\n// fully initializing a wallet.\n//\n// NOTE: This is part of the WalletConfigBuilder interface.\nfunc (d *DefaultWalletImpl) BuildWalletConfig(ctx context.Context,\n\tdbs *DatabaseInstances, aux *AuxComponents,\n\tinterceptorChain *rpcperms.InterceptorChain,\n\tgrpcListeners []*ListenerWithSignal) (*chainreg.PartialChainControl,\n\t*btcwallet.Config, func(), error) {\n\n\t// Keep track of our various cleanup functions. We use a defer function\n\t// as well to not repeat ourselves with every return statement.\n\tvar (\n\t\tcleanUpTasks []func()\n\t\tearlyExit    = true\n\t\tcleanUp      = func() {\n\t\t\tfor _, fn := range cleanUpTasks {\n\t\t\t\tif fn == nil {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tfn()\n\t\t\t}\n\t\t}\n\t)\n\tdefer func() {\n\t\tif earlyExit {\n\t\t\tcleanUp()\n\t\t}\n\t}()\n\n\t// Initialize a new block cache.\n\tblockCache := blockcache.NewBlockCache(d.cfg.BlockCacheSize)\n\n\t// Before starting the wallet, we'll create and start our Neutrino\n\t// light client instance, if enabled, in order to allow it to sync\n\t// while the rest of the daemon continues startup.\n\tmainChain := d.cfg.Bitcoin\n\tvar neutrinoCS *neutrino.ChainService\n\tif mainChain.Node == \"neutrino\" {\n\t\tneutrinoBackend, neutrinoCleanUp, err := initNeutrinoBackend(\n\t\t\tctx, d.cfg, mainChain.ChainDir, blockCache,\n\t\t)\n\t\tif err != nil {\n\t\t\terr := fmt.Errorf(\"unable to initialize neutrino \"+\n\t\t\t\t\"backend: %v\", err)\n\t\t\td.logger.Error(err)\n\t\t\treturn nil, nil, nil, err\n\t\t}\n\t\tcleanUpTasks = append(cleanUpTasks, neutrinoCleanUp)\n\t\tneutrinoCS = neutrinoBackend\n\t}\n\n\tvar (\n\t\twalletInitParams = walletunlocker.WalletUnlockParams{\n\t\t\t// In case we do auto-unlock, we need to be able to send\n\t\t\t// into the channel without blocking so we buffer it.\n\t\t\tMacResponseChan: make(chan []byte, 1),\n\t\t}\n\t\tprivateWalletPw = lnwallet.DefaultPrivatePassphrase\n\t\tpublicWalletPw  = lnwallet.DefaultPublicPassphrase\n\t)\n\n\t// If the user didn't request a seed, then we'll manually assume a\n\t// wallet birthday of now, as otherwise the seed would've specified\n\t// this information.\n\twalletInitParams.Birthday = time.Now()\n\n\td.pwService.SetLoaderOpts([]btcwallet.LoaderOption{dbs.WalletDB})\n\td.pwService.SetMacaroonDB(dbs.MacaroonDB)\n\twalletExists, err := d.pwService.WalletExists()\n\tif err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\n\tif !walletExists {\n\t\tinterceptorChain.SetWalletNotCreated()\n\t} else {\n\t\tinterceptorChain.SetWalletLocked()\n\t}\n\n\t// If we've started in auto unlock mode, then a wallet should already\n\t// exist because we don't want to enable the RPC unlocker in that case\n\t// for security reasons (an attacker could inject their seed since the\n\t// RPC is unauthenticated). Only if the user explicitly wants to allow\n\t// wallet creation we don't error out here.\n\tif d.cfg.WalletUnlockPasswordFile != \"\" && !walletExists &&\n\t\t!d.cfg.WalletUnlockAllowCreate {\n\n\t\treturn nil, nil, nil, fmt.Errorf(\"wallet unlock password file \" +\n\t\t\t\"was specified but wallet does not exist; initialize \" +\n\t\t\t\"the wallet before using auto unlocking\")\n\t}\n\n\t// What wallet mode are we running in? We've already made sure the no\n\t// seed backup and auto unlock aren't both set during config parsing.\n\tswitch {\n\t// No seed backup means we're also using the default password.\n\tcase d.cfg.NoSeedBackup:\n\t\t// We continue normally, the default password has already been\n\t\t// set above.\n\n\t// A password for unlocking is provided in a file.\n\tcase d.cfg.WalletUnlockPasswordFile != \"\" && walletExists:\n\t\td.logger.Infof(\"Attempting automatic wallet unlock with \" +\n\t\t\t\"password provided in file\")\n\t\tpwBytes, err := os.ReadFile(d.cfg.WalletUnlockPasswordFile)\n\t\tif err != nil {\n\t\t\treturn nil, nil, nil, fmt.Errorf(\"error reading \"+\n\t\t\t\t\"password from file %s: %v\",\n\t\t\t\td.cfg.WalletUnlockPasswordFile, err)\n\t\t}\n\n\t\t// Remove any newlines at the end of the file. The lndinit tool\n\t\t// won't ever write a newline but maybe the file was provisioned\n\t\t// by another process or user.\n\t\tpwBytes = bytes.TrimRight(pwBytes, \"\\r\\n\")\n\n\t\t// We have the password now, we can ask the unlocker service to\n\t\t// do the unlock for us.\n\t\tunlockedWallet, unloadWalletFn, err := d.pwService.LoadAndUnlock(\n\t\t\tpwBytes, 0,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, nil, nil, fmt.Errorf(\"error unlocking \"+\n\t\t\t\t\"wallet with password from file: %v\", err)\n\t\t}\n\n\t\tcleanUpTasks = append(cleanUpTasks, func() {\n\t\t\tif err := unloadWalletFn(); err != nil {\n\t\t\t\td.logger.Errorf(\"Could not unload wallet: %v\",\n\t\t\t\t\terr)\n\t\t\t}\n\t\t})\n\n\t\tprivateWalletPw = pwBytes\n\t\tpublicWalletPw = pwBytes\n\t\twalletInitParams.Wallet = unlockedWallet\n\t\twalletInitParams.UnloadWallet = unloadWalletFn\n\n\t// If none of the automatic startup options are selected, we fall back\n\t// to the default behavior of waiting for the wallet creation/unlocking\n\t// over RPC.\n\tdefault:\n\t\tif err := d.interceptor.Notifier.NotifyReady(false); err != nil {\n\t\t\treturn nil, nil, nil, err\n\t\t}\n\n\t\tparams, err := waitForWalletPassword(\n\t\t\td.cfg, d.pwService, []btcwallet.LoaderOption{dbs.WalletDB},\n\t\t\td.interceptor.ShutdownChannel(),\n\t\t)\n\t\tif err != nil {\n\t\t\terr := fmt.Errorf(\"unable to set up wallet password \"+\n\t\t\t\t\"listeners: %v\", err)\n\t\t\td.logger.Error(err)\n\t\t\treturn nil, nil, nil, err\n\t\t}\n\n\t\twalletInitParams = *params\n\t\tprivateWalletPw = walletInitParams.Password\n\t\tpublicWalletPw = walletInitParams.Password\n\t\tcleanUpTasks = append(cleanUpTasks, func() {\n\t\t\tif err := walletInitParams.UnloadWallet(); err != nil {\n\t\t\t\td.logger.Errorf(\"Could not unload wallet: %v\",\n\t\t\t\t\terr)\n\t\t\t}\n\t\t})\n\n\t\tif walletInitParams.RecoveryWindow > 0 {\n\t\t\td.logger.Infof(\"Wallet recovery mode enabled with \"+\n\t\t\t\t\"address lookahead of %d addresses\",\n\t\t\t\twalletInitParams.RecoveryWindow)\n\t\t}\n\t}\n\n\tvar macaroonService *macaroons.Service\n\tif !d.cfg.NoMacaroons {\n\t\t// Create the macaroon authentication/authorization service.\n\t\trootKeyStore, err := macaroons.NewRootKeyStorage(dbs.MacaroonDB)\n\t\tif err != nil {\n\t\t\treturn nil, nil, nil, err\n\t\t}\n\t\tmacaroonService, err = macaroons.NewService(\n\t\t\trootKeyStore, \"lnd\", walletInitParams.StatelessInit,\n\t\t\tmacaroons.IPLockChecker,\n\t\t\tmacaroons.CustomChecker(interceptorChain),\n\t\t)\n\t\tif err != nil {\n\t\t\terr := fmt.Errorf(\"unable to set up macaroon \"+\n\t\t\t\t\"authentication: %v\", err)\n\t\t\td.logger.Error(err)\n\t\t\treturn nil, nil, nil, err\n\t\t}\n\t\tcleanUpTasks = append(cleanUpTasks, func() {\n\t\t\tif err := macaroonService.Close(); err != nil {\n\t\t\t\td.logger.Errorf(\"Could not close macaroon \"+\n\t\t\t\t\t\"service: %v\", err)\n\t\t\t}\n\t\t})\n\n\t\t// Try to unlock the macaroon store with the private password.\n\t\t// Ignore ErrAlreadyUnlocked since it could be unlocked by the\n\t\t// wallet unlocker.\n\t\terr = macaroonService.CreateUnlock(&privateWalletPw)\n\t\tif err != nil && err != macaroons.ErrAlreadyUnlocked {\n\t\t\terr := fmt.Errorf(\"unable to unlock macaroons: %w\", err)\n\t\t\td.logger.Error(err)\n\t\t\treturn nil, nil, nil, err\n\t\t}\n\n\t\t// If we have a macaroon root key from the init wallet params,\n\t\t// set the root key before baking any macaroons.\n\t\tif len(walletInitParams.MacRootKey) > 0 {\n\t\t\terr := macaroonService.SetRootKey(\n\t\t\t\twalletInitParams.MacRootKey,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, nil, nil, err\n\t\t\t}\n\t\t}\n\n\t\t// Send an admin macaroon to all our listeners that requested\n\t\t// one by setting a non-nil macaroon channel.\n\t\tadminMacBytes, err := bakeMacaroon(\n\t\t\tctx, macaroonService, adminPermissions(),\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, nil, nil, err\n\t\t}\n\t\tfor _, lis := range grpcListeners {\n\t\t\tif lis.MacChan != nil {\n\t\t\t\tlis.MacChan <- adminMacBytes\n\t\t\t}\n\t\t}\n\n\t\t// In case we actually needed to unlock the wallet, we now need\n\t\t// to create an instance of the admin macaroon and send it to\n\t\t// the unlocker so it can forward it to the user. In no seed\n\t\t// backup mode, there's nobody listening on the channel and we'd\n\t\t// block here forever.\n\t\tif !d.cfg.NoSeedBackup {\n\t\t\t// The channel is buffered by one element so writing\n\t\t\t// should not block here.\n\t\t\twalletInitParams.MacResponseChan <- adminMacBytes\n\t\t}\n\n\t\t// If the user requested a stateless initialization, no macaroon\n\t\t// files should be created.\n\t\tif !walletInitParams.StatelessInit {\n\t\t\t// Create default macaroon files for lncli to use if\n\t\t\t// they don't exist.\n\t\t\terr = genDefaultMacaroons(\n\t\t\t\tctx, macaroonService, d.cfg.AdminMacPath,\n\t\t\t\td.cfg.ReadMacPath, d.cfg.InvoiceMacPath,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\terr := fmt.Errorf(\"unable to create macaroons \"+\n\t\t\t\t\t\"%v\", err)\n\t\t\t\td.logger.Error(err)\n\t\t\t\treturn nil, nil, nil, err\n\t\t\t}\n\t\t}\n\n\t\t// As a security service to the user, if they requested\n\t\t// stateless initialization and there are macaroon files on disk\n\t\t// we log a warning.\n\t\tif walletInitParams.StatelessInit {\n\t\t\tmsg := \"Found %s macaroon on disk (%s) even though \" +\n\t\t\t\t\"--stateless_init was requested. Unencrypted \" +\n\t\t\t\t\"state is accessible by the host system. You \" +\n\t\t\t\t\"should change the password and use \" +\n\t\t\t\t\"--new_mac_root_key with --stateless_init to \" +\n\t\t\t\t\"clean up and invalidate old macaroons.\"\n\n\t\t\tif lnrpc.FileExists(d.cfg.AdminMacPath) {\n\t\t\t\td.logger.Warnf(msg, \"admin\", d.cfg.AdminMacPath)\n\t\t\t}\n\t\t\tif lnrpc.FileExists(d.cfg.ReadMacPath) {\n\t\t\t\td.logger.Warnf(msg, \"readonly\", d.cfg.ReadMacPath)\n\t\t\t}\n\t\t\tif lnrpc.FileExists(d.cfg.InvoiceMacPath) {\n\t\t\t\td.logger.Warnf(msg, \"invoice\", d.cfg.InvoiceMacPath)\n\t\t\t}\n\t\t}\n\n\t\t// We add the macaroon service to our RPC interceptor. This\n\t\t// will start checking macaroons against permissions on every\n\t\t// RPC invocation.\n\t\tinterceptorChain.AddMacaroonService(macaroonService)\n\t}\n\n\t// Now that the wallet password has been provided, transition the RPC\n\t// state into Unlocked.\n\tinterceptorChain.SetWalletUnlocked()\n\n\t// Since calls to the WalletUnlocker service wait for a response on the\n\t// macaroon channel, we close it here to make sure they return in case\n\t// we did not return the admin macaroon above. This will be the case if\n\t// --no-macaroons is used.\n\tclose(walletInitParams.MacResponseChan)\n\n\t// We'll also close all the macaroon channels since lnd is done sending\n\t// macaroon data over it.\n\tfor _, lis := range grpcListeners {\n\t\tif lis.MacChan != nil {\n\t\t\tclose(lis.MacChan)\n\t\t}\n\t}\n\n\t// With the information parsed from the configuration, create valid\n\t// instances of the pertinent interfaces required to operate the\n\t// Lightning Network Daemon.\n\t//\n\t// When we create the chain control, we need storage for the height\n\t// hints and also the wallet itself, for these two we want them to be\n\t// replicated, so we'll pass in the remote channel DB instance.\n\tchainControlCfg := &chainreg.Config{\n\t\tBitcoin:                     d.cfg.Bitcoin,\n\t\tHeightHintCacheQueryDisable: d.cfg.HeightHintCacheQueryDisable,\n\t\tNeutrinoMode:                d.cfg.NeutrinoMode,\n\t\tBitcoindMode:                d.cfg.BitcoindMode,\n\t\tBtcdMode:                    d.cfg.BtcdMode,\n\t\tHeightHintDB:                dbs.HeightHintDB,\n\t\tChanStateDB:                 dbs.ChanStateDB.ChannelStateDB(),\n\t\tNeutrinoCS:                  neutrinoCS,\n\t\tAuxLeafStore:                aux.AuxLeafStore,\n\t\tAuxSigner:                   aux.AuxSigner,\n\t\tActiveNetParams:             d.cfg.ActiveNetParams,\n\t\tFeeURL:                      d.cfg.FeeURL,\n\t\tFee: &lncfg.Fee{\n\t\t\tURL:              d.cfg.Fee.URL,\n\t\t\tMinUpdateTimeout: d.cfg.Fee.MinUpdateTimeout,\n\t\t\tMaxUpdateTimeout: d.cfg.Fee.MaxUpdateTimeout,\n\t\t},\n\t\tDialer: func(addr string) (net.Conn, error) {\n\t\t\treturn d.cfg.net.Dial(\n\t\t\t\t\"tcp\", addr, d.cfg.ConnectionTimeout,\n\t\t\t)\n\t\t},\n\t\tBlockCache:         blockCache,\n\t\tWalletUnlockParams: &walletInitParams,\n\t}\n\n\t// Let's go ahead and create the partial chain control now that is only\n\t// dependent on our configuration and doesn't require any wallet\n\t// specific information.\n\tpartialChainControl, pccCleanup, err := chainreg.NewPartialChainControl(\n\t\tchainControlCfg,\n\t)\n\tcleanUpTasks = append(cleanUpTasks, pccCleanup)\n\tif err != nil {\n\t\terr := fmt.Errorf(\"unable to create partial chain control: %w\",\n\t\t\terr)\n\t\td.logger.Error(err)\n\t\treturn nil, nil, nil, err\n\t}\n\n\twalletConfig := &btcwallet.Config{\n\t\tPrivatePass:      privateWalletPw,\n\t\tPublicPass:       publicWalletPw,\n\t\tBirthday:         walletInitParams.Birthday,\n\t\tRecoveryWindow:   walletInitParams.RecoveryWindow,\n\t\tNetParams:        d.cfg.ActiveNetParams.Params,\n\t\tCoinType:         d.cfg.ActiveNetParams.CoinType,\n\t\tWallet:           walletInitParams.Wallet,\n\t\tLoaderOptions:    []btcwallet.LoaderOption{dbs.WalletDB},\n\t\tChainSource:      partialChainControl.ChainSource,\n\t\tWatchOnly:        d.watchOnly,\n\t\tMigrateWatchOnly: d.migrateWatchOnly,\n\t}\n\n\t// Parse coin selection strategy.\n\tswitch d.cfg.CoinSelectionStrategy {\n\tcase \"largest\":\n\t\twalletConfig.CoinSelectionStrategy = wallet.CoinSelectionLargest\n\n\tcase \"random\":\n\t\twalletConfig.CoinSelectionStrategy = wallet.CoinSelectionRandom\n\n\tdefault:\n\t\treturn nil, nil, nil, fmt.Errorf(\"unknown coin selection \"+\n\t\t\t\"strategy %v\", d.cfg.CoinSelectionStrategy)\n\t}\n\n\tearlyExit = false\n\treturn partialChainControl, walletConfig, cleanUp, nil\n}\n\n// proxyBlockEpoch proxies a block epoch subsections to the underlying neutrino\n// rebroadcaster client.\nfunc proxyBlockEpoch(\n\tnotifier chainntnfs.ChainNotifier) func() (*blockntfns.Subscription,\n\terror) {\n\n\treturn func() (*blockntfns.Subscription, error) {\n\t\tblockEpoch, err := notifier.RegisterBlockEpochNtfn(\n\t\t\tnil,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tsub := blockntfns.Subscription{\n\t\t\tNotifications: make(chan blockntfns.BlockNtfn, 6),\n\t\t\tCancel:        blockEpoch.Cancel,\n\t\t}\n\t\tgo func() {\n\t\t\tfor blk := range blockEpoch.Epochs {\n\t\t\t\tntfn := blockntfns.NewBlockConnected(\n\t\t\t\t\t*blk.BlockHeader,\n\t\t\t\t\tuint32(blk.Height),\n\t\t\t\t)\n\n\t\t\t\tsub.Notifications <- ntfn\n\t\t\t}\n\t\t}()\n\n\t\treturn &sub, nil\n\t}\n}\n\n// walletReBroadcaster is a simple wrapper around the pushtx.Broadcaster\n// interface to adhere to the expanded lnwallet.Rebroadcaster interface.\ntype walletReBroadcaster struct {\n\tstarted atomic.Bool\n\n\t*pushtx.Broadcaster\n}\n\n// newWalletReBroadcaster creates a new instance of the walletReBroadcaster.\nfunc newWalletReBroadcaster(\n\tbroadcaster *pushtx.Broadcaster) *walletReBroadcaster {\n\n\treturn &walletReBroadcaster{\n\t\tBroadcaster: broadcaster,\n\t}\n}\n\n// Start launches all goroutines the rebroadcaster needs to operate.\nfunc (w *walletReBroadcaster) Start() error {\n\tdefer w.started.Store(true)\n\n\treturn w.Broadcaster.Start()\n}\n\n// Started returns true if the broadcaster is already active.\nfunc (w *walletReBroadcaster) Started() bool {\n\treturn w.started.Load()\n}\n\n// BuildChainControl is responsible for creating a fully populated chain\n// control instance from a wallet.\n//\n// NOTE: This is part of the ChainControlBuilder interface.\nfunc (d *DefaultWalletImpl) BuildChainControl(\n\tpartialChainControl *chainreg.PartialChainControl,\n\twalletConfig *btcwallet.Config) (*chainreg.ChainControl, func(), error) {\n\n\twalletController, err := btcwallet.New(\n\t\t*walletConfig, partialChainControl.Cfg.BlockCache,\n\t)\n\tif err != nil {\n\t\terr := fmt.Errorf(\"unable to create wallet controller: %w\", err)\n\t\td.logger.Error(err)\n\t\treturn nil, nil, err\n\t}\n\n\tkeyRing := keychain.NewBtcWalletKeyRing(\n\t\twalletController.InternalWallet(), walletConfig.CoinType,\n\t)\n\n\t// Create, and start the lnwallet, which handles the core payment\n\t// channel logic, and exposes control via proxy state machines.\n\tlnWalletConfig := lnwallet.Config{\n\t\tDatabase:              partialChainControl.Cfg.ChanStateDB,\n\t\tNotifier:              partialChainControl.ChainNotifier,\n\t\tWalletController:      walletController,\n\t\tSigner:                walletController,\n\t\tFeeEstimator:          partialChainControl.FeeEstimator,\n\t\tSecretKeyRing:         keyRing,\n\t\tChainIO:               walletController,\n\t\tNetParams:             *walletConfig.NetParams,\n\t\tCoinSelectionStrategy: walletConfig.CoinSelectionStrategy,\n\t\tAuxLeafStore:          partialChainControl.Cfg.AuxLeafStore,\n\t\tAuxSigner:             partialChainControl.Cfg.AuxSigner,\n\t}\n\n\t// The broadcast is already always active for neutrino nodes, so we\n\t// don't want to create a rebroadcast loop.\n\tif partialChainControl.Cfg.NeutrinoCS == nil {\n\t\tcs := partialChainControl.ChainSource\n\t\tbroadcastCfg := pushtx.Config{\n\t\t\tBroadcast: func(tx *wire.MsgTx) error {\n\t\t\t\t_, err := cs.SendRawTransaction(\n\t\t\t\t\ttx, true,\n\t\t\t\t)\n\n\t\t\t\treturn err\n\t\t\t},\n\t\t\tSubscribeBlocks: proxyBlockEpoch(\n\t\t\t\tpartialChainControl.ChainNotifier,\n\t\t\t),\n\t\t\tRebroadcastInterval: pushtx.DefaultRebroadcastInterval,\n\t\t\t// In case the backend is different from neutrino we\n\t\t\t// make sure that broadcast backend errors are mapped\n\t\t\t// to the neutrino broadcastErr.\n\t\t\tMapCustomBroadcastError: func(err error) error {\n\t\t\t\trpcErr := cs.MapRPCErr(err)\n\t\t\t\treturn broadcastErrorMapper(rpcErr)\n\t\t\t},\n\t\t}\n\n\t\tlnWalletConfig.Rebroadcaster = newWalletReBroadcaster(\n\t\t\tpushtx.NewBroadcaster(&broadcastCfg),\n\t\t)\n\t}\n\n\t// We've created the wallet configuration now, so we can finish\n\t// initializing the main chain control.\n\tactiveChainControl, cleanUp, err := chainreg.NewChainControl(\n\t\tlnWalletConfig, walletController, partialChainControl,\n\t)\n\tif err != nil {\n\t\terr := fmt.Errorf(\"unable to create chain control: %w\", err)\n\t\td.logger.Error(err)\n\t\treturn nil, nil, err\n\t}\n\n\treturn activeChainControl, cleanUp, nil\n}\n\n// RPCSignerWalletImpl is a wallet implementation that uses a remote signer over\n// an RPC interface.\ntype RPCSignerWalletImpl struct {\n\t// DefaultWalletImpl is the embedded instance of the default\n\t// implementation that the remote signer uses as its watch-only wallet\n\t// for keeping track of addresses and UTXOs.\n\t*DefaultWalletImpl\n}\n\n// NewRPCSignerWalletImpl creates a new instance of the remote signing wallet\n// implementation.\nfunc NewRPCSignerWalletImpl(cfg *Config, logger btclog.Logger,\n\tinterceptor signal.Interceptor,\n\tmigrateWatchOnly bool) *RPCSignerWalletImpl {\n\n\treturn &RPCSignerWalletImpl{\n\t\tDefaultWalletImpl: &DefaultWalletImpl{\n\t\t\tcfg:              cfg,\n\t\t\tlogger:           logger,\n\t\t\tinterceptor:      interceptor,\n\t\t\twatchOnly:        true,\n\t\t\tmigrateWatchOnly: migrateWatchOnly,\n\t\t\tpwService:        createWalletUnlockerService(cfg),\n\t\t},\n\t}\n}\n\n// BuildChainControl is responsible for creating or unlocking and then fully\n// initializing a wallet and returning it as part of a fully populated chain\n// control instance.\n//\n// NOTE: This is part of the ChainControlBuilder interface.\nfunc (d *RPCSignerWalletImpl) BuildChainControl(\n\tpartialChainControl *chainreg.PartialChainControl,\n\twalletConfig *btcwallet.Config) (*chainreg.ChainControl, func(), error) {\n\n\twalletController, err := btcwallet.New(\n\t\t*walletConfig, partialChainControl.Cfg.BlockCache,\n\t)\n\tif err != nil {\n\t\terr := fmt.Errorf(\"unable to create wallet controller: %w\", err)\n\t\td.logger.Error(err)\n\t\treturn nil, nil, err\n\t}\n\n\tbaseKeyRing := keychain.NewBtcWalletKeyRing(\n\t\twalletController.InternalWallet(), walletConfig.CoinType,\n\t)\n\n\trpcKeyRing, err := rpcwallet.NewRPCKeyRing(\n\t\tbaseKeyRing, walletController,\n\t\td.DefaultWalletImpl.cfg.RemoteSigner, walletConfig.NetParams,\n\t)\n\tif err != nil {\n\t\terr := fmt.Errorf(\"unable to create RPC remote signing wallet \"+\n\t\t\t\"%v\", err)\n\t\td.logger.Error(err)\n\t\treturn nil, nil, err\n\t}\n\n\t// Create, and start the lnwallet, which handles the core payment\n\t// channel logic, and exposes control via proxy state machines.\n\tlnWalletConfig := lnwallet.Config{\n\t\tDatabase:              partialChainControl.Cfg.ChanStateDB,\n\t\tNotifier:              partialChainControl.ChainNotifier,\n\t\tWalletController:      rpcKeyRing,\n\t\tSigner:                rpcKeyRing,\n\t\tFeeEstimator:          partialChainControl.FeeEstimator,\n\t\tSecretKeyRing:         rpcKeyRing,\n\t\tChainIO:               walletController,\n\t\tNetParams:             *walletConfig.NetParams,\n\t\tCoinSelectionStrategy: walletConfig.CoinSelectionStrategy,\n\t}\n\n\t// We've created the wallet configuration now, so we can finish\n\t// initializing the main chain control.\n\tactiveChainControl, cleanUp, err := chainreg.NewChainControl(\n\t\tlnWalletConfig, rpcKeyRing, partialChainControl,\n\t)\n\tif err != nil {\n\t\terr := fmt.Errorf(\"unable to create chain control: %w\", err)\n\t\td.logger.Error(err)\n\t\treturn nil, nil, err\n\t}\n\n\treturn activeChainControl, cleanUp, nil\n}\n\n// DatabaseInstances is a struct that holds all instances to the actual\n// databases that are used in lnd.\ntype DatabaseInstances struct {\n\t// GraphDB is the database that stores the channel graph used for path\n\t// finding.\n\tGraphDB *graphdb.ChannelGraph\n\n\t// ChanStateDB is the database that stores all of our node's channel\n\t// state.\n\tChanStateDB *channeldb.DB\n\n\t// HeightHintDB is the database that stores height hints for spends.\n\tHeightHintDB kvdb.Backend\n\n\t// InvoiceDB is the database that stores information about invoices.\n\tInvoiceDB invoices.InvoiceDB\n\n\t// MacaroonDB is the database that stores macaroon root keys.\n\tMacaroonDB kvdb.Backend\n\n\t// DecayedLogDB is the database that stores p2p related encryption\n\t// information.\n\tDecayedLogDB kvdb.Backend\n\n\t// TowerClientDB is the database that stores the watchtower client's\n\t// configuration.\n\tTowerClientDB wtclient.DB\n\n\t// TowerServerDB is the database that stores the watchtower server's\n\t// configuration.\n\tTowerServerDB watchtower.DB\n\n\t// WalletDB is the configuration for loading the wallet database using\n\t// the btcwallet's loader.\n\tWalletDB btcwallet.LoaderOption\n\n\t// NativeSQLStore is a pointer to a native SQL store that can be used\n\t// for native SQL queries for tables that already support it. This may\n\t// be nil if the use-native-sql flag was not set.\n\tNativeSQLStore *sqldb.BaseDB\n}\n\n// DefaultDatabaseBuilder is a type that builds the default database backends\n// for lnd, using the given configuration to decide what actual implementation\n// to use.\ntype DefaultDatabaseBuilder struct {\n\tcfg    *Config\n\tlogger btclog.Logger\n}\n\n// NewDefaultDatabaseBuilder returns a new instance of the default database\n// builder.\nfunc NewDefaultDatabaseBuilder(cfg *Config,\n\tlogger btclog.Logger) *DefaultDatabaseBuilder {\n\n\treturn &DefaultDatabaseBuilder{\n\t\tcfg:    cfg,\n\t\tlogger: logger,\n\t}\n}\n\n// BuildDatabase extracts the current databases that we'll use for normal\n// operation in the daemon. A function closure that closes all opened databases\n// is also returned.\nfunc (d *DefaultDatabaseBuilder) BuildDatabase(\n\tctx context.Context) (*DatabaseInstances, func(), error) {\n\n\td.logger.Infof(\"Opening the main database, this might take a few \" +\n\t\t\"minutes...\")\n\n\tcfg := d.cfg\n\tif cfg.DB.Backend == lncfg.BoltBackend {\n\t\td.logger.Infof(\"Opening bbolt database, sync_freelist=%v, \"+\n\t\t\t\"auto_compact=%v\", !cfg.DB.Bolt.NoFreelistSync,\n\t\t\tcfg.DB.Bolt.AutoCompact)\n\t}\n\n\tstartOpenTime := time.Now()\n\n\tdatabaseBackends, err := cfg.DB.GetBackends(\n\t\tctx, cfg.graphDatabaseDir(), cfg.networkDir, filepath.Join(\n\t\t\tcfg.Watchtower.TowerDir, BitcoinChainName,\n\t\t\tlncfg.NormalizeNetwork(cfg.ActiveNetParams.Name),\n\t\t), cfg.WtClient.Active, cfg.Watchtower.Active, d.logger,\n\t)\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"unable to obtain database \"+\n\t\t\t\"backends: %v\", err)\n\t}\n\n\t// With the full remote mode we made sure both the graph and channel\n\t// state DB point to the same local or remote DB and the same namespace\n\t// within that DB.\n\tdbs := &DatabaseInstances{\n\t\tHeightHintDB:   databaseBackends.HeightHintDB,\n\t\tMacaroonDB:     databaseBackends.MacaroonDB,\n\t\tDecayedLogDB:   databaseBackends.DecayedLogDB,\n\t\tWalletDB:       databaseBackends.WalletDB,\n\t\tNativeSQLStore: databaseBackends.NativeSQLStore,\n\t}\n\tcleanUp := func() {\n\t\t// We can just close the returned close functions directly. Even\n\t\t// if we decorate the channel DB with an additional struct, its\n\t\t// close function still just points to the kvdb backend.\n\t\tfor name, closeFunc := range databaseBackends.CloseFuncs {\n\t\t\tif err := closeFunc(); err != nil {\n\t\t\t\td.logger.Errorf(\"Error closing %s \"+\n\t\t\t\t\t\"database: %v\", name, err)\n\t\t\t}\n\t\t}\n\t}\n\tif databaseBackends.Remote {\n\t\td.logger.Infof(\"Using remote %v database! Creating \"+\n\t\t\t\"graph and channel state DB instances\", cfg.DB.Backend)\n\t} else {\n\t\td.logger.Infof(\"Creating local graph and channel state DB \" +\n\t\t\t\"instances\")\n\t}\n\n\tgraphDBOptions := []graphdb.OptionModifier{\n\t\tgraphdb.WithRejectCacheSize(cfg.Caches.RejectCacheSize),\n\t\tgraphdb.WithChannelCacheSize(cfg.Caches.ChannelCacheSize),\n\t\tgraphdb.WithBatchCommitInterval(cfg.DB.BatchCommitInterval),\n\t\tgraphdb.WithUseGraphCache(!cfg.DB.NoGraphCache),\n\t}\n\n\t// We want to pre-allocate the channel graph cache according to what we\n\t// expect for mainnet to speed up memory allocation.\n\tif cfg.ActiveNetParams.Name == chaincfg.MainNetParams.Name {\n\t\tgraphDBOptions = append(\n\t\t\tgraphDBOptions, graphdb.WithPreAllocCacheNumNodes(\n\t\t\t\tgraphdb.DefaultPreAllocCacheNumNodes,\n\t\t\t),\n\t\t)\n\t}\n\n\tdbs.GraphDB, err = graphdb.NewChannelGraph(\n\t\tdatabaseBackends.GraphDB, graphDBOptions...,\n\t)\n\tif err != nil {\n\t\tcleanUp()\n\n\t\terr := fmt.Errorf(\"unable to open graph DB: %w\", err)\n\t\td.logger.Error(err)\n\n\t\treturn nil, nil, err\n\t}\n\n\tdbOptions := []channeldb.OptionModifier{\n\t\tchanneldb.OptionDryRunMigration(cfg.DryRunMigration),\n\t\tchanneldb.OptionKeepFailedPaymentAttempts(\n\t\t\tcfg.KeepFailedPaymentAttempts,\n\t\t),\n\t\tchanneldb.OptionStoreFinalHtlcResolutions(\n\t\t\tcfg.StoreFinalHtlcResolutions,\n\t\t),\n\t\tchanneldb.OptionPruneRevocationLog(cfg.DB.PruneRevocation),\n\t\tchanneldb.OptionNoRevLogAmtData(cfg.DB.NoRevLogAmtData),\n\t}\n\n\t// Otherwise, we'll open two instances, one for the state we only need\n\t// locally, and the other for things we want to ensure are replicated.\n\tdbs.ChanStateDB, err = channeldb.CreateWithBackend(\n\t\tdatabaseBackends.ChanStateDB, dbOptions...,\n\t)\n\tswitch {\n\t// Give the DB a chance to dry run the migration. Since we know that\n\t// both the channel state and graph DBs are still always behind the same\n\t// backend, we know this would be applied to both of those DBs.\n\tcase err == channeldb.ErrDryRunMigrationOK:\n\t\td.logger.Infof(\"Channel DB dry run migration successful\")\n\t\treturn nil, nil, err\n\n\tcase err != nil:\n\t\tcleanUp()\n\n\t\terr := fmt.Errorf(\"unable to open graph DB: %w\", err)\n\t\td.logger.Error(err)\n\t\treturn nil, nil, err\n\t}\n\n\t// Instantiate a native SQL invoice store if the flag is set.\n\tif d.cfg.DB.UseNativeSQL {\n\t\t// KV invoice db resides in the same database as the channel\n\t\t// state DB. Let's query the database to see if we have any\n\t\t// invoices there. If we do, we won't allow the user to start\n\t\t// lnd with native SQL enabled, as we don't currently migrate\n\t\t// the invoices to the new database schema.\n\t\tinvoiceSlice, err := dbs.ChanStateDB.QueryInvoices(\n\t\t\tctx, invoices.InvoiceQuery{\n\t\t\t\tNumMaxInvoices: 1,\n\t\t\t},\n\t\t)\n\t\tif err != nil {\n\t\t\tcleanUp()\n\t\t\td.logger.Errorf(\"Unable to query KV invoice DB: %v\",\n\t\t\t\terr)\n\n\t\t\treturn nil, nil, err\n\t\t}\n\n\t\tif len(invoiceSlice.Invoices) > 0 {\n\t\t\tcleanUp()\n\t\t\terr := fmt.Errorf(\"found invoices in the KV invoice \" +\n\t\t\t\t\"DB, migration to native SQL is not yet \" +\n\t\t\t\t\"supported\")\n\t\t\td.logger.Error(err)\n\n\t\t\treturn nil, nil, err\n\t\t}\n\n\t\texecutor := sqldb.NewTransactionExecutor(\n\t\t\tdbs.NativeSQLStore,\n\t\t\tfunc(tx *sql.Tx) invoices.SQLInvoiceQueries {\n\t\t\t\treturn dbs.NativeSQLStore.WithTx(tx)\n\t\t\t},\n\t\t)\n\n\t\tdbs.InvoiceDB = invoices.NewSQLStore(\n\t\t\texecutor, clock.NewDefaultClock(),\n\t\t)\n\t} else {\n\t\tdbs.InvoiceDB = dbs.ChanStateDB\n\t}\n\n\t// Wrap the watchtower client DB and make sure we clean up.\n\tif cfg.WtClient.Active {\n\t\tdbs.TowerClientDB, err = wtdb.OpenClientDB(\n\t\t\tdatabaseBackends.TowerClientDB,\n\t\t)\n\t\tif err != nil {\n\t\t\tcleanUp()\n\n\t\t\terr := fmt.Errorf(\"unable to open %s database: %w\",\n\t\t\t\tlncfg.NSTowerClientDB, err)\n\t\t\td.logger.Error(err)\n\t\t\treturn nil, nil, err\n\t\t}\n\t}\n\n\t// Wrap the watchtower server DB and make sure we clean up.\n\tif cfg.Watchtower.Active {\n\t\tdbs.TowerServerDB, err = wtdb.OpenTowerDB(\n\t\t\tdatabaseBackends.TowerServerDB,\n\t\t)\n\t\tif err != nil {\n\t\t\tcleanUp()\n\n\t\t\terr := fmt.Errorf(\"unable to open %s database: %w\",\n\t\t\t\tlncfg.NSTowerServerDB, err)\n\t\t\td.logger.Error(err)\n\t\t\treturn nil, nil, err\n\t\t}\n\t}\n\n\topenTime := time.Since(startOpenTime)\n\td.logger.Infof(\"Database(s) now open (time_to_open=%v)!\", openTime)\n\n\treturn dbs, cleanUp, nil\n}\n\n// waitForWalletPassword blocks until a password is provided by the user to\n// this RPC server.\nfunc waitForWalletPassword(cfg *Config,\n\tpwService *walletunlocker.UnlockerService,\n\tloaderOpts []btcwallet.LoaderOption, shutdownChan <-chan struct{}) (\n\t*walletunlocker.WalletUnlockParams, error) {\n\n\t// Wait for user to provide the password.\n\tltndLog.Infof(\"Waiting for wallet encryption password. Use `lncli \" +\n\t\t\"create` to create a wallet, `lncli unlock` to unlock an \" +\n\t\t\"existing wallet, or `lncli changepassword` to change the \" +\n\t\t\"password of an existing wallet and unlock it.\")\n\n\t// We currently don't distinguish between getting a password to be used\n\t// for creation or unlocking, as a new wallet db will be created if\n\t// none exists when creating the chain control.\n\tselect {\n\t// The wallet is being created for the first time, we'll check to see\n\t// if the user provided any entropy for seed creation. If so, then\n\t// we'll create the wallet early to load the seed.\n\tcase initMsg := <-pwService.InitMsgs:\n\t\tpassword := initMsg.Passphrase\n\t\tcipherSeed := initMsg.WalletSeed\n\t\textendedKey := initMsg.WalletExtendedKey\n\t\twatchOnlyAccounts := initMsg.WatchOnlyAccounts\n\t\trecoveryWindow := initMsg.RecoveryWindow\n\n\t\t// Before we proceed, we'll check the internal version of the\n\t\t// seed. If it's greater than the current key derivation\n\t\t// version, then we'll return an error as we don't understand\n\t\t// this.\n\t\tif cipherSeed != nil &&\n\t\t\t!keychain.IsKnownVersion(cipherSeed.InternalVersion) {\n\n\t\t\treturn nil, fmt.Errorf(\"invalid internal \"+\n\t\t\t\t\"seed version %v, current max version is %v\",\n\t\t\t\tcipherSeed.InternalVersion,\n\t\t\t\tkeychain.CurrentKeyDerivationVersion)\n\t\t}\n\n\t\tloader, err := btcwallet.NewWalletLoader(\n\t\t\tcfg.ActiveNetParams.Params, recoveryWindow,\n\t\t\tloaderOpts...,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// With the seed, we can now use the wallet loader to create\n\t\t// the wallet, then pass it back to avoid unlocking it again.\n\t\tvar (\n\t\t\tbirthday  time.Time\n\t\t\tnewWallet *wallet.Wallet\n\t\t)\n\t\tswitch {\n\t\t// A normal cipher seed was given, use the birthday encoded in\n\t\t// it and create the wallet from that.\n\t\tcase cipherSeed != nil:\n\t\t\tbirthday = cipherSeed.BirthdayTime()\n\t\t\tnewWallet, err = loader.CreateNewWallet(\n\t\t\t\tpassword, password, cipherSeed.Entropy[:],\n\t\t\t\tbirthday,\n\t\t\t)\n\n\t\t// No seed was given, we're importing a wallet from its extended\n\t\t// private key.\n\t\tcase extendedKey != nil:\n\t\t\tbirthday = initMsg.ExtendedKeyBirthday\n\t\t\tnewWallet, err = loader.CreateNewWalletExtendedKey(\n\t\t\t\tpassword, password, extendedKey, birthday,\n\t\t\t)\n\n\t\t// Neither seed nor extended private key was given, so maybe the\n\t\t// third option was chosen, the watch-only initialization. In\n\t\t// this case we need to import each of the xpubs individually.\n\t\tcase watchOnlyAccounts != nil:\n\t\t\tif !cfg.RemoteSigner.Enable {\n\t\t\t\treturn nil, fmt.Errorf(\"cannot initialize \" +\n\t\t\t\t\t\"watch only wallet with remote \" +\n\t\t\t\t\t\"signer config disabled\")\n\t\t\t}\n\n\t\t\tbirthday = initMsg.WatchOnlyBirthday\n\t\t\tnewWallet, err = loader.CreateNewWatchingOnlyWallet(\n\t\t\t\tpassword, birthday,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\terr = importWatchOnlyAccounts(newWallet, initMsg)\n\n\t\tdefault:\n\t\t\t// The unlocker service made sure either the cipher seed\n\t\t\t// or the extended key is set so, we shouldn't get here.\n\t\t\t// The default case is just here for readability and\n\t\t\t// completeness.\n\t\t\terr = fmt.Errorf(\"cannot create wallet, neither seed \" +\n\t\t\t\t\"nor extended key was given\")\n\t\t}\n\t\tif err != nil {\n\t\t\t// Don't leave the file open in case the new wallet\n\t\t\t// could not be created for whatever reason.\n\t\t\tif err := loader.UnloadWallet(); err != nil {\n\t\t\t\tltndLog.Errorf(\"Could not unload new \"+\n\t\t\t\t\t\"wallet: %v\", err)\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// For new wallets, the ResetWalletTransactions flag is a no-op.\n\t\tif cfg.ResetWalletTransactions {\n\t\t\tltndLog.Warnf(\"Ignoring reset-wallet-transactions \" +\n\t\t\t\t\"flag for new wallet as it has no effect\")\n\t\t}\n\n\t\treturn &walletunlocker.WalletUnlockParams{\n\t\t\tPassword:        password,\n\t\t\tBirthday:        birthday,\n\t\t\tRecoveryWindow:  recoveryWindow,\n\t\t\tWallet:          newWallet,\n\t\t\tChansToRestore:  initMsg.ChanBackups,\n\t\t\tUnloadWallet:    loader.UnloadWallet,\n\t\t\tStatelessInit:   initMsg.StatelessInit,\n\t\t\tMacResponseChan: pwService.MacResponseChan,\n\t\t\tMacRootKey:      initMsg.MacRootKey,\n\t\t}, nil\n\n\t// The wallet has already been created in the past, and is simply being\n\t// unlocked. So we'll just return these passphrases.\n\tcase unlockMsg := <-pwService.UnlockMsgs:\n\t\t// Resetting the transactions is something the user likely only\n\t\t// wants to do once so we add a prominent warning to the log to\n\t\t// remind the user to turn off the setting again after\n\t\t// successful completion.\n\t\tif cfg.ResetWalletTransactions {\n\t\t\tltndLog.Warnf(\"Dropped all transaction history from \" +\n\t\t\t\t\"on-chain wallet. Remember to disable \" +\n\t\t\t\t\"reset-wallet-transactions flag for next \" +\n\t\t\t\t\"start of lnd\")\n\t\t}\n\n\t\treturn &walletunlocker.WalletUnlockParams{\n\t\t\tPassword:        unlockMsg.Passphrase,\n\t\t\tRecoveryWindow:  unlockMsg.RecoveryWindow,\n\t\t\tWallet:          unlockMsg.Wallet,\n\t\t\tChansToRestore:  unlockMsg.ChanBackups,\n\t\t\tUnloadWallet:    unlockMsg.UnloadWallet,\n\t\t\tStatelessInit:   unlockMsg.StatelessInit,\n\t\t\tMacResponseChan: pwService.MacResponseChan,\n\t\t}, nil\n\n\t// If we got a shutdown signal we just return with an error immediately\n\tcase <-shutdownChan:\n\t\treturn nil, fmt.Errorf(\"shutting down\")\n\t}\n}\n\n// importWatchOnlyAccounts imports all individual account xpubs into our wallet\n// which we created as watch-only.\nfunc importWatchOnlyAccounts(wallet *wallet.Wallet,\n\tinitMsg *walletunlocker.WalletInitMsg) error {\n\n\tscopes := make([]waddrmgr.ScopedIndex, 0, len(initMsg.WatchOnlyAccounts))\n\tfor scope := range initMsg.WatchOnlyAccounts {\n\t\tscopes = append(scopes, scope)\n\t}\n\n\t// We need to import the accounts in the correct order, otherwise the\n\t// indices will be incorrect.\n\tsort.Slice(scopes, func(i, j int) bool {\n\t\treturn scopes[i].Scope.Purpose < scopes[j].Scope.Purpose ||\n\t\t\tscopes[i].Index < scopes[j].Index\n\t})\n\n\tfor _, scope := range scopes {\n\t\taddrSchema := waddrmgr.ScopeAddrMap[waddrmgr.KeyScopeBIP0084]\n\n\t\t// We want witness pubkey hash by default, except for BIP49\n\t\t// where we want mixed and BIP86 where we want taproot address\n\t\t// formats.\n\t\tswitch scope.Scope.Purpose {\n\t\tcase waddrmgr.KeyScopeBIP0049Plus.Purpose,\n\t\t\twaddrmgr.KeyScopeBIP0086.Purpose:\n\n\t\t\taddrSchema = waddrmgr.ScopeAddrMap[scope.Scope]\n\t\t}\n\n\t\t// We want a human-readable account name. But for the default\n\t\t// on-chain wallet we actually need to call it \"default\" to make\n\t\t// sure everything works correctly.\n\t\tname := fmt.Sprintf(\"%s/%d'\", scope.Scope.String(), scope.Index)\n\t\tif scope.Index == 0 {\n\t\t\tname = \"default\"\n\t\t}\n\n\t\t_, err := wallet.ImportAccountWithScope(\n\t\t\tname, initMsg.WatchOnlyAccounts[scope],\n\t\t\tinitMsg.WatchOnlyMasterFingerprint, scope.Scope,\n\t\t\taddrSchema,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"could not import account %v: %w\",\n\t\t\t\tname, err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// initNeutrinoBackend inits a new instance of the neutrino light client\n// backend given a target chain directory to store the chain state.\nfunc initNeutrinoBackend(ctx context.Context, cfg *Config, chainDir string,\n\tblockCache *blockcache.BlockCache) (*neutrino.ChainService,\n\tfunc(), error) {\n\n\t// Both channel validation flags are false by default but their meaning\n\t// is the inverse of each other. Therefore both cannot be true. For\n\t// every other case, the neutrino.validatechannels overwrites the\n\t// routing.assumechanvalid value.\n\tif cfg.NeutrinoMode.ValidateChannels && cfg.Routing.AssumeChannelValid {\n\t\treturn nil, nil, fmt.Errorf(\"can't set both \" +\n\t\t\t\"neutrino.validatechannels and routing.\" +\n\t\t\t\"assumechanvalid to true at the same time\")\n\t}\n\tcfg.Routing.AssumeChannelValid = !cfg.NeutrinoMode.ValidateChannels\n\n\t// First we'll open the database file for neutrino, creating the\n\t// database if needed. We append the normalized network name here to\n\t// match the behavior of btcwallet.\n\tdbPath := filepath.Join(\n\t\tchainDir, lncfg.NormalizeNetwork(cfg.ActiveNetParams.Name),\n\t)\n\n\t// Ensure that the neutrino db path exists.\n\tif err := os.MkdirAll(dbPath, 0700); err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tvar (\n\t\tdb  walletdb.DB\n\t\terr error\n\t)\n\tswitch {\n\tcase cfg.DB.Backend == kvdb.SqliteBackendName:\n\t\tsqliteConfig := lncfg.GetSqliteConfigKVDB(cfg.DB.Sqlite)\n\t\tdb, err = kvdb.Open(\n\t\t\tkvdb.SqliteBackendName, ctx, sqliteConfig, dbPath,\n\t\t\tlncfg.SqliteNeutrinoDBName, lncfg.NSNeutrinoDB,\n\t\t)\n\n\tdefault:\n\t\tdbName := filepath.Join(dbPath, \"neutrino.db\")\n\t\tdb, err = walletdb.Create(\n\t\t\t\"bdb\", dbName, !cfg.SyncFreelist, cfg.DB.Bolt.DBTimeout,\n\t\t)\n\t}\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"unable to create \"+\n\t\t\t\"neutrino database: %v\", err)\n\t}\n\n\theaderStateAssertion, err := parseHeaderStateAssertion(\n\t\tcfg.NeutrinoMode.AssertFilterHeader,\n\t)\n\tif err != nil {\n\t\tdb.Close()\n\t\treturn nil, nil, err\n\t}\n\n\t// With the database open, we can now create an instance of the\n\t// neutrino light client. We pass in relevant configuration parameters\n\t// required.\n\tconfig := neutrino.Config{\n\t\tDataDir:      dbPath,\n\t\tDatabase:     db,\n\t\tChainParams:  *cfg.ActiveNetParams.Params,\n\t\tAddPeers:     cfg.NeutrinoMode.AddPeers,\n\t\tConnectPeers: cfg.NeutrinoMode.ConnectPeers,\n\t\tDialer: func(addr net.Addr) (net.Conn, error) {\n\t\t\treturn cfg.net.Dial(\n\t\t\t\taddr.Network(), addr.String(),\n\t\t\t\tcfg.ConnectionTimeout,\n\t\t\t)\n\t\t},\n\t\tNameResolver: func(host string) ([]net.IP, error) {\n\t\t\taddrs, err := cfg.net.LookupHost(host)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tips := make([]net.IP, 0, len(addrs))\n\t\t\tfor _, strIP := range addrs {\n\t\t\t\tip := net.ParseIP(strIP)\n\t\t\t\tif ip == nil {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tips = append(ips, ip)\n\t\t\t}\n\n\t\t\treturn ips, nil\n\t\t},\n\t\tAssertFilterHeader: headerStateAssertion,\n\t\tBlockCache:         blockCache.Cache,\n\t\tBroadcastTimeout:   cfg.NeutrinoMode.BroadcastTimeout,\n\t\tPersistToDisk:      cfg.NeutrinoMode.PersistFilters,\n\t}\n\n\tneutrino.MaxPeers = 8\n\tneutrino.BanDuration = time.Hour * 48\n\tneutrino.UserAgentName = cfg.NeutrinoMode.UserAgentName\n\tneutrino.UserAgentVersion = cfg.NeutrinoMode.UserAgentVersion\n\n\tneutrinoCS, err := neutrino.NewChainService(config)\n\tif err != nil {\n\t\tdb.Close()\n\t\treturn nil, nil, fmt.Errorf(\"unable to create neutrino light \"+\n\t\t\t\"client: %v\", err)\n\t}\n\n\tif err := neutrinoCS.Start(); err != nil {\n\t\tdb.Close()\n\t\treturn nil, nil, err\n\t}\n\n\tcleanUp := func() {\n\t\tif err := neutrinoCS.Stop(); err != nil {\n\t\t\tltndLog.Infof(\"Unable to stop neutrino light client: \"+\n\t\t\t\t\"%v\", err)\n\t\t}\n\t\tdb.Close()\n\t}\n\n\treturn neutrinoCS, cleanUp, nil\n}\n\n// parseHeaderStateAssertion parses the user-specified neutrino header state\n// into a headerfs.FilterHeader.\nfunc parseHeaderStateAssertion(state string) (*headerfs.FilterHeader, error) {\n\tif len(state) == 0 {\n\t\treturn nil, nil\n\t}\n\n\tsplit := strings.Split(state, \":\")\n\tif len(split) != 2 {\n\t\treturn nil, fmt.Errorf(\"header state assertion %v in \"+\n\t\t\t\"unexpected format, expected format height:hash\", state)\n\t}\n\n\theight, err := strconv.ParseUint(split[0], 10, 32)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"invalid filter header height: %w\", err)\n\t}\n\n\thash, err := chainhash.NewHashFromStr(split[1])\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"invalid filter header hash: %w\", err)\n\t}\n\n\treturn &headerfs.FilterHeader{\n\t\tHeight:     uint32(height),\n\t\tFilterHash: *hash,\n\t}, nil\n}\n\n// broadcastErrorMapper maps errors from bitcoin backends other than neutrino to\n// the neutrino BroadcastError which allows the Rebroadcaster which currently\n// resides in the neutrino package to use all of its functionalities.\nfunc broadcastErrorMapper(err error) error {\n\tvar returnErr error\n\n\t// We only filter for specific backend errors which are relevant for the\n\t// Rebroadcaster.\n\tswitch {\n\t// This makes sure the tx is removed from the rebroadcaster once it is\n\t// confirmed.\n\tcase errors.Is(err, chain.ErrTxAlreadyKnown),\n\t\terrors.Is(err, chain.ErrTxAlreadyConfirmed):\n\n\t\treturnErr = &pushtx.BroadcastError{\n\t\t\tCode:   pushtx.Confirmed,\n\t\t\tReason: err.Error(),\n\t\t}\n\n\t// Transactions which are still in mempool but might fall out because\n\t// of low fees are rebroadcasted despite of their backend error.\n\tcase errors.Is(err, chain.ErrTxAlreadyInMempool):\n\t\treturnErr = &pushtx.BroadcastError{\n\t\t\tCode:   pushtx.Mempool,\n\t\t\tReason: err.Error(),\n\t\t}\n\n\t// Transactions which are not accepted into mempool because of low fees\n\t// in the first place are rebroadcasted despite of their backend error.\n\t// Mempool conditions change over time so it makes sense to retry\n\t// publishing the transaction. Moreover we log the detailed error so the\n\t// user can intervene and increase the size of his mempool.\n\tcase errors.Is(err, chain.ErrMempoolMinFeeNotMet):\n\t\tltndLog.Warnf(\"Error while broadcasting transaction: %v\", err)\n\n\t\treturnErr = &pushtx.BroadcastError{\n\t\t\tCode:   pushtx.Mempool,\n\t\t\tReason: err.Error(),\n\t\t}\n\t}\n\n\treturn returnErr\n}\n"
        },
        {
          "name": "config_test.go",
          "type": "blob",
          "size": 3.068359375,
          "content": "package lnd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/lightningnetwork/lnd/chainreg\"\n\t\"github.com/lightningnetwork/lnd/routing\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nvar (\n\ttestPassword     = \"testpassword\"\n\tredactedPassword = \"[redacted]\"\n)\n\n// TestConfigToFlatMap tests that the configToFlatMap function works as\n// expected on the default configuration.\nfunc TestConfigToFlatMap(t *testing.T) {\n\tcfg := DefaultConfig()\n\tcfg.BitcoindMode.RPCPass = testPassword\n\tcfg.BtcdMode.RPCPass = testPassword\n\tcfg.Tor.Password = testPassword\n\tcfg.DB.Etcd.Pass = testPassword\n\tcfg.DB.Postgres.Dsn = testPassword\n\n\t// Set a deprecated field.\n\tcfg.Bitcoin.Active = true\n\n\tresult, deprecated, err := configToFlatMap(cfg)\n\trequire.NoError(t, err)\n\n\t// Check that the deprecated option has been parsed out.\n\trequire.Contains(t, deprecated, \"bitcoin.active\")\n\n\t// Pick a couple of random values to check.\n\trequire.Equal(t, DefaultLndDir, result[\"lnddir\"])\n\trequire.Equal(\n\t\tt, fmt.Sprintf(\"%v\", chainreg.DefaultBitcoinTimeLockDelta),\n\t\tresult[\"bitcoin.timelockdelta\"],\n\t)\n\trequire.Equal(\n\t\tt, fmt.Sprintf(\"%v\", routing.DefaultAprioriWeight),\n\t\tresult[\"routerrpc.apriori.weight\"],\n\t)\n\trequire.Contains(t, result, \"routerrpc.routermacaroonpath\")\n\n\t// Check that sensitive values are not included.\n\trequire.Equal(t, redactedPassword, result[\"bitcoind.rpcpass\"])\n\trequire.Equal(t, redactedPassword, result[\"btcd.rpcpass\"])\n\trequire.Equal(t, redactedPassword, result[\"tor.password\"])\n\trequire.Equal(t, redactedPassword, result[\"db.etcd.pass\"])\n\trequire.Equal(t, redactedPassword, result[\"db.postgres.dsn\"])\n}\n\n// TestSupplyEnvValue tests that the supplyEnvValue function works as\n// expected on the passed inputs.\nfunc TestSupplyEnvValue(t *testing.T) {\n\t// Mock environment variables for testing.\n\tt.Setenv(\"EXISTING_VAR\", \"existing_value\")\n\tt.Setenv(\"EMPTY_VAR\", \"\")\n\n\ttests := []struct {\n\t\tinput       string\n\t\texpected    string\n\t\tdescription string\n\t}{\n\t\t{\n\t\t\tinput:    \"$EXISTING_VAR\",\n\t\t\texpected: \"existing_value\",\n\t\t\tdescription: \"Valid environment variable without \" +\n\t\t\t\t\"default value\",\n\t\t},\n\t\t{\n\t\t\tinput:    \"${EXISTING_VAR:-default_value}\",\n\t\t\texpected: \"existing_value\",\n\t\t\tdescription: \"Valid environment variable with \" +\n\t\t\t\t\"default value\",\n\t\t},\n\t\t{\n\t\t\tinput:    \"$NON_EXISTENT_VAR\",\n\t\t\texpected: \"\",\n\t\t\tdescription: \"Non-existent environment variable \" +\n\t\t\t\t\"without default value\",\n\t\t},\n\t\t{\n\t\t\tinput:    \"${NON_EXISTENT_VAR:-default_value}\",\n\t\t\texpected: \"default_value\",\n\t\t\tdescription: \"Non-existent environment variable \" +\n\t\t\t\t\"with default value\",\n\t\t},\n\t\t{\n\t\t\tinput:    \"$EMPTY_VAR\",\n\t\t\texpected: \"\",\n\t\t\tdescription: \"Empty environment variable without \" +\n\t\t\t\t\"default value\",\n\t\t},\n\t\t{\n\t\t\tinput:    \"${EMPTY_VAR:-default_value}\",\n\t\t\texpected: \"default_value\",\n\t\t\tdescription: \"Empty environment variable with \" +\n\t\t\t\t\"default value\",\n\t\t},\n\t\t{\n\t\t\tinput:       \"raw_input\",\n\t\t\texpected:    \"raw_input\",\n\t\t\tdescription: \"Raw input - no matching format\",\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tt.Run(test.description, func(t *testing.T) {\n\t\t\tresult := supplyEnvValue(test.input)\n\t\t\trequire.Equal(t, test.expected, result)\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "contractcourt",
          "type": "tree",
          "content": null
        },
        {
          "name": "contrib",
          "type": "tree",
          "content": null
        },
        {
          "name": "dev.Dockerfile",
          "type": "blob",
          "size": 1.125,
          "content": "# If you change this please also update GO_VERSION in Makefile (then run\n# `make lint` to see where else it needs to be updated as well).\nFROM golang:1.22.6-alpine as builder\n\nLABEL maintainer=\"Olaoluwa Osuntokun <laolu@lightning.engineering>\"\n\n# Force Go to use the cgo based DNS resolver. This is required to ensure DNS\n# queries required to connect to linked containers succeed.\nENV GODEBUG netdns=cgo\n\n# Install dependencies.\nRUN apk add --no-cache --update alpine-sdk \\\n    bash \\\n    git \\\n    make \n\n# Copy in the local repository to build from.\nCOPY . /go/src/github.com/lightningnetwork/lnd\n\n#  Install/build lnd.\nRUN cd /go/src/github.com/lightningnetwork/lnd \\\n&&  make \\\n&&  make install-all tags=\"signrpc walletrpc chainrpc invoicesrpc peersrpc\"\n\n# Start a new, final image to reduce size.\nFROM alpine as final\n\n# Expose lnd ports (server, rpc).\nEXPOSE 9735 10009\n\n# Copy the binaries and entrypoint from the builder image.\nCOPY --from=builder /go/bin/lncli /bin/\nCOPY --from=builder /go/bin/lnd /bin/\n\n# Add bash.\nRUN apk add --no-cache \\\n    bash\n\n# Copy the entrypoint script.\nCOPY \"docker/lnd/start-lnd.sh\" .\nRUN chmod +x start-lnd.sh\n"
        },
        {
          "name": "discovery",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc.go",
          "type": "blob",
          "size": 0.01171875,
          "content": "package lnd\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "feature",
          "type": "tree",
          "content": null
        },
        {
          "name": "fn",
          "type": "tree",
          "content": null
        },
        {
          "name": "funding",
          "type": "tree",
          "content": null
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 10.3173828125,
          "content": "module github.com/lightningnetwork/lnd\n\nrequire (\n\tgithub.com/NebulousLabs/go-upnp v0.0.0-20180202185039-29b680b06c82\n\tgithub.com/Yawning/aez v0.0.0-20211027044916-e49e68abd344\n\tgithub.com/andybalholm/brotli v1.0.4\n\tgithub.com/btcsuite/btcd v0.24.3-0.20241210095828-e646d437e95b\n\tgithub.com/btcsuite/btcd/btcec/v2 v2.3.4\n\tgithub.com/btcsuite/btcd/btcutil v1.1.5\n\tgithub.com/btcsuite/btcd/btcutil/psbt v1.1.8\n\tgithub.com/btcsuite/btcd/chaincfg/chainhash v1.1.0\n\tgithub.com/btcsuite/btclog v0.0.0-20241003133417-09c4e92e319c\n\tgithub.com/btcsuite/btclog/v2 v2.0.1-0.20250110154127-3ae4bf1cb318\n\tgithub.com/btcsuite/btcwallet v0.16.10-0.20241127094224-93c858b2ad63\n\tgithub.com/btcsuite/btcwallet/wallet/txauthor v1.3.5\n\tgithub.com/btcsuite/btcwallet/wallet/txrules v1.2.2\n\tgithub.com/btcsuite/btcwallet/walletdb v1.4.4\n\tgithub.com/btcsuite/btcwallet/wtxmgr v1.5.4\n\tgithub.com/coreos/go-systemd v0.0.0-20190719114852-fd7a80b32e1f\n\tgithub.com/davecgh/go-spew v1.1.1\n\tgithub.com/decred/dcrd/dcrec/secp256k1/v4 v4.3.0\n\tgithub.com/go-errors/errors v1.0.1\n\tgithub.com/gorilla/websocket v1.5.0\n\tgithub.com/grpc-ecosystem/go-grpc-middleware v1.3.0\n\tgithub.com/grpc-ecosystem/go-grpc-prometheus v1.2.0\n\tgithub.com/grpc-ecosystem/grpc-gateway/v2 v2.5.0\n\tgithub.com/jackc/pgx/v4 v4.18.2\n\tgithub.com/jackpal/gateway v1.0.5\n\tgithub.com/jackpal/go-nat-pmp v0.0.0-20170405195558-28a68d0c24ad\n\tgithub.com/jedib0t/go-pretty/v6 v6.2.7\n\tgithub.com/jessevdk/go-flags v1.4.0\n\tgithub.com/jrick/logrotate v1.1.2\n\tgithub.com/kkdai/bstream v1.0.0\n\tgithub.com/lightninglabs/neutrino v0.16.1-0.20240425105051-602843d34ffd\n\tgithub.com/lightninglabs/neutrino/cache v1.1.2\n\tgithub.com/lightningnetwork/lightning-onion v1.2.1-0.20240712235311-98bd56499dfb\n\tgithub.com/lightningnetwork/lnd/cert v1.2.2\n\tgithub.com/lightningnetwork/lnd/clock v1.1.1\n\tgithub.com/lightningnetwork/lnd/fn/v2 v2.0.4\n\tgithub.com/lightningnetwork/lnd/healthcheck v1.2.6\n\tgithub.com/lightningnetwork/lnd/kvdb v1.4.12\n\tgithub.com/lightningnetwork/lnd/queue v1.1.1\n\tgithub.com/lightningnetwork/lnd/sqldb v1.0.6\n\tgithub.com/lightningnetwork/lnd/ticker v1.1.1\n\tgithub.com/lightningnetwork/lnd/tlv v1.3.0\n\tgithub.com/lightningnetwork/lnd/tor v1.1.4\n\tgithub.com/ltcsuite/ltcd v0.0.0-20190101042124-f37f8bf35796\n\tgithub.com/miekg/dns v1.1.43\n\tgithub.com/pkg/errors v0.9.1\n\tgithub.com/prometheus/client_golang v1.11.1\n\tgithub.com/stretchr/testify v1.9.0\n\tgithub.com/tv42/zbase32 v0.0.0-20160707012821-501572607d02\n\tgithub.com/urfave/cli v1.22.9\n\tgo.etcd.io/etcd/client/pkg/v3 v3.5.7\n\tgo.etcd.io/etcd/client/v3 v3.5.7\n\tgolang.org/x/crypto v0.22.0\n\tgolang.org/x/exp v0.0.0-20240325151524-a685a6edb6d8\n\tgolang.org/x/mobile v0.0.0-20190719004257-d2bd2a29d028\n\tgolang.org/x/net v0.24.0\n\tgolang.org/x/sync v0.7.0\n\tgolang.org/x/term v0.19.0\n\tgolang.org/x/time v0.3.0\n\tgoogle.golang.org/grpc v1.59.0\n\tgoogle.golang.org/protobuf v1.33.0\n\tgopkg.in/macaroon-bakery.v2 v2.0.1\n\tgopkg.in/macaroon.v2 v2.0.0\n\tpgregory.net/rapid v1.1.0\n)\n\nrequire (\n\tgithub.com/Azure/go-ansiterm v0.0.0-20230124172434-306776ec8161 // indirect\n\tgithub.com/Microsoft/go-winio v0.6.1 // indirect\n\tgithub.com/NebulousLabs/fastrand v0.0.0-20181203155948-6fb6489aac4e // indirect\n\tgithub.com/Nvveen/Gotty v0.0.0-20120604004816-cd527374f1e5 // indirect\n\tgithub.com/aead/chacha20 v0.0.0-20180709150244-8b13a72661da // indirect\n\tgithub.com/aead/siphash v1.0.1 // indirect\n\tgithub.com/beorn7/perks v1.0.1 // indirect\n\tgithub.com/btcsuite/btcwallet/wallet/txsizes v1.2.5 // indirect\n\tgithub.com/btcsuite/go-socks v0.0.0-20170105172521-4720035b7bfd // indirect\n\tgithub.com/btcsuite/websocket v0.0.0-20150119174127-31079b680792 // indirect\n\tgithub.com/btcsuite/winsvc v1.0.0 // indirect\n\tgithub.com/cenkalti/backoff/v4 v4.1.3 // indirect\n\tgithub.com/cespare/xxhash/v2 v2.2.0 // indirect\n\tgithub.com/containerd/continuity v0.3.0 // indirect\n\tgithub.com/coreos/go-semver v0.3.0 // indirect\n\tgithub.com/coreos/go-systemd/v22 v22.3.2 // indirect\n\tgithub.com/cpuguy83/go-md2man/v2 v2.0.0 // indirect\n\tgithub.com/decred/dcrd/crypto/blake256 v1.0.1 // indirect\n\tgithub.com/decred/dcrd/lru v1.1.2 // indirect\n\tgithub.com/docker/cli v20.10.17+incompatible // indirect\n\tgithub.com/docker/docker v24.0.7+incompatible // indirect\n\tgithub.com/docker/go-connections v0.4.0 // indirect\n\tgithub.com/docker/go-units v0.5.0 // indirect\n\tgithub.com/dustin/go-humanize v1.0.1 // indirect\n\tgithub.com/fergusstrange/embedded-postgres v1.25.0 // indirect\n\tgithub.com/fsnotify/fsnotify v1.5.4 // indirect\n\tgithub.com/gogo/protobuf v1.3.2 // indirect\n\tgithub.com/golang-jwt/jwt/v4 v4.4.2 // indirect\n\tgithub.com/golang-migrate/migrate/v4 v4.17.0 // indirect\n\tgithub.com/golang/protobuf v1.5.3 // indirect\n\tgithub.com/golang/snappy v0.0.4 // indirect\n\tgithub.com/google/btree v1.0.1 // indirect\n\tgithub.com/google/shlex v0.0.0-20191202100458-e7afc7fbc510 // indirect\n\tgithub.com/google/uuid v1.6.0 // indirect\n\tgithub.com/grpc-ecosystem/grpc-gateway v1.16.0 // indirect\n\tgithub.com/hashicorp/errwrap v1.1.0 // indirect\n\tgithub.com/hashicorp/go-multierror v1.1.1 // indirect\n\tgithub.com/hashicorp/golang-lru/v2 v2.0.7 // indirect\n\tgithub.com/imdario/mergo v0.3.12 // indirect\n\tgithub.com/jackc/chunkreader/v2 v2.0.1 // indirect\n\tgithub.com/jackc/pgconn v1.14.3 // indirect\n\tgithub.com/jackc/pgerrcode v0.0.0-20240316143900-6e2875d9b438 // indirect\n\tgithub.com/jackc/pgio v1.0.0 // indirect\n\tgithub.com/jackc/pgpassfile v1.0.0 // indirect\n\tgithub.com/jackc/pgproto3/v2 v2.3.3 // indirect\n\tgithub.com/jackc/pgservicefile v0.0.0-20221227161230-091c0ba34f0a // indirect\n\tgithub.com/jackc/pgtype v1.14.0 // indirect\n\tgithub.com/jackc/pgx/v5 v5.3.1 // indirect\n\tgithub.com/jackc/puddle v1.3.0 // indirect\n\tgithub.com/jonboulle/clockwork v0.2.2 // indirect\n\tgithub.com/json-iterator/go v1.1.11 // indirect\n\tgithub.com/juju/loggo v0.0.0-20210728185423-eebad3a902c4 // indirect\n\tgithub.com/juju/testing v0.0.0-20220203020004-a0ff61f03494 // indirect\n\tgithub.com/klauspost/compress v1.17.9\n\tgithub.com/lib/pq v1.10.9 // indirect\n\tgithub.com/lightninglabs/gozmq v0.0.0-20191113021534-d20a764486bf // indirect\n\tgithub.com/mattn/go-isatty v0.0.20 // indirect\n\tgithub.com/mattn/go-runewidth v0.0.13 // indirect\n\tgithub.com/matttproud/golang_protobuf_extensions v1.0.1 // indirect\n\tgithub.com/mitchellh/mapstructure v1.4.1 // indirect\n\tgithub.com/moby/term v0.5.0 // indirect\n\tgithub.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd // indirect\n\tgithub.com/modern-go/reflect2 v1.0.1 // indirect\n\tgithub.com/ncruces/go-strftime v0.1.9 // indirect\n\tgithub.com/nxadm/tail v1.4.8 // indirect\n\tgithub.com/onsi/gomega v1.26.0 // indirect\n\tgithub.com/opencontainers/go-digest v1.0.0 // indirect\n\tgithub.com/opencontainers/image-spec v1.0.2 // indirect\n\tgithub.com/opencontainers/runc v1.1.12 // indirect\n\tgithub.com/ory/dockertest/v3 v3.10.0 // indirect\n\tgithub.com/pmezard/go-difflib v1.0.0 // indirect\n\tgithub.com/prometheus/client_model v0.2.0 // indirect\n\tgithub.com/prometheus/common v0.26.0 // indirect\n\tgithub.com/prometheus/procfs v0.6.0 // indirect\n\tgithub.com/remyoudompheng/bigfft v0.0.0-20230129092748-24d4a6f8daec // indirect\n\tgithub.com/rivo/uniseg v0.2.0 // indirect\n\tgithub.com/rogpeppe/fastuuid v1.2.0 // indirect\n\tgithub.com/russross/blackfriday/v2 v2.0.1 // indirect\n\tgithub.com/shurcooL/sanitized_anchor_name v1.0.0 // indirect\n\tgithub.com/sirupsen/logrus v1.9.2 // indirect\n\tgithub.com/soheilhy/cmux v0.1.5 // indirect\n\tgithub.com/spf13/pflag v1.0.5 // indirect\n\tgithub.com/stretchr/objx v0.5.2 // indirect\n\tgithub.com/syndtr/goleveldb v1.0.1-0.20210819022825-2ae1ddf74ef7 // indirect\n\tgithub.com/tmc/grpc-websocket-proxy v0.0.0-20201229170055-e5319fda7802 // indirect\n\tgithub.com/xeipuuv/gojsonpointer v0.0.0-20180127040702-4e3ac2762d5f // indirect\n\tgithub.com/xeipuuv/gojsonreference v0.0.0-20180127040603-bd5ef7bd5415 // indirect\n\tgithub.com/xeipuuv/gojsonschema v1.2.0 // indirect\n\tgithub.com/xi2/xz v0.0.0-20171230120015-48954b6210f8 // indirect\n\tgithub.com/xiang90/probing v0.0.0-20190116061207-43a291ad63a2 // indirect\n\tgitlab.com/yawning/bsaes.git v0.0.0-20190805113838-0a714cd429ec // indirect\n\tgo.etcd.io/bbolt v1.3.11 // indirect\n\tgo.etcd.io/etcd/api/v3 v3.5.7 // indirect\n\tgo.etcd.io/etcd/client/v2 v2.305.7 // indirect\n\tgo.etcd.io/etcd/pkg/v3 v3.5.7 // indirect\n\tgo.etcd.io/etcd/raft/v3 v3.5.7 // indirect\n\tgo.etcd.io/etcd/server/v3 v3.5.7 // indirect\n\tgo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc v0.25.0 // indirect\n\tgo.opentelemetry.io/otel v1.0.1 // indirect\n\tgo.opentelemetry.io/otel/exporters/otlp/otlptrace v1.0.1 // indirect\n\tgo.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc v1.0.1 // indirect\n\tgo.opentelemetry.io/otel/sdk v1.0.1 // indirect\n\tgo.opentelemetry.io/otel/trace v1.0.1 // indirect\n\tgo.opentelemetry.io/proto/otlp v0.9.0 // indirect\n\tgo.uber.org/atomic v1.7.0 // indirect\n\tgo.uber.org/multierr v1.6.0 // indirect\n\tgo.uber.org/zap v1.17.0 // indirect\n\tgolang.org/x/mod v0.16.0 // indirect\n\tgolang.org/x/sys v0.19.0 // indirect\n\tgolang.org/x/text v0.14.0 // indirect\n\tgolang.org/x/tools v0.19.0 // indirect\n\tgoogle.golang.org/genproto v0.0.0-20231016165738-49dd2c1f3d0b // indirect\n\tgoogle.golang.org/genproto/googleapis/api v0.0.0-20231016165738-49dd2c1f3d0b // indirect\n\tgoogle.golang.org/genproto/googleapis/rpc v0.0.0-20231030173426-d783a09b4405 // indirect\n\tgopkg.in/errgo.v1 v1.0.1 // indirect\n\tgopkg.in/natefinch/lumberjack.v2 v2.0.0 // indirect\n\tgopkg.in/yaml.v2 v2.4.0 // indirect\n\tgopkg.in/yaml.v3 v3.0.1 // indirect\n\tmodernc.org/gc/v3 v3.0.0-20240107210532-573471604cb6 // indirect\n\tmodernc.org/libc v1.49.3 // indirect\n\tmodernc.org/mathutil v1.6.0 // indirect\n\tmodernc.org/memory v1.8.0 // indirect\n\tmodernc.org/sqlite v1.29.10 // indirect\n\tmodernc.org/strutil v1.2.0 // indirect\n\tmodernc.org/token v1.1.0 // indirect\n\tsigs.k8s.io/yaml v1.2.0 // indirect\n)\n\n// This replace is for https://github.com/advisories/GHSA-25xm-hr59-7c27\nreplace github.com/ulikunitz/xz => github.com/ulikunitz/xz v0.5.11\n\n// This replace is for\n// https://deps.dev/advisory/OSV/GO-2021-0053?from=%2Fgo%2Fgithub.com%252Fgogo%252Fprotobuf%2Fv1.3.1\nreplace github.com/gogo/protobuf => github.com/gogo/protobuf v1.3.2\n\n// We want to format raw bytes as hex instead of base64. The forked version\n// allows us to specify that as an option.\nreplace google.golang.org/protobuf => github.com/lightninglabs/protobuf-go-hex-display v1.30.0-hex-display\n\n// If you change this please also update docs/INSTALL.md and GO_VERSION in\n// Makefile (then run `make lint` to see where else it needs to be updated as\n// well).\ngo 1.22.6\n\nretract v0.0.2\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 103.6484375,
          "content": "cloud.google.com/go v0.26.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\ncloud.google.com/go v0.34.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\ncloud.google.com/go v0.38.0/go.mod h1:990N+gfupTy94rShfmMCWGDn0LpTmnzTp2qbd1dvSRU=\ncloud.google.com/go v0.44.1/go.mod h1:iSa0KzasP4Uvy3f1mN/7PiObzGgflwredwwASm/v6AU=\ncloud.google.com/go v0.44.2/go.mod h1:60680Gw3Yr4ikxnPRS/oxxkBccT6SA1yMk63TGekxKY=\ncloud.google.com/go v0.45.1/go.mod h1:RpBamKRgapWJb87xiFSdk4g1CME7QZg3uwTez+TSTjc=\ncloud.google.com/go v0.46.3/go.mod h1:a6bKKbmY7er1mI7TEI4lsAkts/mkhTSZK8w33B4RAg0=\ncloud.google.com/go v0.50.0/go.mod h1:r9sluTvynVuxRIOHXQEHMFffphuXHOMZMycpNR5e6To=\ncloud.google.com/go v0.52.0/go.mod h1:pXajvRH/6o3+F9jDHZWQ5PbGhn+o8w9qiu/CffaVdO4=\ncloud.google.com/go v0.53.0/go.mod h1:fp/UouUEsRkN6ryDKNW/Upv/JBKnv6WDthjR6+vze6M=\ncloud.google.com/go v0.54.0/go.mod h1:1rq2OEkV3YMf6n/9ZvGWI3GWw0VoqH/1x2nd8Is/bPc=\ncloud.google.com/go v0.56.0/go.mod h1:jr7tqZxxKOVYizybht9+26Z/gUq7tiRzu+ACVAMbKVk=\ncloud.google.com/go v0.57.0/go.mod h1:oXiQ6Rzq3RAkkY7N6t3TcE6jE+CIBBbA36lwQ1JyzZs=\ncloud.google.com/go v0.62.0/go.mod h1:jmCYTdRCQuc1PHIIJ/maLInMho30T/Y0M4hTdTShOYc=\ncloud.google.com/go v0.65.0/go.mod h1:O5N8zS7uWy9vkA9vayVHs65eM1ubvY4h553ofrNHObY=\ncloud.google.com/go v0.110.10 h1:LXy9GEO+timppncPIAZoOj3l58LIU9k+kn48AN7IO3Y=\ncloud.google.com/go/bigquery v1.0.1/go.mod h1:i/xbL2UlR5RvWAURpBYZTtm/cXjCha9lbfbpx4poX+o=\ncloud.google.com/go/bigquery v1.3.0/go.mod h1:PjpwJnslEMmckchkHFfq+HTD2DmtT67aNFKH1/VBDHE=\ncloud.google.com/go/bigquery v1.4.0/go.mod h1:S8dzgnTigyfTmLBfrtrhyYhwRxG72rYxvftPBK2Dvzc=\ncloud.google.com/go/bigquery v1.5.0/go.mod h1:snEHRnqQbz117VIFhE8bmtwIDY80NLUZUMb4Nv6dBIg=\ncloud.google.com/go/bigquery v1.7.0/go.mod h1://okPTzCYNXSlb24MZs83e2Do+h+VXtc4gLoIoXIAPc=\ncloud.google.com/go/bigquery v1.8.0/go.mod h1:J5hqkt3O0uAFnINi6JXValWIb1v0goeZM77hZzJN/fQ=\ncloud.google.com/go/compute v1.23.3 h1:6sVlXXBmbd7jNX0Ipq0trII3e4n1/MsADLK6a+aiVlk=\ncloud.google.com/go/compute v1.23.3/go.mod h1:VCgBUoMnIVIR0CscqQiPJLAG25E3ZRZMzcFZeQ+h8CI=\ncloud.google.com/go/compute/metadata v0.2.3 h1:mg4jlk7mCAj6xXp9UJ4fjI9VUI5rubuGBW5aJ7UnBMY=\ncloud.google.com/go/compute/metadata v0.2.3/go.mod h1:VAV5nSsACxMJvgaAuX6Pk2AawlZn8kiOGuCv6gTkwuA=\ncloud.google.com/go/datastore v1.0.0/go.mod h1:LXYbyblFSglQ5pkeyhO+Qmw7ukd3C+pD7TKLgZqpHYE=\ncloud.google.com/go/datastore v1.1.0/go.mod h1:umbIZjpQpHh4hmRpGhH4tLFup+FVzqBi1b3c64qFpCk=\ncloud.google.com/go/pubsub v1.0.1/go.mod h1:R0Gpsv3s54REJCy4fxDixWD93lHJMoZTyQ2kNxGRt3I=\ncloud.google.com/go/pubsub v1.1.0/go.mod h1:EwwdRX2sKPjnvnqCa270oGRyludottCI76h+R3AArQw=\ncloud.google.com/go/pubsub v1.2.0/go.mod h1:jhfEVHT8odbXTkndysNHCcx0awwzvfOlguIAii9o8iA=\ncloud.google.com/go/pubsub v1.3.1/go.mod h1:i+ucay31+CNRpDW4Lu78I4xXG+O1r/MAHgjpRVR+TSU=\ncloud.google.com/go/storage v1.0.0/go.mod h1:IhtSnM/ZTZV8YYJWCY8RULGVqBDmpoyjwiyrjsg+URw=\ncloud.google.com/go/storage v1.5.0/go.mod h1:tpKbwo567HUNpVclU5sGELwQWBDZ8gh0ZeosJ0Rtdos=\ncloud.google.com/go/storage v1.6.0/go.mod h1:N7U0C8pVQ/+NIKOBQyamJIeKQKkZ+mxpohlUTyfDhBk=\ncloud.google.com/go/storage v1.8.0/go.mod h1:Wv1Oy7z6Yz3DshWRJFhqM/UCfaWIRTdp0RXyy7KQOVs=\ncloud.google.com/go/storage v1.10.0/go.mod h1:FLPqc6j+Ki4BU591ie1oL6qBQGu2Bl/tZ9ullr3+Kg0=\ndmitri.shuralyov.com/gpu/mtl v0.0.0-20190408044501-666a987793e9/go.mod h1:H6x//7gZCb22OMCxBHrMx7a5I7Hp++hsVxbQ4BYO7hU=\ngithub.com/Azure/go-ansiterm v0.0.0-20230124172434-306776ec8161 h1:L/gRVlceqvL25UVaW/CKtUDjefjrs0SPonmDGUVOYP0=\ngithub.com/Azure/go-ansiterm v0.0.0-20230124172434-306776ec8161/go.mod h1:xomTg63KZ2rFqZQzSB4Vz2SUXa1BpHTVz9L5PTmPC4E=\ngithub.com/BurntSushi/toml v0.3.1 h1:WXkYYl6Yr3qBf1K79EBnL4mak0OimBfB0XUf9Vl28OQ=\ngithub.com/BurntSushi/toml v0.3.1/go.mod h1:xHWCNGjB5oqiDr8zfno3MHue2Ht5sIBksp03qcyfWMU=\ngithub.com/BurntSushi/xgb v0.0.0-20160522181843-27f122750802/go.mod h1:IVnqGOEym/WlBOVXweHU+Q+/VP0lqqI8lqeDx9IjBqo=\ngithub.com/Masterminds/semver/v3 v3.1.1 h1:hLg3sBzpNErnxhQtUy/mmLR2I9foDujNK030IGemrRc=\ngithub.com/Masterminds/semver/v3 v3.1.1/go.mod h1:VPu/7SZ7ePZ3QOrcuXROw5FAcLl4a0cBrbBpGY/8hQs=\ngithub.com/Microsoft/go-winio v0.6.1 h1:9/kr64B9VUZrLm5YYwbGtUJnMgqWVOdUAXu6Migciow=\ngithub.com/Microsoft/go-winio v0.6.1/go.mod h1:LRdKpFKfdobln8UmuiYcKPot9D2v6svN5+sAH+4kjUM=\ngithub.com/NebulousLabs/fastrand v0.0.0-20181203155948-6fb6489aac4e h1:n+DcnTNkQnHlwpsrHoQtkrJIO7CBx029fw6oR4vIob4=\ngithub.com/NebulousLabs/fastrand v0.0.0-20181203155948-6fb6489aac4e/go.mod h1:Bdzq+51GR4/0DIhaICZEOm+OHvXGwwB2trKZ8B4Y6eQ=\ngithub.com/NebulousLabs/go-upnp v0.0.0-20180202185039-29b680b06c82 h1:MG93+PZYs9PyEsj/n5/haQu2gK0h4tUtSy9ejtMwWa0=\ngithub.com/NebulousLabs/go-upnp v0.0.0-20180202185039-29b680b06c82/go.mod h1:GbuBk21JqF+driLX3XtJYNZjGa45YDoa9IqCTzNSfEc=\ngithub.com/Nvveen/Gotty v0.0.0-20120604004816-cd527374f1e5 h1:TngWCqHvy9oXAN6lEVMRuU21PR1EtLVZJmdB18Gu3Rw=\ngithub.com/Nvveen/Gotty v0.0.0-20120604004816-cd527374f1e5/go.mod h1:lmUJ/7eu/Q8D7ML55dXQrVaamCz2vxCfdQBasLZfHKk=\ngithub.com/OneOfOne/xxhash v1.2.2/go.mod h1:HSdplMjZKSmBqAxg5vPj2TmRDmfkzw+cTzAElWljhcU=\ngithub.com/Yawning/aez v0.0.0-20211027044916-e49e68abd344 h1:cDVUiFo+npB0ZASqnw4q90ylaVAbnYyx0JYqK4YcGok=\ngithub.com/Yawning/aez v0.0.0-20211027044916-e49e68abd344/go.mod h1:9pIqrY6SXNL8vjRQE5Hd/OL5GyK/9MrGUWs87z/eFfk=\ngithub.com/aead/chacha20 v0.0.0-20180709150244-8b13a72661da h1:KjTM2ks9d14ZYCvmHS9iAKVt9AyzRSqNU1qabPih5BY=\ngithub.com/aead/chacha20 v0.0.0-20180709150244-8b13a72661da/go.mod h1:eHEWzANqSiWQsof+nXEI9bUVUyV6F53Fp89EuCh2EAA=\ngithub.com/aead/siphash v1.0.1 h1:FwHfE/T45KPKYuuSAKyyvE+oPWcaQ+CUmFW0bPlM+kg=\ngithub.com/aead/siphash v1.0.1/go.mod h1:Nywa3cDsYNNK3gaciGTWPwHt0wlpNV15vwmswBAUSII=\ngithub.com/alecthomas/template v0.0.0-20160405071501-a0175ee3bccc/go.mod h1:LOuyumcjzFXgccqObfd/Ljyb9UuFJ6TxHnclSeseNhc=\ngithub.com/alecthomas/template v0.0.0-20190718012654-fb15b899a751/go.mod h1:LOuyumcjzFXgccqObfd/Ljyb9UuFJ6TxHnclSeseNhc=\ngithub.com/alecthomas/units v0.0.0-20151022065526-2efee857e7cf/go.mod h1:ybxpYRFXyAe+OPACYpWeL0wqObRcbAqCMya13uyzqw0=\ngithub.com/alecthomas/units v0.0.0-20190717042225-c3de453c63f4/go.mod h1:ybxpYRFXyAe+OPACYpWeL0wqObRcbAqCMya13uyzqw0=\ngithub.com/alecthomas/units v0.0.0-20190924025748-f65c72e2690d/go.mod h1:rBZYJk541a8SKzHPHnH3zbiI+7dagKZ0cgpgrD7Fyho=\ngithub.com/andybalholm/brotli v1.0.4 h1:V7DdXeJtZscaqfNuAdSRuRFzuiKlHSC/Zh3zl9qY3JY=\ngithub.com/andybalholm/brotli v1.0.4/go.mod h1:fO7iG3H7G2nSZ7m0zPUDn85XEX2GTukHGRSepvi9Eig=\ngithub.com/antihax/optional v1.0.0/go.mod h1:uupD/76wgC+ih3iEmQUL+0Ugr19nfwCT1kdvxnR2qWY=\ngithub.com/beorn7/perks v0.0.0-20180321164747-3a771d992973/go.mod h1:Dwedo/Wpr24TaqPxmxbtue+5NUziq4I4S80YR8gNf3Q=\ngithub.com/beorn7/perks v1.0.0/go.mod h1:KWe93zE9D1o94FZ5RNwFwVgaQK1VOXiVxmqh+CedLV8=\ngithub.com/beorn7/perks v1.0.1 h1:VlbKKnNfV8bJzeqoa4cOKqO6bYr3WgKZxO8Z16+hsOM=\ngithub.com/beorn7/perks v1.0.1/go.mod h1:G2ZrVWU2WbWT9wwq4/hrbKbnv/1ERSJQ0ibhJ6rlkpw=\ngithub.com/btcsuite/btcd v0.20.1-beta/go.mod h1:wVuoA8VJLEcwgqHBwHmzLRazpKxTv13Px/pDuV7OomQ=\ngithub.com/btcsuite/btcd v0.22.0-beta.0.20220111032746-97732e52810c/go.mod h1:tjmYdS6MLJ5/s0Fj4DbLgSbDHbEqLJrtnHecBFkdz5M=\ngithub.com/btcsuite/btcd v0.23.5-0.20231215221805-96c9fd8078fd/go.mod h1:nm3Bko6zh6bWP60UxwoT5LzdGJsQJaPo6HjduXq9p6A=\ngithub.com/btcsuite/btcd v0.24.3-0.20241210095828-e646d437e95b h1:VQoobSrWdxICuqFU3tKVu/Lzk7BTk9SsCgRr5dUvC70=\ngithub.com/btcsuite/btcd v0.24.3-0.20241210095828-e646d437e95b/go.mod h1:zHK7t7sw8XbsCkD64WePHE3r3k9/XoGAcf6mXV14c64=\ngithub.com/btcsuite/btcd/btcec/v2 v2.1.0/go.mod h1:2VzYrv4Gm4apmbVVsSq5bqf1Ec8v56E48Vt0Y/umPgA=\ngithub.com/btcsuite/btcd/btcec/v2 v2.1.3/go.mod h1:ctjw4H1kknNJmRN4iP1R7bTQ+v3GJkZBd6mui8ZsAZE=\ngithub.com/btcsuite/btcd/btcec/v2 v2.3.4 h1:3EJjcN70HCu/mwqlUsGK8GcNVyLVxFDlWurTXGPFfiQ=\ngithub.com/btcsuite/btcd/btcec/v2 v2.3.4/go.mod h1:zYzJ8etWJQIv1Ogk7OzpWjowwOdXY1W/17j2MW85J04=\ngithub.com/btcsuite/btcd/btcutil v1.0.0/go.mod h1:Uoxwv0pqYWhD//tfTiipkxNfdhG9UrLwaeswfjfdF0A=\ngithub.com/btcsuite/btcd/btcutil v1.1.0/go.mod h1:5OapHB7A2hBBWLm48mmw4MOHNJCcUBTwmWH/0Jn8VHE=\ngithub.com/btcsuite/btcd/btcutil v1.1.5 h1:+wER79R5670vs/ZusMTF1yTcRYE5GUsFbdjdisflzM8=\ngithub.com/btcsuite/btcd/btcutil v1.1.5/go.mod h1:PSZZ4UitpLBWzxGd5VGOrLnmOjtPP/a6HaFo12zMs00=\ngithub.com/btcsuite/btcd/btcutil/psbt v1.1.8 h1:4voqtT8UppT7nmKQkXV+T9K8UyQjKOn2z/ycpmJK8wg=\ngithub.com/btcsuite/btcd/btcutil/psbt v1.1.8/go.mod h1:kA6FLH/JfUx++j9pYU0pyu+Z8XGBQuuTmuKYUf6q7/U=\ngithub.com/btcsuite/btcd/chaincfg/chainhash v1.0.0/go.mod h1:7SFka0XMvUgj3hfZtydOrQY2mwhPclbT2snogU7SQQc=\ngithub.com/btcsuite/btcd/chaincfg/chainhash v1.0.1/go.mod h1:7SFka0XMvUgj3hfZtydOrQY2mwhPclbT2snogU7SQQc=\ngithub.com/btcsuite/btcd/chaincfg/chainhash v1.1.0 h1:59Kx4K6lzOW5w6nFlA0v5+lk/6sjybR934QNHSJZPTQ=\ngithub.com/btcsuite/btcd/chaincfg/chainhash v1.1.0/go.mod h1:7SFka0XMvUgj3hfZtydOrQY2mwhPclbT2snogU7SQQc=\ngithub.com/btcsuite/btclog v0.0.0-20170628155309-84c8d2346e9f/go.mod h1:TdznJufoqS23FtqVCzL0ZqgP5MqXbb4fg/WgDys70nA=\ngithub.com/btcsuite/btclog v0.0.0-20241003133417-09c4e92e319c h1:4HxD1lBUGUddhzgaNgrCPsFWd7cGYNpeFUgd9ZIgyM0=\ngithub.com/btcsuite/btclog v0.0.0-20241003133417-09c4e92e319c/go.mod h1:w7xnGOhwT3lmrS4H3b/D1XAXxvh+tbhUm8xeHN2y3TQ=\ngithub.com/btcsuite/btclog/v2 v2.0.1-0.20250110154127-3ae4bf1cb318 h1:oCjIcinPt7XQ644MP/22JcjYEC84qRc3bRBH0d7Hhd4=\ngithub.com/btcsuite/btclog/v2 v2.0.1-0.20250110154127-3ae4bf1cb318/go.mod h1:XItGUfVOxotJL8kkuk2Hj3EVow5KCugXl3wWfQ6K0AE=\ngithub.com/btcsuite/btcutil v0.0.0-20190425235716-9e5f4b9a998d/go.mod h1:+5NJ2+qvTyV9exUAL/rxXi3DcLg2Ts+ymUAY5y4NvMg=\ngithub.com/btcsuite/btcwallet v0.16.10-0.20241127094224-93c858b2ad63 h1:YN+PekOLlLoGxE3P5RJaGgodZD5DDJSU8eXQZVwwCxM=\ngithub.com/btcsuite/btcwallet v0.16.10-0.20241127094224-93c858b2ad63/go.mod h1:1HJXYbjJzgumlnxOC2+ViR1U+gnHWoOn7WeK5OfY1eU=\ngithub.com/btcsuite/btcwallet/wallet/txauthor v1.3.5 h1:Rr0njWI3r341nhSPesKQ2JF+ugDSzdPoeckS75SeDZk=\ngithub.com/btcsuite/btcwallet/wallet/txauthor v1.3.5/go.mod h1:+tXJ3Ym0nlQc/iHSwW1qzjmPs3ev+UVWMbGgfV1OZqU=\ngithub.com/btcsuite/btcwallet/wallet/txrules v1.2.2 h1:YEO+Lx1ZJJAtdRrjuhXjWrYsmAk26wLTlNzxt2q0lhk=\ngithub.com/btcsuite/btcwallet/wallet/txrules v1.2.2/go.mod h1:4v+grppsDpVn91SJv+mZT7B8hEV4nSmpREM4I8Uohws=\ngithub.com/btcsuite/btcwallet/wallet/txsizes v1.2.5 h1:93o5Xz9dYepBP4RMFUc9RGIFXwqP2volSWRkYJFrNtI=\ngithub.com/btcsuite/btcwallet/wallet/txsizes v1.2.5/go.mod h1:lQ+e9HxZ85QP7r3kdxItkiMSloSLg1PEGis5o5CXUQw=\ngithub.com/btcsuite/btcwallet/walletdb v1.4.4 h1:BDel6iT/ltYSIYKs0YbjwnEDi7xR3yzABIsQxN2F1L8=\ngithub.com/btcsuite/btcwallet/walletdb v1.4.4/go.mod h1:jk/hvpLFINF0C1kfTn0bfx2GbnFT+Nvnj6eblZALfjs=\ngithub.com/btcsuite/btcwallet/wtxmgr v1.5.4 h1:hJjHy1h/dJwSfD9uDsCwcH21D1iOrus6OrI5gR9E/O0=\ngithub.com/btcsuite/btcwallet/wtxmgr v1.5.4/go.mod h1:lAv0b1Vj9Ig5U8QFm0yiJ9WqPl8yGO/6l7JxdHY1PKE=\ngithub.com/btcsuite/go-socks v0.0.0-20170105172521-4720035b7bfd h1:R/opQEbFEy9JGkIguV40SvRY1uliPX8ifOvi6ICsFCw=\ngithub.com/btcsuite/go-socks v0.0.0-20170105172521-4720035b7bfd/go.mod h1:HHNXQzUsZCxOoE+CPiyCTO6x34Zs86zZUiwtpXoGdtg=\ngithub.com/btcsuite/golangcrypto v0.0.0-20150304025918-53f62d9b43e8/go.mod h1:tYvUd8KLhm/oXvUeSEs2VlLghFjQt9+ZaF9ghH0JNjc=\ngithub.com/btcsuite/goleveldb v0.0.0-20160330041536-7834afc9e8cd/go.mod h1:F+uVaaLLH7j4eDXPRvw78tMflu7Ie2bzYOH4Y8rRKBY=\ngithub.com/btcsuite/goleveldb v1.0.0/go.mod h1:QiK9vBlgftBg6rWQIj6wFzbPfRjiykIEhBH4obrXJ/I=\ngithub.com/btcsuite/snappy-go v0.0.0-20151229074030-0bdef8d06723/go.mod h1:8woku9dyThutzjeg+3xrA5iCpBRH8XEEg3lh6TiUghc=\ngithub.com/btcsuite/snappy-go v1.0.0/go.mod h1:8woku9dyThutzjeg+3xrA5iCpBRH8XEEg3lh6TiUghc=\ngithub.com/btcsuite/websocket v0.0.0-20150119174127-31079b680792 h1:R8vQdOQdZ9Y3SkEwmHoWBmX1DNXhXZqlTpq6s4tyJGc=\ngithub.com/btcsuite/websocket v0.0.0-20150119174127-31079b680792/go.mod h1:ghJtEyQwv5/p4Mg4C0fgbePVuGr935/5ddU9Z3TmDRY=\ngithub.com/btcsuite/winsvc v1.0.0 h1:J9B4L7e3oqhXOcm+2IuNApwzQec85lE+QaikUcCs+dk=\ngithub.com/btcsuite/winsvc v1.0.0/go.mod h1:jsenWakMcC0zFBFurPLEAyrnc/teJEM1O46fmI40EZs=\ngithub.com/cenkalti/backoff/v4 v4.1.1/go.mod h1:scbssz8iZGpm3xbr14ovlUdkxfGXNInqkPWOWmG2CLw=\ngithub.com/cenkalti/backoff/v4 v4.1.3 h1:cFAlzYUlVYDysBEH2T5hyJZMh3+5+WCBvSnK6Q8UtC4=\ngithub.com/cenkalti/backoff/v4 v4.1.3/go.mod h1:scbssz8iZGpm3xbr14ovlUdkxfGXNInqkPWOWmG2CLw=\ngithub.com/census-instrumentation/opencensus-proto v0.2.1/go.mod h1:f6KPmirojxKA12rnyqOA5BBL4O983OfeGPqjHWSTneU=\ngithub.com/certifi/gocertifi v0.0.0-20200922220541-2c3bb06c6054 h1:uH66TXeswKn5PW5zdZ39xEwfS9an067BirqA+P4QaLI=\ngithub.com/certifi/gocertifi v0.0.0-20200922220541-2c3bb06c6054/go.mod h1:sGbDF6GwGcLpkNXPUTkMRoywsNa/ol15pxFe6ERfguA=\ngithub.com/cespare/xxhash v1.1.0/go.mod h1:XrSqR1VqqWfGrhpAt58auRo0WTKS1nRRg3ghfAqPWnc=\ngithub.com/cespare/xxhash/v2 v2.1.1/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=\ngithub.com/cespare/xxhash/v2 v2.2.0 h1:DC2CZ1Ep5Y4k3ZQ899DldepgrayRUGE6BBZ/cd9Cj44=\ngithub.com/cespare/xxhash/v2 v2.2.0/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=\ngithub.com/chzyer/logex v1.1.10/go.mod h1:+Ywpsq7O8HXn0nuIou7OrIPyXbp3wmkHB+jjWRnGsAI=\ngithub.com/chzyer/readline v0.0.0-20180603132655-2972be24d48e/go.mod h1:nSuG5e5PlCu98SY8svDHJxuZscDgtXS6KTTbou5AhLI=\ngithub.com/chzyer/test v0.0.0-20180213035817-a1ea475d72b1/go.mod h1:Q3SI9o4m/ZMnBNeIyt5eFwwo7qiLfzFZmjNmxjkiQlU=\ngithub.com/client9/misspell v0.3.4/go.mod h1:qj6jICC3Q7zFZvVWo7KLAzC3yx5G7kyvSDkc90ppPyw=\ngithub.com/cncf/udpa/go v0.0.0-20191209042840-269d4d468f6f/go.mod h1:M8M6+tZqaGXZJjfX53e64911xZQV5JYwmTeXPW+k8Sc=\ngithub.com/cncf/udpa/go v0.0.0-20201120205902-5459f2c99403/go.mod h1:WmhPx2Nbnhtbo57+VJT5O0JRkEi1Wbu0z5j0R8u5Hbk=\ngithub.com/cncf/xds/go v0.0.0-20210312221358-fbca930ec8ed/go.mod h1:eXthEFrGJvWHgFFCl3hGmgk+/aYT6PnTQLykKQRLhEs=\ngithub.com/cncf/xds/go v0.0.0-20210805033703-aa0b78936158/go.mod h1:eXthEFrGJvWHgFFCl3hGmgk+/aYT6PnTQLykKQRLhEs=\ngithub.com/cncf/xds/go v0.0.0-20231109132714-523115ebc101 h1:7To3pQ+pZo0i3dsWEbinPNFs5gPSBOsJtx3wTT94VBY=\ngithub.com/cncf/xds/go v0.0.0-20231109132714-523115ebc101/go.mod h1:eXthEFrGJvWHgFFCl3hGmgk+/aYT6PnTQLykKQRLhEs=\ngithub.com/cockroachdb/apd v1.1.0 h1:3LFP3629v+1aKXU5Q37mxmRxX/pIu1nijXydLShEq5I=\ngithub.com/cockroachdb/apd v1.1.0/go.mod h1:8Sl8LxpKi29FqWXR16WEFZRNSz3SoPzUzeMeY4+DwBQ=\ngithub.com/cockroachdb/datadriven v0.0.0-20200714090401-bf6692d28da5 h1:xD/lrqdvwsc+O2bjSSi3YqY73Ke3LAiSCx49aCesA0E=\ngithub.com/cockroachdb/datadriven v0.0.0-20200714090401-bf6692d28da5/go.mod h1:h6jFvWxBdQXxjopDMZyH2UVceIRfR84bdzbkoKrsWNo=\ngithub.com/cockroachdb/errors v1.2.4 h1:Lap807SXTH5tri2TivECb/4abUkMZC9zRoLarvcKDqs=\ngithub.com/cockroachdb/errors v1.2.4/go.mod h1:rQD95gz6FARkaKkQXUksEje/d9a6wBJoCr5oaCLELYA=\ngithub.com/cockroachdb/logtags v0.0.0-20190617123548-eb05cc24525f h1:o/kfcElHqOiXqcou5a3rIlMc7oJbMQkeLk0VQJ7zgqY=\ngithub.com/cockroachdb/logtags v0.0.0-20190617123548-eb05cc24525f/go.mod h1:i/u985jwjWRlyHXQbwatDASoW0RMlZ/3i9yJHE2xLkI=\ngithub.com/containerd/continuity v0.3.0 h1:nisirsYROK15TAMVukJOUyGJjz4BNQJBVsNvAXZJ/eg=\ngithub.com/containerd/continuity v0.3.0/go.mod h1:wJEAIwKOm/pBZuBd0JmeTvnLquTB1Ag8espWhkykbPM=\ngithub.com/coreos/go-semver v0.3.0 h1:wkHLiw0WNATZnSG7epLsujiMCgPAc9xhjJ4tgnAxmfM=\ngithub.com/coreos/go-semver v0.3.0/go.mod h1:nnelYz7RCh+5ahJtPPxZlU+153eP4D4r3EedlOD2RNk=\ngithub.com/coreos/go-systemd v0.0.0-20190321100706-95778dfbb74e/go.mod h1:F5haX7vjVVG0kc13fIWeqUViNPyEJxv/OmvnBo0Yme4=\ngithub.com/coreos/go-systemd v0.0.0-20190719114852-fd7a80b32e1f h1:JOrtw2xFKzlg+cbHpyrpLDmnN1HqhBfnX7WDiW7eG2c=\ngithub.com/coreos/go-systemd v0.0.0-20190719114852-fd7a80b32e1f/go.mod h1:F5haX7vjVVG0kc13fIWeqUViNPyEJxv/OmvnBo0Yme4=\ngithub.com/coreos/go-systemd/v22 v22.3.2 h1:D9/bQk5vlXQFZ6Kwuu6zaiXJ9oTPe68++AzAJc1DzSI=\ngithub.com/coreos/go-systemd/v22 v22.3.2/go.mod h1:Y58oyj3AT4RCenI/lSvhwexgC+NSVTIJ3seZv2GcEnc=\ngithub.com/cpuguy83/go-md2man/v2 v2.0.0-20190314233015-f79a8a8ca69d/go.mod h1:maD7wRr/U5Z6m/iR4s+kqSMx2CaBsrgA7czyZG/E6dU=\ngithub.com/cpuguy83/go-md2man/v2 v2.0.0 h1:EoUDS0afbrsXAZ9YQ9jdu/mZ2sXgT1/2yyNng4PGlyM=\ngithub.com/cpuguy83/go-md2man/v2 v2.0.0/go.mod h1:maD7wRr/U5Z6m/iR4s+kqSMx2CaBsrgA7czyZG/E6dU=\ngithub.com/creack/pty v1.1.7/go.mod h1:lj5s0c3V2DBrqTV7llrYr5NG6My20zk30Fl46Y7DoTY=\ngithub.com/creack/pty v1.1.18 h1:n56/Zwd5o6whRC5PMGretI4IdRLlmBXYNjScPaBgsbY=\ngithub.com/creack/pty v1.1.18/go.mod h1:MOBLtS5ELjhRRrroQr9kyvTxUAFNvYEK993ew/Vr4O4=\ngithub.com/davecgh/go-spew v0.0.0-20171005155431-ecdeabc65495/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/decred/dcrd/crypto/blake256 v1.0.0/go.mod h1:sQl2p6Y26YV+ZOcSTP6thNdn47hh8kt6rqSlvmrXFAc=\ngithub.com/decred/dcrd/crypto/blake256 v1.0.1 h1:7PltbUIQB7u/FfZ39+DGa/ShuMyJ5ilcvdfma9wOH6Y=\ngithub.com/decred/dcrd/crypto/blake256 v1.0.1/go.mod h1:2OfgNZ5wDpcsFmHmCK5gZTPcCXqlm2ArzUIkw9czNJo=\ngithub.com/decred/dcrd/dcrec/secp256k1/v4 v4.0.1/go.mod h1:hyedUtir6IdtD/7lIxGeCxkaw7y45JueMRL4DIyJDKs=\ngithub.com/decred/dcrd/dcrec/secp256k1/v4 v4.3.0 h1:rpfIENRNNilwHwZeG5+P150SMrnNEcHYvcCuK6dPZSg=\ngithub.com/decred/dcrd/dcrec/secp256k1/v4 v4.3.0/go.mod h1:v57UDF4pDQJcEfFUCRop3lJL149eHGSe9Jvczhzjo/0=\ngithub.com/decred/dcrd/lru v1.0.0/go.mod h1:mxKOwFd7lFjN2GZYsiz/ecgqR6kkYAl+0pz0tEMk218=\ngithub.com/decred/dcrd/lru v1.1.2 h1:KdCzlkxppuoIDGEvCGah1fZRicrDH36IipvlB1ROkFY=\ngithub.com/decred/dcrd/lru v1.1.2/go.mod h1:gEdCVgXs1/YoBvFWt7Scgknbhwik3FgVSzlnCcXL2N8=\ngithub.com/dhui/dktest v0.4.0 h1:z05UmuXZHO/bgj/ds2bGMBu8FI4WA+Ag/m3ghL+om7M=\ngithub.com/dhui/dktest v0.4.0/go.mod h1:v/Dbz1LgCBOi2Uki2nUqLBGa83hWBGFMu5MrgMDCc78=\ngithub.com/docker/cli v20.10.17+incompatible h1:eO2KS7ZFeov5UJeaDmIs1NFEDRf32PaqRpvoEkKBy5M=\ngithub.com/docker/cli v20.10.17+incompatible/go.mod h1:JLrzqnKDaYBop7H2jaqPtU4hHvMKP+vjCwu2uszcLI8=\ngithub.com/docker/distribution v2.8.2+incompatible h1:T3de5rq0dB1j30rp0sA2rER+m322EBzniBPB6ZIzuh8=\ngithub.com/docker/distribution v2.8.2+incompatible/go.mod h1:J2gT2udsDAN96Uj4KfcMRqY0/ypR+oyYUYmja8H+y+w=\ngithub.com/docker/docker v24.0.7+incompatible h1:Wo6l37AuwP3JaMnZa226lzVXGA3F9Ig1seQen0cKYlM=\ngithub.com/docker/docker v24.0.7+incompatible/go.mod h1:eEKB0N0r5NX/I1kEveEz05bcu8tLC/8azJZsviup8Sk=\ngithub.com/docker/go-connections v0.4.0 h1:El9xVISelRB7BuFusrZozjnkIM5YnzCViNKohAFqRJQ=\ngithub.com/docker/go-connections v0.4.0/go.mod h1:Gbd7IOopHjR8Iph03tsViu4nIes5XhDvyHbTtUxmeec=\ngithub.com/docker/go-units v0.5.0 h1:69rxXcBk27SvSaaxTtLh/8llcHD8vYHT7WSdRZ/jvr4=\ngithub.com/docker/go-units v0.5.0/go.mod h1:fgPhTUdO+D/Jk86RDLlptpiXQzgHJF7gydDDbaIK4Dk=\ngithub.com/dustin/go-humanize v1.0.1 h1:GzkhY7T5VNhEkwH0PVJgjz+fX1rhBrR7pRT3mDkpeCY=\ngithub.com/dustin/go-humanize v1.0.1/go.mod h1:Mu1zIs6XwVuF/gI1OepvI0qD18qycQx+mFykh5fBlto=\ngithub.com/envoyproxy/go-control-plane v0.9.0/go.mod h1:YTl/9mNaCwkRvm6d1a2C3ymFceY/DCBVvsKhRF0iEA4=\ngithub.com/envoyproxy/go-control-plane v0.9.1-0.20191026205805-5f8ba28d4473/go.mod h1:YTl/9mNaCwkRvm6d1a2C3ymFceY/DCBVvsKhRF0iEA4=\ngithub.com/envoyproxy/go-control-plane v0.9.4/go.mod h1:6rpuAdCZL397s3pYoYcLgu1mIlRU8Am5FuJP05cCM98=\ngithub.com/envoyproxy/go-control-plane v0.9.9-0.20201210154907-fd9021fe5dad/go.mod h1:cXg6YxExXjJnVBQHBLXeUAgxn2UodCpnH306RInaBQk=\ngithub.com/envoyproxy/go-control-plane v0.9.9-0.20210217033140-668b12f5399d/go.mod h1:cXg6YxExXjJnVBQHBLXeUAgxn2UodCpnH306RInaBQk=\ngithub.com/envoyproxy/go-control-plane v0.9.9-0.20210512163311-63b5d3c536b0/go.mod h1:hliV/p42l8fGbc6Y9bQ70uLwIvmJyVE5k4iMKlh8wCQ=\ngithub.com/envoyproxy/go-control-plane v0.9.10-0.20210907150352-cf90f659a021/go.mod h1:AFq3mo9L8Lqqiid3OhADV3RfLJnjiw63cSpi+fDTRC0=\ngithub.com/envoyproxy/protoc-gen-validate v0.1.0/go.mod h1:iSmxcyjqTsJpI2R4NaDN7+kN2VEUnK/pcBlmesArF7c=\ngithub.com/envoyproxy/protoc-gen-validate v1.0.2 h1:QkIBuU5k+x7/QXPvPPnWXWlCdaBFApVqftFV6k087DA=\ngithub.com/envoyproxy/protoc-gen-validate v1.0.2/go.mod h1:GpiZQP3dDbg4JouG/NNS7QWXpgx6x8QiMKdmN72jogE=\ngithub.com/fergusstrange/embedded-postgres v1.25.0 h1:sa+k2Ycrtz40eCRPOzI7Ry7TtkWXXJ+YRsxpKMDhxK0=\ngithub.com/fergusstrange/embedded-postgres v1.25.0/go.mod h1:t/MLs0h9ukYM6FSt99R7InCHs1nW0ordoVCcnzmpTYw=\ngithub.com/frankban/quicktest v1.2.2 h1:xfmOhhoH5fGPgbEAlhLpJH9p0z/0Qizio9osmvn9IUY=\ngithub.com/frankban/quicktest v1.2.2/go.mod h1:Qh/WofXFeiAFII1aEBu529AtJo6Zg2VHscnEsbBnJ20=\ngithub.com/fsnotify/fsnotify v1.4.7/go.mod h1:jwhsz4b93w/PPRr/qN1Yymfu8t87LnFCMoQvtojpjFo=\ngithub.com/fsnotify/fsnotify v1.4.9/go.mod h1:znqG4EE+3YCdAaPaxE2ZRY/06pZUdp0tY4IgpuI1SZQ=\ngithub.com/fsnotify/fsnotify v1.5.4 h1:jRbGcIw6P2Meqdwuo0H1p6JVLbL5DHKAKlYndzMwVZI=\ngithub.com/fsnotify/fsnotify v1.5.4/go.mod h1:OVB6XrOHzAwXMpEM7uPOzcehqUV2UqJxmVXmkdnm1bU=\ngithub.com/getsentry/raven-go v0.2.0 h1:no+xWJRb5ZI7eE8TWgIq1jLulQiIoLG0IfYxv5JYMGs=\ngithub.com/getsentry/raven-go v0.2.0/go.mod h1:KungGk8q33+aIAZUIVWZDr2OfAEBsO49PX4NzFV5kcQ=\ngithub.com/ghodss/yaml v1.0.0/go.mod h1:4dBDuWmgqj2HViK6kFavaiC9ZROes6MMH2rRYeMEF04=\ngithub.com/go-errors/errors v1.0.1 h1:LUHzmkK3GUKUrL/1gfBUxAHzcev3apQlezX/+O7ma6w=\ngithub.com/go-errors/errors v1.0.1/go.mod h1:f4zRHt4oKfwPJE5k8C9vpYG+aDHdBFUsgrm6/TyX73Q=\ngithub.com/go-gl/glfw v0.0.0-20190409004039-e6da0acd62b1/go.mod h1:vR7hzQXu2zJy9AVAgeJqvqgH9Q5CA+iKCZ2gyEVpxRU=\ngithub.com/go-gl/glfw/v3.3/glfw v0.0.0-20191125211704-12ad95a8df72/go.mod h1:tQ2UAYgL5IevRw8kRxooKSPJfGvJ9fJQFa0TUsXzTg8=\ngithub.com/go-gl/glfw/v3.3/glfw v0.0.0-20200222043503-6f7a984d4dc4/go.mod h1:tQ2UAYgL5IevRw8kRxooKSPJfGvJ9fJQFa0TUsXzTg8=\ngithub.com/go-kit/kit v0.8.0/go.mod h1:xBxKIO96dXMWWy0MnWVtmwkA9/13aqxPnvrjFYMA2as=\ngithub.com/go-kit/kit v0.9.0/go.mod h1:xBxKIO96dXMWWy0MnWVtmwkA9/13aqxPnvrjFYMA2as=\ngithub.com/go-kit/log v0.1.0/go.mod h1:zbhenjAZHb184qTLMA9ZjW7ThYL0H2mk7Q6pNt4vbaY=\ngithub.com/go-logfmt/logfmt v0.3.0/go.mod h1:Qt1PoO58o5twSAckw1HlFXLmHsOX5/0LbT9GBnD5lWE=\ngithub.com/go-logfmt/logfmt v0.4.0/go.mod h1:3RMwSq7FuexP4Kalkev3ejPJsZTpXXBr9+V4qmtdjCk=\ngithub.com/go-logfmt/logfmt v0.5.0/go.mod h1:wCYkCAKZfumFQihp8CzCvQ3paCTfi41vtzG1KdI/P7A=\ngithub.com/go-sql-driver/mysql v1.6.0 h1:BCTh4TKNUYmOmMUcQ3IipzF5prigylS7XXjEkfCHuOE=\ngithub.com/go-sql-driver/mysql v1.6.0/go.mod h1:DCzpHaOWr8IXmIStZouvnhqoel9Qv2LBy8hT2VhHyBg=\ngithub.com/go-stack/stack v1.8.0/go.mod h1:v0f6uXyyMGvRgIKkXu+yp6POWl0qKG85gN/melR3HDY=\ngithub.com/godbus/dbus/v5 v5.0.4/go.mod h1:xhWf0FNVPg57R7Z0UbKHbJfkEywrmjJnf7w5xrFpKfA=\ngithub.com/gofrs/uuid v4.0.0+incompatible h1:1SD/1F5pU8p29ybwgQSwpQk+mwdRrXCYuPhW6m+TnJw=\ngithub.com/gofrs/uuid v4.0.0+incompatible/go.mod h1:b2aQJv3Z4Fp6yNu3cdSllBxTCLRxnplIgP/c0N/04lM=\ngithub.com/gogo/protobuf v1.3.2 h1:Ov1cvc58UF3b5XjBnZv7+opcTcQFZebYjWzi34vdm4Q=\ngithub.com/gogo/protobuf v1.3.2/go.mod h1:P1XiOD3dCwIKUDQYPy72D8LYyHL2YPYrpS2s69NZV8Q=\ngithub.com/golang-jwt/jwt/v4 v4.4.2 h1:rcc4lwaZgFMCZ5jxF9ABolDcIHdBytAFgqFPbSJQAYs=\ngithub.com/golang-jwt/jwt/v4 v4.4.2/go.mod h1:m21LjoU+eqJr34lmDMbreY2eSTRJ1cv77w39/MY0Ch0=\ngithub.com/golang-migrate/migrate/v4 v4.17.0 h1:rd40H3QXU0AA4IoLllFcEAEo9dYKRHYND2gB4p7xcaU=\ngithub.com/golang-migrate/migrate/v4 v4.17.0/go.mod h1:+Cp2mtLP4/aXDTKb9wmXYitdrNx2HGs45rbWAo6OsKM=\ngithub.com/golang/glog v0.0.0-20160126235308-23def4e6c14b/go.mod h1:SBH7ygxi8pfUlaOkMMuAQtPIUF8ecWP5IEl/CR7VP2Q=\ngithub.com/golang/glog v0.0.0-20210429001901-424d2337a529/go.mod h1:SBH7ygxi8pfUlaOkMMuAQtPIUF8ecWP5IEl/CR7VP2Q=\ngithub.com/golang/glog v1.1.2 h1:DVjP2PbBOzHyzA+dn3WhHIq4NdVu3Q+pvivFICf/7fo=\ngithub.com/golang/glog v1.1.2/go.mod h1:zR+okUeTbrL6EL3xHUDxZuEtGv04p5shwip1+mL/rLQ=\ngithub.com/golang/groupcache v0.0.0-20190702054246-869f871628b6/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/groupcache v0.0.0-20191227052852-215e87163ea7/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/groupcache v0.0.0-20200121045136-8c9f03a8e57e/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/mock v1.1.1/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=\ngithub.com/golang/mock v1.2.0/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=\ngithub.com/golang/mock v1.3.1/go.mod h1:sBzyDLLjw3U8JLTeZvSv8jJB+tU5PVekmnlKIyFUx0Y=\ngithub.com/golang/mock v1.4.0/go.mod h1:UOMv5ysSaYNkG+OFQykRIcU/QvvxJf3p21QfJ2Bt3cw=\ngithub.com/golang/mock v1.4.1/go.mod h1:UOMv5ysSaYNkG+OFQykRIcU/QvvxJf3p21QfJ2Bt3cw=\ngithub.com/golang/mock v1.4.3/go.mod h1:UOMv5ysSaYNkG+OFQykRIcU/QvvxJf3p21QfJ2Bt3cw=\ngithub.com/golang/mock v1.4.4/go.mod h1:l3mdAwkq5BuhzHwde/uurv3sEJeZMXNpwsxVWU71h+4=\ngithub.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.1/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.2/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.3/go.mod h1:vzj43D7+SQXF/4pzW/hwtAqwc6iTitCiVSaWz5lYuqw=\ngithub.com/golang/protobuf v1.3.4/go.mod h1:vzj43D7+SQXF/4pzW/hwtAqwc6iTitCiVSaWz5lYuqw=\ngithub.com/golang/protobuf v1.3.5/go.mod h1:6O5/vntMXwX2lRkT1hjjk0nAC1IDOTvTlVgjlRvqsdk=\ngithub.com/golang/protobuf v1.4.0/go.mod h1:jodUvKwWbYaEsadDk5Fwe5c77LiNKVO9IDvqG2KuDX0=\ngithub.com/golang/protobuf v1.4.1/go.mod h1:U8fpvMrcmy5pZrNK1lt4xCsGvpyWQ/VVv6QDs8UjoX8=\ngithub.com/golang/protobuf v1.4.2/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=\ngithub.com/golang/protobuf v1.4.3/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=\ngithub.com/golang/protobuf v1.5.0/go.mod h1:FsONVRAS9T7sI+LIUmWTfcYkHO4aIWwzhcaSAoJOfIk=\ngithub.com/golang/protobuf v1.5.2/go.mod h1:XVQd3VNwM+JqD3oG2Ue2ip4fOMUkwXdXDdiuN0vRsmY=\ngithub.com/golang/protobuf v1.5.3 h1:KhyjKVUg7Usr/dYsdSqoFveMYd5ko72D+zANwlG1mmg=\ngithub.com/golang/protobuf v1.5.3/go.mod h1:XVQd3VNwM+JqD3oG2Ue2ip4fOMUkwXdXDdiuN0vRsmY=\ngithub.com/golang/snappy v0.0.4 h1:yAGX7huGHXlcLOEtBnF4w7FQwA26wojNCwOYAEhLjQM=\ngithub.com/golang/snappy v0.0.4/go.mod h1:/XxbfmMg8lxefKM7IXC3fBNl/7bRcc72aCRzEWrmP2Q=\ngithub.com/google/btree v0.0.0-20180813153112-4030bb1f1f0c/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=\ngithub.com/google/btree v1.0.0/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=\ngithub.com/google/btree v1.0.1 h1:gK4Kx5IaGY9CD5sPJ36FHiBJ6ZXl0kilRiiCj+jdYp4=\ngithub.com/google/btree v1.0.1/go.mod h1:xXMiIv4Fb/0kKde4SpL7qlzvu5cMJDRkFDxJfI9uaxA=\ngithub.com/google/go-cmp v0.2.0/go.mod h1:oXzfMopK8JAjlY9xF4vHSVASa0yLyX7SntLO5aqRK0M=\ngithub.com/google/go-cmp v0.2.1-0.20190312032427-6f77996f0c42/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.3.1/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.4.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.4.1/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.1/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.4/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.5/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.6/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.6.0 h1:ofyhxvXcZhMsU5ulbFiLKl/XBFqE1GSq7atu8tAmTRI=\ngithub.com/google/go-cmp v0.6.0/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=\ngithub.com/google/gofuzz v1.0.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=\ngithub.com/google/martian v2.1.0+incompatible/go.mod h1:9I4somxYTbIHy5NJKHRl3wXiIaQGbYVAs8BPL6v8lEs=\ngithub.com/google/martian/v3 v3.0.0/go.mod h1:y5Zk1BBys9G+gd6Jrk0W3cC1+ELVxBWuIGO+w/tUAp0=\ngithub.com/google/pprof v0.0.0-20181206194817-3ea8567a2e57/go.mod h1:zfwlbNMJ+OItoe0UupaVj+oy1omPYYDuagoSzA8v9mc=\ngithub.com/google/pprof v0.0.0-20190515194954-54271f7e092f/go.mod h1:zfwlbNMJ+OItoe0UupaVj+oy1omPYYDuagoSzA8v9mc=\ngithub.com/google/pprof v0.0.0-20191218002539-d4f498aebedc/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\ngithub.com/google/pprof v0.0.0-20200212024743-f11f1df84d12/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\ngithub.com/google/pprof v0.0.0-20200229191704-1ebb73c60ed3/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\ngithub.com/google/pprof v0.0.0-20200430221834-fc25d7d30c6d/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\ngithub.com/google/pprof v0.0.0-20200708004538-1a94d8640e99/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\ngithub.com/google/pprof v0.0.0-20240409012703-83162a5b38cd h1:gbpYu9NMq8jhDVbvlGkMFWCjLFlqqEZjEmObmhUy6Vo=\ngithub.com/google/pprof v0.0.0-20240409012703-83162a5b38cd/go.mod h1:kf6iHlnVGwgKolg33glAes7Yg/8iWP8ukqeldJSO7jw=\ngithub.com/google/renameio v0.1.0/go.mod h1:KWCgfxg9yswjAJkECMjeO8J8rahYeXnNhOm40UhjYkI=\ngithub.com/google/shlex v0.0.0-20191202100458-e7afc7fbc510 h1:El6M4kTTCOh6aBiKaUGG7oYTSPP8MxqL4YI3kZKwcP4=\ngithub.com/google/shlex v0.0.0-20191202100458-e7afc7fbc510/go.mod h1:pupxD2MaaD3pAXIBCelhxNneeOaAeabZDe5s4K6zSpQ=\ngithub.com/google/uuid v1.1.2/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/google/uuid v1.6.0 h1:NIvaJDMOsjHA8n1jAhLSgzrAzy1Hgr+hNrb57e+94F0=\ngithub.com/google/uuid v1.6.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/googleapis/gax-go/v2 v2.0.4/go.mod h1:0Wqv26UfaUD9n4G6kQubkQ+KchISgw+vpHVxEJEs9eg=\ngithub.com/googleapis/gax-go/v2 v2.0.5/go.mod h1:DWXyrwAJ9X0FpwwEdw+IPEYBICEFu5mhpdKc/us6bOk=\ngithub.com/gorilla/websocket v1.5.0 h1:PPwGk2jz7EePpoHN/+ClbZu8SPxiqlu12wZP/3sWmnc=\ngithub.com/gorilla/websocket v1.5.0/go.mod h1:YR8l580nyteQvAITg2hZ9XVh4b55+EU/adAjf1fMHhE=\ngithub.com/grpc-ecosystem/go-grpc-middleware v1.3.0 h1:+9834+KizmvFV7pXQGSXQTsaWhq2GjuNUt0aUU0YBYw=\ngithub.com/grpc-ecosystem/go-grpc-middleware v1.3.0/go.mod h1:z0ButlSOZa5vEBq9m2m2hlwIgKw+rp3sdCBRoJY+30Y=\ngithub.com/grpc-ecosystem/go-grpc-prometheus v1.2.0 h1:Ovs26xHkKqVztRpIrF/92BcuyuQ/YW4NSIpoGtfXNho=\ngithub.com/grpc-ecosystem/go-grpc-prometheus v1.2.0/go.mod h1:8NvIoxWQoOIhqOTXgfV/d3M/q6VIi02HzZEHgUlZvzk=\ngithub.com/grpc-ecosystem/grpc-gateway v1.16.0 h1:gmcG1KaJ57LophUzW0Hy8NmPhnMZb4M0+kPpLofRdBo=\ngithub.com/grpc-ecosystem/grpc-gateway v1.16.0/go.mod h1:BDjrQk3hbvj6Nolgz8mAMFbcEtjT1g+wF4CSlocrBnw=\ngithub.com/grpc-ecosystem/grpc-gateway/v2 v2.5.0 h1:ajue7SzQMywqRjg2fK7dcpc0QhFGpTR2plWfV4EZWR4=\ngithub.com/grpc-ecosystem/grpc-gateway/v2 v2.5.0/go.mod h1:r1hZAcvfFXuYmcKyCJI9wlyOPIZUJl6FCB8Cpca/NLE=\ngithub.com/hashicorp/errwrap v1.0.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=\ngithub.com/hashicorp/errwrap v1.1.0 h1:OxrOeh75EUXMY8TBjag2fzXGZ40LB6IKw45YeGUDY2I=\ngithub.com/hashicorp/errwrap v1.1.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=\ngithub.com/hashicorp/go-multierror v1.1.1 h1:H5DkEtf6CXdFp0N0Em5UCwQpXMWke8IA0+lD48awMYo=\ngithub.com/hashicorp/go-multierror v1.1.1/go.mod h1:iw975J/qwKPdAO1clOe2L8331t/9/fmwbPZ6JB6eMoM=\ngithub.com/hashicorp/golang-lru v0.5.0/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\ngithub.com/hashicorp/golang-lru v0.5.1/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\ngithub.com/hashicorp/golang-lru/v2 v2.0.7 h1:a+bsQ5rvGLjzHuww6tVxozPZFVghXaHOwFs4luLUK2k=\ngithub.com/hashicorp/golang-lru/v2 v2.0.7/go.mod h1:QeFd9opnmA6QUJc5vARoKUSoFhyfM2/ZepoAG6RGpeM=\ngithub.com/hpcloud/tail v1.0.0/go.mod h1:ab1qPbhIpdTxEkNHXyeSf5vhxWSCs/tWer42PpOxQnU=\ngithub.com/ianlancetaylor/demangle v0.0.0-20181102032728-5e5cf60278f6/go.mod h1:aSSvb/t6k1mPoxDqO4vJh6VOCGPwU4O0C2/Eqndh1Sc=\ngithub.com/imdario/mergo v0.3.12 h1:b6R2BslTbIEToALKP7LxUvijTsNI9TAe80pLWN2g/HU=\ngithub.com/imdario/mergo v0.3.12/go.mod h1:jmQim1M+e3UYxmgPu/WyfjB3N3VflVyUjjjwH0dnCYA=\ngithub.com/jackc/chunkreader v1.0.0/go.mod h1:RT6O25fNZIuasFJRyZ4R/Y2BbhasbmZXF9QQ7T3kePo=\ngithub.com/jackc/chunkreader/v2 v2.0.0/go.mod h1:odVSm741yZoC3dpHEUXIqA9tQRhFrgOHwnPIn9lDKlk=\ngithub.com/jackc/chunkreader/v2 v2.0.1 h1:i+RDz65UE+mmpjTfyz0MoVTnzeYxroil2G82ki7MGG8=\ngithub.com/jackc/chunkreader/v2 v2.0.1/go.mod h1:odVSm741yZoC3dpHEUXIqA9tQRhFrgOHwnPIn9lDKlk=\ngithub.com/jackc/pgconn v0.0.0-20190420214824-7e0022ef6ba3/go.mod h1:jkELnwuX+w9qN5YIfX0fl88Ehu4XC3keFuOJJk9pcnA=\ngithub.com/jackc/pgconn v0.0.0-20190824142844-760dd75542eb/go.mod h1:lLjNuW/+OfW9/pnVKPazfWOgNfH2aPem8YQ7ilXGvJE=\ngithub.com/jackc/pgconn v0.0.0-20190831204454-2fabfa3c18b7/go.mod h1:ZJKsE/KZfsUgOEh9hBm+xYTstcNHg7UPMVJqRfQxq4s=\ngithub.com/jackc/pgconn v1.8.0/go.mod h1:1C2Pb36bGIP9QHGBYCjnyhqu7Rv3sGshaQUvmfGIB/o=\ngithub.com/jackc/pgconn v1.9.0/go.mod h1:YctiPyvzfU11JFxoXokUOOKQXQmDMoJL9vJzHH8/2JY=\ngithub.com/jackc/pgconn v1.9.1-0.20210724152538-d89c8390a530/go.mod h1:4z2w8XhRbP1hYxkpTuBjTS3ne3J48K83+u0zoyvg2pI=\ngithub.com/jackc/pgconn v1.14.3 h1:bVoTr12EGANZz66nZPkMInAV/KHD2TxH9npjXXgiB3w=\ngithub.com/jackc/pgconn v1.14.3/go.mod h1:RZbme4uasqzybK2RK5c65VsHxoyaml09lx3tXOcO/VM=\ngithub.com/jackc/pgerrcode v0.0.0-20240316143900-6e2875d9b438 h1:Dj0L5fhJ9F82ZJyVOmBx6msDp/kfd1t9GRfny/mfJA0=\ngithub.com/jackc/pgerrcode v0.0.0-20240316143900-6e2875d9b438/go.mod h1:a/s9Lp5W7n/DD0VrVoyJ00FbP2ytTPDVOivvn2bMlds=\ngithub.com/jackc/pgio v1.0.0 h1:g12B9UwVnzGhueNavwioyEEpAmqMe1E/BN9ES+8ovkE=\ngithub.com/jackc/pgio v1.0.0/go.mod h1:oP+2QK2wFfUWgr+gxjoBH9KGBb31Eio69xUb0w5bYf8=\ngithub.com/jackc/pgmock v0.0.0-20190831213851-13a1b77aafa2/go.mod h1:fGZlG77KXmcq05nJLRkk0+p82V8B8Dw8KN2/V9c/OAE=\ngithub.com/jackc/pgmock v0.0.0-20201204152224-4fe30f7445fd/go.mod h1:hrBW0Enj2AZTNpt/7Y5rr2xe/9Mn757Wtb2xeBzPv2c=\ngithub.com/jackc/pgmock v0.0.0-20210724152146-4ad1a8207f65 h1:DadwsjnMwFjfWc9y5Wi/+Zz7xoE5ALHsRQlOctkOiHc=\ngithub.com/jackc/pgmock v0.0.0-20210724152146-4ad1a8207f65/go.mod h1:5R2h2EEX+qri8jOWMbJCtaPWkrrNc7OHwsp2TCqp7ak=\ngithub.com/jackc/pgpassfile v1.0.0 h1:/6Hmqy13Ss2zCq62VdNG8tM1wchn8zjSGOBJ6icpsIM=\ngithub.com/jackc/pgpassfile v1.0.0/go.mod h1:CEx0iS5ambNFdcRtxPj5JhEz+xB6uRky5eyVu/W2HEg=\ngithub.com/jackc/pgproto3 v1.1.0/go.mod h1:eR5FA3leWg7p9aeAqi37XOTgTIbkABlvcPB3E5rlc78=\ngithub.com/jackc/pgproto3/v2 v2.0.0-alpha1.0.20190420180111-c116219b62db/go.mod h1:bhq50y+xrl9n5mRYyCBFKkpRVTLYJVWeCc+mEAI3yXA=\ngithub.com/jackc/pgproto3/v2 v2.0.0-alpha1.0.20190609003834-432c2951c711/go.mod h1:uH0AWtUmuShn0bcesswc4aBTWGvw0cAxIJp+6OB//Wg=\ngithub.com/jackc/pgproto3/v2 v2.0.0-rc3/go.mod h1:ryONWYqW6dqSg1Lw6vXNMXoBJhpzvWKnT95C46ckYeM=\ngithub.com/jackc/pgproto3/v2 v2.0.0-rc3.0.20190831210041-4c03ce451f29/go.mod h1:ryONWYqW6dqSg1Lw6vXNMXoBJhpzvWKnT95C46ckYeM=\ngithub.com/jackc/pgproto3/v2 v2.0.6/go.mod h1:WfJCnwN3HIg9Ish/j3sgWXnAfK8A9Y0bwXYU5xKaEdA=\ngithub.com/jackc/pgproto3/v2 v2.1.1/go.mod h1:WfJCnwN3HIg9Ish/j3sgWXnAfK8A9Y0bwXYU5xKaEdA=\ngithub.com/jackc/pgproto3/v2 v2.3.3 h1:1HLSx5H+tXR9pW3in3zaztoEwQYRC9SQaYUHjTSUOag=\ngithub.com/jackc/pgproto3/v2 v2.3.3/go.mod h1:WfJCnwN3HIg9Ish/j3sgWXnAfK8A9Y0bwXYU5xKaEdA=\ngithub.com/jackc/pgservicefile v0.0.0-20200714003250-2b9c44734f2b/go.mod h1:vsD4gTJCa9TptPL8sPkXrLZ+hDuNrZCnj29CQpr4X1E=\ngithub.com/jackc/pgservicefile v0.0.0-20221227161230-091c0ba34f0a h1:bbPeKD0xmW/Y25WS6cokEszi5g+S0QxI/d45PkRi7Nk=\ngithub.com/jackc/pgservicefile v0.0.0-20221227161230-091c0ba34f0a/go.mod h1:5TJZWKEWniPve33vlWYSoGYefn3gLQRzjfDlhSJ9ZKM=\ngithub.com/jackc/pgtype v0.0.0-20190421001408-4ed0de4755e0/go.mod h1:hdSHsc1V01CGwFsrv11mJRHWJ6aifDLfdV3aVjFF0zg=\ngithub.com/jackc/pgtype v0.0.0-20190824184912-ab885b375b90/go.mod h1:KcahbBH1nCMSo2DXpzsoWOAfFkdEtEJpPbVLq8eE+mc=\ngithub.com/jackc/pgtype v0.0.0-20190828014616-a8802b16cc59/go.mod h1:MWlu30kVJrUS8lot6TQqcg7mtthZ9T0EoIBFiJcmcyw=\ngithub.com/jackc/pgtype v1.8.1-0.20210724151600-32e20a603178/go.mod h1:C516IlIV9NKqfsMCXTdChteoXmwgUceqaLfjg2e3NlM=\ngithub.com/jackc/pgtype v1.14.0 h1:y+xUdabmyMkJLyApYuPj38mW+aAIqCe5uuBB51rH3Vw=\ngithub.com/jackc/pgtype v1.14.0/go.mod h1:LUMuVrfsFfdKGLw+AFFVv6KtHOFMwRgDDzBt76IqCA4=\ngithub.com/jackc/pgx/v4 v4.0.0-20190420224344-cc3461e65d96/go.mod h1:mdxmSJJuR08CZQyj1PVQBHy9XOp5p8/SHH6a0psbY9Y=\ngithub.com/jackc/pgx/v4 v4.0.0-20190421002000-1b8f0016e912/go.mod h1:no/Y67Jkk/9WuGR0JG/JseM9irFbnEPbuWV2EELPNuM=\ngithub.com/jackc/pgx/v4 v4.0.0-pre1.0.20190824185557-6972a5742186/go.mod h1:X+GQnOEnf1dqHGpw7JmHqHc1NxDoalibchSk9/RWuDc=\ngithub.com/jackc/pgx/v4 v4.12.1-0.20210724153913-640aa07df17c/go.mod h1:1QD0+tgSXP7iUjYm9C1NxKhny7lq6ee99u/z+IHFcgs=\ngithub.com/jackc/pgx/v4 v4.18.2 h1:xVpYkNR5pk5bMCZGfClbO962UIqVABcAGt7ha1s/FeU=\ngithub.com/jackc/pgx/v4 v4.18.2/go.mod h1:Ey4Oru5tH5sB6tV7hDmfWFahwF15Eb7DNXlRKx2CkVw=\ngithub.com/jackc/pgx/v5 v5.3.1 h1:Fcr8QJ1ZeLi5zsPZqQeUZhNhxfkkKBOgJuYkJHoBOtU=\ngithub.com/jackc/pgx/v5 v5.3.1/go.mod h1:t3JDKnCBlYIc0ewLF0Q7B8MXmoIaBOZj/ic7iHozM/8=\ngithub.com/jackc/puddle v0.0.0-20190413234325-e4ced69a3a2b/go.mod h1:m4B5Dj62Y0fbyuIc15OsIqK0+JU8nkqQjsgx7dvjSWk=\ngithub.com/jackc/puddle v0.0.0-20190608224051-11cab39313c9/go.mod h1:m4B5Dj62Y0fbyuIc15OsIqK0+JU8nkqQjsgx7dvjSWk=\ngithub.com/jackc/puddle v1.1.3/go.mod h1:m4B5Dj62Y0fbyuIc15OsIqK0+JU8nkqQjsgx7dvjSWk=\ngithub.com/jackc/puddle v1.3.0 h1:eHK/5clGOatcjX3oWGBO/MpxpbHzSwud5EWTSCI+MX0=\ngithub.com/jackc/puddle v1.3.0/go.mod h1:m4B5Dj62Y0fbyuIc15OsIqK0+JU8nkqQjsgx7dvjSWk=\ngithub.com/jackpal/gateway v1.0.5 h1:qzXWUJfuMdlLMtt0a3Dgt+xkWQiA5itDEITVJtuSwMc=\ngithub.com/jackpal/gateway v1.0.5/go.mod h1:lTpwd4ACLXmpyiCTRtfiNyVnUmqT9RivzCDQetPfnjA=\ngithub.com/jackpal/go-nat-pmp v0.0.0-20170405195558-28a68d0c24ad h1:heFfj7z0pGsNCekUlsFhO2jstxO4b5iQ665LjwM5mDc=\ngithub.com/jackpal/go-nat-pmp v0.0.0-20170405195558-28a68d0c24ad/go.mod h1:QPH045xvCAeXUZOxsnwmrtiCoxIr9eob+4orBN1SBKc=\ngithub.com/jedib0t/go-pretty/v6 v6.2.7 h1:4823Lult/tJ0VI1PgW3aSKw59pMWQ6Kzv9b3Bj6MwY0=\ngithub.com/jedib0t/go-pretty/v6 v6.2.7/go.mod h1:FMkOpgGD3EZ91cW8g/96RfxoV7bdeJyzXPYgz1L1ln0=\ngithub.com/jessevdk/go-flags v0.0.0-20141203071132-1679536dcc89/go.mod h1:4FA24M0QyGHXBuZZK/XkWh8h0e1EYbRYJSGM75WSRxI=\ngithub.com/jessevdk/go-flags v1.4.0 h1:4IU2WS7AumrZ/40jfhf4QVDMsQwqA7VEHozFRrGARJA=\ngithub.com/jessevdk/go-flags v1.4.0/go.mod h1:4FA24M0QyGHXBuZZK/XkWh8h0e1EYbRYJSGM75WSRxI=\ngithub.com/jonboulle/clockwork v0.2.2 h1:UOGuzwb1PwsrDAObMuhUnj0p5ULPj8V/xJ7Kx9qUBdQ=\ngithub.com/jonboulle/clockwork v0.2.2/go.mod h1:Pkfl5aHPm1nk2H9h0bjmnJD/BcgbGXUBGnn1kMkgxc8=\ngithub.com/jpillora/backoff v1.0.0/go.mod h1:J/6gKK9jxlEcS3zixgDgUAsiuZ7yrSoa/FX5e0EB2j4=\ngithub.com/jrick/logrotate v1.0.0/go.mod h1:LNinyqDIJnpAur+b8yyulnQw/wDuN1+BYKlTRt3OuAQ=\ngithub.com/jrick/logrotate v1.1.2 h1:6ePk462NCX7TfKtNp5JJ7MbA2YIslkpfgP03TlTYMN0=\ngithub.com/jrick/logrotate v1.1.2/go.mod h1:f9tdWggSVK3iqavGpyvegq5IhNois7KXmasU6/N96OQ=\ngithub.com/json-iterator/go v1.1.6/go.mod h1:+SdeFBvtyEkXs7REEP0seUULqWtbJapLOCVDaaPEHmU=\ngithub.com/json-iterator/go v1.1.10/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=\ngithub.com/json-iterator/go v1.1.11 h1:uVUAXhF2To8cbw/3xN3pxj6kk7TYKs98NIrTqPlMWAQ=\ngithub.com/json-iterator/go v1.1.11/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=\ngithub.com/jstemmer/go-junit-report v0.0.0-20190106144839-af01ea7f8024/go.mod h1:6v2b51hI/fHJwM22ozAgKL4VKDeJcHhJFhtBdhmNjmU=\ngithub.com/jstemmer/go-junit-report v0.9.1/go.mod h1:Brl9GWCQeLvo8nXZwPNNblvFj/XSXhF0NWZEnDohbsk=\ngithub.com/juju/ansiterm v0.0.0-20180109212912-720a0952cc2a/go.mod h1:UJSiEoRfvx3hP73CvoARgeLjaIOjybY9vj8PUPPFGeU=\ngithub.com/juju/clock v0.0.0-20190205081909-9c5c9712527c h1:3UvYABOQRhJAApj9MdCN+Ydv841ETSoy6xLzdmmr/9A=\ngithub.com/juju/clock v0.0.0-20190205081909-9c5c9712527c/go.mod h1:nD0vlnrUjcjJhqN5WuCWZyzfd5AHZAC9/ajvbSx69xA=\ngithub.com/juju/collections v0.0.0-20200605021417-0d0ec82b7271 h1:4R626WTwa7pRYQFiIRLVPepMhm05eZMEx+wIurRnMLc=\ngithub.com/juju/collections v0.0.0-20200605021417-0d0ec82b7271/go.mod h1:5XgO71dV1JClcOJE+4dzdn4HrI5LiyKd7PlVG6eZYhY=\ngithub.com/juju/errors v0.0.0-20220203013757-bd733f3c86b9 h1:EJHbsNpQyupmMeWTq7inn+5L/WZ7JfzCVPJ+DP9McCQ=\ngithub.com/juju/errors v0.0.0-20220203013757-bd733f3c86b9/go.mod h1:TRm7EVGA3mQOqSVcBySRY7a9Y1/gyVhh/WTCnc5sD4U=\ngithub.com/juju/loggo v0.0.0-20210728185423-eebad3a902c4 h1:NO5tuyw++EGLnz56Q8KMyDZRwJwWO8jQnj285J3FOmY=\ngithub.com/juju/loggo v0.0.0-20210728185423-eebad3a902c4/go.mod h1:NIXFioti1SmKAlKNuUwbMenNdef59IF52+ZzuOmHYkg=\ngithub.com/juju/mgo/v2 v2.0.0-20210302023703-70d5d206e208 h1:/WiCm+Vpj87e4QWuWwPD/bNE9kDrWCLvPBHOQNcG2+A=\ngithub.com/juju/mgo/v2 v2.0.0-20210302023703-70d5d206e208/go.mod h1:0OChplkvPTZ174D2FYZXg4IB9hbEwyHkD+zT+/eK+Fg=\ngithub.com/juju/retry v0.0.0-20180821225755-9058e192b216 h1:/eQL7EJQKFHByJe3DeE8Z36yqManj9UY5zppDoQi4FU=\ngithub.com/juju/retry v0.0.0-20180821225755-9058e192b216/go.mod h1:OohPQGsr4pnxwD5YljhQ+TZnuVRYpa5irjugL1Yuif4=\ngithub.com/juju/testing v0.0.0-20220203020004-a0ff61f03494 h1:XEDzpuZb8Ma7vLja3+5hzUqVTvAqm5Y+ygvnDs5iTMM=\ngithub.com/juju/testing v0.0.0-20220203020004-a0ff61f03494/go.mod h1:rUquetT0ALL48LHZhyRGvjjBH8xZaZ8dFClulKK5wK4=\ngithub.com/juju/utils/v3 v3.0.0-20220130232349-cd7ecef0e94a h1:5ZWDCeCF0RaITrZGemzmDFIhjR/MVSvBUqgSyaeTMbE=\ngithub.com/juju/utils/v3 v3.0.0-20220130232349-cd7ecef0e94a/go.mod h1:LzwbbEN7buYjySp4nqnti6c6olSqRXUk6RkbSUUP1n8=\ngithub.com/juju/version/v2 v2.0.0-20211007103408-2e8da085dc23 h1:wtEPbidt1VyHlb8RSztU6ySQj29FLsOQiI9XiJhXDM4=\ngithub.com/juju/version/v2 v2.0.0-20211007103408-2e8da085dc23/go.mod h1:Ljlbryh9sYaUSGXucslAEDf0A2XUSGvDbHJgW8ps6nc=\ngithub.com/julienschmidt/httprouter v1.2.0/go.mod h1:SYymIcj16QtmaHHD7aYtjjsJG7VTCxuUUipMqKk8s4w=\ngithub.com/julienschmidt/httprouter v1.3.0/go.mod h1:JR6WtHb+2LUe8TCKY3cZOxFyyO8IZAc4RVcycCCAKdM=\ngithub.com/kisielk/errcheck v1.5.0/go.mod h1:pFxgyoBC7bSaBwPgfKdkLd5X25qrDl4LWUI2bnpBCr8=\ngithub.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO+MFFWcvkIS/tQcRk01m1F5IRFswLeQ+oQHNcck=\ngithub.com/kkdai/bstream v0.0.0-20161212061736-f391b8402d23/go.mod h1:J+Gs4SYgM6CZQHDETBtE9HaSEkGmuNXF86RwHhHUvq4=\ngithub.com/kkdai/bstream v1.0.0 h1:Se5gHwgp2VT2uHfDrkbbgbgEvV9cimLELwrPJctSjg8=\ngithub.com/kkdai/bstream v1.0.0/go.mod h1:FDnDOHt5Yx4p3FaHcioFT0QjDOtgUpvjeZqAs+NVZZA=\ngithub.com/klauspost/compress v1.17.9 h1:6KIumPrER1LHsvBVuDa0r5xaG0Es51mhhB9BQB2qeMA=\ngithub.com/klauspost/compress v1.17.9/go.mod h1:Di0epgTjJY877eYKx5yC51cX2A2Vl2ibi7bDH9ttBbw=\ngithub.com/konsorten/go-windows-terminal-sequences v1.0.1/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\ngithub.com/konsorten/go-windows-terminal-sequences v1.0.2/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\ngithub.com/konsorten/go-windows-terminal-sequences v1.0.3/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\ngithub.com/kr/logfmt v0.0.0-20140226030751-b84e30acd515/go.mod h1:+0opPa2QZZtGFBFZlji/RkVcI2GknAs/DXo4wKdlNEc=\ngithub.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=\ngithub.com/kr/pretty v0.3.0 h1:WgNl7dwNpEZ6jJ9k1snq4pZsg7DOEN8hP9Xw0Tsjwk0=\ngithub.com/kr/pretty v0.3.0/go.mod h1:640gp4NfQd8pI5XOwp5fnNeVWj67G7CFk/SaSQn7NBk=\ngithub.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=\ngithub.com/kr/pty v1.1.8/go.mod h1:O1sed60cT9XZ5uDucP5qwvh+TE3NnUj51EiZO/lmSfw=\ngithub.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=\ngithub.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=\ngithub.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=\ngithub.com/lib/pq v1.0.0/go.mod h1:5WUZQaWbwv1U+lTReE5YruASi9Al49XbQIvNi/34Woo=\ngithub.com/lib/pq v1.1.0/go.mod h1:5WUZQaWbwv1U+lTReE5YruASi9Al49XbQIvNi/34Woo=\ngithub.com/lib/pq v1.2.0/go.mod h1:5WUZQaWbwv1U+lTReE5YruASi9Al49XbQIvNi/34Woo=\ngithub.com/lib/pq v1.10.2/go.mod h1:AlVN5x4E4T544tWzH6hKfbfQvm3HdbOxrmggDNAPY9o=\ngithub.com/lib/pq v1.10.9 h1:YXG7RB+JIjhP29X+OtkiDnYaXQwpS4JEWq7dtCCRUEw=\ngithub.com/lib/pq v1.10.9/go.mod h1:AlVN5x4E4T544tWzH6hKfbfQvm3HdbOxrmggDNAPY9o=\ngithub.com/lightninglabs/gozmq v0.0.0-20191113021534-d20a764486bf h1:HZKvJUHlcXI/f/O0Avg7t8sqkPo78HFzjmeYFl6DPnc=\ngithub.com/lightninglabs/gozmq v0.0.0-20191113021534-d20a764486bf/go.mod h1:vxmQPeIQxPf6Jf9rM8R+B4rKBqLA2AjttNxkFBL2Plk=\ngithub.com/lightninglabs/neutrino v0.16.1-0.20240425105051-602843d34ffd h1:D8aRocHpoCv43hL8egXEMYyPmyOiefFHZ66338KQB2s=\ngithub.com/lightninglabs/neutrino v0.16.1-0.20240425105051-602843d34ffd/go.mod h1:x3OmY2wsA18+Kc3TSV2QpSUewOCiscw2mKpXgZv2kZk=\ngithub.com/lightninglabs/neutrino/cache v1.1.2 h1:C9DY/DAPaPxbFC+xNNEI/z1SJY9GS3shmlu5hIQ798g=\ngithub.com/lightninglabs/neutrino/cache v1.1.2/go.mod h1:XJNcgdOw1LQnanGjw8Vj44CvguYA25IMKjWFZczwZuo=\ngithub.com/lightninglabs/protobuf-go-hex-display v1.30.0-hex-display h1:pRdza2wleRN1L2fJXd6ZoQ9ZegVFTAb2bOQfruJPKcY=\ngithub.com/lightninglabs/protobuf-go-hex-display v1.30.0-hex-display/go.mod h1:HV8QOd/L58Z+nl8r43ehVNZIU/HEI6OcFqwMG9pJV4I=\ngithub.com/lightningnetwork/lightning-onion v1.2.1-0.20240712235311-98bd56499dfb h1:yfM05S8DXKhuCBp5qSMZdtSwvJ+GFzl94KbXMNB1JDY=\ngithub.com/lightningnetwork/lightning-onion v1.2.1-0.20240712235311-98bd56499dfb/go.mod h1:c0kvRShutpj3l6B9WtTsNTBUtjSmjZXbJd9ZBRQOSKI=\ngithub.com/lightningnetwork/lnd/cert v1.2.2 h1:71YK6hogeJtxSxw2teq3eGeuy4rHGKcFf0d0Uy4qBjI=\ngithub.com/lightningnetwork/lnd/cert v1.2.2/go.mod h1:jQmFn/Ez4zhDgq2hnYSw8r35bqGVxViXhX6Cd7HXM6U=\ngithub.com/lightningnetwork/lnd/clock v1.1.1 h1:OfR3/zcJd2RhH0RU+zX/77c0ZiOnIMsDIBjgjWdZgA0=\ngithub.com/lightningnetwork/lnd/clock v1.1.1/go.mod h1:mGnAhPyjYZQJmebS7aevElXKTFDuO+uNFFfMXK1W8xQ=\ngithub.com/lightningnetwork/lnd/fn/v2 v2.0.4 h1:DiC/AEa7DhnY4qOEQBISu1cp+1+51LjbVDzNLVBwNjI=\ngithub.com/lightningnetwork/lnd/fn/v2 v2.0.4/go.mod h1:TOzwrhjB/Azw1V7aa8t21ufcQmdsQOQMDtxVOQWNl8s=\ngithub.com/lightningnetwork/lnd/healthcheck v1.2.6 h1:1sWhqr93GdkWy4+6U7JxBfcyZIE78MhIHTJZfPx7qqI=\ngithub.com/lightningnetwork/lnd/healthcheck v1.2.6/go.mod h1:Mu02um4CWY/zdTOvFje7WJgJcHyX2zq/FG3MhOAiGaQ=\ngithub.com/lightningnetwork/lnd/kvdb v1.4.12 h1:Y0WY5Tbjyjn6eCYh068qkWur5oFtioJlfxc8w5SlJeQ=\ngithub.com/lightningnetwork/lnd/kvdb v1.4.12/go.mod h1:hx9buNcxsZpZwh8m1sjTQwy2SOeBoWWOZ3RnOQkMsxI=\ngithub.com/lightningnetwork/lnd/queue v1.1.1 h1:99ovBlpM9B0FRCGYJo6RSFDlt8/vOkQQZznVb18iNMI=\ngithub.com/lightningnetwork/lnd/queue v1.1.1/go.mod h1:7A6nC1Qrm32FHuhx/mi1cieAiBZo5O6l8IBIoQxvkz4=\ngithub.com/lightningnetwork/lnd/sqldb v1.0.6 h1:LJdDSVdN33bVBIefsaJlPW9PDAm6GrXlyFucmzSJ3Ts=\ngithub.com/lightningnetwork/lnd/sqldb v1.0.6/go.mod h1:OG09zL/PHPaBJefp4HsPz2YLUJ+zIQHbpgCtLnOx8I4=\ngithub.com/lightningnetwork/lnd/ticker v1.1.1 h1:J/b6N2hibFtC7JLV77ULQp++QLtCwT6ijJlbdiZFbSM=\ngithub.com/lightningnetwork/lnd/ticker v1.1.1/go.mod h1:waPTRAAcwtu7Ji3+3k+u/xH5GHovTsCoSVpho0KDvdA=\ngithub.com/lightningnetwork/lnd/tlv v1.3.0 h1:exS/KCPEgpOgviIttfiXAPaUqw2rHQrnUOpP7HPBPiY=\ngithub.com/lightningnetwork/lnd/tlv v1.3.0/go.mod h1:pJuiBj1ecr1WWLOtcZ+2+hu9Ey25aJWFIsjmAoPPnmc=\ngithub.com/lightningnetwork/lnd/tor v1.1.4 h1:TUW27EXqoZCcCAQPlD4aaDfh8jMbBS9CghNz50qqwtA=\ngithub.com/lightningnetwork/lnd/tor v1.1.4/go.mod h1:qSRB8llhAK+a6kaTPWOLLXSZc6Hg8ZC0mq1sUQ/8JfI=\ngithub.com/ltcsuite/ltcd v0.0.0-20190101042124-f37f8bf35796 h1:sjOGyegMIhvgfq5oaue6Td+hxZuf3tDC8lAPrFldqFw=\ngithub.com/ltcsuite/ltcd v0.0.0-20190101042124-f37f8bf35796/go.mod h1:3p7ZTf9V1sNPI5H8P3NkTFF4LuwMdPl2DodF60qAKqY=\ngithub.com/ltcsuite/ltcutil v0.0.0-20181217130922-17f3b04680b6/go.mod h1:8Vg/LTOO0KYa/vlHWJ6XZAevPQThGH5sufO0Hrou/lA=\ngithub.com/lunixbochs/vtclean v0.0.0-20160125035106-4fbf7632a2c6/go.mod h1:pHhQNgMf3btfWnGBVipUOjRYhoOsdGqdm/+2c2E2WMI=\ngithub.com/mattn/go-colorable v0.0.6/go.mod h1:9vuHe8Xs5qXnSaW/c/ABM9alt+Vo+STaOChaDxuIBZU=\ngithub.com/mattn/go-colorable v0.1.1/go.mod h1:FuOcm+DKB9mbwrcAfNl7/TZVBZ6rcnceauSikq3lYCQ=\ngithub.com/mattn/go-colorable v0.1.6/go.mod h1:u6P/XSegPjTcexA+o6vUJrdnUu04hMope9wVRipJSqc=\ngithub.com/mattn/go-isatty v0.0.0-20160806122752-66b8e73f3f5c/go.mod h1:M+lRXTBqGeGNdLjl/ufCoiOlB5xdOkqRJdNxMWT7Zi4=\ngithub.com/mattn/go-isatty v0.0.5/go.mod h1:Iq45c/XA43vh69/j3iqttzPXn0bhXyGjM0Hdxcsrc5s=\ngithub.com/mattn/go-isatty v0.0.7/go.mod h1:Iq45c/XA43vh69/j3iqttzPXn0bhXyGjM0Hdxcsrc5s=\ngithub.com/mattn/go-isatty v0.0.12/go.mod h1:cbi8OIDigv2wuxKPP5vlRcQ1OAZbq2CE4Kysco4FUpU=\ngithub.com/mattn/go-isatty v0.0.20 h1:xfD0iDuEKnDkl03q4limB+vH+GxLEtL/jb4xVJSWWEY=\ngithub.com/mattn/go-isatty v0.0.20/go.mod h1:W+V8PltTTMOvKvAeJH7IuucS94S2C6jfK/D7dTCTo3Y=\ngithub.com/mattn/go-runewidth v0.0.13 h1:lTGmDsbAYt5DmK6OnoV7EuIF1wEIFAcxld6ypU4OSgU=\ngithub.com/mattn/go-runewidth v0.0.13/go.mod h1:Jdepj2loyihRzMpdS35Xk/zdY8IAYHsh153qUoGf23w=\ngithub.com/matttproud/golang_protobuf_extensions v1.0.1 h1:4hp9jkHxhMHkqkrB3Ix0jegS5sx/RkqARlsWZ6pIwiU=\ngithub.com/matttproud/golang_protobuf_extensions v1.0.1/go.mod h1:D8He9yQNgCq6Z5Ld7szi9bcBfOoFv/3dc6xSMkL2PC0=\ngithub.com/miekg/dns v1.1.43 h1:JKfpVSCB84vrAmHzyrsxB5NAr5kLoMXZArPSw7Qlgyg=\ngithub.com/miekg/dns v1.1.43/go.mod h1:+evo5L0630/F6ca/Z9+GAqzhjGyn8/c+TBaOyfEl0V4=\ngithub.com/mitchellh/mapstructure v1.4.1 h1:CpVNEelQCZBooIPDn+AR3NpivK/TIKU8bDxdASFVQag=\ngithub.com/mitchellh/mapstructure v1.4.1/go.mod h1:bFUtVrKA4DC2yAKiSyO/QUcy7e+RRV2QTWOzhPopBRo=\ngithub.com/moby/term v0.5.0 h1:xt8Q1nalod/v7BqbG21f8mQPqH+xAaC9C3N3wfWbVP0=\ngithub.com/moby/term v0.5.0/go.mod h1:8FzsFHVUBGZdbDsJw/ot+X+d5HLUbvklYLJ9uGfcI3Y=\ngithub.com/modern-go/concurrent v0.0.0-20180228061459-e0a39a4cb421/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=\ngithub.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd h1:TRLaZ9cD/w8PVh93nsPXa1VrQ6jlwL5oN8l14QlcNfg=\ngithub.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=\ngithub.com/modern-go/reflect2 v0.0.0-20180701023420-4b7aa43c6742/go.mod h1:bx2lNnkwVCuqBIxFjflWJWanXIb3RllmbCylyMrvgv0=\ngithub.com/modern-go/reflect2 v1.0.1 h1:9f412s+6RmYXLWZSEzVVgPGK7C2PphHj5RJrvfx9AWI=\ngithub.com/modern-go/reflect2 v1.0.1/go.mod h1:bx2lNnkwVCuqBIxFjflWJWanXIb3RllmbCylyMrvgv0=\ngithub.com/morikuni/aec v1.0.0 h1:nP9CBfwrvYnBRgY6qfDQkygYDmYwOilePFkwzv4dU8A=\ngithub.com/morikuni/aec v1.0.0/go.mod h1:BbKIizmSmc5MMPqRYbxO4ZU0S0+P200+tUnFx7PXmsc=\ngithub.com/mwitkow/go-conntrack v0.0.0-20161129095857-cc309e4a2223/go.mod h1:qRWi+5nqEBWmkhHvq77mSJWrCKwh8bxhgT7d/eI7P4U=\ngithub.com/mwitkow/go-conntrack v0.0.0-20190716064945-2f068394615f/go.mod h1:qRWi+5nqEBWmkhHvq77mSJWrCKwh8bxhgT7d/eI7P4U=\ngithub.com/ncruces/go-strftime v0.1.9 h1:bY0MQC28UADQmHmaF5dgpLmImcShSi2kHU9XLdhx/f4=\ngithub.com/ncruces/go-strftime v0.1.9/go.mod h1:Fwc5htZGVVkseilnfgOVb9mKy6w1naJmn9CehxcKcls=\ngithub.com/nxadm/tail v1.4.4/go.mod h1:kenIhsEOeOJmVchQTgglprH7qJGnHDVpk1VPCcaMI8A=\ngithub.com/nxadm/tail v1.4.8 h1:nPr65rt6Y5JFSKQO7qToXr7pePgD6Gwiw05lkbyAQTE=\ngithub.com/nxadm/tail v1.4.8/go.mod h1:+ncqLTQzXmGhMZNUePPaPqPvBxHAIsmXswZKocGu+AU=\ngithub.com/onsi/ginkgo v1.6.0/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\ngithub.com/onsi/ginkgo v1.7.0/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=\ngithub.com/onsi/ginkgo v1.12.1/go.mod h1:zj2OWP4+oCPe1qIXoGWkgMRwljMUYCdkwsT2108oapk=\ngithub.com/onsi/ginkgo v1.14.0/go.mod h1:iSB4RoI2tjJc9BBv4NKIKWKya62Rps+oPG/Lv9klQyY=\ngithub.com/onsi/ginkgo v1.16.4 h1:29JGrr5oVBm5ulCWet69zQkzWipVXIol6ygQUe/EzNc=\ngithub.com/onsi/ginkgo v1.16.4/go.mod h1:dX+/inL/fNMqNlz0e9LfyB9TswhZpCVdJM/Z6Vvnwo0=\ngithub.com/onsi/gomega v1.4.1/go.mod h1:C1qb7wdrVGGVU+Z6iS04AVkA3Q65CEZX59MT0QO5uiA=\ngithub.com/onsi/gomega v1.4.3/go.mod h1:ex+gbHU/CVuBBDIJjb2X0qEXbFg53c61hWP/1CpauHY=\ngithub.com/onsi/gomega v1.7.1/go.mod h1:XdKZgCCFLUoM/7CFJVPcG8C1xQ1AJ0vpAezJrB7JYyY=\ngithub.com/onsi/gomega v1.10.1/go.mod h1:iN09h71vgCQne3DLsj+A5owkum+a2tYe+TOCB1ybHNo=\ngithub.com/onsi/gomega v1.26.0 h1:03cDLK28U6hWvCAns6NeydX3zIm4SF3ci69ulidS32Q=\ngithub.com/onsi/gomega v1.26.0/go.mod h1:r+zV744Re+DiYCIPRlYOTxn0YkOLcAnW8k1xXdMPGhM=\ngithub.com/opencontainers/go-digest v1.0.0 h1:apOUWs51W5PlhuyGyz9FCeeBIOUDA/6nW8Oi/yOhh5U=\ngithub.com/opencontainers/go-digest v1.0.0/go.mod h1:0JzlMkj0TRzQZfJkVvzbP0HBR3IKzErnv2BNG4W4MAM=\ngithub.com/opencontainers/image-spec v1.0.2 h1:9yCKha/T5XdGtO0q9Q9a6T5NUCsTn/DrBg0D7ufOcFM=\ngithub.com/opencontainers/image-spec v1.0.2/go.mod h1:BtxoFyWECRxE4U/7sNtV5W15zMzWCbyJoFRP3s7yZA0=\ngithub.com/opencontainers/runc v1.1.12 h1:BOIssBaW1La0/qbNZHXOOa71dZfZEQOzW7dqQf3phss=\ngithub.com/opencontainers/runc v1.1.12/go.mod h1:S+lQwSfncpBha7XTy/5lBwWgm5+y5Ma/O44Ekby9FK8=\ngithub.com/opentracing/opentracing-go v1.1.0/go.mod h1:UkNAQd3GIcIGf0SeVgPpRdFStlNbqXla1AfSYxPUl2o=\ngithub.com/ory/dockertest/v3 v3.10.0 h1:4K3z2VMe8Woe++invjaTB7VRyQXQy5UY+loujO4aNE4=\ngithub.com/ory/dockertest/v3 v3.10.0/go.mod h1:nr57ZbRWMqfsdGdFNLHz5jjNdDb7VVFnzAeW1n5N1Lg=\ngithub.com/pkg/errors v0.8.0/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pkg/errors v0.8.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=\ngithub.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pkg/profile v1.6.0/go.mod h1:qBsxPvzyUincmltOk6iyRVxHYg4adc0OFOv72ZdLa18=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/prometheus/client_golang v0.9.1/go.mod h1:7SWBe2y4D6OKWSNQJUaRYU/AaXPKyh/dDVn+NZz0KFw=\ngithub.com/prometheus/client_golang v1.0.0/go.mod h1:db9x61etRT2tGnBNRi70OPL5FsnadC4Ky3P0J6CfImo=\ngithub.com/prometheus/client_golang v1.7.1/go.mod h1:PY5Wy2awLA44sXw4AOSfFBetzPP4j5+D6mVACh+pe2M=\ngithub.com/prometheus/client_golang v1.11.1 h1:+4eQaD7vAZ6DsfsxB15hbE0odUjGI5ARs9yskGu1v4s=\ngithub.com/prometheus/client_golang v1.11.1/go.mod h1:Z6t4BnS23TR94PD6BsDNk8yVqroYurpAkEiz0P2BEV0=\ngithub.com/prometheus/client_model v0.0.0-20180712105110-5c3871d89910/go.mod h1:MbSGuTsp3dbXC40dX6PRTWyKYBIrTGTE9sqQNg2J8bo=\ngithub.com/prometheus/client_model v0.0.0-20190129233127-fd36f4220a90/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\ngithub.com/prometheus/client_model v0.0.0-20190812154241-14fe0d1b01d4/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\ngithub.com/prometheus/client_model v0.2.0 h1:uq5h0d+GuxiXLJLNABMgp2qUWDPiLvgCzz2dUR+/W/M=\ngithub.com/prometheus/client_model v0.2.0/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\ngithub.com/prometheus/common v0.4.1/go.mod h1:TNfzLD0ON7rHzMJeJkieUDPYmFC7Snx/y86RQel1bk4=\ngithub.com/prometheus/common v0.10.0/go.mod h1:Tlit/dnDKsSWFlCLTWaA1cyBgKHSMdTB80sz/V91rCo=\ngithub.com/prometheus/common v0.26.0 h1:iMAkS2TDoNWnKM+Kopnx/8tnEStIfpYA0ur0xQzzhMQ=\ngithub.com/prometheus/common v0.26.0/go.mod h1:M7rCNAaPfAosfx8veZJCuw84e35h3Cfd9VFqTh1DIvc=\ngithub.com/prometheus/procfs v0.0.0-20181005140218-185b4288413d/go.mod h1:c3At6R/oaqEKCNdg8wHV1ftS6bRYblBhIjjI8uT2IGk=\ngithub.com/prometheus/procfs v0.0.2/go.mod h1:TjEm7ze935MbeOT/UhFTIMYKhuLP4wbCsTZCD3I8kEA=\ngithub.com/prometheus/procfs v0.1.3/go.mod h1:lV6e/gmhEcM9IjHGsFOCxxuZ+z1YqCvr4OA4YeYWdaU=\ngithub.com/prometheus/procfs v0.6.0 h1:mxy4L2jP6qMonqmq+aTtOx1ifVWUgG/TAmntgbh3xv4=\ngithub.com/prometheus/procfs v0.6.0/go.mod h1:cz+aTbrPOrUb4q7XlbU9ygM+/jj0fzG6c1xBZuNvfVA=\ngithub.com/remyoudompheng/bigfft v0.0.0-20230129092748-24d4a6f8daec h1:W09IVJc94icq4NjY3clb7Lk8O1qJ8BdBEF8z0ibU0rE=\ngithub.com/remyoudompheng/bigfft v0.0.0-20230129092748-24d4a6f8daec/go.mod h1:qqbHyh8v60DhA7CoWK5oRCqLrMHRGoxYCSS9EjAz6Eo=\ngithub.com/rivo/uniseg v0.2.0 h1:S1pD9weZBuJdFmowNwbpi7BJ8TNftyUImj/0WQi72jY=\ngithub.com/rivo/uniseg v0.2.0/go.mod h1:J6wj4VEh+S6ZtnVlnTBMWIodfgj8LQOQFoIToxlJtxc=\ngithub.com/rogpeppe/fastuuid v1.2.0 h1:Ppwyp6VYCF1nvBTXL3trRso7mXMlRrw9ooo375wvi2s=\ngithub.com/rogpeppe/fastuuid v1.2.0/go.mod h1:jVj6XXZzXRy/MSR5jhDC/2q6DgLz+nrA6LYCDYWNEvQ=\ngithub.com/rogpeppe/go-internal v1.3.0/go.mod h1:M8bDsm7K2OlrFYOpmOWEs/qY81heoFRclV5y23lUDJ4=\ngithub.com/rogpeppe/go-internal v1.12.0 h1:exVL4IDcn6na9z1rAb56Vxr+CgyK3nn3O+epU5NdKM8=\ngithub.com/rogpeppe/go-internal v1.12.0/go.mod h1:E+RYuTGaKKdloAfM02xzb0FW3Paa99yedzYV+kq4uf4=\ngithub.com/rs/xid v1.2.1/go.mod h1:+uKXf+4Djp6Md1KODXJxgGQPKngRmWyn10oCKFzNHOQ=\ngithub.com/rs/zerolog v1.13.0/go.mod h1:YbFCdg8HfsridGWAh22vktObvhZbQsZXe4/zB0OKkWU=\ngithub.com/rs/zerolog v1.15.0/go.mod h1:xYTKnLHcpfU2225ny5qZjxnj9NvkumZYjJHlAThCjNc=\ngithub.com/russross/blackfriday/v2 v2.0.1 h1:lPqVAte+HuHNfhJ/0LC98ESWRz8afy9tM/0RK8m9o+Q=\ngithub.com/russross/blackfriday/v2 v2.0.1/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=\ngithub.com/satori/go.uuid v1.2.0/go.mod h1:dA0hQrYB0VpLJoorglMZABFdXlWrHn1NEOzdhQKdks0=\ngithub.com/shopspring/decimal v0.0.0-20180709203117-cd690d0c9e24/go.mod h1:M+9NzErvs504Cn4c5DxATwIqPbtswREoFCre64PpcG4=\ngithub.com/shopspring/decimal v1.2.0 h1:abSATXmQEYyShuxI4/vyW3tV1MrKAJzCZ/0zLUXYbsQ=\ngithub.com/shopspring/decimal v1.2.0/go.mod h1:DKyhrW/HYNuLGql+MJL6WCR6knT2jwCFRcu2hWCYk4o=\ngithub.com/shurcooL/sanitized_anchor_name v1.0.0 h1:PdmoCO6wvbs+7yrJyMORt4/BmY5IYyJwS/kOiWx8mHo=\ngithub.com/shurcooL/sanitized_anchor_name v1.0.0/go.mod h1:1NzhyTcUVG4SuEtjjoZeVRXNmyL/1OwPU0+IJeTBvfc=\ngithub.com/sirupsen/logrus v1.2.0/go.mod h1:LxeOpSwHxABJmUn/MG1IvRgCAasNZTLOkJPxbbu5VWo=\ngithub.com/sirupsen/logrus v1.4.1/go.mod h1:ni0Sbl8bgC9z8RoU9G6nDWqqs/fq4eDPysMBDgk/93Q=\ngithub.com/sirupsen/logrus v1.4.2/go.mod h1:tLMulIdttU9McNUspp0xgXVQah82FyeX6MwdIuYE2rE=\ngithub.com/sirupsen/logrus v1.6.0/go.mod h1:7uNnSEd1DgxDLC74fIahvMZmmYsHGZGEOFrfsX/uA88=\ngithub.com/sirupsen/logrus v1.9.2 h1:oxx1eChJGI6Uks2ZC4W1zpLlVgqB8ner4EuQwV4Ik1Y=\ngithub.com/sirupsen/logrus v1.9.2/go.mod h1:naHLuLoDiP4jHNo9R0sCBMtWGeIprob74mVsIT4qYEQ=\ngithub.com/soheilhy/cmux v0.1.5 h1:jjzc5WVemNEDTLwv9tlmemhC73tI08BNOIGwBOo10Js=\ngithub.com/soheilhy/cmux v0.1.5/go.mod h1:T7TcVDs9LWfQgPlPsdngu6I6QIoyIFZDDC6sNE1GqG0=\ngithub.com/spaolacci/murmur3 v0.0.0-20180118202830-f09979ecbc72/go.mod h1:JwIasOWyU6f++ZhiEuf87xNszmSA2myDM2Kzu9HwQUA=\ngithub.com/spf13/pflag v1.0.5 h1:iy+VFUOCP1a+8yFto/drg2CJ5u0yRoB7fZw3DKv/JXA=\ngithub.com/spf13/pflag v1.0.5/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/objx v0.1.1/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/objx v0.2.0/go.mod h1:qt09Ya8vawLte6SNmTgCsAVtYtaKzEcn8ATUoHMkEqE=\ngithub.com/stretchr/objx v0.5.2 h1:xuMeJ0Sdp5ZMRXx/aWO6RZxdr3beISkG5/G/aIRr3pY=\ngithub.com/stretchr/objx v0.5.2/go.mod h1:FRsXN1f5AsAjCGJKqEizvkpNtU+EGNCLh3NxZ/8L+MA=\ngithub.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=\ngithub.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\ngithub.com/stretchr/testify v1.4.0/go.mod h1:j7eGeouHqKxXV5pUuKE4zz7dFj8WfuZ+81PSLYec5m4=\ngithub.com/stretchr/testify v1.5.1/go.mod h1:5W2xD1RspED5o8YsWQXVCued0rvSQ+mT+I5cxcmMvtA=\ngithub.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.9.0 h1:HtqpIVDClZ4nwg75+f6Lvsy/wHu+3BoSGCbBAcpTsTg=\ngithub.com/stretchr/testify v1.9.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=\ngithub.com/syndtr/goleveldb v1.0.1-0.20210819022825-2ae1ddf74ef7 h1:epCh84lMvA70Z7CTTCmYQn2CKbY8j86K7/FAIr141uY=\ngithub.com/syndtr/goleveldb v1.0.1-0.20210819022825-2ae1ddf74ef7/go.mod h1:q4W45IWZaF22tdD+VEXcAWRA037jwmWEB5VWYORlTpc=\ngithub.com/tmc/grpc-websocket-proxy v0.0.0-20201229170055-e5319fda7802 h1:uruHq4dN7GR16kFc5fp3d1RIYzJW5onx8Ybykw2YQFA=\ngithub.com/tmc/grpc-websocket-proxy v0.0.0-20201229170055-e5319fda7802/go.mod h1:ncp9v5uamzpCO7NfCPTXjqaC+bZgJeR0sMTm6dMHP7U=\ngithub.com/tv42/zbase32 v0.0.0-20160707012821-501572607d02 h1:tcJ6OjwOMvExLlzrAVZute09ocAGa7KqOON60++Gz4E=\ngithub.com/tv42/zbase32 v0.0.0-20160707012821-501572607d02/go.mod h1:tHlrkM198S068ZqfrO6S8HsoJq2bF3ETfTL+kt4tInY=\ngithub.com/urfave/cli v1.22.9 h1:cv3/KhXGBGjEXLC4bH0sLuJ9BewaAbpk5oyMOveu4pw=\ngithub.com/urfave/cli v1.22.9/go.mod h1:Gos4lmkARVdJ6EkW0WaNv/tZAAMe9V7XWyB60NtXRu0=\ngithub.com/xeipuuv/gojsonpointer v0.0.0-20180127040702-4e3ac2762d5f h1:J9EGpcZtP0E/raorCMxlFGSTBrsSlaDGf3jU/qvAE2c=\ngithub.com/xeipuuv/gojsonpointer v0.0.0-20180127040702-4e3ac2762d5f/go.mod h1:N2zxlSyiKSe5eX1tZViRH5QA0qijqEDrYZiPEAiq3wU=\ngithub.com/xeipuuv/gojsonreference v0.0.0-20180127040603-bd5ef7bd5415 h1:EzJWgHovont7NscjpAxXsDA8S8BMYve8Y5+7cuRE7R0=\ngithub.com/xeipuuv/gojsonreference v0.0.0-20180127040603-bd5ef7bd5415/go.mod h1:GwrjFmJcFw6At/Gs6z4yjiIwzuJ1/+UwLxMQDVQXShQ=\ngithub.com/xeipuuv/gojsonschema v1.2.0 h1:LhYJRs+L4fBtjZUfuSZIKGeVu0QRy8e5Xi7D17UxZ74=\ngithub.com/xeipuuv/gojsonschema v1.2.0/go.mod h1:anYRn/JVcOK2ZgGU+IjEV4nwlhoK5sQluxsYJ78Id3Y=\ngithub.com/xi2/xz v0.0.0-20171230120015-48954b6210f8 h1:nIPpBwaJSVYIxUFsDv3M8ofmx9yWTog9BfvIu0q41lo=\ngithub.com/xi2/xz v0.0.0-20171230120015-48954b6210f8/go.mod h1:HUYIGzjTL3rfEspMxjDjgmT5uz5wzYJKVo23qUhYTos=\ngithub.com/xiang90/probing v0.0.0-20190116061207-43a291ad63a2 h1:eY9dn8+vbi4tKz5Qo6v2eYzo7kUS51QINcR5jNpbZS8=\ngithub.com/xiang90/probing v0.0.0-20190116061207-43a291ad63a2/go.mod h1:UETIi67q53MR2AWcXfiuqkDkRtnGDLqkBTpCHuJHxtU=\ngithub.com/yuin/goldmark v1.1.25/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\ngithub.com/yuin/goldmark v1.1.27/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\ngithub.com/yuin/goldmark v1.1.32/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\ngithub.com/yuin/goldmark v1.2.1/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\ngithub.com/yuin/goldmark v1.3.5/go.mod h1:mwnBkeHKe2W/ZEtQ+71ViKU8L12m81fl3OWwC1Zlc8k=\ngithub.com/zenazn/goji v0.9.0/go.mod h1:7S9M489iMyHBNxwZnk9/EHS098H4/F6TATF2mIxtB1Q=\ngitlab.com/yawning/bsaes.git v0.0.0-20190805113838-0a714cd429ec h1:FpfFs4EhNehiVfzQttTuxanPIT43FtkkCFypIod8LHo=\ngitlab.com/yawning/bsaes.git v0.0.0-20190805113838-0a714cd429ec/go.mod h1:BZ1RAoRPbCxum9Grlv5aeksu2H8BiKehBYooU2LFiOQ=\ngo.etcd.io/bbolt v1.3.11 h1:yGEzV1wPz2yVCLsD8ZAiGHhHVlczyC9d1rP43/VCRJ0=\ngo.etcd.io/bbolt v1.3.11/go.mod h1:dksAq7YMXoljX0xu6VF5DMZGbhYYoLUalEiSySYAS4I=\ngo.etcd.io/etcd/api/v3 v3.5.7 h1:sbcmosSVesNrWOJ58ZQFitHMdncusIifYcrBfwrlJSY=\ngo.etcd.io/etcd/api/v3 v3.5.7/go.mod h1:9qew1gCdDDLu+VwmeG+iFpL+QlpHTo7iubavdVDgCAA=\ngo.etcd.io/etcd/client/pkg/v3 v3.5.7 h1:y3kf5Gbp4e4q7egZdn5T7W9TSHUvkClN6u+Rq9mEOmg=\ngo.etcd.io/etcd/client/pkg/v3 v3.5.7/go.mod h1:o0Abi1MK86iad3YrWhgUsbGx1pmTS+hrORWc2CamuhY=\ngo.etcd.io/etcd/client/v2 v2.305.7 h1:AELPkjNR3/igjbO7CjyF1fPuVPjrblliiKj+Y6xSGOU=\ngo.etcd.io/etcd/client/v2 v2.305.7/go.mod h1:GQGT5Z3TBuAQGvgPfhR7VPySu/SudxmEkRq9BgzFU6s=\ngo.etcd.io/etcd/client/v3 v3.5.7 h1:u/OhpiuCgYY8awOHlhIhmGIGpxfBU/GZBUP3m/3/Iz4=\ngo.etcd.io/etcd/client/v3 v3.5.7/go.mod h1:sOWmj9DZUMyAngS7QQwCyAXXAL6WhgTOPLNS/NabQgw=\ngo.etcd.io/etcd/pkg/v3 v3.5.7 h1:obOzeVwerFwZ9trMWapU/VjDcYUJb5OfgC1zqEGWO/0=\ngo.etcd.io/etcd/pkg/v3 v3.5.7/go.mod h1:kcOfWt3Ov9zgYdOiJ/o1Y9zFfLhQjylTgL4Lru8opRo=\ngo.etcd.io/etcd/raft/v3 v3.5.7 h1:aN79qxLmV3SvIq84aNTliYGmjwsW6NqJSnqmI1HLJKc=\ngo.etcd.io/etcd/raft/v3 v3.5.7/go.mod h1:TflkAb/8Uy6JFBxcRaH2Fr6Slm9mCPVdI2efzxY96yU=\ngo.etcd.io/etcd/server/v3 v3.5.7 h1:BTBD8IJUV7YFgsczZMHhMTS67XuA4KpRquL0MFOJGRk=\ngo.etcd.io/etcd/server/v3 v3.5.7/go.mod h1:gxBgT84issUVBRpZ3XkW1T55NjOb4vZZRI4wVvNhf4A=\ngo.opencensus.io v0.21.0/go.mod h1:mSImk1erAIZhrmZN+AvHh14ztQfjbGwt4TtuofqLduU=\ngo.opencensus.io v0.22.0/go.mod h1:+kGneAE2xo2IficOXnaByMWTGM9T73dGwxeWcUqIpI8=\ngo.opencensus.io v0.22.2/go.mod h1:yxeiOL68Rb0Xd1ddK5vPZ/oVn4vY4Ynel7k9FzqtOIw=\ngo.opencensus.io v0.22.3/go.mod h1:yxeiOL68Rb0Xd1ddK5vPZ/oVn4vY4Ynel7k9FzqtOIw=\ngo.opencensus.io v0.22.4/go.mod h1:yxeiOL68Rb0Xd1ddK5vPZ/oVn4vY4Ynel7k9FzqtOIw=\ngo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc v0.25.0 h1:Wx7nFnvCaissIUZxPkBqDz2963Z+Cl+PkYbDKzTxDqQ=\ngo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc v0.25.0/go.mod h1:E5NNboN0UqSAki0Atn9kVwaN7I+l25gGxDqBueo/74E=\ngo.opentelemetry.io/otel v1.0.1 h1:4XKyXmfqJLOQ7feyV5DB6gsBFZ0ltB8vLtp6pj4JIcc=\ngo.opentelemetry.io/otel v1.0.1/go.mod h1:OPEOD4jIT2SlZPMmwT6FqZz2C0ZNdQqiWcoK6M0SNFU=\ngo.opentelemetry.io/otel/exporters/otlp/otlptrace v1.0.1 h1:ofMbch7i29qIUf7VtF+r0HRF6ac0SBaPSziSsKp7wkk=\ngo.opentelemetry.io/otel/exporters/otlp/otlptrace v1.0.1/go.mod h1:Kv8liBeVNFkkkbilbgWRpV+wWuu+H5xdOT6HAgd30iw=\ngo.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc v1.0.1 h1:CFMFNoz+CGprjFAFy+RJFrfEe4GBia3RRm2a4fREvCA=\ngo.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc v1.0.1/go.mod h1:xOvWoTOrQjxjW61xtOmD/WKGRYb/P4NzRo3bs65U6Rk=\ngo.opentelemetry.io/otel/sdk v1.0.1 h1:wXxFEWGo7XfXupPwVJvTBOaPBC9FEg0wB8hMNrKk+cA=\ngo.opentelemetry.io/otel/sdk v1.0.1/go.mod h1:HrdXne+BiwsOHYYkBE5ysIcv2bvdZstxzmCQhxTcZkI=\ngo.opentelemetry.io/otel/trace v1.0.1 h1:StTeIH6Q3G4r0Fiw34LTokUFESZgIDUr0qIJ7mKmAfw=\ngo.opentelemetry.io/otel/trace v1.0.1/go.mod h1:5g4i4fKLaX2BQpSBsxw8YYcgKpMMSW3x7ZTuYBr3sUk=\ngo.opentelemetry.io/proto/otlp v0.7.0/go.mod h1:PqfVotwruBrMGOCsRd/89rSnXhoiJIqeYNgFYFoEGnI=\ngo.opentelemetry.io/proto/otlp v0.9.0 h1:C0g6TWmQYvjKRnljRULLWUVJGy8Uvu0NEL/5frY2/t4=\ngo.opentelemetry.io/proto/otlp v0.9.0/go.mod h1:1vKfU9rv61e9EVGthD1zNvUbiwPcimSsOPU9brfSHJg=\ngo.uber.org/atomic v1.3.2/go.mod h1:gD2HeocX3+yG+ygLZcrzQJaqmWj9AIm7n08wl/qW/PE=\ngo.uber.org/atomic v1.4.0/go.mod h1:gD2HeocX3+yG+ygLZcrzQJaqmWj9AIm7n08wl/qW/PE=\ngo.uber.org/atomic v1.5.0/go.mod h1:sABNBOSYdrvTF6hTgEIbc7YasKWGhgEQZyfxyTvoXHQ=\ngo.uber.org/atomic v1.6.0/go.mod h1:sABNBOSYdrvTF6hTgEIbc7YasKWGhgEQZyfxyTvoXHQ=\ngo.uber.org/atomic v1.7.0 h1:ADUqmZGgLDDfbSL9ZmPxKTybcoEYHgpYfELNoN+7hsw=\ngo.uber.org/atomic v1.7.0/go.mod h1:fEN4uk6kAWBTFdckzkM89CLk9XfWZrxpCo0nPH17wJc=\ngo.uber.org/goleak v1.1.12 h1:gZAh5/EyT/HQwlpkCy6wTpqfH9H8Lz8zbm3dZh+OyzA=\ngo.uber.org/goleak v1.1.12/go.mod h1:cwTWslyiVhfpKIDGSZEM2HlOvcqm+tG4zioyIeLoqMQ=\ngo.uber.org/multierr v1.1.0/go.mod h1:wR5kodmAFQ0UK8QlbwjlSNy0Z68gJhDJUG5sjR94q/0=\ngo.uber.org/multierr v1.3.0/go.mod h1:VgVr7evmIr6uPjLBxg28wmKNXyqE9akIJ5XnfpiKl+4=\ngo.uber.org/multierr v1.5.0/go.mod h1:FeouvMocqHpRaaGuG9EjoKcStLC43Zu/fmqdUMPcKYU=\ngo.uber.org/multierr v1.6.0 h1:y6IPFStTAIT5Ytl7/XYmHvzXQ7S3g/IeZW9hyZ5thw4=\ngo.uber.org/multierr v1.6.0/go.mod h1:cdWPpRnG4AhwMwsgIHip0KRBQjJy5kYEpYjJxpXp9iU=\ngo.uber.org/tools v0.0.0-20190618225709-2cfd321de3ee/go.mod h1:vJERXedbb3MVM5f9Ejo0C68/HhF8uaILCdgjnY+goOA=\ngo.uber.org/zap v1.9.1/go.mod h1:vwi/ZaCAaUcBkycHslxD9B2zi4UTXhF60s6SWpuDF0Q=\ngo.uber.org/zap v1.10.0/go.mod h1:vwi/ZaCAaUcBkycHslxD9B2zi4UTXhF60s6SWpuDF0Q=\ngo.uber.org/zap v1.13.0/go.mod h1:zwrFLgMcdUuIBviXEYEH1YKNaOBnKXsx2IPda5bBwHM=\ngo.uber.org/zap v1.17.0 h1:MTjgFu6ZLKvY6Pvaqk97GlxNBuMpV4Hy/3P6tRGlI2U=\ngo.uber.org/zap v1.17.0/go.mod h1:MXVU+bhUf/A7Xi2HNOnopQOrmycQ5Ih87HtOu4q5SSo=\ngolang.org/x/crypto v0.0.0-20170930174604-9419663f5a44/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=\ngolang.org/x/crypto v0.0.0-20180904163835-0709b304e793/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=\ngolang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\ngolang.org/x/crypto v0.0.0-20190411191339-88737f569e3a/go.mod h1:WFFai1msRO1wXaEeE5yQxYXgSfI8pQAWXbQop6sCtWE=\ngolang.org/x/crypto v0.0.0-20190510104115-cbcb75029529/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20190605123033-f99c8df09eb5/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20190820162420-60c769a6c586/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\ngolang.org/x/crypto v0.0.0-20201203163018-be400aefbc4c/go.mod h1:jdWPYTVW3xRLrWPugEBEK3UY2ZEsg3UU495nc5E+M+I=\ngolang.org/x/crypto v0.0.0-20210616213533-5ff15b29337e/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=\ngolang.org/x/crypto v0.0.0-20210711020723-a769d52b0f97/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=\ngolang.org/x/crypto v0.22.0 h1:g1v0xeRhjcugydODzvb3mEM9SQ0HGp9s/nh3COQ/C30=\ngolang.org/x/crypto v0.22.0/go.mod h1:vr6Su+7cTlO45qkww3VDJlzDn0ctJvRgYbC2NvXHt+M=\ngolang.org/x/exp v0.0.0-20190121172915-509febef88a4/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20190306152737-a1d7652674e8/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20190510132918-efd6b22b2522/go.mod h1:ZjyILWgesfNpC6sMxTJOJm9Kp84zZh5NQWvqDGG3Qr8=\ngolang.org/x/exp v0.0.0-20190829153037-c13cbed26979/go.mod h1:86+5VVa7VpoJ4kLfm080zCjGlMRFzhUhsZKEZO7MGek=\ngolang.org/x/exp v0.0.0-20191030013958-a1ab85dbe136/go.mod h1:JXzH8nQsPlswgeRAPE3MuO9GYsAcnJvJ4vnMwN/5qkY=\ngolang.org/x/exp v0.0.0-20191129062945-2f5052295587/go.mod h1:2RIsYlXP63K8oxa1u096TMicItID8zy7Y6sNkU49FU4=\ngolang.org/x/exp v0.0.0-20191227195350-da58074b4299/go.mod h1:2RIsYlXP63K8oxa1u096TMicItID8zy7Y6sNkU49FU4=\ngolang.org/x/exp v0.0.0-20200119233911-0405dc783f0a/go.mod h1:2RIsYlXP63K8oxa1u096TMicItID8zy7Y6sNkU49FU4=\ngolang.org/x/exp v0.0.0-20200207192155-f17229e696bd/go.mod h1:J/WKrq2StrnmMY6+EHIKF9dgMWnmCNThgcyBT1FY9mM=\ngolang.org/x/exp v0.0.0-20200224162631-6cc2880d07d6/go.mod h1:3jZMyOhIsHpP37uCMkUooju7aAi5cS1Q23tOzKc+0MU=\ngolang.org/x/exp v0.0.0-20240325151524-a685a6edb6d8 h1:aAcj0Da7eBAtrTp03QXWvm88pSyOt+UgdZw2BFZ+lEw=\ngolang.org/x/exp v0.0.0-20240325151524-a685a6edb6d8/go.mod h1:CQ1k9gNrJ50XIzaKCRR2hssIjF07kZFEiieALBM/ARQ=\ngolang.org/x/image v0.0.0-20190227222117-0694c2d4d067/go.mod h1:kZ7UVZpmo3dzQBMxlp+ypCbDeSB+sBbTgSJuh5dn5js=\ngolang.org/x/image v0.0.0-20190802002840-cff245a6509b/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\ngolang.org/x/lint v0.0.0-20181026193005-c67002cb31c3/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=\ngolang.org/x/lint v0.0.0-20190227174305-5b3e6a55c961/go.mod h1:wehouNa3lNwaWXcvxsM5YxQ5yQlVC4a0KAMCusXpPoU=\ngolang.org/x/lint v0.0.0-20190301231843-5614ed5bae6f/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=\ngolang.org/x/lint v0.0.0-20190313153728-d0100b6bd8b3/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/lint v0.0.0-20190409202823-959b441ac422/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/lint v0.0.0-20190909230951-414d861bb4ac/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/lint v0.0.0-20190930215403-16217165b5de/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/lint v0.0.0-20191125180803-fdd1cda4f05f/go.mod h1:5qLYkcX4OjUUV8bRuDixDT3tpyyb+LUpUlRWLxfhWrs=\ngolang.org/x/lint v0.0.0-20200130185559-910be7a94367/go.mod h1:3xt1FjdF8hUf6vQPIChWIBhFzV8gjjsPE/fR3IyQdNY=\ngolang.org/x/lint v0.0.0-20200302205851-738671d3881b/go.mod h1:3xt1FjdF8hUf6vQPIChWIBhFzV8gjjsPE/fR3IyQdNY=\ngolang.org/x/lint v0.0.0-20210508222113-6edffad5e616/go.mod h1:3xt1FjdF8hUf6vQPIChWIBhFzV8gjjsPE/fR3IyQdNY=\ngolang.org/x/mobile v0.0.0-20190312151609-d3739f865fa6/go.mod h1:z+o9i4GpDbdi3rU15maQ/Ox0txvL9dWGYEHz965HBQE=\ngolang.org/x/mobile v0.0.0-20190719004257-d2bd2a29d028 h1:4+4C/Iv2U4fMZBiMCc98MG1In4gJY5YRhtpDNeDeHWs=\ngolang.org/x/mobile v0.0.0-20190719004257-d2bd2a29d028/go.mod h1:E/iHnbuqvinMTCcRqshq8CkpyQDoeVncDDYHnLhea+o=\ngolang.org/x/mod v0.0.0-20190513183733-4bf6d317e70e/go.mod h1:mXi4GBBbnImb6dmsKGUJ2LatrhH/nqhxcFungHvyanc=\ngolang.org/x/mod v0.1.0/go.mod h1:0QHyrYULN0/3qlju5TqG8bIK38QM8yzMo5ekMj3DlcY=\ngolang.org/x/mod v0.1.1-0.20191105210325-c90efee705ee/go.mod h1:QqPTAvyqsEbceGzBzNggFXnrqF1CaUcvgkdR5Ot7KZg=\ngolang.org/x/mod v0.1.1-0.20191107180719-034126e5016b/go.mod h1:QqPTAvyqsEbceGzBzNggFXnrqF1CaUcvgkdR5Ot7KZg=\ngolang.org/x/mod v0.2.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/mod v0.3.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/mod v0.4.2/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/mod v0.16.0 h1:QX4fJ0Rr5cPQCF7O9lh9Se4pmwfwskqZfq5moyldzic=\ngolang.org/x/mod v0.16.0/go.mod h1:hTbmBsO62+eylJbnUtE2MGJUyE7QWk4xUqPFrRgJ+7c=\ngolang.org/x/net v0.0.0-20180719180050-a680a1efc54d/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20180724234803-3673e40ba225/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20180826012351-8a410e7b638d/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20180906233101-161cd47e91fd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20181114220301-adae6a3d119a/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190108225652-1e06a53dbb7e/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190213061140-3a22650c66bd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190501004415-9ce7a6920f09/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190503192946-f4e77d36d62c/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190603091049-60506f45cf65/go.mod h1:HSz+uSET+XFnRR8LxR5pz3Of3rY3CfYBVs4xY44aLks=\ngolang.org/x/net v0.0.0-20190613194153-d28f0bde5980/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20190628185345-da137c7871d7/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20190724013045-ca1201d0de80/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20190813141303-74dc4d7220e7/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20191209160850-c0dbc17a3553/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200114155413-6afb5195e5aa/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200202094626-16171245cfb2/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200222125558-5a598a2470a0/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200226121028-0de0cce0169b/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200301022130-244492dfa37a/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200324143707-d3edc9973b7e/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200501053045-e0ff5e5a1de5/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200506145744-7e3656a0809f/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200513185701-a91f0712d120/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200520004742-59133d7f0dd7/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200520182314-0ba52f642ac2/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200625001655-4c5254603344/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\ngolang.org/x/net v0.0.0-20200707034311-ab3426394381/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\ngolang.org/x/net v0.0.0-20200813134508-3edf25e44fcc/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\ngolang.org/x/net v0.0.0-20200822124328-c89045814202/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\ngolang.org/x/net v0.0.0-20201021035429-f5854403a974/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=\ngolang.org/x/net v0.0.0-20201202161906-c7110b5ffcbb/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=\ngolang.org/x/net v0.0.0-20210226172049-e18ecbb05110/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=\ngolang.org/x/net v0.0.0-20210405180319-a5a99cb37ef4/go.mod h1:p54w0d4576C0XHj96bSt6lcn1PtDYWL6XObtHCRCNQM=\ngolang.org/x/net v0.24.0 h1:1PcaxkF854Fu3+lvBIx5SYn9wRlBzzcnHZSiaFFAb0w=\ngolang.org/x/net v0.24.0/go.mod h1:2Q7sJY5mzlzWjKtYUEXSlBWCdyaioyXzRB2RtU8KVE8=\ngolang.org/x/oauth2 v0.0.0-20180821212333-d2e6202438be/go.mod h1:N/0e6XlmueqKjAGxoOufVs8QHGRruUQn6yWY3a++T0U=\ngolang.org/x/oauth2 v0.0.0-20190226205417-e64efc72b421/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/oauth2 v0.0.0-20190604053449-0f29369cfe45/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/oauth2 v0.0.0-20191202225959-858c2ad4c8b6/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/oauth2 v0.0.0-20200107190931-bf48bf16ab8d/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/oauth2 v0.0.0-20210615190721-d04028783cf1/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\ngolang.org/x/oauth2 v0.14.0 h1:P0Vrf/2538nmC0H+pEQ3MNFRRnVR7RlqyVw+bvm26z0=\ngolang.org/x/oauth2 v0.14.0/go.mod h1:lAtNWgaWfL4cm7j2OV8TxGi9Qb7ECORx8DktCY74OwM=\ngolang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20181108010431-42b317875d0f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190227155943-e225da77a7e6/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190911185100-cd5d95a43a6e/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20200317015054-43a5402ce75a/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20200625203802-6e8e738ad208/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20201020160332-67f06af15bc9/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20201207232520-09787c993a3a/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20210220032951-036812b2e83c/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.7.0 h1:YsImfSBoP9QPYL0xyKJPq0gcaJdG3rInoqxTWbfQu9M=\ngolang.org/x/sync v0.7.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=\ngolang.org/x/sys v0.0.0-20180816055513-1c9583448a9c/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20180830151530-49385e6e1522/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20180905080454-ebe1bf3edb33/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20180909124046-d0be0721c37e/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20181116152217-5ac8a444bdc5/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190222072716-a9d3bda3a223/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190312061237-fead79001313/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190403152447-81d4e9dc473e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190422165155-953cdadca894/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190502145724-3ef323f4f1fd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190507160741-ecd444e8653b/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190606165138-5da285871e9c/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190624142023-c5567b49c5d0/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190726091711-fc99dfbffb4e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190804053845-51ab0e2deafa/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190813064441-fde4db37ae7a/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190904154756-749cb33beabd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191001151750-bb3f8db39f24/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191005200804-aed5e4c7ecf9/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191026070338-33540a1f6037/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191120155948-bd437916bb0e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191204072324-ce4227a45e2e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191228213918-04cbcbbfeed8/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200106162015-b016eb3dc98e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200113162924-86b910548bc1/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200116001909-b77594299b42/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200122134326-e047566fdf82/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200202164722-d101bd2416d5/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200212091648-12a6c2dcc1e4/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200223170610-d5e6a3e2c0ae/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200302150141-5c8b2ff67527/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200323222414-85ca7c5b95cd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200331124033-c3d80250170d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200501052902-10377860bb8e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200511232937-7e40ca221e25/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200515095857-1151b9dac4a9/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200519105757-fe76b779f299/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200523222454-059865788121/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200615200032-f1bc736245b1/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200625212154-ddb9806d33ae/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200803210538-64077c9b5642/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200814200057-3d37ad5750ed/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200930185726-fdedc70b468f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20201119102817-f84b799fce68/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210124154548-22da62e12c0c/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210303074136-134d130e1a04/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210330210617-4fbd30eecc44/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210423185535-09eb48e85fd7/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210510120138-977fb7262007/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20210603081109-ebe580a85c40/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20210615035016-665e8c7367d1/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20210616094352-59db8d763f22/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220412211240-33da011f77ad/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220715151400-c0bba94af5f8/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.6.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.19.0 h1:q5f1RH2jigJ1MoAWp2KTp3gm5zAGFUTarQZ5U386+4o=\ngolang.org/x/sys v0.19.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\ngolang.org/x/term v0.0.0-20201117132131-f5c789dd3221/go.mod h1:Nr5EML6q2oocZ2LXRh80K7BxOlk5/8JxuGnuhpl+muw=\ngolang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=\ngolang.org/x/term v0.19.0 h1:+ThwsDv+tYfnJFhF4L8jITxu1tdTWRTZpdsWgEgjL6Q=\ngolang.org/x/term v0.19.0/go.mod h1:2CuTdWZ7KHSQwUzKva0cbMg6q2DMI3Mmxp+gKJbskEk=\ngolang.org/x/text v0.0.0-20170915032832-14c0d48ead0c/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.1-0.20180807135948-17ff2d5776d2/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=\ngolang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.4/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.5/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.6/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.14.0 h1:ScX5w1eTa3QqT8oi6+ziP7dTV1S2+ALU0bI+0zXKWiQ=\ngolang.org/x/text v0.14.0/go.mod h1:18ZOQIKpY8NJVqYksKHtTdi31H5itFRjB5/qKTNYzSU=\ngolang.org/x/time v0.0.0-20181108054448-85acf8d2951c/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\ngolang.org/x/time v0.0.0-20190308202827-9d24e82272b4/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\ngolang.org/x/time v0.0.0-20191024005414-555d28b269f0/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\ngolang.org/x/time v0.3.0 h1:rg5rLMjNzMS1RkNLzCG38eapWhnYLFYXDXj2gOlr8j4=\ngolang.org/x/time v0.3.0/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\ngolang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20190114222345-bf090417da8b/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20190226205152-f727befe758c/go.mod h1:9Yl7xja0Znq3iFh3HoIrodX9oNMXvdceNzlUR8zjMvY=\ngolang.org/x/tools v0.0.0-20190311212946-11955173bddd/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190312151545-0bb0c0a6e846/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190312170243-e65039ee4138/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190425150028-36563e24a262/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\ngolang.org/x/tools v0.0.0-20190425163242-31fd60d6bfdc/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\ngolang.org/x/tools v0.0.0-20190506145303-2d16b83fe98c/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\ngolang.org/x/tools v0.0.0-20190524140312-2c0ae7006135/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\ngolang.org/x/tools v0.0.0-20190606124116-d0a3d012864b/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\ngolang.org/x/tools v0.0.0-20190621195816-6e04913cbbac/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\ngolang.org/x/tools v0.0.0-20190628153133-6cdbf07be9d0/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\ngolang.org/x/tools v0.0.0-20190816200558-6889da9d5479/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20190823170909-c4a336ef6a2f/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20190911174233-4f2ddba30aff/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191012152004-8de300cfc20a/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191029041327-9cc4af7d6b2c/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191029190741-b9c20aec41a5/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191113191852-77e3bb0ad9e7/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191115202509-3a792d9c32b2/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191125144606-a911d9008d1f/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191130070609-6e064ea0cf2d/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191216173652-a0e659d51361/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20191227053925-7b8e75db28f4/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200103221440-774c71fcf114/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200117161641-43d50277825c/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200122220014-bf1340f18c4a/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200130002326-2f3ba24bd6e7/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200204074204-1cc6d1ef6c74/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200207183749-b753a1ba74fa/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200212150539-ea181f53ac56/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200224181240-023911ca70b2/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200227222343-706bc42d1f0d/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200304193943-95d2e580d8eb/go.mod h1:o4KQGtdN14AW+yjsvvwRTJJuXz8XRtIHtEnmAXLyFUw=\ngolang.org/x/tools v0.0.0-20200312045724-11d5b4c81c7d/go.mod h1:o4KQGtdN14AW+yjsvvwRTJJuXz8XRtIHtEnmAXLyFUw=\ngolang.org/x/tools v0.0.0-20200331025713-a30bf2db82d4/go.mod h1:Sl4aGygMT6LrqrWclx+PTx3U+LnKx/seiNR+3G19Ar8=\ngolang.org/x/tools v0.0.0-20200501065659-ab2804fb9c9d/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20200512131952-2bc93b1c0c88/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20200515010526-7d3b6ebf133d/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20200618134242-20370b0cb4b2/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20200619180055-7c47624df98f/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20200729194436-6467de6f59a7/go.mod h1:njjCfa9FT2d7l9Bc6FUM5FLjQPp3cFF28FI3qnDFljA=\ngolang.org/x/tools v0.0.0-20200804011535-6c149bb5ef0d/go.mod h1:njjCfa9FT2d7l9Bc6FUM5FLjQPp3cFF28FI3qnDFljA=\ngolang.org/x/tools v0.0.0-20200825202427-b303f430e36d/go.mod h1:njjCfa9FT2d7l9Bc6FUM5FLjQPp3cFF28FI3qnDFljA=\ngolang.org/x/tools v0.0.0-20210106214847-113979e3529a/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=\ngolang.org/x/tools v0.1.3/go.mod h1:o0xws9oXOQQZyjljx8fwUC0k7L1pTE6eaCbjGeHmOkk=\ngolang.org/x/tools v0.19.0 h1:tfGCXNR1OsFG+sVdLAitlpjAvD/I6dHDKnYrpEZUHkw=\ngolang.org/x/tools v0.19.0/go.mod h1:qoJWxmGSIBmAeriMx19ogtrEPrGtDbPK634QFIcLAhc=\ngolang.org/x/xerrors v0.0.0-20190410155217-1f06c39b4373/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20190513163551-3ee3066db522/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191011141410-1b5146add898/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngoogle.golang.org/api v0.4.0/go.mod h1:8k5glujaEP+g9n7WNsDg8QP6cUVNI86fCNMcbazEtwE=\ngoogle.golang.org/api v0.7.0/go.mod h1:WtwebWUNSVBH/HAw79HIFXZNqEvBhG+Ra+ax0hx3E3M=\ngoogle.golang.org/api v0.8.0/go.mod h1:o4eAsZoiT+ibD93RtjEohWalFOjRDx6CVaqeizhEnKg=\ngoogle.golang.org/api v0.9.0/go.mod h1:o4eAsZoiT+ibD93RtjEohWalFOjRDx6CVaqeizhEnKg=\ngoogle.golang.org/api v0.13.0/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=\ngoogle.golang.org/api v0.14.0/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=\ngoogle.golang.org/api v0.15.0/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=\ngoogle.golang.org/api v0.17.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\ngoogle.golang.org/api v0.18.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\ngoogle.golang.org/api v0.19.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\ngoogle.golang.org/api v0.20.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\ngoogle.golang.org/api v0.22.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\ngoogle.golang.org/api v0.24.0/go.mod h1:lIXQywCXRcnZPGlsd8NbLnOjtAoL6em04bJ9+z0MncE=\ngoogle.golang.org/api v0.28.0/go.mod h1:lIXQywCXRcnZPGlsd8NbLnOjtAoL6em04bJ9+z0MncE=\ngoogle.golang.org/api v0.29.0/go.mod h1:Lcubydp8VUV7KeIHD9z2Bys/sm/vGKnG1UHuDBSrHWM=\ngoogle.golang.org/api v0.30.0/go.mod h1:QGmEvQ87FHZNiUVJkT14jQNYJ4ZJjdRF23ZXz5138Fc=\ngoogle.golang.org/appengine v1.1.0/go.mod h1:EbEs0AVv82hx2wNQdGPgUI5lhzA/G0D9YwlJXL52JkM=\ngoogle.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\ngoogle.golang.org/appengine v1.5.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\ngoogle.golang.org/appengine v1.6.1/go.mod h1:i06prIuMbXzDqacNJfV5OdTW448YApPu5ww/cMBSeb0=\ngoogle.golang.org/appengine v1.6.5/go.mod h1:8WjMMxjGQR8xUklV/ARdw2HLXBOI7O7uCIDZVag1xfc=\ngoogle.golang.org/appengine v1.6.6/go.mod h1:8WjMMxjGQR8xUklV/ARdw2HLXBOI7O7uCIDZVag1xfc=\ngoogle.golang.org/appengine v1.6.7 h1:FZR1q0exgwxzPzp/aF+VccGrSfxfPpkBqjIIEq3ru6c=\ngoogle.golang.org/appengine v1.6.7/go.mod h1:8WjMMxjGQR8xUklV/ARdw2HLXBOI7O7uCIDZVag1xfc=\ngoogle.golang.org/genproto v0.0.0-20180817151627-c66870c02cf8/go.mod h1:JiN7NxoALGmiZfu7CAH4rXhgtRTLTxftemlI0sWmxmc=\ngoogle.golang.org/genproto v0.0.0-20190307195333-5fe7a883aa19/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\ngoogle.golang.org/genproto v0.0.0-20190418145605-e7d98fc518a7/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\ngoogle.golang.org/genproto v0.0.0-20190425155659-357c62f0e4bb/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\ngoogle.golang.org/genproto v0.0.0-20190502173448-54afdca5d873/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\ngoogle.golang.org/genproto v0.0.0-20190801165951-fa694d86fc64/go.mod h1:DMBHOl98Agz4BDEuKkezgsaosCRResVns1a3J2ZsMNc=\ngoogle.golang.org/genproto v0.0.0-20190819201941-24fa4b261c55/go.mod h1:DMBHOl98Agz4BDEuKkezgsaosCRResVns1a3J2ZsMNc=\ngoogle.golang.org/genproto v0.0.0-20190911173649-1774047e7e51/go.mod h1:IbNlFCBrqXvoKpeg0TB2l7cyZUmoaFKYIwrEpbDKLA8=\ngoogle.golang.org/genproto v0.0.0-20191108220845-16a3f7862a1a/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20191115194625-c23dd37a84c9/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20191216164720-4f79533eabd1/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20191230161307-f3c370f40bfb/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20200115191322-ca5a22157cba/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20200122232147-0452cf42e150/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20200204135345-fa8e72b47b90/go.mod h1:GmwEX6Z4W5gMy59cAlVYjN9JhxgbQH6Gn+gFDQe2lzA=\ngoogle.golang.org/genproto v0.0.0-20200212174721-66ed5ce911ce/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200224152610-e50cd9704f63/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200228133532-8c2c7df3a383/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200305110556-506484158171/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200312145019-da6875a35672/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200331122359-1ee6d9798940/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200423170343-7949de9c1215/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200430143042-b979b6f78d84/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200511104702-f5ebc3bea380/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200513103714-09dca8ec2884/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200515170657-fc4c6c6a6587/go.mod h1:YsZOwe1myG/8QRHRsmBRE1LrgQY60beZKjly0O1fX9U=\ngoogle.golang.org/genproto v0.0.0-20200526211855-cb27e3aa2013/go.mod h1:NbSheEEYHJ7i3ixzK3sjbqSGDJWnxyFXZblF3eUsNvo=\ngoogle.golang.org/genproto v0.0.0-20200618031413-b414f8b61790/go.mod h1:jDfRM7FcilCzHH/e9qn6dsT145K34l5v+OpcnNgKAAA=\ngoogle.golang.org/genproto v0.0.0-20200729003335-053ba62fc06f/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20200804131852-c06518451d9c/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20200825200019-8632dd797987/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20210617175327-b9e0b3197ced/go.mod h1:SzzZ/N+nwJDaO1kznhnlzqS8ocJICar6hYhVyhi++24=\ngoogle.golang.org/genproto v0.0.0-20231016165738-49dd2c1f3d0b h1:+YaDE2r2OG8t/z5qmsh7Y+XXwCbvadxxZ0YY6mTdrVA=\ngoogle.golang.org/genproto v0.0.0-20231016165738-49dd2c1f3d0b/go.mod h1:CgAqfJo+Xmu0GwA0411Ht3OU3OntXwsGmrmjI8ioGXI=\ngoogle.golang.org/genproto/googleapis/api v0.0.0-20231016165738-49dd2c1f3d0b h1:CIC2YMXmIhYw6evmhPxBKJ4fmLbOFtXQN/GV3XOZR8k=\ngoogle.golang.org/genproto/googleapis/api v0.0.0-20231016165738-49dd2c1f3d0b/go.mod h1:IBQ646DjkDkvUIsVq/cc03FUFQ9wbZu7yE396YcL870=\ngoogle.golang.org/genproto/googleapis/rpc v0.0.0-20231030173426-d783a09b4405 h1:AB/lmRny7e2pLhFEYIbl5qkDAUt2h0ZRO4wGPhZf+ik=\ngoogle.golang.org/genproto/googleapis/rpc v0.0.0-20231030173426-d783a09b4405/go.mod h1:67X1fPuzjcrkymZzZV1vvkFeTn2Rvc6lYF9MYFGCcwE=\ngoogle.golang.org/grpc v1.19.0/go.mod h1:mqu4LbDTu4XGKhr4mRzUsmM4RtVoemTSY81AxZiDr8c=\ngoogle.golang.org/grpc v1.20.1/go.mod h1:10oTOabMzJvdu6/UiuZezV6QK5dSlG84ov/aaiqXj38=\ngoogle.golang.org/grpc v1.21.1/go.mod h1:oYelfM1adQP15Ek0mdvEgi9Df8B9CZIaU1084ijfRaM=\ngoogle.golang.org/grpc v1.23.0/go.mod h1:Y5yQAOtifL1yxbo5wqy6BxZv8vAUGQwXBOALyacEbxg=\ngoogle.golang.org/grpc v1.25.1/go.mod h1:c3i+UQWmh7LiEpx4sFZnkU36qjEYZ0imhYfXVyQciAY=\ngoogle.golang.org/grpc v1.26.0/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=\ngoogle.golang.org/grpc v1.27.0/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=\ngoogle.golang.org/grpc v1.27.1/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=\ngoogle.golang.org/grpc v1.28.0/go.mod h1:rpkK4SK4GF4Ach/+MFLZUBavHOvF2JJB5uozKKal+60=\ngoogle.golang.org/grpc v1.29.1/go.mod h1:itym6AZVZYACWQqET3MqgPpjcuV5QH3BxFS3IjizoKk=\ngoogle.golang.org/grpc v1.30.0/go.mod h1:N36X2cJ7JwdamYAgDz+s+rVMFjt3numwzf/HckM8pak=\ngoogle.golang.org/grpc v1.31.0/go.mod h1:N36X2cJ7JwdamYAgDz+s+rVMFjt3numwzf/HckM8pak=\ngoogle.golang.org/grpc v1.33.1/go.mod h1:fr5YgcSWrqhRRxogOsw7RzIpsmvOZ6IcH4kBYTpR3n0=\ngoogle.golang.org/grpc v1.36.0/go.mod h1:qjiiYl8FncCW8feJPdyg3v6XW24KsRHe+dy9BAGRRjU=\ngoogle.golang.org/grpc v1.37.1/go.mod h1:NREThFqKR1f3iQ6oBuvc5LadQuXVGo9rkm5ZGrQdJfM=\ngoogle.golang.org/grpc v1.38.0/go.mod h1:NREThFqKR1f3iQ6oBuvc5LadQuXVGo9rkm5ZGrQdJfM=\ngoogle.golang.org/grpc v1.40.0/go.mod h1:ogyxbiOoUXAkP+4+xa6PZSE9DZgIHtSpzjDTB9KAK34=\ngoogle.golang.org/grpc v1.41.0/go.mod h1:U3l9uK9J0sini8mHphKoXyaqDA/8VyGnDee1zzIUK6k=\ngoogle.golang.org/grpc v1.59.0 h1:Z5Iec2pjwb+LEOqzpB2MR12/eKFhDPhuqW91O+4bwUk=\ngoogle.golang.org/grpc v1.59.0/go.mod h1:aUPDwccQo6OTjy7Hct4AfBPD1GptF4fyUjIkQ9YtF98=\ngopkg.in/alecthomas/kingpin.v2 v2.2.6/go.mod h1:FMv+mEhP44yOT+4EoQTLFTRgOQ1FBLkstjWtayDeSgw=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20160105164936-4f90aeace3a2/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=\ngopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=\ngopkg.in/errgo.v1 v1.0.1 h1:oQFRXzZ7CkBGdm1XZm/EbQYaYNNEElNBOd09M6cqNso=\ngopkg.in/errgo.v1 v1.0.1/go.mod h1:3NjfXwocQRYAPTq4/fzX+CwUhPRcR/azYRhj8G+LqMo=\ngopkg.in/errgo.v2 v2.1.0/go.mod h1:hNsd1EY+bozCKY1Ytp96fpM3vjJbqLJn88ws8XvfDNI=\ngopkg.in/fsnotify.v1 v1.4.7/go.mod h1:Tz8NjZHkW78fSQdbUxIjBTcgA1z1m8ZHf0WmKUhAMys=\ngopkg.in/inconshreveable/log15.v2 v2.0.0-20180818164646-67afb5ed74ec/go.mod h1:aPpfJ7XW+gOuirDoZ8gHhLh3kZ1B08FtV2bbmy7Jv3s=\ngopkg.in/macaroon-bakery.v2 v2.0.1 h1:0N1TlEdfLP4HXNCg7MQUMp5XwvOoxk+oe9Owr2cpvsc=\ngopkg.in/macaroon-bakery.v2 v2.0.1/go.mod h1:B4/T17l+ZWGwxFSZQmlBwp25x+og7OkhETfr3S9MbIA=\ngopkg.in/macaroon.v2 v2.0.0 h1:LVWycAfeJBUjCIqfR9gqlo7I8vmiXRr51YEOZ1suop8=\ngopkg.in/macaroon.v2 v2.0.0/go.mod h1:+I6LnTMkm/uV5ew/0nsulNjL16SK4+C8yDmRUzHR17I=\ngopkg.in/natefinch/lumberjack.v2 v2.0.0 h1:1Lc07Kr7qY4U2YPouBjpCLxpiyxIVoxqXgkXLknAOE8=\ngopkg.in/natefinch/lumberjack.v2 v2.0.0/go.mod h1:l0ndWWf7gzL7RNwBG7wST/UCcT4T24xpD6X8LsfU/+k=\ngopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7 h1:uRGJdciOHaEIrze2W8Q3AKkepLTh2hOroT7a+7czfdQ=\ngopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7/go.mod h1:dt/ZhP58zS4L8KSrWDmTeBkI65Dw0HsyUHuEVlX15mw=\ngopkg.in/yaml.v2 v2.2.1/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.3/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.4/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.5/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.8/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.3.0/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.4.0 h1:D8xgwECY7CYvx+Y2n4sBz93Jn9JRvxdiyyo8CTfuKaY=\ngopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=\ngopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngopkg.in/yaml.v3 v3.0.0-20210107192922-496545a6307b/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngotest.tools/v3 v3.3.0 h1:MfDY1b1/0xN1CyMlQDac0ziEy9zJQd9CXBRRDHw2jJo=\ngotest.tools/v3 v3.3.0/go.mod h1:Mcr9QNxkg0uMvy/YElmo4SpXgJKWgQvYrT7Kw5RzJ1A=\nhonnef.co/go/tools v0.0.0-20190102054323-c2f93a96b099/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.0-20190106161140-3f1c8253044a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.0-20190418001031-e561f6794a2a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.0-20190523083050-ea95bdfd59fc/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.1-2019.2.3/go.mod h1:a3bituU0lyd329TUQxRnasdCoJDkEUEAqEt0JzvZhAg=\nhonnef.co/go/tools v0.0.1-2020.1.3/go.mod h1:X/FiERA/W4tHapMX5mGpAtMSVEeEUOyHaw9vFzvIQ3k=\nhonnef.co/go/tools v0.0.1-2020.1.4/go.mod h1:X/FiERA/W4tHapMX5mGpAtMSVEeEUOyHaw9vFzvIQ3k=\nmodernc.org/cc/v4 v4.20.0 h1:45Or8mQfbUqJOG9WaxvlFYOAQO0lQ5RvqBcFCXngjxk=\nmodernc.org/cc/v4 v4.20.0/go.mod h1:HM7VJTZbUCR3rV8EYBi9wxnJ0ZBRiGE5OeGXNA0IsLQ=\nmodernc.org/ccgo/v4 v4.16.0 h1:ofwORa6vx2FMm0916/CkZjpFPSR70VwTjUCe2Eg5BnA=\nmodernc.org/ccgo/v4 v4.16.0/go.mod h1:dkNyWIjFrVIZ68DTo36vHK+6/ShBn4ysU61So6PIqCI=\nmodernc.org/fileutil v1.3.0 h1:gQ5SIzK3H9kdfai/5x41oQiKValumqNTDXMvKo62HvE=\nmodernc.org/fileutil v1.3.0/go.mod h1:XatxS8fZi3pS8/hKG2GH/ArUogfxjpEKs3Ku3aK4JyQ=\nmodernc.org/gc/v2 v2.4.1 h1:9cNzOqPyMJBvrUipmynX0ZohMhcxPtMccYgGOJdOiBw=\nmodernc.org/gc/v2 v2.4.1/go.mod h1:wzN5dK1AzVGoH6XOzc3YZ+ey/jPgYHLuVckd62P0GYU=\nmodernc.org/gc/v3 v3.0.0-20240107210532-573471604cb6 h1:5D53IMaUuA5InSeMu9eJtlQXS2NxAhyWQvkKEgXZhHI=\nmodernc.org/gc/v3 v3.0.0-20240107210532-573471604cb6/go.mod h1:Qz0X07sNOR1jWYCrJMEnbW/X55x206Q7Vt4mz6/wHp4=\nmodernc.org/libc v1.49.3 h1:j2MRCRdwJI2ls/sGbeSk0t2bypOG/uvPZUsGQFDulqg=\nmodernc.org/libc v1.49.3/go.mod h1:yMZuGkn7pXbKfoT/M35gFJOAEdSKdxL0q64sF7KqCDo=\nmodernc.org/mathutil v1.6.0 h1:fRe9+AmYlaej+64JsEEhoWuAYBkOtQiMEU7n/XgfYi4=\nmodernc.org/mathutil v1.6.0/go.mod h1:Ui5Q9q1TR2gFm0AQRqQUaBWFLAhQpCwNcuhBOSedWPo=\nmodernc.org/memory v1.8.0 h1:IqGTL6eFMaDZZhEWwcREgeMXYwmW83LYW8cROZYkg+E=\nmodernc.org/memory v1.8.0/go.mod h1:XPZ936zp5OMKGWPqbD3JShgd/ZoQ7899TUuQqxY+peU=\nmodernc.org/opt v0.1.3 h1:3XOZf2yznlhC+ibLltsDGzABUGVx8J6pnFMS3E4dcq4=\nmodernc.org/opt v0.1.3/go.mod h1:WdSiB5evDcignE70guQKxYUl14mgWtbClRi5wmkkTX0=\nmodernc.org/sortutil v1.2.0 h1:jQiD3PfS2REGJNzNCMMaLSp/wdMNieTbKX920Cqdgqc=\nmodernc.org/sortutil v1.2.0/go.mod h1:TKU2s7kJMf1AE84OoiGppNHJwvB753OYfNl2WRb++Ss=\nmodernc.org/sqlite v1.29.10 h1:3u93dz83myFnMilBGCOLbr+HjklS6+5rJLx4q86RDAg=\nmodernc.org/sqlite v1.29.10/go.mod h1:ItX2a1OVGgNsFh6Dv60JQvGfJfTPHPVpV6DF59akYOA=\nmodernc.org/strutil v1.2.0 h1:agBi9dp1I+eOnxXeiZawM8F4LawKv4NzGWSaLfyeNZA=\nmodernc.org/strutil v1.2.0/go.mod h1:/mdcBmfOibveCTBxUl5B5l6W+TTH1FXPLHZE6bTosX0=\nmodernc.org/token v1.1.0 h1:Xl7Ap9dKaEs5kLoOQeQmPWevfnk/DM5qcLcYlA8ys6Y=\nmodernc.org/token v1.1.0/go.mod h1:UGzOrNV1mAFSEB63lOFHIpNRUVMvYTc6yu1SMY/XTDM=\npgregory.net/rapid v1.1.0 h1:CMa0sjHSru3puNx+J0MIAuiiEV4N0qj8/cMWGBBCsjw=\npgregory.net/rapid v1.1.0/go.mod h1:PY5XlDGj0+V1FCq0o192FdRhpKHGTRIWBgqjDBTrq04=\nrsc.io/binaryregexp v0.2.0/go.mod h1:qTv7/COck+e2FymRvadv62gMdZztPaShugOCi3I+8D8=\nrsc.io/quote/v3 v3.1.0/go.mod h1:yEA65RcK8LyAZtP9Kv3t0HmxON59tX3rD+tICJqUlj0=\nrsc.io/sampler v1.3.0/go.mod h1:T1hPZKmBbMNahiBKFy5HrXp6adAjACjK9JXDnKaTXpA=\nsigs.k8s.io/yaml v1.2.0 h1:kr/MCeFWJWTwyaHoR9c8EjH9OumOmoF9YGiZd7lFm/Q=\nsigs.k8s.io/yaml v1.2.0/go.mod h1:yfXDCHCao9+ENCvLSE62v9VSji2MKu5jeNfTrofGhJc=\n"
        },
        {
          "name": "graph",
          "type": "tree",
          "content": null
        },
        {
          "name": "healthcheck",
          "type": "tree",
          "content": null
        },
        {
          "name": "htlcswitch",
          "type": "tree",
          "content": null
        },
        {
          "name": "input",
          "type": "tree",
          "content": null
        },
        {
          "name": "intercepted_forward.go",
          "type": "blob",
          "size": 2.810546875,
          "content": "package lnd\n\nimport (\n\t\"errors\"\n\n\t\"github.com/lightningnetwork/lnd/fn/v2\"\n\t\"github.com/lightningnetwork/lnd/htlcswitch\"\n\t\"github.com/lightningnetwork/lnd/lntypes\"\n\t\"github.com/lightningnetwork/lnd/lnwire\"\n)\n\nvar (\n\t// ErrCannotResume is returned when an intercepted forward cannot be\n\t// resumed. This is the case in the on-chain resolution flow.\n\tErrCannotResume = errors.New(\"cannot resume in the on-chain flow\")\n\n\t// ErrCannotFail is returned when an intercepted forward cannot be failed.\n\t// This is the case in the on-chain resolution flow.\n\tErrCannotFail = errors.New(\"cannot fail in the on-chain flow\")\n\n\t// ErrPreimageMismatch is returned when the preimage that is specified to\n\t// settle an htlc doesn't match the htlc hash.\n\tErrPreimageMismatch = errors.New(\"preimage does not match hash\")\n)\n\n// interceptedForward implements the on-chain behavior for the resolution of\n// a forwarded htlc.\ntype interceptedForward struct {\n\tpacket *htlcswitch.InterceptedPacket\n\tbeacon *preimageBeacon\n}\n\nfunc newInterceptedForward(\n\tpacket *htlcswitch.InterceptedPacket,\n\tbeacon *preimageBeacon) *interceptedForward {\n\n\treturn &interceptedForward{\n\t\tbeacon: beacon,\n\t\tpacket: packet,\n\t}\n}\n\n// Packet returns the intercepted htlc packet.\nfunc (f *interceptedForward) Packet() htlcswitch.InterceptedPacket {\n\treturn *f.packet\n}\n\n// Resume notifies the intention to resume an existing hold forward. This\n// basically means the caller wants to resume with the default behavior for this\n// htlc which usually means forward it.\nfunc (f *interceptedForward) Resume() error {\n\treturn ErrCannotResume\n}\n\n// ResumeModified notifies the intention to resume an existing hold forward with\n// a modified htlc.\nfunc (f *interceptedForward) ResumeModified(_, _ fn.Option[lnwire.MilliSatoshi],\n\t_ fn.Option[lnwire.CustomRecords]) error {\n\n\treturn ErrCannotResume\n}\n\n// Fail notifies the intention to fail an existing hold forward with an\n// encrypted failure reason.\nfunc (f *interceptedForward) Fail(_ []byte) error {\n\t// We can't actively fail an htlc. The best we could do is abandon the\n\t// resolver, but this wouldn't be a safe operation. There may be a race\n\t// with the preimage beacon supplying a preimage. Therefore we don't\n\t// attempt to fail and just return an error here.\n\treturn ErrCannotFail\n}\n\n// FailWithCode notifies the intention to fail an existing hold forward with the\n// specified failure code.\nfunc (f *interceptedForward) FailWithCode(_ lnwire.FailCode) error {\n\treturn ErrCannotFail\n}\n\n// Settle notifies the intention to settle an existing hold forward with a given\n// preimage.\nfunc (f *interceptedForward) Settle(preimage lntypes.Preimage) error {\n\tif !preimage.Matches(f.packet.Hash) {\n\t\treturn ErrPreimageMismatch\n\t}\n\n\t// Add preimage to the preimage beacon. The onchain resolver will pick\n\t// up the preimage from the beacon.\n\treturn f.beacon.AddPreimages(preimage)\n}\n"
        },
        {
          "name": "internal",
          "type": "tree",
          "content": null
        },
        {
          "name": "invoices",
          "type": "tree",
          "content": null
        },
        {
          "name": "itest",
          "type": "tree",
          "content": null
        },
        {
          "name": "keychain",
          "type": "tree",
          "content": null
        },
        {
          "name": "kvdb",
          "type": "tree",
          "content": null
        },
        {
          "name": "labels",
          "type": "tree",
          "content": null
        },
        {
          "name": "lncfg",
          "type": "tree",
          "content": null
        },
        {
          "name": "lnd.go",
          "type": "blob",
          "size": 30.3330078125,
          "content": "// Copyright (c) 2013-2017 The btcsuite developers\n// Copyright (c) 2015-2016 The Decred developers\n// Copyright (C) 2015-2022 The Lightning Network Developers\n\npackage lnd\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"net/http\"\n\t\"net/http/pprof\"\n\t\"os\"\n\t\"runtime\"\n\truntimePprof \"runtime/pprof\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/btcsuite/btcd/btcutil\"\n\tproxy \"github.com/grpc-ecosystem/grpc-gateway/v2/runtime\"\n\t\"github.com/lightningnetwork/lnd/autopilot\"\n\t\"github.com/lightningnetwork/lnd/build\"\n\t\"github.com/lightningnetwork/lnd/chanacceptor\"\n\t\"github.com/lightningnetwork/lnd/channeldb\"\n\t\"github.com/lightningnetwork/lnd/cluster\"\n\t\"github.com/lightningnetwork/lnd/keychain\"\n\t\"github.com/lightningnetwork/lnd/lncfg\"\n\t\"github.com/lightningnetwork/lnd/lnrpc\"\n\t\"github.com/lightningnetwork/lnd/lnwallet\"\n\t\"github.com/lightningnetwork/lnd/macaroons\"\n\t\"github.com/lightningnetwork/lnd/monitoring\"\n\t\"github.com/lightningnetwork/lnd/rpcperms\"\n\t\"github.com/lightningnetwork/lnd/signal\"\n\t\"github.com/lightningnetwork/lnd/tor\"\n\t\"github.com/lightningnetwork/lnd/walletunlocker\"\n\t\"github.com/lightningnetwork/lnd/watchtower\"\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/credentials\"\n\t\"google.golang.org/grpc/keepalive\"\n\t\"gopkg.in/macaroon-bakery.v2/bakery\"\n\t\"gopkg.in/macaroon.v2\"\n)\n\nconst (\n\t// adminMacaroonFilePermissions is the file permission that is used for\n\t// creating the admin macaroon file.\n\t//\n\t// Why 640 is safe:\n\t// Assuming a reasonably secure Linux system, it will have a\n\t// separate group for each user. E.g. a new user lnd gets assigned group\n\t// lnd which nothing else belongs to. A system that does not do this is\n\t// inherently broken already.\n\t//\n\t// Since there is no other user in the group, no other user can read\n\t// admin macaroon unless the administrator explicitly allowed it. Thus\n\t// there's no harm allowing group read.\n\tadminMacaroonFilePermissions = 0640\n\n\t// leaderResignTimeout is the timeout used when resigning from the\n\t// leader role. This is kept short so LND can shut down quickly in case\n\t// of a system failure or network partition making the cluster\n\t// unresponsive. The cluster itself should ensure that the leader is not\n\t// elected again until the previous leader has resigned or the leader\n\t// election timeout has passed.\n\tleaderResignTimeout = 5 * time.Second\n)\n\n// AdminAuthOptions returns a list of DialOptions that can be used to\n// authenticate with the RPC server with admin capabilities.\n// skipMacaroons=true should be set if we don't want to include macaroons with\n// the auth options. This is needed for instance for the WalletUnlocker\n// service, which must be usable also before macaroons are created.\n//\n// NOTE: This should only be called after the RPCListener has signaled it is\n// ready.\nfunc AdminAuthOptions(cfg *Config, skipMacaroons bool) ([]grpc.DialOption,\n\terror) {\n\n\tcreds, err := credentials.NewClientTLSFromFile(cfg.TLSCertPath, \"\")\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to read TLS cert: %w\", err)\n\t}\n\n\t// Create a dial options array.\n\topts := []grpc.DialOption{\n\t\tgrpc.WithTransportCredentials(creds),\n\t}\n\n\t// Get the admin macaroon if macaroons are active.\n\tif !skipMacaroons && !cfg.NoMacaroons {\n\t\t// Load the admin macaroon file.\n\t\tmacBytes, err := os.ReadFile(cfg.AdminMacPath)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to read macaroon \"+\n\t\t\t\t\"path (check the network setting!): %v\", err)\n\t\t}\n\n\t\tmac := &macaroon.Macaroon{}\n\t\tif err = mac.UnmarshalBinary(macBytes); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to decode macaroon: %w\",\n\t\t\t\terr)\n\t\t}\n\n\t\t// Now we append the macaroon credentials to the dial options.\n\t\tcred, err := macaroons.NewMacaroonCredential(mac)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"error cloning mac: %w\", err)\n\t\t}\n\t\topts = append(opts, grpc.WithPerRPCCredentials(cred))\n\t}\n\n\treturn opts, nil\n}\n\n// ListenerWithSignal is a net.Listener that has an additional Ready channel\n// that will be closed when a server starts listening.\ntype ListenerWithSignal struct {\n\tnet.Listener\n\n\t// Ready will be closed by the server listening on Listener.\n\tReady chan struct{}\n\n\t// MacChan is an optional way to pass the admin macaroon to the program\n\t// that started lnd. The channel should be buffered to avoid lnd being\n\t// blocked on sending to the channel.\n\tMacChan chan []byte\n}\n\n// ListenerCfg is a wrapper around custom listeners that can be passed to lnd\n// when calling its main method.\ntype ListenerCfg struct {\n\t// RPCListeners can be set to the listeners to use for the RPC server.\n\t// If empty a regular network listener will be created.\n\tRPCListeners []*ListenerWithSignal\n}\n\nvar errStreamIsolationWithProxySkip = errors.New(\n\t\"while stream isolation is enabled, the TOR proxy may not be skipped\",\n)\n\n// Main is the true entry point for lnd. It accepts a fully populated and\n// validated main configuration struct and an optional listener config struct.\n// This function starts all main system components then blocks until a signal\n// is received on the shutdownChan at which point everything is shut down again.\nfunc Main(cfg *Config, lisCfg ListenerCfg, implCfg *ImplementationCfg,\n\tinterceptor signal.Interceptor) error {\n\n\tdefer func() {\n\t\tltndLog.Info(\"Shutdown complete\")\n\t\terr := cfg.LogRotator.Close()\n\t\tif err != nil {\n\t\t\tltndLog.Errorf(\"Could not close log rotator: %v\", err)\n\t\t}\n\t}()\n\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tctx, err := build.WithBuildInfo(ctx, cfg.LogConfig)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to add build info to context: %w\",\n\t\t\terr)\n\t}\n\n\tmkErr := func(msg string, err error, attrs ...any) error {\n\t\tltndLog.ErrorS(ctx, \"Shutting down due to error in main \"+\n\t\t\t\"method\", err, attrs...)\n\n\t\tvar (\n\t\t\tparams = []any{err}\n\t\t\tfmtStr = msg + \": %w\"\n\t\t)\n\t\tfor _, attr := range attrs {\n\t\t\tfmtStr += \" %s\"\n\n\t\t\tparams = append(params, attr)\n\t\t}\n\n\t\treturn fmt.Errorf(fmtStr, params...)\n\t}\n\n\t// Show version at startup.\n\tltndLog.InfoS(ctx, \"Version Info\",\n\t\tslog.String(\"version\", build.Version()),\n\t\tslog.String(\"commit\", build.Commit),\n\t\tslog.Any(\"debuglevel\", build.Deployment),\n\t\tslog.String(\"logging\", cfg.DebugLevel))\n\n\tvar network string\n\tswitch {\n\tcase cfg.Bitcoin.TestNet3:\n\t\tnetwork = \"testnet\"\n\n\tcase cfg.Bitcoin.MainNet:\n\t\tnetwork = \"mainnet\"\n\n\tcase cfg.Bitcoin.SimNet:\n\t\tnetwork = \"simnet\"\n\n\tcase cfg.Bitcoin.RegTest:\n\t\tnetwork = \"regtest\"\n\n\tcase cfg.Bitcoin.SigNet:\n\t\tnetwork = \"signet\"\n\t}\n\n\tltndLog.InfoS(ctx, \"Network Info\",\n\t\t\"active_chain\", strings.Title(BitcoinChainName),\n\t\t\"network\", network)\n\n\t// Enable http profiling server if requested.\n\tif cfg.Pprof.Profile != \"\" {\n\t\t// Create the http handler.\n\t\tpprofMux := http.NewServeMux()\n\t\tpprofMux.HandleFunc(\"/debug/pprof/\", pprof.Index)\n\t\tpprofMux.HandleFunc(\"/debug/pprof/cmdline\", pprof.Cmdline)\n\t\tpprofMux.HandleFunc(\"/debug/pprof/profile\", pprof.Profile)\n\t\tpprofMux.HandleFunc(\"/debug/pprof/symbol\", pprof.Symbol)\n\t\tpprofMux.HandleFunc(\"/debug/pprof/trace\", pprof.Trace)\n\n\t\tif cfg.Pprof.BlockingProfile != 0 {\n\t\t\truntime.SetBlockProfileRate(cfg.Pprof.BlockingProfile)\n\t\t}\n\t\tif cfg.Pprof.MutexProfile != 0 {\n\t\t\truntime.SetMutexProfileFraction(cfg.Pprof.MutexProfile)\n\t\t}\n\n\t\t// Redirect all requests to the pprof handler, thus visiting\n\t\t// `127.0.0.1:6060` will be redirected to\n\t\t// `127.0.0.1:6060/debug/pprof`.\n\t\tpprofMux.Handle(\"/\", http.RedirectHandler(\n\t\t\t\"/debug/pprof/\", http.StatusSeeOther,\n\t\t))\n\n\t\tltndLog.InfoS(ctx, \"Pprof listening\", \"addr\", cfg.Pprof.Profile)\n\n\t\t// Create the pprof server.\n\t\tpprofServer := &http.Server{\n\t\t\tAddr:              cfg.Pprof.Profile,\n\t\t\tHandler:           pprofMux,\n\t\t\tReadHeaderTimeout: cfg.HTTPHeaderTimeout,\n\t\t}\n\n\t\t// Shut the server down when lnd is shutting down.\n\t\tdefer func() {\n\t\t\tltndLog.InfoS(ctx, \"Stopping pprof server...\")\n\t\t\terr := pprofServer.Shutdown(ctx)\n\t\t\tif err != nil {\n\t\t\t\tltndLog.ErrorS(ctx, \"Stop pprof server\", err)\n\t\t\t}\n\t\t}()\n\n\t\t// Start the pprof server.\n\t\tgo func() {\n\t\t\terr := pprofServer.ListenAndServe()\n\t\t\tif err != nil && !errors.Is(err, http.ErrServerClosed) {\n\t\t\t\tltndLog.ErrorS(ctx, \"Could not serve pprof \"+\n\t\t\t\t\t\"server\", err)\n\t\t\t}\n\t\t}()\n\t}\n\n\t// Write cpu profile if requested.\n\tif cfg.Pprof.CPUProfile != \"\" {\n\t\tf, err := os.Create(cfg.Pprof.CPUProfile)\n\t\tif err != nil {\n\t\t\treturn mkErr(\"unable to create CPU profile\", err)\n\t\t}\n\t\t_ = runtimePprof.StartCPUProfile(f)\n\t\tdefer func() {\n\t\t\t_ = f.Close()\n\t\t}()\n\t\tdefer runtimePprof.StopCPUProfile()\n\t}\n\n\t// Run configuration dependent DB pre-initialization. Note that this\n\t// needs to be done early and once during the startup process, before\n\t// any DB access.\n\tif err := cfg.DB.Init(ctx, cfg.graphDatabaseDir()); err != nil {\n\t\treturn mkErr(\"error initializing DBs\", err)\n\t}\n\n\ttlsManagerCfg := &TLSManagerCfg{\n\t\tTLSCertPath:        cfg.TLSCertPath,\n\t\tTLSKeyPath:         cfg.TLSKeyPath,\n\t\tTLSEncryptKey:      cfg.TLSEncryptKey,\n\t\tTLSExtraIPs:        cfg.TLSExtraIPs,\n\t\tTLSExtraDomains:    cfg.TLSExtraDomains,\n\t\tTLSAutoRefresh:     cfg.TLSAutoRefresh,\n\t\tTLSDisableAutofill: cfg.TLSDisableAutofill,\n\t\tTLSCertDuration:    cfg.TLSCertDuration,\n\n\t\tLetsEncryptDir:    cfg.LetsEncryptDir,\n\t\tLetsEncryptDomain: cfg.LetsEncryptDomain,\n\t\tLetsEncryptListen: cfg.LetsEncryptListen,\n\n\t\tDisableRestTLS: cfg.DisableRestTLS,\n\n\t\tHTTPHeaderTimeout: cfg.HTTPHeaderTimeout,\n\t}\n\ttlsManager := NewTLSManager(tlsManagerCfg)\n\tserverOpts, restDialOpts, restListen, cleanUp,\n\t\terr := tlsManager.SetCertificateBeforeUnlock()\n\tif err != nil {\n\t\treturn mkErr(\"error setting cert before unlock\", err)\n\t}\n\tif cleanUp != nil {\n\t\tdefer cleanUp()\n\t}\n\n\t// If we have chosen to start with a dedicated listener for the\n\t// rpc server, we set it directly.\n\tgrpcListeners := append([]*ListenerWithSignal{}, lisCfg.RPCListeners...)\n\tif len(grpcListeners) == 0 {\n\t\t// Otherwise we create listeners from the RPCListeners defined\n\t\t// in the config.\n\t\tfor _, grpcEndpoint := range cfg.RPCListeners {\n\t\t\t// Start a gRPC server listening for HTTP/2\n\t\t\t// connections.\n\t\t\tlis, err := lncfg.ListenOnAddress(grpcEndpoint)\n\t\t\tif err != nil {\n\t\t\t\treturn mkErr(\"unable to listen on grpc \"+\n\t\t\t\t\t\"endpoint\", err,\n\t\t\t\t\tslog.String(\n\t\t\t\t\t\t\"endpoint\",\n\t\t\t\t\t\tgrpcEndpoint.String(),\n\t\t\t\t\t))\n\t\t\t}\n\t\t\tdefer lis.Close()\n\n\t\t\tgrpcListeners = append(\n\t\t\t\tgrpcListeners, &ListenerWithSignal{\n\t\t\t\t\tListener: lis,\n\t\t\t\t\tReady:    make(chan struct{}),\n\t\t\t\t},\n\t\t\t)\n\t\t}\n\t}\n\n\t// Create a new RPC interceptor that we'll add to the GRPC server. This\n\t// will be used to log the API calls invoked on the GRPC server.\n\tinterceptorChain := rpcperms.NewInterceptorChain(\n\t\trpcsLog, cfg.NoMacaroons, cfg.RPCMiddleware.Mandatory,\n\t)\n\tif err := interceptorChain.Start(); err != nil {\n\t\treturn mkErr(\"error starting interceptor chain\", err)\n\t}\n\tdefer func() {\n\t\terr := interceptorChain.Stop()\n\t\tif err != nil {\n\t\t\tltndLog.Warnf(\"error stopping RPC interceptor \"+\n\t\t\t\t\"chain: %v\", err)\n\t\t}\n\t}()\n\n\t// Allow the user to overwrite some defaults of the gRPC library related\n\t// to connection keepalive (server side and client side pings).\n\tserverKeepalive := keepalive.ServerParameters{\n\t\tTime:    cfg.GRPC.ServerPingTime,\n\t\tTimeout: cfg.GRPC.ServerPingTimeout,\n\t}\n\tclientKeepalive := keepalive.EnforcementPolicy{\n\t\tMinTime:             cfg.GRPC.ClientPingMinWait,\n\t\tPermitWithoutStream: cfg.GRPC.ClientAllowPingWithoutStream,\n\t}\n\n\trpcServerOpts := interceptorChain.CreateServerOpts()\n\tserverOpts = append(serverOpts, rpcServerOpts...)\n\tserverOpts = append(\n\t\tserverOpts, grpc.MaxRecvMsgSize(lnrpc.MaxGrpcMsgSize),\n\t\tgrpc.KeepaliveParams(serverKeepalive),\n\t\tgrpc.KeepaliveEnforcementPolicy(clientKeepalive),\n\t)\n\n\tgrpcServer := grpc.NewServer(serverOpts...)\n\tdefer grpcServer.Stop()\n\n\t// We'll also register the RPC interceptor chain as the StateServer, as\n\t// it can be used to query for the current state of the wallet.\n\tlnrpc.RegisterStateServer(grpcServer, interceptorChain)\n\n\t// Initialize, and register our implementation of the gRPC interface\n\t// exported by the rpcServer.\n\trpcServer := newRPCServer(cfg, interceptorChain, implCfg, interceptor)\n\terr = rpcServer.RegisterWithGrpcServer(grpcServer)\n\tif err != nil {\n\t\treturn mkErr(\"error registering gRPC server\", err)\n\t}\n\n\t// Now that both the WalletUnlocker and LightningService have been\n\t// registered with the GRPC server, we can start listening.\n\terr = startGrpcListen(cfg, grpcServer, grpcListeners)\n\tif err != nil {\n\t\treturn mkErr(\"error starting gRPC listener\", err)\n\t}\n\n\t// Now start the REST proxy for our gRPC server above. We'll ensure\n\t// we direct LND to connect to its loopback address rather than a\n\t// wildcard to prevent certificate issues when accessing the proxy\n\t// externally.\n\tstopProxy, err := startRestProxy(\n\t\tctx, cfg, rpcServer, restDialOpts, restListen,\n\t)\n\tif err != nil {\n\t\treturn mkErr(\"error starting REST proxy\", err)\n\t}\n\tdefer stopProxy()\n\n\t// Start leader election if we're running on etcd. Continuation will be\n\t// blocked until this instance is elected as the current leader or\n\t// shutting down.\n\telected := false\n\tvar leaderElector cluster.LeaderElector\n\tif cfg.Cluster.EnableLeaderElection {\n\t\telectionCtx, cancelElection := context.WithCancel(ctx)\n\n\t\tgo func() {\n\t\t\t<-interceptor.ShutdownChannel()\n\t\t\tcancelElection()\n\t\t}()\n\n\t\tltndLog.InfoS(ctx, \"Using leader elector\",\n\t\t\t\"elector\", cfg.Cluster.LeaderElector)\n\n\t\tleaderElector, err = cfg.Cluster.MakeLeaderElector(\n\t\t\telectionCtx, cfg.DB,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tdefer func() {\n\t\t\tif !elected {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tltndLog.InfoS(ctx, \"Attempting to resign from \"+\n\t\t\t\t\"leader role\", \"cluster_id\", cfg.Cluster.ID)\n\n\t\t\t// Ensure that we don't block the shutdown process if\n\t\t\t// the leader resigning process takes too long. The\n\t\t\t// cluster will ensure that the leader is not elected\n\t\t\t// again until the previous leader has resigned or the\n\t\t\t// leader election timeout has passed.\n\t\t\ttimeoutCtx, cancel := context.WithTimeout(\n\t\t\t\tctx, leaderResignTimeout,\n\t\t\t)\n\t\t\tdefer cancel()\n\n\t\t\tif err := leaderElector.Resign(timeoutCtx); err != nil {\n\t\t\t\tltndLog.Errorf(\"Leader elector failed to \"+\n\t\t\t\t\t\"resign: %v\", err)\n\t\t\t}\n\t\t}()\n\n\t\tltndLog.InfoS(ctx, \"Starting leadership campaign\",\n\t\t\t\"cluster_id\", cfg.Cluster.ID)\n\n\t\tif err := leaderElector.Campaign(electionCtx); err != nil {\n\t\t\treturn mkErr(\"leadership campaign failed\", err)\n\t\t}\n\n\t\telected = true\n\t\tltndLog.InfoS(ctx, \"Elected as leader\",\n\t\t\t\"cluster_id\", cfg.Cluster.ID)\n\t}\n\n\tdbs, cleanUp, err := implCfg.DatabaseBuilder.BuildDatabase(ctx)\n\tswitch {\n\tcase errors.Is(err, channeldb.ErrDryRunMigrationOK):\n\t\tltndLog.InfoS(ctx, \"Exiting due to BuildDatabase error\",\n\t\t\tslog.Any(\"err\", err))\n\t\treturn nil\n\tcase err != nil:\n\t\treturn mkErr(\"unable to open databases\", err)\n\t}\n\n\tdefer cleanUp()\n\n\tpartialChainControl, walletConfig, cleanUp, err := implCfg.BuildWalletConfig(\n\t\tctx, dbs, &implCfg.AuxComponents, interceptorChain,\n\t\tgrpcListeners,\n\t)\n\tif err != nil {\n\t\treturn mkErr(\"error creating wallet config\", err)\n\t}\n\n\tdefer cleanUp()\n\n\tactiveChainControl, cleanUp, err := implCfg.BuildChainControl(\n\t\tpartialChainControl, walletConfig,\n\t)\n\tif err != nil {\n\t\treturn mkErr(\"error loading chain control\", err)\n\t}\n\n\tdefer cleanUp()\n\n\t// TODO(roasbeef): add rotation\n\tidKeyDesc, err := activeChainControl.KeyRing.DeriveKey(\n\t\tkeychain.KeyLocator{\n\t\t\tFamily: keychain.KeyFamilyNodeKey,\n\t\t\tIndex:  0,\n\t\t},\n\t)\n\tif err != nil {\n\t\treturn mkErr(\"error deriving node key\", err)\n\t}\n\n\tif cfg.Tor.StreamIsolation && cfg.Tor.SkipProxyForClearNetTargets {\n\t\treturn errStreamIsolationWithProxySkip\n\t}\n\n\tif cfg.Tor.Active {\n\t\tif cfg.Tor.SkipProxyForClearNetTargets {\n\t\t\tsrvrLog.InfoS(ctx, \"Onion services are accessible \"+\n\t\t\t\t\"via Tor! NOTE: Traffic to clearnet services \"+\n\t\t\t\t\"is not routed via Tor.\")\n\t\t} else {\n\t\t\tsrvrLog.InfoS(ctx, \"Proxying all network traffic \"+\n\t\t\t\t\"via Tor! NOTE: Ensure the backend node is \"+\n\t\t\t\t\"proxying over Tor as well\",\n\t\t\t\t\"stream_isolation\", cfg.Tor.StreamIsolation)\n\t\t}\n\t}\n\n\t// If tor is active and either v2 or v3 onion services have been\n\t// specified, make a tor controller and pass it into both the watchtower\n\t// server and the regular lnd server.\n\tvar torController *tor.Controller\n\tif cfg.Tor.Active && (cfg.Tor.V2 || cfg.Tor.V3) {\n\t\ttorController = tor.NewController(\n\t\t\tcfg.Tor.Control, cfg.Tor.TargetIPAddress,\n\t\t\tcfg.Tor.Password,\n\t\t)\n\n\t\t// Start the tor controller before giving it to any other\n\t\t// subsystems.\n\t\tif err := torController.Start(); err != nil {\n\t\t\treturn mkErr(\"unable to initialize tor controller\",\n\t\t\t\terr)\n\t\t}\n\t\tdefer func() {\n\t\t\tif err := torController.Stop(); err != nil {\n\t\t\t\tltndLog.ErrorS(ctx, \"Error stopping tor \"+\n\t\t\t\t\t\"controller\", err)\n\t\t\t}\n\t\t}()\n\t}\n\n\tvar tower *watchtower.Standalone\n\tif cfg.Watchtower.Active {\n\t\ttowerKeyDesc, err := activeChainControl.KeyRing.DeriveKey(\n\t\t\tkeychain.KeyLocator{\n\t\t\t\tFamily: keychain.KeyFamilyTowerID,\n\t\t\t\tIndex:  0,\n\t\t\t},\n\t\t)\n\t\tif err != nil {\n\t\t\treturn mkErr(\"error deriving tower key\", err)\n\t\t}\n\n\t\twtCfg := &watchtower.Config{\n\t\t\tBlockFetcher:   activeChainControl.ChainIO,\n\t\t\tDB:             dbs.TowerServerDB,\n\t\t\tEpochRegistrar: activeChainControl.ChainNotifier,\n\t\t\tNet:            cfg.net,\n\t\t\tNewAddress: func() (btcutil.Address, error) {\n\t\t\t\treturn activeChainControl.Wallet.NewAddress(\n\t\t\t\t\tlnwallet.TaprootPubkey, false,\n\t\t\t\t\tlnwallet.DefaultAccountName,\n\t\t\t\t)\n\t\t\t},\n\t\t\tNodeKeyECDH: keychain.NewPubKeyECDH(\n\t\t\t\ttowerKeyDesc, activeChainControl.KeyRing,\n\t\t\t),\n\t\t\tPublishTx: activeChainControl.Wallet.PublishTransaction,\n\t\t\tChainHash: *cfg.ActiveNetParams.GenesisHash,\n\t\t}\n\n\t\t// If there is a tor controller (user wants auto hidden\n\t\t// services), then store a pointer in the watchtower config.\n\t\tif torController != nil {\n\t\t\twtCfg.TorController = torController\n\t\t\twtCfg.WatchtowerKeyPath = cfg.Tor.WatchtowerKeyPath\n\t\t\twtCfg.EncryptKey = cfg.Tor.EncryptKey\n\t\t\twtCfg.KeyRing = activeChainControl.KeyRing\n\n\t\t\tswitch {\n\t\t\tcase cfg.Tor.V2:\n\t\t\t\twtCfg.Type = tor.V2\n\t\t\tcase cfg.Tor.V3:\n\t\t\t\twtCfg.Type = tor.V3\n\t\t\t}\n\t\t}\n\n\t\twtConfig, err := cfg.Watchtower.Apply(\n\t\t\twtCfg, lncfg.NormalizeAddresses,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn mkErr(\"unable to configure watchtower\", err)\n\t\t}\n\n\t\ttower, err = watchtower.New(wtConfig)\n\t\tif err != nil {\n\t\t\treturn mkErr(\"unable to create watchtower\", err)\n\t\t}\n\t}\n\n\t// Initialize the MultiplexAcceptor. If lnd was started with the\n\t// zero-conf feature bit, then this will be a ZeroConfAcceptor.\n\t// Otherwise, this will be a ChainedAcceptor.\n\tvar multiAcceptor chanacceptor.MultiplexAcceptor\n\tif cfg.ProtocolOptions.ZeroConf() {\n\t\tmultiAcceptor = chanacceptor.NewZeroConfAcceptor()\n\t} else {\n\t\tmultiAcceptor = chanacceptor.NewChainedAcceptor()\n\t}\n\n\t// Set up the core server which will listen for incoming peer\n\t// connections.\n\tserver, err := newServer(\n\t\tcfg, cfg.Listeners, dbs, activeChainControl, &idKeyDesc,\n\t\tactiveChainControl.Cfg.WalletUnlockParams.ChansToRestore,\n\t\tmultiAcceptor, torController, tlsManager, leaderElector,\n\t\timplCfg,\n\t)\n\tif err != nil {\n\t\treturn mkErr(\"unable to create server\", err)\n\t}\n\n\t// Set up an autopilot manager from the current config. This will be\n\t// used to manage the underlying autopilot agent, starting and stopping\n\t// it at will.\n\tatplCfg, err := initAutoPilot(\n\t\tserver, cfg.Autopilot, activeChainControl.MinHtlcIn,\n\t\tcfg.ActiveNetParams,\n\t)\n\tif err != nil {\n\t\treturn mkErr(\"unable to initialize autopilot\", err)\n\t}\n\n\tatplManager, err := autopilot.NewManager(atplCfg)\n\tif err != nil {\n\t\treturn mkErr(\"unable to create autopilot manager\", err)\n\t}\n\tif err := atplManager.Start(); err != nil {\n\t\treturn mkErr(\"unable to start autopilot manager\", err)\n\t}\n\tdefer atplManager.Stop()\n\n\terr = tlsManager.LoadPermanentCertificate(activeChainControl.KeyRing)\n\tif err != nil {\n\t\treturn mkErr(\"unable to load permanent TLS certificate\", err)\n\t}\n\n\t// Now we have created all dependencies necessary to populate and\n\t// start the RPC server.\n\terr = rpcServer.addDeps(\n\t\tserver, interceptorChain.MacaroonService(), cfg.SubRPCServers,\n\t\tatplManager, server.invoices, tower, multiAcceptor,\n\t\tserver.invoiceHtlcModifier,\n\t)\n\tif err != nil {\n\t\treturn mkErr(\"unable to add deps to RPC server\", err)\n\t}\n\tif err := rpcServer.Start(); err != nil {\n\t\treturn mkErr(\"unable to start RPC server\", err)\n\t}\n\tdefer rpcServer.Stop()\n\n\t// We transition the RPC state to Active, as the RPC server is up.\n\tinterceptorChain.SetRPCActive()\n\n\tif err := interceptor.Notifier.NotifyReady(true); err != nil {\n\t\treturn mkErr(\"error notifying ready\", err)\n\t}\n\n\t// We'll wait until we're fully synced to continue the start up of the\n\t// remainder of the daemon. This ensures that we don't accept any\n\t// possibly invalid state transitions, or accept channels with spent\n\t// funds.\n\t_, bestHeight, err := activeChainControl.ChainIO.GetBestBlock()\n\tif err != nil {\n\t\treturn mkErr(\"unable to determine chain tip\", err)\n\t}\n\n\tltndLog.InfoS(ctx, \"Waiting for chain backend to finish sync\",\n\t\tslog.Int64(\"start_height\", int64(bestHeight)))\n\n\ttype syncResult struct {\n\t\tsynced        bool\n\t\tbestBlockTime int64\n\t\terr           error\n\t}\n\n\tvar syncedResChan = make(chan syncResult, 1)\n\n\tfor {\n\t\t// We check if the wallet is synced in a separate goroutine as\n\t\t// the call is blocking, and we want to be able to interrupt it\n\t\t// if the daemon is shutting down.\n\t\tgo func() {\n\t\t\tsynced, bestBlockTime, err := activeChainControl.Wallet.\n\t\t\t\tIsSynced()\n\t\t\tsyncedResChan <- syncResult{synced, bestBlockTime, err}\n\t\t}()\n\n\t\tselect {\n\t\tcase <-interceptor.ShutdownChannel():\n\t\t\treturn nil\n\n\t\tcase res := <-syncedResChan:\n\t\t\tif res.err != nil {\n\t\t\t\treturn mkErr(\"unable to determine if wallet \"+\n\t\t\t\t\t\"is synced\", res.err)\n\t\t\t}\n\n\t\t\tltndLog.DebugS(ctx, \"Syncing to block chain\",\n\t\t\t\t\"best_block_time\", time.Unix(res.bestBlockTime, 0),\n\t\t\t\t\"is_synced\", res.synced)\n\n\t\t\tif res.synced {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If we're not yet synced, we'll wait for a second\n\t\t\t// before checking again.\n\t\t\tselect {\n\t\t\tcase <-interceptor.ShutdownChannel():\n\t\t\t\treturn nil\n\n\t\t\tcase <-time.After(time.Second):\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tbreak\n\t}\n\n\t_, bestHeight, err = activeChainControl.ChainIO.GetBestBlock()\n\tif err != nil {\n\t\treturn mkErr(\"unable to determine chain tip\", err)\n\t}\n\n\tltndLog.InfoS(ctx, \"Chain backend is fully synced!\",\n\t\t\"end_height\", bestHeight)\n\n\t// With all the relevant chains initialized, we can finally start the\n\t// server itself. We start the server in an asynchronous goroutine so\n\t// that we are able to interrupt and shutdown the daemon gracefully in\n\t// case the startup of the subservers do not behave as expected.\n\terrChan := make(chan error)\n\tgo func() {\n\t\terrChan <- server.Start()\n\t}()\n\n\tdefer func() {\n\t\terr := server.Stop()\n\t\tif err != nil {\n\t\t\tltndLog.WarnS(ctx, \"Stopping the server including all \"+\n\t\t\t\t\"its subsystems failed with\", err)\n\t\t}\n\t}()\n\n\tselect {\n\tcase err := <-errChan:\n\t\tif err == nil {\n\t\t\tbreak\n\t\t}\n\n\t\treturn mkErr(\"unable to start server\", err)\n\n\tcase <-interceptor.ShutdownChannel():\n\t\treturn nil\n\t}\n\n\t// We transition the server state to Active, as the server is up.\n\tinterceptorChain.SetServerActive()\n\n\t// Now that the server has started, if the autopilot mode is currently\n\t// active, then we'll start the autopilot agent immediately. It will be\n\t// stopped together with the autopilot service.\n\tif cfg.Autopilot.Active {\n\t\tif err := atplManager.StartAgent(); err != nil {\n\t\t\treturn mkErr(\"unable to start autopilot agent\", err)\n\t\t}\n\t}\n\n\tif cfg.Watchtower.Active {\n\t\tif err := tower.Start(); err != nil {\n\t\t\treturn mkErr(\"unable to start watchtower\", err)\n\t\t}\n\t\tdefer tower.Stop()\n\t}\n\n\t// Wait for shutdown signal from either a graceful server stop or from\n\t// the interrupt handler.\n\t<-interceptor.ShutdownChannel()\n\treturn nil\n}\n\n// bakeMacaroon creates a new macaroon with newest version and the given\n// permissions then returns it binary serialized.\nfunc bakeMacaroon(ctx context.Context, svc *macaroons.Service,\n\tpermissions []bakery.Op) ([]byte, error) {\n\n\tmac, err := svc.NewMacaroon(\n\t\tctx, macaroons.DefaultRootKeyID, permissions...,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn mac.M().MarshalBinary()\n}\n\n// saveMacaroon bakes a macaroon with the specified macaroon permissions and\n// writes it to a file with the given filename and file permissions.\nfunc saveMacaroon(ctx context.Context, svc *macaroons.Service, filename string,\n\tmacaroonPermissions []bakery.Op, filePermissions os.FileMode) error {\n\n\tmacaroonBytes, err := bakeMacaroon(ctx, svc, macaroonPermissions)\n\tif err != nil {\n\t\treturn err\n\t}\n\terr = os.WriteFile(filename, macaroonBytes, filePermissions)\n\tif err != nil {\n\t\t_ = os.Remove(filename)\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// genDefaultMacaroons checks for three default macaroon files and generates\n// them if they do not exist; one admin-level, one for invoice access and one\n// read-only. Each macaroon is checked and created independently to ensure all\n// three exist. The admin macaroon can also be used to generate more granular\n// macaroons.\nfunc genDefaultMacaroons(ctx context.Context, svc *macaroons.Service,\n\tadmFile, roFile, invoiceFile string) error {\n\n\t// First, we'll generate a macaroon that only allows the caller to\n\t// access invoice related calls. This is useful for merchants and other\n\t// services to allow an isolated instance that can only query and\n\t// modify invoices.\n\tif !lnrpc.FileExists(invoiceFile) {\n\t\terr := saveMacaroon(\n\t\t\tctx, svc, invoiceFile, invoicePermissions, 0644,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Generate the read-only macaroon and write it to a file.\n\tif !lnrpc.FileExists(roFile) {\n\t\terr := saveMacaroon(\n\t\t\tctx, svc, roFile, readPermissions, 0644,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Generate the admin macaroon and write it to a file.\n\tif !lnrpc.FileExists(admFile) {\n\t\terr := saveMacaroon(\n\t\t\tctx, svc, admFile, adminPermissions(),\n\t\t\tadminMacaroonFilePermissions,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// adminPermissions returns a list of all permissions in a safe way that doesn't\n// modify any of the source lists.\nfunc adminPermissions() []bakery.Op {\n\tadmin := make([]bakery.Op, len(readPermissions)+len(writePermissions))\n\tcopy(admin[:len(readPermissions)], readPermissions)\n\tcopy(admin[len(readPermissions):], writePermissions)\n\treturn admin\n}\n\n// createWalletUnlockerService creates a WalletUnlockerService from the passed\n// config.\nfunc createWalletUnlockerService(cfg *Config) *walletunlocker.UnlockerService {\n\t// The macaroonFiles are passed to the wallet unlocker so they can be\n\t// deleted and recreated in case the root macaroon key is also changed\n\t// during the change password operation.\n\tmacaroonFiles := []string{\n\t\tcfg.AdminMacPath, cfg.ReadMacPath, cfg.InvoiceMacPath,\n\t}\n\n\treturn walletunlocker.New(\n\t\tcfg.ActiveNetParams.Params, macaroonFiles,\n\t\tcfg.ResetWalletTransactions, nil,\n\t)\n}\n\n// startGrpcListen starts the GRPC server on the passed listeners.\nfunc startGrpcListen(cfg *Config, grpcServer *grpc.Server,\n\tlisteners []*ListenerWithSignal) error {\n\n\t// Use a WaitGroup so we can be sure the instructions on how to input the\n\t// password is the last thing to be printed to the console.\n\tvar wg sync.WaitGroup\n\n\tfor _, lis := range listeners {\n\t\twg.Add(1)\n\t\tgo func(lis *ListenerWithSignal) {\n\t\t\trpcsLog.Infof(\"RPC server listening on %s\", lis.Addr())\n\n\t\t\t// Close the ready chan to indicate we are listening.\n\t\t\tclose(lis.Ready)\n\n\t\t\twg.Done()\n\t\t\t_ = grpcServer.Serve(lis)\n\t\t}(lis)\n\t}\n\n\t// If Prometheus monitoring is enabled, start the Prometheus exporter.\n\tif cfg.Prometheus.Enabled() {\n\t\terr := monitoring.ExportPrometheusMetrics(\n\t\t\tgrpcServer, cfg.Prometheus,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Wait for gRPC servers to be up running.\n\twg.Wait()\n\n\treturn nil\n}\n\n// startRestProxy starts the given REST proxy on the listeners found in the\n// config.\nfunc startRestProxy(ctx context.Context, cfg *Config, rpcServer *rpcServer,\n\trestDialOpts []grpc.DialOption,\n\trestListen func(net.Addr) (net.Listener, error)) (func(), error) {\n\n\t// We use the first RPC listener as the destination for our REST proxy.\n\t// If the listener is set to listen on all interfaces, we replace it\n\t// with localhost, as we cannot dial it directly.\n\trestProxyDest := cfg.RPCListeners[0].String()\n\tswitch {\n\tcase strings.Contains(restProxyDest, \"0.0.0.0\"):\n\t\trestProxyDest = strings.Replace(\n\t\t\trestProxyDest, \"0.0.0.0\", \"127.0.0.1\", 1,\n\t\t)\n\n\tcase strings.Contains(restProxyDest, \"[::]\"):\n\t\trestProxyDest = strings.Replace(\n\t\t\trestProxyDest, \"[::]\", \"[::1]\", 1,\n\t\t)\n\t}\n\n\tvar shutdownFuncs []func()\n\tshutdown := func() {\n\t\tfor _, shutdownFn := range shutdownFuncs {\n\t\t\tshutdownFn()\n\t\t}\n\t}\n\n\t// Start a REST proxy for our gRPC server.\n\tctx, cancel := context.WithCancel(ctx)\n\tshutdownFuncs = append(shutdownFuncs, cancel)\n\n\t// We'll set up a proxy that will forward REST calls to the GRPC\n\t// server.\n\t//\n\t// The default JSON marshaler of the REST proxy only sets OrigName to\n\t// true, which instructs it to use the same field names as specified in\n\t// the proto file and not switch to camel case. What we also want is\n\t// that the marshaler prints all values, even if they are falsey.\n\tcustomMarshalerOption := proxy.WithMarshalerOption(\n\t\tproxy.MIMEWildcard, &proxy.JSONPb{\n\t\t\tMarshalOptions:   *lnrpc.RESTJsonMarshalOpts,\n\t\t\tUnmarshalOptions: *lnrpc.RESTJsonUnmarshalOpts,\n\t\t},\n\t)\n\tmux := proxy.NewServeMux(\n\t\tcustomMarshalerOption,\n\n\t\t// Don't allow falling back to other HTTP methods, we want exact\n\t\t// matches only. The actual method to be used can be overwritten\n\t\t// by setting X-HTTP-Method-Override so there should be no\n\t\t// reason for not specifying the correct method in the first\n\t\t// place.\n\t\tproxy.WithDisablePathLengthFallback(),\n\t)\n\n\t// Register our services with the REST proxy.\n\terr := rpcServer.RegisterWithRestProxy(\n\t\tctx, mux, restDialOpts, restProxyDest,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Wrap the default grpc-gateway handler with the WebSocket handler.\n\trestHandler := lnrpc.NewWebSocketProxy(\n\t\tmux, rpcsLog, cfg.WSPingInterval, cfg.WSPongWait,\n\t\tlnrpc.LndClientStreamingURIs,\n\t)\n\n\t// Use a WaitGroup so we can be sure the instructions on how to input the\n\t// password is the last thing to be printed to the console.\n\tvar wg sync.WaitGroup\n\n\t// Now spin up a network listener for each requested port and start a\n\t// goroutine that serves REST with the created mux there.\n\tfor _, restEndpoint := range cfg.RESTListeners {\n\t\tlis, err := restListen(restEndpoint)\n\t\tif err != nil {\n\t\t\tltndLog.Errorf(\"gRPC proxy unable to listen on %s\",\n\t\t\t\trestEndpoint)\n\t\t\treturn nil, err\n\t\t}\n\n\t\tshutdownFuncs = append(shutdownFuncs, func() {\n\t\t\terr := lis.Close()\n\t\t\tif err != nil {\n\t\t\t\trpcsLog.Errorf(\"Error closing listener: %v\",\n\t\t\t\t\terr)\n\t\t\t}\n\t\t})\n\n\t\twg.Add(1)\n\t\tgo func() {\n\t\t\trpcsLog.Infof(\"gRPC proxy started at %s\", lis.Addr())\n\n\t\t\t// Create our proxy chain now. A request will pass\n\t\t\t// through the following chain:\n\t\t\t// req ---> CORS handler --> WS proxy --->\n\t\t\t//   REST proxy --> gRPC endpoint\n\t\t\tcorsHandler := allowCORS(restHandler, cfg.RestCORS)\n\n\t\t\twg.Done()\n\t\t\terr := http.Serve(lis, corsHandler)\n\t\t\tif err != nil && !lnrpc.IsClosedConnError(err) {\n\t\t\t\trpcsLog.Error(err)\n\t\t\t}\n\t\t}()\n\t}\n\n\t// Wait for REST servers to be up running.\n\twg.Wait()\n\n\treturn shutdown, nil\n}\n"
        },
        {
          "name": "lnencrypt",
          "type": "tree",
          "content": null
        },
        {
          "name": "lnmock",
          "type": "tree",
          "content": null
        },
        {
          "name": "lnpeer",
          "type": "tree",
          "content": null
        },
        {
          "name": "lnrpc",
          "type": "tree",
          "content": null
        },
        {
          "name": "lntest",
          "type": "tree",
          "content": null
        },
        {
          "name": "lntypes",
          "type": "tree",
          "content": null
        },
        {
          "name": "lnutils",
          "type": "tree",
          "content": null
        },
        {
          "name": "lnwallet",
          "type": "tree",
          "content": null
        },
        {
          "name": "lnwire",
          "type": "tree",
          "content": null
        },
        {
          "name": "log.go",
          "type": "blob",
          "size": 11.1826171875,
          "content": "package lnd\n\nimport (\n\t\"github.com/btcsuite/btcd/connmgr\"\n\t\"github.com/btcsuite/btcd/rpcclient\"\n\tbtclogv1 \"github.com/btcsuite/btclog\"\n\t\"github.com/btcsuite/btclog/v2\"\n\t\"github.com/lightninglabs/neutrino\"\n\tsphinx \"github.com/lightningnetwork/lightning-onion\"\n\t\"github.com/lightningnetwork/lnd/autopilot\"\n\t\"github.com/lightningnetwork/lnd/build\"\n\t\"github.com/lightningnetwork/lnd/chainio\"\n\t\"github.com/lightningnetwork/lnd/chainntnfs\"\n\t\"github.com/lightningnetwork/lnd/chainreg\"\n\t\"github.com/lightningnetwork/lnd/chanacceptor\"\n\t\"github.com/lightningnetwork/lnd/chanbackup\"\n\t\"github.com/lightningnetwork/lnd/chanfitness\"\n\t\"github.com/lightningnetwork/lnd/channeldb\"\n\t\"github.com/lightningnetwork/lnd/channelnotifier\"\n\t\"github.com/lightningnetwork/lnd/cluster\"\n\t\"github.com/lightningnetwork/lnd/contractcourt\"\n\t\"github.com/lightningnetwork/lnd/discovery\"\n\t\"github.com/lightningnetwork/lnd/funding\"\n\t\"github.com/lightningnetwork/lnd/graph\"\n\tgraphdb \"github.com/lightningnetwork/lnd/graph/db\"\n\t\"github.com/lightningnetwork/lnd/healthcheck\"\n\t\"github.com/lightningnetwork/lnd/htlcswitch\"\n\t\"github.com/lightningnetwork/lnd/invoices\"\n\t\"github.com/lightningnetwork/lnd/kvdb/sqlbase\"\n\t\"github.com/lightningnetwork/lnd/lncfg\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/autopilotrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/chainrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/devrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/invoicesrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/neutrinorpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/peersrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/routerrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/signrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/verrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/walletrpc\"\n\t\"github.com/lightningnetwork/lnd/lnwallet\"\n\t\"github.com/lightningnetwork/lnd/lnwallet/btcwallet\"\n\t\"github.com/lightningnetwork/lnd/lnwallet/chancloser\"\n\t\"github.com/lightningnetwork/lnd/lnwallet/chanfunding\"\n\t\"github.com/lightningnetwork/lnd/lnwallet/rpcwallet\"\n\t\"github.com/lightningnetwork/lnd/monitoring\"\n\t\"github.com/lightningnetwork/lnd/netann\"\n\t\"github.com/lightningnetwork/lnd/peer\"\n\t\"github.com/lightningnetwork/lnd/peernotifier\"\n\t\"github.com/lightningnetwork/lnd/routing\"\n\t\"github.com/lightningnetwork/lnd/routing/blindedpath\"\n\t\"github.com/lightningnetwork/lnd/routing/localchans\"\n\t\"github.com/lightningnetwork/lnd/rpcperms\"\n\t\"github.com/lightningnetwork/lnd/signal\"\n\t\"github.com/lightningnetwork/lnd/sweep\"\n\t\"github.com/lightningnetwork/lnd/tor\"\n\t\"github.com/lightningnetwork/lnd/watchtower\"\n\t\"github.com/lightningnetwork/lnd/watchtower/wtclient\"\n)\n\n// replaceableLogger is a thin wrapper around a logger that is used so the\n// logger can be replaced easily without some black pointer magic.\ntype replaceableLogger struct {\n\tbtclog.Logger\n\tsubsystem string\n}\n\n// Loggers can not be used before the log rotator has been initialized with a\n// log file. This must be performed early during application startup by\n// calling InitLogRotator() on the main log writer instance in the config.\nvar (\n\t// lndPkgLoggers is a list of all lnd package level loggers that are\n\t// registered. They are tracked here so they can be replaced once the\n\t// SetupLoggers function is called with the final root logger.\n\tlndPkgLoggers []*replaceableLogger\n\n\t// addLndPkgLogger is a helper function that creates a new replaceable\n\t// main lnd package level logger and adds it to the list of loggers that\n\t// are replaced again later, once the final root logger is ready.\n\taddLndPkgLogger = func(subsystem string) *replaceableLogger {\n\t\tl := &replaceableLogger{\n\t\t\tLogger:    build.NewSubLogger(subsystem, nil),\n\t\t\tsubsystem: subsystem,\n\t\t}\n\t\tlndPkgLoggers = append(lndPkgLoggers, l)\n\t\treturn l\n\t}\n\n\t// Loggers that need to be accessible from the lnd package can be placed\n\t// here. Loggers that are only used in sub modules can be added directly\n\t// by using the addSubLogger method. We declare all loggers so we never\n\t// run into a nil reference if they are used early. But the SetupLoggers\n\t// function should always be called as soon as possible to finish\n\t// setting them up properly with a root logger.\n\tltndLog = addLndPkgLogger(\"LTND\")\n\trpcsLog = addLndPkgLogger(\"RPCS\")\n\tsrvrLog = addLndPkgLogger(\"SRVR\")\n\tatplLog = addLndPkgLogger(\"ATPL\")\n)\n\n// genSubLogger creates a logger for a subsystem. We provide an instance of\n// a signal.Interceptor to be able to shutdown in the case of a critical error.\nfunc genSubLogger(root *build.SubLoggerManager,\n\tinterceptor signal.Interceptor) func(string) btclog.Logger {\n\n\t// Create a shutdown function which will request shutdown from our\n\t// interceptor if it is listening.\n\tshutdown := func() {\n\t\tif !interceptor.Listening() {\n\t\t\treturn\n\t\t}\n\n\t\tinterceptor.RequestShutdown()\n\t}\n\n\t// Return a function which will create a sublogger from our root\n\t// logger without shutdown fn.\n\treturn func(tag string) btclog.Logger {\n\t\treturn root.GenSubLogger(tag, shutdown)\n\t}\n}\n\n// SetupLoggers initializes all package-global logger variables.\n//\n//nolint:ll\nfunc SetupLoggers(root *build.SubLoggerManager, interceptor signal.Interceptor) {\n\tgenLogger := genSubLogger(root, interceptor)\n\n\t// Now that we have the proper root logger, we can replace the\n\t// placeholder lnd package loggers.\n\tfor _, l := range lndPkgLoggers {\n\t\tl.Logger = build.NewSubLogger(l.subsystem, genLogger)\n\t\tSetSubLogger(root, l.subsystem, l.Logger)\n\t}\n\n\t// Initialize loggers from packages outside of `lnd` first. The\n\t// packages below will overwrite the names of the loggers they import.\n\t// For instance, the logger in `neutrino.query` is overwritten by\n\t// `btcwallet.chain`, which is overwritten by `lnwallet`. To ensure the\n\t// overwriting works, we need to initialize the loggers here so they\n\t// can be overwritten later.\n\tAddV1SubLogger(root, \"BTCN\", interceptor, neutrino.UseLogger)\n\tAddV1SubLogger(root, \"CMGR\", interceptor, connmgr.UseLogger)\n\tAddV1SubLogger(root, \"RPCC\", interceptor, rpcclient.UseLogger)\n\n\t// Some of the loggers declared in the main lnd package are also used\n\t// in sub packages.\n\tsignal.UseLogger(ltndLog)\n\tautopilot.UseLogger(atplLog)\n\n\tAddSubLogger(root, \"LNWL\", interceptor, lnwallet.UseLogger)\n\tAddSubLogger(root, \"DISC\", interceptor, discovery.UseLogger)\n\tAddSubLogger(root, \"NTFN\", interceptor, chainntnfs.UseLogger)\n\tAddSubLogger(root, \"CHDB\", interceptor, channeldb.UseLogger)\n\tAddSubLogger(root, \"SQLB\", interceptor, sqlbase.UseLogger)\n\tAddSubLogger(root, \"HSWC\", interceptor, htlcswitch.UseLogger)\n\tAddSubLogger(root, \"CNCT\", interceptor, contractcourt.UseLogger)\n\tAddSubLogger(root, \"UTXN\", interceptor, contractcourt.UseNurseryLogger)\n\tAddSubLogger(root, \"BRAR\", interceptor, contractcourt.UseBreachLogger)\n\tAddV1SubLogger(root, \"SPHX\", interceptor, sphinx.UseLogger)\n\tAddSubLogger(root, \"SWPR\", interceptor, sweep.UseLogger)\n\tAddSubLogger(root, \"SGNR\", interceptor, signrpc.UseLogger)\n\tAddSubLogger(root, \"WLKT\", interceptor, walletrpc.UseLogger)\n\tAddSubLogger(root, \"ARPC\", interceptor, autopilotrpc.UseLogger)\n\tAddSubLogger(root, \"NRPC\", interceptor, neutrinorpc.UseLogger)\n\tAddSubLogger(root, \"DRPC\", interceptor, devrpc.UseLogger)\n\tAddSubLogger(root, \"INVC\", interceptor, invoices.UseLogger)\n\tAddSubLogger(root, \"NANN\", interceptor, netann.UseLogger)\n\tAddSubLogger(root, \"WTWR\", interceptor, watchtower.UseLogger)\n\tAddSubLogger(root, \"NTFR\", interceptor, chainrpc.UseLogger)\n\tAddSubLogger(root, \"IRPC\", interceptor, invoicesrpc.UseLogger)\n\tAddSubLogger(root, \"CHNF\", interceptor, channelnotifier.UseLogger)\n\tAddSubLogger(root, \"CHBU\", interceptor, chanbackup.UseLogger)\n\tAddSubLogger(root, \"PROM\", interceptor, monitoring.UseLogger)\n\tAddSubLogger(root, \"WTCL\", interceptor, wtclient.UseLogger)\n\tAddSubLogger(root, \"PRNF\", interceptor, peernotifier.UseLogger)\n\tAddSubLogger(root, \"CHFD\", interceptor, chanfunding.UseLogger)\n\tAddSubLogger(root, \"PEER\", interceptor, peer.UseLogger)\n\tAddSubLogger(root, \"CHCL\", interceptor, chancloser.UseLogger)\n\tAddSubLogger(root, \"LCHN\", interceptor, localchans.UseLogger)\n\n\tAddSubLogger(root, routing.Subsystem, interceptor, routing.UseLogger)\n\tAddSubLogger(root, routerrpc.Subsystem, interceptor, routerrpc.UseLogger)\n\tAddSubLogger(root, chanfitness.Subsystem, interceptor, chanfitness.UseLogger)\n\tAddSubLogger(root, verrpc.Subsystem, interceptor, verrpc.UseLogger)\n\tAddSubLogger(root, healthcheck.Subsystem, interceptor, healthcheck.UseLogger)\n\tAddSubLogger(root, chainreg.Subsystem, interceptor, chainreg.UseLogger)\n\tAddSubLogger(root, chanacceptor.Subsystem, interceptor, chanacceptor.UseLogger)\n\tAddSubLogger(root, funding.Subsystem, interceptor, funding.UseLogger)\n\tAddSubLogger(root, cluster.Subsystem, interceptor, cluster.UseLogger)\n\tAddSubLogger(root, rpcperms.Subsystem, interceptor, rpcperms.UseLogger)\n\tAddSubLogger(root, tor.Subsystem, interceptor, tor.UseLogger)\n\tAddSubLogger(root, btcwallet.Subsystem, interceptor, btcwallet.UseLogger)\n\tAddSubLogger(root, rpcwallet.Subsystem, interceptor, rpcwallet.UseLogger)\n\tAddSubLogger(root, peersrpc.Subsystem, interceptor, peersrpc.UseLogger)\n\tAddSubLogger(root, graph.Subsystem, interceptor, graph.UseLogger)\n\tAddSubLogger(root, lncfg.Subsystem, interceptor, lncfg.UseLogger)\n\tAddSubLogger(\n\t\troot, blindedpath.Subsystem, interceptor, blindedpath.UseLogger,\n\t)\n\tAddV1SubLogger(root, graphdb.Subsystem, interceptor, graphdb.UseLogger)\n\tAddSubLogger(root, chainio.Subsystem, interceptor, chainio.UseLogger)\n}\n\n// AddSubLogger is a helper method to conveniently create and register the\n// logger of one or more sub systems.\nfunc AddSubLogger(root *build.SubLoggerManager, subsystem string,\n\tinterceptor signal.Interceptor, useLoggers ...func(btclog.Logger)) {\n\n\t// genSubLogger will return a callback for creating a logger instance,\n\t// which we will give to the root logger.\n\tgenLogger := genSubLogger(root, interceptor)\n\n\t// Create and register just a single logger to prevent them from\n\t// overwriting each other internally.\n\tlogger := build.NewSubLogger(subsystem, genLogger)\n\tSetSubLogger(root, subsystem, logger, useLoggers...)\n}\n\n// SetSubLogger is a helper method to conveniently register the logger of a\n// sub system.\nfunc SetSubLogger(root *build.SubLoggerManager, subsystem string,\n\tlogger btclog.Logger, useLoggers ...func(btclog.Logger)) {\n\n\troot.RegisterSubLogger(subsystem, logger)\n\tfor _, useLogger := range useLoggers {\n\t\tuseLogger(logger)\n\t}\n}\n\n// AddV1SubLogger is a helper method to conveniently create and register the\n// logger of one or more sub systems.\nfunc AddV1SubLogger(root *build.SubLoggerManager, subsystem string,\n\tinterceptor signal.Interceptor, useLoggers ...func(btclogv1.Logger)) {\n\n\t// genSubLogger will return a callback for creating a logger instance,\n\t// which we will give to the root logger.\n\tgenLogger := genSubLogger(root, interceptor)\n\n\t// Create and register just a single logger to prevent them from\n\t// overwriting each other internally.\n\tlogger := build.NewSubLogger(subsystem, genLogger)\n\tSetV1SubLogger(root, subsystem, logger, useLoggers...)\n}\n\n// SetV1SubLogger is a helper method to conveniently register the logger of a\n// sub system. Note that the btclog v2 logger implements the btclog v1 logger\n// which is why we can pass the v2 logger to the UseLogger call-backs that\n// expect the v1 logger.\nfunc SetV1SubLogger(root *build.SubLoggerManager, subsystem string,\n\tlogger btclog.Logger, useLoggers ...func(btclogv1.Logger)) {\n\n\troot.RegisterSubLogger(subsystem, logger)\n\tfor _, useLogger := range useLoggers {\n\t\tuseLogger(logger)\n\t}\n}\n"
        },
        {
          "name": "logo.png",
          "type": "blob",
          "size": 5.41015625,
          "content": null
        },
        {
          "name": "macaroons",
          "type": "tree",
          "content": null
        },
        {
          "name": "make",
          "type": "tree",
          "content": null
        },
        {
          "name": "mobile",
          "type": "tree",
          "content": null
        },
        {
          "name": "monitoring",
          "type": "tree",
          "content": null
        },
        {
          "name": "msgmux",
          "type": "tree",
          "content": null
        },
        {
          "name": "multimutex",
          "type": "tree",
          "content": null
        },
        {
          "name": "nat",
          "type": "tree",
          "content": null
        },
        {
          "name": "netann",
          "type": "tree",
          "content": null
        },
        {
          "name": "peer",
          "type": "tree",
          "content": null
        },
        {
          "name": "peernotifier",
          "type": "tree",
          "content": null
        },
        {
          "name": "pilot.go",
          "type": "blob",
          "size": 8.689453125,
          "content": "package lnd\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\n\t\"github.com/btcsuite/btcd/btcec/v2\"\n\t\"github.com/btcsuite/btcd/btcutil\"\n\t\"github.com/btcsuite/btcd/wire\"\n\t\"github.com/lightningnetwork/lnd/autopilot\"\n\t\"github.com/lightningnetwork/lnd/chainreg\"\n\t\"github.com/lightningnetwork/lnd/funding\"\n\t\"github.com/lightningnetwork/lnd/lncfg\"\n\t\"github.com/lightningnetwork/lnd/lnwallet\"\n\t\"github.com/lightningnetwork/lnd/lnwire\"\n\t\"github.com/lightningnetwork/lnd/tor\"\n)\n\n// validateAtplCfg is a helper method that makes sure the passed\n// configuration is sane. Currently it checks that the heuristic configuration\n// makes sense. In case the config is valid, it will return a list of\n// WeightedHeuristics that can be combined for use with the autopilot agent.\nfunc validateAtplCfg(cfg *lncfg.AutoPilot) ([]*autopilot.WeightedHeuristic,\n\terror) {\n\n\tvar (\n\t\theuristicsStr string\n\t\tsum           float64\n\t\theuristics    []*autopilot.WeightedHeuristic\n\t)\n\n\t// Create a help text that we can return in case the config is not\n\t// correct.\n\tfor _, a := range autopilot.AvailableHeuristics {\n\t\theuristicsStr += fmt.Sprintf(\" '%v' \", a.Name())\n\t}\n\tavailStr := fmt.Sprintf(\"Available heuristics are: [%v]\", heuristicsStr)\n\n\t// We'll go through the config and make sure all the heuristics exists,\n\t// and that the sum of their weights is 1.0.\n\tfor name, weight := range cfg.Heuristic {\n\t\ta, ok := autopilot.AvailableHeuristics[name]\n\t\tif !ok {\n\t\t\t// No heuristic matching this config option was found.\n\t\t\treturn nil, fmt.Errorf(\"heuristic %v not available. %v\",\n\t\t\t\tname, availStr)\n\t\t}\n\n\t\t// If this heuristic was among the registered ones, we add it\n\t\t// to the list we'll give to the agent, and keep track of the\n\t\t// sum of weights.\n\t\theuristics = append(\n\t\t\theuristics,\n\t\t\t&autopilot.WeightedHeuristic{\n\t\t\t\tWeight:              weight,\n\t\t\t\tAttachmentHeuristic: a,\n\t\t\t},\n\t\t)\n\t\tsum += weight\n\t}\n\n\t// Check found heuristics. We must have at least one to operate.\n\tif len(heuristics) == 0 {\n\t\treturn nil, fmt.Errorf(\"no active heuristics: %v\", availStr)\n\t}\n\n\tif sum != 1.0 {\n\t\treturn nil, fmt.Errorf(\"heuristic weights must sum to 1.0\")\n\t}\n\treturn heuristics, nil\n}\n\n// chanController is an implementation of the autopilot.ChannelController\n// interface that's backed by a running lnd instance.\ntype chanController struct {\n\tserver        *server\n\tprivate       bool\n\tminConfs      int32\n\tconfTarget    uint32\n\tchanMinHtlcIn lnwire.MilliSatoshi\n\tnetParams     chainreg.BitcoinNetParams\n}\n\n// OpenChannel opens a channel to a target peer, with a capacity of the\n// specified amount. This function should un-block immediately after the\n// funding transaction that marks the channel open has been broadcast.\nfunc (c *chanController) OpenChannel(target *btcec.PublicKey,\n\tamt btcutil.Amount) error {\n\n\t// With the connection established, we'll now establish our connection\n\t// to the target peer, waiting for the first update before we exit.\n\tfeePerKw, err := c.server.cc.FeeEstimator.EstimateFeePerKW(\n\t\tc.confTarget,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Construct the open channel request and send it to the server to begin\n\t// the funding workflow.\n\treq := &funding.InitFundingMsg{\n\t\tTargetPubkey:     target,\n\t\tChainHash:        *c.netParams.GenesisHash,\n\t\tSubtractFees:     true,\n\t\tLocalFundingAmt:  amt,\n\t\tPushAmt:          0,\n\t\tMinHtlcIn:        c.chanMinHtlcIn,\n\t\tFundingFeePerKw:  feePerKw,\n\t\tPrivate:          c.private,\n\t\tRemoteCsvDelay:   0,\n\t\tMinConfs:         c.minConfs,\n\t\tMaxValueInFlight: 0,\n\t}\n\n\tupdateStream, errChan := c.server.OpenChannel(req)\n\tselect {\n\tcase err := <-errChan:\n\t\treturn err\n\tcase <-updateStream:\n\t\treturn nil\n\tcase <-c.server.quit:\n\t\treturn nil\n\t}\n}\n\nfunc (c *chanController) CloseChannel(chanPoint *wire.OutPoint) error {\n\treturn nil\n}\n\n// A compile time assertion to ensure chanController meets the\n// autopilot.ChannelController interface.\nvar _ autopilot.ChannelController = (*chanController)(nil)\n\n// initAutoPilot initializes a new autopilot.ManagerCfg to manage an autopilot.\n// Agent instance based on the passed configuration structs. The agent and all\n// interfaces needed to drive it won't be launched before the Manager's\n// StartAgent method is called.\nfunc initAutoPilot(svr *server, cfg *lncfg.AutoPilot,\n\tminHTLCIn lnwire.MilliSatoshi, netParams chainreg.BitcoinNetParams) (\n\t*autopilot.ManagerCfg, error) {\n\n\tatplLog.Infof(\"Instantiating autopilot with active=%v, \"+\n\t\t\"max_channels=%d, allocation=%f, min_chan_size=%d, \"+\n\t\t\"max_chan_size=%d, private=%t, min_confs=%d, conf_target=%d\",\n\t\tcfg.Active, cfg.MaxChannels, cfg.Allocation, cfg.MinChannelSize,\n\t\tcfg.MaxChannelSize, cfg.Private, cfg.MinConfs, cfg.ConfTarget)\n\n\t// Set up the constraints the autopilot heuristics must adhere to.\n\tatplConstraints := autopilot.NewConstraints(\n\t\tbtcutil.Amount(cfg.MinChannelSize),\n\t\tbtcutil.Amount(cfg.MaxChannelSize),\n\t\tuint16(cfg.MaxChannels),\n\t\t10,\n\t\tcfg.Allocation,\n\t)\n\theuristics, err := validateAtplCfg(cfg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tweightedAttachment, err := autopilot.NewWeightedCombAttachment(\n\t\theuristics...,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// With the heuristic itself created, we can now populate the remainder\n\t// of the items that the autopilot agent needs to perform its duties.\n\tself := svr.identityECDH.PubKey()\n\tpilotCfg := autopilot.Config{\n\t\tSelf:      self,\n\t\tHeuristic: weightedAttachment,\n\t\tChanController: &chanController{\n\t\t\tserver:        svr,\n\t\t\tprivate:       cfg.Private,\n\t\t\tminConfs:      cfg.MinConfs,\n\t\t\tconfTarget:    cfg.ConfTarget,\n\t\t\tchanMinHtlcIn: minHTLCIn,\n\t\t\tnetParams:     netParams,\n\t\t},\n\t\tWalletBalance: func() (btcutil.Amount, error) {\n\t\t\treturn svr.cc.Wallet.ConfirmedBalance(\n\t\t\t\tcfg.MinConfs, lnwallet.DefaultAccountName,\n\t\t\t)\n\t\t},\n\t\tGraph:       autopilot.ChannelGraphFromDatabase(svr.graphDB),\n\t\tConstraints: atplConstraints,\n\t\tConnectToPeer: func(target *btcec.PublicKey, addrs []net.Addr) (bool, error) {\n\t\t\t// First, we'll check if we're already connected to the\n\t\t\t// target peer. If we are, we can exit early. Otherwise,\n\t\t\t// we'll need to establish a connection.\n\t\t\tif _, err := svr.FindPeer(target); err == nil {\n\t\t\t\treturn true, nil\n\t\t\t}\n\n\t\t\t// We can't establish a channel if no addresses were\n\t\t\t// provided for the peer.\n\t\t\tif len(addrs) == 0 {\n\t\t\t\treturn false, errors.New(\"no addresses specified\")\n\t\t\t}\n\n\t\t\tatplLog.Tracef(\"Attempting to connect to %x\",\n\t\t\t\ttarget.SerializeCompressed())\n\n\t\t\tlnAddr := &lnwire.NetAddress{\n\t\t\t\tIdentityKey: target,\n\t\t\t\tChainNet:    netParams.Net,\n\t\t\t}\n\n\t\t\t// We'll attempt to successively connect to each of the\n\t\t\t// advertised IP addresses until we've either exhausted\n\t\t\t// the advertised IP addresses, or have made a\n\t\t\t// connection.\n\t\t\tvar connected bool\n\t\t\tfor _, addr := range addrs {\n\t\t\t\tswitch addr.(type) {\n\t\t\t\tcase *net.TCPAddr, *tor.OnionAddr:\n\t\t\t\t\tlnAddr.Address = addr\n\t\t\t\tdefault:\n\t\t\t\t\treturn false, fmt.Errorf(\"unknown \"+\n\t\t\t\t\t\t\"address type %T\", addr)\n\t\t\t\t}\n\n\t\t\t\terr := svr.ConnectToPeer(\n\t\t\t\t\tlnAddr, false, svr.cfg.ConnectionTimeout,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\t// If we weren't able to connect to the\n\t\t\t\t\t// peer at this address, then we'll move\n\t\t\t\t\t// onto the next.\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tconnected = true\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If we weren't able to establish a connection at all,\n\t\t\t// then we'll error out.\n\t\t\tif !connected {\n\t\t\t\treturn false, errors.New(\"exhausted all \" +\n\t\t\t\t\t\"advertised addresses\")\n\t\t\t}\n\n\t\t\treturn false, nil\n\t\t},\n\t\tDisconnectPeer: svr.DisconnectPeer,\n\t}\n\n\t// Create and return the autopilot.ManagerCfg that administrates this\n\t// agent-pilot instance.\n\treturn &autopilot.ManagerCfg{\n\t\tSelf:     self,\n\t\tPilotCfg: &pilotCfg,\n\t\tChannelState: func() ([]autopilot.LocalChannel, error) {\n\t\t\t// We'll fetch the current state of open\n\t\t\t// channels from the database to use as initial\n\t\t\t// state for the auto-pilot agent.\n\t\t\tactiveChannels, err := svr.chanStateDB.FetchAllChannels()\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tchanState := make([]autopilot.LocalChannel,\n\t\t\t\tlen(activeChannels))\n\t\t\tfor i, channel := range activeChannels {\n\t\t\t\tlocalCommit := channel.LocalCommitment\n\t\t\t\tbalance := localCommit.LocalBalance.ToSatoshis()\n\n\t\t\t\tchanState[i] = autopilot.LocalChannel{\n\t\t\t\t\tChanID:  channel.ShortChanID(),\n\t\t\t\t\tBalance: balance,\n\t\t\t\t\tNode: autopilot.NewNodeID(\n\t\t\t\t\t\tchannel.IdentityPub,\n\t\t\t\t\t),\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn chanState, nil\n\t\t},\n\t\tChannelInfo: func(chanPoint wire.OutPoint) (\n\t\t\t*autopilot.LocalChannel, error) {\n\n\t\t\tchannel, err := svr.chanStateDB.FetchChannel(chanPoint)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tlocalCommit := channel.LocalCommitment\n\t\t\treturn &autopilot.LocalChannel{\n\t\t\t\tChanID:  channel.ShortChanID(),\n\t\t\t\tBalance: localCommit.LocalBalance.ToSatoshis(),\n\t\t\t\tNode:    autopilot.NewNodeID(channel.IdentityPub),\n\t\t\t}, nil\n\t\t},\n\t\tSubscribeTransactions: svr.cc.Wallet.SubscribeTransactions,\n\t\tSubscribeTopology:     svr.graphBuilder.SubscribeTopology,\n\t}, nil\n}\n"
        },
        {
          "name": "pool",
          "type": "tree",
          "content": null
        },
        {
          "name": "protofsm",
          "type": "tree",
          "content": null
        },
        {
          "name": "queue",
          "type": "tree",
          "content": null
        },
        {
          "name": "record",
          "type": "tree",
          "content": null
        },
        {
          "name": "routing",
          "type": "tree",
          "content": null
        },
        {
          "name": "rpcperms",
          "type": "tree",
          "content": null
        },
        {
          "name": "rpcserver.go",
          "type": "blob",
          "size": 267.318359375,
          "content": "package lnd\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/hex\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"net\"\n\t\"net/http\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/btcsuite/btcd/blockchain\"\n\t\"github.com/btcsuite/btcd/btcec/v2\"\n\t\"github.com/btcsuite/btcd/btcec/v2/ecdsa\"\n\t\"github.com/btcsuite/btcd/btcutil\"\n\t\"github.com/btcsuite/btcd/btcutil/psbt\"\n\t\"github.com/btcsuite/btcd/chaincfg\"\n\t\"github.com/btcsuite/btcd/chaincfg/chainhash\"\n\t\"github.com/btcsuite/btcd/txscript\"\n\t\"github.com/btcsuite/btcd/wire\"\n\t\"github.com/btcsuite/btcwallet/waddrmgr\"\n\t\"github.com/btcsuite/btcwallet/wallet\"\n\t\"github.com/btcsuite/btcwallet/wallet/txauthor\"\n\t\"github.com/davecgh/go-spew/spew\"\n\tproxy \"github.com/grpc-ecosystem/grpc-gateway/v2/runtime\"\n\t\"github.com/lightningnetwork/lnd/autopilot\"\n\t\"github.com/lightningnetwork/lnd/build\"\n\t\"github.com/lightningnetwork/lnd/chainreg\"\n\t\"github.com/lightningnetwork/lnd/chanacceptor\"\n\t\"github.com/lightningnetwork/lnd/chanbackup\"\n\t\"github.com/lightningnetwork/lnd/chanfitness\"\n\t\"github.com/lightningnetwork/lnd/channeldb\"\n\t\"github.com/lightningnetwork/lnd/channelnotifier\"\n\t\"github.com/lightningnetwork/lnd/clock\"\n\t\"github.com/lightningnetwork/lnd/contractcourt\"\n\t\"github.com/lightningnetwork/lnd/discovery\"\n\t\"github.com/lightningnetwork/lnd/feature\"\n\t\"github.com/lightningnetwork/lnd/fn/v2\"\n\t\"github.com/lightningnetwork/lnd/funding\"\n\t\"github.com/lightningnetwork/lnd/graph\"\n\tgraphdb \"github.com/lightningnetwork/lnd/graph/db\"\n\t\"github.com/lightningnetwork/lnd/graph/db/models\"\n\t\"github.com/lightningnetwork/lnd/graph/graphsession\"\n\t\"github.com/lightningnetwork/lnd/htlcswitch\"\n\t\"github.com/lightningnetwork/lnd/htlcswitch/hop\"\n\t\"github.com/lightningnetwork/lnd/input\"\n\t\"github.com/lightningnetwork/lnd/invoices\"\n\t\"github.com/lightningnetwork/lnd/keychain\"\n\t\"github.com/lightningnetwork/lnd/kvdb\"\n\t\"github.com/lightningnetwork/lnd/labels\"\n\t\"github.com/lightningnetwork/lnd/lncfg\"\n\t\"github.com/lightningnetwork/lnd/lnrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/invoicesrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/routerrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/walletrpc\"\n\t\"github.com/lightningnetwork/lnd/lntypes\"\n\t\"github.com/lightningnetwork/lnd/lnutils\"\n\t\"github.com/lightningnetwork/lnd/lnwallet\"\n\t\"github.com/lightningnetwork/lnd/lnwallet/btcwallet\"\n\t\"github.com/lightningnetwork/lnd/lnwallet/chainfee\"\n\t\"github.com/lightningnetwork/lnd/lnwallet/chancloser\"\n\t\"github.com/lightningnetwork/lnd/lnwallet/chanfunding\"\n\t\"github.com/lightningnetwork/lnd/lnwire\"\n\t\"github.com/lightningnetwork/lnd/macaroons\"\n\t\"github.com/lightningnetwork/lnd/peer\"\n\t\"github.com/lightningnetwork/lnd/peernotifier\"\n\t\"github.com/lightningnetwork/lnd/record\"\n\t\"github.com/lightningnetwork/lnd/routing\"\n\t\"github.com/lightningnetwork/lnd/routing/blindedpath\"\n\t\"github.com/lightningnetwork/lnd/routing/route\"\n\t\"github.com/lightningnetwork/lnd/rpcperms\"\n\t\"github.com/lightningnetwork/lnd/signal\"\n\t\"github.com/lightningnetwork/lnd/sweep\"\n\t\"github.com/lightningnetwork/lnd/tlv\"\n\t\"github.com/lightningnetwork/lnd/watchtower\"\n\t\"github.com/lightningnetwork/lnd/zpay32\"\n\t\"github.com/tv42/zbase32\"\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/status\"\n\t\"google.golang.org/protobuf/proto\"\n\t\"gopkg.in/macaroon-bakery.v2/bakery\"\n)\n\nconst (\n\t// defaultNumBlocksEstimate is the number of blocks that we fall back\n\t// to issuing an estimate for if a fee pre fence doesn't specify an\n\t// explicit conf target or fee rate.\n\tdefaultNumBlocksEstimate = 6\n)\n\nvar (\n\t// readPermissions is a slice of all entities that allow read\n\t// permissions for authorization purposes, all lowercase.\n\treadPermissions = []bakery.Op{\n\t\t{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"read\",\n\t\t},\n\t\t{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t},\n\t\t{\n\t\t\tEntity: \"address\",\n\t\t\tAction: \"read\",\n\t\t},\n\t\t{\n\t\t\tEntity: \"message\",\n\t\t\tAction: \"read\",\n\t\t},\n\t\t{\n\t\t\tEntity: \"peers\",\n\t\t\tAction: \"read\",\n\t\t},\n\t\t{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t},\n\t\t{\n\t\t\tEntity: \"invoices\",\n\t\t\tAction: \"read\",\n\t\t},\n\t\t{\n\t\t\tEntity: \"signer\",\n\t\t\tAction: \"read\",\n\t\t},\n\t\t{\n\t\t\tEntity: \"macaroon\",\n\t\t\tAction: \"read\",\n\t\t},\n\t}\n\n\t// writePermissions is a slice of all entities that allow write\n\t// permissions for authorization purposes, all lowercase.\n\twritePermissions = []bakery.Op{\n\t\t{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"write\",\n\t\t},\n\t\t{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t},\n\t\t{\n\t\t\tEntity: \"address\",\n\t\t\tAction: \"write\",\n\t\t},\n\t\t{\n\t\t\tEntity: \"message\",\n\t\t\tAction: \"write\",\n\t\t},\n\t\t{\n\t\t\tEntity: \"peers\",\n\t\t\tAction: \"write\",\n\t\t},\n\t\t{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"write\",\n\t\t},\n\t\t{\n\t\t\tEntity: \"invoices\",\n\t\t\tAction: \"write\",\n\t\t},\n\t\t{\n\t\t\tEntity: \"signer\",\n\t\t\tAction: \"generate\",\n\t\t},\n\t\t{\n\t\t\tEntity: \"macaroon\",\n\t\t\tAction: \"generate\",\n\t\t},\n\t\t{\n\t\t\tEntity: \"macaroon\",\n\t\t\tAction: \"write\",\n\t\t},\n\t}\n\n\t// invoicePermissions is a slice of all the entities that allows a user\n\t// to only access calls that are related to invoices, so: streaming\n\t// RPCs, generating, and listening invoices.\n\tinvoicePermissions = []bakery.Op{\n\t\t{\n\t\t\tEntity: \"invoices\",\n\t\t\tAction: \"read\",\n\t\t},\n\t\t{\n\t\t\tEntity: \"invoices\",\n\t\t\tAction: \"write\",\n\t\t},\n\t\t{\n\t\t\tEntity: \"address\",\n\t\t\tAction: \"read\",\n\t\t},\n\t\t{\n\t\t\tEntity: \"address\",\n\t\t\tAction: \"write\",\n\t\t},\n\t\t{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"read\",\n\t\t},\n\t}\n\n\t// TODO(guggero): Refactor into constants that are used for all\n\t// permissions in this file. Also expose the list of possible\n\t// permissions in an RPC when per RPC permissions are\n\t// implemented.\n\tvalidActions  = []string{\"read\", \"write\", \"generate\"}\n\tvalidEntities = []string{\n\t\t\"onchain\", \"offchain\", \"address\", \"message\",\n\t\t\"peers\", \"info\", \"invoices\", \"signer\", \"macaroon\",\n\t\tmacaroons.PermissionEntityCustomURI,\n\t}\n\n\t// If the --no-macaroons flag is used to start lnd, the macaroon service\n\t// is not initialized. errMacaroonDisabled is then returned when\n\t// macaroon related services are used.\n\terrMacaroonDisabled = fmt.Errorf(\"macaroon authentication disabled, \" +\n\t\t\"remove --no-macaroons flag to enable\")\n)\n\n// stringInSlice returns true if a string is contained in the given slice.\nfunc stringInSlice(a string, slice []string) bool {\n\tfor _, b := range slice {\n\t\tif b == a {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// GetAllPermissions returns all the permissions required to interact with lnd.\nfunc GetAllPermissions() []bakery.Op {\n\tallPerms := make([]bakery.Op, 0)\n\n\t// The map will help keep track of which specific permission pairs have\n\t// already been added to the slice.\n\tallPermsMap := make(map[string]map[string]struct{})\n\n\tfor _, perms := range MainRPCServerPermissions() {\n\t\tfor _, perm := range perms {\n\t\t\tentity := perm.Entity\n\t\t\taction := perm.Action\n\n\t\t\t// If this specific entity-action permission pair isn't\n\t\t\t// in the map yet. Add it to map, and the permission\n\t\t\t// slice.\n\t\t\tif acts, ok := allPermsMap[entity]; ok {\n\t\t\t\tif _, ok := acts[action]; !ok {\n\t\t\t\t\tallPermsMap[entity][action] = struct{}{}\n\n\t\t\t\t\tallPerms = append(\n\t\t\t\t\t\tallPerms, perm,\n\t\t\t\t\t)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tallPermsMap[entity] = make(map[string]struct{})\n\t\t\t\tallPermsMap[entity][action] = struct{}{}\n\t\t\t\tallPerms = append(allPerms, perm)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn allPerms\n}\n\n// MainRPCServerPermissions returns a mapping of the main RPC server calls to\n// the permissions they require.\nfunc MainRPCServerPermissions() map[string][]bakery.Op {\n\treturn map[string][]bakery.Op{\n\t\t\"/lnrpc.Lightning/SendCoins\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ListUnspent\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SendMany\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/NewAddress\": {{\n\t\t\tEntity: \"address\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SignMessage\": {{\n\t\t\tEntity: \"message\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/VerifyMessage\": {{\n\t\t\tEntity: \"message\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ConnectPeer\": {{\n\t\t\tEntity: \"peers\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/DisconnectPeer\": {{\n\t\t\tEntity: \"peers\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/OpenChannel\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"write\",\n\t\t}, {\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/BatchOpenChannel\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"write\",\n\t\t}, {\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/OpenChannelSync\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"write\",\n\t\t}, {\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/CloseChannel\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"write\",\n\t\t}, {\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/AbandonChannel\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/GetInfo\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/GetDebugInfo\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t}, {\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}, {\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"read\",\n\t\t}, {\n\t\t\tEntity: \"peers\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/GetRecoveryInfo\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ListPeers\": {{\n\t\t\tEntity: \"peers\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/WalletBalance\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/EstimateFee\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ChannelBalance\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/PendingChannels\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ListChannels\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SubscribeChannelEvents\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ClosedChannels\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SendPayment\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SendPaymentSync\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SendToRoute\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SendToRouteSync\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/AddInvoice\": {{\n\t\t\tEntity: \"invoices\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/LookupInvoice\": {{\n\t\t\tEntity: \"invoices\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ListInvoices\": {{\n\t\t\tEntity: \"invoices\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SubscribeInvoices\": {{\n\t\t\tEntity: \"invoices\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SubscribeTransactions\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/GetTransactions\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/DescribeGraph\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/GetNodeMetrics\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/GetChanInfo\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/GetNodeInfo\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/QueryRoutes\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/GetNetworkInfo\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/StopDaemon\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SubscribeChannelGraph\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ListPayments\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/DeletePayment\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/DeleteAllPayments\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/DebugLevel\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/DecodePayReq\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/FeeReport\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/UpdateChannelPolicy\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ForwardingHistory\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/RestoreChannelBackups\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ExportChannelBackup\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/VerifyChanBackup\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ExportAllChannelBackups\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SubscribeChannelBackups\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ChannelAcceptor\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"write\",\n\t\t}, {\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/BakeMacaroon\": {{\n\t\t\tEntity: \"macaroon\",\n\t\t\tAction: \"generate\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ListMacaroonIDs\": {{\n\t\t\tEntity: \"macaroon\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/DeleteMacaroonID\": {{\n\t\t\tEntity: \"macaroon\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ListPermissions\": {{\n\t\t\tEntity: \"info\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/CheckMacaroonPermissions\": {{\n\t\t\tEntity: \"macaroon\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SubscribePeerEvents\": {{\n\t\t\tEntity: \"peers\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/FundingStateStep\": {{\n\t\t\tEntity: \"onchain\",\n\t\t\tAction: \"write\",\n\t\t}, {\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\tlnrpc.RegisterRPCMiddlewareURI: {{\n\t\t\tEntity: \"macaroon\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SendCustomMessage\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"write\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/SubscribeCustomMessages\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/LookupHtlcResolution\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t\t\"/lnrpc.Lightning/ListAliases\": {{\n\t\t\tEntity: \"offchain\",\n\t\t\tAction: \"read\",\n\t\t}},\n\t}\n}\n\n// AuxDataParser is an interface that is used to parse auxiliary custom data\n// within RPC messages. This is used to transform binary blobs to human-readable\n// JSON representations.\ntype AuxDataParser interface {\n\t// InlineParseCustomData replaces any custom data binary blob in the\n\t// given RPC message with its corresponding JSON formatted data. This\n\t// transforms the binary (likely TLV encoded) data to a human-readable\n\t// JSON representation (still as byte slice).\n\tInlineParseCustomData(msg proto.Message) error\n}\n\n// rpcServer is a gRPC, RPC front end to the lnd daemon.\n// TODO(roasbeef): pagination support for the list-style calls\ntype rpcServer struct {\n\tstarted  int32 // To be used atomically.\n\tshutdown int32 // To be used atomically.\n\n\t// Required by the grpc-gateway/v2 library for forward compatibility.\n\t// Must be after the atomically used variables to not break struct\n\t// alignment.\n\tlnrpc.UnimplementedLightningServer\n\n\tserver *server\n\n\tcfg *Config\n\n\t// subServers are a set of sub-RPC servers that use the same gRPC and\n\t// listening sockets as the main RPC server, but which maintain their\n\t// own independent service. This allows us to expose a set of\n\t// micro-service like abstractions to the outside world for users to\n\t// consume.\n\tsubServers      []lnrpc.SubServer\n\tsubGrpcHandlers []lnrpc.GrpcHandler\n\n\t// routerBackend contains the backend implementation of the router\n\t// rpc sub server.\n\trouterBackend *routerrpc.RouterBackend\n\n\t// chanPredicate is used in the bidirectional ChannelAcceptor streaming\n\t// method.\n\tchanPredicate chanacceptor.MultiplexAcceptor\n\n\tquit chan struct{}\n\n\t// macService is the macaroon service that we need to mint new\n\t// macaroons.\n\tmacService *macaroons.Service\n\n\t// selfNode is our own pubkey.\n\tselfNode route.Vertex\n\n\t// interceptorChain is the interceptor added to our gRPC server.\n\tinterceptorChain *rpcperms.InterceptorChain\n\n\t// implCfg is the configuration for some of the interfaces that can be\n\t// provided externally.\n\timplCfg *ImplementationCfg\n\n\t// interceptor is used to be able to request a shutdown\n\tinterceptor signal.Interceptor\n\n\tgraphCache        sync.RWMutex\n\tdescribeGraphResp *lnrpc.ChannelGraph\n\tgraphCacheEvictor *time.Timer\n}\n\n// A compile time check to ensure that rpcServer fully implements the\n// LightningServer gRPC service.\nvar _ lnrpc.LightningServer = (*rpcServer)(nil)\n\n// newRPCServer creates and returns a new instance of the rpcServer. Before\n// dependencies are added, this will be an non-functioning RPC server only to\n// be used to register the LightningService with the gRPC server.\nfunc newRPCServer(cfg *Config, interceptorChain *rpcperms.InterceptorChain,\n\timplCfg *ImplementationCfg, interceptor signal.Interceptor) *rpcServer {\n\n\t// We go trhough the list of registered sub-servers, and create a gRPC\n\t// handler for each. These are used to register with the gRPC server\n\t// before all dependencies are available.\n\tregisteredSubServers := lnrpc.RegisteredSubServers()\n\n\tvar subServerHandlers []lnrpc.GrpcHandler\n\tfor _, subServer := range registeredSubServers {\n\t\tsubServerHandlers = append(\n\t\t\tsubServerHandlers, subServer.NewGrpcHandler(),\n\t\t)\n\t}\n\n\treturn &rpcServer{\n\t\tcfg:              cfg,\n\t\tsubGrpcHandlers:  subServerHandlers,\n\t\tinterceptorChain: interceptorChain,\n\t\timplCfg:          implCfg,\n\t\tquit:             make(chan struct{}, 1),\n\t\tinterceptor:      interceptor,\n\t}\n}\n\n// addDeps populates all dependencies needed by the RPC server, and any\n// of the sub-servers that it maintains. When this is done, the RPC server can\n// be started, and start accepting RPC calls.\nfunc (r *rpcServer) addDeps(s *server, macService *macaroons.Service,\n\tsubServerCgs *subRPCServerConfigs, atpl *autopilot.Manager,\n\tinvoiceRegistry *invoices.InvoiceRegistry, tower *watchtower.Standalone,\n\tchanPredicate chanacceptor.MultiplexAcceptor,\n\tinvoiceHtlcModifier *invoices.HtlcModificationInterceptor) error {\n\n\t// Set up router rpc backend.\n\tselfNode, err := s.graphDB.SourceNode()\n\tif err != nil {\n\t\treturn err\n\t}\n\tgraph := s.graphDB\n\n\trouterBackend := &routerrpc.RouterBackend{\n\t\tSelfNode: selfNode.PubKeyBytes,\n\t\tFetchChannelCapacity: func(chanID uint64) (btcutil.Amount,\n\t\t\terror) {\n\n\t\t\tinfo, _, _, err := graph.FetchChannelEdgesByID(chanID)\n\t\t\tif err != nil {\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\treturn info.Capacity, nil\n\t\t},\n\t\tFetchAmountPairCapacity: func(nodeFrom, nodeTo route.Vertex,\n\t\t\tamount lnwire.MilliSatoshi) (btcutil.Amount, error) {\n\n\t\t\treturn routing.FetchAmountPairCapacity(\n\t\t\t\tgraphsession.NewRoutingGraph(graph),\n\t\t\t\tselfNode.PubKeyBytes, nodeFrom, nodeTo, amount,\n\t\t\t)\n\t\t},\n\t\tFetchChannelEndpoints: func(chanID uint64) (route.Vertex,\n\t\t\troute.Vertex, error) {\n\n\t\t\tinfo, _, _, err := graph.FetchChannelEdgesByID(\n\t\t\t\tchanID,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn route.Vertex{}, route.Vertex{},\n\t\t\t\t\tfmt.Errorf(\"unable to fetch channel \"+\n\t\t\t\t\t\t\"edges by channel ID %d: %v\",\n\t\t\t\t\t\tchanID, err)\n\t\t\t}\n\n\t\t\treturn info.NodeKey1Bytes, info.NodeKey2Bytes, nil\n\t\t},\n\t\tFindRoute:              s.chanRouter.FindRoute,\n\t\tMissionControl:         s.defaultMC,\n\t\tActiveNetParams:        r.cfg.ActiveNetParams.Params,\n\t\tTower:                  s.controlTower,\n\t\tMaxTotalTimelock:       r.cfg.MaxOutgoingCltvExpiry,\n\t\tDefaultFinalCltvDelta:  uint16(r.cfg.Bitcoin.TimeLockDelta),\n\t\tSubscribeHtlcEvents:    s.htlcNotifier.SubscribeHtlcEvents,\n\t\tInterceptableForwarder: s.interceptableSwitch,\n\t\tSetChannelEnabled: func(outpoint wire.OutPoint) error {\n\t\t\treturn s.chanStatusMgr.RequestEnable(outpoint, true)\n\t\t},\n\t\tSetChannelDisabled: func(outpoint wire.OutPoint) error {\n\t\t\treturn s.chanStatusMgr.RequestDisable(outpoint, true)\n\t\t},\n\t\tSetChannelAuto:     s.chanStatusMgr.RequestAuto,\n\t\tUseStatusInitiated: subServerCgs.RouterRPC.UseStatusInitiated,\n\t\tParseCustomChannelData: func(msg proto.Message) error {\n\t\t\terr = fn.MapOptionZ(\n\t\t\t\tr.server.implCfg.AuxDataParser,\n\t\t\t\tfunc(parser AuxDataParser) error {\n\t\t\t\t\treturn parser.InlineParseCustomData(msg)\n\t\t\t\t},\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"error parsing custom data: \"+\n\t\t\t\t\t\"%w\", err)\n\t\t\t}\n\n\t\t\treturn nil\n\t\t},\n\t\tShouldSetExpEndorsement: func() bool {\n\t\t\tif s.cfg.ProtocolOptions.NoExperimentalEndorsement() {\n\t\t\t\treturn false\n\t\t\t}\n\n\t\t\treturn clock.NewDefaultClock().Now().Before(\n\t\t\t\tEndorsementExperimentEnd,\n\t\t\t)\n\t\t},\n\t}\n\n\tgenInvoiceFeatures := func() *lnwire.FeatureVector {\n\t\treturn s.featureMgr.Get(feature.SetInvoice)\n\t}\n\tgenAmpInvoiceFeatures := func() *lnwire.FeatureVector {\n\t\treturn s.featureMgr.Get(feature.SetInvoiceAmp)\n\t}\n\n\tparseAddr := func(addr string) (net.Addr, error) {\n\t\treturn parseAddr(addr, r.cfg.net)\n\t}\n\n\tvar (\n\t\tsubServers     []lnrpc.SubServer\n\t\tsubServerPerms []lnrpc.MacaroonPerms\n\t)\n\n\t// Before we create any of the sub-servers, we need to ensure that all\n\t// the dependencies they need are properly populated within each sub\n\t// server configuration struct.\n\t//\n\t// TODO(roasbeef): extend sub-sever config to have both (local vs remote) DB\n\terr = subServerCgs.PopulateDependencies(\n\t\tr.cfg, s.cc, r.cfg.networkDir, macService, atpl, invoiceRegistry,\n\t\ts.htlcSwitch, r.cfg.ActiveNetParams.Params, s.chanRouter,\n\t\trouterBackend, s.nodeSigner, s.graphDB, s.chanStateDB,\n\t\ts.sweeper, tower, s.towerClientMgr, r.cfg.net.ResolveTCPAddr,\n\t\tgenInvoiceFeatures, genAmpInvoiceFeatures,\n\t\ts.getNodeAnnouncement, s.updateAndBroadcastSelfNode, parseAddr,\n\t\trpcsLog, s.aliasMgr, r.implCfg.AuxDataParser,\n\t\tinvoiceHtlcModifier,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Now that the sub-servers have all their dependencies in place, we\n\t// can create each sub-server!\n\tfor _, subServerInstance := range r.subGrpcHandlers {\n\t\tsubServer, macPerms, err := subServerInstance.CreateSubServer(\n\t\t\tsubServerCgs,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// We'll collect the sub-server, and also the set of\n\t\t// permissions it needs for macaroons so we can apply the\n\t\t// interceptors below.\n\t\tsubServers = append(subServers, subServer)\n\t\tsubServerPerms = append(subServerPerms, macPerms)\n\t}\n\n\t// Next, we need to merge the set of sub server macaroon permissions\n\t// with the main RPC server permissions so we can unite them under a\n\t// single set of interceptors.\n\tfor m, ops := range MainRPCServerPermissions() {\n\t\terr := r.interceptorChain.AddPermission(m, ops)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tfor _, subServerPerm := range subServerPerms {\n\t\tfor method, ops := range subServerPerm {\n\t\t\terr := r.interceptorChain.AddPermission(method, ops)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\t// External subserver possibly need to register their own permissions\n\t// and macaroon validator.\n\tfor method, ops := range r.implCfg.ExternalValidator.Permissions() {\n\t\terr := r.interceptorChain.AddPermission(method, ops)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Give the external subservers the possibility to also use\n\t\t// their own validator to check any macaroons attached to calls\n\t\t// to this method. This allows them to have their own root key\n\t\t// ID database and permission entities.\n\t\terr = macService.RegisterExternalValidator(\n\t\t\tmethod, r.implCfg.ExternalValidator,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"could not register external \"+\n\t\t\t\t\"macaroon validator: %v\", err)\n\t\t}\n\t}\n\n\t// Finally, with all the set up complete, add the last dependencies to\n\t// the rpc server.\n\tr.server = s\n\tr.subServers = subServers\n\tr.routerBackend = routerBackend\n\tr.chanPredicate = chanPredicate\n\tr.macService = macService\n\tr.selfNode = selfNode.PubKeyBytes\n\n\tgraphCacheDuration := r.cfg.Caches.RPCGraphCacheDuration\n\tif graphCacheDuration != 0 {\n\t\tr.graphCacheEvictor = time.AfterFunc(graphCacheDuration, func() {\n\t\t\t// Grab the mutex and purge the current populated\n\t\t\t// describe graph response.\n\t\t\tr.graphCache.Lock()\n\t\t\tdefer r.graphCache.Unlock()\n\n\t\t\tr.describeGraphResp = nil\n\n\t\t\t// Reset ourselves as well at the end so we run again\n\t\t\t// after the duration.\n\t\t\tr.graphCacheEvictor.Reset(graphCacheDuration)\n\t\t})\n\t}\n\n\treturn nil\n}\n\n// RegisterWithGrpcServer registers the rpcServer and any subservers with the\n// root gRPC server.\nfunc (r *rpcServer) RegisterWithGrpcServer(grpcServer *grpc.Server) error {\n\t// Register the main RPC server.\n\tlnrpc.RegisterLightningServer(grpcServer, r)\n\n\t// Now the main RPC server has been registered, we'll iterate through\n\t// all the sub-RPC servers and register them to ensure that requests\n\t// are properly routed towards them.\n\tfor _, subServer := range r.subGrpcHandlers {\n\t\terr := subServer.RegisterWithRootServer(grpcServer)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to register \"+\n\t\t\t\t\"sub-server with root: %v\", err)\n\t\t}\n\t}\n\n\t// Before actually listening on the gRPC listener, give external\n\t// subservers the chance to register to our gRPC server. Those external\n\t// subservers (think GrUB) are responsible for starting/stopping on\n\t// their own, we just let them register their services to the same\n\t// server instance so all of them can be exposed on the same\n\t// port/listener.\n\terr := r.implCfg.RegisterGrpcSubserver(grpcServer)\n\tif err != nil {\n\t\trpcsLog.Errorf(\"error registering external gRPC \"+\n\t\t\t\"subserver: %v\", err)\n\t}\n\n\treturn nil\n}\n\n// Start launches any helper goroutines required for the rpcServer to function.\nfunc (r *rpcServer) Start() error {\n\tif atomic.AddInt32(&r.started, 1) != 1 {\n\t\treturn nil\n\t}\n\n\t// First, we'll start all the sub-servers to ensure that they're ready\n\t// to take new requests in.\n\t//\n\t// TODO(roasbeef): some may require that the entire daemon be started\n\t// at that point\n\tfor _, subServer := range r.subServers {\n\t\trpcsLog.Debugf(\"Starting sub RPC server: %v\", subServer.Name())\n\n\t\tif err := subServer.Start(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// RegisterWithRestProxy registers the RPC server and any subservers with the\n// given REST proxy.\nfunc (r *rpcServer) RegisterWithRestProxy(restCtx context.Context,\n\trestMux *proxy.ServeMux, restDialOpts []grpc.DialOption,\n\trestProxyDest string) error {\n\n\t// With our custom REST proxy mux created, register our main RPC and\n\t// give all subservers a chance to register as well.\n\terr := lnrpc.RegisterLightningHandlerFromEndpoint(\n\t\trestCtx, restMux, restProxyDest, restDialOpts,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Register our State service with the REST proxy.\n\terr = lnrpc.RegisterStateHandlerFromEndpoint(\n\t\trestCtx, restMux, restProxyDest, restDialOpts,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Register all the subservers with the REST proxy.\n\tfor _, subServer := range r.subGrpcHandlers {\n\t\terr := subServer.RegisterWithRestServer(\n\t\t\trestCtx, restMux, restProxyDest, restDialOpts,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to register REST sub-server \"+\n\t\t\t\t\"with root: %v\", err)\n\t\t}\n\t}\n\n\t// Before listening on any of the interfaces, we also want to give the\n\t// external subservers a chance to register their own REST proxy stub\n\t// with our mux instance.\n\terr = r.implCfg.RegisterRestSubserver(\n\t\trestCtx, restMux, restProxyDest, restDialOpts,\n\t)\n\tif err != nil {\n\t\trpcsLog.Errorf(\"error registering external REST subserver: %v\",\n\t\t\terr)\n\t}\n\treturn nil\n}\n\n// Stop signals any active goroutines for a graceful closure.\nfunc (r *rpcServer) Stop() error {\n\tif atomic.AddInt32(&r.shutdown, 1) != 1 {\n\t\treturn nil\n\t}\n\n\trpcsLog.Infof(\"Stopping RPC Server\")\n\n\tclose(r.quit)\n\n\t// After we've signalled all of our active goroutines to exit, we'll\n\t// then do the same to signal a graceful shutdown of all the sub\n\t// servers.\n\tfor _, subServer := range r.subServers {\n\t\trpcsLog.Infof(\"Stopping %v Sub-RPC Server\",\n\t\t\tsubServer.Name())\n\n\t\tif err := subServer.Stop(); err != nil {\n\t\t\trpcsLog.Errorf(\"unable to stop sub-server %v: %v\",\n\t\t\t\tsubServer.Name(), err)\n\t\t\tcontinue\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// addrPairsToOutputs converts a map describing a set of outputs to be created,\n// the outputs themselves. The passed map pairs up an address, to a desired\n// output value amount. Each address is converted to its corresponding pkScript\n// to be used within the constructed output(s).\nfunc addrPairsToOutputs(addrPairs map[string]int64,\n\tparams *chaincfg.Params) ([]*wire.TxOut, error) {\n\n\toutputs := make([]*wire.TxOut, 0, len(addrPairs))\n\tfor addr, amt := range addrPairs {\n\t\taddr, err := btcutil.DecodeAddress(addr, params)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif !addr.IsForNet(params) {\n\t\t\treturn nil, fmt.Errorf(\"address is not for %s\",\n\t\t\t\tparams.Name)\n\t\t}\n\n\t\tpkscript, err := txscript.PayToAddrScript(addr)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\toutputs = append(outputs, wire.NewTxOut(amt, pkscript))\n\t}\n\n\treturn outputs, nil\n}\n\n// allowCORS wraps the given http.Handler with a function that adds the\n// Access-Control-Allow-Origin header to the response.\nfunc allowCORS(handler http.Handler, origins []string) http.Handler {\n\tallowHeaders := \"Access-Control-Allow-Headers\"\n\tallowMethods := \"Access-Control-Allow-Methods\"\n\tallowOrigin := \"Access-Control-Allow-Origin\"\n\n\t// If the user didn't supply any origins that means CORS is disabled\n\t// and we should return the original handler.\n\tif len(origins) == 0 {\n\t\treturn handler\n\t}\n\n\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\torigin := r.Header.Get(\"Origin\")\n\n\t\t// Skip everything if the browser doesn't send the Origin field.\n\t\tif origin == \"\" {\n\t\t\thandler.ServeHTTP(w, r)\n\t\t\treturn\n\t\t}\n\n\t\t// Set the static header fields first.\n\t\tw.Header().Set(\n\t\t\tallowHeaders,\n\t\t\t\"Content-Type, Accept, Grpc-Metadata-Macaroon\",\n\t\t)\n\t\tw.Header().Set(allowMethods, \"GET, POST, DELETE\")\n\n\t\t// Either we allow all origins or the incoming request matches\n\t\t// a specific origin in our list of allowed origins.\n\t\tfor _, allowedOrigin := range origins {\n\t\t\tif allowedOrigin == \"*\" || origin == allowedOrigin {\n\t\t\t\t// Only set allowed origin to requested origin.\n\t\t\t\tw.Header().Set(allowOrigin, origin)\n\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\t// For a pre-flight request we only need to send the headers\n\t\t// back. No need to call the rest of the chain.\n\t\tif r.Method == \"OPTIONS\" {\n\t\t\treturn\n\t\t}\n\n\t\t// Everything's prepared now, we can pass the request along the\n\t\t// chain of handlers.\n\t\thandler.ServeHTTP(w, r)\n\t})\n}\n\n// sendCoinsOnChain makes an on-chain transaction in or to send coins to one or\n// more addresses specified in the passed payment map. The payment map maps an\n// address to a specified output value to be sent to that address.\nfunc (r *rpcServer) sendCoinsOnChain(paymentMap map[string]int64,\n\tfeeRate chainfee.SatPerKWeight, minConfs int32, label string,\n\tstrategy wallet.CoinSelectionStrategy,\n\tselectedUtxos fn.Set[wire.OutPoint]) (*chainhash.Hash, error) {\n\n\toutputs, err := addrPairsToOutputs(paymentMap, r.cfg.ActiveNetParams.Params)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// We first do a dry run, to sanity check we won't spend our wallet\n\t// balance below the reserved amount.\n\tauthoredTx, err := r.server.cc.Wallet.CreateSimpleTx(\n\t\tselectedUtxos, outputs, feeRate, minConfs, strategy, true,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Check the authored transaction and use the explicitly set change index\n\t// to make sure that the wallet reserved balance is not invalidated.\n\t_, err = r.server.cc.Wallet.CheckReservedValueTx(\n\t\tlnwallet.CheckReservedValueTxReq{\n\t\t\tTx:          authoredTx.Tx,\n\t\t\tChangeIndex: &authoredTx.ChangeIndex,\n\t\t},\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// If that checks out, we're fairly confident that creating sending to\n\t// these outputs will keep the wallet balance above the reserve.\n\ttx, err := r.server.cc.Wallet.SendOutputs(\n\t\tselectedUtxos, outputs, feeRate, minConfs, label, strategy,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ttxHash := tx.TxHash()\n\treturn &txHash, nil\n}\n\n// ListUnspent returns useful information about each unspent output owned by\n// the wallet, as reported by the underlying `ListUnspentWitness`; the\n// information returned is: outpoint, amount in satoshis, address, address\n// type, scriptPubKey in hex and number of confirmations.  The result is\n// filtered to contain outputs whose number of confirmations is between a\n// minimum and maximum number of confirmations specified by the user, with\n// 0 meaning unconfirmed.\nfunc (r *rpcServer) ListUnspent(ctx context.Context,\n\tin *lnrpc.ListUnspentRequest) (*lnrpc.ListUnspentResponse, error) {\n\n\t// Validate the confirmation arguments.\n\tminConfs, maxConfs, err := lnrpc.ParseConfs(in.MinConfs, in.MaxConfs)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// With our arguments validated, we'll query the internal wallet for\n\t// the set of UTXOs that match our query.\n\t//\n\t// We'll acquire the global coin selection lock to ensure there aren't\n\t// any other concurrent processes attempting to lock any UTXOs which may\n\t// be shown available to us.\n\tvar utxos []*lnwallet.Utxo\n\terr = r.server.cc.Wallet.WithCoinSelectLock(func() error {\n\t\tutxos, err = r.server.cc.Wallet.ListUnspentWitness(\n\t\t\tminConfs, maxConfs, in.Account,\n\t\t)\n\t\treturn err\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trpcUtxos, err := lnrpc.MarshalUtxos(utxos, r.cfg.ActiveNetParams.Params)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tmaxStr := \"\"\n\tif maxConfs != math.MaxInt32 {\n\t\tmaxStr = \" max=\" + fmt.Sprintf(\"%d\", maxConfs)\n\t}\n\n\trpcsLog.Debugf(\"[listunspent] min=%v%v, generated utxos: %v\", minConfs,\n\t\tmaxStr, utxos)\n\n\treturn &lnrpc.ListUnspentResponse{\n\t\tUtxos: rpcUtxos,\n\t}, nil\n}\n\n// EstimateFee handles a request for estimating the fee for sending a\n// transaction spending to multiple specified outputs in parallel.\nfunc (r *rpcServer) EstimateFee(ctx context.Context,\n\tin *lnrpc.EstimateFeeRequest) (*lnrpc.EstimateFeeResponse, error) {\n\n\t// Create the list of outputs we are spending to.\n\toutputs, err := addrPairsToOutputs(in.AddrToAmount, r.cfg.ActiveNetParams.Params)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Query the fee estimator for the fee rate for the given confirmation\n\t// target.\n\ttarget := in.TargetConf\n\tfeePref := sweep.FeeEstimateInfo{\n\t\tConfTarget: uint32(target),\n\t}\n\n\t// Since we are providing a fee estimation as an RPC response, there's\n\t// no need to set a max feerate here, so we use 0.\n\tfeePerKw, err := feePref.Estimate(r.server.cc.FeeEstimator, 0)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Then, we'll extract the minimum number of confirmations that each\n\t// output we use to fund the transaction should satisfy.\n\tminConfs, err := lnrpc.ExtractMinConfs(\n\t\tin.GetMinConfs(), in.GetSpendUnconfirmed(),\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcoinSelectionStrategy, err := lnrpc.UnmarshallCoinSelectionStrategy(\n\t\tin.CoinSelectionStrategy,\n\t\tr.server.cc.Wallet.Cfg.CoinSelectionStrategy,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// We will ask the wallet to create a tx using this fee rate. We set\n\t// dryRun=true to avoid inflating the change addresses in the db.\n\tvar tx *txauthor.AuthoredTx\n\twallet := r.server.cc.Wallet\n\terr = wallet.WithCoinSelectLock(func() error {\n\t\ttx, err = wallet.CreateSimpleTx(\n\t\t\tnil, outputs, feePerKw, minConfs, coinSelectionStrategy,\n\t\t\ttrue,\n\t\t)\n\t\treturn err\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Use the created tx to calculate the total fee.\n\ttotalOutput := int64(0)\n\tfor _, out := range tx.Tx.TxOut {\n\t\ttotalOutput += out.Value\n\t}\n\ttotalFee := int64(tx.TotalInput) - totalOutput\n\n\tresp := &lnrpc.EstimateFeeResponse{\n\t\tFeeSat:      totalFee,\n\t\tSatPerVbyte: uint64(feePerKw.FeePerVByte()),\n\n\t\t// Deprecated field.\n\t\tFeerateSatPerByte: int64(feePerKw.FeePerVByte()),\n\t}\n\n\trpcsLog.Debugf(\"[estimatefee] fee estimate for conf target %d: %v\",\n\t\ttarget, resp)\n\n\treturn resp, nil\n}\n\n// maybeUseDefaultConf makes sure that when the user doesn't set either the fee\n// rate or conf target, the default conf target is used.\nfunc maybeUseDefaultConf(satPerByte int64, satPerVByte uint64,\n\ttargetConf uint32) uint32 {\n\n\t// If the fee rate is set, there's no need to use the default conf\n\t// target. In this case, we just return the targetConf from the\n\t// request.\n\tif satPerByte != 0 || satPerVByte != 0 {\n\t\treturn targetConf\n\t}\n\n\t// Return the user specified conf target if set.\n\tif targetConf != 0 {\n\t\treturn targetConf\n\t}\n\n\t// If the fee rate is not set, yet the conf target is zero, the default\n\t// 6 will be returned.\n\trpcsLog.Errorf(\"Expected either 'sat_per_vbyte' or 'conf_target' to \" +\n\t\t\"be set, using default conf of 6 instead\")\n\n\treturn defaultNumBlocksEstimate\n}\n\n// SendCoins executes a request to send coins to a particular address. Unlike\n// SendMany, this RPC call only allows creating a single output at a time.\nfunc (r *rpcServer) SendCoins(ctx context.Context,\n\tin *lnrpc.SendCoinsRequest) (*lnrpc.SendCoinsResponse, error) {\n\n\t// Keep the old behavior prior to 0.18.0 - when the user doesn't set\n\t// fee rate or conf target, the default conf target of 6 is used.\n\ttargetConf := maybeUseDefaultConf(\n\t\tin.SatPerByte, in.SatPerVbyte, uint32(in.TargetConf),\n\t)\n\n\t// Calculate an appropriate fee rate for this transaction.\n\tfeePerKw, err := lnrpc.CalculateFeeRate(\n\t\tuint64(in.SatPerByte), in.SatPerVbyte, // nolint:staticcheck\n\t\ttargetConf, r.server.cc.FeeEstimator,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Then, we'll extract the minimum number of confirmations that each\n\t// output we use to fund the transaction should satisfy.\n\tminConfs, err := lnrpc.ExtractMinConfs(in.MinConfs, in.SpendUnconfirmed)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trpcsLog.Infof(\"[sendcoins] addr=%v, amt=%v, sat/kw=%v, min_confs=%v, \"+\n\t\t\"send_all=%v, select_outpoints=%v\",\n\t\tin.Addr, btcutil.Amount(in.Amount), int64(feePerKw), minConfs,\n\t\tin.SendAll, len(in.Outpoints))\n\n\t// Decode the address receiving the coins, we need to check whether the\n\t// address is valid for this network.\n\ttargetAddr, err := btcutil.DecodeAddress(\n\t\tin.Addr, r.cfg.ActiveNetParams.Params,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Make the check on the decoded address according to the active network.\n\tif !targetAddr.IsForNet(r.cfg.ActiveNetParams.Params) {\n\t\treturn nil, fmt.Errorf(\"address: %v is not valid for this \"+\n\t\t\t\"network: %v\", targetAddr.String(),\n\t\t\tr.cfg.ActiveNetParams.Params.Name)\n\t}\n\n\t// If the destination address parses to a valid pubkey, we assume the user\n\t// accidentally tried to send funds to a bare pubkey address. This check is\n\t// here to prevent unintended transfers.\n\tdecodedAddr, _ := hex.DecodeString(in.Addr)\n\t_, err = btcec.ParsePubKey(decodedAddr)\n\tif err == nil {\n\t\treturn nil, fmt.Errorf(\"cannot send coins to pubkeys\")\n\t}\n\n\tlabel, err := labels.ValidateAPI(in.Label)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcoinSelectionStrategy, err := lnrpc.UnmarshallCoinSelectionStrategy(\n\t\tin.CoinSelectionStrategy,\n\t\tr.server.cc.Wallet.Cfg.CoinSelectionStrategy,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar txid *chainhash.Hash\n\n\twallet := r.server.cc.Wallet\n\tmaxFeeRate := r.cfg.Sweeper.MaxFeeRate.FeePerKWeight()\n\n\tvar selectOutpoints fn.Set[wire.OutPoint]\n\tif len(in.Outpoints) != 0 {\n\t\twireOutpoints, err := toWireOutpoints(in.Outpoints)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"can't create outpoints \"+\n\t\t\t\t\"%w\", err)\n\t\t}\n\n\t\tif fn.HasDuplicates(wireOutpoints) {\n\t\t\treturn nil, fmt.Errorf(\"selected outpoints contain \" +\n\t\t\t\t\"duplicate values\")\n\t\t}\n\n\t\tselectOutpoints = fn.NewSet(wireOutpoints...)\n\t}\n\n\t// If the send all flag is active, then we'll attempt to sweep all the\n\t// coins in the wallet in a single transaction (if possible),\n\t// otherwise, we'll respect the amount, and attempt a regular 2-output\n\t// send.\n\tif in.SendAll {\n\t\t// At this point, the amount shouldn't be set since we've been\n\t\t// instructed to sweep all the coins from the wallet.\n\t\tif in.Amount != 0 {\n\t\t\treturn nil, fmt.Errorf(\"amount set while SendAll is \" +\n\t\t\t\t\"active\")\n\t\t}\n\n\t\t_, bestHeight, err := r.server.cc.ChainIO.GetBestBlock()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// With the sweeper instance created, we can now generate a\n\t\t// transaction that will sweep ALL outputs from the wallet in a\n\t\t// single transaction. This will be generated in a concurrent\n\t\t// safe manner, so no need to worry about locking. The tx will\n\t\t// pay to the change address created above if we needed to\n\t\t// reserve any value, the rest will go to targetAddr.\n\t\tsweepTxPkg, err := sweep.CraftSweepAllTx(\n\t\t\tfeePerKw, maxFeeRate, uint32(bestHeight), nil,\n\t\t\ttargetAddr, wallet, wallet, wallet.WalletController,\n\t\t\tr.server.cc.Signer, minConfs, selectOutpoints,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Before we publish the transaction we make sure it won't\n\t\t// violate our reserved wallet value.\n\t\tvar reservedVal btcutil.Amount\n\t\terr = wallet.WithCoinSelectLock(func() error {\n\t\t\tvar err error\n\t\t\treservedVal, err = wallet.CheckReservedValueTx(\n\t\t\t\tlnwallet.CheckReservedValueTxReq{\n\t\t\t\t\tTx: sweepTxPkg.SweepTx,\n\t\t\t\t},\n\t\t\t)\n\t\t\treturn err\n\t\t})\n\n\t\t// If sending everything to this address would invalidate our\n\t\t// reserved wallet balance, we create a new sweep tx, where\n\t\t// we'll send the reserved value back to our wallet.\n\t\tif err == lnwallet.ErrReservedValueInvalidated {\n\t\t\tsweepTxPkg.CancelSweepAttempt()\n\n\t\t\trpcsLog.Debugf(\"Reserved value %v not satisfied after \"+\n\t\t\t\t\"send_all, trying with change output\",\n\t\t\t\treservedVal)\n\n\t\t\t// We'll request a change address from the wallet,\n\t\t\t// where we'll send this reserved value back to. This\n\t\t\t// ensures this is an address the wallet knows about,\n\t\t\t// allowing us to pass the reserved value check.\n\t\t\tchangeAddr, err := r.server.cc.Wallet.NewAddress(\n\t\t\t\tlnwallet.TaprootPubkey, true,\n\t\t\t\tlnwallet.DefaultAccountName,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\t// Send the reserved value to this change address, the\n\t\t\t// remaining funds will go to the targetAddr.\n\t\t\toutputs := []sweep.DeliveryAddr{\n\t\t\t\t{\n\t\t\t\t\tAddr: changeAddr,\n\t\t\t\t\tAmt:  reservedVal,\n\t\t\t\t},\n\t\t\t}\n\n\t\t\tsweepTxPkg, err = sweep.CraftSweepAllTx(\n\t\t\t\tfeePerKw, maxFeeRate, uint32(bestHeight),\n\t\t\t\toutputs, targetAddr, wallet, wallet,\n\t\t\t\twallet.WalletController,\n\t\t\t\tr.server.cc.Signer, minConfs, selectOutpoints,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\t// Sanity check the new tx by re-doing the check.\n\t\t\terr = wallet.WithCoinSelectLock(func() error {\n\t\t\t\t_, err := wallet.CheckReservedValueTx(\n\t\t\t\t\tlnwallet.CheckReservedValueTxReq{\n\t\t\t\t\t\tTx: sweepTxPkg.SweepTx,\n\t\t\t\t\t},\n\t\t\t\t)\n\t\t\t\treturn err\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\tsweepTxPkg.CancelSweepAttempt()\n\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t} else if err != nil {\n\t\t\tsweepTxPkg.CancelSweepAttempt()\n\n\t\t\treturn nil, err\n\t\t}\n\n\t\trpcsLog.Debugf(\"Sweeping coins from wallet to addr=%v, \"+\n\t\t\t\"with tx=%v\", in.Addr, spew.Sdump(sweepTxPkg.SweepTx))\n\n\t\t// As our sweep transaction was created, successfully, we'll\n\t\t// now attempt to publish it, cancelling the sweep pkg to\n\t\t// return all outputs if it fails.\n\t\terr = wallet.PublishTransaction(sweepTxPkg.SweepTx, label)\n\t\tif err != nil {\n\t\t\tsweepTxPkg.CancelSweepAttempt()\n\n\t\t\treturn nil, fmt.Errorf(\"unable to broadcast sweep \"+\n\t\t\t\t\"transaction: %v\", err)\n\t\t}\n\n\t\tsweepTXID := sweepTxPkg.SweepTx.TxHash()\n\t\ttxid = &sweepTXID\n\t} else {\n\n\t\t// We'll now construct out payment map, and use the wallet's\n\t\t// coin selection synchronization method to ensure that no coin\n\t\t// selection (funding, sweep alls, other sends) can proceed\n\t\t// while we instruct the wallet to send this transaction.\n\t\tpaymentMap := map[string]int64{targetAddr.String(): in.Amount}\n\t\terr := wallet.WithCoinSelectLock(func() error {\n\t\t\tnewTXID, err := r.sendCoinsOnChain(\n\t\t\t\tpaymentMap, feePerKw, minConfs, label,\n\t\t\t\tcoinSelectionStrategy, selectOutpoints,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\ttxid = newTXID\n\n\t\t\treturn nil\n\t\t})\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\trpcsLog.Infof(\"[sendcoins] spend generated txid: %v\", txid.String())\n\n\treturn &lnrpc.SendCoinsResponse{Txid: txid.String()}, nil\n}\n\n// SendMany handles a request for a transaction create multiple specified\n// outputs in parallel.\nfunc (r *rpcServer) SendMany(ctx context.Context,\n\tin *lnrpc.SendManyRequest) (*lnrpc.SendManyResponse, error) {\n\n\t// Keep the old behavior prior to 0.18.0 - when the user doesn't set\n\t// fee rate or conf target, the default conf target of 6 is used.\n\ttargetConf := maybeUseDefaultConf(\n\t\tin.SatPerByte, in.SatPerVbyte, uint32(in.TargetConf),\n\t)\n\n\t// Calculate an appropriate fee rate for this transaction.\n\tfeePerKw, err := lnrpc.CalculateFeeRate(\n\t\tuint64(in.SatPerByte), in.SatPerVbyte, // nolint:staticcheck\n\t\ttargetConf, r.server.cc.FeeEstimator,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Then, we'll extract the minimum number of confirmations that each\n\t// output we use to fund the transaction should satisfy.\n\tminConfs, err := lnrpc.ExtractMinConfs(in.MinConfs, in.SpendUnconfirmed)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlabel, err := labels.ValidateAPI(in.Label)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcoinSelectionStrategy, err := lnrpc.UnmarshallCoinSelectionStrategy(\n\t\tin.CoinSelectionStrategy,\n\t\tr.server.cc.Wallet.Cfg.CoinSelectionStrategy,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trpcsLog.Infof(\"[sendmany] outputs=%v, sat/kw=%v\",\n\t\tspew.Sdump(in.AddrToAmount), int64(feePerKw))\n\n\tvar txid *chainhash.Hash\n\n\t// We'll attempt to send to the target set of outputs, ensuring that we\n\t// synchronize with any other ongoing coin selection attempts which\n\t// happen to also be concurrently executing.\n\twallet := r.server.cc.Wallet\n\terr = wallet.WithCoinSelectLock(func() error {\n\t\tsendManyTXID, err := r.sendCoinsOnChain(\n\t\t\tin.AddrToAmount, feePerKw, minConfs, label,\n\t\t\tcoinSelectionStrategy, nil,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\ttxid = sendManyTXID\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trpcsLog.Infof(\"[sendmany] spend generated txid: %v\", txid.String())\n\n\treturn &lnrpc.SendManyResponse{Txid: txid.String()}, nil\n}\n\n// NewAddress creates a new address under control of the local wallet.\nfunc (r *rpcServer) NewAddress(ctx context.Context,\n\tin *lnrpc.NewAddressRequest) (*lnrpc.NewAddressResponse, error) {\n\n\t// Always use the default wallet account unless one was specified.\n\taccount := lnwallet.DefaultAccountName\n\tif in.Account != \"\" {\n\t\taccount = in.Account\n\t}\n\n\t// Translate the gRPC proto address type to the wallet controller's\n\t// available address types.\n\tvar (\n\t\taddr btcutil.Address\n\t\terr  error\n\t)\n\tswitch in.Type {\n\tcase lnrpc.AddressType_WITNESS_PUBKEY_HASH:\n\t\taddr, err = r.server.cc.Wallet.NewAddress(\n\t\t\tlnwallet.WitnessPubKey, false, account,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\tcase lnrpc.AddressType_NESTED_PUBKEY_HASH:\n\t\taddr, err = r.server.cc.Wallet.NewAddress(\n\t\t\tlnwallet.NestedWitnessPubKey, false, account,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\tcase lnrpc.AddressType_TAPROOT_PUBKEY:\n\t\taddr, err = r.server.cc.Wallet.NewAddress(\n\t\t\tlnwallet.TaprootPubkey, false, account,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\tcase lnrpc.AddressType_UNUSED_WITNESS_PUBKEY_HASH:\n\t\taddr, err = r.server.cc.Wallet.LastUnusedAddress(\n\t\t\tlnwallet.WitnessPubKey, account,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\tcase lnrpc.AddressType_UNUSED_NESTED_PUBKEY_HASH:\n\t\taddr, err = r.server.cc.Wallet.LastUnusedAddress(\n\t\t\tlnwallet.NestedWitnessPubKey, account,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\tcase lnrpc.AddressType_UNUSED_TAPROOT_PUBKEY:\n\t\taddr, err = r.server.cc.Wallet.LastUnusedAddress(\n\t\t\tlnwallet.TaprootPubkey, account,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown address type: %v\", in.Type)\n\t}\n\n\trpcsLog.Debugf(\"[newaddress] account=%v type=%v addr=%v\", account,\n\t\tin.Type, addr.String())\n\treturn &lnrpc.NewAddressResponse{Address: addr.String()}, nil\n}\n\nvar (\n\t// signedMsgPrefix is a special prefix that we'll prepend to any\n\t// messages we sign/verify. We do this to ensure that we don't\n\t// accidentally sign a sighash, or other sensitive material. By\n\t// prepending this fragment, we mind message signing to our particular\n\t// context.\n\tsignedMsgPrefix = []byte(\"Lightning Signed Message:\")\n)\n\n// SignMessage signs a message with the resident node's private key. The\n// returned signature string is zbase32 encoded and pubkey recoverable, meaning\n// that only the message digest and signature are needed for verification.\nfunc (r *rpcServer) SignMessage(_ context.Context,\n\tin *lnrpc.SignMessageRequest) (*lnrpc.SignMessageResponse, error) {\n\n\tif in.Msg == nil {\n\t\treturn nil, fmt.Errorf(\"need a message to sign\")\n\t}\n\n\tin.Msg = append(signedMsgPrefix, in.Msg...)\n\tsigBytes, err := r.server.nodeSigner.SignMessageCompact(\n\t\tin.Msg, !in.SingleHash,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tsig := zbase32.EncodeToString(sigBytes)\n\treturn &lnrpc.SignMessageResponse{Signature: sig}, nil\n}\n\n// VerifyMessage verifies a signature over a msg. The signature must be zbase32\n// encoded and signed by an active node in the resident node's channel\n// database. In addition to returning the validity of the signature,\n// VerifyMessage also returns the recovered pubkey from the signature.\nfunc (r *rpcServer) VerifyMessage(ctx context.Context,\n\tin *lnrpc.VerifyMessageRequest) (*lnrpc.VerifyMessageResponse, error) {\n\n\tif in.Msg == nil {\n\t\treturn nil, fmt.Errorf(\"need a message to verify\")\n\t}\n\n\t// The signature should be zbase32 encoded\n\tsig, err := zbase32.DecodeString(in.Signature)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to decode signature: %w\", err)\n\t}\n\n\t// The signature is over the double-sha256 hash of the message.\n\tin.Msg = append(signedMsgPrefix, in.Msg...)\n\tdigest := chainhash.DoubleHashB(in.Msg)\n\n\t// RecoverCompact both recovers the pubkey and validates the signature.\n\tpubKey, _, err := ecdsa.RecoverCompact(sig, digest)\n\tif err != nil {\n\t\treturn &lnrpc.VerifyMessageResponse{Valid: false}, nil\n\t}\n\tpubKeyHex := hex.EncodeToString(pubKey.SerializeCompressed())\n\n\tvar pub [33]byte\n\tcopy(pub[:], pubKey.SerializeCompressed())\n\n\t// Query the channel graph to ensure a node in the network with active\n\t// channels signed the message.\n\t//\n\t// TODO(phlip9): Require valid nodes to have capital in active channels.\n\tgraph := r.server.graphDB\n\t_, active, err := graph.HasLightningNode(pub)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to query graph: %w\", err)\n\t}\n\n\treturn &lnrpc.VerifyMessageResponse{\n\t\tValid:  active,\n\t\tPubkey: pubKeyHex,\n\t}, nil\n}\n\n// ConnectPeer attempts to establish a connection to a remote peer.\nfunc (r *rpcServer) ConnectPeer(ctx context.Context,\n\tin *lnrpc.ConnectPeerRequest) (*lnrpc.ConnectPeerResponse, error) {\n\n\t// The server hasn't yet started, so it won't be able to service any of\n\t// our requests, so we'll bail early here.\n\tif !r.server.Started() {\n\t\treturn nil, ErrServerNotActive\n\t}\n\n\tif in.Addr == nil {\n\t\treturn nil, fmt.Errorf(\"need: lnc pubkeyhash@hostname\")\n\t}\n\n\tpubkeyHex, err := hex.DecodeString(in.Addr.Pubkey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpubKey, err := btcec.ParsePubKey(pubkeyHex)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Connections to ourselves are disallowed for obvious reasons.\n\tif pubKey.IsEqual(r.server.identityECDH.PubKey()) {\n\t\treturn nil, fmt.Errorf(\"cannot make connection to self\")\n\t}\n\n\taddr, err := parseAddr(in.Addr.Host, r.cfg.net)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tpeerAddr := &lnwire.NetAddress{\n\t\tIdentityKey: pubKey,\n\t\tAddress:     addr,\n\t\tChainNet:    r.cfg.ActiveNetParams.Net,\n\t}\n\n\trpcsLog.Debugf(\"[connectpeer] requested connection to %x@%s\",\n\t\tpeerAddr.IdentityKey.SerializeCompressed(), peerAddr.Address)\n\n\t// By default, we will use the global connection timeout value.\n\ttimeout := r.cfg.ConnectionTimeout\n\n\t// Check if the connection timeout is set. If set, we will use it in our\n\t// request.\n\tif in.Timeout != 0 {\n\t\ttimeout = time.Duration(in.Timeout) * time.Second\n\t\trpcsLog.Debugf(\"[connectpeer] connection timeout is set to %v\",\n\t\t\ttimeout)\n\t}\n\n\tif err := r.server.ConnectToPeer(\n\t\tpeerAddr, in.Perm, timeout,\n\t); err != nil {\n\t\trpcsLog.Errorf(\"[connectpeer]: error connecting to peer: %v\",\n\t\t\terr)\n\t\treturn nil, err\n\t}\n\n\trpcsLog.Debugf(\"Connected to peer: %v\", peerAddr.String())\n\n\treturn &lnrpc.ConnectPeerResponse{\n\t\tStatus: fmt.Sprintf(\"connection to %v initiated\",\n\t\t\tpeerAddr.String()),\n\t}, nil\n}\n\n// DisconnectPeer attempts to disconnect one peer from another identified by a\n// given pubKey. In the case that we currently have a pending or active channel\n// with the target peer, this action will be disallowed.\nfunc (r *rpcServer) DisconnectPeer(ctx context.Context,\n\tin *lnrpc.DisconnectPeerRequest) (*lnrpc.DisconnectPeerResponse, error) {\n\n\trpcsLog.Debugf(\"[disconnectpeer] from peer(%s)\", in.PubKey)\n\n\tif !r.server.Started() {\n\t\treturn nil, ErrServerNotActive\n\t}\n\n\t// First we'll validate the string passed in within the request to\n\t// ensure that it's a valid hex-string, and also a valid compressed\n\t// public key.\n\tpubKeyBytes, err := hex.DecodeString(in.PubKey)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to decode pubkey bytes: %w\", err)\n\t}\n\tpeerPubKey, err := btcec.ParsePubKey(pubKeyBytes)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to parse pubkey: %w\", err)\n\t}\n\n\t// Next, we'll fetch the pending/active channels we have with a\n\t// particular peer.\n\tnodeChannels, err := r.server.chanStateDB.FetchOpenChannels(peerPubKey)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to fetch channels for peer: %w\",\n\t\t\terr)\n\t}\n\n\t// In order to avoid erroneously disconnecting from a peer that we have\n\t// an active channel with, if we have any channels active with this\n\t// peer, then we'll disallow disconnecting from them.\n\tif len(nodeChannels) != 0 {\n\t\t// If we are not in a dev environment or the configed dev value\n\t\t// `unsafedisconnect` is false, we return an error since there\n\t\t// are active channels.\n\t\tif !r.cfg.Dev.GetUnsafeDisconnect() {\n\t\t\treturn nil, fmt.Errorf(\"cannot disconnect from \"+\n\t\t\t\t\"peer(%x), still has %d active channels\",\n\t\t\t\tpubKeyBytes, len(nodeChannels))\n\t\t}\n\n\t\t// We are in a dev environment, print a warning log and\n\t\t// disconnect.\n\t\trpcsLog.Warnf(\"UnsafeDisconnect mode, disconnecting from \"+\n\t\t\t\"peer(%x) while there are %d active channels\",\n\t\t\tpubKeyBytes, len(nodeChannels))\n\t}\n\n\t// With all initial validation complete, we'll now request that the\n\t// server disconnects from the peer.\n\terr = r.server.DisconnectPeer(peerPubKey)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to disconnect peer: %w\", err)\n\t}\n\n\treturn &lnrpc.DisconnectPeerResponse{\n\t\tStatus: \"disconnect initiated\",\n\t}, nil\n}\n\n// newFundingShimAssembler returns a new fully populated\n// chanfunding.CannedAssembler using a FundingShim obtained from an RPC caller.\nfunc newFundingShimAssembler(chanPointShim *lnrpc.ChanPointShim, initiator bool,\n\tkeyRing keychain.KeyRing) (chanfunding.Assembler, error) {\n\n\t// Perform some basic sanity checks to ensure that all the expected\n\t// fields are populated.\n\tswitch {\n\tcase chanPointShim.RemoteKey == nil:\n\t\treturn nil, fmt.Errorf(\"remote key not set\")\n\n\tcase chanPointShim.LocalKey == nil:\n\t\treturn nil, fmt.Errorf(\"local key desc not set\")\n\n\tcase chanPointShim.LocalKey.RawKeyBytes == nil:\n\t\treturn nil, fmt.Errorf(\"local raw key bytes not set\")\n\n\tcase chanPointShim.LocalKey.KeyLoc == nil:\n\t\treturn nil, fmt.Errorf(\"local key loc not set\")\n\n\tcase chanPointShim.ChanPoint == nil:\n\t\treturn nil, fmt.Errorf(\"chan point not set\")\n\n\tcase len(chanPointShim.PendingChanId) != 32:\n\t\treturn nil, fmt.Errorf(\"pending chan ID not set\")\n\t}\n\n\t// First, we'll map the RPC's channel point to one we can actually use.\n\tindex := chanPointShim.ChanPoint.OutputIndex\n\ttxid, err := lnrpc.GetChanPointFundingTxid(chanPointShim.ChanPoint)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tchanPoint := wire.NewOutPoint(txid, index)\n\n\t// Next we'll parse out the remote party's funding key, as well as our\n\t// full key descriptor.\n\tremoteKey, err := btcec.ParsePubKey(chanPointShim.RemoteKey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tshimKeyDesc := chanPointShim.LocalKey\n\tlocalKey, err := btcec.ParsePubKey(shimKeyDesc.RawKeyBytes)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tlocalKeyDesc := keychain.KeyDescriptor{\n\t\tPubKey: localKey,\n\t\tKeyLocator: keychain.KeyLocator{\n\t\t\tFamily: keychain.KeyFamily(\n\t\t\t\tshimKeyDesc.KeyLoc.KeyFamily,\n\t\t\t),\n\t\t\tIndex: uint32(shimKeyDesc.KeyLoc.KeyIndex),\n\t\t},\n\t}\n\n\t// Verify that if we re-derive this key according to the passed\n\t// KeyLocator, that we get the exact same key back. Otherwise, we may\n\t// end up in a situation where we aren't able to actually sign for this\n\t// newly created channel.\n\tderivedKey, err := keyRing.DeriveKey(localKeyDesc.KeyLocator)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif !derivedKey.PubKey.IsEqual(localKey) {\n\t\treturn nil, fmt.Errorf(\"KeyLocator does not match attached \" +\n\t\t\t\"raw pubkey\")\n\t}\n\n\t// With all the parts assembled, we can now make the canned assembler\n\t// to pass into the wallet.\n\t//\n\t// TODO(roasbeef): update to support musig2\n\treturn chanfunding.NewCannedAssembler(\n\t\tchanPointShim.ThawHeight, *chanPoint,\n\t\tbtcutil.Amount(chanPointShim.Amt), &localKeyDesc,\n\t\tremoteKey, initiator, chanPointShim.Musig2,\n\t), nil\n}\n\n// newPsbtAssembler returns a new fully populated\n// chanfunding.PsbtAssembler using a FundingShim obtained from an RPC caller.\nfunc newPsbtAssembler(req *lnrpc.OpenChannelRequest,\n\tpsbtShim *lnrpc.PsbtShim, netParams *chaincfg.Params) (\n\tchanfunding.Assembler, error) {\n\n\tvar (\n\t\tpacket *psbt.Packet\n\t\terr    error\n\t)\n\n\t// Perform some basic sanity checks to ensure that all the expected\n\t// fields are populated and none of the incompatible fields are.\n\tif len(psbtShim.PendingChanId) != 32 {\n\t\treturn nil, fmt.Errorf(\"pending chan ID not set\")\n\t}\n\tif req.SatPerByte != 0 || req.SatPerVbyte != 0 || req.TargetConf != 0 { // nolint:staticcheck\n\t\treturn nil, fmt.Errorf(\"specifying fee estimation parameters \" +\n\t\t\t\"is not supported for PSBT funding\")\n\t}\n\n\t// The base PSBT is optional. But if it's set, it has to be a valid,\n\t// binary serialized PSBT.\n\tif len(psbtShim.BasePsbt) > 0 {\n\t\tpacket, err = psbt.NewFromRawBytes(\n\t\t\tbytes.NewReader(psbtShim.BasePsbt), false,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"error parsing base PSBT: %w\",\n\t\t\t\terr)\n\t\t}\n\t}\n\n\t// With all the parts assembled, we can now make the canned assembler\n\t// to pass into the wallet.\n\treturn chanfunding.NewPsbtAssembler(\n\t\tbtcutil.Amount(req.LocalFundingAmount), packet, netParams,\n\t\t!psbtShim.NoPublish,\n\t), nil\n}\n\n// canOpenChannel returns an error if the necessary subsystems for channel\n// funding are not ready.\nfunc (r *rpcServer) canOpenChannel() error {\n\t// We can't open a channel until the main server has started.\n\tif !r.server.Started() {\n\t\treturn ErrServerNotActive\n\t}\n\n\t// Creation of channels before the wallet syncs up is currently\n\t// disallowed.\n\tisSynced, _, err := r.server.cc.Wallet.IsSynced()\n\tif err != nil {\n\t\treturn err\n\t}\n\tif !isSynced {\n\t\treturn errors.New(\"channels cannot be created before the \" +\n\t\t\t\"wallet is fully synced\")\n\t}\n\n\treturn nil\n}\n\n// parseOpenChannelReq parses an OpenChannelRequest message into an InitFundingMsg\n// struct. The logic is abstracted so that it can be shared between OpenChannel\n// and OpenChannelSync.\nfunc (r *rpcServer) parseOpenChannelReq(in *lnrpc.OpenChannelRequest,\n\tisSync bool) (*funding.InitFundingMsg, error) {\n\n\trpcsLog.Debugf(\"[openchannel] request to NodeKey(%x) \"+\n\t\t\"allocation(us=%v, them=%v)\", in.NodePubkey,\n\t\tin.LocalFundingAmount, in.PushSat)\n\n\tlocalFundingAmt := btcutil.Amount(in.LocalFundingAmount)\n\tremoteInitialBalance := btcutil.Amount(in.PushSat)\n\n\t// If we are not committing the maximum viable balance towards a channel\n\t// then the local funding amount must be specified. In case FundMax is\n\t// set the funding amount is specified as the interval between minimum\n\t// funding amount and by the configured maximum channel size.\n\tif !in.FundMax && localFundingAmt == 0 {\n\t\treturn nil, fmt.Errorf(\"local funding amount must be non-zero\")\n\t}\n\n\t// Ensure that the initial balance of the remote party (if pushing\n\t// satoshis) does not exceed the amount the local party has requested\n\t// for funding. This is only checked if we are not committing the\n\t// maximum viable amount towards the channel balance. If we do commit\n\t// the maximum then the remote balance is checked in a dedicated FundMax\n\t// check.\n\tif !in.FundMax && remoteInitialBalance >= localFundingAmt {\n\t\treturn nil, fmt.Errorf(\"amount pushed to remote peer for \" +\n\t\t\t\"initial state must be below the local funding amount\")\n\t}\n\n\t// We either allow the fundmax or the psbt flow hence we return an error\n\t// if both are set.\n\tif in.FundingShim != nil && in.FundMax {\n\t\treturn nil, fmt.Errorf(\"cannot provide a psbt funding shim \" +\n\t\t\t\"while committing the maximum wallet balance towards \" +\n\t\t\t\"the channel opening\")\n\t}\n\n\t// If the FundMax flag is set, ensure that the acceptable minimum local\n\t// amount adheres to the amount to be pushed to the remote, and to\n\t// current rules, while also respecting the settings for the maximum\n\t// channel size.\n\tvar minFundAmt, fundUpToMaxAmt btcutil.Amount\n\tif in.FundMax {\n\t\t// We assume the configured maximum channel size to be the upper\n\t\t// bound of our \"maxed\" out funding attempt.\n\t\tfundUpToMaxAmt = btcutil.Amount(r.cfg.MaxChanSize)\n\n\t\t// Since the standard non-fundmax flow requires the minimum\n\t\t// funding amount to be at least in the amount of the initial\n\t\t// remote balance(push amount) we need to adjust the minimum\n\t\t// funding amount accordingly. We initially assume the minimum\n\t\t// allowed channel size as minimum funding amount.\n\t\tminFundAmt = funding.MinChanFundingSize\n\n\t\t// If minFundAmt is less than the initial remote balance we\n\t\t// simply assign the initial remote balance to minFundAmt in\n\t\t// order to fullfil the criterion. Whether or not this so\n\t\t// determined minimum amount is actually available is\n\t\t// ascertained downstream in the lnwallet's reservation\n\t\t// workflow.\n\t\tif remoteInitialBalance >= minFundAmt {\n\t\t\tminFundAmt = remoteInitialBalance\n\t\t}\n\t}\n\n\tminHtlcIn := lnwire.MilliSatoshi(in.MinHtlcMsat)\n\tremoteCsvDelay := uint16(in.RemoteCsvDelay)\n\tmaxValue := lnwire.MilliSatoshi(in.RemoteMaxValueInFlightMsat)\n\tmaxHtlcs := uint16(in.RemoteMaxHtlcs)\n\tremoteChanReserve := btcutil.Amount(in.RemoteChanReserveSat)\n\n\tglobalFeatureSet := r.server.featureMgr.Get(feature.SetNodeAnn)\n\n\t// Determine if the user provided channel fees\n\t// and if so pass them on to the funding workflow.\n\tvar channelBaseFee, channelFeeRate *uint64\n\tif in.UseBaseFee {\n\t\tchannelBaseFee = &in.BaseFee\n\t}\n\tif in.UseFeeRate {\n\t\tchannelFeeRate = &in.FeeRate\n\t}\n\n\t// Ensure that the remote channel reserve does not exceed 20% of the\n\t// channel capacity.\n\tif !in.FundMax && remoteChanReserve >= localFundingAmt/5 {\n\t\treturn nil, fmt.Errorf(\"remote channel reserve must be less \" +\n\t\t\t\"than the %%20 of the channel capacity\")\n\t}\n\n\t// Ensure that the user doesn't exceed the current soft-limit for\n\t// channel size. If the funding amount is above the soft-limit, then\n\t// we'll reject the request.\n\t// If the FundMax flag is set the local amount is determined downstream\n\t// in the wallet hence we do not check it here against the maximum\n\t// funding amount. Only if the localFundingAmt is specified we can check\n\t// if it exceeds the maximum funding amount.\n\twumboEnabled := globalFeatureSet.HasFeature(\n\t\tlnwire.WumboChannelsOptional,\n\t)\n\tif !in.FundMax && !wumboEnabled && localFundingAmt > MaxFundingAmount {\n\t\treturn nil, fmt.Errorf(\"funding amount is too large, the max \"+\n\t\t\t\"channel size is: %v\", MaxFundingAmount)\n\t}\n\n\t// Restrict the size of the channel we'll actually open. At a later\n\t// level, we'll ensure that the output we create, after accounting for\n\t// fees, does not leave a dust output. In case of the FundMax flow\n\t// dedicated checks ensure that the lower boundary of the channel size\n\t// is at least in the amount of MinChanFundingSize or potentially higher\n\t// if a remote balance is specified.\n\tif !in.FundMax && localFundingAmt < funding.MinChanFundingSize {\n\t\treturn nil, fmt.Errorf(\"channel is too small, the minimum \"+\n\t\t\t\"channel size is: %v SAT\", int64(funding.MinChanFundingSize))\n\t}\n\n\t// Prevent users from submitting a max-htlc value that would exceed the\n\t// protocol maximum.\n\tif maxHtlcs > input.MaxHTLCNumber/2 {\n\t\treturn nil, fmt.Errorf(\"remote-max-htlcs (%v) cannot be \"+\n\t\t\t\"greater than %v\", maxHtlcs, input.MaxHTLCNumber/2)\n\t}\n\n\t// Then, we'll extract the minimum number of confirmations that each\n\t// output we use to fund the channel's funding transaction should\n\t// satisfy.\n\tminConfs, err := lnrpc.ExtractMinConfs(in.MinConfs, in.SpendUnconfirmed)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// TODO(roasbeef): also return channel ID?\n\n\tvar nodePubKey *btcec.PublicKey\n\n\t// Parse the remote pubkey the NodePubkey field of the request. If it's\n\t// not present, we'll fallback to the deprecated version that parses the\n\t// key from a hex string if this is for REST for backwards compatibility.\n\tswitch {\n\t// Parse the raw bytes of the node key into a pubkey object so we can\n\t// easily manipulate it.\n\tcase len(in.NodePubkey) > 0:\n\t\tnodePubKey, err = btcec.ParsePubKey(in.NodePubkey)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t// Decode the provided target node's public key, parsing it into a pub\n\t// key object. For all sync call, byte slices are expected to be encoded\n\t// as hex strings.\n\tcase isSync:\n\t\tkeyBytes, err := hex.DecodeString(in.NodePubkeyString) // nolint:staticcheck\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tnodePubKey, err = btcec.ParsePubKey(keyBytes)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"NodePubkey is not set\")\n\t}\n\n\t// Making a channel to ourselves wouldn't be of any use, so we\n\t// explicitly disallow them.\n\tif nodePubKey.IsEqual(r.server.identityECDH.PubKey()) {\n\t\treturn nil, fmt.Errorf(\"cannot open channel to self\")\n\t}\n\n\t// NOTE: We also need to do the fee rate calculation for the psbt\n\t// funding flow because the `batchfund` depends on it.\n\ttargetConf := maybeUseDefaultConf(\n\t\tin.SatPerByte, in.SatPerVbyte, uint32(in.TargetConf),\n\t)\n\n\t// Calculate an appropriate fee rate for this transaction.\n\tfeeRate, err := lnrpc.CalculateFeeRate(\n\t\tuint64(in.SatPerByte), in.SatPerVbyte,\n\t\ttargetConf, r.server.cc.FeeEstimator,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trpcsLog.Debugf(\"[openchannel]: using fee of %v sat/kw for \"+\n\t\t\"funding tx\", int64(feeRate))\n\n\tscript, err := chancloser.ParseUpfrontShutdownAddress(\n\t\tin.CloseAddress, r.cfg.ActiveNetParams.Params,\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error parsing upfront shutdown: %w\",\n\t\t\terr)\n\t}\n\n\tvar channelType *lnwire.ChannelType\n\tswitch in.CommitmentType {\n\tcase lnrpc.CommitmentType_UNKNOWN_COMMITMENT_TYPE:\n\t\tif in.ZeroConf {\n\t\t\treturn nil, fmt.Errorf(\"use anchors for zero-conf\")\n\t\t}\n\n\tcase lnrpc.CommitmentType_LEGACY:\n\t\tchannelType = new(lnwire.ChannelType)\n\t\t*channelType = lnwire.ChannelType(*lnwire.NewRawFeatureVector())\n\n\tcase lnrpc.CommitmentType_STATIC_REMOTE_KEY:\n\t\tchannelType = new(lnwire.ChannelType)\n\t\t*channelType = lnwire.ChannelType(*lnwire.NewRawFeatureVector(\n\t\t\tlnwire.StaticRemoteKeyRequired,\n\t\t))\n\n\tcase lnrpc.CommitmentType_ANCHORS:\n\t\tchannelType = new(lnwire.ChannelType)\n\t\tfv := lnwire.NewRawFeatureVector(\n\t\t\tlnwire.StaticRemoteKeyRequired,\n\t\t\tlnwire.AnchorsZeroFeeHtlcTxRequired,\n\t\t)\n\n\t\tif in.ZeroConf {\n\t\t\tfv.Set(lnwire.ZeroConfRequired)\n\t\t}\n\n\t\tif in.ScidAlias {\n\t\t\tfv.Set(lnwire.ScidAliasRequired)\n\t\t}\n\n\t\t*channelType = lnwire.ChannelType(*fv)\n\n\tcase lnrpc.CommitmentType_SCRIPT_ENFORCED_LEASE:\n\t\tchannelType = new(lnwire.ChannelType)\n\t\tfv := lnwire.NewRawFeatureVector(\n\t\t\tlnwire.StaticRemoteKeyRequired,\n\t\t\tlnwire.AnchorsZeroFeeHtlcTxRequired,\n\t\t\tlnwire.ScriptEnforcedLeaseRequired,\n\t\t)\n\n\t\tif in.ZeroConf {\n\t\t\tfv.Set(lnwire.ZeroConfRequired)\n\t\t}\n\n\t\tif in.ScidAlias {\n\t\t\tfv.Set(lnwire.ScidAliasRequired)\n\t\t}\n\n\t\t*channelType = lnwire.ChannelType(*fv)\n\n\tcase lnrpc.CommitmentType_SIMPLE_TAPROOT:\n\t\t// If the taproot channel type is being set, then the channel\n\t\t// MUST be private (unadvertised) for now.\n\t\tif !in.Private {\n\t\t\treturn nil, fmt.Errorf(\"taproot channels must be \" +\n\t\t\t\t\"private\")\n\t\t}\n\n\t\tchannelType = new(lnwire.ChannelType)\n\t\tfv := lnwire.NewRawFeatureVector(\n\t\t\tlnwire.SimpleTaprootChannelsRequiredStaging,\n\t\t)\n\n\t\t// TODO(roasbeef): no need for the rest as they're now\n\t\t// implicit?\n\n\t\tif in.ZeroConf {\n\t\t\tfv.Set(lnwire.ZeroConfRequired)\n\t\t}\n\n\t\tif in.ScidAlias {\n\t\t\tfv.Set(lnwire.ScidAliasRequired)\n\t\t}\n\n\t\t*channelType = lnwire.ChannelType(*fv)\n\n\tcase lnrpc.CommitmentType_SIMPLE_TAPROOT_OVERLAY:\n\t\t// If the taproot overlay channel type is being set, then the\n\t\t// channel MUST be private.\n\t\tif !in.Private {\n\t\t\treturn nil, fmt.Errorf(\"taproot overlay channels \" +\n\t\t\t\t\"must be private\")\n\t\t}\n\n\t\tchannelType = new(lnwire.ChannelType)\n\t\tfv := lnwire.NewRawFeatureVector(\n\t\t\tlnwire.SimpleTaprootOverlayChansRequired,\n\t\t)\n\n\t\tif in.ZeroConf {\n\t\t\tfv.Set(lnwire.ZeroConfRequired)\n\t\t}\n\n\t\tif in.ScidAlias {\n\t\t\tfv.Set(lnwire.ScidAliasRequired)\n\t\t}\n\n\t\t*channelType = lnwire.ChannelType(*fv)\n\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unhandled request channel type %v\",\n\t\t\tin.CommitmentType)\n\t}\n\n\t// We limit the channel memo to be 500 characters long. This enforces\n\t// a reasonable upper bound on storage consumption. This also mimics\n\t// the length limit for the label of a TX.\n\tconst maxMemoLength = 500\n\tif len(in.Memo) > maxMemoLength {\n\t\treturn nil, fmt.Errorf(\"provided memo (%s) is of length %d, \"+\n\t\t\t\"exceeds %d\", in.Memo, len(in.Memo), maxMemoLength)\n\t}\n\n\t// Check, if manually selected outpoints are present to fund a channel.\n\tvar outpoints []wire.OutPoint\n\tif len(in.Outpoints) > 0 {\n\t\toutpoints, err = toWireOutpoints(in.Outpoints)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"can't create outpoints %w\", err)\n\t\t}\n\t}\n\n\t// Instruct the server to trigger the necessary events to attempt to\n\t// open a new channel. A stream is returned in place, this stream will\n\t// be used to consume updates of the state of the pending channel.\n\treturn &funding.InitFundingMsg{\n\t\tTargetPubkey:    nodePubKey,\n\t\tChainHash:       *r.cfg.ActiveNetParams.GenesisHash,\n\t\tLocalFundingAmt: localFundingAmt,\n\t\tBaseFee:         channelBaseFee,\n\t\tFeeRate:         channelFeeRate,\n\t\tPushAmt: lnwire.NewMSatFromSatoshis(\n\t\t\tremoteInitialBalance,\n\t\t),\n\t\tMinHtlcIn:         minHtlcIn,\n\t\tFundingFeePerKw:   feeRate,\n\t\tPrivate:           in.Private,\n\t\tRemoteCsvDelay:    remoteCsvDelay,\n\t\tRemoteChanReserve: remoteChanReserve,\n\t\tMinConfs:          minConfs,\n\t\tShutdownScript:    script,\n\t\tMaxValueInFlight:  maxValue,\n\t\tMaxHtlcs:          maxHtlcs,\n\t\tMaxLocalCsv:       uint16(in.MaxLocalCsv),\n\t\tChannelType:       channelType,\n\t\tFundUpToMaxAmt:    fundUpToMaxAmt,\n\t\tMinFundAmt:        minFundAmt,\n\t\tMemo:              []byte(in.Memo),\n\t\tOutpoints:         outpoints,\n\t}, nil\n}\n\n// toWireOutpoints converts a list of outpoints from the rpc format to the wire\n// format.\nfunc toWireOutpoints(outpoints []*lnrpc.OutPoint) ([]wire.OutPoint, error) {\n\tvar wireOutpoints []wire.OutPoint\n\tfor _, outpoint := range outpoints {\n\t\thash, err := chainhash.NewHashFromStr(outpoint.TxidStr)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"cannot create chainhash\")\n\t\t}\n\n\t\twireOutpoint := wire.NewOutPoint(\n\t\t\thash, outpoint.OutputIndex,\n\t\t)\n\t\twireOutpoints = append(wireOutpoints, *wireOutpoint)\n\t}\n\n\treturn wireOutpoints, nil\n}\n\n// OpenChannel attempts to open a singly funded channel specified in the\n// request to a remote peer.\nfunc (r *rpcServer) OpenChannel(in *lnrpc.OpenChannelRequest,\n\tupdateStream lnrpc.Lightning_OpenChannelServer) error {\n\n\tif err := r.canOpenChannel(); err != nil {\n\t\treturn err\n\t}\n\n\treq, err := r.parseOpenChannelReq(in, false)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// If the user has provided a shim, then we'll now augment the based\n\t// open channel request with this additional logic.\n\tif in.FundingShim != nil {\n\t\tswitch {\n\t\t// If we have a chan point shim, then this means the funding\n\t\t// transaction was crafted externally. In this case we only\n\t\t// need to hand a channel point down into the wallet.\n\t\tcase in.FundingShim.GetChanPointShim() != nil:\n\t\t\tchanPointShim := in.FundingShim.GetChanPointShim()\n\n\t\t\t// Map the channel point shim into a new\n\t\t\t// chanfunding.CannedAssembler that the wallet will use\n\t\t\t// to obtain the channel point details.\n\t\t\tcopy(req.PendingChanID[:], chanPointShim.PendingChanId)\n\t\t\treq.ChanFunder, err = newFundingShimAssembler(\n\t\t\t\tchanPointShim, true, r.server.cc.KeyRing,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t// If we have a PSBT shim, then this means the funding\n\t\t// transaction will be crafted outside of the wallet, once the\n\t\t// funding multisig output script is known. We'll create an\n\t\t// intent that will supervise the multi-step process.\n\t\tcase in.FundingShim.GetPsbtShim() != nil:\n\t\t\tpsbtShim := in.FundingShim.GetPsbtShim()\n\n\t\t\t// Instruct the wallet to use the new\n\t\t\t// chanfunding.PsbtAssembler to construct the funding\n\t\t\t// transaction.\n\t\t\tcopy(req.PendingChanID[:], psbtShim.PendingChanId)\n\n\t\t\t// NOTE: For the PSBT case we do also allow unconfirmed\n\t\t\t// utxos to fund the psbt transaction because we make\n\t\t\t// sure we only use stable utxos.\n\t\t\treq.ChanFunder, err = newPsbtAssembler(\n\t\t\t\tin, psbtShim,\n\t\t\t\t&r.server.cc.Wallet.Cfg.NetParams,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\tupdateChan, errChan := r.server.OpenChannel(req)\n\n\tvar outpoint wire.OutPoint\nout:\n\tfor {\n\t\tselect {\n\t\tcase err := <-errChan:\n\t\t\trpcsLog.Errorf(\"unable to open channel to NodeKey(%x): %v\",\n\t\t\t\treq.TargetPubkey.SerializeCompressed(), err)\n\t\t\treturn err\n\t\tcase fundingUpdate := <-updateChan:\n\t\t\trpcsLog.Tracef(\"[openchannel] sending update: %v\",\n\t\t\t\tfundingUpdate)\n\t\t\tif err := updateStream.Send(fundingUpdate); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t// If a final channel open update is being sent, then\n\t\t\t// we can break out of our recv loop as we no longer\n\t\t\t// need to process any further updates.\n\t\t\tupdate, ok := fundingUpdate.Update.(*lnrpc.OpenStatusUpdate_ChanOpen)\n\t\t\tif ok {\n\t\t\t\tchanPoint := update.ChanOpen.ChannelPoint\n\t\t\t\ttxid, err := lnrpc.GetChanPointFundingTxid(chanPoint)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\toutpoint = wire.OutPoint{\n\t\t\t\t\tHash:  *txid,\n\t\t\t\t\tIndex: chanPoint.OutputIndex,\n\t\t\t\t}\n\n\t\t\t\tbreak out\n\t\t\t}\n\t\tcase <-r.quit:\n\t\t\treturn nil\n\t\t}\n\t}\n\n\trpcsLog.Tracef(\"[openchannel] success NodeKey(%x), ChannelPoint(%v)\",\n\t\treq.TargetPubkey.SerializeCompressed(), outpoint)\n\treturn nil\n}\n\n// OpenChannelSync is a synchronous version of the OpenChannel RPC call. This\n// call is meant to be consumed by clients to the REST proxy. As with all other\n// sync calls, all byte slices are instead to be populated as hex encoded\n// strings.\nfunc (r *rpcServer) OpenChannelSync(ctx context.Context,\n\tin *lnrpc.OpenChannelRequest) (*lnrpc.ChannelPoint, error) {\n\n\tif err := r.canOpenChannel(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treq, err := r.parseOpenChannelReq(in, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tupdateChan, errChan := r.server.OpenChannel(req)\n\tselect {\n\t// If an error occurs them immediately return the error to the client.\n\tcase err := <-errChan:\n\t\trpcsLog.Errorf(\"unable to open channel to NodeKey(%x): %v\",\n\t\t\treq.TargetPubkey.SerializeCompressed(), err)\n\t\treturn nil, err\n\n\t// Otherwise, wait for the first channel update. The first update sent\n\t// is when the funding transaction is broadcast to the network.\n\tcase fundingUpdate := <-updateChan:\n\t\trpcsLog.Tracef(\"[openchannel] sending update: %v\",\n\t\t\tfundingUpdate)\n\n\t\t// Parse out the txid of the pending funding transaction. The\n\t\t// sync client can use this to poll against the list of\n\t\t// PendingChannels.\n\t\topenUpdate := fundingUpdate.Update.(*lnrpc.OpenStatusUpdate_ChanPending)\n\t\tchanUpdate := openUpdate.ChanPending\n\n\t\treturn &lnrpc.ChannelPoint{\n\t\t\tFundingTxid: &lnrpc.ChannelPoint_FundingTxidBytes{\n\t\t\t\tFundingTxidBytes: chanUpdate.Txid,\n\t\t\t},\n\t\t\tOutputIndex: chanUpdate.OutputIndex,\n\t\t}, nil\n\tcase <-r.quit:\n\t\treturn nil, nil\n\t}\n}\n\n// BatchOpenChannel attempts to open multiple single-funded channels in a\n// single transaction in an atomic way. This means either all channel open\n// requests succeed at once or all attempts are aborted if any of them fail.\n// This is the safer variant of using PSBTs to manually fund a batch of\n// channels through the OpenChannel RPC.\nfunc (r *rpcServer) BatchOpenChannel(ctx context.Context,\n\tin *lnrpc.BatchOpenChannelRequest) (*lnrpc.BatchOpenChannelResponse,\n\terror) {\n\n\tif err := r.canOpenChannel(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// We need the wallet kit server to do the heavy lifting on the PSBT\n\t// part. If we didn't rely on re-using the wallet kit server's logic we\n\t// would need to re-implement everything here. Since we deliver lnd with\n\t// the wallet kit server enabled by default we can assume it's okay to\n\t// make this functionality dependent on that server being active.\n\tvar walletKitServer walletrpc.WalletKitServer\n\tfor _, subServer := range r.subServers {\n\t\tif subServer.Name() == walletrpc.SubServerName {\n\t\t\twalletKitServer = subServer.(walletrpc.WalletKitServer)\n\t\t}\n\t}\n\tif walletKitServer == nil {\n\t\treturn nil, fmt.Errorf(\"batch channel open is only possible \" +\n\t\t\t\"if walletrpc subserver is active\")\n\t}\n\n\trpcsLog.Debugf(\"[batchopenchannel] request to open batch of %d \"+\n\t\t\"channels\", len(in.Channels))\n\n\t// Make sure there is at least one channel to open. We could say we want\n\t// at least two channels for a batch. But maybe it's nice if developers\n\t// can use the same API for a single channel as well as a batch of\n\t// channels.\n\tif len(in.Channels) == 0 {\n\t\treturn nil, fmt.Errorf(\"specify at least one channel\")\n\t}\n\n\t// In case we remove a pending channel from the database, we need to set\n\t// a close height, so we'll just use the current best known height.\n\t_, bestHeight, err := r.server.cc.ChainIO.GetBestBlock()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error fetching best block: %w\", err)\n\t}\n\n\t// So far everything looks good and we can now start the heavy lifting\n\t// that's done in the funding package.\n\trequestParser := func(req *lnrpc.OpenChannelRequest) (\n\t\t*funding.InitFundingMsg, error) {\n\n\t\treturn r.parseOpenChannelReq(req, false)\n\t}\n\tchannelAbandoner := func(point *wire.OutPoint) error {\n\t\treturn r.abandonChan(point, uint32(bestHeight))\n\t}\n\tbatcher := funding.NewBatcher(&funding.BatchConfig{\n\t\tRequestParser:    requestParser,\n\t\tChannelAbandoner: channelAbandoner,\n\t\tChannelOpener:    r.server.OpenChannel,\n\t\tWalletKitServer:  walletKitServer,\n\t\tWallet:           r.server.cc.Wallet,\n\t\tNetParams:        &r.server.cc.Wallet.Cfg.NetParams,\n\t\tQuit:             r.quit,\n\t})\n\trpcPoints, err := batcher.BatchFund(ctx, in)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"batch funding failed: %w\", err)\n\t}\n\n\t// Now all that's left to do is send back the response with the channel\n\t// points we created.\n\treturn &lnrpc.BatchOpenChannelResponse{\n\t\tPendingChannels: rpcPoints,\n\t}, nil\n}\n\n// CloseChannel attempts to close an active channel identified by its channel\n// point. The actions of this method can additionally be augmented to attempt\n// a force close after a timeout period in the case of an inactive peer.\nfunc (r *rpcServer) CloseChannel(in *lnrpc.CloseChannelRequest,\n\tupdateStream lnrpc.Lightning_CloseChannelServer) error {\n\n\tif !r.server.Started() {\n\t\treturn ErrServerNotActive\n\t}\n\n\t// If the user didn't specify a channel point, then we'll reject this\n\t// request all together.\n\tif in.GetChannelPoint() == nil {\n\t\treturn fmt.Errorf(\"must specify channel point in close channel\")\n\t}\n\n\t// If force closing a channel, the fee set in the commitment transaction\n\t// is used.\n\tif in.Force && (in.SatPerByte != 0 || in.SatPerVbyte != 0 || // nolint:staticcheck\n\t\tin.TargetConf != 0) {\n\n\t\treturn fmt.Errorf(\"force closing a channel uses a pre-defined fee\")\n\t}\n\n\tforce := in.Force\n\tindex := in.ChannelPoint.OutputIndex\n\ttxid, err := lnrpc.GetChanPointFundingTxid(in.GetChannelPoint())\n\tif err != nil {\n\t\trpcsLog.Errorf(\"[closechannel] unable to get funding txid: %v\", err)\n\t\treturn err\n\t}\n\tchanPoint := wire.NewOutPoint(txid, index)\n\n\trpcsLog.Tracef(\"[closechannel] request for ChannelPoint(%v), force=%v\",\n\t\tchanPoint, force)\n\n\tvar (\n\t\tupdateChan chan interface{}\n\t\terrChan    chan error\n\t)\n\n\t// TODO(roasbeef): if force and peer online then don't force?\n\n\t// First, we'll fetch the channel as is, as we'll need to examine it\n\t// regardless of if this is a force close or not.\n\tchannel, err := r.server.chanStateDB.FetchChannel(*chanPoint)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// We can't coop or force close restored channels or channels that have\n\t// experienced local data loss. Normally we would detect this in the\n\t// channel arbitrator if the channel has the status\n\t// ChanStatusLocalDataLoss after connecting to its peer. But if no\n\t// connection can be established, the channel arbitrator doesn't know it\n\t// can't be force closed yet.\n\tif channel.HasChanStatus(channeldb.ChanStatusRestored) ||\n\t\tchannel.HasChanStatus(channeldb.ChanStatusLocalDataLoss) {\n\n\t\treturn fmt.Errorf(\"cannot close channel with state: %v\",\n\t\t\tchannel.ChanStatus())\n\t}\n\n\t// Retrieve the best height of the chain, which we'll use to complete\n\t// either closing flow.\n\t_, bestHeight, err := r.server.cc.ChainIO.GetBestBlock()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// If a force closure was requested, then we'll handle all the details\n\t// around the creation and broadcast of the unilateral closure\n\t// transaction here rather than going to the switch as we don't require\n\t// interaction from the peer.\n\tif force {\n\t\t// As we're force closing this channel, as a precaution, we'll\n\t\t// ensure that the switch doesn't continue to see this channel\n\t\t// as eligible for forwarding HTLC's. If the peer is online,\n\t\t// then we'll also purge all of its indexes.\n\t\tremotePub := channel.IdentityPub\n\t\tif peer, err := r.server.FindPeer(remotePub); err == nil {\n\t\t\t// TODO(roasbeef): actually get the active channel\n\t\t\t// instead too?\n\t\t\t//  * so only need to grab from database\n\t\t\tpeer.WipeChannel(&channel.FundingOutpoint)\n\t\t} else {\n\t\t\tchanID := lnwire.NewChanIDFromOutPoint(\n\t\t\t\tchannel.FundingOutpoint,\n\t\t\t)\n\t\t\tr.server.htlcSwitch.RemoveLink(chanID)\n\t\t}\n\n\t\t// With the necessary indexes cleaned up, we'll now force close\n\t\t// the channel.\n\t\tchainArbitrator := r.server.chainArb\n\t\tclosingTx, err := chainArbitrator.ForceCloseContract(\n\t\t\t*chanPoint,\n\t\t)\n\t\tif err != nil {\n\t\t\trpcsLog.Errorf(\"unable to force close transaction: %v\", err)\n\t\t\treturn err\n\t\t}\n\n\t\t// Safety check which should never happen.\n\t\t//\n\t\t// TODO(ziggie): remove pointer as return value from\n\t\t// ForceCloseContract.\n\t\tif closingTx == nil {\n\t\t\treturn fmt.Errorf(\"force close transaction is nil\")\n\t\t}\n\n\t\tclosingTxid := closingTx.TxHash()\n\n\t\t// With the transaction broadcast, we send our first update to\n\t\t// the client.\n\t\tupdateChan = make(chan interface{}, 2)\n\t\tupdateChan <- &peer.PendingUpdate{\n\t\t\tTxid: closingTxid[:],\n\t\t}\n\n\t\terrChan = make(chan error, 1)\n\t\tnotifier := r.server.cc.ChainNotifier\n\t\tgo peer.WaitForChanToClose(\n\t\t\tuint32(bestHeight), notifier, errChan, chanPoint,\n\t\t\t&closingTxid, closingTx.TxOut[0].PkScript, func() {\n\t\t\t\t// Respond to the local subsystem which\n\t\t\t\t// requested the channel closure.\n\t\t\t\tupdateChan <- &peer.ChannelCloseUpdate{\n\t\t\t\t\tClosingTxid: closingTxid[:],\n\t\t\t\t\tSuccess:     true,\n\t\t\t\t\t// Force closure transactions don't have\n\t\t\t\t\t// additional local/remote outputs.\n\t\t\t\t}\n\t\t\t},\n\t\t)\n\t} else {\n\t\t// If this is a frozen channel, then we only allow the co-op\n\t\t// close to proceed if we were the responder to this channel if\n\t\t// the absolute thaw height has not been met.\n\t\tif channel.IsInitiator {\n\t\t\tabsoluteThawHeight, err := channel.AbsoluteThawHeight()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif uint32(bestHeight) < absoluteThawHeight {\n\t\t\t\treturn fmt.Errorf(\"cannot co-op close frozen \"+\n\t\t\t\t\t\"channel as initiator until height=%v, \"+\n\t\t\t\t\t\"(current_height=%v)\",\n\t\t\t\t\tabsoluteThawHeight, bestHeight)\n\t\t\t}\n\t\t}\n\n\t\t// If the link is not known by the switch, we cannot gracefully close\n\t\t// the channel.\n\t\tchannelID := lnwire.NewChanIDFromOutPoint(*chanPoint)\n\t\tif _, err := r.server.htlcSwitch.GetLink(channelID); err != nil {\n\t\t\trpcsLog.Debugf(\"Trying to non-force close offline channel with \"+\n\t\t\t\t\"chan_point=%v\", chanPoint)\n\t\t\treturn fmt.Errorf(\"unable to gracefully close channel while peer \"+\n\t\t\t\t\"is offline (try force closing it instead): %v\", err)\n\t\t}\n\n\t\t// Keep the old behavior prior to 0.18.0 - when the user\n\t\t// doesn't set fee rate or conf target, the default conf target\n\t\t// of 6 is used.\n\t\ttargetConf := maybeUseDefaultConf(\n\t\t\tin.SatPerByte, in.SatPerVbyte, uint32(in.TargetConf),\n\t\t)\n\n\t\t// Based on the passed fee related parameters, we'll determine\n\t\t// an appropriate fee rate for the cooperative closure\n\t\t// transaction.\n\t\tfeeRate, err := lnrpc.CalculateFeeRate(\n\t\t\tuint64(in.SatPerByte), in.SatPerVbyte, // nolint:staticcheck\n\t\t\ttargetConf, r.server.cc.FeeEstimator,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\trpcsLog.Debugf(\"Target sat/kw for closing transaction: %v\",\n\t\t\tint64(feeRate))\n\n\t\t// If the user hasn't specified NoWait, then before we attempt\n\t\t// to close the channel we ensure there are no active HTLCs on\n\t\t// the link.\n\t\tif !in.NoWait && len(channel.ActiveHtlcs()) != 0 {\n\t\t\treturn fmt.Errorf(\"cannot co-op close channel \" +\n\t\t\t\t\"with active htlcs\")\n\t\t}\n\n\t\t// Otherwise, the caller has requested a regular interactive\n\t\t// cooperative channel closure. So we'll forward the request to\n\t\t// the htlc switch which will handle the negotiation and\n\t\t// broadcast details.\n\n\t\tvar deliveryScript lnwire.DeliveryAddress\n\n\t\t// If a delivery address to close out to was specified, decode it.\n\t\tif len(in.DeliveryAddress) > 0 {\n\t\t\t// Decode the address provided.\n\t\t\taddr, err := btcutil.DecodeAddress(\n\t\t\t\tin.DeliveryAddress, r.cfg.ActiveNetParams.Params,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"invalid delivery address: \"+\n\t\t\t\t\t\"%v\", err)\n\t\t\t}\n\n\t\t\tif !addr.IsForNet(r.cfg.ActiveNetParams.Params) {\n\t\t\t\treturn fmt.Errorf(\"delivery address is not \"+\n\t\t\t\t\t\"for %s\",\n\t\t\t\t\tr.cfg.ActiveNetParams.Params.Name)\n\t\t\t}\n\n\t\t\t// Create a script to pay out to the address provided.\n\t\t\tdeliveryScript, err = txscript.PayToAddrScript(addr)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\tmaxFee := chainfee.SatPerKVByte(\n\t\t\tin.MaxFeePerVbyte * 1000,\n\t\t).FeePerKWeight()\n\t\tupdateChan, errChan = r.server.htlcSwitch.CloseLink(\n\t\t\tchanPoint, contractcourt.CloseRegular, feeRate,\n\t\t\tmaxFee, deliveryScript,\n\t\t)\n\t}\n\n\t// If the user doesn't want to wait for the txid to come back then we\n\t// will send an empty update to kick off the stream.\n\tif in.NoWait {\n\t\trpcsLog.Trace(\"[closechannel] sending instant update\")\n\t\tif err := updateStream.Send(\n\t\t\t&lnrpc.CloseStatusUpdate{\n\t\t\t\tUpdate: &lnrpc.CloseStatusUpdate_CloseInstant{},\n\t\t\t},\n\t\t); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\nout:\n\tfor {\n\t\tselect {\n\t\tcase err := <-errChan:\n\t\t\trpcsLog.Errorf(\"[closechannel] unable to close \"+\n\t\t\t\t\"ChannelPoint(%v): %v\", chanPoint, err)\n\t\t\treturn err\n\t\tcase closingUpdate := <-updateChan:\n\t\t\trpcClosingUpdate, err := createRPCCloseUpdate(\n\t\t\t\tclosingUpdate,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\terr = fn.MapOptionZ(\n\t\t\t\tr.server.implCfg.AuxDataParser,\n\t\t\t\tfunc(parser AuxDataParser) error {\n\t\t\t\t\treturn parser.InlineParseCustomData(\n\t\t\t\t\t\trpcClosingUpdate,\n\t\t\t\t\t)\n\t\t\t\t},\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"error parsing custom data: \"+\n\t\t\t\t\t\"%w\", err)\n\t\t\t}\n\n\t\t\trpcsLog.Tracef(\"[closechannel] sending update: %v\",\n\t\t\t\trpcClosingUpdate)\n\n\t\t\tif err := updateStream.Send(rpcClosingUpdate); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t// If a final channel closing updates is being sent,\n\t\t\t// then we can break out of our dispatch loop as we no\n\t\t\t// longer need to process any further updates.\n\t\t\tswitch closeUpdate := closingUpdate.(type) {\n\t\t\tcase *peer.ChannelCloseUpdate:\n\t\t\t\th, _ := chainhash.NewHash(closeUpdate.ClosingTxid)\n\t\t\t\trpcsLog.Infof(\"[closechannel] close completed: \"+\n\t\t\t\t\t\"txid(%v)\", h)\n\t\t\t\tbreak out\n\t\t\t}\n\t\tcase <-r.quit:\n\t\t\treturn nil\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc createRPCCloseUpdate(\n\tupdate interface{}) (*lnrpc.CloseStatusUpdate, error) {\n\n\tswitch u := update.(type) {\n\tcase *peer.ChannelCloseUpdate:\n\t\tccu := &lnrpc.ChannelCloseUpdate{\n\t\t\tClosingTxid: u.ClosingTxid,\n\t\t\tSuccess:     u.Success,\n\t\t}\n\n\t\terr := fn.MapOptionZ(\n\t\t\tu.LocalCloseOutput,\n\t\t\tfunc(closeOut chancloser.CloseOutput) error {\n\t\t\t\tcr, err := closeOut.ShutdownRecords.Serialize()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn fmt.Errorf(\"error serializing \"+\n\t\t\t\t\t\t\"local close out custom \"+\n\t\t\t\t\t\t\"records: %w\", err)\n\t\t\t\t}\n\n\t\t\t\trpcCloseOut := &lnrpc.CloseOutput{\n\t\t\t\t\tAmountSat:         int64(closeOut.Amt),\n\t\t\t\t\tPkScript:          closeOut.PkScript,\n\t\t\t\t\tIsLocal:           true,\n\t\t\t\t\tCustomChannelData: cr,\n\t\t\t\t}\n\t\t\t\tccu.LocalCloseOutput = rpcCloseOut\n\n\t\t\t\treturn nil\n\t\t\t},\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\terr = fn.MapOptionZ(\n\t\t\tu.RemoteCloseOutput,\n\t\t\tfunc(closeOut chancloser.CloseOutput) error {\n\t\t\t\tcr, err := closeOut.ShutdownRecords.Serialize()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn fmt.Errorf(\"error serializing \"+\n\t\t\t\t\t\t\"remote close out custom \"+\n\t\t\t\t\t\t\"records: %w\", err)\n\t\t\t\t}\n\n\t\t\t\trpcCloseOut := &lnrpc.CloseOutput{\n\t\t\t\t\tAmountSat:         int64(closeOut.Amt),\n\t\t\t\t\tPkScript:          closeOut.PkScript,\n\t\t\t\t\tCustomChannelData: cr,\n\t\t\t\t}\n\t\t\t\tccu.RemoteCloseOutput = rpcCloseOut\n\n\t\t\t\treturn nil\n\t\t\t},\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tu.AuxOutputs.WhenSome(func(outs chancloser.AuxCloseOutputs) {\n\t\t\tfor _, out := range outs.ExtraCloseOutputs {\n\t\t\t\tccu.AdditionalOutputs = append(\n\t\t\t\t\tccu.AdditionalOutputs,\n\t\t\t\t\t&lnrpc.CloseOutput{\n\t\t\t\t\t\tAmountSat: out.Value,\n\t\t\t\t\t\tPkScript:  out.PkScript,\n\t\t\t\t\t\tIsLocal:   out.IsLocal,\n\t\t\t\t\t},\n\t\t\t\t)\n\t\t\t}\n\t\t})\n\n\t\treturn &lnrpc.CloseStatusUpdate{\n\t\t\tUpdate: &lnrpc.CloseStatusUpdate_ChanClose{\n\t\t\t\tChanClose: ccu,\n\t\t\t},\n\t\t}, nil\n\n\tcase *peer.PendingUpdate:\n\t\treturn &lnrpc.CloseStatusUpdate{\n\t\t\tUpdate: &lnrpc.CloseStatusUpdate_ClosePending{\n\t\t\t\tClosePending: &lnrpc.PendingUpdate{\n\t\t\t\t\tTxid:        u.Txid,\n\t\t\t\t\tOutputIndex: u.OutputIndex,\n\t\t\t\t},\n\t\t\t},\n\t\t}, nil\n\t}\n\n\treturn nil, errors.New(\"unknown close status update\")\n}\n\n// abandonChanFromGraph attempts to remove a channel from the channel graph. If\n// we can't find the chanID in the graph, then we assume it has already been\n// removed, and will return a nop.\nfunc abandonChanFromGraph(chanGraph *graphdb.ChannelGraph,\n\tchanPoint *wire.OutPoint) error {\n\n\t// First, we'll obtain the channel ID. If we can't locate this, then\n\t// it's the case that the channel may have already been removed from\n\t// the graph, so we'll return a nil error.\n\tchanID, err := chanGraph.ChannelID(chanPoint)\n\tswitch {\n\tcase errors.Is(err, graphdb.ErrEdgeNotFound):\n\t\treturn nil\n\tcase err != nil:\n\t\treturn err\n\t}\n\n\t// If the channel ID is still in the graph, then that means the channel\n\t// is still open, so we'll now move to purge it from the graph.\n\treturn chanGraph.DeleteChannelEdges(false, true, chanID)\n}\n\n// abandonChan removes a channel from the database, graph and contract court.\nfunc (r *rpcServer) abandonChan(chanPoint *wire.OutPoint,\n\tbestHeight uint32) error {\n\n\t// Before we remove the channel we cancel the rebroadcasting of the\n\t// transaction. If this transaction does not exist in the rebroadcast\n\t// queue anymore it is a noop.\n\ttxid, err := chainhash.NewHash(chanPoint.Hash[:])\n\tif err != nil {\n\t\treturn err\n\t}\n\tr.server.cc.Wallet.CancelRebroadcast(*txid)\n\n\t// Abandoning a channel is a three-step process: remove from the open\n\t// channel state, remove from the graph, remove from the contract\n\t// court. Between any step it's possible that the users restarts the\n\t// process all over again. As a result, each of the steps below are\n\t// intended to be idempotent.\n\terr = r.server.chanStateDB.AbandonChannel(chanPoint, bestHeight)\n\tif err != nil {\n\t\treturn err\n\t}\n\terr = abandonChanFromGraph(r.server.graphDB, chanPoint)\n\tif err != nil {\n\t\treturn err\n\t}\n\terr = r.server.chainArb.ResolveContract(*chanPoint)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// If this channel was in the process of being closed, but didn't fully\n\t// close, then it's possible that the nursery is hanging on to some\n\t// state. To err on the side of caution, we'll now attempt to wipe any\n\t// state for this channel from the nursery.\n\terr = r.server.utxoNursery.RemoveChannel(chanPoint)\n\tif err != nil && err != contractcourt.ErrContractNotFound {\n\t\treturn err\n\t}\n\n\t// Finally, notify the backup listeners that the channel can be removed\n\t// from any channel backups.\n\tr.server.channelNotifier.NotifyClosedChannelEvent(*chanPoint)\n\n\treturn nil\n}\n\n// AbandonChannel removes all channel state from the database except for a\n// close summary. This method can be used to get rid of permanently unusable\n// channels due to bugs fixed in newer versions of lnd.\nfunc (r *rpcServer) AbandonChannel(_ context.Context,\n\tin *lnrpc.AbandonChannelRequest) (*lnrpc.AbandonChannelResponse, error) {\n\n\t// If this isn't the dev build, then we won't allow the RPC to be\n\t// executed, as it's an advanced feature and won't be activated in\n\t// regular production/release builds except for the explicit case of\n\t// externally funded channels that are still pending. Due to repeated\n\t// requests, we also allow this requirement to be overwritten by a new\n\t// flag that attests to the user knowing what they're doing and the risk\n\t// associated with the command/RPC.\n\tif !in.IKnowWhatIAmDoing && !in.PendingFundingShimOnly &&\n\t\t!build.IsDevBuild() {\n\n\t\treturn nil, fmt.Errorf(\"AbandonChannel RPC call only \" +\n\t\t\t\"available in dev builds\")\n\t}\n\n\t// We'll parse out the arguments to we can obtain the chanPoint of the\n\t// target channel.\n\ttxid, err := lnrpc.GetChanPointFundingTxid(in.GetChannelPoint())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tindex := in.ChannelPoint.OutputIndex\n\tchanPoint := wire.NewOutPoint(txid, index)\n\n\t// When we remove the channel from the database, we need to set a close\n\t// height, so we'll just use the current best known height.\n\t_, bestHeight, err := r.server.cc.ChainIO.GetBestBlock()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tdbChan, err := r.server.chanStateDB.FetchChannel(*chanPoint)\n\tswitch {\n\t// If the channel isn't found in the set of open channels, then we can\n\t// continue on as it can't be loaded into the link/peer.\n\tcase err == channeldb.ErrChannelNotFound:\n\t\tbreak\n\n\t// If the channel is still known to be open, then before we modify any\n\t// on-disk state, we'll remove the channel from the switch and peer\n\t// state if it's been loaded in.\n\tcase err == nil:\n\t\t// If the user requested the more safe version that only allows\n\t\t// the removal of externally (shim) funded channels that are\n\t\t// still pending, we enforce this option now that we know the\n\t\t// state of the channel.\n\t\t//\n\t\t// TODO(guggero): Properly store the funding type (wallet, shim,\n\t\t// PSBT) on the channel so we don't need to use the thaw height.\n\t\tisShimFunded := dbChan.ThawHeight > 0\n\t\tisPendingShimFunded := isShimFunded && dbChan.IsPending\n\t\tif !in.IKnowWhatIAmDoing && in.PendingFundingShimOnly &&\n\t\t\t!isPendingShimFunded {\n\n\t\t\treturn nil, fmt.Errorf(\"channel %v is not externally \"+\n\t\t\t\t\"funded or not pending\", chanPoint)\n\t\t}\n\n\t\t// We'll mark the channel as borked before we remove the state\n\t\t// from the switch/peer so it won't be loaded back in if the\n\t\t// peer reconnects.\n\t\tif err := dbChan.MarkBorked(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tremotePub := dbChan.IdentityPub\n\t\tif peer, err := r.server.FindPeer(remotePub); err == nil {\n\t\t\tpeer.WipeChannel(chanPoint)\n\t\t}\n\n\tdefault:\n\t\treturn nil, err\n\t}\n\n\t// Remove the channel from the graph, database and contract court.\n\tif err := r.abandonChan(chanPoint, uint32(bestHeight)); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &lnrpc.AbandonChannelResponse{\n\t\tStatus: fmt.Sprintf(\"channel %v abandoned\", chanPoint.String()),\n\t}, nil\n}\n\n// GetInfo returns general information concerning the lightning node including\n// its identity pubkey, alias, the chains it is connected to, and information\n// concerning the number of open+pending channels.\nfunc (r *rpcServer) GetInfo(_ context.Context,\n\t_ *lnrpc.GetInfoRequest) (*lnrpc.GetInfoResponse, error) {\n\n\tserverPeers := r.server.Peers()\n\n\topenChannels, err := r.server.chanStateDB.FetchAllOpenChannels()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar activeChannels uint32\n\tfor _, channel := range openChannels {\n\t\tchanID := lnwire.NewChanIDFromOutPoint(channel.FundingOutpoint)\n\t\tif r.server.htlcSwitch.HasActiveLink(chanID) {\n\t\t\tactiveChannels++\n\t\t}\n\t}\n\n\tinactiveChannels := uint32(len(openChannels)) - activeChannels\n\n\tpendingChannels, err := r.server.chanStateDB.FetchPendingChannels()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to get retrieve pending \"+\n\t\t\t\"channels: %v\", err)\n\t}\n\tnPendingChannels := uint32(len(pendingChannels))\n\n\tidPub := r.server.identityECDH.PubKey().SerializeCompressed()\n\tencodedIDPub := hex.EncodeToString(idPub)\n\n\tbestHash, bestHeight, err := r.server.cc.ChainIO.GetBestBlock()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to get best block info: %w\", err)\n\t}\n\n\tisSynced, bestHeaderTimestamp, err := r.server.cc.Wallet.IsSynced()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to sync PoV of the wallet \"+\n\t\t\t\"with current best block in the main chain: %v\", err)\n\t}\n\n\t// If the router does full channel validation, it has a lot of work to\n\t// do for each block. So it might be possible that it isn't yet up to\n\t// date with the most recent block, even if the wallet is. This can\n\t// happen in environments with high CPU load (such as parallel itests).\n\t// Since the `synced_to_chain` flag in the response of this call is used\n\t// by many wallets (and also our itests) to make sure everything's up to\n\t// date, we add the router's state to it. So the flag will only toggle\n\t// to true once the router was also able to catch up.\n\tif !r.cfg.Routing.AssumeChannelValid {\n\t\trouterHeight := r.server.graphBuilder.SyncedHeight()\n\t\tisSynced = isSynced && uint32(bestHeight) == routerHeight\n\t}\n\n\tnetwork := lncfg.NormalizeNetwork(r.cfg.ActiveNetParams.Name)\n\tactiveChains := []*lnrpc.Chain{\n\t\t{\n\t\t\tChain:   BitcoinChainName,\n\t\t\tNetwork: network,\n\t\t},\n\t}\n\n\t// Check if external IP addresses were provided to lnd and use them\n\t// to set the URIs.\n\tnodeAnn := r.server.getNodeAnnouncement()\n\n\taddrs := nodeAnn.Addresses\n\turis := make([]string, len(addrs))\n\tfor i, addr := range addrs {\n\t\turis[i] = fmt.Sprintf(\"%s@%s\", encodedIDPub, addr.String())\n\t}\n\n\tisGraphSynced := r.server.authGossiper.SyncManager().IsGraphSynced()\n\n\tfeatures := make(map[uint32]*lnrpc.Feature)\n\tsets := r.server.featureMgr.ListSets()\n\n\tfor _, set := range sets {\n\t\t// Get the a list of lnrpc features for each set we support.\n\t\tfeatureVector := r.server.featureMgr.Get(set)\n\t\trpcFeatures := invoicesrpc.CreateRPCFeatures(featureVector)\n\n\t\t// Add the features to our map of features, allowing over writing of\n\t\t// existing values because features in different sets with the same bit\n\t\t// are duplicated across sets.\n\t\tfor bit, feature := range rpcFeatures {\n\t\t\tfeatures[bit] = feature\n\t\t}\n\t}\n\n\t// TODO(roasbeef): add synced height n stuff\n\n\tisTestNet := chainreg.IsTestnet(&r.cfg.ActiveNetParams)\n\tnodeColor := graph.EncodeHexColor(nodeAnn.RGBColor)\n\tversion := build.Version() + \" commit=\" + build.Commit\n\n\treturn &lnrpc.GetInfoResponse{\n\t\tIdentityPubkey:            encodedIDPub,\n\t\tNumPendingChannels:        nPendingChannels,\n\t\tNumActiveChannels:         activeChannels,\n\t\tNumInactiveChannels:       inactiveChannels,\n\t\tNumPeers:                  uint32(len(serverPeers)),\n\t\tBlockHeight:               uint32(bestHeight),\n\t\tBlockHash:                 bestHash.String(),\n\t\tSyncedToChain:             isSynced,\n\t\tTestnet:                   isTestNet,\n\t\tChains:                    activeChains,\n\t\tUris:                      uris,\n\t\tAlias:                     nodeAnn.Alias.String(),\n\t\tColor:                     nodeColor,\n\t\tBestHeaderTimestamp:       bestHeaderTimestamp,\n\t\tVersion:                   version,\n\t\tCommitHash:                build.CommitHash,\n\t\tSyncedToGraph:             isGraphSynced,\n\t\tFeatures:                  features,\n\t\tRequireHtlcInterceptor:    r.cfg.RequireInterceptor,\n\t\tStoreFinalHtlcResolutions: r.cfg.StoreFinalHtlcResolutions,\n\t}, nil\n}\n\n// GetDebugInfo returns debug information concerning the state of the daemon\n// and its subsystems. This includes the full configuration and the latest log\n// entries from the log file.\nfunc (r *rpcServer) GetDebugInfo(_ context.Context,\n\t_ *lnrpc.GetDebugInfoRequest) (*lnrpc.GetDebugInfoResponse, error) {\n\n\tflatConfig, _, err := configToFlatMap(*r.cfg)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error converting config to flat map: \"+\n\t\t\t\"%w\", err)\n\t}\n\n\tlogFileName := filepath.Join(r.cfg.LogDir, defaultLogFilename)\n\tlogContent, err := os.ReadFile(logFileName)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error reading log file '%s': %w\",\n\t\t\tlogFileName, err)\n\t}\n\n\treturn &lnrpc.GetDebugInfoResponse{\n\t\tConfig: flatConfig,\n\t\tLog:    strings.Split(string(logContent), \"\\n\"),\n\t}, nil\n}\n\n// GetRecoveryInfo returns a boolean indicating whether the wallet is started\n// in recovery mode, whether the recovery is finished, and the progress made\n// so far.\nfunc (r *rpcServer) GetRecoveryInfo(ctx context.Context,\n\tin *lnrpc.GetRecoveryInfoRequest) (*lnrpc.GetRecoveryInfoResponse, error) {\n\n\tisRecoveryMode, progress, err := r.server.cc.Wallet.GetRecoveryInfo()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to get wallet recovery info: %w\",\n\t\t\terr)\n\t}\n\n\trpcsLog.Debugf(\"[getrecoveryinfo] is recovery mode=%v, progress=%v\",\n\t\tisRecoveryMode, progress)\n\n\treturn &lnrpc.GetRecoveryInfoResponse{\n\t\tRecoveryMode:     isRecoveryMode,\n\t\tRecoveryFinished: progress == 1,\n\t\tProgress:         progress,\n\t}, nil\n}\n\n// ListPeers returns a verbose listing of all currently active peers.\nfunc (r *rpcServer) ListPeers(ctx context.Context,\n\tin *lnrpc.ListPeersRequest) (*lnrpc.ListPeersResponse, error) {\n\n\tserverPeers := r.server.Peers()\n\tresp := &lnrpc.ListPeersResponse{\n\t\tPeers: make([]*lnrpc.Peer, 0, len(serverPeers)),\n\t}\n\n\tfor _, serverPeer := range serverPeers {\n\t\tvar (\n\t\t\tsatSent int64\n\t\t\tsatRecv int64\n\t\t)\n\n\t\t// In order to display the total number of satoshis of outbound\n\t\t// (sent) and inbound (recv'd) satoshis that have been\n\t\t// transported through this peer, we'll sum up the sent/recv'd\n\t\t// values for each of the active channels we have with the\n\t\t// peer.\n\t\tchans := serverPeer.ChannelSnapshots()\n\t\tfor _, c := range chans {\n\t\t\tsatSent += int64(c.TotalMSatSent.ToSatoshis())\n\t\t\tsatRecv += int64(c.TotalMSatReceived.ToSatoshis())\n\t\t}\n\n\t\tnodePub := serverPeer.PubKey()\n\n\t\t// Retrieve the peer's sync type. If we don't currently have a\n\t\t// syncer for the peer, then we'll default to a passive sync.\n\t\t// This can happen if the RPC is called while a peer is\n\t\t// initializing.\n\t\tsyncer, ok := r.server.authGossiper.SyncManager().GossipSyncer(\n\t\t\tnodePub,\n\t\t)\n\n\t\tvar lnrpcSyncType lnrpc.Peer_SyncType\n\t\tif !ok {\n\t\t\trpcsLog.Warnf(\"Gossip syncer for peer=%x not found\",\n\t\t\t\tnodePub)\n\t\t\tlnrpcSyncType = lnrpc.Peer_UNKNOWN_SYNC\n\t\t} else {\n\t\t\tsyncType := syncer.SyncType()\n\t\t\tswitch syncType {\n\t\t\tcase discovery.ActiveSync:\n\t\t\t\tlnrpcSyncType = lnrpc.Peer_ACTIVE_SYNC\n\t\t\tcase discovery.PassiveSync:\n\t\t\t\tlnrpcSyncType = lnrpc.Peer_PASSIVE_SYNC\n\t\t\tcase discovery.PinnedSync:\n\t\t\t\tlnrpcSyncType = lnrpc.Peer_PINNED_SYNC\n\t\t\tdefault:\n\t\t\t\treturn nil, fmt.Errorf(\"unhandled sync type %v\",\n\t\t\t\t\tsyncType)\n\t\t\t}\n\t\t}\n\n\t\tfeatures := invoicesrpc.CreateRPCFeatures(\n\t\t\tserverPeer.RemoteFeatures(),\n\t\t)\n\n\t\trpcPeer := &lnrpc.Peer{\n\t\t\tPubKey:          hex.EncodeToString(nodePub[:]),\n\t\t\tAddress:         serverPeer.Conn().RemoteAddr().String(),\n\t\t\tInbound:         serverPeer.Inbound(),\n\t\t\tBytesRecv:       serverPeer.BytesReceived(),\n\t\t\tBytesSent:       serverPeer.BytesSent(),\n\t\t\tSatSent:         satSent,\n\t\t\tSatRecv:         satRecv,\n\t\t\tPingTime:        serverPeer.PingTime(),\n\t\t\tSyncType:        lnrpcSyncType,\n\t\t\tFeatures:        features,\n\t\t\tLastPingPayload: serverPeer.LastRemotePingPayload(),\n\t\t}\n\n\t\tvar peerErrors []interface{}\n\n\t\t// If we only want the most recent error, get the most recent\n\t\t// error from the buffer and add it to our list of errors if\n\t\t// it is non-nil. If we want all the stored errors, simply\n\t\t// add the full list to our set of errors.\n\t\tif in.LatestError {\n\t\t\tlatestErr := serverPeer.ErrorBuffer().Latest()\n\t\t\tif latestErr != nil {\n\t\t\t\tpeerErrors = []interface{}{latestErr}\n\t\t\t}\n\t\t} else {\n\t\t\tpeerErrors = serverPeer.ErrorBuffer().List()\n\t\t}\n\n\t\t// Add the relevant peer errors to our response.\n\t\tfor _, error := range peerErrors {\n\t\t\ttsError := error.(*peer.TimestampedError)\n\n\t\t\trpcErr := &lnrpc.TimestampedError{\n\t\t\t\tTimestamp: uint64(tsError.Timestamp.Unix()),\n\t\t\t\tError:     tsError.Error.Error(),\n\t\t\t}\n\n\t\t\trpcPeer.Errors = append(rpcPeer.Errors, rpcErr)\n\t\t}\n\n\t\t// If the server has started, we can query the event store\n\t\t// for our peer's flap count. If we do so when the server has\n\t\t// not started, the request will block.\n\t\tif r.server.Started() {\n\t\t\tvertex, err := route.NewVertexFromBytes(nodePub[:])\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tflap, ts, err := r.server.chanEventStore.FlapCount(\n\t\t\t\tvertex,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\t// If our timestamp is non-nil, we have values for our\n\t\t\t// peer's flap count, so we set them.\n\t\t\tif ts != nil {\n\t\t\t\trpcPeer.FlapCount = int32(flap)\n\t\t\t\trpcPeer.LastFlapNs = ts.UnixNano()\n\t\t\t}\n\t\t}\n\n\t\tresp.Peers = append(resp.Peers, rpcPeer)\n\t}\n\n\trpcsLog.Debugf(\"[listpeers] yielded %v peers\", serverPeers)\n\n\treturn resp, nil\n}\n\n// SubscribePeerEvents returns a uni-directional stream (server -> client)\n// for notifying the client of peer online and offline events.\nfunc (r *rpcServer) SubscribePeerEvents(req *lnrpc.PeerEventSubscription,\n\teventStream lnrpc.Lightning_SubscribePeerEventsServer) error {\n\n\tpeerEventSub, err := r.server.peerNotifier.SubscribePeerEvents()\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer peerEventSub.Cancel()\n\n\tfor {\n\t\tselect {\n\t\t// A new update has been sent by the peer notifier, we'll\n\t\t// marshal it into the form expected by the gRPC client, then\n\t\t// send it off to the client.\n\t\tcase e := <-peerEventSub.Updates():\n\t\t\tvar event *lnrpc.PeerEvent\n\n\t\t\tswitch peerEvent := e.(type) {\n\t\t\tcase peernotifier.PeerOfflineEvent:\n\t\t\t\tevent = &lnrpc.PeerEvent{\n\t\t\t\t\tPubKey: hex.EncodeToString(peerEvent.PubKey[:]),\n\t\t\t\t\tType:   lnrpc.PeerEvent_PEER_OFFLINE,\n\t\t\t\t}\n\n\t\t\tcase peernotifier.PeerOnlineEvent:\n\t\t\t\tevent = &lnrpc.PeerEvent{\n\t\t\t\t\tPubKey: hex.EncodeToString(peerEvent.PubKey[:]),\n\t\t\t\t\tType:   lnrpc.PeerEvent_PEER_ONLINE,\n\t\t\t\t}\n\n\t\t\tdefault:\n\t\t\t\treturn fmt.Errorf(\"unexpected peer event: %v\", event)\n\t\t\t}\n\n\t\t\tif err := eventStream.Send(event); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t// The response stream's context for whatever reason has been\n\t\t// closed. If context is closed by an exceeded deadline we will\n\t\t// return an error.\n\t\tcase <-eventStream.Context().Done():\n\t\t\tif errors.Is(eventStream.Context().Err(), context.Canceled) {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn eventStream.Context().Err()\n\n\t\tcase <-r.quit:\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\n// WalletBalance returns total unspent outputs(confirmed and unconfirmed), all\n// confirmed unspent outputs and all unconfirmed unspent outputs under control\n// by the wallet. This method can be modified by having the request specify\n// only witness outputs should be factored into the final output sum.\n// TODO(roasbeef): add async hooks into wallet balance changes.\nfunc (r *rpcServer) WalletBalance(ctx context.Context,\n\tin *lnrpc.WalletBalanceRequest) (*lnrpc.WalletBalanceResponse, error) {\n\n\t// Retrieve all existing wallet accounts. We'll compute the confirmed\n\t// and unconfirmed balance for each and tally them up.\n\taccounts, err := r.server.cc.Wallet.ListAccounts(in.Account, nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar totalBalance, confirmedBalance, unconfirmedBalance btcutil.Amount\n\trpcAccountBalances := make(\n\t\tmap[string]*lnrpc.WalletAccountBalance, len(accounts),\n\t)\n\tfor _, account := range accounts {\n\t\t// There are two default accounts, one for NP2WKH outputs and\n\t\t// another for P2WKH outputs. The balance will be computed for\n\t\t// both given one call to ConfirmedBalance with the default\n\t\t// wallet and imported account, so we'll skip the second\n\t\t// instance to avoid inflating the balance.\n\t\tswitch account.AccountName {\n\t\tcase waddrmgr.ImportedAddrAccountName:\n\t\t\t// Omit the imported account from the response unless we\n\t\t\t// actually have any keys imported.\n\t\t\tif account.ImportedKeyCount == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tfallthrough\n\n\t\tcase lnwallet.DefaultAccountName:\n\t\t\tif _, ok := rpcAccountBalances[account.AccountName]; ok {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\tdefault:\n\t\t}\n\n\t\t// There now also are the accounts for the internal channel\n\t\t// related keys. We skip those as they'll never have any direct\n\t\t// balance.\n\t\tif account.KeyScope.Purpose == keychain.BIP0043Purpose {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Get total balance, from txs that have >= 0 confirmations.\n\t\ttotalBal, err := r.server.cc.Wallet.ConfirmedBalance(\n\t\t\t0, account.AccountName,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\ttotalBalance += totalBal\n\n\t\t// Get confirmed balance, from txs that have >= 1 confirmations.\n\t\t// TODO(halseth): get both unconfirmed and confirmed balance in\n\t\t// one call, as this is racy.\n\t\tif in.MinConfs <= 0 {\n\t\t\tin.MinConfs = 1\n\t\t}\n\t\tconfirmedBal, err := r.server.cc.Wallet.ConfirmedBalance(\n\t\t\tin.MinConfs, account.AccountName,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tconfirmedBalance += confirmedBal\n\n\t\t// Get unconfirmed balance, from txs with 0 confirmations.\n\t\tunconfirmedBal := totalBal - confirmedBal\n\t\tunconfirmedBalance += unconfirmedBal\n\n\t\trpcAccountBalances[account.AccountName] = &lnrpc.WalletAccountBalance{\n\t\t\tConfirmedBalance:   int64(confirmedBal),\n\t\t\tUnconfirmedBalance: int64(unconfirmedBal),\n\t\t}\n\t}\n\n\t// Now that we have the base balance accounted for with each account,\n\t// we'll look at the set of locked UTXOs to tally that as well. If we\n\t// don't display this, then anytime we attempt a funding reservation,\n\t// the outputs will chose as being \"gone\" until they're confirmed on\n\t// chain.\n\tvar lockedBalance btcutil.Amount\n\tleases, err := r.server.cc.Wallet.ListLeasedOutputs()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, leasedOutput := range leases {\n\t\tlockedBalance += btcutil.Amount(leasedOutput.Value)\n\t}\n\n\t// Get the current number of non-private anchor channels.\n\tcurrentNumAnchorChans, err := r.server.cc.Wallet.CurrentNumAnchorChans()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Get the required reserve for the wallet.\n\trequiredReserve := r.server.cc.Wallet.RequiredReserve(\n\t\tuint32(currentNumAnchorChans),\n\t)\n\n\trpcsLog.Debugf(\"[walletbalance] Total balance=%v (confirmed=%v, \"+\n\t\t\"unconfirmed=%v)\", totalBalance, confirmedBalance,\n\t\tunconfirmedBalance)\n\n\treturn &lnrpc.WalletBalanceResponse{\n\t\tTotalBalance:              int64(totalBalance),\n\t\tConfirmedBalance:          int64(confirmedBalance),\n\t\tUnconfirmedBalance:        int64(unconfirmedBalance),\n\t\tLockedBalance:             int64(lockedBalance),\n\t\tReservedBalanceAnchorChan: int64(requiredReserve),\n\t\tAccountBalance:            rpcAccountBalances,\n\t}, nil\n}\n\n// ChannelBalance returns the total available channel flow across all open\n// channels in satoshis.\nfunc (r *rpcServer) ChannelBalance(ctx context.Context,\n\tin *lnrpc.ChannelBalanceRequest) (\n\t*lnrpc.ChannelBalanceResponse, error) {\n\n\tvar (\n\t\tlocalBalance             lnwire.MilliSatoshi\n\t\tremoteBalance            lnwire.MilliSatoshi\n\t\tunsettledLocalBalance    lnwire.MilliSatoshi\n\t\tunsettledRemoteBalance   lnwire.MilliSatoshi\n\t\tpendingOpenLocalBalance  lnwire.MilliSatoshi\n\t\tpendingOpenRemoteBalance lnwire.MilliSatoshi\n\t\tcustomDataBuf            bytes.Buffer\n\t)\n\n\topenChannels, err := r.server.chanStateDB.FetchAllOpenChannels()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Encode the number of open channels to the custom data buffer.\n\terr = wire.WriteVarInt(&customDataBuf, 0, uint64(len(openChannels)))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor _, channel := range openChannels {\n\t\tc := channel.LocalCommitment\n\t\tlocalBalance += c.LocalBalance\n\t\tremoteBalance += c.RemoteBalance\n\n\t\t// Add pending htlc amount.\n\t\tfor _, htlc := range c.Htlcs {\n\t\t\tif htlc.Incoming {\n\t\t\t\tunsettledLocalBalance += htlc.Amt\n\t\t\t} else {\n\t\t\t\tunsettledRemoteBalance += htlc.Amt\n\t\t\t}\n\t\t}\n\n\t\t// Encode the custom data for this open channel.\n\t\topenChanData := channel.LocalCommitment.CustomBlob.UnwrapOr(nil)\n\t\terr = wire.WriteVarBytes(&customDataBuf, 0, openChanData)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tpendingChannels, err := r.server.chanStateDB.FetchPendingChannels()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Encode the number of pending channels to the custom data buffer.\n\terr = wire.WriteVarInt(&customDataBuf, 0, uint64(len(pendingChannels)))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor _, channel := range pendingChannels {\n\t\tc := channel.LocalCommitment\n\t\tpendingOpenLocalBalance += c.LocalBalance\n\t\tpendingOpenRemoteBalance += c.RemoteBalance\n\n\t\t// Encode the custom data for this pending channel.\n\t\topenChanData := channel.LocalCommitment.CustomBlob.UnwrapOr(nil)\n\t\terr = wire.WriteVarBytes(&customDataBuf, 0, openChanData)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\trpcsLog.Debugf(\"[channelbalance] local_balance=%v remote_balance=%v \"+\n\t\t\"unsettled_local_balance=%v unsettled_remote_balance=%v \"+\n\t\t\"pending_open_local_balance=%v pending_open_remote_balance=%v\",\n\t\tlocalBalance, remoteBalance, unsettledLocalBalance,\n\t\tunsettledRemoteBalance, pendingOpenLocalBalance,\n\t\tpendingOpenRemoteBalance)\n\n\tresp := &lnrpc.ChannelBalanceResponse{\n\t\tLocalBalance: &lnrpc.Amount{\n\t\t\tSat:  uint64(localBalance.ToSatoshis()),\n\t\t\tMsat: uint64(localBalance),\n\t\t},\n\t\tRemoteBalance: &lnrpc.Amount{\n\t\t\tSat:  uint64(remoteBalance.ToSatoshis()),\n\t\t\tMsat: uint64(remoteBalance),\n\t\t},\n\t\tUnsettledLocalBalance: &lnrpc.Amount{\n\t\t\tSat:  uint64(unsettledLocalBalance.ToSatoshis()),\n\t\t\tMsat: uint64(unsettledLocalBalance),\n\t\t},\n\t\tUnsettledRemoteBalance: &lnrpc.Amount{\n\t\t\tSat:  uint64(unsettledRemoteBalance.ToSatoshis()),\n\t\t\tMsat: uint64(unsettledRemoteBalance),\n\t\t},\n\t\tPendingOpenLocalBalance: &lnrpc.Amount{\n\t\t\tSat:  uint64(pendingOpenLocalBalance.ToSatoshis()),\n\t\t\tMsat: uint64(pendingOpenLocalBalance),\n\t\t},\n\t\tPendingOpenRemoteBalance: &lnrpc.Amount{\n\t\t\tSat:  uint64(pendingOpenRemoteBalance.ToSatoshis()),\n\t\t\tMsat: uint64(pendingOpenRemoteBalance),\n\t\t},\n\t\tCustomChannelData: customDataBuf.Bytes(),\n\n\t\t// Deprecated fields.\n\t\tBalance:            int64(localBalance.ToSatoshis()),\n\t\tPendingOpenBalance: int64(pendingOpenLocalBalance.ToSatoshis()),\n\t}\n\n\terr = fn.MapOptionZ(\n\t\tr.server.implCfg.AuxDataParser,\n\t\tfunc(parser AuxDataParser) error {\n\t\t\treturn parser.InlineParseCustomData(resp)\n\t\t},\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error parsing custom data: %w\", err)\n\t}\n\n\treturn resp, nil\n}\n\ntype (\n\tpendingOpenChannels  []*lnrpc.PendingChannelsResponse_PendingOpenChannel\n\tpendingForceClose    []*lnrpc.PendingChannelsResponse_ForceClosedChannel\n\twaitingCloseChannels []*lnrpc.PendingChannelsResponse_WaitingCloseChannel\n)\n\n// fetchPendingOpenChannels queries the database for a list of channels that\n// have pending open state. The returned result is used in the response of the\n// PendingChannels RPC.\nfunc (r *rpcServer) fetchPendingOpenChannels() (pendingOpenChannels, error) {\n\t// First, we'll populate the response with all the channels that are\n\t// soon to be opened. We can easily fetch this data from the database\n\t// and map the db struct to the proto response.\n\tchannels, err := r.server.chanStateDB.FetchPendingChannels()\n\tif err != nil {\n\t\trpcsLog.Errorf(\"unable to fetch pending channels: %v\", err)\n\t\treturn nil, err\n\t}\n\n\t_, currentHeight, err := r.server.cc.ChainIO.GetBestBlock()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tresult := make(pendingOpenChannels, len(channels))\n\tfor i, pendingChan := range channels {\n\t\tpub := pendingChan.IdentityPub.SerializeCompressed()\n\n\t\t// As this is required for display purposes, we'll calculate\n\t\t// the weight of the commitment transaction. We also add on the\n\t\t// estimated weight of the witness to calculate the weight of\n\t\t// the transaction if it were to be immediately unilaterally\n\t\t// broadcast.\n\t\t// TODO(roasbeef): query for funding tx from wallet, display\n\t\t// that also?\n\t\tvar witnessWeight int64\n\t\tif pendingChan.ChanType.IsTaproot() {\n\t\t\twitnessWeight = input.TaprootKeyPathWitnessSize\n\t\t} else {\n\t\t\twitnessWeight = input.WitnessCommitmentTxWeight\n\t\t}\n\n\t\tlocalCommitment := pendingChan.LocalCommitment\n\t\tutx := btcutil.NewTx(localCommitment.CommitTx)\n\t\tcommitBaseWeight := blockchain.GetTransactionWeight(utx)\n\t\tcommitWeight := commitBaseWeight + witnessWeight\n\n\t\t// FundingExpiryBlocks is the distance from the current block\n\t\t// height to the broadcast height + MaxWaitNumBlocksFundingConf.\n\t\tmaxFundingHeight := funding.MaxWaitNumBlocksFundingConf +\n\t\t\tpendingChan.BroadcastHeight()\n\t\tfundingExpiryBlocks := int32(maxFundingHeight) - currentHeight\n\n\t\tcustomChanBytes, err := encodeCustomChanData(pendingChan)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to encode open chan \"+\n\t\t\t\t\"data: %w\", err)\n\t\t}\n\n\t\tresult[i] = &lnrpc.PendingChannelsResponse_PendingOpenChannel{\n\t\t\tChannel: &lnrpc.PendingChannelsResponse_PendingChannel{\n\t\t\t\tRemoteNodePub:        hex.EncodeToString(pub),\n\t\t\t\tChannelPoint:         pendingChan.FundingOutpoint.String(),\n\t\t\t\tCapacity:             int64(pendingChan.Capacity),\n\t\t\t\tLocalBalance:         int64(localCommitment.LocalBalance.ToSatoshis()),\n\t\t\t\tRemoteBalance:        int64(localCommitment.RemoteBalance.ToSatoshis()),\n\t\t\t\tLocalChanReserveSat:  int64(pendingChan.LocalChanCfg.ChanReserve),\n\t\t\t\tRemoteChanReserveSat: int64(pendingChan.RemoteChanCfg.ChanReserve),\n\t\t\t\tInitiator:            rpcInitiator(pendingChan.IsInitiator),\n\t\t\t\tCommitmentType:       rpcCommitmentType(pendingChan.ChanType),\n\t\t\t\tPrivate:              isPrivate(pendingChan),\n\t\t\t\tMemo:                 string(pendingChan.Memo),\n\t\t\t\tCustomChannelData:    customChanBytes,\n\t\t\t},\n\t\t\tCommitWeight:        commitWeight,\n\t\t\tCommitFee:           int64(localCommitment.CommitFee),\n\t\t\tFeePerKw:            int64(localCommitment.FeePerKw),\n\t\t\tFundingExpiryBlocks: fundingExpiryBlocks,\n\t\t\t// TODO(roasbeef): need to track confirmation height\n\t\t}\n\t}\n\n\treturn result, nil\n}\n\n// fetchPendingForceCloseChannels queries the database for a list of channels\n// that have their closing transactions confirmed but not fully resolved yet.\n// The returned result is used in the response of the PendingChannels RPC.\nfunc (r *rpcServer) fetchPendingForceCloseChannels() (pendingForceClose,\n\tint64, error) {\n\n\t_, currentHeight, err := r.server.cc.ChainIO.GetBestBlock()\n\tif err != nil {\n\t\treturn nil, 0, err\n\t}\n\n\t// Next, we'll examine the channels that are soon to be closed so we\n\t// can populate these fields within the response.\n\tchannels, err := r.server.chanStateDB.FetchClosedChannels(true)\n\tif err != nil {\n\t\trpcsLog.Errorf(\"unable to fetch closed channels: %v\", err)\n\t\treturn nil, 0, err\n\t}\n\n\tresult := make(pendingForceClose, 0)\n\tlimboBalance := int64(0)\n\n\tfor _, pendingClose := range channels {\n\t\t// First construct the channel struct itself, this will be\n\t\t// needed regardless of how this channel was closed.\n\t\tpub := pendingClose.RemotePub.SerializeCompressed()\n\t\tchanPoint := pendingClose.ChanPoint\n\n\t\t// Create the pending channel. If this channel was closed before\n\t\t// we started storing historical channel data, we will not know\n\t\t// who initiated the channel, so we set the initiator field to\n\t\t// unknown.\n\t\tchannel := &lnrpc.PendingChannelsResponse_PendingChannel{\n\t\t\tRemoteNodePub:  hex.EncodeToString(pub),\n\t\t\tChannelPoint:   chanPoint.String(),\n\t\t\tCapacity:       int64(pendingClose.Capacity),\n\t\t\tLocalBalance:   int64(pendingClose.SettledBalance),\n\t\t\tCommitmentType: lnrpc.CommitmentType_UNKNOWN_COMMITMENT_TYPE,\n\t\t\tInitiator:      lnrpc.Initiator_INITIATOR_UNKNOWN,\n\t\t}\n\n\t\t// Lookup the channel in the historical channel bucket to obtain\n\t\t// initiator information. If the historical channel bucket was\n\t\t// not found, or the channel itself, this channel was closed\n\t\t// in a version before we started persisting historical\n\t\t// channels, so we silence the error.\n\t\thistorical, err := r.server.chanStateDB.FetchHistoricalChannel(\n\t\t\t&pendingClose.ChanPoint,\n\t\t)\n\t\tswitch err {\n\t\t// If the channel was closed in a version that did not record\n\t\t// historical channels, ignore the error.\n\t\tcase channeldb.ErrNoHistoricalBucket:\n\t\tcase channeldb.ErrChannelNotFound:\n\n\t\tcase nil:\n\t\t\tchannel.Initiator = rpcInitiator(historical.IsInitiator)\n\t\t\tchannel.CommitmentType = rpcCommitmentType(\n\t\t\t\thistorical.ChanType,\n\t\t\t)\n\n\t\t\t// Get the number of forwarding packages from the\n\t\t\t// historical channel.\n\t\t\tfwdPkgs, err := historical.LoadFwdPkgs()\n\t\t\tif err != nil {\n\t\t\t\trpcsLog.Errorf(\"unable to load forwarding \"+\n\t\t\t\t\t\"packages for channel:%s, %v\",\n\t\t\t\t\thistorical.ShortChannelID, err)\n\t\t\t\treturn nil, 0, err\n\t\t\t}\n\t\t\tchannel.NumForwardingPackages = int64(len(fwdPkgs))\n\n\t\t\tchannel.RemoteBalance = int64(\n\t\t\t\thistorical.LocalCommitment.RemoteBalance.ToSatoshis(),\n\t\t\t)\n\n\t\t\tchannel.Private = isPrivate(historical)\n\t\t\tchannel.Memo = string(historical.Memo)\n\n\t\t// If the error is non-nil, and not due to older versions of lnd\n\t\t// not persisting historical channels, return it.\n\t\tdefault:\n\t\t\treturn nil, 0, err\n\t\t}\n\n\t\tcloseTXID := pendingClose.ClosingTXID.String()\n\n\t\tswitch pendingClose.CloseType {\n\n\t\t// A coop closed channel should never be in the \"pending close\"\n\t\t// state. If a node upgraded from an older lnd version in the\n\t\t// middle of a their channel confirming, it will be in this\n\t\t// state. We log a warning that the channel will not be included\n\t\t// in the now deprecated pending close channels field.\n\t\tcase channeldb.CooperativeClose:\n\t\t\trpcsLog.Warnf(\"channel %v cooperatively closed and \"+\n\t\t\t\t\"in pending close state\",\n\t\t\t\tpendingClose.ChanPoint)\n\n\t\t// If the channel was force closed, then we'll need to query\n\t\t// the utxoNursery for additional information.\n\t\t// TODO(halseth): distinguish remote and local case?\n\t\tcase channeldb.LocalForceClose, channeldb.RemoteForceClose:\n\t\t\tforceClose := &lnrpc.PendingChannelsResponse_ForceClosedChannel{\n\t\t\t\tChannel:     channel,\n\t\t\t\tClosingTxid: closeTXID,\n\t\t\t}\n\n\t\t\t// Fetch reports from both nursery and resolvers. At the\n\t\t\t// moment this is not an atomic snapshot. This is\n\t\t\t// planned to be resolved when the nursery is removed\n\t\t\t// and channel arbitrator will be the single source for\n\t\t\t// these kind of reports.\n\t\t\terr := r.nurseryPopulateForceCloseResp(\n\t\t\t\t&chanPoint, currentHeight, forceClose,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\trpcsLog.Errorf(\"unable to populate nursery \"+\n\t\t\t\t\t\"force close resp:%s, %v\",\n\t\t\t\t\tchanPoint, err)\n\t\t\t\treturn nil, 0, err\n\t\t\t}\n\n\t\t\terr = r.arbitratorPopulateForceCloseResp(\n\t\t\t\t&chanPoint, currentHeight, forceClose,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\trpcsLog.Errorf(\"unable to populate arbitrator \"+\n\t\t\t\t\t\"force close resp:%s, %v\",\n\t\t\t\t\tchanPoint, err)\n\t\t\t\treturn nil, 0, err\n\t\t\t}\n\n\t\t\tlimboBalance += forceClose.LimboBalance\n\t\t\tresult = append(result, forceClose)\n\t\t}\n\t}\n\n\treturn result, limboBalance, nil\n}\n\n// fetchWaitingCloseChannels queries the database for a list of channels\n// that have their closing transactions broadcast but not confirmed yet.\n// The returned result is used in the response of the PendingChannels RPC.\nfunc (r *rpcServer) fetchWaitingCloseChannels(\n\tincludeRawTx bool) (waitingCloseChannels, int64, error) {\n\n\t// We'll also fetch all channels that are open, but have had their\n\t// commitment broadcasted, meaning they are waiting for the closing\n\t// transaction to confirm.\n\tchannels, err := r.server.chanStateDB.FetchWaitingCloseChannels()\n\tif err != nil {\n\t\trpcsLog.Errorf(\"unable to fetch channels waiting close: %v\",\n\t\t\terr)\n\t\treturn nil, 0, err\n\t}\n\n\tresult := make(waitingCloseChannels, 0)\n\tlimboBalance := int64(0)\n\n\t// getClosingTx is a helper closure that tries to find the closing tx of\n\t// a given waiting close channel. Notice that if the remote closes the\n\t// channel, we may not have the closing tx.\n\tgetClosingTx := func(c *channeldb.OpenChannel) (*wire.MsgTx, error) {\n\t\tvar (\n\t\t\ttx  *wire.MsgTx\n\t\t\terr error\n\t\t)\n\n\t\t// First, we try to locate the force closing tx. If not found,\n\t\t// we will then try to find its coop closing tx.\n\t\ttx, err = c.BroadcastedCommitment()\n\t\tif err == nil {\n\t\t\treturn tx, nil\n\t\t}\n\n\t\t// If the error returned is not ErrNoCloseTx, something\n\t\t// unexpected happened and we will return the error.\n\t\tif err != channeldb.ErrNoCloseTx {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Otherwise, we continue to locate its coop closing tx.\n\t\ttx, err = c.BroadcastedCooperative()\n\t\tif err == nil {\n\t\t\treturn tx, nil\n\t\t}\n\n\t\t// Return the error if it's not ErrNoCloseTx.\n\t\tif err != channeldb.ErrNoCloseTx {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Otherwise return an empty tx. This can happen if the remote\n\t\t// broadcast the closing tx and we haven't recorded it yet.\n\t\treturn nil, nil\n\t}\n\n\tfor _, waitingClose := range channels {\n\t\tpub := waitingClose.IdentityPub.SerializeCompressed()\n\t\tchanPoint := waitingClose.FundingOutpoint\n\n\t\tvar commitments lnrpc.PendingChannelsResponse_Commitments\n\n\t\t// Report local commit. May not be present when DLP is active.\n\t\tif waitingClose.LocalCommitment.CommitTx != nil {\n\t\t\tcommitments.LocalTxid =\n\t\t\t\twaitingClose.LocalCommitment.CommitTx.TxHash().\n\t\t\t\t\tString()\n\n\t\t\tcommitments.LocalCommitFeeSat = uint64(\n\t\t\t\twaitingClose.LocalCommitment.CommitFee,\n\t\t\t)\n\t\t}\n\n\t\t// Report remote commit. May not be present when DLP is active.\n\t\tif waitingClose.RemoteCommitment.CommitTx != nil {\n\t\t\tcommitments.RemoteTxid =\n\t\t\t\twaitingClose.RemoteCommitment.CommitTx.TxHash().\n\t\t\t\t\tString()\n\n\t\t\tcommitments.RemoteCommitFeeSat = uint64(\n\t\t\t\twaitingClose.RemoteCommitment.CommitFee,\n\t\t\t)\n\t\t}\n\n\t\t// Report the remote pending commit if any.\n\t\tremoteCommitDiff, err := waitingClose.RemoteCommitChainTip()\n\n\t\tswitch {\n\t\t// Don't set hash if there is no pending remote commit.\n\t\tcase err == channeldb.ErrNoPendingCommit:\n\n\t\t// An unexpected error occurred.\n\t\tcase err != nil:\n\t\t\treturn nil, 0, err\n\n\t\t// There is a pending remote commit. Set its hash in the\n\t\t// response.\n\t\tdefault:\n\t\t\thash := remoteCommitDiff.Commitment.CommitTx.TxHash()\n\t\t\tcommitments.RemotePendingTxid = hash.String()\n\t\t\tcommitments.RemoteCommitFeeSat = uint64(\n\t\t\t\tremoteCommitDiff.Commitment.CommitFee,\n\t\t\t)\n\t\t}\n\n\t\tfwdPkgs, err := waitingClose.LoadFwdPkgs()\n\t\tif err != nil {\n\t\t\trpcsLog.Errorf(\"unable to load forwarding packages \"+\n\t\t\t\t\"for channel:%s, %v\",\n\t\t\t\twaitingClose.ShortChannelID, err)\n\t\t\treturn nil, 0, err\n\t\t}\n\n\t\t// Get the closing tx.\n\t\t// NOTE: the closing tx could be nil here if it's the remote\n\t\t// that broadcasted the closing tx.\n\t\tclosingTx, err := getClosingTx(waitingClose)\n\t\tif err != nil {\n\t\t\trpcsLog.Errorf(\"unable to find closing tx for \"+\n\t\t\t\t\"channel:%s, %v\",\n\t\t\t\twaitingClose.ShortChannelID, err)\n\t\t\treturn nil, 0, err\n\t\t}\n\n\t\tchannel := &lnrpc.PendingChannelsResponse_PendingChannel{\n\t\t\tRemoteNodePub:         hex.EncodeToString(pub),\n\t\t\tChannelPoint:          chanPoint.String(),\n\t\t\tCapacity:              int64(waitingClose.Capacity),\n\t\t\tLocalBalance:          int64(waitingClose.LocalCommitment.LocalBalance.ToSatoshis()),\n\t\t\tRemoteBalance:         int64(waitingClose.LocalCommitment.RemoteBalance.ToSatoshis()),\n\t\t\tLocalChanReserveSat:   int64(waitingClose.LocalChanCfg.ChanReserve),\n\t\t\tRemoteChanReserveSat:  int64(waitingClose.RemoteChanCfg.ChanReserve),\n\t\t\tInitiator:             rpcInitiator(waitingClose.IsInitiator),\n\t\t\tCommitmentType:        rpcCommitmentType(waitingClose.ChanType),\n\t\t\tNumForwardingPackages: int64(len(fwdPkgs)),\n\t\t\tChanStatusFlags:       waitingClose.ChanStatus().String(),\n\t\t\tPrivate:               isPrivate(waitingClose),\n\t\t\tMemo:                  string(waitingClose.Memo),\n\t\t}\n\n\t\tvar closingTxid, closingTxHex string\n\t\tif closingTx != nil {\n\t\t\tclosingTxid = closingTx.TxHash().String()\n\t\t\tif includeRawTx {\n\t\t\t\tvar txBuf bytes.Buffer\n\t\t\t\terr = closingTx.Serialize(&txBuf)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, 0, fmt.Errorf(\"failed to \"+\n\t\t\t\t\t\t\"serialize closing transaction\"+\n\t\t\t\t\t\t\": %w\", err)\n\t\t\t\t}\n\t\t\t\tclosingTxHex = hex.EncodeToString(txBuf.Bytes())\n\t\t\t}\n\t\t}\n\n\t\twaitingCloseResp := &lnrpc.PendingChannelsResponse_WaitingCloseChannel{\n\t\t\tChannel:      channel,\n\t\t\tLimboBalance: channel.LocalBalance,\n\t\t\tCommitments:  &commitments,\n\t\t\tClosingTxid:  closingTxid,\n\t\t\tClosingTxHex: closingTxHex,\n\t\t}\n\n\t\t// A close tx has been broadcasted, all our balance will be in\n\t\t// limbo until it confirms.\n\t\tresult = append(result, waitingCloseResp)\n\t\tlimboBalance += channel.LocalBalance\n\t}\n\n\treturn result, limboBalance, nil\n}\n\n// PendingChannels returns a list of all the channels that are currently\n// considered \"pending\". A channel is pending if it has finished the funding\n// workflow and is waiting for confirmations for the funding txn, or is in the\n// process of closure, either initiated cooperatively or non-cooperatively.\nfunc (r *rpcServer) PendingChannels(ctx context.Context,\n\tin *lnrpc.PendingChannelsRequest) (\n\t*lnrpc.PendingChannelsResponse, error) {\n\n\tresp := &lnrpc.PendingChannelsResponse{}\n\n\t// First, we find all the channels that will soon be opened.\n\tpendingOpenChannels, err := r.fetchPendingOpenChannels()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tresp.PendingOpenChannels = pendingOpenChannels\n\n\t// Second, we fetch all channels that considered pending force closing.\n\t// This means the channels here have their closing transactions\n\t// confirmed but not considered fully resolved yet. For instance, they\n\t// may have a second level HTLCs to be resolved onchain.\n\tpendingCloseChannels, limbo, err := r.fetchPendingForceCloseChannels()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tresp.PendingForceClosingChannels = pendingCloseChannels\n\tresp.TotalLimboBalance = limbo\n\n\t// Third, we fetch all channels that are open, but have had their\n\t// commitment broadcasted, meaning they are waiting for the closing\n\t// transaction to confirm.\n\twaitingCloseChannels, limbo, err := r.fetchWaitingCloseChannels(\n\t\tin.IncludeRawTx,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tresp.WaitingCloseChannels = waitingCloseChannels\n\tresp.TotalLimboBalance += limbo\n\n\terr = fn.MapOptionZ(\n\t\tr.server.implCfg.AuxDataParser,\n\t\tfunc(parser AuxDataParser) error {\n\t\t\treturn parser.InlineParseCustomData(resp)\n\t\t},\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error parsing custom data: %w\", err)\n\t}\n\n\treturn resp, nil\n}\n\n// arbitratorPopulateForceCloseResp populates the pending channels response\n// message with channel resolution information from the contract resolvers.\nfunc (r *rpcServer) arbitratorPopulateForceCloseResp(chanPoint *wire.OutPoint,\n\tcurrentHeight int32,\n\tforceClose *lnrpc.PendingChannelsResponse_ForceClosedChannel) error {\n\n\t// Query for contract resolvers state.\n\tarbitrator, err := r.server.chainArb.GetChannelArbitrator(*chanPoint)\n\tif err != nil {\n\t\treturn err\n\t}\n\treports := arbitrator.Report()\n\n\tfor _, report := range reports {\n\t\tswitch report.Type {\n\t\t// For a direct output, populate/update the top level\n\t\t// response properties.\n\t\tcase contractcourt.ReportOutputUnencumbered:\n\t\t\t// Populate the maturity height fields for the direct\n\t\t\t// commitment output to us.\n\t\t\tforceClose.MaturityHeight = report.MaturityHeight\n\n\t\t\t// If the transaction has been confirmed, then we can\n\t\t\t// compute how many blocks it has left.\n\t\t\tif forceClose.MaturityHeight != 0 {\n\t\t\t\tforceClose.BlocksTilMaturity =\n\t\t\t\t\tint32(forceClose.MaturityHeight) -\n\t\t\t\t\t\tcurrentHeight\n\t\t\t}\n\n\t\t// Add htlcs to the PendingHtlcs response property.\n\t\tcase contractcourt.ReportOutputIncomingHtlc,\n\t\t\tcontractcourt.ReportOutputOutgoingHtlc:\n\n\t\t\t// Don't report details on htlcs that are no longer in\n\t\t\t// limbo.\n\t\t\tif report.LimboBalance == 0 {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tincoming := report.Type == contractcourt.ReportOutputIncomingHtlc\n\t\t\thtlc := &lnrpc.PendingHTLC{\n\t\t\t\tIncoming:       incoming,\n\t\t\t\tAmount:         int64(report.Amount),\n\t\t\t\tOutpoint:       report.Outpoint.String(),\n\t\t\t\tMaturityHeight: report.MaturityHeight,\n\t\t\t\tStage:          report.Stage,\n\t\t\t}\n\n\t\t\tif htlc.MaturityHeight != 0 {\n\t\t\t\thtlc.BlocksTilMaturity =\n\t\t\t\t\tint32(htlc.MaturityHeight) - currentHeight\n\t\t\t}\n\n\t\t\tforceClose.PendingHtlcs = append(forceClose.PendingHtlcs, htlc)\n\n\t\tcase contractcourt.ReportOutputAnchor:\n\t\t\t// There are three resolution states for the anchor:\n\t\t\t// limbo, lost and recovered. Derive the current state\n\t\t\t// from the limbo and recovered balances.\n\t\t\tswitch {\n\t\t\tcase report.RecoveredBalance != 0:\n\t\t\t\tforceClose.Anchor = lnrpc.PendingChannelsResponse_ForceClosedChannel_RECOVERED\n\n\t\t\tcase report.LimboBalance != 0:\n\t\t\t\tforceClose.Anchor = lnrpc.PendingChannelsResponse_ForceClosedChannel_LIMBO\n\n\t\t\tdefault:\n\t\t\t\tforceClose.Anchor = lnrpc.PendingChannelsResponse_ForceClosedChannel_LOST\n\t\t\t}\n\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"unknown report output type: %v\",\n\t\t\t\treport.Type)\n\t\t}\n\n\t\tforceClose.LimboBalance += int64(report.LimboBalance)\n\t\tforceClose.RecoveredBalance += int64(report.RecoveredBalance)\n\t}\n\n\treturn nil\n}\n\n// nurseryPopulateForceCloseResp populates the pending channels response\n// message with contract resolution information from utxonursery.\nfunc (r *rpcServer) nurseryPopulateForceCloseResp(chanPoint *wire.OutPoint,\n\tcurrentHeight int32,\n\tforceClose *lnrpc.PendingChannelsResponse_ForceClosedChannel) error {\n\n\t// Query for the maturity state for this force closed channel. If we\n\t// didn't have any time-locked outputs, then the nursery may not know of\n\t// the contract.\n\tnurseryInfo, err := r.server.utxoNursery.NurseryReport(chanPoint)\n\tif err == contractcourt.ErrContractNotFound {\n\t\treturn nil\n\t}\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to obtain \"+\n\t\t\t\"nursery report for ChannelPoint(%v): %v\",\n\t\t\tchanPoint, err)\n\t}\n\n\t// If the nursery knows of this channel, then we can populate\n\t// information detailing exactly how much funds are time locked and also\n\t// the height in which we can ultimately sweep the funds into the\n\t// wallet.\n\tforceClose.LimboBalance = int64(nurseryInfo.LimboBalance)\n\tforceClose.RecoveredBalance = int64(nurseryInfo.RecoveredBalance)\n\n\tfor _, htlcReport := range nurseryInfo.Htlcs {\n\t\t// TODO(conner) set incoming flag appropriately after handling\n\t\t// incoming incubation\n\t\thtlc := &lnrpc.PendingHTLC{\n\t\t\tIncoming:       false,\n\t\t\tAmount:         int64(htlcReport.Amount),\n\t\t\tOutpoint:       htlcReport.Outpoint.String(),\n\t\t\tMaturityHeight: htlcReport.MaturityHeight,\n\t\t\tStage:          htlcReport.Stage,\n\t\t}\n\n\t\tif htlc.MaturityHeight != 0 {\n\t\t\thtlc.BlocksTilMaturity =\n\t\t\t\tint32(htlc.MaturityHeight) -\n\t\t\t\t\tcurrentHeight\n\t\t}\n\n\t\tforceClose.PendingHtlcs = append(forceClose.PendingHtlcs,\n\t\t\thtlc)\n\t}\n\n\treturn nil\n}\n\n// ClosedChannels returns a list of all the channels have been closed.\n// This does not include channels that are still in the process of closing.\nfunc (r *rpcServer) ClosedChannels(ctx context.Context,\n\tin *lnrpc.ClosedChannelsRequest) (*lnrpc.ClosedChannelsResponse,\n\terror) {\n\n\t// Show all channels when no filter flags are set.\n\tfilterResults := in.Cooperative || in.LocalForce ||\n\t\tin.RemoteForce || in.Breach || in.FundingCanceled ||\n\t\tin.Abandoned\n\n\tresp := &lnrpc.ClosedChannelsResponse{}\n\n\tdbChannels, err := r.server.chanStateDB.FetchClosedChannels(false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// In order to make the response easier to parse for clients, we'll\n\t// sort the set of closed channels by their closing height before\n\t// serializing the proto response.\n\tsort.Slice(dbChannels, func(i, j int) bool {\n\t\treturn dbChannels[i].CloseHeight < dbChannels[j].CloseHeight\n\t})\n\n\tfor _, dbChannel := range dbChannels {\n\t\tif dbChannel.IsPending {\n\t\t\tcontinue\n\t\t}\n\n\t\tswitch dbChannel.CloseType {\n\t\tcase channeldb.CooperativeClose:\n\t\t\tif filterResults && !in.Cooperative {\n\t\t\t\tcontinue\n\t\t\t}\n\t\tcase channeldb.LocalForceClose:\n\t\t\tif filterResults && !in.LocalForce {\n\t\t\t\tcontinue\n\t\t\t}\n\t\tcase channeldb.RemoteForceClose:\n\t\t\tif filterResults && !in.RemoteForce {\n\t\t\t\tcontinue\n\t\t\t}\n\t\tcase channeldb.BreachClose:\n\t\t\tif filterResults && !in.Breach {\n\t\t\t\tcontinue\n\t\t\t}\n\t\tcase channeldb.FundingCanceled:\n\t\t\tif filterResults && !in.FundingCanceled {\n\t\t\t\tcontinue\n\t\t\t}\n\t\tcase channeldb.Abandoned:\n\t\t\tif filterResults && !in.Abandoned {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tchannel, err := r.createRPCClosedChannel(dbChannel)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tresp.Channels = append(resp.Channels, channel)\n\t}\n\n\treturn resp, nil\n}\n\n// LookupHtlcResolution retrieves a final htlc resolution from the database. If\n// the htlc has no final resolution yet, a NotFound grpc status code is\n// returned.\nfunc (r *rpcServer) LookupHtlcResolution(\n\t_ context.Context, in *lnrpc.LookupHtlcResolutionRequest) (\n\t*lnrpc.LookupHtlcResolutionResponse, error) {\n\n\tif !r.cfg.StoreFinalHtlcResolutions {\n\t\treturn nil, status.Error(codes.Unavailable, \"cannot lookup \"+\n\t\t\t\"with flag --store-final-htlc-resolutions=false\")\n\t}\n\n\tchanID := lnwire.NewShortChanIDFromInt(in.ChanId)\n\n\tinfo, err := r.server.chanStateDB.LookupFinalHtlc(chanID, in.HtlcIndex)\n\tswitch {\n\tcase errors.Is(err, channeldb.ErrHtlcUnknown):\n\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\n\tcase err != nil:\n\t\treturn nil, err\n\t}\n\n\treturn &lnrpc.LookupHtlcResolutionResponse{\n\t\tSettled:  info.Settled,\n\t\tOffchain: info.Offchain,\n\t}, nil\n}\n\n// ListChannels returns a description of all the open channels that this node\n// is a participant in.\nfunc (r *rpcServer) ListChannels(ctx context.Context,\n\tin *lnrpc.ListChannelsRequest) (*lnrpc.ListChannelsResponse, error) {\n\n\tif in.ActiveOnly && in.InactiveOnly {\n\t\treturn nil, fmt.Errorf(\"either `active_only` or \" +\n\t\t\t\"`inactive_only` can be set, but not both\")\n\t}\n\n\tif in.PublicOnly && in.PrivateOnly {\n\t\treturn nil, fmt.Errorf(\"either `public_only` or \" +\n\t\t\t\"`private_only` can be set, but not both\")\n\t}\n\n\tif len(in.Peer) > 0 && len(in.Peer) != 33 {\n\t\t_, err := route.NewVertexFromBytes(in.Peer)\n\t\treturn nil, fmt.Errorf(\"invalid `peer` key: %w\", err)\n\t}\n\n\tresp := &lnrpc.ListChannelsResponse{}\n\n\tdbChannels, err := r.server.chanStateDB.FetchAllOpenChannels()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trpcsLog.Debugf(\"[listchannels] fetched %v channels from DB\",\n\t\tlen(dbChannels))\n\n\tfor _, dbChannel := range dbChannels {\n\t\tnodePub := dbChannel.IdentityPub\n\t\tnodePubBytes := nodePub.SerializeCompressed()\n\t\tchanPoint := dbChannel.FundingOutpoint\n\n\t\t// If the caller requested channels for a target node, skip any\n\t\t// that don't match the provided pubkey.\n\t\tif len(in.Peer) > 0 && !bytes.Equal(nodePubBytes, in.Peer) {\n\t\t\tcontinue\n\t\t}\n\n\t\tvar peerOnline bool\n\t\tif _, err := r.server.FindPeer(nodePub); err == nil {\n\t\t\tpeerOnline = true\n\t\t}\n\n\t\tchannelID := lnwire.NewChanIDFromOutPoint(chanPoint)\n\t\tvar linkActive bool\n\t\tif link, err := r.server.htlcSwitch.GetLink(channelID); err == nil {\n\t\t\t// A channel is only considered active if it is known\n\t\t\t// by the switch *and* able to forward\n\t\t\t// incoming/outgoing payments.\n\t\t\tlinkActive = link.EligibleToForward()\n\t\t}\n\n\t\t// Next, we'll determine whether we should add this channel to\n\t\t// our list depending on the type of channels requested to us.\n\t\tisActive := peerOnline && linkActive\n\t\tchannel, err := createRPCOpenChannel(\n\t\t\tr, dbChannel, isActive, in.PeerAliasLookup,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// We'll only skip returning this channel if we were requested\n\t\t// for a specific kind and this channel doesn't satisfy it.\n\t\tswitch {\n\t\tcase in.ActiveOnly && !isActive:\n\t\t\tcontinue\n\t\tcase in.InactiveOnly && isActive:\n\t\t\tcontinue\n\t\tcase in.PublicOnly && channel.Private:\n\t\t\tcontinue\n\t\tcase in.PrivateOnly && !channel.Private:\n\t\t\tcontinue\n\t\t}\n\n\t\tresp.Channels = append(resp.Channels, channel)\n\t}\n\n\terr = fn.MapOptionZ(\n\t\tr.server.implCfg.AuxDataParser,\n\t\tfunc(parser AuxDataParser) error {\n\t\t\treturn parser.InlineParseCustomData(resp)\n\t\t},\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error parsing custom data: %w\", err)\n\t}\n\n\treturn resp, nil\n}\n\n// rpcCommitmentType takes the channel type and converts it to an rpc commitment\n// type value.\nfunc rpcCommitmentType(chanType channeldb.ChannelType) lnrpc.CommitmentType {\n\t// Extract the commitment type from the channel type flags. We must\n\t// first check whether it has anchors, since in that case it would also\n\t// be tweakless.\n\tswitch {\n\tcase chanType.HasTapscriptRoot():\n\t\treturn lnrpc.CommitmentType_SIMPLE_TAPROOT_OVERLAY\n\n\tcase chanType.IsTaproot():\n\t\treturn lnrpc.CommitmentType_SIMPLE_TAPROOT\n\n\tcase chanType.HasLeaseExpiration():\n\t\treturn lnrpc.CommitmentType_SCRIPT_ENFORCED_LEASE\n\n\tcase chanType.HasAnchors():\n\t\treturn lnrpc.CommitmentType_ANCHORS\n\n\tcase chanType.IsTweakless():\n\t\treturn lnrpc.CommitmentType_STATIC_REMOTE_KEY\n\n\tdefault:\n\n\t\treturn lnrpc.CommitmentType_LEGACY\n\t}\n}\n\n// createChannelConstraint creates a *lnrpc.ChannelConstraints using the\n// *Channeldb.ChannelConfig.\nfunc createChannelConstraint(\n\tchanCfg *channeldb.ChannelConfig) *lnrpc.ChannelConstraints {\n\treturn &lnrpc.ChannelConstraints{\n\t\tCsvDelay:          uint32(chanCfg.CsvDelay),\n\t\tChanReserveSat:    uint64(chanCfg.ChanReserve),\n\t\tDustLimitSat:      uint64(chanCfg.DustLimit),\n\t\tMaxPendingAmtMsat: uint64(chanCfg.MaxPendingAmount),\n\t\tMinHtlcMsat:       uint64(chanCfg.MinHTLC),\n\t\tMaxAcceptedHtlcs:  uint32(chanCfg.MaxAcceptedHtlcs),\n\t}\n}\n\n// isPrivate evaluates the ChannelFlags of the db channel to determine if the\n// channel is private or not.\nfunc isPrivate(dbChannel *channeldb.OpenChannel) bool {\n\tif dbChannel == nil {\n\t\treturn false\n\t}\n\treturn dbChannel.ChannelFlags&lnwire.FFAnnounceChannel != 1\n}\n\n// encodeCustomChanData encodes the custom channel data for the open channel.\n// It encodes that data as a pair of var bytes blobs.\nfunc encodeCustomChanData(lnChan *channeldb.OpenChannel) ([]byte, error) {\n\tcustomOpenChanData := lnChan.CustomBlob.UnwrapOr(nil)\n\tcustomLocalCommitData := lnChan.LocalCommitment.CustomBlob.UnwrapOr(nil)\n\n\t// Don't write any custom data if both blobs are empty.\n\tif len(customOpenChanData) == 0 && len(customLocalCommitData) == 0 {\n\t\treturn nil, nil\n\t}\n\n\t// We'll encode our custom channel data as two blobs. The first is a\n\t// set of var bytes encoding of the open chan data, the second is an\n\t// encoding of the local commitment data.\n\tvar customChanDataBuf bytes.Buffer\n\terr := wire.WriteVarBytes(&customChanDataBuf, 0, customOpenChanData)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to encode open chan \"+\n\t\t\t\"data: %w\", err)\n\t}\n\terr = wire.WriteVarBytes(&customChanDataBuf, 0, customLocalCommitData)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to encode local commit \"+\n\t\t\t\"data: %w\", err)\n\t}\n\n\treturn customChanDataBuf.Bytes(), nil\n}\n\n// createRPCOpenChannel creates an *lnrpc.Channel from the *channeldb.Channel.\nfunc createRPCOpenChannel(r *rpcServer, dbChannel *channeldb.OpenChannel,\n\tisActive, peerAliasLookup bool) (*lnrpc.Channel, error) {\n\n\tnodePub := dbChannel.IdentityPub\n\tnodeID := hex.EncodeToString(nodePub.SerializeCompressed())\n\tchanPoint := dbChannel.FundingOutpoint\n\tchanID := lnwire.NewChanIDFromOutPoint(chanPoint)\n\n\t// As this is required for display purposes, we'll calculate\n\t// the weight of the commitment transaction. We also add on the\n\t// estimated weight of the witness to calculate the weight of\n\t// the transaction if it were to be immediately unilaterally\n\t// broadcast.\n\tvar witnessWeight int64\n\tif dbChannel.ChanType.IsTaproot() {\n\t\twitnessWeight = input.TaprootKeyPathWitnessSize\n\t} else {\n\t\twitnessWeight = input.WitnessCommitmentTxWeight\n\t}\n\n\tlocalCommit := dbChannel.LocalCommitment\n\tutx := btcutil.NewTx(localCommit.CommitTx)\n\tcommitBaseWeight := blockchain.GetTransactionWeight(utx)\n\tcommitWeight := commitBaseWeight + witnessWeight\n\n\tlocalBalance := localCommit.LocalBalance\n\tremoteBalance := localCommit.RemoteBalance\n\n\t// As an artifact of our usage of mSAT internally, either party\n\t// may end up in a state where they're holding a fractional\n\t// amount of satoshis which can't be expressed within the\n\t// actual commitment output. Since we round down when going\n\t// from mSAT -> SAT, we may at any point be adding an\n\t// additional SAT to miners fees. As a result, we display a\n\t// commitment fee that accounts for this externally.\n\tvar sumOutputs btcutil.Amount\n\tfor _, txOut := range localCommit.CommitTx.TxOut {\n\t\tsumOutputs += btcutil.Amount(txOut.Value)\n\t}\n\texternalCommitFee := dbChannel.Capacity - sumOutputs\n\n\t// Extract the commitment type from the channel type flags.\n\tcommitmentType := rpcCommitmentType(dbChannel.ChanType)\n\n\tdbScid := dbChannel.ShortChannelID\n\n\t// Fetch the set of aliases for the channel.\n\tchannelAliases := r.server.aliasMgr.GetAliases(dbScid)\n\n\t// Fetch the peer alias. If one does not exist, errNoPeerAlias\n\t// is returned and peerScidAlias will be an empty ShortChannelID.\n\tpeerScidAlias, _ := r.server.aliasMgr.GetPeerAlias(chanID)\n\n\t// Finally we'll attempt to encode the custom channel data if any\n\t// exists.\n\tcustomChanBytes, err := encodeCustomChanData(dbChannel)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to encode open chan data: %w\",\n\t\t\terr)\n\t}\n\n\tchannel := &lnrpc.Channel{\n\t\tActive:                isActive,\n\t\tPrivate:               isPrivate(dbChannel),\n\t\tRemotePubkey:          nodeID,\n\t\tChannelPoint:          chanPoint.String(),\n\t\tChanId:                dbScid.ToUint64(),\n\t\tCapacity:              int64(dbChannel.Capacity),\n\t\tLocalBalance:          int64(localBalance.ToSatoshis()),\n\t\tRemoteBalance:         int64(remoteBalance.ToSatoshis()),\n\t\tCommitFee:             int64(externalCommitFee),\n\t\tCommitWeight:          commitWeight,\n\t\tFeePerKw:              int64(localCommit.FeePerKw),\n\t\tTotalSatoshisSent:     int64(dbChannel.TotalMSatSent.ToSatoshis()),\n\t\tTotalSatoshisReceived: int64(dbChannel.TotalMSatReceived.ToSatoshis()),\n\t\tNumUpdates:            localCommit.CommitHeight,\n\t\tPendingHtlcs:          make([]*lnrpc.HTLC, len(localCommit.Htlcs)),\n\t\tInitiator:             dbChannel.IsInitiator,\n\t\tChanStatusFlags:       dbChannel.ChanStatus().String(),\n\t\tStaticRemoteKey:       commitmentType == lnrpc.CommitmentType_STATIC_REMOTE_KEY,\n\t\tCommitmentType:        commitmentType,\n\t\tThawHeight:            dbChannel.ThawHeight,\n\t\tLocalConstraints: createChannelConstraint(\n\t\t\t&dbChannel.LocalChanCfg,\n\t\t),\n\t\tRemoteConstraints: createChannelConstraint(\n\t\t\t&dbChannel.RemoteChanCfg,\n\t\t),\n\t\tAliasScids:            make([]uint64, 0, len(channelAliases)),\n\t\tPeerScidAlias:         peerScidAlias.ToUint64(),\n\t\tZeroConf:              dbChannel.IsZeroConf(),\n\t\tZeroConfConfirmedScid: dbChannel.ZeroConfRealScid().ToUint64(),\n\t\tMemo:                  string(dbChannel.Memo),\n\t\tCustomChannelData:     customChanBytes,\n\t\t// TODO: remove the following deprecated fields\n\t\tCsvDelay:             uint32(dbChannel.LocalChanCfg.CsvDelay),\n\t\tLocalChanReserveSat:  int64(dbChannel.LocalChanCfg.ChanReserve),\n\t\tRemoteChanReserveSat: int64(dbChannel.RemoteChanCfg.ChanReserve),\n\t}\n\n\t// Look up our channel peer's node alias if the caller requests it.\n\tif peerAliasLookup {\n\t\tpeerAlias, err := r.server.graphDB.LookupAlias(nodePub)\n\t\tif err != nil {\n\t\t\tpeerAlias = fmt.Sprintf(\"unable to lookup \"+\n\t\t\t\t\"peer alias: %v\", err)\n\t\t}\n\t\tchannel.PeerAlias = peerAlias\n\t}\n\n\t// Populate the set of aliases.\n\tfor _, chanAlias := range channelAliases {\n\t\tchannel.AliasScids = append(\n\t\t\tchannel.AliasScids, chanAlias.ToUint64(),\n\t\t)\n\t}\n\n\tfor i, htlc := range localCommit.Htlcs {\n\t\tvar rHash [32]byte\n\t\tcopy(rHash[:], htlc.RHash[:])\n\n\t\tcircuitMap := r.server.htlcSwitch.CircuitLookup()\n\n\t\tvar forwardingChannel, forwardingHtlcIndex uint64\n\t\tswitch {\n\t\tcase htlc.Incoming:\n\t\t\tcircuit := circuitMap.LookupCircuit(\n\t\t\t\thtlcswitch.CircuitKey{\n\t\t\t\t\tChanID: dbChannel.ShortChannelID,\n\t\t\t\t\tHtlcID: htlc.HtlcIndex,\n\t\t\t\t},\n\t\t\t)\n\t\t\tif circuit != nil && circuit.Outgoing != nil {\n\t\t\t\tforwardingChannel = circuit.Outgoing.ChanID.\n\t\t\t\t\tToUint64()\n\n\t\t\t\tforwardingHtlcIndex = circuit.Outgoing.HtlcID\n\t\t\t}\n\n\t\tcase !htlc.Incoming:\n\t\t\tcircuit := circuitMap.LookupOpenCircuit(\n\t\t\t\thtlcswitch.CircuitKey{\n\t\t\t\t\tChanID: dbChannel.ShortChannelID,\n\t\t\t\t\tHtlcID: htlc.HtlcIndex,\n\t\t\t\t},\n\t\t\t)\n\n\t\t\t// If the incoming channel id is the special hop.Source\n\t\t\t// value, the htlc index is a local payment identifier.\n\t\t\t// In this case, report nothing.\n\t\t\tif circuit != nil &&\n\t\t\t\tcircuit.Incoming.ChanID != hop.Source {\n\n\t\t\t\tforwardingChannel = circuit.Incoming.ChanID.\n\t\t\t\t\tToUint64()\n\n\t\t\t\tforwardingHtlcIndex = circuit.Incoming.HtlcID\n\t\t\t}\n\t\t}\n\n\t\tchannel.PendingHtlcs[i] = &lnrpc.HTLC{\n\t\t\tIncoming:            htlc.Incoming,\n\t\t\tAmount:              int64(htlc.Amt.ToSatoshis()),\n\t\t\tHashLock:            rHash[:],\n\t\t\tExpirationHeight:    htlc.RefundTimeout,\n\t\t\tHtlcIndex:           htlc.HtlcIndex,\n\t\t\tForwardingChannel:   forwardingChannel,\n\t\t\tForwardingHtlcIndex: forwardingHtlcIndex,\n\t\t}\n\n\t\t// Add the Pending Htlc Amount to UnsettledBalance field.\n\t\tchannel.UnsettledBalance += channel.PendingHtlcs[i].Amount\n\t}\n\n\t// If we initiated opening the channel, the zero height remote balance\n\t// is the push amount. Otherwise, our starting balance is the push\n\t// amount. If there is no push amount, these values will simply be zero.\n\tif dbChannel.IsInitiator {\n\t\tamt := dbChannel.InitialRemoteBalance.ToSatoshis()\n\t\tchannel.PushAmountSat = uint64(amt)\n\t} else {\n\t\tamt := dbChannel.InitialLocalBalance.ToSatoshis()\n\t\tchannel.PushAmountSat = uint64(amt)\n\t}\n\n\tif len(dbChannel.LocalShutdownScript) > 0 {\n\t\t_, addresses, _, err := txscript.ExtractPkScriptAddrs(\n\t\t\tdbChannel.LocalShutdownScript, r.cfg.ActiveNetParams.Params,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// We only expect one upfront shutdown address for a channel. If\n\t\t// LocalShutdownScript is non-zero, there should be one payout\n\t\t// address set.\n\t\tif len(addresses) != 1 {\n\t\t\treturn nil, fmt.Errorf(\"expected one upfront shutdown \"+\n\t\t\t\t\"address, got: %v\", len(addresses))\n\t\t}\n\n\t\tchannel.CloseAddress = addresses[0].String()\n\t}\n\n\t// If the server hasn't fully started yet, it's possible that the\n\t// channel event store hasn't either, so it won't be able to consume any\n\t// requests until then. To prevent blocking, we'll just omit the uptime\n\t// related fields for now.\n\tif !r.server.Started() {\n\t\treturn channel, nil\n\t}\n\n\tpeer, err := route.NewVertexFromBytes(nodePub.SerializeCompressed())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Query the event store for additional information about the channel.\n\t// Do not fail if it is not available, because there is a potential\n\t// race between a channel being added to our node and the event store\n\t// being notified of it.\n\toutpoint := dbChannel.FundingOutpoint\n\tinfo, err := r.server.chanEventStore.GetChanInfo(outpoint, peer)\n\tswitch err {\n\t// If the store does not know about the channel, we just log it.\n\tcase chanfitness.ErrChannelNotFound:\n\t\trpcsLog.Infof(\"channel: %v not found by channel event store\",\n\t\t\toutpoint)\n\n\t// If we got our channel info, we further populate the channel.\n\tcase nil:\n\t\tchannel.Uptime = int64(info.Uptime.Seconds())\n\t\tchannel.Lifetime = int64(info.Lifetime.Seconds())\n\n\t// If we get an unexpected error, we return it.\n\tdefault:\n\t\treturn nil, err\n\t}\n\n\treturn channel, nil\n}\n\n// createRPCClosedChannel creates an *lnrpc.ClosedChannelSummary from a\n// *channeldb.ChannelCloseSummary.\nfunc (r *rpcServer) createRPCClosedChannel(\n\tdbChannel *channeldb.ChannelCloseSummary) (*lnrpc.ChannelCloseSummary, error) {\n\n\tnodePub := dbChannel.RemotePub\n\tnodeID := hex.EncodeToString(nodePub.SerializeCompressed())\n\n\tvar (\n\t\tcloseType      lnrpc.ChannelCloseSummary_ClosureType\n\t\topenInit       lnrpc.Initiator\n\t\tcloseInitiator lnrpc.Initiator\n\t\terr            error\n\t)\n\n\t// Lookup local and remote cooperative initiators. If these values\n\t// are not known they will just return unknown.\n\topenInit, closeInitiator, err = r.getInitiators(\n\t\t&dbChannel.ChanPoint,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Convert the close type to rpc type.\n\tswitch dbChannel.CloseType {\n\tcase channeldb.CooperativeClose:\n\t\tcloseType = lnrpc.ChannelCloseSummary_COOPERATIVE_CLOSE\n\tcase channeldb.LocalForceClose:\n\t\tcloseType = lnrpc.ChannelCloseSummary_LOCAL_FORCE_CLOSE\n\tcase channeldb.RemoteForceClose:\n\t\tcloseType = lnrpc.ChannelCloseSummary_REMOTE_FORCE_CLOSE\n\tcase channeldb.BreachClose:\n\t\tcloseType = lnrpc.ChannelCloseSummary_BREACH_CLOSE\n\tcase channeldb.FundingCanceled:\n\t\tcloseType = lnrpc.ChannelCloseSummary_FUNDING_CANCELED\n\tcase channeldb.Abandoned:\n\t\tcloseType = lnrpc.ChannelCloseSummary_ABANDONED\n\t}\n\n\tdbScid := dbChannel.ShortChanID\n\n\t// Fetch the set of aliases for this channel.\n\tchannelAliases := r.server.aliasMgr.GetAliases(dbScid)\n\n\tchannel := &lnrpc.ChannelCloseSummary{\n\t\tCapacity:          int64(dbChannel.Capacity),\n\t\tRemotePubkey:      nodeID,\n\t\tCloseHeight:       dbChannel.CloseHeight,\n\t\tCloseType:         closeType,\n\t\tChannelPoint:      dbChannel.ChanPoint.String(),\n\t\tChanId:            dbChannel.ShortChanID.ToUint64(),\n\t\tSettledBalance:    int64(dbChannel.SettledBalance),\n\t\tTimeLockedBalance: int64(dbChannel.TimeLockedBalance),\n\t\tChainHash:         dbChannel.ChainHash.String(),\n\t\tClosingTxHash:     dbChannel.ClosingTXID.String(),\n\t\tOpenInitiator:     openInit,\n\t\tCloseInitiator:    closeInitiator,\n\t\tAliasScids:        make([]uint64, 0, len(channelAliases)),\n\t}\n\n\t// Populate the set of aliases.\n\tfor _, chanAlias := range channelAliases {\n\t\tchannel.AliasScids = append(\n\t\t\tchannel.AliasScids, chanAlias.ToUint64(),\n\t\t)\n\t}\n\n\t// Populate any historical data that the summary needs.\n\thistChan, err := r.server.chanStateDB.FetchHistoricalChannel(\n\t\t&dbChannel.ChanPoint,\n\t)\n\tswitch err {\n\t// The channel was closed in a pre-historic version of lnd. Ignore the\n\t// error.\n\tcase channeldb.ErrNoHistoricalBucket:\n\tcase channeldb.ErrChannelNotFound:\n\n\tcase nil:\n\t\tif histChan.IsZeroConf() && histChan.ZeroConfConfirmed() {\n\t\t\t// If the channel was zero-conf, it may have confirmed.\n\t\t\t// Populate the confirmed SCID if so.\n\t\t\tconfirmedScid := histChan.ZeroConfRealScid().ToUint64()\n\t\t\tchannel.ZeroConfConfirmedScid = confirmedScid\n\t\t}\n\n\t// Non-nil error not due to older versions of lnd.\n\tdefault:\n\t\treturn nil, err\n\t}\n\n\treports, err := r.server.miscDB.FetchChannelReports(\n\t\t*r.cfg.ActiveNetParams.GenesisHash, &dbChannel.ChanPoint,\n\t)\n\tswitch err {\n\t// If the channel does not have its resolver outcomes stored,\n\t// ignore it.\n\tcase channeldb.ErrNoChainHashBucket:\n\t\tfallthrough\n\tcase channeldb.ErrNoChannelSummaries:\n\t\treturn channel, nil\n\n\t// If there is no error, fallthrough the switch to process reports.\n\tcase nil:\n\n\t// If another error occurred, return it.\n\tdefault:\n\t\treturn nil, err\n\t}\n\n\tfor _, report := range reports {\n\t\trpcResolution, err := rpcChannelResolution(report)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tchannel.Resolutions = append(channel.Resolutions, rpcResolution)\n\t}\n\n\treturn channel, nil\n}\n\nfunc rpcChannelResolution(report *channeldb.ResolverReport) (*lnrpc.Resolution,\n\terror) {\n\n\tres := &lnrpc.Resolution{\n\t\tAmountSat: uint64(report.Amount),\n\t\tOutpoint:  lnrpc.MarshalOutPoint(&report.OutPoint),\n\t}\n\n\tif report.SpendTxID != nil {\n\t\tres.SweepTxid = report.SpendTxID.String()\n\t}\n\n\tswitch report.ResolverType {\n\tcase channeldb.ResolverTypeAnchor:\n\t\tres.ResolutionType = lnrpc.ResolutionType_ANCHOR\n\n\tcase channeldb.ResolverTypeIncomingHtlc:\n\t\tres.ResolutionType = lnrpc.ResolutionType_INCOMING_HTLC\n\n\tcase channeldb.ResolverTypeOutgoingHtlc:\n\t\tres.ResolutionType = lnrpc.ResolutionType_OUTGOING_HTLC\n\n\tcase channeldb.ResolverTypeCommit:\n\t\tres.ResolutionType = lnrpc.ResolutionType_COMMIT\n\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown resolver type: %v\",\n\t\t\treport.ResolverType)\n\t}\n\n\tswitch report.ResolverOutcome {\n\tcase channeldb.ResolverOutcomeClaimed:\n\t\tres.Outcome = lnrpc.ResolutionOutcome_CLAIMED\n\n\tcase channeldb.ResolverOutcomeUnclaimed:\n\t\tres.Outcome = lnrpc.ResolutionOutcome_UNCLAIMED\n\n\tcase channeldb.ResolverOutcomeAbandoned:\n\t\tres.Outcome = lnrpc.ResolutionOutcome_ABANDONED\n\n\tcase channeldb.ResolverOutcomeFirstStage:\n\t\tres.Outcome = lnrpc.ResolutionOutcome_FIRST_STAGE\n\n\tcase channeldb.ResolverOutcomeTimeout:\n\t\tres.Outcome = lnrpc.ResolutionOutcome_TIMEOUT\n\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown outcome: %v\",\n\t\t\treport.ResolverOutcome)\n\t}\n\n\treturn res, nil\n}\n\n// getInitiators returns an initiator enum that provides information about the\n// party that initiated channel's open and close. This information is obtained\n// from the historical channel bucket, so unknown values are returned when the\n// channel is not present (which indicates that it was closed before we started\n// writing channels to the historical close bucket).\nfunc (r *rpcServer) getInitiators(chanPoint *wire.OutPoint) (\n\tlnrpc.Initiator,\n\tlnrpc.Initiator, error) {\n\n\tvar (\n\t\topenInitiator  = lnrpc.Initiator_INITIATOR_UNKNOWN\n\t\tcloseInitiator = lnrpc.Initiator_INITIATOR_UNKNOWN\n\t)\n\n\t// To get the close initiator for cooperative closes, we need\n\t// to get the channel status from the historical channel bucket.\n\thistChan, err := r.server.chanStateDB.FetchHistoricalChannel(chanPoint)\n\tswitch {\n\t// The node has upgraded from a version where we did not store\n\t// historical channels, and has not closed a channel since. Do\n\t// not return an error, initiator values are unknown.\n\tcase err == channeldb.ErrNoHistoricalBucket:\n\t\treturn openInitiator, closeInitiator, nil\n\n\t// The channel was closed before we started storing historical\n\t// channels. Do  not return an error, initiator values are unknown.\n\tcase err == channeldb.ErrChannelNotFound:\n\t\treturn openInitiator, closeInitiator, nil\n\n\tcase err != nil:\n\t\treturn 0, 0, err\n\t}\n\n\t// If we successfully looked up the channel, determine initiator based\n\t// on channels status.\n\tif histChan.IsInitiator {\n\t\topenInitiator = lnrpc.Initiator_INITIATOR_LOCAL\n\t} else {\n\t\topenInitiator = lnrpc.Initiator_INITIATOR_REMOTE\n\t}\n\n\tlocalInit := histChan.HasChanStatus(\n\t\tchanneldb.ChanStatusLocalCloseInitiator,\n\t)\n\n\tremoteInit := histChan.HasChanStatus(\n\t\tchanneldb.ChanStatusRemoteCloseInitiator,\n\t)\n\n\tswitch {\n\t// There is a possible case where closes were attempted by both parties.\n\t// We return the initiator as both in this case to provide full\n\t// information about the close.\n\tcase localInit && remoteInit:\n\t\tcloseInitiator = lnrpc.Initiator_INITIATOR_BOTH\n\n\tcase localInit:\n\t\tcloseInitiator = lnrpc.Initiator_INITIATOR_LOCAL\n\n\tcase remoteInit:\n\t\tcloseInitiator = lnrpc.Initiator_INITIATOR_REMOTE\n\t}\n\n\treturn openInitiator, closeInitiator, nil\n}\n\n// SubscribeChannelEvents returns a uni-directional stream (server -> client)\n// for notifying the client of newly active, inactive or closed channels.\nfunc (r *rpcServer) SubscribeChannelEvents(req *lnrpc.ChannelEventSubscription,\n\tupdateStream lnrpc.Lightning_SubscribeChannelEventsServer) error {\n\n\tchannelEventSub, err := r.server.channelNotifier.SubscribeChannelEvents()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Ensure that the resources for the client is cleaned up once either\n\t// the server, or client exits.\n\tdefer channelEventSub.Cancel()\n\n\tfor {\n\t\tselect {\n\t\t// A new update has been sent by the channel router, we'll\n\t\t// marshal it into the form expected by the gRPC client, then\n\t\t// send it off to the client(s).\n\t\tcase e := <-channelEventSub.Updates():\n\t\t\tvar update *lnrpc.ChannelEventUpdate\n\t\t\tswitch event := e.(type) {\n\t\t\tcase channelnotifier.PendingOpenChannelEvent:\n\t\t\t\tupdate = &lnrpc.ChannelEventUpdate{\n\t\t\t\t\tType: lnrpc.ChannelEventUpdate_PENDING_OPEN_CHANNEL,\n\t\t\t\t\tChannel: &lnrpc.ChannelEventUpdate_PendingOpenChannel{\n\t\t\t\t\t\tPendingOpenChannel: &lnrpc.PendingUpdate{\n\t\t\t\t\t\t\tTxid:        event.ChannelPoint.Hash[:],\n\t\t\t\t\t\t\tOutputIndex: event.ChannelPoint.Index,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\tcase channelnotifier.OpenChannelEvent:\n\t\t\t\tchannel, err := createRPCOpenChannel(\n\t\t\t\t\tr, event.Channel, true, false,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\tupdate = &lnrpc.ChannelEventUpdate{\n\t\t\t\t\tType: lnrpc.ChannelEventUpdate_OPEN_CHANNEL,\n\t\t\t\t\tChannel: &lnrpc.ChannelEventUpdate_OpenChannel{\n\t\t\t\t\t\tOpenChannel: channel,\n\t\t\t\t\t},\n\t\t\t\t}\n\n\t\t\tcase channelnotifier.ClosedChannelEvent:\n\t\t\t\tclosedChannel, err := r.createRPCClosedChannel(\n\t\t\t\t\tevent.CloseSummary,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\tupdate = &lnrpc.ChannelEventUpdate{\n\t\t\t\t\tType: lnrpc.ChannelEventUpdate_CLOSED_CHANNEL,\n\t\t\t\t\tChannel: &lnrpc.ChannelEventUpdate_ClosedChannel{\n\t\t\t\t\t\tClosedChannel: closedChannel,\n\t\t\t\t\t},\n\t\t\t\t}\n\n\t\t\tcase channelnotifier.ActiveChannelEvent:\n\t\t\t\tupdate = &lnrpc.ChannelEventUpdate{\n\t\t\t\t\tType: lnrpc.ChannelEventUpdate_ACTIVE_CHANNEL,\n\t\t\t\t\tChannel: &lnrpc.ChannelEventUpdate_ActiveChannel{\n\t\t\t\t\t\tActiveChannel: &lnrpc.ChannelPoint{\n\t\t\t\t\t\t\tFundingTxid: &lnrpc.ChannelPoint_FundingTxidBytes{\n\t\t\t\t\t\t\t\tFundingTxidBytes: event.ChannelPoint.Hash[:],\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tOutputIndex: event.ChannelPoint.Index,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t}\n\n\t\t\tcase channelnotifier.InactiveChannelEvent:\n\t\t\t\tupdate = &lnrpc.ChannelEventUpdate{\n\t\t\t\t\tType: lnrpc.ChannelEventUpdate_INACTIVE_CHANNEL,\n\t\t\t\t\tChannel: &lnrpc.ChannelEventUpdate_InactiveChannel{\n\t\t\t\t\t\tInactiveChannel: &lnrpc.ChannelPoint{\n\t\t\t\t\t\t\tFundingTxid: &lnrpc.ChannelPoint_FundingTxidBytes{\n\t\t\t\t\t\t\t\tFundingTxidBytes: event.ChannelPoint.Hash[:],\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tOutputIndex: event.ChannelPoint.Index,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t}\n\n\t\t\t// Completely ignore ActiveLinkEvent and\n\t\t\t// InactiveLinkEvent as this is explicitly not exposed\n\t\t\t// to the RPC.\n\t\t\tcase channelnotifier.ActiveLinkEvent,\n\t\t\t\tchannelnotifier.InactiveLinkEvent:\n\n\t\t\t\tcontinue\n\n\t\t\tcase channelnotifier.FullyResolvedChannelEvent:\n\t\t\t\tupdate = &lnrpc.ChannelEventUpdate{\n\t\t\t\t\tType: lnrpc.ChannelEventUpdate_FULLY_RESOLVED_CHANNEL,\n\t\t\t\t\tChannel: &lnrpc.ChannelEventUpdate_FullyResolvedChannel{\n\t\t\t\t\t\tFullyResolvedChannel: &lnrpc.ChannelPoint{\n\t\t\t\t\t\t\tFundingTxid: &lnrpc.ChannelPoint_FundingTxidBytes{\n\t\t\t\t\t\t\t\tFundingTxidBytes: event.ChannelPoint.Hash[:],\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tOutputIndex: event.ChannelPoint.Index,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t}\n\n\t\t\tdefault:\n\t\t\t\treturn fmt.Errorf(\"unexpected channel event update: %v\", event)\n\t\t\t}\n\n\t\t\tif err := updateStream.Send(update); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t// The response stream's context for whatever reason has been\n\t\t// closed. If context is closed by an exceeded deadline we will\n\t\t// return an error.\n\t\tcase <-updateStream.Context().Done():\n\t\t\tif errors.Is(updateStream.Context().Err(), context.Canceled) {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn updateStream.Context().Err()\n\n\t\tcase <-r.quit:\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\n// paymentStream enables different types of payment streams, such as:\n// lnrpc.Lightning_SendPaymentServer and lnrpc.Lightning_SendToRouteServer to\n// execute sendPayment. We use this struct as a sort of bridge to enable code\n// re-use between SendPayment and SendToRoute.\ntype paymentStream struct {\n\trecv func() (*rpcPaymentRequest, error)\n\tsend func(*lnrpc.SendResponse) error\n}\n\n// rpcPaymentRequest wraps lnrpc.SendRequest so that routes from\n// lnrpc.SendToRouteRequest can be passed to sendPayment.\ntype rpcPaymentRequest struct {\n\t*lnrpc.SendRequest\n\troute *route.Route\n}\n\n// SendPayment dispatches a bi-directional streaming RPC for sending payments\n// through the Lightning Network. A single RPC invocation creates a persistent\n// bi-directional stream allowing clients to rapidly send payments through the\n// Lightning Network with a single persistent connection.\nfunc (r *rpcServer) SendPayment(stream lnrpc.Lightning_SendPaymentServer) error {\n\tvar lock sync.Mutex\n\n\treturn r.sendPayment(&paymentStream{\n\t\trecv: func() (*rpcPaymentRequest, error) {\n\t\t\treq, err := stream.Recv()\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\treturn &rpcPaymentRequest{\n\t\t\t\tSendRequest: req,\n\t\t\t}, nil\n\t\t},\n\t\tsend: func(r *lnrpc.SendResponse) error {\n\t\t\t// Calling stream.Send concurrently is not safe.\n\t\t\tlock.Lock()\n\t\t\tdefer lock.Unlock()\n\t\t\treturn stream.Send(r)\n\t\t},\n\t})\n}\n\n// SendToRoute dispatches a bi-directional streaming RPC for sending payments\n// through the Lightning Network via predefined routes passed in. A single RPC\n// invocation creates a persistent bi-directional stream allowing clients to\n// rapidly send payments through the Lightning Network with a single persistent\n// connection.\nfunc (r *rpcServer) SendToRoute(stream lnrpc.Lightning_SendToRouteServer) error {\n\tvar lock sync.Mutex\n\n\treturn r.sendPayment(&paymentStream{\n\t\trecv: func() (*rpcPaymentRequest, error) {\n\t\t\treq, err := stream.Recv()\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\treturn r.unmarshallSendToRouteRequest(req)\n\t\t},\n\t\tsend: func(r *lnrpc.SendResponse) error {\n\t\t\t// Calling stream.Send concurrently is not safe.\n\t\t\tlock.Lock()\n\t\t\tdefer lock.Unlock()\n\t\t\treturn stream.Send(r)\n\t\t},\n\t})\n}\n\n// unmarshallSendToRouteRequest unmarshalls an rpc sendtoroute request\nfunc (r *rpcServer) unmarshallSendToRouteRequest(\n\treq *lnrpc.SendToRouteRequest) (*rpcPaymentRequest, error) {\n\n\tif req.Route == nil {\n\t\treturn nil, fmt.Errorf(\"unable to send, no route provided\")\n\t}\n\n\troute, err := r.routerBackend.UnmarshallRoute(req.Route)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &rpcPaymentRequest{\n\t\tSendRequest: &lnrpc.SendRequest{\n\t\t\tPaymentHash:       req.PaymentHash,\n\t\t\tPaymentHashString: req.PaymentHashString,\n\t\t},\n\t\troute: route,\n\t}, nil\n}\n\n// rpcPaymentIntent is a small wrapper struct around the of values we can\n// receive from a client over RPC if they wish to send a payment. We'll either\n// extract these fields from a payment request (which may include routing\n// hints), or we'll get a fully populated route from the user that we'll pass\n// directly to the channel router for dispatching.\ntype rpcPaymentIntent struct {\n\tmsat               lnwire.MilliSatoshi\n\tfeeLimit           lnwire.MilliSatoshi\n\tcltvLimit          uint32\n\tdest               route.Vertex\n\trHash              [32]byte\n\tcltvDelta          uint16\n\trouteHints         [][]zpay32.HopHint\n\toutgoingChannelIDs []uint64\n\tlastHop            *route.Vertex\n\tdestFeatures       *lnwire.FeatureVector\n\tpaymentAddr        fn.Option[[32]byte]\n\tpayReq             []byte\n\tmetadata           []byte\n\tblindedPathSet     *routing.BlindedPaymentPathSet\n\n\tdestCustomRecords record.CustomSet\n\n\troute *route.Route\n}\n\n// extractPaymentIntent attempts to parse the complete details required to\n// dispatch a client from the information presented by an RPC client. There are\n// three ways a client can specify their payment details: a payment request,\n// via manual details, or via a complete route.\nfunc (r *rpcServer) extractPaymentIntent(rpcPayReq *rpcPaymentRequest) (rpcPaymentIntent, error) {\n\tpayIntent := rpcPaymentIntent{}\n\n\t// If a route was specified, then we can use that directly.\n\tif rpcPayReq.route != nil {\n\t\t// If the user is using the REST interface, then they'll be\n\t\t// passing the payment hash as a hex encoded string.\n\t\tif rpcPayReq.PaymentHashString != \"\" {\n\t\t\tpaymentHash, err := hex.DecodeString(\n\t\t\t\trpcPayReq.PaymentHashString,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn payIntent, err\n\t\t\t}\n\n\t\t\tcopy(payIntent.rHash[:], paymentHash)\n\t\t} else {\n\t\t\tcopy(payIntent.rHash[:], rpcPayReq.PaymentHash)\n\t\t}\n\n\t\tpayIntent.route = rpcPayReq.route\n\t\treturn payIntent, nil\n\t}\n\n\t// If there are no routes specified, pass along a outgoing channel\n\t// restriction if specified. The main server rpc does not support\n\t// multiple channel restrictions.\n\tif rpcPayReq.OutgoingChanId != 0 {\n\t\tpayIntent.outgoingChannelIDs = []uint64{\n\t\t\trpcPayReq.OutgoingChanId,\n\t\t}\n\t}\n\n\t// Pass along a last hop restriction if specified.\n\tif len(rpcPayReq.LastHopPubkey) > 0 {\n\t\tlastHop, err := route.NewVertexFromBytes(\n\t\t\trpcPayReq.LastHopPubkey,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn payIntent, err\n\t\t}\n\t\tpayIntent.lastHop = &lastHop\n\t}\n\n\t// Take the CLTV limit from the request if set, otherwise use the max.\n\tcltvLimit, err := routerrpc.ValidateCLTVLimit(\n\t\trpcPayReq.CltvLimit, r.cfg.MaxOutgoingCltvExpiry,\n\t)\n\tif err != nil {\n\t\treturn payIntent, err\n\t}\n\tpayIntent.cltvLimit = cltvLimit\n\n\tcustomRecords := record.CustomSet(rpcPayReq.DestCustomRecords)\n\tif err := customRecords.Validate(); err != nil {\n\t\treturn payIntent, err\n\t}\n\tpayIntent.destCustomRecords = customRecords\n\n\tvalidateDest := func(dest route.Vertex) error {\n\t\tif rpcPayReq.AllowSelfPayment {\n\t\t\treturn nil\n\t\t}\n\n\t\tif dest == r.selfNode {\n\t\t\treturn errors.New(\"self-payments not allowed\")\n\t\t}\n\n\t\treturn nil\n\t}\n\n\t// If the payment request field isn't blank, then the details of the\n\t// invoice are encoded entirely within the encoded payReq.  So we'll\n\t// attempt to decode it, populating the payment accordingly.\n\tif rpcPayReq.PaymentRequest != \"\" {\n\t\tpayReq, err := zpay32.Decode(\n\t\t\trpcPayReq.PaymentRequest, r.cfg.ActiveNetParams.Params,\n\t\t\tzpay32.WithErrorOnUnknownFeatureBit(),\n\t\t)\n\t\tif err != nil {\n\t\t\treturn payIntent, err\n\t\t}\n\n\t\t// Next, we'll ensure that this payreq hasn't already expired.\n\t\terr = routerrpc.ValidatePayReqExpiry(payReq)\n\t\tif err != nil {\n\t\t\treturn payIntent, err\n\t\t}\n\n\t\t// If the amount was not included in the invoice, then we let\n\t\t// the payer specify the amount of satoshis they wish to send.\n\t\t// We override the amount to pay with the amount provided from\n\t\t// the payment request.\n\t\tif payReq.MilliSat == nil {\n\t\t\tamt, err := lnrpc.UnmarshallAmt(\n\t\t\t\trpcPayReq.Amt, rpcPayReq.AmtMsat,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn payIntent, err\n\t\t\t}\n\t\t\tif amt == 0 {\n\t\t\t\treturn payIntent, errors.New(\"amount must be \" +\n\t\t\t\t\t\"specified when paying a zero amount \" +\n\t\t\t\t\t\"invoice\")\n\t\t\t}\n\n\t\t\tpayIntent.msat = amt\n\t\t} else {\n\t\t\tpayIntent.msat = *payReq.MilliSat\n\t\t}\n\n\t\t// Calculate the fee limit that should be used for this payment.\n\t\tpayIntent.feeLimit = lnrpc.CalculateFeeLimit(\n\t\t\trpcPayReq.FeeLimit, payIntent.msat,\n\t\t)\n\n\t\tcopy(payIntent.rHash[:], payReq.PaymentHash[:])\n\t\tdestKey := payReq.Destination.SerializeCompressed()\n\t\tcopy(payIntent.dest[:], destKey)\n\t\tpayIntent.cltvDelta = uint16(payReq.MinFinalCLTVExpiry())\n\t\tpayIntent.routeHints = payReq.RouteHints\n\t\tpayIntent.payReq = []byte(rpcPayReq.PaymentRequest)\n\t\tpayIntent.destFeatures = payReq.Features\n\t\tpayIntent.paymentAddr = payReq.PaymentAddr\n\t\tpayIntent.metadata = payReq.Metadata\n\n\t\tif len(payReq.BlindedPaymentPaths) > 0 {\n\t\t\tpathSet, err := routerrpc.BuildBlindedPathSet(\n\t\t\t\tpayReq.BlindedPaymentPaths,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn payIntent, err\n\t\t\t}\n\t\t\tpayIntent.blindedPathSet = pathSet\n\n\t\t\t// Replace the destination node with the target public\n\t\t\t// key of the blinded path set.\n\t\t\tcopy(\n\t\t\t\tpayIntent.dest[:],\n\t\t\t\tpathSet.TargetPubKey().SerializeCompressed(),\n\t\t\t)\n\n\t\t\tpathFeatures := pathSet.Features()\n\t\t\tif !pathFeatures.IsEmpty() {\n\t\t\t\tpayIntent.destFeatures = pathFeatures.Clone()\n\t\t\t}\n\t\t}\n\n\t\tif err := validateDest(payIntent.dest); err != nil {\n\t\t\treturn payIntent, err\n\t\t}\n\n\t\t// Do bounds checking with the block padding.\n\t\terr = routing.ValidateCLTVLimit(\n\t\t\tpayIntent.cltvLimit, payIntent.cltvDelta, true,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn payIntent, err\n\t\t}\n\n\t\treturn payIntent, nil\n\t}\n\n\t// At this point, a destination MUST be specified, so we'll convert it\n\t// into the proper representation now. The destination will either be\n\t// encoded as raw bytes, or via a hex string.\n\tvar pubBytes []byte\n\tif len(rpcPayReq.Dest) != 0 {\n\t\tpubBytes = rpcPayReq.Dest\n\t} else {\n\t\tvar err error\n\t\tpubBytes, err = hex.DecodeString(rpcPayReq.DestString)\n\t\tif err != nil {\n\t\t\treturn payIntent, err\n\t\t}\n\t}\n\tif len(pubBytes) != 33 {\n\t\treturn payIntent, errors.New(\"invalid key length\")\n\t}\n\tcopy(payIntent.dest[:], pubBytes)\n\n\tif err := validateDest(payIntent.dest); err != nil {\n\t\treturn payIntent, err\n\t}\n\n\t// Payment address may not be needed by legacy invoices.\n\tif len(rpcPayReq.PaymentAddr) != 0 && len(rpcPayReq.PaymentAddr) != 32 {\n\t\treturn payIntent, errors.New(\"invalid payment address length\")\n\t}\n\n\t// Set the payment address if it was explicitly defined with the\n\t// rpcPaymentRequest.\n\t// Note that the payment address for the payIntent should be nil if none\n\t// was provided with the rpcPaymentRequest.\n\tif len(rpcPayReq.PaymentAddr) != 0 {\n\t\tvar addr [32]byte\n\t\tcopy(addr[:], rpcPayReq.PaymentAddr)\n\t\tpayIntent.paymentAddr = fn.Some(addr)\n\t}\n\n\t// Otherwise, If the payment request field was not specified\n\t// (and a custom route wasn't specified), construct the payment\n\t// from the other fields.\n\tpayIntent.msat, err = lnrpc.UnmarshallAmt(\n\t\trpcPayReq.Amt, rpcPayReq.AmtMsat,\n\t)\n\tif err != nil {\n\t\treturn payIntent, err\n\t}\n\n\t// Calculate the fee limit that should be used for this payment.\n\tpayIntent.feeLimit = lnrpc.CalculateFeeLimit(\n\t\trpcPayReq.FeeLimit, payIntent.msat,\n\t)\n\n\tif rpcPayReq.FinalCltvDelta != 0 {\n\t\tpayIntent.cltvDelta = uint16(rpcPayReq.FinalCltvDelta)\n\t} else {\n\t\t// If no final cltv delta is given, assume the default that we\n\t\t// use when creating an invoice. We do not assume the default of\n\t\t// 9 blocks that is defined in BOLT-11, because this is never\n\t\t// enough for other lnd nodes.\n\t\tpayIntent.cltvDelta = uint16(r.cfg.Bitcoin.TimeLockDelta)\n\t}\n\n\t// Do bounds checking with the block padding so the router isn't left\n\t// with a zombie payment in case the user messes up.\n\terr = routing.ValidateCLTVLimit(\n\t\tpayIntent.cltvLimit, payIntent.cltvDelta, true,\n\t)\n\tif err != nil {\n\t\treturn payIntent, err\n\t}\n\n\t// If the user is manually specifying payment details, then the payment\n\t// hash may be encoded as a string.\n\tswitch {\n\tcase rpcPayReq.PaymentHashString != \"\":\n\t\tpaymentHash, err := hex.DecodeString(\n\t\t\trpcPayReq.PaymentHashString,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn payIntent, err\n\t\t}\n\n\t\tcopy(payIntent.rHash[:], paymentHash)\n\n\tdefault:\n\t\tcopy(payIntent.rHash[:], rpcPayReq.PaymentHash)\n\t}\n\n\t// Unmarshal any custom destination features.\n\tpayIntent.destFeatures, err = routerrpc.UnmarshalFeatures(\n\t\trpcPayReq.DestFeatures,\n\t)\n\tif err != nil {\n\t\treturn payIntent, err\n\t}\n\n\treturn payIntent, nil\n}\n\ntype paymentIntentResponse struct {\n\tRoute    *route.Route\n\tPreimage [32]byte\n\tErr      error\n}\n\n// dispatchPaymentIntent attempts to fully dispatch an RPC payment intent.\n// We'll either pass the payment as a whole to the channel router, or give it a\n// pre-built route. The first error this method returns denotes if we were\n// unable to save the payment. The second error returned denotes if the payment\n// didn't succeed.\nfunc (r *rpcServer) dispatchPaymentIntent(\n\tpayIntent *rpcPaymentIntent) (*paymentIntentResponse, error) {\n\n\t// Construct a payment request to send to the channel router. If the\n\t// payment is successful, the route chosen will be returned. Otherwise,\n\t// we'll get a non-nil error.\n\tvar (\n\t\tpreImage  [32]byte\n\t\troute     *route.Route\n\t\trouterErr error\n\t)\n\n\t// If a route was specified, then we'll pass the route directly to the\n\t// router, otherwise we'll create a payment session to execute it.\n\tif payIntent.route == nil {\n\t\tpayment := &routing.LightningPayment{\n\t\t\tTarget:             payIntent.dest,\n\t\t\tAmount:             payIntent.msat,\n\t\t\tFinalCLTVDelta:     payIntent.cltvDelta,\n\t\t\tFeeLimit:           payIntent.feeLimit,\n\t\t\tCltvLimit:          payIntent.cltvLimit,\n\t\t\tRouteHints:         payIntent.routeHints,\n\t\t\tOutgoingChannelIDs: payIntent.outgoingChannelIDs,\n\t\t\tLastHop:            payIntent.lastHop,\n\t\t\tPaymentRequest:     payIntent.payReq,\n\t\t\tPayAttemptTimeout:  routing.DefaultPayAttemptTimeout,\n\t\t\tDestCustomRecords:  payIntent.destCustomRecords,\n\t\t\tDestFeatures:       payIntent.destFeatures,\n\t\t\tPaymentAddr:        payIntent.paymentAddr,\n\t\t\tMetadata:           payIntent.metadata,\n\t\t\tBlindedPathSet:     payIntent.blindedPathSet,\n\n\t\t\t// Don't enable multi-part payments on the main rpc.\n\t\t\t// Users need to use routerrpc for that.\n\t\t\tMaxParts: 1,\n\t\t}\n\t\terr := payment.SetPaymentHash(payIntent.rHash)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tpreImage, route, routerErr = r.server.chanRouter.SendPayment(\n\t\t\tpayment,\n\t\t)\n\t} else {\n\t\tvar attempt *channeldb.HTLCAttempt\n\t\tattempt, routerErr = r.server.chanRouter.SendToRoute(\n\t\t\tpayIntent.rHash, payIntent.route, nil,\n\t\t)\n\n\t\tif routerErr == nil {\n\t\t\tpreImage = attempt.Settle.Preimage\n\t\t}\n\n\t\troute = payIntent.route\n\t}\n\n\t// If the route failed, then we'll return a nil save err, but a non-nil\n\t// routing err.\n\tif routerErr != nil {\n\t\trpcsLog.Warnf(\"Unable to send payment: %v\", routerErr)\n\n\t\treturn &paymentIntentResponse{\n\t\t\tErr: routerErr,\n\t\t}, nil\n\t}\n\n\treturn &paymentIntentResponse{\n\t\tRoute:    route,\n\t\tPreimage: preImage,\n\t}, nil\n}\n\n// sendPayment takes a paymentStream (a source of pre-built routes or payment\n// requests) and continually attempt to dispatch payment requests written to\n// the write end of the stream. Responses will also be streamed back to the\n// client via the write end of the stream. This method is by both SendToRoute\n// and SendPayment as the logic is virtually identical.\nfunc (r *rpcServer) sendPayment(stream *paymentStream) error {\n\tpayChan := make(chan *rpcPaymentIntent)\n\terrChan := make(chan error, 1)\n\n\t// We don't allow payments to be sent while the daemon itself is still\n\t// syncing as we may be trying to sent a payment over a \"stale\"\n\t// channel.\n\tif !r.server.Started() {\n\t\treturn ErrServerNotActive\n\t}\n\n\t// TODO(roasbeef): check payment filter to see if already used?\n\n\t// In order to limit the level of concurrency and prevent a client from\n\t// attempting to OOM the server, we'll set up a semaphore to create an\n\t// upper ceiling on the number of outstanding payments.\n\tconst numOutstandingPayments = 2000\n\thtlcSema := make(chan struct{}, numOutstandingPayments)\n\tfor i := 0; i < numOutstandingPayments; i++ {\n\t\thtlcSema <- struct{}{}\n\t}\n\n\t// We keep track of the running goroutines and set up a quit signal we\n\t// can use to request them to exit if the method returns because of an\n\t// encountered error.\n\tvar wg sync.WaitGroup\n\treqQuit := make(chan struct{})\n\tdefer close(reqQuit)\n\n\t// Launch a new goroutine to handle reading new payment requests from\n\t// the client. This way we can handle errors independently of blocking\n\t// and waiting for the next payment request to come through.\n\t// TODO(joostjager): Callers expect result to come in in the same order\n\t// as the request were sent, but this is far from guarantueed in the\n\t// code below.\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-reqQuit:\n\t\t\t\treturn\n\n\t\t\tdefault:\n\t\t\t\t// Receive the next pending payment within the\n\t\t\t\t// stream sent by the client. If we read the\n\t\t\t\t// EOF sentinel, then the client has closed the\n\t\t\t\t// stream, and we can exit normally.\n\t\t\t\tnextPayment, err := stream.recv()\n\t\t\t\tif err == io.EOF {\n\t\t\t\t\tclose(payChan)\n\t\t\t\t\treturn\n\t\t\t\t} else if err != nil {\n\t\t\t\t\trpcsLog.Errorf(\"Failed receiving from \"+\n\t\t\t\t\t\t\"stream: %v\", err)\n\n\t\t\t\t\tselect {\n\t\t\t\t\tcase errChan <- err:\n\t\t\t\t\tdefault:\n\t\t\t\t\t}\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\t// Populate the next payment, either from the\n\t\t\t\t// payment request, or from the explicitly set\n\t\t\t\t// fields. If the payment proto wasn't well\n\t\t\t\t// formed, then we'll send an error reply and\n\t\t\t\t// wait for the next payment.\n\t\t\t\tpayIntent, err := r.extractPaymentIntent(\n\t\t\t\t\tnextPayment,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\tif err := stream.send(&lnrpc.SendResponse{\n\t\t\t\t\t\tPaymentError: err.Error(),\n\t\t\t\t\t\tPaymentHash:  payIntent.rHash[:],\n\t\t\t\t\t}); err != nil {\n\t\t\t\t\t\trpcsLog.Errorf(\"Failed \"+\n\t\t\t\t\t\t\t\"sending on \"+\n\t\t\t\t\t\t\t\"stream: %v\", err)\n\n\t\t\t\t\t\tselect {\n\t\t\t\t\t\tcase errChan <- err:\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t// If the payment was well formed, then we'll\n\t\t\t\t// send to the dispatch goroutine, or exit,\n\t\t\t\t// which ever comes first.\n\t\t\t\tselect {\n\t\t\t\tcase payChan <- &payIntent:\n\t\t\t\tcase <-reqQuit:\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\nsendLoop:\n\tfor {\n\t\tselect {\n\n\t\t// If we encounter and error either during sending or\n\t\t// receiving, we return directly, closing the stream.\n\t\tcase err := <-errChan:\n\t\t\treturn err\n\n\t\tcase <-r.quit:\n\t\t\treturn errors.New(\"rpc server shutting down\")\n\n\t\tcase payIntent, ok := <-payChan:\n\t\t\t// If the receive loop is done, we break the send loop\n\t\t\t// and wait for the ongoing payments to finish before\n\t\t\t// exiting.\n\t\t\tif !ok {\n\t\t\t\tbreak sendLoop\n\t\t\t}\n\n\t\t\t// We launch a new goroutine to execute the current\n\t\t\t// payment so we can continue to serve requests while\n\t\t\t// this payment is being dispatched.\n\t\t\twg.Add(1)\n\t\t\tgo func(payIntent *rpcPaymentIntent) {\n\t\t\t\tdefer wg.Done()\n\n\t\t\t\t// Attempt to grab a free semaphore slot, using\n\t\t\t\t// a defer to eventually release the slot\n\t\t\t\t// regardless of payment success.\n\t\t\t\tselect {\n\t\t\t\tcase <-htlcSema:\n\t\t\t\tcase <-reqQuit:\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tdefer func() {\n\t\t\t\t\thtlcSema <- struct{}{}\n\t\t\t\t}()\n\n\t\t\t\tresp, saveErr := r.dispatchPaymentIntent(\n\t\t\t\t\tpayIntent,\n\t\t\t\t)\n\n\t\t\t\tswitch {\n\t\t\t\t// If we were unable to save the state of the\n\t\t\t\t// payment, then we'll return the error to the\n\t\t\t\t// user, and terminate.\n\t\t\t\tcase saveErr != nil:\n\t\t\t\t\trpcsLog.Errorf(\"Failed dispatching \"+\n\t\t\t\t\t\t\"payment intent: %v\", saveErr)\n\n\t\t\t\t\tselect {\n\t\t\t\t\tcase errChan <- saveErr:\n\t\t\t\t\tdefault:\n\t\t\t\t\t}\n\t\t\t\t\treturn\n\n\t\t\t\t// If we receive payment error than, instead of\n\t\t\t\t// terminating the stream, send error response\n\t\t\t\t// to the user.\n\t\t\t\tcase resp.Err != nil:\n\t\t\t\t\terr := stream.send(&lnrpc.SendResponse{\n\t\t\t\t\t\tPaymentError: resp.Err.Error(),\n\t\t\t\t\t\tPaymentHash:  payIntent.rHash[:],\n\t\t\t\t\t})\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\trpcsLog.Errorf(\"Failed \"+\n\t\t\t\t\t\t\t\"sending error \"+\n\t\t\t\t\t\t\t\"response: %v\", err)\n\n\t\t\t\t\t\tselect {\n\t\t\t\t\t\tcase errChan <- err:\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tbackend := r.routerBackend\n\t\t\t\tmarshalledRouted, err := backend.MarshallRoute(\n\t\t\t\t\tresp.Route,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\terrChan <- err\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\terr = stream.send(&lnrpc.SendResponse{\n\t\t\t\t\tPaymentHash:     payIntent.rHash[:],\n\t\t\t\t\tPaymentPreimage: resp.Preimage[:],\n\t\t\t\t\tPaymentRoute:    marshalledRouted,\n\t\t\t\t})\n\t\t\t\tif err != nil {\n\t\t\t\t\trpcsLog.Errorf(\"Failed sending \"+\n\t\t\t\t\t\t\"response: %v\", err)\n\n\t\t\t\t\tselect {\n\t\t\t\t\tcase errChan <- err:\n\t\t\t\t\tdefault:\n\t\t\t\t\t}\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}(payIntent)\n\t\t}\n\t}\n\n\t// Wait for all goroutines to finish before closing the stream.\n\twg.Wait()\n\treturn nil\n}\n\n// SendPaymentSync is the synchronous non-streaming version of SendPayment.\n// This RPC is intended to be consumed by clients of the REST proxy.\n// Additionally, this RPC expects the destination's public key and the payment\n// hash (if any) to be encoded as hex strings.\nfunc (r *rpcServer) SendPaymentSync(ctx context.Context,\n\tnextPayment *lnrpc.SendRequest) (*lnrpc.SendResponse, error) {\n\n\treturn r.sendPaymentSync(&rpcPaymentRequest{\n\t\tSendRequest: nextPayment,\n\t})\n}\n\n// SendToRouteSync is the synchronous non-streaming version of SendToRoute.\n// This RPC is intended to be consumed by clients of the REST proxy.\n// Additionally, this RPC expects the payment hash (if any) to be encoded as\n// hex strings.\nfunc (r *rpcServer) SendToRouteSync(ctx context.Context,\n\treq *lnrpc.SendToRouteRequest) (*lnrpc.SendResponse, error) {\n\n\tif req.Route == nil {\n\t\treturn nil, fmt.Errorf(\"unable to send, no routes provided\")\n\t}\n\n\tpaymentRequest, err := r.unmarshallSendToRouteRequest(req)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn r.sendPaymentSync(paymentRequest)\n}\n\n// sendPaymentSync is the synchronous variant of sendPayment. It will block and\n// wait until the payment has been fully completed.\nfunc (r *rpcServer) sendPaymentSync(\n\tnextPayment *rpcPaymentRequest) (*lnrpc.SendResponse, error) {\n\n\t// We don't allow payments to be sent while the daemon itself is still\n\t// syncing as we may be trying to sent a payment over a \"stale\"\n\t// channel.\n\tif !r.server.Started() {\n\t\treturn nil, ErrServerNotActive\n\t}\n\n\t// First we'll attempt to map the proto describing the next payment to\n\t// an intent that we can pass to local sub-systems.\n\tpayIntent, err := r.extractPaymentIntent(nextPayment)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// With the payment validated, we'll now attempt to dispatch the\n\t// payment.\n\tresp, saveErr := r.dispatchPaymentIntent(&payIntent)\n\tswitch {\n\tcase saveErr != nil:\n\t\treturn nil, saveErr\n\n\tcase resp.Err != nil:\n\t\treturn &lnrpc.SendResponse{\n\t\t\tPaymentError: resp.Err.Error(),\n\t\t\tPaymentHash:  payIntent.rHash[:],\n\t\t}, nil\n\t}\n\n\trpcRoute, err := r.routerBackend.MarshallRoute(resp.Route)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &lnrpc.SendResponse{\n\t\tPaymentHash:     payIntent.rHash[:],\n\t\tPaymentPreimage: resp.Preimage[:],\n\t\tPaymentRoute:    rpcRoute,\n\t}, nil\n}\n\n// AddInvoice attempts to add a new invoice to the invoice database. Any\n// duplicated invoices are rejected, therefore all invoices *must* have a\n// unique payment preimage.\nfunc (r *rpcServer) AddInvoice(ctx context.Context,\n\tinvoice *lnrpc.Invoice) (*lnrpc.AddInvoiceResponse, error) {\n\n\tvar (\n\t\tdefaultDelta = r.cfg.Bitcoin.TimeLockDelta\n\t\tblindCfg     = invoice.BlindedPathConfig\n\t\tblind        = invoice.IsBlinded\n\t)\n\n\tglobalBlindCfg := r.server.cfg.Routing.BlindedPaths\n\tblindingRestrictions := &routing.BlindedPathRestrictions{\n\t\tMinDistanceFromIntroNode: globalBlindCfg.MinNumRealHops,\n\t\tNumHops:                  globalBlindCfg.NumHops,\n\t\tMaxNumPaths:              globalBlindCfg.MaxNumPaths,\n\t\tNodeOmissionSet:          fn.NewSet[route.Vertex](),\n\t}\n\n\tif blindCfg != nil && !blind {\n\t\treturn nil, fmt.Errorf(\"blinded path config provided but \" +\n\t\t\t\"IsBlinded not set\")\n\t}\n\n\tif blind && blindCfg != nil {\n\t\tif blindCfg.MinNumRealHops != nil {\n\t\t\tblindingRestrictions.MinDistanceFromIntroNode =\n\t\t\t\tuint8(*blindCfg.MinNumRealHops)\n\t\t}\n\t\tif blindCfg.NumHops != nil {\n\t\t\tblindingRestrictions.NumHops = uint8(*blindCfg.NumHops)\n\t\t}\n\t\tif blindCfg.MaxNumPaths != nil {\n\t\t\tblindingRestrictions.MaxNumPaths =\n\t\t\t\tuint8(*blindCfg.MaxNumPaths)\n\t\t}\n\n\t\tfor _, nodeIDBytes := range blindCfg.NodeOmissionList {\n\t\t\tvertex, err := route.NewVertexFromBytes(nodeIDBytes)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tblindingRestrictions.NodeOmissionSet.Add(vertex)\n\t\t}\n\t}\n\n\tif blindingRestrictions.MinDistanceFromIntroNode >\n\t\tblindingRestrictions.NumHops {\n\n\t\treturn nil, fmt.Errorf(\"the minimum number of real \" +\n\t\t\t\"hops in a blinded path must be smaller than \" +\n\t\t\t\"or equal to the number of hops expected to \" +\n\t\t\t\"be included in each path\")\n\t}\n\n\taddInvoiceCfg := &invoicesrpc.AddInvoiceConfig{\n\t\tAddInvoice:        r.server.invoices.AddInvoice,\n\t\tIsChannelActive:   r.server.htlcSwitch.HasActiveLink,\n\t\tChainParams:       r.cfg.ActiveNetParams.Params,\n\t\tNodeSigner:        r.server.nodeSigner,\n\t\tDefaultCLTVExpiry: defaultDelta,\n\t\tChanDB:            r.server.chanStateDB,\n\t\tGraph:             r.server.graphDB,\n\t\tGenInvoiceFeatures: func() *lnwire.FeatureVector {\n\t\t\tv := r.server.featureMgr.Get(feature.SetInvoice)\n\n\t\t\tif blind {\n\t\t\t\t// If an invoice includes blinded paths, then a\n\t\t\t\t// payment address is not required since we use\n\t\t\t\t// the PathID in the final hop's encrypted data\n\t\t\t\t// as equivalent to the payment address\n\t\t\t\tv.Unset(lnwire.PaymentAddrRequired)\n\t\t\t\tv.Set(lnwire.PaymentAddrOptional)\n\n\t\t\t\t// The invoice payer will also need to\n\t\t\t\t// understand the new BOLT 11 tagged field\n\t\t\t\t// containing the blinded path, so we switch\n\t\t\t\t// the bit to required.\n\t\t\t\tv = feature.SetBit(\n\t\t\t\t\tv, lnwire.Bolt11BlindedPathsRequired,\n\t\t\t\t)\n\t\t\t}\n\n\t\t\treturn v\n\t\t},\n\t\tGenAmpInvoiceFeatures: func() *lnwire.FeatureVector {\n\t\t\treturn r.server.featureMgr.Get(feature.SetInvoiceAmp)\n\t\t},\n\t\tGetAlias:   r.server.aliasMgr.GetPeerAlias,\n\t\tBestHeight: r.server.cc.BestBlockTracker.BestHeight,\n\t\tQueryBlindedRoutes: func(amt lnwire.MilliSatoshi) (\n\t\t\t[]*route.Route, error) {\n\n\t\t\treturn r.server.chanRouter.FindBlindedPaths(\n\t\t\t\tr.selfNode, amt,\n\t\t\t\tr.server.defaultMC.GetProbability,\n\t\t\t\tblindingRestrictions,\n\t\t\t)\n\t\t},\n\t}\n\n\tvalue, err := lnrpc.UnmarshallAmt(invoice.Value, invoice.ValueMsat)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Convert the passed routing hints to the required format.\n\trouteHints, err := invoicesrpc.CreateZpay32HopHints(invoice.RouteHints)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar blindedPathCfg *invoicesrpc.BlindedPathConfig\n\tif blind {\n\t\tbpConfig := r.server.cfg.Routing.BlindedPaths\n\n\t\tblindedPathCfg = &invoicesrpc.BlindedPathConfig{\n\t\t\tRoutePolicyIncrMultiplier: bpConfig.\n\t\t\t\tPolicyIncreaseMultiplier,\n\t\t\tRoutePolicyDecrMultiplier: bpConfig.\n\t\t\t\tPolicyDecreaseMultiplier,\n\t\t\tDefaultDummyHopPolicy: &blindedpath.BlindedHopPolicy{\n\t\t\t\tCLTVExpiryDelta: uint16(defaultDelta),\n\t\t\t\tFeeRate: uint32(\n\t\t\t\t\tr.server.cfg.Bitcoin.FeeRate,\n\t\t\t\t),\n\t\t\t\tBaseFee:     r.server.cfg.Bitcoin.BaseFee,\n\t\t\t\tMinHTLCMsat: r.server.cfg.Bitcoin.MinHTLCIn,\n\n\t\t\t\t// MaxHTLCMsat will be calculated on the fly by\n\t\t\t\t// using the introduction node's channel's\n\t\t\t\t// capacities.\n\t\t\t\tMaxHTLCMsat: 0,\n\t\t\t},\n\t\t\tMinNumPathHops: blindingRestrictions.NumHops,\n\t\t}\n\t}\n\n\taddInvoiceData := &invoicesrpc.AddInvoiceData{\n\t\tMemo:            invoice.Memo,\n\t\tValue:           value,\n\t\tDescriptionHash: invoice.DescriptionHash,\n\t\tExpiry:          invoice.Expiry,\n\t\tFallbackAddr:    invoice.FallbackAddr,\n\t\tCltvExpiry:      invoice.CltvExpiry,\n\t\tPrivate:         invoice.Private,\n\t\tRouteHints:      routeHints,\n\t\tAmp:             invoice.IsAmp,\n\t\tBlindedPathCfg:  blindedPathCfg,\n\t}\n\n\tif invoice.RPreimage != nil {\n\t\tpreimage, err := lntypes.MakePreimage(invoice.RPreimage)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\taddInvoiceData.Preimage = &preimage\n\t}\n\n\thash, dbInvoice, err := invoicesrpc.AddInvoice(\n\t\tctx, addInvoiceCfg, addInvoiceData,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &lnrpc.AddInvoiceResponse{\n\t\tAddIndex:       dbInvoice.AddIndex,\n\t\tPaymentRequest: string(dbInvoice.PaymentRequest),\n\t\tRHash:          hash[:],\n\t\tPaymentAddr:    dbInvoice.Terms.PaymentAddr[:],\n\t}, nil\n}\n\n// LookupInvoice attempts to look up an invoice according to its payment hash.\n// The passed payment hash *must* be exactly 32 bytes, if not an error is\n// returned.\nfunc (r *rpcServer) LookupInvoice(ctx context.Context,\n\treq *lnrpc.PaymentHash) (*lnrpc.Invoice, error) {\n\n\tvar (\n\t\tpayHash [32]byte\n\t\trHash   []byte\n\t\terr     error\n\t)\n\n\t// If the RHash as a raw string was provided, then decode that and use\n\t// that directly. Otherwise, we use the raw bytes provided.\n\tif req.RHashStr != \"\" {\n\t\trHash, err = hex.DecodeString(req.RHashStr)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else {\n\t\trHash = req.RHash\n\t}\n\n\t// Ensure that the payment hash is *exactly* 32-bytes.\n\tif len(rHash) != 0 && len(rHash) != 32 {\n\t\treturn nil, fmt.Errorf(\"payment hash must be exactly \"+\n\t\t\t\"32 bytes, is instead %v\", len(rHash))\n\t}\n\tcopy(payHash[:], rHash)\n\n\trpcsLog.Tracef(\"[lookupinvoice] searching for invoice %x\", payHash[:])\n\n\tinvoice, err := r.server.invoices.LookupInvoice(ctx, payHash)\n\tswitch {\n\tcase errors.Is(err, invoices.ErrInvoiceNotFound):\n\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\tcase err != nil:\n\t\treturn nil, err\n\t}\n\n\trpcsLog.Tracef(\"[lookupinvoice] located invoice %v\",\n\t\tlnutils.SpewLogClosure(invoice))\n\n\trpcInvoice, err := invoicesrpc.CreateRPCInvoice(\n\t\t&invoice, r.cfg.ActiveNetParams.Params,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Give the aux data parser a chance to format the custom data in the\n\t// invoice HTLCs.\n\terr = fn.MapOptionZ(\n\t\tr.server.implCfg.AuxDataParser,\n\t\tfunc(parser AuxDataParser) error {\n\t\t\treturn parser.InlineParseCustomData(rpcInvoice)\n\t\t},\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error parsing custom data: %w\",\n\t\t\terr)\n\t}\n\n\treturn rpcInvoice, nil\n}\n\n// ListInvoices returns a list of all the invoices currently stored within the\n// database. Any active debug invoices are ignored.\nfunc (r *rpcServer) ListInvoices(ctx context.Context,\n\treq *lnrpc.ListInvoiceRequest) (*lnrpc.ListInvoiceResponse, error) {\n\n\t// If the number of invoices was not specified, then we'll default to\n\t// returning the latest 100 invoices.\n\tif req.NumMaxInvoices == 0 {\n\t\treq.NumMaxInvoices = 100\n\t}\n\n\t// If both dates are set, we check that the start date is less than the\n\t// end date, otherwise we'll get an empty result.\n\tif req.CreationDateStart != 0 && req.CreationDateEnd != 0 {\n\t\tif req.CreationDateStart >= req.CreationDateEnd {\n\t\t\treturn nil, fmt.Errorf(\"start date(%v) must be before \"+\n\t\t\t\t\"end date(%v)\", req.CreationDateStart,\n\t\t\t\treq.CreationDateEnd)\n\t\t}\n\t}\n\n\t// Next, we'll map the proto request into a format that is understood by\n\t// the database.\n\tq := invoices.InvoiceQuery{\n\t\tIndexOffset:       req.IndexOffset,\n\t\tNumMaxInvoices:    req.NumMaxInvoices,\n\t\tPendingOnly:       req.PendingOnly,\n\t\tReversed:          req.Reversed,\n\t\tCreationDateStart: int64(req.CreationDateStart),\n\t\tCreationDateEnd:   int64(req.CreationDateEnd),\n\t}\n\n\tinvoiceSlice, err := r.server.invoicesDB.QueryInvoices(ctx, q)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to query invoices: %w\", err)\n\t}\n\n\t// Before returning the response, we'll need to convert each invoice\n\t// into it's proto representation.\n\tresp := &lnrpc.ListInvoiceResponse{\n\t\tInvoices:         make([]*lnrpc.Invoice, len(invoiceSlice.Invoices)),\n\t\tFirstIndexOffset: invoiceSlice.FirstIndexOffset,\n\t\tLastIndexOffset:  invoiceSlice.LastIndexOffset,\n\t}\n\tfor i, invoice := range invoiceSlice.Invoices {\n\t\tinvoice := invoice\n\t\tresp.Invoices[i], err = invoicesrpc.CreateRPCInvoice(\n\t\t\t&invoice, r.cfg.ActiveNetParams.Params,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Give the aux data parser a chance to format the custom data\n\t\t// in the invoice HTLCs.\n\t\terr = fn.MapOptionZ(\n\t\t\tr.server.implCfg.AuxDataParser,\n\t\t\tfunc(parser AuxDataParser) error {\n\t\t\t\treturn parser.InlineParseCustomData(\n\t\t\t\t\tresp.Invoices[i],\n\t\t\t\t)\n\t\t\t},\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"error parsing custom data: %w\",\n\t\t\t\terr)\n\t\t}\n\t}\n\n\treturn resp, nil\n}\n\n// SubscribeInvoices returns a uni-directional stream (server -> client) for\n// notifying the client of newly added/settled invoices.\nfunc (r *rpcServer) SubscribeInvoices(req *lnrpc.InvoiceSubscription,\n\tupdateStream lnrpc.Lightning_SubscribeInvoicesServer) error {\n\n\tinvoiceClient, err := r.server.invoices.SubscribeNotifications(\n\t\tupdateStream.Context(), req.AddIndex, req.SettleIndex,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer invoiceClient.Cancel()\n\n\tfor {\n\t\tselect {\n\t\tcase newInvoice := <-invoiceClient.NewInvoices:\n\t\t\trpcInvoice, err := invoicesrpc.CreateRPCInvoice(\n\t\t\t\tnewInvoice, r.cfg.ActiveNetParams.Params,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t// Give the aux data parser a chance to format the\n\t\t\t// custom data in the invoice HTLCs.\n\t\t\terr = fn.MapOptionZ(\n\t\t\t\tr.server.implCfg.AuxDataParser,\n\t\t\t\tfunc(parser AuxDataParser) error {\n\t\t\t\t\treturn parser.InlineParseCustomData(\n\t\t\t\t\t\trpcInvoice,\n\t\t\t\t\t)\n\t\t\t\t},\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"error parsing custom data: \"+\n\t\t\t\t\t\"%w\", err)\n\t\t\t}\n\n\t\t\tif err := updateStream.Send(rpcInvoice); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\tcase settledInvoice := <-invoiceClient.SettledInvoices:\n\t\t\trpcInvoice, err := invoicesrpc.CreateRPCInvoice(\n\t\t\t\tsettledInvoice, r.cfg.ActiveNetParams.Params,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t// Give the aux data parser a chance to format the\n\t\t\t// custom data in the invoice HTLCs.\n\t\t\terr = fn.MapOptionZ(\n\t\t\t\tr.server.implCfg.AuxDataParser,\n\t\t\t\tfunc(parser AuxDataParser) error {\n\t\t\t\t\treturn parser.InlineParseCustomData(\n\t\t\t\t\t\trpcInvoice,\n\t\t\t\t\t)\n\t\t\t\t},\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"error parsing custom data: \"+\n\t\t\t\t\t\"%w\", err)\n\t\t\t}\n\n\t\t\tif err := updateStream.Send(rpcInvoice); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t// The response stream's context for whatever reason has been\n\t\t// closed. If context is closed by an exceeded deadline we will\n\t\t// return an error.\n\t\tcase <-updateStream.Context().Done():\n\t\t\tif errors.Is(updateStream.Context().Err(), context.Canceled) {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn updateStream.Context().Err()\n\n\t\tcase <-r.quit:\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\n// SubscribeTransactions creates a uni-directional stream (server -> client) in\n// which any newly discovered transactions relevant to the wallet are sent\n// over.\nfunc (r *rpcServer) SubscribeTransactions(req *lnrpc.GetTransactionsRequest,\n\tupdateStream lnrpc.Lightning_SubscribeTransactionsServer) error {\n\n\ttxClient, err := r.server.cc.Wallet.SubscribeTransactions()\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer txClient.Cancel()\n\trpcsLog.Infof(\"New transaction subscription\")\n\n\tfor {\n\t\tselect {\n\t\tcase tx := <-txClient.ConfirmedTransactions():\n\t\t\tdetail := lnrpc.RPCTransaction(tx)\n\t\t\tif err := updateStream.Send(detail); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\tcase tx := <-txClient.UnconfirmedTransactions():\n\t\t\tdetail := lnrpc.RPCTransaction(tx)\n\t\t\tif err := updateStream.Send(detail); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t// The response stream's context for whatever reason has been\n\t\t// closed. If context is closed by an exceeded deadline we will\n\t\t// return an error.\n\t\tcase <-updateStream.Context().Done():\n\t\t\trpcsLog.Infof(\"Canceling transaction subscription\")\n\t\t\tif errors.Is(updateStream.Context().Err(), context.Canceled) {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn updateStream.Context().Err()\n\n\t\tcase <-r.quit:\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\n// GetTransactions returns a list of describing all the known transactions\n// relevant to the wallet.\nfunc (r *rpcServer) GetTransactions(ctx context.Context,\n\treq *lnrpc.GetTransactionsRequest) (*lnrpc.TransactionDetails, error) {\n\n\t// To remain backwards compatible with the old api, default to the\n\t// special case end height which will return transactions from the start\n\t// height until the chain tip, including unconfirmed transactions.\n\tvar endHeight = btcwallet.UnconfirmedHeight\n\n\t// If the user has provided an end height, we overwrite our default.\n\tif req.EndHeight != 0 {\n\t\tendHeight = req.EndHeight\n\t}\n\n\ttxns, firstIdx, lastIdx, err :=\n\t\tr.server.cc.Wallet.ListTransactionDetails(\n\t\t\treq.StartHeight, endHeight, req.Account,\n\t\t\treq.IndexOffset, req.MaxTransactions,\n\t\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn lnrpc.RPCTransactionDetails(txns, firstIdx, lastIdx), nil\n}\n\n// DescribeGraph returns a description of the latest graph state from the PoV\n// of the node. The graph information is partitioned into two components: all\n// the nodes/vertexes, and all the edges that connect the vertexes themselves.\n// As this is a directed graph, the edges also contain the node directional\n// specific routing policy which includes: the time lock delta, fee\n// information, etc.\nfunc (r *rpcServer) DescribeGraph(ctx context.Context,\n\treq *lnrpc.ChannelGraphRequest) (*lnrpc.ChannelGraph, error) {\n\n\tresp := &lnrpc.ChannelGraph{}\n\tincludeUnannounced := req.IncludeUnannounced\n\n\t// Check to see if the cache is already populated, if so then we can\n\t// just return it directly.\n\t//\n\t// TODO(roasbeef): move this to an interceptor level feature?\n\tgraphCacheActive := r.cfg.Caches.RPCGraphCacheDuration != 0\n\tif graphCacheActive {\n\t\tr.graphCache.Lock()\n\t\tdefer r.graphCache.Unlock()\n\n\t\tif r.describeGraphResp != nil {\n\t\t\treturn r.describeGraphResp, nil\n\t\t}\n\t}\n\n\t// Obtain the pointer to the global singleton channel graph, this will\n\t// provide a consistent view of the graph due to bolt db's\n\t// transactional model.\n\tgraph := r.server.graphDB\n\n\t// First iterate through all the known nodes (connected or unconnected\n\t// within the graph), collating their current state into the RPC\n\t// response.\n\terr := graph.ForEachNode(func(_ kvdb.RTx,\n\t\tnode *models.LightningNode) error {\n\n\t\tlnNode := marshalNode(node)\n\n\t\tresp.Nodes = append(resp.Nodes, lnNode)\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Next, for each active channel we know of within the graph, create a\n\t// similar response which details both the edge information as well as\n\t// the routing policies of th nodes connecting the two edges.\n\terr = graph.ForEachChannel(func(edgeInfo *models.ChannelEdgeInfo,\n\t\tc1, c2 *models.ChannelEdgePolicy) error {\n\n\t\t// Do not include unannounced channels unless specifically\n\t\t// requested. Unannounced channels include both private channels as\n\t\t// well as public channels whose authentication proof were not\n\t\t// confirmed yet, hence were not announced.\n\t\tif !includeUnannounced && edgeInfo.AuthProof == nil {\n\t\t\treturn nil\n\t\t}\n\n\t\tedge := marshalDBEdge(edgeInfo, c1, c2)\n\t\tresp.Edges = append(resp.Edges, edge)\n\n\t\treturn nil\n\t})\n\tif err != nil && !errors.Is(err, graphdb.ErrGraphNoEdgesFound) {\n\t\treturn nil, err\n\t}\n\n\t// We still have the mutex held, so we can safely populate the cache\n\t// now to save on GC churn for this query, but only if the cache isn't\n\t// disabled.\n\tif graphCacheActive {\n\t\tr.describeGraphResp = resp\n\t}\n\n\treturn resp, nil\n}\n\n// marshalExtraOpaqueData marshals the given tlv data. If the tlv stream is\n// malformed or empty, an empty map is returned. This makes the method safe to\n// use on unvalidated data.\nfunc marshalExtraOpaqueData(data []byte) map[uint64][]byte {\n\tr := bytes.NewReader(data)\n\n\ttlvStream, err := tlv.NewStream()\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\t// Since ExtraOpaqueData is provided by a potentially malicious peer,\n\t// pass it into the P2P decoding variant.\n\tparsedTypes, err := tlvStream.DecodeWithParsedTypesP2P(r)\n\tif err != nil || len(parsedTypes) == 0 {\n\t\treturn nil\n\t}\n\n\trecords := make(map[uint64][]byte)\n\tfor k, v := range parsedTypes {\n\t\trecords[uint64(k)] = v\n\t}\n\n\treturn records\n}\n\n// extractInboundFeeSafe tries to extract the inbound fee from the given extra\n// opaque data tlv block. If parsing fails, a zero inbound fee is returned. This\n// function is typically used on unvalidated data coming stored in the database.\n// There is not much we can do other than ignoring errors here.\nfunc extractInboundFeeSafe(data lnwire.ExtraOpaqueData) lnwire.Fee {\n\tvar inboundFee lnwire.Fee\n\n\t_, err := data.ExtractRecords(&inboundFee)\n\tif err != nil {\n\t\t// Return zero fee. Do not return the inboundFee variable\n\t\t// because it may be undefined.\n\t\treturn lnwire.Fee{}\n\t}\n\n\treturn inboundFee\n}\n\nfunc marshalDBEdge(edgeInfo *models.ChannelEdgeInfo,\n\tc1, c2 *models.ChannelEdgePolicy) *lnrpc.ChannelEdge {\n\n\t// Make sure the policies match the node they belong to. c1 should point\n\t// to the policy for NodeKey1, and c2 for NodeKey2.\n\tif c1 != nil && c1.ChannelFlags&lnwire.ChanUpdateDirection == 1 ||\n\t\tc2 != nil && c2.ChannelFlags&lnwire.ChanUpdateDirection == 0 {\n\n\t\tc2, c1 = c1, c2\n\t}\n\n\tvar lastUpdate int64\n\tif c1 != nil {\n\t\tlastUpdate = c1.LastUpdate.Unix()\n\t}\n\tif c2 != nil && c2.LastUpdate.Unix() > lastUpdate {\n\t\tlastUpdate = c2.LastUpdate.Unix()\n\t}\n\n\tcustomRecords := marshalExtraOpaqueData(edgeInfo.ExtraOpaqueData)\n\n\tedge := &lnrpc.ChannelEdge{\n\t\tChannelId: edgeInfo.ChannelID,\n\t\tChanPoint: edgeInfo.ChannelPoint.String(),\n\t\t// TODO(roasbeef): update should be on edge info itself\n\t\tLastUpdate:    uint32(lastUpdate),\n\t\tNode1Pub:      hex.EncodeToString(edgeInfo.NodeKey1Bytes[:]),\n\t\tNode2Pub:      hex.EncodeToString(edgeInfo.NodeKey2Bytes[:]),\n\t\tCapacity:      int64(edgeInfo.Capacity),\n\t\tCustomRecords: customRecords,\n\t}\n\n\tif c1 != nil {\n\t\tedge.Node1Policy = marshalDBRoutingPolicy(c1)\n\t}\n\n\tif c2 != nil {\n\t\tedge.Node2Policy = marshalDBRoutingPolicy(c2)\n\t}\n\n\treturn edge\n}\n\nfunc marshalDBRoutingPolicy(\n\tpolicy *models.ChannelEdgePolicy) *lnrpc.RoutingPolicy {\n\n\tdisabled := policy.ChannelFlags&lnwire.ChanUpdateDisabled != 0\n\n\tcustomRecords := marshalExtraOpaqueData(policy.ExtraOpaqueData)\n\tinboundFee := extractInboundFeeSafe(policy.ExtraOpaqueData)\n\n\treturn &lnrpc.RoutingPolicy{\n\t\tTimeLockDelta:    uint32(policy.TimeLockDelta),\n\t\tMinHtlc:          int64(policy.MinHTLC),\n\t\tMaxHtlcMsat:      uint64(policy.MaxHTLC),\n\t\tFeeBaseMsat:      int64(policy.FeeBaseMSat),\n\t\tFeeRateMilliMsat: int64(policy.FeeProportionalMillionths),\n\t\tDisabled:         disabled,\n\t\tLastUpdate:       uint32(policy.LastUpdate.Unix()),\n\t\tCustomRecords:    customRecords,\n\n\t\tInboundFeeBaseMsat:      inboundFee.BaseFee,\n\t\tInboundFeeRateMilliMsat: inboundFee.FeeRate,\n\t}\n}\n\n// GetNodeMetrics returns all available node metrics calculated from the\n// current channel graph.\nfunc (r *rpcServer) GetNodeMetrics(ctx context.Context,\n\treq *lnrpc.NodeMetricsRequest) (*lnrpc.NodeMetricsResponse, error) {\n\n\t// Get requested metric types.\n\tgetCentrality := false\n\tfor _, t := range req.Types {\n\t\tif t == lnrpc.NodeMetricType_BETWEENNESS_CENTRALITY {\n\t\t\tgetCentrality = true\n\t\t}\n\t}\n\n\t// Only centrality can be requested for now.\n\tif !getCentrality {\n\t\treturn nil, nil\n\t}\n\n\tresp := &lnrpc.NodeMetricsResponse{\n\t\tBetweennessCentrality: make(map[string]*lnrpc.FloatMetric),\n\t}\n\n\t// Obtain the pointer to the global singleton channel graph, this will\n\t// provide a consistent view of the graph due to bolt db's\n\t// transactional model.\n\tgraph := r.server.graphDB\n\n\t// Calculate betweenness centrality if requested. Note that depending on the\n\t// graph size, this may take up to a few minutes.\n\tchannelGraph := autopilot.ChannelGraphFromDatabase(graph)\n\tcentralityMetric, err := autopilot.NewBetweennessCentralityMetric(\n\t\truntime.NumCPU(),\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := centralityMetric.Refresh(channelGraph); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Fill normalized and non normalized centrality.\n\tcentrality := centralityMetric.GetMetric(true)\n\tfor nodeID, val := range centrality {\n\t\tresp.BetweennessCentrality[hex.EncodeToString(nodeID[:])] =\n\t\t\t&lnrpc.FloatMetric{\n\t\t\t\tNormalizedValue: val,\n\t\t\t}\n\t}\n\n\tcentrality = centralityMetric.GetMetric(false)\n\tfor nodeID, val := range centrality {\n\t\tresp.BetweennessCentrality[hex.EncodeToString(nodeID[:])].Value = val\n\t}\n\n\treturn resp, nil\n}\n\n// GetChanInfo returns the latest authenticated network announcement for the\n// given channel identified by either its channel ID or a channel outpoint. Both\n// uniquely identify the location of transaction's funding output within the\n// blockchain. The former is an 8-byte integer, while the latter is a string\n// formatted as funding_txid:output_index.\nfunc (r *rpcServer) GetChanInfo(_ context.Context,\n\tin *lnrpc.ChanInfoRequest) (*lnrpc.ChannelEdge, error) {\n\n\tgraph := r.server.graphDB\n\n\tvar (\n\t\tedgeInfo     *models.ChannelEdgeInfo\n\t\tedge1, edge2 *models.ChannelEdgePolicy\n\t\terr          error\n\t)\n\n\tswitch {\n\tcase in.ChanId != 0:\n\t\tedgeInfo, edge1, edge2, err = graph.FetchChannelEdgesByID(\n\t\t\tin.ChanId,\n\t\t)\n\n\tcase in.ChanPoint != \"\":\n\t\tvar chanPoint *wire.OutPoint\n\t\tchanPoint, err = wire.NewOutPointFromString(in.ChanPoint)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tedgeInfo, edge1, edge2, err = graph.FetchChannelEdgesByOutpoint(\n\t\t\tchanPoint,\n\t\t)\n\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"specify either chan_id or chan_point\")\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Convert the database's edge format into the network/RPC edge format\n\t// which couples the edge itself along with the directional node\n\t// routing policies of each node involved within the channel.\n\tchannelEdge := marshalDBEdge(edgeInfo, edge1, edge2)\n\n\treturn channelEdge, nil\n}\n\n// GetNodeInfo returns the latest advertised and aggregate authenticated\n// channel information for the specified node identified by its public key.\nfunc (r *rpcServer) GetNodeInfo(ctx context.Context,\n\tin *lnrpc.NodeInfoRequest) (*lnrpc.NodeInfo, error) {\n\n\tgraph := r.server.graphDB\n\n\t// First, parse the hex-encoded public key into a full in-memory public\n\t// key object we can work with for querying.\n\tpubKey, err := route.NewVertexFromStr(in.PubKey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// With the public key decoded, attempt to fetch the node corresponding\n\t// to this public key. If the node cannot be found, then an error will\n\t// be returned.\n\tnode, err := graph.FetchLightningNode(pubKey)\n\tswitch {\n\tcase errors.Is(err, graphdb.ErrGraphNodeNotFound):\n\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\tcase err != nil:\n\t\treturn nil, err\n\t}\n\n\t// With the node obtained, we'll now iterate through all its out going\n\t// edges to gather some basic statistics about its out going channels.\n\tvar (\n\t\tnumChannels   uint32\n\t\ttotalCapacity btcutil.Amount\n\t\tchannels      []*lnrpc.ChannelEdge\n\t)\n\n\terr = graph.ForEachNodeChannel(node.PubKeyBytes,\n\t\tfunc(_ kvdb.RTx, edge *models.ChannelEdgeInfo,\n\t\t\tc1, c2 *models.ChannelEdgePolicy) error {\n\n\t\t\tnumChannels++\n\t\t\ttotalCapacity += edge.Capacity\n\n\t\t\t// Only populate the node's channels if the user\n\t\t\t// requested them.\n\t\t\tif in.IncludeChannels {\n\t\t\t\t// Do not include unannounced channels - private\n\t\t\t\t// channels or public channels whose\n\t\t\t\t// authentication proof were not confirmed yet.\n\t\t\t\tif edge.AuthProof == nil {\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\n\t\t\t\t// Convert the database's edge format into the\n\t\t\t\t// network/RPC edge format.\n\t\t\t\tchannelEdge := marshalDBEdge(edge, c1, c2)\n\t\t\t\tchannels = append(channels, channelEdge)\n\t\t\t}\n\n\t\t\treturn nil\n\t\t},\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &lnrpc.NodeInfo{\n\t\tNode:          marshalNode(node),\n\t\tNumChannels:   numChannels,\n\t\tTotalCapacity: int64(totalCapacity),\n\t\tChannels:      channels,\n\t}, nil\n}\n\nfunc marshalNode(node *models.LightningNode) *lnrpc.LightningNode {\n\tnodeAddrs := make([]*lnrpc.NodeAddress, len(node.Addresses))\n\tfor i, addr := range node.Addresses {\n\t\tnodeAddr := &lnrpc.NodeAddress{\n\t\t\tNetwork: addr.Network(),\n\t\t\tAddr:    addr.String(),\n\t\t}\n\t\tnodeAddrs[i] = nodeAddr\n\t}\n\n\tfeatures := invoicesrpc.CreateRPCFeatures(node.Features)\n\n\tcustomRecords := marshalExtraOpaqueData(node.ExtraOpaqueData)\n\n\treturn &lnrpc.LightningNode{\n\t\tLastUpdate:    uint32(node.LastUpdate.Unix()),\n\t\tPubKey:        hex.EncodeToString(node.PubKeyBytes[:]),\n\t\tAddresses:     nodeAddrs,\n\t\tAlias:         node.Alias,\n\t\tColor:         graph.EncodeHexColor(node.Color),\n\t\tFeatures:      features,\n\t\tCustomRecords: customRecords,\n\t}\n}\n\n// QueryRoutes attempts to query the daemons' Channel Router for a possible\n// route to a target destination capable of carrying a specific amount of\n// satoshis within the route's flow. The returned route contains the full\n// details required to craft and send an HTLC, also including the necessary\n// information that should be present within the Sphinx packet encapsulated\n// within the HTLC.\n//\n// TODO(roasbeef): should return a slice of routes in reality\n//   - create separate PR to send based on well formatted route\nfunc (r *rpcServer) QueryRoutes(ctx context.Context,\n\tin *lnrpc.QueryRoutesRequest) (*lnrpc.QueryRoutesResponse, error) {\n\n\treturn r.routerBackend.QueryRoutes(ctx, in)\n}\n\n// GetNetworkInfo returns some basic stats about the known channel graph from\n// the PoV of the node.\nfunc (r *rpcServer) GetNetworkInfo(ctx context.Context,\n\t_ *lnrpc.NetworkInfoRequest) (*lnrpc.NetworkInfo, error) {\n\n\tgraph := r.server.graphDB\n\n\tvar (\n\t\tnumNodes             uint32\n\t\tnumChannels          uint32\n\t\tmaxChanOut           uint32\n\t\ttotalNetworkCapacity btcutil.Amount\n\t\tminChannelSize       btcutil.Amount = math.MaxInt64\n\t\tmaxChannelSize       btcutil.Amount\n\t\tmedianChanSize       btcutil.Amount\n\t)\n\n\t// We'll use this map to de-duplicate channels during our traversal.\n\t// This is needed since channels are directional, so there will be two\n\t// edges for each channel within the graph.\n\tseenChans := make(map[uint64]struct{})\n\n\t// We also keep a list of all encountered capacities, in order to\n\t// calculate the median channel size.\n\tvar allChans []btcutil.Amount\n\n\t// We'll run through all the known nodes in the within our view of the\n\t// network, tallying up the total number of nodes, and also gathering\n\t// each node so we can measure the graph diameter and degree stats\n\t// below.\n\terr := graph.ForEachNodeCached(func(node route.Vertex,\n\t\tedges map[uint64]*graphdb.DirectedChannel) error {\n\n\t\t// Increment the total number of nodes with each iteration.\n\t\tnumNodes++\n\n\t\t// For each channel we'll compute the out degree of each node,\n\t\t// and also update our running tallies of the min/max channel\n\t\t// capacity, as well as the total channel capacity. We pass\n\t\t// through the db transaction from the outer view so we can\n\t\t// re-use it within this inner view.\n\t\tvar outDegree uint32\n\t\tfor _, edge := range edges {\n\t\t\t// Bump up the out degree for this node for each\n\t\t\t// channel encountered.\n\t\t\toutDegree++\n\n\t\t\t// If we've already seen this channel, then we'll\n\t\t\t// return early to ensure that we don't double-count\n\t\t\t// stats.\n\t\t\tif _, ok := seenChans[edge.ChannelID]; ok {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\t// Compare the capacity of this channel against the\n\t\t\t// running min/max to see if we should update the\n\t\t\t// extrema.\n\t\t\tchanCapacity := edge.Capacity\n\t\t\tif chanCapacity < minChannelSize {\n\t\t\t\tminChannelSize = chanCapacity\n\t\t\t}\n\t\t\tif chanCapacity > maxChannelSize {\n\t\t\t\tmaxChannelSize = chanCapacity\n\t\t\t}\n\n\t\t\t// Accumulate the total capacity of this channel to the\n\t\t\t// network wide-capacity.\n\t\t\ttotalNetworkCapacity += chanCapacity\n\n\t\t\tnumChannels++\n\n\t\t\tseenChans[edge.ChannelID] = struct{}{}\n\t\t\tallChans = append(allChans, edge.Capacity)\n\t\t}\n\n\t\t// Finally, if the out degree of this node is greater than what\n\t\t// we've seen so far, update the maxChanOut variable.\n\t\tif outDegree > maxChanOut {\n\t\t\tmaxChanOut = outDegree\n\t\t}\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Query the graph for the current number of zombie channels.\n\tnumZombies, err := graph.NumZombies()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Find the median.\n\tmedianChanSize = autopilot.Median(allChans)\n\n\t// If we don't have any channels, then reset the minChannelSize to zero\n\t// to avoid outputting NaN in encoded JSON.\n\tif numChannels == 0 {\n\t\tminChannelSize = 0\n\t}\n\n\t// Graph diameter.\n\tchannelGraph := autopilot.ChannelGraphFromCachedDatabase(graph)\n\tsimpleGraph, err := autopilot.NewSimpleGraph(channelGraph)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tstart := time.Now()\n\tdiameter := simpleGraph.DiameterRadialCutoff()\n\trpcsLog.Infof(\"elapsed time for diameter (%d) calculation: %v\", diameter,\n\t\ttime.Since(start))\n\n\t// TODO(roasbeef): also add oldest channel?\n\tnetInfo := &lnrpc.NetworkInfo{\n\t\tGraphDiameter:        diameter,\n\t\tMaxOutDegree:         maxChanOut,\n\t\tAvgOutDegree:         float64(2*numChannels) / float64(numNodes),\n\t\tNumNodes:             numNodes,\n\t\tNumChannels:          numChannels,\n\t\tTotalNetworkCapacity: int64(totalNetworkCapacity),\n\t\tAvgChannelSize:       float64(totalNetworkCapacity) / float64(numChannels),\n\n\t\tMinChannelSize:       int64(minChannelSize),\n\t\tMaxChannelSize:       int64(maxChannelSize),\n\t\tMedianChannelSizeSat: int64(medianChanSize),\n\t\tNumZombieChans:       numZombies,\n\t}\n\n\t// Similarly, if we don't have any channels, then we'll also set the\n\t// average channel size to zero in order to avoid weird JSON encoding\n\t// outputs.\n\tif numChannels == 0 {\n\t\tnetInfo.AvgChannelSize = 0\n\t}\n\n\treturn netInfo, nil\n}\n\n// StopDaemon will send a shutdown request to the interrupt handler, triggering\n// a graceful shutdown of the daemon.\nfunc (r *rpcServer) StopDaemon(_ context.Context,\n\t_ *lnrpc.StopRequest) (*lnrpc.StopResponse, error) {\n\n\t// Before we even consider a shutdown, are we currently in recovery\n\t// mode? We don't want to allow shutting down during recovery because\n\t// that would mean the user would have to manually continue the rescan\n\t// process next time by using `lncli unlock --recovery_window X`\n\t// otherwise some funds wouldn't be picked up.\n\tisRecoveryMode, progress, err := r.server.cc.Wallet.GetRecoveryInfo()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to get wallet recovery info: %w\",\n\t\t\terr)\n\t}\n\tif isRecoveryMode && progress < 1 {\n\t\treturn nil, fmt.Errorf(\"wallet recovery in progress, cannot \" +\n\t\t\t\"shut down, please wait until rescan finishes\")\n\t}\n\n\tr.interceptor.RequestShutdown()\n\n\treturn &lnrpc.StopResponse{\n\t\tStatus: \"shutdown initiated, check logs for progress\",\n\t}, nil\n}\n\n// SubscribeChannelGraph launches a streaming RPC that allows the caller to\n// receive notifications upon any changes the channel graph topology from the\n// review of the responding node. Events notified include: new nodes coming\n// online, nodes updating their authenticated attributes, new channels being\n// advertised, updates in the routing policy for a directional channel edge,\n// and finally when prior channels are closed on-chain.\nfunc (r *rpcServer) SubscribeChannelGraph(req *lnrpc.GraphTopologySubscription,\n\tupdateStream lnrpc.Lightning_SubscribeChannelGraphServer) error {\n\n\t// First, we start by subscribing to a new intent to receive\n\t// notifications from the channel router.\n\tclient, err := r.server.graphBuilder.SubscribeTopology()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Ensure that the resources for the topology update client is cleaned\n\t// up once either the server, or client exists.\n\tdefer client.Cancel()\n\n\tfor {\n\t\tselect {\n\n\t\t// A new update has been sent by the channel router, we'll\n\t\t// marshal it into the form expected by the gRPC client, then\n\t\t// send it off.\n\t\tcase topChange, ok := <-client.TopologyChanges:\n\t\t\t// If the second value from the channel read is nil,\n\t\t\t// then this means that the channel router is exiting\n\t\t\t// or the notification client was canceled. So we'll\n\t\t\t// exit early.\n\t\t\tif !ok {\n\t\t\t\treturn errors.New(\"server shutting down\")\n\t\t\t}\n\n\t\t\t// Convert the struct from the channel router into the\n\t\t\t// form expected by the gRPC service then send it off\n\t\t\t// to the client.\n\t\t\tgraphUpdate := marshallTopologyChange(topChange)\n\t\t\tif err := updateStream.Send(graphUpdate); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t// The response stream's context for whatever reason has been\n\t\t// closed. If context is closed by an exceeded deadline\n\t\t// we will return an error.\n\t\tcase <-updateStream.Context().Done():\n\t\t\tif errors.Is(updateStream.Context().Err(), context.Canceled) {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn updateStream.Context().Err()\n\n\t\t// The server is quitting, so we'll exit immediately. Returning\n\t\t// nil will close the clients read end of the stream.\n\t\tcase <-r.quit:\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\n// marshallTopologyChange performs a mapping from the topology change struct\n// returned by the router to the form of notifications expected by the current\n// gRPC service.\nfunc marshallTopologyChange(\n\ttopChange *graph.TopologyChange) *lnrpc.GraphTopologyUpdate {\n\n\t// encodeKey is a simple helper function that converts a live public\n\t// key into a hex-encoded version of the compressed serialization for\n\t// the public key.\n\tencodeKey := func(k *btcec.PublicKey) string {\n\t\treturn hex.EncodeToString(k.SerializeCompressed())\n\t}\n\n\tnodeUpdates := make([]*lnrpc.NodeUpdate, len(topChange.NodeUpdates))\n\tfor i, nodeUpdate := range topChange.NodeUpdates {\n\t\tnodeAddrs := make(\n\t\t\t[]*lnrpc.NodeAddress, 0, len(nodeUpdate.Addresses),\n\t\t)\n\t\tfor _, addr := range nodeUpdate.Addresses {\n\t\t\tnodeAddr := &lnrpc.NodeAddress{\n\t\t\t\tNetwork: addr.Network(),\n\t\t\t\tAddr:    addr.String(),\n\t\t\t}\n\t\t\tnodeAddrs = append(nodeAddrs, nodeAddr)\n\t\t}\n\n\t\taddrs := make([]string, len(nodeUpdate.Addresses))\n\t\tfor i, addr := range nodeUpdate.Addresses {\n\t\t\taddrs[i] = addr.String()\n\t\t}\n\n\t\tnodeUpdates[i] = &lnrpc.NodeUpdate{\n\t\t\tAddresses:     addrs,\n\t\t\tNodeAddresses: nodeAddrs,\n\t\t\tIdentityKey:   encodeKey(nodeUpdate.IdentityKey),\n\t\t\tAlias:         nodeUpdate.Alias,\n\t\t\tColor:         nodeUpdate.Color,\n\t\t\tFeatures: invoicesrpc.CreateRPCFeatures(\n\t\t\t\tnodeUpdate.Features,\n\t\t\t),\n\t\t}\n\t}\n\n\tchannelUpdates := make([]*lnrpc.ChannelEdgeUpdate, len(topChange.ChannelEdgeUpdates))\n\tfor i, channelUpdate := range topChange.ChannelEdgeUpdates {\n\n\t\tcustomRecords := marshalExtraOpaqueData(\n\t\t\tchannelUpdate.ExtraOpaqueData,\n\t\t)\n\t\tinboundFee := extractInboundFeeSafe(\n\t\t\tchannelUpdate.ExtraOpaqueData,\n\t\t)\n\n\t\tchannelUpdates[i] = &lnrpc.ChannelEdgeUpdate{\n\t\t\tChanId: channelUpdate.ChanID,\n\t\t\tChanPoint: &lnrpc.ChannelPoint{\n\t\t\t\tFundingTxid: &lnrpc.ChannelPoint_FundingTxidBytes{\n\t\t\t\t\tFundingTxidBytes: channelUpdate.ChanPoint.Hash[:],\n\t\t\t\t},\n\t\t\t\tOutputIndex: channelUpdate.ChanPoint.Index,\n\t\t\t},\n\t\t\tCapacity: int64(channelUpdate.Capacity),\n\t\t\tRoutingPolicy: &lnrpc.RoutingPolicy{\n\t\t\t\tTimeLockDelta: uint32(\n\t\t\t\t\tchannelUpdate.TimeLockDelta,\n\t\t\t\t),\n\t\t\t\tMinHtlc: int64(\n\t\t\t\t\tchannelUpdate.MinHTLC,\n\t\t\t\t),\n\t\t\t\tMaxHtlcMsat: uint64(\n\t\t\t\t\tchannelUpdate.MaxHTLC,\n\t\t\t\t),\n\t\t\t\tFeeBaseMsat: int64(\n\t\t\t\t\tchannelUpdate.BaseFee,\n\t\t\t\t),\n\t\t\t\tFeeRateMilliMsat: int64(\n\t\t\t\t\tchannelUpdate.FeeRate,\n\t\t\t\t),\n\t\t\t\tDisabled:                channelUpdate.Disabled,\n\t\t\t\tInboundFeeBaseMsat:      inboundFee.BaseFee,\n\t\t\t\tInboundFeeRateMilliMsat: inboundFee.FeeRate,\n\t\t\t\tCustomRecords:           customRecords,\n\t\t\t},\n\t\t\tAdvertisingNode: encodeKey(channelUpdate.AdvertisingNode),\n\t\t\tConnectingNode:  encodeKey(channelUpdate.ConnectingNode),\n\t\t}\n\t}\n\n\tclosedChans := make([]*lnrpc.ClosedChannelUpdate, len(topChange.ClosedChannels))\n\tfor i, closedChan := range topChange.ClosedChannels {\n\t\tclosedChans[i] = &lnrpc.ClosedChannelUpdate{\n\t\t\tChanId:       closedChan.ChanID,\n\t\t\tCapacity:     int64(closedChan.Capacity),\n\t\t\tClosedHeight: closedChan.ClosedHeight,\n\t\t\tChanPoint: &lnrpc.ChannelPoint{\n\t\t\t\tFundingTxid: &lnrpc.ChannelPoint_FundingTxidBytes{\n\t\t\t\t\tFundingTxidBytes: closedChan.ChanPoint.Hash[:],\n\t\t\t\t},\n\t\t\t\tOutputIndex: closedChan.ChanPoint.Index,\n\t\t\t},\n\t\t}\n\t}\n\n\treturn &lnrpc.GraphTopologyUpdate{\n\t\tNodeUpdates:    nodeUpdates,\n\t\tChannelUpdates: channelUpdates,\n\t\tClosedChans:    closedChans,\n\t}\n}\n\n// ListPayments returns a list of outgoing payments determined by a paginated\n// database query.\nfunc (r *rpcServer) ListPayments(ctx context.Context,\n\treq *lnrpc.ListPaymentsRequest) (*lnrpc.ListPaymentsResponse, error) {\n\n\t// If both dates are set, we check that the start date is less than the\n\t// end date, otherwise we'll get an empty result.\n\tif req.CreationDateStart != 0 && req.CreationDateEnd != 0 {\n\t\tif req.CreationDateStart >= req.CreationDateEnd {\n\t\t\treturn nil, fmt.Errorf(\"start date(%v) must be before \"+\n\t\t\t\t\"end date(%v)\", req.CreationDateStart,\n\t\t\t\treq.CreationDateEnd)\n\t\t}\n\t}\n\n\tquery := channeldb.PaymentsQuery{\n\t\tIndexOffset:       req.IndexOffset,\n\t\tMaxPayments:       req.MaxPayments,\n\t\tReversed:          req.Reversed,\n\t\tIncludeIncomplete: req.IncludeIncomplete,\n\t\tCountTotal:        req.CountTotalPayments,\n\t\tCreationDateStart: int64(req.CreationDateStart),\n\t\tCreationDateEnd:   int64(req.CreationDateEnd),\n\t}\n\n\t// If the maximum number of payments wasn't specified, then we'll\n\t// default to return the maximal number of payments representable.\n\tif req.MaxPayments == 0 {\n\t\tquery.MaxPayments = math.MaxUint64\n\t}\n\n\tpaymentsQuerySlice, err := r.server.miscDB.QueryPayments(query)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tpaymentsResp := &lnrpc.ListPaymentsResponse{\n\t\tLastIndexOffset:  paymentsQuerySlice.LastIndexOffset,\n\t\tFirstIndexOffset: paymentsQuerySlice.FirstIndexOffset,\n\t\tTotalNumPayments: paymentsQuerySlice.TotalCount,\n\t}\n\n\tfor _, payment := range paymentsQuerySlice.Payments {\n\t\tpayment := payment\n\n\t\trpcPayment, err := r.routerBackend.MarshallPayment(payment)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tpaymentsResp.Payments = append(\n\t\t\tpaymentsResp.Payments, rpcPayment,\n\t\t)\n\t}\n\n\treturn paymentsResp, nil\n}\n\n// DeletePayment deletes a payment from the DB given its payment hash. If\n// failedHtlcsOnly is set, only failed HTLC attempts of the payment will be\n// deleted.\nfunc (r *rpcServer) DeletePayment(ctx context.Context,\n\treq *lnrpc.DeletePaymentRequest) (\n\t*lnrpc.DeletePaymentResponse, error) {\n\n\thash, err := lntypes.MakeHash(req.PaymentHash)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trpcsLog.Infof(\"[DeletePayment] payment_identifier=%v, \"+\n\t\t\"failed_htlcs_only=%v\", hash, req.FailedHtlcsOnly)\n\n\terr = r.server.miscDB.DeletePayment(hash, req.FailedHtlcsOnly)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &lnrpc.DeletePaymentResponse{\n\t\tStatus: \"payment deleted\",\n\t}, nil\n}\n\n// DeleteAllPayments deletes all outgoing payments from DB.\nfunc (r *rpcServer) DeleteAllPayments(ctx context.Context,\n\treq *lnrpc.DeleteAllPaymentsRequest) (\n\t*lnrpc.DeleteAllPaymentsResponse, error) {\n\n\tswitch {\n\t// Since this is a destructive operation, at least one of the options\n\t// must be set to true.\n\tcase !req.AllPayments && !req.FailedPaymentsOnly &&\n\t\t!req.FailedHtlcsOnly:\n\n\t\treturn nil, fmt.Errorf(\"at least one of the options \" +\n\t\t\t\"`all_payments`, `failed_payments_only`, or \" +\n\t\t\t\"`failed_htlcs_only` must be set to true\")\n\n\t// `all_payments` cannot be true with `failed_payments_only` or\n\t// `failed_htlcs_only`. `all_payments` includes all records, making\n\t// these options contradictory.\n\tcase req.AllPayments &&\n\t\t(req.FailedPaymentsOnly || req.FailedHtlcsOnly):\n\n\t\treturn nil, fmt.Errorf(\"`all_payments` cannot be set to true \" +\n\t\t\t\"while either `failed_payments_only` or \" +\n\t\t\t\"`failed_htlcs_only` is also set to true\")\n\t}\n\n\trpcsLog.Infof(\"[DeleteAllPayments] failed_payments_only=%v, \"+\n\t\t\"failed_htlcs_only=%v\", req.FailedPaymentsOnly,\n\t\treq.FailedHtlcsOnly)\n\n\tnumDeletedPayments, err := r.server.miscDB.DeletePayments(\n\t\treq.FailedPaymentsOnly, req.FailedHtlcsOnly,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &lnrpc.DeleteAllPaymentsResponse{\n\t\tStatus: fmt.Sprintf(\"%v payments deleted, failed_htlcs_only=%v\",\n\t\t\tnumDeletedPayments, req.FailedHtlcsOnly),\n\t}, nil\n}\n\n// DebugLevel allows a caller to programmatically set the logging verbosity of\n// lnd. The logging can be targeted according to a coarse daemon-wide logging\n// level, or in a granular fashion to specify the logging for a target\n// sub-system.\nfunc (r *rpcServer) DebugLevel(ctx context.Context,\n\treq *lnrpc.DebugLevelRequest) (*lnrpc.DebugLevelResponse, error) {\n\n\t// If show is set, then we simply print out the list of available\n\t// sub-systems.\n\tif req.Show {\n\t\treturn &lnrpc.DebugLevelResponse{\n\t\t\tSubSystems: strings.Join(\n\t\t\t\tr.cfg.SubLogMgr.SupportedSubsystems(), \" \",\n\t\t\t),\n\t\t}, nil\n\t}\n\n\trpcsLog.Infof(\"[debuglevel] changing debug level to: %v\", req.LevelSpec)\n\n\t// Otherwise, we'll attempt to set the logging level using the\n\t// specified level spec.\n\terr := build.ParseAndSetDebugLevels(req.LevelSpec, r.cfg.SubLogMgr)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tsubLoggers := r.cfg.SubLogMgr.SubLoggers()\n\t// Sort alphabetically by subsystem name.\n\tvar tags []string\n\tfor t := range subLoggers {\n\t\ttags = append(tags, t)\n\t}\n\tsort.Strings(tags)\n\n\t// Create the log levels string.\n\tvar logLevels []string\n\tfor _, t := range tags {\n\t\tlogLevels = append(logLevels, fmt.Sprintf(\"%s=%s\", t,\n\t\t\tsubLoggers[t].Level().String()))\n\t}\n\tlogLevelsString := strings.Join(logLevels, \", \")\n\n\t// Propagate the new config level to the main config struct.\n\tr.cfg.DebugLevel = logLevelsString\n\n\treturn &lnrpc.DebugLevelResponse{\n\t\tSubSystems: logLevelsString,\n\t}, nil\n}\n\n// DecodePayReq takes an encoded payment request string and attempts to decode\n// it, returning a full description of the conditions encoded within the\n// payment request.\nfunc (r *rpcServer) DecodePayReq(ctx context.Context,\n\treq *lnrpc.PayReqString) (*lnrpc.PayReq, error) {\n\n\trpcsLog.Tracef(\"[decodepayreq] decoding: %v\", req.PayReq)\n\n\t// Fist we'll attempt to decode the payment request string, if the\n\t// request is invalid or the checksum doesn't match, then we'll exit\n\t// here with an error.\n\tpayReq, err := zpay32.Decode(req.PayReq, r.cfg.ActiveNetParams.Params)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Let the fields default to empty strings.\n\tdesc := \"\"\n\tif payReq.Description != nil {\n\t\tdesc = *payReq.Description\n\t}\n\n\tdescHash := []byte(\"\")\n\tif payReq.DescriptionHash != nil {\n\t\tdescHash = payReq.DescriptionHash[:]\n\t}\n\n\tfallbackAddr := \"\"\n\tif payReq.FallbackAddr != nil {\n\t\tfallbackAddr = payReq.FallbackAddr.String()\n\t}\n\n\t// Expiry time will default to 3600 seconds if not specified\n\t// explicitly.\n\texpiry := int64(payReq.Expiry().Seconds())\n\n\t// Convert between the `lnrpc` and `routing` types.\n\trouteHints := invoicesrpc.CreateRPCRouteHints(payReq.RouteHints)\n\n\tblindedPaymentPaths, err := invoicesrpc.CreateRPCBlindedPayments(\n\t\tpayReq.BlindedPaymentPaths,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar amtSat, amtMsat int64\n\tif payReq.MilliSat != nil {\n\t\tamtSat = int64(payReq.MilliSat.ToSatoshis())\n\t\tamtMsat = int64(*payReq.MilliSat)\n\t}\n\n\t// Extract the payment address from the payment request, if present.\n\tpaymentAddr := payReq.PaymentAddr.UnwrapOr([32]byte{})\n\n\tdest := payReq.Destination.SerializeCompressed()\n\treturn &lnrpc.PayReq{\n\t\tDestination:     hex.EncodeToString(dest),\n\t\tPaymentHash:     hex.EncodeToString(payReq.PaymentHash[:]),\n\t\tNumSatoshis:     amtSat,\n\t\tNumMsat:         amtMsat,\n\t\tTimestamp:       payReq.Timestamp.Unix(),\n\t\tDescription:     desc,\n\t\tDescriptionHash: hex.EncodeToString(descHash[:]),\n\t\tFallbackAddr:    fallbackAddr,\n\t\tExpiry:          expiry,\n\t\tCltvExpiry:      int64(payReq.MinFinalCLTVExpiry()),\n\t\tRouteHints:      routeHints,\n\t\tBlindedPaths:    blindedPaymentPaths,\n\t\tPaymentAddr:     paymentAddr[:],\n\t\tFeatures:        invoicesrpc.CreateRPCFeatures(payReq.Features),\n\t}, nil\n}\n\n// feeBase is the fixed point that fee rate computation are performed over.\n// Nodes on the network advertise their fee rate using this point as a base.\n// This means that the minimal possible fee rate if 1e-6, or 0.000001, or\n// 0.0001%.\nconst feeBase float64 = 1000000\n\n// FeeReport allows the caller to obtain a report detailing the current fee\n// schedule enforced by the node globally for each channel.\nfunc (r *rpcServer) FeeReport(ctx context.Context,\n\t_ *lnrpc.FeeReportRequest) (*lnrpc.FeeReportResponse, error) {\n\n\tchannelGraph := r.server.graphDB\n\tselfNode, err := channelGraph.SourceNode()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar feeReports []*lnrpc.ChannelFeeReport\n\terr = channelGraph.ForEachNodeChannel(selfNode.PubKeyBytes,\n\t\tfunc(_ kvdb.RTx, chanInfo *models.ChannelEdgeInfo,\n\t\t\tedgePolicy, _ *models.ChannelEdgePolicy) error {\n\n\t\t\t// Self node should always have policies for its\n\t\t\t// channels.\n\t\t\tif edgePolicy == nil {\n\t\t\t\treturn fmt.Errorf(\"no policy for outgoing \"+\n\t\t\t\t\t\"channel %v \", chanInfo.ChannelID)\n\t\t\t}\n\n\t\t\t// We'll compute the effective fee rate by converting\n\t\t\t// from a fixed point fee rate to a floating point fee\n\t\t\t// rate. The fee rate field in the database the amount\n\t\t\t// of mSAT charged per 1mil mSAT sent, so will divide by\n\t\t\t// this to get the proper fee rate.\n\t\t\tfeeRateFixedPoint :=\n\t\t\t\tedgePolicy.FeeProportionalMillionths\n\t\t\tfeeRate := float64(feeRateFixedPoint) / feeBase\n\n\t\t\t// Decode inbound fee from extra data.\n\t\t\tvar inboundFee lnwire.Fee\n\t\t\t_, err := edgePolicy.ExtraOpaqueData.ExtractRecords(\n\t\t\t\t&inboundFee,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t// TODO(roasbeef): also add stats for revenue for each\n\t\t\t// channel\n\t\t\tfeeReports = append(feeReports, &lnrpc.ChannelFeeReport{\n\t\t\t\tChanId:       chanInfo.ChannelID,\n\t\t\t\tChannelPoint: chanInfo.ChannelPoint.String(),\n\t\t\t\tBaseFeeMsat:  int64(edgePolicy.FeeBaseMSat),\n\t\t\t\tFeePerMil:    int64(feeRateFixedPoint),\n\t\t\t\tFeeRate:      feeRate,\n\n\t\t\t\tInboundBaseFeeMsat: inboundFee.BaseFee,\n\t\t\t\tInboundFeePerMil:   inboundFee.FeeRate,\n\t\t\t})\n\n\t\t\treturn nil\n\t\t},\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfwdEventLog := r.server.miscDB.ForwardingLog()\n\n\t// computeFeeSum is a helper function that computes the total fees for\n\t// a particular time slice described by a forwarding event query.\n\tcomputeFeeSum := func(query channeldb.ForwardingEventQuery) (lnwire.MilliSatoshi, error) {\n\n\t\tvar totalFees lnwire.MilliSatoshi\n\n\t\t// We'll continue to fetch the next query and accumulate the\n\t\t// fees until the next query returns no events.\n\t\tfor {\n\t\t\ttimeSlice, err := fwdEventLog.Query(query)\n\t\t\tif err != nil {\n\t\t\t\treturn 0, err\n\t\t\t}\n\n\t\t\t// If the timeslice is empty, then we'll return as\n\t\t\t// we've retrieved all the entries in this range.\n\t\t\tif len(timeSlice.ForwardingEvents) == 0 {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// Otherwise, we'll tally up an accumulate the total\n\t\t\t// fees for this time slice.\n\t\t\tfor _, event := range timeSlice.ForwardingEvents {\n\t\t\t\tfee := event.AmtIn - event.AmtOut\n\t\t\t\ttotalFees += fee\n\t\t\t}\n\n\t\t\t// We'll now take the last offset index returned as\n\t\t\t// part of this response, and modify our query to start\n\t\t\t// at this index. This has a pagination effect in the\n\t\t\t// case that our query bounds has more than 100k\n\t\t\t// entries.\n\t\t\tquery.IndexOffset = timeSlice.LastIndexOffset\n\t\t}\n\n\t\treturn totalFees, nil\n\t}\n\n\tnow := time.Now()\n\n\t// Before we perform the queries below, we'll instruct the switch to\n\t// flush any pending events to disk. This ensure we get a complete\n\t// snapshot at this particular time.\n\tif err := r.server.htlcSwitch.FlushForwardingEvents(); err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to flush forwarding \"+\n\t\t\t\"events: %v\", err)\n\t}\n\n\t// In addition to returning the current fee schedule for each channel.\n\t// We'll also perform a series of queries to obtain the total fees\n\t// earned over the past day, week, and month.\n\tdayQuery := channeldb.ForwardingEventQuery{\n\t\tStartTime:    now.Add(-time.Hour * 24),\n\t\tEndTime:      now,\n\t\tNumMaxEvents: 1000,\n\t}\n\tdayFees, err := computeFeeSum(dayQuery)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to retrieve day fees: %w\", err)\n\t}\n\n\tweekQuery := channeldb.ForwardingEventQuery{\n\t\tStartTime:    now.Add(-time.Hour * 24 * 7),\n\t\tEndTime:      now,\n\t\tNumMaxEvents: 1000,\n\t}\n\tweekFees, err := computeFeeSum(weekQuery)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to retrieve day fees: %w\", err)\n\t}\n\n\tmonthQuery := channeldb.ForwardingEventQuery{\n\t\tStartTime:    now.Add(-time.Hour * 24 * 30),\n\t\tEndTime:      now,\n\t\tNumMaxEvents: 1000,\n\t}\n\tmonthFees, err := computeFeeSum(monthQuery)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to retrieve day fees: %w\", err)\n\t}\n\n\treturn &lnrpc.FeeReportResponse{\n\t\tChannelFees: feeReports,\n\t\tDayFeeSum:   uint64(dayFees.ToSatoshis()),\n\t\tWeekFeeSum:  uint64(weekFees.ToSatoshis()),\n\t\tMonthFeeSum: uint64(monthFees.ToSatoshis()),\n\t}, nil\n}\n\n// minFeeRate is the smallest permitted fee rate within the network. This is\n// derived by the fact that fee rates are computed using a fixed point of\n// 1,000,000. As a result, the smallest representable fee rate is 1e-6, or\n// 0.000001, or 0.0001%.\nconst minFeeRate = 1e-6\n\n// UpdateChannelPolicy allows the caller to update the channel forwarding policy\n// for all channels globally, or a particular channel.\nfunc (r *rpcServer) UpdateChannelPolicy(ctx context.Context,\n\treq *lnrpc.PolicyUpdateRequest) (*lnrpc.PolicyUpdateResponse, error) {\n\n\tvar targetChans []wire.OutPoint\n\tswitch scope := req.Scope.(type) {\n\t// If the request is targeting all active channels, then we don't need\n\t// target any channels by their channel point.\n\tcase *lnrpc.PolicyUpdateRequest_Global:\n\n\t// Otherwise, we're targeting an individual channel by its channel\n\t// point.\n\tcase *lnrpc.PolicyUpdateRequest_ChanPoint:\n\t\ttxid, err := lnrpc.GetChanPointFundingTxid(scope.ChanPoint)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\ttargetChans = append(targetChans, wire.OutPoint{\n\t\t\tHash:  *txid,\n\t\t\tIndex: scope.ChanPoint.OutputIndex,\n\t\t})\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown scope: %v\", scope)\n\t}\n\n\tvar feeRateFixed uint32\n\n\tswitch {\n\t// The request should use either the fee rate in percent, or the new\n\t// ppm rate, but not both.\n\tcase req.FeeRate != 0 && req.FeeRatePpm != 0:\n\t\terrMsg := \"cannot set both FeeRate and FeeRatePpm at the \" +\n\t\t\t\"same time\"\n\n\t\treturn nil, status.Errorf(codes.InvalidArgument, errMsg)\n\n\t// If the request is using fee_rate.\n\tcase req.FeeRate != 0:\n\t\t// As a sanity check, if the fee isn't zero, we'll ensure that\n\t\t// the passed fee rate is below 1e-6, or the lowest allowed\n\t\t// non-zero fee rate expressible within the protocol.\n\t\tif req.FeeRate != 0 && req.FeeRate < minFeeRate {\n\t\t\treturn nil, fmt.Errorf(\"fee rate of %v is too \"+\n\t\t\t\t\"small, min fee rate is %v\", req.FeeRate,\n\t\t\t\tminFeeRate)\n\t\t}\n\n\t\t// We'll also need to convert the floating point fee rate we\n\t\t// accept over RPC to the fixed point rate that we use within\n\t\t// the protocol. We do this by multiplying the passed fee rate\n\t\t// by the fee base. This gives us the fixed point, scaled by 1\n\t\t// million that's used within the protocol.\n\t\t//\n\t\t// Because of the inaccurate precision of the IEEE 754\n\t\t// standard, we need to round the product of feerate and\n\t\t// feebase.\n\t\tfeeRateFixed = uint32(math.Round(req.FeeRate * feeBase))\n\n\t// Otherwise, we use the fee_rate_ppm parameter.\n\tcase req.FeeRatePpm != 0:\n\t\tfeeRateFixed = req.FeeRatePpm\n\t}\n\n\t// We'll also ensure that the user isn't setting a CLTV delta that\n\t// won't give outgoing HTLCs enough time to fully resolve if needed.\n\tif req.TimeLockDelta < minTimeLockDelta {\n\t\treturn nil, fmt.Errorf(\"time lock delta of %v is too small, \"+\n\t\t\t\"minimum supported is %v\", req.TimeLockDelta,\n\t\t\tminTimeLockDelta)\n\t} else if req.TimeLockDelta > uint32(MaxTimeLockDelta) {\n\t\treturn nil, fmt.Errorf(\"time lock delta of %v is too big, \"+\n\t\t\t\"maximum supported is %v\", req.TimeLockDelta,\n\t\t\tMaxTimeLockDelta)\n\t}\n\n\t// By default, positive inbound fees are rejected.\n\tif !r.cfg.AcceptPositiveInboundFees && req.InboundFee != nil {\n\t\tif req.InboundFee.BaseFeeMsat > 0 {\n\t\t\treturn nil, fmt.Errorf(\"positive values for inbound \"+\n\t\t\t\t\"base fee msat are not supported: %v\",\n\t\t\t\treq.InboundFee.BaseFeeMsat)\n\t\t}\n\t\tif req.InboundFee.FeeRatePpm > 0 {\n\t\t\treturn nil, fmt.Errorf(\"positive values for inbound \"+\n\t\t\t\t\"fee rate ppm are not supported: %v\",\n\t\t\t\treq.InboundFee.FeeRatePpm)\n\t\t}\n\t}\n\n\t// If no inbound fees have been specified, we indicate with an empty\n\t// option that the previous inbound fee should be retained during the\n\t// edge update.\n\tinboundFee := fn.None[models.InboundFee]()\n\tif req.InboundFee != nil {\n\t\tinboundFee = fn.Some(models.InboundFee{\n\t\t\tBase: req.InboundFee.BaseFeeMsat,\n\t\t\tRate: req.InboundFee.FeeRatePpm,\n\t\t})\n\t}\n\n\tbaseFeeMsat := lnwire.MilliSatoshi(req.BaseFeeMsat)\n\tfeeSchema := routing.FeeSchema{\n\t\tBaseFee:    baseFeeMsat,\n\t\tFeeRate:    feeRateFixed,\n\t\tInboundFee: inboundFee,\n\t}\n\n\tmaxHtlc := lnwire.MilliSatoshi(req.MaxHtlcMsat)\n\tvar minHtlc *lnwire.MilliSatoshi\n\tif req.MinHtlcMsatSpecified {\n\t\tmin := lnwire.MilliSatoshi(req.MinHtlcMsat)\n\t\tminHtlc = &min\n\t}\n\n\tchanPolicy := routing.ChannelPolicy{\n\t\tFeeSchema:     feeSchema,\n\t\tTimeLockDelta: req.TimeLockDelta,\n\t\tMaxHTLC:       maxHtlc,\n\t\tMinHTLC:       minHtlc,\n\t}\n\n\trpcsLog.Debugf(\"[updatechanpolicy] updating channel policy \"+\n\t\t\"base_fee=%v, rate_fixed=%v, time_lock_delta: %v, \"+\n\t\t\"min_htlc=%v, max_htlc=%v, targets=%v\",\n\t\treq.BaseFeeMsat, feeRateFixed, req.TimeLockDelta,\n\t\tminHtlc, maxHtlc,\n\t\tspew.Sdump(targetChans))\n\n\t// With the scope resolved, we'll now send this to the local channel\n\t// manager so it can propagate the new policy for our target channel(s).\n\tfailedUpdates, err := r.server.localChanMgr.UpdatePolicy(chanPolicy,\n\t\treq.CreateMissingEdge, targetChans...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &lnrpc.PolicyUpdateResponse{\n\t\tFailedUpdates: failedUpdates,\n\t}, nil\n}\n\n// ForwardingHistory allows the caller to query the htlcswitch for a record of\n// all HTLC's forwarded within the target time range, and integer offset within\n// that time range. If no time-range is specified, then the first chunk of the\n// past 24 hrs of forwarding history are returned.\n\n// A list of forwarding events are returned. The size of each forwarding event\n// is 40 bytes, and the max message size able to be returned in gRPC is 4 MiB.\n// In order to safely stay under this max limit, we'll return 50k events per\n// response.  Each response has the index offset of the last entry. The index\n// offset can be provided to the request to allow the caller to skip a series\n// of records.\nfunc (r *rpcServer) ForwardingHistory(ctx context.Context,\n\treq *lnrpc.ForwardingHistoryRequest) (*lnrpc.ForwardingHistoryResponse,\n\terror) {\n\n\t// Before we perform the queries below, we'll instruct the switch to\n\t// flush any pending events to disk. This ensure we get a complete\n\t// snapshot at this particular time.\n\tif err := r.server.htlcSwitch.FlushForwardingEvents(); err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to flush forwarding \"+\n\t\t\t\"events: %v\", err)\n\t}\n\n\tvar (\n\t\tstartTime, endTime time.Time\n\n\t\tnumEvents uint32\n\t)\n\n\t// startTime defaults to the Unix epoch (0 unixtime, or\n\t// midnight 01-01-1970).\n\tstartTime = time.Unix(int64(req.StartTime), 0)\n\n\t// If the end time wasn't specified, assume a default end time of now.\n\tif req.EndTime == 0 {\n\t\tnow := time.Now()\n\t\tendTime = now\n\t} else {\n\t\tendTime = time.Unix(int64(req.EndTime), 0)\n\t}\n\n\t// If the number of events wasn't specified, then we'll default to\n\t// returning the last 100 events.\n\tnumEvents = req.NumMaxEvents\n\tif numEvents == 0 {\n\t\tnumEvents = 100\n\t}\n\n\t// Next, we'll map the proto request into a format that is understood by\n\t// the forwarding log.\n\teventQuery := channeldb.ForwardingEventQuery{\n\t\tStartTime:    startTime,\n\t\tEndTime:      endTime,\n\t\tIndexOffset:  req.IndexOffset,\n\t\tNumMaxEvents: numEvents,\n\t}\n\ttimeSlice, err := r.server.miscDB.ForwardingLog().Query(eventQuery)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to query forwarding log: %w\",\n\t\t\terr)\n\t}\n\n\t// chanToPeerAlias caches previously looked up channel information.\n\tchanToPeerAlias := make(map[lnwire.ShortChannelID]string)\n\n\t// Helper function to extract a peer's node alias given its SCID.\n\tgetRemoteAlias := func(chanID lnwire.ShortChannelID) (string, error) {\n\t\t// If we'd previously seen this chanID then return the cached\n\t\t// peer alias.\n\t\tif peerAlias, ok := chanToPeerAlias[chanID]; ok {\n\t\t\treturn peerAlias, nil\n\t\t}\n\n\t\t// Else call the server to look up the peer alias.\n\t\tedge, err := r.GetChanInfo(ctx, &lnrpc.ChanInfoRequest{\n\t\t\tChanId: chanID.ToUint64(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\n\t\tremotePub := edge.Node1Pub\n\t\tif r.selfNode.String() == edge.Node1Pub {\n\t\t\tremotePub = edge.Node2Pub\n\t\t}\n\n\t\tvertex, err := route.NewVertexFromStr(remotePub)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\n\t\tpeer, err := r.server.graphDB.FetchLightningNode(vertex)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\n\t\t// Cache the peer alias.\n\t\tchanToPeerAlias[chanID] = peer.Alias\n\n\t\treturn peer.Alias, nil\n\t}\n\n\t// TODO(roasbeef): add settlement latency?\n\t//  * use FPE on all records?\n\n\t// With the events retrieved, we'll now map them into the proper proto\n\t// response.\n\t//\n\t// TODO(roasbeef): show in ns for the outside?\n\tfwdingEvents := make(\n\t\t[]*lnrpc.ForwardingEvent, len(timeSlice.ForwardingEvents),\n\t)\n\tresp := &lnrpc.ForwardingHistoryResponse{\n\t\tForwardingEvents: fwdingEvents,\n\t\tLastOffsetIndex:  timeSlice.LastIndexOffset,\n\t}\n\tfor i, event := range timeSlice.ForwardingEvents {\n\t\tamtInMsat := event.AmtIn\n\t\tamtOutMsat := event.AmtOut\n\t\tfeeMsat := event.AmtIn - event.AmtOut\n\n\t\tresp.ForwardingEvents[i] = &lnrpc.ForwardingEvent{\n\t\t\tTimestamp:   uint64(event.Timestamp.Unix()),\n\t\t\tTimestampNs: uint64(event.Timestamp.UnixNano()),\n\t\t\tChanIdIn:    event.IncomingChanID.ToUint64(),\n\t\t\tChanIdOut:   event.OutgoingChanID.ToUint64(),\n\t\t\tAmtIn:       uint64(amtInMsat.ToSatoshis()),\n\t\t\tAmtOut:      uint64(amtOutMsat.ToSatoshis()),\n\t\t\tFee:         uint64(feeMsat.ToSatoshis()),\n\t\t\tFeeMsat:     uint64(feeMsat),\n\t\t\tAmtInMsat:   uint64(amtInMsat),\n\t\t\tAmtOutMsat:  uint64(amtOutMsat),\n\t\t}\n\n\t\tif req.PeerAliasLookup {\n\t\t\taliasIn, err := getRemoteAlias(event.IncomingChanID)\n\t\t\tif err != nil {\n\t\t\t\taliasIn = fmt.Sprintf(\"unable to lookup peer \"+\n\t\t\t\t\t\"alias: %v\", err)\n\t\t\t}\n\t\t\taliasOut, err := getRemoteAlias(event.OutgoingChanID)\n\t\t\tif err != nil {\n\t\t\t\taliasOut = fmt.Sprintf(\"unable to lookup peer\"+\n\t\t\t\t\t\"alias: %v\", err)\n\t\t\t}\n\t\t\tresp.ForwardingEvents[i].PeerAliasIn = aliasIn\n\t\t\tresp.ForwardingEvents[i].PeerAliasOut = aliasOut\n\t\t}\n\t}\n\n\treturn resp, nil\n}\n\n// ExportChannelBackup attempts to return an encrypted static channel backup\n// for the target channel identified by it channel point. The backup is\n// encrypted with a key generated from the aezeed seed of the user. The\n// returned backup can either be restored using the RestoreChannelBackup method\n// once lnd is running, or via the InitWallet and UnlockWallet methods from the\n// WalletUnlocker service.\nfunc (r *rpcServer) ExportChannelBackup(ctx context.Context,\n\tin *lnrpc.ExportChannelBackupRequest) (*lnrpc.ChannelBackup, error) {\n\n\t// First, we'll convert the lnrpc channel point into a wire.OutPoint\n\t// that we can manipulate.\n\ttxid, err := lnrpc.GetChanPointFundingTxid(in.ChanPoint)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tchanPoint := wire.OutPoint{\n\t\tHash:  *txid,\n\t\tIndex: in.ChanPoint.OutputIndex,\n\t}\n\n\t// Next, we'll attempt to fetch a channel backup for this channel from\n\t// the database. If this channel has been closed, or the outpoint is\n\t// unknown, then we'll return an error\n\tunpackedBackup, err := chanbackup.FetchBackupForChan(\n\t\tchanPoint, r.server.chanStateDB, r.server.addrSource,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// At this point, we have an unpacked backup (plaintext) so we'll now\n\t// attempt to serialize and encrypt it in order to create a packed\n\t// backup.\n\tpackedBackups, err := chanbackup.PackStaticChanBackups(\n\t\t[]chanbackup.Single{*unpackedBackup},\n\t\tr.server.cc.KeyRing,\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"packing of back ups failed: %w\", err)\n\t}\n\n\t// Before we proceed, we'll ensure that we received a backup for this\n\t// channel, otherwise, we'll bail out.\n\tpackedBackup, ok := packedBackups[chanPoint]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"expected single backup for \"+\n\t\t\t\"ChannelPoint(%v), got %v\", chanPoint,\n\t\t\tlen(packedBackup))\n\t}\n\n\treturn &lnrpc.ChannelBackup{\n\t\tChanPoint:  in.ChanPoint,\n\t\tChanBackup: packedBackup,\n\t}, nil\n}\n\n// VerifyChanBackup allows a caller to verify the integrity of a channel backup\n// snapshot. This method will accept both either a packed Single or a packed\n// Multi. Specifying both will result in an error.\nfunc (r *rpcServer) VerifyChanBackup(ctx context.Context,\n\tin *lnrpc.ChanBackupSnapshot) (*lnrpc.VerifyChanBackupResponse, error) {\n\n\tvar (\n\t\tchannels []chanbackup.Single\n\t\terr      error\n\t)\n\tswitch {\n\t// If neither a Single or Multi has been specified, then we have nothing\n\t// to verify.\n\tcase in.GetSingleChanBackups() == nil && in.GetMultiChanBackup() == nil:\n\t\treturn nil, errors.New(\"either a Single or Multi channel \" +\n\t\t\t\"backup must be specified\")\n\n\t// Either a Single or a Multi must be specified, but not both.\n\tcase in.GetSingleChanBackups() != nil && in.GetMultiChanBackup() != nil:\n\t\treturn nil, errors.New(\"either a Single or Multi channel \" +\n\t\t\t\"backup must be specified, but not both\")\n\n\t// If a Single is specified then we'll only accept one of them to allow\n\t// the caller to map the valid/invalid state for each individual Single.\n\tcase in.GetSingleChanBackups() != nil:\n\t\tchanBackupsProtos := in.GetSingleChanBackups().ChanBackups\n\t\tif len(chanBackupsProtos) != 1 {\n\t\t\treturn nil, errors.New(\"only one Single is accepted \" +\n\t\t\t\t\"at a time\")\n\t\t}\n\n\t\t// First, we'll convert the raw byte slice into a type we can\n\t\t// work with a bit better.\n\t\tchanBackup := chanbackup.PackedSingles(\n\t\t\t[][]byte{chanBackupsProtos[0].ChanBackup},\n\t\t)\n\n\t\t// With our PackedSingles created, we'll attempt to unpack the\n\t\t// backup. If this fails, then we know the backup is invalid for\n\t\t// some reason.\n\t\tchannels, err = chanBackup.Unpack(r.server.cc.KeyRing)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"invalid single channel \"+\n\t\t\t\t\"backup: %v\", err)\n\t\t}\n\n\tcase in.GetMultiChanBackup() != nil:\n\t\t// We'll convert the raw byte slice into a PackedMulti that we\n\t\t// can easily work with.\n\t\tpackedMultiBackup := in.GetMultiChanBackup().MultiChanBackup\n\t\tpackedMulti := chanbackup.PackedMulti(packedMultiBackup)\n\n\t\t// We'll now attempt to unpack the Multi. If this fails, then we\n\t\t// know it's invalid.\n\t\tmulti, err := packedMulti.Unpack(r.server.cc.KeyRing)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"invalid multi channel backup: \"+\n\t\t\t\t\"%v\", err)\n\t\t}\n\n\t\tchannels = multi.StaticBackups\n\t}\n\n\treturn &lnrpc.VerifyChanBackupResponse{\n\t\tChanPoints: fn.Map(channels, func(c chanbackup.Single) string {\n\t\t\treturn c.FundingOutpoint.String()\n\t\t}),\n\t}, nil\n}\n\n// createBackupSnapshot converts the passed Single backup into a snapshot which\n// contains individual packed single backups, as well as a single packed multi\n// backup.\nfunc (r *rpcServer) createBackupSnapshot(backups []chanbackup.Single) (\n\t*lnrpc.ChanBackupSnapshot, error) {\n\n\t// Once we have the set of back ups, we'll attempt to pack them all\n\t// into a series of single channel backups.\n\tsingleChanPackedBackups, err := chanbackup.PackStaticChanBackups(\n\t\tbackups, r.server.cc.KeyRing,\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to pack set of chan \"+\n\t\t\t\"backups: %v\", err)\n\t}\n\n\t// Now that we have our set of single packed backups, we'll morph that\n\t// into a form that the proto response requires.\n\tnumBackups := len(singleChanPackedBackups)\n\tsingleBackupResp := &lnrpc.ChannelBackups{\n\t\tChanBackups: make([]*lnrpc.ChannelBackup, 0, numBackups),\n\t}\n\tfor chanPoint, singlePackedBackup := range singleChanPackedBackups {\n\t\ttxid := chanPoint.Hash\n\t\trpcChanPoint := &lnrpc.ChannelPoint{\n\t\t\tFundingTxid: &lnrpc.ChannelPoint_FundingTxidBytes{\n\t\t\t\tFundingTxidBytes: txid[:],\n\t\t\t},\n\t\t\tOutputIndex: chanPoint.Index,\n\t\t}\n\n\t\tsingleBackupResp.ChanBackups = append(\n\t\t\tsingleBackupResp.ChanBackups,\n\t\t\t&lnrpc.ChannelBackup{\n\t\t\t\tChanPoint:  rpcChanPoint,\n\t\t\t\tChanBackup: singlePackedBackup,\n\t\t\t},\n\t\t)\n\t}\n\n\t// In addition, to the set of single chan backups, we'll also create a\n\t// single multi-channel backup which can be serialized into a single\n\t// file for safe storage.\n\tvar b bytes.Buffer\n\tunpackedMultiBackup := chanbackup.Multi{\n\t\tStaticBackups: backups,\n\t}\n\terr = unpackedMultiBackup.PackToWriter(&b, r.server.cc.KeyRing)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to multi-pack backups: %w\", err)\n\t}\n\n\tmultiBackupResp := &lnrpc.MultiChanBackup{\n\t\tMultiChanBackup: b.Bytes(),\n\t}\n\tfor _, singleBackup := range singleBackupResp.ChanBackups {\n\t\tmultiBackupResp.ChanPoints = append(\n\t\t\tmultiBackupResp.ChanPoints, singleBackup.ChanPoint,\n\t\t)\n\t}\n\n\treturn &lnrpc.ChanBackupSnapshot{\n\t\tSingleChanBackups: singleBackupResp,\n\t\tMultiChanBackup:   multiBackupResp,\n\t}, nil\n}\n\n// ExportAllChannelBackups returns static channel backups for all existing\n// channels known to lnd. A set of regular singular static channel backups for\n// each channel are returned. Additionally, a multi-channel backup is returned\n// as well, which contains a single encrypted blob containing the backups of\n// each channel.\nfunc (r *rpcServer) ExportAllChannelBackups(ctx context.Context,\n\tin *lnrpc.ChanBackupExportRequest) (*lnrpc.ChanBackupSnapshot, error) {\n\n\t// First, we'll attempt to read back ups for ALL currently opened\n\t// channels from disk.\n\tallUnpackedBackups, err := chanbackup.FetchStaticChanBackups(\n\t\tr.server.chanStateDB, r.server.addrSource,\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to fetch all static chan \"+\n\t\t\t\"backups: %v\", err)\n\t}\n\n\t// With the backups assembled, we'll create a full snapshot.\n\treturn r.createBackupSnapshot(allUnpackedBackups)\n}\n\n// RestoreChannelBackups accepts a set of singular channel backups, or a single\n// encrypted multi-chan backup and attempts to recover any funds remaining\n// within the channel. If we're able to unpack the backup, then the new channel\n// will be shown under listchannels, as well as pending channels.\nfunc (r *rpcServer) RestoreChannelBackups(ctx context.Context,\n\tin *lnrpc.RestoreChanBackupRequest) (*lnrpc.RestoreBackupResponse, error) {\n\n\t// The server hasn't yet started, so it won't be able to service any of\n\t// our requests, so we'll bail early here.\n\tif !r.server.Started() {\n\t\treturn nil, ErrServerNotActive\n\t}\n\n\t// First, we'll make our implementation of the\n\t// chanbackup.ChannelRestorer interface which we'll use to properly\n\t// restore either a set of chanbackup.Single or chanbackup.Multi\n\t// backups.\n\tchanRestorer := &chanDBRestorer{\n\t\tdb:         r.server.chanStateDB,\n\t\tsecretKeys: r.server.cc.KeyRing,\n\t\tchainArb:   r.server.chainArb,\n\t}\n\n\t// We'll accept either a list of Single backups, or a single Multi\n\t// backup which contains several single backups.\n\tvar (\n\t\tnumRestored int\n\t\terr         error\n\t)\n\tswitch {\n\tcase in.GetChanBackups() != nil:\n\t\tchanBackupsProtos := in.GetChanBackups()\n\n\t\t// Now that we know what type of backup we're working with,\n\t\t// we'll parse them all out into a more suitable format.\n\t\tpackedBackups := make([][]byte, 0, len(chanBackupsProtos.ChanBackups))\n\t\tfor _, chanBackup := range chanBackupsProtos.ChanBackups {\n\t\t\tpackedBackups = append(\n\t\t\t\tpackedBackups, chanBackup.ChanBackup,\n\t\t\t)\n\t\t}\n\n\t\t// With our backups obtained, we'll now restore them which will\n\t\t// write the new backups to disk, and then attempt to connect\n\t\t// out to any peers that we know of which were our prior\n\t\t// channel peers.\n\t\tnumRestored, err = chanbackup.UnpackAndRecoverSingles(\n\t\t\tchanbackup.PackedSingles(packedBackups),\n\t\t\tr.server.cc.KeyRing, chanRestorer, r.server,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to unpack single \"+\n\t\t\t\t\"backups: %v\", err)\n\t\t}\n\n\tcase in.GetMultiChanBackup() != nil:\n\t\tpackedMultiBackup := in.GetMultiChanBackup()\n\n\t\t// With our backups obtained, we'll now restore them which will\n\t\t// write the new backups to disk, and then attempt to connect\n\t\t// out to any peers that we know of which were our prior\n\t\t// channel peers.\n\t\tpackedMulti := chanbackup.PackedMulti(packedMultiBackup)\n\t\tnumRestored, err = chanbackup.UnpackAndRecoverMulti(\n\t\t\tpackedMulti, r.server.cc.KeyRing, chanRestorer,\n\t\t\tr.server,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to unpack chan \"+\n\t\t\t\t\"backup: %v\", err)\n\t\t}\n\t}\n\n\treturn &lnrpc.RestoreBackupResponse{\n\t\tNumRestored: uint32(numRestored),\n\t}, nil\n}\n\n// SubscribeChannelBackups allows a client to sub-subscribe to the most up to\n// date information concerning the state of all channel back ups. Each time a\n// new channel is added, we return the new set of channels, along with a\n// multi-chan backup containing the backup info for all channels. Each time a\n// channel is closed, we send a new update, which contains new new chan back\n// ups, but the updated set of encrypted multi-chan backups with the closed\n// channel(s) removed.\nfunc (r *rpcServer) SubscribeChannelBackups(req *lnrpc.ChannelBackupSubscription,\n\tupdateStream lnrpc.Lightning_SubscribeChannelBackupsServer) error {\n\n\t// First, we'll subscribe to the primary channel notifier so we can\n\t// obtain events for new pending/opened/closed channels.\n\tchanSubscription, err := r.server.channelNotifier.SubscribeChannelEvents()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdefer chanSubscription.Cancel()\n\tfor {\n\t\tselect {\n\t\t// A new event has been sent by the channel notifier, we'll\n\t\t// assemble, then sling out a new event to the client.\n\t\tcase e := <-chanSubscription.Updates():\n\t\t\t// TODO(roasbeef): batch dispatch ntnfs\n\n\t\t\tswitch e.(type) {\n\n\t\t\t// We only care about new/closed channels, so we'll\n\t\t\t// skip any events for active/inactive channels.\n\t\t\t// To make the subscription behave the same way as the\n\t\t\t// synchronous call and the file based backup, we also\n\t\t\t// include pending channels in the update.\n\t\t\tcase channelnotifier.ActiveChannelEvent:\n\t\t\t\tcontinue\n\t\t\tcase channelnotifier.InactiveChannelEvent:\n\t\t\t\tcontinue\n\t\t\tcase channelnotifier.ActiveLinkEvent:\n\t\t\t\tcontinue\n\t\t\tcase channelnotifier.InactiveLinkEvent:\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Now that we know the channel state has changed,\n\t\t\t// we'll obtains the current set of single channel\n\t\t\t// backups from disk.\n\t\t\tchanBackups, err := chanbackup.FetchStaticChanBackups(\n\t\t\t\tr.server.chanStateDB, r.server.addrSource,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"unable to fetch all \"+\n\t\t\t\t\t\"static chan backups: %v\", err)\n\t\t\t}\n\n\t\t\t// With our backups obtained, we'll pack them into a\n\t\t\t// snapshot and send them back to the client.\n\t\t\tbackupSnapshot, err := r.createBackupSnapshot(\n\t\t\t\tchanBackups,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\terr = updateStream.Send(backupSnapshot)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t// The response stream's context for whatever reason has been\n\t\t// closed. If context is closed by an exceeded deadline we will\n\t\t// return an error.\n\t\tcase <-updateStream.Context().Done():\n\t\t\tif errors.Is(updateStream.Context().Err(), context.Canceled) {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn updateStream.Context().Err()\n\n\t\tcase <-r.quit:\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\n// ChannelAcceptor dispatches a bi-directional streaming RPC in which\n// OpenChannel requests are sent to the client and the client responds with\n// a boolean that tells LND whether or not to accept the channel. This allows\n// node operators to specify their own criteria for accepting inbound channels\n// through a single persistent connection.\nfunc (r *rpcServer) ChannelAcceptor(stream lnrpc.Lightning_ChannelAcceptorServer) error {\n\tchainedAcceptor := r.chanPredicate\n\n\t// Create a new RPCAcceptor which will send requests into the\n\t// newRequests channel when it receives them.\n\trpcAcceptor := chanacceptor.NewRPCAcceptor(\n\t\tstream.Recv, stream.Send, r.cfg.AcceptorTimeout,\n\t\tr.cfg.ActiveNetParams.Params, r.quit,\n\t)\n\n\t// Add the RPCAcceptor to the ChainedAcceptor and defer its removal.\n\tid := chainedAcceptor.AddAcceptor(rpcAcceptor)\n\tdefer chainedAcceptor.RemoveAcceptor(id)\n\n\t// Run the rpc acceptor, which will accept requests for channel\n\t// acceptance decisions from our chained acceptor, send them to the\n\t// channel acceptor and listen for and report responses. This function\n\t// blocks, and will exit if the rpcserver receives the instruction to\n\t// shutdown, or the client cancels.\n\treturn rpcAcceptor.Run()\n}\n\n// BakeMacaroon allows the creation of a new macaroon with custom read and write\n// permissions. No first-party caveats are added since this can be done offline.\n// If the --allow-external-permissions flag is set, the RPC will allow\n// external permissions that LND is not aware of.\nfunc (r *rpcServer) BakeMacaroon(ctx context.Context,\n\treq *lnrpc.BakeMacaroonRequest) (*lnrpc.BakeMacaroonResponse, error) {\n\n\t// If the --no-macaroons flag is used to start lnd, the macaroon service\n\t// is not initialized. Therefore we can't bake new macaroons.\n\tif r.macService == nil {\n\t\treturn nil, errMacaroonDisabled\n\t}\n\n\thelpMsg := fmt.Sprintf(\"supported actions are %v, supported entities \"+\n\t\t\"are %v\", validActions, validEntities)\n\n\t// Don't allow empty permission list as it doesn't make sense to have\n\t// a macaroon that is not allowed to access any RPC.\n\tif len(req.Permissions) == 0 {\n\t\treturn nil, fmt.Errorf(\"permission list cannot be empty. \"+\n\t\t\t\"specify at least one action/entity pair. %s\", helpMsg)\n\t}\n\n\t// Validate and map permission struct used by gRPC to the one used by\n\t// the bakery. If the --allow-external-permissions flag is set, we\n\t// will not validate, but map.\n\trequestedPermissions := make([]bakery.Op, len(req.Permissions))\n\tfor idx, op := range req.Permissions {\n\t\tif req.AllowExternalPermissions {\n\t\t\trequestedPermissions[idx] = bakery.Op{\n\t\t\t\tEntity: op.Entity,\n\t\t\t\tAction: op.Action,\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\tif !stringInSlice(op.Entity, validEntities) {\n\t\t\treturn nil, fmt.Errorf(\"invalid permission entity. %s\",\n\t\t\t\thelpMsg)\n\t\t}\n\n\t\t// Either we have the special entity \"uri\" which specifies a\n\t\t// full gRPC URI or we have one of the pre-defined actions.\n\t\tif op.Entity == macaroons.PermissionEntityCustomURI {\n\t\t\tallPermissions := r.interceptorChain.Permissions()\n\t\t\t_, ok := allPermissions[op.Action]\n\t\t\tif !ok {\n\t\t\t\treturn nil, fmt.Errorf(\"invalid permission \" +\n\t\t\t\t\t\"action, must be an existing URI in \" +\n\t\t\t\t\t\"the format /package.Service/\" +\n\t\t\t\t\t\"MethodName\")\n\t\t\t}\n\t\t} else if !stringInSlice(op.Action, validActions) {\n\t\t\treturn nil, fmt.Errorf(\"invalid permission action. %s\",\n\t\t\t\thelpMsg)\n\t\t}\n\n\t\trequestedPermissions[idx] = bakery.Op{\n\t\t\tEntity: op.Entity,\n\t\t\tAction: op.Action,\n\t\t}\n\t}\n\n\t// Convert root key id from uint64 to bytes. Because the\n\t// DefaultRootKeyID is a digit 0 expressed in a byte slice of a string\n\t// \"0\", we will keep the IDs in the same format - all must be numeric,\n\t// and must be a byte slice of string value of the digit, e.g.,\n\t// uint64(123) to string(123).\n\trootKeyID := []byte(strconv.FormatUint(req.RootKeyId, 10))\n\n\t// Bake new macaroon with the given permissions and send it binary\n\t// serialized and hex encoded to the client.\n\tnewMac, err := r.macService.NewMacaroon(\n\t\tctx, rootKeyID, requestedPermissions...,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tnewMacBytes, err := newMac.M().MarshalBinary()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tresp := &lnrpc.BakeMacaroonResponse{}\n\tresp.Macaroon = hex.EncodeToString(newMacBytes)\n\n\treturn resp, nil\n}\n\n// ListMacaroonIDs returns a list of macaroon root key IDs in use.\nfunc (r *rpcServer) ListMacaroonIDs(ctx context.Context,\n\treq *lnrpc.ListMacaroonIDsRequest) (\n\t*lnrpc.ListMacaroonIDsResponse, error) {\n\n\t// If the --no-macaroons flag is used to start lnd, the macaroon service\n\t// is not initialized. Therefore we can't show any IDs.\n\tif r.macService == nil {\n\t\treturn nil, errMacaroonDisabled\n\t}\n\n\trootKeyIDByteSlice, err := r.macService.ListMacaroonIDs(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar rootKeyIDs []uint64\n\tfor _, value := range rootKeyIDByteSlice {\n\t\t// Convert bytes into uint64.\n\t\tid, err := strconv.ParseUint(string(value), 10, 64)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\trootKeyIDs = append(rootKeyIDs, id)\n\t}\n\n\treturn &lnrpc.ListMacaroonIDsResponse{RootKeyIds: rootKeyIDs}, nil\n}\n\n// DeleteMacaroonID removes a specific macaroon ID.\nfunc (r *rpcServer) DeleteMacaroonID(ctx context.Context,\n\treq *lnrpc.DeleteMacaroonIDRequest) (\n\t*lnrpc.DeleteMacaroonIDResponse, error) {\n\n\t// If the --no-macaroons flag is used to start lnd, the macaroon service\n\t// is not initialized. Therefore we can't delete any IDs.\n\tif r.macService == nil {\n\t\treturn nil, errMacaroonDisabled\n\t}\n\n\t// Convert root key id from uint64 to bytes. Because the\n\t// DefaultRootKeyID is a digit 0 expressed in a byte slice of a string\n\t// \"0\", we will keep the IDs in the same format - all must be digit, and\n\t// must be a byte slice of string value of the digit.\n\trootKeyID := []byte(strconv.FormatUint(req.RootKeyId, 10))\n\tdeletedIDBytes, err := r.macService.DeleteMacaroonID(ctx, rootKeyID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &lnrpc.DeleteMacaroonIDResponse{\n\t\t// If the root key ID doesn't exist, it won't be deleted. We\n\t\t// will return a response with deleted = false, otherwise true.\n\t\tDeleted: deletedIDBytes != nil,\n\t}, nil\n}\n\n// ListPermissions lists all RPC method URIs and their required macaroon\n// permissions to access them.\nfunc (r *rpcServer) ListPermissions(_ context.Context,\n\t_ *lnrpc.ListPermissionsRequest) (*lnrpc.ListPermissionsResponse,\n\terror) {\n\n\tpermissionMap := make(map[string]*lnrpc.MacaroonPermissionList)\n\tfor uri, perms := range r.interceptorChain.Permissions() {\n\t\trpcPerms := make([]*lnrpc.MacaroonPermission, len(perms))\n\t\tfor idx, perm := range perms {\n\t\t\trpcPerms[idx] = &lnrpc.MacaroonPermission{\n\t\t\t\tEntity: perm.Entity,\n\t\t\t\tAction: perm.Action,\n\t\t\t}\n\t\t}\n\t\tpermissionMap[uri] = &lnrpc.MacaroonPermissionList{\n\t\t\tPermissions: rpcPerms,\n\t\t}\n\t}\n\n\treturn &lnrpc.ListPermissionsResponse{\n\t\tMethodPermissions: permissionMap,\n\t}, nil\n}\n\n// CheckMacaroonPermissions checks the caveats and permissions of a macaroon.\nfunc (r *rpcServer) CheckMacaroonPermissions(ctx context.Context,\n\treq *lnrpc.CheckMacPermRequest) (*lnrpc.CheckMacPermResponse, error) {\n\n\t// Turn grpc macaroon permission into bakery.Op for the server to\n\t// process.\n\tpermissions := make([]bakery.Op, len(req.Permissions))\n\tfor idx, perm := range req.Permissions {\n\t\tpermissions[idx] = bakery.Op{\n\t\t\tEntity: perm.Entity,\n\t\t\tAction: perm.Action,\n\t\t}\n\t}\n\n\terr := r.macService.CheckMacAuth(\n\t\tctx, req.Macaroon, permissions, req.FullMethod,\n\t)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.InvalidArgument, err.Error())\n\t}\n\n\treturn &lnrpc.CheckMacPermResponse{\n\t\tValid: true,\n\t}, nil\n}\n\n// FundingStateStep is an advanced funding related call that allows the caller\n// to either execute some preparatory steps for a funding workflow, or manually\n// progress a funding workflow. The primary way a funding flow is identified is\n// via its pending channel ID. As an example, this method can be used to\n// specify that we're expecting a funding flow for a particular pending channel\n// ID, for which we need to use specific parameters.  Alternatively, this can\n// be used to interactively drive PSBT signing for funding for partially\n// complete funding transactions.\nfunc (r *rpcServer) FundingStateStep(ctx context.Context,\n\tin *lnrpc.FundingTransitionMsg) (*lnrpc.FundingStateStepResp, error) {\n\n\tvar pendingChanID [32]byte\n\tswitch {\n\t// If this is a message to register a new shim that is an external\n\t// channel point, then we'll contact the wallet to register this new\n\t// shim. A user will use this method to register a new channel funding\n\t// workflow which has already been partially negotiated outside of the\n\t// core protocol.\n\tcase in.GetShimRegister() != nil &&\n\t\tin.GetShimRegister().GetChanPointShim() != nil:\n\n\t\trpcShimIntent := in.GetShimRegister().GetChanPointShim()\n\n\t\t// Using the rpc shim as a template, we'll construct a new\n\t\t// chanfunding.Assembler that is able to express proper\n\t\t// formulation of this expected channel.\n\t\tshimAssembler, err := newFundingShimAssembler(\n\t\t\trpcShimIntent, false, r.server.cc.KeyRing,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treq := &chanfunding.Request{\n\t\t\tRemoteAmt: btcutil.Amount(rpcShimIntent.Amt),\n\t\t}\n\t\tshimIntent, err := shimAssembler.ProvisionChannel(req)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Once we have the intent, we'll register it with the wallet.\n\t\t// Once we receive an incoming funding request that uses this\n\t\t// pending channel ID, then this shim will be dispatched in\n\t\t// place of our regular funding workflow.\n\t\tcopy(pendingChanID[:], rpcShimIntent.PendingChanId)\n\t\terr = r.server.cc.Wallet.RegisterFundingIntent(\n\t\t\tpendingChanID, shimIntent,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t// There is no need to register a PSBT shim before opening the channel,\n\t// even though our RPC message structure allows for it. Inform the user\n\t// by returning a proper error instead of just doing nothing.\n\tcase in.GetShimRegister() != nil &&\n\t\tin.GetShimRegister().GetPsbtShim() != nil:\n\n\t\treturn nil, fmt.Errorf(\"PSBT shim must only be sent when \" +\n\t\t\t\"opening a channel\")\n\n\t// If this is a transition to cancel an existing shim, then we'll pass\n\t// this message along to the wallet, informing it that the intent no\n\t// longer needs to be considered and should be cleaned up.\n\tcase in.GetShimCancel() != nil:\n\t\trpcsLog.Debugf(\"Canceling funding shim for pending_id=%x\",\n\t\t\tin.GetShimCancel().PendingChanId)\n\n\t\tcopy(pendingChanID[:], in.GetShimCancel().PendingChanId)\n\t\terr := r.server.cc.Wallet.CancelFundingIntent(pendingChanID)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t// If this is a transition to verify the PSBT for an existing shim,\n\t// we'll do so and then store the verified PSBT for later so we can\n\t// compare it to the final, signed one.\n\tcase in.GetPsbtVerify() != nil:\n\t\trpcsLog.Debugf(\"Verifying PSBT for pending_id=%x\",\n\t\t\tin.GetPsbtVerify().PendingChanId)\n\n\t\tcopy(pendingChanID[:], in.GetPsbtVerify().PendingChanId)\n\t\tpacket, err := psbt.NewFromRawBytes(\n\t\t\tbytes.NewReader(in.GetPsbtVerify().FundedPsbt), false,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"error parsing psbt: %w\", err)\n\t\t}\n\n\t\terr = r.server.cc.Wallet.PsbtFundingVerify(\n\t\t\tpendingChanID, packet, in.GetPsbtVerify().SkipFinalize,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t// If this is a transition to finalize the PSBT funding flow, we compare\n\t// the final PSBT to the previously verified one and if nothing\n\t// unexpected was changed, continue the channel opening process.\n\tcase in.GetPsbtFinalize() != nil:\n\t\tmsg := in.GetPsbtFinalize()\n\t\trpcsLog.Debugf(\"Finalizing PSBT for pending_id=%x\",\n\t\t\tmsg.PendingChanId)\n\n\t\tcopy(pendingChanID[:], in.GetPsbtFinalize().PendingChanId)\n\n\t\tvar (\n\t\t\tpacket *psbt.Packet\n\t\t\trawTx  *wire.MsgTx\n\t\t\terr    error\n\t\t)\n\n\t\t// Either the signed PSBT or the raw transaction need to be set\n\t\t// but not both at the same time.\n\t\tswitch {\n\t\tcase len(msg.SignedPsbt) > 0 && len(msg.FinalRawTx) > 0:\n\t\t\treturn nil, fmt.Errorf(\"cannot set both signed PSBT \" +\n\t\t\t\t\"and final raw TX at the same time\")\n\n\t\tcase len(msg.SignedPsbt) > 0:\n\t\t\tpacket, err = psbt.NewFromRawBytes(\n\t\t\t\tbytes.NewReader(in.GetPsbtFinalize().SignedPsbt),\n\t\t\t\tfalse,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"error parsing psbt: %w\",\n\t\t\t\t\terr)\n\t\t\t}\n\n\t\tcase len(msg.FinalRawTx) > 0:\n\t\t\trawTx = &wire.MsgTx{}\n\t\t\terr = rawTx.Deserialize(bytes.NewReader(msg.FinalRawTx))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"error parsing final \"+\n\t\t\t\t\t\"raw TX: %v\", err)\n\t\t\t}\n\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"PSBT or raw transaction to \" +\n\t\t\t\t\"finalize missing\")\n\t\t}\n\n\t\terr = r.server.cc.Wallet.PsbtFundingFinalize(\n\t\t\tpendingChanID, packet, rawTx,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// TODO(roasbeef): extend PendingChannels to also show shims\n\n\t// TODO(roasbeef): return resulting state? also add a method to query\n\t// current state?\n\treturn &lnrpc.FundingStateStepResp{}, nil\n}\n\n// RegisterRPCMiddleware adds a new gRPC middleware to the interceptor chain. A\n// gRPC middleware is software component external to lnd that aims to add\n// additional business logic to lnd by observing/intercepting/validating\n// incoming gRPC client requests and (if needed) replacing/overwriting outgoing\n// messages before they're sent to the client. When registering the middleware\n// must identify itself and indicate what custom macaroon caveats it wants to\n// be responsible for. Only requests that contain a macaroon with that specific\n// custom caveat are then sent to the middleware for inspection. As a security\n// measure, _no_ middleware can intercept requests made with _unencumbered_\n// macaroons!\nfunc (r *rpcServer) RegisterRPCMiddleware(\n\tstream lnrpc.Lightning_RegisterRPCMiddlewareServer) error {\n\n\t// This is a security critical functionality and needs to be enabled\n\t// specifically by the user.\n\tif !r.cfg.RPCMiddleware.Enable {\n\t\treturn fmt.Errorf(\"RPC middleware not enabled in config\")\n\t}\n\n\t// When registering a middleware the first message being sent from the\n\t// middleware must be a registration message containing its name and the\n\t// custom caveat it wants to register for.\n\tvar (\n\t\tregisterChan     = make(chan *lnrpc.MiddlewareRegistration, 1)\n\t\tregisterDoneChan = make(chan struct{})\n\t\terrChan          = make(chan error, 1)\n\t)\n\tctxc, cancel := context.WithTimeout(\n\t\tstream.Context(), r.cfg.RPCMiddleware.InterceptTimeout,\n\t)\n\tdefer cancel()\n\n\t// Read the first message in a goroutine because the Recv method blocks\n\t// until the message arrives.\n\tgo func() {\n\t\tmsg, err := stream.Recv()\n\t\tif err != nil {\n\t\t\terrChan <- err\n\n\t\t\treturn\n\t\t}\n\n\t\tregisterChan <- msg.GetRegister()\n\t}()\n\n\t// Wait for the initial message to arrive or time out if it takes too\n\t// long.\n\tvar registerMsg *lnrpc.MiddlewareRegistration\n\tselect {\n\tcase registerMsg = <-registerChan:\n\t\tif registerMsg == nil {\n\t\t\treturn fmt.Errorf(\"invalid initial middleware \" +\n\t\t\t\t\"registration message\")\n\t\t}\n\n\tcase err := <-errChan:\n\t\treturn fmt.Errorf(\"error receiving initial middleware \"+\n\t\t\t\"registration message: %v\", err)\n\n\tcase <-ctxc.Done():\n\t\treturn ctxc.Err()\n\n\tcase <-r.quit:\n\t\treturn ErrServerShuttingDown\n\t}\n\n\t// Make sure the registration is valid.\n\tconst nameMinLength = 5\n\tif len(registerMsg.MiddlewareName) < nameMinLength {\n\t\treturn fmt.Errorf(\"invalid middleware name, use descriptive \"+\n\t\t\t\"name of at least %d characters\", nameMinLength)\n\t}\n\n\treadOnly := registerMsg.ReadOnlyMode\n\tcaveatName := registerMsg.CustomMacaroonCaveatName\n\tswitch {\n\tcase readOnly && len(caveatName) > 0:\n\t\treturn fmt.Errorf(\"cannot set read-only and custom caveat \" +\n\t\t\t\"name at the same time\")\n\n\tcase !readOnly && len(caveatName) < nameMinLength:\n\t\treturn fmt.Errorf(\"need to set either custom caveat name \"+\n\t\t\t\"of at least %d characters or read-only mode\",\n\t\t\tnameMinLength)\n\t}\n\n\tmiddleware := rpcperms.NewMiddlewareHandler(\n\t\tregisterMsg.MiddlewareName,\n\t\tcaveatName, readOnly, stream.Recv, stream.Send,\n\t\tr.cfg.RPCMiddleware.InterceptTimeout,\n\t\tr.cfg.ActiveNetParams.Params, r.quit,\n\t)\n\n\t// Add the RPC middleware to the interceptor chain and defer its\n\t// removal.\n\tif err := r.interceptorChain.RegisterMiddleware(middleware); err != nil {\n\t\treturn fmt.Errorf(\"error registering middleware: %w\", err)\n\t}\n\tdefer r.interceptorChain.RemoveMiddleware(registerMsg.MiddlewareName)\n\n\t// Send a message to the client to indicate that the registration has\n\t// successfully completed.\n\tregCompleteMsg := &lnrpc.RPCMiddlewareRequest{\n\t\tInterceptType: &lnrpc.RPCMiddlewareRequest_RegComplete{\n\t\t\tRegComplete: true,\n\t\t},\n\t}\n\n\t// Send the message in a goroutine because the Send method blocks until\n\t// the message is read by the client.\n\tgo func() {\n\t\terr := stream.Send(regCompleteMsg)\n\t\tif err != nil {\n\t\t\terrChan <- err\n\t\t\treturn\n\t\t}\n\n\t\tclose(registerDoneChan)\n\t}()\n\n\tselect {\n\tcase err := <-errChan:\n\t\treturn fmt.Errorf(\"error sending middleware registration \"+\n\t\t\t\"complete message: %v\", err)\n\n\tcase <-ctxc.Done():\n\t\treturn ctxc.Err()\n\n\tcase <-r.quit:\n\t\treturn ErrServerShuttingDown\n\n\tcase <-registerDoneChan:\n\t}\n\n\treturn middleware.Run()\n}\n\n// SendCustomMessage sends a custom peer message.\nfunc (r *rpcServer) SendCustomMessage(_ context.Context,\n\treq *lnrpc.SendCustomMessageRequest) (*lnrpc.SendCustomMessageResponse,\n\terror) {\n\n\tpeer, err := route.NewVertexFromBytes(req.Peer)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\terr = r.server.SendCustomMessage(\n\t\tpeer, lnwire.MessageType(req.Type), req.Data,\n\t)\n\tswitch {\n\tcase errors.Is(err, ErrPeerNotConnected):\n\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\tcase err != nil:\n\t\treturn nil, err\n\t}\n\n\treturn &lnrpc.SendCustomMessageResponse{\n\t\tStatus: \"message sent successfully\",\n\t}, nil\n}\n\n// SubscribeCustomMessages subscribes to a stream of incoming custom peer\n// messages.\nfunc (r *rpcServer) SubscribeCustomMessages(\n\t_ *lnrpc.SubscribeCustomMessagesRequest,\n\tserver lnrpc.Lightning_SubscribeCustomMessagesServer) error {\n\n\tclient, err := r.server.SubscribeCustomMessages()\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer client.Cancel()\n\n\tfor {\n\t\tselect {\n\t\tcase <-client.Quit():\n\t\t\treturn errors.New(\"shutdown\")\n\n\t\tcase <-server.Context().Done():\n\t\t\treturn server.Context().Err()\n\n\t\tcase update := <-client.Updates():\n\t\t\tcustomMsg := update.(*CustomMessage)\n\n\t\t\terr := server.Send(&lnrpc.CustomMessage{\n\t\t\t\tPeer: customMsg.Peer[:],\n\t\t\t\tData: customMsg.Msg.Data,\n\t\t\t\tType: uint32(customMsg.Msg.Type),\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n}\n\n// ListAliases returns the set of all aliases we have ever allocated along with\n// their base SCIDs and possibly a separate confirmed SCID in the case of\n// zero-conf.\nfunc (r *rpcServer) ListAliases(_ context.Context,\n\t_ *lnrpc.ListAliasesRequest) (*lnrpc.ListAliasesResponse, error) {\n\n\t// Fetch the map of all aliases.\n\tmapAliases := r.server.aliasMgr.ListAliases()\n\n\t// Fill out the response. This does not include the zero-conf confirmed\n\t// SCID. Doing so would require more database lookups, and it can be\n\t// cross-referenced with the output of ListChannels/ClosedChannels.\n\tresp := &lnrpc.ListAliasesResponse{\n\t\tAliasMaps: make([]*lnrpc.AliasMap, 0),\n\t}\n\n\t// Now we need to parse the created mappings into an rpc response.\n\tresp.AliasMaps = lnrpc.MarshalAliasMap(mapAliases)\n\n\treturn resp, nil\n}\n\n// rpcInitiator returns the correct lnrpc initiator for channels where we have\n// a record of the opening channel.\nfunc rpcInitiator(isInitiator bool) lnrpc.Initiator {\n\tif isInitiator {\n\t\treturn lnrpc.Initiator_INITIATOR_LOCAL\n\t}\n\n\treturn lnrpc.Initiator_INITIATOR_REMOTE\n}\n"
        },
        {
          "name": "rpcserver_test.go",
          "type": "blob",
          "size": 3.4111328125,
          "content": "package lnd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/lightningnetwork/lnd/channeldb\"\n\t\"github.com/lightningnetwork/lnd/fn/v2\"\n\t\"github.com/lightningnetwork/lnd/lnrpc\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n\t\"google.golang.org/protobuf/proto\"\n)\n\nfunc TestGetAllPermissions(t *testing.T) {\n\tperms := GetAllPermissions()\n\n\t// Currently there are 16 entity:action pairs in use.\n\tassert.Equal(t, len(perms), 16)\n}\n\n// mockDataParser is a mock implementation of the AuxDataParser interface.\ntype mockDataParser struct {\n}\n\n// InlineParseCustomData replaces any custom data binary blob in the given RPC\n// message with its corresponding JSON formatted data. This transforms the\n// binary (likely TLV encoded) data to a human-readable JSON representation\n// (still as byte slice).\nfunc (m *mockDataParser) InlineParseCustomData(msg proto.Message) error {\n\tswitch m := msg.(type) {\n\tcase *lnrpc.ChannelBalanceResponse:\n\t\tm.CustomChannelData = []byte(`{\"foo\": \"bar\"}`)\n\n\t\treturn nil\n\n\tdefault:\n\t\treturn fmt.Errorf(\"mock only supports ChannelBalanceResponse\")\n\t}\n}\n\nfunc TestAuxDataParser(t *testing.T) {\n\t// We create an empty channeldb, so we can fetch some channels.\n\tcdb := channeldb.OpenForTesting(t, t.TempDir())\n\n\tr := &rpcServer{\n\t\tserver: &server{\n\t\t\tchanStateDB: cdb.ChannelStateDB(),\n\t\t\timplCfg: &ImplementationCfg{\n\t\t\t\tAuxComponents: AuxComponents{\n\t\t\t\t\tAuxDataParser: fn.Some[AuxDataParser](\n\t\t\t\t\t\t&mockDataParser{},\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\t// With the aux data parser in place, we should get a formatted JSON\n\t// in the custom channel data field.\n\tresp, err := r.ChannelBalance(nil, &lnrpc.ChannelBalanceRequest{})\n\trequire.NoError(t, err)\n\trequire.NotNil(t, resp)\n\trequire.Equal(t, []byte(`{\"foo\": \"bar\"}`), resp.CustomChannelData)\n\n\t// If we don't supply the aux data parser, we should get the raw binary\n\t// data. Which in this case is just two VarInt fields (1 byte each) that\n\t// represent the value of 0 (zero active and zero pending channels).\n\tr.server.implCfg.AuxComponents.AuxDataParser = fn.None[AuxDataParser]()\n\n\tresp, err = r.ChannelBalance(nil, &lnrpc.ChannelBalanceRequest{})\n\trequire.NoError(t, err)\n\trequire.NotNil(t, resp)\n\trequire.Equal(t, []byte{0x00, 0x00}, resp.CustomChannelData)\n}\n\n// TestRpcCommitmentType tests the rpcCommitmentType returns the corect\n// commitment type given a channel type.\nfunc TestRpcCommitmentType(t *testing.T) {\n\ttests := []struct {\n\t\tname     string\n\t\tchanType channeldb.ChannelType\n\t\twant     lnrpc.CommitmentType\n\t}{\n\t\t{\n\t\t\tname: \"tapscript overlay\",\n\t\t\tchanType: channeldb.SimpleTaprootFeatureBit |\n\t\t\t\tchanneldb.TapscriptRootBit,\n\t\t\twant: lnrpc.CommitmentType_SIMPLE_TAPROOT_OVERLAY,\n\t\t},\n\t\t{\n\t\t\tname:     \"simple taproot\",\n\t\t\tchanType: channeldb.SimpleTaprootFeatureBit,\n\t\t\twant:     lnrpc.CommitmentType_SIMPLE_TAPROOT,\n\t\t},\n\t\t{\n\t\t\tname:     \"lease expiration\",\n\t\t\tchanType: channeldb.LeaseExpirationBit,\n\t\t\twant:     lnrpc.CommitmentType_SCRIPT_ENFORCED_LEASE,\n\t\t},\n\t\t{\n\t\t\tname:     \"anchors\",\n\t\t\tchanType: channeldb.AnchorOutputsBit,\n\t\t\twant:     lnrpc.CommitmentType_ANCHORS,\n\t\t},\n\t\t{\n\t\t\tname:     \"tweakless\",\n\t\t\tchanType: channeldb.SingleFunderTweaklessBit,\n\t\t\twant:     lnrpc.CommitmentType_STATIC_REMOTE_KEY,\n\t\t},\n\t\t{\n\t\t\tname:     \"legacy\",\n\t\t\tchanType: channeldb.SingleFunderBit,\n\t\t\twant:     lnrpc.CommitmentType_LEGACY,\n\t\t},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\trequire.Equal(\n\t\t\t\tt, tt.want, rpcCommitmentType(tt.chanType),\n\t\t\t)\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "sample-lnd.conf",
          "type": "blob",
          "size": 70.314453125,
          "content": "; Example configuration for lnd.\n;\n; The default location for this file is in ~/.lnd/lnd.conf on POSIX OSes,\n; $LOCALAPPDATA/Lnd/lnd.conf on Windows,\n; ~/Library/Application Support/Lnd/lnd.conf on Mac OS and $home/lnd/lnd.conf on\n; Plan9.\n; The default location of this file can be overwritten by specifying the\n; --configfile= flag when starting lnd.\n;\n; boolean values can be specified as true/false or 1/0. Per default \n; booleans are always set to false.\n\n; If only one value is specified for an option, then this is also the\n; default value used by lnd. In case of multiple (example) values, the default \n; is explicitly mentioned. \n; If the part after the equal sign is empty then lnd has no default \n; for this option.\n\n[Application Options]\n\n; The directory that lnd stores all wallet, chain, and channel related data\n; within The default is ~/.lnd/data on POSIX OSes, $LOCALAPPDATA/Lnd/data on\n; Windows, ~/Library/Application Support/Lnd/data on Mac OS, and $home/lnd/data\n; on Plan9. Environment variables are expanded so they may be used. NOTE:\n; Windows environment variables are typically %VARIABLE%, but they must be\n; accessed with $VARIABLE here. Also, ~ is expanded to $LOCALAPPDATA on Windows.\n; datadir=~/.lnd/data\n\n; The directory that logs are stored in. The logs are auto-rotated by default.\n; Rotated logs are compressed in place.\n; logdir=~/.lnd/logs\n\n; DEPRECATED: Use logging.file.max-files instead.\n; Number of logfiles that the log rotation should keep. Setting it to 0 disables\n; deletion of old log files.\n; maxlogfiles=10\n;\n; DEPRECATED: Use logging.file.max-file-size instead.\n; Max log file size in MB before it is rotated.\n; maxlogfilesize=20\n\n; Time after which an RPCAcceptor will time out and return false if\n; it hasn't yet received a response.\n; acceptortimeout=15s\n\n; Path to TLS certificate for lnd's RPC and REST services.\n; tlscertpath=~/.lnd/tls.cert\n\n; Path to TLS private key for lnd's RPC and REST services.\n; tlskeypath=~/.lnd/tls.key\n\n; Adds an extra ip to the generated certificate. Setting multiple tlsextraip= entries is allowed.\n; (old tls files must be deleted if changed)\n; tlsextraip=\n\n; Adds an extra domain to the generate certificate. Setting multiple tlsextradomain= entries is allowed.\n; (old tls files must be deleted if changed)\n; Default:\n;   tlsextradomain=\n; Example: (option can be specified multiple times):\n;   tlsextradomain=my-node-domain.com\n\n; If set, then all certs will automatically be refreshed if they're close to\n; expiring, or if any parameters related to extra IPs or domains in the cert\n; change.\n; tlsautorefresh=false\n\n; The duration from generating the self signed certificate to the certificate\n; expiry date. Valid time units are {s, m, h}.\n; The below value is about 14 months (14 * 30 * 24 = 10080)\n; tlscertduration=10080h\n\n; Do not include the interface IPs or the system hostname in TLS certificate,\n; use first --tlsextradomain as Common Name instead, if set.\n; tlsdisableautofill=false\n\n; If set, the TLS private key will be encrypted to the node's seed.\n; tlsencryptkey=false\n\n; A list of domains for lnd to periodically resolve, and advertise the resolved\n; IPs for the backing node. This is useful for users that only have a dynamic IP,\n; or want to expose the node at a domain.\n; Default:\n;   externalhosts=\n; Example (option can be specified multiple times):\n;   externalhosts=my-node-domain.com\n;   externalhosts=my-second-domain.com\n\n; Sets the directory to store Let's Encrypt certificates within\n; letsencryptdir=~/.lnd/letsencrypt\n\n; The IP:port on which lnd will listen for Let's Encrypt challenges. Let's\n; Encrypt will always try to contact on port 80. Often non-root processes are\n; not allowed to bind to ports lower than 1024. This configuration option allows\n; a different port to be used, but must be used in combination with port\n; forwarding from port 80. This configuration can also be used to specify\n; another IP address to listen on, for example an IPv6 address.\n; Default:\n;   letsencryptlisten=:80\n; Example:\n;   letsencryptlisten=localhost:8080\n\n; Request a Let's Encrypt certificate for this domain. Note that the certificate\n; is only requested and stored when the first rpc connection comes in.\n; Default:\n;   letsencryptdomain=\n; Example:\n;   letsencryptdomain=example.com\n\n; Disable macaroon authentication. Macaroons are used as bearer credentials to\n; authenticate all RPC access. If one wishes to opt out of macaroons, uncomment\n; and set to true the line below.\n; no-macaroons=false\n\n; Enable free list syncing for the default bbolt database. This will decrease\n; start up time, but can result in performance degradation for very large\n; databases, and also result in higher memory usage. If \"free list corruption\"\n; is detected, then this flag may resolve things.\n; sync-freelist=false\n\n; Path to write the admin macaroon for lnd's RPC and REST services if it\n; doesn't exist. This can be set if one wishes to store the admin macaroon in a\n; distinct location. By default, it is stored within lnd's network directory.\n; Applications that are able to read this file, gain admin macaroon access.\n; Default:\n;   adminmacaroonpath=~/.lnd/data/chain/bitcoin/${network}/admin.macaroon\n; Example:\n;   adminmacaroonpath=~/.lnd/data/chain/bitcoin/mainnet/admin.macaroon\n\n; Path to write the read-only macaroon for lnd's RPC and REST services if it\n; doesn't exist. This can be set if one wishes to store the read-only macaroon\n; in a distinct location. The read only macaroon allows users which can read\n; the file to access RPCs which don't modify the state of the daemon. By\n; default, it is stored within lnd's network directory.\n; Default:\n;   readonlymacaroonpath=~/.lnd/data/chain/bitcoin/${network}/readonly.macaroon\n; Example:\n;   readonlymacaroonpath=~/.lnd/data/chain/bitcoin/mainnet/readonly.macaroon\n\n; Path to write the invoice macaroon for lnd's RPC and REST services if it\n; doesn't exist. This can be set if one wishes to store the invoice macaroon in\n; a distinct location. By default, it is stored within lnd's network directory.\n; The invoice macaroon allows users which can read the file to gain read and\n; write access to all invoice related RPCs.\n; Default:\n;   invoicemacaroonpath=~/.lnd/data/chain/bitcoin/${network}/invoice.macaroon\n; Example:\n;   invoicemacaroonpath=~/.lnd/data/chain/bitcoin/mainnet/invoice.macaroon\n\n; The strategy to use for selecting coins for wallet transactions. Options are\n; 'largest' and 'random'.\n; coin-selection-strategy=largest\n\n; A period to wait before for closing channels with outgoing htlcs that have\n; timed out and are a result of this nodes initiated payments. In addition to\n; our current block based deadline, if specified this grace period will also be\n; taken into account. Valid time units are {s, m, h}.\n; Default:\n;   payments-expiration-grace-period=0s\n; Example:\n;   payments-expiration-grace-period=30s\n\n; Specify the interfaces to listen on for p2p connections. One listen\n; address per line.\n; Default:\n;   listen=:9735\n; Example (option can be specified multiple times):\n;  All ipv4 on port 9735:\n;   listen=0.0.0.0:9735\n\n;  On all ipv4 interfaces on port 9735 and ipv6 localhost port 9736:\n;   listen=0.0.0.0:9735\n;   listen=[::1]:9736\n\n; Disable listening for incoming p2p connections. This will override all\n; listeners.\n; nolisten=false\n\n; Specify the interfaces to listen on for gRPC connections. One listen\n; address per line.\n; Default:\n;   rpclisten=localhost:10009\n; Example (option can be specified multiple times):\n;  On ipv4 localhost port 10009 and ipv6 port 10010:\n;   rpclisten=localhost:10009\n;   rpclisten=[::1]:10010\n;  On an Unix socket:\n;   rpclisten=unix:///var/run/lnd/lnd-rpclistener.sock\n\n; Specify the interfaces to listen on for REST connections. One listen\n; address per line.\n; Default:\n;   restlisten=localhost:8080\n; Example (option can be specified multiple times):\n;  All ipv4 interfaces on port 8080:\n;   restlisten=0.0.0.0:8080\n;  On ipv4 localhost port 80 and 443:\n;   restlisten=localhost:80\n;   restlisten=localhost:443\n;  On an Unix socket:\n;   restlisten=unix:///var/run/lnd-restlistener.sock\n\n; A series of domains to allow cross origin access from. This controls the CORs\n; policy of the REST RPC proxy.\n; Default:\n;   restcors=\n; Example (option can be specified multiple times):\n;   restcors=https://my-special-site.com\n\n; Adding an external IP will advertise your node to the network. This signals\n; that your node is available to accept incoming channels. If you don't wish to\n; advertise your node, this value doesn't need to be set. Unless specified\n; (with host:port notation), the default port (9735) will be added to the\n; address.\n; externalip=\n;\n; Instead of explicitly stating your external IP address, you can also enable\n; UPnP or NAT-PMP support on the daemon. Both techniques will be tried and\n; require proper hardware support. In order to detect this hardware support,\n; `lnd` uses a dependency that retrieves the router's gateway address by using\n; different built-in binaries in each platform. Therefore, it is possible that\n; we are unable to detect the hardware and `lnd` will exit with an error\n; indicating this. This option will automatically retrieve your external IP\n; address, even after it has changed in the case of dynamic IPs, and advertise\n; it to the network using the ports the daemon is listening on. This does not\n; support devices behind multiple NATs.\n; nat=false\n\n; Disable REST API.\n; norest=false\n\n; Disable TLS for the REST API.\n; no-rest-tls=false\n\n; Specify peer(s) to connect to first.\n; addpeer=\n\n; The ping interval for REST based WebSocket connections, set to 0 to disable\n; sending ping messages from the server side. Valid time units are {s, m, h}.\n; ws-ping-interval=30s\n\n; The time we wait for a pong response message on REST based WebSocket\n; connections before the connection is closed as inactive. Valid time units are\n; {s, m, h}.\n; ws-pong-wait=5s\n\n; Shortest backoff when reconnecting to persistent peers. Valid time units are\n; {s, m, h}.\n; minbackoff=1s\n\n; Longest backoff when reconnecting to persistent peers. Valid time units are\n; {s, m, h}.\n; maxbackoff=1h\n\n; The timeout value for network connections.\n; Valid units are {ms, s, m, h}.\n; connectiontimeout=2m\n\n; Debug logging level.\n; Valid levels are {trace, debug, info, warn, error, critical}\n; You may also specify <global-level>,<subsystem>=<level>,<subsystem2>=<level>,...\n; to set log level for individual subsystems. Use lncli debuglevel --show to\n; list available subsystems.\n; Default:\n;   debuglevel=info\n; Example:\n;   debuglevel=debug,PEER=info\n\n; DEPRECATED: Use pprof.cpuprofile instead. Write CPU profile to the specified \n; file.\n; cpuprofile=\n\n; DEPRECATED: Use pprof.profile instead.Enable HTTP profiling on given port \n; -- NOTE port must be between 1024 and 65536. The profile can be access at:\n; http://localhost:<PORT>/debug/pprof/. You can also provide it as host:port to\n; enable profiling for remote debugging. For example 0.0.0.0:<PORT> to enable\n; profiling for all interfaces on the given port.\n; profile=\n\n; DEPRECATED: Use pprof.blockingprofile instead. Enable a blocking profile to be\n; obtained from the profiling port. A blocking profile can show where goroutines\n; are blocking (stuck on mutexes, I/O, etc). This takes a value from 0 to 1,\n; with 0 turning off the setting, and 1 sampling every blocking event (it's a\n; rate value).\n; blockingprofile=0\n\n; DEPRECATED: Use pprof.mutexprofile instead. Enable a mutex profile to be \n; obtained from the profiling port. A mutex profile can show where goroutines \n; are blocked on mutexes, and which mutexes have high contention. This takes a\n; value from 0 to 1, with 0 turning off the setting, and 1 sampling every mutex\n; event (it's a rate value).\n; mutexprofile=0\n\n\n; DEPRECATED: Allows the rpcserver to intentionally disconnect from peers with\n; open channels. THIS FLAG WILL BE REMOVED IN 0.10.0.\n; unsafe-disconnect=false\n\n; Causes a link to replay the adds on its commitment txn after starting up, this\n; enables testing of the sphinx replay logic.\n; unsafe-replay=false\n\n; The maximum number of incoming pending channels permitted per peer.\n; maxpendingchannels=1\n\n; The target location of the channel backup file.\n; Default:\n;   backupfilepath=~/.lnd/data/chain/bitcoin/${network}/channel.backup\n; Example:\n;   backupfilepath=~/.lnd/data/chain/bitcoin/mainnet/channel.backup\n\n; The maximum capacity of the block cache in bytes. Increasing this will result\n; in more blocks being kept in memory but will increase performance when the\n; same block is required multiple times.\n; The default value below is 20 MB (1024 * 1024 * 20)\n; blockcachesize=20971520\n\n; DEPRECATED: Use 'fee.url' option. Optional URL for external fee estimation.\n; If no URL is specified, the method for fee estimation will depend on the\n; chosen backend and network. Must be set for neutrino on mainnet.\n; Default:\n;   feeurl=\n; Example:\n;   feeurl=https://nodes.lightning.computer/fees/v1/btc-fee-estimates.json\n\n; If true, then automatic network bootstrapping will not be attempted. This\n; means that your node won't attempt to automatically seek out peers on the\n; network.\n; nobootstrap=false\n\n; If true, NO SEED WILL BE EXPOSED -- EVER, AND THE WALLET WILL BE ENCRYPTED\n; USING THE DEFAULT PASSPHRASE. THIS FLAG IS ONLY FOR TESTING AND SHOULD NEVER\n; BE USED ON MAINNET.\n; noseedbackup=false\n\n; The full path to a file (or pipe/device) that contains the password for\n; unlocking the wallet; if set, no unlocking through RPC is possible and lnd\n; will exit if no wallet exists or the password is incorrect; if\n; wallet-unlock-allow-create is also set then lnd will ignore this flag if no\n; wallet exists and allow a wallet to be created through RPC.\n; Default:\n;   wallet-unlock-password-file=\n; Example:\n;   wallet-unlock-password-file=/tmp/example.password\n\n; Don't fail with an error if wallet-unlock-password-file is set but no wallet\n; exists yet. Not recommended for auto-provisioned or high-security systems\n; because the wallet creation RPC is unauthenticated and an attacker could\n; inject a seed while lnd is in that state.\n; wallet-unlock-allow-create=false\n\n; Removes all transaction history from the on-chain wallet on startup, forcing a\n; full chain rescan starting at the wallet's birthday. Implements the same\n; functionality as btcwallet's dropwtxmgr command. Should be set to false after\n; successful execution to avoid rescanning on every restart of lnd.\n; reset-wallet-transactions=false\n\n; The smallest channel size (in satoshis) that we should accept. Incoming\n; channels smaller than this will be rejected.\n; minchansize=20000\n\n; The largest channel size (in satoshis) that we should accept. Incoming\n; channels larger than this will be rejected. For non-Wumbo channels this\n; limit remains 16777215 satoshis by default as specified in BOLT-0002.\n; For wumbo channels this limit is 1,000,000,000 satoshis (10 BTC).\n; Set this config option explicitly to restrict your maximum channel size\n; to better align with your risk tolerance\n; Default:\n;   maxchansize=<see explanations above>\n; Example:\n;   maxchansize=10000000\n\n; The target number of blocks in which a cooperative close initiated by a remote\n; peer should be confirmed. This target is used to estimate the starting fee\n; rate that will be used during fee negotiation with the peer. This target is\n; also used for cooperative closes initiated locally if the --conf_target for\n; the channel closure is not set.\n; coop-close-target-confs=6\n\n; The maximum time that is allowed to pass between receiving a channel state\n; update and signing the next commitment. Setting this to a longer duration\n; allows for more efficient channel operations at the cost of latency. This is\n; capped at 1 hour.\n; channel-commit-interval=50ms\n\n; The maximum time that is allowed to pass while waiting for the remote party\n; to revoke a locally initiated commitment state. Setting this to a longer\n; duration if a slow response is expected from the remote party or large\n; number of payments are attempted at the same time.\n; pending-commit-interval=1m\n\n; The maximum number of channel state updates that is accumulated before signing\n; a new commitment.\n; channel-commit-batch-size=10\n\n; Keeps persistent record of all failed payment attempts for successfully\n; settled payments.\n; keep-failed-payment-attempts=false\n\n; Persistently store the final resolution of incoming htlcs.\n; store-final-htlc-resolutions=false\n\n; The default max_htlc applied when opening or accepting channels. This value\n; limits the number of concurrent HTLCs that the remote party can add to the\n; commitment. The maximum possible value is 483.\n; default-remote-max-htlcs=483\n\n; The duration that a peer connection must be stable before attempting to send a\n; channel update to re-enable or cancel a pending disables of the peer's channels\n; on the network. \n; chan-enable-timeout=19m\n\n; The duration that must elapse after first detecting that an already active\n; channel is actually inactive and sending channel update disabling it to the\n; network. The pending disable can be canceled if the peer reconnects and becomes\n; stable for chan-enable-timeout before the disable update is sent.\n; chan-disable-timeout=20m\n\n; The polling interval between attempts to detect if an active channel has become\n; inactive due to its peer going offline.\n; chan-status-sample-interval=1m\n\n; Disable queries from the height-hint cache to try to recover channels stuck in\n; the pending close state. Disabling height hint queries may cause longer chain\n; rescans, resulting in a performance hit. Unset this after channels are unstuck\n; so you can get better performance again.\n; height-hint-cache-query-disable=false\n\n; The polling interval between historical graph sync attempts. Each historical\n; graph sync attempt ensures we reconcile with the remote peer's graph from the\n; genesis block. \n; historicalsyncinterval=1h\n\n; If true, will not reply with historical data that matches the range specified\n; by a remote peer's gossip_timestamp_filter. Doing so will result in lower\n; memory and bandwidth requirements.\n; ignore-historical-gossip-filters=false\n\n; If true, lnd will not accept channel opening requests with non-zero push\n; amounts. This should prevent accidental pushes to merchant nodes.\n; rejectpush=false\n\n; If true, lnd will not forward any HTLCs that are meant as onward payments. This\n; option will still allow lnd to send HTLCs and receive HTLCs but lnd won't be\n; used as a hop.\n; rejecthtlc=false\n\n; If true, all HTLCs will be held until they are handled by an interceptor\n; requireinterceptor=false\n\n; If true, lnd will also allow setting positive inbound fees. By default, lnd\n; only allows to set negative inbound fees (an inbound \"discount\") to remain\n; backwards compatible with senders whose implementations do not yet support\n; inbound fees. Therefore, you should ONLY set this setting if you know what you\n; are doing. [experimental]\n; accept-positive-inbound-fees=false\n\n; If true, will apply a randomized staggering between 0s and 30s when\n; reconnecting to persistent peers on startup. The first 10 reconnections will be\n; attempted instantly, regardless of the flag's value\n; stagger-initial-reconnect=false\n\n; The maximum number of blocks funds could be locked up for when forwarding\n; payments. \n; max-cltv-expiry=2016\n\n; The maximum percentage of total funds that can be allocated to a channel's\n; commitment fee. This only applies for the initiator of the channel. Valid\n; values are within [0.1, 1]. \n; max-channel-fee-allocation=0.5\n\n; The maximum fee rate in sat/vbyte that will be used for commitments of\n; channels of the anchors type. Must be large enough to ensure transaction\n; propagation \n; max-commit-fee-rate-anchors=10\n\n; DEPRECATED: This value will be deprecated please use the new setting \n; \"channel-max-fee-exposure\". This value is equivalent to the new fee exposure\n; limit but was removed because the name was ambigious.\n; dust-threshold=\n\n; This value replaces the old 'dust-threshold' setting and defines the maximum\n; amount of satoshis that a channel pays in fees in case the commitment \n; transaction is broadcasted. This is enforced in both directions either when\n; we are the channel intiator hence paying the fees but also applies to the \n; channel fee if we are NOT the channel initiator. It is\n; important to note that every HTLC adds fees to the channel state. Non-dust \n; HTLCs add just a new output onto the commitment transaction whereas dust \n; HTLCs are completely attributed the commitment fee. So this limit can also \n; influence adding new HTLCs onto the state. When the limit is reached we won't \n; allow any new HTLCs onto the channel state (outgoing and incoming). So \n; choosing a right limit here must be done with caution. Moreover this is a \n; limit for all channels universally meaning there is no difference made due to\n; the channel size. So it is recommended to use the default value. However if\n; you have a very small channel average size you might want to reduce this \n; value.\n; WARNING: Setting this value too low might cause force closes because the \n; lightning protocol has no way to roll back a channel state when your peer \n; proposes a channel update which exceeds this limit. There are only two options \n; to resolve this situation, either increasing the limit or one side force \n; closes the channel.\n; channel-max-fee-exposure=500000\n\n; If true, lnd will abort committing a migration if it would otherwise have been\n; successful. This leaves the database unmodified, and still compatible with the\n; previously active version of lnd.\n; dry-run-migration=false\n\n; If true, option upfront shutdown script will be enabled. If peers that we open\n; channels with support this feature, we will automatically set the script to\n; which cooperative closes should be paid out to on channel open. This offers the\n; partial protection of a channel peer disconnecting from us if cooperative\n; close is attempted with a different script.\n; enable-upfront-shutdown=false\n\n; If true, spontaneous payments through keysend will be accepted.\n; This is a temporary solution until AMP is implemented which is expected to be soon.\n; This option will then become deprecated in favor of AMP.\n; accept-keysend=false\n\n; If non-zero, keysend payments are accepted but not immediately settled. If the\n; payment isn't settled manually after the specified time, it is canceled\n; automatically. [experimental]\n; Default:\n;   keysend-hold-time=0s\n; Example:\n;   keysend-hold-time=2s\n\n; If true, spontaneous payments through AMP will be accepted. Payments to AMP\n; invoices will be accepted regardless of this setting.\n; accept-amp=false\n\n; If true, we'll attempt to garbage collect canceled invoices upon start.\n; gc-canceled-invoices-on-startup=false\n\n; If true, we'll delete newly canceled invoices on the fly.\n; gc-canceled-invoices-on-the-fly=false\n\n; If true, our node will allow htlc forwards that arrive and depart on the same\n; channel.\n; allow-circular-route=false\n\n; Time in milliseconds between each release of announcements to the network\n; trickledelay=90000\n\n; The number of peers that we should receive new graph updates from. This option\n; can be tuned to save bandwidth for light clients or routing nodes. \n; numgraphsyncpeers=3\n\n; The alias your node will use, which can be up to 32 UTF-8 characters in\n; length.\n; Default:\n;   alias=\n; Example:\n;   alias=My Lightning ☇\n\n; The color of the node in hex format, used to customize node appearance in\n; intelligence services.\n; color=#3399FF\n\n; The maximum duration that the server will wait before timing out reading\n; the headers of an HTTP request.\n; http-header-timeout=5s\n\n\n[fee]\n\n; Optional URL for external fee estimation. If no URL is specified, the method\n; for fee estimation will depend on the chosen backend and network. Must be set\n; for neutrino on mainnet.\n; Default:\n;   fee.url=\n; Example:\n;   fee.url=https://nodes.lightning.computer/fees/v1/btc-fee-estimates.json\n\n; The minimum interval in which fees will be updated from the specified fee URL.\n; fee.min-update-timeout=5m\n\n; The maximum interval in which fees will be updated from the specified fee URL.\n; fee.max-update-timeout=20m\n\n\n[prometheus]\n\n; If true, lnd will start the Prometheus exporter. Prometheus flags are\n; behind a build/compile flag and are not available by default. lnd must be built\n; with the monitoring tag; `make && make install tags=monitoring` to activate them.\n; prometheus.enable=false\n\n; Specify the interface to listen on for Prometheus connections.\n; Default:\n;   prometheus.listen=127.0.0.1:8989\n; Example:\n;   prometheus.listen=0.0.0.0:8989\n\n; If true, then we'll export additional information that allows users to plot\n; the processing latency, and total time spent across each RPC calls+service.\n; This generates additional memory load for the Prometheus server, and will end\n; up using more disk space over time.\n; prometheus.perfhistograms=false\n\n\n[Bitcoin]\n\n; DEPRECATED: If the Bitcoin chain should be active. This field is now ignored\n; since only the Bitcoin chain is supported.\n; bitcoin.active=false\n\n; The directory to store the chain's data within.\n; bitcoin.chaindir=~/.lnd/data/chain/bitcoin\n\n; Use Bitcoin's main network.\n; bitcoin.mainnet=false\n\n; Use Bitcoin's test network.\n; bitcoin.testnet=false\n;\n; Use Bitcoin's simulation test network\n; bitcoin.simnet=false\n\n; Use Bitcoin's regression test network\n; bitcoin.regtest=false\n\n; Use Bitcoin's signet test network\n; bitcoin.signet=false\n\n; Connect to a custom signet network defined by this challenge instead of using\n; the global default signet test network -- Can be specified multiple times\n; bitcoin.signetchallenge=\n\n; Specify a seed node for the signet network instead of using the global default\n; signet network seed nodes\n; Default:\n;   bitcoin.signetseednode=\n; Example:\n;   bitcoin.signetseednode=123.45.67.89\n\n; Specify the chain back-end. Options are btcd, bitcoind and neutrino.\n;\n; NOTE: Please note that switching between a full back-end (btcd/bitcoind) and\n; a light back-end (neutrino) is not supported.\n; Default:\n;   bitcoin.node=btcd\n; Example:\n;   bitcoin.node=bitcoind\n;   bitcoin.node=neutrino\n\n; The default number of confirmations a channel must have before it's considered\n; open. We'll require any incoming channel requests to wait this many\n; confirmations before we consider the channel active. If this is not set, we\n; will scale the value linear to the channel size between 3 and 6. \n; The maximmum value of 6 confs is applied to all channels larger than \n; wumbo size (16777215 sats). The minimum value of 3 is applied to all channels\n; smaller than 8388607 sats (16777215 * 3 / 6).\n; Default:\n;   bitcoin.defaultchanconfs=[3; 6]\n; Example:\n;   bitcoin.defaultchanconfs=3\n\n; The default number of blocks we will require our channel counterparty to wait\n; before accessing its funds in case of unilateral close. If this is not set, we\n; will scale the value linear to the channel size between 144 and 2016. \n; The maximum value of 2016 blocks is applied to all channels larger than \n; wumbo size (16777215). The minimum value of 144 is applied to all channels\n; smaller than 1198372 sats (16777215 * 144 / 2016).\n; Default:\n;   bitcoin.defaultremotedelay=[144; 2016]\n; Example:\n;   bitcoin.defaultremotedelay=144\n\n; The maximum number of blocks we will limit the wait that our own funds are\n; encumbered by in the case when our node unilaterally closes. If a remote peer\n; proposes a channel with a delay above this amount, lnd will reject the\n; channel.\n; bitcoin.maxlocaldelay=2016\n\n; The smallest HTLC we are willing to accept on our channels, in millisatoshi.\n; bitcoin.minhtlc=1\n\n; The smallest HTLC we are willing to send out on our channels, in millisatoshi.\n; bitcoin.minhtlcout=1000\n\n; The base fee in millisatoshi we will charge for forwarding payments on our\n; channels.\n; bitcoin.basefee=1000\n\n; The fee rate used when forwarding payments on our channels. The total fee\n; charged is basefee + (amount * feerate / 1000000), where amount is the\n; forwarded amount.\n; bitcoin.feerate=1\n\n; The CLTV delta we will subtract from a forwarded HTLC's timelock value.\n; bitcoin.timelockdelta=80\n\n; The seed DNS server(s) to use for initial peer discovery. Must be specified as\n; a '<primary_dns>[,<soa_primary_dns>]' tuple where the SOA address is needed\n; for DNS resolution through Tor but is optional for clearnet users. Multiple\n; tuples can be specified, will overwrite the default seed servers.\n; The default seed servers are:\n; Default:\n;  mainnet:\n;   bitcoin.dnsseed=nodes.lightning.directory,soa.nodes.lightning.directory\n;   bitcoin.dnsseed=lseed.bitcoinstats.com\n;  testnet:\n;   bitcoin.dnsseed=test.nodes.lightning.directory,soa.nodes.lightning.directory\n;\n; Example for custom DNS servers:\n;   bitcoin.dnsseed=seed1.test.lightning\n;   bitcoin.dnsseed=seed2.test.lightning,soa.seed2.test.lightning\n\n\n[Btcd]\n\n; The base directory that contains the node's data, logs, configuration file,\n; etc.\n; btcd.dir=~/.btcd\n\n; The host that your local btcd daemon is listening on. By default, this\n; setting is assumed to be localhost with the default port for the current\n; network.\n; btcd.rpchost=localhost\n\n; Username for RPC connections to btcd. By default, lnd will attempt to\n; automatically obtain the credentials, so this likely won't need to be set\n; (other than for simnet mode).\n; Default:\n;   btcd.rpcuser=\n; Example:\n;   btcd.rpcuser=kek\n\n; Password for RPC connections to btcd. By default, lnd will attempt to\n; automatically obtain the credentials, so this likely won't need to be set\n; (other than for simnet mode).\n; Default:\n;   btcd.rpcpass=\n; Example:\n;   btcd.rpcpass=kek\n\n; File containing the daemon's certificate file. This only needs to be set if\n; the node isn't on the same host as lnd.\n; btcd.rpccert=~/.btcd/rpc.cert\n\n; The raw bytes of the daemon's PEM-encoded certificate chain which will be used\n; to authenticate the RPC connection. This only needs to be set if the btcd\n; node is on a remote host.\n; btcd.rawrpccert=\n\n\n[Bitcoind]\n\n; The base directory that contains the node's data, logs, configuration file,\n; etc.\n; bitcoind.dir=~/.bitcoin\n\n; Configuration filepath.\n; Default:\n;   bitcoind.config=\n; Example:\n;   bitcoind.config=~/.bitcoin/bitcoin.conf\n\n; Authentication cookie file for RPC connections.\n; Default:\n;   bitcoind.rpccookie=\n; Example:\n;   bitcoind.rpccookie=~/.bitcoin/.cookie\n\n; The host that your local bitcoind daemon is listening on. By default, this\n; setting is assumed to be localhost with the default port for the current\n; network.\n; bitcoind.rpchost=localhost\n\n; Username for RPC connections to bitcoind. By default, lnd will attempt to\n; automatically obtain the credentials, so this likely won't need to be set\n; (other than for a remote bitcoind instance).\n; Default:\n;   bitcoind.rpcuser=\n; Example:\n;   bitcoind.rpcuser=kek\n\n; Password for RPC connections to bitcoind. By default, lnd will attempt to\n; automatically obtain the credentials, so this likely won't need to be set\n; (other than for a remote bitcoind instance).\n; Default:\n;   bitcoind.rpcpass=\n; Example:\n;   bitcoind.rpcpass=kek\n\n; ZMQ socket which sends rawblock and rawtx notifications from bitcoind. By\n; default, lnd will attempt to automatically obtain this information, so this\n; likely won't need to be set (other than for a remote bitcoind instance).\n; Default:\n;   bitcoind.zmqpubrawblock=\n; Example:\n;   bitcoind.zmqpubrawblock=tcp://127.0.0.1:28332\n\n; Default:\n;   bitcoind.zmqpubrawtx=\n; Example:\n;   bitcoind.zmqpubrawtx=tcp://127.0.0.1:28333\n\n; Default:\n;   bitcoind.zmqreaddeadline=5s\n\n; Use bitcoind's rpc interface to get block and transaction notifications\n; instead of using the zmq interface. Only the rpcpolling option needs to\n; be set in order to enable this, the rest of the options can be used to\n; change the default values used for this configuration.\n; bitcoind.rpcpolling=false\n\n; Default:\n;   bitcoind.blockpollinginterval=0s\n; Example:\n;   bitcoind.blockpollinginterval=1m\n\n; Default:\n;   bitcoind.txpollinginterval=0s\n; Example:\n;   bitcoind.txpollinginterval=30s\n\n; Fee estimate mode for bitcoind. It must be either \"ECONOMICAL\" or \"CONSERVATIVE\".\n; If unset, the default value is \"CONSERVATIVE\".\n; bitcoind.estimatemode=CONSERVATIVE\n\n; The maximum number of peers lnd will choose from the backend node to retrieve\n; pruned blocks from. This only applies to pruned nodes.\n; bitcoind.pruned-node-max-peers=4\n\n\n[neutrino]\n\n; Connect only to the specified peers at startup. This creates a persistent\n; connection to a target peer. This is recommended as there aren't many\n; neutrino compliant full nodes on the test network yet.\n; neutrino.connect=\n\n; Max number of inbound and outbound peers.\n;\n; NOTE: This value is currently unused.\n; neutrino.maxpeers=\n\n; Add a peer to connect with at startup.\n; neutrino.addpeer=\n\n; How long to ban misbehaving peers. Valid time units are {s, m, h}. Minimum 1\n; second.\n;\n; NOTE: This value is currently unused.\n; neutrino.banduration=\n\n; Maximum allowed ban score before disconnecting and banning misbehaving peers.\n;\n; NOTE: This value is currently unused.\n; neutrino.banthreshold=\n\n; Optional filter header in height:hash format to assert the state of neutrino's\n; filter header chain on startup. If the assertion does not hold, then the\n; filter header chain will be re-synced from the genesis block.\n; neutrino.assertfilterheader=\n\n; Used to help identify ourselves to other bitcoin peers.\n; neutrino.useragentname=neutrino\n\n; Used to help identify ourselves to other bitcoin peers.\n; neutrino.useragentversion=0.12.0-beta\n\n; The amount of time to wait before giving up on a transaction broadcast attempt.\n; Default:\n;   neutrino.broadcasttimeout=0s\n; Example:\n;   neutrino.broadcasttimeout=5s\n\n; Whether compact filters fetched from the P2P network should be persisted to disk.\n; neutrino.persistfilters=false\n\n; Validate every channel in the graph during sync by downloading the containing\n; block. This is the inverse of routing.assumechanvalid, meaning that for\n; Neutrino the validation is turned off by default for massively increased graph\n; sync performance. This speedup comes at the risk of using an unvalidated view\n; of the network for routing. Overwrites the value of routing.assumechanvalid if\n; Neutrino is used. \n; neutrino.validatechannels=false\n\n[autopilot]\n\n; If the autopilot agent should be active or not. The autopilot agent will\n; attempt to automatically open up channels to put your node in an advantageous\n; position within the network graph.\n; autopilot.active=false\n\n; The maximum number of channels that should be created.\n; autopilot.maxchannels=5\n\n; The fraction of total funds that should be committed to automatic channel\n; establishment. For example 0.6 means that 60% of the total funds available\n; within the wallet should be used to automatically establish channels. The total\n; amount of attempted channels will still respect the maxchannels param.\n; autopilot.allocation=0.6\n\n; Heuristic to activate, and the weight to give it during scoring. \n; Default:\n;   autopilot.heuristic={top_centrality:1}\n; Example:\n;   autopilot.heuristic={preferential:1}\n\n; The smallest channel that the autopilot agent should create \n; autopilot.minchansize=20000\n\n; The largest channel that the autopilot agent should create \n; autopilot.maxchansize=16777215\n\n; Whether the channels created by the autopilot agent should be private or not.\n; Private channels won't be announced to the network.\n; autopilot.private=false\n\n; The minimum number of confirmations each of your inputs in funding transactions\n; created by the autopilot agent must have. \n; autopilot.minconfs=1\n\n; The confirmation target (in blocks) for channels opened by autopilot.\n; autopilot.conftarget=3\n\n\n[tor]\n\n; Allow outbound and inbound connections to be routed through Tor.\n; tor.active=false\n\n; Allow the node to connect to non-onion services directly via clearnet. This\n; allows the node operator to use direct connections to peers not running behind\n; Tor, thus allowing lower latency and better connection stability.\n; WARNING: This option will reveal the source IP address of the node, and should\n; be used only if privacy is not a concern.\n; tor.skip-proxy-for-clearnet-targets=false\n\n; The port that Tor's exposed SOCKS5 proxy is listening on. Using Tor allows\n; outbound-only connections (listening will be disabled) -- NOTE port must be\n; between 1024 and 65535.\n; Default:\n;   tor.socks=localhost:9050\n; Example:\n;   tor.socks=9050\n\n; The DNS server as IP:PORT that Tor will use for SRV queries - NOTE must have\n; TCP resolution enabled. The current active DNS server for Testnet listening is\n; nodes.lightning.directory.\n; Default:\n;   tor.dns=soa.nodes.lightning.directory:53\n; Example:\n;   tor.dns=nodes.lightning.directory\n\n; Enable Tor stream isolation by randomizing user credentials for each\n; connection. With this mode active, each connection will use a new circuit.\n; This means that multiple applications (other than lnd) using Tor won't be mixed\n; in with lnd's traffic.\n;\n; This option may not be used while direct connections are enabled, since direct\n; connections compromise source IP privacy by default.\n; tor.streamisolation=false\n\n; The host:port that Tor is listening on for Tor control connections.\n; tor.control=localhost:9051\n\n; IP address that Tor should use as the target of the hidden service.\n; tor.targetipaddress=\n\n; The password used to arrive at the HashedControlPassword for the control port.\n; If provided, the HASHEDPASSWORD authentication method will be used instead of\n; the SAFECOOKIE one.\n; Default:\n;   tor.password=\n; Example:\n;   tor.password=plsdonthackme\n\n; Automatically set up a v2 onion service to listen for inbound connections.\n; tor.v2=false\n\n; Automatically set up a v3 onion service to listen for inbound connections.\n; tor.v3=false\n\n; The path to the private key of the onion service being created.\n; Default:\n;   tor.privatekeypath=\n; Example:\n;   tor.privatekeypath=/path/to/torkey\n\n; The path to the private key of the watchtower onion service being created.\n; Default:\n;   tor.watchtowerkeypath=\n; Example:\n;   tor.watchtowerkeypath=/other/path/\n\n; Instructs lnd to encrypt the private key using the wallet's seed.\n; tor.encryptkey=false\n\n[logging]\n\n; Whether to exclude the current build's commit hash from log lines. Note that\n; the commit hash will not currently show up in all LND log lines as this new\n; feature will take a few versions to propagate through the codebase.\n; logging.no-commit-hash=false\n\n; Disable logging to stdout and stderror.\n; logging.console.disable=false\n\n; Don't add timestamps to logs written to stdout and stderr.\n; logging.console.no-timestamps=false\n\n; Include the log call-site in the log line written to stdout\n; and stderr. Options include 'off', 'short' and 'long'.\n; Default:\n;   logging.console.call-site=off\n; Example:\n;   logging.console.call-site=short\n\n; Disable logging to the standard LND log file.\n; logging.file.disable=false\n\n; Number of log files that the log rotation should keep. Setting\n; it to 0 disables deletion of old log files.\n; logging.file.max-files=10\n\n; Max log file size in MB before it is rotated.\n; logging.file.max-file-size=20\n\n; Compression algorithm to use when rotating logs.\n; Default:\n;   logging.file.compressor=gzip\n; Example:\n;   logging.file.compressor=zstd\n\n; Don't add timestamps to logs written to the standard LND log file.\n; logging.file.no-timestamps=false\n\n; Include the log call-site in the log line written the standard LND\n; log file. Options include 'off', 'short' and 'long'.\n; Default:\n;   logging.file.call-site=off\n; Example:\n;   logging.file.call-site=short\n\n[watchtower]\n\n; Enable integrated watchtower listening on :9911 by default.\n; watchtower.active=false\n\n; Specify the interfaces to listen on for watchtower client connections. One\n; listen address per line. If no port is specified the default port of 9911 will\n; be added implicitly.\n; Default:\n;   watchtower.listen=\n; Example (option can be specified multiple times):\n; All ipv4 on port 9911:\n;   watchtower.listen=0.0.0.0:9911\n; On all ipv4 interfaces on port 9911 and ipv6 localhost port 9912:\n;   watchtower.listen=0.0.0.0:9911\n;   watchtower.listen=[::1]:9912\n\n; Configure the external IP address of your watchtower. Setting this field does\n; not have any behavioral changes to the tower or enable any sort of discovery,\n; however it will make the full URI (pubkey@host:port) available via\n; WatchtowerRPC.GetInfo and `lncli tower info`.\n; Default:\n;   watchtower.externalip=\n; Example:\n;   watchtower.externalip=1.2.3.4\n\n; Configure the default watchtower data directory. The default directory is\n; data/watchtower relative to the chosen lnddir. This can be useful if one needs\n; to move the database to a separate volume with more storage. \n; Default:\n;   watchtower.towerdir=~/.lnd/data/watchtower\n; Example:\n;   watchtower.towerdir=/path/to/towerdir\n\n;   In this example, the database will be stored at: \n;   /path/to/towerdir/bitcoin/<network>/watchtower.db\n    \n; Duration the watchtower server will wait for messages to be received before\n; hanging up on client connections.\n; watchtower.readtimeout=15s\n\n; Duration the watchtower server will wait for messages to be written before\n; hanging up on client connections\n; watchtower.writetimeout=15s\n\n\n[wtclient]\n\n; Activate Watchtower Client. To get more information or configure watchtowers\n; run `lncli wtclient -h`.\n; wtclient.active=false\n\n; Specify the fee rate with which justice transactions will be signed. This fee\n; rate should be chosen as a maximum fee rate one is willing to pay in order to\n; sweep funds if a breach occurs while being offline. The fee rate should be\n; specified in sat/vbyte.\n; wtclient.sweep-fee-rate=10\n\n; The range over which to choose a random number of blocks to wait after the\n; last channel of a session is closed before sending the DeleteSession message\n; to the tower server. Note that setting this to a lower value will result in\n; faster session cleanup _but_ that this comes along with reduced privacy from\n; the tower server.\n; wtclient.session-close-range=288\n\n; The maximum number of updates to include in a tower session.\n; wtclient.max-updates=1024\n\n; The maximum number of back-up tasks that should be queued in memory before\n; overflowing to disk.\n; wtclient.max-tasks-in-mem-queue=2000\n\n\n[healthcheck]\n\n; The number of times we should attempt to query our chain backend before\n; gracefully shutting down. Set this value to 0 to disable this health check.\n; healthcheck.chainbackend.attempts=3\n\n; The amount of time we allow a call to our chain backend to take before we fail\n; the attempt. This value must be >= 1s.\n; healthcheck.chainbackend.timeout=30s\n\n; The amount of time we should backoff between failed attempts to query chain\n; backend. This value must be >= 1s.\n; healthcheck.chainbackend.backoff=2m\n\n; The amount of time we should wait between chain backend health checks. This\n; value must be >= 1m.\n; healthcheck.chainbackend.interval=1m\n\n; The minimum ratio of free disk space to total capacity that we require.\n; healthcheck.diskspace.diskrequired=0.1\n\n; The number of times we should attempt to query our available disk space before\n; gracefully shutting down. Set this value to 0 to disable this health check.\n; Default:\n;   healthcheck.diskspace.attempts=0\n; Example:\n;   healthcheck.diskspace.attempts=2\n\n; The amount of time we allow a query for our available disk space to take\n; before we fail the attempt. This value must be >= 1s.\n; healthcheck.diskspace.timeout=5s\n\n; The amount of time we should backoff between failed attempts to query\n; available disk space. This value must be >= 1s.\n; healthcheck.diskspace.backoff=1m\n\n; The amount of time we should wait between disk space health checks. This\n; value must be >= 1m.\n; healthcheck.diskspace.interval=12h\n\n; The number of times we should attempt to check for certificate expiration before\n; gracefully shutting down. Set this value to 0 to disable this health check.\n; Default:\n;   healthcheck.tls.attempts=\n; Example:\n;   healthcheck.tls.attempts=2\n\n; The amount of time we allow a query for certificate expiration to take\n; before we fail the attempt. This value must be >= 1s.\n; healthcheck.tls.timeout=5s\n\n; The amount of time we should backoff between failed attempts to query\n; certificate expiration. This value must be >= 1s.\n; healthcheck.tls.backoff=1m\n\n; The amount of time we should wait between certificate expiration health checks.\n; This value must be >= 1m.\n; healthcheck.tls.interval=1m\n\n; The number of times we should attempt to check our tor connection before\n; gracefully shutting down. Set this value to 0 to disable this health check.\n; Default:\n;   healthcheck.torconnection.attempts=\n; Example:\n;   healthcheck.torconnection.attempts=3\n\n; The amount of time we allow a call to our tor connection to take before we\n; fail the attempt. This value must be >= 1s.\n; Default:\n;   healthcheck.torconnection.timeout=5s\n\n; The amount of time we should backoff between failed attempts to check tor\n; connection. This value must be >= 1s.\n; healthcheck.torconnection.backoff=1m\n\n; The amount of time we should wait between tor connection health checks. This\n; value must be >= 1m.\n; healthcheck.torconnection.interval=1m\n\n; The number of times we should attempt to check our remote signer RPC\n; connection before gracefully shutting down. Set this value to 0 to disable\n; this health check.\n; healthcheck.remotesigner.attempts=1\n\n; The amount of time we allow a call to our remote signer RPC connection to take\n; before we fail the attempt. This value must be >= 1s.\n; healthcheck.remotesigner.timeout=1s\n\n; The amount of time we should backoff between failed attempts to check remote\n; signer RPC connection. This value must be >= 1s.\n; healthcheck.remotesigner.backoff=30s\n\n; The amount of time we should wait between remote signer RPC connection health\n; checks. This value must be >= 1m.\n; healthcheck.remotesigner.interval=1m\n\n; The number of times we should attempt to check the node's leader status\n; before gracefully shutting down. Set this value to 0 to disable this health \n; check.\n; healthcheck.leader.attempts=1\n\n; The amount of time after the leader check times out due to unanswered RPC.\n; This value must be >= 1s.\n; healthcheck.leader.timeout=5s\n\n; The amount of time we should backoff between failed attempts of leader checks.\n; This value must be >= 1s.\n; healthcheck.leader.backoff=5s\n\n; The amount of time we should wait between leader checks. \n; This value must be >= 1m.\n; healthcheck.leader.interval=1m\n\n\n\n[signrpc]\n\n; Path to the signer macaroon.\n; Default:\n;   signrpc.signermacaroonpath=~/.lnd/data/chain/bitcoin/${network}/signer.macaroon\n; Example:\n;   signrpc.signermacaroonpath=~/.lnd/data/chain/bitcoin/mainnet/signer.macaroon\n\n\n[walletrpc]\n\n; Path to the wallet kit macaroon.\n; Default:\n;   walletrpc.walletkitmacaroonpath=~/.lnd/data/chain/bitcoin/${network}/walletkit.macaroon\n; Example:\n;   walletrpc.walletkitmacaroonpath=~/.lnd/data/chain/bitcoin/mainnet/walletkit.macaroon\n\n\n[chainrpc]\n\n; Path to the chain notifier macaroon.\n; Default:\n;   chainrpc.notifiermacaroonpath=~/.lnd/data/chain/bitcoin/${network}/chainnotifier.macaroon\n; Example:\n;   chainrpc.notifiermacaroonpath=~/.lnd/data/chain/bitcoin/mainnet/chainnotifier.macaroon\n\n\n[routerrpc]\n\n; Probability estimator used for pathfinding. Two estimators are available:\n; apriori and bimodal.\n; Note that the bimodal estimator is experimental.\n; Default:\n;   routerrpc.estimator=apriori\n; Example:\n;   routerrpc.estimator=bimodal\n\n; Minimum required route success probability to attempt the payment.\n; routerrpc.minrtprob=0.01\n\n; The maximum number of payment results that are held on disk by mission control.\n; routerrpc.maxmchistory=1000\n\n; The time interval with which the MC store state is flushed to the database.\n; routerrpc.mcflushinterval=1s\n\n; Path to the router macaroon.\n; Default:\n;   routerrpc.routermacaroonpath=~/.lnd/data/chain/bitcoin/${network}/router.macaroon\n; Example:\n;   routerrpc.routermacaroonpath=~/.lnd/data/chain/bitcoin/mainnet/router.macaroon\n\n; The (virtual) fixed cost in sats of a failed payment attempt .\n; routerrpc.attemptcost=100\n\n; The (virtual) proportional cost in ppm of the total amount of a failed payment\n; attempt.\n; routerrpc.attemptcostppm=1000\n\n; Assumed success probability of a hop in a route when no other information is\n; available. \n; routerrpc.apriori.hopprob=0.6\n\n; Weight of the a priori probability in success probability estimation. Valid\n; values are in [0, 1]. \n; routerrpc.apriori.weight=0.5\n\n; Defines the duration after which a penalized node or channel is back at 50%\n; probability.\n; routerrpc.apriori.penaltyhalflife=1h\n\n; Defines the fraction of channels' capacities that is considered liquid in\n; pathfinding, a value between [0.75-1.0]. A value of 1.0 disables this\n; feature. \n; routerrpc.apriori.capacityfraction=0.9999\n\n; Describes the scale over which channels still have some liquidity left on\n; both channel ends. A very low value (compared to typical channel capacities)\n; means that we assume unbalanced channels, a very high value means randomly\n; balanced channels. Value in msat. \n; routerrpc.bimodal.scale=300000000\n\n; Defines how strongly non-routed channels of forwarders should be taken into\n; account for probability estimation. A weight of zero disables this feature.\n; Valid values are in [0, 1]. \n; routerrpc.bimodal.nodeweight=0.2\n\n; Defines the information decay of knowledge about previous successes and\n; failures in channels. \n; routerrpc.bimodal.decaytime=168h\n\n; If set, the router will send `Payment_INITIATED` for new payments, otherwise\n; `Payment_In_FLIGHT` will be sent for compatibility concerns.\n; routerrpc.usestatusinitiated=false\n\n; Defines the maximum duration that the probing fee estimation is allowed to\n; take.\n; routerrpc.fee-estimation-timeout=1m\n\n[workers]\n\n; Maximum number of concurrent read pool workers. This number should be\n; proportional to the number of peers. \n; workers.read=100\n\n; Maximum number of concurrent write pool workers. This number should be\n; proportional to the number of CPUs on the host. \n; workers.write=8\n\n; Maximum number of concurrent sig pool workers. This number should be\n; proportional to the number of CPUs on the host. \n; workers.sig=8\n\n\n[caches]\n\n; Maximum number of entries contained in the reject cache, which is used to speed\n; up filtering of new channel announcements and channel updates from peers. Each\n; entry requires 25 bytes. \n; caches.reject-cache-size=50000\n\n; Maximum number of entries contained in the channel cache, which is used to\n; reduce memory allocations from gossip queries from peers. Each entry requires\n; roughly 2Kb. \n; caches.channel-cache-size=20000\n\n; The duration that the response to DescribeGraph should be cached for. Setting\n; the value to zero disables the cache. \n; Default:\n;   caches.rpc-graph-cache-duration=\n; Example:\n;   caches.rpc-graph-cache-duration=10m\n\n\n[protocol]\n\n; If set, then lnd will create and accept requests for channels larger than 0.16\n; BTC\n; protocol.wumbo-channels=false\n\n; Set to disable support for anchor commitments. If not set, lnd will use anchor\n; channels by default if the remote channel party supports them. Note that lnd\n; will require 1 UTXO to be reserved for this channel type if it is enabled.\n; (Deprecates the previous \"protocol.anchors\" setting.)\n; protocol.no-anchors=false\n\n; Set to disable support for script enforced lease channel commitments. If not\n; set, lnd will accept these channels by default if the remote channel party\n; proposes them. Note that lnd will require 1 UTXO to be reserved for this\n; channel type if it is enabled.\n; protocol.no-script-enforced-lease=false\n\n; Set to enable support for option_scid_alias channels, which can be referred\n; to by an alias instead of the confirmed ShortChannelID. Additionally, is\n; needed to open zero-conf channels.\n; protocol.option-scid-alias=false\n\n; Set to enable support for zero-conf channels. This requires the\n; option-scid-alias flag to also be set.\n; protocol.zero-conf=false\n\n; Set to disable support for using P2TR addresses (and beyond) for co-op\n; closing.\n; protocol.no-any-segwit=false\n\n; Set to disable querying our peers for the timestamps of announcement\n; messages and to disable responding to such queries\n; protocol.no-timestamp-query-option=false\n\n; Set to enable support for the experimental taproot channel type.\n; protocol.simple-taproot-chans=false\n\n; Set to enable support for the experimental taproot overlay channel type.\n; protocol.simple-taproot-overlay-chans=false\n\n; Set to disable blinded route forwarding.\n; protocol.no-route-blinding=false\n\n; Set to disable experimental endorsement signaling.\n; protocol.no-experimental-endorsement=false\n\n; Set to handle messages of a particular type that falls outside of the\n; custom message number range (i.e. 513 is onion messages). Note that you can\n; set this option as many times as you want to support more than one custom\n; message type.\n; Default:\n;   protocol.custom-message=\n; Example:\n;   protocol.custom-message=513\n\n; Specifies feature bits — numbers defined in BOLT 9 — to advertise in the\n; node's init message. Note that you can set this option as many times as you\n; want to support more than one feature bit.\n; Default:\n;   protocol.custom-init=\n; Example:\n;   protocol.custom-init=39\n\n; Specifies custom feature bits — numbers defined in BOLT 9 — to advertise in\n; the node's announcement message. Note that you can set this option as many\n; times as you want to support more than one feature bit.\n; Default:\n;   protocol.custom-nodeann=\n; Example:\n;   protocol.custom-nodeann=39\n\n; Specifies custom feature bits — numbers defined in BOLT 9 — to advertise in\n; the node's invoices. Note that you can set this option as many times as you\n; want to support more than one feature bit.\n; Default:\n;   protocol.custom-invoice=\n; Example:\n;   protocol.custom-invoice=39\n\n[db]\n\n; The selected database backend. The current default backend is \"bolt\". lnd\n; also has experimental support for etcd, a replicated backend, postgres and\n; sqlite.\n; db.backend=bolt\n\n; The maximum interval the graph database will wait between attempting to flush\n; a batch of modifications to disk.\n; db.batch-commit-interval=500ms\n\n; Don't use the in-memory graph cache for path finding. Much slower but uses\n; less RAM. Can only be used with a bolt database backend.\n; db.no-graph-cache=false\n\n; Specify whether the optional migration for pruning old revocation logs\n; should be applied. This migration will only save disk space if there are open\n; channels prior to lnd@v0.15.0.\n; db.prune-revocation=false\n\n; If set to true, then the to-local and to-remote output amount data of revoked\n; commitment transactions will not be stored in the revocation log. Note that\n; this flag can only be set if --wtclient.active is not set. It is not\n; recommended to set this flag if you plan on ever setting wtclient.active in\n; the future.\n; db.no-rev-log-amt-data=false\n\n; If set to true, native SQL will be used instead of KV emulation for tables\n; that support it already. Note: this is an experimental feature, use at your\n; own risk.\n; db.use-native-sql=false\n\n\n[etcd]\n\n; Etcd database host. Supports multiple hosts separated by a comma.\n; Default:\n;   db.etcd.host=\n; Example:\n;   db.etcd.host=localhost:2379\n\n; Etcd database user.\n; Default:\n;   db.etcd.user=\n; Example:\n;   db.etcd.user=userscopedforlnd\n\n; Password for the database user.\n; Default:\n;   db.etcd.pass=\n; Example:\n;   db.etcd.pass=longandsekrit\n\n; Etcd namespace to use.\n; Default:\n;   db.etcd.namespace=\n; Example:\n;   db.etcd.namespace=lnd\n\n; Whether to disable the use of TLS for etcd.\n; db.etcd.disabletls=false\n\n; Path to the TLS certificate for etcd RPC.\n; Default:\n;   db.etcd.cert_file=\n; Example:\n;   db.etcd.cert_file=/key/path\n\n; Path to the TLS private key for etcd RPC.\n; Default:\n;   db.etcd.key_file=\n; Example:\n;   db.etcd.key_file=/a/path\n\n; Whether we intend to skip TLS verification\n; db.etcd.insecure_skip_verify=false\n\n; Whether to collect etcd commit stats.\n; db.etcd.collect_stats=false\n\n; If set LND will use an embedded etcd instance instead of the external one.\n; Useful for testing.\n; db.etcd.embedded=false\n\n; If non zero, LND will use this as client port for the embedded etcd instance.\n; Default:\n;   db.etcd.embedded_client_port=\n; Example:\n;   db.etcd.embedded_client_port=1234\n\n; If non zero, LND will use this as peer port for the embedded etcd instance.\n; Default:\n;   db.etcd.embedded_peer_port=\n; Example:\n;   db.etcd.embedded_peer_port=1235\n\n; If set the embedded etcd instance will log to the specified file. Useful when\n; testing with embedded etcd.\n; Default:\n;   db.etcd.embedded_log_file=\n; Example:\n;   db.etcd.embedded_log_file=/path/etcd.log\n\n; The maximum message size in bytes that we may send to etcd. Defaults to 32 MiB.\n; db.etcd.max_msg_size=33554432\n\n\n[postgres]\n\n; Postgres connection string.\n; Default:\n;   db.postgres.dsn=\n; Example:\n;   db.postgres.dsn=postgres://lnd:lnd@localhost:45432/lnd?sslmode=disable\n\n; Postgres connection timeout. Valid time units are {s, m, h}. Set to zero to\n; disable.\n; db.postgres.timeout=\n\n; Postgres maximum number of connections. Set to zero for unlimited. It is\n; recommended to set a limit that is below the server connection limit.\n; Otherwise errors may occur in lnd under high-load conditions.\n; Default:\n;   db.postgres.maxconnections=50\n; Example:\n;   db.postgres.maxconnections=\n\n; Whether to skip executing schema migrations.\n; db.postgres.skipmigrations=false\n\n\n[sqlite]\n\n; Sqlite connection timeout. Valid time units are {s, m, h}. Set to zero to\n; disable.\n; Default:\n;   db.sqlite.timeout=\n; Example:\n;   db.sqlite.timeout=0s\n\n; Maximum number of connections to the sqlite db. Set to zero for unlimited.\n; db.sqlite.maxconnections=2\n\n; The maximum amount of time to wait to execute a query if the db is locked.\n; db.sqlite.busytimeout=5s\n\n; Raw pragma option pairs to be used when opening the sqlite db. The flag\n; can be specified multiple times to set multiple options.\n; Default:\n;   db.sqlite.pragmaoptions=\n; Example (option can be specified multiple times):\n;   db.sqlite.pragmaoptions=auto_vacuum=incremental\n;   db.sqlite.pragmaoptions=temp_store=MEMORY\n\n; Whether to skip executing schema migrations.\n; db.sqlite.skipmigrations=false\n\n[bolt]\n\n; If true, prevents the database from syncing its freelist to disk.\n; db.bolt.nofreelistsync=false\n;\n; Whether the databases used within lnd should automatically be compacted on\n; every startup (and if the database has the configured minimum age). This is\n; disabled by default because it requires additional disk space to be available\n; during the compaction that is freed afterwards. In general compaction leads to\n; smaller database files.\n; db.bolt.auto-compact=false\n\n; How long ago the last compaction of a database file must be for it to be\n; considered for auto compaction again. Can be set to 0 to compact on every\n; startup. \n; Default:\n;   db.bolt.auto-compact-min-age=168h\n; Example:\n;   db.bolt.auto-compact-min-age=0\n\n; Specify the timeout to be used when opening the database.\n; db.bolt.dbtimeout=1m\n\n\n[cluster]\n\n; Enables leader election if set.\n; cluster.enable-leader-election=false\n\n; Leader elector to use. Valid values: \"etcd\".\n; cluster.leader-elector=etcd\n\n; Election key prefix when using etcd leader elector.\n; cluster.etcd-election-prefix=/leader/\n\n; Identifier for this node inside the cluster (used in leader election).\n; Defaults to the hostname.\n; cluster.id=example.com\n\n; The session TTL in seconds after which a new leader is elected if the old\n; leader is shut down, crashed or becomes unreachable.\n; cluster.leader-session-ttl=90\n\n\n[rpcmiddleware]\n\n; Enable the RPC middleware interceptor functionality.\n; rpcmiddleware.enable=false\n\n; Time after which a RPC middleware intercept request will time out and return\n; an error if it hasn't yet received a response.\n; rpcmiddleware.intercepttimeout=2s\n\n; Add the named middleware to the list of mandatory middlewares. All RPC\n; requests are blocked/denied if any of the mandatory middlewares is not\n; registered. Can be specified multiple times.\n; Default:\n;   rpcmiddleware.addmandatory=\n; Example:\n;   rpcmiddleware.addmandatory=my-example-middleware\n;   rpcmiddleware.addmandatory=other-mandatory-middleware\n\n\n[remotesigner]\n\n; Use a remote signer for signing any on-chain related transactions or messages.\n; Only recommended if local wallet is initialized as watch-only. Remote signer\n; must use the same seed/root key as the local watch-only wallet but must have\n; private keys.\n; remotesigner.enable=false\n\n; The remote signer's RPC host:port.\n; Default:\n;   remotesigner.rpchost=\n; Example:\n;   remotesigner.rpchost=remote.signer.lnd.host:10009\n\n; The macaroon to use for authenticating with the remote signer.\n; Default:\n;   remotesigner.macaroonpath=\n; Example:\n;   remotesigner.macaroonpath=/path/to/remote/signer/admin.macaroon\n\n; The TLS certificate to use for establishing the remote signer's identity.\n; Default:\n;   remotesigner.tlscertpath=\n; Example:\n;   remotesigner.tlscertpath=/path/to/remote/signer/tls.cert\n\n; The timeout for connecting to and signing requests with the remote signer.\n; Valid time units are {s, m, h}.\n; remotesigner.timeout=5s\n\n; If a wallet with private key material already exists, migrate it into a\n; watch-only wallet on first startup.\n; WARNING: This cannot be undone! Make sure you have backed up your seed before\n; you use this flag! All private keys will be purged from the wallet after first\n; unlock with this flag!\n; remotesigner.migrate-wallet-to-watch-only=false\n\n\n[gossip]\n\n; Specify a set of pinned gossip syncers, which will always be actively syncing\n; whenever the corresponding peer is online. A pinned syncer does not count\n; towards the configured `numgraphsyncpeers` since pinned syncers are not\n; rotated. Configuring a pinned syncer does not ensure a persistent connection\n; to the target peer, they will only be pinned if the connection remains active\n; via some other mechanism, e.g. having an open channel.\n;\n; This feature is useful when trying to ensure that a node keeps its\n; routing table tightly synchronized with a set of remote peers, e.g. multiple\n; lightning nodes operated by the same service.\n;\n; Each value should be a hex-encoded pubkey of the pinned peer. Multiple pinned\n; peers can be specified by setting multiple flags/fields in the config.\n; Default:\n;   gossip.pinned-syncers=\n; Example:\n;   gossip.pinned-syncers=pubkey1\n;   gossip.pinned-syncers=pubkey2\n\n; The maximum number of updates for a specific channel and direction that lnd\n; will accept over the channel update interval.\n; gossip.max-channel-update-burst=10\n; gossip.channel-update-interval=1m\n\n; The duration to wait before sending the next announcement batch if there are\n; multiple. Use a small value if there are a lot announcements and they need to\n; be broadcast quickly.\n; gossip.sub-batch-delay=5s\n\n\n[invoices]\n\n; If a hold invoice has accepted htlcs that reach their expiry height and are\n; not timed out, the channel holding the htlc is force closed to resolve the\n; invoice's htlcs. To prevent force closes, lnd automatically cancels these\n; invoices before they reach their expiry height.\n;\n; Hold expiry delta describes the number of blocks before expiry that these\n; invoices should be canceled. Setting this value to 0 will ensure that hold\n; invoices can be settled right up until their expiry height, but will result\n; in the channel they are on being force closed if they are not resolved before\n; expiry.\n;\n; Lnd goes to chain before the expiry for a htlc is reached so that there is\n; time to resolve it on chain. This value needs to be greater than the\n; DefaultIncomingBroadcastDelta set by lnd, otherwise the channel will be force\n; closed anyway. A warning will be logged on startup if this value is not large\n; enough to prevent force closes.\n; invoices.holdexpirydelta=12\n\n[routing]\n\n; DEPRECATED: This is now turned on by default for Neutrino (use\n; neutrino.validatechannels=true to turn off) and shouldn't be used for any\n; other backend!\n; routing.assumechanvalid=false\n\n; If set to true, then we'll prune a channel if only a single edge is seen as\n; being stale. This results in a more compact channel graph, and also is helpful\n; for neutrino nodes as it means they'll only maintain edges where both nodes are\n; seen as being live from it's PoV.\n; routing.strictgraphpruning=false\n\n; The minimum number of real (non-dummy) blinded hops to select for a blinded\n; path. This doesn't include our node, so if the maximum is 1, then the\n; shortest paths will contain our node along with an introduction node hop.\n; routing.blinding.min-num-real-hops=1\n\n; The number of hops to include in a blinded path. This does not include\n; our node, so if is is 1, then the path will at least contain our node along\n; with an introduction node hop. If it is 0, then it will use this node as\n; the introduction node. This number must be greater than or equal to the\n; the number of real hops (invoices.blinding.min-num-real-hops). Any paths\n; shorter than this number will be padded with dummy hops.\n; routing.blinding.num-hops=2\n\n; The maximum number of blinded paths to select and add to an invoice.\n; routing.blinding.max-num-paths=3\n\n; The amount by which to increase certain policy values of hops on a blinded\n; path in order to add a probing buffer. The higher this multiplier, the more\n; buffer is added to the policy values of hops along a blinded path meaning\n; that if they were to increase their policy values before the blinded path\n; expires, the better the chances that the path would still be valid meaning\n; that the path is less prone to probing attacks. However, if the multiplier\n; is too high, the resulting buffered fees might be too much for the payer.\n; routing.blinding.policy-increase-multiplier=1.1\n\n; The amount by which to decrease certain policy values of hops on a blinded\n; path in order to add a probing buffer. The lower this multiplier, the more\n; buffer is added to the policy values of hops along a blinded path meaning\n; that if they were to increase their policy values before the blinded path\n; expires, the better the chances that the path would still be valid meaning\n; that the path is less prone to probing attacks. However, since this value\n; is being applied to the MaxHTLC value of the route, the lower it is, the\n; lower payment amount will need to be.\n; routing.blinding.policy-decrease-multiplier=0.9\n\n[sweeper]\n\n; DEPRECATED: Duration of the sweep batch window. The sweep is held back during\n; the batch window to allow more inputs to be added and thereby lower the fee\n; per input.\n; sweeper.batchwindowduration=30s\n\n; The max fee rate in sat/vb which can be used when sweeping funds. Setting\n; this value too low can result in transactions not being confirmed in time,\n; causing HTLCs to expire hence potentially losing funds.\n; sweeper.maxfeerate=1000\n\n; The conf target to use when sweeping non-time-sensitive outputs. This is\n; useful for sweeping outputs that are not time-sensitive, and can be swept at\n; a lower fee rate.\n; sweeper.nodeadlineconftarget=1008\n\n\n; An optional config group that's used for the automatic sweep fee estimation.\n; The Budget config gives options to limits ones fee exposure when sweeping\n; unilateral close outputs and the fee rate calculated from budgets is capped\n; at sweeper.maxfeerate. Check the budget config options for more details.\n; sweeper.budget=\n\n[sweeper.budget]\n\n; The amount in satoshis to allocate as the budget to pay fees when sweeping\n; the to_local output. If set, the budget calculated using the ratio (if set)\n; will be capped at this value.\n; sweeper.budget.tolocal=\n\n; The ratio of the value in to_local output to allocate as the budget to pay\n; fees when sweeping it.\n; sweeper.budget.tolocalratio=0.5\n\n; The amount in satoshis to allocate as the budget to pay fees when CPFPing a\n; force close tx using the anchor output. If set, the budget calculated using\n; the ratio (if set) will be capped at this value.\n; sweeper.budget.anchorcpfp=\n\n; The ratio of a special value to allocate as the budget to pay fees when \n; CPFPing a force close tx using the anchor output. The special value is the\n; sum of all time-sensitive HTLCs on this commitment subtracted by their\n; budgets.\n; sweeper.budget.anchorcpfpratio=0.5\n\n; The amount in satoshis to allocate as the budget to pay fees when sweeping a\n; time-sensitive (first-level) HTLC. If set, the budget calculated using the\n; ratio (if set) will be capped at this value.\n; sweeper.budget.deadlinehtlc=\n\n; The ratio of the value in a time-sensitive (first-level) HTLC to allocate as\n; the budget to pay fees when sweeping it.\n; sweeper.budget.deadlinehtlcratio=0.5\n\n; The amount in satoshis to allocate as the budget to pay fees when sweeping a\n; non-time-sensitive (second-level) HTLC. If set, the budget calculated using\n; the ratio (if set) will be capped at this value.\n; sweeper.budget.nodeadlinehtlc=\n\n; The ratio of the value in a non-time-sensitive (second-level) HTLC to\n; allocate as the budget to pay fees when sweeping it.\n; sweeper.budget.nodeadlinehtlcratio=0.5\n\n[htlcswitch]\n\n; The timeout value when delivering HTLCs to a channel link. Setting this value\n; too small will result in local payment failures if large number of payments\n; are sent over a short period.\n; htlcswitch.mailboxdeliverytimeout=1m\n\n\n[grpc]\n\n; How long the server waits on a gRPC stream with no activity before pinging the\n; client. Valid time units are {s, m, h}.\n; grpc.server-ping-time=1m\n\n; How long the server waits for the response from the client for the keepalive\n; ping response. Valid time units are {s, m, h}.\n; grpc.server-ping-timeout=20s\n\n; The minimum amount of time the client should wait before sending a keepalive\n; ping. Valid time units are {s, m, h}.\n; grpc.client-ping-min-wait=5s\n\n; If true, the server allows keepalive pings from the client even when there are\n; no active gRPC streams. This might be useful to keep the underlying HTTP/2\n; connection open for future requests.\n; grpc.client-allow-ping-without-stream=false\n\n\n[pprof]\n\n; Enable HTTP profiling on given port -- NOTE port must be between 1024 and\n; 65536. The profile can be access at: http://localhost:<PORT>/debug/pprof/.\n; You can also provide it as host:port to enable profiling for remote debugging.\n; For example 0.0.0.0:<PORT> to enable profiling for all interfaces on the given\n; port. The built-in profiler has minimal overhead, so it is recommended to\n; enable it.\n; pprof.profile=\n\n; Write CPU profile to the specified file. This should only be used for \n; debugging because compared to running a pprof server this will record the cpu\n; profile constantly from the start of the program until the shutdown.\n; pprof.cpuprofile=\n\n; Enable a blocking profile to be obtained from the profiling port. A blocking\n; profile can show where goroutines are blocking (stuck on mutexes, I/O, etc).\n; This takes a value from 0 to 1, with 0 turning off the setting, and 1 sampling\n; every blocking event (it's a rate value). The blocking profile has high \n; overhead and is off by default even when running the pprof server. It should\n; only be used for debugging.\n; pprof.blockingprofile=0\n\n; Enable a mutex profile to be obtained from the profiling port. A mutex \n; profile can show where goroutines are blocked on mutexes, and which mutexes\n; have high contention.  This takes a value from 0 to 1, with 0 turning off the\n; setting, and 1 sampling every mutex event (it's a rate value). The mutex\n; profile has high overhead and is off by default even when running the pprof\n; server. It should only be used for debugging.\n; pprof.mutexprofile=0\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "server.go",
          "type": "blob",
          "size": 161.2216796875,
          "content": "package lnd\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto/rand\"\n\t\"encoding/hex\"\n\t\"fmt\"\n\t\"math/big\"\n\tprand \"math/rand\"\n\t\"net\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/btcsuite/btcd/btcec/v2\"\n\t\"github.com/btcsuite/btcd/btcec/v2/ecdsa\"\n\t\"github.com/btcsuite/btcd/btcutil\"\n\t\"github.com/btcsuite/btcd/chaincfg\"\n\t\"github.com/btcsuite/btcd/chaincfg/chainhash\"\n\t\"github.com/btcsuite/btcd/connmgr\"\n\t\"github.com/btcsuite/btcd/txscript\"\n\t\"github.com/btcsuite/btcd/wire\"\n\t\"github.com/go-errors/errors\"\n\tsphinx \"github.com/lightningnetwork/lightning-onion\"\n\t\"github.com/lightningnetwork/lnd/aliasmgr\"\n\t\"github.com/lightningnetwork/lnd/autopilot\"\n\t\"github.com/lightningnetwork/lnd/brontide\"\n\t\"github.com/lightningnetwork/lnd/chainio\"\n\t\"github.com/lightningnetwork/lnd/chainreg\"\n\t\"github.com/lightningnetwork/lnd/chanacceptor\"\n\t\"github.com/lightningnetwork/lnd/chanbackup\"\n\t\"github.com/lightningnetwork/lnd/chanfitness\"\n\t\"github.com/lightningnetwork/lnd/channeldb\"\n\t\"github.com/lightningnetwork/lnd/channelnotifier\"\n\t\"github.com/lightningnetwork/lnd/clock\"\n\t\"github.com/lightningnetwork/lnd/cluster\"\n\t\"github.com/lightningnetwork/lnd/contractcourt\"\n\t\"github.com/lightningnetwork/lnd/discovery\"\n\t\"github.com/lightningnetwork/lnd/feature\"\n\t\"github.com/lightningnetwork/lnd/fn/v2\"\n\t\"github.com/lightningnetwork/lnd/funding\"\n\t\"github.com/lightningnetwork/lnd/graph\"\n\tgraphdb \"github.com/lightningnetwork/lnd/graph/db\"\n\t\"github.com/lightningnetwork/lnd/graph/db/models\"\n\t\"github.com/lightningnetwork/lnd/graph/graphsession\"\n\t\"github.com/lightningnetwork/lnd/healthcheck\"\n\t\"github.com/lightningnetwork/lnd/htlcswitch\"\n\t\"github.com/lightningnetwork/lnd/htlcswitch/hop\"\n\t\"github.com/lightningnetwork/lnd/input\"\n\t\"github.com/lightningnetwork/lnd/invoices\"\n\t\"github.com/lightningnetwork/lnd/keychain\"\n\t\"github.com/lightningnetwork/lnd/kvdb\"\n\t\"github.com/lightningnetwork/lnd/lncfg\"\n\t\"github.com/lightningnetwork/lnd/lnencrypt\"\n\t\"github.com/lightningnetwork/lnd/lnpeer\"\n\t\"github.com/lightningnetwork/lnd/lnrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/routerrpc\"\n\t\"github.com/lightningnetwork/lnd/lnwallet\"\n\t\"github.com/lightningnetwork/lnd/lnwallet/chainfee\"\n\t\"github.com/lightningnetwork/lnd/lnwallet/chanfunding\"\n\t\"github.com/lightningnetwork/lnd/lnwallet/rpcwallet\"\n\t\"github.com/lightningnetwork/lnd/lnwire\"\n\t\"github.com/lightningnetwork/lnd/nat\"\n\t\"github.com/lightningnetwork/lnd/netann\"\n\t\"github.com/lightningnetwork/lnd/peer\"\n\t\"github.com/lightningnetwork/lnd/peernotifier\"\n\t\"github.com/lightningnetwork/lnd/pool\"\n\t\"github.com/lightningnetwork/lnd/queue\"\n\t\"github.com/lightningnetwork/lnd/routing\"\n\t\"github.com/lightningnetwork/lnd/routing/localchans\"\n\t\"github.com/lightningnetwork/lnd/routing/route\"\n\t\"github.com/lightningnetwork/lnd/subscribe\"\n\t\"github.com/lightningnetwork/lnd/sweep\"\n\t\"github.com/lightningnetwork/lnd/ticker\"\n\t\"github.com/lightningnetwork/lnd/tor\"\n\t\"github.com/lightningnetwork/lnd/walletunlocker\"\n\t\"github.com/lightningnetwork/lnd/watchtower/blob\"\n\t\"github.com/lightningnetwork/lnd/watchtower/wtclient\"\n\t\"github.com/lightningnetwork/lnd/watchtower/wtpolicy\"\n\t\"github.com/lightningnetwork/lnd/watchtower/wtserver\"\n)\n\nconst (\n\t// defaultMinPeers is the minimum number of peers nodes should always be\n\t// connected to.\n\tdefaultMinPeers = 3\n\n\t// defaultStableConnDuration is a floor under which all reconnection\n\t// attempts will apply exponential randomized backoff. Connections\n\t// durations exceeding this value will be eligible to have their\n\t// backoffs reduced.\n\tdefaultStableConnDuration = 10 * time.Minute\n\n\t// numInstantInitReconnect specifies how many persistent peers we should\n\t// always attempt outbound connections to immediately. After this value\n\t// is surpassed, the remaining peers will be randomly delayed using\n\t// maxInitReconnectDelay.\n\tnumInstantInitReconnect = 10\n\n\t// maxInitReconnectDelay specifies the maximum delay in seconds we will\n\t// apply in attempting to reconnect to persistent peers on startup. The\n\t// value used or a particular peer will be chosen between 0s and this\n\t// value.\n\tmaxInitReconnectDelay = 30\n\n\t// multiAddrConnectionStagger is the number of seconds to wait between\n\t// attempting to a peer with each of its advertised addresses.\n\tmultiAddrConnectionStagger = 10 * time.Second\n)\n\nvar (\n\t// ErrPeerNotConnected signals that the server has no connection to the\n\t// given peer.\n\tErrPeerNotConnected = errors.New(\"peer is not connected\")\n\n\t// ErrServerNotActive indicates that the server has started but hasn't\n\t// fully finished the startup process.\n\tErrServerNotActive = errors.New(\"server is still in the process of \" +\n\t\t\"starting\")\n\n\t// ErrServerShuttingDown indicates that the server is in the process of\n\t// gracefully exiting.\n\tErrServerShuttingDown = errors.New(\"server is shutting down\")\n\n\t// MaxFundingAmount is a soft-limit of the maximum channel size\n\t// currently accepted within the Lightning Protocol. This is\n\t// defined in BOLT-0002, and serves as an initial precautionary limit\n\t// while implementations are battle tested in the real world.\n\t//\n\t// At the moment, this value depends on which chain is active. It is set\n\t// to the value under the Bitcoin chain as default.\n\t//\n\t// TODO(roasbeef): add command line param to modify.\n\tMaxFundingAmount = funding.MaxBtcFundingAmount\n\n\t// EndorsementExperimentEnd is the time after which nodes should stop\n\t// propagating experimental endorsement signals.\n\t//\n\t// Per blip04: January 1, 2026 12:00:00 AM UTC in unix seconds.\n\tEndorsementExperimentEnd = time.Unix(1767225600, 0)\n)\n\n// errPeerAlreadyConnected is an error returned by the server when we're\n// commanded to connect to a peer, but they're already connected.\ntype errPeerAlreadyConnected struct {\n\tpeer *peer.Brontide\n}\n\n// Error returns the human readable version of this error type.\n//\n// NOTE: Part of the error interface.\nfunc (e *errPeerAlreadyConnected) Error() string {\n\treturn fmt.Sprintf(\"already connected to peer: %v\", e.peer)\n}\n\n// server is the main server of the Lightning Network Daemon. The server houses\n// global state pertaining to the wallet, database, and the rpcserver.\n// Additionally, the server is also used as a central messaging bus to interact\n// with any of its companion objects.\ntype server struct {\n\tactive   int32 // atomic\n\tstopping int32 // atomic\n\n\tstart sync.Once\n\tstop  sync.Once\n\n\tcfg *Config\n\n\timplCfg *ImplementationCfg\n\n\t// identityECDH is an ECDH capable wrapper for the private key used\n\t// to authenticate any incoming connections.\n\tidentityECDH keychain.SingleKeyECDH\n\n\t// identityKeyLoc is the key locator for the above wrapped identity key.\n\tidentityKeyLoc keychain.KeyLocator\n\n\t// nodeSigner is an implementation of the MessageSigner implementation\n\t// that's backed by the identity private key of the running lnd node.\n\tnodeSigner *netann.NodeSigner\n\n\tchanStatusMgr *netann.ChanStatusManager\n\n\t// listenAddrs is the list of addresses the server is currently\n\t// listening on.\n\tlistenAddrs []net.Addr\n\n\t// torController is a client that will communicate with a locally\n\t// running Tor server. This client will handle initiating and\n\t// authenticating the connection to the Tor server, automatically\n\t// creating and setting up onion services, etc.\n\ttorController *tor.Controller\n\n\t// natTraversal is the specific NAT traversal technique used to\n\t// automatically set up port forwarding rules in order to advertise to\n\t// the network that the node is accepting inbound connections.\n\tnatTraversal nat.Traversal\n\n\t// lastDetectedIP is the last IP detected by the NAT traversal technique\n\t// above. This IP will be watched periodically in a goroutine in order\n\t// to handle dynamic IP changes.\n\tlastDetectedIP net.IP\n\n\tmu sync.RWMutex\n\n\t// peersByPub is a map of the active peers.\n\t//\n\t// NOTE: The key used here is the raw bytes of the peer's public key to\n\t// string conversion, which means it cannot be printed using `%s` as it\n\t// will just print the binary.\n\t//\n\t// TODO(yy): Use the hex string instead.\n\tpeersByPub map[string]*peer.Brontide\n\n\tinboundPeers  map[string]*peer.Brontide\n\toutboundPeers map[string]*peer.Brontide\n\n\tpeerConnectedListeners    map[string][]chan<- lnpeer.Peer\n\tpeerDisconnectedListeners map[string][]chan<- struct{}\n\n\t// TODO(yy): the Brontide.Start doesn't know this value, which means it\n\t// will continue to send messages even if there are no active channels\n\t// and the value below is false. Once it's pruned, all its connections\n\t// will be closed, thus the Brontide.Start will return an error.\n\tpersistentPeers        map[string]bool\n\tpersistentPeersBackoff map[string]time.Duration\n\tpersistentPeerAddrs    map[string][]*lnwire.NetAddress\n\tpersistentConnReqs     map[string][]*connmgr.ConnReq\n\tpersistentRetryCancels map[string]chan struct{}\n\n\t// peerErrors keeps a set of peer error buffers for peers that have\n\t// disconnected from us. This allows us to track historic peer errors\n\t// over connections. The string of the peer's compressed pubkey is used\n\t// as a key for this map.\n\tpeerErrors map[string]*queue.CircularBuffer\n\n\t// ignorePeerTermination tracks peers for which the server has initiated\n\t// a disconnect. Adding a peer to this map causes the peer termination\n\t// watcher to short circuit in the event that peers are purposefully\n\t// disconnected.\n\tignorePeerTermination map[*peer.Brontide]struct{}\n\n\t// scheduledPeerConnection maps a pubkey string to a callback that\n\t// should be executed in the peerTerminationWatcher the prior peer with\n\t// the same pubkey exits.  This allows the server to wait until the\n\t// prior peer has cleaned up successfully, before adding the new peer\n\t// intended to replace it.\n\tscheduledPeerConnection map[string]func()\n\n\t// pongBuf is a shared pong reply buffer we'll use across all active\n\t// peer goroutines. We know the max size of a pong message\n\t// (lnwire.MaxPongBytes), so we can allocate this ahead of time, and\n\t// avoid allocations each time we need to send a pong message.\n\tpongBuf []byte\n\n\tcc *chainreg.ChainControl\n\n\tfundingMgr *funding.Manager\n\n\tgraphDB *graphdb.ChannelGraph\n\n\tchanStateDB *channeldb.ChannelStateDB\n\n\taddrSource channeldb.AddrSource\n\n\t// miscDB is the DB that contains all \"other\" databases within the main\n\t// channel DB that haven't been separated out yet.\n\tmiscDB *channeldb.DB\n\n\tinvoicesDB invoices.InvoiceDB\n\n\taliasMgr *aliasmgr.Manager\n\n\thtlcSwitch *htlcswitch.Switch\n\n\tinterceptableSwitch *htlcswitch.InterceptableSwitch\n\n\tinvoices *invoices.InvoiceRegistry\n\n\tinvoiceHtlcModifier *invoices.HtlcModificationInterceptor\n\n\tchannelNotifier *channelnotifier.ChannelNotifier\n\n\tpeerNotifier *peernotifier.PeerNotifier\n\n\thtlcNotifier *htlcswitch.HtlcNotifier\n\n\twitnessBeacon contractcourt.WitnessBeacon\n\n\tbreachArbitrator *contractcourt.BreachArbitrator\n\n\tmissionController *routing.MissionController\n\tdefaultMC         *routing.MissionControl\n\n\tgraphBuilder *graph.Builder\n\n\tchanRouter *routing.ChannelRouter\n\n\tcontrolTower routing.ControlTower\n\n\tauthGossiper *discovery.AuthenticatedGossiper\n\n\tlocalChanMgr *localchans.Manager\n\n\tutxoNursery *contractcourt.UtxoNursery\n\n\tsweeper *sweep.UtxoSweeper\n\n\tchainArb *contractcourt.ChainArbitrator\n\n\tsphinx *hop.OnionProcessor\n\n\ttowerClientMgr *wtclient.Manager\n\n\tconnMgr *connmgr.ConnManager\n\n\tsigPool *lnwallet.SigPool\n\n\twritePool *pool.Write\n\n\treadPool *pool.Read\n\n\ttlsManager *TLSManager\n\n\t// featureMgr dispatches feature vectors for various contexts within the\n\t// daemon.\n\tfeatureMgr *feature.Manager\n\n\t// currentNodeAnn is the node announcement that has been broadcast to\n\t// the network upon startup, if the attributes of the node (us) has\n\t// changed since last start.\n\tcurrentNodeAnn *lnwire.NodeAnnouncement\n\n\t// chansToRestore is the set of channels that upon starting, the server\n\t// should attempt to restore/recover.\n\tchansToRestore walletunlocker.ChannelsToRecover\n\n\t// chanSubSwapper is a sub-system that will ensure our on-disk channel\n\t// backups are consistent at all times. It interacts with the\n\t// channelNotifier to be notified of newly opened and closed channels.\n\tchanSubSwapper *chanbackup.SubSwapper\n\n\t// chanEventStore tracks the behaviour of channels and their remote peers to\n\t// provide insights into their health and performance.\n\tchanEventStore *chanfitness.ChannelEventStore\n\n\thostAnn *netann.HostAnnouncer\n\n\t// livenessMonitor monitors that lnd has access to critical resources.\n\tlivenessMonitor *healthcheck.Monitor\n\n\tcustomMessageServer *subscribe.Server\n\n\t// txPublisher is a publisher with fee-bumping capability.\n\ttxPublisher *sweep.TxPublisher\n\n\t// blockbeatDispatcher is a block dispatcher that notifies subscribers\n\t// of new blocks.\n\tblockbeatDispatcher *chainio.BlockbeatDispatcher\n\n\tquit chan struct{}\n\n\twg sync.WaitGroup\n}\n\n// updatePersistentPeerAddrs subscribes to topology changes and stores\n// advertised addresses for any NodeAnnouncements from our persisted peers.\nfunc (s *server) updatePersistentPeerAddrs() error {\n\tgraphSub, err := s.graphBuilder.SubscribeTopology()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\ts.wg.Add(1)\n\tgo func() {\n\t\tdefer func() {\n\t\t\tgraphSub.Cancel()\n\t\t\ts.wg.Done()\n\t\t}()\n\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-s.quit:\n\t\t\t\treturn\n\n\t\t\tcase topChange, ok := <-graphSub.TopologyChanges:\n\t\t\t\t// If the router is shutting down, then we will\n\t\t\t\t// as well.\n\t\t\t\tif !ok {\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tfor _, update := range topChange.NodeUpdates {\n\t\t\t\t\tpubKeyStr := string(\n\t\t\t\t\t\tupdate.IdentityKey.\n\t\t\t\t\t\t\tSerializeCompressed(),\n\t\t\t\t\t)\n\n\t\t\t\t\t// We only care about updates from\n\t\t\t\t\t// our persistentPeers.\n\t\t\t\t\ts.mu.RLock()\n\t\t\t\t\t_, ok := s.persistentPeers[pubKeyStr]\n\t\t\t\t\ts.mu.RUnlock()\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\n\t\t\t\t\taddrs := make([]*lnwire.NetAddress, 0,\n\t\t\t\t\t\tlen(update.Addresses))\n\n\t\t\t\t\tfor _, addr := range update.Addresses {\n\t\t\t\t\t\taddrs = append(addrs,\n\t\t\t\t\t\t\t&lnwire.NetAddress{\n\t\t\t\t\t\t\t\tIdentityKey: update.IdentityKey,\n\t\t\t\t\t\t\t\tAddress:     addr,\n\t\t\t\t\t\t\t\tChainNet:    s.cfg.ActiveNetParams.Net,\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t)\n\t\t\t\t\t}\n\n\t\t\t\t\ts.mu.Lock()\n\n\t\t\t\t\t// Update the stored addresses for this\n\t\t\t\t\t// to peer to reflect the new set.\n\t\t\t\t\ts.persistentPeerAddrs[pubKeyStr] = addrs\n\n\t\t\t\t\t// If there are no outstanding\n\t\t\t\t\t// connection requests for this peer\n\t\t\t\t\t// then our work is done since we are\n\t\t\t\t\t// not currently trying to connect to\n\t\t\t\t\t// them.\n\t\t\t\t\tif len(s.persistentConnReqs[pubKeyStr]) == 0 {\n\t\t\t\t\t\ts.mu.Unlock()\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\n\t\t\t\t\ts.mu.Unlock()\n\n\t\t\t\t\ts.connectToPersistentPeer(pubKeyStr)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn nil\n}\n\n// CustomMessage is a custom message that is received from a peer.\ntype CustomMessage struct {\n\t// Peer is the peer pubkey\n\tPeer [33]byte\n\n\t// Msg is the custom wire message.\n\tMsg *lnwire.Custom\n}\n\n// parseAddr parses an address from its string format to a net.Addr.\nfunc parseAddr(address string, netCfg tor.Net) (net.Addr, error) {\n\tvar (\n\t\thost string\n\t\tport int\n\t)\n\n\t// Split the address into its host and port components.\n\th, p, err := net.SplitHostPort(address)\n\tif err != nil {\n\t\t// If a port wasn't specified, we'll assume the address only\n\t\t// contains the host so we'll use the default port.\n\t\thost = address\n\t\tport = defaultPeerPort\n\t} else {\n\t\t// Otherwise, we'll note both the host and ports.\n\t\thost = h\n\t\tportNum, err := strconv.Atoi(p)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tport = portNum\n\t}\n\n\tif tor.IsOnionHost(host) {\n\t\treturn &tor.OnionAddr{OnionService: host, Port: port}, nil\n\t}\n\n\t// If the host is part of a TCP address, we'll use the network\n\t// specific ResolveTCPAddr function in order to resolve these\n\t// addresses over Tor in order to prevent leaking your real IP\n\t// address.\n\thostPort := net.JoinHostPort(host, strconv.Itoa(port))\n\treturn netCfg.ResolveTCPAddr(\"tcp\", hostPort)\n}\n\n// noiseDial is a factory function which creates a connmgr compliant dialing\n// function by returning a closure which includes the server's identity key.\nfunc noiseDial(idKey keychain.SingleKeyECDH,\n\tnetCfg tor.Net, timeout time.Duration) func(net.Addr) (net.Conn, error) {\n\n\treturn func(a net.Addr) (net.Conn, error) {\n\t\tlnAddr := a.(*lnwire.NetAddress)\n\t\treturn brontide.Dial(idKey, lnAddr, timeout, netCfg.Dial)\n\t}\n}\n\n// newServer creates a new instance of the server which is to listen using the\n// passed listener address.\nfunc newServer(cfg *Config, listenAddrs []net.Addr,\n\tdbs *DatabaseInstances, cc *chainreg.ChainControl,\n\tnodeKeyDesc *keychain.KeyDescriptor,\n\tchansToRestore walletunlocker.ChannelsToRecover,\n\tchanPredicate chanacceptor.ChannelAcceptor,\n\ttorController *tor.Controller, tlsManager *TLSManager,\n\tleaderElector cluster.LeaderElector,\n\timplCfg *ImplementationCfg) (*server, error) {\n\n\tvar (\n\t\terr         error\n\t\tnodeKeyECDH = keychain.NewPubKeyECDH(*nodeKeyDesc, cc.KeyRing)\n\n\t\t// We just derived the full descriptor, so we know the public\n\t\t// key is set on it.\n\t\tnodeKeySigner = keychain.NewPubKeyMessageSigner(\n\t\t\tnodeKeyDesc.PubKey, nodeKeyDesc.KeyLocator, cc.KeyRing,\n\t\t)\n\t)\n\n\tlisteners := make([]net.Listener, len(listenAddrs))\n\tfor i, listenAddr := range listenAddrs {\n\t\t// Note: though brontide.NewListener uses ResolveTCPAddr, it\n\t\t// doesn't need to call the general lndResolveTCP function\n\t\t// since we are resolving a local address.\n\t\tlisteners[i], err = brontide.NewListener(\n\t\t\tnodeKeyECDH, listenAddr.String(),\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tvar serializedPubKey [33]byte\n\tcopy(serializedPubKey[:], nodeKeyDesc.PubKey.SerializeCompressed())\n\n\tnetParams := cfg.ActiveNetParams.Params\n\n\t// Initialize the sphinx router.\n\treplayLog := htlcswitch.NewDecayedLog(\n\t\tdbs.DecayedLogDB, cc.ChainNotifier,\n\t)\n\tsphinxRouter := sphinx.NewRouter(nodeKeyECDH, replayLog)\n\n\twriteBufferPool := pool.NewWriteBuffer(\n\t\tpool.DefaultWriteBufferGCInterval,\n\t\tpool.DefaultWriteBufferExpiryInterval,\n\t)\n\n\twritePool := pool.NewWrite(\n\t\twriteBufferPool, cfg.Workers.Write, pool.DefaultWorkerTimeout,\n\t)\n\n\treadBufferPool := pool.NewReadBuffer(\n\t\tpool.DefaultReadBufferGCInterval,\n\t\tpool.DefaultReadBufferExpiryInterval,\n\t)\n\n\treadPool := pool.NewRead(\n\t\treadBufferPool, cfg.Workers.Read, pool.DefaultWorkerTimeout,\n\t)\n\n\t// If the taproot overlay flag is set, but we don't have an aux funding\n\t// controller, then we'll exit as this is incompatible.\n\tif cfg.ProtocolOptions.TaprootOverlayChans &&\n\t\timplCfg.AuxFundingController.IsNone() {\n\n\t\treturn nil, fmt.Errorf(\"taproot overlay flag set, but not \" +\n\t\t\t\"aux controllers\")\n\t}\n\n\t//nolint:ll\n\tfeatureMgr, err := feature.NewManager(feature.Config{\n\t\tNoTLVOnion:                cfg.ProtocolOptions.LegacyOnion(),\n\t\tNoStaticRemoteKey:         cfg.ProtocolOptions.NoStaticRemoteKey(),\n\t\tNoAnchors:                 cfg.ProtocolOptions.NoAnchorCommitments(),\n\t\tNoWumbo:                   !cfg.ProtocolOptions.Wumbo(),\n\t\tNoScriptEnforcementLease:  cfg.ProtocolOptions.NoScriptEnforcementLease(),\n\t\tNoKeysend:                 !cfg.AcceptKeySend,\n\t\tNoOptionScidAlias:         !cfg.ProtocolOptions.ScidAlias(),\n\t\tNoZeroConf:                !cfg.ProtocolOptions.ZeroConf(),\n\t\tNoAnySegwit:               cfg.ProtocolOptions.NoAnySegwit(),\n\t\tCustomFeatures:            cfg.ProtocolOptions.CustomFeatures(),\n\t\tNoTaprootChans:            !cfg.ProtocolOptions.TaprootChans,\n\t\tNoTaprootOverlay:          !cfg.ProtocolOptions.TaprootOverlayChans,\n\t\tNoRouteBlinding:           cfg.ProtocolOptions.NoRouteBlinding(),\n\t\tNoExperimentalEndorsement: cfg.ProtocolOptions.NoExperimentalEndorsement(),\n\t\tNoQuiescence:              cfg.ProtocolOptions.NoQuiescence(),\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tinvoiceHtlcModifier := invoices.NewHtlcModificationInterceptor()\n\tregistryConfig := invoices.RegistryConfig{\n\t\tFinalCltvRejectDelta:        lncfg.DefaultFinalCltvRejectDelta,\n\t\tHtlcHoldDuration:            invoices.DefaultHtlcHoldDuration,\n\t\tClock:                       clock.NewDefaultClock(),\n\t\tAcceptKeySend:               cfg.AcceptKeySend,\n\t\tAcceptAMP:                   cfg.AcceptAMP,\n\t\tGcCanceledInvoicesOnStartup: cfg.GcCanceledInvoicesOnStartup,\n\t\tGcCanceledInvoicesOnTheFly:  cfg.GcCanceledInvoicesOnTheFly,\n\t\tKeysendHoldTime:             cfg.KeysendHoldTime,\n\t\tHtlcInterceptor:             invoiceHtlcModifier,\n\t}\n\n\taddrSource := channeldb.NewMultiAddrSource(dbs.ChanStateDB, dbs.GraphDB)\n\n\ts := &server{\n\t\tcfg:            cfg,\n\t\timplCfg:        implCfg,\n\t\tgraphDB:        dbs.GraphDB,\n\t\tchanStateDB:    dbs.ChanStateDB.ChannelStateDB(),\n\t\taddrSource:     addrSource,\n\t\tmiscDB:         dbs.ChanStateDB,\n\t\tinvoicesDB:     dbs.InvoiceDB,\n\t\tcc:             cc,\n\t\tsigPool:        lnwallet.NewSigPool(cfg.Workers.Sig, cc.Signer),\n\t\twritePool:      writePool,\n\t\treadPool:       readPool,\n\t\tchansToRestore: chansToRestore,\n\n\t\tblockbeatDispatcher: chainio.NewBlockbeatDispatcher(\n\t\t\tcc.ChainNotifier,\n\t\t),\n\t\tchannelNotifier: channelnotifier.New(\n\t\t\tdbs.ChanStateDB.ChannelStateDB(),\n\t\t),\n\n\t\tidentityECDH:   nodeKeyECDH,\n\t\tidentityKeyLoc: nodeKeyDesc.KeyLocator,\n\t\tnodeSigner:     netann.NewNodeSigner(nodeKeySigner),\n\n\t\tlistenAddrs: listenAddrs,\n\n\t\t// TODO(roasbeef): derive proper onion key based on rotation\n\t\t// schedule\n\t\tsphinx: hop.NewOnionProcessor(sphinxRouter),\n\n\t\ttorController: torController,\n\n\t\tpersistentPeers:         make(map[string]bool),\n\t\tpersistentPeersBackoff:  make(map[string]time.Duration),\n\t\tpersistentConnReqs:      make(map[string][]*connmgr.ConnReq),\n\t\tpersistentPeerAddrs:     make(map[string][]*lnwire.NetAddress),\n\t\tpersistentRetryCancels:  make(map[string]chan struct{}),\n\t\tpeerErrors:              make(map[string]*queue.CircularBuffer),\n\t\tignorePeerTermination:   make(map[*peer.Brontide]struct{}),\n\t\tscheduledPeerConnection: make(map[string]func()),\n\t\tpongBuf:                 make([]byte, lnwire.MaxPongBytes),\n\n\t\tpeersByPub:                make(map[string]*peer.Brontide),\n\t\tinboundPeers:              make(map[string]*peer.Brontide),\n\t\toutboundPeers:             make(map[string]*peer.Brontide),\n\t\tpeerConnectedListeners:    make(map[string][]chan<- lnpeer.Peer),\n\t\tpeerDisconnectedListeners: make(map[string][]chan<- struct{}),\n\n\t\tinvoiceHtlcModifier: invoiceHtlcModifier,\n\n\t\tcustomMessageServer: subscribe.NewServer(),\n\n\t\ttlsManager: tlsManager,\n\n\t\tfeatureMgr: featureMgr,\n\t\tquit:       make(chan struct{}),\n\t}\n\n\t// Start the low-level services once they are initialized.\n\t//\n\t// TODO(yy): break the server startup into four steps,\n\t// 1. init the low-level services.\n\t// 2. start the low-level services.\n\t// 3. init the high-level services.\n\t// 4. start the high-level services.\n\tif err := s.startLowLevelServices(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tcurrentHash, currentHeight, err := s.cc.ChainIO.GetBestBlock()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\texpiryWatcher := invoices.NewInvoiceExpiryWatcher(\n\t\tclock.NewDefaultClock(), cfg.Invoices.HoldExpiryDelta,\n\t\tuint32(currentHeight), currentHash, cc.ChainNotifier,\n\t)\n\ts.invoices = invoices.NewRegistry(\n\t\tdbs.InvoiceDB, expiryWatcher, &registryConfig,\n\t)\n\n\ts.htlcNotifier = htlcswitch.NewHtlcNotifier(time.Now)\n\n\tthresholdSats := btcutil.Amount(cfg.MaxFeeExposure)\n\tthresholdMSats := lnwire.NewMSatFromSatoshis(thresholdSats)\n\n\tlinkUpdater := func(shortID lnwire.ShortChannelID) error {\n\t\tlink, err := s.htlcSwitch.GetLinkByShortID(shortID)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\ts.htlcSwitch.UpdateLinkAliases(link)\n\n\t\treturn nil\n\t}\n\n\ts.aliasMgr, err = aliasmgr.NewManager(dbs.ChanStateDB, linkUpdater)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ts.htlcSwitch, err = htlcswitch.New(htlcswitch.Config{\n\t\tDB:                   dbs.ChanStateDB,\n\t\tFetchAllOpenChannels: s.chanStateDB.FetchAllOpenChannels,\n\t\tFetchAllChannels:     s.chanStateDB.FetchAllChannels,\n\t\tFetchClosedChannels:  s.chanStateDB.FetchClosedChannels,\n\t\tLocalChannelClose: func(pubKey []byte,\n\t\t\trequest *htlcswitch.ChanClose) {\n\n\t\t\tpeer, err := s.FindPeerByPubStr(string(pubKey))\n\t\t\tif err != nil {\n\t\t\t\tsrvrLog.Errorf(\"unable to close channel, peer\"+\n\t\t\t\t\t\" with %v id can't be found: %v\",\n\t\t\t\t\tpubKey, err,\n\t\t\t\t)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tpeer.HandleLocalCloseChanReqs(request)\n\t\t},\n\t\tFwdingLog:              dbs.ChanStateDB.ForwardingLog(),\n\t\tSwitchPackager:         channeldb.NewSwitchPackager(),\n\t\tExtractErrorEncrypter:  s.sphinx.ExtractErrorEncrypter,\n\t\tFetchLastChannelUpdate: s.fetchLastChanUpdate(),\n\t\tNotifier:               s.cc.ChainNotifier,\n\t\tHtlcNotifier:           s.htlcNotifier,\n\t\tFwdEventTicker:         ticker.New(htlcswitch.DefaultFwdEventInterval),\n\t\tLogEventTicker:         ticker.New(htlcswitch.DefaultLogInterval),\n\t\tAckEventTicker:         ticker.New(htlcswitch.DefaultAckInterval),\n\t\tAllowCircularRoute:     cfg.AllowCircularRoute,\n\t\tRejectHTLC:             cfg.RejectHTLC,\n\t\tClock:                  clock.NewDefaultClock(),\n\t\tMailboxDeliveryTimeout: cfg.Htlcswitch.MailboxDeliveryTimeout,\n\t\tMaxFeeExposure:         thresholdMSats,\n\t\tSignAliasUpdate:        s.signAliasUpdate,\n\t\tIsAlias:                aliasmgr.IsAlias,\n\t}, uint32(currentHeight))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ts.interceptableSwitch, err = htlcswitch.NewInterceptableSwitch(\n\t\t&htlcswitch.InterceptableSwitchConfig{\n\t\t\tSwitch:             s.htlcSwitch,\n\t\t\tCltvRejectDelta:    lncfg.DefaultFinalCltvRejectDelta,\n\t\t\tCltvInterceptDelta: lncfg.DefaultCltvInterceptDelta,\n\t\t\tRequireInterceptor: s.cfg.RequireInterceptor,\n\t\t\tNotifier:           s.cc.ChainNotifier,\n\t\t},\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ts.witnessBeacon = newPreimageBeacon(\n\t\tdbs.ChanStateDB.NewWitnessCache(),\n\t\ts.interceptableSwitch.ForwardPacket,\n\t)\n\n\tchanStatusMgrCfg := &netann.ChanStatusConfig{\n\t\tChanStatusSampleInterval: cfg.ChanStatusSampleInterval,\n\t\tChanEnableTimeout:        cfg.ChanEnableTimeout,\n\t\tChanDisableTimeout:       cfg.ChanDisableTimeout,\n\t\tOurPubKey:                nodeKeyDesc.PubKey,\n\t\tOurKeyLoc:                nodeKeyDesc.KeyLocator,\n\t\tMessageSigner:            s.nodeSigner,\n\t\tIsChannelActive:          s.htlcSwitch.HasActiveLink,\n\t\tApplyChannelUpdate:       s.applyChannelUpdate,\n\t\tDB:                       s.chanStateDB,\n\t\tGraph:                    dbs.GraphDB,\n\t}\n\n\tchanStatusMgr, err := netann.NewChanStatusManager(chanStatusMgrCfg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ts.chanStatusMgr = chanStatusMgr\n\n\t// If enabled, use either UPnP or NAT-PMP to automatically configure\n\t// port forwarding for users behind a NAT.\n\tif cfg.NAT {\n\t\tsrvrLog.Info(\"Scanning local network for a UPnP enabled device\")\n\n\t\tdiscoveryTimeout := time.Duration(10 * time.Second)\n\n\t\tctx, cancel := context.WithTimeout(\n\t\t\tcontext.Background(), discoveryTimeout,\n\t\t)\n\t\tdefer cancel()\n\t\tupnp, err := nat.DiscoverUPnP(ctx)\n\t\tif err == nil {\n\t\t\ts.natTraversal = upnp\n\t\t} else {\n\t\t\t// If we were not able to discover a UPnP enabled device\n\t\t\t// on the local network, we'll fall back to attempting\n\t\t\t// to discover a NAT-PMP enabled device.\n\t\t\tsrvrLog.Errorf(\"Unable to discover a UPnP enabled \"+\n\t\t\t\t\"device on the local network: %v\", err)\n\n\t\t\tsrvrLog.Info(\"Scanning local network for a NAT-PMP \" +\n\t\t\t\t\"enabled device\")\n\n\t\t\tpmp, err := nat.DiscoverPMP(discoveryTimeout)\n\t\t\tif err != nil {\n\t\t\t\terr := fmt.Errorf(\"unable to discover a \"+\n\t\t\t\t\t\"NAT-PMP enabled device on the local \"+\n\t\t\t\t\t\"network: %v\", err)\n\t\t\t\tsrvrLog.Error(err)\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\ts.natTraversal = pmp\n\t\t}\n\t}\n\n\t// If we were requested to automatically configure port forwarding,\n\t// we'll use the ports that the server will be listening on.\n\texternalIPStrings := make([]string, len(cfg.ExternalIPs))\n\tfor idx, ip := range cfg.ExternalIPs {\n\t\texternalIPStrings[idx] = ip.String()\n\t}\n\tif s.natTraversal != nil {\n\t\tlistenPorts := make([]uint16, 0, len(listenAddrs))\n\t\tfor _, listenAddr := range listenAddrs {\n\t\t\t// At this point, the listen addresses should have\n\t\t\t// already been normalized, so it's safe to ignore the\n\t\t\t// errors.\n\t\t\t_, portStr, _ := net.SplitHostPort(listenAddr.String())\n\t\t\tport, _ := strconv.Atoi(portStr)\n\n\t\t\tlistenPorts = append(listenPorts, uint16(port))\n\t\t}\n\n\t\tips, err := s.configurePortForwarding(listenPorts...)\n\t\tif err != nil {\n\t\t\tsrvrLog.Errorf(\"Unable to automatically set up port \"+\n\t\t\t\t\"forwarding using %s: %v\",\n\t\t\t\ts.natTraversal.Name(), err)\n\t\t} else {\n\t\t\tsrvrLog.Infof(\"Automatically set up port forwarding \"+\n\t\t\t\t\"using %s to advertise external IP\",\n\t\t\t\ts.natTraversal.Name())\n\t\t\texternalIPStrings = append(externalIPStrings, ips...)\n\t\t}\n\t}\n\n\t// If external IP addresses have been specified, add those to the list\n\t// of this server's addresses.\n\texternalIPs, err := lncfg.NormalizeAddresses(\n\t\texternalIPStrings, strconv.Itoa(defaultPeerPort),\n\t\tcfg.net.ResolveTCPAddr,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tselfAddrs := make([]net.Addr, 0, len(externalIPs))\n\tselfAddrs = append(selfAddrs, externalIPs...)\n\n\t// We'll now reconstruct a node announcement based on our current\n\t// configuration so we can send it out as a sort of heart beat within\n\t// the network.\n\t//\n\t// We'll start by parsing the node color from configuration.\n\tcolor, err := lncfg.ParseHexColor(cfg.Color)\n\tif err != nil {\n\t\tsrvrLog.Errorf(\"unable to parse color: %v\\n\", err)\n\t\treturn nil, err\n\t}\n\n\t// If no alias is provided, default to first 10 characters of public\n\t// key.\n\talias := cfg.Alias\n\tif alias == \"\" {\n\t\talias = hex.EncodeToString(serializedPubKey[:10])\n\t}\n\tnodeAlias, err := lnwire.NewNodeAlias(alias)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tselfNode := &models.LightningNode{\n\t\tHaveNodeAnnouncement: true,\n\t\tLastUpdate:           time.Now(),\n\t\tAddresses:            selfAddrs,\n\t\tAlias:                nodeAlias.String(),\n\t\tFeatures:             s.featureMgr.Get(feature.SetNodeAnn),\n\t\tColor:                color,\n\t}\n\tcopy(selfNode.PubKeyBytes[:], nodeKeyDesc.PubKey.SerializeCompressed())\n\n\t// Based on the disk representation of the node announcement generated\n\t// above, we'll generate a node announcement that can go out on the\n\t// network so we can properly sign it.\n\tnodeAnn, err := selfNode.NodeAnnouncement(false)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to gen self node ann: %w\", err)\n\t}\n\n\t// With the announcement generated, we'll sign it to properly\n\t// authenticate the message on the network.\n\tauthSig, err := netann.SignAnnouncement(\n\t\ts.nodeSigner, nodeKeyDesc.KeyLocator, nodeAnn,\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to generate signature for \"+\n\t\t\t\"self node announcement: %v\", err)\n\t}\n\tselfNode.AuthSigBytes = authSig.Serialize()\n\tnodeAnn.Signature, err = lnwire.NewSigFromECDSARawSignature(\n\t\tselfNode.AuthSigBytes,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Finally, we'll update the representation on disk, and update our\n\t// cached in-memory version as well.\n\tif err := dbs.GraphDB.SetSourceNode(selfNode); err != nil {\n\t\treturn nil, fmt.Errorf(\"can't set self node: %w\", err)\n\t}\n\ts.currentNodeAnn = nodeAnn\n\n\t// The router will get access to the payment ID sequencer, such that it\n\t// can generate unique payment IDs.\n\tsequencer, err := htlcswitch.NewPersistentSequencer(dbs.ChanStateDB)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Instantiate mission control with config from the sub server.\n\t//\n\t// TODO(joostjager): When we are further in the process of moving to sub\n\t// servers, the mission control instance itself can be moved there too.\n\troutingConfig := routerrpc.GetRoutingConfig(cfg.SubRPCServers.RouterRPC)\n\n\t// We only initialize a probability estimator if there's no custom one.\n\tvar estimator routing.Estimator\n\tif cfg.Estimator != nil {\n\t\testimator = cfg.Estimator\n\t} else {\n\t\tswitch routingConfig.ProbabilityEstimatorType {\n\t\tcase routing.AprioriEstimatorName:\n\t\t\taCfg := routingConfig.AprioriConfig\n\t\t\taprioriConfig := routing.AprioriConfig{\n\t\t\t\tAprioriHopProbability: aCfg.HopProbability,\n\t\t\t\tPenaltyHalfLife:       aCfg.PenaltyHalfLife,\n\t\t\t\tAprioriWeight:         aCfg.Weight,\n\t\t\t\tCapacityFraction:      aCfg.CapacityFraction,\n\t\t\t}\n\n\t\t\testimator, err = routing.NewAprioriEstimator(\n\t\t\t\taprioriConfig,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\tcase routing.BimodalEstimatorName:\n\t\t\tbCfg := routingConfig.BimodalConfig\n\t\t\tbimodalConfig := routing.BimodalConfig{\n\t\t\t\tBimodalNodeWeight: bCfg.NodeWeight,\n\t\t\t\tBimodalScaleMsat: lnwire.MilliSatoshi(\n\t\t\t\t\tbCfg.Scale,\n\t\t\t\t),\n\t\t\t\tBimodalDecayTime: bCfg.DecayTime,\n\t\t\t}\n\n\t\t\testimator, err = routing.NewBimodalEstimator(\n\t\t\t\tbimodalConfig,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"unknown estimator type %v\",\n\t\t\t\troutingConfig.ProbabilityEstimatorType)\n\t\t}\n\t}\n\n\tmcCfg := &routing.MissionControlConfig{\n\t\tOnConfigUpdate:          fn.Some(s.UpdateRoutingConfig),\n\t\tEstimator:               estimator,\n\t\tMaxMcHistory:            routingConfig.MaxMcHistory,\n\t\tMcFlushInterval:         routingConfig.McFlushInterval,\n\t\tMinFailureRelaxInterval: routing.DefaultMinFailureRelaxInterval,\n\t}\n\n\ts.missionController, err = routing.NewMissionController(\n\t\tdbs.ChanStateDB, selfNode.PubKeyBytes, mcCfg,\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"can't create mission control \"+\n\t\t\t\"manager: %w\", err)\n\t}\n\ts.defaultMC, err = s.missionController.GetNamespacedStore(\n\t\trouting.DefaultMissionControlNamespace,\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"can't create mission control in the \"+\n\t\t\t\"default namespace: %w\", err)\n\t}\n\n\tsrvrLog.Debugf(\"Instantiating payment session source with config: \"+\n\t\t\"AttemptCost=%v + %v%%, MinRouteProbability=%v\",\n\t\tint64(routingConfig.AttemptCost),\n\t\tfloat64(routingConfig.AttemptCostPPM)/10000,\n\t\troutingConfig.MinRouteProbability)\n\n\tpathFindingConfig := routing.PathFindingConfig{\n\t\tAttemptCost: lnwire.NewMSatFromSatoshis(\n\t\t\troutingConfig.AttemptCost,\n\t\t),\n\t\tAttemptCostPPM: routingConfig.AttemptCostPPM,\n\t\tMinProbability: routingConfig.MinRouteProbability,\n\t}\n\n\tsourceNode, err := dbs.GraphDB.SourceNode()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error getting source node: %w\", err)\n\t}\n\tpaymentSessionSource := &routing.SessionSource{\n\t\tGraphSessionFactory: graphsession.NewGraphSessionFactory(\n\t\t\tdbs.GraphDB,\n\t\t),\n\t\tSourceNode:        sourceNode,\n\t\tMissionControl:    s.defaultMC,\n\t\tGetLink:           s.htlcSwitch.GetLinkByShortID,\n\t\tPathFindingConfig: pathFindingConfig,\n\t}\n\n\tpaymentControl := channeldb.NewPaymentControl(dbs.ChanStateDB)\n\n\ts.controlTower = routing.NewControlTower(paymentControl)\n\n\tstrictPruning := cfg.Bitcoin.Node == \"neutrino\" ||\n\t\tcfg.Routing.StrictZombiePruning\n\n\ts.graphBuilder, err = graph.NewBuilder(&graph.Config{\n\t\tSelfNode:            selfNode.PubKeyBytes,\n\t\tGraph:               dbs.GraphDB,\n\t\tChain:               cc.ChainIO,\n\t\tChainView:           cc.ChainView,\n\t\tNotifier:            cc.ChainNotifier,\n\t\tChannelPruneExpiry:  graph.DefaultChannelPruneExpiry,\n\t\tGraphPruneInterval:  time.Hour,\n\t\tFirstTimePruneDelay: graph.DefaultFirstTimePruneDelay,\n\t\tAssumeChannelValid:  cfg.Routing.AssumeChannelValid,\n\t\tStrictZombiePruning: strictPruning,\n\t\tIsAlias:             aliasmgr.IsAlias,\n\t})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"can't create graph builder: %w\", err)\n\t}\n\n\ts.chanRouter, err = routing.New(routing.Config{\n\t\tSelfNode:           selfNode.PubKeyBytes,\n\t\tRoutingGraph:       graphsession.NewRoutingGraph(dbs.GraphDB),\n\t\tChain:              cc.ChainIO,\n\t\tPayer:              s.htlcSwitch,\n\t\tControl:            s.controlTower,\n\t\tMissionControl:     s.defaultMC,\n\t\tSessionSource:      paymentSessionSource,\n\t\tGetLink:            s.htlcSwitch.GetLinkByShortID,\n\t\tNextPaymentID:      sequencer.NextID,\n\t\tPathFindingConfig:  pathFindingConfig,\n\t\tClock:              clock.NewDefaultClock(),\n\t\tApplyChannelUpdate: s.graphBuilder.ApplyChannelUpdate,\n\t\tClosedSCIDs:        s.fetchClosedChannelSCIDs(),\n\t\tTrafficShaper:      implCfg.TrafficShaper,\n\t})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"can't create router: %w\", err)\n\t}\n\n\tchanSeries := discovery.NewChanSeries(s.graphDB)\n\tgossipMessageStore, err := discovery.NewMessageStore(dbs.ChanStateDB)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\twaitingProofStore, err := channeldb.NewWaitingProofStore(dbs.ChanStateDB)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tscidCloserMan := discovery.NewScidCloserMan(s.graphDB, s.chanStateDB)\n\n\ts.authGossiper = discovery.New(discovery.Config{\n\t\tGraph:                 s.graphBuilder,\n\t\tChainIO:               s.cc.ChainIO,\n\t\tNotifier:              s.cc.ChainNotifier,\n\t\tChainHash:             *s.cfg.ActiveNetParams.GenesisHash,\n\t\tBroadcast:             s.BroadcastMessage,\n\t\tChanSeries:            chanSeries,\n\t\tNotifyWhenOnline:      s.NotifyWhenOnline,\n\t\tNotifyWhenOffline:     s.NotifyWhenOffline,\n\t\tFetchSelfAnnouncement: s.getNodeAnnouncement,\n\t\tUpdateSelfAnnouncement: func() (lnwire.NodeAnnouncement,\n\t\t\terror) {\n\n\t\t\treturn s.genNodeAnnouncement(nil)\n\t\t},\n\t\tProofMatureDelta:        0,\n\t\tTrickleDelay:            time.Millisecond * time.Duration(cfg.TrickleDelay),\n\t\tRetransmitTicker:        ticker.New(time.Minute * 30),\n\t\tRebroadcastInterval:     time.Hour * 24,\n\t\tWaitingProofStore:       waitingProofStore,\n\t\tMessageStore:            gossipMessageStore,\n\t\tAnnSigner:               s.nodeSigner,\n\t\tRotateTicker:            ticker.New(discovery.DefaultSyncerRotationInterval),\n\t\tHistoricalSyncTicker:    ticker.New(cfg.HistoricalSyncInterval),\n\t\tNumActiveSyncers:        cfg.NumGraphSyncPeers,\n\t\tNoTimestampQueries:      cfg.ProtocolOptions.NoTimestampQueryOption, //nolint:ll\n\t\tMinimumBatchSize:        10,\n\t\tSubBatchDelay:           cfg.Gossip.SubBatchDelay,\n\t\tIgnoreHistoricalFilters: cfg.IgnoreHistoricalGossipFilters,\n\t\tPinnedSyncers:           cfg.Gossip.PinnedSyncers,\n\t\tMaxChannelUpdateBurst:   cfg.Gossip.MaxChannelUpdateBurst,\n\t\tChannelUpdateInterval:   cfg.Gossip.ChannelUpdateInterval,\n\t\tIsAlias:                 aliasmgr.IsAlias,\n\t\tSignAliasUpdate:         s.signAliasUpdate,\n\t\tFindBaseByAlias:         s.aliasMgr.FindBaseSCID,\n\t\tGetAlias:                s.aliasMgr.GetPeerAlias,\n\t\tFindChannel:             s.findChannel,\n\t\tIsStillZombieChannel:    s.graphBuilder.IsZombieChannel,\n\t\tScidCloser:              scidCloserMan,\n\t}, nodeKeyDesc)\n\n\tselfVertex := route.Vertex(nodeKeyDesc.PubKey.SerializeCompressed())\n\t//nolint:ll\n\ts.localChanMgr = &localchans.Manager{\n\t\tSelfPub:              nodeKeyDesc.PubKey,\n\t\tDefaultRoutingPolicy: cc.RoutingPolicy,\n\t\tForAllOutgoingChannels: func(cb func(*models.ChannelEdgeInfo,\n\t\t\t*models.ChannelEdgePolicy) error) error {\n\n\t\t\treturn s.graphDB.ForEachNodeChannel(selfVertex,\n\t\t\t\tfunc(_ kvdb.RTx, c *models.ChannelEdgeInfo,\n\t\t\t\t\te *models.ChannelEdgePolicy,\n\t\t\t\t\t_ *models.ChannelEdgePolicy) error {\n\n\t\t\t\t\t// NOTE: The invoked callback here may\n\t\t\t\t\t// receive a nil channel policy.\n\t\t\t\t\treturn cb(c, e)\n\t\t\t\t},\n\t\t\t)\n\t\t},\n\t\tPropagateChanPolicyUpdate: s.authGossiper.PropagateChanPolicyUpdate,\n\t\tUpdateForwardingPolicies:  s.htlcSwitch.UpdateForwardingPolicies,\n\t\tFetchChannel:              s.chanStateDB.FetchChannel,\n\t\tAddEdge: func(edge *models.ChannelEdgeInfo) error {\n\t\t\treturn s.graphBuilder.AddEdge(edge)\n\t\t},\n\t}\n\n\tutxnStore, err := contractcourt.NewNurseryStore(\n\t\ts.cfg.ActiveNetParams.GenesisHash, dbs.ChanStateDB,\n\t)\n\tif err != nil {\n\t\tsrvrLog.Errorf(\"unable to create nursery store: %v\", err)\n\t\treturn nil, err\n\t}\n\n\tsweeperStore, err := sweep.NewSweeperStore(\n\t\tdbs.ChanStateDB, s.cfg.ActiveNetParams.GenesisHash,\n\t)\n\tif err != nil {\n\t\tsrvrLog.Errorf(\"unable to create sweeper store: %v\", err)\n\t\treturn nil, err\n\t}\n\n\taggregator := sweep.NewBudgetAggregator(\n\t\tcc.FeeEstimator, sweep.DefaultMaxInputsPerTx,\n\t\ts.implCfg.AuxSweeper,\n\t)\n\n\ts.txPublisher = sweep.NewTxPublisher(sweep.TxPublisherConfig{\n\t\tSigner:     cc.Wallet.Cfg.Signer,\n\t\tWallet:     cc.Wallet,\n\t\tEstimator:  cc.FeeEstimator,\n\t\tNotifier:   cc.ChainNotifier,\n\t\tAuxSweeper: s.implCfg.AuxSweeper,\n\t})\n\n\ts.sweeper = sweep.New(&sweep.UtxoSweeperConfig{\n\t\tFeeEstimator: cc.FeeEstimator,\n\t\tGenSweepScript: newSweepPkScriptGen(\n\t\t\tcc.Wallet, s.cfg.ActiveNetParams.Params,\n\t\t),\n\t\tSigner:               cc.Wallet.Cfg.Signer,\n\t\tWallet:               newSweeperWallet(cc.Wallet),\n\t\tMempool:              cc.MempoolNotifier,\n\t\tNotifier:             cc.ChainNotifier,\n\t\tStore:                sweeperStore,\n\t\tMaxInputsPerTx:       sweep.DefaultMaxInputsPerTx,\n\t\tMaxFeeRate:           cfg.Sweeper.MaxFeeRate,\n\t\tAggregator:           aggregator,\n\t\tPublisher:            s.txPublisher,\n\t\tNoDeadlineConfTarget: cfg.Sweeper.NoDeadlineConfTarget,\n\t})\n\n\ts.utxoNursery = contractcourt.NewUtxoNursery(&contractcourt.NurseryConfig{\n\t\tChainIO:             cc.ChainIO,\n\t\tConfDepth:           1,\n\t\tFetchClosedChannels: s.chanStateDB.FetchClosedChannels,\n\t\tFetchClosedChannel:  s.chanStateDB.FetchClosedChannel,\n\t\tNotifier:            cc.ChainNotifier,\n\t\tPublishTransaction:  cc.Wallet.PublishTransaction,\n\t\tStore:               utxnStore,\n\t\tSweepInput:          s.sweeper.SweepInput,\n\t\tBudget:              s.cfg.Sweeper.Budget,\n\t})\n\n\t// Construct a closure that wraps the htlcswitch's CloseLink method.\n\tcloseLink := func(chanPoint *wire.OutPoint,\n\t\tclosureType contractcourt.ChannelCloseType) {\n\t\t// TODO(conner): Properly respect the update and error channels\n\t\t// returned by CloseLink.\n\n\t\t// Instruct the switch to close the channel.  Provide no close out\n\t\t// delivery script or target fee per kw because user input is not\n\t\t// available when the remote peer closes the channel.\n\t\ts.htlcSwitch.CloseLink(chanPoint, closureType, 0, 0, nil)\n\t}\n\n\t// We will use the following channel to reliably hand off contract\n\t// breach events from the ChannelArbitrator to the BreachArbitrator,\n\tcontractBreaches := make(chan *contractcourt.ContractBreachEvent, 1)\n\n\ts.breachArbitrator = contractcourt.NewBreachArbitrator(\n\t\t&contractcourt.BreachConfig{\n\t\t\tCloseLink: closeLink,\n\t\t\tDB:        s.chanStateDB,\n\t\t\tEstimator: s.cc.FeeEstimator,\n\t\t\tGenSweepScript: newSweepPkScriptGen(\n\t\t\t\tcc.Wallet, s.cfg.ActiveNetParams.Params,\n\t\t\t),\n\t\t\tNotifier:           cc.ChainNotifier,\n\t\t\tPublishTransaction: cc.Wallet.PublishTransaction,\n\t\t\tContractBreaches:   contractBreaches,\n\t\t\tSigner:             cc.Wallet.Cfg.Signer,\n\t\t\tStore: contractcourt.NewRetributionStore(\n\t\t\t\tdbs.ChanStateDB,\n\t\t\t),\n\t\t\tAuxSweeper: s.implCfg.AuxSweeper,\n\t\t},\n\t)\n\n\t//nolint:ll\n\ts.chainArb = contractcourt.NewChainArbitrator(contractcourt.ChainArbitratorConfig{\n\t\tChainHash:              *s.cfg.ActiveNetParams.GenesisHash,\n\t\tIncomingBroadcastDelta: lncfg.DefaultIncomingBroadcastDelta,\n\t\tOutgoingBroadcastDelta: lncfg.DefaultOutgoingBroadcastDelta,\n\t\tNewSweepAddr: func() ([]byte, error) {\n\t\t\taddr, err := newSweepPkScriptGen(\n\t\t\t\tcc.Wallet, netParams,\n\t\t\t)().Unpack()\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\treturn addr.DeliveryAddress, nil\n\t\t},\n\t\tPublishTx: cc.Wallet.PublishTransaction,\n\t\tDeliverResolutionMsg: func(msgs ...contractcourt.ResolutionMsg) error {\n\t\t\tfor _, msg := range msgs {\n\t\t\t\terr := s.htlcSwitch.ProcessContractResolution(msg)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\t},\n\t\tIncubateOutputs: func(chanPoint wire.OutPoint,\n\t\t\toutHtlcRes fn.Option[lnwallet.OutgoingHtlcResolution],\n\t\t\tinHtlcRes fn.Option[lnwallet.IncomingHtlcResolution],\n\t\t\tbroadcastHeight uint32,\n\t\t\tdeadlineHeight fn.Option[int32]) error {\n\n\t\t\treturn s.utxoNursery.IncubateOutputs(\n\t\t\t\tchanPoint, outHtlcRes, inHtlcRes,\n\t\t\t\tbroadcastHeight, deadlineHeight,\n\t\t\t)\n\t\t},\n\t\tPreimageDB:   s.witnessBeacon,\n\t\tNotifier:     cc.ChainNotifier,\n\t\tMempool:      cc.MempoolNotifier,\n\t\tSigner:       cc.Wallet.Cfg.Signer,\n\t\tFeeEstimator: cc.FeeEstimator,\n\t\tChainIO:      cc.ChainIO,\n\t\tMarkLinkInactive: func(chanPoint wire.OutPoint) error {\n\t\t\tchanID := lnwire.NewChanIDFromOutPoint(chanPoint)\n\t\t\ts.htlcSwitch.RemoveLink(chanID)\n\t\t\treturn nil\n\t\t},\n\t\tIsOurAddress: cc.Wallet.IsOurAddress,\n\t\tContractBreach: func(chanPoint wire.OutPoint,\n\t\t\tbreachRet *lnwallet.BreachRetribution) error {\n\n\t\t\t// processACK will handle the BreachArbitrator ACKing\n\t\t\t// the event.\n\t\t\tfinalErr := make(chan error, 1)\n\t\t\tprocessACK := func(brarErr error) {\n\t\t\t\tif brarErr != nil {\n\t\t\t\t\tfinalErr <- brarErr\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\t// If the BreachArbitrator successfully handled\n\t\t\t\t// the event, we can signal that the handoff\n\t\t\t\t// was successful.\n\t\t\t\tfinalErr <- nil\n\t\t\t}\n\n\t\t\tevent := &contractcourt.ContractBreachEvent{\n\t\t\t\tChanPoint:         chanPoint,\n\t\t\t\tProcessACK:        processACK,\n\t\t\t\tBreachRetribution: breachRet,\n\t\t\t}\n\n\t\t\t// Send the contract breach event to the\n\t\t\t// BreachArbitrator.\n\t\t\tselect {\n\t\t\tcase contractBreaches <- event:\n\t\t\tcase <-s.quit:\n\t\t\t\treturn ErrServerShuttingDown\n\t\t\t}\n\n\t\t\t// We'll wait for a final error to be available from\n\t\t\t// the BreachArbitrator.\n\t\t\tselect {\n\t\t\tcase err := <-finalErr:\n\t\t\t\treturn err\n\t\t\tcase <-s.quit:\n\t\t\t\treturn ErrServerShuttingDown\n\t\t\t}\n\t\t},\n\t\tDisableChannel: func(chanPoint wire.OutPoint) error {\n\t\t\treturn s.chanStatusMgr.RequestDisable(chanPoint, false)\n\t\t},\n\t\tSweeper:                       s.sweeper,\n\t\tRegistry:                      s.invoices,\n\t\tNotifyClosedChannel:           s.channelNotifier.NotifyClosedChannelEvent,\n\t\tNotifyFullyResolvedChannel:    s.channelNotifier.NotifyFullyResolvedChannelEvent,\n\t\tOnionProcessor:                s.sphinx,\n\t\tPaymentsExpirationGracePeriod: cfg.PaymentsExpirationGracePeriod,\n\t\tIsForwardedHTLC:               s.htlcSwitch.IsForwardedHTLC,\n\t\tClock:                         clock.NewDefaultClock(),\n\t\tSubscribeBreachComplete:       s.breachArbitrator.SubscribeBreachComplete,\n\t\tPutFinalHtlcOutcome:           s.chanStateDB.PutOnchainFinalHtlcOutcome,\n\t\tHtlcNotifier:                  s.htlcNotifier,\n\t\tBudget:                        *s.cfg.Sweeper.Budget,\n\n\t\t// TODO(yy): remove this hack once PaymentCircuit is interfaced.\n\t\tQueryIncomingCircuit: func(\n\t\t\tcircuit models.CircuitKey) *models.CircuitKey {\n\n\t\t\t// Get the circuit map.\n\t\t\tcircuits := s.htlcSwitch.CircuitLookup()\n\n\t\t\t// Lookup the outgoing circuit.\n\t\t\tpc := circuits.LookupOpenCircuit(circuit)\n\t\t\tif pc == nil {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\treturn &pc.Incoming\n\t\t},\n\t\tAuxLeafStore: implCfg.AuxLeafStore,\n\t\tAuxSigner:    implCfg.AuxSigner,\n\t\tAuxResolver:  implCfg.AuxContractResolver,\n\t}, dbs.ChanStateDB)\n\n\t// Select the configuration and funding parameters for Bitcoin.\n\tchainCfg := cfg.Bitcoin\n\tminRemoteDelay := funding.MinBtcRemoteDelay\n\tmaxRemoteDelay := funding.MaxBtcRemoteDelay\n\n\tvar chanIDSeed [32]byte\n\tif _, err := rand.Read(chanIDSeed[:]); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Wrap the DeleteChannelEdges method so that the funding manager can\n\t// use it without depending on several layers of indirection.\n\tdeleteAliasEdge := func(scid lnwire.ShortChannelID) (\n\t\t*models.ChannelEdgePolicy, error) {\n\n\t\tinfo, e1, e2, err := s.graphDB.FetchChannelEdgesByID(\n\t\t\tscid.ToUint64(),\n\t\t)\n\t\tif errors.Is(err, graphdb.ErrEdgeNotFound) {\n\t\t\t// This is unlikely but there is a slim chance of this\n\t\t\t// being hit if lnd was killed via SIGKILL and the\n\t\t\t// funding manager was stepping through the delete\n\t\t\t// alias edge logic.\n\t\t\treturn nil, nil\n\t\t} else if err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Grab our key to find our policy.\n\t\tvar ourKey [33]byte\n\t\tcopy(ourKey[:], nodeKeyDesc.PubKey.SerializeCompressed())\n\n\t\tvar ourPolicy *models.ChannelEdgePolicy\n\t\tif info != nil && info.NodeKey1Bytes == ourKey {\n\t\t\tourPolicy = e1\n\t\t} else {\n\t\t\tourPolicy = e2\n\t\t}\n\n\t\tif ourPolicy == nil {\n\t\t\t// Something is wrong, so return an error.\n\t\t\treturn nil, fmt.Errorf(\"we don't have an edge\")\n\t\t}\n\n\t\terr = s.graphDB.DeleteChannelEdges(\n\t\t\tfalse, false, scid.ToUint64(),\n\t\t)\n\t\treturn ourPolicy, err\n\t}\n\n\t// For the reservationTimeout and the zombieSweeperInterval different\n\t// values are set in case we are in a dev environment so enhance test\n\t// capacilities.\n\treservationTimeout := chanfunding.DefaultReservationTimeout\n\tzombieSweeperInterval := lncfg.DefaultZombieSweeperInterval\n\n\t// Get the development config for funding manager. If we are not in\n\t// development mode, this would be nil.\n\tvar devCfg *funding.DevConfig\n\tif lncfg.IsDevBuild() {\n\t\tdevCfg = &funding.DevConfig{\n\t\t\tProcessChannelReadyWait: cfg.Dev.ChannelReadyWait(),\n\t\t}\n\n\t\treservationTimeout = cfg.Dev.GetReservationTimeout()\n\t\tzombieSweeperInterval = cfg.Dev.GetZombieSweeperInterval()\n\n\t\tsrvrLog.Debugf(\"Using the dev config for the fundingMgr: %v, \"+\n\t\t\t\"reservationTimeout=%v, zombieSweeperInterval=%v\",\n\t\t\tdevCfg, reservationTimeout, zombieSweeperInterval)\n\t}\n\n\t//nolint:ll\n\ts.fundingMgr, err = funding.NewFundingManager(funding.Config{\n\t\tDev:                devCfg,\n\t\tNoWumboChans:       !cfg.ProtocolOptions.Wumbo(),\n\t\tIDKey:              nodeKeyDesc.PubKey,\n\t\tIDKeyLoc:           nodeKeyDesc.KeyLocator,\n\t\tWallet:             cc.Wallet,\n\t\tPublishTransaction: cc.Wallet.PublishTransaction,\n\t\tUpdateLabel: func(hash chainhash.Hash, label string) error {\n\t\t\treturn cc.Wallet.LabelTransaction(hash, label, true)\n\t\t},\n\t\tNotifier:     cc.ChainNotifier,\n\t\tChannelDB:    s.chanStateDB,\n\t\tFeeEstimator: cc.FeeEstimator,\n\t\tSignMessage:  cc.MsgSigner.SignMessage,\n\t\tCurrentNodeAnnouncement: func() (lnwire.NodeAnnouncement,\n\t\t\terror) {\n\n\t\t\treturn s.genNodeAnnouncement(nil)\n\t\t},\n\t\tSendAnnouncement:     s.authGossiper.ProcessLocalAnnouncement,\n\t\tNotifyWhenOnline:     s.NotifyWhenOnline,\n\t\tTempChanIDSeed:       chanIDSeed,\n\t\tFindChannel:          s.findChannel,\n\t\tDefaultRoutingPolicy: cc.RoutingPolicy,\n\t\tDefaultMinHtlcIn:     cc.MinHtlcIn,\n\t\tNumRequiredConfs: func(chanAmt btcutil.Amount,\n\t\t\tpushAmt lnwire.MilliSatoshi) uint16 {\n\t\t\t// For large channels we increase the number\n\t\t\t// of confirmations we require for the\n\t\t\t// channel to be considered open. As it is\n\t\t\t// always the responder that gets to choose\n\t\t\t// value, the pushAmt is value being pushed\n\t\t\t// to us. This means we have more to lose\n\t\t\t// in the case this gets re-orged out, and\n\t\t\t// we will require more confirmations before\n\t\t\t// we consider it open.\n\n\t\t\t// In case the user has explicitly specified\n\t\t\t// a default value for the number of\n\t\t\t// confirmations, we use it.\n\t\t\tdefaultConf := uint16(chainCfg.DefaultNumChanConfs)\n\t\t\tif defaultConf != 0 {\n\t\t\t\treturn defaultConf\n\t\t\t}\n\n\t\t\tminConf := uint64(3)\n\t\t\tmaxConf := uint64(6)\n\n\t\t\t// If this is a wumbo channel, then we'll require the\n\t\t\t// max amount of confirmations.\n\t\t\tif chanAmt > MaxFundingAmount {\n\t\t\t\treturn uint16(maxConf)\n\t\t\t}\n\n\t\t\t// If not we return a value scaled linearly\n\t\t\t// between 3 and 6, depending on channel size.\n\t\t\t// TODO(halseth): Use 1 as minimum?\n\t\t\tmaxChannelSize := uint64(\n\t\t\t\tlnwire.NewMSatFromSatoshis(MaxFundingAmount))\n\t\t\tstake := lnwire.NewMSatFromSatoshis(chanAmt) + pushAmt\n\t\t\tconf := maxConf * uint64(stake) / maxChannelSize\n\t\t\tif conf < minConf {\n\t\t\t\tconf = minConf\n\t\t\t}\n\t\t\tif conf > maxConf {\n\t\t\t\tconf = maxConf\n\t\t\t}\n\t\t\treturn uint16(conf)\n\t\t},\n\t\tRequiredRemoteDelay: func(chanAmt btcutil.Amount) uint16 {\n\t\t\t// We scale the remote CSV delay (the time the\n\t\t\t// remote have to claim funds in case of a unilateral\n\t\t\t// close) linearly from minRemoteDelay blocks\n\t\t\t// for small channels, to maxRemoteDelay blocks\n\t\t\t// for channels of size MaxFundingAmount.\n\n\t\t\t// In case the user has explicitly specified\n\t\t\t// a default value for the remote delay, we\n\t\t\t// use it.\n\t\t\tdefaultDelay := uint16(chainCfg.DefaultRemoteDelay)\n\t\t\tif defaultDelay > 0 {\n\t\t\t\treturn defaultDelay\n\t\t\t}\n\n\t\t\t// If this is a wumbo channel, then we'll require the\n\t\t\t// max value.\n\t\t\tif chanAmt > MaxFundingAmount {\n\t\t\t\treturn maxRemoteDelay\n\t\t\t}\n\n\t\t\t// If not we scale according to channel size.\n\t\t\tdelay := uint16(btcutil.Amount(maxRemoteDelay) *\n\t\t\t\tchanAmt / MaxFundingAmount)\n\t\t\tif delay < minRemoteDelay {\n\t\t\t\tdelay = minRemoteDelay\n\t\t\t}\n\t\t\tif delay > maxRemoteDelay {\n\t\t\t\tdelay = maxRemoteDelay\n\t\t\t}\n\t\t\treturn delay\n\t\t},\n\t\tWatchNewChannel: func(channel *channeldb.OpenChannel,\n\t\t\tpeerKey *btcec.PublicKey) error {\n\n\t\t\t// First, we'll mark this new peer as a persistent peer\n\t\t\t// for re-connection purposes. If the peer is not yet\n\t\t\t// tracked or the user hasn't requested it to be perm,\n\t\t\t// we'll set false to prevent the server from continuing\n\t\t\t// to connect to this peer even if the number of\n\t\t\t// channels with this peer is zero.\n\t\t\ts.mu.Lock()\n\t\t\tpubStr := string(peerKey.SerializeCompressed())\n\t\t\tif _, ok := s.persistentPeers[pubStr]; !ok {\n\t\t\t\ts.persistentPeers[pubStr] = false\n\t\t\t}\n\t\t\ts.mu.Unlock()\n\n\t\t\t// With that taken care of, we'll send this channel to\n\t\t\t// the chain arb so it can react to on-chain events.\n\t\t\treturn s.chainArb.WatchNewChannel(channel)\n\t\t},\n\t\tReportShortChanID: func(chanPoint wire.OutPoint) error {\n\t\t\tcid := lnwire.NewChanIDFromOutPoint(chanPoint)\n\t\t\treturn s.htlcSwitch.UpdateShortChanID(cid)\n\t\t},\n\t\tRequiredRemoteChanReserve: func(chanAmt,\n\t\t\tdustLimit btcutil.Amount) btcutil.Amount {\n\n\t\t\t// By default, we'll require the remote peer to maintain\n\t\t\t// at least 1% of the total channel capacity at all\n\t\t\t// times. If this value ends up dipping below the dust\n\t\t\t// limit, then we'll use the dust limit itself as the\n\t\t\t// reserve as required by BOLT #2.\n\t\t\treserve := chanAmt / 100\n\t\t\tif reserve < dustLimit {\n\t\t\t\treserve = dustLimit\n\t\t\t}\n\n\t\t\treturn reserve\n\t\t},\n\t\tRequiredRemoteMaxValue: func(chanAmt btcutil.Amount) lnwire.MilliSatoshi {\n\t\t\t// By default, we'll allow the remote peer to fully\n\t\t\t// utilize the full bandwidth of the channel, minus our\n\t\t\t// required reserve.\n\t\t\treserve := lnwire.NewMSatFromSatoshis(chanAmt / 100)\n\t\t\treturn lnwire.NewMSatFromSatoshis(chanAmt) - reserve\n\t\t},\n\t\tRequiredRemoteMaxHTLCs: func(chanAmt btcutil.Amount) uint16 {\n\t\t\tif cfg.DefaultRemoteMaxHtlcs > 0 {\n\t\t\t\treturn cfg.DefaultRemoteMaxHtlcs\n\t\t\t}\n\n\t\t\t// By default, we'll permit them to utilize the full\n\t\t\t// channel bandwidth.\n\t\t\treturn uint16(input.MaxHTLCNumber / 2)\n\t\t},\n\t\tZombieSweeperInterval:         zombieSweeperInterval,\n\t\tReservationTimeout:            reservationTimeout,\n\t\tMinChanSize:                   btcutil.Amount(cfg.MinChanSize),\n\t\tMaxChanSize:                   btcutil.Amount(cfg.MaxChanSize),\n\t\tMaxPendingChannels:            cfg.MaxPendingChannels,\n\t\tRejectPush:                    cfg.RejectPush,\n\t\tMaxLocalCSVDelay:              chainCfg.MaxLocalDelay,\n\t\tNotifyOpenChannelEvent:        s.channelNotifier.NotifyOpenChannelEvent,\n\t\tOpenChannelPredicate:          chanPredicate,\n\t\tNotifyPendingOpenChannelEvent: s.channelNotifier.NotifyPendingOpenChannelEvent,\n\t\tEnableUpfrontShutdown:         cfg.EnableUpfrontShutdown,\n\t\tMaxAnchorsCommitFeeRate: chainfee.SatPerKVByte(\n\t\t\ts.cfg.MaxCommitFeeRateAnchors * 1000).FeePerKWeight(),\n\t\tDeleteAliasEdge:      deleteAliasEdge,\n\t\tAliasManager:         s.aliasMgr,\n\t\tIsSweeperOutpoint:    s.sweeper.IsSweeperOutpoint,\n\t\tAuxFundingController: implCfg.AuxFundingController,\n\t\tAuxSigner:            implCfg.AuxSigner,\n\t\tAuxResolver:          implCfg.AuxContractResolver,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Next, we'll assemble the sub-system that will maintain an on-disk\n\t// static backup of the latest channel state.\n\tchanNotifier := &channelNotifier{\n\t\tchanNotifier: s.channelNotifier,\n\t\taddrs:        s.addrSource,\n\t}\n\tbackupFile := chanbackup.NewMultiFile(cfg.BackupFilePath)\n\tstartingChans, err := chanbackup.FetchStaticChanBackups(\n\t\ts.chanStateDB, s.addrSource,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ts.chanSubSwapper, err = chanbackup.NewSubSwapper(\n\t\tstartingChans, chanNotifier, s.cc.KeyRing, backupFile,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Assemble a peer notifier which will provide clients with subscriptions\n\t// to peer online and offline events.\n\ts.peerNotifier = peernotifier.New()\n\n\t// Create a channel event store which monitors all open channels.\n\ts.chanEventStore = chanfitness.NewChannelEventStore(&chanfitness.Config{\n\t\tSubscribeChannelEvents: func() (subscribe.Subscription, error) {\n\t\t\treturn s.channelNotifier.SubscribeChannelEvents()\n\t\t},\n\t\tSubscribePeerEvents: func() (subscribe.Subscription, error) {\n\t\t\treturn s.peerNotifier.SubscribePeerEvents()\n\t\t},\n\t\tGetOpenChannels: s.chanStateDB.FetchAllOpenChannels,\n\t\tClock:           clock.NewDefaultClock(),\n\t\tReadFlapCount:   s.miscDB.ReadFlapCount,\n\t\tWriteFlapCount:  s.miscDB.WriteFlapCounts,\n\t\tFlapCountTicker: ticker.New(chanfitness.FlapCountFlushRate),\n\t})\n\n\tif cfg.WtClient.Active {\n\t\tpolicy := wtpolicy.DefaultPolicy()\n\t\tpolicy.MaxUpdates = cfg.WtClient.MaxUpdates\n\n\t\t// We expose the sweep fee rate in sat/vbyte, but the tower\n\t\t// protocol operations on sat/kw.\n\t\tsweepRateSatPerVByte := chainfee.SatPerKVByte(\n\t\t\t1000 * cfg.WtClient.SweepFeeRate,\n\t\t)\n\n\t\tpolicy.SweepFeeRate = sweepRateSatPerVByte.FeePerKWeight()\n\n\t\tif err := policy.Validate(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// authDial is the wrapper around the btrontide.Dial for the\n\t\t// watchtower.\n\t\tauthDial := func(localKey keychain.SingleKeyECDH,\n\t\t\tnetAddr *lnwire.NetAddress,\n\t\t\tdialer tor.DialFunc) (wtserver.Peer, error) {\n\n\t\t\treturn brontide.Dial(\n\t\t\t\tlocalKey, netAddr, cfg.ConnectionTimeout, dialer,\n\t\t\t)\n\t\t}\n\n\t\t// buildBreachRetribution is a call-back that can be used to\n\t\t// query the BreachRetribution info and channel type given a\n\t\t// channel ID and commitment height.\n\t\tbuildBreachRetribution := func(chanID lnwire.ChannelID,\n\t\t\tcommitHeight uint64) (*lnwallet.BreachRetribution,\n\t\t\tchanneldb.ChannelType, error) {\n\n\t\t\tchannel, err := s.chanStateDB.FetchChannelByID(\n\t\t\t\tnil, chanID,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, 0, err\n\t\t\t}\n\n\t\t\tbr, err := lnwallet.NewBreachRetribution(\n\t\t\t\tchannel, commitHeight, 0, nil,\n\t\t\t\timplCfg.AuxLeafStore,\n\t\t\t\timplCfg.AuxContractResolver,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, 0, err\n\t\t\t}\n\n\t\t\treturn br, channel.ChanType, nil\n\t\t}\n\n\t\tfetchClosedChannel := s.chanStateDB.FetchClosedChannelForID\n\n\t\t// Copy the policy for legacy channels and set the blob flag\n\t\t// signalling support for anchor channels.\n\t\tanchorPolicy := policy\n\t\tanchorPolicy.BlobType |= blob.Type(blob.FlagAnchorChannel)\n\n\t\t// Copy the policy for legacy channels and set the blob flag\n\t\t// signalling support for taproot channels.\n\t\ttaprootPolicy := policy\n\t\ttaprootPolicy.TxPolicy.BlobType |= blob.Type(\n\t\t\tblob.FlagTaprootChannel,\n\t\t)\n\n\t\ts.towerClientMgr, err = wtclient.NewManager(&wtclient.Config{\n\t\t\tFetchClosedChannel:     fetchClosedChannel,\n\t\t\tBuildBreachRetribution: buildBreachRetribution,\n\t\t\tSessionCloseRange:      cfg.WtClient.SessionCloseRange,\n\t\t\tChainNotifier:          s.cc.ChainNotifier,\n\t\t\tSubscribeChannelEvents: func() (subscribe.Subscription,\n\t\t\t\terror) {\n\n\t\t\t\treturn s.channelNotifier.\n\t\t\t\t\tSubscribeChannelEvents()\n\t\t\t},\n\t\t\tSigner: cc.Wallet.Cfg.Signer,\n\t\t\tNewAddress: func() ([]byte, error) {\n\t\t\t\taddr, err := newSweepPkScriptGen(\n\t\t\t\t\tcc.Wallet, netParams,\n\t\t\t\t)().Unpack()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\n\t\t\t\treturn addr.DeliveryAddress, nil\n\t\t\t},\n\t\t\tSecretKeyRing:      s.cc.KeyRing,\n\t\t\tDial:               cfg.net.Dial,\n\t\t\tAuthDial:           authDial,\n\t\t\tDB:                 dbs.TowerClientDB,\n\t\t\tChainHash:          *s.cfg.ActiveNetParams.GenesisHash,\n\t\t\tMinBackoff:         10 * time.Second,\n\t\t\tMaxBackoff:         5 * time.Minute,\n\t\t\tMaxTasksInMemQueue: cfg.WtClient.MaxTasksInMemQueue,\n\t\t}, policy, anchorPolicy, taprootPolicy)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tif len(cfg.ExternalHosts) != 0 {\n\t\tadvertisedIPs := make(map[string]struct{})\n\t\tfor _, addr := range s.currentNodeAnn.Addresses {\n\t\t\tadvertisedIPs[addr.String()] = struct{}{}\n\t\t}\n\n\t\ts.hostAnn = netann.NewHostAnnouncer(netann.HostAnnouncerConfig{\n\t\t\tHosts:         cfg.ExternalHosts,\n\t\t\tRefreshTicker: ticker.New(defaultHostSampleInterval),\n\t\t\tLookupHost: func(host string) (net.Addr, error) {\n\t\t\t\treturn lncfg.ParseAddressString(\n\t\t\t\t\thost, strconv.Itoa(defaultPeerPort),\n\t\t\t\t\tcfg.net.ResolveTCPAddr,\n\t\t\t\t)\n\t\t\t},\n\t\t\tAdvertisedIPs: advertisedIPs,\n\t\t\tAnnounceNewIPs: netann.IPAnnouncer(\n\t\t\t\tfunc(modifier ...netann.NodeAnnModifier) (\n\t\t\t\t\tlnwire.NodeAnnouncement, error) {\n\n\t\t\t\t\treturn s.genNodeAnnouncement(\n\t\t\t\t\t\tnil, modifier...,\n\t\t\t\t\t)\n\t\t\t\t}),\n\t\t})\n\t}\n\n\t// Create liveness monitor.\n\ts.createLivenessMonitor(cfg, cc, leaderElector)\n\n\t// Create the connection manager which will be responsible for\n\t// maintaining persistent outbound connections and also accepting new\n\t// incoming connections\n\tcmgr, err := connmgr.New(&connmgr.Config{\n\t\tListeners:      listeners,\n\t\tOnAccept:       s.InboundPeerConnected,\n\t\tRetryDuration:  time.Second * 5,\n\t\tTargetOutbound: 100,\n\t\tDial: noiseDial(\n\t\t\tnodeKeyECDH, s.cfg.net, s.cfg.ConnectionTimeout,\n\t\t),\n\t\tOnConnection: s.OutboundPeerConnected,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ts.connMgr = cmgr\n\n\t// Finally, register the subsystems in blockbeat.\n\ts.registerBlockConsumers()\n\n\treturn s, nil\n}\n\n// UpdateRoutingConfig is a callback function to update the routing config\n// values in the main cfg.\nfunc (s *server) UpdateRoutingConfig(cfg *routing.MissionControlConfig) {\n\trouterCfg := s.cfg.SubRPCServers.RouterRPC\n\n\tswitch c := cfg.Estimator.Config().(type) {\n\tcase routing.AprioriConfig:\n\t\trouterCfg.ProbabilityEstimatorType =\n\t\t\trouting.AprioriEstimatorName\n\n\t\ttargetCfg := routerCfg.AprioriConfig\n\t\ttargetCfg.PenaltyHalfLife = c.PenaltyHalfLife\n\t\ttargetCfg.Weight = c.AprioriWeight\n\t\ttargetCfg.CapacityFraction = c.CapacityFraction\n\t\ttargetCfg.HopProbability = c.AprioriHopProbability\n\n\tcase routing.BimodalConfig:\n\t\trouterCfg.ProbabilityEstimatorType =\n\t\t\trouting.BimodalEstimatorName\n\n\t\ttargetCfg := routerCfg.BimodalConfig\n\t\ttargetCfg.Scale = int64(c.BimodalScaleMsat)\n\t\ttargetCfg.NodeWeight = c.BimodalNodeWeight\n\t\ttargetCfg.DecayTime = c.BimodalDecayTime\n\t}\n\n\trouterCfg.MaxMcHistory = cfg.MaxMcHistory\n}\n\n// registerBlockConsumers registers the subsystems that consume block events.\n// By calling `RegisterQueue`, a list of subsystems are registered in the\n// blockbeat for block notifications. When a new block arrives, the subsystems\n// in the same queue are notified sequentially, and different queues are\n// notified concurrently.\n//\n// NOTE: To put a subsystem in a different queue, create a slice and pass it to\n// a new `RegisterQueue` call.\nfunc (s *server) registerBlockConsumers() {\n\t// In this queue, when a new block arrives, it will be received and\n\t// processed in this order: chainArb -> sweeper -> txPublisher.\n\tconsumers := []chainio.Consumer{\n\t\ts.chainArb,\n\t\ts.sweeper,\n\t\ts.txPublisher,\n\t}\n\ts.blockbeatDispatcher.RegisterQueue(consumers)\n}\n\n// signAliasUpdate takes a ChannelUpdate and returns the signature. This is\n// used for option_scid_alias channels where the ChannelUpdate to be sent back\n// may differ from what is on disk.\nfunc (s *server) signAliasUpdate(u *lnwire.ChannelUpdate1) (*ecdsa.Signature,\n\terror) {\n\n\tdata, err := u.DataToSign()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn s.cc.MsgSigner.SignMessage(s.identityKeyLoc, data, true)\n}\n\n// createLivenessMonitor creates a set of health checks using our configured\n// values and uses these checks to create a liveness monitor. Available\n// health checks,\n//   - chainHealthCheck (will be disabled for --nochainbackend mode)\n//   - diskCheck\n//   - tlsHealthCheck\n//   - torController, only created when tor is enabled.\n//\n// If a health check has been disabled by setting attempts to 0, our monitor\n// will not run it.\nfunc (s *server) createLivenessMonitor(cfg *Config, cc *chainreg.ChainControl,\n\tleaderElector cluster.LeaderElector) {\n\n\tchainBackendAttempts := cfg.HealthChecks.ChainCheck.Attempts\n\tif cfg.Bitcoin.Node == \"nochainbackend\" {\n\t\tsrvrLog.Info(\"Disabling chain backend checks for \" +\n\t\t\t\"nochainbackend mode\")\n\n\t\tchainBackendAttempts = 0\n\t}\n\n\tchainHealthCheck := healthcheck.NewObservation(\n\t\t\"chain backend\",\n\t\tcc.HealthCheck,\n\t\tcfg.HealthChecks.ChainCheck.Interval,\n\t\tcfg.HealthChecks.ChainCheck.Timeout,\n\t\tcfg.HealthChecks.ChainCheck.Backoff,\n\t\tchainBackendAttempts,\n\t)\n\n\tdiskCheck := healthcheck.NewObservation(\n\t\t\"disk space\",\n\t\tfunc() error {\n\t\t\tfree, err := healthcheck.AvailableDiskSpaceRatio(\n\t\t\t\tcfg.LndDir,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t// If we have more free space than we require,\n\t\t\t// we return a nil error.\n\t\t\tif free > cfg.HealthChecks.DiskCheck.RequiredRemaining {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\treturn fmt.Errorf(\"require: %v free space, got: %v\",\n\t\t\t\tcfg.HealthChecks.DiskCheck.RequiredRemaining,\n\t\t\t\tfree)\n\t\t},\n\t\tcfg.HealthChecks.DiskCheck.Interval,\n\t\tcfg.HealthChecks.DiskCheck.Timeout,\n\t\tcfg.HealthChecks.DiskCheck.Backoff,\n\t\tcfg.HealthChecks.DiskCheck.Attempts,\n\t)\n\n\ttlsHealthCheck := healthcheck.NewObservation(\n\t\t\"tls\",\n\t\tfunc() error {\n\t\t\texpired, expTime, err := s.tlsManager.IsCertExpired(\n\t\t\t\ts.cc.KeyRing,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif expired {\n\t\t\t\treturn fmt.Errorf(\"TLS certificate is \"+\n\t\t\t\t\t\"expired as of %v\", expTime)\n\t\t\t}\n\n\t\t\t// If the certificate is not outdated, no error needs\n\t\t\t// to be returned\n\t\t\treturn nil\n\t\t},\n\t\tcfg.HealthChecks.TLSCheck.Interval,\n\t\tcfg.HealthChecks.TLSCheck.Timeout,\n\t\tcfg.HealthChecks.TLSCheck.Backoff,\n\t\tcfg.HealthChecks.TLSCheck.Attempts,\n\t)\n\n\tchecks := []*healthcheck.Observation{\n\t\tchainHealthCheck, diskCheck, tlsHealthCheck,\n\t}\n\n\t// If Tor is enabled, add the healthcheck for tor connection.\n\tif s.torController != nil {\n\t\ttorConnectionCheck := healthcheck.NewObservation(\n\t\t\t\"tor connection\",\n\t\t\tfunc() error {\n\t\t\t\treturn healthcheck.CheckTorServiceStatus(\n\t\t\t\t\ts.torController,\n\t\t\t\t\ts.createNewHiddenService,\n\t\t\t\t)\n\t\t\t},\n\t\t\tcfg.HealthChecks.TorConnection.Interval,\n\t\t\tcfg.HealthChecks.TorConnection.Timeout,\n\t\t\tcfg.HealthChecks.TorConnection.Backoff,\n\t\t\tcfg.HealthChecks.TorConnection.Attempts,\n\t\t)\n\t\tchecks = append(checks, torConnectionCheck)\n\t}\n\n\t// If remote signing is enabled, add the healthcheck for the remote\n\t// signing RPC interface.\n\tif s.cfg.RemoteSigner != nil && s.cfg.RemoteSigner.Enable {\n\t\t// Because we have two cascading timeouts here, we need to add\n\t\t// some slack to the \"outer\" one of them in case the \"inner\"\n\t\t// returns exactly on time.\n\t\toverhead := time.Millisecond * 10\n\n\t\tremoteSignerConnectionCheck := healthcheck.NewObservation(\n\t\t\t\"remote signer connection\",\n\t\t\trpcwallet.HealthCheck(\n\t\t\t\ts.cfg.RemoteSigner,\n\n\t\t\t\t// For the health check we might to be even\n\t\t\t\t// stricter than the initial/normal connect, so\n\t\t\t\t// we use the health check timeout here.\n\t\t\t\tcfg.HealthChecks.RemoteSigner.Timeout,\n\t\t\t),\n\t\t\tcfg.HealthChecks.RemoteSigner.Interval,\n\t\t\tcfg.HealthChecks.RemoteSigner.Timeout+overhead,\n\t\t\tcfg.HealthChecks.RemoteSigner.Backoff,\n\t\t\tcfg.HealthChecks.RemoteSigner.Attempts,\n\t\t)\n\t\tchecks = append(checks, remoteSignerConnectionCheck)\n\t}\n\n\t// If we have a leader elector, we add a health check to ensure we are\n\t// still the leader. During normal operation, we should always be the\n\t// leader, but there are circumstances where this may change, such as\n\t// when we lose network connectivity for long enough expiring out lease.\n\tif leaderElector != nil {\n\t\tleaderCheck := healthcheck.NewObservation(\n\t\t\t\"leader status\",\n\t\t\tfunc() error {\n\t\t\t\t// Check if we are still the leader. Note that\n\t\t\t\t// we don't need to use a timeout context here\n\t\t\t\t// as the healthcheck observer will handle the\n\t\t\t\t// timeout case for us.\n\t\t\t\ttimeoutCtx, cancel := context.WithTimeout(\n\t\t\t\t\tcontext.Background(),\n\t\t\t\t\tcfg.HealthChecks.LeaderCheck.Timeout,\n\t\t\t\t)\n\t\t\t\tdefer cancel()\n\n\t\t\t\tleader, err := leaderElector.IsLeader(\n\t\t\t\t\ttimeoutCtx,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn fmt.Errorf(\"unable to check if \"+\n\t\t\t\t\t\t\"still leader: %v\", err)\n\t\t\t\t}\n\n\t\t\t\tif !leader {\n\t\t\t\t\tsrvrLog.Debug(\"Not the current leader\")\n\t\t\t\t\treturn fmt.Errorf(\"not the current \" +\n\t\t\t\t\t\t\"leader\")\n\t\t\t\t}\n\n\t\t\t\treturn nil\n\t\t\t},\n\t\t\tcfg.HealthChecks.LeaderCheck.Interval,\n\t\t\tcfg.HealthChecks.LeaderCheck.Timeout,\n\t\t\tcfg.HealthChecks.LeaderCheck.Backoff,\n\t\t\tcfg.HealthChecks.LeaderCheck.Attempts,\n\t\t)\n\n\t\tchecks = append(checks, leaderCheck)\n\t}\n\n\t// If we have not disabled all of our health checks, we create a\n\t// liveness monitor with our configured checks.\n\ts.livenessMonitor = healthcheck.NewMonitor(\n\t\t&healthcheck.Config{\n\t\t\tChecks:   checks,\n\t\t\tShutdown: srvrLog.Criticalf,\n\t\t},\n\t)\n}\n\n// Started returns true if the server has been started, and false otherwise.\n// NOTE: This function is safe for concurrent access.\nfunc (s *server) Started() bool {\n\treturn atomic.LoadInt32(&s.active) != 0\n}\n\n// cleaner is used to aggregate \"cleanup\" functions during an operation that\n// starts several subsystems. In case one of the subsystem fails to start\n// and a proper resource cleanup is required, the \"run\" method achieves this\n// by running all these added \"cleanup\" functions.\ntype cleaner []func() error\n\n// add is used to add a cleanup function to be called when\n// the run function is executed.\nfunc (c cleaner) add(cleanup func() error) cleaner {\n\treturn append(c, cleanup)\n}\n\n// run is used to run all the previousely added cleanup functions.\nfunc (c cleaner) run() {\n\tfor i := len(c) - 1; i >= 0; i-- {\n\t\tif err := c[i](); err != nil {\n\t\t\tsrvrLog.Infof(\"Cleanup failed: %v\", err)\n\t\t}\n\t}\n}\n\n// startLowLevelServices starts the low-level services of the server. These\n// services must be started successfully before running the main server. The\n// services are,\n// 1. the chain notifier.\n//\n// TODO(yy): identify and add more low-level services here.\nfunc (s *server) startLowLevelServices() error {\n\tvar startErr error\n\n\tcleanup := cleaner{}\n\n\tcleanup = cleanup.add(s.cc.ChainNotifier.Stop)\n\tif err := s.cc.ChainNotifier.Start(); err != nil {\n\t\tstartErr = err\n\t}\n\n\tif startErr != nil {\n\t\tcleanup.run()\n\t}\n\n\treturn startErr\n}\n\n// Start starts the main daemon server, all requested listeners, and any helper\n// goroutines.\n// NOTE: This function is safe for concurrent access.\n//\n//nolint:funlen\nfunc (s *server) Start() error {\n\t// Get the current blockbeat.\n\tbeat, err := s.getStartingBeat()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvar startErr error\n\n\t// If one sub system fails to start, the following code ensures that the\n\t// previous started ones are stopped. It also ensures a proper wallet\n\t// shutdown which is important for releasing its resources (boltdb, etc...)\n\tcleanup := cleaner{}\n\n\ts.start.Do(func() {\n\t\tcleanup = cleanup.add(s.customMessageServer.Stop)\n\t\tif err := s.customMessageServer.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tif s.hostAnn != nil {\n\t\t\tcleanup = cleanup.add(s.hostAnn.Stop)\n\t\t\tif err := s.hostAnn.Start(); err != nil {\n\t\t\t\tstartErr = err\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\tif s.livenessMonitor != nil {\n\t\t\tcleanup = cleanup.add(s.livenessMonitor.Stop)\n\t\t\tif err := s.livenessMonitor.Start(); err != nil {\n\t\t\t\tstartErr = err\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\t// Start the notification server. This is used so channel\n\t\t// management goroutines can be notified when a funding\n\t\t// transaction reaches a sufficient number of confirmations, or\n\t\t// when the input for the funding transaction is spent in an\n\t\t// attempt at an uncooperative close by the counterparty.\n\t\tcleanup = cleanup.add(s.sigPool.Stop)\n\t\tif err := s.sigPool.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tcleanup = cleanup.add(s.writePool.Stop)\n\t\tif err := s.writePool.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tcleanup = cleanup.add(s.readPool.Stop)\n\t\tif err := s.readPool.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tcleanup = cleanup.add(s.cc.BestBlockTracker.Stop)\n\t\tif err := s.cc.BestBlockTracker.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tcleanup = cleanup.add(s.channelNotifier.Stop)\n\t\tif err := s.channelNotifier.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tcleanup = cleanup.add(func() error {\n\t\t\treturn s.peerNotifier.Stop()\n\t\t})\n\t\tif err := s.peerNotifier.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tcleanup = cleanup.add(s.htlcNotifier.Stop)\n\t\tif err := s.htlcNotifier.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tif s.towerClientMgr != nil {\n\t\t\tcleanup = cleanup.add(s.towerClientMgr.Stop)\n\t\t\tif err := s.towerClientMgr.Start(); err != nil {\n\t\t\t\tstartErr = err\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\tcleanup = cleanup.add(s.txPublisher.Stop)\n\t\tif err := s.txPublisher.Start(beat); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tcleanup = cleanup.add(s.sweeper.Stop)\n\t\tif err := s.sweeper.Start(beat); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tcleanup = cleanup.add(s.utxoNursery.Stop)\n\t\tif err := s.utxoNursery.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tcleanup = cleanup.add(s.breachArbitrator.Stop)\n\t\tif err := s.breachArbitrator.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tcleanup = cleanup.add(s.fundingMgr.Stop)\n\t\tif err := s.fundingMgr.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\t// htlcSwitch must be started before chainArb since the latter\n\t\t// relies on htlcSwitch to deliver resolution message upon\n\t\t// start.\n\t\tcleanup = cleanup.add(s.htlcSwitch.Stop)\n\t\tif err := s.htlcSwitch.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tcleanup = cleanup.add(s.interceptableSwitch.Stop)\n\t\tif err := s.interceptableSwitch.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tcleanup = cleanup.add(s.invoiceHtlcModifier.Stop)\n\t\tif err := s.invoiceHtlcModifier.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tcleanup = cleanup.add(s.chainArb.Stop)\n\t\tif err := s.chainArb.Start(beat); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tcleanup = cleanup.add(s.graphBuilder.Stop)\n\t\tif err := s.graphBuilder.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tcleanup = cleanup.add(s.chanRouter.Stop)\n\t\tif err := s.chanRouter.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\t// The authGossiper depends on the chanRouter and therefore\n\t\t// should be started after it.\n\t\tcleanup = cleanup.add(s.authGossiper.Stop)\n\t\tif err := s.authGossiper.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tcleanup = cleanup.add(s.invoices.Stop)\n\t\tif err := s.invoices.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tcleanup = cleanup.add(s.sphinx.Stop)\n\t\tif err := s.sphinx.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tcleanup = cleanup.add(s.chanStatusMgr.Stop)\n\t\tif err := s.chanStatusMgr.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tcleanup = cleanup.add(s.chanEventStore.Stop)\n\t\tif err := s.chanEventStore.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tcleanup.add(func() error {\n\t\t\ts.missionController.StopStoreTickers()\n\t\t\treturn nil\n\t\t})\n\t\ts.missionController.RunStoreTickers()\n\n\t\t// Before we start the connMgr, we'll check to see if we have\n\t\t// any backups to recover. We do this now as we want to ensure\n\t\t// that have all the information we need to handle channel\n\t\t// recovery _before_ we even accept connections from any peers.\n\t\tchanRestorer := &chanDBRestorer{\n\t\t\tdb:         s.chanStateDB,\n\t\t\tsecretKeys: s.cc.KeyRing,\n\t\t\tchainArb:   s.chainArb,\n\t\t}\n\t\tif len(s.chansToRestore.PackedSingleChanBackups) != 0 {\n\t\t\t_, err := chanbackup.UnpackAndRecoverSingles(\n\t\t\t\ts.chansToRestore.PackedSingleChanBackups,\n\t\t\t\ts.cc.KeyRing, chanRestorer, s,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tstartErr = fmt.Errorf(\"unable to unpack single \"+\n\t\t\t\t\t\"backups: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tif len(s.chansToRestore.PackedMultiChanBackup) != 0 {\n\t\t\t_, err := chanbackup.UnpackAndRecoverMulti(\n\t\t\t\ts.chansToRestore.PackedMultiChanBackup,\n\t\t\t\ts.cc.KeyRing, chanRestorer, s,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tstartErr = fmt.Errorf(\"unable to unpack chan \"+\n\t\t\t\t\t\"backup: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\t// chanSubSwapper must be started after the `channelNotifier`\n\t\t// because it depends on channel events as a synchronization\n\t\t// point.\n\t\tcleanup = cleanup.add(s.chanSubSwapper.Stop)\n\t\tif err := s.chanSubSwapper.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\tif s.torController != nil {\n\t\t\tcleanup = cleanup.add(s.torController.Stop)\n\t\t\tif err := s.createNewHiddenService(); err != nil {\n\t\t\t\tstartErr = err\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\tif s.natTraversal != nil {\n\t\t\ts.wg.Add(1)\n\t\t\tgo s.watchExternalIP()\n\t\t}\n\n\t\t// Start connmgr last to prevent connections before init.\n\t\tcleanup = cleanup.add(func() error {\n\t\t\ts.connMgr.Stop()\n\t\t\treturn nil\n\t\t})\n\t\ts.connMgr.Start()\n\n\t\t// If peers are specified as a config option, we'll add those\n\t\t// peers first.\n\t\tfor _, peerAddrCfg := range s.cfg.AddPeers {\n\t\t\tparsedPubkey, parsedHost, err := lncfg.ParseLNAddressPubkey(\n\t\t\t\tpeerAddrCfg,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tstartErr = fmt.Errorf(\"unable to parse peer \"+\n\t\t\t\t\t\"pubkey from config: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\taddr, err := parseAddr(parsedHost, s.cfg.net)\n\t\t\tif err != nil {\n\t\t\t\tstartErr = fmt.Errorf(\"unable to parse peer \"+\n\t\t\t\t\t\"address provided as a config option: \"+\n\t\t\t\t\t\"%v\", err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tpeerAddr := &lnwire.NetAddress{\n\t\t\t\tIdentityKey: parsedPubkey,\n\t\t\t\tAddress:     addr,\n\t\t\t\tChainNet:    s.cfg.ActiveNetParams.Net,\n\t\t\t}\n\n\t\t\terr = s.ConnectToPeer(\n\t\t\t\tpeerAddr, true,\n\t\t\t\ts.cfg.ConnectionTimeout,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tstartErr = fmt.Errorf(\"unable to connect to \"+\n\t\t\t\t\t\"peer address provided as a config \"+\n\t\t\t\t\t\"option: %v\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\t// Subscribe to NodeAnnouncements that advertise new addresses\n\t\t// our persistent peers.\n\t\tif err := s.updatePersistentPeerAddrs(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\t// With all the relevant sub-systems started, we'll now attempt\n\t\t// to establish persistent connections to our direct channel\n\t\t// collaborators within the network. Before doing so however,\n\t\t// we'll prune our set of link nodes found within the database\n\t\t// to ensure we don't reconnect to any nodes we no longer have\n\t\t// open channels with.\n\t\tif err := s.chanStateDB.PruneLinkNodes(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\t\tif err := s.establishPersistentConnections(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\t// setSeedList is a helper function that turns multiple DNS seed\n\t\t// server tuples from the command line or config file into the\n\t\t// data structure we need and does a basic formal sanity check\n\t\t// in the process.\n\t\tsetSeedList := func(tuples []string, genesisHash chainhash.Hash) {\n\t\t\tif len(tuples) == 0 {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tresult := make([][2]string, len(tuples))\n\t\t\tfor idx, tuple := range tuples {\n\t\t\t\ttuple = strings.TrimSpace(tuple)\n\t\t\t\tif len(tuple) == 0 {\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tservers := strings.Split(tuple, \",\")\n\t\t\t\tif len(servers) > 2 || len(servers) == 0 {\n\t\t\t\t\tsrvrLog.Warnf(\"Ignoring invalid DNS \"+\n\t\t\t\t\t\t\"seed tuple: %v\", servers)\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tcopy(result[idx][:], servers)\n\t\t\t}\n\n\t\t\tchainreg.ChainDNSSeeds[genesisHash] = result\n\t\t}\n\n\t\t// Let users overwrite the DNS seed nodes. We only allow them\n\t\t// for bitcoin mainnet/testnet/signet.\n\t\tif s.cfg.Bitcoin.MainNet {\n\t\t\tsetSeedList(\n\t\t\t\ts.cfg.Bitcoin.DNSSeeds,\n\t\t\t\tchainreg.BitcoinMainnetGenesis,\n\t\t\t)\n\t\t}\n\t\tif s.cfg.Bitcoin.TestNet3 {\n\t\t\tsetSeedList(\n\t\t\t\ts.cfg.Bitcoin.DNSSeeds,\n\t\t\t\tchainreg.BitcoinTestnetGenesis,\n\t\t\t)\n\t\t}\n\t\tif s.cfg.Bitcoin.SigNet {\n\t\t\tsetSeedList(\n\t\t\t\ts.cfg.Bitcoin.DNSSeeds,\n\t\t\t\tchainreg.BitcoinSignetGenesis,\n\t\t\t)\n\t\t}\n\n\t\t// If network bootstrapping hasn't been disabled, then we'll\n\t\t// configure the set of active bootstrappers, and launch a\n\t\t// dedicated goroutine to maintain a set of persistent\n\t\t// connections.\n\t\tif shouldPeerBootstrap(s.cfg) {\n\t\t\tbootstrappers, err := initNetworkBootstrappers(s)\n\t\t\tif err != nil {\n\t\t\t\tstartErr = err\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\ts.wg.Add(1)\n\t\t\tgo s.peerBootstrapper(defaultMinPeers, bootstrappers)\n\t\t} else {\n\t\t\tsrvrLog.Infof(\"Auto peer bootstrapping is disabled\")\n\t\t}\n\n\t\t// Start the blockbeat after all other subsystems have been\n\t\t// started so they are ready to receive new blocks.\n\t\tcleanup = cleanup.add(func() error {\n\t\t\ts.blockbeatDispatcher.Stop()\n\t\t\treturn nil\n\t\t})\n\t\tif err := s.blockbeatDispatcher.Start(); err != nil {\n\t\t\tstartErr = err\n\t\t\treturn\n\t\t}\n\n\t\t// Set the active flag now that we've completed the full\n\t\t// startup.\n\t\tatomic.StoreInt32(&s.active, 1)\n\t})\n\n\tif startErr != nil {\n\t\tcleanup.run()\n\t}\n\treturn startErr\n}\n\n// Stop gracefully shutsdown the main daemon server. This function will signal\n// any active goroutines, or helper objects to exit, then blocks until they've\n// all successfully exited. Additionally, any/all listeners are closed.\n// NOTE: This function is safe for concurrent access.\nfunc (s *server) Stop() error {\n\ts.stop.Do(func() {\n\t\tatomic.StoreInt32(&s.stopping, 1)\n\n\t\tclose(s.quit)\n\n\t\t// Shutdown connMgr first to prevent conns during shutdown.\n\t\ts.connMgr.Stop()\n\n\t\t// Stop dispatching blocks to other systems immediately.\n\t\ts.blockbeatDispatcher.Stop()\n\n\t\t// Shutdown the wallet, funding manager, and the rpc server.\n\t\tif err := s.chanStatusMgr.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop chanStatusMgr: %v\", err)\n\t\t}\n\t\tif err := s.htlcSwitch.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop htlcSwitch: %v\", err)\n\t\t}\n\t\tif err := s.sphinx.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop sphinx: %v\", err)\n\t\t}\n\t\tif err := s.invoices.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop invoices: %v\", err)\n\t\t}\n\t\tif err := s.interceptableSwitch.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop interceptable \"+\n\t\t\t\t\"switch: %v\", err)\n\t\t}\n\t\tif err := s.invoiceHtlcModifier.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop htlc invoices \"+\n\t\t\t\t\"modifier: %v\", err)\n\t\t}\n\t\tif err := s.chanRouter.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop chanRouter: %v\", err)\n\t\t}\n\t\tif err := s.graphBuilder.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop graphBuilder %v\", err)\n\t\t}\n\t\tif err := s.chainArb.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop chainArb: %v\", err)\n\t\t}\n\t\tif err := s.fundingMgr.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop fundingMgr: %v\", err)\n\t\t}\n\t\tif err := s.breachArbitrator.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop breachArbitrator: %v\",\n\t\t\t\terr)\n\t\t}\n\t\tif err := s.utxoNursery.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop utxoNursery: %v\", err)\n\t\t}\n\t\tif err := s.authGossiper.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop authGossiper: %v\", err)\n\t\t}\n\t\tif err := s.sweeper.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop sweeper: %v\", err)\n\t\t}\n\t\tif err := s.txPublisher.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop txPublisher: %v\", err)\n\t\t}\n\t\tif err := s.channelNotifier.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop channelNotifier: %v\", err)\n\t\t}\n\t\tif err := s.peerNotifier.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop peerNotifier: %v\", err)\n\t\t}\n\t\tif err := s.htlcNotifier.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop htlcNotifier: %v\", err)\n\t\t}\n\n\t\t// Update channel.backup file. Make sure to do it before\n\t\t// stopping chanSubSwapper.\n\t\tsingles, err := chanbackup.FetchStaticChanBackups(\n\t\t\ts.chanStateDB, s.addrSource,\n\t\t)\n\t\tif err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to fetch channel states: %v\",\n\t\t\t\terr)\n\t\t} else {\n\t\t\terr := s.chanSubSwapper.ManualUpdate(singles)\n\t\t\tif err != nil {\n\t\t\t\tsrvrLog.Warnf(\"Manual update of channel \"+\n\t\t\t\t\t\"backup failed: %v\", err)\n\t\t\t}\n\t\t}\n\n\t\tif err := s.chanSubSwapper.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"failed to stop chanSubSwapper: %v\", err)\n\t\t}\n\t\tif err := s.cc.ChainNotifier.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"Unable to stop ChainNotifier: %v\", err)\n\t\t}\n\t\tif err := s.cc.BestBlockTracker.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"Unable to stop BestBlockTracker: %v\",\n\t\t\t\terr)\n\t\t}\n\t\tif err := s.chanEventStore.Stop(); err != nil {\n\t\t\tsrvrLog.Warnf(\"Unable to stop ChannelEventStore: %v\",\n\t\t\t\terr)\n\t\t}\n\t\ts.missionController.StopStoreTickers()\n\n\t\t// Disconnect from each active peers to ensure that\n\t\t// peerTerminationWatchers signal completion to each peer.\n\t\tfor _, peer := range s.Peers() {\n\t\t\terr := s.DisconnectPeer(peer.IdentityKey())\n\t\t\tif err != nil {\n\t\t\t\tsrvrLog.Warnf(\"could not disconnect peer: %v\"+\n\t\t\t\t\t\"received error: %v\", peer.IdentityKey(),\n\t\t\t\t\terr,\n\t\t\t\t)\n\t\t\t}\n\t\t}\n\n\t\t// Now that all connections have been torn down, stop the tower\n\t\t// client which will reliably flush all queued states to the\n\t\t// tower. If this is halted for any reason, the force quit timer\n\t\t// will kick in and abort to allow this method to return.\n\t\tif s.towerClientMgr != nil {\n\t\t\tif err := s.towerClientMgr.Stop(); err != nil {\n\t\t\t\tsrvrLog.Warnf(\"Unable to shut down tower \"+\n\t\t\t\t\t\"client manager: %v\", err)\n\t\t\t}\n\t\t}\n\n\t\tif s.hostAnn != nil {\n\t\t\tif err := s.hostAnn.Stop(); err != nil {\n\t\t\t\tsrvrLog.Warnf(\"unable to shut down host \"+\n\t\t\t\t\t\"annoucner: %v\", err)\n\t\t\t}\n\t\t}\n\n\t\tif s.livenessMonitor != nil {\n\t\t\tif err := s.livenessMonitor.Stop(); err != nil {\n\t\t\t\tsrvrLog.Warnf(\"unable to shutdown liveness \"+\n\t\t\t\t\t\"monitor: %v\", err)\n\t\t\t}\n\t\t}\n\n\t\t// Wait for all lingering goroutines to quit.\n\t\tsrvrLog.Debug(\"Waiting for server to shutdown...\")\n\t\ts.wg.Wait()\n\n\t\tsrvrLog.Debug(\"Stopping buffer pools...\")\n\t\ts.sigPool.Stop()\n\t\ts.writePool.Stop()\n\t\ts.readPool.Stop()\n\t})\n\n\treturn nil\n}\n\n// Stopped returns true if the server has been instructed to shutdown.\n// NOTE: This function is safe for concurrent access.\nfunc (s *server) Stopped() bool {\n\treturn atomic.LoadInt32(&s.stopping) != 0\n}\n\n// configurePortForwarding attempts to set up port forwarding for the different\n// ports that the server will be listening on.\n//\n// NOTE: This should only be used when using some kind of NAT traversal to\n// automatically set up forwarding rules.\nfunc (s *server) configurePortForwarding(ports ...uint16) ([]string, error) {\n\tip, err := s.natTraversal.ExternalIP()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ts.lastDetectedIP = ip\n\n\texternalIPs := make([]string, 0, len(ports))\n\tfor _, port := range ports {\n\t\tif err := s.natTraversal.AddPortMapping(port); err != nil {\n\t\t\tsrvrLog.Debugf(\"Unable to forward port %d: %v\", port, err)\n\t\t\tcontinue\n\t\t}\n\n\t\thostIP := fmt.Sprintf(\"%v:%d\", ip, port)\n\t\texternalIPs = append(externalIPs, hostIP)\n\t}\n\n\treturn externalIPs, nil\n}\n\n// removePortForwarding attempts to clear the forwarding rules for the different\n// ports the server is currently listening on.\n//\n// NOTE: This should only be used when using some kind of NAT traversal to\n// automatically set up forwarding rules.\nfunc (s *server) removePortForwarding() {\n\tforwardedPorts := s.natTraversal.ForwardedPorts()\n\tfor _, port := range forwardedPorts {\n\t\tif err := s.natTraversal.DeletePortMapping(port); err != nil {\n\t\t\tsrvrLog.Errorf(\"Unable to remove forwarding rules for \"+\n\t\t\t\t\"port %d: %v\", port, err)\n\t\t}\n\t}\n}\n\n// watchExternalIP continuously checks for an updated external IP address every\n// 15 minutes. Once a new IP address has been detected, it will automatically\n// handle port forwarding rules and send updated node announcements to the\n// currently connected peers.\n//\n// NOTE: This MUST be run as a goroutine.\nfunc (s *server) watchExternalIP() {\n\tdefer s.wg.Done()\n\n\t// Before exiting, we'll make sure to remove the forwarding rules set\n\t// up by the server.\n\tdefer s.removePortForwarding()\n\n\t// Keep track of the external IPs set by the user to avoid replacing\n\t// them when detecting a new IP.\n\tipsSetByUser := make(map[string]struct{})\n\tfor _, ip := range s.cfg.ExternalIPs {\n\t\tipsSetByUser[ip.String()] = struct{}{}\n\t}\n\n\tforwardedPorts := s.natTraversal.ForwardedPorts()\n\n\tticker := time.NewTicker(15 * time.Minute)\n\tdefer ticker.Stop()\nout:\n\tfor {\n\t\tselect {\n\t\tcase <-ticker.C:\n\t\t\t// We'll start off by making sure a new IP address has\n\t\t\t// been detected.\n\t\t\tip, err := s.natTraversal.ExternalIP()\n\t\t\tif err != nil {\n\t\t\t\tsrvrLog.Debugf(\"Unable to retrieve the \"+\n\t\t\t\t\t\"external IP address: %v\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Periodically renew the NAT port forwarding.\n\t\t\tfor _, port := range forwardedPorts {\n\t\t\t\terr := s.natTraversal.AddPortMapping(port)\n\t\t\t\tif err != nil {\n\t\t\t\t\tsrvrLog.Warnf(\"Unable to automatically \"+\n\t\t\t\t\t\t\"re-create port forwarding using %s: %v\",\n\t\t\t\t\t\ts.natTraversal.Name(), err)\n\t\t\t\t} else {\n\t\t\t\t\tsrvrLog.Debugf(\"Automatically re-created \"+\n\t\t\t\t\t\t\"forwarding for port %d using %s to \"+\n\t\t\t\t\t\t\"advertise external IP\",\n\t\t\t\t\t\tport, s.natTraversal.Name())\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif ip.Equal(s.lastDetectedIP) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tsrvrLog.Infof(\"Detected new external IP address %s\", ip)\n\n\t\t\t// Next, we'll craft the new addresses that will be\n\t\t\t// included in the new node announcement and advertised\n\t\t\t// to the network. Each address will consist of the new\n\t\t\t// IP detected and one of the currently advertised\n\t\t\t// ports.\n\t\t\tvar newAddrs []net.Addr\n\t\t\tfor _, port := range forwardedPorts {\n\t\t\t\thostIP := fmt.Sprintf(\"%v:%d\", ip, port)\n\t\t\t\taddr, err := net.ResolveTCPAddr(\"tcp\", hostIP)\n\t\t\t\tif err != nil {\n\t\t\t\t\tsrvrLog.Debugf(\"Unable to resolve \"+\n\t\t\t\t\t\t\"host %v: %v\", addr, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tnewAddrs = append(newAddrs, addr)\n\t\t\t}\n\n\t\t\t// Skip the update if we weren't able to resolve any of\n\t\t\t// the new addresses.\n\t\t\tif len(newAddrs) == 0 {\n\t\t\t\tsrvrLog.Debug(\"Skipping node announcement \" +\n\t\t\t\t\t\"update due to not being able to \" +\n\t\t\t\t\t\"resolve any new addresses\")\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Now, we'll need to update the addresses in our node's\n\t\t\t// announcement in order to propagate the update\n\t\t\t// throughout the network. We'll only include addresses\n\t\t\t// that have a different IP from the previous one, as\n\t\t\t// the previous IP is no longer valid.\n\t\t\tcurrentNodeAnn := s.getNodeAnnouncement()\n\n\t\t\tfor _, addr := range currentNodeAnn.Addresses {\n\t\t\t\thost, _, err := net.SplitHostPort(addr.String())\n\t\t\t\tif err != nil {\n\t\t\t\t\tsrvrLog.Debugf(\"Unable to determine \"+\n\t\t\t\t\t\t\"host from address %v: %v\",\n\t\t\t\t\t\taddr, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t// We'll also make sure to include external IPs\n\t\t\t\t// set manually by the user.\n\t\t\t\t_, setByUser := ipsSetByUser[addr.String()]\n\t\t\t\tif setByUser || host != s.lastDetectedIP.String() {\n\t\t\t\t\tnewAddrs = append(newAddrs, addr)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Then, we'll generate a new timestamped node\n\t\t\t// announcement with the updated addresses and broadcast\n\t\t\t// it to our peers.\n\t\t\tnewNodeAnn, err := s.genNodeAnnouncement(\n\t\t\t\tnil, netann.NodeAnnSetAddrs(newAddrs),\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tsrvrLog.Debugf(\"Unable to generate new node \"+\n\t\t\t\t\t\"announcement: %v\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\terr = s.BroadcastMessage(nil, &newNodeAnn)\n\t\t\tif err != nil {\n\t\t\t\tsrvrLog.Debugf(\"Unable to broadcast new node \"+\n\t\t\t\t\t\"announcement to peers: %v\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Finally, update the last IP seen to the current one.\n\t\t\ts.lastDetectedIP = ip\n\t\tcase <-s.quit:\n\t\t\tbreak out\n\t\t}\n\t}\n}\n\n// initNetworkBootstrappers initializes a set of network peer bootstrappers\n// based on the server, and currently active bootstrap mechanisms as defined\n// within the current configuration.\nfunc initNetworkBootstrappers(s *server) ([]discovery.NetworkPeerBootstrapper, error) {\n\tsrvrLog.Infof(\"Initializing peer network bootstrappers!\")\n\n\tvar bootStrappers []discovery.NetworkPeerBootstrapper\n\n\t// First, we'll create an instance of the ChannelGraphBootstrapper as\n\t// this can be used by default if we've already partially seeded the\n\t// network.\n\tchanGraph := autopilot.ChannelGraphFromDatabase(s.graphDB)\n\tgraphBootstrapper, err := discovery.NewGraphBootstrapper(chanGraph)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tbootStrappers = append(bootStrappers, graphBootstrapper)\n\n\t// If this isn't simnet mode, then one of our additional bootstrapping\n\t// sources will be the set of running DNS seeds.\n\tif !s.cfg.Bitcoin.SimNet {\n\t\tdnsSeeds, ok := chainreg.ChainDNSSeeds[*s.cfg.ActiveNetParams.GenesisHash]\n\n\t\t// If we have a set of DNS seeds for this chain, then we'll add\n\t\t// it as an additional bootstrapping source.\n\t\tif ok {\n\t\t\tsrvrLog.Infof(\"Creating DNS peer bootstrapper with \"+\n\t\t\t\t\"seeds: %v\", dnsSeeds)\n\n\t\t\tdnsBootStrapper := discovery.NewDNSSeedBootstrapper(\n\t\t\t\tdnsSeeds, s.cfg.net, s.cfg.ConnectionTimeout,\n\t\t\t)\n\t\t\tbootStrappers = append(bootStrappers, dnsBootStrapper)\n\t\t}\n\t}\n\n\treturn bootStrappers, nil\n}\n\n// createBootstrapIgnorePeers creates a map of peers that the bootstrap process\n// needs to ignore, which is made of three parts,\n//   - the node itself needs to be skipped as it doesn't make sense to connect\n//     to itself.\n//   - the peers that already have connections with, as in s.peersByPub.\n//   - the peers that we are attempting to connect, as in s.persistentPeers.\nfunc (s *server) createBootstrapIgnorePeers() map[autopilot.NodeID]struct{} {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\n\tignore := make(map[autopilot.NodeID]struct{})\n\n\t// We should ignore ourselves from bootstrapping.\n\tselfKey := autopilot.NewNodeID(s.identityECDH.PubKey())\n\tignore[selfKey] = struct{}{}\n\n\t// Ignore all connected peers.\n\tfor _, peer := range s.peersByPub {\n\t\tnID := autopilot.NewNodeID(peer.IdentityKey())\n\t\tignore[nID] = struct{}{}\n\t}\n\n\t// Ignore all persistent peers as they have a dedicated reconnecting\n\t// process.\n\tfor pubKeyStr := range s.persistentPeers {\n\t\tvar nID autopilot.NodeID\n\t\tcopy(nID[:], []byte(pubKeyStr))\n\t\tignore[nID] = struct{}{}\n\t}\n\n\treturn ignore\n}\n\n// peerBootstrapper is a goroutine which is tasked with attempting to establish\n// and maintain a target minimum number of outbound connections. With this\n// invariant, we ensure that our node is connected to a diverse set of peers\n// and that nodes newly joining the network receive an up to date network view\n// as soon as possible.\nfunc (s *server) peerBootstrapper(numTargetPeers uint32,\n\tbootstrappers []discovery.NetworkPeerBootstrapper) {\n\n\tdefer s.wg.Done()\n\n\t// Before we continue, init the ignore peers map.\n\tignoreList := s.createBootstrapIgnorePeers()\n\n\t// We'll start off by aggressively attempting connections to peers in\n\t// order to be a part of the network as soon as possible.\n\ts.initialPeerBootstrap(ignoreList, numTargetPeers, bootstrappers)\n\n\t// Once done, we'll attempt to maintain our target minimum number of\n\t// peers.\n\t//\n\t// We'll use a 15 second backoff, and double the time every time an\n\t// epoch fails up to a ceiling.\n\tbackOff := time.Second * 15\n\n\t// We'll create a new ticker to wake us up every 15 seconds so we can\n\t// see if we've reached our minimum number of peers.\n\tsampleTicker := time.NewTicker(backOff)\n\tdefer sampleTicker.Stop()\n\n\t// We'll use the number of attempts and errors to determine if we need\n\t// to increase the time between discovery epochs.\n\tvar epochErrors uint32 // To be used atomically.\n\tvar epochAttempts uint32\n\n\tfor {\n\t\tselect {\n\t\t// The ticker has just woken us up, so we'll need to check if\n\t\t// we need to attempt to connect our to any more peers.\n\t\tcase <-sampleTicker.C:\n\t\t\t// Obtain the current number of peers, so we can gauge\n\t\t\t// if we need to sample more peers or not.\n\t\t\ts.mu.RLock()\n\t\t\tnumActivePeers := uint32(len(s.peersByPub))\n\t\t\ts.mu.RUnlock()\n\n\t\t\t// If we have enough peers, then we can loop back\n\t\t\t// around to the next round as we're done here.\n\t\t\tif numActivePeers >= numTargetPeers {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// If all of our attempts failed during this last back\n\t\t\t// off period, then will increase our backoff to 5\n\t\t\t// minute ceiling to avoid an excessive number of\n\t\t\t// queries\n\t\t\t//\n\t\t\t// TODO(roasbeef): add reverse policy too?\n\n\t\t\tif epochAttempts > 0 &&\n\t\t\t\tatomic.LoadUint32(&epochErrors) >= epochAttempts {\n\n\t\t\t\tsampleTicker.Stop()\n\n\t\t\t\tbackOff *= 2\n\t\t\t\tif backOff > bootstrapBackOffCeiling {\n\t\t\t\t\tbackOff = bootstrapBackOffCeiling\n\t\t\t\t}\n\n\t\t\t\tsrvrLog.Debugf(\"Backing off peer bootstrapper to \"+\n\t\t\t\t\t\"%v\", backOff)\n\t\t\t\tsampleTicker = time.NewTicker(backOff)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tatomic.StoreUint32(&epochErrors, 0)\n\t\t\tepochAttempts = 0\n\n\t\t\t// Since we know need more peers, we'll compute the\n\t\t\t// exact number we need to reach our threshold.\n\t\t\tnumNeeded := numTargetPeers - numActivePeers\n\n\t\t\tsrvrLog.Debugf(\"Attempting to obtain %v more network \"+\n\t\t\t\t\"peers\", numNeeded)\n\n\t\t\t// With the number of peers we need calculated, we'll\n\t\t\t// query the network bootstrappers to sample a set of\n\t\t\t// random addrs for us.\n\t\t\t//\n\t\t\t// Before we continue, get a copy of the ignore peers\n\t\t\t// map.\n\t\t\tignoreList = s.createBootstrapIgnorePeers()\n\n\t\t\tpeerAddrs, err := discovery.MultiSourceBootstrap(\n\t\t\t\tignoreList, numNeeded*2, bootstrappers...,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tsrvrLog.Errorf(\"Unable to retrieve bootstrap \"+\n\t\t\t\t\t\"peers: %v\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Finally, we'll launch a new goroutine for each\n\t\t\t// prospective peer candidates.\n\t\t\tfor _, addr := range peerAddrs {\n\t\t\t\tepochAttempts++\n\n\t\t\t\tgo func(a *lnwire.NetAddress) {\n\t\t\t\t\t// TODO(roasbeef): can do AS, subnet,\n\t\t\t\t\t// country diversity, etc\n\t\t\t\t\terrChan := make(chan error, 1)\n\t\t\t\t\ts.connectToPeer(\n\t\t\t\t\t\ta, errChan,\n\t\t\t\t\t\ts.cfg.ConnectionTimeout,\n\t\t\t\t\t)\n\t\t\t\t\tselect {\n\t\t\t\t\tcase err := <-errChan:\n\t\t\t\t\t\tif err == nil {\n\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tsrvrLog.Errorf(\"Unable to \"+\n\t\t\t\t\t\t\t\"connect to %v: %v\",\n\t\t\t\t\t\t\ta, err)\n\t\t\t\t\t\tatomic.AddUint32(&epochErrors, 1)\n\t\t\t\t\tcase <-s.quit:\n\t\t\t\t\t}\n\t\t\t\t}(addr)\n\t\t\t}\n\t\tcase <-s.quit:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// bootstrapBackOffCeiling is the maximum amount of time we'll wait between\n// failed attempts to locate a set of bootstrap peers. We'll slowly double our\n// query back off each time we encounter a failure.\nconst bootstrapBackOffCeiling = time.Minute * 5\n\n// initialPeerBootstrap attempts to continuously connect to peers on startup\n// until the target number of peers has been reached. This ensures that nodes\n// receive an up to date network view as soon as possible.\nfunc (s *server) initialPeerBootstrap(ignore map[autopilot.NodeID]struct{},\n\tnumTargetPeers uint32,\n\tbootstrappers []discovery.NetworkPeerBootstrapper) {\n\n\tsrvrLog.Debugf(\"Init bootstrap with targetPeers=%v, bootstrappers=%v, \"+\n\t\t\"ignore=%v\", numTargetPeers, len(bootstrappers), len(ignore))\n\n\t// We'll start off by waiting 2 seconds between failed attempts, then\n\t// double each time we fail until we hit the bootstrapBackOffCeiling.\n\tvar delaySignal <-chan time.Time\n\tdelayTime := time.Second * 2\n\n\t// As want to be more aggressive, we'll use a lower back off celling\n\t// then the main peer bootstrap logic.\n\tbackOffCeiling := bootstrapBackOffCeiling / 5\n\n\tfor attempts := 0; ; attempts++ {\n\t\t// Check if the server has been requested to shut down in order\n\t\t// to prevent blocking.\n\t\tif s.Stopped() {\n\t\t\treturn\n\t\t}\n\n\t\t// We can exit our aggressive initial peer bootstrapping stage\n\t\t// if we've reached out target number of peers.\n\t\ts.mu.RLock()\n\t\tnumActivePeers := uint32(len(s.peersByPub))\n\t\ts.mu.RUnlock()\n\n\t\tif numActivePeers >= numTargetPeers {\n\t\t\treturn\n\t\t}\n\n\t\tif attempts > 0 {\n\t\t\tsrvrLog.Debugf(\"Waiting %v before trying to locate \"+\n\t\t\t\t\"bootstrap peers (attempt #%v)\", delayTime,\n\t\t\t\tattempts)\n\n\t\t\t// We've completed at least one iterating and haven't\n\t\t\t// finished, so we'll start to insert a delay period\n\t\t\t// between each attempt.\n\t\t\tdelaySignal = time.After(delayTime)\n\t\t\tselect {\n\t\t\tcase <-delaySignal:\n\t\t\tcase <-s.quit:\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// After our delay, we'll double the time we wait up to\n\t\t\t// the max back off period.\n\t\t\tdelayTime *= 2\n\t\t\tif delayTime > backOffCeiling {\n\t\t\t\tdelayTime = backOffCeiling\n\t\t\t}\n\t\t}\n\n\t\t// Otherwise, we'll request for the remaining number of peers\n\t\t// in order to reach our target.\n\t\tpeersNeeded := numTargetPeers - numActivePeers\n\t\tbootstrapAddrs, err := discovery.MultiSourceBootstrap(\n\t\t\tignore, peersNeeded, bootstrappers...,\n\t\t)\n\t\tif err != nil {\n\t\t\tsrvrLog.Errorf(\"Unable to retrieve initial bootstrap \"+\n\t\t\t\t\"peers: %v\", err)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Then, we'll attempt to establish a connection to the\n\t\t// different peer addresses retrieved by our bootstrappers.\n\t\tvar wg sync.WaitGroup\n\t\tfor _, bootstrapAddr := range bootstrapAddrs {\n\t\t\twg.Add(1)\n\t\t\tgo func(addr *lnwire.NetAddress) {\n\t\t\t\tdefer wg.Done()\n\n\t\t\t\terrChan := make(chan error, 1)\n\t\t\t\tgo s.connectToPeer(\n\t\t\t\t\taddr, errChan, s.cfg.ConnectionTimeout,\n\t\t\t\t)\n\n\t\t\t\t// We'll only allow this connection attempt to\n\t\t\t\t// take up to 3 seconds. This allows us to move\n\t\t\t\t// quickly by discarding peers that are slowing\n\t\t\t\t// us down.\n\t\t\t\tselect {\n\t\t\t\tcase err := <-errChan:\n\t\t\t\t\tif err == nil {\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tsrvrLog.Errorf(\"Unable to connect to \"+\n\t\t\t\t\t\t\"%v: %v\", addr, err)\n\t\t\t\t// TODO: tune timeout? 3 seconds might be *too*\n\t\t\t\t// aggressive but works well.\n\t\t\t\tcase <-time.After(3 * time.Second):\n\t\t\t\t\tsrvrLog.Tracef(\"Skipping peer %v due \"+\n\t\t\t\t\t\t\"to not establishing a \"+\n\t\t\t\t\t\t\"connection within 3 seconds\",\n\t\t\t\t\t\taddr)\n\t\t\t\tcase <-s.quit:\n\t\t\t\t}\n\t\t\t}(bootstrapAddr)\n\t\t}\n\n\t\twg.Wait()\n\t}\n}\n\n// createNewHiddenService automatically sets up a v2 or v3 onion service in\n// order to listen for inbound connections over Tor.\nfunc (s *server) createNewHiddenService() error {\n\t// Determine the different ports the server is listening on. The onion\n\t// service's virtual port will map to these ports and one will be picked\n\t// at random when the onion service is being accessed.\n\tlistenPorts := make([]int, 0, len(s.listenAddrs))\n\tfor _, listenAddr := range s.listenAddrs {\n\t\tport := listenAddr.(*net.TCPAddr).Port\n\t\tlistenPorts = append(listenPorts, port)\n\t}\n\n\tencrypter, err := lnencrypt.KeyRingEncrypter(s.cc.KeyRing)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Once the port mapping has been set, we can go ahead and automatically\n\t// create our onion service. The service's private key will be saved to\n\t// disk in order to regain access to this service when restarting `lnd`.\n\tonionCfg := tor.AddOnionConfig{\n\t\tVirtualPort: defaultPeerPort,\n\t\tTargetPorts: listenPorts,\n\t\tStore: tor.NewOnionFile(\n\t\t\ts.cfg.Tor.PrivateKeyPath, 0600, s.cfg.Tor.EncryptKey,\n\t\t\tencrypter,\n\t\t),\n\t}\n\n\tswitch {\n\tcase s.cfg.Tor.V2:\n\t\tonionCfg.Type = tor.V2\n\tcase s.cfg.Tor.V3:\n\t\tonionCfg.Type = tor.V3\n\t}\n\n\taddr, err := s.torController.AddOnion(onionCfg)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Now that the onion service has been created, we'll add the onion\n\t// address it can be reached at to our list of advertised addresses.\n\tnewNodeAnn, err := s.genNodeAnnouncement(\n\t\tnil, func(currentAnn *lnwire.NodeAnnouncement) {\n\t\t\tcurrentAnn.Addresses = append(currentAnn.Addresses, addr)\n\t\t},\n\t)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to generate new node \"+\n\t\t\t\"announcement: %v\", err)\n\t}\n\n\t// Finally, we'll update the on-disk version of our announcement so it\n\t// will eventually propagate to nodes in the network.\n\tselfNode := &models.LightningNode{\n\t\tHaveNodeAnnouncement: true,\n\t\tLastUpdate:           time.Unix(int64(newNodeAnn.Timestamp), 0),\n\t\tAddresses:            newNodeAnn.Addresses,\n\t\tAlias:                newNodeAnn.Alias.String(),\n\t\tFeatures: lnwire.NewFeatureVector(\n\t\t\tnewNodeAnn.Features, lnwire.Features,\n\t\t),\n\t\tColor:        newNodeAnn.RGBColor,\n\t\tAuthSigBytes: newNodeAnn.Signature.ToSignatureBytes(),\n\t}\n\tcopy(selfNode.PubKeyBytes[:], s.identityECDH.PubKey().SerializeCompressed())\n\tif err := s.graphDB.SetSourceNode(selfNode); err != nil {\n\t\treturn fmt.Errorf(\"can't set self node: %w\", err)\n\t}\n\n\treturn nil\n}\n\n// findChannel finds a channel given a public key and ChannelID. It is an\n// optimization that is quicker than seeking for a channel given only the\n// ChannelID.\nfunc (s *server) findChannel(node *btcec.PublicKey, chanID lnwire.ChannelID) (\n\t*channeldb.OpenChannel, error) {\n\n\tnodeChans, err := s.chanStateDB.FetchOpenChannels(node)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor _, channel := range nodeChans {\n\t\tif chanID.IsChanPoint(&channel.FundingOutpoint) {\n\t\t\treturn channel, nil\n\t\t}\n\t}\n\n\treturn nil, fmt.Errorf(\"unable to find channel\")\n}\n\n// getNodeAnnouncement fetches the current, fully signed node announcement.\nfunc (s *server) getNodeAnnouncement() lnwire.NodeAnnouncement {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\treturn *s.currentNodeAnn\n}\n\n// genNodeAnnouncement generates and returns the current fully signed node\n// announcement. The time stamp of the announcement will be updated in order\n// to ensure it propagates through the network.\nfunc (s *server) genNodeAnnouncement(features *lnwire.RawFeatureVector,\n\tmodifiers ...netann.NodeAnnModifier) (lnwire.NodeAnnouncement, error) {\n\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// First, try to update our feature manager with the updated set of\n\t// features.\n\tif features != nil {\n\t\tproposedFeatures := map[feature.Set]*lnwire.RawFeatureVector{\n\t\t\tfeature.SetNodeAnn: features,\n\t\t}\n\t\terr := s.featureMgr.UpdateFeatureSets(proposedFeatures)\n\t\tif err != nil {\n\t\t\treturn lnwire.NodeAnnouncement{}, err\n\t\t}\n\n\t\t// If we could successfully update our feature manager, add\n\t\t// an update modifier to include these new features to our\n\t\t// set.\n\t\tmodifiers = append(\n\t\t\tmodifiers, netann.NodeAnnSetFeatures(features),\n\t\t)\n\t}\n\n\t// Always update the timestamp when refreshing to ensure the update\n\t// propagates.\n\tmodifiers = append(modifiers, netann.NodeAnnSetTimestamp)\n\n\t// Apply the requested changes to the node announcement.\n\tfor _, modifier := range modifiers {\n\t\tmodifier(s.currentNodeAnn)\n\t}\n\n\t// Sign a new update after applying all of the passed modifiers.\n\terr := netann.SignNodeAnnouncement(\n\t\ts.nodeSigner, s.identityKeyLoc, s.currentNodeAnn,\n\t)\n\tif err != nil {\n\t\treturn lnwire.NodeAnnouncement{}, err\n\t}\n\n\treturn *s.currentNodeAnn, nil\n}\n\n// updateAndBroadcastSelfNode generates a new node announcement\n// applying the giving modifiers and updating the time stamp\n// to ensure it propagates through the network. Then it broadcasts\n// it to the network.\nfunc (s *server) updateAndBroadcastSelfNode(features *lnwire.RawFeatureVector,\n\tmodifiers ...netann.NodeAnnModifier) error {\n\n\tnewNodeAnn, err := s.genNodeAnnouncement(features, modifiers...)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to generate new node \"+\n\t\t\t\"announcement: %v\", err)\n\t}\n\n\t// Update the on-disk version of our announcement.\n\t// Load and modify self node istead of creating anew instance so we\n\t// don't risk overwriting any existing values.\n\tselfNode, err := s.graphDB.SourceNode()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to get current source node: %w\", err)\n\t}\n\n\tselfNode.HaveNodeAnnouncement = true\n\tselfNode.LastUpdate = time.Unix(int64(newNodeAnn.Timestamp), 0)\n\tselfNode.Addresses = newNodeAnn.Addresses\n\tselfNode.Alias = newNodeAnn.Alias.String()\n\tselfNode.Features = s.featureMgr.Get(feature.SetNodeAnn)\n\tselfNode.Color = newNodeAnn.RGBColor\n\tselfNode.AuthSigBytes = newNodeAnn.Signature.ToSignatureBytes()\n\n\tcopy(selfNode.PubKeyBytes[:], s.identityECDH.PubKey().SerializeCompressed())\n\n\tif err := s.graphDB.SetSourceNode(selfNode); err != nil {\n\t\treturn fmt.Errorf(\"can't set self node: %w\", err)\n\t}\n\n\t// Finally, propagate it to the nodes in the network.\n\terr = s.BroadcastMessage(nil, &newNodeAnn)\n\tif err != nil {\n\t\trpcsLog.Debugf(\"Unable to broadcast new node \"+\n\t\t\t\"announcement to peers: %v\", err)\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\ntype nodeAddresses struct {\n\tpubKey    *btcec.PublicKey\n\taddresses []net.Addr\n}\n\n// establishPersistentConnections attempts to establish persistent connections\n// to all our direct channel collaborators. In order to promote liveness of our\n// active channels, we instruct the connection manager to attempt to establish\n// and maintain persistent connections to all our direct channel counterparties.\nfunc (s *server) establishPersistentConnections() error {\n\t// nodeAddrsMap stores the combination of node public keys and addresses\n\t// that we'll attempt to reconnect to. PubKey strings are used as keys\n\t// since other PubKey forms can't be compared.\n\tnodeAddrsMap := map[string]*nodeAddresses{}\n\n\t// Iterate through the list of LinkNodes to find addresses we should\n\t// attempt to connect to based on our set of previous connections. Set\n\t// the reconnection port to the default peer port.\n\tlinkNodes, err := s.chanStateDB.LinkNodeDB().FetchAllLinkNodes()\n\tif err != nil && err != channeldb.ErrLinkNodesNotFound {\n\t\treturn err\n\t}\n\tfor _, node := range linkNodes {\n\t\tpubStr := string(node.IdentityPub.SerializeCompressed())\n\t\tnodeAddrs := &nodeAddresses{\n\t\t\tpubKey:    node.IdentityPub,\n\t\t\taddresses: node.Addresses,\n\t\t}\n\t\tnodeAddrsMap[pubStr] = nodeAddrs\n\t}\n\n\t// After checking our previous connections for addresses to connect to,\n\t// iterate through the nodes in our channel graph to find addresses\n\t// that have been added via NodeAnnouncement messages.\n\tsourceNode, err := s.graphDB.SourceNode()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// TODO(roasbeef): instead iterate over link nodes and query graph for\n\t// each of the nodes.\n\tselfPub := s.identityECDH.PubKey().SerializeCompressed()\n\terr = s.graphDB.ForEachNodeChannel(sourceNode.PubKeyBytes, func(\n\t\ttx kvdb.RTx,\n\t\tchanInfo *models.ChannelEdgeInfo,\n\t\tpolicy, _ *models.ChannelEdgePolicy) error {\n\n\t\t// If the remote party has announced the channel to us, but we\n\t\t// haven't yet, then we won't have a policy. However, we don't\n\t\t// need this to connect to the peer, so we'll log it and move on.\n\t\tif policy == nil {\n\t\t\tsrvrLog.Warnf(\"No channel policy found for \"+\n\t\t\t\t\"ChannelPoint(%v): \", chanInfo.ChannelPoint)\n\t\t}\n\n\t\t// We'll now fetch the peer opposite from us within this\n\t\t// channel so we can queue up a direct connection to them.\n\t\tchannelPeer, err := s.graphDB.FetchOtherNode(\n\t\t\ttx, chanInfo, selfPub,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to fetch channel peer for \"+\n\t\t\t\t\"ChannelPoint(%v): %v\", chanInfo.ChannelPoint,\n\t\t\t\terr)\n\t\t}\n\n\t\tpubStr := string(channelPeer.PubKeyBytes[:])\n\n\t\t// Add all unique addresses from channel\n\t\t// graph/NodeAnnouncements to the list of addresses we'll\n\t\t// connect to for this peer.\n\t\taddrSet := make(map[string]net.Addr)\n\t\tfor _, addr := range channelPeer.Addresses {\n\t\t\tswitch addr.(type) {\n\t\t\tcase *net.TCPAddr:\n\t\t\t\taddrSet[addr.String()] = addr\n\n\t\t\t// We'll only attempt to connect to Tor addresses if Tor\n\t\t\t// outbound support is enabled.\n\t\t\tcase *tor.OnionAddr:\n\t\t\t\tif s.cfg.Tor.Active {\n\t\t\t\t\taddrSet[addr.String()] = addr\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// If this peer is also recorded as a link node, we'll add any\n\t\t// additional addresses that have not already been selected.\n\t\tlinkNodeAddrs, ok := nodeAddrsMap[pubStr]\n\t\tif ok {\n\t\t\tfor _, lnAddress := range linkNodeAddrs.addresses {\n\t\t\t\tswitch lnAddress.(type) {\n\t\t\t\tcase *net.TCPAddr:\n\t\t\t\t\taddrSet[lnAddress.String()] = lnAddress\n\n\t\t\t\t// We'll only attempt to connect to Tor\n\t\t\t\t// addresses if Tor outbound support is enabled.\n\t\t\t\tcase *tor.OnionAddr:\n\t\t\t\t\tif s.cfg.Tor.Active {\n\t\t\t\t\t\taddrSet[lnAddress.String()] = lnAddress\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Construct a slice of the deduped addresses.\n\t\tvar addrs []net.Addr\n\t\tfor _, addr := range addrSet {\n\t\t\taddrs = append(addrs, addr)\n\t\t}\n\n\t\tn := &nodeAddresses{\n\t\t\taddresses: addrs,\n\t\t}\n\t\tn.pubKey, err = channelPeer.PubKey()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tnodeAddrsMap[pubStr] = n\n\t\treturn nil\n\t})\n\tif err != nil && !errors.Is(err, graphdb.ErrGraphNoEdgesFound) {\n\t\treturn err\n\t}\n\n\tsrvrLog.Debugf(\"Establishing %v persistent connections on start\",\n\t\tlen(nodeAddrsMap))\n\n\t// Acquire and hold server lock until all persistent connection requests\n\t// have been recorded and sent to the connection manager.\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// Iterate through the combined list of addresses from prior links and\n\t// node announcements and attempt to reconnect to each node.\n\tvar numOutboundConns int\n\tfor pubStr, nodeAddr := range nodeAddrsMap {\n\t\t// Add this peer to the set of peers we should maintain a\n\t\t// persistent connection with. We set the value to false to\n\t\t// indicate that we should not continue to reconnect if the\n\t\t// number of channels returns to zero, since this peer has not\n\t\t// been requested as perm by the user.\n\t\ts.persistentPeers[pubStr] = false\n\t\tif _, ok := s.persistentPeersBackoff[pubStr]; !ok {\n\t\t\ts.persistentPeersBackoff[pubStr] = s.cfg.MinBackoff\n\t\t}\n\n\t\tfor _, address := range nodeAddr.addresses {\n\t\t\t// Create a wrapper address which couples the IP and\n\t\t\t// the pubkey so the brontide authenticated connection\n\t\t\t// can be established.\n\t\t\tlnAddr := &lnwire.NetAddress{\n\t\t\t\tIdentityKey: nodeAddr.pubKey,\n\t\t\t\tAddress:     address,\n\t\t\t}\n\n\t\t\ts.persistentPeerAddrs[pubStr] = append(\n\t\t\t\ts.persistentPeerAddrs[pubStr], lnAddr)\n\t\t}\n\n\t\t// We'll connect to the first 10 peers immediately, then\n\t\t// randomly stagger any remaining connections if the\n\t\t// stagger initial reconnect flag is set. This ensures\n\t\t// that mobile nodes or nodes with a small number of\n\t\t// channels obtain connectivity quickly, but larger\n\t\t// nodes are able to disperse the costs of connecting to\n\t\t// all peers at once.\n\t\tif numOutboundConns < numInstantInitReconnect ||\n\t\t\t!s.cfg.StaggerInitialReconnect {\n\n\t\t\tgo s.connectToPersistentPeer(pubStr)\n\t\t} else {\n\t\t\tgo s.delayInitialReconnect(pubStr)\n\t\t}\n\n\t\tnumOutboundConns++\n\t}\n\n\treturn nil\n}\n\n// delayInitialReconnect will attempt a reconnection to the given peer after\n// sampling a value for the delay between 0s and the maxInitReconnectDelay.\n//\n// NOTE: This method MUST be run as a goroutine.\nfunc (s *server) delayInitialReconnect(pubStr string) {\n\tdelay := time.Duration(prand.Intn(maxInitReconnectDelay)) * time.Second\n\tselect {\n\tcase <-time.After(delay):\n\t\ts.connectToPersistentPeer(pubStr)\n\tcase <-s.quit:\n\t}\n}\n\n// prunePersistentPeerConnection removes all internal state related to\n// persistent connections to a peer within the server. This is used to avoid\n// persistent connection retries to peers we do not have any open channels with.\nfunc (s *server) prunePersistentPeerConnection(compressedPubKey [33]byte) {\n\tpubKeyStr := string(compressedPubKey[:])\n\n\ts.mu.Lock()\n\tif perm, ok := s.persistentPeers[pubKeyStr]; ok && !perm {\n\t\tdelete(s.persistentPeers, pubKeyStr)\n\t\tdelete(s.persistentPeersBackoff, pubKeyStr)\n\t\tdelete(s.persistentPeerAddrs, pubKeyStr)\n\t\ts.cancelConnReqs(pubKeyStr, nil)\n\t\ts.mu.Unlock()\n\n\t\tsrvrLog.Infof(\"Pruned peer %x from persistent connections, \"+\n\t\t\t\"peer has no open channels\", compressedPubKey)\n\n\t\treturn\n\t}\n\ts.mu.Unlock()\n}\n\n// BroadcastMessage sends a request to the server to broadcast a set of\n// messages to all peers other than the one specified by the `skips` parameter.\n// All messages sent via BroadcastMessage will be queued for lazy delivery to\n// the target peers.\n//\n// NOTE: This function is safe for concurrent access.\nfunc (s *server) BroadcastMessage(skips map[route.Vertex]struct{},\n\tmsgs ...lnwire.Message) error {\n\n\t// Filter out peers found in the skips map. We synchronize access to\n\t// peersByPub throughout this process to ensure we deliver messages to\n\t// exact set of peers present at the time of invocation.\n\ts.mu.RLock()\n\tpeers := make([]*peer.Brontide, 0, len(s.peersByPub))\n\tfor pubStr, sPeer := range s.peersByPub {\n\t\tif skips != nil {\n\t\t\tif _, ok := skips[sPeer.PubKey()]; ok {\n\t\t\t\tsrvrLog.Tracef(\"Skipping %x in broadcast with \"+\n\t\t\t\t\t\"pubStr=%x\", sPeer.PubKey(), pubStr)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tpeers = append(peers, sPeer)\n\t}\n\ts.mu.RUnlock()\n\n\t// Iterate over all known peers, dispatching a go routine to enqueue\n\t// all messages to each of peers.\n\tvar wg sync.WaitGroup\n\tfor _, sPeer := range peers {\n\t\tsrvrLog.Debugf(\"Sending %v messages to peer %x\", len(msgs),\n\t\t\tsPeer.PubKey())\n\n\t\t// Dispatch a go routine to enqueue all messages to this peer.\n\t\twg.Add(1)\n\t\ts.wg.Add(1)\n\t\tgo func(p lnpeer.Peer) {\n\t\t\tdefer s.wg.Done()\n\t\t\tdefer wg.Done()\n\n\t\t\tp.SendMessageLazy(false, msgs...)\n\t\t}(sPeer)\n\t}\n\n\t// Wait for all messages to have been dispatched before returning to\n\t// caller.\n\twg.Wait()\n\n\treturn nil\n}\n\n// NotifyWhenOnline can be called by other subsystems to get notified when a\n// particular peer comes online. The peer itself is sent across the peerChan.\n//\n// NOTE: This function is safe for concurrent access.\nfunc (s *server) NotifyWhenOnline(peerKey [33]byte,\n\tpeerChan chan<- lnpeer.Peer) {\n\n\ts.mu.Lock()\n\n\t// Compute the target peer's identifier.\n\tpubStr := string(peerKey[:])\n\n\t// Check if peer is connected.\n\tpeer, ok := s.peersByPub[pubStr]\n\tif ok {\n\t\t// Unlock here so that the mutex isn't held while we are\n\t\t// waiting for the peer to become active.\n\t\ts.mu.Unlock()\n\n\t\t// Wait until the peer signals that it is actually active\n\t\t// rather than only in the server's maps.\n\t\tselect {\n\t\tcase <-peer.ActiveSignal():\n\t\tcase <-peer.QuitSignal():\n\t\t\t// The peer quit, so we'll add the channel to the slice\n\t\t\t// and return.\n\t\t\ts.mu.Lock()\n\t\t\ts.peerConnectedListeners[pubStr] = append(\n\t\t\t\ts.peerConnectedListeners[pubStr], peerChan,\n\t\t\t)\n\t\t\ts.mu.Unlock()\n\t\t\treturn\n\t\t}\n\n\t\t// Connected, can return early.\n\t\tsrvrLog.Debugf(\"Notifying that peer %x is online\", peerKey)\n\n\t\tselect {\n\t\tcase peerChan <- peer:\n\t\tcase <-s.quit:\n\t\t}\n\n\t\treturn\n\t}\n\n\t// Not connected, store this listener such that it can be notified when\n\t// the peer comes online.\n\ts.peerConnectedListeners[pubStr] = append(\n\t\ts.peerConnectedListeners[pubStr], peerChan,\n\t)\n\ts.mu.Unlock()\n}\n\n// NotifyWhenOffline delivers a notification to the caller of when the peer with\n// the given public key has been disconnected. The notification is signaled by\n// closing the channel returned.\nfunc (s *server) NotifyWhenOffline(peerPubKey [33]byte) <-chan struct{} {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tc := make(chan struct{})\n\n\t// If the peer is already offline, we can immediately trigger the\n\t// notification.\n\tpeerPubKeyStr := string(peerPubKey[:])\n\tif _, ok := s.peersByPub[peerPubKeyStr]; !ok {\n\t\tsrvrLog.Debugf(\"Notifying that peer %x is offline\", peerPubKey)\n\t\tclose(c)\n\t\treturn c\n\t}\n\n\t// Otherwise, the peer is online, so we'll keep track of the channel to\n\t// trigger the notification once the server detects the peer\n\t// disconnects.\n\ts.peerDisconnectedListeners[peerPubKeyStr] = append(\n\t\ts.peerDisconnectedListeners[peerPubKeyStr], c,\n\t)\n\n\treturn c\n}\n\n// FindPeer will return the peer that corresponds to the passed in public key.\n// This function is used by the funding manager, allowing it to update the\n// daemon's local representation of the remote peer.\n//\n// NOTE: This function is safe for concurrent access.\nfunc (s *server) FindPeer(peerKey *btcec.PublicKey) (*peer.Brontide, error) {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\n\tpubStr := string(peerKey.SerializeCompressed())\n\n\treturn s.findPeerByPubStr(pubStr)\n}\n\n// FindPeerByPubStr will return the peer that corresponds to the passed peerID,\n// which should be a string representation of the peer's serialized, compressed\n// public key.\n//\n// NOTE: This function is safe for concurrent access.\nfunc (s *server) FindPeerByPubStr(pubStr string) (*peer.Brontide, error) {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\n\treturn s.findPeerByPubStr(pubStr)\n}\n\n// findPeerByPubStr is an internal method that retrieves the specified peer from\n// the server's internal state using.\nfunc (s *server) findPeerByPubStr(pubStr string) (*peer.Brontide, error) {\n\tpeer, ok := s.peersByPub[pubStr]\n\tif !ok {\n\t\treturn nil, ErrPeerNotConnected\n\t}\n\n\treturn peer, nil\n}\n\n// nextPeerBackoff computes the next backoff duration for a peer's pubkey using\n// exponential backoff. If no previous backoff was known, the default is\n// returned.\nfunc (s *server) nextPeerBackoff(pubStr string,\n\tstartTime time.Time) time.Duration {\n\n\t// Now, determine the appropriate backoff to use for the retry.\n\tbackoff, ok := s.persistentPeersBackoff[pubStr]\n\tif !ok {\n\t\t// If an existing backoff was unknown, use the default.\n\t\treturn s.cfg.MinBackoff\n\t}\n\n\t// If the peer failed to start properly, we'll just use the previous\n\t// backoff to compute the subsequent randomized exponential backoff\n\t// duration. This will roughly double on average.\n\tif startTime.IsZero() {\n\t\treturn computeNextBackoff(backoff, s.cfg.MaxBackoff)\n\t}\n\n\t// The peer succeeded in starting. If the connection didn't last long\n\t// enough to be considered stable, we'll continue to back off retries\n\t// with this peer.\n\tconnDuration := time.Since(startTime)\n\tif connDuration < defaultStableConnDuration {\n\t\treturn computeNextBackoff(backoff, s.cfg.MaxBackoff)\n\t}\n\n\t// The peer succeed in starting and this was stable peer, so we'll\n\t// reduce the timeout duration by the length of the connection after\n\t// applying randomized exponential backoff. We'll only apply this in the\n\t// case that:\n\t//   reb(curBackoff) - connDuration > cfg.MinBackoff\n\trelaxedBackoff := computeNextBackoff(backoff, s.cfg.MaxBackoff) - connDuration\n\tif relaxedBackoff > s.cfg.MinBackoff {\n\t\treturn relaxedBackoff\n\t}\n\n\t// Lastly, if reb(currBackoff) - connDuration <= cfg.MinBackoff, meaning\n\t// the stable connection lasted much longer than our previous backoff.\n\t// To reward such good behavior, we'll reconnect after the default\n\t// timeout.\n\treturn s.cfg.MinBackoff\n}\n\n// shouldDropLocalConnection determines if our local connection to a remote peer\n// should be dropped in the case of concurrent connection establishment. In\n// order to deterministically decide which connection should be dropped, we'll\n// utilize the ordering of the local and remote public key. If we didn't use\n// such a tie breaker, then we risk _both_ connections erroneously being\n// dropped.\nfunc shouldDropLocalConnection(local, remote *btcec.PublicKey) bool {\n\tlocalPubBytes := local.SerializeCompressed()\n\tremotePubPbytes := remote.SerializeCompressed()\n\n\t// The connection that comes from the node with a \"smaller\" pubkey\n\t// should be kept. Therefore, if our pubkey is \"greater\" than theirs, we\n\t// should drop our established connection.\n\treturn bytes.Compare(localPubBytes, remotePubPbytes) > 0\n}\n\n// InboundPeerConnected initializes a new peer in response to a new inbound\n// connection.\n//\n// NOTE: This function is safe for concurrent access.\nfunc (s *server) InboundPeerConnected(conn net.Conn) {\n\t// Exit early if we have already been instructed to shutdown, this\n\t// prevents any delayed callbacks from accidentally registering peers.\n\tif s.Stopped() {\n\t\treturn\n\t}\n\n\tnodePub := conn.(*brontide.Conn).RemotePub()\n\tpubSer := nodePub.SerializeCompressed()\n\tpubStr := string(pubSer)\n\n\tvar pubBytes [33]byte\n\tcopy(pubBytes[:], pubSer)\n\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// If the remote node's public key is banned, drop the connection.\n\tshouldDc, dcErr := s.authGossiper.ShouldDisconnect(nodePub)\n\tif dcErr != nil {\n\t\tsrvrLog.Errorf(\"Unable to check if we should disconnect \"+\n\t\t\t\"peer: %v\", dcErr)\n\t\tconn.Close()\n\n\t\treturn\n\t}\n\n\tif shouldDc {\n\t\tsrvrLog.Debugf(\"Dropping connection for %v since they are \"+\n\t\t\t\"banned.\", pubSer)\n\n\t\tconn.Close()\n\n\t\treturn\n\t}\n\n\t// If we already have an outbound connection to this peer, then ignore\n\t// this new connection.\n\tif p, ok := s.outboundPeers[pubStr]; ok {\n\t\tsrvrLog.Debugf(\"Already have outbound connection for %v, \"+\n\t\t\t\"ignoring inbound connection from local=%v, remote=%v\",\n\t\t\tp, conn.LocalAddr(), conn.RemoteAddr())\n\n\t\tconn.Close()\n\t\treturn\n\t}\n\n\t// If we already have a valid connection that is scheduled to take\n\t// precedence once the prior peer has finished disconnecting, we'll\n\t// ignore this connection.\n\tif p, ok := s.scheduledPeerConnection[pubStr]; ok {\n\t\tsrvrLog.Debugf(\"Ignoring connection from %v, peer %v already \"+\n\t\t\t\"scheduled\", conn.RemoteAddr(), p)\n\t\tconn.Close()\n\t\treturn\n\t}\n\n\tsrvrLog.Infof(\"New inbound connection from %v\", conn.RemoteAddr())\n\n\t// Check to see if we already have a connection with this peer. If so,\n\t// we may need to drop our existing connection. This prevents us from\n\t// having duplicate connections to the same peer. We forgo adding a\n\t// default case as we expect these to be the only error values returned\n\t// from findPeerByPubStr.\n\tconnectedPeer, err := s.findPeerByPubStr(pubStr)\n\tswitch err {\n\tcase ErrPeerNotConnected:\n\t\t// We were unable to locate an existing connection with the\n\t\t// target peer, proceed to connect.\n\t\ts.cancelConnReqs(pubStr, nil)\n\t\ts.peerConnected(conn, nil, true)\n\n\tcase nil:\n\t\t// We already have a connection with the incoming peer. If the\n\t\t// connection we've already established should be kept and is\n\t\t// not of the same type of the new connection (inbound), then\n\t\t// we'll close out the new connection s.t there's only a single\n\t\t// connection between us.\n\t\tlocalPub := s.identityECDH.PubKey()\n\t\tif !connectedPeer.Inbound() &&\n\t\t\t!shouldDropLocalConnection(localPub, nodePub) {\n\n\t\t\tsrvrLog.Warnf(\"Received inbound connection from \"+\n\t\t\t\t\"peer %v, but already have outbound \"+\n\t\t\t\t\"connection, dropping conn\", connectedPeer)\n\t\t\tconn.Close()\n\t\t\treturn\n\t\t}\n\n\t\t// Otherwise, if we should drop the connection, then we'll\n\t\t// disconnect our already connected peer.\n\t\tsrvrLog.Debugf(\"Disconnecting stale connection to %v\",\n\t\t\tconnectedPeer)\n\n\t\ts.cancelConnReqs(pubStr, nil)\n\n\t\t// Remove the current peer from the server's internal state and\n\t\t// signal that the peer termination watcher does not need to\n\t\t// execute for this peer.\n\t\ts.removePeer(connectedPeer)\n\t\ts.ignorePeerTermination[connectedPeer] = struct{}{}\n\t\ts.scheduledPeerConnection[pubStr] = func() {\n\t\t\ts.peerConnected(conn, nil, true)\n\t\t}\n\t}\n}\n\n// OutboundPeerConnected initializes a new peer in response to a new outbound\n// connection.\n// NOTE: This function is safe for concurrent access.\nfunc (s *server) OutboundPeerConnected(connReq *connmgr.ConnReq, conn net.Conn) {\n\t// Exit early if we have already been instructed to shutdown, this\n\t// prevents any delayed callbacks from accidentally registering peers.\n\tif s.Stopped() {\n\t\treturn\n\t}\n\n\tnodePub := conn.(*brontide.Conn).RemotePub()\n\tpubSer := nodePub.SerializeCompressed()\n\tpubStr := string(pubSer)\n\n\tvar pubBytes [33]byte\n\tcopy(pubBytes[:], pubSer)\n\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// If the remote node's public key is banned, drop the connection.\n\tshouldDc, dcErr := s.authGossiper.ShouldDisconnect(nodePub)\n\tif dcErr != nil {\n\t\tsrvrLog.Errorf(\"Unable to check if we should disconnect \"+\n\t\t\t\"peer: %v\", dcErr)\n\t\tconn.Close()\n\n\t\treturn\n\t}\n\n\tif shouldDc {\n\t\tsrvrLog.Debugf(\"Dropping connection for %v since they are \"+\n\t\t\t\"banned.\", pubSer)\n\n\t\tif connReq != nil {\n\t\t\ts.connMgr.Remove(connReq.ID())\n\t\t}\n\n\t\tconn.Close()\n\n\t\treturn\n\t}\n\n\t// If we already have an inbound connection to this peer, then ignore\n\t// this new connection.\n\tif p, ok := s.inboundPeers[pubStr]; ok {\n\t\tsrvrLog.Debugf(\"Already have inbound connection for %v, \"+\n\t\t\t\"ignoring outbound connection from local=%v, remote=%v\",\n\t\t\tp, conn.LocalAddr(), conn.RemoteAddr())\n\n\t\tif connReq != nil {\n\t\t\ts.connMgr.Remove(connReq.ID())\n\t\t}\n\t\tconn.Close()\n\t\treturn\n\t}\n\tif _, ok := s.persistentConnReqs[pubStr]; !ok && connReq != nil {\n\t\tsrvrLog.Debugf(\"Ignoring canceled outbound connection\")\n\t\ts.connMgr.Remove(connReq.ID())\n\t\tconn.Close()\n\t\treturn\n\t}\n\n\t// If we already have a valid connection that is scheduled to take\n\t// precedence once the prior peer has finished disconnecting, we'll\n\t// ignore this connection.\n\tif _, ok := s.scheduledPeerConnection[pubStr]; ok {\n\t\tsrvrLog.Debugf(\"Ignoring connection, peer already scheduled\")\n\n\t\tif connReq != nil {\n\t\t\ts.connMgr.Remove(connReq.ID())\n\t\t}\n\n\t\tconn.Close()\n\t\treturn\n\t}\n\n\tsrvrLog.Infof(\"Established connection to: %x@%v\", pubStr,\n\t\tconn.RemoteAddr())\n\n\tif connReq != nil {\n\t\t// A successful connection was returned by the connmgr.\n\t\t// Immediately cancel all pending requests, excluding the\n\t\t// outbound connection we just established.\n\t\tignore := connReq.ID()\n\t\ts.cancelConnReqs(pubStr, &ignore)\n\t} else {\n\t\t// This was a successful connection made by some other\n\t\t// subsystem. Remove all requests being managed by the connmgr.\n\t\ts.cancelConnReqs(pubStr, nil)\n\t}\n\n\t// If we already have a connection with this peer, decide whether or not\n\t// we need to drop the stale connection. We forgo adding a default case\n\t// as we expect these to be the only error values returned from\n\t// findPeerByPubStr.\n\tconnectedPeer, err := s.findPeerByPubStr(pubStr)\n\tswitch err {\n\tcase ErrPeerNotConnected:\n\t\t// We were unable to locate an existing connection with the\n\t\t// target peer, proceed to connect.\n\t\ts.peerConnected(conn, connReq, false)\n\n\tcase nil:\n\t\t// We already have a connection with the incoming peer. If the\n\t\t// connection we've already established should be kept and is\n\t\t// not of the same type of the new connection (outbound), then\n\t\t// we'll close out the new connection s.t there's only a single\n\t\t// connection between us.\n\t\tlocalPub := s.identityECDH.PubKey()\n\t\tif connectedPeer.Inbound() &&\n\t\t\tshouldDropLocalConnection(localPub, nodePub) {\n\n\t\t\tsrvrLog.Warnf(\"Established outbound connection to \"+\n\t\t\t\t\"peer %v, but already have inbound \"+\n\t\t\t\t\"connection, dropping conn\", connectedPeer)\n\t\t\tif connReq != nil {\n\t\t\t\ts.connMgr.Remove(connReq.ID())\n\t\t\t}\n\t\t\tconn.Close()\n\t\t\treturn\n\t\t}\n\n\t\t// Otherwise, _their_ connection should be dropped. So we'll\n\t\t// disconnect the peer and send the now obsolete peer to the\n\t\t// server for garbage collection.\n\t\tsrvrLog.Debugf(\"Disconnecting stale connection to %v\",\n\t\t\tconnectedPeer)\n\n\t\t// Remove the current peer from the server's internal state and\n\t\t// signal that the peer termination watcher does not need to\n\t\t// execute for this peer.\n\t\ts.removePeer(connectedPeer)\n\t\ts.ignorePeerTermination[connectedPeer] = struct{}{}\n\t\ts.scheduledPeerConnection[pubStr] = func() {\n\t\t\ts.peerConnected(conn, connReq, false)\n\t\t}\n\t}\n}\n\n// UnassignedConnID is the default connection ID that a request can have before\n// it actually is submitted to the connmgr.\n// TODO(conner): move into connmgr package, or better, add connmgr method for\n// generating atomic IDs\nconst UnassignedConnID uint64 = 0\n\n// cancelConnReqs stops all persistent connection requests for a given pubkey.\n// Any attempts initiated by the peerTerminationWatcher are canceled first.\n// Afterwards, each connection request removed from the connmgr. The caller can\n// optionally specify a connection ID to ignore, which prevents us from\n// canceling a successful request. All persistent connreqs for the provided\n// pubkey are discarded after the operationjw.\nfunc (s *server) cancelConnReqs(pubStr string, skip *uint64) {\n\t// First, cancel any lingering persistent retry attempts, which will\n\t// prevent retries for any with backoffs that are still maturing.\n\tif cancelChan, ok := s.persistentRetryCancels[pubStr]; ok {\n\t\tclose(cancelChan)\n\t\tdelete(s.persistentRetryCancels, pubStr)\n\t}\n\n\t// Next, check to see if we have any outstanding persistent connection\n\t// requests to this peer. If so, then we'll remove all of these\n\t// connection requests, and also delete the entry from the map.\n\tconnReqs, ok := s.persistentConnReqs[pubStr]\n\tif !ok {\n\t\treturn\n\t}\n\n\tfor _, connReq := range connReqs {\n\t\tsrvrLog.Tracef(\"Canceling %s:\", connReqs)\n\n\t\t// Atomically capture the current request identifier.\n\t\tconnID := connReq.ID()\n\n\t\t// Skip any zero IDs, this indicates the request has not\n\t\t// yet been schedule.\n\t\tif connID == UnassignedConnID {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Skip a particular connection ID if instructed.\n\t\tif skip != nil && connID == *skip {\n\t\t\tcontinue\n\t\t}\n\n\t\ts.connMgr.Remove(connID)\n\t}\n\n\tdelete(s.persistentConnReqs, pubStr)\n}\n\n// handleCustomMessage dispatches an incoming custom peers message to\n// subscribers.\nfunc (s *server) handleCustomMessage(peer [33]byte, msg *lnwire.Custom) error {\n\tsrvrLog.Debugf(\"Custom message received: peer=%x, type=%d\",\n\t\tpeer, msg.Type)\n\n\treturn s.customMessageServer.SendUpdate(&CustomMessage{\n\t\tPeer: peer,\n\t\tMsg:  msg,\n\t})\n}\n\n// SubscribeCustomMessages subscribes to a stream of incoming custom peer\n// messages.\nfunc (s *server) SubscribeCustomMessages() (*subscribe.Client, error) {\n\treturn s.customMessageServer.Subscribe()\n}\n\n// peerConnected is a function that handles initialization a newly connected\n// peer by adding it to the server's global list of all active peers, and\n// starting all the goroutines the peer needs to function properly. The inbound\n// boolean should be true if the peer initiated the connection to us.\nfunc (s *server) peerConnected(conn net.Conn, connReq *connmgr.ConnReq,\n\tinbound bool) {\n\n\tbrontideConn := conn.(*brontide.Conn)\n\taddr := conn.RemoteAddr()\n\tpubKey := brontideConn.RemotePub()\n\n\tsrvrLog.Infof(\"Finalizing connection to %x@%s, inbound=%v\",\n\t\tpubKey.SerializeCompressed(), addr, inbound)\n\n\tpeerAddr := &lnwire.NetAddress{\n\t\tIdentityKey: pubKey,\n\t\tAddress:     addr,\n\t\tChainNet:    s.cfg.ActiveNetParams.Net,\n\t}\n\n\t// With the brontide connection established, we'll now craft the feature\n\t// vectors to advertise to the remote node.\n\tinitFeatures := s.featureMgr.Get(feature.SetInit)\n\tlegacyFeatures := s.featureMgr.Get(feature.SetLegacyGlobal)\n\n\t// Lookup past error caches for the peer in the server. If no buffer is\n\t// found, create a fresh buffer.\n\tpkStr := string(peerAddr.IdentityKey.SerializeCompressed())\n\terrBuffer, ok := s.peerErrors[pkStr]\n\tif !ok {\n\t\tvar err error\n\t\terrBuffer, err = queue.NewCircularBuffer(peer.ErrorBufferSize)\n\t\tif err != nil {\n\t\t\tsrvrLog.Errorf(\"unable to create peer %v\", err)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// If we directly set the peer.Config TowerClient member to the\n\t// s.towerClientMgr then in the case that the s.towerClientMgr is nil,\n\t// the peer.Config's TowerClient member will not evaluate to nil even\n\t// though the underlying value is nil. To avoid this gotcha which can\n\t// cause a panic, we need to explicitly pass nil to the peer.Config's\n\t// TowerClient if needed.\n\tvar towerClient wtclient.ClientManager\n\tif s.towerClientMgr != nil {\n\t\ttowerClient = s.towerClientMgr\n\t}\n\n\tthresholdSats := btcutil.Amount(s.cfg.MaxFeeExposure)\n\tthresholdMSats := lnwire.NewMSatFromSatoshis(thresholdSats)\n\n\t// Now that we've established a connection, create a peer, and it to the\n\t// set of currently active peers. Configure the peer with the incoming\n\t// and outgoing broadcast deltas to prevent htlcs from being accepted or\n\t// offered that would trigger channel closure. In case of outgoing\n\t// htlcs, an extra block is added to prevent the channel from being\n\t// closed when the htlc is outstanding and a new block comes in.\n\tpCfg := peer.Config{\n\t\tConn:                    brontideConn,\n\t\tConnReq:                 connReq,\n\t\tAddr:                    peerAddr,\n\t\tInbound:                 inbound,\n\t\tFeatures:                initFeatures,\n\t\tLegacyFeatures:          legacyFeatures,\n\t\tOutgoingCltvRejectDelta: lncfg.DefaultOutgoingCltvRejectDelta,\n\t\tChanActiveTimeout:       s.cfg.ChanEnableTimeout,\n\t\tErrorBuffer:             errBuffer,\n\t\tWritePool:               s.writePool,\n\t\tReadPool:                s.readPool,\n\t\tSwitch:                  s.htlcSwitch,\n\t\tInterceptSwitch:         s.interceptableSwitch,\n\t\tChannelDB:               s.chanStateDB,\n\t\tChannelGraph:            s.graphDB,\n\t\tChainArb:                s.chainArb,\n\t\tAuthGossiper:            s.authGossiper,\n\t\tChanStatusMgr:           s.chanStatusMgr,\n\t\tChainIO:                 s.cc.ChainIO,\n\t\tFeeEstimator:            s.cc.FeeEstimator,\n\t\tSigner:                  s.cc.Wallet.Cfg.Signer,\n\t\tSigPool:                 s.sigPool,\n\t\tWallet:                  s.cc.Wallet,\n\t\tChainNotifier:           s.cc.ChainNotifier,\n\t\tBestBlockView:           s.cc.BestBlockTracker,\n\t\tRoutingPolicy:           s.cc.RoutingPolicy,\n\t\tSphinx:                  s.sphinx,\n\t\tWitnessBeacon:           s.witnessBeacon,\n\t\tInvoices:                s.invoices,\n\t\tChannelNotifier:         s.channelNotifier,\n\t\tHtlcNotifier:            s.htlcNotifier,\n\t\tTowerClient:             towerClient,\n\t\tDisconnectPeer:          s.DisconnectPeer,\n\t\tGenNodeAnnouncement: func(...netann.NodeAnnModifier) (\n\t\t\tlnwire.NodeAnnouncement, error) {\n\n\t\t\treturn s.genNodeAnnouncement(nil)\n\t\t},\n\n\t\tPongBuf: s.pongBuf,\n\n\t\tPrunePersistentPeerConnection: s.prunePersistentPeerConnection,\n\n\t\tFetchLastChanUpdate: s.fetchLastChanUpdate(),\n\n\t\tFundingManager: s.fundingMgr,\n\n\t\tHodl:                    s.cfg.Hodl,\n\t\tUnsafeReplay:            s.cfg.UnsafeReplay,\n\t\tMaxOutgoingCltvExpiry:   s.cfg.MaxOutgoingCltvExpiry,\n\t\tMaxChannelFeeAllocation: s.cfg.MaxChannelFeeAllocation,\n\t\tCoopCloseTargetConfs:    s.cfg.CoopCloseTargetConfs,\n\t\tMaxAnchorsCommitFeeRate: chainfee.SatPerKVByte(\n\t\t\ts.cfg.MaxCommitFeeRateAnchors * 1000).FeePerKWeight(),\n\t\tChannelCommitInterval:  s.cfg.ChannelCommitInterval,\n\t\tPendingCommitInterval:  s.cfg.PendingCommitInterval,\n\t\tChannelCommitBatchSize: s.cfg.ChannelCommitBatchSize,\n\t\tHandleCustomMessage:    s.handleCustomMessage,\n\t\tGetAliases:             s.aliasMgr.GetAliases,\n\t\tRequestAlias:           s.aliasMgr.RequestAlias,\n\t\tAddLocalAlias:          s.aliasMgr.AddLocalAlias,\n\t\tDisallowRouteBlinding:  s.cfg.ProtocolOptions.NoRouteBlinding(),\n\t\tDisallowQuiescence:     s.cfg.ProtocolOptions.NoQuiescence(),\n\t\tMaxFeeExposure:         thresholdMSats,\n\t\tQuit:                   s.quit,\n\t\tAuxLeafStore:           s.implCfg.AuxLeafStore,\n\t\tAuxSigner:              s.implCfg.AuxSigner,\n\t\tMsgRouter:              s.implCfg.MsgRouter,\n\t\tAuxChanCloser:          s.implCfg.AuxChanCloser,\n\t\tAuxResolver:            s.implCfg.AuxContractResolver,\n\t\tAuxTrafficShaper:       s.implCfg.TrafficShaper,\n\t\tShouldFwdExpEndorsement: func() bool {\n\t\t\tif s.cfg.ProtocolOptions.NoExperimentalEndorsement() {\n\t\t\t\treturn false\n\t\t\t}\n\n\t\t\treturn clock.NewDefaultClock().Now().Before(\n\t\t\t\tEndorsementExperimentEnd,\n\t\t\t)\n\t\t},\n\t}\n\n\tcopy(pCfg.PubKeyBytes[:], peerAddr.IdentityKey.SerializeCompressed())\n\tcopy(pCfg.ServerPubKey[:], s.identityECDH.PubKey().SerializeCompressed())\n\n\tp := peer.NewBrontide(pCfg)\n\n\t// TODO(roasbeef): update IP address for link-node\n\t//  * also mark last-seen, do it one single transaction?\n\n\ts.addPeer(p)\n\n\t// Once we have successfully added the peer to the server, we can\n\t// delete the previous error buffer from the server's map of error\n\t// buffers.\n\tdelete(s.peerErrors, pkStr)\n\n\t// Dispatch a goroutine to asynchronously start the peer. This process\n\t// includes sending and receiving Init messages, which would be a DOS\n\t// vector if we held the server's mutex throughout the procedure.\n\ts.wg.Add(1)\n\tgo s.peerInitializer(p)\n}\n\n// addPeer adds the passed peer to the server's global state of all active\n// peers.\nfunc (s *server) addPeer(p *peer.Brontide) {\n\tif p == nil {\n\t\treturn\n\t}\n\n\tpubBytes := p.IdentityKey().SerializeCompressed()\n\n\t// Ignore new peers if we're shutting down.\n\tif s.Stopped() {\n\t\tsrvrLog.Infof(\"Server stopped, skipped adding peer=%x\",\n\t\t\tpubBytes)\n\t\tp.Disconnect(ErrServerShuttingDown)\n\n\t\treturn\n\t}\n\n\t// Track the new peer in our indexes so we can quickly look it up either\n\t// according to its public key, or its peer ID.\n\t// TODO(roasbeef): pipe all requests through to the\n\t// queryHandler/peerManager\n\n\t// NOTE: This pubStr is a raw bytes to string conversion and will NOT\n\t// be human-readable.\n\tpubStr := string(pubBytes)\n\n\ts.peersByPub[pubStr] = p\n\n\tif p.Inbound() {\n\t\ts.inboundPeers[pubStr] = p\n\t} else {\n\t\ts.outboundPeers[pubStr] = p\n\t}\n\n\t// Inform the peer notifier of a peer online event so that it can be reported\n\t// to clients listening for peer events.\n\tvar pubKey [33]byte\n\tcopy(pubKey[:], pubBytes)\n\n\ts.peerNotifier.NotifyPeerOnline(pubKey)\n}\n\n// peerInitializer asynchronously starts a newly connected peer after it has\n// been added to the server's peer map. This method sets up a\n// peerTerminationWatcher for the given peer, and ensures that it executes even\n// if the peer failed to start. In the event of a successful connection, this\n// method reads the negotiated, local feature-bits and spawns the appropriate\n// graph synchronization method. Any registered clients of NotifyWhenOnline will\n// be signaled of the new peer once the method returns.\n//\n// NOTE: This MUST be launched as a goroutine.\nfunc (s *server) peerInitializer(p *peer.Brontide) {\n\tdefer s.wg.Done()\n\n\tpubBytes := p.IdentityKey().SerializeCompressed()\n\n\t// Avoid initializing peers while the server is exiting.\n\tif s.Stopped() {\n\t\tsrvrLog.Infof(\"Server stopped, skipped initializing peer=%x\",\n\t\t\tpubBytes)\n\t\treturn\n\t}\n\n\t// Create a channel that will be used to signal a successful start of\n\t// the link. This prevents the peer termination watcher from beginning\n\t// its duty too early.\n\tready := make(chan struct{})\n\n\t// Before starting the peer, launch a goroutine to watch for the\n\t// unexpected termination of this peer, which will ensure all resources\n\t// are properly cleaned up, and re-establish persistent connections when\n\t// necessary. The peer termination watcher will be short circuited if\n\t// the peer is ever added to the ignorePeerTermination map, indicating\n\t// that the server has already handled the removal of this peer.\n\ts.wg.Add(1)\n\tgo s.peerTerminationWatcher(p, ready)\n\n\t// Start the peer! If an error occurs, we Disconnect the peer, which\n\t// will unblock the peerTerminationWatcher.\n\tif err := p.Start(); err != nil {\n\t\tsrvrLog.Warnf(\"Starting peer=%x got error: %v\", pubBytes, err)\n\n\t\tp.Disconnect(fmt.Errorf(\"unable to start peer: %w\", err))\n\t\treturn\n\t}\n\n\t// Otherwise, signal to the peerTerminationWatcher that the peer startup\n\t// was successful, and to begin watching the peer's wait group.\n\tclose(ready)\n\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// Check if there are listeners waiting for this peer to come online.\n\tsrvrLog.Debugf(\"Notifying that peer %v is online\", p)\n\n\t// TODO(guggero): Do a proper conversion to a string everywhere, or use\n\t// route.Vertex as the key type of peerConnectedListeners.\n\tpubStr := string(pubBytes)\n\tfor _, peerChan := range s.peerConnectedListeners[pubStr] {\n\t\tselect {\n\t\tcase peerChan <- p:\n\t\tcase <-s.quit:\n\t\t\treturn\n\t\t}\n\t}\n\tdelete(s.peerConnectedListeners, pubStr)\n}\n\n// peerTerminationWatcher waits until a peer has been disconnected unexpectedly,\n// and then cleans up all resources allocated to the peer, notifies relevant\n// sub-systems of its demise, and finally handles re-connecting to the peer if\n// it's persistent. If the server intentionally disconnects a peer, it should\n// have a corresponding entry in the ignorePeerTermination map which will cause\n// the cleanup routine to exit early. The passed `ready` chan is used to\n// synchronize when WaitForDisconnect should begin watching on the peer's\n// waitgroup. The ready chan should only be signaled if the peer starts\n// successfully, otherwise the peer should be disconnected instead.\n//\n// NOTE: This MUST be launched as a goroutine.\nfunc (s *server) peerTerminationWatcher(p *peer.Brontide, ready chan struct{}) {\n\tdefer s.wg.Done()\n\n\tp.WaitForDisconnect(ready)\n\n\tsrvrLog.Debugf(\"Peer %v has been disconnected\", p)\n\n\t// If the server is exiting then we can bail out early ourselves as all\n\t// the other sub-systems will already be shutting down.\n\tif s.Stopped() {\n\t\tsrvrLog.Debugf(\"Server quitting, exit early for peer %v\", p)\n\t\treturn\n\t}\n\n\t// Next, we'll cancel all pending funding reservations with this node.\n\t// If we tried to initiate any funding flows that haven't yet finished,\n\t// then we need to unlock those committed outputs so they're still\n\t// available for use.\n\ts.fundingMgr.CancelPeerReservations(p.PubKey())\n\n\tpubKey := p.IdentityKey()\n\n\t// We'll also inform the gossiper that this peer is no longer active,\n\t// so we don't need to maintain sync state for it any longer.\n\ts.authGossiper.PruneSyncState(p.PubKey())\n\n\t// Tell the switch to remove all links associated with this peer.\n\t// Passing nil as the target link indicates that all links associated\n\t// with this interface should be closed.\n\t//\n\t// TODO(roasbeef): instead add a PurgeInterfaceLinks function?\n\tlinks, err := s.htlcSwitch.GetLinksByInterface(p.PubKey())\n\tif err != nil && err != htlcswitch.ErrNoLinksFound {\n\t\tsrvrLog.Errorf(\"Unable to get channel links for %v: %v\", p, err)\n\t}\n\n\tfor _, link := range links {\n\t\ts.htlcSwitch.RemoveLink(link.ChanID())\n\t}\n\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// If there were any notification requests for when this peer\n\t// disconnected, we can trigger them now.\n\tsrvrLog.Debugf(\"Notifying that peer %v is offline\", p)\n\tpubStr := string(pubKey.SerializeCompressed())\n\tfor _, offlineChan := range s.peerDisconnectedListeners[pubStr] {\n\t\tclose(offlineChan)\n\t}\n\tdelete(s.peerDisconnectedListeners, pubStr)\n\n\t// If the server has already removed this peer, we can short circuit the\n\t// peer termination watcher and skip cleanup.\n\tif _, ok := s.ignorePeerTermination[p]; ok {\n\t\tdelete(s.ignorePeerTermination, p)\n\n\t\tpubKey := p.PubKey()\n\t\tpubStr := string(pubKey[:])\n\n\t\t// If a connection callback is present, we'll go ahead and\n\t\t// execute it now that previous peer has fully disconnected. If\n\t\t// the callback is not present, this likely implies the peer was\n\t\t// purposefully disconnected via RPC, and that no reconnect\n\t\t// should be attempted.\n\t\tconnCallback, ok := s.scheduledPeerConnection[pubStr]\n\t\tif ok {\n\t\t\tdelete(s.scheduledPeerConnection, pubStr)\n\t\t\tconnCallback()\n\t\t}\n\t\treturn\n\t}\n\n\t// First, cleanup any remaining state the server has regarding the peer\n\t// in question.\n\ts.removePeer(p)\n\n\t// Next, check to see if this is a persistent peer or not.\n\tif _, ok := s.persistentPeers[pubStr]; !ok {\n\t\treturn\n\t}\n\n\t// Get the last address that we used to connect to the peer.\n\taddrs := []net.Addr{\n\t\tp.NetAddress().Address,\n\t}\n\n\t// We'll ensure that we locate all the peers advertised addresses for\n\t// reconnection purposes.\n\tadvertisedAddrs, err := s.fetchNodeAdvertisedAddrs(pubKey)\n\tswitch {\n\t// We found advertised addresses, so use them.\n\tcase err == nil:\n\t\taddrs = advertisedAddrs\n\n\t// The peer doesn't have an advertised address.\n\tcase err == errNoAdvertisedAddr:\n\t\t// If it is an outbound peer then we fall back to the existing\n\t\t// peer address.\n\t\tif !p.Inbound() {\n\t\t\tbreak\n\t\t}\n\n\t\t// Fall back to the existing peer address if\n\t\t// we're not accepting connections over Tor.\n\t\tif s.torController == nil {\n\t\t\tbreak\n\t\t}\n\n\t\t// If we are, the peer's address won't be known\n\t\t// to us (we'll see a private address, which is\n\t\t// the address used by our onion service to dial\n\t\t// to lnd), so we don't have enough information\n\t\t// to attempt a reconnect.\n\t\tsrvrLog.Debugf(\"Ignoring reconnection attempt \"+\n\t\t\t\"to inbound peer %v without \"+\n\t\t\t\"advertised address\", p)\n\t\treturn\n\n\t// We came across an error retrieving an advertised\n\t// address, log it, and fall back to the existing peer\n\t// address.\n\tdefault:\n\t\tsrvrLog.Errorf(\"Unable to retrieve advertised \"+\n\t\t\t\"address for node %x: %v\", p.PubKey(),\n\t\t\terr)\n\t}\n\n\t// Make an easy lookup map so that we can check if an address\n\t// is already in the address list that we have stored for this peer.\n\texistingAddrs := make(map[string]bool)\n\tfor _, addr := range s.persistentPeerAddrs[pubStr] {\n\t\texistingAddrs[addr.String()] = true\n\t}\n\n\t// Add any missing addresses for this peer to persistentPeerAddr.\n\tfor _, addr := range addrs {\n\t\tif existingAddrs[addr.String()] {\n\t\t\tcontinue\n\t\t}\n\n\t\ts.persistentPeerAddrs[pubStr] = append(\n\t\t\ts.persistentPeerAddrs[pubStr],\n\t\t\t&lnwire.NetAddress{\n\t\t\t\tIdentityKey: p.IdentityKey(),\n\t\t\t\tAddress:     addr,\n\t\t\t\tChainNet:    p.NetAddress().ChainNet,\n\t\t\t},\n\t\t)\n\t}\n\n\t// Record the computed backoff in the backoff map.\n\tbackoff := s.nextPeerBackoff(pubStr, p.StartTime())\n\ts.persistentPeersBackoff[pubStr] = backoff\n\n\t// Initialize a retry canceller for this peer if one does not\n\t// exist.\n\tcancelChan, ok := s.persistentRetryCancels[pubStr]\n\tif !ok {\n\t\tcancelChan = make(chan struct{})\n\t\ts.persistentRetryCancels[pubStr] = cancelChan\n\t}\n\n\t// We choose not to wait group this go routine since the Connect\n\t// call can stall for arbitrarily long if we shutdown while an\n\t// outbound connection attempt is being made.\n\tgo func() {\n\t\tsrvrLog.Debugf(\"Scheduling connection re-establishment to \"+\n\t\t\t\"persistent peer %x in %s\",\n\t\t\tp.IdentityKey().SerializeCompressed(), backoff)\n\n\t\tselect {\n\t\tcase <-time.After(backoff):\n\t\tcase <-cancelChan:\n\t\t\treturn\n\t\tcase <-s.quit:\n\t\t\treturn\n\t\t}\n\n\t\tsrvrLog.Debugf(\"Attempting to re-establish persistent \"+\n\t\t\t\"connection to peer %x\",\n\t\t\tp.IdentityKey().SerializeCompressed())\n\n\t\ts.connectToPersistentPeer(pubStr)\n\t}()\n}\n\n// connectToPersistentPeer uses all the stored addresses for a peer to attempt\n// to connect to the peer. It creates connection requests if there are\n// currently none for a given address and it removes old connection requests\n// if the associated address is no longer in the latest address list for the\n// peer.\nfunc (s *server) connectToPersistentPeer(pubKeyStr string) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// Create an easy lookup map of the addresses we have stored for the\n\t// peer. We will remove entries from this map if we have existing\n\t// connection requests for the associated address and then any leftover\n\t// entries will indicate which addresses we should create new\n\t// connection requests for.\n\taddrMap := make(map[string]*lnwire.NetAddress)\n\tfor _, addr := range s.persistentPeerAddrs[pubKeyStr] {\n\t\taddrMap[addr.String()] = addr\n\t}\n\n\t// Go through each of the existing connection requests and\n\t// check if they correspond to the latest set of addresses. If\n\t// there is a connection requests that does not use one of the latest\n\t// advertised addresses then remove that connection request.\n\tvar updatedConnReqs []*connmgr.ConnReq\n\tfor _, connReq := range s.persistentConnReqs[pubKeyStr] {\n\t\tlnAddr := connReq.Addr.(*lnwire.NetAddress).Address.String()\n\n\t\tswitch _, ok := addrMap[lnAddr]; ok {\n\t\t// If the existing connection request is using one of the\n\t\t// latest advertised addresses for the peer then we add it to\n\t\t// updatedConnReqs and remove the associated address from\n\t\t// addrMap so that we don't recreate this connReq later on.\n\t\tcase true:\n\t\t\tupdatedConnReqs = append(\n\t\t\t\tupdatedConnReqs, connReq,\n\t\t\t)\n\t\t\tdelete(addrMap, lnAddr)\n\n\t\t// If the existing connection request is using an address that\n\t\t// is not one of the latest advertised addresses for the peer\n\t\t// then we remove the connecting request from the connection\n\t\t// manager.\n\t\tcase false:\n\t\t\tsrvrLog.Info(\n\t\t\t\t\"Removing conn req:\", connReq.Addr.String(),\n\t\t\t)\n\t\t\ts.connMgr.Remove(connReq.ID())\n\t\t}\n\t}\n\n\ts.persistentConnReqs[pubKeyStr] = updatedConnReqs\n\n\tcancelChan, ok := s.persistentRetryCancels[pubKeyStr]\n\tif !ok {\n\t\tcancelChan = make(chan struct{})\n\t\ts.persistentRetryCancels[pubKeyStr] = cancelChan\n\t}\n\n\t// Any addresses left in addrMap are new ones that we have not made\n\t// connection requests for. So create new connection requests for those.\n\t// If there is more than one address in the address map, stagger the\n\t// creation of the connection requests for those.\n\tgo func() {\n\t\tticker := time.NewTicker(multiAddrConnectionStagger)\n\t\tdefer ticker.Stop()\n\n\t\tfor _, addr := range addrMap {\n\t\t\t// Send the persistent connection request to the\n\t\t\t// connection manager, saving the request itself so we\n\t\t\t// can cancel/restart the process as needed.\n\t\t\tconnReq := &connmgr.ConnReq{\n\t\t\t\tAddr:      addr,\n\t\t\t\tPermanent: true,\n\t\t\t}\n\n\t\t\ts.mu.Lock()\n\t\t\ts.persistentConnReqs[pubKeyStr] = append(\n\t\t\t\ts.persistentConnReqs[pubKeyStr], connReq,\n\t\t\t)\n\t\t\ts.mu.Unlock()\n\n\t\t\tsrvrLog.Debugf(\"Attempting persistent connection to \"+\n\t\t\t\t\"channel peer %v\", addr)\n\n\t\t\tgo s.connMgr.Connect(connReq)\n\n\t\t\tselect {\n\t\t\tcase <-s.quit:\n\t\t\t\treturn\n\t\t\tcase <-cancelChan:\n\t\t\t\treturn\n\t\t\tcase <-ticker.C:\n\t\t\t}\n\t\t}\n\t}()\n}\n\n// removePeer removes the passed peer from the server's state of all active\n// peers.\nfunc (s *server) removePeer(p *peer.Brontide) {\n\tif p == nil {\n\t\treturn\n\t}\n\n\tsrvrLog.Debugf(\"removing peer %v\", p)\n\n\t// As the peer is now finished, ensure that the TCP connection is\n\t// closed and all of its related goroutines have exited.\n\tp.Disconnect(fmt.Errorf(\"server: disconnecting peer %v\", p))\n\n\t// If this peer had an active persistent connection request, remove it.\n\tif p.ConnReq() != nil {\n\t\ts.connMgr.Remove(p.ConnReq().ID())\n\t}\n\n\t// Ignore deleting peers if we're shutting down.\n\tif s.Stopped() {\n\t\treturn\n\t}\n\n\tpKey := p.PubKey()\n\tpubSer := pKey[:]\n\tpubStr := string(pubSer)\n\n\tdelete(s.peersByPub, pubStr)\n\n\tif p.Inbound() {\n\t\tdelete(s.inboundPeers, pubStr)\n\t} else {\n\t\tdelete(s.outboundPeers, pubStr)\n\t}\n\n\t// Copy the peer's error buffer across to the server if it has any items\n\t// in it so that we can restore peer errors across connections.\n\tif p.ErrorBuffer().Total() > 0 {\n\t\ts.peerErrors[pubStr] = p.ErrorBuffer()\n\t}\n\n\t// Inform the peer notifier of a peer offline event so that it can be\n\t// reported to clients listening for peer events.\n\tvar pubKey [33]byte\n\tcopy(pubKey[:], pubSer)\n\n\ts.peerNotifier.NotifyPeerOffline(pubKey)\n}\n\n// ConnectToPeer requests that the server connect to a Lightning Network peer\n// at the specified address. This function will *block* until either a\n// connection is established, or the initial handshake process fails.\n//\n// NOTE: This function is safe for concurrent access.\nfunc (s *server) ConnectToPeer(addr *lnwire.NetAddress,\n\tperm bool, timeout time.Duration) error {\n\n\ttargetPub := string(addr.IdentityKey.SerializeCompressed())\n\n\t// Acquire mutex, but use explicit unlocking instead of defer for\n\t// better granularity.  In certain conditions, this method requires\n\t// making an outbound connection to a remote peer, which requires the\n\t// lock to be released, and subsequently reacquired.\n\ts.mu.Lock()\n\n\t// Ensure we're not already connected to this peer.\n\tpeer, err := s.findPeerByPubStr(targetPub)\n\tif err == nil {\n\t\ts.mu.Unlock()\n\t\treturn &errPeerAlreadyConnected{peer: peer}\n\t}\n\n\t// Peer was not found, continue to pursue connection with peer.\n\n\t// If there's already a pending connection request for this pubkey,\n\t// then we ignore this request to ensure we don't create a redundant\n\t// connection.\n\tif reqs, ok := s.persistentConnReqs[targetPub]; ok {\n\t\tsrvrLog.Warnf(\"Already have %d persistent connection \"+\n\t\t\t\"requests for %v, connecting anyway.\", len(reqs), addr)\n\t}\n\n\t// If there's not already a pending or active connection to this node,\n\t// then instruct the connection manager to attempt to establish a\n\t// persistent connection to the peer.\n\tsrvrLog.Debugf(\"Connecting to %v\", addr)\n\tif perm {\n\t\tconnReq := &connmgr.ConnReq{\n\t\t\tAddr:      addr,\n\t\t\tPermanent: true,\n\t\t}\n\n\t\t// Since the user requested a permanent connection, we'll set\n\t\t// the entry to true which will tell the server to continue\n\t\t// reconnecting even if the number of channels with this peer is\n\t\t// zero.\n\t\ts.persistentPeers[targetPub] = true\n\t\tif _, ok := s.persistentPeersBackoff[targetPub]; !ok {\n\t\t\ts.persistentPeersBackoff[targetPub] = s.cfg.MinBackoff\n\t\t}\n\t\ts.persistentConnReqs[targetPub] = append(\n\t\t\ts.persistentConnReqs[targetPub], connReq,\n\t\t)\n\t\ts.mu.Unlock()\n\n\t\tgo s.connMgr.Connect(connReq)\n\n\t\treturn nil\n\t}\n\ts.mu.Unlock()\n\n\t// If we're not making a persistent connection, then we'll attempt to\n\t// connect to the target peer. If the we can't make the connection, or\n\t// the crypto negotiation breaks down, then return an error to the\n\t// caller.\n\terrChan := make(chan error, 1)\n\ts.connectToPeer(addr, errChan, timeout)\n\n\tselect {\n\tcase err := <-errChan:\n\t\treturn err\n\tcase <-s.quit:\n\t\treturn ErrServerShuttingDown\n\t}\n}\n\n// connectToPeer establishes a connection to a remote peer. errChan is used to\n// notify the caller if the connection attempt has failed. Otherwise, it will be\n// closed.\nfunc (s *server) connectToPeer(addr *lnwire.NetAddress,\n\terrChan chan<- error, timeout time.Duration) {\n\n\tconn, err := brontide.Dial(\n\t\ts.identityECDH, addr, timeout, s.cfg.net.Dial,\n\t)\n\tif err != nil {\n\t\tsrvrLog.Errorf(\"Unable to connect to %v: %v\", addr, err)\n\t\tselect {\n\t\tcase errChan <- err:\n\t\tcase <-s.quit:\n\t\t}\n\t\treturn\n\t}\n\n\tclose(errChan)\n\n\tsrvrLog.Tracef(\"Brontide dialer made local=%v, remote=%v\",\n\t\tconn.LocalAddr(), conn.RemoteAddr())\n\n\ts.OutboundPeerConnected(nil, conn)\n}\n\n// DisconnectPeer sends the request to server to close the connection with peer\n// identified by public key.\n//\n// NOTE: This function is safe for concurrent access.\nfunc (s *server) DisconnectPeer(pubKey *btcec.PublicKey) error {\n\tpubBytes := pubKey.SerializeCompressed()\n\tpubStr := string(pubBytes)\n\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\t// Check that were actually connected to this peer. If not, then we'll\n\t// exit in an error as we can't disconnect from a peer that we're not\n\t// currently connected to.\n\tpeer, err := s.findPeerByPubStr(pubStr)\n\tif err == ErrPeerNotConnected {\n\t\treturn fmt.Errorf(\"peer %x is not connected\", pubBytes)\n\t}\n\n\tsrvrLog.Infof(\"Disconnecting from %v\", peer)\n\n\ts.cancelConnReqs(pubStr, nil)\n\n\t// If this peer was formerly a persistent connection, then we'll remove\n\t// them from this map so we don't attempt to re-connect after we\n\t// disconnect.\n\tdelete(s.persistentPeers, pubStr)\n\tdelete(s.persistentPeersBackoff, pubStr)\n\n\t// Remove the peer by calling Disconnect. Previously this was done with\n\t// removePeer, which bypassed the peerTerminationWatcher.\n\tpeer.Disconnect(fmt.Errorf(\"server: DisconnectPeer called\"))\n\n\treturn nil\n}\n\n// OpenChannel sends a request to the server to open a channel to the specified\n// peer identified by nodeKey with the passed channel funding parameters.\n//\n// NOTE: This function is safe for concurrent access.\nfunc (s *server) OpenChannel(\n\treq *funding.InitFundingMsg) (chan *lnrpc.OpenStatusUpdate, chan error) {\n\n\t// The updateChan will have a buffer of 2, since we expect a ChanPending\n\t// + a ChanOpen update, and we want to make sure the funding process is\n\t// not blocked if the caller is not reading the updates.\n\treq.Updates = make(chan *lnrpc.OpenStatusUpdate, 2)\n\treq.Err = make(chan error, 1)\n\n\t// First attempt to locate the target peer to open a channel with, if\n\t// we're unable to locate the peer then this request will fail.\n\tpubKeyBytes := req.TargetPubkey.SerializeCompressed()\n\ts.mu.RLock()\n\tpeer, ok := s.peersByPub[string(pubKeyBytes)]\n\tif !ok {\n\t\ts.mu.RUnlock()\n\n\t\treq.Err <- fmt.Errorf(\"peer %x is not online\", pubKeyBytes)\n\t\treturn req.Updates, req.Err\n\t}\n\treq.Peer = peer\n\ts.mu.RUnlock()\n\n\t// We'll wait until the peer is active before beginning the channel\n\t// opening process.\n\tselect {\n\tcase <-peer.ActiveSignal():\n\tcase <-peer.QuitSignal():\n\t\treq.Err <- fmt.Errorf(\"peer %x disconnected\", pubKeyBytes)\n\t\treturn req.Updates, req.Err\n\tcase <-s.quit:\n\t\treq.Err <- ErrServerShuttingDown\n\t\treturn req.Updates, req.Err\n\t}\n\n\t// If the fee rate wasn't specified at this point we fail the funding\n\t// because of the missing fee rate information. The caller of the\n\t// `OpenChannel` method needs to make sure that default values for the\n\t// fee rate are set beforehand.\n\tif req.FundingFeePerKw == 0 {\n\t\treq.Err <- fmt.Errorf(\"no FundingFeePerKw specified for \" +\n\t\t\t\"the channel opening transaction\")\n\n\t\treturn req.Updates, req.Err\n\t}\n\n\t// Spawn a goroutine to send the funding workflow request to the funding\n\t// manager. This allows the server to continue handling queries instead\n\t// of blocking on this request which is exported as a synchronous\n\t// request to the outside world.\n\tgo s.fundingMgr.InitFundingWorkflow(req)\n\n\treturn req.Updates, req.Err\n}\n\n// Peers returns a slice of all active peers.\n//\n// NOTE: This function is safe for concurrent access.\nfunc (s *server) Peers() []*peer.Brontide {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\n\tpeers := make([]*peer.Brontide, 0, len(s.peersByPub))\n\tfor _, peer := range s.peersByPub {\n\t\tpeers = append(peers, peer)\n\t}\n\n\treturn peers\n}\n\n// computeNextBackoff uses a truncated exponential backoff to compute the next\n// backoff using the value of the exiting backoff. The returned duration is\n// randomized in either direction by 1/20 to prevent tight loops from\n// stabilizing.\nfunc computeNextBackoff(currBackoff, maxBackoff time.Duration) time.Duration {\n\t// Double the current backoff, truncating if it exceeds our maximum.\n\tnextBackoff := 2 * currBackoff\n\tif nextBackoff > maxBackoff {\n\t\tnextBackoff = maxBackoff\n\t}\n\n\t// Using 1/10 of our duration as a margin, compute a random offset to\n\t// avoid the nodes entering connection cycles.\n\tmargin := nextBackoff / 10\n\n\tvar wiggle big.Int\n\twiggle.SetUint64(uint64(margin))\n\tif _, err := rand.Int(rand.Reader, &wiggle); err != nil {\n\t\t// Randomizing is not mission critical, so we'll just return the\n\t\t// current backoff.\n\t\treturn nextBackoff\n\t}\n\n\t// Otherwise add in our wiggle, but subtract out half of the margin so\n\t// that the backoff can tweaked by 1/20 in either direction.\n\treturn nextBackoff + (time.Duration(wiggle.Uint64()) - margin/2)\n}\n\n// errNoAdvertisedAddr is an error returned when we attempt to retrieve the\n// advertised address of a node, but they don't have one.\nvar errNoAdvertisedAddr = errors.New(\"no advertised address found\")\n\n// fetchNodeAdvertisedAddrs attempts to fetch the advertised addresses of a node.\nfunc (s *server) fetchNodeAdvertisedAddrs(pub *btcec.PublicKey) ([]net.Addr, error) {\n\tvertex, err := route.NewVertexFromBytes(pub.SerializeCompressed())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tnode, err := s.graphDB.FetchLightningNode(vertex)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif len(node.Addresses) == 0 {\n\t\treturn nil, errNoAdvertisedAddr\n\t}\n\n\treturn node.Addresses, nil\n}\n\n// fetchLastChanUpdate returns a function which is able to retrieve our latest\n// channel update for a target channel.\nfunc (s *server) fetchLastChanUpdate() func(lnwire.ShortChannelID) (\n\t*lnwire.ChannelUpdate1, error) {\n\n\tourPubKey := s.identityECDH.PubKey().SerializeCompressed()\n\treturn func(cid lnwire.ShortChannelID) (*lnwire.ChannelUpdate1, error) {\n\t\tinfo, edge1, edge2, err := s.graphBuilder.GetChannelByID(cid)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\treturn netann.ExtractChannelUpdate(\n\t\t\tourPubKey[:], info, edge1, edge2,\n\t\t)\n\t}\n}\n\n// applyChannelUpdate applies the channel update to the different sub-systems of\n// the server. The useAlias boolean denotes whether or not to send an alias in\n// place of the real SCID.\nfunc (s *server) applyChannelUpdate(update *lnwire.ChannelUpdate1,\n\top *wire.OutPoint, useAlias bool) error {\n\n\tvar (\n\t\tpeerAlias    *lnwire.ShortChannelID\n\t\tdefaultAlias lnwire.ShortChannelID\n\t)\n\n\tchanID := lnwire.NewChanIDFromOutPoint(*op)\n\n\t// Fetch the peer's alias from the lnwire.ChannelID so it can be used\n\t// in the ChannelUpdate if it hasn't been announced yet.\n\tif useAlias {\n\t\tfoundAlias, _ := s.aliasMgr.GetPeerAlias(chanID)\n\t\tif foundAlias != defaultAlias {\n\t\t\tpeerAlias = &foundAlias\n\t\t}\n\t}\n\n\terrChan := s.authGossiper.ProcessLocalAnnouncement(\n\t\tupdate, discovery.RemoteAlias(peerAlias),\n\t)\n\tselect {\n\tcase err := <-errChan:\n\t\treturn err\n\tcase <-s.quit:\n\t\treturn ErrServerShuttingDown\n\t}\n}\n\n// SendCustomMessage sends a custom message to the peer with the specified\n// pubkey.\nfunc (s *server) SendCustomMessage(peerPub [33]byte, msgType lnwire.MessageType,\n\tdata []byte) error {\n\n\tpeer, err := s.FindPeerByPubStr(string(peerPub[:]))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// We'll wait until the peer is active.\n\tselect {\n\tcase <-peer.ActiveSignal():\n\tcase <-peer.QuitSignal():\n\t\treturn fmt.Errorf(\"peer %x disconnected\", peerPub)\n\tcase <-s.quit:\n\t\treturn ErrServerShuttingDown\n\t}\n\n\tmsg, err := lnwire.NewCustom(msgType, data)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Send the message as low-priority. For now we assume that all\n\t// application-defined message are low priority.\n\treturn peer.SendMessageLazy(true, msg)\n}\n\n// newSweepPkScriptGen creates closure that generates a new public key script\n// which should be used to sweep any funds into the on-chain wallet.\n// Specifically, the script generated is a version 0, pay-to-witness-pubkey-hash\n// (p2wkh) output.\nfunc newSweepPkScriptGen(\n\twallet lnwallet.WalletController,\n\tnetParams *chaincfg.Params) func() fn.Result[lnwallet.AddrWithKey] {\n\n\treturn func() fn.Result[lnwallet.AddrWithKey] {\n\t\tsweepAddr, err := wallet.NewAddress(\n\t\t\tlnwallet.TaprootPubkey, false,\n\t\t\tlnwallet.DefaultAccountName,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn fn.Err[lnwallet.AddrWithKey](err)\n\t\t}\n\n\t\taddr, err := txscript.PayToAddrScript(sweepAddr)\n\t\tif err != nil {\n\t\t\treturn fn.Err[lnwallet.AddrWithKey](err)\n\t\t}\n\n\t\tinternalKeyDesc, err := lnwallet.InternalKeyForAddr(\n\t\t\twallet, netParams, addr,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn fn.Err[lnwallet.AddrWithKey](err)\n\t\t}\n\n\t\treturn fn.Ok(lnwallet.AddrWithKey{\n\t\t\tDeliveryAddress: addr,\n\t\t\tInternalKey:     internalKeyDesc,\n\t\t})\n\t}\n}\n\n// shouldPeerBootstrap returns true if we should attempt to perform peer\n// bootstrapping to actively seek our peers using the set of active network\n// bootstrappers.\nfunc shouldPeerBootstrap(cfg *Config) bool {\n\tisSimnet := cfg.Bitcoin.SimNet\n\tisSignet := cfg.Bitcoin.SigNet\n\tisRegtest := cfg.Bitcoin.RegTest\n\tisDevNetwork := isSimnet || isSignet || isRegtest\n\n\t// TODO(yy): remove the check on simnet/regtest such that the itest is\n\t// covering the bootstrapping process.\n\treturn !cfg.NoNetBootstrap && !isDevNetwork\n}\n\n// fetchClosedChannelSCIDs returns a set of SCIDs that have their force closing\n// finished.\nfunc (s *server) fetchClosedChannelSCIDs() map[lnwire.ShortChannelID]struct{} {\n\t// Get a list of closed channels.\n\tchannels, err := s.chanStateDB.FetchClosedChannels(false)\n\tif err != nil {\n\t\tsrvrLog.Errorf(\"Failed to fetch closed channels: %v\", err)\n\t\treturn nil\n\t}\n\n\t// Save the SCIDs in a map.\n\tclosedSCIDs := make(map[lnwire.ShortChannelID]struct{}, len(channels))\n\tfor _, c := range channels {\n\t\t// If the channel is not pending, its FC has been finalized.\n\t\tif !c.IsPending {\n\t\t\tclosedSCIDs[c.ShortChanID] = struct{}{}\n\t\t}\n\t}\n\n\t// Double check whether the reported closed channel has indeed finished\n\t// closing.\n\t//\n\t// NOTE: There are misalignments regarding when a channel's FC is\n\t// marked as finalized. We double check the pending channels to make\n\t// sure the returned SCIDs are indeed terminated.\n\t//\n\t// TODO(yy): fix the misalignments in `FetchClosedChannels`.\n\tpendings, err := s.chanStateDB.FetchPendingChannels()\n\tif err != nil {\n\t\tsrvrLog.Errorf(\"Failed to fetch pending channels: %v\", err)\n\t\treturn nil\n\t}\n\n\tfor _, c := range pendings {\n\t\tif _, ok := closedSCIDs[c.ShortChannelID]; !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\t// If the channel is still reported as pending, remove it from\n\t\t// the map.\n\t\tdelete(closedSCIDs, c.ShortChannelID)\n\n\t\tsrvrLog.Warnf(\"Channel=%v is prematurely marked as finalized\",\n\t\t\tc.ShortChannelID)\n\t}\n\n\treturn closedSCIDs\n}\n\n// getStartingBeat returns the current beat. This is used during the startup to\n// initialize blockbeat consumers.\nfunc (s *server) getStartingBeat() (*chainio.Beat, error) {\n\t// beat is the current blockbeat.\n\tvar beat *chainio.Beat\n\n\t// We should get a notification with the current best block immediately\n\t// by passing a nil block.\n\tblockEpochs, err := s.cc.ChainNotifier.RegisterBlockEpochNtfn(nil)\n\tif err != nil {\n\t\treturn beat, fmt.Errorf(\"register block epoch ntfn: %w\", err)\n\t}\n\tdefer blockEpochs.Cancel()\n\n\t// We registered for the block epochs with a nil request. The notifier\n\t// should send us the current best block immediately. So we need to\n\t// wait for it here because we need to know the current best height.\n\tselect {\n\tcase bestBlock := <-blockEpochs.Epochs:\n\t\tsrvrLog.Infof(\"Received initial block %v at height %d\",\n\t\t\tbestBlock.Hash, bestBlock.Height)\n\n\t\t// Update the current blockbeat.\n\t\tbeat = chainio.NewBeat(*bestBlock)\n\n\tcase <-s.quit:\n\t\tsrvrLog.Debug(\"LND shutting down\")\n\t}\n\n\treturn beat, nil\n}\n"
        },
        {
          "name": "server_test.go",
          "type": "blob",
          "size": 1.4248046875,
          "content": "package lnd\n\nimport (\n\t\"testing\"\n\n\t\"github.com/lightningnetwork/lnd/lncfg\"\n)\n\n// TestShouldPeerBootstrap tests that we properly skip network bootstrap for\n// the developer networks, and also if bootstrapping is explicitly disabled.\nfunc TestShouldPeerBootstrap(t *testing.T) {\n\tt.Parallel()\n\n\ttestCases := []struct {\n\t\tcfg            *Config\n\t\tshouldBoostrap bool\n\t}{\n\t\t// Simnet active, no bootstrap.\n\t\t{\n\t\t\tcfg: &Config{\n\t\t\t\tBitcoin: &lncfg.Chain{\n\t\t\t\t\tSimNet: true,\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\n\t\t// Regtest active, no bootstrap.\n\t\t{\n\t\t\tcfg: &Config{\n\t\t\t\tBitcoin: &lncfg.Chain{\n\t\t\t\t\tRegTest: true,\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\n\t\t// Signet active, no bootstrap.\n\t\t{\n\t\t\tcfg: &Config{\n\t\t\t\tBitcoin: &lncfg.Chain{\n\t\t\t\t\tSigNet: true,\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\n\t\t// Mainnet active, but bootstrap disabled, no bootstrap.\n\t\t{\n\t\t\tcfg: &Config{\n\t\t\t\tBitcoin: &lncfg.Chain{\n\t\t\t\t\tMainNet: true,\n\t\t\t\t},\n\t\t\t\tNoNetBootstrap: true,\n\t\t\t},\n\t\t},\n\n\t\t// Mainnet active, should bootstrap.\n\t\t{\n\t\t\tcfg: &Config{\n\t\t\t\tBitcoin: &lncfg.Chain{\n\t\t\t\t\tMainNet: true,\n\t\t\t\t},\n\t\t\t},\n\t\t\tshouldBoostrap: true,\n\t\t},\n\n\t\t// Testnet active, should bootstrap.\n\t\t{\n\t\t\tcfg: &Config{\n\t\t\t\tBitcoin: &lncfg.Chain{\n\t\t\t\t\tTestNet3: true,\n\t\t\t\t},\n\t\t\t},\n\t\t\tshouldBoostrap: true,\n\t\t},\n\t}\n\tfor i, testCase := range testCases {\n\t\tbootstrapped := shouldPeerBootstrap(testCase.cfg)\n\t\tif bootstrapped != testCase.shouldBoostrap {\n\t\t\tt.Fatalf(\"#%v: expected bootstrap=%v, got bootstrap=%v\",\n\t\t\t\ti, testCase.shouldBoostrap, bootstrapped)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "shachain",
          "type": "tree",
          "content": null
        },
        {
          "name": "signal",
          "type": "tree",
          "content": null
        },
        {
          "name": "sqlc.yaml",
          "type": "blob",
          "size": 0.2021484375,
          "content": "version: \"2\"\nsql:\n  - engine: \"postgresql\"\n    schema: \"sqldb/sqlc/migrations\"\n    queries: \"sqldb/sqlc/queries\"\n    gen:\n      go:\n        out: sqldb/sqlc\n        package: sqlc\n        emit_interface: true\n"
        },
        {
          "name": "sqldb",
          "type": "tree",
          "content": null
        },
        {
          "name": "subrpcserver_config.go",
          "type": "blob",
          "size": 14.00390625,
          "content": "package lnd\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\n\t\"github.com/btcsuite/btcd/chaincfg\"\n\t\"github.com/btcsuite/btclog/v2\"\n\t\"github.com/lightningnetwork/lnd/aliasmgr\"\n\t\"github.com/lightningnetwork/lnd/autopilot\"\n\t\"github.com/lightningnetwork/lnd/chainreg\"\n\t\"github.com/lightningnetwork/lnd/channeldb\"\n\t\"github.com/lightningnetwork/lnd/fn/v2\"\n\tgraphdb \"github.com/lightningnetwork/lnd/graph/db\"\n\t\"github.com/lightningnetwork/lnd/htlcswitch\"\n\t\"github.com/lightningnetwork/lnd/invoices\"\n\t\"github.com/lightningnetwork/lnd/lncfg\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/autopilotrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/chainrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/devrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/invoicesrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/neutrinorpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/peersrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/routerrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/signrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/walletrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/watchtowerrpc\"\n\t\"github.com/lightningnetwork/lnd/lnrpc/wtclientrpc\"\n\t\"github.com/lightningnetwork/lnd/lnwire\"\n\t\"github.com/lightningnetwork/lnd/macaroons\"\n\t\"github.com/lightningnetwork/lnd/netann\"\n\t\"github.com/lightningnetwork/lnd/routing\"\n\t\"github.com/lightningnetwork/lnd/sweep\"\n\t\"github.com/lightningnetwork/lnd/watchtower\"\n\t\"github.com/lightningnetwork/lnd/watchtower/wtclient\"\n\t\"google.golang.org/protobuf/proto\"\n)\n\n// subRPCServerConfigs is special sub-config in the main configuration that\n// houses the configuration for the optional sub-servers. These sub-RPC servers\n// are meant to house experimental new features that may eventually make it\n// into the main RPC server that lnd exposes. Special methods are present on\n// this struct to allow the main RPC server to create and manipulate the\n// sub-RPC servers in a generalized manner.\ntype subRPCServerConfigs struct {\n\t// SignRPC is a sub-RPC server that exposes signing of arbitrary inputs\n\t// as a gRPC service.\n\tSignRPC *signrpc.Config `group:\"signrpc\" namespace:\"signrpc\"`\n\n\t// WalletKitRPC is a sub-RPC server that exposes functionality allowing\n\t// a client to send transactions through a wallet, publish them, and\n\t// also requests keys and addresses under control of the backing\n\t// wallet.\n\tWalletKitRPC *walletrpc.Config `group:\"walletrpc\" namespace:\"walletrpc\"`\n\n\t// AutopilotRPC is a sub-RPC server that exposes methods on the running\n\t// autopilot as a gRPC service.\n\tAutopilotRPC *autopilotrpc.Config `group:\"autopilotrpc\" namespace:\"autopilotrpc\"`\n\n\t// ChainRPC is a sub-RPC server that exposes functionality allowing a\n\t// client to be notified of certain on-chain events (new blocks,\n\t// confirmations, spends).\n\tChainRPC *chainrpc.Config `group:\"chainrpc\" namespace:\"chainrpc\"`\n\n\t// InvoicesRPC is a sub-RPC server that exposes invoice related methods\n\t// as a gRPC service.\n\tInvoicesRPC *invoicesrpc.Config `group:\"invoicesrpc\" namespace:\"invoicesrpc\"`\n\n\t// PeersRPC is a sub-RPC server that exposes peer related methods\n\t// as a gRPC service.\n\tPeersRPC *peersrpc.Config `group:\"peersrpc\" namespace:\"peersrpc\"`\n\n\t// NeutrinoKitRPC is a sub-RPC server that exposes functionality allowing\n\t// a client to interact with a running neutrino node.\n\tNeutrinoKitRPC *neutrinorpc.Config `group:\"neutrinorpc\" namespace:\"neutrinorpc\"`\n\n\t// RouterRPC is a sub-RPC server the exposes functionality that allows\n\t// clients to send payments on the network, and perform Lightning\n\t// payment related queries such as requests for estimates of off-chain\n\t// fees.\n\tRouterRPC *routerrpc.Config `group:\"routerrpc\" namespace:\"routerrpc\"`\n\n\t// WatchtowerRPC is a sub-RPC server that exposes functionality allowing\n\t// clients to monitor and control their embedded watchtower.\n\tWatchtowerRPC *watchtowerrpc.Config `group:\"watchtowerrpc\" namespace:\"watchtowerrpc\"`\n\n\t// WatchtowerClientRPC is a sub-RPC server that exposes functionality\n\t// that allows clients to interact with the active watchtower client\n\t// instance within lnd in order to add, remove, list registered client\n\t// towers, etc.\n\tWatchtowerClientRPC *wtclientrpc.Config `group:\"wtclientrpc\" namespace:\"wtclientrpc\"`\n\n\t// DevRPC is a sub-RPC server that exposes functionality that allows\n\t// developers manipulate LND state that is normally not possible.\n\t// Should only be used for development purposes.\n\tDevRPC *devrpc.Config `group:\"devrpc\" namespace:\"devrpc\"`\n}\n\n// PopulateDependencies attempts to iterate through all the sub-server configs\n// within this struct, and populate the items it requires based on the main\n// configuration file, and the chain control.\n//\n// NOTE: This MUST be called before any callers are permitted to execute the\n// FetchConfig method.\nfunc (s *subRPCServerConfigs) PopulateDependencies(cfg *Config,\n\tcc *chainreg.ChainControl,\n\tnetworkDir string, macService *macaroons.Service,\n\tatpl *autopilot.Manager,\n\tinvoiceRegistry *invoices.InvoiceRegistry,\n\thtlcSwitch *htlcswitch.Switch,\n\tactiveNetParams *chaincfg.Params,\n\tchanRouter *routing.ChannelRouter,\n\trouterBackend *routerrpc.RouterBackend,\n\tnodeSigner *netann.NodeSigner,\n\tgraphDB *graphdb.ChannelGraph,\n\tchanStateDB *channeldb.ChannelStateDB,\n\tsweeper *sweep.UtxoSweeper,\n\ttower *watchtower.Standalone,\n\ttowerClientMgr *wtclient.Manager,\n\ttcpResolver lncfg.TCPResolver,\n\tgenInvoiceFeatures func() *lnwire.FeatureVector,\n\tgenAmpInvoiceFeatures func() *lnwire.FeatureVector,\n\tgetNodeAnnouncement func() lnwire.NodeAnnouncement,\n\tupdateNodeAnnouncement func(features *lnwire.RawFeatureVector,\n\t\tmodifiers ...netann.NodeAnnModifier) error,\n\tparseAddr func(addr string) (net.Addr, error),\n\trpcLogger btclog.Logger, aliasMgr *aliasmgr.Manager,\n\tauxDataParser fn.Option[AuxDataParser],\n\tinvoiceHtlcModifier *invoices.HtlcModificationInterceptor) error {\n\n\t// First, we'll use reflect to obtain a version of the config struct\n\t// that allows us to programmatically inspect its fields.\n\tselfVal := extractReflectValue(s)\n\tselfType := selfVal.Type()\n\n\tnumFields := selfVal.NumField()\n\tfor i := 0; i < numFields; i++ {\n\t\tfield := selfVal.Field(i)\n\t\tfieldElem := field.Elem()\n\t\tfieldName := selfType.Field(i).Name\n\n\t\tltndLog.Debugf(\"Populating dependencies for sub RPC \"+\n\t\t\t\"server: %v\", fieldName)\n\n\t\t// If this sub-config doesn't actually have any fields, then we\n\t\t// can skip it, as the build tag for it is likely off.\n\t\tif fieldElem.NumField() == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tif !fieldElem.CanSet() {\n\t\t\tcontinue\n\t\t}\n\n\t\tswitch subCfg := field.Interface().(type) {\n\t\tcase *signrpc.Config:\n\t\t\tsubCfgValue := extractReflectValue(subCfg)\n\n\t\t\tsubCfgValue.FieldByName(\"MacService\").Set(\n\t\t\t\treflect.ValueOf(macService),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"NetworkDir\").Set(\n\t\t\t\treflect.ValueOf(networkDir),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"Signer\").Set(\n\t\t\t\treflect.ValueOf(cc.Signer),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"KeyRing\").Set(\n\t\t\t\treflect.ValueOf(cc.KeyRing),\n\t\t\t)\n\n\t\tcase *walletrpc.Config:\n\t\t\tsubCfgValue := extractReflectValue(subCfg)\n\n\t\t\tsubCfgValue.FieldByName(\"NetworkDir\").Set(\n\t\t\t\treflect.ValueOf(networkDir),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"MacService\").Set(\n\t\t\t\treflect.ValueOf(macService),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"FeeEstimator\").Set(\n\t\t\t\treflect.ValueOf(cc.FeeEstimator),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"Wallet\").Set(\n\t\t\t\treflect.ValueOf(cc.Wallet),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"CoinSelectionLocker\").Set(\n\t\t\t\treflect.ValueOf(cc.Wallet),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"KeyRing\").Set(\n\t\t\t\treflect.ValueOf(cc.KeyRing),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"Sweeper\").Set(\n\t\t\t\treflect.ValueOf(sweeper),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"Chain\").Set(\n\t\t\t\treflect.ValueOf(cc.ChainIO),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"ChainParams\").Set(\n\t\t\t\treflect.ValueOf(activeNetParams),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"CurrentNumAnchorChans\").Set(\n\t\t\t\treflect.ValueOf(cc.Wallet.CurrentNumAnchorChans),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"CoinSelectionStrategy\").Set(\n\t\t\t\treflect.ValueOf(\n\t\t\t\t\tcc.Wallet.Cfg.CoinSelectionStrategy,\n\t\t\t\t),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"ChanStateDB\").Set(\n\t\t\t\treflect.ValueOf(chanStateDB),\n\t\t\t)\n\n\t\tcase *autopilotrpc.Config:\n\t\t\tsubCfgValue := extractReflectValue(subCfg)\n\n\t\t\tsubCfgValue.FieldByName(\"Manager\").Set(\n\t\t\t\treflect.ValueOf(atpl),\n\t\t\t)\n\n\t\tcase *chainrpc.Config:\n\t\t\tsubCfgValue := extractReflectValue(subCfg)\n\n\t\t\tsubCfgValue.FieldByName(\"NetworkDir\").Set(\n\t\t\t\treflect.ValueOf(networkDir),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"MacService\").Set(\n\t\t\t\treflect.ValueOf(macService),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"ChainNotifier\").Set(\n\t\t\t\treflect.ValueOf(cc.ChainNotifier),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"Chain\").Set(\n\t\t\t\treflect.ValueOf(cc.ChainIO),\n\t\t\t)\n\n\t\tcase *invoicesrpc.Config:\n\t\t\tsubCfgValue := extractReflectValue(subCfg)\n\n\t\t\tsubCfgValue.FieldByName(\"NetworkDir\").Set(\n\t\t\t\treflect.ValueOf(networkDir),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"MacService\").Set(\n\t\t\t\treflect.ValueOf(macService),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"InvoiceRegistry\").Set(\n\t\t\t\treflect.ValueOf(invoiceRegistry),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"HtlcModifier\").Set(\n\t\t\t\treflect.ValueOf(invoiceHtlcModifier),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"IsChannelActive\").Set(\n\t\t\t\treflect.ValueOf(htlcSwitch.HasActiveLink),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"ChainParams\").Set(\n\t\t\t\treflect.ValueOf(activeNetParams),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"NodeSigner\").Set(\n\t\t\t\treflect.ValueOf(nodeSigner),\n\t\t\t)\n\t\t\tdefaultDelta := cfg.Bitcoin.TimeLockDelta\n\t\t\tsubCfgValue.FieldByName(\"DefaultCLTVExpiry\").Set(\n\t\t\t\treflect.ValueOf(defaultDelta),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"GraphDB\").Set(\n\t\t\t\treflect.ValueOf(graphDB),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"ChanStateDB\").Set(\n\t\t\t\treflect.ValueOf(chanStateDB),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"GenInvoiceFeatures\").Set(\n\t\t\t\treflect.ValueOf(genInvoiceFeatures),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"GenAmpInvoiceFeatures\").Set(\n\t\t\t\treflect.ValueOf(genAmpInvoiceFeatures),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"GetAlias\").Set(\n\t\t\t\treflect.ValueOf(aliasMgr.GetPeerAlias),\n\t\t\t)\n\n\t\t\tparseAuxData := func(m proto.Message) error {\n\t\t\t\treturn fn.MapOptionZ(\n\t\t\t\t\tauxDataParser,\n\t\t\t\t\tfunc(p AuxDataParser) error {\n\t\t\t\t\t\treturn p.InlineParseCustomData(\n\t\t\t\t\t\t\tm,\n\t\t\t\t\t\t)\n\t\t\t\t\t},\n\t\t\t\t)\n\t\t\t}\n\t\t\tsubCfgValue.FieldByName(\"ParseAuxData\").Set(\n\t\t\t\treflect.ValueOf(parseAuxData),\n\t\t\t)\n\n\t\tcase *neutrinorpc.Config:\n\t\t\tsubCfgValue := extractReflectValue(subCfg)\n\n\t\t\tsubCfgValue.FieldByName(\"NeutrinoCS\").Set(\n\t\t\t\treflect.ValueOf(cc.Cfg.NeutrinoCS),\n\t\t\t)\n\n\t\t// RouterRPC isn't conditionally compiled and doesn't need to be\n\t\t// populated using reflection.\n\t\tcase *routerrpc.Config:\n\t\t\tsubCfgValue := extractReflectValue(subCfg)\n\n\t\t\tsubCfgValue.FieldByName(\"AliasMgr\").Set(\n\t\t\t\treflect.ValueOf(aliasMgr),\n\t\t\t)\n\n\t\tcase *watchtowerrpc.Config:\n\t\t\tsubCfgValue := extractReflectValue(subCfg)\n\n\t\t\tsubCfgValue.FieldByName(\"Active\").Set(\n\t\t\t\treflect.ValueOf(tower != nil),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"Tower\").Set(\n\t\t\t\treflect.ValueOf(tower),\n\t\t\t)\n\n\t\tcase *wtclientrpc.Config:\n\t\t\tsubCfgValue := extractReflectValue(subCfg)\n\n\t\t\tif towerClientMgr != nil {\n\t\t\t\tsubCfgValue.FieldByName(\"Active\").Set(\n\t\t\t\t\treflect.ValueOf(towerClientMgr != nil),\n\t\t\t\t)\n\t\t\t\tsubCfgValue.FieldByName(\"ClientMgr\").Set(\n\t\t\t\t\treflect.ValueOf(towerClientMgr),\n\t\t\t\t)\n\t\t\t}\n\t\t\tsubCfgValue.FieldByName(\"Resolver\").Set(\n\t\t\t\treflect.ValueOf(tcpResolver),\n\t\t\t)\n\t\t\tsubCfgValue.FieldByName(\"Log\").Set(\n\t\t\t\treflect.ValueOf(rpcLogger),\n\t\t\t)\n\n\t\tcase *devrpc.Config:\n\t\t\tsubCfgValue := extractReflectValue(subCfg)\n\n\t\t\tsubCfgValue.FieldByName(\"ActiveNetParams\").Set(\n\t\t\t\treflect.ValueOf(activeNetParams),\n\t\t\t)\n\n\t\t\tsubCfgValue.FieldByName(\"GraphDB\").Set(\n\t\t\t\treflect.ValueOf(graphDB),\n\t\t\t)\n\n\t\t\tsubCfgValue.FieldByName(\"Switch\").Set(\n\t\t\t\treflect.ValueOf(htlcSwitch),\n\t\t\t)\n\n\t\tcase *peersrpc.Config:\n\t\t\tsubCfgValue := extractReflectValue(subCfg)\n\n\t\t\tsubCfgValue.FieldByName(\"GetNodeAnnouncement\").Set(\n\t\t\t\treflect.ValueOf(getNodeAnnouncement),\n\t\t\t)\n\n\t\t\tsubCfgValue.FieldByName(\"ParseAddr\").Set(\n\t\t\t\treflect.ValueOf(parseAddr),\n\t\t\t)\n\n\t\t\tsubCfgValue.FieldByName(\"UpdateNodeAnnouncement\").Set(\n\t\t\t\treflect.ValueOf(updateNodeAnnouncement),\n\t\t\t)\n\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"unknown field: %v, %T\", fieldName,\n\t\t\t\tcfg)\n\t\t}\n\t}\n\n\t// Populate routerrpc dependencies.\n\ts.RouterRPC.NetworkDir = networkDir\n\ts.RouterRPC.MacService = macService\n\ts.RouterRPC.Router = chanRouter\n\ts.RouterRPC.RouterBackend = routerBackend\n\n\treturn nil\n}\n\n// FetchConfig attempts to locate an existing configuration file mapped to the\n// target sub-server. If we're unable to find a config file matching the\n// subServerName name, then false will be returned for the second parameter.\n//\n// NOTE: Part of the lnrpc.SubServerConfigDispatcher interface.\nfunc (s *subRPCServerConfigs) FetchConfig(subServerName string) (interface{}, bool) {\n\t// First, we'll use reflect to obtain a version of the config struct\n\t// that allows us to programmatically inspect its fields.\n\tselfVal := extractReflectValue(s)\n\n\t// Now that we have the value of the struct, we can check to see if it\n\t// has an attribute with the same name as the subServerName.\n\tconfigVal := selfVal.FieldByName(subServerName)\n\n\t// We'll now ensure that this field actually exists in this value. If\n\t// not, then we'll return false for the ok value to indicate to the\n\t// caller that this field doesn't actually exist.\n\tif !configVal.IsValid() {\n\t\treturn nil, false\n\t}\n\n\tconfigValElem := configVal.Elem()\n\n\t// If a config of this type is found, it doesn't have any fields, then\n\t// it's the same as if it wasn't present. This can happen if the build\n\t// tag for the sub-server is inactive.\n\tif configValElem.NumField() == 0 {\n\t\treturn nil, false\n\t}\n\n\t// At this pint, we know that the field is actually present in the\n\t// config struct, so we can return it directly.\n\treturn configVal.Interface(), true\n}\n\n// extractReflectValue attempts to extract the value from an interface using\n// the reflect package. The resulting reflect.Value allows the caller to\n// programmatically examine and manipulate the underlying value.\nfunc extractReflectValue(instance interface{}) reflect.Value {\n\tvar val reflect.Value\n\n\t// If the type of the instance is a pointer, then we need to deference\n\t// the pointer one level to get its value. Otherwise, we can access the\n\t// value directly.\n\tif reflect.TypeOf(instance).Kind() == reflect.Ptr {\n\t\tval = reflect.ValueOf(instance).Elem()\n\t} else {\n\t\tval = reflect.ValueOf(instance)\n\t}\n\n\treturn val\n}\n"
        },
        {
          "name": "subscribe",
          "type": "tree",
          "content": null
        },
        {
          "name": "sweep",
          "type": "tree",
          "content": null
        },
        {
          "name": "sweeper_wallet.go",
          "type": "blob",
          "size": 0.814453125,
          "content": "package lnd\n\nimport (\n\t\"github.com/btcsuite/btcd/chaincfg/chainhash\"\n\t\"github.com/lightningnetwork/lnd/lnwallet\"\n)\n\n// sweeperWallet is a wrapper around the LightningWallet that implements the\n// sweeper's Wallet interface.\ntype sweeperWallet struct {\n\t*lnwallet.LightningWallet\n}\n\n// newSweeperWallet creates a new sweeper wallet from the given\n// LightningWallet.\nfunc newSweeperWallet(w *lnwallet.LightningWallet) *sweeperWallet {\n\treturn &sweeperWallet{\n\t\tLightningWallet: w,\n\t}\n}\n\n// CancelRebroadcast cancels the rebroadcast of the given transaction.\nfunc (s *sweeperWallet) CancelRebroadcast(txid chainhash.Hash) {\n\t// For neutrino, we don't config the rebroadcaster for the wallet as it\n\t// manages the rebroadcasting logic in neutrino itself.\n\tif s.Cfg.Rebroadcaster != nil {\n\t\ts.Cfg.Rebroadcaster.MarkAsConfirmed(txid)\n\t}\n}\n"
        },
        {
          "name": "ticker",
          "type": "tree",
          "content": null
        },
        {
          "name": "tls_manager.go",
          "type": "blob",
          "size": 17.20703125,
          "content": "package lnd\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"net/http\"\n\t\"os\"\n\t\"time\"\n\n\t\"github.com/lightningnetwork/lnd/cert\"\n\t\"github.com/lightningnetwork/lnd/keychain\"\n\t\"github.com/lightningnetwork/lnd/lncfg\"\n\t\"github.com/lightningnetwork/lnd/lnencrypt\"\n\t\"github.com/lightningnetwork/lnd/lnrpc\"\n\t\"golang.org/x/crypto/acme/autocert\"\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/credentials\"\n)\n\nconst (\n\t// modifyFilePermissons is the file permission used for writing\n\t// encrypted tls files.\n\tmodifyFilePermissions = 0600\n\n\t// validityHours is the number of hours the ephemeral tls certificate\n\t// will be valid, if encrypting tls certificates is turned on.\n\tvalidityHours = 24\n)\n\nvar (\n\t// privateKeyPrefix is the prefix to a plaintext TLS key.\n\t// It should match these two key formats:\n\t// - `-----BEGIN PRIVATE KEY-----`    (PKCS8).\n\t// - `-----BEGIN EC PRIVATE KEY-----` (SEC1/rfc5915, the legacy format).\n\tprivateKeyPrefix = []byte(\"-----BEGIN \")\n)\n\n// TLSManagerCfg houses a set of values and methods that is passed to the\n// TLSManager for it to properly manage LND's TLS options.\ntype TLSManagerCfg struct {\n\tTLSCertPath        string\n\tTLSKeyPath         string\n\tTLSEncryptKey      bool\n\tTLSExtraIPs        []string\n\tTLSExtraDomains    []string\n\tTLSAutoRefresh     bool\n\tTLSDisableAutofill bool\n\tTLSCertDuration    time.Duration\n\n\tLetsEncryptDir    string\n\tLetsEncryptDomain string\n\tLetsEncryptListen string\n\n\tDisableRestTLS bool\n\n\tHTTPHeaderTimeout time.Duration\n}\n\n// TLSManager generates/renews a TLS cert/key pair when needed. When required,\n// it encrypts the TLS key. It also returns the certificate configuration\n// options needed for gRPC and REST.\ntype TLSManager struct {\n\tcfg *TLSManagerCfg\n\n\t// tlsReloader is able to reload the certificate with the\n\t// GetCertificate function. In getConfig, tlsCfg.GetCertificate is\n\t// pointed towards t.tlsReloader.GetCertificateFunc(). When\n\t// TLSReloader's AttemptReload is called, the cert that tlsReloader\n\t// holds is changed, in turn changing the cert data\n\t// tlsCfg.GetCertificate will return.\n\ttlsReloader *cert.TLSReloader\n\n\t// These options are only used if we're currently using an ephemeral\n\t// TLS certificate, used when we're encrypting the TLS key.\n\tephemeralKey      []byte\n\tephemeralCert     []byte\n\tephemeralCertPath string\n}\n\n// NewTLSManager returns a reference to a new TLSManager.\nfunc NewTLSManager(cfg *TLSManagerCfg) *TLSManager {\n\treturn &TLSManager{\n\t\tcfg: cfg,\n\t}\n}\n\n// getConfig returns a TLS configuration for the gRPC server and credentials\n// and a proxy destination for the REST reverse proxy.\nfunc (t *TLSManager) getConfig() ([]grpc.ServerOption, []grpc.DialOption,\n\tfunc(net.Addr) (net.Listener, error), func(), error) {\n\n\tvar (\n\t\tkeyBytes, certBytes []byte\n\t\terr                 error\n\t)\n\tif t.ephemeralKey != nil {\n\t\tkeyBytes = t.ephemeralKey\n\t\tcertBytes = t.ephemeralCert\n\t} else {\n\t\tcertBytes, keyBytes, err = cert.GetCertBytesFromPath(\n\t\t\tt.cfg.TLSCertPath, t.cfg.TLSKeyPath,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, nil, nil, nil, err\n\t\t}\n\t}\n\n\tcertData, _, err := cert.LoadCertFromBytes(certBytes, keyBytes)\n\tif err != nil {\n\t\treturn nil, nil, nil, nil, err\n\t}\n\n\tif t.tlsReloader == nil {\n\t\ttlsr, err := cert.NewTLSReloader(certBytes, keyBytes)\n\t\tif err != nil {\n\t\t\treturn nil, nil, nil, nil, err\n\t\t}\n\t\tt.tlsReloader = tlsr\n\t}\n\n\ttlsCfg := cert.TLSConfFromCert(certData)\n\ttlsCfg.GetCertificate = t.tlsReloader.GetCertificateFunc()\n\n\t// If Let's Encrypt is enabled, we need to set up the autocert manager\n\t// and override the TLS config's GetCertificate function.\n\tcleanUp := t.setUpLetsEncrypt(&certData, tlsCfg)\n\n\t// Now that we know that we have a certificate, let's generate the\n\t// required config options.\n\tserverCreds := credentials.NewTLS(tlsCfg)\n\tserverOpts := []grpc.ServerOption{grpc.Creds(serverCreds)}\n\n\t// For our REST dial options, we skip TLS verification, and we also\n\t// increase the max message size that we'll decode to allow clients to\n\t// hit endpoints which return more data such as the DescribeGraph call.\n\t// We set this to 200MiB atm. Should be the same value as maxMsgRecvSize\n\t// in cmd/lncli/main.go.\n\trestDialOpts := []grpc.DialOption{\n\t\t// We are forwarding the requests directly to the address of our\n\t\t// own local listener. To not need to mess with the TLS\n\t\t// certificate (which might be tricky if we're using Let's\n\t\t// Encrypt or if the ephemeral tls cert is being used), we just\n\t\t// skip the certificate verification. Injecting a malicious\n\t\t// hostname into the listener address will result in an error\n\t\t// on startup so this should be quite safe.\n\t\tgrpc.WithTransportCredentials(credentials.NewTLS(\n\t\t\t&tls.Config{InsecureSkipVerify: true},\n\t\t)),\n\t\tgrpc.WithDefaultCallOptions(\n\t\t\tgrpc.MaxCallRecvMsgSize(lnrpc.MaxGrpcMsgSize),\n\t\t),\n\t}\n\n\t// Return a function closure that can be used to listen on a given\n\t// address with the current TLS config.\n\trestListen := func(addr net.Addr) (net.Listener, error) {\n\t\t// For restListen we will call ListenOnAddress if TLS is\n\t\t// disabled.\n\t\tif t.cfg.DisableRestTLS {\n\t\t\treturn lncfg.ListenOnAddress(addr)\n\t\t}\n\n\t\treturn lncfg.TLSListenOnAddress(addr, tlsCfg)\n\t}\n\n\treturn serverOpts, restDialOpts, restListen, cleanUp, nil\n}\n\n// generateOrRenewCert generates a new TLS certificate if we're not using one\n// yet or renews it if it's outdated.\nfunc (t *TLSManager) generateOrRenewCert() (*tls.Config, error) {\n\t// Generete a TLS pair if we don't have one yet.\n\tvar emptyKeyRing keychain.SecretKeyRing\n\terr := t.generateCertPair(emptyKeyRing)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcertData, parsedCert, err := cert.LoadCert(\n\t\tt.cfg.TLSCertPath, t.cfg.TLSKeyPath,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Check to see if the certificate needs to be renewed. If it does, we\n\t// return the newly generated certificate data instead.\n\treloadedCertData, err := t.maintainCert(parsedCert)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif reloadedCertData != nil {\n\t\tcertData = *reloadedCertData\n\t}\n\n\ttlsCfg := cert.TLSConfFromCert(certData)\n\n\treturn tlsCfg, nil\n}\n\n// generateCertPair creates and writes a TLS pair to disk if the pair\n// doesn't exist yet. If the TLSEncryptKey setting is on, and a plaintext key\n// is already written to disk, this function overwrites the plaintext key with\n// the encrypted form.\nfunc (t *TLSManager) generateCertPair(keyRing keychain.SecretKeyRing) error {\n\t// Ensure we create TLS key and certificate if they don't exist.\n\tif lnrpc.FileExists(t.cfg.TLSCertPath) ||\n\t\tlnrpc.FileExists(t.cfg.TLSKeyPath) {\n\n\t\t// Handle discrepencies related to the TLSEncryptKey setting.\n\t\treturn t.ensureEncryption(keyRing)\n\t}\n\n\trpcsLog.Infof(\"Generating TLS certificates...\")\n\tcertBytes, keyBytes, err := cert.GenCertPair(\n\t\t\"lnd autogenerated cert\", t.cfg.TLSExtraIPs,\n\t\tt.cfg.TLSExtraDomains, t.cfg.TLSDisableAutofill,\n\t\tt.cfg.TLSCertDuration,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif t.cfg.TLSEncryptKey {\n\t\tvar b bytes.Buffer\n\t\te, err := lnencrypt.KeyRingEncrypter(keyRing)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to create \"+\n\t\t\t\t\"encrypt key %v\", err)\n\t\t}\n\n\t\terr = e.EncryptPayloadToWriter(\n\t\t\tkeyBytes, &b,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tkeyBytes = b.Bytes()\n\t}\n\n\terr = cert.WriteCertPair(\n\t\tt.cfg.TLSCertPath, t.cfg.TLSKeyPath, certBytes, keyBytes,\n\t)\n\n\trpcsLog.Infof(\"Done generating TLS certificates\")\n\n\treturn err\n}\n\n// ensureEncryption takes a look at a couple of things:\n// 1) If the TLS key is in plaintext, but TLSEncryptKey is set, we need to\n// encrypt the file and rewrite it to disk.\n// 2) On the flip side, if TLSEncryptKey is not set, but the key on disk\n// is encrypted, we need to error out and warn the user.\nfunc (t *TLSManager) ensureEncryption(keyRing keychain.SecretKeyRing) error {\n\t_, keyBytes, err := cert.GetCertBytesFromPath(\n\t\tt.cfg.TLSCertPath, t.cfg.TLSKeyPath,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif t.cfg.TLSEncryptKey && bytes.HasPrefix(keyBytes, privateKeyPrefix) {\n\t\tvar b bytes.Buffer\n\t\te, err := lnencrypt.KeyRingEncrypter(keyRing)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to generate encrypt key %w\",\n\t\t\t\terr)\n\t\t}\n\n\t\terr = e.EncryptPayloadToWriter(keyBytes, &b)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\terr = os.WriteFile(\n\t\t\tt.cfg.TLSKeyPath, b.Bytes(), modifyFilePermissions,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// If the private key is encrypted but the user didn't pass\n\t// --tlsencryptkey we error out. This is because the wallet is not\n\t// unlocked yet and we don't have access to the keys yet for decryption.\n\tif !t.cfg.TLSEncryptKey && !bytes.HasPrefix(keyBytes,\n\t\tprivateKeyPrefix) {\n\n\t\tltndLog.Errorf(\"The TLS private key is encrypted on disk.\")\n\n\t\treturn errors.New(\"the TLS key is encrypted but the \" +\n\t\t\t\"--tlsencryptkey flag is not passed. Please either \" +\n\t\t\t\"restart lnd with the --tlsencryptkey flag or delete \" +\n\t\t\t\"the TLS files for regeneration\")\n\t}\n\n\treturn nil\n}\n\n// decryptTLSKeyBytes decrypts the TLS key.\nfunc decryptTLSKeyBytes(keyRing keychain.SecretKeyRing,\n\tencryptedData []byte) ([]byte, error) {\n\n\treader := bytes.NewReader(encryptedData)\n\tencrypter, err := lnencrypt.KeyRingEncrypter(keyRing)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tplaintext, err := encrypter.DecryptPayloadFromReader(\n\t\treader,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn plaintext, nil\n}\n\n// maintainCert checks if the certificate IP and domains matches the config,\n// and renews the certificate if either this data is outdated or the\n// certificate is expired.\nfunc (t *TLSManager) maintainCert(\n\tparsedCert *x509.Certificate) (*tls.Certificate, error) {\n\n\t// We check whether the certificate we have on disk match the IPs and\n\t// domains specified by the config. If the extra IPs or domains have\n\t// changed from when the certificate was created, we will refresh the\n\t// certificate if auto refresh is active.\n\trefresh := false\n\tvar err error\n\tif t.cfg.TLSAutoRefresh {\n\t\trefresh, err = cert.IsOutdated(\n\t\t\tparsedCert, t.cfg.TLSExtraIPs,\n\t\t\tt.cfg.TLSExtraDomains, t.cfg.TLSDisableAutofill,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// If the certificate expired or it was outdated, delete it and the TLS\n\t// key and generate a new pair.\n\tif !time.Now().After(parsedCert.NotAfter) && !refresh {\n\t\treturn nil, nil\n\t}\n\n\tltndLog.Info(\"TLS certificate is expired or outdated, \" +\n\t\t\"generating a new one\")\n\n\terr = os.Remove(t.cfg.TLSCertPath)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\terr = os.Remove(t.cfg.TLSKeyPath)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trpcsLog.Infof(\"Renewing TLS certificates...\")\n\tcertBytes, keyBytes, err := cert.GenCertPair(\n\t\t\"lnd autogenerated cert\", t.cfg.TLSExtraIPs,\n\t\tt.cfg.TLSExtraDomains, t.cfg.TLSDisableAutofill,\n\t\tt.cfg.TLSCertDuration,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\terr = cert.WriteCertPair(\n\t\tt.cfg.TLSCertPath, t.cfg.TLSKeyPath, certBytes, keyBytes,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trpcsLog.Infof(\"Done renewing TLS certificates\")\n\n\t// Reload the certificate data.\n\treloadedCertData, _, err := cert.LoadCert(\n\t\tt.cfg.TLSCertPath, t.cfg.TLSKeyPath,\n\t)\n\n\treturn &reloadedCertData, err\n}\n\n// setUpLetsEncrypt automatically generates a Let's Encrypt certificate if the\n// option is set.\nfunc (t *TLSManager) setUpLetsEncrypt(certData *tls.Certificate,\n\ttlsCfg *tls.Config) func() {\n\n\t// If Let's Encrypt is enabled, instantiate autocert to request/renew\n\t// the certificates.\n\tcleanUp := func() {}\n\tif t.cfg.LetsEncryptDomain == \"\" {\n\t\treturn cleanUp\n\t}\n\n\tltndLog.Infof(\"Using Let's Encrypt certificate for domain %v\",\n\t\tt.cfg.LetsEncryptDomain)\n\n\tmanager := autocert.Manager{\n\t\tCache:  autocert.DirCache(t.cfg.LetsEncryptDir),\n\t\tPrompt: autocert.AcceptTOS,\n\t\tHostPolicy: autocert.HostWhitelist(\n\t\t\tt.cfg.LetsEncryptDomain,\n\t\t),\n\t}\n\n\tsrv := &http.Server{\n\t\tAddr:              t.cfg.LetsEncryptListen,\n\t\tHandler:           manager.HTTPHandler(nil),\n\t\tReadHeaderTimeout: t.cfg.HTTPHeaderTimeout,\n\t}\n\tshutdownCompleted := make(chan struct{})\n\tcleanUp = func() {\n\t\terr := srv.Shutdown(context.Background())\n\t\tif err != nil {\n\t\t\tltndLog.Errorf(\"Autocert listener shutdown \"+\n\t\t\t\t\" error: %v\", err)\n\n\t\t\treturn\n\t\t}\n\t\t<-shutdownCompleted\n\t\tltndLog.Infof(\"Autocert challenge listener stopped\")\n\t}\n\n\tgo func() {\n\t\tltndLog.Infof(\"Autocert challenge listener started \"+\n\t\t\t\"at %v\", t.cfg.LetsEncryptListen)\n\n\t\terr := srv.ListenAndServe()\n\t\tif err != http.ErrServerClosed {\n\t\t\tltndLog.Errorf(\"autocert http: %v\", err)\n\t\t}\n\t\tclose(shutdownCompleted)\n\t}()\n\n\tgetCertificate := func(h *tls.ClientHelloInfo) (\n\t\t*tls.Certificate, error) {\n\n\t\tlecert, err := manager.GetCertificate(h)\n\t\tif err != nil {\n\t\t\tltndLog.Errorf(\"GetCertificate: %v\", err)\n\t\t\treturn certData, nil\n\t\t}\n\n\t\treturn lecert, err\n\t}\n\n\t// The self-signed tls.cert remains available as fallback.\n\ttlsCfg.GetCertificate = getCertificate\n\n\treturn cleanUp\n}\n\n// SetCertificateBeforeUnlock takes care of loading the certificate before\n// the wallet is unlocked. If the TLSEncryptKey setting is on, we need to\n// generate an ephemeral certificate we're able to use until the wallet is\n// unlocked and a new TLS pair can be encrypted to disk. Otherwise we can\n// process the certificate normally.\nfunc (t *TLSManager) SetCertificateBeforeUnlock() ([]grpc.ServerOption,\n\t[]grpc.DialOption, func(net.Addr) (net.Listener, error), func(),\n\terror) {\n\n\tif t.cfg.TLSEncryptKey {\n\t\t_, err := t.loadEphemeralCertificate()\n\t\tif err != nil {\n\t\t\treturn nil, nil, nil, nil, fmt.Errorf(\"unable to load \"+\n\t\t\t\t\"ephemeral certificate: %v\", err)\n\t\t}\n\t} else {\n\t\t_, err := t.generateOrRenewCert()\n\t\tif err != nil {\n\t\t\treturn nil, nil, nil, nil, fmt.Errorf(\"unable to \"+\n\t\t\t\t\"generate or renew TLS certificate: %v\", err)\n\t\t}\n\t}\n\n\tserverOpts, restDialOpts, restListen, cleanUp, err := t.getConfig()\n\tif err != nil {\n\t\treturn nil, nil, nil, nil, fmt.Errorf(\"unable to load TLS \"+\n\t\t\t\"credentials: %v\", err)\n\t}\n\n\treturn serverOpts, restDialOpts, restListen, cleanUp, nil\n}\n\n// loadEphemeralCertificate creates and loads the ephemeral certificate which\n// is used temporarily for secure communications before the wallet is unlocked.\nfunc (t *TLSManager) loadEphemeralCertificate() ([]byte, error) {\n\trpcsLog.Infof(\"Generating ephemeral TLS certificates...\")\n\n\ttmpValidity := validityHours * time.Hour\n\t// Append .tmp to the end of the cert for differentiation.\n\ttmpCertPath := t.cfg.TLSCertPath + \".tmp\"\n\n\t// Pass in a blank string for the key path so the\n\t// function doesn't write them to disk.\n\tcertBytes, keyBytes, err := cert.GenCertPair(\n\t\t\"lnd ephemeral autogenerated cert\", t.cfg.TLSExtraIPs,\n\t\tt.cfg.TLSExtraDomains, t.cfg.TLSDisableAutofill, tmpValidity,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tt.setEphemeralSettings(keyBytes, certBytes)\n\n\terr = cert.WriteCertPair(tmpCertPath, \"\", certBytes, keyBytes)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trpcsLog.Infof(\"Done generating ephemeral TLS certificates\")\n\n\treturn keyBytes, nil\n}\n\n// LoadPermanentCertificate deletes the ephemeral certificate file and\n// generates a new one with the real keyring.\nfunc (t *TLSManager) LoadPermanentCertificate(\n\tkeyRing keychain.SecretKeyRing) error {\n\n\tif !t.cfg.TLSEncryptKey {\n\t\treturn nil\n\t}\n\n\ttmpCertPath := t.cfg.TLSCertPath + \".tmp\"\n\terr := os.Remove(tmpCertPath)\n\tif err != nil {\n\t\tltndLog.Warn(\"Unable to delete temp cert at %v\",\n\t\t\ttmpCertPath)\n\t}\n\n\terr = t.generateCertPair(keyRing)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tcertBytes, encryptedKeyBytes, err := cert.GetCertBytesFromPath(\n\t\tt.cfg.TLSCertPath, t.cfg.TLSKeyPath,\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treader := bytes.NewReader(encryptedKeyBytes)\n\te, err := lnencrypt.KeyRingEncrypter(keyRing)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to generate encrypt key %w\",\n\t\t\terr)\n\t}\n\n\tkeyBytes, err := e.DecryptPayloadFromReader(reader)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Switch the server's TLS certificate to the persistent one. By\n\t// changing the cert data the TLSReloader points to,\n\terr = t.tlsReloader.AttemptReload(certBytes, keyBytes)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tt.deleteEphemeralSettings()\n\n\treturn nil\n}\n\n// setEphemeralSettings sets the TLSManager settings needed when an ephemeral\n// certificate is created.\nfunc (t *TLSManager) setEphemeralSettings(keyBytes, certBytes []byte) {\n\tt.ephemeralKey = keyBytes\n\tt.ephemeralCert = certBytes\n\tt.ephemeralCertPath = t.cfg.TLSCertPath + \".tmp\"\n}\n\n// deleteEphemeralSettings deletes the TLSManager ephemeral settings that are\n// no longer needed when the ephemeral certificate is deleted so the Manager\n// knows we're no longer using it.\nfunc (t *TLSManager) deleteEphemeralSettings() {\n\tt.ephemeralKey = nil\n\tt.ephemeralCert = nil\n\tt.ephemeralCertPath = \"\"\n}\n\n// IsCertExpired checks if the current TLS certificate is expired.\nfunc (t *TLSManager) IsCertExpired(keyRing keychain.SecretKeyRing) (bool,\n\ttime.Time, error) {\n\n\tcertBytes, keyBytes, err := cert.GetCertBytesFromPath(\n\t\tt.cfg.TLSCertPath, t.cfg.TLSKeyPath,\n\t)\n\tif err != nil {\n\t\treturn false, time.Time{}, err\n\t}\n\n\t// If TLSEncryptKey is set, there are two states the\n\t// certificate can be in: ephemeral or permanent.\n\t// Retrieve the key depending on which state it is in.\n\tif t.ephemeralKey != nil {\n\t\tkeyBytes = t.ephemeralKey\n\t} else if t.cfg.TLSEncryptKey {\n\t\tkeyBytes, err = decryptTLSKeyBytes(keyRing, keyBytes)\n\t\tif err != nil {\n\t\t\treturn false, time.Time{}, err\n\t\t}\n\t}\n\n\t_, parsedCert, err := cert.LoadCertFromBytes(\n\t\tcertBytes, keyBytes,\n\t)\n\tif err != nil {\n\t\treturn false, time.Time{}, err\n\t}\n\n\t// If the current time is passed the certificate's\n\t// expiry time, then it is considered expired\n\tif time.Now().After(parsedCert.NotAfter) {\n\t\treturn true, parsedCert.NotAfter, nil\n\t}\n\n\treturn false, parsedCert.NotAfter, nil\n}\n"
        },
        {
          "name": "tls_manager_test.go",
          "type": "blob",
          "size": 10.99609375,
          "content": "package lnd\n\nimport (\n\t\"bytes\"\n\t\"crypto/ecdsa\"\n\t\"crypto/elliptic\"\n\t\"crypto/rand\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"crypto/x509/pkix\"\n\t\"encoding/pem\"\n\t\"math/big\"\n\t\"net\"\n\t\"os\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/btcsuite/btcd/btcec/v2\"\n\t\"github.com/lightningnetwork/lnd/cert\"\n\t\"github.com/lightningnetwork/lnd/keychain\"\n\t\"github.com/lightningnetwork/lnd/lnencrypt\"\n\t\"github.com/lightningnetwork/lnd/lntest/channels\"\n\t\"github.com/lightningnetwork/lnd/lntest/mock\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nconst (\n\ttestTLSCertDuration = 42 * time.Hour\n)\n\nvar (\n\tprivKeyBytes = channels.AlicesPrivKey\n\n\tprivKey, _ = btcec.PrivKeyFromBytes(privKeyBytes)\n)\n\n// TestGenerateOrRenewCert creates an expired TLS certificate, to test that a\n// new TLS certificate pair is regenerated when the old pair expires. This is\n// necessary because the pair expires after a little over a year.\nfunc TestGenerateOrRenewCert(t *testing.T) {\n\tt.Parallel()\n\n\t// Write an expired certificate to disk.\n\tcertPath, keyPath, expiredCert := writeTestCertFiles(\n\t\tt, true, false, nil,\n\t)\n\n\t// Now let's run the TLSManager's getConfig. If it works properly, it\n\t// should delete the cert and create a new one.\n\tcfg := &TLSManagerCfg{\n\t\tTLSCertPath:     certPath,\n\t\tTLSKeyPath:      keyPath,\n\t\tTLSCertDuration: testTLSCertDuration,\n\t}\n\ttlsManager := NewTLSManager(cfg)\n\t_, err := tlsManager.generateOrRenewCert()\n\trequire.NoError(t, err)\n\t_, _, _, cleanUp, err := tlsManager.getConfig()\n\trequire.NoError(t, err, \"couldn't retrieve TLS config\")\n\tt.Cleanup(cleanUp)\n\n\t// Grab the certificate to test that getTLSConfig did its job correctly\n\t// and generated a new cert.\n\tnewCertData, err := tls.LoadX509KeyPair(certPath, keyPath)\n\trequire.NoError(t, err, \"couldn't grab new certificate\")\n\n\tnewCert, err := x509.ParseCertificate(newCertData.Certificate[0])\n\trequire.NoError(t, err, \"couldn't parse new certificate\")\n\n\t// Check that the expired certificate was successfully deleted and\n\t// replaced with a new one.\n\trequire.True(t, newCert.NotAfter.After(expiredCert.NotAfter),\n\t\t\"New certificate expiration is too old\")\n}\n\n// TestTLSManagerGenCert tests that the new TLS Manager loads correctly,\n// whether the encrypted TLS key flag is set or not.\nfunc TestTLSManagerGenCert(t *testing.T) {\n\tt.Parallel()\n\n\t_, certPath, keyPath := newTestDirectory(t)\n\n\tcfg := &TLSManagerCfg{\n\t\tTLSCertPath: certPath,\n\t\tTLSKeyPath:  keyPath,\n\t}\n\ttlsManager := NewTLSManager(cfg)\n\n\t_, err := tlsManager.generateOrRenewCert()\n\trequire.NoError(t, err, \"failed to generate new certificate\")\n\n\t// After this is run, a new certificate should be created and written\n\t// to disk. Since the TLSEncryptKey flag isn't set, we should be able\n\t// to read it in plaintext from disk.\n\t_, keyBytes, err := cert.GetCertBytesFromPath(\n\t\tcfg.TLSCertPath, cfg.TLSKeyPath,\n\t)\n\trequire.NoError(t, err, \"unable to load certificate\")\n\trequire.True(t, bytes.HasPrefix(keyBytes, privateKeyPrefix),\n\t\t\"key is encrypted, but shouldn't be\")\n\n\t// Now test that if the TLSEncryptKey flag is set, an encrypted key is\n\t// created and written to disk.\n\t_, certPath, keyPath = newTestDirectory(t)\n\n\tcfg = &TLSManagerCfg{\n\t\tTLSEncryptKey:   true,\n\t\tTLSCertPath:     certPath,\n\t\tTLSKeyPath:      keyPath,\n\t\tTLSCertDuration: testTLSCertDuration,\n\t}\n\ttlsManager = NewTLSManager(cfg)\n\tkeyRing := &mock.SecretKeyRing{\n\t\tRootKey: privKey,\n\t}\n\n\terr = tlsManager.generateCertPair(keyRing)\n\trequire.NoError(t, err, \"failed to generate new certificate\")\n\n\t_, keyBytes, err = cert.GetCertBytesFromPath(\n\t\tcertPath, keyPath,\n\t)\n\trequire.NoError(t, err, \"unable to load certificate\")\n\trequire.False(t, bytes.HasPrefix(keyBytes, privateKeyPrefix),\n\t\t\"key isn't encrypted, but should be\")\n}\n\n// TestEnsureEncryption tests that ensureEncryption does a couple of things:\n// 1) If we have cfg.TLSEncryptKey set, but the tls file saved to disk is not\n// encrypted, generateOrRenewCert encrypts the file and rewrites it to disk.\n// 2) If cfg.TLSEncryptKey is not set, but the file *is* encrypted, then we\n// need to return an error to the user.\nfunc TestEnsureEncryption(t *testing.T) {\n\tt.Parallel()\n\n\tkeyRing := &mock.SecretKeyRing{\n\t\tRootKey: privKey,\n\t}\n\n\t// Write an unencrypted cert file to disk.\n\tcertPath, keyPath, _ := writeTestCertFiles(\n\t\tt, false, false, keyRing,\n\t)\n\n\tcfg := &TLSManagerCfg{\n\t\tTLSEncryptKey: true,\n\t\tTLSCertPath:   certPath,\n\t\tTLSKeyPath:    keyPath,\n\t}\n\ttlsManager := NewTLSManager(cfg)\n\n\t// Check that the keyBytes are initially plaintext.\n\t_, newKeyBytes, err := cert.GetCertBytesFromPath(\n\t\tcfg.TLSCertPath, cfg.TLSKeyPath,\n\t)\n\n\trequire.NoError(t, err, \"unable to load certificate files\")\n\trequire.True(t, bytes.HasPrefix(newKeyBytes, privateKeyPrefix),\n\t\t\"key doesn't have correct plaintext prefix\")\n\n\t// ensureEncryption should detect that the TLS key is in plaintext,\n\t// encrypt it, and rewrite the encrypted version to disk.\n\terr = tlsManager.ensureEncryption(keyRing)\n\trequire.NoError(t, err, \"failed to generate new certificate\")\n\n\t// Grab the file from disk to check that the key is no longer\n\t// plaintext.\n\t_, newKeyBytes, err = cert.GetCertBytesFromPath(\n\t\tcfg.TLSCertPath, cfg.TLSKeyPath,\n\t)\n\trequire.NoError(t, err, \"unable to load certificate\")\n\trequire.False(t, bytes.HasPrefix(newKeyBytes, privateKeyPrefix),\n\t\t\"key isn't encrypted, but should be\")\n\n\t// Now let's flip the cfg.TLSEncryptKey to false. Since the key on file\n\t// is encrypted, ensureEncryption should error out.\n\ttlsManager.cfg.TLSEncryptKey = false\n\terr = tlsManager.ensureEncryption(keyRing)\n\trequire.Error(t, err)\n}\n\n// TestGenerateEphemeralCert tests that an ephemeral certificate is created and\n// stored to disk in a .tmp file and that LoadPermanentCertificate deletes\n// file and replaces it with a fresh certificate pair.\nfunc TestGenerateEphemeralCert(t *testing.T) {\n\tt.Parallel()\n\n\t_, certPath, keyPath := newTestDirectory(t)\n\ttmpCertPath := certPath + \".tmp\"\n\n\tcfg := &TLSManagerCfg{\n\t\tTLSCertPath:     certPath,\n\t\tTLSKeyPath:      keyPath,\n\t\tTLSEncryptKey:   true,\n\t\tTLSCertDuration: testTLSCertDuration,\n\t}\n\ttlsManager := NewTLSManager(cfg)\n\n\tkeyBytes, err := tlsManager.loadEphemeralCertificate()\n\trequire.NoError(t, err, \"failed to generate new certificate\")\n\n\tcertBytes, err := os.ReadFile(tmpCertPath)\n\trequire.NoError(t, err)\n\n\ttlsr, err := cert.NewTLSReloader(certBytes, keyBytes)\n\trequire.NoError(t, err)\n\ttlsManager.tlsReloader = tlsr\n\n\t// Make sure .tmp file is created at the tmp cert path.\n\t_, err = os.ReadFile(tmpCertPath)\n\trequire.NoError(t, err, \"couldn't find temp cert file\")\n\n\t// But no key should be stored.\n\t_, err = os.ReadFile(cfg.TLSKeyPath)\n\trequire.Error(t, err, \"shouldn't have found file\")\n\n\t// And no permanent cert file should be stored.\n\t_, err = os.ReadFile(cfg.TLSCertPath)\n\trequire.Error(t, err, \"shouldn't have found a permanent cert file\")\n\n\t// Now test that when we reload the certificate it generates the new\n\t// certificate properly.\n\tkeyRing := &mock.SecretKeyRing{\n\t\tRootKey: privKey,\n\t}\n\terr = tlsManager.LoadPermanentCertificate(keyRing)\n\trequire.NoError(t, err, \"unable to reload certificate\")\n\n\t// Make sure .tmp file is deleted.\n\t_, _, err = cert.GetCertBytesFromPath(\n\t\ttmpCertPath, cfg.TLSKeyPath,\n\t)\n\trequire.Error(t, err, \".tmp file should have been deleted\")\n\n\t// Make sure a certificate now exists at the permanent cert path.\n\t_, _, err = cert.GetCertBytesFromPath(\n\t\tcfg.TLSCertPath, cfg.TLSKeyPath,\n\t)\n\trequire.NoError(t, err, \"error loading permanent certificate\")\n}\n\n// genCertPair generates a key/cert pair, with the option of generating expired\n// certificates to make sure they are being regenerated correctly.\nfunc genCertPair(t *testing.T, expired bool) ([]byte, []byte) {\n\tt.Helper()\n\n\t// Max serial number.\n\tserialNumberLimit := new(big.Int).Lsh(big.NewInt(1), 128)\n\n\t// Generate a serial number that's below the serialNumberLimit.\n\tserialNumber, err := rand.Int(rand.Reader, serialNumberLimit)\n\trequire.NoError(t, err, \"failed to generate serial number\")\n\n\thost := \"lightning\"\n\n\t// Create a simple ip address for the fake certificate.\n\tipAddresses := []net.IP{net.ParseIP(\"127.0.0.1\"), net.ParseIP(\"::1\")}\n\n\tdnsNames := []string{host, \"unix\", \"unixpacket\"}\n\n\tvar notBefore, notAfter time.Time\n\tif expired {\n\t\tnotBefore = time.Now().Add(-time.Hour * 24)\n\t\tnotAfter = time.Now()\n\t} else {\n\t\tnotBefore = time.Now()\n\t\tnotAfter = time.Now().Add(time.Hour * 24)\n\t}\n\n\t// Construct the certificate template.\n\ttemplate := x509.Certificate{\n\t\tSerialNumber: serialNumber,\n\t\tSubject: pkix.Name{\n\t\t\tOrganization: []string{\"lnd autogenerated cert\"},\n\t\t\tCommonName:   host,\n\t\t},\n\t\tNotBefore: notBefore,\n\t\tNotAfter:  notAfter,\n\t\tKeyUsage: x509.KeyUsageKeyEncipherment |\n\t\t\tx509.KeyUsageDigitalSignature | x509.KeyUsageCertSign,\n\t\tIsCA:                  true, // so can sign self.\n\t\tBasicConstraintsValid: true,\n\t\tDNSNames:              dnsNames,\n\t\tIPAddresses:           ipAddresses,\n\t}\n\n\t// Generate a private key for the certificate.\n\tpriv, err := ecdsa.GenerateKey(elliptic.P256(), rand.Reader)\n\tif err != nil {\n\t\tt.Fatalf(\"failed to generate a private key\")\n\t}\n\n\tcertDerBytes, err := x509.CreateCertificate(\n\t\trand.Reader, &template, &template, &priv.PublicKey, priv,\n\t)\n\trequire.NoError(t, err, \"failed to create certificate\")\n\n\tkeyBytes, err := x509.MarshalECPrivateKey(priv)\n\trequire.NoError(t, err, \"unable to encode privkey\")\n\n\treturn certDerBytes, keyBytes\n}\n\n// writeTestCertFiles creates test files and writes them to a temporary testing\n// directory.\nfunc writeTestCertFiles(t *testing.T, expiredCert, encryptTLSKey bool,\n\tkeyRing keychain.KeyRing) (string, string, *x509.Certificate) {\n\n\tt.Helper()\n\n\ttempDir, certPath, keyPath := newTestDirectory(t)\n\n\tvar certDerBytes, keyBytes []byte\n\t// Either create a valid certificate or an expired certificate pair,\n\t// depending on the test.\n\tif expiredCert {\n\t\tcertDerBytes, keyBytes = genCertPair(t, true)\n\t} else {\n\t\tcertDerBytes, keyBytes = genCertPair(t, false)\n\t}\n\n\tparsedCert, err := x509.ParseCertificate(certDerBytes)\n\trequire.NoError(t, err, \"failed to parse certificate\")\n\n\tcertBuf := bytes.Buffer{}\n\terr = pem.Encode(\n\t\t&certBuf, &pem.Block{\n\t\t\tType:  \"CERTIFICATE\",\n\t\t\tBytes: certDerBytes,\n\t\t},\n\t)\n\trequire.NoError(t, err, \"failed to encode certificate\")\n\n\tvar keyBuf *bytes.Buffer\n\tif !encryptTLSKey {\n\t\tkeyBuf = &bytes.Buffer{}\n\t\terr = pem.Encode(\n\t\t\tkeyBuf, &pem.Block{\n\t\t\t\tType:  \"EC PRIVATE KEY\",\n\t\t\t\tBytes: keyBytes,\n\t\t\t},\n\t\t)\n\t\trequire.NoError(t, err, \"failed to encode private key\")\n\t} else {\n\t\te, err := lnencrypt.KeyRingEncrypter(keyRing)\n\t\trequire.NoError(t, err, \"unable to generate key encrypter\")\n\t\terr = e.EncryptPayloadToWriter(\n\t\t\tkeyBytes, keyBuf,\n\t\t)\n\t\trequire.NoError(t, err, \"failed to encrypt private key\")\n\t}\n\n\terr = os.WriteFile(tempDir+\"/tls.cert\", certBuf.Bytes(), 0644)\n\trequire.NoError(t, err, \"failed to write cert file\")\n\terr = os.WriteFile(tempDir+\"/tls.key\", keyBuf.Bytes(), 0600)\n\trequire.NoError(t, err, \"failed to write key file\")\n\n\treturn certPath, keyPath, parsedCert\n}\n\n// newTestDirectory creates a new test directory and returns the location of\n// the test tls.cert and tls.key files.\nfunc newTestDirectory(t *testing.T) (string, string, string) {\n\tt.Helper()\n\n\ttempDir := t.TempDir()\n\tcertPath := tempDir + \"/tls.cert\"\n\tkeyPath := tempDir + \"/tls.key\"\n\n\treturn tempDir, certPath, keyPath\n}\n"
        },
        {
          "name": "tlv",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "tor",
          "type": "tree",
          "content": null
        },
        {
          "name": "walletunlocker",
          "type": "tree",
          "content": null
        },
        {
          "name": "watchtower",
          "type": "tree",
          "content": null
        },
        {
          "name": "witness_beacon.go",
          "type": "blob",
          "size": 5.2236328125,
          "content": "package lnd\n\nimport (\n\t\"errors\"\n\t\"sync\"\n\n\t\"github.com/lightningnetwork/lnd/channeldb\"\n\t\"github.com/lightningnetwork/lnd/contractcourt\"\n\t\"github.com/lightningnetwork/lnd/graph/db/models\"\n\t\"github.com/lightningnetwork/lnd/htlcswitch\"\n\t\"github.com/lightningnetwork/lnd/htlcswitch/hop\"\n\t\"github.com/lightningnetwork/lnd/lntypes\"\n\t\"github.com/lightningnetwork/lnd/lnwire\"\n)\n\n// preimageSubscriber reprints an active subscription to be notified once the\n// daemon discovers new preimages, either on chain or off-chain.\ntype preimageSubscriber struct {\n\tupdateChan chan lntypes.Preimage\n\n\tquit chan struct{}\n}\n\ntype witnessCache interface {\n\t// LookupSha256Witness attempts to lookup the preimage for a sha256\n\t// hash. If the witness isn't found, ErrNoWitnesses will be returned.\n\tLookupSha256Witness(hash lntypes.Hash) (lntypes.Preimage, error)\n\n\t// AddSha256Witnesses adds a batch of new sha256 preimages into the\n\t// witness cache. This is an alias for AddWitnesses that uses\n\t// Sha256HashWitness as the preimages' witness type.\n\tAddSha256Witnesses(preimages ...lntypes.Preimage) error\n}\n\n// preimageBeacon is an implementation of the contractcourt.WitnessBeacon\n// interface, and the lnwallet.PreimageCache interface. This implementation is\n// concerned with a single witness type: sha256 hahsh preimages.\ntype preimageBeacon struct {\n\tsync.RWMutex\n\n\twCache witnessCache\n\n\tclientCounter uint64\n\tsubscribers   map[uint64]*preimageSubscriber\n\n\tinterceptor func(htlcswitch.InterceptedForward) error\n}\n\nfunc newPreimageBeacon(wCache witnessCache,\n\tinterceptor func(htlcswitch.InterceptedForward) error) *preimageBeacon {\n\n\treturn &preimageBeacon{\n\t\twCache:      wCache,\n\t\tinterceptor: interceptor,\n\t\tsubscribers: make(map[uint64]*preimageSubscriber),\n\t}\n}\n\n// SubscribeUpdates returns a channel that will be sent upon *each* time a new\n// preimage is discovered.\nfunc (p *preimageBeacon) SubscribeUpdates(\n\tchanID lnwire.ShortChannelID, htlc *channeldb.HTLC,\n\tpayload *hop.Payload,\n\tnextHopOnionBlob []byte) (*contractcourt.WitnessSubscription, error) {\n\n\tp.Lock()\n\tdefer p.Unlock()\n\n\tclientID := p.clientCounter\n\tclient := &preimageSubscriber{\n\t\tupdateChan: make(chan lntypes.Preimage, 10),\n\t\tquit:       make(chan struct{}),\n\t}\n\n\tp.subscribers[p.clientCounter] = client\n\n\tp.clientCounter++\n\n\tsrvrLog.Debugf(\"Creating new witness beacon subscriber, id=%v\",\n\t\tp.clientCounter)\n\n\tsub := &contractcourt.WitnessSubscription{\n\t\tWitnessUpdates: client.updateChan,\n\t\tCancelSubscription: func() {\n\t\t\tp.Lock()\n\t\t\tdefer p.Unlock()\n\n\t\t\tdelete(p.subscribers, clientID)\n\n\t\t\tclose(client.quit)\n\t\t},\n\t}\n\n\t// Notify the htlc interceptor. There may be a client connected\n\t// and willing to supply a preimage.\n\tpacket := &htlcswitch.InterceptedPacket{\n\t\tHash:           htlc.RHash,\n\t\tIncomingExpiry: htlc.RefundTimeout,\n\t\tIncomingAmount: htlc.Amt,\n\t\tIncomingCircuit: models.CircuitKey{\n\t\t\tChanID: chanID,\n\t\t\tHtlcID: htlc.HtlcIndex,\n\t\t},\n\t\tOutgoingChanID:       payload.FwdInfo.NextHop,\n\t\tOutgoingExpiry:       payload.FwdInfo.OutgoingCTLV,\n\t\tOutgoingAmount:       payload.FwdInfo.AmountToForward,\n\t\tInOnionCustomRecords: payload.CustomRecords(),\n\t\tInWireCustomRecords:  htlc.CustomRecords,\n\t}\n\tcopy(packet.OnionBlob[:], nextHopOnionBlob)\n\n\tfwd := newInterceptedForward(packet, p)\n\n\terr := p.interceptor(fwd)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn sub, nil\n}\n\n// LookupPreimage attempts to lookup a preimage in the global cache.  True is\n// returned for the second argument if the preimage is found.\nfunc (p *preimageBeacon) LookupPreimage(\n\tpayHash lntypes.Hash) (lntypes.Preimage, bool) {\n\n\tp.RLock()\n\tdefer p.RUnlock()\n\n\t// Otherwise, we'll perform a final check using the witness cache.\n\tpreimage, err := p.wCache.LookupSha256Witness(payHash)\n\tif errors.Is(err, channeldb.ErrNoWitnesses) {\n\t\tltndLog.Debugf(\"No witness for payment %v\", payHash)\n\t\treturn lntypes.Preimage{}, false\n\t}\n\n\tif err != nil {\n\t\tltndLog.Errorf(\"Unable to lookup witness: %v\", err)\n\t\treturn lntypes.Preimage{}, false\n\t}\n\n\treturn preimage, true\n}\n\n// AddPreimages adds a batch of newly discovered preimages to the global cache,\n// and also signals any subscribers of the newly discovered witness.\nfunc (p *preimageBeacon) AddPreimages(preimages ...lntypes.Preimage) error {\n\t// Exit early if no preimages are presented.\n\tif len(preimages) == 0 {\n\t\treturn nil\n\t}\n\n\t// Copy the preimages to ensure the backing area can't be modified by\n\t// the caller when delivering notifications.\n\tpreimageCopies := make([]lntypes.Preimage, 0, len(preimages))\n\tfor _, preimage := range preimages {\n\t\tsrvrLog.Infof(\"Adding preimage=%v to witness cache for %v\",\n\t\t\tpreimage, preimage.Hash())\n\n\t\tpreimageCopies = append(preimageCopies, preimage)\n\t}\n\n\t// First, we'll add the witness to the decaying witness cache.\n\terr := p.wCache.AddSha256Witnesses(preimages...)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tp.Lock()\n\tdefer p.Unlock()\n\n\t// With the preimage added to our state, we'll now send a new\n\t// notification to all subscribers.\n\tfor _, client := range p.subscribers {\n\t\tgo func(c *preimageSubscriber) {\n\t\t\tfor _, preimage := range preimageCopies {\n\t\t\t\tselect {\n\t\t\t\tcase c.updateChan <- preimage:\n\t\t\t\tcase <-c.quit:\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}(client)\n\t}\n\n\tsrvrLog.Debugf(\"Added %d preimage(s) to witness cache\",\n\t\tlen(preimageCopies))\n\n\treturn nil\n}\n\nvar _ contractcourt.WitnessBeacon = (*preimageBeacon)(nil)\n"
        },
        {
          "name": "witness_beacon_test.go",
          "type": "blob",
          "size": 1.2197265625,
          "content": "package lnd\n\nimport (\n\t\"testing\"\n\n\t\"github.com/lightningnetwork/lnd/channeldb\"\n\t\"github.com/lightningnetwork/lnd/htlcswitch\"\n\t\"github.com/lightningnetwork/lnd/htlcswitch/hop\"\n\t\"github.com/lightningnetwork/lnd/lntypes\"\n\t\"github.com/lightningnetwork/lnd/lnwire\"\n\t\"github.com/stretchr/testify/require\"\n)\n\n// TestWitnessBeaconIntercept tests that the beacon passes on subscriptions to\n// the interceptor correctly.\nfunc TestWitnessBeaconIntercept(t *testing.T) {\n\tvar interceptedFwd htlcswitch.InterceptedForward\n\tinterceptor := func(fwd htlcswitch.InterceptedForward) error {\n\t\tinterceptedFwd = fwd\n\n\t\treturn nil\n\t}\n\n\tp := newPreimageBeacon(\n\t\t&mockWitnessCache{}, interceptor,\n\t)\n\n\tpreimage := lntypes.Preimage{1, 2, 3}\n\thash := preimage.Hash()\n\n\tsubscription, err := p.SubscribeUpdates(\n\t\tlnwire.NewShortChanIDFromInt(1),\n\t\t&channeldb.HTLC{\n\t\t\tRHash: hash,\n\t\t},\n\t\t&hop.Payload{},\n\t\t[]byte{2},\n\t)\n\trequire.NoError(t, err)\n\tt.Cleanup(subscription.CancelSubscription)\n\n\trequire.NoError(t, interceptedFwd.Settle(preimage))\n\n\tupdate := <-subscription.WitnessUpdates\n\trequire.Equal(t, preimage, update)\n}\n\ntype mockWitnessCache struct {\n\twitnessCache\n}\n\nfunc (w *mockWitnessCache) AddSha256Witnesses(\n\tpreimages ...lntypes.Preimage) error {\n\n\treturn nil\n}\n"
        },
        {
          "name": "zpay32",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}