{
  "metadata": {
    "timestamp": 1736567746845,
    "page": 333,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "nats-io/nats.go",
      "stars": 5658,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.375,
          "content": "# Compiled Object files, Static and Dynamic libs (Shared Objects)\n*.o\n*.a\n*.so\n\n# Folders\n_obj\n_test\n\n# Architecture specific extensions/prefixes\n*.[568vq]\n[568vq].out\n\n*.cgo1.go\n*.cgo2.c\n_cgo_defun.c\n_cgo_gotypes.go\n_cgo_export.*\n\n_testmain.go\n\n*.exe\n\n# Emacs\n*~\n\\#*\\#\n.\\#*\n\n# vi/vim\n.??*.swp\n\n# Mac\n.DS_Store\n\n# Eclipse\n.project\n.settings/\n\n# bin\n\n# Goland\n.idea\n\n# VS Code\n.vscode "
        },
        {
          "name": ".golangci.yaml",
          "type": "blob",
          "size": 0.2890625,
          "content": "issues:\n  max-issues-per-linter: 0\n  max-same-issues: 0\n  exclude-rules:\n    - linters:\n      - errcheck\n      text: \"Unsubscribe\"\n    - linters:\n      - errcheck\n      text: \"Drain\"\n    - linters:\n      - errcheck\n      text: \"msg.Ack\"\n    - linters:\n      - errcheck\n      text: \"watcher.Stop\"\n"
        },
        {
          "name": ".words",
          "type": "blob",
          "size": 0.720703125,
          "content": "1\n\nderek\ndlc\nivan\n\nacknowledgement/SM\narity\ndeduplication/S\ndemarshal/SDG\ndurables\niff\nobservable/S\nredelivery/S\nretransmitting\nretry/SB\n\nSlowConsumer\n\nAppendInt\nReadMIMEHeader\n\nclientProtoZero\njetstream\nv1\nv2\n\nack/SGD\nauth\nauthToken\nchans\ncreds\nconfig/S\ncseq\nimpl\nmsgh\nmsgId\nmux/S\nnack\nptr\npuback\nscanf\nstderr\nstdout\nstructs\ntm\ntodo\nunsub/S\n\npermessage\npermessage-deflate\nurlA\nurlB\nwebsocket\nws\nwss\n\nNKey\npList\n\nbackend/S\nbackoff/S\ndecompressor/CGS\ninflight\ninlined\nlookups\nreconnection/MS\nredeliver/ADGS\nresponder/S\nrewrap/S\nrollup/S\nunreceive/DRSZGB\nvariadic\nwakeup/S\nwhitespace\nwrap/AS\n\nomitempty\n\napache\nhtml\nietf\nwww\n\nsum256\n32bit/S\n64bit/S\n64k\n128k\n512k\n\nhacky\nhandroll/D\n\nrfc6455\nrfc7692\n0x00\n0xff\n20x\n40x\n50x\n\nErrXXX\n\natlanta\neu\n"
        },
        {
          "name": ".words.readme",
          "type": "blob",
          "size": 1.087890625,
          "content": "The .words file is used by gospel (v1.2+), which wraps the Hunspell libraries\nbut populates the dictionary with identifiers from the Go source.\n\n<https://github.com/kortschak/gospel>\n\nAlas, no comments are allowed in the .words file and newer versions of gospel\nerror out on seeing them.  This is really a hunspell restriction.\n\nWe assume en_US hunspell dictionaries are installed and used.\nThe /AFFIXRULES are defined in en_US.aff (eg: /usr/share/hunspell/en_US.aff)\nInvoke `hunspell -D` to see the actual locations.\n\nWords which are in the base dictionary can't have extra affix rules added to\nthem, so we have to start with the affixed variant we want to add.\nThus `creds` rather than `cred/S` and so on.\n\nSo we can't use receive/DRSZGBU, adding 'U', to allow unreceive and variants,\nwe have to use unreceive as the stem.\n\nWe can't define our own affix or compound rules,\nto capture rfc\\d{3,} or 0x[0-9A-Fa-f]{2}\n\nThe spelling tokenizer doesn't take \"permessage-deflate\" as allowing for ...\n\"permessage-deflate\", which is an RFC7692 registered extension for websockets.\nWe have to explicitly list \"permessage\".\n"
        },
        {
          "name": "CODE-OF-CONDUCT.md",
          "type": "blob",
          "size": 0.134765625,
          "content": "## Community Code of Conduct\n\nNATS follows the [CNCF Code of Conduct](https://github.com/cncf/foundation/blob/master/code-of-conduct.md).\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 3.8505859375,
          "content": "# Contributing\n\nThanks for your interest in contributing! This document contains `nats-io/nats.go` specific contributing details. If you\nare a first-time contributor, please refer to the general [NATS Contributor Guide](https://nats.io/contributing/) to get\na comprehensive overview of contributing to the NATS project.\n\n## Getting started\n\nThere are three general ways you can contribute to this repo:\n\n- Proposing an enhancement or new feature\n- Reporting a bug or regression\n- Contributing changes to the source code\n\nFor the first two, refer to the [GitHub Issues](https://github.com/nats-io/nats.go/issues/new/choose) which guides you\nthrough the available options along with the needed information to collect.\n\n## Contributing changes\n\n_Prior to opening a pull request, it is recommended to open an issue first to ensure the maintainers can review intended\nchanges. Exceptions to this rule include fixing non-functional source such as code comments, documentation or other\nsupporting files._\n\nProposing source code changes is done through GitHub's standard pull request workflow.\n\nIf your branch is a work-in-progress then please start by creating your pull requests as draft, by clicking the\ndown-arrow next to the `Create pull request` button and instead selecting `Create draft pull request`.\n\nThis will defer the automatic process of requesting a review from the NATS team and significantly reduces noise until\nyou are ready. Once you are happy, you can click the `Ready for review` button.\n\n### Guidelines\n\nA good pull request includes:\n\n- A high-level description of the changes, including links to any issues that are related by adding comments\n  like `Resolves #NNN` to your description.\n  See [Linking a Pull Request to an Issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue)\n  for more information.\n- An up-to-date parent commit. Please make sure you are pulling in the latest `main` branch and rebasing your work on\n  top of it, i.e. `git rebase main`.\n- Unit tests where appropriate. Bug fixes will benefit from the addition of regression tests. New features will not be\n  accepted without suitable test coverage!\n- No more commits than necessary. Sometimes having multiple commits is useful for telling a story or isolating changes\n  from one another, but please squash down any unnecessary commits that may just be for clean-up, comments or small\n  changes.\n- No additional external dependencies that aren't absolutely essential. Please do everything you can to avoid pulling in\n  additional libraries/dependencies into `go.mod` as we will be very critical of these.\n\n### Sign-off\n\nIn order to accept a contribution, you will first need to certify that the contribution is your original work and that\nyou license the work to the project under\nthe [Apache-2.0 license](https://github.com/nats-io/nats.go/blob/main/LICENSE).\n\nThis is done by using `Signed-off-by` statements, which should appear in **both** your commit messages and your PR\ndescription. Please note that we can only accept sign-offs under a legal name. Nicknames and aliases are not permitted.\n\nTo perform a sign-off with `git`, use `git commit -s` (or `--signoff`).\n\n## Get help\n\nIf you have questions about the contribution process, please start\na [GitHub discussion](https://github.com/nats-io/nats.go/discussions), join the [NATS Slack](https://slack.nats.io/), or\nsend your question to the [NATS Google Group](https://groups.google.com/forum/#!forum/natsio).\n\n## Testing\n\nYou should use `go_test.mod` to manage your testing dependencies. Please use the following command to update your\ndependencies and avoid changing the main `go.mod` in a PR:\n\n```shell\ngo mod tidy -modfile=go_test.mod\n```\n\nTo the tests you can pass `-modfile=go_test.mod` flag to `go test` or instead you can also set `GOFLAGS=\"-modfile=go_test.mod\"` as an environment variable:\n\n```shell\ngo test ./... -modfile=go_test.mod\n```\n"
        },
        {
          "name": "GOVERNANCE.md",
          "type": "blob",
          "size": 0.1845703125,
          "content": "# NATS Go Client Governance\n\nNATS Go Client (go-nats) is part of the NATS project and is subject to the [NATS Governance](https://github.com/nats-io/nats-general/blob/master/GOVERNANCE.md)."
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MAINTAINERS.md",
          "type": "blob",
          "size": 0.302734375,
          "content": "# Maintainers\n\nMaintainership is on a per project basis.\n\n### Maintainers\n  - Derek Collison <derek@nats.io> [@derekcollison](https://github.com/derekcollison)\n  - Ivan Kozlovic <ivan@nats.io> [@kozlovic](https://github.com/kozlovic)\n  - Waldemar Quevedo <wally@nats.io> [@wallyqs](https://github.com/wallyqs)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 13.3564453125,
          "content": "# NATS - Go Client\nA [Go](http://golang.org) client for the [NATS messaging system](https://nats.io).\n\n[![License Apache 2][License-Image]][License-Url] [![Go Report Card][ReportCard-Image]][ReportCard-Url] [![Build Status][Build-Status-Image]][Build-Status-Url] [![GoDoc][GoDoc-Image]][GoDoc-Url] [![Coverage Status][Coverage-image]][Coverage-Url]\n\n[License-Url]: https://www.apache.org/licenses/LICENSE-2.0\n[License-Image]: https://img.shields.io/badge/License-Apache2-blue.svg\n[ReportCard-Url]: https://goreportcard.com/report/github.com/nats-io/nats.go\n[ReportCard-Image]: https://goreportcard.com/badge/github.com/nats-io/nats.go\n[Build-Status-Url]: https://github.com/nats-io/nats.go/actions\n[Build-Status-Image]: https://github.com/nats-io/nats.go/actions/workflows/ci.yaml/badge.svg?branch=main\n[GoDoc-Url]: https://pkg.go.dev/github.com/nats-io/nats.go\n[GoDoc-Image]: https://img.shields.io/badge/GoDoc-reference-007d9c\n[Coverage-Url]: https://coveralls.io/r/nats-io/nats.go?branch=main\n[Coverage-image]: https://coveralls.io/repos/github/nats-io/nats.go/badge.svg?branch=main\n\n**Check out [NATS by example](https://natsbyexample.com) - An evolving collection of runnable, cross-client reference examples for NATS.**\n\n## Installation\n\n```bash\n# To get the latest released Go client:\ngo get github.com/nats-io/nats.go@latest\n\n# To get a specific version:\ngo get github.com/nats-io/nats.go@v1.38.0\n\n# Note that the latest major version for NATS Server is v2:\ngo get github.com/nats-io/nats-server/v2@latest\n```\n\n## Basic Usage\n\n```go\nimport \"github.com/nats-io/nats.go\"\n\n// Connect to a server\nnc, _ := nats.Connect(nats.DefaultURL)\n\n// Simple Publisher\nnc.Publish(\"foo\", []byte(\"Hello World\"))\n\n// Simple Async Subscriber\nnc.Subscribe(\"foo\", func(m *nats.Msg) {\n    fmt.Printf(\"Received a message: %s\\n\", string(m.Data))\n})\n\n// Responding to a request message\nnc.Subscribe(\"request\", func(m *nats.Msg) {\n    m.Respond([]byte(\"answer is 42\"))\n})\n\n// Simple Sync Subscriber\nsub, err := nc.SubscribeSync(\"foo\")\nm, err := sub.NextMsg(timeout)\n\n// Channel Subscriber\nch := make(chan *nats.Msg, 64)\nsub, err := nc.ChanSubscribe(\"foo\", ch)\nmsg := <- ch\n\n// Unsubscribe\nsub.Unsubscribe()\n\n// Drain\nsub.Drain()\n\n// Requests\nmsg, err := nc.Request(\"help\", []byte(\"help me\"), 10*time.Millisecond)\n\n// Replies\nnc.Subscribe(\"help\", func(m *nats.Msg) {\n    nc.Publish(m.Reply, []byte(\"I can help!\"))\n})\n\n// Drain connection (Preferred for responders)\n// Close() not needed if this is called.\nnc.Drain()\n\n// Close connection\nnc.Close()\n```\n\n## JetStream\n[![JetStream API Reference](https://pkg.go.dev/badge/github.com/nats-io/nats.go/jetstream.svg)](https://pkg.go.dev/github.com/nats-io/nats.go/jetstream)\n\nJetStream is the built-in NATS persistence system. `nats.go` provides a built-in\nAPI enabling both managing JetStream assets as well as publishing/consuming\npersistent messages.\n\n\n### Basic usage\n\n```go\n// connect to nats server\nnc, _ := nats.Connect(nats.DefaultURL)\n\n// create jetstream context from nats connection\njs, _ := jetstream.New(nc)\n\nctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)\ndefer cancel()\n\n// get existing stream handle\nstream, _ := js.Stream(ctx, \"foo\")\n\n// retrieve consumer handle from a stream\ncons, _ := stream.Consumer(ctx, \"cons\")\n\n// consume messages from the consumer in callback\ncc, _ := cons.Consume(func(msg jetstream.Msg) {\n    fmt.Println(\"Received jetstream message: \", string(msg.Data()))\n    msg.Ack()\n})\ndefer cc.Stop()\n```\n\nTo find more information on `nats.go` JetStream API, visit\n[`jetstream/README.md`](jetstream/README.md)\n\n> The current JetStream API replaces the [legacy JetStream API](legacy_jetstream.md)\n\n## Service API\n\nThe service API (`micro`) allows you to [easily build NATS services](micro/README.md) The\nservices API is currently in beta release.\n\n## New Authentication (Nkeys and User Credentials)\nThis requires server with version >= 2.0.0\n\nNATS servers have a new security and authentication mechanism to authenticate with user credentials and Nkeys.\nThe simplest form is to use the helper method UserCredentials(credsFilepath).\n```go\nnc, err := nats.Connect(url, nats.UserCredentials(\"user.creds\"))\n```\n\nThe helper methods creates two callback handlers to present the user JWT and sign the nonce challenge from the server.\nThe core client library never has direct access to your private key and simply performs the callback for signing the server challenge.\nThe helper will load and wipe and erase memory it uses for each connect or reconnect.\n\nThe helper also can take two entries, one for the JWT and one for the NKey seed file.\n```go\nnc, err := nats.Connect(url, nats.UserCredentials(\"user.jwt\", \"user.nk\"))\n```\n\nYou can also set the callback handlers directly and manage challenge signing directly.\n```go\nnc, err := nats.Connect(url, nats.UserJWT(jwtCB, sigCB))\n```\n\nBare Nkeys are also supported. The nkey seed should be in a read only file, e.g. seed.txt\n```bash\n> cat seed.txt\n# This is my seed nkey!\nSUAGMJH5XLGZKQQWAWKRZJIGMOU4HPFUYLXJMXOO5NLFEO2OOQJ5LPRDPM\n```\n\nThis is a helper function which will load and decode and do the proper signing for the server nonce.\nIt will clear memory in between invocations.\nYou can choose to use the low level option and provide the public key and a signature callback on your own.\n\n```go\nopt, err := nats.NkeyOptionFromSeed(\"seed.txt\")\nnc, err := nats.Connect(serverUrl, opt)\n\n// Direct\nnc, err := nats.Connect(serverUrl, nats.Nkey(pubNkey, sigCB))\n```\n\n## TLS\n\n```go\n// tls as a scheme will enable secure connections by default. This will also verify the server name.\nnc, err := nats.Connect(\"tls://nats.demo.io:4443\")\n\n// If you are using a self-signed certificate, you need to have a tls.Config with RootCAs setup.\n// We provide a helper method to make this case easier.\nnc, err = nats.Connect(\"tls://localhost:4443\", nats.RootCAs(\"./configs/certs/ca.pem\"))\n\n// If the server requires client certificate, there is an helper function for that too:\ncert := nats.ClientCert(\"./configs/certs/client-cert.pem\", \"./configs/certs/client-key.pem\")\nnc, err = nats.Connect(\"tls://localhost:4443\", cert)\n\n// You can also supply a complete tls.Config\n\ncertFile := \"./configs/certs/client-cert.pem\"\nkeyFile := \"./configs/certs/client-key.pem\"\ncert, err := tls.LoadX509KeyPair(certFile, keyFile)\nif err != nil {\n    t.Fatalf(\"error parsing X509 certificate/key pair: %v\", err)\n}\n\nconfig := &tls.Config{\n    ServerName: \topts.Host,\n    Certificates: \t[]tls.Certificate{cert},\n    RootCAs:    \tpool,\n    MinVersion: \ttls.VersionTLS12,\n}\n\nnc, err = nats.Connect(\"nats://localhost:4443\", nats.Secure(config))\nif err != nil {\n\tt.Fatalf(\"Got an error on Connect with Secure Options: %+v\\n\", err)\n}\n\n```\n\n## Wildcard Subscriptions\n\n```go\n\n// \"*\" matches any token, at any level of the subject.\nnc.Subscribe(\"foo.*.baz\", func(m *Msg) {\n    fmt.Printf(\"Msg received on [%s] : %s\\n\", m.Subject, string(m.Data));\n})\n\nnc.Subscribe(\"foo.bar.*\", func(m *Msg) {\n    fmt.Printf(\"Msg received on [%s] : %s\\n\", m.Subject, string(m.Data));\n})\n\n// \">\" matches any length of the tail of a subject, and can only be the last token\n// E.g. 'foo.>' will match 'foo.bar', 'foo.bar.baz', 'foo.foo.bar.bax.22'\nnc.Subscribe(\"foo.>\", func(m *Msg) {\n    fmt.Printf(\"Msg received on [%s] : %s\\n\", m.Subject, string(m.Data));\n})\n\n// Matches all of the above\nnc.Publish(\"foo.bar.baz\", []byte(\"Hello World\"))\n\n```\n\n## Queue Groups\n\n```go\n// All subscriptions with the same queue name will form a queue group.\n// Each message will be delivered to only one subscriber per queue group,\n// using queuing semantics. You can have as many queue groups as you wish.\n// Normal subscribers will continue to work as expected.\n\nnc.QueueSubscribe(\"foo\", \"job_workers\", func(_ *Msg) {\n  received += 1;\n})\n```\n\n## Advanced Usage\n\n```go\n\n// Normally, the library will return an error when trying to connect and\n// there is no server running. The RetryOnFailedConnect option will set\n// the connection in reconnecting state if it failed to connect right away.\nnc, err := nats.Connect(nats.DefaultURL,\n    nats.RetryOnFailedConnect(true),\n    nats.MaxReconnects(10),\n    nats.ReconnectWait(time.Second),\n    nats.ReconnectHandler(func(_ *nats.Conn) {\n        // Note that this will be invoked for the first asynchronous connect.\n    }))\nif err != nil {\n    // Should not return an error even if it can't connect, but you still\n    // need to check in case there are some configuration errors.\n}\n\n// Flush connection to server, returns when all messages have been processed.\nnc.Flush()\nfmt.Println(\"All clear!\")\n\n// FlushTimeout specifies a timeout value as well.\nerr := nc.FlushTimeout(1*time.Second)\nif err != nil {\n    fmt.Println(\"All clear!\")\n} else {\n    fmt.Println(\"Flushed timed out!\")\n}\n\n// Auto-unsubscribe after MAX_WANTED messages received\nconst MAX_WANTED = 10\nsub, err := nc.Subscribe(\"foo\")\nsub.AutoUnsubscribe(MAX_WANTED)\n\n// Multiple connections\nnc1 := nats.Connect(\"nats://host1:4222\")\nnc2 := nats.Connect(\"nats://host2:4222\")\n\nnc1.Subscribe(\"foo\", func(m *Msg) {\n    fmt.Printf(\"Received a message: %s\\n\", string(m.Data))\n})\n\nnc2.Publish(\"foo\", []byte(\"Hello World!\"));\n\n```\n\n## Clustered Usage\n\n```go\n\nvar servers = \"nats://localhost:1222, nats://localhost:1223, nats://localhost:1224\"\n\nnc, err := nats.Connect(servers)\n\n// Optionally set ReconnectWait and MaxReconnect attempts.\n// This example means 10 seconds total per backend.\nnc, err = nats.Connect(servers, nats.MaxReconnects(5), nats.ReconnectWait(2 * time.Second))\n\n// You can also add some jitter for the reconnection.\n// This call will add up to 500 milliseconds for non TLS connections and 2 seconds for TLS connections.\n// If not specified, the library defaults to 100 milliseconds and 1 second, respectively.\nnc, err = nats.Connect(servers, nats.ReconnectJitter(500*time.Millisecond, 2*time.Second))\n\n// You can also specify a custom reconnect delay handler. If set, the library will invoke it when it has tried\n// all URLs in its list. The value returned will be used as the total sleep time, so add your own jitter.\n// The library will pass the number of times it went through the whole list.\nnc, err = nats.Connect(servers, nats.CustomReconnectDelay(func(attempts int) time.Duration {\n    return someBackoffFunction(attempts)\n}))\n\n// Optionally disable randomization of the server pool\nnc, err = nats.Connect(servers, nats.DontRandomize())\n\n// Setup callbacks to be notified on disconnects, reconnects and connection closed.\nnc, err = nats.Connect(servers,\n\tnats.DisconnectErrHandler(func(nc *nats.Conn, err error) {\n\t\tfmt.Printf(\"Got disconnected! Reason: %q\\n\", err)\n\t}),\n\tnats.ReconnectHandler(func(nc *nats.Conn) {\n\t\tfmt.Printf(\"Got reconnected to %v!\\n\", nc.ConnectedUrl())\n\t}),\n\tnats.ClosedHandler(func(nc *nats.Conn) {\n\t\tfmt.Printf(\"Connection closed. Reason: %q\\n\", nc.LastError())\n\t})\n)\n\n// When connecting to a mesh of servers with auto-discovery capabilities,\n// you may need to provide a username/password or token in order to connect\n// to any server in that mesh when authentication is required.\n// Instead of providing the credentials in the initial URL, you will use\n// new option setters:\nnc, err = nats.Connect(\"nats://localhost:4222\", nats.UserInfo(\"foo\", \"bar\"))\n\n// For token based authentication:\nnc, err = nats.Connect(\"nats://localhost:4222\", nats.Token(\"S3cretT0ken\"))\n\n// You can even pass the two at the same time in case one of the server\n// in the mesh requires token instead of user name and password.\nnc, err = nats.Connect(\"nats://localhost:4222\",\n    nats.UserInfo(\"foo\", \"bar\"),\n    nats.Token(\"S3cretT0ken\"))\n\n// Note that if credentials are specified in the initial URLs, they take\n// precedence on the credentials specified through the options.\n// For instance, in the connect call below, the client library will use\n// the user \"my\" and password \"pwd\" to connect to localhost:4222, however,\n// it will use username \"foo\" and password \"bar\" when (re)connecting to\n// a different server URL that it got as part of the auto-discovery.\nnc, err = nats.Connect(\"nats://my:pwd@localhost:4222\", nats.UserInfo(\"foo\", \"bar\"))\n\n```\n\n## Context support (+Go 1.7)\n\n```go\nctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)\ndefer cancel()\n\nnc, err := nats.Connect(nats.DefaultURL)\n\n// Request with context\nmsg, err := nc.RequestWithContext(ctx, \"foo\", []byte(\"bar\"))\n\n// Synchronous subscriber with context\nsub, err := nc.SubscribeSync(\"foo\")\nmsg, err := sub.NextMsgWithContext(ctx)\n\n```\n\n## Backwards compatibility\n\nIn the development of nats.go, we are committed to maintaining backward compatibility and ensuring a stable and reliable  experience for all users. In general, we follow the standard go compatibility guidelines.\nHowever, it's important to clarify our stance on certain types of changes:\n\n- **Expanding structures:**\nAdding new fields to structs is not considered a breaking change.\n\n- **Adding methods to exported interfaces:**\nExtending public interfaces with new methods is also not viewed as a breaking change within the context of this project. It is important to note that no unexported methods will be added to interfaces allowing users to implement them.\n\nAdditionally, this library always supports at least 2 latest minor Go versions. For example, if the latest Go version is 1.22, the library will support Go 1.21 and 1.22.\n\n## License\n\nUnless otherwise noted, the NATS source files are distributed\nunder the Apache Version 2.0 license found in the LICENSE file.\n\n[![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2Fnats-io%2Fgo-nats.svg?type=large)](https://app.fossa.io/projects/git%2Bgithub.com%2Fnats-io%2Fgo-nats?ref=badge_large)\n"
        },
        {
          "name": "bench",
          "type": "tree",
          "content": null
        },
        {
          "name": "context.go",
          "type": "blob",
          "size": 5.7041015625,
          "content": "// Copyright 2016-2023 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nats\n\nimport (\n\t\"context\"\n\t\"reflect\"\n)\n\n// RequestMsgWithContext takes a context, a subject and payload\n// in bytes and request expecting a single response.\nfunc (nc *Conn) RequestMsgWithContext(ctx context.Context, msg *Msg) (*Msg, error) {\n\tif msg == nil {\n\t\treturn nil, ErrInvalidMsg\n\t}\n\thdr, err := msg.headerBytes()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn nc.requestWithContext(ctx, msg.Subject, hdr, msg.Data)\n}\n\n// RequestWithContext takes a context, a subject and payload\n// in bytes and request expecting a single response.\nfunc (nc *Conn) RequestWithContext(ctx context.Context, subj string, data []byte) (*Msg, error) {\n\treturn nc.requestWithContext(ctx, subj, nil, data)\n}\n\nfunc (nc *Conn) requestWithContext(ctx context.Context, subj string, hdr, data []byte) (*Msg, error) {\n\tif ctx == nil {\n\t\treturn nil, ErrInvalidContext\n\t}\n\tif nc == nil {\n\t\treturn nil, ErrInvalidConnection\n\t}\n\t// Check whether the context is done already before making\n\t// the request.\n\tif ctx.Err() != nil {\n\t\treturn nil, ctx.Err()\n\t}\n\n\tvar m *Msg\n\tvar err error\n\n\t// If user wants the old style.\n\tif nc.useOldRequestStyle() {\n\t\tm, err = nc.oldRequestWithContext(ctx, subj, hdr, data)\n\t} else {\n\t\tmch, token, err := nc.createNewRequestAndSend(subj, hdr, data)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tvar ok bool\n\n\t\tselect {\n\t\tcase m, ok = <-mch:\n\t\t\tif !ok {\n\t\t\t\treturn nil, ErrConnectionClosed\n\t\t\t}\n\t\tcase <-ctx.Done():\n\t\t\tnc.mu.Lock()\n\t\t\tdelete(nc.respMap, token)\n\t\t\tnc.mu.Unlock()\n\t\t\treturn nil, ctx.Err()\n\t\t}\n\t}\n\t// Check for no responder status.\n\tif err == nil && len(m.Data) == 0 && m.Header.Get(statusHdr) == noResponders {\n\t\tm, err = nil, ErrNoResponders\n\t}\n\treturn m, err\n}\n\n// oldRequestWithContext utilizes inbox and subscription per request.\nfunc (nc *Conn) oldRequestWithContext(ctx context.Context, subj string, hdr, data []byte) (*Msg, error) {\n\tinbox := nc.NewInbox()\n\tch := make(chan *Msg, RequestChanLen)\n\n\ts, err := nc.subscribe(inbox, _EMPTY_, nil, ch, nil, true, nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ts.AutoUnsubscribe(1)\n\tdefer s.Unsubscribe()\n\n\terr = nc.publish(subj, inbox, hdr, data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn s.NextMsgWithContext(ctx)\n}\n\nfunc (s *Subscription) nextMsgWithContext(ctx context.Context, pullSubInternal, waitIfNoMsg bool) (*Msg, error) {\n\tif ctx == nil {\n\t\treturn nil, ErrInvalidContext\n\t}\n\tif s == nil {\n\t\treturn nil, ErrBadSubscription\n\t}\n\tif ctx.Err() != nil {\n\t\treturn nil, ctx.Err()\n\t}\n\n\ts.mu.Lock()\n\terr := s.validateNextMsgState(pullSubInternal)\n\tif err != nil {\n\t\ts.mu.Unlock()\n\t\treturn nil, err\n\t}\n\n\t// snapshot\n\tmch := s.mch\n\ts.mu.Unlock()\n\n\tvar ok bool\n\tvar msg *Msg\n\n\t// If something is available right away, let's optimize that case.\n\tselect {\n\tcase msg, ok = <-mch:\n\t\tif !ok {\n\t\t\treturn nil, s.getNextMsgErr()\n\t\t}\n\t\tif err := s.processNextMsgDelivered(msg); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn msg, nil\n\tdefault:\n\t\t// If internal and we don't want to wait, signal that there is no\n\t\t// message in the internal queue.\n\t\tif pullSubInternal && !waitIfNoMsg {\n\t\t\treturn nil, errNoMessages\n\t\t}\n\t}\n\n\tselect {\n\tcase msg, ok = <-mch:\n\t\tif !ok {\n\t\t\treturn nil, s.getNextMsgErr()\n\t\t}\n\t\tif err := s.processNextMsgDelivered(msg); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\tcase <-ctx.Done():\n\t\treturn nil, ctx.Err()\n\t}\n\n\treturn msg, nil\n}\n\n// NextMsgWithContext takes a context and returns the next message\n// available to a synchronous subscriber, blocking until it is delivered\n// or context gets canceled.\nfunc (s *Subscription) NextMsgWithContext(ctx context.Context) (*Msg, error) {\n\treturn s.nextMsgWithContext(ctx, false, true)\n}\n\n// FlushWithContext will allow a context to control the duration\n// of a Flush() call. This context should be non-nil and should\n// have a deadline set. We will return an error if none is present.\nfunc (nc *Conn) FlushWithContext(ctx context.Context) error {\n\tif nc == nil {\n\t\treturn ErrInvalidConnection\n\t}\n\tif ctx == nil {\n\t\treturn ErrInvalidContext\n\t}\n\t_, ok := ctx.Deadline()\n\tif !ok {\n\t\treturn ErrNoDeadlineContext\n\t}\n\n\tnc.mu.Lock()\n\tif nc.isClosed() {\n\t\tnc.mu.Unlock()\n\t\treturn ErrConnectionClosed\n\t}\n\t// Create a buffered channel to prevent chan send to block\n\t// in processPong()\n\tch := make(chan struct{}, 1)\n\tnc.sendPing(ch)\n\tnc.mu.Unlock()\n\n\tvar err error\n\n\tselect {\n\tcase _, ok := <-ch:\n\t\tif !ok {\n\t\t\terr = ErrConnectionClosed\n\t\t} else {\n\t\t\tclose(ch)\n\t\t}\n\tcase <-ctx.Done():\n\t\terr = ctx.Err()\n\t}\n\n\tif err != nil {\n\t\tnc.removeFlushEntry(ch)\n\t}\n\n\treturn err\n}\n\n// RequestWithContext will create an Inbox and perform a Request\n// using the provided cancellation context with the Inbox reply\n// for the data v. A response will be decoded into the vPtr last parameter.\n//\n// Deprecated: Encoded connections are no longer supported.\nfunc (c *EncodedConn) RequestWithContext(ctx context.Context, subject string, v any, vPtr any) error {\n\tif ctx == nil {\n\t\treturn ErrInvalidContext\n\t}\n\n\tb, err := c.Enc.Encode(subject, v)\n\tif err != nil {\n\t\treturn err\n\t}\n\tm, err := c.Conn.RequestWithContext(ctx, subject, b)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif reflect.TypeOf(vPtr) == emptyMsgType {\n\t\tmPtr := vPtr.(*Msg)\n\t\t*mPtr = *m\n\t} else {\n\t\terr := c.Enc.Decode(m.Subject, m.Data, vPtr)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n"
        },
        {
          "name": "dependencies.md",
          "type": "blob",
          "size": 0.6513671875,
          "content": "# External Dependencies\n\nThis file lists the dependencies used in this repository.\n\n| Dependency                        | License      |\n|-----------------------------------|--------------|\n| Go                                | BSD 3-Clause |\n| github.com/golang/protobuf/proto  | BSD-3-Clause |\n| github.com/klauspost/compress     | BSD-3-Clause |\n| github.com/nats-io/nats-server/v2 | Apache-2.0   |\n| github.com/nats-io/nkeys          | Apache-2.0   |\n| github.com/nats-io/nuid           | Apache-2.0   |\n| go.uber.org/goleak                | MIT          |\n| golang.org/x/text                 | BSD-3-Clause |\n| google.golang.org/protobuf        | BSD-3-Clause |\n"
        },
        {
          "name": "dependencies.tpl",
          "type": "blob",
          "size": 0.3701171875,
          "content": "# External Dependencies\n\nThis file lists the dependencies used in this repository.\n\n| Dependency                                       | License                                 |\n|--------------------------------------------------|-----------------------------------------|\n{{ range . }}| {{.Name}}                           | {{.LicenseName}}                        |\n{{ end }}\n"
        },
        {
          "name": "enc.go",
          "type": "blob",
          "size": 9.19921875,
          "content": "// Copyright 2012-2023 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nats\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"sync\"\n\t\"time\"\n\n\t// Default Encoders\n\t\"github.com/nats-io/nats.go/encoders/builtin\"\n)\n\n//lint:file-ignore SA1019 Ignore deprecation warnings for EncodedConn\n\n// Encoder interface is for all register encoders\n//\n// Deprecated: Encoded connections are no longer supported.\ntype Encoder interface {\n\tEncode(subject string, v any) ([]byte, error)\n\tDecode(subject string, data []byte, vPtr any) error\n}\n\nvar encMap map[string]Encoder\nvar encLock sync.Mutex\n\n// Indexed names into the Registered Encoders.\nconst (\n\tJSON_ENCODER    = \"json\"\n\tGOB_ENCODER     = \"gob\"\n\tDEFAULT_ENCODER = \"default\"\n)\n\nfunc init() {\n\tencMap = make(map[string]Encoder)\n\t// Register json, gob and default encoder\n\tRegisterEncoder(JSON_ENCODER, &builtin.JsonEncoder{})\n\tRegisterEncoder(GOB_ENCODER, &builtin.GobEncoder{})\n\tRegisterEncoder(DEFAULT_ENCODER, &builtin.DefaultEncoder{})\n}\n\n// EncodedConn are the preferred way to interface with NATS. They wrap a bare connection to\n// a nats server and have an extendable encoder system that will encode and decode messages\n// from raw Go types.\n//\n// Deprecated: Encoded connections are no longer supported.\ntype EncodedConn struct {\n\tConn *Conn\n\tEnc  Encoder\n}\n\n// NewEncodedConn will wrap an existing Connection and utilize the appropriate registered\n// encoder.\n//\n// Deprecated: Encoded connections are no longer supported.\nfunc NewEncodedConn(c *Conn, encType string) (*EncodedConn, error) {\n\tif c == nil {\n\t\treturn nil, errors.New(\"nats: Nil Connection\")\n\t}\n\tif c.IsClosed() {\n\t\treturn nil, ErrConnectionClosed\n\t}\n\tec := &EncodedConn{Conn: c, Enc: EncoderForType(encType)}\n\tif ec.Enc == nil {\n\t\treturn nil, fmt.Errorf(\"no encoder registered for '%s'\", encType)\n\t}\n\treturn ec, nil\n}\n\n// RegisterEncoder will register the encType with the given Encoder. Useful for customization.\n//\n// Deprecated: Encoded connections are no longer supported.\nfunc RegisterEncoder(encType string, enc Encoder) {\n\tencLock.Lock()\n\tdefer encLock.Unlock()\n\tencMap[encType] = enc\n}\n\n// EncoderForType will return the registered Encoder for the encType.\n//\n// Deprecated: Encoded connections are no longer supported.\nfunc EncoderForType(encType string) Encoder {\n\tencLock.Lock()\n\tdefer encLock.Unlock()\n\treturn encMap[encType]\n}\n\n// Publish publishes the data argument to the given subject. The data argument\n// will be encoded using the associated encoder.\n//\n// Deprecated: Encoded connections are no longer supported.\nfunc (c *EncodedConn) Publish(subject string, v any) error {\n\tb, err := c.Enc.Encode(subject, v)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn c.Conn.publish(subject, _EMPTY_, nil, b)\n}\n\n// PublishRequest will perform a Publish() expecting a response on the\n// reply subject. Use Request() for automatically waiting for a response\n// inline.\n//\n// Deprecated: Encoded connections are no longer supported.\nfunc (c *EncodedConn) PublishRequest(subject, reply string, v any) error {\n\tb, err := c.Enc.Encode(subject, v)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn c.Conn.publish(subject, reply, nil, b)\n}\n\n// Request will create an Inbox and perform a Request() call\n// with the Inbox reply for the data v. A response will be\n// decoded into the vPtr Response.\n//\n// Deprecated: Encoded connections are no longer supported.\nfunc (c *EncodedConn) Request(subject string, v any, vPtr any, timeout time.Duration) error {\n\tb, err := c.Enc.Encode(subject, v)\n\tif err != nil {\n\t\treturn err\n\t}\n\tm, err := c.Conn.Request(subject, b, timeout)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif reflect.TypeOf(vPtr) == emptyMsgType {\n\t\tmPtr := vPtr.(*Msg)\n\t\t*mPtr = *m\n\t} else {\n\t\terr = c.Enc.Decode(m.Subject, m.Data, vPtr)\n\t}\n\treturn err\n}\n\n// Handler is a specific callback used for Subscribe. It is generalized to\n// an any, but we will discover its format and arguments at runtime\n// and perform the correct callback, including demarshaling encoded data\n// back into the appropriate struct based on the signature of the Handler.\n//\n// Handlers are expected to have one of four signatures.\n//\n//\ttype person struct {\n//\t\tName string `json:\"name,omitempty\"`\n//\t\tAge  uint   `json:\"age,omitempty\"`\n//\t}\n//\n//\thandler := func(m *Msg)\n//\thandler := func(p *person)\n//\thandler := func(subject string, o *obj)\n//\thandler := func(subject, reply string, o *obj)\n//\n// These forms allow a callback to request a raw Msg ptr, where the processing\n// of the message from the wire is untouched. Process a JSON representation\n// and demarshal it into the given struct, e.g. person.\n// There are also variants where the callback wants either the subject, or the\n// subject and the reply subject.\n//\n// Deprecated: Encoded connections are no longer supported.\ntype Handler any\n\n// Dissect the cb Handler's signature\nfunc argInfo(cb Handler) (reflect.Type, int) {\n\tcbType := reflect.TypeOf(cb)\n\tif cbType.Kind() != reflect.Func {\n\t\tpanic(\"nats: Handler needs to be a func\")\n\t}\n\tnumArgs := cbType.NumIn()\n\tif numArgs == 0 {\n\t\treturn nil, numArgs\n\t}\n\treturn cbType.In(numArgs - 1), numArgs\n}\n\nvar emptyMsgType = reflect.TypeOf(&Msg{})\n\n// Subscribe will create a subscription on the given subject and process incoming\n// messages using the specified Handler. The Handler should be a func that matches\n// a signature from the description of Handler from above.\n//\n// Deprecated: Encoded connections are no longer supported.\nfunc (c *EncodedConn) Subscribe(subject string, cb Handler) (*Subscription, error) {\n\treturn c.subscribe(subject, _EMPTY_, cb)\n}\n\n// QueueSubscribe will create a queue subscription on the given subject and process\n// incoming messages using the specified Handler. The Handler should be a func that\n// matches a signature from the description of Handler from above.\n//\n// Deprecated: Encoded connections are no longer supported.\nfunc (c *EncodedConn) QueueSubscribe(subject, queue string, cb Handler) (*Subscription, error) {\n\treturn c.subscribe(subject, queue, cb)\n}\n\n// Internal implementation that all public functions will use.\nfunc (c *EncodedConn) subscribe(subject, queue string, cb Handler) (*Subscription, error) {\n\tif cb == nil {\n\t\treturn nil, errors.New(\"nats: Handler required for EncodedConn Subscription\")\n\t}\n\targType, numArgs := argInfo(cb)\n\tif argType == nil {\n\t\treturn nil, errors.New(\"nats: Handler requires at least one argument\")\n\t}\n\n\tcbValue := reflect.ValueOf(cb)\n\twantsRaw := (argType == emptyMsgType)\n\n\tnatsCB := func(m *Msg) {\n\t\tvar oV []reflect.Value\n\t\tif wantsRaw {\n\t\t\toV = []reflect.Value{reflect.ValueOf(m)}\n\t\t} else {\n\t\t\tvar oPtr reflect.Value\n\t\t\tif argType.Kind() != reflect.Ptr {\n\t\t\t\toPtr = reflect.New(argType)\n\t\t\t} else {\n\t\t\t\toPtr = reflect.New(argType.Elem())\n\t\t\t}\n\t\t\tif err := c.Enc.Decode(m.Subject, m.Data, oPtr.Interface()); err != nil {\n\t\t\t\tif c.Conn.Opts.AsyncErrorCB != nil {\n\t\t\t\t\tc.Conn.ach.push(func() {\n\t\t\t\t\t\tc.Conn.Opts.AsyncErrorCB(c.Conn, m.Sub, errors.New(\"nats: Got an error trying to unmarshal: \"+err.Error()))\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif argType.Kind() != reflect.Ptr {\n\t\t\t\toPtr = reflect.Indirect(oPtr)\n\t\t\t}\n\n\t\t\t// Callback Arity\n\t\t\tswitch numArgs {\n\t\t\tcase 1:\n\t\t\t\toV = []reflect.Value{oPtr}\n\t\t\tcase 2:\n\t\t\t\tsubV := reflect.ValueOf(m.Subject)\n\t\t\t\toV = []reflect.Value{subV, oPtr}\n\t\t\tcase 3:\n\t\t\t\tsubV := reflect.ValueOf(m.Subject)\n\t\t\t\treplyV := reflect.ValueOf(m.Reply)\n\t\t\t\toV = []reflect.Value{subV, replyV, oPtr}\n\t\t\t}\n\n\t\t}\n\t\tcbValue.Call(oV)\n\t}\n\n\treturn c.Conn.subscribe(subject, queue, natsCB, nil, nil, false, nil)\n}\n\n// FlushTimeout allows a Flush operation to have an associated timeout.\n//\n// Deprecated: Encoded connections are no longer supported.\nfunc (c *EncodedConn) FlushTimeout(timeout time.Duration) (err error) {\n\treturn c.Conn.FlushTimeout(timeout)\n}\n\n// Flush will perform a round trip to the server and return when it\n// receives the internal reply.\n//\n// Deprecated: Encoded connections are no longer supported.\nfunc (c *EncodedConn) Flush() error {\n\treturn c.Conn.Flush()\n}\n\n// Close will close the connection to the server. This call will release\n// all blocking calls, such as Flush(), etc.\n//\n// Deprecated: Encoded connections are no longer supported.\nfunc (c *EncodedConn) Close() {\n\tc.Conn.Close()\n}\n\n// Drain will put a connection into a drain state. All subscriptions will\n// immediately be put into a drain state. Upon completion, the publishers\n// will be drained and can not publish any additional messages. Upon draining\n// of the publishers, the connection will be closed. Use the ClosedCB()\n// option to know when the connection has moved from draining to closed.\n//\n// Deprecated: Encoded connections are no longer supported.\nfunc (c *EncodedConn) Drain() error {\n\treturn c.Conn.Drain()\n}\n\n// LastError reports the last error encountered via the Connection.\n//\n// Deprecated: Encoded connections are no longer supported.\nfunc (c *EncodedConn) LastError() error {\n\treturn c.Conn.LastError()\n}\n"
        },
        {
          "name": "encoders",
          "type": "tree",
          "content": null
        },
        {
          "name": "example_test.go",
          "type": "blob",
          "size": 21.2109375,
          "content": "// Copyright 2012-2023 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nats_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/nats-io/nats.go\"\n)\n\n// Shows different ways to create a Conn.\nfunc ExampleConnect() {\n\tnc, _ := nats.Connect(\"demo.nats.io\")\n\tnc.Close()\n\n\tnc, _ = nats.Connect(\"nats://derek:secretpassword@demo.nats.io:4222\")\n\tnc.Close()\n\n\tnc, _ = nats.Connect(\"tls://derek:secretpassword@demo.nats.io:4443\")\n\tnc.Close()\n\n\topts := nats.Options{\n\t\tAllowReconnect: true,\n\t\tMaxReconnect:   10,\n\t\tReconnectWait:  5 * time.Second,\n\t\tTimeout:        1 * time.Second,\n\t}\n\n\tnc, _ = opts.Connect()\n\tnc.Close()\n}\n\ntype skipTLSDialer struct {\n\tdialer  *net.Dialer\n\tskipTLS bool\n}\n\nfunc (sd *skipTLSDialer) Dial(network, address string) (net.Conn, error) {\n\treturn sd.dialer.Dial(network, address)\n}\n\nfunc (sd *skipTLSDialer) SkipTLSHandshake() bool {\n\treturn sd.skipTLS\n}\n\nfunc ExampleCustomDialer() {\n\t// Given the following CustomDialer implementation:\n\t//\n\t//  type skipTLSDialer struct {\n\t//  \t    dialer  *net.Dialer\n\t//  \t    skipTLS bool\n\t//  }\n\t//\n\t//  func (sd *skipTLSDialer) Dial(network, address string) (net.Conn, error) {\n\t//  \t    return sd.dialer.Dial(network, address)\n\t//  }\n\t//\n\t//  func (sd *skipTLSDialer) SkipTLSHandshake() bool {\n\t//  \t    return true\n\t//  }\n\t//\n\tsd := &skipTLSDialer{dialer: &net.Dialer{Timeout: 2 * time.Second}, skipTLS: true}\n\tnc, _ := nats.Connect(\"demo.nats.io\", nats.SetCustomDialer(sd))\n\tdefer nc.Close()\n}\n\n// This Example shows an asynchronous subscriber.\nfunc ExampleConn_Subscribe() {\n\tnc, _ := nats.Connect(nats.DefaultURL)\n\tdefer nc.Close()\n\n\tnc.Subscribe(\"foo\", func(m *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(m.Data))\n\t})\n}\n\nfunc ExampleConn_ForceReconnect() {\n\tnc, _ := nats.Connect(nats.DefaultURL)\n\tdefer nc.Close()\n\n\tnc.Subscribe(\"foo\", func(m *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(m.Data))\n\t})\n\n\t// Reconnect to the server.\n\t// the subscription will be recreated after the reconnect.\n\tnc.ForceReconnect()\n}\n\n// This Example shows a synchronous subscriber.\nfunc ExampleConn_SubscribeSync() {\n\tnc, _ := nats.Connect(nats.DefaultURL)\n\tdefer nc.Close()\n\n\tsub, _ := nc.SubscribeSync(\"foo\")\n\tm, err := sub.NextMsg(1 * time.Second)\n\tif err == nil {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(m.Data))\n\t} else {\n\t\tfmt.Println(\"NextMsg timed out.\")\n\t}\n}\n\nfunc ExampleSubscription_NextMsg() {\n\tnc, _ := nats.Connect(nats.DefaultURL)\n\tdefer nc.Close()\n\n\tsub, _ := nc.SubscribeSync(\"foo\")\n\tm, err := sub.NextMsg(1 * time.Second)\n\tif err == nil {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(m.Data))\n\t} else {\n\t\tfmt.Println(\"NextMsg timed out.\")\n\t}\n}\n\nfunc ExampleSubscription_Unsubscribe() {\n\tnc, _ := nats.Connect(nats.DefaultURL)\n\tdefer nc.Close()\n\n\tsub, _ := nc.SubscribeSync(\"foo\")\n\t// ...\n\tsub.Unsubscribe()\n}\n\nfunc ExampleConn_Publish() {\n\tnc, _ := nats.Connect(nats.DefaultURL)\n\tdefer nc.Close()\n\n\tnc.Publish(\"foo\", []byte(\"Hello World!\"))\n}\n\nfunc ExampleConn_PublishMsg() {\n\tnc, _ := nats.Connect(nats.DefaultURL)\n\tdefer nc.Close()\n\n\tmsg := &nats.Msg{Subject: \"foo\", Reply: \"bar\", Data: []byte(\"Hello World!\")}\n\tnc.PublishMsg(msg)\n}\n\nfunc ExampleConn_Flush() {\n\tnc, _ := nats.Connect(nats.DefaultURL)\n\tdefer nc.Close()\n\n\tmsg := &nats.Msg{Subject: \"foo\", Reply: \"bar\", Data: []byte(\"Hello World!\")}\n\tfor i := 0; i < 1000; i++ {\n\t\tnc.PublishMsg(msg)\n\t}\n\terr := nc.Flush()\n\tif err == nil {\n\t\t// Everything has been processed by the server for nc *Conn.\n\t}\n}\n\nfunc ExampleConn_FlushTimeout() {\n\tnc, _ := nats.Connect(nats.DefaultURL)\n\tdefer nc.Close()\n\n\tmsg := &nats.Msg{Subject: \"foo\", Reply: \"bar\", Data: []byte(\"Hello World!\")}\n\tfor i := 0; i < 1000; i++ {\n\t\tnc.PublishMsg(msg)\n\t}\n\t// Only wait for up to 1 second for Flush\n\terr := nc.FlushTimeout(1 * time.Second)\n\tif err == nil {\n\t\t// Everything has been processed by the server for nc *Conn.\n\t}\n}\n\nfunc ExampleConn_Request() {\n\tnc, _ := nats.Connect(nats.DefaultURL)\n\tdefer nc.Close()\n\n\tnc.Subscribe(\"foo\", func(m *nats.Msg) {\n\t\tnc.Publish(m.Reply, []byte(\"I will help you\"))\n\t})\n\tnc.Request(\"foo\", []byte(\"help\"), 50*time.Millisecond)\n}\n\nfunc ExampleConn_QueueSubscribe() {\n\tnc, _ := nats.Connect(nats.DefaultURL)\n\tdefer nc.Close()\n\n\treceived := 0\n\n\tnc.QueueSubscribe(\"foo\", \"worker_group\", func(_ *nats.Msg) {\n\t\treceived++\n\t})\n}\n\nfunc ExampleSubscription_AutoUnsubscribe() {\n\tnc, _ := nats.Connect(nats.DefaultURL)\n\tdefer nc.Close()\n\n\treceived, wanted, total := 0, 10, 100\n\n\tsub, _ := nc.Subscribe(\"foo\", func(_ *nats.Msg) {\n\t\treceived++\n\t})\n\tsub.AutoUnsubscribe(wanted)\n\n\tfor i := 0; i < total; i++ {\n\t\tnc.Publish(\"foo\", []byte(\"Hello\"))\n\t}\n\tnc.Flush()\n\n\tfmt.Printf(\"Received = %d\", received)\n}\n\nfunc ExampleConn_Close() {\n\tnc, _ := nats.Connect(nats.DefaultURL)\n\tnc.Close()\n}\n\nfunc ExampleJetStream() {\n\tnc, err := nats.Connect(\"localhost\")\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// Use the JetStream context to produce and consumer messages\n\t// that have been persisted.\n\tjs, err := nc.JetStream(nats.PublishAsyncMaxPending(256))\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tjs.AddStream(&nats.StreamConfig{\n\t\tName:     \"FOO\",\n\t\tSubjects: []string{\"foo\"},\n\t})\n\n\tjs.Publish(\"foo\", []byte(\"Hello JS!\"))\n\n\t// Publish messages asynchronously.\n\tfor i := 0; i < 500; i++ {\n\t\tjs.PublishAsync(\"foo\", []byte(\"Hello JS Async!\"))\n\t}\n\tselect {\n\tcase <-js.PublishAsyncComplete():\n\tcase <-time.After(5 * time.Second):\n\t\tfmt.Println(\"Did not resolve in time\")\n\t}\n\n\t// Create async consumer on subject 'foo'. Async subscribers\n\t// ack a message once exiting the callback.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tmeta, _ := msg.Metadata()\n\t\tfmt.Printf(\"Stream Sequence  : %v\\n\", meta.Sequence.Stream)\n\t\tfmt.Printf(\"Consumer Sequence: %v\\n\", meta.Sequence.Consumer)\n\t})\n\n\t// Async subscriber with manual acks.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tmsg.Ack()\n\t}, nats.ManualAck())\n\n\t// Async queue subscription where members load balance the\n\t// received messages together.\n\t// If no consumer name is specified, either with nats.Bind()\n\t// or nats.Durable() options, the queue name is used as the\n\t// durable name (that is, as if you were passing the\n\t// nats.Durable(<queue group name>) option.\n\t// It is recommended to use nats.Bind() or nats.Durable()\n\t// and preferably create the JetStream consumer beforehand\n\t// (using js.AddConsumer) so that the JS consumer is not\n\t// deleted on an Unsubscribe() or Drain() when the member\n\t// that created the consumer goes away first.\n\t// Check Godoc for the QueueSubscribe() API for more details.\n\tjs.QueueSubscribe(\"foo\", \"group\", func(msg *nats.Msg) {\n\t\tmsg.Ack()\n\t}, nats.ManualAck())\n\n\t// Subscriber to consume messages synchronously.\n\tsub, _ := js.SubscribeSync(\"foo\")\n\tmsg, _ := sub.NextMsg(2 * time.Second)\n\tmsg.Ack()\n\n\t// We can add a member to the group, with this member using\n\t// the synchronous version of the QueueSubscribe.\n\tsub, _ = js.QueueSubscribeSync(\"foo\", \"group\")\n\tmsg, _ = sub.NextMsg(2 * time.Second)\n\tmsg.Ack()\n\n\t// ChanSubscribe\n\tmsgCh := make(chan *nats.Msg, 8192)\n\tsub, _ = js.ChanSubscribe(\"foo\", msgCh)\n\n\tselect {\n\tcase msg := <-msgCh:\n\t\tfmt.Println(\"[Received]\", msg)\n\tcase <-time.After(1 * time.Second):\n\t}\n\n\t// Create Pull based consumer with maximum 128 inflight.\n\tsub, _ = js.PullSubscribe(\"foo\", \"wq\", nats.PullMaxWaiting(128))\n\n\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n\tdefer cancel()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tdefault:\n\t\t}\n\n\t\tmsgs, _ := sub.Fetch(10, nats.Context(ctx))\n\t\tfor _, msg := range msgs {\n\t\t\tmsg.Ack()\n\t\t}\n\t}\n}\n\n// A JetStream context can be configured with a default timeout using nats.MaxWait\n// or with a custom API prefix in case of using an imported JetStream from another account.\nfunc ExampleJSOpt() {\n\tnc, err := nats.Connect(\"localhost\")\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// Use the JetStream context to manage streams and consumers (with nats.APIPrefix JSOpt)\n\tjs, err := nc.JetStream(nats.APIPrefix(\"dlc\"), nats.MaxWait(5*time.Second))\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tsub, _ := js.SubscribeSync(\"foo\")\n\tjs.Publish(\"foo\", []byte(\"Hello JS!\"))\n\tsub.NextMsg(2 * time.Second)\n}\n\nfunc ExampleJetStreamManager() {\n\tnc, _ := nats.Connect(\"localhost\")\n\n\tjs, _ := nc.JetStream()\n\n\t// Create a stream\n\tjs.AddStream(&nats.StreamConfig{\n\t\tName:     \"FOO\",\n\t\tSubjects: []string{\"foo\"},\n\t\tMaxBytes: 1024,\n\t})\n\n\t// Update a stream\n\tjs.UpdateStream(&nats.StreamConfig{\n\t\tName:     \"FOO\",\n\t\tMaxBytes: 2048,\n\t})\n\n\t// Create a durable consumer\n\tjs.AddConsumer(\"FOO\", &nats.ConsumerConfig{\n\t\tDurable: \"BAR\",\n\t})\n\n\t// Get information about all streams (with Context JSOpt)\n\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n\tdefer cancel()\n\tfor info := range js.StreamsInfo(nats.Context(ctx)) {\n\t\tfmt.Println(\"stream name:\", info.Config.Name)\n\t}\n\n\t// Get information about all consumers (with MaxWait JSOpt)\n\tfor info := range js.ConsumersInfo(\"FOO\", nats.MaxWait(10*time.Second)) {\n\t\tfmt.Println(\"consumer name:\", info.Name)\n\t}\n\n\t// Delete a consumer\n\tjs.DeleteConsumer(\"FOO\", \"BAR\")\n\n\t// Delete a stream\n\tjs.DeleteStream(\"FOO\")\n}\n\n// A JetStreamContext is the composition of a JetStream and JetStreamManagement interfaces.\n// In case of only requiring publishing/consuming messages, can create a context that\n// only uses the JetStream interface.\nfunc ExampleJetStreamContext() {\n\tnc, _ := nats.Connect(\"localhost\")\n\n\tvar js nats.JetStream\n\tvar jsm nats.JetStreamManager\n\tvar jsctx nats.JetStreamContext\n\n\t// JetStream that can publish/subscribe but cannot manage streams.\n\tjs, _ = nc.JetStream()\n\tjs.Publish(\"foo\", []byte(\"hello\"))\n\n\t// JetStream context that can manage streams/consumers but cannot produce messages.\n\tjsm, _ = nc.JetStream()\n\tjsm.AddStream(&nats.StreamConfig{Name: \"FOO\"})\n\n\t// JetStream context that can both manage streams/consumers\n\t// as well as publish/subscribe.\n\tjsctx, _ = nc.JetStream()\n\tjsctx.AddStream(&nats.StreamConfig{Name: \"BAR\"})\n\tjsctx.Publish(\"bar\", []byte(\"hello world\"))\n}\n\nfunc ExamplePubOpt() {\n\tnc, err := nats.Connect(\"localhost\")\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// Create JetStream context to produce/consumer messages that will be persisted.\n\tjs, err := nc.JetStream()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// Create stream to persist messages published on 'foo'.\n\tjs.AddStream(&nats.StreamConfig{\n\t\tName:     \"FOO\",\n\t\tSubjects: []string{\"foo\"},\n\t})\n\n\t// Publish is synchronous by default, and waits for a PubAck response.\n\tjs.Publish(\"foo\", []byte(\"Hello JS!\"))\n\n\t// Publish with a custom timeout.\n\tjs.Publish(\"foo\", []byte(\"Hello JS!\"), nats.AckWait(500*time.Millisecond))\n\n\t// Publish with a context.\n\tctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)\n\tdefer cancel()\n\n\tjs.Publish(\"foo\", []byte(\"Hello JS!\"), nats.Context(ctx))\n\n\t// Publish and assert the expected stream name.\n\tjs.Publish(\"foo\", []byte(\"Hello JS!\"), nats.ExpectStream(\"FOO\"))\n\n\t// Publish and assert the last sequence.\n\tjs.Publish(\"foo\", []byte(\"Hello JS!\"), nats.ExpectLastSequence(5))\n\n\t// Publish and tag the message with an ID.\n\tjs.Publish(\"foo\", []byte(\"Hello JS!\"), nats.MsgId(\"foo:6\"))\n\n\t// Publish and assert the last msg ID.\n\tjs.Publish(\"foo\", []byte(\"Hello JS!\"), nats.ExpectLastMsgId(\"foo:6\"))\n}\n\nfunc ExampleSubOpt() {\n\tnc, err := nats.Connect(\"localhost\")\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// Create JetStream context to produce/consumer messages that will be persisted.\n\tjs, err := nc.JetStream()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// Auto-ack each individual message.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(msg.Data))\n\t})\n\n\t// Auto-ack current sequence and all below.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(msg.Data))\n\t}, nats.AckAll())\n\n\t// Auto-ack each individual message.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(msg.Data))\n\t}, nats.AckExplicit())\n\n\t// Acks are not required.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(msg.Data))\n\t}, nats.AckNone())\n\n\t// Manually acknowledge messages.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tmsg.Ack()\n\t}, nats.ManualAck())\n\n\t// Bind to an existing stream.\n\tsub, _ := js.SubscribeSync(\"origin\", nats.BindStream(\"m1\"))\n\tmsg, _ := sub.NextMsg(2 * time.Second)\n\tmsg.Ack()\n\n\t// Deliver all messages from the beginning.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(msg.Data))\n\t}, nats.DeliverAll())\n\n\t// Deliver messages starting from the last one.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(msg.Data))\n\t}, nats.DeliverLast())\n\n\t// Deliver only new messages that arrive after subscription.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(msg.Data))\n\t}, nats.DeliverNew())\n\n\t// Create durable consumer FOO, if it doesn't exist.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(msg.Data))\n\t}, nats.Durable(\"FOO\"))\n\n\t// Create consumer on Foo with flow control and heartbeats.\n\tjs.SubscribeSync(\"foo\",\n\t\t// Redeliver after 30s\n\t\tnats.AckWait(30*time.Second),\n\t\t// Redeliver only once\n\t\tnats.MaxDeliver(1),\n\t\t// Activate Flow control algorithm from the server.\n\t\tnats.EnableFlowControl(),\n\t\t// Track heartbeats from the server for missed sequences.\n\t\tnats.IdleHeartbeat(500*time.Millisecond),\n\t)\n\n\t// Set the allowable number of outstanding acks.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(msg.Data))\n\t}, nats.MaxAckPending(5))\n\n\t// Set the number of redeliveries for a message.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(msg.Data))\n\t}, nats.MaxDeliver(5))\n\n\t// Set the number the max inflight pull requests.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(msg.Data))\n\t}, nats.PullMaxWaiting(5))\n\n\t// Set the number the max inflight pull requests.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(msg.Data))\n\t}, nats.PullMaxWaiting(5))\n\n\t// Set the rate limit on a push consumer.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(msg.Data))\n\t}, nats.RateLimit(1024))\n\n\t// Replay messages at original speed, instead of as fast as possible.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(msg.Data))\n\t}, nats.ReplayOriginal())\n\n\t// Start delivering messages at a given sequence.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(msg.Data))\n\t}, nats.StartSequence(10))\n\n\t// Start delivering messages at a given time.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(msg.Data))\n\t}, nats.StartTime(time.Now().Add(-2*time.Hour)))\n\n\t// Start delivering messages with delay based on BackOff array of time durations.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(msg.Data))\n\t}, nats.ManualAck(), nats.MaxDeliver(2), nats.BackOff([]time.Duration{50 * time.Millisecond, 250 * time.Millisecond}))\n\n\t// Set consumer replicas count for a durable while subscribing.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(msg.Data))\n\t}, nats.Durable(\"FOO\"), nats.ConsumerReplicas(1))\n\n\t// Force memory storage while subscribing.\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(msg.Data))\n\t}, nats.Durable(\"FOO\"), nats.ConsumerMemoryStorage())\n\n\t// Skip consumer lookup when using explicit consumer name\n\tjs.Subscribe(\"foo\", func(msg *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(msg.Data))\n\t}, nats.Durable(\"FOO\"), nats.SkipConsumerLookup())\n\n\t// Use multiple subject filters.\n\tjs.Subscribe(\"\", func(msg *nats.Msg) {\n\t\tfmt.Printf(\"Received a message: %s\\n\", string(msg.Data))\n\t}, nats.Durable(\"FOO\"), nats.ConsumerFilterSubjects(\"foo\", \"bar\"), nats.BindStream(\"test_stream\"))\n}\n\nfunc ExampleMaxWait() {\n\tnc, _ := nats.Connect(\"localhost\")\n\n\t// Set default timeout for JetStream API requests,\n\t// following requests will inherit this timeout.\n\tjs, _ := nc.JetStream(nats.MaxWait(3 * time.Second))\n\n\t// Set custom timeout for a JetStream API request.\n\tjs.AddStream(&nats.StreamConfig{\n\t\tName:     \"FOO\",\n\t\tSubjects: []string{\"foo\"},\n\t}, nats.MaxWait(2*time.Second))\n\n\tsub, _ := js.PullSubscribe(\"foo\", \"my-durable-name\")\n\n\t// Fetch using the default timeout of 3 seconds.\n\tmsgs, _ := sub.Fetch(1)\n\n\t// Set custom timeout for a pull batch request.\n\tmsgs, _ = sub.Fetch(1, nats.MaxWait(2*time.Second))\n\n\tfor _, msg := range msgs {\n\t\tmsg.Ack()\n\t}\n}\n\nfunc ExampleAckWait() {\n\tnc, _ := nats.Connect(\"localhost\")\n\tjs, _ := nc.JetStream()\n\n\t// Set custom timeout for a JetStream API request.\n\tjs.AddStream(&nats.StreamConfig{\n\t\tName:     \"FOO\",\n\t\tSubjects: []string{\"foo\"},\n\t})\n\n\t// Wait for an ack response for 2 seconds.\n\tjs.Publish(\"foo\", []byte(\"Hello JS!\"), nats.AckWait(2*time.Second))\n\n\t// Create consumer on 'foo' subject that waits for an ack for 10s,\n\t// after which the message will be delivered.\n\tsub, _ := js.SubscribeSync(\"foo\", nats.AckWait(10*time.Second))\n\tmsg, _ := sub.NextMsg(2 * time.Second)\n\n\t// Wait for ack of ack for 2s.\n\tmsg.AckSync(nats.AckWait(2 * time.Second))\n}\n\nfunc ExampleMsg_AckSync() {\n\tnc, _ := nats.Connect(\"localhost\")\n\tjs, _ := nc.JetStream()\n\n\t// Set custom timeout for a JetStream API request.\n\tjs.AddStream(&nats.StreamConfig{\n\t\tName:     \"FOO\",\n\t\tSubjects: []string{\"foo\"},\n\t})\n\n\tsub, _ := js.SubscribeSync(\"foo\")\n\tmsg, _ := sub.NextMsg(2 * time.Second)\n\n\t// Wait for ack of an ack.\n\tmsg.AckSync()\n}\n\n// When a message has been delivered by JetStream, it will be possible\n// to access some of its metadata such as sequence numbers.\nfunc ExampleMsg_Metadata() {\n\tnc, _ := nats.Connect(\"localhost\")\n\tjs, _ := nc.JetStream()\n\n\t// Set custom timeout for a JetStream API request.\n\tjs.AddStream(&nats.StreamConfig{\n\t\tName:     \"FOO\",\n\t\tSubjects: []string{\"foo\"},\n\t})\n\n\tjs.Publish(\"foo\", []byte(\"hello\"))\n\n\tsub, _ := js.SubscribeSync(\"foo\")\n\tmsg, _ := sub.NextMsg(2 * time.Second)\n\n\t//\n\tmeta, _ := msg.Metadata()\n\n\t// Stream and Consumer sequences.\n\tfmt.Printf(\"Stream seq: %s:%d, Consumer seq: %s:%d\\n\", meta.Stream, meta.Sequence.Stream, meta.Consumer, meta.Sequence.Consumer)\n\tfmt.Printf(\"Pending: %d\\n\", meta.NumPending)\n\tfmt.Printf(\"Pending: %d\\n\", meta.NumDelivered)\n}\n\n// AckOpt are the options that can be passed when acknowledge a message.\nfunc ExampleAckOpt() {\n\tnc, err := nats.Connect(\"localhost\")\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// Create JetStream context to produce/consumer messages that will be persisted.\n\tjs, err := nc.JetStream()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// Create stream to persist messages published on 'foo'.\n\tjs.AddStream(&nats.StreamConfig{\n\t\tName:     \"FOO\",\n\t\tSubjects: []string{\"foo\"},\n\t})\n\n\t// Publish is synchronous by default, and waits for a PubAck response.\n\tjs.Publish(\"foo\", []byte(\"Hello JS!\"))\n\n\tsub, _ := js.SubscribeSync(\"foo\")\n\tmsg, _ := sub.NextMsg(2 * time.Second)\n\n\t// Ack and wait for 2 seconds\n\tmsg.InProgress(nats.AckWait(2))\n\n\t// Using a context.\n\tctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)\n\tdefer cancel()\n\tmsg.Ack(nats.Context(ctx))\n}\n\nfunc ExamplePullOpt() {\n\tnc, err := nats.Connect(\"localhost\")\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// Create JetStream context to produce/consumer messages that will be persisted.\n\tjs, err := nc.JetStream()\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// Create stream to persist messages published on 'foo'.\n\tjs.AddStream(&nats.StreamConfig{\n\t\tName:     \"FOO\",\n\t\tSubjects: []string{\"foo\"},\n\t})\n\n\t// Publish is synchronous by default, and waits for a PubAck response.\n\tjs.Publish(\"foo\", []byte(\"Hello JS!\"))\n\n\tsub, _ := js.PullSubscribe(\"foo\", \"wq\")\n\n\t// Pull one message,\n\tmsgs, _ := sub.Fetch(1, nats.MaxWait(2*time.Second))\n\tfor _, msg := range msgs {\n\t\tmsg.Ack()\n\t}\n\n\t// Using a context to timeout waiting for a message.\n\tctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)\n\tdefer cancel()\n\n\tmsgs, _ = sub.Fetch(1, nats.Context(ctx))\n\tfor _, msg := range msgs {\n\t\tmsg.Ack()\n\t}\n}\n\nfunc ExampleContext() {\n\tnc, err := nats.Connect(\"localhost\")\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tjs, _ := nc.JetStream()\n\n\t// Base context\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\t// nats.Context option implements context.Context interface, so can be used\n\t// to create a new context from top level one.\n\tnctx := nats.Context(ctx)\n\n\t// JetStreamManager functions all can use context.\n\tjs.AddStream(&nats.StreamConfig{\n\t\tName:     \"FOO\",\n\t\tSubjects: []string{\"foo\"},\n\t}, nctx)\n\n\t// Custom context with timeout\n\ttctx, tcancel := context.WithTimeout(nctx, 2*time.Second)\n\tdefer tcancel()\n\n\t// Set a timeout for publishing using context.\n\tdeadlineCtx := nats.Context(tctx)\n\n\tjs.Publish(\"foo\", []byte(\"Hello JS!\"), deadlineCtx)\n\tsub, _ := js.SubscribeSync(\"foo\")\n\tmsg, _ := sub.NextMsgWithContext(deadlineCtx)\n\n\t// Acks can also use a context to await for a response.\n\tmsg.Ack(deadlineCtx)\n}\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 0.2724609375,
          "content": "module github.com/nats-io/nats.go\n\ngo 1.20\n\nrequire (\n\tgithub.com/klauspost/compress v1.17.9\n\tgithub.com/nats-io/nkeys v0.4.9\n\tgithub.com/nats-io/nuid v1.0.1\n\tgolang.org/x/text v0.21.0\n)\n\nrequire (\n\tgolang.org/x/crypto v0.31.0 // indirect\n\tgolang.org/x/sys v0.28.0 // indirect\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 1.236328125,
          "content": "github.com/klauspost/compress v1.17.9 h1:6KIumPrER1LHsvBVuDa0r5xaG0Es51mhhB9BQB2qeMA=\ngithub.com/klauspost/compress v1.17.9/go.mod h1:Di0epgTjJY877eYKx5yC51cX2A2Vl2ibi7bDH9ttBbw=\ngithub.com/klauspost/compress v1.17.10 h1:oXAz+Vh0PMUvJczoi+flxpnBEPxoER1IaAnU/NMPtT0=\ngithub.com/klauspost/compress v1.17.10/go.mod h1:pMDklpSncoRMuLFrf1W9Ss9KT+0rH90U12bZKk7uwG0=\ngithub.com/klauspost/compress v1.17.11 h1:In6xLpyWOi1+C7tXUUWv2ot1QvBjxevKAaI6IXrJmUc=\ngithub.com/klauspost/compress v1.17.11/go.mod h1:pMDklpSncoRMuLFrf1W9Ss9KT+0rH90U12bZKk7uwG0=\ngithub.com/nats-io/nkeys v0.4.9 h1:qe9Faq2Gxwi6RZnZMXfmGMZkg3afLLOtrU+gDZJ35b0=\ngithub.com/nats-io/nkeys v0.4.9/go.mod h1:jcMqs+FLG+W5YO36OX6wFIFcmpdAns+w1Wm6D3I/evE=\ngithub.com/nats-io/nuid v1.0.1 h1:5iA8DT8V7q8WK2EScv2padNa/rTESc1KdnPw4TC2paw=\ngithub.com/nats-io/nuid v1.0.1/go.mod h1:19wcPz3Ph3q0Jbyiqsd0kePYG7A95tJPxeL+1OSON2c=\ngolang.org/x/crypto v0.31.0 h1:ihbySMvVjLAeSH1IbfcRTkD/iNscyz8rGzjF/E5hV6U=\ngolang.org/x/crypto v0.31.0/go.mod h1:kDsLvtWBEx7MV9tJOj9bnXsPbxwJQ6csT/x4KIN4Ssk=\ngolang.org/x/sys v0.28.0 h1:Fksou7UEQUWlKvIdsqzJmUmCX3cZuD2+P3XyyzwMhlA=\ngolang.org/x/sys v0.28.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\ngolang.org/x/text v0.21.0/go.mod h1:4IBbMaMmOPCJ8SecivzSH54+73PCFmPWxNTLm+vZkEQ=\n"
        },
        {
          "name": "go_test.mod",
          "type": "blob",
          "size": 0.5908203125,
          "content": "module github.com/nats-io/nats.go\n\ngo 1.21\n\ntoolchain go1.22.5\n\nrequire (\n\tgithub.com/golang/protobuf v1.4.2\n\tgithub.com/klauspost/compress v1.17.9\n\tgithub.com/nats-io/jwt v1.2.2\n\tgithub.com/nats-io/nats-server/v2 v2.10.17\n\tgithub.com/nats-io/nkeys v0.4.9\n\tgithub.com/nats-io/nuid v1.0.1\n\tgo.uber.org/goleak v1.3.0\n\tgolang.org/x/text v0.21.0\n\tgoogle.golang.org/protobuf v1.23.0\n)\n\nrequire (\n\tgithub.com/minio/highwayhash v1.0.2 // indirect\n\tgithub.com/nats-io/jwt/v2 v2.5.7 // indirect\n\tgolang.org/x/crypto v0.31.0 // indirect\n\tgolang.org/x/sys v0.28.0 // indirect\n\tgolang.org/x/time v0.5.0 // indirect\n)\n"
        },
        {
          "name": "go_test.sum",
          "type": "blob",
          "size": 5.3271484375,
          "content": "github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/golang/protobuf v1.4.0-rc.1/go.mod h1:ceaxUfeHdC40wWswd/P6IGgMaK3YpKi5j83Wpe3EHw8=\ngithub.com/golang/protobuf v1.4.0-rc.1.0.20200221234624-67d41d38c208/go.mod h1:xKAWHe0F5eneWXFV3EuXVDTCmh+JuBKY0li0aMyXATA=\ngithub.com/golang/protobuf v1.4.0-rc.2/go.mod h1:LlEzMj4AhA7rCAGe4KMBDvJI+AwstrUpVNzEA03Pprs=\ngithub.com/golang/protobuf v1.4.0-rc.4.0.20200313231945-b860323f09d0/go.mod h1:WU3c8KckQ9AFe+yFwt9sWVRKCVIyN9cPHBJSNnbL67w=\ngithub.com/golang/protobuf v1.4.0/go.mod h1:jodUvKwWbYaEsadDk5Fwe5c77LiNKVO9IDvqG2KuDX0=\ngithub.com/golang/protobuf v1.4.2 h1:+Z5KGCizgyZCbGh1KZqA0fcLLkwbsjIzS4aV2v7wJX0=\ngithub.com/golang/protobuf v1.4.2/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=\ngithub.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.3.1/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.4.0 h1:xsAVV57WRhGj6kEIi8ReJzQlHHqcBYCElAvkovg3B/4=\ngithub.com/google/go-cmp v0.4.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/klauspost/compress v1.17.9 h1:6KIumPrER1LHsvBVuDa0r5xaG0Es51mhhB9BQB2qeMA=\ngithub.com/klauspost/compress v1.17.9/go.mod h1:Di0epgTjJY877eYKx5yC51cX2A2Vl2ibi7bDH9ttBbw=\ngithub.com/minio/highwayhash v1.0.2 h1:Aak5U0nElisjDCfPSG79Tgzkn2gl66NxOMspRrKnA/g=\ngithub.com/minio/highwayhash v1.0.2/go.mod h1:BQskDq+xkJ12lmlUUi7U0M5Swg3EWR+dLTk+kldvVxY=\ngithub.com/nats-io/jwt v1.2.2 h1:w3GMTO969dFg+UOKTmmyuu7IGdusK+7Ytlt//OYH/uU=\ngithub.com/nats-io/jwt v1.2.2/go.mod h1:/xX356yQA6LuXI9xWW7mZNpxgF2mBmGecH+Fj34sP5Q=\ngithub.com/nats-io/jwt/v2 v2.5.7 h1:j5lH1fUXCnJnY8SsQeB/a/z9Azgu2bYIDvtPVNdxe2c=\ngithub.com/nats-io/jwt/v2 v2.5.7/go.mod h1:ZdWS1nZa6WMZfFwwgpEaqBV8EPGVgOTDHN/wTbz0Y5A=\ngithub.com/nats-io/nats-server/v2 v2.10.17 h1:PTVObNBD3TZSNUDgzFb1qQsQX4mOgFmOuG9vhT+KBUY=\ngithub.com/nats-io/nats-server/v2 v2.10.17/go.mod h1:5OUyc4zg42s/p2i92zbbqXvUNsbF0ivdTLKshVMn2YQ=\ngithub.com/nats-io/nkeys v0.2.0/go.mod h1:XdZpAbhgyyODYqjTawOnIOI7VlbKSarI9Gfy1tqEu/s=\ngithub.com/nats-io/nkeys v0.4.9 h1:qe9Faq2Gxwi6RZnZMXfmGMZkg3afLLOtrU+gDZJ35b0=\ngithub.com/nats-io/nkeys v0.4.9/go.mod h1:jcMqs+FLG+W5YO36OX6wFIFcmpdAns+w1Wm6D3I/evE=\ngithub.com/nats-io/nuid v1.0.1 h1:5iA8DT8V7q8WK2EScv2padNa/rTESc1KdnPw4TC2paw=\ngithub.com/nats-io/nuid v1.0.1/go.mod h1:19wcPz3Ph3q0Jbyiqsd0kePYG7A95tJPxeL+1OSON2c=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/stretchr/testify v1.8.0 h1:pSgiaMZlXftHpm5L7V1+rVB+AZJydKsMxsQBIJw4PKk=\ngithub.com/stretchr/testify v1.8.0/go.mod h1:yNjHg4UonilssWZ8iaSj1OCr/vHnekPRkoO+kdMU+MU=\ngo.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=\ngo.uber.org/goleak v1.3.0/go.mod h1:CoHD4mav9JJNrW/WLlf7HGZPjdw8EucARQHekz1X6bE=\ngolang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\ngolang.org/x/crypto v0.0.0-20200323165209-0ec3e9974c59/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\ngolang.org/x/crypto v0.31.0 h1:ihbySMvVjLAeSH1IbfcRTkD/iNscyz8rGzjF/E5hV6U=\ngolang.org/x/crypto v0.31.0/go.mod h1:kDsLvtWBEx7MV9tJOj9bnXsPbxwJQ6csT/x4KIN4Ssk=\ngolang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/sys v0.0.0-20190130150945-aca44879d564/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.28.0 h1:Fksou7UEQUWlKvIdsqzJmUmCX3cZuD2+P3XyyzwMhlA=\ngolang.org/x/sys v0.28.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\ngolang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.21.0 h1:zyQAAkrwaneQ066sspRyJaG9VNi/YJ1NfzcGB3hZ/qo=\ngolang.org/x/text v0.21.0/go.mod h1:4IBbMaMmOPCJ8SecivzSH54+73PCFmPWxNTLm+vZkEQ=\ngolang.org/x/time v0.5.0 h1:o7cqy6amK/52YcAKIPlM3a+Fpj35zvRj2TP+e1xFSfk=\ngolang.org/x/time v0.5.0/go.mod h1:3BpzKBy/shNhVucY/MWOyx10tF3SFh9QdLuxbVysPQM=\ngolang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543 h1:E7g+9GITq07hpfrRu66IVDexMakfv52eLZ2CXBWiKr4=\ngolang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngoogle.golang.org/protobuf v0.0.0-20200109180630-ec00e32a8dfd/go.mod h1:DFci5gLYBciE7Vtevhsrf46CRTquxDuWsQurQQe4oz8=\ngoogle.golang.org/protobuf v0.0.0-20200221191635-4d8936d0db64/go.mod h1:kwYJMbMJ01Woi6D6+Kah6886xMZcty6N08ah7+eCXa0=\ngoogle.golang.org/protobuf v0.0.0-20200228230310-ab0ca4ff8a60/go.mod h1:cfTl7dwQJ+fmap5saPgwCLgHXTUD7jkjRqWcaiX5VyM=\ngoogle.golang.org/protobuf v1.20.1-0.20200309200217-e05f789c0967/go.mod h1:A+miEFZTKqfCUM6K7xSMQL9OKL/b6hQv+e19PK+JZNE=\ngoogle.golang.org/protobuf v1.21.0/go.mod h1:47Nbq4nVaFHyn7ilMalzfO3qCViNmqZ2kzikPIcrTAo=\ngoogle.golang.org/protobuf v1.23.0 h1:4MY060fB1DLGMB/7MBTLnwQUY6+F09GEiz6SsrNqyzM=\ngoogle.golang.org/protobuf v1.23.0/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\n"
        },
        {
          "name": "internal",
          "type": "tree",
          "content": null
        },
        {
          "name": "jetstream",
          "type": "tree",
          "content": null
        },
        {
          "name": "js.go",
          "type": "blob",
          "size": 112.65625,
          "content": "// Copyright 2020-2024 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nats\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto/sha256\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math/rand\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/nats-io/nats.go/internal/parser\"\n\t\"github.com/nats-io/nuid\"\n)\n\n// JetStream allows persistent messaging through JetStream.\n//\n// NOTE: JetStream is part of legacy API.\n// Users are encouraged to switch to the new JetStream API for enhanced capabilities and\n// simplified API. Please refer to the `jetstream` package.\n// See: https://github.com/nats-io/nats.go/blob/main/jetstream/README.md\ntype JetStream interface {\n\t// Publish publishes a message to JetStream.\n\tPublish(subj string, data []byte, opts ...PubOpt) (*PubAck, error)\n\n\t// PublishMsg publishes a Msg to JetStream.\n\tPublishMsg(m *Msg, opts ...PubOpt) (*PubAck, error)\n\n\t// PublishAsync publishes a message to JetStream and returns a PubAckFuture.\n\t// The data should not be changed until the PubAckFuture has been processed.\n\tPublishAsync(subj string, data []byte, opts ...PubOpt) (PubAckFuture, error)\n\n\t// PublishMsgAsync publishes a Msg to JetStream and returns a PubAckFuture.\n\t// The message should not be changed until the PubAckFuture has been processed.\n\tPublishMsgAsync(m *Msg, opts ...PubOpt) (PubAckFuture, error)\n\n\t// PublishAsyncPending returns the number of async publishes outstanding for this context.\n\tPublishAsyncPending() int\n\n\t// PublishAsyncComplete returns a channel that will be closed when all outstanding messages are ack'd.\n\tPublishAsyncComplete() <-chan struct{}\n\n\t// CleanupPublisher will cleanup the publishing side of JetStreamContext.\n\t//\n\t// This will unsubscribe from the internal reply subject if needed.\n\t// All pending async publishes will fail with ErrJetStreamPublisherClosed.\n\t//\n\t// If an error handler was provided, it will be called for each pending async\n\t// publish and PublishAsyncComplete will be closed.\n\t//\n\t// After completing JetStreamContext is still usable - internal subscription\n\t// will be recreated on next publish, but the acks from previous publishes will\n\t// be lost.\n\tCleanupPublisher()\n\n\t// Subscribe creates an async Subscription for JetStream.\n\t// The stream and consumer names can be provided with the nats.Bind() option.\n\t// For creating an ephemeral (where the consumer name is picked by the server),\n\t// you can provide the stream name with nats.BindStream().\n\t// If no stream name is specified, the library will attempt to figure out which\n\t// stream the subscription is for. See important notes below for more details.\n\t//\n\t// IMPORTANT NOTES:\n\t// * If none of the options Bind() nor Durable() are specified, the library will\n\t// send a request to the server to create an ephemeral JetStream consumer,\n\t// which will be deleted after an Unsubscribe() or Drain(), or automatically\n\t// by the server after a short period of time after the NATS subscription is\n\t// gone.\n\t// * If Durable() option is specified, the library will attempt to lookup a JetStream\n\t// consumer with this name, and if found, will bind to it and not attempt to\n\t// delete it. However, if not found, the library will send a request to\n\t// create such durable JetStream consumer. Note that the library will delete\n\t// the JetStream consumer after an Unsubscribe() or Drain() only if it\n\t// created the durable consumer while subscribing. If the durable consumer\n\t// already existed prior to subscribing it won't be deleted.\n\t// * If Bind() option is provided, the library will attempt to lookup the\n\t// consumer with the given name, and if successful, bind to it. If the lookup fails,\n\t// then the Subscribe() call will return an error.\n\tSubscribe(subj string, cb MsgHandler, opts ...SubOpt) (*Subscription, error)\n\n\t// SubscribeSync creates a Subscription that can be used to process messages synchronously.\n\t// See important note in Subscribe()\n\tSubscribeSync(subj string, opts ...SubOpt) (*Subscription, error)\n\n\t// ChanSubscribe creates channel based Subscription.\n\t// See important note in Subscribe()\n\tChanSubscribe(subj string, ch chan *Msg, opts ...SubOpt) (*Subscription, error)\n\n\t// ChanQueueSubscribe creates channel based Subscription with a queue group.\n\t// See important note in QueueSubscribe()\n\tChanQueueSubscribe(subj, queue string, ch chan *Msg, opts ...SubOpt) (*Subscription, error)\n\n\t// QueueSubscribe creates a Subscription with a queue group.\n\t// If no optional durable name nor binding options are specified, the queue name will be used as a durable name.\n\t// See important note in Subscribe()\n\tQueueSubscribe(subj, queue string, cb MsgHandler, opts ...SubOpt) (*Subscription, error)\n\n\t// QueueSubscribeSync creates a Subscription with a queue group that can be used to process messages synchronously.\n\t// See important note in QueueSubscribe()\n\tQueueSubscribeSync(subj, queue string, opts ...SubOpt) (*Subscription, error)\n\n\t// PullSubscribe creates a Subscription that can fetch messages.\n\t// See important note in Subscribe(). Additionally, for an ephemeral pull consumer, the \"durable\" value must be\n\t// set to an empty string.\n\tPullSubscribe(subj, durable string, opts ...SubOpt) (*Subscription, error)\n}\n\n// JetStreamContext allows JetStream messaging and stream management.\n//\n// NOTE: JetStreamContext is part of legacy API.\n// Users are encouraged to switch to the new JetStream API for enhanced capabilities and\n// simplified API. Please refer to the `jetstream` package.\n// See: https://github.com/nats-io/nats.go/blob/main/jetstream/README.md\ntype JetStreamContext interface {\n\tJetStream\n\tJetStreamManager\n\tKeyValueManager\n\tObjectStoreManager\n}\n\n// Request API subjects for JetStream.\nconst (\n\t// defaultAPIPrefix is the default prefix for the JetStream API.\n\tdefaultAPIPrefix = \"$JS.API.\"\n\n\t// jsDomainT is used to create JetStream API prefix by specifying only Domain\n\tjsDomainT = \"$JS.%s.API.\"\n\n\t// jsExtDomainT is used to create a StreamSource External APIPrefix\n\tjsExtDomainT = \"$JS.%s.API\"\n\n\t// apiAccountInfo is for obtaining general information about JetStream.\n\tapiAccountInfo = \"INFO\"\n\n\t// apiConsumerCreateT is used to create consumers.\n\t// it accepts stream name and consumer name.\n\tapiConsumerCreateT = \"CONSUMER.CREATE.%s.%s\"\n\n\t// apiConsumerCreateT is used to create consumers.\n\t// it accepts stream name, consumer name and filter subject\n\tapiConsumerCreateWithFilterSubjectT = \"CONSUMER.CREATE.%s.%s.%s\"\n\n\t// apiLegacyConsumerCreateT is used to create consumers.\n\t// this is a legacy endpoint to support creating ephemerals before nats-server v2.9.0.\n\tapiLegacyConsumerCreateT = \"CONSUMER.CREATE.%s\"\n\n\t// apiDurableCreateT is used to create durable consumers.\n\t// this is a legacy endpoint to support creating durable consumers before nats-server v2.9.0.\n\tapiDurableCreateT = \"CONSUMER.DURABLE.CREATE.%s.%s\"\n\n\t// apiConsumerInfoT is used to create consumers.\n\tapiConsumerInfoT = \"CONSUMER.INFO.%s.%s\"\n\n\t// apiRequestNextT is the prefix for the request next message(s) for a consumer in worker/pull mode.\n\tapiRequestNextT = \"CONSUMER.MSG.NEXT.%s.%s\"\n\n\t// apiConsumerDeleteT is used to delete consumers.\n\tapiConsumerDeleteT = \"CONSUMER.DELETE.%s.%s\"\n\n\t// apiConsumerListT is used to return all detailed consumer information\n\tapiConsumerListT = \"CONSUMER.LIST.%s\"\n\n\t// apiConsumerNamesT is used to return a list with all consumer names for the stream.\n\tapiConsumerNamesT = \"CONSUMER.NAMES.%s\"\n\n\t// apiStreams can lookup a stream by subject.\n\tapiStreams = \"STREAM.NAMES\"\n\n\t// apiStreamCreateT is the endpoint to create new streams.\n\tapiStreamCreateT = \"STREAM.CREATE.%s\"\n\n\t// apiStreamInfoT is the endpoint to get information on a stream.\n\tapiStreamInfoT = \"STREAM.INFO.%s\"\n\n\t// apiStreamUpdateT is the endpoint to update existing streams.\n\tapiStreamUpdateT = \"STREAM.UPDATE.%s\"\n\n\t// apiStreamDeleteT is the endpoint to delete streams.\n\tapiStreamDeleteT = \"STREAM.DELETE.%s\"\n\n\t// apiStreamPurgeT is the endpoint to purge streams.\n\tapiStreamPurgeT = \"STREAM.PURGE.%s\"\n\n\t// apiStreamListT is the endpoint that will return all detailed stream information\n\tapiStreamListT = \"STREAM.LIST\"\n\n\t// apiMsgGetT is the endpoint to get a message.\n\tapiMsgGetT = \"STREAM.MSG.GET.%s\"\n\n\t// apiMsgGetT is the endpoint to perform a direct get of a message.\n\tapiDirectMsgGetT = \"DIRECT.GET.%s\"\n\n\t// apiDirectMsgGetLastBySubjectT is the endpoint to perform a direct get of a message by subject.\n\tapiDirectMsgGetLastBySubjectT = \"DIRECT.GET.%s.%s\"\n\n\t// apiMsgDeleteT is the endpoint to remove a message.\n\tapiMsgDeleteT = \"STREAM.MSG.DELETE.%s\"\n\n\t// orderedHeartbeatsInterval is how fast we want HBs from the server during idle.\n\torderedHeartbeatsInterval = 5 * time.Second\n\n\t// Scale for threshold of missed HBs or lack of activity.\n\thbcThresh = 2\n\n\t// For ChanSubscription, we can't update sub.delivered as we do for other\n\t// type of subscriptions, since the channel is user provided.\n\t// With flow control in play, we will check for flow control on incoming\n\t// messages (as opposed to when they are delivered), but also from a go\n\t// routine. Without this, the subscription would possibly stall until\n\t// a new message or heartbeat/fc are received.\n\tchanSubFCCheckInterval = 250 * time.Millisecond\n\n\t// Default time wait between retries on Publish iff err is NoResponders.\n\tDefaultPubRetryWait = 250 * time.Millisecond\n\n\t// Default number of retries\n\tDefaultPubRetryAttempts = 2\n\n\t// defaultAsyncPubAckInflight is the number of async pub acks inflight.\n\tdefaultAsyncPubAckInflight = 4000\n)\n\n// Types of control messages, so far heartbeat and flow control\nconst (\n\tjsCtrlHB = 1\n\tjsCtrlFC = 2\n)\n\n// js is an internal struct from a JetStreamContext.\ntype js struct {\n\tnc   *Conn\n\topts *jsOpts\n\n\t// For async publish context.\n\tmu             sync.RWMutex\n\trpre           string\n\trsub           *Subscription\n\tpafs           map[string]*pubAckFuture\n\tstc            chan struct{}\n\tdch            chan struct{}\n\trr             *rand.Rand\n\tconnStatusCh   chan (Status)\n\treplyPrefix    string\n\treplyPrefixLen int\n}\n\ntype jsOpts struct {\n\tctx context.Context\n\t// For importing JetStream from other accounts.\n\tpre string\n\t// Amount of time to wait for API requests.\n\twait time.Duration\n\t// For async publish error handling.\n\taecb MsgErrHandler\n\t// Max async pub ack in flight\n\tmaxpa int\n\t// the domain that produced the pre\n\tdomain string\n\t// enables protocol tracing\n\tctrace      ClientTrace\n\tshouldTrace bool\n\t// purgeOpts contains optional stream purge options\n\tpurgeOpts *StreamPurgeRequest\n\t// streamInfoOpts contains optional stream info options\n\tstreamInfoOpts *StreamInfoRequest\n\t// streamListSubject is used for subject filtering when listing streams / stream names\n\tstreamListSubject string\n\t// For direct get message requests\n\tdirectGet bool\n\t// For direct get next message\n\tdirectNextFor string\n\n\t// featureFlags are used to enable/disable specific JetStream features\n\tfeatureFlags featureFlags\n}\n\nconst (\n\tdefaultRequestWait  = 5 * time.Second\n\tdefaultAccountCheck = 20 * time.Second\n)\n\n// JetStream returns a JetStreamContext for messaging and stream management.\n// Errors are only returned if inconsistent options are provided.\n//\n// NOTE: JetStreamContext is part of legacy API.\n// Users are encouraged to switch to the new JetStream API for enhanced capabilities and\n// simplified API. Please refer to the `jetstream` package.\n// See: https://github.com/nats-io/nats.go/blob/main/jetstream/README.md\nfunc (nc *Conn) JetStream(opts ...JSOpt) (JetStreamContext, error) {\n\tjs := &js{\n\t\tnc: nc,\n\t\topts: &jsOpts{\n\t\t\tpre:   defaultAPIPrefix,\n\t\t\twait:  defaultRequestWait,\n\t\t\tmaxpa: defaultAsyncPubAckInflight,\n\t\t},\n\t}\n\tinboxPrefix := InboxPrefix\n\tif js.nc.Opts.InboxPrefix != _EMPTY_ {\n\t\tinboxPrefix = js.nc.Opts.InboxPrefix + \".\"\n\t}\n\tjs.replyPrefix = inboxPrefix\n\tjs.replyPrefixLen = len(js.replyPrefix) + aReplyTokensize + 1\n\n\tfor _, opt := range opts {\n\t\tif err := opt.configureJSContext(js.opts); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\treturn js, nil\n}\n\n// JSOpt configures a JetStreamContext.\ntype JSOpt interface {\n\tconfigureJSContext(opts *jsOpts) error\n}\n\n// jsOptFn configures an option for the JetStreamContext.\ntype jsOptFn func(opts *jsOpts) error\n\nfunc (opt jsOptFn) configureJSContext(opts *jsOpts) error {\n\treturn opt(opts)\n}\n\ntype featureFlags struct {\n\tuseDurableConsumerCreate bool\n}\n\n// UseLegacyDurableConsumers makes JetStream use the legacy (pre nats-server v2.9.0) subjects for consumer creation.\n// If this option is used when creating JetStremContext, $JS.API.CONSUMER.DURABLE.CREATE.<stream>.<consumer> will be used\n// to create a consumer with Durable provided, rather than $JS.API.CONSUMER.CREATE.<stream>.<consumer>.\nfunc UseLegacyDurableConsumers() JSOpt {\n\treturn jsOptFn(func(opts *jsOpts) error {\n\t\topts.featureFlags.useDurableConsumerCreate = true\n\t\treturn nil\n\t})\n}\n\n// ClientTrace can be used to trace API interactions for the JetStream Context.\ntype ClientTrace struct {\n\tRequestSent      func(subj string, payload []byte)\n\tResponseReceived func(subj string, payload []byte, hdr Header)\n}\n\nfunc (ct ClientTrace) configureJSContext(js *jsOpts) error {\n\tjs.ctrace = ct\n\tjs.shouldTrace = true\n\treturn nil\n}\n\n// Domain changes the domain part of JetStream API prefix.\nfunc Domain(domain string) JSOpt {\n\tif domain == _EMPTY_ {\n\t\treturn APIPrefix(_EMPTY_)\n\t}\n\n\treturn jsOptFn(func(js *jsOpts) error {\n\t\tjs.domain = domain\n\t\tjs.pre = fmt.Sprintf(jsDomainT, domain)\n\n\t\treturn nil\n\t})\n\n}\n\nfunc (s *StreamPurgeRequest) configureJSContext(js *jsOpts) error {\n\tjs.purgeOpts = s\n\treturn nil\n}\n\nfunc (s *StreamInfoRequest) configureJSContext(js *jsOpts) error {\n\tjs.streamInfoOpts = s\n\treturn nil\n}\n\n// APIPrefix changes the default prefix used for the JetStream API.\nfunc APIPrefix(pre string) JSOpt {\n\treturn jsOptFn(func(js *jsOpts) error {\n\t\tif pre == _EMPTY_ {\n\t\t\treturn nil\n\t\t}\n\n\t\tjs.pre = pre\n\t\tif !strings.HasSuffix(js.pre, \".\") {\n\t\t\tjs.pre = js.pre + \".\"\n\t\t}\n\n\t\treturn nil\n\t})\n}\n\n// DirectGet is an option that can be used to make GetMsg() or GetLastMsg()\n// retrieve message directly from a group of servers (leader and replicas)\n// if the stream was created with the AllowDirect option.\nfunc DirectGet() JSOpt {\n\treturn jsOptFn(func(js *jsOpts) error {\n\t\tjs.directGet = true\n\t\treturn nil\n\t})\n}\n\n// DirectGetNext is an option that can be used to make GetMsg() retrieve message\n// directly from a group of servers (leader and replicas) if the stream was\n// created with the AllowDirect option.\n// The server will find the next message matching the filter `subject` starting\n// at the start sequence (argument in GetMsg()). The filter `subject` can be a\n// wildcard.\nfunc DirectGetNext(subject string) JSOpt {\n\treturn jsOptFn(func(js *jsOpts) error {\n\t\tjs.directGet = true\n\t\tjs.directNextFor = subject\n\t\treturn nil\n\t})\n}\n\n// StreamListFilter is an option that can be used to configure `StreamsInfo()` and `StreamNames()` requests.\n// It allows filtering the returned streams by subject associated with each stream.\n// Wildcards can be used. For example, `StreamListFilter(FOO.*.A) will return\n// all streams which have at least one subject matching the provided pattern (e.g. FOO.TEST.A).\nfunc StreamListFilter(subject string) JSOpt {\n\treturn jsOptFn(func(opts *jsOpts) error {\n\t\topts.streamListSubject = subject\n\t\treturn nil\n\t})\n}\n\nfunc (js *js) apiSubj(subj string) string {\n\tif js.opts.pre == _EMPTY_ {\n\t\treturn subj\n\t}\n\tvar b strings.Builder\n\tb.WriteString(js.opts.pre)\n\tb.WriteString(subj)\n\treturn b.String()\n}\n\n// PubOpt configures options for publishing JetStream messages.\ntype PubOpt interface {\n\tconfigurePublish(opts *pubOpts) error\n}\n\n// pubOptFn is a function option used to configure JetStream Publish.\ntype pubOptFn func(opts *pubOpts) error\n\nfunc (opt pubOptFn) configurePublish(opts *pubOpts) error {\n\treturn opt(opts)\n}\n\ntype pubOpts struct {\n\tctx context.Context\n\tttl time.Duration\n\tid  string\n\tlid string  // Expected last msgId\n\tstr string  // Expected stream name\n\tseq *uint64 // Expected last sequence\n\tlss *uint64 // Expected last sequence per subject\n\n\t// Publish retries for NoResponders err.\n\trwait time.Duration // Retry wait between attempts\n\trnum  int           // Retry attempts\n\n\t// stallWait is the max wait of a async pub ack.\n\tstallWait time.Duration\n\n\t// internal option to re-use existing paf in case of retry.\n\tpafRetry *pubAckFuture\n}\n\n// pubAckResponse is the ack response from the JetStream API when publishing a message.\ntype pubAckResponse struct {\n\tapiResponse\n\t*PubAck\n}\n\n// PubAck is an ack received after successfully publishing a message.\ntype PubAck struct {\n\tStream    string `json:\"stream\"`\n\tSequence  uint64 `json:\"seq\"`\n\tDuplicate bool   `json:\"duplicate,omitempty\"`\n\tDomain    string `json:\"domain,omitempty\"`\n}\n\n// Headers for published messages.\nconst (\n\tMsgIdHdr               = \"Nats-Msg-Id\"\n\tExpectedStreamHdr      = \"Nats-Expected-Stream\"\n\tExpectedLastSeqHdr     = \"Nats-Expected-Last-Sequence\"\n\tExpectedLastSubjSeqHdr = \"Nats-Expected-Last-Subject-Sequence\"\n\tExpectedLastMsgIdHdr   = \"Nats-Expected-Last-Msg-Id\"\n\tMsgRollup              = \"Nats-Rollup\"\n)\n\n// Headers for republished messages and direct gets.\nconst (\n\tJSStream       = \"Nats-Stream\"\n\tJSSequence     = \"Nats-Sequence\"\n\tJSTimeStamp    = \"Nats-Time-Stamp\"\n\tJSSubject      = \"Nats-Subject\"\n\tJSLastSequence = \"Nats-Last-Sequence\"\n)\n\n// MsgSize is a header that will be part of a consumer's delivered message if HeadersOnly requested.\nconst MsgSize = \"Nats-Msg-Size\"\n\n// Rollups, can be subject only or all messages.\nconst (\n\tMsgRollupSubject = \"sub\"\n\tMsgRollupAll     = \"all\"\n)\n\n// PublishMsg publishes a Msg to a stream from JetStream.\nfunc (js *js) PublishMsg(m *Msg, opts ...PubOpt) (*PubAck, error) {\n\tvar o = pubOpts{rwait: DefaultPubRetryWait, rnum: DefaultPubRetryAttempts}\n\tif len(opts) > 0 {\n\t\tif m.Header == nil {\n\t\t\tm.Header = Header{}\n\t\t}\n\t\tfor _, opt := range opts {\n\t\t\tif err := opt.configurePublish(&o); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\t// Check for option collisions. Right now just timeout and context.\n\tif o.ctx != nil && o.ttl != 0 {\n\t\treturn nil, ErrContextAndTimeout\n\t}\n\tif o.ttl == 0 && o.ctx == nil {\n\t\to.ttl = js.opts.wait\n\t}\n\tif o.stallWait > 0 {\n\t\treturn nil, errors.New(\"nats: stall wait cannot be set to sync publish\")\n\t}\n\n\tif o.id != _EMPTY_ {\n\t\tm.Header.Set(MsgIdHdr, o.id)\n\t}\n\tif o.lid != _EMPTY_ {\n\t\tm.Header.Set(ExpectedLastMsgIdHdr, o.lid)\n\t}\n\tif o.str != _EMPTY_ {\n\t\tm.Header.Set(ExpectedStreamHdr, o.str)\n\t}\n\tif o.seq != nil {\n\t\tm.Header.Set(ExpectedLastSeqHdr, strconv.FormatUint(*o.seq, 10))\n\t}\n\tif o.lss != nil {\n\t\tm.Header.Set(ExpectedLastSubjSeqHdr, strconv.FormatUint(*o.lss, 10))\n\t}\n\n\tvar resp *Msg\n\tvar err error\n\n\tif o.ttl > 0 {\n\t\tresp, err = js.nc.RequestMsg(m, time.Duration(o.ttl))\n\t} else {\n\t\tresp, err = js.nc.RequestMsgWithContext(o.ctx, m)\n\t}\n\n\tif err != nil {\n\t\tfor r, ttl := 0, o.ttl; errors.Is(err, ErrNoResponders) && (r < o.rnum || o.rnum < 0); r++ {\n\t\t\t// To protect against small blips in leadership changes etc, if we get a no responders here retry.\n\t\t\tif o.ctx != nil {\n\t\t\t\tselect {\n\t\t\t\tcase <-o.ctx.Done():\n\t\t\t\tcase <-time.After(o.rwait):\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\ttime.Sleep(o.rwait)\n\t\t\t}\n\t\t\tif o.ttl > 0 {\n\t\t\t\tttl -= o.rwait\n\t\t\t\tif ttl <= 0 {\n\t\t\t\t\terr = ErrTimeout\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tresp, err = js.nc.RequestMsg(m, time.Duration(ttl))\n\t\t\t} else {\n\t\t\t\tresp, err = js.nc.RequestMsgWithContext(o.ctx, m)\n\t\t\t}\n\t\t}\n\t\tif err != nil {\n\t\t\tif errors.Is(err, ErrNoResponders) {\n\t\t\t\terr = ErrNoStreamResponse\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tvar pa pubAckResponse\n\tif err := json.Unmarshal(resp.Data, &pa); err != nil {\n\t\treturn nil, ErrInvalidJSAck\n\t}\n\tif pa.Error != nil {\n\t\treturn nil, pa.Error\n\t}\n\tif pa.PubAck == nil || pa.PubAck.Stream == _EMPTY_ {\n\t\treturn nil, ErrInvalidJSAck\n\t}\n\treturn pa.PubAck, nil\n}\n\n// Publish publishes a message to a stream from JetStream.\nfunc (js *js) Publish(subj string, data []byte, opts ...PubOpt) (*PubAck, error) {\n\treturn js.PublishMsg(&Msg{Subject: subj, Data: data}, opts...)\n}\n\n// PubAckFuture is a future for a PubAck.\ntype PubAckFuture interface {\n\t// Ok returns a receive only channel that can be used to get a PubAck.\n\tOk() <-chan *PubAck\n\n\t// Err returns a receive only channel that can be used to get the error from an async publish.\n\tErr() <-chan error\n\n\t// Msg returns the message that was sent to the server.\n\tMsg() *Msg\n}\n\ntype pubAckFuture struct {\n\tjs         *js\n\tmsg        *Msg\n\tpa         *PubAck\n\tst         time.Time\n\terr        error\n\terrCh      chan error\n\tdoneCh     chan *PubAck\n\tretries    int\n\tmaxRetries int\n\tretryWait  time.Duration\n\treply      string\n}\n\nfunc (paf *pubAckFuture) Ok() <-chan *PubAck {\n\tpaf.js.mu.Lock()\n\tdefer paf.js.mu.Unlock()\n\n\tif paf.doneCh == nil {\n\t\tpaf.doneCh = make(chan *PubAck, 1)\n\t\tif paf.pa != nil {\n\t\t\tpaf.doneCh <- paf.pa\n\t\t}\n\t}\n\n\treturn paf.doneCh\n}\n\nfunc (paf *pubAckFuture) Err() <-chan error {\n\tpaf.js.mu.Lock()\n\tdefer paf.js.mu.Unlock()\n\n\tif paf.errCh == nil {\n\t\tpaf.errCh = make(chan error, 1)\n\t\tif paf.err != nil {\n\t\t\tpaf.errCh <- paf.err\n\t\t}\n\t}\n\n\treturn paf.errCh\n}\n\nfunc (paf *pubAckFuture) Msg() *Msg {\n\tpaf.js.mu.RLock()\n\tdefer paf.js.mu.RUnlock()\n\treturn paf.msg\n}\n\n// For quick token lookup etc.\nconst aReplyTokensize = 6\n\nfunc (js *js) newAsyncReply() string {\n\tjs.mu.Lock()\n\tif js.rsub == nil {\n\t\t// Create our wildcard reply subject.\n\t\tsha := sha256.New()\n\t\tsha.Write([]byte(nuid.Next()))\n\t\tb := sha.Sum(nil)\n\t\tfor i := 0; i < aReplyTokensize; i++ {\n\t\t\tb[i] = rdigits[int(b[i]%base)]\n\t\t}\n\t\tjs.rpre = fmt.Sprintf(\"%s%s.\", js.replyPrefix, b[:aReplyTokensize])\n\t\tsub, err := js.nc.Subscribe(fmt.Sprintf(\"%s*\", js.rpre), js.handleAsyncReply)\n\t\tif err != nil {\n\t\t\tjs.mu.Unlock()\n\t\t\treturn _EMPTY_\n\t\t}\n\t\tjs.rsub = sub\n\t\tjs.rr = rand.New(rand.NewSource(time.Now().UnixNano()))\n\t}\n\tif js.connStatusCh == nil {\n\t\tjs.connStatusCh = js.nc.StatusChanged(RECONNECTING, CLOSED)\n\t\tgo js.resetPendingAcksOnReconnect()\n\t}\n\tvar sb strings.Builder\n\tsb.WriteString(js.rpre)\n\trn := js.rr.Int63()\n\tvar b [aReplyTokensize]byte\n\tfor i, l := 0, rn; i < len(b); i++ {\n\t\tb[i] = rdigits[l%base]\n\t\tl /= base\n\t}\n\tsb.Write(b[:])\n\tjs.mu.Unlock()\n\treturn sb.String()\n}\n\nfunc (js *js) resetPendingAcksOnReconnect() {\n\tjs.mu.Lock()\n\tconnStatusCh := js.connStatusCh\n\tjs.mu.Unlock()\n\tfor {\n\t\tnewStatus, ok := <-connStatusCh\n\t\tif !ok || newStatus == CLOSED {\n\t\t\treturn\n\t\t}\n\t\tjs.mu.Lock()\n\t\terrCb := js.opts.aecb\n\t\tfor id, paf := range js.pafs {\n\t\t\tpaf.err = ErrDisconnected\n\t\t\tif paf.errCh != nil {\n\t\t\t\tpaf.errCh <- paf.err\n\t\t\t}\n\t\t\tif errCb != nil {\n\t\t\t\tdefer errCb(js, paf.msg, ErrDisconnected)\n\t\t\t}\n\t\t\tdelete(js.pafs, id)\n\t\t}\n\t\tif js.dch != nil {\n\t\t\tclose(js.dch)\n\t\t\tjs.dch = nil\n\t\t}\n\t\tjs.mu.Unlock()\n\t}\n}\n\n// CleanupPublisher will cleanup the publishing side of JetStreamContext.\n//\n// This will unsubscribe from the internal reply subject if needed.\n// All pending async publishes will fail with ErrJetStreamContextClosed.\n//\n// If an error handler was provided, it will be called for each pending async\n// publish and PublishAsyncComplete will be closed.\n//\n// After completing JetStreamContext is still usable - internal subscription\n// will be recreated on next publish, but the acks from previous publishes will\n// be lost.\nfunc (js *js) CleanupPublisher() {\n\tjs.cleanupReplySub()\n\tjs.mu.Lock()\n\terrCb := js.opts.aecb\n\tfor id, paf := range js.pafs {\n\t\tpaf.err = ErrJetStreamPublisherClosed\n\t\tif paf.errCh != nil {\n\t\t\tpaf.errCh <- paf.err\n\t\t}\n\t\tif errCb != nil {\n\t\t\tdefer errCb(js, paf.msg, ErrJetStreamPublisherClosed)\n\t\t}\n\t\tdelete(js.pafs, id)\n\t}\n\tif js.dch != nil {\n\t\tclose(js.dch)\n\t\tjs.dch = nil\n\t}\n\tjs.mu.Unlock()\n}\n\nfunc (js *js) cleanupReplySub() {\n\tjs.mu.Lock()\n\tif js.rsub != nil {\n\t\tjs.rsub.Unsubscribe()\n\t\tjs.rsub = nil\n\t}\n\tif js.connStatusCh != nil {\n\t\tclose(js.connStatusCh)\n\t\tjs.connStatusCh = nil\n\t}\n\tjs.mu.Unlock()\n}\n\n// registerPAF will register for a PubAckFuture.\nfunc (js *js) registerPAF(id string, paf *pubAckFuture) (int, int) {\n\tjs.mu.Lock()\n\tif js.pafs == nil {\n\t\tjs.pafs = make(map[string]*pubAckFuture)\n\t}\n\tpaf.js = js\n\tjs.pafs[id] = paf\n\tnp := len(js.pafs)\n\tmaxpa := js.opts.maxpa\n\tjs.mu.Unlock()\n\treturn np, maxpa\n}\n\n// Lock should be held.\nfunc (js *js) getPAF(id string) *pubAckFuture {\n\tif js.pafs == nil {\n\t\treturn nil\n\t}\n\treturn js.pafs[id]\n}\n\n// clearPAF will remove a PubAckFuture that was registered.\nfunc (js *js) clearPAF(id string) {\n\tjs.mu.Lock()\n\tdelete(js.pafs, id)\n\tjs.mu.Unlock()\n}\n\n// PublishAsyncPending returns how many PubAckFutures are pending.\nfunc (js *js) PublishAsyncPending() int {\n\tjs.mu.RLock()\n\tdefer js.mu.RUnlock()\n\treturn len(js.pafs)\n}\n\nfunc (js *js) asyncStall() <-chan struct{} {\n\tjs.mu.Lock()\n\tif js.stc == nil {\n\t\tjs.stc = make(chan struct{})\n\t}\n\tstc := js.stc\n\tjs.mu.Unlock()\n\treturn stc\n}\n\n// Handle an async reply from PublishAsync.\nfunc (js *js) handleAsyncReply(m *Msg) {\n\tif len(m.Subject) <= js.replyPrefixLen {\n\t\treturn\n\t}\n\tid := m.Subject[js.replyPrefixLen:]\n\n\tjs.mu.Lock()\n\tpaf := js.getPAF(id)\n\tif paf == nil {\n\t\tjs.mu.Unlock()\n\t\treturn\n\t}\n\n\tcloseStc := func() {\n\t\t// Check on anyone stalled and waiting.\n\t\tif js.stc != nil && len(js.pafs) < js.opts.maxpa {\n\t\t\tclose(js.stc)\n\t\t\tjs.stc = nil\n\t\t}\n\t}\n\n\tcloseDchFn := func() func() {\n\t\tvar dch chan struct{}\n\t\t// Check on anyone one waiting on done status.\n\t\tif js.dch != nil && len(js.pafs) == 0 {\n\t\t\tdch = js.dch\n\t\t\tjs.dch = nil\n\t\t}\n\t\t// Return function to close done channel which\n\t\t// should be deferred so that error is processed and\n\t\t// can be checked.\n\t\treturn func() {\n\t\t\tif dch != nil {\n\t\t\t\tclose(dch)\n\t\t\t}\n\t\t}\n\t}\n\n\tdoErr := func(err error) {\n\t\tpaf.err = err\n\t\tif paf.errCh != nil {\n\t\t\tpaf.errCh <- paf.err\n\t\t}\n\t\tcb := js.opts.aecb\n\t\tjs.mu.Unlock()\n\t\tif cb != nil {\n\t\t\tcb(paf.js, paf.msg, err)\n\t\t}\n\t}\n\n\t// Process no responders etc.\n\tif len(m.Data) == 0 && m.Header.Get(statusHdr) == noResponders {\n\t\tif paf.retries < paf.maxRetries {\n\t\t\tpaf.retries++\n\t\t\ttime.AfterFunc(paf.retryWait, func() {\n\t\t\t\tjs.mu.Lock()\n\t\t\t\tpaf := js.getPAF(id)\n\t\t\t\tjs.mu.Unlock()\n\t\t\t\tif paf == nil {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\t_, err := js.PublishMsgAsync(paf.msg, pubOptFn(func(po *pubOpts) error {\n\t\t\t\t\tpo.pafRetry = paf\n\t\t\t\t\treturn nil\n\t\t\t\t}))\n\t\t\t\tif err != nil {\n\t\t\t\t\tjs.mu.Lock()\n\t\t\t\t\tdoErr(err)\n\t\t\t\t}\n\t\t\t})\n\t\t\tjs.mu.Unlock()\n\t\t\treturn\n\t\t}\n\t\tdelete(js.pafs, id)\n\t\tcloseStc()\n\t\tdefer closeDchFn()()\n\t\tdoErr(ErrNoResponders)\n\t\treturn\n\t}\n\n\t//remove\n\tdelete(js.pafs, id)\n\tcloseStc()\n\tdefer closeDchFn()()\n\n\tvar pa pubAckResponse\n\tif err := json.Unmarshal(m.Data, &pa); err != nil {\n\t\tdoErr(ErrInvalidJSAck)\n\t\treturn\n\t}\n\tif pa.Error != nil {\n\t\tdoErr(pa.Error)\n\t\treturn\n\t}\n\tif pa.PubAck == nil || pa.PubAck.Stream == _EMPTY_ {\n\t\tdoErr(ErrInvalidJSAck)\n\t\treturn\n\t}\n\n\t// So here we have received a proper puback.\n\tpaf.pa = pa.PubAck\n\tif paf.doneCh != nil {\n\t\tpaf.doneCh <- paf.pa\n\t}\n\tjs.mu.Unlock()\n}\n\n// MsgErrHandler is used to process asynchronous errors from\n// JetStream PublishAsync. It will return the original\n// message sent to the server for possible retransmitting and the error encountered.\ntype MsgErrHandler func(JetStream, *Msg, error)\n\n// PublishAsyncErrHandler sets the error handler for async publishes in JetStream.\nfunc PublishAsyncErrHandler(cb MsgErrHandler) JSOpt {\n\treturn jsOptFn(func(js *jsOpts) error {\n\t\tjs.aecb = cb\n\t\treturn nil\n\t})\n}\n\n// PublishAsyncMaxPending sets the maximum outstanding async publishes that can be inflight at one time.\nfunc PublishAsyncMaxPending(max int) JSOpt {\n\treturn jsOptFn(func(js *jsOpts) error {\n\t\tif max < 1 {\n\t\t\treturn errors.New(\"nats: max ack pending should be >= 1\")\n\t\t}\n\t\tjs.maxpa = max\n\t\treturn nil\n\t})\n}\n\n// PublishAsync publishes a message to JetStream and returns a PubAckFuture\nfunc (js *js) PublishAsync(subj string, data []byte, opts ...PubOpt) (PubAckFuture, error) {\n\treturn js.PublishMsgAsync(&Msg{Subject: subj, Data: data}, opts...)\n}\n\nconst defaultStallWait = 200 * time.Millisecond\n\nfunc (js *js) PublishMsgAsync(m *Msg, opts ...PubOpt) (PubAckFuture, error) {\n\tvar o pubOpts\n\tif len(opts) > 0 {\n\t\tif m.Header == nil {\n\t\t\tm.Header = Header{}\n\t\t}\n\t\tfor _, opt := range opts {\n\t\t\tif err := opt.configurePublish(&o); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\tif o.rnum < 0 {\n\t\treturn nil, fmt.Errorf(\"%w: retry attempts cannot be negative\", ErrInvalidArg)\n\t}\n\n\t// Timeouts and contexts do not make sense for these.\n\tif o.ttl != 0 || o.ctx != nil {\n\t\treturn nil, ErrContextAndTimeout\n\t}\n\tstallWait := defaultStallWait\n\tif o.stallWait > 0 {\n\t\tstallWait = o.stallWait\n\t}\n\n\t// FIXME(dlc) - Make common.\n\tif o.id != _EMPTY_ {\n\t\tm.Header.Set(MsgIdHdr, o.id)\n\t}\n\tif o.lid != _EMPTY_ {\n\t\tm.Header.Set(ExpectedLastMsgIdHdr, o.lid)\n\t}\n\tif o.str != _EMPTY_ {\n\t\tm.Header.Set(ExpectedStreamHdr, o.str)\n\t}\n\tif o.seq != nil {\n\t\tm.Header.Set(ExpectedLastSeqHdr, strconv.FormatUint(*o.seq, 10))\n\t}\n\tif o.lss != nil {\n\t\tm.Header.Set(ExpectedLastSubjSeqHdr, strconv.FormatUint(*o.lss, 10))\n\t}\n\n\t// Reply\n\tpaf := o.pafRetry\n\tif paf == nil && m.Reply != _EMPTY_ {\n\t\treturn nil, errors.New(\"nats: reply subject should be empty\")\n\t}\n\tvar id string\n\tvar reply string\n\n\t// register new paf if not retrying\n\tif paf == nil {\n\t\treply = js.newAsyncReply()\n\n\t\tif reply == _EMPTY_ {\n\t\t\treturn nil, errors.New(\"nats: error creating async reply handler\")\n\t\t}\n\n\t\tid = reply[js.replyPrefixLen:]\n\t\tpaf = &pubAckFuture{msg: m, st: time.Now(), maxRetries: o.rnum, retryWait: o.rwait, reply: reply}\n\t\tnumPending, maxPending := js.registerPAF(id, paf)\n\n\t\tif maxPending > 0 && numPending > maxPending {\n\t\t\tselect {\n\t\t\tcase <-js.asyncStall():\n\t\t\tcase <-time.After(stallWait):\n\t\t\t\tjs.clearPAF(id)\n\t\t\t\treturn nil, errors.New(\"nats: stalled with too many outstanding async published messages\")\n\t\t\t}\n\t\t}\n\t} else {\n\t\treply = paf.reply\n\t\tid = reply[js.replyPrefixLen:]\n\t}\n\thdr, err := m.headerBytes()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := js.nc.publish(m.Subject, reply, hdr, m.Data); err != nil {\n\t\tjs.clearPAF(id)\n\t\treturn nil, err\n\t}\n\n\treturn paf, nil\n}\n\n// PublishAsyncComplete returns a channel that will be closed when all outstanding messages have been ack'd.\nfunc (js *js) PublishAsyncComplete() <-chan struct{} {\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\tif js.dch == nil {\n\t\tjs.dch = make(chan struct{})\n\t}\n\tdch := js.dch\n\tif len(js.pafs) == 0 {\n\t\tclose(js.dch)\n\t\tjs.dch = nil\n\t}\n\treturn dch\n}\n\n// MsgId sets the message ID used for deduplication.\nfunc MsgId(id string) PubOpt {\n\treturn pubOptFn(func(opts *pubOpts) error {\n\t\topts.id = id\n\t\treturn nil\n\t})\n}\n\n// ExpectStream sets the expected stream to respond from the publish.\nfunc ExpectStream(stream string) PubOpt {\n\treturn pubOptFn(func(opts *pubOpts) error {\n\t\topts.str = stream\n\t\treturn nil\n\t})\n}\n\n// ExpectLastSequence sets the expected sequence in the response from the publish.\nfunc ExpectLastSequence(seq uint64) PubOpt {\n\treturn pubOptFn(func(opts *pubOpts) error {\n\t\topts.seq = &seq\n\t\treturn nil\n\t})\n}\n\n// ExpectLastSequencePerSubject sets the expected sequence per subject in the response from the publish.\nfunc ExpectLastSequencePerSubject(seq uint64) PubOpt {\n\treturn pubOptFn(func(opts *pubOpts) error {\n\t\topts.lss = &seq\n\t\treturn nil\n\t})\n}\n\n// ExpectLastMsgId sets the expected last msgId in the response from the publish.\nfunc ExpectLastMsgId(id string) PubOpt {\n\treturn pubOptFn(func(opts *pubOpts) error {\n\t\topts.lid = id\n\t\treturn nil\n\t})\n}\n\n// RetryWait sets the retry wait time when ErrNoResponders is encountered.\nfunc RetryWait(dur time.Duration) PubOpt {\n\treturn pubOptFn(func(opts *pubOpts) error {\n\t\topts.rwait = dur\n\t\treturn nil\n\t})\n}\n\n// RetryAttempts sets the retry number of attempts when ErrNoResponders is encountered.\nfunc RetryAttempts(num int) PubOpt {\n\treturn pubOptFn(func(opts *pubOpts) error {\n\t\topts.rnum = num\n\t\treturn nil\n\t})\n}\n\n// StallWait sets the max wait when the producer becomes stall producing messages.\nfunc StallWait(ttl time.Duration) PubOpt {\n\treturn pubOptFn(func(opts *pubOpts) error {\n\t\tif ttl <= 0 {\n\t\t\treturn errors.New(\"nats: stall wait should be more than 0\")\n\t\t}\n\t\topts.stallWait = ttl\n\t\treturn nil\n\t})\n}\n\ntype ackOpts struct {\n\tttl      time.Duration\n\tctx      context.Context\n\tnakDelay time.Duration\n}\n\n// AckOpt are the options that can be passed when acknowledge a message.\ntype AckOpt interface {\n\tconfigureAck(opts *ackOpts) error\n}\n\n// MaxWait sets the maximum amount of time we will wait for a response.\ntype MaxWait time.Duration\n\nfunc (ttl MaxWait) configureJSContext(js *jsOpts) error {\n\tjs.wait = time.Duration(ttl)\n\treturn nil\n}\n\nfunc (ttl MaxWait) configurePull(opts *pullOpts) error {\n\topts.ttl = time.Duration(ttl)\n\treturn nil\n}\n\n// AckWait sets the maximum amount of time we will wait for an ack.\ntype AckWait time.Duration\n\nfunc (ttl AckWait) configurePublish(opts *pubOpts) error {\n\topts.ttl = time.Duration(ttl)\n\treturn nil\n}\n\nfunc (ttl AckWait) configureSubscribe(opts *subOpts) error {\n\topts.cfg.AckWait = time.Duration(ttl)\n\treturn nil\n}\n\nfunc (ttl AckWait) configureAck(opts *ackOpts) error {\n\topts.ttl = time.Duration(ttl)\n\treturn nil\n}\n\n// ContextOpt is an option used to set a context.Context.\ntype ContextOpt struct {\n\tcontext.Context\n}\n\nfunc (ctx ContextOpt) configureJSContext(opts *jsOpts) error {\n\topts.ctx = ctx\n\treturn nil\n}\n\nfunc (ctx ContextOpt) configurePublish(opts *pubOpts) error {\n\topts.ctx = ctx\n\treturn nil\n}\n\nfunc (ctx ContextOpt) configureSubscribe(opts *subOpts) error {\n\topts.ctx = ctx\n\treturn nil\n}\n\nfunc (ctx ContextOpt) configurePull(opts *pullOpts) error {\n\topts.ctx = ctx\n\treturn nil\n}\n\nfunc (ctx ContextOpt) configureAck(opts *ackOpts) error {\n\topts.ctx = ctx\n\treturn nil\n}\n\n// Context returns an option that can be used to configure a context for APIs\n// that are context aware such as those part of the JetStream interface.\nfunc Context(ctx context.Context) ContextOpt {\n\treturn ContextOpt{ctx}\n}\n\ntype nakDelay time.Duration\n\nfunc (d nakDelay) configureAck(opts *ackOpts) error {\n\topts.nakDelay = time.Duration(d)\n\treturn nil\n}\n\n// Subscribe\n\n// ConsumerConfig is the configuration of a JetStream consumer.\ntype ConsumerConfig struct {\n\tDurable         string          `json:\"durable_name,omitempty\"`\n\tName            string          `json:\"name,omitempty\"`\n\tDescription     string          `json:\"description,omitempty\"`\n\tDeliverPolicy   DeliverPolicy   `json:\"deliver_policy\"`\n\tOptStartSeq     uint64          `json:\"opt_start_seq,omitempty\"`\n\tOptStartTime    *time.Time      `json:\"opt_start_time,omitempty\"`\n\tAckPolicy       AckPolicy       `json:\"ack_policy\"`\n\tAckWait         time.Duration   `json:\"ack_wait,omitempty\"`\n\tMaxDeliver      int             `json:\"max_deliver,omitempty\"`\n\tBackOff         []time.Duration `json:\"backoff,omitempty\"`\n\tFilterSubject   string          `json:\"filter_subject,omitempty\"`\n\tFilterSubjects  []string        `json:\"filter_subjects,omitempty\"`\n\tReplayPolicy    ReplayPolicy    `json:\"replay_policy\"`\n\tRateLimit       uint64          `json:\"rate_limit_bps,omitempty\"` // Bits per sec\n\tSampleFrequency string          `json:\"sample_freq,omitempty\"`\n\tMaxWaiting      int             `json:\"max_waiting,omitempty\"`\n\tMaxAckPending   int             `json:\"max_ack_pending,omitempty\"`\n\tFlowControl     bool            `json:\"flow_control,omitempty\"`\n\tHeartbeat       time.Duration   `json:\"idle_heartbeat,omitempty\"`\n\tHeadersOnly     bool            `json:\"headers_only,omitempty\"`\n\n\t// Pull based options.\n\tMaxRequestBatch    int           `json:\"max_batch,omitempty\"`\n\tMaxRequestExpires  time.Duration `json:\"max_expires,omitempty\"`\n\tMaxRequestMaxBytes int           `json:\"max_bytes,omitempty\"`\n\n\t// Push based consumers.\n\tDeliverSubject string `json:\"deliver_subject,omitempty\"`\n\tDeliverGroup   string `json:\"deliver_group,omitempty\"`\n\n\t// Inactivity threshold.\n\tInactiveThreshold time.Duration `json:\"inactive_threshold,omitempty\"`\n\n\t// Generally inherited by parent stream and other markers, now can be configured directly.\n\tReplicas int `json:\"num_replicas\"`\n\t// Force memory storage.\n\tMemoryStorage bool `json:\"mem_storage,omitempty\"`\n\n\t// Metadata is additional metadata for the Consumer.\n\t// Keys starting with `_nats` are reserved.\n\t// NOTE: Metadata requires nats-server v2.10.0+\n\tMetadata map[string]string `json:\"metadata,omitempty\"`\n}\n\n// ConsumerInfo is the info from a JetStream consumer.\ntype ConsumerInfo struct {\n\tStream         string         `json:\"stream_name\"`\n\tName           string         `json:\"name\"`\n\tCreated        time.Time      `json:\"created\"`\n\tConfig         ConsumerConfig `json:\"config\"`\n\tDelivered      SequenceInfo   `json:\"delivered\"`\n\tAckFloor       SequenceInfo   `json:\"ack_floor\"`\n\tNumAckPending  int            `json:\"num_ack_pending\"`\n\tNumRedelivered int            `json:\"num_redelivered\"`\n\tNumWaiting     int            `json:\"num_waiting\"`\n\tNumPending     uint64         `json:\"num_pending\"`\n\tCluster        *ClusterInfo   `json:\"cluster,omitempty\"`\n\tPushBound      bool           `json:\"push_bound,omitempty\"`\n}\n\n// SequenceInfo has both the consumer and the stream sequence and last activity.\ntype SequenceInfo struct {\n\tConsumer uint64     `json:\"consumer_seq\"`\n\tStream   uint64     `json:\"stream_seq\"`\n\tLast     *time.Time `json:\"last_active,omitempty\"`\n}\n\n// SequencePair includes the consumer and stream sequence info from a JetStream consumer.\ntype SequencePair struct {\n\tConsumer uint64 `json:\"consumer_seq\"`\n\tStream   uint64 `json:\"stream_seq\"`\n}\n\n// nextRequest is for getting next messages for pull based consumers from JetStream.\ntype nextRequest struct {\n\tExpires   time.Duration `json:\"expires,omitempty\"`\n\tBatch     int           `json:\"batch,omitempty\"`\n\tNoWait    bool          `json:\"no_wait,omitempty\"`\n\tMaxBytes  int           `json:\"max_bytes,omitempty\"`\n\tHeartbeat time.Duration `json:\"idle_heartbeat,omitempty\"`\n}\n\n// jsSub includes JetStream subscription info.\ntype jsSub struct {\n\tjs *js\n\n\t// For pull subscribers, this is the next message subject to send requests to.\n\tnms string\n\n\tpsubj    string // the subject that was passed by user to the subscribe calls\n\tconsumer string\n\tstream   string\n\tdeliver  string\n\tpull     bool\n\tdc       bool // Delete JS consumer\n\tackNone  bool\n\n\t// This is ConsumerInfo's Pending+Consumer.Delivered that we get from the\n\t// add consumer response. Note that some versions of the server gather the\n\t// consumer info *after* the creation of the consumer, which means that\n\t// some messages may have been already delivered. So the sum of the two\n\t// is a more accurate representation of the number of messages pending or\n\t// in the process of being delivered to the subscription when created.\n\tpending uint64\n\n\t// Ordered consumers\n\tordered bool\n\tdseq    uint64\n\tsseq    uint64\n\tccreq   *createConsumerRequest\n\n\t// Heartbeats and Flow Control handling from push consumers.\n\thbc    *time.Timer\n\thbi    time.Duration\n\tactive bool\n\tcmeta  string\n\tfcr    string\n\tfcd    uint64\n\tfciseq uint64\n\tcsfct  *time.Timer\n\n\t// Cancellation function to cancel context on drain/unsubscribe.\n\tcancel func()\n}\n\n// Deletes the JS Consumer.\n// No connection nor subscription lock must be held on entry.\nfunc (sub *Subscription) deleteConsumer() error {\n\tsub.mu.Lock()\n\tjsi := sub.jsi\n\tif jsi == nil {\n\t\tsub.mu.Unlock()\n\t\treturn nil\n\t}\n\tif jsi.stream == _EMPTY_ || jsi.consumer == _EMPTY_ {\n\t\tsub.mu.Unlock()\n\t\treturn nil\n\t}\n\tstream, consumer := jsi.stream, jsi.consumer\n\tjs := jsi.js\n\tsub.mu.Unlock()\n\n\treturn js.DeleteConsumer(stream, consumer)\n}\n\n// SubOpt configures options for subscribing to JetStream consumers.\ntype SubOpt interface {\n\tconfigureSubscribe(opts *subOpts) error\n}\n\n// subOptFn is a function option used to configure a JetStream Subscribe.\ntype subOptFn func(opts *subOpts) error\n\nfunc (opt subOptFn) configureSubscribe(opts *subOpts) error {\n\treturn opt(opts)\n}\n\n// Subscribe creates an async Subscription for JetStream.\n// The stream and consumer names can be provided with the nats.Bind() option.\n// For creating an ephemeral (where the consumer name is picked by the server),\n// you can provide the stream name with nats.BindStream().\n// If no stream name is specified, the library will attempt to figure out which\n// stream the subscription is for. See important notes below for more details.\n//\n// IMPORTANT NOTES:\n// * If none of the options Bind() nor Durable() are specified, the library will\n// send a request to the server to create an ephemeral JetStream consumer,\n// which will be deleted after an Unsubscribe() or Drain(), or automatically\n// by the server after a short period of time after the NATS subscription is\n// gone.\n// * If Durable() option is specified, the library will attempt to lookup a JetStream\n// consumer with this name, and if found, will bind to it and not attempt to\n// delete it. However, if not found, the library will send a request to create\n// such durable JetStream consumer. The library will delete the JetStream consumer\n// after an Unsubscribe() or Drain().\n// * If Bind() option is provided, the library will attempt to lookup the\n// consumer with the given name, and if successful, bind to it. If the lookup fails,\n// then the Subscribe() call will return an error.\nfunc (js *js) Subscribe(subj string, cb MsgHandler, opts ...SubOpt) (*Subscription, error) {\n\tif cb == nil {\n\t\treturn nil, ErrBadSubscription\n\t}\n\treturn js.subscribe(subj, _EMPTY_, cb, nil, false, false, opts)\n}\n\n// SubscribeSync creates a Subscription that can be used to process messages synchronously.\n// See important note in Subscribe()\nfunc (js *js) SubscribeSync(subj string, opts ...SubOpt) (*Subscription, error) {\n\tmch := make(chan *Msg, js.nc.Opts.SubChanLen)\n\treturn js.subscribe(subj, _EMPTY_, nil, mch, true, false, opts)\n}\n\n// QueueSubscribe creates a Subscription with a queue group.\n// If no optional durable name nor binding options are specified, the queue name will be used as a durable name.\n// See important note in Subscribe()\nfunc (js *js) QueueSubscribe(subj, queue string, cb MsgHandler, opts ...SubOpt) (*Subscription, error) {\n\tif cb == nil {\n\t\treturn nil, ErrBadSubscription\n\t}\n\treturn js.subscribe(subj, queue, cb, nil, false, false, opts)\n}\n\n// QueueSubscribeSync creates a Subscription with a queue group that can be used to process messages synchronously.\n// See important note in QueueSubscribe()\nfunc (js *js) QueueSubscribeSync(subj, queue string, opts ...SubOpt) (*Subscription, error) {\n\tmch := make(chan *Msg, js.nc.Opts.SubChanLen)\n\treturn js.subscribe(subj, queue, nil, mch, true, false, opts)\n}\n\n// ChanSubscribe creates channel based Subscription.\n// Using ChanSubscribe without buffered capacity is not recommended since\n// it will be prone to dropping messages with a slow consumer error.  Make sure to give the channel enough\n// capacity to handle bursts in traffic, for example other Subscribe APIs use a default of 512k capacity in comparison.\n// See important note in Subscribe()\nfunc (js *js) ChanSubscribe(subj string, ch chan *Msg, opts ...SubOpt) (*Subscription, error) {\n\treturn js.subscribe(subj, _EMPTY_, nil, ch, false, false, opts)\n}\n\n// ChanQueueSubscribe creates channel based Subscription with a queue group.\n// See important note in QueueSubscribe()\nfunc (js *js) ChanQueueSubscribe(subj, queue string, ch chan *Msg, opts ...SubOpt) (*Subscription, error) {\n\treturn js.subscribe(subj, queue, nil, ch, false, false, opts)\n}\n\n// PullSubscribe creates a Subscription that can fetch messages.\n// See important note in Subscribe()\nfunc (js *js) PullSubscribe(subj, durable string, opts ...SubOpt) (*Subscription, error) {\n\tmch := make(chan *Msg, js.nc.Opts.SubChanLen)\n\tif durable != \"\" {\n\t\topts = append(opts, Durable(durable))\n\t}\n\treturn js.subscribe(subj, _EMPTY_, nil, mch, true, true, opts)\n}\n\nfunc processConsInfo(info *ConsumerInfo, userCfg *ConsumerConfig, isPullMode bool, subj, queue string) (string, error) {\n\tccfg := &info.Config\n\n\t// Make sure this new subject matches or is a subset.\n\tif ccfg.FilterSubject != _EMPTY_ && subj != ccfg.FilterSubject {\n\t\treturn _EMPTY_, ErrSubjectMismatch\n\t}\n\n\t// Prevent binding a subscription against incompatible consumer types.\n\tif isPullMode && ccfg.DeliverSubject != _EMPTY_ {\n\t\treturn _EMPTY_, ErrPullSubscribeToPushConsumer\n\t} else if !isPullMode && ccfg.DeliverSubject == _EMPTY_ {\n\t\treturn _EMPTY_, ErrPullSubscribeRequired\n\t}\n\n\t// If pull mode, nothing else to check here.\n\tif isPullMode {\n\t\treturn _EMPTY_, checkConfig(ccfg, userCfg)\n\t}\n\n\t// At this point, we know the user wants push mode, and the JS consumer is\n\t// really push mode.\n\n\tdg := info.Config.DeliverGroup\n\tif dg == _EMPTY_ {\n\t\t// Prevent an user from attempting to create a queue subscription on\n\t\t// a JS consumer that was not created with a deliver group.\n\t\tif queue != _EMPTY_ {\n\t\t\treturn _EMPTY_, errors.New(\"cannot create a queue subscription for a consumer without a deliver group\")\n\t\t} else if info.PushBound {\n\t\t\t// Need to reject a non queue subscription to a non queue consumer\n\t\t\t// if the consumer is already bound.\n\t\t\treturn _EMPTY_, errors.New(\"consumer is already bound to a subscription\")\n\t\t}\n\t} else {\n\t\t// If the JS consumer has a deliver group, we need to fail a non queue\n\t\t// subscription attempt:\n\t\tif queue == _EMPTY_ {\n\t\t\treturn _EMPTY_, fmt.Errorf(\"cannot create a subscription for a consumer with a deliver group %q\", dg)\n\t\t} else if queue != dg {\n\t\t\t// Here the user's queue group name does not match the one associated\n\t\t\t// with the JS consumer.\n\t\t\treturn _EMPTY_, fmt.Errorf(\"cannot create a queue subscription %q for a consumer with a deliver group %q\",\n\t\t\t\tqueue, dg)\n\t\t}\n\t}\n\tif err := checkConfig(ccfg, userCfg); err != nil {\n\t\treturn _EMPTY_, err\n\t}\n\treturn ccfg.DeliverSubject, nil\n}\n\nfunc checkConfig(s, u *ConsumerConfig) error {\n\tmakeErr := func(fieldName string, usrVal, srvVal any) error {\n\t\treturn fmt.Errorf(\"nats: configuration requests %s to be %v, but consumer's value is %v\", fieldName, usrVal, srvVal)\n\t}\n\n\tif u.Durable != _EMPTY_ && u.Durable != s.Durable {\n\t\treturn makeErr(\"durable\", u.Durable, s.Durable)\n\t}\n\tif u.Description != _EMPTY_ && u.Description != s.Description {\n\t\treturn makeErr(\"description\", u.Description, s.Description)\n\t}\n\tif u.DeliverPolicy != deliverPolicyNotSet && u.DeliverPolicy != s.DeliverPolicy {\n\t\treturn makeErr(\"deliver policy\", u.DeliverPolicy, s.DeliverPolicy)\n\t}\n\tif u.OptStartSeq > 0 && u.OptStartSeq != s.OptStartSeq {\n\t\treturn makeErr(\"optional start sequence\", u.OptStartSeq, s.OptStartSeq)\n\t}\n\tif u.OptStartTime != nil && !u.OptStartTime.IsZero() && !(*u.OptStartTime).Equal(*s.OptStartTime) {\n\t\treturn makeErr(\"optional start time\", u.OptStartTime, s.OptStartTime)\n\t}\n\tif u.AckPolicy != ackPolicyNotSet && u.AckPolicy != s.AckPolicy {\n\t\treturn makeErr(\"ack policy\", u.AckPolicy, s.AckPolicy)\n\t}\n\tif u.AckWait > 0 && u.AckWait != s.AckWait {\n\t\treturn makeErr(\"ack wait\", u.AckWait, s.AckWait)\n\t}\n\tif u.MaxDeliver > 0 && u.MaxDeliver != s.MaxDeliver {\n\t\treturn makeErr(\"max deliver\", u.MaxDeliver, s.MaxDeliver)\n\t}\n\tif u.ReplayPolicy != replayPolicyNotSet && u.ReplayPolicy != s.ReplayPolicy {\n\t\treturn makeErr(\"replay policy\", u.ReplayPolicy, s.ReplayPolicy)\n\t}\n\tif u.RateLimit > 0 && u.RateLimit != s.RateLimit {\n\t\treturn makeErr(\"rate limit\", u.RateLimit, s.RateLimit)\n\t}\n\tif u.SampleFrequency != _EMPTY_ && u.SampleFrequency != s.SampleFrequency {\n\t\treturn makeErr(\"sample frequency\", u.SampleFrequency, s.SampleFrequency)\n\t}\n\tif u.MaxWaiting > 0 && u.MaxWaiting != s.MaxWaiting {\n\t\treturn makeErr(\"max waiting\", u.MaxWaiting, s.MaxWaiting)\n\t}\n\tif u.MaxAckPending > 0 && u.MaxAckPending != s.MaxAckPending {\n\t\treturn makeErr(\"max ack pending\", u.MaxAckPending, s.MaxAckPending)\n\t}\n\t// For flow control, we want to fail if the user explicit wanted it, but\n\t// it is not set in the existing consumer. If it is not asked by the user,\n\t// the library still handles it and so no reason to fail.\n\tif u.FlowControl && !s.FlowControl {\n\t\treturn makeErr(\"flow control\", u.FlowControl, s.FlowControl)\n\t}\n\tif u.Heartbeat > 0 && u.Heartbeat != s.Heartbeat {\n\t\treturn makeErr(\"heartbeat\", u.Heartbeat, s.Heartbeat)\n\t}\n\tif u.Replicas > 0 && u.Replicas != s.Replicas {\n\t\treturn makeErr(\"replicas\", u.Replicas, s.Replicas)\n\t}\n\tif u.MemoryStorage && !s.MemoryStorage {\n\t\treturn makeErr(\"memory storage\", u.MemoryStorage, s.MemoryStorage)\n\t}\n\treturn nil\n}\n\nfunc (js *js) subscribe(subj, queue string, cb MsgHandler, ch chan *Msg, isSync, isPullMode bool, opts []SubOpt) (*Subscription, error) {\n\tcfg := ConsumerConfig{\n\t\tDeliverPolicy: deliverPolicyNotSet,\n\t\tAckPolicy:     ackPolicyNotSet,\n\t\tReplayPolicy:  replayPolicyNotSet,\n\t}\n\to := subOpts{cfg: &cfg}\n\tif len(opts) > 0 {\n\t\tfor _, opt := range opts {\n\t\t\tif opt == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err := opt.configureSubscribe(&o); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\t// If no stream name is specified, the subject cannot be empty.\n\tif subj == _EMPTY_ && o.stream == _EMPTY_ {\n\t\treturn nil, errors.New(\"nats: subject required\")\n\t}\n\n\t// Note that these may change based on the consumer info response we may get.\n\thasHeartbeats := o.cfg.Heartbeat > 0\n\thasFC := o.cfg.FlowControl\n\n\t// Some checks for pull subscribers\n\tif isPullMode {\n\t\t// No deliver subject should be provided\n\t\tif o.cfg.DeliverSubject != _EMPTY_ {\n\t\t\treturn nil, ErrPullSubscribeToPushConsumer\n\t\t}\n\t}\n\n\t// Some check/setting specific to queue subs\n\tif queue != _EMPTY_ {\n\t\t// Queue subscriber cannot have HB or FC (since messages will be randomly dispatched\n\t\t// to members). We may in the future have a separate NATS subscription that all members\n\t\t// would subscribe to and server would send on.\n\t\tif o.cfg.Heartbeat > 0 || o.cfg.FlowControl {\n\t\t\t// Not making this a public ErrXXX in case we allow in the future.\n\t\t\treturn nil, errors.New(\"nats: queue subscription doesn't support idle heartbeat nor flow control\")\n\t\t}\n\n\t\t// If this is a queue subscription and no consumer nor durable name was specified,\n\t\t// then we will use the queue name as a durable name.\n\t\tif o.consumer == _EMPTY_ && o.cfg.Durable == _EMPTY_ {\n\t\t\tif err := checkConsumerName(queue); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\to.cfg.Durable = queue\n\t\t}\n\t}\n\n\tvar (\n\t\terr           error\n\t\tshouldCreate  bool\n\t\tinfo          *ConsumerInfo\n\t\tdeliver       string\n\t\tstream        = o.stream\n\t\tconsumer      = o.consumer\n\t\tisDurable     = o.cfg.Durable != _EMPTY_\n\t\tconsumerBound = o.bound\n\t\tctx           = o.ctx\n\t\tskipCInfo     = o.skipCInfo\n\t\tnotFoundErr   bool\n\t\tlookupErr     bool\n\t\tnc            = js.nc\n\t\tnms           string\n\t\thbi           time.Duration\n\t\tccreq         *createConsumerRequest // In case we need to hold onto it for ordered consumers.\n\t\tmaxap         int\n\t)\n\n\t// Do some quick checks here for ordered consumers. We do these here instead of spread out\n\t// in the individual SubOpts.\n\tif o.ordered {\n\t\t// Make sure we are not durable.\n\t\tif isDurable {\n\t\t\treturn nil, errors.New(\"nats: durable can not be set for an ordered consumer\")\n\t\t}\n\t\t// Check ack policy.\n\t\tif o.cfg.AckPolicy != ackPolicyNotSet {\n\t\t\treturn nil, errors.New(\"nats: ack policy can not be set for an ordered consumer\")\n\t\t}\n\t\t// Check max deliver.\n\t\tif o.cfg.MaxDeliver != 1 && o.cfg.MaxDeliver != 0 {\n\t\t\treturn nil, errors.New(\"nats: max deliver can not be set for an ordered consumer\")\n\t\t}\n\t\t// No deliver subject, we pick our own.\n\t\tif o.cfg.DeliverSubject != _EMPTY_ {\n\t\t\treturn nil, errors.New(\"nats: deliver subject can not be set for an ordered consumer\")\n\t\t}\n\t\t// Queue groups not allowed.\n\t\tif queue != _EMPTY_ {\n\t\t\treturn nil, errors.New(\"nats: queues not be set for an ordered consumer\")\n\t\t}\n\t\t// Check for bound consumers.\n\t\tif consumer != _EMPTY_ {\n\t\t\treturn nil, errors.New(\"nats: can not bind existing consumer for an ordered consumer\")\n\t\t}\n\t\t// Check for pull mode.\n\t\tif isPullMode {\n\t\t\treturn nil, errors.New(\"nats: can not use pull mode for an ordered consumer\")\n\t\t}\n\t\t// Setup how we need it to be here.\n\t\to.cfg.FlowControl = true\n\t\to.cfg.AckPolicy = AckNonePolicy\n\t\to.cfg.MaxDeliver = 1\n\t\to.cfg.AckWait = 22 * time.Hour // Just set to something known, not utilized.\n\t\t// Force R1 and MemoryStorage for these.\n\t\to.cfg.Replicas = 1\n\t\to.cfg.MemoryStorage = true\n\n\t\tif !hasHeartbeats {\n\t\t\to.cfg.Heartbeat = orderedHeartbeatsInterval\n\t\t}\n\t\thasFC, hasHeartbeats = true, true\n\t\to.mack = true // To avoid auto-ack wrapping call below.\n\t\thbi = o.cfg.Heartbeat\n\t}\n\n\t// In case a consumer has not been set explicitly, then the\n\t// durable name will be used as the consumer name.\n\tif consumer == _EMPTY_ {\n\t\tconsumer = o.cfg.Durable\n\t}\n\n\t// Find the stream mapped to the subject if not bound to a stream already.\n\tif stream == _EMPTY_ {\n\t\tstream, err = js.StreamNameBySubject(subj)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// With an explicit durable name, we can lookup the consumer first\n\t// to which it should be attaching to.\n\t// If SkipConsumerLookup was used, do not call consumer info.\n\tif consumer != _EMPTY_ && !o.skipCInfo {\n\t\tinfo, err = js.ConsumerInfo(stream, consumer)\n\t\tnotFoundErr = errors.Is(err, ErrConsumerNotFound)\n\t\tlookupErr = err == ErrJetStreamNotEnabled || errors.Is(err, ErrTimeout) || errors.Is(err, context.DeadlineExceeded)\n\t}\n\n\tswitch {\n\tcase info != nil:\n\t\tdeliver, err = processConsInfo(info, o.cfg, isPullMode, subj, queue)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\ticfg := &info.Config\n\t\thasFC, hbi = icfg.FlowControl, icfg.Heartbeat\n\t\thasHeartbeats = hbi > 0\n\t\tmaxap = icfg.MaxAckPending\n\tcase (err != nil && !notFoundErr) || (notFoundErr && consumerBound):\n\t\t// If the consumer is being bound and we got an error on pull subscribe then allow the error.\n\t\tif !(isPullMode && lookupErr && consumerBound) {\n\t\t\treturn nil, err\n\t\t}\n\tcase skipCInfo:\n\t\t// When skipping consumer info, need to rely on the manually passed sub options\n\t\t// to match the expected behavior from the subscription.\n\t\thasFC, hbi = o.cfg.FlowControl, o.cfg.Heartbeat\n\t\thasHeartbeats = hbi > 0\n\t\tmaxap = o.cfg.MaxAckPending\n\t\tdeliver = o.cfg.DeliverSubject\n\t\tif consumerBound {\n\t\t\tbreak\n\t\t}\n\n\t\t// When not bound to a consumer already, proceed to create.\n\t\tfallthrough\n\tdefault:\n\t\t// Attempt to create consumer if not found nor using Bind.\n\t\tshouldCreate = true\n\t\tif o.cfg.DeliverSubject != _EMPTY_ {\n\t\t\tdeliver = o.cfg.DeliverSubject\n\t\t} else if !isPullMode {\n\t\t\tdeliver = nc.NewInbox()\n\t\t\tcfg.DeliverSubject = deliver\n\t\t}\n\t\t// Do filtering always, server will clear as needed.\n\t\tcfg.FilterSubject = subj\n\n\t\t// Pass the queue to the consumer config\n\t\tif queue != _EMPTY_ {\n\t\t\tcfg.DeliverGroup = queue\n\t\t}\n\n\t\t// If not set, default to deliver all\n\t\tif cfg.DeliverPolicy == deliverPolicyNotSet {\n\t\t\tcfg.DeliverPolicy = DeliverAllPolicy\n\t\t}\n\t\t// If not set, default to ack explicit.\n\t\tif cfg.AckPolicy == ackPolicyNotSet {\n\t\t\tcfg.AckPolicy = AckExplicitPolicy\n\t\t}\n\t\t// If not set, default to instant\n\t\tif cfg.ReplayPolicy == replayPolicyNotSet {\n\t\t\tcfg.ReplayPolicy = ReplayInstantPolicy\n\t\t}\n\n\t\t// If we have acks at all and the MaxAckPending is not set go ahead\n\t\t// and set to the internal max for channel based consumers\n\t\tif cfg.MaxAckPending == 0 && ch != nil && cfg.AckPolicy != AckNonePolicy {\n\t\t\tcfg.MaxAckPending = cap(ch)\n\t\t}\n\t\t// Create request here.\n\t\tccreq = &createConsumerRequest{\n\t\t\tStream: stream,\n\t\t\tConfig: &cfg,\n\t\t}\n\t\thbi = cfg.Heartbeat\n\t}\n\n\tif isPullMode {\n\t\tnms = fmt.Sprintf(js.apiSubj(apiRequestNextT), stream, consumer)\n\t\tdeliver = nc.NewInbox()\n\t\t// for pull consumers, create a wildcard subscription to differentiate pull requests\n\t\tdeliver += \".*\"\n\t}\n\n\t// In case this has a context, then create a child context that\n\t// is possible to cancel via unsubscribe / drain.\n\tvar cancel func()\n\tif ctx != nil {\n\t\tctx, cancel = context.WithCancel(ctx)\n\t}\n\n\tjsi := &jsSub{\n\t\tjs:       js,\n\t\tstream:   stream,\n\t\tconsumer: consumer,\n\t\tdeliver:  deliver,\n\t\thbi:      hbi,\n\t\tordered:  o.ordered,\n\t\tccreq:    ccreq,\n\t\tdseq:     1,\n\t\tpull:     isPullMode,\n\t\tnms:      nms,\n\t\tpsubj:    subj,\n\t\tcancel:   cancel,\n\t\tackNone:  o.cfg.AckPolicy == AckNonePolicy,\n\t}\n\n\t// Auto acknowledge unless manual ack is set or policy is set to AckNonePolicy\n\tif cb != nil && !o.mack && o.cfg.AckPolicy != AckNonePolicy {\n\t\tocb := cb\n\t\tcb = func(m *Msg) { ocb(m); m.Ack() }\n\t}\n\tsub, err := nc.subscribe(deliver, queue, cb, ch, nil, isSync, jsi)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// If we fail and we had the sub we need to cleanup, but can't just do a straight Unsubscribe or Drain.\n\t// We need to clear the jsi so we do not remove any durables etc.\n\tcleanUpSub := func() {\n\t\tif sub != nil {\n\t\t\tsub.mu.Lock()\n\t\t\tsub.jsi = nil\n\t\t\tsub.mu.Unlock()\n\t\t\tsub.Unsubscribe()\n\t\t}\n\t}\n\n\t// If we are creating or updating let's process that request.\n\tconsName := o.cfg.Name\n\tif shouldCreate {\n\t\tif cfg.Durable != \"\" {\n\t\t\tconsName = cfg.Durable\n\t\t} else if consName == \"\" {\n\t\t\tconsName = getHash(nuid.Next())\n\t\t}\n\t\tinfo, err := js.upsertConsumer(stream, consName, ccreq.Config)\n\t\tif err != nil {\n\t\t\tvar apiErr *APIError\n\t\t\tif ok := errors.As(err, &apiErr); !ok {\n\t\t\t\tcleanUpSub()\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tif consumer == _EMPTY_ ||\n\t\t\t\t(apiErr.ErrorCode != JSErrCodeConsumerAlreadyExists && apiErr.ErrorCode != JSErrCodeConsumerNameExists) {\n\t\t\t\tcleanUpSub()\n\t\t\t\tif errors.Is(apiErr, ErrStreamNotFound) {\n\t\t\t\t\treturn nil, ErrStreamNotFound\n\t\t\t\t}\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\t// We will not be using this sub here if we were push based.\n\t\t\tif !isPullMode {\n\t\t\t\tcleanUpSub()\n\t\t\t}\n\n\t\t\tinfo, err = js.ConsumerInfo(stream, consumer)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tdeliver, err = processConsInfo(info, o.cfg, isPullMode, subj, queue)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tif !isPullMode {\n\t\t\t\t// We can't reuse the channel, so if one was passed, we need to create a new one.\n\t\t\t\tif isSync {\n\t\t\t\t\tch = make(chan *Msg, cap(ch))\n\t\t\t\t} else if ch != nil {\n\t\t\t\t\t// User provided (ChanSubscription), simply try to drain it.\n\t\t\t\t\tfor done := false; !done; {\n\t\t\t\t\t\tselect {\n\t\t\t\t\t\tcase <-ch:\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\tdone = true\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tjsi.deliver = deliver\n\t\t\t\tjsi.hbi = info.Config.Heartbeat\n\n\t\t\t\t// Recreate the subscription here.\n\t\t\t\tsub, err = nc.subscribe(jsi.deliver, queue, cb, ch, nil, isSync, jsi)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\thasFC = info.Config.FlowControl\n\t\t\t\thasHeartbeats = info.Config.Heartbeat > 0\n\t\t\t}\n\t\t} else {\n\t\t\t// Since the library created the JS consumer, it will delete it on Unsubscribe()/Drain()\n\t\t\tsub.mu.Lock()\n\t\t\tsub.jsi.dc = true\n\t\t\tsub.jsi.pending = info.NumPending + info.Delivered.Consumer\n\t\t\t// If this is an ephemeral, we did not have a consumer name, we get it from the info\n\t\t\t// after the AddConsumer returns.\n\t\t\tif consumer == _EMPTY_ {\n\t\t\t\tsub.jsi.consumer = info.Name\n\t\t\t\tif isPullMode {\n\t\t\t\t\tsub.jsi.nms = fmt.Sprintf(js.apiSubj(apiRequestNextT), stream, info.Name)\n\t\t\t\t}\n\t\t\t}\n\t\t\tsub.mu.Unlock()\n\t\t}\n\t\t// Capture max ack pending from the info response here which covers both\n\t\t// success and failure followed by consumer lookup.\n\t\tmaxap = info.Config.MaxAckPending\n\t}\n\n\t// If maxap is greater than the default sub's pending limit, use that.\n\tif maxap > DefaultSubPendingMsgsLimit {\n\t\t// For bytes limit, use the min of maxp*1MB or DefaultSubPendingBytesLimit\n\t\tbl := maxap * 1024 * 1024\n\t\tif bl < DefaultSubPendingBytesLimit {\n\t\t\tbl = DefaultSubPendingBytesLimit\n\t\t}\n\t\tif err := sub.SetPendingLimits(maxap, bl); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// Do heartbeats last if needed.\n\tif hasHeartbeats {\n\t\tsub.scheduleHeartbeatCheck()\n\t}\n\t// For ChanSubscriptions, if we know that there is flow control, we will\n\t// start a go routine that evaluates the number of delivered messages\n\t// and process flow control.\n\tif sub.Type() == ChanSubscription && hasFC {\n\t\tsub.chanSubcheckForFlowControlResponse()\n\t}\n\n\t// Wait for context to get canceled if there is one.\n\tif ctx != nil {\n\t\tgo func() {\n\t\t\t<-ctx.Done()\n\t\t\tsub.Unsubscribe()\n\t\t}()\n\t}\n\n\treturn sub, nil\n}\n\n// InitialConsumerPending returns the number of messages pending to be\n// delivered to the consumer when the subscription was created.\nfunc (sub *Subscription) InitialConsumerPending() (uint64, error) {\n\tsub.mu.Lock()\n\tdefer sub.mu.Unlock()\n\tif sub.jsi == nil || sub.jsi.consumer == _EMPTY_ {\n\t\treturn 0, fmt.Errorf(\"%w: not a JetStream subscription\", ErrTypeSubscription)\n\t}\n\treturn sub.jsi.pending, nil\n}\n\n// This long-lived routine is used per ChanSubscription to check\n// on the number of delivered messages and check for flow control response.\nfunc (sub *Subscription) chanSubcheckForFlowControlResponse() {\n\tsub.mu.Lock()\n\t// We don't use defer since if we need to send an RC reply, we need\n\t// to do it outside the sub's lock. So doing explicit unlock...\n\tif sub.closed {\n\t\tsub.mu.Unlock()\n\t\treturn\n\t}\n\tvar fcReply string\n\tvar nc *Conn\n\n\tjsi := sub.jsi\n\tif jsi.csfct == nil {\n\t\tjsi.csfct = time.AfterFunc(chanSubFCCheckInterval, sub.chanSubcheckForFlowControlResponse)\n\t} else {\n\t\tfcReply = sub.checkForFlowControlResponse()\n\t\tnc = sub.conn\n\t\t// Do the reset here under the lock, it's ok...\n\t\tjsi.csfct.Reset(chanSubFCCheckInterval)\n\t}\n\tsub.mu.Unlock()\n\t// This call will return an error (which we don't care here)\n\t// if nc is nil or fcReply is empty.\n\tnc.Publish(fcReply, nil)\n}\n\n// ErrConsumerSequenceMismatch represents an error from a consumer\n// that received a Heartbeat including sequence different to the\n// one expected from the view of the client.\ntype ErrConsumerSequenceMismatch struct {\n\t// StreamResumeSequence is the stream sequence from where the consumer\n\t// should resume consuming from the stream.\n\tStreamResumeSequence uint64\n\n\t// ConsumerSequence is the sequence of the consumer that is behind.\n\tConsumerSequence uint64\n\n\t// LastConsumerSequence is the sequence of the consumer when the heartbeat\n\t// was received.\n\tLastConsumerSequence uint64\n}\n\nfunc (ecs *ErrConsumerSequenceMismatch) Error() string {\n\treturn fmt.Sprintf(\"nats: sequence mismatch for consumer at sequence %d (%d sequences behind), should restart consumer from stream sequence %d\",\n\t\tecs.ConsumerSequence,\n\t\tecs.LastConsumerSequence-ecs.ConsumerSequence,\n\t\tecs.StreamResumeSequence,\n\t)\n}\n\n// isJSControlMessage will return true if this is an empty control status message\n// and indicate what type of control message it is, say jsCtrlHB or jsCtrlFC\nfunc isJSControlMessage(msg *Msg) (bool, int) {\n\tif len(msg.Data) > 0 || msg.Header.Get(statusHdr) != controlMsg {\n\t\treturn false, 0\n\t}\n\tval := msg.Header.Get(descrHdr)\n\tif strings.HasPrefix(val, \"Idle\") {\n\t\treturn true, jsCtrlHB\n\t}\n\tif strings.HasPrefix(val, \"Flow\") {\n\t\treturn true, jsCtrlFC\n\t}\n\treturn true, 0\n}\n\n// Keeps track of the incoming message's reply subject so that the consumer's\n// state (deliver sequence, etc..) can be checked against heartbeats.\n// We will also bump the incoming data message sequence that is used in FC cases.\n// Runs under the subscription lock\nfunc (sub *Subscription) trackSequences(reply string) {\n\t// For flow control, keep track of incoming message sequence.\n\tsub.jsi.fciseq++\n\tsub.jsi.cmeta = reply\n}\n\n// Check to make sure messages are arriving in order.\n// Returns true if the sub had to be replaced. Will cause upper layers to return.\n// The caller has verified that sub.jsi != nil and that this is not a control message.\n// Lock should be held.\nfunc (sub *Subscription) checkOrderedMsgs(m *Msg) bool {\n\t// Ignore msgs with no reply like HBs and flow control, they are handled elsewhere.\n\tif m.Reply == _EMPTY_ {\n\t\treturn false\n\t}\n\n\t// Normal message here.\n\ttokens, err := parser.GetMetadataFields(m.Reply)\n\tif err != nil {\n\t\treturn false\n\t}\n\tsseq, dseq := parser.ParseNum(tokens[parser.AckStreamSeqTokenPos]), parser.ParseNum(tokens[parser.AckConsumerSeqTokenPos])\n\n\tjsi := sub.jsi\n\tif dseq != jsi.dseq {\n\t\tsub.resetOrderedConsumer(jsi.sseq + 1)\n\t\treturn true\n\t}\n\t// Update our tracking here.\n\tjsi.dseq, jsi.sseq = dseq+1, sseq\n\treturn false\n}\n\n// Update and replace sid.\n// Lock should be held on entry but will be unlocked to prevent lock inversion.\nfunc (sub *Subscription) applyNewSID() (osid int64) {\n\tnc := sub.conn\n\tsub.mu.Unlock()\n\n\tnc.subsMu.Lock()\n\tosid = sub.sid\n\tdelete(nc.subs, osid)\n\t// Place new one.\n\tnc.ssid++\n\tnsid := nc.ssid\n\tnc.subs[nsid] = sub\n\tnc.subsMu.Unlock()\n\n\tsub.mu.Lock()\n\tsub.sid = nsid\n\treturn osid\n}\n\n// We are here if we have detected a gap with an ordered consumer.\n// We will create a new consumer and rewire the low level subscription.\n// Lock should be held.\nfunc (sub *Subscription) resetOrderedConsumer(sseq uint64) {\n\tnc := sub.conn\n\tif sub.jsi == nil || nc == nil || sub.closed {\n\t\treturn\n\t}\n\n\tvar maxStr string\n\t// If there was an AUTO_UNSUB done, we need to adjust the new value\n\t// to send after the SUB for the new sid.\n\tif sub.max > 0 {\n\t\tif sub.jsi.fciseq < sub.max {\n\t\t\tadjustedMax := sub.max - sub.jsi.fciseq\n\t\t\tmaxStr = strconv.Itoa(int(adjustedMax))\n\t\t} else {\n\t\t\t// We are already at the max, so we should just unsub the\n\t\t\t// existing sub and be done\n\t\t\tgo func(sid int64) {\n\t\t\t\tnc.mu.Lock()\n\t\t\t\tnc.bw.appendString(fmt.Sprintf(unsubProto, sid, _EMPTY_))\n\t\t\t\tnc.kickFlusher()\n\t\t\t\tnc.mu.Unlock()\n\t\t\t}(sub.sid)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Quick unsubscribe. Since we know this is a simple push subscriber we do in place.\n\tosid := sub.applyNewSID()\n\n\t// Grab new inbox.\n\tnewDeliver := nc.NewInbox()\n\tsub.Subject = newDeliver\n\n\t// Snapshot the new sid under sub lock.\n\tnsid := sub.sid\n\n\t// We are still in the low level readLoop for the connection so we need\n\t// to spin a go routine to try to create the new consumer.\n\tgo func() {\n\t\t// Unsubscribe and subscribe with new inbox and sid.\n\t\t// Remap a new low level sub into this sub since its client accessible.\n\t\t// This is done here in this go routine to prevent lock inversion.\n\t\tnc.mu.Lock()\n\t\tnc.bw.appendString(fmt.Sprintf(unsubProto, osid, _EMPTY_))\n\t\tnc.bw.appendString(fmt.Sprintf(subProto, newDeliver, _EMPTY_, nsid))\n\t\tif maxStr != _EMPTY_ {\n\t\t\tnc.bw.appendString(fmt.Sprintf(unsubProto, nsid, maxStr))\n\t\t}\n\t\tnc.kickFlusher()\n\t\tnc.mu.Unlock()\n\n\t\tpushErr := func(err error) {\n\t\t\tnc.handleConsumerSequenceMismatch(sub, fmt.Errorf(\"%w: recreating ordered consumer\", err))\n\t\t\tnc.unsubscribe(sub, 0, true)\n\t\t}\n\n\t\tsub.mu.Lock()\n\t\tjsi := sub.jsi\n\t\t// Reset some items in jsi.\n\t\tjsi.dseq = 1\n\t\tjsi.cmeta = _EMPTY_\n\t\tjsi.fcr, jsi.fcd = _EMPTY_, 0\n\t\tjsi.deliver = newDeliver\n\t\t// Reset consumer request for starting policy.\n\t\tcfg := jsi.ccreq.Config\n\t\tcfg.DeliverSubject = newDeliver\n\t\tcfg.DeliverPolicy = DeliverByStartSequencePolicy\n\t\tcfg.OptStartSeq = sseq\n\t\t// In case the consumer was created with a start time, we need to clear it\n\t\t// since we are now using a start sequence.\n\t\tcfg.OptStartTime = nil\n\n\t\tjs := jsi.js\n\t\tsub.mu.Unlock()\n\n\t\tsub.mu.Lock()\n\t\t// Attempt to delete the existing consumer.\n\t\t// We don't wait for the response since even if it's unsuccessful,\n\t\t// inactivity threshold will kick in and delete it.\n\t\tif jsi.consumer != _EMPTY_ {\n\t\t\tgo js.DeleteConsumer(jsi.stream, jsi.consumer)\n\t\t}\n\t\tjsi.consumer = \"\"\n\t\tsub.mu.Unlock()\n\t\tconsName := getHash(nuid.Next())\n\t\tcinfo, err := js.upsertConsumer(jsi.stream, consName, cfg)\n\t\tif err != nil {\n\t\t\tvar apiErr *APIError\n\t\t\tif errors.Is(err, ErrJetStreamNotEnabled) || errors.Is(err, ErrTimeout) || errors.Is(err, context.DeadlineExceeded) {\n\t\t\t\t// if creating consumer failed, retry\n\t\t\t\treturn\n\t\t\t} else if errors.As(err, &apiErr) && apiErr.ErrorCode == JSErrCodeInsufficientResourcesErr {\n\t\t\t\t// retry for insufficient resources, as it may mean that client is connected to a running\n\t\t\t\t// server in cluster while the server hosting R1 JetStream resources is restarting\n\t\t\t\treturn\n\t\t\t}\n\t\t\tpushErr(err)\n\t\t\treturn\n\t\t}\n\n\t\tsub.mu.Lock()\n\t\tjsi.consumer = cinfo.Name\n\t\tsub.mu.Unlock()\n\t}()\n}\n\n// For jetstream subscriptions, returns the number of delivered messages.\n// For ChanSubscription, this value is computed based on the known number\n// of messages added to the channel minus the current size of that channel.\n// Lock held on entry\nfunc (sub *Subscription) getJSDelivered() uint64 {\n\tif sub.typ == ChanSubscription {\n\t\treturn sub.jsi.fciseq - uint64(len(sub.mch))\n\t}\n\treturn sub.delivered\n}\n\n// checkForFlowControlResponse will check to see if we should send a flow control response\n// based on the subscription current delivered index and the target.\n// Runs under subscription lock\nfunc (sub *Subscription) checkForFlowControlResponse() string {\n\t// Caller has verified that there is a sub.jsi and fc\n\tjsi := sub.jsi\n\tjsi.active = true\n\tif sub.getJSDelivered() >= jsi.fcd {\n\t\tfcr := jsi.fcr\n\t\tjsi.fcr, jsi.fcd = _EMPTY_, 0\n\t\treturn fcr\n\t}\n\treturn _EMPTY_\n}\n\n// Record an inbound flow control message.\n// Runs under subscription lock\nfunc (sub *Subscription) scheduleFlowControlResponse(reply string) {\n\tsub.jsi.fcr, sub.jsi.fcd = reply, sub.jsi.fciseq\n}\n\n// Checks for activity from our consumer.\n// If we do not think we are active send an async error.\nfunc (sub *Subscription) activityCheck() {\n\tsub.mu.Lock()\n\tjsi := sub.jsi\n\tif jsi == nil || sub.closed {\n\t\tsub.mu.Unlock()\n\t\treturn\n\t}\n\n\tactive := jsi.active\n\tjsi.hbc.Reset(jsi.hbi * hbcThresh)\n\tjsi.active = false\n\tnc := sub.conn\n\tsub.mu.Unlock()\n\n\tif !active {\n\t\tif !jsi.ordered || nc.Status() != CONNECTED {\n\t\t\tnc.mu.Lock()\n\t\t\tif errCB := nc.Opts.AsyncErrorCB; errCB != nil {\n\t\t\t\tnc.ach.push(func() { errCB(nc, sub, ErrConsumerNotActive) })\n\t\t\t}\n\t\t\tnc.mu.Unlock()\n\t\t\treturn\n\t\t}\n\t\tsub.mu.Lock()\n\t\tsub.resetOrderedConsumer(jsi.sseq + 1)\n\t\tsub.mu.Unlock()\n\t}\n}\n\n// scheduleHeartbeatCheck sets up the timer check to make sure we are active\n// or receiving idle heartbeats..\nfunc (sub *Subscription) scheduleHeartbeatCheck() {\n\tsub.mu.Lock()\n\tdefer sub.mu.Unlock()\n\n\tjsi := sub.jsi\n\tif jsi == nil {\n\t\treturn\n\t}\n\n\tif jsi.hbc == nil {\n\t\tjsi.hbc = time.AfterFunc(jsi.hbi*hbcThresh, sub.activityCheck)\n\t} else {\n\t\tjsi.hbc.Reset(jsi.hbi * hbcThresh)\n\t}\n}\n\n// handleConsumerSequenceMismatch will send an async error that can be used to restart a push based consumer.\nfunc (nc *Conn) handleConsumerSequenceMismatch(sub *Subscription, err error) {\n\tnc.mu.Lock()\n\terrCB := nc.Opts.AsyncErrorCB\n\tif errCB != nil {\n\t\tnc.ach.push(func() { errCB(nc, sub, err) })\n\t}\n\tnc.mu.Unlock()\n}\n\n// checkForSequenceMismatch will make sure we have not missed any messages since last seen.\nfunc (nc *Conn) checkForSequenceMismatch(msg *Msg, s *Subscription, jsi *jsSub) {\n\t// Process heartbeat received, get latest control metadata if present.\n\ts.mu.Lock()\n\tctrl, ordered := jsi.cmeta, jsi.ordered\n\tjsi.active = true\n\ts.mu.Unlock()\n\n\tif ctrl == _EMPTY_ {\n\t\treturn\n\t}\n\n\ttokens, err := parser.GetMetadataFields(ctrl)\n\tif err != nil {\n\t\treturn\n\t}\n\n\t// Consumer sequence.\n\tvar ldseq string\n\tdseq := tokens[parser.AckConsumerSeqTokenPos]\n\thdr := msg.Header[lastConsumerSeqHdr]\n\tif len(hdr) == 1 {\n\t\tldseq = hdr[0]\n\t}\n\n\t// Detect consumer sequence mismatch and whether\n\t// should restart the consumer.\n\tif ldseq != dseq {\n\t\t// Dispatch async error including details such as\n\t\t// from where the consumer could be restarted.\n\t\tsseq := parser.ParseNum(tokens[parser.AckStreamSeqTokenPos])\n\t\tif ordered {\n\t\t\ts.mu.Lock()\n\t\t\ts.resetOrderedConsumer(jsi.sseq + 1)\n\t\t\ts.mu.Unlock()\n\t\t} else {\n\t\t\tecs := &ErrConsumerSequenceMismatch{\n\t\t\t\tStreamResumeSequence: uint64(sseq),\n\t\t\t\tConsumerSequence:     parser.ParseNum(dseq),\n\t\t\t\tLastConsumerSequence: parser.ParseNum(ldseq),\n\t\t\t}\n\t\t\tnc.handleConsumerSequenceMismatch(s, ecs)\n\t\t}\n\t}\n}\n\ntype streamRequest struct {\n\tSubject string `json:\"subject,omitempty\"`\n}\n\ntype streamNamesResponse struct {\n\tapiResponse\n\tapiPaged\n\tStreams []string `json:\"streams\"`\n}\n\ntype subOpts struct {\n\t// For attaching.\n\tstream, consumer string\n\t// For creating or updating.\n\tcfg *ConsumerConfig\n\t// For binding a subscription to a consumer without creating it.\n\tbound bool\n\t// For manual ack\n\tmack bool\n\t// For an ordered consumer.\n\tordered bool\n\tctx     context.Context\n\n\t// To disable calling ConsumerInfo\n\tskipCInfo bool\n}\n\n// SkipConsumerLookup will omit looking up consumer when [Bind], [Durable]\n// or [ConsumerName] are provided.\n//\n// NOTE: This setting may cause an existing consumer to be overwritten. Also,\n// because consumer lookup is skipped, all consumer options like AckPolicy,\n// DeliverSubject etc. need to be provided even if consumer already exists.\nfunc SkipConsumerLookup() SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.skipCInfo = true\n\t\treturn nil\n\t})\n}\n\n// OrderedConsumer will create a FIFO direct/ephemeral consumer for in order delivery of messages.\n// There are no redeliveries and no acks, and flow control and heartbeats will be added but\n// will be taken care of without additional client code.\nfunc OrderedConsumer() SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.ordered = true\n\t\treturn nil\n\t})\n}\n\n// ManualAck disables auto ack functionality for async subscriptions.\nfunc ManualAck() SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.mack = true\n\t\treturn nil\n\t})\n}\n\n// Description will set the description for the created consumer.\nfunc Description(description string) SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.Description = description\n\t\treturn nil\n\t})\n}\n\n// Durable defines the consumer name for JetStream durable subscribers.\n// This function will return ErrInvalidConsumerName if the name contains\n// any dot \".\".\nfunc Durable(consumer string) SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\tif opts.cfg.Durable != _EMPTY_ {\n\t\t\treturn errors.New(\"nats: option Durable set more than once\")\n\t\t}\n\t\tif opts.consumer != _EMPTY_ && opts.consumer != consumer {\n\t\t\treturn fmt.Errorf(\"nats: duplicate consumer names (%s and %s)\", opts.consumer, consumer)\n\t\t}\n\t\tif err := checkConsumerName(consumer); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\topts.cfg.Durable = consumer\n\t\treturn nil\n\t})\n}\n\n// DeliverAll will configure a Consumer to receive all the\n// messages from a Stream.\nfunc DeliverAll() SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.DeliverPolicy = DeliverAllPolicy\n\t\treturn nil\n\t})\n}\n\n// DeliverLast configures a Consumer to receive messages\n// starting with the latest one.\nfunc DeliverLast() SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.DeliverPolicy = DeliverLastPolicy\n\t\treturn nil\n\t})\n}\n\n// DeliverLastPerSubject configures a Consumer to receive messages\n// starting with the latest one for each filtered subject.\nfunc DeliverLastPerSubject() SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.DeliverPolicy = DeliverLastPerSubjectPolicy\n\t\treturn nil\n\t})\n}\n\n// DeliverNew configures a Consumer to receive messages\n// published after the subscription.\nfunc DeliverNew() SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.DeliverPolicy = DeliverNewPolicy\n\t\treturn nil\n\t})\n}\n\n// StartSequence configures a Consumer to receive\n// messages from a start sequence.\nfunc StartSequence(seq uint64) SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.DeliverPolicy = DeliverByStartSequencePolicy\n\t\topts.cfg.OptStartSeq = seq\n\t\treturn nil\n\t})\n}\n\n// StartTime configures a Consumer to receive\n// messages from a start time.\nfunc StartTime(startTime time.Time) SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.DeliverPolicy = DeliverByStartTimePolicy\n\t\topts.cfg.OptStartTime = &startTime\n\t\treturn nil\n\t})\n}\n\n// AckNone requires no acks for delivered messages.\nfunc AckNone() SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.AckPolicy = AckNonePolicy\n\t\treturn nil\n\t})\n}\n\n// AckAll when acking a sequence number, this implicitly acks all sequences\n// below this one as well.\nfunc AckAll() SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.AckPolicy = AckAllPolicy\n\t\treturn nil\n\t})\n}\n\n// AckExplicit requires ack or nack for all messages.\nfunc AckExplicit() SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.AckPolicy = AckExplicitPolicy\n\t\treturn nil\n\t})\n}\n\n// MaxDeliver sets the number of redeliveries for a message.\nfunc MaxDeliver(n int) SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.MaxDeliver = n\n\t\treturn nil\n\t})\n}\n\n// MaxAckPending sets the number of outstanding acks that are allowed before\n// message delivery is halted.\nfunc MaxAckPending(n int) SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.MaxAckPending = n\n\t\treturn nil\n\t})\n}\n\n// ReplayOriginal replays the messages at the original speed.\nfunc ReplayOriginal() SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.ReplayPolicy = ReplayOriginalPolicy\n\t\treturn nil\n\t})\n}\n\n// ReplayInstant replays the messages as fast as possible.\nfunc ReplayInstant() SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.ReplayPolicy = ReplayInstantPolicy\n\t\treturn nil\n\t})\n}\n\n// RateLimit is the Bits per sec rate limit applied to a push consumer.\nfunc RateLimit(n uint64) SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.RateLimit = n\n\t\treturn nil\n\t})\n}\n\n// BackOff is an array of time durations that represent the time to delay based on delivery count.\nfunc BackOff(backOff []time.Duration) SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.BackOff = backOff\n\t\treturn nil\n\t})\n}\n\n// BindStream binds a consumer to a stream explicitly based on a name.\n// When a stream name is not specified, the library uses the subscribe\n// subject as a way to find the stream name. It is done by making a request\n// to the server to get list of stream names that have a filter for this\n// subject. If the returned list contains a single stream, then this\n// stream name will be used, otherwise the `ErrNoMatchingStream` is returned.\n// To avoid the stream lookup, provide the stream name with this function.\n// See also `Bind()`.\nfunc BindStream(stream string) SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\tif opts.stream != _EMPTY_ && opts.stream != stream {\n\t\t\treturn fmt.Errorf(\"nats: duplicate stream name (%s and %s)\", opts.stream, stream)\n\t\t}\n\n\t\topts.stream = stream\n\t\treturn nil\n\t})\n}\n\n// Bind binds a subscription to an existing consumer from a stream without attempting to create.\n// The first argument is the stream name and the second argument will be the consumer name.\nfunc Bind(stream, consumer string) SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\tif stream == _EMPTY_ {\n\t\t\treturn ErrStreamNameRequired\n\t\t}\n\t\tif consumer == _EMPTY_ {\n\t\t\treturn ErrConsumerNameRequired\n\t\t}\n\n\t\t// In case of pull subscribers, the durable name is a required parameter\n\t\t// so check that they are not different.\n\t\tif opts.cfg.Durable != _EMPTY_ && opts.cfg.Durable != consumer {\n\t\t\treturn fmt.Errorf(\"nats: duplicate consumer names (%s and %s)\", opts.cfg.Durable, consumer)\n\t\t}\n\t\tif opts.stream != _EMPTY_ && opts.stream != stream {\n\t\t\treturn fmt.Errorf(\"nats: duplicate stream name (%s and %s)\", opts.stream, stream)\n\t\t}\n\t\topts.stream = stream\n\t\topts.consumer = consumer\n\t\topts.bound = true\n\t\treturn nil\n\t})\n}\n\n// EnableFlowControl enables flow control for a push based consumer.\nfunc EnableFlowControl() SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.FlowControl = true\n\t\treturn nil\n\t})\n}\n\n// IdleHeartbeat enables push based consumers to have idle heartbeats delivered.\n// For pull consumers, idle heartbeat has to be set on each [Fetch] call.\nfunc IdleHeartbeat(duration time.Duration) SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.Heartbeat = duration\n\t\treturn nil\n\t})\n}\n\n// DeliverSubject specifies the JetStream consumer deliver subject.\n//\n// This option is used only in situations where the consumer does not exist\n// and a creation request is sent to the server. If not provided, an inbox\n// will be selected.\n// If a consumer exists, then the NATS subscription will be created on\n// the JetStream consumer's DeliverSubject, not necessarily this subject.\nfunc DeliverSubject(subject string) SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.DeliverSubject = subject\n\t\treturn nil\n\t})\n}\n\n// HeadersOnly() will instruct the consumer to only deliver headers and no payloads.\nfunc HeadersOnly() SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.HeadersOnly = true\n\t\treturn nil\n\t})\n}\n\n// MaxRequestBatch sets the maximum pull consumer batch size that a Fetch()\n// can request.\nfunc MaxRequestBatch(max int) SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.MaxRequestBatch = max\n\t\treturn nil\n\t})\n}\n\n// MaxRequestExpires sets the maximum pull consumer request expiration that a\n// Fetch() can request (using the Fetch's timeout value).\nfunc MaxRequestExpires(max time.Duration) SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.MaxRequestExpires = max\n\t\treturn nil\n\t})\n}\n\n// MaxRequesMaxBytes sets the maximum pull consumer request bytes that a\n// Fetch() can receive.\nfunc MaxRequestMaxBytes(bytes int) SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.MaxRequestMaxBytes = bytes\n\t\treturn nil\n\t})\n}\n\n// InactiveThreshold indicates how long the server should keep a consumer\n// after detecting a lack of activity. In NATS Server 2.8.4 and earlier, this\n// option only applies to ephemeral consumers. In NATS Server 2.9.0 and later,\n// this option applies to both ephemeral and durable consumers, allowing durable\n// consumers to also be deleted automatically after the inactivity threshold has\n// passed.\nfunc InactiveThreshold(threshold time.Duration) SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\tif threshold < 0 {\n\t\t\treturn fmt.Errorf(\"invalid InactiveThreshold value (%v), needs to be greater or equal to 0\", threshold)\n\t\t}\n\t\topts.cfg.InactiveThreshold = threshold\n\t\treturn nil\n\t})\n}\n\n// ConsumerReplicas sets the number of replica count for a consumer.\nfunc ConsumerReplicas(replicas int) SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\tif replicas < 1 {\n\t\t\treturn fmt.Errorf(\"invalid ConsumerReplicas value (%v), needs to be greater than 0\", replicas)\n\t\t}\n\t\topts.cfg.Replicas = replicas\n\t\treturn nil\n\t})\n}\n\n// ConsumerMemoryStorage sets the memory storage to true for a consumer.\nfunc ConsumerMemoryStorage() SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.MemoryStorage = true\n\t\treturn nil\n\t})\n}\n\n// ConsumerName sets the name for a consumer.\nfunc ConsumerName(name string) SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.Name = name\n\t\treturn nil\n\t})\n}\n\n// ConsumerFilterSubjects can be used to set multiple subject filters on the consumer.\n// It has to be used in conjunction with [nats.BindStream] and\n// with empty 'subject' parameter.\nfunc ConsumerFilterSubjects(subjects ...string) SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.FilterSubjects = subjects\n\t\treturn nil\n\t})\n}\n\nfunc (sub *Subscription) ConsumerInfo() (*ConsumerInfo, error) {\n\tsub.mu.Lock()\n\t// TODO(dlc) - Better way to mark especially if we attach.\n\tif sub.jsi == nil || sub.jsi.consumer == _EMPTY_ {\n\t\tsub.mu.Unlock()\n\t\treturn nil, ErrTypeSubscription\n\t}\n\n\t// Consumer info lookup should fail if in direct mode.\n\tjs := sub.jsi.js\n\tstream, consumer := sub.jsi.stream, sub.jsi.consumer\n\tsub.mu.Unlock()\n\n\treturn js.getConsumerInfo(stream, consumer)\n}\n\ntype pullOpts struct {\n\tmaxBytes int\n\tttl      time.Duration\n\tctx      context.Context\n\thb       time.Duration\n}\n\n// PullOpt are the options that can be passed when pulling a batch of messages.\ntype PullOpt interface {\n\tconfigurePull(opts *pullOpts) error\n}\n\n// PullMaxWaiting defines the max inflight pull requests.\nfunc PullMaxWaiting(n int) SubOpt {\n\treturn subOptFn(func(opts *subOpts) error {\n\t\topts.cfg.MaxWaiting = n\n\t\treturn nil\n\t})\n}\n\ntype PullHeartbeat time.Duration\n\nfunc (h PullHeartbeat) configurePull(opts *pullOpts) error {\n\tif h <= 0 {\n\t\treturn fmt.Errorf(\"%w: idle heartbeat has to be greater than 0\", ErrInvalidArg)\n\t}\n\topts.hb = time.Duration(h)\n\treturn nil\n}\n\n// PullMaxBytes defines the max bytes allowed for a fetch request.\ntype PullMaxBytes int\n\nfunc (n PullMaxBytes) configurePull(opts *pullOpts) error {\n\topts.maxBytes = int(n)\n\treturn nil\n}\n\nvar (\n\t// errNoMessages is an error that a Fetch request using no_wait can receive to signal\n\t// that there are no more messages available.\n\terrNoMessages = errors.New(\"nats: no messages\")\n\n\t// errRequestsPending is an error that represents a sub.Fetch requests that was using\n\t// no_wait and expires time got discarded by the server.\n\terrRequestsPending = errors.New(\"nats: requests pending\")\n)\n\n// Returns if the given message is a user message or not, and if\n// `checkSts` is true, returns appropriate error based on the\n// content of the status (404, etc..)\nfunc checkMsg(msg *Msg, checkSts, isNoWait bool) (usrMsg bool, err error) {\n\t// Assume user message\n\tusrMsg = true\n\n\t// If payload or no header, consider this a user message\n\tif len(msg.Data) > 0 || len(msg.Header) == 0 {\n\t\treturn\n\t}\n\t// Look for status header\n\tval := msg.Header.Get(statusHdr)\n\t// If not present, then this is considered a user message\n\tif val == _EMPTY_ {\n\t\treturn\n\t}\n\t// At this point, this is not a user message since there is\n\t// no payload and a \"Status\" header.\n\tusrMsg = false\n\n\t// If we don't care about status, we are done.\n\tif !checkSts {\n\t\treturn\n\t}\n\n\t// if it's a heartbeat message, report as not user msg\n\tif isHb, _ := isJSControlMessage(msg); isHb {\n\t\treturn\n\t}\n\tswitch val {\n\tcase noResponders:\n\t\terr = ErrNoResponders\n\tcase noMessagesSts:\n\t\t// 404 indicates that there are no messages.\n\t\terr = errNoMessages\n\tcase reqTimeoutSts:\n\t\t// In case of a fetch request with no wait request and expires time,\n\t\t// need to skip 408 errors and retry.\n\t\tif isNoWait {\n\t\t\terr = errRequestsPending\n\t\t} else {\n\t\t\t// Older servers may send a 408 when a request in the server was expired\n\t\t\t// and interest is still found, which will be the case for our\n\t\t\t// implementation. Regardless, ignore 408 errors until receiving at least\n\t\t\t// one message when making requests without no_wait.\n\t\t\terr = ErrTimeout\n\t\t}\n\tcase jetStream409Sts:\n\t\tif strings.Contains(strings.ToLower(msg.Header.Get(descrHdr)), \"consumer deleted\") {\n\t\t\terr = ErrConsumerDeleted\n\t\t\tbreak\n\t\t}\n\n\t\tif strings.Contains(strings.ToLower(msg.Header.Get(descrHdr)), \"leadership change\") {\n\t\t\terr = ErrConsumerLeadershipChanged\n\t\t\tbreak\n\t\t}\n\t\tfallthrough\n\tdefault:\n\t\terr = fmt.Errorf(\"nats: %s\", msg.Header.Get(descrHdr))\n\t}\n\treturn\n}\n\n// Fetch pulls a batch of messages from a stream for a pull consumer.\nfunc (sub *Subscription) Fetch(batch int, opts ...PullOpt) ([]*Msg, error) {\n\tif sub == nil {\n\t\treturn nil, ErrBadSubscription\n\t}\n\tif batch < 1 {\n\t\treturn nil, ErrInvalidArg\n\t}\n\n\tvar o pullOpts\n\tfor _, opt := range opts {\n\t\tif err := opt.configurePull(&o); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tif o.ctx != nil && o.ttl != 0 {\n\t\treturn nil, ErrContextAndTimeout\n\t}\n\n\tsub.mu.Lock()\n\tjsi := sub.jsi\n\t// Reject if this is not a pull subscription. Note that sub.typ is SyncSubscription,\n\t// so check for jsi.pull boolean instead.\n\tif jsi == nil || !jsi.pull {\n\t\tsub.mu.Unlock()\n\t\treturn nil, ErrTypeSubscription\n\t}\n\n\tnc := sub.conn\n\tnms := sub.jsi.nms\n\trply, _ := newFetchInbox(jsi.deliver)\n\tjs := sub.jsi.js\n\tpmc := len(sub.mch) > 0\n\n\t// All fetch requests have an expiration, in case of no explicit expiration\n\t// then the default timeout of the JetStream context is used.\n\tttl := o.ttl\n\tif ttl == 0 {\n\t\tttl = js.opts.wait\n\t}\n\tsub.mu.Unlock()\n\n\t// Use the given context or setup a default one for the span\n\t// of the pull batch request.\n\tvar (\n\t\tctx    = o.ctx\n\t\terr    error\n\t\tcancel context.CancelFunc\n\t)\n\tif ctx == nil {\n\t\tctx, cancel = context.WithTimeout(context.Background(), ttl)\n\t} else if _, hasDeadline := ctx.Deadline(); !hasDeadline {\n\t\t// Prevent from passing the background context which will just block\n\t\t// and cannot be canceled either.\n\t\tif octx, ok := ctx.(ContextOpt); ok && octx.Context == context.Background() {\n\t\t\treturn nil, ErrNoDeadlineContext\n\t\t}\n\n\t\t// If the context did not have a deadline, then create a new child context\n\t\t// that will use the default timeout from the JS context.\n\t\tctx, cancel = context.WithTimeout(ctx, ttl)\n\t} else {\n\t\tctx, cancel = context.WithCancel(ctx)\n\t}\n\tdefer cancel()\n\n\t// if heartbeat is set, validate it against the context timeout\n\tif o.hb > 0 {\n\t\tdeadline, _ := ctx.Deadline()\n\t\tif 2*o.hb >= time.Until(deadline) {\n\t\t\treturn nil, fmt.Errorf(\"%w: idle heartbeat value too large\", ErrInvalidArg)\n\t\t}\n\t}\n\n\t// Check if context not done already before making the request.\n\tselect {\n\tcase <-ctx.Done():\n\t\tif o.ctx != nil { // Timeout or Cancel triggered by context object option\n\t\t\terr = ctx.Err()\n\t\t} else { // Timeout triggered by timeout option\n\t\t\terr = ErrTimeout\n\t\t}\n\tdefault:\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tmsgs = make([]*Msg, 0, batch)\n\t\tmsg  *Msg\n\t)\n\tfor pmc && len(msgs) < batch {\n\t\t// Check next msg with booleans that say that this is an internal call\n\t\t// for a pull subscribe (so don't reject it) and don't wait if there\n\t\t// are no messages.\n\t\tmsg, err = sub.nextMsgWithContext(ctx, true, false)\n\t\tif err != nil {\n\t\t\tif errors.Is(err, errNoMessages) {\n\t\t\t\terr = nil\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\t\t// Check msg but just to determine if this is a user message\n\t\t// or status message, however, we don't care about values of status\n\t\t// messages at this point in the Fetch() call, so checkMsg can't\n\t\t// return an error.\n\t\tif usrMsg, _ := checkMsg(msg, false, false); usrMsg {\n\t\t\tmsgs = append(msgs, msg)\n\t\t}\n\t}\n\tvar hbTimer *time.Timer\n\tvar hbErr error\n\tsub.mu.Lock()\n\tsubClosed := sub.closed || sub.draining\n\tsub.mu.Unlock()\n\tif subClosed {\n\t\terr = errors.Join(ErrBadSubscription, ErrSubscriptionClosed)\n\t}\n\thbLock := sync.Mutex{}\n\tif err == nil && len(msgs) < batch && !subClosed {\n\t\t// For batch real size of 1, it does not make sense to set no_wait in\n\t\t// the request.\n\t\tnoWait := batch-len(msgs) > 1\n\n\t\tvar nr nextRequest\n\n\t\tsendReq := func() error {\n\t\t\t// The current deadline for the context will be used\n\t\t\t// to set the expires TTL for a fetch request.\n\t\t\tdeadline, _ := ctx.Deadline()\n\t\t\tttl = time.Until(deadline)\n\n\t\t\t// Check if context has already been canceled or expired.\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn ctx.Err()\n\t\t\tdefault:\n\t\t\t}\n\n\t\t\t// Make our request expiration a bit shorter than the current timeout.\n\t\t\texpiresDiff := time.Duration(float64(ttl) * 0.1)\n\t\t\tif expiresDiff > 5*time.Second {\n\t\t\t\texpiresDiff = 5 * time.Second\n\t\t\t}\n\t\t\texpires := ttl - expiresDiff\n\n\t\t\tnr.Batch = batch - len(msgs)\n\t\t\tnr.Expires = expires\n\t\t\tnr.NoWait = noWait\n\t\t\tnr.MaxBytes = o.maxBytes\n\t\t\tif 2*o.hb < expires {\n\t\t\t\tnr.Heartbeat = o.hb\n\t\t\t} else {\n\t\t\t\tnr.Heartbeat = 0\n\t\t\t}\n\t\t\treq, _ := json.Marshal(nr)\n\t\t\tif err := nc.PublishRequest(nms, rply, req); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif o.hb > 0 {\n\t\t\t\tif hbTimer == nil {\n\t\t\t\t\thbTimer = time.AfterFunc(2*o.hb, func() {\n\t\t\t\t\t\thbLock.Lock()\n\t\t\t\t\t\thbErr = ErrNoHeartbeat\n\t\t\t\t\t\thbLock.Unlock()\n\t\t\t\t\t\tcancel()\n\t\t\t\t\t})\n\t\t\t\t} else {\n\t\t\t\t\thbTimer.Reset(2 * o.hb)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\n\t\terr = sendReq()\n\t\tfor err == nil && len(msgs) < batch {\n\t\t\t// Ask for next message and wait if there are no messages\n\t\t\tmsg, err = sub.nextMsgWithContext(ctx, true, true)\n\t\t\tif err == nil {\n\t\t\t\tif hbTimer != nil {\n\t\t\t\t\thbTimer.Reset(2 * o.hb)\n\t\t\t\t}\n\t\t\t\tvar usrMsg bool\n\n\t\t\t\tusrMsg, err = checkMsg(msg, true, noWait)\n\t\t\t\tif err == nil && usrMsg {\n\t\t\t\t\tmsgs = append(msgs, msg)\n\t\t\t\t} else if noWait && (errors.Is(err, errNoMessages) || errors.Is(err, errRequestsPending)) && len(msgs) == 0 {\n\t\t\t\t\t// If we have a 404/408 for our \"no_wait\" request and have\n\t\t\t\t\t// not collected any message, then resend request to\n\t\t\t\t\t// wait this time.\n\t\t\t\t\tnoWait = false\n\t\t\t\t\terr = sendReq()\n\t\t\t\t} else if errors.Is(err, ErrTimeout) && len(msgs) == 0 {\n\t\t\t\t\t// If we get a 408, we will bail if we already collected some\n\t\t\t\t\t// messages, otherwise ignore and go back calling nextMsg.\n\t\t\t\t\terr = nil\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif hbTimer != nil {\n\t\t\thbTimer.Stop()\n\t\t}\n\t}\n\t// If there is at least a message added to msgs, then need to return OK and no error\n\tif err != nil && len(msgs) == 0 {\n\t\thbLock.Lock()\n\t\tdefer hbLock.Unlock()\n\t\tif hbErr != nil {\n\t\t\treturn nil, hbErr\n\t\t}\n\t\treturn nil, o.checkCtxErr(err)\n\t}\n\treturn msgs, nil\n}\n\n// newFetchInbox returns subject used as reply subject when sending pull requests\n// as well as request ID. For non-wildcard subject, request ID is empty and\n// passed subject is not transformed\nfunc newFetchInbox(subj string) (string, string) {\n\tif !strings.HasSuffix(subj, \".*\") {\n\t\treturn subj, \"\"\n\t}\n\treqID := nuid.Next()\n\tvar sb strings.Builder\n\tsb.WriteString(subj[:len(subj)-1])\n\tsb.WriteString(reqID)\n\treturn sb.String(), reqID\n}\n\nfunc subjectMatchesReqID(subject, reqID string) bool {\n\tsubjectParts := strings.Split(subject, \".\")\n\tif len(subjectParts) < 2 {\n\t\treturn false\n\t}\n\treturn subjectParts[len(subjectParts)-1] == reqID\n}\n\n// MessageBatch provides methods to retrieve messages consumed using [Subscribe.FetchBatch].\ntype MessageBatch interface {\n\t// Messages returns a channel on which messages will be published.\n\tMessages() <-chan *Msg\n\n\t// Error returns an error encountered when fetching messages.\n\tError() error\n\n\t// Done signals end of execution.\n\tDone() <-chan struct{}\n}\n\ntype messageBatch struct {\n\tsync.Mutex\n\tmsgs chan *Msg\n\terr  error\n\tdone chan struct{}\n}\n\nfunc (mb *messageBatch) Messages() <-chan *Msg {\n\tmb.Lock()\n\tdefer mb.Unlock()\n\treturn mb.msgs\n}\n\nfunc (mb *messageBatch) Error() error {\n\tmb.Lock()\n\tdefer mb.Unlock()\n\treturn mb.err\n}\n\nfunc (mb *messageBatch) Done() <-chan struct{} {\n\tmb.Lock()\n\tdefer mb.Unlock()\n\treturn mb.done\n}\n\n// FetchBatch pulls a batch of messages from a stream for a pull consumer.\n// Unlike [Subscription.Fetch], it is non blocking and returns [MessageBatch],\n// allowing to retrieve incoming messages from a channel.\n// The returned channel is always closed after all messages for a batch have been\n// delivered by the server - it is safe to iterate over it using range.\n//\n// To avoid using default JetStream timeout as fetch expiry time, use [nats.MaxWait]\n// or [nats.Context] (with deadline set).\n//\n// This method will not return error in case of pull request expiry (even if there are no messages).\n// Any other error encountered when receiving messages will cause FetchBatch to stop receiving new messages.\nfunc (sub *Subscription) FetchBatch(batch int, opts ...PullOpt) (MessageBatch, error) {\n\tif sub == nil {\n\t\treturn nil, ErrBadSubscription\n\t}\n\tif batch < 1 {\n\t\treturn nil, ErrInvalidArg\n\t}\n\n\tvar o pullOpts\n\tfor _, opt := range opts {\n\t\tif err := opt.configurePull(&o); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tif o.ctx != nil && o.ttl != 0 {\n\t\treturn nil, ErrContextAndTimeout\n\t}\n\tsub.mu.Lock()\n\tjsi := sub.jsi\n\t// Reject if this is not a pull subscription. Note that sub.typ is SyncSubscription,\n\t// so check for jsi.pull boolean instead.\n\tif jsi == nil || !jsi.pull {\n\t\tsub.mu.Unlock()\n\t\treturn nil, ErrTypeSubscription\n\t}\n\n\tnc := sub.conn\n\tnms := sub.jsi.nms\n\trply, reqID := newFetchInbox(sub.jsi.deliver)\n\tjs := sub.jsi.js\n\tpmc := len(sub.mch) > 0\n\n\t// All fetch requests have an expiration, in case of no explicit expiration\n\t// then the default timeout of the JetStream context is used.\n\tttl := o.ttl\n\tif ttl == 0 {\n\t\tttl = js.opts.wait\n\t}\n\tsub.mu.Unlock()\n\n\t// Use the given context or setup a default one for the span\n\t// of the pull batch request.\n\tvar (\n\t\tctx           = o.ctx\n\t\tcancel        context.CancelFunc\n\t\tcancelContext = true\n\t)\n\tif ctx == nil {\n\t\tctx, cancel = context.WithTimeout(context.Background(), ttl)\n\t} else if _, hasDeadline := ctx.Deadline(); !hasDeadline {\n\t\t// Prevent from passing the background context which will just block\n\t\t// and cannot be canceled either.\n\t\tif octx, ok := ctx.(ContextOpt); ok && octx.Context == context.Background() {\n\t\t\treturn nil, ErrNoDeadlineContext\n\t\t}\n\n\t\t// If the context did not have a deadline, then create a new child context\n\t\t// that will use the default timeout from the JS context.\n\t\tctx, cancel = context.WithTimeout(ctx, ttl)\n\t} else {\n\t\tctx, cancel = context.WithCancel(ctx)\n\t}\n\tdefer func() {\n\t\t// only cancel the context here if we are sure the fetching goroutine has not been started yet\n\t\tif cancelContext {\n\t\t\tcancel()\n\t\t}\n\t}()\n\n\t// if heartbeat is set, validate it against the context timeout\n\tif o.hb > 0 {\n\t\tdeadline, _ := ctx.Deadline()\n\t\tif 2*o.hb >= time.Until(deadline) {\n\t\t\treturn nil, fmt.Errorf(\"%w: idle heartbeat value too large\", ErrInvalidArg)\n\t\t}\n\t}\n\n\t// Check if context not done already before making the request.\n\tselect {\n\tcase <-ctx.Done():\n\t\tif o.ctx != nil { // Timeout or Cancel triggered by context object option\n\t\t\treturn nil, ctx.Err()\n\t\t} else { // Timeout triggered by timeout option\n\t\t\treturn nil, ErrTimeout\n\t\t}\n\tdefault:\n\t}\n\n\tresult := &messageBatch{\n\t\tmsgs: make(chan *Msg, batch),\n\t\tdone: make(chan struct{}, 1),\n\t}\n\tvar msg *Msg\n\tfor pmc && len(result.msgs) < batch {\n\t\t// Check next msg with booleans that say that this is an internal call\n\t\t// for a pull subscribe (so don't reject it) and don't wait if there\n\t\t// are no messages.\n\t\tmsg, err := sub.nextMsgWithContext(ctx, true, false)\n\t\tif err != nil {\n\t\t\tif errors.Is(err, errNoMessages) {\n\t\t\t\terr = nil\n\t\t\t}\n\t\t\tresult.err = err\n\t\t\tbreak\n\t\t}\n\t\t// Check msg but just to determine if this is a user message\n\t\t// or status message, however, we don't care about values of status\n\t\t// messages at this point in the Fetch() call, so checkMsg can't\n\t\t// return an error.\n\t\tif usrMsg, _ := checkMsg(msg, false, false); usrMsg {\n\t\t\tresult.msgs <- msg\n\t\t}\n\t}\n\tsub.mu.Lock()\n\tsubClosed := sub.closed || sub.draining\n\tsub.mu.Unlock()\n\tif len(result.msgs) == batch || result.err != nil || subClosed {\n\t\tclose(result.msgs)\n\t\tif subClosed && len(result.msgs) == 0 {\n\t\t\treturn nil, errors.Join(ErrBadSubscription, ErrSubscriptionClosed)\n\t\t}\n\t\tresult.done <- struct{}{}\n\t\treturn result, nil\n\t}\n\n\tdeadline, _ := ctx.Deadline()\n\tttl = time.Until(deadline)\n\n\t// Make our request expiration a bit shorter than the current timeout.\n\texpiresDiff := time.Duration(float64(ttl) * 0.1)\n\tif expiresDiff > 5*time.Second {\n\t\texpiresDiff = 5 * time.Second\n\t}\n\texpires := ttl - expiresDiff\n\n\trequestBatch := batch - len(result.msgs)\n\treq := nextRequest{\n\t\tExpires:   expires,\n\t\tBatch:     requestBatch,\n\t\tMaxBytes:  o.maxBytes,\n\t\tHeartbeat: o.hb,\n\t}\n\treqJSON, err := json.Marshal(req)\n\tif err != nil {\n\t\tclose(result.msgs)\n\t\tresult.done <- struct{}{}\n\t\tresult.err = err\n\t\treturn result, nil\n\t}\n\tif err := nc.PublishRequest(nms, rply, reqJSON); err != nil {\n\t\tif len(result.msgs) == 0 {\n\t\t\treturn nil, err\n\t\t}\n\t\tclose(result.msgs)\n\t\tresult.done <- struct{}{}\n\t\tresult.err = err\n\t\treturn result, nil\n\t}\n\tvar hbTimer *time.Timer\n\tvar hbErr error\n\tif o.hb > 0 {\n\t\thbTimer = time.AfterFunc(2*o.hb, func() {\n\t\t\tresult.Lock()\n\t\t\thbErr = ErrNoHeartbeat\n\t\t\tresult.Unlock()\n\t\t\tcancel()\n\t\t})\n\t}\n\tcancelContext = false\n\tgo func() {\n\t\tdefer cancel()\n\t\tvar requestMsgs int\n\t\tfor requestMsgs < requestBatch {\n\t\t\t// Ask for next message and wait if there are no messages\n\t\t\tmsg, err = sub.nextMsgWithContext(ctx, true, true)\n\t\t\tif err != nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif hbTimer != nil {\n\t\t\t\thbTimer.Reset(2 * o.hb)\n\t\t\t}\n\t\t\tvar usrMsg bool\n\n\t\t\tusrMsg, err = checkMsg(msg, true, false)\n\t\t\tif err != nil {\n\t\t\t\tif errors.Is(err, ErrTimeout) {\n\t\t\t\t\tif reqID != \"\" && !subjectMatchesReqID(msg.Subject, reqID) {\n\t\t\t\t\t\t// ignore timeout message from server if it comes from a different pull request\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\terr = nil\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif usrMsg {\n\t\t\t\tresult.Lock()\n\t\t\t\tresult.msgs <- msg\n\t\t\t\tresult.Unlock()\n\t\t\t\trequestMsgs++\n\t\t\t}\n\t\t}\n\t\tif err != nil {\n\t\t\tresult.Lock()\n\t\t\tif hbErr != nil {\n\t\t\t\tresult.err = hbErr\n\t\t\t} else {\n\t\t\t\tresult.err = o.checkCtxErr(err)\n\t\t\t}\n\t\t\tresult.Unlock()\n\t\t}\n\t\tclose(result.msgs)\n\t\tresult.Lock()\n\t\tresult.done <- struct{}{}\n\t\tresult.Unlock()\n\t}()\n\treturn result, nil\n}\n\n// checkCtxErr is used to determine whether ErrTimeout should be returned in case of context timeout\nfunc (o *pullOpts) checkCtxErr(err error) error {\n\tif o.ctx == nil && errors.Is(err, context.DeadlineExceeded) {\n\t\treturn ErrTimeout\n\t}\n\treturn err\n}\n\nfunc (js *js) getConsumerInfo(stream, consumer string) (*ConsumerInfo, error) {\n\tctx, cancel := context.WithTimeout(context.Background(), js.opts.wait)\n\tdefer cancel()\n\treturn js.getConsumerInfoContext(ctx, stream, consumer)\n}\n\nfunc (js *js) getConsumerInfoContext(ctx context.Context, stream, consumer string) (*ConsumerInfo, error) {\n\tccInfoSubj := fmt.Sprintf(apiConsumerInfoT, stream, consumer)\n\tresp, err := js.apiRequestWithContext(ctx, js.apiSubj(ccInfoSubj), nil)\n\tif err != nil {\n\t\tif errors.Is(err, ErrNoResponders) {\n\t\t\terr = ErrJetStreamNotEnabled\n\t\t}\n\t\treturn nil, err\n\t}\n\n\tvar info consumerResponse\n\tif err := json.Unmarshal(resp.Data, &info); err != nil {\n\t\treturn nil, err\n\t}\n\tif info.Error != nil {\n\t\tif errors.Is(info.Error, ErrConsumerNotFound) {\n\t\t\treturn nil, ErrConsumerNotFound\n\t\t}\n\t\tif errors.Is(info.Error, ErrStreamNotFound) {\n\t\t\treturn nil, ErrStreamNotFound\n\t\t}\n\t\treturn nil, info.Error\n\t}\n\tif info.Error == nil && info.ConsumerInfo == nil {\n\t\treturn nil, ErrConsumerNotFound\n\t}\n\treturn info.ConsumerInfo, nil\n}\n\n// a RequestWithContext with tracing via TraceCB\nfunc (js *js) apiRequestWithContext(ctx context.Context, subj string, data []byte) (*Msg, error) {\n\tif js.opts.shouldTrace {\n\t\tctrace := js.opts.ctrace\n\t\tif ctrace.RequestSent != nil {\n\t\t\tctrace.RequestSent(subj, data)\n\t\t}\n\t}\n\tresp, err := js.nc.RequestWithContext(ctx, subj, data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif js.opts.shouldTrace {\n\t\tctrace := js.opts.ctrace\n\t\tif ctrace.RequestSent != nil {\n\t\t\tctrace.ResponseReceived(subj, resp.Data, resp.Header)\n\t\t}\n\t}\n\n\treturn resp, nil\n}\n\nfunc (m *Msg) checkReply() error {\n\tif m == nil || m.Sub == nil {\n\t\treturn ErrMsgNotBound\n\t}\n\tif m.Reply == _EMPTY_ {\n\t\treturn ErrMsgNoReply\n\t}\n\treturn nil\n}\n\n// ackReply handles all acks. Will do the right thing for pull and sync mode.\n// It ensures that an ack is only sent a single time, regardless of\n// how many times it is being called to avoid duplicated acks.\nfunc (m *Msg) ackReply(ackType []byte, sync bool, opts ...AckOpt) error {\n\tvar o ackOpts\n\tfor _, opt := range opts {\n\t\tif err := opt.configureAck(&o); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif err := m.checkReply(); err != nil {\n\t\treturn err\n\t}\n\n\tvar ackNone bool\n\tvar js *js\n\n\tsub := m.Sub\n\tsub.mu.Lock()\n\tnc := sub.conn\n\tif jsi := sub.jsi; jsi != nil {\n\t\tjs = jsi.js\n\t\tackNone = jsi.ackNone\n\t}\n\tsub.mu.Unlock()\n\n\t// Skip if already acked.\n\tif atomic.LoadUint32(&m.ackd) == 1 {\n\t\treturn ErrMsgAlreadyAckd\n\t}\n\tif ackNone {\n\t\treturn ErrCantAckIfConsumerAckNone\n\t}\n\n\tusesCtx := o.ctx != nil\n\tusesWait := o.ttl > 0\n\n\t// Only allow either AckWait or Context option to set the timeout.\n\tif usesWait && usesCtx {\n\t\treturn ErrContextAndTimeout\n\t}\n\n\tsync = sync || usesCtx || usesWait\n\tctx := o.ctx\n\twait := defaultRequestWait\n\tif usesWait {\n\t\twait = o.ttl\n\t} else if js != nil {\n\t\twait = js.opts.wait\n\t}\n\n\tvar body []byte\n\tvar err error\n\t// This will be > 0 only when called from NakWithDelay()\n\tif o.nakDelay > 0 {\n\t\tbody = []byte(fmt.Sprintf(\"%s {\\\"delay\\\": %d}\", ackType, o.nakDelay.Nanoseconds()))\n\t} else {\n\t\tbody = ackType\n\t}\n\n\tif sync {\n\t\tif usesCtx {\n\t\t\t_, err = nc.RequestWithContext(ctx, m.Reply, body)\n\t\t} else {\n\t\t\t_, err = nc.Request(m.Reply, body, wait)\n\t\t}\n\t} else {\n\t\terr = nc.Publish(m.Reply, body)\n\t}\n\n\t// Mark that the message has been acked unless it is ackProgress\n\t// which can be sent many times.\n\tif err == nil && !bytes.Equal(ackType, ackProgress) {\n\t\tatomic.StoreUint32(&m.ackd, 1)\n\t}\n\n\treturn err\n}\n\n// Ack acknowledges a message. This tells the server that the message was\n// successfully processed and it can move on to the next message.\nfunc (m *Msg) Ack(opts ...AckOpt) error {\n\treturn m.ackReply(ackAck, false, opts...)\n}\n\n// AckSync is the synchronous version of Ack. This indicates successful message\n// processing.\nfunc (m *Msg) AckSync(opts ...AckOpt) error {\n\treturn m.ackReply(ackAck, true, opts...)\n}\n\n// Nak negatively acknowledges a message. This tells the server to redeliver\n// the message. You can configure the number of redeliveries by passing\n// nats.MaxDeliver when you Subscribe. The default is infinite redeliveries.\nfunc (m *Msg) Nak(opts ...AckOpt) error {\n\treturn m.ackReply(ackNak, false, opts...)\n}\n\n// Nak negatively acknowledges a message. This tells the server to redeliver\n// the message after the give `delay` duration. You can configure the number\n// of redeliveries by passing nats.MaxDeliver when you Subscribe.\n// The default is infinite redeliveries.\nfunc (m *Msg) NakWithDelay(delay time.Duration, opts ...AckOpt) error {\n\tif delay > 0 {\n\t\topts = append(opts, nakDelay(delay))\n\t}\n\treturn m.ackReply(ackNak, false, opts...)\n}\n\n// Term tells the server to not redeliver this message, regardless of the value\n// of nats.MaxDeliver.\nfunc (m *Msg) Term(opts ...AckOpt) error {\n\treturn m.ackReply(ackTerm, false, opts...)\n}\n\n// InProgress tells the server that this message is being worked on. It resets\n// the redelivery timer on the server.\nfunc (m *Msg) InProgress(opts ...AckOpt) error {\n\treturn m.ackReply(ackProgress, false, opts...)\n}\n\n// MsgMetadata is the JetStream metadata associated with received messages.\ntype MsgMetadata struct {\n\tSequence     SequencePair\n\tNumDelivered uint64\n\tNumPending   uint64\n\tTimestamp    time.Time\n\tStream       string\n\tConsumer     string\n\tDomain       string\n}\n\n// Metadata retrieves the metadata from a JetStream message. This method will\n// return an error for non-JetStream Msgs.\nfunc (m *Msg) Metadata() (*MsgMetadata, error) {\n\tif err := m.checkReply(); err != nil {\n\t\treturn nil, err\n\t}\n\n\ttokens, err := parser.GetMetadataFields(m.Reply)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tmeta := &MsgMetadata{\n\t\tDomain:       tokens[parser.AckDomainTokenPos],\n\t\tNumDelivered: parser.ParseNum(tokens[parser.AckNumDeliveredTokenPos]),\n\t\tNumPending:   parser.ParseNum(tokens[parser.AckNumPendingTokenPos]),\n\t\tTimestamp:    time.Unix(0, int64(parser.ParseNum(tokens[parser.AckTimestampSeqTokenPos]))),\n\t\tStream:       tokens[parser.AckStreamTokenPos],\n\t\tConsumer:     tokens[parser.AckConsumerTokenPos],\n\t}\n\tmeta.Sequence.Stream = parser.ParseNum(tokens[parser.AckStreamSeqTokenPos])\n\tmeta.Sequence.Consumer = parser.ParseNum(tokens[parser.AckConsumerSeqTokenPos])\n\treturn meta, nil\n}\n\n// AckPolicy determines how the consumer should acknowledge delivered messages.\ntype AckPolicy int\n\nconst (\n\t// AckNonePolicy requires no acks for delivered messages.\n\tAckNonePolicy AckPolicy = iota\n\n\t// AckAllPolicy when acking a sequence number, this implicitly acks all\n\t// sequences below this one as well.\n\tAckAllPolicy\n\n\t// AckExplicitPolicy requires ack or nack for all messages.\n\tAckExplicitPolicy\n\n\t// For configuration mismatch check\n\tackPolicyNotSet = 99\n)\n\nfunc jsonString(s string) string {\n\treturn \"\\\"\" + s + \"\\\"\"\n}\n\nfunc (p *AckPolicy) UnmarshalJSON(data []byte) error {\n\tswitch string(data) {\n\tcase jsonString(\"none\"):\n\t\t*p = AckNonePolicy\n\tcase jsonString(\"all\"):\n\t\t*p = AckAllPolicy\n\tcase jsonString(\"explicit\"):\n\t\t*p = AckExplicitPolicy\n\tdefault:\n\t\treturn fmt.Errorf(\"nats: can not unmarshal %q\", data)\n\t}\n\n\treturn nil\n}\n\nfunc (p AckPolicy) MarshalJSON() ([]byte, error) {\n\tswitch p {\n\tcase AckNonePolicy:\n\t\treturn json.Marshal(\"none\")\n\tcase AckAllPolicy:\n\t\treturn json.Marshal(\"all\")\n\tcase AckExplicitPolicy:\n\t\treturn json.Marshal(\"explicit\")\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"nats: unknown acknowledgement policy %v\", p)\n\t}\n}\n\nfunc (p AckPolicy) String() string {\n\tswitch p {\n\tcase AckNonePolicy:\n\t\treturn \"AckNone\"\n\tcase AckAllPolicy:\n\t\treturn \"AckAll\"\n\tcase AckExplicitPolicy:\n\t\treturn \"AckExplicit\"\n\tcase ackPolicyNotSet:\n\t\treturn \"Not Initialized\"\n\tdefault:\n\t\treturn \"Unknown AckPolicy\"\n\t}\n}\n\n// ReplayPolicy determines how the consumer should replay messages it already has queued in the stream.\ntype ReplayPolicy int\n\nconst (\n\t// ReplayInstantPolicy will replay messages as fast as possible.\n\tReplayInstantPolicy ReplayPolicy = iota\n\n\t// ReplayOriginalPolicy will maintain the same timing as the messages were received.\n\tReplayOriginalPolicy\n\n\t// For configuration mismatch check\n\treplayPolicyNotSet = 99\n)\n\nfunc (p *ReplayPolicy) UnmarshalJSON(data []byte) error {\n\tswitch string(data) {\n\tcase jsonString(\"instant\"):\n\t\t*p = ReplayInstantPolicy\n\tcase jsonString(\"original\"):\n\t\t*p = ReplayOriginalPolicy\n\tdefault:\n\t\treturn fmt.Errorf(\"nats: can not unmarshal %q\", data)\n\t}\n\n\treturn nil\n}\n\nfunc (p ReplayPolicy) MarshalJSON() ([]byte, error) {\n\tswitch p {\n\tcase ReplayOriginalPolicy:\n\t\treturn json.Marshal(\"original\")\n\tcase ReplayInstantPolicy:\n\t\treturn json.Marshal(\"instant\")\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"nats: unknown replay policy %v\", p)\n\t}\n}\n\nvar (\n\tackAck      = []byte(\"+ACK\")\n\tackNak      = []byte(\"-NAK\")\n\tackProgress = []byte(\"+WPI\")\n\tackTerm     = []byte(\"+TERM\")\n)\n\n// DeliverPolicy determines how the consumer should select the first message to deliver.\ntype DeliverPolicy int\n\nconst (\n\t// DeliverAllPolicy starts delivering messages from the very beginning of a\n\t// stream. This is the default.\n\tDeliverAllPolicy DeliverPolicy = iota\n\n\t// DeliverLastPolicy will start the consumer with the last sequence\n\t// received.\n\tDeliverLastPolicy\n\n\t// DeliverNewPolicy will only deliver new messages that are sent after the\n\t// consumer is created.\n\tDeliverNewPolicy\n\n\t// DeliverByStartSequencePolicy will deliver messages starting from a given\n\t// sequence.\n\tDeliverByStartSequencePolicy\n\n\t// DeliverByStartTimePolicy will deliver messages starting from a given\n\t// time.\n\tDeliverByStartTimePolicy\n\n\t// DeliverLastPerSubjectPolicy will start the consumer with the last message\n\t// for all subjects received.\n\tDeliverLastPerSubjectPolicy\n\n\t// For configuration mismatch check\n\tdeliverPolicyNotSet = 99\n)\n\nfunc (p *DeliverPolicy) UnmarshalJSON(data []byte) error {\n\tswitch string(data) {\n\tcase jsonString(\"all\"), jsonString(\"undefined\"):\n\t\t*p = DeliverAllPolicy\n\tcase jsonString(\"last\"):\n\t\t*p = DeliverLastPolicy\n\tcase jsonString(\"new\"):\n\t\t*p = DeliverNewPolicy\n\tcase jsonString(\"by_start_sequence\"):\n\t\t*p = DeliverByStartSequencePolicy\n\tcase jsonString(\"by_start_time\"):\n\t\t*p = DeliverByStartTimePolicy\n\tcase jsonString(\"last_per_subject\"):\n\t\t*p = DeliverLastPerSubjectPolicy\n\t}\n\n\treturn nil\n}\n\nfunc (p DeliverPolicy) MarshalJSON() ([]byte, error) {\n\tswitch p {\n\tcase DeliverAllPolicy:\n\t\treturn json.Marshal(\"all\")\n\tcase DeliverLastPolicy:\n\t\treturn json.Marshal(\"last\")\n\tcase DeliverNewPolicy:\n\t\treturn json.Marshal(\"new\")\n\tcase DeliverByStartSequencePolicy:\n\t\treturn json.Marshal(\"by_start_sequence\")\n\tcase DeliverByStartTimePolicy:\n\t\treturn json.Marshal(\"by_start_time\")\n\tcase DeliverLastPerSubjectPolicy:\n\t\treturn json.Marshal(\"last_per_subject\")\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"nats: unknown deliver policy %v\", p)\n\t}\n}\n\n// RetentionPolicy determines how messages in a set are retained.\ntype RetentionPolicy int\n\nconst (\n\t// LimitsPolicy (default) means that messages are retained until any given limit is reached.\n\t// This could be one of MaxMsgs, MaxBytes, or MaxAge.\n\tLimitsPolicy RetentionPolicy = iota\n\t// InterestPolicy specifies that when all known observables have acknowledged a message it can be removed.\n\tInterestPolicy\n\t// WorkQueuePolicy specifies that when the first worker or subscriber acknowledges the message it can be removed.\n\tWorkQueuePolicy\n)\n\n// DiscardPolicy determines how to proceed when limits of messages or bytes are\n// reached.\ntype DiscardPolicy int\n\nconst (\n\t// DiscardOld will remove older messages to return to the limits. This is\n\t// the default.\n\tDiscardOld DiscardPolicy = iota\n\t//DiscardNew will fail to store new messages.\n\tDiscardNew\n)\n\nconst (\n\tlimitsPolicyString    = \"limits\"\n\tinterestPolicyString  = \"interest\"\n\tworkQueuePolicyString = \"workqueue\"\n)\n\nfunc (rp RetentionPolicy) String() string {\n\tswitch rp {\n\tcase LimitsPolicy:\n\t\treturn \"Limits\"\n\tcase InterestPolicy:\n\t\treturn \"Interest\"\n\tcase WorkQueuePolicy:\n\t\treturn \"WorkQueue\"\n\tdefault:\n\t\treturn \"Unknown Retention Policy\"\n\t}\n}\n\nfunc (rp RetentionPolicy) MarshalJSON() ([]byte, error) {\n\tswitch rp {\n\tcase LimitsPolicy:\n\t\treturn json.Marshal(limitsPolicyString)\n\tcase InterestPolicy:\n\t\treturn json.Marshal(interestPolicyString)\n\tcase WorkQueuePolicy:\n\t\treturn json.Marshal(workQueuePolicyString)\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"nats: can not marshal %v\", rp)\n\t}\n}\n\nfunc (rp *RetentionPolicy) UnmarshalJSON(data []byte) error {\n\tswitch string(data) {\n\tcase jsonString(limitsPolicyString):\n\t\t*rp = LimitsPolicy\n\tcase jsonString(interestPolicyString):\n\t\t*rp = InterestPolicy\n\tcase jsonString(workQueuePolicyString):\n\t\t*rp = WorkQueuePolicy\n\tdefault:\n\t\treturn fmt.Errorf(\"nats: can not unmarshal %q\", data)\n\t}\n\treturn nil\n}\n\nfunc (dp DiscardPolicy) String() string {\n\tswitch dp {\n\tcase DiscardOld:\n\t\treturn \"DiscardOld\"\n\tcase DiscardNew:\n\t\treturn \"DiscardNew\"\n\tdefault:\n\t\treturn \"Unknown Discard Policy\"\n\t}\n}\n\nfunc (dp DiscardPolicy) MarshalJSON() ([]byte, error) {\n\tswitch dp {\n\tcase DiscardOld:\n\t\treturn json.Marshal(\"old\")\n\tcase DiscardNew:\n\t\treturn json.Marshal(\"new\")\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"nats: can not marshal %v\", dp)\n\t}\n}\n\nfunc (dp *DiscardPolicy) UnmarshalJSON(data []byte) error {\n\tswitch strings.ToLower(string(data)) {\n\tcase jsonString(\"old\"):\n\t\t*dp = DiscardOld\n\tcase jsonString(\"new\"):\n\t\t*dp = DiscardNew\n\tdefault:\n\t\treturn fmt.Errorf(\"nats: can not unmarshal %q\", data)\n\t}\n\treturn nil\n}\n\n// StorageType determines how messages are stored for retention.\ntype StorageType int\n\nconst (\n\t// FileStorage specifies on disk storage. It's the default.\n\tFileStorage StorageType = iota\n\t// MemoryStorage specifies in memory only.\n\tMemoryStorage\n)\n\nconst (\n\tmemoryStorageString = \"memory\"\n\tfileStorageString   = \"file\"\n)\n\nfunc (st StorageType) String() string {\n\tswitch st {\n\tcase MemoryStorage:\n\t\treturn \"Memory\"\n\tcase FileStorage:\n\t\treturn \"File\"\n\tdefault:\n\t\treturn \"Unknown Storage Type\"\n\t}\n}\n\nfunc (st StorageType) MarshalJSON() ([]byte, error) {\n\tswitch st {\n\tcase MemoryStorage:\n\t\treturn json.Marshal(memoryStorageString)\n\tcase FileStorage:\n\t\treturn json.Marshal(fileStorageString)\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"nats: can not marshal %v\", st)\n\t}\n}\n\nfunc (st *StorageType) UnmarshalJSON(data []byte) error {\n\tswitch string(data) {\n\tcase jsonString(memoryStorageString):\n\t\t*st = MemoryStorage\n\tcase jsonString(fileStorageString):\n\t\t*st = FileStorage\n\tdefault:\n\t\treturn fmt.Errorf(\"nats: can not unmarshal %q\", data)\n\t}\n\treturn nil\n}\n\ntype StoreCompression uint8\n\nconst (\n\tNoCompression StoreCompression = iota\n\tS2Compression\n)\n\nfunc (alg StoreCompression) String() string {\n\tswitch alg {\n\tcase NoCompression:\n\t\treturn \"None\"\n\tcase S2Compression:\n\t\treturn \"S2\"\n\tdefault:\n\t\treturn \"Unknown StoreCompression\"\n\t}\n}\n\nfunc (alg StoreCompression) MarshalJSON() ([]byte, error) {\n\tvar str string\n\tswitch alg {\n\tcase S2Compression:\n\t\tstr = \"s2\"\n\tcase NoCompression:\n\t\tstr = \"none\"\n\tdefault:\n\t\treturn nil, errors.New(\"unknown compression algorithm\")\n\t}\n\treturn json.Marshal(str)\n}\n\nfunc (alg *StoreCompression) UnmarshalJSON(b []byte) error {\n\tvar str string\n\tif err := json.Unmarshal(b, &str); err != nil {\n\t\treturn err\n\t}\n\tswitch str {\n\tcase \"s2\":\n\t\t*alg = S2Compression\n\tcase \"none\":\n\t\t*alg = NoCompression\n\tdefault:\n\t\treturn errors.New(\"unknown compression algorithm\")\n\t}\n\treturn nil\n}\n\n// Length of our hash used for named consumers.\nconst nameHashLen = 8\n\n// Computes a hash for the given `name`.\nfunc getHash(name string) string {\n\tsha := sha256.New()\n\tsha.Write([]byte(name))\n\tb := sha.Sum(nil)\n\tfor i := 0; i < nameHashLen; i++ {\n\t\tb[i] = rdigits[int(b[i]%base)]\n\t}\n\treturn string(b[:nameHashLen])\n}\n"
        },
        {
          "name": "js_test.go",
          "type": "blob",
          "size": 2.76171875,
          "content": "// Copyright 2012-2023 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nats\n\n////////////////////////////////////////////////////////////////////////////////\n// Package scoped specific tests here..\n////////////////////////////////////////////////////////////////////////////////\n\nimport (\n\t\"strings\"\n\t\"testing\"\n)\n\nfunc TestJetStreamConvertDirectMsgResponseToMsg(t *testing.T) {\n\t// This test checks the conversion of a \"direct get message\" response\n\t// to a JS message based on the content of specific NATS headers.\n\t// It is very specific to the order headers retrieval is made in\n\t// convertDirectGetMsgResponseToMsg(), so it may need adjustment\n\t// if changes are made there.\n\n\tmsg := NewMsg(\"inbox\")\n\n\tcheck := func(errTxt string) {\n\t\tt.Helper()\n\t\tm, err := convertDirectGetMsgResponseToMsg(\"test\", msg)\n\t\tif err == nil || !strings.Contains(err.Error(), errTxt) {\n\t\t\tt.Fatalf(\"Expected error contain %q, got %v\", errTxt, err)\n\t\t}\n\t\tif m != nil {\n\t\t\tt.Fatalf(\"Expected nil message, got %v\", m)\n\t\t}\n\t}\n\n\tcheck(\"should have headers\")\n\n\tmsg.Header.Set(statusHdr, noMessagesSts)\n\tcheck(ErrMsgNotFound.Error())\n\n\tmsg.Header.Set(statusHdr, reqTimeoutSts)\n\tcheck(\"unable to get message\")\n\n\tmsg.Header.Set(descrHdr, \"some error text\")\n\tcheck(\"some error text\")\n\n\tmsg.Header.Del(statusHdr)\n\tmsg.Header.Del(descrHdr)\n\tmsg.Header.Set(\"some\", \"header\")\n\tcheck(\"missing stream\")\n\n\tmsg.Header.Set(JSStream, \"test\")\n\tcheck(\"missing sequence\")\n\n\tmsg.Header.Set(JSSequence, \"abc\")\n\tcheck(\"invalid sequence\")\n\n\tmsg.Header.Set(JSSequence, \"1\")\n\tcheck(\"missing timestamp\")\n\n\tmsg.Header.Set(JSTimeStamp, \"aaaaaaaaa bbbbbbbbbbbb cccccccccc ddddddddddd eeeeeeeeee ffffff\")\n\tcheck(\"invalid timestamp\")\n\n\tmsg.Header.Set(JSTimeStamp, \"2006-01-02 15:04:05.999999999 +0000 UTC\")\n\tcheck(\"missing subject\")\n\n\tmsg.Header.Set(JSSubject, \"foo\")\n\tr, err := convertDirectGetMsgResponseToMsg(\"test\", msg)\n\tif err != nil {\n\t\tt.Fatalf(\"Error during convert: %v\", err)\n\t}\n\tif r.Subject != \"foo\" {\n\t\tt.Fatalf(\"Expected subject to be 'foo', got %q\", r.Subject)\n\t}\n\tif r.Sequence != 1 {\n\t\tt.Fatalf(\"Expected sequence to be 1, got %v\", r.Sequence)\n\t}\n\tif r.Time.UnixNano() != 0xFC4A4D639917BFF {\n\t\tt.Fatalf(\"Invalid timestamp: %v\", r.Time.UnixNano())\n\t}\n\tif r.Header.Get(\"some\") != \"header\" {\n\t\tt.Fatalf(\"Wrong header: %v\", r.Header)\n\t}\n}\n"
        },
        {
          "name": "jserrors.go",
          "type": "blob",
          "size": 12.4482421875,
          "content": "// Copyright 2020-2023 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nats\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n)\n\nvar (\n\t// API errors\n\n\t// ErrJetStreamNotEnabled is an error returned when JetStream is not enabled for an account.\n\t//\n\t// Note: This error will not be returned in clustered mode, even if each\n\t// server in the cluster does not have JetStream enabled. In clustered mode,\n\t// requests will time out instead.\n\tErrJetStreamNotEnabled JetStreamError = &jsError{apiErr: &APIError{ErrorCode: JSErrCodeJetStreamNotEnabled, Description: \"jetstream not enabled\", Code: 503}}\n\n\t// ErrJetStreamNotEnabledForAccount is an error returned when JetStream is not enabled for an account.\n\tErrJetStreamNotEnabledForAccount JetStreamError = &jsError{apiErr: &APIError{ErrorCode: JSErrCodeJetStreamNotEnabledForAccount, Description: \"jetstream not enabled for account\", Code: 503}}\n\n\t// ErrStreamNotFound is an error returned when stream with given name does not exist.\n\tErrStreamNotFound JetStreamError = &jsError{apiErr: &APIError{ErrorCode: JSErrCodeStreamNotFound, Description: \"stream not found\", Code: 404}}\n\n\t// ErrStreamNameAlreadyInUse is returned when a stream with given name already exists and has a different configuration.\n\tErrStreamNameAlreadyInUse JetStreamError = &jsError{apiErr: &APIError{ErrorCode: JSErrCodeStreamNameInUse, Description: \"stream name already in use\", Code: 400}}\n\n\t// ErrStreamSubjectTransformNotSupported is returned when the connected nats-server version does not support setting\n\t// the stream subject transform. If this error is returned when executing AddStream(), the stream with invalid\n\t// configuration was already created in the server.\n\tErrStreamSubjectTransformNotSupported JetStreamError = &jsError{message: \"stream subject transformation not supported by nats-server\"}\n\n\t// ErrStreamSourceSubjectTransformNotSupported is returned when the connected nats-server version does not support setting\n\t// the stream source subject transform. If this error is returned when executing AddStream(), the stream with invalid\n\t// configuration was already created in the server.\n\tErrStreamSourceSubjectTransformNotSupported JetStreamError = &jsError{message: \"stream subject transformation not supported by nats-server\"}\n\n\t// ErrStreamSourceNotSupported is returned when the connected nats-server version does not support setting\n\t// the stream sources. If this error is returned when executing AddStream(), the stream with invalid\n\t// configuration was already created in the server.\n\tErrStreamSourceNotSupported JetStreamError = &jsError{message: \"stream sourcing is not supported by nats-server\"}\n\n\t// ErrStreamSourceMultipleSubjectTransformsNotSupported is returned when the connected nats-server version does not support setting\n\t// the stream sources. If this error is returned when executing AddStream(), the stream with invalid\n\t// configuration was already created in the server.\n\tErrStreamSourceMultipleSubjectTransformsNotSupported JetStreamError = &jsError{message: \"stream sourcing with multiple subject transforms not supported by nats-server\"}\n\n\t// ErrConsumerNotFound is an error returned when consumer with given name does not exist.\n\tErrConsumerNotFound JetStreamError = &jsError{apiErr: &APIError{ErrorCode: JSErrCodeConsumerNotFound, Description: \"consumer not found\", Code: 404}}\n\n\t// ErrMsgNotFound is returned when message with provided sequence number does npt exist.\n\tErrMsgNotFound JetStreamError = &jsError{apiErr: &APIError{ErrorCode: JSErrCodeMessageNotFound, Description: \"message not found\", Code: 404}}\n\n\t// ErrBadRequest is returned when invalid request is sent to JetStream API.\n\tErrBadRequest JetStreamError = &jsError{apiErr: &APIError{ErrorCode: JSErrCodeBadRequest, Description: \"bad request\", Code: 400}}\n\n\t// ErrDuplicateFilterSubjects is returned when both FilterSubject and FilterSubjects are specified when creating consumer.\n\tErrDuplicateFilterSubjects JetStreamError = &jsError{apiErr: &APIError{ErrorCode: JSErrCodeDuplicateFilterSubjects, Description: \"consumer cannot have both FilterSubject and FilterSubjects specified\", Code: 500}}\n\n\t// ErrDuplicateFilterSubjects is returned when filter subjects overlap when creating consumer.\n\tErrOverlappingFilterSubjects JetStreamError = &jsError{apiErr: &APIError{ErrorCode: JSErrCodeOverlappingFilterSubjects, Description: \"consumer subject filters cannot overlap\", Code: 500}}\n\n\t// ErrEmptyFilter is returned when a filter in FilterSubjects is empty.\n\tErrEmptyFilter JetStreamError = &jsError{apiErr: &APIError{ErrorCode: JSErrCodeConsumerEmptyFilter, Description: \"consumer filter in FilterSubjects cannot be empty\", Code: 500}}\n\n\t// Client errors\n\n\t// ErrConsumerNameAlreadyInUse is an error returned when consumer with given name already exists.\n\tErrConsumerNameAlreadyInUse JetStreamError = &jsError{message: \"consumer name already in use\"}\n\n\t// ErrConsumerNotActive is an error returned when consumer is not active.\n\tErrConsumerNotActive JetStreamError = &jsError{message: \"consumer not active\"}\n\n\t// ErrInvalidJSAck is returned when JetStream ack from message publish is invalid.\n\tErrInvalidJSAck JetStreamError = &jsError{message: \"invalid jetstream publish response\"}\n\n\t// ErrStreamConfigRequired is returned when empty stream configuration is supplied to add/update stream.\n\tErrStreamConfigRequired JetStreamError = &jsError{message: \"stream configuration is required\"}\n\n\t// ErrStreamNameRequired is returned when the provided stream name is empty.\n\tErrStreamNameRequired JetStreamError = &jsError{message: \"stream name is required\"}\n\n\t// ErrConsumerNameRequired is returned when the provided consumer durable name is empty.\n\tErrConsumerNameRequired JetStreamError = &jsError{message: \"consumer name is required\"}\n\n\t// ErrConsumerMultipleFilterSubjectsNotSupported is returned when the connected nats-server version does not support setting\n\t// multiple filter subjects with filter_subjects field. If this error is returned when executing AddConsumer(), the consumer with invalid\n\t// configuration was already created in the server.\n\tErrConsumerMultipleFilterSubjectsNotSupported JetStreamError = &jsError{message: \"multiple consumer filter subjects not supported by nats-server\"}\n\n\t// ErrConsumerConfigRequired is returned when empty consumer consuguration is supplied to add/update consumer.\n\tErrConsumerConfigRequired JetStreamError = &jsError{message: \"consumer configuration is required\"}\n\n\t// ErrPullSubscribeToPushConsumer is returned when attempting to use PullSubscribe on push consumer.\n\tErrPullSubscribeToPushConsumer JetStreamError = &jsError{message: \"cannot pull subscribe to push based consumer\"}\n\n\t// ErrPullSubscribeRequired is returned when attempting to use subscribe methods not suitable for pull consumers for pull consumers.\n\tErrPullSubscribeRequired JetStreamError = &jsError{message: \"must use pull subscribe to bind to pull based consumer\"}\n\n\t// ErrMsgAlreadyAckd is returned when attempting to acknowledge message more than once.\n\tErrMsgAlreadyAckd JetStreamError = &jsError{message: \"message was already acknowledged\"}\n\n\t// ErrNoStreamResponse is returned when there is no response from stream (e.g. no responders error).\n\tErrNoStreamResponse JetStreamError = &jsError{message: \"no response from stream\"}\n\n\t// ErrNotJSMessage is returned when attempting to get metadata from non JetStream message .\n\tErrNotJSMessage JetStreamError = &jsError{message: \"not a jetstream message\"}\n\n\t// ErrInvalidStreamName is returned when the provided stream name is invalid (contains '.' or ' ').\n\tErrInvalidStreamName JetStreamError = &jsError{message: \"invalid stream name\"}\n\n\t// ErrInvalidConsumerName is returned when the provided consumer name is invalid (contains '.' or ' ').\n\tErrInvalidConsumerName JetStreamError = &jsError{message: \"invalid consumer name\"}\n\n\t// ErrInvalidFilterSubject is returned when the provided filter subject is invalid.\n\tErrInvalidFilterSubject JetStreamError = &jsError{message: \"invalid filter subject\"}\n\n\t// ErrNoMatchingStream is returned when stream lookup by subject is unsuccessful.\n\tErrNoMatchingStream JetStreamError = &jsError{message: \"no stream matches subject\"}\n\n\t// ErrSubjectMismatch is returned when the provided subject does not match consumer's filter subject.\n\tErrSubjectMismatch JetStreamError = &jsError{message: \"subject does not match consumer\"}\n\n\t// ErrContextAndTimeout is returned when attempting to use both context and timeout.\n\tErrContextAndTimeout JetStreamError = &jsError{message: \"context and timeout can not both be set\"}\n\n\t// ErrCantAckIfConsumerAckNone is returned when attempting to ack a message for consumer with AckNone policy set.\n\tErrCantAckIfConsumerAckNone JetStreamError = &jsError{message: \"cannot acknowledge a message for a consumer with AckNone policy\"}\n\n\t// ErrConsumerDeleted is returned when attempting to send pull request to a consumer which does not exist\n\tErrConsumerDeleted JetStreamError = &jsError{message: \"consumer deleted\"}\n\n\t// ErrConsumerLeadershipChanged is returned when pending requests are no longer valid after leadership has changed\n\tErrConsumerLeadershipChanged JetStreamError = &jsError{message: \"Leadership Changed\"}\n\n\t// ErrNoHeartbeat is returned when no heartbeat is received from server when sending requests with pull consumer.\n\tErrNoHeartbeat JetStreamError = &jsError{message: \"no heartbeat received\"}\n\n\t// ErrSubscriptionClosed is returned when attempting to send pull request to a closed subscription\n\tErrSubscriptionClosed JetStreamError = &jsError{message: \"subscription closed\"}\n\n\t// ErrJetStreamPublisherClosed is returned for each unfinished ack future when JetStream.Cleanup is called.\n\tErrJetStreamPublisherClosed JetStreamError = &jsError{message: \"jetstream context closed\"}\n\n\t// Deprecated: ErrInvalidDurableName is no longer returned and will be removed in future releases.\n\t// Use ErrInvalidConsumerName instead.\n\tErrInvalidDurableName = errors.New(\"nats: invalid durable name\")\n)\n\n// Error code represents JetStream error codes returned by the API\ntype ErrorCode uint16\n\nconst (\n\tJSErrCodeJetStreamNotEnabledForAccount ErrorCode = 10039\n\tJSErrCodeJetStreamNotEnabled           ErrorCode = 10076\n\tJSErrCodeInsufficientResourcesErr      ErrorCode = 10023\n\n\tJSErrCodeStreamNotFound  ErrorCode = 10059\n\tJSErrCodeStreamNameInUse ErrorCode = 10058\n\n\tJSErrCodeConsumerNotFound          ErrorCode = 10014\n\tJSErrCodeConsumerNameExists        ErrorCode = 10013\n\tJSErrCodeConsumerAlreadyExists     ErrorCode = 10105\n\tJSErrCodeDuplicateFilterSubjects   ErrorCode = 10136\n\tJSErrCodeOverlappingFilterSubjects ErrorCode = 10138\n\tJSErrCodeConsumerEmptyFilter       ErrorCode = 10139\n\n\tJSErrCodeMessageNotFound ErrorCode = 10037\n\n\tJSErrCodeBadRequest   ErrorCode = 10003\n\tJSStreamInvalidConfig ErrorCode = 10052\n\n\tJSErrCodeStreamWrongLastSequence ErrorCode = 10071\n)\n\n// APIError is included in all API responses if there was an error.\ntype APIError struct {\n\tCode        int       `json:\"code\"`\n\tErrorCode   ErrorCode `json:\"err_code\"`\n\tDescription string    `json:\"description,omitempty\"`\n}\n\n// Error prints the JetStream API error code and description\nfunc (e *APIError) Error() string {\n\treturn fmt.Sprintf(\"nats: %s\", e.Description)\n}\n\n// APIError implements the JetStreamError interface.\nfunc (e *APIError) APIError() *APIError {\n\treturn e\n}\n\n// Is matches against an APIError.\nfunc (e *APIError) Is(err error) bool {\n\tif e == nil {\n\t\treturn false\n\t}\n\t// Extract internal APIError to match against.\n\tvar aerr *APIError\n\tok := errors.As(err, &aerr)\n\tif !ok {\n\t\treturn ok\n\t}\n\treturn e.ErrorCode == aerr.ErrorCode\n}\n\n// JetStreamError is an error result that happens when using JetStream.\n// In case of client-side error, `APIError()` returns nil\ntype JetStreamError interface {\n\tAPIError() *APIError\n\terror\n}\n\ntype jsError struct {\n\tapiErr  *APIError\n\tmessage string\n}\n\nfunc (err *jsError) APIError() *APIError {\n\treturn err.apiErr\n}\n\nfunc (err *jsError) Error() string {\n\tif err.apiErr != nil && err.apiErr.Description != \"\" {\n\t\treturn err.apiErr.Error()\n\t}\n\treturn fmt.Sprintf(\"nats: %s\", err.message)\n}\n\nfunc (err *jsError) Unwrap() error {\n\t// Allow matching to embedded APIError in case there is one.\n\tif err.apiErr == nil {\n\t\treturn nil\n\t}\n\treturn err.apiErr\n}\n"
        },
        {
          "name": "jsm.go",
          "type": "blob",
          "size": 49.1318359375,
          "content": "// Copyright 2021-2023 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nats\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n)\n\n// JetStreamManager manages JetStream Streams and Consumers.\ntype JetStreamManager interface {\n\t// AddStream creates a stream.\n\tAddStream(cfg *StreamConfig, opts ...JSOpt) (*StreamInfo, error)\n\n\t// UpdateStream updates a stream.\n\tUpdateStream(cfg *StreamConfig, opts ...JSOpt) (*StreamInfo, error)\n\n\t// DeleteStream deletes a stream.\n\tDeleteStream(name string, opts ...JSOpt) error\n\n\t// StreamInfo retrieves information from a stream.\n\tStreamInfo(stream string, opts ...JSOpt) (*StreamInfo, error)\n\n\t// PurgeStream purges a stream messages.\n\tPurgeStream(name string, opts ...JSOpt) error\n\n\t// StreamsInfo can be used to retrieve a list of StreamInfo objects.\n\t// Deprecated: Use Streams() instead.\n\tStreamsInfo(opts ...JSOpt) <-chan *StreamInfo\n\n\t// Streams can be used to retrieve a list of StreamInfo objects.\n\tStreams(opts ...JSOpt) <-chan *StreamInfo\n\n\t// StreamNames is used to retrieve a list of Stream names.\n\tStreamNames(opts ...JSOpt) <-chan string\n\n\t// GetMsg retrieves a raw stream message stored in JetStream by sequence number.\n\t// Use options nats.DirectGet() or nats.DirectGetNext() to trigger retrieval\n\t// directly from a distributed group of servers (leader and replicas).\n\t// The stream must have been created/updated with the AllowDirect boolean.\n\tGetMsg(name string, seq uint64, opts ...JSOpt) (*RawStreamMsg, error)\n\n\t// GetLastMsg retrieves the last raw stream message stored in JetStream by subject.\n\t// Use option nats.DirectGet() to trigger retrieval\n\t// directly from a distributed group of servers (leader and replicas).\n\t// The stream must have been created/updated with the AllowDirect boolean.\n\tGetLastMsg(name, subject string, opts ...JSOpt) (*RawStreamMsg, error)\n\n\t// DeleteMsg deletes a message from a stream. The message is marked as erased, but its value is not overwritten.\n\tDeleteMsg(name string, seq uint64, opts ...JSOpt) error\n\n\t// SecureDeleteMsg deletes a message from a stream. The deleted message is overwritten with random data\n\t// As a result, this operation is slower than DeleteMsg()\n\tSecureDeleteMsg(name string, seq uint64, opts ...JSOpt) error\n\n\t// AddConsumer adds a consumer to a stream.\n\t// If the consumer already exists, and the configuration is the same, it\n\t// will return the existing consumer.\n\t// If the consumer already exists, and the configuration is different, it\n\t// will return ErrConsumerNameAlreadyInUse.\n\tAddConsumer(stream string, cfg *ConsumerConfig, opts ...JSOpt) (*ConsumerInfo, error)\n\n\t// UpdateConsumer updates an existing consumer.\n\tUpdateConsumer(stream string, cfg *ConsumerConfig, opts ...JSOpt) (*ConsumerInfo, error)\n\n\t// DeleteConsumer deletes a consumer.\n\tDeleteConsumer(stream, consumer string, opts ...JSOpt) error\n\n\t// ConsumerInfo retrieves information of a consumer from a stream.\n\tConsumerInfo(stream, name string, opts ...JSOpt) (*ConsumerInfo, error)\n\n\t// ConsumersInfo is used to retrieve a list of ConsumerInfo objects.\n\t// Deprecated: Use Consumers() instead.\n\tConsumersInfo(stream string, opts ...JSOpt) <-chan *ConsumerInfo\n\n\t// Consumers is used to retrieve a list of ConsumerInfo objects.\n\tConsumers(stream string, opts ...JSOpt) <-chan *ConsumerInfo\n\n\t// ConsumerNames is used to retrieve a list of Consumer names.\n\tConsumerNames(stream string, opts ...JSOpt) <-chan string\n\n\t// AccountInfo retrieves info about the JetStream usage from an account.\n\tAccountInfo(opts ...JSOpt) (*AccountInfo, error)\n\n\t// StreamNameBySubject returns a stream matching given subject.\n\tStreamNameBySubject(string, ...JSOpt) (string, error)\n}\n\n// StreamConfig will determine the properties for a stream.\n// There are sensible defaults for most. If no subjects are\n// given the name will be used as the only subject.\ntype StreamConfig struct {\n\t// Name is the name of the stream. It is required and must be unique\n\t// across the JetStream account.\n\t//\n\t// Name Names cannot contain whitespace, ., *, >, path separators\n\t// (forward or backwards slash), and non-printable characters.\n\tName string `json:\"name\"`\n\n\t// Description is an optional description of the stream.\n\tDescription string `json:\"description,omitempty\"`\n\n\t// Subjects is a list of subjects that the stream is listening on.\n\t// Wildcards are supported. Subjects cannot be set if the stream is\n\t// created as a mirror.\n\tSubjects []string `json:\"subjects,omitempty\"`\n\n\t// Retention defines the message retention policy for the stream.\n\t// Defaults to LimitsPolicy.\n\tRetention RetentionPolicy `json:\"retention\"`\n\n\t// MaxConsumers specifies the maximum number of consumers allowed for\n\t// the stream.\n\tMaxConsumers int `json:\"max_consumers\"`\n\n\t// MaxMsgs is the maximum number of messages the stream will store.\n\t// After reaching the limit, stream adheres to the discard policy.\n\t// If not set, server default is -1 (unlimited).\n\tMaxMsgs int64 `json:\"max_msgs\"`\n\n\t// MaxBytes is the maximum total size of messages the stream will store.\n\t// After reaching the limit, stream adheres to the discard policy.\n\t// If not set, server default is -1 (unlimited).\n\tMaxBytes int64 `json:\"max_bytes\"`\n\n\t// Discard defines the policy for handling messages when the stream\n\t// reaches its limits in terms of number of messages or total bytes.\n\tDiscard DiscardPolicy `json:\"discard\"`\n\n\t// DiscardNewPerSubject is a flag to enable discarding new messages per\n\t// subject when limits are reached. Requires DiscardPolicy to be\n\t// DiscardNew and the MaxMsgsPerSubject to be set.\n\tDiscardNewPerSubject bool `json:\"discard_new_per_subject,omitempty\"`\n\n\t// MaxAge is the maximum age of messages that the stream will retain.\n\tMaxAge time.Duration `json:\"max_age\"`\n\n\t// MaxMsgsPerSubject is the maximum number of messages per subject that\n\t// the stream will retain.\n\tMaxMsgsPerSubject int64 `json:\"max_msgs_per_subject\"`\n\n\t// MaxMsgSize is the maximum size of any single message in the stream.\n\tMaxMsgSize int32 `json:\"max_msg_size,omitempty\"`\n\n\t// Storage specifies the type of storage backend used for the stream\n\t// (file or memory).\n\tStorage StorageType `json:\"storage\"`\n\n\t// Replicas is the number of stream replicas in clustered JetStream.\n\t// Defaults to 1, maximum is 5.\n\tReplicas int `json:\"num_replicas\"`\n\n\t// NoAck is a flag to disable acknowledging messages received by this\n\t// stream.\n\t//\n\t// If set to true, publish methods from the JetStream client will not\n\t// work as expected, since they rely on acknowledgements. Core NATS\n\t// publish methods should be used instead. Note that this will make\n\t// message delivery less reliable.\n\tNoAck bool `json:\"no_ack,omitempty\"`\n\n\t// Duplicates is the window within which to track duplicate messages.\n\t// If not set, server default is 2 minutes.\n\tDuplicates time.Duration `json:\"duplicate_window,omitempty\"`\n\n\t// Placement is used to declare where the stream should be placed via\n\t// tags and/or an explicit cluster name.\n\tPlacement *Placement `json:\"placement,omitempty\"`\n\n\t// Mirror defines the configuration for mirroring another stream.\n\tMirror *StreamSource `json:\"mirror,omitempty\"`\n\n\t// Sources is a list of other streams this stream sources messages from.\n\tSources []*StreamSource `json:\"sources,omitempty\"`\n\n\t// Sealed streams do not allow messages to be published or deleted via limits or API,\n\t// sealed streams can not be unsealed via configuration update. Can only\n\t// be set on already created streams via the Update API.\n\tSealed bool `json:\"sealed,omitempty\"`\n\n\t// DenyDelete restricts the ability to delete messages from a stream via\n\t// the API. Defaults to false.\n\tDenyDelete bool `json:\"deny_delete,omitempty\"`\n\n\t// DenyPurge restricts the ability to purge messages from a stream via\n\t// the API. Defaults to false.\n\tDenyPurge bool `json:\"deny_purge,omitempty\"`\n\n\t// AllowRollup allows the use of the Nats-Rollup header to replace all\n\t// contents of a stream, or subject in a stream, with a single new\n\t// message.\n\tAllowRollup bool `json:\"allow_rollup_hdrs,omitempty\"`\n\n\t// Compression specifies the message storage compression algorithm.\n\t// Defaults to NoCompression.\n\tCompression StoreCompression `json:\"compression\"`\n\n\t// FirstSeq is the initial sequence number of the first message in the\n\t// stream.\n\tFirstSeq uint64 `json:\"first_seq,omitempty\"`\n\n\t// SubjectTransform allows applying a transformation to matching\n\t// messages' subjects.\n\tSubjectTransform *SubjectTransformConfig `json:\"subject_transform,omitempty\"`\n\n\t// RePublish allows immediate republishing a message to the configured\n\t// subject after it's stored.\n\tRePublish *RePublish `json:\"republish,omitempty\"`\n\n\t// AllowDirect enables direct access to individual messages using direct\n\t// get API. Defaults to false.\n\tAllowDirect bool `json:\"allow_direct\"`\n\n\t// MirrorDirect enables direct access to individual messages from the\n\t// origin stream using direct get API. Defaults to false.\n\tMirrorDirect bool `json:\"mirror_direct\"`\n\n\t// ConsumerLimits defines limits of certain values that consumers can\n\t// set, defaults for those who don't set these settings\n\tConsumerLimits StreamConsumerLimits `json:\"consumer_limits,omitempty\"`\n\n\t// Metadata is a set of application-defined key-value pairs for\n\t// associating metadata on the stream. This feature requires nats-server\n\t// v2.10.0 or later.\n\tMetadata map[string]string `json:\"metadata,omitempty\"`\n\n\t// Template identifies the template that manages the Stream. Deprecated:\n\t// This feature is no longer supported.\n\tTemplate string `json:\"template_owner,omitempty\"`\n}\n\n// SubjectTransformConfig is for applying a subject transform (to matching messages) before doing anything else when a new message is received.\ntype SubjectTransformConfig struct {\n\tSource      string `json:\"src,omitempty\"`\n\tDestination string `json:\"dest\"`\n}\n\n// RePublish is for republishing messages once committed to a stream. The original\n// subject cis remapped from the subject pattern to the destination pattern.\ntype RePublish struct {\n\tSource      string `json:\"src,omitempty\"`\n\tDestination string `json:\"dest\"`\n\tHeadersOnly bool   `json:\"headers_only,omitempty\"`\n}\n\n// Placement is used to guide placement of streams in clustered JetStream.\ntype Placement struct {\n\tCluster string   `json:\"cluster\"`\n\tTags    []string `json:\"tags,omitempty\"`\n}\n\n// StreamSource dictates how streams can source from other streams.\ntype StreamSource struct {\n\tName              string                   `json:\"name\"`\n\tOptStartSeq       uint64                   `json:\"opt_start_seq,omitempty\"`\n\tOptStartTime      *time.Time               `json:\"opt_start_time,omitempty\"`\n\tFilterSubject     string                   `json:\"filter_subject,omitempty\"`\n\tSubjectTransforms []SubjectTransformConfig `json:\"subject_transforms,omitempty\"`\n\tExternal          *ExternalStream          `json:\"external,omitempty\"`\n\tDomain            string                   `json:\"-\"`\n}\n\n// ExternalStream allows you to qualify access to a stream source in another\n// account.\ntype ExternalStream struct {\n\tAPIPrefix     string `json:\"api\"`\n\tDeliverPrefix string `json:\"deliver,omitempty\"`\n}\n\n// StreamConsumerLimits are the limits for a consumer on a stream.\n// These can be overridden on a per consumer basis.\ntype StreamConsumerLimits struct {\n\tInactiveThreshold time.Duration `json:\"inactive_threshold,omitempty\"`\n\tMaxAckPending     int           `json:\"max_ack_pending,omitempty\"`\n}\n\n// Helper for copying when we do not want to change user's version.\nfunc (ss *StreamSource) copy() *StreamSource {\n\tnss := *ss\n\t// Check pointers\n\tif ss.OptStartTime != nil {\n\t\tt := *ss.OptStartTime\n\t\tnss.OptStartTime = &t\n\t}\n\tif ss.External != nil {\n\t\text := *ss.External\n\t\tnss.External = &ext\n\t}\n\treturn &nss\n}\n\n// If we have a Domain, convert to the appropriate ext.APIPrefix.\n// This will change the stream source, so should be a copy passed in.\nfunc (ss *StreamSource) convertDomain() error {\n\tif ss.Domain == _EMPTY_ {\n\t\treturn nil\n\t}\n\tif ss.External != nil {\n\t\t// These should be mutually exclusive.\n\t\t// TODO(dlc) - Make generic?\n\t\treturn errors.New(\"nats: domain and external are both set\")\n\t}\n\tss.External = &ExternalStream{APIPrefix: fmt.Sprintf(jsExtDomainT, ss.Domain)}\n\treturn nil\n}\n\n// apiResponse is a standard response from the JetStream JSON API\ntype apiResponse struct {\n\tType  string    `json:\"type\"`\n\tError *APIError `json:\"error,omitempty\"`\n}\n\n// apiPaged includes variables used to create paged responses from the JSON API\ntype apiPaged struct {\n\tTotal  int `json:\"total\"`\n\tOffset int `json:\"offset\"`\n\tLimit  int `json:\"limit\"`\n}\n\n// apiPagedRequest includes parameters allowing specific pages to be requested\n// from APIs responding with apiPaged.\ntype apiPagedRequest struct {\n\tOffset int `json:\"offset,omitempty\"`\n}\n\n// AccountInfo contains info about the JetStream usage from the current account.\ntype AccountInfo struct {\n\tTier\n\tDomain string          `json:\"domain\"`\n\tAPI    APIStats        `json:\"api\"`\n\tTiers  map[string]Tier `json:\"tiers\"`\n}\n\ntype Tier struct {\n\tMemory         uint64        `json:\"memory\"`\n\tStore          uint64        `json:\"storage\"`\n\tReservedMemory uint64        `json:\"reserved_memory\"`\n\tReservedStore  uint64        `json:\"reserved_storage\"`\n\tStreams        int           `json:\"streams\"`\n\tConsumers      int           `json:\"consumers\"`\n\tLimits         AccountLimits `json:\"limits\"`\n}\n\n// APIStats reports on API calls to JetStream for this account.\ntype APIStats struct {\n\tTotal  uint64 `json:\"total\"`\n\tErrors uint64 `json:\"errors\"`\n}\n\n// AccountLimits includes the JetStream limits of the current account.\ntype AccountLimits struct {\n\tMaxMemory            int64 `json:\"max_memory\"`\n\tMaxStore             int64 `json:\"max_storage\"`\n\tMaxStreams           int   `json:\"max_streams\"`\n\tMaxConsumers         int   `json:\"max_consumers\"`\n\tMaxAckPending        int   `json:\"max_ack_pending\"`\n\tMemoryMaxStreamBytes int64 `json:\"memory_max_stream_bytes\"`\n\tStoreMaxStreamBytes  int64 `json:\"storage_max_stream_bytes\"`\n\tMaxBytesRequired     bool  `json:\"max_bytes_required\"`\n}\n\ntype accountInfoResponse struct {\n\tapiResponse\n\tAccountInfo\n}\n\n// AccountInfo fetches account information from the server, containing details\n// about the account associated with this JetStream connection. If account is\n// not enabled for JetStream, ErrJetStreamNotEnabledForAccount is returned.\n//\n// If the server does not have JetStream enabled, ErrJetStreamNotEnabled is\n// returned (for a single server setup). For clustered topologies, AccountInfo\n// will time out.\nfunc (js *js) AccountInfo(opts ...JSOpt) (*AccountInfo, error) {\n\to, cancel, err := getJSContextOpts(js.opts, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif cancel != nil {\n\t\tdefer cancel()\n\t}\n\n\tresp, err := js.apiRequestWithContext(o.ctx, js.apiSubj(apiAccountInfo), nil)\n\tif err != nil {\n\t\t// todo maybe nats server should never have no responder on this subject and always respond if they know there is no js to be had\n\t\tif errors.Is(err, ErrNoResponders) {\n\t\t\terr = ErrJetStreamNotEnabled\n\t\t}\n\t\treturn nil, err\n\t}\n\tvar info accountInfoResponse\n\tif err := json.Unmarshal(resp.Data, &info); err != nil {\n\t\treturn nil, err\n\t}\n\tif info.Error != nil {\n\t\t// Internally checks based on error code instead of description match.\n\t\tif errors.Is(info.Error, ErrJetStreamNotEnabledForAccount) {\n\t\t\treturn nil, ErrJetStreamNotEnabledForAccount\n\t\t}\n\t\treturn nil, info.Error\n\t}\n\n\treturn &info.AccountInfo, nil\n}\n\ntype createConsumerRequest struct {\n\tStream string          `json:\"stream_name\"`\n\tConfig *ConsumerConfig `json:\"config\"`\n}\n\ntype consumerResponse struct {\n\tapiResponse\n\t*ConsumerInfo\n}\n\n// AddConsumer adds a consumer to a stream.\n// If the consumer already exists, and the configuration is the same, it\n// will return the existing consumer.\n// If the consumer already exists, and the configuration is different, it\n// will return ErrConsumerNameAlreadyInUse.\nfunc (js *js) AddConsumer(stream string, cfg *ConsumerConfig, opts ...JSOpt) (*ConsumerInfo, error) {\n\tif cfg == nil {\n\t\tcfg = &ConsumerConfig{}\n\t}\n\tconsumerName := cfg.Name\n\tif consumerName == _EMPTY_ {\n\t\tconsumerName = cfg.Durable\n\t}\n\tif consumerName != _EMPTY_ {\n\t\tconsInfo, err := js.ConsumerInfo(stream, consumerName, opts...)\n\t\tif err != nil && !errors.Is(err, ErrConsumerNotFound) && !errors.Is(err, ErrStreamNotFound) {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif consInfo != nil {\n\t\t\tsameConfig := checkConfig(&consInfo.Config, cfg)\n\t\t\tif sameConfig != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"%w: creating consumer %q on stream %q\", ErrConsumerNameAlreadyInUse, consumerName, stream)\n\t\t\t} else {\n\t\t\t\treturn consInfo, nil\n\t\t\t}\n\t\t}\n\t}\n\n\treturn js.upsertConsumer(stream, consumerName, cfg, opts...)\n}\n\nfunc (js *js) UpdateConsumer(stream string, cfg *ConsumerConfig, opts ...JSOpt) (*ConsumerInfo, error) {\n\tif cfg == nil {\n\t\treturn nil, ErrConsumerConfigRequired\n\t}\n\tconsumerName := cfg.Name\n\tif consumerName == _EMPTY_ {\n\t\tconsumerName = cfg.Durable\n\t}\n\tif consumerName == _EMPTY_ {\n\t\treturn nil, ErrConsumerNameRequired\n\t}\n\treturn js.upsertConsumer(stream, consumerName, cfg, opts...)\n}\n\nfunc (js *js) upsertConsumer(stream, consumerName string, cfg *ConsumerConfig, opts ...JSOpt) (*ConsumerInfo, error) {\n\tif err := checkStreamName(stream); err != nil {\n\t\treturn nil, err\n\t}\n\to, cancel, err := getJSContextOpts(js.opts, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif cancel != nil {\n\t\tdefer cancel()\n\t}\n\n\treq, err := json.Marshal(&createConsumerRequest{Stream: stream, Config: cfg})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar ccSubj string\n\tif consumerName == _EMPTY_ {\n\t\t// if consumer name is empty (neither Durable nor Name is set), use the legacy ephemeral endpoint\n\t\tccSubj = fmt.Sprintf(apiLegacyConsumerCreateT, stream)\n\t} else if err := checkConsumerName(consumerName); err != nil {\n\t\treturn nil, err\n\t} else if js.nc.serverMinVersion(2, 9, 0) {\n\t\tif cfg.Durable != \"\" && js.opts.featureFlags.useDurableConsumerCreate {\n\t\t\t// if user set the useDurableConsumerCreate flag, use the legacy DURABLE.CREATE endpoint\n\t\t\tccSubj = fmt.Sprintf(apiDurableCreateT, stream, consumerName)\n\t\t} else if cfg.FilterSubject == _EMPTY_ || cfg.FilterSubject == \">\" {\n\t\t\t// if filter subject is empty or \">\", use the endpoint without filter subject\n\t\t\tccSubj = fmt.Sprintf(apiConsumerCreateT, stream, consumerName)\n\t\t} else {\n\t\t\t// safeguard against passing invalid filter subject in request subject\n\t\t\tif cfg.FilterSubject[0] == '.' || cfg.FilterSubject[len(cfg.FilterSubject)-1] == '.' {\n\t\t\t\treturn nil, fmt.Errorf(\"%w: %q\", ErrInvalidFilterSubject, cfg.FilterSubject)\n\t\t\t}\n\t\t\t// if filter subject is not empty, use the endpoint with filter subject\n\t\t\tccSubj = fmt.Sprintf(apiConsumerCreateWithFilterSubjectT, stream, consumerName, cfg.FilterSubject)\n\t\t}\n\t} else {\n\t\tif cfg.Durable != \"\" {\n\t\t\t// if Durable is set, use the DURABLE.CREATE endpoint\n\t\t\tccSubj = fmt.Sprintf(apiDurableCreateT, stream, consumerName)\n\t\t} else {\n\t\t\t// if Durable is not set, use the legacy ephemeral endpoint\n\t\t\tccSubj = fmt.Sprintf(apiLegacyConsumerCreateT, stream)\n\t\t}\n\t}\n\n\tresp, err := js.apiRequestWithContext(o.ctx, js.apiSubj(ccSubj), req)\n\tif err != nil {\n\t\tif errors.Is(err, ErrNoResponders) {\n\t\t\terr = ErrJetStreamNotEnabled\n\t\t}\n\t\treturn nil, err\n\t}\n\tvar info consumerResponse\n\terr = json.Unmarshal(resp.Data, &info)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif info.Error != nil {\n\t\tif errors.Is(info.Error, ErrStreamNotFound) {\n\t\t\treturn nil, ErrStreamNotFound\n\t\t}\n\t\tif errors.Is(info.Error, ErrConsumerNotFound) {\n\t\t\treturn nil, ErrConsumerNotFound\n\t\t}\n\t\treturn nil, info.Error\n\t}\n\n\t// check whether multiple filter subjects (if used) are reflected in the returned ConsumerInfo\n\tif len(cfg.FilterSubjects) != 0 && len(info.Config.FilterSubjects) == 0 {\n\t\treturn nil, ErrConsumerMultipleFilterSubjectsNotSupported\n\t}\n\treturn info.ConsumerInfo, nil\n}\n\n// consumerDeleteResponse is the response for a Consumer delete request.\ntype consumerDeleteResponse struct {\n\tapiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nfunc checkStreamName(stream string) error {\n\tif stream == _EMPTY_ {\n\t\treturn ErrStreamNameRequired\n\t}\n\tif strings.ContainsAny(stream, \". \") {\n\t\treturn ErrInvalidStreamName\n\t}\n\treturn nil\n}\n\n// Check that the consumer name is not empty and is valid (does not contain \".\" and \" \").\n// Additional consumer name validation is done in nats-server.\n// Returns ErrConsumerNameRequired if consumer name is empty, ErrInvalidConsumerName is invalid, otherwise nil\nfunc checkConsumerName(consumer string) error {\n\tif consumer == _EMPTY_ {\n\t\treturn ErrConsumerNameRequired\n\t}\n\tif strings.ContainsAny(consumer, \". \") {\n\t\treturn ErrInvalidConsumerName\n\t}\n\treturn nil\n}\n\n// DeleteConsumer deletes a Consumer.\nfunc (js *js) DeleteConsumer(stream, consumer string, opts ...JSOpt) error {\n\tif err := checkStreamName(stream); err != nil {\n\t\treturn err\n\t}\n\tif err := checkConsumerName(consumer); err != nil {\n\t\treturn err\n\t}\n\to, cancel, err := getJSContextOpts(js.opts, opts...)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif cancel != nil {\n\t\tdefer cancel()\n\t}\n\n\tdcSubj := js.apiSubj(fmt.Sprintf(apiConsumerDeleteT, stream, consumer))\n\tr, err := js.apiRequestWithContext(o.ctx, dcSubj, nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\tvar resp consumerDeleteResponse\n\tif err := json.Unmarshal(r.Data, &resp); err != nil {\n\t\treturn err\n\t}\n\n\tif resp.Error != nil {\n\t\tif errors.Is(resp.Error, ErrConsumerNotFound) {\n\t\t\treturn ErrConsumerNotFound\n\t\t}\n\t\treturn resp.Error\n\t}\n\treturn nil\n}\n\n// ConsumerInfo returns information about a Consumer.\nfunc (js *js) ConsumerInfo(stream, consumer string, opts ...JSOpt) (*ConsumerInfo, error) {\n\tif err := checkStreamName(stream); err != nil {\n\t\treturn nil, err\n\t}\n\tif err := checkConsumerName(consumer); err != nil {\n\t\treturn nil, err\n\t}\n\to, cancel, err := getJSContextOpts(js.opts, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif cancel != nil {\n\t\tdefer cancel()\n\t}\n\treturn js.getConsumerInfoContext(o.ctx, stream, consumer)\n}\n\n// consumerLister fetches pages of ConsumerInfo objects. This object is not\n// safe to use for multiple threads.\ntype consumerLister struct {\n\tstream string\n\tjs     *js\n\n\terr      error\n\toffset   int\n\tpage     []*ConsumerInfo\n\tpageInfo *apiPaged\n}\n\n// consumersRequest is the type used for Consumers requests.\ntype consumersRequest struct {\n\tapiPagedRequest\n}\n\n// consumerListResponse is the response for a Consumers List request.\ntype consumerListResponse struct {\n\tapiResponse\n\tapiPaged\n\tConsumers []*ConsumerInfo `json:\"consumers\"`\n}\n\n// Next fetches the next ConsumerInfo page.\nfunc (c *consumerLister) Next() bool {\n\tif c.err != nil {\n\t\treturn false\n\t}\n\tif err := checkStreamName(c.stream); err != nil {\n\t\tc.err = err\n\t\treturn false\n\t}\n\tif c.pageInfo != nil && c.offset >= c.pageInfo.Total {\n\t\treturn false\n\t}\n\n\treq, err := json.Marshal(consumersRequest{\n\t\tapiPagedRequest: apiPagedRequest{Offset: c.offset},\n\t})\n\tif err != nil {\n\t\tc.err = err\n\t\treturn false\n\t}\n\n\tvar cancel context.CancelFunc\n\tctx := c.js.opts.ctx\n\tif ctx == nil {\n\t\tctx, cancel = context.WithTimeout(context.Background(), c.js.opts.wait)\n\t\tdefer cancel()\n\t}\n\n\tclSubj := c.js.apiSubj(fmt.Sprintf(apiConsumerListT, c.stream))\n\tr, err := c.js.apiRequestWithContext(ctx, clSubj, req)\n\tif err != nil {\n\t\tc.err = err\n\t\treturn false\n\t}\n\tvar resp consumerListResponse\n\tif err := json.Unmarshal(r.Data, &resp); err != nil {\n\t\tc.err = err\n\t\treturn false\n\t}\n\tif resp.Error != nil {\n\t\tc.err = resp.Error\n\t\treturn false\n\t}\n\n\tc.pageInfo = &resp.apiPaged\n\tc.page = resp.Consumers\n\tc.offset += len(c.page)\n\treturn true\n}\n\n// Page returns the current ConsumerInfo page.\nfunc (c *consumerLister) Page() []*ConsumerInfo {\n\treturn c.page\n}\n\n// Err returns any errors found while fetching pages.\nfunc (c *consumerLister) Err() error {\n\treturn c.err\n}\n\n// Consumers is used to retrieve a list of ConsumerInfo objects.\nfunc (jsc *js) Consumers(stream string, opts ...JSOpt) <-chan *ConsumerInfo {\n\to, cancel, err := getJSContextOpts(jsc.opts, opts...)\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\tch := make(chan *ConsumerInfo)\n\tl := &consumerLister{js: &js{nc: jsc.nc, opts: o}, stream: stream}\n\tgo func() {\n\t\tif cancel != nil {\n\t\t\tdefer cancel()\n\t\t}\n\t\tdefer close(ch)\n\t\tfor l.Next() {\n\t\t\tfor _, info := range l.Page() {\n\t\t\t\tselect {\n\t\t\t\tcase ch <- info:\n\t\t\t\tcase <-o.ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn ch\n}\n\n// ConsumersInfo is used to retrieve a list of ConsumerInfo objects.\n// Deprecated: Use Consumers() instead.\nfunc (jsc *js) ConsumersInfo(stream string, opts ...JSOpt) <-chan *ConsumerInfo {\n\treturn jsc.Consumers(stream, opts...)\n}\n\ntype consumerNamesLister struct {\n\tstream string\n\tjs     *js\n\n\terr      error\n\toffset   int\n\tpage     []string\n\tpageInfo *apiPaged\n}\n\n// consumerNamesListResponse is the response for a Consumers Names List request.\ntype consumerNamesListResponse struct {\n\tapiResponse\n\tapiPaged\n\tConsumers []string `json:\"consumers\"`\n}\n\n// Next fetches the next consumer names page.\nfunc (c *consumerNamesLister) Next() bool {\n\tif c.err != nil {\n\t\treturn false\n\t}\n\tif err := checkStreamName(c.stream); err != nil {\n\t\tc.err = err\n\t\treturn false\n\t}\n\tif c.pageInfo != nil && c.offset >= c.pageInfo.Total {\n\t\treturn false\n\t}\n\n\tvar cancel context.CancelFunc\n\tctx := c.js.opts.ctx\n\tif ctx == nil {\n\t\tctx, cancel = context.WithTimeout(context.Background(), c.js.opts.wait)\n\t\tdefer cancel()\n\t}\n\n\treq, err := json.Marshal(consumersRequest{\n\t\tapiPagedRequest: apiPagedRequest{Offset: c.offset},\n\t})\n\tif err != nil {\n\t\tc.err = err\n\t\treturn false\n\t}\n\tclSubj := c.js.apiSubj(fmt.Sprintf(apiConsumerNamesT, c.stream))\n\tr, err := c.js.apiRequestWithContext(ctx, clSubj, req)\n\tif err != nil {\n\t\tc.err = err\n\t\treturn false\n\t}\n\tvar resp consumerNamesListResponse\n\tif err := json.Unmarshal(r.Data, &resp); err != nil {\n\t\tc.err = err\n\t\treturn false\n\t}\n\tif resp.Error != nil {\n\t\tc.err = resp.Error\n\t\treturn false\n\t}\n\n\tc.pageInfo = &resp.apiPaged\n\tc.page = resp.Consumers\n\tc.offset += len(c.page)\n\treturn true\n}\n\n// Page returns the current ConsumerInfo page.\nfunc (c *consumerNamesLister) Page() []string {\n\treturn c.page\n}\n\n// Err returns any errors found while fetching pages.\nfunc (c *consumerNamesLister) Err() error {\n\treturn c.err\n}\n\n// ConsumerNames is used to retrieve a list of Consumer names.\nfunc (jsc *js) ConsumerNames(stream string, opts ...JSOpt) <-chan string {\n\to, cancel, err := getJSContextOpts(jsc.opts, opts...)\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\tch := make(chan string)\n\tl := &consumerNamesLister{stream: stream, js: &js{nc: jsc.nc, opts: o}}\n\tgo func() {\n\t\tif cancel != nil {\n\t\t\tdefer cancel()\n\t\t}\n\t\tdefer close(ch)\n\t\tfor l.Next() {\n\t\t\tfor _, info := range l.Page() {\n\t\t\t\tselect {\n\t\t\t\tcase ch <- info:\n\t\t\t\tcase <-o.ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn ch\n}\n\n// streamCreateResponse stream creation.\ntype streamCreateResponse struct {\n\tapiResponse\n\t*StreamInfo\n}\n\nfunc (js *js) AddStream(cfg *StreamConfig, opts ...JSOpt) (*StreamInfo, error) {\n\tif cfg == nil {\n\t\treturn nil, ErrStreamConfigRequired\n\t}\n\tif err := checkStreamName(cfg.Name); err != nil {\n\t\treturn nil, err\n\t}\n\to, cancel, err := getJSContextOpts(js.opts, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif cancel != nil {\n\t\tdefer cancel()\n\t}\n\n\t// In case we need to change anything, copy so we do not change the caller's version.\n\tncfg := *cfg\n\n\t// If we have a mirror and an external domain, convert to ext.APIPrefix.\n\tif cfg.Mirror != nil && cfg.Mirror.Domain != _EMPTY_ {\n\t\t// Copy so we do not change the caller's version.\n\t\tncfg.Mirror = ncfg.Mirror.copy()\n\t\tif err := ncfg.Mirror.convertDomain(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\t// Check sources for the same.\n\tif len(ncfg.Sources) > 0 {\n\t\tncfg.Sources = append([]*StreamSource(nil), ncfg.Sources...)\n\t\tfor i, ss := range ncfg.Sources {\n\t\t\tif ss.Domain != _EMPTY_ {\n\t\t\t\tncfg.Sources[i] = ss.copy()\n\t\t\t\tif err := ncfg.Sources[i].convertDomain(); err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treq, err := json.Marshal(&ncfg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcsSubj := js.apiSubj(fmt.Sprintf(apiStreamCreateT, cfg.Name))\n\tr, err := js.apiRequestWithContext(o.ctx, csSubj, req)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar resp streamCreateResponse\n\tif err := json.Unmarshal(r.Data, &resp); err != nil {\n\t\treturn nil, err\n\t}\n\tif resp.Error != nil {\n\t\tif errors.Is(resp.Error, ErrStreamNameAlreadyInUse) {\n\t\t\treturn nil, ErrStreamNameAlreadyInUse\n\t\t}\n\t\treturn nil, resp.Error\n\t}\n\n\t// check that input subject transform (if used) is reflected in the returned ConsumerInfo\n\tif cfg.SubjectTransform != nil && resp.StreamInfo.Config.SubjectTransform == nil {\n\t\treturn nil, ErrStreamSubjectTransformNotSupported\n\t}\n\tif len(cfg.Sources) != 0 {\n\t\tif len(cfg.Sources) != len(resp.Config.Sources) {\n\t\t\treturn nil, ErrStreamSourceNotSupported\n\t\t}\n\t\tfor i := range cfg.Sources {\n\t\t\tif len(cfg.Sources[i].SubjectTransforms) != 0 && len(resp.Sources[i].SubjectTransforms) == 0 {\n\t\t\t\treturn nil, ErrStreamSourceMultipleSubjectTransformsNotSupported\n\t\t\t}\n\t\t}\n\t}\n\n\treturn resp.StreamInfo, nil\n}\n\ntype (\n\t// StreamInfoRequest contains additional option to return\n\tStreamInfoRequest struct {\n\t\tapiPagedRequest\n\t\t// DeletedDetails when true includes information about deleted messages\n\t\tDeletedDetails bool `json:\"deleted_details,omitempty\"`\n\t\t// SubjectsFilter when set, returns information on the matched subjects\n\t\tSubjectsFilter string `json:\"subjects_filter,omitempty\"`\n\t}\n\tstreamInfoResponse = struct {\n\t\tapiResponse\n\t\tapiPaged\n\t\t*StreamInfo\n\t}\n)\n\nfunc (js *js) StreamInfo(stream string, opts ...JSOpt) (*StreamInfo, error) {\n\tif err := checkStreamName(stream); err != nil {\n\t\treturn nil, err\n\t}\n\to, cancel, err := getJSContextOpts(js.opts, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif cancel != nil {\n\t\tdefer cancel()\n\t}\n\n\tvar i int\n\tvar subjectMessagesMap map[string]uint64\n\tvar req []byte\n\tvar requestPayload bool\n\n\tvar siOpts StreamInfoRequest\n\tif o.streamInfoOpts != nil {\n\t\trequestPayload = true\n\t\tsiOpts = *o.streamInfoOpts\n\t}\n\n\tfor {\n\t\tif requestPayload {\n\t\t\tsiOpts.Offset = i\n\t\t\tif req, err = json.Marshal(&siOpts); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\n\t\tsiSubj := js.apiSubj(fmt.Sprintf(apiStreamInfoT, stream))\n\n\t\tr, err := js.apiRequestWithContext(o.ctx, siSubj, req)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tvar resp streamInfoResponse\n\t\tif err := json.Unmarshal(r.Data, &resp); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif resp.Error != nil {\n\t\t\tif errors.Is(resp.Error, ErrStreamNotFound) {\n\t\t\t\treturn nil, ErrStreamNotFound\n\t\t\t}\n\t\t\treturn nil, resp.Error\n\t\t}\n\n\t\tvar total int\n\t\t// for backwards compatibility\n\t\tif resp.Total != 0 {\n\t\t\ttotal = resp.Total\n\t\t} else {\n\t\t\ttotal = len(resp.State.Subjects)\n\t\t}\n\n\t\tif requestPayload && len(resp.StreamInfo.State.Subjects) > 0 {\n\t\t\tif subjectMessagesMap == nil {\n\t\t\t\tsubjectMessagesMap = make(map[string]uint64, total)\n\t\t\t}\n\n\t\t\tfor k, j := range resp.State.Subjects {\n\t\t\t\tsubjectMessagesMap[k] = j\n\t\t\t\ti++\n\t\t\t}\n\t\t}\n\n\t\tif i >= total {\n\t\t\tif requestPayload {\n\t\t\t\tresp.StreamInfo.State.Subjects = subjectMessagesMap\n\t\t\t}\n\t\t\treturn resp.StreamInfo, nil\n\t\t}\n\t}\n}\n\n// StreamInfo shows config and current state for this stream.\ntype StreamInfo struct {\n\tConfig     StreamConfig        `json:\"config\"`\n\tCreated    time.Time           `json:\"created\"`\n\tState      StreamState         `json:\"state\"`\n\tCluster    *ClusterInfo        `json:\"cluster,omitempty\"`\n\tMirror     *StreamSourceInfo   `json:\"mirror,omitempty\"`\n\tSources    []*StreamSourceInfo `json:\"sources,omitempty\"`\n\tAlternates []*StreamAlternate  `json:\"alternates,omitempty\"`\n}\n\n// StreamAlternate is an alternate stream represented by a mirror.\ntype StreamAlternate struct {\n\tName    string `json:\"name\"`\n\tDomain  string `json:\"domain,omitempty\"`\n\tCluster string `json:\"cluster\"`\n}\n\n// StreamSourceInfo shows information about an upstream stream source.\ntype StreamSourceInfo struct {\n\tName              string                   `json:\"name\"`\n\tLag               uint64                   `json:\"lag\"`\n\tActive            time.Duration            `json:\"active\"`\n\tExternal          *ExternalStream          `json:\"external\"`\n\tError             *APIError                `json:\"error\"`\n\tFilterSubject     string                   `json:\"filter_subject,omitempty\"`\n\tSubjectTransforms []SubjectTransformConfig `json:\"subject_transforms,omitempty\"`\n}\n\n// StreamState is information about the given stream.\ntype StreamState struct {\n\tMsgs        uint64            `json:\"messages\"`\n\tBytes       uint64            `json:\"bytes\"`\n\tFirstSeq    uint64            `json:\"first_seq\"`\n\tFirstTime   time.Time         `json:\"first_ts\"`\n\tLastSeq     uint64            `json:\"last_seq\"`\n\tLastTime    time.Time         `json:\"last_ts\"`\n\tConsumers   int               `json:\"consumer_count\"`\n\tDeleted     []uint64          `json:\"deleted\"`\n\tNumDeleted  int               `json:\"num_deleted\"`\n\tNumSubjects uint64            `json:\"num_subjects\"`\n\tSubjects    map[string]uint64 `json:\"subjects\"`\n}\n\n// ClusterInfo shows information about the underlying set of servers\n// that make up the stream or consumer.\ntype ClusterInfo struct {\n\tName     string      `json:\"name,omitempty\"`\n\tLeader   string      `json:\"leader,omitempty\"`\n\tReplicas []*PeerInfo `json:\"replicas,omitempty\"`\n}\n\n// PeerInfo shows information about all the peers in the cluster that\n// are supporting the stream or consumer.\ntype PeerInfo struct {\n\tName    string        `json:\"name\"`\n\tCurrent bool          `json:\"current\"`\n\tOffline bool          `json:\"offline,omitempty\"`\n\tActive  time.Duration `json:\"active\"`\n\tLag     uint64        `json:\"lag,omitempty\"`\n}\n\n// UpdateStream updates a Stream.\nfunc (js *js) UpdateStream(cfg *StreamConfig, opts ...JSOpt) (*StreamInfo, error) {\n\tif cfg == nil {\n\t\treturn nil, ErrStreamConfigRequired\n\t}\n\tif err := checkStreamName(cfg.Name); err != nil {\n\t\treturn nil, err\n\t}\n\to, cancel, err := getJSContextOpts(js.opts, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif cancel != nil {\n\t\tdefer cancel()\n\t}\n\n\treq, err := json.Marshal(cfg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tusSubj := js.apiSubj(fmt.Sprintf(apiStreamUpdateT, cfg.Name))\n\tr, err := js.apiRequestWithContext(o.ctx, usSubj, req)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar resp streamInfoResponse\n\tif err := json.Unmarshal(r.Data, &resp); err != nil {\n\t\treturn nil, err\n\t}\n\tif resp.Error != nil {\n\t\tif errors.Is(resp.Error, ErrStreamNotFound) {\n\t\t\treturn nil, ErrStreamNotFound\n\t\t}\n\t\treturn nil, resp.Error\n\t}\n\n\t// check that input subject transform (if used) is reflected in the returned StreamInfo\n\tif cfg.SubjectTransform != nil && resp.StreamInfo.Config.SubjectTransform == nil {\n\t\treturn nil, ErrStreamSubjectTransformNotSupported\n\t}\n\n\tif len(cfg.Sources) != 0 {\n\t\tif len(cfg.Sources) != len(resp.Config.Sources) {\n\t\t\treturn nil, ErrStreamSourceNotSupported\n\t\t}\n\t\tfor i := range cfg.Sources {\n\t\t\tif len(cfg.Sources[i].SubjectTransforms) != 0 && len(resp.Sources[i].SubjectTransforms) == 0 {\n\t\t\t\treturn nil, ErrStreamSourceMultipleSubjectTransformsNotSupported\n\t\t\t}\n\t\t}\n\t}\n\n\treturn resp.StreamInfo, nil\n}\n\n// streamDeleteResponse is the response for a Stream delete request.\ntype streamDeleteResponse struct {\n\tapiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\n// DeleteStream deletes a Stream.\nfunc (js *js) DeleteStream(name string, opts ...JSOpt) error {\n\tif err := checkStreamName(name); err != nil {\n\t\treturn err\n\t}\n\to, cancel, err := getJSContextOpts(js.opts, opts...)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif cancel != nil {\n\t\tdefer cancel()\n\t}\n\n\tdsSubj := js.apiSubj(fmt.Sprintf(apiStreamDeleteT, name))\n\tr, err := js.apiRequestWithContext(o.ctx, dsSubj, nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\tvar resp streamDeleteResponse\n\tif err := json.Unmarshal(r.Data, &resp); err != nil {\n\t\treturn err\n\t}\n\n\tif resp.Error != nil {\n\t\tif errors.Is(resp.Error, ErrStreamNotFound) {\n\t\t\treturn ErrStreamNotFound\n\t\t}\n\t\treturn resp.Error\n\t}\n\treturn nil\n}\n\ntype apiMsgGetRequest struct {\n\tSeq     uint64 `json:\"seq,omitempty\"`\n\tLastFor string `json:\"last_by_subj,omitempty\"`\n\tNextFor string `json:\"next_by_subj,omitempty\"`\n}\n\n// RawStreamMsg is a raw message stored in JetStream.\ntype RawStreamMsg struct {\n\tSubject  string\n\tSequence uint64\n\tHeader   Header\n\tData     []byte\n\tTime     time.Time\n}\n\n// storedMsg is a raw message stored in JetStream.\ntype storedMsg struct {\n\tSubject  string    `json:\"subject\"`\n\tSequence uint64    `json:\"seq\"`\n\tHeader   []byte    `json:\"hdrs,omitempty\"`\n\tData     []byte    `json:\"data,omitempty\"`\n\tTime     time.Time `json:\"time\"`\n}\n\n// apiMsgGetResponse is the response for a Stream get request.\ntype apiMsgGetResponse struct {\n\tapiResponse\n\tMessage *storedMsg `json:\"message,omitempty\"`\n}\n\n// GetLastMsg retrieves the last raw stream message stored in JetStream by subject.\nfunc (js *js) GetLastMsg(name, subject string, opts ...JSOpt) (*RawStreamMsg, error) {\n\treturn js.getMsg(name, &apiMsgGetRequest{LastFor: subject}, opts...)\n}\n\n// GetMsg retrieves a raw stream message stored in JetStream by sequence number.\nfunc (js *js) GetMsg(name string, seq uint64, opts ...JSOpt) (*RawStreamMsg, error) {\n\treturn js.getMsg(name, &apiMsgGetRequest{Seq: seq}, opts...)\n}\n\n// Low level getMsg\nfunc (js *js) getMsg(name string, mreq *apiMsgGetRequest, opts ...JSOpt) (*RawStreamMsg, error) {\n\to, cancel, err := getJSContextOpts(js.opts, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif cancel != nil {\n\t\tdefer cancel()\n\t}\n\n\tif err := checkStreamName(name); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar apiSubj string\n\tif o.directGet && mreq.LastFor != _EMPTY_ {\n\t\tapiSubj = apiDirectMsgGetLastBySubjectT\n\t\tdsSubj := js.apiSubj(fmt.Sprintf(apiSubj, name, mreq.LastFor))\n\t\tr, err := js.apiRequestWithContext(o.ctx, dsSubj, nil)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn convertDirectGetMsgResponseToMsg(name, r)\n\t}\n\n\tif o.directGet {\n\t\tapiSubj = apiDirectMsgGetT\n\t\tmreq.NextFor = o.directNextFor\n\t} else {\n\t\tapiSubj = apiMsgGetT\n\t}\n\n\treq, err := json.Marshal(mreq)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tdsSubj := js.apiSubj(fmt.Sprintf(apiSubj, name))\n\tr, err := js.apiRequestWithContext(o.ctx, dsSubj, req)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif o.directGet {\n\t\treturn convertDirectGetMsgResponseToMsg(name, r)\n\t}\n\n\tvar resp apiMsgGetResponse\n\tif err := json.Unmarshal(r.Data, &resp); err != nil {\n\t\treturn nil, err\n\t}\n\tif resp.Error != nil {\n\t\tif errors.Is(resp.Error, ErrMsgNotFound) {\n\t\t\treturn nil, ErrMsgNotFound\n\t\t}\n\t\tif errors.Is(resp.Error, ErrStreamNotFound) {\n\t\t\treturn nil, ErrStreamNotFound\n\t\t}\n\t\treturn nil, resp.Error\n\t}\n\n\tmsg := resp.Message\n\n\tvar hdr Header\n\tif len(msg.Header) > 0 {\n\t\thdr, err = DecodeHeadersMsg(msg.Header)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn &RawStreamMsg{\n\t\tSubject:  msg.Subject,\n\t\tSequence: msg.Sequence,\n\t\tHeader:   hdr,\n\t\tData:     msg.Data,\n\t\tTime:     msg.Time,\n\t}, nil\n}\n\nfunc convertDirectGetMsgResponseToMsg(name string, r *Msg) (*RawStreamMsg, error) {\n\t// Check for 404/408. We would get a no-payload message and a \"Status\" header\n\tif len(r.Data) == 0 {\n\t\tval := r.Header.Get(statusHdr)\n\t\tif val != _EMPTY_ {\n\t\t\tswitch val {\n\t\t\tcase noMessagesSts:\n\t\t\t\treturn nil, ErrMsgNotFound\n\t\t\tdefault:\n\t\t\t\tdesc := r.Header.Get(descrHdr)\n\t\t\t\tif desc == _EMPTY_ {\n\t\t\t\t\tdesc = \"unable to get message\"\n\t\t\t\t}\n\t\t\t\treturn nil, fmt.Errorf(\"nats: %s\", desc)\n\t\t\t}\n\t\t}\n\t}\n\t// Check for headers that give us the required information to\n\t// reconstruct the message.\n\tif len(r.Header) == 0 {\n\t\treturn nil, errors.New(\"nats: response should have headers\")\n\t}\n\tstream := r.Header.Get(JSStream)\n\tif stream == _EMPTY_ {\n\t\treturn nil, errors.New(\"nats: missing stream header\")\n\t}\n\n\t// Mirrors can now answer direct gets, so removing check for name equality.\n\t// TODO(dlc) - We could have server also have a header with origin and check that?\n\n\tseqStr := r.Header.Get(JSSequence)\n\tif seqStr == _EMPTY_ {\n\t\treturn nil, errors.New(\"nats: missing sequence header\")\n\t}\n\tseq, err := strconv.ParseUint(seqStr, 10, 64)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"nats: invalid sequence header '%s': %v\", seqStr, err)\n\t}\n\ttimeStr := r.Header.Get(JSTimeStamp)\n\tif timeStr == _EMPTY_ {\n\t\treturn nil, errors.New(\"nats: missing timestamp header\")\n\t}\n\t// Temporary code: the server in main branch is sending with format\n\t// \"2006-01-02 15:04:05.999999999 +0000 UTC\", but will be changed\n\t// to use format RFC3339Nano. Because of server test deps/cycle,\n\t// support both until the server PR lands.\n\ttm, err := time.Parse(time.RFC3339Nano, timeStr)\n\tif err != nil {\n\t\ttm, err = time.Parse(\"2006-01-02 15:04:05.999999999 +0000 UTC\", timeStr)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"nats: invalid timestamp header '%s': %v\", timeStr, err)\n\t\t}\n\t}\n\tsubj := r.Header.Get(JSSubject)\n\tif subj == _EMPTY_ {\n\t\treturn nil, errors.New(\"nats: missing subject header\")\n\t}\n\treturn &RawStreamMsg{\n\t\tSubject:  subj,\n\t\tSequence: seq,\n\t\tHeader:   r.Header,\n\t\tData:     r.Data,\n\t\tTime:     tm,\n\t}, nil\n}\n\ntype msgDeleteRequest struct {\n\tSeq     uint64 `json:\"seq\"`\n\tNoErase bool   `json:\"no_erase,omitempty\"`\n}\n\n// msgDeleteResponse is the response for a Stream delete request.\ntype msgDeleteResponse struct {\n\tapiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\n// DeleteMsg deletes a message from a stream.\n// The message is marked as erased, but not overwritten\nfunc (js *js) DeleteMsg(name string, seq uint64, opts ...JSOpt) error {\n\to, cancel, err := getJSContextOpts(js.opts, opts...)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif cancel != nil {\n\t\tdefer cancel()\n\t}\n\n\treturn js.deleteMsg(o.ctx, name, &msgDeleteRequest{Seq: seq, NoErase: true})\n}\n\n// SecureDeleteMsg deletes a message from a stream. The deleted message is overwritten with random data\n// As a result, this operation is slower than DeleteMsg()\nfunc (js *js) SecureDeleteMsg(name string, seq uint64, opts ...JSOpt) error {\n\to, cancel, err := getJSContextOpts(js.opts, opts...)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif cancel != nil {\n\t\tdefer cancel()\n\t}\n\n\treturn js.deleteMsg(o.ctx, name, &msgDeleteRequest{Seq: seq})\n}\n\nfunc (js *js) deleteMsg(ctx context.Context, stream string, req *msgDeleteRequest) error {\n\tif err := checkStreamName(stream); err != nil {\n\t\treturn err\n\t}\n\treqJSON, err := json.Marshal(req)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdsSubj := js.apiSubj(fmt.Sprintf(apiMsgDeleteT, stream))\n\tr, err := js.apiRequestWithContext(ctx, dsSubj, reqJSON)\n\tif err != nil {\n\t\treturn err\n\t}\n\tvar resp msgDeleteResponse\n\tif err := json.Unmarshal(r.Data, &resp); err != nil {\n\t\treturn err\n\t}\n\tif resp.Error != nil {\n\t\treturn resp.Error\n\t}\n\treturn nil\n}\n\n// StreamPurgeRequest is optional request information to the purge API.\ntype StreamPurgeRequest struct {\n\t// Purge up to but not including sequence.\n\tSequence uint64 `json:\"seq,omitempty\"`\n\t// Subject to match against messages for the purge command.\n\tSubject string `json:\"filter,omitempty\"`\n\t// Number of messages to keep.\n\tKeep uint64 `json:\"keep,omitempty\"`\n}\n\ntype streamPurgeResponse struct {\n\tapiResponse\n\tSuccess bool   `json:\"success,omitempty\"`\n\tPurged  uint64 `json:\"purged\"`\n}\n\n// PurgeStream purges messages on a Stream.\nfunc (js *js) PurgeStream(stream string, opts ...JSOpt) error {\n\tif err := checkStreamName(stream); err != nil {\n\t\treturn err\n\t}\n\tvar req *StreamPurgeRequest\n\tvar ok bool\n\tfor _, opt := range opts {\n\t\t// For PurgeStream, only request body opt is relevant\n\t\tif req, ok = opt.(*StreamPurgeRequest); ok {\n\t\t\tbreak\n\t\t}\n\t}\n\treturn js.purgeStream(stream, req)\n}\n\nfunc (js *js) purgeStream(stream string, req *StreamPurgeRequest, opts ...JSOpt) error {\n\to, cancel, err := getJSContextOpts(js.opts, opts...)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif cancel != nil {\n\t\tdefer cancel()\n\t}\n\n\tvar b []byte\n\tif req != nil {\n\t\tif b, err = json.Marshal(req); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tpsSubj := js.apiSubj(fmt.Sprintf(apiStreamPurgeT, stream))\n\tr, err := js.apiRequestWithContext(o.ctx, psSubj, b)\n\tif err != nil {\n\t\treturn err\n\t}\n\tvar resp streamPurgeResponse\n\tif err := json.Unmarshal(r.Data, &resp); err != nil {\n\t\treturn err\n\t}\n\tif resp.Error != nil {\n\t\tif errors.Is(resp.Error, ErrBadRequest) {\n\t\t\treturn fmt.Errorf(\"%w: %s\", ErrBadRequest, \"invalid purge request body\")\n\t\t}\n\t\treturn resp.Error\n\t}\n\treturn nil\n}\n\n// streamLister fetches pages of StreamInfo objects. This object is not safe\n// to use for multiple threads.\ntype streamLister struct {\n\tjs   *js\n\tpage []*StreamInfo\n\terr  error\n\n\toffset   int\n\tpageInfo *apiPaged\n}\n\n// streamListResponse list of detailed stream information.\n// A nil request is valid and means all streams.\ntype streamListResponse struct {\n\tapiResponse\n\tapiPaged\n\tStreams []*StreamInfo `json:\"streams\"`\n}\n\n// streamNamesRequest is used for Stream Name requests.\ntype streamNamesRequest struct {\n\tapiPagedRequest\n\t// These are filters that can be applied to the list.\n\tSubject string `json:\"subject,omitempty\"`\n}\n\n// Next fetches the next StreamInfo page.\nfunc (s *streamLister) Next() bool {\n\tif s.err != nil {\n\t\treturn false\n\t}\n\tif s.pageInfo != nil && s.offset >= s.pageInfo.Total {\n\t\treturn false\n\t}\n\n\treq, err := json.Marshal(streamNamesRequest{\n\t\tapiPagedRequest: apiPagedRequest{Offset: s.offset},\n\t\tSubject:         s.js.opts.streamListSubject,\n\t})\n\tif err != nil {\n\t\ts.err = err\n\t\treturn false\n\t}\n\n\tvar cancel context.CancelFunc\n\tctx := s.js.opts.ctx\n\tif ctx == nil {\n\t\tctx, cancel = context.WithTimeout(context.Background(), s.js.opts.wait)\n\t\tdefer cancel()\n\t}\n\n\tslSubj := s.js.apiSubj(apiStreamListT)\n\tr, err := s.js.apiRequestWithContext(ctx, slSubj, req)\n\tif err != nil {\n\t\ts.err = err\n\t\treturn false\n\t}\n\tvar resp streamListResponse\n\tif err := json.Unmarshal(r.Data, &resp); err != nil {\n\t\ts.err = err\n\t\treturn false\n\t}\n\tif resp.Error != nil {\n\t\ts.err = resp.Error\n\t\treturn false\n\t}\n\n\ts.pageInfo = &resp.apiPaged\n\ts.page = resp.Streams\n\ts.offset += len(s.page)\n\treturn true\n}\n\n// Page returns the current StreamInfo page.\nfunc (s *streamLister) Page() []*StreamInfo {\n\treturn s.page\n}\n\n// Err returns any errors found while fetching pages.\nfunc (s *streamLister) Err() error {\n\treturn s.err\n}\n\n// Streams can be used to retrieve a list of StreamInfo objects.\nfunc (jsc *js) Streams(opts ...JSOpt) <-chan *StreamInfo {\n\to, cancel, err := getJSContextOpts(jsc.opts, opts...)\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\tch := make(chan *StreamInfo)\n\tl := &streamLister{js: &js{nc: jsc.nc, opts: o}}\n\tgo func() {\n\t\tif cancel != nil {\n\t\t\tdefer cancel()\n\t\t}\n\t\tdefer close(ch)\n\t\tfor l.Next() {\n\t\t\tfor _, info := range l.Page() {\n\t\t\t\tselect {\n\t\t\t\tcase ch <- info:\n\t\t\t\tcase <-o.ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn ch\n}\n\n// StreamsInfo can be used to retrieve a list of StreamInfo objects.\n// Deprecated: Use Streams() instead.\nfunc (jsc *js) StreamsInfo(opts ...JSOpt) <-chan *StreamInfo {\n\treturn jsc.Streams(opts...)\n}\n\ntype streamNamesLister struct {\n\tjs *js\n\n\terr      error\n\toffset   int\n\tpage     []string\n\tpageInfo *apiPaged\n}\n\n// Next fetches the next stream names page.\nfunc (l *streamNamesLister) Next() bool {\n\tif l.err != nil {\n\t\treturn false\n\t}\n\tif l.pageInfo != nil && l.offset >= l.pageInfo.Total {\n\t\treturn false\n\t}\n\n\tvar cancel context.CancelFunc\n\tctx := l.js.opts.ctx\n\tif ctx == nil {\n\t\tctx, cancel = context.WithTimeout(context.Background(), l.js.opts.wait)\n\t\tdefer cancel()\n\t}\n\n\treq, err := json.Marshal(streamNamesRequest{\n\t\tapiPagedRequest: apiPagedRequest{Offset: l.offset},\n\t\tSubject:         l.js.opts.streamListSubject,\n\t})\n\tif err != nil {\n\t\tl.err = err\n\t\treturn false\n\t}\n\tr, err := l.js.apiRequestWithContext(ctx, l.js.apiSubj(apiStreams), req)\n\tif err != nil {\n\t\tl.err = err\n\t\treturn false\n\t}\n\tvar resp streamNamesResponse\n\tif err := json.Unmarshal(r.Data, &resp); err != nil {\n\t\tl.err = err\n\t\treturn false\n\t}\n\tif resp.Error != nil {\n\t\tl.err = resp.Error\n\t\treturn false\n\t}\n\n\tl.pageInfo = &resp.apiPaged\n\tl.page = resp.Streams\n\tl.offset += len(l.page)\n\treturn true\n}\n\n// Page returns the current ConsumerInfo page.\nfunc (l *streamNamesLister) Page() []string {\n\treturn l.page\n}\n\n// Err returns any errors found while fetching pages.\nfunc (l *streamNamesLister) Err() error {\n\treturn l.err\n}\n\n// StreamNames is used to retrieve a list of Stream names.\nfunc (jsc *js) StreamNames(opts ...JSOpt) <-chan string {\n\to, cancel, err := getJSContextOpts(jsc.opts, opts...)\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\tch := make(chan string)\n\tl := &streamNamesLister{js: &js{nc: jsc.nc, opts: o}}\n\tgo func() {\n\t\tif cancel != nil {\n\t\t\tdefer cancel()\n\t\t}\n\t\tdefer close(ch)\n\t\tfor l.Next() {\n\t\t\tfor _, info := range l.Page() {\n\t\t\t\tselect {\n\t\t\t\tcase ch <- info:\n\t\t\t\tcase <-o.ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn ch\n}\n\n// StreamNameBySubject returns a stream name that matches the subject.\nfunc (jsc *js) StreamNameBySubject(subj string, opts ...JSOpt) (string, error) {\n\to, cancel, err := getJSContextOpts(jsc.opts, opts...)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tif cancel != nil {\n\t\tdefer cancel()\n\t}\n\n\tvar slr streamNamesResponse\n\treq := &streamRequest{subj}\n\tj, err := json.Marshal(req)\n\tif err != nil {\n\t\treturn _EMPTY_, err\n\t}\n\n\tresp, err := jsc.apiRequestWithContext(o.ctx, jsc.apiSubj(apiStreams), j)\n\tif err != nil {\n\t\tif errors.Is(err, ErrNoResponders) {\n\t\t\terr = ErrJetStreamNotEnabled\n\t\t}\n\t\treturn _EMPTY_, err\n\t}\n\tif err := json.Unmarshal(resp.Data, &slr); err != nil {\n\t\treturn _EMPTY_, err\n\t}\n\n\tif slr.Error != nil || len(slr.Streams) != 1 {\n\t\treturn _EMPTY_, ErrNoMatchingStream\n\t}\n\treturn slr.Streams[0], nil\n}\n\nfunc getJSContextOpts(defs *jsOpts, opts ...JSOpt) (*jsOpts, context.CancelFunc, error) {\n\tvar o jsOpts\n\tfor _, opt := range opts {\n\t\tif err := opt.configureJSContext(&o); err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t}\n\n\t// Check for option collisions. Right now just timeout and context.\n\tif o.ctx != nil && o.wait != 0 {\n\t\treturn nil, nil, ErrContextAndTimeout\n\t}\n\tif o.wait == 0 && o.ctx == nil {\n\t\to.wait = defs.wait\n\t}\n\tvar cancel context.CancelFunc\n\tif o.ctx == nil && o.wait > 0 {\n\t\to.ctx, cancel = context.WithTimeout(context.Background(), o.wait)\n\t}\n\tif o.pre == _EMPTY_ {\n\t\to.pre = defs.pre\n\t}\n\n\treturn &o, cancel, nil\n}\n"
        },
        {
          "name": "kv.go",
          "type": "blob",
          "size": 33.2822265625,
          "content": "// Copyright 2021-2023 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nats\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/nats-io/nats.go/internal/parser\"\n)\n\n// KeyValueManager is used to manage KeyValue stores.\ntype KeyValueManager interface {\n\t// KeyValue will lookup and bind to an existing KeyValue store.\n\tKeyValue(bucket string) (KeyValue, error)\n\t// CreateKeyValue will create a KeyValue store with the following configuration.\n\tCreateKeyValue(cfg *KeyValueConfig) (KeyValue, error)\n\t// DeleteKeyValue will delete this KeyValue store (JetStream stream).\n\tDeleteKeyValue(bucket string) error\n\t// KeyValueStoreNames is used to retrieve a list of key value store names\n\tKeyValueStoreNames() <-chan string\n\t// KeyValueStores is used to retrieve a list of key value store statuses\n\tKeyValueStores() <-chan KeyValueStatus\n}\n\n// KeyValue contains methods to operate on a KeyValue store.\ntype KeyValue interface {\n\t// Get returns the latest value for the key.\n\tGet(key string) (entry KeyValueEntry, err error)\n\t// GetRevision returns a specific revision value for the key.\n\tGetRevision(key string, revision uint64) (entry KeyValueEntry, err error)\n\t// Put will place the new value for the key into the store.\n\tPut(key string, value []byte) (revision uint64, err error)\n\t// PutString will place the string for the key into the store.\n\tPutString(key string, value string) (revision uint64, err error)\n\t// Create will add the key/value pair iff it does not exist.\n\tCreate(key string, value []byte) (revision uint64, err error)\n\t// Update will update the value iff the latest revision matches.\n\t// Update also resets the TTL associated with the key (if any).\n\tUpdate(key string, value []byte, last uint64) (revision uint64, err error)\n\t// Delete will place a delete marker and leave all revisions.\n\tDelete(key string, opts ...DeleteOpt) error\n\t// Purge will place a delete marker and remove all previous revisions.\n\tPurge(key string, opts ...DeleteOpt) error\n\t// Watch for any updates to keys that match the keys argument which could include wildcards.\n\t// Watch will send a nil entry when it has received all initial values.\n\tWatch(keys string, opts ...WatchOpt) (KeyWatcher, error)\n\t// WatchAll will invoke the callback for all updates.\n\tWatchAll(opts ...WatchOpt) (KeyWatcher, error)\n\t// WatchFiltered will watch for any updates to keys that match the keys\n\t// argument. It can be configured with the same options as Watch.\n\tWatchFiltered(keys []string, opts ...WatchOpt) (KeyWatcher, error)\n\t// Keys will return all keys.\n\t// Deprecated: Use ListKeys instead to avoid memory issues.\n\tKeys(opts ...WatchOpt) ([]string, error)\n\t// ListKeys will return all keys in a channel.\n\tListKeys(opts ...WatchOpt) (KeyLister, error)\n\t// History will return all historical values for the key.\n\tHistory(key string, opts ...WatchOpt) ([]KeyValueEntry, error)\n\t// Bucket returns the current bucket name.\n\tBucket() string\n\t// PurgeDeletes will remove all current delete markers.\n\tPurgeDeletes(opts ...PurgeOpt) error\n\t// Status retrieves the status and configuration of a bucket\n\tStatus() (KeyValueStatus, error)\n}\n\n// KeyValueStatus is run-time status about a Key-Value bucket\ntype KeyValueStatus interface {\n\t// Bucket the name of the bucket\n\tBucket() string\n\n\t// Values is how many messages are in the bucket, including historical values\n\tValues() uint64\n\n\t// History returns the configured history kept per key\n\tHistory() int64\n\n\t// TTL is how long the bucket keeps values for\n\tTTL() time.Duration\n\n\t// BackingStore indicates what technology is used for storage of the bucket\n\tBackingStore() string\n\n\t// Bytes returns the size in bytes of the bucket\n\tBytes() uint64\n\n\t// IsCompressed indicates if the data is compressed on disk\n\tIsCompressed() bool\n}\n\n// KeyWatcher is what is returned when doing a watch.\ntype KeyWatcher interface {\n\t// Context returns watcher context optionally provided by nats.Context option.\n\tContext() context.Context\n\t// Updates returns a channel to read any updates to entries.\n\tUpdates() <-chan KeyValueEntry\n\t// Stop will stop this watcher.\n\tStop() error\n}\n\n// KeyLister is used to retrieve a list of key value store keys\ntype KeyLister interface {\n\tKeys() <-chan string\n\tStop() error\n}\n\ntype WatchOpt interface {\n\tconfigureWatcher(opts *watchOpts) error\n}\n\n// For nats.Context() support.\nfunc (ctx ContextOpt) configureWatcher(opts *watchOpts) error {\n\topts.ctx = ctx\n\treturn nil\n}\n\ntype watchOpts struct {\n\tctx context.Context\n\t// Do not send delete markers to the update channel.\n\tignoreDeletes bool\n\t// Include all history per subject, not just last one.\n\tincludeHistory bool\n\t// Include only updates for keys.\n\tupdatesOnly bool\n\t// retrieve only the meta data of the entry\n\tmetaOnly bool\n}\n\ntype watchOptFn func(opts *watchOpts) error\n\nfunc (opt watchOptFn) configureWatcher(opts *watchOpts) error {\n\treturn opt(opts)\n}\n\n// IncludeHistory instructs the key watcher to include historical values as well.\nfunc IncludeHistory() WatchOpt {\n\treturn watchOptFn(func(opts *watchOpts) error {\n\t\tif opts.updatesOnly {\n\t\t\treturn errors.New(\"nats: include history can not be used with updates only\")\n\t\t}\n\t\topts.includeHistory = true\n\t\treturn nil\n\t})\n}\n\n// UpdatesOnly instructs the key watcher to only include updates on values (without latest values when started).\nfunc UpdatesOnly() WatchOpt {\n\treturn watchOptFn(func(opts *watchOpts) error {\n\t\tif opts.includeHistory {\n\t\t\treturn errors.New(\"nats: updates only can not be used with include history\")\n\t\t}\n\t\topts.updatesOnly = true\n\t\treturn nil\n\t})\n}\n\n// IgnoreDeletes will have the key watcher not pass any deleted keys.\nfunc IgnoreDeletes() WatchOpt {\n\treturn watchOptFn(func(opts *watchOpts) error {\n\t\topts.ignoreDeletes = true\n\t\treturn nil\n\t})\n}\n\n// MetaOnly instructs the key watcher to retrieve only the entry meta data, not the entry value\nfunc MetaOnly() WatchOpt {\n\treturn watchOptFn(func(opts *watchOpts) error {\n\t\topts.metaOnly = true\n\t\treturn nil\n\t})\n}\n\ntype PurgeOpt interface {\n\tconfigurePurge(opts *purgeOpts) error\n}\n\ntype purgeOpts struct {\n\tdmthr time.Duration // Delete markers threshold\n\tctx   context.Context\n}\n\n// DeleteMarkersOlderThan indicates that delete or purge markers older than that\n// will be deleted as part of PurgeDeletes() operation, otherwise, only the data\n// will be removed but markers that are recent will be kept.\n// Note that if no option is specified, the default is 30 minutes. You can set\n// this option to a negative value to instruct to always remove the markers,\n// regardless of their age.\ntype DeleteMarkersOlderThan time.Duration\n\nfunc (ttl DeleteMarkersOlderThan) configurePurge(opts *purgeOpts) error {\n\topts.dmthr = time.Duration(ttl)\n\treturn nil\n}\n\n// For nats.Context() support.\nfunc (ctx ContextOpt) configurePurge(opts *purgeOpts) error {\n\topts.ctx = ctx\n\treturn nil\n}\n\ntype DeleteOpt interface {\n\tconfigureDelete(opts *deleteOpts) error\n}\n\ntype deleteOpts struct {\n\t// Remove all previous revisions.\n\tpurge bool\n\n\t// Delete only if the latest revision matches.\n\trevision uint64\n}\n\ntype deleteOptFn func(opts *deleteOpts) error\n\nfunc (opt deleteOptFn) configureDelete(opts *deleteOpts) error {\n\treturn opt(opts)\n}\n\n// LastRevision deletes if the latest revision matches.\nfunc LastRevision(revision uint64) DeleteOpt {\n\treturn deleteOptFn(func(opts *deleteOpts) error {\n\t\topts.revision = revision\n\t\treturn nil\n\t})\n}\n\n// purge removes all previous revisions.\nfunc purge() DeleteOpt {\n\treturn deleteOptFn(func(opts *deleteOpts) error {\n\t\topts.purge = true\n\t\treturn nil\n\t})\n}\n\n// KeyValueConfig is for configuring a KeyValue store.\ntype KeyValueConfig struct {\n\tBucket       string          `json:\"bucket\"`\n\tDescription  string          `json:\"description,omitempty\"`\n\tMaxValueSize int32           `json:\"max_value_size,omitempty\"`\n\tHistory      uint8           `json:\"history,omitempty\"`\n\tTTL          time.Duration   `json:\"ttl,omitempty\"`\n\tMaxBytes     int64           `json:\"max_bytes,omitempty\"`\n\tStorage      StorageType     `json:\"storage,omitempty\"`\n\tReplicas     int             `json:\"num_replicas,omitempty\"`\n\tPlacement    *Placement      `json:\"placement,omitempty\"`\n\tRePublish    *RePublish      `json:\"republish,omitempty\"`\n\tMirror       *StreamSource   `json:\"mirror,omitempty\"`\n\tSources      []*StreamSource `json:\"sources,omitempty\"`\n\n\t// Enable underlying stream compression.\n\t// NOTE: Compression is supported for nats-server 2.10.0+\n\tCompression bool `json:\"compression,omitempty\"`\n}\n\n// Used to watch all keys.\nconst (\n\tKeyValueMaxHistory = 64\n\tAllKeys            = \">\"\n\tkvLatestRevision   = 0\n\tkvop               = \"KV-Operation\"\n\tkvdel              = \"DEL\"\n\tkvpurge            = \"PURGE\"\n)\n\ntype KeyValueOp uint8\n\nconst (\n\tKeyValuePut KeyValueOp = iota\n\tKeyValueDelete\n\tKeyValuePurge\n)\n\nfunc (op KeyValueOp) String() string {\n\tswitch op {\n\tcase KeyValuePut:\n\t\treturn \"KeyValuePutOp\"\n\tcase KeyValueDelete:\n\t\treturn \"KeyValueDeleteOp\"\n\tcase KeyValuePurge:\n\t\treturn \"KeyValuePurgeOp\"\n\tdefault:\n\t\treturn \"Unknown Operation\"\n\t}\n}\n\n// KeyValueEntry is a retrieved entry for Get or List or Watch.\ntype KeyValueEntry interface {\n\t// Bucket is the bucket the data was loaded from.\n\tBucket() string\n\t// Key is the key that was retrieved.\n\tKey() string\n\t// Value is the retrieved value.\n\tValue() []byte\n\t// Revision is a unique sequence for this value.\n\tRevision() uint64\n\t// Created is the time the data was put in the bucket.\n\tCreated() time.Time\n\t// Delta is distance from the latest value.\n\tDelta() uint64\n\t// Operation returns Put or Delete or Purge.\n\tOperation() KeyValueOp\n}\n\n// Errors\nvar (\n\tErrKeyValueConfigRequired = errors.New(\"nats: config required\")\n\tErrInvalidBucketName      = errors.New(\"nats: invalid bucket name\")\n\tErrInvalidKey             = errors.New(\"nats: invalid key\")\n\tErrBucketNotFound         = errors.New(\"nats: bucket not found\")\n\tErrBadBucket              = errors.New(\"nats: bucket not valid key-value store\")\n\tErrKeyNotFound            = errors.New(\"nats: key not found\")\n\tErrKeyDeleted             = errors.New(\"nats: key was deleted\")\n\tErrHistoryToLarge         = errors.New(\"nats: history limited to a max of 64\")\n\tErrNoKeysFound            = errors.New(\"nats: no keys found\")\n)\n\nvar (\n\tErrKeyExists JetStreamError = &jsError{apiErr: &APIError{ErrorCode: JSErrCodeStreamWrongLastSequence, Code: 400}, message: \"key exists\"}\n)\n\nconst (\n\tkvBucketNamePre         = \"KV_\"\n\tkvBucketNameTmpl        = \"KV_%s\"\n\tkvSubjectsTmpl          = \"$KV.%s.>\"\n\tkvSubjectsPreTmpl       = \"$KV.%s.\"\n\tkvSubjectsPreDomainTmpl = \"%s.$KV.%s.\"\n\tkvNoPending             = \"0\"\n)\n\n// Regex for valid keys and buckets.\nvar (\n\tvalidBucketRe    = regexp.MustCompile(`^[a-zA-Z0-9_-]+$`)\n\tvalidKeyRe       = regexp.MustCompile(`^[-/_=\\.a-zA-Z0-9]+$`)\n\tvalidSearchKeyRe = regexp.MustCompile(`^[-/_=\\.a-zA-Z0-9*]*[>]?$`)\n)\n\n// KeyValue will lookup and bind to an existing KeyValue store.\nfunc (js *js) KeyValue(bucket string) (KeyValue, error) {\n\tif !js.nc.serverMinVersion(2, 6, 2) {\n\t\treturn nil, errors.New(\"nats: key-value requires at least server version 2.6.2\")\n\t}\n\tif !bucketValid(bucket) {\n\t\treturn nil, ErrInvalidBucketName\n\t}\n\tstream := fmt.Sprintf(kvBucketNameTmpl, bucket)\n\tsi, err := js.StreamInfo(stream)\n\tif err != nil {\n\t\tif errors.Is(err, ErrStreamNotFound) {\n\t\t\terr = ErrBucketNotFound\n\t\t}\n\t\treturn nil, err\n\t}\n\t// Do some quick sanity checks that this is a correctly formed stream for KV.\n\t// Max msgs per subject should be > 0.\n\tif si.Config.MaxMsgsPerSubject < 1 {\n\t\treturn nil, ErrBadBucket\n\t}\n\n\treturn mapStreamToKVS(js, si), nil\n}\n\n// CreateKeyValue will create a KeyValue store with the following configuration.\nfunc (js *js) CreateKeyValue(cfg *KeyValueConfig) (KeyValue, error) {\n\tif !js.nc.serverMinVersion(2, 6, 2) {\n\t\treturn nil, errors.New(\"nats: key-value requires at least server version 2.6.2\")\n\t}\n\tif cfg == nil {\n\t\treturn nil, ErrKeyValueConfigRequired\n\t}\n\tif !bucketValid(cfg.Bucket) {\n\t\treturn nil, ErrInvalidBucketName\n\t}\n\tif _, err := js.AccountInfo(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Default to 1 for history. Max is 64 for now.\n\thistory := int64(1)\n\tif cfg.History > 0 {\n\t\tif cfg.History > KeyValueMaxHistory {\n\t\t\treturn nil, ErrHistoryToLarge\n\t\t}\n\t\thistory = int64(cfg.History)\n\t}\n\n\treplicas := cfg.Replicas\n\tif replicas == 0 {\n\t\treplicas = 1\n\t}\n\n\t// We will set explicitly some values so that we can do comparison\n\t// if we get an \"already in use\" error and need to check if it is same.\n\tmaxBytes := cfg.MaxBytes\n\tif maxBytes == 0 {\n\t\tmaxBytes = -1\n\t}\n\tmaxMsgSize := cfg.MaxValueSize\n\tif maxMsgSize == 0 {\n\t\tmaxMsgSize = -1\n\t}\n\t// When stream's MaxAge is not set, server uses 2 minutes as the default\n\t// for the duplicate window. If MaxAge is set, and lower than 2 minutes,\n\t// then the duplicate window will be set to that. If MaxAge is greater,\n\t// we will cap the duplicate window to 2 minutes (to be consistent with\n\t// previous behavior).\n\tduplicateWindow := 2 * time.Minute\n\tif cfg.TTL > 0 && cfg.TTL < duplicateWindow {\n\t\tduplicateWindow = cfg.TTL\n\t}\n\tvar compression StoreCompression\n\tif cfg.Compression {\n\t\tcompression = S2Compression\n\t}\n\tscfg := &StreamConfig{\n\t\tName:              fmt.Sprintf(kvBucketNameTmpl, cfg.Bucket),\n\t\tDescription:       cfg.Description,\n\t\tMaxMsgsPerSubject: history,\n\t\tMaxBytes:          maxBytes,\n\t\tMaxAge:            cfg.TTL,\n\t\tMaxMsgSize:        maxMsgSize,\n\t\tStorage:           cfg.Storage,\n\t\tReplicas:          replicas,\n\t\tPlacement:         cfg.Placement,\n\t\tAllowRollup:       true,\n\t\tDenyDelete:        true,\n\t\tDuplicates:        duplicateWindow,\n\t\tMaxMsgs:           -1,\n\t\tMaxConsumers:      -1,\n\t\tAllowDirect:       true,\n\t\tRePublish:         cfg.RePublish,\n\t\tCompression:       compression,\n\t}\n\tif cfg.Mirror != nil {\n\t\t// Copy in case we need to make changes so we do not change caller's version.\n\t\tm := cfg.Mirror.copy()\n\t\tif !strings.HasPrefix(m.Name, kvBucketNamePre) {\n\t\t\tm.Name = fmt.Sprintf(kvBucketNameTmpl, m.Name)\n\t\t}\n\t\tscfg.Mirror = m\n\t\tscfg.MirrorDirect = true\n\t} else if len(cfg.Sources) > 0 {\n\t\tfor _, ss := range cfg.Sources {\n\t\t\tvar sourceBucketName string\n\t\t\tif strings.HasPrefix(ss.Name, kvBucketNamePre) {\n\t\t\t\tsourceBucketName = ss.Name[len(kvBucketNamePre):]\n\t\t\t} else {\n\t\t\t\tsourceBucketName = ss.Name\n\t\t\t\tss.Name = fmt.Sprintf(kvBucketNameTmpl, ss.Name)\n\t\t\t}\n\n\t\t\tif ss.External == nil || sourceBucketName != cfg.Bucket {\n\t\t\t\tss.SubjectTransforms = []SubjectTransformConfig{{Source: fmt.Sprintf(kvSubjectsTmpl, sourceBucketName), Destination: fmt.Sprintf(kvSubjectsTmpl, cfg.Bucket)}}\n\t\t\t}\n\t\t\tscfg.Sources = append(scfg.Sources, ss)\n\t\t}\n\t\tscfg.Subjects = []string{fmt.Sprintf(kvSubjectsTmpl, cfg.Bucket)}\n\t} else {\n\t\tscfg.Subjects = []string{fmt.Sprintf(kvSubjectsTmpl, cfg.Bucket)}\n\t}\n\n\t// If we are at server version 2.7.2 or above use DiscardNew. We can not use DiscardNew for 2.7.1 or below.\n\tif js.nc.serverMinVersion(2, 7, 2) {\n\t\tscfg.Discard = DiscardNew\n\t}\n\n\tsi, err := js.AddStream(scfg)\n\tif err != nil {\n\t\t// If we have a failure to add, it could be because we have\n\t\t// a config change if the KV was created against a pre 2.7.2\n\t\t// and we are now moving to a v2.7.2+. If that is the case\n\t\t// and the only difference is the discard policy, then update\n\t\t// the stream.\n\t\t// The same logic applies for KVs created pre 2.9.x and\n\t\t// the AllowDirect setting.\n\t\tif errors.Is(err, ErrStreamNameAlreadyInUse) {\n\t\t\tif si, _ = js.StreamInfo(scfg.Name); si != nil {\n\t\t\t\t// To compare, make the server's stream info discard\n\t\t\t\t// policy same than ours.\n\t\t\t\tsi.Config.Discard = scfg.Discard\n\t\t\t\t// Also need to set allow direct for v2.9.x+\n\t\t\t\tsi.Config.AllowDirect = scfg.AllowDirect\n\t\t\t\tif reflect.DeepEqual(&si.Config, scfg) {\n\t\t\t\t\tsi, err = js.UpdateStream(scfg)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\treturn mapStreamToKVS(js, si), nil\n}\n\n// DeleteKeyValue will delete this KeyValue store (JetStream stream).\nfunc (js *js) DeleteKeyValue(bucket string) error {\n\tif !bucketValid(bucket) {\n\t\treturn ErrInvalidBucketName\n\t}\n\tstream := fmt.Sprintf(kvBucketNameTmpl, bucket)\n\treturn js.DeleteStream(stream)\n}\n\ntype kvs struct {\n\tname   string\n\tstream string\n\tpre    string\n\tputPre string\n\tjs     *js\n\t// If true, it means that APIPrefix/Domain was set in the context\n\t// and we need to add something to some of our high level protocols\n\t// (such as Put, etc..)\n\tuseJSPfx bool\n\t// To know if we can use the stream direct get API\n\tuseDirect bool\n}\n\n// Underlying entry.\ntype kve struct {\n\tbucket   string\n\tkey      string\n\tvalue    []byte\n\trevision uint64\n\tdelta    uint64\n\tcreated  time.Time\n\top       KeyValueOp\n}\n\nfunc (e *kve) Bucket() string        { return e.bucket }\nfunc (e *kve) Key() string           { return e.key }\nfunc (e *kve) Value() []byte         { return e.value }\nfunc (e *kve) Revision() uint64      { return e.revision }\nfunc (e *kve) Created() time.Time    { return e.created }\nfunc (e *kve) Delta() uint64         { return e.delta }\nfunc (e *kve) Operation() KeyValueOp { return e.op }\n\nfunc bucketValid(bucket string) bool {\n\tif len(bucket) == 0 {\n\t\treturn false\n\t}\n\treturn validBucketRe.MatchString(bucket)\n}\n\nfunc keyValid(key string) bool {\n\tif len(key) == 0 || key[0] == '.' || key[len(key)-1] == '.' {\n\t\treturn false\n\t}\n\treturn validKeyRe.MatchString(key)\n}\n\nfunc searchKeyValid(key string) bool {\n\tif len(key) == 0 || key[0] == '.' || key[len(key)-1] == '.' {\n\t\treturn false\n\t}\n\treturn validSearchKeyRe.MatchString(key)\n}\n\n// Get returns the latest value for the key.\nfunc (kv *kvs) Get(key string) (KeyValueEntry, error) {\n\te, err := kv.get(key, kvLatestRevision)\n\tif err != nil {\n\t\tif errors.Is(err, ErrKeyDeleted) {\n\t\t\treturn nil, ErrKeyNotFound\n\t\t}\n\t\treturn nil, err\n\t}\n\n\treturn e, nil\n}\n\n// GetRevision returns a specific revision value for the key.\nfunc (kv *kvs) GetRevision(key string, revision uint64) (KeyValueEntry, error) {\n\te, err := kv.get(key, revision)\n\tif err != nil {\n\t\tif errors.Is(err, ErrKeyDeleted) {\n\t\t\treturn nil, ErrKeyNotFound\n\t\t}\n\t\treturn nil, err\n\t}\n\n\treturn e, nil\n}\n\nfunc (kv *kvs) get(key string, revision uint64) (KeyValueEntry, error) {\n\tif !keyValid(key) {\n\t\treturn nil, ErrInvalidKey\n\t}\n\n\tvar b strings.Builder\n\tb.WriteString(kv.pre)\n\tb.WriteString(key)\n\n\tvar m *RawStreamMsg\n\tvar err error\n\tvar _opts [1]JSOpt\n\topts := _opts[:0]\n\tif kv.useDirect {\n\t\topts = append(opts, DirectGet())\n\t}\n\n\tif revision == kvLatestRevision {\n\t\tm, err = kv.js.GetLastMsg(kv.stream, b.String(), opts...)\n\t} else {\n\t\tm, err = kv.js.GetMsg(kv.stream, revision, opts...)\n\t\t// If a sequence was provided, just make sure that the retrieved\n\t\t// message subject matches the request.\n\t\tif err == nil && m.Subject != b.String() {\n\t\t\treturn nil, ErrKeyNotFound\n\t\t}\n\t}\n\tif err != nil {\n\t\tif errors.Is(err, ErrMsgNotFound) {\n\t\t\terr = ErrKeyNotFound\n\t\t}\n\t\treturn nil, err\n\t}\n\n\tentry := &kve{\n\t\tbucket:   kv.name,\n\t\tkey:      key,\n\t\tvalue:    m.Data,\n\t\trevision: m.Sequence,\n\t\tcreated:  m.Time,\n\t}\n\n\t// Double check here that this is not a DEL Operation marker.\n\tif len(m.Header) > 0 {\n\t\tswitch m.Header.Get(kvop) {\n\t\tcase kvdel:\n\t\t\tentry.op = KeyValueDelete\n\t\t\treturn entry, ErrKeyDeleted\n\t\tcase kvpurge:\n\t\t\tentry.op = KeyValuePurge\n\t\t\treturn entry, ErrKeyDeleted\n\t\t}\n\t}\n\n\treturn entry, nil\n}\n\n// Put will place the new value for the key into the store.\nfunc (kv *kvs) Put(key string, value []byte) (revision uint64, err error) {\n\tif !keyValid(key) {\n\t\treturn 0, ErrInvalidKey\n\t}\n\n\tvar b strings.Builder\n\tif kv.useJSPfx {\n\t\tb.WriteString(kv.js.opts.pre)\n\t}\n\tif kv.putPre != _EMPTY_ {\n\t\tb.WriteString(kv.putPre)\n\t} else {\n\t\tb.WriteString(kv.pre)\n\t}\n\tb.WriteString(key)\n\n\tpa, err := kv.js.Publish(b.String(), value)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn pa.Sequence, err\n}\n\n// PutString will place the string for the key into the store.\nfunc (kv *kvs) PutString(key string, value string) (revision uint64, err error) {\n\treturn kv.Put(key, []byte(value))\n}\n\n// Create will add the key/value pair if it does not exist.\nfunc (kv *kvs) Create(key string, value []byte) (revision uint64, err error) {\n\tv, err := kv.Update(key, value, 0)\n\tif err == nil {\n\t\treturn v, nil\n\t}\n\n\t// TODO(dlc) - Since we have tombstones for DEL ops for watchers, this could be from that\n\t// so we need to double check.\n\tif e, err := kv.get(key, kvLatestRevision); errors.Is(err, ErrKeyDeleted) {\n\t\treturn kv.Update(key, value, e.Revision())\n\t}\n\n\t// Check if the expected last subject sequence is not zero which implies\n\t// the key already exists.\n\tif errors.Is(err, ErrKeyExists) {\n\t\tjserr := ErrKeyExists.(*jsError)\n\t\treturn 0, fmt.Errorf(\"%w: %s\", err, jserr.message)\n\t}\n\n\treturn 0, err\n}\n\n// Update will update the value if the latest revision matches.\nfunc (kv *kvs) Update(key string, value []byte, revision uint64) (uint64, error) {\n\tif !keyValid(key) {\n\t\treturn 0, ErrInvalidKey\n\t}\n\n\tvar b strings.Builder\n\tif kv.useJSPfx {\n\t\tb.WriteString(kv.js.opts.pre)\n\t}\n\tb.WriteString(kv.pre)\n\tb.WriteString(key)\n\n\tm := Msg{Subject: b.String(), Header: Header{}, Data: value}\n\tm.Header.Set(ExpectedLastSubjSeqHdr, strconv.FormatUint(revision, 10))\n\n\tpa, err := kv.js.PublishMsg(&m)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn pa.Sequence, err\n}\n\n// Delete will place a delete marker and leave all revisions.\nfunc (kv *kvs) Delete(key string, opts ...DeleteOpt) error {\n\tif !keyValid(key) {\n\t\treturn ErrInvalidKey\n\t}\n\n\tvar b strings.Builder\n\tif kv.useJSPfx {\n\t\tb.WriteString(kv.js.opts.pre)\n\t}\n\tif kv.putPre != _EMPTY_ {\n\t\tb.WriteString(kv.putPre)\n\t} else {\n\t\tb.WriteString(kv.pre)\n\t}\n\tb.WriteString(key)\n\n\t// DEL op marker. For watch functionality.\n\tm := NewMsg(b.String())\n\n\tvar o deleteOpts\n\tfor _, opt := range opts {\n\t\tif opt != nil {\n\t\t\tif err := opt.configureDelete(&o); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\tif o.purge {\n\t\tm.Header.Set(kvop, kvpurge)\n\t\tm.Header.Set(MsgRollup, MsgRollupSubject)\n\t} else {\n\t\tm.Header.Set(kvop, kvdel)\n\t}\n\n\tif o.revision != 0 {\n\t\tm.Header.Set(ExpectedLastSubjSeqHdr, strconv.FormatUint(o.revision, 10))\n\t}\n\n\t_, err := kv.js.PublishMsg(m)\n\treturn err\n}\n\n// Purge will remove the key and all revisions.\nfunc (kv *kvs) Purge(key string, opts ...DeleteOpt) error {\n\treturn kv.Delete(key, append(opts, purge())...)\n}\n\nconst kvDefaultPurgeDeletesMarkerThreshold = 30 * time.Minute\n\n// PurgeDeletes will remove all current delete markers.\n// This is a maintenance option if there is a larger buildup of delete markers.\n// See DeleteMarkersOlderThan() option for more information.\nfunc (kv *kvs) PurgeDeletes(opts ...PurgeOpt) error {\n\tvar o purgeOpts\n\tfor _, opt := range opts {\n\t\tif opt != nil {\n\t\t\tif err := opt.configurePurge(&o); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\t// Transfer possible context purge option to the watcher. This is the\n\t// only option that matters for the PurgeDeletes() feature.\n\tvar wopts []WatchOpt\n\tif o.ctx != nil {\n\t\twopts = append(wopts, Context(o.ctx))\n\t}\n\twatcher, err := kv.WatchAll(wopts...)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer watcher.Stop()\n\n\tvar limit time.Time\n\tolderThan := o.dmthr\n\t// Negative value is used to instruct to always remove markers, regardless\n\t// of age. If set to 0 (or not set), use our default value.\n\tif olderThan == 0 {\n\t\tolderThan = kvDefaultPurgeDeletesMarkerThreshold\n\t}\n\tif olderThan > 0 {\n\t\tlimit = time.Now().Add(-olderThan)\n\t}\n\n\tvar deleteMarkers []KeyValueEntry\n\tfor entry := range watcher.Updates() {\n\t\tif entry == nil {\n\t\t\tbreak\n\t\t}\n\t\tif op := entry.Operation(); op == KeyValueDelete || op == KeyValuePurge {\n\t\t\tdeleteMarkers = append(deleteMarkers, entry)\n\t\t}\n\t}\n\n\tvar (\n\t\tpr StreamPurgeRequest\n\t\tb  strings.Builder\n\t)\n\t// Do actual purges here.\n\tfor _, entry := range deleteMarkers {\n\t\tb.WriteString(kv.pre)\n\t\tb.WriteString(entry.Key())\n\t\tpr.Subject = b.String()\n\t\tpr.Keep = 0\n\t\tif olderThan > 0 && entry.Created().After(limit) {\n\t\t\tpr.Keep = 1\n\t\t}\n\t\tif err := kv.js.purgeStream(kv.stream, &pr); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tb.Reset()\n\t}\n\treturn nil\n}\n\n// Keys() will return all keys.\nfunc (kv *kvs) Keys(opts ...WatchOpt) ([]string, error) {\n\topts = append(opts, IgnoreDeletes(), MetaOnly())\n\twatcher, err := kv.WatchAll(opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer watcher.Stop()\n\n\tvar keys []string\n\tfor entry := range watcher.Updates() {\n\t\tif entry == nil {\n\t\t\tbreak\n\t\t}\n\t\tkeys = append(keys, entry.Key())\n\t}\n\tif len(keys) == 0 {\n\t\treturn nil, ErrNoKeysFound\n\t}\n\treturn keys, nil\n}\n\ntype keyLister struct {\n\twatcher KeyWatcher\n\tkeys    chan string\n}\n\n// ListKeys will return all keys.\nfunc (kv *kvs) ListKeys(opts ...WatchOpt) (KeyLister, error) {\n\topts = append(opts, IgnoreDeletes(), MetaOnly())\n\twatcher, err := kv.WatchAll(opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tkl := &keyLister{watcher: watcher, keys: make(chan string, 256)}\n\n\tgo func() {\n\t\tdefer close(kl.keys)\n\t\tdefer watcher.Stop()\n\t\tfor entry := range watcher.Updates() {\n\t\t\tif entry == nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tkl.keys <- entry.Key()\n\t\t}\n\t}()\n\treturn kl, nil\n}\n\nfunc (kl *keyLister) Keys() <-chan string {\n\treturn kl.keys\n}\n\nfunc (kl *keyLister) Stop() error {\n\treturn kl.watcher.Stop()\n}\n\n// History will return all values for the key.\nfunc (kv *kvs) History(key string, opts ...WatchOpt) ([]KeyValueEntry, error) {\n\topts = append(opts, IncludeHistory())\n\twatcher, err := kv.Watch(key, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer watcher.Stop()\n\n\tvar entries []KeyValueEntry\n\tfor entry := range watcher.Updates() {\n\t\tif entry == nil {\n\t\t\tbreak\n\t\t}\n\t\tentries = append(entries, entry)\n\t}\n\tif len(entries) == 0 {\n\t\treturn nil, ErrKeyNotFound\n\t}\n\treturn entries, nil\n}\n\n// Implementation for Watch\ntype watcher struct {\n\tmu          sync.Mutex\n\tupdates     chan KeyValueEntry\n\tsub         *Subscription\n\tinitDone    bool\n\tinitPending uint64\n\treceived    uint64\n\tctx         context.Context\n}\n\n// Context returns the context for the watcher if set.\nfunc (w *watcher) Context() context.Context {\n\tif w == nil {\n\t\treturn nil\n\t}\n\treturn w.ctx\n}\n\n// Updates returns the interior channel.\nfunc (w *watcher) Updates() <-chan KeyValueEntry {\n\tif w == nil {\n\t\treturn nil\n\t}\n\treturn w.updates\n}\n\n// Stop will unsubscribe from the watcher.\nfunc (w *watcher) Stop() error {\n\tif w == nil {\n\t\treturn nil\n\t}\n\treturn w.sub.Unsubscribe()\n}\n\n// WatchAll watches all keys.\nfunc (kv *kvs) WatchAll(opts ...WatchOpt) (KeyWatcher, error) {\n\treturn kv.Watch(AllKeys, opts...)\n}\n\nfunc (kv *kvs) WatchFiltered(keys []string, opts ...WatchOpt) (KeyWatcher, error) {\n\tfor _, key := range keys {\n\t\tif !searchKeyValid(key) {\n\t\t\treturn nil, fmt.Errorf(\"%w: %s\", ErrInvalidKey, \"key cannot be empty and must be a valid NATS subject\")\n\t\t}\n\t}\n\tvar o watchOpts\n\tfor _, opt := range opts {\n\t\tif opt != nil {\n\t\t\tif err := opt.configureWatcher(&o); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\t// Could be a pattern so don't check for validity as we normally do.\n\tfor i, key := range keys {\n\t\tvar b strings.Builder\n\t\tb.WriteString(kv.pre)\n\t\tb.WriteString(key)\n\t\tkeys[i] = b.String()\n\t}\n\n\t// if no keys are provided, watch all keys\n\tif len(keys) == 0 {\n\t\tvar b strings.Builder\n\t\tb.WriteString(kv.pre)\n\t\tb.WriteString(AllKeys)\n\t\tkeys = []string{b.String()}\n\t}\n\n\t// We will block below on placing items on the chan. That is by design.\n\tw := &watcher{updates: make(chan KeyValueEntry, 256), ctx: o.ctx}\n\n\tupdate := func(m *Msg) {\n\t\ttokens, err := parser.GetMetadataFields(m.Reply)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tif len(m.Subject) <= len(kv.pre) {\n\t\t\treturn\n\t\t}\n\t\tsubj := m.Subject[len(kv.pre):]\n\n\t\tvar op KeyValueOp\n\t\tif len(m.Header) > 0 {\n\t\t\tswitch m.Header.Get(kvop) {\n\t\t\tcase kvdel:\n\t\t\t\top = KeyValueDelete\n\t\t\tcase kvpurge:\n\t\t\t\top = KeyValuePurge\n\t\t\t}\n\t\t}\n\t\tdelta := parser.ParseNum(tokens[parser.AckNumPendingTokenPos])\n\t\tw.mu.Lock()\n\t\tdefer w.mu.Unlock()\n\t\tif !o.ignoreDeletes || (op != KeyValueDelete && op != KeyValuePurge) {\n\t\t\tentry := &kve{\n\t\t\t\tbucket:   kv.name,\n\t\t\t\tkey:      subj,\n\t\t\t\tvalue:    m.Data,\n\t\t\t\trevision: parser.ParseNum(tokens[parser.AckStreamSeqTokenPos]),\n\t\t\t\tcreated:  time.Unix(0, int64(parser.ParseNum(tokens[parser.AckTimestampSeqTokenPos]))),\n\t\t\t\tdelta:    delta,\n\t\t\t\top:       op,\n\t\t\t}\n\t\t\tw.updates <- entry\n\t\t}\n\t\t// Check if done and initial values.\n\t\t// Skip if UpdatesOnly() is set, since there will never be updates initially.\n\t\tif !w.initDone {\n\t\t\tw.received++\n\t\t\t// We set this on the first trip through..\n\t\t\tif w.initPending == 0 {\n\t\t\t\tw.initPending = delta\n\t\t\t}\n\t\t\tif w.received > w.initPending || delta == 0 {\n\t\t\t\tw.initDone = true\n\t\t\t\tw.updates <- nil\n\t\t\t}\n\t\t}\n\t}\n\n\t// Used ordered consumer to deliver results.\n\tsubOpts := []SubOpt{BindStream(kv.stream), OrderedConsumer()}\n\tif !o.includeHistory {\n\t\tsubOpts = append(subOpts, DeliverLastPerSubject())\n\t}\n\tif o.updatesOnly {\n\t\tsubOpts = append(subOpts, DeliverNew())\n\t}\n\tif o.metaOnly {\n\t\tsubOpts = append(subOpts, HeadersOnly())\n\t}\n\tif o.ctx != nil {\n\t\tsubOpts = append(subOpts, Context(o.ctx))\n\t}\n\t// Create the sub and rest of initialization under the lock.\n\t// We want to prevent the race between this code and the\n\t// update() callback.\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\tvar sub *Subscription\n\tvar err error\n\tif len(keys) == 1 {\n\t\tsub, err = kv.js.Subscribe(keys[0], update, subOpts...)\n\t} else {\n\t\tsubOpts = append(subOpts, ConsumerFilterSubjects(keys...))\n\t\tsub, err = kv.js.Subscribe(\"\", update, subOpts...)\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsub.mu.Lock()\n\t// If there were no pending messages at the time of the creation\n\t// of the consumer, send the marker.\n\t// Skip if UpdatesOnly() is set, since there will never be updates initially.\n\tif !o.updatesOnly {\n\t\tif sub.jsi != nil && sub.jsi.pending == 0 {\n\t\t\tw.initDone = true\n\t\t\tw.updates <- nil\n\t\t}\n\t} else {\n\t\t// if UpdatesOnly was used, mark initialization as complete\n\t\tw.initDone = true\n\t}\n\t// Set us up to close when the waitForMessages func returns.\n\tsub.pDone = func(_ string) {\n\t\tclose(w.updates)\n\t}\n\tsub.mu.Unlock()\n\n\tw.sub = sub\n\treturn w, nil\n}\n\n// Watch will fire the callback when a key that matches the keys pattern is updated.\n// keys needs to be a valid NATS subject.\nfunc (kv *kvs) Watch(keys string, opts ...WatchOpt) (KeyWatcher, error) {\n\treturn kv.WatchFiltered([]string{keys}, opts...)\n}\n\n// Bucket returns the current bucket name (JetStream stream).\nfunc (kv *kvs) Bucket() string {\n\treturn kv.name\n}\n\n// KeyValueBucketStatus represents status of a Bucket, implements KeyValueStatus\ntype KeyValueBucketStatus struct {\n\tnfo    *StreamInfo\n\tbucket string\n}\n\n// Bucket the name of the bucket\nfunc (s *KeyValueBucketStatus) Bucket() string { return s.bucket }\n\n// Values is how many messages are in the bucket, including historical values\nfunc (s *KeyValueBucketStatus) Values() uint64 { return s.nfo.State.Msgs }\n\n// History returns the configured history kept per key\nfunc (s *KeyValueBucketStatus) History() int64 { return s.nfo.Config.MaxMsgsPerSubject }\n\n// TTL is how long the bucket keeps values for\nfunc (s *KeyValueBucketStatus) TTL() time.Duration { return s.nfo.Config.MaxAge }\n\n// BackingStore indicates what technology is used for storage of the bucket\nfunc (s *KeyValueBucketStatus) BackingStore() string { return \"JetStream\" }\n\n// StreamInfo is the stream info retrieved to create the status\nfunc (s *KeyValueBucketStatus) StreamInfo() *StreamInfo { return s.nfo }\n\n// Bytes is the size of the stream\nfunc (s *KeyValueBucketStatus) Bytes() uint64 { return s.nfo.State.Bytes }\n\n// IsCompressed indicates if the data is compressed on disk\nfunc (s *KeyValueBucketStatus) IsCompressed() bool { return s.nfo.Config.Compression != NoCompression }\n\n// Status retrieves the status and configuration of a bucket\nfunc (kv *kvs) Status() (KeyValueStatus, error) {\n\tnfo, err := kv.js.StreamInfo(kv.stream)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &KeyValueBucketStatus{nfo: nfo, bucket: kv.name}, nil\n}\n\n// KeyValueStoreNames is used to retrieve a list of key value store names\nfunc (js *js) KeyValueStoreNames() <-chan string {\n\tch := make(chan string)\n\tl := &streamNamesLister{js: js}\n\tl.js.opts.streamListSubject = fmt.Sprintf(kvSubjectsTmpl, \"*\")\n\tgo func() {\n\t\tdefer close(ch)\n\t\tfor l.Next() {\n\t\t\tfor _, name := range l.Page() {\n\t\t\t\tif !strings.HasPrefix(name, kvBucketNamePre) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tch <- strings.TrimPrefix(name, kvBucketNamePre)\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn ch\n}\n\n// KeyValueStores is used to retrieve a list of key value store statuses\nfunc (js *js) KeyValueStores() <-chan KeyValueStatus {\n\tch := make(chan KeyValueStatus)\n\tl := &streamLister{js: js}\n\tl.js.opts.streamListSubject = fmt.Sprintf(kvSubjectsTmpl, \"*\")\n\tgo func() {\n\t\tdefer close(ch)\n\t\tfor l.Next() {\n\t\t\tfor _, info := range l.Page() {\n\t\t\t\tif !strings.HasPrefix(info.Config.Name, kvBucketNamePre) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tch <- &KeyValueBucketStatus{nfo: info, bucket: strings.TrimPrefix(info.Config.Name, kvBucketNamePre)}\n\t\t\t}\n\t\t}\n\t}()\n\treturn ch\n}\n\nfunc mapStreamToKVS(js *js, info *StreamInfo) *kvs {\n\tbucket := strings.TrimPrefix(info.Config.Name, kvBucketNamePre)\n\n\tkv := &kvs{\n\t\tname:   bucket,\n\t\tstream: info.Config.Name,\n\t\tpre:    fmt.Sprintf(kvSubjectsPreTmpl, bucket),\n\t\tjs:     js,\n\t\t// Determine if we need to use the JS prefix in front of Put and Delete operations\n\t\tuseJSPfx:  js.opts.pre != defaultAPIPrefix,\n\t\tuseDirect: info.Config.AllowDirect,\n\t}\n\n\t// If we are mirroring, we will have mirror direct on, so just use the mirror name\n\t// and override use\n\tif m := info.Config.Mirror; m != nil {\n\t\tbucket := strings.TrimPrefix(m.Name, kvBucketNamePre)\n\t\tif m.External != nil && m.External.APIPrefix != _EMPTY_ {\n\t\t\tkv.useJSPfx = false\n\t\t\tkv.pre = fmt.Sprintf(kvSubjectsPreTmpl, bucket)\n\t\t\tkv.putPre = fmt.Sprintf(kvSubjectsPreDomainTmpl, m.External.APIPrefix, bucket)\n\t\t} else {\n\t\t\tkv.putPre = fmt.Sprintf(kvSubjectsPreTmpl, bucket)\n\t\t}\n\t}\n\n\treturn kv\n}\n"
        },
        {
          "name": "legacy_jetstream.md",
          "type": "blob",
          "size": 1.7373046875,
          "content": "# Legacy JetStream API\n\nThis is a documentation for the legacy JetStream API. A README for the current\nAPI can be found [here](jetstream/README.md)\n\n## JetStream Basic Usage\n\n```go\nimport \"github.com/nats-io/nats.go\"\n\n// Connect to NATS\nnc, _ := nats.Connect(nats.DefaultURL)\n\n// Create JetStream Context\njs, _ := nc.JetStream(nats.PublishAsyncMaxPending(256))\n\n// Simple Stream Publisher\njs.Publish(\"ORDERS.scratch\", []byte(\"hello\"))\n\n// Simple Async Stream Publisher\nfor i := 0; i < 500; i++ {\n    js.PublishAsync(\"ORDERS.scratch\", []byte(\"hello\"))\n}\nselect {\ncase <-js.PublishAsyncComplete():\ncase <-time.After(5 * time.Second):\n    fmt.Println(\"Did not resolve in time\")\n}\n\n// Simple Async Ephemeral Consumer\njs.Subscribe(\"ORDERS.*\", func(m *nats.Msg) {\n    fmt.Printf(\"Received a JetStream message: %s\\n\", string(m.Data))\n})\n\n// Simple Sync Durable Consumer (optional SubOpts at the end)\nsub, err := js.SubscribeSync(\"ORDERS.*\", nats.Durable(\"MONITOR\"), nats.MaxDeliver(3))\nm, err := sub.NextMsg(timeout)\n\n// Simple Pull Consumer\nsub, err := js.PullSubscribe(\"ORDERS.*\", \"MONITOR\")\nmsgs, err := sub.Fetch(10)\n\n// Unsubscribe\nsub.Unsubscribe()\n\n// Drain\nsub.Drain()\n```\n\n## JetStream Basic Management\n\n```go\nimport \"github.com/nats-io/nats.go\"\n\n// Connect to NATS\nnc, _ := nats.Connect(nats.DefaultURL)\n\n// Create JetStream Context\njs, _ := nc.JetStream()\n\n// Create a Stream\njs.AddStream(&nats.StreamConfig{\n    Name:     \"ORDERS\",\n    Subjects: []string{\"ORDERS.*\"},\n})\n\n// Update a Stream\njs.UpdateStream(&nats.StreamConfig{\n    Name:     \"ORDERS\",\n    MaxBytes: 8,\n})\n\n// Create a Consumer\njs.AddConsumer(\"ORDERS\", &nats.ConsumerConfig{\n    Durable: \"MONITOR\",\n})\n\n// Delete Consumer\njs.DeleteConsumer(\"ORDERS\", \"MONITOR\")\n\n// Delete Stream\njs.DeleteStream(\"ORDERS\")\n```\n"
        },
        {
          "name": "micro",
          "type": "tree",
          "content": null
        },
        {
          "name": "nats.go",
          "type": "blob",
          "size": 159.751953125,
          "content": "// Copyright 2012-2024 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// A Go client for the NATS messaging system (https://nats.io).\npackage nats\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"math/rand\"\n\t\"net\"\n\t\"net/http\"\n\t\"net/textproto\"\n\t\"net/url\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"regexp\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/nats-io/nkeys\"\n\t\"github.com/nats-io/nuid\"\n\n\t\"github.com/nats-io/nats.go/util\"\n)\n\n// Default Constants\nconst (\n\tVersion                   = \"1.38.0\"\n\tDefaultURL                = \"nats://127.0.0.1:4222\"\n\tDefaultPort               = 4222\n\tDefaultMaxReconnect       = 60\n\tDefaultReconnectWait      = 2 * time.Second\n\tDefaultReconnectJitter    = 100 * time.Millisecond\n\tDefaultReconnectJitterTLS = time.Second\n\tDefaultTimeout            = 2 * time.Second\n\tDefaultPingInterval       = 2 * time.Minute\n\tDefaultMaxPingOut         = 2\n\tDefaultMaxChanLen         = 64 * 1024       // 64k\n\tDefaultReconnectBufSize   = 8 * 1024 * 1024 // 8MB\n\tRequestChanLen            = 8\n\tDefaultDrainTimeout       = 30 * time.Second\n\tDefaultFlusherTimeout     = time.Minute\n\tLangString                = \"go\"\n)\n\nconst (\n\t// STALE_CONNECTION is for detection and proper handling of stale connections.\n\tSTALE_CONNECTION = \"stale connection\"\n\n\t// PERMISSIONS_ERR is for when nats server subject authorization has failed.\n\tPERMISSIONS_ERR = \"permissions violation\"\n\n\t// AUTHORIZATION_ERR is for when nats server user authorization has failed.\n\tAUTHORIZATION_ERR = \"authorization violation\"\n\n\t// AUTHENTICATION_EXPIRED_ERR is for when nats server user authorization has expired.\n\tAUTHENTICATION_EXPIRED_ERR = \"user authentication expired\"\n\n\t// AUTHENTICATION_REVOKED_ERR is for when user authorization has been revoked.\n\tAUTHENTICATION_REVOKED_ERR = \"user authentication revoked\"\n\n\t// ACCOUNT_AUTHENTICATION_EXPIRED_ERR is for when nats server account authorization has expired.\n\tACCOUNT_AUTHENTICATION_EXPIRED_ERR = \"account authentication expired\"\n\n\t// MAX_CONNECTIONS_ERR is for when nats server denies the connection due to server max_connections limit\n\tMAX_CONNECTIONS_ERR = \"maximum connections exceeded\"\n\n\t// MAX_SUBSCRIPTIONS_ERR is for when nats server denies the connection due to server subscriptions limit\n\tMAX_SUBSCRIPTIONS_ERR = \"maximum subscriptions exceeded\"\n)\n\n// Errors\nvar (\n\tErrConnectionClosed            = errors.New(\"nats: connection closed\")\n\tErrConnectionDraining          = errors.New(\"nats: connection draining\")\n\tErrDrainTimeout                = errors.New(\"nats: draining connection timed out\")\n\tErrConnectionReconnecting      = errors.New(\"nats: connection reconnecting\")\n\tErrSecureConnRequired          = errors.New(\"nats: secure connection required\")\n\tErrSecureConnWanted            = errors.New(\"nats: secure connection not available\")\n\tErrBadSubscription             = errors.New(\"nats: invalid subscription\")\n\tErrTypeSubscription            = errors.New(\"nats: invalid subscription type\")\n\tErrBadSubject                  = errors.New(\"nats: invalid subject\")\n\tErrBadQueueName                = errors.New(\"nats: invalid queue name\")\n\tErrSlowConsumer                = errors.New(\"nats: slow consumer, messages dropped\")\n\tErrTimeout                     = errors.New(\"nats: timeout\")\n\tErrBadTimeout                  = errors.New(\"nats: timeout invalid\")\n\tErrAuthorization               = errors.New(\"nats: authorization violation\")\n\tErrAuthExpired                 = errors.New(\"nats: authentication expired\")\n\tErrAuthRevoked                 = errors.New(\"nats: authentication revoked\")\n\tErrPermissionViolation         = errors.New(\"nats: permissions violation\")\n\tErrAccountAuthExpired          = errors.New(\"nats: account authentication expired\")\n\tErrNoServers                   = errors.New(\"nats: no servers available for connection\")\n\tErrJsonParse                   = errors.New(\"nats: connect message, json parse error\")\n\tErrChanArg                     = errors.New(\"nats: argument needs to be a channel type\")\n\tErrMaxPayload                  = errors.New(\"nats: maximum payload exceeded\")\n\tErrMaxMessages                 = errors.New(\"nats: maximum messages delivered\")\n\tErrSyncSubRequired             = errors.New(\"nats: illegal call on an async subscription\")\n\tErrMultipleTLSConfigs          = errors.New(\"nats: multiple tls.Configs not allowed\")\n\tErrClientCertOrRootCAsRequired = errors.New(\"nats: at least one of certCB or rootCAsCB must be set\")\n\tErrNoInfoReceived              = errors.New(\"nats: protocol exception, INFO not received\")\n\tErrReconnectBufExceeded        = errors.New(\"nats: outbound buffer limit exceeded\")\n\tErrInvalidConnection           = errors.New(\"nats: invalid connection\")\n\tErrInvalidMsg                  = errors.New(\"nats: invalid message or message nil\")\n\tErrInvalidArg                  = errors.New(\"nats: invalid argument\")\n\tErrInvalidContext              = errors.New(\"nats: invalid context\")\n\tErrNoDeadlineContext           = errors.New(\"nats: context requires a deadline\")\n\tErrNoEchoNotSupported          = errors.New(\"nats: no echo option not supported by this server\")\n\tErrClientIDNotSupported        = errors.New(\"nats: client ID not supported by this server\")\n\tErrUserButNoSigCB              = errors.New(\"nats: user callback defined without a signature handler\")\n\tErrNkeyButNoSigCB              = errors.New(\"nats: nkey defined without a signature handler\")\n\tErrNoUserCB                    = errors.New(\"nats: user callback not defined\")\n\tErrNkeyAndUser                 = errors.New(\"nats: user callback and nkey defined\")\n\tErrNkeysNotSupported           = errors.New(\"nats: nkeys not supported by the server\")\n\tErrStaleConnection             = errors.New(\"nats: \" + STALE_CONNECTION)\n\tErrTokenAlreadySet             = errors.New(\"nats: token and token handler both set\")\n\tErrUserInfoAlreadySet          = errors.New(\"nats: cannot set user info callback and user/pass\")\n\tErrMsgNotBound                 = errors.New(\"nats: message is not bound to subscription/connection\")\n\tErrMsgNoReply                  = errors.New(\"nats: message does not have a reply\")\n\tErrClientIPNotSupported        = errors.New(\"nats: client IP not supported by this server\")\n\tErrDisconnected                = errors.New(\"nats: server is disconnected\")\n\tErrHeadersNotSupported         = errors.New(\"nats: headers not supported by this server\")\n\tErrBadHeaderMsg                = errors.New(\"nats: message could not decode headers\")\n\tErrNoResponders                = errors.New(\"nats: no responders available for request\")\n\tErrMaxConnectionsExceeded      = errors.New(\"nats: server maximum connections exceeded\")\n\tErrConnectionNotTLS            = errors.New(\"nats: connection is not tls\")\n\tErrMaxSubscriptionsExceeded    = errors.New(\"nats: server maximum subscriptions exceeded\")\n)\n\n// GetDefaultOptions returns default configuration options for the client.\nfunc GetDefaultOptions() Options {\n\treturn Options{\n\t\tAllowReconnect:     true,\n\t\tMaxReconnect:       DefaultMaxReconnect,\n\t\tReconnectWait:      DefaultReconnectWait,\n\t\tReconnectJitter:    DefaultReconnectJitter,\n\t\tReconnectJitterTLS: DefaultReconnectJitterTLS,\n\t\tTimeout:            DefaultTimeout,\n\t\tPingInterval:       DefaultPingInterval,\n\t\tMaxPingsOut:        DefaultMaxPingOut,\n\t\tSubChanLen:         DefaultMaxChanLen,\n\t\tReconnectBufSize:   DefaultReconnectBufSize,\n\t\tDrainTimeout:       DefaultDrainTimeout,\n\t\tFlusherTimeout:     DefaultFlusherTimeout,\n\t}\n}\n\n// Deprecated: Use GetDefaultOptions() instead.\n// DefaultOptions is not safe for use by multiple clients.\n// For details see #308.\nvar DefaultOptions = GetDefaultOptions()\n\n// Status represents the state of the connection.\ntype Status int\n\nconst (\n\tDISCONNECTED = Status(iota)\n\tCONNECTED\n\tCLOSED\n\tRECONNECTING\n\tCONNECTING\n\tDRAINING_SUBS\n\tDRAINING_PUBS\n)\n\nfunc (s Status) String() string {\n\tswitch s {\n\tcase DISCONNECTED:\n\t\treturn \"DISCONNECTED\"\n\tcase CONNECTED:\n\t\treturn \"CONNECTED\"\n\tcase CLOSED:\n\t\treturn \"CLOSED\"\n\tcase RECONNECTING:\n\t\treturn \"RECONNECTING\"\n\tcase CONNECTING:\n\t\treturn \"CONNECTING\"\n\tcase DRAINING_SUBS:\n\t\treturn \"DRAINING_SUBS\"\n\tcase DRAINING_PUBS:\n\t\treturn \"DRAINING_PUBS\"\n\t}\n\treturn \"unknown status\"\n}\n\n// ConnHandler is used for asynchronous events such as\n// disconnected and closed connections.\ntype ConnHandler func(*Conn)\n\n// ConnErrHandler is used to process asynchronous events like\n// disconnected connection with the error (if any).\ntype ConnErrHandler func(*Conn, error)\n\n// ErrHandler is used to process asynchronous errors encountered\n// while processing inbound messages.\ntype ErrHandler func(*Conn, *Subscription, error)\n\n// UserJWTHandler is used to fetch and return the account signed\n// JWT for this user.\ntype UserJWTHandler func() (string, error)\n\n// TLSCertHandler is used to fetch and return tls certificate.\ntype TLSCertHandler func() (tls.Certificate, error)\n\n// RootCAsHandler is used to fetch and return a set of root certificate\n// authorities that clients use when verifying server certificates.\ntype RootCAsHandler func() (*x509.CertPool, error)\n\n// SignatureHandler is used to sign a nonce from the server while\n// authenticating with nkeys. The user should sign the nonce and\n// return the raw signature. The client will base64 encode this to\n// send to the server.\ntype SignatureHandler func([]byte) ([]byte, error)\n\n// AuthTokenHandler is used to generate a new token.\ntype AuthTokenHandler func() string\n\n// UserInfoCB is used to pass the username and password when establishing connection.\ntype UserInfoCB func() (string, string)\n\n// ReconnectDelayHandler is used to get from the user the desired\n// delay the library should pause before attempting to reconnect\n// again. Note that this is invoked after the library tried the\n// whole list of URLs and failed to reconnect.\ntype ReconnectDelayHandler func(attempts int) time.Duration\n\n// asyncCB is used to preserve order for async callbacks.\ntype asyncCB struct {\n\tf    func()\n\tnext *asyncCB\n}\n\ntype asyncCallbacksHandler struct {\n\tmu   sync.Mutex\n\tcond *sync.Cond\n\thead *asyncCB\n\ttail *asyncCB\n}\n\n// Option is a function on the options for a connection.\ntype Option func(*Options) error\n\n// CustomDialer can be used to specify any dialer, not necessarily a\n// *net.Dialer.  A CustomDialer may also implement `SkipTLSHandshake() bool`\n// in order to skip the TLS handshake in case not required.\ntype CustomDialer interface {\n\tDial(network, address string) (net.Conn, error)\n}\n\ntype InProcessConnProvider interface {\n\tInProcessConn() (net.Conn, error)\n}\n\n// Options can be used to create a customized connection.\ntype Options struct {\n\n\t// Url represents a single NATS server url to which the client\n\t// will be connecting. If the Servers option is also set, it\n\t// then becomes the first server in the Servers array.\n\tUrl string\n\n\t// InProcessServer represents a NATS server running within the\n\t// same process. If this is set then we will attempt to connect\n\t// to the server directly rather than using external TCP conns.\n\tInProcessServer InProcessConnProvider\n\n\t// Servers is a configured set of servers which this client\n\t// will use when attempting to connect.\n\tServers []string\n\n\t// NoRandomize configures whether we will randomize the\n\t// server pool.\n\tNoRandomize bool\n\n\t// NoEcho configures whether the server will echo back messages\n\t// that are sent on this connection if we also have matching subscriptions.\n\t// Note this is supported on servers >= version 1.2. Proto 1 or greater.\n\tNoEcho bool\n\n\t// Name is an optional name label which will be sent to the server\n\t// on CONNECT to identify the client.\n\tName string\n\n\t// Verbose signals the server to send an OK ack for commands\n\t// successfully processed by the server.\n\tVerbose bool\n\n\t// Pedantic signals the server whether it should be doing further\n\t// validation of subjects.\n\tPedantic bool\n\n\t// Secure enables TLS secure connections that skip server\n\t// verification by default. NOT RECOMMENDED.\n\tSecure bool\n\n\t// TLSConfig is a custom TLS configuration to use for secure\n\t// transports.\n\tTLSConfig *tls.Config\n\n\t// TLSCertCB is used to fetch and return custom tls certificate.\n\tTLSCertCB TLSCertHandler\n\n\t// TLSHandshakeFirst is used to instruct the library perform\n\t// the TLS handshake right after the connect and before receiving\n\t// the INFO protocol from the server. If this option is enabled\n\t// but the server is not configured to perform the TLS handshake\n\t// first, the connection will fail.\n\tTLSHandshakeFirst bool\n\n\t// RootCAsCB is used to fetch and return a set of root certificate\n\t// authorities that clients use when verifying server certificates.\n\tRootCAsCB RootCAsHandler\n\n\t// AllowReconnect enables reconnection logic to be used when we\n\t// encounter a disconnect from the current server.\n\tAllowReconnect bool\n\n\t// MaxReconnect sets the number of reconnect attempts that will be\n\t// tried before giving up. If negative, then it will never give up\n\t// trying to reconnect.\n\t// Defaults to 60.\n\tMaxReconnect int\n\n\t// ReconnectWait sets the time to backoff after attempting a reconnect\n\t// to a server that we were already connected to previously.\n\t// Defaults to 2s.\n\tReconnectWait time.Duration\n\n\t// CustomReconnectDelayCB is invoked after the library tried every\n\t// URL in the server list and failed to reconnect. It passes to the\n\t// user the current number of attempts. This function returns the\n\t// amount of time the library will sleep before attempting to reconnect\n\t// again. It is strongly recommended that this value contains some\n\t// jitter to prevent all connections to attempt reconnecting at the same time.\n\tCustomReconnectDelayCB ReconnectDelayHandler\n\n\t// ReconnectJitter sets the upper bound for a random delay added to\n\t// ReconnectWait during a reconnect when no TLS is used.\n\t// Defaults to 100ms.\n\tReconnectJitter time.Duration\n\n\t// ReconnectJitterTLS sets the upper bound for a random delay added to\n\t// ReconnectWait during a reconnect when TLS is used.\n\t// Defaults to 1s.\n\tReconnectJitterTLS time.Duration\n\n\t// Timeout sets the timeout for a Dial operation on a connection.\n\t// Defaults to 2s.\n\tTimeout time.Duration\n\n\t// DrainTimeout sets the timeout for a Drain Operation to complete.\n\t// Defaults to 30s.\n\tDrainTimeout time.Duration\n\n\t// FlusherTimeout is the maximum time to wait for write operations\n\t// to the underlying connection to complete (including the flusher loop).\n\t// Defaults to 1m.\n\tFlusherTimeout time.Duration\n\n\t// PingInterval is the period at which the client will be sending ping\n\t// commands to the server, disabled if 0 or negative.\n\t// Defaults to 2m.\n\tPingInterval time.Duration\n\n\t// MaxPingsOut is the maximum number of pending ping commands that can\n\t// be awaiting a response before raising an ErrStaleConnection error.\n\t// Defaults to 2.\n\tMaxPingsOut int\n\n\t// ClosedCB sets the closed handler that is called when a client will\n\t// no longer be connected.\n\tClosedCB ConnHandler\n\n\t// DisconnectedCB sets the disconnected handler that is called\n\t// whenever the connection is disconnected.\n\t// Will not be called if DisconnectedErrCB is set\n\t// Deprecated. Use DisconnectedErrCB which passes error that caused\n\t// the disconnect event.\n\tDisconnectedCB ConnHandler\n\n\t// DisconnectedErrCB sets the disconnected error handler that is called\n\t// whenever the connection is disconnected.\n\t// Disconnected error could be nil, for instance when user explicitly closes the connection.\n\t// DisconnectedCB will not be called if DisconnectedErrCB is set\n\tDisconnectedErrCB ConnErrHandler\n\n\t// ConnectedCB sets the connected handler called when the initial connection\n\t// is established. It is not invoked on successful reconnects - for reconnections,\n\t// use ReconnectedCB. ConnectedCB can be used in conjunction with RetryOnFailedConnect\n\t// to detect whether the initial connect was successful.\n\tConnectedCB ConnHandler\n\n\t// ReconnectedCB sets the reconnected handler called whenever\n\t// the connection is successfully reconnected.\n\tReconnectedCB ConnHandler\n\n\t// DiscoveredServersCB sets the callback that is invoked whenever a new\n\t// server has joined the cluster.\n\tDiscoveredServersCB ConnHandler\n\n\t// AsyncErrorCB sets the async error handler (e.g. slow consumer errors)\n\tAsyncErrorCB ErrHandler\n\n\t// ReconnectBufSize is the size of the backing bufio during reconnect.\n\t// Once this has been exhausted publish operations will return an error.\n\t// Defaults to 8388608 bytes (8MB).\n\tReconnectBufSize int\n\n\t// SubChanLen is the size of the buffered channel used between the socket\n\t// Go routine and the message delivery for SyncSubscriptions.\n\t// NOTE: This does not affect AsyncSubscriptions which are\n\t// dictated by PendingLimits()\n\t// Defaults to 65536.\n\tSubChanLen int\n\n\t// UserJWT sets the callback handler that will fetch a user's JWT.\n\tUserJWT UserJWTHandler\n\n\t// Nkey sets the public nkey that will be used to authenticate\n\t// when connecting to the server. UserJWT and Nkey are mutually exclusive\n\t// and if defined, UserJWT will take precedence.\n\tNkey string\n\n\t// SignatureCB designates the function used to sign the nonce\n\t// presented from the server.\n\tSignatureCB SignatureHandler\n\n\t// User sets the username to be used when connecting to the server.\n\tUser string\n\n\t// Password sets the password to be used when connecting to a server.\n\tPassword string\n\n\t// UserInfo sets the callback handler that will fetch the username and password.\n\tUserInfo UserInfoCB\n\n\t// Token sets the token to be used when connecting to a server.\n\tToken string\n\n\t// TokenHandler designates the function used to generate the token to be used when connecting to a server.\n\tTokenHandler AuthTokenHandler\n\n\t// Dialer allows a custom net.Dialer when forming connections.\n\t// Deprecated: should use CustomDialer instead.\n\tDialer *net.Dialer\n\n\t// CustomDialer allows to specify a custom dialer (not necessarily\n\t// a *net.Dialer).\n\tCustomDialer CustomDialer\n\n\t// UseOldRequestStyle forces the old method of Requests that utilize\n\t// a new Inbox and a new Subscription for each request.\n\tUseOldRequestStyle bool\n\n\t// NoCallbacksAfterClientClose allows preventing the invocation of\n\t// callbacks after Close() is called. Client won't receive notifications\n\t// when Close is invoked by user code. Default is to invoke the callbacks.\n\tNoCallbacksAfterClientClose bool\n\n\t// LameDuckModeHandler sets the callback to invoke when the server notifies\n\t// the connection that it entered lame duck mode, that is, going to\n\t// gradually disconnect all its connections before shutting down. This is\n\t// often used in deployments when upgrading NATS Servers.\n\tLameDuckModeHandler ConnHandler\n\n\t// RetryOnFailedConnect sets the connection in reconnecting state right\n\t// away if it can't connect to a server in the initial set. The\n\t// MaxReconnect and ReconnectWait options are used for this process,\n\t// similarly to when an established connection is disconnected.\n\t// If a ReconnectHandler is set, it will be invoked on the first\n\t// successful reconnect attempt (if the initial connect fails),\n\t// and if a ClosedHandler is set, it will be invoked if\n\t// it fails to connect (after exhausting the MaxReconnect attempts).\n\tRetryOnFailedConnect bool\n\n\t// For websocket connections, indicates to the server that the connection\n\t// supports compression. If the server does too, then data will be compressed.\n\tCompression bool\n\n\t// For websocket connections, adds a path to connections url.\n\t// This is useful when connecting to NATS behind a proxy.\n\tProxyPath string\n\n\t// InboxPrefix allows the default _INBOX prefix to be customized\n\tInboxPrefix string\n\n\t// IgnoreAuthErrorAbort - if set to true, client opts out of the default connect behavior of aborting\n\t// subsequent reconnect attempts if server returns the same auth error twice (regardless of reconnect policy).\n\tIgnoreAuthErrorAbort bool\n\n\t// SkipHostLookup skips the DNS lookup for the server hostname.\n\tSkipHostLookup bool\n\n\t// PermissionErrOnSubscribe - if set to true, the client will return ErrPermissionViolation\n\t// from SubscribeSync if the server returns a permissions error for a subscription.\n\t// Defaults to false.\n\tPermissionErrOnSubscribe bool\n}\n\nconst (\n\t// Scratch storage for assembling protocol headers\n\tscratchSize = 512\n\n\t// The size of the bufio reader/writer on top of the socket.\n\tdefaultBufSize = 32768\n\n\t// The buffered size of the flush \"kick\" channel\n\tflushChanSize = 1\n\n\t// Default server pool size\n\tsrvPoolSize = 4\n\n\t// NUID size\n\tnuidSize = 22\n\n\t// Default ports used if none is specified in given URL(s)\n\tdefaultWSPortString  = \"80\"\n\tdefaultWSSPortString = \"443\"\n\tdefaultPortString    = \"4222\"\n)\n\n// A Conn represents a bare connection to a nats-server.\n// It can send and receive []byte payloads.\n// The connection is safe to use in multiple Go routines concurrently.\ntype Conn struct {\n\t// Keep all members for which we use atomic at the beginning of the\n\t// struct and make sure they are all 64bits (or use padding if necessary).\n\t// atomic.* functions crash on 32bit machines if operand is not aligned\n\t// at 64bit. See https://github.com/golang/go/issues/599\n\tStatistics\n\tmu sync.RWMutex\n\t// Opts holds the configuration of the Conn.\n\t// Modifying the configuration of a running Conn is a race.\n\tOpts          Options\n\twg            sync.WaitGroup\n\tsrvPool       []*srv\n\tcurrent       *srv\n\turls          map[string]struct{} // Keep track of all known URLs (used by processInfo)\n\tconn          net.Conn\n\tbw            *natsWriter\n\tbr            *natsReader\n\tfch           chan struct{}\n\tinfo          serverInfo\n\tssid          int64\n\tsubsMu        sync.RWMutex\n\tsubs          map[int64]*Subscription\n\tach           *asyncCallbacksHandler\n\tpongs         []chan struct{}\n\tscratch       [scratchSize]byte\n\tstatus        Status\n\tstatListeners map[Status][]chan Status\n\tinitc         bool // true if the connection is performing the initial connect\n\terr           error\n\tps            *parseState\n\tptmr          *time.Timer\n\tpout          int\n\tar            bool // abort reconnect\n\trqch          chan struct{}\n\tws            bool // true if a websocket connection\n\n\t// New style response handler\n\trespSub       string               // The wildcard subject\n\trespSubPrefix string               // the wildcard prefix including trailing .\n\trespSubLen    int                  // the length of the wildcard prefix excluding trailing .\n\trespMux       *Subscription        // A single response subscription\n\trespMap       map[string]chan *Msg // Request map for the response msg channels\n\trespRand      *rand.Rand           // Used for generating suffix\n\n\t// Msg filters for testing.\n\t// Protected by subsMu\n\tfilters map[string]msgFilter\n}\n\ntype natsReader struct {\n\tr   io.Reader\n\tbuf []byte\n\toff int\n\tn   int\n}\n\ntype natsWriter struct {\n\tw       io.Writer\n\tbufs    []byte\n\tlimit   int\n\tpending *bytes.Buffer\n\tplimit  int\n}\n\n// Subscription represents interest in a given subject.\ntype Subscription struct {\n\tmu  sync.Mutex\n\tsid int64\n\n\t// Subject that represents this subscription. This can be different\n\t// than the received subject inside a Msg if this is a wildcard.\n\tSubject string\n\n\t// Optional queue group name. If present, all subscriptions with the\n\t// same name will form a distributed queue, and each message will\n\t// only be processed by one member of the group.\n\tQueue string\n\n\t// For holding information about a JetStream consumer.\n\tjsi *jsSub\n\n\tdelivered      uint64\n\tmax            uint64\n\tconn           *Conn\n\tmcb            MsgHandler\n\tmch            chan *Msg\n\terrCh          chan (error)\n\tclosed         bool\n\tsc             bool\n\tconnClosed     bool\n\tdraining       bool\n\tstatus         SubStatus\n\tstatListeners  map[chan SubStatus][]SubStatus\n\tpermissionsErr error\n\n\t// Type of Subscription\n\ttyp SubscriptionType\n\n\t// Async linked list\n\tpHead *Msg\n\tpTail *Msg\n\tpCond *sync.Cond\n\tpDone func(subject string)\n\n\t// Pending stats, async subscriptions, high-speed etc.\n\tpMsgs       int\n\tpBytes      int\n\tpMsgsMax    int\n\tpBytesMax   int\n\tpMsgsLimit  int\n\tpBytesLimit int\n\tdropped     int\n}\n\n// Status represents the state of the connection.\ntype SubStatus int\n\nconst (\n\tSubscriptionActive = SubStatus(iota)\n\tSubscriptionDraining\n\tSubscriptionClosed\n\tSubscriptionSlowConsumer\n)\n\nfunc (s SubStatus) String() string {\n\tswitch s {\n\tcase SubscriptionActive:\n\t\treturn \"Active\"\n\tcase SubscriptionDraining:\n\t\treturn \"Draining\"\n\tcase SubscriptionClosed:\n\t\treturn \"Closed\"\n\tcase SubscriptionSlowConsumer:\n\t\treturn \"SlowConsumer\"\n\t}\n\treturn \"unknown status\"\n}\n\n// Msg represents a message delivered by NATS. This structure is used\n// by Subscribers and PublishMsg().\n//\n// # Types of Acknowledgements\n//\n// In case using JetStream, there are multiple ways to ack a Msg:\n//\n//\t// Acknowledgement that a message has been processed.\n//\tmsg.Ack()\n//\n//\t// Negatively acknowledges a message.\n//\tmsg.Nak()\n//\n//\t// Terminate a message so that it is not redelivered further.\n//\tmsg.Term()\n//\n//\t// Signal the server that the message is being worked on and reset redelivery timer.\n//\tmsg.InProgress()\ntype Msg struct {\n\tSubject string\n\tReply   string\n\tHeader  Header\n\tData    []byte\n\tSub     *Subscription\n\t// Internal\n\tnext    *Msg\n\twsz     int\n\tbarrier *barrierInfo\n\tackd    uint32\n}\n\n// Compares two msgs, ignores sub but checks all other public fields.\nfunc (m *Msg) Equal(msg *Msg) bool {\n\tif m == msg {\n\t\treturn true\n\t}\n\tif m == nil || msg == nil {\n\t\treturn false\n\t}\n\tif m.Subject != msg.Subject || m.Reply != msg.Reply {\n\t\treturn false\n\t}\n\tif !bytes.Equal(m.Data, msg.Data) {\n\t\treturn false\n\t}\n\tif len(m.Header) != len(msg.Header) {\n\t\treturn false\n\t}\n\tfor k, v := range m.Header {\n\t\tval, ok := msg.Header[k]\n\t\tif !ok || len(v) != len(val) {\n\t\t\treturn false\n\t\t}\n\t\tfor i, hdr := range v {\n\t\t\tif hdr != val[i] {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\n// Size returns a message size in bytes.\nfunc (m *Msg) Size() int {\n\tif m.wsz != 0 {\n\t\treturn m.wsz\n\t}\n\thdr, _ := m.headerBytes()\n\treturn len(m.Subject) + len(m.Reply) + len(hdr) + len(m.Data)\n}\n\nfunc (m *Msg) headerBytes() ([]byte, error) {\n\tvar hdr []byte\n\tif len(m.Header) == 0 {\n\t\treturn hdr, nil\n\t}\n\n\tvar b bytes.Buffer\n\t_, err := b.WriteString(hdrLine)\n\tif err != nil {\n\t\treturn nil, ErrBadHeaderMsg\n\t}\n\n\terr = http.Header(m.Header).Write(&b)\n\tif err != nil {\n\t\treturn nil, ErrBadHeaderMsg\n\t}\n\n\t_, err = b.WriteString(crlf)\n\tif err != nil {\n\t\treturn nil, ErrBadHeaderMsg\n\t}\n\n\treturn b.Bytes(), nil\n}\n\ntype barrierInfo struct {\n\trefs int64\n\tf    func()\n}\n\n// Tracks various stats received and sent on this connection,\n// including counts for messages and bytes.\ntype Statistics struct {\n\tInMsgs     uint64\n\tOutMsgs    uint64\n\tInBytes    uint64\n\tOutBytes   uint64\n\tReconnects uint64\n}\n\n// Tracks individual backend servers.\ntype srv struct {\n\turl        *url.URL\n\tdidConnect bool\n\treconnects int\n\tlastErr    error\n\tisImplicit bool\n\ttlsName    string\n}\n\n// The INFO block received from the server.\ntype serverInfo struct {\n\tID           string   `json:\"server_id\"`\n\tName         string   `json:\"server_name\"`\n\tProto        int      `json:\"proto\"`\n\tVersion      string   `json:\"version\"`\n\tHost         string   `json:\"host\"`\n\tPort         int      `json:\"port\"`\n\tHeaders      bool     `json:\"headers\"`\n\tAuthRequired bool     `json:\"auth_required,omitempty\"`\n\tTLSRequired  bool     `json:\"tls_required,omitempty\"`\n\tTLSAvailable bool     `json:\"tls_available,omitempty\"`\n\tMaxPayload   int64    `json:\"max_payload\"`\n\tCID          uint64   `json:\"client_id,omitempty\"`\n\tClientIP     string   `json:\"client_ip,omitempty\"`\n\tNonce        string   `json:\"nonce,omitempty\"`\n\tCluster      string   `json:\"cluster,omitempty\"`\n\tConnectURLs  []string `json:\"connect_urls,omitempty\"`\n\tLameDuckMode bool     `json:\"ldm,omitempty\"`\n}\n\nconst (\n\t// clientProtoZero is the original client protocol from 2009.\n\t// http://nats.io/documentation/internals/nats-protocol/\n\t/* clientProtoZero */ _ = iota\n\t// clientProtoInfo signals a client can receive more then the original INFO block.\n\t// This can be used to update clients on other cluster members, etc.\n\tclientProtoInfo\n)\n\ntype connectInfo struct {\n\tVerbose      bool   `json:\"verbose\"`\n\tPedantic     bool   `json:\"pedantic\"`\n\tUserJWT      string `json:\"jwt,omitempty\"`\n\tNkey         string `json:\"nkey,omitempty\"`\n\tSignature    string `json:\"sig,omitempty\"`\n\tUser         string `json:\"user,omitempty\"`\n\tPass         string `json:\"pass,omitempty\"`\n\tToken        string `json:\"auth_token,omitempty\"`\n\tTLS          bool   `json:\"tls_required\"`\n\tName         string `json:\"name\"`\n\tLang         string `json:\"lang\"`\n\tVersion      string `json:\"version\"`\n\tProtocol     int    `json:\"protocol\"`\n\tEcho         bool   `json:\"echo\"`\n\tHeaders      bool   `json:\"headers\"`\n\tNoResponders bool   `json:\"no_responders\"`\n}\n\n// MsgHandler is a callback function that processes messages delivered to\n// asynchronous subscribers.\ntype MsgHandler func(msg *Msg)\n\n// Connect will attempt to connect to the NATS system.\n// The url can contain username/password semantics. e.g. nats://derek:pass@localhost:4222\n// Comma separated arrays are also supported, e.g. urlA, urlB.\n// Options start with the defaults but can be overridden.\n// To connect to a NATS Server's websocket port, use the `ws` or `wss` scheme, such as\n// `ws://localhost:8080`. Note that websocket schemes cannot be mixed with others (nats/tls).\nfunc Connect(url string, options ...Option) (*Conn, error) {\n\topts := GetDefaultOptions()\n\topts.Servers = processUrlString(url)\n\tfor _, opt := range options {\n\t\tif opt != nil {\n\t\t\tif err := opt(&opts); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\treturn opts.Connect()\n}\n\n// Options that can be passed to Connect.\n\n// Name is an Option to set the client name.\nfunc Name(name string) Option {\n\treturn func(o *Options) error {\n\t\to.Name = name\n\t\treturn nil\n\t}\n}\n\n// InProcessServer is an Option that will try to establish a direction to a NATS server\n// running within the process instead of dialing via TCP.\nfunc InProcessServer(server InProcessConnProvider) Option {\n\treturn func(o *Options) error {\n\t\to.InProcessServer = server\n\t\treturn nil\n\t}\n}\n\n// Secure is an Option to enable TLS secure connections that skip server verification by default.\n// Pass a TLS Configuration for proper TLS.\n// A TLS Configuration using InsecureSkipVerify should NOT be used in a production setting.\nfunc Secure(tls ...*tls.Config) Option {\n\treturn func(o *Options) error {\n\t\to.Secure = true\n\t\t// Use of variadic just simplifies testing scenarios. We only take the first one.\n\t\tif len(tls) > 1 {\n\t\t\treturn ErrMultipleTLSConfigs\n\t\t}\n\t\tif len(tls) == 1 {\n\t\t\to.TLSConfig = tls[0]\n\t\t}\n\t\treturn nil\n\t}\n}\n\n// ClientTLSConfig is an Option to set the TLS configuration for secure\n// connections. It can be used to e.g. set TLS config with cert and root CAs\n// from memory. For simple use case of loading cert and CAs from file,\n// ClientCert and RootCAs options are more convenient.\n// If Secure is not already set this will set it as well.\nfunc ClientTLSConfig(certCB TLSCertHandler, rootCAsCB RootCAsHandler) Option {\n\treturn func(o *Options) error {\n\t\to.Secure = true\n\n\t\tif certCB == nil && rootCAsCB == nil {\n\t\t\treturn ErrClientCertOrRootCAsRequired\n\t\t}\n\n\t\t// Smoke test the callbacks to fail early\n\t\t// if they are not valid.\n\t\tif certCB != nil {\n\t\t\tif _, err := certCB(); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif rootCAsCB != nil {\n\t\t\tif _, err := rootCAsCB(); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif o.TLSConfig == nil {\n\t\t\to.TLSConfig = &tls.Config{MinVersion: tls.VersionTLS12}\n\t\t}\n\t\to.TLSCertCB = certCB\n\t\to.RootCAsCB = rootCAsCB\n\t\treturn nil\n\t}\n}\n\n// RootCAs is a helper option to provide the RootCAs pool from a list of filenames.\n// If Secure is not already set this will set it as well.\nfunc RootCAs(file ...string) Option {\n\treturn func(o *Options) error {\n\t\trootCAsCB := func() (*x509.CertPool, error) {\n\t\t\tpool := x509.NewCertPool()\n\t\t\tfor _, f := range file {\n\t\t\t\trootPEM, err := os.ReadFile(f)\n\t\t\t\tif err != nil || rootPEM == nil {\n\t\t\t\t\treturn nil, fmt.Errorf(\"nats: error loading or parsing rootCA file: %w\", err)\n\t\t\t\t}\n\t\t\t\tok := pool.AppendCertsFromPEM(rootPEM)\n\t\t\t\tif !ok {\n\t\t\t\t\treturn nil, fmt.Errorf(\"nats: failed to parse root certificate from %q\", f)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn pool, nil\n\t\t}\n\t\tif o.TLSConfig == nil {\n\t\t\to.TLSConfig = &tls.Config{MinVersion: tls.VersionTLS12}\n\t\t}\n\t\tif _, err := rootCAsCB(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\to.RootCAsCB = rootCAsCB\n\t\to.Secure = true\n\t\treturn nil\n\t}\n}\n\n// ClientCert is a helper option to provide the client certificate from a file.\n// If Secure is not already set this will set it as well.\nfunc ClientCert(certFile, keyFile string) Option {\n\treturn func(o *Options) error {\n\t\ttlsCertCB := func() (tls.Certificate, error) {\n\t\t\tcert, err := tls.LoadX509KeyPair(certFile, keyFile)\n\t\t\tif err != nil {\n\t\t\t\treturn tls.Certificate{}, fmt.Errorf(\"nats: error loading client certificate: %w\", err)\n\t\t\t}\n\t\t\tcert.Leaf, err = x509.ParseCertificate(cert.Certificate[0])\n\t\t\tif err != nil {\n\t\t\t\treturn tls.Certificate{}, fmt.Errorf(\"nats: error parsing client certificate: %w\", err)\n\t\t\t}\n\t\t\treturn cert, nil\n\t\t}\n\t\tif o.TLSConfig == nil {\n\t\t\to.TLSConfig = &tls.Config{MinVersion: tls.VersionTLS12}\n\t\t}\n\t\tif _, err := tlsCertCB(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\to.TLSCertCB = tlsCertCB\n\t\to.Secure = true\n\t\treturn nil\n\t}\n}\n\n// NoReconnect is an Option to turn off reconnect behavior.\nfunc NoReconnect() Option {\n\treturn func(o *Options) error {\n\t\to.AllowReconnect = false\n\t\treturn nil\n\t}\n}\n\n// DontRandomize is an Option to turn off randomizing the server pool.\nfunc DontRandomize() Option {\n\treturn func(o *Options) error {\n\t\to.NoRandomize = true\n\t\treturn nil\n\t}\n}\n\n// NoEcho is an Option to turn off messages echoing back from a server.\n// Note this is supported on servers >= version 1.2. Proto 1 or greater.\nfunc NoEcho() Option {\n\treturn func(o *Options) error {\n\t\to.NoEcho = true\n\t\treturn nil\n\t}\n}\n\n// ReconnectWait is an Option to set the wait time between reconnect attempts.\n// Defaults to 2s.\nfunc ReconnectWait(t time.Duration) Option {\n\treturn func(o *Options) error {\n\t\to.ReconnectWait = t\n\t\treturn nil\n\t}\n}\n\n// MaxReconnects is an Option to set the maximum number of reconnect attempts.\n// If negative, it will never stop trying to reconnect.\n// Defaults to 60.\nfunc MaxReconnects(max int) Option {\n\treturn func(o *Options) error {\n\t\to.MaxReconnect = max\n\t\treturn nil\n\t}\n}\n\n// ReconnectJitter is an Option to set the upper bound of a random delay added ReconnectWait.\n// Defaults to 100ms and 1s, respectively.\nfunc ReconnectJitter(jitter, jitterForTLS time.Duration) Option {\n\treturn func(o *Options) error {\n\t\to.ReconnectJitter = jitter\n\t\to.ReconnectJitterTLS = jitterForTLS\n\t\treturn nil\n\t}\n}\n\n// CustomReconnectDelay is an Option to set the CustomReconnectDelayCB option.\n// See CustomReconnectDelayCB Option for more details.\nfunc CustomReconnectDelay(cb ReconnectDelayHandler) Option {\n\treturn func(o *Options) error {\n\t\to.CustomReconnectDelayCB = cb\n\t\treturn nil\n\t}\n}\n\n// PingInterval is an Option to set the period for client ping commands.\n// Defaults to 2m.\nfunc PingInterval(t time.Duration) Option {\n\treturn func(o *Options) error {\n\t\to.PingInterval = t\n\t\treturn nil\n\t}\n}\n\n// MaxPingsOutstanding is an Option to set the maximum number of ping requests\n// that can go unanswered by the server before closing the connection.\n// Defaults to 2.\nfunc MaxPingsOutstanding(max int) Option {\n\treturn func(o *Options) error {\n\t\to.MaxPingsOut = max\n\t\treturn nil\n\t}\n}\n\n// ReconnectBufSize sets the buffer size of messages kept while busy reconnecting.\n// Defaults to 8388608 bytes (8MB).  It can be disabled by setting it to -1.\nfunc ReconnectBufSize(size int) Option {\n\treturn func(o *Options) error {\n\t\to.ReconnectBufSize = size\n\t\treturn nil\n\t}\n}\n\n// Timeout is an Option to set the timeout for Dial on a connection.\n// Defaults to 2s.\nfunc Timeout(t time.Duration) Option {\n\treturn func(o *Options) error {\n\t\to.Timeout = t\n\t\treturn nil\n\t}\n}\n\n// FlusherTimeout is an Option to set the write (and flush) timeout on a connection.\nfunc FlusherTimeout(t time.Duration) Option {\n\treturn func(o *Options) error {\n\t\to.FlusherTimeout = t\n\t\treturn nil\n\t}\n}\n\n// DrainTimeout is an Option to set the timeout for draining a connection.\n// Defaults to 30s.\nfunc DrainTimeout(t time.Duration) Option {\n\treturn func(o *Options) error {\n\t\to.DrainTimeout = t\n\t\treturn nil\n\t}\n}\n\n// DisconnectErrHandler is an Option to set the disconnected error handler.\nfunc DisconnectErrHandler(cb ConnErrHandler) Option {\n\treturn func(o *Options) error {\n\t\to.DisconnectedErrCB = cb\n\t\treturn nil\n\t}\n}\n\n// DisconnectHandler is an Option to set the disconnected handler.\n// Deprecated: Use DisconnectErrHandler.\nfunc DisconnectHandler(cb ConnHandler) Option {\n\treturn func(o *Options) error {\n\t\to.DisconnectedCB = cb\n\t\treturn nil\n\t}\n}\n\n// ConnectHandler is an Option to set the connected handler.\nfunc ConnectHandler(cb ConnHandler) Option {\n\treturn func(o *Options) error {\n\t\to.ConnectedCB = cb\n\t\treturn nil\n\t}\n}\n\n// ReconnectHandler is an Option to set the reconnected handler.\nfunc ReconnectHandler(cb ConnHandler) Option {\n\treturn func(o *Options) error {\n\t\to.ReconnectedCB = cb\n\t\treturn nil\n\t}\n}\n\n// ClosedHandler is an Option to set the closed handler.\nfunc ClosedHandler(cb ConnHandler) Option {\n\treturn func(o *Options) error {\n\t\to.ClosedCB = cb\n\t\treturn nil\n\t}\n}\n\n// DiscoveredServersHandler is an Option to set the new servers handler.\nfunc DiscoveredServersHandler(cb ConnHandler) Option {\n\treturn func(o *Options) error {\n\t\to.DiscoveredServersCB = cb\n\t\treturn nil\n\t}\n}\n\n// ErrorHandler is an Option to set the async error handler.\nfunc ErrorHandler(cb ErrHandler) Option {\n\treturn func(o *Options) error {\n\t\to.AsyncErrorCB = cb\n\t\treturn nil\n\t}\n}\n\n// UserInfo is an Option to set the username and password to\n// use when not included directly in the URLs.\nfunc UserInfo(user, password string) Option {\n\treturn func(o *Options) error {\n\t\to.User = user\n\t\to.Password = password\n\t\treturn nil\n\t}\n}\n\nfunc UserInfoHandler(cb UserInfoCB) Option {\n\treturn func(o *Options) error {\n\t\to.UserInfo = cb\n\t\treturn nil\n\t}\n}\n\n// Token is an Option to set the token to use\n// when a token is not included directly in the URLs\n// and when a token handler is not provided.\nfunc Token(token string) Option {\n\treturn func(o *Options) error {\n\t\tif o.TokenHandler != nil {\n\t\t\treturn ErrTokenAlreadySet\n\t\t}\n\t\to.Token = token\n\t\treturn nil\n\t}\n}\n\n// TokenHandler is an Option to set the token handler to use\n// when a token is not included directly in the URLs\n// and when a token is not set.\nfunc TokenHandler(cb AuthTokenHandler) Option {\n\treturn func(o *Options) error {\n\t\tif o.Token != \"\" {\n\t\t\treturn ErrTokenAlreadySet\n\t\t}\n\t\to.TokenHandler = cb\n\t\treturn nil\n\t}\n}\n\n// UserCredentials is a convenience function that takes a filename\n// for a user's JWT and a filename for the user's private Nkey seed.\nfunc UserCredentials(userOrChainedFile string, seedFiles ...string) Option {\n\tuserCB := func() (string, error) {\n\t\treturn userFromFile(userOrChainedFile)\n\t}\n\tvar keyFile string\n\tif len(seedFiles) > 0 {\n\t\tkeyFile = seedFiles[0]\n\t} else {\n\t\tkeyFile = userOrChainedFile\n\t}\n\tsigCB := func(nonce []byte) ([]byte, error) {\n\t\treturn sigHandler(nonce, keyFile)\n\t}\n\treturn UserJWT(userCB, sigCB)\n}\n\n// UserJWTAndSeed is a convenience function that takes the JWT and seed\n// values as strings.\nfunc UserJWTAndSeed(jwt string, seed string) Option {\n\tuserCB := func() (string, error) {\n\t\treturn jwt, nil\n\t}\n\n\tsigCB := func(nonce []byte) ([]byte, error) {\n\t\tkp, err := nkeys.FromSeed([]byte(seed))\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to extract key pair from seed: %w\", err)\n\t\t}\n\t\t// Wipe our key on exit.\n\t\tdefer kp.Wipe()\n\n\t\tsig, _ := kp.Sign(nonce)\n\t\treturn sig, nil\n\t}\n\n\treturn UserJWT(userCB, sigCB)\n}\n\n// UserJWT will set the callbacks to retrieve the user's JWT and\n// the signature callback to sign the server nonce. This an the Nkey\n// option are mutually exclusive.\nfunc UserJWT(userCB UserJWTHandler, sigCB SignatureHandler) Option {\n\treturn func(o *Options) error {\n\t\tif userCB == nil {\n\t\t\treturn ErrNoUserCB\n\t\t}\n\t\tif sigCB == nil {\n\t\t\treturn ErrUserButNoSigCB\n\t\t}\n\t\t// Smoke test the user callback to ensure it is setup properly\n\t\t// when processing options.\n\t\tif _, err := userCB(); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\to.UserJWT = userCB\n\t\to.SignatureCB = sigCB\n\t\treturn nil\n\t}\n}\n\n// Nkey will set the public Nkey and the signature callback to\n// sign the server nonce.\nfunc Nkey(pubKey string, sigCB SignatureHandler) Option {\n\treturn func(o *Options) error {\n\t\to.Nkey = pubKey\n\t\to.SignatureCB = sigCB\n\t\tif pubKey != \"\" && sigCB == nil {\n\t\t\treturn ErrNkeyButNoSigCB\n\t\t}\n\t\treturn nil\n\t}\n}\n\n// SyncQueueLen will set the maximum queue len for the internal\n// channel used for SubscribeSync().\n// Defaults to 65536.\nfunc SyncQueueLen(max int) Option {\n\treturn func(o *Options) error {\n\t\to.SubChanLen = max\n\t\treturn nil\n\t}\n}\n\n// Dialer is an Option to set the dialer which will be used when\n// attempting to establish a connection.\n// Deprecated: Should use CustomDialer instead.\nfunc Dialer(dialer *net.Dialer) Option {\n\treturn func(o *Options) error {\n\t\to.Dialer = dialer\n\t\treturn nil\n\t}\n}\n\n// SetCustomDialer is an Option to set a custom dialer which will be\n// used when attempting to establish a connection. If both Dialer\n// and CustomDialer are specified, CustomDialer takes precedence.\nfunc SetCustomDialer(dialer CustomDialer) Option {\n\treturn func(o *Options) error {\n\t\to.CustomDialer = dialer\n\t\treturn nil\n\t}\n}\n\n// UseOldRequestStyle is an Option to force usage of the old Request style.\nfunc UseOldRequestStyle() Option {\n\treturn func(o *Options) error {\n\t\to.UseOldRequestStyle = true\n\t\treturn nil\n\t}\n}\n\n// NoCallbacksAfterClientClose is an Option to disable callbacks when user code\n// calls Close(). If close is initiated by any other condition, callbacks\n// if any will be invoked.\nfunc NoCallbacksAfterClientClose() Option {\n\treturn func(o *Options) error {\n\t\to.NoCallbacksAfterClientClose = true\n\t\treturn nil\n\t}\n}\n\n// LameDuckModeHandler sets the callback to invoke when the server notifies\n// the connection that it entered lame duck mode, that is, going to\n// gradually disconnect all its connections before shutting down. This is\n// often used in deployments when upgrading NATS Servers.\nfunc LameDuckModeHandler(cb ConnHandler) Option {\n\treturn func(o *Options) error {\n\t\to.LameDuckModeHandler = cb\n\t\treturn nil\n\t}\n}\n\n// RetryOnFailedConnect sets the connection in reconnecting state right away\n// if it can't connect to a server in the initial set.\n// See RetryOnFailedConnect option for more details.\nfunc RetryOnFailedConnect(retry bool) Option {\n\treturn func(o *Options) error {\n\t\to.RetryOnFailedConnect = retry\n\t\treturn nil\n\t}\n}\n\n// Compression is an Option to indicate if this connection supports\n// compression. Currently only supported for Websocket connections.\nfunc Compression(enabled bool) Option {\n\treturn func(o *Options) error {\n\t\to.Compression = enabled\n\t\treturn nil\n\t}\n}\n\n// ProxyPath is an option for websocket connections that adds a path to connections url.\n// This is useful when connecting to NATS behind a proxy.\nfunc ProxyPath(path string) Option {\n\treturn func(o *Options) error {\n\t\to.ProxyPath = path\n\t\treturn nil\n\t}\n}\n\n// CustomInboxPrefix configures the request + reply inbox prefix\nfunc CustomInboxPrefix(p string) Option {\n\treturn func(o *Options) error {\n\t\tif p == \"\" || strings.Contains(p, \">\") || strings.Contains(p, \"*\") || strings.HasSuffix(p, \".\") {\n\t\t\treturn errors.New(\"nats: invalid custom prefix\")\n\t\t}\n\t\to.InboxPrefix = p\n\t\treturn nil\n\t}\n}\n\n// IgnoreAuthErrorAbort opts out of the default connect behavior of aborting\n// subsequent reconnect attempts if server returns the same auth error twice.\nfunc IgnoreAuthErrorAbort() Option {\n\treturn func(o *Options) error {\n\t\to.IgnoreAuthErrorAbort = true\n\t\treturn nil\n\t}\n}\n\n// SkipHostLookup is an Option to skip the host lookup when connecting to a server.\nfunc SkipHostLookup() Option {\n\treturn func(o *Options) error {\n\t\to.SkipHostLookup = true\n\t\treturn nil\n\t}\n}\n\nfunc PermissionErrOnSubscribe(enabled bool) Option {\n\treturn func(o *Options) error {\n\t\to.PermissionErrOnSubscribe = enabled\n\t\treturn nil\n\t}\n}\n\n// TLSHandshakeFirst is an Option to perform the TLS handshake first, that is\n// before receiving the INFO protocol. This requires the server to also be\n// configured with such option, otherwise the connection will fail.\nfunc TLSHandshakeFirst() Option {\n\treturn func(o *Options) error {\n\t\to.TLSHandshakeFirst = true\n\t\to.Secure = true\n\t\treturn nil\n\t}\n}\n\n// Handler processing\n\n// SetDisconnectHandler will set the disconnect event handler.\n// Deprecated: Use SetDisconnectErrHandler\nfunc (nc *Conn) SetDisconnectHandler(dcb ConnHandler) {\n\tif nc == nil {\n\t\treturn\n\t}\n\tnc.mu.Lock()\n\tdefer nc.mu.Unlock()\n\tnc.Opts.DisconnectedCB = dcb\n}\n\n// SetDisconnectErrHandler will set the disconnect event handler.\nfunc (nc *Conn) SetDisconnectErrHandler(dcb ConnErrHandler) {\n\tif nc == nil {\n\t\treturn\n\t}\n\tnc.mu.Lock()\n\tdefer nc.mu.Unlock()\n\tnc.Opts.DisconnectedErrCB = dcb\n}\n\n// DisconnectErrHandler will return the disconnect event handler.\nfunc (nc *Conn) DisconnectErrHandler() ConnErrHandler {\n\tif nc == nil {\n\t\treturn nil\n\t}\n\tnc.mu.Lock()\n\tdefer nc.mu.Unlock()\n\treturn nc.Opts.DisconnectedErrCB\n}\n\n// SetReconnectHandler will set the reconnect event handler.\nfunc (nc *Conn) SetReconnectHandler(rcb ConnHandler) {\n\tif nc == nil {\n\t\treturn\n\t}\n\tnc.mu.Lock()\n\tdefer nc.mu.Unlock()\n\tnc.Opts.ReconnectedCB = rcb\n}\n\n// ReconnectHandler will return the reconnect event handler.\nfunc (nc *Conn) ReconnectHandler() ConnHandler {\n\tif nc == nil {\n\t\treturn nil\n\t}\n\tnc.mu.Lock()\n\tdefer nc.mu.Unlock()\n\treturn nc.Opts.ReconnectedCB\n}\n\n// SetDiscoveredServersHandler will set the discovered servers handler.\nfunc (nc *Conn) SetDiscoveredServersHandler(dscb ConnHandler) {\n\tif nc == nil {\n\t\treturn\n\t}\n\tnc.mu.Lock()\n\tdefer nc.mu.Unlock()\n\tnc.Opts.DiscoveredServersCB = dscb\n}\n\n// DiscoveredServersHandler will return the discovered servers handler.\nfunc (nc *Conn) DiscoveredServersHandler() ConnHandler {\n\tif nc == nil {\n\t\treturn nil\n\t}\n\tnc.mu.Lock()\n\tdefer nc.mu.Unlock()\n\treturn nc.Opts.DiscoveredServersCB\n}\n\n// SetClosedHandler will set the closed event handler.\nfunc (nc *Conn) SetClosedHandler(cb ConnHandler) {\n\tif nc == nil {\n\t\treturn\n\t}\n\tnc.mu.Lock()\n\tdefer nc.mu.Unlock()\n\tnc.Opts.ClosedCB = cb\n}\n\n// ClosedHandler will return the closed event handler.\nfunc (nc *Conn) ClosedHandler() ConnHandler {\n\tif nc == nil {\n\t\treturn nil\n\t}\n\tnc.mu.Lock()\n\tdefer nc.mu.Unlock()\n\treturn nc.Opts.ClosedCB\n}\n\n// SetErrorHandler will set the async error handler.\nfunc (nc *Conn) SetErrorHandler(cb ErrHandler) {\n\tif nc == nil {\n\t\treturn\n\t}\n\tnc.mu.Lock()\n\tdefer nc.mu.Unlock()\n\tnc.Opts.AsyncErrorCB = cb\n}\n\n// ErrorHandler will return the async error handler.\nfunc (nc *Conn) ErrorHandler() ErrHandler {\n\tif nc == nil {\n\t\treturn nil\n\t}\n\tnc.mu.Lock()\n\tdefer nc.mu.Unlock()\n\treturn nc.Opts.AsyncErrorCB\n}\n\n// Process the url string argument to Connect.\n// Return an array of urls, even if only one.\nfunc processUrlString(url string) []string {\n\turls := strings.Split(url, \",\")\n\tvar j int\n\tfor _, s := range urls {\n\t\tu := strings.TrimSuffix(strings.TrimSpace(s), \"/\")\n\t\tif len(u) > 0 {\n\t\t\turls[j] = u\n\t\t\tj++\n\t\t}\n\t}\n\treturn urls[:j]\n}\n\n// Connect will attempt to connect to a NATS server with multiple options.\nfunc (o Options) Connect() (*Conn, error) {\n\tnc := &Conn{Opts: o}\n\n\t// Some default options processing.\n\tif nc.Opts.MaxPingsOut == 0 {\n\t\tnc.Opts.MaxPingsOut = DefaultMaxPingOut\n\t}\n\t// Allow old default for channel length to work correctly.\n\tif nc.Opts.SubChanLen == 0 {\n\t\tnc.Opts.SubChanLen = DefaultMaxChanLen\n\t}\n\t// Default ReconnectBufSize\n\tif nc.Opts.ReconnectBufSize == 0 {\n\t\tnc.Opts.ReconnectBufSize = DefaultReconnectBufSize\n\t}\n\t// Ensure that Timeout is not 0\n\tif nc.Opts.Timeout == 0 {\n\t\tnc.Opts.Timeout = DefaultTimeout\n\t}\n\n\t// Check first for user jwt callback being defined and nkey.\n\tif nc.Opts.UserJWT != nil && nc.Opts.Nkey != \"\" {\n\t\treturn nil, ErrNkeyAndUser\n\t}\n\n\t// Check if we have an nkey but no signature callback defined.\n\tif nc.Opts.Nkey != \"\" && nc.Opts.SignatureCB == nil {\n\t\treturn nil, ErrNkeyButNoSigCB\n\t}\n\n\t// Allow custom Dialer for connecting using a timeout by default\n\tif nc.Opts.Dialer == nil {\n\t\tnc.Opts.Dialer = &net.Dialer{\n\t\t\tTimeout: nc.Opts.Timeout,\n\t\t}\n\t}\n\n\t// If the TLSHandshakeFirst option is specified, make sure that\n\t// the Secure boolean is true.\n\tif nc.Opts.TLSHandshakeFirst {\n\t\tnc.Opts.Secure = true\n\t}\n\n\tif err := nc.setupServerPool(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Create the async callback handler.\n\tnc.ach = &asyncCallbacksHandler{}\n\tnc.ach.cond = sync.NewCond(&nc.ach.mu)\n\n\t// Set a default error handler that will print to stderr.\n\tif nc.Opts.AsyncErrorCB == nil {\n\t\tnc.Opts.AsyncErrorCB = defaultErrHandler\n\t}\n\n\t// Create reader/writer\n\tnc.newReaderWriter()\n\n\tconnectionEstablished, err := nc.connect()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Spin up the async cb dispatcher on success\n\tgo nc.ach.asyncCBDispatcher()\n\n\tif connectionEstablished && nc.Opts.ConnectedCB != nil {\n\t\tnc.ach.push(func() { nc.Opts.ConnectedCB(nc) })\n\t}\n\n\treturn nc, nil\n}\n\nfunc defaultErrHandler(nc *Conn, sub *Subscription, err error) {\n\tvar cid uint64\n\tif nc != nil {\n\t\tnc.mu.RLock()\n\t\tcid = nc.info.CID\n\t\tnc.mu.RUnlock()\n\t}\n\tvar errStr string\n\tif sub != nil {\n\t\tvar subject string\n\t\tsub.mu.Lock()\n\t\tif sub.jsi != nil {\n\t\t\tsubject = sub.jsi.psubj\n\t\t} else {\n\t\t\tsubject = sub.Subject\n\t\t}\n\t\tsub.mu.Unlock()\n\t\terrStr = fmt.Sprintf(\"%s on connection [%d] for subscription on %q\\n\", err.Error(), cid, subject)\n\t} else {\n\t\terrStr = fmt.Sprintf(\"%s on connection [%d]\\n\", err.Error(), cid)\n\t}\n\tos.Stderr.WriteString(errStr)\n}\n\nconst (\n\t_CRLF_   = \"\\r\\n\"\n\t_EMPTY_  = \"\"\n\t_SPC_    = \" \"\n\t_PUB_P_  = \"PUB \"\n\t_HPUB_P_ = \"HPUB \"\n)\n\nvar _CRLF_BYTES_ = []byte(_CRLF_)\n\nconst (\n\t_OK_OP_   = \"+OK\"\n\t_ERR_OP_  = \"-ERR\"\n\t_PONG_OP_ = \"PONG\"\n\t_INFO_OP_ = \"INFO\"\n)\n\nconst (\n\tconnectProto = \"CONNECT %s\" + _CRLF_\n\tpingProto    = \"PING\" + _CRLF_\n\tpongProto    = \"PONG\" + _CRLF_\n\tsubProto     = \"SUB %s %s %d\" + _CRLF_\n\tunsubProto   = \"UNSUB %d %s\" + _CRLF_\n\tokProto      = _OK_OP_ + _CRLF_\n)\n\n// Return the currently selected server\nfunc (nc *Conn) currentServer() (int, *srv) {\n\tfor i, s := range nc.srvPool {\n\t\tif s == nil {\n\t\t\tcontinue\n\t\t}\n\t\tif s == nc.current {\n\t\t\treturn i, s\n\t\t}\n\t}\n\treturn -1, nil\n}\n\n// Pop the current server and put onto the end of the list. Select head of list as long\n// as number of reconnect attempts under MaxReconnect.\nfunc (nc *Conn) selectNextServer() (*srv, error) {\n\ti, s := nc.currentServer()\n\tif i < 0 {\n\t\treturn nil, ErrNoServers\n\t}\n\tsp := nc.srvPool\n\tnum := len(sp)\n\tcopy(sp[i:num-1], sp[i+1:num])\n\tmaxReconnect := nc.Opts.MaxReconnect\n\tif maxReconnect < 0 || s.reconnects < maxReconnect {\n\t\tnc.srvPool[num-1] = s\n\t} else {\n\t\tnc.srvPool = sp[0 : num-1]\n\t}\n\tif len(nc.srvPool) <= 0 {\n\t\tnc.current = nil\n\t\treturn nil, ErrNoServers\n\t}\n\tnc.current = nc.srvPool[0]\n\treturn nc.srvPool[0], nil\n}\n\n// Will assign the correct server to nc.current\nfunc (nc *Conn) pickServer() error {\n\tnc.current = nil\n\tif len(nc.srvPool) <= 0 {\n\t\treturn ErrNoServers\n\t}\n\n\tfor _, s := range nc.srvPool {\n\t\tif s != nil {\n\t\t\tnc.current = s\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn ErrNoServers\n}\n\nconst tlsScheme = \"tls\"\n\n// Create the server pool using the options given.\n// We will place a Url option first, followed by any\n// Server Options. We will randomize the server pool unless\n// the NoRandomize flag is set.\nfunc (nc *Conn) setupServerPool() error {\n\tnc.srvPool = make([]*srv, 0, srvPoolSize)\n\tnc.urls = make(map[string]struct{}, srvPoolSize)\n\n\t// Create srv objects from each url string in nc.Opts.Servers\n\t// and add them to the pool.\n\tfor _, urlString := range nc.Opts.Servers {\n\t\tif err := nc.addURLToPool(urlString, false, false); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Randomize if allowed to\n\tif !nc.Opts.NoRandomize {\n\t\tnc.shufflePool(0)\n\t}\n\n\t// Normally, if this one is set, Options.Servers should not be,\n\t// but we always allowed that, so continue to do so.\n\tif nc.Opts.Url != _EMPTY_ {\n\t\t// Add to the end of the array\n\t\tif err := nc.addURLToPool(nc.Opts.Url, false, false); err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// Then swap it with first to guarantee that Options.Url is tried first.\n\t\tlast := len(nc.srvPool) - 1\n\t\tif last > 0 {\n\t\t\tnc.srvPool[0], nc.srvPool[last] = nc.srvPool[last], nc.srvPool[0]\n\t\t}\n\t} else if len(nc.srvPool) <= 0 {\n\t\t// Place default URL if pool is empty.\n\t\tif err := nc.addURLToPool(DefaultURL, false, false); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Check for Scheme hint to move to TLS mode.\n\tfor _, srv := range nc.srvPool {\n\t\tif srv.url.Scheme == tlsScheme || srv.url.Scheme == wsSchemeTLS {\n\t\t\t// FIXME(dlc), this is for all in the pool, should be case by case.\n\t\t\tnc.Opts.Secure = true\n\t\t\tif nc.Opts.TLSConfig == nil {\n\t\t\t\tnc.Opts.TLSConfig = &tls.Config{MinVersion: tls.VersionTLS12}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nc.pickServer()\n}\n\n// Helper function to return scheme\nfunc (nc *Conn) connScheme() string {\n\tif nc.ws {\n\t\tif nc.Opts.Secure {\n\t\t\treturn wsSchemeTLS\n\t\t}\n\t\treturn wsScheme\n\t}\n\tif nc.Opts.Secure {\n\t\treturn tlsScheme\n\t}\n\treturn \"nats\"\n}\n\n// Return true iff u.Hostname() is an IP address.\nfunc hostIsIP(u *url.URL) bool {\n\treturn net.ParseIP(u.Hostname()) != nil\n}\n\n// addURLToPool adds an entry to the server pool\nfunc (nc *Conn) addURLToPool(sURL string, implicit, saveTLSName bool) error {\n\tif !strings.Contains(sURL, \"://\") {\n\t\tsURL = fmt.Sprintf(\"%s://%s\", nc.connScheme(), sURL)\n\t}\n\tvar (\n\t\tu   *url.URL\n\t\terr error\n\t)\n\tfor i := 0; i < 2; i++ {\n\t\tu, err = url.Parse(sURL)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif u.Port() != \"\" {\n\t\t\tbreak\n\t\t}\n\t\t// In case given URL is of the form \"localhost:\", just add\n\t\t// the port number at the end, otherwise, add \":4222\".\n\t\tif sURL[len(sURL)-1] != ':' {\n\t\t\tsURL += \":\"\n\t\t}\n\t\tswitch u.Scheme {\n\t\tcase wsScheme:\n\t\t\tsURL += defaultWSPortString\n\t\tcase wsSchemeTLS:\n\t\t\tsURL += defaultWSSPortString\n\t\tdefault:\n\t\t\tsURL += defaultPortString\n\t\t}\n\t}\n\n\tisWS := isWebsocketScheme(u)\n\t// We don't support mix and match of websocket and non websocket URLs.\n\t// If this is the first URL, then we accept and switch the global state\n\t// to websocket. After that, we will know how to reject mixed URLs.\n\tif len(nc.srvPool) == 0 {\n\t\tnc.ws = isWS\n\t} else if isWS && !nc.ws || !isWS && nc.ws {\n\t\treturn errors.New(\"mixing of websocket and non websocket URLs is not allowed\")\n\t}\n\n\tvar tlsName string\n\tif implicit {\n\t\tcurl := nc.current.url\n\t\t// Check to see if we do not have a url.User but current connected\n\t\t// url does. If so copy over.\n\t\tif u.User == nil && curl.User != nil {\n\t\t\tu.User = curl.User\n\t\t}\n\t\t// We are checking to see if we have a secure connection and are\n\t\t// adding an implicit server that just has an IP. If so we will remember\n\t\t// the current hostname we are connected to.\n\t\tif saveTLSName && hostIsIP(u) {\n\t\t\ttlsName = curl.Hostname()\n\t\t}\n\t}\n\n\ts := &srv{url: u, isImplicit: implicit, tlsName: tlsName}\n\tnc.srvPool = append(nc.srvPool, s)\n\tnc.urls[u.Host] = struct{}{}\n\treturn nil\n}\n\n// shufflePool swaps randomly elements in the server pool\n// The `offset` value indicates that the shuffling should start at\n// this offset and leave the elements from [0..offset) intact.\nfunc (nc *Conn) shufflePool(offset int) {\n\tif len(nc.srvPool) <= offset+1 {\n\t\treturn\n\t}\n\tsource := rand.NewSource(time.Now().UnixNano())\n\tr := rand.New(source)\n\tfor i := offset; i < len(nc.srvPool); i++ {\n\t\tj := offset + r.Intn(i+1-offset)\n\t\tnc.srvPool[i], nc.srvPool[j] = nc.srvPool[j], nc.srvPool[i]\n\t}\n}\n\nfunc (nc *Conn) newReaderWriter() {\n\tnc.br = &natsReader{\n\t\tbuf: make([]byte, defaultBufSize),\n\t\toff: -1,\n\t}\n\tnc.bw = &natsWriter{\n\t\tlimit:  defaultBufSize,\n\t\tplimit: nc.Opts.ReconnectBufSize,\n\t}\n}\n\nfunc (nc *Conn) bindToNewConn() {\n\tbw := nc.bw\n\tbw.w, bw.bufs = nc.newWriter(), nil\n\tbr := nc.br\n\tbr.r, br.n, br.off = nc.conn, 0, -1\n}\n\nfunc (nc *Conn) newWriter() io.Writer {\n\tvar w io.Writer = nc.conn\n\tif nc.Opts.FlusherTimeout > 0 {\n\t\tw = &timeoutWriter{conn: nc.conn, timeout: nc.Opts.FlusherTimeout}\n\t}\n\treturn w\n}\n\nfunc (w *natsWriter) appendString(str string) error {\n\treturn w.appendBufs([]byte(str))\n}\n\nfunc (w *natsWriter) appendBufs(bufs ...[]byte) error {\n\tfor _, buf := range bufs {\n\t\tif len(buf) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tif w.pending != nil {\n\t\t\tw.pending.Write(buf)\n\t\t} else {\n\t\t\tw.bufs = append(w.bufs, buf...)\n\t\t}\n\t}\n\tif w.pending == nil && len(w.bufs) >= w.limit {\n\t\treturn w.flush()\n\t}\n\treturn nil\n}\n\nfunc (w *natsWriter) writeDirect(strs ...string) error {\n\tfor _, str := range strs {\n\t\tif _, err := w.w.Write([]byte(str)); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (w *natsWriter) flush() error {\n\t// If a pending buffer is set, we don't flush. Code that needs to\n\t// write directly to the socket, by-passing buffers during (re)connect,\n\t// will use the writeDirect() API.\n\tif w.pending != nil {\n\t\treturn nil\n\t}\n\t// Do not skip calling w.w.Write() here if len(w.bufs) is 0 because\n\t// the actual writer (if websocket for instance) may have things\n\t// to do such as sending control frames, etc..\n\t_, err := w.w.Write(w.bufs)\n\tw.bufs = w.bufs[:0]\n\treturn err\n}\n\nfunc (w *natsWriter) buffered() int {\n\tif w.pending != nil {\n\t\treturn w.pending.Len()\n\t}\n\treturn len(w.bufs)\n}\n\nfunc (w *natsWriter) switchToPending() {\n\tw.pending = new(bytes.Buffer)\n}\n\nfunc (w *natsWriter) flushPendingBuffer() error {\n\tif w.pending == nil || w.pending.Len() == 0 {\n\t\treturn nil\n\t}\n\t_, err := w.w.Write(w.pending.Bytes())\n\t// Reset the pending buffer at this point because we don't want\n\t// to take the risk of sending duplicates or partials.\n\tw.pending.Reset()\n\treturn err\n}\n\nfunc (w *natsWriter) atLimitIfUsingPending() bool {\n\tif w.pending == nil {\n\t\treturn false\n\t}\n\treturn w.pending.Len() >= w.plimit\n}\n\nfunc (w *natsWriter) doneWithPending() {\n\tw.pending = nil\n}\n\n// Notify the reader that we are done with the connect, where \"read\" operations\n// happen synchronously and under the connection lock. After this point, \"read\"\n// will be happening from the read loop, without the connection lock.\n//\n// Note: this runs under the connection lock.\nfunc (r *natsReader) doneWithConnect() {\n\tif wsr, ok := r.r.(*websocketReader); ok {\n\t\twsr.doneWithConnect()\n\t}\n}\n\nfunc (r *natsReader) Read() ([]byte, error) {\n\tif r.off >= 0 {\n\t\toff := r.off\n\t\tr.off = -1\n\t\treturn r.buf[off:r.n], nil\n\t}\n\tvar err error\n\tr.n, err = r.r.Read(r.buf)\n\treturn r.buf[:r.n], err\n}\n\nfunc (r *natsReader) ReadString(delim byte) (string, error) {\n\tvar s string\nbuild_string:\n\t// First look if we have something in the buffer\n\tif r.off >= 0 {\n\t\ti := bytes.IndexByte(r.buf[r.off:r.n], delim)\n\t\tif i >= 0 {\n\t\t\tend := r.off + i + 1\n\t\t\ts += string(r.buf[r.off:end])\n\t\t\tr.off = end\n\t\t\tif r.off >= r.n {\n\t\t\t\tr.off = -1\n\t\t\t}\n\t\t\treturn s, nil\n\t\t}\n\t\t// We did not find the delim, so will have to read more.\n\t\ts += string(r.buf[r.off:r.n])\n\t\tr.off = -1\n\t}\n\tif _, err := r.Read(); err != nil {\n\t\treturn s, err\n\t}\n\tr.off = 0\n\tgoto build_string\n}\n\n// createConn will connect to the server and wrap the appropriate\n// bufio structures. It will do the right thing when an existing\n// connection is in place.\nfunc (nc *Conn) createConn() (err error) {\n\tif nc.Opts.Timeout < 0 {\n\t\treturn ErrBadTimeout\n\t}\n\tif _, cur := nc.currentServer(); cur == nil {\n\t\treturn ErrNoServers\n\t}\n\n\t// If we have a reference to an in-process server then establish a\n\t// connection using that.\n\tif nc.Opts.InProcessServer != nil {\n\t\tconn, err := nc.Opts.InProcessServer.InProcessConn()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to get in-process connection: %w\", err)\n\t\t}\n\t\tnc.conn = conn\n\t\tnc.bindToNewConn()\n\t\treturn nil\n\t}\n\n\t// We will auto-expand host names if they resolve to multiple IPs\n\thosts := []string{}\n\tu := nc.current.url\n\n\tif !nc.Opts.SkipHostLookup && net.ParseIP(u.Hostname()) == nil {\n\t\taddrs, _ := net.LookupHost(u.Hostname())\n\t\tfor _, addr := range addrs {\n\t\t\thosts = append(hosts, net.JoinHostPort(addr, u.Port()))\n\t\t}\n\t}\n\t// Fall back to what we were given.\n\tif len(hosts) == 0 {\n\t\thosts = append(hosts, u.Host)\n\t}\n\n\t// CustomDialer takes precedence. If not set, use Opts.Dialer which\n\t// is set to a default *net.Dialer (in Connect()) if not explicitly\n\t// set by the user.\n\tdialer := nc.Opts.CustomDialer\n\tif dialer == nil {\n\t\t// We will copy and shorten the timeout if we have multiple hosts to try.\n\t\tcopyDialer := *nc.Opts.Dialer\n\t\tcopyDialer.Timeout = copyDialer.Timeout / time.Duration(len(hosts))\n\t\tdialer = &copyDialer\n\t}\n\n\tif len(hosts) > 1 && !nc.Opts.NoRandomize {\n\t\trand.Shuffle(len(hosts), func(i, j int) {\n\t\t\thosts[i], hosts[j] = hosts[j], hosts[i]\n\t\t})\n\t}\n\tfor _, host := range hosts {\n\t\tnc.conn, err = dialer.Dial(\"tcp\", host)\n\t\tif err == nil {\n\t\t\tbreak\n\t\t}\n\t}\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// If scheme starts with \"ws\" then branch out to websocket code.\n\tif isWebsocketScheme(u) {\n\t\treturn nc.wsInitHandshake(u)\n\t}\n\n\t// Reset reader/writer to this new TCP connection\n\tnc.bindToNewConn()\n\treturn nil\n}\n\ntype skipTLSDialer interface {\n\tSkipTLSHandshake() bool\n}\n\n// makeTLSConn will wrap an existing Conn using TLS\nfunc (nc *Conn) makeTLSConn() error {\n\tif nc.Opts.CustomDialer != nil {\n\t\t// we do nothing when asked to skip the TLS wrapper\n\t\tsd, ok := nc.Opts.CustomDialer.(skipTLSDialer)\n\t\tif ok && sd.SkipTLSHandshake() {\n\t\t\treturn nil\n\t\t}\n\t}\n\t// Allow the user to configure their own tls.Config structure.\n\ttlsCopy := &tls.Config{}\n\tif nc.Opts.TLSConfig != nil {\n\t\ttlsCopy = util.CloneTLSConfig(nc.Opts.TLSConfig)\n\t}\n\tif nc.Opts.TLSCertCB != nil {\n\t\tcert, err := nc.Opts.TLSCertCB()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\ttlsCopy.Certificates = []tls.Certificate{cert}\n\t}\n\tif nc.Opts.RootCAsCB != nil {\n\t\trootCAs, err := nc.Opts.RootCAsCB()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\ttlsCopy.RootCAs = rootCAs\n\t}\n\t// If its blank we will override it with the current host\n\tif tlsCopy.ServerName == _EMPTY_ {\n\t\tif nc.current.tlsName != _EMPTY_ {\n\t\t\ttlsCopy.ServerName = nc.current.tlsName\n\t\t} else {\n\t\t\th, _, _ := net.SplitHostPort(nc.current.url.Host)\n\t\t\ttlsCopy.ServerName = h\n\t\t}\n\t}\n\tnc.conn = tls.Client(nc.conn, tlsCopy)\n\tconn := nc.conn.(*tls.Conn)\n\tif err := conn.Handshake(); err != nil {\n\t\treturn err\n\t}\n\tnc.bindToNewConn()\n\treturn nil\n}\n\n// TLSConnectionState retrieves the state of the TLS connection to the server\nfunc (nc *Conn) TLSConnectionState() (tls.ConnectionState, error) {\n\tif !nc.isConnected() {\n\t\treturn tls.ConnectionState{}, ErrDisconnected\n\t}\n\n\tnc.mu.RLock()\n\tconn := nc.conn\n\tnc.mu.RUnlock()\n\n\ttc, ok := conn.(*tls.Conn)\n\tif !ok {\n\t\treturn tls.ConnectionState{}, ErrConnectionNotTLS\n\t}\n\n\treturn tc.ConnectionState(), nil\n}\n\n// waitForExits will wait for all socket watcher Go routines to\n// be shutdown before proceeding.\nfunc (nc *Conn) waitForExits() {\n\t// Kick old flusher forcefully.\n\tselect {\n\tcase nc.fch <- struct{}{}:\n\tdefault:\n\t}\n\n\t// Wait for any previous go routines.\n\tnc.wg.Wait()\n}\n\n// ForceReconnect forces a reconnect attempt to the server.\n// This is a non-blocking call and will start the reconnect\n// process without waiting for it to complete.\n//\n// If the connection is already in the process of reconnecting,\n// this call will force an immediate reconnect attempt (bypassing\n// the current reconnect delay).\nfunc (nc *Conn) ForceReconnect() error {\n\tnc.mu.Lock()\n\tdefer nc.mu.Unlock()\n\n\tif nc.isClosed() {\n\t\treturn ErrConnectionClosed\n\t}\n\tif nc.isReconnecting() {\n\t\t// if we're already reconnecting, force a reconnect attempt\n\t\t// even if we're in the middle of a backoff\n\t\tif nc.rqch != nil {\n\t\t\tclose(nc.rqch)\n\t\t}\n\t\treturn nil\n\t}\n\n\t// Clear any queued pongs\n\tnc.clearPendingFlushCalls()\n\n\t// Clear any queued and blocking requests.\n\tnc.clearPendingRequestCalls()\n\n\t// Stop ping timer if set.\n\tnc.stopPingTimer()\n\n\t// Go ahead and make sure we have flushed the outbound\n\tnc.bw.flush()\n\tnc.conn.Close()\n\n\tnc.changeConnStatus(RECONNECTING)\n\tgo nc.doReconnect(nil, true)\n\treturn nil\n}\n\n// ConnectedUrl reports the connected server's URL\nfunc (nc *Conn) ConnectedUrl() string {\n\tif nc == nil {\n\t\treturn _EMPTY_\n\t}\n\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\n\tif nc.status != CONNECTED {\n\t\treturn _EMPTY_\n\t}\n\treturn nc.current.url.String()\n}\n\n// ConnectedUrlRedacted reports the connected server's URL with passwords redacted\nfunc (nc *Conn) ConnectedUrlRedacted() string {\n\tif nc == nil {\n\t\treturn _EMPTY_\n\t}\n\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\n\tif nc.status != CONNECTED {\n\t\treturn _EMPTY_\n\t}\n\treturn nc.current.url.Redacted()\n}\n\n// ConnectedAddr returns the connected server's IP\nfunc (nc *Conn) ConnectedAddr() string {\n\tif nc == nil {\n\t\treturn _EMPTY_\n\t}\n\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\n\tif nc.status != CONNECTED {\n\t\treturn _EMPTY_\n\t}\n\treturn nc.conn.RemoteAddr().String()\n}\n\n// ConnectedServerId reports the connected server's Id\nfunc (nc *Conn) ConnectedServerId() string {\n\tif nc == nil {\n\t\treturn _EMPTY_\n\t}\n\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\n\tif nc.status != CONNECTED {\n\t\treturn _EMPTY_\n\t}\n\treturn nc.info.ID\n}\n\n// ConnectedServerName reports the connected server's name\nfunc (nc *Conn) ConnectedServerName() string {\n\tif nc == nil {\n\t\treturn _EMPTY_\n\t}\n\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\n\tif nc.status != CONNECTED {\n\t\treturn _EMPTY_\n\t}\n\treturn nc.info.Name\n}\n\nvar semVerRe = regexp.MustCompile(`\\Av?([0-9]+)\\.?([0-9]+)?\\.?([0-9]+)?`)\n\nfunc versionComponents(version string) (major, minor, patch int, err error) {\n\tm := semVerRe.FindStringSubmatch(version)\n\tif m == nil {\n\t\treturn 0, 0, 0, errors.New(\"invalid semver\")\n\t}\n\tmajor, err = strconv.Atoi(m[1])\n\tif err != nil {\n\t\treturn -1, -1, -1, err\n\t}\n\tminor, err = strconv.Atoi(m[2])\n\tif err != nil {\n\t\treturn -1, -1, -1, err\n\t}\n\tpatch, err = strconv.Atoi(m[3])\n\tif err != nil {\n\t\treturn -1, -1, -1, err\n\t}\n\treturn major, minor, patch, err\n}\n\n// Check for minimum server requirement.\nfunc (nc *Conn) serverMinVersion(major, minor, patch int) bool {\n\tsmajor, sminor, spatch, _ := versionComponents(nc.ConnectedServerVersion())\n\tif smajor < major || (smajor == major && sminor < minor) || (smajor == major && sminor == minor && spatch < patch) {\n\t\treturn false\n\t}\n\treturn true\n}\n\n// ConnectedServerVersion reports the connected server's version as a string\nfunc (nc *Conn) ConnectedServerVersion() string {\n\tif nc == nil {\n\t\treturn _EMPTY_\n\t}\n\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\n\tif nc.status != CONNECTED {\n\t\treturn _EMPTY_\n\t}\n\treturn nc.info.Version\n}\n\n// ConnectedClusterName reports the connected server's cluster name if any\nfunc (nc *Conn) ConnectedClusterName() string {\n\tif nc == nil {\n\t\treturn _EMPTY_\n\t}\n\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\n\tif nc.status != CONNECTED {\n\t\treturn _EMPTY_\n\t}\n\treturn nc.info.Cluster\n}\n\n// Low level setup for structs, etc\nfunc (nc *Conn) setup() {\n\tnc.subs = make(map[int64]*Subscription)\n\tnc.pongs = make([]chan struct{}, 0, 8)\n\n\tnc.fch = make(chan struct{}, flushChanSize)\n\tnc.rqch = make(chan struct{})\n\n\t// Setup scratch outbound buffer for PUB/HPUB\n\tpub := nc.scratch[:len(_HPUB_P_)]\n\tcopy(pub, _HPUB_P_)\n}\n\n// Process a connected connection and initialize properly.\nfunc (nc *Conn) processConnectInit() error {\n\n\t// Set our deadline for the whole connect process\n\tnc.conn.SetDeadline(time.Now().Add(nc.Opts.Timeout))\n\tdefer nc.conn.SetDeadline(time.Time{})\n\n\t// Set our status to connecting.\n\tnc.changeConnStatus(CONNECTING)\n\n\t// If we need to have a TLS connection and want the TLS handshake to occur\n\t// first, do it now.\n\tif nc.Opts.Secure && nc.Opts.TLSHandshakeFirst {\n\t\tif err := nc.makeTLSConn(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Process the INFO protocol received from the server\n\terr := nc.processExpectedInfo()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Send the CONNECT protocol along with the initial PING protocol.\n\t// Wait for the PONG response (or any error that we get from the server).\n\terr = nc.sendConnect()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Reset the number of PING sent out\n\tnc.pout = 0\n\n\t// Start or reset Timer\n\tif nc.Opts.PingInterval > 0 {\n\t\tif nc.ptmr == nil {\n\t\t\tnc.ptmr = time.AfterFunc(nc.Opts.PingInterval, nc.processPingTimer)\n\t\t} else {\n\t\t\tnc.ptmr.Reset(nc.Opts.PingInterval)\n\t\t}\n\t}\n\n\t// Start the readLoop and flusher go routines, we will wait on both on a reconnect event.\n\tnc.wg.Add(2)\n\tgo nc.readLoop()\n\tgo nc.flusher()\n\n\t// Notify the reader that we are done with the connect handshake, where\n\t// reads were done synchronously and under the connection lock.\n\tnc.br.doneWithConnect()\n\n\treturn nil\n}\n\n// Main connect function. Will connect to the nats-server.\nfunc (nc *Conn) connect() (bool, error) {\n\tvar err error\n\tvar connectionEstablished bool\n\n\t// Create actual socket connection\n\t// For first connect we walk all servers in the pool and try\n\t// to connect immediately.\n\tnc.mu.Lock()\n\tdefer nc.mu.Unlock()\n\tnc.initc = true\n\t// The pool may change inside the loop iteration due to INFO protocol.\n\tfor i := 0; i < len(nc.srvPool); i++ {\n\t\tnc.current = nc.srvPool[i]\n\n\t\tif err = nc.createConn(); err == nil {\n\t\t\t// This was moved out of processConnectInit() because\n\t\t\t// that function is now invoked from doReconnect() too.\n\t\t\tnc.setup()\n\n\t\t\terr = nc.processConnectInit()\n\n\t\t\tif err == nil {\n\t\t\t\tnc.current.didConnect = true\n\t\t\t\tnc.current.reconnects = 0\n\t\t\t\tnc.current.lastErr = nil\n\t\t\t\tbreak\n\t\t\t} else {\n\t\t\t\tnc.mu.Unlock()\n\t\t\t\tnc.close(DISCONNECTED, false, err)\n\t\t\t\tnc.mu.Lock()\n\t\t\t\t// Do not reset nc.current here since it would prevent\n\t\t\t\t// RetryOnFailedConnect to work should this be the last server\n\t\t\t\t// to try before starting doReconnect().\n\t\t\t}\n\t\t} else {\n\t\t\t// Cancel out default connection refused, will trigger the\n\t\t\t// No servers error conditional\n\t\t\tif strings.Contains(err.Error(), \"connection refused\") {\n\t\t\t\terr = nil\n\t\t\t}\n\t\t}\n\t}\n\n\tif err == nil && nc.status != CONNECTED {\n\t\terr = ErrNoServers\n\t}\n\n\tif err == nil {\n\t\tconnectionEstablished = true\n\t\tnc.initc = false\n\t} else if nc.Opts.RetryOnFailedConnect {\n\t\tnc.setup()\n\t\tnc.changeConnStatus(RECONNECTING)\n\t\tnc.bw.switchToPending()\n\t\tgo nc.doReconnect(ErrNoServers, false)\n\t\terr = nil\n\t} else {\n\t\tnc.current = nil\n\t}\n\n\treturn connectionEstablished, err\n}\n\n// This will check to see if the connection should be\n// secure. This can be dictated from either end and should\n// only be called after the INIT protocol has been received.\nfunc (nc *Conn) checkForSecure() error {\n\t// Check to see if we need to engage TLS\n\to := nc.Opts\n\n\t// Check for mismatch in setups\n\tif o.Secure && !nc.info.TLSRequired && !nc.info.TLSAvailable {\n\t\treturn ErrSecureConnWanted\n\t} else if nc.info.TLSRequired && !o.Secure {\n\t\t// Switch to Secure since server needs TLS.\n\t\to.Secure = true\n\t}\n\n\tif o.Secure {\n\t\t// If TLS handshake first is true, we have already done\n\t\t// the handshake, so we are done here.\n\t\tif o.TLSHandshakeFirst {\n\t\t\treturn nil\n\t\t}\n\t\t// Need to rewrap with bufio\n\t\tif err := nc.makeTLSConn(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// processExpectedInfo will look for the expected first INFO message\n// sent when a connection is established. The lock should be held entering.\nfunc (nc *Conn) processExpectedInfo() error {\n\n\tc := &control{}\n\n\t// Read the protocol\n\terr := nc.readOp(c)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// The nats protocol should send INFO first always.\n\tif c.op != _INFO_OP_ {\n\t\treturn ErrNoInfoReceived\n\t}\n\n\t// Parse the protocol\n\tif err := nc.processInfo(c.args); err != nil {\n\t\treturn err\n\t}\n\n\tif nc.Opts.Nkey != \"\" && nc.info.Nonce == \"\" {\n\t\treturn ErrNkeysNotSupported\n\t}\n\n\t// For websocket connections, we already switched to TLS if need be,\n\t// so we are done here.\n\tif nc.ws {\n\t\treturn nil\n\t}\n\n\treturn nc.checkForSecure()\n}\n\n// Sends a protocol control message by queuing into the bufio writer\n// and kicking the flush Go routine.  These writes are protected.\nfunc (nc *Conn) sendProto(proto string) {\n\tnc.mu.Lock()\n\tnc.bw.appendString(proto)\n\tnc.kickFlusher()\n\tnc.mu.Unlock()\n}\n\n// Generate a connect protocol message, issuing user/password if\n// applicable. The lock is assumed to be held upon entering.\nfunc (nc *Conn) connectProto() (string, error) {\n\to := nc.Opts\n\tvar nkey, sig, user, pass, token, ujwt string\n\tu := nc.current.url.User\n\tif u != nil {\n\t\t// if no password, assume username is authToken\n\t\tif _, ok := u.Password(); !ok {\n\t\t\ttoken = u.Username()\n\t\t} else {\n\t\t\tuser = u.Username()\n\t\t\tpass, _ = u.Password()\n\t\t}\n\t} else {\n\t\t// Take from options (possibly all empty strings)\n\t\tuser = o.User\n\t\tpass = o.Password\n\t\ttoken = o.Token\n\t\tnkey = o.Nkey\n\n\t\tif nc.Opts.UserInfo != nil {\n\t\t\tif user != _EMPTY_ || pass != _EMPTY_ {\n\t\t\t\treturn _EMPTY_, ErrUserInfoAlreadySet\n\t\t\t}\n\t\t\tuser, pass = nc.Opts.UserInfo()\n\t\t}\n\t}\n\n\t// Look for user jwt.\n\tif o.UserJWT != nil {\n\t\tif jwt, err := o.UserJWT(); err != nil {\n\t\t\treturn _EMPTY_, err\n\t\t} else {\n\t\t\tujwt = jwt\n\t\t}\n\t\tif nkey != _EMPTY_ {\n\t\t\treturn _EMPTY_, ErrNkeyAndUser\n\t\t}\n\t}\n\n\tif ujwt != _EMPTY_ || nkey != _EMPTY_ {\n\t\tif o.SignatureCB == nil {\n\t\t\tif ujwt == _EMPTY_ {\n\t\t\t\treturn _EMPTY_, ErrNkeyButNoSigCB\n\t\t\t}\n\t\t\treturn _EMPTY_, ErrUserButNoSigCB\n\t\t}\n\t\tsigraw, err := o.SignatureCB([]byte(nc.info.Nonce))\n\t\tif err != nil {\n\t\t\treturn _EMPTY_, fmt.Errorf(\"error signing nonce: %w\", err)\n\t\t}\n\t\tsig = base64.RawURLEncoding.EncodeToString(sigraw)\n\t}\n\n\tif nc.Opts.TokenHandler != nil {\n\t\tif token != _EMPTY_ {\n\t\t\treturn _EMPTY_, ErrTokenAlreadySet\n\t\t}\n\t\ttoken = nc.Opts.TokenHandler()\n\t}\n\n\t// If our server does not support headers then we can't do them or no responders.\n\thdrs := nc.info.Headers\n\tcinfo := connectInfo{o.Verbose, o.Pedantic, ujwt, nkey, sig, user, pass, token,\n\t\to.Secure, o.Name, LangString, Version, clientProtoInfo, !o.NoEcho, hdrs, hdrs}\n\n\tb, err := json.Marshal(cinfo)\n\tif err != nil {\n\t\treturn _EMPTY_, ErrJsonParse\n\t}\n\n\t// Check if NoEcho is set and we have a server that supports it.\n\tif o.NoEcho && nc.info.Proto < 1 {\n\t\treturn _EMPTY_, ErrNoEchoNotSupported\n\t}\n\n\treturn fmt.Sprintf(connectProto, b), nil\n}\n\n// normalizeErr removes the prefix -ERR, trim spaces and remove the quotes.\nfunc normalizeErr(line string) string {\n\ts := strings.TrimSpace(strings.TrimPrefix(line, _ERR_OP_))\n\ts = strings.TrimLeft(strings.TrimRight(s, \"'\"), \"'\")\n\treturn s\n}\n\n// natsProtoErr represents an -ERR protocol message sent by the server.\ntype natsProtoErr struct {\n\tdescription string\n}\n\nfunc (nerr *natsProtoErr) Error() string {\n\treturn fmt.Sprintf(\"nats: %s\", nerr.description)\n}\n\nfunc (nerr *natsProtoErr) Is(err error) bool {\n\treturn strings.ToLower(nerr.Error()) == err.Error()\n}\n\n// Send a connect protocol message to the server, issue user/password if\n// applicable. Will wait for a flush to return from the server for error\n// processing.\nfunc (nc *Conn) sendConnect() error {\n\t// Construct the CONNECT protocol string\n\tcProto, err := nc.connectProto()\n\tif err != nil {\n\t\tif !nc.initc && nc.Opts.AsyncErrorCB != nil {\n\t\t\tnc.ach.push(func() { nc.Opts.AsyncErrorCB(nc, nil, err) })\n\t\t}\n\t\treturn err\n\t}\n\n\t// Write the protocol and PING directly to the underlying writer.\n\tif err := nc.bw.writeDirect(cProto, pingProto); err != nil {\n\t\treturn err\n\t}\n\n\t// We don't want to read more than we need here, otherwise\n\t// we would need to transfer the excess read data to the readLoop.\n\t// Since in normal situations we just are looking for a PONG\\r\\n,\n\t// reading byte-by-byte here is ok.\n\tproto, err := nc.readProto()\n\tif err != nil {\n\t\tif !nc.initc && nc.Opts.AsyncErrorCB != nil {\n\t\t\tnc.ach.push(func() { nc.Opts.AsyncErrorCB(nc, nil, err) })\n\t\t}\n\t\treturn err\n\t}\n\n\t// If opts.Verbose is set, handle +OK\n\tif nc.Opts.Verbose && proto == okProto {\n\t\t// Read the rest now...\n\t\tproto, err = nc.readProto()\n\t\tif err != nil {\n\t\t\tif !nc.initc && nc.Opts.AsyncErrorCB != nil {\n\t\t\t\tnc.ach.push(func() { nc.Opts.AsyncErrorCB(nc, nil, err) })\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// We expect a PONG\n\tif proto != pongProto {\n\t\t// But it could be something else, like -ERR\n\n\t\t// Since we no longer use ReadLine(), trim the trailing \"\\r\\n\"\n\t\tproto = strings.TrimRight(proto, \"\\r\\n\")\n\n\t\t// If it's a server error...\n\t\tif strings.HasPrefix(proto, _ERR_OP_) {\n\t\t\t// Remove -ERR, trim spaces and quotes, and convert to lower case.\n\t\t\tproto = normalizeErr(proto)\n\n\t\t\t// Check if this is an auth error\n\t\t\tif authErr := checkAuthError(strings.ToLower(proto)); authErr != nil {\n\t\t\t\t// This will schedule an async error if we are in reconnect,\n\t\t\t\t// and keep track of the auth error for the current server.\n\t\t\t\t// If we have got the same error twice, this sets nc.ar to true to\n\t\t\t\t// indicate that the reconnect should be aborted (will be checked\n\t\t\t\t// in doReconnect()).\n\t\t\t\tnc.processAuthError(authErr)\n\t\t\t}\n\t\t\treturn &natsProtoErr{proto}\n\t\t}\n\n\t\t// Notify that we got an unexpected protocol.\n\t\treturn fmt.Errorf(\"nats: expected '%s', got '%s'\", _PONG_OP_, proto)\n\t}\n\n\t// This is where we are truly connected.\n\tnc.changeConnStatus(CONNECTED)\n\n\treturn nil\n}\n\n// reads a protocol line.\nfunc (nc *Conn) readProto() (string, error) {\n\treturn nc.br.ReadString('\\n')\n}\n\n// A control protocol line.\ntype control struct {\n\top, args string\n}\n\n// Read a control line and process the intended op.\nfunc (nc *Conn) readOp(c *control) error {\n\tline, err := nc.readProto()\n\tif err != nil {\n\t\treturn err\n\t}\n\tparseControl(line, c)\n\treturn nil\n}\n\n// Parse a control line from the server.\nfunc parseControl(line string, c *control) {\n\ttoks := strings.SplitN(line, _SPC_, 2)\n\tif len(toks) == 1 {\n\t\tc.op = strings.TrimSpace(toks[0])\n\t\tc.args = _EMPTY_\n\t} else if len(toks) == 2 {\n\t\tc.op, c.args = strings.TrimSpace(toks[0]), strings.TrimSpace(toks[1])\n\t} else {\n\t\tc.op = _EMPTY_\n\t}\n}\n\n// flushReconnectPendingItems will push the pending items that were\n// gathered while we were in a RECONNECTING state to the socket.\nfunc (nc *Conn) flushReconnectPendingItems() error {\n\treturn nc.bw.flushPendingBuffer()\n}\n\n// Stops the ping timer if set.\n// Connection lock is held on entry.\nfunc (nc *Conn) stopPingTimer() {\n\tif nc.ptmr != nil {\n\t\tnc.ptmr.Stop()\n\t}\n}\n\n// Try to reconnect using the option parameters.\n// This function assumes we are allowed to reconnect.\nfunc (nc *Conn) doReconnect(err error, forceReconnect bool) {\n\t// We want to make sure we have the other watchers shutdown properly\n\t// here before we proceed past this point.\n\tnc.waitForExits()\n\n\t// FIXME(dlc) - We have an issue here if we have\n\t// outstanding flush points (pongs) and they were not\n\t// sent out, but are still in the pipe.\n\n\t// Hold the lock manually and release where needed below,\n\t// can't do defer here.\n\tnc.mu.Lock()\n\n\t// Clear any errors.\n\tnc.err = nil\n\t// Perform appropriate callback if needed for a disconnect.\n\t// DisconnectedErrCB has priority over deprecated DisconnectedCB\n\tif !nc.initc {\n\t\tif nc.Opts.DisconnectedErrCB != nil {\n\t\t\tnc.ach.push(func() { nc.Opts.DisconnectedErrCB(nc, err) })\n\t\t} else if nc.Opts.DisconnectedCB != nil {\n\t\t\tnc.ach.push(func() { nc.Opts.DisconnectedCB(nc) })\n\t\t}\n\t}\n\n\t// This is used to wait on go routines exit if we start them in the loop\n\t// but an error occurs after that.\n\twaitForGoRoutines := false\n\tvar rt *time.Timer\n\t// Channel used to kick routine out of sleep when conn is closed.\n\trqch := nc.rqch\n\t// Counter that is increased when the whole list of servers has been tried.\n\tvar wlf int\n\n\tvar jitter time.Duration\n\tvar rw time.Duration\n\t// If a custom reconnect delay handler is set, this takes precedence.\n\tcrd := nc.Opts.CustomReconnectDelayCB\n\tif crd == nil {\n\t\trw = nc.Opts.ReconnectWait\n\t\t// TODO: since we sleep only after the whole list has been tried, we can't\n\t\t// rely on individual *srv to know if it is a TLS or non-TLS url.\n\t\t// We have to pick which type of jitter to use, for now, we use these hints:\n\t\tjitter = nc.Opts.ReconnectJitter\n\t\tif nc.Opts.Secure || nc.Opts.TLSConfig != nil {\n\t\t\tjitter = nc.Opts.ReconnectJitterTLS\n\t\t}\n\t}\n\n\tfor i := 0; len(nc.srvPool) > 0; {\n\t\tcur, err := nc.selectNextServer()\n\t\tif err != nil {\n\t\t\tnc.err = err\n\t\t\tbreak\n\t\t}\n\n\t\tdoSleep := i+1 >= len(nc.srvPool) && !forceReconnect\n\t\tforceReconnect = false\n\t\tnc.mu.Unlock()\n\n\t\tif !doSleep {\n\t\t\ti++\n\t\t\t// Release the lock to give a chance to a concurrent nc.Close() to break the loop.\n\t\t\truntime.Gosched()\n\t\t} else {\n\t\t\ti = 0\n\t\t\tvar st time.Duration\n\t\t\tif crd != nil {\n\t\t\t\twlf++\n\t\t\t\tst = crd(wlf)\n\t\t\t} else {\n\t\t\t\tst = rw\n\t\t\t\tif jitter > 0 {\n\t\t\t\t\tst += time.Duration(rand.Int63n(int64(jitter)))\n\t\t\t\t}\n\t\t\t}\n\t\t\tif rt == nil {\n\t\t\t\trt = time.NewTimer(st)\n\t\t\t} else {\n\t\t\t\trt.Reset(st)\n\t\t\t}\n\t\t\tselect {\n\t\t\tcase <-rqch:\n\t\t\t\trt.Stop()\n\n\t\t\t\t// we need to reset the rqch channel to avoid\n\t\t\t\t// closing a closed channel in the next iteration\n\t\t\t\tnc.mu.Lock()\n\t\t\t\tnc.rqch = make(chan struct{})\n\t\t\t\tnc.mu.Unlock()\n\t\t\tcase <-rt.C:\n\t\t\t}\n\t\t}\n\t\t// If the readLoop, etc.. go routines were started, wait for them to complete.\n\t\tif waitForGoRoutines {\n\t\t\tnc.waitForExits()\n\t\t\twaitForGoRoutines = false\n\t\t}\n\t\tnc.mu.Lock()\n\n\t\t// Check if we have been closed first.\n\t\tif nc.isClosed() {\n\t\t\tbreak\n\t\t}\n\n\t\t// Mark that we tried a reconnect\n\t\tcur.reconnects++\n\n\t\t// Try to create a new connection\n\t\terr = nc.createConn()\n\n\t\t// Not yet connected, retry...\n\t\t// Continue to hold the lock\n\t\tif err != nil {\n\t\t\tnc.err = nil\n\t\t\tcontinue\n\t\t}\n\n\t\t// We are reconnected\n\t\tnc.Reconnects++\n\n\t\t// Process connect logic\n\t\tif nc.err = nc.processConnectInit(); nc.err != nil {\n\t\t\t// Check if we should abort reconnect. If so, break out\n\t\t\t// of the loop and connection will be closed.\n\t\t\tif nc.ar {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tnc.changeConnStatus(RECONNECTING)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Clear possible lastErr under the connection lock after\n\t\t// a successful processConnectInit().\n\t\tnc.current.lastErr = nil\n\n\t\t// Clear out server stats for the server we connected to..\n\t\tcur.didConnect = true\n\t\tcur.reconnects = 0\n\n\t\t// Send existing subscription state\n\t\tnc.resendSubscriptions()\n\n\t\t// Now send off and clear pending buffer\n\t\tnc.err = nc.flushReconnectPendingItems()\n\t\tif nc.err != nil {\n\t\t\tnc.changeConnStatus(RECONNECTING)\n\t\t\t// Stop the ping timer (if set)\n\t\t\tnc.stopPingTimer()\n\t\t\t// Since processConnectInit() returned without error, the\n\t\t\t// go routines were started, so wait for them to return\n\t\t\t// on the next iteration (after releasing the lock).\n\t\t\twaitForGoRoutines = true\n\t\t\tcontinue\n\t\t}\n\n\t\t// Done with the pending buffer\n\t\tnc.bw.doneWithPending()\n\n\t\t// Queue up the correct callback. If we are in initial connect state\n\t\t// (using retry on failed connect), we will call the ConnectedCB,\n\t\t// otherwise the ReconnectedCB.\n\t\tif nc.Opts.ReconnectedCB != nil && !nc.initc {\n\t\t\tnc.ach.push(func() { nc.Opts.ReconnectedCB(nc) })\n\t\t} else if nc.Opts.ConnectedCB != nil && nc.initc {\n\t\t\tnc.ach.push(func() { nc.Opts.ConnectedCB(nc) })\n\t\t}\n\n\t\t// If we are here with a retry on failed connect, indicate that the\n\t\t// initial connect is now complete.\n\t\tnc.initc = false\n\n\t\t// Release lock here, we will return below.\n\t\tnc.mu.Unlock()\n\n\t\t// Make sure to flush everything\n\t\tnc.Flush()\n\n\t\treturn\n\t}\n\n\t// Call into close.. We have no servers left..\n\tif nc.err == nil {\n\t\tnc.err = ErrNoServers\n\t}\n\tnc.mu.Unlock()\n\tnc.close(CLOSED, true, nil)\n}\n\n// processOpErr handles errors from reading or parsing the protocol.\n// The lock should not be held entering this function.\nfunc (nc *Conn) processOpErr(err error) bool {\n\tnc.mu.Lock()\n\tdefer nc.mu.Unlock()\n\tif nc.isConnecting() || nc.isClosed() || nc.isReconnecting() {\n\t\treturn false\n\t}\n\n\tif nc.Opts.AllowReconnect && nc.status == CONNECTED {\n\t\t// Set our new status\n\t\tnc.changeConnStatus(RECONNECTING)\n\t\t// Stop ping timer if set\n\t\tnc.stopPingTimer()\n\t\tif nc.conn != nil {\n\t\t\tnc.conn.Close()\n\t\t\tnc.conn = nil\n\t\t}\n\n\t\t// Create pending buffer before reconnecting.\n\t\tnc.bw.switchToPending()\n\n\t\t// Clear any queued pongs, e.g. pending flush calls.\n\t\tnc.clearPendingFlushCalls()\n\n\t\tgo nc.doReconnect(err, false)\n\t\treturn false\n\t}\n\n\tnc.changeConnStatus(DISCONNECTED)\n\tnc.err = err\n\treturn true\n}\n\n// dispatch is responsible for calling any async callbacks\nfunc (ac *asyncCallbacksHandler) asyncCBDispatcher() {\n\tfor {\n\t\tac.mu.Lock()\n\t\t// Protect for spurious wakeups. We should get out of the\n\t\t// wait only if there is an element to pop from the list.\n\t\tfor ac.head == nil {\n\t\t\tac.cond.Wait()\n\t\t}\n\t\tcur := ac.head\n\t\tac.head = cur.next\n\t\tif cur == ac.tail {\n\t\t\tac.tail = nil\n\t\t}\n\t\tac.mu.Unlock()\n\n\t\t// This signals that the dispatcher has been closed and all\n\t\t// previous callbacks have been dispatched.\n\t\tif cur.f == nil {\n\t\t\treturn\n\t\t}\n\t\t// Invoke callback outside of handler's lock\n\t\tcur.f()\n\t}\n}\n\n// Add the given function to the tail of the list and\n// signals the dispatcher.\nfunc (ac *asyncCallbacksHandler) push(f func()) {\n\tac.pushOrClose(f, false)\n}\n\n// Signals that we are closing...\nfunc (ac *asyncCallbacksHandler) close() {\n\tac.pushOrClose(nil, true)\n}\n\n// Add the given function to the tail of the list and\n// signals the dispatcher.\nfunc (ac *asyncCallbacksHandler) pushOrClose(f func(), close bool) {\n\tac.mu.Lock()\n\tdefer ac.mu.Unlock()\n\t// Make sure that library is not calling push with nil function,\n\t// since this is used to notify the dispatcher that it should stop.\n\tif !close && f == nil {\n\t\tpanic(\"pushing a nil callback\")\n\t}\n\tcb := &asyncCB{f: f}\n\tif ac.tail != nil {\n\t\tac.tail.next = cb\n\t} else {\n\t\tac.head = cb\n\t}\n\tac.tail = cb\n\tif close {\n\t\tac.cond.Broadcast()\n\t} else {\n\t\tac.cond.Signal()\n\t}\n}\n\n// readLoop() will sit on the socket reading and processing the\n// protocol from the server. It will dispatch appropriately based\n// on the op type.\nfunc (nc *Conn) readLoop() {\n\t// Release the wait group on exit\n\tdefer nc.wg.Done()\n\n\t// Create a parseState if needed.\n\tnc.mu.Lock()\n\tif nc.ps == nil {\n\t\tnc.ps = &parseState{}\n\t}\n\tconn := nc.conn\n\tbr := nc.br\n\tnc.mu.Unlock()\n\n\tif conn == nil {\n\t\treturn\n\t}\n\n\tfor {\n\t\tbuf, err := br.Read()\n\t\tif err == nil {\n\t\t\t// With websocket, it is possible that there is no error but\n\t\t\t// also no buffer returned (either WS control message or read of a\n\t\t\t// partial compressed message). We could call parse(buf) which\n\t\t\t// would ignore an empty buffer, but simply go back to top of the loop.\n\t\t\tif len(buf) == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\terr = nc.parse(buf)\n\t\t}\n\t\tif err != nil {\n\t\t\tif shouldClose := nc.processOpErr(err); shouldClose {\n\t\t\t\tnc.close(CLOSED, true, nil)\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\t}\n\t// Clear the parseState here..\n\tnc.mu.Lock()\n\tnc.ps = nil\n\tnc.mu.Unlock()\n}\n\n// waitForMsgs waits on the conditional shared with readLoop and processMsg.\n// It is used to deliver messages to asynchronous subscribers.\nfunc (nc *Conn) waitForMsgs(s *Subscription) {\n\tvar closed bool\n\tvar delivered, max uint64\n\n\t// Used to account for adjustments to sub.pBytes when we wrap back around.\n\tmsgLen := -1\n\n\tfor {\n\t\ts.mu.Lock()\n\t\t// Do accounting for last msg delivered here so we only lock once\n\t\t// and drain state trips after callback has returned.\n\t\tif msgLen >= 0 {\n\t\t\ts.pMsgs--\n\t\t\ts.pBytes -= msgLen\n\t\t\tmsgLen = -1\n\t\t}\n\n\t\tif s.pHead == nil && !s.closed {\n\t\t\ts.pCond.Wait()\n\t\t}\n\t\t// Pop the msg off the list\n\t\tm := s.pHead\n\t\tif m != nil {\n\t\t\ts.pHead = m.next\n\t\t\tif s.pHead == nil {\n\t\t\t\ts.pTail = nil\n\t\t\t}\n\t\t\tif m.barrier != nil {\n\t\t\t\ts.mu.Unlock()\n\t\t\t\tif atomic.AddInt64(&m.barrier.refs, -1) == 0 {\n\t\t\t\t\tm.barrier.f()\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tmsgLen = len(m.Data)\n\t\t}\n\t\tmcb := s.mcb\n\t\tmax = s.max\n\t\tclosed = s.closed\n\t\tvar fcReply string\n\t\tif !s.closed {\n\t\t\ts.delivered++\n\t\t\tdelivered = s.delivered\n\t\t\tif s.jsi != nil {\n\t\t\t\tfcReply = s.checkForFlowControlResponse()\n\t\t\t}\n\t\t}\n\t\ts.mu.Unlock()\n\n\t\t// Respond to flow control if applicable\n\t\tif fcReply != _EMPTY_ {\n\t\t\tnc.Publish(fcReply, nil)\n\t\t}\n\n\t\tif closed {\n\t\t\tbreak\n\t\t}\n\n\t\t// Deliver the message.\n\t\tif m != nil && (max == 0 || delivered <= max) {\n\t\t\tmcb(m)\n\t\t}\n\t\t// If we have hit the max for delivered msgs, remove sub.\n\t\tif max > 0 && delivered >= max {\n\t\t\tnc.mu.Lock()\n\t\t\tnc.removeSub(s)\n\t\t\tnc.mu.Unlock()\n\t\t\tbreak\n\t\t}\n\t}\n\t// Check for barrier messages\n\ts.mu.Lock()\n\tfor m := s.pHead; m != nil; m = s.pHead {\n\t\tif m.barrier != nil {\n\t\t\ts.mu.Unlock()\n\t\t\tif atomic.AddInt64(&m.barrier.refs, -1) == 0 {\n\t\t\t\tm.barrier.f()\n\t\t\t}\n\t\t\ts.mu.Lock()\n\t\t}\n\t\ts.pHead = m.next\n\t}\n\t// Now check for pDone\n\tdone := s.pDone\n\ts.mu.Unlock()\n\n\tif done != nil {\n\t\tdone(s.Subject)\n\t}\n}\n\n// Used for debugging and simulating loss for certain tests.\n// Return what is to be used. If we return nil the message will be dropped.\ntype msgFilter func(m *Msg) *Msg\n\n// processMsg is called by parse and will place the msg on the\n// appropriate channel/pending queue for processing. If the channel is full,\n// or the pending queue is over the pending limits, the connection is\n// considered a slow consumer.\nfunc (nc *Conn) processMsg(data []byte) {\n\t// Stats\n\tatomic.AddUint64(&nc.InMsgs, 1)\n\tatomic.AddUint64(&nc.InBytes, uint64(len(data)))\n\n\t// Don't lock the connection to avoid server cutting us off if the\n\t// flusher is holding the connection lock, trying to send to the server\n\t// that is itself trying to send data to us.\n\tnc.subsMu.RLock()\n\tsub := nc.subs[nc.ps.ma.sid]\n\tvar mf msgFilter\n\tif nc.filters != nil {\n\t\tmf = nc.filters[string(nc.ps.ma.subject)]\n\t}\n\tnc.subsMu.RUnlock()\n\n\tif sub == nil {\n\t\treturn\n\t}\n\n\t// Copy them into string\n\tsubj := string(nc.ps.ma.subject)\n\treply := string(nc.ps.ma.reply)\n\n\t// Doing message create outside of the sub's lock to reduce contention.\n\t// It's possible that we end-up not using the message, but that's ok.\n\n\t// FIXME(dlc): Need to copy, should/can do COW?\n\tvar msgPayload = data\n\tif !nc.ps.msgCopied {\n\t\tmsgPayload = make([]byte, len(data))\n\t\tcopy(msgPayload, data)\n\t}\n\n\t// Check if we have headers encoded here.\n\tvar h Header\n\tvar err error\n\tvar ctrlMsg bool\n\tvar ctrlType int\n\tvar fcReply string\n\n\tif nc.ps.ma.hdr > 0 {\n\t\thbuf := msgPayload[:nc.ps.ma.hdr]\n\t\tmsgPayload = msgPayload[nc.ps.ma.hdr:]\n\t\th, err = DecodeHeadersMsg(hbuf)\n\t\tif err != nil {\n\t\t\t// We will pass the message through but send async error.\n\t\t\tnc.mu.Lock()\n\t\t\tnc.err = ErrBadHeaderMsg\n\t\t\tif nc.Opts.AsyncErrorCB != nil {\n\t\t\t\tnc.ach.push(func() { nc.Opts.AsyncErrorCB(nc, sub, ErrBadHeaderMsg) })\n\t\t\t}\n\t\t\tnc.mu.Unlock()\n\t\t}\n\t}\n\n\t// FIXME(dlc): Should we recycle these containers?\n\tm := &Msg{\n\t\tSubject: subj,\n\t\tReply:   reply,\n\t\tHeader:  h,\n\t\tData:    msgPayload,\n\t\tSub:     sub,\n\t\twsz:     len(data) + len(subj) + len(reply),\n\t}\n\n\t// Check for message filters.\n\tif mf != nil {\n\t\tif m = mf(m); m == nil {\n\t\t\t// Drop message.\n\t\t\treturn\n\t\t}\n\t}\n\n\tsub.mu.Lock()\n\n\t// Check if closed.\n\tif sub.closed {\n\t\tsub.mu.Unlock()\n\t\treturn\n\t}\n\n\t// Skip flow control messages in case of using a JetStream context.\n\tjsi := sub.jsi\n\tif jsi != nil {\n\t\t// There has to be a header for it to be a control message.\n\t\tif h != nil {\n\t\t\tctrlMsg, ctrlType = isJSControlMessage(m)\n\t\t\tif ctrlMsg && ctrlType == jsCtrlHB {\n\t\t\t\t// Check if the heartbeat has a \"Consumer Stalled\" header, if\n\t\t\t\t// so, the value is the FC reply to send a nil message to.\n\t\t\t\t// We will send it at the end of this function.\n\t\t\t\tfcReply = m.Header.Get(consumerStalledHdr)\n\t\t\t}\n\t\t}\n\t\t// Check for ordered consumer here. If checkOrderedMsgs returns true that means it detected a gap.\n\t\tif !ctrlMsg && jsi.ordered && sub.checkOrderedMsgs(m) {\n\t\t\tsub.mu.Unlock()\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Skip processing if this is a control message and\n\t// if not a pull consumer heartbeat. For pull consumers,\n\t// heartbeats have to be handled on per request basis.\n\tif !ctrlMsg || (jsi != nil && jsi.pull) {\n\t\tvar chanSubCheckFC bool\n\t\t// Subscription internal stats (applicable only for non ChanSubscription's)\n\t\tif sub.typ != ChanSubscription {\n\t\t\tsub.pMsgs++\n\t\t\tif sub.pMsgs > sub.pMsgsMax {\n\t\t\t\tsub.pMsgsMax = sub.pMsgs\n\t\t\t}\n\t\t\tsub.pBytes += len(m.Data)\n\t\t\tif sub.pBytes > sub.pBytesMax {\n\t\t\t\tsub.pBytesMax = sub.pBytes\n\t\t\t}\n\n\t\t\t// Check for a Slow Consumer\n\t\t\tif (sub.pMsgsLimit > 0 && sub.pMsgs > sub.pMsgsLimit) ||\n\t\t\t\t(sub.pBytesLimit > 0 && sub.pBytes > sub.pBytesLimit) {\n\t\t\t\tgoto slowConsumer\n\t\t\t}\n\t\t} else if jsi != nil {\n\t\t\tchanSubCheckFC = true\n\t\t}\n\n\t\t// We have two modes of delivery. One is the channel, used by channel\n\t\t// subscribers and syncSubscribers, the other is a linked list for async.\n\t\tif sub.mch != nil {\n\t\t\tselect {\n\t\t\tcase sub.mch <- m:\n\t\t\tdefault:\n\t\t\t\tgoto slowConsumer\n\t\t\t}\n\t\t} else {\n\t\t\t// Push onto the async pList\n\t\t\tif sub.pHead == nil {\n\t\t\t\tsub.pHead = m\n\t\t\t\tsub.pTail = m\n\t\t\t\tif sub.pCond != nil {\n\t\t\t\t\tsub.pCond.Signal()\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tsub.pTail.next = m\n\t\t\t\tsub.pTail = m\n\t\t\t}\n\t\t}\n\t\tif jsi != nil {\n\t\t\t// Store the ACK metadata from the message to\n\t\t\t// compare later on with the received heartbeat.\n\t\t\tsub.trackSequences(m.Reply)\n\t\t\tif chanSubCheckFC {\n\t\t\t\t// For ChanSubscription, since we can't call this when a message\n\t\t\t\t// is \"delivered\" (since user is pull from their own channel),\n\t\t\t\t// we have a go routine that does this check, however, we do it\n\t\t\t\t// also here to make it much more responsive. The go routine is\n\t\t\t\t// really to avoid stalling when there is no new messages coming.\n\t\t\t\tfcReply = sub.checkForFlowControlResponse()\n\t\t\t}\n\t\t}\n\t} else if ctrlType == jsCtrlFC && m.Reply != _EMPTY_ {\n\t\t// This is a flow control message.\n\t\t// We will schedule the send of the FC reply once we have delivered the\n\t\t// DATA message that was received before this flow control message, which\n\t\t// has sequence `jsi.fciseq`. However, it is possible that this message\n\t\t// has already been delivered, in that case, we need to send the FC reply now.\n\t\tif sub.getJSDelivered() >= jsi.fciseq {\n\t\t\tfcReply = m.Reply\n\t\t} else {\n\t\t\t// Schedule a reply after the previous message is delivered.\n\t\t\tsub.scheduleFlowControlResponse(m.Reply)\n\t\t}\n\t}\n\n\t// Clear any SlowConsumer status.\n\tif sub.sc {\n\t\tsub.changeSubStatus(SubscriptionActive)\n\t}\n\tsub.sc = false\n\tsub.mu.Unlock()\n\n\tif fcReply != _EMPTY_ {\n\t\tnc.Publish(fcReply, nil)\n\t}\n\n\t// Handle control heartbeat messages.\n\tif ctrlMsg && ctrlType == jsCtrlHB && m.Reply == _EMPTY_ {\n\t\tnc.checkForSequenceMismatch(m, sub, jsi)\n\t}\n\n\treturn\n\nslowConsumer:\n\tsub.dropped++\n\tsc := !sub.sc\n\tsub.sc = true\n\t// Undo stats from above\n\tif sub.typ != ChanSubscription {\n\t\tsub.pMsgs--\n\t\tsub.pBytes -= len(m.Data)\n\t}\n\tif sc {\n\t\tsub.changeSubStatus(SubscriptionSlowConsumer)\n\t\tsub.mu.Unlock()\n\t\t// Now we need connection's lock and we may end-up in the situation\n\t\t// that we were trying to avoid, except that in this case, the client\n\t\t// is already experiencing client-side slow consumer situation.\n\t\tnc.mu.Lock()\n\t\tnc.err = ErrSlowConsumer\n\t\tif nc.Opts.AsyncErrorCB != nil {\n\t\t\tnc.ach.push(func() { nc.Opts.AsyncErrorCB(nc, sub, ErrSlowConsumer) })\n\t\t}\n\t\tnc.mu.Unlock()\n\t} else {\n\t\tsub.mu.Unlock()\n\t}\n}\n\nvar permissionsRe = regexp.MustCompile(`Subscription to \"(\\S+)\"`)\nvar permissionsQueueRe = regexp.MustCompile(`using queue \"(\\S+)\"`)\n\n// processTransientError is called when the server signals a non terminal error\n// which does not close the connection or trigger a reconnect.\n// This will trigger the async error callback if set.\n// These errors include the following:\n// - permissions violation on publish or subscribe\n// - maximum subscriptions exceeded\nfunc (nc *Conn) processTransientError(err error) {\n\tnc.mu.Lock()\n\tnc.err = err\n\tif errors.Is(err, ErrPermissionViolation) {\n\t\tmatches := permissionsRe.FindStringSubmatch(err.Error())\n\t\tif len(matches) >= 2 {\n\t\t\tqueueMatches := permissionsQueueRe.FindStringSubmatch(err.Error())\n\t\t\tvar q string\n\t\t\tif len(queueMatches) >= 2 {\n\t\t\t\tq = queueMatches[1]\n\t\t\t}\n\t\t\tsubject := matches[1]\n\t\t\tfor _, sub := range nc.subs {\n\t\t\t\tif sub.Subject == subject && sub.Queue == q && sub.permissionsErr == nil {\n\t\t\t\t\tsub.mu.Lock()\n\t\t\t\t\tif sub.errCh != nil {\n\t\t\t\t\t\tsub.errCh <- err\n\t\t\t\t\t}\n\t\t\t\t\tsub.permissionsErr = err\n\t\t\t\t\tsub.mu.Unlock()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif nc.Opts.AsyncErrorCB != nil {\n\t\tnc.ach.push(func() { nc.Opts.AsyncErrorCB(nc, nil, err) })\n\t}\n\tnc.mu.Unlock()\n}\n\n// processAuthError generally processing for auth errors. We want to do retries\n// unless we get the same error again. This allows us for instance to swap credentials\n// and have the app reconnect, but if nothing is changing we should bail.\n// This function will return true if the connection should be closed, false otherwise.\n// Connection lock is held on entry\nfunc (nc *Conn) processAuthError(err error) bool {\n\tnc.err = err\n\tif !nc.initc && nc.Opts.AsyncErrorCB != nil {\n\t\tnc.ach.push(func() { nc.Opts.AsyncErrorCB(nc, nil, err) })\n\t}\n\t// We should give up if we tried twice on this server and got the\n\t// same error. This behavior can be modified using IgnoreAuthErrorAbort.\n\tif nc.current.lastErr == err && !nc.Opts.IgnoreAuthErrorAbort {\n\t\tnc.ar = true\n\t} else {\n\t\tnc.current.lastErr = err\n\t}\n\treturn nc.ar\n}\n\n// flusher is a separate Go routine that will process flush requests for the write\n// bufio. This allows coalescing of writes to the underlying socket.\nfunc (nc *Conn) flusher() {\n\t// Release the wait group\n\tdefer nc.wg.Done()\n\n\t// snapshot the bw and conn since they can change from underneath of us.\n\tnc.mu.Lock()\n\tbw := nc.bw\n\tconn := nc.conn\n\tfch := nc.fch\n\tnc.mu.Unlock()\n\n\tif conn == nil || bw == nil {\n\t\treturn\n\t}\n\n\tfor {\n\t\tif _, ok := <-fch; !ok {\n\t\t\treturn\n\t\t}\n\t\tnc.mu.Lock()\n\n\t\t// Check to see if we should bail out.\n\t\tif !nc.isConnected() || nc.isConnecting() || conn != nc.conn {\n\t\t\tnc.mu.Unlock()\n\t\t\treturn\n\t\t}\n\t\tif bw.buffered() > 0 {\n\t\t\tif err := bw.flush(); err != nil {\n\t\t\t\tif nc.err == nil {\n\t\t\t\t\tnc.err = err\n\t\t\t\t}\n\t\t\t\tif nc.Opts.AsyncErrorCB != nil {\n\t\t\t\t\tnc.ach.push(func() { nc.Opts.AsyncErrorCB(nc, nil, err) })\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tnc.mu.Unlock()\n\t}\n}\n\n// processPing will send an immediate pong protocol response to the\n// server. The server uses this mechanism to detect dead clients.\nfunc (nc *Conn) processPing() {\n\tnc.sendProto(pongProto)\n}\n\n// processPong is used to process responses to the client's ping\n// messages. We use pings for the flush mechanism as well.\nfunc (nc *Conn) processPong() {\n\tvar ch chan struct{}\n\n\tnc.mu.Lock()\n\tif len(nc.pongs) > 0 {\n\t\tch = nc.pongs[0]\n\t\tnc.pongs = append(nc.pongs[:0], nc.pongs[1:]...)\n\t}\n\tnc.pout = 0\n\tnc.mu.Unlock()\n\tif ch != nil {\n\t\tch <- struct{}{}\n\t}\n}\n\n// processOK is a placeholder for processing OK messages.\nfunc (nc *Conn) processOK() {\n\t// do nothing\n}\n\n// processInfo is used to parse the info messages sent\n// from the server.\n// This function may update the server pool.\nfunc (nc *Conn) processInfo(info string) error {\n\tif info == _EMPTY_ {\n\t\treturn nil\n\t}\n\tvar ncInfo serverInfo\n\tif err := json.Unmarshal([]byte(info), &ncInfo); err != nil {\n\t\treturn err\n\t}\n\n\t// Copy content into connection's info structure.\n\tnc.info = ncInfo\n\t// The array could be empty/not present on initial connect,\n\t// if advertise is disabled on that server, or servers that\n\t// did not include themselves in the async INFO protocol.\n\t// If empty, do not remove the implicit servers from the pool.\n\tif len(nc.info.ConnectURLs) == 0 {\n\t\tif !nc.initc && ncInfo.LameDuckMode && nc.Opts.LameDuckModeHandler != nil {\n\t\t\tnc.ach.push(func() { nc.Opts.LameDuckModeHandler(nc) })\n\t\t}\n\t\treturn nil\n\t}\n\t// Note about pool randomization: when the pool was first created,\n\t// it was randomized (if allowed). We keep the order the same (removing\n\t// implicit servers that are no longer sent to us). New URLs are sent\n\t// to us in no specific order so don't need extra randomization.\n\thasNew := false\n\t// This is what we got from the server we are connected to.\n\turls := nc.info.ConnectURLs\n\t// Transform that to a map for easy lookups\n\ttmp := make(map[string]struct{}, len(urls))\n\tfor _, curl := range urls {\n\t\ttmp[curl] = struct{}{}\n\t}\n\t// Walk the pool and removed the implicit servers that are no longer in the\n\t// given array/map\n\tsp := nc.srvPool\n\tfor i := 0; i < len(sp); i++ {\n\t\tsrv := sp[i]\n\t\tcurl := srv.url.Host\n\t\t// Check if this URL is in the INFO protocol\n\t\t_, inInfo := tmp[curl]\n\t\t// Remove from the temp map so that at the end we are left with only\n\t\t// new (or restarted) servers that need to be added to the pool.\n\t\tdelete(tmp, curl)\n\t\t// Keep servers that were set through Options, but also the one that\n\t\t// we are currently connected to (even if it is a discovered server).\n\t\tif !srv.isImplicit || srv.url == nc.current.url {\n\t\t\tcontinue\n\t\t}\n\t\tif !inInfo {\n\t\t\t// Remove from server pool. Keep current order.\n\t\t\tcopy(sp[i:], sp[i+1:])\n\t\t\tnc.srvPool = sp[:len(sp)-1]\n\t\t\tsp = nc.srvPool\n\t\t\ti--\n\t\t}\n\t}\n\t// Figure out if we should save off the current non-IP hostname if we encounter a bare IP.\n\tsaveTLS := nc.current != nil && !hostIsIP(nc.current.url)\n\n\t// If there are any left in the tmp map, these are new (or restarted) servers\n\t// and need to be added to the pool.\n\tfor curl := range tmp {\n\t\t// Before adding, check if this is a new (as in never seen) URL.\n\t\t// This is used to figure out if we invoke the DiscoveredServersCB\n\t\tif _, present := nc.urls[curl]; !present {\n\t\t\thasNew = true\n\t\t}\n\t\tnc.addURLToPool(fmt.Sprintf(\"%s://%s\", nc.connScheme(), curl), true, saveTLS)\n\t}\n\tif hasNew {\n\t\t// Randomize the pool if allowed but leave the first URL in place.\n\t\tif !nc.Opts.NoRandomize {\n\t\t\tnc.shufflePool(1)\n\t\t}\n\t\tif !nc.initc && nc.Opts.DiscoveredServersCB != nil {\n\t\t\tnc.ach.push(func() { nc.Opts.DiscoveredServersCB(nc) })\n\t\t}\n\t}\n\tif !nc.initc && ncInfo.LameDuckMode && nc.Opts.LameDuckModeHandler != nil {\n\t\tnc.ach.push(func() { nc.Opts.LameDuckModeHandler(nc) })\n\t}\n\treturn nil\n}\n\n// processAsyncInfo does the same than processInfo, but is called\n// from the parser. Calls processInfo under connection's lock\n// protection.\nfunc (nc *Conn) processAsyncInfo(info []byte) {\n\tnc.mu.Lock()\n\t// Ignore errors, we will simply not update the server pool...\n\tnc.processInfo(string(info))\n\tnc.mu.Unlock()\n}\n\n// LastError reports the last error encountered via the connection.\n// It can be used reliably within ClosedCB in order to find out reason\n// why connection was closed for example.\nfunc (nc *Conn) LastError() error {\n\tif nc == nil {\n\t\treturn ErrInvalidConnection\n\t}\n\tnc.mu.RLock()\n\terr := nc.err\n\tnc.mu.RUnlock()\n\treturn err\n}\n\n// Check if the given error string is an auth error, and if so returns\n// the corresponding ErrXXX error, nil otherwise\nfunc checkAuthError(e string) error {\n\tif strings.HasPrefix(e, AUTHORIZATION_ERR) {\n\t\treturn ErrAuthorization\n\t}\n\tif strings.HasPrefix(e, AUTHENTICATION_EXPIRED_ERR) {\n\t\treturn ErrAuthExpired\n\t}\n\tif strings.HasPrefix(e, AUTHENTICATION_REVOKED_ERR) {\n\t\treturn ErrAuthRevoked\n\t}\n\tif strings.HasPrefix(e, ACCOUNT_AUTHENTICATION_EXPIRED_ERR) {\n\t\treturn ErrAccountAuthExpired\n\t}\n\treturn nil\n}\n\n// processErr processes any error messages from the server and\n// sets the connection's LastError.\nfunc (nc *Conn) processErr(ie string) {\n\t// Trim, remove quotes\n\tne := normalizeErr(ie)\n\t// convert to lower case.\n\te := strings.ToLower(ne)\n\n\tvar close bool\n\n\t// FIXME(dlc) - process Slow Consumer signals special.\n\tif e == STALE_CONNECTION {\n\t\tclose = nc.processOpErr(ErrStaleConnection)\n\t} else if e == MAX_CONNECTIONS_ERR {\n\t\tclose = nc.processOpErr(ErrMaxConnectionsExceeded)\n\t} else if strings.HasPrefix(e, PERMISSIONS_ERR) {\n\t\tnc.processTransientError(fmt.Errorf(\"%w: %s\", ErrPermissionViolation, ne))\n\t} else if strings.HasPrefix(e, MAX_SUBSCRIPTIONS_ERR) {\n\t\tnc.processTransientError(ErrMaxSubscriptionsExceeded)\n\t} else if authErr := checkAuthError(e); authErr != nil {\n\t\tnc.mu.Lock()\n\t\tclose = nc.processAuthError(authErr)\n\t\tnc.mu.Unlock()\n\t} else {\n\t\tclose = true\n\t\tnc.mu.Lock()\n\t\tnc.err = errors.New(\"nats: \" + ne)\n\t\tnc.mu.Unlock()\n\t}\n\tif close {\n\t\tnc.close(CLOSED, true, nil)\n\t}\n}\n\n// kickFlusher will send a bool on a channel to kick the\n// flush Go routine to flush data to the server.\nfunc (nc *Conn) kickFlusher() {\n\tif nc.bw != nil {\n\t\tselect {\n\t\tcase nc.fch <- struct{}{}:\n\t\tdefault:\n\t\t}\n\t}\n}\n\n// Publish publishes the data argument to the given subject. The data\n// argument is left untouched and needs to be correctly interpreted on\n// the receiver.\nfunc (nc *Conn) Publish(subj string, data []byte) error {\n\treturn nc.publish(subj, _EMPTY_, nil, data)\n}\n\n// Header represents the optional Header for a NATS message,\n// based on the implementation of http.Header.\ntype Header map[string][]string\n\n// Add adds the key, value pair to the header. It is case-sensitive\n// and appends to any existing values associated with key.\nfunc (h Header) Add(key, value string) {\n\th[key] = append(h[key], value)\n}\n\n// Set sets the header entries associated with key to the single\n// element value. It is case-sensitive and replaces any existing\n// values associated with key.\nfunc (h Header) Set(key, value string) {\n\th[key] = []string{value}\n}\n\n// Get gets the first value associated with the given key.\n// It is case-sensitive.\nfunc (h Header) Get(key string) string {\n\tif h == nil {\n\t\treturn _EMPTY_\n\t}\n\tif v := h[key]; v != nil {\n\t\treturn v[0]\n\t}\n\treturn _EMPTY_\n}\n\n// Values returns all values associated with the given key.\n// It is case-sensitive.\nfunc (h Header) Values(key string) []string {\n\treturn h[key]\n}\n\n// Del deletes the values associated with a key.\n// It is case-sensitive.\nfunc (h Header) Del(key string) {\n\tdelete(h, key)\n}\n\n// NewMsg creates a message for publishing that will use headers.\nfunc NewMsg(subject string) *Msg {\n\treturn &Msg{\n\t\tSubject: subject,\n\t\tHeader:  make(Header),\n\t}\n}\n\nconst (\n\thdrLine            = \"NATS/1.0\\r\\n\"\n\tcrlf               = \"\\r\\n\"\n\thdrPreEnd          = len(hdrLine) - len(crlf)\n\tstatusHdr          = \"Status\"\n\tdescrHdr           = \"Description\"\n\tlastConsumerSeqHdr = \"Nats-Last-Consumer\"\n\tlastStreamSeqHdr   = \"Nats-Last-Stream\"\n\tconsumerStalledHdr = \"Nats-Consumer-Stalled\"\n\tnoResponders       = \"503\"\n\tnoMessagesSts      = \"404\"\n\treqTimeoutSts      = \"408\"\n\tjetStream409Sts    = \"409\"\n\tcontrolMsg         = \"100\"\n\tstatusLen          = 3 // e.g. 20x, 40x, 50x\n)\n\n// DecodeHeadersMsg will decode and headers.\nfunc DecodeHeadersMsg(data []byte) (Header, error) {\n\tbr := bufio.NewReaderSize(bytes.NewReader(data), 128)\n\ttp := textproto.NewReader(br)\n\tl, err := tp.ReadLine()\n\tif err != nil || len(l) < hdrPreEnd || l[:hdrPreEnd] != hdrLine[:hdrPreEnd] {\n\t\treturn nil, ErrBadHeaderMsg\n\t}\n\n\tmh, err := readMIMEHeader(tp)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Check if we have an inlined status.\n\tif len(l) > hdrPreEnd {\n\t\tvar description string\n\t\tstatus := strings.TrimSpace(l[hdrPreEnd:])\n\t\tif len(status) != statusLen {\n\t\t\tdescription = strings.TrimSpace(status[statusLen:])\n\t\t\tstatus = status[:statusLen]\n\t\t}\n\t\tmh.Add(statusHdr, status)\n\t\tif len(description) > 0 {\n\t\t\tmh.Add(descrHdr, description)\n\t\t}\n\t}\n\treturn Header(mh), nil\n}\n\n// readMIMEHeader returns a MIMEHeader that preserves the\n// original case of the MIME header, based on the implementation\n// of textproto.ReadMIMEHeader.\n//\n// https://golang.org/pkg/net/textproto/#Reader.ReadMIMEHeader\nfunc readMIMEHeader(tp *textproto.Reader) (textproto.MIMEHeader, error) {\n\tm := make(textproto.MIMEHeader)\n\tfor {\n\t\tkv, err := tp.ReadLine()\n\t\tif len(kv) == 0 {\n\t\t\treturn m, err\n\t\t}\n\n\t\t// Process key fetching original case.\n\t\ti := strings.IndexByte(kv, ':')\n\t\tif i < 0 {\n\t\t\treturn nil, ErrBadHeaderMsg\n\t\t}\n\t\tkey := kv[:i]\n\t\tif key == \"\" {\n\t\t\t// Skip empty keys.\n\t\t\tcontinue\n\t\t}\n\t\ti++\n\t\tfor i < len(kv) && (kv[i] == ' ' || kv[i] == '\\t') {\n\t\t\ti++\n\t\t}\n\t\tm[key] = append(m[key], kv[i:])\n\t\tif err != nil {\n\t\t\treturn m, err\n\t\t}\n\t}\n}\n\n// PublishMsg publishes the Msg structure, which includes the\n// Subject, an optional Reply and an optional Data field.\nfunc (nc *Conn) PublishMsg(m *Msg) error {\n\tif m == nil {\n\t\treturn ErrInvalidMsg\n\t}\n\thdr, err := m.headerBytes()\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nc.publish(m.Subject, m.Reply, hdr, m.Data)\n}\n\n// PublishRequest will perform a Publish() expecting a response on the\n// reply subject. Use Request() for automatically waiting for a response\n// inline.\nfunc (nc *Conn) PublishRequest(subj, reply string, data []byte) error {\n\treturn nc.publish(subj, reply, nil, data)\n}\n\n// Used for handrolled Itoa\nconst digits = \"0123456789\"\n\n// publish is the internal function to publish messages to a nats-server.\n// Sends a protocol data message by queuing into the bufio writer\n// and kicking the flush go routine. These writes should be protected.\nfunc (nc *Conn) publish(subj, reply string, hdr, data []byte) error {\n\tif nc == nil {\n\t\treturn ErrInvalidConnection\n\t}\n\tif subj == \"\" {\n\t\treturn ErrBadSubject\n\t}\n\tnc.mu.Lock()\n\n\t// Check if headers attempted to be sent to server that does not support them.\n\tif len(hdr) > 0 && !nc.info.Headers {\n\t\tnc.mu.Unlock()\n\t\treturn ErrHeadersNotSupported\n\t}\n\n\tif nc.isClosed() {\n\t\tnc.mu.Unlock()\n\t\treturn ErrConnectionClosed\n\t}\n\n\tif nc.isDrainingPubs() {\n\t\tnc.mu.Unlock()\n\t\treturn ErrConnectionDraining\n\t}\n\n\t// Proactively reject payloads over the threshold set by server.\n\tmsgSize := int64(len(data) + len(hdr))\n\t// Skip this check if we are not yet connected (RetryOnFailedConnect)\n\tif !nc.initc && msgSize > nc.info.MaxPayload {\n\t\tnc.mu.Unlock()\n\t\treturn ErrMaxPayload\n\t}\n\n\t// Check if we are reconnecting, and if so check if\n\t// we have exceeded our reconnect outbound buffer limits.\n\tif nc.bw.atLimitIfUsingPending() {\n\t\tnc.mu.Unlock()\n\t\treturn ErrReconnectBufExceeded\n\t}\n\n\tvar mh []byte\n\tif hdr != nil {\n\t\tmh = nc.scratch[:len(_HPUB_P_)]\n\t} else {\n\t\tmh = nc.scratch[1:len(_HPUB_P_)]\n\t}\n\tmh = append(mh, subj...)\n\tmh = append(mh, ' ')\n\tif reply != \"\" {\n\t\tmh = append(mh, reply...)\n\t\tmh = append(mh, ' ')\n\t}\n\n\t// We could be smarter here, but simple loop is ok,\n\t// just avoid strconv in fast path.\n\t// FIXME(dlc) - Find a better way here.\n\t// msgh = strconv.AppendInt(msgh, int64(len(data)), 10)\n\t// go 1.14 some values strconv faster, may be able to switch over.\n\n\tvar b [12]byte\n\tvar i = len(b)\n\n\tif hdr != nil {\n\t\tif len(hdr) > 0 {\n\t\t\tfor l := len(hdr); l > 0; l /= 10 {\n\t\t\t\ti--\n\t\t\t\tb[i] = digits[l%10]\n\t\t\t}\n\t\t} else {\n\t\t\ti--\n\t\t\tb[i] = digits[0]\n\t\t}\n\t\tmh = append(mh, b[i:]...)\n\t\tmh = append(mh, ' ')\n\t\t// reset for below.\n\t\ti = len(b)\n\t}\n\n\tif msgSize > 0 {\n\t\tfor l := msgSize; l > 0; l /= 10 {\n\t\t\ti--\n\t\t\tb[i] = digits[l%10]\n\t\t}\n\t} else {\n\t\ti--\n\t\tb[i] = digits[0]\n\t}\n\n\tmh = append(mh, b[i:]...)\n\tmh = append(mh, _CRLF_...)\n\n\tif err := nc.bw.appendBufs(mh, hdr, data, _CRLF_BYTES_); err != nil {\n\t\tnc.mu.Unlock()\n\t\treturn err\n\t}\n\n\tnc.OutMsgs++\n\tnc.OutBytes += uint64(len(data) + len(hdr))\n\n\tif len(nc.fch) == 0 {\n\t\tnc.kickFlusher()\n\t}\n\tnc.mu.Unlock()\n\treturn nil\n}\n\n// respHandler is the global response handler. It will look up\n// the appropriate channel based on the last token and place\n// the message on the channel if possible.\nfunc (nc *Conn) respHandler(m *Msg) {\n\tnc.mu.Lock()\n\n\t// Just return if closed.\n\tif nc.isClosed() {\n\t\tnc.mu.Unlock()\n\t\treturn\n\t}\n\n\tvar mch chan *Msg\n\n\t// Grab mch\n\trt := nc.respToken(m.Subject)\n\tif rt != _EMPTY_ {\n\t\tmch = nc.respMap[rt]\n\t\t// Delete the key regardless, one response only.\n\t\tdelete(nc.respMap, rt)\n\t} else if len(nc.respMap) == 1 {\n\t\t// If the server has rewritten the subject, the response token (rt)\n\t\t// will not match (could be the case with JetStream). If that is the\n\t\t// case and there is a single entry, use that.\n\t\tfor k, v := range nc.respMap {\n\t\t\tmch = v\n\t\t\tdelete(nc.respMap, k)\n\t\t\tbreak\n\t\t}\n\t}\n\tnc.mu.Unlock()\n\n\t// Don't block, let Request timeout instead, mch is\n\t// buffered and we should delete the key before a\n\t// second response is processed.\n\tselect {\n\tcase mch <- m:\n\tdefault:\n\t\treturn\n\t}\n}\n\n// Helper to setup and send new request style requests. Return the chan to receive the response.\nfunc (nc *Conn) createNewRequestAndSend(subj string, hdr, data []byte) (chan *Msg, string, error) {\n\tnc.mu.Lock()\n\t// Create new literal Inbox and map to a chan msg.\n\tmch := make(chan *Msg, RequestChanLen)\n\trespInbox := nc.newRespInbox()\n\ttoken := respInbox[nc.respSubLen:]\n\n\tnc.respMap[token] = mch\n\tif nc.respMux == nil {\n\t\t// Create the response subscription we will use for all new style responses.\n\t\t// This will be on an _INBOX with an additional terminal token. The subscription\n\t\t// will be on a wildcard.\n\t\ts, err := nc.subscribeLocked(nc.respSub, _EMPTY_, nc.respHandler, nil, nil, false, nil)\n\t\tif err != nil {\n\t\t\tnc.mu.Unlock()\n\t\t\treturn nil, token, err\n\t\t}\n\t\tnc.respMux = s\n\t}\n\tnc.mu.Unlock()\n\n\tif err := nc.publish(subj, respInbox, hdr, data); err != nil {\n\t\treturn nil, token, err\n\t}\n\n\treturn mch, token, nil\n}\n\n// RequestMsg will send a request payload including optional headers and deliver\n// the response message, or an error, including a timeout if no message was received properly.\nfunc (nc *Conn) RequestMsg(msg *Msg, timeout time.Duration) (*Msg, error) {\n\tif msg == nil {\n\t\treturn nil, ErrInvalidMsg\n\t}\n\thdr, err := msg.headerBytes()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn nc.request(msg.Subject, hdr, msg.Data, timeout)\n}\n\n// Request will send a request payload and deliver the response message,\n// or an error, including a timeout if no message was received properly.\nfunc (nc *Conn) Request(subj string, data []byte, timeout time.Duration) (*Msg, error) {\n\treturn nc.request(subj, nil, data, timeout)\n}\n\nfunc (nc *Conn) useOldRequestStyle() bool {\n\tnc.mu.RLock()\n\tr := nc.Opts.UseOldRequestStyle\n\tnc.mu.RUnlock()\n\treturn r\n}\n\nfunc (nc *Conn) request(subj string, hdr, data []byte, timeout time.Duration) (*Msg, error) {\n\tif nc == nil {\n\t\treturn nil, ErrInvalidConnection\n\t}\n\n\tvar m *Msg\n\tvar err error\n\n\tif nc.useOldRequestStyle() {\n\t\tm, err = nc.oldRequest(subj, hdr, data, timeout)\n\t} else {\n\t\tm, err = nc.newRequest(subj, hdr, data, timeout)\n\t}\n\n\t// Check for no responder status.\n\tif err == nil && len(m.Data) == 0 && m.Header.Get(statusHdr) == noResponders {\n\t\tm, err = nil, ErrNoResponders\n\t}\n\treturn m, err\n}\n\nfunc (nc *Conn) newRequest(subj string, hdr, data []byte, timeout time.Duration) (*Msg, error) {\n\tmch, token, err := nc.createNewRequestAndSend(subj, hdr, data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tt := globalTimerPool.Get(timeout)\n\tdefer globalTimerPool.Put(t)\n\n\tvar ok bool\n\tvar msg *Msg\n\n\tselect {\n\tcase msg, ok = <-mch:\n\t\tif !ok {\n\t\t\treturn nil, ErrConnectionClosed\n\t\t}\n\tcase <-t.C:\n\t\tnc.mu.Lock()\n\t\tdelete(nc.respMap, token)\n\t\tnc.mu.Unlock()\n\t\treturn nil, ErrTimeout\n\t}\n\n\treturn msg, nil\n}\n\n// oldRequest will create an Inbox and perform a Request() call\n// with the Inbox reply and return the first reply received.\n// This is optimized for the case of multiple responses.\nfunc (nc *Conn) oldRequest(subj string, hdr, data []byte, timeout time.Duration) (*Msg, error) {\n\tinbox := nc.NewInbox()\n\tch := make(chan *Msg, RequestChanLen)\n\n\ts, err := nc.subscribe(inbox, _EMPTY_, nil, ch, nil, true, nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ts.AutoUnsubscribe(1)\n\tdefer s.Unsubscribe()\n\n\terr = nc.publish(subj, inbox, hdr, data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn s.NextMsg(timeout)\n}\n\n// InboxPrefix is the prefix for all inbox subjects.\nconst (\n\tInboxPrefix    = \"_INBOX.\"\n\tinboxPrefixLen = len(InboxPrefix)\n\treplySuffixLen = 8 // Gives us 62^8\n\trdigits        = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n\tbase           = 62\n)\n\n// NewInbox will return an inbox string which can be used for directed replies from\n// subscribers. These are guaranteed to be unique, but can be shared and subscribed\n// to by others.\nfunc NewInbox() string {\n\tvar b [inboxPrefixLen + nuidSize]byte\n\tpres := b[:inboxPrefixLen]\n\tcopy(pres, InboxPrefix)\n\tns := b[inboxPrefixLen:]\n\tcopy(ns, nuid.Next())\n\treturn string(b[:])\n}\n\n// Create a new inbox that is prefix aware.\nfunc (nc *Conn) NewInbox() string {\n\tif nc.Opts.InboxPrefix == _EMPTY_ {\n\t\treturn NewInbox()\n\t}\n\n\tvar sb strings.Builder\n\tsb.WriteString(nc.Opts.InboxPrefix)\n\tsb.WriteByte('.')\n\tsb.WriteString(nuid.Next())\n\treturn sb.String()\n}\n\n// Function to init new response structures.\nfunc (nc *Conn) initNewResp() {\n\tnc.respSubPrefix = fmt.Sprintf(\"%s.\", nc.NewInbox())\n\tnc.respSubLen = len(nc.respSubPrefix)\n\tnc.respSub = fmt.Sprintf(\"%s*\", nc.respSubPrefix)\n\tnc.respMap = make(map[string]chan *Msg)\n\tnc.respRand = rand.New(rand.NewSource(time.Now().UnixNano()))\n}\n\n// newRespInbox creates a new literal response subject\n// that will trigger the mux subscription handler.\n// Lock should be held.\nfunc (nc *Conn) newRespInbox() string {\n\tif nc.respMap == nil {\n\t\tnc.initNewResp()\n\t}\n\n\tvar sb strings.Builder\n\tsb.WriteString(nc.respSubPrefix)\n\n\trn := nc.respRand.Int63()\n\tfor i := 0; i < replySuffixLen; i++ {\n\t\tsb.WriteByte(rdigits[rn%base])\n\t\trn /= base\n\t}\n\n\treturn sb.String()\n}\n\n// NewRespInbox is the new format used for _INBOX.\nfunc (nc *Conn) NewRespInbox() string {\n\tnc.mu.Lock()\n\ts := nc.newRespInbox()\n\tnc.mu.Unlock()\n\treturn s\n}\n\n// respToken will return the last token of a literal response inbox\n// which we use for the message channel lookup. This needs to verify the subject\n// prefix matches to protect itself against the server changing the subject.\n// Lock should be held.\nfunc (nc *Conn) respToken(respInbox string) string {\n\tif token, found := strings.CutPrefix(respInbox, nc.respSubPrefix); found {\n\t\treturn token\n\t}\n\treturn \"\"\n}\n\n// Subscribe will express interest in the given subject. The subject\n// can have wildcards.\n// There are two type of wildcards: * for partial, and > for full.\n// A subscription on subject time.*.east would receive messages sent to time.us.east and time.eu.east.\n// A subscription on subject time.us.> would receive messages sent to\n// time.us.east and time.us.east.atlanta, while time.us.* would only match time.us.east\n// since it can't match more than one token.\n// Messages will be delivered to the associated MsgHandler.\nfunc (nc *Conn) Subscribe(subj string, cb MsgHandler) (*Subscription, error) {\n\treturn nc.subscribe(subj, _EMPTY_, cb, nil, nil, false, nil)\n}\n\n// ChanSubscribe will express interest in the given subject and place\n// all messages received on the channel.\n// You should not close the channel until sub.Unsubscribe() has been called.\nfunc (nc *Conn) ChanSubscribe(subj string, ch chan *Msg) (*Subscription, error) {\n\treturn nc.subscribe(subj, _EMPTY_, nil, ch, nil, false, nil)\n}\n\n// ChanQueueSubscribe will express interest in the given subject.\n// All subscribers with the same queue name will form the queue group\n// and only one member of the group will be selected to receive any given message,\n// which will be placed on the channel.\n// You should not close the channel until sub.Unsubscribe() has been called.\n// Note: This is the same than QueueSubscribeSyncWithChan.\nfunc (nc *Conn) ChanQueueSubscribe(subj, group string, ch chan *Msg) (*Subscription, error) {\n\treturn nc.subscribe(subj, group, nil, ch, nil, false, nil)\n}\n\n// SubscribeSync will express interest on the given subject. Messages will\n// be received synchronously using Subscription.NextMsg().\nfunc (nc *Conn) SubscribeSync(subj string) (*Subscription, error) {\n\tif nc == nil {\n\t\treturn nil, ErrInvalidConnection\n\t}\n\tmch := make(chan *Msg, nc.Opts.SubChanLen)\n\tvar errCh chan error\n\tif nc.Opts.PermissionErrOnSubscribe {\n\t\terrCh = make(chan error, 100)\n\t}\n\treturn nc.subscribe(subj, _EMPTY_, nil, mch, errCh, true, nil)\n}\n\n// QueueSubscribe creates an asynchronous queue subscriber on the given subject.\n// All subscribers with the same queue name will form the queue group and\n// only one member of the group will be selected to receive any given\n// message asynchronously.\nfunc (nc *Conn) QueueSubscribe(subj, queue string, cb MsgHandler) (*Subscription, error) {\n\treturn nc.subscribe(subj, queue, cb, nil, nil, false, nil)\n}\n\n// QueueSubscribeSync creates a synchronous queue subscriber on the given\n// subject. All subscribers with the same queue name will form the queue\n// group and only one member of the group will be selected to receive any\n// given message synchronously using Subscription.NextMsg().\nfunc (nc *Conn) QueueSubscribeSync(subj, queue string) (*Subscription, error) {\n\tmch := make(chan *Msg, nc.Opts.SubChanLen)\n\tvar errCh chan error\n\tif nc.Opts.PermissionErrOnSubscribe {\n\t\terrCh = make(chan error, 100)\n\t}\n\treturn nc.subscribe(subj, queue, nil, mch, errCh, true, nil)\n}\n\n// QueueSubscribeSyncWithChan will express interest in the given subject.\n// All subscribers with the same queue name will form the queue group\n// and only one member of the group will be selected to receive any given message,\n// which will be placed on the channel.\n// You should not close the channel until sub.Unsubscribe() has been called.\n// Note: This is the same than ChanQueueSubscribe.\nfunc (nc *Conn) QueueSubscribeSyncWithChan(subj, queue string, ch chan *Msg) (*Subscription, error) {\n\treturn nc.subscribe(subj, queue, nil, ch, nil, false, nil)\n}\n\n// badSubject will do quick test on whether a subject is acceptable.\n// Spaces are not allowed and all tokens should be > 0 in len.\nfunc badSubject(subj string) bool {\n\tif strings.ContainsAny(subj, \" \\t\\r\\n\") {\n\t\treturn true\n\t}\n\ttokens := strings.Split(subj, \".\")\n\tfor _, t := range tokens {\n\t\tif len(t) == 0 {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// badQueue will check a queue name for whitespace.\nfunc badQueue(qname string) bool {\n\treturn strings.ContainsAny(qname, \" \\t\\r\\n\")\n}\n\n// subscribe is the internal subscribe function that indicates interest in a subject.\nfunc (nc *Conn) subscribe(subj, queue string, cb MsgHandler, ch chan *Msg, errCh chan (error), isSync bool, js *jsSub) (*Subscription, error) {\n\tif nc == nil {\n\t\treturn nil, ErrInvalidConnection\n\t}\n\tnc.mu.Lock()\n\tdefer nc.mu.Unlock()\n\treturn nc.subscribeLocked(subj, queue, cb, ch, errCh, isSync, js)\n}\n\nfunc (nc *Conn) subscribeLocked(subj, queue string, cb MsgHandler, ch chan *Msg, errCh chan (error), isSync bool, js *jsSub) (*Subscription, error) {\n\tif nc == nil {\n\t\treturn nil, ErrInvalidConnection\n\t}\n\tif badSubject(subj) {\n\t\treturn nil, ErrBadSubject\n\t}\n\tif queue != _EMPTY_ && badQueue(queue) {\n\t\treturn nil, ErrBadQueueName\n\t}\n\n\t// Check for some error conditions.\n\tif nc.isClosed() {\n\t\treturn nil, ErrConnectionClosed\n\t}\n\tif nc.isDraining() {\n\t\treturn nil, ErrConnectionDraining\n\t}\n\n\tif cb == nil && ch == nil {\n\t\treturn nil, ErrBadSubscription\n\t}\n\n\tsub := &Subscription{\n\t\tSubject: subj,\n\t\tQueue:   queue,\n\t\tmcb:     cb,\n\t\tconn:    nc,\n\t\tjsi:     js,\n\t}\n\t// Set pending limits.\n\tif ch != nil {\n\t\tsub.pMsgsLimit = cap(ch)\n\t} else {\n\t\tsub.pMsgsLimit = DefaultSubPendingMsgsLimit\n\t}\n\tsub.pBytesLimit = DefaultSubPendingBytesLimit\n\n\t// If we have an async callback, start up a sub specific\n\t// Go routine to deliver the messages.\n\tvar sr bool\n\tif cb != nil {\n\t\tsub.typ = AsyncSubscription\n\t\tsub.pCond = sync.NewCond(&sub.mu)\n\t\tsr = true\n\t} else if !isSync {\n\t\tsub.typ = ChanSubscription\n\t\tsub.mch = ch\n\t} else { // Sync Subscription\n\t\tsub.typ = SyncSubscription\n\t\tsub.mch = ch\n\t\tsub.errCh = errCh\n\t}\n\n\tnc.subsMu.Lock()\n\tnc.ssid++\n\tsub.sid = nc.ssid\n\tnc.subs[sub.sid] = sub\n\tnc.subsMu.Unlock()\n\n\t// Let's start the go routine now that it is fully setup and registered.\n\tif sr {\n\t\tgo nc.waitForMsgs(sub)\n\t}\n\n\t// We will send these for all subs when we reconnect\n\t// so that we can suppress here if reconnecting.\n\tif !nc.isReconnecting() {\n\t\tnc.bw.appendString(fmt.Sprintf(subProto, subj, queue, sub.sid))\n\t\tnc.kickFlusher()\n\t}\n\n\tsub.changeSubStatus(SubscriptionActive)\n\treturn sub, nil\n}\n\n// NumSubscriptions returns active number of subscriptions.\nfunc (nc *Conn) NumSubscriptions() int {\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\treturn len(nc.subs)\n}\n\n// Lock for nc should be held here upon entry\nfunc (nc *Conn) removeSub(s *Subscription) {\n\tnc.subsMu.Lock()\n\tdelete(nc.subs, s.sid)\n\tnc.subsMu.Unlock()\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\t// Release callers on NextMsg for SyncSubscription only\n\tif s.mch != nil && s.typ == SyncSubscription {\n\t\tclose(s.mch)\n\t}\n\ts.mch = nil\n\n\t// If JS subscription then stop HB timer.\n\tif jsi := s.jsi; jsi != nil {\n\t\tif jsi.hbc != nil {\n\t\t\tjsi.hbc.Stop()\n\t\t\tjsi.hbc = nil\n\t\t}\n\t\tif jsi.csfct != nil {\n\t\t\tjsi.csfct.Stop()\n\t\t\tjsi.csfct = nil\n\t\t}\n\t}\n\n\tif s.typ != AsyncSubscription {\n\t\tdone := s.pDone\n\t\tif done != nil {\n\t\t\tdone(s.Subject)\n\t\t}\n\t}\n\t// Mark as invalid\n\ts.closed = true\n\ts.changeSubStatus(SubscriptionClosed)\n\tif s.pCond != nil {\n\t\ts.pCond.Broadcast()\n\t}\n}\n\n// SubscriptionType is the type of the Subscription.\ntype SubscriptionType int\n\n// The different types of subscription types.\nconst (\n\tAsyncSubscription = SubscriptionType(iota)\n\tSyncSubscription\n\tChanSubscription\n\tNilSubscription\n\tPullSubscription\n)\n\n// Type returns the type of Subscription.\nfunc (s *Subscription) Type() SubscriptionType {\n\tif s == nil {\n\t\treturn NilSubscription\n\t}\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\t// Pull subscriptions are really a SyncSubscription and we want this\n\t// type to be set internally for all delivered messages management, etc..\n\t// So check when to return PullSubscription to the user.\n\tif s.jsi != nil && s.jsi.pull {\n\t\treturn PullSubscription\n\t}\n\treturn s.typ\n}\n\n// IsValid returns a boolean indicating whether the subscription\n// is still active. This will return false if the subscription has\n// already been closed.\nfunc (s *Subscription) IsValid() bool {\n\tif s == nil {\n\t\treturn false\n\t}\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\treturn s.conn != nil && !s.closed\n}\n\n// Drain will remove interest but continue callbacks until all messages\n// have been processed.\n//\n// For a JetStream subscription, if the library has created the JetStream\n// consumer, the library will send a DeleteConsumer request to the server\n// when the Drain operation completes. If a failure occurs when deleting\n// the JetStream consumer, an error will be reported to the asynchronous\n// error callback.\n// If you do not wish the JetStream consumer to be automatically deleted,\n// ensure that the consumer is not created by the library, which means\n// create the consumer with AddConsumer and bind to this consumer.\nfunc (s *Subscription) Drain() error {\n\tif s == nil {\n\t\treturn ErrBadSubscription\n\t}\n\ts.mu.Lock()\n\tconn := s.conn\n\ts.mu.Unlock()\n\tif conn == nil {\n\t\treturn ErrBadSubscription\n\t}\n\treturn conn.unsubscribe(s, 0, true)\n}\n\n// IsDraining returns a boolean indicating whether the subscription\n// is being drained.\n// This will return false if the subscription has already been closed.\nfunc (s *Subscription) IsDraining() bool {\n\tif s == nil {\n\t\treturn false\n\t}\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\treturn s.draining\n}\n\n// StatusChanged returns a channel on which given list of subscription status\n// changes will be sent. If no status is provided, all status changes will be sent.\n// Available statuses are SubscriptionActive, SubscriptionDraining, SubscriptionClosed,\n// and SubscriptionSlowConsumer.\n// The returned channel will be closed when the subscription is closed.\nfunc (s *Subscription) StatusChanged(statuses ...SubStatus) <-chan SubStatus {\n\tif len(statuses) == 0 {\n\t\tstatuses = []SubStatus{SubscriptionActive, SubscriptionDraining, SubscriptionClosed, SubscriptionSlowConsumer}\n\t}\n\tch := make(chan SubStatus, 10)\n\tfor _, status := range statuses {\n\t\ts.registerStatusChangeListener(status, ch)\n\t\t// initial status\n\t\tif status == s.status {\n\t\t\tch <- status\n\t\t}\n\t}\n\treturn ch\n}\n\n// registerStatusChangeListener registers a channel waiting for a specific status change event.\n// Status change events are non-blocking - if no receiver is waiting for the status change,\n// it will not be sent on the channel. Closed channels are ignored.\nfunc (s *Subscription) registerStatusChangeListener(status SubStatus, ch chan SubStatus) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\tif s.statListeners == nil {\n\t\ts.statListeners = make(map[chan SubStatus][]SubStatus)\n\t}\n\tif _, ok := s.statListeners[ch]; !ok {\n\t\ts.statListeners[ch] = make([]SubStatus, 0)\n\t}\n\ts.statListeners[ch] = append(s.statListeners[ch], status)\n}\n\n// sendStatusEvent sends subscription status event to all channels.\n// If there is no listener, sendStatusEvent\n// will not block. Lock should be held entering.\nfunc (s *Subscription) sendStatusEvent(status SubStatus) {\n\tfor ch, statuses := range s.statListeners {\n\t\tif !containsStatus(statuses, status) {\n\t\t\tcontinue\n\t\t}\n\t\t// only send event if someone's listening\n\t\tselect {\n\t\tcase ch <- status:\n\t\tdefault:\n\t\t}\n\t\tif status == SubscriptionClosed {\n\t\t\tclose(ch)\n\t\t}\n\t}\n}\n\nfunc containsStatus(statuses []SubStatus, status SubStatus) bool {\n\tfor _, s := range statuses {\n\t\tif s == status {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// changeSubStatus changes subscription status and sends events\n// to all listeners. Lock should be held entering.\nfunc (s *Subscription) changeSubStatus(status SubStatus) {\n\tif s == nil {\n\t\treturn\n\t}\n\ts.sendStatusEvent(status)\n\ts.status = status\n}\n\n// Unsubscribe will remove interest in the given subject.\n//\n// For a JetStream subscription, if the library has created the JetStream\n// consumer, it will send a DeleteConsumer request to the server (if the\n// unsubscribe itself was successful). If the delete operation fails, the\n// error will be returned.\n// If you do not wish the JetStream consumer to be automatically deleted,\n// ensure that the consumer is not created by the library, which means\n// create the consumer with AddConsumer and bind to this consumer (using\n// the nats.Bind() option).\nfunc (s *Subscription) Unsubscribe() error {\n\tif s == nil {\n\t\treturn ErrBadSubscription\n\t}\n\ts.mu.Lock()\n\tconn := s.conn\n\tclosed := s.closed\n\tdc := s.jsi != nil && s.jsi.dc\n\ts.mu.Unlock()\n\tif conn == nil || conn.IsClosed() {\n\t\treturn ErrConnectionClosed\n\t}\n\tif closed {\n\t\treturn ErrBadSubscription\n\t}\n\tif conn.IsDraining() {\n\t\treturn ErrConnectionDraining\n\t}\n\terr := conn.unsubscribe(s, 0, false)\n\tif err == nil && dc {\n\t\terr = s.deleteConsumer()\n\t}\n\treturn err\n}\n\n// checkDrained will watch for a subscription to be fully drained\n// and then remove it.\nfunc (nc *Conn) checkDrained(sub *Subscription) {\n\tdefer func() {\n\t\tsub.mu.Lock()\n\t\tdefer sub.mu.Unlock()\n\t\tsub.draining = false\n\t}()\n\tif nc == nil || sub == nil {\n\t\treturn\n\t}\n\n\t// This allows us to know that whatever we have in the client pending\n\t// is correct and the server will not send additional information.\n\tnc.Flush()\n\n\tsub.mu.Lock()\n\t// For JS subscriptions, check if we are going to delete the\n\t// JS consumer when drain completes.\n\tdc := sub.jsi != nil && sub.jsi.dc\n\tsub.mu.Unlock()\n\n\t// Once we are here we just wait for Pending to reach 0 or\n\t// any other state to exit this go routine.\n\tfor {\n\t\t// check connection is still valid.\n\t\tif nc.IsClosed() {\n\t\t\treturn\n\t\t}\n\n\t\t// Check subscription state\n\t\tsub.mu.Lock()\n\t\tconn := sub.conn\n\t\tclosed := sub.closed\n\t\tpMsgs := sub.pMsgs\n\t\tsub.mu.Unlock()\n\n\t\tif conn == nil || closed || pMsgs == 0 {\n\t\t\tnc.mu.Lock()\n\t\t\tnc.removeSub(sub)\n\t\t\tnc.mu.Unlock()\n\t\t\tif dc {\n\t\t\t\tif err := sub.deleteConsumer(); err != nil {\n\t\t\t\t\tnc.mu.Lock()\n\t\t\t\t\tif errCB := nc.Opts.AsyncErrorCB; errCB != nil {\n\t\t\t\t\t\tnc.ach.push(func() { errCB(nc, sub, err) })\n\t\t\t\t\t}\n\t\t\t\t\tnc.mu.Unlock()\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\ttime.Sleep(100 * time.Millisecond)\n\t}\n}\n\n// AutoUnsubscribe will issue an automatic Unsubscribe that is\n// processed by the server when max messages have been received.\n// This can be useful when sending a request to an unknown number\n// of subscribers.\nfunc (s *Subscription) AutoUnsubscribe(max int) error {\n\tif s == nil {\n\t\treturn ErrBadSubscription\n\t}\n\ts.mu.Lock()\n\tconn := s.conn\n\tclosed := s.closed\n\ts.mu.Unlock()\n\tif conn == nil || closed {\n\t\treturn ErrBadSubscription\n\t}\n\treturn conn.unsubscribe(s, max, false)\n}\n\n// SetClosedHandler will set the closed handler for when a subscription\n// is closed (either unsubscribed or drained).\nfunc (s *Subscription) SetClosedHandler(handler func(subject string)) {\n\ts.mu.Lock()\n\ts.pDone = handler\n\ts.mu.Unlock()\n}\n\n// unsubscribe performs the low level unsubscribe to the server.\n// Use Subscription.Unsubscribe()\nfunc (nc *Conn) unsubscribe(sub *Subscription, max int, drainMode bool) error {\n\tvar maxStr string\n\tif max > 0 {\n\t\tsub.mu.Lock()\n\t\tsub.max = uint64(max)\n\t\tif sub.delivered < sub.max {\n\t\t\tmaxStr = strconv.Itoa(max)\n\t\t}\n\t\tsub.mu.Unlock()\n\t}\n\n\tnc.mu.Lock()\n\t// ok here, but defer is expensive\n\tdefer nc.mu.Unlock()\n\n\tif nc.isClosed() {\n\t\treturn ErrConnectionClosed\n\t}\n\n\tnc.subsMu.RLock()\n\ts := nc.subs[sub.sid]\n\tnc.subsMu.RUnlock()\n\t// Already unsubscribed\n\tif s == nil {\n\t\treturn nil\n\t}\n\n\tif maxStr == _EMPTY_ && !drainMode {\n\t\tnc.removeSub(s)\n\t}\n\n\tif drainMode {\n\t\ts.mu.Lock()\n\t\ts.draining = true\n\t\tsub.changeSubStatus(SubscriptionDraining)\n\t\ts.mu.Unlock()\n\t\tgo nc.checkDrained(sub)\n\t}\n\n\t// We will send these for all subs when we reconnect\n\t// so that we can suppress here.\n\tif !nc.isReconnecting() {\n\t\tnc.bw.appendString(fmt.Sprintf(unsubProto, s.sid, maxStr))\n\t\tnc.kickFlusher()\n\t}\n\n\t// For JetStream subscriptions cancel the attached context if there is any.\n\tvar cancel func()\n\tsub.mu.Lock()\n\tjsi := sub.jsi\n\tif jsi != nil {\n\t\tcancel = jsi.cancel\n\t\tjsi.cancel = nil\n\t}\n\tsub.mu.Unlock()\n\tif cancel != nil {\n\t\tcancel()\n\t}\n\n\treturn nil\n}\n\n// NextMsg will return the next message available to a synchronous subscriber\n// or block until one is available. An error is returned if the subscription is invalid (ErrBadSubscription),\n// the connection is closed (ErrConnectionClosed), the timeout is reached (ErrTimeout),\n// or if there were no responders (ErrNoResponders) when used in the context of a request/reply.\nfunc (s *Subscription) NextMsg(timeout time.Duration) (*Msg, error) {\n\tif s == nil {\n\t\treturn nil, ErrBadSubscription\n\t}\n\n\ts.mu.Lock()\n\terr := s.validateNextMsgState(false)\n\tif err != nil {\n\t\ts.mu.Unlock()\n\t\treturn nil, err\n\t}\n\n\t// snapshot\n\tmch := s.mch\n\ts.mu.Unlock()\n\n\tvar ok bool\n\tvar msg *Msg\n\n\t// If something is available right away, let's optimize that case.\n\tselect {\n\tcase msg, ok = <-mch:\n\t\tif !ok {\n\t\t\treturn nil, s.getNextMsgErr()\n\t\t}\n\t\tif err := s.processNextMsgDelivered(msg); err != nil {\n\t\t\treturn nil, err\n\t\t} else {\n\t\t\treturn msg, nil\n\t\t}\n\tdefault:\n\t}\n\n\t// If we are here a message was not immediately available, so lets loop\n\t// with a timeout.\n\n\tt := globalTimerPool.Get(timeout)\n\tdefer globalTimerPool.Put(t)\n\n\tif s.errCh != nil {\n\t\tselect {\n\t\tcase msg, ok = <-mch:\n\t\t\tif !ok {\n\t\t\t\treturn nil, s.getNextMsgErr()\n\t\t\t}\n\t\t\tif err := s.processNextMsgDelivered(msg); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\tcase err := <-s.errCh:\n\t\t\treturn nil, err\n\t\tcase <-t.C:\n\t\t\treturn nil, ErrTimeout\n\t\t}\n\t} else {\n\t\tselect {\n\t\tcase msg, ok = <-mch:\n\t\t\tif !ok {\n\t\t\t\treturn nil, s.getNextMsgErr()\n\t\t\t}\n\t\t\tif err := s.processNextMsgDelivered(msg); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\tcase <-t.C:\n\t\t\treturn nil, ErrTimeout\n\t\t}\n\t}\n\n\treturn msg, nil\n}\n\n// nextMsgNoTimeout works similarly to Subscription.NextMsg() but will not\n// time out. It is only used internally for non-timeout subscription iterator.\nfunc (s *Subscription) nextMsgNoTimeout() (*Msg, error) {\n\tif s == nil {\n\t\treturn nil, ErrBadSubscription\n\t}\n\n\ts.mu.Lock()\n\terr := s.validateNextMsgState(false)\n\tif err != nil {\n\t\ts.mu.Unlock()\n\t\treturn nil, err\n\t}\n\n\t// snapshot\n\tmch := s.mch\n\ts.mu.Unlock()\n\n\tvar ok bool\n\tvar msg *Msg\n\n\t// If something is available right away, let's optimize that case.\n\tselect {\n\tcase msg, ok = <-mch:\n\t\tif !ok {\n\t\t\treturn nil, s.getNextMsgErr()\n\t\t}\n\t\tif err := s.processNextMsgDelivered(msg); err != nil {\n\t\t\treturn nil, err\n\t\t} else {\n\t\t\treturn msg, nil\n\t\t}\n\tdefault:\n\t}\n\n\tif s.errCh != nil {\n\t\tselect {\n\t\tcase msg, ok = <-mch:\n\t\t\tif !ok {\n\t\t\t\treturn nil, s.getNextMsgErr()\n\t\t\t}\n\t\t\tif err := s.processNextMsgDelivered(msg); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\tcase err := <-s.errCh:\n\t\t\treturn nil, err\n\t\t}\n\t} else {\n\t\tmsg, ok = <-mch\n\t\tif !ok {\n\t\t\treturn nil, s.getNextMsgErr()\n\t\t}\n\t\tif err := s.processNextMsgDelivered(msg); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn msg, nil\n}\n\n// validateNextMsgState checks whether the subscription is in a valid\n// state to call NextMsg and be delivered another message synchronously.\n// This should be called while holding the lock.\nfunc (s *Subscription) validateNextMsgState(pullSubInternal bool) error {\n\tif s.connClosed {\n\t\treturn ErrConnectionClosed\n\t}\n\tif s.mch == nil {\n\t\tif s.max > 0 && s.delivered >= s.max {\n\t\t\treturn ErrMaxMessages\n\t\t} else if s.closed {\n\t\t\treturn ErrBadSubscription\n\t\t}\n\t}\n\tif s.mcb != nil {\n\t\treturn ErrSyncSubRequired\n\t}\n\t// if this subscription previously had a permissions error\n\t// and no reconnect has been attempted, return the permissions error\n\t// since the subscription does not exist on the server\n\tif s.conn.Opts.PermissionErrOnSubscribe && s.permissionsErr != nil {\n\t\treturn s.permissionsErr\n\t}\n\tif s.sc {\n\t\ts.changeSubStatus(SubscriptionActive)\n\t\ts.sc = false\n\t\treturn ErrSlowConsumer\n\t}\n\t// Unless this is from an internal call, reject use of this API.\n\t// Users should use Fetch() instead.\n\tif !pullSubInternal && s.jsi != nil && s.jsi.pull {\n\t\treturn ErrTypeSubscription\n\t}\n\treturn nil\n}\n\n// This is called when the sync channel has been closed.\n// The error returned will be either connection or subscription\n// closed depending on what caused NextMsg() to fail.\nfunc (s *Subscription) getNextMsgErr() error {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\tif s.connClosed {\n\t\treturn ErrConnectionClosed\n\t}\n\treturn ErrBadSubscription\n}\n\n// processNextMsgDelivered takes a message and applies the needed\n// accounting to the stats from the subscription, returning an\n// error in case we have the maximum number of messages have been\n// delivered already. It should not be called while holding the lock.\nfunc (s *Subscription) processNextMsgDelivered(msg *Msg) error {\n\ts.mu.Lock()\n\tnc := s.conn\n\tmax := s.max\n\n\tvar fcReply string\n\t// Update some stats.\n\ts.delivered++\n\tdelivered := s.delivered\n\tif s.jsi != nil {\n\t\tfcReply = s.checkForFlowControlResponse()\n\t}\n\n\tif s.typ == SyncSubscription {\n\t\ts.pMsgs--\n\t\ts.pBytes -= len(msg.Data)\n\t}\n\ts.mu.Unlock()\n\n\tif fcReply != _EMPTY_ {\n\t\tnc.Publish(fcReply, nil)\n\t}\n\n\tif max > 0 {\n\t\tif delivered > max {\n\t\t\treturn ErrMaxMessages\n\t\t}\n\t\t// Remove subscription if we have reached max.\n\t\tif delivered == max {\n\t\t\tnc.mu.Lock()\n\t\t\tnc.removeSub(s)\n\t\t\tnc.mu.Unlock()\n\t\t}\n\t}\n\tif len(msg.Data) == 0 && msg.Header.Get(statusHdr) == noResponders {\n\t\treturn ErrNoResponders\n\t}\n\n\treturn nil\n}\n\n// Queued returns the number of queued messages in the client for this subscription.\n//\n// Deprecated: Use Pending()\nfunc (s *Subscription) QueuedMsgs() (int, error) {\n\tm, _, err := s.Pending()\n\treturn int(m), err\n}\n\n// Pending returns the number of queued messages and queued bytes in the client for this subscription.\nfunc (s *Subscription) Pending() (int, int, error) {\n\tif s == nil {\n\t\treturn -1, -1, ErrBadSubscription\n\t}\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\tif s.conn == nil || s.closed {\n\t\treturn -1, -1, ErrBadSubscription\n\t}\n\tif s.typ == ChanSubscription {\n\t\treturn -1, -1, ErrTypeSubscription\n\t}\n\treturn s.pMsgs, s.pBytes, nil\n}\n\n// MaxPending returns the maximum number of queued messages and queued bytes seen so far.\nfunc (s *Subscription) MaxPending() (int, int, error) {\n\tif s == nil {\n\t\treturn -1, -1, ErrBadSubscription\n\t}\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\tif s.conn == nil || s.closed {\n\t\treturn -1, -1, ErrBadSubscription\n\t}\n\tif s.typ == ChanSubscription {\n\t\treturn -1, -1, ErrTypeSubscription\n\t}\n\treturn s.pMsgsMax, s.pBytesMax, nil\n}\n\n// ClearMaxPending resets the maximums seen so far.\nfunc (s *Subscription) ClearMaxPending() error {\n\tif s == nil {\n\t\treturn ErrBadSubscription\n\t}\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\tif s.conn == nil || s.closed {\n\t\treturn ErrBadSubscription\n\t}\n\tif s.typ == ChanSubscription {\n\t\treturn ErrTypeSubscription\n\t}\n\ts.pMsgsMax, s.pBytesMax = 0, 0\n\treturn nil\n}\n\n// Pending Limits\nconst (\n\t// DefaultSubPendingMsgsLimit will be 512k msgs.\n\tDefaultSubPendingMsgsLimit = 512 * 1024\n\t// DefaultSubPendingBytesLimit is 64MB\n\tDefaultSubPendingBytesLimit = 64 * 1024 * 1024\n)\n\n// PendingLimits returns the current limits for this subscription.\n// If no error is returned, a negative value indicates that the\n// given metric is not limited.\nfunc (s *Subscription) PendingLimits() (int, int, error) {\n\tif s == nil {\n\t\treturn -1, -1, ErrBadSubscription\n\t}\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\tif s.conn == nil || s.closed {\n\t\treturn -1, -1, ErrBadSubscription\n\t}\n\tif s.typ == ChanSubscription {\n\t\treturn -1, -1, ErrTypeSubscription\n\t}\n\treturn s.pMsgsLimit, s.pBytesLimit, nil\n}\n\n// SetPendingLimits sets the limits for pending msgs and bytes for this subscription.\n// Zero is not allowed. Any negative value means that the given metric is not limited.\nfunc (s *Subscription) SetPendingLimits(msgLimit, bytesLimit int) error {\n\tif s == nil {\n\t\treturn ErrBadSubscription\n\t}\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\tif s.conn == nil || s.closed {\n\t\treturn ErrBadSubscription\n\t}\n\tif s.typ == ChanSubscription {\n\t\treturn ErrTypeSubscription\n\t}\n\tif msgLimit == 0 || bytesLimit == 0 {\n\t\treturn ErrInvalidArg\n\t}\n\ts.pMsgsLimit, s.pBytesLimit = msgLimit, bytesLimit\n\treturn nil\n}\n\n// Delivered returns the number of delivered messages for this subscription.\nfunc (s *Subscription) Delivered() (int64, error) {\n\tif s == nil {\n\t\treturn -1, ErrBadSubscription\n\t}\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\tif s.conn == nil || s.closed {\n\t\treturn -1, ErrBadSubscription\n\t}\n\treturn int64(s.delivered), nil\n}\n\n// Dropped returns the number of known dropped messages for this subscription.\n// This will correspond to messages dropped by violations of PendingLimits. If\n// the server declares the connection a SlowConsumer, this number may not be\n// valid.\nfunc (s *Subscription) Dropped() (int, error) {\n\tif s == nil {\n\t\treturn -1, ErrBadSubscription\n\t}\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\tif s.conn == nil || s.closed {\n\t\treturn -1, ErrBadSubscription\n\t}\n\treturn s.dropped, nil\n}\n\n// Respond allows a convenient way to respond to requests in service based subscriptions.\nfunc (m *Msg) Respond(data []byte) error {\n\tif m == nil || m.Sub == nil {\n\t\treturn ErrMsgNotBound\n\t}\n\tif m.Reply == \"\" {\n\t\treturn ErrMsgNoReply\n\t}\n\tm.Sub.mu.Lock()\n\tnc := m.Sub.conn\n\tm.Sub.mu.Unlock()\n\t// No need to check the connection here since the call to publish will do all the checking.\n\treturn nc.Publish(m.Reply, data)\n}\n\n// RespondMsg allows a convenient way to respond to requests in service based subscriptions that might include headers\nfunc (m *Msg) RespondMsg(msg *Msg) error {\n\tif m == nil || m.Sub == nil {\n\t\treturn ErrMsgNotBound\n\t}\n\tif m.Reply == \"\" {\n\t\treturn ErrMsgNoReply\n\t}\n\tmsg.Subject = m.Reply\n\tm.Sub.mu.Lock()\n\tnc := m.Sub.conn\n\tm.Sub.mu.Unlock()\n\t// No need to check the connection here since the call to publish will do all the checking.\n\treturn nc.PublishMsg(msg)\n}\n\n// FIXME: This is a hack\n// removeFlushEntry is needed when we need to discard queued up responses\n// for our pings as part of a flush call. This happens when we have a flush\n// call outstanding and we call close.\nfunc (nc *Conn) removeFlushEntry(ch chan struct{}) bool {\n\tnc.mu.Lock()\n\tdefer nc.mu.Unlock()\n\tif nc.pongs == nil {\n\t\treturn false\n\t}\n\tfor i, c := range nc.pongs {\n\t\tif c == ch {\n\t\t\tnc.pongs[i] = nil\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// The lock must be held entering this function.\nfunc (nc *Conn) sendPing(ch chan struct{}) {\n\tnc.pongs = append(nc.pongs, ch)\n\tnc.bw.appendString(pingProto)\n\t// Flush in place.\n\tnc.bw.flush()\n}\n\n// This will fire periodically and send a client origin\n// ping to the server. Will also check that we have received\n// responses from the server.\nfunc (nc *Conn) processPingTimer() {\n\tnc.mu.Lock()\n\n\tif nc.status != CONNECTED {\n\t\tnc.mu.Unlock()\n\t\treturn\n\t}\n\n\t// Check for violation\n\tnc.pout++\n\tif nc.pout > nc.Opts.MaxPingsOut {\n\t\tnc.mu.Unlock()\n\t\tif shouldClose := nc.processOpErr(ErrStaleConnection); shouldClose {\n\t\t\tnc.close(CLOSED, true, nil)\n\t\t}\n\t\treturn\n\t}\n\n\tnc.sendPing(nil)\n\tnc.ptmr.Reset(nc.Opts.PingInterval)\n\tnc.mu.Unlock()\n}\n\n// FlushTimeout allows a Flush operation to have an associated timeout.\nfunc (nc *Conn) FlushTimeout(timeout time.Duration) (err error) {\n\tif nc == nil {\n\t\treturn ErrInvalidConnection\n\t}\n\tif timeout <= 0 {\n\t\treturn ErrBadTimeout\n\t}\n\n\tnc.mu.Lock()\n\tif nc.isClosed() {\n\t\tnc.mu.Unlock()\n\t\treturn ErrConnectionClosed\n\t}\n\tt := globalTimerPool.Get(timeout)\n\tdefer globalTimerPool.Put(t)\n\n\t// Create a buffered channel to prevent chan send to block\n\t// in processPong() if this code here times out just when\n\t// PONG was received.\n\tch := make(chan struct{}, 1)\n\tnc.sendPing(ch)\n\tnc.mu.Unlock()\n\n\tselect {\n\tcase _, ok := <-ch:\n\t\tif !ok {\n\t\t\terr = ErrConnectionClosed\n\t\t} else {\n\t\t\tclose(ch)\n\t\t}\n\tcase <-t.C:\n\t\terr = ErrTimeout\n\t}\n\n\tif err != nil {\n\t\tnc.removeFlushEntry(ch)\n\t}\n\treturn\n}\n\n// RTT calculates the round trip time between this client and the server.\nfunc (nc *Conn) RTT() (time.Duration, error) {\n\tif nc.IsClosed() {\n\t\treturn 0, ErrConnectionClosed\n\t}\n\tif nc.IsReconnecting() {\n\t\treturn 0, ErrDisconnected\n\t}\n\tstart := time.Now()\n\tif err := nc.FlushTimeout(10 * time.Second); err != nil {\n\t\treturn 0, err\n\t}\n\treturn time.Since(start), nil\n}\n\n// Flush will perform a round trip to the server and return when it\n// receives the internal reply.\nfunc (nc *Conn) Flush() error {\n\treturn nc.FlushTimeout(10 * time.Second)\n}\n\n// Buffered will return the number of bytes buffered to be sent to the server.\n// FIXME(dlc) take into account disconnected state.\nfunc (nc *Conn) Buffered() (int, error) {\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\tif nc.isClosed() || nc.bw == nil {\n\t\treturn -1, ErrConnectionClosed\n\t}\n\treturn nc.bw.buffered(), nil\n}\n\n// resendSubscriptions will send our subscription state back to the\n// server. Used in reconnects\nfunc (nc *Conn) resendSubscriptions() {\n\t// Since we are going to send protocols to the server, we don't want to\n\t// be holding the subsMu lock (which is used in processMsg). So copy\n\t// the subscriptions in a temporary array.\n\tnc.subsMu.RLock()\n\tsubs := make([]*Subscription, 0, len(nc.subs))\n\tfor _, s := range nc.subs {\n\t\tsubs = append(subs, s)\n\t}\n\tnc.subsMu.RUnlock()\n\tfor _, s := range subs {\n\t\tadjustedMax := uint64(0)\n\t\ts.mu.Lock()\n\t\t// when resending subscriptions, the permissions error should be cleared\n\t\t// since the user may have fixed the permissions issue\n\t\ts.permissionsErr = nil\n\t\tif s.max > 0 {\n\t\t\tif s.delivered < s.max {\n\t\t\t\tadjustedMax = s.max - s.delivered\n\t\t\t}\n\t\t\t// adjustedMax could be 0 here if the number of delivered msgs\n\t\t\t// reached the max, if so unsubscribe.\n\t\t\tif adjustedMax == 0 {\n\t\t\t\ts.mu.Unlock()\n\t\t\t\tnc.bw.writeDirect(fmt.Sprintf(unsubProto, s.sid, _EMPTY_))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t\tsubj, queue, sid := s.Subject, s.Queue, s.sid\n\t\ts.mu.Unlock()\n\n\t\tnc.bw.writeDirect(fmt.Sprintf(subProto, subj, queue, sid))\n\t\tif adjustedMax > 0 {\n\t\t\tmaxStr := strconv.Itoa(int(adjustedMax))\n\t\t\tnc.bw.writeDirect(fmt.Sprintf(unsubProto, sid, maxStr))\n\t\t}\n\t}\n}\n\n// This will clear any pending flush calls and release pending calls.\n// Lock is assumed to be held by the caller.\nfunc (nc *Conn) clearPendingFlushCalls() {\n\t// Clear any queued pongs, e.g. pending flush calls.\n\tfor _, ch := range nc.pongs {\n\t\tif ch != nil {\n\t\t\tclose(ch)\n\t\t}\n\t}\n\tnc.pongs = nil\n}\n\n// This will clear any pending Request calls.\n// Lock is assumed to be held by the caller.\nfunc (nc *Conn) clearPendingRequestCalls() {\n\tfor key, ch := range nc.respMap {\n\t\tif ch != nil {\n\t\t\tclose(ch)\n\t\t\tdelete(nc.respMap, key)\n\t\t}\n\t}\n}\n\n// Low level close call that will do correct cleanup and set\n// desired status. Also controls whether user defined callbacks\n// will be triggered. The lock should not be held entering this\n// function. This function will handle the locking manually.\nfunc (nc *Conn) close(status Status, doCBs bool, err error) {\n\tnc.mu.Lock()\n\tif nc.isClosed() {\n\t\tnc.status = status\n\t\tnc.mu.Unlock()\n\t\treturn\n\t}\n\tnc.status = CLOSED\n\n\t// Kick the Go routines so they fall out.\n\tnc.kickFlusher()\n\n\t// If the reconnect timer is waiting between a reconnect attempt,\n\t// this will kick it out.\n\tif nc.rqch != nil {\n\t\tclose(nc.rqch)\n\t\tnc.rqch = nil\n\t}\n\n\t// Clear any queued pongs, e.g. pending flush calls.\n\tnc.clearPendingFlushCalls()\n\n\t// Clear any queued and blocking Requests.\n\tnc.clearPendingRequestCalls()\n\n\t// Stop ping timer if set.\n\tnc.stopPingTimer()\n\tnc.ptmr = nil\n\n\t// Need to close and set TCP conn to nil if reconnect loop has stopped,\n\t// otherwise we would incorrectly invoke Disconnect handler (if set)\n\t// down below.\n\tif nc.ar && nc.conn != nil {\n\t\tnc.conn.Close()\n\t\tnc.conn = nil\n\t} else if nc.conn != nil {\n\t\t// Go ahead and make sure we have flushed the outbound\n\t\tnc.bw.flush()\n\t\tdefer nc.conn.Close()\n\t}\n\n\t// Close sync subscriber channels and release any\n\t// pending NextMsg() calls.\n\tnc.subsMu.Lock()\n\tfor _, s := range nc.subs {\n\t\ts.mu.Lock()\n\n\t\t// Release callers on NextMsg for SyncSubscription only\n\t\tif s.mch != nil && s.typ == SyncSubscription {\n\t\t\tclose(s.mch)\n\t\t}\n\t\ts.mch = nil\n\t\t// Mark as invalid, for signaling to waitForMsgs\n\t\ts.closed = true\n\t\t// Mark connection closed in subscription\n\t\ts.connClosed = true\n\t\t// If we have an async subscription, signals it to exit\n\t\tif s.typ == AsyncSubscription && s.pCond != nil {\n\t\t\ts.pCond.Signal()\n\t\t}\n\n\t\ts.mu.Unlock()\n\t}\n\tnc.subs = nil\n\tnc.subsMu.Unlock()\n\n\tnc.changeConnStatus(status)\n\n\t// Perform appropriate callback if needed for a disconnect.\n\tif doCBs {\n\t\tif nc.conn != nil {\n\t\t\tif disconnectedErrCB := nc.Opts.DisconnectedErrCB; disconnectedErrCB != nil {\n\t\t\t\tnc.ach.push(func() { disconnectedErrCB(nc, err) })\n\t\t\t} else if disconnectedCB := nc.Opts.DisconnectedCB; disconnectedCB != nil {\n\t\t\t\tnc.ach.push(func() { disconnectedCB(nc) })\n\t\t\t}\n\t\t}\n\t\tif nc.Opts.ClosedCB != nil {\n\t\t\tnc.ach.push(func() { nc.Opts.ClosedCB(nc) })\n\t\t}\n\t}\n\t// If this is terminal, then we have to notify the asyncCB handler that\n\t// it can exit once all async callbacks have been dispatched.\n\tif status == CLOSED {\n\t\tnc.ach.close()\n\t}\n\tnc.mu.Unlock()\n}\n\n// Close will close the connection to the server. This call will release\n// all blocking calls, such as Flush() and NextMsg()\nfunc (nc *Conn) Close() {\n\tif nc != nil {\n\t\t// This will be a no-op if the connection was not websocket.\n\t\t// We do this here as opposed to inside close() because we want\n\t\t// to do this only for the final user-driven close of the client.\n\t\t// Otherwise, we would need to change close() to pass a boolean\n\t\t// indicating that this is the case.\n\t\tnc.wsClose()\n\t\tnc.close(CLOSED, !nc.Opts.NoCallbacksAfterClientClose, nil)\n\t}\n}\n\n// IsClosed tests if a Conn has been closed.\nfunc (nc *Conn) IsClosed() bool {\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\treturn nc.isClosed()\n}\n\n// IsReconnecting tests if a Conn is reconnecting.\nfunc (nc *Conn) IsReconnecting() bool {\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\treturn nc.isReconnecting()\n}\n\n// IsConnected tests if a Conn is connected.\nfunc (nc *Conn) IsConnected() bool {\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\treturn nc.isConnected()\n}\n\n// drainConnection will run in a separate Go routine and will\n// flush all publishes and drain all active subscriptions.\nfunc (nc *Conn) drainConnection() {\n\t// Snapshot subs list.\n\tnc.mu.Lock()\n\n\t// Check again here if we are in a state to not process.\n\tif nc.isClosed() {\n\t\tnc.mu.Unlock()\n\t\treturn\n\t}\n\tif nc.isConnecting() || nc.isReconnecting() {\n\t\tnc.mu.Unlock()\n\t\t// Move to closed state.\n\t\tnc.Close()\n\t\treturn\n\t}\n\n\tsubs := make([]*Subscription, 0, len(nc.subs))\n\tfor _, s := range nc.subs {\n\t\tif s == nc.respMux {\n\t\t\t// Skip since might be in use while messages\n\t\t\t// are being processed (can miss responses).\n\t\t\tcontinue\n\t\t}\n\t\tsubs = append(subs, s)\n\t}\n\terrCB := nc.Opts.AsyncErrorCB\n\tdrainWait := nc.Opts.DrainTimeout\n\trespMux := nc.respMux\n\tnc.mu.Unlock()\n\n\t// for pushing errors with context.\n\tpushErr := func(err error) {\n\t\tnc.mu.Lock()\n\t\tnc.err = err\n\t\tif errCB != nil {\n\t\t\tnc.ach.push(func() { errCB(nc, nil, err) })\n\t\t}\n\t\tnc.mu.Unlock()\n\t}\n\n\t// Do subs first, skip request handler if present.\n\tfor _, s := range subs {\n\t\tif err := s.Drain(); err != nil {\n\t\t\t// We will notify about these but continue.\n\t\t\tpushErr(err)\n\t\t}\n\t}\n\n\t// Wait for the subscriptions to drop to zero.\n\ttimeout := time.Now().Add(drainWait)\n\tvar min int\n\tif respMux != nil {\n\t\tmin = 1\n\t} else {\n\t\tmin = 0\n\t}\n\tfor time.Now().Before(timeout) {\n\t\tif nc.NumSubscriptions() == min {\n\t\t\tbreak\n\t\t}\n\t\ttime.Sleep(10 * time.Millisecond)\n\t}\n\n\t// In case there was a request/response handler\n\t// then need to call drain at the end.\n\tif respMux != nil {\n\t\tif err := respMux.Drain(); err != nil {\n\t\t\t// We will notify about these but continue.\n\t\t\tpushErr(err)\n\t\t}\n\t\tfor time.Now().Before(timeout) {\n\t\t\tif nc.NumSubscriptions() == 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\ttime.Sleep(10 * time.Millisecond)\n\t\t}\n\t}\n\n\t// Check if we timed out.\n\tif nc.NumSubscriptions() != 0 {\n\t\tpushErr(ErrDrainTimeout)\n\t}\n\n\t// Flip State\n\tnc.mu.Lock()\n\tnc.changeConnStatus(DRAINING_PUBS)\n\tnc.mu.Unlock()\n\n\t// Do publish drain via Flush() call.\n\terr := nc.FlushTimeout(5 * time.Second)\n\tif err != nil {\n\t\tpushErr(err)\n\t}\n\n\t// Move to closed state.\n\tnc.Close()\n}\n\n// Drain will put a connection into a drain state. All subscriptions will\n// immediately be put into a drain state. Upon completion, the publishers\n// will be drained and can not publish any additional messages. Upon draining\n// of the publishers, the connection will be closed. Use the ClosedCB\n// option to know when the connection has moved from draining to closed.\n//\n// See note in Subscription.Drain for JetStream subscriptions.\nfunc (nc *Conn) Drain() error {\n\tnc.mu.Lock()\n\tif nc.isClosed() {\n\t\tnc.mu.Unlock()\n\t\treturn ErrConnectionClosed\n\t}\n\tif nc.isConnecting() || nc.isReconnecting() {\n\t\tnc.mu.Unlock()\n\t\tnc.Close()\n\t\treturn ErrConnectionReconnecting\n\t}\n\tif nc.isDraining() {\n\t\tnc.mu.Unlock()\n\t\treturn nil\n\t}\n\tnc.changeConnStatus(DRAINING_SUBS)\n\tgo nc.drainConnection()\n\tnc.mu.Unlock()\n\n\treturn nil\n}\n\n// IsDraining tests if a Conn is in the draining state.\nfunc (nc *Conn) IsDraining() bool {\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\treturn nc.isDraining()\n}\n\n// caller must lock\nfunc (nc *Conn) getServers(implicitOnly bool) []string {\n\tpoolSize := len(nc.srvPool)\n\tvar servers = make([]string, 0)\n\tfor i := 0; i < poolSize; i++ {\n\t\tif implicitOnly && !nc.srvPool[i].isImplicit {\n\t\t\tcontinue\n\t\t}\n\t\turl := nc.srvPool[i].url\n\t\tservers = append(servers, fmt.Sprintf(\"%s://%s\", url.Scheme, url.Host))\n\t}\n\treturn servers\n}\n\n// Servers returns the list of known server urls, including additional\n// servers discovered after a connection has been established.  If\n// authentication is enabled, use UserInfo or Token when connecting with\n// these urls.\nfunc (nc *Conn) Servers() []string {\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\treturn nc.getServers(false)\n}\n\n// DiscoveredServers returns only the server urls that have been discovered\n// after a connection has been established. If authentication is enabled,\n// use UserInfo or Token when connecting with these urls.\nfunc (nc *Conn) DiscoveredServers() []string {\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\treturn nc.getServers(true)\n}\n\n// Status returns the current state of the connection.\nfunc (nc *Conn) Status() Status {\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\treturn nc.status\n}\n\n// Test if Conn has been closed Lock is assumed held.\nfunc (nc *Conn) isClosed() bool {\n\treturn nc.status == CLOSED\n}\n\n// Test if Conn is in the process of connecting\nfunc (nc *Conn) isConnecting() bool {\n\treturn nc.status == CONNECTING\n}\n\n// Test if Conn is being reconnected.\nfunc (nc *Conn) isReconnecting() bool {\n\treturn nc.status == RECONNECTING\n}\n\n// Test if Conn is connected or connecting.\nfunc (nc *Conn) isConnected() bool {\n\treturn nc.status == CONNECTED || nc.isDraining()\n}\n\n// Test if Conn is in the draining state.\nfunc (nc *Conn) isDraining() bool {\n\treturn nc.status == DRAINING_SUBS || nc.status == DRAINING_PUBS\n}\n\n// Test if Conn is in the draining state for pubs.\nfunc (nc *Conn) isDrainingPubs() bool {\n\treturn nc.status == DRAINING_PUBS\n}\n\n// Stats will return a race safe copy of the Statistics section for the connection.\nfunc (nc *Conn) Stats() Statistics {\n\t// Stats are updated either under connection's mu or with atomic operations\n\t// for inbound stats in processMsg().\n\tnc.mu.Lock()\n\tstats := Statistics{\n\t\tInMsgs:     atomic.LoadUint64(&nc.InMsgs),\n\t\tInBytes:    atomic.LoadUint64(&nc.InBytes),\n\t\tOutMsgs:    nc.OutMsgs,\n\t\tOutBytes:   nc.OutBytes,\n\t\tReconnects: nc.Reconnects,\n\t}\n\tnc.mu.Unlock()\n\treturn stats\n}\n\n// MaxPayload returns the size limit that a message payload can have.\n// This is set by the server configuration and delivered to the client\n// upon connect.\nfunc (nc *Conn) MaxPayload() int64 {\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\treturn nc.info.MaxPayload\n}\n\n// HeadersSupported will return if the server supports headers\nfunc (nc *Conn) HeadersSupported() bool {\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\treturn nc.info.Headers\n}\n\n// AuthRequired will return if the connected server requires authorization.\nfunc (nc *Conn) AuthRequired() bool {\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\treturn nc.info.AuthRequired\n}\n\n// TLSRequired will return if the connected server requires TLS connections.\nfunc (nc *Conn) TLSRequired() bool {\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\treturn nc.info.TLSRequired\n}\n\n// Barrier schedules the given function `f` to all registered asynchronous\n// subscriptions.\n// Only the last subscription to see this barrier will invoke the function.\n// If no subscription is registered at the time of this call, `f()` is invoked\n// right away.\n// ErrConnectionClosed is returned if the connection is closed prior to\n// the call.\nfunc (nc *Conn) Barrier(f func()) error {\n\tnc.mu.Lock()\n\tif nc.isClosed() {\n\t\tnc.mu.Unlock()\n\t\treturn ErrConnectionClosed\n\t}\n\tnc.subsMu.Lock()\n\t// Need to figure out how many non chan subscriptions there are\n\tnumSubs := 0\n\tfor _, sub := range nc.subs {\n\t\tif sub.typ == AsyncSubscription {\n\t\t\tnumSubs++\n\t\t}\n\t}\n\tif numSubs == 0 {\n\t\tnc.subsMu.Unlock()\n\t\tnc.mu.Unlock()\n\t\tf()\n\t\treturn nil\n\t}\n\tbarrier := &barrierInfo{refs: int64(numSubs), f: f}\n\tfor _, sub := range nc.subs {\n\t\tsub.mu.Lock()\n\t\tif sub.mch == nil {\n\t\t\tmsg := &Msg{barrier: barrier}\n\t\t\t// Push onto the async pList\n\t\t\tif sub.pTail != nil {\n\t\t\t\tsub.pTail.next = msg\n\t\t\t} else {\n\t\t\t\tsub.pHead = msg\n\t\t\t\tsub.pCond.Signal()\n\t\t\t}\n\t\t\tsub.pTail = msg\n\t\t}\n\t\tsub.mu.Unlock()\n\t}\n\tnc.subsMu.Unlock()\n\tnc.mu.Unlock()\n\treturn nil\n}\n\n// GetClientIP returns the client IP as known by the server.\n// Supported as of server version 2.1.6.\nfunc (nc *Conn) GetClientIP() (net.IP, error) {\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\tif nc.isClosed() {\n\t\treturn nil, ErrConnectionClosed\n\t}\n\tif nc.info.ClientIP == \"\" {\n\t\treturn nil, ErrClientIPNotSupported\n\t}\n\tip := net.ParseIP(nc.info.ClientIP)\n\treturn ip, nil\n}\n\n// GetClientID returns the client ID assigned by the server to which\n// the client is currently connected to. Note that the value may change if\n// the client reconnects.\n// This function returns ErrClientIDNotSupported if the server is of a\n// version prior to 1.2.0.\nfunc (nc *Conn) GetClientID() (uint64, error) {\n\tnc.mu.RLock()\n\tdefer nc.mu.RUnlock()\n\tif nc.isClosed() {\n\t\treturn 0, ErrConnectionClosed\n\t}\n\tif nc.info.CID == 0 {\n\t\treturn 0, ErrClientIDNotSupported\n\t}\n\treturn nc.info.CID, nil\n}\n\n// StatusChanged returns a channel on which given list of connection status changes will be reported.\n// If no statuses are provided, defaults will be used: CONNECTED, RECONNECTING, DISCONNECTED, CLOSED.\nfunc (nc *Conn) StatusChanged(statuses ...Status) chan Status {\n\tif len(statuses) == 0 {\n\t\tstatuses = []Status{CONNECTED, RECONNECTING, DISCONNECTED, CLOSED}\n\t}\n\tch := make(chan Status, 10)\n\tfor _, s := range statuses {\n\t\tnc.registerStatusChangeListener(s, ch)\n\t}\n\treturn ch\n}\n\n// registerStatusChangeListener registers a channel waiting for a specific status change event.\n// Status change events are non-blocking - if no receiver is waiting for the status change,\n// it will not be sent on the channel. Closed channels are ignored.\nfunc (nc *Conn) registerStatusChangeListener(status Status, ch chan Status) {\n\tnc.mu.Lock()\n\tdefer nc.mu.Unlock()\n\tif nc.statListeners == nil {\n\t\tnc.statListeners = make(map[Status][]chan Status)\n\t}\n\tif _, ok := nc.statListeners[status]; !ok {\n\t\tnc.statListeners[status] = make([]chan Status, 0)\n\t}\n\tnc.statListeners[status] = append(nc.statListeners[status], ch)\n}\n\n// sendStatusEvent sends connection status event to all channels.\n// If channel is closed, or there is no listener, sendStatusEvent\n// will not block. Lock should be held entering.\nfunc (nc *Conn) sendStatusEvent(s Status) {\nLoop:\n\tfor i := 0; i < len(nc.statListeners[s]); i++ {\n\t\t// make sure channel is not closed\n\t\tselect {\n\t\tcase <-nc.statListeners[s][i]:\n\t\t\t// if chan is closed, remove it\n\t\t\tnc.statListeners[s][i] = nc.statListeners[s][len(nc.statListeners[s])-1]\n\t\t\tnc.statListeners[s] = nc.statListeners[s][:len(nc.statListeners[s])-1]\n\t\t\ti--\n\t\t\tcontinue Loop\n\t\tdefault:\n\t\t}\n\t\t// only send event if someone's listening\n\t\tselect {\n\t\tcase nc.statListeners[s][i] <- s:\n\t\tdefault:\n\t\t}\n\t}\n}\n\n// changeConnStatus changes connections status and sends events\n// to all listeners. Lock should be held entering.\nfunc (nc *Conn) changeConnStatus(status Status) {\n\tif nc == nil {\n\t\treturn\n\t}\n\tnc.sendStatusEvent(status)\n\tnc.status = status\n}\n\n// NkeyOptionFromSeed will load an nkey pair from a seed file.\n// It will return the NKey Option and will handle\n// signing of nonce challenges from the server. It will take\n// care to not hold keys in memory and to wipe memory.\nfunc NkeyOptionFromSeed(seedFile string) (Option, error) {\n\tkp, err := nkeyPairFromSeedFile(seedFile)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// Wipe our key on exit.\n\tdefer kp.Wipe()\n\n\tpub, err := kp.PublicKey()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif !nkeys.IsValidPublicUserKey(pub) {\n\t\treturn nil, errors.New(\"nats: Not a valid nkey user seed\")\n\t}\n\tsigCB := func(nonce []byte) ([]byte, error) {\n\t\treturn sigHandler(nonce, seedFile)\n\t}\n\treturn Nkey(string(pub), sigCB), nil\n}\n\n// Just wipe slice with 'x', for clearing contents of creds or nkey seed file.\nfunc wipeSlice(buf []byte) {\n\tfor i := range buf {\n\t\tbuf[i] = 'x'\n\t}\n}\n\nfunc userFromFile(userFile string) (string, error) {\n\tpath, err := expandPath(userFile)\n\tif err != nil {\n\t\treturn _EMPTY_, fmt.Errorf(\"nats: %w\", err)\n\t}\n\n\tcontents, err := os.ReadFile(path)\n\tif err != nil {\n\t\treturn _EMPTY_, fmt.Errorf(\"nats: %w\", err)\n\t}\n\tdefer wipeSlice(contents)\n\treturn nkeys.ParseDecoratedJWT(contents)\n}\n\nfunc homeDir() (string, error) {\n\tif runtime.GOOS == \"windows\" {\n\t\thomeDrive, homePath := os.Getenv(\"HOMEDRIVE\"), os.Getenv(\"HOMEPATH\")\n\t\tuserProfile := os.Getenv(\"USERPROFILE\")\n\n\t\tvar home string\n\t\tif homeDrive == \"\" || homePath == \"\" {\n\t\t\tif userProfile == \"\" {\n\t\t\t\treturn _EMPTY_, errors.New(\"nats: failed to get home dir, require %HOMEDRIVE% and %HOMEPATH% or %USERPROFILE%\")\n\t\t\t}\n\t\t\thome = userProfile\n\t\t} else {\n\t\t\thome = filepath.Join(homeDrive, homePath)\n\t\t}\n\n\t\treturn home, nil\n\t}\n\n\thome := os.Getenv(\"HOME\")\n\tif home == \"\" {\n\t\treturn _EMPTY_, errors.New(\"nats: failed to get home dir, require $HOME\")\n\t}\n\treturn home, nil\n}\n\nfunc expandPath(p string) (string, error) {\n\tp = os.ExpandEnv(p)\n\n\tif !strings.HasPrefix(p, \"~\") {\n\t\treturn p, nil\n\t}\n\n\thome, err := homeDir()\n\tif err != nil {\n\t\treturn _EMPTY_, err\n\t}\n\n\treturn filepath.Join(home, p[1:]), nil\n}\n\nfunc nkeyPairFromSeedFile(seedFile string) (nkeys.KeyPair, error) {\n\tcontents, err := os.ReadFile(seedFile)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"nats: %w\", err)\n\t}\n\tdefer wipeSlice(contents)\n\treturn nkeys.ParseDecoratedNKey(contents)\n}\n\n// Sign authentication challenges from the server.\n// Do not keep private seed in memory.\nfunc sigHandler(nonce []byte, seedFile string) ([]byte, error) {\n\tkp, err := nkeyPairFromSeedFile(seedFile)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to extract key pair from file %q: %w\", seedFile, err)\n\t}\n\t// Wipe our key on exit.\n\tdefer kp.Wipe()\n\n\tsig, _ := kp.Sign(nonce)\n\treturn sig, nil\n}\n\ntype timeoutWriter struct {\n\ttimeout time.Duration\n\tconn    net.Conn\n\terr     error\n}\n\n// Write implements the io.Writer interface.\nfunc (tw *timeoutWriter) Write(p []byte) (int, error) {\n\tif tw.err != nil {\n\t\treturn 0, tw.err\n\t}\n\n\tvar n int\n\ttw.conn.SetWriteDeadline(time.Now().Add(tw.timeout))\n\tn, tw.err = tw.conn.Write(p)\n\ttw.conn.SetWriteDeadline(time.Time{})\n\treturn n, tw.err\n}\n"
        },
        {
          "name": "nats_iter.go",
          "type": "blob",
          "size": 2.064453125,
          "content": "// Copyright 2012-2024 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build go1.23\n// +build go1.23\n\npackage nats\n\nimport (\n\t\"errors\"\n\t\"iter\"\n\t\"time\"\n)\n\n// Msgs returns an iter.Seq2[*Msg, error] that can be used to iterate over\n// messages. It can only be used with a subscription that has been created with\n// SubscribeSync or QueueSubscribeSync, otherwise it will return an error on the\n// first iteration.\n//\n// The iterator will block until a message is available. The\n// subscription will not be closed when the iterator is done.\nfunc (sub *Subscription) Msgs() iter.Seq2[*Msg, error] {\n\treturn func(yield func(*Msg, error) bool) {\n\t\tfor {\n\t\t\tmsg, err := sub.nextMsgNoTimeout()\n\t\t\tif err != nil {\n\t\t\t\tyield(nil, err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif !yield(msg, nil) {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t}\n\t}\n}\n\n// MsgsTimeout returns an iter.Seq2[*Msg, error] that can be used to iterate\n// over messages. It can only be used with a subscription that has been created\n// with SubscribeSync or QueueSubscribeSync, otherwise it will return an error\n// on the first iteration.\n//\n// The iterator will block until a message is available or the timeout is\n// reached. If the timeout is reached, the iterator will return nats.ErrTimeout\n// but it will not be closed.\nfunc (sub *Subscription) MsgsTimeout(timeout time.Duration) iter.Seq2[*Msg, error] {\n\treturn func(yield func(*Msg, error) bool) {\n\t\tfor {\n\t\t\tmsg, err := sub.NextMsg(timeout)\n\t\t\tif err != nil {\n\t\t\t\tif !yield(nil, err) {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif !errors.Is(err, ErrTimeout) {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !yield(msg, nil) {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "nats_test.go",
          "type": "blob",
          "size": 49.744140625,
          "content": "// Copyright 2012-2023 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nats\n\n////////////////////////////////////////////////////////////////////////////////\n// Package scoped specific tests here..\n////////////////////////////////////////////////////////////////////////////////\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"net/http\"\n\t\"os\"\n\t\"reflect\"\n\t\"regexp\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/nats-io/nkeys\"\n)\n\nfunc TestVersion(t *testing.T) {\n\t// Semantic versioning\n\tverRe := regexp.MustCompile(`\\d+.\\d+.\\d+(-\\S+)?`)\n\tif !verRe.MatchString(Version) {\n\t\tt.Fatalf(\"Version not compatible with semantic versioning: %q\", Version)\n\t}\n}\n\n// Dumb wait program to sync on callbacks, etc... Will timeout\nfunc Wait(ch chan bool) error {\n\treturn WaitTime(ch, 5*time.Second)\n}\n\nfunc WaitTime(ch chan bool, timeout time.Duration) error {\n\tselect {\n\tcase <-ch:\n\t\treturn nil\n\tcase <-time.After(timeout):\n\t}\n\treturn errors.New(\"timeout\")\n}\n\nfunc stackFatalf(t *testing.T, f string, args ...any) {\n\tlines := make([]string, 0, 32)\n\tmsg := fmt.Sprintf(f, args...)\n\tlines = append(lines, msg)\n\n\t// Generate the Stack of callers: Skip us and verify* frames.\n\tfor i := 1; true; i++ {\n\t\t_, file, line, ok := runtime.Caller(i)\n\t\tif !ok {\n\t\t\tbreak\n\t\t}\n\t\tmsg := fmt.Sprintf(\"%d - %s:%d\", i, file, line)\n\t\tlines = append(lines, msg)\n\t}\n\tt.Fatalf(\"%s\", strings.Join(lines, \"\\n\"))\n}\n\n// Check the error channel for an error and if one is present,\n// calls t.Fatal(e.Error()). Note that this supports tests that\n// send nil to the error channel and so report error only if\n// e is != nil.\nfunc checkErrChannel(t *testing.T, errCh chan error) {\n\tt.Helper()\n\tselect {\n\tcase e := <-errCh:\n\t\tif e != nil {\n\t\t\tt.Fatal(e.Error())\n\t\t}\n\tdefault:\n\t}\n}\n\nfunc TestVersionMatchesTag(t *testing.T) {\n\trefType := os.Getenv(\"GITHUB_REF_TYPE\")\n\tif refType != \"tag\" {\n\t\tt.SkipNow()\n\t}\n\ttag := os.Getenv(\"GITHUB_REF_NAME\")\n\t// We expect a tag of the form vX.Y.Z. If that's not the case,\n\t// we need someone to have a look. So fail if first letter is not\n\t// a `v`\n\tif tag[0] != 'v' {\n\t\tt.Fatalf(\"Expect tag to start with `v`, tag is: %s\", tag)\n\t}\n\t// Strip the `v` from the tag for the version comparison.\n\tif Version != tag[1:] {\n\t\tt.Fatalf(\"Version (%s) does not match tag (%s)\", Version, tag[1:])\n\t}\n}\n\nfunc TestExpandPath(t *testing.T) {\n\tif runtime.GOOS == \"windows\" {\n\t\torigUserProfile := os.Getenv(\"USERPROFILE\")\n\t\torigHomeDrive, origHomePath := os.Getenv(\"HOMEDRIVE\"), os.Getenv(\"HOMEPATH\")\n\t\tdefer func() {\n\t\t\tos.Setenv(\"USERPROFILE\", origUserProfile)\n\t\t\tos.Setenv(\"HOMEDRIVE\", origHomeDrive)\n\t\t\tos.Setenv(\"HOMEPATH\", origHomePath)\n\t\t}()\n\n\t\tcases := []struct {\n\t\t\tpath        string\n\t\t\tuserProfile string\n\t\t\thomeDrive   string\n\t\t\thomePath    string\n\n\t\t\twantPath string\n\t\t\twantErr  bool\n\t\t}{\n\t\t\t// Missing HOMEDRIVE and HOMEPATH.\n\t\t\t{path: \"/Foo/Bar\", userProfile: `C:\\Foo\\Bar`, wantPath: \"/Foo/Bar\"},\n\t\t\t{path: \"Foo/Bar\", userProfile: `C:\\Foo\\Bar`, wantPath: \"Foo/Bar\"},\n\t\t\t{path: \"~/Fizz\", userProfile: `C:\\Foo\\Bar`, wantPath: `C:\\Foo\\Bar\\Fizz`},\n\n\t\t\t// Missing USERPROFILE.\n\t\t\t{path: \"~/Fizz\", homeDrive: \"X:\", homePath: `\\Foo\\Bar`, wantPath: `X:\\Foo\\Bar\\Fizz`},\n\n\t\t\t// Set all environment variables. HOMEDRIVE and HOMEPATH take\n\t\t\t// precedence.\n\t\t\t{path: \"~/Fizz\", userProfile: `C:\\Foo\\Bar`,\n\t\t\t\thomeDrive: \"X:\", homePath: `\\Foo\\Bar`, wantPath: `X:\\Foo\\Bar\\Fizz`},\n\n\t\t\t// Missing all environment variables.\n\t\t\t{path: \"~/Fizz\", wantErr: true},\n\t\t}\n\t\tfor i, c := range cases {\n\t\t\tt.Run(fmt.Sprintf(\"windows case %d\", i), func(t *testing.T) {\n\t\t\t\tos.Setenv(\"USERPROFILE\", c.userProfile)\n\t\t\t\tos.Setenv(\"HOMEDRIVE\", c.homeDrive)\n\t\t\t\tos.Setenv(\"HOMEPATH\", c.homePath)\n\n\t\t\t\tgotPath, err := expandPath(c.path)\n\t\t\t\tif !c.wantErr && err != nil {\n\t\t\t\t\tt.Fatalf(\"unexpected error: got=%v; want=%v\", err, nil)\n\t\t\t\t} else if c.wantErr && err == nil {\n\t\t\t\t\tt.Fatalf(\"unexpected success: got=%v; want=%v\", nil, \"err\")\n\t\t\t\t}\n\n\t\t\t\tif gotPath != c.wantPath {\n\t\t\t\t\tt.Fatalf(\"unexpected path: got=%v; want=%v\", gotPath, c.wantPath)\n\t\t\t\t}\n\t\t\t})\n\t\t}\n\n\t\treturn\n\t}\n\n\t// Unix tests\n\n\torigHome := os.Getenv(\"HOME\")\n\tdefer os.Setenv(\"HOME\", origHome)\n\n\tcases := []struct {\n\t\tpath    string\n\t\thome    string\n\t\ttestEnv string\n\n\t\twantPath string\n\t\twantErr  bool\n\t}{\n\t\t{path: \"/foo/bar\", home: \"/fizz/buzz\", wantPath: \"/foo/bar\"},\n\t\t{path: \"foo/bar\", home: \"/fizz/buzz\", wantPath: \"foo/bar\"},\n\t\t{path: \"~/fizz\", home: \"/foo/bar\", wantPath: \"/foo/bar/fizz\"},\n\t\t{path: \"$HOME/fizz\", home: \"/foo/bar\", wantPath: \"/foo/bar/fizz\"},\n\n\t\t// missing HOME env var\n\t\t{path: \"~/fizz\", wantErr: true},\n\t}\n\tfor i, c := range cases {\n\t\tt.Run(fmt.Sprintf(\"unix case %d\", i), func(t *testing.T) {\n\t\t\tos.Setenv(\"HOME\", c.home)\n\n\t\t\tgotPath, err := expandPath(c.path)\n\t\t\tif !c.wantErr && err != nil {\n\t\t\t\tt.Fatalf(\"unexpected error: got=%v; want=%v\", err, nil)\n\t\t\t} else if c.wantErr && err == nil {\n\t\t\t\tt.Fatalf(\"unexpected success: got=%v; want=%v\", nil, \"err\")\n\t\t\t}\n\n\t\t\tif gotPath != c.wantPath {\n\t\t\t\tt.Fatalf(\"unexpected path: got=%v; want=%v\", gotPath, c.wantPath)\n\t\t\t}\n\t\t})\n\t}\n}\n\n////////////////////////////////////////////////////////////////////////////////\n// ServerPool tests\n////////////////////////////////////////////////////////////////////////////////\n\nvar testServers = []string{\n\t\"nats://localhost:1222\",\n\t\"nats://localhost:1223\",\n\t\"nats://localhost:1224\",\n\t\"nats://localhost:1225\",\n\t\"nats://localhost:1226\",\n\t\"nats://localhost:1227\",\n\t\"nats://localhost:1228\",\n}\n\nfunc TestSimplifiedURLs(t *testing.T) {\n\tfor _, test := range []struct {\n\t\tname     string\n\t\tservers  []string\n\t\texpected []string\n\t}{\n\t\t{\n\t\t\t\"nats\",\n\t\t\t[]string{\n\t\t\t\t\"nats://host1:1234/\",\n\t\t\t\t\"nats://host1:1234\",\n\t\t\t\t\"nats://host2:\",\n\t\t\t\t\"nats://host3\",\n\t\t\t\t\"host4:1234\",\n\t\t\t\t\"host5:\",\n\t\t\t\t\"host6\",\n\t\t\t\t\"nats://[1:2:3:4]:1234\",\n\t\t\t\t\"nats://[5:6:7:8]:\",\n\t\t\t\t\"nats://[9:10:11:12]\",\n\t\t\t\t\"[13:14:15:16]:\",\n\t\t\t\t\"[17:18:19:20]:1234\",\n\t\t\t},\n\t\t\t[]string{\n\t\t\t\t\"nats://host1:1234/\",\n\t\t\t\t\"nats://host1:1234\",\n\t\t\t\t\"nats://host2:4222\",\n\t\t\t\t\"nats://host3:4222\",\n\t\t\t\t\"nats://host4:1234\",\n\t\t\t\t\"nats://host5:4222\",\n\t\t\t\t\"nats://host6:4222\",\n\t\t\t\t\"nats://[1:2:3:4]:1234\",\n\t\t\t\t\"nats://[5:6:7:8]:4222\",\n\t\t\t\t\"nats://[9:10:11:12]:4222\",\n\t\t\t\t\"nats://[13:14:15:16]:4222\",\n\t\t\t\t\"nats://[17:18:19:20]:1234\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"ws\",\n\t\t\t[]string{\n\t\t\t\t\"ws://host1:1234\",\n\t\t\t\t\"ws://host2:\",\n\t\t\t\t\"ws://host3\",\n\t\t\t\t\"ws://[1:2:3:4]:1234\",\n\t\t\t\t\"ws://[5:6:7:8]:\",\n\t\t\t\t\"ws://[9:10:11:12]\",\n\t\t\t},\n\t\t\t[]string{\n\t\t\t\t\"ws://host1:1234\",\n\t\t\t\t\"ws://host2:80\",\n\t\t\t\t\"ws://host3:80\",\n\t\t\t\t\"ws://[1:2:3:4]:1234\",\n\t\t\t\t\"ws://[5:6:7:8]:80\",\n\t\t\t\t\"ws://[9:10:11:12]:80\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"wss\",\n\t\t\t[]string{\n\t\t\t\t\"wss://host1:1234\",\n\t\t\t\t\"wss://host2:\",\n\t\t\t\t\"wss://host3\",\n\t\t\t\t\"wss://[1:2:3:4]:1234\",\n\t\t\t\t\"wss://[5:6:7:8]:\",\n\t\t\t\t\"wss://[9:10:11:12]\",\n\t\t\t},\n\t\t\t[]string{\n\t\t\t\t\"wss://host1:1234\",\n\t\t\t\t\"wss://host2:443\",\n\t\t\t\t\"wss://host3:443\",\n\t\t\t\t\"wss://[1:2:3:4]:1234\",\n\t\t\t\t\"wss://[5:6:7:8]:443\",\n\t\t\t\t\"wss://[9:10:11:12]:443\",\n\t\t\t},\n\t\t},\n\t} {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\topts := GetDefaultOptions()\n\t\t\topts.NoRandomize = true\n\t\t\topts.Servers = test.servers\n\n\t\t\tnc := &Conn{Opts: opts}\n\t\t\tif err := nc.setupServerPool(); err != nil {\n\t\t\t\tt.Fatalf(\"Problem setting up Server Pool: %v\\n\", err)\n\t\t\t}\n\t\t\t// Check server pool directly\n\t\t\tfor i, u := range nc.srvPool {\n\t\t\t\tif u.url.String() != test.expected[i] {\n\t\t\t\t\tt.Fatalf(\"Expected url %q, got %q\", test.expected[i], u.url.String())\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestServersRandomize(t *testing.T) {\n\topts := GetDefaultOptions()\n\topts.Servers = testServers\n\tnc := &Conn{Opts: opts}\n\tif err := nc.setupServerPool(); err != nil {\n\t\tt.Fatalf(\"Problem setting up Server Pool: %v\\n\", err)\n\t}\n\t// Build []string from srvPool\n\tclientServers := []string{}\n\tfor _, s := range nc.srvPool {\n\t\tclientServers = append(clientServers, s.url.String())\n\t}\n\t// In theory this could happen..\n\tif reflect.DeepEqual(testServers, clientServers) {\n\t\tt.Fatalf(\"ServerPool list not randomized\\n\")\n\t}\n\n\t// Now test that we do not randomize if proper flag is set.\n\topts = GetDefaultOptions()\n\topts.Servers = testServers\n\topts.NoRandomize = true\n\tnc = &Conn{Opts: opts}\n\tif err := nc.setupServerPool(); err != nil {\n\t\tt.Fatalf(\"Problem setting up Server Pool: %v\\n\", err)\n\t}\n\t// Build []string from srvPool\n\tclientServers = []string{}\n\tfor _, s := range nc.srvPool {\n\t\tclientServers = append(clientServers, s.url.String())\n\t}\n\tif !reflect.DeepEqual(testServers, clientServers) {\n\t\tt.Fatalf(\"ServerPool list should not be randomized\\n\")\n\t}\n\n\t// Although the original intent was that if Opts.Url is\n\t// set, Opts.Servers is not (and vice versa), the behavior\n\t// is that Opts.Url is always first, even when randomization\n\t// is enabled. So make sure that this is still the case.\n\topts = GetDefaultOptions()\n\topts.Url = DefaultURL\n\topts.Servers = testServers\n\tnc = &Conn{Opts: opts}\n\tif err := nc.setupServerPool(); err != nil {\n\t\tt.Fatalf(\"Problem setting up Server Pool: %v\\n\", err)\n\t}\n\t// Build []string from srvPool\n\tclientServers = []string{}\n\tfor _, s := range nc.srvPool {\n\t\tclientServers = append(clientServers, s.url.String())\n\t}\n\t// In theory this could happen..\n\tif reflect.DeepEqual(testServers, clientServers) {\n\t\tt.Fatalf(\"ServerPool list not randomized\\n\")\n\t}\n\tif clientServers[0] != DefaultURL {\n\t\tt.Fatalf(\"Options.Url should be first in the array, got %v\", clientServers[0])\n\t}\n}\n\nfunc TestSelectNextServer(t *testing.T) {\n\topts := GetDefaultOptions()\n\topts.Servers = testServers\n\topts.NoRandomize = true\n\tnc := &Conn{Opts: opts}\n\tif err := nc.setupServerPool(); err != nil {\n\t\tt.Fatalf(\"Problem setting up Server Pool: %v\\n\", err)\n\t}\n\tif nc.current != nc.srvPool[0] {\n\t\tt.Fatalf(\"Wrong default selection: %v\\n\", nc.current.url)\n\t}\n\n\tsel, err := nc.selectNextServer()\n\tif err != nil {\n\t\tt.Fatalf(\"Got an err: %v\\n\", err)\n\t}\n\t// Check that we are now looking at #2, and current is now last.\n\tif len(nc.srvPool) != len(testServers) {\n\t\tt.Fatalf(\"List is incorrect size: %d vs %d\\n\", len(nc.srvPool), len(testServers))\n\t}\n\tif nc.current.url.String() != testServers[1] {\n\t\tt.Fatalf(\"Selection incorrect: %v vs %v\\n\", nc.current.url, testServers[1])\n\t}\n\tif nc.srvPool[len(nc.srvPool)-1].url.String() != testServers[0] {\n\t\tt.Fatalf(\"Did not push old to last position\\n\")\n\t}\n\tif sel != nc.srvPool[0] {\n\t\tt.Fatalf(\"Did not return correct server: %v vs %v\\n\", sel.url, nc.srvPool[0].url)\n\t}\n\n\t// Test that we do not keep servers where we have tried to reconnect past our limit.\n\tnc.srvPool[0].reconnects = int(opts.MaxReconnect)\n\tif _, err := nc.selectNextServer(); err != nil {\n\t\tt.Fatalf(\"Got an err: %v\\n\", err)\n\t}\n\t// Check that we are now looking at #3, and current is not in the list.\n\tif len(nc.srvPool) != len(testServers)-1 {\n\t\tt.Fatalf(\"List is incorrect size: %d vs %d\\n\", len(nc.srvPool), len(testServers)-1)\n\t}\n\tif nc.current.url.String() != testServers[2] {\n\t\tt.Fatalf(\"Selection incorrect: %v vs %v\\n\", nc.current.url, testServers[2])\n\t}\n\tif nc.srvPool[len(nc.srvPool)-1].url.String() == testServers[1] {\n\t\tt.Fatalf(\"Did not throw away the last server correctly\\n\")\n\t}\n}\n\n// This will test that comma separated url strings work properly for\n// the Connect() command.\nfunc TestUrlArgument(t *testing.T) {\n\tcheck := func(url string, expected []string) {\n\t\tif !reflect.DeepEqual(processUrlString(url), expected) {\n\t\t\tt.Fatalf(\"Got wrong response processing URL: %q, RES: %#v\\n\", url, processUrlString(url))\n\t\t}\n\t}\n\t// This is normal case\n\toneExpected := []string{\"nats://localhost:1222\"}\n\n\tcheck(\"nats://localhost:1222\", oneExpected)\n\tcheck(\"nats://localhost:1222 \", oneExpected)\n\tcheck(\" nats://localhost:1222\", oneExpected)\n\tcheck(\" nats://localhost:1222 \", oneExpected)\n\tcheck(\"nats://localhost:1222/\", oneExpected)\n\n\tvar multiExpected = []string{\n\t\t\"nats://localhost:1222\",\n\t\t\"nats://localhost:1223\",\n\t\t\"nats://localhost:1224\",\n\t}\n\n\tcheck(\"nats://localhost:1222,nats://localhost:1223,nats://localhost:1224\", multiExpected)\n\tcheck(\"nats://localhost:1222, nats://localhost:1223, nats://localhost:1224\", multiExpected)\n\tcheck(\" nats://localhost:1222, nats://localhost:1223, nats://localhost:1224 \", multiExpected)\n\tcheck(\"nats://localhost:1222,   nats://localhost:1223  ,nats://localhost:1224\", multiExpected)\n\tcheck(\"nats://localhost:1222/,nats://localhost:1223/,nats://localhost:1224/\", multiExpected)\n}\n\nfunc TestParserPing(t *testing.T) {\n\tc := &Conn{}\n\tc.newReaderWriter()\n\tc.bw.switchToPending()\n\n\tc.ps = &parseState{}\n\n\tif c.ps.state != OP_START {\n\t\tt.Fatalf(\"Expected OP_START vs %d\\n\", c.ps.state)\n\t}\n\tping := []byte(\"PING\\r\\n\")\n\terr := c.parse(ping[:1])\n\tif err != nil || c.ps.state != OP_P {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\terr = c.parse(ping[1:2])\n\tif err != nil || c.ps.state != OP_PI {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\terr = c.parse(ping[2:3])\n\tif err != nil || c.ps.state != OP_PIN {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\terr = c.parse(ping[3:4])\n\tif err != nil || c.ps.state != OP_PING {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\terr = c.parse(ping[4:5])\n\tif err != nil || c.ps.state != OP_PING {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\terr = c.parse(ping[5:6])\n\tif err != nil || c.ps.state != OP_START {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\terr = c.parse(ping)\n\tif err != nil || c.ps.state != OP_START {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\t// Should tolerate spaces\n\tping = []byte(\"PING  \\r\")\n\terr = c.parse(ping)\n\tif err != nil || c.ps.state != OP_PING {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\tc.ps.state = OP_START\n\tping = []byte(\"PING  \\r  \\n\")\n\terr = c.parse(ping)\n\tif err != nil || c.ps.state != OP_START {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n}\n\nfunc TestParserErr(t *testing.T) {\n\tc := &Conn{}\n\tc.status = CLOSED\n\tc.newReaderWriter()\n\tc.bw.switchToPending()\n\n\tc.ps = &parseState{}\n\n\t// This test focuses on the parser only, not how the error is\n\t// actually processed by the upper layer.\n\n\tif c.ps.state != OP_START {\n\t\tt.Fatalf(\"Expected OP_START vs %d\\n\", c.ps.state)\n\t}\n\n\texpectedError := \"'Any kind of error'\"\n\terrProto := []byte(\"-ERR  \" + expectedError + \"\\r\\n\")\n\terr := c.parse(errProto[:1])\n\tif err != nil || c.ps.state != OP_MINUS {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\terr = c.parse(errProto[1:2])\n\tif err != nil || c.ps.state != OP_MINUS_E {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\terr = c.parse(errProto[2:3])\n\tif err != nil || c.ps.state != OP_MINUS_ER {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\terr = c.parse(errProto[3:4])\n\tif err != nil || c.ps.state != OP_MINUS_ERR {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\terr = c.parse(errProto[4:5])\n\tif err != nil || c.ps.state != OP_MINUS_ERR_SPC {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\terr = c.parse(errProto[5:6])\n\tif err != nil || c.ps.state != OP_MINUS_ERR_SPC {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\n\t// Check with split arg buffer\n\terr = c.parse(errProto[6:7])\n\tif err != nil || c.ps.state != MINUS_ERR_ARG {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\terr = c.parse(errProto[7:10])\n\tif err != nil || c.ps.state != MINUS_ERR_ARG {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\terr = c.parse(errProto[10 : len(errProto)-2])\n\tif err != nil || c.ps.state != MINUS_ERR_ARG {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\tif c.ps.argBuf == nil {\n\t\tt.Fatal(\"ArgBuf should not be nil\")\n\t}\n\ts := string(c.ps.argBuf)\n\tif s != expectedError {\n\t\tt.Fatalf(\"Expected %v, got %v\", expectedError, s)\n\t}\n\terr = c.parse(errProto[len(errProto)-2:])\n\tif err != nil || c.ps.state != OP_START {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\n\t// Check without split arg buffer\n\terrProto = []byte(\"-ERR 'Any error'\\r\\n\")\n\terr = c.parse(errProto)\n\tif err != nil || c.ps.state != OP_START {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n}\n\nfunc TestParserOK(t *testing.T) {\n\tc := &Conn{}\n\tc.ps = &parseState{}\n\n\tif c.ps.state != OP_START {\n\t\tt.Fatalf(\"Expected OP_START vs %d\\n\", c.ps.state)\n\t}\n\terrProto := []byte(\"+OKay\\r\\n\")\n\terr := c.parse(errProto[:1])\n\tif err != nil || c.ps.state != OP_PLUS {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\terr = c.parse(errProto[1:2])\n\tif err != nil || c.ps.state != OP_PLUS_O {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\terr = c.parse(errProto[2:3])\n\tif err != nil || c.ps.state != OP_PLUS_OK {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\terr = c.parse(errProto[3:])\n\tif err != nil || c.ps.state != OP_START {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n}\n\nfunc TestParserShouldFail(t *testing.T) {\n\tc := &Conn{}\n\tc.ps = &parseState{}\n\n\tif err := c.parse([]byte(\" PING\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"POO\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"Px\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"PIx\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"PINx\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\t// Stop here because 'PING' protos are tolerant for anything between PING and \\n\n\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"POx\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"PONx\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\t// Stop here because 'PONG' protos are tolerant for anything between PONG and \\n\n\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"ZOO\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"Mx\\r\\n\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"MSx\\r\\n\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"MSGx\\r\\n\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"MSG  foo\\r\\n\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"MSG \\r\\n\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"MSG foo 1\\r\\n\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"MSG foo bar 1\\r\\n\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"MSG foo bar 1 baz\\r\\n\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"MSG foo 1 bar baz\\r\\n\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"+x\\r\\n\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"+Ox\\r\\n\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"-x\\r\\n\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"-Ex\\r\\n\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"-ERx\\r\\n\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n\tc.ps.state = OP_START\n\tif err := c.parse([]byte(\"-ERRx\\r\\n\")); err == nil {\n\t\tt.Fatal(\"Should have received a parse error\")\n\t}\n}\n\nfunc TestParserSplitMsg(t *testing.T) {\n\tnc := &Conn{}\n\tnc.ps = &parseState{}\n\n\tbuf := []byte(\"MSG a\\r\\n\")\n\terr := nc.parse(buf)\n\tif err == nil {\n\t\tt.Fatal(\"Expected an error\")\n\t}\n\tnc.ps = &parseState{}\n\n\tbuf = []byte(\"MSG a b c\\r\\n\")\n\terr = nc.parse(buf)\n\tif err == nil {\n\t\tt.Fatal(\"Expected an error\")\n\t}\n\tnc.ps = &parseState{}\n\n\texpectedCount := uint64(1)\n\texpectedSize := uint64(3)\n\n\tbuf = []byte(\"MSG a\")\n\terr = nc.parse(buf)\n\tif err != nil {\n\t\tt.Fatalf(\"Parser error: %v\", err)\n\t}\n\tif nc.ps.argBuf == nil {\n\t\tt.Fatal(\"Arg buffer should have been created\")\n\t}\n\n\tbuf = []byte(\" 1 3\\r\\nf\")\n\terr = nc.parse(buf)\n\tif err != nil {\n\t\tt.Fatalf(\"Parser error: %v\", err)\n\t}\n\tif nc.ps.ma.size != 3 {\n\t\tt.Fatalf(\"Wrong msg size: %d instead of 3\", nc.ps.ma.size)\n\t}\n\tif nc.ps.ma.sid != 1 {\n\t\tt.Fatalf(\"Wrong sid: %d instead of 1\", nc.ps.ma.sid)\n\t}\n\tif string(nc.ps.ma.subject) != \"a\" {\n\t\tt.Fatalf(\"Wrong subject: '%s' instead of 'a'\", string(nc.ps.ma.subject))\n\t}\n\tif nc.ps.msgBuf == nil {\n\t\tt.Fatal(\"Msg buffer should have been created\")\n\t}\n\n\tbuf = []byte(\"oo\\r\\n\")\n\terr = nc.parse(buf)\n\tif err != nil {\n\t\tt.Fatalf(\"Parser error: %v\", err)\n\t}\n\tif (nc.Statistics.InMsgs != expectedCount) || (nc.Statistics.InBytes != expectedSize) {\n\t\tt.Fatalf(\"Wrong stats: %d - %d instead of %d - %d\", nc.Statistics.InMsgs, nc.Statistics.InBytes, expectedCount, expectedSize)\n\t}\n\tif (nc.ps.argBuf != nil) || (nc.ps.msgBuf != nil) {\n\t\tt.Fatal(\"Buffers should be nil now\")\n\t}\n\n\tbuf = []byte(\"MSG a 1 3\\r\\nfo\")\n\terr = nc.parse(buf)\n\tif err != nil {\n\t\tt.Fatalf(\"Parser error: %v\", err)\n\t}\n\tif nc.ps.ma.size != 3 {\n\t\tt.Fatalf(\"Wrong msg size: %d instead of 3\", nc.ps.ma.size)\n\t}\n\tif nc.ps.ma.sid != 1 {\n\t\tt.Fatalf(\"Wrong sid: %d instead of 1\", nc.ps.ma.sid)\n\t}\n\tif string(nc.ps.ma.subject) != \"a\" {\n\t\tt.Fatalf(\"Wrong subject: '%s' instead of 'a'\", string(nc.ps.ma.subject))\n\t}\n\tif nc.ps.argBuf == nil {\n\t\tt.Fatal(\"Arg buffer should have been created\")\n\t}\n\tif nc.ps.msgBuf == nil {\n\t\tt.Fatal(\"Msg buffer should have been created\")\n\t}\n\n\texpectedCount++\n\texpectedSize += 3\n\n\tbuf = []byte(\"o\\r\\n\")\n\terr = nc.parse(buf)\n\tif err != nil {\n\t\tt.Fatalf(\"Parser error: %v\", err)\n\t}\n\tif (nc.Statistics.InMsgs != expectedCount) || (nc.Statistics.InBytes != expectedSize) {\n\t\tt.Fatalf(\"Wrong stats: %d - %d instead of %d - %d\", nc.Statistics.InMsgs, nc.Statistics.InBytes, expectedCount, expectedSize)\n\t}\n\tif (nc.ps.argBuf != nil) || (nc.ps.msgBuf != nil) {\n\t\tt.Fatal(\"Buffers should be nil now\")\n\t}\n\n\tbuf = []byte(\"MSG a 1 6\\r\\nfo\")\n\terr = nc.parse(buf)\n\tif err != nil {\n\t\tt.Fatalf(\"Parser error: %v\", err)\n\t}\n\tif nc.ps.ma.size != 6 {\n\t\tt.Fatalf(\"Wrong msg size: %d instead of 3\", nc.ps.ma.size)\n\t}\n\tif nc.ps.ma.sid != 1 {\n\t\tt.Fatalf(\"Wrong sid: %d instead of 1\", nc.ps.ma.sid)\n\t}\n\tif string(nc.ps.ma.subject) != \"a\" {\n\t\tt.Fatalf(\"Wrong subject: '%s' instead of 'a'\", string(nc.ps.ma.subject))\n\t}\n\tif nc.ps.argBuf == nil {\n\t\tt.Fatal(\"Arg buffer should have been created\")\n\t}\n\tif nc.ps.msgBuf == nil {\n\t\tt.Fatal(\"Msg buffer should have been created\")\n\t}\n\n\tbuf = []byte(\"ob\")\n\terr = nc.parse(buf)\n\tif err != nil {\n\t\tt.Fatalf(\"Parser error: %v\", err)\n\t}\n\n\texpectedCount++\n\texpectedSize += 6\n\n\tbuf = []byte(\"ar\\r\\n\")\n\terr = nc.parse(buf)\n\tif err != nil {\n\t\tt.Fatalf(\"Parser error: %v\", err)\n\t}\n\tif (nc.Statistics.InMsgs != expectedCount) || (nc.Statistics.InBytes != expectedSize) {\n\t\tt.Fatalf(\"Wrong stats: %d - %d instead of %d - %d\", nc.Statistics.InMsgs, nc.Statistics.InBytes, expectedCount, expectedSize)\n\t}\n\tif (nc.ps.argBuf != nil) || (nc.ps.msgBuf != nil) {\n\t\tt.Fatal(\"Buffers should be nil now\")\n\t}\n\n\t// Let's have a msg that is bigger than the parser's scratch size.\n\t// Since we prepopulate the msg with 'foo', adding 3 to the size.\n\tmsgSize := cap(nc.ps.scratch) + 100 + 3\n\tbuf = []byte(fmt.Sprintf(\"MSG a 1 b %d\\r\\nfoo\", msgSize))\n\terr = nc.parse(buf)\n\tif err != nil {\n\t\tt.Fatalf(\"Parser error: %v\", err)\n\t}\n\tif nc.ps.ma.size != msgSize {\n\t\tt.Fatalf(\"Wrong msg size: %d instead of %d\", nc.ps.ma.size, msgSize)\n\t}\n\tif nc.ps.ma.sid != 1 {\n\t\tt.Fatalf(\"Wrong sid: %d instead of 1\", nc.ps.ma.sid)\n\t}\n\tif string(nc.ps.ma.subject) != \"a\" {\n\t\tt.Fatalf(\"Wrong subject: '%s' instead of 'a'\", string(nc.ps.ma.subject))\n\t}\n\tif string(nc.ps.ma.reply) != \"b\" {\n\t\tt.Fatalf(\"Wrong reply: '%s' instead of 'b'\", string(nc.ps.ma.reply))\n\t}\n\tif nc.ps.argBuf == nil {\n\t\tt.Fatal(\"Arg buffer should have been created\")\n\t}\n\tif nc.ps.msgBuf == nil {\n\t\tt.Fatal(\"Msg buffer should have been created\")\n\t}\n\n\texpectedCount++\n\texpectedSize += uint64(msgSize)\n\n\tbufSize := msgSize - 3\n\n\tbuf = make([]byte, bufSize)\n\tfor i := 0; i < bufSize; i++ {\n\t\tbuf[i] = byte('a' + (i % 26))\n\t}\n\n\terr = nc.parse(buf)\n\tif err != nil {\n\t\tt.Fatalf(\"Parser error: %v\", err)\n\t}\n\tif nc.ps.state != MSG_PAYLOAD {\n\t\tt.Fatalf(\"Wrong state: %v instead of %v\", nc.ps.state, MSG_PAYLOAD)\n\t}\n\tif nc.ps.ma.size != msgSize {\n\t\tt.Fatalf(\"Wrong (ma) msg size: %d instead of %d\", nc.ps.ma.size, msgSize)\n\t}\n\tif len(nc.ps.msgBuf) != msgSize {\n\t\tt.Fatalf(\"Wrong msg size: %d instead of %d\", len(nc.ps.msgBuf), msgSize)\n\t}\n\t// Check content:\n\tif string(nc.ps.msgBuf[0:3]) != \"foo\" {\n\t\tt.Fatalf(\"Wrong msg content: %s\", string(nc.ps.msgBuf))\n\t}\n\tfor k := 3; k < nc.ps.ma.size; k++ {\n\t\tif nc.ps.msgBuf[k] != byte('a'+((k-3)%26)) {\n\t\t\tt.Fatalf(\"Wrong msg content: %s\", string(nc.ps.msgBuf))\n\t\t}\n\t}\n\n\tbuf = []byte(\"\\r\\n\")\n\tif err := nc.parse(buf); err != nil {\n\t\tt.Fatalf(\"Unexpected error during parsing: %v\", err)\n\t}\n\tif (nc.Statistics.InMsgs != expectedCount) || (nc.Statistics.InBytes != expectedSize) {\n\t\tt.Fatalf(\"Wrong stats: %d - %d instead of %d - %d\", nc.Statistics.InMsgs, nc.Statistics.InBytes, expectedCount, expectedSize)\n\t}\n\tif (nc.ps.argBuf != nil) || (nc.ps.msgBuf != nil) {\n\t\tt.Fatal(\"Buffers should be nil now\")\n\t}\n\tif nc.ps.state != OP_START {\n\t\tt.Fatalf(\"Wrong state: %v\", nc.ps.state)\n\t}\n}\n\nfunc TestNormalizeError(t *testing.T) {\n\texpected := \"Typical Error\"\n\tif s := normalizeErr(\"-ERR '\" + expected + \"'\"); s != expected {\n\t\tt.Fatalf(\"Expected '%s', got '%s'\", expected, s)\n\t}\n\n\texpected = \"Trim Surrounding Spaces\"\n\tif s := normalizeErr(\"-ERR    '\" + expected + \"'   \"); s != expected {\n\t\tt.Fatalf(\"Expected '%s', got '%s'\", expected, s)\n\t}\n\n\texpected = \"Trim Surrounding Spaces Without Quotes\"\n\tif s := normalizeErr(\"-ERR    \" + expected + \"   \"); s != expected {\n\t\tt.Fatalf(\"Expected '%s', got '%s'\", expected, s)\n\t}\n\n\texpected = \"Error Without Quotes\"\n\tif s := normalizeErr(\"-ERR \" + expected); s != expected {\n\t\tt.Fatalf(\"Expected '%s', got '%s'\", expected, s)\n\t}\n\n\texpected = \"Error With Quote Only On Left\"\n\tif s := normalizeErr(\"-ERR '\" + expected); s != expected {\n\t\tt.Fatalf(\"Expected '%s', got '%s'\", expected, s)\n\t}\n\n\texpected = \"Error With Quote Only On Right\"\n\tif s := normalizeErr(\"-ERR \" + expected + \"'\"); s != expected {\n\t\tt.Fatalf(\"Expected '%s', got '%s'\", expected, s)\n\t}\n}\n\nfunc TestAsyncINFO(t *testing.T) {\n\topts := GetDefaultOptions()\n\tc := &Conn{Opts: opts}\n\n\tc.ps = &parseState{}\n\n\tif c.ps.state != OP_START {\n\t\tt.Fatalf(\"Expected OP_START vs %d\\n\", c.ps.state)\n\t}\n\n\tinfo := []byte(\"INFO {}\\r\\n\")\n\tif c.ps.state != OP_START {\n\t\tt.Fatalf(\"Expected OP_START vs %d\\n\", c.ps.state)\n\t}\n\terr := c.parse(info[:1])\n\tif err != nil || c.ps.state != OP_I {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\terr = c.parse(info[1:2])\n\tif err != nil || c.ps.state != OP_IN {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\terr = c.parse(info[2:3])\n\tif err != nil || c.ps.state != OP_INF {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\terr = c.parse(info[3:4])\n\tif err != nil || c.ps.state != OP_INFO {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\terr = c.parse(info[4:5])\n\tif err != nil || c.ps.state != OP_INFO_SPC {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\terr = c.parse(info[5:])\n\tif err != nil || c.ps.state != OP_START {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\n\t// All at once\n\terr = c.parse(info)\n\tif err != nil || c.ps.state != OP_START {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\n\t// Server pool needs to be setup\n\tc.setupServerPool()\n\n\t// Partials requiring argBuf\n\texpectedServer := serverInfo{\n\t\tID:           \"test\",\n\t\tHost:         \"localhost\",\n\t\tPort:         4222,\n\t\tAuthRequired: true,\n\t\tTLSRequired:  true,\n\t\tMaxPayload:   2 * 1024 * 1024,\n\t\tConnectURLs:  []string{\"localhost:5222\", \"localhost:6222\"},\n\t}\n\t// Set NoRandomize so that the check with expectedServer info\n\t// matches.\n\tc.Opts.NoRandomize = true\n\n\tb, _ := json.Marshal(expectedServer)\n\tinfo = []byte(fmt.Sprintf(\"INFO %s\\r\\n\", b))\n\tif c.ps.state != OP_START {\n\t\tt.Fatalf(\"Expected OP_START vs %d\\n\", c.ps.state)\n\t}\n\terr = c.parse(info[:9])\n\tif err != nil || c.ps.state != INFO_ARG || c.ps.argBuf == nil {\n\t\tt.Fatalf(\"Unexpected: %d err: %v argBuf: %v\\n\", c.ps.state, err, c.ps.argBuf)\n\t}\n\terr = c.parse(info[9:11])\n\tif err != nil || c.ps.state != INFO_ARG || c.ps.argBuf == nil {\n\t\tt.Fatalf(\"Unexpected: %d err: %v argBuf: %v\\n\", c.ps.state, err, c.ps.argBuf)\n\t}\n\terr = c.parse(info[11:])\n\tif err != nil || c.ps.state != OP_START || c.ps.argBuf != nil {\n\t\tt.Fatalf(\"Unexpected: %d err: %v argBuf: %v\\n\", c.ps.state, err, c.ps.argBuf)\n\t}\n\tif !reflect.DeepEqual(c.info, expectedServer) {\n\t\tt.Fatalf(\"Expected server info to be: %v, got: %v\", expectedServer, c.info)\n\t}\n\n\t// Good INFOs\n\tgood := []string{\"INFO {}\\r\\n\", \"INFO  {}\\r\\n\", \"INFO {} \\r\\n\", \"INFO { \\\"server_id\\\": \\\"test\\\"  }   \\r\\n\", \"INFO {\\\"connect_urls\\\":[]}\\r\\n\"}\n\tfor _, gi := range good {\n\t\tc.ps = &parseState{}\n\t\terr = c.parse([]byte(gi))\n\t\tif err != nil || c.ps.state != OP_START {\n\t\t\tt.Fatalf(\"Protocol %q should be fine. Err=%v state=%v\", gi, err, c.ps.state)\n\t\t}\n\t}\n\n\t// Wrong INFOs\n\twrong := []string{\"IxNFO {}\\r\\n\", \"INxFO {}\\r\\n\", \"INFxO {}\\r\\n\", \"INFOx {}\\r\\n\", \"INFO{}\\r\\n\", \"INFO {}\"}\n\tfor _, wi := range wrong {\n\t\tc.ps = &parseState{}\n\t\terr = c.parse([]byte(wi))\n\t\tif err == nil && c.ps.state == OP_START {\n\t\t\tt.Fatalf(\"Protocol %q should have failed\", wi)\n\t\t}\n\t}\n\n\tcheckPool := func(urls ...string) {\n\t\t// Check both pool and urls map\n\t\tif len(c.srvPool) != len(urls) {\n\t\t\tstackFatalf(t, \"Pool should have %d elements, has %d\", len(urls), len(c.srvPool))\n\t\t}\n\t\tif len(c.urls) != len(urls) {\n\t\t\tstackFatalf(t, \"Map should have %d elements, has %d\", len(urls), len(c.urls))\n\t\t}\n\t\tfor _, url := range urls {\n\t\t\tif _, present := c.urls[url]; !present {\n\t\t\t\tstackFatalf(t, \"Pool should have %q\", url)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Now test the decoding of \"connect_urls\"\n\n\t// Reset the pool\n\tc.setupServerPool()\n\t// Reinitialize the parser\n\tc.ps = &parseState{}\n\n\tinfo = []byte(\"INFO {\\\"connect_urls\\\":[\\\"localhost:4222\\\", \\\"localhost:5222\\\"]}\\r\\n\")\n\terr = c.parse(info)\n\tif err != nil || c.ps.state != OP_START {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\t// Pool now should contain 127.0.0.1:4222 (the default URL), localhost:4222 and localhost:5222\n\tcheckPool(\"127.0.0.1:4222\", \"localhost:4222\", \"localhost:5222\")\n\n\t// Make sure that if client receives the same, it is not added again.\n\terr = c.parse(info)\n\tif err != nil || c.ps.state != OP_START {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\t// Pool should still contain 127.0.0.1:4222 (the default URL), localhost:4222 and localhost:5222\n\tcheckPool(\"127.0.0.1:4222\", \"localhost:4222\", \"localhost:5222\")\n\n\t// Receive a new URL\n\tinfo = []byte(\"INFO {\\\"connect_urls\\\":[\\\"localhost:4222\\\", \\\"localhost:5222\\\", \\\"localhost:6222\\\"]}\\r\\n\")\n\terr = c.parse(info)\n\tif err != nil || c.ps.state != OP_START {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\t// Pool now should contain 127.0.0.1:4222 (the default URL), localhost:4222, localhost:5222 and localhost:6222\n\tcheckPool(\"127.0.0.1:4222\", \"localhost:4222\", \"localhost:5222\", \"localhost:6222\")\n\n\t// Check that pool may be randomized on setup, but new URLs are always\n\t// added at end of pool.\n\tc.Opts.NoRandomize = false\n\tc.Opts.Servers = testServers\n\t// Reset the pool\n\tc.setupServerPool()\n\t// Reinitialize the parser\n\tc.ps = &parseState{}\n\t// Capture the pool sequence after randomization\n\turlsAfterPoolSetup := make([]string, 0, len(c.srvPool))\n\tfor _, srv := range c.srvPool {\n\t\turlsAfterPoolSetup = append(urlsAfterPoolSetup, srv.url.Host)\n\t}\n\tcheckNewURLsAddedRandomly := func() {\n\t\tt.Helper()\n\t\tvar ok bool\n\t\tfor i := 0; i < len(urlsAfterPoolSetup); i++ {\n\t\t\tif c.srvPool[i].url.Host != urlsAfterPoolSetup[i] {\n\t\t\t\tok = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !ok {\n\t\t\tt.Fatalf(\"New URLs were not added randmonly: %q\", c.Servers())\n\t\t}\n\t}\n\t// Add new urls\n\tnewURLs := \"\\\"impA:4222\\\", \\\"impB:4222\\\", \\\"impC:4222\\\", \" +\n\t\t\"\\\"impD:4222\\\", \\\"impE:4222\\\", \\\"impF:4222\\\", \\\"impG:4222\\\", \" +\n\t\t\"\\\"impH:4222\\\", \\\"impI:4222\\\", \\\"impJ:4222\\\"\"\n\tinfo = []byte(\"INFO {\\\"connect_urls\\\":[\" + newURLs + \"]}\\r\\n\")\n\terr = c.parse(info)\n\tif err != nil || c.ps.state != OP_START {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\tcheckNewURLsAddedRandomly()\n\t// Check that we have not moved the first URL\n\tif u := c.srvPool[0].url.Host; u != urlsAfterPoolSetup[0] {\n\t\tt.Fatalf(\"Expected first URL to be %q, got %q\", urlsAfterPoolSetup[0], u)\n\t}\n}\n\nfunc TestConnServers(t *testing.T) {\n\topts := GetDefaultOptions()\n\tc := &Conn{Opts: opts}\n\tc.ps = &parseState{}\n\tc.setupServerPool()\n\n\tvalidateURLs := func(serverUrls []string, expectedUrls ...string) {\n\t\tvar found bool\n\t\tif len(serverUrls) != len(expectedUrls) {\n\t\t\tstackFatalf(t, \"Array should have %d elements, has %d\", len(expectedUrls), len(serverUrls))\n\t\t}\n\n\t\tfor _, ev := range expectedUrls {\n\t\t\tfound = false\n\t\t\tfor _, av := range serverUrls {\n\t\t\t\tif ev == av {\n\t\t\t\t\tfound = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !found {\n\t\t\t\tstackFatalf(t, \"array is missing %q in %v\", ev, serverUrls)\n\t\t\t}\n\t\t}\n\t}\n\n\t// check the default url\n\tvalidateURLs(c.Servers(), \"nats://127.0.0.1:4222\")\n\tif len(c.DiscoveredServers()) != 0 {\n\t\tt.Fatalf(\"Expected no discovered servers\")\n\t}\n\n\t// Add a new URL\n\terr := c.parse([]byte(\"INFO {\\\"connect_urls\\\":[\\\"localhost:5222\\\"]}\\r\\n\"))\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected: %d : %v\\n\", c.ps.state, err)\n\t}\n\t// Server list should now contain both the default and the new url.\n\tvalidateURLs(c.Servers(), \"nats://127.0.0.1:4222\", \"nats://localhost:5222\")\n\t// Discovered servers should only contain the new url.\n\tvalidateURLs(c.DiscoveredServers(), \"nats://localhost:5222\")\n\n\t// verify user credentials are stripped out.\n\topts.Servers = []string{\"nats://user:pass@localhost:4333\", \"nats://token@localhost:4444\"}\n\tc = &Conn{Opts: opts}\n\tc.ps = &parseState{}\n\tc.setupServerPool()\n\n\tvalidateURLs(c.Servers(), \"nats://localhost:4333\", \"nats://localhost:4444\")\n}\n\nfunc TestNoEchoOldServer(t *testing.T) {\n\topts := GetDefaultOptions()\n\topts.Url = DefaultURL\n\topts.NoEcho = true\n\n\tnc := &Conn{Opts: opts}\n\tif err := nc.setupServerPool(); err != nil {\n\t\tt.Fatalf(\"Problem setting up Server Pool: %v\\n\", err)\n\t}\n\n\t// Old style with no proto, meaning 0. We need Proto:1 for NoEcho support.\n\toldInfo := \"{\\\"server_id\\\":\\\"22\\\",\\\"version\\\":\\\"1.1.0\\\",\\\"go\\\":\\\"go1.10.2\\\",\\\"port\\\":4222,\\\"max_payload\\\":1048576}\"\n\n\terr := nc.processInfo(oldInfo)\n\tif err != nil {\n\t\tt.Fatalf(\"Error processing old style INFO: %v\\n\", err)\n\t}\n\n\t// Make sure connectProto generates an error.\n\t_, err = nc.connectProto()\n\tif err == nil {\n\t\tt.Fatalf(\"Expected an error but got none\\n\")\n\t}\n}\n\nfunc TestExpiredAuthentication(t *testing.T) {\n\t// The goal of these tests was to check how a client with an expiring JWT\n\t// behaves. It should receive an async -ERR indicating that the auth\n\t// has expired, which will trigger reconnects. There, the lib should\n\t// received -ERR for auth violation in response to the CONNECT (instead\n\t// of the PONG). The library should close the connection after receiving\n\t// twice the same auth error.\n\t// If we use an actual JWT that expires, the way the JWT library expires\n\t// a JWT cause the server to send the async -ERR first but then accepts\n\t// the CONNECT (since JWT lib does not say that it has expired), but\n\t// when the server sets up the expire callback, that callback fires right\n\t// away and so client receives async -ERR again.\n\t// So for a deterministic test, we won't use an actual NATS Server.\n\t// Instead, we will use a mock that simply returns appropriate -ERR and\n\t// ensure the client behaves as expected.\n\tfor _, test := range []struct {\n\t\tname          string\n\t\texpectedProto string\n\t\texpectedErr   error\n\t\tignoreAbort   bool\n\t}{\n\t\t{\"expired users credentials\", AUTHENTICATION_EXPIRED_ERR, ErrAuthExpired, false},\n\t\t{\"revoked users credentials\", AUTHENTICATION_REVOKED_ERR, ErrAuthRevoked, false},\n\t\t{\"expired account\", ACCOUNT_AUTHENTICATION_EXPIRED_ERR, ErrAccountAuthExpired, false},\n\t\t{\"expired users credentials\", AUTHENTICATION_EXPIRED_ERR, ErrAuthExpired, true},\n\t\t{\"revoked users credentials\", AUTHENTICATION_REVOKED_ERR, ErrAuthRevoked, true},\n\t\t{\"expired account\", ACCOUNT_AUTHENTICATION_EXPIRED_ERR, ErrAccountAuthExpired, true},\n\t} {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tl, e := net.Listen(\"tcp\", \"127.0.0.1:0\")\n\t\t\tif e != nil {\n\t\t\t\tt.Fatal(\"Could not listen on an ephemeral port\")\n\t\t\t}\n\t\t\ttl := l.(*net.TCPListener)\n\t\t\tdefer tl.Close()\n\n\t\t\taddr := tl.Addr().(*net.TCPAddr)\n\n\t\t\twg := sync.WaitGroup{}\n\t\t\twg.Add(1)\n\n\t\t\tgo func() {\n\t\t\t\tdefer wg.Done()\n\t\t\t\tconnect := 0\n\t\t\t\tfor {\n\t\t\t\t\tconn, err := l.Accept()\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tdefer conn.Close()\n\n\t\t\t\t\tinfo := \"INFO {\\\"server_id\\\":\\\"foobar\\\",\\\"nonce\\\":\\\"anonce\\\"}\\r\\n\"\n\t\t\t\t\tconn.Write([]byte(info))\n\n\t\t\t\t\t// Read connect and ping commands sent from the client\n\t\t\t\t\tbr := bufio.NewReaderSize(conn, 10*1024)\n\t\t\t\t\tbr.ReadLine()\n\t\t\t\t\tbr.ReadLine()\n\n\t\t\t\t\tif connect++; connect == 1 {\n\t\t\t\t\t\tconn.Write([]byte(fmt.Sprintf(\"%s%s\", _PONG_OP_, _CRLF_)))\n\t\t\t\t\t\ttime.Sleep(300 * time.Millisecond)\n\t\t\t\t\t\tconn.Write([]byte(fmt.Sprintf(\"-ERR '%s'\\r\\n\", test.expectedProto)))\n\t\t\t\t\t} else {\n\t\t\t\t\t\tconn.Write([]byte(fmt.Sprintf(\"-ERR '%s'\\r\\n\", AUTHORIZATION_ERR)))\n\t\t\t\t\t}\n\t\t\t\t\tconn.Close()\n\t\t\t\t}\n\t\t\t}()\n\n\t\t\tch := make(chan bool)\n\t\t\terrCh := make(chan error, 10)\n\n\t\t\turl := fmt.Sprintf(\"nats://127.0.0.1:%d\", addr.Port)\n\t\t\topts := []Option{\n\t\t\t\tReconnectWait(25 * time.Millisecond),\n\t\t\t\tReconnectJitter(0, 0),\n\t\t\t\tMaxReconnects(-1),\n\t\t\t\tErrorHandler(func(_ *Conn, _ *Subscription, e error) {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase errCh <- e:\n\t\t\t\t\tdefault:\n\t\t\t\t\t}\n\t\t\t\t}),\n\t\t\t\tClosedHandler(func(nc *Conn) {\n\t\t\t\t\tch <- true\n\t\t\t\t}),\n\t\t\t}\n\t\t\tif test.ignoreAbort {\n\t\t\t\topts = append(opts, IgnoreAuthErrorAbort())\n\t\t\t}\n\t\t\tnc, err := Connect(url, opts...)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Expected to connect, got %v\", err)\n\t\t\t}\n\t\t\tdefer nc.Close()\n\n\t\t\tif test.ignoreAbort {\n\t\t\t\t// We expect more than 3 errors, as the connect attempt should not be aborted after 2 failed attempts.\n\t\t\t\tfor i := 0; i < 4; i++ {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase e := <-errCh:\n\t\t\t\t\t\tif i == 0 && e != test.expectedErr {\n\t\t\t\t\t\t\tt.Fatalf(\"Expected error %q, got %q\", test.expectedErr, e)\n\t\t\t\t\t\t} else if i > 0 && e != ErrAuthorization {\n\t\t\t\t\t\t\tt.Fatalf(\"Expected error %q, got %q\", ErrAuthorization, e)\n\t\t\t\t\t\t}\n\t\t\t\t\tcase <-time.After(time.Second):\n\t\t\t\t\t\tif i == 0 {\n\t\t\t\t\t\t\tt.Fatalf(\"Missing %q error\", test.expectedErr)\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tt.Fatalf(\"Missing %q error\", ErrAuthorization)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// We should give up since we get the same error on both tries.\n\t\t\tif err := WaitTime(ch, 2*time.Second); err != nil {\n\t\t\t\tt.Fatal(\"Should have closed after multiple failed attempts.\")\n\t\t\t}\n\t\t\tif stats := nc.Stats(); stats.Reconnects > 2 {\n\t\t\t\tt.Fatalf(\"Expected at most 2 reconnects, got %d\", stats.Reconnects)\n\t\t\t}\n\n\t\t\t// We expect 3 errors, the expired auth/revoke error, then 2 AUTHORIZATION_ERR\n\t\t\t// before the connection is closed.\n\t\t\tfor i := 0; i < 3; i++ {\n\t\t\t\tselect {\n\t\t\t\tcase e := <-errCh:\n\t\t\t\t\tif i == 0 && e != test.expectedErr {\n\t\t\t\t\t\tt.Fatalf(\"Expected error %q, got %q\", test.expectedErr, e)\n\t\t\t\t\t} else if i > 0 && e != ErrAuthorization {\n\t\t\t\t\t\tt.Fatalf(\"Expected error %q, got %q\", ErrAuthorization, e)\n\t\t\t\t\t}\n\t\t\t\tdefault:\n\t\t\t\t\tif i == 0 {\n\t\t\t\t\t\tt.Fatalf(\"Missing %q error\", test.expectedErr)\n\t\t\t\t\t} else {\n\t\t\t\t\t\tt.Fatalf(\"Missing %q error\", ErrAuthorization)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t// We should not have any more error\n\t\t\tselect {\n\t\t\tcase e := <-errCh:\n\t\t\t\tt.Fatalf(\"Extra error: %v\", e)\n\t\t\tdefault:\n\t\t\t}\n\t\t\t// Close the listener and wait for go routine to end.\n\t\t\tl.Close()\n\t\t\twg.Wait()\n\t\t})\n\t}\n}\n\nfunc createTmpFile(t *testing.T, content []byte) string {\n\tt.Helper()\n\tconf, err := os.CreateTemp(\"\", \"\")\n\tif err != nil {\n\t\tt.Fatalf(\"Error creating conf file: %v\", err)\n\t}\n\tfName := conf.Name()\n\tconf.Close()\n\tif err := os.WriteFile(fName, content, 0666); err != nil {\n\t\tos.Remove(fName)\n\t\tt.Fatalf(\"Error writing conf file: %v\", err)\n\t}\n\treturn fName\n}\n\nfunc TestNKeyOptionFromSeed(t *testing.T) {\n\tif _, err := NkeyOptionFromSeed(\"file_that_does_not_exist\"); err == nil {\n\t\tt.Fatal(\"Expected error got none\")\n\t}\n\n\tseedFile := createTmpFile(t, []byte(`\n\t\t# No seed\n\t\tTHIS_NOT_A_NKEY_SEED\n\t`))\n\tdefer os.Remove(seedFile)\n\tif _, err := NkeyOptionFromSeed(seedFile); err == nil || !strings.Contains(err.Error(), \"seed found\") {\n\t\tt.Fatalf(\"Expected error about seed not found, got %v\", err)\n\t}\n\tos.Remove(seedFile)\n\n\tseedFile = createTmpFile(t, []byte(`\n\t\t# Invalid seed\n\t\tSUBADSEED\n\t`))\n\t// Make sure that we detect SU (trim space) but it still fails because\n\t// this is not a valid NKey.\n\tif _, err := NkeyOptionFromSeed(seedFile); err == nil || strings.Contains(err.Error(), \"seed found\") {\n\t\tt.Fatalf(\"Expected error about invalid key, got %v\", err)\n\t}\n\tos.Remove(seedFile)\n\n\tkp, _ := nkeys.CreateUser()\n\tseed, _ := kp.Seed()\n\tseedFile = createTmpFile(t, seed)\n\topt, err := NkeyOptionFromSeed(seedFile)\n\tif err != nil {\n\t\tt.Fatalf(\"Error: %v\", err)\n\t}\n\n\tl, e := net.Listen(\"tcp\", \"127.0.0.1:0\")\n\tif e != nil {\n\t\tt.Fatal(\"Could not listen on an ephemeral port\")\n\t}\n\ttl := l.(*net.TCPListener)\n\tdefer tl.Close()\n\n\taddr := tl.Addr().(*net.TCPAddr)\n\n\tch := make(chan bool, 1)\n\terrCh := make(chan error, 1)\n\trs := func(ch chan bool) {\n\t\tconn, err := l.Accept()\n\t\tif err != nil {\n\t\t\terrCh <- fmt.Errorf(\"error accepting client connection: %v\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer conn.Close()\n\t\tinfo := \"INFO {\\\"server_id\\\":\\\"foobar\\\",\\\"nonce\\\":\\\"anonce\\\"}\\r\\n\"\n\t\tconn.Write([]byte(info))\n\n\t\t// Read connect and ping commands sent from the client\n\t\tbr := bufio.NewReaderSize(conn, 10*1024)\n\t\tline, _, err := br.ReadLine()\n\t\tif err != nil {\n\t\t\terrCh <- fmt.Errorf(\"expected CONNECT and PING from client, got: %s\", err)\n\t\t\treturn\n\t\t}\n\t\t// If client got an error reading the seed, it will not send it\n\t\tif bytes.Contains(line, []byte(`\"sig\":`)) {\n\t\t\tconn.Write([]byte(\"PONG\\r\\n\"))\n\t\t} else {\n\t\t\tconn.Write([]byte(`-ERR go away\\r\\n`))\n\t\t\tconn.Close()\n\t\t}\n\t\t// Now wait to be notified that we can finish\n\t\t<-ch\n\t\terrCh <- nil\n\t}\n\tgo rs(ch)\n\n\tnc, err := Connect(fmt.Sprintf(\"nats://127.0.0.1:%d\", addr.Port), opt)\n\tif err != nil {\n\t\tt.Fatalf(\"Error on connect: %v\", err)\n\t}\n\tnc.Close()\n\tclose(ch)\n\n\tcheckErrChannel(t, errCh)\n\n\t// Now that option is already created, change content of file\n\tos.WriteFile(seedFile, []byte(`xxxxx`), 0666)\n\tch = make(chan bool, 1)\n\tgo rs(ch)\n\n\tif _, err := Connect(fmt.Sprintf(\"nats://127.0.0.1:%d\", addr.Port), opt); err == nil {\n\t\tt.Fatal(\"Expected error, got none\")\n\t}\n\tclose(ch)\n\tcheckErrChannel(t, errCh)\n}\n\nfunc TestNoPanicOnSrvPoolSizeChanging(t *testing.T) {\n\tlisteners := []net.Listener{}\n\tports := []int{}\n\n\tfor i := 0; i < 3; i++ {\n\t\tl, err := net.Listen(\"tcp\", \"127.0.0.1:0\")\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"Could not listen on an ephemeral port: %v\", err)\n\t\t}\n\t\tdefer l.Close()\n\t\ttl := l.(*net.TCPListener)\n\t\tports = append(ports, tl.Addr().(*net.TCPAddr).Port)\n\t\tlisteners = append(listeners, l)\n\t}\n\n\twg := sync.WaitGroup{}\n\twg.Add(len(listeners))\n\n\tconnect := int32(0)\n\tsrv := func(l net.Listener) {\n\t\tdefer wg.Done()\n\t\tfor {\n\t\t\tconn, err := l.Accept()\n\t\t\tif err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tdefer conn.Close()\n\n\t\t\tvar info string\n\n\t\t\treject := atomic.AddInt32(&connect, 1) <= 2\n\t\t\tif reject {\n\t\t\t\t// Sends a list of 3 servers, where the second does not actually run.\n\t\t\t\t// This server is going to reject the connect (with auth error), so\n\t\t\t\t// client will move to 2nd, fail, then go to third...\n\t\t\t\tinfo = fmt.Sprintf(\"INFO {\\\"server_id\\\":\\\"foobar\\\",\\\"connect_urls\\\":[\\\"127.0.0.1:%d\\\",\\\"127.0.0.1:%d\\\",\\\"127.0.0.1:%d\\\"]}\\r\\n\",\n\t\t\t\t\tports[0], ports[1], ports[2])\n\t\t\t} else {\n\t\t\t\t// This third server will return the INFO with only the original server\n\t\t\t\t// and the third one, which will make the srvPool size shrink down to 2.\n\t\t\t\tinfo = fmt.Sprintf(\"INFO {\\\"server_id\\\":\\\"foobar\\\",\\\"connect_urls\\\":[\\\"127.0.0.1:%d\\\",\\\"127.0.0.1:%d\\\"]}\\r\\n\",\n\t\t\t\t\tports[0], ports[2])\n\t\t\t}\n\t\t\tconn.Write([]byte(info))\n\n\t\t\t// Read connect and ping commands sent from the client\n\t\t\tbr := bufio.NewReaderSize(conn, 10*1024)\n\t\t\tbr.ReadLine()\n\t\t\tbr.ReadLine()\n\n\t\t\tif reject {\n\t\t\t\tconn.Write([]byte(fmt.Sprintf(\"-ERR '%s'\\r\\n\", AUTHORIZATION_ERR)))\n\t\t\t\tconn.Close()\n\t\t\t} else {\n\t\t\t\tconn.Write([]byte(pongProto))\n\t\t\t\tbr.ReadLine()\n\t\t\t}\n\t\t}\n\t}\n\n\tfor _, l := range listeners {\n\t\tgo srv(l)\n\t}\n\n\ttime.Sleep(250 * time.Millisecond)\n\n\tnc, err := Connect(fmt.Sprintf(\"nats://127.0.0.1:%d\", ports[0]))\n\tif err != nil {\n\t\tt.Fatalf(\"Error on connect: %v\", err)\n\t}\n\tnc.Close()\n\tfor _, l := range listeners {\n\t\tl.Close()\n\t}\n\twg.Wait()\n}\n\nfunc TestHeaderParser(t *testing.T) {\n\tshouldErr := func(hdr string) {\n\t\tt.Helper()\n\t\tif _, err := DecodeHeadersMsg([]byte(hdr)); err == nil {\n\t\t\tt.Fatalf(\"Expected an error\")\n\t\t}\n\t}\n\tshouldErr(\"NATS/1.0\")\n\tshouldErr(\"NATS/1.0\\r\\n\")\n\tshouldErr(\"NATS/1.0\\r\\nk1:v1\")\n\tshouldErr(\"NATS/1.0\\r\\nk1:v1\\r\\n\")\n\n\t// Check that we can do inline status and descriptions\n\tcheckStatus := func(hdr string, status int, description string) {\n\t\tt.Helper()\n\t\thdrs, err := DecodeHeadersMsg([]byte(hdr + \"\\r\\n\\r\\n\"))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"Unexpected error: %v\", err)\n\t\t}\n\t\tif code, err := strconv.Atoi(hdrs.Get(statusHdr)); err != nil || code != status {\n\t\t\tt.Fatalf(\"Expected status of %d, got %s\", status, hdrs.Get(statusHdr))\n\t\t}\n\t\tif len(description) > 0 {\n\t\t\tif descr := hdrs.Get(descrHdr); err != nil || descr != description {\n\t\t\t\tt.Fatalf(\"Expected description of %q, got %q\", description, descr)\n\t\t\t}\n\t\t}\n\t}\n\n\tcheckStatus(\"NATS/1.0 503\", 503, \"\")\n\tcheckStatus(\"NATS/1.0 503 No Responders\", 503, \"No Responders\")\n\tcheckStatus(\"NATS/1.0  404   No Messages\", 404, \"No Messages\")\n}\n\nfunc TestHeaderMultiLine(t *testing.T) {\n\tm := NewMsg(\"foo\")\n\tm.Header = Header{\n\t\t\"CorrelationID\": []string{\"123\"},\n\t\t\"Msg-ID\":        []string{\"456\"},\n\t\t\"X-NATS-Keys\":   []string{\"A\", \"B\", \"C\"},\n\t\t\"X-Test-Keys\":   []string{\"D\", \"E\", \"F\"},\n\t}\n\t// Users can opt-in to canonicalize like http.Header does\n\t// by using http.Header#Set or http.Header#Add.\n\thttp.Header(m.Header).Set(\"accept-encoding\", \"json\")\n\thttp.Header(m.Header).Add(\"AUTHORIZATION\", \"s3cr3t\")\n\n\t// Multi Value Header becomes represented as multi-lines in the wire\n\t// since internally using same Write from http stdlib.\n\tm.Header.Set(\"X-Test\", \"First\")\n\tm.Header.Add(\"X-Test\", \"Second\")\n\tm.Header.Add(\"X-Test\", \"Third\")\n\n\tb, err := m.headerBytes()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tresult := string(b)\n\n\texpectedHeader := `NATS/1.0\nAccept-Encoding: json\nAuthorization: s3cr3t\nCorrelationID: 123\nMsg-ID: 456\nX-NATS-Keys: A\nX-NATS-Keys: B\nX-NATS-Keys: C\nX-Test: First\nX-Test: Second\nX-Test: Third\nX-Test-Keys: D\nX-Test-Keys: E\nX-Test-Keys: F\n\n`\n\tif strings.Replace(expectedHeader, \"\\n\", \"\\r\\n\", -1) != result {\n\t\tt.Fatalf(\"Expected: %q, got: %q\", expectedHeader, result)\n\t}\n}\n\nfunc TestLameDuckMode(t *testing.T) {\n\tl, err := net.Listen(\"tcp\", \"127.0.0.1:0\")\n\tif err != nil {\n\t\tt.Fatalf(\"Could not listen on an ephemeral port: %v\", err)\n\t}\n\ttl := l.(*net.TCPListener)\n\tdefer tl.Close()\n\n\taddr := tl.Addr().(*net.TCPAddr)\n\n\twg := sync.WaitGroup{}\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\tldmInfos := []string{\"INFO {\\\"ldm\\\":true}\\r\\n\", \"INFO {\\\"connect_urls\\\":[\\\"127.0.0.1:1234\\\"],\\\"ldm\\\":true}\\r\\n\"}\n\t\tfor _, ldmInfo := range ldmInfos {\n\t\t\tconn, err := l.Accept()\n\t\t\tif err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tdefer conn.Close()\n\n\t\t\tinfo := \"INFO {\\\"server_id\\\":\\\"foobar\\\"}\\r\\n\"\n\t\t\tconn.Write([]byte(info))\n\n\t\t\t// Read connect and ping commands sent from the client\n\t\t\tbr := bufio.NewReaderSize(conn, 10*1024)\n\t\t\tbr.ReadLine()\n\t\t\tbr.ReadLine()\n\t\t\tconn.Write([]byte(pongProto))\n\n\t\t\t// Wait a bit and then send a INFO with LDM\n\t\t\ttime.Sleep(100 * time.Millisecond)\n\t\t\tconn.Write([]byte(ldmInfo))\n\t\t\tbr.ReadLine()\n\t\t\tconn.Close()\n\t\t}\n\t}()\n\n\turl := fmt.Sprintf(\"nats://127.0.0.1:%d\", addr.Port)\n\ttime.Sleep(100 * time.Millisecond)\n\n\tfor _, test := range []struct {\n\t\tname  string\n\t\tcurls bool\n\t}{\n\t\t{\"without connect urls\", false},\n\t\t{\"with connect urls\", true},\n\t} {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tch := make(chan bool, 1)\n\t\t\terrCh := make(chan error, 1)\n\t\t\tnc, err := Connect(url,\n\t\t\t\tDiscoveredServersHandler(func(nc *Conn) {\n\t\t\t\t\tds := nc.DiscoveredServers()\n\t\t\t\t\tif !reflect.DeepEqual(ds, []string{\"nats://127.0.0.1:1234\"}) {\n\t\t\t\t\t\terrCh <- fmt.Errorf(\"wrong discovered servers: %q\", ds)\n\t\t\t\t\t} else {\n\t\t\t\t\t\terrCh <- nil\n\t\t\t\t\t}\n\t\t\t\t}),\n\t\t\t\tLameDuckModeHandler(func(_ *Conn) {\n\t\t\t\t\tch <- true\n\t\t\t\t}),\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Expected to connect, got %v\", err)\n\t\t\t}\n\t\t\tdefer nc.Close()\n\n\t\t\tselect {\n\t\t\tcase <-ch:\n\t\t\tcase <-time.After(2 * time.Second):\n\t\t\t\tt.Fatal(\"should have been notified of LDM\")\n\t\t\t}\n\t\t\tselect {\n\t\t\tcase e := <-errCh:\n\t\t\t\tif !test.curls {\n\t\t\t\t\tt.Fatal(\"should not have received connect urls\")\n\t\t\t\t} else if e != nil {\n\t\t\t\t\tt.Fatal(e.Error())\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\tif test.curls {\n\t\t\t\t\tt.Fatal(\"should have received notification about discovered servers\")\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tnc.Close()\n\t\t})\n\t}\n\twg.Wait()\n}\n\nfunc BenchmarkHeaderDecode(b *testing.B) {\n\tbenchmarks := []struct {\n\t\tname   string\n\t\theader Header\n\t}{\n\t\t{\"Small - 25\", Header{\n\t\t\t\"Msg-ID\": []string{\"123\"}},\n\t\t},\n\t\t{\"Medium - 141\", Header{\n\t\t\t\"CorrelationID\": []string{\"123\"},\n\t\t\t\"Msg-ID\":        []string{\"456\"},\n\t\t\t\"X-NATS-Keys\":   []string{\"A\", \"B\", \"C\"},\n\t\t\t\"X-Test-Keys\":   []string{\"D\", \"E\", \"F\"},\n\t\t}},\n\t\t{\"Large - 368\", Header{\n\t\t\t\"CorrelationID\":     []string{\"123\"},\n\t\t\t\"Msg-ID\":            []string{\"456\"},\n\t\t\t\"X-NATS-Keys\":       []string{\"A\", \"B\", \"C\"},\n\t\t\t\"X-Test-Keys\":       []string{\"D\", \"E\", \"F\"},\n\t\t\t\"X-A-Long-Header-1\": []string{strings.Repeat(\"A\", 100)},\n\t\t\t\"X-A-Long-Header-2\": []string{strings.Repeat(\"A\", 100)},\n\t\t}},\n\t}\n\tfor _, bm := range benchmarks {\n\t\tb.Run(bm.name, func(b *testing.B) {\n\t\t\tb.ReportAllocs()\n\n\t\t\tm := NewMsg(\"foo\")\n\t\t\tm.Header = bm.header\n\t\t\thdr, err := m.headerBytes()\n\t\t\tif err != nil {\n\t\t\t\tb.Fatalf(\"Unexpected error: %v\", err)\n\t\t\t}\n\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\tif _, err := DecodeHeadersMsg(hdr); err != nil {\n\t\t\t\t\tb.Fatalf(\"Unexpected error: %v\", err)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "netchan.go",
          "type": "blob",
          "size": 3.7392578125,
          "content": "// Copyright 2013-2023 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nats\n\nimport (\n\t\"errors\"\n\t\"reflect\"\n)\n\n// This allows the functionality for network channels by binding send and receive Go chans\n// to subjects and optionally queue groups.\n// Data will be encoded and decoded via the EncodedConn and its associated encoders.\n\n// BindSendChan binds a channel for send operations to NATS.\n//\n// Deprecated: Encoded connections are no longer supported.\nfunc (c *EncodedConn) BindSendChan(subject string, channel any) error {\n\tchVal := reflect.ValueOf(channel)\n\tif chVal.Kind() != reflect.Chan {\n\t\treturn ErrChanArg\n\t}\n\tgo chPublish(c, chVal, subject)\n\treturn nil\n}\n\n// Publish all values that arrive on the channel until it is closed or we\n// encounter an error.\nfunc chPublish(c *EncodedConn, chVal reflect.Value, subject string) {\n\tfor {\n\t\tval, ok := chVal.Recv()\n\t\tif !ok {\n\t\t\t// Channel has most likely been closed.\n\t\t\treturn\n\t\t}\n\t\tif e := c.Publish(subject, val.Interface()); e != nil {\n\t\t\t// Do this under lock.\n\t\t\tc.Conn.mu.Lock()\n\t\t\tdefer c.Conn.mu.Unlock()\n\n\t\t\tif c.Conn.Opts.AsyncErrorCB != nil {\n\t\t\t\t// FIXME(dlc) - Not sure this is the right thing to do.\n\t\t\t\t// FIXME(ivan) - If the connection is not yet closed, try to schedule the callback\n\t\t\t\tif c.Conn.isClosed() {\n\t\t\t\t\tgo c.Conn.Opts.AsyncErrorCB(c.Conn, nil, e)\n\t\t\t\t} else {\n\t\t\t\t\tc.Conn.ach.push(func() { c.Conn.Opts.AsyncErrorCB(c.Conn, nil, e) })\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// BindRecvChan binds a channel for receive operations from NATS.\n//\n// Deprecated: Encoded connections are no longer supported.\nfunc (c *EncodedConn) BindRecvChan(subject string, channel any) (*Subscription, error) {\n\treturn c.bindRecvChan(subject, _EMPTY_, channel)\n}\n\n// BindRecvQueueChan binds a channel for queue-based receive operations from NATS.\n//\n// Deprecated: Encoded connections are no longer supported.\nfunc (c *EncodedConn) BindRecvQueueChan(subject, queue string, channel any) (*Subscription, error) {\n\treturn c.bindRecvChan(subject, queue, channel)\n}\n\n// Internal function to bind receive operations for a channel.\nfunc (c *EncodedConn) bindRecvChan(subject, queue string, channel any) (*Subscription, error) {\n\tchVal := reflect.ValueOf(channel)\n\tif chVal.Kind() != reflect.Chan {\n\t\treturn nil, ErrChanArg\n\t}\n\targType := chVal.Type().Elem()\n\n\tcb := func(m *Msg) {\n\t\tvar oPtr reflect.Value\n\t\tif argType.Kind() != reflect.Ptr {\n\t\t\toPtr = reflect.New(argType)\n\t\t} else {\n\t\t\toPtr = reflect.New(argType.Elem())\n\t\t}\n\t\tif err := c.Enc.Decode(m.Subject, m.Data, oPtr.Interface()); err != nil {\n\t\t\tc.Conn.err = errors.New(\"nats: Got an error trying to unmarshal: \" + err.Error())\n\t\t\tif c.Conn.Opts.AsyncErrorCB != nil {\n\t\t\t\tc.Conn.ach.push(func() { c.Conn.Opts.AsyncErrorCB(c.Conn, m.Sub, c.Conn.err) })\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\tif argType.Kind() != reflect.Ptr {\n\t\t\toPtr = reflect.Indirect(oPtr)\n\t\t}\n\t\t// This is a bit hacky, but in this instance we may be trying to send to a closed channel.\n\t\t// and the user does not know when it is safe to close the channel.\n\t\tdefer func() {\n\t\t\t// If we have panicked, recover and close the subscription.\n\t\t\tif r := recover(); r != nil {\n\t\t\t\tm.Sub.Unsubscribe()\n\t\t\t}\n\t\t}()\n\t\t// Actually do the send to the channel.\n\t\tchVal.Send(oPtr)\n\t}\n\n\treturn c.Conn.subscribe(subject, queue, cb, nil, nil, false, nil)\n}\n"
        },
        {
          "name": "object.go",
          "type": "blob",
          "size": 38.2275390625,
          "content": "// Copyright 2021-2023 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nats\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto/sha256\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"io\"\n\t\"net\"\n\t\"os\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/nats-io/nats.go/internal/parser\"\n\t\"github.com/nats-io/nuid\"\n)\n\n// ObjectStoreManager creates, loads and deletes Object Stores\ntype ObjectStoreManager interface {\n\t// ObjectStore will look up and bind to an existing object store instance.\n\tObjectStore(bucket string) (ObjectStore, error)\n\t// CreateObjectStore will create an object store.\n\tCreateObjectStore(cfg *ObjectStoreConfig) (ObjectStore, error)\n\t// DeleteObjectStore will delete the underlying stream for the named object.\n\tDeleteObjectStore(bucket string) error\n\t// ObjectStoreNames is used to retrieve a list of bucket names\n\tObjectStoreNames(opts ...ObjectOpt) <-chan string\n\t// ObjectStores is used to retrieve a list of bucket statuses\n\tObjectStores(opts ...ObjectOpt) <-chan ObjectStoreStatus\n}\n\n// ObjectStore is a blob store capable of storing large objects efficiently in\n// JetStream streams\ntype ObjectStore interface {\n\t// Put will place the contents from the reader into a new object.\n\tPut(obj *ObjectMeta, reader io.Reader, opts ...ObjectOpt) (*ObjectInfo, error)\n\t// Get will pull the named object from the object store.\n\tGet(name string, opts ...GetObjectOpt) (ObjectResult, error)\n\n\t// PutBytes is convenience function to put a byte slice into this object store.\n\tPutBytes(name string, data []byte, opts ...ObjectOpt) (*ObjectInfo, error)\n\t// GetBytes is a convenience function to pull an object from this object store and return it as a byte slice.\n\tGetBytes(name string, opts ...GetObjectOpt) ([]byte, error)\n\n\t// PutString is convenience function to put a string into this object store.\n\tPutString(name string, data string, opts ...ObjectOpt) (*ObjectInfo, error)\n\t// GetString is a convenience function to pull an object from this object store and return it as a string.\n\tGetString(name string, opts ...GetObjectOpt) (string, error)\n\n\t// PutFile is convenience function to put a file into this object store.\n\tPutFile(file string, opts ...ObjectOpt) (*ObjectInfo, error)\n\t// GetFile is a convenience function to pull an object from this object store and place it in a file.\n\tGetFile(name, file string, opts ...GetObjectOpt) error\n\n\t// GetInfo will retrieve the current information for the object.\n\tGetInfo(name string, opts ...GetObjectInfoOpt) (*ObjectInfo, error)\n\t// UpdateMeta will update the metadata for the object.\n\tUpdateMeta(name string, meta *ObjectMeta) error\n\n\t// Delete will delete the named object.\n\tDelete(name string) error\n\n\t// AddLink will add a link to another object.\n\tAddLink(name string, obj *ObjectInfo) (*ObjectInfo, error)\n\n\t// AddBucketLink will add a link to another object store.\n\tAddBucketLink(name string, bucket ObjectStore) (*ObjectInfo, error)\n\n\t// Seal will seal the object store, no further modifications will be allowed.\n\tSeal() error\n\n\t// Watch for changes in the underlying store and receive meta information updates.\n\tWatch(opts ...WatchOpt) (ObjectWatcher, error)\n\n\t// List will list all the objects in this store.\n\tList(opts ...ListObjectsOpt) ([]*ObjectInfo, error)\n\n\t// Status retrieves run-time status about the backing store of the bucket.\n\tStatus() (ObjectStoreStatus, error)\n}\n\ntype ObjectOpt interface {\n\tconfigureObject(opts *objOpts) error\n}\n\ntype objOpts struct {\n\tctx context.Context\n}\n\n// For nats.Context() support.\nfunc (ctx ContextOpt) configureObject(opts *objOpts) error {\n\topts.ctx = ctx\n\treturn nil\n}\n\n// ObjectWatcher is what is returned when doing a watch.\ntype ObjectWatcher interface {\n\t// Updates returns a channel to read any updates to entries.\n\tUpdates() <-chan *ObjectInfo\n\t// Stop will stop this watcher.\n\tStop() error\n}\n\nvar (\n\tErrObjectConfigRequired = errors.New(\"nats: object-store config required\")\n\tErrBadObjectMeta        = errors.New(\"nats: object-store meta information invalid\")\n\tErrObjectNotFound       = errors.New(\"nats: object not found\")\n\tErrInvalidStoreName     = errors.New(\"nats: invalid object-store name\")\n\tErrDigestMismatch       = errors.New(\"nats: received a corrupt object, digests do not match\")\n\tErrInvalidDigestFormat  = errors.New(\"nats: object digest hash has invalid format\")\n\tErrNoObjectsFound       = errors.New(\"nats: no objects found\")\n\tErrObjectAlreadyExists  = errors.New(\"nats: an object already exists with that name\")\n\tErrNameRequired         = errors.New(\"nats: name is required\")\n\tErrNeeds262             = errors.New(\"nats: object-store requires at least server version 2.6.2\")\n\tErrLinkNotAllowed       = errors.New(\"nats: link cannot be set when putting the object in bucket\")\n\tErrObjectRequired       = errors.New(\"nats: object required\")\n\tErrNoLinkToDeleted      = errors.New(\"nats: not allowed to link to a deleted object\")\n\tErrNoLinkToLink         = errors.New(\"nats: not allowed to link to another link\")\n\tErrCantGetBucket        = errors.New(\"nats: invalid Get, object is a link to a bucket\")\n\tErrBucketRequired       = errors.New(\"nats: bucket required\")\n\tErrBucketMalformed      = errors.New(\"nats: bucket malformed\")\n\tErrUpdateMetaDeleted    = errors.New(\"nats: cannot update meta for a deleted object\")\n)\n\n// ObjectStoreConfig is the config for the object store.\ntype ObjectStoreConfig struct {\n\tBucket      string        `json:\"bucket\"`\n\tDescription string        `json:\"description,omitempty\"`\n\tTTL         time.Duration `json:\"max_age,omitempty\"`\n\tMaxBytes    int64         `json:\"max_bytes,omitempty\"`\n\tStorage     StorageType   `json:\"storage,omitempty\"`\n\tReplicas    int           `json:\"num_replicas,omitempty\"`\n\tPlacement   *Placement    `json:\"placement,omitempty\"`\n\n\t// Bucket-specific metadata\n\t// NOTE: Metadata requires nats-server v2.10.0+\n\tMetadata map[string]string `json:\"metadata,omitempty\"`\n\t// Enable underlying stream compression.\n\t// NOTE: Compression is supported for nats-server 2.10.0+\n\tCompression bool `json:\"compression,omitempty\"`\n}\n\ntype ObjectStoreStatus interface {\n\t// Bucket is the name of the bucket\n\tBucket() string\n\t// Description is the description supplied when creating the bucket\n\tDescription() string\n\t// TTL indicates how long objects are kept in the bucket\n\tTTL() time.Duration\n\t// Storage indicates the underlying JetStream storage technology used to store data\n\tStorage() StorageType\n\t// Replicas indicates how many storage replicas are kept for the data in the bucket\n\tReplicas() int\n\t// Sealed indicates the stream is sealed and cannot be modified in any way\n\tSealed() bool\n\t// Size is the combined size of all data in the bucket including metadata, in bytes\n\tSize() uint64\n\t// BackingStore provides details about the underlying storage\n\tBackingStore() string\n\t// Metadata is the user supplied metadata for the bucket\n\tMetadata() map[string]string\n\t// IsCompressed indicates if the data is compressed on disk\n\tIsCompressed() bool\n}\n\n// ObjectMetaOptions\ntype ObjectMetaOptions struct {\n\tLink      *ObjectLink `json:\"link,omitempty\"`\n\tChunkSize uint32      `json:\"max_chunk_size,omitempty\"`\n}\n\n// ObjectMeta is high level information about an object.\ntype ObjectMeta struct {\n\tName        string            `json:\"name\"`\n\tDescription string            `json:\"description,omitempty\"`\n\tHeaders     Header            `json:\"headers,omitempty\"`\n\tMetadata    map[string]string `json:\"metadata,omitempty\"`\n\n\t// Optional options.\n\tOpts *ObjectMetaOptions `json:\"options,omitempty\"`\n}\n\n// ObjectInfo is meta plus instance information.\ntype ObjectInfo struct {\n\tObjectMeta\n\tBucket  string    `json:\"bucket\"`\n\tNUID    string    `json:\"nuid\"`\n\tSize    uint64    `json:\"size\"`\n\tModTime time.Time `json:\"mtime\"`\n\tChunks  uint32    `json:\"chunks\"`\n\tDigest  string    `json:\"digest,omitempty\"`\n\tDeleted bool      `json:\"deleted,omitempty\"`\n}\n\n// ObjectLink is used to embed links to other buckets and objects.\ntype ObjectLink struct {\n\t// Bucket is the name of the other object store.\n\tBucket string `json:\"bucket\"`\n\t// Name can be used to link to a single object.\n\t// If empty means this is a link to the whole store, like a directory.\n\tName string `json:\"name,omitempty\"`\n}\n\n// ObjectResult will return the underlying stream info and also be an io.ReadCloser.\ntype ObjectResult interface {\n\tio.ReadCloser\n\tInfo() (*ObjectInfo, error)\n\tError() error\n}\n\nconst (\n\tobjNameTmpl         = \"OBJ_%s\"     // OBJ_<bucket> // stream name\n\tobjAllChunksPreTmpl = \"$O.%s.C.>\"  // $O.<bucket>.C.> // chunk stream subject\n\tobjAllMetaPreTmpl   = \"$O.%s.M.>\"  // $O.<bucket>.M.> // meta stream subject\n\tobjChunksPreTmpl    = \"$O.%s.C.%s\" // $O.<bucket>.C.<object-nuid> // chunk message subject\n\tobjMetaPreTmpl      = \"$O.%s.M.%s\" // $O.<bucket>.M.<name-encoded> // meta message subject\n\tobjNoPending        = \"0\"\n\tobjDefaultChunkSize = uint32(128 * 1024) // 128k\n\tobjDigestType       = \"SHA-256=\"\n\tobjDigestTmpl       = objDigestType + \"%s\"\n)\n\ntype obs struct {\n\tname   string\n\tstream string\n\tjs     *js\n}\n\n// CreateObjectStore will create an object store.\nfunc (js *js) CreateObjectStore(cfg *ObjectStoreConfig) (ObjectStore, error) {\n\tif !js.nc.serverMinVersion(2, 6, 2) {\n\t\treturn nil, ErrNeeds262\n\t}\n\tif cfg == nil {\n\t\treturn nil, ErrObjectConfigRequired\n\t}\n\tif !validBucketRe.MatchString(cfg.Bucket) {\n\t\treturn nil, ErrInvalidStoreName\n\t}\n\n\tname := cfg.Bucket\n\tchunks := fmt.Sprintf(objAllChunksPreTmpl, name)\n\tmeta := fmt.Sprintf(objAllMetaPreTmpl, name)\n\n\t// We will set explicitly some values so that we can do comparison\n\t// if we get an \"already in use\" error and need to check if it is same.\n\t// See kv\n\treplicas := cfg.Replicas\n\tif replicas == 0 {\n\t\treplicas = 1\n\t}\n\tmaxBytes := cfg.MaxBytes\n\tif maxBytes == 0 {\n\t\tmaxBytes = -1\n\t}\n\tvar compression StoreCompression\n\tif cfg.Compression {\n\t\tcompression = S2Compression\n\t}\n\tscfg := &StreamConfig{\n\t\tName:        fmt.Sprintf(objNameTmpl, name),\n\t\tDescription: cfg.Description,\n\t\tSubjects:    []string{chunks, meta},\n\t\tMaxAge:      cfg.TTL,\n\t\tMaxBytes:    maxBytes,\n\t\tStorage:     cfg.Storage,\n\t\tReplicas:    replicas,\n\t\tPlacement:   cfg.Placement,\n\t\tDiscard:     DiscardNew,\n\t\tAllowRollup: true,\n\t\tAllowDirect: true,\n\t\tMetadata:    cfg.Metadata,\n\t\tCompression: compression,\n\t}\n\n\t// Create our stream.\n\t_, err := js.AddStream(scfg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &obs{name: name, stream: scfg.Name, js: js}, nil\n}\n\n// ObjectStore will look up and bind to an existing object store instance.\nfunc (js *js) ObjectStore(bucket string) (ObjectStore, error) {\n\tif !validBucketRe.MatchString(bucket) {\n\t\treturn nil, ErrInvalidStoreName\n\t}\n\tif !js.nc.serverMinVersion(2, 6, 2) {\n\t\treturn nil, ErrNeeds262\n\t}\n\n\tstream := fmt.Sprintf(objNameTmpl, bucket)\n\tsi, err := js.StreamInfo(stream)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &obs{name: bucket, stream: si.Config.Name, js: js}, nil\n}\n\n// DeleteObjectStore will delete the underlying stream for the named object.\nfunc (js *js) DeleteObjectStore(bucket string) error {\n\tstream := fmt.Sprintf(objNameTmpl, bucket)\n\treturn js.DeleteStream(stream)\n}\n\nfunc encodeName(name string) string {\n\treturn base64.URLEncoding.EncodeToString([]byte(name))\n}\n\n// Put will place the contents from the reader into this object-store.\nfunc (obs *obs) Put(meta *ObjectMeta, r io.Reader, opts ...ObjectOpt) (*ObjectInfo, error) {\n\tif meta == nil || meta.Name == \"\" {\n\t\treturn nil, ErrBadObjectMeta\n\t}\n\n\tif meta.Opts == nil {\n\t\tmeta.Opts = &ObjectMetaOptions{ChunkSize: objDefaultChunkSize}\n\t} else if meta.Opts.Link != nil {\n\t\treturn nil, ErrLinkNotAllowed\n\t} else if meta.Opts.ChunkSize == 0 {\n\t\tmeta.Opts.ChunkSize = objDefaultChunkSize\n\t}\n\n\tvar o objOpts\n\tfor _, opt := range opts {\n\t\tif opt != nil {\n\t\t\tif err := opt.configureObject(&o); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\tctx := o.ctx\n\n\t// Create the new nuid so chunks go on a new subject if the name is re-used\n\tnewnuid := nuid.Next()\n\n\t// These will be used in more than one place\n\tchunkSubj := fmt.Sprintf(objChunksPreTmpl, obs.name, newnuid)\n\n\t// Grab existing meta info (einfo). Ok to be found or not found, any other error is a problem\n\t// Chunks on the old nuid can be cleaned up at the end\n\teinfo, err := obs.GetInfo(meta.Name, GetObjectInfoShowDeleted()) // GetInfo will encode the name\n\tif err != nil && err != ErrObjectNotFound {\n\t\treturn nil, err\n\t}\n\n\t// For async error handling\n\tvar perr error\n\tvar mu sync.Mutex\n\tsetErr := func(err error) {\n\t\tmu.Lock()\n\t\tdefer mu.Unlock()\n\t\tperr = err\n\t}\n\tgetErr := func() error {\n\t\tmu.Lock()\n\t\tdefer mu.Unlock()\n\t\treturn perr\n\t}\n\n\t// Create our own JS context to handle errors etc.\n\tjetStream, err := obs.js.nc.JetStream(PublishAsyncErrHandler(func(js JetStream, _ *Msg, err error) { setErr(err) }))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tdefer jetStream.(*js).cleanupReplySub()\n\n\tpurgePartial := func() error {\n\t\t// wait until all pubs are complete or up to default timeout before attempting purge\n\t\tselect {\n\t\tcase <-jetStream.PublishAsyncComplete():\n\t\tcase <-time.After(obs.js.opts.wait):\n\t\t}\n\t\tif err := obs.js.purgeStream(obs.stream, &StreamPurgeRequest{Subject: chunkSubj}); err != nil {\n\t\t\treturn fmt.Errorf(\"could not cleanup bucket after erroneous put operation: %w\", err)\n\t\t}\n\t\treturn nil\n\t}\n\n\tm, h := NewMsg(chunkSubj), sha256.New()\n\tchunk, sent, total := make([]byte, meta.Opts.ChunkSize), 0, uint64(0)\n\n\t// set up the info object. The chunk upload sets the size and digest\n\tinfo := &ObjectInfo{Bucket: obs.name, NUID: newnuid, ObjectMeta: *meta}\n\n\tfor r != nil {\n\t\tif ctx != nil {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tif ctx.Err() == context.Canceled {\n\t\t\t\t\terr = ctx.Err()\n\t\t\t\t} else {\n\t\t\t\t\terr = ErrTimeout\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\tif purgeErr := purgePartial(); purgeErr != nil {\n\t\t\t\t\treturn nil, errors.Join(err, purgeErr)\n\t\t\t\t}\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\n\t\t// Actual read.\n\t\t// TODO(dlc) - Deadline?\n\t\tn, readErr := r.Read(chunk)\n\n\t\t// Handle all non EOF errors\n\t\tif readErr != nil && readErr != io.EOF {\n\t\t\tif purgeErr := purgePartial(); purgeErr != nil {\n\t\t\t\treturn nil, errors.Join(readErr, purgeErr)\n\t\t\t}\n\t\t\treturn nil, readErr\n\t\t}\n\n\t\t// Add chunk only if we received data\n\t\tif n > 0 {\n\t\t\t// Chunk processing.\n\t\t\tm.Data = chunk[:n]\n\t\t\th.Write(m.Data)\n\n\t\t\t// Send msg itself.\n\t\t\tif _, err := jetStream.PublishMsgAsync(m); err != nil {\n\t\t\t\tif purgeErr := purgePartial(); purgeErr != nil {\n\t\t\t\t\treturn nil, errors.Join(err, purgeErr)\n\t\t\t\t}\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tif err := getErr(); err != nil {\n\t\t\t\tif purgeErr := purgePartial(); purgeErr != nil {\n\t\t\t\t\treturn nil, errors.Join(err, purgeErr)\n\t\t\t\t}\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\t// Update totals.\n\t\t\tsent++\n\t\t\ttotal += uint64(n)\n\t\t}\n\n\t\t// EOF Processing.\n\t\tif readErr == io.EOF {\n\t\t\t// Place meta info.\n\t\t\tinfo.Size, info.Chunks = uint64(total), uint32(sent)\n\t\t\tinfo.Digest = GetObjectDigestValue(h)\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// Prepare the meta message\n\tmetaSubj := fmt.Sprintf(objMetaPreTmpl, obs.name, encodeName(meta.Name))\n\tmm := NewMsg(metaSubj)\n\tmm.Header.Set(MsgRollup, MsgRollupSubject)\n\tmm.Data, err = json.Marshal(info)\n\tif err != nil {\n\t\tif r != nil {\n\t\t\tif purgeErr := purgePartial(); purgeErr != nil {\n\t\t\t\treturn nil, errors.Join(err, purgeErr)\n\t\t\t}\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// Publish the meta message.\n\t_, err = jetStream.PublishMsgAsync(mm)\n\tif err != nil {\n\t\tif r != nil {\n\t\t\tif purgeErr := purgePartial(); purgeErr != nil {\n\t\t\t\treturn nil, errors.Join(err, purgeErr)\n\t\t\t}\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// Wait for all to be processed.\n\tselect {\n\tcase <-jetStream.PublishAsyncComplete():\n\t\tif err := getErr(); err != nil {\n\t\t\tif r != nil {\n\t\t\t\tif purgeErr := purgePartial(); purgeErr != nil {\n\t\t\t\t\treturn nil, errors.Join(err, purgeErr)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\tcase <-time.After(obs.js.opts.wait):\n\t\treturn nil, ErrTimeout\n\t}\n\n\tinfo.ModTime = time.Now().UTC() // This time is not actually the correct time\n\n\t// Delete any original chunks.\n\tif einfo != nil && !einfo.Deleted {\n\t\techunkSubj := fmt.Sprintf(objChunksPreTmpl, obs.name, einfo.NUID)\n\t\tif err := obs.js.purgeStream(obs.stream, &StreamPurgeRequest{Subject: echunkSubj}); err != nil {\n\t\t\treturn info, err\n\t\t}\n\t}\n\n\t// TODO would it be okay to do this to return the info with the correct time?\n\t// With the understanding that it is an extra call to the server.\n\t// Otherwise the time the user gets back is the client time, not the server time.\n\t// return obs.GetInfo(info.Name)\n\n\treturn info, nil\n}\n\n// GetObjectDigestValue calculates the base64 value of hashed data\nfunc GetObjectDigestValue(data hash.Hash) string {\n\tsha := data.Sum(nil)\n\treturn fmt.Sprintf(objDigestTmpl, base64.URLEncoding.EncodeToString(sha[:]))\n}\n\n// DecodeObjectDigest decodes base64 hash\nfunc DecodeObjectDigest(data string) ([]byte, error) {\n\tdigest := strings.SplitN(data, \"=\", 2)\n\tif len(digest) != 2 {\n\t\treturn nil, ErrInvalidDigestFormat\n\t}\n\treturn base64.URLEncoding.DecodeString(digest[1])\n}\n\n// ObjectResult impl.\ntype objResult struct {\n\tsync.Mutex\n\tinfo        *ObjectInfo\n\tr           io.ReadCloser\n\terr         error\n\tctx         context.Context\n\tdigest      hash.Hash\n\treadTimeout time.Duration\n}\n\nfunc (info *ObjectInfo) isLink() bool {\n\treturn info.ObjectMeta.Opts != nil && info.ObjectMeta.Opts.Link != nil\n}\n\ntype GetObjectOpt interface {\n\tconfigureGetObject(opts *getObjectOpts) error\n}\ntype getObjectOpts struct {\n\tctx context.Context\n\t// Include deleted object in the result.\n\tshowDeleted bool\n}\n\ntype getObjectFn func(opts *getObjectOpts) error\n\nfunc (opt getObjectFn) configureGetObject(opts *getObjectOpts) error {\n\treturn opt(opts)\n}\n\n// GetObjectShowDeleted makes Get() return object if it was marked as deleted.\nfunc GetObjectShowDeleted() GetObjectOpt {\n\treturn getObjectFn(func(opts *getObjectOpts) error {\n\t\topts.showDeleted = true\n\t\treturn nil\n\t})\n}\n\n// For nats.Context() support.\nfunc (ctx ContextOpt) configureGetObject(opts *getObjectOpts) error {\n\topts.ctx = ctx\n\treturn nil\n}\n\n// Get will pull the object from the underlying stream.\nfunc (obs *obs) Get(name string, opts ...GetObjectOpt) (ObjectResult, error) {\n\tvar o getObjectOpts\n\tfor _, opt := range opts {\n\t\tif opt != nil {\n\t\t\tif err := opt.configureGetObject(&o); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\tctx := o.ctx\n\tinfoOpts := make([]GetObjectInfoOpt, 0)\n\tif ctx != nil {\n\t\tinfoOpts = append(infoOpts, Context(ctx))\n\t}\n\tif o.showDeleted {\n\t\tinfoOpts = append(infoOpts, GetObjectInfoShowDeleted())\n\t}\n\n\t// Grab meta info.\n\tinfo, err := obs.GetInfo(name, infoOpts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif info.NUID == _EMPTY_ {\n\t\treturn nil, ErrBadObjectMeta\n\t}\n\n\t// Check for object links. If single objects we do a pass through.\n\tif info.isLink() {\n\t\tif info.ObjectMeta.Opts.Link.Name == _EMPTY_ {\n\t\t\treturn nil, ErrCantGetBucket\n\t\t}\n\n\t\t// is the link in the same bucket?\n\t\tlbuck := info.ObjectMeta.Opts.Link.Bucket\n\t\tif lbuck == obs.name {\n\t\t\treturn obs.Get(info.ObjectMeta.Opts.Link.Name)\n\t\t}\n\n\t\t// different bucket\n\t\tlobs, err := obs.js.ObjectStore(lbuck)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn lobs.Get(info.ObjectMeta.Opts.Link.Name)\n\t}\n\n\tresult := &objResult{info: info, ctx: ctx, readTimeout: obs.js.opts.wait}\n\tif info.Size == 0 {\n\t\treturn result, nil\n\t}\n\n\tpr, pw := net.Pipe()\n\tresult.r = pr\n\n\tgotErr := func(m *Msg, err error) {\n\t\tpw.Close()\n\t\tm.Sub.Unsubscribe()\n\t\tresult.setErr(err)\n\t}\n\n\t// For calculating sum256\n\tresult.digest = sha256.New()\n\n\tprocessChunk := func(m *Msg) {\n\t\tvar err error\n\t\tif ctx != nil {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tif errors.Is(ctx.Err(), context.Canceled) {\n\t\t\t\t\terr = ctx.Err()\n\t\t\t\t} else {\n\t\t\t\t\terr = ErrTimeout\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\tgotErr(m, err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\ttokens, err := parser.GetMetadataFields(m.Reply)\n\t\tif err != nil {\n\t\t\tgotErr(m, err)\n\t\t\treturn\n\t\t}\n\n\t\t// Write to our pipe.\n\t\tfor b := m.Data; len(b) > 0; {\n\t\t\tn, err := pw.Write(b)\n\t\t\tif err != nil {\n\t\t\t\tgotErr(m, err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tb = b[n:]\n\t\t}\n\t\t// Update sha256\n\t\tresult.digest.Write(m.Data)\n\n\t\t// Check if we are done.\n\t\tif tokens[parser.AckNumPendingTokenPos] == objNoPending {\n\t\t\tpw.Close()\n\t\t\tm.Sub.Unsubscribe()\n\t\t}\n\t}\n\n\tchunkSubj := fmt.Sprintf(objChunksPreTmpl, obs.name, info.NUID)\n\tstreamName := fmt.Sprintf(objNameTmpl, obs.name)\n\tsubscribeOpts := []SubOpt{\n\t\tOrderedConsumer(),\n\t\tBindStream(streamName),\n\t}\n\t_, err = obs.js.Subscribe(chunkSubj, processChunk, subscribeOpts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn result, nil\n}\n\n// Delete will delete the object.\nfunc (obs *obs) Delete(name string) error {\n\t// Grab meta info.\n\tinfo, err := obs.GetInfo(name, GetObjectInfoShowDeleted())\n\tif err != nil {\n\t\treturn err\n\t}\n\tif info.NUID == _EMPTY_ {\n\t\treturn ErrBadObjectMeta\n\t}\n\n\t// Place a rollup delete marker and publish the info\n\tinfo.Deleted = true\n\tinfo.Size, info.Chunks, info.Digest = 0, 0, _EMPTY_\n\n\tif err = publishMeta(info, obs.js); err != nil {\n\t\treturn err\n\t}\n\n\t// Purge chunks for the object.\n\tchunkSubj := fmt.Sprintf(objChunksPreTmpl, obs.name, info.NUID)\n\treturn obs.js.purgeStream(obs.stream, &StreamPurgeRequest{Subject: chunkSubj})\n}\n\nfunc publishMeta(info *ObjectInfo, js JetStreamContext) error {\n\t// marshal the object into json, don't store an actual time\n\tinfo.ModTime = time.Time{}\n\tdata, err := json.Marshal(info)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Prepare and publish the message.\n\tmm := NewMsg(fmt.Sprintf(objMetaPreTmpl, info.Bucket, encodeName(info.ObjectMeta.Name)))\n\tmm.Header.Set(MsgRollup, MsgRollupSubject)\n\tmm.Data = data\n\tif _, err := js.PublishMsg(mm); err != nil {\n\t\treturn err\n\t}\n\n\t// set the ModTime in case it's returned to the user, even though it's not the correct time.\n\tinfo.ModTime = time.Now().UTC()\n\treturn nil\n}\n\n// AddLink will add a link to another object if it's not deleted and not another link\n// name is the name of this link object\n// obj is what is being linked too\nfunc (obs *obs) AddLink(name string, obj *ObjectInfo) (*ObjectInfo, error) {\n\tif name == \"\" {\n\t\treturn nil, ErrNameRequired\n\t}\n\n\t// TODO Handle stale info\n\n\tif obj == nil || obj.Name == \"\" {\n\t\treturn nil, ErrObjectRequired\n\t}\n\tif obj.Deleted {\n\t\treturn nil, ErrNoLinkToDeleted\n\t}\n\tif obj.isLink() {\n\t\treturn nil, ErrNoLinkToLink\n\t}\n\n\t// If object with link's name is found, error.\n\t// If link with link's name is found, that's okay to overwrite.\n\t// If there was an error that was not ErrObjectNotFound, error.\n\teinfo, err := obs.GetInfo(name, GetObjectInfoShowDeleted())\n\tif einfo != nil {\n\t\tif !einfo.isLink() {\n\t\t\treturn nil, ErrObjectAlreadyExists\n\t\t}\n\t} else if err != ErrObjectNotFound {\n\t\treturn nil, err\n\t}\n\n\t// create the meta for the link\n\tmeta := &ObjectMeta{\n\t\tName: name,\n\t\tOpts: &ObjectMetaOptions{Link: &ObjectLink{Bucket: obj.Bucket, Name: obj.Name}},\n\t}\n\tinfo := &ObjectInfo{Bucket: obs.name, NUID: nuid.Next(), ModTime: time.Now().UTC(), ObjectMeta: *meta}\n\n\t// put the link object\n\tif err = publishMeta(info, obs.js); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn info, nil\n}\n\n// AddBucketLink will add a link to another object store.\nfunc (ob *obs) AddBucketLink(name string, bucket ObjectStore) (*ObjectInfo, error) {\n\tif name == \"\" {\n\t\treturn nil, ErrNameRequired\n\t}\n\tif bucket == nil {\n\t\treturn nil, ErrBucketRequired\n\t}\n\tbos, ok := bucket.(*obs)\n\tif !ok {\n\t\treturn nil, ErrBucketMalformed\n\t}\n\n\t// If object with link's name is found, error.\n\t// If link with link's name is found, that's okay to overwrite.\n\t// If there was an error that was not ErrObjectNotFound, error.\n\teinfo, err := ob.GetInfo(name, GetObjectInfoShowDeleted())\n\tif einfo != nil {\n\t\tif !einfo.isLink() {\n\t\t\treturn nil, ErrObjectAlreadyExists\n\t\t}\n\t} else if err != ErrObjectNotFound {\n\t\treturn nil, err\n\t}\n\n\t// create the meta for the link\n\tmeta := &ObjectMeta{\n\t\tName: name,\n\t\tOpts: &ObjectMetaOptions{Link: &ObjectLink{Bucket: bos.name}},\n\t}\n\tinfo := &ObjectInfo{Bucket: ob.name, NUID: nuid.Next(), ObjectMeta: *meta}\n\n\t// put the link object\n\terr = publishMeta(info, ob.js)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn info, nil\n}\n\n// PutBytes is convenience function to put a byte slice into this object store.\nfunc (obs *obs) PutBytes(name string, data []byte, opts ...ObjectOpt) (*ObjectInfo, error) {\n\treturn obs.Put(&ObjectMeta{Name: name}, bytes.NewReader(data), opts...)\n}\n\n// GetBytes is a convenience function to pull an object from this object store and return it as a byte slice.\nfunc (obs *obs) GetBytes(name string, opts ...GetObjectOpt) ([]byte, error) {\n\tresult, err := obs.Get(name, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer result.Close()\n\n\tvar b bytes.Buffer\n\tif _, err := b.ReadFrom(result); err != nil {\n\t\treturn nil, err\n\t}\n\treturn b.Bytes(), nil\n}\n\n// PutString is convenience function to put a string into this object store.\nfunc (obs *obs) PutString(name string, data string, opts ...ObjectOpt) (*ObjectInfo, error) {\n\treturn obs.Put(&ObjectMeta{Name: name}, strings.NewReader(data), opts...)\n}\n\n// GetString is a convenience function to pull an object from this object store and return it as a string.\nfunc (obs *obs) GetString(name string, opts ...GetObjectOpt) (string, error) {\n\tresult, err := obs.Get(name, opts...)\n\tif err != nil {\n\t\treturn _EMPTY_, err\n\t}\n\tdefer result.Close()\n\n\tvar b bytes.Buffer\n\tif _, err := b.ReadFrom(result); err != nil {\n\t\treturn _EMPTY_, err\n\t}\n\treturn b.String(), nil\n}\n\n// PutFile is convenience function to put a file into an object store.\nfunc (obs *obs) PutFile(file string, opts ...ObjectOpt) (*ObjectInfo, error) {\n\tf, err := os.Open(file)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer f.Close()\n\treturn obs.Put(&ObjectMeta{Name: file}, f, opts...)\n}\n\n// GetFile is a convenience function to pull and object and place in a file.\nfunc (obs *obs) GetFile(name, file string, opts ...GetObjectOpt) error {\n\t// Expect file to be new.\n\tf, err := os.OpenFile(file, os.O_WRONLY|os.O_CREATE, 0600)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer f.Close()\n\n\tresult, err := obs.Get(name, opts...)\n\tif err != nil {\n\t\tos.Remove(f.Name())\n\t\treturn err\n\t}\n\tdefer result.Close()\n\n\t// Stream copy to the file.\n\t_, err = io.Copy(f, result)\n\treturn err\n}\n\ntype GetObjectInfoOpt interface {\n\tconfigureGetInfo(opts *getObjectInfoOpts) error\n}\ntype getObjectInfoOpts struct {\n\tctx context.Context\n\t// Include deleted object in the result.\n\tshowDeleted bool\n}\n\ntype getObjectInfoFn func(opts *getObjectInfoOpts) error\n\nfunc (opt getObjectInfoFn) configureGetInfo(opts *getObjectInfoOpts) error {\n\treturn opt(opts)\n}\n\n// GetObjectInfoShowDeleted makes GetInfo() return object if it was marked as deleted.\nfunc GetObjectInfoShowDeleted() GetObjectInfoOpt {\n\treturn getObjectInfoFn(func(opts *getObjectInfoOpts) error {\n\t\topts.showDeleted = true\n\t\treturn nil\n\t})\n}\n\n// For nats.Context() support.\nfunc (ctx ContextOpt) configureGetInfo(opts *getObjectInfoOpts) error {\n\topts.ctx = ctx\n\treturn nil\n}\n\n// GetInfo will retrieve the current information for the object.\nfunc (obs *obs) GetInfo(name string, opts ...GetObjectInfoOpt) (*ObjectInfo, error) {\n\t// Grab last meta value we have.\n\tif name == \"\" {\n\t\treturn nil, ErrNameRequired\n\t}\n\tvar o getObjectInfoOpts\n\tfor _, opt := range opts {\n\t\tif opt != nil {\n\t\t\tif err := opt.configureGetInfo(&o); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\tmetaSubj := fmt.Sprintf(objMetaPreTmpl, obs.name, encodeName(name)) // used as data in a JS API call\n\tstream := fmt.Sprintf(objNameTmpl, obs.name)\n\n\tm, err := obs.js.GetLastMsg(stream, metaSubj)\n\tif err != nil {\n\t\tif errors.Is(err, ErrMsgNotFound) {\n\t\t\terr = ErrObjectNotFound\n\t\t}\n\t\treturn nil, err\n\t}\n\tvar info ObjectInfo\n\tif err := json.Unmarshal(m.Data, &info); err != nil {\n\t\treturn nil, ErrBadObjectMeta\n\t}\n\tif !o.showDeleted && info.Deleted {\n\t\treturn nil, ErrObjectNotFound\n\t}\n\tinfo.ModTime = m.Time\n\treturn &info, nil\n}\n\n// UpdateMeta will update the meta for the object.\nfunc (obs *obs) UpdateMeta(name string, meta *ObjectMeta) error {\n\tif meta == nil {\n\t\treturn ErrBadObjectMeta\n\t}\n\n\t// Grab the current meta.\n\tinfo, err := obs.GetInfo(name)\n\tif err != nil {\n\t\tif errors.Is(err, ErrObjectNotFound) {\n\t\t\treturn ErrUpdateMetaDeleted\n\t\t}\n\t\treturn err\n\t}\n\n\t// If the new name is different from the old, and it exists, error\n\t// If there was an error that was not ErrObjectNotFound, error.\n\tif name != meta.Name {\n\t\texistingInfo, err := obs.GetInfo(meta.Name, GetObjectInfoShowDeleted())\n\t\tif err != nil && !errors.Is(err, ErrObjectNotFound) {\n\t\t\treturn err\n\t\t}\n\t\tif err == nil && !existingInfo.Deleted {\n\t\t\treturn ErrObjectAlreadyExists\n\t\t}\n\t}\n\n\t// Update Meta prevents update of ObjectMetaOptions (Link, ChunkSize)\n\t// These should only be updated internally when appropriate.\n\tinfo.Name = meta.Name\n\tinfo.Description = meta.Description\n\tinfo.Headers = meta.Headers\n\tinfo.Metadata = meta.Metadata\n\n\t// Prepare the meta message\n\tif err = publishMeta(info, obs.js); err != nil {\n\t\treturn err\n\t}\n\n\t// did the name of this object change? We just stored the meta under the new name\n\t// so delete the meta from the old name via purge stream for subject\n\tif name != meta.Name {\n\t\tmetaSubj := fmt.Sprintf(objMetaPreTmpl, obs.name, encodeName(name))\n\t\treturn obs.js.purgeStream(obs.stream, &StreamPurgeRequest{Subject: metaSubj})\n\t}\n\n\treturn nil\n}\n\n// Seal will seal the object store, no further modifications will be allowed.\nfunc (obs *obs) Seal() error {\n\tstream := fmt.Sprintf(objNameTmpl, obs.name)\n\tsi, err := obs.js.StreamInfo(stream)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// Seal the stream from being able to take on more messages.\n\tcfg := si.Config\n\tcfg.Sealed = true\n\t_, err = obs.js.UpdateStream(&cfg)\n\treturn err\n}\n\n// Implementation for Watch\ntype objWatcher struct {\n\tupdates chan *ObjectInfo\n\tsub     *Subscription\n}\n\n// Updates returns the interior channel.\nfunc (w *objWatcher) Updates() <-chan *ObjectInfo {\n\tif w == nil {\n\t\treturn nil\n\t}\n\treturn w.updates\n}\n\n// Stop will unsubscribe from the watcher.\nfunc (w *objWatcher) Stop() error {\n\tif w == nil {\n\t\treturn nil\n\t}\n\treturn w.sub.Unsubscribe()\n}\n\n// Watch for changes in the underlying store and receive meta information updates.\nfunc (obs *obs) Watch(opts ...WatchOpt) (ObjectWatcher, error) {\n\tvar o watchOpts\n\tfor _, opt := range opts {\n\t\tif opt != nil {\n\t\t\tif err := opt.configureWatcher(&o); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\tvar initDoneMarker bool\n\n\tw := &objWatcher{updates: make(chan *ObjectInfo, 32)}\n\n\tupdate := func(m *Msg) {\n\t\tvar info ObjectInfo\n\t\tif err := json.Unmarshal(m.Data, &info); err != nil {\n\t\t\treturn // TODO(dlc) - Communicate this upwards?\n\t\t}\n\t\tmeta, err := m.Metadata()\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\n\t\tif !o.ignoreDeletes || !info.Deleted {\n\t\t\tinfo.ModTime = meta.Timestamp\n\t\t\tw.updates <- &info\n\t\t}\n\n\t\t// if UpdatesOnly is set, no not send nil to the channel\n\t\t// as it would always be triggered after initializing the watcher\n\t\tif !initDoneMarker && meta.NumPending == 0 {\n\t\t\tinitDoneMarker = true\n\t\t\tw.updates <- nil\n\t\t}\n\t}\n\n\tallMeta := fmt.Sprintf(objAllMetaPreTmpl, obs.name)\n\t_, err := obs.js.GetLastMsg(obs.stream, allMeta)\n\t// if there are no messages on the stream and we are not watching\n\t// updates only, send nil to the channel to indicate that the initial\n\t// watch is done\n\tif !o.updatesOnly {\n\t\tif errors.Is(err, ErrMsgNotFound) {\n\t\t\tinitDoneMarker = true\n\t\t\tw.updates <- nil\n\t\t}\n\t} else {\n\t\t// if UpdatesOnly was used, mark initialization as complete\n\t\tinitDoneMarker = true\n\t}\n\n\t// Used ordered consumer to deliver results.\n\tstreamName := fmt.Sprintf(objNameTmpl, obs.name)\n\tsubOpts := []SubOpt{OrderedConsumer(), BindStream(streamName)}\n\tif !o.includeHistory {\n\t\tsubOpts = append(subOpts, DeliverLastPerSubject())\n\t}\n\tif o.updatesOnly {\n\t\tsubOpts = append(subOpts, DeliverNew())\n\t}\n\tsub, err := obs.js.Subscribe(allMeta, update, subOpts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tw.sub = sub\n\treturn w, nil\n}\n\ntype ListObjectsOpt interface {\n\tconfigureListObjects(opts *listObjectOpts) error\n}\ntype listObjectOpts struct {\n\tctx context.Context\n\t// Include deleted objects in the result channel.\n\tshowDeleted bool\n}\n\ntype listObjectsFn func(opts *listObjectOpts) error\n\nfunc (opt listObjectsFn) configureListObjects(opts *listObjectOpts) error {\n\treturn opt(opts)\n}\n\n// ListObjectsShowDeleted makes ListObjects() return deleted objects.\nfunc ListObjectsShowDeleted() ListObjectsOpt {\n\treturn listObjectsFn(func(opts *listObjectOpts) error {\n\t\topts.showDeleted = true\n\t\treturn nil\n\t})\n}\n\n// For nats.Context() support.\nfunc (ctx ContextOpt) configureListObjects(opts *listObjectOpts) error {\n\topts.ctx = ctx\n\treturn nil\n}\n\n// List will list all the objects in this store.\nfunc (obs *obs) List(opts ...ListObjectsOpt) ([]*ObjectInfo, error) {\n\tvar o listObjectOpts\n\tfor _, opt := range opts {\n\t\tif opt != nil {\n\t\t\tif err := opt.configureListObjects(&o); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\twatchOpts := make([]WatchOpt, 0)\n\tif !o.showDeleted {\n\t\twatchOpts = append(watchOpts, IgnoreDeletes())\n\t}\n\twatcher, err := obs.Watch(watchOpts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer watcher.Stop()\n\tif o.ctx == nil {\n\t\to.ctx = context.Background()\n\t}\n\n\tvar objs []*ObjectInfo\n\tupdates := watcher.Updates()\nUpdates:\n\tfor {\n\t\tselect {\n\t\tcase entry := <-updates:\n\t\t\tif entry == nil {\n\t\t\t\tbreak Updates\n\t\t\t}\n\t\t\tobjs = append(objs, entry)\n\t\tcase <-o.ctx.Done():\n\t\t\treturn nil, o.ctx.Err()\n\t\t}\n\t}\n\tif len(objs) == 0 {\n\t\treturn nil, ErrNoObjectsFound\n\t}\n\treturn objs, nil\n}\n\n// ObjectBucketStatus  represents status of a Bucket, implements ObjectStoreStatus\ntype ObjectBucketStatus struct {\n\tnfo    *StreamInfo\n\tbucket string\n}\n\n// Bucket is the name of the bucket\nfunc (s *ObjectBucketStatus) Bucket() string { return s.bucket }\n\n// Description is the description supplied when creating the bucket\nfunc (s *ObjectBucketStatus) Description() string { return s.nfo.Config.Description }\n\n// TTL indicates how long objects are kept in the bucket\nfunc (s *ObjectBucketStatus) TTL() time.Duration { return s.nfo.Config.MaxAge }\n\n// Storage indicates the underlying JetStream storage technology used to store data\nfunc (s *ObjectBucketStatus) Storage() StorageType { return s.nfo.Config.Storage }\n\n// Replicas indicates how many storage replicas are kept for the data in the bucket\nfunc (s *ObjectBucketStatus) Replicas() int { return s.nfo.Config.Replicas }\n\n// Sealed indicates the stream is sealed and cannot be modified in any way\nfunc (s *ObjectBucketStatus) Sealed() bool { return s.nfo.Config.Sealed }\n\n// Size is the combined size of all data in the bucket including metadata, in bytes\nfunc (s *ObjectBucketStatus) Size() uint64 { return s.nfo.State.Bytes }\n\n// BackingStore indicates what technology is used for storage of the bucket\nfunc (s *ObjectBucketStatus) BackingStore() string { return \"JetStream\" }\n\n// Metadata is the metadata supplied when creating the bucket\nfunc (s *ObjectBucketStatus) Metadata() map[string]string { return s.nfo.Config.Metadata }\n\n// StreamInfo is the stream info retrieved to create the status\nfunc (s *ObjectBucketStatus) StreamInfo() *StreamInfo { return s.nfo }\n\n// IsCompressed indicates if the data is compressed on disk\nfunc (s *ObjectBucketStatus) IsCompressed() bool { return s.nfo.Config.Compression != NoCompression }\n\n// Status retrieves run-time status about a bucket\nfunc (obs *obs) Status() (ObjectStoreStatus, error) {\n\tnfo, err := obs.js.StreamInfo(obs.stream)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tstatus := &ObjectBucketStatus{\n\t\tnfo:    nfo,\n\t\tbucket: obs.name,\n\t}\n\n\treturn status, nil\n}\n\n// Read impl.\nfunc (o *objResult) Read(p []byte) (n int, err error) {\n\to.Lock()\n\tdefer o.Unlock()\n\treadDeadline := time.Now().Add(o.readTimeout)\n\tif ctx := o.ctx; ctx != nil {\n\t\tif deadline, ok := ctx.Deadline(); ok {\n\t\t\treadDeadline = deadline\n\t\t}\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\tif ctx.Err() == context.Canceled {\n\t\t\t\to.err = ctx.Err()\n\t\t\t} else {\n\t\t\t\to.err = ErrTimeout\n\t\t\t}\n\t\tdefault:\n\t\t}\n\t}\n\tif o.err != nil {\n\t\treturn 0, o.err\n\t}\n\tif o.r == nil {\n\t\treturn 0, io.EOF\n\t}\n\n\tr := o.r.(net.Conn)\n\tr.SetReadDeadline(readDeadline)\n\tn, err = r.Read(p)\n\tif err, ok := err.(net.Error); ok && err.Timeout() {\n\t\tif ctx := o.ctx; ctx != nil {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tif ctx.Err() == context.Canceled {\n\t\t\t\t\treturn 0, ctx.Err()\n\t\t\t\t} else {\n\t\t\t\t\treturn 0, ErrTimeout\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\terr = nil\n\t\t\t}\n\t\t}\n\t}\n\tif err == io.EOF {\n\t\t// Make sure the digest matches.\n\t\tsha := o.digest.Sum(nil)\n\t\trsha, decodeErr := DecodeObjectDigest(o.info.Digest)\n\t\tif decodeErr != nil {\n\t\t\to.err = decodeErr\n\t\t\treturn 0, o.err\n\t\t}\n\t\tif !bytes.Equal(sha[:], rsha) {\n\t\t\to.err = ErrDigestMismatch\n\t\t\treturn 0, o.err\n\t\t}\n\t}\n\treturn n, err\n}\n\n// Close impl.\nfunc (o *objResult) Close() error {\n\to.Lock()\n\tdefer o.Unlock()\n\tif o.r == nil {\n\t\treturn nil\n\t}\n\treturn o.r.Close()\n}\n\nfunc (o *objResult) setErr(err error) {\n\to.Lock()\n\tdefer o.Unlock()\n\to.err = err\n}\n\nfunc (o *objResult) Info() (*ObjectInfo, error) {\n\to.Lock()\n\tdefer o.Unlock()\n\treturn o.info, o.err\n}\n\nfunc (o *objResult) Error() error {\n\to.Lock()\n\tdefer o.Unlock()\n\treturn o.err\n}\n\n// ObjectStoreNames is used to retrieve a list of bucket names\nfunc (js *js) ObjectStoreNames(opts ...ObjectOpt) <-chan string {\n\tvar o objOpts\n\tfor _, opt := range opts {\n\t\tif opt != nil {\n\t\t\tif err := opt.configureObject(&o); err != nil {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t}\n\tch := make(chan string)\n\tvar cancel context.CancelFunc\n\tif o.ctx == nil {\n\t\to.ctx, cancel = context.WithTimeout(context.Background(), defaultRequestWait)\n\t}\n\tl := &streamLister{js: js}\n\tl.js.opts.streamListSubject = fmt.Sprintf(objAllChunksPreTmpl, \"*\")\n\tl.js.opts.ctx = o.ctx\n\tgo func() {\n\t\tif cancel != nil {\n\t\t\tdefer cancel()\n\t\t}\n\t\tdefer close(ch)\n\t\tfor l.Next() {\n\t\t\tfor _, info := range l.Page() {\n\t\t\t\tif !strings.HasPrefix(info.Config.Name, \"OBJ_\") {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tselect {\n\t\t\t\tcase ch <- info.Config.Name:\n\t\t\t\tcase <-o.ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn ch\n}\n\n// ObjectStores is used to retrieve a list of bucket statuses\nfunc (js *js) ObjectStores(opts ...ObjectOpt) <-chan ObjectStoreStatus {\n\tvar o objOpts\n\tfor _, opt := range opts {\n\t\tif opt != nil {\n\t\t\tif err := opt.configureObject(&o); err != nil {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t}\n\tch := make(chan ObjectStoreStatus)\n\tvar cancel context.CancelFunc\n\tif o.ctx == nil {\n\t\to.ctx, cancel = context.WithTimeout(context.Background(), defaultRequestWait)\n\t}\n\tl := &streamLister{js: js}\n\tl.js.opts.streamListSubject = fmt.Sprintf(objAllChunksPreTmpl, \"*\")\n\tl.js.opts.ctx = o.ctx\n\tgo func() {\n\t\tif cancel != nil {\n\t\t\tdefer cancel()\n\t\t}\n\t\tdefer close(ch)\n\t\tfor l.Next() {\n\t\t\tfor _, info := range l.Page() {\n\t\t\t\tif !strings.HasPrefix(info.Config.Name, \"OBJ_\") {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tselect {\n\t\t\t\tcase ch <- &ObjectBucketStatus{\n\t\t\t\t\tnfo:    info,\n\t\t\t\t\tbucket: strings.TrimPrefix(info.Config.Name, \"OBJ_\"),\n\t\t\t\t}:\n\t\t\t\tcase <-o.ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn ch\n}\n"
        },
        {
          "name": "parser.go",
          "type": "blob",
          "size": 11.373046875,
          "content": "// Copyright 2012-2023 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nats\n\nimport (\n\t\"fmt\"\n)\n\ntype msgArg struct {\n\tsubject []byte\n\treply   []byte\n\tsid     int64\n\thdr     int\n\tsize    int\n}\n\nconst MAX_CONTROL_LINE_SIZE = 4096\n\ntype parseState struct {\n\tstate     int\n\tas        int\n\tdrop      int\n\thdr       int\n\tma        msgArg\n\targBuf    []byte\n\tmsgBuf    []byte\n\tmsgCopied bool\n\tscratch   [MAX_CONTROL_LINE_SIZE]byte\n}\n\nconst (\n\tOP_START = iota\n\tOP_PLUS\n\tOP_PLUS_O\n\tOP_PLUS_OK\n\tOP_MINUS\n\tOP_MINUS_E\n\tOP_MINUS_ER\n\tOP_MINUS_ERR\n\tOP_MINUS_ERR_SPC\n\tMINUS_ERR_ARG\n\tOP_M\n\tOP_MS\n\tOP_MSG\n\tOP_MSG_SPC\n\tMSG_ARG\n\tMSG_PAYLOAD\n\tMSG_END\n\tOP_H\n\tOP_P\n\tOP_PI\n\tOP_PIN\n\tOP_PING\n\tOP_PO\n\tOP_PON\n\tOP_PONG\n\tOP_I\n\tOP_IN\n\tOP_INF\n\tOP_INFO\n\tOP_INFO_SPC\n\tINFO_ARG\n)\n\n// parse is the fast protocol parser engine.\nfunc (nc *Conn) parse(buf []byte) error {\n\tvar i int\n\tvar b byte\n\n\t// Move to loop instead of range syntax to allow jumping of i\n\tfor i = 0; i < len(buf); i++ {\n\t\tb = buf[i]\n\n\t\tswitch nc.ps.state {\n\t\tcase OP_START:\n\t\t\tswitch b {\n\t\t\tcase 'M', 'm':\n\t\t\t\tnc.ps.state = OP_M\n\t\t\t\tnc.ps.hdr = -1\n\t\t\t\tnc.ps.ma.hdr = -1\n\t\t\tcase 'H', 'h':\n\t\t\t\tnc.ps.state = OP_H\n\t\t\t\tnc.ps.hdr = 0\n\t\t\t\tnc.ps.ma.hdr = 0\n\t\t\tcase 'P', 'p':\n\t\t\t\tnc.ps.state = OP_P\n\t\t\tcase '+':\n\t\t\t\tnc.ps.state = OP_PLUS\n\t\t\tcase '-':\n\t\t\t\tnc.ps.state = OP_MINUS\n\t\t\tcase 'I', 'i':\n\t\t\t\tnc.ps.state = OP_I\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_H:\n\t\t\tswitch b {\n\t\t\tcase 'M', 'm':\n\t\t\t\tnc.ps.state = OP_M\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_M:\n\t\t\tswitch b {\n\t\t\tcase 'S', 's':\n\t\t\t\tnc.ps.state = OP_MS\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_MS:\n\t\t\tswitch b {\n\t\t\tcase 'G', 'g':\n\t\t\t\tnc.ps.state = OP_MSG\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_MSG:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tnc.ps.state = OP_MSG_SPC\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_MSG_SPC:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tcontinue\n\t\t\tdefault:\n\t\t\t\tnc.ps.state = MSG_ARG\n\t\t\t\tnc.ps.as = i\n\t\t\t}\n\t\tcase MSG_ARG:\n\t\t\tswitch b {\n\t\t\tcase '\\r':\n\t\t\t\tnc.ps.drop = 1\n\t\t\tcase '\\n':\n\t\t\t\tvar arg []byte\n\t\t\t\tif nc.ps.argBuf != nil {\n\t\t\t\t\targ = nc.ps.argBuf\n\t\t\t\t} else {\n\t\t\t\t\targ = buf[nc.ps.as : i-nc.ps.drop]\n\t\t\t\t}\n\t\t\t\tif err := nc.processMsgArgs(arg); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tnc.ps.drop, nc.ps.as, nc.ps.state = 0, i+1, MSG_PAYLOAD\n\n\t\t\t\t// jump ahead with the index. If this overruns\n\t\t\t\t// what is left we fall out and process a split buffer.\n\t\t\t\ti = nc.ps.as + nc.ps.ma.size - 1\n\t\t\tdefault:\n\t\t\t\tif nc.ps.argBuf != nil {\n\t\t\t\t\tnc.ps.argBuf = append(nc.ps.argBuf, b)\n\t\t\t\t}\n\t\t\t}\n\t\tcase MSG_PAYLOAD:\n\t\t\tif nc.ps.msgBuf != nil {\n\t\t\t\tif len(nc.ps.msgBuf) >= nc.ps.ma.size {\n\t\t\t\t\tnc.processMsg(nc.ps.msgBuf)\n\t\t\t\t\tnc.ps.argBuf, nc.ps.msgBuf, nc.ps.msgCopied, nc.ps.state = nil, nil, false, MSG_END\n\t\t\t\t} else {\n\t\t\t\t\t// copy as much as we can to the buffer and skip ahead.\n\t\t\t\t\ttoCopy := nc.ps.ma.size - len(nc.ps.msgBuf)\n\t\t\t\t\tavail := len(buf) - i\n\n\t\t\t\t\tif avail < toCopy {\n\t\t\t\t\t\ttoCopy = avail\n\t\t\t\t\t}\n\n\t\t\t\t\tif toCopy > 0 {\n\t\t\t\t\t\tstart := len(nc.ps.msgBuf)\n\t\t\t\t\t\t// This is needed for copy to work.\n\t\t\t\t\t\tnc.ps.msgBuf = nc.ps.msgBuf[:start+toCopy]\n\t\t\t\t\t\tcopy(nc.ps.msgBuf[start:], buf[i:i+toCopy])\n\t\t\t\t\t\t// Update our index\n\t\t\t\t\t\ti = (i + toCopy) - 1\n\t\t\t\t\t} else {\n\t\t\t\t\t\tnc.ps.msgBuf = append(nc.ps.msgBuf, b)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else if i-nc.ps.as >= nc.ps.ma.size {\n\t\t\t\tnc.processMsg(buf[nc.ps.as:i])\n\t\t\t\tnc.ps.argBuf, nc.ps.msgBuf, nc.ps.msgCopied, nc.ps.state = nil, nil, false, MSG_END\n\t\t\t}\n\t\tcase MSG_END:\n\t\t\tswitch b {\n\t\t\tcase '\\n':\n\t\t\t\tnc.ps.drop, nc.ps.as, nc.ps.state = 0, i+1, OP_START\n\t\t\tdefault:\n\t\t\t\tcontinue\n\t\t\t}\n\t\tcase OP_PLUS:\n\t\t\tswitch b {\n\t\t\tcase 'O', 'o':\n\t\t\t\tnc.ps.state = OP_PLUS_O\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_PLUS_O:\n\t\t\tswitch b {\n\t\t\tcase 'K', 'k':\n\t\t\t\tnc.ps.state = OP_PLUS_OK\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_PLUS_OK:\n\t\t\tswitch b {\n\t\t\tcase '\\n':\n\t\t\t\tnc.processOK()\n\t\t\t\tnc.ps.drop, nc.ps.state = 0, OP_START\n\t\t\t}\n\t\tcase OP_MINUS:\n\t\t\tswitch b {\n\t\t\tcase 'E', 'e':\n\t\t\t\tnc.ps.state = OP_MINUS_E\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_MINUS_E:\n\t\t\tswitch b {\n\t\t\tcase 'R', 'r':\n\t\t\t\tnc.ps.state = OP_MINUS_ER\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_MINUS_ER:\n\t\t\tswitch b {\n\t\t\tcase 'R', 'r':\n\t\t\t\tnc.ps.state = OP_MINUS_ERR\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_MINUS_ERR:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tnc.ps.state = OP_MINUS_ERR_SPC\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_MINUS_ERR_SPC:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tcontinue\n\t\t\tdefault:\n\t\t\t\tnc.ps.state = MINUS_ERR_ARG\n\t\t\t\tnc.ps.as = i\n\t\t\t}\n\t\tcase MINUS_ERR_ARG:\n\t\t\tswitch b {\n\t\t\tcase '\\r':\n\t\t\t\tnc.ps.drop = 1\n\t\t\tcase '\\n':\n\t\t\t\tvar arg []byte\n\t\t\t\tif nc.ps.argBuf != nil {\n\t\t\t\t\targ = nc.ps.argBuf\n\t\t\t\t\tnc.ps.argBuf = nil\n\t\t\t\t} else {\n\t\t\t\t\targ = buf[nc.ps.as : i-nc.ps.drop]\n\t\t\t\t}\n\t\t\t\tnc.processErr(string(arg))\n\t\t\t\tnc.ps.drop, nc.ps.as, nc.ps.state = 0, i+1, OP_START\n\t\t\tdefault:\n\t\t\t\tif nc.ps.argBuf != nil {\n\t\t\t\t\tnc.ps.argBuf = append(nc.ps.argBuf, b)\n\t\t\t\t}\n\t\t\t}\n\t\tcase OP_P:\n\t\t\tswitch b {\n\t\t\tcase 'I', 'i':\n\t\t\t\tnc.ps.state = OP_PI\n\t\t\tcase 'O', 'o':\n\t\t\t\tnc.ps.state = OP_PO\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_PO:\n\t\t\tswitch b {\n\t\t\tcase 'N', 'n':\n\t\t\t\tnc.ps.state = OP_PON\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_PON:\n\t\t\tswitch b {\n\t\t\tcase 'G', 'g':\n\t\t\t\tnc.ps.state = OP_PONG\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_PONG:\n\t\t\tswitch b {\n\t\t\tcase '\\n':\n\t\t\t\tnc.processPong()\n\t\t\t\tnc.ps.drop, nc.ps.state = 0, OP_START\n\t\t\t}\n\t\tcase OP_PI:\n\t\t\tswitch b {\n\t\t\tcase 'N', 'n':\n\t\t\t\tnc.ps.state = OP_PIN\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_PIN:\n\t\t\tswitch b {\n\t\t\tcase 'G', 'g':\n\t\t\t\tnc.ps.state = OP_PING\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_PING:\n\t\t\tswitch b {\n\t\t\tcase '\\n':\n\t\t\t\tnc.processPing()\n\t\t\t\tnc.ps.drop, nc.ps.state = 0, OP_START\n\t\t\t}\n\t\tcase OP_I:\n\t\t\tswitch b {\n\t\t\tcase 'N', 'n':\n\t\t\t\tnc.ps.state = OP_IN\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_IN:\n\t\t\tswitch b {\n\t\t\tcase 'F', 'f':\n\t\t\t\tnc.ps.state = OP_INF\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_INF:\n\t\t\tswitch b {\n\t\t\tcase 'O', 'o':\n\t\t\t\tnc.ps.state = OP_INFO\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_INFO:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tnc.ps.state = OP_INFO_SPC\n\t\t\tdefault:\n\t\t\t\tgoto parseErr\n\t\t\t}\n\t\tcase OP_INFO_SPC:\n\t\t\tswitch b {\n\t\t\tcase ' ', '\\t':\n\t\t\t\tcontinue\n\t\t\tdefault:\n\t\t\t\tnc.ps.state = INFO_ARG\n\t\t\t\tnc.ps.as = i\n\t\t\t}\n\t\tcase INFO_ARG:\n\t\t\tswitch b {\n\t\t\tcase '\\r':\n\t\t\t\tnc.ps.drop = 1\n\t\t\tcase '\\n':\n\t\t\t\tvar arg []byte\n\t\t\t\tif nc.ps.argBuf != nil {\n\t\t\t\t\targ = nc.ps.argBuf\n\t\t\t\t\tnc.ps.argBuf = nil\n\t\t\t\t} else {\n\t\t\t\t\targ = buf[nc.ps.as : i-nc.ps.drop]\n\t\t\t\t}\n\t\t\t\tnc.processAsyncInfo(arg)\n\t\t\t\tnc.ps.drop, nc.ps.as, nc.ps.state = 0, i+1, OP_START\n\t\t\tdefault:\n\t\t\t\tif nc.ps.argBuf != nil {\n\t\t\t\t\tnc.ps.argBuf = append(nc.ps.argBuf, b)\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\tgoto parseErr\n\t\t}\n\t}\n\t// Check for split buffer scenarios\n\tif (nc.ps.state == MSG_ARG || nc.ps.state == MINUS_ERR_ARG || nc.ps.state == INFO_ARG) && nc.ps.argBuf == nil {\n\t\tnc.ps.argBuf = nc.ps.scratch[:0]\n\t\tnc.ps.argBuf = append(nc.ps.argBuf, buf[nc.ps.as:i-nc.ps.drop]...)\n\t\t// FIXME, check max len\n\t}\n\t// Check for split msg\n\tif nc.ps.state == MSG_PAYLOAD && nc.ps.msgBuf == nil {\n\t\t// We need to clone the msgArg if it is still referencing the\n\t\t// read buffer and we are not able to process the msg.\n\t\tif nc.ps.argBuf == nil {\n\t\t\tnc.cloneMsgArg()\n\t\t}\n\n\t\t// If we will overflow the scratch buffer, just create a\n\t\t// new buffer to hold the split message.\n\t\tif nc.ps.ma.size > cap(nc.ps.scratch)-len(nc.ps.argBuf) {\n\t\t\tlrem := len(buf[nc.ps.as:])\n\n\t\t\tnc.ps.msgBuf = make([]byte, lrem, nc.ps.ma.size)\n\t\t\tcopy(nc.ps.msgBuf, buf[nc.ps.as:])\n\t\t\tnc.ps.msgCopied = true\n\t\t} else {\n\t\t\tnc.ps.msgBuf = nc.ps.scratch[len(nc.ps.argBuf):len(nc.ps.argBuf)]\n\t\t\tnc.ps.msgBuf = append(nc.ps.msgBuf, (buf[nc.ps.as:])...)\n\t\t}\n\t}\n\n\treturn nil\n\nparseErr:\n\treturn fmt.Errorf(\"nats: Parse Error [%d]: '%s'\", nc.ps.state, buf[i:])\n}\n\n// cloneMsgArg is used when the split buffer scenario has the pubArg in the existing read buffer, but\n// we need to hold onto it into the next read.\nfunc (nc *Conn) cloneMsgArg() {\n\tnc.ps.argBuf = nc.ps.scratch[:0]\n\tnc.ps.argBuf = append(nc.ps.argBuf, nc.ps.ma.subject...)\n\tnc.ps.argBuf = append(nc.ps.argBuf, nc.ps.ma.reply...)\n\tnc.ps.ma.subject = nc.ps.argBuf[:len(nc.ps.ma.subject)]\n\tif nc.ps.ma.reply != nil {\n\t\tnc.ps.ma.reply = nc.ps.argBuf[len(nc.ps.ma.subject):]\n\t}\n}\n\nconst argsLenMax = 4\n\nfunc (nc *Conn) processMsgArgs(arg []byte) error {\n\t// Use separate function for header based messages.\n\tif nc.ps.hdr >= 0 {\n\t\treturn nc.processHeaderMsgArgs(arg)\n\t}\n\n\t// Unroll splitArgs to avoid runtime/heap issues\n\ta := [argsLenMax][]byte{}\n\targs := a[:0]\n\tstart := -1\n\tfor i, b := range arg {\n\t\tswitch b {\n\t\tcase ' ', '\\t', '\\r', '\\n':\n\t\t\tif start >= 0 {\n\t\t\t\targs = append(args, arg[start:i])\n\t\t\t\tstart = -1\n\t\t\t}\n\t\tdefault:\n\t\t\tif start < 0 {\n\t\t\t\tstart = i\n\t\t\t}\n\t\t}\n\t}\n\tif start >= 0 {\n\t\targs = append(args, arg[start:])\n\t}\n\n\tswitch len(args) {\n\tcase 3:\n\t\tnc.ps.ma.subject = args[0]\n\t\tnc.ps.ma.sid = parseInt64(args[1])\n\t\tnc.ps.ma.reply = nil\n\t\tnc.ps.ma.size = int(parseInt64(args[2]))\n\tcase 4:\n\t\tnc.ps.ma.subject = args[0]\n\t\tnc.ps.ma.sid = parseInt64(args[1])\n\t\tnc.ps.ma.reply = args[2]\n\t\tnc.ps.ma.size = int(parseInt64(args[3]))\n\tdefault:\n\t\treturn fmt.Errorf(\"nats: processMsgArgs Parse Error: '%s'\", arg)\n\t}\n\tif nc.ps.ma.sid < 0 {\n\t\treturn fmt.Errorf(\"nats: processMsgArgs Bad or Missing Sid: '%s'\", arg)\n\t}\n\tif nc.ps.ma.size < 0 {\n\t\treturn fmt.Errorf(\"nats: processMsgArgs Bad or Missing Size: '%s'\", arg)\n\t}\n\treturn nil\n}\n\n// processHeaderMsgArgs is for a header based message.\nfunc (nc *Conn) processHeaderMsgArgs(arg []byte) error {\n\t// Unroll splitArgs to avoid runtime/heap issues\n\ta := [argsLenMax][]byte{}\n\targs := a[:0]\n\tstart := -1\n\tfor i, b := range arg {\n\t\tswitch b {\n\t\tcase ' ', '\\t', '\\r', '\\n':\n\t\t\tif start >= 0 {\n\t\t\t\targs = append(args, arg[start:i])\n\t\t\t\tstart = -1\n\t\t\t}\n\t\tdefault:\n\t\t\tif start < 0 {\n\t\t\t\tstart = i\n\t\t\t}\n\t\t}\n\t}\n\tif start >= 0 {\n\t\targs = append(args, arg[start:])\n\t}\n\n\tswitch len(args) {\n\tcase 4:\n\t\tnc.ps.ma.subject = args[0]\n\t\tnc.ps.ma.sid = parseInt64(args[1])\n\t\tnc.ps.ma.reply = nil\n\t\tnc.ps.ma.hdr = int(parseInt64(args[2]))\n\t\tnc.ps.ma.size = int(parseInt64(args[3]))\n\tcase 5:\n\t\tnc.ps.ma.subject = args[0]\n\t\tnc.ps.ma.sid = parseInt64(args[1])\n\t\tnc.ps.ma.reply = args[2]\n\t\tnc.ps.ma.hdr = int(parseInt64(args[3]))\n\t\tnc.ps.ma.size = int(parseInt64(args[4]))\n\tdefault:\n\t\treturn fmt.Errorf(\"nats: processHeaderMsgArgs Parse Error: '%s'\", arg)\n\t}\n\tif nc.ps.ma.sid < 0 {\n\t\treturn fmt.Errorf(\"nats: processHeaderMsgArgs Bad or Missing Sid: '%s'\", arg)\n\t}\n\tif nc.ps.ma.hdr < 0 || nc.ps.ma.hdr > nc.ps.ma.size {\n\t\treturn fmt.Errorf(\"nats: processHeaderMsgArgs Bad or Missing Header Size: '%s'\", arg)\n\t}\n\tif nc.ps.ma.size < 0 {\n\t\treturn fmt.Errorf(\"nats: processHeaderMsgArgs Bad or Missing Size: '%s'\", arg)\n\t}\n\treturn nil\n}\n\n// ASCII numbers 0-9\nconst (\n\tascii_0 = 48\n\tascii_9 = 57\n)\n\n// parseInt64 expects decimal positive numbers. We\n// return -1 to signal error\nfunc parseInt64(d []byte) (n int64) {\n\tif len(d) == 0 {\n\t\treturn -1\n\t}\n\tfor _, dec := range d {\n\t\tif dec < ascii_0 || dec > ascii_9 {\n\t\t\treturn -1\n\t\t}\n\t\tn = n*10 + (int64(dec) - ascii_0)\n\t}\n\treturn n\n}\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "testing_internal.go",
          "type": "blob",
          "size": 1.810546875,
          "content": "// Copyright 2023 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build internal_testing\n// +build internal_testing\n\n// Functions in this file are only available when building nats.go with the\n// internal_testing build tag. They are used by the nats.go test suite.\npackage nats\n\n// AddMsgFilter adds a message filter for the given subject\n// to the connection. The filter will be called for each\n// message received on the subject. If the filter returns\n// nil, the message will be dropped.\nfunc (nc *Conn) AddMsgFilter(subject string, filter msgFilter) {\n\tnc.subsMu.Lock()\n\tdefer nc.subsMu.Unlock()\n\n\tif nc.filters == nil {\n\t\tnc.filters = make(map[string]msgFilter)\n\t}\n\tnc.filters[subject] = filter\n}\n\n// RemoveMsgFilter removes a message filter for the given subject.\nfunc (nc *Conn) RemoveMsgFilter(subject string) {\n\tnc.subsMu.Lock()\n\tdefer nc.subsMu.Unlock()\n\n\tif nc.filters != nil {\n\t\tdelete(nc.filters, subject)\n\t\tif len(nc.filters) == 0 {\n\t\t\tnc.filters = nil\n\t\t}\n\t}\n}\n\n// IsJSControlMessage returns true if the message is a JetStream control message.\nfunc IsJSControlMessage(msg *Msg) (bool, int) {\n\treturn isJSControlMessage(msg)\n}\n\n// CloseTCPConn closes the underlying TCP connection.\n// It can be used to simulate a disconnect.\nfunc (nc *Conn) CloseTCPConn() {\n\tnc.mu.Lock()\n\tdefer nc.mu.Unlock()\n\tnc.conn.Close()\n}\n"
        },
        {
          "name": "timer.go",
          "type": "blob",
          "size": 1.4462890625,
          "content": "// Copyright 2017-2022 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nats\n\nimport (\n\t\"sync\"\n\t\"time\"\n)\n\n// global pool of *time.Timer's. can be used by multiple goroutines concurrently.\nvar globalTimerPool timerPool\n\n// timerPool provides GC-able pooling of *time.Timer's.\n// can be used by multiple goroutines concurrently.\ntype timerPool struct {\n\tp sync.Pool\n}\n\n// Get returns a timer that completes after the given duration.\nfunc (tp *timerPool) Get(d time.Duration) *time.Timer {\n\tif t, ok := tp.p.Get().(*time.Timer); ok && t != nil {\n\t\tt.Reset(d)\n\t\treturn t\n\t}\n\n\treturn time.NewTimer(d)\n}\n\n// Put pools the given timer.\n//\n// There is no need to call t.Stop() before calling Put.\n//\n// Put will try to stop the timer before pooling. If the\n// given timer already expired, Put will read the unreceived\n// value if there is one.\nfunc (tp *timerPool) Put(t *time.Timer) {\n\tif !t.Stop() {\n\t\tselect {\n\t\tcase <-t.C:\n\t\tdefault:\n\t\t}\n\t}\n\n\ttp.p.Put(t)\n}\n"
        },
        {
          "name": "timer_test.go",
          "type": "blob",
          "size": 0.9619140625,
          "content": "// Copyright 2017-2022 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nats\n\nimport (\n\t\"testing\"\n\t\"time\"\n)\n\nfunc TestTimerPool(t *testing.T) {\n\tvar tp timerPool\n\n\tfor i := 0; i < 10; i++ {\n\t\ttm := tp.Get(time.Millisecond * 20)\n\n\t\tselect {\n\t\tcase <-tm.C:\n\t\t\tt.Errorf(\"Timer already expired\")\n\t\t\tcontinue\n\t\tdefault:\n\t\t}\n\n\t\tselect {\n\t\tcase <-tm.C:\n\t\tcase <-time.After(time.Millisecond * 100):\n\t\t\tt.Errorf(\"Timer didn't expire in time\")\n\t\t}\n\n\t\ttp.Put(tm)\n\t}\n}\n"
        },
        {
          "name": "util",
          "type": "tree",
          "content": null
        },
        {
          "name": "ws.go",
          "type": "blob",
          "size": 18.9853515625,
          "content": "// Copyright 2021-2023 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nats\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"crypto/rand\"\n\t\"crypto/sha1\"\n\t\"encoding/base64\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\tmrand \"math/rand\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"strings\"\n\t\"time\"\n\t\"unicode/utf8\"\n\n\t\"github.com/klauspost/compress/flate\"\n)\n\ntype wsOpCode int\n\nconst (\n\t// From https://tools.ietf.org/html/rfc6455#section-5.2\n\twsTextMessage   = wsOpCode(1)\n\twsBinaryMessage = wsOpCode(2)\n\twsCloseMessage  = wsOpCode(8)\n\twsPingMessage   = wsOpCode(9)\n\twsPongMessage   = wsOpCode(10)\n\n\twsFinalBit = 1 << 7\n\twsRsv1Bit  = 1 << 6 // Used for compression, from https://tools.ietf.org/html/rfc7692#section-6\n\twsRsv2Bit  = 1 << 5\n\twsRsv3Bit  = 1 << 4\n\n\twsMaskBit = 1 << 7\n\n\twsContinuationFrame     = 0\n\twsMaxFrameHeaderSize    = 14\n\twsMaxControlPayloadSize = 125\n\twsCloseSatusSize        = 2\n\n\t// From https://tools.ietf.org/html/rfc6455#section-11.7\n\twsCloseStatusNormalClosure      = 1000\n\twsCloseStatusNoStatusReceived   = 1005\n\twsCloseStatusAbnormalClosure    = 1006\n\twsCloseStatusInvalidPayloadData = 1007\n\n\twsScheme    = \"ws\"\n\twsSchemeTLS = \"wss\"\n\n\twsPMCExtension      = \"permessage-deflate\" // per-message compression\n\twsPMCSrvNoCtx       = \"server_no_context_takeover\"\n\twsPMCCliNoCtx       = \"client_no_context_takeover\"\n\twsPMCReqHeaderValue = wsPMCExtension + \"; \" + wsPMCSrvNoCtx + \"; \" + wsPMCCliNoCtx\n)\n\n// From https://tools.ietf.org/html/rfc6455#section-1.3\nvar wsGUID = []byte(\"258EAFA5-E914-47DA-95CA-C5AB0DC85B11\")\n\nvar compressFinalBlock = []byte{0x00, 0x00, 0xff, 0xff, 0x01, 0x00, 0x00, 0xff, 0xff}\n\ntype websocketReader struct {\n\tr       io.Reader\n\tpending [][]byte\n\tib      []byte\n\tff      bool\n\tfc      bool\n\tnl      bool\n\tdc      *wsDecompressor\n\tnc      *Conn\n}\n\ntype wsDecompressor struct {\n\tflate io.ReadCloser\n\tbufs  [][]byte\n\toff   int\n}\n\ntype websocketWriter struct {\n\tw          io.Writer\n\tcompress   bool\n\tcompressor *flate.Writer\n\tctrlFrames [][]byte // pending frames that should be sent at the next Write()\n\tcm         []byte   // close message that needs to be sent when everything else has been sent\n\tcmDone     bool     // a close message has been added or sent (never going back to false)\n\tnoMoreSend bool     // if true, even if there is a Write() call, we should not send anything\n}\n\nfunc (d *wsDecompressor) Read(dst []byte) (int, error) {\n\tif len(dst) == 0 {\n\t\treturn 0, nil\n\t}\n\tif len(d.bufs) == 0 {\n\t\treturn 0, io.EOF\n\t}\n\tcopied := 0\n\trem := len(dst)\n\tfor buf := d.bufs[0]; buf != nil && rem > 0; {\n\t\tn := len(buf[d.off:])\n\t\tif n > rem {\n\t\t\tn = rem\n\t\t}\n\t\tcopy(dst[copied:], buf[d.off:d.off+n])\n\t\tcopied += n\n\t\trem -= n\n\t\td.off += n\n\t\tbuf = d.nextBuf()\n\t}\n\treturn copied, nil\n}\n\nfunc (d *wsDecompressor) nextBuf() []byte {\n\t// We still have remaining data in the first buffer\n\tif d.off != len(d.bufs[0]) {\n\t\treturn d.bufs[0]\n\t}\n\t// We read the full first buffer. Reset offset.\n\td.off = 0\n\t// We were at the last buffer, so we are done.\n\tif len(d.bufs) == 1 {\n\t\td.bufs = nil\n\t\treturn nil\n\t}\n\t// Here we move to the next buffer.\n\td.bufs = d.bufs[1:]\n\treturn d.bufs[0]\n}\n\nfunc (d *wsDecompressor) ReadByte() (byte, error) {\n\tif len(d.bufs) == 0 {\n\t\treturn 0, io.EOF\n\t}\n\tb := d.bufs[0][d.off]\n\td.off++\n\td.nextBuf()\n\treturn b, nil\n}\n\nfunc (d *wsDecompressor) addBuf(b []byte) {\n\td.bufs = append(d.bufs, b)\n}\n\nfunc (d *wsDecompressor) decompress() ([]byte, error) {\n\td.off = 0\n\t// As per https://tools.ietf.org/html/rfc7692#section-7.2.2\n\t// add 0x00, 0x00, 0xff, 0xff and then a final block so that flate reader\n\t// does not report unexpected EOF.\n\td.bufs = append(d.bufs, compressFinalBlock)\n\t// Create or reset the decompressor with his object (wsDecompressor)\n\t// that provides Read() and ReadByte() APIs that will consume from\n\t// the compressed buffers (d.bufs).\n\tif d.flate == nil {\n\t\td.flate = flate.NewReader(d)\n\t} else {\n\t\td.flate.(flate.Resetter).Reset(d, nil)\n\t}\n\tb, err := io.ReadAll(d.flate)\n\t// Now reset the compressed buffers list\n\td.bufs = nil\n\treturn b, err\n}\n\nfunc wsNewReader(r io.Reader) *websocketReader {\n\treturn &websocketReader{r: r, ff: true}\n}\n\n// From now on, reads will be from the readLoop and we will need to\n// acquire the connection lock should we have to send/write a control\n// message from handleControlFrame.\n//\n// Note: this runs under the connection lock.\nfunc (r *websocketReader) doneWithConnect() {\n\tr.nl = true\n}\n\nfunc (r *websocketReader) Read(p []byte) (int, error) {\n\tvar err error\n\tvar buf []byte\n\n\tif l := len(r.ib); l > 0 {\n\t\tbuf = r.ib\n\t\tr.ib = nil\n\t} else {\n\t\tif len(r.pending) > 0 {\n\t\t\treturn r.drainPending(p), nil\n\t\t}\n\n\t\t// Get some data from the underlying reader.\n\t\tn, err := r.r.Read(p)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\tbuf = p[:n]\n\t}\n\n\t// Now parse this and decode frames. We will possibly read more to\n\t// ensure that we get a full frame.\n\tvar (\n\t\ttmpBuf []byte\n\t\tpos    int\n\t\tmax    = len(buf)\n\t\trem    = 0\n\t)\n\tfor pos < max {\n\t\tb0 := buf[pos]\n\t\tframeType := wsOpCode(b0 & 0xF)\n\t\tfinal := b0&wsFinalBit != 0\n\t\tcompressed := b0&wsRsv1Bit != 0\n\t\tpos++\n\n\t\ttmpBuf, pos, err = wsGet(r.r, buf, pos, 1)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\tb1 := tmpBuf[0]\n\n\t\t// Store size in case it is < 125\n\t\trem = int(b1 & 0x7F)\n\n\t\tswitch frameType {\n\t\tcase wsPingMessage, wsPongMessage, wsCloseMessage:\n\t\t\tif rem > wsMaxControlPayloadSize {\n\t\t\t\treturn 0, fmt.Errorf(\n\t\t\t\t\t\"control frame length bigger than maximum allowed of %v bytes\",\n\t\t\t\t\twsMaxControlPayloadSize)\n\t\t\t}\n\t\t\tif compressed {\n\t\t\t\treturn 0, errors.New(\"control frame should not be compressed\")\n\t\t\t}\n\t\t\tif !final {\n\t\t\t\treturn 0, errors.New(\"control frame does not have final bit set\")\n\t\t\t}\n\t\tcase wsTextMessage, wsBinaryMessage:\n\t\t\tif !r.ff {\n\t\t\t\treturn 0, errors.New(\"new message started before final frame for previous message was received\")\n\t\t\t}\n\t\t\tr.ff = final\n\t\t\tr.fc = compressed\n\t\tcase wsContinuationFrame:\n\t\t\t// Compressed bit must be only set in the first frame\n\t\t\tif r.ff || compressed {\n\t\t\t\treturn 0, errors.New(\"invalid continuation frame\")\n\t\t\t}\n\t\t\tr.ff = final\n\t\tdefault:\n\t\t\treturn 0, fmt.Errorf(\"unknown opcode %v\", frameType)\n\t\t}\n\n\t\t// If the encoded size is <= 125, then `rem` is simply the remainder size of the\n\t\t// frame. If it is 126, then the actual size is encoded as a uint16. For larger\n\t\t// frames, `rem` will initially be 127 and the actual size is encoded as a uint64.\n\t\tswitch rem {\n\t\tcase 126:\n\t\t\ttmpBuf, pos, err = wsGet(r.r, buf, pos, 2)\n\t\t\tif err != nil {\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\trem = int(binary.BigEndian.Uint16(tmpBuf))\n\t\tcase 127:\n\t\t\ttmpBuf, pos, err = wsGet(r.r, buf, pos, 8)\n\t\t\tif err != nil {\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\trem = int(binary.BigEndian.Uint64(tmpBuf))\n\t\t}\n\n\t\t// Handle control messages in place...\n\t\tif wsIsControlFrame(frameType) {\n\t\t\tpos, err = r.handleControlFrame(frameType, buf, pos, rem)\n\t\t\tif err != nil {\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\trem = 0\n\t\t\tcontinue\n\t\t}\n\n\t\tvar b []byte\n\t\t// This ensures that we get the full payload for this frame.\n\t\tb, pos, err = wsGet(r.r, buf, pos, rem)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\t// We read the full frame.\n\t\trem = 0\n\t\taddToPending := true\n\t\tif r.fc {\n\t\t\t// Don't add to pending if we are not dealing with the final frame.\n\t\t\taddToPending = r.ff\n\t\t\t// Add the compressed payload buffer to the list.\n\t\t\tr.addCBuf(b)\n\t\t\t// Decompress only when this is the final frame.\n\t\t\tif r.ff {\n\t\t\t\tb, err = r.dc.decompress()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn 0, err\n\t\t\t\t}\n\t\t\t\tr.fc = false\n\t\t\t}\n\t\t}\n\t\t// Add to the pending list if dealing with uncompressed frames or\n\t\t// after we have received the full compressed message and decompressed it.\n\t\tif addToPending {\n\t\t\tr.pending = append(r.pending, b)\n\t\t}\n\t}\n\t// In case of compression, there may be nothing to drain\n\tif len(r.pending) > 0 {\n\t\treturn r.drainPending(p), nil\n\t}\n\treturn 0, nil\n}\n\nfunc (r *websocketReader) addCBuf(b []byte) {\n\tif r.dc == nil {\n\t\tr.dc = &wsDecompressor{}\n\t}\n\t// Add a copy of the incoming buffer to the list of compressed buffers.\n\tr.dc.addBuf(append([]byte(nil), b...))\n}\n\nfunc (r *websocketReader) drainPending(p []byte) int {\n\tvar n int\n\tvar max = len(p)\n\n\tfor i, buf := range r.pending {\n\t\tif n+len(buf) <= max {\n\t\t\tcopy(p[n:], buf)\n\t\t\tn += len(buf)\n\t\t} else {\n\t\t\t// Is there room left?\n\t\t\tif n < max {\n\t\t\t\t// Write the partial and update this slice.\n\t\t\t\trem := max - n\n\t\t\t\tcopy(p[n:], buf[:rem])\n\t\t\t\tn += rem\n\t\t\t\tr.pending[i] = buf[rem:]\n\t\t\t}\n\t\t\t// These are the remaining slices that will need to be used at\n\t\t\t// the next Read() call.\n\t\t\tr.pending = r.pending[i:]\n\t\t\treturn n\n\t\t}\n\t}\n\tr.pending = r.pending[:0]\n\treturn n\n}\n\nfunc wsGet(r io.Reader, buf []byte, pos, needed int) ([]byte, int, error) {\n\tavail := len(buf) - pos\n\tif avail >= needed {\n\t\treturn buf[pos : pos+needed], pos + needed, nil\n\t}\n\tb := make([]byte, needed)\n\tstart := copy(b, buf[pos:])\n\tfor start != needed {\n\t\tn, err := r.Read(b[start:cap(b)])\n\t\tstart += n\n\t\tif err != nil {\n\t\t\treturn b, start, err\n\t\t}\n\t}\n\treturn b, pos + avail, nil\n}\n\nfunc (r *websocketReader) handleControlFrame(frameType wsOpCode, buf []byte, pos, rem int) (int, error) {\n\tvar payload []byte\n\tvar err error\n\n\tif rem > 0 {\n\t\tpayload, pos, err = wsGet(r.r, buf, pos, rem)\n\t\tif err != nil {\n\t\t\treturn pos, err\n\t\t}\n\t}\n\tswitch frameType {\n\tcase wsCloseMessage:\n\t\tstatus := wsCloseStatusNoStatusReceived\n\t\tvar body string\n\t\tlp := len(payload)\n\t\t// If there is a payload, the status is represented as a 2-byte\n\t\t// unsigned integer (in network byte order). Then, there may be an\n\t\t// optional body.\n\t\thasStatus, hasBody := lp >= wsCloseSatusSize, lp > wsCloseSatusSize\n\t\tif hasStatus {\n\t\t\t// Decode the status\n\t\t\tstatus = int(binary.BigEndian.Uint16(payload[:wsCloseSatusSize]))\n\t\t\t// Now if there is a body, capture it and make sure this is a valid UTF-8.\n\t\t\tif hasBody {\n\t\t\t\tbody = string(payload[wsCloseSatusSize:])\n\t\t\t\tif !utf8.ValidString(body) {\n\t\t\t\t\t// https://tools.ietf.org/html/rfc6455#section-5.5.1\n\t\t\t\t\t// If body is present, it must be a valid utf8\n\t\t\t\t\tstatus = wsCloseStatusInvalidPayloadData\n\t\t\t\t\tbody = \"invalid utf8 body in close frame\"\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tr.nc.wsEnqueueCloseMsg(r.nl, status, body)\n\t\t// Return io.EOF so that readLoop will close the connection as client closed\n\t\t// after processing pending buffers.\n\t\treturn pos, io.EOF\n\tcase wsPingMessage:\n\t\tr.nc.wsEnqueueControlMsg(r.nl, wsPongMessage, payload)\n\tcase wsPongMessage:\n\t\t// Nothing to do..\n\t}\n\treturn pos, nil\n}\n\nfunc (w *websocketWriter) Write(p []byte) (int, error) {\n\tif w.noMoreSend {\n\t\treturn 0, nil\n\t}\n\tvar total int\n\tvar n int\n\tvar err error\n\t// If there are control frames, they can be sent now. Actually spec says\n\t// that they should be sent ASAP, so we will send before any application data.\n\tif len(w.ctrlFrames) > 0 {\n\t\tn, err = w.writeCtrlFrames()\n\t\tif err != nil {\n\t\t\treturn n, err\n\t\t}\n\t\ttotal += n\n\t}\n\t// Do the following only if there is something to send.\n\t// We will end with checking for need to send close message.\n\tif len(p) > 0 {\n\t\tif w.compress {\n\t\t\tbuf := &bytes.Buffer{}\n\t\t\tif w.compressor == nil {\n\t\t\t\tw.compressor, _ = flate.NewWriter(buf, flate.BestSpeed)\n\t\t\t} else {\n\t\t\t\tw.compressor.Reset(buf)\n\t\t\t}\n\t\t\tif n, err = w.compressor.Write(p); err != nil {\n\t\t\t\treturn n, err\n\t\t\t}\n\t\t\tif err = w.compressor.Flush(); err != nil {\n\t\t\t\treturn n, err\n\t\t\t}\n\t\t\tb := buf.Bytes()\n\t\t\tp = b[:len(b)-4]\n\t\t}\n\t\tfh, key := wsCreateFrameHeader(w.compress, wsBinaryMessage, len(p))\n\t\twsMaskBuf(key, p)\n\t\tn, err = w.w.Write(fh)\n\t\ttotal += n\n\t\tif err == nil {\n\t\t\tn, err = w.w.Write(p)\n\t\t\ttotal += n\n\t\t}\n\t}\n\tif err == nil && w.cm != nil {\n\t\tn, err = w.writeCloseMsg()\n\t\ttotal += n\n\t}\n\treturn total, err\n}\n\nfunc (w *websocketWriter) writeCtrlFrames() (int, error) {\n\tvar (\n\t\tn     int\n\t\ttotal int\n\t\ti     int\n\t\terr   error\n\t)\n\tfor ; i < len(w.ctrlFrames); i++ {\n\t\tbuf := w.ctrlFrames[i]\n\t\tn, err = w.w.Write(buf)\n\t\ttotal += n\n\t\tif err != nil {\n\t\t\tbreak\n\t\t}\n\t}\n\tif i != len(w.ctrlFrames) {\n\t\tw.ctrlFrames = w.ctrlFrames[i+1:]\n\t} else {\n\t\tw.ctrlFrames = w.ctrlFrames[:0]\n\t}\n\treturn total, err\n}\n\nfunc (w *websocketWriter) writeCloseMsg() (int, error) {\n\tn, err := w.w.Write(w.cm)\n\tw.cm, w.noMoreSend = nil, true\n\treturn n, err\n}\n\nfunc wsMaskBuf(key, buf []byte) {\n\tfor i := 0; i < len(buf); i++ {\n\t\tbuf[i] ^= key[i&3]\n\t}\n}\n\n// Create the frame header.\n// Encodes the frame type and optional compression flag, and the size of the payload.\nfunc wsCreateFrameHeader(compressed bool, frameType wsOpCode, l int) ([]byte, []byte) {\n\tfh := make([]byte, wsMaxFrameHeaderSize)\n\tn, key := wsFillFrameHeader(fh, compressed, frameType, l)\n\treturn fh[:n], key\n}\n\nfunc wsFillFrameHeader(fh []byte, compressed bool, frameType wsOpCode, l int) (int, []byte) {\n\tvar n int\n\tb := byte(frameType)\n\tb |= wsFinalBit\n\tif compressed {\n\t\tb |= wsRsv1Bit\n\t}\n\tb1 := byte(wsMaskBit)\n\tswitch {\n\tcase l <= 125:\n\t\tn = 2\n\t\tfh[0] = b\n\t\tfh[1] = b1 | byte(l)\n\tcase l < 65536:\n\t\tn = 4\n\t\tfh[0] = b\n\t\tfh[1] = b1 | 126\n\t\tbinary.BigEndian.PutUint16(fh[2:], uint16(l))\n\tdefault:\n\t\tn = 10\n\t\tfh[0] = b\n\t\tfh[1] = b1 | 127\n\t\tbinary.BigEndian.PutUint64(fh[2:], uint64(l))\n\t}\n\tvar key []byte\n\tvar keyBuf [4]byte\n\tif _, err := io.ReadFull(rand.Reader, keyBuf[:4]); err != nil {\n\t\tkv := mrand.Int31()\n\t\tbinary.LittleEndian.PutUint32(keyBuf[:4], uint32(kv))\n\t}\n\tcopy(fh[n:], keyBuf[:4])\n\tkey = fh[n : n+4]\n\tn += 4\n\treturn n, key\n}\n\nfunc (nc *Conn) wsInitHandshake(u *url.URL) error {\n\tcompress := nc.Opts.Compression\n\ttlsRequired := u.Scheme == wsSchemeTLS || nc.Opts.Secure || nc.Opts.TLSConfig != nil || nc.Opts.TLSCertCB != nil || nc.Opts.RootCAsCB != nil\n\t// Do TLS here as needed.\n\tif tlsRequired {\n\t\tif err := nc.makeTLSConn(); err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\tnc.bindToNewConn()\n\t}\n\n\tvar err error\n\n\t// For http request, we need the passed URL to contain either http or https scheme.\n\tscheme := \"http\"\n\tif tlsRequired {\n\t\tscheme = \"https\"\n\t}\n\tustr := fmt.Sprintf(\"%s://%s\", scheme, u.Host)\n\n\tif nc.Opts.ProxyPath != \"\" {\n\t\tproxyPath := nc.Opts.ProxyPath\n\t\tif !strings.HasPrefix(proxyPath, \"/\") {\n\t\t\tproxyPath = \"/\" + proxyPath\n\t\t}\n\t\tustr += proxyPath\n\t}\n\n\tu, err = url.Parse(ustr)\n\tif err != nil {\n\t\treturn err\n\t}\n\treq := &http.Request{\n\t\tMethod:     \"GET\",\n\t\tURL:        u,\n\t\tProto:      \"HTTP/1.1\",\n\t\tProtoMajor: 1,\n\t\tProtoMinor: 1,\n\t\tHeader:     make(http.Header),\n\t\tHost:       u.Host,\n\t}\n\twsKey, err := wsMakeChallengeKey()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treq.Header[\"Upgrade\"] = []string{\"websocket\"}\n\treq.Header[\"Connection\"] = []string{\"Upgrade\"}\n\treq.Header[\"Sec-WebSocket-Key\"] = []string{wsKey}\n\treq.Header[\"Sec-WebSocket-Version\"] = []string{\"13\"}\n\tif compress {\n\t\treq.Header.Add(\"Sec-WebSocket-Extensions\", wsPMCReqHeaderValue)\n\t}\n\tif err := req.Write(nc.conn); err != nil {\n\t\treturn err\n\t}\n\n\tvar resp *http.Response\n\n\tbr := bufio.NewReaderSize(nc.conn, 4096)\n\tnc.conn.SetReadDeadline(time.Now().Add(nc.Opts.Timeout))\n\tresp, err = http.ReadResponse(br, req)\n\tif err == nil &&\n\t\t(resp.StatusCode != 101 ||\n\t\t\t!strings.EqualFold(resp.Header.Get(\"Upgrade\"), \"websocket\") ||\n\t\t\t!strings.EqualFold(resp.Header.Get(\"Connection\"), \"upgrade\") ||\n\t\t\tresp.Header.Get(\"Sec-Websocket-Accept\") != wsAcceptKey(wsKey)) {\n\n\t\terr = errors.New(\"invalid websocket connection\")\n\t}\n\t// Check compression extension...\n\tif err == nil && compress {\n\t\t// Check that not only permessage-deflate extension is present, but that\n\t\t// we also have server and client no context take over.\n\t\tsrvCompress, noCtxTakeover := wsPMCExtensionSupport(resp.Header)\n\n\t\t// If server does not support compression, then simply disable it in our side.\n\t\tif !srvCompress {\n\t\t\tcompress = false\n\t\t} else if !noCtxTakeover {\n\t\t\terr = errors.New(\"compression negotiation error\")\n\t\t}\n\t}\n\tif resp != nil {\n\t\tresp.Body.Close()\n\t}\n\tnc.conn.SetReadDeadline(time.Time{})\n\tif err != nil {\n\t\treturn err\n\t}\n\n\twsr := wsNewReader(nc.br.r)\n\twsr.nc = nc\n\t// We have to slurp whatever is in the bufio reader and copy to br.r\n\tif n := br.Buffered(); n != 0 {\n\t\twsr.ib, _ = br.Peek(n)\n\t}\n\tnc.br.r = wsr\n\tnc.bw.w = &websocketWriter{w: nc.bw.w, compress: compress}\n\tnc.ws = true\n\treturn nil\n}\n\nfunc (nc *Conn) wsClose() {\n\tnc.mu.Lock()\n\tdefer nc.mu.Unlock()\n\tif !nc.ws {\n\t\treturn\n\t}\n\tnc.wsEnqueueCloseMsgLocked(wsCloseStatusNormalClosure, _EMPTY_)\n}\n\nfunc (nc *Conn) wsEnqueueCloseMsg(needsLock bool, status int, payload string) {\n\t// In some low-level unit tests it will happen...\n\tif nc == nil {\n\t\treturn\n\t}\n\tif needsLock {\n\t\tnc.mu.Lock()\n\t\tdefer nc.mu.Unlock()\n\t}\n\tnc.wsEnqueueCloseMsgLocked(status, payload)\n}\n\nfunc (nc *Conn) wsEnqueueCloseMsgLocked(status int, payload string) {\n\twr, ok := nc.bw.w.(*websocketWriter)\n\tif !ok || wr.cmDone {\n\t\treturn\n\t}\n\tstatusAndPayloadLen := 2 + len(payload)\n\tframe := make([]byte, 2+4+statusAndPayloadLen)\n\tn, key := wsFillFrameHeader(frame, false, wsCloseMessage, statusAndPayloadLen)\n\t// Set the status\n\tbinary.BigEndian.PutUint16(frame[n:], uint16(status))\n\t// If there is a payload, copy\n\tif len(payload) > 0 {\n\t\tcopy(frame[n+2:], payload)\n\t}\n\t// Mask status + payload\n\twsMaskBuf(key, frame[n:n+statusAndPayloadLen])\n\twr.cm = frame\n\twr.cmDone = true\n\tnc.bw.flush()\n\tif c := wr.compressor; c != nil {\n\t\tc.Close()\n\t}\n}\n\nfunc (nc *Conn) wsEnqueueControlMsg(needsLock bool, frameType wsOpCode, payload []byte) {\n\t// In some low-level unit tests it will happen...\n\tif nc == nil {\n\t\treturn\n\t}\n\tif needsLock {\n\t\tnc.mu.Lock()\n\t\tdefer nc.mu.Unlock()\n\t}\n\twr, ok := nc.bw.w.(*websocketWriter)\n\tif !ok {\n\t\treturn\n\t}\n\tfh, key := wsCreateFrameHeader(false, frameType, len(payload))\n\twr.ctrlFrames = append(wr.ctrlFrames, fh)\n\tif len(payload) > 0 {\n\t\twsMaskBuf(key, payload)\n\t\twr.ctrlFrames = append(wr.ctrlFrames, payload)\n\t}\n\tnc.bw.flush()\n}\n\nfunc wsPMCExtensionSupport(header http.Header) (bool, bool) {\n\tfor _, extensionList := range header[\"Sec-Websocket-Extensions\"] {\n\t\textensions := strings.Split(extensionList, \",\")\n\t\tfor _, extension := range extensions {\n\t\t\textension = strings.Trim(extension, \" \\t\")\n\t\t\tparams := strings.Split(extension, \";\")\n\t\t\tfor i, p := range params {\n\t\t\t\tp = strings.Trim(p, \" \\t\")\n\t\t\t\tif strings.EqualFold(p, wsPMCExtension) {\n\t\t\t\t\tvar snc bool\n\t\t\t\t\tvar cnc bool\n\t\t\t\t\tfor j := i + 1; j < len(params); j++ {\n\t\t\t\t\t\tp = params[j]\n\t\t\t\t\t\tp = strings.Trim(p, \" \\t\")\n\t\t\t\t\t\tif strings.EqualFold(p, wsPMCSrvNoCtx) {\n\t\t\t\t\t\t\tsnc = true\n\t\t\t\t\t\t} else if strings.EqualFold(p, wsPMCCliNoCtx) {\n\t\t\t\t\t\t\tcnc = true\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif snc && cnc {\n\t\t\t\t\t\t\treturn true, true\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\treturn true, false\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn false, false\n}\n\nfunc wsMakeChallengeKey() (string, error) {\n\tp := make([]byte, 16)\n\tif _, err := io.ReadFull(rand.Reader, p); err != nil {\n\t\treturn \"\", err\n\t}\n\treturn base64.StdEncoding.EncodeToString(p), nil\n}\n\nfunc wsAcceptKey(key string) string {\n\th := sha1.New()\n\th.Write([]byte(key))\n\th.Write(wsGUID)\n\treturn base64.StdEncoding.EncodeToString(h.Sum(nil))\n}\n\n// Returns true if the op code corresponds to a control frame.\nfunc wsIsControlFrame(frameType wsOpCode) bool {\n\treturn frameType >= wsCloseMessage\n}\n\nfunc isWebsocketScheme(u *url.URL) bool {\n\treturn u.Scheme == wsScheme || u.Scheme == wsSchemeTLS\n}\n"
        },
        {
          "name": "ws_test.go",
          "type": "blob",
          "size": 16.1279296875,
          "content": "// Copyright 2021-2023 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage nats\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"net/http\"\n\t\"reflect\"\n\t\"strings\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/klauspost/compress/flate\"\n)\n\ntype fakeReader struct {\n\tmu     sync.Mutex\n\tbuf    bytes.Buffer\n\tch     chan []byte\n\tclosed bool\n}\n\nfunc (f *fakeReader) Read(p []byte) (int, error) {\n\tf.mu.Lock()\n\tclosed := f.closed\n\tf.mu.Unlock()\n\tif closed {\n\t\treturn 0, io.EOF\n\t}\n\tfor {\n\t\tif f.buf.Len() > 0 {\n\t\t\tn, err := f.buf.Read(p)\n\t\t\treturn n, err\n\t\t}\n\t\tbuf, ok := <-f.ch\n\t\tif !ok {\n\t\t\treturn 0, io.EOF\n\t\t}\n\t\tf.buf.Write(buf)\n\t}\n}\n\nfunc (f *fakeReader) close() {\n\tf.mu.Lock()\n\tdefer f.mu.Unlock()\n\tif f.closed {\n\t\treturn\n\t}\n\tf.closed = true\n\tclose(f.ch)\n}\n\nfunc TestWSReader(t *testing.T) {\n\tmr := &fakeReader{ch: make(chan []byte, 1)}\n\tdefer mr.close()\n\tr := wsNewReader(mr)\n\n\tp := make([]byte, 100)\n\tcheckRead := func(limit int, expected []byte, lenPending int) {\n\t\tt.Helper()\n\t\tn, err := r.Read(p[:limit])\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"Error reading: %v\", err)\n\t\t}\n\t\tif !bytes.Equal(p[:n], expected) {\n\t\t\tt.Fatalf(\"Expected %q, got %q\", expected, p[:n])\n\t\t}\n\t\tif len(r.pending) != lenPending {\n\t\t\tt.Fatalf(\"Expected len(r.pending) to be %v, got %v\", lenPending, len(r.pending))\n\t\t}\n\t}\n\n\t// Test with a buffer that contains a single pending with all data that\n\t// fits in the read buffer.\n\tmr.buf.Write([]byte{130, 10})\n\tmr.buf.WriteString(\"ABCDEFGHIJ\")\n\tcheckRead(100, []byte(\"ABCDEFGHIJ\"), 0)\n\n\t// Write 2 frames in the buffer. Since we will call with a read buffer\n\t// that can fit both, we will create 2 pending and consume them at once.\n\tmr.buf.Write([]byte{130, 5})\n\tmr.buf.WriteString(\"ABCDE\")\n\tmr.buf.Write([]byte{130, 5})\n\tmr.buf.WriteString(\"FGHIJ\")\n\tcheckRead(100, []byte(\"ABCDEFGHIJ\"), 0)\n\n\t// We also write 2 frames, but this time we will call the first read\n\t// with a read buffer that can accommodate only the first frame.\n\t// So internally only a single frame is going to be read in pending.\n\tmr.buf.Write([]byte{130, 5})\n\tmr.buf.WriteString(\"ABCDE\")\n\tmr.buf.Write([]byte{130, 5})\n\tmr.buf.WriteString(\"FGHIJ\")\n\tcheckRead(6, []byte(\"ABCDE\"), 0)\n\tcheckRead(100, []byte(\"FGHIJ\"), 0)\n\n\t// To test partials, we need to directly set the pending buffers.\n\tr.pending = append(r.pending, []byte(\"ABCDE\"))\n\tr.pending = append(r.pending, []byte(\"FGHIJ\"))\n\t// Now check that the first read cannot get the full first pending\n\t// buffer and gets only a partial.\n\tcheckRead(3, []byte(\"ABC\"), 2)\n\t// Since the read buffer is big enough to get everything else, after\n\t// this call we should have no pending.\n\tcheckRead(7, []byte(\"DEFGHIJ\"), 0)\n\n\t// Similar to above but with both partials.\n\tr.pending = append(r.pending, []byte(\"ABCDE\"))\n\tr.pending = append(r.pending, []byte(\"FGHIJ\"))\n\tcheckRead(3, []byte(\"ABC\"), 2)\n\t// Exact amount of the partial of 1st pending\n\tcheckRead(2, []byte(\"DE\"), 1)\n\tcheckRead(3, []byte(\"FGH\"), 1)\n\t// More space in read buffer than last partial\n\tcheckRead(10, []byte(\"IJ\"), 0)\n\n\t// This test the fact that read will return only when a frame is complete.\n\tmr.buf.Write([]byte{130, 5})\n\tmr.buf.WriteString(\"AB\")\n\twg := sync.WaitGroup{}\n\twg.Add(1)\n\tgo func() {\n\t\ttime.Sleep(100 * time.Millisecond)\n\t\tmr.ch <- []byte{'C', 'D', 'E', 130, 2, 'F', 'G'}\n\t\twg.Done()\n\t}()\n\t// Read() will get \"load\" only the first frame, so after this call there\n\t// should be no pending.\n\tcheckRead(100, []byte(\"ABCDE\"), 0)\n\t// This will load the second frame.\n\tcheckRead(100, []byte(\"FG\"), 0)\n\twg.Wait()\n\n\t// Set the buffer that may be populated during the init handshake.\n\t// Make sure that we process that one first.\n\tr.ib = []byte{130, 4, 'A', 'B'}\n\tmr.buf.WriteString(\"CD\")\n\tmr.buf.Write([]byte{130, 2})\n\tmr.buf.WriteString(\"EF\")\n\t// This will only read up to ABCD and have no pending after the call.\n\tcheckRead(100, []byte(\"ABCD\"), 0)\n\t// We need another Read() call to read/load the second frame.\n\tcheckRead(100, []byte(\"EF\"), 0)\n\n\t// Close the underlying reader while reading.\n\tmr.buf.Write([]byte{130, 4, 'A', 'B'})\n\twg.Add(1)\n\tgo func() {\n\t\ttime.Sleep(100 * time.Millisecond)\n\t\tmr.close()\n\t\twg.Done()\n\t}()\n\tif _, err := r.Read(p); err != io.EOF {\n\t\tt.Fatalf(\"Expected EOF, got %v\", err)\n\t}\n\twg.Wait()\n}\n\nfunc TestWSParseControlFrames(t *testing.T) {\n\tmr := &fakeReader{ch: make(chan []byte, 1)}\n\tdefer mr.close()\n\tr := wsNewReader(mr)\n\n\tp := make([]byte, 100)\n\n\t// Write a PING\n\tmr.buf.Write([]byte{137, 0})\n\tn, err := r.Read(p)\n\tif err != nil || n != 0 {\n\t\tt.Fatalf(\"Error on read: n=%v err=%v\", n, err)\n\t}\n\n\t// Write a PONG\n\tmr.buf.Write([]byte{138, 0})\n\tn, err = r.Read(p)\n\tif err != nil || n != 0 {\n\t\tt.Fatalf(\"Error on read: n=%v err=%v\", n, err)\n\t}\n\n\t// Write a CLOSE\n\tmr.buf.Write([]byte{136, 6, 3, 232, 't', 'e', 's', 't'})\n\tn, err = r.Read(p)\n\tif err != io.EOF || n != 0 {\n\t\tt.Fatalf(\"Error on read: n=%v err=%v\", n, err)\n\t}\n\n\t// Write a CLOSE without payload\n\tmr.buf.Write([]byte{136, 2, 3, 232})\n\tn, err = r.Read(p)\n\tif err != io.EOF || n != 0 {\n\t\tt.Fatalf(\"Error on read: n=%v err=%v\", n, err)\n\t}\n\n\t// Write a CLOSE with invalid status\n\tmr.buf.Write([]byte{136, 1, 100})\n\tn, err = r.Read(p)\n\tif err != io.EOF || n != 0 {\n\t\tt.Fatalf(\"Error on read: n=%v err=%v\", n, err)\n\t}\n\n\t// Write CLOSE with valid status and payload but call with a read buffer\n\t// that has capacity of 1.\n\tmr.buf.Write([]byte{136, 6, 3, 232, 't', 'e', 's', 't'})\n\tpl := []byte{136}\n\tn, err = r.Read(pl[:])\n\tif err != io.EOF || n != 0 {\n\t\tt.Fatalf(\"Error on read: n=%v err=%v\", n, err)\n\t}\n}\n\nfunc TestWSParseInvalidFrames(t *testing.T) {\n\n\tnewReader := func() (*fakeReader, *websocketReader) {\n\t\tmr := &fakeReader{}\n\t\tr := wsNewReader(mr)\n\t\treturn mr, r\n\t}\n\n\tp := make([]byte, 100)\n\n\t// Invalid utf-8 of close message\n\tmr, r := newReader()\n\tmr.buf.Write([]byte{136, 6, 3, 232, 't', 'e', 0xF1, 't'})\n\tn, err := r.Read(p)\n\tif err != io.EOF || n != 0 {\n\t\tt.Fatalf(\"Error on read: n=%v err=%v\", n, err)\n\t}\n\n\t// control frame length too long\n\tmr, r = newReader()\n\tmr.buf.Write([]byte{137, 126, 0, wsMaxControlPayloadSize + 10})\n\tfor i := 0; i < wsMaxControlPayloadSize+10; i++ {\n\t\tmr.buf.WriteByte('a')\n\t}\n\tn, err = r.Read(p)\n\tif n != 0 || err == nil || !strings.Contains(err.Error(), \"maximum\") {\n\t\tt.Fatalf(\"Unexpected error: n=%v err=%v\", n, err)\n\t}\n\n\t// Not a final frame\n\tmr, r = newReader()\n\tmr.buf.Write([]byte{byte(wsPingMessage), 0})\n\tn, err = r.Read(p[:2])\n\tif n != 0 || err == nil || !strings.Contains(err.Error(), \"final\") {\n\t\tt.Fatalf(\"Unexpected error: n=%v err=%v\", n, err)\n\t}\n\n\t// Marked as compressed\n\tmr, r = newReader()\n\tmr.buf.Write([]byte{byte(wsPingMessage) | wsRsv1Bit, 0})\n\tn, err = r.Read(p[:2])\n\tif n != 0 || err == nil || !strings.Contains(err.Error(), \"compressed\") {\n\t\tt.Fatalf(\"Unexpected error: n=%v err=%v\", n, err)\n\t}\n\n\t// Continuation frame marked as compressed\n\tmr, r = newReader()\n\tmr.buf.Write([]byte{2, 3})\n\tmr.buf.WriteString(\"ABC\")\n\tmr.buf.Write([]byte{0 | wsRsv1Bit, 3})\n\tmr.buf.WriteString(\"DEF\")\n\tn, err = r.Read(p)\n\tif n != 0 || err == nil || !strings.Contains(err.Error(), \"invalid continuation frame\") {\n\t\tt.Fatalf(\"Unexpected error: n=%v err=%v\", n, err)\n\t}\n\n\t// Continuation frame after a final frame\n\tmr, r = newReader()\n\tmr.buf.Write([]byte{130, 3})\n\tmr.buf.WriteString(\"ABC\")\n\tmr.buf.Write([]byte{0, 3})\n\tmr.buf.WriteString(\"DEF\")\n\tn, err = r.Read(p)\n\tif n != 0 || err == nil || !strings.Contains(err.Error(), \"invalid continuation frame\") {\n\t\tt.Fatalf(\"Unexpected error: n=%v err=%v\", n, err)\n\t}\n\n\t// New message received before previous ended\n\tmr, r = newReader()\n\tmr.buf.Write([]byte{2, 3})\n\tmr.buf.WriteString(\"ABC\")\n\tmr.buf.Write([]byte{0, 3})\n\tmr.buf.WriteString(\"DEF\")\n\tmr.buf.Write([]byte{130, 3})\n\tmr.buf.WriteString(\"GHI\")\n\tn, err = r.Read(p)\n\tif n != 0 || err == nil || !strings.Contains(err.Error(), \"started before final frame\") {\n\t\tt.Fatalf(\"Unexpected error: n=%v err=%v\", n, err)\n\t}\n\n\t// Unknown frame type\n\tmr, r = newReader()\n\tmr.buf.Write([]byte{99, 3})\n\tmr.buf.WriteString(\"ABC\")\n\tn, err = r.Read(p)\n\tif n != 0 || err == nil || !strings.Contains(err.Error(), \"unknown opcode\") {\n\t\tt.Fatalf(\"Unexpected error: n=%v err=%v\", n, err)\n\t}\n}\n\nfunc TestWSControlFrameBetweenDataFrames(t *testing.T) {\n\tmr := &fakeReader{ch: make(chan []byte, 1)}\n\tdefer mr.close()\n\tr := wsNewReader(mr)\n\n\tp := make([]byte, 100)\n\n\t// Write a frame that will continue after the PONG\n\tmr.buf.Write([]byte{2, 3})\n\tmr.buf.WriteString(\"ABC\")\n\t// Write a PONG\n\tmr.buf.Write([]byte{138, 0})\n\t// Continuation of the frame\n\tmr.buf.Write([]byte{0, 3})\n\tmr.buf.WriteString(\"DEF\")\n\t// Another PONG\n\tmr.buf.Write([]byte{138, 0})\n\t// End of frame\n\tmr.buf.Write([]byte{128, 3})\n\tmr.buf.WriteString(\"GHI\")\n\n\tn, err := r.Read(p)\n\tif err != nil {\n\t\tt.Fatalf(\"Error on read: %v\", err)\n\t}\n\tif string(p[:n]) != \"ABCDEFGHI\" {\n\t\tt.Fatalf(\"Unexpected result: %q\", p[:n])\n\t}\n}\n\nfunc TestWSDecompressor(t *testing.T) {\n\tvar br *wsDecompressor\n\n\tp := make([]byte, 100)\n\tcheckRead := func(limit int, expected []byte) {\n\t\tt.Helper()\n\t\tn, err := br.Read(p[:limit])\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"Error on read: %v\", err)\n\t\t}\n\t\tif got := p[:n]; !bytes.Equal(expected, got) {\n\t\t\tt.Fatalf(\"Expected %v, got %v\", expected, got)\n\t\t}\n\t}\n\tcheckEOF := func() {\n\t\tt.Helper()\n\t\tn, err := br.Read(p)\n\t\tif err != io.EOF || n > 0 {\n\t\t\tt.Fatalf(\"Unexpected result: n=%v err=%v\", n, err)\n\t\t}\n\t}\n\tcheckReadByte := func(expected byte) {\n\t\tt.Helper()\n\t\tb, err := br.ReadByte()\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"Error on read: %v\", err)\n\t\t}\n\t\tif b != expected {\n\t\t\tt.Fatalf(\"Expected %c, got %c\", expected, b)\n\t\t}\n\t}\n\tcheckEOFWithReadByte := func() {\n\t\tt.Helper()\n\t\tn, err := br.ReadByte()\n\t\tif err != io.EOF || n > 0 {\n\t\t\tt.Fatalf(\"Unexpected result: n=%v err=%v\", n, err)\n\t\t}\n\t}\n\n\tnewDecompressor := func(str string) *wsDecompressor {\n\t\td := &wsDecompressor{}\n\t\td.addBuf([]byte(str))\n\t\treturn d\n\t}\n\n\t// Read with enough room\n\tbr = newDecompressor(\"ABCDE\")\n\tcheckRead(100, []byte(\"ABCDE\"))\n\tcheckEOF()\n\tcheckEOFWithReadByte()\n\n\t// Read with a partial from our buffer\n\tbr = newDecompressor(\"FGHIJ\")\n\tcheckRead(2, []byte(\"FG\"))\n\t// Call with more than the end of our buffer.\n\tcheckRead(10, []byte(\"HIJ\"))\n\tcheckEOF()\n\tcheckEOFWithReadByte()\n\n\t// Read with a partial from our buffer\n\tbr = newDecompressor(\"KLMNO\")\n\tcheckRead(2, []byte(\"KL\"))\n\t// Call with exact number of bytes left for our buffer.\n\tcheckRead(3, []byte(\"MNO\"))\n\tcheckEOF()\n\tcheckEOFWithReadByte()\n\n\t// Finally, check ReadByte.\n\tbr = newDecompressor(\"UVWXYZ\")\n\tcheckRead(4, []byte(\"UVWX\"))\n\tcheckReadByte('Y')\n\tcheckReadByte('Z')\n\tcheckEOFWithReadByte()\n\tcheckEOF()\n\n\tbr = newDecompressor(\"ABC\")\n\tbuf := make([]byte, 0)\n\tn, err := br.Read(buf)\n\tif n != 0 || err != nil {\n\t\tt.Fatalf(\"Unexpected n=%v err=%v\", n, err)\n\t}\n}\n\nfunc TestWSNoMixingScheme(t *testing.T) {\n\t// Check opts.Connect() first\n\tfor _, test := range []struct {\n\t\turl     string\n\t\tservers []string\n\t}{\n\t\t{\"ws://127.0.0.1:1234\", []string{\"nats://127.0.0.1:1235\"}},\n\t\t{\"ws://127.0.0.1:1234\", []string{\"ws://127.0.0.1:1235\", \"nats://127.0.0.1:1236\"}},\n\t\t{\"ws://127.0.0.1:1234\", []string{\"wss://127.0.0.1:1235\", \"nats://127.0.0.1:1236\"}},\n\t\t{\"wss://127.0.0.1:1234\", []string{\"nats://127.0.0.1:1235\"}},\n\t\t{\"wss://127.0.0.1:1234\", []string{\"wss://127.0.0.1:1235\", \"nats://127.0.0.1:1236\"}},\n\t\t{\"wss://127.0.0.1:1234\", []string{\"ws://127.0.0.1:1235\", \"nats://127.0.0.1:1236\"}},\n\t} {\n\t\tt.Run(\"Options\", func(t *testing.T) {\n\t\t\topts := GetDefaultOptions()\n\t\t\topts.Url = test.url\n\t\t\topts.Servers = test.servers\n\t\t\tnc, err := opts.Connect()\n\t\t\tif err == nil || !strings.Contains(err.Error(), \"mixing\") {\n\t\t\t\tif nc != nil {\n\t\t\t\t\tnc.Close()\n\t\t\t\t}\n\t\t\t\tt.Fatalf(\"Expected error about mixing, got %v\", err)\n\t\t\t}\n\t\t})\n\t}\n\t// Check Connect() now.\n\tfor _, test := range []struct {\n\t\turls    string\n\t\tservers []string\n\t}{\n\t\t{\"ws://127.0.0.1:1234,nats://127.0.0.1:1235\", nil},\n\t\t{\"ws://127.0.0.1:1234,tcp://127.0.0.1:1235\", nil},\n\t\t{\"ws://127.0.0.1:1234,tls://127.0.0.1:1235\", nil},\n\t\t{\"nats://127.0.0.1:1234,ws://127.0.0.1:1235\", nil},\n\t\t{\"nats://127.0.0.1:1234,wss://127.0.0.1:1235\", nil},\n\t\t{\"nats://127.0.0.1:1234,tls://127.0.0.1:1235,ws://127.0.0.1:1236\", nil},\n\t\t{\"nats://127.0.0.1:1234,tls://127.0.0.1:1235,wss://127.0.0.1:1236\", nil},\n\t\t// In Connect(), the URL is ignored when Servers() is provided.\n\t\t{\"\", []string{\"nats://127.0.0.1:1235\", \"ws://127.0.0.1:1236\"}},\n\t\t{\"\", []string{\"nats://127.0.0.1:1235\", \"wss://127.0.0.1:1236\"}},\n\t\t{\"\", []string{\"ws://127.0.0.1:1235\", \"nats://127.0.0.1:1236\"}},\n\t\t{\"\", []string{\"wss://127.0.0.1:1235\", \"nats://127.0.0.1:1236\"}},\n\t} {\n\t\tt.Run(\"Connect\", func(t *testing.T) {\n\t\t\tvar opt Option\n\t\t\tif len(test.servers) > 0 {\n\t\t\t\topt = func(o *Options) error {\n\t\t\t\t\to.Servers = test.servers\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\t\t\t}\n\t\t\tnc, err := Connect(test.urls, opt)\n\t\t\tif err == nil || !strings.Contains(err.Error(), \"mixing\") {\n\t\t\t\tif nc != nil {\n\t\t\t\t\tnc.Close()\n\t\t\t\t}\n\t\t\t\tt.Fatalf(\"Expected error about mixing, got %v\", err)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestWSCompressionWithContinuationFrames(t *testing.T) {\n\tuncompressed := []byte(\"this is an uncompressed message with AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\")\n\tbuf := &bytes.Buffer{}\n\tcompressor, _ := flate.NewWriter(buf, flate.BestSpeed)\n\tcompressor.Write(uncompressed)\n\tcompressor.Close()\n\tb := buf.Bytes()\n\tif len(b) < 30 {\n\t\tpanic(\"revisit test so that compressed buffer is more than 30 bytes long\")\n\t}\n\n\tsrbuf := &bytes.Buffer{}\n\t// We are going to split this in several frames.\n\tfh := []byte{66, 10}\n\tsrbuf.Write(fh)\n\tsrbuf.Write(b[:10])\n\tfh = []byte{0, 10}\n\tsrbuf.Write(fh)\n\tsrbuf.Write(b[10:20])\n\tfh = []byte{wsFinalBit, 0}\n\tfh[1] = byte(len(b) - 20)\n\tsrbuf.Write(fh)\n\tsrbuf.Write(b[20:])\n\n\tr := wsNewReader(srbuf)\n\trbuf := make([]byte, 100)\n\tn, err := r.Read(rbuf[:15])\n\t// Since we have a partial of compressed message, the library keeps track\n\t// of buffer, but it can't return anything at this point, so n==0 err==nil\n\t// is the expected result.\n\tif n != 0 || err != nil {\n\t\tt.Fatalf(\"Error reading: n=%v err=%v\", n, err)\n\t}\n\tn, err = r.Read(rbuf)\n\tif n != len(uncompressed) || err != nil {\n\t\tt.Fatalf(\"Error reading: n=%v err=%v\", n, err)\n\t}\n\tif !reflect.DeepEqual(uncompressed, rbuf[:n]) {\n\t\tt.Fatalf(\"Unexpected uncompressed data: %v\", rbuf[:n])\n\t}\n}\n\nfunc TestWSTlsNoConfig(t *testing.T) {\n\topts := GetDefaultOptions()\n\topts.Servers = []string{\"wss://localhost:443\"}\n\n\tnc := &Conn{Opts: opts}\n\tif err := nc.setupServerPool(); err != nil {\n\t\tt.Fatalf(\"Error setting up pool: %v\", err)\n\t}\n\t// Verify that this has set Secure/TLSConfig\n\tnc.mu.Lock()\n\tok := nc.Opts.Secure && nc.Opts.TLSConfig != nil\n\tnc.mu.Unlock()\n\tif !ok {\n\t\tt.Fatal(\"Secure and TLSConfig were not set\")\n\t}\n\t// Now try to add a bare host:ip to the pool and verify\n\t// that the wss:// scheme is added.\n\tif err := nc.addURLToPool(\"1.2.3.4:443\", true, false); err != nil {\n\t\tt.Fatalf(\"Error adding to pool: %v\", err)\n\t}\n\tnc.mu.Lock()\n\tfor _, srv := range nc.srvPool {\n\t\tif srv.url.Scheme != wsSchemeTLS {\n\t\t\tnc.mu.Unlock()\n\t\t\tt.Fatalf(\"Expected scheme to be %q, got url: %s\", wsSchemeTLS, srv.url)\n\t\t}\n\t}\n\tnc.mu.Unlock()\n}\n\nfunc TestWSProxyPath(t *testing.T) {\n\tconst proxyPath = \"proxy1\"\n\n\t// Listen to a random port\n\tl, err := net.Listen(\"tcp\", \":0\")\n\tif err != nil {\n\t\tt.Fatalf(\"Error in listen: %v\", err)\n\t}\n\tdefer l.Close()\n\n\tproxyPort := l.Addr().(*net.TCPAddr).Port\n\n\tch := make(chan struct{}, 1)\n\tproxySrv := &http.Server{\n\t\tHandler: http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\t\tif r.URL.Path == \"/\"+proxyPath {\n\t\t\t\tch <- struct{}{}\n\t\t\t}\n\t\t}),\n\t}\n\tdefer proxySrv.Shutdown(context.Background())\n\tgo proxySrv.Serve(l)\n\n\tfor _, test := range []struct {\n\t\tname string\n\t\tpath string\n\t}{\n\t\t{\"without slash\", proxyPath},\n\t\t{\"with slash\", \"/\" + proxyPath},\n\t} {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\turl := fmt.Sprintf(\"ws://127.0.0.1:%d\", proxyPort)\n\t\t\tnc, err := Connect(url, ProxyPath(test.path))\n\t\t\tif err == nil {\n\t\t\t\tnc.Close()\n\t\t\t\tt.Fatal(\"Did not expect to connect\")\n\t\t\t}\n\t\t\tselect {\n\t\t\tcase <-ch:\n\t\t\t\t// OK:\n\t\t\tcase <-time.After(time.Second):\n\t\t\t\tt.Fatal(\"Proxy was not reached\")\n\t\t\t}\n\t\t})\n\t}\n}\n"
        }
      ]
    }
  ]
}