{
  "metadata": {
    "timestamp": 1736567762058,
    "page": 343,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "gorgonia/gorgonia",
      "stars": 5604,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.478515625,
          "content": "# Set the default behavior, in case people don't have core.autocrlf set.\n* text=auto\n\n# Explicitly declare text files you want to always be normalized and converted\n# to native line endings on checkout.\n*.c text\n*.h text\n*.go text\n*.md text\n*.s text\n*.yml text\n*.toml text\n\n# Declare files that will always have CRLF line endings on checkout.\n*.sln text eol=crlf\n*.bat text eol=crlf\n\n# Denote all files that are truly binary and should not be modified.\n*.png binary\n*.jpg binary\n*.svg text\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.408203125,
          "content": "# Compiled Object files, Static and Dynamic libs (Shared Objects)\n*.o\n*.a\n*.so\n\n# Folders\n_obj\n_test\n\n# Architecture specific extensions/prefixes\n*.[568vq]\n[568vq].out\n\n*.cgo1.go\n*.cgo2.c\n_cgo_defun.c\n_cgo_gotypes.go\n_cgo_export.*\n\n_testmain.go\n\n*.exe\n*.test\n*.prof\n\ncudamodules.go\n\n# vendor\n/vendor\n\n# Example files\n*.dot\nexamples/convnet/tmp.dot\nexamples/iris/cmd/theta.bin\nexamples/iris/theta.bin\nexamples/testdata\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.1328125,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment include:\n\n* Using inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at chewxy+gorgonia@gmail.com. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4, available at [http://contributor-covenant.org/version/1/4][version]\n\n[homepage]: http://contributor-covenant.org\n[version]: http://contributor-covenant.org/version/1/4/\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 2.845703125,
          "content": "# Contributing #\n\nWe want to make contributing as easy as possible. There are a number of [issues](https://github.com/gorgonia/gorgonia/issues) that can be solved. Most of the issues are labelled.\n\n## Labels ##\n\nHere's the current explanation of the labels:\n\n<table>\n<thead><th>Label</th><th>Expertise Required</th><th>Notes</th></thead>\n<tbody>\n<tr><td>easy</td><td>Familiar with Go</td><td>Usually there would be a link to an existing implementation that is similar</td></tr>\n<tr><td>requires Linear Algebra knowledge</td><td>Linear algebra knowledge required on top of being familiar with Go</td><td>Linear Algebra knowledge will go a long way in helping identify subtler bugs</td></tr>\n<tr><td>no label</td><td>Knowledge about Gorgonia</td><td></td></tr>\n<tr><td>help wanted</td><td>Various expertise</td><td>Typically it means the task requires extranormal speciality that the author doesn't possess</td></tr>\n<tr><td>complicated</td><td>-</td><td>The code change will affect, and involve many files</td></tr>\n</tbody>\n</table>\n\n## Steps ##\n1. Fork this project on Github\n2. Clone to your local drive\n3. Check if there are any pending issues in the issues tracker\n4. Pick an unassigned issue that you can accomplish. Comment on the issue to pick it up.\n5. Work on it, using topic branches is highly recommended.\n\n## Testing ##\n\nTesting is important\n\n\n## How To Get Your Pull Request Accepted ##\n\n1. Test, test, test. Make sure your new code doesn't break the existing tests\n2. If you add new code, you must add tests.\n3. `gofmt` your code\n5. Atomic pull requests - solve one issue per pull request. Some issues may break down to atomic tasks. In those cases, it's OK to solve them partially.\n\n\n## Git Workflow ##\n\nThe master branch is considered to be the \"canonical\" branch. There is no develop branch. The author prefers use of topic branches. The workflow can best be described by the [Github Flow](https://guides.github.com/introduction/flow/). Please try to keep to this flow.\n\n# Development #\n\n## How the `Tensor` Libs Are Developed ##\n\n\n## Debugging ##\n\nWhilst the author encourages the use of [Delve](https://github.com/derekparker/delve), it may often be easier to log the trace using the debug loggers. Gorgonia comes with a debug build tag precisely to help with that. To build debug builds, simply do this:\n\n```go\ngo build -tags='debug' . \n```\n\nThe debug tag enables various tracing options, available in `debug.go`. There are several debug constants that are used:\n\n* `compileDev`       \n* `shapeInferenceDev`\n* `typeSystemDev`    \n* `symdiffDev`       \n* `autodiffDev`      \n* `machineDev`       \n* `stabilizationDev` \n* `solverDev`\n* `cudaDev`        \n\nThese are the bools that you need to set in order to get a trace. If for example, you think there is something wrong with the type system, simply set `typeSystemDev` to `true` and then insert `typeSysLogf` into wherever you want to trace. \n\n"
        },
        {
          "name": "CONTRIBUTORS.md",
          "type": "blob",
          "size": 1.5908203125,
          "content": "# Contributors 1 #\n\nThe following contributors have contributed in ways that required knowing how Gorgonia works - essentially forming the \"backbone\" of expertise\n\n* Xuanyi Chew (@chewxy) - initial package\n* Ethan Lin (@lynic) - convolution bug fixes\n* Joe Kabaka (@kabaka0) - masked array functionality\n* Jorge Landivar (@docmerlin) - tests, bug finding and documentation\n* Dan Kortschak (@kortschak) - Gonum Graphs\n* Gareth Seneque (@jokebroker) - Release Czar\n* Matoko Ito (@ynqa) - tests, documentation and bug finding\n* Naseer Dari (@ndari) - errors and error handling\n* Olivier Wulveryck (@owulveryck) - Gonnx and more\n* Pascal Masschelier (@pa-m) - Max op\n* Siquus (@siquus) - TensorDot, contraction\n* Tan Chen (@tantanchen) - Bug finding and documentation\n* Geoff Sindel (@durp) - gomod\n* Ben Leitner (@bdleitner)\n* Nabil Servais (@blackrez)\n* Yasuhiro Matsumoto (@mattn)\n\n\n# Contributors 2 #\n\nThe following contributors have contributed to the quality of the project\n\n* Andrew Murray | @radarhere\n* Ankit Raj | @aj07\n* Bart Fokker | @barthr\n* Bradley J Kemp | @bradleyjkemp\n* David Soller | @3ygun\n* Davor Kapsa | @dvrkps\n* Ferhat Elmas | @ferhatelmas\n* Fransesc Campoy | @campoy\n* Harald Nordgred | @HaraldNordgren\n* Ivan Fraixedes | @ifraixedes\n* James Michael DuPont | @h4ck3rm1k3\n* Vadim | @trigun117\n* Yuanlin Lian | @alienchow\n* Mathieu Ronse (Logo) | @WeAreBears\n* Vicente Olmedo | @volmedo\n* Ilya Kaznacheev | @ilyakaznacheev\n* Abhimithra Karthikeya | @freakomonk\n* @bezineb5\n* @stock1218\n\nFor more contributors, check out the [github contributors page](https://github.com/gorgonia/gorgonia/graphs/contributors)\n"
        },
        {
          "name": "DEVNOTES.md",
          "type": "blob",
          "size": 2.0791015625,
          "content": "# Cgo #\n\nCgo is used pretty heavily in Gorgonia. Here are some Cgo guidelines that I've come up with and used over the years:\n\n1. Use astyle to fmt your C code. This is the command to use: `astyle --style=google -lineend=linux --indent=tab --indent-switches --align-pointer=type --align-reference=name --delete-empty-lines`. Yes, the choices are  a bit weird, but it makes C more like Go code, which is readable af.\n2. When passing Go slices to a C function, pass a splatted [fat pointer](http://www.drdobbs.com/architecture-and-design/cs-biggest-mistake/228701625). What I mean by this is to do something like this (cap is optional but recommended):\n\n\t```c\n\tvoid foo(double* sliceF64, int len, int cap) {\n\n\t}\n\t```\n3. Brackets are your friends. It's tempting to write this:\n\t```c\n\tif (foo)\n\t\tbar()\n\t```.\n\tDon't. Write shis instead:\n\t```c\n\tif (foo) {\n\t\tbar()\n\t}\n\t```\n\n# Go Assembly #\n\nWhen writing Go Assembly, use [asmfmt](https://github.com/klauspost/asmfmt)\n\n\n# General Patterns #\n\nThis section describes the general patterns preferred in this library.\n\n## API ##\n\nThe API of this library exposes functions to users of this library. API functions and methods should return `error` whenever an error may occur so the user of the library will be able to handle it on their own.\n\n## Kernels ##\n\nFunctions that return exactly one thing (i.e. no errors) are called *kernels*. It is preferable to write your API functions so they conform to this pattern:\n\n\n```\nfunc Foo(x, y, z T) (retVal T, err error) {\n\t// checks for errors\n\tif err := check(x, y, z); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// from this point on, there should be no errors returned.\n\n\t// do thing\n\tretVal = doThing(x, y, z)\n\treturn retVal, nil\n}\n```\n\nHere, we see `Foo` being actually composed of two other functions: `check` and `doThing`. `doThing` is considered the kernel.\n\nThe reason for preferring this is that we can reuse the kernel functions when it comes to optimizing code.\n\n**Moral**: where possible, abstract all checking and error returning things into another function. This allows the kernel to be used in tighter loops when it comes to optimization.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0810546875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2019 Gorgonia Authors\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 15.6787109375,
          "content": "![Logo](https://raw.githubusercontent.com/gorgonia/gorgonia/master/media/Logo_horizontal_small.png)\n\n[![GoDoc](https://godoc.org/gorgonia.org/gorgonia?status.svg)](https://godoc.org/gorgonia.org/gorgonia) [![GitHub version](https://badge.fury.io/gh/gorgonia%2Fgorgonia.svg)](https://badge.fury.io/gh/gorgonia%2Fgorgonia) \n![Build and Tests](https://github.com/gorgonia/gorgonia/workflows/Build%20and%20Tests%20on%20Linux/amd64/badge.svg)\n[![codecov](https://codecov.io/gh/gorgonia/gorgonia/branch/master/graph/badge.svg)](https://codecov.io/gh/gorgonia/gorgonia)\n[![Go Report Card](https://goreportcard.com/badge/gorgonia.org/gorgonia)](https://goreportcard.com/report/gorgonia.org/gorgonia) [![unstable](http://badges.github.io/stability-badges/dist/unstable.svg)](http://github.com/badges/stability-badges)\n\n#\n\nGorgonia is a library that helps facilitate machine learning in Go. Write and evaluate mathematical equations involving multidimensional arrays easily. If this sounds like [Theano](http://deeplearning.net/software/theano/) or [TensorFlow](https://www.tensorflow.org/), it's because the idea is quite similar. Specifically, the library is pretty low-level, like Theano, but has higher goals like Tensorflow.\n\nGorgonia:\n\n* Can perform automatic differentiation\n* Can perform symbolic differentiation\n* Can perform gradient descent optimizations\n* Can perform numerical stabilization\n* Provides a number of convenience functions to help create neural networks\n* Is fairly quick (comparable to Theano and TensorFlow speed)\n* Supports CUDA/GPGPU computation (OpenCL not yet supported, send a pull request)\n* Will support distributed computing\n\n# Goals #\n\nThe primary goal for Gorgonia is to be a *highly performant* machine learning/graph computation-based library that can scale across multiple machines. It should bring the appeal of Go (simple compilation and deployment process) to the ML world. It's a long way from there currently, however, the baby steps are already there.\n\nThe secondary goal for Gorgonia is to provide a platform for the exploration of non-standard deep-learning and neural network-related things. This includes things like neo-hebbian learning, corner-cutting algorithms, evolutionary algorithms, and the like.\n\n# Why Use Gorgonia? #\n\nThe main reason to use Gorgonia is developer comfort. If you're using a Go stack extensively, now you have access to the ability to create production-ready machine learning systems in an environment that you are already familiar with and comfortable with.\n\nML/AI at large is usually split into two stages: the experimental stage where one builds various models, tests, and retests; and the deployed state where a model after being tested and played with, is deployed. This necessitates different roles like data scientist and data engineer.\n\nTypically the two phases have different tools: Python ([PyTorch](http://pytorch.org/), etc) is commonly used for the experimental stage, and then the model is rewritten in some more performant language like C++ (using [dlib](http://dlib.net/ml.html), [mlpack](http://mlpack.org) etc). Of course, nowadays the gap is closing and people frequently share the tools between them. Tensorflow is one such tool that bridges the gap.\n\nGorgonia aims to do the same but for the Go environment. Gorgonia is currently fairly performant - its speeds are comparable to PyTorch's and Tensorflow's  CPU implementations. GPU implementations are a bit finicky to compare due to the heavy CGO tax, but rest assured that this is an area of active improvement.\n\n# Getting started\n\n## Installation #\n\nThe package is go-gettable: `go get -u gorgonia.org/gorgonia`.\n\nGorgonia is compatible with Go modules.\n\n## Documentation\n\nUp-to-date documentation, references, and tutorials are present on the official Gorgonia website at [https://gorgonia.org](https://gorgonia.org).\n\n## Keeping Updated\n\nGorgonia's project has a [Slack channel on gopherslack](https://gophers.slack.com/messages/gorgonia/), as well as a [Twitter account](https://twitter.com/gorgoniaML). Official updates and announcements will be posted to those two sites.\n\n## Usage\n\nGorgonia works by creating a computation graph and then executing it. Think of it as a programming language, but is limited to mathematical functions, and has no branching capability (no if/then or loops). In fact, this is the dominant paradigm that the user should be used to thinking about. The computation graph is an [AST](http://en.wikipedia.org/wiki/Abstract_syntax_tree).\n\nMicrosoft's [CNTK](https://github.com/Microsoft/CNTK), with its BrainScript, is perhaps the best at exemplifying the idea that building a computation graph and running the computation graphs are different things and that the user should be in different modes of thought when going about them.\n\nWhilst Gorgonia's implementation doesn't enforce the separation of thought as far as CNTK's BrainScript does, the syntax does help a little bit.\n\nHere's an example - say you want to define a math expression `z = x + y`. Here's how you'd do it:\n\n[embedmd]:# (example_basic_test.go)\n```Go\npackage gorgonia_test\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\n\t. \"gorgonia.org/gorgonia\"\n)\n\n// Basic example of representing mathematical equations as graphs.\n//\n// In this example, we want to represent the following equation\n//\t\tz = x + y\nfunc Example_basic() {\n\tg := NewGraph()\n\n\tvar x, y, z *Node\n\tvar err error\n\n\t// define the expression\n\tx = NewScalar(g, Float64, WithName(\"x\"))\n\ty = NewScalar(g, Float64, WithName(\"y\"))\n\tif z, err = Add(x, y); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// create a VM to run the program on\n\tmachine := NewTapeMachine(g)\n\tdefer machine.Close()\n\n\t// set initial values then run\n\tLet(x, 2.0)\n\tLet(y, 2.5)\n\tif err = machine.RunAll(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tfmt.Printf(\"%v\", z.Value())\n\t// Output: 4.5\n}\n```\n\nYou might note that it's a little more verbose than other packages of similar nature. For example, instead of compiling to a callable function, Gorgonia specifically compiles into a `*program` which requires a `*TapeMachine` to run. It also requires manual a `Let(...)` call.\n\nThe author would like to contend that this is a Good Thing - to shift one's thinking to machine-based thinking. It helps a lot in figuring out where things might go wrong.\n\nAdditionally, there is no support for branching - that is to say, there are no conditionals (if/else) or loops. The aim is not to build a Turing-complete computer.\n\n---\nMore examples are present in the `example` subfolder of the project, and step-by-step tutorials are present on the [main website](https://gorgonia.org/tutorials/)\n\n## Using CUDA ##\n\nGorgonia comes with CUDA support out of the box.\nPlease see the reference documentation about how cuda works on [the Gorgonia.org](https://gorgonia.org/reference/cuda/) website, or jump to the [tutorial](https://gorgonia.org/tutorials/mnist-cuda/).\n\n# About Gorgonia's development process\n\n## Versioning ##\n\nWe use [semver 2.0.0](http://semver.org/) for our versioning. Before 1.0, Gorgonia's APIs are expected to change quite a bit. API is defined by the exported functions, variables, and methods. For the developers' sanity, there are minor differences to SemVer that we will apply before version 1.0. They are enumerated below:\n\n* The MINOR number will be incremented every time there is a deleterious break in API. This means any deletion or any change in function signature or interface methods will lead to a change in the MINOR number.\n* Additive changes will NOT change the MINOR version number before version 1.0. This means that if new functionality were added that does not break the way you use Gorgonia, there would not be an increment in the MINOR version. There will be an increment in the PATCH version.\n\n### API Stability #\nGorgonia's API is as of right now, not considered stable. It will be stable from version 1.0 forward.\n\n\n## Go Version Support ##\n\nGorgonia supports 2 versions below the Master branch of Go. This means Gorgonia will support the current released version of Go, and up to 4 previous versions - providing something doesn't break. Where possible a shim will be provided (for things like new `sort` APIs or `math/bits` which came out in Go 1.9).\n\nThe current version of Go is 1.13.1. The earliest version Gorgonia supports is Go 1.11.x but Gonum supports only 1.12+. Therefore, the minimum Go version to run the master branch is Go > 1.12.\n\n## Hardware and OS supported ##\n\nGorgonia runs on :\n- linux/AMD64\n- linux/ARM7\n- linux/ARM64\n- win32/AMD64\n- darwin/AMD64\n- freeBSD/AMD64\n\nIf you have tested Gorgonia on other platforms, please update this list.\n\n## Hardware acceleration\n\nGorgonia uses some pure assembler instructions to accelerate some mathematical operations. Unfortunately, only amd64 is supported.\n\n\n# Contributing #\n\nObviously, since you are most probably reading this on Github, Github will form the major part of the workflow for contributing to this package.\n\nSee also: [CONTRIBUTING.md](CONTRIBUTING.md)\n\n\n## Contributors and Significant Contributors ##\nAll contributions are welcome. However, there is a new class of contributors, called Significant Contributors.\n\nA Significant Contributor has shown *a deep understanding* of how the library works and/or its environs.  Here are examples of what constitutes a Significant Contribution:\n\n* Wrote significant amounts of documentation on **why**/the mechanics of particular functions/methods and how the different parts affect one another\n* Wrote code and tests around the more intricately connected parts of Gorgonia\n* Wrote code and tests, and had at least 5 pull requests accepted\n* Provided expert analysis on parts of the package (for example, you may be a floating point operations expert who optimized one function)\n* Answered at least 10 support questions.\n\nThe significant Contributors list will be updated once a month (if anyone even uses Gorgonia that is).\n\n# How To Get Support #\nThe best way of support right now is to open a [ticket on Github](https://github.com/gorgonia/gorgonia/issues/new).\n\n# Frequently Asked Questions #\n\n### Why are there seemingly random `runtime.GC()` calls in the tests? ###\n\nThe answer to this is simple - the design of the package uses CUDA in a particular way: specifically, a CUDA device and context are tied to a `VM`, instead of at the package level. This means for every `VM` created, a different CUDA context is created per device per `VM`. This way all the operations will play nicely with other applications that may be using CUDA (this needs to be stress-tested, however).\n\nThe CUDA contexts are only destroyed when the `VM` gets garbage collected (with the help of a finalizer function). In the tests, about 100 `VM`s get created, and garbage collection for the most part can be considered random. This leads to cases where the GPU runs out of memory as there are too many contexts being used.\n\nTherefore at the end of any tests that may use GPU, a `runtime.GC()` call is made to force garbage collection, freeing GPU memories.\n\nIn production, one is unlikely to start that many `VM`s, therefore it's not a problem. If there is, open a ticket on GitHub, and we'll look into adding a `Finish()` method for the `VM`s.\n\n\n# Licence #\n\nGorgonia is licensed under a variant of Apache 2.0. It's the same as the Apache 2.0 Licence, except not being able to commercially profit directly from the package unless you're a Significant Contributor (for example, providing commercial support for the package). It's perfectly fine to profit directly from a derivative of Gorgonia (for example, if you use Gorgonia as a library in your product)\n\nEveryone is still allowed to use Gorgonia for commercial purposes (for example: using it in software for your business).\n\n## Dependencies ##\n\nThere are very few dependencies that Gorgonia uses - and they're all pretty stable, so as of now there isn't a need for vendoring tools. These are the list of external packages that Gorgonia calls, ranked in order of reliance that this package has (sub-packages are omitted):\n\n|Package|Used For|Vitality|Notes|Licence|\n|-------|--------|--------|-----|-------|\n|[gonum/graph](https://github.com/gonum/gonum/tree/master/graph)| Sorting `*ExprGraph`| Vital. Removal means Gorgonia will not work | Development of Gorgonia is committed to keeping up with the most updated version|[gonum license](https://github.com/gonum/license) (MIT/BSD-like)|\n|[gonum/blas](https://github.com/gonum/gonum/tree/master/blas)|Tensor subpackage linear algebra operations|Vital. Removal means Gorgonial will not work|Development of Gorgonia is committed to keeping up with the most updated version|[gonum license](https://github.com/gonum/license) (MIT/BSD-like)|\n|[cu](https://gorgonia.org/cu)| CUDA drivers | Needed for CUDA operations | Same maintainer as Gorgonia | MIT/BSD-like|\n|[math32](https://github.com/chewxy/math32)|`float32` operations|Can be replaced by `float32(math.XXX(float64(x)))`|Same maintainer as Gorgonia, same API as the built-in `math` package|MIT/BSD-like|\n|[hm](https://github.com/chewxy/hm)|Type system for Gorgonia|Gorgonia's graphs are pretty tightly coupled with the type system | Same maintainer as Gorgonia | MIT/BSD-like|\n|[vecf64](https://gorgonia.org/vecf64)| optimized `[]float64` operations | Can be generated in the `tensor/genlib` package. However, plenty of optimizations have been made/will be made | Same maintainer as Gorgonia | MIT/BSD-like|\n|[vecf32](https://gorgonia.org/vecf32)| optimized `[]float32` operations | Can be generated in the `tensor/genlib` package. However, plenty of optimizations have been made/will be made | Same maintainer as Gorgonia | MIT/BSD-like|\n|[set](https://github.com/xtgo/set)|Various set operations|Can be easily replaced|Stable API for the past 1 year|[set licence](https://github.com/xtgo/set/blob/master/LICENSE) (MIT/BSD-like)|\n|[gographviz](https://github.com/awalterschulze/gographviz)|Used for printing graphs|Graph printing is only vital to debugging. Gorgonia can survive without, but with a major (but arguably nonvital) feature loss|Last update 12th April 2017|[gographviz license](https://github.com/awalterschulze/gographviz/blob/master/LICENSE) (Apache 2.0)|\n|[rng](https://github.com/leesper/go_rng)|Used to implement helper functions to generate initial weights|Can be replaced fairly easily. Gorgonia can do without the convenience functions too||[rng license](https://github.com/leesper/go_rng/blob/master/LICENSE) (Apache 2.0)|\n|[errors](https://github.com/pkg/errors)|Error wrapping|Gorgonia won't die without it. In fact Gorgonia has also used [goerrors/errors](https://github.com/go-errors/errors) in the past.|Stable API for the past 6 months|[errors licence](https://github.com/pkg/errors/blob/master/LICENSE) (MIT/BSD-like)|\n|[gonum/mat](http://github.com/gonum/gonum)|Compatibility between `Tensor` and Gonum's Matrix|Development of Gorgonia is committed to keeping up with the most updated version||[gonum license](https://github.com/gonum/license) (MIT/BSD-like)|\n|[testify/assert](https://github.com/stretchr/testify)|Testing|Can do without but will be a massive pain in the ass to test||[testify license](https://github.com/stretchr/testify/blob/master/LICENSE) (MIT/BSD-like)|\n\n\n## Various Other Copyright Notices ##\n\nThese are the packages and libraries that inspired and were adapted from in the process of writing Gorgonia (the Go packages that were used were already declared above):\n\n| Source | How it's Used | Licence |\n|------|---|-------|\n| Numpy  | Inspired large portions. Directly adapted algorithms for a few methods (explicitly labeled in the docs) | MIT/BSD-like. [Numpy Licence](https://github.com/numpy/numpy/blob/master/LICENSE.txt) |\n| Theano | Inspired large portions. (Unsure: number of directly adapted algorithms) | MIT/BSD-like [Theano's license](http://deeplearning.net/software/theano/LICENSE.html) |\n| Caffe | `im2col` and `col2im` directly taken from Caffe. Convolution algorithms inspired by the original Caffee methods | [Caffe Licence](https://github.com/BVLC/caffe/blob/master/LICENSE)\n"
        },
        {
          "name": "analysis.go",
          "type": "blob",
          "size": 7.66796875,
          "content": "package gorgonia\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n)\n\n// dataflow analysis\n\ntype dataflow struct {\n\tuniques map[uint32]*Node\n\n\treplacements map[*Node]*Node\n\tintervals    map[*Node]*interval\n\n\t// tracks the special nodes' children and parents\n\tdevTransChildren map[*Node]Nodes\n\tdevTransRepl     map[*Node]*Node\n}\n\nfunc newdataflow() *dataflow {\n\tdf := new(dataflow)\n\tdf.uniques = make(map[uint32]*Node)\n\tdf.devTransChildren = make(map[*Node]Nodes)\n\tdf.devTransRepl = make(map[*Node]*Node)\n\treturn df\n}\n\n// equivalent to the value numbering algorithm\n// it returns true if it is unique\nfunc (df *dataflow) vn(n *Node) (retVal *Node, unique bool) {\n\tcompileLogf(\"Value numbering\")\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tnode, ok := df.uniques[n.Hashcode()]\n\n\tif ok {\n\t\treturn node, false\n\t}\n\n\tcompileLogf(\"adding a new unique\")\n\t// otherwise, add it to uniques, and then return itself\n\tdf.uniques[n.Hashcode()] = n\n\n\treturn n, true\n}\n\n// analyzeDevice records which node is supposed to be executed on which device.\n//\n// Currently it will only use Device 0. In the future, we can be smart about which device to use\nfunc (df *dataflow) analyzeDevice(n *Node) {\n\tswitch n.op.(type) {\n\tcase CUDADoer:\n\t\tif n.dataOn == CPU {\n\t\t\tn.dataOn = Device(0)\n\t\t}\n\tcase CLDoer:\n\t\tif n.dataOn == CPU {\n\t\t\tn.dataOn = Device(0)\n\t\t}\n\tdefault:\n\t\tn.dataOn = CPU\n\t}\n}\n\n// replaceWithSelf fills the replacement map with itself. This is the method used in the lispMachine only, as it skips value numbering\nfunc (df *dataflow) replaceWithSelf(sorted Nodes) {\n\tdf.replacements = make(map[*Node]*Node)\n\tfor _, n := range sorted {\n\t\tdf.replacements[n] = n\n\t\tdf.analyzeDevice(n) // Device Targeting\n\t}\n}\n\n// fixIntervalDevices is used only by the lispMachine. It fixes the intervals to have the correct devices\nfunc (df *dataflow) fixIntervalDevices(sorted Nodes) {\n\tfor _, n := range sorted {\n\t\tdf.intervals[n].result.device = n.dataOn\n\t}\n}\n\nfunc analyze(g *ExprGraph, sorted Nodes) *dataflow {\n\tcompileLogf(\"Performing dataflow analysis\")\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tcompileLogf(\"Finding unique leaves\")\n\tdf := newdataflow()\n\tfor _, n := range g.leaves {\n\t\tdf.uniques[n.Hashcode()] = n\n\t}\n\n\t// compileLogf(\"Common subexpression elimination\")\n\t// compileLogf(\"analyzing devices\")\n\treplacements := make(map[*Node]*Node)\n\tfor _, n := range sorted {\n\t\tr, _ := df.vn(n)\n\t\treplacements[n] = r // CSE\n\t\tdf.analyzeDevice(n) // Device targeting\n\t}\n\tdf.replacements = replacements\n\tcompileLogf(\"replacements: %-p\", FmtNodeMap(replacements))\n\n\t// TODO\n\t// constant propagation\n\t/*\n\t\tfor _, node := range g.nodes {\n\t\t\tn := node.(*Node)\n\t\t\tif len(n.Children) > 0 {\n\t\t\t\tallConst := true\n\t\t\t\tfor _, child := range n.Children {\n\t\t\t\t\tif _, ok := child.Op.(constant); !ok {\n\t\t\t\t\t\tallConst = false\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t*/\n\treturn df\n}\n\nfunc newDevTransNode(read, write *Node, from, to Device) *Node {\n\top := devTrans{from, to, write}\n\tn := borrowNode()\n\tn.id = -1\n\tn.op = op\n\tn.shape = read.shape.Clone()\n\tn.t = read.t\n\tn.isStmt = true\n\tn.children = Nodes{read}\n\treturn n\n}\n\nfunc (df *dataflow) insertDeviceInstr(sorted Nodes) Nodes {\n\tcompileLogf(\"Inserting Device Transport Instructions\")\n\tenterLogScope()\n\tdefer leaveLogScope()\n\t// input -> output\n\tfor i := 0; i < len(sorted); i++ {\n\t\tnode := sorted[i]\n\t\tn := df.replacements[node]\n\t\tdev := n.dataOn\n\n\t\tcompileLogf(\"Working on %v. Replacement %v. Device %v\", node, n, dev)\n\t\tvar incr int\n\t\tvar useReplacement bool\n\t\treplacementChildren := make(Nodes, len(n.children))\n\t\tenterLogScope()\n\t\tfor j, child := range n.children {\n\t\t\tc := df.replacements[child]\n\t\t\tchildDev := c.dataOn\n\n\t\t\tcompileLogf(\"Working on child :%v. Device: %v, Parent Device %v\", c, childDev, dev)\n\t\t\tif childDev != dev {\n\t\t\t\tuseReplacement = true\n\t\t\t\tif repl, ok := df.devTransRepl[c]; ok {\n\t\t\t\t\treplacementChildren[j] = repl\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\ttransport := newDevTransNode(c, n, childDev, dev)\n\t\t\t\tsorted = append(sorted, nil)\n\t\t\t\tcopy(sorted[i+1:], sorted[i:])\n\t\t\t\tsorted[i] = transport\n\t\t\t\tincr++\n\t\t\t\tcompileLogf(\"Inserted %v\", transport)\n\n\t\t\t\t// other stateful stuff\n\t\t\t\tdf.devTransRepl[c] = transport\n\t\t\t\tdf.replacements[transport] = transport\n\t\t\t\treplacementChildren[j] = transport\n\t\t\t} else {\n\t\t\t\treplacementChildren[j] = child\n\t\t\t}\n\t\t}\n\t\tleaveLogScope()\n\n\t\tif useReplacement {\n\t\t\tdf.devTransChildren[n] = replacementChildren\n\t\t}\n\n\t\ti += incr\n\t}\n\treturn sorted\n}\n\n/*\n\tNotes on handling the live set:\n\n\t1. We load all the SSAs listed in the block's LiveIn\n\t2. Then we load all the SSAs used as input in this block Phi nodes\n\t\t- The reason for this is so that those SSAs can have intervals created\n\t\t  that are live in this block (well, they are kinda live)\n\t3. These input SSAs are temporary only, because a path-dependent liveset will be calculated below\n\n\tConsider a CFG that looks like this:\n\n                           BLOCK 1           BLOCK 3\n                           +-------+        +-------+\n                     +---->| x = 1 +------->| y = 3 +----------------+\n        BLOCK 0      |     +-------+        | use x |                v  BLOCK 4\n       +-------+     |                      +-------+              +-------------+\n       |       |+----+                                             | x = ϕ(1, 2) |\n       +-------+     |     BLOCK 2                                 +-------------+\n                     |     +-------+                                 ^\n                     +---->| x = 2 +---------------------------------+\n                           +-------+\n\n\t`x = 1` needs to be live in BLOCK 1, BLOCK 3 and BLOCK 4\n\t`x = 2` needs to be live in BLOCK 2 and BLOCK 4.\n\n\tThe solution: in BLOCK 4, load `x = 1` and `x = 2` so they can be considered live in Block 4.\n\n\tThe interval building process comes to BLOCK 3 next. It considers the SSAs that are live in BLOCK 4.\n\tIf `x = 2` is live in BLOCK 4, it's Bad News with capital letters (see comment below).\n\n\tThe solution: remove the InputSSAs of the Phi nodes when we're leaving this block.\n*/\n// TODO: rephrase above to fit this package's function.\n// It's like the above, but without basic blocks, phi nodes, etc, making it a LOT simpler\nfunc (df *dataflow) buildIntervals(sorted Nodes) {\n\tcompileLogf(\"Building intervals for %v\", sorted)\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tintervals := make(map[*Node]*interval)\n\n\tvar g *ExprGraph\n\tfor _, n := range sorted {\n\t\tif g == nil && n.g != nil {\n\t\t\tg = n.g\n\t\t}\n\n\t\tintervals[n] = newInterval()\n\t}\n\n\tinstructions := len(sorted)\n\tfor i := len(sorted) - 1; i >= 0; i-- {\n\t\tn := sorted[i]\n\t\tinstrNum := i\n\t\tnInter := intervals[n]\n\t\tcompileLogf(\"n %v | %v\", n, nInter)\n\n\t\t// inputs and constants will be live the entire program\n\t\tif n.isInput() || n.isConstant() {\n\t\t\tnInter.addRange(instrNum, instructions)\n\t\t\trepl, ok := df.devTransRepl[n]\n\t\t\tif ok {\n\t\t\t\tinterv, ok := intervals[repl]\n\t\t\t\tif ok {\n\t\t\t\t\tinterv.addRange(instrNum, instructions)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tcontinue\n\t\t}\n\t\tnInter.addRange(instrNum, instrNum)\n\n\t\t// check for special cases requiring copying from device to device\n\n\t\tvar children Nodes\n\t\tvar ok bool\n\t\tif children, ok = df.devTransChildren[n]; !ok {\n\t\t\tchildren = n.children\n\t\t}\n\n\t\tfor _, child := range children {\n\t\t\tiv, ok := intervals[child]\n\t\t\tif !ok {\n\t\t\t\t// do something\n\t\t\t\t// parents := g.to[n]\n\t\t\t\t// for i, from := range parents {\n\t\t\t\t// \tioutil.WriteFile(fmt.Sprintf(\"n_%d.dot\", i), []byte(from.ToDot()), 0644)\n\t\t\t\t// }\n\t\t\t}\n\t\t\tiv.addUsePositions(instrNum)\n\t\t\t// iv.setTo(instrNum)\n\t\t}\n\t\t// assume all derivations of input\n\t\tif len(n.derivOf) > 0 {\n\t\t\tfor _, d := range n.derivOf {\n\t\t\t\tif d.isInput() {\n\t\t\t\t\tnInter.addUsePositions(instructions)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tfor _, iv := range intervals {\n\t\tiv.fix()\n\t}\n\n\tvar buf bytes.Buffer\n\tfor k, v := range intervals {\n\t\tfmt.Fprintf(&buf, \"%v: %v\\n\", k, v)\n\t}\n\tcompileLogf(\"Intervals: %v\", buf.String())\n\n\tdf.intervals = intervals\n\treturn\n}\n"
        },
        {
          "name": "analysis_test.go",
          "type": "blob",
          "size": 2.7548828125,
          "content": "package gorgonia\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestBuildIntervals(t *testing.T) {\n\tassert := assert.New(t)\n\tvar err error\n\tg, x, y, z := simpleVecEqn()\n\tvar readVal Value\n\tr := Read(z, &readVal)\n\n\tz2 := Must(Square(z))\n\tz2y := Must(HadamardProd(z2, y))\n\tc := NewConstant(1.0, WithName(\"FOOO\")) // const\n\tg.addToAll(c)                           // this is a hack because there is no good way to get a constant into a graph since In() won't work on constatns\n\n\t// because sorting is unstable, we need to test many times\n\tvar sorted Nodes\n\tvar intervals map[*Node]*interval\n\n\tfor i := 0; i < 100; i++ {\n\t\tif sorted, err = Sort(g); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\treverseNodes(sorted)\n\n\t\tdf := analyze(g, sorted)\n\t\tdf.buildIntervals(sorted)\n\t\tdf.debugIntervals(sorted) // prints intervals on debug mode\n\t\tintervals = df.intervals\n\n\t\t// inputs are live until the last instruction\n\t\tassert.Equal(len(intervals), intervals[x].end, \"%v\", len(sorted))\n\t\tif intervals[x].start != 1 && intervals[x].start != 0 {\n\t\t\tt.Errorf(\"x starts at 1 or 0 (depending on how the sort allocates it)\")\n\t\t}\n\n\t\tassert.Equal(len(g.AllNodes()), intervals[y].end)\n\t\tif intervals[y].start != 1 && intervals[y].start != 0 {\n\t\t\tt.Errorf(\"y starts at 1 or 0 (depending on how the sort allocates it)\")\n\t\t}\n\n\t\t// constants should be live until the last instruction\n\t\tassert.Equal(len(intervals), intervals[c].end, \"%v\", len(sorted))\n\n\t\tassert.Equal(2, intervals[z].start)\n\t\tif intervals[z2].start > intervals[z].end {\n\t\t\tt.Error(\"z2 should start before z ends\")\n\t\t}\n\n\t\tassert.Equal(intervals[r].start, intervals[r].end)\n\t\tif intervals[r].start < intervals[z].start {\n\t\t\tt.Error(\"z should have an earlier start than r\")\n\t\t}\n\t\tif intervals[r].start > intervals[z].end {\n\t\t\tt.Error(\"z should end before r starts (or at the same as r start\")\n\t\t}\n\n\t\tif intervals[z2].end <= intervals[z2].start {\n\t\t\tt.Error(\"Given that z2y uses z2, the intervals should not end at the same as its start\")\n\t\t}\n\t\tif intervals[z2].start < intervals[z].start {\n\t\t\tt.Error(\"z should have an earlier start than z2\")\n\t\t}\n\t\tif intervals[z2].start > intervals[z].end {\n\t\t\tt.Error(\"z should end before r starts (or at the same as z2 start\")\n\t\t}\n\n\t\tassert.Equal(intervals[z2y].start, intervals[z2y].end)\n\t\tif intervals[z2y].start < intervals[z2].start {\n\t\t\tt.Error(\"z2 should have an earlier start than z2y\")\n\t\t}\n\t\tif intervals[z2y].start > intervals[z2].end {\n\t\t\tt.Error(\"z2 should end before r starts (or at the same as z2y start\")\n\t\t}\n\n\t\tif t.Failed() {\n\t\t\tbreak\n\t\t}\n\n\t}\n\n\t// visual reminder\n\tvar buf bytes.Buffer\n\tbuf.WriteString(\"VISUAL REMINDER OF INTERVALS\\n\")\n\tsorted.reverse()\n\tfor i, n := range sorted {\n\t\tin := intervals[n]\n\t\tfmt.Fprintf(&buf, \"%d\\t%v\\tfrom %v to %v \\n\", i, n, in.start, in.end)\n\n\t}\n\tt.Log(buf.String())\n}\n"
        },
        {
          "name": "api_gen.go",
          "type": "blob",
          "size": 9.123046875,
          "content": "package gorgonia\n\n// Code generated by genapi, which is a API generation tool for Gorgonia. DO NOT EDIT.\n\n// Abs performs a pointwise abs.\nfunc Abs(a *Node) (*Node, error) { return unaryOpNode(newElemUnaryOp(absOpType, a), a) }\n\n// Sign performs a pointwise sign.\nfunc Sign(a *Node) (*Node, error) { return unaryOpNode(newElemUnaryOp(signOpType, a), a) }\n\n// Ceil performs a pointwise ceil.\nfunc Ceil(a *Node) (*Node, error) { return unaryOpNode(newElemUnaryOp(ceilOpType, a), a) }\n\n// Floor performs a pointwise floor.\nfunc Floor(a *Node) (*Node, error) { return unaryOpNode(newElemUnaryOp(floorOpType, a), a) }\n\n// Sin performs a pointwise sin.\nfunc Sin(a *Node) (*Node, error) { return unaryOpNode(newElemUnaryOp(sinOpType, a), a) }\n\n// Cos performs a pointwise cos.\nfunc Cos(a *Node) (*Node, error) { return unaryOpNode(newElemUnaryOp(cosOpType, a), a) }\n\n// Exp performs a pointwise exp.\nfunc Exp(a *Node) (*Node, error) { return unaryOpNode(newElemUnaryOp(expOpType, a), a) }\n\n// Log performs a pointwise log.\nfunc Log(a *Node) (*Node, error) { return unaryOpNode(newElemUnaryOp(lnOpType, a), a) }\n\n// Log2 performs a pointwise log2.\nfunc Log2(a *Node) (*Node, error) { return unaryOpNode(newElemUnaryOp(log2OpType, a), a) }\n\n// Neg performs a pointwise neg.\nfunc Neg(a *Node) (*Node, error) { return unaryOpNode(newElemUnaryOp(negOpType, a), a) }\n\n// Square performs a pointwise square.\nfunc Square(a *Node) (*Node, error) { return unaryOpNode(newElemUnaryOp(squareOpType, a), a) }\n\n// Sqrt performs a pointwise sqrt.\nfunc Sqrt(a *Node) (*Node, error) { return unaryOpNode(newElemUnaryOp(sqrtOpType, a), a) }\n\n// Inverse performs a pointwise inverse.\nfunc Inverse(a *Node) (*Node, error) { return unaryOpNode(newElemUnaryOp(inverseOpType, a), a) }\n\n// InverseSqrt performs a pointwise inversesqrt.\nfunc InverseSqrt(a *Node) (*Node, error) { return unaryOpNode(newElemUnaryOp(inverseSqrtOpType, a), a) }\n\n// Cube performs a pointwise cube.\nfunc Cube(a *Node) (*Node, error) { return unaryOpNode(newElemUnaryOp(cubeOpType, a), a) }\n\n// Tanh performs a pointwise tanh.\nfunc Tanh(a *Node) (*Node, error) { return unaryOpNode(newElemUnaryOp(tanhOpType, a), a) }\n\n// Sigmoid performs a pointwise sigmoid.\nfunc Sigmoid(a *Node) (*Node, error) { return unaryOpNode(newElemUnaryOp(sigmoidOpType, a), a) }\n\n// Log1p performs a pointwise log1p.\nfunc Log1p(a *Node) (*Node, error) { return unaryOpNode(newElemUnaryOp(log1pOpType, a), a) }\n\n// Expm1 performs a pointwise expm1.\nfunc Expm1(a *Node) (*Node, error) { return unaryOpNode(newElemUnaryOp(expm1OpType, a), a) }\n\n// Softplus performs a pointwise softplus.\nfunc Softplus(a *Node) (*Node, error) { return unaryOpNode(newElemUnaryOp(softplusOpType, a), a) }\n\n// Add performs a pointwise add operation.\nfunc Add(a, b *Node) (*Node, error) { return binOpNode(newElemBinOp(addOpType, a, b), a, b) }\n\n// Sub performs a pointwise sub operation.\nfunc Sub(a, b *Node) (*Node, error) { return binOpNode(newElemBinOp(subOpType, a, b), a, b) }\n\n// HadamardProd performs a pointwise hadamardprod operation.\nfunc HadamardProd(a, b *Node) (*Node, error) { return binOpNode(newElemBinOp(mulOpType, a, b), a, b) }\n\n// HadamardDiv performs a pointwise hadamarddiv operation.\nfunc HadamardDiv(a, b *Node) (*Node, error) { return binOpNode(newElemBinOp(divOpType, a, b), a, b) }\n\n// Pow performs a pointwise pow operation.\nfunc Pow(a, b *Node) (*Node, error) { return binOpNode(newElemBinOp(powOpType, a, b), a, b) }\n\n// Lt performs a pointwise lt operation.\n// retSame indicates if the data type of the return value should be the same as the input data type. It defaults to Bool otherwise.\nfunc Lt(a, b *Node, retSame bool) (*Node, error) {\n\top := newElemBinOp(ltOpType, a, b)\n\top.retSame = retSame\n\treturn binOpNode(op, a, b)\n}\n\n// Gt performs a pointwise gt operation.\n// retSame indicates if the data type of the return value should be the same as the input data type. It defaults to Bool otherwise.\nfunc Gt(a, b *Node, retSame bool) (*Node, error) {\n\top := newElemBinOp(gtOpType, a, b)\n\top.retSame = retSame\n\treturn binOpNode(op, a, b)\n}\n\n// Lte performs a pointwise lte operation.\n// retSame indicates if the data type of the return value should be the same as the input data type. It defaults to Bool otherwise.\nfunc Lte(a, b *Node, retSame bool) (*Node, error) {\n\top := newElemBinOp(lteOpType, a, b)\n\top.retSame = retSame\n\treturn binOpNode(op, a, b)\n}\n\n// Gte performs a pointwise gte operation.\n// retSame indicates if the data type of the return value should be the same as the input data type. It defaults to Bool otherwise.\nfunc Gte(a, b *Node, retSame bool) (*Node, error) {\n\top := newElemBinOp(gteOpType, a, b)\n\top.retSame = retSame\n\treturn binOpNode(op, a, b)\n}\n\n// Eq performs a pointwise eq operation.\n// retSame indicates if the data type of the return value should be the same as the input data type. It defaults to Bool otherwise.\nfunc Eq(a, b *Node, retSame bool) (*Node, error) {\n\top := newElemBinOp(eqOpType, a, b)\n\top.retSame = retSame\n\treturn binOpNode(op, a, b)\n}\n\n// Ne performs a pointwise ne operation.\n// retSame indicates if the data type of the return value should be the same as the input data type. It defaults to Bool otherwise.\nfunc Ne(a, b *Node, retSame bool) (*Node, error) {\n\top := newElemBinOp(neOpType, a, b)\n\top.retSame = retSame\n\treturn binOpNode(op, a, b)\n}\n\n//Add performs a add. The operation is precomposed with a broadcast such that the shapes matches before operations commence.\nfunc BroadcastAdd(a, b *Node, leftPattern, rightPattern []byte) (*Node, error) {\n\ta2, b2, err := Broadcast(a, b, NewBroadcastPattern(leftPattern, rightPattern))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn Add(a2, b2)\n}\n\n//Sub performs a sub. The operation is precomposed with a broadcast such that the shapes matches before operations commence.\nfunc BroadcastSub(a, b *Node, leftPattern, rightPattern []byte) (*Node, error) {\n\ta2, b2, err := Broadcast(a, b, NewBroadcastPattern(leftPattern, rightPattern))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn Sub(a2, b2)\n}\n\n//HadamardProd performs a hadamardprod. The operation is precomposed with a broadcast such that the shapes matches before operations commence.\nfunc BroadcastHadamardProd(a, b *Node, leftPattern, rightPattern []byte) (*Node, error) {\n\ta2, b2, err := Broadcast(a, b, NewBroadcastPattern(leftPattern, rightPattern))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn HadamardProd(a2, b2)\n}\n\n//HadamardDiv performs a hadamarddiv. The operation is precomposed with a broadcast such that the shapes matches before operations commence.\nfunc BroadcastHadamardDiv(a, b *Node, leftPattern, rightPattern []byte) (*Node, error) {\n\ta2, b2, err := Broadcast(a, b, NewBroadcastPattern(leftPattern, rightPattern))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn HadamardDiv(a2, b2)\n}\n\n//Pow performs a pow. The operation is precomposed with a broadcast such that the shapes matches before operations commence.\nfunc BroadcastPow(a, b *Node, leftPattern, rightPattern []byte) (*Node, error) {\n\ta2, b2, err := Broadcast(a, b, NewBroadcastPattern(leftPattern, rightPattern))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn Pow(a2, b2)\n}\n\n//Lt performs a lt. The operation is precomposed with a broadcast such that the shapes matches before operations commence.\nfunc BroadcastLt(a, b *Node, retSame bool, leftPattern, rightPattern []byte) (*Node, error) {\n\ta2, b2, err := Broadcast(a, b, NewBroadcastPattern(leftPattern, rightPattern))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn Lt(a2, b2, retSame)\n}\n\n//Gt performs a gt. The operation is precomposed with a broadcast such that the shapes matches before operations commence.\nfunc BroadcastGt(a, b *Node, retSame bool, leftPattern, rightPattern []byte) (*Node, error) {\n\ta2, b2, err := Broadcast(a, b, NewBroadcastPattern(leftPattern, rightPattern))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn Gt(a2, b2, retSame)\n}\n\n//Lte performs a lte. The operation is precomposed with a broadcast such that the shapes matches before operations commence.\nfunc BroadcastLte(a, b *Node, retSame bool, leftPattern, rightPattern []byte) (*Node, error) {\n\ta2, b2, err := Broadcast(a, b, NewBroadcastPattern(leftPattern, rightPattern))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn Lte(a2, b2, retSame)\n}\n\n//Gte performs a gte. The operation is precomposed with a broadcast such that the shapes matches before operations commence.\nfunc BroadcastGte(a, b *Node, retSame bool, leftPattern, rightPattern []byte) (*Node, error) {\n\ta2, b2, err := Broadcast(a, b, NewBroadcastPattern(leftPattern, rightPattern))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn Gte(a2, b2, retSame)\n}\n\n//Eq performs a eq. The operation is precomposed with a broadcast such that the shapes matches before operations commence.\nfunc BroadcastEq(a, b *Node, retSame bool, leftPattern, rightPattern []byte) (*Node, error) {\n\ta2, b2, err := Broadcast(a, b, NewBroadcastPattern(leftPattern, rightPattern))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn Eq(a2, b2, retSame)\n}\n\n//Ne performs a ne. The operation is precomposed with a broadcast such that the shapes matches before operations commence.\nfunc BroadcastNe(a, b *Node, retSame bool, leftPattern, rightPattern []byte) (*Node, error) {\n\ta2, b2, err := Broadcast(a, b, NewBroadcastPattern(leftPattern, rightPattern))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn Ne(a2, b2, retSame)\n}\n"
        },
        {
          "name": "batch.go",
          "type": "blob",
          "size": 0.4365234375,
          "content": "package gorgonia\n\n// Batched interface describes any object that can process batch work\ntype Batched interface {\n\tWorkAvailable() <-chan struct{}\n\tDoWork()\n}\n\n// BatchedBLAS interface describes any object that can process BLAS work in batch\ntype BatchedBLAS interface {\n\tBatched\n\tBLAS\n}\n\n// BatchedDevice is the superset of BatchedBLAS and the batched CUDA workflow.\ntype BatchedDevice interface {\n\tBatched\n\tRetval() interface{}\n\tErrors() error\n}\n"
        },
        {
          "name": "bench_concurrent_training_test.go",
          "type": "blob",
          "size": 0.587890625,
          "content": "package gorgonia_test\n\nimport (\n\t\"io\"\n\t\"runtime\"\n\t\"testing\"\n\n\t\"gorgonia.org/tensor\"\n)\n\nfunc BenchmarkTrainingConcurrent(b *testing.B) {\n\txV, yV, bs := prep()\n\n\tfor i := 0; i < b.N; i++ {\n\t\tconcurrentTraining(xV, yV, bs, 10)\n\t}\n\n\truntime.GC()\n}\n\nfunc BenchmarkTrainingNonConcurrent(b *testing.B) {\n\txV, yV, _ := prep()\n\n\tfor i := 0; i < b.N; i++ {\n\t\tnonConcurrentTraining(xV, yV, 10)\n\t}\n\n\truntime.GC()\n}\n\nfunc BenchmarkTapeMachineExecution(b *testing.B) {\n\tm, c, machine := linregSetup(tensor.Float64)\n\tfor i := 0; i < b.N; i++ {\n\t\tlinregRun(m, c, machine, 100, false)\n\t}\n\tmachine.(io.Closer).Close()\n}\n"
        },
        {
          "name": "bench_typesystem_test.go",
          "type": "blob",
          "size": 0.32421875,
          "content": "package gorgonia\n\nimport \"testing\"\n\nfunc BenchmarkTypeSystem(b *testing.B) {\n\tg := NewGraph()\n\tx := NewTensor(g, Float64, 2, WithName(\"x\"), WithShape(10, 10))\n\ty := NewTensor(g, Float64, 2, WithName(\"y\"), WithShape(10, 10))\n\top := newEBOByType(addOpType, Float64, Float64)\n\tfor i := 0; i < b.N; i++ {\n\t\tinferNodeType(op, x, y)\n\t}\n}\n"
        },
        {
          "name": "benchmark_operations_test.go",
          "type": "blob",
          "size": 0.78125,
          "content": "package gorgonia\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc BenchmarkReshape_Dense(b *testing.B) {\n\tfor _, rst := range reshapeTests {\n\t\tb.Run(rst.testName, func(b *testing.B) {\n\t\t\tg := NewGraph()\n\t\t\ttT := tensor.New(tensor.Of(tensor.Float64), tensor.WithShape(rst.input.Clone()...))\n\t\t\tT := NodeFromAny(g, tT)\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\tT2, err := Reshape(T, rst.to.Clone())\n\t\t\t\tswitch {\n\t\t\t\tcase rst.err && err == nil:\n\t\t\t\t\tb.Fatalf(\"Expected Error when testing %v\", rst)\n\t\t\t\tcase rst.err:\n\t\t\t\t\tcontinue\n\t\t\t\tcase err != nil:\n\t\t\t\t\tb.Fatal(err)\n\t\t\t\tdefault:\n\t\t\t\t\tassert.True(b, rst.output.Eq(T2.Shape()), \"expected both to be the same\")\n\t\t\t\t}\n\t\t\t}\n\t\t\tm := NewTapeMachine(g)\n\t\t\tif err := m.RunAll(); err != nil {\n\t\t\t\tb.Fatal(err)\n\t\t\t}\n\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "bitmap.go",
          "type": "blob",
          "size": 1.7978515625,
          "content": "package gorgonia\n\nimport \"math/bits\"\n\nconst (\n\tbitmapBits = 64\n)\n\n// bitmap is a very simple bitmap. It only supports Set, IsSet and Clear methods. It's mostly used for tracking which element has been set\ntype bitmap struct {\n\tn   []uint64\n\tmax int\n}\n\n// newBitmap creates a new bitmap.\nfunc newBitmap(size int) *bitmap {\n\tq, r := divmod(size, bitmapBits)\n\n\tif r > 0 {\n\t\tq++\n\t}\n\n\treturn &bitmap{\n\t\tn:   make([]uint64, q),\n\t\tmax: size,\n\t}\n}\n\n// Set sets the ith bit of the bit map to 1. It panics if i is greater or equal to the defined max\nfunc (bm *bitmap) Set(i int) {\n\tif i >= bm.max || i < 0 {\n\t\tpanic(\"Index out of range\")\n\t}\n\n\tblock, pos := divmod(i, bitmapBits)\n\tbm.n[block] |= uint64(1) << uint64(pos)\n}\n\n// IsSet returns true if the ith bit is set. It panics if the i is greater or equal to the defined max\nfunc (bm *bitmap) IsSet(i int) bool {\n\tif i >= bm.max || i < 0 {\n\t\tpanic(\"Index out of range\")\n\t}\n\n\tblock, pos := divmod(i, bitmapBits)\n\treturn bm.n[block]>>uint64(pos)&uint64(1) == uint64(1)\n}\n\n// Clear clears the ith bit. It panics if i is greater or equal to the defined max\nfunc (bm *bitmap) Clear(i int) {\n\tif i >= bm.max || i < 0 {\n\t\tpanic(\"Index out of range\")\n\t}\n\n\tblock, pos := divmod(i, bitmapBits)\n\tbm.n[block] &= ^(uint64(1) << uint64(pos))\n}\n\n// BlocksWithZero finds the first block with zeroes in the bit. atleast specifies how many consecutive zeroes need be found\nfunc (bm *bitmap) BlocksWithZero(atleast int) int {\n\tretVal := -1\n\tfor i, b := range bm.n {\n\t\tif bits.OnesCount64(b) != bitmapBits {\n\t\t\t// shortcut:\n\t\t\tif bits.LeadingZeros64(b) > atleast {\n\t\t\t\treturn i\n\t\t\t}\n\n\t\t\tvar consecutive int\n\t\t\tfor j := 0; j < bitmapBits; j++ {\n\t\t\t\tif b>>uint64(j)&uint64(1) == 0 {\n\t\t\t\t\tconsecutive++\n\t\t\t\t} else {\n\t\t\t\t\tconsecutive = 0\n\t\t\t\t}\n\t\t\t\tif consecutive > atleast {\n\t\t\t\t\treturn i\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn retVal\n}\n"
        },
        {
          "name": "bitmap_test.go",
          "type": "blob",
          "size": 2.017578125,
          "content": "package gorgonia\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestBitMap(t *testing.T) {\n\tassert := assert.New(t)\n\tbm := newBitmap(64)\n\tassert.Equal(1, len(bm.n))\n\n\ttrack := uint64(0)\n\tfor i := 0; i < 64; i++ {\n\t\tbm.Set(i)\n\t\ttrack |= uint64(1) << uint64(i)\n\t\tassert.Equal(track, bm.n[0])\n\t\tassert.Equal(true, bm.IsSet(i))\n\t\tif i < 63 {\n\t\t\tassert.Equal(false, bm.IsSet(i+1))\n\t\t} else {\n\t\t\tfails := func() {\n\t\t\t\tbm.IsSet(i + 1)\n\t\t\t}\n\t\t\tassert.Panics(fails)\n\t\t}\n\t}\n\n\tfor i := 0; i < 64; i++ {\n\t\tbm.Clear(i)\n\t\ttrack &= ^(uint64(1) << uint64(i))\n\t\tassert.Equal(track, bm.n[0])\n\t\tassert.Equal(false, bm.IsSet(i))\n\t}\n\n\tbm = newBitmap(124)\n\tassert.Equal(2, len(bm.n))\n\n\ttrack0 := uint64(0)\n\ttrack1 := uint64(0)\n\tfor i := 0; i < 128; i++ {\n\t\tif i < 124 {\n\t\t\tbm.Set(i)\n\t\t} else {\n\t\t\tfails := func() {\n\t\t\t\tbm.Set(i)\n\t\t\t}\n\t\t\tassert.Panics(fails)\n\t\t}\n\t\tif i < 64 {\n\t\t\ttrack0 |= uint64(1) << uint64(i)\n\t\t\tassert.Equal(track0, bm.n[0])\n\t\t\tassert.Equal(true, bm.IsSet(i))\n\t\t} else if i > 123 {\n\t\t\tfails := func() {\n\t\t\t\tbm.IsSet(i)\n\t\t\t}\n\t\t\tassert.Panics(fails)\n\t\t} else {\n\t\t\ttrack1 |= uint64(1) << uint64(i-64)\n\t\t\tassert.Equal(track1, bm.n[1])\n\t\t\tassert.Equal(true, bm.IsSet(i))\n\t\t}\n\n\t\tif i < 123 {\n\t\t\tassert.Equal(false, bm.IsSet(i+1))\n\t\t} else {\n\t\t\tfails := func() {\n\t\t\t\tbm.IsSet(i + 1)\n\t\t\t}\n\t\t\tassert.Panics(fails)\n\t\t}\n\t}\n\n\tfor i := 48; i < 70; i++ {\n\t\tbm.Clear(i)\n\t}\n\n\tfor i := 48; i < 70; i++ {\n\t\tassert.Equal(false, bm.IsSet(i))\n\t}\n\n\tfails := func() {\n\t\tbm.Clear(125)\n\t}\n\tassert.Panics(fails)\n\n\t// idiots section!\n\tbm = newBitmap(3)\n\tfails = func() {\n\t\tbm.Set(-1)\n\t}\n\tassert.Panics(fails)\n\n\tfails = func() {\n\t\tbm.Set(3)\n\t}\n\tassert.Panics(fails)\n\n}\n\nfunc TestBitmap_BlocksWithZero(t *testing.T) {\n\tbm := newBitmap(128)\n\tfor i := 0; i < 64; i++ {\n\t\tif i%2 == 0 {\n\t\t\tbm.Set(i)\n\t\t}\n\t}\n\tt.Logf(\"%v\", bm)\n\n\tblockID := bm.BlocksWithZero(3)\n\tif blockID != 1 {\n\t\tt.Errorf(\"Expected 1 Got %d instead\", blockID)\n\t}\n\n\tfor i := 0; i < 4; i++ {\n\t\tbm.Clear(i)\n\t}\n\n\tblockID = bm.BlocksWithZero(3)\n\tif blockID != 0 {\n\t\tt.Errorf(\"Expected 0 Got %d instead\", blockID)\n\t}\n}\n"
        },
        {
          "name": "blas.go",
          "type": "blob",
          "size": 1.369140625,
          "content": "package gorgonia\n\nimport (\n\t\"sync\"\n\n\t\"gonum.org/v1/gonum/blas\"\n\t\"gonum.org/v1/gonum/blas/gonum\"\n\t\"gorgonia.org/tensor\"\n)\n\nvar blasdoor sync.Mutex\nvar whichblas BLAS\n\n// BLAS represents all the possible implementations of BLAS.\n// The default is Gonum's Native\ntype BLAS interface {\n\tblas.Float32\n\tblas.Float64\n\tblas.Complex64\n\tblas.Complex128\n}\n\n// only blase.Implementation() and cubone.Implementation() are batchedBLAS -\n// they both batch cgo calls (and cubone batches cuda calls)\ntype batchedBLAS interface {\n\tWorkAvailable() <-chan struct{}\n\tDoWork()\n\tClose() error\n\tBLAS\n}\n\n// Use defines which BLAS implementation gorgonia should use.\n// The default is Gonum's Native. These are the other options:\n//\t\tUse(blase.Implementation())\n//\t\tUse(cubone.Implementation())\n//\t\tUse(cgo.Implementation)\n// Note the differences in the brackets. The blase and cubone ones are functions.\nfunc Use(b BLAS) {\n\t// close the blast door! close the blast door!\n\tblasdoor.Lock()\n\t// open the blast door! open the blast door!\n\tdefer blasdoor.Unlock()\n\t// those lines were few of the better additions to the Special Edition. There, I said it. The Special Edition is superior. Except Han still shot first in my mind.\n\n\twhichblas = b\n\ttensor.Use(b)\n\n\t// TODO:\n\t// float32\n}\n\n// WhichBLAS returns the BLAS that gorgonia uses.\nfunc WhichBLAS() BLAS { return whichblas }\n\nfunc init() {\n\twhichblas = gonum.Implementation{}\n}\n"
        },
        {
          "name": "blas_test.go",
          "type": "blob",
          "size": 15.125,
          "content": "package gorgonia\n\nimport (\n\t\"testing\"\n\n\t\"gonum.org/v1/gonum/blas\"\n\t\"gonum.org/v1/gonum/blas/gonum\"\n\t\"gorgonia.org/tensor\"\n)\n\nvar gonumImpl = gonum.Implementation{}\n\n// testBLASImplementation of the interface\ntype testBLASImplementation struct {\n\tgonum.Implementation\n\tused bool\n}\n\n// Sdsdot computes the dot product of the two vectors plus a constant\n//  alpha + ∑_i x[i]*y[i]\n//\n// Float32 implementations are autogenerated and not directly tested.\n// Sdsdot ...\nfunc (*testBLASImplementation) Sdsdot(n int, alpha float32, x []float32, incX int, y []float32, incY int) float32 {\n\treturn gonumImpl.Sdsdot(n, alpha, x, incX, y, incY)\n}\n\n// Dsdot computes the dot product of the two vectors\n//  ∑_i x[i]*y[i]\n//\n// Float32 implementations are autogenerated and not directly tested.\n// Dsdot ...\nfunc (*testBLASImplementation) Dsdot(n int, x []float32, incX int, y []float32, incY int) float64 {\n\treturn gonumImpl.Dsdot(n, x, incX, y, incY)\n}\n\n// Sdot ...\nfunc (*testBLASImplementation) Sdot(n int, x []float32, incX int, y []float32, incY int) float32 {\n\treturn gonumImpl.Sdot(n, x, incX, y, incY)\n}\n\n// Snrm2 ...\nfunc (*testBLASImplementation) Snrm2(n int, x []float32, incX int) float32 {\n\treturn gonumImpl.Snrm2(n, x, incX)\n}\n\n// Sasum ...\nfunc (*testBLASImplementation) Sasum(n int, x []float32, incX int) float32 {\n\treturn gonumImpl.Sasum(n, x, incX)\n}\n\n// Isamax ...\nfunc (*testBLASImplementation) Isamax(n int, x []float32, incX int) int {\n\treturn gonumImpl.Isamax(n, x, incX)\n}\n\n// Sswap ...\nfunc (*testBLASImplementation) Sswap(n int, x []float32, incX int, y []float32, incY int) {\n\tgonumImpl.Sswap(n, x, incX, y, incY)\n}\n\n// Scopy ...\nfunc (*testBLASImplementation) Scopy(n int, x []float32, incX int, y []float32, incY int) {\n\tgonumImpl.Scopy(n, x, incX, y, incY)\n}\n\n// Saxpy ...\nfunc (*testBLASImplementation) Saxpy(n int, alpha float32, x []float32, incX int, y []float32, incY int) {\n\tgonumImpl.Saxpy(n, alpha, x, incX, y, incY)\n}\n\n// Srotg ...\nfunc (*testBLASImplementation) Srotg(a float32, b float32) (c float32, s float32, r float32, z float32) {\n\treturn gonumImpl.Srotg(a, b)\n}\n\n// Srotmg ...\nfunc (*testBLASImplementation) Srotmg(d1 float32, d2 float32, b1 float32, b2 float32) (p blas.SrotmParams, rd1 float32, rd2 float32, rb1 float32) {\n\treturn gonumImpl.Srotmg(d1, d2, b1, b2)\n}\n\n// Srot ...\nfunc (*testBLASImplementation) Srot(n int, x []float32, incX int, y []float32, incY int, c float32, s float32) {\n\tgonumImpl.Srot(n, x, incX, y, incY, c, s)\n}\n\n// Srotm ...\nfunc (*testBLASImplementation) Srotm(n int, x []float32, incX int, y []float32, incY int, p blas.SrotmParams) {\n\tgonumImpl.Srotm(n, x, incX, y, incY, p)\n}\n\n// Sscal ...\nfunc (*testBLASImplementation) Sscal(n int, alpha float32, x []float32, incX int) {\n\tgonumImpl.Sscal(n, alpha, x, incX)\n}\n\n// Sgemv ...\nfunc (*testBLASImplementation) Sgemv(tA blas.Transpose, m int, n int, alpha float32, a []float32, lda int, x []float32, incX int, beta float32, y []float32, incY int) {\n\tgonumImpl.Sgemv(tA, m, n, alpha, a, lda, x, incX, beta, y, incY)\n}\n\n// Sgbmv ...\nfunc (*testBLASImplementation) Sgbmv(tA blas.Transpose, m int, n int, kL int, kU int, alpha float32, a []float32, lda int, x []float32, incX int, beta float32, y []float32, incY int) {\n\tgonumImpl.Sgbmv(tA, m, n, kL, kU, alpha, a, lda, x, incX, beta, y, incY)\n}\n\n// Strmv ...\nfunc (*testBLASImplementation) Strmv(ul blas.Uplo, tA blas.Transpose, d blas.Diag, n int, a []float32, lda int, x []float32, incX int) {\n\tgonumImpl.Strmv(ul, tA, d, n, a, lda, x, incX)\n}\n\n// Stbmv ...\nfunc (*testBLASImplementation) Stbmv(ul blas.Uplo, tA blas.Transpose, d blas.Diag, n int, k int, a []float32, lda int, x []float32, incX int) {\n\tgonumImpl.Stbmv(ul, tA, d, n, k, a, lda, x, incX)\n}\n\n// Stpmv ...\nfunc (*testBLASImplementation) Stpmv(ul blas.Uplo, tA blas.Transpose, d blas.Diag, n int, ap []float32, x []float32, incX int) {\n\tgonumImpl.Stpmv(ul, tA, d, n, ap, x, incX)\n}\n\n// Strsv ...\nfunc (*testBLASImplementation) Strsv(ul blas.Uplo, tA blas.Transpose, d blas.Diag, n int, a []float32, lda int, x []float32, incX int) {\n\tgonumImpl.Strsv(ul, tA, d, n, a, lda, x, incX)\n}\n\n// Stbsv ...\nfunc (*testBLASImplementation) Stbsv(ul blas.Uplo, tA blas.Transpose, d blas.Diag, n int, k int, a []float32, lda int, x []float32, incX int) {\n\tgonumImpl.Stbsv(ul, tA, d, n, k, a, lda, x, incX)\n}\n\n// Stpsv ...\nfunc (*testBLASImplementation) Stpsv(ul blas.Uplo, tA blas.Transpose, d blas.Diag, n int, ap []float32, x []float32, incX int) {\n\tgonumImpl.Stpsv(ul, tA, d, n, ap, x, incX)\n\n}\n\n// Ssymv ...\nfunc (*testBLASImplementation) Ssymv(ul blas.Uplo, n int, alpha float32, a []float32, lda int, x []float32, incX int, beta float32, y []float32, incY int) {\n\tgonumImpl.Ssymv(ul, n, alpha, a, lda, x, incX, beta, y, incY)\n\n}\n\n// Ssbmv ...\nfunc (*testBLASImplementation) Ssbmv(ul blas.Uplo, n int, k int, alpha float32, a []float32, lda int, x []float32, incX int, beta float32, y []float32, incY int) {\n\tgonumImpl.Ssbmv(ul, n, k, alpha, a, lda, x, incX, beta, y, incY)\n\n}\n\n// Sspmv ...\nfunc (*testBLASImplementation) Sspmv(ul blas.Uplo, n int, alpha float32, ap []float32, x []float32, incX int, beta float32, y []float32, incY int) {\n\tgonumImpl.Sspmv(ul, n, alpha, ap, x, incX, beta, y, incY)\n\n}\n\n// Sger ...\nfunc (*testBLASImplementation) Sger(m int, n int, alpha float32, x []float32, incX int, y []float32, incY int, a []float32, lda int) {\n\tgonumImpl.Sger(m, n, alpha, x, incX, y, incY, a, lda)\n\n}\n\n// Ssyr ...\nfunc (*testBLASImplementation) Ssyr(ul blas.Uplo, n int, alpha float32, x []float32, incX int, a []float32, lda int) {\n\tgonumImpl.Ssyr(ul, n, alpha, x, incX, a, lda)\n\n}\n\n// Sspr ...\nfunc (*testBLASImplementation) Sspr(ul blas.Uplo, n int, alpha float32, x []float32, incX int, ap []float32) {\n\tgonumImpl.Sspr(ul, n, alpha, x, incX, ap)\n\n}\n\n// Ssyr2 ...\nfunc (*testBLASImplementation) Ssyr2(ul blas.Uplo, n int, alpha float32, x []float32, incX int, y []float32, incY int, a []float32, lda int) {\n\tgonumImpl.Ssyr2(ul, n, alpha, x, incX, y, incY, a, lda)\n\n}\n\n// Sspr2 ...\nfunc (*testBLASImplementation) Sspr2(ul blas.Uplo, n int, alpha float32, x []float32, incX int, y []float32, incY int, a []float32) {\n\tgonumImpl.Sspr2(ul, n, alpha, x, incX, y, incY, a)\n\n}\n\n// Ssymm ...\nfunc (*testBLASImplementation) Ssymm(s blas.Side, ul blas.Uplo, m int, n int, alpha float32, a []float32, lda int, b []float32, ldb int, beta float32, c []float32, ldc int) {\n\tgonumImpl.Ssymm(s, ul, m, n, alpha, a, lda, b, ldb, beta, c, ldc)\n\n}\n\n// Ssyrk ...\nfunc (*testBLASImplementation) Ssyrk(ul blas.Uplo, t blas.Transpose, n int, k int, alpha float32, a []float32, lda int, beta float32, c []float32, ldc int) {\n\tgonumImpl.Ssyrk(ul, t, n, k, alpha, a, lda, beta, c, ldc)\n\n}\n\n// Ssyr2k ...\nfunc (*testBLASImplementation) Ssyr2k(ul blas.Uplo, t blas.Transpose, n int, k int, alpha float32, a []float32, lda int, b []float32, ldb int, beta float32, c []float32, ldc int) {\n\tgonumImpl.Ssyr2k(ul, t, n, k, alpha, a, lda, b, ldb, beta, c, ldc)\n\n}\n\n// Strmm ...\nfunc (*testBLASImplementation) Strmm(s blas.Side, ul blas.Uplo, tA blas.Transpose, d blas.Diag, m int, n int, alpha float32, a []float32, lda int, b []float32, ldb int) {\n\tgonumImpl.Strmm(s, ul, tA, d, m, n, alpha, a, lda, b, ldb)\n\n}\n\n// Strsm ...\nfunc (*testBLASImplementation) Strsm(s blas.Side, ul blas.Uplo, tA blas.Transpose, d blas.Diag, m int, n int, alpha float32, a []float32, lda int, b []float32, ldb int) {\n\tgonumImpl.Strsm(s, ul, tA, d, m, n, alpha, a, lda, b, ldb)\n\n}\n\n// Ddot ...\nfunc (*testBLASImplementation) Ddot(n int, x []float64, incX int, y []float64, incY int) float64 {\n\treturn gonumImpl.Ddot(n, x, incX, y, incY)\n\n}\n\n// Dnrm2 ...\nfunc (*testBLASImplementation) Dnrm2(n int, x []float64, incX int) float64 {\n\treturn gonumImpl.Dnrm2(n, x, incX)\n\n}\n\n// Dasum ...\nfunc (*testBLASImplementation) Dasum(n int, x []float64, incX int) float64 {\n\treturn gonumImpl.Dasum(n, x, incX)\n\n}\n\n// Idamax ...\nfunc (*testBLASImplementation) Idamax(n int, x []float64, incX int) int {\n\treturn gonumImpl.Idamax(n, x, incX)\n\n}\n\n// Dswap ...\nfunc (*testBLASImplementation) Dswap(n int, x []float64, incX int, y []float64, incY int) {\n\tgonumImpl.Dswap(n, x, incX, y, incY)\n\n}\n\n// Dcopy ...\nfunc (*testBLASImplementation) Dcopy(n int, x []float64, incX int, y []float64, incY int) {\n\tgonumImpl.Dcopy(n, x, incX, y, incY)\n\n}\n\n// Daxpy ...\nfunc (*testBLASImplementation) Daxpy(n int, alpha float64, x []float64, incX int, y []float64, incY int) {\n\tgonumImpl.Daxpy(n, alpha, x, incX, y, incY)\n\n}\n\n// Drotg ...\nfunc (*testBLASImplementation) Drotg(a float64, b float64) (c float64, s float64, r float64, z float64) {\n\treturn gonumImpl.Drotg(a, b)\n\n}\n\n// Drotmg ...\nfunc (*testBLASImplementation) Drotmg(d1 float64, d2 float64, b1 float64, b2 float64) (p blas.DrotmParams, rd1 float64, rd2 float64, rb1 float64) {\n\treturn gonumImpl.Drotmg(d1, d2, b1, b2)\n\n}\n\n// Drot ...\nfunc (*testBLASImplementation) Drot(n int, x []float64, incX int, y []float64, incY int, c float64, s float64) {\n\tgonumImpl.Drot(n, x, incX, y, incY, c, s)\n\n}\n\n// Drotm ...\nfunc (*testBLASImplementation) Drotm(n int, x []float64, incX int, y []float64, incY int, p blas.DrotmParams) {\n\tgonumImpl.Drotm(n, x, incX, y, incY, p)\n\n}\n\n// Dscal ...\nfunc (*testBLASImplementation) Dscal(n int, alpha float64, x []float64, incX int) {\n\tgonumImpl.Dscal(n, alpha, x, incX)\n\n}\n\n// Dgemv ...\nfunc (*testBLASImplementation) Dgemv(tA blas.Transpose, m int, n int, alpha float64, a []float64, lda int, x []float64, incX int, beta float64, y []float64, incY int) {\n\tgonumImpl.Dgemv(tA, m, n, alpha, a, lda, x, incX, beta, y, incY)\n\n}\n\n// Dgbmv ...\nfunc (*testBLASImplementation) Dgbmv(tA blas.Transpose, m int, n int, kL int, kU int, alpha float64, a []float64, lda int, x []float64, incX int, beta float64, y []float64, incY int) {\n\tgonumImpl.Dgbmv(tA, m, n, kL, kU, alpha, a, lda, x, incX, beta, y, incY)\n\n}\n\n// Dtrmv ...\nfunc (*testBLASImplementation) Dtrmv(ul blas.Uplo, tA blas.Transpose, d blas.Diag, n int, a []float64, lda int, x []float64, incX int) {\n\tgonumImpl.Dtrmv(ul, tA, d, n, a, lda, x, incX)\n\n}\n\n// Dtbmv ...\nfunc (*testBLASImplementation) Dtbmv(ul blas.Uplo, tA blas.Transpose, d blas.Diag, n int, k int, a []float64, lda int, x []float64, incX int) {\n\tgonumImpl.Dtbmv(ul, tA, d, n, k, a, lda, x, incX)\n\n}\n\n// Dtpmv ...\nfunc (*testBLASImplementation) Dtpmv(ul blas.Uplo, tA blas.Transpose, d blas.Diag, n int, ap []float64, x []float64, incX int) {\n\tgonumImpl.Dtpmv(ul, tA, d, n, ap, x, incX)\n\n}\n\n// Dtrsv ...\nfunc (*testBLASImplementation) Dtrsv(ul blas.Uplo, tA blas.Transpose, d blas.Diag, n int, a []float64, lda int, x []float64, incX int) {\n\tgonumImpl.Dtrsv(ul, tA, d, n, a, lda, x, incX)\n\n}\n\n// Dtbsv ...\nfunc (*testBLASImplementation) Dtbsv(ul blas.Uplo, tA blas.Transpose, d blas.Diag, n int, k int, a []float64, lda int, x []float64, incX int) {\n\tgonumImpl.Dtbsv(ul, tA, d, n, k, a, lda, x, incX)\n\n}\n\n// Dtpsv ...\nfunc (*testBLASImplementation) Dtpsv(ul blas.Uplo, tA blas.Transpose, d blas.Diag, n int, ap []float64, x []float64, incX int) {\n\tgonumImpl.Dtpsv(ul, tA, d, n, ap, x, incX)\n\n}\n\n// Dsymv ...\nfunc (*testBLASImplementation) Dsymv(ul blas.Uplo, n int, alpha float64, a []float64, lda int, x []float64, incX int, beta float64, y []float64, incY int) {\n\tgonumImpl.Dsymv(ul, n, alpha, a, lda, x, incX, beta, y, incY)\n\n}\n\n// Dsbmv ...\nfunc (*testBLASImplementation) Dsbmv(ul blas.Uplo, n int, k int, alpha float64, a []float64, lda int, x []float64, incX int, beta float64, y []float64, incY int) {\n\tgonumImpl.Dsbmv(ul, n, k, alpha, a, lda, x, incX, beta, y, incY)\n\n}\n\n// Dspmv ...\nfunc (*testBLASImplementation) Dspmv(ul blas.Uplo, n int, alpha float64, ap []float64, x []float64, incX int, beta float64, y []float64, incY int) {\n\tgonumImpl.Dspmv(ul, n, alpha, ap, x, incX, beta, y, incY)\n\n}\n\n// Dger ...\nfunc (*testBLASImplementation) Dger(m int, n int, alpha float64, x []float64, incX int, y []float64, incY int, a []float64, lda int) {\n\tgonumImpl.Dger(m, n, alpha, x, incX, y, incY, a, lda)\n\n}\n\n// Dsyr ...\nfunc (*testBLASImplementation) Dsyr(ul blas.Uplo, n int, alpha float64, x []float64, incX int, a []float64, lda int) {\n\tgonumImpl.Dsyr(ul, n, alpha, x, incX, a, lda)\n\n}\n\n// Dspr ...\nfunc (*testBLASImplementation) Dspr(ul blas.Uplo, n int, alpha float64, x []float64, incX int, ap []float64) {\n\tgonumImpl.Dspr(ul, n, alpha, x, incX, ap)\n\n}\n\n// Dsyr2 ...\nfunc (*testBLASImplementation) Dsyr2(ul blas.Uplo, n int, alpha float64, x []float64, incX int, y []float64, incY int, a []float64, lda int) {\n\tgonumImpl.Dsyr2(ul, n, alpha, x, incX, y, incY, a, lda)\n\n}\n\n// Dspr2 ...\nfunc (*testBLASImplementation) Dspr2(ul blas.Uplo, n int, alpha float64, x []float64, incX int, y []float64, incY int, a []float64) {\n\tgonumImpl.Dspr2(ul, n, alpha, x, incX, y, incY, a)\n\n}\n\n// Dsymm ...\nfunc (*testBLASImplementation) Dsymm(s blas.Side, ul blas.Uplo, m int, n int, alpha float64, a []float64, lda int, b []float64, ldb int, beta float64, c []float64, ldc int) {\n\tgonumImpl.Dsymm(s, ul, m, n, alpha, a, lda, b, ldb, beta, c, ldc)\n\n}\n\n// Dsyrk ...\nfunc (*testBLASImplementation) Dsyrk(ul blas.Uplo, t blas.Transpose, n int, k int, alpha float64, a []float64, lda int, beta float64, c []float64, ldc int) {\n\tgonumImpl.Dsyrk(ul, t, n, k, alpha, a, lda, beta, c, ldc)\n\n}\n\n// Dsyr2k ...\nfunc (*testBLASImplementation) Dsyr2k(ul blas.Uplo, t blas.Transpose, n int, k int, alpha float64, a []float64, lda int, b []float64, ldb int, beta float64, c []float64, ldc int) {\n\tgonumImpl.Dsyr2k(ul, t, n, k, alpha, a, lda, b, ldb, beta, c, ldc)\n\n}\n\n// Dtrmm ...\nfunc (*testBLASImplementation) Dtrmm(s blas.Side, ul blas.Uplo, tA blas.Transpose, d blas.Diag, m int, n int, alpha float64, a []float64, lda int, b []float64, ldb int) {\n\tgonumImpl.Dtrmm(s, ul, tA, d, m, n, alpha, a, lda, b, ldb)\n\n}\n\n// Dtrsm ...\nfunc (*testBLASImplementation) Dtrsm(s blas.Side, ul blas.Uplo, tA blas.Transpose, d blas.Diag, m int, n int, alpha float64, a []float64, lda int, b []float64, ldb int) {\n\tgonumImpl.Dtrsm(s, ul, tA, d, m, n, alpha, a, lda, b, ldb)\n\n}\n\n// Sgemm ...\nfunc (t *testBLASImplementation) Sgemm(tA blas.Transpose, tB blas.Transpose, m int, n int, k int, alpha float32, a []float32, lda int, b []float32, ldb int, beta float32, c []float32, ldc int) {\n\tt.used = true\n\tgonumImpl.Sgemm(tA, tB, m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)\n\n}\n\n// Dgemm ...\nfunc (*testBLASImplementation) Dgemm(tA blas.Transpose, tB blas.Transpose, m int, n int, k int, alpha float64, a []float64, lda int, b []float64, ldb int, beta float64, c []float64, ldc int) {\n\tgonumImpl.Dgemm(tA, tB, m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)\n\n}\nfunc TestUse(t *testing.T) {\n\tblasI := &testBLASImplementation{}\n\tUse(blasI)\n\tg := NewGraph()\n\tx := NodeFromAny(g, tensor.New(\n\t\ttensor.WithShape(1, 1, 7, 5),\n\t\ttensor.WithBacking([]float32{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34})))\n\tfilter := NodeFromAny(g, tensor.New(\n\t\ttensor.WithShape(1, 1, 3, 3),\n\t\ttensor.WithBacking([]float32{1, 1, 1, 1, 1, 1, 1, 1, 1})))\n\ty := Must(Conv2d(x, filter, []int{3, 3}, []int{0, 0}, []int{2, 2}, []int{1, 1}))\n\tm := NewTapeMachine(g)\n\tif err := m.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\t//54 72 144 162 234 252\n\toutput := y.Value().Data().([]float32)\n\tif output[0] != 54 ||\n\t\toutput[1] != 72 ||\n\t\toutput[2] != 144 ||\n\t\toutput[3] != 162 ||\n\t\toutput[4] != 234 ||\n\t\toutput[5] != 252 {\n\t\tt.Fatal(\"wrong computation value\")\n\t}\n\n\tif !blasI.used {\n\t\tt.Fail()\n\t}\n\tif WhichBLAS() != blasI {\n\t\tt.Fail()\n\t}\n}\n"
        },
        {
          "name": "blase",
          "type": "tree",
          "content": null
        },
        {
          "name": "broadcast.go",
          "type": "blob",
          "size": 3.994140625,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\nconst (\n\tbcAllowableAxes = 4\n)\n\n// BroadcastPattern is actually a bit array.\n// It's split into 2 nibbles - the left nibble represents the left operand, the right nibble represents the right operand:\n//\t\txxxx|xxxx\n// The least significant bit of each nibble is elem 0.\n// Concrete examples:\n//\t\t00000010 (0x02) = broadcast axis 1 of the right operand\n//\t\t00000001 (0x01) = broadcast axis 0 of the right operand\n//\t\t00000101 (0x09) = broadcast axis 0 AND axis 2 of the right operand\n//\t\t00010000 (0x10) = broadcast axis 0 of the left operand\n//\t\t00110000 (0x30) = broadcast axis 0 and axis 1 of the lef operand\n// You get the drill.\n//\n// Do note that the current limitation of the BroadcastPattern allows only up to 4 dimensions per operand.\ntype BroadcastPattern byte\n\n// NewBroadcastPattern is a helper function to create broadcast patterns\nfunc NewBroadcastPattern(leftAxes, rightAxes []byte) BroadcastPattern {\n\tvar start byte\n\tfor _, a := range leftAxes {\n\t\ta += bcAllowableAxes\n\t\tstart |= byte(1) << a\n\t}\n\tfor _, a := range rightAxes {\n\t\tstart |= byte(1) << a\n\t}\n\treturn BroadcastPattern(start)\n}\n\nfunc (bcpat BroadcastPattern) bc(left bool, axis byte) bool {\n\toperand := axis\n\tif left {\n\t\toperand += bcAllowableAxes\n\t}\n\treturn (byte(bcpat)>>operand)&byte(1) == 1\n}\n\nfunc (bcpat BroadcastPattern) on() (retVal [2][]int) {\n\tfor i := 0; i < bcAllowableAxes; i++ {\n\t\tif bcpat.bc(true, byte(i)) {\n\t\t\tretVal[0] = append(retVal[0], i)\n\t\t}\n\t}\n\n\tfor i := 0; i < bcAllowableAxes; i++ {\n\t\tif bcpat.bc(false, byte(i)) {\n\t\t\tretVal[1] = append(retVal[1], i)\n\t\t}\n\t}\n\n\treturn\n}\n\n// Broadcast apply the pattern to the input nodes\n// and returns two nodes suitable for a binary operator.\n// Broadcast works somewhat like Numpy's broadcast, except it's now exposed as a function.\nfunc Broadcast(a, b *Node, pattern BroadcastPattern) (*Node, *Node, error) {\n\tbroadcastOn := pattern.on()\n\n\tvar err error\n\tvar newShape tensor.Shape\n\tx := a\n\ty := b\n\txshape := x.Shape()\n\tyshape := y.Shape()\n\n\tif len(broadcastOn[0]) > 0 {\n\n\t\tfor _, a := range broadcastOn[0] {\n\t\t\tif a >= yshape.Dims() {\n\t\t\t\treturn nil, nil, errors.Errorf(\"Attempting to broadcast a on axis %d of b. But b has shape %v\", a, yshape)\n\t\t\t}\n\t\t}\n\t\tnewShape = calcBroadcastShape(x, yshape.Dims(), broadcastOn[0])\n\t\tif x, err = Reshape(x, newShape); err != nil {\n\t\t\treturn nil, nil, errors.Wrapf(err, \"Cannot reshape x to %v for broadcasting\", newShape)\n\t\t}\n\t\tchildren := Nodes{x}\n\t\tfor _, a := range broadcastOn[0] {\n\t\t\tvar size *Node\n\t\t\tif size, err = SizeOf(a, y); err != nil {\n\t\t\t\treturn nil, nil, errors.Wrap(err, operationError)\n\t\t\t}\n\t\t\tchildren = append(children, size)\n\t\t}\n\t\tif x, err = repeatedApply(broadcastOn[0], children); err != nil {\n\t\t\treturn nil, nil, errors.Wrap(err, operationError)\n\t\t}\n\t}\n\n\tif len(broadcastOn[1]) > 0 {\n\t\tfor _, a := range broadcastOn[1] {\n\t\t\tif a >= xshape.Dims() {\n\t\t\t\treturn nil, nil, errors.Errorf(\"Attempting to broadcast b on axis %d of a. But a has shape %v\", a, xshape)\n\t\t\t}\n\t\t}\n\n\t\tnewShape = calcBroadcastShape(y, xshape.Dims(), broadcastOn[1])\n\n\t\tif y, err = Reshape(y, newShape); err != nil {\n\t\t\treturn nil, nil, errors.Wrapf(err, \"Cannot reshape y to %v for broadcast\", newShape)\n\t\t}\n\t\tchildren := Nodes{y}\n\t\tfor _, a := range broadcastOn[1] {\n\t\t\tvar size *Node\n\t\t\tif size, err = SizeOf(a, x); err != nil {\n\t\t\t\treturn nil, nil, errors.Wrap(err, operationError)\n\t\t\t}\n\t\t\tchildren = append(children, size)\n\t\t}\n\n\t\tif y, err = repeatedApply(broadcastOn[1], children); err != nil {\n\t\t\treturn nil, nil, errors.Wrap(err, operationError)\n\t\t}\n\t}\n\treturn x, y, nil\n}\n\nfunc autoBroadcastPattern(aShape, bShape tensor.Shape) (leftPattern, rightPattern []byte, err error) {\n\tif aShape.Dims() != bShape.Dims() {\n\t\treturn nil, nil, fmt.Errorf(\"shapes %v and %v should have the same dimensions\", aShape, bShape)\n\t}\n\tfor i := 0; i < aShape.Dims(); i++ {\n\t\tif aShape[i] > bShape[i] {\n\t\t\tleftPattern = append(leftPattern, byte(i))\n\t\t} else if aShape[i] < bShape[i] {\n\t\t\trightPattern = append(rightPattern, byte(i))\n\t\t}\n\t}\n\treturn\n}\n"
        },
        {
          "name": "broadcast_test.go",
          "type": "blob",
          "size": 2.8046875,
          "content": "package gorgonia\n\nimport (\n\t\"io/ioutil\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc TestBroadcastPattern(t *testing.T) {\n\tassert := assert.New(t)\n\tvar bcpat BroadcastPattern\n\n\t// make sure that the basics work\n\tbcpat = NewBroadcastPattern(nil, []byte{1})\n\tassert.Equal(BroadcastPattern(0x02), bcpat)\n\n\tbcpat = NewBroadcastPattern(nil, []byte{0})\n\tassert.Equal(BroadcastPattern(0x01), bcpat)\n\n\tbcpat = NewBroadcastPattern([]byte{1, 0}, nil)\n\tassert.Equal(BroadcastPattern(0x30), bcpat)\n\n\tbcpat = NewBroadcastPattern([]byte{0}, nil)\n\tassert.Equal(BroadcastPattern(0x10), bcpat)\n\n\t// checks\n\tbcpat = NewBroadcastPattern(nil, []byte{1})\n\tassert.True(bcpat.bc(false, 1))\n\tassert.False(bcpat.bc(true, 1))\n\n\t// ons\n\tbcpat = NewBroadcastPattern(nil, []byte{1})\n\tassert.Equal([]int{1}, bcpat.on()[1])\n\tassert.Nil(bcpat.on()[0])\n\n\tbcpat = NewBroadcastPattern([]byte{2, 1}, []byte{1})\n\tassert.Equal([]int{1, 2}, bcpat.on()[0])\n\tassert.Equal([]int{1}, bcpat.on()[1])\n}\n\nfunc TestBroadcast(t *testing.T) {\n\tif CUDA {\n\t\tt.SkipNow()\n\t}\n\n\tassert := assert.New(t)\n\tvar g *ExprGraph\n\tvar x, y, a, b, z *Node\n\tvar m *lispMachine\n\tvar err error\n\n\txT := tensor.New(tensor.WithShape(2, 3), tensor.WithBacking(tensor.Range(tensor.Float64, 0, 6)))\n\tyT := tensor.New(tensor.WithShape(2), tensor.WithBacking([]float64{100, 200}))\n\n\tg = NewGraph()\n\tx = NewMatrix(g, Float64, WithShape(2, 3), WithValue(xT), WithName(\"x\"))\n\ty = NewVector(g, Float64, WithShape(2), WithValue(yT), WithName(\"y\"))\n\tif a, b, err = Broadcast(x, y, NewBroadcastPattern(nil, []byte{1})); err != nil {\n\t\tioutil.WriteFile(\"Broadcast.dot\", []byte(g.ToDot()), 0644)\n\t\tt.Fatal(err)\n\t}\n\tz, err = Add(a, b)\n\tif err != nil {\n\t\tt.Fatalf(\"Error: %v. a %v + b %v\", err, a.Shape(), b.Shape())\n\t}\n\tif _, _, err = Broadcast(x, y, NewBroadcastPattern(nil, []byte{1})); err != nil {\n\t\tioutil.WriteFile(\"Broadcast.dot\", []byte(g.ToDot()), 0644)\n\t\tt.Fatal(err)\n\t}\n\n\tm = NewLispMachine(g, ExecuteFwdOnly())\n\tdefer m.Close()\n\tif err := m.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tassert.Equal([]float64{100, 101, 102, 203, 204, 205}, extractF64s(z.Value()))\n\n\tg = NewGraph()\n\tx = NewMatrix(g, Float64, WithShape(2, 3), WithValue(xT), WithName(\"x\"))\n\ty = NewVector(g, Float64, WithShape(2), WithValue(yT), WithName(\"y\"))\n\tif a, b, err = Broadcast(y, x, NewBroadcastPattern([]byte{1}, nil)); err != nil {\n\t\tioutil.WriteFile(\"Broadcast.dot\", []byte(g.ToDot()), 0644)\n\t\tt.Fatalf(\"%+v\", err)\n\t}\n\t// TODO: Check the error returned by Add?\n\tz, _ = Add(a, b)\n\tif _, _, err = Broadcast(x, y, NewBroadcastPattern(nil, []byte{1})); err != nil {\n\t\tioutil.WriteFile(\"Broadcast.dot\", []byte(g.ToDot()), 0644)\n\t\tt.Fatal(err)\n\t}\n\n\tm = NewLispMachine(g, ExecuteFwdOnly())\n\tdefer m.Close()\n\tif err := m.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tassert.Equal([]float64{100, 101, 102, 203, 204, 205}, extractF64s(z.Value()))\n}\n"
        },
        {
          "name": "cmd",
          "type": "tree",
          "content": null
        },
        {
          "name": "codecov.yml",
          "type": "blob",
          "size": 0.033203125,
          "content": "coverage:\n  status:\n    patch: off"
        },
        {
          "name": "collections.go",
          "type": "blob",
          "size": 4.3486328125,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"sort\"\n\t\"unsafe\"\n\n\t\"github.com/xtgo/set\"\n)\n\n// Nodes is a slice of nodes, but it also acts as a set of nodes by implementing the Sort interface\ntype Nodes []*Node\n\n// Node returns nil. Always. This is bound to cause a panic somewhere if an program is not using it correctly.\n// The reason for implementing this is so that it may fulfil common interfaces.\nfunc (ns Nodes) Node() *Node { return nil }\n\n// Nodes returns itself. This is useful for interfaces\nfunc (ns Nodes) Nodes() Nodes { return ns }\n\n// Err returns nil always\nfunc (ns Nodes) Err() error { return nil }\n\n// implement sort.Interface\n\nfunc (ns Nodes) Len() int { return len(ns) }\nfunc (ns Nodes) Less(i, j int) bool {\n\treturn uintptr(unsafe.Pointer(ns[i])) < uintptr(unsafe.Pointer(ns[j]))\n}\nfunc (ns Nodes) Swap(i, j int) { ns[i], ns[j] = ns[j], ns[i] }\n\n// uses xtgo/set stuff\n\n// Set returns a uniquifies slice. It mutates the slice.\nfunc (ns Nodes) Set() Nodes {\n\tsort.Sort(ns)\n\tsize := set.Uniq(ns)\n\tns = ns[:size]\n\treturn ns\n}\n\n// Add adds to set\nfunc (ns Nodes) Add(n *Node) Nodes {\n\tfor _, node := range ns {\n\t\tif node == n {\n\t\t\treturn ns\n\t\t}\n\t}\n\tns = append(ns, n)\n\treturn ns\n}\n\n// Contains checks if the wanted node is in the set\nfunc (ns Nodes) Contains(want *Node) bool {\n\tfor _, n := range ns {\n\t\tif n == want {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Format implements fmt.Formatter, which allows Nodes to be differently formatted depending on the verbs\nfunc (ns Nodes) Format(s fmt.State, c rune) {\n\tdelimiter := \", \"\n\tif s.Flag(' ') {\n\t\tdelimiter = \"  \"\n\t}\n\tif s.Flag('+') {\n\t\tdelimiter = \", \\n\"\n\t}\n\tswitch c {\n\tcase 'd':\n\t\ts.Write([]byte(\"[\"))\n\t\tfor i, n := range ns {\n\t\t\tfmt.Fprintf(s, \"%x\", n.ID())\n\t\t\tif i < len(ns)-1 {\n\t\t\t\tfmt.Fprintf(s, \"%s\", delimiter)\n\t\t\t}\n\t\t}\n\t\ts.Write([]byte(\"]\"))\n\tcase 'v', 's':\n\t\ts.Write([]byte(\"[\"))\n\t\tfor i, n := range ns {\n\t\t\tif s.Flag('#') {\n\t\t\t\tfmt.Fprintf(s, \"%s :: %v\", n.Name(), n.t)\n\t\t\t} else {\n\t\t\t\tfmt.Fprintf(s, \"%s\", n.Name())\n\t\t\t}\n\t\t\tif i < len(ns)-1 {\n\t\t\t\tfmt.Fprintf(s, \"%s\", delimiter)\n\t\t\t}\n\t\t}\n\t\ts.Write([]byte(\"]\"))\n\tcase 'Y':\n\t\ts.Write([]byte(\"[\"))\n\t\tfor i, n := range ns {\n\t\t\tfmt.Fprintf(s, \"%v\", n.t)\n\t\t\tif i < len(ns)-1 {\n\t\t\t\tfmt.Fprintf(s, \"%s\", delimiter)\n\t\t\t}\n\t\t}\n\t\ts.Write([]byte(\"]\"))\n\n\tcase 'P':\n\t\ts.Write([]byte(\"[\"))\n\t\tfor i, n := range ns {\n\t\t\tfmt.Fprintf(s, \"%p\", n)\n\t\t\tif i < len(ns)-1 {\n\t\t\t\tfmt.Fprintf(s, \"%s\", delimiter)\n\t\t\t}\n\t\t}\n\t\ts.Write([]byte(\"]\"))\n\t}\n}\n\n// Difference is ns - other. Bear in mind it is NOT commutative\nfunc (ns Nodes) Difference(other Nodes) Nodes {\n\tsort.Sort(ns)\n\tsort.Sort(other)\n\ts := append(ns, other...)\n\tcount := set.Diff(s, len(ns))\n\treturn s[:count]\n}\n\n// Intersect performs an intersection with other Nodes\nfunc (ns Nodes) Intersect(other Nodes) Nodes {\n\tsort.Sort(ns)\n\tsort.Sort(other)\n\ts := append(ns, other...)\n\tcount := set.Inter(s, len(ns))\n\treturn s[:count]\n}\n\n// AllSameGraph returns true if all the nodes in the slice belong to the same graph. Note that constants do not have to belong to the same graph.\nfunc (ns Nodes) AllSameGraph() bool {\n\tif len(ns) == 0 {\n\t\treturn false\n\t}\n\n\tvar g *ExprGraph\n\tfor _, n := range ns {\n\t\tif !n.isConstant() {\n\t\t\tg = n.g\n\t\t\tbreak\n\t\t}\n\t}\n\n\tfor _, n := range ns {\n\t\tif n.g != g && !n.isConstant() {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Equals returns true if two Nodes are the same\nfunc (ns Nodes) Equals(other Nodes) bool {\n\tif len(ns) != len(other) {\n\t\treturn false\n\t}\n\n\tfor _, n := range ns {\n\t\tif !other.Contains(n) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc (ns Nodes) mapSet() NodeSet { return NewNodeSet(ns...) }\n\nfunc (ns Nodes) index(n *Node) int {\n\tfor i, node := range ns {\n\t\tif node == n {\n\t\t\treturn i\n\t\t}\n\t}\n\treturn -1\n}\n\nfunc (ns Nodes) reverse() {\n\tl := len(ns)\n\tfor i := l/2 - 1; i >= 0; i-- {\n\t\to := l - 1 - i\n\t\tns[i], ns[o] = ns[o], ns[i]\n\t}\n}\n\nfunc (ns Nodes) replace(what, with *Node) Nodes {\n\tfor i, n := range ns {\n\t\tif n == what {\n\t\t\tns[i] = with\n\t\t}\n\t}\n\treturn ns\n}\n\nvar removers = make(map[string]int)\n\nfunc (ns Nodes) remove(what *Node) Nodes {\n\tfor i := ns.index(what); i != -1; i = ns.index(what) {\n\t\tcopy(ns[i:], ns[i+1:])\n\t\tns[len(ns)-1] = nil // to prevent any unwanted references so things can be GC'd away\n\t\tns = ns[:len(ns)-1]\n\t}\n\n\treturn ns\n}\n\nfunc (ns Nodes) dimSizers() []DimSizer {\n\tretVal := borrowDimSizers(len(ns))\n\tfor i, n := range ns {\n\t\tif s, ok := n.op.(sizeOp); ok {\n\t\t\tretVal[i] = s\n\t\t} else {\n\t\t\tretVal[i] = n.shape\n\t\t}\n\t}\n\treturn retVal\n}\n"
        },
        {
          "name": "collections_test.go",
          "type": "blob",
          "size": 2.455078125,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestNodes(t *testing.T) {\n\tassert := assert.New(t)\n\tg := NewGraph()\n\tn0 := newNode(In(g), WithName(\"n0\"))\n\tn1 := newNode(In(g), WithName(\"n1\"))\n\tn2 := newNode(In(g), WithName(\"n2\"))\n\tn3 := newNode(In(g), WithName(\"n3\"))\n\n\t// calculate hashcode first\n\tn0h := n0.Hashcode()\n\tn1h := n1.Hashcode()\n\tn2h := n2.Hashcode()\n\tn3h := n3.Hashcode()\n\tt.Logf(\"%x, %x, %x, %x\", n0h, n1h, n2h, n3h)\n\n\tset := Nodes{n0, n1, n2, n3, n0, n0}\n\n\tset = set.Set()\n\tcorrect := Nodes{n0, n1, n2, n3}\n\tfor _, n := range correct {\n\t\tassert.Contains(set, n, \"SET: %v\", set)\n\t}\n\tassert.Equal(len(correct), len(set))\n\n\tt.Log(\"Test add\")\n\tset = Nodes{}\n\tset = set.Add(n0)\n\tset = set.Add(n2)\n\tset = set.Add(n0)\n\tset = set.Add(n3)\n\tset = set.Add(n1)\n\tcorrect = Nodes{n0, n2, n3, n1}\n\tassert.Equal(correct, set)\n\n\tt.Log(\"Testing intersection\")\n\tset = Nodes{n0, n2, n1, n3} // out of order, on purpose\n\tother := Nodes{n0, n1}\n\tinter := set.Intersect(other)\n\n\tcorrect = Nodes{n0, n1}\n\tfor _, n := range correct {\n\t\tassert.Contains(inter, n, \"inter: %v\", inter)\n\t}\n\tassert.Equal(len(correct), len(inter))\n\n\tt.Log(\"Testing difference\")\n\tn4 := newNode(In(g))\n\tn5 := newNode(In(g))\n\tset = Nodes{n3, n0, n1, n2}\n\tother = Nodes{n0, n3, n4, n5}\n\n\tdiff := set.Difference(other)\n\tcorrect = Nodes{n1, n2}\n\tfor _, n := range correct {\n\t\tassert.Contains(diff, n)\n\t}\n\tassert.Equal(len(correct), len(diff))\n\n\tt.Log(\"Testing replace\")\n\tset = Nodes{n0, n2, n1, n2, n1} // not yet a set\n\tset = set.replace(n2, n3)\n\tcorrect = Nodes{n0, n3, n1, n3, n1}\n\tassert.Equal(correct, set)\n\n\tt.Log(\"Formatting\")\n\tformats := []string{\"% v\", \"%+v\", \"%d\", \"%v\", \"%#v\", \"%Y\", \"%P\"}\n\tcorrectFormats := []string{\n\t\t\"[n0  n1  n2  n3]\",\n\t\t`[n0, \nn1, \nn2, \nn3]`,\n\t\tfmt.Sprintf(\"[%x, %x, %x, %x]\", n0.id, n1.id, n2.id, n3.id),\n\t\t\"[n0, n1, n2, n3]\",\n\t\t\"[n0 :: <nil>, n1 :: <nil>, n2 :: <nil>, n3 :: <nil>]\",\n\t\t\"[<nil>, <nil>, <nil>, <nil>]\",\n\t\tfmt.Sprintf(\"[%p, %p, %p, %p]\", n0, n1, n2, n3),\n\t}\n\n\tset = Nodes{n0, n1, n2, n3}\n\tfor i, f := range formats {\n\t\ts := fmt.Sprintf(f, set)\n\t\tif s != correctFormats[i] {\n\t\t\tt.Errorf(\"Format %q. Expected %q. Got %q\", f, correctFormats[i], s)\n\t\t}\n\t}\n\n\t// corner cases\n\tset = Nodes{}\n\tif set.AllSameGraph() {\n\t\tt.Error(\"Empty list of nodes cannot be of the same graph!\")\n\t}\n\n\tnAbnormal := newNode(In(NewGraph()))\n\tset = Nodes{n0, n1, nAbnormal, n2}\n\tif set.AllSameGraph() {\n\t\tt.Error(\"One node is in a different graph! This should have returned false\")\n\t}\n}\n"
        },
        {
          "name": "compile.go",
          "type": "blob",
          "size": 17.6396484375,
          "content": "package gorgonia\n\nimport (\n\t\"encoding/csv\"\n\t\"fmt\"\n\t\"io\"\n\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\n// This file deals with the compilation from a expression graph into a program\n// that is executed by an interpreter\n\n// Compile takes a graph and outputs a program suitable for *tapeMachine to run\nfunc Compile(g *ExprGraph) (prog *program, locMap map[*Node]register, err error) {\n\tcompileLogf(\"Compiling\")\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tswitch {\n\tcase len(g.AllNodes()) == 0:\n\t\terr = errors.Errorf(\"Cannot compile an empty graph\")\n\t\treturn\n\tcase g.Inputs().Len() == 0:\n\t\terr = errors.Errorf(\"Cannot compile a graph that has no input nodes\")\n\t\treturn\n\t}\n\n\tcompileLogf(\"sorting\")\n\tvar sortedNodes Nodes\n\tif sortedNodes, err = Sort(g); err != nil {\n\t\treturn nil, nil, errors.Wrap(err, sortFail)\n\t}\n\treverseNodes(sortedNodes)\n\n\tdf := analyze(g, sortedNodes)\n\tsortedNodes = df.insertDeviceInstr(sortedNodes)\n\tdf.buildIntervals(sortedNodes)\n\n\tra := newRegalloc(df)\n\tra.alloc(sortedNodes)\n\n\t// debug related stuff\n\tdf.debugIntervals(sortedNodes)\n\tlogCompileState(g.name, g, df)\n\n\tinputs := g.Inputs()\n\tcg := newCodeGenerator(inputs, sortedNodes, df)\n\tprog, locMap = cg.gen()\n\tprog.cpulocs = ra.cpucount\n\tprog.gpulocs = ra.gpucount\n\tprog.cpumem = cg.cpumem\n\tprog.gpumem = cg.gpumem\n\tprog.df = df\n\tprog.g = g\n\tprog.sorted = sortedNodes\n\n\treturn\n}\n\n// CompileFunction takes a graph, subsets it based on the input and output nodes provided and outputs a program suitable for *tapeMachine to run.\n// It is analogous to theano.Function().\n// If some input nodes are not used or is not reachable, this function will return an error\nfunc CompileFunction(g *ExprGraph, inputs, outputs Nodes) (prog *program, locMap map[*Node]register, err error) {\n\tcompileLogf(\"CompileFunctionNEW. Inputs: %d; outputs: %d\", inputs, outputs)\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tsubgraph := g.ExactSubgraphRoots(outputs...)\n\tvar unused Nodes\n\tfor _, in := range inputs {\n\t\tif !subgraph.all.Contains(in) {\n\t\t\tunused = append(unused, in)\n\t\t}\n\t}\n\n\tif len(unused) > 0 {\n\t\treturn nil, nil, errors.Errorf(\"Not all the inputs are used: %v\", unused)\n\t}\n\n\tvar sortedNodes Nodes\n\tif sortedNodes, err = Sort(subgraph); err != nil {\n\t\treturn nil, nil, errors.Wrap(err, sortFail)\n\t}\n\treverseNodes(sortedNodes)\n\n\tdf := analyze(subgraph, sortedNodes)\n\tsortedNodes = df.insertDeviceInstr(sortedNodes)\n\tdf.buildIntervals(sortedNodes)\n\n\tra := newRegalloc(df)\n\tra.alloc(sortedNodes)\n\n\tcg := newCodeGenerator(inputs, sortedNodes, df)\n\tprog, locMap = cg.gen()\n\tprog.cpulocs = ra.cpucount\n\tprog.gpulocs = ra.gpucount\n\tprog.df = df\n\tprog.g = subgraph\n\tprog.sorted = sortedNodes\n\n\treturn\n}\n\n// codgenerator holds the state for the code generation process\ntype codegenerator struct {\n\tlocMap     map[*Node]register\n\tlastWrites map[register]*Node\n\tflushed    map[int]struct{}\n\tallocated  map[register]struct{}\n\tfreed      map[register]struct{}\n\tdeferFree  map[register]struct{}\n\tinstrMap   map[*Node]fragment\n\tqueue      []int // queue to flush\n\n\tlastReads map[register]int\n\n\tcpumem int64\n\tgpumem []int64\n\n\tg              *ExprGraph\n\tinputs, sorted Nodes\n\tdf             *dataflow\n\tinstructions   fragment\n}\n\nfunc newCodeGenerator(inputs, sorted Nodes, df *dataflow) *codegenerator {\n\treturn &codegenerator{\n\t\tlocMap:     make(map[*Node]register),\n\t\tlastWrites: make(map[register]*Node),\n\t\tflushed:    make(map[int]struct{}),\n\t\tallocated:  make(map[register]struct{}),\n\t\tfreed:      make(map[register]struct{}),\n\t\tdeferFree:  make(map[register]struct{}),\n\t\tinstrMap:   make(map[*Node]fragment),\n\t\tlastReads:  make(map[register]int),\n\n\t\tg:      inputs[0].g,\n\t\tinputs: inputs,\n\t\tsorted: sorted,\n\t\tdf:     df,\n\t}\n}\n\n// addInstr adds the instruction to the associated node in the instrMap.\n// when we add instructions to the node map, we also try to determine the size of the allocations required\nfunc (cg *codegenerator) addInstr(node *Node, instr tapeInstr) {\n\tif instrs := cg.instrMap[node]; instrs != nil {\n\t\tinstrs = append(instrs, instr)\n\t\tcg.instrMap[node] = instrs\n\t} else {\n\t\tcg.instrMap[node] = fragment{instr}\n\t}\n\n\tvar dt tensor.Dtype\n\tvar err error\n\tswitch inst := instr.(type) {\n\tcase loadArg:\n\t\tif dt, err = dtypeOf(node.t); err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\td := instr.writes().device\n\t\tif d != CPU {\n\t\t\tif len(cg.gpumem) < int(d)+1 {\n\t\t\t\tdiff := int(d) + 1 - len(cg.gpumem)\n\t\t\t\tcg.gpumem = append(cg.gpumem, make([]int64, diff)...)\n\t\t\t}\n\t\t}\n\n\t\tswitch d {\n\t\tcase CPU:\n\t\t\tcg.cpumem += calcMemSize(dt, node.Shape())\n\t\tdefault:\n\t\t\tcg.gpumem[int(d)] += calcMemSize(dt, node.Shape())\n\t\t}\n\tcase alloc:\n\t\tif dt, err = dtypeOf(inst.t); err != nil {\n\t\t\tpanic(err)\n\t\t}\n\n\t\td := instr.writes().device\n\t\tif d != CPU {\n\t\t\tif len(cg.gpumem) < int(d)+1 {\n\t\t\t\tdiff := int(d) + 1 - len(cg.gpumem)\n\t\t\t\tcg.gpumem = append(cg.gpumem, make([]int64, diff)...)\n\t\t\t}\n\t\t}\n\n\t\tswitch d {\n\t\tcase CPU:\n\t\t\tcg.cpumem += calcMemSize(dt, inst.s)\n\t\tdefault:\n\t\t\tcg.gpumem[int(d)] += calcMemSize(dt, inst.s)\n\t\t}\n\tcase *execOp:\n\t\tif !inst.op.ReturnsPtr() {\n\t\t\td := instr.writes().device\n\t\t\tif d != CPU {\n\t\t\t\tif len(cg.gpumem) < int(d)+1 {\n\t\t\t\t\tdiff := int(d) + 1 - len(cg.gpumem)\n\t\t\t\t\tcg.gpumem = append(cg.gpumem, make([]int64, diff)...)\n\t\t\t\t}\n\t\t\t}\n\t\t\tswitch d {\n\t\t\tcase CPU:\n\t\t\t\tcg.cpumem += inst.size\n\t\t\tdefault:\n\t\t\t\tcg.gpumem[int(d)] += inst.size\n\t\t\t}\n\t\t}\n\n\tdefault:\n\t\t// panic(\"EHLP\")\n\t}\n}\n\n// every time an instruction is added to the list of instructions,\n// also add the instructionID and the register the instruction writes to.\n// This helps with determining if a flushInstruction needs to be issued.\nfunc (cg *codegenerator) updateLastWrites(reg register, n *Node) {\n\tcg.lastWrites[reg] = n\n}\n\nfunc (cg *codegenerator) flush() {\n\tcompileLogf(\"Flushing\")\n\tfor _, instrID := range cg.queue {\n\t\tcg.flushed[instrID] = struct{}{}\n\t}\n\tcg.queue = cg.queue[:0]\n}\n\nfunc (cg *codegenerator) addArg(node *Node, interv *interval) {\n\tcompileLogf(\"LoadArg: %x\", node.ID())\n\twriteTo := interv.result\n\n\tcg.locMap[node] = writeTo\n\tinstr := loadArg{\n\t\t// index:   index,\n\t\tindex:   node.ID(),\n\t\twriteTo: writeTo,\n\t\tname:    node.Name(),\n\t}\n\t// cg.instructions = append(cg.instructions, instr)\n\n\tcg.addInstr(node, instr)\n\tcg.updateLastWrites(writeTo, node)\n}\n\nfunc (cg *codegenerator) addStmt(node *Node, interv *interval, i int) {\n\tcompileLogf(\"Add Statement\")\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\twriteTo := interv.result\n\n\tvar children Nodes\n\tvar ok bool\n\tif children, ok = cg.df.devTransChildren[node]; !ok {\n\t\tchildren = node.children\n\t}\n\n\tswitch op := node.op.(type) {\n\tcase letOp:\n\t\t// there should be only 2 chilren\n\t\tif len(children) != 2 {\n\t\t\tpanic(\"Expected only two children\")\n\t\t}\n\t\tcompileLogf(\"node.children %d. [1]: %v; [0]: %v\", node.ID(), children[1], children[0])\n\t\tcompileLogf(\"node isInput %v\", node.isInput())\n\t\tfrom := cg.df.intervals[children[1]].result\n\t\tto := cg.df.intervals[children[0]].result\n\n\t\tinstr := letInstr{\n\t\t\treadFrom: from,\n\t\t\twriteTo:  to,\n\t\t}\n\t\t// cg.instructions = append(cg.instructions, instr)\n\n\t\tcg.addInstr(node, instr)\n\t\tcg.updateLastWrites(writeTo, node)\n\tcase readOp:\n\t\t// there should be only 1 child\n\t\tif len(children) != 1 {\n\t\t\tpanic(\"Expected only one child\")\n\t\t}\n\t\tcompileLogf(\"node.children %d. [0]: %v\", node.ID(), children[0])\n\t\tcompileLogf(\"node isInput %v\", node.isInput())\n\t\tcompileLogf(\"node.children[0] Type %v, shape %v\", children[0].t, children[0].shape)\n\n\t\tif _, ok := cg.flushed[i]; !ok {\n\t\t\tcg.addInstr(node, flushInstr{})\n\t\t\tcg.flush()\n\t\t}\n\n\t\tfrom := cg.df.intervals[children[0]].result\n\t\tinstr := &readInstr{\n\t\t\tinto:     op.into,\n\t\t\treadFrom: from,\n\n\t\t\tt: children[0].t,\n\t\t\ts: children[0].shape,\n\t\t}\n\t\t// cg.instructions = append(cg.instructions, instr)\n\n\t\tcg.addInstr(node, instr)\n\t\tcg.updateLastWrites(writeTo, node)\n\tcase devTrans:\n\t\tif _, ok := cg.allocated[writeTo]; !ok {\n\t\t\t// insert new alloc\n\t\t\tvar instr alloc\n\t\t\tinstr = newAlloc(node, writeTo)\n\t\t\t// cg.instructions = append(cg.instructions, instr)\n\n\t\t\tcg.addInstr(node, instr)\n\t\t\tcg.updateLastWrites(writeTo, node)\n\t\t\tcg.queue = append(cg.queue, i)\n\t\t\tcg.allocated[writeTo] = struct{}{}\n\t\t}\n\n\t\tcompileLogf(\"devTrans\")\n\t\tif len(children) != 1 {\n\t\t\tpanic(\"Expected only one child\")\n\t\t}\n\n\t\tfrom := cg.df.intervals[children[0]].result\n\t\tto := cg.df.intervals[node].result\n\n\t\tinstr := deviceTransport{\n\t\t\tfrom: from, to: to,\n\t\t}\n\t\tcg.addInstr(node, instr)\n\n\t\tif op.from != CPU && op.to == CPU {\n\t\t\tinstrID := cg.sorted.index(op.toNode)\n\t\t\tif _, ok := cg.flushed[instrID]; !ok {\n\t\t\t\t// cg.instructions = append(cg.instructions, flushInstr{})\n\t\t\t\tcg.addInstr(node, flushInstr{})\n\t\t\t\tcg.flush()\n\t\t\t}\n\t\t}\n\t\tcg.updateLastWrites(writeTo, node)\n\n\t}\n}\n\nfunc (cg *codegenerator) addNode(node, replacement *Node, interv *interval, i int) {\n\tcompileLogf(\"AddNode: %x %v\", node.ID(), node.op)\n\tcompileLogf(\"interval %v\", interv)\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\twriteTo := interv.result\n\n\tvar reads []register\n\tvar children Nodes\n\tvar ok bool\n\tif children, ok = cg.df.devTransChildren[node]; !ok {\n\t\tchildren = node.children\n\t}\n\tfor _, child := range children {\n\t\tcReplacement := cg.df.replacements[child]\n\t\tcInterv := cg.df.intervals[cReplacement]\n\t\treads = append(reads, cInterv.result)\n\t}\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tvar prealloc bool\n\tvar useUnsafe bool\n\t// if it's not mutable, there is no chance it will be overwritten\n\tif node.isMutable() {\n\t\t// if the instruction calls an extern (cBLAS or cuBlas), then we should preallocate the vector\n\t\tif node.op.CallsExtern() {\n\t\t\tcompileLogf(\"calls extern\")\n\t\t\tif _, ok := cg.allocated[writeTo]; !ok {\n\t\t\t\tcompileLogf(\"Inserting new alloc\")\n\t\t\t\tvar instr alloc\n\t\t\t\tinstr = newAlloc(node, writeTo)\n\t\t\t\tcg.addInstr(node, instr)\n\t\t\t\tcg.updateLastWrites(writeTo, node)\n\n\t\t\t\tprealloc = true\n\n\t\t\t\tcg.queue = append(cg.queue, i)\n\t\t\t\tcg.allocated[writeTo] = struct{}{}\n\t\t\t}\n\t\t}\n\t}\n\tcompileLogf(\"Node Reads %v\", reads)\n\t// check if any previously buffered cBLAS or cuBLAS calls need to be flushed\n\t// it doesn't matter if the machine isn't using a batchedBLAS. flushInstr would just be a no-op at runtime\n\tfor _, read := range reads {\n\t\tif lastWriteNode, ok := cg.lastWrites[read]; ok {\n\t\t\tinstrID := cg.sorted.index(lastWriteNode)\n\t\t\tvar op Op\n\t\t\tvar onDev, nodeOnDev Device\n\n\t\t\t_, isDevTrans := lastWriteNode.Op().(devTrans)\n\t\t\tswitch {\n\t\t\tcase lastWriteNode.isArg(), lastWriteNode.isStmt && !isDevTrans:\n\t\t\t\tcontinue\n\t\t\tdefault:\n\t\t\t\top = lastWriteNode.op\n\t\t\t}\n\t\t\tswitch op.(type) {\n\t\t\tcase CUDADoer:\n\t\t\t\tonDev = Device(0)\n\t\t\tcase CLDoer:\n\t\t\t\tonDev = Device(0)\n\t\t\tdefault:\n\t\t\t\tonDev = CPU\n\t\t\t}\n\n\t\t\tswitch node.op.(type) {\n\t\t\tcase CUDADoer:\n\t\t\t\tnodeOnDev = Device(0)\n\t\t\tcase CLDoer:\n\t\t\t\tnodeOnDev = Device(0)\n\t\t\tdefault:\n\t\t\t\tnodeOnDev = CPU\n\t\t\t}\n\n\t\t\t// if we have sequential Extern calls,  we just add it to the batch.\n\t\t\t// sequential in this can mean several instructions apart. For example:\n\t\t\t//\t\t4 \tA × B \t; read %2\t; write to %3\n\t\t\t//\t\t \t⋮\t(doesn't use %3 or %10)\n\t\t\t//\t\t\t⋮\n\t\t\t//\t\t10  Aᵀ × B\t; read %3\t; write to %10\n\t\t\t//\t\t\t⋮\t(doesn't use %3, or %10)\n\t\t\t//\t\t\t⋮\n\t\t\t//\t\t12 \t+\t\t; read %10\t; write to %12\n\t\t\t//\n\t\t\t// It is before instruction 12 that the flush will be added. 4 and 10 are considered sequential\n\t\t\t//\n\t\t\t// It is not sequential when both are not the same devices\n\t\t\tswitch {\n\t\t\tcase !op.CallsExtern():\n\t\t\t\tcompileLogf(\"ToFlush: Node doesn't call extern. NO FLUSH\")\n\t\t\t\t// op doesn't call extern... don't bother flushing\n\t\t\tcase op.CallsExtern() && node.op.CallsExtern() && onDev == nodeOnDev && !isDevTrans:\n\t\t\t\tcompileLogf(\"ToFlush: Both calls extern, both same device. NO FLUSH\")\n\t\t\t\t// same device, both calls extern\n\t\t\t\t// no flush needed\n\t\t\tcase op.CallsExtern() && node.op.CallsExtern() && onDev != nodeOnDev:\n\t\t\t\tcompileLogf(\"ToFlush:  Differing devices\")\n\t\t\t\t// different devices, both calls extern\n\t\t\t\t// flush needed\n\t\t\t\tfallthrough\n\t\t\tcase op.CallsExtern() && !node.op.CallsExtern():\n\t\t\t\tcompileLogf(\"ToFlush: Node requires value immediately\")\n\t\t\t\t// node is gonna use the value immediately\n\t\t\t\t// flush needed\n\t\t\t\tfallthrough\n\t\t\tdefault:\n\t\t\t\tcompileLogf(\"ToFlush: FLUSH\")\n\t\t\t\tif _, ok := cg.flushed[instrID]; !ok {\n\t\t\t\t\t// cg.instructions = append(cg.instructions, flushInstr{})\n\t\t\t\t\tcg.addInstr(node, flushInstr{})\n\t\t\t\t\tcg.flush()\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// viaticum := cg.instructions[instrID] // ;) - it IS on the way\n\t\t\t// if instr, ok := viaticum.(*execOp); ok {\n\t\t\t// if op.CallsExtern() && !node.op.CallsExtern() {\n\t\t\t// }\n\t\t\t// }\n\t\t}\n\n\t\t// check the overwrites - if the overwrite and the resulting register is the same,\n\t\t// then use unsafe options when available\n\t\toverwrites := node.op.OverwritesInput()\n\t\tif overwrites >= 0 {\n\t\t\tcompileLogf(\"Overwrites %d\", overwrites)\n\t\t\toverwritten := reads[overwrites]\n\t\t\tcompileLogf(\"NodeID:%d overwritten: %v, reads: %v, interval: %v\", node.ID(), overwritten, interv.reads, interv.result)\n\t\t\tif overwritten == interv.result {\n\t\t\t\tcompileLogf(\"Use unsafe\")\n\t\t\t\tuseUnsafe = true\n\t\t\t}\n\t\t}\n\n\t}\n\n\tcg.locMap[node] = writeTo\n\n\t// otherwise, the replacement has already been written\n\tif node == replacement {\n\t\tcompileLogf(\"New Exec Op: %v\", node.op)\n\t\tinstr := newExecOp(node)\n\t\tinstr.readFrom = reads\n\t\tinstr.writeTo = writeTo\n\t\tinstr.preAllocated = prealloc\n\t\tinstr.useUnsafe = useUnsafe\n\n\t\t// cg.instructions = append(cg.instructions, instr)\n\t\tcg.addInstr(node, instr)\n\t\tcg.updateLastWrites(writeTo, node)\n\t}\n}\n\nfunc (cg *codegenerator) insertFree(instrID int, node *Node) {\n\tcompileLogf(\"Inserting Free for instrID %d | instr: %v | op: %v\", instrID, node, node.op)\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tvar reads []register\n\tvar children Nodes\n\tvar ok bool\n\tif children, ok = cg.df.devTransChildren[node]; !ok {\n\t\tchildren = node.children\n\t}\n\tfor _, child := range children {\n\t\tcReplacement := cg.df.replacements[child]\n\t\tcInterv := cg.df.intervals[cReplacement]\n\t\treads = append(reads, cInterv.result)\n\t}\n\tcompileLogf(\"reads %v\", reads)\n\n\t// check if anything needs to be freed\n\tfor _, read := range reads {\n\t\tvar readNode *Node\n\t\tfor n, reg := range cg.locMap {\n\t\t\tif reg == read {\n\t\t\t\tif readNode == nil {\n\t\t\t\t\treadNode = n\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif readNode.id < n.id {\n\t\t\t\t\treadNode = n\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// interv := cg.df.intervals[readNode]\n\t\treadRepl := cg.df.replacements[readNode]\n\t\tif readRepl == nil {\n\t\t\treadRepl = readNode\n\t\t}\n\t\tif readRepl == nil {\n\t\t\tcontinue\n\t\t}\n\t\tinterv := cg.df.intervals[readRepl]\n\t\tcompileLogf(\"interv for readRepl %v: %v\", readRepl, interv)\n\t\tlastUse := interv.lastUse()\n\t\tcompileLogf(\"Interval: %v; read: %v; Read Node %v; Op %v; LastUse %v; Instrid: %v\", interv, read, readNode, readNode.op, lastUse, instrID)\n\t\tif lastUse >= 0 && lastUse <= instrID && read.device != CPU {\n\t\t\tif _, ok := cg.freed[read]; !ok {\n\t\t\t\tcompileLogf(\"Adding Free %v. LastUse %d\", read, interv.lastUse())\n\t\t\t\t// cg.instructions = append(cg.instructions, free{read})\n\t\t\t\tcg.addInstr(node, free{read})\n\t\t\t\tcg.freed[read] = struct{}{}\n\t\t\t}\n\t\t}\n\t}\n\n\twrite := cg.locMap[node]\n\trepl := cg.df.replacements[node]\n\tinterv := cg.df.intervals[repl]\n\tcompileLogf(\"Node %v | write  %v | Last Use %v | %v\", node, write, interv.lastUse(), node.isRoot())\n\tif interv.lastUse() == -1 || interv.lastUse() >= len(cg.sorted) {\n\t\t// if node.isRoot() {\n\t\tcg.deferFree[write] = struct{}{}\n\t\t// return\n\t\t// }\n\n\t\t// otherwise, it's essentially a NOOP, so we free the memory immediately after the Op is executed\n\t\t// TODO: do NO-OP optimizations\n\t\t// if _, ok := cg.freed[write]; !ok {\n\t\t// \tcompileLogf(\"Adding Free %v. Last Use %d\", write, interv.lastUse())\n\t\t// \tcg.addInstr(node, free{write})\n\t\t// \tcg.freed[write] = struct{}{}\n\t\t// }\n\t}\n}\n\nfunc (cg *codegenerator) insertLastFrees() int {\n\tnode := cg.sorted[len(cg.sorted)-1]\n\tvar instructionsAdded int\n\tfor reg := range cg.deferFree {\n\t\tif _, ok := cg.freed[reg]; !ok {\n\t\t\tcompileLogf(\"Adding Free %v to the final instruction\", reg)\n\t\t\tcg.addInstr(node, free{reg})\n\t\t\tinstructionsAdded++\n\t\t}\n\t}\n\treturn instructionsAdded\n}\n\nfunc (cg *codegenerator) gen() (*program, map[*Node]register) {\n\tcompileLogf(\"Generating from SORTED: %v\", cg.sorted)\n\tenterLogScope()\n\tdefer leaveLogScope()\n\tfor i, node := range cg.sorted {\n\t\t// for i := len(cg.sorted) - 1; i ⩾ 0; i-- {\n\t\t// node := cg.sorted[i]\n\t\treplacement := cg.df.replacements[node]\n\t\tcompileLogf(\"Working on %x. Replacement: %x\", node.ID(), replacement.ID())\n\n\t\tnInterv := cg.df.intervals[replacement]\n\t\tswitch {\n\t\tcase node.isArg():\n\t\t\tcg.addArg(node, nInterv)\n\t\tcase node.isStmt:\n\t\t\tcg.addStmt(node, nInterv, i)\n\t\tdefault:\n\t\t\tcg.addNode(node, replacement, nInterv, i)\n\t\t}\n\t}\n\n\tvar instructionCount int\n\tfor i := len(cg.sorted) - 1; i >= 0; i-- {\n\t\tnode := cg.sorted[i]\n\t\tcg.insertFree(i, node)\n\n\t\tinstructionCount += len(cg.instrMap[node])\n\t}\n\n\tinstructionCount += cg.insertLastFrees()\n\n\tr := make(map[int64]*Node)\n\tcg.instructions = make(fragment, 0, instructionCount)\n\tfor _, node := range cg.sorted {\n\t\tinstrs := cg.instrMap[node]\n\t\tcg.instructions = append(cg.instructions, instrs...)\n\t\tfor _, in := range instrs {\n\t\t\tr[in.ID()] = node\n\t\t}\n\t}\n\n\treturn &program{\n\t\tinstructions: cg.instructions,\n\t\targs:         len(cg.inputs),\n\t\tg:            cg.g,\n\t\tm:            cg.instrMap,\n\t\tr:            r,\n\t}, cg.locMap\n}\n\nfunc compileState(w io.Writer, g *ExprGraph, df *dataflow) {\n\theader := []string{\n\t\t\"ID\", \"Op\", \"Type\", \"Register\", \"Interval\", \"Used By\", \"Uses\", \"Overwrites\", \"BLAS?\",\n\t}\n\n\tvar rows [][]string\n\tfor _, n := range g.AllNodes() {\n\t\tinterv := df.intervals[n]\n\n\t\trow := make([]string, len(header))\n\t\trow[0] = fmt.Sprintf(\"%d\", n.ID())\n\t\trow[2] = fmt.Sprintf(\"%s\", n.t)\n\t\trow[3] = fmt.Sprintf(\"%s\", interv.result)\n\t\trow[4] = fmt.Sprintf(\"%d - %d\", interv.start, interv.end)\n\t\trow[5] = fmt.Sprintf(\"%v\", interv.usePositions)\n\t\trow[6] = fmt.Sprintf(\"%d\", n.children)\n\n\t\tif n.op != nil {\n\t\t\trow[1] = fmt.Sprintf(\"%s\", n.op)\n\t\t\toverwrites := n.op.OverwritesInput()\n\t\t\tif overwrites >= 0 {\n\t\t\t\trow[7] = fmt.Sprintf(\"%d\", n.children[overwrites].ID())\n\t\t\t}\n\n\t\t\tif n.op.CallsExtern() {\n\t\t\t\trow[8] = \"yes\"\n\t\t\t}\n\t\t}\n\n\t\trows = append(rows, row)\n\t}\n\tcw := csv.NewWriter(w)\n\tcw.Comma = ';'\n\t// TODO: Check errors on writes here.\n\tcw.Write(header)\n\tcw.WriteAll(rows)\n}\n"
        },
        {
          "name": "compile_test.go",
          "type": "blob",
          "size": 3.375,
          "content": "package gorgonia\n\nimport \"testing\"\n\nfunc TestCompile_medium(t *testing.T) {\n\tg := NewGraph()\n\tx := NewMatrix(g, Float64, WithShape(20, 20), WithName(\"x\"))\n\ty := NewMatrix(g, Float64, WithShape(20, 20), WithName(\"y\"))\n\txpy := Must(Add(x, y))\n\txmy := Must(Sub(x, y))\n\txpys := Must(Slice(xpy, S(0, 10)))\n\tMust(Square(xpys))\n\txmy2 := Must(Square(xmy))\n\n\tvar final Value\n\tSet(xmy2, xpy)\n\tRead(xmy2, &final)\n\n\tprog, _, err := Compile(g)\n\tif err != nil {\n\t\tt.Fatalf(\"error while compiling: %v\", err)\n\t}\n\tt.Log(prog)\n\n\tonDev := xpy.Device() != CPU\n\n\t// leakage test\n\tif onDev {\n\t\treg0 := register{device: Device(0), id: 0}\n\t\treg1 := register{device: Device(0), id: 1}\n\t\treg2 := register{device: Device(0), id: 2}\n\n\t\tif !prog.instructions.has(free{reg0}) {\n\t\t\tt.Error(\"Expected GPU(0)0 to be freed\")\n\t\t}\n\n\t\tif !prog.instructions.has(free{reg1}) {\n\t\t\tt.Error(\"Expected GPU(0)1 to be freed\")\n\t\t}\n\n\t\tif !prog.instructions.has(free{reg2}) {\n\t\t\tt.Error(\"Expected GPU(0)2 to be freed\")\n\t\t}\n\t}\n\n\t// position tests\n\tif onDev {\n\t\t// last two instructions should be free\n\t\tif _, ok := prog.instructions[len(prog.instructions)-1].(free); !ok {\n\t\t\tt.Error(\"Expected last instruction to be a Free\")\n\t\t}\n\t\tif _, ok := prog.instructions[len(prog.instructions)-2].(free); !ok {\n\t\t\tt.Error(\"Expected second last instruction to be a Free\")\n\t\t}\n\n\t\t// frag = prog.m[set]\n\t\t// if _, ok := frag[len(frag)-1].(free); !ok {\n\t\t// \tt.Error(\"Expected a `free` instruction after LET\")\n\t\t// }\n\n\t\t// frag = prog.m[read]\n\t\t// if _, ok := frag[len(frag)-2].(free); !ok {\n\t\t// \tt.Error(\"Expected a `free` instruction after READ\")\n\t\t// }\n\t}\n}\n\nfunc TestCompile_CompileFn(t *testing.T) {\n\tg := NewGraph()\n\tx := NewScalar(g, Float32, WithName(\"x\"))\n\ty := NewScalar(g, Float32, WithName(\"y\"))\n\txpy := Must(Add(x, y))\n\txmy := Must(Mul(x, y))\n\tx2 := Must(Square(x))\n\n\tprogAll, _, err := Compile(g)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tprogAdd, _, err := CompileFunction(g, Nodes{x, y}, Nodes{xpy})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tprogMul, _, err := CompileFunction(g, Nodes{x, y}, Nodes{xmy})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, _, err = CompileFunction(g, Nodes{x, y}, Nodes{x2}); err == nil {\n\t\tt.Error(\"expected an error when there is an unused node\")\n\t}\n\n\t// properties based testing\n\tif len(progAll.sorted) <= len(progAdd.sorted) || len(progAll.sorted) <= len(progMul.sorted) {\n\t\tt.Error(\"progAll should have more nodes included than progAdd or progMul\")\n\t}\n\n\tif len(progAll.instructions) <= len(progAdd.instructions) || len(progAll.instructions) <= len(progMul.instructions) {\n\t\tt.Error(\"progAll should have more instructions than either progAdd or progMul\")\n\t}\n\n\t// really this is more checking of the subgraphing\n\tif !progAdd.sorted.Contains(x) {\n\t\tt.Error(\"Expected progAdd to contain x\")\n\t}\n\tif !progAdd.sorted.Contains(y) {\n\t\tt.Error(\"Expected progAdd to contain y\")\n\t}\n\tif !progAdd.sorted.Contains(xpy) {\n\t\tt.Error(\"Expected progAdd to contain xpy\")\n\t}\n\tif progAdd.sorted.Contains(xmy) || progAdd.sorted.Contains(x2) {\n\t\tt.Error(\"Expected progAdd to not contain either x2 or xmy\")\n\t}\n\n\t// same as above\n\tif !progMul.sorted.Contains(x) {\n\t\tt.Error(\"Expected progMul to contain x\")\n\t}\n\tif !progMul.sorted.Contains(y) {\n\t\tt.Error(\"Expected progMul to contain y\")\n\t}\n\tif !progMul.sorted.Contains(xmy) {\n\t\tt.Error(\"Expected progMul to contain xmy\")\n\t}\n\tif progMul.sorted.Contains(xpy) || progMul.sorted.Contains(x2) {\n\t\tt.Error(\"Expected progMul to not contain either x2 or xpy\")\n\t}\n}\n"
        },
        {
          "name": "complex_test.go",
          "type": "blob",
          "size": 3.220703125,
          "content": "package gorgonia\n\nimport (\n\t\"runtime/debug\"\n\t\"testing\"\n)\n\nfunc TestWeirdNetwork(t *testing.T) {\n\tconst (\n\t\tembeddingDims = 50\n\t\thiddenSize    = 200\n\n\t\txs     = 64\n\t\txFeats = 20\n\n\t\tps     = 20\n\t\tpFeats = 10\n\n\t\tqs     = 50\n\t\tqFeats = 12\n\n\t\toutSize = 10\n\t)\n\tvar err error\n\n\tg := NewGraph()\n\tvar x *Node // NewVector(g, Float64, WithShape(xFeats*embeddingDims), WithName(\"x\"), WithInit(Zeroes()))\n\tvar p *Node\n\tvar q *Node\n\n\teX := NewMatrix(g, Float64, WithShape(xs, embeddingDims), WithName(\"x embeddings\"), WithInit(GlorotU(1)))\n\teP := NewMatrix(g, Float64, WithShape(ps, embeddingDims), WithName(\"p embeddings\"), WithInit(GlorotU(1)))\n\teQ := NewMatrix(g, Float64, WithShape(qs, embeddingDims), WithName(\"q embeddings\"), WithInit(GlorotU(1)))\n\tw0X := NewMatrix(g, Float64, WithShape(hiddenSize, xFeats*embeddingDims), WithName(\"layer0 weights for x\"), WithInit(GlorotU(1)))\n\tw0P := NewMatrix(g, Float64, WithShape(hiddenSize, pFeats*embeddingDims), WithName(\"layer0 weights for p\"), WithInit(GlorotU(1)))\n\tw0Q := NewMatrix(g, Float64, WithShape(hiddenSize, qFeats*embeddingDims), WithName(\"layer0 weights for q\"), WithInit(GlorotU(1)))\n\tb := NewVector(g, Float64, WithShape(hiddenSize), WithName(\"bias\"), WithInit(Zeroes()))\n\tw1 := NewMatrix(g, Float64, WithShape(outSize, hiddenSize), WithName(\"layer 1\"), WithInit(GlorotU(1)))\n\n\tmodel := Nodes{eX, eP, eQ, w0X, w0P, w0Q, b, w1}\n\n\t/* SET UP NEURAL NETWORK */\n\n\tslicesX := make(Nodes, xFeats)\n\tslicesP := make(Nodes, pFeats)\n\tslicesQ := make(Nodes, qFeats)\n\n\tfor i := 0; i < xFeats; i++ {\n\t\tif slicesX[i], err = Slice(eX, S(i)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tfor i := 0; i < pFeats; i++ {\n\t\tif slicesP[i], err = Slice(eP, S(i)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tfor i := 0; i < qFeats; i++ {\n\t\tif slicesQ[i], err = Slice(eQ, S(i)); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tif x, err = Concat(0, slicesX...); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif p, err = Concat(0, slicesP...); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif q, err = Concat(0, slicesQ...); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar wx, wp, wq *Node\n\tif wx, err = Mul(w0X, x); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif wp, err = Mul(w0P, p); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif wq, err = Mul(w0Q, q); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// add all them layers\n\tvar add0, add1, add2 *Node\n\tif add0, err = Add(wx, wp); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif add1, err = Add(add0, wq); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif add2, err = Add(add1, b); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// activate\n\tvar act0 *Node\n\tif act0, err = Cube(add2); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// layer 1\n\tvar layer1 *Node\n\tif layer1, err = Mul(w1, act0); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// activate\n\tvar logProb *Node\n\tif logProb, err = SoftMax(layer1); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar cost *Node\n\tif cost, err = Slice(logProb, S(0)); err != nil { // dummy slice\n\t\tt.Fatal(err)\n\t}\n\n\t// backprop\n\tif _, err = Grad(cost, model...); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t/* SET UP COMPLETE */\n\n\tm := NewTapeMachine(g, BindDualValues(model...))\n\tdefer m.Close()\n\n\t// for debug purposes\n\t// prog, locMap, err := Compile(g)\n\t// log.Println(prog)\n\n\t// for i := 0; i < 104729; i++ {\n\tfor i := 0; i < 2; i++ {\n\t\tif err = m.RunAll(); err != nil {\n\t\t\tt.Errorf(\"%d %v\", i, err)\n\t\t\tt.Log(string(debug.Stack()))\n\n\t\t\tbreak\n\t\t}\n\n\t\tm.Reset()\n\t}\n\n}\n"
        },
        {
          "name": "concurrency.go",
          "type": "blob",
          "size": 0.71484375,
          "content": "package gorgonia\n\nimport \"runtime\"\n\nconst (\n\t// defaultBlockSize indicates the default size to chunk an array.\n\tdefaultBlockSize = 64\n\n\t// minParallelBlocks indicates how many blocks an array must be split up into\n\t// before we decide to parallelize\n\tminParallelBlocks = 4\n)\n\n// workersChan creates a channel that is limited by the number of processor cores.\n// To signal that a processor core is being used:\n// \tch <- struct{}{}\n// When done:\n// \t<- ch\nfunc workersChan() chan struct{} { return make(chan struct{}, runtime.GOMAXPROCS(0)) }\n\n// it's just a generic ceiling function. Added here to avoid mixing with any potential ceilInt operation\nfunc calcBlocks(n, maxThreads int) int {\n\treturn (n + maxThreads - 1) / maxThreads\n}\n"
        },
        {
          "name": "const.go",
          "type": "blob",
          "size": 4.197265625,
          "content": "package gorgonia\n\nimport (\n\t\"math\"\n\n\t\"gorgonia.org/tensor\"\n)\n\nconst (\n\t// graphviz name for a full graph\n\tfullGraphName = \"fullGraph\"\n\n\t// group names\n\texprgraphClust = \"expressionGraph\"\n\tconstantsClust = \"constants\"\n\tinputsClust    = \"inputs\"\n\tgradClust      = \"gradients\"\n\tstrayClust     = \"undifferentiated nodes\"\n\n\t// subgraphs to rank the same\n\toutsideSubG = \"outsides\"\n\tinputConsts = \"inputConsts\"\n\n\t// special nodes for graphviz hacking\n\toutsideRoot   = \"outsideRoot\"\n\toutsideInputs = \"outsideInputs\"\n\tinsideInputs  = \"insideInputs\"\n\toutsideConsts = \"outsideConsts\"\n\tinsideConsts  = \"insideConsts\"\n\toutsideExprG  = \"outsideExprG\"\n\tinsideExprG   = \"insideExprG\"\n\toutsideGrads  = \"outsideGrads\"\n\tinsideGrads   = \"insideGrads\"\n\n\t// error messages\n\tsortFail            = \"Failed to sort\"\n\tcloneFail           = \"Failed to carry clone(%v)\"\n\tclone0Fail          = \"Failed to carry clone0()\"\n\tnyiTypeFail         = \"%s not yet implemented for %T\"\n\tnyiFail             = \"%s not yet implemented for %v\"\n\tdtypeOfFail         = \"Failed to carry dtypeOf()\"\n\tmulFail             = \"Failed to carry Mul()\"\n\tapplyOpFail         = \"Failed to carryApplyOp()\"\n\topDoFail            = \"Failed to carry op.Do()\"\n\tbinOpDoFail         = \"Failed to carry binOp.Do()\"\n\tbinOpNodeFail       = \"Failed to carry binary operation %T\"\n\tapplyFail           = \"Failed to carry Apply()\"\n\tbinOpFail           = \"Binary operator received %d arguments\"\n\thadamardProdFail    = \"Failed to carry hadamardProd()\"\n\thadamardDivFail     = \"Failed to carry hadamardDiv()\"\n\tcubeFail            = \"Failed to carry cube()\"\n\tnegFail             = \"Failed to carry Neg()\"\n\tinvFail             = \"Failed to carry Inv()\"\n\tpointWiseMulFail    = \"Failed to carry PointWiseMul()\"\n\tpointWiseSquareFail = \"Failed to carry PointWiseSquare()\"\n\tclampFail           = \"Failed to carry Clamp()\"\n\tinvSqrtFail         = \"Failed to carry InvSqrt()\"\n\tsubFail             = \"Failed to carry Sub()\"\n\taddFail             = \"Failed to carry Add()\"\n\tsignFail            = \"Failed to carry Sign()\"\n\tsoftplusFail        = \"Failed to carry Softplus()\"\n\tincrErr             = \"increment couldn't be done. Safe op was performed instead\"\n\tbindFail            = \"Failed to bind\"\n\tanyToValueFail      = \"Failed to convert %v(%T) into a Value\"\n\tdtypeExtractionFail = \"Failed to extract dtype from %v\"\n\toperationError      = \"Operation failed\"\n\tdoFail              = \"Doing %v failed\"\n\tunsafeDoFail        = \"UnsafeDoing %v failed.\"\n\ttFail               = \"Failed to transpose Tensor\"\n\trepFail             = \"Failed to repeat Tensor along %d %d times\"\n\treshapeFail         = \"Failed to reshape Tensor into %v. DataSize was: %d\"\n\tsliceFail           = \"Failed to slice Tensor with %v\"\n\texecFail            = \"Failed to execute %v in node %v\"\n\tautodiffFail        = \"Failed to differentiate %v\"\n\tundefinedOnShape    = \"%v undefined on shape %v\"\n\tunsupportedDtype    = \"dtype %v is not yet supported\"\n\tgradOnDeviceFail    = \"Cannot get gradient of %v on %v\"\n\tmakeValueFail       = \"Unable to make value of %v with shape %v\"\n\tallocFail           = \"Unable to allocate %v bytes on %v\"\n\n\tshapeMismatchErr = \"Shape Mismatch. Expected %v. Got %v instead.\"\n)\n\nvar empty struct{}\n\nvar (\n\tonef32     = NewConstant(float32(1.0))\n\tonef64     = NewConstant(float64(1.0))\n\toneMoref32 = NewConstant(float32(1.0 + 1e-16))\n\toneMoref64 = NewConstant(float64(1.0 + 1e-16))\n\tzerof32    = NewConstant(float32(0.0))\n\tzerof64    = NewConstant(float64(0.0))\n\ttwof64     = NewConstant(float64(2.0))\n\ttwof32     = NewConstant(float32(2.0))\n\tthreef64   = NewConstant(float64(3.0))\n\tthreef32   = NewConstant(float32(3.0))\n\tln2f64     = NewConstant(math.Ln2)\n\tln2f32     = NewConstant(float32(math.Ln2))\n\n\tonef32ConstOp  = onef32.op.(constant)\n\tonef64ConstOp  = onef64.op.(constant)\n\tzerof32ConstOp = zerof32.op.(constant)\n\tzerof64ConstOp = zerof64.op.(constant)\n\n\tconstmap map[string]map[tensor.Dtype]*Node\n)\n\nvar oneone = tensor.Shape{1, 1}\n\nfunc init() {\n\tconstmap = map[string]map[tensor.Dtype]*Node{\n\t\t\"zero\": {\n\t\t\tFloat32: zerof32,\n\t\t\tFloat64: zerof64,\n\t\t},\n\t\t\"one\": {\n\t\t\tFloat32: onef32,\n\t\t\tFloat64: onef64,\n\t\t},\n\t\t\"two\": {\n\t\t\tFloat32: twof32,\n\t\t\tFloat64: twof64,\n\t\t},\n\t\t\"three\": {\n\t\t\tFloat32: threef32,\n\t\t\tFloat64: threef64,\n\t\t},\n\t\t\"log2\": {\n\t\t\tFloat32: ln2f32,\n\t\t\tFloat64: ln2f64,\n\t\t},\n\t}\n\n}\n"
        },
        {
          "name": "cuda modules",
          "type": "tree",
          "content": null
        },
        {
          "name": "cuda.go",
          "type": "blob",
          "size": 9.169921875,
          "content": "// +build cuda\n\npackage gorgonia\n\n// for non-cuda builds, look at noextern.go\n\nimport (\n\t\"log\"\n\t\"sync\"\n\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/cu\"\n\tcudnn \"gorgonia.org/cu/dnn\"\n\t\"gorgonia.org/gorgonia/cuda\"\n\t\"gorgonia.org/tensor\"\n)\n\n// CUDA tells the package that CUDA is used\nconst CUDA = true\n\nvar (\n\t_ External    = &ExternMetadata{}\n\t_ CUDAMachine = &tapeMachine{}\n\t_ CUDAMachine = &lispMachine{}\n)\n\nconst (\n\t// Any address of a variable residing in global memory or returned by one of the\n\t// memory allocation routines from the driver or runtime API is always aligned to at\n\t// least 256 bytes.\n\t//\n\tmemalign    = 32\n\tscalarAlign = 8\n)\n\n//go:generate cudagen -same-module\n\nvar cudaStdLib []cudaLib\n\ntype cudaLib struct {\n\tname  string\n\tdata  string\n\tfuncs []string\n}\n\n// CUDAMachine is a representation of CUDA capable VMs.\ntype CUDAMachine interface {\n\tExternal\n\tEngines() []cuda.Engine\n\tContexts() []*cu.BatchedContext\n\tCUDNNContexts() []*cudnn.Context\n\n\tElemGridSize(n, dev int) (gridDimX, gridDimY, gridDimZ, blockDimX, blockDimY, blockDimZ int)\n}\n\n// ExternMetadata holds any metadata for CUDA related stuff.\n// The slices in there are indexed by deviceID\ntype ExternMetadata struct {\n\ttensor.Engine\n\tsync.Mutex\n\n\t// operational stuff\n\tu cu.Device   // device currently in use\n\tb batchedBLAS // UNUSED\n\n\tengines       []cuda.Engine\n\tworkAvailable chan bool\n\tsyncChan      chan struct{}\n\tinitialized   bool\n}\n\n// ElemGridSize calculates the gridsize for elementwise operations\nfunc (m *ExternMetadata) ElemGridSize(n, dev int) (gridDimX, gridDimY, gridDimZ, blockDimX, blockDimY, blockDimZ int) {\n\tif dev >= len(m.engines) {\n\t\t// error\n\t}\n\treturn m.engines[dev].ElemGridSize(n)\n}\n\n// WorkAvailable returns a channel of empty struct, which is used to signal to the VM when there is work available. The VM will then call the DoWork method\nfunc (m *ExternMetadata) WorkAvailable() <-chan bool { return m.workAvailable }\n\n// Sync the channels\nfunc (m *ExternMetadata) Sync() chan struct{} { return m.syncChan }\n\n// DoWork flushes any batched cgo calls. In this build it flushes any batched CUDA calls and any batched CBLAS calls.\nfunc (m *ExternMetadata) DoWork() error {\n\tfor _, e := range m.engines {\n\t\tif err := e.DoWork(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// Engines ...\nfunc (m *ExternMetadata) Engines() []cuda.Engine { return m.engines }\n\n// Contexts return a slice of contexts that is being used by this CUDAMachine\nfunc (m *ExternMetadata) Contexts() []*cu.BatchedContext {\n\tretVal := make([]*cu.BatchedContext, 0, len(m.engines))\n\tfor _, e := range m.engines {\n\t\tretVal = append(retVal, e.Context())\n\t}\n\treturn retVal\n}\n\n// CUDNNContexts returns the CUDNN context\nfunc (m *ExternMetadata) CUDNNContexts() []*cudnn.Context {\n\tretVal := make([]*cudnn.Context, 0, len(m.engines))\n\tfor _, e := range m.engines {\n\t\tretVal = append(retVal, e.CUDNNContext())\n\t}\n\treturn retVal\n}\n\n// Get gets a previously allocated memory slab of the provided size. If no memories of that size exist,\n// it returns a NoOpError. The caller is then responsible for allocating the memory themselves.\nfunc (m *ExternMetadata) Get(dev Device, size int64) (tensor.Memory, error) {\n\td := int(dev)\n\tif d >= len(m.engines) {\n\t\treturn nil, noopError{} // this should not be a noopError\n\t}\n\treturn m.engines[dev].Get(size)\n}\n\n// GetFromValue allocates a memory on the GPU, and then copies the data over. v MUST be on CPU.\nfunc (m *ExternMetadata) GetFromValue(dev Device, v Value) (tensor.Memory, error) {\n\td := int(dev)\n\tif d >= len(m.engines) {\n\t\treturn nil, noopError{}\n\t}\n\tmemsize := calcMemSize(v.Dtype(), v.Shape())\n\n\tmem, err := m.engines[dev].Get(memsize)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tptr := cu.DevicePtr(mem.Uintptr())\n\tctx := m.engines[dev].Context()\n\tctx.MemcpyHtoD(ptr, valueToPointer(v), memsize)\n\treturn cu.DevicePtr(ptr), nil\n}\n\n// Put puts a previously allocated memory slab of the provided size back into the pool\nfunc (m *ExternMetadata) Put(dev Device, mem tensor.Memory, size int64) {\n\td := int(dev)\n\tif d >= len(m.engines) {\n\t\treturn // wat??\n\t}\n\n\tm.engines[dev].Put(mem, size)\n}\n\n// PutValue puts a previously allocated memory slab back into the pool\nfunc (m *ExternMetadata) PutValue(dev Device, v Value) {\n\td := int(dev)\n\tif d >= len(m.engines) {\n\t\treturn\n\t}\n\tmemsize := calcMemSize(v.Dtype(), v.Shape())\n\tm.engines[dev].Put(v, memsize)\n}\n\n// Transfer transfers data from device to device.\nfunc (m *ExternMetadata) Transfer(toDev, fromDev Device, v Value, synchronous bool) (retVal Value, err error) {\n\tdefer func() {\n\t\tif synchronous {\n\t\t\tm.Signal()\n\t\t}\n\t}()\n\n\tmemsize := calcMemSize(v.Dtype(), v.Shape())\n\tswitch {\n\tcase fromDev == CPU && toDev != CPU:\n\t\td := int(toDev)\n\t\tif d > len(m.engines) {\n\t\t\treturn nil, errors.Errorf(\"No context for ToDev\")\n\t\t}\n\n\t\tctx := m.engines[d].Context()\n\t\tvar mem tensor.Memory\n\t\tif mem, err = m.Get(toDev, memsize); err != nil {\n\t\t\treturn\n\t\t}\n\t\tctx.MemcpyHtoD(cu.DevicePtr(mem.Uintptr()), valueToPointer(v), memsize)\n\t\treturn makeValueFromMem(TypeOf(v), v.Shape(), mem)\n\n\tcase fromDev != CPU && toDev == CPU:\n\t\td := int(fromDev)\n\t\tif d > len(m.engines) {\n\t\t\treturn nil, errors.Errorf(\"No context for FromDev\")\n\t\t}\n\n\t\tctx := m.engines[d].Context()\n\t\tif retVal, err = makeValue(TypeOf(v), v.Shape()); err != nil {\n\t\t\treturn\n\t\t}\n\t\tctx.MemcpyDtoH(valueToPointer(retVal), cu.DevicePtr(v.Uintptr()), memsize)\n\t\treturn\n\tcase fromDev == toDev:\n\t\treturn v, nil\n\tcase fromDev != toDev && fromDev != CPU && toDev != CPU:\n\n\t}\n\tpanic(\"Unreachable\")\n}\n\n// Signal sends a signal down the workavailable channel, telling the VM to call the DoWork method. Signal is a synchronous method\nfunc (m *ExternMetadata) Signal() {\n\tif m.workAvailable != nil {\n\t\tm.signal()\n\t\t<-m.syncChan\n\t}\n}\n\n// Reset frees all the memories, and coalesces the allocator\nfunc (m *ExternMetadata) Reset() {\n\tfor i := range m.engines {\n\t\tm.engines[i].ResetAllocator()\n\t}\n}\n\nfunc (m *ExternMetadata) init(sizes []int64) (err error) {\n\tm.Lock()\n\tinitialized := m.initialized\n\tm.Unlock()\n\tif initialized {\n\t\treturn nil\n\t}\n\tdevices, err := cu.NumDevices()\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"Failed to get number of devices\")\n\t}\n\n\tif devices == 0 {\n\t\treturn errors.New(\"No Devices Found\")\n\t}\n\n\tcudaLogf(\"Creating Engines\")\n\tm.Lock()\n\tdefer m.Unlock()\n\tm.engines = make([]cuda.Engine, len(sizes))\n\tfor i := range m.engines {\n\t\te := &m.engines[i]\n\t\tdev, err := cu.GetDevice(i)\n\t\tif err != nil {\n\t\t\treturn errors.Wrapf(err, \"Failed to get device %d\", i)\n\t\t}\n\n\t\tif err = e.Init(dev, sizes[i]); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx := e.Context()\n\t\tgo m.collectWork(i, ctx.WorkAvailable())\n\t}\n\n\tm.initialized = true\n\tcudaLogf(\"CUDA initialized. Engines: %v\", m.engines)\n\treturn nil\n}\n\nfunc (m *ExternMetadata) initFail() {\n\tcudaLogf(\"Cleanup\")\n\tm.engines = nil\n\n\tif m.workAvailable != nil {\n\t\tclose(m.workAvailable)\n\t}\n\tm.workAvailable = nil\n}\n\n// cleanup cleans up the ancillary allocations made during the calling of batched CUDA functions.\nfunc (m *ExternMetadata) cleanup() {\n\tfor _, e := range m.engines {\n\t\te.Close()\n\t}\n}\n\n// collectWork is a muxer for all the channels for the different devices\nfunc (m *ExternMetadata) collectWork(devID int, workAvailable <-chan struct{}) {\n\tfor range workAvailable {\n\t\tm.workAvailable <- false\n\t}\n}\n\n// collectBLASWork is a muxer for CBLAS/CuBLAS (if any) and the devices\nfunc (m *ExternMetadata) collectBLASWork() {}\n\nfunc (m *ExternMetadata) signal() { m.workAvailable <- true }\n\nfunc (m *ExternMetadata) setEngine(e tensor.Engine) {}\n\n// AddToStdLib allows for custom ops to be included into the \"stdlib\" of CUDA functions, so that when the VMs are created, they're loaded automatically\n// without having to specify extra loading.\nfunc AddToStdLib(name, data string, funcs []string) {\n\tcudaStdLib = append(cudaStdLib, cudaLib{\n\t\tname:  name,\n\t\tdata:  data,\n\t\tfuncs: funcs,\n\t})\n}\n\nfunc init() {\n\tlog.Println(\"Using CUDA build\")\n}\n\n// ValueOnDevice gets the value of the node as a Value but on the desired device. If the node's valud is not on the same device\n// as the desired device, a copy will be made.\nfunc (n *Node) ValueOnDevice(toDev Device, extern External) (retVal Value, allocOnExtern bool, err error) {\n\tif n.dataOn == toDev {\n\t\treturn n.Value(), false, nil\n\t}\n\tv := n.Value()\n\tfromDev := n.Device()\n\n\tvar synchronous bool\n\tif toDev == CPU {\n\t\tsynchronous = true\n\t}\n\tif toDev != fromDev && toDev != CPU {\n\t\tallocOnExtern = true\n\t}\n\tretVal, err = extern.Transfer(toDev, fromDev, v, synchronous)\n\treturn\n}\n\n// GradOnDevice gets the gradient value of the node as a Value but on the desired device. If the node's valud is not on the same device\n// as the desired device, a copy will be made.\nfunc (n *Node) GradOnDevice(toDev Device, extern External) (retVal Value, allocOnExtern bool, err error) {\n\tif n.dataOn == toDev {\n\t\tretVal, err = n.Grad()\n\t\treturn\n\t}\n\n\tvar d Value\n\tif dv, ok := n.boundTo.(*dualValue); ok {\n\t\td = dv.d\n\t} else if n.deriv != nil {\n\t\treturn n.deriv.ValueOnDevice(toDev, extern)\n\t} else {\n\t\treturn nil, false, errors.Errorf(\"No gradient node/value found for %v\", n)\n\t}\n\tif d == nil {\n\t\treturn nil, false, errors.Errorf(\"No gradient node/value found for %v\", n)\n\t}\n\n\tfromDev := n.Device()\n\n\tvar synchronous bool\n\tif toDev == CPU {\n\t\tsynchronous = true\n\t}\n\tif toDev != CPU && toDev != fromDev {\n\t\tallocOnExtern = true\n\t}\n\tretVal, err = extern.Transfer(toDev, fromDev, d, synchronous)\n\treturn\n}\n"
        },
        {
          "name": "cuda",
          "type": "tree",
          "content": null
        },
        {
          "name": "cuda_test.go",
          "type": "blob",
          "size": 3.1962890625,
          "content": "// +build cuda\n\npackage gorgonia\n\nimport (\n\t\"io/ioutil\"\n\t\"log\"\n\t\"os\"\n\t\"testing\"\n\n\t\"gorgonia.org/tensor\"\n)\n\nfunc TestDevCUDA(t *testing.T) {\n\tt.SkipNow()\n\n\tg := NewGraph()\n\tx := NewMatrix(g, Float64, WithShape(1024, 100), WithName(\"x\"), WithInit(ValuesOf(2.0)))\n\ty := NewMatrix(g, Float64, WithShape(1024, 100), WithName(\"y\"), WithInit(ValuesOf(8.0)))\n\txpy := Must(Add(x, y))\n\txmy := Must(Sub(x, y))\n\txpy2 := Must(Square(xpy))\n\tWithName(\"xpy2\")(xpy2)\n\txmy2 := Must(Square(xmy))\n\txpy2s := Must(Slice(xpy2, S(0)))\n\tioutil.WriteFile(\"fullgraph.dot\", []byte(g.ToDot()), 0644)\n\n\tvar xpyV, xmyV, xpy2V, xpy2sV, xmy2V Value\n\tRead(xpy, &xpyV)\n\tRead(xmy, &xmyV)\n\tRead(xpy2, &xpy2V)\n\tRead(xpy2s, &xpy2sV)\n\tRead(xmy2, &xmy2V)\n\n\tlogger := log.New(os.Stderr, \"\", 0)\n\tm := NewTapeMachine(g, WithLogger(logger), WithWatchlist(), WithValueFmt(\"0x%x\"))\n\tdefer m.Close()\n\n\tprog, locMap, _ := Compile(g)\n\tt.Logf(\"prog:\\n%v\\n\", prog)\n\tt.Logf(\"locMap %-v\", FmtNodeMap(locMap))\n\tif err := m.RunAll(); err != nil {\n\t\tt.Errorf(\"%+v\", err)\n\t}\n\n\tt.Logf(\"x: \\n%v\", x.Value())\n\n\tt.Logf(\"y: \\n%v\", y.Value())\n\tt.Logf(\"xpy \\n%v\", xpyV)\n\tt.Logf(\"xmy \\n%v\", xmyV)\n\tt.Logf(\"xpy2: \\n%v\", xpy2V)\n\tt.Logf(\"xpy2s \\n%v\", xpy2sV)\n\tt.Logf(\"xmy2 \\n%v\", xmy2V)\n\n\tif assertGraphEngine(t, g, stdengType); t.Failed() {\n\t\tt.FailNow()\n\t}\n}\n\nfunc TestExternMetadata_Transfer(t *testing.T) {\n\tm := new(ExternMetadata)\n\tm.init([]int64{1024}) // allocate 1024 bytes\n\n\tv := tensor.New(tensor.Of(Float64), tensor.WithShape(2, 2))\n\tgo func() {\n\t\tfor s := range m.WorkAvailable() {\n\t\t\tm.DoWork()\n\t\t\tif s {\n\t\t\t\tm.syncChan <- struct{}{}\n\t\t\t}\n\t\t}\n\t}()\n\n\t//\t transfer from CPU to GPU\n\tv2, err := m.Transfer(Device(0), CPU, v, true)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tif vt, ok := v2.(*tensor.Dense); (ok && !vt.IsManuallyManaged()) || !ok {\n\t\tt.Errorf(\"Expected manually managed value\")\n\t}\n\tt.Logf(\"v2: 0x%x\", v2.Uintptr())\n\n\t// transfer from GPU to CPU\n\tv3, err := m.Transfer(CPU, Device(0), v2, true)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tif vt, ok := v3.(*tensor.Dense); (ok && vt.IsManuallyManaged()) || !ok {\n\t\tt.Errorf(\"Expected Go managed value\")\n\t}\n\tt.Logf(\"v3: 0x%x\", v3.Uintptr())\n\n\t// transfer from CPU to CPU\n\tv4, err := m.Transfer(CPU, CPU, v3, true)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tif v4 != v3 {\n\t\tt.Errorf(\"Expected the values to be returned exactly the same\")\n\t}\n}\n\nfunc BenchmarkOneMilCUDA(b *testing.B) {\n\txT := tensor.New(tensor.WithShape(1000000), tensor.WithBacking(tensor.Random(tensor.Float32, 1000000)))\n\tg := NewGraph()\n\tx := NewVector(g, Float32, WithShape(1000000), WithName(\"x\"), WithValue(xT))\n\tMust(Sigmoid(x))\n\n\tm := NewTapeMachine(g)\n\tdefer m.Close()\n\n\t// runtime.LockOSThread()\n\tfor n := 0; n < b.N; n++ {\n\t\tif err := m.RunAll(); err != nil {\n\t\t\tb.Fatalf(\"Failed at n: %d. Error: %v\", n, err)\n\t\t\tbreak\n\t\t}\n\t\tm.Reset()\n\t}\n\t// runtime.UnlockOSThread()\n}\n\nfunc BenchmarkOneMil(b *testing.B) {\n\txT := tensor.New(tensor.WithShape(1000000), tensor.WithBacking(tensor.Random(tensor.Float32, 1000000)))\n\tg := NewGraph()\n\tx := NewVector(g, Float32, WithShape(1000000), WithName(\"x\"), WithValue(xT))\n\tMust(Sigmoid(x))\n\n\tm := NewTapeMachine(g)\n\tdefer m.Close()\n\n\tfor n := 0; n < b.N; n++ {\n\t\tif err := m.RunAll(); err != nil {\n\t\t\tb.Fatalf(\"Failed at n: %d. Error: %v\", n, err)\n\t\t\tbreak\n\t\t}\n\t\tm.Reset()\n\t}\n}\n"
        },
        {
          "name": "dcu_test.go",
          "type": "blob",
          "size": 1.087890625,
          "content": "package gorgonia\n\nimport (\n\t\"io/ioutil\"\n\t\"testing\"\n\n\t\"gorgonia.org/tensor\"\n)\n\nfunc TestBatchNormUnderstanding(t *testing.T) {\n\tg := NewGraph()\n\n\txV := tensor.New(tensor.WithShape(2, 3, 2, 1), tensor.WithBacking([]float64{\n\t\t0.0001,\n\t\t0.003,\n\n\t\t0.02,\n\t\t0.007,\n\n\t\t0.007,\n\t\t0.05,\n\n\t\t// ---\n\n\t\t0.00015,\n\t\t0.0035,\n\n\t\t0.025,\n\t\t0.0075,\n\n\t\t0.0075,\n\t\t0.055,\n\t}))\n\tx := NodeFromAny(g, xV, WithName(\"x\"))\n\n\ts2 := NewTensor(g, x.Dtype(), x.Shape().Dims(), WithShape(x.Shape().Clone()...), WithInit(Ones()), WithName(\"Scale\"))\n\tb2 := NewTensor(g, x.Dtype(), x.Shape().Dims(), WithShape(x.Shape().Clone()...), WithInit(Zeroes()), WithName(\"Bias\"))\n\n\ty2, scale2, bias2, op2, err := BatchNorm(x, s2, b2, 0.1, 1e-05)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tWithName(\"y2\")(y2)\n\n\top2.SetTraining(true)\n\n\tC, _ := Sum(y2)\n\n\t_, err = Grad(C, x, scale2, bias2)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tm := NewTapeMachine(g, TraceExec())\n\tif err := m.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tioutil.WriteFile(\"foo.dot\", []byte(g.ToDot()), 0644)\n\tt.Logf(\"\\n%v\", y2.Value())\n\t// _, _ = scale, scale2\n\t// _, _ = bias, bias2\n\t// _, _ = op, op2\n\n}\n"
        },
        {
          "name": "debug.go",
          "type": "blob",
          "size": 3.8994140625,
          "content": "//go:build debug\n// +build debug\n\npackage gorgonia\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\t\"strings\"\n\t\"sync/atomic\"\n)\n\n// DEBUG is a global flag that activates various debugging functions\nconst DEBUG = true\n\nfunc init() {\n\tlog.Printf(\"DEBUG\")\n}\n\n// these constants are used during development time - mainly on tracing statements to see the values of certain things.\n// I use these instead of say, Delve because most of the time, the larger picture has to be known. Delve tends to give small picture views\nvar (\n\tcompileDev        = false\n\tshapeInferenceDev = false\n\ttypeSystemDev     = false\n\tsymdiffDev        = false\n\tautodiffDev       = false\n\tmachineDev        = false\n\tstabilizationDev  = false\n\tsolverDev         = false\n\tcudaDev           = false\n\tallocatorDev      = false\n)\n\n// TABCOUNT is a global flag used when debugging\nvar TABCOUNT uint32\n\nvar logger = log.New(os.Stderr, \"\", 0)\nvar replacement = \"\\n\"\n\nfunc tabcount() int {\n\treturn int(atomic.LoadUint32(&TABCOUNT))\n}\n\nfunc enterLogScope() {\n\tatomic.AddUint32(&TABCOUNT, 1)\n\ttabcount := tabcount()\n\tlogger.SetPrefix(strings.Repeat(\"\\t\", tabcount))\n\treplacement = \"\\n\" + strings.Repeat(\"\\t\", tabcount)\n}\n\nfunc leaveLogScope() {\n\ttabcount := tabcount()\n\ttabcount--\n\n\tif tabcount < 0 {\n\t\tatomic.StoreUint32(&TABCOUNT, 0)\n\t\ttabcount = 0\n\t} else {\n\t\tatomic.StoreUint32(&TABCOUNT, uint32(tabcount))\n\t}\n\tlogger.SetPrefix(strings.Repeat(\"\\t\", tabcount))\n\treplacement = \"\\n\" + strings.Repeat(\"\\t\", tabcount)\n}\n\nfunc logf(format string, others ...interface{}) {\n\tif DEBUG {\n\t\t// format = strings.Replace(format, \"\\n\", replacement, -1)\n\t\ts := fmt.Sprintf(format, others...)\n\t\ts = strings.Replace(s, \"\\n\", replacement, -1)\n\t\tlogger.Println(s)\n\t\t// logger.Printf(format, others...)\n\t}\n}\n\nfunc compileLogf(format string, attrs ...interface{}) {\n\tif compileDev {\n\t\tlogf(format, attrs...)\n\t}\n}\n\nfunc shapeLogf(format string, attrs ...interface{}) {\n\tif shapeInferenceDev {\n\t\tlogf(format, attrs...)\n\t}\n}\n\nfunc typeSysLogf(format string, attrs ...interface{}) {\n\tif typeSystemDev {\n\t\tlogf(format, attrs...)\n\t}\n}\n\nfunc symdiffLogf(format string, attrs ...interface{}) {\n\tif symdiffDev {\n\t\tlogf(format, attrs...)\n\t}\n}\n\nfunc autodiffLogf(format string, attrs ...interface{}) {\n\tif autodiffDev {\n\t\tlogf(format, attrs...)\n\t}\n}\n\nfunc machineLogf(format string, attrs ...interface{}) {\n\tif machineDev {\n\t\tlogf(format, attrs...)\n\t}\n}\n\nfunc stabLogf(format string, attrs ...interface{}) {\n\tif stabilizationDev {\n\t\tlogf(format, attrs...)\n\t}\n}\n\nfunc solverLogf(format string, attrs ...interface{}) {\n\tif solverDev {\n\t\tlogf(format, attrs...)\n\t}\n}\n\nfunc cudaLogf(format string, attrs ...interface{}) {\n\tif cudaDev {\n\t\tlogf(format, attrs...)\n\t}\n}\n\nfunc allocatorLogf(format string, attrs ...interface{}) {\n\tif allocatorDev {\n\t\tlogf(format, attrs...)\n\t}\n}\n\nfunc recoverFrom(format string, attrs ...interface{}) {\n\tif r := recover(); r != nil {\n\t\tlogger.Printf(format, attrs...)\n\t\tpanic(r)\n\t}\n}\n\n/* Graph Collision related debugging code */\nvar nnc, cc, ec int64\n\nfunc incrNN() {\n\tatomic.AddInt64(&nnc, 1)\n}\n\nfunc incrCC() {\n\tatomic.AddInt64(&cc, 1)\n}\n\nfunc incrEC() {\n\tatomic.AddInt64(&ec, 1)\n}\n\n// GraphCollisionStats ...\nfunc GraphCollisionStats() (int, int, int) {\n\treturn int(atomic.LoadInt64(&nnc)), int(atomic.LoadInt64(&cc)), int(atomic.LoadInt64(&ec))\n}\n\n/* Compilation related debug utility functions/methods*/\nfunc logCompileState(name string, g *ExprGraph, df *dataflow) {\n\tvar fname string\n\tif name == \"\" {\n\t\tfname = \"TotallyRandomName.csv\"\n\t} else {\n\t\tfname = name + \".csv\"\n\t}\n\tf, err := os.OpenFile(fname, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0644)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer f.Close()\n\n\tcompileState(f, g, df)\n\tcompileLogf(\"Written Compile State to %v\", fname)\n}\n\n/* Analysis Debug Utility Functions/Methods */\nfunc (df *dataflow) debugIntervals(sorted Nodes) {\n\tvar buf bytes.Buffer\n\tbuf.Write([]byte(\"Intervals:\\n\"))\n\tfor _, n := range sorted {\n\t\tfmt.Fprintf(&buf, \"\\t%v:\\t%v\\n\", n, df.intervals[n])\n\t}\n\tcompileLogf(buf.String())\n}\n"
        },
        {
          "name": "device.go",
          "type": "blob",
          "size": 0.7998046875,
          "content": "// +build !cuda\n\npackage gorgonia\n\nimport \"gorgonia.org/tensor\"\n\n// Device represents the device where the code will be executed on. In this build, all code will run on the CPU\ntype Device int\n\nconst (\n\t// CPU the only device the graph will be executed on\n\tCPU Device = 0\n)\n\n// String implements fmt.Stringer and runtime.Stringer\nfunc (d Device) String() string { return \"CPU\" }\n\n// IsGPU will always return false in this build\nfunc (d Device) IsGPU() bool { return false }\n\n// Alloc allocates memory on the device. This is currently a NO-OP in this build\nfunc (d Device) Alloc(extern External, size int64) (tensor.Memory, error) { return nil, nil }\n\n// Free frees the memory on the device. This is currently a NO-OP in this build\nfunc (d Device) Free(extern External, mem tensor.Memory, sie uint) error { return nil }\n"
        },
        {
          "name": "device_cuda.go",
          "type": "blob",
          "size": 1.4130859375,
          "content": "// +build cuda\n\npackage gorgonia\n\nimport (\n\t\"gorgonia.org/cu\"\n\t\"gorgonia.org/tensor\"\n)\n\n// Device represents the device where the code will be executed on. It can either be a GPU or CPU\ntype Device cu.Device\n\n// CPU is the default the graph will be executed on.\nconst CPU = Device(cu.CPU)\n\n// String implements fmt.Stringer and runtime.Stringer\nfunc (d Device) String() string { return cu.Device(d).String() }\n\n// Alloc allocates memory on the device. If the device is CPU, the allocations is a NO-OP because Go handles all the allocations in the CPU\nfunc (d Device) Alloc(extern External, size int64) (tensor.Memory, error) {\n\tif d == CPU {\n\t\tcudaLogf(\"device is CPU\")\n\t\treturn nil, nil // well there should be an error because this wouldn't be called\n\t}\n\n\tmachine := extern.(CUDAMachine)\n\tctxes := machine.Contexts()\n\tif len(ctxes) == 0 {\n\t\tcudaLogf(\"allocate nothing\")\n\t\treturn nil, nil\n\t}\n\tctx := ctxes[int(d)]\n\n\tcudaLogf(\"calling ctx.MemAlloc(%d)\", size)\n\treturn ctx.MemAlloc(size)\n}\n\n// Free the memory of the device\nfunc (d Device) Free(extern External, mem tensor.Memory, size int64) (err error) {\n\tvar devptr cu.DevicePtr\n\tvar ok bool\n\tif devptr, ok = mem.(cu.DevicePtr); !ok {\n\t\treturn nil\n\t}\n\n\tmachine := extern.(CUDAMachine)\n\tmachine.Put(d, devptr, size)\n\n\t// FUTURE: actually free memory if there ain't enough to go round\n\n\t// ctx := machine.Contexts()[int(d)]\n\t// cudaLogf(\"MemFree %v\", devptr)\n\t// ctx.MemFree(devptr)\n\treturn nil\n}\n"
        },
        {
          "name": "device_test.go",
          "type": "blob",
          "size": 0.248046875,
          "content": "// +build !cuda\n\npackage gorgonia\n\nimport \"testing\"\n\nfunc TestDeviceCPU(t *testing.T) {\n\tif CPU.IsGPU() {\n\t\tt.Fail()\n\t}\n\ta, err := CPU.Alloc(nil, 0)\n\tif a != nil || err != nil {\n\t\tt.Fail()\n\t}\n\terr = CPU.Free(nil, nil, 0)\n\tif err != nil {\n\t\tt.Fail()\n\t}\n}\n"
        },
        {
          "name": "differentiation.go",
          "type": "blob",
          "size": 10.5419921875,
          "content": "package gorgonia\n\nimport (\n\t\"github.com/pkg/errors\"\n\t\"gonum.org/v1/gonum/graph\"\n)\n\n/*\nThis file holds code for symbolic differentiation.\nThe purpose of the symbolic differentiation is to analyze and prepare the nodes for automatic differentiation.\n\nThe main function that does all the magic is in Backpropagate().\n\n\nsee also: http://colah.github.io/posts/2015-08-Backprop/\n*/\n\n// forwardDiffAnalysis returns the nodes that affect outputs.\n//\n// Given a list of outputs, we want to know which nodes will affect the output\nfunc forwardDiffAnalysis(outputs, sortedNodes Nodes) (retVal NodeSet, err error) {\n\tsymdiffLogf(\"Forward analysis. Already sorted?\")\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tif !outputs.AllSameGraph() {\n\t\treturn nil, errors.New(\"The supplied output Nodes are not the same graph\")\n\t}\n\n\tdiffSet := outputs.mapSet()\n\n\tsymdiffLogf(\"Diff Set: %v\", diffSet)\n\tsymdiffLogf(\"%d\", sortedNodes)\n\tfor _, n := range sortedNodes {\n\t\tif diffSet.Contains(n) && !n.isInput() {\n\t\t\tdiffs := n.diffWRT()\n\t\t\tfor j, child := range n.children {\n\t\t\t\td := diffs[j]\n\t\t\t\tif d {\n\t\t\t\t\tsymdiffLogf(\"Adding %x to  differentiable set\", child.ID())\n\t\t\t\t\tdiffSet.Add(child)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn diffSet, nil\n}\n\n// backwardDiffAnalysis returns a list of Nodes that are affected by differentiating output.\n// Given a list of WRTs, we want to find a list of nodes that will be affected when backpropagating.\nfunc backwardDiffAnalysis(wrt, sortedNodes Nodes) (retVal NodeSet, err error) {\n\tsymdiffLogf(\"Backwards analysis\")\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tif !wrt.AllSameGraph() {\n\t\treturn nil, errors.New(\"The supplied output Nodes are not the same graph\")\n\t}\n\n\tdiffSet := wrt.mapSet()\n\tsymdiffLogf(\"wrt:%d diffset: %d\", len(wrt), len(diffSet))\n\tsymdiffLogf(\"%v\", diffSet)\n\tsymdiffLogf(\"sorted: %d\", sortedNodes)\n\n\tenterLogScope()\n\tfor i := len(sortedNodes) - 1; i >= 0; i-- {\n\t\tn := sortedNodes[i]\n\t\tsymdiffLogf(\"working on %v. Has %d children\", n, len(n.children))\n\n\t\tvar op SDOp\n\t\tvar ok bool\n\t\tvar diffs []bool\n\t\tif op, ok = n.op.(SDOp); ok {\n\t\t\tdiffs = op.DiffWRT(len(n.children))\n\t\t}\n\n\t\tsymdiffLogf(\"differentiable WRT: %v\", diffs)\n\t\tenterLogScope()\n\t\tsymdiffLogf(\"Children: %v\", n.children)\n\t\tif len(diffs) == 0 {\n\t\t\t// check if this makes nodes unreachable. If it does, then error out\n\t\t\tif n.isStmt {\n\t\t\t\tsymdiffLogf(\"Statement nodes are Non differentiable!\")\n\t\t\t\tleaveLogScope()\n\t\t\t\tcontinue\n\t\t\t} else if n.isInput() {\n\t\t\t\tsymdiffLogf(\"Input nodes are Non differentiable\")\n\t\t\t\tleaveLogScope()\n\t\t\t\tcontinue\n\t\t\t} else if len(n.children) == 0 {\n\t\t\t\tsymdiffLogf(\"Leaf nodes have no children\")\n\t\t\t\tleaveLogScope()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tg := n.g\n\t\t\tfor _, child := range n.children {\n\t\t\t\tparents := graph.NodesOf(g.To(child.ID()))\n\t\t\t\tif len(parents) == 1 && len(child.children) > 0 {\n\t\t\t\t\tleaveLogScope()\n\t\t\t\t\treturn nil, errors.Errorf(\"Being unable to differentiate %v would leave a portion of the graph unreachable. Unable to continue\", n)\n\t\t\t\t}\n\t\t\t}\n\t\t\tsymdiffLogf(\"SKIPPING... Non differentiable!\")\n\t\t\tleaveLogScope()\n\t\t\tcontinue\n\t\t}\n\n\tinner:\n\t\tfor j, child := range n.children {\n\t\t\td := diffs[j]\n\t\t\tif diffSet.Contains(child) && d {\n\t\t\t\tsymdiffLogf(\"Adding %x to differentiable set\", child.ID())\n\t\t\t\tdiffSet.Add(n)\n\t\t\t\tbreak inner\n\t\t\t}\n\t\t}\n\t\tleaveLogScope()\n\t}\n\tleaveLogScope()\n\treturn diffSet, nil\n}\n\n// Backpropagate backpropagates errors by performing reverse-mode symbolic differentiation, starting from the outputs, and working its way towads the inputs.\n//\n// This is the rough algorithm:\n//\t\t1. Filter out nodes that are unreachable\n//\t\t2. Forwards analysis, where a list of nodes affecting the output is added to consideration\n//\t\t3. Backwards analysis, where a list of nodes affected by differentiating the output are added to the consideration\n//\t\t4. If there is a difference in both sets, it will cause an error (both sets should be the same)\n//\t\t5. Traverse the graph from output towards input. On each visit, perform the symbolic differentiation\n//\n// For most cases, Grad() should be used instead of Backpropagate(), as Grad() performs several checks which would be the general use case, before calling Backpropagate()\nfunc Backpropagate(outputs, gradOutputs, wrt Nodes) (retVal Nodes, err error) {\n\tsymdiffLogf(\"BACKPROP START\")\n\tsymdiffLogf(\"Outputs: %d\", outputs)\n\tsymdiffLogf(\"gradOutputs: %d\", gradOutputs)\n\tsymdiffLogf(\"WRT: %d\", wrt)\n\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tg := outputs[0].g\n\n\t// this entire section about removing foreveralone nodes need a rethink\n\tsymdiffLogf(\"removing foreveralone nodes\")\n\tenterLogScope()\n\tfor i := 0; i < len(g.AllNodes()); i++ {\n\t\tn := g.AllNodes()[i]\n\n\t\tfr := g.From(n.ID()).Len()\n\t\tto := g.To(n.ID()).Len()\n\n\t\tif fr == 0 && to == 0 && !n.isConstant() && !n.isInput() {\n\t\t\tg.RemoveNode(n)\n\t\t\tsymdiffLogf(\"removed %v(%p); %x; %s\", n, n, n.ID(), n.Name())\n\t\t}\n\t}\n\tleaveLogScope()\n\n\tvar sortedNodes Nodes\n\tif sortedNodes, err = Sort(g); err != nil {\n\t\treturn nil, errors.Wrap(err, sortFail)\n\t}\n\tsymdiffLogf(\"sorted nodes: %v\", sortedNodes)\n\tsymdiffLogf(\"sorted nodes: %d\", sortedNodes)\n\n\tvar affectsOutput NodeSet\n\tvar affectedByOutput NodeSet\n\tif affectsOutput, err = forwardDiffAnalysis(outputs, sortedNodes); err != nil {\n\t\treturn nil, errors.Wrap(err, \"Failed during forward differentiation analysis\")\n\t}\n\n\tif affectedByOutput, err = backwardDiffAnalysis(wrt, sortedNodes); err != nil {\n\t\treturn nil, errors.Wrap(err, \"Failed during forward differentiation analysis\")\n\t}\n\n\tsymdiffLogf(\"affects output: %v\", affectsOutput)\n\tsymdiffLogf(\"affected by output : %v\", affectedByOutput)\n\n\twrtSet := wrt.mapSet()\n\tbadWRTs := wrtSet.Difference(affectsOutput)\n\tif len(badWRTs) > 0 {\n\t\treturn nil, SymDiffError{nodes: badWRTs.ToSlice(), err: errors.Errorf(\"Non Differentiable WRTs: %v\", badWRTs)}\n\t}\n\n\toutputSet := outputs.mapSet()\n\tbadOutputs := outputSet.Difference(affectedByOutput)\n\tif len(badOutputs) > 0 {\n\t\tsymdiffLogf(\"badOutputs: %#v\", badOutputs)\n\t\treturn nil, SymDiffError{nodes: badOutputs.ToSlice(), err: errors.Errorf(\"Non-Differentable Outputs: %v\", badOutputs)}\n\t}\n\n\t// map a node to a list of gradient terms\n\t// these  gradient terms will be summed up when we visit the node\n\t// when iterating through the nondes in reverse topological order\n\tnodeGradMap := make(map[*Node]Nodes)\n\tfor i, n := range outputs {\n\t\tsymdiffLogf(\"Adding outputs for %x\", n.ID())\n\t\tnodeGradMap[n] = Nodes{gradOutputs[i]}\n\t}\n\n\t// \"active\" nodes are the ones that are differentially influenced by the inputs\n\t// and also differentiably influence the outputs. These are the nodes where we need to call the\n\t// \"pullback\" function to backpropagate derivatives\n\tactiveNodes := affectsOutput.Intersect(affectedByOutput)\n\n\tsymdiffLogf(\"Active: %v\", activeNodes)\n\n\tsymdiffLogf(\"Sorted: %d\", sortedNodes)\n\tsymdiffLogf(\"nodeGradMap: %+#d\", FmtNodeMap(nodeGradMap))\n\tenterLogScope()\n\n\tfor _, node := range sortedNodes {\n\t\tif _, ok := activeNodes[node]; !ok {\n\t\t\tsymdiffLogf(\"skipping %x\", node.ID())\n\t\t\tcontinue\n\t\t}\n\n\t\tif node.deriv != nil {\n\t\t\tsymdiffLogf(\"skipping %x - previously differentiated\", node.ID())\n\t\t\tnodeGradMap[node] = append(nodeGradMap[node], node.deriv)\n\t\t\tcontinue\n\t\t}\n\n\t\tsymdiffLogf(\"Working on %x %v\", node.ID(), node)\n\t\tenterLogScope()\n\n\t\t// Check if there is any grads coming into this node\n\t\tif len(nodeGradMap[node]) < 1 {\n\t\t\tleaveLogScope()\n\t\t\treturn nil, SymDiffError{\n\t\t\t\tsingle:  node,\n\t\t\t\tgradMap: nodeGradMap,\n\t\t\t\terr:     errors.New(\"No gradients found for node\"),\n\t\t\t}\n\t\t}\n\n\t\t// once we've reached a node, we already backpropagated from its dependents\n\t\t// so we sum up the gradients\n\t\tsymdiffLogf(\"nodeGradMap[%x]: %d\", node.ID(), nodeGradMap[node])\n\t\tif len(nodeGradMap[node]) > 1 {\n\t\t\tvar n *Node\n\t\t\tsymdiffLogf(\"reduce adding\")\n\t\t\tif n, err = ReduceAdd(nodeGradMap[node], WithGroupName(gradClust)); err != nil {\n\t\t\t\tleaveLogScope()\n\t\t\t\treturn nil, SymDiffError{\n\t\t\t\t\tsingle:  node,\n\t\t\t\t\tnodes:   nodeGradMap[node],\n\t\t\t\t\tgradMap: nodeGradMap,\n\t\t\t\t\terr:     errors.Wrap(err, \"ReduceAdd failed during differentiation\"),\n\t\t\t\t}\n\n\t\t\t}\n\t\t\tsymdiffLogf(\"reduced to... %x\", n.ID())\n\t\t\t// node.derives = append(node.derives, n)\n\t\t\tn.derivOf = append(n.derivOf, node)\n\t\t\tnode.deriv = n\n\t\t\tnodeGradMap[node] = Nodes{n}\n\t\t\t// }\n\t\t} else if len(nodeGradMap[node]) == 1 {\n\t\t\tderiv := nodeGradMap[node][0]\n\t\t\tderiv.derivOf = append(deriv.derivOf, node)\n\t\t\tnode.deriv = deriv\n\t\t}\n\n\t\tgradNode := nodeGradMap[node][0]\n\t\tif !node.isInput() {\n\t\t\tsymdiffLogf(\"differentiating %x (%v)\", node.ID(), node.op)\n\t\t\tenterLogScope()\n\n\t\t\tvar op SDOp\n\t\t\tvar childrenGrads Nodes\n\t\t\tvar ok bool\n\n\t\t\tif op, ok = node.op.(SDOp); !ok {\n\t\t\t\treturn nil, SymDiffError{\n\t\t\t\t\tsingle: node,\n\t\t\t\t\terr:    errors.New(\"Not a SymDifOp\"),\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tsymdiffLogf(\"op: %v || optype: %v ||  node: %v || Children: %#Y || Grad: %v\", node.op, node.op.Type(), node.t, node.children, gradNode)\n\t\t\tif childrenGrads, err = op.SymDiff(node.children, node, gradNode); err != nil {\n\t\t\t\tleaveLogScope()\n\t\t\t\treturn nil, SymDiffError{\n\t\t\t\t\tsingle:  node,\n\t\t\t\t\tgrad:    gradNode,\n\t\t\t\t\tgradMap: nodeGradMap,\n\t\t\t\t\terr:     errors.Wrapf(err, \".SymDiff() failed\"),\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tsymdiffLogf(\"Derived(%d): %P\", len(childrenGrads), childrenGrads)\n\t\t\tleaveLogScope()\n\n\t\t\tdiffs := node.diffWRT()\n\t\t\tfor i, child := range node.children {\n\t\t\t\tsymdiffLogf(\"child is %v, i: %v\", child, i)\n\t\t\t\tdifferentiable := diffs[i]\n\t\t\t\tchildGrad := childrenGrads[i]\n\n\t\t\t\tif differentiable {\n\t\t\t\t\tchildGrad.setGroup(gradClust)\n\t\t\t\t\tif grads, ok := nodeGradMap[child]; ok {\n\t\t\t\t\t\tgrads = append(grads, childGrad)\n\t\t\t\t\t\tnodeGradMap[child] = grads\n\t\t\t\t\t} else {\n\t\t\t\t\t\tnodeGradMap[child] = Nodes{childGrad}\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tsymdiffLogf(\"Child %x is non differentiable\", child.ID())\n\t\t\t\t\tif childGrad != nil {\n\t\t\t\t\t\tchildGrad.setGroup(strayClust)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tsymdiffLogf(\"iz input\")\n\t\t\tsymdiffLogf(\"%d \", nodeGradMap[node])\n\t\t}\n\t\tleaveLogScope()\n\n\t}\n\tleaveLogScope()\n\t// only we already summed up the gradients for the input nodes, so just take\n\t// 0th element\n\tfor _, n := range wrt {\n\t\tsymdiffLogf(\"nodeGradMap wrt: %d\", nodeGradMap[n])\n\t\tretVal = append(retVal, nodeGradMap[n][0])\n\t}\n\treturn\n}\n\n// SetDerivOf is used to hack around the fundamental limitations of Gorgonia.\n//\n// Specifically it is used to set a node as the derivative of another node,\n// used in the cuDNN version of batch norm.\n//\n// The cuDNN BatchNorm operation produces the derivatives for the scale and bias as a side effect\n// of calculating the derivative of the input. Because Gorgonia's Ops are modelled as pure functions (and no tuples)\n// this causes a bit of trouble. With the clever use of scratch space ops multireturn can be simulated.\n// But this causes derivatives to not be set correctly.\nfunc SetDerivOf(deriv, of *Node) {\n\tderiv.derivOf = append(deriv.derivOf, of)\n\tof.deriv = deriv\n}\n"
        },
        {
          "name": "differentiation_test.go",
          "type": "blob",
          "size": 4.2822265625,
          "content": "package gorgonia\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"gonum.org/v1/gonum/graph/iterator\"\n\t\"gonum.org/v1/gonum/graph/topo\"\n)\n\nfunc TestForwardDiffAnalysis(t *testing.T) {\n\tg := NewGraph()\n\tx := NewScalar(g, Float64, WithName(\"x\"))\n\ty := NewScalar(g, Float64, WithName(\"y\"))\n\tz := NewScalar(g, Float64, WithName(\"z\"))\n\n\tres1 := Must(Log(Must(Mul(x, y))))\n\n\tsorted, err := topo.Sort(g)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tsortedNodes := graphNodeToNode(iterator.NewOrderedNodes(sorted))\n\taffectsOutput, err := forwardDiffAnalysis(Nodes{res1}, sortedNodes)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tt.Logf(\"%v\", affectsOutput)\n\tif affectsOutput.Contains(z) {\n\t\tt.Error(\"It shouldn't contain res2 or z\")\n\t}\n}\n\nfunc TestBackwardDiffAnalysis(t *testing.T) {\n\tg := NewGraph()\n\tx := NewScalar(g, Float64, WithName(\"x\"))\n\ty := NewScalar(g, Float64, WithName(\"y\"))\n\tz := NewScalar(g, Float64, WithName(\"z\"))\n\n\tres1 := Must(Log(Must(Mul(x, y))))\n\tres2 := Must(Log(Must(Mul(x, y)))) // yes it's a duplicate\n\n\tsorted, err := topo.Sort(g)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tsortedNodes := graphNodeToNode(iterator.NewOrderedNodes(sorted))\n\taffectedByOutput, err := backwardDiffAnalysis(Nodes{x, y}, sortedNodes)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tt.Logf(\"%v\", affectedByOutput)\n\n\tif !affectedByOutput.Contains(res1) || !affectedByOutput.Contains(res2) {\n\t\tt.Error(\"Expected res1 and res2 to be affected by wrts\")\n\t}\n\n\tif affectedByOutput.Contains(z) {\n\t\tt.Error(\"z shouldn't be in the list at all\")\n\t}\n}\n\nfunc TestBackprop(t *testing.T) {\n\tassert := assert.New(t)\n\tgradOut := NewConstant(ones(Float64), WithName(\"GradOut\"))\n\n\tt.Log(\"Simple backprop\")\n\tg := NewGraph()\n\tx := NewVector(g, Float64, WithName(\"x\"), WithShape(10)) // horizontal vector\n\ty := NewVector(g, Float64, WithName(\"y\"), WithShape(10)) // horizontal vector\n\n\tres := Must(Mul(x, y))\n\n\tgrad := g.AddNode(gradOut)\n\tinputs := Nodes{x, y}\n\tret, err := Backpropagate(Nodes{res}, Nodes{grad}, inputs)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tassert.Equal(Nodes{inputs[1], grad}, ret[0].children)\n\tassert.Equal(Nodes{inputs[0], grad}, ret[1].children)\n\tassert.Equal(mulOpType, ret[0].op.(elemBinOp).ʘBinaryOperator.binOpType())\n\tassert.Equal(mulOpType, ret[1].op.(elemBinOp).ʘBinaryOperator.binOpType())\n\n\t// reset\n\tt.Log(\"Progressively more complex\")\n\tg = NewGraph()\n\tx = NewMatrix(g, Float64, WithName(\"x\"), WithShape(1, 10))  // row vector\n\tw := NewMatrix(g, Float64, WithName(\"w\"), WithShape(10, 1)) // col vector\n\n\tmul := Must(Mul(x, w))\n\tres = Must(Exp(mul))\n\n\tgrad = g.AddNode(gradOut)\n\tinputs = Nodes{x, w}\n\tif ret, err = Backpropagate(Nodes{res}, Nodes{grad}, inputs); err != nil {\n\t\tt.Error(err)\n\t}\n\n\t// Notes:\n\t//\n\t// extra was created in the Backprop process\n\n\textra := Must(Mul(res, onef64))\n\tdzdxExpectedPath := Nodes{ret[0], w, extra, res, mul, x, w, grad}\n\tdzdwExpectedPath := Nodes{ret[1], x, extra, res, mul, x, w, grad}\n\n\tassert.True(dzdxExpectedPath.Equals(ret[0].seqWalk()))\n\tassert.True(dzdwExpectedPath.Equals(ret[1].seqWalk()))\n\n\t/*\n\t\tioutil.WriteFile(\"Test_Res.dot\", []byte(res.ToDot()), 0644)\n\t\tfor i, n := range ret {\n\t\t\tWithName(fmt.Sprintf(\"dz/d%s\", inputs[i].Name()))(n)\n\t\t\tioutil.WriteFile(fmt.Sprintf(\"Test_Grad_%d.dot\", i), []byte(n.ToDot()), 0644)\n\t\t}\n\t\tioutil.WriteFile(\"WholeGraph.dot\", []byte(g.ToDot()), 0644)\n\t*/\n}\n\n// Compound ops (like expm1, log1p and sigmoid) have fairly complex diff results. Got bitten by log1p's diffExpr, so here's the test for them all\nfunc TestCompoundOpDiff(t *testing.T) {\n\tg := NewGraph()\n\n\tsaved := stabilization\n\tstabilization = true\n\tdefer func() {\n\t\tstabilization = saved\n\t}()\n\n\t// log1p\n\tx := NewVector(g, Float64, WithName(\"x\"), WithShape(2))\n\tp := Must(Add(x, onef64))\n\tlp := Must(Log(p))\n\top := lp.op.(elemUnaryOp)\n\tdiffs, err := op.SymDiff(Nodes{x}, lp, onef64)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tif len(diffs) != 1 {\n\t\tt.Fatal(\"Expected only one result\")\n\t}\n\n\tdiff := diffs[0]\n\tebo, ok := diff.op.(elemBinOp)\n\tif !ok || ok && ebo.binOpType() != divOpType {\n\t\tt.Error(\"Expected an elemBinOp\")\n\t\tt.Error(\"Expected divOp to be the result of differentiating log1p\")\n\t}\n\tif diff.children[0].Hashcode() != onef64.Hashcode() {\n\t\tt.Errorf(\"Expected 1 as the numerator. Got %v instead\", diff.children[0])\n\t}\n\tebo, ok = diff.children[1].op.(elemBinOp)\n\tif !ok || ok && ebo.binOpType() != addOpType {\n\t\tt.Error(\"Expected child1 to be (+)\")\n\t}\n\n}\n"
        },
        {
          "name": "doc.go",
          "type": "blob",
          "size": 0.22265625,
          "content": "/*\nPackage gorgonia is a library that helps facilitate machine learning in Go.\nWrite and evaluate mathematical equations involving multidimensional arrays easily.\nDo differentiation with them just as easily.\n*/\npackage gorgonia\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "dual.go",
          "type": "blob",
          "size": 7.3583984375,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\ntype dualValue struct {\n\tValue\n\td Value // the derivative wrt to each input\n}\n\nfunc (dv *dualValue) SetDeriv(d Value) error {\n\tif t, ok := d.(tensor.Tensor); ok && t.IsScalar() {\n\t\td, _ = anyToScalar(t.ScalarValue())\n\t}\n\tdv.d = d\n\n\treturn dv.sanity()\n}\n\nfunc (dv *dualValue) SetValue(v Value) error {\n\tdv.Value = v\n\treturn dv.sanity()\n}\n\nfunc (dv *dualValue) Clone() (retVal interface{}, err error) {\n\tvar v, d Value\n\tif v, err = CloneValue(dv.Value); err != nil {\n\t\treturn nil, errors.Wrap(err, cloneFail)\n\t}\n\n\tif dv.d != nil {\n\t\tif d, err = CloneValue(dv.d); err != nil {\n\t\t\treturn nil, errors.Wrap(err, cloneFail)\n\t\t}\n\t}\n\n\tdv2 := borrowDV()\n\tdv2.Value = v\n\tdv2.d = d\n\tretVal = dv2\n\treturn\n}\n\nfunc (dv *dualValue) Type() hm.Type       { return TypeOf(dv.Value) }\nfunc (dv *dualValue) Dtype() tensor.Dtype { return dv.Value.Dtype() }\n\nfunc (dv *dualValue) ValueEq(a Value) bool {\n\tswitch at := a.(type) {\n\tcase *dualValue:\n\t\tif at == dv {\n\t\t\treturn true\n\t\t}\n\t\tveq := ValueEq(at.Value, dv.Value)\n\t\tdeq := ValueEq(at.d, dv.d)\n\t\treturn veq && deq\n\t// case Value:\n\t// \treturn ValueEq(at, dv.Value)\n\tdefault:\n\t\treturn false\n\t}\n}\n\nfunc (dv *dualValue) String() string {\n\treturn fmt.Sprintf(\"%#+v\", dv.Value)\n}\n\nfunc (dv *dualValue) sanity() error {\n\t// check that d and v are the same type\n\n\t// dvv := typeCheckTypeOf(dv.Value)\n\t// dvd := typeCheckTypeOf(dv.d)\n\t// if !dvv.Eq(dvd) {\n\t// \treturn errors.Errorf(\"DualValues do not have the same types: %v and %v\", dvv, dvd)\n\t// }\n\t// ReturnType(dvv)\n\t// ReturnType(dvd)\n\n\t// TODO: check that the shapes are the same\n\n\treturn nil\n}\n\n// clones the dualValue and zeroes out the ndarrays\nfunc (dv *dualValue) clone0() (retVal *dualValue, err error) {\n\tvar v, d Value\n\tif v, err = CloneValue(dv.Value); err != nil {\n\t\treturn nil, errors.Wrap(err, cloneFail)\n\t}\n\n\tif d, err = CloneValue(dv.d); err != nil {\n\t\treturn nil, errors.Wrap(err, cloneFail)\n\t}\n\n\tv = ZeroValue(v)\n\td = ZeroValue(d)\n\n\tdv2 := borrowDV()\n\tdv2.Value = v\n\tdv2.d = d\n\tretVal = dv2\n\treturn\n}\n\n// the derivative of a constant is zero.\n//\n// The original implementation was to have a constantDualValue type. This would lead to waaay less allocations of matrices\n// but as it turns out, as I waws working, the constants turn out to be not so constant afterall.\n// Is this a problem with the graph that leads to derivation of constant values? I don't quite know. TO CHECK\nfunc constantDV(val Value) *dualValue {\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\t// retVal := &dualValue{Value: val}\n\tretVal := borrowDV()\n\tretVal.Value = val\n\n\tvar err error\n\tif retVal.d, err = CloneValue(val); err != nil {\n\t\tpanic(err)\n\t}\n\n\tretVal.d = ZeroValue(retVal.d)\n\treturn retVal\n}\n\n// the derivative of x is 1.\nfunc variableDV(val Value) *dualValue {\n\t// retVal := &dualValue{Value: val}\n\tretVal := borrowDV()\n\tretVal.Value = val\n\n\tswitch v := val.(type) {\n\tcase Scalar:\n\t\tretVal.d = one(v.Dtype())\n\tcase tensor.Tensor:\n\t\tshp := v.Shape()\n\t\tdt := v.Dtype()\n\t\tretVal.d = tensor.Ones(dt, shp...)\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"%v(%T) not handled yet\", v, v))\n\t}\n\n\treturn retVal\n}\n\n// monadic unit() function. This unit() function will allocate a Value for dv.d\n// this is useful for forward mode autodiff\nfunc dvUnit(v Value) *dualValue {\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tif dv, ok := v.(*dualValue); ok {\n\t\treturn dv\n\t}\n\treturn constantDV(v)\n}\n\nfunc dvUnitVar(v Value) *dualValue {\n\tif dv, ok := v.(*dualValue); ok {\n\t\treturn dv\n\t}\n\treturn variableDV(v)\n}\n\n// no alloc is done. It'll just return a *dualValue with nil as the dv.d\nfunc dvUnit0(v Value) *dualValue {\n\tif dv, ok := v.(*dualValue); ok {\n\t\treturn dv\n\t}\n\n\tretVal := borrowDV()\n\tretVal.Value = v\n\n\treturn retVal\n}\n\n// dvUnitManaged does dvUnit for values whose memories are manually managed\nfunc dvUnitManaged(v Value, op *ExternalOp) (*dualValue, error) {\n\tif op.Device == CPU {\n\t\treturn dvUnit(v), nil\n\t}\n\n\tif dv, ok := v.(*dualValue); ok {\n\t\treturn dv, nil\n\t}\n\n\tretVal := borrowDV()\n\tretVal.Value = v\n\n\ts := v.Shape()\n\tdt := v.Dtype()\n\tmemsize := calcMemSize(dt, s)\n\t// allocate on device\n\tmem, err := op.Get(op.Device, memsize)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\td, err := makeValueFromMem(TypeOf(v), s, mem)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tretVal.d = d\n\n\treturn retVal, nil\n}\n\nfunc dvUnitVarManaged(v Value, op *ExternalOp) (*dualValue, error) {\n\tdv, err := dvUnitManaged(v, op)\n\tif err != nil {\n\t\treturn dv, err\n\t}\n\n\tswitch d := dv.d.(type) {\n\tcase tensor.Tensor:\n\t\tdt := d.Dtype()\n\t\tswitch dt {\n\t\tcase tensor.Float64:\n\t\t\td.Memset(1.0)\n\t\tcase tensor.Float32:\n\t\t\td.Memset(float32(1))\n\t\tcase tensor.Bool:\n\t\t\td.Memset(true)\n\t\tdefault:\n\t\t\treturn dv, errors.Errorf(\"Unhandled dtype: %v\", dt)\n\t\t}\n\tcase *F64:\n\t\t*d = F64(1)\n\tcase *F32:\n\t\t*d = F32(1)\n\tcase *I:\n\t\t*d = I(1)\n\tcase *I64:\n\t\t*d = I64(1)\n\tcase *I32:\n\t\t*d = I32(1)\n\tcase *U8:\n\t\t*d = U8(1)\n\tcase *B:\n\t\t*d = B(true)\n\tdefault:\n\t\treturn dv, errors.Errorf(\"Unhandeled type: %T\", d)\n\t}\n\treturn dv, nil\n}\n\n// helper to unpack from []*dualValue\nfunc idValue(inputs []*dualValue) (retVals []Value) {\n\tretVals = make([]Value, len(inputs))\n\tfor i, input := range inputs {\n\t\tretVals[i] = input.Value\n\t}\n\treturn\n}\n\n// dvBind applies an op to the inputs, and returns a *dualValue\nfunc dvBind(op Op, inputs []*dualValue) (retVal *dualValue, err error) {\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tvals := idValue(inputs)\n\n\tvar ret Value\n\tif ret, err = op.Do(vals...); err != nil {\n\t\treturn nil, errors.Wrap(err, opDoFail)\n\t}\n\tif o, ok := op.(*ExternalOp); ok {\n\t\treturn dvUnitManaged(ret, o)\n\t}\n\treturn dvUnit(ret), nil\n}\n\n// dvBindVar returns a dvUnitVar instead of dvUnit (which zeroes the derivative).\n// The default derivative of a variable wrt itself is 1 (dx/dx == 1)\nfunc dvBindVar(op Op, inputs []*dualValue) (retVal *dualValue, err error) {\n\tvals := idValue(inputs)\n\n\tvar ret Value\n\tif ret, err = op.Do(vals...); err != nil {\n\t\treturn nil, errors.Wrap(err, opDoFail)\n\t}\n\tif o, ok := op.(*ExternalOp); ok {\n\t\treturn dvUnitVarManaged(ret, o)\n\t}\n\treturn dvUnitVar(ret), nil\n}\n\n//TODO test vecvecdot divBind0\n\n// doesn't alloc a dualValue, and reuses whatever that is there, and zeroes out the deriv\nfunc dvBind0(op Op, retVal *dualValue, inputs []*dualValue) (err error) {\n\tprealloc := retVal.Value\n\tvals := idValue(inputs)\n\n\tvar ret Value\n\tif pd, ok := op.(UsePreallocDoer); ok {\n\t\tif ret, err = pd.UsePreallocDo(prealloc, vals...); err == nil {\n\t\t\tgoto next\n\t\t}\n\t}\n\tif ret, err = op.Do(vals...); err != nil {\n\t\treturn errors.Wrap(err, opDoFail)\n\t}\n\nnext:\n\tif err != nil {\n\t\treturn\n\t}\n\n\tif err = retVal.SetValue(ret); err != nil {\n\t\treturn\n\t}\n\n\tretVal.SetDeriv(ZeroValue(retVal.d))\n\treturn\n}\n\nfunc dvBindVar0(op Op, retVal *dualValue, inputs []*dualValue) (err error) {\n\tprealloc := retVal.Value\n\n\tvals := idValue(inputs)\n\n\tvar ret Value\n\tif pd, ok := op.(UsePreallocDoer); ok {\n\t\tret, err = pd.UsePreallocDo(prealloc, vals...)\n\t} else {\n\t\tif ret, err = op.Do(vals...); err != nil {\n\t\t\treturn errors.Wrap(err, opDoFail)\n\t\t}\n\t}\n\n\tif err != nil {\n\t\treturn errors.Wrapf(err, opDoFail)\n\t}\n\n\tif err = retVal.SetValue(ret); err != nil {\n\t\treturn errors.Wrap(err, \"Failed at setting the value\")\n\t}\n\n\tswitch v := retVal.d.(type) {\n\tcase Scalar:\n\t\tretVal.d = one(v.Dtype())\n\tcase tensor.Tensor:\n\t\tswitch v.Dtype() {\n\t\tcase tensor.Float64:\n\t\t\terr = v.Memset(float64(1))\n\t\tcase tensor.Float32:\n\t\t\terr = v.Memset(float32(1))\n\t\t}\n\t\tretVal.d = v\n\tdefault:\n\t\terr = errors.Errorf(nyiTypeFail, \"dvBindVar0\", retVal.d)\n\t}\n\treturn\n}\n"
        },
        {
          "name": "dual_test.go",
          "type": "blob",
          "size": 1.0712890625,
          "content": "package gorgonia\n\nimport (\n\t\"testing\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc Test_dvBind0(t *testing.T) {\n\tvar x, y, z Value\n\tvar xT, yT, zT hm.Type\n\tx, xT = anyToScalar(2.0)\n\ty, yT = anyToScalar(3.0)\n\tz, zT = anyToScalar(0.0)\n\n\top := newEBOByType(addOpType, xT, yT)\n\txdv := constantDV(x)\n\tydv := constantDV(y)\n\tzdv := constantDV(z)\n\tdvBind0(op, zdv, []*dualValue{xdv, ydv})\n\n\tt.Logf(\"%v %v\", zdv, zT)\n\n}\n\nfunc TestDVBindVar(t *testing.T) {\n\tvar x, y Value\n\tvar xT, yT hm.Type\n\tx, xT = anyToScalar(2.0)\n\ty, yT = anyToScalar(3.0)\n\n\top := newEBOByType(addOpType, xT, yT)\n\txdv := constantDV(x)\n\tydv := constantDV(y)\n\tretVal, err := dvBindVar(op, []*dualValue{xdv, ydv})\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tassert.Equal(t, 1.0, retVal.d.Data())\n\n\tx = tensor.New(tensor.WithBacking([]float64{4, 3, 2, 1}))\n\top = newEBOByType(addOpType, TypeOf(x), TypeOf(y))\n\txdv = constantDV(x)\n\tydv = constantDV(y)\n\tif retVal, err = dvBindVar(op, []*dualValue{xdv, ydv}); err != nil {\n\t\tt.Error(err)\n\t}\n\tassert.Equal(t, []float64{1, 1, 1, 1}, retVal.d.Data())\n}\n"
        },
        {
          "name": "encoding",
          "type": "tree",
          "content": null
        },
        {
          "name": "engine.go",
          "type": "blob",
          "size": 1.38671875,
          "content": "package gorgonia\n\nimport (\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\n// StandardEngine is the default CPU engine for gorgonia\ntype StandardEngine struct {\n\ttensor.StdEng\n}\n\n// Transpose tensor a according to expStrides\nfunc (e StandardEngine) Transpose(a tensor.Tensor, expStrides []int) error {\n\tif !a.IsNativelyAccessible() {\n\t\treturn errors.Errorf(\"Cannot Transpose() on non-natively accessible tensor\")\n\t}\n\n\tsize := a.DataSize()\n\tit := a.Iterator()\n\tvar i int\n\tswitch a.Dtype() {\n\tcase tensor.Float64:\n\t\ttmp := make([]float64, size)\n\t\tdata := a.Data().([]float64)\n\t\tdata = data[:size]\n\t\tnexts := make([]int, 0, size)\n\t\tfor next, err := it.Next(); err == nil; next, err = it.Next() {\n\t\t\t//tmp[i] = data[next]\n\t\t\t//i++\n\t\t\tnexts = append(nexts, next)\n\t\t}\n\t\tmaxNexts := nexts[len(nexts)-1] + 1 // this is possible because first and last elems don't change in a transpose\n\n\t\tdata = data[:maxNexts] // bounds check\n\t\ttmp = tmp[:maxNexts]   // bounds check\n\n\t\t// you might feel like this loop can be parallelized.\n\t\t// you would be wrong.\n\t\tfor i := 0; i < len(nexts); i++ {\n\t\t\tnext := nexts[i]\n\t\t\ttmp[i] = data[next]\n\t\t}\n\n\t\tcopy(data, tmp)\n\tcase tensor.Float32:\n\t\ttmp := make([]float32, size)\n\t\tdata := a.Data().([]float32)\n\t\tfor next, err := it.Next(); err == nil; next, err = it.Next() {\n\t\t\ttmp[i] = data[next]\n\t\t\ti++\n\t\t}\n\t\tcopy(data, tmp)\n\tdefault:\n\t\treturn e.StdEng.Transpose(a, expStrides)\n\t}\n\treturn nil\n\n}\n"
        },
        {
          "name": "engine_test.go",
          "type": "blob",
          "size": 1.5087890625,
          "content": "package gorgonia\n\nimport (\n\t\"reflect\"\n\t\"testing\"\n\n\t\"gorgonia.org/tensor\"\n)\n\nvar stdengType reflect.Type\n\nfunc init() {\n\tstdengType = reflect.TypeOf(StandardEngine{})\n}\n\nfunc assertEngine(v Value, eT reflect.Type) bool {\n\tte := engineOf(v)\n\tif te == nil {\n\t\treturn true\n\t}\n\tteT := reflect.TypeOf(te)\n\treturn eT == teT\n}\n\nfunc assertGraphEngine(t *testing.T, g *ExprGraph, eT reflect.Type) {\n\tfor _, n := range g.AllNodes() {\n\t\tif n.isInput() {\n\t\t\tinputEng := reflect.TypeOf(engineOf(n.Value()))\n\t\t\tif grad, err := n.Grad(); err == nil {\n\t\t\t\tif !assertEngine(grad, inputEng) {\n\t\t\t\t\tt.Errorf(\"Expected input %v value and gradient to share the same engine %v: Got %T\", n.Name(), inputEng, engineOf(grad))\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tif !assertEngine(n.Value(), eT) {\n\t\t\tt.Errorf(\"Expected node %v to be %v. Got %T instead\", n, eT, engineOf(n.Value()))\n\t\t\treturn\n\t\t}\n\n\t\tif grad, err := n.Grad(); err == nil {\n\t\t\tif !assertEngine(grad, eT) {\n\t\t\t\tt.Errorf(\"Expected gradient of node %v to be %v. Got %T instead\", n, eT, engineOf(grad))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc engineOf(v Value) tensor.Engine {\n\tif t, ok := v.(tensor.Tensor); ok {\n\t\treturn t.Engine()\n\t}\n\treturn nil\n}\n\nfunc TestBasicEngine(t *testing.T) {\n\tg, x, y, _ := simpleVecEqn()\n\n\tLet(x, tensor.New(tensor.WithBacking([]float64{0, 1})))\n\tLet(y, tensor.New(tensor.WithBacking([]float64{3, 2})))\n\tm := NewTapeMachine(g, TraceExec())\n\tdefer m.Close()\n\tif err := m.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif assertGraphEngine(t, g, stdengType); t.Failed() {\n\t\tt.FailNow()\n\t}\n}\n"
        },
        {
          "name": "equalities.go",
          "type": "blob",
          "size": 3.1767578125,
          "content": "package gorgonia\n\nimport (\n\t\"gorgonia.org/dawson\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc scalarEq(a, b Scalar) bool {\n\tswitch at := a.(type) {\n\tcase *F64:\n\t\tif bt, ok := b.(*F64); ok {\n\t\t\tif at == bt {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\treturn *at == *bt\n\t\t}\n\t\treturn false\n\tcase *F32:\n\t\tif bt, ok := b.(*F32); ok {\n\t\t\tif at == bt {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\treturn *at == *bt\n\t\t}\n\t\treturn false\n\tcase *I:\n\t\tif bt, ok := b.(*I); ok {\n\t\t\tif at == bt {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\treturn *at == *bt\n\t\t}\n\t\treturn false\n\tcase *I32:\n\t\tif bt, ok := b.(*I32); ok {\n\t\t\tif at == bt {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\treturn *at == *bt\n\t\t}\n\t\treturn false\n\tcase *I64:\n\t\tif bt, ok := b.(*I64); ok {\n\t\t\tif at == bt {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\treturn *at == *bt\n\t\t}\n\t\treturn false\n\tcase *U8:\n\t\tif bt, ok := b.(*U8); ok {\n\t\t\tif at == bt {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\treturn *at == *bt\n\t\t}\n\t\treturn false\n\tcase *B:\n\t\tif bt, ok := b.(*B); ok {\n\t\t\tif at == bt {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\treturn *at == *bt\n\t\t}\n\t\treturn false\n\t}\n\treturn false\n}\n\nfunc scalarClose(a, b Scalar) bool {\n\tswitch at := a.(type) {\n\tcase *F64:\n\t\tif bt, ok := b.(*F64); ok {\n\t\t\treturn dawson.CloseF64(float64(*at), float64(*bt))\n\t\t}\n\t\treturn false\n\tcase *F32:\n\t\tif bt, ok := b.(*F32); ok {\n\t\t\treturn dawson.CloseF32(float32(*at), float32(*bt))\n\t\t}\n\t\treturn false\n\tdefault:\n\t\treturn scalarEq(a, b)\n\t}\n}\n\nfunc tensorClose(a, b tensor.Tensor) bool {\n\taDt := a.Dtype()\n\tbDt := b.Dtype()\n\tif aDt != bDt {\n\t\treturn false\n\t}\n\n\tswitch aDt {\n\tcase tensor.Float64:\n\t\taFs := a.Data().([]float64)\n\t\tbFs := b.Data().([]float64)\n\t\tif len(aFs) != len(bFs) {\n\t\t\treturn false\n\t\t}\n\t\taFs = aFs[:]\n\t\tbFs = bFs[:len(aFs)]\n\t\tfor i, v := range aFs {\n\t\t\tif !dawson.CloseF64(v, bFs[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\treturn true\n\tcase tensor.Float32:\n\t\taFs := a.Data().([]float32)\n\t\tbFs := b.Data().([]float32)\n\t\tif len(aFs) != len(bFs) {\n\t\t\treturn false\n\t\t}\n\t\taFs = aFs[:]\n\t\tbFs = bFs[:len(aFs)]\n\t\tfor i, v := range aFs {\n\t\t\tif !dawson.CloseF32(v, bFs[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\treturn true\n\tdefault:\n\t\treturn a.Eq(b)\n\t}\n\n}\n\n/*\nfunc axesEq(a, b axes) bool {\n\tif len(a) != len(b) {\n\t\treturn false\n\t}\n\n\tfor i, s := range a {\n\t\tif b[i] != s {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// yes it's exactly the same as axesEq\nfunc coordEq(a, b coordinates) bool {\n\tif len(a) != len(b) {\n\t\treturn false\n\t}\n\n\tfor i, s := range a {\n\t\tif b[i] != s {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n*/\n\nfunc constEq(a, b constant) (ok bool) {\n\tswitch at := a.(type) {\n\tcase constantScalar:\n\t\tvar bt constantScalar\n\t\tif bt, ok = b.(constantScalar); !ok {\n\t\t\treturn\n\t\t}\n\n\t\treturn bt == at\n\tcase constantTensor:\n\t\tvar bt constantTensor\n\t\tif bt, ok = b.(constantTensor); !ok {\n\t\t\treturn\n\t\t}\n\t\treturn at.v.Eq(bt.v)\n\tdefault:\n\t\tpanic(\"Not yet implemented\")\n\t}\n}\n\n// fastest comparisons to least fastest\nfunc nodeEq(a, b *Node) bool {\n\tif a == b {\n\t\treturn true\n\t}\n\n\tif a.isInput() {\n\t\tif !b.isInput() {\n\t\t\treturn false\n\t\t}\n\t\treturn a.name == b.name\n\t}\n\n\tif b.isInput() {\n\t\treturn false\n\t}\n\n\t// hashcode is good for comparing Op (TODO: benchmark this vs reflect.DeepEq)\n\tif a.op.Hashcode() != b.op.Hashcode() {\n\t\treturn false\n\t}\n\n\tif len(a.children) != len(b.children) {\n\t\treturn false\n\t}\n\n\tif a.t != b.t {\n\t\treturn false\n\t}\n\n\tif !a.shape.Eq(b.shape) {\n\t\treturn false\n\t}\n\n\treturn true\n}\n"
        },
        {
          "name": "equalities_test.go",
          "type": "blob",
          "size": 1.00390625,
          "content": "package gorgonia\n\nimport \"testing\"\n\nvar scalarEqualities = []struct {\n\ta, b Scalar\n\teq   bool\n}{\n\t{NewF64(1), NewF64(1), true},\n\t{NewF64(1), NewF64(0), false},\n\t{NewF64(1), NewF32(1), false},\n\n\t{NewF32(1), NewF32(1), true},\n\t{NewF32(1), NewF32(0), false},\n\t{NewF32(1), NewI(1), false},\n\n\t{NewI(1), NewI(1), true},\n\t{NewI(1), NewI(0), false},\n\t{NewI(1), NewI64(1), false},\n\n\t{NewI64(1), NewI64(1), true},\n\t{NewI64(1), NewI64(0), false},\n\t{NewI64(1), NewI32(1), false},\n\n\t{NewI32(1), NewI32(1), true},\n\t{NewI32(1), NewI32(0), false},\n\t{NewI32(1), NewU8(1), false},\n\n\t{NewU8(1), NewU8(1), true},\n\t{NewU8(1), NewU8(0), false},\n\t{NewU8(1), NewB(true), false},\n\n\t{NewB(true), NewB(true), true},\n\t{NewB(true), NewB(false), false},\n\t{NewB(true), NewF64(1), false},\n}\n\nfunc TestScalarEq(t *testing.T) {\n\tfor _, seq := range scalarEqualities {\n\t\tif (scalarEq(seq.a, seq.b) && !seq.eq) || (!scalarEq(seq.a, seq.b) && seq.eq) {\n\t\t\tt.Errorf(\"expected %v(%v) and %v(%v) to be %v\", seq.a, TypeOf(seq.a), seq.b, TypeOf(seq.b), seq.eq)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "ermagerdmonards.go",
          "type": "blob",
          "size": 4.1669921875,
          "content": "package gorgonia\n\nimport (\n\t\"github.com/pkg/errors\"\n)\n\nvar (\n\t_ Result = (*Node)(nil)\n\t_ Result = (Nodes)(nil)\n\t_ Result = gErr{}\n)\n\n// Result is either a Node or Nodes or error. It's a poor man's sum types and it's not sealed for good reason\ntype Result interface {\n\tInput\n\tErrer\n}\n\n// Input is something that can produce both a *Node and Nodes. Returning nil is OK.\ntype Input interface {\n\tNode() *Node\n\tNodes() Nodes\n}\n\n// Errer is an interface that can return an error.\ntype Errer interface {\n\tErr() error\n}\n\n// Mker is an interface of any Input that can make a new version of itself\ntype Mker interface {\n\tMk(...Input) Input\n}\n\n// Lift1  decorates a function with a precheck and post function lifting\nfunc Lift1(fn func(a *Node) (*Node, error)) func(a Input) Result {\n\treturn func(a Input) Result {\n\t\tif err := CheckOne(a); err != nil {\n\t\t\treturn Err(errors.WithStack(err))\n\t\t}\n\t\treturn TransformResult(a)(fn(a.Node()))\n\t}\n}\n\n// Lift1Axial  decorates a function with a precheck and post function lifting\nfunc Lift1Axial(fn func(a *Node, axes ...int) (*Node, error)) func(a Input, axes ...int) Result {\n\treturn func(a Input, axes ...int) Result {\n\t\tif err := CheckOne(a); err != nil {\n\t\t\treturn Err(errors.WithStack(err))\n\t\t}\n\t\treturn TransformResult(a)(fn(a.Node(), axes...))\n\t}\n}\n\n// Lift2 decorates a function with a precheck and post function lifting\nfunc Lift2(fn func(a, b *Node) (*Node, error)) func(a, b Input) Result {\n\treturn func(a, b Input) Result {\n\t\tif err := CheckOne(a); err != nil {\n\t\t\treturn Err(errors.WithStack(err))\n\t\t}\n\t\tif err := CheckOne(b); err != nil {\n\t\t\treturn Err(errors.WithStack(err))\n\t\t}\n\t\treturn TransformResult(a, b)(fn(a.Node(), b.Node()))\n\t}\n}\n\n// Lift2Broadcast decorates a function with a precheck and post function lifting\nfunc Lift2Broadcast(fn func(a, b *Node, pat1, pat2 []byte) (*Node, error)) func(a, b Input, pat1, pat2 []byte) Result {\n\treturn func(a, b Input, pat1, pat2 []byte) Result {\n\t\tif err := CheckOne(a); err != nil {\n\t\t\treturn Err(errors.WithStack(err))\n\t\t}\n\t\tif err := CheckOne(b); err != nil {\n\t\t\treturn Err(errors.WithStack(err))\n\t\t}\n\t\treturn TransformResult(a, b)(fn(a.Node(), b.Node(), pat1, pat2))\n\t}\n}\n\n// gErr implements Result and error.\ntype gErr struct{ error }\n\n// Err is a function that returns a gErr. It wraps errors with stack information.\n// A gErr implements Result, as well as error.\n// This way, the Err() method acts as an unwrapper.\nfunc Err(e error) gErr { return gErr{errors.WithStack(e)} }\n\nfunc (err gErr) Node() *Node  { return nil }\nfunc (err gErr) Nodes() Nodes { return nil }\nfunc (err gErr) Err() error   { return err.error }\n\n// resultM is a wrapper for Input to create a Result. This is the default Result if an unknown Input was passed in.\ntype resultM struct{ Input }\n\nfunc (r resultM) Err() error { return nil }\n\n// LiftResult creates a Result from a Input and error pair.\n// If the error is not nil, the Input is discarded.\n//\n// The usual use case is in a function that returns a `(*Node, error)`.\n// e.g LiftResult(Add(a, b))\nfunc LiftResult(a Input, err error) Result {\n\tif err != nil {\n\t\treturn Err(err)\n\t}\n\tswitch at := a.(type) {\n\tcase Result:\n\t\treturn at\n\tdefault:\n\t\treturn resultM{a}\n\t}\n}\n\n// TransformResult is like LiftResult, but allows for custom data types that fulfil Mker\nfunc TransformResult(ins ...Input) func(a Input, err error) Result {\n\treturn func(a Input, err error) Result {\n\t\tif err != nil {\n\t\t\treturn Err(err)\n\t\t}\n\t\tfor _, in := range ins {\n\t\t\tif mk, ok := in.(Mker); ok {\n\t\t\t\ta = mk.Mk(a)\n\t\t\t}\n\t\t}\n\t\tswitch at := a.(type) {\n\t\tcase Result:\n\t\t\treturn at\n\t\tdefault:\n\t\t\treturn resultM{a}\n\t\t}\n\t}\n}\n\n// CheckOne checks whether an input is an error\nfunc CheckOne(in Input) error {\n\tif errer, ok := in.(Errer); ok && errer.Err() != nil {\n\t\treturn errer.Err()\n\t}\n\treturn nil\n}\n\n// NodesFromInputs creates a Nodes from a list of Input.\nfunc NodesFromInputs(xs ...Input) (Nodes, error) {\n\tfor i := range xs {\n\t\tif err := CheckOne(xs[i]); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, \"NodesFromInputs %dth input\", i)\n\t\t}\n\t\t// check if the Input is a *Node\n\t\tif xs[i].Node() == nil {\n\t\t\treturn nil, errors.Errorf(\"Input %d is not a *Node\", i)\n\t\t}\n\t}\n\n\tretVal := make(Nodes, len(xs))\n\tfor i := range xs {\n\t\tretVal[i] = xs[i].Node()\n\t}\n\treturn retVal, nil\n}\n"
        },
        {
          "name": "errors.go",
          "type": "blob",
          "size": 3.1611328125,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pkg/errors\"\n)\n\n// NoOpError is an error returned when an operation does nothing.\ntype NoOpError interface {\n\tNoOp() bool\n}\n\ntype noopError struct{}\n\nfunc (e noopError) NoOp() bool    { return true }\nfunc (e noopError) Error() string { return \"NoOp\" }\n\n// errNoStabilization is an error used internally for when there is no stabilization mechanism is found.\ntype errNoStabilization interface {\n\terror\n\tnoStabilization() bool\n}\n\n// nostabilizationErr is used internally to communicate that there isn't any stabilization possible\ntype noStabilizationErr struct{}\n\nfunc (noStabilizationErr) Error() string         { return \"No stabilization mechanism found\" }\nfunc (noStabilizationErr) noStabilization() bool { return true }\n\n// noIncrErr is an error used internally when a Value cannot be incremented\ntype noIncrErr struct {\n\tv Value\n}\n\nfunc (noIncrErr) Error() string  { return incrErr }\nfunc (e noIncrErr) Value() Value { return e.v }\n\n// oomError represents an Out of tensor.Memory error. It is typically used for CUDA related machine work\ntype oomError struct {\n\tres       int64\n\tallocated int64\n}\n\nfunc (e oomError) Reserved() int64  { return e.res }\nfunc (e oomError) Allocated() int64 { return e.allocated }\nfunc (e oomError) Error() string    { return fmt.Sprintf(\"allocated/reserved: %v/%v\", e.allocated, e.res) }\n\n// AutoDiffError is an error which should be passed if the function is not differentiable. This is useful for Op implementations\ntype AutoDiffError struct{}\n\nfunc (err AutoDiffError) Error() string { return \"AutoDiffError\" }\n\n// vmContextualError is an error that is used to wrap errors that arise from the VM\ntype vmContextualError struct {\n\terror\n\tnode  *Node // which node was it processing\n\tinstr int   // what instruction ID it was\n}\n\nfunc (err vmContextualError) Node() *Node        { return err.node }\nfunc (err vmContextualError) Value() Value       { return err.node.Value() }\nfunc (err vmContextualError) InstructionID() int { return err.instr }\nfunc (err vmContextualError) Err() error         { return err.error }\n\nfunc nyi(what string, implFor interface{}) error {\n\treturn errors.Errorf(nyiFail, what, implFor)\n}\n\nfunc nondiffErr(op Op) error {\n\treturn errors.Errorf(\"%s is a non-differentiable function\", op)\n}\n\n// checkErrSetDeriv sets the deriv if the error is a Valuer. Helper function for linalg operations\nfunc checkErrSetDeriv(err error, dv *dualValue) error {\n\tif ver, ok := err.(Valuer); ok {\n\t\treturn dv.SetDeriv(ver.Value())\n\t}\n\treturn err\n}\n\n// SymDiffError provides the context at which an error occurred\ntype SymDiffError struct {\n\tnodes   Nodes\n\tsingle  *Node\n\tgrad    *Node\n\tgradMap map[*Node]Nodes\n\terr     error\n}\n\nfunc (err SymDiffError) Error() string { return err.err.Error() }\n\n// Nodes returns the nodes involved in the error\nfunc (err SymDiffError) Nodes() Nodes { return err.nodes }\n\n// Node returns a specific node involved in the error\nfunc (err SymDiffError) Node() *Node { return err.single }\n\n// Grads returns the grads involved in the error\nfunc (err SymDiffError) Grads() map[*Node]Nodes { return err.gradMap }\n\n// Grad returns a specific grad involved in the error\nfunc (err SymDiffError) Grad() *Node { return err.grad }\n"
        },
        {
          "name": "example_autodiff_test.go",
          "type": "blob",
          "size": 0.8330078125,
          "content": "package gorgonia_test\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\n\t. \"gorgonia.org/gorgonia\"\n)\n\n// Autodiff showcases automatic differentiation\nfunc Example_autodiff() {\n\tg := NewGraph()\n\n\tvar x, y, z *Node\n\tvar err error\n\n\t// define the expression\n\tx = NewScalar(g, Float64, WithName(\"x\"))\n\ty = NewScalar(g, Float64, WithName(\"y\"))\n\tif z, err = Add(x, y); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// set initial values then run\n\tLet(x, 2.0)\n\tLet(y, 2.5)\n\n\t// by default, LispMachine performs forward mode and backwards mode execution\n\tm := NewLispMachine(g)\n\tdefer m.Close()\n\tif err = m.RunAll(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tfmt.Printf(\"z: %v\\n\", z.Value())\n\n\tif xgrad, err := x.Grad(); err == nil {\n\t\tfmt.Printf(\"dz/dx: %v\\n\", xgrad)\n\t}\n\n\tif ygrad, err := y.Grad(); err == nil {\n\t\tfmt.Printf(\"dz/dy: %v\\n\", ygrad)\n\t}\n\n\t// Output:\n\t// z: 4.5\n\t// dz/dx: 1\n\t// dz/dy: 1\n}\n"
        },
        {
          "name": "example_basic_test.go",
          "type": "blob",
          "size": 0.7109375,
          "content": "package gorgonia_test\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\n\t. \"gorgonia.org/gorgonia\"\n)\n\n// Basic example of representing mathematical equations as graphs.\n//\n// In this example, we want to represent the following equation\n//\t\tz = x + y\nfunc Example_basic() {\n\tg := NewGraph()\n\n\tvar x, y, z *Node\n\tvar err error\n\n\t// define the expression\n\tx = NewScalar(g, Float64, WithName(\"x\"))\n\ty = NewScalar(g, Float64, WithName(\"y\"))\n\tif z, err = Add(x, y); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// create a VM to run the program on\n\tmachine := NewTapeMachine(g)\n\tdefer machine.Close()\n\n\t// set initial values then run\n\tLet(x, 2.0)\n\tLet(y, 2.5)\n\tif err = machine.RunAll(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tfmt.Printf(\"%v\", z.Value())\n\t// Output: 4.5\n}\n"
        },
        {
          "name": "example_broadcast_op_test.go",
          "type": "blob",
          "size": 2.7294921875,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\n\t// . \"gorgonia.org/gorgonia\"\n\t\"gorgonia.org/tensor\"\n)\n\n// By default, Gorgonia operations do not perform broadcasting.\n// To do broadcasting, you would need to manually specify the operation\nfunc ExampleBroadcastAdd() {\n\tg := NewGraph()\n\ta := NewVector(g, tensor.Float64, WithShape(2), WithName(\"a\"), WithValue(tensor.New(tensor.WithBacking([]float64{100, 100}))))\n\tb := NewMatrix(g, tensor.Float64, WithShape(2, 2), WithName(\"b\"), WithValue(tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 1, 2, 2}))))\n\n\tfmt.Printf(\"a = %v\\nb =\\n%v\\n\", a.Value(), b.Value())\n\n\t_, err := Add(a, b)\n\tfmt.Printf(\"a + b yields an error: %v\\n\\n\", err)\n\n\t// Note here the broadcasting of a is on the first axis, not the zeroth axis. Simply put, assume that it's already a (2,1) matrix.\n\tab, err := BroadcastAdd(a, b, []byte{1}, nil)\n\tif err != nil {\n\t\tfmt.Printf(\"uh oh, something went wrong: %v\\n\", err)\n\t}\n\n\tba, err := BroadcastAdd(b, a, nil, []byte{1})\n\tif err != nil {\n\t\tfmt.Printf(\"uh oh, something went wrong: %v\\n\", err)\n\t}\n\n\t// Now, let's run the program\n\tmachine := NewTapeMachine(g)\n\tdefer machine.Close()\n\tif err = machine.RunAll(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tfmt.Printf(\"a +⃗ b =\\n%v\\n\", ab.Value())\n\tfmt.Printf(\"b +⃗ a =\\n%v\", ba.Value())\n\n\t// Output:\n\t// a = [100  100]\n\t// b =\n\t// ⎡1  1⎤\n\t// ⎣2  2⎦\n\t//\n\t// a + b yields an error: Failed to infer shape. Op: + false: Shape mismatch: (2) and (2, 2)\n\t//\n\t// a +⃗ b =\n\t// ⎡101  101⎤\n\t// ⎣102  102⎦\n\t//\n\t// b +⃗ a =\n\t// ⎡101  101⎤\n\t// ⎣102  102⎦\n\n}\n\nfunc ExampleBroadcastGte_creatingTriangleMatrices() {\n\t// Broadcasting is useful. We can create triangular dense matrices simply\n\n\tg := NewGraph()\n\ta := NewMatrix(g, tensor.Float64, WithShape(3, 1), WithName(\"a\"), WithInit(RangedFrom(0)))\n\tb := NewMatrix(g, tensor.Float64, WithShape(1, 4), WithName(\"b\"), WithInit(RangedFrom(0)))\n\ttl, err := BroadcastGte(a, b, true, []byte{1}, []byte{0})\n\tif err != nil {\n\t\tlog.Fatalf(\"uh oh. Something went wrong %v\", err)\n\t}\n\n\ttu, err := BroadcastLt(a, b, true, []byte{1}, []byte{0})\n\tif err != nil {\n\t\tlog.Fatalf(\"uh oh. Something went wrong %v\", err)\n\t}\n\n\tm := NewTapeMachine(g)\n\n\t// PEDAGOGICAL:\n\t// Uncomment the following code if you want to see what happens behind the scenes\n\t// m.Close()\n\t// logger := log.New(os.Stderr, \"\",0)\n\t// m = NewTapeMachine(g, WithLogger(logger), WithWatchlist())\n\n\tdefer m.Close()\n\tif err = m.RunAll(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tfmt.Printf(\"triangular, lower:\\n%v\\n\", tl.Value())\n\tfmt.Printf(\"triangular, upper:\\n%v\\n\", tu.Value())\n\n\t// Output:\n\t// triangular, lower:\n\t// ⎡1  0  0  0⎤\n\t// ⎢1  1  0  0⎥\n\t// ⎣1  1  1  0⎦\n\t//\n\t// triangular, upper:\n\t// ⎡0  1  1  1⎤\n\t// ⎢0  0  1  1⎥\n\t// ⎣0  0  0  1⎦\n\n}\n"
        },
        {
          "name": "example_concurrent_training_test.go",
          "type": "blob",
          "size": 5.1025390625,
          "content": "package gorgonia_test\n\nimport (\n\t\"fmt\"\n\t\"runtime\"\n\t\"sync\"\n\n\t. \"gorgonia.org/gorgonia\"\n\t\"gorgonia.org/tensor\"\n)\n\nconst (\n\t// rows      = 373127\n\t// cols      = 53\n\n\t// We'll use a nice even sized batch size, instead of weird prime numbers\n\trows      = 30000\n\tcols      = 5\n\tbatchSize = 100\n\tepochs    = 10\n)\n\ntype concurrentTrainer struct {\n\tg    *ExprGraph\n\tx, y *Node\n\tvm   VM\n\t// cost Value\n\n\tbatchSize int\n\tepoch     int // number of epochs done\n}\n\nfunc newConcurrentTrainer() *concurrentTrainer {\n\tg := NewGraph()\n\tx := NewMatrix(g, Float64, WithShape(batchSize, cols), WithName(\"x\"))\n\ty := NewVector(g, Float64, WithShape(batchSize), WithName(\"y\"))\n\txT := Must(Transpose(x))\n\tz := Must(Mul(xT, y))\n\tsz := Must(Sum(z))\n\n\t// Read(sz, &ct.cost)\n\tGrad(sz, x, y)\n\tvm := NewTapeMachine(g, BindDualValues(x, y))\n\n\treturn &concurrentTrainer{\n\t\tg:  g,\n\t\tx:  x,\n\t\ty:  y,\n\t\tvm: vm,\n\n\t\tbatchSize: batchSize,\n\t\tepoch:     -1,\n\t}\n}\n\ntype cost struct {\n\tNodes []ValueGrad\n\tVM    // placed for debugging purposes. In real life use you can just use a channel of Nodes\n\n\t// cost Value\n}\n\nfunc (t *concurrentTrainer) train(x, y Value, costChan chan cost, wg *sync.WaitGroup) {\n\tLet(t.x, x)\n\tLet(t.y, y)\n\tif err := t.vm.RunAll(); err != nil {\n\t\tpanic(\"HELP\")\n\t}\n\n\tcostChan <- cost{\n\t\t[]ValueGrad{t.x, t.y},\n\t\tt.vm,\n\t\t// t.cost,\n\t}\n\n\tt.vm.Reset()\n\twg.Done()\n}\n\nfunc trainEpoch(bs []batch, ts []*concurrentTrainer, threads int) {\n\t// costs := make([]float64, 0, len(bs))\n\tchunks := len(bs) / len(ts)\n\tfor chunk := 0; chunk <= chunks; chunk++ {\n\t\tcostChan := make(chan cost, len(bs))\n\n\t\tvar wg sync.WaitGroup\n\t\tfor i, t := range ts {\n\t\t\tidx := chunk*threads + i\n\t\t\tif idx >= len(bs) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tb := bs[idx]\n\n\t\t\twg.Add(1)\n\t\t\tgo t.train(b.xs, b.ys, costChan, &wg)\n\t\t}\n\t\twg.Wait()\n\t\tclose(costChan)\n\n\t\tsolver := NewVanillaSolver(WithLearnRate(0.01), WithBatchSize(batchSize))\n\t\tfor cost := range costChan {\n\t\t\t// y := cost.Nodes[1].Value()\n\t\t\t// yG, _ := cost.Nodes[1].Grad()\n\t\t\t// c := cost.cost.Data().(float64)\n\t\t\t// costs = append(costs, c)\n\t\t\tsolver.Step(cost.Nodes)\n\t\t}\n\t}\n\n\t// var avg float64\n\t// for _, c := range costs {\n\t// \tavg += c\n\t// }\n\t// avg /= float64(len(costs))\n}\n\ntype batch struct {\n\txs Value\n\tys Value\n}\n\nfunc prep() (x, y Value, bs []batch) {\n\txV := tensor.New(tensor.WithShape(rows, cols), tensor.WithBacking(tensor.Range(Float64, 0, cols*rows)))\n\tyV := tensor.New(tensor.WithShape(rows), tensor.WithBacking(tensor.Range(Float64, 0, rows)))\n\n\t// prep the data: y = ΣnX, where n = col ID, x ∈ X = colID / 100\n\txData := xV.Data().([]float64)\n\tyData := yV.Data().([]float64)\n\tfor r := 0; r < rows; r++ {\n\t\tvar sum float64\n\t\tfor c := 0; c < cols; c++ {\n\t\t\tidx := r*cols + c\n\t\t\tfc := float64(c)\n\t\t\tv := fc * fc / 100\n\t\t\txData[idx] = v\n\t\t\tsum += v\n\t\t}\n\t\tyData[r] = sum\n\t}\n\n\t// batch the examples up into their respective batchSize\n\tfor i := 0; i < rows; i += batchSize {\n\t\txVS, _ := xV.Slice(S(i, i+batchSize))\n\t\tyVS, _ := yV.Slice(S(i, i+batchSize))\n\t\tb := batch{xVS, yVS}\n\t\tbs = append(bs, b)\n\t}\n\treturn xV, yV, bs\n}\n\nfunc concurrentTraining(xV, yV Value, bs []batch, es int) {\n\tthreads := runtime.NumCPU()\n\n\tts := make([]*concurrentTrainer, threads)\n\tfor chunk := 0; chunk < threads; chunk++ {\n\t\ttrainer := newConcurrentTrainer()\n\t\tts[chunk] = trainer\n\t\tdefer trainer.vm.Close()\n\t}\n\n\tfor e := 0; e < es; e++ {\n\t\ttrainEpoch(bs, ts, threads)\n\t}\n}\n\nfunc nonConcurrentTraining(xV, yV Value, es int) {\n\tg := NewGraph()\n\tx := NewMatrix(g, Float64, WithValue(xV))\n\ty := NewVector(g, Float64, WithValue(yV))\n\txT := Must(Transpose(x))\n\tz := Must(Mul(xT, y))\n\tsz := Must(Sum(z))\n\tGrad(sz, x, y)\n\tvm := NewTapeMachine(g, BindDualValues(x, y))\n\n\tLet(x, xV)\n\tLet(y, yV)\n\tsolver := NewVanillaSolver(WithLearnRate(0.01), WithBatchSize(batchSize))\n\tfor i := 0; i < es; i++ {\n\t\tvm.RunAll()\n\t\tsolver.Step([]ValueGrad{x, y})\n\t\tvm.Reset()\n\t\truntime.GC()\n\t}\n}\n\nfunc Example_concurrentTraining() {\n\txV, yV, bs := prep()\n\tconcurrentTraining(xV, yV, bs, epochs)\n\n\tfmt.Printf(\"x:\\n%1.1v\", xV)\n\tfmt.Printf(\"y:\\n%1.1v\", yV)\n\n\t// Output:\n\t// x:\n\t// ⎡-0.0003     0.01     0.04     0.09      0.2⎤\n\t// ⎢-0.0003     0.01     0.04     0.09      0.2⎥\n\t// ⎢-0.0003     0.01     0.04     0.09      0.2⎥\n\t// ⎢-0.0003     0.01     0.04     0.09      0.2⎥\n\t// .\n\t// .\n\t// .\n\t// ⎢-0.0003     0.01     0.04     0.09      0.2⎥\n\t// ⎢-0.0003     0.01     0.04     0.09      0.2⎥\n\t// ⎢-0.0003     0.01     0.04     0.09      0.2⎥\n\t// ⎣-0.0003     0.01     0.04     0.09      0.2⎦\n\t// y:\n\t// [0.3  0.3  0.3  0.3  ... 0.3  0.3  0.3  0.3]\n\n}\n\nfunc Example_nonConcurrentTraining() {\n\txV, yV, _ := prep()\n\tnonConcurrentTraining(xV, yV, epochs)\n\n\tfmt.Printf(\"x:\\n%1.1v\", xV)\n\tfmt.Printf(\"y:\\n%1.1v\", yV)\n\n\t//Output:\n\t// x:\n\t// ⎡-0.0003     0.01     0.04     0.09      0.2⎤\n\t// ⎢-0.0003     0.01     0.04     0.09      0.2⎥\n\t// ⎢-0.0003     0.01     0.04     0.09      0.2⎥\n\t// ⎢-0.0003     0.01     0.04     0.09      0.2⎥\n\t// .\n\t// .\n\t// .\n\t// ⎢-0.0003     0.01     0.04     0.09      0.2⎥\n\t// ⎢-0.0003     0.01     0.04     0.09      0.2⎥\n\t// ⎢-0.0003     0.01     0.04     0.09      0.2⎥\n\t// ⎣-0.0003     0.01     0.04     0.09      0.2⎦\n\t// y:\n\t// [0.3  0.3  0.3  0.3  ... 0.3  0.3  0.3  0.3]\n\n}\n"
        },
        {
          "name": "example_err_test.go",
          "type": "blob",
          "size": 2.6962890625,
          "content": "package gorgonia_test\n\nimport (\n\t\"fmt\"\n\n\t. \"gorgonia.org/gorgonia\"\n)\n\n// Gorgonia provides an API that is fairly idiomatic - most of the functions in in the API return (T, error).\n// This is useful for many cases, such as an interactive shell for deep learning.\n// However, it must also be acknowledged that this makes composing functions together a bit cumbersome.\n//\n// To that end, Gorgonia provides two alternative methods. First, the `Lift` based functions; Second the `Must` function\nfunc Example_errorHandling() {\n\t// Lift\n\tg := NewGraph()\n\tx := NewMatrix(g, Float32, WithShape(2, 3), WithInit(RangedFrom(0)), WithName(\"a\"))\n\ty := NewMatrix(g, Float32, WithShape(3, 2), WithInit(ValuesOf(float32(2))), WithName(\"b\"))\n\tz := NewMatrix(g, Float32, WithShape(2, 1), WithInit(Zeroes()), WithName(\"bias\"))\n\twrong := NewMatrix(g, Float64, WithShape(2, 3), WithInit(RangedFrom(0)), WithName(\"wrong\"))\n\n\t// Different LiftXXX functions exist for different API signatures\n\t// A good way to do this is to have some instantiated functions at the top level of the package\n\tmul := Lift2(Mul)\n\tadd := Lift2(Add)\n\taddB := Lift2Broadcast(BroadcastAdd)\n\tsq := Lift1(Square)\n\tsm := Lift1Axial(SoftMax)\n\n\tnn := sm(sq(addB(mul(x, y), z, nil, []byte{1}))) // OK\n\tnnPlusWrong := add(nn, wrong)                    // Wrong types. Will Error\n\tfmt.Printf(\"nn: %v\\nAn error occurs: %v\\n\", nn, nnPlusWrong.Err())\n\n\t// Must()\n\th := NewGraph()\n\ta := NewMatrix(h, Float32, WithShape(2, 3), WithInit(RangedFrom(0)), WithName(\"a\"))\n\tb := NewMatrix(h, Float32, WithShape(3, 2), WithInit(ValuesOf(float32(2))), WithName(\"b\"))\n\tc := NewMatrix(h, Float32, WithShape(2, 1), WithInit(RangedFrom(0)), WithName(\"c\"))\n\twrong2 := NewMatrix(h, Float64, WithShape(2, 3), WithInit(RangedFrom(0)), WithName(\"wrong\"))\n\n\t// This is OK\n\tnn2 := Must(SoftMax(\n\t\tMust(Square(\n\t\t\tMust(BroadcastAdd(\n\t\t\t\tMust(Mul(a, b)),\n\t\t\t\tc,\n\t\t\t\tnil, []byte{1},\n\t\t\t)),\n\t\t)),\n\t))\n\n\tfmt.Printf(\"nn2: %v\\n\", nn2)\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tfmt.Printf(\"An error occurs (caught by recover()): %v\\n\", r)\n\t\t}\n\t}()\n\n\tnn2PlusWrong := Must(Add(nn2, wrong2))\n\t_ = nn2PlusWrong\n\n\t// Output:\n\t// nn: Softmax{-1, false}()(%9) :: Matrix float32\n\t// An error occurs: Type inference error. Op: + false. Children: [Matrix float32, Matrix float64], OpType:Matrix a → Matrix a → Matrix a: Unable to unify while inferring type of + false: Unification Fail: float64 ~ float32 cannot be unified\n\t// nn2: Softmax{-1, false}()(%9) :: Matrix float32\n\t// An error occurs (caught by recover()): Type inference error. Op: + false. Children: [Matrix float32, Matrix float64], OpType:Matrix a → Matrix a → Matrix a: Unable to unify while inferring type of + false: Unification Fail: float64 ~ float32 cannot be unified\n\n}\n"
        },
        {
          "name": "example_iop_test.go",
          "type": "blob",
          "size": 3.158203125,
          "content": "package gorgonia_test\n\nimport (\n\t\"fmt\"\n\t\"hash\"\n\t\"hash/fnv\"\n\t\"io/ioutil\"\n\n\t\"github.com/chewxy/hm\"\n\t. \"gorgonia.org/gorgonia\"\n\t\"gorgonia.org/tensor\"\n)\n\ntype MyNewOp struct{}\n\nfunc (op MyNewOp) Arity() int { return 2 }\nfunc (op MyNewOp) Type() hm.Type {\n\tt := TensorType{Dims: 4, Of: hm.TypeVariable('a')}\n\treturn hm.NewFnType(t, t, t)\n}\nfunc (op MyNewOp) InferShape(ns ...DimSizer) (tensor.Shape, error) {\n\treturn ns[0].(tensor.Shape).Clone(), nil\n}\n\nfunc (op MyNewOp) Do(values ...Value) (retVal Value, err error) {\n\tin1 := values[0]\n\tin2 := values[1]\n\tout, err := CloneValue(in1)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn op.UsePreallocDo(out, in1, in2)\n}\n\nfunc (op MyNewOp) UsePreallocDo(prealloc Value, inputs ...Value) (Value, error) {\n\tin1 := inputs[0]\n\tin2 := inputs[1]\n\treturn tensor.Add(in1, in2, tensor.WithReuse(prealloc.(tensor.Tensor)))\n}\n\nfunc (op MyNewOp) ReturnsPtr() bool      { return true }\nfunc (op MyNewOp) CallsExtern() bool     { return false }\nfunc (op MyNewOp) OverwritesInput() int  { return -1 }\nfunc (op MyNewOp) WriteHash(h hash.Hash) { fmt.Fprintf(h, \"XXX\") }\nfunc (op MyNewOp) Hashcode() uint32 {\n\th := fnv.New32a()\n\top.WriteHash(h)\n\treturn h.Sum32()\n}\nfunc (op MyNewOp) String() string { return \"XXX\" }\n\nfunc (op MyNewOp) DiffWRT(inputs int) []bool { return []bool{true, true, true} }\n\nfunc (op MyNewOp) SymDiff(inputs Nodes, output *Node, grad *Node) (retVal Nodes, err error) {\n\tin1 := inputs[0]\n\tin2 := inputs[1]\n\n\tdiffOp := MyNewDiffOp{op}\n\tg := in1.Graph()\n\tin2Diff := NewUniqueNode(WithType(in2.Type()), WithShape(in2.Shape().Clone()...), WithChildren(Nodes{in2}), In(g), WithOp(Iop{}))\n\n\tvar in1Diff *Node\n\tif in1Diff, err = ApplyOp(diffOp, in1, in2, in2Diff); err != nil {\n\t\treturn nil, err\n\t}\n\treturn Nodes{in1Diff, in2Diff}, nil\n\n}\n\ntype MyNewDiffOp struct{ MyNewOp }\n\nfunc (op MyNewDiffOp) Arity() int { return 3 }\nfunc (op MyNewDiffOp) Type() hm.Type {\n\tt := TensorType{Dims: 4, Of: hm.TypeVariable('a')}\n\treturn hm.NewFnType(t, t, t, t)\n}\n\nfunc (op MyNewDiffOp) Do(values ...Value) (Value, error) {\n\t//in1 := values[0]\n\tin2 := values[1]\n\tin2Diff := values[2]\n\n\tretVal, err := CloneValue(in2)\n\tswitch data := in2Diff.Data().(type) {\n\tcase []float64:\n\t\tfor i := range data {\n\t\t\tdata[i] = 1000\n\t\t}\n\t}\n\treturn retVal, err\n}\nfunc (op MyNewDiffOp) String() string { return \"XXXDiff\" }\n\nfunc Example_iop() {\n\tg := NewGraph()\n\tx := NewTensor(g, tensor.Float64, 4, WithShape(4, 5, 6, 7), WithName(\"x\"), WithInit(Ones()))\n\ty := NewTensor(g, tensor.Float64, 4, WithShape(4, 5, 6, 7), WithName(\"y\"), WithInit(Zeroes()))\n\tz, err := ApplyOp(MyNewOp{}, x, y)\n\tif err != nil {\n\t\tfmt.Println(err)\n\t\treturn\n\t}\n\ts, err := Sum(z)\n\tif err != nil {\n\t\tfmt.Println(err)\n\t\treturn\n\t}\n\t_, err = Grad(s, x, y)\n\tif err != nil {\n\t\tfmt.Println(err)\n\t\treturn\n\t}\n\n\tm := NewTapeMachine(g, BindDualValues(x, y, z), TraceExec())\n\tif err := m.RunAll(); err != nil {\n\t\tfmt.Println(err)\n\t\treturn\n\t}\n\n\tyGrad, err := y.Grad()\n\tif err != nil {\n\t\tfmt.Println(err)\n\t\treturn\n\t}\n\n\tall1000 := func(a []float64) bool {\n\t\tfor i := range a {\n\t\t\tif a[i] != 1000 {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\treturn true\n\t}\n\tioutil.WriteFile(\"xxx.dot\", []byte(g.ToDot()), 0644)\n\tfmt.Printf(\"%v\", all1000(yGrad.Data().([]float64)))\n\n\t// Output:\n\t// true\n}\n"
        },
        {
          "name": "example_linalg_test.go",
          "type": "blob",
          "size": 8.2490234375,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"testing\"\n\n\t\"gorgonia.org/tensor\"\n)\n\nfunc ExampleBatchedMatMul() {\n\tg := NewGraph()\n\ta := NewTensor(g, Float64, 3, WithShape(2, 2, 3), WithInit(RangedFrom(1)), WithName(\"a\"))\n\tb := NewTensor(g, Float64, 3, WithShape(2, 3, 2), WithInit(RangedFrom(13)), WithName(\"b\"))\n\tc, err := BatchedMatMul(a, b)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\td := NewTensor(g, Float64, 3, WithShape(10, 1, 1), WithInit(RangedFrom(1)), WithName(\"d\"))\n\te := NewTensor(g, Float64, 3, WithShape(10, 1, 10), WithInit(RangedFrom(11)), WithName(\"e\"))\n\tf, err := BatchedMatMul(d, e)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\th, err := BatchedMatMul(e, d, true, true)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\ti := NewTensor(g, Float64, 4, WithShape(1, 3, 2, 4), WithInit(RangedFrom(1)), WithName(\"i\"))\n\tj := NewTensor(g, Float64, 4, WithShape(1, 3, 4, 2), WithInit(ValuesOf(10.0)), WithName(\"j\"))\n\tk, err := BatchedMatMul(i, j)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tx := NewTensor(g, Float64, 4, WithShape(3, 2, 2, 3), WithInit(RangedFrom(1)), WithName(\"x\"))\n\ty := NewTensor(g, Float64, 4, WithShape(3, 2, 3, 2), WithInit(RangedFrom(37)), WithName(\"y\"))\n\tz, err := BatchedMatMul(x, y)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tm := NewTapeMachine(g)\n\tif err := m.RunAll(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tfmt.Printf(\"a: %v\\n%v\\n\", a.Value().Shape(), a.Value().Data())\n\tfmt.Printf(\"b: %v\\n%v\\n\", b.Value().Shape(), b.Value().Data())\n\tfmt.Printf(\"c: %v\\n%v\\n\", c.Value().Shape(), c.Value().Data())\n\tfmt.Printf(\"d: %v\\n%v\\n\", d.Value().Shape(), d.Value().Data())\n\tfmt.Printf(\"e: %v\\n%v\\n\", e.Value().Shape(), e.Value().Data())\n\tfmt.Printf(\"f: %v\\n%v\\n\", f.Value().Shape(), f.Value().Data())\n\tfmt.Printf(\"h: %v\\n%v\\n\", h.Value().Shape(), h.Value().Data())\n\tfmt.Printf(\"i: %v\\n%v\\n\", i.Value().Shape(), i.Value().Data())\n\tfmt.Printf(\"j: %v\\n%v\\n\", j.Value().Shape(), j.Value().Data())\n\tfmt.Printf(\"k: %v\\n%v\\n\", k.Value().Shape(), k.Value().Data())\n\tfmt.Printf(\"x: %v\\n%v\\n\", x.Value().Shape(), x.Value().Data())\n\tfmt.Printf(\"y: %v\\n%v\\n\", y.Value().Shape(), y.Value().Data())\n\tfmt.Printf(\"z: %v\\n%v\\n\", z.Value().Shape(), z.Value().Data())\n\n\t// Output:\n\t// a: (2, 2, 3)\n\t// [1 2 3 4 5 6 7 8 9 10 11 12]\n\t// b: (2, 3, 2)\n\t// [13 14 15 16 17 18 19 20 21 22 23 24]\n\t// c: (2, 2, 2)\n\t// [94 100 229 244 508 532 697 730]\n\t// d: (10, 1, 1)\n\t// [1 2 3 4 5 6 7 8 9 10]\n\t// e: (10, 1, 10)\n\t// [11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110]\n\t// f: (10, 1, 10)\n\t// [11 12 13 14 15 16 17 18 19 20 42 44 46 48 50 52 54 56 58 60 93 96 99 102 105 108 111 114 117 120 164 168 172 176 180 184 188 192 196 200 255 260 265 270 275 280 285 290 295 300 366 372 378 384 390 396 402 408 414 420 497 504 511 518 525 532 539 546 553 560 648 656 664 672 680 688 696 704 712 720 819 828 837 846 855 864 873 882 891 900 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100]\n\t// h: (10, 10, 1)\n\t// [11 12 13 14 15 16 17 18 19 20 42 44 46 48 50 52 54 56 58 60 93 96 99 102 105 108 111 114 117 120 164 168 172 176 180 184 188 192 196 200 255 260 265 270 275 280 285 290 295 300 366 372 378 384 390 396 402 408 414 420 497 504 511 518 525 532 539 546 553 560 648 656 664 672 680 688 696 704 712 720 819 828 837 846 855 864 873 882 891 900 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100]\n\t// i: (1, 3, 2, 4)\n\t// [1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n\t// j: (1, 3, 4, 2)\n\t// [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\n\t// k: (1, 3, 2, 2)\n\t// [100 100 260 260 420 420 580 580 740 740 900 900]\n\t// x: (3, 2, 2, 3)\n\t// [1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36]\n\t// y: (3, 2, 3, 2)\n\t// [37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n\t// z: (3, 2, 2, 2)\n\t// [238 244 589 604 1084 1108 1489 1522 2146 2188 2605 2656 3424 3484 3937 4006 4918 4996 5485 5572 6628 6724 7249 7354]\n\n}\n\nfunc TestIncrSlices(t *testing.T) {\n\t// validSlices to see if the slice and shape matches\n\tvalidSlices := func(a []sli, shp tensor.Shape) bool {\n\t\tfor i := range a {\n\t\t\tif a[i].start < shp[i] {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t\treturn false\n\t}\n\n\tshp := tensor.Shape{2, 3, 4}\n\tslices := make([]sli, len(shp))\n\tfor i := range slices {\n\t\tslices[i].end = 1\n\t}\n\n\tfor halt := false; !halt; halt = incrSlices(slices, shp) {\n\t\tif !validSlices(slices, shp) {\n\t\t\tt.Errorf(\"Generated invalid slice %v\", slices)\n\t\t}\n\t}\n}\n\nfunc ExampleBatchedMatMul_withBackprop() {\n\tg := NewGraph()\n\ta := NewTensor(g, Float64, 4, WithShape(2, 4, 3, 9), WithInit(RangedFrom(1)), WithName(\"a\"))\n\tb := NewTensor(g, Float64, 4, WithShape(2, 4, 3, 9), WithInit(RangedFrom(13)), WithName(\"b\"))\n\tc, err := BatchedMatMul(a, b, false, true)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\ts, err := Sum(c)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tgrads, err := Grad(s, a, b)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tm := NewTapeMachine(g)\n\tif err := m.RunAll(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tfmt.Printf(\"a: %v\\n%v\\n\", a.Value().Shape(), a.Value().Data())\n\tfmt.Printf(\"b: %v\\n%v\\n\", b.Value().Shape(), b.Value().Data())\n\tfmt.Printf(\"c: %v\\n%v\\n\", c.Value().Shape(), c.Value().Data())\n\tfmt.Printf(\"grads[0]:%v\\n%v\\n\", grads[0].Shape(), grads[0].Value().Data())\n\t// Output:\n\t// a: (2, 4, 3, 9)\n\t// [1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216]\n\t// b: (2, 4, 3, 9)\n\t// [13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228]\n\t// c: (2, 4, 3, 3)\n\t// [825 1230 1635 2202 3336 4470 3579 5442 7305 12732 15324 17916 16296 19617 22938 19860 23910 27960 37761 42540 47319 43512 49020 54528 49263 55500 61737 75912 82878 89844 83850 91545 99240 91788 100212 108636 127185 136338 145491 137310 147192 157074 147435 158046 168657 191580 202920 214260 203892 215961 228030 216204 229002 241800 269097 282624 296151 283596 297852 312108 298095 313080 328065 359736 375450 391164 376422 392865 409308 393108 410280 427452]\n\t// grads[0]:(2, 4, 3, 9)\n\t// [66 69 72 75 78 81 84 87 90 66 69 72 75 78 81 84 87 90 66 69 72 75 78 81 84 87 90 147 150 153 156 159 162 165 168 171 147 150 153 156 159 162 165 168 171 147 150 153 156 159 162 165 168 171 228 231 234 237 240 243 246 249 252 228 231 234 237 240 243 246 249 252 228 231 234 237 240 243 246 249 252 309 312 315 318 321 324 327 330 333 309 312 315 318 321 324 327 330 333 309 312 315 318 321 324 327 330 333 390 393 396 399 402 405 408 411 414 390 393 396 399 402 405 408 411 414 390 393 396 399 402 405 408 411 414 471 474 477 480 483 486 489 492 495 471 474 477 480 483 486 489 492 495 471 474 477 480 483 486 489 492 495 552 555 558 561 564 567 570 573 576 552 555 558 561 564 567 570 573 576 552 555 558 561 564 567 570 573 576 633 636 639 642 645 648 651 654 657 633 636 639 642 645 648 651 654 657 633 636 639 642 645 648 651 654 657]\n}\n"
        },
        {
          "name": "example_linearregression_test.go",
          "type": "blob",
          "size": 3.22265625,
          "content": "package gorgonia_test\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"math/rand\"\n\t\"runtime\"\n\n\t. \"gorgonia.org/gorgonia\"\n\t\"gorgonia.org/tensor\"\n)\n\nconst (\n\tvecSize = 10000\n)\n\n// manually generate a fake dataset which is y=2x+random\nfunc xy(dt tensor.Dtype) (x tensor.Tensor, y tensor.Tensor) {\n\tvar xBack, yBack interface{}\n\tswitch dt {\n\tcase Float32:\n\t\txBack = tensor.Range(tensor.Float32, 1, vecSize+1).([]float32)\n\t\tyBackC := tensor.Range(tensor.Float32, 1, vecSize+1).([]float32)\n\n\t\tfor i, v := range yBackC {\n\t\t\tyBackC[i] = v*2 + rand.Float32()\n\t\t}\n\t\tyBack = yBackC\n\tcase Float64:\n\t\txBack = tensor.Range(tensor.Float64, 1, vecSize+1).([]float64)\n\t\tyBackC := tensor.Range(tensor.Float64, 1, vecSize+1).([]float64)\n\n\t\tfor i, v := range yBackC {\n\t\t\tyBackC[i] = v*2 + rand.Float64()\n\t\t}\n\t\tyBack = yBackC\n\t}\n\n\tx = tensor.New(tensor.WithBacking(xBack), tensor.WithShape(vecSize))\n\ty = tensor.New(tensor.WithBacking(yBack), tensor.WithShape(vecSize))\n\treturn\n}\n\nfunc random(dt tensor.Dtype) interface{} {\n\trand.Seed(13370)\n\tswitch dt {\n\tcase tensor.Float32:\n\t\treturn rand.Float32()\n\tcase tensor.Float64:\n\t\treturn rand.Float64()\n\tdefault:\n\t\tpanic(\"Unhandled dtype\")\n\t}\n}\n\nfunc linregSetup(Float tensor.Dtype) (m, c *Node, machine VM) {\n\tvar xT, yT Value\n\txT, yT = xy(Float)\n\n\tg := NewGraph()\n\tx := NewVector(g, Float, WithShape(vecSize), WithName(\"x\"), WithValue(xT))\n\ty := NewVector(g, Float, WithShape(vecSize), WithName(\"y\"), WithValue(yT))\n\tm = NewScalar(g, Float, WithName(\"m\"), WithValue(random(Float)))\n\tc = NewScalar(g, Float, WithName(\"c\"), WithValue(random(Float)))\n\n\tpred := Must(Add(Must(Mul(x, m)), c))\n\tse := Must(Square(Must(Sub(pred, y))))\n\tcost := Must(Mean(se))\n\n\tif _, err := Grad(cost, m, c); err != nil {\n\t\tlog.Fatalf(\"Failed to backpropagate: %v\", err)\n\t}\n\n\t// machine := NewLispMachine(g)  // you can use a LispMachine, but it'll be VERY slow.\n\tmachine = NewTapeMachine(g, BindDualValues(m, c))\n\treturn m, c, machine\n}\n\nfunc linregRun(m, c *Node, machine VM, iter int, autoCleanup bool) (retM, retC Value) {\n\tif autoCleanup {\n\t\tdefer machine.Close()\n\t}\n\tmodel := []ValueGrad{m, c}\n\tsolver := NewVanillaSolver(WithLearnRate(0.001), WithClip(5)) // good idea to clip\n\n\tif CUDA {\n\t\truntime.LockOSThread()\n\t\tdefer runtime.UnlockOSThread()\n\t}\n\tvar err error\n\tfor i := 0; i < iter; i++ {\n\t\tif err = machine.RunAll(); err != nil {\n\t\t\tfmt.Printf(\"Error during iteration: %v: %v\\n\", i, err)\n\t\t\tbreak\n\t\t}\n\n\t\tif err = solver.Step(model); err != nil {\n\t\t\tlog.Fatal(err)\n\t\t}\n\n\t\tmachine.Reset() // Reset is necessary in a loop like this\n\t}\n\treturn m.Value(), c.Value()\n\n}\n\nfunc linearRegression(Float tensor.Dtype, iter int) (retM, retC Value) {\n\tdefer runtime.GC()\n\tm, c, machine := linregSetup(Float)\n\treturn linregRun(m, c, machine, iter, true)\n}\n\n// Linear Regression Example\n//\n// The formula for a straight line is\n//\t\ty = mx + c\n// We want to find an `m` and a `c` that fits the equation well. We'll do it in both float32 and float64 to showcase the extensibility of Gorgonia\nfunc Example_linearRegression() {\n\tvar m, c Value\n\t// Float32\n\tm, c = linearRegression(Float32, 500)\n\tfmt.Printf(\"float32: y = %3.3fx + %3.3f\\n\", m, c)\n\n\t// Float64\n\tm, c = linearRegression(Float64, 500)\n\tfmt.Printf(\"float64: y = %3.3fx + %3.3f\\n\", m, c)\n\n\t// Output:\n\t// float32: y = 2.001x + 2.001\n\t// float64: y = 2.001x + 2.001\n}\n"
        },
        {
          "name": "example_monad_test.go",
          "type": "blob",
          "size": 4.9775390625,
          "content": "package gorgonia_test\n\nimport (\n\t\"fmt\"\n\n\t. \"gorgonia.org/gorgonia\"\n)\n\n// This example showcases the reasons for the more confusing functions.\nfunc Example_monad_raison_detre() {\n\t// The main reason for the following function is to make it easier to create APIs.\n\t// Gorgonia;s APIs are very explicit hence not very user friendly.\n\n\tconst (\n\t\tn        = 32\n\t\tfeatures = 784\n\t\tsize     = 100\n\t)\n\n\t// The following is an example of how to set up a neural network\n\n\t// First, we set up the components\n\tg := NewGraph()\n\tw1 := NewMatrix(g, Float32, WithShape(features, size), WithName(\"w\"), WithInit(GlorotU(1)))\n\tb1 := NewMatrix(g, Float32, WithShape(1, size), WithName(\"b\"), WithInit(Zeroes()))\n\tx1 := NewMatrix(g, Float32, WithShape(n, features), WithName(\"x\"))\n\n\t// Then we write the expression:\n\tvar xw, xwb, act *Node\n\tvar err error\n\tif xw, err = Mul(x1, w1); err != nil {\n\t\tfmt.Printf(\"Err while Mul: %v\\n\", err)\n\t}\n\tif xwb, err = BroadcastAdd(xw, b1, nil, []byte{0}); err != nil {\n\t\tfmt.Printf(\"Err while Add: %v\\n\", err)\n\t}\n\tif act, err = Tanh(xwb); err != nil {\n\t\tfmt.Printf(\"Err while Tanh: %v\\n\", err)\n\t}\n\tfmt.Printf(\"act is a %T\\n\", act)\n\n\t// The following is how to set up the exact same network\n\n\t// First we set up our environment\n\t//\n\t// These LiftXXX functions transforms Gorgonia's default API into functions that return `Result`\n\tvar mul = Lift2(Mul)                   // Lift2 turns a func(*Node, *Node) (*Node, error)\n\tvar tanh = Lift1(Tanh)                 // Lift1 turns a func(*Node) (*Node, error)\n\tvar add = Lift2Broadcast(BroadcastAdd) // Lift2Broadcast turns a func(*Node, *Node, []byte, []byte) (*Nide, error)\n\n\t// First we set up the components\n\th := NewGraph()\n\tw2 := NewMatrix(h, Float32, WithShape(features, size), WithName(\"w\"), WithInit(GlorotU(1)))\n\tb2 := NewMatrix(h, Float32, WithShape(1, size), WithName(\"b\"), WithInit(Zeroes()))\n\tx2 := NewMatrix(h, Float32, WithShape(n, features), WithName(\"x\"))\n\n\t// Then we write the expression\n\tact2 := tanh(add(mul(x2, w2), b2, nil, []byte{0}))\n\tfmt.Printf(\"act2 is a %T (note it's wrapped in the `Result` type)\\n\", act2)\n\tfmt.Println()\n\t// both g and h are the same graph but the expression is easier to write for act2\n\tfmt.Printf(\"Both g and h are the same graph:\\ng: %v\\nh: %v\\n\", g.AllNodes(), h.AllNodes())\n\n\t// Output:\n\t// act is a *gorgonia.Node\n\t// act2 is a *gorgonia.Node (note it's wrapped in the `Result` type)\n\t//\n\t// Both g and h are the same graph:\n\t// g: [w, b, x, A × B(%2, %0), Reshape(1, 100)(%1), SizeOf=32(%3), Repeat0(%4, %5), + false(%3, %6), tanh(%7)]\n\t// h: [w, b, x, A × B(%2, %0), Reshape(1, 100)(%1), SizeOf=32(%3), Repeat0(%4, %5), + false(%3, %6), tanh(%7)]\n}\n\n// This example showcases dealing with errors. This is part 2 of the raison d'être of the more complicated functions - dealing with errors\nfunc Example_monad_raison_detre_errors() {\n\t// Observe that in a similar example, errors are manually controllable in the original case,\n\t// but automated in the second case\n\tconst (\n\t\tn        = 32\n\t\tfeatures = 784\n\t\tsize     = 100\n\t)\n\n\t// The following is an example of how to set up a neural network\n\n\t// First, we set up the components\n\tg := NewGraph()\n\tw1 := NewMatrix(g, Float32, WithShape(features, size), WithName(\"w\"), WithInit(GlorotU(1)))\n\tb1 := NewMatrix(g, Float32, WithShape(1, size), WithName(\"b\"), WithInit(Zeroes()))\n\tx1 := NewMatrix(g, Float32, WithShape(n, features), WithName(\"x\"))\n\n\t// Then we write the expression:\n\tvar xw, xwb, act *Node\n\tvar err error\n\tif xw, err = Mul(x1, w1); err != nil {\n\t\tfmt.Printf(\"Err while Mul: %v\\n\", err)\n\t}\n\t// we introduce an error here - it should be []byte{0}\n\tif xwb, err = BroadcastAdd(xw, b1, nil, []byte{1}); err != nil {\n\t\tfmt.Printf(\"Err while Add: %v\\n\", err)\n\t\tgoto case2\n\t}\n\tif act, err = Tanh(xwb); err != nil {\n\t\tfmt.Printf(\"Err while Tanh: %v\\n\", err)\n\t}\n\t_ = act // will never happen\n\ncase2:\n\n\t// The following is how to set up the exact same network\n\n\t// First we set up our environment\n\t//\n\t// Now, remember all these functions no longer return (*Node, error). Instead they return `Result`\n\tvar mul = Lift2(Mul)\n\tvar tanh = Lift1(Tanh)\n\tvar add = Lift2Broadcast(BroadcastAdd)\n\n\t// First we set up the components\n\th := NewGraph()\n\tw2 := NewMatrix(h, Float32, WithShape(features, size), WithName(\"w\"), WithInit(GlorotU(1)))\n\tb2 := NewMatrix(h, Float32, WithShape(1, size), WithName(\"b\"), WithInit(Zeroes()))\n\tx2 := NewMatrix(h, Float32, WithShape(n, features), WithName(\"x\"))\n\n\t// Then we write the expression\n\tact2 := tanh(add(mul(x2, w2), b2, nil, []byte{1}))\n\n\t// REMEMBER: act2 is not a *Node! It is a Result\n\tfmt.Printf(\"act2: %v\\n\", act2)\n\n\t// To extract error, use CheckOne\n\tfmt.Printf(\"error: %v\\n\", CheckOne(act2))\n\n\t// If you extract the *Node from an error, you get nil\n\tfmt.Printf(\"Node: %v\\n\", act2.Node())\n\n\t// Output:\n\t// Err while Add: Failed to infer shape. Op: + false: Shape mismatch: (32, 100) and (1, 10000)\n\t// act2: Failed to infer shape. Op: + false: Shape mismatch: (32, 100) and (1, 10000)\n\t// error: Failed to infer shape. Op: + false: Shape mismatch: (32, 100) and (1, 10000)\n\t// Node: <nil>\n}\n"
        },
        {
          "name": "example_operations_test.go",
          "type": "blob",
          "size": 6.15234375,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n\n\t\"gorgonia.org/tensor\"\n)\n\nfunc ExampleSoftMax() {\n\tg := NewGraph()\n\tt := tensor.New(tensor.WithShape(2, 3), tensor.WithBacking([]float64{1, 3, 2, 3, 2, 1}))\n\tu := t.Clone().(*tensor.Dense)\n\tv := tensor.New(tensor.WithShape(2, 2, 3), tensor.WithBacking([]float64{\n\t\t1, 3, 2,\n\t\t4, 2, 1,\n\n\t\t3, 5, 3,\n\t\t2, 1, 5,\n\t}))\n\n\ta := NodeFromAny(g, t, WithName(\"a\"))\n\tb := NodeFromAny(g, u, WithName(\"b\"))\n\tc := NodeFromAny(g, v, WithName(\"c\"))\n\n\tsm1 := Must(SoftMax(a))\n\tsm0 := Must(SoftMax(b, 0))\n\tsm := Must(SoftMax(c))\n\tm := NewTapeMachine(g)\n\tif err := m.RunAll(); err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Printf(\"a:\\n%v\\nsoftmax(a) - along last axis (default behaviour):\\n%1.2f\", a.Value(), sm1.Value())\n\tfmt.Printf(\"b:\\n%v\\nsoftmax(b) - along axis 0:\\n%1.2f\", b.Value(), sm0.Value())\n\n\ttmp := fmt.Sprintf(\"c %v:\\n%v\\nsoftmax(c) - along last axis (default behaviour) %v:\\n%1.2f\", c.Value().Shape(), c.Value(), sm.Value().Shape(), sm.Value())\n\n\tfmt.Println(strings.Replace(tmp, \"\\n\\n\\n\", \"\\n\\n\", -1))\n\n\t// the requirement to use tmp and strings.Replace is because when Go runs example tests, it strips excess newlines.\n\n\t// Output:\n\t// a:\n\t// ⎡1  3  2⎤\n\t// ⎣3  2  1⎦\n\t//\n\t// softmax(a) - along last axis (default behaviour):\n\t// ⎡0.09  0.67  0.24⎤\n\t// ⎣0.67  0.24  0.09⎦\n\t// b:\n\t// ⎡1  3  2⎤\n\t// ⎣3  2  1⎦\n\t//\n\t// softmax(b) - along axis 0:\n\t// ⎡0.12  0.73  0.73⎤\n\t// ⎣0.88  0.27  0.27⎦\n\t// c (2, 2, 3):\n\t// ⎡1  3  2⎤\n\t// ⎣4  2  1⎦\n\t//\n\t// ⎡3  5  3⎤\n\t// ⎣2  1  5⎦\n\t//\n\t//\n\t// softmax(c) - along last axis (default behaviour) (2, 2, 3):\n\t// ⎡0.09  0.67  0.24⎤\n\t// ⎣0.84  0.11  0.04⎦\n\t//\n\t// ⎡0.11  0.79  0.11⎤\n\t// ⎣0.05  0.02  0.94⎦\n\n}\n\nfunc ExampleConcat() {\n\tg := NewGraph()\n\tx := NewTensor(g, Float64, 4, WithShape(2, 3, 4, 5), WithInit(RangedFrom(0)), WithName(\"x\"))\n\ty := NewTensor(g, Float64, 4, WithShape(2, 3, 4, 5), WithInit(RangedFrom(120)), WithName(\"y\"))\n\n\tz, err := Concat(2, x, y)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tm := NewTapeMachine(g)\n\tif err := m.RunAll(); err != nil {\n\t\tpanic(err)\n\t}\n\ttmp := fmt.Sprintf(\"z %v\\n%v\", z.Value().Shape(), z.Value())\n\tfmt.Println(strings.Replace(tmp, \"\\n\\n\", \"\\n\", -1)) // this is because\n\n\t// Output:\n\t//z (2, 3, 8, 5)\n\t//⎡  0    1    2    3    4⎤\n\t//⎢  5    6    7    8    9⎥\n\t//⎢ 10   11   12   13   14⎥\n\t//⎢ 15   16   17   18   19⎥\n\t//⎢120  121  122  123  124⎥\n\t//⎢125  126  127  128  129⎥\n\t//⎢130  131  132  133  134⎥\n\t//⎣135  136  137  138  139⎦\n\t//\n\t//\n\t//⎡ 20   21   22   23   24⎤\n\t//⎢ 25   26   27   28   29⎥\n\t//⎢ 30   31   32   33   34⎥\n\t//⎢ 35   36   37   38   39⎥\n\t//⎢140  141  142  143  144⎥\n\t//⎢145  146  147  148  149⎥\n\t//⎢150  151  152  153  154⎥\n\t//⎣155  156  157  158  159⎦\n\t//\n\t//\n\t//⎡ 40   41   42   43   44⎤\n\t//⎢ 45   46   47   48   49⎥\n\t//⎢ 50   51   52   53   54⎥\n\t//⎢ 55   56   57   58   59⎥\n\t//⎢160  161  162  163  164⎥\n\t//⎢165  166  167  168  169⎥\n\t//⎢170  171  172  173  174⎥\n\t//⎣175  176  177  178  179⎦\n\t//\n\t//\n\t//⎡ 60   61   62   63   64⎤\n\t//⎢ 65   66   67   68   69⎥\n\t//⎢ 70   71   72   73   74⎥\n\t//⎢ 75   76   77   78   79⎥\n\t//⎢180  181  182  183  184⎥\n\t//⎢185  186  187  188  189⎥\n\t//⎢190  191  192  193  194⎥\n\t//⎣195  196  197  198  199⎦\n\t//\n\t//\n\t//⎡ 80   81   82   83   84⎤\n\t//⎢ 85   86   87   88   89⎥\n\t//⎢ 90   91   92   93   94⎥\n\t//⎢ 95   96   97   98   99⎥\n\t//⎢200  201  202  203  204⎥\n\t//⎢205  206  207  208  209⎥\n\t//⎢210  211  212  213  214⎥\n\t//⎣215  216  217  218  219⎦\n\t//\n\t//\n\t//⎡100  101  102  103  104⎤\n\t//⎢105  106  107  108  109⎥\n\t//⎢110  111  112  113  114⎥\n\t//⎢115  116  117  118  119⎥\n\t//⎢220  221  222  223  224⎥\n\t//⎢225  226  227  228  229⎥\n\t//⎢230  231  232  233  234⎥\n\t//⎣235  236  237  238  239⎦\n}\n\nfunc ExampleUnconcat() {\n\tg := NewGraph()\n\tx := NewTensor(g, Float64, 4, WithShape(2, 3, 4, 5), WithInit(RangedFrom(0)), WithName(\"x\"))\n\ty := NewTensor(g, Float64, 4, WithShape(2, 3, 4, 5), WithInit(RangedFrom(120)), WithName(\"y\"))\n\n\tz, err := Concat(2, x, y)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tunconcats, err := Unconcat(z, 2, 2)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\ta, b := unconcats[0], unconcats[1]\n\n\tm := NewTapeMachine(g)\n\tif err := m.RunAll(); err != nil {\n\t\tpanic(err)\n\t}\n\ttmp := fmt.Sprintf(\"a %v\\n%v\\nb %v\\n%v\", a.Value().Shape(), a.Value(), b.Value().Shape(), b.Value())\n\tfmt.Println(strings.Replace(tmp, \"\\n\\n\", \"\\n\", -1))\n\n\t// Output:\n\t// a (2, 3, 4, 5)\n\t// ⎡  0    1    2    3    4⎤\n\t// ⎢  5    6    7    8    9⎥\n\t// ⎢ 10   11   12   13   14⎥\n\t// ⎣ 15   16   17   18   19⎦\n\t//\n\t//\n\t// ⎡ 20   21   22   23   24⎤\n\t// ⎢ 25   26   27   28   29⎥\n\t// ⎢ 30   31   32   33   34⎥\n\t// ⎣ 35   36   37   38   39⎦\n\t//\n\t//\n\t// ⎡ 40   41   42   43   44⎤\n\t// ⎢ 45   46   47   48   49⎥\n\t// ⎢ 50   51   52   53   54⎥\n\t// ⎣ 55   56   57   58   59⎦\n\t//\n\t//\n\t// ⎡ 60   61   62   63   64⎤\n\t// ⎢ 65   66   67   68   69⎥\n\t// ⎢ 70   71   72   73   74⎥\n\t// ⎣ 75   76   77   78   79⎦\n\t//\n\t//\n\t// ⎡ 80   81   82   83   84⎤\n\t// ⎢ 85   86   87   88   89⎥\n\t// ⎢ 90   91   92   93   94⎥\n\t// ⎣ 95   96   97   98   99⎦\n\t//\n\t//\n\t// ⎡100  101  102  103  104⎤\n\t// ⎢105  106  107  108  109⎥\n\t// ⎢110  111  112  113  114⎥\n\t// ⎣115  116  117  118  119⎦\n\t//\n\t//\n\t//\n\t// b (2, 3, 4, 5)\n\t// ⎡120  121  122  123  124⎤\n\t// ⎢125  126  127  128  129⎥\n\t// ⎢130  131  132  133  134⎥\n\t// ⎣135  136  137  138  139⎦\n\t//\n\t//\n\t// ⎡140  141  142  143  144⎤\n\t// ⎢145  146  147  148  149⎥\n\t// ⎢150  151  152  153  154⎥\n\t// ⎣155  156  157  158  159⎦\n\t//\n\t//\n\t// ⎡160  161  162  163  164⎤\n\t// ⎢165  166  167  168  169⎥\n\t// ⎢170  171  172  173  174⎥\n\t// ⎣175  176  177  178  179⎦\n\t//\n\t//\n\t// ⎡180  181  182  183  184⎤\n\t// ⎢185  186  187  188  189⎥\n\t// ⎢190  191  192  193  194⎥\n\t// ⎣195  196  197  198  199⎦\n\t//\n\t//\n\t// ⎡200  201  202  203  204⎤\n\t// ⎢205  206  207  208  209⎥\n\t// ⎢210  211  212  213  214⎥\n\t// ⎣215  216  217  218  219⎦\n\t//\n\t//\n\t// ⎡220  221  222  223  224⎤\n\t// ⎢225  226  227  228  229⎥\n\t// ⎢230  231  232  233  234⎥\n\t// ⎣235  236  237  238  239⎦\n}\n"
        },
        {
          "name": "example_symdiff_test.go",
          "type": "blob",
          "size": 1.052734375,
          "content": "package gorgonia_test\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\n\t. \"gorgonia.org/gorgonia\"\n)\n\n// SymbolicDiff showcases symbolic differentiation\nfunc Example_symbolicDiff() {\n\tg := NewGraph()\n\n\tvar x, y, z *Node\n\tvar err error\n\n\t// define the expression\n\tx = NewScalar(g, Float64, WithName(\"x\"))\n\ty = NewScalar(g, Float64, WithName(\"y\"))\n\tif z, err = Add(x, y); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// symbolically differentiate z with regards to x and y\n\t// this adds the gradient nodes to the graph g\n\tvar grads Nodes\n\tif grads, err = Grad(z, x, y); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// create a VM to run the program on\n\tmachine := NewTapeMachine(g)\n\tdefer machine.Close()\n\n\t// set initial values then run\n\tLet(x, 2.0)\n\tLet(y, 2.5)\n\tif err = machine.RunAll(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tfmt.Printf(\"z: %v\\n\", z.Value())\n\tif xgrad, err := x.Grad(); err == nil {\n\t\tfmt.Printf(\"dz/dx: %v | %v\\n\", xgrad, grads[0].Value())\n\t}\n\n\tif ygrad, err := y.Grad(); err == nil {\n\t\tfmt.Printf(\"dz/dy: %v | %v\\n\", ygrad, grads[1].Value())\n\t}\n\n\t// Output:\n\t// z: 4.5\n\t// dz/dx: 1 | 1\n\t// dz/dy: 1 | 1\n}\n"
        },
        {
          "name": "example_tensordot_test.go",
          "type": "blob",
          "size": 1.2216796875,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n)\n\n/*\nfunc ExampleTensordot_scalar() {\n\t// Scalars\n\tg := NewGraph()\n\ta := NewScalar(g, Float64, WithValue(2.0), WithName(\"a\"))\n\tb := NewScalar(g, Float64, WithValue(21.0), WithName(\"b\"))\n\tc, err := Tensordot([]int{0}, []int{0}, a, b)\n\tif err != nil {\n\t\tfmt.Printf(\"Cannot call Tensordot. Error: %v\\n\", err)\n\t\treturn\n\t}\n\n\tvm := NewTapeMachine(g)\n\tif err := vm.RunAll(); err != nil {\n\t\tfmt.Printf(\"Cannot perform scalars. Error %v\\n\", err)\n\t}\n\tfmt.Printf(\"c: %v (%v) of %v\", c.Value(), c.Value().Dtype(), c.Value().Shape())\n\n\t// Output:\n\t//...\n}\n*/\nfunc ExampleTensordot_vectors() {\n\tg := NewGraph()\n\ta := NewVector(g, Float64, WithName(\"a\"), WithShape(2), WithInit(RangedFrom(2)))\n\tb := NewVector(g, Float64, WithName(\"b\"), WithShape(2), WithInit(RangedFrom(21)))\n\n\tc, err := Tensordot([]int{0}, []int{0}, a, b)\n\tif err != nil {\n\t\tfmt.Printf(\"Cannot call Tensordot. Error: %v\\n\", err)\n\t\treturn\n\t}\n\n\tvm := NewTapeMachine(g)\n\tif err := vm.RunAll(); err != nil {\n\t\tfmt.Printf(\"Cannot perform tensordot on vectors. Error %v\\n\", err)\n\t}\n\tfmt.Printf(\"a %v b %v \", a.Value(), b.Value())\n\tfmt.Printf(\"c: %v (%v) of %v\", c.Value(), c.Type(), c.Value().Shape())\n\n\t// Output:\n\t// a [2  3] b [21  22] c: [108] (float64) of (1)\n\n}\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "execution.go",
          "type": "blob",
          "size": 3.9169921875,
          "content": "package gorgonia\n\nimport (\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\n// Arena is a representation of a pool of tensor.Memory\ntype Arena interface {\n\tGet(dev Device, size int64) (tensor.Memory, error)       // Get returns a NoOpError when it cannot get a memory. Please allocate\n\tGetFromValue(dev Device, v Value) (tensor.Memory, error) // Gets a memory and copies the values into the memory and returns it.\n\tPut(dev Device, mem tensor.Memory, size int64)           // puts the memory back into the arena\n\tPutValue(dev Device, v Value)                            // puts the memory back into the arena\n\n\t// Transfers memory from device to device\n\tTransfer(toDev, fromDev Device, v Value, synchronous bool) (retVal Value, err error)\n}\n\n// External is a representation of an external device (cuda/cgo/openCL), conceptually modelled as a machine.\ntype External interface {\n\tArena\n\tSignal() // signals the machine to do work\n\tSync() chan struct{}\n}\n\n// ExecutionContext informs how an op should be executed\ntype ExecutionContext struct {\n\tExternal\n\tDevice\n}\n\n// ExternalOp is an op that contains an external context. This allows for ops to be run without needing a VM\ntype ExternalOp struct {\n\tOp\n\tExecutionContext\n\n\tPrealloc  Value\n\tIncr      Value // is this a Incr? IncrDoers have higher precedence over PreallocDo\n\tUseUnsafe bool  // Is this an unsafe op? Lowest of all \"special\" Dos\n}\n\n// NewExternalOp creates a new *ExternalOp.\nfunc NewExternalOp(op Op, ctx ExecutionContext, prealloc Value) *ExternalOp {\n\tretVal := &ExternalOp{\n\t\tOp:               op,\n\t\tExecutionContext: ctx,\n\t\tPrealloc:         prealloc,\n\t\tUseUnsafe:        false,\n\t}\n\n\treturn retVal\n}\n\n// DetermineDevice ...\nfunc (op *ExternalOp) DetermineDevice(inputs Nodes, output *Node) error {\n\tdev := output.dataOn\n\tvar inDev Device = -2\n\tvar allSame bool\n\tfor _, in := range inputs {\n\t\tif in.dataOn != dev {\n\t\t\tallSame = false\n\t\t}\n\n\t\tif inDev == -2 {\n\t\t\tinDev = in.dataOn\n\t\t\tcontinue\n\t\t}\n\t\tif in.dataOn != inDev && in.dataOn != dev {\n\t\t\treturn errors.Errorf(\"Cannot automatically determine device.\")\n\t\t}\n\t}\n\n\tif !allSame {\n\t\treturn errors.Errorf(\"Not all the same devices\")\n\t}\n\top.Device = dev\n\treturn nil\n}\n\n// Do performs the op,\nfunc (op *ExternalOp) Do(vals ...Value) (Value, error) {\n\tif op.Device == CPU {\n\t\tswitch {\n\t\tcase op.Incr != nil:\n\t\t\tif id, ok := op.Op.(IncrDoer); ok {\n\t\t\t\tif err := id.IncrDo(op.Incr, vals...); err != nil {\n\t\t\t\t\tif ver, ok := err.(Valuer); ok {\n\t\t\t\t\t\treturn ver.Value(), nil\n\t\t\t\t\t}\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\treturn op.Incr, nil\n\t\t\t}\n\t\tcase op.Prealloc != nil:\n\t\t\tif pd, ok := op.Op.(UsePreallocDoer); ok {\n\t\t\t\tpd.UsePreallocDo(op.Prealloc, vals...)\n\t\t\t}\n\t\t\tretVal, err := op.Op.Do(vals...)\n\t\t\tif err != nil {\n\t\t\t\treturn retVal, err\n\t\t\t}\n\t\t\treturn Copy(op.Prealloc, retVal)\n\t\tcase op.UseUnsafe:\n\t\t\tif ud, ok := op.Op.(UnsafeDoer); ok {\n\t\t\t\treturn ud.UnsafeDo(vals...)\n\t\t\t}\n\t\t\tfallthrough\n\t\tdefault:\n\t\t\treturn op.Op.Do(vals...)\n\t\t}\n\t}\n\n\tswitch o := op.Op.(type) {\n\tcase CUDADoer:\n\t\tif op.Incr != nil {\n\t\t\tv, err := o.CUDADo(op.External, op.Device, op.Prealloc, vals...)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tadd := newEBOByType(addOpType, TypeOf(op.Incr), TypeOf(v))\n\t\t\taddOp := NewExternalOp(add, op.ExecutionContext, nil)\n\t\t\taddOp.UseUnsafe = true\n\t\t\tretVal, err := addOp.Do(op.Incr, v)\n\t\t\treturn retVal, err\n\t\t}\n\t\treturn o.CUDADo(op.External, op.Device, op.Prealloc, vals...)\n\tcase CLDoer:\n\tcase IncrDoer:\n\t\tif op.Incr != nil {\n\t\t\tif err := o.IncrDo(op.Incr, vals...); err != nil {\n\t\t\t\tif ver, ok := err.(Valuer); ok {\n\t\t\t\t\treturn ver.Value(), nil\n\t\t\t\t}\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\treturn op.Incr, nil\n\t\t}\n\t\treturn op.Op.Do(vals...)\n\tcase UsePreallocDoer:\n\t\tif op.Prealloc != nil {\n\t\t\treturn o.UsePreallocDo(op.Prealloc, vals...)\n\t\t}\n\t\treturn op.Op.Do(vals...)\n\tcase UnsafeDoer:\n\t\tif op.UseUnsafe {\n\t\t\treturn o.UnsafeDo(vals...)\n\t\t}\n\t\treturn op.Op.Do(vals...)\n\tdefault:\n\t\treturn o.Do(vals...)\n\t}\n\n\tpanic(\"Unreachable\")\n}\n\nfunc (op *ExternalOp) String() string {\n\treturn op.Op.String()\n}\n"
        },
        {
          "name": "formatter.go",
          "type": "blob",
          "size": 4.30078125,
          "content": "package gorgonia\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"reflect\"\n)\n\ntype mapFmt struct {\n\tm reflect.Value // map\n}\n\n// FmtNodeMap is a convenience function to print map[*Node]<T>\n//\n// The fmt flag that makes it all nicely formatted is \"-\". Because a map consists of two types (key's type\n// and val's type), and the Go fmt verb doesn't quite allow us to do something like \"%ds\", a hack is introduced\n// to enable nicer printing of map[*Node]<T>\n//\n// Here's the hack:\n// The \"#\" flag is used to indicate if the map will use the Node's ID or Name when formatting the map.\n//\t\t%-v \tnodeName:%v\n//\t\t%-#v\tnodeID:%v\n//\t\t%-d \tnodeName:%x\n//\t\t%-#d \tnodeID: %x\n//\t\t%-p \tnodeName:%p\n// \t\t%-#p\tnodeID:%p\n//\n// If the \"-\" flag is not found, then the formatter returns the default Go format for map[<T>]<T2>\nfunc FmtNodeMap(m interface{}) mapFmt {\n\trefVal := reflect.ValueOf(m)\n\tif refVal.Kind() != reflect.Map {\n\t\tpanic(\"Only expect maps in FmtNodeMap\")\n\t}\n\n\tt := refVal.Type()\n\tkeyType := t.Key()\n\n\tvar n *Node\n\tif keyType != reflect.TypeOf(n) {\n\t\tpanic(\"Only expected map[*Node]<T>\")\n\t}\n\n\treturn mapFmt{\n\t\tm: refVal,\n\t}\n}\n\nfunc (mf mapFmt) defaultFmt(s fmt.State, c rune) {\n\tvar buf bytes.Buffer\n\tbuf.WriteRune('%')\n\tfor i := 0; i < 128; i++ {\n\t\tif s.Flag(i) {\n\t\t\tbuf.WriteByte(byte(i))\n\t\t}\n\t}\n\tif w, ok := s.Width(); ok {\n\t\tfmt.Fprintf(&buf, \"%d\", w)\n\t}\n\tif p, ok := s.Precision(); ok {\n\t\tfmt.Fprintf(&buf, \".%d\", p)\n\t}\n\tbuf.WriteRune(c)\n\tfmt.Fprintf(s, buf.String(), mf.m)\n}\n\nfunc (mf mapFmt) format(s fmt.State, c rune) string {\n\tvar tmpl string\n\tswitch {\n\tcase c == 'v':\n\t\tif s.Flag('#') {\n\t\t\ttmpl = \"\\t%x: %v\\n\"\n\t\t} else {\n\t\t\ttmpl = \"\\t%s: %v\\n\"\n\t\t}\n\tcase c == 'd':\n\t\tif s.Flag('#') {\n\t\t\ttmpl = \"\\t%x: %x\\n\"\n\t\t} else {\n\t\t\ttmpl = \"\\t%s: %x\\n\"\n\t\t}\n\tcase c == 'p':\n\t\tif s.Flag('#') {\n\t\t\ttmpl = \"\\t%x: %p\\n\"\n\t\t} else {\n\t\t\ttmpl = \"\\t%s: %p\\n\"\n\t\t}\n\tdefault:\n\t\ttmpl = \"\\t%s: %s\\n\"\n\t}\n\treturn tmpl\n}\n\nfunc (mf mapFmt) Format(s fmt.State, c rune) {\n\trefVal := mf.m\n\tvar n *Node\n\tt := refVal.Type()\n\tkeyType := t.Key()\n\tif keyType != reflect.TypeOf(n) {\n\t\tpanic(\"Only map[*Node]<T> is expected\")\n\t}\n\n\ttmpl := mf.format(s, c)\n\n\tkeys := refVal.MapKeys()\n\tif s.Flag('-') {\n\t\tif s.Flag('#') {\n\t\t\t// then key, will try its best to be a number\n\n\t\t\tfmt.Fprintf(s, \"map[Node.ID]%s {\\n\", t.Elem())\n\t\t\tfor i := 0; i < refVal.Len(); i++ {\n\t\t\t\tkey := keys[i]\n\t\t\t\tval := refVal.MapIndex(key)\n\n\t\t\t\tmeth := key.MethodByName(\"ID\")\n\t\t\t\tid := meth.Call(nil)[0]\n\n\t\t\t\tvalType := val.Type()\n\t\t\t\tif valType == reflect.TypeOf(n) {\n\t\t\t\t\tswitch c {\n\t\t\t\t\tcase 'd':\n\t\t\t\t\t\tvalMeth := val.MethodByName(\"ID\")\n\t\t\t\t\t\tvalID := valMeth.Call(nil)[0]\n\t\t\t\t\t\tfmt.Fprintf(s, tmpl, id, valID)\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tstrMeth := val.MethodByName(\"String\")\n\t\t\t\t\t\tstr := strMeth.Call(nil)[0]\n\t\t\t\t\t\tfmt.Fprintf(s, tmpl, id, str)\n\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tif _, ok := valType.MethodByName(\"Format\"); ok {\n\t\t\t\t\t\tfmt.Fprintf(s, \"\\t%x: \", id)\n\t\t\t\t\t\tfmtMeth := val.MethodByName(\"Format\")\n\t\t\t\t\t\tfmtMeth.Call([]reflect.Value{reflect.ValueOf(s), reflect.ValueOf(c)})\n\t\t\t\t\t\tfmt.Fprintf(s, \"\\n\")\n\t\t\t\t\t} else if _, ok := valType.MethodByName(\"String\"); ok {\n\t\t\t\t\t\tstrMeth := val.MethodByName(\"String\")\n\t\t\t\t\t\tstr := strMeth.Call(nil)[0]\n\t\t\t\t\t\tfmt.Fprintf(s, tmpl, id, str)\n\t\t\t\t\t} else {\n\t\t\t\t\t\tfmt.Fprintf(s, tmpl, id, val)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\ts.Write([]byte(\"}\"))\n\n\t\t} else {\n\t\t\tfmt.Fprintf(s, \"map[Node.Name]%s {\\n\", t.Elem())\n\t\t\tfor i := 0; i < refVal.Len(); i++ {\n\t\t\t\tkey := keys[i]\n\t\t\t\tval := refVal.MapIndex(key)\n\n\t\t\t\tmeth := key.MethodByName(\"String\")\n\t\t\t\tid := meth.Call(nil)[0]\n\n\t\t\t\tvalType := val.Type()\n\t\t\t\tif valType == reflect.TypeOf(n) {\n\t\t\t\t\tswitch c {\n\t\t\t\t\tcase 'd':\n\t\t\t\t\t\tvalMeth := val.MethodByName(\"ID\")\n\t\t\t\t\t\tvalID := valMeth.Call(nil)[0]\n\t\t\t\t\t\tfmt.Fprintf(s, tmpl, id, valID)\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tstrMeth := val.MethodByName(\"String\")\n\t\t\t\t\t\tstr := strMeth.Call(nil)[0]\n\t\t\t\t\t\tfmt.Fprintf(s, tmpl, id, str)\n\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tif _, ok := valType.MethodByName(\"Format\"); ok {\n\t\t\t\t\t\tfmt.Fprintf(s, \"\\t%s: \", id)\n\t\t\t\t\t\tfmtMeth := val.MethodByName(\"Format\")\n\t\t\t\t\t\tfmtMeth.Call([]reflect.Value{reflect.ValueOf(s), reflect.ValueOf(c)})\n\t\t\t\t\t\tfmt.Fprintf(s, \"\\n\")\n\t\t\t\t\t} else if _, ok := valType.MethodByName(\"String\"); ok {\n\t\t\t\t\t\tstrMeth := val.MethodByName(\"String\")\n\t\t\t\t\t\tstr := strMeth.Call(nil)[0]\n\t\t\t\t\t\tfmt.Fprintf(s, tmpl, id, str)\n\t\t\t\t\t} else {\n\t\t\t\t\t\tfmt.Fprintf(s, tmpl, id, val)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\ts.Write([]byte(\"}\"))\n\t\t}\n\t\treturn\n\t}\n\tmf.defaultFmt(s, c)\n}\n"
        },
        {
          "name": "formatter_test.go",
          "type": "blob",
          "size": 1.8505859375,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestMapFormat(t *testing.T) {\n\n\tg := NewGraph()\n\tx := NewVector(g, Float64, WithName(\"x\"))\n\ty := NewScalar(g, Float64, WithName(\"y\"))\n\n\tm := make(map[*Node]string)\n\tm[x] = \"x\"\n\tm[y] = \"y\"\n\n\ts := fmt.Sprintf(\"%-#v\", FmtNodeMap(m))\n\texpected0 := fmt.Sprintf(\"map[Node.ID]string {\\n\\t%x: x\\n\\t%x: y\\n}\", x.ID(), y.ID())\n\texpected1 := fmt.Sprintf(\"map[Node.ID]string {\\n\\t%x: y\\n\\t%x: x\\n}\", y.ID(), x.ID())\n\tif s != expected0 && s != expected1 {\n\t\tt.Errorf(\"Case 1 failed. Got \\n%v\", s)\n\t}\n\n\tm2 := make(map[*Node]*Node)\n\tm2[x] = x\n\tm2[y] = y\n\n\t// s = fmt.Sprintf(\"%+#v\", FmtNodeMap(m2))\n\t// expected0 = fmt.Sprintf(\"map[Node.ID]*gorgonia.Node {\\n\\t%x: x :: Vector float64\\n\\t%x: y :: float64\\n}\", x.ID(), y.ID())\n\t// expected1 = fmt.Sprintf(\"map[Node.ID]*gorgonia.Node {\\n\\t%x: y :: float64\\n\\t%x: x :: Vector float64\\n}\", y.ID(), x.ID())\n\t// if s != expected0 && s != expected1 {\n\t// \tt.Errorf(\"Case 2 failed. Expected : %q. Got %q instead\", expected0, s)\n\t// }\n\n\ts = fmt.Sprintf(\"%#-d\", FmtNodeMap(m2))\n\texpected0 = fmt.Sprintf(\"map[Node.ID]*gorgonia.Node {\\n\\t%x: %x\\n\\t%x: %x\\n}\", x.ID(), x.ID(), y.ID(), y.ID())\n\texpected1 = fmt.Sprintf(\"map[Node.ID]*gorgonia.Node {\\n\\t%x: %x\\n\\t%x: %x\\n}\", y.ID(), y.ID(), x.ID(), x.ID())\n\tif s != expected0 && s != expected1 {\n\t\tt.Errorf(\"Case 3 failed\")\n\t}\n\n\tm3 := make(map[*Node]Nodes)\n\tm3[x] = Nodes{x, y}\n\ts = fmt.Sprintf(\"%-v\", FmtNodeMap(m3))\n\texpected0 = fmt.Sprintf(\"map[Node.Name]gorgonia.Nodes {\\n\\tx :: Vector float64: [x, y]\\n}\")\n\tif s != expected0 {\n\t\tt.Errorf(\"Case 4 failed. Expected : %q. Got %q instead\", expected0, s)\n\t}\n\n\t/* TODO: COME BACK TO THIS\n\n\ts = fmt.Sprintf(\"%#-d\", FmtNodeMap(m3))\n\texpected0 = fmt.Sprintf(\"map[Node.ID]gorgonia.Nodes {\\n\\t%x: [%x, %x]\\n}\", x.ID(), x.ID(), y.ID())\n\tif s != expected0 {\n\t\tt.Errorf(\"Case 5 failed. Got %q instead of %q\", s, expected0)\n\t}\n\t*/\n}\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 1.08984375,
          "content": "module gorgonia.org/gorgonia\n\ngo 1.16\n\nrequire (\n\n\tgithub.com/apache/arrow/go/arrow v0.0.0-20211112161151-bc219186db40 // indirect\n\n\tgithub.com/awalterschulze/gographviz v2.0.3+incompatible\n\tgithub.com/chewxy/hm v1.0.0\n\tgithub.com/chewxy/math32 v1.10.1\n\tgithub.com/go-gota/gota v0.12.0\n\tgithub.com/google/flatbuffers v2.0.6+incompatible // indirect\n\tgithub.com/google/uuid v1.3.0 // indirect\n\tgithub.com/leesper/go_rng v0.0.0-20190531154944-a612b043e353\n\tgithub.com/mattn/go-runewidth v0.0.13 // indirect\n\tgithub.com/pkg/errors v0.9.1\n\tgithub.com/stretchr/testify v1.7.0\n\tgithub.com/xtgo/set v1.0.0\n\tgo4.org/unsafe/assume-no-moving-gc v0.0.0-20211027215541-db492cf91b37 // indirect\n\tgolang.org/x/net v0.0.0-20220401154927-543a649e0bdd // indirect\n\tgolang.org/x/sys v0.0.0-20220330033206-e17cdc41300f // indirect\n\tgonum.org/v1/gonum v0.11.0\n\tgonum.org/v1/netlib v0.0.0-20220323200511-14de99971b2d\n\tgoogle.golang.org/protobuf v1.28.0 // indirect\n\tgopkg.in/cheggaaa/pb.v1 v1.0.28\n\tgorgonia.org/cu v0.9.4\n\tgorgonia.org/dawson v1.2.0\n\tgorgonia.org/tensor v0.9.23\n\tgorgonia.org/vecf32 v0.9.0\n\tgorgonia.org/vecf64 v0.9.0\n)\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 37.4033203125,
          "content": "cloud.google.com/go v0.26.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\ncloud.google.com/go v0.34.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\ndmitri.shuralyov.com/gpu/mtl v0.0.0-20190408044501-666a987793e9/go.mod h1:H6x//7gZCb22OMCxBHrMx7a5I7Hp++hsVxbQ4BYO7hU=\ngioui.org v0.0.0-20210308172011-57750fc8a0a6/go.mod h1:RSH6KIUZ0p2xy5zHDxgAM4zumjgTw83q2ge/PI+yyw8=\ngit.sr.ht/~sbinet/gg v0.3.1/go.mod h1:KGYtlADtqsqANL9ueOFkWymvzUvLMQllU5Ixo+8v3pc=\ngithub.com/BurntSushi/toml v0.3.1/go.mod h1:xHWCNGjB5oqiDr8zfno3MHue2Ht5sIBksp03qcyfWMU=\ngithub.com/BurntSushi/xgb v0.0.0-20160522181843-27f122750802/go.mod h1:IVnqGOEym/WlBOVXweHU+Q+/VP0lqqI8lqeDx9IjBqo=\ngithub.com/ajstarks/deck v0.0.0-20200831202436-30c9fc6549a9/go.mod h1:JynElWSGnm/4RlzPXRlREEwqTHAN3T56Bv2ITsFT3gY=\ngithub.com/ajstarks/deck/generate v0.0.0-20210309230005-c3f852c02e19/go.mod h1:T13YZdzov6OU0A1+RfKZiZN9ca6VeKdBdyDV+BY97Tk=\ngithub.com/ajstarks/svgo v0.0.0-20180226025133-644b8db467af/go.mod h1:K08gAheRH3/J6wwsYMMT4xOr94bZjxIelGM0+d/wbFw=\ngithub.com/ajstarks/svgo v0.0.0-20211024235047-1546f124cd8b/go.mod h1:1KcenG0jGWcpt8ov532z81sp/kMMUG485J2InIOyADM=\ngithub.com/antihax/optional v1.0.0/go.mod h1:uupD/76wgC+ih3iEmQUL+0Ugr19nfwCT1kdvxnR2qWY=\ngithub.com/apache/arrow/go/arrow v0.0.0-20201229220542-30ce2eb5d4dc/go.mod h1:c9sxoIT3YgLxH4UhLOCKaBlEojuMhVYpk4Ntv3opUTQ=\ngithub.com/apache/arrow/go/arrow v0.0.0-20210105145422-88aaea5262db/go.mod h1:c9sxoIT3YgLxH4UhLOCKaBlEojuMhVYpk4Ntv3opUTQ=\ngithub.com/apache/arrow/go/arrow v0.0.0-20211112161151-bc219186db40 h1:q4dksr6ICHXqG5hm0ZW5IHyeEJXoIJSOZeBLmWPNeIQ=\ngithub.com/apache/arrow/go/arrow v0.0.0-20211112161151-bc219186db40/go.mod h1:Q7yQnSMnLvcXlZ8RV+jwz/6y1rQTqbX6C82SndT52Zs=\ngithub.com/awalterschulze/gographviz v0.0.0-20190221210632-1e9ccb565bca/go.mod h1:GEV5wmg4YquNw7v1kkyoX9etIk8yVmXj+AkDHuuETHs=\ngithub.com/awalterschulze/gographviz v2.0.3+incompatible h1:9sVEXJBJLwGX7EQVhLm2elIKCm7P2YHFC8v6096G09E=\ngithub.com/awalterschulze/gographviz v2.0.3+incompatible/go.mod h1:GEV5wmg4YquNw7v1kkyoX9etIk8yVmXj+AkDHuuETHs=\ngithub.com/boombuler/barcode v1.0.0/go.mod h1:paBWMcWSl3LHKBqUq+rly7CNSldXjb2rDl3JlRe0mD8=\ngithub.com/boombuler/barcode v1.0.1/go.mod h1:paBWMcWSl3LHKBqUq+rly7CNSldXjb2rDl3JlRe0mD8=\ngithub.com/census-instrumentation/opencensus-proto v0.2.1/go.mod h1:f6KPmirojxKA12rnyqOA5BBL4O983OfeGPqjHWSTneU=\ngithub.com/chewxy/hm v1.0.0 h1:zy/TSv3LV2nD3dwUEQL2VhXeoXbb9QkpmdRAVUFiA6k=\ngithub.com/chewxy/hm v1.0.0/go.mod h1:qg9YI4q6Fkj/whwHR1D+bOGeF7SniIP40VweVepLjg0=\ngithub.com/chewxy/math32 v1.0.0/go.mod h1:Miac6hA1ohdDUTagnvJy/q+aNnEk16qWUdb8ZVhvCN0=\ngithub.com/chewxy/math32 v1.0.6/go.mod h1:dOB2rcuFrCn6UHrze36WSLVPKtzPMRAQvBvUwkSsLqs=\ngithub.com/chewxy/math32 v1.0.7-0.20210223031236-a3549c8cb6a9/go.mod h1:dOB2rcuFrCn6UHrze36WSLVPKtzPMRAQvBvUwkSsLqs=\ngithub.com/chewxy/math32 v1.0.8/go.mod h1:dOB2rcuFrCn6UHrze36WSLVPKtzPMRAQvBvUwkSsLqs=\ngithub.com/chewxy/math32 v1.10.1 h1:LFpeY0SLJXeaiej/eIp2L40VYfscTvKh/FSEZ68uMkU=\ngithub.com/chewxy/math32 v1.10.1/go.mod h1:dOB2rcuFrCn6UHrze36WSLVPKtzPMRAQvBvUwkSsLqs=\ngithub.com/client9/misspell v0.3.4/go.mod h1:qj6jICC3Q7zFZvVWo7KLAzC3yx5G7kyvSDkc90ppPyw=\ngithub.com/cloudflare/cfssl v0.0.0-20190808011637-b1ec8c586c2a/go.mod h1:yMWuSON2oQp+43nFtAV/uvKQIFpSPerB57DCt9t8sSA=\ngithub.com/cncf/udpa/go v0.0.0-20191209042840-269d4d468f6f/go.mod h1:M8M6+tZqaGXZJjfX53e64911xZQV5JYwmTeXPW+k8Sc=\ngithub.com/cncf/udpa/go v0.0.0-20201120205902-5459f2c99403/go.mod h1:WmhPx2Nbnhtbo57+VJT5O0JRkEi1Wbu0z5j0R8u5Hbk=\ngithub.com/cncf/xds/go v0.0.0-20210312221358-fbca930ec8ed/go.mod h1:eXthEFrGJvWHgFFCl3hGmgk+/aYT6PnTQLykKQRLhEs=\ngithub.com/cznic/cc v0.0.0-20181122101902-d673e9b70d4d/go.mod h1:m3fD/V+XTB35Kh9zw6dzjMY+We0Q7PMf6LLIC4vuG9k=\ngithub.com/cznic/golex v0.0.0-20181122101858-9c343928389c/go.mod h1:+bmmJDNmKlhWNG+gwWCkaBoTy39Fs+bzRxVBzoTQbIc=\ngithub.com/cznic/mathutil v0.0.0-20181122101859-297441e03548/go.mod h1:e6NPNENfs9mPDVNRekM7lKScauxd5kXTr1Mfyig6TDM=\ngithub.com/cznic/strutil v0.0.0-20181122101858-275e90344537/go.mod h1:AHHPPPXTw0h6pVabbcbyGRK1DckRn7r/STdZEeIDzZc=\ngithub.com/cznic/xc v0.0.0-20181122101856-45b06973881e/go.mod h1:3oFoiOvCDBYH+swwf5+k/woVmWy7h1Fcyu8Qig/jjX0=\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/dustin/go-humanize v1.0.0/go.mod h1:HtrtbFcZ19U5GC7JDqmcUSB87Iq5E25KnS6fMYU6eOk=\ngithub.com/envoyproxy/go-control-plane v0.9.0/go.mod h1:YTl/9mNaCwkRvm6d1a2C3ymFceY/DCBVvsKhRF0iEA4=\ngithub.com/envoyproxy/go-control-plane v0.9.1-0.20191026205805-5f8ba28d4473/go.mod h1:YTl/9mNaCwkRvm6d1a2C3ymFceY/DCBVvsKhRF0iEA4=\ngithub.com/envoyproxy/go-control-plane v0.9.4/go.mod h1:6rpuAdCZL397s3pYoYcLgu1mIlRU8Am5FuJP05cCM98=\ngithub.com/envoyproxy/go-control-plane v0.9.9-0.20201210154907-fd9021fe5dad/go.mod h1:cXg6YxExXjJnVBQHBLXeUAgxn2UodCpnH306RInaBQk=\ngithub.com/envoyproxy/go-control-plane v0.9.9-0.20210217033140-668b12f5399d/go.mod h1:cXg6YxExXjJnVBQHBLXeUAgxn2UodCpnH306RInaBQk=\ngithub.com/envoyproxy/go-control-plane v0.9.9-0.20210512163311-63b5d3c536b0/go.mod h1:hliV/p42l8fGbc6Y9bQ70uLwIvmJyVE5k4iMKlh8wCQ=\ngithub.com/envoyproxy/protoc-gen-validate v0.1.0/go.mod h1:iSmxcyjqTsJpI2R4NaDN7+kN2VEUnK/pcBlmesArF7c=\ngithub.com/fatih/color v1.10.0 h1:s36xzo75JdqLaaWoiEHk767eHiwo0598uUxyfiPkDsg=\ngithub.com/fatih/color v1.10.0/go.mod h1:ELkj/draVOlAH/xkhN6mQ50Qd0MPOk5AAr3maGEBuJM=\ngithub.com/fogleman/gg v1.2.1-0.20190220221249-0403632d5b90/go.mod h1:R/bRT+9gY/C5z7JzPU0zXsXHKM4/ayA+zqcVNZzPa1k=\ngithub.com/fogleman/gg v1.3.0/go.mod h1:R/bRT+9gY/C5z7JzPU0zXsXHKM4/ayA+zqcVNZzPa1k=\ngithub.com/ghodss/yaml v1.0.0/go.mod h1:4dBDuWmgqj2HViK6kFavaiC9ZROes6MMH2rRYeMEF04=\ngithub.com/go-fonts/dejavu v0.1.0/go.mod h1:4Wt4I4OU2Nq9asgDCteaAaWZOV24E+0/Pwo0gppep4g=\ngithub.com/go-fonts/latin-modern v0.2.0/go.mod h1:rQVLdDMK+mK1xscDwsqM5J8U2jrRa3T0ecnM9pNujks=\ngithub.com/go-fonts/liberation v0.1.1/go.mod h1:K6qoJYypsmfVjWg8KOVDQhLc8UDgIK2HYqyqAO9z7GY=\ngithub.com/go-fonts/liberation v0.2.0/go.mod h1:K6qoJYypsmfVjWg8KOVDQhLc8UDgIK2HYqyqAO9z7GY=\ngithub.com/go-fonts/stix v0.1.0/go.mod h1:w/c1f0ldAUlJmLBvlbkvVXLAD+tAMqobIIQpmnUIzUY=\ngithub.com/go-gl/glfw v0.0.0-20190409004039-e6da0acd62b1/go.mod h1:vR7hzQXu2zJy9AVAgeJqvqgH9Q5CA+iKCZ2gyEVpxRU=\ngithub.com/go-gota/gota v0.12.0 h1:T5BDg1hTf5fZ/CO+T/N0E+DDqUhvoKBl+UVckgcAAQg=\ngithub.com/go-gota/gota v0.12.0/go.mod h1:UT+NsWpZC/FhaOyWb9Hui0jXg0Iq8e/YugZHTbyW/34=\ngithub.com/go-latex/latex v0.0.0-20210118124228-b3d85cf34e07/go.mod h1:CO1AlKB2CSIqUrmQPqA0gdRIlnLEY0gK5JGjh37zN5U=\ngithub.com/go-latex/latex v0.0.0-20210823091927-c0d11ff05a81/go.mod h1:SX0U8uGpxhq9o2S/CELCSUxEWWAuoCUcVCQWv7G2OCk=\ngithub.com/go-pdf/fpdf v0.5.0/go.mod h1:HzcnA+A23uwogo0tp9yU+l3V+KXhiESpt1PMayhOh5M=\ngithub.com/go-pdf/fpdf v0.6.0/go.mod h1:HzcnA+A23uwogo0tp9yU+l3V+KXhiESpt1PMayhOh5M=\ngithub.com/gogo/protobuf v1.2.1/go.mod h1:hp+jE20tsWTFYpLwKvXlhS1hjn+gTNwPg2I6zVXpSg4=\ngithub.com/gogo/protobuf v1.3.1/go.mod h1:SlYgWuQ5SjCEi6WLHjHCa1yvBfUnHcTbrrZtXPKa29o=\ngithub.com/gogo/protobuf v1.3.2 h1:Ov1cvc58UF3b5XjBnZv7+opcTcQFZebYjWzi34vdm4Q=\ngithub.com/gogo/protobuf v1.3.2/go.mod h1:P1XiOD3dCwIKUDQYPy72D8LYyHL2YPYrpS2s69NZV8Q=\ngithub.com/golang/freetype v0.0.0-20170609003504-e2365dfdc4a0/go.mod h1:E/TSTwGwJL78qG/PmXZO1EjYhfJinVAhrmmHX6Z8B9k=\ngithub.com/golang/glog v0.0.0-20160126235308-23def4e6c14b/go.mod h1:SBH7ygxi8pfUlaOkMMuAQtPIUF8ecWP5IEl/CR7VP2Q=\ngithub.com/golang/mock v1.1.1/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=\ngithub.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.0/go.mod h1:Qd/q+1AKNOZr9uGQzbzCmRO6sUih6GTPZv6a1/R87v0=\ngithub.com/golang/protobuf v1.3.2/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.3/go.mod h1:vzj43D7+SQXF/4pzW/hwtAqwc6iTitCiVSaWz5lYuqw=\ngithub.com/golang/protobuf v1.4.0-rc.1/go.mod h1:ceaxUfeHdC40wWswd/P6IGgMaK3YpKi5j83Wpe3EHw8=\ngithub.com/golang/protobuf v1.4.0-rc.1.0.20200221234624-67d41d38c208/go.mod h1:xKAWHe0F5eneWXFV3EuXVDTCmh+JuBKY0li0aMyXATA=\ngithub.com/golang/protobuf v1.4.0-rc.2/go.mod h1:LlEzMj4AhA7rCAGe4KMBDvJI+AwstrUpVNzEA03Pprs=\ngithub.com/golang/protobuf v1.4.0-rc.4.0.20200313231945-b860323f09d0/go.mod h1:WU3c8KckQ9AFe+yFwt9sWVRKCVIyN9cPHBJSNnbL67w=\ngithub.com/golang/protobuf v1.4.0/go.mod h1:jodUvKwWbYaEsadDk5Fwe5c77LiNKVO9IDvqG2KuDX0=\ngithub.com/golang/protobuf v1.4.1/go.mod h1:U8fpvMrcmy5pZrNK1lt4xCsGvpyWQ/VVv6QDs8UjoX8=\ngithub.com/golang/protobuf v1.4.2/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=\ngithub.com/golang/protobuf v1.4.3/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=\ngithub.com/golang/protobuf v1.5.0/go.mod h1:FsONVRAS9T7sI+LIUmWTfcYkHO4aIWwzhcaSAoJOfIk=\ngithub.com/golang/protobuf v1.5.2 h1:ROPKBNFfQgOUMifHyP+KYbvpjbdoFNs+aK7DXlji0Tw=\ngithub.com/golang/protobuf v1.5.2/go.mod h1:XVQd3VNwM+JqD3oG2Ue2ip4fOMUkwXdXDdiuN0vRsmY=\ngithub.com/golang/snappy v0.0.3 h1:fHPg5GQYlCeLIPB9BZqMVR5nR9A+IM5zcgeTdjMYmLA=\ngithub.com/golang/snappy v0.0.3/go.mod h1:/XxbfmMg8lxefKM7IXC3fBNl/7bRcc72aCRzEWrmP2Q=\ngithub.com/gonum/blas v0.0.0-20181208220705-f22b278b28ac/go.mod h1:P32wAyui1PQ58Oce/KYkOqQv8cVw1zAapXOl+dRFGbc=\ngithub.com/google/flatbuffers v1.10.0/go.mod h1:1AeVuKshWv4vARoZatz6mlQ0JxURH0Kv5+zNeJKJCa8=\ngithub.com/google/flatbuffers v1.11.0/go.mod h1:1AeVuKshWv4vARoZatz6mlQ0JxURH0Kv5+zNeJKJCa8=\ngithub.com/google/flatbuffers v1.12.0/go.mod h1:1AeVuKshWv4vARoZatz6mlQ0JxURH0Kv5+zNeJKJCa8=\ngithub.com/google/flatbuffers v2.0.0+incompatible/go.mod h1:1AeVuKshWv4vARoZatz6mlQ0JxURH0Kv5+zNeJKJCa8=\ngithub.com/google/flatbuffers v2.0.6+incompatible h1:XHFReMv7nFFusa+CEokzWbzaYocKXI6C7hdU5Kgh9Lw=\ngithub.com/google/flatbuffers v2.0.6+incompatible/go.mod h1:1AeVuKshWv4vARoZatz6mlQ0JxURH0Kv5+zNeJKJCa8=\ngithub.com/google/go-cmp v0.2.0/go.mod h1:oXzfMopK8JAjlY9xF4vHSVASa0yLyX7SntLO5aqRK0M=\ngithub.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.3.1/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.4.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.3/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.5/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.6/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.7 h1:81/ik6ipDQS2aGcBfIN5dHDB36BwrStyeAQquSYCV4o=\ngithub.com/google/go-cmp v0.5.7/go.mod h1:n+brtR0CgQNWTVd5ZUFpTBC8YFBDLK/h/bpaJ8/DtOE=\ngithub.com/google/uuid v1.1.1/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/google/uuid v1.1.2/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/google/uuid v1.3.0 h1:t6JiXgmwXMjEs8VusXIJk2BXHsn+wx8BZdTaoZ5fu7I=\ngithub.com/google/uuid v1.3.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/gorgonia/bindgen v0.0.0-20180812032444-09626750019e/go.mod h1:YzKk63P9jQHkwAo2rXHBv02yPxDzoQT2cBV0x5bGV/8=\ngithub.com/gorgonia/bindgen v0.0.0-20210223094355-432cd89e7765/go.mod h1:BLHSe436vhQKRfm6wxJgebeK4fDY+ER/8jV3vVH9yYU=\ngithub.com/grpc-ecosystem/grpc-gateway v1.16.0/go.mod h1:BDjrQk3hbvj6Nolgz8mAMFbcEtjT1g+wF4CSlocrBnw=\ngithub.com/jung-kurt/gofpdf v1.0.0/go.mod h1:7Id9E/uU8ce6rXgefFLlgrJj/GYY22cpxn+r32jIOes=\ngithub.com/jung-kurt/gofpdf v1.0.3-0.20190309125859-24315acbbda5/go.mod h1:7Id9E/uU8ce6rXgefFLlgrJj/GYY22cpxn+r32jIOes=\ngithub.com/kisielk/errcheck v1.1.0/go.mod h1:EZBBE59ingxPouuu3KfxchcWSUPOHkagtvWXihfKN4Q=\ngithub.com/kisielk/errcheck v1.2.0/go.mod h1:/BMXB+zMLi60iA8Vv6Ksmxu/1UDYcXs4uQLJ+jE2L00=\ngithub.com/kisielk/errcheck v1.5.0/go.mod h1:pFxgyoBC7bSaBwPgfKdkLd5X25qrDl4LWUI2bnpBCr8=\ngithub.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO+MFFWcvkIS/tQcRk01m1F5IRFswLeQ+oQHNcck=\ngithub.com/klauspost/compress v1.13.1 h1:wXr2uRxZTJXHLly6qhJabee5JqIhTRoLBhDOA74hDEQ=\ngithub.com/klauspost/compress v1.13.1/go.mod h1:8dP1Hq4DHOhN9w426knH3Rhby4rFm6D8eO+e+Dq5Gzg=\ngithub.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=\ngithub.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=\ngithub.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=\ngithub.com/leesper/go_rng v0.0.0-20171009123644-5344a9259b21/go.mod h1:N0SVk0uhy+E1PZ3C9ctsPRlvOPAFPkCNlcPBDkt0N3U=\ngithub.com/leesper/go_rng v0.0.0-20190531154944-a612b043e353 h1:X/79QL0b4YJVO5+OsPH9rF2u428CIrGL/jLmPsoOQQ4=\ngithub.com/leesper/go_rng v0.0.0-20190531154944-a612b043e353/go.mod h1:N0SVk0uhy+E1PZ3C9ctsPRlvOPAFPkCNlcPBDkt0N3U=\ngithub.com/mattn/go-colorable v0.1.8 h1:c1ghPdyEDarC70ftn0y+A/Ee++9zz8ljHG1b13eJ0s8=\ngithub.com/mattn/go-colorable v0.1.8/go.mod h1:u6P/XSegPjTcexA+o6vUJrdnUu04hMope9wVRipJSqc=\ngithub.com/mattn/go-isatty v0.0.12 h1:wuysRhFDzyxgEmMf5xjvJ2M9dZoWAXNNr5LSBS7uHXY=\ngithub.com/mattn/go-isatty v0.0.12/go.mod h1:cbi8OIDigv2wuxKPP5vlRcQ1OAZbq2CE4Kysco4FUpU=\ngithub.com/mattn/go-runewidth v0.0.4/go.mod h1:LwmH8dsx7+W8Uxz3IHJYH5QSwggIsqBzpuz5H//U1FU=\ngithub.com/mattn/go-runewidth v0.0.13 h1:lTGmDsbAYt5DmK6OnoV7EuIF1wEIFAcxld6ypU4OSgU=\ngithub.com/mattn/go-runewidth v0.0.13/go.mod h1:Jdepj2loyihRzMpdS35Xk/zdY8IAYHsh153qUoGf23w=\ngithub.com/phpdave11/gofpdf v1.4.2/go.mod h1:zpO6xFn9yxo3YLyMvW8HcKWVdbNqgIfOOp2dXMnm1mY=\ngithub.com/phpdave11/gofpdi v1.0.12/go.mod h1:vBmVV0Do6hSBHC8uKUQ71JGW+ZGQq74llk/7bXwjDoI=\ngithub.com/phpdave11/gofpdi v1.0.13/go.mod h1:vBmVV0Do6hSBHC8uKUQ71JGW+ZGQq74llk/7bXwjDoI=\ngithub.com/pierrec/lz4/v4 v4.1.8 h1:ieHkV+i2BRzngO4Wd/3HGowuZStgq6QkPsD1eolNAO4=\ngithub.com/pierrec/lz4/v4 v4.1.8/go.mod h1:gZWDp/Ze/IJXGXf23ltt2EXimqmTUXEy0GFuRQyBid4=\ngithub.com/pkg/errors v0.8.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=\ngithub.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/prometheus/client_model v0.0.0-20190812154241-14fe0d1b01d4/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\ngithub.com/remyoudompheng/bigfft v0.0.0-20170806203942-52369c62f446/go.mod h1:uYEyJGbgTkfkS4+E/PavXkNJcbFIpEtjt2B0KDQ5+9M=\ngithub.com/remyoudompheng/bigfft v0.0.0-20190728182440-6a916e37a237/go.mod h1:qqbHyh8v60DhA7CoWK5oRCqLrMHRGoxYCSS9EjAz6Eo=\ngithub.com/remyoudompheng/bigfft v0.0.0-20200410134404-eec4a21b6bb0/go.mod h1:qqbHyh8v60DhA7CoWK5oRCqLrMHRGoxYCSS9EjAz6Eo=\ngithub.com/rivo/uniseg v0.2.0 h1:S1pD9weZBuJdFmowNwbpi7BJ8TNftyUImj/0WQi72jY=\ngithub.com/rivo/uniseg v0.2.0/go.mod h1:J6wj4VEh+S6ZtnVlnTBMWIodfgj8LQOQFoIToxlJtxc=\ngithub.com/rogpeppe/fastuuid v1.2.0/go.mod h1:jVj6XXZzXRy/MSR5jhDC/2q6DgLz+nrA6LYCDYWNEvQ=\ngithub.com/ruudk/golang-pdf417 v0.0.0-20181029194003-1af4ab5afa58/go.mod h1:6lfFZQK844Gfx8o5WFuvpxWRwnSoipWe/p622j1v06w=\ngithub.com/ruudk/golang-pdf417 v0.0.0-20201230142125-a7e3863a1245/go.mod h1:pQAZKsJ8yyVxGRWYNEm9oFB8ieLgKFnamEyDmSA0BRk=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/testify v1.1.4/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=\ngithub.com/stretchr/testify v1.2.0/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=\ngithub.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=\ngithub.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\ngithub.com/stretchr/testify v1.4.0/go.mod h1:j7eGeouHqKxXV5pUuKE4zz7dFj8WfuZ+81PSLYec5m4=\ngithub.com/stretchr/testify v1.5.1/go.mod h1:5W2xD1RspED5o8YsWQXVCued0rvSQ+mT+I5cxcmMvtA=\ngithub.com/stretchr/testify v1.6.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.7.0 h1:nwc3DEeHmmLAfoZucVR881uASk0Mfjw8xYJ99tb5CcY=\ngithub.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/xtgo/set v1.0.0 h1:6BCNBRv3ORNDQ7fyoJXRv+tstJz3m1JVFQErfeZz2pY=\ngithub.com/xtgo/set v1.0.0/go.mod h1:d3NHzGzSa0NmB2NhFyECA+QdRp29oEn2xbT+TpeFoM8=\ngithub.com/yuin/goldmark v1.1.27/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\ngithub.com/yuin/goldmark v1.2.1/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\ngithub.com/yuin/goldmark v1.3.5/go.mod h1:mwnBkeHKe2W/ZEtQ+71ViKU8L12m81fl3OWwC1Zlc8k=\ngithub.com/yuin/goldmark v1.4.1/go.mod h1:mwnBkeHKe2W/ZEtQ+71ViKU8L12m81fl3OWwC1Zlc8k=\ngo.opentelemetry.io/proto/otlp v0.7.0/go.mod h1:PqfVotwruBrMGOCsRd/89rSnXhoiJIqeYNgFYFoEGnI=\ngo4.org/unsafe/assume-no-moving-gc v0.0.0-20201222180813-1025295fd063/go.mod h1:FftLjUGFEDu5k8lt0ddY+HcrH/qU/0qk+H8j9/nTl3E=\ngo4.org/unsafe/assume-no-moving-gc v0.0.0-20211027215541-db492cf91b37 h1:Tx9kY6yUkLge/pFG7IEMwDZy6CS2ajFc9TvQdPCW0uA=\ngo4.org/unsafe/assume-no-moving-gc v0.0.0-20211027215541-db492cf91b37/go.mod h1:FftLjUGFEDu5k8lt0ddY+HcrH/qU/0qk+H8j9/nTl3E=\ngolang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\ngolang.org/x/crypto v0.0.0-20190510104115-cbcb75029529/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\ngolang.org/x/exp v0.0.0-20180321215751-8460e604b9de/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20180807140117-3d87b88a115f/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20181106170214-d68db9428509/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20190121172915-509febef88a4/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20190125153040-c74c464bbbf2/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20190306152737-a1d7652674e8/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20190312203227-4b39c73a6495/go.mod h1:ZjyILWgesfNpC6sMxTJOJm9Kp84zZh5NQWvqDGG3Qr8=\ngolang.org/x/exp v0.0.0-20191002040644-a1355ae1e2c3 h1:n9HxLrNxWWtEb1cA950nuEEj3QnKbtsCJ6KjcgisNUs=\ngolang.org/x/exp v0.0.0-20191002040644-a1355ae1e2c3/go.mod h1:NOZ3BPKG0ec/BKJQgnvsSFpcKLM5xXVWnvZS97DWHgE=\ngolang.org/x/image v0.0.0-20180708004352-c73c2afc3b81/go.mod h1:ux5Hcp/YLpHSI86hEcLt0YII63i6oz57MZXIpbrjZUs=\ngolang.org/x/image v0.0.0-20190227222117-0694c2d4d067/go.mod h1:kZ7UVZpmo3dzQBMxlp+ypCbDeSB+sBbTgSJuh5dn5js=\ngolang.org/x/image v0.0.0-20190802002840-cff245a6509b/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\ngolang.org/x/image v0.0.0-20190910094157-69e4b8554b2a/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\ngolang.org/x/image v0.0.0-20200119044424-58c23975cae1/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\ngolang.org/x/image v0.0.0-20200430140353-33d19683fad8/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\ngolang.org/x/image v0.0.0-20200618115811-c13761719519/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\ngolang.org/x/image v0.0.0-20201208152932-35266b937fa6/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\ngolang.org/x/image v0.0.0-20210216034530-4410531fe030/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\ngolang.org/x/image v0.0.0-20210607152325-775e3b0c77b9/go.mod h1:023OzeP/+EPmXeapQh35lcL3II3LrY8Ic+EFFKVhULM=\ngolang.org/x/image v0.0.0-20210628002857-a66eb6448b8d/go.mod h1:023OzeP/+EPmXeapQh35lcL3II3LrY8Ic+EFFKVhULM=\ngolang.org/x/image v0.0.0-20211028202545-6944b10bf410/go.mod h1:023OzeP/+EPmXeapQh35lcL3II3LrY8Ic+EFFKVhULM=\ngolang.org/x/image v0.0.0-20220302094943-723b81ca9867/go.mod h1:023OzeP/+EPmXeapQh35lcL3II3LrY8Ic+EFFKVhULM=\ngolang.org/x/lint v0.0.0-20181026193005-c67002cb31c3/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=\ngolang.org/x/lint v0.0.0-20190227174305-5b3e6a55c961/go.mod h1:wehouNa3lNwaWXcvxsM5YxQ5yQlVC4a0KAMCusXpPoU=\ngolang.org/x/lint v0.0.0-20190313153728-d0100b6bd8b3/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/lint v0.0.0-20210508222113-6edffad5e616/go.mod h1:3xt1FjdF8hUf6vQPIChWIBhFzV8gjjsPE/fR3IyQdNY=\ngolang.org/x/mobile v0.0.0-20190312151609-d3739f865fa6/go.mod h1:z+o9i4GpDbdi3rU15maQ/Ox0txvL9dWGYEHz965HBQE=\ngolang.org/x/mobile v0.0.0-20190719004257-d2bd2a29d028/go.mod h1:E/iHnbuqvinMTCcRqshq8CkpyQDoeVncDDYHnLhea+o=\ngolang.org/x/mod v0.1.0/go.mod h1:0QHyrYULN0/3qlju5TqG8bIK38QM8yzMo5ekMj3DlcY=\ngolang.org/x/mod v0.1.1-0.20191105210325-c90efee705ee/go.mod h1:QqPTAvyqsEbceGzBzNggFXnrqF1CaUcvgkdR5Ot7KZg=\ngolang.org/x/mod v0.2.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/mod v0.3.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/mod v0.4.2/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/mod v0.5.1/go.mod h1:5OXOZSfqPIIbmVBIIKWRFfZjPR0E5r58TLhUjH0a2Ro=\ngolang.org/x/net v0.0.0-20180724234803-3673e40ba225/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20180826012351-8a410e7b638d/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20180906233101-161cd47e91fd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190108225652-1e06a53dbb7e/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190213061140-3a22650c66bd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200226121028-0de0cce0169b/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200822124328-c89045814202/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\ngolang.org/x/net v0.0.0-20200904194848-62affa334b73/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\ngolang.org/x/net v0.0.0-20201021035429-f5854403a974/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=\ngolang.org/x/net v0.0.0-20210405180319-a5a99cb37ef4/go.mod h1:p54w0d4576C0XHj96bSt6lcn1PtDYWL6XObtHCRCNQM=\ngolang.org/x/net v0.0.0-20210423184538-5f58ad60dda6/go.mod h1:OJAsFXCWl8Ukc7SiCT/9KSuxbyM7479/AVlXFRxuMCk=\ngolang.org/x/net v0.0.0-20210614182718-04defd469f4e/go.mod h1:9nx3DQGgdP8bBQD5qxJ1jj9UTztislL4KSBs9R2vV5Y=\ngolang.org/x/net v0.0.0-20211015210444-4f30a5c0130f/go.mod h1:9nx3DQGgdP8bBQD5qxJ1jj9UTztislL4KSBs9R2vV5Y=\ngolang.org/x/net v0.0.0-20220401154927-543a649e0bdd h1:zYlwaUHTmxuf6H7hwO2dgwqozQmH7zf4x+/qql4oVWc=\ngolang.org/x/net v0.0.0-20220401154927-543a649e0bdd/go.mod h1:CfG3xpIq0wQ8r1q4Su4UZFWDARRcnwPjda9FqA0JpMk=\ngolang.org/x/oauth2 v0.0.0-20180821212333-d2e6202438be/go.mod h1:N/0e6XlmueqKjAGxoOufVs8QHGRruUQn6yWY3a++T0U=\ngolang.org/x/oauth2 v0.0.0-20200107190931-bf48bf16ab8d/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20181108010431-42b317875d0f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190911185100-cd5d95a43a6e/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20201020160332-67f06af15bc9/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20210220032951-036812b2e83c/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sys v0.0.0-20180830151530-49385e6e1522/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190226215855-775f8194d0f9/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190312061237-fead79001313/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200116001909-b77594299b42/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200223170610-d5e6a3e2c0ae/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200323222414-85ca7c5b95cd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200909081042-eff7692f9009/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200930185726-fdedc70b468f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20201119102817-f84b799fce68/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210119212857-b64e53b001e4/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210304124612-50617c2ba197/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210330210617-4fbd30eecc44/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210423082822-04245dca01da/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210510120138-977fb7262007/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20210615035016-665e8c7367d1/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20210630005230-0f9fa26af87c/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20211019181941-9d821ace8654/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20211216021012-1d35b9e2eb4e/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220330033206-e17cdc41300f h1:rlezHXNlxYWvBCzNses9Dlc7nGFaNMJeqLolcmQSSZY=\ngolang.org/x/sys v0.0.0-20220330033206-e17cdc41300f/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=\ngolang.org/x/term v0.0.0-20210927222741-03fcf44c2211/go.mod h1:jbD1KX2456YbFQfuXm/mYQcufACuNUgVhRMnK/tPxf8=\ngolang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.5/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.6/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.7/go.mod h1:u+2+/6zg+i71rQMx5EYifcz6MCKuco9NR6JIITiCfzQ=\ngolang.org/x/tools v0.0.0-20180221164845-07fd8470d635/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20180525024113-a5b4c53f6e8b/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20181030221726-6c7e314b6563/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20190114222345-bf090417da8b/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20190206041539-40960b6deb8e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20190226205152-f727befe758c/go.mod h1:9Yl7xja0Znq3iFh3HoIrodX9oNMXvdceNzlUR8zjMvY=\ngolang.org/x/tools v0.0.0-20190311212946-11955173bddd/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190312151545-0bb0c0a6e846/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190524140312-2c0ae7006135/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\ngolang.org/x/tools v0.0.0-20190927191325-030b2cf1153e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20200130002326-2f3ba24bd6e7/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200619180055-7c47624df98f/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20210106214847-113979e3529a/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=\ngolang.org/x/tools v0.1.0/go.mod h1:xkSsbof2nBLbhDlRMhhhyNLN/zl3eTqcnHD5viDpcZ0=\ngolang.org/x/tools v0.1.4/go.mod h1:o0xws9oXOQQZyjljx8fwUC0k7L1pTE6eaCbjGeHmOkk=\ngolang.org/x/tools v0.1.9/go.mod h1:nABZi5QlRsZVlzPpHl034qft6wpY4eDcsTt5AaioBiU=\ngolang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191011141410-1b5146add898/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1 h1:go1bK/D/BFZV2I8cIQd1NKEZ+0owSTG1fDTci4IqFcE=\ngolang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngonum.org/v1/gonum v0.0.0-20180816165407-929014505bf4/go.mod h1:Y+Yx5eoAFn32cQvJDxZx5Dpnq+c3wtXuadVZAcxbbBo=\ngonum.org/v1/gonum v0.0.0-20190226202314-149afe6ec0b6/go.mod h1:jevfED4GnIEnJrWW55YmY9DMhajHcnkqVnEXmEtMyNI=\ngonum.org/v1/gonum v0.0.0-20190902003836-43865b531bee/go.mod h1:9mxDZsDKxgMAuccQkewq682L+0eCu4dCN2yonUJTCLU=\ngonum.org/v1/gonum v0.8.1-0.20200930085651-eea0b5cb5cc9/go.mod h1:oe/vMfY3deqTw+1EZJhuvEW2iwGF1bW9wwu7XCu0+v0=\ngonum.org/v1/gonum v0.8.2/go.mod h1:oe/vMfY3deqTw+1EZJhuvEW2iwGF1bW9wwu7XCu0+v0=\ngonum.org/v1/gonum v0.9.1/go.mod h1:TZumC3NeyVQskjXqmyWt4S3bINhy7B4eYwW69EbyX+0=\ngonum.org/v1/gonum v0.9.3/go.mod h1:TZumC3NeyVQskjXqmyWt4S3bINhy7B4eYwW69EbyX+0=\ngonum.org/v1/gonum v0.11.0 h1:f1IJhK4Km5tBJmaiJXtk/PkL4cdVX6J+tGiM187uT5E=\ngonum.org/v1/gonum v0.11.0/go.mod h1:fSG4YDCxxUZQJ7rKsQrj0gMOg00Il0Z96/qMA4bVQhA=\ngonum.org/v1/netlib v0.0.0-20190221094214-0632e2ebbd2d/go.mod h1:wa6Ws7BG/ESfp6dHfk7C6KdzKA7wR7u/rKwOGE66zvw=\ngonum.org/v1/netlib v0.0.0-20190313105609-8cb42192e0e0/go.mod h1:wa6Ws7BG/ESfp6dHfk7C6KdzKA7wR7u/rKwOGE66zvw=\ngonum.org/v1/netlib v0.0.0-20201012070519-2390d26c3658/go.mod h1:zQa7n16lh3Z6FbSTYgjG+KNhz1bA/b9t3plFEaGMp+A=\ngonum.org/v1/netlib v0.0.0-20220323200511-14de99971b2d h1:Miy/8hAaViOoUdAtXzhdZvhJ6ocHabPJNSj6WXSdtNM=\ngonum.org/v1/netlib v0.0.0-20220323200511-14de99971b2d/go.mod h1:ObwMamC//3VQXZ2+uTOuOfnJNnZPdwBUibkUGgltkQA=\ngonum.org/v1/plot v0.0.0-20190515093506-e2840ee46a6b/go.mod h1:Wt8AAjI+ypCyYX3nZBvf6cAIx93T+c/OS2HFAYskSZc=\ngonum.org/v1/plot v0.9.0/go.mod h1:3Pcqqmp6RHvJI72kgb8fThyUnav364FOsdDo2aGW5lY=\ngonum.org/v1/plot v0.10.1/go.mod h1:VZW5OlhkL1mysU9vaqNHnsy86inf6Ot+jB3r+BczCEo=\ngoogle.golang.org/appengine v1.1.0/go.mod h1:EbEs0AVv82hx2wNQdGPgUI5lhzA/G0D9YwlJXL52JkM=\ngoogle.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\ngoogle.golang.org/genproto v0.0.0-20180817151627-c66870c02cf8/go.mod h1:JiN7NxoALGmiZfu7CAH4rXhgtRTLTxftemlI0sWmxmc=\ngoogle.golang.org/genproto v0.0.0-20180831171423-11092d34479b/go.mod h1:JiN7NxoALGmiZfu7CAH4rXhgtRTLTxftemlI0sWmxmc=\ngoogle.golang.org/genproto v0.0.0-20190819201941-24fa4b261c55/go.mod h1:DMBHOl98Agz4BDEuKkezgsaosCRResVns1a3J2ZsMNc=\ngoogle.golang.org/genproto v0.0.0-20200513103714-09dca8ec2884/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200526211855-cb27e3aa2013/go.mod h1:NbSheEEYHJ7i3ixzK3sjbqSGDJWnxyFXZblF3eUsNvo=\ngoogle.golang.org/genproto v0.0.0-20200911024640-645f7a48b24f/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20210630183607-d20f26d13c79/go.mod h1:yiaVoXHpRzHGyxV3o4DktVWY4mSUErTKaeEOq6C3t3U=\ngoogle.golang.org/grpc v1.19.0/go.mod h1:mqu4LbDTu4XGKhr4mRzUsmM4RtVoemTSY81AxZiDr8c=\ngoogle.golang.org/grpc v1.23.0/go.mod h1:Y5yQAOtifL1yxbo5wqy6BxZv8vAUGQwXBOALyacEbxg=\ngoogle.golang.org/grpc v1.25.1/go.mod h1:c3i+UQWmh7LiEpx4sFZnkU36qjEYZ0imhYfXVyQciAY=\ngoogle.golang.org/grpc v1.27.0/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=\ngoogle.golang.org/grpc v1.32.0/go.mod h1:N36X2cJ7JwdamYAgDz+s+rVMFjt3numwzf/HckM8pak=\ngoogle.golang.org/grpc v1.33.1/go.mod h1:fr5YgcSWrqhRRxogOsw7RzIpsmvOZ6IcH4kBYTpR3n0=\ngoogle.golang.org/grpc v1.36.0/go.mod h1:qjiiYl8FncCW8feJPdyg3v6XW24KsRHe+dy9BAGRRjU=\ngoogle.golang.org/grpc v1.38.0/go.mod h1:NREThFqKR1f3iQ6oBuvc5LadQuXVGo9rkm5ZGrQdJfM=\ngoogle.golang.org/grpc v1.39.0/go.mod h1:PImNr+rS9TWYb2O4/emRugxiyHZ5JyHW5F+RPnDzfrE=\ngoogle.golang.org/grpc/cmd/protoc-gen-go-grpc v0.0.0-20200910201057-6591123024b3/go.mod h1:6Kw0yEErY5E/yWrBtf03jp27GLLJujG4z/JK95pnjjw=\ngoogle.golang.org/protobuf v0.0.0-20200109180630-ec00e32a8dfd/go.mod h1:DFci5gLYBciE7Vtevhsrf46CRTquxDuWsQurQQe4oz8=\ngoogle.golang.org/protobuf v0.0.0-20200221191635-4d8936d0db64/go.mod h1:kwYJMbMJ01Woi6D6+Kah6886xMZcty6N08ah7+eCXa0=\ngoogle.golang.org/protobuf v0.0.0-20200228230310-ab0ca4ff8a60/go.mod h1:cfTl7dwQJ+fmap5saPgwCLgHXTUD7jkjRqWcaiX5VyM=\ngoogle.golang.org/protobuf v1.20.1-0.20200309200217-e05f789c0967/go.mod h1:A+miEFZTKqfCUM6K7xSMQL9OKL/b6hQv+e19PK+JZNE=\ngoogle.golang.org/protobuf v1.21.0/go.mod h1:47Nbq4nVaFHyn7ilMalzfO3qCViNmqZ2kzikPIcrTAo=\ngoogle.golang.org/protobuf v1.22.0/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\ngoogle.golang.org/protobuf v1.23.0/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\ngoogle.golang.org/protobuf v1.23.1-0.20200526195155-81db48ad09cc/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\ngoogle.golang.org/protobuf v1.24.0/go.mod h1:r/3tXBNzIEhYS9I1OUVjXDlt8tc493IdKGjtUeSXeh4=\ngoogle.golang.org/protobuf v1.25.0/go.mod h1:9JNX74DMeImyA3h4bdi1ymwjUzf21/xIlbajtzgsN7c=\ngoogle.golang.org/protobuf v1.26.0-rc.1/go.mod h1:jlhhOSvTdKEhbULTjvd4ARK9grFBp09yW+WbY/TyQbw=\ngoogle.golang.org/protobuf v1.26.0/go.mod h1:9q0QmTI4eRPtz6boOQmLYwt+qCgq0jsYwAQnmE0givc=\ngoogle.golang.org/protobuf v1.27.1/go.mod h1:9q0QmTI4eRPtz6boOQmLYwt+qCgq0jsYwAQnmE0givc=\ngoogle.golang.org/protobuf v1.28.0 h1:w43yiav+6bVFTBQFZX0r7ipe9JQ1QsbMgHwbBziscLw=\ngoogle.golang.org/protobuf v1.28.0/go.mod h1:HV8QOd/L58Z+nl8r43ehVNZIU/HEI6OcFqwMG9pJV4I=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405 h1:yhCVgyC4o1eVCa2tZl7eS0r+SDo693bJlVdllGtEeKM=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/cheggaaa/pb.v1 v1.0.27/go.mod h1:V/YB90LKu/1FcN3WVnfiiE5oMCibMjukxqG/qStrOgw=\ngopkg.in/cheggaaa/pb.v1 v1.0.28 h1:n1tBJnnK2r7g9OW2btFH91V92STTUevLXYFb8gy9EMk=\ngopkg.in/cheggaaa/pb.v1 v1.0.28/go.mod h1:V/YB90LKu/1FcN3WVnfiiE5oMCibMjukxqG/qStrOgw=\ngopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.3/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c h1:dUUwHk2QECo/6vqA44rthZ8ie2QXMNeKRTHCNY2nXvo=\ngopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngorgonia.org/cu v0.9.0-beta/go.mod h1:RPEPIfaxxqUmeRe7T1T8a0NER+KxBI2McoLEXhP1Vd8=\ngorgonia.org/cu v0.9.3/go.mod h1:LgyAYDkN7HWhh8orGnCY2R8pP9PYbO44ivEbLMatkVU=\ngorgonia.org/cu v0.9.4 h1:XTnzfusx/0caMCfG3oJse+LW8SBmReA/613Mo7ZSVQI=\ngorgonia.org/cu v0.9.4/go.mod h1:nR6RAm64n9htu6Orv1NVbsMJXHjnsC3SHPfgcxI08e4=\ngorgonia.org/dawson v1.1.0/go.mod h1:Px1mcziba8YUBIDsbzGwbKJ11uIblv/zkln4jNrZ9Ws=\ngorgonia.org/dawson v1.2.0 h1:hJ/aofhfkReSnJdSMDzypRZ/oWDL1TmeYOauBnXKdFw=\ngorgonia.org/dawson v1.2.0/go.mod h1:Px1mcziba8YUBIDsbzGwbKJ11uIblv/zkln4jNrZ9Ws=\ngorgonia.org/gorgonia v0.9.2/go.mod h1:ZtOb9f/wM2OMta1ISGspQ4roGDgz9d9dKOaPNvGR+ec=\ngorgonia.org/gorgonia v0.9.17/go.mod h1:g66b5Z6ATUdhVqYl2ZAAwblv5hnGW08vNinGLcnrceI=\ngorgonia.org/tensor v0.9.0-beta/go.mod h1:05Y4laKuVlj4qFoZIZW1q/9n1jZkgDBOLmKXZdBLG1w=\ngorgonia.org/tensor v0.9.17/go.mod h1:75SMdLLhZ+2oB0/EE8lFEIt1Caoykdd4bz1mAe59deg=\ngorgonia.org/tensor v0.9.20/go.mod h1:75SMdLLhZ+2oB0/EE8lFEIt1Caoykdd4bz1mAe59deg=\ngorgonia.org/tensor v0.9.23 h1:2yZgu6OCabQ6yVWo3x9Wp/Ny9+DfkmsFOYSbOADY43c=\ngorgonia.org/tensor v0.9.23/go.mod h1:ZaFaLqBTKTzTbTzfnfbW8gDxFP2mXScMzjffUkSsK5Y=\ngorgonia.org/vecf32 v0.7.0/go.mod h1:iHG+kvTMqGYA0SgahfO2k62WRnxmHsqAREGbayRDzy8=\ngorgonia.org/vecf32 v0.9.0 h1:PClazic1r+JVJ1dEzRXgeiVl4g1/Hf/w+wUSqnco1Xg=\ngorgonia.org/vecf32 v0.9.0/go.mod h1:NCc+5D2oxddRL11hd+pCB1PEyXWOyiQxfZ/1wwhOXCA=\ngorgonia.org/vecf64 v0.7.0/go.mod h1:1y4pmcSd+wh3phG+InwWQjYrqwyrtN9h27WLFVQfV1Q=\ngorgonia.org/vecf64 v0.9.0 h1:bgZDP5x0OzBF64PjMGC3EvTdOoMEcmfAh1VCUnZFm1A=\ngorgonia.org/vecf64 v0.9.0/go.mod h1:hp7IOWCnRiVQKON73kkC/AUMtEXyf9kGlVrtPQ9ccVA=\nhonnef.co/go/tools v0.0.0-20190102054323-c2f93a96b099/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.0-20190523083050-ea95bdfd59fc/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.1.3/go.mod h1:NgwopIslSNH47DimFoV78dnkksY2EFtX0ajyb3K/las=\nmodernc.org/cc v1.0.0/go.mod h1:1Sk4//wdnYJiUIxnW8ddKpaOJCF37yAdqYnkxUpaYxw=\nmodernc.org/cc v1.0.1/go.mod h1:uj1/YV+GYVdtSfGOgOtY62Jz8YIiEC0EzZNq481HIQs=\nmodernc.org/fileutil v1.0.0/go.mod h1:JHsWpkrk/CnVV1H/eGlFf85BEpfkrp56ro8nojIq9Q8=\nmodernc.org/golex v1.0.0/go.mod h1:b/QX9oBD/LhixY6NDh+IdGv17hgB+51fET1i2kPSmvk=\nmodernc.org/golex v1.0.1/go.mod h1:QCA53QtsT1NdGkaZZkF5ezFwk4IXh4BGNafAARTC254=\nmodernc.org/internal v1.0.0/go.mod h1:VUD/+JAkhCpvkUitlEOnhpVxCgsBI90oTzSCRcqQVSM=\nmodernc.org/ir v1.0.0/go.mod h1:wxK1nK3PS04CASoUY+HJr+FQywv4+D38y2sRrd71y7s=\nmodernc.org/lex v1.0.0/go.mod h1:G6rxMTy3cH2iA0iXL/HRRv4Znu8MK4higxph/lE7ypk=\nmodernc.org/lexer v1.0.0/go.mod h1:F/Dld0YKYdZCLQ7bD0USbWL4YKCyTDRDHiDTOs0q0vk=\nmodernc.org/mathutil v1.0.0/go.mod h1:wU0vUrJsVWBZ4P6e7xtFJEhFSNsfRLJ8H458uRjg03k=\nmodernc.org/mathutil v1.1.1/go.mod h1:mZW8CKdRPY1v87qxC/wUdX5O1qDzXMP5TH3wjfpga6E=\nmodernc.org/strutil v1.1.0/go.mod h1:lstksw84oURvj9y3tn8lGvRxyRC1S2+g5uuIzNfIOBs=\nmodernc.org/token v1.0.0/go.mod h1:UGzOrNV1mAFSEB63lOFHIpNRUVMvYTc6yu1SMY/XTDM=\nmodernc.org/xc v1.0.0/go.mod h1:mRNCo0bvLjGhHO9WsyuKVU4q0ceiDDDoEeWDJHrNx8I=\nrsc.io/pdf v0.1.1/go.mod h1:n8OzWcQ6Sp37PL01nO98y4iUCRdTGarVfzxY20ICaU4=\n"
        },
        {
          "name": "gorgonia.go",
          "type": "blob",
          "size": 9.271484375,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\n// Functions in this file returns *Node and panics if an error happens\n\n/* Helper functions to create new input nodes */\n\n// Must indicates a node must be created. If there isn't a node created, or there was an error,\n// it subsumes the error, and immediately panics\nfunc Must(n *Node, err error, opts ...NodeConsOpt) *Node {\n\tif err != nil || n == nil {\n\t\tpanic(err)\n\t}\n\treturn n\n}\n\n// NodeFromAny creates a Node from a tensor.Tensor, automatically filling in shape and type info\nfunc NodeFromAny(g *ExprGraph, any interface{}, opts ...NodeConsOpt) *Node {\n\tv, t, dt, err := anyToValue(any)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\topts = append(opts, WithValue(v))\n\n\tswitch t.(type) {\n\tcase tensor.Dtype:\n\t\treturn NewScalar(g, dt, opts...)\n\tcase TensorType:\n\t\topts = append(opts, nil)\n\t\tcopy(opts[1:], opts[0:len(opts)-1])\n\t\topts[0] = WithShape(v.Shape()...)\n\t\treturn NewTensor(g, dt, v.Shape().Dims(), opts...)\n\tdefault:\n\t\tpanic(nyi(\"NewNodeFromAny\", any))\n\t}\n}\n\n// NewScalar creates a Node representing a variable that holds a scalar value\nfunc NewScalar(g *ExprGraph, t tensor.Dtype, opts ...NodeConsOpt) *Node {\n\tcurOpts := []NodeConsOpt{WithType(t), In(g), WithShape()}\n\tcurOpts = append(curOpts, opts...)\n\n\treturn NewUniqueNode(curOpts...)\n}\n\n// NewVector creates a Node representing a variable that holds a vector (nx1 matrix)\nfunc NewVector(g *ExprGraph, t tensor.Dtype, opts ...NodeConsOpt) *Node {\n\ttt := makeTensorType(1, t)\n\tcurOpts := []NodeConsOpt{WithType(tt), In(g)}\n\tcurOpts = append(curOpts, opts...)\n\n\treturn NewUniqueNode(curOpts...)\n}\n\n// NewMatrix creates a Node representing a variable that holds a matrix (nxm)\nfunc NewMatrix(g *ExprGraph, t tensor.Dtype, opts ...NodeConsOpt) *Node {\n\ttt := makeTensorType(2, t)\n\tcurOpts := []NodeConsOpt{WithType(tt), In(g)}\n\tcurOpts = append(curOpts, opts...)\n\n\treturn NewUniqueNode(curOpts...)\n}\n\n// NewTensor creates a Node representing a variable that holds a tensor (any n-dimensional array with dimensions greater than 2)\nfunc NewTensor(g *ExprGraph, t tensor.Dtype, dims int, opts ...NodeConsOpt) *Node {\n\tvar tt hm.Type\n\tif dims == 0 {\n\t\ttt = t\n\t} else {\n\t\ttt = makeTensorType(dims, t)\n\t}\n\tcurOpts := []NodeConsOpt{WithType(tt), In(g)}\n\tcurOpts = append(curOpts, opts...)\n\n\treturn NewUniqueNode(curOpts...)\n}\n\n// NewConstant takes in any reasonable value and makes it a constant node.\nfunc NewConstant(v interface{}, opts ...NodeConsOpt) *Node {\n\tvar op Op\n\tvar t hm.Type\n\tvar name string\n\tvar s tensor.Shape\n\tvar val Value\n\n\tval, t, _, err := anyToValue(v)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tswitch vt := val.(type) {\n\tcase Scalar:\n\t\top = constantScalar{vt}\n\t\ts = scalarShape\n\tcase tensor.Tensor:\n\t\top = constantTensor{vt}\n\t\ts = vt.Shape()\n\t}\n\n\tif op == nil || t == nil {\n\t\tpanic(fmt.Sprintf(\"HELP. Op: %v, t: %v\", op, t))\n\t}\n\n\tdummy := borrowNode()\n\tconsOpts := []NodeConsOpt{WithOp(op), WithType(t), WithShape(s...), WithValue(val)}\n\tconsOpts = append(consOpts, opts...)\n\tfor i := range opts {\n\t\topts[i](dummy)\n\t}\n\tif dummy.name == \"\" {\n\t\tname = fmt.Sprintf(\"%v\", v)\n\t} else {\n\t\tname = dummy.name\n\t}\n\treturnNode(dummy)\n\n\tconsOpts = append(consOpts, WithName(name))\n\treturn newNode(consOpts...)\n}\n\n// UniformRandomNode creates an input node that has a random op so everytime the node is passed, random values will be plucked from\n// a uniform distribution. The type of the node depends on the\n// shape passed in. To get a scalar value at run time, don't pass in any shapes\nfunc UniformRandomNode(g *ExprGraph, dt tensor.Dtype, low, high float64, shape ...int) *Node {\n\top := makeRandomOp(uniform, dt, low, high, shape...)\n\ts := tensor.Shape(shape)\n\n\tvar t hm.Type\n\tif s.Eq(scalarShape) {\n\t\tt = dt\n\t} else {\n\t\tt = makeTensorType(s.Dims(), dt)\n\t}\n\n\tretVal := NewUniqueNode(WithType(t), WithOp(op), In(g), WithShape(shape...))\n\treturn retVal\n}\n\n// GaussianRandomNode creates an input node that has a random op so everytime the node is passed, random values will be plucked from\n// a gaussian distribution with the mean and stdev provided. The type of the node depends on the\n// shape passed in. To get a scalar value at run time, don't pass in any shapes\nfunc GaussianRandomNode(g *ExprGraph, dt tensor.Dtype, mean, stdev float64, shape ...int) *Node {\n\top := makeRandomOp(gaussian, dt, mean, stdev, shape...)\n\ts := tensor.Shape(shape)\n\n\tvar t hm.Type\n\tif s.Eq(scalarShape) {\n\t\tt = dt\n\t} else {\n\t\tt = makeTensorType(s.Dims(), dt)\n\t}\n\n\tretVal := NewUniqueNode(WithType(t), WithOp(op), In(g), WithShape(shape...))\n\treturn retVal\n}\n\n// BinomialRandomNode creates an input node that has a random op so that everytime the node is passed, random values will be plucked from\n// a binomial distribution with the mean and stdev provided. The type of the node depends on the\n// shape passed in. To get a scalar value at run time, don't pass in any shapes\n//\n// Whilst technically the number of trials of a binomal distribution should be a discrete value (you can't have half a trial), to keep with\n// API uniformity, trials is passed in as a float64, but will be truncated to an int at runtime.\nfunc BinomialRandomNode(g *ExprGraph, dt tensor.Dtype, trials, prob float64, shape ...int) *Node {\n\top := makeRandomOp(binomial, dt, trials, prob, shape...)\n\ts := tensor.Shape(shape)\n\n\tvar t hm.Type\n\tif s.Eq(scalarShape) {\n\t\tt = dt\n\t} else {\n\t\tt = makeTensorType(s.Dims(), dt)\n\t}\n\n\tretVal := NewUniqueNode(WithType(t), WithOp(op), In(g), WithShape(shape...))\n\treturn retVal\n}\n\n// OneHotVector creates a node representing a one hot vector\nfunc OneHotVector(id, classes int, t tensor.Dtype, opts ...NodeConsOpt) *Node {\n\tT := tensor.New(tensor.Of(t), tensor.WithShape(classes))\n\tvar err error\n\t// This is stupid, I want generics. - docmerlin\n\tswitch t {\n\tcase tensor.Float32:\n\t\terr = T.SetAt(float32(1), id)\n\tcase tensor.Float64:\n\t\terr = T.SetAt(float64(1), id)\n\tcase tensor.Int64:\n\t\terr = T.SetAt(int64(1), id)\n\tcase tensor.Int:\n\t\terr = T.SetAt(int(1), id)\n\tcase tensor.Int32:\n\t\terr = T.SetAt(int32(1), id)\n\tdefault:\n\t\tpanic(\"tensor.Dtype not implemented\")\n\t}\n\tif err != nil {\n\t\tpanic(err.Error())\n\t}\n\treturn NewConstant(T, opts...)\n}\n\n// Grad takes a scalar cost node and a list of with-regards-to, and returns the gradient\nfunc Grad(cost *Node, WRTs ...*Node) (retVal Nodes, err error) {\n\tsymdiffLogf(\"Cost:%v\", cost)\n\tif !cost.IsScalar() {\n\t\treturn nil, errors.Errorf(\"Expected Cost to be a scalar. Got %v instead\", cost)\n\t}\n\n\tfor i, n := range WRTs {\n\t\tif !n.isInput() {\n\t\t\terr = errors.Errorf(\"Can only differentiate with regards to input nodes. %dth Node %v isn't an input\", i, n)\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tvar dt tensor.Dtype\n\tvar ok bool\n\tif dt, ok = cost.t.(tensor.Dtype); !ok {\n\t\terr = errors.Wrap(err, \"Expected a scalar dtype for cost\")\n\t\treturn\n\t}\n\n\tvar gradOut *Node\n\tswitch dt {\n\tcase Float64:\n\t\tgradOut = onef64\n\tcase Float32:\n\t\tgradOut = onef32\n\tdefault:\n\t\treturn nil, errors.Wrapf(err, \"%s not yet implemented for %v of %T\", dt.String(), \"Grad()'s gradOut\", gradOut)\n\t}\n\n\tgradOut = cost.g.AddNode(gradOut)\n\treturn Backpropagate(Nodes{cost}, Nodes{gradOut}, Nodes(WRTs))\n}\n\n// Let binds a Value to a node that is a variable. A variable is represented as a *Node with no Op.\n// It is equivalent to :\n//\t\tx = 2\nfunc Let(n *Node, be interface{}) error {\n\tif !n.isInput() {\n\t\treturn errors.New(\"Cannot bind a value to a non input node\")\n\t}\n\n\treturn UnsafeLet(n, be)\n}\n\n// UnsafeLet binds a Value to any node, not just a variable node. This means that you can use it to change any node's value at the runtime of the graph. UNSAFE!\n//\n// Additional notes: if `be` is a tensor.Slice, and the node's op is a sliceOp or sliceIncrOp, the op's slice will be replaced with the new slice.\nfunc UnsafeLet(n *Node, be interface{}) error {\n\tswitch v := be.(type) {\n\tcase tensor.Slice:\n\t\tswitch so := n.op.(type) {\n\t\tcase *sliceOp:\n\t\t\tso.Slice = v\n\t\t\tn.op = so\n\t\tcase sliceIncrOp:\n\t\t\tso.Slice = v\n\t\t\tn.op = so\n\t\tdefault:\n\t\t\treturn errors.Errorf(\"Trying to Let() a node with a slice. Node's op is %v, not sliceOp\", n.op)\n\t\t}\n\n\tcase Value:\n\t\tif !n.Shape().Eq(v.Shape()) {\n\t\t\treturn fmt.Errorf(\"Node's expected shape is %v. Got %v instead\", n.Shape(), v.Shape())\n\t\t}\n\n\t\tif !n.Dtype().Eq(v.Dtype()) {\n\t\t\treturn errors.Errorf(\"Unable to let %v be %v. Expected Dtype of %v. Got %v instead\", n.name, be, n.Dtype(), v.Dtype())\n\t\t}\n\t\tn.bind(v)\n\tcase *Node:\n\t\t// Case of letOp\n\t\tif _, ok := n.op.(letOp); ok {\n\n\t\t}\n\tdefault:\n\t\tvar val Value\n\t\tvar err error\n\t\tif val, _, _, err = anyToValue(be); err != nil {\n\t\t\treturn errors.Wrapf(err, anyToValueFail, be, be)\n\t\t}\n\n\t\tn.bind(val)\n\t}\n\treturn nil\n}\n\n// Set is the equivalent of doing this:\n//\t\ta = b\n// where a and b are both variables\nfunc Set(a, b *Node) (retVal *Node) {\n\top := letOp{}\n\tname := fmt.Sprintf(\"%v %s %v\", a, op, b)\n\treturn NewUniqueNode(WithOp(op), WithChildren(Nodes{a, b}), WithName(name), In(a.g))\n}\n\n// Read allows for extraction of the value of the *Node at runtime into a Value.\n// To achieve this, a pointer to a Value (*Value) is passed into this function, not a Value.\n// The 'into' value remains nil until the execution of the graph (via a call to the Run() methods of the VM)\nfunc Read(n *Node, into *Value) (retVal *Node) {\n\top := readOp{into}\n\tname := fmt.Sprintf(\"read %v into %v\", n, into)\n\tretVal = NewUniqueNode(WithOp(op), WithChildren(Nodes{n}), WithName(name), In(n.g))\n\tretVal.op = op // this ensures the correct pointer is written\n\tretVal.name = name\n\treturn\n}\n"
        },
        {
          "name": "gorgonia_test.go",
          "type": "blob",
          "size": 4.2666015625,
          "content": "package gorgonia\n\nimport (\n\t\"log\"\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n\t\"gorgonia.org/tensor\"\n\tnd \"gorgonia.org/tensor\"\n)\n\nfunc TestNewConstant(t *testing.T) {\n\tassert := assert.New(t)\n\n\tvar expectedType hm.Type\n\n\tt.Log(\"Testing New Constant Tensors\")\n\tbacking := nd.Random(Float64, 9)\n\tT := nd.New(nd.WithBacking(backing), nd.WithShape(3, 3))\n\n\tct := NewConstant(T)\n\texpectedTT := makeTensorType(2, Float64)\n\texpectedType = expectedTT\n\n\tassert.Equal(nd.Shape{3, 3}, ct.shape)\n\tassert.Equal(expectedType, ct.t)\n\n\tct = NewConstant(T, WithName(\"From TensorValue\"))\n\tassert.Equal(nd.Shape{3, 3}, ct.shape)\n\tassert.Equal(expectedType, ct.t)\n\tassert.Equal(\"From TensorValue\", ct.name)\n\n\tt.Log(\"Testing Constant Scalars\")\n\tcs := NewConstant(3.14)\n\texpectedType = Float64\n\tassert.Equal(scalarShape, cs.shape)\n\tassert.Equal(expectedType, cs.t)\n}\n\nvar anyNodeTest = []struct {\n\tname string\n\tany  interface{}\n\n\tcorrectType  hm.Type\n\tcorrectShape nd.Shape\n}{\n\t{\"float32\", float32(3.14), Float32, scalarShape},\n\t{\"float64\", float64(3.14), Float64, scalarShape},\n\t{\"int\", int(3), Int, scalarShape},\n\t{\"bool\", true, Bool, scalarShape},\n\t{\"nd.Tensor\", nd.New(nd.Of(nd.Float64), nd.WithShape(2, 3, 4)), &TensorType{Dims: 3, Of: Float64}, nd.Shape{2, 3, 4}},\n\t{\"nd.Tensor\", nd.New(nd.Of(nd.Float32), nd.WithShape(2, 3, 4)), &TensorType{Dims: 3, Of: Float32}, nd.Shape{2, 3, 4}},\n\t{\"ScalarValue\", NewF64(3.14), Float64, scalarShape},\n\t{\"TensorValue\", nd.New(nd.Of(nd.Float64), nd.WithShape(2, 3)), &TensorType{Dims: 2, Of: Float64}, nd.Shape{2, 3}},\n}\n\nfunc TestNodeFromAny(t *testing.T) {\n\tassert := assert.New(t)\n\tg := NewGraph()\n\tfor _, a := range anyNodeTest {\n\t\tn := NodeFromAny(g, a.any, WithName(a.name))\n\t\tassert.Equal(a.name, n.name)\n\t\tassert.Equal(g, n.g)\n\t\tassert.True(a.correctType.Eq(n.t), \"%v type error: Want %v. Got %v\", a.name, a.correctType, n.t)\n\t\tassert.True(a.correctShape.Eq(n.shape), \"%v shape error: Want %v. Got %v\", a.name, a.correctShape, n.shape)\n\t}\n}\n\nfunc TestOneHotVector(t *testing.T) {\n\tassert := assert.New(t)\n\tassert.EqualValues(\n\t\t[]float32{0, 0, 0, 0, 0, 0, 1, 0, 0, 0},\n\t\tOneHotVector(6, 10, nd.Float32).Value().Data())\n\tassert.EqualValues(\n\t\t[]float32{0, 1, 0, 0, 0},\n\t\tOneHotVector(1, 5, nd.Float32).Value().Data())\n\tassert.EqualValues(\n\t\t[]float32{0, 1, 0, 0, 0, 0},\n\t\tOneHotVector(1, 6, nd.Float32).Value().Data())\n\tassert.EqualValues(\n\t\t[]int{0, 0, 0, 1, 0},\n\t\tOneHotVector(3, 5, nd.Int).Value().Data())\n\tassert.EqualValues(\n\t\t[]int32{0, 0, 0, 0, 0, 0, 1, 0, 0, 0},\n\t\tOneHotVector(6, 10, nd.Int32).Value().Data())\n\tassert.EqualValues(\n\t\t[]float64{0, 1, 0, 0, 0},\n\t\tOneHotVector(1, 5, nd.Float64).Value().Data())\n\tassert.EqualValues(\n\t\t[]int64{0, 1, 0, 0, 0, 0},\n\t\tOneHotVector(1, 6, nd.Int64).Value().Data())\n}\n\nfunc TestRandomNodeBackprop(t *testing.T) {\n\tg := NewGraph()\n\ta := NewVector(g, Float64, WithShape(10), WithName(\"a\"), WithInit(Zeroes()))\n\tb := GaussianRandomNode(g, Float64, 0, 1, 10)\n\tc := Must(Add(a, b))\n\td := Must(Sum(c))\n\tvm := NewLispMachine(g, WithLogger(log.New(os.Stderr, \"\", 0)))\n\tvm.RunAll()\n\tt.Logf(\"d.Value %v\", d.Value())\n}\n\nfunc TestLetErrors(t *testing.T) {\n\tg := NewGraph()\n\n\ttestCases := []struct {\n\t\tdesc string\n\t\tnode *Node\n\t\tval  interface{}\n\t\terr  string\n\t}{\n\t\t{\n\t\t\tdesc: \"DifferentShapes\",\n\t\t\tnode: NewTensor(g, tensor.Float64, 2, WithShape(1, 1), WithInit(GlorotN(1.0)), WithName(\"x\")),\n\t\t\tval:  tensor.New(tensor.WithShape(1, 1, 1), tensor.WithBacking([]float64{0.5})),\n\t\t\terr:  \"Node's expected shape is (1, 1). Got (1, 1, 1) instead\",\n\t\t},\n\t\t{\n\t\t\tdesc: \"AssigningConst\",\n\t\t\tnode: NewConstant(2, WithName(\"x\")),\n\t\t\tval:  tensor.New(tensor.WithShape(1, 1), tensor.WithBacking([]float64{0.5})),\n\t\t\terr:  \"Cannot bind a value to a non input node\",\n\t\t},\n\t}\n\n\tfor _, tC := range testCases {\n\t\tt.Run(tC.desc, func(t *testing.T) {\n\t\t\terr := Let(tC.node, tC.val)\n\t\t\tif tC.err != \"\" {\n\t\t\t\trequire.Error(t, err)\n\t\t\t\tassert.Equal(t, tC.err, err.Error())\n\t\t\t} else {\n\t\t\t\trequire.NoError(t, err)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestRead(t *testing.T) {\n\tg := NewGraph()\n\txVal := tensor.New(tensor.WithShape(2, 4), tensor.WithBacking(tensor.Range(tensor.Float64, 0, 8)))\n\tx := NodeFromAny(g, xVal, WithName(\"x\"))\n\n\tvar v1, v2 Value\n\tr1 := Read(x, &v1)\n\tr2 := Read(x, &v2)\n\tr3 := Read(x, &v1)\n\n\tassert.Equal(t, r1, r3)\n\tassert.NotEqual(t, r1, r2)\n}\n"
        },
        {
          "name": "graph.go",
          "type": "blob",
          "size": 17.6376953125,
          "content": "package gorgonia\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\n\t\"github.com/awalterschulze/gographviz\"\n\t\"gonum.org/v1/gonum/graph\"\n\t\"gonum.org/v1/gonum/graph/iterator\"\n)\n\n// ExprGraph is a data structure for a directed acyclic graph (of expressions). This structure is the main entry point\n// for Gorgonia.\ntype ExprGraph struct {\n\tname string\n\n\tall Nodes\n\n\tbyID   map[int64]int\n\tbyHash map[uint32]*Node\n\tevac   map[uint32]Nodes\n\tto     map[*Node]Nodes\n\n\tleaves    Nodes\n\tconstants Nodes\n\troots     Nodes\n\tcounter   uint\n}\n\n// graphconopt sets options\ntype graphconopt func(g *ExprGraph)\n\n// WithGraphName is a ExprGraph construction option that provides a name.\nfunc WithGraphName(name string) graphconopt {\n\tf := func(g *ExprGraph) {\n\t\tg.name = name\n\t}\n\treturn f\n}\n\n// NewGraph creates a new graph. Duh\nfunc NewGraph(opts ...graphconopt) *ExprGraph {\n\tg := &ExprGraph{\n\t\tbyID:   make(map[int64]int),\n\t\tbyHash: make(map[uint32]*Node),\n\t\tevac:   make(map[uint32]Nodes),\n\t\tto:     make(map[*Node]Nodes),\n\n\t\tleaves:    make(Nodes, 0, 64),\n\t\tconstants: make(Nodes, 0, 8),\n\t}\n\n\tfor _, opt := range opts {\n\t\topt(g)\n\t}\n\n\treturn g\n}\n\n// Clone clones the graph. All nodes gets cloned, and their values are cloned as well.\nfunc (g *ExprGraph) Clone() interface{} {\n\tg2 := new(ExprGraph)\n\tg2.name = g.name\n\n\tmapping := make(map[*Node]*Node) // a map of old nodes to new nodes\n\tg2.all = make(Nodes, len(g.all))\n\tfor i, n := range g.all {\n\t\tcloned := n.Clone().(*Node)\n\t\tcloned.g = g2\n\t\tcloned.id = n.id\n\n\t\tg2.all[i] = cloned\n\t\tmapping[n] = cloned\n\t}\n\n\t// handle each node's children, deriv ofs, etc\n\tfor i, n := range g.all {\n\t\tcloned := g2.all[i]\n\t\tcloned.children = make(Nodes, len(n.children))\n\t\tfor j, c := range n.children {\n\t\t\tcloned.children[j] = mapping[c]\n\t\t}\n\n\t\tcloned.derivOf = make(Nodes, len(n.derivOf))\n\t\tfor j, c := range n.derivOf {\n\t\t\tcloned.derivOf[j] = mapping[c]\n\t\t}\n\n\t\tif n.deriv != nil {\n\t\t\tcloned.deriv = mapping[n.deriv]\n\t\t}\n\t}\n\n\tg2.byID = make(map[int64]int)\n\tg2.byHash = make(map[uint32]*Node)\n\tfor k, v := range g.byHash {\n\t\tg2.byHash[k] = mapping[v]\n\t}\n\n\tg2.evac = make(map[uint32]Nodes)\n\tfor k, v := range g.evac {\n\t\tg2.evac[k] = make(Nodes, len(v))\n\t\tfor i, n := range v {\n\t\t\tg2.evac[k][i] = mapping[n]\n\t\t}\n\t}\n\n\tg2.to = make(map[*Node]Nodes)\n\tfor k, v := range g.to {\n\t\tto := mapping[k]\n\t\tg2.to[to] = make(Nodes, len(v))\n\t\tfor i, n := range v {\n\t\t\tg2.to[to][i] = mapping[n]\n\t\t}\n\t}\n\n\tg2.leaves = make(Nodes, len(g.leaves))\n\tfor i, n := range g.leaves {\n\t\tg2.leaves[i] = mapping[n]\n\t}\n\n\tg2.constants = make(Nodes, len(g.constants))\n\tfor i, n := range g.constants {\n\t\tg2.constants[i] = mapping[n]\n\t}\n\n\tg2.roots = make(Nodes, len(g.roots))\n\tfor i, n := range g.roots {\n\t\tg2.roots[i] = mapping[n]\n\t}\n\n\tg2.counter = g.counter\n\treturn g2\n}\n\n// AddNode adds n to the graph. It panics if the added node ID matches an existing node ID.\nfunc (g *ExprGraph) AddNode(n *Node) (retVal *Node) {\n\tdefer func() {\n\t\tif _, ok := g.to[retVal]; !ok {\n\t\t\tg.to[retVal] = nil\n\t\t}\n\t}()\n\t// check for node with the same name in the graph\n\t// we don't update the graph if this is the case\n\tfor _, node := range g.constants {\n\t\tif node.name == n.name && n.isConstant() {\n\t\t\treturn node\n\t\t}\n\t}\n\thash := n.Hashcode()\n\tif existing, ok := g.byHash[hash]; ok {\n\t\tif existing == nil {\n\t\t\t// this means that there has been previous collisions\n\t\t\t// so look at evac map\n\t\t\tfor _, e := range g.evac[hash] {\n\t\t\t\tif nodeEq(n, e) {\n\t\t\t\t\treturn e\n\t\t\t\t}\n\t\t\t}\n\t\t\tg.evac[hash] = append(g.evac[hash], n)\n\t\t\tg.addToAll(n)\n\t\t\tincrCC() // collision counter\n\t\t\treturn n\n\t\t}\n\n\t\tif !nodeEq(n, existing) {\n\t\t\tg.evac[hash] = Nodes{existing, n}\n\t\t\tg.byHash[hash] = nil // to signal that it's collided\n\t\t\tg.addToAll(n)\n\t\t\tincrCC()\n\t\t\treturn n\n\t\t}\n\t\tincrEC() // expected collision (they're the same node!)\n\t\treturn existing\n\t}\n\n\tif n.isConstant() {\n\t\tn = n.clone()\n\t\tg.constants = g.constants.Add(n)\n\t\tn.g = g\n\t}\n\n\tg.addToAll(n)\n\tg.byHash[hash] = n\n\treturn n\n}\n\nfunc (g *ExprGraph) addToAll(n *Node) {\n\tif n == nil {\n\t\tpanic(\"HELP! trying to add nil\")\n\t}\n\tg.all = append(g.all, n)\n\tn.id = int64(g.counter)\n\tg.counter++\n}\n\n// RemoveNode removes n from the graph, as well as any edges attached to it. If the node\n// is not in the graph it is a no-op.\nfunc (g *ExprGraph) RemoveNode(node graph.Node) {\n\tn := node.(*Node)\n\tif n.id == -1 {\n\t\treturn // if it's -1, it was never in the graph to begin with\n\t}\n\n\thash := n.Hashcode()\n\n\tdelete(g.byHash, hash)\n\tdelete(g.to, n)\n\tg.evac[hash] = g.evac[hash].remove(n)\n\tg.all = g.all.remove(n)\n}\n\n// SetEdge adds e, an edge from one node to another. If the nodes do not exist, they are added.\n// It will panic if the IDs of the e.From and e.To are equal.\nfunc (g *ExprGraph) SetEdge(e graph.Edge) {\n\tfrom := e.From().(*Node)\n\tto := e.To().(*Node)\n\n\tif from == to {\n\t\tpanic(fmt.Sprintf(\"cannot add self edge: from %v to %v\", from, to))\n\t}\n\n\tif !g.Has(from.ID()) {\n\t\tfrom = g.AddNode(from)\n\t}\n\n\tif !g.Has(to.ID()) {\n\t\tto = g.AddNode(to)\n\t}\n\n\t// g.to[to] = g.to[to].Add(from)\n\tg.to[to] = append(g.to[to], from)\n}\n\n// Roots returns a list of nodes that are not children of any other nodes\nfunc (g *ExprGraph) Roots() (retVal Nodes) {\n\t// handle subgraph\n\tif g.roots != nil {\n\t\treturn g.roots\n\t}\n\n\tfor n, tos := range g.to {\n\t\tif len(tos) == 0 {\n\t\t\tretVal = append(retVal, n)\n\t\t}\n\t\t// if the root is a statement (typically a read), and it only has one child\n\t\tif len(n.children) == 1 && n.isStmt {\n\t\t\tchild := n.children[0]\n\t\t\tif len(g.to[child]) == 1 {\n\t\t\t\tretVal = append(retVal, child)\n\t\t\t}\n\t\t}\n\t}\n\tg.roots = retVal\n\treturn retVal\n}\n\n// Inputs returns a list of nodes which are inputs (that is to say, the user is required to set a value in it)\nfunc (g *ExprGraph) Inputs() (retVal Nodes) {\n\tfor _, n := range g.all {\n\t\tif n.isInput() {\n\t\t\tretVal = append(retVal, n)\n\t\t}\n\t}\n\treturn\n}\n\n// UnbindAll unbinds all the values from the nodes\nfunc (g *ExprGraph) UnbindAll() {\n\tfor _, n := range g.all {\n\t\tn.unbind()\n\t}\n}\n\n// UnbindAllNonInputs unbinds all the values from nodes that aren't input nodes\nfunc (g *ExprGraph) UnbindAllNonInputs() {\n\tfor _, n := range g.all {\n\t\tif n.isInput() || n.isConstant() {\n\t\t\tcontinue\n\t\t}\n\t\tn.unbind()\n\t}\n}\n\n// ByName returns nodes that have the name provided.\n// Bear in mind that the name that is compared to is the internal name,\n// not the result of calling node.Name(). The reason for doing this is\n// for ease of finding only names that are user-supplied, instead of autogenerated names\nfunc (g *ExprGraph) ByName(name string) (retVal Nodes) {\n\tfor _, n := range g.all {\n\t\tif n.name == name {\n\t\t\tretVal = append(retVal, n)\n\t\t}\n\t}\n\treturn\n}\n\n// Constant returns a constant that may be found in the graph. If no constant were found, a new one is created instead\nfunc (g *ExprGraph) Constant(v Value) *Node {\n\tfor _, n := range g.constants {\n\t\tif ValueEq(n.Value(), v) {\n\t\t\treturn n\n\t\t}\n\t}\n\n\tn := NewConstant(v)\n\treturn g.AddNode(n)\n}\n\nfunc (g *ExprGraph) String() string {\n\tvar buf bytes.Buffer\n\tbuf.WriteString(\"Graph: [\\n\")\n\tfor _, n := range g.byHash {\n\t\tfmt.Fprintf(&buf, \"\\t%d: %s\\n\", n.Hashcode(), n)\n\t}\n\tbuf.WriteString(\"]\")\n\treturn buf.String()\n}\n\n// ToDot generates the graph in graphviz format. The use of this is to generate for the entire graph\n// which may have multiple trees with different roots\n// TODO: This is getting unwieldy. Perhaps refactor out into a ToDot(...Opt)?\nfunc (g *ExprGraph) ToDot() string {\n\tgv := gographviz.NewEscape()\n\tgv.SetName(fullGraphName)\n\tgv.SetDir(true)\n\n\tgv.AddAttr(fullGraphName, \"nodesep\", \"1\")\n\tgv.AddAttr(fullGraphName, \"ranksep\", \"1.5 equally\")\n\tgv.AddAttr(fullGraphName, \"rankdir\", \"TB\")\n\tif len(g.byHash) > 100 {\n\t\tgv.AddAttr(fullGraphName, \"nslimit\", \"3\") // numiter=3*len(nodes)\n\t\t// gv.AddAttr(fullGraphName, \"splines\", \"line\") // ugly as sin.\n\t}\n\n\tgroups := make(map[string]struct{})\n\tfor h, n := range g.byHash {\n\t\tif n != nil {\n\t\t\tgroup := n.dotCluster()\n\t\t\tgroups[group] = struct{}{}\n\t\t\tcontinue\n\t\t}\n\t\t// other wise it'se a clash of hash\n\t\tfor _, n := range g.evac[h] {\n\t\t\tgroup := n.dotCluster()\n\t\t\tgroups[group] = struct{}{}\n\n\t\t}\n\t}\n\n\tfor grp := range groups {\n\t\tattrs := map[string]string{\"label\": grp}\n\n\t\tparentGraph := fullGraphName\n\t\tif grp == inputsClust || grp == constantsClust {\n\t\t\tparentGraph = inputConsts\n\t\t\tif !gv.IsSubGraph(inputConsts) {\n\t\t\t\tgroupAttrs := map[string]string{\"rank\": \"max\"}\n\t\t\t\tgv.AddSubGraph(fullGraphName, inputConsts, groupAttrs)\n\t\t\t}\n\t\t}\n\t\tgv.AddSubGraph(parentGraph, \"cluster_\"+grp, attrs)\n\t}\n\n\t// for _, n := range g.byHash {\n\tfor _, n := range g.all {\n\t\tgroup := n.dotCluster()\n\t\tn.dotString(gv, \"cluster_\"+group)\n\t}\n\n\t// for _, from := range g.byHash {\n\tfor _, from := range g.all {\n\t\tfor i, child := range from.children {\n\t\t\tif ok := g.all.Contains(child); !ok {\n\t\t\t\t// not in graph, so ignore it...\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfromID := fmt.Sprintf(\"Node_%p\", from)\n\t\t\ttoID := fmt.Sprintf(\"Node_%p\", child)\n\n\t\t\tedgeAttrs := map[string]string{\n\t\t\t\t\"taillabel\":  fmt.Sprintf(\" %d \", i),\n\t\t\t\t\"labelfloat\": \"false\",\n\t\t\t}\n\n\t\t\t// we invert the from and to nodes for gradients, As the expressionGraph builds upwards from bottom, the gradient builds downwards.\n\t\t\tif from.group == gradClust && child.group == gradClust {\n\t\t\t\tedgeAttrs[\"dir\"] = \"back\"\n\t\t\t\tgv.AddPortEdge(toID, toID+\":anchor:s\", fromID, fromID+\":anchor:n\", true, edgeAttrs)\n\t\t\t} else {\n\t\t\t\tgv.AddPortEdge(fromID, fromID+\":anchor:s\", toID, toID+\":anchor:n\", true, edgeAttrs)\n\t\t\t}\n\t\t}\n\t}\n\n\t// draw deriv lines\n\tif debugDerives {\n\t\tedgeAttrs := map[string]string{\n\t\t\t\"style\":      \"dashed\",\n\t\t\t\"constraint\": \"false\",\n\t\t\t\"weight\":     \"999\",\n\t\t}\n\n\t\tfor _, n := range g.byHash {\n\t\t\tif n == nil {\n\t\t\t\t// collision found... what to do?\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif n.derivOf != nil {\n\t\t\t\tid := fmt.Sprintf(\"Node_%p\", n)\n\t\t\t\tfor _, derivOf := range n.derivOf {\n\t\t\t\t\tif _, ok := g.to[derivOf]; !ok {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tofID := fmt.Sprintf(\"Node_%p\", derivOf)\n\t\t\t\t\t// gv.AddPortEdge(id, \":anchor:w\", ofID, ofID+\":anchor:e\", true, edgeAttrs)\n\t\t\t\t\tgv.AddEdge(id, ofID, true, edgeAttrs)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// stupid invisible nodes to keep expressiongraph on the left\n\tsubGAttrs := make(map[string]string)\n\t// subGAttrs.Add(\"rank\", \"max\")\n\tgv.AddSubGraph(fullGraphName, outsideSubG, subGAttrs)\n\n\tattrs := map[string]string{\n\t\t\"style\": \"invis\",\n\t}\n\tgv.AddNode(outsideSubG, outsideRoot, attrs)\n\n\toutsides := []string{outsideRoot}\n\tvar insides []string\n\n\t// build the inside and outside list\n\tif _, hasInputs := groups[inputsClust]; hasInputs {\n\t\tinsides = append(insides, insideInputs)\n\t\tgv.AddNode(\"cluster_inputs\", insideInputs, attrs)\n\t}\n\n\tif _, hasConst := groups[constantsClust]; hasConst {\n\t\tif len(insides) > 0 {\n\t\t\toutsides = append(outsides, outsideConsts)\n\t\t\tgv.AddNode(outsideSubG, outsideConsts, attrs)\n\t\t}\n\t\tinsides = append(insides, insideConsts)\n\t\tgv.AddNode(\"cluster_constants\", insideConsts, attrs)\n\t}\n\n\tif len(insides) > 0 {\n\t\toutsides = append(outsides, outsideExprG)\n\t\tgv.AddNode(outsideSubG, outsideExprG, attrs)\n\t}\n\tinsides = append(insides, insideExprG)\n\tgv.AddNode(\"cluster_expressionGraph\", insideExprG, attrs)\n\n\tfor group := range groups {\n\t\tif group == exprgraphClust || group == constantsClust || group == inputsClust {\n\t\t\tcontinue\n\t\t}\n\t\tinside := \"inside_\" + group\n\t\toutside := \"outside_\" + group\n\t\tinsides = append(insides, inside)\n\t\toutsides = append(outsides, outside)\n\n\t\tgv.AddNode(outsideSubG, outside, attrs)\n\t\tgv.AddNode(\"cluster_\"+group, inside, attrs)\n\t}\n\n\tedgeAttrs := map[string]string{\n\t\t\"style\":      \"invis\",\n\t\t\"weight\":     \"999\",\n\t\t\"constraint\": \"false\",\n\t}\n\tfor i, o := range outsides {\n\t\t// outside-inside\n\t\tgv.AddEdge(o, insides[i], true, edgeAttrs)\n\n\t\tif i > 0 {\n\t\t\t// outside-outside\n\t\t\tgv.AddEdge(outsides[i-1], o, true, edgeAttrs)\n\n\t\t\t// inside-inside\n\t\t\tgv.AddEdge(insides[i-1], insides[i], true, edgeAttrs)\n\t\t}\n\t}\n\treturn gv.String()\n}\n\n// Edges returns all the edges in the graph.\nfunc (g *ExprGraph) Edges() graph.Edges {\n\tvar edges []graph.Edge\n\tfor _, n := range g.all {\n\t\tfor _, toN := range g.to[n] {\n\t\t\tedges = append(edges, edge{\n\t\t\t\tfrom: n,\n\t\t\t\tto:   toN,\n\t\t\t})\n\t\t}\n\t}\n\tif len(edges) == 0 {\n\t\treturn graph.Empty\n\t}\n\treturn iterator.NewOrderedEdges(edges)\n}\n\n// other private methods\nfunc (g *ExprGraph) removeAllEdgesFrom(n *Node) {\n\tfor k, ns := range g.to {\n\t\tg.to[k] = ns.remove(n)\n\t}\n}\n\n/* Graph interface */\n\n// Node returns the node in the graph with the given ID.\nfunc (g *ExprGraph) Node(id int64) graph.Node {\n\t// n := (*Node)(unsafe.Pointer(uintptr(id)))\n\t// for _, n := range g.all {\n\t// \tif n.id == id {\n\t// \t\treturn n\n\t// \t}\n\t// }\n\t// return nil\n\treturn g.node(id)\n}\n\nfunc (g *ExprGraph) node(id int64) *Node {\n\tif idx, ok := g.byID[id]; ok {\n\t\tif idx >= len(g.all) {\n\t\t\treturn nil\n\t\t}\n\t\treturn g.all[idx]\n\t}\n\tfor i, n := range g.all {\n\t\tif n.id == id {\n\t\t\tg.byID[id] = i\n\t\t\treturn n\n\t\t}\n\t}\n\treturn nil\n}\n\n// Has returns whether the node exists within the graph.\nfunc (g *ExprGraph) Has(nodeid int64) bool {\n\tn := g.node(nodeid)\n\treturn n != nil\n}\n\n// Nodes returns all the nodes in the graph.\nfunc (g *ExprGraph) Nodes() graph.Nodes {\n\t// nodes := make([]graph.Node, len(g.from))\n\tns := g.AllNodes()\n\n\treturn nodeToGraphNode(ns)\n}\n\n// AllNodes is like Nodes, but returns Nodes instead of []graph.Node.\n// Nodes() has been reserved for the graph.Directed interface, so this one is named AllNodes instead\nfunc (g *ExprGraph) AllNodes() Nodes { return g.all }\n\n// From returns all nodes in g that can be reached directly from n.\nfunc (g *ExprGraph) From(nodeid int64) graph.Nodes {\n\tif n := g.node(nodeid); n != nil {\n\t\treturn nodeToGraphNode(n.children)\n\t}\n\treturn nil\n}\n\n// HasEdgeBetween returns whether an edge exists between nodes x and y without\n// considering direction.\nfunc (g *ExprGraph) HasEdgeBetween(x, y int64) bool {\n\txid := g.node(x)\n\tyid := g.node(y)\n\tif xid == nil || yid == nil {\n\t\treturn false\n\t}\n\n\treturn xid.children.Contains(yid) || yid.children.Contains(xid)\n}\n\n// Edge returns the edge from u to v if such an edge exists and nil otherwise.\n// The node v must be directly reachable from u as defined by the From method.\nfunc (g *ExprGraph) Edge(u, v int64) graph.Edge {\n\tuid := g.node(u)\n\tvid := g.node(v)\n\n\tif uid == nil || vid == nil {\n\t\treturn nil\n\t}\n\n\tif !uid.children.Contains(vid) {\n\t\treturn nil\n\t}\n\te := edge{from: uid, to: vid}\n\treturn e\n}\n\n/* Directed interface */\n\n// HasEdgeFromTo returns whether an edge exists in the graph from u to v.\nfunc (g *ExprGraph) HasEdgeFromTo(u, v int64) bool {\n\tuid := g.node(u)\n\tvid := g.node(v)\n\tif uid == nil || vid == nil {\n\t\treturn false\n\t}\n\n\treturn uid.children.Contains(vid)\n}\n\n// To returns all nodes in g that can reach directly to n.\nfunc (g *ExprGraph) To(nid int64) graph.Nodes {\n\tn := g.node(nid)\n\tif n == nil {\n\t\treturn nil\n\t}\n\n\tns := g.to[n]\n\tns = ns.Set()\n\tg.to[n] = ns\n\treturn nodeToGraphNode(ns)\n}\n\n// subgraph is basically a subset of nodes. This is useful for compiling sub sections of the graph\nfunc (g *ExprGraph) subgraph(ns Nodes, findMissing bool, opts ...Nodes) *ExprGraph {\n\t// ns = ns.Set()\n\n\tvar roots Nodes\n\t// add missing stuff first\n\tif findMissing {\n\t\tfor _, n := range ns {\n\t\t\tfor _, parent := range g.to[n] {\n\t\t\t\tif parent.isStmt {\n\t\t\t\t\troots = append(roots, parent)\n\t\t\t\t\tns = append(ns, parent)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// uniquify the froms and at the same time build a new roots\n\tallset := ns.mapSet()\n\tif len(opts) == 0 {\n\t\tfor _, n := range ns {\n\t\t\tif len(g.to[n]) == 0 {\n\t\t\t\tif n.isStmt {\n\t\t\t\t\troots = append(roots, n.children[0])\n\t\t\t\t} else {\n\t\t\t\t\troots = append(roots, n)\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tvar hasParent bool\n\t\t\tfor _, parent := range g.to[n] {\n\t\t\t\tif allset.Contains(parent) {\n\t\t\t\t\thasParent = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !hasParent {\n\t\t\t\troots = append(roots, n)\n\t\t\t}\n\t\t}\n\t} else {\n\t\trs := opts[0]\n\t\troots = make(Nodes, len(rs))\n\t\tfor i, n := range rs {\n\t\t\tif n.isStmt {\n\t\t\t\troots[i] = n.children[0]\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\troots[i] = n\n\n\t\t}\n\t}\n\tvar leaves Nodes\n\tfor _, n := range ns {\n\t\tif len(n.children) == 0 {\n\t\t\tleaves = append(leaves, n)\n\t\t}\n\t}\n\n\t// uniquify all the things\n\troots = roots.Set()\n\tleaves = leaves.Set()\n\tns = ns.Set()\n\n\tretVal := &ExprGraph{\n\t\tall:    ns,\n\t\tbyID:   make(map[int64]int),\n\t\tbyHash: g.byHash,\n\t\tevac:   g.evac,\n\t\tto:     g.to,\n\n\t\tleaves:    leaves,\n\t\tconstants: g.constants,\n\t\troots:     roots,\n\t}\n\n\treturn retVal\n}\n\n// Subgraph subsets a graph. This function has overloaded meanings - If only one node is passed in, it assumes that the one node is the root,\n// otherwise, it treats ns as the subset of nodes to be included in the subgraph\nfunc (g *ExprGraph) Subgraph(ns ...*Node) *ExprGraph {\n\tif len(ns) == 1 {\n\t\tg.SubgraphRoots(ns[0])\n\t}\n\treturn g.subgraph(ns, true)\n}\n\n// SubgraphRoots creates a subgraph, assuming the provided nodes are roots to the new subgraph.\nfunc (g *ExprGraph) SubgraphRoots(ns ...*Node) *ExprGraph {\n\tsub := g.walkFromRoots(ns...)\n\treturn g.subgraph(sub, true, ns)\n}\n\n// ExactSubgraphRoots creates a subgraph from the roots provided.\n// The difference between SubgraphRoots and ExactSubgraphRoots is that ExactSubGraphRoots\n// will not attempt to discover if any nodes are missing.\n//\n// Given a function like the following:\n//\t\tz = x + y\n//\t\tset(x, -x.Grad) // setting the value of x to the negative of the gradient\n//\n// When SubgraphRoots is used on z, the `-x.Grad` will be included.\n// When using ExactSubgraphRoots, only `x` and `y` are included in the subgraph\nfunc (g *ExprGraph) ExactSubgraphRoots(ns ...*Node) *ExprGraph {\n\tsub := g.walkFromRoots(ns...)\n\treturn g.subgraph(sub, false, ns)\n}\n\nfunc (g *ExprGraph) walkFromRoots(ns ...*Node) Nodes {\n\tsub := make(Nodes, len(ns))\n\tcopy(sub, ns)\n\n\twalked := NewNodeSet()\n\tfor _, n := range ns {\n\t\tch := make(chan *Node)\n\t\tgo func(ch chan *Node) {\n\t\t\tdefer close(ch)\n\t\t\twalkGraph(n, ch, walked)\n\t\t}(ch)\n\n\t\tfor node := range ch {\n\t\t\tsub = append(sub, node)\n\t\t}\n\t}\n\treturn sub\n}\n\ntype edge struct {\n\tfrom, to graph.Node\n\tweight   float64\n}\n\nfunc (e edge) From() graph.Node         { return e.from }\nfunc (e edge) To() graph.Node           { return e.to }\nfunc (e edge) ReversedEdge() graph.Edge { e.from, e.to = e.to, e.from; return e }\nfunc (e edge) Weight() float64          { return e.weight }\n"
        },
        {
          "name": "graph_test.go",
          "type": "blob",
          "size": 9.2763671875,
          "content": "package gorgonia\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"gonum.org/v1/gonum/graph\"\n\t\"gonum.org/v1/gonum/graph/iterator\"\n\t\"gonum.org/v1/gonum/graph/topo\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc TestGraphBasics(t *testing.T) {\n\tassert := assert.New(t)\n\tg, x, y, xy := simpleEqn()\n\n\t// basic stuff\n\tassert.Equal(g, xy.g)\n\tassert.Contains(g.AllNodes(), x)\n\tassert.Contains(g.AllNodes(), y)\n\tassert.Contains(g.AllNodes(), xy)\n\n\tassert.Equal(Nodes{x, y}, g.leaves)\n\n\t// Node/addressing stuff\n\txid := x.ID()\n\txFromID := g.Node(xid)\n\tassert.Equal(x, xFromID)\n\n\tvar correctTo Nodes\n\tcorrectTo = Nodes{xy}\n\tassert.Equal(correctTo, g.to[x])\n\tassert.Equal(correctTo, g.to[y])\n\n\t// test Uniquifying ability of ExprGraph\n\tnewX := g.AddNode(x)\n\tassert.Equal(x, newX)\n\n\tnewY := g.AddNode(y)\n\tassert.Equal(y, newY)\n\n\tnewXY := Must(Add(x, y))\n\tcorrectTo = append(correctTo, xy) // note this is correct. .Set() will be called when graph.To() is called\n\tassert.Equal(xy, newXY)\n\tassert.Equal(correctTo, g.to[y])\n\tassert.Equal(correctTo, g.to[x])\n\n\tcorrectTo = Nodes{xy}\n\tassert.Equal(correctTo, sliceNodesToNodes(graph.NodesOf(g.To(y.ID()))))\n\tassert.Equal(correctTo, sliceNodesToNodes(graph.NodesOf(g.To(x.ID()))))\n\n\tassert.Equal(3, g.Nodes().Len())\n\n\t// Now, time to deal with constants\n\txy1 := Must(Add(xy, onef64))\n\tassert.Nil(onef64.g)\n\tassert.Equal(g, xy1.g)\n\n\tvar containsOne bool\n\n\tit := g.Nodes()\n\tfor it.Next() {\n\t\tnode := it.Node()\n\t\tn := node.(*Node)\n\t\tif n.Hashcode() == onef64.Hashcode() {\n\t\t\tcontainsOne = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif !containsOne {\n\t\tt.Errorf(\"graph does not contain a clone of onef64: %v\", g.Nodes())\n\t}\n\n\t// duplicate constants\n\tone := NewConstant(1.0)\n\tnewOne := g.AddNode(one)\n\tif one == newOne {\n\t\tt.Error(\"one should not have been added to the graph\")\n\t}\n\tassert.NotNil(newOne.g)\n\tassert.NotEqual(one, newOne)\n}\n\n// This test is added to make sure I'm sane when dealing with sorted graphs\n// because sometimes Eobard Thawne is needed\nfunc TestGraphSort(t *testing.T) {\n\tassert := assert.New(t)\n\tg, _, _, z := simpleVecEqn()\n\tWithName(\"z\")(z)\n\n\tvar sortedNodes []graph.Node\n\tvar err error\n\n\t// stability tests\n\tfor i := 0; i < 100; i++ {\n\t\tif sortedNodes, err = topo.Sort(g); err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t\t// expected := Nodes{z, y, x} // the old version of ExprGraph was stable with topo.Sort, but the new version ain't\n\t\t// assert.Equal(expected, sortedNodes)\n\t\tassert.Equal(z, sortedNodes[0])\n\t}\n\n\t// this is to remind myself how this thing sorts:\n\tt.Logf(\"%v\", graphNodeToNode(iterator.NewOrderedNodes(sortedNodes)))\n}\n\n// test that collisions are handled correctly\nfunc TestGraphCollisions(t *testing.T) {\n\tassert := assert.New(t)\n\tg, _, _, xy := simpleEqn()\n\tdelete(g.byHash, xy.hash)\n\tg.byHash[0xdeadbeef] = xy\n\txy.hash = 0xdeadbeef\n\txy.name = \"original\"\n\tt.Logf(\"original: %p, hash %x\", xy, xy.Hashcode())\n\n\tcol := new(Node)\n\tcol.name = \"COLIN THE COLLISION\"\n\tcol.hash = 0xdeadbeef\n\tcol.hashed = true\n\tcol2 := g.AddNode(col)\n\n\tassert.Equal(col, col2)\n\tassert.Equal(4, len(g.AllNodes()), \"%v\", g.AllNodes())\n\tassert.True(g.Has(col.ID()))\n\n\tcolleen := new(Node)\n\tcolleen.name = \"COLLEEN THE COLLISION\"\n\tcolleen.hash = 0xdeadbeef\n\tcolleen.hashed = true\n\tcolleen2 := g.AddNode(colleen)\n\n\tassert.Equal(colleen, colleen2)\n\tassert.Equal(5, len(g.AllNodes()), \"%v\", g.AllNodes())\n\tassert.True(g.Has(colleen.ID()))\n\n}\n\nfunc TestGraphEquality(t *testing.T) {\n\t_, x, y, z := simpleVecEqn()\n\n\txh1 := x.Hashcode()\n\tyh1 := y.Hashcode()\n\tif xh1 == yh1 {\n\t\tt.Error(\"Different nodes, should have different hashes\")\n\t}\n\n\t_, x2, y2, z2 := simpleVecEqn()\n\n\tif x.Hashcode() != x2.Hashcode() {\n\t\tt.Error(\"They should have the same hash\")\n\t}\n\n\tif y.Hashcode() != y2.Hashcode() {\n\t\tt.Error(\"They should have the same hash\")\n\t}\n\n\tif z.Hashcode() != z2.Hashcode() {\n\t\tt.Error(\"They should have the same hash\")\n\t}\n}\n\nfunc TestGraphSubgraph(t *testing.T) {\n\tvar err error\n\tvar sortedNodes Nodes\n\tassert := assert.New(t)\n\n\tg, x, y, z := simpleVecEqn()\n\n\tsub := Nodes{x, y}\n\tg2 := g.subgraph(sub, true)\n\n\tt.Logf(\"%v\", g2.AllNodes())\n\n\tif sortedNodes, err = Sort(g2); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tassert.NotContains(sortedNodes, z)\n\tassert.Contains(g2.roots, x)\n\tassert.Contains(g2.roots, y)\n\tassert.Equal(2, len(g2.roots))\n}\n\nfunc TestGraph_SubgraphRoots(t *testing.T) {\n\tassert := assert.New(t)\n\tg, x, y, z := simpleVecEqn()\n\tsz := Must(Sum(z))\n\ta := NewVector(g, Float64, WithName(\"a\"), WithShape(2))\n\tb := NewVector(g, Float64, WithName(\"b\"), WithShape(2))\n\tc := Must(Add(a, b))\n\tsc := Must(Sum(c))\n\n\tvar szVal, scVal Value\n\treadSZ := Read(sz, &szVal)\n\treadSC := Read(sc, &scVal)\n\n\t// check that stmt nodes aren't included in the roots\n\tsg := g.SubgraphRoots(readSZ, readSC)\n\tassert.Contains(sg.roots, sz)\n\tassert.Contains(sg.roots, sc)\n\tassert.Equal(2, len(sg.roots))\n\n\t// check that subgrapphing actually works\n\tsg = g.SubgraphRoots(c)\n\tns := sg.AllNodes()\n\tassert.NotContains(ns, sc)\n\tassert.NotContains(ns, readSC)\n\tassert.NotContains(ns, x)\n\tassert.NotContains(ns, y)\n\tassert.NotContains(ns, z)\n\tassert.NotContains(ns, sz)\n\tassert.NotContains(ns, readSZ)\n}\n\nfunc TestGraph_ExactSubgraphRoots(t *testing.T) {\n\tassert := assert.New(t)\n\tg, x, y, z := simpleVecEqn()\n\tsz := Must(Sum(z))\n\tsetXtoZ := Set(x, z) // setting x = z\n\n\tsg0 := g.SubgraphRoots(sz)\n\tsg1 := g.ExactSubgraphRoots(sz)\n\tns0 := sg0.AllNodes()\n\tns1 := sg1.AllNodes()\n\tassert.Contains(ns0, setXtoZ)\n\tassert.NotContains(ns1, setXtoZ)\n\tassert.Contains(ns0, x)\n\tassert.Contains(ns0, y)\n\tassert.Contains(ns0, z)\n\tassert.Contains(ns0, sz)\n\n}\n\nfunc TestGraph_Constant(t *testing.T) {\n\tg := NewGraph()\n\n\tv1 := NewF64(1.0)\n\tc0 := g.Constant(v1)\n\tc1 := g.Constant(v1)\n\n\tif c0 != c1 {\n\t\tt.Errorf(\"Expected c0 and c1 to be the same (pointer and all that)\")\n\t}\n}\n\nfunc TestGraph_Clone(t *testing.T) {\n\tg, x, y, z := simpleVecEqn()\n\tz2 := Must(Square(z))\n\n\t// add a collided\n\tz2t := z2.Type()\n\tdelete(g.byHash, z2.hash)\n\tg.byHash[0xdeadbeef] = z2\n\tcol := new(Node)\n\tcol.g = g\n\tcol.name = \"COLIN THE COLLISION\"\n\tcol.hash = 0xdeadbeef\n\tcol.hashed = true\n\tcol.boundTo = NewF64(0)\n\tcol.t = z2t\n\tg.AddNode(col)\n\n\tcolleen := new(Node)\n\tcolleen.g = g\n\tcolleen.name = \"COLLEEN THE COLLISION\"\n\tcolleen.hash = 0xdeadbeef\n\tcolleen.hashed = true\n\tcolleen.boundTo = NewF64(0)\n\tcolleen.t = z2t\n\tg.AddNode(colleen)\n\n\tone := onef64\n\tz2p1 := Must(Add(z2, one))                                    // add a constant\n\trando := UniformRandomNode(g, Float64, 0, 1, z2p1.Shape()...) // add a weird node\n\tblah := Must(HadamardProd(z2p1, rando))\n\tcost := Must(Sum(blah))\n\t_, err := Grad(cost, x, y)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tg.Roots() // call it to populate the roots field\n\n\t// clone with nil values\n\tg2 := g.Clone().(*ExprGraph)\n\tfor i, n := range g.all {\n\t\tcloned := g2.all[i]\n\t\tif !deepNodeEq(n, cloned) {\n\t\t\tt.Errorf(\"Expected %d of all to be %v. Got %v instead\", i, n, cloned)\n\t\t\tbreak\n\t\t}\n\t}\n\tif len(g.evac) != len(g2.evac) && len(g.evac) > 0 {\n\t\tt.Errorf(\"Expected the evacs to have the same length\")\n\t}\n\tfor k, v := range g.evac {\n\t\tvar v2 Nodes\n\t\tvar ok bool\n\t\tif v2, ok = g2.evac[k]; !ok {\n\t\t\tt.Errorf(\"Key %v not found in cloned evac\", k)\n\t\t\tbreak\n\t\t}\n\t\tfor i, n := range v {\n\t\t\tif !deepNodeEq(n, v2[i]) {\n\t\t\t\tt.Errorf(\"Expected v[%d] to have equal values\", i)\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif t.Failed() {\n\t\t\tbreak\n\t\t}\n\t}\n\tif len(g.roots) != len(g2.roots) {\n\t\tt.Errorf(\"Expected roots to be %d. Got %d instead\", len(g.roots), len(g2.roots))\n\t}\n\tfor i, root := range g.roots {\n\t\tif !deepNodeEq(root, g2.roots[i]) {\n\t\t\tt.Errorf(\"Expected roots[%d] to have equal nodes\", i)\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif len(g.leaves) != len(g2.leaves) {\n\t\tt.Errorf(\"Expected leaves to be %d. Got %d instead\", len(g.leaves), len(g2.leaves))\n\t}\n\tfor i, leaf := range g.leaves {\n\t\tif !deepNodeEq(leaf, g2.leaves[i]) {\n\t\t\tt.Errorf(\"Expected leaves[%d] to be equal\", i)\n\t\t\tbreak\n\t\t}\n\t}\n\n\tLet(x, tensor.New(tensor.WithBacking([]float64{1, 2})))\n\tLet(y, tensor.New(tensor.WithBacking([]float64{3, 4})))\n\tm := NewLispMachine(g, ExecuteFwdOnly()) // the gradient has been precalculated\n\tdefer m.Close()\n\tif err := m.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tg2 = g.Clone().(*ExprGraph)\n\tfor i, n := range g.all {\n\t\tcloned := g2.all[i]\n\t\tif !deepNodeEq(n, cloned) {\n\t\t\tt.Errorf(\"Expected %d of all to be %v. Got %v instead\", i, n, cloned)\n\t\t\tbreak\n\t\t}\n\t}\n\tif len(g.evac) != len(g2.evac) && len(g.evac) > 0 {\n\t\tt.Errorf(\"Expected the evacs to have the same length\")\n\t}\n\tfor k, v := range g.evac {\n\t\tvar v2 Nodes\n\t\tvar ok bool\n\t\tif v2, ok = g2.evac[k]; !ok {\n\t\t\tt.Errorf(\"Key %v not found in cloned evac\", k)\n\t\t\tbreak\n\t\t}\n\t\tfor i, n := range v {\n\t\t\tif !deepNodeEq(n, v2[i]) {\n\t\t\t\tt.Errorf(\"Expected v[%d] to have equal values\", i)\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif t.Failed() {\n\t\t\tbreak\n\t\t}\n\t}\n\tif len(g.roots) != len(g2.roots) {\n\t\tt.Errorf(\"Expected roots to be %d. Got %d instead\", len(g.roots), len(g2.roots))\n\t}\n\tfor i, root := range g.roots {\n\t\tif !deepNodeEq(root, g2.roots[i]) {\n\t\t\tt.Errorf(\"Expected roots[%d] to have equal nodes\", i)\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif len(g.leaves) != len(g2.leaves) {\n\t\tt.Errorf(\"Expected leaves to be %d. Got %d instead\", len(g.leaves), len(g2.leaves))\n\t}\n\tfor i, leaf := range g.leaves {\n\t\tif !deepNodeEq(leaf, g2.leaves[i]) {\n\t\t\tt.Errorf(\"Expected leaves[%d] to be equal\", i)\n\t\t\tbreak\n\t\t}\n\t}\n}\n\nfunc TestExprGraph_Edges(t *testing.T) {\n\tg := NewGraph()\n\n\tvar x, y *Node\n\n\t// define the expression\n\tx = NewScalar(g, Float64, WithName(\"x\"))\n\ty = NewScalar(g, Float64, WithName(\"y\"))\n\tAdd(x, y)\n\tedgesIT := g.Edges()\n\tif edgesIT.Len() != 2 {\n\t\tt.Fail()\n\t}\n}\n"
        },
        {
          "name": "inpout.json",
          "type": "blob",
          "size": 18.90625,
          "content": "{\"insh\": [1, 5, 5, 8],\n \"indata\": [3.588184118270874, 6.765244483947754, 8.703499794006348, 7.832754611968994, 7.205557346343994, 2.2351908683776855, 9.140625, 1.8968262672424316, 7.579838752746582, 2.0422589778900146, 1.7213886976242065, 8.008468627929688, 1.8242313861846924, 0.15505588054656982, 7.218662738800049, 6.959044456481934, 5.268470287322998, 3.0470921993255615, 6.904945373535156, 0.048324376344680786, 5.862209796905518, 9.752281188964844, 9.729859352111816, 6.917296409606934, 5.983068466186523, 8.5852689743042, 8.877313613891602, 0.06957779079675674, 8.360713958740234, 1.0679903030395508, 1.2100963592529297, 9.838610649108887, 2.814711809158325, 6.38685941696167, 8.260359764099121, 2.532329559326172, 7.589834213256836, 1.294305443763733, 0.6550663113594055, 0.8506288528442383, 2.1071043014526367, 8.425798416137695, 8.408304214477539, 7.394162178039551, 2.590216875076294, 0.7849066257476807, 2.676325559616089, 8.072299003601074, 3.4864327907562256, 2.422727346420288, 9.429472923278809, 2.553463935852051, 5.96559476852417, 7.482579231262207, 2.3580377101898193, 1.5595018863677979, 0.034589122980833054, 3.0223019123077393, 4.7403154373168945, 0.2549790143966675, 6.765586853027344, 6.91905403137207, 1.7624300718307495, 1.8117458820343018, 5.739025592803955, 7.67076301574707, 8.772150039672852, 2.1648266315460205, 2.8233001232147217, 3.455390214920044, 1.954986333847046, 6.2339935302734375, 8.180193901062012, 3.4818568229675293, 0.5501848459243774, 7.886703014373779, 3.614240884780884, 7.023927211761475, 0.4005505442619324, 2.0974090099334717, 6.166986465454102, 4.413336753845215, 7.7545647621154785, 5.656180381774902, 8.241524696350098, 0.1758895069360733, 7.527948379516602, 2.4103832244873047, 2.5865209102630615, 8.264690399169922, 4.0821614265441895, 4.711780548095703, 8.985666275024414, 3.614298105239868, 2.9979336261749268, 1.0938977003097534, 0.6748584508895874, 8.776729583740234, 7.792627811431885, 4.120683670043945, 8.067517280578613, 6.481339931488037, 3.7537691593170166, 1.5674660205841064, 4.622176647186279, 6.572947025299072, 6.97075891494751, 0.5755885243415833, 9.231983184814453, 5.7552490234375, 6.862707138061523, 6.564248085021973, 4.892410755157471, 5.318929672241211, 6.568413257598877, 9.133810043334961, 7.237424373626709, 4.397247314453125, 7.976856708526611, 2.3416335582733154, 6.3428730964660645, 9.161764144897461, 9.220541000366211, 8.534843444824219, 4.4301581382751465, 3.7176058292388916, 4.751829624176025, 5.455737113952637, 0.894568920135498, 5.666994571685791, 1.3768253326416016, 9.591104507446289, 8.1528902053833, 7.843255519866943, 1.170852541923523, 9.404593467712402, 1.0466225147247314, 2.0946621894836426, 8.859201431274414, 7.388158321380615, 0.26116275787353516, 4.805743217468262, 7.201430320739746, 3.692185878753662, 4.12822151184082, 5.931211471557617, 6.904507637023926, 6.349696636199951, 4.302095413208008, 6.546483516693115, 8.069499969482422, 2.374738931655884, 9.065333366394043, 9.402444839477539, 6.593384265899658, 8.194018363952637, 3.910661458969116, 2.1357905864715576, 0.8497279286384583, 1.6840051412582397, 7.609548568725586, 1.4399187564849854, 1.0644301176071167, 8.14090347290039, 2.919234275817871, 2.675851583480835, 3.0641536712646484, 6.6956963539123535, 3.498112678527832, 0.69023597240448, 1.1889065504074097, 8.269831657409668, 8.937707901000977, 1.6059719324111938, 0.15695418417453766, 0.9913957118988037, 7.1435065269470215, 4.237936973571777, 2.295494556427002, 5.945472240447998, 3.4239118099212646, 4.425090312957764, 3.2222511768341064, 1.6377434730529785, 4.766551971435547, 6.3335795402526855, 3.467780351638794, 5.677145004272461, 1.9361212253570557, 2.3081438541412354, 5.466902256011963, 4.283600807189941, 2.0228371620178223, 5.984248638153076, 6.023360252380371, 8.454516410827637, 0.48479247093200684, 1.7577219009399414, 7.09525203704834, 1.0326943397521973],\n \"outsh\":[1, 5, 10, 16],\n \"outdata\": [3.588184118270874, 3.588184118270874, 6.765244483947754, 6.765244483947754, 8.703499794006348, 8.703499794006348, 7.832754611968994, 7.832754611968994, 7.205557346343994, 7.205557346343994, 2.2351908683776855, 2.2351908683776855, 9.140625, 9.140625, 1.8968262672424316, 1.8968262672424316, 3.588184118270874, 3.588184118270874, 6.765244483947754, 6.765244483947754, 8.703499794006348, 8.703499794006348, 7.832754611968994, 7.832754611968994, 7.205557346343994, 7.205557346343994, 2.2351908683776855, 2.2351908683776855, 9.140625, 9.140625, 1.8968262672424316, 1.8968262672424316, 7.579838752746582, 7.579838752746582, 2.0422589778900146, 2.0422589778900146, 1.7213886976242065, 1.7213886976242065, 8.008468627929688, 8.008468627929688, 1.8242313861846924, 1.8242313861846924, 0.15505588054656982, 0.15505588054656982, 7.218662738800049, 7.218662738800049, 6.959044456481934, 6.959044456481934, 7.579838752746582, 7.579838752746582, 2.0422589778900146, 2.0422589778900146, 1.7213886976242065, 1.7213886976242065, 8.008468627929688, 8.008468627929688, 1.8242313861846924, 1.8242313861846924, 0.15505588054656982, 0.15505588054656982, 7.218662738800049, 7.218662738800049, 6.959044456481934, 6.959044456481934, 5.268470287322998, 5.268470287322998, 3.0470921993255615, 3.0470921993255615, 6.904945373535156, 6.904945373535156, 0.048324376344680786, 0.048324376344680786, 5.862209796905518, 5.862209796905518, 9.752281188964844, 9.752281188964844, 9.729859352111816, 9.729859352111816, 6.917296409606934, 6.917296409606934, 5.268470287322998, 5.268470287322998, 3.0470921993255615, 3.0470921993255615, 6.904945373535156, 6.904945373535156, 0.048324376344680786, 0.048324376344680786, 5.862209796905518, 5.862209796905518, 9.752281188964844, 9.752281188964844, 9.729859352111816, 9.729859352111816, 6.917296409606934, 6.917296409606934, 5.983068466186523, 5.983068466186523, 8.5852689743042, 8.5852689743042, 8.877313613891602, 8.877313613891602, 0.06957779079675674, 0.06957779079675674, 8.360713958740234, 8.360713958740234, 1.0679903030395508, 1.0679903030395508, 1.2100963592529297, 1.2100963592529297, 9.838610649108887, 9.838610649108887, 5.983068466186523, 5.983068466186523, 8.5852689743042, 8.5852689743042, 8.877313613891602, 8.877313613891602, 0.06957779079675674, 0.06957779079675674, 8.360713958740234, 8.360713958740234, 1.0679903030395508, 1.0679903030395508, 1.2100963592529297, 1.2100963592529297, 9.838610649108887, 9.838610649108887, 2.814711809158325, 2.814711809158325, 6.38685941696167, 6.38685941696167, 8.260359764099121, 8.260359764099121, 2.532329559326172, 2.532329559326172, 7.589834213256836, 7.589834213256836, 1.294305443763733, 1.294305443763733, 0.6550663113594055, 0.6550663113594055, 0.8506288528442383, 0.8506288528442383, 2.814711809158325, 2.814711809158325, 6.38685941696167, 6.38685941696167, 8.260359764099121, 8.260359764099121, 2.532329559326172, 2.532329559326172, 7.589834213256836, 7.589834213256836, 1.294305443763733, 1.294305443763733, 0.6550663113594055, 0.6550663113594055, 0.8506288528442383, 0.8506288528442383, 2.1071043014526367, 2.1071043014526367, 8.425798416137695, 8.425798416137695, 8.408304214477539, 8.408304214477539, 7.394162178039551, 7.394162178039551, 2.590216875076294, 2.590216875076294, 0.7849066257476807, 0.7849066257476807, 2.676325559616089, 2.676325559616089, 8.072299003601074, 8.072299003601074, 2.1071043014526367, 2.1071043014526367, 8.425798416137695, 8.425798416137695, 8.408304214477539, 8.408304214477539, 7.394162178039551, 7.394162178039551, 2.590216875076294, 2.590216875076294, 0.7849066257476807, 0.7849066257476807, 2.676325559616089, 2.676325559616089, 8.072299003601074, 8.072299003601074, 3.4864327907562256, 3.4864327907562256, 2.422727346420288, 2.422727346420288, 9.429472923278809, 9.429472923278809, 2.553463935852051, 2.553463935852051, 5.96559476852417, 5.96559476852417, 7.482579231262207, 7.482579231262207, 2.3580377101898193, 2.3580377101898193, 1.5595018863677979, 1.5595018863677979, 3.4864327907562256, 3.4864327907562256, 2.422727346420288, 2.422727346420288, 9.429472923278809, 9.429472923278809, 2.553463935852051, 2.553463935852051, 5.96559476852417, 5.96559476852417, 7.482579231262207, 7.482579231262207, 2.3580377101898193, 2.3580377101898193, 1.5595018863677979, 1.5595018863677979, 0.034589122980833054, 0.034589122980833054, 3.0223019123077393, 3.0223019123077393, 4.7403154373168945, 4.7403154373168945, 0.2549790143966675, 0.2549790143966675, 6.765586853027344, 6.765586853027344, 6.91905403137207, 6.91905403137207, 1.7624300718307495, 1.7624300718307495, 1.8117458820343018, 1.8117458820343018, 0.034589122980833054, 0.034589122980833054, 3.0223019123077393, 3.0223019123077393, 4.7403154373168945, 4.7403154373168945, 0.2549790143966675, 0.2549790143966675, 6.765586853027344, 6.765586853027344, 6.91905403137207, 6.91905403137207, 1.7624300718307495, 1.7624300718307495, 1.8117458820343018, 1.8117458820343018, 5.739025592803955, 5.739025592803955, 7.67076301574707, 7.67076301574707, 8.772150039672852, 8.772150039672852, 2.1648266315460205, 2.1648266315460205, 2.8233001232147217, 2.8233001232147217, 3.455390214920044, 3.455390214920044, 1.954986333847046, 1.954986333847046, 6.2339935302734375, 6.2339935302734375, 5.739025592803955, 5.739025592803955, 7.67076301574707, 7.67076301574707, 8.772150039672852, 8.772150039672852, 2.1648266315460205, 2.1648266315460205, 2.8233001232147217, 2.8233001232147217, 3.455390214920044, 3.455390214920044, 1.954986333847046, 1.954986333847046, 6.2339935302734375, 6.2339935302734375, 8.180193901062012, 8.180193901062012, 3.4818568229675293, 3.4818568229675293, 0.5501848459243774, 0.5501848459243774, 7.886703014373779, 7.886703014373779, 3.614240884780884, 3.614240884780884, 7.023927211761475, 7.023927211761475, 0.4005505442619324, 0.4005505442619324, 2.0974090099334717, 2.0974090099334717, 8.180193901062012, 8.180193901062012, 3.4818568229675293, 3.4818568229675293, 0.5501848459243774, 0.5501848459243774, 7.886703014373779, 7.886703014373779, 3.614240884780884, 3.614240884780884, 7.023927211761475, 7.023927211761475, 0.4005505442619324, 0.4005505442619324, 2.0974090099334717, 2.0974090099334717, 6.166986465454102, 6.166986465454102, 4.413336753845215, 4.413336753845215, 7.7545647621154785, 7.7545647621154785, 5.656180381774902, 5.656180381774902, 8.241524696350098, 8.241524696350098, 0.1758895069360733, 0.1758895069360733, 7.527948379516602, 7.527948379516602, 2.4103832244873047, 2.4103832244873047, 6.166986465454102, 6.166986465454102, 4.413336753845215, 4.413336753845215, 7.7545647621154785, 7.7545647621154785, 5.656180381774902, 5.656180381774902, 8.241524696350098, 8.241524696350098, 0.1758895069360733, 0.1758895069360733, 7.527948379516602, 7.527948379516602, 2.4103832244873047, 2.4103832244873047, 2.5865209102630615, 2.5865209102630615, 8.264690399169922, 8.264690399169922, 4.0821614265441895, 4.0821614265441895, 4.711780548095703, 4.711780548095703, 8.985666275024414, 8.985666275024414, 3.614298105239868, 3.614298105239868, 2.9979336261749268, 2.9979336261749268, 1.0938977003097534, 1.0938977003097534, 2.5865209102630615, 2.5865209102630615, 8.264690399169922, 8.264690399169922, 4.0821614265441895, 4.0821614265441895, 4.711780548095703, 4.711780548095703, 8.985666275024414, 8.985666275024414, 3.614298105239868, 3.614298105239868, 2.9979336261749268, 2.9979336261749268, 1.0938977003097534, 1.0938977003097534, 0.6748584508895874, 0.6748584508895874, 8.776729583740234, 8.776729583740234, 7.792627811431885, 7.792627811431885, 4.120683670043945, 4.120683670043945, 8.067517280578613, 8.067517280578613, 6.481339931488037, 6.481339931488037, 3.7537691593170166, 3.7537691593170166, 1.5674660205841064, 1.5674660205841064, 0.6748584508895874, 0.6748584508895874, 8.776729583740234, 8.776729583740234, 7.792627811431885, 7.792627811431885, 4.120683670043945, 4.120683670043945, 8.067517280578613, 8.067517280578613, 6.481339931488037, 6.481339931488037, 3.7537691593170166, 3.7537691593170166, 1.5674660205841064, 1.5674660205841064, 4.622176647186279, 4.622176647186279, 6.572947025299072, 6.572947025299072, 6.97075891494751, 6.97075891494751, 0.5755885243415833, 0.5755885243415833, 9.231983184814453, 9.231983184814453, 5.7552490234375, 5.7552490234375, 6.862707138061523, 6.862707138061523, 6.564248085021973, 6.564248085021973, 4.622176647186279, 4.622176647186279, 6.572947025299072, 6.572947025299072, 6.97075891494751, 6.97075891494751, 0.5755885243415833, 0.5755885243415833, 9.231983184814453, 9.231983184814453, 5.7552490234375, 5.7552490234375, 6.862707138061523, 6.862707138061523, 6.564248085021973, 6.564248085021973, 4.892410755157471, 4.892410755157471, 5.318929672241211, 5.318929672241211, 6.568413257598877, 6.568413257598877, 9.133810043334961, 9.133810043334961, 7.237424373626709, 7.237424373626709, 4.397247314453125, 4.397247314453125, 7.976856708526611, 7.976856708526611, 2.3416335582733154, 2.3416335582733154, 4.892410755157471, 4.892410755157471, 5.318929672241211, 5.318929672241211, 6.568413257598877, 6.568413257598877, 9.133810043334961, 9.133810043334961, 7.237424373626709, 7.237424373626709, 4.397247314453125, 4.397247314453125, 7.976856708526611, 7.976856708526611, 2.3416335582733154, 2.3416335582733154, 6.3428730964660645, 6.3428730964660645, 9.161764144897461, 9.161764144897461, 9.220541000366211, 9.220541000366211, 8.534843444824219, 8.534843444824219, 4.4301581382751465, 4.4301581382751465, 3.7176058292388916, 3.7176058292388916, 4.751829624176025, 4.751829624176025, 5.455737113952637, 5.455737113952637, 6.3428730964660645, 6.3428730964660645, 9.161764144897461, 9.161764144897461, 9.220541000366211, 9.220541000366211, 8.534843444824219, 8.534843444824219, 4.4301581382751465, 4.4301581382751465, 3.7176058292388916, 3.7176058292388916, 4.751829624176025, 4.751829624176025, 5.455737113952637, 5.455737113952637, 0.894568920135498, 0.894568920135498, 5.666994571685791, 5.666994571685791, 1.3768253326416016, 1.3768253326416016, 9.591104507446289, 9.591104507446289, 8.1528902053833, 8.1528902053833, 7.843255519866943, 7.843255519866943, 1.170852541923523, 1.170852541923523, 9.404593467712402, 9.404593467712402, 0.894568920135498, 0.894568920135498, 5.666994571685791, 5.666994571685791, 1.3768253326416016, 1.3768253326416016, 9.591104507446289, 9.591104507446289, 8.1528902053833, 8.1528902053833, 7.843255519866943, 7.843255519866943, 1.170852541923523, 1.170852541923523, 9.404593467712402, 9.404593467712402, 1.0466225147247314, 1.0466225147247314, 2.0946621894836426, 2.0946621894836426, 8.859201431274414, 8.859201431274414, 7.388158321380615, 7.388158321380615, 0.26116275787353516, 0.26116275787353516, 4.805743217468262, 4.805743217468262, 7.201430320739746, 7.201430320739746, 3.692185878753662, 3.692185878753662, 1.0466225147247314, 1.0466225147247314, 2.0946621894836426, 2.0946621894836426, 8.859201431274414, 8.859201431274414, 7.388158321380615, 7.388158321380615, 0.26116275787353516, 0.26116275787353516, 4.805743217468262, 4.805743217468262, 7.201430320739746, 7.201430320739746, 3.692185878753662, 3.692185878753662, 4.12822151184082, 4.12822151184082, 5.931211471557617, 5.931211471557617, 6.904507637023926, 6.904507637023926, 6.349696636199951, 6.349696636199951, 4.302095413208008, 4.302095413208008, 6.546483516693115, 6.546483516693115, 8.069499969482422, 8.069499969482422, 2.374738931655884, 2.374738931655884, 4.12822151184082, 4.12822151184082, 5.931211471557617, 5.931211471557617, 6.904507637023926, 6.904507637023926, 6.349696636199951, 6.349696636199951, 4.302095413208008, 4.302095413208008, 6.546483516693115, 6.546483516693115, 8.069499969482422, 8.069499969482422, 2.374738931655884, 2.374738931655884, 9.065333366394043, 9.065333366394043, 9.402444839477539, 9.402444839477539, 6.593384265899658, 6.593384265899658, 8.194018363952637, 8.194018363952637, 3.910661458969116, 3.910661458969116, 2.1357905864715576, 2.1357905864715576, 0.8497279286384583, 0.8497279286384583, 1.6840051412582397, 1.6840051412582397, 9.065333366394043, 9.065333366394043, 9.402444839477539, 9.402444839477539, 6.593384265899658, 6.593384265899658, 8.194018363952637, 8.194018363952637, 3.910661458969116, 3.910661458969116, 2.1357905864715576, 2.1357905864715576, 0.8497279286384583, 0.8497279286384583, 1.6840051412582397, 1.6840051412582397, 7.609548568725586, 7.609548568725586, 1.4399187564849854, 1.4399187564849854, 1.0644301176071167, 1.0644301176071167, 8.14090347290039, 8.14090347290039, 2.919234275817871, 2.919234275817871, 2.675851583480835, 2.675851583480835, 3.0641536712646484, 3.0641536712646484, 6.6956963539123535, 6.6956963539123535, 7.609548568725586, 7.609548568725586, 1.4399187564849854, 1.4399187564849854, 1.0644301176071167, 1.0644301176071167, 8.14090347290039, 8.14090347290039, 2.919234275817871, 2.919234275817871, 2.675851583480835, 2.675851583480835, 3.0641536712646484, 3.0641536712646484, 6.6956963539123535, 6.6956963539123535, 3.498112678527832, 3.498112678527832, 0.69023597240448, 0.69023597240448, 1.1889065504074097, 1.1889065504074097, 8.269831657409668, 8.269831657409668, 8.937707901000977, 8.937707901000977, 1.6059719324111938, 1.6059719324111938, 0.15695418417453766, 0.15695418417453766, 0.9913957118988037, 0.9913957118988037, 3.498112678527832, 3.498112678527832, 0.69023597240448, 0.69023597240448, 1.1889065504074097, 1.1889065504074097, 8.269831657409668, 8.269831657409668, 8.937707901000977, 8.937707901000977, 1.6059719324111938, 1.6059719324111938, 0.15695418417453766, 0.15695418417453766, 0.9913957118988037, 0.9913957118988037, 7.1435065269470215, 7.1435065269470215, 4.237936973571777, 4.237936973571777, 2.295494556427002, 2.295494556427002, 5.945472240447998, 5.945472240447998, 3.4239118099212646, 3.4239118099212646, 4.425090312957764, 4.425090312957764, 3.2222511768341064, 3.2222511768341064, 1.6377434730529785, 1.6377434730529785, 7.1435065269470215, 7.1435065269470215, 4.237936973571777, 4.237936973571777, 2.295494556427002, 2.295494556427002, 5.945472240447998, 5.945472240447998, 3.4239118099212646, 3.4239118099212646, 4.425090312957764, 4.425090312957764, 3.2222511768341064, 3.2222511768341064, 1.6377434730529785, 1.6377434730529785, 4.766551971435547, 4.766551971435547, 6.3335795402526855, 6.3335795402526855, 3.467780351638794, 3.467780351638794, 5.677145004272461, 5.677145004272461, 1.9361212253570557, 1.9361212253570557, 2.3081438541412354, 2.3081438541412354, 5.466902256011963, 5.466902256011963, 4.283600807189941, 4.283600807189941, 4.766551971435547, 4.766551971435547, 6.3335795402526855, 6.3335795402526855, 3.467780351638794, 3.467780351638794, 5.677145004272461, 5.677145004272461, 1.9361212253570557, 1.9361212253570557, 2.3081438541412354, 2.3081438541412354, 5.466902256011963, 5.466902256011963, 4.283600807189941, 4.283600807189941, 2.0228371620178223, 2.0228371620178223, 5.984248638153076, 5.984248638153076, 6.023360252380371, 6.023360252380371, 8.454516410827637, 8.454516410827637, 0.48479247093200684, 0.48479247093200684, 1.7577219009399414, 1.7577219009399414, 7.09525203704834, 7.09525203704834, 1.0326943397521973, 1.0326943397521973, 2.0228371620178223, 2.0228371620178223, 5.984248638153076, 5.984248638153076, 6.023360252380371, 6.023360252380371, 8.454516410827637, 8.454516410827637, 0.48479247093200684, 0.48479247093200684, 1.7577219009399414, 1.7577219009399414, 7.09525203704834, 7.09525203704834, 1.0326943397521973, 1.0326943397521973]\n}"
        },
        {
          "name": "interfaces.go",
          "type": "blob",
          "size": 0.9775390625,
          "content": "package gorgonia\n\nimport (\n\t\"hash\"\n\t\"unsafe\"\n\n\t\"gorgonia.org/tensor\"\n)\n\n// Tensor is an interface that describes an ndarray\ntype Tensor interface {\n\t// info about the ndarrayN\n\tShape() tensor.Shape\n\tStrides() []int\n\tDtype() tensor.Dtype\n\tDims() int\n\tSize() int\n\tDataSize() int\n\n\t// type overloading methods\n\tIsScalar() bool\n\tScalarValue() interface{}\n\n\t// engine/memory related stuff\n\t// all Tensors should be able to be expressed of as a slab of memory\n\t// Note: the size of each element can be acquired by T.Dtype().Size()\n\tEngine() tensor.Engine      // Engine can be nil\n\tMemSize() uintptr           // the size in memory\n\tUintptr() uintptr           // the pointer to the first element, as a uintptr\n\tPointer() unsafe.Pointer    // the pointer to the first elemment as a unsafe.Ponter\n\tIsNativelyAccessible() bool // Can Go access the memory\n\tIsManuallyManaged() bool    // Must Go manage the memory\n}\n\ntype hashWriter interface {\n\tWriteHash(hash.Hash)\n}\n\ntype arityer interface {\n\tArity() int\n}\n"
        },
        {
          "name": "internal",
          "type": "tree",
          "content": null
        },
        {
          "name": "known_issues_test.go",
          "type": "blob",
          "size": 8.966796875,
          "content": "package gorgonia\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc TestIssue182(t *testing.T) {\n\t// This test revolves around repeated calls to run a VM.\n\t// Formerly, upon running the VM once, the derivation of the constant is set.\n\t// This derivation value would get Add()ed to upon subsequqent calls to run the VM.\n\t//\n\t// This behaviour was fixed to make sure constants do not have derivatives\n\tassert := assert.New(t)\n\n\t// Build the graph\n\tg := NewGraph()\n\n\taback := []float64{2.0, 2.0, 2.0}\n\tx := NewVector(g, tensor.Float64, WithName(\"x\"), WithShape(3))\n\ta := NewConstant(tensor.New(tensor.WithBacking(aback), tensor.WithShape(3)))\n\n\tb := NewScalar(g, tensor.Float64)\n\n\txT := tensor.New(tensor.WithBacking([]float64{1, 1, 1}), tensor.WithShape(3))\n\ty, err := Mul(x, a)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tz, err := Mul(y, b)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdz, err := Grad(z, x)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tmachine := NewTapeMachine(g)\n\tdefer machine.Close()\n\n\tmachine.Let(x, xT)\n\tmachine.Let(b, -0.5)\n\tfor turns := 0; turns < 4; turns++ {\n\t\tif err := machine.RunAll(); err != nil {\n\t\t\tt.Fatalf(\"Machine failed to run at turn %v\", turns)\n\t\t}\n\t\tmachine.Reset()\n\t}\n\n\tcorrect := []float64{-1, -1, -1}\n\tassert.Equal(correct, dz[0].Value().Data().([]float64))\n\tif _, ok := a.boundTo.(*dualValue); ok {\n\t\tt.Fatalf(\"Expected constants to not have derivatives\")\n\t}\n}\n\n// func TestIssue217(t *testing.T) {\n// \t//it works, cost = 22\n// \tif err := issue217(tensor.Shape{2, 2}, tensor.Shape{2, 2}); err != nil {\n// \t\tt.Fatal(err)\n// \t}\n\n// \t//panic: Node Σ[0](%2) :: float32, has 0 dimensions(Shape: ()). Input shape is (1, 1)...\n// \tif err := issue217(tensor.Shape{2, 2}, tensor.Shape{2, 1}); err != nil {\n// \t\tt.Fatal(err)\n// \t}\n\n// \t//panic: Node Σ[1](%2) :: float32, has 0 dimensions(Shape: ()). Input shape is (1, 1)...\n// \tif err := issue217(tensor.Shape{1, 2}, tensor.Shape{2, 2}); err != nil {\n// \t\tt.Fatal(err)\n// \t}\n// }\n\n// func issue217(xS, yS tensor.Shape) error {\n\n// \tg := NewGraph()\n// \tx := NewMatrix(g, Float32, WithName(\"x\"), WithShape(xS...), WithInit(RangedFrom(0)))\n// \ty := NewMatrix(g, Float32, WithName(\"y\"), WithShape(yS...), WithInit(RangedFrom(0)))\n\n// \tz := Must(Mul(x, y))\n// \tcost := Must(Sum(z))\n// \t//cost := Must(Mean(z))\n\n// \t_, err := Grad(cost, x, y)\n// \tif err != nil {\n// \t\treturn errors.Wrap(err, \"Grad\")\n// \t}\n\n// \tm := NewTapeMachine(g)\n// \tif err = m.RunAll(); err != nil {\n// \t\treturn errors.Wrap(err, \"Run\")\n// \t}\n// \treturn nil\n// }\n\nfunc TestIssue268_im2col(t *testing.T) {\n\tg := NewGraph()\n\tx := NewTensor(g, tensor.Float32, 4, WithShape(1, 2, 5, 5), WithInit(RangedFrom(0)))\n\tyT := tensor.New(tensor.WithShape(1, 5, 5, 18), tensor.WithBacking([]float32{\n\t\t0, 0, 0, 0, 0, 1, 0, 5, 6, 0, 0, 0, 0, 25, 26, 0, 30, 31, 0, 0, 0, 0, 1, 2, 5, 6, 7, 0, 0, 0, 25, 26, 27, 30,\n\t\t31, 32, 0, 0, 0, 1, 2, 3, 6, 7, 8, 0, 0, 0, 26, 27, 28, 31, 32, 33, 0, 0, 0, 2, 3, 4, 7, 8, 9, 0, 0, 0, 27, 28,\n\t\t29, 32, 33, 34, 0, 0, 0, 3, 4, 0, 8, 9, 0, 0, 0, 0, 28, 29, 0, 33, 34, 0, 0, 0, 1, 0, 5, 6, 0, 10, 11, 0, 25,\n\t\t26, 0, 30, 31, 0, 35, 36, 0, 1, 2, 5, 6, 7, 10, 11, 12, 25, 26, 27, 30, 31, 32, 35, 36, 37, 1, 2, 3, 6, 7, 8,\n\t\t11, 12, 13, 26, 27, 28, 31, 32, 33, 36, 37, 38, 2, 3, 4, 7, 8, 9, 12, 13, 14, 27, 28, 29, 32, 33, 34, 37, 38,\n\t\t39, 3, 4, 0, 8, 9, 0, 13, 14, 0, 28, 29, 0, 33, 34, 0, 38, 39, 0, 0, 5, 6, 0, 10, 11, 0, 15, 16, 0, 30, 31, 0, 35,\n\t\t36, 0, 40, 41, 5, 6, 7, 10, 11, 12, 15, 16, 17, 30, 31, 32, 35, 36, 37, 40, 41, 42, 6, 7, 8, 11, 12, 13, 16,\n\t\t17, 18, 31, 32, 33, 36, 37, 38, 41, 42, 43, 7, 8, 9, 12, 13, 14, 17, 18, 19, 32, 33, 34, 37, 38, 39, 42, 43,\n\t\t44, 8, 9, 0, 13, 14, 0, 18, 19, 0, 33, 34, 0, 38, 39, 0, 43, 44, 0, 0, 10, 11, 0, 15, 16, 0, 20, 21, 0, 35, 36,\n\t\t0, 40, 41, 0, 45, 46, 10, 11, 12, 15, 16, 17, 20, 21, 22, 35, 36, 37, 40, 41, 42, 45, 46, 47, 11, 12, 13, 16,\n\t\t17, 18, 21, 22, 23, 36, 37, 38, 41, 42, 43, 46, 47, 48, 12, 13, 14, 17, 18, 19, 22, 23, 24, 37, 38, 39, 42,\n\t\t43, 44, 47, 48, 49, 13, 14, 0, 18, 19, 0, 23, 24, 0, 38, 39, 0, 43, 44, 0, 48, 49, 0, 0, 15, 16, 0, 20, 21, 0,\n\t\t0, 0, 0, 40, 41, 0, 45, 46, 0, 0, 0, 15, 16, 17, 20, 21, 22, 0, 0, 0, 40, 41, 42, 45, 46, 47, 0, 0, 0, 16, 17,\n\t\t18, 21, 22, 23, 0, 0, 0, 41, 42, 43, 46, 47, 48, 0, 0, 0, 17, 18, 19, 22, 23, 24, 0, 0, 0, 42, 43, 44, 47, 48,\n\t\t49, 0, 0, 0, 18, 19, 0, 23, 24, 0, 0, 0, 0, 43, 44, 0, 48, 49, 0, 0, 0, 0,\n\t}))\n\ty, err := Im2Col(x, []int{3, 3}, []int{1, 1}, []int{1, 1}, []int{1, 1})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tmachine := NewTapeMachine(g)\n\tif err = machine.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tassert.Equal(t, yT.Shape(), y.Shape(), \"Tensors should be the same\")\n\tassert.InDeltaSlice(t, yT.Data(), y.Value().Data(), 1e-5, \"Tensors should be the same\")\n}\n\nfunc TestIssue273_maxpool_pads(t *testing.T) {\n\tg := NewGraph()\n\tx := NewTensor(g, tensor.Float32, 4, WithShape(1, 2, 5, 5), WithInit(RangedFrom(0)))\n\tyT := tensor.New(\n\t\ttensor.WithShape(1, 2, 7, 7),\n\t\ttensor.WithBacking([]float32{\n\t\t\t0, 1, 2, 3, 4, 4, 4, 5, 6, 7, 8, 9, 9, 9, 10, 11, 12, 13, 14, 14, 14, 15, 16,\n\t\t\t17, 18, 19, 19, 19, 20, 21, 22, 23, 24, 24, 24, 20, 21, 22, 23, 24, 24, 24,\n\t\t\t20, 21, 22, 23, 24, 24, 24, 25, 26, 27, 28, 29, 29, 29, 30, 31, 32, 33, 34,\n\t\t\t34, 34, 35, 36, 37, 38, 39, 39, 39, 40, 41, 42, 43, 44, 44, 44, 45, 46, 47,\n\t\t\t48, 49, 49, 49, 45, 46, 47, 48, 49, 49, 49, 45, 46, 47, 48, 49, 49, 49,\n\t\t}))\n\n\ty, err := MaxPool2D(x, []int{3, 3}, []int{2, 2}, []int{1, 1})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tmachine := NewTapeMachine(g)\n\tif err = machine.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tassert.Equal(t, yT.Shape(), y.Shape(), \"Tensors should be the same\")\n\tassert.InDeltaSlice(t, yT.Data(), y.Value().Data(), 1e-5, \"Tensors should be the same\")\n\n}\n\nfunc TestIssue233_F32(t *testing.T) {\n\tg := NewGraph()\n\txV := tensor.New(tensor.WithShape(1, 1, 5, 5), tensor.WithBacking([]float32{\n\t\t0, 0, 0, 0, 0,\n\t\t1, 1, 1, 1, 1,\n\t\t2, 2, 2, 2, 2,\n\t\t3, 3, 3, 3, 3,\n\t\t4, 4, 4, 4, 4,\n\t}))\n\tkernelV := tensor.New(tensor.WithShape(1, 1, 3, 3), tensor.WithBacking([]float32{\n\t\t1, 1, 1,\n\t\t1, 1, 1,\n\t\t1, 1, 1,\n\t}))\n\n\tx := NewTensor(g, Float32, 4, WithShape(1, 1, 5, 5), WithValue(xV), WithName(\"x\"))\n\tw := NewTensor(g, Float32, 4, WithShape(1, 1, 3, 3), WithValue(kernelV), WithName(\"w\"))\n\n\ty, err := Conv2d(x, w, tensor.Shape{3, 3}, []int{1, 1}, []int{1, 1}, []int{1, 1})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\t// logger := log.New(os.Stderr, \"\", 0)\n\t// vm := NewTapeMachine(g, WithLogger(logger), WithWatchlist(), WithValueFmt(\"%#v\"))\n\tvm := NewTapeMachine(g)\n\tif err := vm.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tcorrect := []float32{\n\t\t2, 3, 3, 3, 2,\n\t\t6, 9, 9, 9, 6,\n\t\t12, 18, 18, 18, 12,\n\t\t18, 27, 27, 27, 18,\n\t\t14, 21, 21, 21, 14,\n\t}\n\tt.Logf(\"%v\", y.Value())\n\n\tassert.Equal(t, correct, y.Value().Data())\n}\n\nfunc TestIssue233_F64(t *testing.T) {\n\tg := NewGraph()\n\txV := tensor.New(tensor.WithShape(1, 1, 5, 5), tensor.WithBacking([]float64{\n\t\t0, 0, 0, 0, 0,\n\t\t1, 1, 1, 1, 1,\n\t\t2, 2, 2, 2, 2,\n\t\t3, 3, 3, 3, 3,\n\t\t4, 4, 4, 4, 4,\n\t}))\n\tkernelV := tensor.New(tensor.WithShape(1, 1, 3, 3), tensor.WithBacking([]float64{\n\t\t1, 1, 1,\n\t\t1, 1, 1,\n\t\t1, 1, 1,\n\t}))\n\n\tx := NewTensor(g, Float64, 4, WithShape(1, 1, 5, 5), WithValue(xV), WithName(\"x\"))\n\tw := NewTensor(g, Float64, 4, WithShape(1, 1, 3, 3), WithValue(kernelV), WithName(\"w\"))\n\n\ty, err := Conv2d(x, w, tensor.Shape{3, 3}, []int{1, 1}, []int{1, 1}, []int{1, 1})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\t// logger := log.New(os.Stderr, \"\", 0)\n\t// vm := NewTapeMachine(g, WithLogger(logger), WithWatchlist(), WithValueFmt(\"%#v\"))\n\tvm := NewTapeMachine(g)\n\tif err := vm.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tcorrect := []float64{\n\t\t2, 3, 3, 3, 2,\n\t\t6, 9, 9, 9, 6,\n\t\t12, 18, 18, 18, 12,\n\t\t18, 27, 27, 27, 18,\n\t\t14, 21, 21, 21, 14,\n\t}\n\n\tassert.Equal(t, correct, y.Value().Data())\n}\n\nfunc TestIssue363(t *testing.T) {\n\tg := NewGraph()\n\n\tx := NewScalar(g, Float64, WithName(\"x\"))\n\ty := NewScalar(g, Float64, WithName(\"y\"))\n\n\tz, err := Add(x, y)\n\n\tif err != nil {\n\t\tt.Fatal(\"error adding:\", err)\n\t}\n\n\tz, err = Neg(z) //last node is Neg operator. So gradients are zero\n\n\t// z, err = gorgonia.Mul(gorgonia.NewConstant(-1.0), z) //This results in non zero gradients\n\n\tif err != nil {\n\t\tt.Fatal(\"error in Multiply with -1:\", err)\n\t}\n\n\tLet(x, 2.5)\n\tLet(y, 2.0)\n\n\tm := NewLispMachine(g)\n\n\tdefer m.Close()\n\n\terr = m.RunAll()\n\n\tif err != nil {\n\t\tt.Fatal(\"error in running the lisp machine:\", err)\n\t}\n\n\tt.Log(\"value of z:\", z.Value())\n\n\txgrad, err := x.Grad()\n\n\tif err != nil {\n\t\tt.Fatal(\"error in getting the xgrad:\", err)\n\t}\n\n\tygrad, err := y.Grad()\n\n\tif err != nil {\n\t\tt.Fatal(\"error in getting the ygrad:\", err)\n\t}\n\n\tactualxgrad := xgrad.Data().(float64)\n\n\tactualygrad := ygrad.Data().(float64)\n\n\tif actualxgrad == 0.0 {\n\t\tt.Log(\"xgrad=\", actualxgrad, \"ygrad=\", actualygrad)\n\t\tt.Fatal(\"zero xgrad\")\n\t}\n\n\tif actualygrad == 0.0 {\n\t\tt.Fatal(\"zero ygrad\")\n\t}\n\n\tt.Log(\"xgrad=\", actualxgrad, \"ygrad=\", actualygrad)\n\n}\n\nfunc TestIssue368(t *testing.T) {\n\tc := require.New(t)\n\n\tg := NewGraph()\n\tx := NewTensor(g, Float32, 2, WithShape(2, 5), WithInit(GlorotU(1.0)))\n\n\tsm, err := SoftMax(x, 1)\n\tc.NoError(err)\n\tc.NotNil(sm)\n}\n"
        },
        {
          "name": "math.go",
          "type": "blob",
          "size": 1.4140625,
          "content": "package gorgonia\n\nimport (\n\t\"math\"\n\n\t\"github.com/chewxy/math32\"\n)\n\n// functions in this file are functions that do not have an optimized/hacked up version\n// typically I'd have done some benchmark vs some hacked up version and determined that the default implementation is indeed superior\n\n/* UNARY OPS */\nfunc _signf32(x float32) float32 {\n\tif math32.Signbit(x) {\n\t\treturn float32(-1.0)\n\t}\n\treturn 1\n}\n\nfunc _signf64(x float64) float64 {\n\tif math.Signbit(x) {\n\t\treturn -1.0\n\t}\n\treturn 1\n}\n\nfunc _squaref64(x float64) float64 { return x * x }\nfunc _squaref32(x float32) float32 { return x * x }\n\nfunc _cubef64(x float64) float64 { return x * x * x }\nfunc _cubef32(x float32) float32 { return x * x * x }\n\nfunc _negf32(x float32) float32 { return -x }\nfunc _negf64(x float64) float64 { return -x }\n\n/* TODO: write optimized versions of these */\n\n// bounds acquired with this:\n/*\nfunc main() {\n\tvar buf bytes.Buffer\n\tfor i := -1000; i < 1000; i++ {\n\t\tres := math.Log1p(math.Exp(float64(i)))\n\t\tfmt.Fprintf(&buf, \"%d\\t%v\\n\", i, res)\n\t}\n\tfmt.Println(buf.String())\n}\n*/\n// I chose 16 because from 17 onwards to 709, its pretty much  returns x (with some stupid small decimals)\nfunc _softplusf64(x float64) float64 {\n\tif x < -708 {\n\t\treturn 0\n\t}\n\tif x > 16 {\n\t\treturn x\n\t}\n\treturn math.Log1p(math.Exp(x))\n}\n\nfunc _softplusf32(x float32) float32 {\n\tif x < -103 {\n\t\treturn 0\n\t}\n\tif x > 14 {\n\t\treturn x\n\t}\n\treturn float32(math.Log1p(math.Exp(float64(x))))\n}\n"
        },
        {
          "name": "math_fast.go",
          "type": "blob",
          "size": 4.9169921875,
          "content": "// +build fastmath\n\npackage gorgonia\n\nimport (\n\t\"math\"\n\t\"unsafe\"\n)\n\n// SetOptimizationLevel to i\nfunc SetOptimizationLevel(i int) { optimizationLevel = i }\n\nfunc castFU32(x float32) uint32 { return *(*uint32)(unsafe.Pointer(&x)) }\nfunc castFU64(x float64) uint64 { return *(*uint64)(unsafe.Pointer(&x)) }\nfunc castUF64(x uint64) float64 { return *(*float64)(unsafe.Pointer(&x)) }\nfunc castUF32(x uint32) float32 { return *(*float32)(unsafe.Pointer(&x)) }\n\n/*\nINVERSE/RECIPROCAL HACKS\n\nResources:\n\thttp://stackoverflow.com/questions/31555260/fast-vectorized-rsqrt-and-reciprocal-with-sse-avx-depending-on-precision\n\thttps://github.com/stgatilov/recip_rsqrt_benchmark\n\nAlso many thanks to the countless crazy people out there who have done their homework, written papers about it\nso that I may benefit from it.\n*/\n\n// magic numbers are acquired from here:\n// \t\t0x7FDE623822FC16E6 - https://www.pvk.ca/Blog/LowLevel/software-reciprocal.html\n// \t\t0x7FDE6238DA3C2118 - http://www.hackersdelight.org/hdcodetxt/recip.c.txt\n// Paul Khong's magic number seems to be the best performing for my use case, with 3 newton iterations\n//\n// On the number of refinement steps required, 4 refinement steps will yield the same\n// results as the naive function for most values. However, the gains in accuracy is offset\n// by the loss in speed gains:\n//\t\tBenchmarkInv64-8    \t300000000\t         5.99 ns/op\n//\t\tBenchmarkApp4Inv64-8\t300000000\t         5.09 ns/op\n//\t\tBenchmarkApp3Inv64-8\t500000000\t         3.70 ns/op\nfunc _inversef64(x float64) float64 {\n\tu := uint64(0x7FDE623822FC16E6) - castFU64(x)\n\t// u := uint64(0x7FDE6238DA3C2118) - castFU64(x)\n\n\tf := castUF64(u)\n\n\t// 3 newton raphson refinement steps:\n\tfor i := 0; i < 3; i++ {\n\t\tf = 2.0*f - f*f*x\n\t}\n\treturn f\n}\n\n// magic numbers acquired from here:\n//\t\thttp://bits.stephan-brumme.com/inverse.html\n// On the number of refinement steps:\n// \t\tBenchmarkInv32-8    \t500000000\t         3.85 ns/op\n//\t\tBenchmarkApp3Inv32-8\t500000000\t         3.69 ns/op\n// \t\tBenchmarkApp2Inv32-8\t1000000000\t         2.47 ns/op\n//\n// I have also found that 2 refinement steps are more than sufficient to get decent results. No funny gradient explosions for sure\n// TODO: use RCPSS when available\nfunc _inversef32(x float32) float32 {\n\tu := uint32(0x7F000000) - castFU32(x)\n\tf := castUF32(u)\n\n\t// 2 newton raphson refinement steps:\n\tfor i := 0; i < 2; i++ {\n\t\tf = 2.0*f - f*f*x\n\t}\n\treturn castUF32(u)\n}\n\n/*\nTANH HACKS\nResources:\n\t\"Speed Improvement of the Back-Propagation on Current Generation Workstations\" D. Anguita, G. Parodi and R. Zunino. Proceedings of the World Congress on Neural Networking, 1993.\n\tFuzzpillz's Tanh http://www.musicdsp.org/showone.php?id=178\n\tvarosound's lambert expansion Tanh -  https://varietyofsound.wordpress.com/2011/02/14/efficient-tanh-computation-using-lamberts-continued-fraction/\n\nHere are the relative benchmarks:\n\nfloat32\n\tBenchmarkTanh32UW-8       \t200000000\t         7.34 ns/op\n\tBenchmarkTanh32-8         \t100000000\t        10.6 ns/op\n\tBenchmarkFuzzpillsTanh32-8\t200000000\t         9.24 ns/op\n\tBenchmarkAnguitaTanh32-8  \t500000000\t         3.52 ns/op\n\tBenchmarkVarosoundTanh32-8\t200000000\t         8.96 ns/op\n\nfloat64\n\tBenchmarkTanh64UW-8       \t200000000\t         7.25 ns/op\n\tBenchmarkTanh64-8         \t200000000\t         9.64 ns/op\n\tBenchmarkFuzzpillsTanh64-8\t200000000\t         5.98 ns/op\n\tBenchmarkAnguitaTanh64-8  \t500000000\t         3.26 ns/op\n\tBenchmarkVarosoundTanh64-8\t300000000\t         6.03 ns/op\n\nThere appears to be a problem when using float32 - the conversion takes extra time.\nTanh32UW and Tanh64UW is a direct call to math.Tanh(), without a wrapping function call. The results of the float32 version is inaccurate, because if you\nwrap the results in float32(), the benchmark won't run\n\nOn the precisions, I found Anguita's the least precise, but surprisingly works well for the very limited scope of things I am doing.\nVarietyOfSound's approximation algorithm is also very close to the actual math.Tanh() implementation.\n*/\n\nfunc _tanhf32(x float32) float32 {\n\tswitch optimizationLevel {\n\tcase 0, 1:\n\t\treturn float32(math.Tanh(float64(x)))\n\tcase 2:\n\t\t// Use Anguita\n\t\tswitch {\n\t\tcase x > 1.92033:\n\t\t\treturn 0.96016\n\t\tcase x > 0:\n\t\t\treturn 0.96016 - 0.26037*(x-1.92033)*(x-1.92033)\n\t\tcase x <= -1.92033:\n\t\t\treturn -0.96016\n\t\tcase x < 0:\n\t\t\treturn 0.26037*(x+1.92033)*(x+1.92033) - 0.96016\n\t\t}\n\t}\n\tpanic(\"unreachable\")\n}\n\nfunc _tanhf64(x float64) float64 {\n\tswitch optimizationLevel {\n\tcase 0:\n\t\treturn math.Tanh(x)\n\tcase 1:\n\t\t// use Variety of Sound's\n\t\tx2 := x * x\n\t\ta := x * (135135.0 + x2*(17325.0+x2*(378.0+x2)))\n\t\tb := 135135.0 + x2*(62370.0+x2*(3150.0+x2*28.0))\n\t\treturn a / b\n\tcase 2:\n\t\t// Use Anguita\n\t\tswitch {\n\t\tcase x > 1.92033:\n\t\t\treturn 0.96016\n\t\tcase x > 0:\n\t\t\treturn 0.96016 - 0.26037*(x-1.92033)*(x-1.92033)\n\t\tcase x <= -1.92033:\n\t\t\treturn -0.96016\n\t\tcase x < 0:\n\t\t\treturn 0.26037*(x+1.92033)*(x+1.92033) - 0.96016\n\t\t}\n\t}\n\tpanic(\"unreachable\")\n}\n\nfunc _sigmoidf64(x float64) float64 {\n\treturn 0\n}\n\nfunc _sigmoidf32(x float32) float32 {\n\treturn 0\n}\n"
        },
        {
          "name": "math_nooptim.go",
          "type": "blob",
          "size": 1.052734375,
          "content": "// +build !fastmath\n\npackage gorgonia\n\n// this file holds the non-hacky version of anything that is in the math_fast.go file\n\nimport (\n\t\"math\"\n\n\t\"github.com/chewxy/math32\"\n)\n\n// SetOptimizationLevel sets the fast math optimization level. By default, fast math is turned off,\n// and this function is a no-op.\n//\n// Use the `fastmath` build tag to use fast math\nfunc SetOptimizationLevel(i int) {}\n\nfunc _inversef32(x float32) float32 { return float32(1) / x }\nfunc _inversef64(x float64) float64 { return float64(1) / x }\n\nfunc _tanhf32(x float32) float32 { return float32(math.Tanh(float64(x))) }\nfunc _tanhf64(x float64) float64 { return math.Tanh(x) }\n\nfunc _sigmoidf64(x float64) float64 {\n\tif x < -709 {\n\t\treturn 0\n\t}\n\tif x > 19 {\n\t\treturn 1\n\t}\n\n\treturn 1.0 / (1.0 + math.Exp(-x))\n}\n\nfunc _sigmoidf32(x float32) float32 {\n\tif x < -88 {\n\t\treturn 0\n\t}\n\tif x > 15 {\n\t\treturn 1\n\t}\n\treturn float32(1.0 / (1.0 + math.Exp(float64(-x))))\n}\n\nfunc _inverseSqrtf64(x float64) float64 {\n\treturn 1 / math.Sqrt(x)\n}\n\nfunc _inverseSqrtf32(x float32) float32 {\n\treturn 1 / math32.Sqrt(x)\n}\n"
        },
        {
          "name": "mathutils.go",
          "type": "blob",
          "size": 0.1005859375,
          "content": "// +build !amd64\n\npackage gorgonia\n\nfunc divmod(a, b int) (q, r int) {\n\tq = a / b\n\tr = a % b\n\treturn\n}\n"
        },
        {
          "name": "mathutils_amd64.go",
          "type": "blob",
          "size": 0.0498046875,
          "content": "package gorgonia\n\nfunc divmod(a, b int) (q, r int)\n"
        },
        {
          "name": "mathutils_amd64.s",
          "type": "blob",
          "size": 0.3623046875,
          "content": "// +build !noasm,!wasm, !arm64\n\n#include \"textflag.h\"\n\n// divmod(a, b int) (q,r int)\nTEXT ·divmod(SB),NOSPLIT,$0\n\tMOVQ\ta+0(FP), SI\n\tMOVQ\tb+8(FP), CX\n\tMOVQ\tSI, AX\n\tCMPQ\tCX, $-1\n\tJEQ\t$1, denomIsOne \t// if denominator is 1, then jump to end\n\n\tCQO\n\tIDIVQ\tCX\n\tMOVQ\tAX, q+16(FP)\n\tMOVQ\tDX, r+24(FP)\nbye:\n\tRET\ndenomIsOne:\n\tNEGQ\tAX\n\tMOVQ\tAX, q+16(FP)\n\tMOVQ\t$0, r+24(FP)\n\tJMP\tbye\n"
        },
        {
          "name": "mathutils_test.go",
          "type": "blob",
          "size": 0.5849609375,
          "content": "package gorgonia\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestDivmod(t *testing.T) {\n\tas := []int{0, 1, 2, 3, 4, 5}\n\tbs := []int{1, 2, 3, 3, 2, 3}\n\tqs := []int{0, 0, 0, 1, 2, 1}\n\trs := []int{0, 1, 2, 0, 0, 2}\n\n\tfor i, a := range as {\n\t\tb := bs[i]\n\t\teq := qs[i]\n\t\ter := rs[i]\n\n\t\tq, r := divmod(a, b)\n\t\tif q != eq {\n\t\t\tt.Errorf(\"Expected %d / %d to equal %d. Got %d instead\", a, b, eq, q)\n\t\t}\n\t\tif r != er {\n\t\t\tt.Errorf(\"Expected %d %% %d to equal %d. Got %d instead\", a, b, er, r)\n\t\t}\n\t}\n\n\tassert := assert.New(t)\n\tfail := func() {\n\t\tdivmod(1, 0)\n\t}\n\tassert.Panics(fail)\n}\n"
        },
        {
          "name": "media",
          "type": "tree",
          "content": null
        },
        {
          "name": "nn.go",
          "type": "blob",
          "size": 12.833984375,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\trng \"github.com/leesper/go_rng\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/gorgonia/internal/encoding\"\n\t\"gorgonia.org/tensor\"\n)\n\n// BinaryXent is a convenience function for doing binary crossentropy stuff.\n// The formula is as below:\n// \t\t-(y * log(prob)) - (1-y)log(1-prob)\nfunc BinaryXent(output, target *Node) (retVal *Node, err error) {\n\tvar one, oneMore *Node\n\tvar logO, omt, omo, tLogO *Node\n\n\t// which constant one to use?\n\tvar dt tensor.Dtype\n\tif dt, err = dtypeOf(output.t); err != nil {\n\t\treturn nil, errors.Wrapf(err, dtypeExtractionFail, output.t)\n\t}\n\n\tswitch dt {\n\tcase Float64:\n\t\tone = onef64\n\t\toneMore = oneMoref64\n\tcase Float32:\n\t\tone = onef32\n\t\toneMore = oneMoref32\n\tdefault:\n\t\treturn nil, errors.Errorf(nyiFail, \"BinaryXEnt\", dt)\n\t}\n\n\tif logO, err = Log(output); err != nil {\n\t\treturn nil, errors.Wrap(err, operationError)\n\t}\n\n\tif omt, err = Sub(one, target); err != nil {\n\t\treturn nil, errors.Wrap(err, operationError)\n\t}\n\n\tif omo, err = Sub(oneMore, output); err != nil {\n\t\treturn nil, errors.Wrap(err, operationError)\n\t}\n\n\tif tLogO, err = HadamardProd(target, logO); err != nil {\n\t\treturn nil, errors.Wrap(err, operationError)\n\t}\n\n\tif retVal, err = Log(omo); err != nil {\n\t\treturn nil, errors.Wrap(err, operationError)\n\t}\n\n\tif retVal, err = HadamardProd(omt, retVal); err != nil {\n\t\treturn nil, errors.Wrap(err, operationError)\n\t}\n\n\tif retVal, err = Add(tLogO, retVal); err != nil {\n\t\treturn nil, errors.Wrap(err, operationError)\n\t}\n\n\treturn Neg(retVal)\n}\n\n// Dropout is a convenience function to implement dropout.\n// It uses randomly zeroes out a *Tensor with a probability drawn from\n// a uniform distribution\nfunc Dropout(x *Node, dropProb float64) (retVal *Node, err error) {\n\trand := rng.NewUniformGenerator(time.Now().UnixNano())\n\n\top := newDropoutOp(dropProb, func() float64 { return rand.Float64Range(0, 1) })\n\n\treturn ApplyOp(op, x)\n}\n\n// LeakyRelu returns a node whose underlying value is:\n//   f(x) = alpha * x if x < 0\n//   f(x) = x for x ⩾ 0\n// applied elementwise.\nfunc LeakyRelu(x *Node, alpha float64) (*Node, error) {\n\tvar zero *Node\n\tvar dt tensor.Dtype\n\tvar err error\n\tvar alphaN *Node\n\n\t// which zero to use?\n\tif dt, err = dtypeOf(x.t); err != nil {\n\t\treturn nil, errors.Wrap(err, dtypeOfFail)\n\t}\n\tswitch dt {\n\tcase Float64:\n\t\tzero = zerof64\n\t\talphaN = NewConstant(alpha)\n\tcase Float32:\n\t\tzero = zerof32\n\t\talphaN = NewConstant(float32(alpha))\n\tdefault:\n\t\treturn nil, errors.Errorf(nyiFail, \"ReLu\", dt)\n\t}\n\n\tgteZeroOp := newElemBinOp(gteOpType, x, zero)\n\tgteZeroOp.retSame = true\n\n\txGteZeroCmp, err := ApplyOp(gteZeroOp, x, zero)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, applyOpFail)\n\t}\n\tltZeroOp := newElemBinOp(ltOpType, x, zero)\n\tltZeroOp.retSame = true\n\n\txLtZeroCmp, err := ApplyOp(ltZeroOp, x, zero)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, applyOpFail)\n\t}\n\txGteZero, err := HadamardProd(x, xGteZeroCmp)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, applyOpFail)\n\t}\n\txLtZero, err := HadamardProd(x, xLtZeroCmp)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, applyOpFail)\n\t}\n\txLtZeroAlpha, err := HadamardProd(xLtZero, alphaN)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, applyOpFail)\n\t}\n\treturn Add(xGteZero, xLtZeroAlpha)\n}\n\n// Rectify is a convenience function for creating rectified linear units activation functions.\n// This function uses ⩾, which is the canonical version. If you want to use >, you can create\n// your own by just following this.\nfunc Rectify(x *Node) (retVal *Node, err error) {\n\tvar zero *Node\n\tvar dt tensor.Dtype\n\tgroup := encoding.NewGroup(\"Rectify\")\n\n\t// which zero to use?\n\tif dt, err = dtypeOf(x.t); err != nil {\n\t\treturn nil, errors.Wrap(err, dtypeOfFail)\n\t}\n\tswitch dt {\n\tcase Float64:\n\t\tzero = zerof64\n\tcase Float32:\n\t\tzero = zerof32\n\tdefault:\n\t\treturn nil, errors.Errorf(nyiFail, \"ReLu\", dt)\n\t}\n\n\tcmp := newElemBinOp(gteOpType, x, zero)\n\tcmp.retSame = true\n\n\tif retVal, err = ApplyOp(cmp, x, zero); err != nil {\n\t\treturn nil, errors.Wrap(err, applyOpFail)\n\t}\n\tretVal.groups = retVal.groups.Upsert(group)\n\n\treturn HadamardProd(x, retVal)\n}\n\n// Im2Col converts a BCHW image block to columns. The kernel, pad and stride parameter must be shape of size 2, no more no less\n// This poor naming scheme clearly comes from matlab\nfunc Im2Col(n *Node, kernel, pad, stride, dilation tensor.Shape) (retVal *Node, err error) {\n\tif kernel.Dims() != 2 {\n\t\treturn nil, errors.Errorf(\"kernel shape is supposed to have a dim of 2\")\n\t}\n\tif pad.Dims() != 2 {\n\t\treturn nil, errors.Errorf(\"pad is supposed to have a dim of 2\")\n\t}\n\tif stride.Dims() != 2 {\n\t\treturn nil, errors.Errorf(\"strides is supposed to have a dim of 2\")\n\t}\n\tif dilation.Dims() != 2 {\n\t\treturn nil, errors.Errorf(\"dilation is supposed to have a dim of 2\")\n\t}\n\n\tif kernel[0] <= 0 || kernel[1] <= 0 {\n\t\treturn nil, errors.Errorf(\"cannot have negative or 0 in kernel shape\")\n\t}\n\n\tif stride[0] <= 0 || stride[1] <= 0 {\n\t\treturn nil, errors.Errorf(\"cannot have negative or 0 in stride: %v\", stride)\n\t}\n\n\tif pad[0] < 0 || pad[1] < 0 {\n\t\treturn nil, errors.Errorf(\"cannot have negative padding\")\n\t}\n\n\tif dilation[0] <= 0 || dilation[1] <= 0 {\n\t\treturn nil, errors.Errorf(\"cannot have negative or 0 in dilation. %v\", dilation)\n\t}\n\n\top := makeIm2ColOp(kernel[0], kernel[1], pad[0], pad[1], stride[0], stride[1], dilation[0], dilation[1])\n\treturn ApplyOp(op, n)\n}\n\n// Conv2d is a simple 2D convolution, to be used for CPU computation only.\n// If CuDNN is used, use the CUDAConv2D function.\n// These are the properties the inputs must fulfil:\n//\n// - im: must have 4D shape. Expected format is BCHW (batch, channels, height, width)\n// - filter: must have 4D shape: (batch, kernel, height, width)\n// - kernelShape: shape of the filter kernel\n// - pad: len(pad) == 2, defaults to []int{0, 0} if nil is passed\n// - stride: len(stride) == 2, example: []int{1, 1}\n// - dilation: len(dilation) == 2, defaults to []int{1, 1} if nil is passed\nfunc Conv2d(im, filter *Node, kernelShape tensor.Shape, pad, stride, dilation []int) (retVal *Node, err error) {\n\tgroup := encoding.NewGroup(\"Convolution\")\n\t// niceness for defaults\n\tif pad == nil {\n\t\tpad = []int{0, 0}\n\t}\n\tif dilation == nil {\n\t\tdilation = []int{1, 1}\n\t}\n\n\tif im.Shape().Dims() != 4 {\n\t\treturn nil, fmt.Errorf(\"im should have 4 dims, got %v dims\", im.Shape().Dims())\n\t}\n\n\tif filter.Shape().Dims() != 4 {\n\t\treturn nil, fmt.Errorf(\"filter should have 4 dims, got %v dims\", filter.Shape().Dims())\n\t}\n\n\t// checks\n\tfor _, s := range stride {\n\t\tif s <= 0 {\n\t\t\treturn nil, errors.Errorf(\"Cannot use strides of less than or equal 0: %v\", stride)\n\t\t}\n\t}\n\n\tfor _, p := range pad {\n\t\tif p < 0 {\n\t\t\treturn nil, errors.Errorf(\"Cannot use padding of less than 0: %v\", pad)\n\t\t}\n\t}\n\n\tfor _, d := range dilation {\n\t\tif d <= 0 {\n\t\t\treturn nil, errors.Errorf(\"Cannot use dilation less than or eq 0 %v\", dilation)\n\t\t}\n\t}\n\n\tvar colIm *Node\n\tif colIm, err = Im2Col(im, kernelShape, pad, stride, dilation); err != nil {\n\t\treturn nil, fmt.Errorf(\"Im2Col to failed: %w\", err)\n\t}\n\tcolIm.groups = colIm.groups.Upsert(group)\n\n\tlayer := filter.Shape()[0]\n\tkernel := filter.Shape()[1]\n\trow := filter.Shape()[2]\n\tcol := filter.Shape()[3]\n\n\tif colIm.Shape()[3] != kernel*row*col {\n\t\treturn nil, fmt.Errorf(\"%d (kernel) * %d (width) * %d (height) must be %d, got %d\", kernel, row, col, colIm.Shape()[3], kernel*row*col)\n\t}\n\n\tvar flattened *Node\n\tif flattened, err = Reshape(filter, tensor.Shape{layer, kernel * row * col}); err != nil {\n\t\treturn nil, fmt.Errorf(\"reshaping filter from %v to (%v, %v * %v * %v) failed: %w\", filter.Shape(), layer, kernel, row, col, err)\n\t}\n\tflattened.groups = flattened.groups.Upsert(group)\n\n\t// extract patch\n\tbatch := colIm.Shape()[0]\n\tm := colIm.Shape()[1]\n\tn := colIm.Shape()[2]\n\tz := colIm.Shape()[3]\n\n\tvar patch, colImLayer *Node\n\tif patch, err = Reshape(colIm, tensor.Shape{batch * m * n, z}); err != nil {\n\t\treturn nil, fmt.Errorf(\"reshaping colIm from %v to (%v * %v * %v * %v) failed: %w\", colIm.Shape(), batch, m, n, z, err)\n\t}\n\tpatch.groups = patch.groups.Upsert(group)\n\n\top := linAlgBinOp{\n\t\tāBinaryOperator: matMulOperator,\n\t\ttransA:          false,\n\t\ttransB:          true,\n\t}\n\n\tif colImLayer, err = ApplyOp(op, patch, flattened); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to apply op: %w\", err)\n\t}\n\tcolImLayer.groups = colImLayer.groups.Upsert(group)\n\n\t// now reshape and transpose the values back into the original order\n\tvar res *Node\n\tif res, err = Reshape(colImLayer, tensor.Shape{batch, m, n, layer}); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to reshape %v to (%v, %v, %v, %v): %w\", colImLayer.Shape(), batch, m, n, layer, err)\n\t}\n\tres.groups = res.groups.Upsert(group)\n\tret, err := Transpose(res, 0, 3, 1, 2)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"transpose %v failed: %w\", res.Shape(), err)\n\t}\n\n\tret.groups = ret.groups.Upsert(group)\n\treturn ret, nil\n}\n\n// Conv1d is a 1D convlution. It relies on Conv2D\nfunc Conv1d(in, filter *Node, kernel, pad, stride, dilation int) (*Node, error) {\n\treturn Conv2d(in, filter, tensor.Shape{1, kernel}, []int{0, pad}, []int{1, stride}, []int{1, dilation})\n}\n\n// MaxPool2D applies the kernel filter to the input node.\n// The pad slice can have two different lengths.\n//\n// - if len(pad) == 2, padding is assume to be symetric, and a padding is adding up *and* down to each dimension\n//   paddedOutputH = pad[0] + inputH + pad[0]\n//   paddedOutputW = pad[1] + inputW + pad[1]\n//\n// - if len(pad) == 4, padding is explicit and can be asymmetric.\n//   paddedOutputH = pad[0] + inputH + pad[1]\n//   paddedOutputW = pad[2] + inputW + pad[3]\nfunc MaxPool2D(x *Node, kernel tensor.Shape, pad, stride []int) (*Node, error) {\n\tgroup := encoding.NewGroup(\"Maxpool\")\n\txShape := x.Shape()\n\n\t// check shape\n\tif xShape.Dims() != 4 {\n\t\treturn nil, errors.Errorf(\"Expected input to have a shape with dimension 4\")\n\t}\n\tif kernel.Dims() != 2 {\n\t\treturn nil, errors.Errorf(\"Expected kernel to have a shape of dimension 2\")\n\t}\n\n\t// checks\n\tfor _, s := range stride {\n\t\tif s <= 0 {\n\t\t\treturn nil, errors.Errorf(\"Cannot use strides of less than or equal 0: %v\", stride)\n\t\t}\n\t}\n\n\tfor _, p := range pad {\n\t\tif p < 0 {\n\t\t\treturn nil, errors.Errorf(\"Cannot use padding of less than 0: %v\", pad)\n\t\t}\n\t}\n\n\th, w := xShape[2], xShape[3]\n\tkh, kw := kernel[0], kernel[1]\n\n\tpadNorth := pad[0]\n\tpadWest := pad[1]\n\tpadSouth := pad[0]\n\tpadEast := pad[1]\n\tif len(pad) == 4 {\n\t\tpadNorth = pad[0]\n\t\tpadSouth = pad[1]\n\t\tpadWest = pad[2]\n\t\tpadEast = pad[3]\n\t}\n\n\tif h-kh+padNorth+padSouth < 0 {\n\t\t// error\n\t\treturn nil, errors.New(\"Impossible height/kernel/pad combination\")\n\t}\n\n\tif w-kw+padWest+padEast < 0 {\n\t\t// error\n\t\treturn nil, errors.New(\"Impossible width/kernel/pad combination\")\n\t}\n\n\top := newMaxPoolOp(xShape, kernel, pad, stride)\n\tretVal, err := ApplyOp(op, x)\n\tretVal.groups = retVal.groups.Upsert(group)\n\treturn retVal, err\n}\n\n// MaxPool1D applies a maxpool on the node x.\nfunc MaxPool1D(x *Node, kernel, pad, stride int) (*Node, error) {\n\treturn MaxPool2D(x, tensor.Shape{1, kernel}, []int{0, pad}, []int{1, stride})\n}\n\n// BatchNorm applies a batchnormalization. This operator can be used in forward pass or for training.\n// In an evaluation only, the \"op\" output can be discared.\n// In training phase, γ, β can be discarded and the op should be used.\n// Input must be a matrix with shape (B, N) or a 4d tensor with shape (B, C, W, H)\nfunc BatchNorm(x, scale, bias *Node, momentum, epsilon float64) (retVal, γ, β *Node, op *BatchNormOp, err error) {\n\tdt, err := dtypeOf(x.Type())\n\tif err != nil {\n\t\treturn nil, nil, nil, nil, err\n\t}\n\n\tchannels := x.Shape()[1]\n\n\tmean := tensor.New(tensor.Of(dt), tensor.WithShape(channels))\n\tvariance := tensor.New(tensor.Of(dt), tensor.WithShape(channels))\n\n\tsaveMean := tensor.New(tensor.Of(dt), tensor.WithShape(channels))\n\tsaveVar := tensor.New(tensor.Of(dt), tensor.WithShape(channels))\n\talpha := tensor.New(tensor.Of(dt), tensor.WithShape(channels))\n\tbeta := tensor.New(tensor.Of(dt), tensor.WithShape(channels))\n\n\tg := x.Graph()\n\tdims := x.Shape().Dims()\n\n\tif scale == nil {\n\t\tscale = NewTensor(g, dt, dims, WithShape(x.Shape().Clone()...), WithName(x.Name()+\"_γ\"), WithInit(GlorotN(1.0)))\n\t}\n\tif bias == nil {\n\t\tbias = NewTensor(g, dt, dims, WithShape(x.Shape().Clone()...), WithName(x.Name()+\"_β\"), WithInit(GlorotN(1.0)))\n\t}\n\n\top = &BatchNormOp{\n\t\tmomentum: momentum,\n\t\tepsilon:  epsilon,\n\n\t\trunningMean:     mean,\n\t\trunningVariance: variance,\n\n\t\tsaveMean:     saveMean,\n\t\tsaveVariance: saveVar,\n\n\t\talpha: alpha,\n\t\tbeta:  beta,\n\n\t\ttraining: true,\n\t\tdims:     x.Dims(),\n\t}\n\n\tif retVal, err = ApplyOp(op, x); err != nil {\n\t\treturn nil, nil, nil, nil, err\n\t}\n\n\tretVal, err = Auto(BroadcastHadamardProd, scale, retVal)\n\tif err != nil {\n\t\treturn nil, nil, nil, nil, err\n\t}\n\n\tretVal, err = Auto(BroadcastAdd, retVal, bias)\n\n\treturn retVal, scale, bias, op, err\n}\n\n// GlobalAveragePool2D consumes an input tensor X and applies average pooling across the values in the same channel.\n// The expected input shape is BCHW where B is the batch size, C is the number of channels, and H and W are the height and the width of the data.\nfunc GlobalAveragePool2D(x *Node) (*Node, error) {\n\treturn ApplyOp(&globalAveragePoolOp{}, x)\n}\n"
        },
        {
          "name": "nn_amd64_test.go",
          "type": "blob",
          "size": 15.9267578125,
          "content": "//go:build !arm64\n// +build !arm64\n\npackage gorgonia\n\nimport \"gorgonia.org/tensor\"\n\nvar bnAllCases = []struct {\n\tdesc string\n\n\tDtype tensor.Dtype\n\n\tX      interface{}\n\tXShape tensor.Shape\n\n\tScaleInit  InitWFn\n\tScaleShape tensor.Shape\n\n\tBiasInit  InitWFn\n\tBiasShape tensor.Shape\n\n\tExpectedTrainResult, ExpectedOutputGrad, ExpectedBiasGrad, ExpectedScaleGrad, ExpectedInputGrad, ExpectedMean, ExpectedVariance interface{}\n\tExpectedEvalResult                                                                                                              interface{}\n}{\n\t{\n\t\tdesc:                \"Float32 (3,2)\",\n\t\tDtype:               tensor.Float32,\n\t\tX:                   RangedFromWithStep(0.5, 0.01),\n\t\tXShape:              tensor.Shape{3, 2},\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 2},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 2},\n\t\tExpectedTrainResult: []float32{-0.24108347, -0.53038347, -0.81968343, 0.29999986, 0.65999985, 1.0199997, 0.84108317, 1.8503832, 2.859683},\n\t\tExpectedMean:        []float32{0.4680, 0.4770},\n\t\tExpectedVariance:    []float32{0.10036002, 0.10036002},\n\t\tExpectedOutputGrad:  []float32{0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111},\n\t\tExpectedBiasGrad:    []float32{0.90000004, 1.2},\n\t\tExpectedInputGrad:   []float32{0, 1.075037e-06, 0, 1.075037e-06, 0, 1.075037e-06},\n\t\tExpectedScaleGrad:   []float32{3.1355107e-09, -2.6427998e-08},\n\t\tExpectedEvalResult:  []float32{0.31553295, 0.7072125, 1.098892, 0.3428046, 0.76857376, 1.1943429, 0.37007624, 0.82993495, 1.2897936},\n\t},\n\t{\n\t\tdesc:                \"Float32 (3,2)\",\n\t\tDtype:               tensor.Float32,\n\t\tX:                   []float32{-0.1607, -0.3214, 0.2000, 0.4000, 0.5607, 1.1214},\n\t\tXShape:              tensor.Shape{3, 2},\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 2},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 2},\n\t\tExpectedTrainResult: []float32{-0.2511225, -0.5524657, -0.8538088, 0.3, 0.66, 1.02, 0.8511225, 1.8724657, 2.8938088},\n\t\tExpectedMean:        []float32{0.17999999, 0.35999998},\n\t\tExpectedVariance:    []float32{0.21709406, 0.5683762},\n\t\tExpectedOutputGrad:  []float32{0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111},\n\t\tExpectedInputGrad:   []float32{0, 3.0357402e-08, 0, 3.0357402e-08, 0, 3.0357402e-08},\n\t\tExpectedBiasGrad:    []float32{0.90000004, 1.2},\n\t\tExpectedScaleGrad:   []float32{9.5399075e-09, 1.7419076e-08},\n\t\tExpectedEvalResult:  []float32{-0.10514938, -0.22614017, -0.34713092, 0.29286906, 0.65569556, 1.0185219, 0.69088745, 1.5375311, 2.3841748},\n\t},\n\t{\n\t\tdesc:                \"Float32 (3,4)\",\n\t\tDtype:               tensor.Float32,\n\t\tX:                   RangedFromWithStep(0.5, 0.01),\n\t\tXShape:              tensor.Shape{3, 4},\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 4},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 4},\n\t\tExpectedTrainResult: []float32{-1.4914193, -3.4799783, -5.468537, 1.8, 4.1999993, 6.5999985, 5.091419, 11.879976, 18.668533},\n\t\tExpectedMean:        []float32{0.486, 0.495, 0.50400007, 0.513},\n\t\tExpectedVariance:    []float32{0.10144002, 0.10144002, 0.10144002, 0.10144002},\n\t\tExpectedOutputGrad:  []float32{0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111},\n\t\tExpectedInputGrad:   []float32{0, 0, -1.6348671e-06, 0, 0, 0, -1.6348671e-06, 0, 0, 0, -1.6348671e-06, 0},\n\t\tExpectedBiasGrad:    []float32{1.4999999, 1.8, 2.1, 2.3999999},\n\t\tExpectedScaleGrad:   []float32{-4.7683716e-07, 0, -6.556511e-07, 0},\n\t\tExpectedEvalResult:  []float32{1.8381538, 4.3252144, 6.8122754, 2.1725607, 5.1176443, 8.062727, 2.5069678, 5.9100733, 9.313179},\n\t},\n\t{\n\t\tdesc:                \"Float64 (3,4)\",\n\t\tDtype:               tensor.Float64,\n\t\tX:                   RangedFromWithStep(0.5, 0.01),\n\t\tXShape:              tensor.Shape{3, 4},\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 4},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 4},\n\t\tExpectedTrainResult: []float64{-1.491418620064566, -3.4799767801506545, -5.468534940236742, 1.8000000000000012, 4.200000000000003, 6.600000000000002, 5.091418620064569, 11.87997678015066, 18.668534940236746},\n\t\tExpectedMean:        []float64{0.48600000000000004, 0.49500000000000005, 0.5040000000000001, 0.5130000000000001},\n\t\tExpectedVariance:    []float64{0.10143999999999997, 0.10143999999999997, 0.10143999999999997, 0.10143999999999997},\n\t\tExpectedOutputGrad:  []float64{0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111},\n\t\tExpectedInputGrad:   []float64{5.075289910660611e-16, 0, 3.045173946396366e-15, 0, 5.075289910660611e-16, 0, 3.045173946396366e-15, 0, 5.075289910660611e-16, 0, 3.045173946396366e-15, 0},\n\t\tExpectedBiasGrad:    []float64{1.4999999999999998, 1.7999999999999998, 2.0999999999999996, 2.3999999999999995},\n\t\tExpectedScaleGrad:   []float64{8.881784197001252e-16, -2.1094237467877974e-15, 1.2212453270876722e-15, 2.886579864025407e-15},\n\t\tExpectedEvalResult:  []float64{1.837042238419874, 4.321883054440342, 6.806723870460809, 2.1685837337807436, 5.105526588643988, 8.04246944350723, 2.5001252291416134, 5.889170122847634, 9.278215016553652},\n\t},\n\t{\n\t\tdesc:                \"Float64 (3,2)\",\n\t\tDtype:               tensor.Float64,\n\t\tX:                   RangedFromWithStep(0.5, 0.01),\n\t\tXShape:              tensor.Shape{3, 2},\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 2},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 2},\n\t\tExpectedTrainResult: []float64{-0.24108325083793647, -0.5303831518434603, -0.8196830528489841, 0.30000000000000004, 0.66, 1.02, 0.8410832508379366, 1.8503831518434604, 2.8596830528489843},\n\t\tExpectedMean:        []float64{0.4680, 0.4770},\n\t\tExpectedVariance:    []float64{0.10035999999999998, 0.10035999999999998},\n\t\tExpectedOutputGrad:  []float64{0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111},\n\t\tExpectedBiasGrad:    []float64{0.8999999999999998, 1.2},\n\t\tExpectedScaleGrad:   []float64{0, -1.4432899320127035e-15},\n\t\tExpectedInputGrad:   []float64{0, -2.0024102777310188e-15, 0, -2.0024102777310188e-15, 0, -2.0024102777310188e-15},\n\t\tExpectedEvalResult:  []float64{0.3155331207656929, 0.7072127868293113, 1.0988924528929298, 0.3428047939563865, 0.7685740513435537, 1.194343308730721, 0.3700764671470802, 0.8299353158577962, 1.2897941645685123},\n\t},\n\t{\n\t\tdesc:                \"Float32 (3,2)\",\n\t\tDtype:               tensor.Float32,\n\t\tX:                   RangedFromWithStep(0.1, 0.001),\n\t\tXShape:              tensor.Shape{3, 2},\n\t\tScaleInit:           Ones(),\n\t\tScaleShape:          tensor.Shape{1, 2},\n\t\tBiasInit:            Zeroes(),\n\t\tBiasShape:           tensor.Shape{1, 2},\n\t\tExpectedTrainResult: []float32{-1.1239058, -1.1239058, -1.1239058, -1.0457852e-06, -1.0457852e-06, -1.0457852e-06, 1.1239038, 1.1239038, 1.1239038},\n\t\tExpectedMean:        []float32{0.0918, 0.0927},\n\t\tExpectedVariance:    []float32{0.10000362, 0.10000362},\n\t\tExpectedOutputGrad:  []float32{0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111},\n\t\tExpectedInputGrad:   []float32{0, 0, 0, 0, 0, 0},\n\t\tExpectedBiasGrad:    []float32{1, 1},\n\t\tExpectedScaleGrad:   []float32{-6.4074993e-07, 0},\n\t\tExpectedEvalResult:  []float32{0.012807339, 0.012807339, 0.012807339, 0.025703307, 0.025703307, 0.025703307, 0.038599305, 0.038599305, 0.038599305},\n\t},\n\t{\n\t\tdesc:                \"Float64 (2,2,2,2)\",\n\t\tDtype:               tensor.Float64,\n\t\tX:                   RangedFromWithStep(0.1, 2),\n\t\tXShape:              tensor.Shape{2, 2, 2, 2},\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 2, 1, 1},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 2, 1, 1},\n\t\tExpectedTrainResult: []float64{-1.328982312548889, -3.8900518748612214, 9.419065872104541, 23.500135434416876},\n\t\tExpectedVariance:    []float64{71.07142857142858, 71.07142857142858},\n\t\tExpectedOutputGrad:  []float64{0.25, 0.25, 0.25, 0.25},\n\t\tExpectedInputGrad:   []float64{-0.005966950859558195, -0.000942150342331545, 0.004082650174895101, 0.009107450692121748, -0.011933901719116366, -0.001884300684663068, 0.008165300349790232, 0.018214901384243514, -0.009107450692121745, -0.004082650174895094, 0.000942150342331552, 0.005966950859558199, -0.018214901384243528, -0.008165300349790234, 0.0018843006846630658, 0.011933901719116352},\n\t\tExpectedMean:        []float64{9.990000000000002, 17.189999999999998},\n\t\tExpectedBiasGrad:    []float64{7.799999999999999, 12.599999999999998},\n\t\tExpectedScaleGrad:   []float64{0.3611575330864715, 0.36115753308647924},\n\t\tExpectedEvalResult:  []float64{-0.13753229022631047, -0.9806340999853157, 9.91220707244465, 24.738135347525837},\n\t},\n\t{\n\t\tdesc:                \"Float32 (2,2,2,2)\",\n\t\tDtype:               tensor.Float32,\n\t\tX:                   RangedFromWithStep(0.1, 2),\n\t\tXShape:              tensor.Shape{2, 2, 2, 2},\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 2, 1, 1},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 2, 1, 1},\n\t\tExpectedTrainResult: []float32{-1.328982312548889, -3.8900518748612214, 9.419065872104541, 23.500135434416876},\n\t\tExpectedVariance:    []float32{71.07142857142858, 71.07142857142858},\n\t\tExpectedOutputGrad:  []float32{0.25, 0.25, 0.25, 0.25},\n\t\tExpectedInputGrad:   []float32{-0.0059669508595582, -0.0009421503423315501, 0.004082650174895096, 0.009107450692121741, -0.011933901719116317, -0.0018843006846630268, 0.008165300349790263, 0.01821490138424354, -0.009107450692121745, -0.004082650174895095, 0.0009421503423315506, 0.005966950859558196, -0.01821490138424354, -0.00816530034979025, 0.0018843006846630404, 0.011933901719116315},\n\t\tExpectedMean:        []float32{9.990000000000002, 17.189999999999998},\n\t\tExpectedBiasGrad:    []float32{7.799999999999999, 12.599999999999998},\n\t\tExpectedScaleGrad:   []float32{0.3611575330864715, 0.36115753308647924},\n\t\tExpectedEvalResult:  []float32{-0.13753274, -0.9806347, 9.912203, 24.738127},\n\t},\n\t{\n\t\tdesc:                \"Float64 (2,3,2,2)\",\n\t\tDtype:               tensor.Float64,\n\t\tX:                   RangedFromWithStep(0.1, 2),\n\t\tXShape:              tensor.Shape{2, 3, 2, 2},\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 3, 1, 1},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 3, 1, 1},\n\t\tExpectedTrainResult: []float64{-4.911299133806133, -13.11268793455021, 28.356069578276426, 71.11745837902049},\n\t\tExpectedMean:        []float64{13.590000000000002, 20.79, 27.99},\n\t\tExpectedVariance:    []float64{153.35714285714286, 153.3571428571429, 153.35714285714286},\n\t\tExpectedOutputGrad:  []float64{0.25, 0.25, 0.25, 0.25},\n\t\tExpectedBiasGrad:    []float64{10.199999999999998, 14.999999999999996, 19.799999999999997},\n\t\tExpectedScaleGrad:   []float64{0.24576956790840931, 0.24576956790841376, 0.24576956790841997},\n\t\tExpectedInputGrad:   []float64{-0.004601993984850203, -0.00103915997196924, 0.002523674040911718, 0.006086508053792685, -0.009203987969700385, -0.0020783199439384737, 0.005047348081823455, 0.012173016107585385, -0.013805981954550545, -0.003117479915907676, 0.007571022122735192, 0.018259524161378098, -0.00608650805379268, -0.002523674040911718, 0.0010391599719692401, 0.004601993984850207, -0.012173016107585385, -0.0050473480818234735, 0.0020783199439384547, 0.009203987969700383, -0.01825952416137806, -0.007571022122735192, 0.0031174799159076746, 0.013805981954550578},\n\t\tExpectedEvalResult:  []float64{-0.963147483026699, -3.5947482385463116, 30.651455733371414, 76.57899246632469},\n\t},\n\t{\n\t\tdesc:                \"Float32 (2,3,2,2)\",\n\t\tDtype:               tensor.Float32,\n\t\tX:                   RangedFromWithStep(0.1, 2),\n\t\tXShape:              tensor.Shape{2, 3, 2, 2},\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 3, 1, 1},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 3, 1, 1},\n\t\tExpectedTrainResult: []float32{-4.911299133806133, -13.11268793455021, 28.356069578276426, 71.11745837902049},\n\t\tExpectedMean:        []float32{13.590000000000002, 20.79, 27.99},\n\t\tExpectedVariance:    []float32{153.35714285714286, 153.3571428571429, 153.35714285714286},\n\t\tExpectedOutputGrad:  []float32{0.25, 0.25, 0.25, 0.25},\n\t\tExpectedBiasGrad:    []float32{10.199999999999998, 14.999999999999996, 19.799999999999997},\n\t\tExpectedScaleGrad:   []float32{0.24577019, 0.24577145, 0.24576364},\n\t\tExpectedInputGrad:   []float32{-0.0046019927, -0.0010391591, 0.002523677, 0.00608651, -0.009203983, -0.0020783104, 0.0050473553, 0.012173022, -0.013806061, -0.0031175415, 0.00757096, 0.018259479, -0.00608651, -0.0025236772, 0.001039159, 0.004601992, -0.012173033, -0.0050473614, 0.002078304, 0.0092039695, -0.018259495, -0.007570976, 0.003117525, 0.013806043},\n\t\tExpectedEvalResult:  []float32{-0.96314955, -3.5947528, 30.651447, 76.57899},\n\t},\n}\n\nvar bnstackedCases = []struct {\n\tdesc string\n\n\tEpochs int\n\n\tDtype tensor.Dtype\n\n\tXInit  InitWFn\n\tXShape tensor.Shape\n\n\tScaleInit  InitWFn\n\tScaleShape tensor.Shape\n\n\tBiasInit  InitWFn\n\tBiasShape tensor.Shape\n\n\tExpectedTrainResult, ExpectedOutputGrad, ExpectedBiasGrad, ExpectedScaleGrad, ExpectedMean, ExpectedVariance interface{}\n\tExpectedEvalResult                                                                                           interface{}\n}{\n\t{\n\t\tdesc:                \"Example (1d Float32)\",\n\t\tDtype:               tensor.Float32,\n\t\tEpochs:              1,\n\t\tXInit:               RangedFromWithStep(float32(0.5), float32(0.01)),\n\t\tXShape:              tensor.Shape{3, 2},\n\t\tScaleInit:           RangedFromWithStep(float32(0.3), float32(0.3)),\n\t\tScaleShape:          tensor.Shape{1, 2},\n\t\tBiasInit:            RangedFromWithStep(float32(0.2), float32(0.2)),\n\t\tBiasShape:           tensor.Shape{1, 2},\n\t\tExpectedTrainResult: []float32{-0.16740213, -0.33483604, 0.19999963, 0.39999926, 0.5674025, 1.1348367},\n\t\tExpectedMean:        []float32{0.18000033, 0.36000067},\n\t\tExpectedVariance:    []float32{0.21710846, 0.56843376},\n\t\tExpectedOutputGrad:  []float32{0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666},\n\t\tExpectedBiasGrad:    []float32{0.5, 0.5},\n\t\tExpectedScaleGrad:   []float32{1.6863456e-08, -8.432093e-09},\n\t\tExpectedEvalResult:  []float32{0.15759754, 0.40007672, 0.17100696, 0.43021968, 0.18441638, 0.4603627},\n\t},\n\t{\n\t\tdesc:                \"Example (1d Float64)\",\n\t\tDtype:               tensor.Float64,\n\t\tEpochs:              1,\n\t\tXInit:               RangedFromWithStep(0.5, 0.01),\n\t\tXShape:              tensor.Shape{3, 2},\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 2},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 2},\n\t\tExpectedTrainResult: []float64{-0.1674022853682372, -0.33483633412378944, 0.19999999999999998, 0.4, 0.5674022853682372, 1.1348363341237895},\n\t\tExpectedMean:        []float64{0.17999999999999985, 0.36000000000000015},\n\t\tExpectedVariance:    []float64{0.21710843373493974, 0.568433734939759},\n\t\tExpectedOutputGrad:  []float64{0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666},\n\t\tExpectedBiasGrad:    []float64{0.5, 0.5},\n\t\tExpectedScaleGrad:   []float64{1.6863456e-08, -8.432093e-09},\n\t\tExpectedEvalResult:  []float64{0.1823859733138815, 0.4315682494492552, 0.1945792995638166, 0.4617112715803343, 0.20677262581375164, 0.4918542937114134},\n\t},\n}\n"
        },
        {
          "name": "nn_arm64_test.go",
          "type": "blob",
          "size": 15.859375,
          "content": "//go:build arm64\n// +build arm64\n\npackage gorgonia\n\nimport \"gorgonia.org/tensor\"\n\nvar bnAllCases = []struct {\n\tdesc string\n\n\tDtype tensor.Dtype\n\n\tX      interface{}\n\tXShape tensor.Shape\n\n\tScaleInit  InitWFn\n\tScaleShape tensor.Shape\n\n\tBiasInit  InitWFn\n\tBiasShape tensor.Shape\n\n\tExpectedTrainResult, ExpectedOutputGrad, ExpectedBiasGrad, ExpectedScaleGrad, ExpectedInputGrad, ExpectedMean, ExpectedVariance interface{}\n\tExpectedEvalResult                                                                                                              interface{}\n}{\n\t{\n\t\tdesc:                \"Float32 (3,2)\",\n\t\tDtype:               tensor.Float32,\n\t\tX:                   RangedFromWithStep(0.5, 0.01),\n\t\tXShape:              tensor.Shape{3, 2},\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 2},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 2},\n\t\tExpectedTrainResult: []float32{-0.24108347, -0.53038347, -0.81968343, 0.29999986, 0.65999985, 1.0199997, 0.84108317, 1.8503832, 2.859683},\n\t\tExpectedMean:        []float32{0.4680, 0.4770},\n\t\tExpectedVariance:    []float32{0.10036002, 0.10036002},\n\t\tExpectedOutputGrad:  []float32{0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111},\n\t\tExpectedBiasGrad:    []float32{0.90000004, 1.2},\n\t\tExpectedInputGrad:   []float32{0, 0, 0, 0, 0, 0},\n\t\tExpectedScaleGrad:   []float32{-4.7683716e-07, -9.536743e-07},\n\t\tExpectedEvalResult:  []float32{0.31728277, 0.71138656, 1.1054902, 0.34562534, 0.7753104, 1.2049953, 0.37396795, 0.83923423, 1.3045003},\n\t},\n\t{\n\t\tdesc:                \"Float32 (3,2)\",\n\t\tDtype:               tensor.Float32,\n\t\tX:                   []float32{-0.1607, -0.3214, 0.2000, 0.4000, 0.5607, 1.1214},\n\t\tXShape:              tensor.Shape{3, 2},\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 2},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 2},\n\t\tExpectedTrainResult: []float32{-0.2511225, -0.5524657, -0.8538088, 0.3, 0.66, 1.02, 0.8511225, 1.8724657, 2.8938088},\n\t\tExpectedMean:        []float32{0.17999999, 0.35999998},\n\t\tExpectedVariance:    []float32{0.21709406, 0.5683762},\n\t\tExpectedOutputGrad:  []float32{0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111},\n\t\tExpectedInputGrad:   []float32{0, 0, 0, 0, 0, 0},\n\t\tExpectedBiasGrad:    []float32{0.90000004, 1.2},\n\t\tExpectedScaleGrad:   []float32{9.5399075e-09, 1.7419076e-08},\n\t\tExpectedEvalResult:  []float32{-0.10514938, -0.22614017, -0.34713092, 0.29286906, 0.6556955, 1.0185219, 0.6908875, 1.5375313, 2.3841748},\n\t},\n\t{\n\t\tdesc:                \"Float32 (3,4)\",\n\t\tDtype:               tensor.Float32,\n\t\tX:                   RangedFromWithStep(0.5, 0.01),\n\t\tXShape:              tensor.Shape{3, 4},\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 4},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 4},\n\t\tExpectedTrainResult: []float32{-1.4914193, -3.4799783, -5.468537, 1.8, 4.1999993, 6.5999985, 5.091419, 11.879976, 18.668533},\n\t\tExpectedMean:        []float32{0.486, 0.495, 0.50400007, 0.513},\n\t\tExpectedVariance:    []float32{0.10144002, 0.10144002, 0.10144002, 0.10144002},\n\t\tExpectedOutputGrad:  []float32{0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111},\n\t\tExpectedInputGrad:   []float32{0, 0, -1.6348671e-06, 0, 0, 0, -1.6348671e-06, 0, 0, 0, -1.6348671e-06, 0},\n\t\tExpectedBiasGrad:    []float32{1.4999999, 1.8, 2.1, 2.3999999},\n\t\tExpectedScaleGrad:   []float32{-1.013279e-06, 6.556511e-07, -1.0728836e-06, 1.2516975e-06},\n\t\tExpectedEvalResult:  []float32{1.8363738, 4.321069, 6.8057637, 2.166426, 5.1032343, 8.040043, 2.4964783, 5.8854003, 9.2743225},\n\t},\n\t{\n\t\tdesc:                \"Float64 (3,4)\",\n\t\tDtype:               tensor.Float64,\n\t\tX:                   RangedFromWithStep(0.5, 0.01),\n\t\tXShape:              tensor.Shape{3, 4},\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 4},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 4},\n\t\tExpectedTrainResult: []float64{-1.491418620064566, -3.4799767801506545, -5.468534940236742, 1.8000000000000012, 4.200000000000003, 6.600000000000002, 5.091418620064569, 11.87997678015066, 18.668534940236746},\n\t\tExpectedMean:        []float64{0.48600000000000004, 0.49500000000000005, 0.5040000000000001, 0.5130000000000001},\n\t\tExpectedVariance:    []float64{0.10143999999999997, 0.10143999999999997, 0.10143999999999997, 0.10143999999999997},\n\t\tExpectedOutputGrad:  []float64{0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111},\n\t\tExpectedInputGrad:   []float64{0, -1.6917633035535369e-15, 0, 3.3835266071070737e-15, 0, -1.6917633035535369e-15, 0, 3.3835266071070737e-15, 0, -1.6917633035535369e-15, 0, 3.3835266071070737e-15},\n\t\tExpectedBiasGrad:    []float64{1.4999999999999998, 1.7999999999999998, 2.0999999999999996, 2.3999999999999995},\n\t\tExpectedScaleGrad:   []float64{6.661338147750939e-16, -3.1086244689504383e-15, -3.3306690738754696e-16, 2.7755575615628914e-15},\n\t\tExpectedEvalResult:  []float64{1.8370422386313277, 4.321883054989764, 6.806723871348201, 2.168583734529824, 5.105526590601056, 8.042469446672287, 2.500125230428321, 5.889170126212348, 9.278215021996374},\n\t},\n\t{\n\t\tdesc:                \"Float64 (3,2)\",\n\t\tDtype:               tensor.Float64,\n\t\tX:                   RangedFromWithStep(0.5, 0.01),\n\t\tXShape:              tensor.Shape{3, 2},\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 2},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 2},\n\t\tExpectedTrainResult: []float64{-0.24108325083793647, -0.5303831518434603, -0.8196830528489841, 0.30000000000000004, 0.66, 1.02, 0.8410832508379366, 1.8503831518434604, 2.8596830528489843},\n\t\tExpectedMean:        []float64{0.4680, 0.4770},\n\t\tExpectedVariance:    []float64{0.10035999999999998, 0.10035999999999998},\n\t\tExpectedOutputGrad:  []float64{0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111},\n\t\tExpectedBiasGrad:    []float64{0.8999999999999998, 1.2},\n\t\tExpectedScaleGrad:   []float64{-5.551115123125783e-16, -1.1102230246251565e-15},\n\t\tExpectedInputGrad:   []float64{0, 0, 0, 0, 0, 0},\n\t\tExpectedEvalResult:  []float64{0.3155331207568479, 0.7072127868461183, 1.0988924529353887, 0.34280479394277563, 0.7685740513724157, 1.1943433088020559, 0.37007646712870335, 0.829935315898713, 1.289794164668723},\n\t},\n\t{\n\t\tdesc:                \"Float32 (3,2)\",\n\t\tDtype:               tensor.Float32,\n\t\tX:                   RangedFromWithStep(0.1, 0.001),\n\t\tXShape:              tensor.Shape{3, 2},\n\t\tScaleInit:           Ones(),\n\t\tScaleShape:          tensor.Shape{1, 2},\n\t\tBiasInit:            Zeroes(),\n\t\tBiasShape:           tensor.Shape{1, 2},\n\t\tExpectedTrainResult: []float32{-1.1239058, -1.1239058, -1.1239058, -1.0457852e-06, -1.0457852e-06, -1.0457852e-06, 1.1239038, 1.1239038, 1.1239038},\n\t\tExpectedMean:        []float32{0.0918, 0.0927},\n\t\tExpectedVariance:    []float32{0.10000362, 0.10000362},\n\t\tExpectedOutputGrad:  []float32{0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111},\n\t\tExpectedInputGrad:   []float32{0, 0, 0, 0, 0, 0},\n\t\tExpectedBiasGrad:    []float32{1, 1},\n\t\tExpectedScaleGrad:   []float32{-9.536743e-07, -8.940697e-08},\n\t\tExpectedEvalResult:  []float32{0.013384091, 0.013384091, 0.013384091, 0.026492337, 0.026492337, 0.026492337, 0.03960059, 0.03960059, 0.03960059},\n\t},\n\t{\n\t\tdesc:                \"Float64 (2,2,2,2)\",\n\t\tDtype:               tensor.Float64,\n\t\tX:                   RangedFromWithStep(0.1, 2),\n\t\tXShape:              tensor.Shape{2, 2, 2, 2},\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 2, 1, 1},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 2, 1, 1},\n\t\tExpectedTrainResult: []float64{-1.328982312548889, -3.8900518748612214, 9.419065872104541, 23.500135434416876},\n\t\tExpectedVariance:    []float64{71.07142857142858, 71.07142857142858},\n\t\tExpectedOutputGrad:  []float64{0.25, 0.25, 0.25, 0.25},\n\t\tExpectedInputGrad:   []float64{-0.005966950859558195, -0.000942150342331545, 0.004082650174895101, 0.009107450692121748, -0.01193390171911636, -0.0018843006846630643, 0.008165300349790234, 0.018214901384243518, -0.009107450692121745, -0.004082650174895094, 0.000942150342331552, 0.005966950859558199, -0.01821490138424353, -0.008165300349790234, 0.0018843006846630621, 0.011933901719116347},\n\t\tExpectedMean:        []float64{9.990000000000002, 17.189999999999998},\n\t\tExpectedBiasGrad:    []float64{7.799999999999999, 12.599999999999998},\n\t\tExpectedScaleGrad:   []float64{0.3611575330864715, 0.36115753308647924},\n\t\tExpectedEvalResult:  []float64{-0.13753229022630986, -0.9806340999853148, 9.912207072444648, 24.738135347525834},\n\t},\n\t{\n\t\tdesc:                \"Float32 (2,2,2,2)\",\n\t\tDtype:               tensor.Float32,\n\t\tX:                   RangedFromWithStep(0.1, 2),\n\t\tXShape:              tensor.Shape{2, 2, 2, 2},\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 2, 1, 1},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 2, 1, 1},\n\t\tExpectedTrainResult: []float32{-1.328982312548889, -3.8900518748612214, 9.419065872104541, 23.500135434416876},\n\t\tExpectedVariance:    []float32{71.07142857142858, 71.07142857142858},\n\t\tExpectedOutputGrad:  []float32{0.25, 0.25, 0.25, 0.25},\n\t\tExpectedInputGrad:   []float32{-0.0059669508595582, -0.0009421503423315501, 0.004082650174895096, 0.009107450692121741, -0.011933901719116317, -0.0018843006846630268, 0.008165300349790263, 0.01821490138424354, -0.009107450692121745, -0.004082650174895095, 0.0009421503423315506, 0.005966950859558196, -0.01821490138424354, -0.00816530034979025, 0.0018843006846630404, 0.011933901719116315},\n\t\tExpectedMean:        []float32{9.990000000000002, 17.189999999999998},\n\t\tExpectedBiasGrad:    []float32{7.799999999999999, 12.599999999999998},\n\t\tExpectedScaleGrad:   []float32{0.3611575330864715, 0.36115753308647924},\n\t\tExpectedEvalResult:  []float32{-0.13753273, -0.9806347, 9.912204, 24.738127},\n\t},\n\t{\n\t\tdesc:                \"Float64 (2,3,2,2)\",\n\t\tDtype:               tensor.Float64,\n\t\tX:                   RangedFromWithStep(0.1, 2),\n\t\tXShape:              tensor.Shape{2, 3, 2, 2},\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 3, 1, 1},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 3, 1, 1},\n\t\tExpectedTrainResult: []float64{-4.911299133806133, -13.11268793455021, 28.356069578276426, 71.11745837902049},\n\t\tExpectedMean:        []float64{13.590000000000002, 20.79, 27.99},\n\t\tExpectedVariance:    []float64{153.35714285714286, 153.3571428571429, 153.35714285714286},\n\t\tExpectedOutputGrad:  []float64{0.25, 0.25, 0.25, 0.25},\n\t\tExpectedBiasGrad:    []float64{10.199999999999998, 14.999999999999996, 19.799999999999997},\n\t\tExpectedScaleGrad:   []float64{0.2457695679084111, 0.24576956790841464, 0.24576956790841997},\n\t\tExpectedInputGrad:   []float64{-0.004601993984850206, -0.0010391599719692432, 0.0025236740409117155, 0.006086508053792683, -0.009203987969700385, -0.0020783199439384733, 0.005047348081823457, 0.012173016107585387, -0.01380598195455052, -0.0031174799159076573, 0.007571022122735208, 0.018259524161378108, -0.006086508053792679, -0.0025236740409117155, 0.0010391599719692432, 0.004601993984850211, -0.012173016107585387, -0.005047348081823475, 0.0020783199439384542, 0.009203987969700385, -0.018259524161378073, -0.007571022122735209, 0.003117479915907656, 0.013805981954550557},\n\t\tExpectedEvalResult:  []float64{-0.9631474830266994, -3.5947482385463116, 30.651455733371417, 76.57899246632469},\n\t},\n\t{\n\t\tdesc:                \"Float32 (2,3,2,2)\",\n\t\tDtype:               tensor.Float32,\n\t\tX:                   RangedFromWithStep(0.1, 2),\n\t\tXShape:              tensor.Shape{2, 3, 2, 2},\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 3, 1, 1},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 3, 1, 1},\n\t\tExpectedTrainResult: []float32{-4.911299133806133, -13.11268793455021, 28.356069578276426, 71.11745837902049},\n\t\tExpectedMean:        []float32{13.590000000000002, 20.79, 27.99},\n\t\tExpectedVariance:    []float32{153.35714285714286, 153.3571428571429, 153.35714285714286},\n\t\tExpectedOutputGrad:  []float32{0.25, 0.25, 0.25, 0.25},\n\t\tExpectedBiasGrad:    []float32{10.199999999999998, 14.999999999999996, 19.799999999999997},\n\t\tExpectedScaleGrad:   []float32{0.24577019, 0.24577145, 0.24576364},\n\t\tExpectedInputGrad:   []float32{-0.0046019927, -0.0010391591, 0.002523677, 0.00608651, -0.009203983, -0.0020783104, 0.0050473553, 0.012173022, -0.013806061, -0.0031175415, 0.00757096, 0.018259479, -0.00608651, -0.0025236772, 0.001039159, 0.004601992, -0.012173033, -0.0050473614, 0.002078304, 0.0092039695, -0.018259495, -0.007570976, 0.003117525, 0.013806043},\n\t\tExpectedEvalResult:  []float32{-0.9631493, -3.5947526, 30.651445, 76.57899},\n\t},\n}\n\nvar bnstackedCases = []struct {\n\tdesc string\n\n\tEpochs int\n\n\tDtype tensor.Dtype\n\n\tXInit  InitWFn\n\tXShape tensor.Shape\n\n\tScaleInit  InitWFn\n\tScaleShape tensor.Shape\n\n\tBiasInit  InitWFn\n\tBiasShape tensor.Shape\n\n\tExpectedTrainResult, ExpectedOutputGrad, ExpectedBiasGrad, ExpectedScaleGrad, ExpectedMean, ExpectedVariance interface{}\n\tExpectedEvalResult                                                                                           interface{}\n}{\n\t{\n\t\tdesc:                \"Example (1d Float32)\",\n\t\tDtype:               tensor.Float32,\n\t\tEpochs:              1,\n\t\tXInit:               RangedFromWithStep(float32(0.5), float32(0.01)),\n\t\tXShape:              tensor.Shape{3, 2},\n\t\tScaleInit:           RangedFromWithStep(float32(0.3), float32(0.3)),\n\t\tScaleShape:          tensor.Shape{1, 2},\n\t\tBiasInit:            RangedFromWithStep(float32(0.2), float32(0.2)),\n\t\tBiasShape:           tensor.Shape{1, 2},\n\t\tExpectedTrainResult: []float32{-0.16740213, -0.33483604, 0.19999963, 0.39999926, 0.5674025, 1.1348367},\n\t\tExpectedMean:        []float32{0.18000033, 0.36000067},\n\t\tExpectedVariance:    []float32{0.21710846, 0.56843376},\n\t\tExpectedOutputGrad:  []float32{0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666},\n\t\tExpectedBiasGrad:    []float32{0.5, 0.5},\n\t\tExpectedScaleGrad:   []float32{1.6863456e-08, -8.432093e-09},\n\t\tExpectedEvalResult:  []float32{0.15490656, 0.39957356, 0.16663405, 0.4294255, 0.17836154, 0.45927745},\n\t},\n\t{\n\t\tdesc:                \"Example (1d Float64)\",\n\t\tDtype:               tensor.Float64,\n\t\tEpochs:              1,\n\t\tXInit:               RangedFromWithStep(0.5, 0.01),\n\t\tXShape:              tensor.Shape{3, 2},\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 2},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 2},\n\t\tExpectedTrainResult: []float64{-0.1674022853682372, -0.33483633412378944, 0.19999999999999998, 0.4, 0.5674022853682372, 1.1348363341237895},\n\t\tExpectedMean:        []float64{0.17999999999999985, 0.36000000000000015},\n\t\tExpectedVariance:    []float64{0.21710843373493974, 0.568433734939759},\n\t\tExpectedOutputGrad:  []float64{0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666},\n\t\tExpectedBiasGrad:    []float64{0.5, 0.5},\n\t\tExpectedScaleGrad:   []float64{1.6863456e-08, -8.432093e-09},\n\t\tExpectedEvalResult:  []float64{0.18238597329890016, 0.431568249406907, 0.19457929954319464, 0.46171127151678276, 0.20677262578748914, 0.4918542936266586},\n\t},\n}\n"
        },
        {
          "name": "nn_test.go",
          "type": "blob",
          "size": 164.2001953125,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"math/rand\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n\t\"gorgonia.org/dawson\"\n\t\"gorgonia.org/tensor\"\n)\n\nvar im2colTests = []struct {\n\tkernel   tensor.Shape\n\tpad      tensor.Shape\n\tstride   tensor.Shape\n\tdilation tensor.Shape\n}{\n\t{tensor.Shape{4, 4}, tensor.Shape{0, 0}, tensor.Shape{1, 1}, tensor.Shape{1, 1}},\n\t{tensor.Shape{3, 3}, tensor.Shape{1, 1}, tensor.Shape{2, 2}, tensor.Shape{1, 1}},\n\t{tensor.Shape{3, 3}, tensor.Shape{1, 1}, tensor.Shape{3, 3}, tensor.Shape{1, 1}},\n}\n\nfunc im2colTest(t *testing.T, dt tensor.Dtype, kernel, pad, stride, dilation tensor.Shape) {\n\tassert := assert.New(t)\n\tg := NewGraph()\n\tx := NewTensor(g, dt, 4, WithShape(2, 1, 28, 28), WithInit(RangedFrom(0))) // mnist, in batches of 10\n\ty, err := Im2Col(x, kernel, pad, stride, dilation)\n\tif err != nil {\n\t\tt.Error(err)\n\t\treturn\n\t}\n\tcost := Must(Sum(y))\n\n\tgrads, err := Grad(cost, x)\n\tif err != nil {\n\t\tt.Errorf(\"error while Grad(): %v\", err)\n\t\treturn\n\t}\n\n\tm := NewTapeMachine(g, BindDualValues())\n\tdefer m.Close()\n\tif err := m.RunAll(); err != nil {\n\t\tt.Error(err)\n\t\treturn\n\t}\n\t// t.Logf(\"x: %v\", x.Value())\n\t// t.Logf(\"c: %3.3f\", cost.Value())\n\t// t.Logf(\"xG: %v\", grads[0].Value())\n\n\th := NewGraph()\n\ta := NewTensor(h, dt, 4, WithShape(2, 1, 28, 28), WithInit(RangedFrom(0)))\n\tb, err := Im2Col(a, kernel, pad, stride, dilation)\n\tif err != nil {\n\t\tt.Error(err)\n\t\treturn\n\t}\n\tcost2 := Must(Sum(b))\n\tn := NewLispMachine(h)\n\tdefer n.Close()\n\tif err = n.RunAll(); err != nil {\n\t\tt.Error(err)\n\t\treturn\n\t}\n\taG, err := a.Grad()\n\tif err != nil {\n\t\tt.Error(err)\n\t\treturn\n\t}\n\n\t// t.Logf(\"a: %v\", a.Value())\n\t// t.Logf(\"c: %3.3f\", cost2.Value())\n\t// t.Logf(\"aG: %v\", aG)\n\n\tassert.Equal(x.Value().Data(), a.Value().Data())\n\tassert.Equal(grads[0].Value().Data(), aG.Data())\n\tassert.Equal(cost.Value().Data(), cost2.Value().Data())\n}\n\nfunc TestIm2Col(t *testing.T) {\n\t// assert := assert.New(t)\n\tdts := []tensor.Dtype{tensor.Float64, tensor.Float32}\n\tfor _, dt := range dts {\n\t\tfor _, i2ct := range im2colTests {\n\t\t\tim2colTest(t, dt, i2ct.kernel, i2ct.pad, i2ct.stride, i2ct.dilation)\n\t\t}\n\t}\n}\n\nfunc TestMaxPool2D_asymmetricPadding(t *testing.T) {\n\tassert := assert.New(t)\n\ttsts := []struct {\n\t\tinputT  tensor.Tensor\n\t\toutputT tensor.Tensor\n\t\tpadding []int\n\t}{\n\t\t{\n\t\t\tinputT: tensor.New(tensor.WithShape(1, 3, 32, 32),\n\t\t\t\ttensor.WithBacking([]float32{1.7640524, 0.4001572, 0.978738, 2.2408931, 1.867558, -0.9772779, 0.95008844, -0.1513572, -0.10321885, 0.41059852, 0.14404356, 1.4542735, 0.7610377, 0.121675014, 0.44386324, 0.33367434, 1.4940791, -0.20515826, 0.3130677, -0.85409576, -2.5529897, 0.6536186, 0.8644362, -0.742165, 2.2697546, -1.4543657, 0.045758516, -0.18718386, 1.5327792, 1.4693588, 0.15494743, 0.37816253, -0.88778573, -1.9807965, -0.34791216, 0.15634897, 1.2302907, 1.2023798, -0.3873268, -0.30230275, -1.048553, -1.420018, -1.7062702, 1.9507754, -0.5096522, -0.4380743, -1.2527953, 0.7774904, -1.6138978, -0.21274029, -0.89546657, 0.3869025, -0.51080513, -1.1806322, -0.028182229, 0.42833188, 0.06651722, 0.3024719, -0.6343221, -0.36274117, -0.67246044, -0.35955316, -0.8131463, -1.7262826, 0.17742614, -0.40178093, -1.6301984, 0.46278226, -0.9072984, 0.051945396, 0.7290906, 0.12898292, 1.1394007, -1.2348258, 0.40234163, -0.6848101, -0.87079716, -0.5788497, -0.31155252, 0.05616534, -1.1651498, 0.9008265, 0.46566245, -1.5362437, 1.4882522, 1.8958892, 1.1787796, -0.17992483, -1.0707526, 1.0544517, -0.40317693, 1.222445, 0.20827498, 0.97663903, 0.3563664, 0.7065732, 0.01050002, 1.7858706, 0.12691209, 0.40198937, 1.8831507, -1.347759, -1.270485, 0.9693967, -1.1731234, 1.9436212, -0.41361898, -0.7474548, 1.922942, 1.4805148, 1.867559, 0.90604466, -0.86122566, 1.9100649, -0.26800337, 0.8024564, 0.947252, -0.15501009, 0.61407936, 0.9222067, 0.37642553, -1.0994008, 0.2982382, 1.3263859, -0.69456786, -0.14963454, -0.43515354, 1.8492638, 0.67229474, 0.40746182, -0.76991606, 0.5392492, -0.6743327, 0.031830557, -0.6358461, 0.67643327, 0.57659084, -0.20829876, 0.3960067, -1.0930616, -1.4912575, 0.4393917, 0.1666735, 0.63503146, 2.3831449, 0.94447947, -0.91282225, 1.1170163, -1.3159074, -0.4615846, -0.0682416, 1.7133427, -0.74475485, -0.82643855, -0.09845252, -0.6634783, 1.1266359, -1.0799315, -1.1474687, -0.43782005, -0.49803245, 1.929532, 0.9494208, 0.08755124, -1.2254355, 0.844363, -1.0002153, -1.5447711, 1.1880298, 0.3169426, 0.9208588, 0.31872764, 0.8568306, -0.6510256, -1.0342429, 0.6815945, -0.80340964, -0.6895498, -0.4555325, 0.017479159, -0.35399392, -1.3749512, -0.6436184, -2.2234032, 0.62523144, -1.6020577, -1.1043833, 0.05216508, -0.739563, 1.5430146, -1.2928569, 0.26705086, -0.039282817, -1.1680934, 0.5232767, -0.17154633, 0.77179056, 0.82350415, 2.163236, 1.336528, -0.36918184, -0.23937918, 1.0996596, 0.6552637, 0.64013153, -1.616956, -0.024326125, -0.7380309, 0.2799246, -0.09815039, 0.9101789, 0.3172182, 0.78632796, -0.4664191, -0.94444627, -0.4100497, -0.017020414, 0.37915173, 2.259309, -0.042257152, -0.955945, -0.34598178, -0.463596, 0.48148146, -1.540797, 0.06326199, 0.15650654, 0.23218104, -0.5973161, -0.23792173, -1.424061, -0.49331987, -0.54286146, 0.41605005, -1.1561824, 0.7811981, 1.4944845, -2.069985, 0.42625874, 0.676908, -0.63743705, -0.3972718, -0.13288058, -0.29779088, -0.30901298, -1.6760038, 1.1523316, 1.0796186, -0.81336427, -1.4664243, 0.5210649, -0.57578796, 0.14195317, -0.31932843, 0.69153875, 0.6947491, -0.7255974, -1.383364, -1.5829384, 0.6103794, -1.1888592, -0.5068163, -0.596314, -0.052567296, -1.9362798, 0.1887786, 0.52389103, 0.08842209, -0.31088617, 0.097400166, 0.39904633, -2.7725928, 1.9559124, 0.39009333, -0.6524086, -0.39095336, 0.49374178, -0.11610394, -2.0306845, 2.064493, -0.11054066, 1.0201727, -0.69204986, 1.5363771, 0.2863437, 0.60884386, -1.0452534, 1.2111453, 0.68981814, 1.3018463, -0.6280876, -0.48102713, 2.3039167, -1.0600158, -0.1359497, 1.1368914, 0.09772497, 0.5829537, -0.39944902, 0.37005588, -1.3065269, 1.6581306, -0.11816405, -0.6801782, 0.6663831, -0.4607198, -1.3342584, -1.3467175, 0.69377315, -0.15957344, -0.13370156, 1.0777438, -1.1268258, -0.7306777, -0.3848798, 0.09435159, -0.042171452, -0.2868872, -0.0616264, -0.10730527, -0.7196044, -0.812993, 0.27451634, -0.8909151, -1.1573553, -0.31229225, -0.15766701, 2.2567234, -0.7047003, 0.9432607, 0.7471883, -1.1889449, 0.77325296, -1.1838807, -2.6591723, 0.60631955, -1.7558906, 0.45093447, -0.6840109, 1.6595508, 1.0685093, -0.4533858, -0.6878376, -1.2140774, -0.44092262, -0.28035548, -0.36469355, 0.15670386, 0.5785215, 0.34965447, -0.76414394, -1.4377915, 1.3645319, -0.6894492, -0.6522936, -0.52118933, -1.8430696, -0.477974, -0.4796558, 0.6203583, 0.6984571, 0.003770889, 0.93184835, 0.339965, -0.015682112, 0.16092817, -0.19065349, -0.3948495, -0.26773354, -1.1280113, 0.2804417, -0.9931236, 0.8416313, -0.24945858, 0.04949498, 0.4938368, 0.6433145, -1.5706234, -0.20690368, 0.8801789, -1.6981058, 0.38728046, -2.2555642, -1.0225068, 0.038630553, -1.6567152, -0.98551077, -1.471835, 1.648135, 0.16422775, 0.5672903, -0.2226751, -0.35343176, -1.6164742, -0.29183736, -0.7614922, 0.8579239, 1.1411018, 1.4665787, 0.85255194, -0.5986539, -1.1158969, 0.7666632, 0.3562928, -1.7685385, 0.3554818, 0.8145198, 0.058925588, -0.18505368, -0.8076485, -1.4465348, 0.800298, -0.30911446, -0.23346666, 1.7327212, 0.6845011, 0.370825, 0.1420618, 1.5199949, 1.7195894, 0.9295051, 0.5822246, -2.094603, 0.12372191, -0.13010696, 0.09395323, 0.9430461, -2.7396772, -0.56931204, 0.26990435, -0.46684554, -1.4169061, 0.8689635, 0.27687192, -0.97110456, 0.3148172, 0.8215857, 0.005292646, 0.8005648, 0.078260176, -0.39522898, -1.1594205, -0.085930765, 0.19429293, 0.87583274, -0.11510747, 0.4574156, -0.964612, -0.78262913, -0.1103893, -1.0546285, 0.8202478, 0.46313033, 0.27909577, 0.3389041, 2.0210435, -0.4688642, -2.2014413, 0.1993002, -0.050603542, -0.51751906, -0.97882986, -0.43918952, 0.18133843, -0.5028167, 2.4124537, -0.96050435, -0.79311734, -2.28862, 0.25148442, -2.0164065, -0.53945464, -0.27567053, -0.70972794, 1.7388726, 0.99439436, 1.3191369, -0.8824188, 1.128594, 0.49600095, 0.77140594, 1.0294389, -0.90876323, -0.42431763, 0.86259604, -2.6556191, 1.5133281, 0.55313206, -0.045703962, 0.22050765, -1.0299352, -0.34994337, 1.1002843, 1.298022, 2.696224, -0.07392467, -0.65855294, -0.51423395, -1.0180418, -0.07785475, 0.38273242, -0.03424228, 1.0963469, -0.2342158, -0.34745064, -0.5812685, -1.6326345, -1.5677677, -1.179158, 1.3014281, 0.8952603, 1.3749641, -1.3322116, -1.9686247, -0.6600563, 0.17581895, 0.49869028, 1.0479722, 0.28427967, 1.7426687, -0.22260568, -0.9130792, -1.6812183, -0.8889713, 0.24211796, -0.8887203, 0.9367425, 1.4123276, -2.369587, 0.8640523, -2.239604, 0.40149906, 1.2248706, 0.064856105, -1.2796892, -0.5854312, -0.26164544, -0.18224478, -0.20289683, -0.10988278, 0.21348006, -1.2085737, -0.24201983, 1.5182612, -0.38464543, -0.4438361, 1.0781974, -2.5591846, 1.1813786, -0.63190377, 0.16392857, 0.09632136, 0.9424681, -0.26759475, -0.6780258, 1.2978458, -2.364174, 0.020334182, -1.3479254, -0.7615734, 2.0112567, -0.044595428, 0.1950697, -1.7815628, -0.7290447, 0.1965574, 0.3547577, 0.61688656, 0.008627899, 0.5270042, 0.4537819, -1.8297404, 0.037005723, 0.76790243, 0.5898798, -0.36385882, -0.8056265, -1.1183119, -0.13105401, 1.1330799, -1.9518042, -0.6598917, -1.1398025, 0.7849575, -0.5543096, -0.47063765, -0.21694957, 0.44539326, -0.392389, -3.046143, 0.5433119, 0.43904296, -0.21954103, -1.0840366, 0.35178012, 0.37923554, -0.47003287, -0.21673147, -0.9301565, -0.17858909, -1.5504293, 0.41731882, -0.9443685, 0.23810315, -1.405963, -0.5900577, -0.110489406, -1.6606998, 0.115147874, -0.37914756, -1.7423562, -1.3032428, 0.60512006, 0.895556, -0.13190864, 0.40476182, 0.22384356, 0.32962298, 1.285984, -1.5069984, 0.67646074, -0.38200897, -0.22425893, -0.30224973, -0.3751471, -1.2261962, 0.1833392, 1.670943, -0.05613302, -0.0013850428, -0.687299, -0.11747455, 0.46616644, -0.37024245, -0.45380405, 0.40326455, -0.91800475, 0.25249663, 0.8203218, 1.3599485, -0.09038201, 1.3675972, 1.0344099, -0.99621266, -1.2179385, -0.30496365, 1.0289356, -0.07228701, -0.6006576, 1.5522432, 0.28690448, -2.3205943, 0.31716064, 0.52004063, 0.22560866, 0.4497121, -0.067275606, -1.3183959, -0.370704, -0.94561577, -0.9327409, -1.2630683, 0.45248908, 0.097896144, -0.44816536, -0.64933795, -0.023423105, 1.0791948, -2.0042157, 0.37687653, -0.545712, -1.8845859, -1.945703, -0.9127835, 0.21950956, 0.39306292, -0.9389816, 1.017021, 1.4229835, 0.39608657, -0.59140265, 1.1244192, 0.7553957, 0.86740744, -0.6564637, -2.8345544, 2.116791, -1.6108783, -0.035768073, 2.3807454, 0.33057675, 0.94924647, -1.5023966, -1.7776669, -0.5327028, 1.0907497, -0.34624946, -0.7946363, 0.19796729, 1.0819352, -1.4449402, -1.210543, -0.7886692, 1.0946383, 0.23482153, 2.1321535, 0.9364457, -0.035095178, 1.2650778, 0.21149701, -0.70492136, 0.67997485, -0.6963267, -0.2903971, 1.3277828, -0.10128149, -0.8031414, -0.46433768, 1.0217906, -0.55254066, -0.38687086, -0.51029277, 0.1839255, -0.38548976, -1.6018361, -0.8871809, -0.932789, 1.2433194, 0.81267405, 0.58725935, -0.50535834, -0.81579155, -0.5075176, -1.0518801, 2.4972005, -2.2453218, 0.56400853, -1.2845523, -0.10434349, -0.98800194, -1.177629, -1.1401963, 1.7549862, -0.13298842, -0.7657022, 0.55578697, 0.010349315, 0.72003376, -1.8242567, 0.30360392, 0.7726948, -1.6615983, 0.44819528, 1.6961815, -0.014857704, 0.82140595, 0.67057043, -0.7075057, 0.039766736, -1.5669947, -0.45130304, 0.26568797, 0.7231005, 0.024612125, 0.71998376, -1.1029062, -0.10169727, 0.019279385, 1.8495913, -0.21416666, -0.49901664, 0.021351224, -0.91911346, 0.19275385, -0.3650552, -1.7913276, -0.058586553, -0.3175431, -1.6324233, -0.06713416, 1.4893559, 0.5213038, 0.6119272, -1.3414967, 0.47689837, 0.14844958, 0.5290452, 0.4226286, -1.3597807, -0.041400813, -0.75787085, -0.050084095, -0.8974009, 1.3124703, -0.8589724, -0.8989422, 0.07458641, -1.0770991, -0.4246633, -0.8299646, 1.411172, 0.78580385, -0.057469517, -0.39121705, 0.9409176, 0.4052041, 0.49805242, -0.026192237, -1.68823, -0.112465985, -0.5324899, 0.6450553, 1.0118425, -0.65795106, 0.46838522, 1.735879, -0.66771275, 1.6819217, -0.85258585, 0.022959756, -0.011145612, 0.0114989, -0.837678, -0.5911831, -0.66772026, 0.3269626, 0.33003512, 2.2259443, 1.370989, -0.50984323, 0.3248696, 0.997118, 0.030601824, -0.069641575, 0.05157494, 0.8672766, -0.84832054, -0.32566947, 0.47043315, 0.31144708, 0.23958276, -0.36980116, 0.9725358, 2.1338682, 0.4064155, -0.1931767, 0.7557403, -0.53913265, -0.74969035, 0.032808747, -2.5827966, -1.1539503, -0.34796184, -1.3533889, -1.0326431, -0.43674833, -1.6429653, -0.40607178, -0.53527015, 0.025405208, 1.154184, 0.17250441, 0.021062022, 0.099454455, 0.22739278, -1.0167387, -0.11477532, 0.30875126, -1.37076, 0.8656529, 1.0813761, -0.63137597, -0.24133779, -0.87819034, 0.69938046, -1.0612223, -0.222477, -0.8589199, 0.05095428, -1.7942293, 1.3264617, -0.9646064, 0.059894685, -0.21252304, -0.7621145, -0.88778013, 0.93639857, -0.5256406, 0.2711702, -0.80149686, -0.64718145, 0.47224715, 0.9304085, -0.17531641, -1.4219198, 1.997956, -0.8565493, -1.5415874, 2.5944245, -0.4040323, -1.4617327, -0.6834398, 0.3675449, 0.19031155, -0.8517292, 1.8227236, -0.5215797, -1.1846865, 0.9606934, 1.3290628, -0.8174931, -1.4013473, 1.0304383, -2.0473237, -1.2266216, 0.96744615, -0.055352546, -0.26393735, 0.3528166, -0.15277442, -1.2986867, 1.2760754, 1.325014, 0.20533256, 0.045134015, 2.339625, -0.27643284, -0.25957698, 0.36448124, 1.471322, 1.5927707, -0.25857264, 0.30833125, -1.3780835, -0.3119761, -0.84029037, -1.0068318, 1.6815767, -0.79228663, -0.5316059, 0.36584878, 1.2978252, 0.48111513, 2.759355, -0.074667975, 0.25871643, 0.27560067, 1.4350494, 0.5072389, -0.1162297, -0.9474886, 0.24444346, 1.4013448, -0.4103818, 0.5289436, 0.24614778, 0.86351967, -0.8047537, 2.346647, -1.2791611, -0.36555108, 0.9380925, 0.29673317, 0.82998616, -0.49610233, -0.074804984, 0.012231983, 1.5692596, 0.69042903, 0.7966721, -0.6579261, 0.9688826, 0.22558166, 1.3891454, 2.0140603, -0.30676576, -0.40630314, -0.86404496, -0.14357951, -0.38202545, 0.3595044, -0.14456682, -0.36159927, 1.0645851, -0.9378802, 0.43310794, -0.40594172, 0.7243685, 1.3852615, -0.30309826, 0.44103292, 0.17879286, -0.7994224, 0.2407875, 0.2891205, 0.41287082, -0.1983989, 0.0941923, -1.1476109, -0.35811406, 0.5559627, 0.8924739, -0.42231482, 0.10471403, 0.22805333, 0.20147994, 0.5407736, -1.8180777, -0.04932407, 0.2390336, -1.0003303, 1.6739857, 0.16155927, 1.5634048, -0.790523, -0.9073001, 0.22425222, -1.6786884, 0.2149656, 0.09721923, 1.0156653, 0.70104134, -0.41747734, -1.0974966, 1.7123052, -0.79211503, -1.0455246, -1.084856, 1.1173053, -0.5189002, -0.7537045, 0.13768983, -0.2069447, -0.67809546, 0.7539915, 1.0653155, 0.9853175, 0.7669197, 0.40262553, -1.775888, 1.6692508, 0.3019892, 0.60815644, 1.1149623, 1.4333525, 0.41839802, 0.43554616, -0.59922427, 0.03308975, -0.85416126, -0.71994054, -0.8935744, -0.15602389, 1.0490932, 3.1709747, 0.18949963, -1.3484131, 1.2649833, -0.30078387, -0.6606086, 0.20984948, -1.2406245, 0.22246316, -0.08837552, 0.098377906, 0.38141626, 0.067492254, 0.016338084, 0.2843145, 0.41540062, -1.0314825, -1.4299912, -0.061638054, -1.4327354, 0.08753147, 0.93874687, 0.6071117, -1.0481704, -0.86026245, 0.32830128, -0.4012978, -0.3166553, 0.5969065, -0.9872867, -0.40123472, -0.8000825, -1.0431294, -0.8570782, 0.67746216, 0.05182039, -0.87916064, -0.2311016, -1.6388073, -0.7333128, 2.1495745, -0.090243846, 0.73165894, -0.065488376, 0.34816924, 0.6632581, -1.1046166, -0.030936258, 1.5788652, -0.7955006, -0.56643987, -0.30769128, 0.26902407, 0.52491784, 1.2674117, 0.49949825, -0.062053125, 1.2591671, 0.70411104, -1.4956795, 2.5263681, 1.7699214, -0.16821422, 0.3779101, 1.3243587, -0.1722008, 0.7303518, 1.1045785, -1.0148259, -0.6023319, 0.9214084, 0.46081448, 0.92379653, -0.13256802, -0.28900522, -1.9986395, -1.1460004, 0.047066096, 0.82455724, 0.53117836, -0.12824197, -0.27177158, 0.21717963, 0.07821118, 1.4045455, 0.14644077, -1.481246, -1.2725581, 1.5187594, -1.1711605, 0.76449746, -0.26837274, -0.16975829, -0.13413279, 1.221385, -0.19284183, -0.033319283, -1.5308034, 0.2066905, 0.5310425, 0.23914558, 1.3978963, 0.055171356, 0.29897746, 1.648504, -1.5500141, -0.45582536, 1.4261588, 0.93612915, 0.6783801, 0.8326507, 0.3270662, 1.6315974, 0.37775916, 0.2398671, 0.15895867, 0.19286396, -1.1570172, 0.77067304, -0.13043973, 1.8219151, -0.07565047, 0.4209183, 0.24660219, -0.625557, 0.99213684, 1.9050636, -0.01477722, -0.3004788, -0.35502872, -1.8923619, -0.17781314, 0.2509981, 1.054758, 0.9600477, -0.41649908, -0.27682298, 1.1239053, -0.1734639, -0.51002955, 1.3925184, 1.0375856, 0.018791791, -0.5937774, -2.0118804, 0.5897036, -0.8963697, -1.962732, 1.5848205, 0.6479678, -1.1390082, -1.2144014, 0.8709618, -0.87797064, 1.2961498, 0.6164593, 0.53659654, 0.40469545, 0.19145088, 0.8805112, -0.45408037, 0.08595198, 0.75194657, 0.5629897, -1.1949868, -0.50040966, 0.2528035, -0.4080147, 1.7746586, -0.3931532, -0.16221845, 0.76943016, 0.33053273, -0.14527446, -0.7564935, 0.30151406, 1.0390965, 0.47909522, -0.7781835, 1.7367749, -1.4465779, -1.5826856, 0.9605572, 0.22584048, -0.54949856, -1.0985707, 2.3207998, 0.11709087, 0.53420115, 0.3178851, 0.43480796, 0.54009444, 0.732424, -0.3752224, -0.29164198, -1.7410228, -0.78030443, 0.2711128, 1.0450233, 0.59903955, -0.34069234, -1.2631729, -2.7773592, 1.151734, -0.589229, -0.44846502, 0.13157398, -1.40556, -0.34978217, 2.0234718, 0.50538695, 0.35924914, -1.5824945, 2.2436018, -1.4227949, 1.9223248, -2.115056, 1.4053655, 1.6180543, -0.8244091, 0.42258036, 0.5474806, -0.8137945, -1.4491177, -1.3177173, 0.54100823, -0.085115604, -0.564301, 0.966768, 0.5080679, -0.7554627, -1.2012016, 0.5232617, -0.53758335, 0.09920486, 1.576299, 0.5023282, -0.862267, 0.16066119, -0.95264494, 1.6085222, -0.56157875, 0.20727074, 0.30773258, 0.15925047, -1.9585489, -1.446421, -0.4523503, 0.31943184, -0.13777922, -0.9571475, -1.3484243, -0.40155753, -0.46847606, 0.51283646, -0.32631847, 0.6027077, -0.5946498, -0.25595766, -0.3480464, -0.782367, 0.6251187, -0.813596, -0.5216415, -0.07311965, -1.2973796, -0.32493496, -0.71130633, -0.38815418, -0.059928004, -0.79991364, -0.22007579, 1.3086687, -0.025798557, 1.1452621, 0.34649444, 0.7741606, -0.77445894, 0.10490716, 0.13391292, -0.6126257, -0.82282835, -1.4902654, 1.4961396, -0.9724029, 1.3462211, -0.46749318, -0.8624933, 0.62251914, -0.63119197, 0.5684589, -0.33281177, 0.4804245, -0.9681861, 0.83135104, 0.48797268, -0.9196507, 2.6429358, 0.54012305, 2.290467, 1.6002678, -0.18883479, -0.41227177, -0.4034592, -1.8300285, -0.6958351, 0.24676603, 1.5259576, -0.7727719, 0.8820566, -1.2525934, -0.58632004, -0.4576406, 0.3718111, 0.45730963, 0.9623417, 0.77083695, 0.24316822, 0.39036494, 1.5885307, -0.5109262, 0.7747283, -1.808144, 0.41133425, -0.48324955, 0.0025711823, 1.0400863, 0.16464381, 0.88518757, 1.4737648, 0.38909397, 1.171041, -0.32656097, -0.008209882, -0.5226194, 1.0429776, 0.41409135, -0.50723445, 0.15466884, 1.0415684, -0.03926799, -0.9489328, 0.13191175, -1.9805655, 0.76877064, -0.4213276, -0.46931073, 0.8756957, -1.3651628, 1.9470986, -0.48024204, -0.52325094, 1.0212247, 0.7086953, 2.4512298, -0.21120599, -0.120406635, -1.479316, -0.33210227, -0.7214313, -0.448767, -1.7441877, 1.6606076, -1.4166034, -2.8022027, -1.1884245, -0.6038396, -1.149554, 1.0983036, -0.13783918, 0.025385605, 0.61039174, 0.28601253, 0.9785673, -1.1094775, -0.5475181, 0.66596717, -2.5345545, -1.3751845, 0.50099224, -0.48024905, 0.9361076, 0.8091803, -1.1980929, 0.4066571, 1.2016978, 0.1474344, -0.97746485, 0.87938994, 0.63542455, 0.54261076, 0.71593887, -2.994613, 0.8809376, 1.8081318, 0.43663847, 0.192729, 0.69643867, 0.33822548, 0.65178126, 0.0014710003, -0.76670486, -1.0043228, -0.9981917, -1.3730426, -1.067742, 1.7612661, 0.7540957, -0.6250274, -0.3903927, 0.11255753, -0.65554506, 0.067516856, 0.77760416, -0.035742734, 0.33601573, 0.88649154, -0.27213177, 0.2847906, -0.30937758, -0.02852887, -0.32473028, -0.52886987, 0.17371185, 0.5665453, 0.14630444, 0.49872696, -0.7379318, -1.2037352, 0.4170435, 0.6878814, 0.049857266, 1.3480358, 0.9076988, 2.6805708, -0.20080851, -0.9988488, -0.7401368, -0.5654978, 0.4760314, -2.1580687, 1.3185511, -0.23929659, -0.24679355, -1.0793432, -0.11422555, 0.013239767, -0.12194493, 0.33905926, -0.58963203, -0.8958158, 0.5483281, 0.09866745, 0.19718106, 1.0590272, -1.0225644, -0.85524046, 1.2572197, -1.4828833, -1.3094121, 0.81786186, 0.23820019, 0.105232134, -0.09165941, 0.031267546, -0.09211212, 1.3554426, -0.39814812, -0.16137354, 1.7944489, 0.027509702, 2.2320163, -0.1049797, 1.367415, -1.655344, 0.15364446, -1.5844736, 0.8444543, -1.2128679, 0.28376955, -0.28219587, -1.1582032, -1.61936, -0.51104045, 1.7406294, -0.29348505, 0.91722155, -0.057042867, 0.87672675, -1.8269113, -0.40318832, 0.94940555, -0.16325495, -0.086455286, -0.4304619, 1.1493794, 0.29751435, 0.044022277, 0.64305454, 0.58822495, 0.21258704, 1.5470315, -0.060287535, 0.27808106, -0.64295256, 0.15011522, 1.5877615, -0.6432576, -1.1335928, 0.99675965, -0.14876615, 0.0960042, -0.045113303, 0.079121724, 0.8505307, -0.8391242, -1.0117741, 0.084968135, -1.6064397, -1.3730536, 1.8666831, 0.75746834, -0.010056471, 1.238007, -1.0405992, -0.31560314, 0.6234536, 0.8906717, 0.51291686, -2.5412388, -0.96808213, 0.4770681, -0.3559515, 2.5402317, 0.9265583, 0.55808187, -1.1169496, -0.03529674, 0.24120396, 1.1277837, 0.8811311, 1.0329891, -0.923912, 1.4121517, -1.3804307, -0.53591454, 0.43077114, -0.14989159, -1.0060369, -0.82154983, -1.5482544, 0.5319746, 1.2605689, -0.100393504, -0.4003488, -1.472323, 0.9132019, 2.2113044, -1.7974558, -1.0634329, -0.679593, -0.5643179, 0.22734594, 1.6142496, 1.0085973, 0.52759737, -0.7239287, -1.1196282, -0.7967753, 1.5480669, -0.0617433, -0.44683626, -0.18375573, 0.8246182, -1.3128496, 1.4148741, 0.15647626, -0.21634398, 0.44284612, 0.21839707, -0.34419647, -0.25271067, -0.86886257, 0.6563907, -0.5319938, -0.9562584, 0.16586353, 1.3291413, -0.048344623, -0.60810125, 0.40389603, 1.9367125, -1.4519055, 0.38220277, 0.20508662, 1.1615338, 0.99090916, -0.1867091, -1.6845173, 0.8065638, -0.8351927, -0.9467404, 1.1483506, -0.9108504, 1.4028448, 0.33584473, 0.3191184, 0.30726478, -1.6384237, -1.7763886, 0.21555306, 0.56800735, 0.08261103, -0.8215345, 0.018922104, -0.082034156, -0.9571581, 1.0139722, -1.7302761, 0.58874243, 0.38432342, 1.0097119, -1.0053118, 0.10140715, 2.171165, 0.66207427, 0.10058121, 0.53916126, 0.08617684, 2.190898, 0.9836362, -0.08561496, 0.25233144, -0.390798, 1.2098501, -1.4061048, -1.6047385, 1.4587147, 2.1531198, 0.4683049, 0.11273794, 0.6572677, -0.64705354, 0.17124355, 0.038908705, 0.62656426, -1.5579985, -0.5070348, 0.8449956, -0.67559385, -0.99336135, 2.042072, 0.038118, -0.57891816, -1.6923704, 0.72934633, 0.69913614, -0.2987596, -1.1022302, -0.024549423, -0.8358561, -0.9420936, -0.10321275, -1.0513904, 0.24664895, 0.60799253, -0.83963245, -1.3682451, 1.5612797, -0.94027025, -0.6599427, 0.21301717, 0.59936935, -0.2563169, 0.46079433, -0.40098616, -0.97117066, 1.4263169, 2.4884417, 1.6959696, 0.14180663, 1.8334354, 0.3557035, -0.47728628, 0.46637958, -0.09439251, -0.9831182, -0.898322, 0.8020517, -1.846532, 0.60413677, -1.6295836, -2.1211765, -1.8388466, 1.966764, -0.19623396, 0.08658318, 1.419255, 0.9341797, -1.3915052, 0.86900634, 0.18418126, -0.34167808, 0.024290914, 1.279812, -0.8859665, 0.40088567, -0.009657237, -1.7971646, -0.8022532, 0.19321355, 1.2973421, 1.001331, 0.5972125, -0.81527567, 1.801214, 0.21524046, -1.0063655, -0.18290497, 0.8962484, 0.0076174983, 0.88686466, 1.103694, 0.4005307, -0.8577026, 0.13545467, 0.045165855, 1.8593464, -1.6263219, -0.13482246, -0.5840936, 0.33510563, -2.4375644, 1.1149246, 0.013748487, -1.8447012, -0.36111313, 0.60896236, -1.5914478, 0.0032222164, -1.0574737, -0.55598503, 0.026738383, 0.18345025, -0.4707425, 0.2727964, 0.8179776, -0.27891427, 1.4315678, 1.4622141, -0.42870206, -0.63784057, -1.664173, -0.12656933, -0.36343777, 0.77905124, -1.5096616, -0.2773914, 0.9687444, -0.7303571, -0.7623615, -1.4469403, 2.6205738, -0.7474732, -1.3003469, -0.8038504, -0.7742951, -0.26938978, 0.8253722, -0.29832315, -0.9228233, -1.4513385, 0.021857359, 0.042539075, 1.5309323, 0.092447735, -0.099008314, -1.0506538, -0.30595258, -0.43847445, -0.37016416, -0.9592554, 0.5383296, -0.14244542, -0.20035347, -1.7140461, 0.4936441, 0.48701534, -0.8391294, 0.99012136, -1.3647583, -0.021869909, -0.27120733, -1.3171748, 0.18970262, 1.7025702, 0.06763423, -0.46302176, 0.44702417, 0.10572, 0.027762132, -0.4255422, 1.4219756, 0.45636335, -0.52867067, -0.10800384, -0.7408667, -0.60829115, -0.64072573, -1.1343116, 0.777277, -0.29104146, 0.5541276, -0.6701259, -0.060362495, -0.7110406, 0.71966815, -0.2484193, -0.7308736, -1.6417032, 0.27566653, -0.70838505, -0.015779218, -0.4917301, 0.9541896, 0.54414475, 0.4472121, -0.6161211, 0.46629006, 1.7148316, -0.83218604, 0.17233914, -1.649217, 1.3985621, -0.39791209, 0.7825789, -1.7232282, 1.7975394, -0.35687152, 0.54565734, 0.1508182, -0.25547078, 1.6857923, -1.6480463, 0.29871365, 0.91064566, -0.029856121, -0.11817078, -0.14268771, -1.2276365, 0.038127385, 0.51271755, 0.068599224, -0.2722761, -0.48972502, -0.27929667, 1.2577442, -2.0866349, 0.040071458, -0.3277549, 1.4558079, 0.055492226, 1.4849256, -2.12389, 0.4595849, 0.28005785, 1.3905339, -1.6413486, -0.15503581, 0.06606026, -0.4957955, 1.2165778, -0.33868217, 2.0347626, 1.0541779, 0.9508337, 0.559299, -1.0636955, -0.43109635, 0.57275134, 0.67755705, 1.3071839, -0.46744102, -0.8601534, 0.8591042, -0.8096266, 0.8733118, 1.1997361, 0.45615304, -0.35757902, 0.041082226, 0.5934659, 0.010185518, 2.1982963, -0.9906709, -1.0026686, -0.9768954, -0.58957994, -2.1789315, -0.6296504, -0.6532847, 0.078514025, 0.41780058, -1.2402164, 0.9000542, 1.8022423, -0.20828511, 1.5743712, 0.1989895, 1.9887319, 1.1172835, -1.5639046, 0.01862737, 1.054325, 0.030546581, -0.03688353, 1.2697648, -0.7098542, 0.017515613, 0.32362577, -0.33379096, -0.020129103, 0.7750233, 0.43283764, -0.80871755, -1.104124, -0.7891022, 0.0012484558, -0.15993978, -0.8319575, -0.59815043, -1.5200393, 0.4178537, -0.040018726, -1.2597873, 0.028620504, 1.342622, -0.7399359, 1.3151376, -0.32345748, 0.19782817, 0.097750805, 1.4015235, 0.15843384, -1.1419014, -1.3109704, -1.5329211, -1.7119702, 0.04613506, -0.9583745, -0.08081161, -0.70385903, -0.7707843, -0.48084533, 0.70358557, 0.92914516, 0.37117255, -0.98982257, 0.6436313, 0.68889666, 0.2746472, -0.6036204, 0.70885956, 0.42281857, -3.1168566, 0.64445204, -1.9137427, 0.6635616, -0.1540724, 1.1936116, -0.09816121, -0.88661426, -0.14735366, 1.0598063, 0.026246618, -0.11433516, 0.7435535, 0.21035936, -0.005927406, 1.36606, 1.555114, 0.61332625, -0.28595915, 1.496911, 1.1831195, 0.71889716, -1.2160766, 0.14067191, -0.7436722, -0.15901226, 0.24005693, 0.10015941, -0.4751751, 1.2729537, -1.6961312, 0.73018354, -1.8574833, 0.38259813, -0.8869043, 0.87830377, 0.08645252, 0.24770638, -1.0182793, -0.65457016, 0.2072174, 0.58356994, 2.9290962, 0.22285832, 0.9760375, -1.5569339, -1.3298919, -0.35549477, -1.1974277, 1.4863993, -0.4102187, 1.3821819, 1.4867824, 0.04277972, 0.50179976, -0.056099474, 0.538437, 0.48334184, -0.12364963, 0.50496995, 1.7236962, 0.7130162, 0.3257996, 0.124769524, -1.0126731, -1.0272969, 0.32335654, -1.3693911, -0.7663276, 1.2815113, 1.9142298, -1.665956, 1.6266496, -0.2114383, -0.0150050875, -0.11341163, 1.0805441, -1.6076766, 0.45616361, -0.9448702, 0.5707885, 1.5427964, -0.0004173264, 0.37415507, 0.40955177, -0.7995935, 1.5116394, 1.7064682, 0.70178336, 0.07328543, -0.46189383, -0.62649024, 1.7108365, 1.414415, -0.063661486, -1.5799305, -2.832012, -1.0834267, -0.13062039, 1.400689, -0.6516562, 0.50481546, 1.3031809, 0.12853631, -0.14244787, -1.3087635, -1.2024753, 0.41609964, -0.20090753, 0.12253132, -0.047277715, 0.66414404, -0.7846874, -0.33558065, 1.8961822, -0.79978615, -0.28157544, -0.5893867, 0.44478136, 1.0223923, -0.49821162, -0.43141434, -0.2789816, 0.5298338, -0.7393953, -0.37595996, -2.3721938, -1.381745, -0.11244375, 0.89786416, 0.29507577, -1.0987685, -1.4002562, 0.1746801, -1.6528037, 1.0659268, 0.063896194, -1.6073202, -0.9659539, -0.7243113, -0.7731925, -1.489933, -0.8746625, -0.6844016, -0.71128577, 1.1279566, 0.10482781, -0.9932572, -0.3346216, -0.8795571, -0.30000666, 0.87550914, 0.2522708, 2.2856011, 0.37592742, -0.9135945, 0.8097407, 1.0799313, 1.094167, -1.0942409, -0.14763741, 1.131812, -1.684729, -0.49941677, -1.4269377, -0.9325702, -1.0124571, 1.2505698, -0.23453803, -0.8633556, -1.0356058, 0.14166717, -0.0111356275, 1.3440744, 0.5000167, -1.4317977, -0.6289807, 1.0700725, -0.6210827, 1.7345722, -1.0982895, 0.57261336, -0.86121553, -0.50959516, 1.0985817, -0.12706716, 0.81345224, 0.4732906, 0.75386566, -0.8881882, -0.2215744, 0.42425263, -0.8490729, 1.6295, -0.77722806, -0.3000036, -1.006559, -2.1433082, 1.7969185, -0.20433894, -0.44791484, -0.19871506, 1.4198639, -0.9651066, 0.6795679, -0.42378825, -0.59667087, 0.5670582, 0.9882406, -0.51390296, -0.76884913, -1.1690958, 1.1035038, -0.575256, -1.8491307, 1.4099522, -1.3698595, 0.77946055, 0.18342865, 0.28791544, -0.58437526, 0.36559147, -1.6677799, 0.5880377, 1.55701, 0.8840272, -2.01954, -0.984209, -0.18779492, 0.4869373, -0.10665268, -0.4932144, 0.5953003, 1.1641518, -0.23229401, 0.7289299, -2.5790508, -0.93750936, -0.32125893, -0.48856622, 0.3327982, 1.0137506, 0.50666904, -0.62222546, -1.5227681, 0.5569641, -1.8381767, 0.6530373, -0.18844908, -1.175835, 0.2872573, -0.0028761027, -0.036597293, -0.0842233, 0.4195241, 0.924434, 0.4966152, 1.0121332, -0.04413972, 1.6184593, 0.57110983, -0.543694, -1.0938951, 0.20579681, -1.3065215, -0.973376, 0.23908707, -0.60788745, -0.93331623, -0.034475047, 0.072677895, -0.20583403, -0.3775469, 0.85464275, 0.34242734, -0.22342612, 2.4643219, 0.19383174, 1.1320051, -0.560981, -1.3629409, -0.7917565, -0.26800978, -0.4966082, 1.3363862, -0.120041125, 0.46146888, -0.046481155, -0.43355432, 0.037996013, 1.7140515, -0.76794857, 0.7669904, -1.0260073, -0.45962644, 0.0035832059, 0.3263751, 1.4831287, -0.050082643, -0.8436156, 0.650042, -0.3641698, 0.23868157, -0.11622244, -1.9434569, 0.5082992, 0.583368, 0.92660475, 1.8004627, -1.1951038, 0.51650745, 0.409295, -0.419082, 0.39710623, 0.49964696, -1.2186838, 0.24622276, -0.9179843, -0.6518565, -1.7747449, -0.47336093, -0.20357068, 0.54985684, 0.00089992664, -1.5422882, 0.86214805, -0.11858662, 0.4883706, 0.9659361, 1.4226048, 1.9612269, -0.07223876, 0.31112444, -1.078361, 1.0616002, -1.1848874, -1.8052517, 0.830386, -0.5216965, 0.77760726, 0.40807465, -1.6300026, -2.7196794, -1.0966017, 0.016491488, -1.2217764, -0.65276146, -1.4589407, 0.16987796, 0.09082593, -0.48139262, 1.3970653, 1.497715, 0.5652672, -1.7997712, -1.1046902, 0.40713033, -0.62855756, -0.48709142, 0.8989674, 0.5108748, 1.3141544, -0.4292093, 1.3752254, -0.55413127, 1.4994915, 0.10583464, -0.86050975, -1.6312195, -0.3014723, -0.2562327, 0.8576619, -0.1105905, -0.43243197, 1.0770375, -0.22482656, -0.5762418, 0.5746089, -0.48982823, 0.65880215, -0.5969171, -0.22295918, 0.15217698, -0.37412632, -0.013451469, 0.81547195, 0.4106018, 0.48096985, -0.63543046, 0.85282975, 0.66956234, 1.0044192, -0.7263658, -0.1724586, 0.6335339, -0.60881513, -0.22612247, 1.9258057, 1.951761, 1.2399405, 0.93858516, -1.0192511, 0.5125622, -0.35911658, -1.0585719, -0.50900584, 0.11566507, -0.5473556, -0.5507994, 0.7920415, 0.14410649, 0.23345809, 0.1118724, -0.67570317, -1.370572, 0.3105647, -0.5070366, -2.0107822, -0.39256725, -1.0922179, 0.69865024, 0.5216252, 0.49689314, -0.6650416, 0.7315516, 0.3196498, -0.40985453, -0.45333743, 0.8927082, -0.47360405, 0.30365646, 1.033957, 1.9093426, 1.6638731, 0.90082276, -1.5059114, -0.6890484, -0.5480872, 1.6531498, -0.69931793, 0.38616636, 0.10086706, -0.9351272, 0.38182402, 0.3982961, -1.2557749, 1.2228775, -2.08651, -0.59075713, 0.9719703, -1.1932578, 0.35026592, -1.2963604, -0.09302414, -2.3137732, -0.8425717, -1.5429214, -0.40176374, -0.4152314, -0.67366415, 0.7979132, -0.8868796, 0.63438666, 1.6292758, 0.13906415, -0.8576702, -1.2493385, -0.7097851, 0.7046427, 0.15559073, 0.93679523, 0.7703309, 0.14081065, 0.47348827, 1.8552462, 1.4156562, -0.30274603, 0.98967946, 0.58585083, 1.1363881, 0.67161655, -0.9741674, -1.6196846, 0.572627, 1.9026182, -0.7756641, -0.18808974, -1.0357478, 1.1778295, -2.305167, -2.2636602, 0.3750199, -0.082343645, -0.47962302, -0.3010948, 0.5369879, -0.413804, -1.096925, -0.9273629, 0.88833886, -0.52474195, -1.3852776, 0.10217833, 0.50499475, 1.3289608, 0.21790339, -0.65971124, 0.47400787, 0.7271749, -0.038905308, -0.04459939, 0.2601329, -0.069856495, 0.2501139, -1.0219133, -1.1504377, -0.83611137, 0.64221096, 0.25879756, 1.040239, -0.18669093, -1.1436414, 1.1445535, -0.018767055, 1.283455, 0.59794647, 2.1886187, -0.21977298, 0.90072393, 0.8913641, -0.55512637, -0.17248231, -1.4617383, -1.5487962, 0.1265688, 0.7930071, 0.63802403, 0.3400246, 0.86301714, -0.5896978, -0.27253276, 0.7375215, 0.43311873, -0.21018882, 1.3207943, -1.2920012, -0.51867867, -0.28339776, 0.8165349, 0.002385198, -1.2614918, 0.5140042, 1.0875463, 0.73930454, 0.61915493, -1.8743135, -0.8998865, 0.4820806, -0.054888185, 0.5225576, -1.2663426, -0.061494764, -1.389781, -1.9536786, 0.29577908, 0.8425888, 0.24561642, -0.03299648, -1.5620143, 1.0061071, -0.044044897, 1.9595621, 0.9423143, -2.0051255, 0.7550497, -1.3965353, -0.7594955, -0.25075668, -0.09406245, 0.39756522, -1.022855, -1.150692, 0.6006052, -0.013250268, 0.17437305, -2.1936834, -0.17713739, -0.8907292, -0.9206264, 0.9219348, -1.0956712, -1.0928966, -0.3310106, 0.45028883, -0.8840147, 1.2341441, 1.4498476, -0.8814471, -0.24508175, -0.7786755, -1.6853821, 0.30301106, 0.7335949, 2.0118642, -0.8974095, 1.336235, 1.3423537, 0.19785331, 0.6021635, 0.8732731, 1.9741, 0.47780856, -0.060137887, -0.8661688, 0.30532077, 1.0241649, 0.24461035, -0.77992326, 0.089076206, -0.12915348, 0.26473877, -1.6618484, 0.55078864, 0.59542316, 0.44485343, -0.0037628172, -1.8059362, -0.019322792, 1.060715, -0.8601289, -1.9892695, -1.540558, 0.3140257, 0.37287602, 0.8862932, -0.055258997, -1.5003284, -0.81850415, 0.8188394, 0.14049591, 0.6498296, 0.4347888, -0.20496055, -0.17400683, 1.8571023, 0.41467425, -0.12858754, 0.45542, 0.22290581, -2.1573563, 0.6500845, 1.8209393, -0.7802799, 1.4540358, -0.2568697, 0.2934714, 1.0703601, -0.72000146, 1.2424939, -1.2142173, -0.87515473, -0.59352034, 0.66200536, -0.3408744, -1.5199745, -0.21653287, -0.7842214, 0.7312936, -0.34323505, 0.07077408, -0.40547246, 0.43393898, -0.18359077, 0.3251987, -2.5933886, 0.09725088, 0.41391367, -0.19928005, 0.66939247, 0.73860705, 1.3042139, 0.10481161, -1.9138007, -2.2854993, -1.601841, -0.03790706, -0.15730529, 0.27623984, -0.6252459, -0.73649114, 0.5550479, 0.65592444, -0.25665015, -0.038476657, 0.40431434, 0.50434357, -1.1439807, -0.71957386, -1.230546, -0.5069066, 0.8123336, 0.54627186, -1.0980979, 0.51226676, 0.08584311, -0.4939267, -1.4064597, -0.17482337, 0.679944, -2.1630976, -0.3961232, 2.2542837, 0.67263675, 0.2598325, -0.7371852, -0.6783298, -0.083288394, 1.6028637, 0.4655892, -0.8721584, 1.176787, -0.2925942, 1.6973464, -0.566603, -1.0032657, 0.17462958, 0.982327, 1.0374448, 0.15919177, -0.9880967, -0.5053407, -2.018282, -0.9131215, -0.17845681, 0.38900214, -0.33945432, -0.056979056, -0.39618546, 0.7510253, -0.89911294, 0.8375479, 1.9608808, 0.47278965, -0.5270916, -0.53627014, 1.2098372, -1.1265894, -0.95380443, -1.1644485, -1.2785138, -1.0448164, 0.78990495, 1.1022825, -0.6970731, 0.20733404, 0.7591567, 0.100564204, -0.95494276, -1.4704018, 1.0104276, 0.4961794, 0.5769559, -1.107647, 0.23497719, 0.6289996, 0.31403384, -0.7450232, 1.0122606, -1.527632, 0.92874193, 1.081056, 1.5723304, -0.3424922, -0.99943, 0.79388034, -0.6992153, 0.04399551, -0.3174622, -0.90207195, 0.32099947, -1.3920159, 0.5922057, -0.9669311, -1.7317313, -0.05010746, 0.43163386, 0.5769346, 0.8183537, -2.3536403, -1.0051445, 0.1066523, 1.5190033, 0.7837445, 1.90134, -0.5249394, 0.27441698, -1.0999708, -0.40435222, -0.7352957, -0.6339887, -0.39344913, 0.00271754, 0.022212664, 0.54345345, 0.13998847, -0.34404564, -0.52257854, -0.3071317, -0.44903713, 0.49097106, 0.8655252, 1.2740445, -0.7977028, 0.4693722, -1.3946797, 0.37317473, 1.0826722, -0.14958951, 1.072636, -1.1385679, -0.8886453, -0.13580984, 1.0222104, -0.41742945, -0.4535531, -0.99162835, 0.20288104, 1.2466952, 0.70068014, 0.6966507, -0.20697448, -0.5633094, 0.6772459, -0.031911075, -0.17360823, 0.8982406, -0.19778745, -0.83777624, 0.9091885, 0.08071989, -1.0370294, -1.1129059, 0.095411874, 2.3374097, -0.3928206, -0.33627385, 1.5237712, -0.0572812, -1.4484669, -1.5727965, 1.226664, 0.66635454, 0.8261257, -0.057756558, -0.72671205, -0.21716312, 0.13603121, -0.83831114, 0.5614499, -1.2595961, -0.33275875, -0.20400788, -0.69101983, -2.2055054, 0.44786966, -0.7557508, 1.3257079, -0.34198228, -0.5413596, 0.09152195, 1.0534397, -0.56340766, 1.0147377, 1.4403037, 0.9903228, 1.6264315, 1.292646, 1.5148823, 1.6043264, 0.20806953, -0.4292239, -2.2622437, -1.3227332, -0.4482828, -0.3817351, -0.15279447, -1.0007604, -1.5957776, -0.13022317, -0.18941793, -0.80755407, -0.74215215, -0.9401566, -0.39652374, -0.8563028, 1.2598753, 0.24099673, -0.97231793, -0.28044778, -1.1802856, 1.0121683, 1.3841867, 1.252002, -1.1446927, -0.09126702, -0.40157068, 0.5620131, -1.0079098, -0.6758917, -0.41321704, 0.15328847, 0.6941287, -0.3287277, 0.66396505, 0.8220764, -0.21321523, -1.2456582, -1.1711904, 0.59172696, -0.47622442, -1.7126293, 0.61295235, 0.12955453, -1.4059671, 1.17942, 0.836636, 0.13874525, -1.2743194, -1.4023305, -0.3070685, -1.7139153, 0.40508026, -1.4108233, 0.16491273, -0.28813145, 0.71178526, -0.9379476, 0.27372944, -1.3948402, 0.7955496, -0.114961766, 0.49585068, -1.3205253, 0.49908426, 0.3062034, 0.3636979, 0.31263396, -0.19346388, 1.2412993, -0.15589799, -0.7391692, -0.05872619, -0.95051795, -0.4639964, -0.17724662, -0.37955412, 0.19939707, 1.9457614, 0.57094985, 1.0723007, -0.50370944, -0.5870163, -0.37817806, 0.8528891, -2.1481185, -1.0331647, 0.10233585, -0.22409236, 1.9677297, 0.44768322, -0.66219145, -1.577607, -0.34056005, -1.30322, 0.46675065, 0.16110632, 0.32003194, 2.0791767, -0.907466, -0.19240421, -1.2125157, -0.08059852, 1.5932736, 0.5687224, -0.114487045, 0.25163025, -1.2108556, -0.3937337, 0.085252576, 0.099421985, -1.5306163, 0.3276232, 0.2791965, -0.3770512, 0.004174999, -1.4834915, -1.4797956, 0.13468726, -0.6677232, -0.01155552, 0.83949065, -0.17392993, -2.810668, -0.15065365, -0.48104402, -0.23469436, 0.8997308, -1.5785302, 0.24395663, 1.5703039, -0.6259431, 0.4723279, 0.9663058, 0.21023144, -0.685097, -0.709521, 0.74380016, 0.5921491, -0.7864684, -1.1764731, -1.2808067, 1.6616518, -0.06794512, 2.3602285, 0.5555456, 0.43952233, 0.30627248, 0.99914986, -0.9660632, 2.1600132, -0.100301705, -0.7034001, 0.302561, 1.0923389, -1.0075549, 0.5668694, -0.71644413, -0.5062735, -0.48948243, 0.76354146, -1.1090727, 0.1926161, -0.34341785, -0.84721017, -1.2135236, -1.2028884, -1.633796, 0.8961672, -0.24165316, 0.15865193, 1.1781894, -1.2201172, -0.94154567, 0.25471553}),\n\t\t\t),\n\t\t\toutputT: tensor.New(tensor.WithShape(1, 3, 32, 32),\n\t\t\t\ttensor.WithBacking([]float32{1.7640524, 0.978738, 2.2408931, 2.2408931, 1.867558, 1.2023798, 0.95008844, -0.10321885, 0.41059852, 0.41059852, 1.9507754, 1.9507754, 0.7610377, 0.44386324, 0.7774904, 1.4940791, 1.4940791, 0.3130677, 0.3869025, 0.3869025, 0.6536186, 0.8644362, 0.8644362, 2.2697546, 2.2697546, 0.3024719, 0.045758516, 1.5327792, 1.5327792, 1.4693588, 0.37816253, 0.37816253, 0.17742614, -0.34791216, 0.46278226, 1.2302907, 1.2302907, 1.2023798, 0.7290906, 1.1394007, 1.1394007, 0.40234163, 1.9507754, 1.9507754, -0.4380743, -0.31155252, 0.7774904, 0.7774904, 0.9008265, 0.9008265, 0.46566245, 1.4882522, 1.8958892, 1.8958892, 1.1787796, 0.42833188, 1.0544517, 1.0544517, 1.222445, 1.222445, 0.97663903, 0.97663903, 0.7065732, 0.7065732, 1.7858706, 1.7858706, 0.46278226, 1.8831507, 1.8831507, 0.7290906, 0.9693967, 1.1394007, 1.9436212, 1.9436212, 0.40234163, 1.922942, 1.922942, 1.867559, 1.867559, 0.90604466, 1.9100649, 1.9100649, 0.8024564, 1.4882522, 1.8958892, 1.8958892, 1.1787796, 0.9222067, 1.0544517, 1.0544517, 1.3263859, 1.3263859, 0.97663903, 0.97663903, 1.8492638, 1.8492638, 1.7858706, 1.7858706, 0.5392492, 1.8831507, 1.8831507, 0.031830557, 0.9693967, 0.9693967, 1.9436212, 1.9436212, 0.3960067, 1.922942, 1.922942, 1.867559, 1.867559, 2.3831449, 2.3831449, 1.9100649, 1.1170163, 1.1170163, 0.947252, 0.61407936, 1.7133427, 1.7133427, 0.37642553, 0.2982382, 1.3263859, 1.3263859, 1.1266359, -0.14963454, 1.8492638, 1.8492638, 1.929532, 1.929532, 0.9494208, 0.5392492, 0.844363, 0.844363, 0.67643327, 1.1880298, 1.1880298, 0.9208588, 0.9208588, 0.8568306, 0.8568306, 0.4393917, 0.6815945, 2.3831449, 2.3831449, 0.94447947, 1.1170163, 1.1170163, -0.35399392, -0.0682416, 1.7133427, 1.7133427, 0.62523144, -0.09845252, 0.05216508, 1.1266359, 1.5430146, 1.5430146, 0.26705086, 0.26705086, 1.929532, 1.929532, 0.9494208, 0.77179056, 0.844363, 2.163236, 2.163236, 1.336528, 1.1880298, 1.0996596, 1.0996596, 0.8568306, 0.8568306, -0.024326125, 0.6815945, 0.6815945, 0.2799246, 0.9101789, 0.9101789, 0.78632796, 0.78632796, -0.4664191, -0.4100497, 0.62523144, 0.62523144, 2.259309, 2.259309, 0.05216508, 1.5430146, 1.5430146, 0.48148146, 0.48148146, 0.06326199, 0.5232767, 0.5232767, 0.77179056, 0.82350415, 2.163236, 2.163236, 1.336528, 0.41605005, 1.0996596, 1.0996596, 1.4944845, 1.4944845, 0.42625874, 0.676908, 0.676908, 0.2799246, 0.9101789, 0.9101789, 0.78632796, 0.78632796, 1.1523316, 1.1523316, 1.0796186, 0.37915173, 2.259309, 2.259309, 0.14195317, 0.14195317, 0.69153875, 0.6947491, 0.6947491, 0.06326199, 0.15650654, 0.6103794, 0.6103794, -0.23792173, -0.23792173, -0.052567296, -0.052567296, 0.41605005, 0.52389103, 0.7811981, 1.4944845, 1.4944845, 0.42625874, 0.676908, 1.9559124, 1.9559124, 0.39009333, -0.13288058, 0.49374178, 0.49374178, 1.1523316, 2.064493, 2.064493, 1.0201727, 1.0201727, 1.5363771, 1.5363771, 0.60884386, 0.69153875, 1.2111453, 1.2111453, 1.3018463, 1.3018463, 0.6103794, 2.3039167, 2.3039167, -0.1359497, 1.1368914, 1.1368914, 0.5829537, 0.5829537, 0.52389103, 0.37005588, 1.6581306, 1.6581306, 0.39904633, 1.9559124, 1.9559124, 0.39009333, -0.39095336, 0.69377315, 0.69377315, -0.11610394, 2.064493, 2.064493, 1.0201727, 1.0201727, 1.5363771, 1.5363771, 0.60884386, 0.60884386, 1.2111453, 1.2111453, 1.3018463, 1.3018463, 0.27451634, 2.3039167, 2.3039167, -0.1359497, 2.2567234, 2.2567234, 0.9432607, 0.9432607, 0.7471883, 0.77325296, 1.6581306, 1.6581306, 0.60631955, 0.6663831, 0.6663831, 0.45093447, 1.6595508, 1.6595508, 1.0685093, -0.13370156, 1.0777438, 1.0777438, -0.28035548, -0.28035548, 0.15670386, 0.5785215, 0.5785215, 0.34965447, -0.0616264, -0.10730527, 1.3645319, 0.27451634, 0.27451634, -0.52118933, -0.31229225, -0.15766701, 2.2567234, 2.2567234, 0.9432607, 0.9432607, 0.93184835, 0.77325296, 0.77325296, 0.16092817, 0.60631955, 0.60631955, 0.45093447, 0.45093447, 1.6595508, 1.6595508, 1.0685093, 0.04949498, 0.4938368, 0.6433145, 0.6433145, -0.20690368, 0.8801789, 0.8801789, 0.5785215, 0.38728046, -0.76414394, -1.0225068, 1.3645319, -0.6522936, -0.52118933, 1.648135, 1.648135, 0.5672903, 0.6203583, 0.6984571, 0.6984571, 0.93184835, 0.93184835, 0.8579239, 1.1411018, 1.4665787, 1.4665787, 0.85255194, -0.26773354, 0.7666632, 0.7666632, 0.8416313, 0.8416313, 0.8145198, 0.8145198, 0.6433145, 0.6433145, -0.20690368, 0.8801789, 0.8801789, 0.38728046, 1.7327212, 1.7327212, 0.6845011, 0.370825, 1.5199949, 1.7195894, 1.7195894, 1.648135, 0.5822246, 0.5672903, 0.12372191, 0.09395323, 0.9430461, 0.9430461, 0.8579239, 1.1411018, 1.4665787, 1.4665787, 0.8689635, 0.8689635, 0.7666632, 0.7666632, 0.8215857, 0.8215857, 0.8145198, 0.8145198, 0.078260176, -0.18505368, -0.085930765, 0.800298, 0.87583274, 0.87583274, 1.7327212, 1.7327212, 0.6845011, 0.370825, 1.5199949, 1.7195894, 1.7195894, 0.9295051, 0.5822246, 2.0210435, 2.0210435, 0.09395323, 0.9430461, 0.9430461, -0.050603542, 0.26990435, 0.26990435, 0.18133843, 0.8689635, 2.4124537, 2.4124537, 0.3148172, 0.8215857, 0.8215857, 0.8005648, 0.8005648, 0.078260176, -0.27567053, 1.7388726, 1.7388726, 1.3191369, 1.3191369, 1.128594, 1.128594, 0.49600095, 1.0294389, 1.0294389, 0.8202478, 0.86259604, 0.86259604, 1.5133281, 2.0210435, 2.0210435, 0.22050765, 0.22050765, 0.1993002, 1.1002843, 1.298022, 2.696224, 2.696224, 0.18133843, 2.4124537, 2.4124537, -0.07785475, 0.38273242, 0.38273242, 1.0963469, 1.0963469, -0.2342158, -0.27567053, 1.7388726, 1.7388726, 1.3191369, 1.3191369, 1.3014281, 1.3749641, 1.3749641, 1.0294389, 1.0294389, 0.17581895, 0.86259604, 1.0479722, 1.5133281, 1.7426687, 1.7426687, 0.22050765, 0.22050765, -0.34994337, 1.1002843, 1.298022, 2.696224, 2.696224, 1.4123276, 0.8640523, 0.8640523, 0.40149906, 1.2248706, 1.2248706, 1.0963469, 1.0963469, -0.2342158, -0.18224478, -0.18224478, -0.10988278, 0.21348006, 1.3014281, 1.3014281, 1.5182612, 1.5182612, -0.38464543, 1.0781974, 1.0781974, 1.1813786, 1.1813786, 1.0479722, 1.7426687, 1.7426687, 0.9424681, -0.26759475, 1.2978458, 1.2978458, 0.24211796, 0.9367425, 1.4123276, 2.0112567, 2.0112567, 0.8640523, 0.40149906, 1.2248706, 1.2248706, 0.3547577, 0.61688656, 0.61688656, 0.5270042, 0.5270042, 0.4537819, 0.21348006, 0.76790243, 0.76790243, 1.5182612, 1.5182612, -0.38464543, 1.0781974, 1.1330799, 1.1813786, 1.1813786, 0.16392857, 0.7849575, 0.9424681, 0.9424681, -0.21694957, 1.2978458, 1.2978458, 0.020334182, 0.5433119, 0.5433119, 2.0112567, 2.0112567, 0.35178012, 0.37923554, 0.37923554, 0.1965574, 0.3547577, 0.61688656, 0.61688656, 0.5270042, 0.5270042, 0.4537819, 0.23810315, 0.76790243, 0.76790243, 0.5898798, -0.36385882, 0.115147874, -0.13105401, 1.1330799, 1.1330799, 0.895556, 0.895556, 0.7849575, 0.7849575, 0.32962298, 1.285984, 1.285984, 0.67646074, 0.67646074, 0.5433119, 0.5433119, 0.43904296, -0.21954103, 0.35178012, 1.670943, 1.670943, -0.0013850428, -0.0013850428, -0.11747455, 0.46616644, 0.46616644, 0.41731882, 0.40326455, 0.40326455, 0.25249663, 0.8203218, 1.3599485, 1.3599485, 1.3675972, 1.3675972, 1.0344099, 0.60512006, 0.895556, 1.0289356, 1.0289356, 0.40476182, 1.5522432, 1.5522432, 1.285984, 0.67646074, 0.67646074, 0.52004063, 0.4497121, 0.4497121, -0.067275606, 0.1833392, 1.670943, 1.670943, -0.0013850428, 0.45248908, 0.45248908, 0.46616644, 0.46616644, -0.023423105, 1.0791948, 1.0791948, 0.37687653, 0.8203218, 1.3599485, 1.3599485, 1.3675972, 1.3675972, 1.0344099, 0.39306292, 1.017021, 1.4229835, 1.4229835, 0.39608657, 1.5522432, 1.5522432, 0.86740744, 0.86740744, 0.52004063, 2.116791, 2.116791, 0.4497121, 2.3807454, 2.3807454, 0.94924647, 0.94924647, -0.9327409, 0.45248908, 1.0907497, 1.0907497, -0.34624946, 0.19796729, 1.0819352, 1.0819352, 0.37687653, 0.37687653, 1.0946383, 1.0946383, 2.1321535, 2.1321535, 0.9364457, 1.2650778, 1.2650778, 1.4229835, 1.4229835, 0.67997485, 1.1244192, 1.3277828, 1.3277828, 0.86740744, -0.46433768, 2.116791, 2.116791, -0.035768073, 2.3807454, 2.3807454, 0.94924647, 0.94924647, -0.8871809, -0.5327028, 1.2433194, 1.2433194, 0.81267405, 0.58725935, 1.0819352, 1.0819352, -0.5075176, 2.4972005, 2.4972005, 1.0946383, 2.1321535, 2.1321535, 0.9364457, 1.2650778, 1.2650778, 1.7549862, 1.7549862, 0.67997485, 0.55578697, 1.3277828, 1.3277828, 0.72003376, 0.30360392, 1.0217906, 1.0217906, 0.44819528, 1.6961815, 1.6961815, 0.82140595, 0.82140595, 0.67057043, 0.039766736, 1.2433194, 1.2433194, 0.81267405, 0.7231005, 0.7231005, 0.71998376, 0.71998376, 2.4972005, 2.4972005, 0.019279385, 1.8495913, -0.10434349, 0.021351224, 0.021351224, 0.19275385, 1.7549862, 1.7549862, -0.058586553, 0.55578697, 0.55578697, 0.72003376, 1.4893559, 1.4893559, 0.7726948, 0.7726948, 0.47689837, 1.6961815, 1.6961815, 0.82140595, 0.82140595, 0.67057043, 0.039766736, 0.039766736, -0.050084095, 1.3124703, 1.3124703, 0.7231005, 0.71998376, 0.71998376, -0.10169727, 0.019279385, 0.019279385, 1.8495913, 0.78580385, 0.021351224, 0.9409176, 0.9409176, 0.49805242, 0.49805242, -0.026192237, -0.058586553, -0.112465985, 0.6450553, 1.4893559, 1.4893559, 0.6119272, 1.735879, 1.735879, 1.6819217, 1.6819217, 0.5290452, 0.4226286, 0.0114989, 0.0114989, -0.050084095, -0.050084095, 1.3124703, 1.3124703, 2.2259443, 2.2259443, 1.370989, 0.3248696, 0.997118, 0.997118, 1.411172, 0.78580385, 0.8672766, 0.9409176, 0.9409176, 0.49805242, 0.49805242, 0.31144708, 0.23958276, 0.9725358, 2.1338682, 2.1338682, 1.0118425, 0.7557403, 1.735879, 1.735879, 1.6819217, 1.6819217, 0.022959756, 0.022959756, 0.0114989, 0.0114989, -0.43674833, -0.43674833, 0.3269626, 0.33003512, 2.2259443, 2.2259443, 1.370989, 0.3248696, 0.997118, 0.997118, 0.22739278, 0.05157494, 0.8672766, 0.8672766, 0.8656529, 1.0813761, 1.0813761, 0.31144708, 0.23958276, 0.9725358, 2.1338682, 2.1338682, 0.4064155, 0.7557403, 0.7557403, 1.3264617, 1.3264617, 0.059894685, 0.059894685, -0.21252304, -0.34796184, 0.93639857, 0.93639857, 0.2711702, 0.2711702, -0.40607178, 0.47224715, 1.154184, 1.154184, 0.17250441, 1.997956, 1.997956, 0.22739278, 2.5944245, 2.5944245, 0.30875126, 0.8656529, 1.0813761, 1.0813761, 0.19031155, 1.8227236, 1.8227236, 0.69938046, 0.9606934, 1.3290628, 1.3290628, 0.05095428, 1.3264617, 1.3264617, 0.059894685, 0.96744615, 0.96744615, -0.055352546, 0.93639857, 0.93639857, 0.2711702, 1.2760754, 1.325014, 1.325014, 0.9304085, 2.339625, 2.339625, 1.997956, 1.997956, 1.471322, 2.5944245, 2.5944245, 0.30833125, 0.30833125, 0.3675449, 0.3675449, 0.19031155, 1.8227236, 1.8227236, -0.5215797, 0.9606934, 1.3290628, 1.3290628, 2.759355, 2.759355, 1.0304383, 0.27560067, 1.4350494, 1.4350494, 0.5072389, 0.3528166, 0.3528166, 1.4013448, 1.4013448, 1.325014, 1.325014, 0.86351967, 2.339625, 2.346647, 2.346647, -0.25957698, 1.471322, 1.5927707, 1.5927707, 0.82998616, 0.30833125, 0.012231983, 1.5692596, 1.5692596, 1.6815767, 1.6815767, 0.9688826, 0.9688826, 1.3891454, 2.0140603, 2.759355, 2.759355, 0.25871643, 0.27560067, 1.4350494, 1.4350494, 0.5072389, -0.1162297, 1.0645851, 1.4013448, 1.4013448, 0.5289436, 0.7243685, 1.3852615, 1.3852615, 2.346647, 2.346647, 0.17879286, 0.9380925, 0.9380925, 0.82998616, 0.82998616, 0.0941923, 0.0941923, 1.5692596, 1.5692596, 0.8924739, 0.8924739, 0.9688826, 0.9688826, 1.3891454, 2.0140603, 2.0140603, -0.04932407, 0.2390336, 0.2390336, 1.6739857, 1.6739857, 1.5634048, 1.5634048, 1.0645851, 1.0645851, 0.43310794, 0.43310794, 0.7243685, 1.3852615, 1.3852615, 0.70104134, 0.44103292, 0.17879286, 0.2407875, 0.2891205, 0.41287082, 0.41287082, 0.0941923, 0.0941923, -0.35811406, 0.5559627, 0.8924739, 0.8924739, 0.10471403, 0.22805333, 0.22805333, 0.5407736, 0.5407736, -0.04932407, 0.2390336, 0.2390336, 1.6739857, 1.6739857, 1.5634048, 1.5634048, -0.790523, 0.22425222, 0.22425222, 0.2149656, 0.2149656, 1.0156653, 1.0156653, 0.70104134, -0.41747734, -1.0974966, 1.7123052, 1.2649833, -0.30078387, 1.1173053, 1.1173053, 0.22246316, 0.22246316, 0.13768983, 0.38141626, 0.7539915, 1.0653155, 1.0653155, 0.9853175, 0.7669197, 0.40262553, 1.6692508, 1.6692508, 0.60815644, 1.1149623, 1.4333525, 1.4333525, 0.43554616, 0.43554616, 0.32830128, 0.03308975, 0.5969065, 0.5969065, -0.15602389, 1.0490932, 3.1709747, 3.1709747, 0.18949963, 1.2649833, 1.2649833, -0.2311016, 0.20984948, 0.20984948, 2.1495745, 2.1495745, 0.73165894, 0.73165894, 0.38141626, 0.6632581, 0.6632581, 0.41540062, 1.5788652, 1.5788652, -0.061638054, -0.061638054, 0.26902407, 0.93874687, 1.2674117, 1.2674117, 0.49949825, 1.2591671, 1.2591671, 0.70411104, 2.5263681, 2.5263681, 1.7699214, 0.3779101, 1.3243587, 1.3243587, -0.1722008, 1.1045785, 1.1045785, -0.2311016, 0.9214084, 0.9214084, 2.1495745, 2.1495745, 0.73165894, 0.73165894, 0.34816924, 0.6632581, 0.82455724, 0.82455724, 1.5788652, 1.5788652, 0.21717963, 0.21717963, 1.4045455, 1.4045455, 1.2674117, 1.2674117, 1.5187594, 1.5187594, 1.2591671, 0.76449746, 2.5263681, 2.5263681, 1.7699214, 1.221385, 1.3243587, 1.3243587, -0.1722008, 1.1045785, 1.1045785, 1.3978963, 1.3978963, 0.9214084, 1.648504, 1.648504, -0.13256802, 1.4261588, 1.4261588, 0.93612915, 0.8326507, 0.8326507, 1.6315974, 1.6315974, 0.37775916, 0.2398671, 1.4045455, 1.4045455, 0.77067304, 0.77067304, 1.8219151, 1.8219151, 0.76449746, 0.76449746, 0.24660219, 0.99213684, 1.9050636, 1.9050636, -0.01477722, -0.033319283, -0.35502872, 0.5310425, 0.5310425, 1.3978963, 1.3978963, 0.9600477, 1.648504, 1.648504, 1.1239053, 1.4261588, 1.4261588, 1.3925184, 1.0375856, 0.8326507, 1.6315974, 1.6315974, 0.5897036, 0.2398671, 1.5848205, 1.5848205, 0.77067304, 0.77067304, 1.8219151, 1.8219151, 1.2961498, 1.2961498, 0.6164593, 0.99213684, 1.9050636, 1.9050636, 0.8805112, 0.08595198, 0.08595198, 0.75194657, 0.5629897, 1.054758, 1.054758, 0.9600477, 1.7746586, 1.7746586, 1.1239053, 0.76943016, 1.3925184, 1.3925184, 1.0375856, 0.30151406, 1.0390965, 1.0390965, 0.5897036, 1.7367749, 1.7367749, 1.5848205, 0.9605572, 0.9605572, 0.8709618, 0.8709618, 2.3207998, 2.3207998, 0.6164593, 0.53659654, 0.43480796, 0.8805112, 0.8805112, 0.732424, 0.08595198, 0.75194657, 0.5629897, 0.2711128, 1.0450233, 1.0450233, 1.7746586, 1.7746586, -0.16221845, 1.151734, 1.151734, 0.33053273, 0.13157398, 0.30151406, 1.0390965, 2.0234718, 2.0234718, 1.7367749, 1.7367749, 2.2436018, 2.2436018, 1.9223248, 1.9223248, 1.4053655, 2.3207998, 2.3207998, 0.53420115, 0.5474806, 0.5474806, 0.54009444, 0.732424, 0.732424, 0.54100823, -0.085115604, 0.966768, 0.966768, 1.0450233, 1.0450233, 0.59903955, 0.5232617, 0.09920486, 1.576299, 1.576299, 0.5023282, 0.16066119, 0.16066119, 1.6085222, 2.0234718, 2.0234718, 0.50538695, 0.35924914, 2.2436018, 2.2436018, 1.9223248, 1.9223248, 1.4053655, 1.6180543, 1.6180543, 0.42258036, 0.5474806, 0.5474806, 0.51283646, 0.6027077, 0.6027077, 0.54100823, -0.085115604, 0.966768, 0.966768, 0.6251187, -0.5216415, 0.5232617, 0.5232617, 0.09920486, 1.576299, 1.576299, 0.5023282, 0.16066119, 0.16066119, 1.6085222, 1.6085222, 1.1452621, 1.1452621, 0.7741606, 0.7741606, 0.10490716, 0.13391292, 0.31943184, 0.31943184, -0.13777922, 1.4961396, 1.4961396, 1.3462211, 1.3462211, 0.51283646, 0.62251914, 0.62251914, -0.5946498, 0.5684589, 0.4804245, 0.6251187, 0.83135104, 0.83135104, 0.48797268, 2.6429358, 2.6429358, 2.290467, 2.290467, 1.6002678, -0.059928004, -0.22007579, 1.3086687, 1.3086687, 1.1452621, 1.5259576, 1.5259576, 0.8820566, 0.8820566, 0.13391292, 0.13391292, 0.3718111, 0.45730963, 1.4961396, 1.4961396, 1.3462211, 1.3462211, 1.5885307, 1.5885307, 0.7747283, 0.7747283, 0.5684589, 0.4804245, 0.4804245, 1.0400863, 1.0400863, 0.88518757, 2.6429358, 2.6429358, 2.290467, 2.290467, 1.6002678, -0.008209882, 1.0429776, 1.0429776, 0.41409135, 0.24676603, 1.5259576, 1.5259576, 0.8820566, 0.8820566, 0.13191175, 0.76877064, 0.76877064, 0.45730963, 0.9623417, 0.9623417, 1.9470986, 1.9470986, 1.5885307, 1.5885307, 1.0212247, 0.7747283, 2.4512298, 0.41133425, 0.0025711823, 1.0400863, 1.0400863, 0.88518757, 1.4737648, 1.6606076, 1.6606076, 1.171041, -0.008209882, -0.008209882, 1.0429776, 1.0983036, 1.0983036, 0.15466884, 1.0415684, 1.0415684, 0.9785673, 0.9785673, 0.13191175, 0.76877064, 0.76877064, -0.4213276, 0.8756957, 0.8756957, 1.9470986, 1.9470986, 0.8091803, 1.0212247, 1.2016978, 1.2016978, 2.4512298, 0.87938994, 0.87938994, 0.63542455, 0.71593887, 0.71593887, 0.8809376, 1.8081318, 1.8081318, 0.43663847, 0.69643867, 0.69643867, 0.65178126, 1.0983036, 1.0983036, 0.025385605, 0.61039174, 0.61039174, 0.9785673, 1.7612661, 1.7612661, 0.7540957, 0.66596717, 0.11255753, 0.50099224, 0.50099224, 0.9361076, 0.9361076, 0.8091803, 0.88649154, 1.2016978, 1.2016978, 0.2847906, 0.87938994, 0.87938994, 0.63542455, 0.71593887, 0.71593887, 0.8809376, 1.8081318, 1.8081318, 0.43663847, 0.69643867, 0.69643867, 0.6878814, 1.3480358, 1.3480358, 2.6805708, 2.6805708, -0.20080851, -0.7401368, 1.7612661, 1.7612661, 0.7540957, 1.3185511, 1.3185511, 0.11255753, 0.067516856, 0.77760416, 0.77760416, 0.33601573, 0.88649154, 0.88649154, -0.27213177, 0.5483281, 0.5483281, 0.19718106, 1.0590272, 1.0590272, 0.5665453, 1.2572197, 1.2572197, 0.49872696, 0.81786186, 0.81786186, 0.6878814, 0.6878814, 1.3480358, 1.3480358, 2.6805708, 2.6805708, -0.16137354, 1.7944489, 1.7944489, 2.2320163, 2.2320163, 1.367415, 1.367415, 0.15364446, 0.15364446, 0.8444543, 0.8444543, 0.28376955, 0.33905926, 0.33905926, -0.58963203, 0.5483281, 1.7406294, 1.7406294, 1.0590272, 1.0590272, 0.87672675, 1.2572197, 1.2572197, 0.94940555, 0.94940555, 0.81786186, 0.23820019, 1.1493794, 1.1493794, 0.29751435, 1.3554426, 1.3554426, 0.58822495, 1.7944489, 1.7944489, 2.2320163, 2.2320163, 1.367415, 1.5877615, 1.5877615, 0.15364446, 0.99675965, 0.99675965, 0.28376955, 0.28376955, 0.079121724, 0.079121724, 0.8505307, 1.7406294, 1.7406294, 0.91722155, 0.91722155, 1.8666831, 1.8666831, 0.75746834, 1.238007, 1.238007, -0.086455286, 0.6234536, 1.1493794, 1.1493794, 0.51291686, 0.64305454, 0.64305454, 0.58822495, 2.5402317, 2.5402317, 0.9265583, 0.55808187, 0.15011522, 1.5877615, 1.5877615, 1.1277837, 1.0329891, 1.0329891, 1.4121517, 1.4121517, 0.079121724, 0.079121724, 0.8505307, -0.14989159, 0.084968135, 0.084968135, 0.5319746, 1.8666831, 1.8666831, 0.75746834, 1.238007, 1.238007, 2.2113044, 2.2113044, 0.8906717, 0.8906717, 0.51291686, 0.22734594, 1.6142496, 1.6142496, 2.5402317, 2.5402317, 0.9265583, 0.55808187, 1.5480669, 1.5480669, 1.1277837, 1.1277837, 1.0329891, 1.0329891, 1.4148741, 1.4148741, 0.15647626, -0.21634398, 0.44284612, 0.21839707, -0.25271067, -0.25271067, 0.6563907, 1.2605689, 1.2605689, 0.16586353, 1.3291413, 1.3291413, 2.2113044, 2.2113044, 1.9367125, 1.9367125, 0.38220277, 0.38220277, 1.6142496, 1.6142496, 1.0085973, 0.52759737, 0.8065638, 0.8065638, 1.5480669, 1.5480669, 1.1483506, 1.4028448, 1.4028448, 0.8246182, 1.4148741, 1.4148741, 0.15647626, -0.21634398, 0.56800735, 0.56800735, 0.08261103, 0.018922104, 0.6563907, 0.6563907, 1.0139722, 1.0139722, 1.3291413, 1.3291413, 1.0097119, 1.0097119, 1.9367125, 2.171165, 2.171165, 0.66207427, 1.1615338, 1.1615338, 2.190898, 2.190898, 0.9836362, 0.8065638, 0.25233144, 1.2098501, 1.2098501, 1.4028448, 1.4587147, 2.1531198, 2.1531198, 0.4683049, 0.6572677, 0.6572677, 0.56800735, 0.56800735, 0.62656426, 0.62656426, 0.018922104, 0.8449956, 1.0139722, 1.0139722, 2.042072, 2.042072, 1.0097119, 1.0097119, 0.72934633, 2.171165, 2.171165, 0.66207427, 0.53916126, 0.53916126, 2.190898, 2.190898, 0.9836362, 0.25233144, 0.60799253, 1.2098501, 1.2098501, 1.5612797, 1.5612797, 2.1531198, 2.1531198, 0.59936935, 0.6572677, 0.6572677, 0.46079433, 0.17124355, 1.4263169, 2.4884417, 2.4884417, 1.6959696, 1.8334354, 1.8334354, 2.042072, 2.042072, 0.46637958, -0.09439251, 0.72934633, 0.8020517, 0.8020517, 0.60413677, 0.60413677, -0.024549423, -0.8358561, 1.966764, 1.966764, 0.24664895, 1.419255, 1.419255, 0.9341797, 1.5612797, 1.5612797, 0.18418126, 0.21301717, 1.279812, 1.279812, -0.2563169, 0.46079433, -0.009657237, 1.4263169, 2.4884417, 2.4884417, 1.6959696, 1.8334354, 1.8334354, 1.801214, 1.801214, 0.46637958, -0.09439251, 0.8962484, 0.8962484, 0.88686466, 1.103694, 1.103694, 0.4005307, 0.13545467, 1.966764, 1.966764, 1.8593464, 1.419255, 1.419255, 0.9341797, 0.86900634, 1.1149246, 1.1149246, 0.024290914, 1.279812, 1.279812, 0.60896236, 0.40088567, 0.0032222164, -0.55598503, 0.19321355, 1.2973421, 1.2973421, 1.001331, 0.8179776, 1.801214, 1.801214, 1.4622141, 1.4622141, 0.8962484, 0.8962484, 0.88686466, 1.103694, 1.103694, 0.77905124, 0.13545467, 0.9687444, 1.8593464, 1.8593464, -0.13482246, 2.6205738, 2.6205738, 0.33510563, 1.1149246, 1.1149246, 0.013748487, 0.8253722, 0.8253722, 0.60896236, 0.0032222164, 0.021857359, 0.042539075, 1.5309323, 1.5309323, 0.18345025, 0.2727964, 0.8179776, 0.8179776, 1.4315678, 1.4622141, 1.4622141, 0.5383296, -0.14244542, -0.12656933, 0.4936441, 0.77905124, 0.77905124, 0.99012136, 0.99012136, 0.9687444, -0.021869909, -0.27120733, 2.6205738, 2.6205738, 1.7025702, 0.06763423, 0.44702417, 0.44702417, 0.8253722, 0.8253722, -0.29832315, 1.4219756, 0.45636335, 0.042539075, 1.5309323, 1.5309323, 0.092447735, -0.099008314, 0.777277, 0.777277, 0.5541276, 0.5541276, 0.5383296, 0.5383296, 0.71966815, 0.71966815, 0.4936441, 0.4936441, 0.48701534, 0.99012136, 0.99012136, -0.015779218, 0.9541896, 0.9541896, 0.54414475, 1.7025702, 1.7025702, 1.7148316, 1.7148316, 0.44702417, 0.17233914, 1.3985621, 1.3985621, 1.4219756, 0.7825789, 1.7975394, 1.7975394, 0.54565734, 0.54565734, 0.1508182, 1.6857923, 1.6857923, 0.5541276, 0.91064566, 0.91064566, -0.029856121, 0.71966815, 0.71966815, 0.038127385, 0.51271755, 0.51271755, 0.27566653, -0.015779218, -0.015779218, 1.2577442, 1.2577442, 0.54414475, 0.4472121, 1.4558079, 1.7148316, 1.7148316, 1.4849256, 0.4595849, 1.3985621, 1.3985621, 1.3905339, 0.7825789, 1.7975394, 1.7975394, 1.2165778, 1.2165778, 2.0347626, 2.0347626, 1.6857923, 0.9508337, 0.91064566, 0.91064566, 0.57275134, 0.67755705, 1.3071839, 1.3071839, 0.51271755, 0.8591042, 0.8591042, 0.8733118, 1.1997361, 1.2577442, 1.2577442, 0.041082226, 0.5934659, 1.4558079, 2.1982963, 2.1982963, 1.4849256, 0.4595849, 0.4595849, 0.28005785, 1.3905339, -0.15503581, 0.078514025, 0.41780058, 1.2165778, 1.2165778, 2.0347626, 2.0347626, 1.5743712, 1.5743712, 1.9887319, 1.9887319, 1.1172835, 0.67755705, 1.3071839, 1.3071839, 0.030546581, 1.2697648, 1.2697648, 0.8733118, 1.1997361, 1.1997361, 0.45615304, 0.7750233, 0.7750233, 0.5934659, 2.1982963, 2.1982963, 0.0012484558, 0.0012484558, -0.15993978, -0.58957994, -0.59815043, 0.4178537, 0.4178537, 0.41780058, 0.41780058, 1.342622, 1.8022423, 1.8022423, 1.5743712, 1.5743712, 1.9887319, 1.9887319, 1.4015235, 0.15843384, 1.054325, 1.054325, 0.030546581, 1.2697648, 1.2697648, 0.017515613, 0.32362577, 0.32362577, -0.020129103, 0.7750233, 0.92914516, 0.92914516, 0.37117255, 0.6436313, 0.68889666, 0.68889666, 0.2746472, -0.6036204, 0.70885956, 0.42281857, 0.64445204, 0.64445204, 0.6635616, 1.342622, 1.342622, 1.3151376, 1.3151376, 0.19782817, 1.0598063, 1.4015235, 1.4015235, 0.7435535, 0.7435535, 0.21035936, 1.36606, 1.555114, 1.555114, 0.61332625, 1.496911, 1.496911, 1.1831195, 0.71889716, 0.92914516, 0.92914516, 0.37117255, 0.6436313, 0.68889666, 0.68889666, 1.2729537, 1.2729537, 0.70885956, 0.42281857, 0.64445204, 0.64445204, 0.6635616, 0.6635616, 1.1936116, 1.1936116, -0.09816121, -0.14735366, 1.0598063, 1.0598063, 0.026246618, 0.7435535, 0.7435535, 0.21035936, 1.36606, 1.555114, 1.555114, 0.61332625, 1.496911, 1.496911, 1.1831195, 0.71889716, 0.14067191, 0.14067191, -0.15901226, 0.24005693, 0.24005693, 0.10015941, 1.2729537, 1.2729537, 0.73018354, 0.73018354, 0.38259813, 0.38259813, 0.87830377, 0.87830377, 1.2815113, 1.9142298, 1.9142298, 1.6266496, 1.6266496, 2.9290962, 2.9290962, 1.0805441, 1.0805441, 0.45616361, 0.45616361, 0.5707885, 1.5427964, 1.5427964, 1.3821819, 1.4867824, 1.4867824, 1.5116394, 1.7064682, 1.7064682, 0.70178336, 0.48334184, 0.50496995, 1.7236962, 1.7236962, 1.414415, 0.3257996, 0.124769524, -1.0126731, 0.32335654, 1.400689, 1.400689, 1.2815113, 1.9142298, 1.9142298, 1.6266496, 1.6266496, -0.0150050875, 0.41609964, 1.0805441, 1.0805441, 0.45616361, 0.66414404, 0.66414404, 1.5427964, 1.8961822, 1.8961822, 0.40955177, 0.40955177, 1.5116394, 1.7064682, 1.7064682, 0.70178336, 0.07328543, 0.5298338, 1.7108365, 1.7108365, 1.414415, -0.063661486, -0.11244375, 0.89786416, 0.89786416, 1.400689, 1.400689, 0.50481546, 1.3031809, 1.3031809, 1.0659268, 0.063896194, -0.9659539, 0.41609964, 0.41609964, 0.12253132, 0.12253132, 0.66414404, 0.66414404, 1.1279566, 1.8961822, 1.8961822, -0.28157544, -0.28157544, 0.44478136, 1.0223923, 1.0223923, 2.2856011, 2.2856011, 0.5298338, 0.8097407, 1.0799313, 1.0799313, 1.094167, -0.11244375, 1.131812, 1.131812, 0.29507577, -0.49941677, 0.1746801, 0.1746801, 1.2505698, 1.2505698, 0.063896194, -0.8633556, 0.14166717, 0.14166717, 1.3440744, 1.3440744, 0.5000167, -0.6289807, 1.1279566, 1.1279566, 1.7345722, 1.7345722, 0.57261336, 0.57261336, 0.87550914, 1.0985817, 2.2856011, 2.2856011, 0.81345224, 0.8097407, 1.0799313, 1.0799313, 1.094167, 0.42425263, 1.6295, 1.6295, -0.3000036, -0.3000036, -0.9325702, 1.7969185, 1.7969185, 1.2505698, -0.19871506, 1.4198639, 1.4198639, 0.6795679, 1.3440744, 1.3440744, 0.5670582, 0.9882406, 1.0700725, 1.0700725, 1.7345722, 1.7345722, 1.1035038, 0.57261336, 1.4099522, 1.4099522, 1.0985817, 0.81345224, 0.81345224, 0.75386566, 0.75386566, 0.36559147, 0.5880377, 1.55701, 1.6295, 1.6295, -0.3000036, -0.18779492, 0.4869373, 1.7969185, 1.7969185, 0.5953003, 1.1641518, 1.4198639, 1.4198639, 0.7289299, 0.6795679, -0.32125893, 0.5670582, 0.9882406, 1.0137506, 1.0137506, 0.50666904, 1.1035038, 1.1035038, 0.5569641, 1.4099522, 1.4099522, 0.77946055, 0.77946055, 0.28791544, 0.28791544, 0.36559147, 0.36559147, 0.924434, 1.55701, 1.55701, 1.0121332, 1.6184593, 1.6184593, 0.57110983, 0.4869373, 0.20579681, 0.5953003, 1.1641518, 1.1641518, 0.7289299, 0.7289299, -0.034475047, 0.072677895, 0.072677895, 0.3327982, 1.0137506, 1.0137506, 0.50666904, 2.4643219, 2.4643219, 1.1320051, 1.1320051, 0.6530373, -0.18844908, 0.2872573, 0.2872573, 1.3363862, 1.3363862, -0.0842233, 0.924434, 0.924434, 1.0121332, 1.7140515, 1.7140515, 1.6184593, 0.7669904, -0.45962644, 0.20579681, 0.3263751, 1.4831287, 1.4831287, 0.23908707, 0.650042, 0.650042, 0.23868157, 0.23868157, -0.11622244, 0.85464275, 0.85464275, 0.92660475, 2.4643219, 2.4643219, 1.1320051, 1.1320051, 0.409295, 0.39710623, 0.49964696, 0.49964696, 1.3363862, 1.3363862, -0.120041125, 0.46146888, -0.046481155, 0.037996013, 1.7140515, 1.7140515, 0.7669904, 0.86214805, 0.86214805, 0.4883706, 0.9659361, 1.4831287, 1.9612269, 1.9612269, 0.650042, 0.650042, 1.0616002, 1.0616002, -0.11622244, 0.830386, 0.830386, 0.92660475, 1.8004627, 1.8004627, 0.51650745, 0.51650745, 0.409295, 0.39710623, 0.49964696, 0.49964696, 0.24622276, 0.24622276, 0.09082593, 1.3970653, 1.497715, 1.497715, 0.5652672, 0.54985684, 0.40713033, 0.86214805, 0.86214805, 0.8989674, 0.9659361, 1.4226048, 1.9612269, 1.9612269, 1.3752254, 1.4994915, 1.4994915, 1.0616002, -0.86050975, 0.830386, 0.830386, 0.8576619, 0.8576619, 0.40807465, 1.0770375, 1.0770375, 0.016491488, 0.5746089, 0.5746089, 0.65880215, 0.65880215, 0.16987796, 0.09082593, 1.3970653, 1.497715, 1.497715, 0.81547195, 0.48096985, 0.48096985, 0.85282975, 0.85282975, 1.0044192, 1.0044192, 1.3141544, 1.3141544, 1.3752254, 1.3752254, 1.9258057, 1.951761, 1.951761, 1.2399405, 0.93858516, 0.5125622, 0.8576619, 0.8576619, -0.1105905, 1.0770375, 1.0770375, -0.22482656, 0.7920415, 0.7920415, 0.65880215, 0.65880215, 0.1118724, -0.22295918, 0.3105647, 0.3105647, 0.81547195, 0.81547195, 0.48096985, 0.69865024, 0.85282975, 0.85282975, 1.0044192, 1.0044192, 0.7315516, 0.6335339, 0.6335339, 0.8927082, 1.9258057, 1.951761, 1.951761, 1.9093426, 1.9093426, 1.6638731, 0.90082276, -0.35911658, -0.50900584, 1.6531498, 1.6531498, 0.38616636, 0.7920415, 0.7920415, 0.38182402, 0.3982961, 0.3982961, -0.67570317, 1.2228775, 0.3105647, 0.9719703, 0.9719703, 0.35026592, 0.69865024, 0.69865024, 0.5216252, 0.49689314, 0.7315516, 0.7315516, 0.3196498, -0.40985453, 0.8927082, 0.8927082, 0.63438666, 1.6292758, 1.9093426, 1.9093426, 1.6638731, 0.90082276, 0.7046427, 0.7046427, 1.6531498, 1.6531498, 0.7703309, 0.47348827, 1.8552462, 1.8552462, 1.4156562, 0.98967946, 0.98967946, 1.2228775, 1.1363881, 0.9719703, 0.9719703, 0.572627, 1.9026182, 1.9026182, -0.09302414, -0.18808974, 1.1778295, 1.1778295, -0.40176374, 0.3750199, 0.7979132, 0.7979132, 0.63438666, 1.6292758, 1.6292758, 0.13906415, -0.8576702, 0.88833886, 0.88833886, 0.7046427, 0.93679523, 0.93679523, 1.3289608, 1.3289608, 1.8552462, 1.8552462, 1.4156562, 0.98967946, 0.98967946, 1.1363881, 1.1363881, 0.67161655, 0.2501139, 0.572627, 1.9026182, 1.9026182, 0.64221096, 1.040239, 1.1778295, 1.1778295, 1.1445535, 1.1445535, 1.283455, 1.283455, 2.1886187, 2.1886187, 0.90072393, 0.90072393, 0.8913641, 0.88833886, 0.88833886, -0.52474195, 0.1265688, 0.7930071, 1.3289608, 1.3289608, 0.86301714, 0.86301714, 0.7271749, 0.7375215, 0.7375215, 0.43311873, 1.3207943, 1.3207943, 0.2501139, -0.28339776, 0.8165349, 0.8165349, 0.64221096, 1.040239, 1.0875463, 1.0875463, 1.1445535, 1.1445535, 1.283455, 1.283455, 2.1886187, 2.1886187, 0.90072393, 0.90072393, 0.8913641, -0.17248231, 0.29577908, 0.8425888, 0.8425888, 0.7930071, 0.7930071, 1.0061071, 1.0061071, 1.9595621, 1.9595621, 0.9423143, 0.7375215, 0.7550497, 1.3207943, 1.3207943, -0.09406245, 0.39756522, 0.8165349, 0.8165349, 0.6006052, 0.6006052, 1.0875463, 1.0875463, 0.73930454, 0.61915493, -0.8907292, 0.9219348, 0.9219348, 0.5225576, 0.5225576, 0.45028883, 0.45028883, 1.2341441, 1.4498476, 1.4498476, 0.8425888, 0.24561642, -0.03299648, 1.0061071, 1.0061071, 2.0118642, 2.0118642, 1.336235, 1.336235, 1.3423537, 0.6021635, 0.8732731, 1.9741, 1.9741, 0.47780856, -0.060137887, 0.6006052, 1.0241649, 1.0241649, 0.24461035, 0.089076206, 0.089076206, 0.26473877, 0.9219348, 0.9219348, 0.59542316, 0.59542316, 0.45028883, 0.45028883, 1.2341441, 1.4498476, 1.4498476, -0.24508175, -0.24508175, 0.3140257, 0.37287602, 0.8862932, 2.0118642, 2.0118642, 1.336235, 1.336235, 1.3423537, 0.6498296, 0.8732731, 1.9741, 1.9741, 1.8571023, 1.8571023, 0.41467425, 1.0241649, 1.0241649, 0.24461035, 0.6500845, 1.8209393, 1.8209393, 1.4540358, 1.4540358, 0.59542316, 1.0703601, 1.0703601, 1.2424939, 1.2424939, 1.060715, 1.060715, 0.66200536, 0.66200536, 0.3140257, 0.37287602, 0.8862932, 0.8862932, 0.7312936, 0.07077408, 0.07077408, 0.8188394, 0.6498296, 0.6498296, 0.4347888, 0.09725088, 1.8571023, 1.8571023, 0.66939247, 0.73860705, 1.3042139, 1.3042139, 0.6500845, 1.8209393, 1.8209393, 1.4540358, 1.4540358, 0.2934714, 1.0703601, 1.0703601, 1.2424939, 1.2424939, 0.65592444, -0.038476657, 0.66200536, 0.66200536, 0.50434357, -0.21653287, -0.21653287, 0.7312936, 0.8123336, 0.8123336, 0.54627186, 0.51226676, 0.51226676, 0.3251987, 0.3251987, 0.09725088, 0.679944, 0.679944, 0.66939247, 2.2542837, 2.2542837, 1.3042139, 0.2598325, -0.6783298, -0.083288394, 1.6028637, 1.6028637, 0.4655892, 1.176787, 1.176787, 1.6973464, 1.6973464, 0.65592444, 0.17462958, 0.982327, 1.0374448, 1.0374448, 0.15919177, -0.5053407, -0.5053407, 0.8123336, 0.8123336, 0.54627186, 0.51226676, 0.51226676, 0.08584311, 0.7510253, 0.7510253, 0.8375479, 1.9608808, 1.9608808, 2.2542837, 2.2542837, 1.2098372, 1.2098372, -0.6783298, -0.083288394, 1.6028637, 1.6028637, 0.78990495, 1.176787, 1.176787, 1.6973464, 1.6973464, 0.7591567, 0.17462958, 0.982327, 1.0374448, 1.0374448, 0.5769559, 0.5769559, 0.23497719, 0.6289996, 0.6289996, 0.31403384, 1.0122606, 1.0122606, 0.92874193, 1.081056, 1.5723304, 1.5723304, 1.9608808, 1.9608808, 0.79388034, 0.04399551, 1.2098372, 1.2098372, 0.32099947, 0.32099947, 0.5922057, 0.5922057, 0.78990495, 1.1022825, 1.1022825, 0.5769346, 0.8183537, 0.8183537, 0.100564204, 0.1066523, 1.5190033, 1.5190033, 1.90134, 1.90134, 0.27441698, 0.6289996, 0.6289996, 0.31403384, 1.0122606, 1.0122606, 0.92874193, 1.081056, 1.5723304, 1.5723304, 0.13998847, 0.79388034, 0.79388034, 0.04399551, 0.49097106, 0.8655252, 1.2740445, 1.2740445, 0.5922057, 0.5922057, 0.37317473, 1.0826722, 1.0826722, 1.072636, 1.072636, 0.8183537, -0.13580984, 1.0222104, 1.5190033, 1.5190033, 1.90134, 1.90134, 1.2466952, 1.2466952, 0.70068014, 0.6966507, -0.20697448, 0.6772459, 0.6772459, 0.022212664, 0.8982406, 0.8982406, 0.13998847, 0.9091885, 0.9091885, 0.08071989, 0.49097106, 0.8655252, 2.3374097, 2.3374097, 0.4693722, 1.5237712, 1.5237712, 1.0826722, 1.0826722, 1.226664, 1.226664, 0.8261257, 0.8261257, 1.0222104, 1.0222104, 0.13603121, 0.13603121, 0.5614499, 1.2466952, 1.2466952, 0.70068014, 0.6966507, -0.20697448, 0.6772459, 0.6772459, 1.3257079, 1.3257079, 0.8982406, 0.09152195, 1.0534397, 1.0534397, 1.0147377, 1.4403037, 1.4403037, 2.3374097, 2.3374097, 1.5148823, 1.6043264, 1.6043264, 0.20806953, -0.4292239, 1.226664, 1.226664, 0.8261257, 0.8261257, -0.057756558, -0.21716312, 0.13603121, 0.13603121, 0.5614499, 0.5614499, -0.33275875, -0.20400788, -0.20400788, 1.2598753, 1.2598753, 0.44786966, 1.3257079, 1.3257079, 1.0121683, 1.3841867, 1.3841867, 1.252002, 1.0147377, 1.4403037, 1.4403037, 1.6264315, 1.6264315, 1.5148823, 1.6043264, 1.6043264, 0.6941287, 0.66396505, 0.8220764, 0.8220764, -0.21321523, -0.15279447, 0.59172696, 0.59172696, -0.13022317, 0.61295235, 0.61295235, 0.12955453, 1.17942, 1.17942, 0.836636, 1.2598753, 1.2598753, 0.24099673, -0.28044778, 0.40508026, 1.0121683, 1.3841867, 1.3841867, 1.252002, 0.71178526, 0.27372944, 0.5620131, 0.7955496, 0.7955496, 0.49585068, 0.49585068, 0.6941287, 0.6941287, 0.66396505, 0.8220764, 0.8220764, 1.2412993, 1.2412993, 0.59172696, 0.59172696, -0.05872619, 0.61295235, 0.61295235, 0.12955453, 1.17942, 1.9457614, 1.9457614, 1.0723007, 1.0723007, -0.3070685, -0.3070685, 0.8528891, 0.8528891, 0.16491273, 0.16491273, 0.71178526, 1.9677297, 1.9677297, 0.44768322, 0.7955496, 0.7955496, 0.49585068, 0.49585068, 0.49908426, 0.49908426, 2.0791767, 2.0791767, 0.31263396, 1.2412993, 1.2412993, 1.5932736, 1.5932736, 0.5687224, 0.25163025, 0.25163025, -0.17724662, 0.19939707, 1.9457614, 1.9457614, 1.0723007, 1.0723007, 0.2791965, 0.004174999, 0.8528891, 0.8528891, 0.13468726, 0.13468726, 0.10233585, 1.9677297, 1.9677297, 0.44768322, -0.15065365, -0.15065365, -0.23469436, 0.8997308, 0.8997308, 0.32003194, 2.0791767, 2.0791767, 0.4723279, 0.9663058, 0.9663058, 1.5932736, 1.5932736, 0.74380016, 0.74380016, 0.5921491, -0.3937337, 0.085252576, 1.6616518, 1.6616518, 2.3602285, 2.3602285, 0.5555456, 0.43952233, 0.99914986, 0.99914986, 2.1600132, 2.1600132, -0.01155552, 0.83949065, 1.0923389, 1.0923389, 0.5668694, 0.5668694, -0.23469436, 0.8997308, 0.8997308, 0.76354146, 1.5703039, 1.5703039, 0.4723279, 0.9663058, 0.9663058, 0.21023144, 0.8961672, 0.8961672, 0.74380016, 1.1781894, 1.1781894, -0.94154567, 1.6616518, 1.6616518, 2.3602285, 2.3602285, 0.5555456, 0.43952233, 0.99914986, 0.99914986, 2.1600132, 2.1600132, -0.100301705, 0.302561, 1.0923389, 1.0923389, 0.5668694, 0.5668694, -0.5062735, -0.48948243, 0.76354146, 0.76354146, 0.1926161, 0.1926161, -0.34341785, -0.84721017, -1.2028884, -1.2028884, 0.8961672, 0.8961672, 0.15865193, 1.1781894, 1.1781894, -0.94154567, 0.25471553, 0.25471553}),\n\t\t\t),\n\t\t},\n\t\t{\n\t\t\tinputT: tensor.New(tensor.WithShape(1, 3, 32, 32),\n\t\t\t\ttensor.WithBacking([]float64{1.7640524, 0.4001572, 0.978738, 2.2408931, 1.867558, -0.9772779, 0.95008844, -0.1513572, -0.10321885, 0.41059852, 0.14404356, 1.4542735, 0.7610377, 0.121675014, 0.44386324, 0.33367434, 1.4940791, -0.20515826, 0.3130677, -0.85409576, -2.5529897, 0.6536186, 0.8644362, -0.742165, 2.2697546, -1.4543657, 0.045758516, -0.18718386, 1.5327792, 1.4693588, 0.15494743, 0.37816253, -0.88778573, -1.9807965, -0.34791216, 0.15634897, 1.2302907, 1.2023798, -0.3873268, -0.30230275, -1.048553, -1.420018, -1.7062702, 1.9507754, -0.5096522, -0.4380743, -1.2527953, 0.7774904, -1.6138978, -0.21274029, -0.89546657, 0.3869025, -0.51080513, -1.1806322, -0.028182229, 0.42833188, 0.06651722, 0.3024719, -0.6343221, -0.36274117, -0.67246044, -0.35955316, -0.8131463, -1.7262826, 0.17742614, -0.40178093, -1.6301984, 0.46278226, -0.9072984, 0.051945396, 0.7290906, 0.12898292, 1.1394007, -1.2348258, 0.40234163, -0.6848101, -0.87079716, -0.5788497, -0.31155252, 0.05616534, -1.1651498, 0.9008265, 0.46566245, -1.5362437, 1.4882522, 1.8958892, 1.1787796, -0.17992483, -1.0707526, 1.0544517, -0.40317693, 1.222445, 0.20827498, 0.97663903, 0.3563664, 0.7065732, 0.01050002, 1.7858706, 0.12691209, 0.40198937, 1.8831507, -1.347759, -1.270485, 0.9693967, -1.1731234, 1.9436212, -0.41361898, -0.7474548, 1.922942, 1.4805148, 1.867559, 0.90604466, -0.86122566, 1.9100649, -0.26800337, 0.8024564, 0.947252, -0.15501009, 0.61407936, 0.9222067, 0.37642553, -1.0994008, 0.2982382, 1.3263859, -0.69456786, -0.14963454, -0.43515354, 1.8492638, 0.67229474, 0.40746182, -0.76991606, 0.5392492, -0.6743327, 0.031830557, -0.6358461, 0.67643327, 0.57659084, -0.20829876, 0.3960067, -1.0930616, -1.4912575, 0.4393917, 0.1666735, 0.63503146, 2.3831449, 0.94447947, -0.91282225, 1.1170163, -1.3159074, -0.4615846, -0.0682416, 1.7133427, -0.74475485, -0.82643855, -0.09845252, -0.6634783, 1.1266359, -1.0799315, -1.1474687, -0.43782005, -0.49803245, 1.929532, 0.9494208, 0.08755124, -1.2254355, 0.844363, -1.0002153, -1.5447711, 1.1880298, 0.3169426, 0.9208588, 0.31872764, 0.8568306, -0.6510256, -1.0342429, 0.6815945, -0.80340964, -0.6895498, -0.4555325, 0.017479159, -0.35399392, -1.3749512, -0.6436184, -2.2234032, 0.62523144, -1.6020577, -1.1043833, 0.05216508, -0.739563, 1.5430146, -1.2928569, 0.26705086, -0.039282817, -1.1680934, 0.5232767, -0.17154633, 0.77179056, 0.82350415, 2.163236, 1.336528, -0.36918184, -0.23937918, 1.0996596, 0.6552637, 0.64013153, -1.616956, -0.024326125, -0.7380309, 0.2799246, -0.09815039, 0.9101789, 0.3172182, 0.78632796, -0.4664191, -0.94444627, -0.4100497, -0.017020414, 0.37915173, 2.259309, -0.042257152, -0.955945, -0.34598178, -0.463596, 0.48148146, -1.540797, 0.06326199, 0.15650654, 0.23218104, -0.5973161, -0.23792173, -1.424061, -0.49331987, -0.54286146, 0.41605005, -1.1561824, 0.7811981, 1.4944845, -2.069985, 0.42625874, 0.676908, -0.63743705, -0.3972718, -0.13288058, -0.29779088, -0.30901298, -1.6760038, 1.1523316, 1.0796186, -0.81336427, -1.4664243, 0.5210649, -0.57578796, 0.14195317, -0.31932843, 0.69153875, 0.6947491, -0.7255974, -1.383364, -1.5829384, 0.6103794, -1.1888592, -0.5068163, -0.596314, -0.052567296, -1.9362798, 0.1887786, 0.52389103, 0.08842209, -0.31088617, 0.097400166, 0.39904633, -2.7725928, 1.9559124, 0.39009333, -0.6524086, -0.39095336, 0.49374178, -0.11610394, -2.0306845, 2.064493, -0.11054066, 1.0201727, -0.69204986, 1.5363771, 0.2863437, 0.60884386, -1.0452534, 1.2111453, 0.68981814, 1.3018463, -0.6280876, -0.48102713, 2.3039167, -1.0600158, -0.1359497, 1.1368914, 0.09772497, 0.5829537, -0.39944902, 0.37005588, -1.3065269, 1.6581306, -0.11816405, -0.6801782, 0.6663831, -0.4607198, -1.3342584, -1.3467175, 0.69377315, -0.15957344, -0.13370156, 1.0777438, -1.1268258, -0.7306777, -0.3848798, 0.09435159, -0.042171452, -0.2868872, -0.0616264, -0.10730527, -0.7196044, -0.812993, 0.27451634, -0.8909151, -1.1573553, -0.31229225, -0.15766701, 2.2567234, -0.7047003, 0.9432607, 0.7471883, -1.1889449, 0.77325296, -1.1838807, -2.6591723, 0.60631955, -1.7558906, 0.45093447, -0.6840109, 1.6595508, 1.0685093, -0.4533858, -0.6878376, -1.2140774, -0.44092262, -0.28035548, -0.36469355, 0.15670386, 0.5785215, 0.34965447, -0.76414394, -1.4377915, 1.3645319, -0.6894492, -0.6522936, -0.52118933, -1.8430696, -0.477974, -0.4796558, 0.6203583, 0.6984571, 0.003770889, 0.93184835, 0.339965, -0.015682112, 0.16092817, -0.19065349, -0.3948495, -0.26773354, -1.1280113, 0.2804417, -0.9931236, 0.8416313, -0.24945858, 0.04949498, 0.4938368, 0.6433145, -1.5706234, -0.20690368, 0.8801789, -1.6981058, 0.38728046, -2.2555642, -1.0225068, 0.038630553, -1.6567152, -0.98551077, -1.471835, 1.648135, 0.16422775, 0.5672903, -0.2226751, -0.35343176, -1.6164742, -0.29183736, -0.7614922, 0.8579239, 1.1411018, 1.4665787, 0.85255194, -0.5986539, -1.1158969, 0.7666632, 0.3562928, -1.7685385, 0.3554818, 0.8145198, 0.058925588, -0.18505368, -0.8076485, -1.4465348, 0.800298, -0.30911446, -0.23346666, 1.7327212, 0.6845011, 0.370825, 0.1420618, 1.5199949, 1.7195894, 0.9295051, 0.5822246, -2.094603, 0.12372191, -0.13010696, 0.09395323, 0.9430461, -2.7396772, -0.56931204, 0.26990435, -0.46684554, -1.4169061, 0.8689635, 0.27687192, -0.97110456, 0.3148172, 0.8215857, 0.005292646, 0.8005648, 0.078260176, -0.39522898, -1.1594205, -0.085930765, 0.19429293, 0.87583274, -0.11510747, 0.4574156, -0.964612, -0.78262913, -0.1103893, -1.0546285, 0.8202478, 0.46313033, 0.27909577, 0.3389041, 2.0210435, -0.4688642, -2.2014413, 0.1993002, -0.050603542, -0.51751906, -0.97882986, -0.43918952, 0.18133843, -0.5028167, 2.4124537, -0.96050435, -0.79311734, -2.28862, 0.25148442, -2.0164065, -0.53945464, -0.27567053, -0.70972794, 1.7388726, 0.99439436, 1.3191369, -0.8824188, 1.128594, 0.49600095, 0.77140594, 1.0294389, -0.90876323, -0.42431763, 0.86259604, -2.6556191, 1.5133281, 0.55313206, -0.045703962, 0.22050765, -1.0299352, -0.34994337, 1.1002843, 1.298022, 2.696224, -0.07392467, -0.65855294, -0.51423395, -1.0180418, -0.07785475, 0.38273242, -0.03424228, 1.0963469, -0.2342158, -0.34745064, -0.5812685, -1.6326345, -1.5677677, -1.179158, 1.3014281, 0.8952603, 1.3749641, -1.3322116, -1.9686247, -0.6600563, 0.17581895, 0.49869028, 1.0479722, 0.28427967, 1.7426687, -0.22260568, -0.9130792, -1.6812183, -0.8889713, 0.24211796, -0.8887203, 0.9367425, 1.4123276, -2.369587, 0.8640523, -2.239604, 0.40149906, 1.2248706, 0.064856105, -1.2796892, -0.5854312, -0.26164544, -0.18224478, -0.20289683, -0.10988278, 0.21348006, -1.2085737, -0.24201983, 1.5182612, -0.38464543, -0.4438361, 1.0781974, -2.5591846, 1.1813786, -0.63190377, 0.16392857, 0.09632136, 0.9424681, -0.26759475, -0.6780258, 1.2978458, -2.364174, 0.020334182, -1.3479254, -0.7615734, 2.0112567, -0.044595428, 0.1950697, -1.7815628, -0.7290447, 0.1965574, 0.3547577, 0.61688656, 0.008627899, 0.5270042, 0.4537819, -1.8297404, 0.037005723, 0.76790243, 0.5898798, -0.36385882, -0.8056265, -1.1183119, -0.13105401, 1.1330799, -1.9518042, -0.6598917, -1.1398025, 0.7849575, -0.5543096, -0.47063765, -0.21694957, 0.44539326, -0.392389, -3.046143, 0.5433119, 0.43904296, -0.21954103, -1.0840366, 0.35178012, 0.37923554, -0.47003287, -0.21673147, -0.9301565, -0.17858909, -1.5504293, 0.41731882, -0.9443685, 0.23810315, -1.405963, -0.5900577, -0.110489406, -1.6606998, 0.115147874, -0.37914756, -1.7423562, -1.3032428, 0.60512006, 0.895556, -0.13190864, 0.40476182, 0.22384356, 0.32962298, 1.285984, -1.5069984, 0.67646074, -0.38200897, -0.22425893, -0.30224973, -0.3751471, -1.2261962, 0.1833392, 1.670943, -0.05613302, -0.0013850428, -0.687299, -0.11747455, 0.46616644, -0.37024245, -0.45380405, 0.40326455, -0.91800475, 0.25249663, 0.8203218, 1.3599485, -0.09038201, 1.3675972, 1.0344099, -0.99621266, -1.2179385, -0.30496365, 1.0289356, -0.07228701, -0.6006576, 1.5522432, 0.28690448, -2.3205943, 0.31716064, 0.52004063, 0.22560866, 0.4497121, -0.067275606, -1.3183959, -0.370704, -0.94561577, -0.9327409, -1.2630683, 0.45248908, 0.097896144, -0.44816536, -0.64933795, -0.023423105, 1.0791948, -2.0042157, 0.37687653, -0.545712, -1.8845859, -1.945703, -0.9127835, 0.21950956, 0.39306292, -0.9389816, 1.017021, 1.4229835, 0.39608657, -0.59140265, 1.1244192, 0.7553957, 0.86740744, -0.6564637, -2.8345544, 2.116791, -1.6108783, -0.035768073, 2.3807454, 0.33057675, 0.94924647, -1.5023966, -1.7776669, -0.5327028, 1.0907497, -0.34624946, -0.7946363, 0.19796729, 1.0819352, -1.4449402, -1.210543, -0.7886692, 1.0946383, 0.23482153, 2.1321535, 0.9364457, -0.035095178, 1.2650778, 0.21149701, -0.70492136, 0.67997485, -0.6963267, -0.2903971, 1.3277828, -0.10128149, -0.8031414, -0.46433768, 1.0217906, -0.55254066, -0.38687086, -0.51029277, 0.1839255, -0.38548976, -1.6018361, -0.8871809, -0.932789, 1.2433194, 0.81267405, 0.58725935, -0.50535834, -0.81579155, -0.5075176, -1.0518801, 2.4972005, -2.2453218, 0.56400853, -1.2845523, -0.10434349, -0.98800194, -1.177629, -1.1401963, 1.7549862, -0.13298842, -0.7657022, 0.55578697, 0.010349315, 0.72003376, -1.8242567, 0.30360392, 0.7726948, -1.6615983, 0.44819528, 1.6961815, -0.014857704, 0.82140595, 0.67057043, -0.7075057, 0.039766736, -1.5669947, -0.45130304, 0.26568797, 0.7231005, 0.024612125, 0.71998376, -1.1029062, -0.10169727, 0.019279385, 1.8495913, -0.21416666, -0.49901664, 0.021351224, -0.91911346, 0.19275385, -0.3650552, -1.7913276, -0.058586553, -0.3175431, -1.6324233, -0.06713416, 1.4893559, 0.5213038, 0.6119272, -1.3414967, 0.47689837, 0.14844958, 0.5290452, 0.4226286, -1.3597807, -0.041400813, -0.75787085, -0.050084095, -0.8974009, 1.3124703, -0.8589724, -0.8989422, 0.07458641, -1.0770991, -0.4246633, -0.8299646, 1.411172, 0.78580385, -0.057469517, -0.39121705, 0.9409176, 0.4052041, 0.49805242, -0.026192237, -1.68823, -0.112465985, -0.5324899, 0.6450553, 1.0118425, -0.65795106, 0.46838522, 1.735879, -0.66771275, 1.6819217, -0.85258585, 0.022959756, -0.011145612, 0.0114989, -0.837678, -0.5911831, -0.66772026, 0.3269626, 0.33003512, 2.2259443, 1.370989, -0.50984323, 0.3248696, 0.997118, 0.030601824, -0.069641575, 0.05157494, 0.8672766, -0.84832054, -0.32566947, 0.47043315, 0.31144708, 0.23958276, -0.36980116, 0.9725358, 2.1338682, 0.4064155, -0.1931767, 0.7557403, -0.53913265, -0.74969035, 0.032808747, -2.5827966, -1.1539503, -0.34796184, -1.3533889, -1.0326431, -0.43674833, -1.6429653, -0.40607178, -0.53527015, 0.025405208, 1.154184, 0.17250441, 0.021062022, 0.099454455, 0.22739278, -1.0167387, -0.11477532, 0.30875126, -1.37076, 0.8656529, 1.0813761, -0.63137597, -0.24133779, -0.87819034, 0.69938046, -1.0612223, -0.222477, -0.8589199, 0.05095428, -1.7942293, 1.3264617, -0.9646064, 0.059894685, -0.21252304, -0.7621145, -0.88778013, 0.93639857, -0.5256406, 0.2711702, -0.80149686, -0.64718145, 0.47224715, 0.9304085, -0.17531641, -1.4219198, 1.997956, -0.8565493, -1.5415874, 2.5944245, -0.4040323, -1.4617327, -0.6834398, 0.3675449, 0.19031155, -0.8517292, 1.8227236, -0.5215797, -1.1846865, 0.9606934, 1.3290628, -0.8174931, -1.4013473, 1.0304383, -2.0473237, -1.2266216, 0.96744615, -0.055352546, -0.26393735, 0.3528166, -0.15277442, -1.2986867, 1.2760754, 1.325014, 0.20533256, 0.045134015, 2.339625, -0.27643284, -0.25957698, 0.36448124, 1.471322, 1.5927707, -0.25857264, 0.30833125, -1.3780835, -0.3119761, -0.84029037, -1.0068318, 1.6815767, -0.79228663, -0.5316059, 0.36584878, 1.2978252, 0.48111513, 2.759355, -0.074667975, 0.25871643, 0.27560067, 1.4350494, 0.5072389, -0.1162297, -0.9474886, 0.24444346, 1.4013448, -0.4103818, 0.5289436, 0.24614778, 0.86351967, -0.8047537, 2.346647, -1.2791611, -0.36555108, 0.9380925, 0.29673317, 0.82998616, -0.49610233, -0.074804984, 0.012231983, 1.5692596, 0.69042903, 0.7966721, -0.6579261, 0.9688826, 0.22558166, 1.3891454, 2.0140603, -0.30676576, -0.40630314, -0.86404496, -0.14357951, -0.38202545, 0.3595044, -0.14456682, -0.36159927, 1.0645851, -0.9378802, 0.43310794, -0.40594172, 0.7243685, 1.3852615, -0.30309826, 0.44103292, 0.17879286, -0.7994224, 0.2407875, 0.2891205, 0.41287082, -0.1983989, 0.0941923, -1.1476109, -0.35811406, 0.5559627, 0.8924739, -0.42231482, 0.10471403, 0.22805333, 0.20147994, 0.5407736, -1.8180777, -0.04932407, 0.2390336, -1.0003303, 1.6739857, 0.16155927, 1.5634048, -0.790523, -0.9073001, 0.22425222, -1.6786884, 0.2149656, 0.09721923, 1.0156653, 0.70104134, -0.41747734, -1.0974966, 1.7123052, -0.79211503, -1.0455246, -1.084856, 1.1173053, -0.5189002, -0.7537045, 0.13768983, -0.2069447, -0.67809546, 0.7539915, 1.0653155, 0.9853175, 0.7669197, 0.40262553, -1.775888, 1.6692508, 0.3019892, 0.60815644, 1.1149623, 1.4333525, 0.41839802, 0.43554616, -0.59922427, 0.03308975, -0.85416126, -0.71994054, -0.8935744, -0.15602389, 1.0490932, 3.1709747, 0.18949963, -1.3484131, 1.2649833, -0.30078387, -0.6606086, 0.20984948, -1.2406245, 0.22246316, -0.08837552, 0.098377906, 0.38141626, 0.067492254, 0.016338084, 0.2843145, 0.41540062, -1.0314825, -1.4299912, -0.061638054, -1.4327354, 0.08753147, 0.93874687, 0.6071117, -1.0481704, -0.86026245, 0.32830128, -0.4012978, -0.3166553, 0.5969065, -0.9872867, -0.40123472, -0.8000825, -1.0431294, -0.8570782, 0.67746216, 0.05182039, -0.87916064, -0.2311016, -1.6388073, -0.7333128, 2.1495745, -0.090243846, 0.73165894, -0.065488376, 0.34816924, 0.6632581, -1.1046166, -0.030936258, 1.5788652, -0.7955006, -0.56643987, -0.30769128, 0.26902407, 0.52491784, 1.2674117, 0.49949825, -0.062053125, 1.2591671, 0.70411104, -1.4956795, 2.5263681, 1.7699214, -0.16821422, 0.3779101, 1.3243587, -0.1722008, 0.7303518, 1.1045785, -1.0148259, -0.6023319, 0.9214084, 0.46081448, 0.92379653, -0.13256802, -0.28900522, -1.9986395, -1.1460004, 0.047066096, 0.82455724, 0.53117836, -0.12824197, -0.27177158, 0.21717963, 0.07821118, 1.4045455, 0.14644077, -1.481246, -1.2725581, 1.5187594, -1.1711605, 0.76449746, -0.26837274, -0.16975829, -0.13413279, 1.221385, -0.19284183, -0.033319283, -1.5308034, 0.2066905, 0.5310425, 0.23914558, 1.3978963, 0.055171356, 0.29897746, 1.648504, -1.5500141, -0.45582536, 1.4261588, 0.93612915, 0.6783801, 0.8326507, 0.3270662, 1.6315974, 0.37775916, 0.2398671, 0.15895867, 0.19286396, -1.1570172, 0.77067304, -0.13043973, 1.8219151, -0.07565047, 0.4209183, 0.24660219, -0.625557, 0.99213684, 1.9050636, -0.01477722, -0.3004788, -0.35502872, -1.8923619, -0.17781314, 0.2509981, 1.054758, 0.9600477, -0.41649908, -0.27682298, 1.1239053, -0.1734639, -0.51002955, 1.3925184, 1.0375856, 0.018791791, -0.5937774, -2.0118804, 0.5897036, -0.8963697, -1.962732, 1.5848205, 0.6479678, -1.1390082, -1.2144014, 0.8709618, -0.87797064, 1.2961498, 0.6164593, 0.53659654, 0.40469545, 0.19145088, 0.8805112, -0.45408037, 0.08595198, 0.75194657, 0.5629897, -1.1949868, -0.50040966, 0.2528035, -0.4080147, 1.7746586, -0.3931532, -0.16221845, 0.76943016, 0.33053273, -0.14527446, -0.7564935, 0.30151406, 1.0390965, 0.47909522, -0.7781835, 1.7367749, -1.4465779, -1.5826856, 0.9605572, 0.22584048, -0.54949856, -1.0985707, 2.3207998, 0.11709087, 0.53420115, 0.3178851, 0.43480796, 0.54009444, 0.732424, -0.3752224, -0.29164198, -1.7410228, -0.78030443, 0.2711128, 1.0450233, 0.59903955, -0.34069234, -1.2631729, -2.7773592, 1.151734, -0.589229, -0.44846502, 0.13157398, -1.40556, -0.34978217, 2.0234718, 0.50538695, 0.35924914, -1.5824945, 2.2436018, -1.4227949, 1.9223248, -2.115056, 1.4053655, 1.6180543, -0.8244091, 0.42258036, 0.5474806, -0.8137945, -1.4491177, -1.3177173, 0.54100823, -0.085115604, -0.564301, 0.966768, 0.5080679, -0.7554627, -1.2012016, 0.5232617, -0.53758335, 0.09920486, 1.576299, 0.5023282, -0.862267, 0.16066119, -0.95264494, 1.6085222, -0.56157875, 0.20727074, 0.30773258, 0.15925047, -1.9585489, -1.446421, -0.4523503, 0.31943184, -0.13777922, -0.9571475, -1.3484243, -0.40155753, -0.46847606, 0.51283646, -0.32631847, 0.6027077, -0.5946498, -0.25595766, -0.3480464, -0.782367, 0.6251187, -0.813596, -0.5216415, -0.07311965, -1.2973796, -0.32493496, -0.71130633, -0.38815418, -0.059928004, -0.79991364, -0.22007579, 1.3086687, -0.025798557, 1.1452621, 0.34649444, 0.7741606, -0.77445894, 0.10490716, 0.13391292, -0.6126257, -0.82282835, -1.4902654, 1.4961396, -0.9724029, 1.3462211, -0.46749318, -0.8624933, 0.62251914, -0.63119197, 0.5684589, -0.33281177, 0.4804245, -0.9681861, 0.83135104, 0.48797268, -0.9196507, 2.6429358, 0.54012305, 2.290467, 1.6002678, -0.18883479, -0.41227177, -0.4034592, -1.8300285, -0.6958351, 0.24676603, 1.5259576, -0.7727719, 0.8820566, -1.2525934, -0.58632004, -0.4576406, 0.3718111, 0.45730963, 0.9623417, 0.77083695, 0.24316822, 0.39036494, 1.5885307, -0.5109262, 0.7747283, -1.808144, 0.41133425, -0.48324955, 0.0025711823, 1.0400863, 0.16464381, 0.88518757, 1.4737648, 0.38909397, 1.171041, -0.32656097, -0.008209882, -0.5226194, 1.0429776, 0.41409135, -0.50723445, 0.15466884, 1.0415684, -0.03926799, -0.9489328, 0.13191175, -1.9805655, 0.76877064, -0.4213276, -0.46931073, 0.8756957, -1.3651628, 1.9470986, -0.48024204, -0.52325094, 1.0212247, 0.7086953, 2.4512298, -0.21120599, -0.120406635, -1.479316, -0.33210227, -0.7214313, -0.448767, -1.7441877, 1.6606076, -1.4166034, -2.8022027, -1.1884245, -0.6038396, -1.149554, 1.0983036, -0.13783918, 0.025385605, 0.61039174, 0.28601253, 0.9785673, -1.1094775, -0.5475181, 0.66596717, -2.5345545, -1.3751845, 0.50099224, -0.48024905, 0.9361076, 0.8091803, -1.1980929, 0.4066571, 1.2016978, 0.1474344, -0.97746485, 0.87938994, 0.63542455, 0.54261076, 0.71593887, -2.994613, 0.8809376, 1.8081318, 0.43663847, 0.192729, 0.69643867, 0.33822548, 0.65178126, 0.0014710003, -0.76670486, -1.0043228, -0.9981917, -1.3730426, -1.067742, 1.7612661, 0.7540957, -0.6250274, -0.3903927, 0.11255753, -0.65554506, 0.067516856, 0.77760416, -0.035742734, 0.33601573, 0.88649154, -0.27213177, 0.2847906, -0.30937758, -0.02852887, -0.32473028, -0.52886987, 0.17371185, 0.5665453, 0.14630444, 0.49872696, -0.7379318, -1.2037352, 0.4170435, 0.6878814, 0.049857266, 1.3480358, 0.9076988, 2.6805708, -0.20080851, -0.9988488, -0.7401368, -0.5654978, 0.4760314, -2.1580687, 1.3185511, -0.23929659, -0.24679355, -1.0793432, -0.11422555, 0.013239767, -0.12194493, 0.33905926, -0.58963203, -0.8958158, 0.5483281, 0.09866745, 0.19718106, 1.0590272, -1.0225644, -0.85524046, 1.2572197, -1.4828833, -1.3094121, 0.81786186, 0.23820019, 0.105232134, -0.09165941, 0.031267546, -0.09211212, 1.3554426, -0.39814812, -0.16137354, 1.7944489, 0.027509702, 2.2320163, -0.1049797, 1.367415, -1.655344, 0.15364446, -1.5844736, 0.8444543, -1.2128679, 0.28376955, -0.28219587, -1.1582032, -1.61936, -0.51104045, 1.7406294, -0.29348505, 0.91722155, -0.057042867, 0.87672675, -1.8269113, -0.40318832, 0.94940555, -0.16325495, -0.086455286, -0.4304619, 1.1493794, 0.29751435, 0.044022277, 0.64305454, 0.58822495, 0.21258704, 1.5470315, -0.060287535, 0.27808106, -0.64295256, 0.15011522, 1.5877615, -0.6432576, -1.1335928, 0.99675965, -0.14876615, 0.0960042, -0.045113303, 0.079121724, 0.8505307, -0.8391242, -1.0117741, 0.084968135, -1.6064397, -1.3730536, 1.8666831, 0.75746834, -0.010056471, 1.238007, -1.0405992, -0.31560314, 0.6234536, 0.8906717, 0.51291686, -2.5412388, -0.96808213, 0.4770681, -0.3559515, 2.5402317, 0.9265583, 0.55808187, -1.1169496, -0.03529674, 0.24120396, 1.1277837, 0.8811311, 1.0329891, -0.923912, 1.4121517, -1.3804307, -0.53591454, 0.43077114, -0.14989159, -1.0060369, -0.82154983, -1.5482544, 0.5319746, 1.2605689, -0.100393504, -0.4003488, -1.472323, 0.9132019, 2.2113044, -1.7974558, -1.0634329, -0.679593, -0.5643179, 0.22734594, 1.6142496, 1.0085973, 0.52759737, -0.7239287, -1.1196282, -0.7967753, 1.5480669, -0.0617433, -0.44683626, -0.18375573, 0.8246182, -1.3128496, 1.4148741, 0.15647626, -0.21634398, 0.44284612, 0.21839707, -0.34419647, -0.25271067, -0.86886257, 0.6563907, -0.5319938, -0.9562584, 0.16586353, 1.3291413, -0.048344623, -0.60810125, 0.40389603, 1.9367125, -1.4519055, 0.38220277, 0.20508662, 1.1615338, 0.99090916, -0.1867091, -1.6845173, 0.8065638, -0.8351927, -0.9467404, 1.1483506, -0.9108504, 1.4028448, 0.33584473, 0.3191184, 0.30726478, -1.6384237, -1.7763886, 0.21555306, 0.56800735, 0.08261103, -0.8215345, 0.018922104, -0.082034156, -0.9571581, 1.0139722, -1.7302761, 0.58874243, 0.38432342, 1.0097119, -1.0053118, 0.10140715, 2.171165, 0.66207427, 0.10058121, 0.53916126, 0.08617684, 2.190898, 0.9836362, -0.08561496, 0.25233144, -0.390798, 1.2098501, -1.4061048, -1.6047385, 1.4587147, 2.1531198, 0.4683049, 0.11273794, 0.6572677, -0.64705354, 0.17124355, 0.038908705, 0.62656426, -1.5579985, -0.5070348, 0.8449956, -0.67559385, -0.99336135, 2.042072, 0.038118, -0.57891816, -1.6923704, 0.72934633, 0.69913614, -0.2987596, -1.1022302, -0.024549423, -0.8358561, -0.9420936, -0.10321275, -1.0513904, 0.24664895, 0.60799253, -0.83963245, -1.3682451, 1.5612797, -0.94027025, -0.6599427, 0.21301717, 0.59936935, -0.2563169, 0.46079433, -0.40098616, -0.97117066, 1.4263169, 2.4884417, 1.6959696, 0.14180663, 1.8334354, 0.3557035, -0.47728628, 0.46637958, -0.09439251, -0.9831182, -0.898322, 0.8020517, -1.846532, 0.60413677, -1.6295836, -2.1211765, -1.8388466, 1.966764, -0.19623396, 0.08658318, 1.419255, 0.9341797, -1.3915052, 0.86900634, 0.18418126, -0.34167808, 0.024290914, 1.279812, -0.8859665, 0.40088567, -0.009657237, -1.7971646, -0.8022532, 0.19321355, 1.2973421, 1.001331, 0.5972125, -0.81527567, 1.801214, 0.21524046, -1.0063655, -0.18290497, 0.8962484, 0.0076174983, 0.88686466, 1.103694, 0.4005307, -0.8577026, 0.13545467, 0.045165855, 1.8593464, -1.6263219, -0.13482246, -0.5840936, 0.33510563, -2.4375644, 1.1149246, 0.013748487, -1.8447012, -0.36111313, 0.60896236, -1.5914478, 0.0032222164, -1.0574737, -0.55598503, 0.026738383, 0.18345025, -0.4707425, 0.2727964, 0.8179776, -0.27891427, 1.4315678, 1.4622141, -0.42870206, -0.63784057, -1.664173, -0.12656933, -0.36343777, 0.77905124, -1.5096616, -0.2773914, 0.9687444, -0.7303571, -0.7623615, -1.4469403, 2.6205738, -0.7474732, -1.3003469, -0.8038504, -0.7742951, -0.26938978, 0.8253722, -0.29832315, -0.9228233, -1.4513385, 0.021857359, 0.042539075, 1.5309323, 0.092447735, -0.099008314, -1.0506538, -0.30595258, -0.43847445, -0.37016416, -0.9592554, 0.5383296, -0.14244542, -0.20035347, -1.7140461, 0.4936441, 0.48701534, -0.8391294, 0.99012136, -1.3647583, -0.021869909, -0.27120733, -1.3171748, 0.18970262, 1.7025702, 0.06763423, -0.46302176, 0.44702417, 0.10572, 0.027762132, -0.4255422, 1.4219756, 0.45636335, -0.52867067, -0.10800384, -0.7408667, -0.60829115, -0.64072573, -1.1343116, 0.777277, -0.29104146, 0.5541276, -0.6701259, -0.060362495, -0.7110406, 0.71966815, -0.2484193, -0.7308736, -1.6417032, 0.27566653, -0.70838505, -0.015779218, -0.4917301, 0.9541896, 0.54414475, 0.4472121, -0.6161211, 0.46629006, 1.7148316, -0.83218604, 0.17233914, -1.649217, 1.3985621, -0.39791209, 0.7825789, -1.7232282, 1.7975394, -0.35687152, 0.54565734, 0.1508182, -0.25547078, 1.6857923, -1.6480463, 0.29871365, 0.91064566, -0.029856121, -0.11817078, -0.14268771, -1.2276365, 0.038127385, 0.51271755, 0.068599224, -0.2722761, -0.48972502, -0.27929667, 1.2577442, -2.0866349, 0.040071458, -0.3277549, 1.4558079, 0.055492226, 1.4849256, -2.12389, 0.4595849, 0.28005785, 1.3905339, -1.6413486, -0.15503581, 0.06606026, -0.4957955, 1.2165778, -0.33868217, 2.0347626, 1.0541779, 0.9508337, 0.559299, -1.0636955, -0.43109635, 0.57275134, 0.67755705, 1.3071839, -0.46744102, -0.8601534, 0.8591042, -0.8096266, 0.8733118, 1.1997361, 0.45615304, -0.35757902, 0.041082226, 0.5934659, 0.010185518, 2.1982963, -0.9906709, -1.0026686, -0.9768954, -0.58957994, -2.1789315, -0.6296504, -0.6532847, 0.078514025, 0.41780058, -1.2402164, 0.9000542, 1.8022423, -0.20828511, 1.5743712, 0.1989895, 1.9887319, 1.1172835, -1.5639046, 0.01862737, 1.054325, 0.030546581, -0.03688353, 1.2697648, -0.7098542, 0.017515613, 0.32362577, -0.33379096, -0.020129103, 0.7750233, 0.43283764, -0.80871755, -1.104124, -0.7891022, 0.0012484558, -0.15993978, -0.8319575, -0.59815043, -1.5200393, 0.4178537, -0.040018726, -1.2597873, 0.028620504, 1.342622, -0.7399359, 1.3151376, -0.32345748, 0.19782817, 0.097750805, 1.4015235, 0.15843384, -1.1419014, -1.3109704, -1.5329211, -1.7119702, 0.04613506, -0.9583745, -0.08081161, -0.70385903, -0.7707843, -0.48084533, 0.70358557, 0.92914516, 0.37117255, -0.98982257, 0.6436313, 0.68889666, 0.2746472, -0.6036204, 0.70885956, 0.42281857, -3.1168566, 0.64445204, -1.9137427, 0.6635616, -0.1540724, 1.1936116, -0.09816121, -0.88661426, -0.14735366, 1.0598063, 0.026246618, -0.11433516, 0.7435535, 0.21035936, -0.005927406, 1.36606, 1.555114, 0.61332625, -0.28595915, 1.496911, 1.1831195, 0.71889716, -1.2160766, 0.14067191, -0.7436722, -0.15901226, 0.24005693, 0.10015941, -0.4751751, 1.2729537, -1.6961312, 0.73018354, -1.8574833, 0.38259813, -0.8869043, 0.87830377, 0.08645252, 0.24770638, -1.0182793, -0.65457016, 0.2072174, 0.58356994, 2.9290962, 0.22285832, 0.9760375, -1.5569339, -1.3298919, -0.35549477, -1.1974277, 1.4863993, -0.4102187, 1.3821819, 1.4867824, 0.04277972, 0.50179976, -0.056099474, 0.538437, 0.48334184, -0.12364963, 0.50496995, 1.7236962, 0.7130162, 0.3257996, 0.124769524, -1.0126731, -1.0272969, 0.32335654, -1.3693911, -0.7663276, 1.2815113, 1.9142298, -1.665956, 1.6266496, -0.2114383, -0.0150050875, -0.11341163, 1.0805441, -1.6076766, 0.45616361, -0.9448702, 0.5707885, 1.5427964, -0.0004173264, 0.37415507, 0.40955177, -0.7995935, 1.5116394, 1.7064682, 0.70178336, 0.07328543, -0.46189383, -0.62649024, 1.7108365, 1.414415, -0.063661486, -1.5799305, -2.832012, -1.0834267, -0.13062039, 1.400689, -0.6516562, 0.50481546, 1.3031809, 0.12853631, -0.14244787, -1.3087635, -1.2024753, 0.41609964, -0.20090753, 0.12253132, -0.047277715, 0.66414404, -0.7846874, -0.33558065, 1.8961822, -0.79978615, -0.28157544, -0.5893867, 0.44478136, 1.0223923, -0.49821162, -0.43141434, -0.2789816, 0.5298338, -0.7393953, -0.37595996, -2.3721938, -1.381745, -0.11244375, 0.89786416, 0.29507577, -1.0987685, -1.4002562, 0.1746801, -1.6528037, 1.0659268, 0.063896194, -1.6073202, -0.9659539, -0.7243113, -0.7731925, -1.489933, -0.8746625, -0.6844016, -0.71128577, 1.1279566, 0.10482781, -0.9932572, -0.3346216, -0.8795571, -0.30000666, 0.87550914, 0.2522708, 2.2856011, 0.37592742, -0.9135945, 0.8097407, 1.0799313, 1.094167, -1.0942409, -0.14763741, 1.131812, -1.684729, -0.49941677, -1.4269377, -0.9325702, -1.0124571, 1.2505698, -0.23453803, -0.8633556, -1.0356058, 0.14166717, -0.0111356275, 1.3440744, 0.5000167, -1.4317977, -0.6289807, 1.0700725, -0.6210827, 1.7345722, -1.0982895, 0.57261336, -0.86121553, -0.50959516, 1.0985817, -0.12706716, 0.81345224, 0.4732906, 0.75386566, -0.8881882, -0.2215744, 0.42425263, -0.8490729, 1.6295, -0.77722806, -0.3000036, -1.006559, -2.1433082, 1.7969185, -0.20433894, -0.44791484, -0.19871506, 1.4198639, -0.9651066, 0.6795679, -0.42378825, -0.59667087, 0.5670582, 0.9882406, -0.51390296, -0.76884913, -1.1690958, 1.1035038, -0.575256, -1.8491307, 1.4099522, -1.3698595, 0.77946055, 0.18342865, 0.28791544, -0.58437526, 0.36559147, -1.6677799, 0.5880377, 1.55701, 0.8840272, -2.01954, -0.984209, -0.18779492, 0.4869373, -0.10665268, -0.4932144, 0.5953003, 1.1641518, -0.23229401, 0.7289299, -2.5790508, -0.93750936, -0.32125893, -0.48856622, 0.3327982, 1.0137506, 0.50666904, -0.62222546, -1.5227681, 0.5569641, -1.8381767, 0.6530373, -0.18844908, -1.175835, 0.2872573, -0.0028761027, -0.036597293, -0.0842233, 0.4195241, 0.924434, 0.4966152, 1.0121332, -0.04413972, 1.6184593, 0.57110983, -0.543694, -1.0938951, 0.20579681, -1.3065215, -0.973376, 0.23908707, -0.60788745, -0.93331623, -0.034475047, 0.072677895, -0.20583403, -0.3775469, 0.85464275, 0.34242734, -0.22342612, 2.4643219, 0.19383174, 1.1320051, -0.560981, -1.3629409, -0.7917565, -0.26800978, -0.4966082, 1.3363862, -0.120041125, 0.46146888, -0.046481155, -0.43355432, 0.037996013, 1.7140515, -0.76794857, 0.7669904, -1.0260073, -0.45962644, 0.0035832059, 0.3263751, 1.4831287, -0.050082643, -0.8436156, 0.650042, -0.3641698, 0.23868157, -0.11622244, -1.9434569, 0.5082992, 0.583368, 0.92660475, 1.8004627, -1.1951038, 0.51650745, 0.409295, -0.419082, 0.39710623, 0.49964696, -1.2186838, 0.24622276, -0.9179843, -0.6518565, -1.7747449, -0.47336093, -0.20357068, 0.54985684, 0.00089992664, -1.5422882, 0.86214805, -0.11858662, 0.4883706, 0.9659361, 1.4226048, 1.9612269, -0.07223876, 0.31112444, -1.078361, 1.0616002, -1.1848874, -1.8052517, 0.830386, -0.5216965, 0.77760726, 0.40807465, -1.6300026, -2.7196794, -1.0966017, 0.016491488, -1.2217764, -0.65276146, -1.4589407, 0.16987796, 0.09082593, -0.48139262, 1.3970653, 1.497715, 0.5652672, -1.7997712, -1.1046902, 0.40713033, -0.62855756, -0.48709142, 0.8989674, 0.5108748, 1.3141544, -0.4292093, 1.3752254, -0.55413127, 1.4994915, 0.10583464, -0.86050975, -1.6312195, -0.3014723, -0.2562327, 0.8576619, -0.1105905, -0.43243197, 1.0770375, -0.22482656, -0.5762418, 0.5746089, -0.48982823, 0.65880215, -0.5969171, -0.22295918, 0.15217698, -0.37412632, -0.013451469, 0.81547195, 0.4106018, 0.48096985, -0.63543046, 0.85282975, 0.66956234, 1.0044192, -0.7263658, -0.1724586, 0.6335339, -0.60881513, -0.22612247, 1.9258057, 1.951761, 1.2399405, 0.93858516, -1.0192511, 0.5125622, -0.35911658, -1.0585719, -0.50900584, 0.11566507, -0.5473556, -0.5507994, 0.7920415, 0.14410649, 0.23345809, 0.1118724, -0.67570317, -1.370572, 0.3105647, -0.5070366, -2.0107822, -0.39256725, -1.0922179, 0.69865024, 0.5216252, 0.49689314, -0.6650416, 0.7315516, 0.3196498, -0.40985453, -0.45333743, 0.8927082, -0.47360405, 0.30365646, 1.033957, 1.9093426, 1.6638731, 0.90082276, -1.5059114, -0.6890484, -0.5480872, 1.6531498, -0.69931793, 0.38616636, 0.10086706, -0.9351272, 0.38182402, 0.3982961, -1.2557749, 1.2228775, -2.08651, -0.59075713, 0.9719703, -1.1932578, 0.35026592, -1.2963604, -0.09302414, -2.3137732, -0.8425717, -1.5429214, -0.40176374, -0.4152314, -0.67366415, 0.7979132, -0.8868796, 0.63438666, 1.6292758, 0.13906415, -0.8576702, -1.2493385, -0.7097851, 0.7046427, 0.15559073, 0.93679523, 0.7703309, 0.14081065, 0.47348827, 1.8552462, 1.4156562, -0.30274603, 0.98967946, 0.58585083, 1.1363881, 0.67161655, -0.9741674, -1.6196846, 0.572627, 1.9026182, -0.7756641, -0.18808974, -1.0357478, 1.1778295, -2.305167, -2.2636602, 0.3750199, -0.082343645, -0.47962302, -0.3010948, 0.5369879, -0.413804, -1.096925, -0.9273629, 0.88833886, -0.52474195, -1.3852776, 0.10217833, 0.50499475, 1.3289608, 0.21790339, -0.65971124, 0.47400787, 0.7271749, -0.038905308, -0.04459939, 0.2601329, -0.069856495, 0.2501139, -1.0219133, -1.1504377, -0.83611137, 0.64221096, 0.25879756, 1.040239, -0.18669093, -1.1436414, 1.1445535, -0.018767055, 1.283455, 0.59794647, 2.1886187, -0.21977298, 0.90072393, 0.8913641, -0.55512637, -0.17248231, -1.4617383, -1.5487962, 0.1265688, 0.7930071, 0.63802403, 0.3400246, 0.86301714, -0.5896978, -0.27253276, 0.7375215, 0.43311873, -0.21018882, 1.3207943, -1.2920012, -0.51867867, -0.28339776, 0.8165349, 0.002385198, -1.2614918, 0.5140042, 1.0875463, 0.73930454, 0.61915493, -1.8743135, -0.8998865, 0.4820806, -0.054888185, 0.5225576, -1.2663426, -0.061494764, -1.389781, -1.9536786, 0.29577908, 0.8425888, 0.24561642, -0.03299648, -1.5620143, 1.0061071, -0.044044897, 1.9595621, 0.9423143, -2.0051255, 0.7550497, -1.3965353, -0.7594955, -0.25075668, -0.09406245, 0.39756522, -1.022855, -1.150692, 0.6006052, -0.013250268, 0.17437305, -2.1936834, -0.17713739, -0.8907292, -0.9206264, 0.9219348, -1.0956712, -1.0928966, -0.3310106, 0.45028883, -0.8840147, 1.2341441, 1.4498476, -0.8814471, -0.24508175, -0.7786755, -1.6853821, 0.30301106, 0.7335949, 2.0118642, -0.8974095, 1.336235, 1.3423537, 0.19785331, 0.6021635, 0.8732731, 1.9741, 0.47780856, -0.060137887, -0.8661688, 0.30532077, 1.0241649, 0.24461035, -0.77992326, 0.089076206, -0.12915348, 0.26473877, -1.6618484, 0.55078864, 0.59542316, 0.44485343, -0.0037628172, -1.8059362, -0.019322792, 1.060715, -0.8601289, -1.9892695, -1.540558, 0.3140257, 0.37287602, 0.8862932, -0.055258997, -1.5003284, -0.81850415, 0.8188394, 0.14049591, 0.6498296, 0.4347888, -0.20496055, -0.17400683, 1.8571023, 0.41467425, -0.12858754, 0.45542, 0.22290581, -2.1573563, 0.6500845, 1.8209393, -0.7802799, 1.4540358, -0.2568697, 0.2934714, 1.0703601, -0.72000146, 1.2424939, -1.2142173, -0.87515473, -0.59352034, 0.66200536, -0.3408744, -1.5199745, -0.21653287, -0.7842214, 0.7312936, -0.34323505, 0.07077408, -0.40547246, 0.43393898, -0.18359077, 0.3251987, -2.5933886, 0.09725088, 0.41391367, -0.19928005, 0.66939247, 0.73860705, 1.3042139, 0.10481161, -1.9138007, -2.2854993, -1.601841, -0.03790706, -0.15730529, 0.27623984, -0.6252459, -0.73649114, 0.5550479, 0.65592444, -0.25665015, -0.038476657, 0.40431434, 0.50434357, -1.1439807, -0.71957386, -1.230546, -0.5069066, 0.8123336, 0.54627186, -1.0980979, 0.51226676, 0.08584311, -0.4939267, -1.4064597, -0.17482337, 0.679944, -2.1630976, -0.3961232, 2.2542837, 0.67263675, 0.2598325, -0.7371852, -0.6783298, -0.083288394, 1.6028637, 0.4655892, -0.8721584, 1.176787, -0.2925942, 1.6973464, -0.566603, -1.0032657, 0.17462958, 0.982327, 1.0374448, 0.15919177, -0.9880967, -0.5053407, -2.018282, -0.9131215, -0.17845681, 0.38900214, -0.33945432, -0.056979056, -0.39618546, 0.7510253, -0.89911294, 0.8375479, 1.9608808, 0.47278965, -0.5270916, -0.53627014, 1.2098372, -1.1265894, -0.95380443, -1.1644485, -1.2785138, -1.0448164, 0.78990495, 1.1022825, -0.6970731, 0.20733404, 0.7591567, 0.100564204, -0.95494276, -1.4704018, 1.0104276, 0.4961794, 0.5769559, -1.107647, 0.23497719, 0.6289996, 0.31403384, -0.7450232, 1.0122606, -1.527632, 0.92874193, 1.081056, 1.5723304, -0.3424922, -0.99943, 0.79388034, -0.6992153, 0.04399551, -0.3174622, -0.90207195, 0.32099947, -1.3920159, 0.5922057, -0.9669311, -1.7317313, -0.05010746, 0.43163386, 0.5769346, 0.8183537, -2.3536403, -1.0051445, 0.1066523, 1.5190033, 0.7837445, 1.90134, -0.5249394, 0.27441698, -1.0999708, -0.40435222, -0.7352957, -0.6339887, -0.39344913, 0.00271754, 0.022212664, 0.54345345, 0.13998847, -0.34404564, -0.52257854, -0.3071317, -0.44903713, 0.49097106, 0.8655252, 1.2740445, -0.7977028, 0.4693722, -1.3946797, 0.37317473, 1.0826722, -0.14958951, 1.072636, -1.1385679, -0.8886453, -0.13580984, 1.0222104, -0.41742945, -0.4535531, -0.99162835, 0.20288104, 1.2466952, 0.70068014, 0.6966507, -0.20697448, -0.5633094, 0.6772459, -0.031911075, -0.17360823, 0.8982406, -0.19778745, -0.83777624, 0.9091885, 0.08071989, -1.0370294, -1.1129059, 0.095411874, 2.3374097, -0.3928206, -0.33627385, 1.5237712, -0.0572812, -1.4484669, -1.5727965, 1.226664, 0.66635454, 0.8261257, -0.057756558, -0.72671205, -0.21716312, 0.13603121, -0.83831114, 0.5614499, -1.2595961, -0.33275875, -0.20400788, -0.69101983, -2.2055054, 0.44786966, -0.7557508, 1.3257079, -0.34198228, -0.5413596, 0.09152195, 1.0534397, -0.56340766, 1.0147377, 1.4403037, 0.9903228, 1.6264315, 1.292646, 1.5148823, 1.6043264, 0.20806953, -0.4292239, -2.2622437, -1.3227332, -0.4482828, -0.3817351, -0.15279447, -1.0007604, -1.5957776, -0.13022317, -0.18941793, -0.80755407, -0.74215215, -0.9401566, -0.39652374, -0.8563028, 1.2598753, 0.24099673, -0.97231793, -0.28044778, -1.1802856, 1.0121683, 1.3841867, 1.252002, -1.1446927, -0.09126702, -0.40157068, 0.5620131, -1.0079098, -0.6758917, -0.41321704, 0.15328847, 0.6941287, -0.3287277, 0.66396505, 0.8220764, -0.21321523, -1.2456582, -1.1711904, 0.59172696, -0.47622442, -1.7126293, 0.61295235, 0.12955453, -1.4059671, 1.17942, 0.836636, 0.13874525, -1.2743194, -1.4023305, -0.3070685, -1.7139153, 0.40508026, -1.4108233, 0.16491273, -0.28813145, 0.71178526, -0.9379476, 0.27372944, -1.3948402, 0.7955496, -0.114961766, 0.49585068, -1.3205253, 0.49908426, 0.3062034, 0.3636979, 0.31263396, -0.19346388, 1.2412993, -0.15589799, -0.7391692, -0.05872619, -0.95051795, -0.4639964, -0.17724662, -0.37955412, 0.19939707, 1.9457614, 0.57094985, 1.0723007, -0.50370944, -0.5870163, -0.37817806, 0.8528891, -2.1481185, -1.0331647, 0.10233585, -0.22409236, 1.9677297, 0.44768322, -0.66219145, -1.577607, -0.34056005, -1.30322, 0.46675065, 0.16110632, 0.32003194, 2.0791767, -0.907466, -0.19240421, -1.2125157, -0.08059852, 1.5932736, 0.5687224, -0.114487045, 0.25163025, -1.2108556, -0.3937337, 0.085252576, 0.099421985, -1.5306163, 0.3276232, 0.2791965, -0.3770512, 0.004174999, -1.4834915, -1.4797956, 0.13468726, -0.6677232, -0.01155552, 0.83949065, -0.17392993, -2.810668, -0.15065365, -0.48104402, -0.23469436, 0.8997308, -1.5785302, 0.24395663, 1.5703039, -0.6259431, 0.4723279, 0.9663058, 0.21023144, -0.685097, -0.709521, 0.74380016, 0.5921491, -0.7864684, -1.1764731, -1.2808067, 1.6616518, -0.06794512, 2.3602285, 0.5555456, 0.43952233, 0.30627248, 0.99914986, -0.9660632, 2.1600132, -0.100301705, -0.7034001, 0.302561, 1.0923389, -1.0075549, 0.5668694, -0.71644413, -0.5062735, -0.48948243, 0.76354146, -1.1090727, 0.1926161, -0.34341785, -0.84721017, -1.2135236, -1.2028884, -1.633796, 0.8961672, -0.24165316, 0.15865193, 1.1781894, -1.2201172, -0.94154567, 0.25471553}),\n\t\t\t),\n\t\t\toutputT: tensor.New(tensor.WithShape(1, 3, 32, 32),\n\t\t\t\ttensor.WithBacking([]float64{1.7640524, 0.978738, 2.2408931, 2.2408931, 1.867558, 1.2023798, 0.95008844, -0.10321885, 0.41059852, 0.41059852, 1.9507754, 1.9507754, 0.7610377, 0.44386324, 0.7774904, 1.4940791, 1.4940791, 0.3130677, 0.3869025, 0.3869025, 0.6536186, 0.8644362, 0.8644362, 2.2697546, 2.2697546, 0.3024719, 0.045758516, 1.5327792, 1.5327792, 1.4693588, 0.37816253, 0.37816253, 0.17742614, -0.34791216, 0.46278226, 1.2302907, 1.2302907, 1.2023798, 0.7290906, 1.1394007, 1.1394007, 0.40234163, 1.9507754, 1.9507754, -0.4380743, -0.31155252, 0.7774904, 0.7774904, 0.9008265, 0.9008265, 0.46566245, 1.4882522, 1.8958892, 1.8958892, 1.1787796, 0.42833188, 1.0544517, 1.0544517, 1.222445, 1.222445, 0.97663903, 0.97663903, 0.7065732, 0.7065732, 1.7858706, 1.7858706, 0.46278226, 1.8831507, 1.8831507, 0.7290906, 0.9693967, 1.1394007, 1.9436212, 1.9436212, 0.40234163, 1.922942, 1.922942, 1.867559, 1.867559, 0.90604466, 1.9100649, 1.9100649, 0.8024564, 1.4882522, 1.8958892, 1.8958892, 1.1787796, 0.9222067, 1.0544517, 1.0544517, 1.3263859, 1.3263859, 0.97663903, 0.97663903, 1.8492638, 1.8492638, 1.7858706, 1.7858706, 0.5392492, 1.8831507, 1.8831507, 0.031830557, 0.9693967, 0.9693967, 1.9436212, 1.9436212, 0.3960067, 1.922942, 1.922942, 1.867559, 1.867559, 2.3831449, 2.3831449, 1.9100649, 1.1170163, 1.1170163, 0.947252, 0.61407936, 1.7133427, 1.7133427, 0.37642553, 0.2982382, 1.3263859, 1.3263859, 1.1266359, -0.14963454, 1.8492638, 1.8492638, 1.929532, 1.929532, 0.9494208, 0.5392492, 0.844363, 0.844363, 0.67643327, 1.1880298, 1.1880298, 0.9208588, 0.9208588, 0.8568306, 0.8568306, 0.4393917, 0.6815945, 2.3831449, 2.3831449, 0.94447947, 1.1170163, 1.1170163, -0.35399392, -0.0682416, 1.7133427, 1.7133427, 0.62523144, -0.09845252, 0.05216508, 1.1266359, 1.5430146, 1.5430146, 0.26705086, 0.26705086, 1.929532, 1.929532, 0.9494208, 0.77179056, 0.844363, 2.163236, 2.163236, 1.336528, 1.1880298, 1.0996596, 1.0996596, 0.8568306, 0.8568306, -0.024326125, 0.6815945, 0.6815945, 0.2799246, 0.9101789, 0.9101789, 0.78632796, 0.78632796, -0.4664191, -0.4100497, 0.62523144, 0.62523144, 2.259309, 2.259309, 0.05216508, 1.5430146, 1.5430146, 0.48148146, 0.48148146, 0.06326199, 0.5232767, 0.5232767, 0.77179056, 0.82350415, 2.163236, 2.163236, 1.336528, 0.41605005, 1.0996596, 1.0996596, 1.4944845, 1.4944845, 0.42625874, 0.676908, 0.676908, 0.2799246, 0.9101789, 0.9101789, 0.78632796, 0.78632796, 1.1523316, 1.1523316, 1.0796186, 0.37915173, 2.259309, 2.259309, 0.14195317, 0.14195317, 0.69153875, 0.6947491, 0.6947491, 0.06326199, 0.15650654, 0.6103794, 0.6103794, -0.23792173, -0.23792173, -0.052567296, -0.052567296, 0.41605005, 0.52389103, 0.7811981, 1.4944845, 1.4944845, 0.42625874, 0.676908, 1.9559124, 1.9559124, 0.39009333, -0.13288058, 0.49374178, 0.49374178, 1.1523316, 2.064493, 2.064493, 1.0201727, 1.0201727, 1.5363771, 1.5363771, 0.60884386, 0.69153875, 1.2111453, 1.2111453, 1.3018463, 1.3018463, 0.6103794, 2.3039167, 2.3039167, -0.1359497, 1.1368914, 1.1368914, 0.5829537, 0.5829537, 0.52389103, 0.37005588, 1.6581306, 1.6581306, 0.39904633, 1.9559124, 1.9559124, 0.39009333, -0.39095336, 0.69377315, 0.69377315, -0.11610394, 2.064493, 2.064493, 1.0201727, 1.0201727, 1.5363771, 1.5363771, 0.60884386, 0.60884386, 1.2111453, 1.2111453, 1.3018463, 1.3018463, 0.27451634, 2.3039167, 2.3039167, -0.1359497, 2.2567234, 2.2567234, 0.9432607, 0.9432607, 0.7471883, 0.77325296, 1.6581306, 1.6581306, 0.60631955, 0.6663831, 0.6663831, 0.45093447, 1.6595508, 1.6595508, 1.0685093, -0.13370156, 1.0777438, 1.0777438, -0.28035548, -0.28035548, 0.15670386, 0.5785215, 0.5785215, 0.34965447, -0.0616264, -0.10730527, 1.3645319, 0.27451634, 0.27451634, -0.52118933, -0.31229225, -0.15766701, 2.2567234, 2.2567234, 0.9432607, 0.9432607, 0.93184835, 0.77325296, 0.77325296, 0.16092817, 0.60631955, 0.60631955, 0.45093447, 0.45093447, 1.6595508, 1.6595508, 1.0685093, 0.04949498, 0.4938368, 0.6433145, 0.6433145, -0.20690368, 0.8801789, 0.8801789, 0.5785215, 0.38728046, -0.76414394, -1.0225068, 1.3645319, -0.6522936, -0.52118933, 1.648135, 1.648135, 0.5672903, 0.6203583, 0.6984571, 0.6984571, 0.93184835, 0.93184835, 0.8579239, 1.1411018, 1.4665787, 1.4665787, 0.85255194, -0.26773354, 0.7666632, 0.7666632, 0.8416313, 0.8416313, 0.8145198, 0.8145198, 0.6433145, 0.6433145, -0.20690368, 0.8801789, 0.8801789, 0.38728046, 1.7327212, 1.7327212, 0.6845011, 0.370825, 1.5199949, 1.7195894, 1.7195894, 1.648135, 0.5822246, 0.5672903, 0.12372191, 0.09395323, 0.9430461, 0.9430461, 0.8579239, 1.1411018, 1.4665787, 1.4665787, 0.8689635, 0.8689635, 0.7666632, 0.7666632, 0.8215857, 0.8215857, 0.8145198, 0.8145198, 0.078260176, -0.18505368, -0.085930765, 0.800298, 0.87583274, 0.87583274, 1.7327212, 1.7327212, 0.6845011, 0.370825, 1.5199949, 1.7195894, 1.7195894, 0.9295051, 0.5822246, 2.0210435, 2.0210435, 0.09395323, 0.9430461, 0.9430461, -0.050603542, 0.26990435, 0.26990435, 0.18133843, 0.8689635, 2.4124537, 2.4124537, 0.3148172, 0.8215857, 0.8215857, 0.8005648, 0.8005648, 0.078260176, -0.27567053, 1.7388726, 1.7388726, 1.3191369, 1.3191369, 1.128594, 1.128594, 0.49600095, 1.0294389, 1.0294389, 0.8202478, 0.86259604, 0.86259604, 1.5133281, 2.0210435, 2.0210435, 0.22050765, 0.22050765, 0.1993002, 1.1002843, 1.298022, 2.696224, 2.696224, 0.18133843, 2.4124537, 2.4124537, -0.07785475, 0.38273242, 0.38273242, 1.0963469, 1.0963469, -0.2342158, -0.27567053, 1.7388726, 1.7388726, 1.3191369, 1.3191369, 1.3014281, 1.3749641, 1.3749641, 1.0294389, 1.0294389, 0.17581895, 0.86259604, 1.0479722, 1.5133281, 1.7426687, 1.7426687, 0.22050765, 0.22050765, -0.34994337, 1.1002843, 1.298022, 2.696224, 2.696224, 1.4123276, 0.8640523, 0.8640523, 0.40149906, 1.2248706, 1.2248706, 1.0963469, 1.0963469, -0.2342158, -0.18224478, -0.18224478, -0.10988278, 0.21348006, 1.3014281, 1.3014281, 1.5182612, 1.5182612, -0.38464543, 1.0781974, 1.0781974, 1.1813786, 1.1813786, 1.0479722, 1.7426687, 1.7426687, 0.9424681, -0.26759475, 1.2978458, 1.2978458, 0.24211796, 0.9367425, 1.4123276, 2.0112567, 2.0112567, 0.8640523, 0.40149906, 1.2248706, 1.2248706, 0.3547577, 0.61688656, 0.61688656, 0.5270042, 0.5270042, 0.4537819, 0.21348006, 0.76790243, 0.76790243, 1.5182612, 1.5182612, -0.38464543, 1.0781974, 1.1330799, 1.1813786, 1.1813786, 0.16392857, 0.7849575, 0.9424681, 0.9424681, -0.21694957, 1.2978458, 1.2978458, 0.020334182, 0.5433119, 0.5433119, 2.0112567, 2.0112567, 0.35178012, 0.37923554, 0.37923554, 0.1965574, 0.3547577, 0.61688656, 0.61688656, 0.5270042, 0.5270042, 0.4537819, 0.23810315, 0.76790243, 0.76790243, 0.5898798, -0.36385882, 0.115147874, -0.13105401, 1.1330799, 1.1330799, 0.895556, 0.895556, 0.7849575, 0.7849575, 0.32962298, 1.285984, 1.285984, 0.67646074, 0.67646074, 0.5433119, 0.5433119, 0.43904296, -0.21954103, 0.35178012, 1.670943, 1.670943, -0.0013850428, -0.0013850428, -0.11747455, 0.46616644, 0.46616644, 0.41731882, 0.40326455, 0.40326455, 0.25249663, 0.8203218, 1.3599485, 1.3599485, 1.3675972, 1.3675972, 1.0344099, 0.60512006, 0.895556, 1.0289356, 1.0289356, 0.40476182, 1.5522432, 1.5522432, 1.285984, 0.67646074, 0.67646074, 0.52004063, 0.4497121, 0.4497121, -0.067275606, 0.1833392, 1.670943, 1.670943, -0.0013850428, 0.45248908, 0.45248908, 0.46616644, 0.46616644, -0.023423105, 1.0791948, 1.0791948, 0.37687653, 0.8203218, 1.3599485, 1.3599485, 1.3675972, 1.3675972, 1.0344099, 0.39306292, 1.017021, 1.4229835, 1.4229835, 0.39608657, 1.5522432, 1.5522432, 0.86740744, 0.86740744, 0.52004063, 2.116791, 2.116791, 0.4497121, 2.3807454, 2.3807454, 0.94924647, 0.94924647, -0.9327409, 0.45248908, 1.0907497, 1.0907497, -0.34624946, 0.19796729, 1.0819352, 1.0819352, 0.37687653, 0.37687653, 1.0946383, 1.0946383, 2.1321535, 2.1321535, 0.9364457, 1.2650778, 1.2650778, 1.4229835, 1.4229835, 0.67997485, 1.1244192, 1.3277828, 1.3277828, 0.86740744, -0.46433768, 2.116791, 2.116791, -0.035768073, 2.3807454, 2.3807454, 0.94924647, 0.94924647, -0.8871809, -0.5327028, 1.2433194, 1.2433194, 0.81267405, 0.58725935, 1.0819352, 1.0819352, -0.5075176, 2.4972005, 2.4972005, 1.0946383, 2.1321535, 2.1321535, 0.9364457, 1.2650778, 1.2650778, 1.7549862, 1.7549862, 0.67997485, 0.55578697, 1.3277828, 1.3277828, 0.72003376, 0.30360392, 1.0217906, 1.0217906, 0.44819528, 1.6961815, 1.6961815, 0.82140595, 0.82140595, 0.67057043, 0.039766736, 1.2433194, 1.2433194, 0.81267405, 0.7231005, 0.7231005, 0.71998376, 0.71998376, 2.4972005, 2.4972005, 0.019279385, 1.8495913, -0.10434349, 0.021351224, 0.021351224, 0.19275385, 1.7549862, 1.7549862, -0.058586553, 0.55578697, 0.55578697, 0.72003376, 1.4893559, 1.4893559, 0.7726948, 0.7726948, 0.47689837, 1.6961815, 1.6961815, 0.82140595, 0.82140595, 0.67057043, 0.039766736, 0.039766736, -0.050084095, 1.3124703, 1.3124703, 0.7231005, 0.71998376, 0.71998376, -0.10169727, 0.019279385, 0.019279385, 1.8495913, 0.78580385, 0.021351224, 0.9409176, 0.9409176, 0.49805242, 0.49805242, -0.026192237, -0.058586553, -0.112465985, 0.6450553, 1.4893559, 1.4893559, 0.6119272, 1.735879, 1.735879, 1.6819217, 1.6819217, 0.5290452, 0.4226286, 0.0114989, 0.0114989, -0.050084095, -0.050084095, 1.3124703, 1.3124703, 2.2259443, 2.2259443, 1.370989, 0.3248696, 0.997118, 0.997118, 1.411172, 0.78580385, 0.8672766, 0.9409176, 0.9409176, 0.49805242, 0.49805242, 0.31144708, 0.23958276, 0.9725358, 2.1338682, 2.1338682, 1.0118425, 0.7557403, 1.735879, 1.735879, 1.6819217, 1.6819217, 0.022959756, 0.022959756, 0.0114989, 0.0114989, -0.43674833, -0.43674833, 0.3269626, 0.33003512, 2.2259443, 2.2259443, 1.370989, 0.3248696, 0.997118, 0.997118, 0.22739278, 0.05157494, 0.8672766, 0.8672766, 0.8656529, 1.0813761, 1.0813761, 0.31144708, 0.23958276, 0.9725358, 2.1338682, 2.1338682, 0.4064155, 0.7557403, 0.7557403, 1.3264617, 1.3264617, 0.059894685, 0.059894685, -0.21252304, -0.34796184, 0.93639857, 0.93639857, 0.2711702, 0.2711702, -0.40607178, 0.47224715, 1.154184, 1.154184, 0.17250441, 1.997956, 1.997956, 0.22739278, 2.5944245, 2.5944245, 0.30875126, 0.8656529, 1.0813761, 1.0813761, 0.19031155, 1.8227236, 1.8227236, 0.69938046, 0.9606934, 1.3290628, 1.3290628, 0.05095428, 1.3264617, 1.3264617, 0.059894685, 0.96744615, 0.96744615, -0.055352546, 0.93639857, 0.93639857, 0.2711702, 1.2760754, 1.325014, 1.325014, 0.9304085, 2.339625, 2.339625, 1.997956, 1.997956, 1.471322, 2.5944245, 2.5944245, 0.30833125, 0.30833125, 0.3675449, 0.3675449, 0.19031155, 1.8227236, 1.8227236, -0.5215797, 0.9606934, 1.3290628, 1.3290628, 2.759355, 2.759355, 1.0304383, 0.27560067, 1.4350494, 1.4350494, 0.5072389, 0.3528166, 0.3528166, 1.4013448, 1.4013448, 1.325014, 1.325014, 0.86351967, 2.339625, 2.346647, 2.346647, -0.25957698, 1.471322, 1.5927707, 1.5927707, 0.82998616, 0.30833125, 0.012231983, 1.5692596, 1.5692596, 1.6815767, 1.6815767, 0.9688826, 0.9688826, 1.3891454, 2.0140603, 2.759355, 2.759355, 0.25871643, 0.27560067, 1.4350494, 1.4350494, 0.5072389, -0.1162297, 1.0645851, 1.4013448, 1.4013448, 0.5289436, 0.7243685, 1.3852615, 1.3852615, 2.346647, 2.346647, 0.17879286, 0.9380925, 0.9380925, 0.82998616, 0.82998616, 0.0941923, 0.0941923, 1.5692596, 1.5692596, 0.8924739, 0.8924739, 0.9688826, 0.9688826, 1.3891454, 2.0140603, 2.0140603, -0.04932407, 0.2390336, 0.2390336, 1.6739857, 1.6739857, 1.5634048, 1.5634048, 1.0645851, 1.0645851, 0.43310794, 0.43310794, 0.7243685, 1.3852615, 1.3852615, 0.70104134, 0.44103292, 0.17879286, 0.2407875, 0.2891205, 0.41287082, 0.41287082, 0.0941923, 0.0941923, -0.35811406, 0.5559627, 0.8924739, 0.8924739, 0.10471403, 0.22805333, 0.22805333, 0.5407736, 0.5407736, -0.04932407, 0.2390336, 0.2390336, 1.6739857, 1.6739857, 1.5634048, 1.5634048, -0.790523, 0.22425222, 0.22425222, 0.2149656, 0.2149656, 1.0156653, 1.0156653, 0.70104134, -0.41747734, -1.0974966, 1.7123052, 1.2649833, -0.30078387, 1.1173053, 1.1173053, 0.22246316, 0.22246316, 0.13768983, 0.38141626, 0.7539915, 1.0653155, 1.0653155, 0.9853175, 0.7669197, 0.40262553, 1.6692508, 1.6692508, 0.60815644, 1.1149623, 1.4333525, 1.4333525, 0.43554616, 0.43554616, 0.32830128, 0.03308975, 0.5969065, 0.5969065, -0.15602389, 1.0490932, 3.1709747, 3.1709747, 0.18949963, 1.2649833, 1.2649833, -0.2311016, 0.20984948, 0.20984948, 2.1495745, 2.1495745, 0.73165894, 0.73165894, 0.38141626, 0.6632581, 0.6632581, 0.41540062, 1.5788652, 1.5788652, -0.061638054, -0.061638054, 0.26902407, 0.93874687, 1.2674117, 1.2674117, 0.49949825, 1.2591671, 1.2591671, 0.70411104, 2.5263681, 2.5263681, 1.7699214, 0.3779101, 1.3243587, 1.3243587, -0.1722008, 1.1045785, 1.1045785, -0.2311016, 0.9214084, 0.9214084, 2.1495745, 2.1495745, 0.73165894, 0.73165894, 0.34816924, 0.6632581, 0.82455724, 0.82455724, 1.5788652, 1.5788652, 0.21717963, 0.21717963, 1.4045455, 1.4045455, 1.2674117, 1.2674117, 1.5187594, 1.5187594, 1.2591671, 0.76449746, 2.5263681, 2.5263681, 1.7699214, 1.221385, 1.3243587, 1.3243587, -0.1722008, 1.1045785, 1.1045785, 1.3978963, 1.3978963, 0.9214084, 1.648504, 1.648504, -0.13256802, 1.4261588, 1.4261588, 0.93612915, 0.8326507, 0.8326507, 1.6315974, 1.6315974, 0.37775916, 0.2398671, 1.4045455, 1.4045455, 0.77067304, 0.77067304, 1.8219151, 1.8219151, 0.76449746, 0.76449746, 0.24660219, 0.99213684, 1.9050636, 1.9050636, -0.01477722, -0.033319283, -0.35502872, 0.5310425, 0.5310425, 1.3978963, 1.3978963, 0.9600477, 1.648504, 1.648504, 1.1239053, 1.4261588, 1.4261588, 1.3925184, 1.0375856, 0.8326507, 1.6315974, 1.6315974, 0.5897036, 0.2398671, 1.5848205, 1.5848205, 0.77067304, 0.77067304, 1.8219151, 1.8219151, 1.2961498, 1.2961498, 0.6164593, 0.99213684, 1.9050636, 1.9050636, 0.8805112, 0.08595198, 0.08595198, 0.75194657, 0.5629897, 1.054758, 1.054758, 0.9600477, 1.7746586, 1.7746586, 1.1239053, 0.76943016, 1.3925184, 1.3925184, 1.0375856, 0.30151406, 1.0390965, 1.0390965, 0.5897036, 1.7367749, 1.7367749, 1.5848205, 0.9605572, 0.9605572, 0.8709618, 0.8709618, 2.3207998, 2.3207998, 0.6164593, 0.53659654, 0.43480796, 0.8805112, 0.8805112, 0.732424, 0.08595198, 0.75194657, 0.5629897, 0.2711128, 1.0450233, 1.0450233, 1.7746586, 1.7746586, -0.16221845, 1.151734, 1.151734, 0.33053273, 0.13157398, 0.30151406, 1.0390965, 2.0234718, 2.0234718, 1.7367749, 1.7367749, 2.2436018, 2.2436018, 1.9223248, 1.9223248, 1.4053655, 2.3207998, 2.3207998, 0.53420115, 0.5474806, 0.5474806, 0.54009444, 0.732424, 0.732424, 0.54100823, -0.085115604, 0.966768, 0.966768, 1.0450233, 1.0450233, 0.59903955, 0.5232617, 0.09920486, 1.576299, 1.576299, 0.5023282, 0.16066119, 0.16066119, 1.6085222, 2.0234718, 2.0234718, 0.50538695, 0.35924914, 2.2436018, 2.2436018, 1.9223248, 1.9223248, 1.4053655, 1.6180543, 1.6180543, 0.42258036, 0.5474806, 0.5474806, 0.51283646, 0.6027077, 0.6027077, 0.54100823, -0.085115604, 0.966768, 0.966768, 0.6251187, -0.5216415, 0.5232617, 0.5232617, 0.09920486, 1.576299, 1.576299, 0.5023282, 0.16066119, 0.16066119, 1.6085222, 1.6085222, 1.1452621, 1.1452621, 0.7741606, 0.7741606, 0.10490716, 0.13391292, 0.31943184, 0.31943184, -0.13777922, 1.4961396, 1.4961396, 1.3462211, 1.3462211, 0.51283646, 0.62251914, 0.62251914, -0.5946498, 0.5684589, 0.4804245, 0.6251187, 0.83135104, 0.83135104, 0.48797268, 2.6429358, 2.6429358, 2.290467, 2.290467, 1.6002678, -0.059928004, -0.22007579, 1.3086687, 1.3086687, 1.1452621, 1.5259576, 1.5259576, 0.8820566, 0.8820566, 0.13391292, 0.13391292, 0.3718111, 0.45730963, 1.4961396, 1.4961396, 1.3462211, 1.3462211, 1.5885307, 1.5885307, 0.7747283, 0.7747283, 0.5684589, 0.4804245, 0.4804245, 1.0400863, 1.0400863, 0.88518757, 2.6429358, 2.6429358, 2.290467, 2.290467, 1.6002678, -0.008209882, 1.0429776, 1.0429776, 0.41409135, 0.24676603, 1.5259576, 1.5259576, 0.8820566, 0.8820566, 0.13191175, 0.76877064, 0.76877064, 0.45730963, 0.9623417, 0.9623417, 1.9470986, 1.9470986, 1.5885307, 1.5885307, 1.0212247, 0.7747283, 2.4512298, 0.41133425, 0.0025711823, 1.0400863, 1.0400863, 0.88518757, 1.4737648, 1.6606076, 1.6606076, 1.171041, -0.008209882, -0.008209882, 1.0429776, 1.0983036, 1.0983036, 0.15466884, 1.0415684, 1.0415684, 0.9785673, 0.9785673, 0.13191175, 0.76877064, 0.76877064, -0.4213276, 0.8756957, 0.8756957, 1.9470986, 1.9470986, 0.8091803, 1.0212247, 1.2016978, 1.2016978, 2.4512298, 0.87938994, 0.87938994, 0.63542455, 0.71593887, 0.71593887, 0.8809376, 1.8081318, 1.8081318, 0.43663847, 0.69643867, 0.69643867, 0.65178126, 1.0983036, 1.0983036, 0.025385605, 0.61039174, 0.61039174, 0.9785673, 1.7612661, 1.7612661, 0.7540957, 0.66596717, 0.11255753, 0.50099224, 0.50099224, 0.9361076, 0.9361076, 0.8091803, 0.88649154, 1.2016978, 1.2016978, 0.2847906, 0.87938994, 0.87938994, 0.63542455, 0.71593887, 0.71593887, 0.8809376, 1.8081318, 1.8081318, 0.43663847, 0.69643867, 0.69643867, 0.6878814, 1.3480358, 1.3480358, 2.6805708, 2.6805708, -0.20080851, -0.7401368, 1.7612661, 1.7612661, 0.7540957, 1.3185511, 1.3185511, 0.11255753, 0.067516856, 0.77760416, 0.77760416, 0.33601573, 0.88649154, 0.88649154, -0.27213177, 0.5483281, 0.5483281, 0.19718106, 1.0590272, 1.0590272, 0.5665453, 1.2572197, 1.2572197, 0.49872696, 0.81786186, 0.81786186, 0.6878814, 0.6878814, 1.3480358, 1.3480358, 2.6805708, 2.6805708, -0.16137354, 1.7944489, 1.7944489, 2.2320163, 2.2320163, 1.367415, 1.367415, 0.15364446, 0.15364446, 0.8444543, 0.8444543, 0.28376955, 0.33905926, 0.33905926, -0.58963203, 0.5483281, 1.7406294, 1.7406294, 1.0590272, 1.0590272, 0.87672675, 1.2572197, 1.2572197, 0.94940555, 0.94940555, 0.81786186, 0.23820019, 1.1493794, 1.1493794, 0.29751435, 1.3554426, 1.3554426, 0.58822495, 1.7944489, 1.7944489, 2.2320163, 2.2320163, 1.367415, 1.5877615, 1.5877615, 0.15364446, 0.99675965, 0.99675965, 0.28376955, 0.28376955, 0.079121724, 0.079121724, 0.8505307, 1.7406294, 1.7406294, 0.91722155, 0.91722155, 1.8666831, 1.8666831, 0.75746834, 1.238007, 1.238007, -0.086455286, 0.6234536, 1.1493794, 1.1493794, 0.51291686, 0.64305454, 0.64305454, 0.58822495, 2.5402317, 2.5402317, 0.9265583, 0.55808187, 0.15011522, 1.5877615, 1.5877615, 1.1277837, 1.0329891, 1.0329891, 1.4121517, 1.4121517, 0.079121724, 0.079121724, 0.8505307, -0.14989159, 0.084968135, 0.084968135, 0.5319746, 1.8666831, 1.8666831, 0.75746834, 1.238007, 1.238007, 2.2113044, 2.2113044, 0.8906717, 0.8906717, 0.51291686, 0.22734594, 1.6142496, 1.6142496, 2.5402317, 2.5402317, 0.9265583, 0.55808187, 1.5480669, 1.5480669, 1.1277837, 1.1277837, 1.0329891, 1.0329891, 1.4148741, 1.4148741, 0.15647626, -0.21634398, 0.44284612, 0.21839707, -0.25271067, -0.25271067, 0.6563907, 1.2605689, 1.2605689, 0.16586353, 1.3291413, 1.3291413, 2.2113044, 2.2113044, 1.9367125, 1.9367125, 0.38220277, 0.38220277, 1.6142496, 1.6142496, 1.0085973, 0.52759737, 0.8065638, 0.8065638, 1.5480669, 1.5480669, 1.1483506, 1.4028448, 1.4028448, 0.8246182, 1.4148741, 1.4148741, 0.15647626, -0.21634398, 0.56800735, 0.56800735, 0.08261103, 0.018922104, 0.6563907, 0.6563907, 1.0139722, 1.0139722, 1.3291413, 1.3291413, 1.0097119, 1.0097119, 1.9367125, 2.171165, 2.171165, 0.66207427, 1.1615338, 1.1615338, 2.190898, 2.190898, 0.9836362, 0.8065638, 0.25233144, 1.2098501, 1.2098501, 1.4028448, 1.4587147, 2.1531198, 2.1531198, 0.4683049, 0.6572677, 0.6572677, 0.56800735, 0.56800735, 0.62656426, 0.62656426, 0.018922104, 0.8449956, 1.0139722, 1.0139722, 2.042072, 2.042072, 1.0097119, 1.0097119, 0.72934633, 2.171165, 2.171165, 0.66207427, 0.53916126, 0.53916126, 2.190898, 2.190898, 0.9836362, 0.25233144, 0.60799253, 1.2098501, 1.2098501, 1.5612797, 1.5612797, 2.1531198, 2.1531198, 0.59936935, 0.6572677, 0.6572677, 0.46079433, 0.17124355, 1.4263169, 2.4884417, 2.4884417, 1.6959696, 1.8334354, 1.8334354, 2.042072, 2.042072, 0.46637958, -0.09439251, 0.72934633, 0.8020517, 0.8020517, 0.60413677, 0.60413677, -0.024549423, -0.8358561, 1.966764, 1.966764, 0.24664895, 1.419255, 1.419255, 0.9341797, 1.5612797, 1.5612797, 0.18418126, 0.21301717, 1.279812, 1.279812, -0.2563169, 0.46079433, -0.009657237, 1.4263169, 2.4884417, 2.4884417, 1.6959696, 1.8334354, 1.8334354, 1.801214, 1.801214, 0.46637958, -0.09439251, 0.8962484, 0.8962484, 0.88686466, 1.103694, 1.103694, 0.4005307, 0.13545467, 1.966764, 1.966764, 1.8593464, 1.419255, 1.419255, 0.9341797, 0.86900634, 1.1149246, 1.1149246, 0.024290914, 1.279812, 1.279812, 0.60896236, 0.40088567, 0.0032222164, -0.55598503, 0.19321355, 1.2973421, 1.2973421, 1.001331, 0.8179776, 1.801214, 1.801214, 1.4622141, 1.4622141, 0.8962484, 0.8962484, 0.88686466, 1.103694, 1.103694, 0.77905124, 0.13545467, 0.9687444, 1.8593464, 1.8593464, -0.13482246, 2.6205738, 2.6205738, 0.33510563, 1.1149246, 1.1149246, 0.013748487, 0.8253722, 0.8253722, 0.60896236, 0.0032222164, 0.021857359, 0.042539075, 1.5309323, 1.5309323, 0.18345025, 0.2727964, 0.8179776, 0.8179776, 1.4315678, 1.4622141, 1.4622141, 0.5383296, -0.14244542, -0.12656933, 0.4936441, 0.77905124, 0.77905124, 0.99012136, 0.99012136, 0.9687444, -0.021869909, -0.27120733, 2.6205738, 2.6205738, 1.7025702, 0.06763423, 0.44702417, 0.44702417, 0.8253722, 0.8253722, -0.29832315, 1.4219756, 0.45636335, 0.042539075, 1.5309323, 1.5309323, 0.092447735, -0.099008314, 0.777277, 0.777277, 0.5541276, 0.5541276, 0.5383296, 0.5383296, 0.71966815, 0.71966815, 0.4936441, 0.4936441, 0.48701534, 0.99012136, 0.99012136, -0.015779218, 0.9541896, 0.9541896, 0.54414475, 1.7025702, 1.7025702, 1.7148316, 1.7148316, 0.44702417, 0.17233914, 1.3985621, 1.3985621, 1.4219756, 0.7825789, 1.7975394, 1.7975394, 0.54565734, 0.54565734, 0.1508182, 1.6857923, 1.6857923, 0.5541276, 0.91064566, 0.91064566, -0.029856121, 0.71966815, 0.71966815, 0.038127385, 0.51271755, 0.51271755, 0.27566653, -0.015779218, -0.015779218, 1.2577442, 1.2577442, 0.54414475, 0.4472121, 1.4558079, 1.7148316, 1.7148316, 1.4849256, 0.4595849, 1.3985621, 1.3985621, 1.3905339, 0.7825789, 1.7975394, 1.7975394, 1.2165778, 1.2165778, 2.0347626, 2.0347626, 1.6857923, 0.9508337, 0.91064566, 0.91064566, 0.57275134, 0.67755705, 1.3071839, 1.3071839, 0.51271755, 0.8591042, 0.8591042, 0.8733118, 1.1997361, 1.2577442, 1.2577442, 0.041082226, 0.5934659, 1.4558079, 2.1982963, 2.1982963, 1.4849256, 0.4595849, 0.4595849, 0.28005785, 1.3905339, -0.15503581, 0.078514025, 0.41780058, 1.2165778, 1.2165778, 2.0347626, 2.0347626, 1.5743712, 1.5743712, 1.9887319, 1.9887319, 1.1172835, 0.67755705, 1.3071839, 1.3071839, 0.030546581, 1.2697648, 1.2697648, 0.8733118, 1.1997361, 1.1997361, 0.45615304, 0.7750233, 0.7750233, 0.5934659, 2.1982963, 2.1982963, 0.0012484558, 0.0012484558, -0.15993978, -0.58957994, -0.59815043, 0.4178537, 0.4178537, 0.41780058, 0.41780058, 1.342622, 1.8022423, 1.8022423, 1.5743712, 1.5743712, 1.9887319, 1.9887319, 1.4015235, 0.15843384, 1.054325, 1.054325, 0.030546581, 1.2697648, 1.2697648, 0.017515613, 0.32362577, 0.32362577, -0.020129103, 0.7750233, 0.92914516, 0.92914516, 0.37117255, 0.6436313, 0.68889666, 0.68889666, 0.2746472, -0.6036204, 0.70885956, 0.42281857, 0.64445204, 0.64445204, 0.6635616, 1.342622, 1.342622, 1.3151376, 1.3151376, 0.19782817, 1.0598063, 1.4015235, 1.4015235, 0.7435535, 0.7435535, 0.21035936, 1.36606, 1.555114, 1.555114, 0.61332625, 1.496911, 1.496911, 1.1831195, 0.71889716, 0.92914516, 0.92914516, 0.37117255, 0.6436313, 0.68889666, 0.68889666, 1.2729537, 1.2729537, 0.70885956, 0.42281857, 0.64445204, 0.64445204, 0.6635616, 0.6635616, 1.1936116, 1.1936116, -0.09816121, -0.14735366, 1.0598063, 1.0598063, 0.026246618, 0.7435535, 0.7435535, 0.21035936, 1.36606, 1.555114, 1.555114, 0.61332625, 1.496911, 1.496911, 1.1831195, 0.71889716, 0.14067191, 0.14067191, -0.15901226, 0.24005693, 0.24005693, 0.10015941, 1.2729537, 1.2729537, 0.73018354, 0.73018354, 0.38259813, 0.38259813, 0.87830377, 0.87830377, 1.2815113, 1.9142298, 1.9142298, 1.6266496, 1.6266496, 2.9290962, 2.9290962, 1.0805441, 1.0805441, 0.45616361, 0.45616361, 0.5707885, 1.5427964, 1.5427964, 1.3821819, 1.4867824, 1.4867824, 1.5116394, 1.7064682, 1.7064682, 0.70178336, 0.48334184, 0.50496995, 1.7236962, 1.7236962, 1.414415, 0.3257996, 0.124769524, -1.0126731, 0.32335654, 1.400689, 1.400689, 1.2815113, 1.9142298, 1.9142298, 1.6266496, 1.6266496, -0.0150050875, 0.41609964, 1.0805441, 1.0805441, 0.45616361, 0.66414404, 0.66414404, 1.5427964, 1.8961822, 1.8961822, 0.40955177, 0.40955177, 1.5116394, 1.7064682, 1.7064682, 0.70178336, 0.07328543, 0.5298338, 1.7108365, 1.7108365, 1.414415, -0.063661486, -0.11244375, 0.89786416, 0.89786416, 1.400689, 1.400689, 0.50481546, 1.3031809, 1.3031809, 1.0659268, 0.063896194, -0.9659539, 0.41609964, 0.41609964, 0.12253132, 0.12253132, 0.66414404, 0.66414404, 1.1279566, 1.8961822, 1.8961822, -0.28157544, -0.28157544, 0.44478136, 1.0223923, 1.0223923, 2.2856011, 2.2856011, 0.5298338, 0.8097407, 1.0799313, 1.0799313, 1.094167, -0.11244375, 1.131812, 1.131812, 0.29507577, -0.49941677, 0.1746801, 0.1746801, 1.2505698, 1.2505698, 0.063896194, -0.8633556, 0.14166717, 0.14166717, 1.3440744, 1.3440744, 0.5000167, -0.6289807, 1.1279566, 1.1279566, 1.7345722, 1.7345722, 0.57261336, 0.57261336, 0.87550914, 1.0985817, 2.2856011, 2.2856011, 0.81345224, 0.8097407, 1.0799313, 1.0799313, 1.094167, 0.42425263, 1.6295, 1.6295, -0.3000036, -0.3000036, -0.9325702, 1.7969185, 1.7969185, 1.2505698, -0.19871506, 1.4198639, 1.4198639, 0.6795679, 1.3440744, 1.3440744, 0.5670582, 0.9882406, 1.0700725, 1.0700725, 1.7345722, 1.7345722, 1.1035038, 0.57261336, 1.4099522, 1.4099522, 1.0985817, 0.81345224, 0.81345224, 0.75386566, 0.75386566, 0.36559147, 0.5880377, 1.55701, 1.6295, 1.6295, -0.3000036, -0.18779492, 0.4869373, 1.7969185, 1.7969185, 0.5953003, 1.1641518, 1.4198639, 1.4198639, 0.7289299, 0.6795679, -0.32125893, 0.5670582, 0.9882406, 1.0137506, 1.0137506, 0.50666904, 1.1035038, 1.1035038, 0.5569641, 1.4099522, 1.4099522, 0.77946055, 0.77946055, 0.28791544, 0.28791544, 0.36559147, 0.36559147, 0.924434, 1.55701, 1.55701, 1.0121332, 1.6184593, 1.6184593, 0.57110983, 0.4869373, 0.20579681, 0.5953003, 1.1641518, 1.1641518, 0.7289299, 0.7289299, -0.034475047, 0.072677895, 0.072677895, 0.3327982, 1.0137506, 1.0137506, 0.50666904, 2.4643219, 2.4643219, 1.1320051, 1.1320051, 0.6530373, -0.18844908, 0.2872573, 0.2872573, 1.3363862, 1.3363862, -0.0842233, 0.924434, 0.924434, 1.0121332, 1.7140515, 1.7140515, 1.6184593, 0.7669904, -0.45962644, 0.20579681, 0.3263751, 1.4831287, 1.4831287, 0.23908707, 0.650042, 0.650042, 0.23868157, 0.23868157, -0.11622244, 0.85464275, 0.85464275, 0.92660475, 2.4643219, 2.4643219, 1.1320051, 1.1320051, 0.409295, 0.39710623, 0.49964696, 0.49964696, 1.3363862, 1.3363862, -0.120041125, 0.46146888, -0.046481155, 0.037996013, 1.7140515, 1.7140515, 0.7669904, 0.86214805, 0.86214805, 0.4883706, 0.9659361, 1.4831287, 1.9612269, 1.9612269, 0.650042, 0.650042, 1.0616002, 1.0616002, -0.11622244, 0.830386, 0.830386, 0.92660475, 1.8004627, 1.8004627, 0.51650745, 0.51650745, 0.409295, 0.39710623, 0.49964696, 0.49964696, 0.24622276, 0.24622276, 0.09082593, 1.3970653, 1.497715, 1.497715, 0.5652672, 0.54985684, 0.40713033, 0.86214805, 0.86214805, 0.8989674, 0.9659361, 1.4226048, 1.9612269, 1.9612269, 1.3752254, 1.4994915, 1.4994915, 1.0616002, -0.86050975, 0.830386, 0.830386, 0.8576619, 0.8576619, 0.40807465, 1.0770375, 1.0770375, 0.016491488, 0.5746089, 0.5746089, 0.65880215, 0.65880215, 0.16987796, 0.09082593, 1.3970653, 1.497715, 1.497715, 0.81547195, 0.48096985, 0.48096985, 0.85282975, 0.85282975, 1.0044192, 1.0044192, 1.3141544, 1.3141544, 1.3752254, 1.3752254, 1.9258057, 1.951761, 1.951761, 1.2399405, 0.93858516, 0.5125622, 0.8576619, 0.8576619, -0.1105905, 1.0770375, 1.0770375, -0.22482656, 0.7920415, 0.7920415, 0.65880215, 0.65880215, 0.1118724, -0.22295918, 0.3105647, 0.3105647, 0.81547195, 0.81547195, 0.48096985, 0.69865024, 0.85282975, 0.85282975, 1.0044192, 1.0044192, 0.7315516, 0.6335339, 0.6335339, 0.8927082, 1.9258057, 1.951761, 1.951761, 1.9093426, 1.9093426, 1.6638731, 0.90082276, -0.35911658, -0.50900584, 1.6531498, 1.6531498, 0.38616636, 0.7920415, 0.7920415, 0.38182402, 0.3982961, 0.3982961, -0.67570317, 1.2228775, 0.3105647, 0.9719703, 0.9719703, 0.35026592, 0.69865024, 0.69865024, 0.5216252, 0.49689314, 0.7315516, 0.7315516, 0.3196498, -0.40985453, 0.8927082, 0.8927082, 0.63438666, 1.6292758, 1.9093426, 1.9093426, 1.6638731, 0.90082276, 0.7046427, 0.7046427, 1.6531498, 1.6531498, 0.7703309, 0.47348827, 1.8552462, 1.8552462, 1.4156562, 0.98967946, 0.98967946, 1.2228775, 1.1363881, 0.9719703, 0.9719703, 0.572627, 1.9026182, 1.9026182, -0.09302414, -0.18808974, 1.1778295, 1.1778295, -0.40176374, 0.3750199, 0.7979132, 0.7979132, 0.63438666, 1.6292758, 1.6292758, 0.13906415, -0.8576702, 0.88833886, 0.88833886, 0.7046427, 0.93679523, 0.93679523, 1.3289608, 1.3289608, 1.8552462, 1.8552462, 1.4156562, 0.98967946, 0.98967946, 1.1363881, 1.1363881, 0.67161655, 0.2501139, 0.572627, 1.9026182, 1.9026182, 0.64221096, 1.040239, 1.1778295, 1.1778295, 1.1445535, 1.1445535, 1.283455, 1.283455, 2.1886187, 2.1886187, 0.90072393, 0.90072393, 0.8913641, 0.88833886, 0.88833886, -0.52474195, 0.1265688, 0.7930071, 1.3289608, 1.3289608, 0.86301714, 0.86301714, 0.7271749, 0.7375215, 0.7375215, 0.43311873, 1.3207943, 1.3207943, 0.2501139, -0.28339776, 0.8165349, 0.8165349, 0.64221096, 1.040239, 1.0875463, 1.0875463, 1.1445535, 1.1445535, 1.283455, 1.283455, 2.1886187, 2.1886187, 0.90072393, 0.90072393, 0.8913641, -0.17248231, 0.29577908, 0.8425888, 0.8425888, 0.7930071, 0.7930071, 1.0061071, 1.0061071, 1.9595621, 1.9595621, 0.9423143, 0.7375215, 0.7550497, 1.3207943, 1.3207943, -0.09406245, 0.39756522, 0.8165349, 0.8165349, 0.6006052, 0.6006052, 1.0875463, 1.0875463, 0.73930454, 0.61915493, -0.8907292, 0.9219348, 0.9219348, 0.5225576, 0.5225576, 0.45028883, 0.45028883, 1.2341441, 1.4498476, 1.4498476, 0.8425888, 0.24561642, -0.03299648, 1.0061071, 1.0061071, 2.0118642, 2.0118642, 1.336235, 1.336235, 1.3423537, 0.6021635, 0.8732731, 1.9741, 1.9741, 0.47780856, -0.060137887, 0.6006052, 1.0241649, 1.0241649, 0.24461035, 0.089076206, 0.089076206, 0.26473877, 0.9219348, 0.9219348, 0.59542316, 0.59542316, 0.45028883, 0.45028883, 1.2341441, 1.4498476, 1.4498476, -0.24508175, -0.24508175, 0.3140257, 0.37287602, 0.8862932, 2.0118642, 2.0118642, 1.336235, 1.336235, 1.3423537, 0.6498296, 0.8732731, 1.9741, 1.9741, 1.8571023, 1.8571023, 0.41467425, 1.0241649, 1.0241649, 0.24461035, 0.6500845, 1.8209393, 1.8209393, 1.4540358, 1.4540358, 0.59542316, 1.0703601, 1.0703601, 1.2424939, 1.2424939, 1.060715, 1.060715, 0.66200536, 0.66200536, 0.3140257, 0.37287602, 0.8862932, 0.8862932, 0.7312936, 0.07077408, 0.07077408, 0.8188394, 0.6498296, 0.6498296, 0.4347888, 0.09725088, 1.8571023, 1.8571023, 0.66939247, 0.73860705, 1.3042139, 1.3042139, 0.6500845, 1.8209393, 1.8209393, 1.4540358, 1.4540358, 0.2934714, 1.0703601, 1.0703601, 1.2424939, 1.2424939, 0.65592444, -0.038476657, 0.66200536, 0.66200536, 0.50434357, -0.21653287, -0.21653287, 0.7312936, 0.8123336, 0.8123336, 0.54627186, 0.51226676, 0.51226676, 0.3251987, 0.3251987, 0.09725088, 0.679944, 0.679944, 0.66939247, 2.2542837, 2.2542837, 1.3042139, 0.2598325, -0.6783298, -0.083288394, 1.6028637, 1.6028637, 0.4655892, 1.176787, 1.176787, 1.6973464, 1.6973464, 0.65592444, 0.17462958, 0.982327, 1.0374448, 1.0374448, 0.15919177, -0.5053407, -0.5053407, 0.8123336, 0.8123336, 0.54627186, 0.51226676, 0.51226676, 0.08584311, 0.7510253, 0.7510253, 0.8375479, 1.9608808, 1.9608808, 2.2542837, 2.2542837, 1.2098372, 1.2098372, -0.6783298, -0.083288394, 1.6028637, 1.6028637, 0.78990495, 1.176787, 1.176787, 1.6973464, 1.6973464, 0.7591567, 0.17462958, 0.982327, 1.0374448, 1.0374448, 0.5769559, 0.5769559, 0.23497719, 0.6289996, 0.6289996, 0.31403384, 1.0122606, 1.0122606, 0.92874193, 1.081056, 1.5723304, 1.5723304, 1.9608808, 1.9608808, 0.79388034, 0.04399551, 1.2098372, 1.2098372, 0.32099947, 0.32099947, 0.5922057, 0.5922057, 0.78990495, 1.1022825, 1.1022825, 0.5769346, 0.8183537, 0.8183537, 0.100564204, 0.1066523, 1.5190033, 1.5190033, 1.90134, 1.90134, 0.27441698, 0.6289996, 0.6289996, 0.31403384, 1.0122606, 1.0122606, 0.92874193, 1.081056, 1.5723304, 1.5723304, 0.13998847, 0.79388034, 0.79388034, 0.04399551, 0.49097106, 0.8655252, 1.2740445, 1.2740445, 0.5922057, 0.5922057, 0.37317473, 1.0826722, 1.0826722, 1.072636, 1.072636, 0.8183537, -0.13580984, 1.0222104, 1.5190033, 1.5190033, 1.90134, 1.90134, 1.2466952, 1.2466952, 0.70068014, 0.6966507, -0.20697448, 0.6772459, 0.6772459, 0.022212664, 0.8982406, 0.8982406, 0.13998847, 0.9091885, 0.9091885, 0.08071989, 0.49097106, 0.8655252, 2.3374097, 2.3374097, 0.4693722, 1.5237712, 1.5237712, 1.0826722, 1.0826722, 1.226664, 1.226664, 0.8261257, 0.8261257, 1.0222104, 1.0222104, 0.13603121, 0.13603121, 0.5614499, 1.2466952, 1.2466952, 0.70068014, 0.6966507, -0.20697448, 0.6772459, 0.6772459, 1.3257079, 1.3257079, 0.8982406, 0.09152195, 1.0534397, 1.0534397, 1.0147377, 1.4403037, 1.4403037, 2.3374097, 2.3374097, 1.5148823, 1.6043264, 1.6043264, 0.20806953, -0.4292239, 1.226664, 1.226664, 0.8261257, 0.8261257, -0.057756558, -0.21716312, 0.13603121, 0.13603121, 0.5614499, 0.5614499, -0.33275875, -0.20400788, -0.20400788, 1.2598753, 1.2598753, 0.44786966, 1.3257079, 1.3257079, 1.0121683, 1.3841867, 1.3841867, 1.252002, 1.0147377, 1.4403037, 1.4403037, 1.6264315, 1.6264315, 1.5148823, 1.6043264, 1.6043264, 0.6941287, 0.66396505, 0.8220764, 0.8220764, -0.21321523, -0.15279447, 0.59172696, 0.59172696, -0.13022317, 0.61295235, 0.61295235, 0.12955453, 1.17942, 1.17942, 0.836636, 1.2598753, 1.2598753, 0.24099673, -0.28044778, 0.40508026, 1.0121683, 1.3841867, 1.3841867, 1.252002, 0.71178526, 0.27372944, 0.5620131, 0.7955496, 0.7955496, 0.49585068, 0.49585068, 0.6941287, 0.6941287, 0.66396505, 0.8220764, 0.8220764, 1.2412993, 1.2412993, 0.59172696, 0.59172696, -0.05872619, 0.61295235, 0.61295235, 0.12955453, 1.17942, 1.9457614, 1.9457614, 1.0723007, 1.0723007, -0.3070685, -0.3070685, 0.8528891, 0.8528891, 0.16491273, 0.16491273, 0.71178526, 1.9677297, 1.9677297, 0.44768322, 0.7955496, 0.7955496, 0.49585068, 0.49585068, 0.49908426, 0.49908426, 2.0791767, 2.0791767, 0.31263396, 1.2412993, 1.2412993, 1.5932736, 1.5932736, 0.5687224, 0.25163025, 0.25163025, -0.17724662, 0.19939707, 1.9457614, 1.9457614, 1.0723007, 1.0723007, 0.2791965, 0.004174999, 0.8528891, 0.8528891, 0.13468726, 0.13468726, 0.10233585, 1.9677297, 1.9677297, 0.44768322, -0.15065365, -0.15065365, -0.23469436, 0.8997308, 0.8997308, 0.32003194, 2.0791767, 2.0791767, 0.4723279, 0.9663058, 0.9663058, 1.5932736, 1.5932736, 0.74380016, 0.74380016, 0.5921491, -0.3937337, 0.085252576, 1.6616518, 1.6616518, 2.3602285, 2.3602285, 0.5555456, 0.43952233, 0.99914986, 0.99914986, 2.1600132, 2.1600132, -0.01155552, 0.83949065, 1.0923389, 1.0923389, 0.5668694, 0.5668694, -0.23469436, 0.8997308, 0.8997308, 0.76354146, 1.5703039, 1.5703039, 0.4723279, 0.9663058, 0.9663058, 0.21023144, 0.8961672, 0.8961672, 0.74380016, 1.1781894, 1.1781894, -0.94154567, 1.6616518, 1.6616518, 2.3602285, 2.3602285, 0.5555456, 0.43952233, 0.99914986, 0.99914986, 2.1600132, 2.1600132, -0.100301705, 0.302561, 1.0923389, 1.0923389, 0.5668694, 0.5668694, -0.5062735, -0.48948243, 0.76354146, 0.76354146, 0.1926161, 0.1926161, -0.34341785, -0.84721017, -1.2028884, -1.2028884, 0.8961672, 0.8961672, 0.15865193, 1.1781894, 1.1781894, -0.94154567, 0.25471553, 0.25471553}),\n\t\t\t),\n\t\t},\n\t}\n\n\tfor _, tst := range tsts {\n\t\tg := NewGraph()\n\t\tx := NodeFromAny(g, tst.inputT)\n\t\ty, err := MaxPool2D(x, tensor.Shape{2, 2}, []int{1, 0, 1, 0}, []int{1, 1})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tm := NewTapeMachine(g, BindDualValues())\n\t\tif err := m.RunAll(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tm.Close()\n\t\tif len(y.Shape()) != len(tst.outputT.Shape()) {\n\t\t\tt.Fatalf(\"Maxpool: expected shape %v, got %v\", tst.outputT.Shape(), y.Shape())\n\t\t}\n\t\tfor i, v := range y.Shape() {\n\t\t\tif v != tst.outputT.Shape()[i] {\n\t\t\t\tt.Fatalf(\"Maxpool: expected shape %v, got %v\", tst.outputT.Shape(), y.Shape())\n\t\t\t}\n\t\t}\n\t\tassert.Equal(y.Value().Data(), tst.outputT.Data())\n\t}\n}\n\nfunc TestMaxPool2D(t *testing.T) {\n\tassert := assert.New(t)\n\tdts := []tensor.Dtype{tensor.Float64, tensor.Float32}\n\tfor _, dt := range dts {\n\t\tg := NewGraph()\n\t\tx := NewTensor(g, dt, 4, WithShape(1, 2, 3, 4), WithInit(RangedFrom(0)))\n\t\ty, err := MaxPool2D(x, tensor.Shape{2, 2}, []int{0, 0}, []int{1, 1})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tcost := Must(Sum(y))\n\t\tgrads, err := Grad(cost, x)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tm := NewTapeMachine(g, BindDualValues())\n\t\tif err := m.RunAll(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\t// t.Logf(\"x %v\", x.Value())\n\t\t// t.Logf(\"y: %v\", y.Value())\n\t\t// t.Logf(\"c: %v\", cost.Value())\n\t\t// t.Logf(\"xG: %v\", grads[0])\n\n\t\th := NewGraph()\n\t\ta := NewTensor(h, dt, 4, WithShape(1, 2, 3, 4), WithInit(RangedFrom(0)))\n\t\tb, err := MaxPool2D(a, tensor.Shape{2, 2}, []int{0, 0}, []int{1, 1})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tcost2 := Must(Sum(b))\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tm2 := NewLispMachine(h)\n\t\tif err = m2.RunAll(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\taG, err := a.Grad()\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t\treturn\n\t\t}\n\n\t\tassert.Equal(x.Value().Data(), a.Value().Data())\n\t\tassert.Equal(grads[0].Value().Data(), aG.Data())\n\t\tassert.Equal(cost.Value().Data(), cost2.Value().Data())\n\n\t\tm.Close()\n\t\tm2.Close()\n\t}\n}\n\nfunc TestMaxPool(t *testing.T) {\n\ttestCases := []struct {\n\t\tdesc           string\n\t\tinput          tensor.Tensor\n\t\tkernelSize     tensor.Shape\n\t\tpad            []int\n\t\tstride         []int\n\t\texpectedOutput []float64\n\t\texpectedShape  tensor.Shape\n\t\texpectedCost   float64\n\t\tPoolFunc       func(*Node, tensor.Shape, []int, []int) (*Node, error)\n\t}{\n\t\t{\n\t\t\tdesc: \"Example 1\",\n\t\t\tinput: tensor.New(\n\t\t\t\ttensor.WithShape(1, 1, 4, 4),\n\t\t\t\ttensor.WithBacking(tensor.Range(tensor.Float64, 0, 16)),\n\t\t\t),\n\t\t\tkernelSize:     []int{4, 4},\n\t\t\tpad:            []int{0, 0},\n\t\t\tstride:         []int{1, 1},\n\t\t\texpectedOutput: []float64{15},\n\t\t\texpectedCost:   196,\n\t\t\texpectedShape:  tensor.Shape{1, 1, 1, 1},\n\t\t\tPoolFunc:       MaxPool2D,\n\t\t},\n\t}\n\n\tfor _, tcase := range testCases {\n\t\tt.Run(tcase.desc, func(t *testing.T) {\n\t\t\tc := require.New(t)\n\n\t\t\tg := NewGraph()\n\n\t\t\tinput := NewTensor(g, tensor.Float64, tcase.input.Shape().Dims(), WithName(\"input\"), WithShape(tcase.input.Shape()...), WithValue(tcase.input))\n\n\t\t\toutput, err := tcase.PoolFunc(input, tcase.kernelSize, tcase.pad, tcase.stride)\n\t\t\tc.NoError(err)\n\n\t\t\tt.Logf(\"%v output shape: %v\", tcase.desc, output.Shape())\n\t\t\tt.Logf(\"%v input shape: %v\", tcase.desc, input.Shape())\n\n\t\t\ty := NewTensor(g, output.Dtype(), output.Dims(), WithShape(output.Shape()...), WithInit(Ones()))\n\n\t\t\tcost := Must(Mean(Must(Square(Must(Sub(output, y)))))) // MSE\n\n\t\t\t_, err = Grad(cost, input)\n\t\t\tc.NoError(err)\n\n\t\t\t// logger := log.New(os.Stdout, \"\", 0)\n\n\t\t\tvm := NewTapeMachine(\n\t\t\t\tg,\n\t\t\t\t//WithLogger(logger),\n\t\t\t\tWithWatchlist(),\n\t\t\t\tBindDualValues(output),\n\t\t\t\tTraceExec(),\n\t\t\t)\n\n\t\t\tc.NoError(vm.RunAll())\n\t\t\tc.NoError(vm.Close())\n\n\t\t\tt.Logf(\"%v input %v\", tcase.desc, input.Value())\n\t\t\tt.Logf(\"%v result: %v\", tcase.desc, output.Value())\n\t\t\tt.Logf(\"%v cost: %v\", tcase.desc, cost.Value())\n\n\t\t\tc.Equal(tcase.expectedOutput, output.Value().Data())\n\t\t\tc.Equal(tcase.expectedShape, output.Shape())\n\t\t\tc.Equal(tcase.expectedCost, cost.Value().Data().(float64))\n\t\t})\n\t}\n}\n\nvar (\n\tbnSetStatsCases = []struct {\n\t\tDtype       tensor.Dtype\n\t\tInit        InitWFn\n\t\tShape       tensor.Shape\n\t\texpectedErr string\n\t}{\n\t\t{Float32, RangedFromWithStep(0.0, 0.1), tensor.Shape{2, 2}, \"invalid runningMean shape (2, 2). Expected: (2)\"},\n\t\t{Float64, RangedFromWithStep(0.0, 0.1), tensor.Shape{2}, \"invalid runningMean type float64. Expected: float32\"},\n\t\t{Float32, RangedFromWithStep(0.0, 0.1), tensor.Shape{2}, \"\"},\n\t}\n)\n\nfunc TestBatchNormSetStats(t *testing.T) {\n\tg := NewGraph()\n\tx := NewTensor(g, Float32, 2, WithShape(2, 2), WithInit(Zeroes()), WithName(\"x\"))\n\n\t_, _, _, op, _ := BatchNorm(x, nil, nil, 0.9, 1e-5)\n\n\tfor i, tC := range bnSetStatsCases {\n\t\tt.Run(fmt.Sprintf(\"Example %d (%v)\", i+1, tC.Dtype), func(t *testing.T) {\n\t\t\tc := assert.New(t)\n\n\t\t\tbacking := tC.Init(tC.Dtype, tC.Shape...)\n\n\t\t\terr := op.SetStats(\n\t\t\t\ttensor.New(\n\t\t\t\t\ttensor.Of(tC.Dtype),\n\t\t\t\t\ttensor.WithShape(tC.Shape...),\n\t\t\t\t\ttensor.WithBacking(backing),\n\t\t\t\t),\n\t\t\t\ttensor.New(\n\t\t\t\t\ttensor.Of(tC.Dtype),\n\t\t\t\t\ttensor.WithShape(tC.Shape...),\n\t\t\t\t\ttensor.WithBacking(backing),\n\t\t\t\t),\n\t\t\t)\n\n\t\t\tif tC.expectedErr == \"\" {\n\t\t\t\tc.NoError(err)\n\n\t\t\t\tm, v := op.Stats()\n\t\t\t\tc.EqualValues(backing, m.Data())\n\t\t\t\tc.EqualValues(backing, v.Data())\n\t\t\t} else {\n\t\t\t\tc.EqualError(err, tC.expectedErr)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestBatchNormAll(t *testing.T) {\n\tfor i, tC := range bnAllCases {\n\t\tt.Run(fmt.Sprintf(\"#%d %v\", i+1, tC.desc), func(t *testing.T) {\n\t\t\trand.Seed(0)\n\n\t\t\tc := require.New(t)\n\n\t\t\tg := NewGraph()\n\n\t\t\tvar initOpt NodeConsOpt\n\n\t\t\tswitch v := tC.X.(type) {\n\t\t\tcase []float32:\n\t\t\t\tinitOpt = WithValue(\n\t\t\t\t\ttensor.New(\n\t\t\t\t\t\ttensor.Of(tensor.Float32),\n\t\t\t\t\t\ttensor.WithShape(tC.XShape...),\n\t\t\t\t\t\ttensor.WithBacking(v),\n\t\t\t\t\t),\n\t\t\t\t)\n\t\t\tcase []float64:\n\t\t\t\tinitOpt = WithValue(\n\t\t\t\t\ttensor.New(\n\t\t\t\t\t\ttensor.Of(tensor.Float32),\n\t\t\t\t\t\ttensor.WithShape(tC.XShape...),\n\t\t\t\t\t\ttensor.WithBacking(v),\n\t\t\t\t\t),\n\t\t\t\t)\n\t\t\tcase InitWFn:\n\t\t\t\tinitOpt = WithInit(v)\n\t\t\t}\n\n\t\t\tx := NewTensor(g, tC.Dtype, tC.XShape.Dims(), WithShape(tC.XShape...), initOpt, WithName(\"x\"))\n\n\t\t\tscale := NewTensor(g, tC.Dtype, tC.ScaleShape.Dims(), WithShape(tC.ScaleShape...), WithInit(tC.ScaleInit), WithName(\"scale\"))\n\t\t\tbias := NewTensor(g, tC.Dtype, tC.BiasShape.Dims(), WithShape(tC.BiasShape...), WithInit(tC.BiasInit), WithName(\"bias\"))\n\n\t\t\tfcWeight := NewTensor(g, tC.Dtype, 2, WithShape(tC.XShape[0], tensor.Shape(tC.XShape[1:]).TotalSize()), WithInit(tC.ScaleInit), WithName(\"fcWeight\"))\n\n\t\t\ty, _, _, op, err := BatchNorm(x, scale, bias, 0.9, 1e-5)\n\t\t\tc.NoError(err)\n\n\t\t\tif y.Dims() > 2 {\n\t\t\t\ty = Must(Reshape(y, fcWeight.Shape()))\n\t\t\t}\n\n\t\t\twT := Must(Transpose(fcWeight, 1, 0))\n\n\t\t\ty = Must(Mul(y, wT))\n\n\t\t\tvar yVal, scaleVal Value\n\t\t\tRead(y, &yVal)\n\t\t\tRead(scale, &scaleVal)\n\n\t\t\tcost, _ := Mean(y)\n\n\t\t\tif _, err := Grad(cost, x, fcWeight, scale, bias); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\toptimizer := NewAdamSolver(WithLearnRate(0.02))\n\t\t\tm := NewTapeMachine(g, BindDualValues(x, fcWeight, scale, bias), TraceExec(), WithInfWatch(), WithNaNWatch())\n\n\t\t\terr = m.RunAll()\n\t\t\tc.NoError(err)\n\n\t\t\tc.NoError(m.Close())\n\n\t\t\terr = optimizer.Step(NodesToValueGrads(Nodes{fcWeight, scale, bias}))\n\t\t\tc.NoError(err)\n\n\t\t\t// for visual inspection\n\t\t\tt.Logf(\"%v input:\\n%v\", tC.desc, x.Value())\n\t\t\tt.Logf(\"%v running mean: %v\", tC.desc, op.runningMean)\n\t\t\tt.Logf(\"%v running var: %v\", tC.desc, op.runningVariance)\n\t\t\tt.Logf(\"%v output:\\n%v\", tC.desc, y.Value())\n\t\t\tt.Logf(\"%v output grad:\\n%v\", tC.desc, y.Deriv().Value())\n\n\t\t\tt.Logf(\"%v fc weight:\\n%v\", tC.desc, fcWeight.Value())\n\n\t\t\tt.Logf(\"%v scale: %v\", tC.desc, scale.Value())\n\t\t\tt.Logf(\"%v scale grad: %v\", tC.desc, scale.Deriv().Value())\n\t\t\tt.Logf(\"%v bias: %v\", tC.desc, bias.Value())\n\t\t\tt.Logf(\"%v bias grad: %v\", tC.desc, bias.Deriv().Value())\n\t\t\tt.Logf(\"%v input grad:\\n%v\", tC.desc, x.Deriv().Value())\n\n\t\t\trunningMean, runningVariance := op.Stats()\n\n\t\t\tc.NotNil(runningMean)\n\t\t\tc.NotNil(runningVariance)\n\n\t\t\tc.True(dawson.AllClose(tC.ExpectedMean, runningMean.Data()), \"Mean doesn't match:\\ngot=%#v expected=%#v\", op.runningMean.Data(), tC.ExpectedMean)\n\t\t\tc.True(dawson.AllClose(tC.ExpectedVariance, runningVariance.Data()), \"Var doesn't match:\\ngot=%#v expected=%#v\", op.runningVariance.Data(), tC.ExpectedVariance)\n\t\t\tc.True(dawson.AllClose(tC.ExpectedTrainResult, yVal.Data()), \"Wrong Output\\ngot=%#v\\nexpected=%#v\", yVal.Data(), tC.ExpectedTrainResult)\n\n\t\t\tc.True(dawson.AllClose(tC.ExpectedOutputGrad, y.Deriv().Value().Data()), \"Output Grad doesn't match:\\ngot=%#v expected=%#v\", y.Deriv().Value().Data(), tC.ExpectedOutputGrad)\n\t\t\tc.True(dawson.AllClose(tC.ExpectedBiasGrad, bias.Deriv().Value().Data()), \"Bias Grad doesn't match:\\ngot=%#v expected=%#v\", bias.Deriv().Value().Data(), tC.ExpectedBiasGrad)\n\t\t\tc.True(dawson.AllClose(tC.ExpectedScaleGrad, scale.Deriv().Value().Data()), \"Scale Grad doens't match:\\ngot=%#v expected=%#v\", scale.Deriv().Value().Data(), tC.ExpectedScaleGrad)\n\t\t\tc.True(dawson.AllClose(tC.ExpectedInputGrad, x.Deriv().Value().Data()), \"Input Grad doesn't match:\\ngot=%#v expected=%#v\", x.Deriv().Value().Data(), tC.ExpectedInputGrad)\n\n\t\t\tt.Logf(\"-------- Switching to Eval Mode --------\")\n\n\t\t\tm2 := NewTapeMachine(g, TraceExec(), WithNaNWatch(), WithInfWatch(), EvalMode())\n\n\t\t\terr = m2.RunAll()\n\t\t\tc.NoError(err)\n\t\t\tc.NoError(m2.Close())\n\n\t\t\tt.Logf(\"%v output:\\n%v\", tC.desc, yVal)\n\t\t\tt.Logf(\"%v input grad:\\n%v\", tC.desc, x.Deriv().Value())\n\t\t\tt.Logf(\"%v running mean: %v\", tC.desc, op.runningMean)\n\t\t\tt.Logf(\"%v running var: %v\", tC.desc, op.runningVariance)\n\t\t\tt.Logf(\"%v bias: %v\", tC.desc, bias.Value())\n\t\t\tt.Logf(\"%v bias grad: %v\", tC.desc, bias.Deriv().Value())\n\t\t\tt.Logf(\"%v scale: %v\", tC.desc, scale.Value())\n\t\t\tt.Logf(\"%v scale grad: %v\", tC.desc, scale.Deriv().Value().Data())\n\n\t\t\tc.True(dawson.AllClose(tC.ExpectedEvalResult, yVal.Data()), \"Output doesn't match\\ngot=%#v\\nexpected=%#v\", yVal.Data(), tC.ExpectedEvalResult)\n\t\t})\n\n\t}\n}\n\nfunc TestBatchNormStacked(t *testing.T) {\n\tfor _, tC := range bnstackedCases {\n\t\tt.Run(tC.desc, func(t *testing.T) {\n\t\t\trand.Seed(0)\n\n\t\t\tc := require.New(t)\n\n\t\t\tg := NewGraph()\n\n\t\t\tx := NewTensor(g, tC.Dtype, tC.XShape.Dims(), WithShape(tC.XShape...), WithInit(tC.XInit), WithName(\"x\"))\n\n\t\t\tt.Logf(\"%v input:\\n%v\", tC.desc, x.Value())\n\n\t\t\tscale1 := NewTensor(g, tC.Dtype, tC.ScaleShape.Dims(), WithShape(tC.ScaleShape...), WithInit(tC.ScaleInit), WithName(\"scale1\"))\n\t\t\tbias1 := NewTensor(g, tC.Dtype, tC.BiasShape.Dims(), WithShape(tC.BiasShape...), WithInit(tC.BiasInit), WithName(\"bias1\"))\n\n\t\t\ty1, _, _, op1, err := BatchNorm(x, scale1, bias1, 0.9, 1e-5)\n\t\t\tc.NoError(err)\n\n\t\t\top1.SetTraining(true)\n\n\t\t\tscale2 := NewTensor(g, tC.Dtype, tC.ScaleShape.Dims(), WithShape(tC.ScaleShape...), WithInit(tC.ScaleInit), WithName(\"scale2\"))\n\t\t\tbias2 := NewTensor(g, tC.Dtype, tC.BiasShape.Dims(), WithShape(tC.BiasShape...), WithInit(tC.BiasInit), WithName(\"bias2\"))\n\n\t\t\ty2, _, _, op2, err := BatchNorm(y1, scale2, bias2, 0.9, 1e-5)\n\t\t\tc.NoError(err)\n\n\t\t\top2.SetTraining(true)\n\n\t\t\tvar yVal, scaleVal Value\n\t\t\tRead(y2, &yVal)\n\t\t\tRead(scale2, &scaleVal)\n\n\t\t\tcost := Must(Mean(y2))\n\n\t\t\tif _, err := Grad(cost, x, scale1, bias1, scale2, bias2); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tm := NewTapeMachine(g, BindDualValues(x, scale1, bias1, scale2, bias2), TraceExec(), WithInfWatch())\n\n\t\t\toptim := NewAdamSolver(WithLearnRate(0.05))\n\t\t\t// optim := NewRMSPropSolver(WithLearnRate(0.05))\n\n\t\t\tfor i := 0; i < tC.Epochs; i++ {\n\t\t\t\tt.Logf(\"-------- STEP %v\", i+1)\n\t\t\t\top1.SetTraining(true)\n\t\t\t\top2.SetTraining(true)\n\n\t\t\t\terr = m.RunAll()\n\t\t\t\tc.NoError(err)\n\n\t\t\t\terr = optim.Step(NodesToValueGrads(Nodes{\n\t\t\t\t\tscale1,\n\t\t\t\t\tbias1,\n\t\t\t\t\tscale2,\n\t\t\t\t\tbias2,\n\t\t\t\t}))\n\t\t\t\tc.NoError(err)\n\n\t\t\t\tm.Reset()\n\n\t\t\t}\n\n\t\t\tc.NoError(err)\n\n\t\t\tc.NoError(m.Close())\n\n\t\t\tt.Logf(\"%v running mean: %v\", tC.desc, op2.runningMean)\n\t\t\tt.Logf(\"%v running var: %v\", tC.desc, op2.runningVariance)\n\t\t\tt.Logf(\"%v y1:\\n%v\", tC.desc, y1.Value())\n\t\t\tt.Logf(\"%v output:\\n%v\", tC.desc, y2.Value())\n\t\t\tt.Logf(\"%v output grad:\\n%v\", tC.desc, y2.Deriv().Value())\n\t\t\tt.Logf(\"%v scale grad: %v\", tC.desc, scale2.Deriv().Value())\n\t\t\tt.Logf(\"%v bias grad: %v\", tC.desc, bias2.Deriv().Value())\n\t\t\tt.Logf(\"%v input grad:\\n%v\", tC.desc, x.Deriv().Value())\n\n\t\t\tc.True(dawson.AllClose(tC.ExpectedMean, op2.runningMean.Data()), \"Mean doesn't match:\\ngot=%#v expected=%#v\", op2.runningMean.Data(), tC.ExpectedMean)\n\t\t\tc.True(dawson.AllClose(tC.ExpectedVariance, op2.runningVariance.Data()), \"Var doesn't match:\\ngot=%#v expected=%#v\", op2.runningVariance.Data(), tC.ExpectedVariance)\n\n\t\t\tc.True(dawson.AllClose(tC.ExpectedTrainResult, yVal.Data()), \"Wrong Output\\ngot=%#v\\nexpected=%#v\", yVal.Data(), tC.ExpectedTrainResult)\n\n\t\t\tt.Logf(\"-------- Switching to Eval Mode --------\")\n\n\t\t\tm2 := NewTapeMachine(g, TraceExec(), WithInfWatch(), EvalMode())\n\n\t\t\terr = m2.RunAll()\n\t\t\tc.NoError(err)\n\n\t\t\tc.NoError(m2.Close())\n\n\t\t\tc.InDeltaSlice(tC.ExpectedEvalResult, yVal.Data(), 1e-5, \"Output doesn't match\\ngot=%#v\\nexpected=%#v\", yVal.Data(), tC.ExpectedEvalResult)\n\t\t})\n\t}\n}\n\nfunc TestLeakyRelu(t *testing.T) {\n\ttests := []struct {\n\t\tname  string\n\t\talpha float64\n\t\txT    tensor.Tensor\n\t\tyT    tensor.Tensor\n\t}{\n\t\t{\n\t\t\tname:  \"simple float32\",\n\t\t\talpha: 0.1,\n\t\t\txT: tensor.New(\n\t\t\t\ttensor.WithShape(3, 4, 5),\n\t\t\t\ttensor.WithBacking([]float32{1.7640524, 0.4001572, 0.978738, 2.2408931, 1.867558, -0.9772779, 0.95008844, -0.1513572, -0.10321885, 0.41059852, 0.14404356, 1.4542735, 0.7610377, 0.121675014, 0.44386324, 0.33367434, 1.4940791, -0.20515826, 0.3130677, -0.85409576, -2.5529897, 0.6536186, 0.8644362, -0.742165, 2.2697546, -1.4543657, 0.045758516, -0.18718386, 1.5327792, 1.4693588, 0.15494743, 0.37816253, -0.88778573, -1.9807965, -0.34791216, 0.15634897, 1.2302907, 1.2023798, -0.3873268, -0.30230275, -1.048553, -1.420018, -1.7062702, 1.9507754, -0.5096522, -0.4380743, -1.2527953, 0.7774904, -1.6138978, -0.21274029, -0.89546657, 0.3869025, -0.51080513, -1.1806322, -0.028182229, 0.42833188, 0.06651722, 0.3024719, -0.6343221, -0.36274117}),\n\t\t\t),\n\t\t\tyT: tensor.New(\n\t\t\t\ttensor.WithShape(3, 4, 5),\n\t\t\t\ttensor.WithBacking([]float32{1.7640524, 0.4001572, 0.978738, 2.2408931, 1.867558, -0.09772779, 0.95008844, -0.01513572, -0.010321885, 0.41059852, 0.14404356, 1.4542735, 0.7610377, 0.121675014, 0.44386324, 0.33367434, 1.4940791, -0.020515827, 0.3130677, -0.085409574, -0.25529897, 0.6536186, 0.8644362, -0.07421651, 2.2697546, -0.14543657, 0.045758516, -0.018718386, 1.5327792, 1.4693588, 0.15494743, 0.37816253, -0.08877858, -0.19807965, -0.034791216, 0.15634897, 1.2302907, 1.2023798, -0.03873268, -0.030230274, -0.1048553, -0.1420018, -0.17062703, 1.9507754, -0.05096522, -0.04380743, -0.12527953, 0.7774904, -0.16138978, -0.021274028, -0.08954666, 0.3869025, -0.051080514, -0.11806323, -0.0028182229, 0.42833188, 0.06651722, 0.3024719, -0.06343221, -0.036274116}),\n\t\t\t),\n\t\t},\n\t\t{\n\t\t\tname:  \"simple float64\",\n\t\t\talpha: 0.1,\n\t\t\txT: tensor.New(\n\t\t\t\ttensor.WithShape(3, 4, 5),\n\t\t\t\ttensor.WithBacking([]float64{1.7640524, 0.4001572, 0.978738, 2.2408931, 1.867558, -0.9772779, 0.95008844, -0.1513572, -0.10321885, 0.41059852, 0.14404356, 1.4542735, 0.7610377, 0.121675014, 0.44386324, 0.33367434, 1.4940791, -0.20515826, 0.3130677, -0.85409576, -2.5529897, 0.6536186, 0.8644362, -0.742165, 2.2697546, -1.4543657, 0.045758516, -0.18718386, 1.5327792, 1.4693588, 0.15494743, 0.37816253, -0.88778573, -1.9807965, -0.34791216, 0.15634897, 1.2302907, 1.2023798, -0.3873268, -0.30230275, -1.048553, -1.420018, -1.7062702, 1.9507754, -0.5096522, -0.4380743, -1.2527953, 0.7774904, -1.6138978, -0.21274029, -0.89546657, 0.3869025, -0.51080513, -1.1806322, -0.028182229, 0.42833188, 0.06651722, 0.3024719, -0.6343221, -0.36274117}),\n\t\t\t),\n\t\t\tyT: tensor.New(\n\t\t\t\ttensor.WithShape(3, 4, 5),\n\t\t\t\ttensor.WithBacking([]float64{1.7640524, 0.4001572, 0.978738, 2.2408931, 1.867558, -0.09772779, 0.95008844, -0.01513572, -0.010321885, 0.41059852, 0.14404356, 1.4542735, 0.7610377, 0.121675014, 0.44386324, 0.33367434, 1.4940791, -0.020515827, 0.3130677, -0.085409574, -0.25529897, 0.6536186, 0.8644362, -0.07421651, 2.2697546, -0.14543657, 0.045758516, -0.018718386, 1.5327792, 1.4693588, 0.15494743, 0.37816253, -0.08877858, -0.19807965, -0.034791216, 0.15634897, 1.2302907, 1.2023798, -0.03873268, -0.030230274, -0.1048553, -0.1420018, -0.17062703, 1.9507754, -0.05096522, -0.04380743, -0.12527953, 0.7774904, -0.16138978, -0.021274028, -0.08954666, 0.3869025, -0.051080514, -0.11806323, -0.0028182229, 0.42833188, 0.06651722, 0.3024719, -0.06343221, -0.036274116}),\n\t\t\t),\n\t\t},\n\t}\n\tfor _, tst := range tests {\n\t\tt.Run(tst.name, func(t *testing.T) {\n\t\t\tg := NewGraph()\n\t\t\tassert := assert.New(t)\n\t\t\tx := NodeFromAny(g, tst.xT)\n\t\t\toutput, err := LeakyRelu(x, tst.alpha)\n\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"%+v\", err)\n\t\t\t}\n\t\t\tm := NewTapeMachine(g)\n\t\t\tif err := m.RunAll(); err != nil {\n\t\t\t\tt.Fatalf(\"%+v\", err)\n\t\t\t}\n\t\t\tdefer m.Close()\n\t\t\tassert.InDeltaSlice(tst.yT.Data(), output.Value().Data(), 1e-6, \"the two tensors should be equal.\")\n\t\t})\n\t}\n}\n\nfunc TestGlobalAveragePool2D_fwdPass(t *testing.T) {\n\tfor _, tst := range []struct {\n\t\tinputT         tensor.Tensor\n\t\texpectedOutput tensor.Tensor\n\t}{\n\t\t{\n\t\t\tinputT: tensor.New(\n\t\t\t\ttensor.WithShape(1, 3, 5, 5),\n\t\t\t\ttensor.WithBacking([]float32{\n\t\t\t\t\t1.7640524, 0.4001572, 0.978738, 2.2408931, 1.867558,\n\t\t\t\t\t-0.9772779, 0.95008844, -0.1513572, -0.10321885, 0.41059852,\n\t\t\t\t\t0.14404356, 1.4542735, 0.7610377, 0.121675014, 0.44386324,\n\t\t\t\t\t0.33367434, 1.4940791, -0.20515826, 0.3130677, -0.85409576,\n\t\t\t\t\t-2.5529897, 0.6536186, 0.8644362, -0.742165, 2.2697546,\n\n\t\t\t\t\t-1.4543657, 0.045758516, -0.18718386, 1.5327792, 1.4693588,\n\t\t\t\t\t0.15494743, 0.37816253, -0.88778573, -1.9807965, -0.34791216,\n\t\t\t\t\t0.15634897, 1.2302907, 1.2023798, -0.3873268, -0.30230275,\n\t\t\t\t\t-1.048553, -1.420018, -1.7062702, 1.9507754, -0.5096522,\n\t\t\t\t\t-0.4380743, -1.2527953, 0.7774904, -1.6138978, -0.21274029,\n\n\t\t\t\t\t-0.89546657, 0.3869025, -0.51080513, -1.1806322, -0.028182229,\n\t\t\t\t\t0.42833188, 0.06651722, 0.3024719, -0.6343221, -0.36274117,\n\t\t\t\t\t-0.67246044, -0.35955316, -0.8131463, -1.7262826, 0.17742614,\n\t\t\t\t\t-0.40178093, -1.6301984, 0.46278226, -0.9072984, 0.051945396,\n\t\t\t\t\t0.7290906, 0.12898292, 1.1394007, -1.2348258, 0.40234163})),\n\t\t\texpectedOutput: tensor.New(\n\t\t\t\ttensor.WithShape(1, 3, 1, 1),\n\t\t\t\ttensor.WithBacking([]float32{0.47517386, -0.1940553, -0.28326008})),\n\t\t},\n\t\t{\n\t\t\tinputT: tensor.New(\n\t\t\t\ttensor.WithShape(1, 3, 5, 5),\n\t\t\t\ttensor.WithBacking([]float64{\n\t\t\t\t\t1.7640524, 0.4001572, 0.978738, 2.2408931, 1.867558,\n\t\t\t\t\t-0.9772779, 0.95008844, -0.1513572, -0.10321885, 0.41059852,\n\t\t\t\t\t0.14404356, 1.4542735, 0.7610377, 0.121675014, 0.44386324,\n\t\t\t\t\t0.33367434, 1.4940791, -0.20515826, 0.3130677, -0.85409576,\n\t\t\t\t\t-2.5529897, 0.6536186, 0.8644362, -0.742165, 2.2697546,\n\n\t\t\t\t\t-1.4543657, 0.045758516, -0.18718386, 1.5327792, 1.4693588,\n\t\t\t\t\t0.15494743, 0.37816253, -0.88778573, -1.9807965, -0.34791216,\n\t\t\t\t\t0.15634897, 1.2302907, 1.2023798, -0.3873268, -0.30230275,\n\t\t\t\t\t-1.048553, -1.420018, -1.7062702, 1.9507754, -0.5096522,\n\t\t\t\t\t-0.4380743, -1.2527953, 0.7774904, -1.6138978, -0.21274029,\n\n\t\t\t\t\t-0.89546657, 0.3869025, -0.51080513, -1.1806322, -0.028182229,\n\t\t\t\t\t0.42833188, 0.06651722, 0.3024719, -0.6343221, -0.36274117,\n\t\t\t\t\t-0.67246044, -0.35955316, -0.8131463, -1.7262826, 0.17742614,\n\t\t\t\t\t-0.40178093, -1.6301984, 0.46278226, -0.9072984, 0.051945396,\n\t\t\t\t\t0.7290906, 0.12898292, 1.1394007, -1.2348258, 0.40234163})),\n\t\t\texpectedOutput: tensor.New(\n\t\t\t\ttensor.WithShape(1, 3, 1, 1),\n\t\t\t\ttensor.WithBacking([]float64{0.47517386, -0.1940553, -0.28326008})),\n\t\t},\n\t} {\n\t\tinputT := tst.inputT\n\t\texpectedOutput := tst.expectedOutput\n\t\tg := NewGraph()\n\t\tassert := assert.New(t)\n\t\tx := NodeFromAny(g, inputT)\n\t\toutput, err := GlobalAveragePool2D(x)\n\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tm := NewTapeMachine(g)\n\t\tif err := m.RunAll(); err != nil {\n\t\t\tt.Fatalf(\"%+v\", err)\n\t\t}\n\t\tdefer m.Close()\n\t\tif len(output.Shape()) != len(expectedOutput.Shape()) {\n\t\t\tt.Fatalf(\"Bad output shape, expected %v, got %v\", expectedOutput.Shape(), output.Shape())\n\t\t}\n\t\tfor i, d := range output.Shape() {\n\t\t\tif expectedOutput.Shape()[i] != d {\n\t\t\t\tt.Fatalf(\"Bad output shape, expected %v, got %v\", expectedOutput.Shape(), output.Shape())\n\t\t\t}\n\t\t}\n\t\tassert.InDeltaSlice(expectedOutput.Data(), output.Value().Data(), 1e-6, \"the two tensors should be equal.\")\n\t}\n}\n\nfunc TestConv2dErrors(t *testing.T) {\n\tg := NewGraph()\n\n\ttestCases := []struct {\n\t\tdesc                  string\n\t\tim                    *Node\n\t\tfilter                *Node\n\t\tkernelShape           tensor.Shape\n\t\tpad, stride, dilation []int\n\t\terr                   string\n\t\tpanics                bool\n\t}{\n\t\t{\n\t\t\tdesc:        \"Succesful\",\n\t\t\tim:          NewTensor(g, tensor.Float64, 4, WithShape(1, 1, 28, 28)),\n\t\t\tfilter:      NewTensor(g, tensor.Float64, 4, WithShape(32, 1, 3, 3)),\n\t\t\tkernelShape: tensor.Shape{3, 3},\n\t\t\tpad:         []int{1, 1},\n\t\t\tstride:      []int{1, 1},\n\t\t\tdilation:    []int{1, 1},\n\t\t\terr:         \"\",\n\t\t},\n\t\t{\n\t\t\tdesc:        \"5dIM\",\n\t\t\tim:          NewTensor(g, tensor.Float64, 5, WithShape(1, 1, 1, 28, 28)),\n\t\t\tfilter:      NewTensor(g, tensor.Float64, 4, WithShape(32, 1, 3, 3)),\n\t\t\tkernelShape: tensor.Shape{3, 3},\n\t\t\tpad:         []int{1, 1},\n\t\t\tstride:      []int{1, 1},\n\t\t\tdilation:    []int{1, 1},\n\t\t\terr:         \"im should have 4 dims, got 5 dims\",\n\t\t},\n\t\t{\n\t\t\tdesc:        \"5dFilter\",\n\t\t\tim:          NewTensor(g, tensor.Float64, 4, WithShape(1, 1, 28, 28)),\n\t\t\tfilter:      NewTensor(g, tensor.Float64, 5, WithShape(32, 1, 1, 3, 3)),\n\t\t\tkernelShape: tensor.Shape{3, 3},\n\t\t\tpad:         []int{1, 1},\n\t\t\tstride:      []int{1, 1},\n\t\t\tdilation:    []int{1, 1},\n\t\t\terr:         \"filter should have 4 dims, got 5 dims\",\n\t\t},\n\t\t{\n\t\t\tdesc:        \"Shapes\",\n\t\t\tim:          NewTensor(g, tensor.Float64, 4, WithShape(1, 1, 28, 28)),\n\t\t\tfilter:      NewTensor(g, tensor.Float64, 4, WithShape(32, 3, 3, 3)),\n\t\t\tkernelShape: tensor.Shape{3, 3},\n\t\t\tpad:         []int{1, 1},\n\t\t\tstride:      []int{1, 1},\n\t\t\tdilation:    []int{1, 1},\n\t\t\terr:         \"3 (kernel) * 3 (width) * 3 (height) must be 9, got 27\",\n\t\t},\n\t}\n\n\tfor _, tC := range testCases {\n\t\tt.Run(tC.desc, func(t *testing.T) {\n\t\t\tc := assert.New(t)\n\n\t\t\t_, err := Conv2d(tC.im, tC.filter, tC.kernelShape, tC.pad, tC.stride, tC.dilation)\n\t\t\tif tC.err != \"\" {\n\t\t\t\trequire.Error(t, err)\n\t\t\t\tc.Equal(tC.err, err.Error())\n\t\t\t} else {\n\t\t\t\tc.NoError(err)\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "node.go",
          "type": "blob",
          "size": 21.7314453125,
          "content": "package gorgonia\n\nimport (\n\t\"bytes\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"hash\"\n\t\"hash/fnv\"\n\n\t\"github.com/awalterschulze/gographviz\"\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n\t\"gonum.org/v1/gonum/graph\"\n\t\"gorgonia.org/gorgonia/internal/encoding\"\n\t\"gorgonia.org/tensor\"\n)\n\n// NodeID represents the ID of a node. In this version\n// it doesn't really do much (except you can pass it into some VMs)\ntype NodeID int64\n\n// A Node is a node in the computation graph\ntype Node struct {\n\t// metadata of the node\n\tt     hm.Type // pruned types only plz\n\tshape tensor.Shape\n\n\t// this node is the result of applying the op to the children\n\top       Op\n\tchildren Nodes // shortcut, instead of having to go through the graph\n\n\t// For nicely grouping stuff in graphviz.\n\t// TODO: Should this be in *Node?\n\tname string\n\t// DEPRECATED: the group attribute will be removed in the next version in favor of groups\n\tgroup string\n\t// the grouping notion is only useful for exporting to another format\n\tgroups encoding.Groups\n\n\tg *ExprGraph // this node belongs in this graph\n\n\t// value bondage\n\t// inputs are bound to values directly\n\tboundTo Value\n\tdataOn  Device // where is the data on\n\n\t// to track derivations\n\tderivOf Nodes\n\tderiv   *Node\n\n\t// for hashing nodes\n\tid   int64 // id is the ID at which the node is added to the graph\n\thash uint32\n\n\thashed        bool\n\tinferredShape bool // is shape inferred?\n\tunchanged     bool // has this node been modified\n\tisStmt        bool // is this a statement node\n\tofInterest    bool // is this node of particular interest? (for debugging)\n}\n\n// NodeConsOpt is a function that provides construction options for any Node.\ntype NodeConsOpt func(*Node)\n\n// WithType is a node construction option to set a node to the specified type.\n// Types in *Node are immutable once set. If the type has already been specified in the node,\n// a check will be made to see if the both types are the same. If it isn't, it will panic.\nfunc WithType(t hm.Type) NodeConsOpt {\n\tf := func(n *Node) {\n\t\tif n.t == nil {\n\t\t\tn.t = t\n\t\t} else if !n.t.Eq(t) {\n\t\t\tpanic(fmt.Sprintf(\"Node's type is %v. Asking to construct a Node with %v\", n.t, t))\n\t\t}\n\t}\n\treturn f\n}\n\n// WithChildren sets the children of a node to the specified chidren.\n// This construction option does NOT check if existing children exists, and will overwrite the existing children.\nfunc WithChildren(children Nodes) NodeConsOpt {\n\tf := func(n *Node) {\n\t\tn.children = children\n\t}\n\treturn f\n}\n\n// WithOp is a node construction option to set a node's Op to the specified Op.\n// `Op`s in `*Node`s are immutable once set and cannot be changed. If the node already has an Op specified\n// a check will be made to see if the provided Op and the one already specified in the `*Node` is the same -\n// do note that comparison of Ops is done using the `Hashcode()` method of Ops, and hash collisions MAY occur -\n// If both ops are different, this function will panic.\nfunc WithOp(op Op) NodeConsOpt {\n\tf := func(n *Node) {\n\t\tif n.op != nil {\n\t\t\tif op.Hashcode() != n.op.Hashcode() {\n\t\t\t\tpanic(fmt.Sprintf(\"Node Ops are immutable. Cannot set op %v\", op))\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\tn.op = op\n\t\tif _, ok := op.(stmtOp); ok {\n\t\t\tn.isStmt = true\n\t\t}\n\t}\n\treturn f\n}\n\n// In is a node construction option to set a node's graph.\n// A `*Node`'s graph is immutable. If the graph has already been set, a check will be made that the specifiec *Graph\n// and the *Graph set in *Node are the same. If they are not, the function will panic/\nfunc In(g *ExprGraph) NodeConsOpt {\n\tf := func(n *Node) {\n\t\tif n.g != nil {\n\t\t\tif g != n.g {\n\t\t\t\tpanic(fmt.Sprintf(\"Node Graphs are immutable. Cannot set g %v\", g))\n\t\t\t}\n\t\t}\n\t\tn.g = g\n\t}\n\treturn f\n}\n\n// WithName is a node construction option that gives the *Node the provided name. This is especially useful in debugging graphs.\nfunc WithName(name string) NodeConsOpt {\n\tf := func(n *Node) {\n\t\tn.name = name\n\t}\n\treturn f\n}\n\n// WithValue is a node construction option that binds the value to the *Node. This function may panic if:\n//\t- Gorgonia was unable to convert interface{} into a Value.\n//\t- The type of the Value does not match the type of the nodes.\nfunc WithValue(any interface{}) NodeConsOpt {\n\tv, t, _, err := anyToValue(any)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tf := func(n *Node) {\n\t\tif n.t == nil {\n\t\t\tn.t = t\n\t\t} else if !n.t.Eq(t) {\n\t\t\t// scalars are exempted\n\t\t\t// if n.t is a scalar type, and the `anyToValue` returns a vector that is scalar-like,\n\t\t\t// we'll mark it as a scalar\n\t\t\ttt, ok1 := t.(TensorType)\n\t\t\tndt, ok2 := n.t.(tensor.Dtype)\n\n\t\t\tif !(ok1 && ok2 && v.Shape().IsScalarEquiv() && tt.Of == ndt) {\n\t\t\t\tpanic(fmt.Sprintf(\"TypeError: Want %v, Got %v instead (%T %T)\", n.t, t, n.t, t)) // yes this is a runtime error\n\t\t\t}\n\t\t\tv.(tensor.Tensor).Reshape() // rehsape to scalar\n\n\t\t}\n\n\t\tn.bind(v)\n\t\tif n.shape == nil {\n\t\t\tn.shape = v.Shape()\n\t\t}\n\t}\n\treturn f\n}\n\n// WithGrad is a node construction option that binds the value to the *Node. This function may panic if:\n//\t- There isn't already a value associated with the node (.boundTo == nil)\n//\t- The type of the Value does not match the value of the node.\nfunc WithGrad(any interface{}) NodeConsOpt {\n\tv, t, _, err := anyToValue(any)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tf := func(n *Node) {\n\t\tif n.boundTo == nil {\n\t\t\tpanic(\"No value already bound to node\")\n\t\t}\n\t\tif !TypeOf(n.boundTo).Eq(t) {\n\t\t\tpanic(\"Different types \")\n\t\t}\n\n\t\tif dv, ok := n.boundTo.(*dualValue); !ok {\n\t\t\tif err := n.bind(&dualValue{Value: n.boundTo, d: v}); err != nil {\n\t\t\t\tpanic(err)\n\t\t\t}\n\t\t} else {\n\t\t\tdv.d = v\n\t\t}\n\t}\n\treturn f\n}\n\n// WithInit is a node construction option to initialize a *Node with the InitWFn provided.\nfunc WithInit(fn InitWFn) NodeConsOpt {\n\tf := func(n *Node) {\n\t\tdt, err := dtypeOf(n.t)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\n\t\tvar v Value\n\t\tv = tensor.New(tensor.WithShape(n.shape...), tensor.WithBacking(fn(dt, n.shape...)))\n\t\tWithValue(v)(n)\n\t}\n\treturn f\n}\n\n// WithShape is a node construction option to initialize a *Node with a particular shape.\n// This function panics if the shape's dimensions do not match the specified dimensions of the *Node.\nfunc WithShape(shp ...int) NodeConsOpt {\n\ts := tensor.Shape(tensor.BorrowInts(len(shp)))\n\tcopy(s, shp)\n\tf := func(n *Node) {\n\t\tif n.t == nil && n.shape == nil {\n\t\t\tn.shape = s\n\t\t\treturn\n\t\t}\n\t\tnd := n.Dims()\n\t\tisVec := s.IsColVec() || s.IsRowVec()\n\t\tacceptVec := (isVec && (nd == 1))\n\t\tsameDims := nd == s.Dims()\n\t\tacceptScalar := nd == 0 && scalarEquiv(s)\n\n\t\tif !acceptVec && !sameDims && !acceptScalar {\n\t\t\tpanic(fmt.Sprintf(\"Node %v, has %d dimensions(Shape: %v). Input shape is %v, which has %d dimensions\", n, n.Dims(), n.shape, s, s.Dims()))\n\t\t}\n\t\tn.shape = s\n\t}\n\treturn f\n}\n\n// WithGroupName is a node construction option to group a *Node within a particular group. This option is useful for debugging with graphs.\n// This function is deprecated and will proabably be remove in the next version.\nfunc WithGroupName(name string) NodeConsOpt {\n\tf := func(n *Node) {\n\t\tif n.group == \"\" {\n\t\t\tn.group = name\n\t\t}\n\t}\n\treturn f\n}\n\n// withGroup is a node construction option to group a *Node within a particular group. This option is useful for debugging with graphs.\nfunc withGroup(group encoding.Group) NodeConsOpt {\n\tf := func(n *Node) {\n\t\tn.groups = n.groups.Upsert(group)\n\t}\n\treturn f\n}\n\n// Groups to fulfil the encoding Grouper interface\nfunc (n *Node) Groups() encoding.Groups {\n\tvar isConst bool\n\tvar isInput = n.isInput()\n\n\tif n.op != nil {\n\t\t_, isConst = n.op.(constant)\n\t}\n\n\tswitch {\n\tcase isConst:\n\t\tn.groups = n.groups.Upsert(encoding.ConstantCluster)\n\tcase isInput:\n\t\tn.groups = n.groups.Upsert(encoding.InputCluster)\n\tdefault:\n\t\tn.groups = n.groups.Upsert(encoding.ExprGraphCluster)\n\t}\n\treturn n.groups\n}\n\nfunc newNode(opts ...NodeConsOpt) *Node {\n\tn := borrowNode()\n\tn.dataOn = CPU\n\tn.id = -1\n\tn.t = nil\n\tn.shape = nil\n\n\tfor _, opt := range opts {\n\t\topt(n)\n\t}\n\tn.fix()\n\n\tincrNN()\n\treturn n\n}\n\n// NewUniqueNode creates a new unique node in a graph. If no graph was specified in the construction options then it will just return a graphless node.\nfunc NewUniqueNode(opts ...NodeConsOpt) *Node {\n\tn := newNode(opts...)\n\tif n.g == nil {\n\t\treturn n\n\t}\n\tn.fixChildren() // ensure that all the kids are in the graph first\n\n\tm := n.g.AddNode(n)\n\tif n != m {\n\t\treturnNode(n)\n\t}\n\tm.fixEdges()\n\treturn m\n}\n\n// ID returns the ID of the node. This satisfies the gonum/graph.Node interface\nfunc (n *Node) ID() int64 { return n.id }\n\n// Node returns itself. This sorts of monoidal patterns are useful for compositions via interfaces.\nfunc (n *Node) Node() *Node { return n }\n\n// Nodes returns n as a slice of *Node. Again, this is mostly useful for interfaces\nfunc (n *Node) Nodes() Nodes { return Nodes{n} }\n\n// Err always returns nil. However, this method is implemented to enable nicer composition of functions\nfunc (n *Node) Err() error { return nil }\n\nfunc (n *Node) DataSize() int { return n.Shape().TotalSize() }\n\nfunc (n *Node) DerivOf() Nodes { return n.derivOf }\n\nfunc (n *Node) Deriv() *Node { return n.deriv }\n\n// helper functions to help compilation process\nfunc (n *Node) isArg() bool      { return n.op == nil }\nfunc (n *Node) isInput() bool    { return (n.isArg() || n.isRandom()) && !n.isStmt }\nfunc (n *Node) isMutable() bool  { return !n.isInput() && n.op.ReturnsPtr() }\nfunc (n *Node) isConstant() bool { _, ok := n.op.(constant); return ok }\nfunc (n *Node) isRandom() bool   { _, ok := n.op.(randomOp); return ok }\n\nfunc (n *Node) isRoot() bool {\n\tif n.g == nil {\n\t\treturn true\n\t}\n\treturn len(n.g.to[n]) == 0\n}\n\n// IsVar returns true if  the node represents a differentiable variable (i.e. it's an argument to the function that is not a statement)\nfunc (n *Node) IsVar() bool { return n.isArg() && !n.isStmt && !n.isConstant() }\n\n// type related isX() helper methods\n\n// IsScalar indicates if a node represents a a scalar value. This is based on the type of the node, not the actual value associated with the node\nfunc (n *Node) IsScalar() bool { _, ok := n.t.(tensor.Dtype); return ok }\n\n// IsVector indicates if a node represents a vector value. This is based on the type of the node, not the actual value associated with the node\nfunc (n *Node) IsVector() bool {\n\tif t, ok := n.t.(TensorType); ok {\n\t\treturn t.Dims == 1\n\t}\n\n\treturn false\n}\n\n// IsColVec indicates if a node represents a Column Vector. This is based on the type of the node, not the actual value associated with the node\nfunc (n *Node) IsColVec() bool {\n\tif _, ok := n.t.(TensorType); ok {\n\t\tif n.shape != nil {\n\t\t\treturn n.shape.IsColVec()\n\t\t}\n\t}\n\treturn false\n}\n\n// IsRowVec indicates if a node represents a Row Vector. This is based on the type of the node, not the actual value associated with the node\nfunc (n *Node) IsRowVec() bool {\n\tif _, ok := n.t.(TensorType); ok {\n\t\tif n.shape != nil {\n\t\t\treturn n.shape.IsRowVec()\n\t\t}\n\t}\n\treturn false\n}\n\n// IsMatrix indicates if a node represents a matrix. This is based on the type of the node, not the actual value associated with the node\nfunc (n *Node) IsMatrix() bool {\n\tif _, ok := n.t.(TensorType); ok {\n\t\treturn n.shape.Dims() == 2\n\t}\n\treturn false\n}\n\n// methods\n\n// Graph returns the graph of the node\nfunc (n *Node) Graph() *ExprGraph { return n.g }\n\n// CloneTo clones the node into a new graph. If CloneTo() is called on the same graph as the n, it will return n. The reason this is done is because\n// at any given time, every node  should be unique in the *ExprGraph.\n//\n//TODO: clone children as well (this means that CloneTo() is only currently suitable fo input nodes)\nfunc (n *Node) CloneTo(g *ExprGraph) *Node {\n\tif n.g != nil && g == n.g {\n\t\treturn n\n\t}\n\tn2 := n.Clone().(*Node)\n\tn2.g = g\n\tn2 = g.AddNode(n2)\n\treturn n2\n}\n\n// Clone clones the node. There are some caveats:\n//\t\t- the graph is not copied over - the node essentially does not belong to a collection\n//\t\t- there is no ID\n// \t\t- the children are not cloned\nfunc (n *Node) Clone() (retVal interface{}) {\n\tn2 := newNode(In(n.g), WithOp(n.op), WithName(n.name), WithType(n.t))\n\tif n.shape != nil {\n\t\tn2.shape = n.shape.Clone()\n\t\tn2.inferredShape = n.inferredShape\n\t}\n\n\tif n.boundTo != nil {\n\t\tvar err error\n\t\tif n2.boundTo, err = CloneValue(n.boundTo); err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n\n\t// reset\n\tn2.g = nil\n\n\t// other things\n\tn2.name = n.name\n\tn2.group = n.group\n\tn2.dataOn = n.dataOn\n\tn2.hash = n.hash\n\n\tn2.hashed = n.hashed\n\tn2.inferredShape = n.inferredShape\n\tn2.unchanged = n.unchanged\n\tn2.isStmt = n.isStmt\n\tn2.ofInterest = n.ofInterest\n\treturn n2\n}\n\n// Value returns the values bound to the node. May return nil\nfunc (n *Node) Value() Value {\n\tif n.isConstant() {\n\t\treturn n.op.(constant).Value()\n\t}\n\tif dv, ok := n.boundTo.(*dualValue); ok {\n\t\treturn dv.Value\n\t}\n\treturn n.boundTo\n}\n\n// Grad returns the gradient if there is one.\nfunc (n *Node) Grad() (Value, error) {\n\tif dv, ok := n.boundTo.(*dualValue); ok {\n\t\treturn dv.d, nil\n\t}\n\tif n.deriv != nil {\n\t\treturn n.deriv.Value(), nil\n\t}\n\n\treturn nil, errors.Errorf(\"No Gradient node/value found for %T\", n)\n}\n\n// Dims indicates how many dimensions the node's result has\nfunc (n *Node) Dims() int {\n\tif n.shape != nil {\n\t\treturn n.shape.Dims()\n\t}\n\tswitch nt := n.t.(type) {\n\tcase TensorType:\n\t\treturn nt.Dims\n\tcase tensor.Dtype:\n\t\treturn 0\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"Dims undefined for %v(%T)\", nt, nt))\n\t}\n}\n\n// Type returns the type of the node\nfunc (n *Node) Type() hm.Type { return n.t }\n\n// Dtype returns the dtype of the node\nfunc (n *Node) Dtype() tensor.Dtype {\n\tdt, err := dtypeOf(n.t)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn dt\n}\n\n// Shape returns the shape of the node\nfunc (n *Node) Shape() tensor.Shape { return n.shape.Clone() }\n\n// Strides returns the strides of the value of the node\nfunc (n *Node) Strides() []int {\n\tif n.boundTo != nil {\n\t\tswitch v := n.boundTo.(type) {\n\t\tcase *dualValue:\n\t\t\treturn v.Value.(tensor.Tensor).Strides()\n\t\tcase tensor.Tensor:\n\t\t\treturn v.Strides()\n\t\tdefault:\n\t\t\tpanic(fmt.Sprintf(\"Unhandled type for Strides(): %T. Using fallback method and assuming dense tensor types\", n.boundTo))\n\t\t}\n\t}\n\treturn n.shape.CalcStrides()\n}\n\n// Device returns the device the data will be on\nfunc (n *Node) Device() Device { return n.dataOn }\n\n// Op returns the Op of the node\nfunc (n *Node) Op() Op { return n.op }\n\n// IsVec returns whether this node is a vector\nfunc (n *Node) IsVec() bool { return n.IsVector() }\n\n// Name returns the name of the node. If a name was specified and it is too long,\n// the short name will be used instead (except in inputs)\n//\n// The short name is typically of the form: OpName(%1, %2 ...), making it read more like a function call\nfunc (n *Node) Name() string {\n\tif n.name != \"\" {\n\t\treturn n.name\n\t}\n\n\tvar buf bytes.Buffer\n\tfmt.Fprintf(&buf, \"%s(\", n.op)\n\tfor i, child := range n.children {\n\t\tfmt.Fprintf(&buf, \"%%%x\", child.id)\n\t\tif i < len(n.children)-1 {\n\t\t\tbuf.WriteString(\", \")\n\t\t}\n\t}\n\tbuf.WriteString(\")\")\n\treturn buf.String()\n}\n\n// WriteHash writes the hash to the provided Hash32.\nfunc (n *Node) WriteHash(h hash.Hash32) {\n\tfmt.Fprintf(h, \"%v%v\", n.t, n.shape)\n\n\tif n.isInput() {\n\t\th.Write([]byte(n.name))\n\t} else {\n\n\t\tn.op.WriteHash(h)\n\t}\n\n\t// if len(n.children) == 0 {\n\t// \tbinary.Write(h, binary.LittleEndian, byte(0))\n\t// }\n\n\tbinary.Write(h, binary.LittleEndian, byte(len(n.children)))\n\tfor _, child := range n.children {\n\t\tbinary.Write(h, binary.LittleEndian, child.Hashcode())\n\t}\n\n}\n\n// Hashcode provides the hash for the tree, assuming that the node is the root of the tree.\n// Original implementation was here by Vatine (who's apparently 80 years old and using SO!?!):\n//\t\thttp://stackoverflow.com/questions/1988665/hashing-a-tree-structure\nfunc (n *Node) Hashcode() uint32 {\n\tif n.hashed {\n\t\treturn n.hash\n\t}\n\th := fnv.New32a()\n\tn.WriteHash(h)\n\tn.hash = h.Sum32()\n\tn.hashed = true\n\treturn n.hash\n}\n\n// ToDot returns the graph as a graphviz compatible string.\n// DEPRECATED: This function will be removed in the next release, please use the encoding/dot package\nfunc (n *Node) ToDot() string {\n\tgraphName := exprgraphClust\n\n\tg := gographviz.NewEscape()\n\tg.SetName(graphName)\n\tg.SetDir(true)\n\n\tg.AddAttr(exprgraphClust, \"splines\", \"spline\")\n\tg.AddAttr(exprgraphClust, \"nodesep\", \"0.5\")\n\tg.AddAttr(exprgraphClust, \"ranksep\", \"1.2 equally\")\n\n\tseen := make(map[*Node]string)\n\tn.dot(g, graphName, seen)\n\n\treturn g.String()\n}\n\n// RestrictedToDot prints the graphviz compatible string but does not print the entire tree\n// up and down indicates how many levels to look up, and how many levels to look down\nfunc (n *Node) RestrictedToDot(up, down int) string {\n\tif n.g == nil {\n\t\treturn n.ToDot()\n\t}\n\n\tg := n.g\n\tvar ns, upQ, downQ Nodes\n\n\t//\tup\n\tns = Nodes{n}\n\tupQ = Nodes{n}\n\tfor l := 0; l < up; l++ {\n\t\torigLen := len(upQ)\n\t\tfor i := 0; i < origLen; i++ {\n\t\t\tqn := upQ[i]\n\t\t\ttoQN := sliceNodesToNodes(graph.NodesOf(g.To(qn.ID())))\n\t\t\tupQ = append(upQ, toQN...)\n\t\t\tns = append(ns, toQN...)\n\t\t}\n\t\tupQ = upQ[origLen:]\n\t}\n\n\t// down\n\tdownQ = Nodes{n}\n\tfor d := 0; d < down; d++ {\n\t\torigLen := len(downQ)\n\t\tfor i := 0; i < origLen; i++ {\n\t\t\tqn := downQ[i]\n\t\t\tdownQ = append(downQ, qn.children...)\n\t\t\tns = append(ns, qn.children...)\n\t\t}\n\t\tdownQ = downQ[origLen:]\n\t}\n\n\tsg := g.subgraph(ns, false)\n\n\tn.ofInterest = true\n\tdefer func() {\n\t\tn.ofInterest = false\n\t}()\n\treturn sg.ToDot()\n}\n\n// String() implements the fmt.Stringer interface\nfunc (n *Node) String() string {\n\tvar buf bytes.Buffer\n\tif n.Name() != \"\" {\n\t\tfmt.Fprintf(&buf, \"%s :: \", n.Name())\n\t} else {\n\t\tfmt.Fprintf(&buf, \"%s :: \", n.op)\n\t}\n\tif c, ok := n.op.(constant); ok {\n\t\tfmt.Fprintf(&buf, \"%v{%v}\", n.t, c.Value())\n\t} else {\n\t\tfmt.Fprintf(&buf, \"%v\", n.t)\n\t}\n\treturn buf.String()\n}\n\n// private methods\n\n// TODO: check type, check shape, check if needsGrad -> promote to dualValue\nfunc (n *Node) bind(v Value) error {\n\tif n.boundTo == nil {\n\t\tn.boundTo = v\n\t\treturn nil\n\t}\n\n\tif dv, ok := n.boundTo.(*dualValue); ok {\n\t\tif vdv, ok := v.(*dualValue); ok {\n\t\t\tif vdv == dv {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tif n.isRandom() {\n\t\t\t\t// then simply replace the value in it\n\t\t\t\tdv.Value = vdv.Value\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\t// n.boundTo = vdv\n\t\t\t// return nil\n\t\t\tpanic(\"Undefined behaviour\") // no seriously there literally is no defined behaviour of what should the right thing be. I'll come back to this TODO.\n\t\t}\n\t\tdv.Value = v\n\t\treturn nil\n\t}\n\n\tn.boundTo = v\n\n\treturn nil\n}\n\n// bindCopy copies the value if to the bound value.\nfunc (n *Node) bindCopy(v Value) (err error) {\n\tif n.boundTo == nil {\n\t\tvar cloned Value\n\t\tif cloned, err = CloneValue(v); err != nil {\n\t\t\treturn\n\t\t}\n\t\tn.boundTo = cloned\n\t\treturn nil\n\t}\n\n\tvar copied Value\n\tif dv, ok := n.boundTo.(*dualValue); ok {\n\n\t\tif vdv, ok := v.(*dualValue); ok {\n\t\t\tif vdv == dv {\n\t\t\t\treturn nil // no need to copy!\n\t\t\t}\n\n\t\t\tif n.isRandom() {\n\t\t\t\t// returnValue(dv.Value)\n\t\t\t\tdv.Value = vdv.Value\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\treturn errors.Errorf(\"Cannot yet handle bindCopy() of *dualValue into *dualValue\") // TODO FIX\n\t\t}\n\t\tif copied, err = Copy(dv.Value, v); err != nil {\n\t\t\treturn errors.Wrapf(err, \"Failed to copy while binding to node with *dualValue\")\n\t\t}\n\t\tdv.Value = copied // in case they're scalars\n\t\treturn nil\n\t}\n\tif copied, err = Copy(n.boundTo, v); err != nil {\n\t\treturn errors.Wrapf(err, \"Failed to copy while binding to node\")\n\t}\n\tn.boundTo = copied // in case it's a scalar\n\treturn nil\n}\n\n// unbind releases the values back to the pool\nfunc (n *Node) unbind() {\n\tif n.boundTo == nil {\n\t\treturn\n\t}\n\n\tif dv, ok := n.boundTo.(*dualValue); ok {\n\t\treturnDV(dv)\n\t}\n\n\tif t, ok := n.boundTo.(tensor.Tensor); ok {\n\t\treturnTensor(t)\n\t}\n\tn.boundTo = nil\n}\n\nfunc (n *Node) dotCluster() string {\n\tvar group string\n\tvar isConst bool\n\tvar isInput = n.isInput()\n\n\tif n.op != nil {\n\t\t_, isConst = n.op.(constant)\n\t}\n\n\tswitch {\n\tcase isConst:\n\t\tgroup = constantsClust\n\tcase isInput:\n\t\tgroup = inputsClust\n\tcase n.group == \"\":\n\t\tgroup = exprgraphClust\n\tdefault:\n\t\tgroup = n.group\n\t}\n\treturn group\n}\n\nfunc (n *Node) dot(g *gographviz.Escape, graphName string, seen map[*Node]string) string {\n\tvar id string\n\tvar ok bool\n\tif id, ok = seen[n]; !ok {\n\t\tid = n.dotString(g, graphName)\n\t\tseen[n] = id\n\t} else {\n\t\treturn id\n\t}\n\n\tfor i, child := range n.children {\n\t\tchildID := child.dot(g, graphName, seen)\n\t\tedgeAttrs := map[string]string{\n\t\t\t\"taillabel\":  fmt.Sprintf(\" %d \", i+1),\n\t\t\t\"labelfloat\": \"false\",\n\t\t}\n\n\t\tg.AddPortEdge(id, id+\":anchor:s\", childID, childID+\":anchor:n\", true, edgeAttrs)\n\t}\n\treturn id\n}\n\nfunc (n *Node) fix() {\n\tif n.IsScalar() {\n\t\tn.shape = scalarShape\n\t}\n\n\tif n.isConstant() {\n\t\treturn\n\t}\n\n\tif n.g == nil {\n\t\tpanic(fmt.Sprintf(\"no graph supplied %v\", n))\n\t}\n}\n\nfunc (n *Node) fixChildren() {\n\tif n.g == nil {\n\t\treturn\n\t}\n\n\tfor i, child := range n.children {\n\t\tnewChild := n.g.AddNode(child)\n\t\tif child != newChild {\n\t\t\tn.children[i] = newChild\n\t\t}\n\t}\n}\n\nfunc (n *Node) fixEdges() {\n\tif n.g == nil {\n\t\treturn\n\t}\n\n\tif len(n.children) > 0 {\n\t\tfor _, child := range n.children {\n\t\t\te := edge{from: n, to: child}\n\t\t\tn.g.SetEdge(e)\n\t\t}\n\t} else {\n\t\tn.g.leaves = append(n.g.leaves, n)\n\t}\n}\n\nfunc (n *Node) setShape(s tensor.Shape, inferred bool) {\n\tn.shape = s\n\tn.inferredShape = inferred\n}\n\nfunc (n *Node) setGroup(grp string) {\n\tn.group = grp\n}\n\nfunc (n *Node) clone(opts ...NodeConsOpt) *Node {\n\tif n.isInput() {\n\t\treturn n\n\t}\n\n\tnn := newNode(WithChildren(n.children),\n\t\tWithType(n.t),\n\t\tWithOp(n.op),\n\t\tWithName(n.name),\n\t\tIn(n.g),\n\t)\n\n\tfor _, opt := range opts {\n\t\topt(nn)\n\t}\n\n\t// if the shape is already known...\n\tif n.shape != nil {\n\t\tnn.shape = n.shape\n\t\tnn.inferredShape = n.inferredShape\n\t}\n\n\treturn nn\n}\n\nfunc (n *Node) diffWRT() []bool {\n\tif sdop, ok := n.op.(SDOp); ok {\n\t\treturn sdop.DiffWRT(len(n.children))\n\t}\n\treturn nil\n}\n\n// dfs but does not use channels. useful for extracting paths. used particularly in test\nfunc (n *Node) seqWalk() Nodes {\n\tretVal := Nodes{n}\n\tfor _, child := range n.children {\n\t\tretVal = append(retVal, child.seqWalk()...)\n\t}\n\treturn retVal\n}\n\n// dotString returns the ID of the node.\nfunc (n *Node) dotString(g *gographviz.Escape, graphName string) string {\n\tvar buf bytes.Buffer\n\tif err := exprNodeTempl.ExecuteTemplate(&buf, \"node\", n); err != nil {\n\t\tpanic(err)\n\t}\n\n\tid := fmt.Sprintf(\"Node_%p\", n)\n\tlabel := buf.String()\n\tattrs := map[string]string{\n\t\t\"fontname\": \"monospace\",\n\t\t\"shape\":    \"none\",\n\t\t\"label\":    label,\n\t}\n\n\tg.AddNode(graphName, id, attrs)\n\treturn id\n}\n"
        },
        {
          "name": "node_set.go",
          "type": "blob",
          "size": 3.927734375,
          "content": "// Generated by: main\n// TypeWriter: set\n// Directive: +gen on *Node\n\npackage gorgonia\n\n// Set is a modification of https://github.com/deckarep/golang-set\n// The MIT License (MIT)\n// Copyright (c) 2013 Ralph Caraveo (deckarep@gmail.com)\n\n// NodeSet is the primary type that represents a set\ntype NodeSet map[*Node]struct{}\n\n// NewNodeSet creates and returns a reference to an empty set.\nfunc NewNodeSet(a ...*Node) NodeSet {\n\ts := make(NodeSet)\n\tfor _, i := range a {\n\t\ts.Add(i)\n\t}\n\treturn s\n}\n\n// ToSlice returns the elements of the current set as a slice\nfunc (set NodeSet) ToSlice() Nodes {\n\tvar s Nodes\n\tfor v := range set {\n\t\ts = append(s, v)\n\t}\n\treturn s\n}\n\n// Add adds an item to the current set if it doesn't already exist in the set.\nfunc (set NodeSet) Add(i *Node) bool {\n\t_, found := set[i]\n\tset[i] = struct{}{}\n\treturn !found //False if it existed already\n}\n\n// Contains determines if a given item is already in the set.\nfunc (set NodeSet) Contains(i *Node) bool {\n\t_, found := set[i]\n\treturn found\n}\n\n// ContainsAll determines if the given items are all in the set\nfunc (set NodeSet) ContainsAll(i ...*Node) bool {\n\tfor _, v := range i {\n\t\tif !set.Contains(v) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// IsSubset determines if every item in the other set is in this set.\nfunc (set NodeSet) IsSubset(other NodeSet) bool {\n\tfor elem := range set {\n\t\tif !other.Contains(elem) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// IsSuperset determines if every item of this set is in the other set.\nfunc (set NodeSet) IsSuperset(other NodeSet) bool {\n\treturn other.IsSubset(set)\n}\n\n// Union returns a new set with all items in both sets.\nfunc (set NodeSet) Union(other NodeSet) NodeSet {\n\tunionedSet := NewNodeSet()\n\n\tfor elem := range set {\n\t\tunionedSet.Add(elem)\n\t}\n\tfor elem := range other {\n\t\tunionedSet.Add(elem)\n\t}\n\treturn unionedSet\n}\n\n// Intersect returns a new set with items that exist only in both sets.\nfunc (set NodeSet) Intersect(other NodeSet) NodeSet {\n\tintersection := NewNodeSet()\n\t// loop over smaller set\n\tif set.Cardinality() < other.Cardinality() {\n\t\tfor elem := range set {\n\t\t\tif other.Contains(elem) {\n\t\t\t\tintersection.Add(elem)\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfor elem := range other {\n\t\t\tif set.Contains(elem) {\n\t\t\t\tintersection.Add(elem)\n\t\t\t}\n\t\t}\n\t}\n\treturn intersection\n}\n\n// Difference returns a new set with items in the current set but not in the other set\nfunc (set NodeSet) Difference(other NodeSet) NodeSet {\n\tdifferencedSet := NewNodeSet()\n\tfor elem := range set {\n\t\tif !other.Contains(elem) {\n\t\t\tdifferencedSet.Add(elem)\n\t\t}\n\t}\n\treturn differencedSet\n}\n\n// SymmetricDifference returns a new set with items in the current set or the other set but not in both.\nfunc (set NodeSet) SymmetricDifference(other NodeSet) NodeSet {\n\taDiff := set.Difference(other)\n\tbDiff := other.Difference(set)\n\treturn aDiff.Union(bDiff)\n}\n\n// Clear clears the entire set to be the empty set.\nfunc (set *NodeSet) Clear() {\n\t*set = make(NodeSet)\n}\n\n// Remove allows the removal of a single item in the set.\nfunc (set NodeSet) Remove(i *Node) {\n\tdelete(set, i)\n}\n\n// Cardinality returns how many items are currently in the set.\nfunc (set NodeSet) Cardinality() int {\n\treturn len(set)\n}\n\n// Iter returns a channel of type *Node that you can range over.\nfunc (set NodeSet) Iter() <-chan *Node {\n\tch := make(chan *Node)\n\tgo func() {\n\t\tfor elem := range set {\n\t\t\tch <- elem\n\t\t}\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// Equal determines if two sets are equal to each other.\n// If they both are the same size and have the same items they are considered equal.\n// Order of items is not relevant for sets to be equal.\nfunc (set NodeSet) Equal(other NodeSet) bool {\n\tif set.Cardinality() != other.Cardinality() {\n\t\treturn false\n\t}\n\tfor elem := range set {\n\t\tif !other.Contains(elem) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Clone returns a clone of the set.\n// Does NOT clone the underlying elements.\nfunc (set NodeSet) Clone() NodeSet {\n\tclonedSet := NewNodeSet()\n\tfor elem := range set {\n\t\tclonedSet.Add(elem)\n\t}\n\treturn clonedSet\n}\n"
        },
        {
          "name": "node_test.go",
          "type": "blob",
          "size": 4.240234375,
          "content": "package gorgonia\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestNodeBasics(t *testing.T) {\n\tvar n *Node\n\tvar c0, c1 *Node\n\tg := NewGraph()\n\n\t// withGraph\n\tn = newNode(In(g))\n\tif n.g == nil {\n\t\tt.Error(\"Expected *Node to be constructed with a graph\")\n\t}\n\treturnNode(n)\n\n\t// withType\n\tn = newNode(In(g), WithType(Float64))\n\tif !n.t.Eq(Float64) {\n\t\tt.Error(\"Expected *Node to be constructed with Float64\")\n\t}\n\treturnNode(n)\n\n\t// withOp\n\tn = newNode(In(g), WithOp(newEBOByType(addOpType, Float64, Float64)))\n\tif op, ok := n.op.(elemBinOp); ok {\n\t\tif op.binOpType() != addOpType {\n\t\t\tt.Error(\"expected addOpType\")\n\t\t}\n\t} else {\n\t\tt.Error(\"Expected *Node to be constructed with an addOp\")\n\t}\n\treturnNode(n)\n\n\t// withOp - statement op\n\tn = newNode(In(g), WithOp(letOp{}))\n\tif _, ok := n.op.(letOp); ok {\n\t\tif !n.isStmt {\n\t\t\tt.Errorf(\"Expected *Node.isStmt to be true when a statement op is passed in\")\n\t\t}\n\t} else {\n\t\tt.Error(\"Expected  *Node to be constructed with a letOp\")\n\t}\n\treturnNode(n)\n\n\t// WithName\n\tn = newNode(In(g), WithName(\"n\"))\n\tif n.name != \"n\" {\n\t\tt.Error(\"Expected *Node to be constructed with a name \\\"n\\\"\")\n\t}\n\treturnNode(n)\n\n\t// withChildren\n\tc0 = newNode(In(g), WithName(\"C0\"))\n\tc1 = newNode(In(g), WithName(\"C1\"))\n\tn = newNode(In(g), WithChildren(Nodes{c0, c1}))\n\tif len(n.children) == 2 {\n\t\tif !n.children.Contains(c0) || !n.children.Contains(c1) {\n\t\t\tt.Error(\"Expected *Node to contain those two children\")\n\t\t}\n\t} else {\n\t\tt.Error(\"Expected *Node to be constructed with 2 children\")\n\t}\n\tif !n.isRoot() {\n\t\tt.Error(\"n is supposed to be root\")\n\t}\n\n\treturnNode(n)\n\treturnNode(c0)\n\treturnNode(c1)\n\n\t// withChildren but they're constants\n\tc0 = NewConstant(3.14)\n\tn = newNode(In(g), WithChildren(Nodes{c0}))\n\tif len(n.children) != 1 {\n\t\tt.Error(\"Expected *Node to have 1 child\")\n\t}\n\treturnNode(n)\n\treturnNode(c0)\n\n\tn = newNode(In(g), WithValue(F64(3.14)), WithGrad(F64(1)))\n\tif _, ok := n.boundTo.(*dualValue); !ok {\n\t\tt.Error(\"Expected a dual Value\")\n\t}\n\treturnNode(n)\n\n\t// WithValue but no type\n\tn = newNode(In(g), WithValue(F64(3.14)))\n\tif !n.t.Eq(Float64) {\n\t\tt.Error(\"Expected a *Node to be constructed WithValue to get its type from the value if none exists\")\n\t}\n\tif !ValueEq(n.boundTo, NewF64(3.14)) {\n\t\tt.Error(\"Expected *Node to be bound to the correct value. Something has gone really wrong here\")\n\t}\n\treturnNode(n)\n\n\t// WithValue but with existing type that is the same\n\tn = newNode(In(g), WithType(Float64), WithValue(F64(3.14)))\n\tif !ValueEq(n.boundTo, NewF64(3.14)) {\n\t\tt.Error(\"Expected *Node to be bound to the correct value. Something has gone really wrong here\")\n\t}\n\treturnNode(n)\n\n\t// This is acceptable and should not panic\n\tn = newNode(In(g), WithType(makeTensorType(1, Float64)), WithShape(2, 1))\n\treturnNode(n)\n\n\t// Returns itsef\n\tn = newNode(In(g), WithType(makeTensorType(2, Float32)), WithShape(2, 3))\n\tm := n.Node()\n\tif n != m {\n\t\tt.Error(\"Expected n.Node() to return itself, pointers and all\")\n\t}\n\tns := n.Nodes()\n\tif len(ns) != 1 {\n\t\tt.Errorf(\"Expected Nodes() to return a slice of length 1. Got %v\", ns)\n\t}\n\tif ns[0] != n {\n\t\tt.Error(\"Expected first slice to be itself.\")\n\t}\n\tm = nil\n\treturnNode(n)\n\n\t// bad stuff\n\tvar f func()\n\n\t// no graph\n\tf = func() {\n\t\tn = newNode(WithType(Float64))\n\t}\n\tassert.Panics(t, f)\n\n\t// conflicting types, types were set first\n\tf = func() {\n\t\tn = newNode(In(g), WithType(Float32), WithValue(F64(1)))\n\t}\n\tassert.Panics(t, f)\n\n\t// type mismatch - values were set first\n\tf = func() {\n\t\tn = newNode(In(g), WithValue(F64(1)), WithType(Float32))\n\t}\n\tassert.Panics(t, f)\n\n\t// shape type mismatch\n\tf = func() {\n\t\tn = newNode(In(g), WithType(makeTensorType(1, Float64)), WithShape(2, 2))\n\t}\n\tassert.Panics(t, f)\n\n\t// bad grads\n\tf = func() {\n\t\tn = newNode(WithGrad(F64(3.14)))\n\t}\n\tassert.Panics(t, f)\n}\n\nfunc TestNewUniqueNodes(t *testing.T) {\n\tvar n *Node\n\tvar c0, c1 *Node\n\tg := NewGraph()\n\n\t// withChildren but they're constants\n\tc0 = NewConstant(3.14)\n\tc1 = newNode(In(g), WithValue(5.0))\n\tn = NewUniqueNode(In(g), WithChildren(Nodes{c0, c1}))\n\tif n.children[0].g == nil {\n\t\tt.Error(\"Expected a cloned constant child to have graph g\")\n\t}\n\n\treturnNode(n)\n}\n\nfunc TestCloneTo(t *testing.T) {\n\tg := NewGraph()\n\tg2 := NewGraph()\n\n\tn := NewUniqueNode(WithName(\"n\"), WithType(Float64), In(g))\n\tn.CloneTo(g2)\n\n\tassert.True(t, nodeEq(g2.AllNodes()[0], n))\n}\n"
        },
        {
          "name": "noextern.go",
          "type": "blob",
          "size": 4.0146484375,
          "content": "// +build !cuda\n\npackage gorgonia\n\nimport \"gorgonia.org/tensor\"\n\n// CUDA indicates if this build is using CUDA\nconst CUDA = false\n\nvar _ tensor.Engine = ExternMetadata{}\n\n// ExternMetadata is used to hold metadata about external execution devices.\n// In this build, it's an empty struct because the default build doesn't use external devices to execute the graph on\ntype ExternMetadata struct {\n\ttensor.Engine\n\tb             batchedBLAS\n\tworkAvailable chan bool\n\tsyncChan      chan struct{}\n}\n\nfunc (m *ExternMetadata) init() error {\n\tm.syncChan = make(chan struct{})\n\tif m.b != nil {\n\t\tm.workAvailable = make(chan bool)\n\t\tgo m.collectBLASWork()\n\t}\n\treturn nil\n}\n\n// initFail is a no-op\nfunc (m *ExternMetadata) initFail() {}\n\n// HasFunc will always return false in this build\nfunc (m ExternMetadata) HasFunc(name string) bool { return false }\n\n// WorkAvailable returns a channel of empty struct, which is used to signal to the VM when there is work available. The VM will then call the DoWork method.\nfunc (m *ExternMetadata) WorkAvailable() <-chan bool {\n\tif m.b != nil {\n\t\treturn m.workAvailable\n\t}\n\n\treturn nil\n}\n\n// Sync returns the sync channel\nfunc (m *ExternMetadata) Sync() chan struct{} { return m.syncChan }\n\n// DoWork flushes any batched cgo calls. In this build it only flushes the batched BLAS calls.\nfunc (m *ExternMetadata) DoWork() error {\n\tif m.b != nil {\n\t\tm.b.DoWork()\n\t}\n\treturn nil\n}\n\n// Get allocates a memory of the size. In this build it returns a NoOpError.\nfunc (m *ExternMetadata) Get(dev Device, size int64) (tensor.Memory, error) { return nil, noopError{} }\n\n// GetFromValue allocates a memory of the size of v. In this build it returns a NoOpError, and v itself\nfunc (m *ExternMetadata) GetFromValue(dev Device, v Value) (tensor.Memory, error) {\n\treturn v, noopError{}\n}\n\n// Put puts a previously allocated memory slab of the provided size back into the pool. Currently this is a No-op in this build.\nfunc (m *ExternMetadata) Put(dev Device, mem tensor.Memory, size int64) {}\n\n// PutValue puts a previously allocated value into the pool. In this build,  it is a noop.\nfunc (m *ExternMetadata) PutValue(dev Device, v Value) {}\n\n// Transfer transfers a value from device to device. In this build, it's a noop, returning the input value, and a nil error\nfunc (m *ExternMetadata) Transfer(toDev, fromDev Device, v Value, synchronous bool) (retVal Value, err error) {\n\treturn v, nil\n}\n\n// Reset is a noop function for compatibility with the Cuda build\nfunc (m *ExternMetadata) Reset() {}\n\n// Cleanup cleans up the ancillary allocations made during the calling of batched external device function.\n//\n// The reason for this method is due to the fact that there is currently no way to free memory while the context is still running without causing\n// some weirdness to the CUDA calls.\n//\n// This is a No-op in this build\nfunc (m *ExternMetadata) Cleanup() {}\n\n// Signal sends a signal down the workavailable channel, telling the VM to call the DoWork method. Signal is a synchronous method\nfunc (m *ExternMetadata) Signal() {\n\tm.signal()\n\tif m.workAvailable != nil {\n\t\t<-m.syncChan\n\t}\n}\n\n// collectBLASWork is a muxer for CBLAS/CuBLAS (if any) and the devices\nfunc (m *ExternMetadata) collectBLASWork() {\n\tif m.b != nil {\n\t\tfor range m.b.WorkAvailable() {\n\t\t\tm.workAvailable <- false\n\t\t}\n\t}\n}\n\nfunc (m *ExternMetadata) signal() {\n\tif m.workAvailable != nil {\n\t\tm.workAvailable <- true\n\t}\n}\n\nfunc (m *ExternMetadata) setEngine(e tensor.Engine) { m.Engine = e }\n\n// ValueOnDevice gets the value of the node as a Value but on the desired device. In this build the device is always CPU, so it's equivalent to calling .Value()\nfunc (n *Node) ValueOnDevice(dev Device, extern External) (retVal Value, allocOnExtern bool, err error) {\n\treturn n.Value(), false, nil\n}\n\n// GradOnDevice gets the gradient value of the node as a Value but on the desired device. In this build the device is always CPU, so it's equivalent to calling .Grad()\nfunc (n *Node) GradOnDevice(dev Device, extern External) (retVal Value, allocOnExtern bool, err error) {\n\tretVal, err = n.Grad()\n\treturn retVal, false, err\n}\n"
        },
        {
          "name": "noextern_test.go",
          "type": "blob",
          "size": 0.5361328125,
          "content": "// +build !cuda\n\npackage gorgonia\n\nimport (\n\t\"runtime\"\n\t\"testing\"\n\n\t\"gorgonia.org/tensor\"\n)\n\nfunc BenchmarkOneMil(b *testing.B) {\n\txT := tensor.New(tensor.WithShape(1000000), tensor.WithBacking(tensor.Random(tensor.Float32, 1000000)))\n\tg := NewGraph()\n\tx := NewVector(g, Float32, WithShape(1000000), WithName(\"x\"), WithValue(xT))\n\tMust(Sigmoid(x))\n\n\tm := NewTapeMachine(g)\n\tdefer m.Close()\n\tfor n := 0; n < b.N; n++ {\n\t\tif err := m.RunAll(); err != nil {\n\t\t\tb.Fatalf(\"Failed at n: %d. Error: %v\", n, err)\n\t\t\tbreak\n\t\t}\n\t\tm.Reset()\n\t}\n\truntime.GC()\n}\n"
        },
        {
          "name": "op.go",
          "type": "blob",
          "size": 11.0712890625,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"hash\"\n\t\"hash/fnv\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\n// DimSizer is any type (typically a tensor.Shape) that allows querying for a dimension size given an input dimension.\ntype DimSizer interface {\n\tDimSize(int) (int, error)\n}\n\n// ShapesToDimSizers is a convenience function to convert a slice of tensor.Shape to a slice of DimSizer\nfunc ShapesToDimSizers(shapes []tensor.Shape) []DimSizer {\n\tretVal := make([]DimSizer, len(shapes))\n\tfor i, s := range shapes {\n\t\tretVal[i] = s\n\t}\n\treturn retVal\n}\n\n// DimSizersToShapes is a convenience function to convert a slice of DimSizer to a slice of tensor.Shape. It will return an error if any of them isn't a tensor.Shape\nfunc DimSizersToShapes(ds []DimSizer) ([]tensor.Shape, error) {\n\tretVal := make([]tensor.Shape, len(ds))\n\tvar ok bool\n\tfor i, d := range ds {\n\t\tif retVal[i], ok = d.(tensor.Shape); !ok {\n\t\t\treturn nil, errors.Errorf(\"Dimsizer %d is not a Shape.\", i)\n\t\t}\n\t}\n\treturn retVal, nil\n}\n\n// An Op is a symbolic representation of an operation\n// Think of them as functions, taking an input (or multiple), and outputting something\n//\n// All Ops have type signatures that look like this:\n//\t\tOpName :: (Floats a) ⇒ Tensor a → Tensor a → Tensor a\ntype Op interface {\n\t/* Graph Building Related Methods */\n\n\t// Arity returns the number of inputs the Op expects. -1 indicates that it's n-ary and will be determined at runtime\n\tArity() int\n\n\t// Informs the type of the Op (not the node). This will be used by the type system to infer the final type of the node\n\tType() hm.Type\n\n\t// returns the output shape as a function of the inputs\n\tInferShape(...DimSizer) (tensor.Shape, error)\n\n\t/* Machine related */\n\n\t// executes the op\n\tDo(...Value) (Value, error)\n\n\t/* Analysis Related Methods */\n\n\t// indicates if the Op will return a pointer (allowing possible inplace edits) or by value\n\t// if it's false, the return value of the Op will be a copy of its input\n\tReturnsPtr() bool\n\n\t// Does this op potentially call external (cgo or cuda) functions (thereby requiring extra overhead for Go's trampolining thing)\n\tCallsExtern() bool\n\n\t// overwriteInput() is a method which states which input the output will be overwriting.\n\t// This allows for some efficiency gains as the underlying arrays wouldn't have to be re-allocated.\n\t// The method returns an int instead of a bool because potentially different operations may be allowed\n\t// to overwrite certain inputs. For example, consider an operation to increment a value:\n\t// the IncrementOp would be a unary operator, and assuming we would like to overwrite the input,\n\t// the retVal of overwriteInput() will be 0 (inputs[0]).\n\t// -1 is returned if overwriting of input is disallowed\n\tOverwritesInput() int\n\n\t/* Other methods */\n\tWriteHash(h hash.Hash)\n\tHashcode() uint32\n\tfmt.Stringer\n}\n\n// A UnaryOp is an Op that takes only one input\ntype UnaryOp interface {\n\tOp\n\n\tIsUnary() bool\n}\n\n// A BinaryOp is an Op that takes only two inputs\ntype BinaryOp interface {\n\tOp\n\n\tIsBinary() bool\n}\n\n// A NoRetOp is an Op that reads a value, but does not return any value. It's a representation of a not-pure function\ntype NoRetOp interface {\n\tOp\n\n\tReturnsNothing() bool\n}\n\n// An ADOp is an Op that supports automatic differentiation.\ntype ADOp interface {\n\tOp\n\n\tDoDiff(ctx ExecutionContext, inputs Nodes, output *Node) error\n}\n\n// A SDOp is an Op that supports symbolic differentiation\ntype SDOp interface {\n\tOp\n\n\t// DiffWRT indicates if the op is differentiable with regards to the given number of inputs\n\t// returns []bool to indicate which input it is differentiable to\n\tDiffWRT(inputs int) []bool\n\n\t// SymDiff symbolically differentiates the op\n\tSymDiff(inputs Nodes, output, grad *Node) (retVal Nodes, err error)\n}\n\n// ReductionOp changes the shape of the node\ntype ReductionOp interface {\n\tOp\n\n\tIsReduction() bool\n}\n\n// A TrainModeOp is an Op that supports modes to enable/disable training\ntype TrainModeOp interface {\n\tSetTraining(isTraining bool) error\n}\n\n// IncrDoer increments the toIncr with the result of doing\ntype IncrDoer interface {\n\tIncrDo(toIncr Value, inputs ...Value) error\n}\n\n// UsePreallocDoer is an op that works when a preallocated value is provided\ntype UsePreallocDoer interface {\n\tUsePreallocDo(prealloc Value, inputs ...Value) (Value, error)\n}\n\n// UnsafeDoer is an op that will overwrite the underlying value.\ntype UnsafeDoer interface {\n\tUnsafeDo(inputs ...Value) (Value, error)\n}\n\n// CUDADoer uses CUDA to perform the Op.\ntype CUDADoer interface {\n\tCUDADo(extern External, dev Device, prealloc Value, inputs ...Value) (retVal Value, err error)\n}\n\n// CLDoer uses OpenCL to perform the Op. As of now, there are NO Ops that support OpenCL\ntype CLDoer interface {\n\tCLDo(inputs ...Value) (Value, error)\n}\n\n// A CUDAADOp operation have a specific method to run with CUDA\ntype CUDAADOp interface {\n\tADOp\n\tCUDADoDiff(extern External, dev Device, inputs Nodes, output *Node) error\n}\n\n// ApplyOp is the generic function application - for when no specialization is required\nfunc ApplyOp(op Op, children ...*Node) (retVal *Node, err error) {\n\tvar g *ExprGraph\n\n\tfor _, child := range children {\n\t\tif child.g != nil {\n\t\t\tg = child.g\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif g == nil {\n\t\treturn nil, errors.New(\"No Graph Supplied\")\n\t}\n\n\tif !Nodes(children).AllSameGraph() {\n\t\treturn nil, errors.New(\"Not all children have the same graph\")\n\t}\n\n\t// typecheck  before creating\n\ttypeSysLogf(\"Inferring node type of %v :: %v with children: %#Y\", op, op.Type(), Nodes(children))\n\tenterLogScope()\n\tdefer leaveLogScope()\n\tvar retType hm.Type\n\tif retType, err = inferNodeType(op, children...); err != nil {\n\t\treturn nil, errors.Wrapf(err, \"Type inference error. Op: %v. Children: %#Y, OpType:%v\", op, Nodes(children), op.Type())\n\t}\n\ttypeSysLogf(\"Done inferring. Return type is: %#v(%T)\", retType, retType)\n\n\t// infer shapes, but print errors instead of returning\n\tshapeLogf(\"op: %v(%T) inferring shape\", op, op)\n\tif err = checkArity(op, len(children)); err != nil {\n\t\treturn\n\t}\n\n\tds := Nodes(children).dimSizers()\n\tvar s tensor.Shape\n\tif s, err = op.InferShape(ds...); err == nil {\n\t\tshapeLogf(\"inferred shape %v\", s)\n\t\tretVal = NewUniqueNode(WithType(retType), WithOp(op), WithChildren(children), In(g), WithShape(s...))\n\t} else {\n\t\terr = errors.Wrapf(err, \"Failed to infer shape. Op: %v\", op)\n\t\t// retVal = newUniqueNode(withType(retType), withOp(op), withChildren(children), withGraph(g))\n\t}\n\treturnDimSizers(ds)\n\treturn\n}\n\n// ApplyOpWithName applies the op, and then gives the node the given name\nfunc ApplyOpWithName(op Op, name string, children ...*Node) (retVal *Node, err error) {\n\tif retVal, err = ApplyOp(op, children...); err == nil {\n\t\tWithName(name)(retVal)\n\t} else {\n\t\treturn nil, errors.Wrap(err, applyOpFail)\n\t}\n\treturn\n}\n\n// a constant is an unchanging value. I think everyone would know what a constant is\n// a constant op is an op that creates a constant. It is also a Value of a constant value\ntype constant interface {\n\tOp\n\n\tisconstant() bool\n\tValue() Value\n}\n\ntype constantScalar struct {\n\tv Scalar\n}\n\nfunc (c constantScalar) Arity() int                                   { return 0 }\nfunc (c constantScalar) Type() hm.Type                                { return TypeOf(c.v) }\nfunc (c constantScalar) InferShape(...DimSizer) (tensor.Shape, error) { return scalarShape, nil }\nfunc (c constantScalar) ReturnsPtr() bool                             { return false }\nfunc (c constantScalar) CallsExtern() bool                            { return false }\nfunc (c constantScalar) OverwritesInput() int                         { return -1 }\nfunc (c constantScalar) DiffWRT(i int) []bool                         { return nil }\nfunc (c constantScalar) SymDiff(Nodes, *Node, *Node) (Nodes, error)   { return nil, nil }\n\nfunc (c constantScalar) Do(...Value) (Value, error) { return c.v, nil }\nfunc (c constantScalar) String() string             { return fmt.Sprintf(\"const %s\", c.v) }\n\nfunc (c constantScalar) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, \"const %v: %v\", TypeOf(c.v), c.v)\n}\n\nfunc (c constantScalar) Hashcode() uint32 {\n\th := fnv.New32a()\n\tc.WriteHash(h)\n\treturn h.Sum32()\n}\n\nfunc (c constantScalar) isconstant() bool { return true }\nfunc (c constantScalar) Value() Value     { return c.v }\n\ntype constantTensor struct {\n\tv tensor.Tensor\n}\n\nfunc (c constantTensor) Arity() int                                   { return 1 }\nfunc (c constantTensor) Type() hm.Type                                { return TypeOf(c.v) }\nfunc (c constantTensor) InferShape(...DimSizer) (tensor.Shape, error) { return c.v.Shape(), nil }\n\n// danger! The only reason why this is the case is because matrices may be too large. copying is costly.\n// constants should return value but for the sake of memory, we're going to return pointers\nfunc (c constantTensor) ReturnsPtr() bool                           { return true }\nfunc (c constantTensor) OverwritesInput() int                       { return -1 }\nfunc (c constantTensor) CallsExtern() bool                          { return false }\nfunc (c constantTensor) DiffWRT(i int) []bool                       { return nil }\nfunc (c constantTensor) SymDiff(Nodes, *Node, *Node) (Nodes, error) { return nil, nil }\nfunc (c constantTensor) Do(...Value) (Value, error)                 { return c.v, nil }\nfunc (c constantTensor) String() string                             { return fmt.Sprintf(\"const %s\", TypeOf(c.v)) }\n\nfunc (c constantTensor) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, \"const %v:%v\", c.Type(), c.v)\n}\n\nfunc (c constantTensor) Hashcode() uint32 {\n\th := fnv.New32a()\n\tc.WriteHash(h)\n\treturn h.Sum32()\n}\n\nfunc (c constantTensor) isconstant() bool { return true }\nfunc (c constantTensor) Value() Value     { return c.v }\n\ntype Iop struct{}\n\n// Arity returns 1\nfunc (i Iop) Arity() int { return 1 }\n\n// Type returns a → a\nfunc (i Iop) Type() hm.Type { return hm.NewFnType(hm.TypeVariable('a'), hm.TypeVariable('a')) }\n\n// InferShape returns the output shape as a function of the inputs\nfunc (i Iop) InferShape(ds ...DimSizer) (tensor.Shape, error) { return ds[0].(tensor.Shape), nil }\n\n// Do executes the op\nfunc (i Iop) Do(vs ...Value) (Value, error) { return vs[0], nil }\n\n// ReturnsPtr indicates if the Op will return a pointer (allowing possible inplace edits) or by value\n// if it's false, the return value of the Op will be a copy of its input\nfunc (i Iop) ReturnsPtr() bool { return true }\n\n// CallsExtern returns false.\nfunc (i Iop) CallsExtern() bool { return false }\n\n// OverwritesInput is a method which states which input the output will be overwriting.\n// This allows for some efficiency gains as the underlying arrays wouldn't have to be re-allocated.\n// The method returns an int instead of a bool because potentially different operations may be allowed\n// to overwrite certain inputs. For example, consider an operation to increment a value:\n// the IncrementOp would be a unary operator, and assuming we would like to overwrite the input,\n// the retVal of overwriteInput() will be 0 (inputs[0]).\n// -1 is returned if overwriting of input is disallowed\nfunc (i Iop) OverwritesInput() int { return -1 }\n\n/* Other methods */\n\nfunc (i Iop) WriteHash(h hash.Hash) { fmt.Fprintf(h, \"I\") }\n\nfunc (i Iop) Hashcode() uint32 {\n\th := fnv.New32a()\n\ti.WriteHash(h)\n\treturn h.Sum32()\n}\n\nfunc (i Iop) String() string { return \"I\" }\n"
        },
        {
          "name": "op_avg_pool.go",
          "type": "blob",
          "size": 15.5302734375,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"hash\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/gorgonia/internal/encoding\"\n\t\"gorgonia.org/tensor\"\n)\n\n// AveragePool1D applies a average pool on the node x.\nfunc AveragePool1D(x *Node, kernel, pad, stride int) (*Node, error) {\n\treturn MaxPool2D(x, tensor.Shape{1, kernel}, []int{0, pad}, []int{1, stride})\n}\n\n// AveragePool2D applies the average operation to the given input\nfunc AveragePool2D(x *Node, kernel tensor.Shape, pad, stride []int) (*Node, error) {\n\tgroup := encoding.NewGroup(\"Maxpool\")\n\txShape := x.Shape()\n\n\t// check shape\n\tif xShape.Dims() != 4 {\n\t\treturn nil, errors.Errorf(\"Expected input to have a shape with dimension 4\")\n\t}\n\n\tif kernel.Dims() != 2 {\n\t\treturn nil, errors.Errorf(\"Expected kernel to have a shape of dimension 2\")\n\t}\n\n\t// checks\n\tfor _, s := range stride {\n\t\tif s <= 0 {\n\t\t\treturn nil, errors.Errorf(\"Cannot use strides of less than or equal 0: %v\", stride)\n\t\t}\n\t}\n\n\tfor _, p := range pad {\n\t\tif p < 0 {\n\t\t\treturn nil, errors.Errorf(\"Cannot use padding of less than 0: %v\", pad)\n\t\t}\n\t}\n\n\th, w := xShape[2], xShape[3]\n\tkh, kw := kernel[0], kernel[1]\n\n\tpadNorth := pad[0]\n\tpadWest := pad[1]\n\tpadSouth := pad[0]\n\tpadEast := pad[1]\n\tif len(pad) == 4 {\n\t\tpadNorth = pad[0]\n\t\tpadSouth = pad[1]\n\t\tpadWest = pad[2]\n\t\tpadEast = pad[3]\n\t}\n\n\tif h-kh+padNorth+padSouth < 0 {\n\t\t// error\n\t\treturn nil, errors.New(\"Impossible height/kernel/pad combination\")\n\t}\n\n\tif w-kw+padWest+padEast < 0 {\n\t\t// error\n\t\treturn nil, errors.New(\"Impossible width/kernel/pad combination\")\n\t}\n\n\top := newAvgPoolOp(xShape, kernel, pad, stride)\n\tretVal, err := ApplyOp(op, x)\n\tretVal.groups = retVal.groups.Upsert(group)\n\n\treturn retVal, err\n}\n\ntype avgPoolOp struct {\n\t// Shape of Input\n\tunpaddedB int\n\tunpaddedC int\n\tunpaddedH int\n\tunpaddedW int\n\n\th, w              int // patch height and width\n\tpadNorth, padWest int\n\tpadSouth, padEast int\n\texplicitPadding   bool\n\tstrideH, strideW  int\n\n\t// execution state\n\t// the mask is only filled at execution time\n\tmask tensor.Tensor\n}\n\nfunc newAvgPoolOp(inputShape, kernel tensor.Shape, pad, stride []int) *avgPoolOp {\n\tpadNorth := pad[0]\n\tpadWest := pad[1]\n\tpadSouth := pad[0]\n\tpadEast := pad[1]\n\texplicitPadding := false\n\n\tif len(pad) == 4 {\n\t\texplicitPadding = true\n\t\tpadNorth = pad[0]\n\t\tpadSouth = pad[1]\n\t\tpadWest = pad[2]\n\t\tpadEast = pad[3]\n\t}\n\n\tavgPoolOp := &avgPoolOp{\n\t\tunpaddedB: inputShape[0],\n\t\tunpaddedC: inputShape[1],\n\t\tunpaddedH: inputShape[2],\n\t\tunpaddedW: inputShape[3],\n\n\t\th:               kernel[0],\n\t\tw:               kernel[1],\n\t\tpadNorth:        padNorth,\n\t\tpadWest:         padWest,\n\t\tpadSouth:        padSouth,\n\t\tpadEast:         padEast,\n\t\texplicitPadding: explicitPadding,\n\t\tstrideH:         stride[0],\n\t\tstrideW:         stride[1],\n\t}\n\n\tavgPoolOp.mask = tensor.New(tensor.Of(tensor.Int), tensor.WithShape(avgPoolOp.calcShape(inputShape)...))\n\n\treturn avgPoolOp\n}\n\nfunc (op *avgPoolOp) Arity() int { return 1 }\n\n// avgPoolOp has this type:\n// \t\top :: (...) → (...)\nfunc (op *avgPoolOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\tt := newTensorType(4, a)\n\treturn hm.NewFnType(t, t)\n}\nfunc (op *avgPoolOp) InferShape(inputs ...DimSizer) (tensor.Shape, error) {\n\tif s, ok := inputs[0].(tensor.Shape); ok {\n\t\treturn op.calcShape(s), nil\n\t}\n\treturn nil, errors.Errorf(\"Expected a shape\")\n}\n\nfunc (op *avgPoolOp) Do(inputs ...Value) (retVal Value, err error) {\n\tvar in, out tensor.Tensor\n\tif in, err = op.checkInput(inputs...); err != nil {\n\t\treturn nil, err\n\t}\n\tinShp := in.Shape()\n\tout = tensor.New(tensor.Of(in.Dtype()), tensor.WithShape(op.calcShape(inShp)...), tensor.WithEngine(in.Engine()))\n\top.do(out, in)\n\treturn out, nil\n}\n\nfunc (op *avgPoolOp) ReturnsPtr() bool     { return false }\nfunc (op *avgPoolOp) CallsExtern() bool    { return false }\nfunc (op *avgPoolOp) OverwritesInput() int { return -1 }\nfunc (op *avgPoolOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, \"AvgPool{%d, %d, %d, %d}(kernel: (%d, %d), pad: (%d, %d), stride: (%d, %d))\",\n\t\top.unpaddedB, op.unpaddedC, op.unpaddedH, op.unpaddedW,\n\t\top.h, op.w, op.padNorth, op.padWest, op.strideH, op.strideW)\n}\n\nfunc (op *avgPoolOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op *avgPoolOp) String() string {\n\treturn fmt.Sprintf(\"AvgPool{%d, %d, %d, %d}(kernel: (%d, %d), pad: (%d, %d), stride: (%d, %d))\",\n\t\top.unpaddedB, op.unpaddedC, op.unpaddedH, op.unpaddedW,\n\t\top.h, op.w, op.padNorth, op.padWest, op.strideH, op.strideW)\n}\n\nfunc (op *avgPoolOp) UsePreallocDo(prealloc Value, inputs ...Value) (Value, error) {\n\tvar in tensor.Tensor\n\tvar err error\n\tif in, err = op.checkInput(inputs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif p, ok := prealloc.(tensor.Tensor); ok {\n\t\top.do(p, in)\n\t\treturn p, nil\n\t}\n\treturn nil, errors.Errorf(\"Expected prealloc to be a tensor\")\n}\n\nfunc (op *avgPoolOp) DiffWRT(inputs int) []bool { return []bool{true} }\n\nfunc (op *avgPoolOp) SymDiff(inputs Nodes, output, grad *Node) (retVal Nodes, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\tinput := inputs[0]\n\n\tvar op2 avgPoolOp\n\top2 = *op\n\tdiff := &avgPoolDiffOp{op2}\n\n\tvar ret *Node\n\tif ret, err = ApplyOp(diff, input, output, grad); err != nil {\n\t\treturn nil, err\n\t}\n\treturn Nodes{ret}, nil\n}\n\nfunc (op *avgPoolOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) (err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\tinput := inputs[0]\n\tinputDV, outDV := getDV(input, output)\n\n\tvar op2 avgPoolOp\n\top2 = *op\n\tdiff := &avgPoolDiffOp{op2}\n\n\tif _, err = diff.UsePreallocDo(inputDV.d, inputDV.Value, outDV.Value, outDV.d); err != nil {\n\t\treturn errors.Wrapf(err, doFail, diff)\n\t}\n\treturn\n}\n\nfunc (op *avgPoolOp) checkInput(inputs ...Value) (tensor.Tensor, error) {\n\tif err := checkArity(op, len(inputs)); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar in tensor.Tensor\n\tvar ok bool\n\tif in, ok = inputs[0].(tensor.Tensor); !ok {\n\t\treturn nil, errors.Errorf(\"Expected input to be a tensor\")\n\t}\n\n\tif in.Shape().Dims() != 4 {\n\t\treturn nil, errors.Errorf(\"Expected input to have 4 dimensions\")\n\t}\n\treturn in, nil\n}\n\n// calcShape calculates the output shape given an input shape\nfunc (op *avgPoolOp) calcShape(s tensor.Shape) tensor.Shape {\n\tb, c, h, w := s[0], s[1], s[2], s[3]\n\n\tpooledH := (h+op.padSouth+op.padNorth-(op.h-1)-1)/op.strideH + 1\n\tpooledW := (w+op.padEast+op.padWest-(op.w-1)-1)/op.strideW + 1\n\treturn tensor.Shape{b, c, pooledH, pooledW}\n}\n\n// do prepares the data, and then dispatches it to the correct (computation) kernel.\n// out is the preallocated tensor\nfunc (op *avgPoolOp) do(out, in tensor.Tensor) {\n\toutShape := out.Shape()\n\toutStride := out.Strides()[1]\n\tinShape := in.Shape()\n\tinStride := in.Strides()[1]\n\tmaskStride := op.mask.Strides()[1]\n\n\tb, c, h, w := outShape[0], outShape[1], outShape[2], outShape[3]\n\tinH, inW := inShape[2], inShape[3]\n\n\tif op.mask == nil {\n\t\top.mask = tensor.New(tensor.Of(tensor.Int), tensor.WithShape(op.calcShape(inShape)...))\n\t}\n\n\tmaskData := op.mask.Data().([]int)\n\n\tswitch in.Dtype() {\n\tcase tensor.Float64:\n\t\top.f64s(b, c, h, w, inH, inW,\n\t\t\toutStride, inStride, maskStride,\n\t\t\tout.Data().([]float64), in.Data().([]float64),\n\t\t\tmaskData)\n\tcase tensor.Float32:\n\t\top.f32s(b, c, h, w, inH, inW,\n\t\t\toutStride, inStride, maskStride,\n\t\t\tout.Data().([]float32), in.Data().([]float32),\n\t\t\tmaskData)\n\t}\n}\n\nfunc (op *avgPoolOp) f32s(batches, channels, outH, outW, inH, inW,\n\toutStride, inStride, maskStride int,\n\toutData, inData []float32,\n\tmaskData []int) {\n\n\t// set values\n\tfor i := range outData {\n\t\toutData[i] = 0\n\t\tmaskData[i] = -1\n\t}\n\n\tpadH := op.padNorth\n\tpadW := op.padWest\n\tif op.explicitPadding {\n\t\tpadH = op.padSouth\n\t\tpadW = op.padEast\n\t}\n\n\tfor b := 0; b < batches; b++ {\n\t\tfor c := 0; c < channels; c++ {\n\t\t\tfor ph := 0; ph < outH; ph++ {\n\t\t\t\tfor pw := 0; pw < outW; pw++ {\n\t\t\t\t\thStart := ph*op.strideH - padH\n\t\t\t\t\twStart := pw*op.strideW - padW\n\t\t\t\t\thEnd := minInt(hStart+op.h, inH)\n\t\t\t\t\twEnd := minInt(wStart+op.w, inW)\n\t\t\t\t\thStart = maxInt(hStart, 0)\n\t\t\t\t\twStart = maxInt(wStart, 0)\n\n\t\t\t\t\tpoolIndex := ph*outW + pw\n\n\t\t\t\t\tfor hi := hStart; hi < hEnd; hi++ {\n\t\t\t\t\t\tfor wi := wStart; wi < wEnd; wi++ {\n\t\t\t\t\t\t\ti := hi*inW + wi\n\t\t\t\t\t\t\toutData[poolIndex] += inData[i]\n\t\t\t\t\t\t\tmaskData[poolIndex] = i\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\toutData[poolIndex] /= float32(inW)\n\t\t\t\t}\n\t\t\t}\n\t\t\t// skip by strides\n\t\t\tinData = inData[inStride:]\n\t\t\toutData = outData[outStride:]\n\t\t\tmaskData = maskData[maskStride:]\n\t\t}\n\t}\n}\n\nfunc (op *avgPoolOp) f64s(batches, channels, outH, outW, inH, inW,\n\toutStride, inStride, maskStride int,\n\toutData, inData []float64,\n\tmaskData []int) {\n\n\t// set values\n\tfor i := range outData {\n\t\toutData[i] = 0\n\t\tmaskData[i] = -1\n\t}\n\n\tpadH := op.padNorth\n\tpadW := op.padWest\n\tif op.explicitPadding {\n\t\tpadH = op.padSouth\n\t\tpadW = op.padEast\n\t}\n\n\tfor b := 0; b < batches; b++ {\n\t\tfor c := 0; c < channels; c++ {\n\t\t\tfor ph := 0; ph < outH; ph++ {\n\t\t\t\tfor pw := 0; pw < outW; pw++ {\n\t\t\t\t\thStart := ph*op.strideH - padH\n\t\t\t\t\twStart := pw*op.strideW - padW\n\t\t\t\t\thEnd := minInt(hStart+op.h, inH)\n\t\t\t\t\twEnd := minInt(wStart+op.w, inW)\n\t\t\t\t\thStart = maxInt(hStart, 0)\n\t\t\t\t\twStart = maxInt(wStart, 0)\n\n\t\t\t\t\tpoolIndex := ph*outW + pw\n\n\t\t\t\t\tfor hi := hStart; hi < hEnd; hi++ {\n\t\t\t\t\t\tfor wi := wStart; wi < wEnd; wi++ {\n\t\t\t\t\t\t\ti := hi*inW + wi\n\n\t\t\t\t\t\t\toutData[poolIndex] += inData[i]\n\t\t\t\t\t\t\tmaskData[poolIndex] = i\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\toutData[poolIndex] /= float64(inW)\n\t\t\t\t}\n\t\t\t}\n\t\t\t// skip by strides\n\t\t\tinData = inData[inStride:]\n\t\t\toutData = outData[outStride:]\n\t\t\tmaskData = maskData[maskStride:]\n\t\t}\n\t}\n}\n\ntype avgPoolDiffOp struct {\n\tavgPoolOp\n}\n\nfunc (op *avgPoolDiffOp) Arity() int { return 3 }\nfunc (op *avgPoolDiffOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\tt := newTensorType(4, a)\n\treturn hm.NewFnType(t, t, t, t)\n}\n\nfunc (op *avgPoolDiffOp) InferShape(inputs ...DimSizer) (tensor.Shape, error) {\n\ts := inputs[0].(tensor.Shape).Clone()\n\treturn s, nil\n}\n\nfunc (op *avgPoolDiffOp) Do(inputs ...Value) (Value, error) {\n\tvar in, out, pooled, pooledGrad tensor.Tensor\n\tvar err error\n\tif in, pooled, pooledGrad, err = op.checkInput(inputs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// out is the gradient of in\n\tout = tensor.New(tensor.Of(in.Dtype()), tensor.WithShape(in.Shape().Clone()...), tensor.WithEngine(in.Engine()))\n\top.do(out, in, pooled, pooledGrad)\n\n\treturn out, nil\n}\n\nfunc (op *avgPoolDiffOp) ReturnsPtr() bool     { return true }\nfunc (op *avgPoolDiffOp) CallsExtern() bool    { return false }\nfunc (op *avgPoolDiffOp) OverwritesInput() int { return -1 }\nfunc (op *avgPoolDiffOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, \"AvgPoolDiff{%d, %d, %d, %d}(kernel: (%d, %d), pad: (%d, %d), stride: (%d, %d))\",\n\t\top.unpaddedB, op.unpaddedC, op.unpaddedH, op.unpaddedW,\n\t\top.h, op.w, op.padNorth, op.padWest, op.strideH, op.strideW)\n}\n\nfunc (op *avgPoolDiffOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op *avgPoolDiffOp) String() string {\n\treturn fmt.Sprintf(\"AvgPoolDiff{%d, %d, %d, %d}(kernel: (%d, %d), pad: (%d, %d), stride: (%d, %d))\",\n\t\top.unpaddedB, op.unpaddedC, op.unpaddedH, op.unpaddedW,\n\t\top.h, op.w, op.padNorth, op.padWest, op.strideH, op.strideW)\n}\n\nfunc (op *avgPoolDiffOp) UsePreallocDo(prealloc Value, inputs ...Value) (Value, error) {\n\tvar in, pooled, pooledGrad tensor.Tensor\n\tvar err error\n\tif in, pooled, pooledGrad, err = op.checkInput(inputs...); err != nil {\n\t\treturn nil, err\n\t}\n\tif p, ok := prealloc.(tensor.Tensor); ok {\n\t\top.do(p, in, pooled, pooledGrad)\n\t\treturn prealloc, nil\n\t}\n\treturn nil, errors.Errorf(\"Cannot do with PreallocDo - expected PreAlloc to be tensor\")\n}\n\nfunc (op *avgPoolDiffOp) checkInput(inputs ...Value) (in, pooled, pooledGrad tensor.Tensor, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tvar ok bool\n\tif in, ok = inputs[0].(tensor.Tensor); !ok {\n\t\terr = errors.Errorf(\"Expected input to be a tensor\")\n\t\treturn\n\t}\n\n\tif in.Shape().Dims() != 4 {\n\t\terr = errors.Errorf(\"Expected input to have 4 dimensions\")\n\t\treturn\n\t}\n\n\tif pooled, ok = inputs[1].(tensor.Tensor); !ok {\n\t\terr = errors.Errorf(\"Expected pooled to be a tensor\")\n\t\treturn\n\t}\n\tif pooledGrad, ok = inputs[2].(tensor.Tensor); !ok {\n\t\terr = errors.Errorf(\"Expected pooledGrad to be a tensor\")\n\t\treturn\n\t}\n\n\treturn\n}\n\nfunc (op *avgPoolDiffOp) do(inGrad, in, pooled, pooledGrad tensor.Tensor) {\n\tinShape := inGrad.Shape()\n\tpooledShape := pooled.Shape()\n\tpooledStride := pooled.Strides()[1]\n\tinStride := in.Strides()[1]\n\tmaskStride := op.mask.Strides()[1]\n\tmaskData := op.mask.Data().([]int)\n\n\tb, c, h, w := pooledShape[0], pooledShape[1], pooledShape[2], pooledShape[3]\n\tinH, inW := inShape[2], inShape[3]\n\n\tswitch in.Dtype() {\n\tcase tensor.Float32:\n\t\tinGradData := inGrad.Data().([]float32)\n\t\tpooledGradData := pooledGrad.Data().([]float32)\n\t\top.f32s(b, c, h, w, inH, inW,\n\t\t\tinStride, pooledStride, maskStride,\n\t\t\tinGradData, pooledGradData, maskData)\n\tcase tensor.Float64:\n\t\tinGradData := inGrad.Data().([]float64)\n\t\tpooledGradData := pooledGrad.Data().([]float64)\n\t\top.f64s(b, c, h, w, inH, inW,\n\t\t\tinStride, pooledStride, maskStride,\n\t\t\tinGradData, pooledGradData, maskData)\n\t}\n}\n\n// in is the \"bottom\", while out is the \"top\" (bottom being the unpooled, and top being the pooled)\nfunc (op *avgPoolDiffOp) f32s(batches, channels, pooledH, pooledW, inH, inW int,\n\tinStride, outStride, maskStride int,\n\tinDiffData, outDiffData []float32,\n\tmaskData []int) {\n\n\t// zero out. let's hope go's optimizer is smart enought\n\tfor i := range inDiffData {\n\t\tinDiffData[i] = 0\n\t}\n\n\tpadH := op.padNorth\n\tpadW := op.padWest\n\tif op.explicitPadding {\n\t\tpadH = op.padSouth\n\t\tpadW = op.padEast\n\t}\n\n\tfor b := 0; b < batches; b++ {\n\t\tfor c := 0; c < channels; c++ {\n\t\t\tfor ph := 0; ph < pooledH; ph++ {\n\t\t\t\tfor pw := 0; pw < pooledW; pw++ {\n\t\t\t\t\tindex := ph*pooledW + pw\n\t\t\t\t\tinIndex := maskData[index]\n\n\t\t\t\t\tinDiffData[inIndex] += outDiffData[index]\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor ph := 0; ph < inH; ph++ {\n\t\t\t\tfor pw := 0; pw < inW; pw++ {\n\t\t\t\t\thStart := ph*op.strideH - padH\n\t\t\t\t\twStart := pw*op.strideW - padW\n\t\t\t\t\thEnd := minInt(hStart+op.h, inH)\n\t\t\t\t\twEnd := minInt(wStart+op.w, inW)\n\t\t\t\t\thStart = maxInt(hStart, 0)\n\t\t\t\t\twStart = maxInt(wStart, 0)\n\n\t\t\t\t\tpoolIndex := ph*inW + pw\n\t\t\t\t\ttotal := float32(0.0)\n\n\t\t\t\t\tfor hi := hStart; hi < hEnd; hi++ {\n\t\t\t\t\t\tfor wi := wStart; wi < wEnd; wi++ {\n\t\t\t\t\t\t\ti := hi*inW + wi\n\n\t\t\t\t\t\t\ttotal += inDiffData[i]\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tinDiffData[poolIndex] = total / float32(inW)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\toutDiffData = outDiffData[outStride:]\n\t\t\tinDiffData = inDiffData[inStride:]\n\t\t\tmaskData = maskData[maskStride:]\n\t\t}\n\t}\n}\n\n// in is the \"bottom\", while out is the \"top\" (bottom being the unpooled, and top being the pooled)\nfunc (op *avgPoolDiffOp) f64s(batches, channels, pooledH, pooledW, inH, inW int,\n\tinStride, outStride, maskStride int,\n\tinDiffData, outDiffData []float64,\n\tmaskData []int) {\n\n\t// zero out. let's hope go's optimizer is smart enought\n\tfor i := range inDiffData {\n\t\tinDiffData[i] = 0\n\t}\n\n\tpadH := op.padNorth\n\tpadW := op.padWest\n\tif op.explicitPadding {\n\t\tpadH = op.padSouth\n\t\tpadW = op.padEast\n\t}\n\n\tfor b := 0; b < batches; b++ {\n\t\tfor c := 0; c < channels; c++ {\n\t\t\tfor ph := 0; ph < pooledH; ph++ {\n\t\t\t\tfor pw := 0; pw < pooledW; pw++ {\n\t\t\t\t\tindex := ph*pooledW + pw\n\t\t\t\t\tinIndex := maskData[index]\n\n\t\t\t\t\tinDiffData[inIndex] += outDiffData[index]\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor ph := 0; ph < inH; ph++ {\n\t\t\t\tfor pw := 0; pw < inW; pw++ {\n\t\t\t\t\thStart := ph*op.strideH - padH\n\t\t\t\t\twStart := pw*op.strideW - padW\n\t\t\t\t\thEnd := minInt(hStart+op.h, inH)\n\t\t\t\t\twEnd := minInt(wStart+op.w, inW)\n\t\t\t\t\thStart = maxInt(hStart, 0)\n\t\t\t\t\twStart = maxInt(wStart, 0)\n\n\t\t\t\t\tpoolIndex := ph*inW + pw\n\t\t\t\t\ttotal := float64(0.0)\n\n\t\t\t\t\tfor hi := hStart; hi < hEnd; hi++ {\n\t\t\t\t\t\tfor wi := wStart; wi < wEnd; wi++ {\n\t\t\t\t\t\t\ti := hi*inW + wi\n\n\t\t\t\t\t\t\ttotal += inDiffData[i]\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tinDiffData[poolIndex] = total / float64(inW)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\toutDiffData = outDiffData[outStride:]\n\t\t\tinDiffData = inDiffData[inStride:]\n\t\t\tmaskData = maskData[maskStride:]\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "op_avg_pool_test.go",
          "type": "blob",
          "size": 2.55859375,
          "content": "package gorgonia\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc TestAvgPoolOp(t *testing.T) {\n\ttestCases := []struct {\n\t\tdesc           string\n\t\tinput          tensor.Tensor\n\t\tkernelSize     tensor.Shape\n\t\tpad            []int\n\t\tstride         []int\n\t\texpectedOutput interface{}\n\t\texpectedShape  tensor.Shape\n\t\texpectedCost   interface{}\n\t\tPoolFunc       func(*Node, tensor.Shape, []int, []int) (*Node, error)\n\t}{\n\t\t{\n\t\t\tdesc: \"Example 1\",\n\t\t\tinput: tensor.New(\n\t\t\t\ttensor.WithShape(1, 1, 4, 4),\n\t\t\t\ttensor.WithBacking(tensor.Range(tensor.Float64, 0, 16)),\n\t\t\t),\n\t\t\tkernelSize:     []int{2, 2},\n\t\t\tpad:            []int{0, 0},\n\t\t\tstride:         []int{1, 1},\n\t\t\texpectedOutput: []float64{2.5, 3.5, 4.5, 6.5, 7.5, 8.5, 10.5, 11.5, 12.5},\n\t\t\texpectedShape:  tensor.Shape{1, 1, 3, 3},\n\t\t\texpectedCost:   53.583333333333336,\n\t\t\tPoolFunc:       AveragePool2D,\n\t\t},\n\t\t{\n\t\t\tdesc: \"Example 2\",\n\t\t\tinput: tensor.New(\n\t\t\t\ttensor.WithShape(1, 1, 4, 4),\n\t\t\t\ttensor.WithBacking(tensor.Range(tensor.Float32, 0, 16)),\n\t\t\t),\n\t\t\tkernelSize:     []int{2, 2},\n\t\t\tpad:            []int{0, 0},\n\t\t\tstride:         []int{1, 1},\n\t\t\texpectedOutput: []float32{2.5, 3.5, 4.5, 6.5, 7.5, 8.5, 10.5, 11.5, 12.5},\n\t\t\texpectedShape:  tensor.Shape{1, 1, 3, 3},\n\t\t\texpectedCost:   float32(53.583332),\n\t\t\tPoolFunc:       AveragePool2D,\n\t\t},\n\t}\n\n\tfor _, tcase := range testCases {\n\t\tt.Run(tcase.desc, func(t *testing.T) {\n\t\t\tc := require.New(t)\n\n\t\t\tg := NewGraph()\n\n\t\t\tinput := NewTensor(g, tcase.input.Dtype(), tcase.input.Shape().Dims(), WithName(\"input\"), WithShape(tcase.input.Shape()...), WithValue(tcase.input))\n\n\t\t\toutput, err := tcase.PoolFunc(input, tcase.kernelSize, tcase.pad, tcase.stride)\n\t\t\tc.NoError(err)\n\n\t\t\tt.Logf(\"%v output shape: %v\", tcase.desc, output.Shape())\n\t\t\tt.Logf(\"%v input shape: %v\", tcase.desc, input.Shape())\n\n\t\t\ty := NewTensor(g, output.Dtype(), output.Dims(), WithShape(output.Shape()...), WithInit(Ones()))\n\n\t\t\tcost := Must(Mean(Must(Square(Must(Sub(output, y)))))) // MSE\n\n\t\t\t_, err = Grad(cost, input)\n\t\t\tc.NoError(err)\n\n\t\t\t// logger := log.New(os.Stdout, \"\", 0)\n\n\t\t\tvm := NewTapeMachine(\n\t\t\t\tg,\n\t\t\t\t//WithLogger(logger),\n\t\t\t\tWithWatchlist(),\n\t\t\t\tBindDualValues(output),\n\t\t\t\tTraceExec(),\n\t\t\t)\n\n\t\t\tc.NoError(vm.RunAll())\n\t\t\tc.NoError(vm.Close())\n\n\t\t\tt.Logf(\"%v input:\\n%v\", tcase.desc, input.Value())\n\t\t\tt.Logf(\"%v result:\\n%v\", tcase.desc, output.Value())\n\t\t\tt.Logf(\"%v cost: %v\", tcase.desc, cost.Value())\n\n\t\t\tc.Equal(tcase.expectedOutput, output.Value().Data())\n\t\t\tc.Equal(tcase.expectedShape, output.Shape())\n\t\t\tc.Equal(tcase.expectedCost, cost.Value().Data())\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "op_by_indices.go",
          "type": "blob",
          "size": 6.0751953125,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"hash\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\ntype byIndicesOp struct {\n\taxis int\n}\n\nfunc newByIndicesOp(axis int) *byIndicesOp {\n\tif axis < 0 {\n\t\taxis = 0\n\t}\n\n\treturn &byIndicesOp{\n\t\taxis: axis,\n\t}\n}\n\n// ByIndices is an operation that takes the indices as input and return the selected values from those indices.\n// The default axis in 0\nfunc ByIndices(x *Node, indices *Node, axis int) (*Node, error) {\n\top := newByIndicesOp(axis)\n\n\treturn ApplyOp(op, x, indices)\n}\n\nfunc (op *byIndicesOp) Arity() int { return 2 }\n\nfunc (op *byIndicesOp) ReturnsPtr() bool { return false }\n\nfunc (op *byIndicesOp) CallsExtern() bool { return false }\n\nfunc (op *byIndicesOp) WriteHash(h hash.Hash) { fmt.Fprintf(h, op.String()) }\n\nfunc (op *byIndicesOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op *byIndicesOp) String() string {\n\treturn fmt.Sprintf(\"ByIndicesOp{axis=%d}\", op.axis)\n}\n\nfunc (op *byIndicesOp) InferShape(inputs ...DimSizer) (tensor.Shape, error) {\n\ts := inputs[0].(tensor.Shape).Clone()\n\ti := inputs[1].(tensor.Shape).Clone()\n\tif !i.IsVectorLike() {\n\t\treturn nil, errors.Errorf(\"Expected indices to be a vector-like. Got %v instead\", i)\n\t}\n\n\ts[op.axis] = i.TotalSize()\n\n\treturn s, nil\n}\n\nfunc (op *byIndicesOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\tb := makeTensorType(1, tensor.Int)\n\n\treturn hm.NewFnType(a, b, a)\n}\n\nfunc (op *byIndicesOp) OverwritesInput() int { return -1 }\n\nfunc (op *byIndicesOp) checkInput(inputs ...Value) (x, indices tensor.Tensor, err error) {\n\tif err := checkArity(op, len(inputs)); err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tvar ok bool\n\tif x, ok = inputs[0].(tensor.Tensor); !ok {\n\t\treturn nil, nil, errors.Errorf(\"Expected input to be a tensor, got %T\", inputs[0])\n\t}\n\tif indices, ok = inputs[1].(tensor.Tensor); !ok {\n\t\treturn nil, nil, errors.Errorf(\"Expected indices to be a tensor. Got %T instead\", inputs[1])\n\t}\n\n\tif indices.Dtype() != tensor.Int {\n\t\treturn nil, nil, errors.Errorf(\"Expected indices to have tensor.Int as a Dtype. Got %T instead\", indices.Dtype())\n\t}\n\n\treturn x, indices, nil\n}\n\nfunc (op *byIndicesOp) Do(inputs ...Value) (Value, error) {\n\tinputTensor, indices, err := op.checkInput(inputs...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Can't check ByIndicesOp input: %w\", err)\n\t}\n\n\treturn tensor.ByIndices(inputTensor, indices, op.axis)\n}\n\n// DoDiff calculates the diff and sets its value to the output node. Implementation for ADOp interface.\nfunc (op *byIndicesOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) error {\n\tif len(inputs) != 2 {\n\t\treturn fmt.Errorf(\"byIndicesOp.DoDiff needs 2 arguments\")\n\t}\n\n\todv := output.boundTo.(*dualValue)\n\todvd := odv.Value.(tensor.Tensor)\n\n\tdiffOp := &byIndicesOpDiffOp{op}\n\n\tresult, err := diffOp.Do(inputs[0].boundTo, inputs[1].boundTo)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = result.(*tensor.Dense).Reshape(odvd.Shape()...)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tsum, err := odvd.(*tensor.Dense).Add(result.(*tensor.Dense), tensor.UseUnsafe())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\todv.d = sum\n\n\treturn nil\n}\n\n// SymDiff applies the diff op. Implementation for SDOp interface.\nfunc (op *byIndicesOp) SymDiff(inputs Nodes, output, grad *Node) (Nodes, error) {\n\terr := checkArity(op, len(inputs))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tx := inputs[0]\n\tindices := inputs[1]\n\n\tdiffOp := &byIndicesOpDiffOp{op}\n\tnodes := make(Nodes, op.Arity())\n\n\tnodes[0], err = ApplyOp(diffOp, x, grad, indices)\n\n\treturn nodes, err\n}\n\n// DiffWRT is an implementation for the SDOp interface\nfunc (op *byIndicesOp) DiffWRT(inputs int) []bool {\n\tif inputs != op.Arity() {\n\t\tpanic(fmt.Sprintf(\"ByIndicesOp operator needs %d inputs, got %d instead\", op.Arity(), inputs))\n\t}\n\n\treturn []bool{true, false}\n}\n\ntype byIndicesOpDiffOp struct {\n\t*byIndicesOp\n}\n\nfunc (op *byIndicesOpDiffOp) Arity() int { return 3 }\n\nfunc (op *byIndicesOpDiffOp) ReturnsPtr() bool { return false }\n\nfunc (op *byIndicesOpDiffOp) CallsExtern() bool { return false }\n\nfunc (op *byIndicesOpDiffOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, op.String())\n}\n\nfunc (op *byIndicesOpDiffOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op *byIndicesOpDiffOp) String() string {\n\treturn fmt.Sprintf(\"ByIndicesOpDiff{}(%d)\", op.axis)\n}\n\nfunc (op *byIndicesOpDiffOp) InferShape(inputs ...DimSizer) (tensor.Shape, error) {\n\ts := inputs[0].(tensor.Shape).Clone()\n\n\treturn s, nil\n}\n\nfunc (op *byIndicesOpDiffOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\tb := makeTensorType(1, tensor.Int)\n\n\treturn hm.NewFnType(a, a, b, a)\n}\n\nfunc (op *byIndicesOpDiffOp) OverwritesInput() int { return -1 }\n\nfunc (op *byIndicesOpDiffOp) checkInput(inputs ...Value) (in, indices, gradient *tensor.Dense, err error) {\n\tif err := checkArity(op, len(inputs)); err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\n\tvar (\n\t\tok bool\n\t)\n\n\tswitch t := inputs[0].(type) {\n\tcase *dualValue:\n\t\tif in, ok = t.Value.(*tensor.Dense); !ok {\n\t\t\treturn nil, nil, nil, errors.Errorf(\"input should be a tensor.Tensor, got %T\", inputs[0])\n\t\t}\n\tcase *tensor.Dense:\n\t\tin = t\n\tdefault:\n\t\treturn nil, nil, nil, errors.Errorf(\"input type is not supported, got %T\", inputs[0])\n\t}\n\n\tswitch t := inputs[2].(type) {\n\tcase *dualValue:\n\t\tif gradient, ok = t.Value.(*tensor.Dense); !ok {\n\t\t\treturn nil, nil, nil, errors.Errorf(\"gradient should be a tensor, got %T\", inputs[2])\n\t\t}\n\tcase *tensor.Dense:\n\t\tgradient = t\n\tdefault:\n\t\treturn nil, nil, nil, errors.Errorf(\"gradient type is not supported, got %T\", inputs[2])\n\t}\n\n\tswitch t := inputs[1].(type) {\n\tcase *tensor.Dense:\n\t\tindices = t\n\tdefault:\n\t\treturn nil, nil, nil, errors.Errorf(\"indices type %T is not supported\", inputs[1])\n\t}\n\n\treturn in, indices, gradient, nil\n}\n\nfunc (op *byIndicesOpDiffOp) Do(inputs ...Value) (Value, error) {\n\tinputTensor, gradTensor, indices, err := op.checkInput(inputs...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Can't check ByIndicesOpDiff input: %w\", err)\n\t}\n\n\toutput, err := tensor.ByIndicesB(inputTensor, gradTensor, indices, op.axis)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn output, nil\n}\n\n// ensure it complies with the Op interface\nvar (\n\t_ Op = &byIndicesOpDiffOp{}\n\n\t_ Op   = &byIndicesOp{}\n\t_ SDOp = &byIndicesOp{}\n\t_ ADOp = &byIndicesOp{}\n)\n"
        },
        {
          "name": "op_by_indices_test.go",
          "type": "blob",
          "size": 4.3095703125,
          "content": "package gorgonia\n\nimport (\n\t\"log\"\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc TestByIndicesOpDo(t *testing.T) {\n\ttestCases := []struct {\n\t\tdesc           string\n\t\tinput          tensor.Tensor\n\t\tindices        tensor.Tensor\n\t\taxis           int\n\t\texpectedOutput []float64\n\t\texpectedShape  tensor.Shape\n\t}{\n\t\t{\n\t\t\tdesc: \"Example 1\",\n\t\t\tinput: tensor.New(\n\t\t\t\ttensor.WithShape(4, 2),\n\t\t\t\ttensor.WithBacking(tensor.Range(tensor.Float64, 0, 8)),\n\t\t\t),\n\t\t\tindices: tensor.New(\n\t\t\t\ttensor.WithShape(4),\n\t\t\t\ttensor.WithBacking([]int{0, 3, 2, 1}),\n\t\t\t),\n\t\t\taxis:           0,\n\t\t\texpectedOutput: []float64{0, 1, 6, 7, 4, 5, 2, 3},\n\t\t\texpectedShape:  tensor.Shape{4, 2},\n\t\t},\n\t\t{\n\t\t\t// 0 1 2\n\t\t\t// 3 4 5\n\t\t\tdesc: \"Example 2\",\n\t\t\tinput: tensor.New(\n\t\t\t\ttensor.WithShape(2, 3),\n\t\t\t\ttensor.WithBacking(tensor.Range(tensor.Float64, 0, 6)),\n\t\t\t),\n\t\t\tindices: tensor.New(\n\t\t\t\ttensor.WithShape(4),\n\t\t\t\ttensor.WithBacking([]int{0, 2, 1, 1}),\n\t\t\t),\n\t\t\taxis:           1,\n\t\t\texpectedOutput: []float64{0, 2, 1, 1, 3, 5, 4, 4},\n\t\t\texpectedShape:  tensor.Shape{2, 4},\n\t\t},\n\t\t{\n\t\t\tdesc: \"Example 3\",\n\t\t\tinput: tensor.New(\n\t\t\t\ttensor.WithShape(2, 5),\n\t\t\t\ttensor.WithBacking(tensor.Range(tensor.Float64, 0, 10)),\n\t\t\t),\n\t\t\tindices: tensor.New(\n\t\t\t\ttensor.WithShape(2),\n\t\t\t\ttensor.WithBacking([]int{1, 1}),\n\t\t\t),\n\t\t\taxis:           0,\n\t\t\texpectedOutput: []float64{5, 6, 7, 8, 9, 5, 6, 7, 8, 9},\n\t\t\texpectedShape:  tensor.Shape{2, 5},\n\t\t},\n\t}\n\n\tfor _, tcase := range testCases {\n\t\tt.Run(tcase.desc, func(t *testing.T) {\n\t\t\tc := require.New(t)\n\n\t\t\top := newByIndicesOp(tcase.axis)\n\n\t\t\tinputV, _, _, err := anyToValue(tcase.input)\n\t\t\tc.NoError(err)\n\n\t\t\tindicesV, _, _, err := anyToValue(tcase.indices)\n\t\t\tc.NoError(err)\n\n\t\t\toutput, err := op.Do(inputV, indicesV)\n\t\t\tc.NoError(err)\n\n\t\t\tc.Equal(tcase.expectedOutput, output.Data())\n\t\t\tc.Equal(tcase.expectedShape, output.Shape())\n\t\t})\n\t}\n}\n\nfunc TestByIndicesOpFull(t *testing.T) {\n\ttestCases := []struct {\n\t\tdesc           string\n\t\tinput          tensor.Tensor\n\t\tindices        tensor.Tensor\n\t\taxis           int\n\t\texpectedOutput []float64\n\t\texpectedShape  tensor.Shape\n\t}{\n\t\t{\n\t\t\tdesc: \"Example 0\",\n\t\t\tinput: tensor.New(\n\t\t\t\ttensor.WithShape(4, 2),\n\t\t\t\ttensor.WithBacking(tensor.Range(tensor.Float64, 0, 8)),\n\t\t\t),\n\t\t\tindices: tensor.New(\n\t\t\t\ttensor.WithShape(4),\n\t\t\t\ttensor.WithBacking([]int{0, 3, 2, 1}),\n\t\t\t),\n\t\t\taxis:           0,\n\t\t\texpectedOutput: []float64{0, 1, 6, 7, 4, 5, 2, 3},\n\t\t\texpectedShape:  tensor.Shape{4, 2},\n\t\t},\n\t\t{\n\t\t\tdesc: \"Example 1\",\n\t\t\tinput: tensor.New(\n\t\t\t\ttensor.WithShape(4, 2),\n\t\t\t\ttensor.WithBacking(tensor.Range(tensor.Float64, 0, 8)),\n\t\t\t),\n\t\t\tindices: tensor.New(\n\t\t\t\ttensor.WithShape(7),\n\t\t\t\ttensor.WithBacking([]int{0, 3, 2, 1, 0, 5, 1}),\n\t\t\t),\n\t\t\taxis:           0,\n\t\t\texpectedOutput: []float64{0, 1, 6, 7, 4, 5, 2, 3, 0, 1, 0, 1, 2, 3},\n\t\t\texpectedShape:  tensor.Shape{7, 2},\n\t\t},\n\t}\n\n\tfor i, tcase := range testCases {\n\t\tif i != 1 {\n\t\t\tcontinue\n\t\t}\n\t\tt.Run(tcase.desc, func(t *testing.T) {\n\n\t\t\tc := require.New(t)\n\n\t\t\tg := NewGraph()\n\n\t\t\tindices := NewTensor(g, tensor.Int, 1, WithName(\"indices\"), WithShape(tcase.indices.Shape().TotalSize()), WithValue(tcase.indices))\n\t\t\tinput := NewTensor(g, tensor.Float64, tcase.input.Shape().Dims(), WithName(\"input\"), WithShape(tcase.input.Shape().Clone()...), WithValue(tcase.input))\n\n\t\t\toutput, err := ByIndices(input, indices, tcase.axis)\n\t\t\tc.NoError(err)\n\n\t\t\tt.Logf(\"%v output shape: %v\", tcase.desc, output.Shape())\n\t\t\tt.Logf(\"%v input shape: %v\", tcase.desc, input.Shape())\n\n\t\t\ty := NewTensor(g, tensor.Float64, output.Shape().Dims(), WithName(\"target\"), WithShape(output.Shape().Clone()...), WithValue(tensor.New(tensor.WithShape(tcase.expectedShape.Clone()...), tensor.WithBacking(tcase.expectedOutput))))\n\n\t\t\tcost := Must(Mean(Must(Square(Must(Sub(output, y)))))) // MSE\n\n\t\t\t_, err = Grad(cost, input)\n\t\t\tc.NoError(err)\n\n\t\t\tlogger := log.New(os.Stdout, \"\", 0)\n\n\t\t\tvm := NewTapeMachine(\n\t\t\t\tg,\n\t\t\t\tWithLogger(logger),\n\t\t\t\tWithWatchlist(),\n\t\t\t\tBindDualValues(output),\n\t\t\t\tTraceExec(),\n\t\t\t)\n\n\t\t\tc.NoError(vm.RunAll())\n\t\t\tc.NoError(vm.Close())\n\n\t\t\tt.Logf(\"%v input %v\", tcase.desc, input.Value())\n\t\t\tt.Logf(\"%v result: %v\", tcase.desc, output.Value())\n\t\t\tt.Logf(\"%v cost: %v\", tcase.desc, cost.Value())\n\n\t\t\tc.Equal(tcase.expectedOutput, output.Value().Data())\n\t\t\tc.Equal(tcase.expectedShape, output.Shape())\n\t\t\tc.Equal(0.0, cost.Value().Data().(float64))\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "op_ctc_loss.go",
          "type": "blob",
          "size": 23.712890625,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"hash\"\n\t\"log\"\n\t\"math\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/chewxy/math32\"\n\t\"gorgonia.org/tensor\"\n)\n\ntype Reduction uint\n\nconst (\n\tReductionMean Reduction = iota\n\tReductionSum\n)\n\n// CTCLoss -  implements the ctc loss operation\n// This is the implementation of the following paper: http://www.cs.toronto.edu/~graves/icml_2006.pdf\nfunc CTCLoss(logProbs, targets, inputLengths, targetLengths *Node, reduction Reduction) (*Node, error) {\n\top := newCTCLossOp(logProbs.Dtype(), targets.Shape().Dims(), reduction)\n\n\toutput, err := ApplyOp(op, logProbs, targets, inputLengths, targetLengths)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn output, nil\n}\n\ntype ctcLossOp struct {\n\tdtype      tensor.Dtype\n\ttargetDims int\n\treduction  Reduction\n\n\tlogAlpha         *tensor.Dense\n\tnegLogLikelihood *tensor.Dense\n}\n\nfunc newCTCLossOp(dtype tensor.Dtype, targetDims int, reduction Reduction) *ctcLossOp {\n\top := &ctcLossOp{\n\t\tdtype:      dtype,\n\t\ttargetDims: targetDims,\n\t\treduction:  reduction,\n\t}\n\n\treturn op\n}\n\nfunc (op *ctcLossOp) Arity() int { return 4 }\n\nfunc (op *ctcLossOp) ReturnsPtr() bool { return false }\n\nfunc (op *ctcLossOp) CallsExtern() bool { return false }\n\nfunc (op *ctcLossOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, \"CTCLoss{}()\")\n}\n\nfunc (op *ctcLossOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op *ctcLossOp) String() string {\n\treturn fmt.Sprintf(\"CTCLoss{}()\")\n}\n\nfunc (op *ctcLossOp) InferShape(inputs ...DimSizer) (tensor.Shape, error) {\n\treturn tensor.Shape{}, nil\n}\n\nfunc (op *ctcLossOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\tb := makeTensorType(op.targetDims, tensor.Int)\n\tc := makeTensorType(1, tensor.Int)\n\n\td := op.dtype\n\n\treturn hm.NewFnType(a, b, c, c, d)\n}\n\nfunc (op *ctcLossOp) OverwritesInput() int { return -1 }\n\nfunc (op *ctcLossOp) getPrimeTarget(targets []int, offset, stride, idx int) int {\n\tdiv, mod := divmod(idx, 2)\n\tif mod == 0 {\n\t\treturn 0\n\t}\n\n\treturn targets[offset+stride*div]\n}\n\nfunc (op *ctcLossOp) UsePreallocDo(prealloc Value, inputs ...Value) (Value, error) {\n\tif err := checkArity(op, len(inputs)); err != nil {\n\t\treturn nil, err\n\t}\n\n\tlogProbsT := inputs[0].(*tensor.Dense)\n\ttargetsT := inputs[1].(*tensor.Dense)\n\tif targetsT.Dtype() != tensor.Int {\n\t\treturn nil, fmt.Errorf(\"invalid type %v for targets. it should be Int\", targetsT.Dtype())\n\t}\n\n\tinputLengthsT := inputs[2].(*tensor.Dense)\n\tif inputLengthsT.Dtype() != tensor.Int {\n\t\treturn nil, fmt.Errorf(\"invalid type %v for inputLengths. it should be Int\", inputLengthsT.Dtype())\n\t}\n\n\ttargetLengthsT := inputs[3].(*tensor.Dense)\n\tif targetLengthsT.Dtype() != tensor.Int {\n\t\treturn nil, fmt.Errorf(\"invalid type %v for inputLengths. it should be Int\", targetLengthsT.Dtype())\n\t}\n\n\tvar err error\n\n\tswitch logProbsT.Dtype() {\n\tcase Float64:\n\t\terr = op.f64s(logProbsT, prealloc.(*tensor.Dense), targetsT, inputLengthsT, targetLengthsT)\n\tcase Float32:\n\t\terr = op.f32s(logProbsT, prealloc.(*tensor.Dense), targetsT, inputLengthsT, targetLengthsT)\n\tdefault:\n\t\treturn nil, nyi(\"CTCLoss Do\", logProbsT.Dtype())\n\t}\n\n\treturn prealloc, err\n}\n\nfunc (op *ctcLossOp) f64s(logProbsT, prealloc, targetsT, inputLengthsT, targetLengthsT *tensor.Dense) error {\n\ttargets := targetsT.Ints()\n\ttargetLengths := targetLengthsT.Ints()\n\tinputLengths := inputLengthsT.Ints()\n\n\tinputSize := logProbsT.Shape()[0] // rows\n\tbatchSize := logProbsT.Shape()[1] // blocks\n\tnumLabels := logProbsT.Shape()[2] // columns\n\tspatialDim := inputSize * numLabels\n\n\tmaxTargetLength := 0\n\ttargetStride := 0\n\n\ttargetBatchOffsets := make([]int, batchSize)\n\tif targetsT.Dims() == 1 {\n\t\tpos := 0\n\t\tfor i := 0; i < batchSize; i++ {\n\t\t\ttargetBatchOffsets[i] = pos\n\t\t\tpos += targetLengths[i]\n\t\t\tif maxTargetLength < targetLengths[i] {\n\t\t\t\tmaxTargetLength = targetLengths[i]\n\t\t\t}\n\t\t}\n\n\t\ttargetStride = targetsT.Strides()[0]\n\t} else {\n\t\tbatchStride := targetsT.Strides()[0]\n\t\tfor i := 0; i < batchSize; i++ {\n\t\t\ttargetBatchOffsets[i] = i * batchStride\n\t\t\tif maxTargetLength < targetLengths[i] {\n\t\t\t\tmaxTargetLength = targetLengths[i]\n\t\t\t}\n\t\t}\n\n\t\ttargetStride = targetsT.Strides()[1]\n\t}\n\n\tmaxInputLength := logProbsT.Shape()[0]\n\tfor i := 0; i < batchSize; i++ {\n\t\tif inputLengths[i] > maxInputLength {\n\t\t\treturn fmt.Errorf(\"expected inputLengths to have value at most %v, but got %v\", maxInputLength, inputLengths[i])\n\t\t}\n\t}\n\tnegInf := math.Inf(-1)\n\n\tlogAlphaWidth := 2*maxTargetLength + 1\n\tlogAlpha := tensor.New(\n\t\ttensor.Of(logProbsT.Dtype()),\n\t\ttensor.WithShape(batchSize, logProbsT.Shape()[0], logAlphaWidth),\n\t)\n\n\tlogAlphaSpatialDim := tensor.Shape(logAlpha.Shape()[1:]).TotalSize()\n\n\tlogAlphaView, err := logAlpha.Narrow(1, 0, 1)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif err := logAlphaView.Memset(negInf); err != nil {\n\t\treturn err\n\t}\n\n\tnegLogLikelihood := tensor.New(\n\t\ttensor.Of(logProbsT.Dtype()),\n\t\ttensor.WithShape(batchSize),\n\t)\n\n\tlpp, err := tensor.Transpose(logProbsT, 1, 0, 2)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tlogAlphaA := logAlpha.Float64s()\n\tlppA := lpp.(*tensor.Dense).Float64s()\n\tnegLogLikelihoodA := negLogLikelihood.Float64s()\n\n\trunInParallel(0, batchSize, func(b int) {\n\t\tinputLength := inputLengths[b]\n\t\ttargetLength := targetLengths[b]\n\t\ttargetWidth := 2*targetLength + 1\n\t\ttargetsOffset := targetBatchOffsets[b]\n\n\t\tinitialIndex := b * spatialDim\n\t\tfinalIndex := (b + 1) * spatialDim\n\t\tlppSection := lppA[initialIndex:finalIndex]\n\n\t\tinitialLogAlphaIndex := b * logAlphaSpatialDim\n\t\tfinalLogAlphaIndex := (b + 1) * logAlphaSpatialDim\n\t\tlogAlphaSection := logAlphaA[initialLogAlphaIndex:finalLogAlphaIndex]\n\n\t\tlogAlphaSection[0] = lppSection[0]\n\n\t\tif targetLength > 0 {\n\t\t\tlogAlphaSection[1] = lppSection[op.getPrimeTarget(targets, targetsOffset, targetStride, 1)]\n\t\t}\n\n\t\tfor t := 1; t < inputLength; t++ {\n\t\t\tfor s := 0; s < targetWidth; s++ {\n\t\t\t\tcurrentTargetPrime := op.getPrimeTarget(targets, targetsOffset, targetStride, s)\n\n\t\t\t\ti := (t-1)*(targetWidth) + s\n\t\t\t\tla1 := logAlphaSection[i]\n\n\t\t\t\tlamax := la1\n\t\t\t\tvar la2, la3 float64\n\n\t\t\t\tif s > 0 {\n\t\t\t\t\tla2 = logAlphaSection[i-1]\n\t\t\t\t\tif la2 > lamax {\n\t\t\t\t\t\tlamax = la2\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tla2 = negInf\n\t\t\t\t}\n\n\t\t\t\tif s > 1 && op.getPrimeTarget(targets, targetsOffset, targetStride, s-2) != currentTargetPrime {\n\t\t\t\t\tla3 = logAlphaSection[i-2]\n\t\t\t\t\tif la3 > lamax {\n\t\t\t\t\t\tlamax = la3\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tla3 = negInf\n\t\t\t\t}\n\n\t\t\t\tif lamax == negInf {\n\t\t\t\t\tlamax = 0\n\t\t\t\t}\n\n\t\t\t\tlogAlphaSection[t*targetWidth+s] = math.Log(math.Exp(la1-lamax)+math.Exp(la2-lamax)+math.Exp(la3-lamax)) + lamax + lppSection[t*numLabels+currentTargetPrime]\n\t\t\t}\n\t\t}\n\n\t\tif targetLength == 0 {\n\t\t\tnegLogLikelihoodA[b] = logAlphaSection[(inputLength-1)*targetWidth]\n\t\t} else {\n\t\t\tl1 := logAlphaSection[(inputLength-1)*targetWidth+targetLength*2]\n\t\t\tl2 := logAlphaSection[(inputLength-1)*targetWidth+targetLength*2-1]\n\t\t\tmax := l1\n\t\t\tif l2 > max {\n\t\t\t\tmax = l2\n\t\t\t}\n\n\t\t\tif max == negInf {\n\t\t\t\tmax = 0\n\t\t\t}\n\n\t\t\tlogLikelihood := math.Log(math.Exp(l1-max)+math.Exp(l2-max)) + max\n\t\t\tnegLogLikelihoodA[b] = -logLikelihood\n\t\t}\n\t})\n\n\tloss := 0.0\n\n\tfor i, v := range targetLengths {\n\t\tif op.reduction == ReductionSum {\n\t\t\tloss += negLogLikelihoodA[i]\n\t\t} else {\n\t\t\tif v < 1 {\n\t\t\t\tv = 1\n\t\t\t}\n\n\t\t\tloss += negLogLikelihoodA[i] / float64(v)\n\t\t}\n\t}\n\n\tif op.reduction == ReductionMean {\n\t\tloss /= float64(len(targetLengths))\n\t}\n\n\tprealloc.Set(0, loss)\n\top.logAlpha = logAlpha\n\top.negLogLikelihood = negLogLikelihood\n\n\treturn nil\n}\n\nfunc (op *ctcLossOp) f32s(logProbsT, prealloc, targetsT, inputLengthsT, targetLengthsT *tensor.Dense) error {\n\ttargets := targetsT.Ints()\n\ttargetLengths := targetLengthsT.Ints()\n\tinputLengths := inputLengthsT.Ints()\n\n\tinputSize := logProbsT.Shape()[0] // rows\n\tbatchSize := logProbsT.Shape()[1] // blocks\n\tnumLabels := logProbsT.Shape()[2] // columns\n\tspatialDim := inputSize * numLabels\n\n\tmaxTargetLength := 0\n\ttargetStride := 0\n\n\ttargetBatchOffsets := make([]int, batchSize)\n\tif targetsT.Dims() == 1 {\n\t\tpos := 0\n\t\tfor i := 0; i < batchSize; i++ {\n\t\t\ttargetBatchOffsets[i] = pos\n\t\t\tpos += targetLengths[i]\n\t\t\tif maxTargetLength < targetLengths[i] {\n\t\t\t\tmaxTargetLength = targetLengths[i]\n\t\t\t}\n\t\t}\n\n\t\ttargetStride = targetsT.Strides()[0]\n\t} else {\n\t\tbatchStride := targetsT.Strides()[0]\n\t\tfor i := 0; i < batchSize; i++ {\n\t\t\ttargetBatchOffsets[i] = i * batchStride\n\t\t\tif maxTargetLength < targetLengths[i] {\n\t\t\t\tmaxTargetLength = targetLengths[i]\n\t\t\t}\n\t\t}\n\n\t\ttargetStride = targetsT.Strides()[1]\n\t}\n\n\tmaxInputLength := logProbsT.Shape()[0]\n\tfor i := 0; i < batchSize; i++ {\n\t\tif inputLengths[i] > maxInputLength {\n\t\t\treturn fmt.Errorf(\"expected inputLengths to have value at most %v, but got %v\", maxInputLength, inputLengths[i])\n\t\t}\n\t}\n\tnegInf := math32.Inf(-1)\n\n\tlogAlphaWidth := 2*maxTargetLength + 1\n\tlogAlpha := tensor.New(\n\t\ttensor.Of(logProbsT.Dtype()),\n\t\ttensor.WithShape(batchSize, logProbsT.Shape()[0], logAlphaWidth),\n\t)\n\n\tlogAlphaSpatialDim := tensor.Shape(logAlpha.Shape()[1:]).TotalSize()\n\n\tlogAlphaView, err := logAlpha.Narrow(1, 0, 1)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif err := logAlphaView.Memset(negInf); err != nil {\n\t\treturn err\n\t}\n\n\tnegLogLikelihood := tensor.New(\n\t\ttensor.Of(logProbsT.Dtype()),\n\t\ttensor.WithShape(batchSize),\n\t)\n\n\tlpp, err := tensor.Transpose(logProbsT, 1, 0, 2)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tlogAlphaA := logAlpha.Float32s()\n\tlppA := lpp.(*tensor.Dense).Float32s()\n\tnegLogLikelihoodA := negLogLikelihood.Float32s()\n\n\trunInParallel(0, batchSize, func(b int) {\n\t\tinputLength := inputLengths[b]\n\t\ttargetLength := targetLengths[b]\n\t\ttargetWidth := 2*targetLength + 1\n\t\ttargetsOffset := targetBatchOffsets[b]\n\n\t\tinitialIndex := b * spatialDim\n\t\tfinalIndex := (b + 1) * spatialDim\n\t\tlppSection := lppA[initialIndex:finalIndex]\n\n\t\tinitialLogAlphaIndex := b * logAlphaSpatialDim\n\t\tfinalLogAlphaIndex := (b + 1) * logAlphaSpatialDim\n\t\tlogAlphaSection := logAlphaA[initialLogAlphaIndex:finalLogAlphaIndex]\n\n\t\tlogAlphaSection[0] = lppSection[0]\n\n\t\tif targetLength > 0 {\n\t\t\tlogAlphaSection[1] = lppSection[op.getPrimeTarget(targets, targetsOffset, targetStride, 1)]\n\t\t}\n\n\t\tfor t := 1; t < inputLength; t++ {\n\t\t\tfor s := 0; s < targetWidth; s++ {\n\t\t\t\tcurrentTargetPrime := op.getPrimeTarget(targets, targetsOffset, targetStride, s)\n\n\t\t\t\ti := (t-1)*(targetWidth) + s\n\t\t\t\tla1 := logAlphaSection[i]\n\n\t\t\t\tlamax := la1\n\t\t\t\tvar la2, la3 float32\n\n\t\t\t\tif s > 0 {\n\t\t\t\t\tla2 = logAlphaSection[i-1]\n\t\t\t\t\tif la2 > lamax {\n\t\t\t\t\t\tlamax = la2\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tla2 = negInf\n\t\t\t\t}\n\n\t\t\t\tif s > 1 && op.getPrimeTarget(targets, targetsOffset, targetStride, s-2) != currentTargetPrime {\n\t\t\t\t\tla3 = logAlphaSection[i-2]\n\t\t\t\t\tif la3 > lamax {\n\t\t\t\t\t\tlamax = la3\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tla3 = negInf\n\t\t\t\t}\n\n\t\t\t\tif lamax == negInf {\n\t\t\t\t\tlamax = 0\n\t\t\t\t}\n\n\t\t\t\tlogAlphaSection[t*targetWidth+s] = math32.Log(math32.Exp(la1-lamax)+math32.Exp(la2-lamax)+math32.Exp(la3-lamax)) + lamax + lppSection[t*numLabels+currentTargetPrime]\n\t\t\t}\n\t\t}\n\n\t\tif targetLength == 0 {\n\t\t\tnegLogLikelihoodA[b] = logAlphaSection[(inputLength-1)*targetWidth]\n\t\t} else {\n\t\t\tl1 := logAlphaSection[(inputLength-1)*targetWidth+targetLength*2]\n\t\t\tl2 := logAlphaSection[(inputLength-1)*targetWidth+targetLength*2-1]\n\t\t\tmax := l1\n\t\t\tif l2 > max {\n\t\t\t\tmax = l2\n\t\t\t}\n\n\t\t\tif max == negInf {\n\t\t\t\tmax = 0\n\t\t\t}\n\n\t\t\tlogLikelihood := math32.Log(math32.Exp(l1-max)+math32.Exp(l2-max)) + max\n\t\t\tnegLogLikelihoodA[b] = -logLikelihood\n\t\t}\n\t})\n\n\tloss := float32(0.0)\n\n\tfor i, v := range targetLengths {\n\t\tif op.reduction == ReductionSum {\n\t\t\tloss += negLogLikelihoodA[i]\n\t\t} else {\n\t\t\tif v < 1 {\n\t\t\t\tv = 1\n\t\t\t}\n\n\t\t\tloss += negLogLikelihoodA[i] / float32(v)\n\t\t}\n\t}\n\n\tif op.reduction == ReductionMean {\n\t\tloss /= float32(len(targetLengths))\n\t}\n\n\tprealloc.Set(0, loss)\n\top.logAlpha = logAlpha\n\top.negLogLikelihood = negLogLikelihood\n\n\treturn nil\n}\n\nfunc (op *ctcLossOp) Do(inputs ...Value) (retVal Value, err error) {\n\tlogProbsT := inputs[0].(*tensor.Dense)\n\n\tprealloc := tensor.New(\n\t\ttensor.Of(logProbsT.Dtype()),\n\t\ttensor.WithShape(),\n\t)\n\n\treturn op.UsePreallocDo(prealloc, inputs...)\n}\n\n// SymDiff applies the diff op. Implementation for SDOp interface.\nfunc (op *ctcLossOp) SymDiff(inputs Nodes, output, grad *Node) (Nodes, error) {\n\terr := checkArity(op, len(inputs))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlogProbs := inputs[0]\n\ttargets := inputs[1]\n\tinputLengths := inputs[2]\n\ttargetLengths := inputs[3]\n\n\tdiffOp := &ctcLossDiffOp{op}\n\n\tret, err := ApplyOp(diffOp, logProbs, targets, inputLengths, targetLengths, grad)\n\n\treturn Nodes{ret, nil, nil, nil, nil}, err\n}\n\n// DiffWRT is an implementation for the SDOp interface\nfunc (op *ctcLossOp) DiffWRT(inputs int) []bool {\n\treturn []bool{true, false, false, false, false}\n}\n\ntype ctcLossDiffOp struct {\n\t*ctcLossOp\n}\n\nfunc (op *ctcLossDiffOp) Arity() int { return 5 }\n\nfunc (op *ctcLossDiffOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, op.String())\n}\n\nfunc (op *ctcLossDiffOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op *ctcLossDiffOp) String() string {\n\treturn fmt.Sprintf(\"ctcLossDiff{}()\")\n}\n\nfunc (op *ctcLossDiffOp) InferShape(inputs ...DimSizer) (tensor.Shape, error) {\n\ts := inputs[0].(tensor.Shape).Clone()\n\n\treturn s, nil\n}\n\nfunc (op *ctcLossDiffOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\tb := makeTensorType(op.targetDims, tensor.Int)\n\tc := makeTensorType(1, tensor.Int)\n\td := hm.TypeVariable('d')\n\n\treturn hm.NewFnType(a, b, c, c, d, a)\n}\n\nfunc (op *ctcLossDiffOp) OverwritesInput() int { return -1 }\n\nfunc (op *ctcLossDiffOp) Do(inputs ...Value) (Value, error) {\n\tinput := inputs[0]\n\tprealloc := tensor.New(tensor.WithShape(input.Shape().Clone()...), tensor.Of(input.Dtype()))\n\n\treturn op.UsePreallocDo(prealloc, inputs...)\n}\n\nfunc (op *ctcLossDiffOp) UsePreallocDo(prealloc Value, inputs ...Value) (Value, error) {\n\tif err := checkArity(op, len(inputs)); err != nil {\n\t\treturn nil, err\n\t}\n\n\tlogProbsT := inputs[0].(*tensor.Dense)\n\ttargetsT := inputs[1].(*tensor.Dense)\n\tinputLengthsT := inputs[2].(*tensor.Dense)\n\ttargetLengthsT := inputs[3].(*tensor.Dense)\n\tgradOutT := inputs[4]\n\n\tswitch logProbsT.Dtype() {\n\tcase Float64:\n\t\top.f64s(logProbsT, targetsT, inputLengthsT, targetLengthsT, prealloc.(*tensor.Dense), gradOutT.(*F64))\n\tcase Float32:\n\t\top.f32s(logProbsT, targetsT, inputLengthsT, targetLengthsT, prealloc.(*tensor.Dense), gradOutT.(*F32))\n\tdefault:\n\t\tlog.Panicf(\"%T type is not supported for CTCLoss op\", logProbsT.Dtype())\n\t}\n\n\treturn prealloc, nil\n}\n\nfunc (op *ctcLossDiffOp) f64s(logProbsT, targetsT, inputLengthsT, targetLengthsT, gradT *tensor.Dense, gradOutT *F64) error {\n\ttargets := targetsT.Ints()\n\ttargetLengths := targetLengthsT.Ints()\n\tinputLengths := inputLengthsT.Ints()\n\n\tinputSize := logProbsT.Shape()[0] // rows\n\tbatchSize := logProbsT.Shape()[1] // blocks\n\tnumLabels := logProbsT.Shape()[2] // columns\n\tspatialDim := inputSize * numLabels\n\tlogAlphaSpatialDim := tensor.Shape(op.logAlpha.Shape()[1:]).TotalSize()\n\n\tmaxTargetLength := 0\n\ttargetStride := 0\n\n\ttargetBatchOffsets := make([]int, batchSize)\n\n\tif targetsT.Dims() == 1 {\n\t\tpos := 0\n\n\t\tfor i := 0; i < batchSize; i++ {\n\t\t\ttargetBatchOffsets[i] = pos\n\t\t\tpos += targetLengths[i]\n\n\t\t\tif maxTargetLength < targetLengths[i] {\n\t\t\t\tmaxTargetLength = targetLengths[i]\n\t\t\t}\n\t\t}\n\n\t\ttargetStride = targetsT.Strides()[0]\n\t} else {\n\t\tbatchStride := targetsT.Strides()[0]\n\n\t\tfor i := 0; i < batchSize; i++ {\n\t\t\ttargetBatchOffsets[i] = i * batchStride\n\t\t}\n\n\t\ttargetStride = targetsT.Strides()[1]\n\t\tmaxTargetLength = targetsT.Shape()[1]\n\t}\n\n\tnegInf := math.Inf(-1)\n\tif err := gradT.Memset(negInf); err != nil {\n\t\treturn err\n\t}\n\n\tlogBetaT := tensor.New(tensor.WithShape(op.logAlpha.Shape()...), tensor.Of(op.logAlpha.Dtype()))\n\tif err := logBetaT.Memset(negInf); err != nil {\n\t\treturn err\n\t}\n\n\tlppT, err := tensor.Transpose(logProbsT, 1, 0, 2) // NOTE: I think we can optimize memory usage here\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = gradT.T(1, 0, 2)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tnegLogLikelihood := op.negLogLikelihood.Float64s()\n\tlogBeta := logBetaT.Float64s()\n\tlogAlpha := op.logAlpha.Float64s()\n\tlpp := lppT.(*tensor.Dense).Float64s()\n\n\t// this can be parallelized\n\trunInParallel(0, batchSize, func(b int) {\n\t\tinputLength := inputLengths[b]\n\t\ttargetLength := targetLengths[b]\n\t\ttargetsOffset := targetBatchOffsets[b]\n\t\ttargetWidth := 2*targetLength + 1\n\n\t\tinitialIndex := b * spatialDim\n\t\tfinalIndex := (b + 1) * spatialDim\n\t\tlppSection := lpp[initialIndex:finalIndex]\n\t\tgradSlice, err := gradT.Slice(S(b))\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\n\t\tnll := negLogLikelihood[b]\n\n\t\tinitialLogAlphaIndex := b * logAlphaSpatialDim\n\t\tfinalLogAlphaIndex := (b + 1) * logAlphaSpatialDim\n\t\tlogAlphaSection := logAlpha[initialLogAlphaIndex:finalLogAlphaIndex]\n\t\tlogBetaSection := logBeta[initialLogAlphaIndex:finalLogAlphaIndex]\n\n\t\tif inputLength > 0 {\n\t\t\tlogBetaSection[(inputLength-1)*targetWidth+2*targetLength] = lppSection[(inputLength-1)*numLabels]\n\n\t\t\tgradSlice.SetAt(logAlphaSection[(inputLength-1)*targetWidth+2*targetLength]+logBetaSection[(inputLength-1)*targetWidth+2*targetLength], inputLength-1, 0)\n\n\t\t\tif targetLength > 0 {\n\t\t\t\tcurrentPrime := op.getPrimeTarget(targets, targetsOffset, targetStride, 2*targetLength-1)\n\n\t\t\t\tlogBetaSection[(inputLength-1)*targetWidth+(2*targetLength-1)] = lppSection[(inputLength-1)*numLabels+currentPrime]\n\n\t\t\t\tgradSlice.SetAt(logAlphaSection[(inputLength-1)*targetWidth+(2*targetLength-1)]+logBetaSection[(inputLength-1)*targetWidth+(2*targetLength-1)], (inputLength - 1), currentPrime)\n\t\t\t}\n\n\t\t\tfor t := inputLength - 2; t >= 0; t-- {\n\t\t\t\tfor s := 2 * targetLength; s >= 0; s-- {\n\t\t\t\t\tbaseIndex := (t+1)*targetWidth + s\n\t\t\t\t\tlb1 := logBetaSection[baseIndex]\n\t\t\t\t\tlbmax := lb1\n\n\t\t\t\t\tvar lb2, lb3 float64\n\n\t\t\t\t\tcurrentTargetPrime := op.getPrimeTarget(targets, targetsOffset, targetStride, s)\n\n\t\t\t\t\tif s < 2*targetLength {\n\t\t\t\t\t\tlb2 = logBetaSection[baseIndex+1]\n\t\t\t\t\t\tif lb2 > lbmax {\n\t\t\t\t\t\t\tlbmax = lb2\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tlb2 = negInf\n\t\t\t\t\t}\n\n\t\t\t\t\tif s < 2*targetLength-1 && op.getPrimeTarget(targets, targetsOffset, targetStride, s+2) != currentTargetPrime {\n\t\t\t\t\t\tlb3 = logBetaSection[baseIndex+2]\n\t\t\t\t\t\tif lb3 > lbmax {\n\t\t\t\t\t\t\tlbmax = lb3\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tlb3 = negInf\n\t\t\t\t\t}\n\n\t\t\t\t\tif lbmax == negInf {\n\t\t\t\t\t\tlbmax = 0\n\t\t\t\t\t}\n\n\t\t\t\t\tlogBetaSection[t*targetWidth+s] = math.Log(\n\t\t\t\t\t\tmath.Exp(lb1-lbmax)+math.Exp(lb2-lbmax)+math.Exp(lb3-lbmax)) + lbmax + lppSection[t*numLabels+currentTargetPrime]\n\n\t\t\t\t\tlogAlphaBeta := logAlphaSection[t*targetWidth+s] + logBetaSection[t*targetWidth+s]\n\n\t\t\t\t\tlcab := op.getOrPanicF64(gradSlice, t, currentTargetPrime)\n\t\t\t\t\tif lcab == negInf {\n\t\t\t\t\t\tgradSlice.SetAt(logAlphaBeta, t, currentTargetPrime)\n\t\t\t\t\t} else {\n\t\t\t\t\t\tmax := math.Max(lcab, logAlphaBeta)\n\t\t\t\t\t\tv := math.Log(math.Exp(lcab-max)+math.Exp(logAlphaBeta-max)) + max\n\t\t\t\t\t\tgradSlice.SetAt(\n\t\t\t\t\t\t\tv,\n\t\t\t\t\t\t\tt, currentTargetPrime,\n\t\t\t\t\t\t)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor t := 0; t < inputLength; t++ {\n\t\t\t\tfor c := 0; c < numLabels; c++ {\n\t\t\t\t\tres := op.getOrPanicF64(gradSlice, t, c)\n\t\t\t\t\tlp := lppSection[t*numLabels+c]\n\n\t\t\t\t\tv := (math.Exp(lp) - math.Exp(res+nll-lp)) * float64(*gradOutT)\n\n\t\t\t\t\tgradSlice.SetAt(v, t, c)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t})\n\n\tgradT.UT()\n\n\treturn nil\n}\n\nfunc (op *ctcLossDiffOp) f32s(logProbsT, targetsT, inputLengthsT, targetLengthsT, gradT *tensor.Dense, gradOutT *F32) error {\n\ttargets := targetsT.Ints()\n\ttargetLengths := targetLengthsT.Ints()\n\tinputLengths := inputLengthsT.Ints()\n\n\tinputSize := logProbsT.Shape()[0] // rows\n\tbatchSize := logProbsT.Shape()[1] // blocks\n\tnumLabels := logProbsT.Shape()[2] // columns\n\tspatialDim := inputSize * numLabels\n\tlogAlphaSpatialDim := tensor.Shape(op.logAlpha.Shape()[1:]).TotalSize()\n\n\tmaxTargetLength := 0\n\ttargetStride := 0\n\n\ttargetBatchOffsets := make([]int, batchSize)\n\n\tif targetsT.Dims() == 1 {\n\t\tpos := 0\n\n\t\tfor i := 0; i < batchSize; i++ {\n\t\t\ttargetBatchOffsets[i] = pos\n\t\t\tpos += targetLengths[i]\n\n\t\t\tif maxTargetLength < targetLengths[i] {\n\t\t\t\tmaxTargetLength = targetLengths[i]\n\t\t\t}\n\t\t}\n\n\t\ttargetStride = targetsT.Strides()[0]\n\t} else {\n\t\tbatchStride := targetsT.Strides()[0]\n\n\t\tfor i := 0; i < batchSize; i++ {\n\t\t\ttargetBatchOffsets[i] = i * batchStride\n\t\t}\n\n\t\ttargetStride = targetsT.Strides()[1]\n\t\tmaxTargetLength = targetsT.Shape()[1]\n\t}\n\n\tnegInf := math32.Inf(-1)\n\tif err := gradT.Memset(negInf); err != nil {\n\t\treturn err\n\t}\n\n\tlogBetaT := tensor.New(tensor.WithShape(op.logAlpha.Shape()...), tensor.Of(op.logAlpha.Dtype()))\n\tif err := logBetaT.Memset(negInf); err != nil {\n\t\treturn err\n\t}\n\n\tlppT, err := tensor.Transpose(logProbsT, 1, 0, 2) // NOTE: I think we can optimize memory usage here\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = gradT.T(1, 0, 2)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tnegLogLikelihood := op.negLogLikelihood.Float32s()\n\tlogBeta := logBetaT.Float32s()\n\tlogAlpha := op.logAlpha.Float32s()\n\tlpp := lppT.(*tensor.Dense).Float32s()\n\n\t// this can be parallelized\n\trunInParallel(0, batchSize, func(b int) {\n\t\tinputLength := inputLengths[b]\n\t\ttargetLength := targetLengths[b]\n\t\ttargetsOffset := targetBatchOffsets[b]\n\t\ttargetWidth := 2*targetLength + 1\n\n\t\tinitialIndex := b * spatialDim\n\t\tfinalIndex := (b + 1) * spatialDim\n\t\tlppSection := lpp[initialIndex:finalIndex]\n\t\tgradSlice, err := gradT.Slice(S(b))\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\n\t\tnll := negLogLikelihood[b]\n\n\t\tinitialLogAlphaIndex := b * logAlphaSpatialDim\n\t\tfinalLogAlphaIndex := (b + 1) * logAlphaSpatialDim\n\t\tlogAlphaSection := logAlpha[initialLogAlphaIndex:finalLogAlphaIndex]\n\t\tlogBetaSection := logBeta[initialLogAlphaIndex:finalLogAlphaIndex]\n\n\t\tif inputLength > 0 {\n\t\t\tlogBetaSection[(inputLength-1)*targetWidth+2*targetLength] = lppSection[(inputLength-1)*numLabels]\n\n\t\t\tgradSlice.SetAt(logAlphaSection[(inputLength-1)*targetWidth+2*targetLength]+logBetaSection[(inputLength-1)*targetWidth+2*targetLength], inputLength-1, 0)\n\n\t\t\tif targetLength > 0 {\n\t\t\t\tcurrentPrime := op.getPrimeTarget(targets, targetsOffset, targetStride, 2*targetLength-1)\n\n\t\t\t\tlogBetaSection[(inputLength-1)*targetWidth+(2*targetLength-1)] = lppSection[(inputLength-1)*numLabels+currentPrime]\n\n\t\t\t\tgradSlice.SetAt(logAlphaSection[(inputLength-1)*targetWidth+(2*targetLength-1)]+logBetaSection[(inputLength-1)*targetWidth+(2*targetLength-1)], (inputLength - 1), currentPrime)\n\t\t\t}\n\n\t\t\tfor t := inputLength - 2; t >= 0; t-- {\n\t\t\t\tfor s := 2 * targetLength; s >= 0; s-- {\n\t\t\t\t\tbaseIndex := (t+1)*targetWidth + s\n\t\t\t\t\tlb1 := logBetaSection[baseIndex]\n\t\t\t\t\tlbmax := lb1\n\n\t\t\t\t\tvar lb2, lb3 float32\n\n\t\t\t\t\tcurrentTargetPrime := op.getPrimeTarget(targets, targetsOffset, targetStride, s)\n\n\t\t\t\t\tif s < 2*targetLength {\n\t\t\t\t\t\tlb2 = logBetaSection[baseIndex+1]\n\t\t\t\t\t\tif lb2 > lbmax {\n\t\t\t\t\t\t\tlbmax = lb2\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tlb2 = negInf\n\t\t\t\t\t}\n\n\t\t\t\t\tif s < 2*targetLength-1 && op.getPrimeTarget(targets, targetsOffset, targetStride, s+2) != currentTargetPrime {\n\t\t\t\t\t\tlb3 = logBetaSection[baseIndex+2]\n\t\t\t\t\t\tif lb3 > lbmax {\n\t\t\t\t\t\t\tlbmax = lb3\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tlb3 = negInf\n\t\t\t\t\t}\n\n\t\t\t\t\tif lbmax == negInf {\n\t\t\t\t\t\tlbmax = 0\n\t\t\t\t\t}\n\n\t\t\t\t\tlogBetaSection[t*targetWidth+s] = math32.Log(\n\t\t\t\t\t\tmath32.Exp(lb1-lbmax)+math32.Exp(lb2-lbmax)+math32.Exp(lb3-lbmax)) + lbmax + lppSection[t*numLabels+currentTargetPrime]\n\n\t\t\t\t\tlogAlphaBeta := logAlphaSection[t*targetWidth+s] + logBetaSection[t*targetWidth+s]\n\n\t\t\t\t\tlcab := op.getOrPanicF32(gradSlice, t, currentTargetPrime)\n\t\t\t\t\tif lcab == negInf {\n\t\t\t\t\t\tgradSlice.SetAt(logAlphaBeta, t, currentTargetPrime)\n\t\t\t\t\t} else {\n\t\t\t\t\t\tmax := math32.Max(lcab, logAlphaBeta)\n\t\t\t\t\t\tv := math32.Log(math32.Exp(lcab-max)+math32.Exp(logAlphaBeta-max)) + max\n\t\t\t\t\t\tgradSlice.SetAt(\n\t\t\t\t\t\t\tv,\n\t\t\t\t\t\t\tt, currentTargetPrime,\n\t\t\t\t\t\t)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor t := 0; t < inputLength; t++ {\n\t\t\t\tfor c := 0; c < numLabels; c++ {\n\t\t\t\t\tres := op.getOrPanicF32(gradSlice, t, c)\n\t\t\t\t\tlp := lppSection[t*numLabels+c]\n\n\t\t\t\t\tv := (math32.Exp(lp) - math32.Exp(res+nll-lp)) * float32(*gradOutT)\n\n\t\t\t\t\tgradSlice.SetAt(v, t, c)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t})\n\n\tgradT.UT()\n\n\treturn nil\n}\n\nfunc (op ctcLossDiffOp) getOrPanic(view tensor.View, coords ...int) interface{} {\n\tv, err := view.At(coords...)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\treturn v\n}\n\nfunc (op ctcLossDiffOp) getOrPanicF64(view tensor.View, coords ...int) float64 {\n\treturn op.getOrPanic(view, coords...).(float64)\n}\n\nfunc (op ctcLossDiffOp) getOrPanicF32(view tensor.View, coords ...int) float32 {\n\treturn op.getOrPanic(view, coords...).(float32)\n}\n\n// ensure it complies with the Op interface\nvar (\n\t_ Op = &ctcLossDiffOp{}\n)\n"
        },
        {
          "name": "op_ctc_loss_test.go",
          "type": "blob",
          "size": 10.0546875,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc TestCTCLossDo(t *testing.T) {\n\ttestCases := []struct {\n\t\tDtype tensor.Dtype\n\n\t\treduction Reduction\n\n\t\tlogProbsInit  InitWFn\n\t\tlogProbsShape tensor.Shape\n\n\t\ttargetsInit  InitWFn\n\t\ttargetsShape tensor.Shape\n\n\t\tinputLengthsInit  InitWFn\n\t\tinputLengthsShape tensor.Shape\n\n\t\ttargetLengthsInit  InitWFn\n\t\ttargetLengthsShape tensor.Shape\n\n\t\texpectedOutput    tensor.Tensor\n\t\texpectedInputGrad tensor.Tensor\n\t}{\n\t\t{\n\t\t\tDtype:              Float64,\n\t\t\treduction:          ReductionMean,\n\t\t\tlogProbsInit:       RangedFromWithStep(0.0, 0.01),\n\t\t\tlogProbsShape:      tensor.Shape{4, 4, 4},\n\t\t\ttargetsInit:        RangedFromWithStep(2, 0),\n\t\t\ttargetsShape:       tensor.Shape{4, 4},\n\t\t\tinputLengthsInit:   RangedFromWithStep(4, 0),\n\t\t\tinputLengthsShape:  tensor.Shape{4},\n\t\t\ttargetLengthsInit:  RangedFromWithStep(2, 0),\n\t\t\ttargetLengthsShape: tensor.Shape{4},\n\t\t\texpectedOutput: tensor.New(\n\t\t\t\ttensor.WithShape(),\n\t\t\t\ttensor.WithBacking([]float64{-1.428742987863855}),\n\t\t\t),\n\t\t\texpectedInputGrad: tensor.New(\n\t\t\t\ttensor.WithShape(4, 4, 4),\n\t\t\t\ttensor.WithBacking([]float64{0.8016031528676627, 1.010050167084168, 0.21859818715909318, 1.030454533953517, 0.842413927060051, 1.0512710963760241, 0.2602333936776974, 1.0725081812542165, 0.8848902205426215, 1.0941742837052104, 0.30356776520798545, 1.1162780704588713, 0.9291000044470383, 1.1388283833246218, 0.3486706459895643, 1.161834242728283, 0.5743124474256416, 1.1853048513203654, 0.7964157866879789, 1.2092495976572515, 0.6222043345940014, 1.2336780599567434, 0.8452751541535497, 1.258600009929478, 0.6720507267552366, 1.2840254166877416, 0.8961285102319406, 1.3099644507332475, 0.7239313887712684, 1.3364274880254723, 0.9490572311421717, 1.3634251141321778, 0.7779293407697887, 1.3909681284637805, 1.0041460141297627, 1.4190675485932576, 0.8341309909941721, 1.4477346146633248, 1.0614830130003936, 1.476980793882643, 0.8926262740751023, 1.506817785112854, 1.121159979184803, 1.5372575235482817, 0.9535087949451677, 1.5683121854901692, 1.1832724085606505, 1.5999941932173607, 1.4176775550605565, 1.6323162199553793, 0.8471181178324659, 1.6652911949458866, 1.4836308025665494, 1.698932308618551, 0.9144037093171966, 1.7332530178673957, 1.5522756531637645, 1.7682670514337357, 0.9844352778824115, 1.8039884153978574, 1.6237219532581721, 1.840431398781638, 1.0573248889786804, 1.8776105792643438}),\n\t\t\t),\n\t\t},\n\t\t{\n\t\t\tDtype:              Float32,\n\t\t\treduction:          ReductionSum,\n\t\t\tlogProbsInit:       RangedFromWithStep(0.0, 0.01),\n\t\t\tlogProbsShape:      tensor.Shape{4, 4, 4},\n\t\t\ttargetsInit:        RangedFromWithStep(2, 0),\n\t\t\ttargetsShape:       tensor.Shape{4, 4},\n\t\t\tinputLengthsInit:   RangedFromWithStep(4, 0),\n\t\t\tinputLengthsShape:  tensor.Shape{4},\n\t\t\ttargetLengthsInit:  RangedFromWithStep(2, 0),\n\t\t\ttargetLengthsShape: tensor.Shape{4},\n\t\t\texpectedOutput: tensor.New(\n\t\t\t\ttensor.WithShape(),\n\t\t\t\ttensor.WithBacking([]float32{-11.429942}),\n\t\t\t),\n\t\t\texpectedInputGrad: tensor.New(\n\t\t\t\ttensor.WithShape(4, 4, 4),\n\t\t\t\ttensor.WithBacking([]float32{0.8016031, 1.0100502, 0.21859795, 1.0304545, 0.8424139, 1.0512711, 0.26023316, 1.0725082, 0.88489014, 1.0941743, 0.303568, 1.116278, 0.9291, 1.1388284, 0.3486709, 1.1618342, 0.57431227, 1.1853049, 0.79641575, 1.2092496, 0.6222042, 1.2336781, 0.84527516, 1.2586, 0.6720507, 1.2840254, 0.8961285, 1.3099644, 0.7239313, 1.3364275, 0.94905716, 1.363425, 0.7779292, 1.3909681, 1.0041459, 1.4190674, 0.83413076, 1.4477345, 1.0614828, 1.4769807, 0.89262605, 1.5068176, 1.1211598, 1.5372573, 0.9535085, 1.5683119, 1.1832721, 1.599994, 1.4176772, 1.6323159, 0.8471178, 1.6652908, 1.4836304, 1.6989319, 0.9144034, 1.7332526, 1.5522753, 1.7682667, 0.98443496, 1.803988, 1.6237215, 1.8404309, 1.0573245, 1.87761}),\n\t\t\t),\n\t\t},\n\t\t{\n\t\t\tDtype:              Float64,\n\t\t\treduction:          ReductionSum,\n\t\t\tlogProbsInit:       RangedFromWithStep(0.0, 0.01),\n\t\t\tlogProbsShape:      tensor.Shape{4, 3, 5},\n\t\t\ttargetsInit:        RangedFromWithStep(2, 0),\n\t\t\ttargetsShape:       tensor.Shape{3, 4},\n\t\t\tinputLengthsInit:   RangedFromWithStep(4, 0),\n\t\t\tinputLengthsShape:  tensor.Shape{3},\n\t\t\ttargetLengthsInit:  RangedFromWithStep(2, 0),\n\t\t\ttargetLengthsShape: tensor.Shape{3},\n\t\t\texpectedOutput: tensor.New(\n\t\t\t\ttensor.WithShape(),\n\t\t\t\ttensor.WithBacking([]float64{-8.27245792718313}),\n\t\t\t),\n\t\t\texpectedInputGrad: tensor.New(\n\t\t\t\ttensor.WithShape(4, 3, 5),\n\t\t\t\ttensor.WithBacking([]float64{0.8016031528676626, 1.010050167084168, 0.21859818715909318, 1.030454533953517, 1.0408107741923882, 0.8528742492436868, 1.0618365465453596, 0.27090502838655417, 1.0832870676749586, 1.0941742837052104, 0.9067740709433104, 1.1162780704588713, 0.32589369871171314, 1.1388283833246218, 1.1502737988572274, 0.5626358191621145, 1.1735108709918103, 0.784503274886534, 1.1972173631218102, 1.2092495976572515, 0.6222043345940012, 1.2336780599567434, 0.8452751541535495, 1.258600009929478, 1.2712491503214047, 0.684826993121573, 1.2969300866657718, 0.9091628742994162, 1.323129812337437, 1.3364274880254723, 0.7506603840098345, 1.3634251141321778, 0.9763261879021259, 1.3909681284637805, 1.404947590563594, 0.8198691250270892, 1.4333294145603404, 1.0469330382294935, 1.462284589434225, 1.476980793882643, 0.8926262740751021, 1.506817785112854, 1.1211599791848028, 1.5372575235482817, 1.5527072185113364, 1.3699153383578317, 1.584073984994482, 0.7983910403496982, 1.6160744021928939, 1.6323162199553793, 1.4503244235677912, 1.6652911949458866, 0.8804244968312241, 1.698932308618551, 1.716006862184859, 1.5348561707350583, 1.7506725002961017, 0.9666638985660732, 1.7860384307500738, 1.8039884153978574}),\n\t\t\t),\n\t\t},\n\t}\n\n\tfor i, tC := range testCases {\n\t\tt.Run(fmt.Sprintf(\"Example #%v %v (%v)\", i+1, tC.Dtype, tC.logProbsShape), func(t *testing.T) {\n\t\t\tac := require.New(t)\n\n\t\t\tg := NewGraph()\n\t\t\tlogProbs := NewTensor(g, tC.Dtype, tC.logProbsShape.Dims(), WithShape(tC.logProbsShape...), WithInit(tC.logProbsInit), WithName(\"logProbs\"))\n\t\t\ttargets := NewTensor(g, tensor.Int, tC.targetsShape.Dims(), WithShape(tC.targetsShape...), WithInit(tC.targetsInit), WithName(\"targets\"))\n\t\t\tinputLengths := NewTensor(g, tensor.Int, tC.inputLengthsShape.Dims(), WithShape(tC.inputLengthsShape...), WithInit(tC.inputLengthsInit), WithName(\"inputLengths\"))\n\t\t\ttargetLengths := NewTensor(g, tensor.Int, tC.targetLengthsShape.Dims(), WithShape(tC.targetLengthsShape...), WithInit(tC.targetLengthsInit), WithName(\"targetLengths\"))\n\n\t\t\tval, err := CTCLoss(logProbs, targets, inputLengths, targetLengths, tC.reduction)\n\t\t\tac.NoError(err)\n\n\t\t\t_, err = Grad(val, logProbs)\n\t\t\tac.NoError(err)\n\n\t\t\tvm := NewTapeMachine(g)\n\t\t\tac.NoError(vm.RunAll())\n\n\t\t\tac.Equal(tC.expectedOutput.Shape(), val.Shape())\n\t\t\tac.InDelta(tC.expectedOutput.Data(), val.Value().Data(), 1e-5, \"actual: %#v\", val.Value().Data())\n\n\t\t\tt.Logf(\"dx: %#v\", logProbs.Deriv().Value().Data())\n\n\t\t\tac.Equal(tC.expectedInputGrad.Shape(), logProbs.Deriv().Shape())\n\t\t\tac.InDeltaSlice(tC.expectedInputGrad.Data(), logProbs.Deriv().Value().Data(), 1e-5, \"actual: %#v\", logProbs.Deriv().Value().Data())\n\t\t})\n\t}\n}\n\nfunc BenchmarkCTCLossForward(b *testing.B) {\n\ttestCases := []struct {\n\t\tDtype tensor.Dtype\n\n\t\treduction Reduction\n\n\t\tlogProbsInit  InitWFn\n\t\tlogProbsShape tensor.Shape\n\n\t\ttargetsInit  InitWFn\n\t\ttargetsShape tensor.Shape\n\n\t\tinputLengthsInit  InitWFn\n\t\tinputLengthsShape tensor.Shape\n\n\t\ttargetLengthsInit  InitWFn\n\t\ttargetLengthsShape tensor.Shape\n\t}{\n\t\t{\n\t\t\tDtype:              Float64,\n\t\t\treduction:          ReductionMean,\n\t\t\tlogProbsInit:       RangedFromWithStep(0.0, 0.01),\n\t\t\tlogProbsShape:      tensor.Shape{4, 4, 4},\n\t\t\ttargetsInit:        RangedFromWithStep(2, 0),\n\t\t\ttargetsShape:       tensor.Shape{4, 4},\n\t\t\tinputLengthsInit:   RangedFromWithStep(4, 0),\n\t\t\tinputLengthsShape:  tensor.Shape{4},\n\t\t\ttargetLengthsInit:  RangedFromWithStep(2, 0),\n\t\t\ttargetLengthsShape: tensor.Shape{4},\n\t\t},\n\t\t{\n\t\t\tDtype:              Float64,\n\t\t\treduction:          ReductionMean,\n\t\t\tlogProbsInit:       RangedFromWithStep(0.0, 0.01),\n\t\t\tlogProbsShape:      tensor.Shape{4, 1024, 4},\n\t\t\ttargetsInit:        RangedFromWithStep(2, 0),\n\t\t\ttargetsShape:       tensor.Shape{1024, 4},\n\t\t\tinputLengthsInit:   RangedFromWithStep(4, 0),\n\t\t\tinputLengthsShape:  tensor.Shape{1024},\n\t\t\ttargetLengthsInit:  RangedFromWithStep(2, 0),\n\t\t\ttargetLengthsShape: tensor.Shape{1024},\n\t\t},\n\t\t{\n\t\t\tDtype:              Float64,\n\t\t\treduction:          ReductionMean,\n\t\t\tlogProbsInit:       RangedFromWithStep(0.0, 0.01),\n\t\t\tlogProbsShape:      tensor.Shape{4, 1024, 1024},\n\t\t\ttargetsInit:        RangedFromWithStep(2, 0),\n\t\t\ttargetsShape:       tensor.Shape{1024, 4},\n\t\t\tinputLengthsInit:   RangedFromWithStep(4, 0),\n\t\t\tinputLengthsShape:  tensor.Shape{1024},\n\t\t\ttargetLengthsInit:  RangedFromWithStep(2, 0),\n\t\t\ttargetLengthsShape: tensor.Shape{1024},\n\t\t},\n\t\t{\n\t\t\tDtype:              Float64,\n\t\t\treduction:          ReductionMean,\n\t\t\tlogProbsInit:       RangedFromWithStep(0.0, 0.01),\n\t\t\tlogProbsShape:      tensor.Shape{4, 2048, 8},\n\t\t\ttargetsInit:        RangedFromWithStep(2, 0),\n\t\t\ttargetsShape:       tensor.Shape{2048, 4},\n\t\t\tinputLengthsInit:   RangedFromWithStep(4, 0),\n\t\t\tinputLengthsShape:  tensor.Shape{2048},\n\t\t\ttargetLengthsInit:  RangedFromWithStep(2, 0),\n\t\t\ttargetLengthsShape: tensor.Shape{2048},\n\t\t},\n\t}\n\n\tfor i, tC := range testCases {\n\t\top := newCTCLossOp(tC.Dtype, tC.targetsShape.Dims(), ReductionSum)\n\n\t\tlogsProbs := tensor.New(\n\t\t\ttensor.WithShape(tC.logProbsShape...),\n\t\t\ttensor.WithBacking(tC.logProbsInit(op.dtype, tC.logProbsShape...)),\n\t\t)\n\t\ttargets := tensor.New(\n\t\t\ttensor.WithShape(tC.targetsShape...),\n\t\t\ttensor.WithBacking(tC.targetsInit(Int, tC.targetsShape...)),\n\t\t)\n\t\tinputLengths := tensor.New(\n\t\t\ttensor.WithShape(tC.inputLengthsShape...),\n\t\t\ttensor.WithBacking(tC.inputLengthsInit(Int, tC.inputLengthsShape...)),\n\t\t)\n\t\ttargetLengths := tensor.New(\n\t\t\ttensor.WithShape(tC.targetLengthsShape...),\n\t\t\ttensor.WithBacking(tC.targetLengthsInit(Int, tC.targetLengthsShape...)),\n\t\t)\n\n\t\tb.Run(fmt.Sprintf(\"Benchmark #%v %v (%v)\", i+1, tC.Dtype, tC.logProbsShape), func(b *testing.B) {\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t_, err := op.Do(logsProbs, targets, inputLengths, targetLengths)\n\t\t\t\tb.StopTimer()\n\t\t\t\tif err != nil {\n\t\t\t\t\tpanic(err)\n\t\t\t\t}\n\t\t\t\tb.StartTimer()\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "op_dropout.go",
          "type": "blob",
          "size": 6.802734375,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"hash\"\n\t\"hash/fnv\"\n\t\"log\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\ntype randomGenF func() float64\n\ntype dropoutOp struct {\n\tprobability float64\n\tisTraining  bool\n\trndGen      randomGenF\n}\n\nfunc newDropoutOp(probability float64, rndGen randomGenF) *dropoutOp {\n\tdropoutop := &dropoutOp{\n\t\tprobability: probability,\n\t\tisTraining:  true,\n\t\trndGen:      rndGen,\n\t}\n\n\treturn dropoutop\n}\n\nfunc (op *dropoutOp) SetTraining(isTraining bool) error { op.isTraining = isTraining; return nil }\n\nfunc (op *dropoutOp) Arity() int { return 1 }\n\nfunc (op *dropoutOp) ReturnsPtr() bool { return false }\n\nfunc (op *dropoutOp) CallsExtern() bool { return false }\n\nfunc (op *dropoutOp) WriteHash(h hash.Hash) { fmt.Fprintf(h, op.String()) }\n\nfunc (op *dropoutOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op *dropoutOp) String() string { return fmt.Sprintf(\"Dropout{training=%v}()\", op.isTraining) }\n\nfunc (op *dropoutOp) InferShape(inputs ...DimSizer) (tensor.Shape, error) {\n\treturn inputs[0].(tensor.Shape), nil\n}\n\nfunc (op *dropoutOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\treturn hm.NewFnType(a, a) // f(float64) float64\n}\n\nfunc (op *dropoutOp) OverwritesInput() int { return -1 }\n\nfunc (op *dropoutOp) checkInput(inputs ...Value) (tensor.Tensor, error) {\n\tif err := checkArity(op, len(inputs)); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tin tensor.Tensor\n\t\tok bool\n\t)\n\n\tif in, ok = inputs[0].(tensor.Tensor); !ok {\n\t\treturn nil, errors.Errorf(\"Expected input to be a tensor\")\n\t}\n\n\treturn in, nil\n}\n\nfunc (op *dropoutOp) Do(inputs ...Value) (retVal Value, err error) {\n\tinputTensor, err := op.checkInput(inputs...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Can't check Dropout input: %w\", err)\n\t}\n\n\tif op.probability == 0.0 || !op.isTraining {\n\t\treturn inputTensor, nil\n\t}\n\n\tret := tensor.New(tensor.WithShape(inputTensor.Shape().Clone()...), tensor.Of(inputTensor.Dtype()))\n\n\top.do(inputTensor.Data(), ret.Data())\n\n\treturn ret, nil\n}\n\nfunc (op *dropoutOp) UsePreallocDo(prealloc Value, inputs ...Value) (Value, error) {\n\tinputTensor, err := op.checkInput(inputs...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Can't check Dropout input: %w\", err)\n\t}\n\n\tif op.probability == 0.0 || !op.isTraining {\n\t\treturn inputTensor, nil\n\t}\n\n\top.do(inputTensor.Data, prealloc.Data())\n\n\treturn prealloc, nil\n}\n\nfunc (op *dropoutOp) do(input, output interface{}) {\n\tkeepProb := 1.0 - op.probability\n\n\tswitch v := input.(type) {\n\tcase []float32:\n\t\toutputV := output.([]float32)\n\t\tfor i, d := range v {\n\t\t\tr := float32(op.rndGen())\n\t\t\tif r < float32(keepProb) {\n\t\t\t\toutputV[i] = d / float32(keepProb)\n\t\t\t}\n\t\t}\n\tcase []float64:\n\t\toutputV := output.([]float64)\n\t\tfor i, d := range v {\n\t\t\tr := op.rndGen()\n\t\t\tif r < keepProb {\n\t\t\t\toutputV[i] = d / keepProb\n\t\t\t}\n\t\t}\n\tdefault:\n\t\tlog.Panicf(\"unknown dtype: %T\", output)\n\t}\n}\n\n// DoDiff calculates the diff and sets its value to the output node. Implementation for ADOp interface.\nfunc (op *dropoutOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) error {\n\tif len(inputs) != 1 {\n\t\treturn fmt.Errorf(\"dropout diff requires 1 arguments\")\n\t}\n\n\tdiff := &dropoutDiffOp{op}\n\txdv, ydv := getDV(inputs[0], output)\n\n\t_, err := diff.UsePreallocDo(xdv.d, xdv.Value, output.Value(), ydv.d)\n\n\treturn err\n}\n\n// SymDiff applies the diff op. Implementation for SDOp interface.\nfunc (op *dropoutOp) SymDiff(inputs Nodes, output, grad *Node) (Nodes, error) {\n\terr := checkArity(op, len(inputs))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tinput := inputs[0]\n\tdiff := &dropoutDiffOp{op}\n\n\tret, err := ApplyOp(diff, input, output, grad)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn Nodes{ret}, nil\n}\n\n// DiffWRT is an implementation for the SDOp interface\nfunc (op *dropoutOp) DiffWRT(inputs int) []bool {\n\tif inputs != 1 {\n\t\tpanic(fmt.Sprintf(\"dropout operator only supports one input, got %d instead\", inputs))\n\t}\n\n\treturn []bool{true}\n}\n\ntype dropoutDiffOp struct {\n\t*dropoutOp\n}\n\n// Arity returns 1\nfunc (op dropoutDiffOp) Arity() int { return 3 }\n\n// Type returns a → a\nfunc (op dropoutDiffOp) Type() hm.Type {\n\tt := hm.TypeVariable('a')\n\treturn hm.NewFnType(t, t, t, t)\n}\n\n// InferShape returns the output shape as a function of the inputs\nfunc (op dropoutDiffOp) InferShape(ds ...DimSizer) (tensor.Shape, error) {\n\treturn ds[0].(tensor.Shape), nil\n}\n\n// Do executes the op\nfunc (op *dropoutDiffOp) Do(values ...Value) (Value, error) {\n\tinput := values[0].(*tensor.Dense)\n\toutput := values[1].(*tensor.Dense)\n\tgrad := values[2].(*tensor.Dense)\n\n\tdy, err := CloneValue(input)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tv, err := op.UsePreallocDo(dy, input, output, grad)\n\n\treturn v, err\n}\n\nfunc (op *dropoutDiffOp) UsePreallocDo(prealloc Value, inputs ...Value) (retVal Value, err error) {\n\tinput := inputs[0].(*tensor.Dense)\n\tresult := prealloc.(*tensor.Dense)\n\toutput := inputs[1].(*tensor.Dense)\n\toutGrad := inputs[2].(*tensor.Dense)\n\n\tprobability := op.probability\n\n\tswitch input.Dtype() {\n\tcase Float64:\n\t\tdy := outGrad.Float64s()\n\t\tdx := result.Float64s()\n\t\toutputA := output.Float64s()\n\n\t\tfor i := 0; i < len(dy); i++ {\n\t\t\tif probability != 0 && outputA[i] != 0 {\n\t\t\t\tdx[i] = dy[i] / probability\n\t\t\t} else {\n\t\t\t\tdx[i] = 0.0\n\t\t\t}\n\t\t}\n\tcase Float32:\n\t\tdy := outGrad.Float32s()\n\t\tdx := result.Float32s()\n\t\toutputA := output.Float32s()\n\n\t\tfor i := 0; i < len(dy); i++ {\n\t\t\tif probability != 0 && outputA[i] != 0 {\n\t\t\t\tdx[i] = dy[i] / float32(probability)\n\t\t\t} else {\n\t\t\t\tdx[i] = 0.0\n\t\t\t}\n\t\t}\n\t}\n\n\treturn prealloc, nil\n}\n\n// ReturnsPtr indicates if the Op will return a pointer (allowing possible inplace edits) or by value\n// if it's false, the return value of the Op will be a copy of its input\nfunc (op dropoutDiffOp) ReturnsPtr() bool { return true }\n\n// CallsExtern returns false.\nfunc (op dropoutDiffOp) CallsExtern() bool { return false }\n\n// OverwritesInput is a method which states which input the output will be overwriting.\n// This allows for some efficiency gains as the underlying arrays wouldn't have to be re-allocated.\n// The method returns an int instead of a bool because potentially different operations may be allowed\n// to overwrite certain inputs. For example, consider an operation to increment a value:\n// the IncrementOp would be a unary operator, and assuming we would like to overwrite the input,\n// the retVal of overwriteInput() will be 0 (inputs[0]).\n// -1 is returned if overwriting of input is disallowed\nfunc (op dropoutDiffOp) OverwritesInput() int { return -1 }\n\n/* Other methods */\n\nfunc (op dropoutDiffOp) WriteHash(h hash.Hash) { h.Write([]byte(op.String())) }\n\nfunc (op dropoutDiffOp) Hashcode() uint32 {\n\th := fnv.New32a()\n\top.WriteHash(h)\n\treturn h.Sum32()\n}\n\nfunc (op dropoutDiffOp) String() string { return fmt.Sprintf(\"dropoutDiffOp(%f)\", op.probability) }\n\n// ensure it complies with the Op interface\nvar (\n\t_ Op          = &dropoutOp{}\n\t_ ADOp        = &dropoutOp{}\n\t_ SDOp        = &dropoutOp{}\n\t_ TrainModeOp = &dropoutOp{}\n)\n"
        },
        {
          "name": "op_dropout_test.go",
          "type": "blob",
          "size": 3.275390625,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"runtime\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc TestDropoutAll(t *testing.T) {\n\tvar tests = []struct {\n\t\tdt                tensor.Dtype\n\t\tprob              float64\n\t\trand              []float64\n\t\texpected          interface{}\n\t\texpectedGrad      interface{}\n\t\texpectedInputGrad interface{}\n\t}{\n\t\t{Float64, 0.0, []float64{0.0, 0.2, 0.5, 0.8, 1.0}, []float64{1.0, 1.0, 1.0, 1.0, 1.0}, []float64{0.2, 0.2, 0.2, 0.2, 0.2}, []float64{0, 0, 0, 0, 0}},\n\t\t{Float64, 0.2, []float64{0.0, 0.2, 0.5, 0.8, 1.0}, []float64{1.25, 1.25, 1.25, 0.0, 0.0}, []float64{0.2, 0.2, 0.2, 0.2, 0.2}, []float64{1, 1, 1, 0.0, 0.0}},\n\t\t{Float64, 0.5, []float64{0.0, 0.2, 0.5, 0.8, 1.0}, []float64{2.0, 2.0, 0.0, 0.0, 0.0}, []float64{0.2, 0.2, 0.2, 0.2, 0.2}, []float64{0.4, 0.4, 0, 0, 0}},\n\t\t{Float32, 0.2, []float64{0.0, 0.2, 0.5, 0.8, 1.0}, []float32{1.25, 1.25, 1.25, 0.0, 0.0}, []float32{0.2, 0.2, 0.2, 0.2, 0.2}, []float32{1, 1, 1, 0, 0}},\n\t\t{Float32, 0.5, []float64{0.0, 0.2, 0.5, 0.8, 1.0}, []float32{2.0, 2.0, 0.0, 0.0, 0.0}, []float32{0.2, 0.2, 0.2, 0.2, 0.2}, []float32{0.4, 0.4, 0, 0, 0}},\n\t}\n\n\tfor _, tt := range tests {\n\t\tname := fmt.Sprintf(\"%v-%.1f\", tt.dt, tt.prob)\n\t\tt.Run(name, func(t *testing.T) {\n\t\t\trandCount := 0\n\t\t\trandFn := func() float64 {\n\t\t\t\tv := tt.rand[randCount%len(tt.rand)]\n\t\t\t\trandCount++\n\n\t\t\t\treturn v\n\t\t\t}\n\n\t\t\tg := NewGraph()\n\t\t\tx := NewVector(g, tt.dt, WithShape(5), WithName(\"x\"), WithInit(Ones()))\n\n\t\t\ty, err := ApplyOp(newDropoutOp(tt.prob, randFn), x)\n\t\t\tassert.NoError(t, err)\n\n\t\t\tcost, _ := Mean(y)\n\t\t\tif _, err := Grad(cost, x); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tm := NewTapeMachine(g, BindDualValues())\n\t\t\tdefer m.Close()\n\t\t\tdefer runtime.GC()\n\n\t\t\trequire.NoError(t, m.RunAll())\n\t\t\tassert.Equal(t, tt.expected, y.Value().Data())\n\n\t\t\tyGrad, err := y.Grad()\n\t\t\trequire.NoError(t, err)\n\t\t\tassert.Equal(t, tt.expectedGrad, yGrad.Data())\n\n\t\t\txGrad, err := x.Grad()\n\t\t\trequire.NoError(t, err)\n\t\t\tassert.Equal(t, tt.expectedInputGrad, xGrad.Data())\n\t\t})\n\t}\n}\n\nfunc dropoutTest(t *testing.T, dt tensor.Dtype) error {\n\tg := NewGraph()\n\tx := NewVector(g, dt, WithShape(10), WithName(\"x\"), WithInit(RangedFrom(0)))\n\tw := NewMatrix(g, dt, WithShape(20, 10), WithName(\"w\"), WithInit(RangedFrom(0)))\n\tw2 := NewMatrix(g, dt, WithShape(10, 20), WithName(\"w2\"), WithInit(RangedFrom(0)))\n\twx := Must(Mul(w, x))\n\tact := Must(Cube(wx))\n\tdo := Must(Dropout(act, 0.5))\n\n\tact2 := Must(Cube(Must(Mul(w2, do))))\n\tdo2 := Must(Dropout(act2, 0.1))\n\tcost := Must(Sum(do2))\n\n\t_, err := Grad(cost, x, w, w2)\n\n\tif err != nil {\n\t\tioutil.WriteFile(\"fullGraph.dot\", []byte(g.ToDot()), 0644)\n\t\t// t.Fatalf(\"%+v\", err)\n\t\treturn err\n\t}\n\n\t// logger := log.New(os.Stderr, \"\", 0)\n\n\t// m := NewTapeMachine(g, TraceExec(), BindDualValues(), WithLogger(logger), WithWatchlist())\n\tm := NewTapeMachine(g, TraceExec(), BindDualValues())\n\tdefer m.Close()\n\tcudaLogf(\"%v\", m.Prog())\n\tdefer runtime.GC()\n\tif err := m.RunAll(); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc TestDropout_integration(t *testing.T) {\n\tif err := dropoutTest(t, Float64); err != nil {\n\t\tt.Errorf(\"%+v\", err)\n\t}\n\n\tif err := dropoutTest(t, Float32); err != nil {\n\t\tt.Errorf(\"%+v\", err)\n\t}\n\n\t// visual inspection\n\t// ioutil.WriteFile(\"fullGraph.dot\", []byte(g.ToDot()), 0644)\n}\n"
        },
        {
          "name": "op_group_norm.go",
          "type": "blob",
          "size": 17.353515625,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"hash\"\n\t\"log\"\n\t\"math\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/chewxy/math32\"\n\t\"gorgonia.org/tensor\"\n\t\"gorgonia.org/vecf32\"\n\t\"gorgonia.org/vecf64\"\n)\n\nconst (\n\tgroupNormChunkSize = 16\n\tgroupNormVecSize   = 8\n)\n\nfunc GroupNorm(x, scale, bias *Node, numGroups, numChannels int, epsilon float64) (*Node, error) {\n\txShape := x.Shape()\n\n\tmean := tensor.New(\n\t\ttensor.Of(x.Dtype()),\n\t\ttensor.WithShape(xShape[0]*numGroups),\n\t)\n\n\trstd := tensor.New(\n\t\ttensor.Of(x.Dtype()),\n\t\ttensor.WithShape(xShape[0]*numGroups),\n\t)\n\n\top := &GroupNormOp{\n\t\tnumGroups:   numGroups,\n\t\tnumChannels: numChannels,\n\t\tepsilon:     epsilon,\n\t\tmean:        mean,\n\t\trstd:        rstd,\n\t}\n\n\tresult, err := ApplyOp(op, x)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif result, err = Auto(BroadcastHadamardProd, scale, result); err != nil {\n\t\treturn nil, err\n\t}\n\n\tresult, err = Auto(BroadcastAdd, result, bias)\n\n\treturn result, err\n}\n\ntype GroupNormOp struct {\n\tnumGroups, numChannels int\n\tepsilon                float64\n\n\t// cache\n\tmean, rstd *tensor.Dense\n}\n\nfunc (op *GroupNormOp) Arity() int { return 1 }\n\nfunc (op *GroupNormOp) ReturnsPtr() bool { return false }\n\nfunc (op *GroupNormOp) CallsExtern() bool { return false }\n\nfunc (op *GroupNormOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, op.String())\n}\n\nfunc (op *GroupNormOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op *GroupNormOp) String() string {\n\treturn fmt.Sprintf(\"GroupNorm{%d, %v}()\", op.numGroups, op.numChannels)\n}\n\nfunc (op *GroupNormOp) InferShape(inputs ...DimSizer) (tensor.Shape, error) {\n\ts := inputs[0].(tensor.Shape).Clone()\n\n\treturn s, nil\n}\n\nfunc (op *GroupNormOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\n\treturn hm.NewFnType(a, a) // f(float64) float64\n}\n\nfunc (op *GroupNormOp) OverwritesInput() int { return -1 }\n\nfunc (op *GroupNormOp) Do(inputs ...Value) (Value, error) {\n\tinput := inputs[0]\n\tprealloc := tensor.New(tensor.WithShape(input.Shape().Clone()...), tensor.Of(input.Dtype()))\n\n\treturn op.UsePreallocDo(prealloc, inputs...)\n}\n\nfunc (op *GroupNormOp) UsePreallocDo(prealloc Value, inputs ...Value) (Value, error) {\n\tif err := checkArity(op, len(inputs)); err != nil {\n\t\treturn nil, err\n\t}\n\n\txT := inputs[0].(*tensor.Dense)\n\txShape := xT.Shape()\n\n\tbatchSize := xShape[0]\n\tchannels := xShape[1]\n\timageSize := 1\n\tif len(xShape) > 2 {\n\t\timageSize = tensor.Shape(xShape[2:]).TotalSize()\n\t}\n\n\tswitch xT.Dtype() {\n\tcase Float64:\n\t\treturn op.f64s(xT, prealloc.(*tensor.Dense), batchSize, channels, imageSize)\n\tcase Float32:\n\t\treturn op.f32s(xT, prealloc.(*tensor.Dense), batchSize, channels, imageSize)\n\t}\n\n\treturn nil, nyi(\"groupNormOp\", \"Do\")\n}\n\nfunc (op *GroupNormOp) f64s(xT, prealloc *tensor.Dense, batchSize, channels, imageSize int) (Value, error) {\n\td := channels / op.numGroups\n\tinnerSize := d * imageSize\n\n\tx := xT.Float64s()\n\ty := prealloc.Float64s()\n\n\tmeanA := op.mean.Float64s()\n\trstdA := op.rstd.Float64s()\n\n\trunInParallel(0, op.numGroups*batchSize, func(i int) {\n\t\tbaseIndex := i * innerSize\n\t\txSection := x[baseIndex : baseIndex+innerSize]\n\n\t\tmean, rstd := op.rowwiseMomentsF64(xSection, innerSize, 0)\n\t\trstd = 1 / math.Sqrt(math.Max(rstd, 0)+op.epsilon)\n\n\t\tfor j := 0; j < d; j++ {\n\t\t\tscale := rstd\n\t\t\tbias := -scale * mean\n\n\t\t\tbaseIndex := (i*d + j) * imageSize\n\t\t\txSection := x[baseIndex : baseIndex+imageSize]\n\t\t\tySection := y[baseIndex : baseIndex+imageSize]\n\n\t\t\tfor k := 0; k < imageSize; k++ {\n\t\t\t\tySection[k] = scale*xSection[k] + bias\n\t\t\t}\n\t\t}\n\n\t\tmeanA[i] = mean\n\t\trstdA[i] = rstd\n\t})\n\n\treturn prealloc, nil\n}\n\nfunc (op *GroupNormOp) rowwiseMomentsF64(x []float64, n int, ddof int) (mean float64, variance float64) {\n\tnn := n / groupNormVecSize\n\tm := (nn + groupNormChunkSize - 1) / groupNormChunkSize\n\tdepth := op.ceilLog2F64(m)\n\n\tm0stk := make([]int, depth)\n\tm1stk := make([][]float64, depth)\n\tm2stk := make([][]float64, depth)\n\n\tfor i := 0; i < depth; i++ {\n\t\tm1stk[i] = make([]float64, groupNormVecSize)\n\t\tm2stk[i] = make([]float64, groupNormVecSize)\n\t}\n\n\tfor i := 0; i < m; i++ {\n\t\tm0 := int(math.Min(groupNormChunkSize, float64(nn-i*groupNormChunkSize)))\n\n\t\txSection1 := x[i*groupNormChunkSize*groupNormVecSize:]\n\n\t\t// TODO: optimize allocs\n\t\tm1vec := make([]float64, groupNormVecSize)\n\t\tm2vec := make([]float64, groupNormVecSize)\n\t\tdelta := make([]float64, groupNormVecSize)\n\t\ttmp := make([]float64, groupNormVecSize)\n\n\t\tfor j := 0; j < m0; j++ {\n\t\t\tbaseIndex := j * groupNormVecSize\n\t\t\txSection2 := xSection1[baseIndex : baseIndex+groupNormVecSize]\n\n\t\t\tcopy(tmp, xSection2)\n\t\t\tvecf64.Sub(tmp, m1vec)\n\n\t\t\tc := 1.0 / float64(j+1)\n\n\t\t\t// update m1vec\n\t\t\tcopy(delta, tmp)\n\t\t\tvecf64.Scale(tmp, c)\n\t\t\tvecf64.Add(m1vec, tmp)\n\n\t\t\t// update m2vec\n\t\t\tcopy(tmp, xSection2)\n\t\t\tvecf64.Sub(tmp, m1vec)\n\t\t\tvecf64.Mul(tmp, delta)\n\t\t\tvecf64.Add(m2vec, tmp)\n\t\t}\n\n\t\top.addMomentsVecF64(m0, m1vec, m2vec, &m0stk[0], m1stk[0], m2stk[0])\n\n\t\tmask := i + 1\n\t\tfor j := 1; j < depth && (mask&1 == 0); j++ {\n\t\t\top.addMomentsVecF64(m0stk[j-1], m1stk[j-1], m2stk[j-1], &m0stk[j], m1stk[j], m2stk[j])\n\t\t\tm0stk[j-1] = 0\n\t\t\tm1stk[j-1] = make([]float64, groupNormVecSize) // is this optimized by the compiler?\n\t\t\tm2stk[j-1] = make([]float64, groupNormVecSize)\n\t\t\tmask >>= 1\n\t\t}\n\t}\n\n\tfor i := 1; i < depth; i++ {\n\t\top.addMomentsVecF64(m0stk[i], m1stk[i], m2stk[i], &m0stk[0], m1stk[0], m1stk[0])\n\t}\n\n\tvar (\n\t\tm0     int\n\t\tm1, m2 float64\n\t)\n\n\tfor i := nn * groupNormVecSize; i < n; i++ {\n\t\tdelta := x[i] - m1\n\t\tm0++\n\t\tm1 += delta / float64(m0)\n\t\tm2 += delta * (x[i] - m1)\n\t}\n\n\tfor i := 0; i < groupNormVecSize; i++ {\n\t\top.addMomentsF64(nn, m1stk[0][i], m2stk[0][i], &m0, &m1, &m2)\n\t}\n\n\treturn m1, m2 / float64(n-ddof)\n}\n\nfunc (op *GroupNormOp) addMomentsF64(m0add int, m1add, m2add float64, m0 *int, m1, m2 *float64) {\n\tn := *m0 + m0add\n\tc := 0.0\n\tif n != 0 {\n\t\tc = float64(m0add) / float64(n)\n\t}\n\n\tdelta := m1add - *m1\n\n\t*m1 += c * delta\n\t*m2 += m2add + delta*delta*c*float64(*m0)\n\t*m0 = n\n}\n\nfunc (op *GroupNormOp) addMomentsVecF64(m0add int, m1add, m2add []float64, m0 *int, m1, m2 []float64) {\n\tn := *m0 + m0add\n\tc := 0.0\n\tif n != 0 {\n\t\tc = float64(m0add) / float64(n)\n\t}\n\n\tdelta := make([]float64, len(m1add))\n\tcopy(delta, m1add)\n\tvecf64.Sub(delta, m1)\n\n\t// update m1\n\ttmp := make([]float64, len(delta))\n\tcopy(tmp, delta)\n\tvecf64.Scale(tmp, c) // delta * c\n\tvecf64.Add(m1, tmp)\n\n\t// update m2\n\tcopy(tmp, delta)\n\tvecf64.Mul(tmp, delta) // delta * delta\n\tvecf64.Scale(tmp, c)\n\tvecf64.Scale(tmp, float64(*m0))\n\tvecf64.Add(tmp, m2add)\n\tvecf64.Add(m2, tmp)\n\n\t// update m0\n\t*m0 = n\n}\n\nfunc (op *GroupNormOp) ceilLog2F64(x int) int {\n\tif x <= 2 {\n\t\treturn 1\n\t}\n\n\treturn int(math.Ceil(math.Log2(float64(x))))\n}\n\nfunc (op *GroupNormOp) f32s(xT, prealloc *tensor.Dense, batchSize, channels, imageSize int) (Value, error) {\n\td := channels / op.numGroups\n\tinnerSize := d * imageSize\n\n\tx := xT.Float32s()\n\ty := prealloc.Float32s()\n\n\tmeanA := op.mean.Float32s()\n\trstdA := op.rstd.Float32s()\n\n\trunInParallel(0, op.numGroups*batchSize, func(i int) {\n\t\tbaseIndex := i * innerSize\n\t\txSection := x[baseIndex : baseIndex+innerSize]\n\n\t\tmean, rstd := op.rowwiseMomentsF32(xSection, innerSize, 0)\n\t\trstd = 1 / math32.Sqrt(math32.Max(rstd, 0)+float32(op.epsilon))\n\n\t\tfor j := 0; j < d; j++ {\n\t\t\tscale := rstd\n\t\t\tbias := -scale * mean\n\n\t\t\tbaseIndex := (i*d + j) * imageSize\n\t\t\txSection := x[baseIndex : baseIndex+imageSize]\n\t\t\tySection := y[baseIndex : baseIndex+imageSize]\n\n\t\t\tfor k := 0; k < imageSize; k++ {\n\t\t\t\tySection[k] = scale*xSection[k] + bias\n\t\t\t}\n\t\t}\n\n\t\tmeanA[i] = mean\n\t\trstdA[i] = rstd\n\t})\n\n\treturn prealloc, nil\n}\n\nfunc (op *GroupNormOp) rowwiseMomentsF32(x []float32, n int, ddof int) (mean float32, variance float32) {\n\tnn := n / groupNormVecSize\n\tm := (nn + groupNormChunkSize - 1) / groupNormChunkSize\n\tdepth := op.ceilLog2F32(m)\n\n\tm0stk := make([]int, depth)\n\tm1stk := make([][]float32, depth)\n\tm2stk := make([][]float32, depth)\n\n\tfor i := 0; i < depth; i++ {\n\t\tm1stk[i] = make([]float32, groupNormVecSize)\n\t\tm2stk[i] = make([]float32, groupNormVecSize)\n\t}\n\n\tfor i := 0; i < m; i++ {\n\t\tm0 := int(math32.Min(groupNormChunkSize, float32(nn-i*groupNormChunkSize)))\n\n\t\txSection1 := x[i*groupNormChunkSize*groupNormVecSize:]\n\n\t\t// TODO: optimize allocs\n\t\tm1vec := make([]float32, groupNormVecSize)\n\t\tm2vec := make([]float32, groupNormVecSize)\n\t\tdelta := make([]float32, groupNormVecSize)\n\t\ttmp := make([]float32, groupNormVecSize)\n\n\t\tfor j := 0; j < m0; j++ {\n\t\t\tbaseIndex := j * groupNormVecSize\n\t\t\txSection2 := xSection1[baseIndex : baseIndex+groupNormVecSize]\n\n\t\t\tcopy(tmp, xSection2)\n\t\t\tvecf32.Sub(tmp, m1vec)\n\n\t\t\tc := 1.0 / float32(j+1)\n\n\t\t\t// update m1vec\n\t\t\tcopy(delta, tmp)\n\t\t\tvecf32.Scale(tmp, c)\n\t\t\tvecf32.Add(m1vec, tmp)\n\n\t\t\t// update m2vec\n\t\t\tcopy(tmp, xSection2)\n\t\t\tvecf32.Sub(tmp, m1vec)\n\t\t\tvecf32.Mul(tmp, delta)\n\t\t\tvecf32.Add(m2vec, tmp)\n\t\t}\n\n\t\top.addMomentsVecF32(m0, m1vec, m2vec, &m0stk[0], m1stk[0], m2stk[0])\n\n\t\tmask := i + 1\n\t\tfor j := 1; j < depth && (mask&1 == 0); j++ {\n\t\t\top.addMomentsVecF32(m0stk[j-1], m1stk[j-1], m2stk[j-1], &m0stk[j], m1stk[j], m2stk[j])\n\t\t\tm0stk[j-1] = 0\n\t\t\tm1stk[j-1] = make([]float32, groupNormVecSize) // is this optimized by the compiler?\n\t\t\tm2stk[j-1] = make([]float32, groupNormVecSize)\n\t\t\tmask >>= 1\n\t\t}\n\t}\n\n\tfor i := 1; i < depth; i++ {\n\t\top.addMomentsVecF32(m0stk[i], m1stk[i], m2stk[i], &m0stk[0], m1stk[0], m1stk[0])\n\t}\n\n\tvar (\n\t\tm0     int\n\t\tm1, m2 float32\n\t)\n\n\tfor i := nn * groupNormVecSize; i < n; i++ {\n\t\tdelta := x[i] - m1\n\t\tm0++\n\t\tm1 += delta / float32(m0)\n\t\tm2 += delta * (x[i] - m1)\n\t}\n\n\tfor i := 0; i < groupNormVecSize; i++ {\n\t\top.addMomentsF32(nn, m1stk[0][i], m2stk[0][i], &m0, &m1, &m2)\n\t}\n\n\treturn m1, m2 / float32(n-ddof)\n}\n\nfunc (op *GroupNormOp) addMomentsF32(m0add int, m1add, m2add float32, m0 *int, m1, m2 *float32) {\n\tn := *m0 + m0add\n\tc := float32(0.0)\n\tif n != 0 {\n\t\tc = float32(m0add) / float32(n)\n\t}\n\n\tdelta := m1add - *m1\n\n\t*m1 += c * delta\n\t*m2 += m2add + delta*delta*c*float32(*m0)\n\t*m0 = n\n}\n\nfunc (op *GroupNormOp) addMomentsVecF32(m0add int, m1add, m2add []float32, m0 *int, m1, m2 []float32) {\n\tn := *m0 + m0add\n\tc := float32(0.0)\n\tif n != 0 {\n\t\tc = float32(m0add) / float32(n)\n\t}\n\n\tdelta := make([]float32, len(m1add))\n\tcopy(delta, m1add)\n\tvecf32.Sub(delta, m1)\n\n\t// update m1\n\ttmp := make([]float32, len(delta))\n\tcopy(tmp, delta)\n\tvecf32.Scale(tmp, c) // delta * c\n\tvecf32.Add(m1, tmp)\n\n\t// update m2\n\tcopy(tmp, delta)\n\tvecf32.Mul(tmp, delta) // delta * delta\n\tvecf32.Scale(tmp, c)\n\tvecf32.Scale(tmp, float32(*m0))\n\tvecf32.Add(tmp, m2add)\n\tvecf32.Add(m2, tmp)\n\n\t// update m0\n\t*m0 = n\n}\n\nfunc (op *GroupNormOp) ceilLog2F32(x int) int {\n\tif x <= 2 {\n\t\treturn 1\n\t}\n\n\treturn int(math32.Ceil(math32.Log2(float32(x))))\n}\n\n// DoDiff calculates the diff and sets its value to the output node. Implementation for ADOp interface.\nfunc (op *GroupNormOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) error {\n\tif len(inputs) != 1 {\n\t\treturn fmt.Errorf(\"GroupNorm.DoDiff needs 1 arguments\")\n\t}\n\n\todv := output.boundTo.(*dualValue)\n\tidv := inputs[0].boundTo.(*dualValue)\n\tidvd := idv.d.(*tensor.Dense)\n\tdiffOp := &groupNormDiffOp{op}\n\n\tresult, err := diffOp.Do(idv.Value, odv.Value, odv.d)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tsum, err := idvd.Add(result.(*tensor.Dense), tensor.UseUnsafe())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\todv.d = sum\n\n\treturn nil\n}\n\n// SymDiff applies the diff op. Implementation for SDOp interface.\nfunc (op *GroupNormOp) SymDiff(inputs Nodes, output, grad *Node) (Nodes, error) {\n\terr := checkArity(op, len(inputs))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tdiffOp := &groupNormDiffOp{op}\n\n\tdy, err := ApplyOp(diffOp, inputs[0], grad)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn Nodes{dy, nil, nil}, err\n}\n\n// DiffWRT is an implementation for the SDOp interface\nfunc (op *GroupNormOp) DiffWRT(inputs int) []bool {\n\treturn []bool{true}\n}\n\ntype groupNormDiffOp struct {\n\t*GroupNormOp\n}\n\nfunc (op *groupNormDiffOp) Arity() int { return 2 }\n\nfunc (op *groupNormDiffOp) ReturnsPtr() bool { return false }\n\nfunc (op *groupNormDiffOp) CallsExtern() bool { return false }\n\nfunc (op *groupNormDiffOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, op.String())\n}\n\nfunc (op *groupNormDiffOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op *groupNormDiffOp) String() string {\n\treturn fmt.Sprintf(\"groupNormDiff{%d, %v}()\", op.numGroups, op.numChannels)\n}\n\nfunc (op *groupNormDiffOp) InferShape(inputs ...DimSizer) (tensor.Shape, error) {\n\ts := inputs[0].(tensor.Shape).Clone()\n\n\treturn s, nil\n}\n\nfunc (op *groupNormDiffOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\n\treturn hm.NewFnType(a, a, a) // f(float64) float64\n}\n\nfunc (op *groupNormDiffOp) OverwritesInput() int { return -1 }\n\nfunc (op *groupNormDiffOp) Do(inputs ...Value) (Value, error) {\n\tinput := inputs[0]\n\tgrad := inputs[1]\n\n\tdy, err := CloneValue(input)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn op.UsePreallocDo(dy, input, grad)\n}\n\nfunc (op *groupNormDiffOp) UsePreallocDo(prealloc Value, inputs ...Value) (Value, error) {\n\tvar err error\n\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn nil, err\n\t}\n\n\tinput := inputs[0].(*tensor.Dense)\n\tbuffer := prealloc.(*tensor.Dense)\n\toutGrad := inputs[1].(*tensor.Dense)\n\n\tlog.Printf(\"outgrad: %v\", outGrad)\n\n\tswitch input.Dtype() {\n\tcase Float64:\n\t\terr = op.f64s(input, buffer, outGrad)\n\tcase Float32:\n\t\terr = op.f32s(input, buffer, outGrad)\n\tdefault:\n\t\treturn nil, nyi(\"batchnormDiffOp\", \"Do\")\n\t}\n\n\treturn prealloc, err\n}\n\nfunc (op *groupNormDiffOp) f64s(input, prealloc, outGrad *tensor.Dense) (err error) {\n\tin := input.Float64s()\n\tdx := prealloc.Float64s()\n\n\tmean := op.mean.Float64s()\n\trstd := op.rstd.Float64s()\n\n\tdy := outGrad.Float64s()\n\n\txShape := input.Shape()\n\n\tbatchSize := xShape[0]\n\tchannels := xShape[1]\n\timageSize := 1\n\tif len(xShape) > 2 {\n\t\timageSize = tensor.Shape(xShape[2:]).TotalSize()\n\t}\n\n\tds, db := op.computeInternalGradientsF64(batchSize, channels, imageSize, input, outGrad)\n\td := channels / op.numGroups\n\ts := 1.0 / float64(d*imageSize)\n\n\trunInParallel(0, batchSize*op.numGroups, func(i int) {\n\t\tbaseIndex := i * d\n\n\t\tdsSection := ds[baseIndex : baseIndex+d]\n\t\tdbSection := db[baseIndex : baseIndex+d]\n\n\t\tds := 0.0\n\t\tdb := 0.0\n\n\t\tfor j := 0; j < d; j++ {\n\t\t\tds += dsSection[j]\n\t\t\tdb += dbSection[j]\n\t\t}\n\n\t\tc1 := rstd[i]\n\t\tc2 := (db*mean[i] - ds) * c1 * c1 * c1 * s\n\t\tc3 := -c2*mean[i] - db*c1*s\n\n\t\tfor j := 0; j < d; j++ {\n\t\t\tbaseIndex := (i*d + j) * imageSize\n\t\t\txSection := in[baseIndex : baseIndex+imageSize]\n\t\t\tdySection := dy[baseIndex : baseIndex+imageSize]\n\t\t\tdxSection := dx[baseIndex : baseIndex+imageSize]\n\n\t\t\tfor k := 0; k < imageSize; k++ {\n\t\t\t\tdxSection[k] = c1*dySection[k] + c2*xSection[k] + c3\n\t\t\t}\n\t\t}\n\t})\n\n\treturn nil\n}\n\nfunc (op *groupNormDiffOp) computeInternalGradientsF64(batchSize, channels, imageSize int, input, dyT *tensor.Dense) ([]float64, []float64) {\n\tin := input.Float64s()\n\tdy := dyT.Float64s()\n\n\tdsA := make([]float64, batchSize*channels)\n\tdbA := make([]float64, batchSize*channels)\n\n\trunInParallel(0, batchSize*channels, func(i int) {\n\t\tbaseIndex := i * imageSize\n\n\t\tdySection := dy[baseIndex : baseIndex+imageSize]\n\t\tinSection := in[baseIndex : baseIndex+imageSize]\n\n\t\tfor j := 0; j < imageSize; j++ {\n\t\t\tdsA[i] += dySection[j] * inSection[j]\n\t\t\tdbA[i] += dySection[j]\n\t\t}\n\t})\n\n\treturn dsA, dbA\n}\n\nfunc (op *groupNormDiffOp) f32s(input, prealloc, outGrad *tensor.Dense) (err error) {\n\tin := input.Float32s()\n\tdx := prealloc.Float32s()\n\n\tmean := op.mean.Float32s()\n\trstd := op.rstd.Float32s()\n\n\tdy := outGrad.Float32s()\n\n\txShape := input.Shape()\n\n\tbatchSize := xShape[0]\n\tchannels := xShape[1]\n\timageSize := 1\n\tif len(xShape) > 2 {\n\t\timageSize = tensor.Shape(xShape[2:]).TotalSize()\n\t}\n\n\tds, db := op.computeInternalGradientsF32(batchSize, channels, imageSize, input, outGrad)\n\td := channels / op.numGroups\n\ts := 1.0 / float32(d*imageSize)\n\n\trunInParallel(0, batchSize*op.numGroups, func(i int) {\n\t\tbaseIndex := i * d\n\n\t\tdsSection := ds[baseIndex : baseIndex+d]\n\t\tdbSection := db[baseIndex : baseIndex+d]\n\n\t\tds := float32(0.0)\n\t\tdb := float32(0.0)\n\n\t\tfor j := 0; j < d; j++ {\n\t\t\tds += dsSection[j]\n\t\t\tdb += dbSection[j]\n\t\t}\n\n\t\tc1 := rstd[i]\n\t\tc2 := (db*mean[i] - ds) * c1 * c1 * c1 * s\n\t\tc3 := -c2*mean[i] - db*c1*s\n\n\t\tfor j := 0; j < d; j++ {\n\t\t\tbaseIndex := (i*d + j) * imageSize\n\t\t\txSection := in[baseIndex : baseIndex+imageSize]\n\t\t\tdySection := dy[baseIndex : baseIndex+imageSize]\n\t\t\tdxSection := dx[baseIndex : baseIndex+imageSize]\n\n\t\t\tfor k := 0; k < imageSize; k++ {\n\t\t\t\tdxSection[k] = c1*dySection[k] + c2*xSection[k] + c3\n\t\t\t}\n\t\t}\n\t})\n\n\treturn nil\n}\n\nfunc (op *groupNormDiffOp) computeInternalGradientsF32(batchSize, channels, imageSize int, input, dyT *tensor.Dense) ([]float32, []float32) {\n\tin := input.Float32s()\n\tdy := dyT.Float32s()\n\n\tdsA := make([]float32, batchSize*channels)\n\tdbA := make([]float32, batchSize*channels)\n\n\trunInParallel(0, batchSize*channels, func(i int) {\n\t\tbaseIndex := i * imageSize\n\n\t\tdySection := dy[baseIndex : baseIndex+imageSize]\n\t\tinSection := in[baseIndex : baseIndex+imageSize]\n\n\t\tfor j := 0; j < imageSize; j++ {\n\t\t\tdsA[i] += dySection[j] * inSection[j]\n\t\t\tdbA[i] += dySection[j]\n\t\t}\n\t})\n\n\treturn dsA, dbA\n}\n\n// DoDiff calculates the diff and sets its value to the output node. Implementation for ADOp interface.\nfunc (op *groupNormDiffOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) error {\n\treturn nyi(\"DoDiff\", \"groupNormDiffOp\")\n}\n\n// SymDiff applies the diff op. Implementation for SDOp interface.\nfunc (op *groupNormDiffOp) SymDiff(inputs Nodes, output, grad *Node) (Nodes, error) {\n\treturn nil, nyi(\"SymDiff\", \"groupNormDiffOp\")\n}\n\n// DiffWRT is an implementation for the SDOp interface\nfunc (op *groupNormDiffOp) DiffWRT(inputs int) []bool {\n\treturn []bool{false, false}\n}\n\n// ensure it complies with the Op interface\nvar (\n\t_ Op   = &GroupNormOp{}\n\t_ ADOp = &GroupNormOp{}\n\t_ SDOp = &GroupNormOp{}\n\n\t_ Op = &groupNormDiffOp{}\n)\n"
        },
        {
          "name": "op_group_norm_test.go",
          "type": "blob",
          "size": 17.251953125,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"math/rand\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n\t\"gorgonia.org/tensor\"\n)\n\nvar groupNormTestCases = []struct {\n\tDtype  tensor.Dtype\n\tX      interface{}\n\tXShape tensor.Shape\n\n\tGroups, Channels int\n\n\tScaleInit  InitWFn\n\tScaleShape tensor.Shape\n\n\tBiasInit  InitWFn\n\tBiasShape tensor.Shape\n\n\tExpectedTrainResult, ExpectedCost                      interface{}\n\tExpectedBiasGrad, ExpectedScaleGrad, ExpectedInputGrad interface{}\n}{\n\t{\n\t\tDtype:               tensor.Float64,\n\t\tX:                   RangedFromWithStep(0.1, 2),\n\t\tXShape:              tensor.Shape{2, 20, 2, 2},\n\t\tGroups:              2,\n\t\tChannels:            20,\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 20, 1, 1},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 20, 1, 1},\n\t\tExpectedTrainResult: []float64{4385.900222723546, 10064.555955139396, 4385.900222723546, 10064.555955139393},\n\t\tExpectedCost:        7225.228088931472,\n\t\tExpectedInputGrad:   []float64{0.1455456629614057, 0.10756983263957326, 0.06959400231774085, 0.03161817199590841, 0.08135482742065983, 0.04532816344875147, 0.009301499476843111, -0.02672516449506525, 0.03275732267930655, -0.0013201749426776932, -0.03539767256466193, -0.06947517018664628, -0.00024685126265393265, -0.03237518253471408, -0.06450351380677433, -0.09663184507883454, -0.017657694405221844, -0.047836859327357845, -0.07801602424949394, -0.10819518917163007, -0.019475206748396964, -0.04770520532060915, -0.07593520389282099, -0.10416520246503308, -0.005699388292179461, -0.03198022051446739, -0.058261052736755414, -0.08454188495904336, 0.023669760963430445, -0.0006619049089335582, -0.02499357078129756, -0.04932523665366134, 0.068632241018433, 0.04624974149599304, 0.023867241973553298, 0.0014847424511133245, 0.12918805187282817, 0.10875471870031223, 0.08832138552779653, 0.06788805235528059, 0.1712175958138913, 0.11393295134647796, 0.05664830687906508, -0.0006363375883475797, 0.10775815768778663, 0.05242267957029734, -0.0029127985471915085, -0.05824827666467991, 0.059892050361074256, 0.006505738593509225, -0.04688057317405536, -0.1002668849416204, 0.027619273833755065, -0.023817871583885708, -0.07525501700152648, -0.12669216241916725, 0.010939828105828608, -0.038548150961888794, -0.08803613002960531, -0.13752410909732182, 0.009853713177293999, -0.0376850995404987, -0.08522391225829185, -0.13276272497608366, 0.02436092904815257, -0.021228717319716317, -0.06681836368758476, -0.11240801005545364, 0.054461475718402985, 0.010820995700458802, -0.032819484317485825, -0.07645996433542956, 0.10015535318804658, 0.05846403952002577, 0.016772725852005843, -0.02491858781601497, 0.16144256145708247, 0.12170041413898636, 0.08195826682088936, 0.042216119502794136, 0.14554566296140425, 0.10756983263957176, 0.06959400231773927, 0.03161817199590722, 0.0813548274206588, 0.045328163448750125, 0.00930149947684189, -0.026725164495066345, 0.032757322679305645, -0.0013201749426783316, -0.03539767256466275, -0.06947517018664673, -0.0002468512626543351, -0.0323751825347145, -0.06450351380677466, -0.09663184507883482, -0.017657694405222024, -0.04783685932735793, -0.07801602424949383, -0.10819518917163018, -0.019475206748396978, -0.04770520532060907, -0.07593520389282071, -0.1041652024650328, -0.005699388292179197, -0.031980220514466584, -0.05826105273675486, -0.08454188495904269, 0.02366976096343132, -0.0006619049089326978, -0.024993570781296715, -0.04932523665366029, 0.06863224101843413, 0.04624974149599437, 0.02386724197355461, 0.001484742451114407, 0.12918805187282967, 0.10875471870031372, 0.08832138552779778, 0.06788805235528228, 0.17121759581388218, 0.1139329513464693, 0.05664830687905731, -0.0006363375883546851, 0.10775815768777974, 0.052422679570290676, -0.0029127985471975038, -0.058248276664684795, 0.059892050361068705, 0.00650573859350434, -0.046880573174060025, -0.10026688494162439, 0.027619273833751734, -0.023817871583888817, -0.07525501700152937, -0.12669216241916992, 0.010939828105827054, -0.03854815096188968, -0.08803613002960642, -0.13752410909732227, 0.009853713177294665, -0.03768509954049826, -0.08522391225829029, -0.13276272497608232, 0.024360929048154567, -0.021228717319713653, -0.06681836368758098, -0.11240801005545009, 0.05446147571840765, 0.010820995700464131, -0.032819484317480274, -0.07645996433542379, 0.10015535318805302, 0.05846403952003243, 0.016772725852013615, -0.024918587816006976, 0.16144256145709068, 0.12170041413899568, 0.0819582668208989, 0.042216119502803906},\n\t\tExpectedBiasGrad:    []float64{51.00000000000007, 55.80000000000008, 60.60000000000008, 65.40000000000009, 70.20000000000009, 75.0000000000001, 79.8000000000001, 84.60000000000008, 89.40000000000006, 94.20000000000005, 99.00000000000003, 103.80000000000001, 108.6, 113.39999999999998, 118.19999999999996, 122.99999999999994, 127.79999999999993, 132.5999999999999, 137.3999999999999, 142.19999999999987},\n\t\tExpectedScaleGrad:   []float64{-79.3960426535744, -67.54511124603596, -52.36760260129374, -33.863516719347764, -12.03285360019801, 13.124386756155513, 41.60820434971281, 73.41859918047385, 108.55557124843864, 147.0191205536072, -154.24403049065916, -125.76021289710187, -93.94981806634075, -58.81284599837597, -20.349296693207414, 21.440829849164896, 66.55753362874096, 115.00081464552079, 166.77067289950435, 221.86710839069167},\n\t},\n\t{\n\t\tDtype:               Float64,\n\t\tX:                   RangedFromWithStep(0.5, 5),\n\t\tXShape:              tensor.Shape{3, 2},\n\t\tGroups:              2,\n\t\tChannels:            2,\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 2},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 2},\n\t\tExpectedTrainResult: []float64{0.3, 0.66, 1.02, 0.3000000000000614, 0.6600000000001024, 1.0200000000001432, 0.3000000000000614, 0.6600000000001024, 1.0200000000001432},\n\t\tExpectedCost:        0.6600000000000682,\n\t\tExpectedInputGrad:   []float64{0, 0, 0, 0, 0, 0},\n\t\tExpectedBiasGrad:    []float64{0.8999999999999998, 1.2},\n\t\tExpectedScaleGrad:   []float64{0, 0},\n\t},\n\t{\n\t\tDtype:               tensor.Float64,\n\t\tX:                   RangedFromWithStep(0.1, 2),\n\t\tXShape:              tensor.Shape{2, 20, 2, 3},\n\t\tGroups:              2,\n\t\tChannels:            20,\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 20, 1, 1},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 20, 1, 1},\n\t\tExpectedTrainResult: []float64{9841.673486015334, 22618.005443766822, 9841.673486015337, 22618.005443766833},\n\t\tExpectedCost:        16229.83946489108,\n\t\tExpectedInputGrad:   []float64{0.1515293450089557, 0.12627689001893583, 0.10102443502891593, 0.07577198003889604, 0.050519525048876135, 0.02526707005885622, 0.08706225857660721, 0.06310902214640476, 0.03915578571620232, 0.0152025492859999, -0.008750687144202546, -0.03270392357440498, 0.03818579486206826, 0.01553177699168333, -0.0071222408787015995, -0.029776258749086654, -0.05243027661947153, -0.07508429448985651, 0.004899953865339035, -0.016454845445228447, -0.03780964475579597, -0.05916444406636345, -0.08051924337693092, -0.10187404268749845, -0.012795264413580504, -0.03285084516433051, -0.05290642591508063, -0.07296200666583053, -0.09301758741658053, -0.11307316816733068, -0.014899859974690552, -0.033656222165622945, -0.05241258435655535, -0.07116894654748807, -0.08992530873842056, -0.10868167092935305, -0.0014138328179905402, -0.018870976449105625, -0.036328120080220724, -0.053785263711335796, -0.0712424073424508, -0.08869955097356588, 0.027662817056518692, 0.011504891985221227, -0.00465303308607623, -0.020810958157374132, -0.036968883228671603, -0.05312680829996905, 0.0723300896488379, 0.057471383137357515, 0.042612676625877345, 0.027753970114397397, 0.012895263602917234, -0.001963442908562707, 0.1325879849589662, 0.11902849700730399, 0.10546900905564112, 0.09190952110397847, 0.07835003315231559, 0.06479054520065294, 0.18028253628573188, 0.14216424392528837, 0.10404595156484509, 0.0659276592044018, 0.027809366843958294, -0.01030892551648499, 0.11657353921989033, 0.07975446541926479, 0.04293539161863924, 0.006116317818012806, -0.03070275598261296, -0.06752182978323873, 0.06845516487185921, 0.032935309631050735, -0.00258454560975796, -0.03810440085056643, -0.0736242560913749, -0.10914411133218338, 0.035927413241637396, 0.0017067765606464391, -0.032513860120344296, -0.06673449680133547, -0.10095513348232665, -0.1351757701633174, 0.018990284329225338, -0.013931133791948325, -0.04685255191312221, -0.0797739700342952, -0.11269538815546865, -0.14561680627664209, 0.017643778134622368, -0.013978421426733778, -0.04560062098808948, -0.07722282054944563, -0.10884502011080133, -0.1404672196721577, 0.03188789465782982, 0.0015649136562909671, -0.02875806734524744, -0.05908104834678629, -0.08940402934832425, -0.11972701034986333, 0.061722633898846135, 0.03269887145712547, 0.003675109015404354, -0.025348653426317203, -0.05437241586803854, -0.0833961783097592, 0.10714799585767265, 0.07942345197576883, 0.051698908093865015, 0.023974364211960975, -0.0037501796699428436, -0.03147472355184666, 0.1681639805343067, 0.14173865521222062, 0.11531332989013454, 0.08888800456804824, 0.06246267924596127, 0.03603735392387608, 0.15152934500895698, 0.12627689001893705, 0.10102443502891711, 0.07577198003889718, 0.050519525048877245, 0.02526707005885731, 0.08706225857660854, 0.0631090221464059, 0.03915578571620326, 0.015202549286000622, -0.008750687144201574, -0.032703923574404214, 0.03818579486206897, 0.015531776991684065, -0.007122240878700836, -0.029776258749086182, -0.05243027661947108, -0.07508429448985598, 0.00489995386533959, -0.016454845445228017, -0.037809644755795624, -0.05916444406636279, -0.0805192433769304, -0.101874042687498, -0.012795264413580032, -0.032850845164330345, -0.05290642591508021, -0.07296200666583053, -0.0930175874165804, -0.11307316816733026, -0.014899859974690344, -0.03365622216562292, -0.05241258435655549, -0.07116894654748807, -0.08992530873842064, -0.10868167092935321, -0.001413832817990457, -0.018870976449105736, -0.036328120080221016, -0.05378526371133585, -0.07124240734245113, -0.08869955097356597, 0.027662817056518296, 0.011504891985220755, -0.0046530330860767855, -0.020810958157374326, -0.03696888322867231, -0.05312680829996941, 0.07233008964883769, 0.057471383137357, 0.042612676625876755, 0.02775397011439651, 0.012895263602916707, -0.0019634429085635396, 0.1325879849589655, 0.119028497007303, 0.1054690090556405, 0.09190952110397754, 0.07835003315231504, 0.06479054520065208, 0.18028253628572166, 0.14216424392527838, 0.10404595156483509, 0.0659276592043927, 0.027809366843949412, -0.010308925516492984, 0.1165735392198819, 0.07975446541925724, 0.04293539161863169, 0.006116317818006145, -0.0307027559826194, -0.06752182978324495, 0.06845516487185321, 0.03293530963104452, -0.002584545609763289, -0.038104400850571984, -0.07362425609137979, -0.1091441113321876, 0.035927413241632955, 0.0017067765606428864, -0.03251386012034807, -0.06673449680133814, -0.1009551334823291, -0.13517577016331916, 0.018990284329222895, -0.013931133791950323, -0.04685255191312354, -0.07977397003429587, -0.11269538815546998, -0.1456168062766432, 0.017643778134622146, -0.013978421426733334, -0.045600620988088814, -0.0772228205494443, -0.10884502011079977, -0.14046721967215614, 0.031887894657831595, 0.0015649136562938537, -0.028758067345244775, -0.059081048346783405, -0.08940402934832115, -0.11972701034985977, 0.061722633898850354, 0.03269887145713035, 0.0036751090154094612, -0.025348653426312318, -0.05437241586803321, -0.08339617830975321, 0.10714799585767842, 0.07942345197577438, 0.05169890809387123, 0.02397436421196808, -0.003750179669935072, -0.031474723551838224, 0.16816398053431492, 0.1417386552122295, 0.1153133298901432, 0.08888800456805779, 0.062462679245971486, 0.036037353923885185},\n\t\tExpectedBiasGrad:    []float64{114.30000000000001, 125.09999999999997, 135.89999999999992, 146.69999999999987, 157.49999999999983, 168.29999999999976, 179.09999999999974, 189.8999999999997, 200.69999999999965, 211.4999999999996, 222.29999999999956, 233.0999999999995, 243.89999999999952, 254.69999999999942, 265.49999999999943, 276.2999999999994, 287.09999999999934, 297.8999999999993, 308.6999999999992, 319.4999999999991},\n\t\tExpectedScaleGrad:   []float64{-177.89766666727434, -151.3936080469977, -117.4060505221725, -75.93499409279866, -26.980438758876208, 29.457615479594864, 93.37916862261457, 164.78422067018292, 243.67277162229993, 330.04482147896556, -346.27639201961847, -282.3548388765987, -210.9497868290304, -132.0612358769134, -45.68918602024776, 48.166362740966505, 149.5054104067294, 258.3279569770409, 374.63400245190104, 498.42354683130975},\n\t},\n\t{\n\t\tDtype:               Float32,\n\t\tX:                   RangedFromWithStep(0.5, 5),\n\t\tXShape:              tensor.Shape{3, 2},\n\t\tGroups:              2,\n\t\tChannels:            2,\n\t\tScaleInit:           RangedFromWithStep(0.3, 0.3),\n\t\tScaleShape:          tensor.Shape{1, 2},\n\t\tBiasInit:            RangedFromWithStep(0.2, 0.2),\n\t\tBiasShape:           tensor.Shape{1, 2},\n\t\tExpectedTrainResult: []float32{0.3, 0.66, 1.02, 0.3, 0.66, 1.02, 0.3, 0.66, 1.02},\n\t\tExpectedCost:        0.66,\n\t\tExpectedInputGrad:   []float32{0, 0, 0, 0, 0, 0},\n\t\tExpectedBiasGrad:    []float32{0.8999999999999998, 1.2},\n\t\tExpectedScaleGrad:   []float32{0, 0},\n\t},\n}\n\nfunc TestGroupNorm(t *testing.T) {\n\tfor i, tC := range groupNormTestCases {\n\t\tdesc := fmt.Sprintf(\"Example #%d %v - %v\", i+1, tC.Dtype, tC.XShape)\n\t\tt.Run(desc, func(t *testing.T) {\n\t\t\trand.Seed(0)\n\n\t\t\tc := require.New(t)\n\n\t\t\tg := NewGraph()\n\n\t\t\tvar initOpt NodeConsOpt\n\n\t\t\tswitch v := tC.X.(type) {\n\t\t\tcase []float32:\n\t\t\t\tinitOpt = WithValue(\n\t\t\t\t\ttensor.New(\n\t\t\t\t\t\ttensor.Of(tensor.Float32),\n\t\t\t\t\t\ttensor.WithShape(tC.XShape...),\n\t\t\t\t\t\ttensor.WithBacking(v),\n\t\t\t\t\t),\n\t\t\t\t)\n\t\t\tcase []float64:\n\t\t\t\tinitOpt = WithValue(\n\t\t\t\t\ttensor.New(\n\t\t\t\t\t\ttensor.Of(tensor.Float32),\n\t\t\t\t\t\ttensor.WithShape(tC.XShape...),\n\t\t\t\t\t\ttensor.WithBacking(v),\n\t\t\t\t\t),\n\t\t\t\t)\n\t\t\tcase InitWFn:\n\t\t\t\tinitOpt = WithInit(v)\n\t\t\t}\n\n\t\t\tvar err error\n\n\t\t\tx := NewTensor(g, tC.Dtype, tC.XShape.Dims(), WithShape(tC.XShape...), initOpt, WithName(\"x\"))\n\n\t\t\tscale := NewTensor(g, tC.Dtype, tC.ScaleShape.Dims(), WithShape(tC.ScaleShape...), WithInit(tC.ScaleInit), WithName(\"scale\"))\n\t\t\tbias := NewTensor(g, tC.Dtype, tC.BiasShape.Dims(), WithShape(tC.BiasShape...), WithInit(tC.BiasInit), WithName(\"bias\"))\n\n\t\t\tfcWeight := NewTensor(g, tC.Dtype, 2, WithShape(tC.XShape[0], tensor.Shape(tC.XShape[1:]).TotalSize()), WithInit(tC.ScaleInit), WithName(\"fcWeight\"))\n\n\t\t\ty, err := GroupNorm(x, scale, bias, tC.Groups, tC.Channels, 1e-5)\n\t\t\tc.NoError(err)\n\n\t\t\tif y.Dims() > 2 {\n\t\t\t\ty = Must(Reshape(y, fcWeight.Shape()))\n\t\t\t}\n\n\t\t\twT := Must(Transpose(fcWeight, 1, 0))\n\n\t\t\ty = Must(Mul(y, wT))\n\n\t\t\tcost := Must(Mean(y))\n\n\t\t\tif _, err := Grad(cost, x, fcWeight, scale, bias); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tm := NewTapeMachine(g, BindDualValues(x, fcWeight, scale, bias), TraceExec(), WithInfWatch(), WithNaNWatch())\n\n\t\t\terr = m.RunAll()\n\t\t\tc.NoError(err)\n\n\t\t\t// ioutil.WriteFile(\"gn.dot\", []byte(g.ToDot()), 0644)\n\n\t\t\tc.NoError(m.Close())\n\n\t\t\tt.Logf(\"%v output:\\n%v\", desc, y.Value())\n\t\t\tt.Logf(\"%v cost:\\n%v\", desc, cost.Value())\n\t\t\tt.Logf(\"%v input grad:\\n%v\", desc, x.Deriv().Value())\n\t\t\t// t.Logf(\"%v output grad:\\n%v\", desc, y.Deriv().Value())\n\t\t\t// t.Logf(\"%v bias grad: %v\", desc, bias.Deriv().Value())\n\t\t\t// t.Logf(\"%v scale grad: %v\", desc, scale.Deriv().Value())\n\n\t\t\tc.InDeltaSlice(tC.ExpectedTrainResult, y.Value().Data(), 1e-3, \"Wrong Output\\ngot=%#v\\nexpected=%#v\", y.Value().Data(), tC.ExpectedTrainResult)\n\t\t\tc.InDelta(tC.ExpectedCost, cost.Value().Data(), 1e-3)\n\n\t\t\tc.InDeltaSlice(tC.ExpectedBiasGrad, bias.Deriv().Value().Data(), 1e-3, \"Bias Grad doesn't match:\\ngot=%#v expected=%#v\", bias.Deriv().Value().Data(), tC.ExpectedBiasGrad)\n\t\t\tc.InDeltaSlice(tC.ExpectedScaleGrad, scale.Deriv().Value().Data(), 1e-3, \"Scale Grad doens't match:\\ngot=%#v expected=%#v\", scale.Deriv().Value().Data(), tC.ExpectedScaleGrad)\n\t\t\tc.InDeltaSlice(tC.ExpectedInputGrad, x.Deriv().Value().Data(), 1e-3, \"Input Grad doesn't match:\\ngot=%#v expected=%#v\", x.Deriv().Value().Data(), tC.ExpectedInputGrad)\n\t\t})\n\t}\n}\n\nfunc BenchmarkGroupNorm(b *testing.B) {\n\tb.StopTimer()\n\n\ttC := groupNormTestCases[0]\n\n\trand.Seed(0)\n\n\tc := require.New(b)\n\n\tg := NewGraph()\n\n\tvar initOpt NodeConsOpt\n\n\tswitch v := tC.X.(type) {\n\tcase []float32:\n\t\tinitOpt = WithValue(\n\t\t\ttensor.New(\n\t\t\t\ttensor.Of(tensor.Float32),\n\t\t\t\ttensor.WithShape(tC.XShape...),\n\t\t\t\ttensor.WithBacking(v),\n\t\t\t),\n\t\t)\n\tcase []float64:\n\t\tinitOpt = WithValue(\n\t\t\ttensor.New(\n\t\t\t\ttensor.Of(tensor.Float32),\n\t\t\t\ttensor.WithShape(tC.XShape...),\n\t\t\t\ttensor.WithBacking(v),\n\t\t\t),\n\t\t)\n\tcase InitWFn:\n\t\tinitOpt = WithInit(v)\n\t}\n\n\tvar err error\n\n\tx := NewTensor(g, tC.Dtype, tC.XShape.Dims(), WithShape(tC.XShape...), initOpt, WithName(\"x\"))\n\n\tscale := NewTensor(g, tC.Dtype, tC.ScaleShape.Dims(), WithShape(tC.ScaleShape...), WithInit(tC.ScaleInit), WithName(\"scale\"))\n\tbias := NewTensor(g, tC.Dtype, tC.BiasShape.Dims(), WithShape(tC.BiasShape...), WithInit(tC.BiasInit), WithName(\"bias\"))\n\n\ty, err := GroupNorm(x, scale, bias, tC.Groups, tC.Channels, 1e-5)\n\tc.NoError(err)\n\n\tcost := Must(Mean(y))\n\n\tif _, err := Grad(cost, x, scale, bias); err != nil {\n\t\tc.NoError(err)\n\t}\n\n\tm := NewTapeMachine(g, BindDualValues(x, scale, bias), TraceExec(), WithInfWatch(), WithNaNWatch())\n\n\tfor i := 0; i < b.N; i++ {\n\t\tb.StartTimer()\n\t\terr = m.RunAll()\n\t\tb.StopTimer()\n\n\t\tc.NoError(err)\n\t}\n}\n"
        },
        {
          "name": "op_infidel.go",
          "type": "blob",
          "size": 4.5791015625,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"hash\"\n\n\t\"github.com/chewxy/hm\"\n\t\"gorgonia.org/tensor\"\n)\n\n/*\nThis file contains code for Ops that aren't really functions in the sense that they aren't pure.\n\nSince they're not adherents to the Church of Lambda, they are INFIDELS! A fatwa will be issued on them shortly\n\n*/\n\ntype stmtOp interface {\n\tOp\n\tisStmt() bool\n}\n\n// letOp is not really a function. It's more of a binding statement.\n// However, it's implemented as a Op so that it can be counted for register allocation and liveness\ntype letOp struct{}\n\nfunc (op letOp) Arity() int                                                      { return 0 }\nfunc (op letOp) Type() hm.Type                                                   { return nil }\nfunc (op letOp) ReturnsPtr() bool                                                { return true }\nfunc (op letOp) OverwritesInput() int                                            { return 0 }\nfunc (op letOp) CallsExtern() bool                                               { return false }\nfunc (op letOp) InferShape(...DimSizer) (tensor.Shape, error)                    { return nil, nil }\nfunc (op letOp) DiffWRT(int) []bool                                              { return nil }\nfunc (op letOp) SymDiff(inputs Nodes, outputNode, gradNode *Node) (Nodes, error) { return nil, nil }\nfunc (op letOp) Do(vals ...Value) (Value, error)                                 { return nil, nil }\nfunc (op letOp) String() string                                                  { return \"=\" }\nfunc (op letOp) WriteHash(h hash.Hash)                                           { h.Write([]byte(\"let\")) }\nfunc (op letOp) Hashcode() uint32                                                { return simpleHash(op) }\n\nfunc (op letOp) isStmt() bool { return true }\n\n// readOp reads a value off the input. This op ensures that a value used, and hence codegen'd out\ntype readOp struct {\n\tinto *Value // no, it's not a mistake. It's a pointer to a Value (which is an interface{} type)\n}\n\nfunc (op readOp) Arity() int                                                      { return 0 }\nfunc (op readOp) Type() hm.Type                                                   { return nil }\nfunc (op readOp) ReturnsPtr() bool                                                { return true }\nfunc (op readOp) OverwritesInput() int                                            { return 0 }\nfunc (op readOp) CallsExtern() bool                                               { return false }\nfunc (op readOp) InferShape(...DimSizer) (tensor.Shape, error)                    { return nil, nil }\nfunc (op readOp) DiffWRT(int) []bool                                              { return nil }\nfunc (op readOp) SymDiff(inputs Nodes, outputNode, gradNode *Node) (Nodes, error) { return nil, nil }\nfunc (op readOp) Do(vals ...Value) (Value, error)                                 { return nil, nil }\nfunc (op readOp) String() string                                                  { return \"print\" }\nfunc (op readOp) WriteHash(h hash.Hash)                                           { fmt.Fprintf(h, \"print %p\", op.into) }\nfunc (op readOp) Hashcode() uint32                                                { return simpleHash(op) }\n\nfunc (op readOp) isStmt() bool { return true }\n\n// devTrans is a dummy Op, used to aid in creating the program that is run in a *tapeMachine. It is inserted not into the graph, but into a slice of sorted nodes, and will not show up in thegraph.\ntype devTrans struct {\n\tfrom, to Device\n\ttoNode   *Node\n}\n\nfunc (op devTrans) Arity() int                                   { panic(\"not implemented\") }\nfunc (op devTrans) Type() hm.Type                                { panic(\"not implemented\") }\nfunc (op devTrans) InferShape(...DimSizer) (tensor.Shape, error) { panic(\"not implemented\") }\nfunc (op devTrans) Do(...Value) (Value, error)                   { panic(\"not implemented\") }\nfunc (op devTrans) ReturnsPtr() bool                             { return false }\nfunc (op devTrans) CallsExtern() bool                            { return true }\nfunc (op devTrans) OverwritesInput() int                         { return -1 }\nfunc (op devTrans) WriteHash(h hash.Hash)                        { fmt.Fprintf(h, \"from:%vto%v\", op.from, op.to) }\nfunc (op devTrans) Hashcode() uint32                             { return simpleHash(op) }\n\nfunc (op devTrans) String() string { return fmt.Sprintf(\"[CP %v %v]\", op.from, op.to) }\nfunc (op devTrans) isStmt() bool   { return true }\n\nfunc (op devTrans) CUDADo(extern External, dev Device, prealloc Value, inputs ...Value) (retVal Value, err error) {\n\treturn nil, nil\n}\nfunc (op devTrans) CUDAFuncName() string { return op.String() }\n"
        },
        {
          "name": "op_math.go",
          "type": "blob",
          "size": 31.5693359375,
          "content": "package gorgonia\n\n/*\nThis file holds all the Ops that are related to doing math-related work. Due to the numerousness of\nmathematical operations, they're classified into 3 main types:\n\telemBinOp - a representation of a binary mathematical operation that is performed elementwise (example: +, *, -, or >, <)\n\telemUnaryOp - a representation of a mathematical operation that is performed elmentwise\n\tlinAlgBinOp - a representation of a binary mathematical operation that is performed on matrices\n\nThe individual operators are further exanded on operator*.go files. Their datatypes are often embedded in the datatypes here.\n\nFor all data type, the methods are standardized by arrangement in the order the Op interface is defined.\nAny additional interfaces that the data type fulfils will be declared AFTER the Op interface methods.\n*/\n\nimport (\n\t\"bytes\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"hash\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\n/* ELEMENTWISE BINARY OPERATION */\n\n// elemBinOp is the representation of an operation that is to be performed elementwise\ntype elemBinOp struct {\n\tʘBinaryOperator\n\targ0, arg1 hm.Type // pruned types only plz\n\tretSame    bool    // for comparison ops, return same type?\n}\n\nfunc newEBOByType(ot ʘBinaryOperatorType, at, bt hm.Type) elemBinOp {\n\tvar binOp ʘBinaryOperator\n\tswitch att := at.(type) {\n\tcase tensor.Dtype:\n\t\tswitch bt.(type) {\n\t\tcase tensor.Dtype:\n\t\t\tbinOp = scalarBinOp{\n\t\t\t\tʘBinaryOperatorType: ot,\n\t\t\t\tt:                   att,\n\t\t\t}\n\t\tcase TensorType:\n\t\t\tbinOp = tBinOp{\n\t\t\t\tʘBinaryOperatorType: ot,\n\t\t\t\ttensorLeft:          false,\n\t\t\t}\n\t\tdefault:\n\t\t\tpanic(fmt.Sprintf(\"Unsupported type of b %v!\", bt))\n\t\t}\n\tcase TensorType:\n\t\tbinOp = tBinOp{\n\t\t\tʘBinaryOperatorType: ot,\n\t\t\ttensorLeft:          true,\n\t\t}\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"Unsupported type of a %v!\", at))\n\t}\n\treturn elemBinOp{\n\t\tʘBinaryOperator: binOp,\n\t\targ0:            at,\n\t\targ1:            bt,\n\t}\n}\n\nfunc newElemBinOp(ot ʘBinaryOperatorType, a, b *Node) elemBinOp {\n\t// at := hm.Prune(a.t)\n\t// bt := hm.Prune(b.t)\n\n\treturn newEBOByType(ot, a.t, b.t)\n}\n\nfunc (op elemBinOp) Arity() int { return 2 }\n\n// elemBinOp has either of these types:\n// \t\telemBinOp :: (Floats a) ⇒ Tensor a → Tensor a → Tensor a\n// \t\telemBinOp :: (Floats a) ⇒ Tensor a → a → Tensor a\n//\t\telemBinOp :: (Floats a) ⇒ a → Tensor a → a\n//\t\telemBinOp :: (Floats a) ⇒ a → a → a\n//\t\telemBinOp :: (Floats a) ⇒ a → a → Bool\n// \t\telemBinOp :: (Floats a) ⇒ Tensor a → Tensor a → Tensor Bool\n// \t\telemBinOp :: (Floats a) ⇒ Tensor a → a → Tensor Bool\n//\t\telemBinOp :: (Floats a) ⇒ a → Tensor a → Bool\n//\n// To make things clearer, it helps to consider elemBinOp to be the representation of\n// a dispatch table for different functions. In a sense it's \"overloading\" functions.\n//\n// At the moment, due to my refusal to create a sum type (which requires more finnicking with data constructors)\n// Type() happens pretty much at close to run time\nfunc (op elemBinOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\n\tvar a0, a1, retType hm.Type\n\tvar arg0Dims int\n\tswitch arg0 := op.arg0.(type) {\n\tcase TensorType:\n\t\targ0Dims = arg0.Dims\n\t\ta0 = makeFromTensorType(arg0, a)\n\t\tretType = makeFromTensorType(arg0, a)\n\tdefault:\n\t\ta0 = a\n\t\tretType = a\n\t}\n\n\tswitch arg1 := op.arg1.(type) {\n\tcase TensorType:\n\t\tif arg1.Dims >= arg0Dims {\n\t\t\tretType = makeFromTensorType(arg1, a)\n\t\t}\n\t\ta1 = makeFromTensorType(arg1, a)\n\tdefault:\n\t\ta1 = a\n\t}\n\n\tif op.isArith() || (!op.isArith() && op.retSame) {\n\t\treturn hm.NewFnType(a0, a1, retType)\n\t}\n\n\tswitch rt := retType.(type) {\n\tcase TensorType:\n\t\trt.Of = Bool\n\t\tretType = rt\n\tdefault:\n\t\tretType = Bool\n\t}\n\n\treturn hm.NewFnType(a0, a1, retType)\n}\n\n// elemBinOp has these allowed shapes:\n// \t\top :: () → () → ()\n//\t\top :: () → (...) → (...)\n//\t\top :: (...) → () → (...)\nfunc (op elemBinOp) InferShape(inputs ...DimSizer) (retVal tensor.Shape, err error) {\n\tshapeLogf(\"Inferring shape of %v\", op)\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tif inputs[0] == nil || inputs[1] == nil {\n\t\treturn nil, errors.Errorf(nyiFail, \"elemBinOp.inferShape\", \"runtime impl\")\n\t}\n\n\tswitch x := inputs[0].(type) {\n\tcase tensor.Shape:\n\t\tswitch y := inputs[1].(type) {\n\t\tcase tensor.Shape:\n\t\t\tswitch {\n\t\t\tcase x.IsScalarEquiv() && y.IsScalarEquiv():\n\t\t\t\t// preserve ambiguous scalar shape\n\t\t\t\tswitch {\n\t\t\t\tcase len(x) > 0 && x[0] == 1:\n\t\t\t\t\tretVal = x\n\t\t\t\tcase len(y) > 0 && y[0] == 1:\n\t\t\t\t\tretVal = y\n\t\t\t\tcase x.IsScalar() && y.IsScalar():\n\t\t\t\t\tretVal = scalarShape\n\t\t\t\tdefault:\n\t\t\t\t\tretVal = scalarShape\n\t\t\t\t}\n\t\t\tcase x.IsScalar() && !y.IsScalar():\n\t\t\t\tretVal = y\n\t\t\tcase !x.IsScalar() && y.IsScalar():\n\t\t\t\tretVal = x\n\t\t\tcase !x.IsScalar() && !y.IsScalar():\n\t\t\t\tif !x.Eq(y) {\n\t\t\t\t\treturn nil, errors.Errorf(\"Shape mismatch: %v and %v\", x, y)\n\t\t\t\t}\n\t\t\t\tif x.Dims() > y.Dims() {\n\t\t\t\t\tretVal = x\n\t\t\t\t} else {\n\t\t\t\t\tretVal = y\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\tretVal = x\n\t\t}\n\tdefault:\n\t\tswitch y := inputs[1].(type) {\n\t\tcase tensor.Shape:\n\t\t\tretVal = y\n\t\tdefault:\n\t\t\tretVal = scalarShape\n\t\t}\n\t}\n\treturn\n}\n\n// DiffWRT gives info on whether or not the operation is actually differentiable\n// For example, this is differentiable:\n//\t\tc = a ** b\n// The result of the differentiation wrt to a and b would be:\n// \t\tdc/da = b * a ** (b-1)\n// \t\tdc/db = a ** b * ln(a)\n//\n// However, operators like < and > are NOT differentiable\n//\n// This method returns a slice of bools, indicating whether differentiation with regards to its operands\n// can be done. Since binOp has 2 operands, we'll return a slice\nfunc (op elemBinOp) DiffWRT(inputs int) []bool {\n\tif inputs != 2 {\n\t\tpanic(fmt.Sprintf(binOpFail, inputs))\n\t}\n\n\tb := op.ʘBinaryOperator.binOpType()\n\n\tif b >= maxʘBinaryOpType {\n\t\tpanic(\"Unsupported unary operator is not differentiable\")\n\t}\n\n\tif b.isArith() {\n\t\treturn []bool{true, true}\n\t}\n\treturn []bool{false, false}\n}\n\nfunc (op elemBinOp) SymDiff(inputs Nodes, output, gradNode *Node) (retVal Nodes, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tb := op.ʘBinaryOperator.binOpType()\n\n\tif retVal, err = ʘBinOpDiffExprs[b](inputs[0], inputs[1], output, gradNode); err == nil {\n\t\tfor _, n := range retVal {\n\t\t\tn.setGroup(gradClust)\n\t\t}\n\t}\n\n\t// needed to handle scalar gradients such as b in the logit regression example\n\tfor i, grad := range retVal {\n\t\tif inputs[i].IsScalar() && !grad.IsScalar() {\n\t\t\tif retVal[i], err = Sum(grad); err != nil {\n\t\t\t\terr = errors.Wrap(err, operationError)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\treturn\n}\n\nfunc (op elemBinOp) Do(values ...Value) (Value, error) {\n\treturn op.ʘBinaryOperator.Do(op.retSame, values...)\n}\n\nfunc (op elemBinOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) (err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tb := op.ʘBinaryOperator.binOpType()\n\tif err = ʘBinOpDiffFns[b](ctx, inputs[0], inputs[1], output); err != nil {\n\t\tif _, ok := err.(AutoDiffError); !ok {\n\t\t\treturn errors.Wrapf(err, autodiffFail, b)\n\t\t}\n\t\terr = nil\n\t}\n\n\t//handle scalar gradients\n\tfor _, in := range inputs {\n\t\tindv := in.boundTo.(*dualValue)\n\t\tif _, ok := indv.d.(Scalar); in.IsScalar() && !ok {\n\t\t\tindvdT := indv.d.(tensor.Tensor)\n\t\t\tdefer returnTensor(indvdT)\n\n\t\t\tvar d Value\n\t\t\tvar t tensor.Tensor\n\t\t\tif t, err = tensor.Sum(indvdT); err != nil {\n\t\t\t\treturn errors.Wrap(err, operationError)\n\t\t\t}\n\t\t\tdefer returnTensor(t)\n\n\t\t\td, _ = anyToScalar(t.ScalarValue())\n\t\t\tindv.SetDeriv(d)\n\t\t}\n\t}\n\treturn\n}\n\nfunc (op elemBinOp) ReturnsPtr() bool { return true }\n\nfunc (op elemBinOp) OverwritesInput() int {\n\tif _, ok := op.arg0.(TensorType); ok {\n\t\treturn 0\n\t}\n\n\tif _, ok := op.arg1.(TensorType); ok {\n\t\treturn 1\n\t}\n\treturn -1\n}\n\nfunc (op elemBinOp) WriteHash(h hash.Hash) {\n\tif err := binary.Write(h, binary.LittleEndian, op.binOpType()); err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Fprintf(h, \"%v,%v\", op.arg0, op.arg1)\n}\n\nfunc (op elemBinOp) Hashcode() uint32 { return simpleHash(op) }\n\n// Fulfils UsePreallocDoer interface\nfunc (op elemBinOp) UsePreallocDo(prealloc Value, inputs ...Value) (retVal Value, err error) {\n\tif !op.ReturnsPtr() {\n\t\treturn op.Do(inputs...)\n\t}\n\n\tif pd, ok := op.ʘBinaryOperator.(usePreallocDoerBinOp); ok {\n\t\treturn pd.UsePreallocDo(prealloc, op.retSame, inputs...)\n\t}\n\n\tif retVal, err = op.Do(inputs...); err != nil {\n\t\treturn\n\t}\n\treturn Copy(prealloc, retVal)\n}\n\n// Fulfils UnsafeDoer interface\nfunc (op elemBinOp) UnsafeDo(inputs ...Value) (retVal Value, err error) {\n\tif !op.ReturnsPtr() {\n\t\treturn op.Do(inputs...)\n\t}\n\n\ta := inputs[0]\n\tb := inputs[1]\n\n\tashp, bshp := a.Shape(), b.Shape()\n\n\tif !ashp.Eq(bshp) && ashp.TotalSize() == bshp.TotalSize() {\n\t\t// temporarily reshape\n\t\tvar toReshape tensor.Tensor = b.(tensor.Tensor)\n\t\tas := a\n\t\tbackup := b.Shape()\n\t\tif bshp.Dims() < ashp.Dims() {\n\t\t\ttoReshape = a.(tensor.Tensor)\n\t\t\tas = b\n\t\t\tbackup = a.Shape()\n\t\t}\n\t\ttoReshape.Reshape(as.Shape().Clone()...)\n\t\tdefer toReshape.Reshape(backup...)\n\t}\n\n\tif ud, ok := op.ʘBinaryOperator.(unsafeDoerBinOp); ok {\n\t\treturn ud.UnsafeDo(op.retSame, inputs...)\n\t}\n\treturn op.Do(inputs...)\n}\n\n// Fulfils the IncrDoer interface\nfunc (op elemBinOp) IncrDo(incr Value, inputs ...Value) (err error) {\n\tif id, ok := op.ʘBinaryOperator.(incrDoerBinOp); ok {\n\t\treturn id.IncrDo(incr, op.retSame, inputs...)\n\t}\n\n\t// if !op.ReturnsPtr() {\n\tvar retVal Value\n\tif retVal, err = op.Do(inputs...); err != nil {\n\t\treturn errors.Wrapf(err, doFail, op)\n\t}\n\n\tadd := newEBOByType(addOpType, TypeOf(incr), TypeOf(retVal))\n\tif retVal, err = add.UnsafeDo(incr, retVal); err != nil {\n\t\treturn errors.Wrapf(err, unsafeDoFail, add)\n\t}\n\terr = noIncrErr{retVal}\n\treturn\n\t// }\n}\n\nfunc (op elemBinOp) String() string { return fmt.Sprintf(\"%v %t\", op.ʘBinaryOperator, op.retSame) }\n\n// Fulfils the BinaryOp interface\nfunc (op elemBinOp) IsBinary() bool { return true }\n\n/* ELEMENTWISE UNARY OP */\n\ntype elemUnaryOp struct {\n\tʘUnaryOperator\n\n\targTensor     bool\n\tnumericResult bool // indicate if boolean results should be converted to 1 and 0 in the respective Dtype\n}\n\nfunc newElemUnaryOp(op ʘUnaryOperatorType, a *Node) elemUnaryOp {\n\tdt, err := dtypeOf(a.t)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t_, isTensor := a.t.(TensorType)\n\n\tvar operator ʘUnaryOperator\n\tswitch dt {\n\tcase Float32:\n\t\toperator = sf32UnaryOperators[op]\n\tcase Float64:\n\t\toperator = sf64UnaryOperators[op]\n\t}\n\n\treturn elemUnaryOp{\n\t\tʘUnaryOperator: operator,\n\t\targTensor:      isTensor,\n\t}\n}\n\nfunc (op elemUnaryOp) Arity() int { return 1 }\n\n// all pointwise unary operations have this type:\n//\t\top :: (Arithable a) ⇒ a → a\nfunc (op elemUnaryOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\treturn hm.NewFnType(a, a)\n}\n\nfunc (op elemUnaryOp) InferShape(inputs ...DimSizer) (retVal tensor.Shape, err error) {\n\tif inputs[0] == nil {\n\t\treturn nil, errors.Errorf(nyiFail, \"inferShape\", \"nil shape\")\n\t}\n\n\treturn inputs[0].(tensor.Shape), nil\n}\n\n// diffWRT gives info on whether or not the operation is actually differentiable wrt to its inputs\n//\n// some operations, such as ceil(), sign(), floor cannot be differentiated wrt to its inputs (or I don't actually know how to do them)\nfunc (op elemUnaryOp) DiffWRT(inputs int) []bool {\n\tif inputs != 1 {\n\t\tpanic(fmt.Sprintf(\"unary operator only supports one input, got %d instead\", inputs))\n\t}\n\n\tu := op.ʘUnaryOperator.unaryOpType()\n\n\tif u >= maxʘUnaryOperator {\n\t\tpanic(\"Unsupported unary operator is not differentiable\")\n\t}\n\treturn []bool{ʘUnaryOpDifferentiable[u]}\n}\n\nfunc (op elemUnaryOp) SymDiff(inputs Nodes, output, gradNode *Node) (retVal Nodes, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tu := op.ʘUnaryOperator.unaryOpType()\n\n\tvar n *Node\n\tif n, err = ʘUnaryOpDiffExprs[u](inputs[0], output, gradNode); err == nil {\n\t\tn.setGroup(gradClust)\n\t\tretVal = Nodes{n}\n\t}\n\treturn\n}\n\nfunc (op elemUnaryOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) (err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tu := op.ʘUnaryOperator.unaryOpType()\n\treturn ʘUnaryOpDiffFns[u](inputs[0], output)\n}\n\nfunc (op elemUnaryOp) Do(inputs ...Value) (retVal Value, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\treturn op.do(inputs[0])\n}\n\nfunc (op elemUnaryOp) ReturnsPtr() bool { return true }\n\nfunc (op elemUnaryOp) OverwritesInput() int {\n\tif op.argTensor {\n\t\treturn 0\n\t}\n\treturn -1\n}\n\nfunc (op elemUnaryOp) WriteHash(h hash.Hash) {\n\tif err := binary.Write(h, binary.LittleEndian, op.unaryOpType()); err != nil {\n\t\tpanic(err)\n\t}\n\n\tif op.argTensor {\n\t\th.Write([]byte{1})\n\t} else {\n\t\th.Write([]byte{0})\n\t}\n}\n\nfunc (op elemUnaryOp) Hashcode() uint32 { return simpleHash(op) }\n\n// fulfils UnsafeDoer interface\nfunc (op elemUnaryOp) UnsafeDo(inputs ...Value) (Value, error) {\n\tif err := checkArity(op, len(inputs)); err != nil {\n\t\treturn nil, err\n\t}\n\treturn op.do(inputs[0], tensor.UseUnsafe())\n}\n\n// fulfils UnaryOp interface\n\nfunc (op elemUnaryOp) isUnary() bool { return true }\n\n// misc private methods\n\nfunc (op elemUnaryOp) do(a Value, opts ...tensor.FuncOpt) (retVal Value, err error) {\n\tswitch v := a.(type) {\n\tcase tensor.Tensor:\n\t\treturn unaryCheckApply(op.ʘUnaryOperator, v, opts...)\n\tcase Scalar:\n\t\tvt := v.Dtype()\n\t\tswitch vt {\n\t\tcase tensor.Float32:\n\t\t\tvs := v.(*F32)\n\t\t\tf := float32(*vs)\n\t\t\topFn := op.ʘUnaryOperator.(*sf32UnaryOperator)\n\t\t\tretVal, _ = anyToScalar((*opFn)(f))\n\t\tcase tensor.Float64:\n\t\t\tvs := v.(*F64)\n\t\t\tf := float64(*vs)\n\t\t\topFn := op.ʘUnaryOperator.(*sf64UnaryOperator)\n\t\t\tretVal, _ = anyToScalar((*opFn)(f))\n\t\tdefault:\n\t\t\treturn nil, errors.Errorf(nyiFail, \"elemUnaryOp.do\", vt)\n\t\t}\n\t}\n\treturn\n}\n\n/* LINEAR ALGEBRA RELATED OPERATIONS */\n\ntype linAlgBinOp struct {\n\tāBinaryOperator\n\ttransA, transB bool\n}\n\nfunc (op linAlgBinOp) Arity() int { return 2 }\n\nfunc (op linAlgBinOp) InferShape(inputs ...DimSizer) (retVal tensor.Shape, err error) {\n\tshapeLogf(\"Inferring shape of %v\", op)\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tif inputs[0] == nil || inputs[1] == nil {\n\t\treturn nil, nyi(\"InferShape for linalgBinOp\", \"runtime impl\")\n\t}\n\n\tx, y := inputs[0].(tensor.Shape), inputs[1].(tensor.Shape)\n\tif x == nil || y == nil {\n\t\treturn nil, errors.Errorf(\"Cannot infer shape from %v %v\", x, y)\n\t}\n\n\tshapeLogf(\"x.shape: %v; y.shape: %v\", x, y)\n\t// TODO: add checks for tensors greater than 2 d\n\n\tswitch op.āBinaryOperator {\n\tcase matMulOperator:\n\t\tif op.transA {\n\t\t\tx = transpose2D(x)\n\t\t\tdefer tensor.ReturnInts(x)\n\t\t}\n\t\tif op.transB {\n\t\t\ty = transpose2D(y)\n\t\t\tdefer tensor.ReturnInts(y)\n\t\t}\n\n\t\tif x[1] != y[0] {\n\t\t\treturn nil, errors.Errorf(\"Inner dimensions do not match up\")\n\t\t}\n\n\t\tretVal = tensor.Shape{x[0], y[1]}\n\tcase matVecMulOperator:\n\t\tif op.transA {\n\t\t\tx = transpose2D(x)\n\t\t\tdefer tensor.ReturnInts(x)\n\t\t}\n\t\tif x[0] != y[0] && x[1] != y[0] {\n\t\t\treturn nil, errors.Errorf(\"Incompatible shapes: %v and %v\", x, y)\n\t\t}\n\n\t\tswitch {\n\t\tcase x[0] == y[0]:\n\t\t\tretVal = tensor.Shape{x[1]}\n\t\tcase x[1] == y[0]:\n\t\t\tretVal = tensor.Shape{x[0]}\n\t\t}\n\n\tcase vecDotOperator:\n\t\tretVal = scalarShape\n\tcase outerProdOperator:\n\t\t// outerprods only handles vec x vec for now\n\t\tretVal = tensor.Shape{x.TotalSize(), y.TotalSize()}\n\tcase batchedMatMulOperator:\n\t\tx = x.Clone()\n\t\ty = y.Clone()\n\t\tinnerX := x[len(x)-2:]\n\t\touterX := x[:len(x)-2]\n\t\tinnerY := y[len(y)-2:]\n\t\touterY := y[:len(y)-2]\n\t\tif !outerX.Eq(outerY) {\n\t\t\treturn nil, errors.Errorf(\"Expected outer dimensions of %v and %v to match. Got %v and %v\", x, y, outerX, outerY)\n\t\t}\n\n\t\t// batchSize := outerX.TotalSize()\n\t\tif op.transA {\n\t\t\tinnerX = transpose2D(innerX)\n\t\t\tdefer tensor.ReturnInts(innerX)\n\t\t}\n\t\tif op.transB {\n\t\t\tinnerY = transpose2D(innerY)\n\t\t\tdefer tensor.ReturnInts(innerY)\n\t\t}\n\t\tretVal = append(outerX, innerX[0], innerY[1])\n\t}\n\treturn\n}\n\nfunc (op linAlgBinOp) SymDiff(inputs Nodes, output, gradNode *Node) (retVal Nodes, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\to := op.āBinaryOperator\n\n\tif retVal, err = āBinOpDiffExprs[o](op.transA, op.transB, inputs[0], inputs[1], output, gradNode); err != nil {\n\t\treturn nil, errors.Wrap(err, \"Failed to differentiate expressions\")\n\t}\n\n\tfor _, n := range retVal {\n\t\tn.setGroup(gradClust)\n\t}\n\treturn\n}\n\nfunc (op linAlgBinOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) (err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\to := op.āBinaryOperator\n\treturn āBinOpDiffs[o](ctx, op.transA, op.transB, inputs[0], inputs[1], output)\n}\n\nfunc (op linAlgBinOp) Do(inputs ...Value) (retVal Value, err error) { return op.do(inputs) }\nfunc (op linAlgBinOp) ReturnsPtr() bool                             { return true }\nfunc (op linAlgBinOp) OverwritesInput() int                         { return -1 }\n\nfunc (op linAlgBinOp) WriteHash(h hash.Hash) {\n\tif err := binary.Write(h, binary.LittleEndian, op.āBinaryOperator); err != nil {\n\t\tpanic(err)\n\t}\n\n\tif op.transA {\n\t\th.Write([]byte{1})\n\t} else {\n\t\th.Write([]byte{0})\n\t}\n\n\tif op.transB {\n\t\th.Write([]byte{1})\n\t} else {\n\t\th.Write([]byte{0})\n\t}\n}\n\nfunc (op linAlgBinOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op linAlgBinOp) String() string {\n\tvar buf bytes.Buffer\n\n\tswitch op.āBinaryOperator {\n\tcase matMulOperator, matVecMulOperator, batchedMatMulOperator:\n\t\tbuf.WriteString(\"A\")\n\tcase vecDotOperator, outerProdOperator:\n\t\tbuf.WriteString(\"a\")\n\t}\n\n\tif op.transA {\n\t\tbuf.WriteString(\"ᵀ\")\n\t}\n\n\tswitch op.āBinaryOperator {\n\tcase matMulOperator, batchedMatMulOperator:\n\t\tfmt.Fprintf(&buf, \" %v B\", op.āBinaryOperator)\n\tcase matVecMulOperator, vecDotOperator, outerProdOperator:\n\t\tfmt.Fprintf(&buf, \" %v b\", op.āBinaryOperator)\n\t}\n\n\tif op.transB {\n\t\tbuf.WriteString(\"ᵀ\")\n\t}\n\n\treturn buf.String()\n}\n\n// fulfils IncrDoer\nfunc (op linAlgBinOp) IncrDo(incr Value, inputs ...Value) (err error) {\n\tt, ok := incr.(tensor.Tensor)\n\n\tswitch {\n\tcase ok && op.āBinaryOperator != batchedMatMulOperator:\n\t\t_, err = op.do(inputs, tensor.WithIncr(t))\n\t\treturn\n\tcase ok && op.āBinaryOperator == batchedMatMulOperator:\n\t\t_, err = op.preallocBatchMatMul(true, incr, inputs...)\n\t\treturn\n\t}\n\n\tvar retVal Value\n\tif retVal, err = op.do(inputs); err != nil {\n\t\treturn errors.Wrapf(err, doFail, op)\n\t}\n\n\tadd := newEBOByType(addOpType, TypeOf(incr), TypeOf(retVal))\n\tif retVal, err = add.UnsafeDo(incr, retVal); err != nil {\n\t\treturn errors.Wrapf(err, unsafeDoFail, add)\n\t}\n\n\terr = noIncrErr{retVal}\n\treturn\n}\n\n// fulfils UsePreallocDoer\nfunc (op linAlgBinOp) UsePreallocDo(prealloc Value, inputs ...Value) (retVal Value, err error) {\n\tt, ok := prealloc.(tensor.Tensor)\n\tif !ok {\n\t\treturn nil, errors.Errorf(\"Expected Tensor as preallocated value. Got %v of %T instead\", prealloc, prealloc)\n\t}\n\tif op.āBinaryOperator == batchedMatMulOperator {\n\t\treturn op.preallocBatchMatMul(false, prealloc, inputs...)\n\t}\n\treturn op.do(inputs, tensor.WithReuse(t))\n}\n\n// fulfils BinaryOp\nfunc (op linAlgBinOp) IsBinary() bool { return true }\n\n/* PRIVATE METHODS */\n\nfunc (op linAlgBinOp) do(inputs []Value, opts ...tensor.FuncOpt) (retVal Value, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\ta, b := inputs[0].(tensor.Tensor), inputs[1].(tensor.Tensor)\n\n\tif op.transA && op.āBinaryOperator != batchedMatMulOperator {\n\t\tif err = a.T(); err != nil {\n\t\t\treturn nil, errors.Wrap(err, tFail)\n\t\t}\n\n\t\t// untranspose\n\t\tdefer a.T()\n\t}\n\n\tif op.transB && op.āBinaryOperator != batchedMatMulOperator {\n\t\tif err = b.T(); err != nil {\n\t\t\treturn nil, errors.Wrap(err, tFail)\n\t\t}\n\n\t\t// untranspose\n\t\tdefer b.T()\n\t}\n\n\tswitch op.āBinaryOperator {\n\tcase matMulOperator:\n\t\tretVal, err = tensor.MatMul(a, b, opts...)\n\tcase matVecMulOperator:\n\t\tretVal, err = tensor.MatVecMul(a, b, opts...)\n\tcase vecDotOperator:\n\t\tvar ret interface{}\n\n\t\tif ret, err = tensor.Inner(a, b); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, \"Failed to carry out linalgBinOp operation %v\", op)\n\t\t}\n\n\t\tretVal, _ = anyToScalar(ret)\n\tcase outerProdOperator:\n\t\tretVal, err = tensor.Outer(a, b, opts...)\n\tcase batchedMatMulOperator:\n\t\t// checks were done when the op was created\n\t\tretVal, err = batchedMatMul(a, b, nil, op.transA, op.transB, false)\n\t}\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"linAlgBinOp %v %s %v error: %w\", a.Shape(), op.āBinaryOperator, b.Shape(), err)\n\t}\n\n\treturn retVal, nil\n}\n\nfunc (op linAlgBinOp) preallocBatchMatMul(incr bool, prealloc Value, inputs ...Value) (retVal Value, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\ta, b := inputs[0].(tensor.Tensor), inputs[1].(tensor.Tensor)\n\tc := prealloc.(tensor.Tensor)\n\treturn batchedMatMul(a, b, c, op.transA, op.transB, incr)\n}\n\ntype tensordotOp struct {\n\taAxes   []int\n\tbAxes   []int\n\taDims   int\n\tbDims   int\n\tretDims int // Dimension of the tensor resulting from operation\n}\n\nfunc makeTensordotOp(a, b *Node, aAxes, bAxes []int) tensordotOp {\n\taDims := a.Shape().Dims()\n\tbDims := b.Shape().Dims()\n\tretDims := a.Shape().Dims() + b.Shape().Dims() - 2*len(aAxes)\n\tif retDims < 0 {\n\t\tretDims = 0\n\t}\n\treturn tensordotOp{\n\t\taAxes:   aAxes,\n\t\tbAxes:   bAxes,\n\t\taDims:   aDims,\n\t\tbDims:   bDims,\n\t\tretDims: retDims,\n\t}\n}\n\nfunc (op tensordotOp) Arity() int { return 2 }\n\nfunc (op tensordotOp) Type() hm.Type {\n\tvar tRet hm.Type\n\tif op.retDims == 0 {\n\t\ttRet = hm.TypeVariable('a')\n\t} else {\n\t\ttRet = newTensorType(op.retDims, hm.TypeVariable('a'))\n\t}\n\tta := newTensorType(op.aDims, hm.TypeVariable('a'))\n\ttb := newTensorType(op.bDims, hm.TypeVariable('a'))\n\n\treturn hm.NewFnType(ta, tb, tRet)\n}\n\nfunc (op tensordotOp) InferShape(ds ...DimSizer) (tensor.Shape, error) {\n\tif err := checkArity(op, len(ds)); err != nil {\n\t\treturn nil, errors.Wrap(err, \"tensordot\")\n\t}\n\n\tshapes, err := DimSizersToShapes(ds)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\taShape := shapes[0]\n\tbShape := shapes[1]\n\n\taAxes := op.aAxes\n\tbAxes := op.bAxes\n\n\tshapeBackingLen := op.retDims\n\n\tshapeBacking := make([]int, shapeBackingLen, shapeBackingLen)\n\n\tshapeBackingPos := 0\n\n\tfor aShapeIndex, aShapeValue := range aShape {\n\t\tif 0 > contains(aAxes, aShapeIndex) {\n\t\t\tshapeBacking[shapeBackingPos] = aShapeValue\n\t\t\tshapeBackingPos++\n\t\t}\n\t}\n\n\tfor bShapeIndex, bShapeValue := range bShape {\n\t\tif 0 > contains(bAxes, bShapeIndex) {\n\t\t\tshapeBacking[shapeBackingPos] = bShapeValue\n\t\t\tshapeBackingPos++\n\t\t}\n\t}\n\n\treturn tensor.Shape(shapeBacking), nil\n}\n\nfunc (op tensordotOp) Do(vals ...Value) (Value, error) {\n\tif err := checkArity(op, len(vals)); err != nil {\n\t\treturn nil, errors.Wrap(err, \"tensordot\")\n\t}\n\n\tts, err := valuesToTensors(vals)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"tensordot - valuesToTensors failed\")\n\t}\n\n\treturn tensor.Contract(ts[0], ts[1], op.aAxes, op.bAxes)\n}\n\nfunc (op tensordotOp) ReturnsPtr() bool { return true }\n\nfunc (op tensordotOp) CallsExtern() bool { return false }\n\nfunc (op tensordotOp) OverwritesInput() int { return -1 }\n\nfunc (op tensordotOp) WriteHash(h hash.Hash) {\n\th.Write([]byte(\"tensordotOp\"))\n\tfmt.Fprintf(h, \"aAxes: %d, bAxes: %d, dims: %d\", op.aAxes, op.bAxes, op.retDims)\n\n\treturn\n}\n\nfunc (op tensordotOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op tensordotOp) String() string {\n\treturn fmt.Sprintf(\"Tensordot(aAxes=%d, bAxes=%d)\", op.aAxes, op.bAxes)\n}\n\nfunc (op tensordotOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) error {\n\todv := output.boundTo.(*dualValue)\n\todvd := odv.d.(tensor.Tensor)\n\n\tfor inNr, in := range inputs {\n\t\t// abuse of language below: \"i\" up front will refer to current \"in\"\n\t\t// \"other\" for the other input (there are only two)\n\n\t\t// Who's derivative are we calculating?\n\t\tvar iAxes []int\n\t\tvar otherAxes []int\n\t\tvar otherdv *dualValue\n\t\tvar iWasFirstArgument bool\n\n\t\tif 0 == inNr {\n\t\t\tiAxes = op.aAxes\n\t\t\totherAxes = op.bAxes\n\t\t\totherdv = inputs[1].boundTo.(*dualValue)\n\t\t\tiWasFirstArgument = true\n\t\t} else {\n\t\t\tiAxes = op.bAxes\n\t\t\totherAxes = op.aAxes\n\t\t\totherdv = inputs[0].boundTo.(*dualValue)\n\t\t\tiWasFirstArgument = false\n\t\t}\n\n\t\tidv := in.boundTo.(*dualValue)\n\t\tidvd := idv.d.(tensor.Tensor)\n\n\t\totherdvv := otherdv.Value.(tensor.Tensor)\n\n\t\t// Below a tensordot will be performed: Its output axes will be in the wrong order w.r.t to the input.\n\t\t// What is the correct permutation/pattern?\n\t\tiAxesCoSorted := make([]int, len(iAxes))\n\t\tfor index, value := range iAxes {\n\t\t\tiAxesCoSorted[index] = value\n\t\t}\n\n\t\totherAxesSorted := make([]int, len(otherAxes))\n\t\tfor index, value := range otherAxes {\n\t\t\totherAxesSorted[index] = value\n\t\t}\n\n\t\tsortUniqueIntWithImitator(otherAxesSorted, iAxesCoSorted)\n\t\tpattern := make([]int, len(in.Shape()))\n\t\tcounter := len(iAxes)\n\n\t\tfor patternIndex := 0; patternIndex < len(pattern); patternIndex++ {\n\t\t\tiAxesCoSortedIndex := contains(iAxesCoSorted, patternIndex)\n\t\t\tif 0 <= iAxesCoSortedIndex {\n\t\t\t\tpattern[patternIndex] = iAxesCoSortedIndex\n\t\t\t} else {\n\t\t\t\tpattern[patternIndex] = counter\n\t\t\t\tcounter++\n\t\t\t}\n\t\t}\n\t\t// if the shape is scalar equivalent, then we'll not have any transforms\n\t\tif in.Shape().IsScalarEquiv() {\n\t\t\tpattern = pattern[:0]\n\t\t}\n\n\t\t// Which axes of the other tensor and the output should be contracted?\n\t\t// Other tensor: All axes that weren't contracted (with i ;-) ) in the original tensordot\n\t\t// With the exception of scalars\n\t\tdOtherAxes := make([]int, otherdvv.Dims())\n\n\t\tif !otherdvv.Shape().IsScalarEquiv() {\n\t\t\tvar dOtherAxesIndex int\n\n\t\t\tfor axis := 0; axis < otherdvv.Dims(); axis++ {\n\t\t\t\tif 0 > contains(otherAxes, axis) {\n\t\t\t\t\tdOtherAxes[dOtherAxesIndex] = axis\n\t\t\t\t\tdOtherAxesIndex++\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tdOtherAxes = dOtherAxes[0:dOtherAxesIndex]\n\t\t}\n\n\t\t// Output: All axes which belong to other in the output of original tensordot, so this depends on input ordering\n\t\tdOutputAxes := make([]int, len(dOtherAxes))\n\t\tif iWasFirstArgument {\n\t\t\toutputOtherAxesStart := odvd.Dims() - len(dOtherAxes)\n\n\t\t\tfor axis := 0; axis < len(dOtherAxes); axis++ {\n\t\t\t\tdOutputAxes[axis] = outputOtherAxesStart + axis\n\t\t\t}\n\t\t} else {\n\t\t\tfor axis := 0; axis < len(dOtherAxes); axis++ {\n\t\t\t\tdOutputAxes[axis] = axis\n\t\t\t}\n\t\t}\n\n\t\t// perform tensordot\n\t\tswitch st := odvd.(type) {\n\t\tcase *tensor.Dense:\n\n\t\t\totherdvvDense := otherdvv.(*tensor.Dense)\n\t\t\todvdDense := odvd.(*tensor.Dense)\n\t\t\tvar tensordot *tensor.Dense\n\t\t\tvar err error\n\n\t\t\tswitch {\n\t\t\tcase odvdDense.Shape().IsScalarEquiv():\n\t\t\t\ttensordot, err = otherdvvDense.MulScalar(odvdDense, true)\n\t\t\tcase otherdvvDense.IsVector() && odvdDense.IsVector() && 0 == len(dOtherAxes): // TensorMul does not support creating matrix from two vectors\n\t\t\t\t// Reformat vectors, so that MatMul will create a matrix from them\n\t\t\t\tvar otherdvvDenseShapeOld tensor.Shape\n\t\t\t\tvar odvdDenseShapeOld tensor.Shape\n\n\t\t\t\totherdvvDenseReshaped := false\n\t\t\t\tif !otherdvvDense.IsColVec() {\n\t\t\t\t\totherdvvDenseShapeOld = otherdvvDense.Shape().Clone()\n\n\t\t\t\t\totherdvvVecDims, err := (otherdvvDense.AP.Shape()).DimSize(0)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\n\t\t\t\t\totherdvvDenseReshaped = true\n\t\t\t\t\totherdvvDense.Reshape(otherdvvVecDims, 1)\n\t\t\t\t}\n\n\t\t\t\todvdDenseReshaped := false\n\t\t\t\tif !odvdDense.IsRowVec() {\n\t\t\t\t\todvdDenseShapeOld = odvdDense.Shape().Clone()\n\t\t\t\t\todvdDenseVecDims, err := (odvdDense.AP.Shape()).DimSize(0)\n\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\n\t\t\t\t\todvdDenseReshaped = true\n\t\t\t\t\todvdDense.Reshape(1, odvdDenseVecDims)\n\t\t\t\t}\n\n\t\t\t\ttensordot, err = otherdvvDense.MatMul(odvdDense)\n\n\t\t\t\t// Undo Reshape\n\t\t\t\tif otherdvvDenseReshaped {\n\t\t\t\t\totherdvvDense.Reshape(otherdvvDenseShapeOld...)\n\t\t\t\t}\n\n\t\t\t\tif odvdDenseReshaped {\n\t\t\t\t\todvdDense.Reshape(odvdDenseShapeOld...)\n\t\t\t\t}\n\n\t\t\tdefault:\n\t\t\t\ttensordot, err = otherdvvDense.TensorMul(odvdDense, dOtherAxes, dOutputAxes)\n\n\t\t\t}\n\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\ttensordotPerm, err := tensor.T(tensordot, pattern...)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\ttensordotPermDense := tensordotPerm.(*tensor.Dense)\n\n\t\t\td := idvd.(*tensor.Dense)\n\t\t\td.Add(tensordotPermDense, tensor.UseUnsafe()) // TODO: Should output directly into d and save the add\n\n\t\tdefault:\n\t\t\treturn errors.Errorf(nyiTypeFail, \"Do Diff (hack)\", st)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (op tensordotOp) DiffWRT(inputs int) []bool {\n\tretVal := make([]bool, inputs)\n\tfor i := range retVal {\n\t\tretVal[i] = true\n\t}\n\treturn retVal\n}\n\nfunc (op tensordotOp) SymDiff(inputs Nodes, output *Node, grad *Node) (retVal Nodes, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tretVal = make(Nodes, len(inputs))\n\n\tfor inNr, in := range inputs {\n\t\t// abuse of language below: \"i\" up front will refer to current \"in\"\n\t\t// \"other\" for the other input (there are only two)\n\n\t\t// Who's derivative are we calculating?\n\t\tvar iAxes []int\n\t\tvar otherAxes []int\n\t\tvar iWasFirstArgument bool\n\t\tvar other *Node\n\n\t\tif 0 == inNr {\n\t\t\tiAxes = op.aAxes\n\t\t\totherAxes = op.bAxes\n\t\t\tother = inputs[1]\n\t\t\tiWasFirstArgument = true\n\t\t} else {\n\t\t\tiAxes = op.bAxes\n\t\t\totherAxes = op.aAxes\n\t\t\tother = inputs[0]\n\t\t\tiWasFirstArgument = false\n\t\t}\n\n\t\t// Below a tensordot will be performed: Its output axes will be in the wrong order w.r.t to the input.\n\t\t// What is the correct permutation/pattern?\n\t\tiAxesCoSorted := make([]int, len(iAxes))\n\t\tfor index, value := range iAxes {\n\t\t\tiAxesCoSorted[index] = value\n\t\t}\n\n\t\totherAxesSorted := make([]int, len(otherAxes))\n\t\tfor index, value := range otherAxes {\n\t\t\totherAxesSorted[index] = value\n\t\t}\n\n\t\tsortUniqueIntWithImitator(otherAxesSorted, iAxesCoSorted)\n\n\t\tpattern := make([]int, len(in.shape))\n\t\tcounter := len(iAxes)\n\n\t\tfor patternIndex := 0; patternIndex < len(pattern); patternIndex++ {\n\t\t\tiAxesCoSortedIndex := contains(iAxesCoSorted, patternIndex)\n\t\t\tif 0 <= iAxesCoSortedIndex {\n\t\t\t\tpattern[patternIndex] = iAxesCoSortedIndex\n\t\t\t} else {\n\t\t\t\tpattern[patternIndex] = counter\n\t\t\t\tcounter++\n\t\t\t}\n\t\t}\n\n\t\t// Which axes of the other tensor and the output should be contracted?\n\t\t// Other tensor: All axes that weren't contracted (with i ;-) ) in the original tensordot\n\t\t// With the exception of scalars\n\t\tdOtherAxes := make([]int, other.Dims())\n\t\tif !other.Shape().IsScalarEquiv() {\n\t\t\tvar dOtherAxesIndex int\n\n\t\t\tfor axis := 0; axis < other.Dims(); axis++ {\n\t\t\t\tif 0 > contains(otherAxes, axis) {\n\t\t\t\t\tdOtherAxes[dOtherAxesIndex] = axis\n\t\t\t\t\tdOtherAxesIndex++\n\t\t\t\t}\n\t\t\t}\n\t\t\tdOtherAxes = dOtherAxes[0:dOtherAxesIndex]\n\t\t}\n\n\t\t// Grad: All axes which belong to other in the output of original tensordot, so this depends on input ordering\n\t\tdGradAxes := make([]int, len(dOtherAxes))\n\t\tif iWasFirstArgument {\n\t\t\tgradAxesStart := grad.Dims() - len(dOtherAxes)\n\n\t\t\tfor axis := 0; axis < len(dOtherAxes); axis++ {\n\t\t\t\tdGradAxes[axis] = gradAxesStart + axis\n\t\t\t}\n\t\t} else {\n\t\t\tfor axis := 0; axis < len(dOtherAxes); axis++ {\n\t\t\t\tdGradAxes[axis] = axis\n\t\t\t}\n\t\t}\n\n\t\t// perform tensordot\n\t\tvar tensordot *Node\n\t\tswitch {\n\t\tcase grad.Shape().IsScalarEquiv():\n\t\t\tif tensordot, err = HadamardProd(other, grad); err != nil {\n\t\t\t\terr = SymDiffError{\n\t\t\t\t\tnodes:  inputs,\n\t\t\t\t\tsingle: other,\n\t\t\t\t\tgrad:   grad,\n\t\t\t\t\terr:    errors.Wrap(err, \"While performing tensordot of (other × grad) in SymDiff of `tensordotOp`. Nodes() returns the inputs. Node() returns the `other`, Grad() returns grad`\"),\n\t\t\t\t}\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\tcase other.Shape().IsVector() && grad.Shape().IsVector() && 0 == len(dOtherAxes): // TensorMul does not support creating matrix from two vectors\n\t\t\t// Reformat vectors, so that MatMul will create a matrix from them\n\t\t\totherCorrectShape := other\n\t\t\tif !other.IsColVec() {\n\t\t\t\totherVecDims, err := (other.Shape()).DimSize(0)\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = SymDiffError{\n\t\t\t\t\t\tnodes:  inputs,\n\t\t\t\t\t\tsingle: other,\n\t\t\t\t\t\terr:    errors.Wrap(err, \"While getting .DimSize(0) of other, while SymDiff-ing. Nodes() returns the inputs, Node() returns `other`. There is no Grad or Grad map.\"),\n\t\t\t\t\t}\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\n\t\t\t\tif otherCorrectShape, err = Reshape(other, tensor.Shape{otherVecDims, 1}); err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tgradCorrectShape := grad\n\t\t\tif !grad.IsRowVec() {\n\t\t\t\tgradVecDims, err := (grad.Shape()).DimSize(0)\n\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\n\t\t\t\tif gradCorrectShape, err = Reshape(grad, tensor.Shape{1, gradVecDims}); err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t}\n\n\t\t\top := linAlgBinOp{āBinaryOperator: matMulOperator}\n\t\t\tif tensordot, err = binOpNode(op, otherCorrectShape, gradCorrectShape); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\tdefault:\n\t\t\ttensordot, err = Tensordot(dOtherAxes, dGradAxes, other, grad)\n\t\t}\n\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tret, err := Transpose(tensordot, pattern...)\n\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tretVal[inNr] = ret\n\t}\n\n\treturn retVal, nil\n}\n"
        },
        {
          "name": "op_math_cuda.go",
          "type": "blob",
          "size": 7.373046875,
          "content": "// +build cuda\n\npackage gorgonia\n\nimport (\n\t\"fmt\"\n\t\"unsafe\"\n\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/cu\"\n\t\"gorgonia.org/gorgonia/cuda\"\n\t\"gorgonia.org/tensor\"\n)\n\n// module names\nconst (\n\telemBinOpMod   = \"elembinop\"\n\telemUnaryOpMod = \"elemunaryop\"\n)\n\nfunc (op elemUnaryOp) CallsExtern() bool { return true }\n\nfunc (op elemUnaryOp) CUDADo(extern External, dev Device, prealloc Value, inputs ...Value) (retVal Value, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tcudaLogf(\"CUDADoing %v | prealloc %x | %x\", op, prealloc.Uintptr(), inputs[0].Uintptr())\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\t// check\n\tcudaLogf(\"checking if input is scalar\")\n\ta := inputs[0]\n\tdt := a.Dtype()\n\n\t// build name\n\tname := fmt.Sprintf(\"%v.%v_f%d\", elemUnaryOpMod, op.unaryOpType(), int(dt.Size())*8)\n\n\tmachine := extern.(CUDAMachine)\n\teng := machine.Engines()[int(dev)]\n\tif !eng.HasFunc(name) {\n\t\tcudaLogf(\"extern does not have func %q\", name)\n\t\textern.Signal()\n\n\t\tif retVal, err = op.do(a); err != nil {\n\t\t\treturn\n\t\t}\n\t\tif prealloc == nil {\n\t\t\treturn\n\t\t}\n\t\treturn Copy(prealloc, retVal)\n\t}\n\tfn := eng.Functions()[name]\n\tctx := machine.Contexts()[int(dev)]\n\n\tretVal = prealloc\n\tif prealloc == nil {\n\t\tprealloc = a\n\t\tretVal = a\n\t}\n\n\tvar mem cu.DevicePtr\n\tif prealloc.Uintptr() == a.Uintptr() && a.Shape().Eq(prealloc.Shape()) {\n\t\tmem = cu.DevicePtr(a.Uintptr())\n\t} else {\n\t\tmem = cu.DevicePtr(prealloc.Uintptr())\n\t\tmemSize := int64(a.MemSize())\n\t\tmemA := cu.DevicePtr(a.Uintptr())\n\t\tctx.Memcpy(mem, memA, memSize)\n\t}\n\tsize := logicalSize(a.Shape())\n\n\t// blocks, threads := machine.(*tapeMachine).blockThread(int(size), int(dev))\n\tgridDimX, gridDimY, gridDimZ, blockDimX, blockDimY, blockDimZ := machine.ElemGridSize(int(size), int(dev))\n\targs := []unsafe.Pointer{\n\t\tunsafe.Pointer(&mem),\n\t\tunsafe.Pointer(&size),\n\t}\n\tcudaLogf(\"gx %d, gy %d, gz %d | bx %d by %d, bz %d\", gridDimX, gridDimY, gridDimZ, blockDimX, blockDimY, blockDimZ)\n\tcudaLogf(\"CUDADO %q, Mem: %v size %v, args %v\", name, mem, size, args)\n\tcudaLogf(\"LaunchKernel Params. mem: %v. Size %v\", mem, size)\n\tctx.LaunchAndSync(fn, gridDimX, gridDimY, gridDimZ, blockDimX, blockDimY, blockDimZ, 0, cu.NoStream, args)\n\treturn\n}\n\nfunc (op elemBinOp) CallsExtern() bool { return true }\n\nfunc (op elemBinOp) CUDADo(extern External, dev Device, prealloc Value, inputs ...Value) (retVal Value, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\tcudaLogf(\"CUDADoing %v\", op)\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\ta := inputs[0]\n\tb := inputs[1]\n\tas := a.Shape()\n\tbs := b.Shape()\n\n\tm := extern.(CUDAMachine)\n\te := &m.Engines()[int(dev)]\n\n\tif as.IsScalar() && bs.IsScalar() {\n\t\treturn op.ssop(a, b, prealloc, e)\n\t}\n\n\tif aT, ok := a.(tensor.Tensor); ok {\n\t\ttensor.WithEngine(e)(aT)\n\t}\n\tif bT, ok := b.(tensor.Tensor); ok {\n\t\ttensor.WithEngine(e)(bT)\n\t}\n\n\tpT, toReuse := prealloc.(tensor.Tensor)\n\tif toReuse {\n\t\ttensor.WithEngine(e)(pT)\n\t}\n\n\tboType := op.binOpType()\n\tif fn := binOps[boType]; fn != nil {\n\t\tif toReuse {\n\t\t\treturn (*fn)(a, b, tensor.WithReuse(pT))\n\t\t}\n\t\treturn (*fn)(a, b, tensor.UseUnsafe())\n\t}\n\n\tif fn := cmpOps[boType]; fn != nil {\n\t\tif toReuse {\n\t\t\treturn (*fn)(a, b, tensor.WithReuse(pT))\n\t\t}\n\t\treturn (*fn)(a, b, tensor.UseUnsafe())\n\t}\n\n\treturn nil, errors.Errorf(\"op %v cannot be done by CUDA\", op)\n}\n\nfunc (op elemBinOp) ssop(a, b, prealloc Value, e *cuda.Engine) (retVal Value, err error) {\n\tdt := a.Dtype()\n\tctx := e.Context()\n\topName := ʘBinOpNames[op.binOpType()]\n\tname := fmt.Sprintf(\"%v.%v_ss_f%d\", elemBinOpMod, opName, int(dt.Size())*8)\n\tvar mem, memB cu.DevicePtr\n\tvar size int64\n\tif prealloc == nil {\n\t\tmem = cu.DevicePtr(a.Uintptr())\n\t\tretVal = a\n\t\tsize = int64(logicalSize(a.Shape()))\n\t} else {\n\t\tmem = cu.DevicePtr(prealloc.Uintptr())\n\t\tmemA := cu.DevicePtr(a.Uintptr())\n\t\tmemSize := int64(a.MemSize())\n\t\tctx.Memcpy(mem, memA, memSize)\n\n\t\tsize = int64(logicalSize(prealloc.Shape()))\n\t\tretVal = prealloc\n\t}\n\tmemB = cu.DevicePtr(b.Uintptr())\n\tfn := e.Functions()[name]\n\n\tvar args []unsafe.Pointer\n\tcudaLogf(\"%v mem %v, memB %v\", op, mem, memB)\n\tgridDimX, gridDimY, gridDimZ, blockDimX, blockDimY, blockDimZ := e.ElemGridSize(int(size))\n\targs = []unsafe.Pointer{\n\t\tunsafe.Pointer(&mem),\n\t\tunsafe.Pointer(&memB),\n\t\tunsafe.Pointer(&size),\n\t}\n\n\tcudaLogf(\"CUDADO %q, size %v\", name, size)\n\tcudaLogf(\"LaunchKernel params. mem: %v memB: %v size: %v\", mem, memB, size)\n\tcudaLogf(\"%d, %d, %d, %d, %d, %d\", gridDimX, gridDimY, gridDimZ, blockDimX, blockDimY, blockDimZ)\n\tctx.LaunchAndSync(fn, gridDimX, gridDimY, gridDimZ, blockDimX, blockDimY, blockDimZ, 0, cu.NoStream, args)\n\treturn\n}\n\n/* LINEAR ALGEBRA STUFF */\n\nfunc (op linAlgBinOp) CallsExtern() bool { return true }\n\nfunc (op linAlgBinOp) CUDADo(extern External, dev Device, prealloc Value, inputs ...Value) (retVal Value, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tm := extern.(CUDAMachine)\n\te := &m.Engines()[int(dev)]\n\n\ta := inputs[0]\n\tb := inputs[1]\n\n\taT, ok := a.(tensor.Tensor)\n\tif !ok {\n\t\treturn nil, errors.Errorf(\"Expected a a to be a Tensor. Got %T instead\", a)\n\t}\n\tbT, ok := b.(tensor.Tensor)\n\tif !ok {\n\t\treturn nil, errors.Errorf(\"Expected a b to be a Tensor. Got %T instead\", b)\n\t}\n\n\tpT, ok := prealloc.(tensor.Tensor)\n\tif !ok {\n\t\treturn nil, errors.Errorf(\"Expected a prealloc to be a Tensor. Got %T instead\", prealloc)\n\t}\n\ttensor.WithEngine(e)(bT)\n\ttensor.WithEngine(e)(aT)\n\ttensor.WithEngine(e)(pT)\n\n\tif op.transA && op.āBinaryOperator != batchedMatMulOperator {\n\t\tif err = aT.T(); err != nil {\n\t\t\treturn nil, errors.Wrap(err, tFail)\n\t\t}\n\t\t// untranspose\n\t\tdefer aT.T()\n\t}\n\n\tif op.transB && op.āBinaryOperator != batchedMatMulOperator {\n\t\tif err = bT.T(); err != nil {\n\t\t\treturn nil, errors.Wrap(err, tFail)\n\t\t}\n\t\t// untranspose\n\t\tdefer bT.T()\n\t}\n\n\tswitch op.āBinaryOperator {\n\tcase matMulOperator:\n\t\treturn tensor.MatMul(aT, bT, tensor.WithReuse(pT))\n\tcase matVecMulOperator:\n\t\treturn tensor.MatVecMul(aT, bT, tensor.WithReuse(pT))\n\tcase vecDotOperator:\n\t\treturn nil, errors.New(\"NYI\")\n\tcase outerProdOperator:\n\t\treturn tensor.Outer(aT, bT, tensor.WithReuse(pT))\n\tcase batchedMatMulOperator:\n\t\t// checks were done when the op was created\n\t\treturn batchedMatMul(aT, bT, nil, op.transA, op.transB, false)\n\t}\n\tpanic(\"Unreachable\")\n}\n\n/* API stuff  */\n\n// NewAddOp creates a new *ExternalOp that wraps a add op\nfunc NewAddOp(a, b *Node, ctx ExecutionContext) *ExternalOp {\n\tadd := newElemBinOp(addOpType, a, b)\n\top := NewExternalOp(add, ctx, nil)\n\tif a.Device() == CPU && b.Device() == CPU {\n\t\top.Device = CPU\n\t\treturn op\n\t}\n\n\tif a.Device() != CPU {\n\t\top.Device = a.Device()\n\t\treturn op\n\t}\n\n\tif b.Device() != CPU {\n\t\top.Device = b.Device()\n\t\treturn op\n\t}\n\n\treturn op\n}\n\n// NewSubOp creates a new *ExternalOp that wraps a sub op\nfunc NewSubOp(a, b *Node, ctx ExecutionContext) *ExternalOp {\n\tsub := newEBOByType(subOpType, a.t, b.t)\n\top := NewExternalOp(sub, ctx, nil)\n\n\tif a.Device() == CPU && b.Device() == CPU {\n\t\top.Device = CPU\n\t\treturn op\n\t}\n\n\tif a.Device() != CPU {\n\t\top.Device = a.Device()\n\t\treturn op\n\t}\n\n\tif b.Device() != CPU {\n\t\top.Device = b.Device()\n\t\treturn op\n\t}\n\treturn op\n}\n\n// NewHadamardProdOp creates a new *ExternalOp that wraps a mul op\nfunc NewHadamardProdOp(a, b *Node, ctx ExecutionContext) *ExternalOp {\n\tmul := newEBOByType(mulOpType, a.t, b.t)\n\top := NewExternalOp(mul, ctx, nil)\n\n\tif a.Device() == CPU && b.Device() == CPU {\n\t\top.Device = CPU\n\t\treturn op\n\t}\n\n\tif a.Device() != CPU {\n\t\top.Device = a.Device()\n\t\treturn op\n\t}\n\n\tif b.Device() != CPU {\n\t\top.Device = b.Device()\n\t\treturn op\n\t}\n\treturn op\n}\n"
        },
        {
          "name": "op_math_cuda_test.go",
          "type": "blob",
          "size": 3.169921875,
          "content": "// +build cuda\n\npackage gorgonia\n\nimport (\n\t\"log\"\n\t\"os\"\n\t\"runtime\"\n\t\"testing\"\n\n\t\"github.com/pkg/errors\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc TestCUDACube(t *testing.T) {\n\tdefer runtime.GC()\n\n\tassert := assert.New(t)\n\txT := tensor.New(tensor.Of(tensor.Float32), tensor.WithBacking(tensor.Range(Float32, 0, 32)), tensor.WithShape(8, 4))\n\n\tg := NewGraph(WithGraphName(\"Test\"))\n\tx := NewMatrix(g, tensor.Float32, WithName(\"x\"), WithShape(8, 4), WithValue(xT))\n\tx3 := Must(Cube(x))\n\tvar x3Val Value\n\tRead(x3, &x3Val)\n\n\tm := NewTapeMachine(g)\n\tdefer m.Close()\n\tif err := m.RunAll(); err != nil {\n\t\tt.Error(err)\n\t}\n\tcorrect := []float32{0, 1, 8, 27, 64, 125, 216, 343, 512, 729, 1000, 1331, 1728, 2197, 2744, 3375, 4096, 4913, 5832, 6859, 8000, 9261, 10648, 12167, 13824, 15625, 17576, 19683, 21952, 24389, 27000, 29791}\n\tassert.Equal(correct, x3Val.Data())\n\n\tt.Logf(\"0x%x\", x3Val.Uintptr())\n\tt.Logf(\"\\n%v\", m.cpumem[1])\n\tt.Logf(\"0x%x\", m.cpumem[1].Uintptr())\n\n\tcorrect = tensor.Range(tensor.Float32, 0, 32).([]float32)\n\tassert.Equal(correct, x.Value().Data())\n}\n\nfunc TestCUDABasicArithmetic(t *testing.T) {\n\tfor i, bot := range binOpTests {\n\t\t// if i != 5 {\n\t\t// \tcontinue\n\t\t// }\n\t\t// log.Printf(\"Test %d\", i)\n\t\tif err := testOneCUDABasicArithmetic(t, bot, i); err != nil {\n\t\t\tt.Fatalf(\"Test %d. Err %+v\", i, err)\n\t\t}\n\t\truntime.GC()\n\t}\n\n\t// logger = spare\n}\n\nfunc testOneCUDABasicArithmetic(t *testing.T, bot binOpTest, i int) error {\n\tg := NewGraph()\n\txV, _ := CloneValue(bot.a)\n\tyV, _ := CloneValue(bot.b)\n\tx := NodeFromAny(g, xV, WithName(\"x\"))\n\ty := NodeFromAny(g, yV, WithName(\"y\"))\n\n\tvar ret *Node\n\tvar retVal Value\n\tvar err error\n\tif ret, err = bot.binOp(x, y); err != nil {\n\t\treturn err\n\t}\n\tRead(ret, &retVal)\n\n\tcost := Must(Sum(ret))\n\tvar grads Nodes\n\tif grads, err = Grad(cost, x, y); err != nil {\n\t\treturn err\n\t}\n\n\tm1 := NewTapeMachine(g)\n\tdefer m1.Close()\n\tif err = m1.RunAll(); err != nil {\n\t\tt.Logf(\"%v\", m1.Prog())\n\t\treturn err\n\t}\n\n\tas := newAssertState(assert.New(t))\n\tas.Equal(bot.correct.Data(), retVal.Data(), \"Test %d result\", i)\n\tas.True(bot.correctShape.Eq(ret.Shape()))\n\tas.Equal(2, len(grads))\n\tas.Equal(bot.correctDerivA.Data(), grads[0].Value().Data(), \"Test %v xgrad\", i)\n\tas.Equal(bot.correctDerivB.Data(), grads[1].Value().Data(), \"Test %v ygrad. Expected %v. Got %v\", i, bot.correctDerivB, grads[1].Value())\n\tif !as.cont {\n\t\tprog := m1.Prog()\n\t\treturn errors.Errorf(\"Failed. Prog %v\", prog)\n\t}\n\treturn nil\n\n}\n\nfunc TestMultiDeviceArithmetic(t *testing.T) {\n\tg := NewGraph()\n\tx := NewMatrix(g, Float64, WithName(\"x\"), WithShape(2, 2))\n\ty := NewMatrix(g, Float64, WithName(\"y\"), WithShape(2, 2))\n\tz := Must(Add(x, y))\n\tzpx := Must(Add(x, z)) // z would be on device\n\tMust(Sum(zpx))\n\n\txV := tensor.New(tensor.WithBacking([]float64{0, 1, 2, 3}), tensor.WithShape(2, 2))\n\tyV := tensor.New(tensor.WithBacking([]float64{0, 1, 2, 3}), tensor.WithShape(2, 2))\n\n\tLet(x, xV)\n\tLet(y, yV)\n\n\tlogger := log.New(os.Stderr, \"\", 0)\n\tm := NewLispMachine(g, WithLogger(logger), WithWatchlist(), LogBothDir())\n\tdefer m.Close()\n\tt.Logf(\"zpx.Device: %v\", zpx.Device())\n\tt.Logf(\"x.Device: %v\", x.Device())\n\tt.Logf(\"y.Device: %v\", y.Device())\n\n\tif err := m.RunAll(); err != nil {\n\t\tt.Errorf(\"err: %+v\", err)\n\t}\n\n}\n"
        },
        {
          "name": "op_math_noextern.go",
          "type": "blob",
          "size": 0.9501953125,
          "content": "// +build !cuda\n\npackage gorgonia\n\nfunc (op elemUnaryOp) CallsExtern() bool { return false }\nfunc (op elemBinOp) CallsExtern() bool   { return false }\nfunc (op linAlgBinOp) CallsExtern() bool {\n\tif op.āBinaryOperator != vecDotOperator {\n\t\treturn true\n\t}\n\treturn false\n}\n\n// NewAddOp creates a new *ExternalOp that wraps an add op\nfunc NewAddOp(a, b *Node, ctx ExecutionContext) *ExternalOp {\n\tadd := newElemBinOp(addOpType, a, b)\n\top := NewExternalOp(add, ctx, nil)\n\top.Device = CPU\n\treturn op\n}\n\n// NewSubOp creates a new *ExternalOp that wraps a sub op\nfunc NewSubOp(a, b *Node, ctx ExecutionContext) *ExternalOp {\n\tsub := newEBOByType(subOpType, a.t, b.t)\n\top := NewExternalOp(sub, ctx, nil)\n\top.Device = CPU\n\treturn op\n}\n\n// NewHadamardProdOp creates a new *ExternalOp that wraps a mul op\nfunc NewHadamardProdOp(a, b *Node, ctx ExecutionContext) *ExternalOp {\n\tmul := newEBOByType(mulOpType, a.t, b.t)\n\top := NewExternalOp(mul, ctx, nil)\n\top.Device = CPU\n\treturn op\n}\n"
        },
        {
          "name": "op_math_test.go",
          "type": "blob",
          "size": 17.603515625,
          "content": "package gorgonia\n\nimport (\n\t\"log\"\n\t\"runtime\"\n\t\"testing\"\n\n\t\"github.com/pkg/errors\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"gorgonia.org/tensor\"\n)\n\ntype binOpTest struct {\n\tbinOp func(*Node, *Node) (*Node, error)\n\ta, b  Value\n\n\tcorrect       Value\n\tcorrectDerivA Value\n\tcorrectDerivB Value\n\tcorrectShape  tensor.Shape\n}\n\nvar binOpTests = []binOpTest{\n\n\t{Add,\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4})),\n\n\t\ttensor.New(tensor.WithBacking([]float64{2, 4, 6, 8})),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 1, 1, 1})),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 1, 1, 1})),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Add,\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\tNewF64(1.0),\n\n\t\ttensor.New(tensor.WithBacking([]float64{2, 3, 4, 5})),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 1, 1, 1})),\n\t\tNewF64(4.0),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Add,\n\t\tNewF64(1.0),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4})),\n\n\t\ttensor.New(tensor.WithBacking([]float64{2, 3, 4, 5})),\n\t\tNewF64(4.0),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 1, 1, 1})),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Add,\n\t\tNewF64(1.0),\n\t\tNewF64(1.0),\n\n\t\tNewF64(2.0),\n\t\tNewF64(1.0),\n\t\tNewF64(1.0),\n\t\tscalarShape,\n\t},\n\n\t{Sub,\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4})),\n\n\t\ttensor.New(tensor.WithBacking([]float64{0, 0, 0, 0})),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 1, 1, 1})),\n\t\ttensor.New(tensor.WithBacking([]float64{-1, -1, -1, -1})),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Sub,\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\tNewF64(1.0),\n\n\t\ttensor.New(tensor.WithBacking([]float64{0, 1, 2, 3})),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 1, 1, 1})),\n\t\tNewF64(-4.0),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Sub,\n\t\tNewF64(1.0),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4})),\n\n\t\ttensor.New(tensor.WithBacking([]float64{0, -1, -2, -3})),\n\t\tNewF64(4.0),\n\t\ttensor.New(tensor.WithBacking([]float64{-1, -1, -1, -1})),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Sub,\n\t\tNewF64(1.0),\n\t\tNewF64(1.0),\n\n\t\tNewF64(0.0),\n\t\tNewF64(1.0),\n\t\tNewF64(-1.0),\n\t\tscalarShape,\n\t},\n\n\t{HadamardProd,\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4})),\n\n\t\ttensor.New(tensor.WithBacking([]float64{1, 4, 9, 16})),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Mul,\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\tNewF64(1.0),\n\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 1, 1, 1})),\n\t\tNewF64(10),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Mul,\n\t\tNewF64(1.0),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4})),\n\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\tNewF64(10),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 1, 1, 1})),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Mul,\n\t\tNewF64(1.0),\n\t\tNewF64(1.0),\n\n\t\tNewF64(1.0),\n\t\tNewF64(1.0),\n\t\tNewF64(1.0),\n\t\tscalarShape,\n\t},\n\n\t{HadamardDiv,\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 1, 1, 1})),\n\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 1, 1, 1})),\n\t\ttensor.New(tensor.WithBacking([]float64{-1, -2, -3, -4})),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Div,\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\tNewF64(1.0),\n\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 1, 1, 1})),\n\t\tNewF64(-10),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Div,\n\t\tNewF64(1),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 1, 1, 1})),\n\n\t\ttensor.New(tensor.WithBacking([]float64{1, 1, 1, 1})),\n\t\tNewF64(4),\n\t\ttensor.New(tensor.WithBacking([]float64{-1, -1, -1, -1})),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Div,\n\t\tNewF64(1.0),\n\t\tNewF64(1.0),\n\n\t\tNewF64(1.0),\n\t\tNewF64(1.0),\n\t\tNewF64(-1.0),\n\t\tscalarShape,\n\t},\n\n\t// Float32\n\n\t{Add,\n\t\ttensor.New(tensor.WithBacking([]float32{1, 2, 3, 4})),\n\t\ttensor.New(tensor.WithBacking([]float32{1, 2, 3, 4})),\n\n\t\ttensor.New(tensor.WithBacking([]float32{2, 4, 6, 8})),\n\t\ttensor.New(tensor.WithBacking([]float32{1, 1, 1, 1})),\n\t\ttensor.New(tensor.WithBacking([]float32{1, 1, 1, 1})),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Add,\n\t\ttensor.New(tensor.WithBacking([]float32{1, 2, 3, 4})),\n\t\tNewF32(1.0),\n\n\t\ttensor.New(tensor.WithBacking([]float32{2, 3, 4, 5})),\n\t\ttensor.New(tensor.WithBacking([]float32{1, 1, 1, 1})),\n\t\tNewF32(4.0),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Add,\n\t\tNewF32(1.0),\n\t\ttensor.New(tensor.WithBacking([]float32{1, 2, 3, 4})),\n\n\t\ttensor.New(tensor.WithBacking([]float32{2, 3, 4, 5})),\n\t\tNewF32(4.0),\n\t\ttensor.New(tensor.WithBacking([]float32{1, 1, 1, 1})),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Add,\n\t\tNewF32(1.0),\n\t\tNewF32(1.0),\n\n\t\tNewF32(2.0),\n\t\tNewF32(1.0),\n\t\tNewF32(1.0),\n\t\tscalarShape,\n\t},\n\n\t{Sub,\n\t\ttensor.New(tensor.WithBacking([]float32{1, 2, 3, 4})),\n\t\ttensor.New(tensor.WithBacking([]float32{1, 2, 3, 4})),\n\n\t\ttensor.New(tensor.WithBacking([]float32{0, 0, 0, 0})),\n\t\ttensor.New(tensor.WithBacking([]float32{1, 1, 1, 1})),\n\t\ttensor.New(tensor.WithBacking([]float32{-1, -1, -1, -1})),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Sub,\n\t\ttensor.New(tensor.WithBacking([]float32{1, 2, 3, 4})),\n\t\tNewF32(1.0),\n\n\t\ttensor.New(tensor.WithBacking([]float32{0, 1, 2, 3})),\n\t\ttensor.New(tensor.WithBacking([]float32{1, 1, 1, 1})),\n\t\tNewF32(-4.0),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Sub,\n\t\tNewF32(1.0),\n\t\ttensor.New(tensor.WithBacking([]float32{1, 2, 3, 4})),\n\n\t\ttensor.New(tensor.WithBacking([]float32{0, -1, -2, -3})),\n\t\tNewF32(4.0),\n\t\ttensor.New(tensor.WithBacking([]float32{-1, -1, -1, -1})),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Sub,\n\t\tNewF32(1.0),\n\t\tNewF32(1.0),\n\n\t\tNewF32(0.0),\n\t\tNewF32(1.0),\n\t\tNewF32(-1.0),\n\t\tscalarShape,\n\t},\n\n\t{HadamardProd,\n\t\ttensor.New(tensor.WithBacking([]float32{1, 2, 3, 4})),\n\t\ttensor.New(tensor.WithBacking([]float32{1, 2, 3, 4})),\n\n\t\ttensor.New(tensor.WithBacking([]float32{1, 4, 9, 16})),\n\t\ttensor.New(tensor.WithBacking([]float32{1, 2, 3, 4})),\n\t\ttensor.New(tensor.WithBacking([]float32{1, 2, 3, 4})),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Mul,\n\t\ttensor.New(tensor.WithBacking([]float32{1, 2, 3, 4})),\n\t\tNewF32(1.0),\n\n\t\ttensor.New(tensor.WithBacking([]float32{1, 2, 3, 4})),\n\t\ttensor.New(tensor.WithBacking([]float32{1, 1, 1, 1})),\n\t\tNewF32(10),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Mul,\n\t\tNewF32(1.0),\n\t\ttensor.New(tensor.WithBacking([]float32{1, 2, 3, 4})),\n\n\t\ttensor.New(tensor.WithBacking([]float32{1, 2, 3, 4})),\n\t\tNewF32(10),\n\t\ttensor.New(tensor.WithBacking([]float32{1, 1, 1, 1})),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Mul,\n\t\tNewF32(1.0),\n\t\tNewF32(1.0),\n\n\t\tNewF32(1.0),\n\t\tNewF32(1.0),\n\t\tNewF32(1.0),\n\t\tscalarShape,\n\t},\n\n\t{HadamardDiv,\n\t\ttensor.New(tensor.WithBacking([]float32{1, 2, 3, 4})),\n\t\ttensor.New(tensor.WithBacking([]float32{1, 1, 1, 1})),\n\n\t\ttensor.New(tensor.WithBacking([]float32{1, 2, 3, 4})),\n\t\ttensor.New(tensor.WithBacking([]float32{1, 1, 1, 1})),\n\t\ttensor.New(tensor.WithBacking([]float32{-1, -2, -3, -4})),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Div,\n\t\ttensor.New(tensor.WithBacking([]float32{1, 2, 3, 4})),\n\t\tNewF32(1.0),\n\n\t\ttensor.New(tensor.WithBacking([]float32{1, 2, 3, 4})),\n\t\ttensor.New(tensor.WithBacking([]float32{1, 1, 1, 1})),\n\t\tNewF32(-10),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Div,\n\t\tNewF32(1),\n\t\ttensor.New(tensor.WithBacking([]float32{1, 1, 1, 1})),\n\n\t\ttensor.New(tensor.WithBacking([]float32{1, 1, 1, 1})),\n\t\tNewF32(4),\n\t\ttensor.New(tensor.WithBacking([]float32{-1, -1, -1, -1})),\n\t\ttensor.Shape{4},\n\t},\n\n\t{Div,\n\t\tNewF32(1.0),\n\t\tNewF32(1.0),\n\n\t\tNewF32(1.0),\n\t\tNewF32(1.0),\n\t\tNewF32(-1.0),\n\t\tscalarShape,\n\t},\n\n\t{\n\t\tfunc(a *Node, b *Node) (*Node, error) {\n\t\t\treturn BatchedMatMul(a, b, false, false)\n\t\t},\n\t\ttensor.New(tensor.WithShape(2, 3, 4), tensor.WithBacking([]float64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12})),\n\t\ttensor.New(tensor.WithShape(2, 4, 1), tensor.WithBacking([]float64{1, 2, 3, 4, 1, 2, 3, 4})),\n\n\t\ttensor.New(tensor.WithBacking([]float64{30, 70, 110, 30, 70, 110})),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4})),\n\t\ttensor.New(tensor.WithBacking([]float64{15, 18, 21, 24, 15, 18, 21, 24})),\n\t\ttensor.Shape{2, 3, 1},\n\t},\n}\n\nfunc TestBasicArithmetic(t *testing.T) {\n\tfor i, bot := range binOpTests {\n\t\tif err := testOneArithTape(t, bot, i); err != nil {\n\t\t\tt.Fatalf(\"Test %d, Err: %+v\", i, err)\n\t\t}\n\t\truntime.GC()\n\t}\n\n\tfor i, bot := range binOpTests {\n\t\t// log.Printf(\"Test %d\", i)\n\t\tif err := testOneArithLisp(t, bot, i); err != nil {\n\t\t\tt.Fatalf(\"Test %d, Err: %+v\", i, err)\n\t\t}\n\t\truntime.GC()\n\t}\n}\n\nfunc testOneArithLisp(t *testing.T, bot binOpTest, i int) error {\n\tg := NewGraph()\n\txV, _ := CloneValue(bot.a)\n\tyV, _ := CloneValue(bot.b)\n\tx := NodeFromAny(g, xV, WithName(\"x\"))\n\ty := NodeFromAny(g, yV, WithName(\"y\"))\n\n\tvar ret *Node\n\tvar retVal Value\n\tvar err error\n\tif ret, err = bot.binOp(x, y); err != nil {\n\t\treturn errors.Wrapf(err, \"do binop failure\")\n\t}\n\tRead(ret, &retVal)\n\n\tif !(xV.Shape().IsScalar() && yV.Shape().IsScalar()) {\n\t\tMust(Sum(ret))\n\t}\n\tm1 := NewLispMachine(g)\n\tdefer m1.Close()\n\tif err = m1.RunAll(); err != nil {\n\t\treturn errors.Wrapf(err, \"Error while running\")\n\t}\n\n\tas := newAssertState(assert.New(t))\n\tas.Equal(bot.correct.Data(), retVal.Data(), \"Test %d result\", i)\n\tas.True(bot.correctShape.Eq(ret.Shape()))\n\n\tvar xG, yG Value\n\tif xG, err = x.Grad(); err != nil {\n\t\treturn errors.Wrapf(err, \"Failed to get the grad of x\")\n\t}\n\n\tif yG, err = y.Grad(); err != nil {\n\t\treturn errors.Wrapf(err, \"Failed to get the grad of y\")\n\t}\n\n\tas.Equal(bot.correctDerivA.Data(), xG.Data(), \"Test %v xgrad\", i)\n\tas.Equal(bot.correctDerivB.Data(), yG.Data(), \"Test %v ygrad. Expected %v. Got %v\", i, bot.correctDerivB, yG)\n\tif !as.cont {\n\t\tt.Errorf(\"an error occurred\")\n\t}\n\n\tif assertGraphEngine(t, g, stdengType); t.Failed() {\n\t\treturn errors.New(\"Lisp Machine Graph Engine expected\")\n\t}\n\treturn nil\n}\n\nfunc testOneArithTape(t *testing.T, bot binOpTest, i int) error {\n\tg := NewGraph()\n\txV, _ := CloneValue(bot.a)\n\tyV, _ := CloneValue(bot.b)\n\tx := NodeFromAny(g, xV, WithName(\"x\"))\n\ty := NodeFromAny(g, yV, WithName(\"y\"))\n\n\tvar ret *Node\n\tvar retVal Value\n\tvar err error\n\tif ret, err = bot.binOp(x, y); err != nil {\n\t\treturn errors.Wrapf(err, \"binOp() failed\")\n\t}\n\tRead(ret, &retVal)\n\n\tcost := Must(Sum(ret))\n\tvar grads Nodes\n\tif grads, err = Grad(cost, x, y); err != nil {\n\t\treturn errors.Wrapf(err, \"Grad failed\")\n\t}\n\n\tm1 := NewTapeMachine(g)\n\tdefer m1.Close()\n\tif err = m1.RunAll(); err != nil {\n\t\tt.Logf(\"%v\", m1.Prog())\n\t\treturn errors.Wrapf(err, \"Error while running\")\n\t}\n\n\tas := newAssertState(assert.New(t))\n\tas.True(bot.a.Shape().Eq(x.Shape()), \"Test op doesn't change shape of input node\")\n\tas.True(bot.b.Shape().Eq(y.Shape()), \"Test op doesn't change shape of input node\")\n\tas.Equal(bot.correct.Data(), retVal.Data(), \"Test %d result\", i)\n\tas.True(bot.correctShape.Eq(ret.Shape()))\n\tas.Equal(2, len(grads))\n\tas.Equal(bot.correctDerivA.Data(), grads[0].Value().Data(), \"Test %v xgrad\", i)\n\tas.Equal(bot.correctDerivB.Data(), grads[1].Value().Data(), \"Test %v ygrad. Expected %v. Got %v\", i, bot.correctDerivB, grads[1].Value())\n\tif !as.cont {\n\t\tprog := m1.Prog()\n\t\treturn errors.Errorf(\"Failed. Prog %v\", prog)\n\t}\n\n\tif assertGraphEngine(t, g, stdengType); t.Failed() {\n\t\treturn errors.Errorf(\"BasicArithmetic. Engine of Graph is not stdengType.\")\n\t}\n\treturn nil\n}\n\nfunc TestTensordotOpDoDiff(t *testing.T) {\n\tassert := assert.New(t)\n\n\t// Vectors\n\tg := NewGraph()\n\ta := NewTensor(g, Float64, 1, WithName(\"a\"), WithShape(1))\n\tb := NewTensor(g, Float64, 1, WithName(\"b\"), WithShape(1))\n\n\ttensordot := tensordotOp{\n\t\taAxes:   []int{0},\n\t\tbAxes:   []int{0},\n\t\taDims:   0,\n\t\tbDims:   0,\n\t\tretDims: 0,\n\t}\n\n\tc, err := ApplyOp(tensordot, a, b)\n\n\tif err != nil {\n\t\tlog.Fatalf(\"scalars: Cannot ApplyOp: %+v\", err)\n\t\treturn\n\t}\n\n\taT := tensor.New(tensor.WithShape(), tensor.WithBacking([]float64{2}))\n\tbT := tensor.New(tensor.WithShape(), tensor.WithBacking([]float64{21}))\n\tcT := tensor.New(tensor.WithShape(), tensor.WithBacking([]float64{1})) // Backing doesn't matter as long as it is set\n\n\taVal, _, _, _ := anyToValue(aT)\n\tbVal, _, _, _ := anyToValue(bT)\n\tcVal, _, _, _ := anyToValue(cT)\n\n\ta.bind(dvUnit(aVal))\n\tb.bind(dvUnit(bVal))\n\tc.bind(dvUnitVar(cVal)) // Will set Output derivative to all ones\n\n\tif err := tensordot.DoDiff(ExecutionContext{}, Nodes{a, b}, c); err != nil {\n\t\tt.Fatalf(\"scalars: Cannot DoDiff: %+v\", err)\n\t}\n\n\taG, _ := a.Grad()\n\taGfloat := aG.Data()\n\n\tbG, _ := b.Grad()\n\tbGfloat := bG.Data()\n\n\taGcorrect := 21.0\n\tbGcorrect := 2.0\n\n\tassert.Equal(aGcorrect, aGfloat)\n\tassert.Equal(bGcorrect, bGfloat)\n\n\t// Vectors\n\n\tg = NewGraph()\n\ta = NewTensor(g, Float64, 1, WithName(\"a\"), WithShape(2))\n\tb = NewTensor(g, Float64, 1, WithName(\"b\"), WithShape(2))\n\n\ttensordot = tensordotOp{\n\t\taAxes:   []int{0},\n\t\tbAxes:   []int{0},\n\t\taDims:   1,\n\t\tbDims:   1,\n\t\tretDims: 1,\n\t}\n\n\tif c, err = ApplyOp(tensordot, a, b); err != nil {\n\t\tlog.Fatal(\"vectors: Cannot ApplyOp:\", err)\n\t\treturn\n\t}\n\n\taT = tensor.New(tensor.WithShape(2), tensor.WithBacking([]float64{1, 2}))\n\tbT = tensor.New(tensor.WithShape(2), tensor.WithBacking([]float64{3, 4}))\n\tcT = tensor.New(tensor.WithShape(1), tensor.WithBacking([]float64{1})) // Backing doesn't matter as long as it is set\n\n\taVal, _, _, _ = anyToValue(aT)\n\tbVal, _, _, _ = anyToValue(bT)\n\tcVal, _, _, _ = anyToValue(cT)\n\n\ta.bind(dvUnit(aVal))\n\tb.bind(dvUnit(bVal))\n\tc.bind(dvUnitVar(cVal)) // Will set Output derivative to all ones\n\n\tif err := tensordot.DoDiff(ExecutionContext{}, Nodes{a, b}, c); err != nil {\n\t\tlog.Fatal(\"vectors: Cannot DoDiff:\", err)\n\t\treturn\n\t}\n\n\taG, _ = a.Grad()\n\tbG, _ = b.Grad()\n\n\taGfloats := extractF64s(aG)\n\tbGfloats := extractF64s(bG)\n\n\taGcorrectFloats := []float64{3, 4}\n\tbGcorrectFloats := []float64{1, 2}\n\n\tassert.Equal(aGcorrectFloats, aGfloats)\n\tassert.Equal(bGcorrectFloats, bGfloats)\n\n\t// Matrix and Vector\n\n\tg = NewGraph()\n\ta = NewTensor(g, Float64, 2, WithName(\"a\"), WithShape(2, 2))\n\tb = NewTensor(g, Float64, 1, WithName(\"b\"), WithShape(2))\n\n\ttensordot = tensordotOp{\n\t\taAxes:   []int{1},\n\t\tbAxes:   []int{0},\n\t\taDims:   2,\n\t\tbDims:   1,\n\t\tretDims: 1,\n\t}\n\n\tif c, err = ApplyOp(tensordot, a, b); err != nil {\n\t\tlog.Fatal(\"matrix vector: Cannot ApplyOp:\", err)\n\t\treturn\n\t}\n\n\taT = tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 2, 3, 4}))\n\tbT = tensor.New(tensor.WithShape(2), tensor.WithBacking([]float64{1, 2}))\n\tcT = tensor.New(tensor.WithShape(2), tensor.WithBacking([]float64{1, 1})) // Backing doesn't matter as long as it is set\n\n\taVal, _, _, _ = anyToValue(aT)\n\tbVal, _, _, _ = anyToValue(bT)\n\tcVal, _, _, _ = anyToValue(cT)\n\n\ta.bind(dvUnit(aVal))\n\tb.bind(dvUnit(bVal))\n\tc.bind(dvUnitVar(cVal)) // Will set Output derivative to all ones\n\n\tif err := tensordot.DoDiff(ExecutionContext{}, Nodes{a, b}, c); err != nil {\n\t\tlog.Fatal(\"matrix vector: Cannot DoDiff:\", err)\n\t\treturn\n\t}\n\n\taG, _ = a.Grad()\n\tbG, _ = b.Grad()\n\n\taGfloats = extractF64s(aG)\n\tbGfloats = extractF64s(bG)\n\n\taGcorrectFloats = []float64{1, 2, 1, 2}\n\tbGcorrectFloats = []float64{4, 6}\n\n\tassert.Equal(aGcorrectFloats, aGfloats)\n\tassert.Equal(bGcorrectFloats, bGfloats)\n\n\t// Matrix multiplication\n\n\tg = NewGraph()\n\n\ta = NewTensor(g, Float64, 2, WithName(\"a\"), WithShape(2, 2))\n\tb = NewTensor(g, Float64, 2, WithName(\"b\"), WithShape(2, 2))\n\n\ttensordot = tensordotOp{\n\t\taAxes:   []int{1},\n\t\tbAxes:   []int{0},\n\t\taDims:   2,\n\t\tbDims:   2,\n\t\tretDims: 2,\n\t}\n\n\tif c, err = ApplyOp(tensordot, a, b); err != nil {\n\t\tlog.Fatal(\"matrices: Cannot ApplyOp:\", err)\n\t\treturn\n\t}\n\n\taT = tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 2, 3, 4}))\n\tbT = tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 2, 3, 4}))\n\tcT = tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 1, 1, 1})) // Backing doesn't matter as long as it is set\n\n\taVal, _, _, _ = anyToValue(aT)\n\tbVal, _, _, _ = anyToValue(bT)\n\tcVal, _, _, _ = anyToValue(cT)\n\n\ta.bind(dvUnit(aVal))\n\tb.bind(dvUnit(bVal))\n\tc.bind(dvUnitVar(cVal)) // Will set Output derivative to all ones\n\n\tif err := tensordot.DoDiff(ExecutionContext{}, Nodes{a, b}, c); err != nil {\n\t\tlog.Fatal(\"matrices: Cannot DoDiff:\", err)\n\t\treturn\n\t}\n\n\taG, _ = a.Grad()\n\tbG, _ = b.Grad()\n\n\taGfloats = extractF64s(aG)\n\tbGfloats = extractF64s(bG)\n\n\taGcorrectFloats = []float64{3, 7, 3, 7}\n\tbGcorrectFloats = []float64{4, 4, 6, 6}\n\n\tassert.Equal(aGcorrectFloats, aGfloats)\n\tassert.Equal(bGcorrectFloats, bGfloats)\n\n\t// Total matrix contraction\n\n\tg = NewGraph()\n\n\ta = NewTensor(g, Float64, 2, WithName(\"a\"), WithShape(2, 2))\n\tb = NewTensor(g, Float64, 2, WithName(\"b\"), WithShape(2, 2))\n\n\ttensordot = tensordotOp{\n\t\taAxes:   []int{1, 0},\n\t\tbAxes:   []int{0, 1},\n\t\taDims:   2,\n\t\tbDims:   2,\n\t\tretDims: 1,\n\t}\n\n\tif c, err = ApplyOp(tensordot, a, b); err != nil {\n\t\tlog.Fatal(\"matrices total contraction: Cannot ApplyOp:\", err)\n\t\treturn\n\t}\n\n\taT = tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 2, 3, 4}))\n\tbT = tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{5, 6, 7, 8}))\n\tcT = tensor.New(tensor.WithShape(1), tensor.WithBacking([]float64{1})) // Backing doesn't matter as long as it is set\n\n\taVal, _, _, _ = anyToValue(aT)\n\tbVal, _, _, _ = anyToValue(bT)\n\tcVal, _, _, _ = anyToValue(cT)\n\n\ta.bind(dvUnit(aVal))\n\tb.bind(dvUnit(bVal))\n\tc.bind(dvUnitVar(cVal)) // Will set Output derivative to all ones\n\n\tif err := tensordot.DoDiff(ExecutionContext{}, Nodes{a, b}, c); err != nil {\n\t\tlog.Fatal(\"matrices total contraction: Cannot DoDiff:\", err)\n\t\treturn\n\t}\n\n\taG, _ = a.Grad()\n\tbG, _ = b.Grad()\n\n\taGfloats = extractF64s(aG)\n\tbGfloats = extractF64s(bG)\n\n\taGcorrectFloats = []float64{5, 7, 6, 8}\n\tbGcorrectFloats = []float64{1, 3, 2, 4}\n\n\tassert.Equal(aGcorrectFloats, aGfloats)\n\tassert.Equal(bGcorrectFloats, bGfloats)\n\n}\n\nfunc TestLinearAlgebraOps(t *testing.T) {\n\tg := NewGraph()\n\tx := NewMatrix(g, Float64, WithShape(2, 3), WithName(\"x\"))\n\ty := NewMatrix(g, Float64, WithShape(3, 5), WithName(\"y\"))\n\tif _, err := Mul(x, y); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif _, err := Mul(y, x); err == nil {\n\t\tt.Error(\"Expect an error\")\n\t}\n}\n"
        },
        {
          "name": "op_minmaxBetween.go",
          "type": "blob",
          "size": 5.982421875,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"hash\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\n/* MIN BETWEEN */\n\ntype minBetween struct{}\n\n// Arity returns the number of inputs the Op expects. -1 indicates that it's n-ary and will be determined at runtime\nfunc (op minBetween) Arity() int { return 2 }\n\n// Informs the type of the Op (not the node). This will be used by the type system to infer the final type of the node\nfunc (op minBetween) Type() hm.Type {\n\treturn hm.NewFnType(hm.TypeVariable('a'), hm.TypeVariable('a'), hm.TypeVariable('a'))\n}\n\n// returns the output shape as a function of the inputs\nfunc (op minBetween) InferShape(shps ...DimSizer) (tensor.Shape, error) {\n\tif err := checkArity(op, len(shps)); err != nil {\n\t\treturn nil, err\n\t}\n\ta := shps[0].(tensor.Shape)\n\tb := shps[1].(tensor.Shape)\n\tif !a.Eq(b) {\n\t\treturn nil, errors.Errorf(\"Expected both inputs to have the same shape. Got %v and %v instead\", a, b)\n\t}\n\treturn a.Clone(), nil\n}\n\n// Do executes the op\nfunc (op minBetween) Do(vs ...Value) (Value, error) {\n\tif err := checkArity(op, len(vs)); err != nil {\n\t\treturn nil, err\n\t}\n\ta := vs[0]\n\tb := vs[1]\n\n\treturn tensor.MinBetween(a, b)\n}\n\n// ReturnsPtr returns false\nfunc (op minBetween) ReturnsPtr() bool { return false }\n\n// CallsExtern returns false\nfunc (op minBetween) CallsExtern() bool { return false }\n\nfunc (op minBetween) OverwritesInput() int { return -1 }\n\n/* Other methods */\nfunc (op minBetween) WriteHash(h hash.Hash) { fmt.Fprintf(h, op.String()) }\n\nfunc (op minBetween) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op minBetween) String() string { return \"MinBetween\" }\n\nfunc (op minBetween) UsePreallocDo(prealloc Value, vs ...Value) (Value, error) {\n\tif err := checkArity(op, len(vs)); err != nil {\n\t\treturn nil, err\n\t}\n\ta := vs[0]\n\tb := vs[1]\n\n\treturn tensor.MinBetween(a, b, tensor.WithReuse(prealloc.(tensor.Tensor)))\n}\n\nfunc (op minBetween) DiffWRT(inputs int) []bool { return []bool{true, true} }\nfunc (op minBetween) SymDiff(inputs Nodes, output, grad *Node) (Nodes, error) {\n\treturn minmaxSymDiff(inputs[0], inputs[1], output, grad)\n}\nfunc (op minBetween) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) error {\n\treturn minmaxAutoDiff(ctx, inputs[0], inputs[1], output)\n}\n\n/* MAX BETWEEN */\n\ntype maxBetween struct{}\n\n// Arity returns the number of inputs the Op expects. -1 indicates that it's n-ary and will be determaxed at runtime\nfunc (op maxBetween) Arity() int { return 2 }\n\n// Informs the type of the Op (not the node). This will be used by the type system to infer the final type of the node\nfunc (op maxBetween) Type() hm.Type {\n\treturn hm.NewFnType(hm.TypeVariable('a'), hm.TypeVariable('a'), hm.TypeVariable('a'))\n}\n\n// returns the output shape as a function of the inputs\nfunc (op maxBetween) InferShape(shps ...DimSizer) (tensor.Shape, error) {\n\tif err := checkArity(op, len(shps)); err != nil {\n\t\treturn nil, err\n\t}\n\ta := shps[0].(tensor.Shape)\n\tb := shps[1].(tensor.Shape)\n\tif !a.Eq(b) {\n\t\treturn nil, errors.Errorf(\"Expected both inputs to have the same shape. Got %v and %v instead\", a, b)\n\t}\n\treturn a.Clone(), nil\n}\n\n// Do executes the op\nfunc (op maxBetween) Do(vs ...Value) (Value, error) {\n\tif err := checkArity(op, len(vs)); err != nil {\n\t\treturn nil, err\n\t}\n\ta := vs[0]\n\tb := vs[1]\n\n\treturn tensor.MaxBetween(a, b)\n}\n\n// ReturnsPtr returns false\nfunc (op maxBetween) ReturnsPtr() bool { return false }\n\n// CallsExtern returns false\nfunc (op maxBetween) CallsExtern() bool { return false }\n\nfunc (op maxBetween) OverwritesInput() int { return -1 }\n\n/* Other methods */\nfunc (op maxBetween) WriteHash(h hash.Hash) { fmt.Fprintf(h, op.String()) }\n\nfunc (op maxBetween) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op maxBetween) String() string { return \"MaxBetween\" }\n\nfunc (op maxBetween) UsePreallocDo(prealloc Value, vs ...Value) (Value, error) {\n\tif err := checkArity(op, len(vs)); err != nil {\n\t\treturn nil, err\n\t}\n\ta := vs[0]\n\tb := vs[1]\n\n\treturn tensor.MaxBetween(a, b, tensor.WithReuse(prealloc.(tensor.Tensor)))\n}\n\nfunc (op maxBetween) DiffWRT(inputs int) []bool { return []bool{true, true} }\nfunc (op maxBetween) SymDiff(inputs Nodes, output, grad *Node) (Nodes, error) {\n\treturn minmaxSymDiff(inputs[0], inputs[1], output, grad)\n}\nfunc (op maxBetween) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) error {\n\treturn minmaxAutoDiff(ctx, inputs[0], inputs[1], output)\n}\n\nfunc minmaxSymDiff(a, b *Node, out *Node, grad *Node) (Nodes, error) {\n\tmask, err := Eq(a, out, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tWithGroupName(gradClust)(mask)\n\n\tgradA, err := HadamardProd(grad, mask)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tWithGroupName(gradClust)(gradA)\n\tgradB, err := Sub(grad, gradA)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tWithGroupName(gradClust)(gradB)\n\treturn Nodes{gradA, gradB}, nil\n}\n\nfunc minmaxAutoDiff(ctx ExecutionContext, a, b *Node, output *Node) (err error) {\n\t// dummy for now so let's keep everything as simple as possible\n\tadv, bdv := getDV(a, b)\n\toutdv := output.boundTo.(*dualValue)\n\n\teqOp := newElemBinOp(ltOpType, a, b)\n\teqOp.retSame = true\n\teq := &ExternalOp{\n\t\tOp:               eqOp,\n\t\tExecutionContext: ctx,\n\t}\n\tctx.Device = a.Device()\n\tmask, err := eq.Do(adv.Value, outdv.Value)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"Unable to get mask\")\n\t}\n\n\tdev := a.Device()\n\n\tvar gradA, gradB, gradOut Value\n\tvar extra bool\n\n\tif gradOut, extra, err = output.GradOnDevice(dev, ctx.External); err != nil {\n\t\treturn errors.Wrapf(err, gradOnDeviceFail, output, dev)\n\t}\n\tif extra {\n\t\tdefer ctx.PutValue(dev, gradOut)\n\t}\n\n\tif gradA, extra, err = a.GradOnDevice(dev, ctx.External); err != nil {\n\t\treturn errors.Wrapf(err, gradOnDeviceFail, a, dev)\n\t}\n\tif extra {\n\t\tdefer ctx.PutValue(dev, gradA)\n\t}\n\n\tmul := NewHadamardProdOp(a, output, ctx)\n\tmul.Incr = gradA\n\n\tvar d Value\n\tif d, err = mul.Do(gradOut, mask); err != nil {\n\t\treturn errors.Wrapf(err, \"IncrDo gradA failed\")\n\t}\n\tadv.SetDeriv(d)\n\n\tsub := NewSubOp(b, a, ctx)\n\tsub.Incr = gradB\n\tif d, err = sub.Do(gradOut, adv.d); err != nil {\n\t\treturn errors.Wrapf(err, \"IncrDo gradB failed\")\n\t}\n\tbdv.SetDeriv(d)\n\treturn nil\n\n}\n"
        },
        {
          "name": "op_minmax_test.go",
          "type": "blob",
          "size": 1.697265625,
          "content": "package gorgonia\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc TestMinBetween(t *testing.T) {\n\tassert := assert.New(t)\n\tg := NewGraph()\n\ta := NodeFromAny(g, tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{\n\t\t1000, 2,\n\t\t3, 4,\n\t})), WithName(\"a\"))\n\tb := NodeFromAny(g, tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{\n\t\t100, 200,\n\t\t300, 400,\n\t})), WithName(\"b\"))\n\n\top := minBetween{}\n\tc, err := ApplyOp(op, a, b)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\ts, err := Sum(c)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tgrads, err := Grad(s, a, b)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tm := NewTapeMachine(g, TraceExec())\n\tif err = m.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tcorrectGradA := []float64{0, 1, 1, 1}\n\tcorrectGradB := []float64{1, 0, 0, 0}\n\n\tassert.Equal(correctGradA, grads[0].Value().Data())\n\tassert.Equal(correctGradB, grads[1].Value().Data())\n}\n\nfunc TestMaxBetween(t *testing.T) {\n\tassert := assert.New(t)\n\tg := NewGraph()\n\ta := NodeFromAny(g, tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{\n\t\t1000, 2,\n\t\t3, 4,\n\t})), WithName(\"a\"))\n\tb := NodeFromAny(g, tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{\n\t\t100, 200,\n\t\t300, 400,\n\t})), WithName(\"b\"))\n\n\top := maxBetween{}\n\tc, err := ApplyOp(op, a, b)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\ts, err := Sum(c)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tgrads, err := Grad(s, a, b)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tm := NewTapeMachine(g, TraceExec())\n\tif err = m.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tcorrectGradA := []float64{1, 0, 0, 0}\n\tcorrectGradB := []float64{0, 1, 1, 1}\n\n\tassert.Equal(correctGradA, grads[0].Value().Data())\n\tassert.Equal(correctGradB, grads[1].Value().Data())\n}\n"
        },
        {
          "name": "op_nn.go",
          "type": "blob",
          "size": 52.9814453125,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"hash\"\n\t\"math\"\n\t\"runtime\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/chewxy/math32\"\n\trng \"github.com/leesper/go_rng\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\n// Sanity checks\nvar (\n\t_ SDOp = im2colOp{}\n\t_ Op   = col2imOp{}\n\t_ Op   = &maxPoolOp{}\n\t_ Op   = &maxPoolDiffOp{}\n\t_ Op   = &BatchNormOp{}\n\t_ Op   = &batchnormDiffOp{}\n\t_ Op   = &globalAveragePoolOp{}\n\n\t_ UsePreallocDoer = im2colOp{}\n)\n\n/*\n\tThis file contains all the Ops related to building a neural network.\n\n\tBear in mind that not all things that are related to a neural network are here, as not everything\n\tare encoded as Ops the way theano does it.\n\n\tSee also: nn.go for functions that relate to neural networks\n*/\n\ntype randomness byte\n\nconst (\n\tuniform randomness = iota\n\tgaussian\n\tbinomial\n)\n\ntype randomOp struct {\n\twhich randomness\n\tshape tensor.Shape\n\tdt    tensor.Dtype\n\n\ta, b float64 // when uniform, a,b = low, high; when gaussian, a,b = mean, stdev\n}\n\nfunc makeRandomOp(which randomness, dt tensor.Dtype, a, b float64, shape ...int) randomOp {\n\treturn randomOp{\n\t\twhich: which,\n\t\tshape: tensor.Shape(shape),\n\t\tdt:    dt,\n\t\ta:     a,\n\t\tb:     b,\n\t}\n}\n\nfunc (op randomOp) Arity() int { return 0 }\n\n// randomOp :: a\n// randomOp :: Tensor a\nfunc (op randomOp) Type() hm.Type {\n\tif op.shape.IsScalar() {\n\t\treturn op.dt\n\t}\n\ttt := newTensorType(op.shape.Dims(), op.dt)\n\treturn tt\n}\n\nfunc (op randomOp) InferShape(...DimSizer) (tensor.Shape, error) { return op.shape, nil }\n\nfunc (op randomOp) Do(...Value) (retVal Value, err error) {\n\tif op.shape.IsScalar() {\n\t\tvar v interface{}\n\t\tswitch op.dt {\n\t\tcase Float64:\n\t\t\tswitch op.which {\n\t\t\tcase uniform:\n\t\t\t\trand := rng.NewUniformGenerator(time.Now().UnixNano())\n\t\t\t\tv = rand.Float64Range(op.a, op.b)\n\t\t\tcase gaussian:\n\t\t\t\trand := rng.NewGaussianGenerator(time.Now().UnixNano())\n\t\t\t\tv = rand.Gaussian(op.a, op.b)\n\t\t\tcase binomial:\n\t\t\t\trand := rng.NewBinomialGenerator(time.Now().UnixNano())\n\t\t\t\tv = float64(rand.Binomial(int64(op.a), op.b))\n\t\t\t}\n\t\tcase Float32:\n\t\t\tswitch op.which {\n\t\t\tcase uniform:\n\t\t\t\trand := rng.NewUniformGenerator(time.Now().UnixNano())\n\t\t\t\tv = rand.Float32Range(float32(op.a), float32(op.b))\n\t\t\tcase gaussian:\n\t\t\t\trand := rng.NewGaussianGenerator(time.Now().UnixNano())\n\t\t\t\tv = float32(rand.Gaussian(op.a, op.b))\n\t\t\tcase binomial:\n\t\t\t\trand := rng.NewBinomialGenerator(time.Now().UnixNano())\n\t\t\t\tv = float32(rand.Binomial(int64(op.a), op.b))\n\t\t\t}\n\t\tdefault:\n\t\t\treturn nil, errors.Errorf(nyiFail, \"randomOp.do()\", op.dt)\n\t\t}\n\n\t\tretVal, _ = anyToScalar(v)\n\t\treturn\n\t}\n\n\tswitch op.dt {\n\tcase Float64:\n\t\tswitch op.which {\n\t\tcase uniform:\n\t\t\tbacking := Uniform64(op.a, op.b, op.shape...)\n\t\t\tretVal = tensor.New(tensor.WithBacking(backing), tensor.WithShape(op.shape...))\n\t\tcase gaussian:\n\t\t\tbacking := Gaussian64(op.a, op.b, op.shape...)\n\t\t\tretVal = tensor.New(tensor.WithBacking(backing), tensor.WithShape(op.shape...))\n\t\tcase binomial:\n\t\t\tbacking := Binomial64(op.a, op.b, op.shape...)\n\t\t\tretVal = tensor.New(tensor.WithBacking(backing), tensor.WithShape(op.shape...))\n\t\t}\n\t\treturn\n\tcase Float32:\n\t\tswitch op.which {\n\t\tcase uniform:\n\t\t\tbacking := Uniform32(op.a, op.b, op.shape...)\n\t\t\tretVal = tensor.New(tensor.WithBacking(backing), tensor.WithShape(op.shape...))\n\t\tcase gaussian:\n\t\t\tbacking := Gaussian32(op.a, op.b, op.shape...)\n\t\t\tretVal = tensor.New(tensor.WithBacking(backing), tensor.WithShape(op.shape...))\n\t\tcase binomial:\n\t\t\tbacking := Binomial32(op.a, op.b, op.shape...)\n\t\t\tretVal = tensor.New(tensor.WithBacking(backing), tensor.WithShape(op.shape...))\n\t\t}\n\t\treturn\n\tdefault:\n\t\treturn nil, errors.Errorf(nyiFail, \"randomOp.do() for non-scalar\", op.dt)\n\t}\n}\n\nfunc (op randomOp) ReturnsPtr() bool     { return false }\nfunc (op randomOp) CallsExtern() bool    { return false }\nfunc (op randomOp) OverwritesInput() int { return -1 }\nfunc (op randomOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, \"%d%v%f%f\", op.which, op.shape, op.a, op.b)\n}\n\nfunc (op randomOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op randomOp) String() string {\n\treturn fmt.Sprintf(\"%v(%v, %v) - %v\", op.which, op.a, op.b, op.shape)\n}\n\ntype im2colOp struct {\n\th, w                 int // kernel height and width\n\tpadH, padW           int\n\tstrideH, strideW     int\n\tdilationH, dilationW int\n}\n\nfunc makeIm2ColOp(kernelHeight, kernelWidth, padHeight, padWidth, strideHeight, strideWidth, dilationHeight, dilationWidth int) im2colOp {\n\treturn im2colOp{\n\t\th:         kernelHeight,\n\t\tw:         kernelWidth,\n\t\tpadH:      padHeight,\n\t\tpadW:      padWidth,\n\t\tstrideH:   strideHeight,\n\t\tstrideW:   strideWidth,\n\t\tdilationH: dilationHeight,\n\t\tdilationW: dilationWidth,\n\t}\n}\n\nfunc (op im2colOp) Arity() int { return 1 }\n\n// im2col :: (Floats a) ⇒ Tensor a →  Tensor a\nfunc (op im2colOp) Type() hm.Type {\n\tt := makeTensorType(4, hm.TypeVariable('a'))\n\treturn hm.NewFnType(t, t)\n}\n\nfunc (op im2colOp) InferShape(shapes ...DimSizer) (retVal tensor.Shape, err error) {\n\tif err = checkArity(op, len(shapes)); err != nil {\n\t\treturn\n\t}\n\n\tif s, ok := shapes[0].(tensor.Shape); ok {\n\t\treturn op.calcShape(s), nil\n\t}\n\treturn nil, errors.Errorf(\"expected tensor.Shape. got %T instead\", shapes[0])\n}\n\nfunc (op im2colOp) Do(inputs ...Value) (retVal Value, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tim := inputs[0]\n\n\t// todo type check values\n\t// todo shape check values\n\n\tretShape := op.calcShape(im.Shape())\n\tprealloc := tensor.New(tensor.Of(im.Dtype()), tensor.WithShape(retShape...))\n\n\treturn op.do(prealloc, im)\n}\n\nfunc (op im2colOp) UsePreallocDo(prealloc Value, inputs ...Value) (Value, error) {\n\tif err := checkArity(op, len(inputs)); err != nil {\n\t\treturn nil, err\n\t}\n\treturn op.do(prealloc, inputs[0])\n}\n\nfunc (op im2colOp) ReturnsPtr() bool     { return false }\nfunc (op im2colOp) CallsExtern() bool    { return false }\nfunc (op im2colOp) OverwritesInput() int { return -1 }\n\nfunc (op im2colOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, \"im2col:%d-%d-%d-%d-%d-%d\", op.h, op.w, op.padH, op.padW, op.strideH, op.strideW)\n}\n\nfunc (op im2colOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op im2colOp) String() string {\n\treturn fmt.Sprintf(\"im2col<(%d,%d), (%d, %d), (%d,%d) (%d, %d)>\", op.h, op.w, op.padH, op.padW, op.strideH, op.strideW, op.dilationH, op.dilationW)\n}\n\nfunc (op im2colOp) DiffWRT(i int) []bool { return []bool{true} }\n\nfunc (op im2colOp) SymDiff(inputs Nodes, output, grad *Node) (retVal Nodes, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\tim := inputs[0]\n\ts := im.Shape()\n\tif s.Dims() != 4 {\n\t\treturn nil, errors.Errorf(\"Expected input to have a shape with 4 dims\")\n\t}\n\tvar unpaddedB, unpaddedC, unpaddedH, unpaddedW int\n\tunpaddedB, unpaddedC, unpaddedH, unpaddedW = s[0], s[1], s[2], s[3]\n\tdiffOp := col2imOp{\n\t\tunpaddedB: unpaddedB,\n\t\tunpaddedC: unpaddedC,\n\t\tunpaddedH: unpaddedH,\n\t\tunpaddedW: unpaddedW,\n\n\t\tim2colOp: op,\n\t}\n\n\tvar ret *Node\n\tif ret, err = ApplyOp(diffOp, grad); err != nil {\n\t\treturn\n\t}\n\tretVal = Nodes{ret}\n\treturn\n}\n\nfunc (op im2colOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) (err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tim := inputs[0]\n\ts := im.Shape()\n\timv, colv := getDV(im, output)\n\n\tvar unpaddedB, unpaddedC, unpaddedH, unpaddedW int\n\tunpaddedB, unpaddedC, unpaddedH, unpaddedW = s[0], s[1], s[2], s[3]\n\tdiffOp := col2imOp{\n\t\tunpaddedB: unpaddedB,\n\t\tunpaddedC: unpaddedC,\n\t\tunpaddedH: unpaddedH,\n\t\tunpaddedW: unpaddedW,\n\n\t\tim2colOp: op,\n\t}\n\n\tif _, err = diffOp.UsePreallocDo(imv.d, colv.d); err != nil {\n\t\treturn errors.Wrapf(err, doFail, diffOp)\n\t}\n\treturn\n}\n\nfunc (op im2colOp) calcShape(s tensor.Shape) (retVal tensor.Shape) {\n\tb := s[0]\n\tc := s[1]\n\th := s[2]\n\tw := s[3]\n\n\tretHeight, retWidth := op.retHW(h, w)\n\tretVal = tensor.Shape(tensor.BorrowInts(4))\n\n\t// todo: double check this with tests\n\tretVal[0] = b\n\tretVal[1] = retHeight\n\tretVal[2] = retWidth\n\tretVal[3] = c * op.w * op.h\n\n\treturn\n}\n\nfunc (op im2colOp) retHW(h, w int) (retHeight, retWidth int) {\n\tretHeight = (h+2*op.padH-(op.dilationH*(op.h-1)+1))/op.strideH + 1\n\tretWidth = (w+2*op.padW-(op.dilationW*(op.w-1)+1))/op.strideW + 1\n\treturn\n}\n\nfunc (op im2colOp) do(prealloc, input Value) (retVal Value, err error) {\n\tinputT := input.(*tensor.Dense)\n\toutputT := prealloc.(*tensor.Dense)\n\n\t// extract bchw - this bit can be expanded in the future, but for now we only support bchw\n\ts := inputT.Shape()\n\tb := s[0]\n\tc := s[1]\n\th := s[2]\n\tw := s[3]\n\n\tinputStrides := inputT.Strides()\n\tretHeight, retWidth := op.retHW(h, w)\n\tbatchStrideIm := inputStrides[0]\n\tbatchStrideCol := outputT.Strides()[0]\n\tchanStride := h * w\n\tinRowStride := inputStrides[2]\n\n\tvar wg sync.WaitGroup\n\tworkers := make(chan struct{}, runtime.NumCPU())\n\tswitch input.Dtype() {\n\tcase tensor.Float64:\n\t\timData := input.Data().([]float64)\n\t\tcolData := prealloc.Data().([]float64)\n\n\t\tfor i := 0; i < b; i++ {\n\t\t\timStart := i * batchStrideIm\n\t\t\tcolStart := i * batchStrideCol\n\t\t\timEnd := imStart + batchStrideIm\n\t\t\tcolEnd := colStart + batchStrideCol\n\n\t\t\tif imEnd >= len(imData) {\n\t\t\t\timEnd = len(imData)\n\t\t\t}\n\t\t\tif colEnd >= len(colData) {\n\t\t\t\tcolEnd = len(colData)\n\t\t\t}\n\t\t\twg.Add(1)\n\n\t\t\tgo op.f64s(c, h, w, chanStride, inRowStride, retHeight, retWidth, imData[imStart:imEnd], colData[colStart:colEnd], &wg, workers)\n\t\t}\n\tcase tensor.Float32:\n\t\timData := input.Data().([]float32)\n\t\tcolData := prealloc.Data().([]float32)\n\t\tfor i := 0; i < b; i++ {\n\t\t\timStart := i * batchStrideIm\n\t\t\tcolStart := i * batchStrideCol\n\t\t\timEnd := imStart + batchStrideIm\n\t\t\tcolEnd := colStart + batchStrideCol\n\n\t\t\tif imEnd >= len(imData) {\n\t\t\t\timEnd = len(imData)\n\t\t\t}\n\t\t\tif colEnd >= len(colData) {\n\t\t\t\tcolEnd = len(colData)\n\t\t\t}\n\t\t\twg.Add(1)\n\n\t\t\tgo op.f32s(c, h, w, chanStride, inRowStride, retHeight, retWidth, imData[imStart:imEnd], colData[colStart:colEnd], &wg, workers)\n\t\t}\n\tdefault:\n\t\treturn nil, errors.Errorf(nyiFail, \"im2col\", input.Dtype())\n\t}\n\twg.Wait()\n\treturn prealloc, nil\n}\n\nfunc (op im2colOp) f64s(chans, height, width, chanStride, inRowStride, retHeight, retWidth int, im, col []float64, wg *sync.WaitGroup, workers chan struct{}) {\n\tworkers <- struct{}{}\n\tvar colIdx, inputRow, inputCol int\n\tfor outputRow := 0; outputRow < retHeight; outputRow++ {\n\t\tfor outputCol := 0; outputCol < retWidth; outputCol++ {\n\t\t\tfor ch := 0; ch < chans; ch++ {\n\t\t\t\tfor kernelRow := 0; kernelRow < op.h; kernelRow++ {\n\t\t\t\t\tinputRow = -op.padH + kernelRow*op.dilationH + outputRow*op.strideH\n\t\t\t\t\tfor kernelCol := 0; kernelCol < op.w; kernelCol++ {\n\t\t\t\t\t\tif inputRow < 0 || inputRow >= height {\n\t\t\t\t\t\t\tcol[colIdx] = 0\n\t\t\t\t\t\t\tcolIdx++\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t\tinputCol = -op.padW + kernelCol*op.dilationW + outputCol*op.strideW\n\t\t\t\t\t\tif inputCol < 0 || inputCol >= width {\n\t\t\t\t\t\t\tcol[colIdx] = 0\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\timIdx := chanStride*ch + inputRow*width + inputCol\n\t\t\t\t\t\t\tcol[colIdx] = im[imIdx]\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcolIdx++\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\t<-workers\n\twg.Done()\n}\n\nfunc (op im2colOp) f32s(chans, height, width, chanStride, inRowStride, retHeight, retWidth int, im, col []float32, wg *sync.WaitGroup, workers chan struct{}) {\n\tworkers <- struct{}{}\n\tvar colIdx, inputRow, inputCol int\n\tfor outputRow := 0; outputRow < retHeight; outputRow++ {\n\t\tfor outputCol := 0; outputCol < retWidth; outputCol++ {\n\t\t\tfor ch := 0; ch < chans; ch++ {\n\t\t\t\tfor kernelRow := 0; kernelRow < op.h; kernelRow++ {\n\t\t\t\t\tinputRow = -op.padH + kernelRow*op.dilationH + outputRow*op.strideH\n\t\t\t\t\tfor kernelCol := 0; kernelCol < op.w; kernelCol++ {\n\t\t\t\t\t\tif inputRow < 0 || inputRow >= height {\n\t\t\t\t\t\t\tcol[colIdx] = 0\n\t\t\t\t\t\t\tcolIdx++\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t\tinputCol = -op.padW + kernelCol*op.dilationW + outputCol*op.strideW\n\t\t\t\t\t\tif inputCol < 0 || inputCol >= width {\n\t\t\t\t\t\t\tcol[colIdx] = 0\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\timIdx := chanStride*ch + inputRow*width + inputCol\n\t\t\t\t\t\t\tcol[colIdx] = im[imIdx]\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcolIdx++\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\t<-workers\n\twg.Done()\n}\n\ntype col2imOp struct {\n\t// input shapes of im2col\n\tunpaddedB int\n\tunpaddedC int\n\tunpaddedH int\n\tunpaddedW int\n\n\tim2colOp\n}\n\nfunc (op col2imOp) Arity() int { return 1 }\n\n// im2col :: (Floats a) ⇒ a →  a\nfunc (op col2imOp) Type() hm.Type {\n\treturn hm.NewFnType(hm.TypeVariable('a'), hm.TypeVariable('a'))\n}\n\nfunc (op col2imOp) InferShape(shapes ...DimSizer) (retVal tensor.Shape, err error) {\n\treturn tensor.Shape{op.unpaddedB, op.unpaddedC, op.unpaddedH, op.unpaddedW}, nil\n}\n\nfunc (op col2imOp) Do(inputs ...Value) (retVal Value, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tim := inputs[0]\n\n\t// todo type check values\n\t// todo shape check values\n\n\tretShape := tensor.Shape{op.unpaddedB, op.unpaddedC, op.unpaddedH, op.unpaddedW}\n\tprealloc := tensor.New(tensor.Of(im.Dtype()), tensor.WithShape(retShape...))\n\n\treturn op.do(prealloc, im)\n}\n\nfunc (op col2imOp) ReturnsPtr() bool     { return false }\nfunc (op col2imOp) CallsExtern() bool    { return false }\nfunc (op col2imOp) OverwritesInput() int { return -1 }\n\nfunc (op col2imOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, \"col2im:%d-%d-%d-%d-%d-%d\", op.h, op.w, op.padH, op.padW, op.strideH, op.strideW)\n}\n\nfunc (op col2imOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op col2imOp) String() string {\n\treturn fmt.Sprintf(\"col2im<(%d,%d), (%d, %d), (%d,%d)>\", op.h, op.w, op.padH, op.padW, op.strideH, op.strideW)\n}\n\nfunc (op col2imOp) UsePreallocDo(prealloc Value, inputs ...Value) (Value, error) {\n\tif err := checkArity(op, len(inputs)); err != nil {\n\t\treturn nil, err\n\t}\n\treturn op.do(prealloc, inputs[0])\n}\n\nfunc (op col2imOp) do(prealloc, input Value) (retVal Value, err error) {\n\tb := op.unpaddedB\n\tc := op.unpaddedC\n\tretHeight := op.unpaddedH\n\tretWidth := op.unpaddedW\n\tbatchStrideIm := c * retHeight * retWidth\n\n\ts := input.Shape()\n\th := s[1]\n\tw := s[2]\n\tchanStride := retHeight * retWidth\n\tbatchStrideCol := h * w * s[3]\n\n\tvar imStart, imEnd, colStart, colEnd int\n\timEnd = imStart + batchStrideIm\n\tcolEnd = colStart + batchStrideCol\n\n\tvar wg sync.WaitGroup\n\tworkers := make(chan struct{}, runtime.NumCPU())\n\tswitch input.Dtype() {\n\tcase tensor.Float64:\n\t\tcolData := input.Data().([]float64)\n\t\timData := prealloc.Data().([]float64)\n\t\tfor i := 0; i < b; i++ {\n\t\t\twg.Add(1)\n\t\t\tgo op.f64s(c, retHeight, retWidth, chanStride, h, w, colData[colStart:colEnd], imData[imStart:imEnd], &wg, workers)\n\n\t\t\tcolStart += batchStrideCol\n\t\t\tcolEnd += batchStrideCol\n\n\t\t\timStart += batchStrideIm\n\t\t\timEnd += batchStrideIm\n\n\t\t\tif imEnd > len(imData) {\n\t\t\t\timEnd = len(imData)\n\t\t\t}\n\t\t\tif colEnd > len(colData) {\n\t\t\t\tcolEnd = len(colData)\n\t\t\t}\n\t\t}\n\tcase tensor.Float32:\n\t\tcolData := input.Data().([]float32)\n\t\timData := prealloc.Data().([]float32)\n\t\tfor i := 0; i < b; i++ {\n\t\t\twg.Add(1)\n\t\t\tgo op.f32s(c, retHeight, retWidth, chanStride, h, w, colData[colStart:colEnd], imData[imStart:imEnd], &wg, workers)\n\n\t\t\tcolStart += batchStrideCol\n\t\t\tcolEnd += batchStrideCol\n\n\t\t\timStart += batchStrideIm\n\t\t\timEnd += batchStrideIm\n\n\t\t\tif imEnd > len(imData) {\n\t\t\t\timEnd = len(imData)\n\t\t\t}\n\t\t\tif colEnd > len(colData) {\n\t\t\t\tcolEnd = len(colData)\n\t\t\t}\n\t\t}\n\tdefault:\n\t\treturn nil, errors.Errorf(nyiFail, \"col2im\", input.Dtype())\n\t}\n\twg.Wait()\n\treturn prealloc, nil\n}\n\nfunc (op col2imOp) f64s(chans, height, width, chanStride, retHeight, retWidth int, col, im []float64, wg *sync.WaitGroup, workers chan struct{}) {\n\tworkers <- struct{}{}\n\t// memset im to 0\n\tfor i := 0; i < len(im); i++ {\n\t\tim[i] = 0\n\t}\n\tcolIdx := 0\n\tvar inputRow int\n\tvar inputCol int\n\tfor outputRow := 0; outputRow < retHeight; outputRow++ {\n\t\tfor outputCol := 0; outputCol < retWidth; outputCol++ {\n\t\t\tfor ch := 0; ch < chans; ch++ {\n\t\t\t\tfor kernelRow := 0; kernelRow < op.h; kernelRow++ {\n\t\t\t\t\tinputRow = -op.padH + kernelRow*op.dilationH + outputRow*op.strideH\n\t\t\t\t\tfor kernelCol := 0; kernelCol < op.w; kernelCol++ {\n\t\t\t\t\t\tif inputRow < 0 || inputRow >= height {\n\t\t\t\t\t\t\tcolIdx++\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t\tinputCol = -op.padW + kernelCol*op.dilationW + outputCol*op.strideW\n\t\t\t\t\t\tif inputCol >= 0 && inputCol < width {\n\t\t\t\t\t\t\timIdx := chanStride*ch + inputRow*width + inputCol\n\t\t\t\t\t\t\tim[imIdx] += col[colIdx]\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcolIdx++\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\t<-workers\n\twg.Done()\n}\n\nfunc (op col2imOp) f32s(chans, height, width, chanStride, retHeight, retWidth int, col, im []float32, wg *sync.WaitGroup, workers chan struct{}) {\n\tworkers <- struct{}{}\n\t// memset im to 0\n\tfor i := 0; i < len(im); i++ {\n\t\tim[i] = 0\n\t}\n\tcolIdx := 0\n\tvar inputRow int\n\tvar inputCol int\n\tfor outputRow := 0; outputRow < retHeight; outputRow++ {\n\t\tfor outputCol := 0; outputCol < retWidth; outputCol++ {\n\t\t\tfor ch := 0; ch < chans; ch++ {\n\t\t\t\tfor kernelRow := 0; kernelRow < op.h; kernelRow++ {\n\t\t\t\t\tinputRow = -op.padH + kernelRow*op.dilationH + outputRow*op.strideH\n\t\t\t\t\tfor kernelCol := 0; kernelCol < op.w; kernelCol++ {\n\t\t\t\t\t\tif inputRow < 0 || inputRow >= height {\n\t\t\t\t\t\t\tcolIdx++\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t\tinputCol = -op.padW + kernelCol*op.dilationW + outputCol*op.strideW\n\t\t\t\t\t\tif inputCol >= 0 && inputCol < width {\n\t\t\t\t\t\t\timIdx := chanStride*ch + inputRow*width + inputCol\n\t\t\t\t\t\t\tim[imIdx] += col[colIdx]\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcolIdx++\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\t<-workers\n\twg.Done()\n}\n\n// It's important to note that this op actually produces TWO values - one argmax, which will be used\n// as a mask, and the actual pooled value.\n//\n// The argmax is stored as an internal state and is not exposed to anything outside the op.\n// There are alternative ways of designing this op, but they all don't particularly seem nice.\n// Caffe's technique seemed the nicest.\ntype maxPoolOp struct {\n\t// Shape of Input\n\tunpaddedB int\n\tunpaddedC int\n\tunpaddedH int\n\tunpaddedW int\n\n\th, w              int // patch height and width\n\tpadNorth, padWest int\n\tpadSouth, padEast int\n\texplicitPadding   bool\n\tstrideH, strideW  int\n\n\t// execution state\n\t// the mask is only filled at execution time\n\tmask tensor.Tensor\n}\n\nfunc newMaxPoolOp(inputShape, kernel tensor.Shape, pad, stride []int) *maxPoolOp {\n\tpadNorth := pad[0]\n\tpadWest := pad[1]\n\tpadSouth := pad[0]\n\tpadEast := pad[1]\n\texplicitPadding := false\n\tif len(pad) == 4 {\n\t\texplicitPadding = true\n\t\tpadNorth = pad[0]\n\t\tpadSouth = pad[1]\n\t\tpadWest = pad[2]\n\t\tpadEast = pad[3]\n\t}\n\tmaxpoolOp := &maxPoolOp{\n\t\t// Shape of Input\n\t\tunpaddedB: inputShape[0],\n\t\tunpaddedC: inputShape[1],\n\t\tunpaddedH: inputShape[2],\n\t\tunpaddedW: inputShape[3],\n\n\t\th:               kernel[0],\n\t\tw:               kernel[1],\n\t\tpadNorth:        padNorth,\n\t\tpadWest:         padWest,\n\t\tpadSouth:        padSouth,\n\t\tpadEast:         padEast,\n\t\texplicitPadding: explicitPadding,\n\t\tstrideH:         stride[0],\n\t\tstrideW:         stride[1],\n\t}\n\tmaxpoolOp.mask = tensor.New(tensor.Of(tensor.Int), tensor.WithShape(maxpoolOp.calcShape(inputShape)...))\n\treturn maxpoolOp\n}\n\nfunc (op *maxPoolOp) Arity() int { return 1 }\n\n// maxPoolOp has this type:\n// \t\top :: (...) → (...)\nfunc (op *maxPoolOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\tt := newTensorType(4, a)\n\treturn hm.NewFnType(t, t)\n}\nfunc (op *maxPoolOp) InferShape(inputs ...DimSizer) (tensor.Shape, error) {\n\tif s, ok := inputs[0].(tensor.Shape); ok {\n\t\treturn op.calcShape(s), nil\n\t}\n\treturn nil, errors.Errorf(\"Expected a shape\")\n}\n\nfunc (op *maxPoolOp) Do(inputs ...Value) (retVal Value, err error) {\n\tvar in, out tensor.Tensor\n\tif in, err = op.checkInput(inputs...); err != nil {\n\t\treturn nil, err\n\t}\n\tinShp := in.Shape()\n\tout = tensor.New(tensor.Of(in.Dtype()), tensor.WithShape(op.calcShape(inShp)...), tensor.WithEngine(in.Engine()))\n\n\top.do(out, in)\n\treturn out, nil\n}\n\nfunc (op *maxPoolOp) ReturnsPtr() bool     { return false }\nfunc (op *maxPoolOp) CallsExtern() bool    { return false }\nfunc (op *maxPoolOp) OverwritesInput() int { return -1 }\nfunc (op *maxPoolOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, \"MaxPool{%d, %d, %d, %d}(kernel: (%d, %d), pad: (%d, %d), stride: (%d, %d))\",\n\t\top.unpaddedB, op.unpaddedC, op.unpaddedH, op.unpaddedW,\n\t\top.h, op.w, op.padNorth, op.padWest, op.strideH, op.strideW)\n}\n\nfunc (op *maxPoolOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op *maxPoolOp) String() string {\n\treturn fmt.Sprintf(\"MaxPool{%d, %d, %d, %d}(kernel: (%d, %d), pad: (%d, %d), stride: (%d, %d))\",\n\t\top.unpaddedB, op.unpaddedC, op.unpaddedH, op.unpaddedW,\n\t\top.h, op.w, op.padNorth, op.padWest, op.strideH, op.strideW)\n}\n\nfunc (op *maxPoolOp) UsePreallocDo(prealloc Value, inputs ...Value) (Value, error) {\n\tvar in tensor.Tensor\n\tvar err error\n\tif in, err = op.checkInput(inputs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif p, ok := prealloc.(tensor.Tensor); ok {\n\t\top.do(p, in)\n\t\treturn p, nil\n\t}\n\treturn nil, errors.Errorf(\"Expected prealloc to be a tensor\")\n}\n\nfunc (op *maxPoolOp) DiffWRT(inputs int) []bool { return []bool{true} }\n\nfunc (op *maxPoolOp) SymDiff(inputs Nodes, output, grad *Node) (retVal Nodes, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\tinput := inputs[0]\n\n\tvar op2 maxPoolOp\n\top2 = *op\n\tdiff := &maxPoolDiffOp{op2}\n\n\tvar ret *Node\n\tif ret, err = ApplyOp(diff, input, output, grad); err != nil {\n\t\treturn nil, err\n\t}\n\treturn Nodes{ret}, nil\n}\n\nfunc (op *maxPoolOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) (err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\tinput := inputs[0]\n\tinputDV, outDV := getDV(input, output)\n\n\tvar op2 maxPoolOp\n\top2 = *op\n\tdiff := &maxPoolDiffOp{op2}\n\n\tif _, err = diff.UsePreallocDo(inputDV.d, inputDV.Value, outDV.Value, outDV.d); err != nil {\n\t\treturn errors.Wrapf(err, doFail, diff)\n\t}\n\treturn\n}\n\nfunc (op *maxPoolOp) checkInput(inputs ...Value) (tensor.Tensor, error) {\n\tif err := checkArity(op, len(inputs)); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar in tensor.Tensor\n\tvar ok bool\n\tif in, ok = inputs[0].(tensor.Tensor); !ok {\n\t\treturn nil, errors.Errorf(\"Expected input to be a tensor\")\n\t}\n\n\tif in.Shape().Dims() != 4 {\n\t\treturn nil, errors.Errorf(\"Expected input to have 4 dimensions\")\n\t}\n\treturn in, nil\n}\n\n// calcShape calculates the output shape given an input shape\nfunc (op *maxPoolOp) calcShape(s tensor.Shape) tensor.Shape {\n\tb, c, h, w := s[0], s[1], s[2], s[3]\n\n\tpooledH := (h+op.padSouth+op.padNorth-(op.h-1)-1)/op.strideH + 1\n\tpooledW := (w+op.padEast+op.padWest-(op.w-1)-1)/op.strideW + 1\n\treturn tensor.Shape{b, c, pooledH, pooledW}\n}\n\nfunc (op *maxPoolOp) strideValue(strides []int) int {\n\tif len(strides) < 2 {\n\t\treturn 0\n\t}\n\n\treturn strides[1]\n}\n\n// do prepares the data, and then dispatches it to the correct (computation) kernel.\n// out is the preallocated tensor\nfunc (op *maxPoolOp) do(out, in tensor.Tensor) {\n\toutShape := out.Shape()\n\toutStride := op.strideValue(out.Strides())\n\n\tinShape := in.Shape()\n\tinStride := op.strideValue(in.Strides())\n\n\tmaskStride := op.strideValue(op.mask.Strides())\n\n\tb, c, h, w := outShape[0], outShape[1], outShape[2], outShape[3]\n\tinH, inW := inShape[2], inShape[3]\n\n\tif op.mask == nil {\n\t\top.mask = tensor.New(tensor.Of(tensor.Int), tensor.WithShape(op.calcShape(inShape)...))\n\t}\n\n\tmaskData := op.mask.Data().([]int)\n\n\tswitch in.Dtype() {\n\tcase tensor.Float64:\n\t\top.f64s(b, c, h, w, inH, inW,\n\t\t\toutStride, inStride, maskStride,\n\t\t\tout.Data().([]float64), in.Data().([]float64),\n\t\t\tmaskData)\n\tcase tensor.Float32:\n\t\top.f32s(b, c, h, w, inH, inW,\n\t\t\toutStride, inStride, maskStride,\n\t\t\tout.Data().([]float32), in.Data().([]float32),\n\t\t\tmaskData)\n\t}\n}\n\nfunc (op *maxPoolOp) f32s(batches, channels, outH, outW, inH, inW,\n\toutStride, inStride, maskStride int,\n\toutData, inData []float32,\n\tmaskData []int) {\n\n\t// set values\n\tfor i := range outData {\n\t\toutData[i] = -maxFloat32\n\t\tmaskData[i] = -1\n\t}\n\tpadH := op.padNorth\n\tpadW := op.padWest\n\tif op.explicitPadding {\n\t\tpadH = op.padSouth\n\t\tpadW = op.padEast\n\t}\n\n\tfor b := 0; b < batches; b++ {\n\t\tfor c := 0; c < channels; c++ {\n\t\t\tfor ph := 0; ph < outH; ph++ {\n\t\t\t\tfor pw := 0; pw < outW; pw++ {\n\n\t\t\t\t\thStart := ph*op.strideH - padH\n\t\t\t\t\twStart := pw*op.strideW - padW\n\t\t\t\t\thEnd := minInt(hStart+op.h, inH)\n\t\t\t\t\twEnd := minInt(wStart+op.w, inW)\n\t\t\t\t\thStart = maxInt(hStart, 0)\n\t\t\t\t\twStart = maxInt(wStart, 0)\n\n\t\t\t\t\tpoolIndex := ph*outW + pw\n\t\t\t\t\tfor hi := hStart; hi < hEnd; hi++ {\n\t\t\t\t\t\tfor wi := wStart; wi < wEnd; wi++ {\n\t\t\t\t\t\t\ti := hi*inW + wi\n\t\t\t\t\t\t\tif inData[i] > outData[poolIndex] {\n\t\t\t\t\t\t\t\toutData[poolIndex] = inData[i]\n\t\t\t\t\t\t\t\tmaskData[poolIndex] = i\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t// skip by strides\n\t\t\tinData = inData[inStride:]\n\t\t\toutData = outData[outStride:]\n\t\t\tmaskData = maskData[maskStride:]\n\t\t}\n\t}\n}\n\nfunc (op *maxPoolOp) f64s(batches, channels, outH, outW, inH, inW,\n\toutStride, inStride, maskStride int,\n\toutData, inData []float64,\n\tmaskData []int) {\n\n\t// set values\n\tfor i := range outData {\n\t\toutData[i] = -maxFloat64\n\t\tmaskData[i] = -1\n\t}\n\tpadH := op.padNorth\n\tpadW := op.padWest\n\tif op.explicitPadding {\n\t\tpadH = op.padSouth\n\t\tpadW = op.padEast\n\t}\n\n\tfor b := 0; b < batches; b++ {\n\t\tfor c := 0; c < channels; c++ {\n\t\t\tfor ph := 0; ph < outH; ph++ {\n\t\t\t\tfor pw := 0; pw < outW; pw++ {\n\t\t\t\t\thStart := ph*op.strideH - padH\n\t\t\t\t\twStart := pw*op.strideW - padW\n\t\t\t\t\thEnd := minInt(hStart+op.h, inH)\n\t\t\t\t\twEnd := minInt(wStart+op.w, inW)\n\t\t\t\t\thStart = maxInt(hStart, 0)\n\t\t\t\t\twStart = maxInt(wStart, 0)\n\n\t\t\t\t\tpoolIndex := ph*outW + pw\n\n\t\t\t\t\tfor hi := hStart; hi < hEnd; hi++ {\n\t\t\t\t\t\tfor wi := wStart; wi < wEnd; wi++ {\n\t\t\t\t\t\t\ti := hi*inW + wi\n\t\t\t\t\t\t\tif inData[i] > outData[poolIndex] {\n\t\t\t\t\t\t\t\toutData[poolIndex] = inData[i]\n\t\t\t\t\t\t\t\tmaskData[poolIndex] = i\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t// skip by strides\n\t\t\tinData = inData[inStride:]\n\t\t\toutData = outData[outStride:]\n\t\t\tmaskData = maskData[maskStride:]\n\t\t}\n\t}\n}\n\ntype maxPoolDiffOp struct {\n\tmaxPoolOp\n}\n\nfunc (op *maxPoolDiffOp) Arity() int { return 3 }\nfunc (op *maxPoolDiffOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\tt := newTensorType(4, a)\n\treturn hm.NewFnType(t, t, t, t)\n}\n\nfunc (op *maxPoolDiffOp) InferShape(inputs ...DimSizer) (tensor.Shape, error) {\n\ts := inputs[0].(tensor.Shape).Clone()\n\treturn s, nil\n}\n\nfunc (op *maxPoolDiffOp) Do(inputs ...Value) (Value, error) {\n\tvar in, out, pooled, pooledGrad tensor.Tensor\n\tvar err error\n\tif in, pooled, pooledGrad, err = op.checkInput(inputs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// out is the gradient of in\n\tout = tensor.New(tensor.Of(in.Dtype()), tensor.WithShape(in.Shape().Clone()...), tensor.WithEngine(in.Engine()))\n\top.do(out, in, pooled, pooledGrad)\n\treturn out, nil\n}\nfunc (op *maxPoolDiffOp) ReturnsPtr() bool     { return true }\nfunc (op *maxPoolDiffOp) CallsExtern() bool    { return false }\nfunc (op *maxPoolDiffOp) OverwritesInput() int { return -1 }\nfunc (op *maxPoolDiffOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, \"MaxPoolDiff{%d, %d, %d, %d}(kernel: (%d, %d), pad: (%d, %d), stride: (%d, %d))\",\n\t\top.unpaddedB, op.unpaddedC, op.unpaddedH, op.unpaddedW,\n\t\top.h, op.w, op.padNorth, op.padWest, op.strideH, op.strideW)\n}\n\nfunc (op *maxPoolDiffOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op *maxPoolDiffOp) String() string {\n\treturn fmt.Sprintf(\"MaxPoolDiff{%d, %d, %d, %d}(kernel: (%d, %d), pad: (%d, %d), stride: (%d, %d))\",\n\t\top.unpaddedB, op.unpaddedC, op.unpaddedH, op.unpaddedW,\n\t\top.h, op.w, op.padNorth, op.padWest, op.strideH, op.strideW)\n}\n\nfunc (op *maxPoolDiffOp) UsePreallocDo(prealloc Value, inputs ...Value) (Value, error) {\n\tvar in, pooled, pooledGrad tensor.Tensor\n\tvar err error\n\tif in, pooled, pooledGrad, err = op.checkInput(inputs...); err != nil {\n\t\treturn nil, err\n\t}\n\tif p, ok := prealloc.(tensor.Tensor); ok {\n\t\top.do(p, in, pooled, pooledGrad)\n\t\treturn prealloc, nil\n\t}\n\treturn nil, errors.Errorf(\"Cannot do with PreallocDo - expected PreAlloc to be tensor\")\n}\n\nfunc (op *maxPoolDiffOp) checkInput(inputs ...Value) (in, pooled, pooledGrad tensor.Tensor, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tvar ok bool\n\tif in, ok = inputs[0].(tensor.Tensor); !ok {\n\t\terr = errors.Errorf(\"Expected input to be a tensor\")\n\t\treturn\n\t}\n\tif in.Shape().Dims() != 4 {\n\t\terr = errors.Errorf(\"Expected input to have 4 dimensions\")\n\t\treturn\n\t}\n\n\tif pooled, ok = inputs[1].(tensor.Tensor); !ok {\n\t\terr = errors.Errorf(\"Expected pooled to be a tensor\")\n\t\treturn\n\t}\n\tif pooledGrad, ok = inputs[2].(tensor.Tensor); !ok {\n\t\terr = errors.Errorf(\"Expected pooledGrad to be a tensor\")\n\t\treturn\n\t}\n\treturn\n}\n\nfunc (op *maxPoolDiffOp) do(inGrad, in, pooled, pooledGrad tensor.Tensor) {\n\tpooledShape := pooled.Shape()\n\tpooledStride := op.strideValue(pooled.Strides())\n\tinStride := in.Strides()[1]\n\tmaskStride := op.strideValue(op.mask.Strides())\n\tmaskData := op.mask.Data().([]int)\n\n\tb, c, h, w := pooledShape[0], pooledShape[1], pooledShape[2], pooledShape[3]\n\tswitch in.Dtype() {\n\tcase tensor.Float32:\n\t\tinGradData := inGrad.Data().([]float32)\n\t\tpooledGradData := pooledGrad.Data().([]float32)\n\t\top.f32s(b, c, h, w,\n\t\t\tinStride, pooledStride, maskStride,\n\t\t\tinGradData, pooledGradData, maskData)\n\tcase tensor.Float64:\n\t\tinGradData := inGrad.Data().([]float64)\n\t\tpooledGradData := pooledGrad.Data().([]float64)\n\t\top.f64s(b, c, h, w,\n\t\t\tinStride, pooledStride, maskStride,\n\t\t\tinGradData, pooledGradData, maskData)\n\t}\n}\n\n// in is the \"bottom\", while out is the \"top\" (bottom being the unpooled, and top being the pooled)\nfunc (op *maxPoolDiffOp) f32s(batches, channels, pooledH, pooledW int,\n\tinStride, outStride, maskStride int,\n\tinDiffData, outDiffData []float32,\n\tmaskData []int) {\n\n\t// zero out. let's hope go's optimizer is smart enought\n\tfor i := range inDiffData {\n\t\tinDiffData[i] = 0\n\t}\n\n\t// this loop can be goroutine'd\n\tfor b := 0; b < batches; b++ {\n\t\tfor c := 0; c < channels; c++ {\n\t\t\tfor ph := 0; ph < pooledH; ph++ {\n\t\t\t\tfor pw := 0; pw < pooledW; pw++ {\n\t\t\t\t\tindex := ph*pooledW + pw\n\t\t\t\t\tinIndex := maskData[index]\n\t\t\t\t\tinDiffData[inIndex] += outDiffData[index]\n\t\t\t\t}\n\t\t\t}\n\t\t\toutDiffData = outDiffData[outStride:]\n\t\t\tinDiffData = inDiffData[inStride:]\n\t\t\tmaskData = maskData[maskStride:]\n\t\t}\n\t}\n}\n\n// in is the \"bottom\", while out is the \"top\" (bottom being the unpooled, and top being the pooled)\nfunc (op *maxPoolDiffOp) f64s(batches, channels, pooledH, pooledW int,\n\tinStride, outStride, maskStride int,\n\tinDiffData, outDiffData []float64,\n\tmaskData []int) {\n\n\t// zero out. let's hope go's optimizer is smart enought\n\tfor i := range inDiffData {\n\t\tinDiffData[i] = 0\n\t}\n\n\t// this loop can be goroutine'd\n\tfor b := 0; b < batches; b++ {\n\t\tfor c := 0; c < channels; c++ {\n\t\t\tfor ph := 0; ph < pooledH; ph++ {\n\t\t\t\tfor pw := 0; pw < pooledW; pw++ {\n\t\t\t\t\tindex := ph*pooledW + pw\n\t\t\t\t\tinIndex := maskData[index]\n\t\t\t\t\tinDiffData[inIndex] += outDiffData[index]\n\t\t\t\t}\n\t\t\t}\n\t\t\toutDiffData = outDiffData[outStride:]\n\t\t\tinDiffData = inDiffData[inStride:]\n\t\t\tmaskData = maskData[maskStride:]\n\t\t}\n\t}\n}\n\n// clampOp is a constant clamping operation\ntype clampOp struct {\n\tmin, max Scalar\n}\n\nfunc (op *clampOp) Arity() int { return 1 }\n\nfunc (op *clampOp) Type() hm.Type {\n\treturn hm.NewFnType(hm.TypeVariable('a'), hm.TypeVariable('a'))\n}\n\nfunc (op *clampOp) InferShape(shps ...DimSizer) (tensor.Shape, error) {\n\treturn shps[0].(tensor.Shape), nil\n}\n\nfunc (op *clampOp) Do(vals ...Value) (Value, error) {\n\treturn nil, nil\n}\n\nfunc (op *clampOp) ReturnsPtr() bool { return true }\n\nfunc (op *clampOp) CallsExtern() bool { return false }\n\nfunc (op *clampOp) OverwritesInput() int { return 0 }\n\nfunc (op *clampOp) WriteHash(h hash.Hash) { fmt.Fprintf(h, \"ConstClamp{%f, %f}()\", op.min, op.max) }\n\nfunc (op *clampOp) Hashcode() uint32 { return simpleHash(op) }\nfunc (op *clampOp) String() string   { return fmt.Sprintf(\"ConstClamp{%f, %f}()\", op.min, op.max) }\n\n// BatchNormOp is a batch normalization process as described by Ioffe and Szegedy (2015) -\n// http://arxiv.org/abs/1502.03167\n//\n// Normalization is done as:\n// \tγ(x - μ) / σ + β\n// γ is the scaling factor and β is the offset factor. These are created by BatchNorm()\ntype BatchNormOp struct {\n\tmomentum float64 // momentum for the moving average\n\tepsilon  float64 // small variance to be added to avoid dividing by 0\n\tdims     int     // 2 or 4. defaults to 4\n\n\t// learnables\n\trunningMean, runningVariance *tensor.Dense\n\tsaveMean, saveVariance       *tensor.Dense\n\n\t// internal use\n\talpha, beta *tensor.Dense // shape: (channels, )\n\n\t// training? if training then update movingMean and movingVar\n\ttraining bool\n}\n\n// Arity\nfunc (op *BatchNormOp) Arity() int { return 1 }\n\n// Type ...\nfunc (op *BatchNormOp) Type() hm.Type {\n\tdims := op.dims\n\tif dims == 0 {\n\t\tdims = 4 // default to 4 if not set\n\t}\n\n\tt := TensorType{Dims: dims, Of: hm.TypeVariable('a')}\n\treturn hm.NewFnType(t, t)\n}\n\n// InferShape from the input values\nfunc (op *BatchNormOp) InferShape(ns ...DimSizer) (tensor.Shape, error) {\n\tif err := checkArity(op, len(ns)); err != nil {\n\t\treturn nil, errors.Wrapf(err, \"batchNorm\")\n\t}\n\n\treturn ns[0].(tensor.Shape).Clone(), nil\n}\n\n// Do performs the batchnorm computation on the values\nfunc (op *BatchNormOp) Do(values ...Value) (retVal Value, err error) {\n\tif err := checkArity(op, len(values)); err != nil {\n\t\treturn nil, errors.Wrapf(err, \"batchNorm Do\")\n\t}\n\n\tvar v, out Value\n\tv = values[0]\n\tif out, err = CloneValue(v); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn op.UsePreallocDo(out, v)\n}\n\n// ReturnsPtr is true\nfunc (op *BatchNormOp) ReturnsPtr() bool { return true }\n\n// CallsExtern is false\nfunc (op *BatchNormOp) CallsExtern() bool { return false }\n\n// OverwritesInput is -1 (operator doesn't overwrite any input value)\nfunc (op *BatchNormOp) OverwritesInput() int { return -1 }\n\n// WriteHash ...\nfunc (op *BatchNormOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, \"batchnorm-%1.1f-%1.1f\", op.momentum, op.epsilon)\n}\n\n// Hashcode ...\nfunc (op *BatchNormOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op *BatchNormOp) String() string {\n\treturn fmt.Sprintf(\"batchnorm-%1.1f-%1.1f\", op.momentum, op.epsilon)\n}\n\n// DoDiff does the gradient computation\nfunc (op *BatchNormOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) error {\n\tdiff := &batchnormDiffOp{op}\n\txdv, ydv := getDV(inputs[0], output)\n\tsdv, bdv := getDV(inputs[1], inputs[2])\n\t_, err := diff.UsePreallocDo(xdv.d, xdv.Value, ydv.d, sdv.Value, bdv.Value)\n\n\treturn err\n}\n\n// DiffWRT ...\nfunc (op *BatchNormOp) DiffWRT(inputs int) []bool { return []bool{true} }\n\n// SymDiff ...\nfunc (op *BatchNormOp) SymDiff(inputs Nodes, output *Node, grad *Node) (retVal Nodes, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tdiff := &batchnormDiffOp{op}\n\tinput := inputs[0]\n\n\tvar dy *Node\n\tif dy, err = ApplyOp(diff, input, grad); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn Nodes{dy, nil, nil}, nil\n}\n\n// UsePreallocDo ...\nfunc (op *BatchNormOp) UsePreallocDo(prealloc Value, inputs ...Value) (retVal Value, err error) {\n\tv := inputs[0]\n\n\tswitch v.Dtype() {\n\tcase Float64:\n\t\terr = op.f64s(v.(*tensor.Dense), prealloc.(*tensor.Dense))\n\tcase Float32:\n\t\terr = op.f32s(v.(*tensor.Dense), prealloc.(*tensor.Dense))\n\tdefault:\n\t\treturn nil, nyi(\"BatchNorm Do\", v.Dtype())\n\t}\n\n\treturn prealloc, err\n\n}\n\n// Stats returns the running mean and running variance\nfunc (op *BatchNormOp) Stats() (runningMean tensor.Tensor, runningVariance tensor.Tensor) {\n\treturn op.runningMean, op.runningVariance\n}\n\n// SetStats sets the running mean and running variance. The given values are copied\nfunc (op *BatchNormOp) SetStats(runningMean tensor.Tensor, runningVariance tensor.Tensor) error {\n\tif !runningMean.Shape().Eq(op.runningMean.Shape()) {\n\t\treturn fmt.Errorf(\"invalid runningMean shape %v. Expected: %v\", runningMean.Shape(), op.runningMean.Shape())\n\t}\n\n\tif runningMean.Dtype() != op.runningMean.Dtype() {\n\t\treturn fmt.Errorf(\"invalid runningMean type %v. Expected: %v\", runningMean.Dtype(), op.runningMean.Dtype())\n\t}\n\n\tif !runningVariance.Shape().Eq(op.runningVariance.Shape()) {\n\t\treturn fmt.Errorf(\"invalid runningVariance shape %v. Expected: %v\", runningMean.Shape(), op.runningMean.Shape())\n\t}\n\n\tif runningVariance.Dtype() != op.runningVariance.Dtype() {\n\t\treturn fmt.Errorf(\"invalid runningVariance type %v. Expected: %v\", runningMean.Dtype(), op.runningMean.Dtype())\n\t}\n\n\tswitch op.runningMean.Dtype() {\n\tcase Float32:\n\t\tcopy(op.runningMean.Data().([]float32), runningMean.Data().([]float32))\n\t\tcopy(op.runningVariance.Data().([]float32), runningVariance.Data().([]float32))\n\tcase Float64:\n\t\tcopy(op.runningMean.Data().([]float64), runningMean.Data().([]float64))\n\t\tcopy(op.runningVariance.Data().([]float64), runningVariance.Data().([]float64))\n\t}\n\n\treturn nil\n}\n\n// SetTraining configure the op for training mode.\n// A call to this function with `true` implicitly calls the Reset() method\nfunc (op *BatchNormOp) SetTraining(isTraining bool) error {\n\tif isTraining {\n\t\top.Reset()\n\t}\n\n\top.training = isTraining\n\n\treturn nil\n}\n\n// Reset the operator by zeroing the internals scratch spaces\nfunc (op *BatchNormOp) Reset() error {\n\tdt := op.runningMean.Dtype()\n\tvar uno interface{}\n\tswitch dt {\n\tcase Float64:\n\t\tuno = float64(1)\n\tcase Float32:\n\t\tuno = float32(1)\n\t}\n\n\tif err := op.runningVariance.Memset(uno); err != nil {\n\t\treturn err\n\t}\n\n\top.runningMean.Zero()\n\treturn nil\n}\n\nfunc (op *BatchNormOp) updateStatsF64(batchSize, channels, spatialDim int, inputT *tensor.Dense) (saveMean []float64, saveVar []float64) {\n\tmomentum := float64(op.momentum)\n\n\tinputA := inputT.Float64s()\n\n\top.saveMean.Zero()\n\top.saveVariance.Zero()\n\n\tsaveMean = op.saveMean.Float64s()\n\tsaveVar = op.saveVariance.Float64s()\n\n\trunningMean := op.runningMean.Float64s()\n\trunningVar := op.runningVariance.Float64s()\n\tn := spatialDim * batchSize\n\n\tif spatialDim == 1 { // image size = 1\n\t\trunInParallel(0, channels, func(c int) {\n\t\t\tfor s := 0; s < batchSize; s++ {\n\t\t\t\ti := s*channels + c\n\n\t\t\t\tsaveMean[c] += inputA[i]\n\t\t\t}\n\n\t\t\tsaveMean[c] /= float64(n)\n\n\t\t\tfor s := 0; s < batchSize; s++ {\n\t\t\t\ti := s*channels + c\n\n\t\t\t\tsaveVar[c] += (inputA[i] - saveMean[c]) * (inputA[i] - saveMean[c])\n\t\t\t}\n\n\t\t\trunningMean[c] = (momentum*saveMean[c] + (1-momentum)*runningMean[c])\n\n\t\t\tunbiasedVar := saveVar[c] / float64(n-1)\n\t\t\trunningVar[c] = (momentum*unbiasedVar + (1-momentum)*runningVar[c])\n\t\t})\n\t} else { // image size > 1\n\t\trunInParallel(0, channels, func(c int) {\n\t\t\tfor s := 0; s < batchSize; s++ {\n\t\t\t\tfor d := 0; d < spatialDim; d++ {\n\t\t\t\t\ti := s*channels*spatialDim + c*spatialDim + d\n\n\t\t\t\t\tsaveMean[c] += inputA[i]\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tsaveMean[c] /= float64(n)\n\n\t\t\tfor s := 0; s < batchSize; s++ {\n\t\t\t\tfor d := 0; d < spatialDim; d++ {\n\t\t\t\t\ti := s*channels*spatialDim + c*spatialDim + d\n\n\t\t\t\t\tx := inputA[i]\n\n\t\t\t\t\tsaveVar[c] += (x - saveMean[c]) * (x - saveMean[c])\n\t\t\t\t}\n\t\t\t}\n\n\t\t\trunningMean[c] = (momentum*saveMean[c] + (1-momentum)*runningMean[c])\n\n\t\t\tunbiasedVar := saveVar[c] / float64(n-1)\n\t\t\trunningVar[c] = (momentum*unbiasedVar + (1-momentum)*runningVar[c])\n\t\t})\n\t}\n\n\treturn saveMean, saveVar\n}\n\n// alpha = scale / sqrt(variance+eps)\n// beta = bias - mean * alpha\nfunc (op *BatchNormOp) calculateAlphaAndBetaF64(batchSize, channels, spatialDim int, saveMean, saveVar []float64) (alpha []float64, beta []float64) {\n\trunningMean := op.runningMean.Float64s()\n\trunningVar := op.runningVariance.Float64s()\n\tn := spatialDim * batchSize\n\n\talpha = op.alpha.Float64s()\n\tbeta = op.beta.Float64s()\n\n\trunInParallel(0, channels, func(c int) {\n\t\tvar invStd, mean float64\n\n\t\tif op.training {\n\t\t\tmean = saveMean[c]\n\t\t\tinvStd = 1 / math.Sqrt(saveVar[c]/float64(n)+float64(op.epsilon))\n\t\t} else {\n\t\t\tmean = runningMean[c]\n\t\t\tinvStd = 1 / math.Sqrt(runningVar[c]+float64(op.epsilon))\n\t\t}\n\n\t\talpha[c] = invStd\n\t\tbeta[c] = -(mean * alpha[c])\n\t})\n\n\treturn alpha, beta\n}\n\nfunc (op *BatchNormOp) f64s(input, output *tensor.Dense) (err error) {\n\tbatchSize := input.Shape()[0]\n\tchannels := input.Shape()[1]\n\tnc := channels * batchSize\n\tspatialDim := input.Shape().TotalSize() / nc\n\n\tvar alpha, beta []float64\n\n\tif op.training {\n\t\tsaveMean, saveVar := op.updateStatsF64(batchSize, channels, spatialDim, input)\n\t\talpha, beta = op.calculateAlphaAndBetaF64(batchSize, channels, spatialDim, saveMean, saveVar)\n\t} else {\n\n\t\top.saveMean.Zero()\n\t\top.saveVariance.Zero()\n\t\tsaveMean := op.saveMean.Float64s()\n\t\tsaveVar := op.saveVariance.Float64s()\n\n\t\talpha, beta = op.calculateAlphaAndBetaF64(batchSize, channels, spatialDim, saveMean, saveVar)\n\t}\n\n\t// output = input * alpha + beta\n\toutputF64s := output.Float64s()\n\n\tif spatialDim == 1 {\n\t\trunInParallel(0, batchSize, func(s int) {\n\t\t\tfor c := 0; c < channels; c++ {\n\t\t\t\ti := s*channels + c\n\n\t\t\t\toutputF64s[i] = outputF64s[i]*alpha[c] + beta[c]\n\t\t\t}\n\t\t})\n\t} else {\n\t\trunInParallel(0, channels, func(c int) {\n\t\t\tfor s := 0; s < batchSize; s++ {\n\t\t\t\tfor d := 0; d < spatialDim; d++ {\n\t\t\t\t\ti := s*channels*spatialDim + c*spatialDim + d\n\n\t\t\t\t\toutputF64s[i] = outputF64s[i]*alpha[c] + beta[c]\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n\n\treturn nil\n}\n\nfunc (op *BatchNormOp) updateStatsF32(batchSize, channels, spatialDim int, inputT *tensor.Dense) (saveMean []float32, saveVar []float32) {\n\tmomentum := float32(op.momentum)\n\n\tinputA := inputT.Float32s()\n\n\top.saveMean.Zero()\n\top.saveVariance.Zero()\n\n\tsaveMean = op.saveMean.Float32s()\n\tsaveVar = op.saveVariance.Float32s()\n\n\trunningMean := op.runningMean.Float32s()\n\trunningVar := op.runningVariance.Float32s()\n\tn := spatialDim * batchSize\n\n\tif spatialDim == 1 { // image size = 1\n\t\trunInParallel(0, channels, func(c int) {\n\t\t\tfor s := 0; s < batchSize; s++ {\n\t\t\t\ti := s*channels + c\n\n\t\t\t\tsaveMean[c] += inputA[i]\n\t\t\t}\n\n\t\t\tsaveMean[c] /= float32(n)\n\n\t\t\tfor s := 0; s < batchSize; s++ {\n\t\t\t\ti := s*channels + c\n\n\t\t\t\tsaveVar[c] += (inputA[i] - saveMean[c]) * (inputA[i] - saveMean[c])\n\t\t\t}\n\n\t\t\trunningMean[c] = (momentum * saveMean[c]) + (1-momentum)*runningMean[c]\n\n\t\t\tunbiasedVar := (saveVar[c]) / float32(n-1)\n\t\t\trunningVar[c] = (momentum*unbiasedVar + (1-momentum)*(runningVar[c]))\n\t\t})\n\t} else { // image size > 1\n\t\trunInParallel(0, channels, func(c int) {\n\t\t\tfor s := 0; s < batchSize; s++ {\n\t\t\t\tfor d := 0; d < spatialDim; d++ {\n\t\t\t\t\ti := s*channels*spatialDim + c*spatialDim + d\n\n\t\t\t\t\tsaveMean[c] += inputA[i]\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tsaveMean[c] /= float32(n)\n\n\t\t\tfor s := 0; s < batchSize; s++ {\n\t\t\t\tfor d := 0; d < spatialDim; d++ {\n\t\t\t\t\ti := s*channels*spatialDim + c*spatialDim + d\n\n\t\t\t\t\tx := inputA[i]\n\n\t\t\t\t\tsaveVar[c] += (x - saveMean[c]) * (x - saveMean[c])\n\t\t\t\t}\n\t\t\t}\n\n\t\t\trunningMean[c] = (momentum*saveMean[c] + (1-momentum)*runningMean[c])\n\n\t\t\tunbiasedVar := saveVar[c] / float32(n-1)\n\t\t\trunningVar[c] = momentum*unbiasedVar + (1-momentum)*float32(runningVar[c])\n\t\t})\n\t}\n\n\treturn saveMean, saveVar\n}\n\n// alpha = scale / sqrt(variance+eps)\n// beta = bias - mean * alpha\nfunc (op *BatchNormOp) calculateAlphaAndBetaF32(batchSize, channels, spatialDim int, saveMean, saveVar []float32) (alpha []float32, beta []float32) {\n\trunningMean := op.runningMean.Float32s()\n\trunningVar := op.runningVariance.Float32s()\n\n\tn := spatialDim * batchSize\n\n\talpha = op.alpha.Float32s()\n\tbeta = op.beta.Float32s()\n\n\trunInParallel(0, channels, func(c int) {\n\t\tvar invStd, mean float32\n\n\t\tif op.training {\n\t\t\tmean = saveMean[c]\n\t\t\tinvStd = 1 / math32.Sqrt(saveVar[c]/float32(n)+float32(op.epsilon))\n\t\t} else {\n\t\t\tmean = runningMean[c]\n\t\t\tinvStd = 1 / math32.Sqrt(runningVar[c]+float32(op.epsilon))\n\t\t}\n\n\t\talpha[c] = invStd\n\t\tbeta[c] = -mean * alpha[c]\n\t})\n\n\treturn alpha, beta\n}\n\nfunc (op *BatchNormOp) f32s(input, output *tensor.Dense) (err error) {\n\tbatchSize := input.Shape()[0]\n\tchannels := input.Shape()[1]\n\tnc := channels * batchSize\n\tspatialDim := input.Shape().TotalSize() / nc\n\n\tvar alpha, beta []float32\n\n\tif op.training {\n\t\tsaveMean, saveVar := op.updateStatsF32(batchSize, channels, spatialDim, input)\n\t\talpha, beta = op.calculateAlphaAndBetaF32(batchSize, channels, spatialDim, saveMean, saveVar)\n\t} else {\n\t\t/*\n\t\t\top.saveMean.Zero()\n\t\t\top.saveVariance.Zero()\n\t\t\tsaveMean := op.saveMean.Float32s()\n\t\t\tsaveVar := op.saveVariance.Float32s()\n\t\t\talpha, beta = op.calculateAlphaAndBetaF32(batchSize, channels, spatialDim, scale, bias, saveMean, saveVar)\n\t\t*/\n\n\t\talpha, beta = op.calculateAlphaAndBetaF32(batchSize, channels, spatialDim, nil, nil)\n\n\t}\n\n\t// output = input * alpha + beta\n\toutputF32s := output.Float32s()\n\n\tif spatialDim == 1 {\n\t\trunInParallel(0, batchSize, func(s int) {\n\t\t\tfor c := 0; c < channels; c++ {\n\t\t\t\ti := s*channels + c\n\n\t\t\t\toutputF32s[i] = outputF32s[i]*alpha[c] + beta[c]\n\t\t\t}\n\t\t})\n\n\t} else {\n\t\trunInParallel(0, channels, func(c int) {\n\t\t\tfor s := 0; s < batchSize; s++ {\n\t\t\t\tfor d := 0; d < spatialDim; d++ {\n\t\t\t\t\ti := s*channels*spatialDim + c*spatialDim + d\n\n\t\t\t\t\toutputF32s[i] = outputF32s[i]*alpha[c] + beta[c]\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n\n\treturn nil\n}\n\ntype batchnormDiffOp struct{ *BatchNormOp }\n\nfunc (op *batchnormDiffOp) Arity() int { return 2 }\n\nfunc (op *batchnormDiffOp) Type() hm.Type {\n\tdims := op.dims\n\tif dims == 0 {\n\t\tdims = 4\n\t}\n\n\tt := TensorType{Dims: dims, Of: hm.TypeVariable('a')}\n\treturn hm.NewFnType(t, t, t)\n}\n\nfunc (op *batchnormDiffOp) InferShape(ns ...DimSizer) (tensor.Shape, error) {\n\tif err := checkArity(op, len(ns)); err != nil {\n\t\treturn nil, errors.Wrapf(err, \"batchNorm\")\n\t}\n\n\toriginalShape := ns[0].(tensor.Shape).Clone()\n\n\treturn originalShape, nil\n}\n\nfunc (op *batchnormDiffOp) Do(values ...Value) (Value, error) {\n\tinput := values[0].(*tensor.Dense)\n\tgrad := values[1].(*tensor.Dense)\n\n\tdy, err := CloneValue(input)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tv, err := op.UsePreallocDo(dy, input, grad)\n\n\treturn v, err\n}\n\n// ReturnsPtr is the same exact characteristics of batchnorm\n// CallsExtern is the same exact characteristics of batchnorm\n// OverwritesInput is the same exact characteristics of batchnorm\n\nfunc (op *batchnormDiffOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, \"batchnormdiff-%1.1f-%1.1f\", op.momentum, op.epsilon)\n}\n\nfunc (op *batchnormDiffOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op *batchnormDiffOp) String() string {\n\treturn fmt.Sprintf(\"batchnormdiff-%1.1f-%1.1f\", op.momentum, op.epsilon)\n}\n\nfunc (op *batchnormDiffOp) DiffWRT(inputs int) []bool {\n\t// god help those who want to  do 2nd order differentiation on batchnorm\n\treturn []bool{false, false, false, false}\n}\n\nfunc (op *batchnormDiffOp) SymDiff(inputs Nodes, output *Node, grad *Node) (retVal Nodes, err error) {\n\t// god help those who want to  do 2nd order differentiation on batchnorm\n\treturn nil, nyi(\"SymDiff\", \"batchNormDiffOp\")\n}\n\nfunc (op *batchnormDiffOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) error {\n\t// god help those who want to  do 2nd order differentiation on batchnorm\n\treturn nyi(\"DoDiff\", \"batchnormDiffOp\")\n}\n\nfunc (op *batchnormDiffOp) UsePreallocDo(prealloc Value, inputs ...Value) (retVal Value, err error) {\n\tinput := inputs[0].(*tensor.Dense)\n\tbuffer := prealloc.(*tensor.Dense)\n\toutGrad := inputs[1].(*tensor.Dense)\n\n\t// log.Printf(\"out grad: %v\", outGrad)\n\n\tswitch input.Dtype() {\n\tcase Float64:\n\t\terr = op.f64s(input, buffer, outGrad)\n\tcase Float32:\n\t\terr = op.f32s(input, buffer, outGrad)\n\tdefault:\n\t\treturn nil, nyi(\"batchnormDiffOp\", \"Do\")\n\t}\n\n\treturn prealloc, err\n}\n\nfunc (op *batchnormDiffOp) f64s(input, prealloc, outGrad *tensor.Dense) (err error) {\n\tin := input.Float64s()\n\tig := prealloc.Float64s()\n\n\tdy := outGrad.Float64s()\n\n\tsaveMean := op.saveMean.Float64s()\n\tsaveVariance := op.saveVariance.Float64s()\n\trunningVar := op.runningVariance.Float64s()\n\trunningMean := op.runningVariance.Float64s()\n\n\tbatchSize := input.Shape()[0]\n\tchannels := input.Shape()[1]\n\tnc := batchSize * channels\n\tspatialDim := len(in) / nc\n\tn := batchSize * spatialDim\n\n\tif spatialDim == 1 {\n\t\trunInParallel(0, channels, func(c int) {\n\t\t\tdotp := float64(0.0)\n\t\t\tsum := 0.0\n\n\t\t\tvar mean, invstd float64\n\t\t\tif op.training {\n\t\t\t\tmean = saveMean[c]\n\t\t\t\tinvstd = 1 / math.Sqrt(saveVariance[c]/float64(n)+op.epsilon)\n\t\t\t} else {\n\t\t\t\tmean = runningMean[c]\n\t\t\t\tinvstd = 1 / math.Sqrt(runningVar[c]+op.epsilon)\n\t\t\t}\n\n\t\t\tfor s := 0; s < n; s++ {\n\t\t\t\ti := s*channels + c\n\n\t\t\t\tsum += dy[i]\n\t\t\t\tpartialDotp := (in[i] - mean) * float64(dy[i])\n\t\t\t\tdotp += partialDotp\n\t\t\t}\n\n\t\t\t// grad_mean = dySum / N\n\t\t\tgradMean := sum / float64(n)\n\n\t\t\tif op.training {\n\t\t\t\tk := float64(dotp*invstd*invstd) / float64(n)\n\n\t\t\t\tfor s := 0; s < n; s++ {\n\t\t\t\t\ti := s*channels + c\n\n\t\t\t\t\t// dx = (x - mean) * k\n\t\t\t\t\tig[i] = ((in[i] - mean) * k)\n\n\t\t\t\t\t// dx = (dy - dx - grad_mean) / variance\n\t\t\t\t\tig[i] = (float64(dy[i]-ig[i]-gradMean) * invstd)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfor s := 0; s < n; s++ {\n\t\t\t\t\ti := s*channels + c\n\n\t\t\t\t\tig[i] = dy[i] * invstd\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t} else {\n\t\trunInParallel(0, channels, func(c int) {\n\t\t\tdotp := float64(0.0)\n\t\t\tsum := 0.0\n\n\t\t\tfor s := 0; s < batchSize; s++ {\n\t\t\t\tfor d := 0; d < spatialDim; d++ {\n\t\t\t\t\ti := s*channels*spatialDim + c*spatialDim + d\n\n\t\t\t\t\tsum += dy[i]\n\n\t\t\t\t\tpartialDotp := (float64(in[i])*dy[i] - float64(saveMean[c])*float64(dy[i]))\n\t\t\t\t\tdotp += partialDotp\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tinvstd := 1 / math.Sqrt(float64(saveVariance[c])/float64(n)+float64(op.epsilon))\n\t\t\tk := float64(dotp*invstd*invstd) / float64(n)\n\n\t\t\t// grad_mean = dySum / N\n\t\t\tgradMean := sum / float64(n)\n\n\t\t\tif op.training {\n\t\t\t\tfor s := 0; s < batchSize; s++ {\n\t\t\t\t\tfor d := 0; d < spatialDim; d++ {\n\t\t\t\t\t\ti := s*channels*spatialDim + c*spatialDim + d\n\n\t\t\t\t\t\t// dx = (x - mean) * k\n\t\t\t\t\t\tig[i] = (in[i] - saveMean[c]) * k\n\n\t\t\t\t\t\t// dx = (dy - dx - grad_mean) / variance\n\t\t\t\t\t\tig[i] = (dy[i] - gradMean - ig[i]) * invstd\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfor s := 0; s < batchSize; s++ {\n\t\t\t\t\tfor d := 0; d < spatialDim; d++ {\n\t\t\t\t\t\ti := s*channels*spatialDim + c*spatialDim + d\n\n\t\t\t\t\t\tig[i] = dy[i] * invstd\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n\n\treturn nil\n}\n\nfunc (op *batchnormDiffOp) f32s(input, prealloc, outGrad *tensor.Dense) (err error) {\n\tin := input.Float32s()\n\tig := prealloc.Float32s()\n\n\tdy := outGrad.Float32s()\n\n\tsaveMean := op.saveMean.Float32s()\n\tsaveVariance := op.saveVariance.Float32s()\n\trunningVar := op.runningVariance.Float32s()\n\trunningMean := op.runningVariance.Float32s()\n\n\tbatchSize := input.Shape()[0]\n\tchannels := input.Shape()[1]\n\tnc := batchSize * channels\n\tspatialDim := len(in) / nc\n\tn := batchSize * spatialDim\n\n\tif spatialDim == 1 {\n\t\trunInParallel(0, channels, func(c int) {\n\t\t\tdotp := float32(0.0)\n\t\t\tsum := float32(0.0)\n\n\t\t\tvar mean, invstd float32\n\t\t\tif op.training {\n\t\t\t\tmean = saveMean[c]\n\t\t\t\tinvstd = 1 / math32.Sqrt(saveVariance[c]/float32(n)+float32(op.epsilon))\n\t\t\t} else {\n\t\t\t\tmean = runningMean[c]\n\t\t\t\tinvstd = 1 / math32.Sqrt(runningVar[c]+float32(op.epsilon))\n\t\t\t}\n\n\t\t\tfor s := 0; s < n; s++ {\n\t\t\t\ti := s*channels + c\n\n\t\t\t\tsum += dy[i]\n\t\t\t\tpartialDotp := (in[i] - mean) * dy[i]\n\t\t\t\tdotp += partialDotp\n\t\t\t}\n\n\t\t\t// grad_mean = dySum / N\n\t\t\tgradMean := sum / float32(n)\n\n\t\t\tif op.training {\n\t\t\t\tk := float32(dotp*invstd*invstd) / float32(n)\n\n\t\t\t\tfor s := 0; s < n; s++ {\n\t\t\t\t\ti := s*channels + c\n\n\t\t\t\t\t// dx = (x - mean) * k\n\t\t\t\t\tig[i] = ((in[i] - mean) * k)\n\n\t\t\t\t\t// dx = (dy - dx - grad_mean) / variance\n\t\t\t\t\tig[i] = (dy[i] - ig[i] - gradMean) * invstd\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfor s := 0; s < n; s++ {\n\t\t\t\t\ti := s*channels + c\n\n\t\t\t\t\tig[i] = dy[i] * invstd\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t} else {\n\t\trunInParallel(0, channels, func(c int) {\n\t\t\tdotp := float32(0.0)\n\t\t\tsum := float32(0.0)\n\n\t\t\tfor s := 0; s < batchSize; s++ {\n\t\t\t\tfor d := 0; d < spatialDim; d++ {\n\t\t\t\t\ti := s*channels*spatialDim + c*spatialDim + d\n\n\t\t\t\t\tsum += dy[i]\n\n\t\t\t\t\tpartialDotp := (in[i]*dy[i] - saveMean[c]*dy[i])\n\t\t\t\t\tdotp += partialDotp\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tinvstd := 1 / math32.Sqrt(saveVariance[c]/float32(n)+float32(op.epsilon))\n\t\t\tk := float32(dotp*invstd*invstd) / float32(n)\n\n\t\t\t// grad_mean = dySum / N\n\t\t\tgradMean := sum / float32(n)\n\n\t\t\tif op.training {\n\t\t\t\tfor s := 0; s < batchSize; s++ {\n\t\t\t\t\tfor d := 0; d < spatialDim; d++ {\n\t\t\t\t\t\ti := s*channels*spatialDim + c*spatialDim + d\n\n\t\t\t\t\t\t// dx = (x - mean) * k\n\t\t\t\t\t\tig[i] = (in[i] - saveMean[c]) * k\n\n\t\t\t\t\t\t// dx = (dy - dx - grad_mean) / variance\n\t\t\t\t\t\tig[i] = (dy[i] - gradMean - ig[i]) * invstd\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfor s := 0; s < batchSize; s++ {\n\t\t\t\t\tfor d := 0; d < spatialDim; d++ {\n\t\t\t\t\t\ti := s*channels*spatialDim + c*spatialDim + d\n\n\t\t\t\t\t\tig[i] = dy[i] * invstd\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n\n\treturn nil\n}\n\ntype globalAveragePoolOp struct{}\n\nfunc (g *globalAveragePoolOp) Arity() int {\n\treturn 1\n}\n\nfunc (g *globalAveragePoolOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\tt := newTensorType(4, a)\n\treturn hm.NewFnType(t, t)\n}\n\nfunc (g *globalAveragePoolOp) InferShape(inputs ...DimSizer) (tensor.Shape, error) {\n\tb, err := inputs[0].DimSize(0)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tc, err := inputs[0].DimSize(1)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// check if the shape is correct without doing type inference\n\tif _, err := inputs[0].DimSize(2); err != nil {\n\t\treturn nil, err\n\t}\n\tif _, err := inputs[0].DimSize(3); err != nil {\n\t\treturn nil, err\n\t}\n\treturn tensor.Shape{b, c, 1, 1}, nil\n}\n\nfunc (g *globalAveragePoolOp) Do(inputs ...Value) (Value, error) {\n\tim := inputs[0]\n\tswitch im.(type) {\n\tcase tensor.Tensor:\n\t\tv := im.(tensor.Tensor)\n\t\tB, C, H, W := v.Shape()[0], v.Shape()[1], v.Shape()[2], v.Shape()[3]\n\t\ts, err := g.InferShape(v.Shape())\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\toutput := tensor.New(tensor.Of(v.Dtype()), tensor.WithShape(s...))\n\t\tswitch v.Dtype() {\n\t\tcase tensor.Float64:\n\t\t\tfor b := 0; b < B; b++ {\n\t\t\t\tfor c := 0; c < C; c++ {\n\t\t\t\t\tvar sum float64\n\t\t\t\t\tfor h := 0; h < H; h++ {\n\t\t\t\t\t\tfor w := 0; w < W; w++ {\n\t\t\t\t\t\t\tval, err := v.At(b, c, h, w)\n\t\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\t\treturn nil, err\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tsum += val.(float64)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\terr := output.SetAt(sum/float64(H*W), b, c, 0, 0)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn nil, err\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\tcase tensor.Float32:\n\t\t\tfor b := 0; b < B; b++ {\n\t\t\t\tfor c := 0; c < C; c++ {\n\t\t\t\t\tvar sum float32\n\t\t\t\t\tfor h := 0; h < H; h++ {\n\t\t\t\t\t\tfor w := 0; w < W; w++ {\n\t\t\t\t\t\t\tval, err := v.At(b, c, h, w)\n\t\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\t\treturn nil, err\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tsum += val.(float32)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\terr := output.SetAt(sum/float32(H*W), b, c, 0, 0)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn nil, err\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\treturn nil, nyi(\"Global Average Pool\", v.Dtype())\n\t\t}\n\n\t\treturn output, nil\n\n\tdefault:\n\t\treturn nil, nyi(\"globalAveragePoolOp\", inputs)\n\t}\n}\n\nfunc (g *globalAveragePoolOp) ReturnsPtr() bool {\n\treturn false\n}\n\nfunc (g *globalAveragePoolOp) CallsExtern() bool {\n\treturn false\n}\n\nfunc (g *globalAveragePoolOp) OverwritesInput() int {\n\treturn -1\n}\n\nfunc (g *globalAveragePoolOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, \"GlobalAveragePool\")\n}\n\nfunc (g *globalAveragePoolOp) Hashcode() uint32 {\n\treturn simpleHash(g)\n}\n\nfunc (g *globalAveragePoolOp) String() string {\n\treturn \"GlobalAveragePool\"\n}\n"
        },
        {
          "name": "op_nn_deprecated.go",
          "type": "blob",
          "size": 12.3125,
          "content": "// +build ignore\n\npackage gorgonia\n\nimport (\n\t\"gonum.org/v1/gonum/blas\"\n\t\"gorgonia.org/tensor\"\n\t\"gorgonia.org/vecf32\"\n\t\"gorgonia.org/vecf64\"\n)\n\nfunc (op *BatchNormOp) f64sOld(input, output *tensor.Dense) (err error) {\n\tn := input.Shape()[0]\n\tchannels := input.Shape()[1]\n\tnc := channels * n\n\tspatialDim := input.Shape().TotalSize() / (nc)\n\n\tinputF64s := input.Float64s()\n\toutputF64s := output.Float64s()\n\tcopy(outputF64s, inputF64s)\n\n\tmeanTmp := op.runningMean.Float64s()\n\tmean := op.mean.Float64s()\n\tvarianceTmp := op.runningVariance.Float64s()\n\tvariance := op.variance.Float64s()\n\ttmp := op.tmpSpace.Float64s()\n\tssm := op.spatialSumMultiplier.Float64s()\n\tnbc := op.numByChans.Float64s()\n\tbsm := op.batchSumMultiplier.Float64s()\n\n\tmomentum := op.momentum\n\teps := op.epsilon\n\n\tif !op.training {\n\t\t// use stored mean/variance estimates\n\t\tscaleFactor := float64(1)\n\t\tif fst := op.ma.Float64s()[0]; fst != 1 {\n\t\t\tscaleFactor = fst\n\t\t}\n\t\tcopy(meanTmp, mean)\n\t\twhichblas.Dscal(len(meanTmp), scaleFactor, meanTmp, 1)\n\t\tcopy(varianceTmp, variance)\n\t\twhichblas.Dscal(len(varianceTmp), scaleFactor, varianceTmp, 1)\n\t} else {\n\t\t// compute mean\n\t\talpha := 1.0 / float64(n*spatialDim)\n\t\twhichblas.Dgemv(blas.NoTrans, nc, spatialDim, alpha, inputF64s, spatialDim, ssm, 1, 0, nbc, 1)\n\t\twhichblas.Dgemv(blas.Trans, n, channels, 1, nbc, channels, bsm, 1, 0, meanTmp, 1)\n\t}\n\n\t// subtract mean\n\twhichblas.Dgemm(blas.NoTrans, blas.NoTrans, n, channels, 1, 1, bsm, 1, meanTmp, channels, 0, nbc, channels)\n\twhichblas.Dgemm(blas.NoTrans, blas.NoTrans, nc, spatialDim, 1, -1, nbc, 1, ssm, spatialDim, 1, outputF64s, spatialDim)\n\n\tif op.training {\n\t\t// compute variance using var(X) = E(X-EX)²)\n\t\tcopy(tmp, outputF64s)\n\t\tvecf64.Mul(tmp, tmp) // (X-EX) ^ 2\n\n\t\twhichblas.Dgemv(blas.NoTrans, nc, spatialDim, 1.0/(float64(n*spatialDim)), tmp, spatialDim, ssm, 1, 0, nbc, 1)\n\t\twhichblas.Dgemv(blas.Trans, n, channels, 1.0, nbc, channels, bsm, 1, 0, varianceTmp, 1) // E((X_EX)²)\n\n\t\t// compute and save moving average\n\t\top.ma.Float64s()[0] *= momentum\n\t\top.ma.Float64s()[0]++\n\n\t\t// TODO: write axpby for gonum\n\t\twhichblas.Dscal(len(mean), momentum, mean, 1)\n\t\twhichblas.Daxpy(len(meanTmp), 1.0, meanTmp, 1, mean, 1)\n\n\t\tm := len(inputF64s) / channels\n\t\tcorrectionFactor := float64(1)\n\t\tif m > 1 {\n\t\t\tcorrectionFactor = float64(m) / (float64(m - 1))\n\t\t}\n\t\twhichblas.Dscal(len(variance), momentum, variance, 1)\n\t\twhichblas.Daxpy(len(varianceTmp), correctionFactor, varianceTmp, 1, variance, 1)\n\t}\n\n\t// normalize variance\n\tvecf64.Trans(varianceTmp, eps)\n\tvecf64.Sqrt(varianceTmp)\n\n\t// replicate variance to inputsize\n\twhichblas.Dgemm(blas.NoTrans, blas.NoTrans, n, channels, 1, 1, bsm, 1, varianceTmp, channels, 0, nbc, channels)\n\twhichblas.Dgemm(blas.NoTrans, blas.NoTrans, nc, spatialDim, 1, 1, nbc, 1, ssm, spatialDim, 0, tmp, spatialDim)\n\tvecf64.Div(outputF64s, tmp)\n\tcopy(op.xNorm.Float64s(), outputF64s) // caching\n\n\treturn nil\n}\n\nfunc (op *BatchNormOp) f32sOld(input, output *tensor.Dense) (err error) {\n\tn := input.Shape()[0]\n\tchannels := input.Shape()[1]\n\tnc := channels * n\n\tspatialDim := input.Shape().TotalSize() / (nc)\n\n\tinputF32s := input.Float32s()\n\toutputF32s := output.Float32s()\n\tcopy(outputF32s, inputF32s)\n\n\tmeanTmp := op.runningMean.Float32s()\n\tmean := op.mean.Float32s()\n\tvarianceTmp := op.runningVariance.Float32s()\n\tvariance := op.variance.Float32s()\n\ttmp := op.tmpSpace.Float32s()\n\tssm := op.spatialSumMultiplier.Float32s()\n\tnbc := op.numByChans.Float32s()\n\tbsm := op.batchSumMultiplier.Float32s()\n\n\tmomentum := float32(op.momentum)\n\teps := float32(op.epsilon)\n\n\tif !op.training {\n\t\t// use stored mean/variance estimates\n\t\tscaleFactor := float32(1)\n\t\tif fst := op.ma.Float32s()[0]; fst != 1 {\n\t\t\tscaleFactor = fst\n\t\t}\n\t\tcopy(meanTmp, mean)\n\t\twhichblas.Sscal(len(meanTmp), scaleFactor, meanTmp, 1)\n\t\tcopy(varianceTmp, variance)\n\t\twhichblas.Sscal(len(varianceTmp), scaleFactor, varianceTmp, 1)\n\t} else {\n\t\t// compute mean\n\t\talpha := 1.0 / float32(n*spatialDim)\n\t\twhichblas.Sgemv(blas.NoTrans, nc, spatialDim, alpha, inputF32s, spatialDim, ssm, 1, 0, nbc, 1)\n\t\twhichblas.Sgemv(blas.Trans, n, channels, 1, nbc, channels, bsm, 1, 0, meanTmp, 1)\n\t}\n\n\t// subtract mean\n\twhichblas.Sgemm(blas.NoTrans, blas.NoTrans, n, channels, 1, 1, bsm, 1, meanTmp, channels, 0, nbc, channels)\n\twhichblas.Sgemm(blas.NoTrans, blas.NoTrans, nc, spatialDim, 1, -1, nbc, 1, ssm, spatialDim, 1, outputF32s, spatialDim)\n\n\tif op.training {\n\t\t// compute variance using var(X) = E(X-EX)²)\n\t\tcopy(tmp, outputF32s)\n\t\tvecf32.Mul(tmp, tmp) // (X-EX) ^ 2\n\n\t\twhichblas.Sgemv(blas.NoTrans, nc, spatialDim, 1.0/(float32(n*spatialDim)), tmp, spatialDim, ssm, 1, 0, nbc, 1)\n\t\twhichblas.Sgemv(blas.Trans, n, channels, 1.0, nbc, channels, bsm, 1, 0, varianceTmp, 1) // E((X_EX)²)\n\n\t\t// compute and save moving average\n\t\top.ma.Float32s()[0] *= momentum\n\t\top.ma.Float32s()[0]++\n\n\t\t// TODO: write axpby for gonum\n\t\twhichblas.Sscal(len(mean), momentum, mean, 1)\n\t\twhichblas.Saxpy(len(meanTmp), 1.0, meanTmp, 1, mean, 1)\n\n\t\tm := len(inputF32s) / channels\n\t\tcorrectionFactor := float32(1)\n\t\tif m > 1 {\n\t\t\tcorrectionFactor = float32(m) / (float32(m - 1))\n\t\t}\n\t\twhichblas.Sscal(len(variance), momentum, variance, 1)\n\t\twhichblas.Saxpy(len(varianceTmp), correctionFactor, varianceTmp, 1, variance, 1)\n\t}\n\n\t// normalize variance\n\tvecf32.Trans(varianceTmp, eps)\n\tvecf32.Sqrt(varianceTmp)\n\n\t// replicate variance to inputsize\n\twhichblas.Sgemm(blas.NoTrans, blas.NoTrans, n, channels, 1, 1, bsm, 1, varianceTmp, channels, 0, nbc, channels)\n\twhichblas.Sgemm(blas.NoTrans, blas.NoTrans, nc, spatialDim, 1, 1, nbc, 1, ssm, spatialDim, 0, tmp, spatialDim)\n\tvecf32.Div(outputF32s, tmp)\n\tcopy(op.xNorm.Float32s(), outputF32s) // caching\n\n\treturn nil\n}\n\nfunc (op *batchnormDiffOp) f64sOld(input, inGrad, outGrad *tensor.Dense) (err error) {\n\tin := input.Float64s()\n\tig := inGrad.Float64s()\n\tog := outGrad.Float64s()\n\ttmp := op.tmpSpace.Float64s()\n\tout := op.xNorm.Float64s()\n\tssm := op.spatialSumMultiplier.Float64s()\n\tnbc := op.numByChans.Float64s()\n\tbsm := op.batchSumMultiplier.Float64s()\n\tmeanTmp := op.runningMean.Float64s()\n\n\tif !op.training {\n\t\tcopy(ig, og)\n\t\tvecf64.Div(og, tmp)\n\t\treturn nil\n\t}\n\n\tn := input.Shape()[0]\n\tchannels := input.Shape()[1]\n\tnc := n * channels\n\tspatialDim := len(in) / nc\n\n\t// if Y = (X-mean(X))/(sqrt(var(X)+eps)), then\n\t//\n\t// dE(Y)/dX =\n\t//   (dE/dY - mean(dE/dY) - mean(dE/dY ⋅ Y) ⋅ Y)\n\t//     ./ sqrt(var(X) + eps)\n\t//\n\t// where ⋅ and ./ are hadamard product and elementwise division,\n\t// respectively, dE/dY is the top diff, and mean/var/sum are all computed\n\t// along all dimensions except the channels dimension.  In the above\n\t// equation, the operations allow for expansion (i.e. broadcast) along all\n\t// dimensions except the channels dimension where required.\n\n\t// sum(dE/dY ⋅ Y)\n\tcopy(ig, out)\n\tvecf64.Mul(ig, og)\n\twhichblas.Dgemv(blas.NoTrans, nc, spatialDim, 1, ig, spatialDim, ssm, 1, 0, nbc, 1)\n\twhichblas.Dgemv(blas.Trans, n, channels, 1, nbc, channels, bsm, 1, 0, meanTmp, 1)\n\n\t// reshape (broadcast) the above\n\twhichblas.Dgemm(blas.NoTrans, blas.NoTrans, n, channels, 1, 1, bsm, 1, meanTmp, channels, 0, nbc, channels)\n\twhichblas.Dgemm(blas.NoTrans, blas.NoTrans, nc, spatialDim, 1, 1, nbc, 1, ssm, spatialDim, 0, ig, spatialDim)\n\n\t// sum(dE/dY ⋅ Y) ⋅ Y\n\tvecf64.Mul(ig, out)\n\n\t// sum(dE/dY)-sum(dE/dY ⋅ Y) ⋅ Y\n\twhichblas.Dgemv(blas.NoTrans, nc, spatialDim, 1, og, spatialDim, ssm, 1, 0, nbc, 1)\n\twhichblas.Dgemv(blas.Trans, n, channels, 1, nbc, channels, bsm, 1, 0, meanTmp, 1)\n\n\t// reshape (broadcast) the above to make\n\t// sum(dE/dY)-sum(dE/dY ⋅ Y) ⋅ Y\n\twhichblas.Dgemm(blas.NoTrans, blas.NoTrans, n, channels, 1, 1, bsm, 1, meanTmp, channels, 0, nbc, channels)\n\twhichblas.Dgemm(blas.NoTrans, blas.NoTrans, nc, spatialDim, 1, 1, nbc, 1, ssm, spatialDim, 1, ig, spatialDim)\n\n\t// dE/dY - mean(dE/dY)-mean(dE/dY ⋅ Y) ⋅ Y\n\tbeta := (-1.0 / float64(nc))\n\n\tvecf64.Scale(ig, beta)\n\tvecf64.Add(ig, og)\n\n\t// note: temp_ still contains sqrt(var(X)+eps), computed during the forward\n\t// pass.\n\tvecf64.Div(ig, tmp)\n\treturn nil\n\n}\n\nfunc (op *batchnormDiffOp) f32sOld(input, inGrad, outGrad *tensor.Dense) (err error) {\n\tin := input.Float32s()\n\tig := inGrad.Float32s()\n\tog := outGrad.Float32s()\n\ttmp := op.tmpSpace.Float32s()\n\tout := op.xNorm.Float32s()\n\tssm := op.spatialSumMultiplier.Float32s()\n\tnbc := op.numByChans.Float32s()\n\tbsm := op.batchSumMultiplier.Float32s()\n\tmeanTmp := op.runningMean.Float32s()\n\n\tif !op.training {\n\t\tcopy(ig, og)\n\t\tvecf32.Div(og, tmp)\n\t\treturn nil\n\t}\n\n\tn := input.Shape()[0]\n\tchannels := input.Shape()[1]\n\tnc := n * channels\n\tspatialDim := len(in) / nc\n\n\t// if Y = (X-mean(X))/(sqrt(var(X)+eps)), then\n\t//\n\t// dE(Y)/dX =\n\t//   (dE/dY - mean(dE/dY) - mean(dE/dY ⋅ Y) ⋅ Y)\n\t//     ./ sqrt(var(X) + eps)\n\t//\n\t// where ⋅ and ./ are hadamard product and elementwise division,\n\t// respectively, dE/dY is the top diff, and mean/var/sum are all computed\n\t// along all dimensions except the channels dimension.  In the above\n\t// equation, the operations allow for expansion (i.e. broadcast) along all\n\t// dimensions except the channels dimension where required.\n\n\t// sum(dE/dY ⋅ Y)\n\tcopy(ig, out)\n\tvecf32.Mul(ig, og)\n\twhichblas.Sgemv(blas.NoTrans, nc, spatialDim, 1, ig, spatialDim, ssm, 1, 0, nbc, 1)\n\twhichblas.Sgemv(blas.Trans, n, channels, 1, nbc, channels, bsm, 1, 0, meanTmp, 1)\n\n\t// reshape (broadcast) the above\n\twhichblas.Sgemm(blas.NoTrans, blas.NoTrans, n, channels, 1, 1, bsm, 1, meanTmp, channels, 0, nbc, channels)\n\twhichblas.Sgemm(blas.NoTrans, blas.NoTrans, nc, spatialDim, 1, 1, nbc, 1, ssm, spatialDim, 0, ig, spatialDim)\n\n\t// sum(dE/dY ⋅ Y) ⋅ Y\n\tvecf32.Mul(ig, out)\n\n\t// sum(dE/dY)-sum(dE/dY ⋅ Y) ⋅ Y\n\twhichblas.Sgemv(blas.NoTrans, nc, spatialDim, 1, og, spatialDim, ssm, 1, 0, nbc, 1)\n\twhichblas.Sgemv(blas.Trans, n, channels, 1, nbc, channels, bsm, 1, 0, meanTmp, 1)\n\n\t// reshape (broadcast) the above to make\n\t// sum(dE/dY)-sum(dE/dY ⋅ Y) ⋅ Y\n\twhichblas.Sgemm(blas.NoTrans, blas.NoTrans, n, channels, 1, 1, bsm, 1, meanTmp, channels, 0, nbc, channels)\n\twhichblas.Sgemm(blas.NoTrans, blas.NoTrans, nc, spatialDim, 1, 1, nbc, 1, ssm, spatialDim, 1, ig, spatialDim)\n\n\t// dE/dY - mean(dE/dY)-mean(dE/dY ⋅ Y) ⋅ Y\n\tbeta := (-1.0 / float32(n*spatialDim))\n\tvecf32.Scale(ig, beta)\n\tvecf32.Add(ig, og)\n\n\t// note: temp_ still contains sqrt(var(X)+eps), computed during the forward\n\t// pass.\n\tvecf32.Div(ig, tmp)\n\treturn nil\n\n}\n\nfunc (op *BatchNormOp) mul64(prealloc Value, scale Value) error {\n\tretVal := prealloc.(*tensor.Dense)\n\ts := scale.(*tensor.Dense)\n\n\tsTotal := s.Shape().TotalSize()\n\trTotal := retVal.Shape().TotalSize()\n\t// e.g.\n\t// (1, 3) × (2, 3)... we'll just assume it gets repeated on the outer dim\n\tif sTotal < rTotal {\n\t\tsData := s.Float64s()\n\t\trData := retVal.Float64s()\n\n\t\tn := rTotal / sTotal\n\n\t\tfor i := 0; i < n; i++ {\n\t\t\tstart := i * len(sData)\n\t\t\tend := start + len(sData)\n\t\t\tit := rData[start:end]\n\n\t\t\tvecf64.Mul(it, sData)\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (op *BatchNormOp) add64(prealloc, bias Value) error {\n\tretVal := prealloc.(*tensor.Dense)\n\tb := bias.(*tensor.Dense)\n\n\tsTotal := b.Shape().TotalSize()\n\trTotal := retVal.Shape().TotalSize()\n\t// e.g.\n\t// (1, 3) × (2, 3)... we'll just assume it gets repeated on the outer dim\n\tif sTotal < rTotal {\n\t\tsData := b.Float64s()\n\t\trData := retVal.Float64s()\n\n\t\tn := rTotal / sTotal // broadcast the operation n times\n\n\t\tfor i := 0; i < n; i++ {\n\t\t\tstart := i * len(sData)\n\t\t\tend := start + len(sData)\n\t\t\tit := rData[start:end]\n\n\t\t\tvecf64.Add(it, sData)\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (op *BatchNormOp) mul32(prealloc Value, scale Value) error {\n\tretVal := prealloc.(*tensor.Dense)\n\ts := scale.(*tensor.Dense)\n\n\tsTotal := s.Shape().TotalSize()\n\trTotal := retVal.Shape().TotalSize()\n\t// e.g.\n\t// (1, 3) × (2, 3)... we'll just assume it gets repeated on the outer dim\n\tif sTotal < rTotal {\n\t\tsData := s.Float32s()\n\t\trData := retVal.Float32s()\n\n\t\tn := rTotal / sTotal\n\n\t\tfor i := 0; i < n; i++ {\n\t\t\tstart := i * len(sData)\n\t\t\tend := start + len(sData)\n\t\t\tit := rData[start:end]\n\n\t\t\tvecf32.Mul(it, sData)\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (op *BatchNormOp) add32(prealloc, bias Value) error {\n\tretVal := prealloc.(*tensor.Dense)\n\tb := bias.(*tensor.Dense)\n\n\tsTotal := b.Shape().TotalSize()\n\trTotal := retVal.Shape().TotalSize()\n\t// e.g.\n\t// (1, 3) × (2, 3)... we'll just assume it gets repeated on the outer dim\n\tif sTotal < rTotal {\n\t\tsData := b.Float32s()\n\t\trData := retVal.Float32s()\n\n\t\tn := rTotal / sTotal // broadcast the operation n times\n\n\t\tfor i := 0; i < n; i++ {\n\t\t\tstart := i * len(sData)\n\t\t\tend := start + len(sData)\n\t\t\tit := rData[start:end]\n\n\t\t\tvecf32.Add(it, sData)\n\t\t}\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "op_nondiff.go",
          "type": "blob",
          "size": 2.3994140625,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"hash\"\n\n\t\"github.com/chewxy/hm\"\n\t\"gorgonia.org/tensor\"\n)\n\ntype diagFlatOp struct{}\n\n/* Graph Building Related Methods */\n\n// Arity returns the number of inputs the Op expects. -1 indicates that it's n-ary and will be determined at runtime\nfunc (op diagFlatOp) Arity() int { return 1 }\n\n// Informs the type of the Op (not the node). This will be used by the type system to infer the final type of the node\nfunc (op diagFlatOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\tb := hm.TypeVariable('a')\n\tT := makeTensorType(2, b)\n\treturn hm.NewFnType(a, T)\n}\n\n// returns the output shape as a function of the inputs\nfunc (op diagFlatOp) InferShape(inputs ...DimSizer) (tensor.Shape, error) {\n\tif err := checkArity(op, len(inputs)); err != nil {\n\t\treturn nil, err\n\t}\n\tin := inputs[0].(tensor.Shape)\n\treturn tensor.Shape{in.TotalSize(), in.TotalSize()}, nil\n}\n\n/* Machine related */ // executes the op\nfunc (op diagFlatOp) Do(vals ...Value) (Value, error) {\n\tif err := checkArity(op, len(vals)); err != nil {\n\t\treturn nil, err\n\t}\n\n\tT := vals[0].(tensor.Tensor)\n\treturn tensor.New(tensor.AsDenseDiag(T.Data())), nil\n}\n\n/* Analysis Related Methods */\n\n// indicates if the Op will return a pointer (allowing possible inplace edits) or by value\n// if it's false, the return value of the Op will be a copy of its input\nfunc (op diagFlatOp) ReturnsPtr() bool { return false }\n\n// Does this op potentially call external (cgo or cuda) functions (thereby requiring extra overhead for Go's trampolining thing)\nfunc (op diagFlatOp) CallsExtern() bool { return false }\n\n// overwriteInput() is a method which states which input the output will be overwriting.\n// This allows for some efficiency gains as the underlying arrays wouldn't have to be re-allocated.\n// The method returns an int instead of a bool because potentially different operations may be allowed\n// to overwrite certain inputs. For example, consider an operation to increment a value:\n// the IncrementOp would be a unary operator, and assuming we would like to overwrite the input,\n// the retVal of overwriteInput() will be 0 (inputs[0]).\n// -1 is returned if overwriting of input is disallowed\nfunc (op diagFlatOp) OverwritesInput() int { return -1 }\n\n/* Other methods */\nfunc (op diagFlatOp) WriteHash(h hash.Hash) { fmt.Fprintf(h, \"DiagFlatOp\") }\n\nfunc (op diagFlatOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op diagFlatOp) String() string { return \"DiagFlat\" }\n"
        },
        {
          "name": "op_nondiff_test.go",
          "type": "blob",
          "size": 2.0498046875,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\n\t\"gorgonia.org/tensor\"\n)\n\nfunc ExampleDiagFlat() {\n\tg := NewGraph()\n\n\t// 2 dimensional\n\taV := tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 2, 3, 4}))\n\ta := NodeFromAny(g, aV)\n\tb, err := DiagFlat(a)\n\tif err != nil {\n\t\tfmt.Println(err)\n\t\treturn\n\t}\n\tm := NewTapeMachine(g)\n\tif err := m.RunAll(); err != nil {\n\t\tfmt.Println(err)\n\t\treturn\n\t}\n\tfmt.Printf(\"a:\\n%v\\n\", a.Value())\n\tfmt.Printf(\"b:\\n%v\\n\", b.Value())\n\n\t// 3 dimensional\n\taV = tensor.New(tensor.WithShape(2, 3, 2), tensor.WithBacking([]float64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}))\n\ta = NodeFromAny(g, aV, WithName(\"a'\"))\n\tb2, err := DiagFlat(a)\n\tif err != nil {\n\t\tfmt.Println(err)\n\t\treturn\n\t}\n\tm = NewTapeMachine(g)\n\tif err := m.RunAll(); err != nil {\n\t\tfmt.Println(err)\n\t}\n\n\tfmt.Printf(\"a:\\n%v\", a.Value())\n\tfmt.Printf(\"b:\\n%v\\n\", b2.Value())\n\n\t// 1 dimensional\n\taV = tensor.New(tensor.WithShape(2), tensor.WithBacking([]float64{1, 2}))\n\ta = NodeFromAny(g, aV, WithName(\"a''\"))\n\tb3, err := DiagFlat(a)\n\tif err != nil {\n\t\tfmt.Println(err)\n\t\treturn\n\t}\n\tm = NewTapeMachine(g)\n\tif err := m.RunAll(); err != nil {\n\t\tfmt.Println(err)\n\t}\n\n\tfmt.Printf(\"a:\\n%v\\n\", a.Value())\n\tfmt.Printf(\"b:\\n%v\\n\", b3.Value())\n\n\t// Scalars\n\n\ta = NodeFromAny(g, 100.0, WithName(\"aScalar\"))\n\t_, err = DiagFlat(a)\n\tfmt.Println(err)\n\n\t// Output:\n\t// a:\n\t// ⎡1  2⎤\n\t// ⎣3  4⎦\n\t//\n\t// b:\n\t// ⎡1  0  0  0⎤\n\t// ⎢0  2  0  0⎥\n\t// ⎢0  0  3  0⎥\n\t// ⎣0  0  0  4⎦\n\t//\n\t// a:\n\t// ⎡ 1   2⎤\n\t// ⎢ 3   4⎥\n\t// ⎣ 5   6⎦\n\t//\n\t// ⎡ 7   8⎤\n\t// ⎢ 9  10⎥\n\t// ⎣11  12⎦\n\t//\n\t//\n\t// b:\n\t// ⎡ 1   0   0   0  ...  0   0   0   0⎤\n\t// ⎢ 0   2   0   0  ...  0   0   0   0⎥\n\t// ⎢ 0   0   3   0  ...  0   0   0   0⎥\n\t// ⎢ 0   0   0   4  ...  0   0   0   0⎥\n\t// .\n\t// .\n\t// .\n\t// ⎢ 0   0   0   0  ...  9   0   0   0⎥\n\t// ⎢ 0   0   0   0  ...  0  10   0   0⎥\n\t// ⎢ 0   0   0   0  ...  0   0  11   0⎥\n\t// ⎣ 0   0   0   0  ...  0   0   0  12⎦\n\t//\n\t// a:\n\t// [1  2]\n\t// b:\n\t// ⎡1  0⎤\n\t// ⎣0  2⎦\n\t//\n\t// Cannot perform DiagFlat on a scalar equivalent node\n\n}\n"
        },
        {
          "name": "op_reduction.go",
          "type": "blob",
          "size": 8.9365234375,
          "content": "package gorgonia\n\n/*\nThis file holds code for ndarray related reduction Ops.\nWhat this means is we take a ndarray, and reduce the dimensions down - typically to 1.\nFor example, summing all the values in a matrix, or finding the max value.\nThere is an additional field in each of these Ops - the 'along' field. This is because it's not always we want to reduce a ndarray down to a single scalar number\n*/\n\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"hash\"\n\t\"strings\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc reductionType(d int, along []int) hm.Type {\n\ta := hm.TypeVariable('a')\n\tt := makeTensorType(d-len(along), a)\n\n\taxes := make(map[int]bool)\n\tfor _, axis := range along {\n\t\tif axis < d {\n\t\t\taxes[axis] = true\n\t\t}\n\t}\n\n\tif d == 1 || len(axes) == 0 || len(axes) == d {\n\t\t// then it reduces down\n\t\treturn hm.NewFnType(t, a)\n\t}\n\n\tvar retType hm.Type\n\tif len(axes) == d-1 { // Only 1 non-reduced dim, so we can reduce to a vector as before.\n\t\tretType = makeTensorType(1, a)\n\t} else {\n\t\tretType = t\n\t}\n\treturn hm.NewFnType(t, retType)\n}\n\nfunc reductionInferShape(along []int, in tensor.Shape) (tensor.Shape, error) {\n\tif len(along) == 0 {\n\t\treturn tensor.ScalarShape(), nil\n\t}\n\tshape := in.Clone()\n\tfor _, d := range along {\n\t\tif d >= shape.Dims() {\n\t\t\treturn nil, fmt.Errorf(\"shape error, along %d is not a valid axis for shape %v\", d, in)\n\t\t}\n\t\tshape[d] = 0\n\t}\n\n\tvar dims []int\n\tfor _, d := range shape {\n\t\tif d != 0 {\n\t\t\tdims = append(dims, d)\n\t\t}\n\t}\n\tif len(dims) == 0 {\n\t\treturn tensor.ScalarShape(), nil\n\t}\n\treturn tensor.Shape(dims), nil\n}\n\nfunc reductionDo(op Op, s string, f func(*tensor.Dense, ...int) (*tensor.Dense, error), along []int, inputs ...Value) (retVal Value, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\tat := inputs[0].(tensor.Tensor)\n\tswitch t := at.(type) {\n\tcase *tensor.Dense:\n\t\tvar ret *tensor.Dense\n\t\tif ret, err = f(t, along...); err == nil {\n\t\t\tif ret.IsScalar() {\n\t\t\t\tretVal, _ = anyToScalar(ret.ScalarValue())\n\t\t\t} else {\n\t\t\t\t// the tensor reduction ops remove collapsed dimensions, but here we preserve them except in special cases.\n\t\t\t\t// so we reshape the return to ensure the dimensions match.\n\t\t\t\tvar sh tensor.Shape\n\t\t\t\tif sh, err = reductionInferShape(along, t.Shape()); err == nil {\n\t\t\t\t\tif err = ret.Reshape(sh...); err == nil {\n\t\t\t\t\t\tretVal = ret\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\treturn nil, errors.Wrap(err, fmt.Sprintf(\"failed to apply *tensor.Dense.%s()\", strings.Title(s)))\n\t\t}\n\tdefault:\n\t\treturn nil, errors.Errorf(nyiFail, fmt.Sprintf(\"%sOp.Do()\", s), at)\n\t}\n\treturn\n\n}\n\ntype maxOp struct {\n\talong axes\n\td     int\n}\n\nfunc newMaxOp(along axes, dim int) *maxOp {\n\treturn &maxOp{\n\t\talong: along,\n\t\td:     dim,\n\t}\n}\n\nfunc (op maxOp) Arity() int { return 1 }\n\nfunc (op maxOp) Type() hm.Type {\n\treturn reductionType(op.d, op.along)\n}\n\nfunc (op maxOp) InferShape(dimsizers ...DimSizer) (tensor.Shape, error) {\n\tif len(dimsizers) != 1 {\n\t\treturn nil, errors.Errorf(\"maxOp only takes one input shape to infer \")\n\t}\n\treturn reductionInferShape(op.along, dimsizers[0].(tensor.Shape))\n}\nfunc (op maxOp) DiffWRT(i int) []bool { return []bool{true} }\n\nfunc (op maxOp) SymDiff(inputs Nodes, output, gradNode *Node) (retVal Nodes, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tt := inputs[0]\n\topDim := len(t.Shape())\n\n\tvar leftAxes []byte\n\tfor i := 0; i < opDim; i++ {\n\t\tfor _, ax := range op.along {\n\t\t\tif i == ax {\n\t\t\t\tleftAxes = append(leftAxes, byte(i))\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\tvar a, b, a2, b2, eq *Node\n\tbcpat := NewBroadcastPattern(leftAxes, nil)\n\tif a, b, err = Broadcast(output, t, bcpat); err != nil {\n\t\treturn nil, errors.Wrap(err, operationError)\n\t}\n\tif eq, err = Eq(a, b, true); err != nil {\n\t\treturn nil, errors.Wrap(err, operationError)\n\t}\n\n\tif a2, b2, err = Broadcast(gradNode, eq, bcpat); err != nil {\n\t\treturn nil, errors.Wrap(err, operationError)\n\t}\n\tretVal = make(Nodes, 1)\n\tif retVal[0], err = HadamardProd(a2, b2); err != nil {\n\t\treturn nil, errors.Wrap(err, operationError)\n\t}\n\treturn\n}\n\nfunc (op maxOp) Do(inputs ...Value) (retVal Value, err error) {\n\treturn reductionDo(op, \"max\", (*tensor.Dense).Max, op.along, inputs...)\n}\n\nfunc (op maxOp) ReturnsPtr() bool     { return true }\nfunc (op maxOp) OverwritesInput() int { return 0 }\nfunc (op maxOp) CallsExtern() bool    { return false }\n\nfunc (op maxOp) WriteHash(h hash.Hash) {\n\th.Write([]byte(\"max\"))\n\tif err := binary.Write(h, binary.LittleEndian, byte(op.d)); err != nil {\n\t\tpanic(err)\n\t}\n\tfmt.Fprintf(h, \"%v->%v\", op.d, op.along)\n}\n\nfunc (op maxOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op maxOp) String() string { return fmt.Sprintf(\"MaxAlong%v\", op.along) }\nfunc (op maxOp) isUnary() bool  { return true }\n\n/* ARGMAX OP */\n// type argmaxOp struct {\n// \talong int // axis\n// }\n\n// func (op argmaxOp) Type() hm.Type {\n// \ta := hm.TypeVariable('a')\n\n// }\n\n/* SUM OP */\n\ntype sumOp struct {\n\talong      axes\n\td          int\n\tinputShape tensor.Shape\n}\n\nfunc newSumOp(along axes, s tensor.Shape, d int) sumOp {\n\treturn sumOp{\n\t\talong:      along,\n\t\td:          d,\n\t\tinputShape: s,\n\t}\n}\n\nfunc (op sumOp) Arity() int { return 1 }\n\n// sumOp is a function with this type:\n//\t\tsumOp :: (Summable a) ⇒ Tensor d a → Tensor d-1 a\nfunc (op sumOp) Type() hm.Type {\n\treturn reductionType(op.d, op.along)\n}\n\n// InferShape infers the shape of a sumOp. It's purpose is to fulfil the Op interface. Only one input is expected, and the type is expected to be a tensor.Shape\nfunc (op sumOp) InferShape(inputs ...DimSizer) (shape tensor.Shape, err error) {\n\treturn reductionInferShape(op.along, inputs[0].(tensor.Shape))\n}\n\nfunc (op sumOp) DiffWRT(i int) []bool { return []bool{true} }\n\nfunc (op sumOp) SymDiff(inputs Nodes, output, gradNode *Node) (retVal Nodes, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tnewShape := calcBroadcastShape(gradNode, op.d, op.along)\n\tif gradNode, err = Reshape(gradNode, newShape); err != nil {\n\t\treturn nil, errors.Wrapf(err, \"Unable to reshape grad node to %v\", newShape)\n\t}\n\tgradNode.setGroup(gradClust)\n\n\tchildren := make(Nodes, len(op.along)+1)\n\tchildren[0] = gradNode\n\n\tfor i, a := range op.along {\n\t\tvar n *Node\n\t\tif n, err = SizeOf(a, inputs[0]); err != nil {\n\t\t\treturn nil, errors.Wrap(err, operationError)\n\t\t}\n\t\tWithGroupName(gradClust)(n)\n\t\tchildren[i+1] = n\n\t}\n\n\tretVal = make(Nodes, 1)\n\tif retVal[0], err = repeatedApply(op.along, children); err != nil {\n\t\treturn nil, errors.Wrap(err, applyOpFail)\n\t}\n\tretVal[0].setGroup(gradClust)\n\treturn\n}\n\nfunc (op sumOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) (err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tx := inputs[0]\n\txdv, ydv := getDV(x, output)\n\txShape := xdv.Value.Shape()\n\n\tvar T tensor.Tensor\n\tswitch ydvd := ydv.d.(type) {\n\tcase Scalar:\n\t\tdt := ydvd.Dtype()\n\t\tT = tensor.New(tensor.Of(dt), tensor.WithShape(xdv.d.Shape().Clone()...))\n\t\tT.Memset(ydvd.Data())\n\tcase tensor.Tensor:\n\t\t// handle broadcasting\n\t\tif ydvd.Shape().Dims() == xdv.d.Shape().Dims()-len(op.along) {\n\t\t\tnewShape := xdv.d.Shape().Clone()\n\t\t\tfor _, a := range op.along {\n\t\t\t\tnewShape[a] = 1\n\t\t\t}\n\t\t\tydvd.Reshape(newShape...)\n\t\t}\n\n\t\tT = ydvd\n\tdefault:\n\t\terr = errors.Errorf(nyiTypeFail, \"sumOp.DoDiff()\", ydv.d)\n\t\treturn\n\t}\n\n\tvar val Value\n\tif !T.Shape().Eq(xdv.d.Shape()) {\n\t\t// TO DO: Optimize: figure out a way to bunch it all up so you can repeat in one call\n\t\tfor _, a := range op.along {\n\t\t\tif xShape[a] == 1 {\n\t\t\t\tcontinue // don't need to repeat\n\t\t\t}\n\n\t\t\tif T, err = tensor.Repeat(T, a, xShape[a]); err != nil {\n\t\t\t\treturn errors.Wrapf(err, repFail, a, xShape[a])\n\t\t\t}\n\t\t}\n\t\tval = T\n\t} else {\n\t\tval = T\n\t}\n\n\t// then just add the two\n\tadd := newEBOByType(addOpType, TypeOf(xdv.d), TypeOf(val))\n\taddOp := NewExternalOp(add, ctx, nil)\n\taddOp.UseUnsafe = true\n\taddOp.Device = x.Device()\n\n\tdev := x.Device()\n\tif output.Device() != dev && dev != CPU {\n\t\tvar valOnDev Value\n\t\tif valOnDev, err = ctx.Transfer(dev, output.Device(), val, false); err != nil {\n\t\t\treturn\n\t\t}\n\t\tdefer ctx.PutValue(dev, valOnDev)\n\t\tval = valOnDev\n\n\t\t// Copy(valOnDev, val)\n\t}\n\tvar xd, d Value\n\tvar extra bool\n\tif xd, extra, err = x.GradOnDevice(dev, ctx.External); err != nil {\n\t\treturn errors.Wrapf(err, gradOnDeviceFail, x, dev)\n\t}\n\tif extra {\n\t\tdefer ctx.PutValue(dev, xd)\n\t}\n\tif d, err = addOp.Do(xd, val); err != nil {\n\t\treturn errors.Wrapf(err, unsafeDoFail, add)\n\t}\n\n\treturn xdv.SetDeriv(d)\n\n\t// var d Value\n\t// if d, err = add.UnsafeDo(xdv.d, val); err != nil {\n\t// \treturn errors.Wrapf(err, unsafeDoFail, add)\n\t// }\n}\n\nfunc (op sumOp) Do(inputs ...Value) (retVal Value, err error) {\n\treturn reductionDo(op, \"sum\", (*tensor.Dense).Sum, op.along, inputs...)\n}\n\nfunc (op sumOp) ReturnsPtr() bool      { return true }\nfunc (op sumOp) OverwritesInput() int  { return 0 }\nfunc (op sumOp) CallsExtern() bool     { return false }\nfunc (op sumOp) WriteHash(h hash.Hash) { fmt.Fprintf(h, \"sum%v->%v\", op.along, op.inputShape) }\nfunc (op sumOp) Hashcode() uint32      { return simpleHash(op) }\nfunc (op sumOp) String() string        { return fmt.Sprintf(\"Σ%v\", op.along) }\nfunc (op sumOp) isUnary() bool         { return true }\n"
        },
        {
          "name": "op_reduction_test.go",
          "type": "blob",
          "size": 16.015625,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"runtime\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc TestSumOpGrad(t *testing.T) {\n\tt.SkipNow()\n\tassert := assert.New(t)\n\t// var g *ExprGraph\n\tvar z, sz *Node\n\tvar grads Nodes\n\tvar err error\n\tvar op sumOp\n\n\t_, _, _, z = simpleVecEqn()\n\tsz = Must(Sum(z))\n\t// t.Logf(\" %v  %v %v %v\", g, x, y, z)\n\n\tdiffWRT := sz.diffWRT()\n\tassert.Equal([]bool{true}, diffWRT)\n\n\top = sz.op.(sumOp)\n\tgrads, err = op.SymDiff(Nodes{z}, sz, onef64)\n\tassert.Nilf(err, \"Got %+v\", err)\n\tassert.Equal(1, len(grads))\n\tt.Logf(\"%v\", grads[0])\n}\n\nfunc TestSumOpFakeVec(t *testing.T) {\n\tg := NewGraph()\n\n\txv := tensor.New(tensor.WithBacking([]float64{1, 2}), tensor.WithShape(2, 1))\n\tyv := tensor.New(tensor.WithBacking([]float64{10, 20}), tensor.WithShape(1, 2))\n\tx := NewMatrix(g, Float64, WithName(\"x\"), WithShape(2, 1), WithValue(xv))\n\ty := NewMatrix(g, Float64, WithName(\"y\"), WithShape(1, 2), WithValue(yv))\n\tsx, _ := Sum(x)\n\tsy, _ := Sum(y)\n\n\tassert.True(t, sx.Shape().Eq(tensor.ScalarShape()))\n\tassert.True(t, sy.Shape().Eq(tensor.ScalarShape()))\n\n\tsx2, _ := Sum(x, 1)\n\tassert.True(t, sx2.Shape().Eq(tensor.Shape{2}))\n\n\tvm := NewTapeMachine(g)\n\tvm.RunAll()\n\n\tassert.Equal(t, 3.0, sx.Value().Data(), \"Expected sx to be 3.0\")\n\tassert.Equal(t, 30.0, sy.Value().Data(), \"Expected sy to be 30.0\")\n\tassert.Equal(t, []float64{1, 2}, sx2.Value().Data(), \"sx2 should be a flat array\")\n}\n\nfunc TestSumOpDiff(t *testing.T) {\n\tdefer runtime.GC()\n\tassert := assert.New(t)\n\tvar g, g2 *ExprGraph\n\tvar x, y, z, a, b, c *Node\n\t// var x, y, a, b *Node\n\tvar xG, yG, aG, bG Value\n\t// var xG, aG Value\n\t// var prog *program\n\t// var locMap map[*Node]register\n\tvar m *tapeMachine\n\tvar m2 *lispMachine\n\tvar err error\n\n\t// Basic Test case: a vector is summed\n\n\tg = NewGraph()\n\tx = NewVector(g, Float64, WithName(\"x\"), WithShape(5), WithInit(RangedFrom(0)))\n\ty = Must(Sum(x))\n\tWithName(\"y\")(y)\n\n\tGrad(y, x)\n\n\t// ioutil.WriteFile(\"SumOp.dot\", []byte(g.ToDot()), 0644)\n\n\tm = NewTapeMachine(g)\n\tdefer m.Close()\n\tif err = m.RunAll(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tg2 = NewGraph()\n\ta = NewVector(g2, Float64, WithShape(5), WithInit(RangedFrom(0)))\n\tb = Must(Sum(a))\n\n\tm2 = NewLispMachine(g2, WithWatchlist())\n\tdefer m2.Close()\n\tif err = m2.RunAll(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tif aG, err = a.Grad(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tif xG, err = x.Grad(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tif bG, err = b.Grad(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tif yG, err = y.Grad(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tassert.True(ValueEq(x.Value(), a.Value()))\n\tassert.True(ValueEq(xG, aG))\n\tassert.True(ValueEq(y.Value(), b.Value()))\n\tassert.True(ValueEq(yG, bG))\n\n\t// long standing bug: sometimes the derivation will get executed in the machine first\n\t// for example, the deriv of y is 1, and occasionally, the machine will choose to\n\t// execute const 1 into register 0\n\t// It would then fail to bind to y's boundTo, because at that point in time, y is still unknown.\n\n\t// assert.Equal(y.Grad(), b.Grad())\n\n\t// Slightly more advanced test case: A matrix is summed\n\tg = NewGraph()\n\tx = NewMatrix(g, Float64, WithName(\"x\"), WithShape(11, 7), WithInit(RangedFrom(0)))\n\ty = Must(Sum(x))\n\tWithName(\"y\")(y)\n\n\tGrad(y, x)\n\n\tm = NewTapeMachine(g)\n\tdefer m.Close()\n\tif err = m.RunAll(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tg2 = NewGraph()\n\ta = NewMatrix(g2, Float64, WithName(\"x\"), WithShape(11, 7), WithInit(RangedFrom(0)))\n\tb = Must(Sum(a))\n\n\tm2 = NewLispMachine(g2)\n\tdefer m2.Close()\n\tif err = m2.RunAll(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tif aG, err = a.Grad(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tif xG, err = x.Grad(); err != nil {\n\t\tt.Error(err)\n\t}\n\tif bG, err = b.Grad(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tif yG, err = y.Grad(); err != nil {\n\t\tt.Error(err)\n\t}\n\tassert.True(ValueEq(x.Value(), a.Value()))\n\tassert.True(ValueEq(xG, aG))\n\tassert.True(ValueEq(y.Value(), b.Value()))\n\tassert.True(ValueEq(yG, bG))\n\n\t/* Sum is not the root node */\n\n\tg = NewGraph()\n\tx = NewMatrix(g, Float64, WithName(\"x\"), WithShape(11, 7), WithInit(RangedFrom(0)))\n\ty = Must(Sum(x))\n\tz = Must(Add(y, twof64))\n\n\tif _, err = Grad(z, x); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tm = NewTapeMachine(g)\n\tdefer m.Close()\n\tif err = m.RunAll(); err != nil {\n\t\tt.Errorf(\"%v\", m.Prog())\n\t\tt.Error(err)\n\t}\n\n\tg2 = NewGraph()\n\ta = NewMatrix(g2, Float64, WithName(\"x\"), WithShape(11, 7), WithInit(RangedFrom(0)))\n\tb = Must(Sum(a))\n\tc = Must(Add(b, twof64))\n\n\tm2 = NewLispMachine(g2)\n\tdefer m2.Close()\n\tif err = m2.RunAll(); err != nil {\n\t\tt.Fatalf(\"%+v\", err)\n\t}\n\n\tif aG, err = a.Grad(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tif xG, err = x.Grad(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tif bG, err = b.Grad(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tif yG, err = b.Grad(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tassert.True(ValueEq(x.Value(), a.Value()))\n\tassert.True(ValueEq(xG, aG))\n\tassert.True(ValueEq(y.Value(), b.Value()))\n\tassert.True(ValueEq(yG, bG))\n\tassert.True(ValueEq(z.Value(), c.Value()))\n\n\truntime.GC()\n}\n\nfunc TestMaxOp(t *testing.T) {\n\tsubTests := []reductionTest{\n\t\t{dt: Float32, inShape: []int{3, 2}, inData: []float32{1, 2, 3, 4, 5, 6}, op: Max, along: []int{0}, wantShape: []int{2}, wantData: []float32{5, 6}},\n\t\t{dt: Float32, inShape: []int{3, 2}, inData: []float32{1, 2, 3, 4, 5, 6}, op: Max, along: []int{1}, wantShape: []int{3}, wantData: []float32{2, 4, 6}},\n\t\t{dt: Float32, inShape: []int{3, 2}, inData: []float32{1, 2, 3, 4, 5, 6}, op: Max, along: []int{}, wantShape: []int{}, wantData: float32(6)},\n\t\t{dt: Float32, inShape: []int{3, 2}, inData: []float32{1, 2, 3, 4, 5, 6}, op: Max, along: []int{0, 1}, wantShape: []int{}, wantData: float32(6)},\n\t\t{dt: Float32, inShape: []int{3, 2}, inData: []float32{1, 2, 3, 4, 5, 6}, op: Max, along: []int{1, 0}, wantShape: []int{}, wantData: float32(6)},\n\t\t//{dt: Float32, inShape: []int{1, 6}, inData: []float32{1, 2, 3, 4, 5, 6}, op: Max, along: []int{1}, wantShape: []int{}, wantData: float32(6)},\n\t\t{\n\t\t\tdt:        Float32,\n\t\t\tinShape:   []int{2, 2, 2, 2},\n\t\t\tinData:    []float32{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16},\n\t\t\top:        Max,\n\t\t\talong:     []int{0, 1, 2, 3},\n\t\t\twantShape: []int{},\n\t\t\twantData:  float32(16),\n\t\t},\n\t\t{\n\t\t\tdt:        Float32,\n\t\t\tinShape:   []int{2, 2, 2, 2},\n\t\t\tinData:    []float32{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16},\n\t\t\top:        Max,\n\t\t\talong:     []int{},\n\t\t\twantShape: []int{},\n\t\t\twantData:  float32(16),\n\t\t},\n\t\t{\n\t\t\tdt:        Float32,\n\t\t\tinShape:   []int{2, 2, 2, 2},\n\t\t\tinData:    []float32{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16},\n\t\t\top:        Max,\n\t\t\talong:     []int{0},\n\t\t\twantShape: []int{2, 2, 2},\n\t\t\twantData:  []float32{9, 10, 11, 12, 13, 14, 15, 16},\n\t\t},\n\t\t{\n\t\t\tdt:        Float32,\n\t\t\tinShape:   []int{2, 2, 2, 2},\n\t\t\tinData:    []float32{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16},\n\t\t\top:        Max,\n\t\t\talong:     []int{1},\n\t\t\twantShape: []int{2, 2, 2},\n\t\t\twantData:  []float32{5, 6, 7, 8, 13, 14, 15, 16},\n\t\t},\n\t\t{\n\t\t\tdt:        Float32,\n\t\t\tinShape:   []int{2, 2, 2, 2},\n\t\t\tinData:    []float32{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16},\n\t\t\top:        Max,\n\t\t\talong:     []int{2},\n\t\t\twantShape: []int{2, 2, 2},\n\t\t\twantData:  []float32{3, 4, 7, 8, 11, 12, 15, 16},\n\t\t},\n\t\t{\n\t\t\tdt:        Float32,\n\t\t\tinShape:   []int{2, 2, 2, 2},\n\t\t\tinData:    []float32{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16},\n\t\t\top:        Max,\n\t\t\talong:     []int{3},\n\t\t\twantShape: []int{2, 2, 2},\n\t\t\twantData:  []float32{2, 4, 6, 8, 10, 12, 14, 16},\n\t\t},\n\t\t{\n\t\t\tdt:        Float32,\n\t\t\tinShape:   []int{2, 2, 2, 2},\n\t\t\tinData:    []float32{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16},\n\t\t\top:        Max,\n\t\t\talong:     []int{1, 3},\n\t\t\twantShape: []int{2, 2},\n\t\t\twantData:  []float32{6, 8, 14, 16},\n\t\t},\n\t\t{\n\t\t\tdt:        Float32,\n\t\t\tinShape:   []int{2, 2, 2, 2},\n\t\t\tinData:    []float32{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16},\n\t\t\top:        Max,\n\t\t\talong:     []int{0, 2, 3},\n\t\t\twantShape: []int{2},\n\t\t\twantData:  []float32{12, 16},\n\t\t},\n\t}\n\n\tfor _, subTest := range subTests {\n\t\tt.Run(fmt.Sprintf(\"along %v\", subTest.along), func(t *testing.T) {\n\t\t\ttestReductionOp(t, subTest)\n\t\t})\n\t}\n}\n\nfunc TestSumOp(t *testing.T) {\n\tsubTests := []reductionTest{\n\t\t{dt: Float32, inShape: []int{3, 2}, inData: []float32{1, 2, 3, 4, 5, 6}, op: Sum, along: []int{0}, wantShape: []int{2}, wantData: []float32{9, 12}},\n\t\t{dt: Float32, inShape: []int{3, 2}, inData: []float32{1, 2, 3, 4, 5, 6}, op: Sum, along: []int{1}, wantShape: []int{3}, wantData: []float32{3, 7, 11}},\n\t\t{dt: Float32, inShape: []int{3, 2}, inData: []float32{1, 2, 3, 4, 5, 6}, op: Sum, along: []int{}, wantShape: []int{}, wantData: float32(21)},\n\t\t{dt: Float32, inShape: []int{3, 2}, inData: []float32{1, 2, 3, 4, 5, 6}, op: Sum, along: []int{0, 1}, wantShape: []int{}, wantData: float32(21)},\n\t\t{dt: Float32, inShape: []int{3, 2}, inData: []float32{1, 2, 3, 4, 5, 6}, op: Sum, along: []int{1, 0}, wantShape: []int{}, wantData: float32(21)},\n\t\t{\n\t\t\tdt:        Float32,\n\t\t\tinShape:   []int{2, 2, 2, 2},\n\t\t\tinData:    []float32{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16},\n\t\t\top:        Sum,\n\t\t\talong:     []int{0, 1, 2, 3},\n\t\t\twantShape: []int{},\n\t\t\twantData:  float32(136),\n\t\t},\n\t\t{\n\t\t\tdt:        Float32,\n\t\t\tinShape:   []int{2, 2, 2, 2},\n\t\t\tinData:    []float32{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16},\n\t\t\top:        Sum,\n\t\t\talong:     []int{},\n\t\t\twantShape: []int{},\n\t\t\twantData:  float32(136),\n\t\t},\n\t\t{\n\t\t\tdt:        Float32,\n\t\t\tinShape:   []int{2, 2, 2, 2},\n\t\t\tinData:    []float32{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16},\n\t\t\top:        Sum,\n\t\t\talong:     []int{0},\n\t\t\twantShape: []int{2, 2, 2},\n\t\t\twantData:  []float32{10, 12, 14, 16, 18, 20, 22, 24},\n\t\t},\n\t\t{\n\t\t\tdt:        Float32,\n\t\t\tinShape:   []int{2, 2, 2, 2},\n\t\t\tinData:    []float32{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16},\n\t\t\top:        Sum,\n\t\t\talong:     []int{1},\n\t\t\twantShape: []int{2, 2, 2},\n\t\t\twantData:  []float32{6, 8, 10, 12, 22, 24, 26, 28},\n\t\t},\n\t\t{\n\t\t\tdt:        Float32,\n\t\t\tinShape:   []int{2, 2, 2, 2},\n\t\t\tinData:    []float32{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16},\n\t\t\top:        Sum,\n\t\t\talong:     []int{2},\n\t\t\twantShape: []int{2, 2, 2},\n\t\t\twantData:  []float32{4, 6, 12, 14, 20, 22, 28, 30},\n\t\t},\n\t\t{\n\t\t\tdt:        Float32,\n\t\t\tinShape:   []int{2, 2, 2, 2},\n\t\t\tinData:    []float32{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16},\n\t\t\top:        Sum,\n\t\t\talong:     []int{3},\n\t\t\twantShape: []int{2, 2, 2},\n\t\t\twantData:  []float32{3, 7, 11, 15, 19, 23, 27, 31},\n\t\t},\n\t\t{\n\t\t\tdt:        Float32,\n\t\t\tinShape:   []int{2, 2, 2, 2},\n\t\t\tinData:    []float32{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16},\n\t\t\top:        Sum,\n\t\t\talong:     []int{1, 3},\n\t\t\twantShape: []int{2, 2},\n\t\t\twantData:  []float32{14, 22, 46, 54},\n\t\t},\n\t\t{\n\t\t\tdt:        Float32,\n\t\t\tinShape:   []int{2, 2, 2, 2},\n\t\t\tinData:    []float32{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16},\n\t\t\top:        Sum,\n\t\t\talong:     []int{0, 2, 3},\n\t\t\twantShape: []int{2},\n\t\t\twantData:  []float32{52, 84},\n\t\t},\n\t}\n\n\tfor _, subTest := range subTests {\n\t\tt.Run(fmt.Sprintf(\"along %v\", subTest.along), func(t *testing.T) {\n\t\t\ttestReductionOp(t, subTest)\n\t\t})\n\t}\n}\n\ntype reductionTest struct {\n\tdt        tensor.Dtype\n\tinShape   tensor.Shape\n\tinData    interface{}\n\top        func(*Node, ...int) (*Node, error)\n\talong     []int\n\twantShape tensor.Shape\n\twantData  interface{}\n}\n\nfunc testReductionOp(t *testing.T, test reductionTest) {\n\tg := NewGraph()\n\tXn := NewTensor(g, test.dt, len(test.inShape), WithShape(test.inShape...))\n\tgot := Must(test.op(Xn, test.along...))\n\n\txT := tensor.New(tensor.WithShape(test.inShape...), tensor.WithBacking(test.inData))\n\tvm := NewTapeMachine(g)\n\tdefer vm.Close()\n\tvm.Let(Xn, xT)\n\terr := vm.RunAll()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tassert := assert.New(t)\n\tassert.Equal(test.wantShape, got.Value().Shape(), \"shape mismatch\")\n\tassert.Equal(test.wantData, got.Value().Data(), \"data mismatch\")\n}\n\nfunc TestMaxOpGrad(t *testing.T) {\n\tsubTests := []reductionGradTest{\n\t\t{\n\t\t\tdt:           Float64,\n\t\t\tinShape:      tensor.Shape{6},\n\t\t\tinData:       []float64{1, 2, 3, 4, 5, 6},\n\t\t\top:           Max,\n\t\t\talong:        []int{},\n\t\t\toutGradShape: tensor.Shape{1},\n\t\t\toutGrad:      []float64{1},\n\t\t\twantInGrad:   []float64{0, 0, 0, 0, 0, 1},\n\t\t},\n\t\t{\n\t\t\tdt:           Float32,\n\t\t\tinShape:      tensor.Shape{6},\n\t\t\tinData:       []float32{1, 2, 3, 4, 5, 6},\n\t\t\top:           Max,\n\t\t\talong:        []int{0},\n\t\t\toutGradShape: tensor.Shape{1},\n\t\t\toutGrad:      []float32{1},\n\t\t\twantInGrad:   []float32{0, 0, 0, 0, 0, 1},\n\t\t},\n\t\t{\n\t\t\tdt:           Float32,\n\t\t\tinShape:      tensor.Shape{6},\n\t\t\tinData:       []float32{1, 2, 3, 4, 5, 6},\n\t\t\top:           Max,\n\t\t\talong:        []int{},\n\t\t\toutGradShape: tensor.Shape{1},\n\t\t\toutGrad:      []float32{1},\n\t\t\twantInGrad:   []float32{0, 0, 0, 0, 0, 1},\n\t\t},\n\t\t{\n\t\t\tdt:           Float32,\n\t\t\tinShape:      tensor.Shape{3, 2},\n\t\t\tinData:       []float32{1, 2, 3, 4, 5, 6},\n\t\t\top:           Max,\n\t\t\talong:        []int{0},\n\t\t\toutGradShape: tensor.Shape{2},\n\t\t\toutGrad:      []float32{0.2, 0.8},\n\t\t\twantInGrad:   []float32{0, 0, 0, 0, 0.2, 0.8},\n\t\t},\n\t\t{\n\t\t\tdt:           Float32,\n\t\t\tinShape:      tensor.Shape{3, 2},\n\t\t\tinData:       []float32{1, 2, 3, 4, 5, 6},\n\t\t\top:           Max,\n\t\t\talong:        []int{1},\n\t\t\toutGradShape: tensor.Shape{3},\n\t\t\toutGrad:      []float32{0.1, 0.3, 0.6},\n\t\t\twantInGrad:   []float32{0, 0.1, 0, 0.3, 0, 0.6},\n\t\t},\n\t\t{\n\t\t\tdt:           Float32,\n\t\t\tinShape:      tensor.Shape{3, 2},\n\t\t\tinData:       []float32{1, 2, 3, 4, 5, 6},\n\t\t\top:           Max,\n\t\t\talong:        []int{0, 1},\n\t\t\toutGradShape: tensor.Shape{1},\n\t\t\toutGrad:      []float32{1},\n\t\t\twantInGrad:   []float32{0, 0, 0, 0, 0, 1},\n\t\t},\n\t\t//{\n\t\t//\tdt:           Float32,\n\t\t//\tinShape:      tensor.Shape{1, 6},\n\t\t//\tinData:       []float32{1, 2, 3, 4, 5, 6},\n\t\t//\top:           Max,\n\t\t//\talong:        []int{1},\n\t\t//\toutGradShape: tensor.Shape{6},\n\t\t//\toutGrad:      []float32{1},\n\t\t//\twantInGrad:   []float32{0, 0, 0, 0, 0, 1},\n\t\t//},\n\t}\n\n\tfor _, subTest := range subTests {\n\t\tt.Run(fmt.Sprintf(\"%v along %v %v\", subTest.inShape, subTest.along, subTest.dt), func(t *testing.T) {\n\t\t\ttestReductionOpGrad(t, subTest)\n\t\t})\n\t}\n}\n\ntype reductionGradTest struct {\n\tdt           tensor.Dtype\n\tinShape      tensor.Shape\n\tinData       interface{}\n\top           func(*Node, ...int) (*Node, error)\n\talong        []int\n\toutGradShape tensor.Shape\n\toutGrad      interface{}\n\twantInGrad   interface{}\n}\n\nfunc testReductionOpGrad(t *testing.T, test reductionGradTest) {\n\tassert := assert.New(t)\n\n\tvar xG Value\n\tvar err error\n\n\t// Run op\n\tg := NewGraph()\n\txN := NewTensor(g, test.dt, len(test.inShape), WithShape(test.inShape...))\n\ty := Must(test.op(xN, test.along...))\n\n\toutGrad := NewTensor(g, test.dt, len(test.outGradShape), WithValue(tensor.New(tensor.WithShape(test.outGradShape...), tensor.WithBacking(test.outGrad))))\n\tif _, err = Backpropagate(Nodes{y}, Nodes{outGrad}, Nodes{xN}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\txT := tensor.New(tensor.WithShape(test.inShape...), tensor.WithBacking(test.inData))\n\tvm := NewTapeMachine(g)\n\tdefer vm.Close()\n\tvm.Let(xN, xT)\n\tif err = vm.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Test grad functions\n\tdiffWRT := y.diffWRT()\n\tassert.Equal([]bool{true}, diffWRT)\n\n\tif xG, err = xN.Grad(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tassert.Equal(test.inShape, xG.Shape(), \"grad shape mismatch\")\n\tassert.Equal(test.wantInGrad, xG.Data(), \"grad data mismatch\")\n}\n\n// TestFollowupOp confirms that an element-wise binary op will work as expected after a sum/max.\n// The underlying reduction on the tensor changes the number of dimensions, but the gorgonia node does not.\n// We therefore confirm that the resulting nodes actually work.\nfunc TestFollowupOp(t *testing.T) {\n\tg := NewGraph()\n\tXn := NewTensor(g, tensor.Float64, 4, WithShape(2, 2, 2, 2), WithInit(RangedFrom(1)))\n\tmx := Must(Max(Xn, 1, 2))\n\tsx := Must(Sum(Xn, 1, 2))\n\ty := NewTensor(g, tensor.Float64, 2, WithShape(2, 2), WithInit(RangedFrom(1)))\n\n\tamx := Must(Add(mx, y))\n\tasx := Must(Add(sx, y))\n\tassert.Equal(t, amx.Shape(), tensor.Shape{2, 2})\n\tassert.Equal(t, asx.Shape(), tensor.Shape{2, 2})\n\tvm := NewTapeMachine(g)\n\tdefer vm.Close()\n\terr := vm.RunAll()\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tassert.Equal(t, []float64{8, 10, 18, 20}, amx.Value().Data(), \"data mismatch\")\n\tassert.Equal(t, []float64{17, 22, 51, 56}, asx.Value().Data(), \"data mismatch\")\n}\n"
        },
        {
          "name": "op_softmax.go",
          "type": "blob",
          "size": 6.9931640625,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"hash\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\n// Applies SoftMax to the input x\nfunc SoftMax(x *Node, axes ...int) (*Node, error) {\n\top := newSoftmaxOp(x.Shape(), axes...)\n\n\treturn ApplyOp(op, x)\n}\n\n// Applies LogSoftMax to the input x\nfunc LogSoftMax(x *Node, axes ...int) (*Node, error) {\n\top := newSoftmaxOp(x.Shape(), axes...)\n\top.isLog = true\n\n\treturn ApplyOp(op, x)\n}\n\ntype softmaxOp struct {\n\tshape tensor.Shape\n\taxis  int\n\tisLog bool\n}\n\nfunc newSoftmaxOp(inputShape tensor.Shape, axes ...int) *softmaxOp {\n\taxis := -1\n\tif len(axes) > 0 {\n\t\taxis = axes[0]\n\t}\n\tsoftmaxop := &softmaxOp{\n\t\tshape: inputShape,\n\t\taxis:  axis,\n\t}\n\n\treturn softmaxop\n}\n\nfunc (op *softmaxOp) Arity() int { return 1 }\n\nfunc (op *softmaxOp) ReturnsPtr() bool { return false }\n\nfunc (op *softmaxOp) CallsExtern() bool { return false }\n\nfunc (op *softmaxOp) WriteHash(h hash.Hash) { fmt.Fprintf(h, \"Softmax{%v}()\", op.axis) }\n\nfunc (op *softmaxOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op *softmaxOp) String() string { return fmt.Sprintf(\"Softmax{%d, %v}()\", op.axis, op.isLog) }\n\nfunc (op *softmaxOp) InferShape(inputs ...DimSizer) (tensor.Shape, error) {\n\treturn inputs[0].(tensor.Shape), nil\n}\n\nfunc (op *softmaxOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\treturn hm.NewFnType(a, a) // f(float64) float64\n}\n\nfunc (op *softmaxOp) OverwritesInput() int { return -1 }\n\nfunc (op *softmaxOp) checkInput(inputs ...Value) (tensor.Tensor, error) {\n\tif err := checkArity(op, len(inputs)); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\tin tensor.Tensor\n\t\tok bool\n\t)\n\n\tif in, ok = inputs[0].(tensor.Tensor); !ok {\n\t\treturn nil, errors.Errorf(\"Expected input to be a tensor\")\n\t}\n\n\treturn in, nil\n}\n\nfunc (op *softmaxOp) Do(inputs ...Value) (retVal Value, err error) {\n\tinputTensor, err := op.checkInput(inputs...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Can't check Softmax input: %w\", err)\n\t}\n\n\taShape := inputTensor.Shape()\n\tret := tensor.New(tensor.WithShape(aShape.Clone()...), tensor.Of(inputTensor.Dtype()))\n\n\treturn op.UsePreallocDo(ret, inputTensor)\n\n}\n\nfunc (op *softmaxOp) UsePreallocDo(prealloc Value, inputs ...Value) (Value, error) {\n\tinputTensor, err := op.checkInput(inputs...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Can't check Softmax input: %w\", err)\n\t}\n\n\taShape := inputTensor.Shape()\n\taxis := aShape.Dims() - 1 // default: last dim\n\n\tif aShape.IsColVec() || (aShape.IsVector() && !aShape.IsRowVec()) {\n\t\taxis = 0\n\t}\n\tif op.axis != -1 {\n\t\taxis = op.axis\n\t}\n\n\tif !op.isLog {\n\t\t_, err = tensor.SoftMax(inputTensor, axis, tensor.WithReuse(prealloc.(tensor.Tensor)), tensor.UseUnsafe())\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else {\n\t\t_, err = tensor.LogSoftMax(inputTensor, axis, tensor.WithReuse(prealloc.(tensor.Tensor)), tensor.UseUnsafe())\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn prealloc, nil\n}\n\n// DoDiff calculates the diff and sets its value to the output node. Implementation for ADOp interface.\nfunc (op *softmaxOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) error {\n\tif len(inputs) != 1 {\n\t\treturn fmt.Errorf(\"SoftmaxOp.DoDiff needs 1 arguments\")\n\t}\n\n\todv := output.boundTo.(*dualValue)\n\tidv := inputs[0].boundTo.(*dualValue)\n\tidvd := idv.d.(*tensor.Dense)\n\tdiffOp := &softmaxDiffOp{op}\n\n\tresult, err := diffOp.Do(idv.Value, odv.Value, odv.d)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tsum, err := idvd.Add(result.(*tensor.Dense), tensor.UseUnsafe())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\todv.d = sum\n\n\treturn nil\n}\n\n// SymDiff applies the diff op. Implementation for SDOp interface.\nfunc (op *softmaxOp) SymDiff(inputs Nodes, output, grad *Node) (Nodes, error) {\n\terr := checkArity(op, len(inputs))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tdiffOp := &softmaxDiffOp{op}\n\tnodes := make(Nodes, 1)\n\n\tnodes[0], err = ApplyOp(diffOp, inputs[0], output, grad)\n\n\treturn nodes, err\n}\n\n// DiffWRT is an implementation for the SDOp interface\nfunc (op *softmaxOp) DiffWRT(inputs int) []bool {\n\tif inputs != 1 {\n\t\tpanic(fmt.Sprintf(\"softmax operator only supports one input, got %d instead\", inputs))\n\t}\n\n\treturn []bool{true}\n}\n\ntype softmaxDiffOp struct {\n\t*softmaxOp\n}\n\nfunc (op *softmaxDiffOp) Arity() int { return 3 }\n\nfunc (op *softmaxDiffOp) ReturnsPtr() bool { return false }\n\nfunc (op *softmaxDiffOp) CallsExtern() bool { return false }\n\nfunc (op *softmaxDiffOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, \"SoftmaxDiff{%d, %v}()\", op.axis, op.isLog)\n}\n\nfunc (op *softmaxDiffOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op *softmaxDiffOp) String() string {\n\treturn fmt.Sprintf(\"SoftmaxDiff{%d, %v}()\", op.axis, op.isLog)\n}\n\nfunc (op *softmaxDiffOp) InferShape(inputs ...DimSizer) (tensor.Shape, error) {\n\ts := inputs[0].(tensor.Shape).Clone()\n\n\treturn s, nil\n}\n\nfunc (op *softmaxDiffOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\n\treturn hm.NewFnType(a, a, a, a) // f(float64) float64\n}\n\nfunc (op *softmaxDiffOp) OverwritesInput() int { return -1 }\n\nfunc (op *softmaxDiffOp) checkInput(inputs ...Value) (tensor.Tensor, tensor.Tensor, tensor.Tensor, error) {\n\tif err := checkArity(op, len(inputs)); err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\n\tvar (\n\t\tin   tensor.Tensor\n\t\tout  tensor.Tensor\n\t\tgrad tensor.Tensor\n\t\tok   bool\n\t)\n\n\tswitch t := inputs[0].(type) {\n\tcase *dualValue:\n\t\tif in, ok = t.Value.(tensor.Tensor); !ok {\n\t\t\treturn nil, nil, nil, errors.Errorf(\"input should be a tensor, got %T\", inputs[0])\n\t\t}\n\tcase tensor.Tensor:\n\t\tin = t\n\tdefault:\n\t\treturn nil, nil, nil, errors.Errorf(\"input type is not supported, got %T\", inputs[0])\n\t}\n\n\tswitch t := inputs[1].(type) {\n\tcase *dualValue:\n\t\tif out, ok = t.Value.(tensor.Tensor); !ok {\n\t\t\treturn nil, nil, nil, errors.Errorf(\"output should be a tensor, got %T\", inputs[1])\n\t\t}\n\tcase tensor.Tensor:\n\t\tout = t\n\tdefault:\n\t\treturn nil, nil, nil, errors.Errorf(\"output type is not supported, got %T\", inputs[1])\n\t}\n\n\tswitch t := inputs[2].(type) {\n\tcase *dualValue:\n\t\tif grad, ok = t.Value.(tensor.Tensor); !ok {\n\t\t\treturn nil, nil, nil, errors.Errorf(\"grad should be a tensor, got %T\", inputs[1])\n\t\t}\n\tcase tensor.Tensor:\n\t\tgrad = t\n\tdefault:\n\t\treturn nil, nil, nil, errors.Errorf(\"grad type is not supported, got %T\", inputs[1])\n\t}\n\n\treturn in, out, grad, nil\n}\n\nfunc (op *softmaxDiffOp) Do(inputs ...Value) (Value, error) {\n\tx, y, grad, err := op.checkInput(inputs...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Can't check SoftmaxDiff input: %w\", err)\n\t}\n\n\tret := tensor.New(tensor.WithShape(y.Shape().Clone()...), tensor.Of(y.Dtype()))\n\n\treturn op.UsePreallocDo(ret, x, y, grad)\n\n}\n\nfunc (op *softmaxDiffOp) UsePreallocDo(prealloc Value, inputs ...Value) (Value, error) {\n\t_, y, grad, err := op.checkInput(inputs...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Can't check SoftmaxDiff input: %w\", err)\n\t}\n\n\tif op.isLog {\n\t\treturn tensor.LogSoftMaxB(y, grad, op.axis, tensor.WithReuse(prealloc.(tensor.Tensor)), tensor.UseUnsafe())\n\t}\n\n\treturn tensor.SoftMaxB(y, grad, op.axis, tensor.WithReuse(prealloc.(tensor.Tensor)), tensor.UseUnsafe())\n}\n\n// ensure it complies with the Op interface\nvar (\n\t_ Op   = &softmaxOp{}\n\t_ ADOp = &softmaxOp{}\n\t_ SDOp = &softmaxOp{}\n\n\t_ Op = &softmaxDiffOp{}\n)\n"
        },
        {
          "name": "op_softmax_test.go",
          "type": "blob",
          "size": 21.6376953125,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"testing\"\n\n\t\"github.com/pkg/errors\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"gorgonia.org/tensor\"\n)\n\nvar testCasesSoftMaxDo = []struct {\n\tinput    []float64\n\tshape    tensor.Shape\n\texpected []float64\n}{\n\t{\n\t\t[]float64{0.2094, -1.0, 0.6411, 0.0, -0.3909}, tensor.Shape{5}, []float64{0.2382105379413429, 0.07107636737487558, 0.36681399568548617, 0.19320559786800362, 0.13069350113029174},\n\t},\n\t{\n\t\t[]float64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, tensor.Shape{10}, []float64{7.801341612780742e-05, 0.00021206245143623275, 0.0005764455082375902, 0.0015669413501390804, 0.004259388198344144, 0.0115782175399118, 0.031472858344688034, 0.08555209892803112, 0.23255471590259755, 0.6321492583604866},\n\t},\n\t{\n\t\t[]float64{0.1, 0.1, 0.1}, tensor.Shape{3}, []float64{0.3333333333333333, 0.3333333333333333, 0.3333333333333333},\n\t},\n\t{\n\t\t[]float64{-0.1, 0.3, -1.1, 2.7}, tensor.Shape{4}, []float64{0.05180179352659075, 0.07727919496508177, 0.019056814854240642, 0.8518621966540868},\n\t},\n\t{\n\t\tinput: []float64{\n\t\t\t0.0878, 0.0238, -0.034, -0.0281, 0.0148, 0.219, -0.0949, 0.0735, -0.0224, 0.00209, -0.07, 0.0604, -0.0134, 0.132,\n\t\t\t0.0832, 0.0226, -0.0318, -0.0277, 0.0171, 0.213, -0.0934, 0.0722, -0.0233, 0.000468, -0.0682, 0.0598, -0.014, 0.128,\n\t\t\t0.0839, 0.023, -0.0331, -0.0261, 0.0139, 0.207, -0.0908, 0.0701, -0.0206, 0.00072, -0.0662, 0.0579, -0.0133, 0.125,\n\t\t\t0.0871, 0.0234, -0.0344, -0.0289, 0.0173, 0.224, -0.0961, 0.075, -0.0247, 0.00173, -0.0712, 0.0615, -0.0151, 0.135,\n\t\t\t0.0881, 0.0241, -0.0334, -0.0291, 0.0174, 0.223, -0.0977, 0.0759, -0.0238, 0.000915, -0.0708, 0.0626, -0.014, 0.135,\n\t\t\t0.0862, 0.0241, -0.0335, -0.0275, 0.0158, 0.216, -0.0938, 0.0727, -0.0227, 0.00159, -0.0699, 0.06, -0.0147, 0.131,\n\t\t\t0.0896, 0.0243, -0.0342, -0.0288, 0.0164, 0.224, -0.0973, 0.0755, -0.0232, 0.00216, -0.071, 0.0622, -0.0135, 0.136,\n\t\t\t0.0926, 0.0253, -0.0356, -0.0297, 0.0174, 0.234, -0.101, 0.079, -0.0244, 0.00166, -0.0744, 0.065, -0.0147, 0.142,\n\t\t\t0.0917, 0.0251, -0.0347, -0.0291, 0.0166, 0.229, -0.1, 0.0779, -0.0233, 0.00135, -0.0731, 0.0643, -0.0139, 0.139,\n\t\t\t0.0908, 0.0249, -0.0345, -0.0296, 0.0184, 0.232, -0.1, 0.0783, -0.0249, 0.0015, -0.074, 0.0644, -0.0148, 0.14,\n\t\t\t0.0866, 0.0232, -0.0339, -0.0267, 0.0149, 0.216, -0.094, 0.073, -0.0213, 0.00114, -0.0682, 0.06, -0.0128, 0.131,\n\t\t\t0.0836, 0.0232, -0.0329, -0.0279, 0.017, 0.215, -0.0929, 0.0719, -0.0238, 0.00139, -0.0692, 0.0597, -0.0151, 0.13,\n\t\t\t0.0921, 0.025, -0.0353, -0.0295, 0.0174, 0.233, -0.101, 0.0788, -0.0243, 0.00159, -0.0741, 0.0647, -0.0144, 0.141,\n\t\t\t0.086, 0.0232, -0.0334, -0.027, 0.0149, 0.214, -0.0927, 0.0722, -0.0216, 0.00198, -0.068, 0.0589, -0.0133, 0.13,\n\t\t\t0.0893, 0.0239, -0.0332, -0.0287, 0.0172, 0.224, -0.0978, 0.0763, -0.0234, 0.00142, -0.0714, 0.0627, -0.0132, 0.136,\n\t\t\t0.0891, 0.0242, -0.0348, -0.029, 0.0171, 0.226, -0.0981, 0.0766, -0.0241, 0.00134, -0.0721, 0.0628, -0.0148, 0.137,\n\t\t\t0.0869, 0.0231, -0.0339, -0.0275, 0.0155, 0.218, -0.0945, 0.0738, -0.022, 0.00168, -0.0691, 0.0598, -0.0133, 0.132,\n\t\t\t0.084, 0.023, -0.0311, -0.0275, 0.0178, 0.215, -0.0941, 0.0726, -0.0237, 0.000525, -0.069, 0.0602, -0.0134, 0.131,\n\t\t\t0.0873, 0.0231, -0.0336, -0.0285, 0.0166, 0.221, -0.0958, 0.0746, -0.0239, 0.00139, -0.0703, 0.0615, -0.0137, 0.134,\n\t\t\t0.0861, 0.0231, -0.0325, -0.0283, 0.0174, 0.219, -0.0959, 0.0741, -0.0238, 0.00106, -0.0695, 0.0611, -0.0138, 0.133,\n\t\t\t0.0843, 0.0228, -0.0322, -0.0272, 0.0141, 0.21, -0.0917, 0.0712, -0.0214, 0.000737, -0.0674, 0.0589, -0.0131, 0.127,\n\t\t\t0.0857, 0.0228, -0.0316, -0.0282, 0.0156, 0.214, -0.0939, 0.0729, -0.0225, 0.000756, -0.0681, 0.0602, -0.0128, 0.129,\n\t\t\t0.0894, 0.0244, -0.0353, -0.0283, 0.0164, 0.226, -0.098, 0.0765, -0.0232, 0.00113, -0.0721, 0.0628, -0.0147, 0.137,\n\t\t\t0.0792, 0.0213, -0.0316, -0.0246, 0.0139, 0.2, -0.0862, 0.0661, -0.021, 0.00137, -0.0644, 0.0545, -0.0142, 0.121,\n\t\t\t0.0847, 0.0231, -0.0323, -0.0266, 0.0159, 0.213, -0.0924, 0.072, -0.0218, 0.00133, -0.0685, 0.0588, -0.0133, 0.129,\n\t\t\t0.0842, 0.0223, -0.0332, -0.0266, 0.014, 0.21, -0.0914, 0.0708, -0.0216, 0.000902, -0.0666, 0.0587, -0.0133, 0.126,\n\t\t\t0.0914, 0.0248, -0.0355, -0.029, 0.0169, 0.231, -0.0997, 0.078, -0.0238, 0.00171, -0.0735, 0.0637, -0.0144, 0.14,\n\t\t\t0.0851, 0.023, -0.0327, -0.028, 0.0175, 0.22, -0.0947, 0.0736, -0.0243, 0.00106, -0.0704, 0.0605, -0.0146, 0.133,\n\t\t\t0.0856, 0.0231, -0.0332, -0.0271, 0.0166, 0.217, -0.0941, 0.0734, -0.0227, 0.00107, -0.069, 0.0601, -0.0139, 0.132,\n\t\t\t0.0825, 0.0223, -0.0329, -0.026, 0.0151, 0.209, -0.0896, 0.0692, -0.0221, 0.00171, -0.0669, 0.0568, -0.0145, 0.126,\n\t\t\t0.0856, 0.0232, -0.0322, -0.0287, 0.0172, 0.218, -0.095, 0.074, -0.024, 0.00106, -0.07, 0.0606, -0.0141, 0.132,\n\t\t\t0.0819, 0.0224, -0.0316, -0.0255, 0.015, 0.205, -0.0903, 0.0691, -0.0205, 0.000611, -0.0651, 0.0577, -0.0123, 0.124,\n\t\t},\n\t\tshape: tensor.Shape{32, 14},\n\t\texpected: []float64{\n\t\t\t0.07580373195832904, 0.07110427955999997, 0.06711097052129973, 0.06750809561440703, 0.07046721214751539, 0.08643109662652378, 0.06314587498214086, 0.07472745233129745, 0.06789399051506614, 0.06957724162455223, 0.06473794629960379, 0.07375490678724296, 0.06850779440403143, 0.07922940662799016,\n\t\t\t0.07553698070492218, 0.07109537937458059, 0.0673311072133112, 0.06760773144505652, 0.07070542813692551, 0.08600645460624466, 0.06330867382399817, 0.07471062719385999, 0.06790586086716144, 0.0695391808527201, 0.06492432409851422, 0.07378993550224344, 0.06854033108680987, 0.07899798509365204,\n\t\t\t0.07563735949020686, 0.07116850209805829, 0.06728587505246605, 0.06775852853502212, 0.0705238065426734, 0.0855456656279284, 0.06351336360776534, 0.07460073310252557, 0.06813222717118268, 0.06960040137967038, 0.06509516878658524, 0.0736961334366599, 0.06863141223823133, 0.07881082293102451,\n\t\t\t0.07571828316350099, 0.07104543859810176, 0.06705543414453265, 0.0674252551077198, 0.07061338053947136, 0.08682717533785694, 0.06304316513167385, 0.07480761260501406, 0.06770903670336426, 0.06952244517535047, 0.06463264686736289, 0.07380449610600028, 0.06836217349623949, 0.07943345702381124,\n\t\t\t0.07577489761197814, 0.07107723279896462, 0.06710557139359428, 0.06739474663177814, 0.07060260711076131, 0.08671848536522016, 0.06292648068737672, 0.0748560601661504, 0.0677528870226157, 0.06944826394751061, 0.06464217565864186, 0.07386706595599397, 0.06842012946324129, 0.0794133961861729,\n\t\t\t0.07571667561503748, 0.07115769153033254, 0.06717481642562362, 0.0675790768927982, 0.07056952695018931, 0.0862110553459827, 0.06324388364833107, 0.07470136923223723, 0.06790423621996317, 0.06957382518787383, 0.06477362000090751, 0.07375866071284001, 0.06844964885138428, 0.07918591338649904,\n\t\t\t0.07586555623009507, 0.07106982018973229, 0.0670315079520724, 0.06739445717595188, 0.07051058051545611, 0.08677883684519459, 0.06293250335813079, 0.07480335798280527, 0.06777292485658129, 0.06951362501868301, 0.06460958515479787, 0.07381506007103907, 0.06843352093907455, 0.07946866371038583,\n\t\t\t0.07598825125930292, 0.07104253197884605, 0.066845149278942, 0.06724070139098821, 0.07048350703214519, 0.08752975035591505, 0.06261336411851758, 0.07496180668615737, 0.06759802317465682, 0.06938278206204422, 0.06430122868181473, 0.07391965348672717, 0.06825691445591649, 0.07983633603802633,\n\t\t\t0.0759650476816685, 0.07107057060457, 0.06694513046425878, 0.06732107485668994, 0.07046903091985701, 0.08714499456226771, 0.06271328678119782, 0.07492393025637709, 0.06771267162370743, 0.06940253092966628, 0.06442316900544866, 0.07391186246516075, 0.06835217167838212, 0.07964452817074806,\n\t\t\t0.07588261555515456, 0.07104316244539605, 0.06694608634866177, 0.06727492717383647, 0.07058287942988004, 0.08739059027411504, 0.06270164065844434, 0.07493998656568233, 0.06759186354860738, 0.06940005181302644, 0.06435326134422872, 0.07390552690268615, 0.0682780005294755, 0.07970940741080541,\n\t\t\t0.07572849056812758, 0.07107633577009935, 0.0671315717478957, 0.06761666329846892, 0.07048884364820387, 0.08619002495303499, 0.06321581145869905, 0.0747055548262468, 0.06798278090816147, 0.06952555975158711, 0.06486800099339428, 0.07374066796687649, 0.06856309739697362, 0.07916659671223078,\n\t\t\t0.07555205748210254, 0.07112379296811323, 0.06724360504758324, 0.06758066502054558, 0.07068418963025343, 0.08616136899703665, 0.06332764234416066, 0.07467324946146231, 0.0678583145397035, 0.06958937667642051, 0.06484643405936474, 0.07376777047080536, 0.06845125743782313, 0.07914027586462503,\n\t\t\t0.07596459029290821, 0.07103461649660746, 0.06687781611862521, 0.06726683451491486, 0.07049679970373515, 0.08745875532447635, 0.0626251725389753, 0.0749609502427929, 0.06761753308042232, 0.06939100960430751, 0.06433265232214526, 0.07391141743871536, 0.06829027121711537, 0.07977158110425897,\n\t\t\t0.0757076953146851, 0.07109946489485341, 0.06718700230148886, 0.06761837804617317, 0.07051178159570559, 0.08604580843717952, 0.06331864342633538, 0.07467010495930128, 0.0679845049405494, 0.06960662925274143, 0.06490208896853973, 0.07368356757926006, 0.06855112455007047, 0.07911320573311643,\n\t\t\t0.07583718024893207, 0.07103613398831786, 0.06709360118049353, 0.06739620272864916, 0.0705617827417398, 0.08677240678919816, 0.0628963841896962, 0.07485767746839475, 0.0677543508572872, 0.06945705701975846, 0.06457896102358412, 0.07384650461571897, 0.06844898183156023, 0.07946277531666916,\n\t\t\t0.07582745683861136, 0.07106254853366956, 0.06699114553624523, 0.06738082315304855, 0.07055979133911858, 0.08695236626183257, 0.06288203145680492, 0.07488551304172053, 0.06771179941611492, 0.06945648591065866, 0.06453840380731399, 0.07385919087239427, 0.0683444564460048, 0.07954798738646204,\n\t\t\t0.07573860930571823, 0.07105740424606419, 0.06712040263541186, 0.06755135077535768, 0.07051941492274218, 0.08634820879904852, 0.06317369908893183, 0.0747529039899984, 0.06792390679452369, 0.0695515400287077, 0.06479886325742716, 0.07371365505105491, 0.0685174228348136, 0.0792326182702,\n\t\t\t0.07555287707789624, 0.07108190258682497, 0.06733854251103616, 0.06758139814192747, 0.07071323605707901, 0.0861278456542269, 0.06322708501847565, 0.07469646510239021, 0.06783869601120339, 0.06950215572849829, 0.06483416938890552, 0.07377594793641269, 0.06854104547049059, 0.07918863331463287,\n\t\t\t0.07574410047570453, 0.07103413668305468, 0.06711855678038553, 0.06746173578258094, 0.07057391214473711, 0.08657928293302596, 0.06307096744900711, 0.0747882330057183, 0.06777277460801437, 0.06950860513842702, 0.06469995848466632, 0.07381489642741564, 0.06846759446621521, 0.07936524562104719,\n\t\t\t0.0756717060516291, 0.07105145454009963, 0.06720880907724458, 0.06749167968783155, 0.07064761329018292, 0.08642736294272377, 0.06308003559618351, 0.07476907221361767, 0.06779607662586773, 0.06950261142997739, 0.06476752539297144, 0.07380336497224824, 0.06847743852360876, 0.07930524965581359,\n\t\t\t0.07563645146680772, 0.07112495994208998, 0.06731871823397463, 0.06765615471335058, 0.07050885672565142, 0.08576734410785562, 0.06343008839586677, 0.07465207569127591, 0.06804970059049206, 0.06957291430008673, 0.06499031957843418, 0.07373947913462867, 0.06861686357579774, 0.07893607354368812,\n\t\t\t0.07569158658182383, 0.07107722860166533, 0.06731391746556793, 0.0675431743007199, 0.07056731046387382, 0.08605331213010359, 0.06324822175577374, 0.07472890855666557, 0.06792926972082854, 0.0695275455347745, 0.06490125835629644, 0.07378585249938487, 0.06859138972758888, 0.0790410243049331,\n\t\t\t0.07584616545178219, 0.07107297386680439, 0.06695408929827328, 0.06742441213279868, 0.07050665835825033, 0.08694773144811387, 0.0628849678442258, 0.0748740336485852, 0.06776915498171088, 0.06943820012494253, 0.06453496372290675, 0.07385525396295956, 0.06834764791099958, 0.07954374724764714,\n\t\t\t0.07538565512371828, 0.07114478361763509, 0.0674790383417934, 0.0679530487109372, 0.07062025536698704, 0.08506511368360894, 0.06389345986184054, 0.07440454335436263, 0.06819812055093102, 0.06974090420713536, 0.06530163057984227, 0.07354643728883982, 0.06866344809125371, 0.07860356122111473,\n\t\t\t0.07562397749379275, 0.07110611903737409, 0.0672739706266011, 0.0676585272042326, 0.0705959936354916, 0.08597644776224045, 0.06334990426233511, 0.0746696259392387, 0.06798406880962622, 0.06957486697999139, 0.0648822050302828, 0.07369046356590116, 0.06856439629225838, 0.07904943336063361,\n\t\t\t0.07563986730000552, 0.07109972645158245, 0.06726119610287586, 0.06770658817421822, 0.07051204099049824, 0.08577979501497728, 0.06345833123151746, 0.07463305379391726, 0.06804596885976037, 0.06959449639477064, 0.06505177487882444, 0.07373543538604437, 0.06861310074280513, 0.07886862467820287,\n\t\t\t0.07593485247268887, 0.07104232089240368, 0.06688506966894726, 0.06732123863525878, 0.07048329760671607, 0.08731093985767327, 0.06272598329217195, 0.0749241125311565, 0.06767222083901234, 0.06942074681995235, 0.064391122122705, 0.07386032195241513, 0.06831133886358241, 0.07971643444531631,\n\t\t\t0.07560629485457866, 0.07105395691652636, 0.06720445533714883, 0.06752105971470501, 0.07066423287702295, 0.08652553260367433, 0.06316431748750224, 0.07474180282034944, 0.06777135038785487, 0.06951201011082883, 0.06471801132999752, 0.07376907051087278, 0.06843193112369053, 0.0793159739252478,\n\t\t\t0.07565081367829454, 0.07106736258185826, 0.06717681653743945, 0.0675878474881799, 0.07060692277557727, 0.08627399291946071, 0.06320783063962776, 0.07473348085957007, 0.06788588922811524, 0.06951886787922641, 0.06481442560885718, 0.07374610616056912, 0.0684859213223351, 0.07924372232088901,\n\t\t\t0.07553407196386892, 0.07112108441337289, 0.06730158843979399, 0.06776757520556947, 0.07061085164775649, 0.08571980011329053, 0.06359175596462249, 0.0745361198985853, 0.06803238479191852, 0.0696716741696257, 0.06505179760274592, 0.07361757873659114, 0.06855140067851208, 0.0788923163737467,\n\t\t\t0.07565440995291926, 0.07107784839868066, 0.06724722358791015, 0.06748300124067051, 0.0706526581545908, 0.08636441544447576, 0.0631539712428355, 0.07478188920166674, 0.06780091786533832, 0.06952147743627629, 0.06475272163644419, 0.07378649591573709, 0.06847548052790334, 0.07924748939455148,\n\t\t\t0.07549697252896907, 0.07113593019535562, 0.06739646419295066, 0.06780883908925636, 0.07061146721822258, 0.08538688832362386, 0.06355416640221782, 0.07453676968870958, 0.06814873230964302, 0.06960271369819923, 0.06517608169742432, 0.07369187556096998, 0.0687098493503266, 0.07874324974413119},\n\t},\n}\n\nfunc TestSoftMaxFull(t *testing.T) {\n\ttestCases := []struct {\n\t\tDtype    tensor.Dtype\n\t\tXInit    InitWFn\n\t\tXShape   tensor.Shape\n\t\tExpected tensor.Tensor\n\t\tIsLog    bool\n\t}{\n\t\t{\n\t\t\tDtype:    tensor.Float64,\n\t\t\tXInit:    RangedFromWithStep(0.0, 0.01),\n\t\t\tXShape:   tensor.Shape{2, 3},\n\t\t\tIsLog:    false,\n\t\t\tExpected: tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{0.49932500041006217, 0.5006749995899378, 0.49730002624369385, 0.502699973756306})),\n\t\t},\n\t\t{\n\t\t\tDtype:    tensor.Float32,\n\t\t\tXInit:    RangedFromWithStep(0.0, 0.01),\n\t\t\tXShape:   tensor.Shape{2, 3},\n\t\t\tIsLog:    false,\n\t\t\tExpected: tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float32{0.49932498, 0.50067496, 0.49730003, 0.5027})),\n\t\t},\n\t\t{\n\t\t\tDtype:    tensor.Float64,\n\t\t\tXInit:    RangedFromWithStep(0.0, 0.01),\n\t\t\tXShape:   tensor.Shape{2, 3},\n\t\t\tIsLog:    true,\n\t\t\tExpected: tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{-0.6944980918096686, -0.6917980918096686, -0.6985617604890871, -0.6877617604890871})),\n\t\t},\n\t\t{\n\t\t\tDtype:    tensor.Float32,\n\t\t\tXInit:    RangedFromWithStep(0.0, 0.01),\n\t\t\tXShape:   tensor.Shape{2, 3},\n\t\t\tIsLog:    true,\n\t\t\tExpected: tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float32{-0.6944981, -0.69179815, -0.6985617, -0.6877617})),\n\t\t},\n\t}\n\tfor i, tC := range testCases {\n\t\tt.Run(fmt.Sprintf(\"#%d %v\", i+1, tC.XShape), func(t *testing.T) {\n\t\t\tc := assert.New(t)\n\n\t\t\tg := NewGraph()\n\n\t\t\tx := NewTensor(g, tC.Dtype, 2, WithShape(tC.XShape...), WithInit(tC.XInit), WithName(\"x\"))\n\t\t\tw := NewTensor(g, tC.Dtype, 2, WithShape(tC.XShape...), WithInit(RangedFromWithStep(-0.05, 0.03)), WithName(\"w\"))\n\n\t\t\tt.Logf(\"Input: %v\", x.Value())\n\n\t\t\toptim := NewAdamSolver(WithLearnRate(0.1))\n\n\t\t\twT := Must(Transpose(w, 1, 0))\n\n\t\t\toutput := Must(Mul(x, wT))\n\n\t\t\tvar fcVal Value\n\t\t\tRead(output, &fcVal)\n\n\t\t\tsoftMaxFn := SoftMax\n\t\t\tif tC.IsLog {\n\t\t\t\tsoftMaxFn = LogSoftMax\n\t\t\t}\n\n\t\t\toutput = Must(softMaxFn(output))\n\n\t\t\tcost := Must(Mean(output))\n\n\t\t\t_, err := Grad(cost, x, w)\n\t\t\tc.NoError(err)\n\n\t\t\tvm := NewTapeMachine(g, BindDualValues(w))\n\t\t\tc.NoError(vm.RunAll())\n\n\t\t\tt.Logf(\"dx: %v\", x.Deriv().Value())\n\n\t\t\tc.NoError(optim.Step(NodesToValueGrads(Nodes{w})))\n\n\t\t\tt.Logf(\"wT: %v\", wT.Value())\n\t\t\tt.Logf(\"output: %v\", output.Value())\n\t\t\tt.Logf(\"FC Val: %v\", fcVal)\n\t\t\tt.Logf(\"cost: %v\", cost.Value())\n\t\t\tt.Logf(\"w: %v\", w.Value())\n\n\t\t\tc.InDeltaSlice(tC.Expected.Data(), output.Value().Data(), 1e-5, \"got=%#v\\nexpected=%#v\", output.Value().Data(), tC.Expected.Data())\n\t\t\tc.Equal(tC.Expected.Shape(), output.Shape())\n\t\t})\n\t}\n}\n\nfunc TestSoftmaxDo(t *testing.T) {\n\tassert := assert.New(t)\n\n\tfor i, testCase := range testCasesSoftMaxDo {\n\t\ttt := tensor.New(tensor.Of(tensor.Float64), tensor.WithShape(testCase.shape...), tensor.WithBacking(testCase.input))\n\t\top := newSoftmaxOp(tt.Shape())\n\n\t\tout, err := op.Do(tt)\n\n\t\tt.Logf(\"out: %v\", out)\n\n\t\tassert.NoError(err, \"failed test case: %d\", i)\n\t\tassert.InDeltaSlice(testCase.expected, out.Data().([]float64), 1e-7)\n\t}\n}\n\n// func TestSoftmaxKernel(t *testing.T) {\n// \t// this test is used for migrating to a new algorithm for softmax\n// \tassert := assert.New(t)\n// \ta := tensor.New(tensor.WithShape(2, 3), tensor.WithBacking([]float64{-0.1, 0.3, -1.1, 2.7, 3.14, 0.1}))\n// \top := newSoftmaxOp(a.Shape())\n// \top.axis = 0\n// \tb0, _ := op.Do(a)\n// \top.axis = 1\n// \tb1, _ := op.Do(a)\n\n// \t// across axis 0\n// \tout := make([]float64, 6)\n// \top.do(tensor.Shape{2, 3}, 0, a.Data().([]float64), out)\n// \tassert.True(floatsEqual64(out, b0.Data().([]float64)))\n// \tt.Logf(\"\\n%v\\n%v\", out, b0.Data())\n\n// \t// acros axis 1\n// \tout = make([]float64, 6)\n// \top.do(tensor.Shape{2, 3}, 1, a.Data().([]float64), out)\n// \tassert.True(floatsEqual64(out, b1.Data().([]float64)))\n// \t/*\n// \t\t// super large\n// \t\ta = tensor.New(tensor.WithShape(10, 1024, 2048, 30), tensor.WithBacking(Uniform64(-1, 1, 10, 1024, 2048, 30)))\n// \t\top = newSoftmaxOp(a.Shape())\n// \t\top.axis = 0\n// \t\tb, _ := op.Do(a)\n\n// \t\tout = make([]float64, 10*1024*2048*30)\n// \t\top.doF64s(tensor.Shape{10, 1024, 2048, 30}, 0, a.Data().([]float64), out)\n// \t\tassert.True(floatsEqual64(out, b.Data().([]float64)))\n// \t*/\n// }\n\nfunc oldsoftmax(a *Node, axes ...int) (retVal *Node, err error) {\n\taShape := a.Shape()\n\taxis := aShape.Dims() - 1 // default: last dim\n\tif a.IsColVec() || (a.IsVector() && !a.IsRowVec()) {\n\t\taxis = 0\n\t}\n\n\tif len(axes) > 0 {\n\t\tif axes[0] >= axis+1 || axes[0] < 0 {\n\t\t\treturn nil, errors.Errorf(\"Cannot perform SoftMax on axis %d. Input has shape %v\", axes[0], a.Shape())\n\t\t}\n\t\taxis = axes[0]\n\t}\n\n\tvar exp, sum *Node\n\tif exp, err = Exp(a); err != nil {\n\t\treturn nil, errors.Wrap(err, operationError)\n\t}\n\tif sum, err = Sum(exp, axis); err != nil {\n\t\treturn nil, errors.Wrap(err, operationError)\n\t}\n\n\tif sum.IsScalar() {\n\t\treturn HadamardDiv(exp, sum)\n\t}\n\n\t// reshape if necessary\n\tss := sum.Shape()\n\tdiff := exp.Shape().Dims() - ss.Dims()\n\n\t// TODO: multirank softmax\n\tif diff > 0 {\n\t\tnewShape := tensor.Shape(tensor.BorrowInts(ss.Dims() + diff))\n\t\tcopy(newShape, ss)\n\t\tcopy(newShape[axis+1:], newShape[axis:])\n\t\tnewShape[axis] = 1\n\n\t\tif sum, err = Reshape(sum, newShape); err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"Failed to reshape\")\n\t\t}\n\t}\n\n\treturn BroadcastHadamardDiv(exp, sum, nil, []byte{byte(axis)})\n}\n\nfunc TestOld_NewSoftmax(t *testing.T) {\n\ta := tensor.New(tensor.WithBacking([]float64{0.1, 0.1, 0.3, 0.1, 0.4}))\n\n\tg := NewGraph()\n\tA := NodeFromAny(g, a, WithName(\"A\"))\n\tsm := Must(SoftMax(A))\n\tsum := Must(Sum(sm))\n\tif _, err := Grad(sum, A); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\th := NewGraph()\n\tA2 := NodeFromAny(h, a, WithName(\"A\"))\n\tsm2 := Must(oldsoftmax(A2))\n\tsum2 := Must(Sum(sm2))\n\tif _, err := Grad(sum2, A2); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tm1 := NewTapeMachine(g, TraceExec(), BindDualValues())\n\tif err := m1.RunAll(); err != nil {\n\t\tt.Fatalf(\"m1 %v\", err)\n\t}\n\n\tm2 := NewTapeMachine(h, TraceExec(), BindDualValues())\n\tif err := m2.RunAll(); err != nil {\n\t\tt.Fatalf(\"m2 %v\", err)\n\t}\n\n\tAgrad, err := A.Grad()\n\tif err != nil {\n\t\tt.Fatalf(\"No grad for A %v\", err)\n\t}\n\n\tA2grad, err := A2.Grad()\n\tif err != nil {\n\t\tt.Fatalf(\"No grad for A2 %v\", err)\n\t}\n\n\tt.Logf(\"\\n%v\\n%v\", sm.Value(), sm2.Value())\n\tt.Logf(\"\\n%v\\n%v\", Agrad, A2grad)\n\n\tioutil.WriteFile(\"oldsm.dot\", []byte(h.ToDot()), 0644)\n\tioutil.WriteFile(\"newsm.dot\", []byte(g.ToDot()), 0644)\n\n}\n\nfunc BenchmarkSoftmaxLargeOldAxis0(b *testing.B) {\n\tb.StopTimer()\n\ta := tensor.New(tensor.WithShape(10, 1024, 2048, 30), tensor.WithBacking(Uniform64(-1, 1, 10, 1024, 2048, 30)))\n\top := newSoftmaxOp(a.Shape())\n\top.axis = 0\n\tvar v Value\n\n\tb.ResetTimer()\n\tb.StartTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tv, _ = op.Do(a)\n\t}\n\t_ = v\n}\n\n// func BenchmarkSoftmaxLargeNewAxis0(b *testing.B) {\n// \tb.StopTimer()\n// \ta := tensor.New(tensor.WithShape(10, 1024, 2048, 30), tensor.WithBacking(Uniform64(-1, 1, 10, 1024, 2048, 30)))\n// \top := newSoftmaxOp(a.Shape())\n// \top.axis = 0\n// \tout := make([]float64, len(a.Data().([]float64)))\n\n// \tb.ResetTimer()\n// \tb.StartTimer()\n// \tfor i := 0; i < b.N; i++ {\n// \t\top.do(a.Shape(), 0, a.Data().([]float64), out)\n// \t}\n\n// }\n\n// func BenchmarkSoftmaxMedOldAxis0(b *testing.B) {\n// \tb.StopTimer()\n// \ta := tensor.New(tensor.WithShape(1200, 2500), tensor.WithBacking(Uniform64(-1, 1, 1200, 2500)))\n// \top := newSoftmaxOp(a.Shape())\n// \top.axis = 0\n// \tvar v Value\n\n// \tb.ResetTimer()\n// \tb.StartTimer()\n// \tfor i := 0; i < b.N; i++ {\n// \t\tv, _ = op.Do(a)\n// \t}\n// \t_ = v\n// }\n\n// func BenchmarkSoftmaxMedNewAxis0(b *testing.B) {\n// \tb.StopTimer()\n// \ta := tensor.New(tensor.WithShape(1200, 2500), tensor.WithBacking(Uniform64(-1, 1, 1200, 2500)))\n// \top := newSoftmaxOp(a.Shape())\n// \top.axis = 0\n// \tout := make([]float64, len(a.Data().([]float64)))\n\n// \tb.ResetTimer()\n// \tb.StartTimer()\n// \tfor i := 0; i < b.N; i++ {\n// \t\top.do(a.Shape(), 0, a.Data().([]float64), out)\n// \t}\n\n// }\n"
        },
        {
          "name": "op_sparsemax.go",
          "type": "blob",
          "size": 10.560546875,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"hash\"\n\t\"math\"\n\t\"sort\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\ntype sparsemaxOp struct {\n\taxis int\n}\n\nfunc newSparsemaxOp(axes ...int) *sparsemaxOp {\n\taxis := -1\n\tif len(axes) > 0 {\n\t\taxis = axes[0]\n\t}\n\n\tsparsemaxop := &sparsemaxOp{\n\t\taxis: axis,\n\t}\n\n\treturn sparsemaxop\n}\n\n// Sparsemax -  implements the sparsemax operation described here: http://proceedings.mlr.press/v48/martins16.pdf\nfunc Sparsemax(x *Node, axes ...int) (*Node, error) {\n\top := newSparsemaxOp(axes...)\n\n\treturn ApplyOp(op, x)\n}\n\nfunc (op *sparsemaxOp) Arity() int {\n\treturn 1\n}\n\nfunc (op *sparsemaxOp) ReturnsPtr() bool { return false }\n\nfunc (op *sparsemaxOp) CallsExtern() bool { return false }\n\nfunc (op *sparsemaxOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, \"Sparsemax{}()\")\n}\n\nfunc (op *sparsemaxOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op *sparsemaxOp) String() string {\n\treturn fmt.Sprintf(\"Sparsemax{}()\")\n}\n\nfunc (op *sparsemaxOp) InferShape(inputs ...DimSizer) (tensor.Shape, error) {\n\ts := inputs[0].(tensor.Shape).Clone()\n\treturn s, nil\n}\n\nfunc (op *sparsemaxOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\treturn hm.NewFnType(a, a)\n}\n\nfunc (op *sparsemaxOp) OverwritesInput() int { return -1 }\n\nfunc (op *sparsemaxOp) checkInput(inputs ...Value) (tensor.Tensor, error) {\n\tif err := checkArity(op, len(inputs)); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar in tensor.Tensor\n\tvar ok bool\n\n\tif in, ok = inputs[0].(tensor.Tensor); !ok {\n\t\treturn nil, errors.Errorf(\"Expected input to be a tensor, got %T\", inputs[0])\n\t}\n\n\treturn in, nil\n}\n\nfunc (op *sparsemaxOp) Do(inputs ...Value) (Value, error) {\n\tinputTensor, err := op.checkInput(inputs...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Can't check Sparsemax input: %w\", err)\n\t}\n\n\tinputShape := inputTensor.Shape()\n\n\tif op.axis != -1 {\n\t\taxes := make([]int, inputTensor.Dims())\n\t\taxes[op.axis] = 1\n\n\t\tinputTensor, err = tensor.Transpose(inputTensor, axes...)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"error tranposing the input tensor: %w\", err)\n\t\t}\n\t}\n\n\tvar output interface{}\n\n\tswitch inputTensor.Dtype() {\n\tcase tensor.Float64:\n\t\toutput, err = op.float64sparseMax(inputTensor)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\tcase tensor.Float32:\n\t\toutput, err = op.float32sparseMax(inputTensor)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"invalid input type for Sparsemax, expected float64 or float32, got: %v\", inputTensor.Dtype())\n\t}\n\n\treturn tensor.New(tensor.Of(inputTensor.Dtype()), tensor.WithShape(inputShape.Clone()...), tensor.WithEngine(inputTensor.Engine()), tensor.WithBacking(output)), nil\n}\n\n// FIXME: go2 generics\nfunc (op *sparsemaxOp) float32sparseMax(inputTensor tensor.Tensor) (interface{}, error) {\n\tinputData := inputTensor.Data().([]float32)\n\tdims := inputTensor.Dims()\n\tit := 0\n\n\tto := inputTensor.Shape()[dims-1]\n\tfrom := tensor.Shape(inputTensor.Shape()[0 : dims-1]).TotalSize()\n\tif from == 0 {\n\t\tfrom = 1\n\t}\n\n\tmaxValues := make([]float32, from)\n\n\tfor i := 0; i < from; i++ {\n\t\tmaxValue := float32(-math.MaxFloat32)\n\n\t\tfor j := 0; j < to; j++ {\n\t\t\tif inputData[it] > maxValue {\n\t\t\t\tmaxValue = inputData[it]\n\t\t\t}\n\n\t\t\tit++\n\t\t}\n\n\t\tmaxValues[i] = maxValue\n\t}\n\n\t// this is math trick for numerical stability\n\tstableInput := make([]float32, len(inputData))\n\tit = 0\n\n\tfor i := 0; i < from; i++ {\n\t\tfor j := 0; j < to; j++ {\n\t\t\tstableInput[it] = inputData[it] - maxValues[i]\n\t\t\tit++\n\t\t}\n\t}\n\n\tsortedData := make([]float32, len(inputData))\n\tcopy(sortedData, stableInput)\n\n\tit = 0\n\tfor i := 0; i < from; i++ {\n\t\tstart := it\n\t\tit += to\n\n\t\tsort.Slice(sortedData[start:it], func(i, j int) bool {\n\t\t\treturn sortedData[start:it][i] > sortedData[start:it][j]\n\t\t})\n\t}\n\n\tthresholds := make([]float32, from)\n\tit = 0\n\n\tfor i := 0; i < from; i++ {\n\t\tcumSum := float32(0.0)\n\t\tprevCum := float32(0.0)\n\t\tmaxIndex := 0\n\n\t\tfor j := 0; j < to; j++ {\n\t\t\tk := 1 + float32(j+1)*sortedData[it]\n\n\t\t\tprevCum += sortedData[it]\n\n\t\t\tif k > prevCum {\n\t\t\t\tmaxIndex = j + 1\n\n\t\t\t\tcumSum += sortedData[it]\n\t\t\t}\n\n\t\t\tit++\n\t\t}\n\n\t\tthresholds[i] = (cumSum - 1) / float32(maxIndex)\n\t}\n\n\toutput := make([]float32, len(stableInput))\n\tit = 0\n\n\tfor i := 0; i < from; i++ {\n\t\tfor j := 0; j < to; j++ {\n\t\t\tvF := stableInput[it]\n\n\t\t\tif vF-thresholds[i] > 0 {\n\t\t\t\toutput[it] = vF - thresholds[i]\n\t\t\t}\n\n\t\t\tit++\n\t\t}\n\t}\n\n\treturn output, nil\n}\n\nfunc (op *sparsemaxOp) float64sparseMax(inputTensor tensor.Tensor) (interface{}, error) {\n\tinputData := inputTensor.Data().([]float64)\n\tdims := inputTensor.Dims()\n\tit := 0\n\n\tto := inputTensor.Shape()[dims-1]\n\tfrom := tensor.Shape(inputTensor.Shape()[0 : dims-1]).TotalSize()\n\tif from == 0 {\n\t\tfrom = 1\n\t}\n\n\tmaxValues := make([]float64, from)\n\n\tfor i := 0; i < from; i++ {\n\t\tmaxValue := -math.MaxFloat64\n\n\t\tfor j := 0; j < to; j++ {\n\t\t\tif inputData[it] > maxValue {\n\t\t\t\tmaxValue = inputData[it]\n\t\t\t}\n\n\t\t\tit++\n\t\t}\n\n\t\tmaxValues[i] = maxValue\n\t}\n\n\t// this is math trick for numerical stability\n\tstableInput := make([]float64, len(inputData))\n\tit = 0\n\n\tfor i := 0; i < from; i++ {\n\t\tfor j := 0; j < to; j++ {\n\t\t\tstableInput[it] = inputData[it] - maxValues[i]\n\t\t\tit++\n\t\t}\n\t}\n\n\tsortedData := make([]float64, len(inputData))\n\tcopy(sortedData, stableInput)\n\n\tit = 0\n\tfor i := 0; i < from; i++ {\n\t\tstart := it\n\t\tit += to\n\n\t\tsort.Slice(sortedData[start:it], func(i, j int) bool {\n\t\t\treturn sortedData[start:it][i] > sortedData[start:it][j]\n\t\t})\n\t}\n\n\tthresholds := make([]float64, from)\n\tit = 0\n\n\tfor i := 0; i < from; i++ {\n\t\tcumSum := 0.0\n\t\tprevCum := 0.0\n\t\tmaxIndex := 0\n\n\t\tfor j := 0; j < to; j++ {\n\t\t\tk := 1 + float64(j+1)*sortedData[it]\n\n\t\t\tprevCum += sortedData[it]\n\n\t\t\tif k > prevCum {\n\t\t\t\tmaxIndex = j + 1\n\n\t\t\t\tcumSum += sortedData[it]\n\t\t\t}\n\n\t\t\tit++\n\t\t}\n\n\t\tthresholds[i] = (cumSum - 1) / float64(maxIndex)\n\t}\n\n\toutput := make([]float64, len(stableInput))\n\tit = 0\n\n\tfor i := 0; i < from; i++ {\n\t\tfor j := 0; j < to; j++ {\n\t\t\tvF := stableInput[it]\n\n\t\t\tif vF-thresholds[i] > 0 {\n\t\t\t\toutput[it] = vF - thresholds[i]\n\t\t\t}\n\n\t\t\tit++\n\t\t}\n\t}\n\n\treturn output, nil\n}\n\n// DoDiff calculates the diff and sets its value to the output node. Implementation for ADOp interface.\nfunc (op *sparsemaxOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) error {\n\tif len(inputs) != 2 {\n\t\treturn fmt.Errorf(\"SparsemaxOp.DoDiff needs 2 arguments\")\n\t}\n\n\todv := output.boundTo.(*dualValue)\n\todvd := odv.Value.(tensor.Tensor)\n\tdiffOp := &sparsemaxDiffOp{}\n\n\tresult, err := diffOp.Do(odvd, inputs[1].boundTo)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = result.(*tensor.Dense).Reshape(odvd.Shape()...)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tsum, err := odvd.(*tensor.Dense).Add(result.(*tensor.Dense), tensor.UseUnsafe())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\todv.d = sum\n\n\treturn nil\n}\n\n// SymDiff applies the diff op. Implementation for SDOp interface.\nfunc (op *sparsemaxOp) SymDiff(inputs Nodes, output, grad *Node) (Nodes, error) {\n\terr := checkArity(op, len(inputs))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tt := output\n\n\tdiffOp := &sparsemaxDiffOp{}\n\tnodes := make(Nodes, 1)\n\n\tnodes[0], err = ApplyOp(diffOp, t, grad)\n\n\treturn nodes, err\n}\n\n// DiffWRT is an implementation for the SDOp interface\nfunc (op *sparsemaxOp) DiffWRT(inputs int) []bool {\n\tif inputs != 1 {\n\t\tpanic(fmt.Sprintf(\"sparsemax operator only supports one input, got %d instead\", inputs))\n\t}\n\n\treturn []bool{true}\n}\n\ntype sparsemaxDiffOp struct {\n}\n\nfunc newSparsemaxOpDiff() *sparsemaxDiffOp {\n\treturn &sparsemaxDiffOp{}\n}\n\nfunc (op *sparsemaxDiffOp) Arity() int {\n\treturn 2\n}\n\nfunc (op *sparsemaxDiffOp) ReturnsPtr() bool { return false }\n\nfunc (op *sparsemaxDiffOp) CallsExtern() bool { return false }\n\nfunc (op *sparsemaxDiffOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, \"SparsemaxDiff{}()\")\n}\n\nfunc (op *sparsemaxDiffOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op *sparsemaxDiffOp) String() string {\n\treturn fmt.Sprintf(\"SparsemaxDiff{}()\")\n}\n\nfunc (op *sparsemaxDiffOp) InferShape(inputs ...DimSizer) (tensor.Shape, error) {\n\ts := inputs[0].(tensor.Shape).Clone()\n\n\treturn s, nil\n}\n\nfunc (op *sparsemaxDiffOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\treturn hm.NewFnType(a, a, a)\n}\n\nfunc (op *sparsemaxDiffOp) OverwritesInput() int { return -1 }\n\nfunc (op *sparsemaxDiffOp) checkInput(inputs ...Value) (*tensor.Dense, *tensor.Dense, error) {\n\tif err := checkArity(op, len(inputs)); err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tvar (\n\t\tin *tensor.Dense\n\n\t\tgradient *tensor.Dense\n\t\tok       bool\n\t)\n\n\tswitch t := inputs[0].(type) {\n\tcase *dualValue:\n\t\tif in, ok = t.Value.(*tensor.Dense); !ok {\n\t\t\treturn nil, nil, errors.Errorf(\"input should be a tensor.Tensor, got %T\", inputs[0])\n\t\t}\n\tcase *tensor.Dense:\n\t\tin = t\n\tdefault:\n\t\treturn nil, nil, errors.Errorf(\"input type is not supported, got %T\", inputs[0])\n\t}\n\n\tswitch t := inputs[1].(type) {\n\tcase *dualValue:\n\t\tif gradient, ok = t.Value.(*tensor.Dense); !ok {\n\t\t\treturn nil, nil, errors.Errorf(\"gradient should be a tensor, got %T\", inputs[1])\n\t\t}\n\tcase *tensor.Dense:\n\t\tgradient = t\n\tdefault:\n\t\treturn nil, nil, errors.Errorf(\"gradient type is not supported, got %T\", inputs[1])\n\t}\n\n\treturn in, gradient, nil\n}\n\nfunc (op *sparsemaxDiffOp) mul(a tensor.Tensor, b tensor.Tensor) (tensor.Tensor, error) {\n\tif a.Dims() != b.Dims() {\n\t\treturn tensor.Outer(a, b)\n\t}\n\n\treturn tensor.Mul(a, b)\n}\n\nfunc (op *sparsemaxDiffOp) Do(inputs ...Value) (Value, error) {\n\tinputTensor, gradTensor, err := op.checkInput(inputs...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Can't check SparsemaxDiff input: %w\", err)\n\t}\n\n\tif inputTensor.Size() != gradTensor.Size() {\n\t\treturn nil, fmt.Errorf(\"sparsemaxDiffOp.Do inputs sizes should be equal\")\n\t}\n\n\tvar zero interface{}\n\n\tif inputTensor.Dtype() == tensor.Float32 {\n\t\tzero = float32(0.0)\n\t} else {\n\t\tzero = float64(0.0)\n\t}\n\n\tnonZeros, err := inputTensor.ElNeScalar(zero, false, tensor.AsSameType())\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"sparsemaxDiffOp.Do failed to get non-zeros: %w\", err)\n\t}\n\n\tmul, err := op.mul(nonZeros, gradTensor)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"sparsemaxDiffOp.Do failed to mul grad tensor: %w\", err)\n\t}\n\n\ta, err := tensor.Sum(mul, 1)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tb, err := tensor.Sum(nonZeros, 1)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tsum, err := tensor.Div(a, b)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif sum.Dims() == 1 && gradTensor.Dims() == 2 {\n\t\terr := sum.Reshape(sum.Shape()[0], 1)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tsum, err = tensor.Repeat(sum, 1, gradTensor.Shape()[1])\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n\n\tsub, err := tensor.Sub(gradTensor, sum)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tresult, err := op.mul(nonZeros, sub)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn result, nil\n}\n\n// ensure it complies with the Op interface\nvar (\n\t_ Op = &sparsemaxDiffOp{}\n\n\t_ Op   = &sparsemaxOp{}\n\t_ SDOp = &sparsemaxOp{}\n\t_ ADOp = &sparsemaxOp{}\n)\n"
        },
        {
          "name": "op_sparsemax_test.go",
          "type": "blob",
          "size": 2.8427734375,
          "content": "package gorgonia\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n\t\"gorgonia.org/tensor\"\n)\n\nvar testCasesSparseMaxDo = []struct {\n\tdesc     string\n\tsize     tensor.Shape\n\tinput    interface{}\n\tweights  interface{}\n\texpected interface{}\n\taxis     int\n}{\n\t{\n\t\t\"SparseMax Case 1\",\n\t\ttensor.Shape{2, 3}, []float64{-2.1714, 0.0000, 0.0000, -0.4233, 0.0000, -1.2849}, []float64{0.3, 0.0, 1.0, 0.7, 0.0, 1.0}, []float64{0.17428999999999994, 0.8257099999999999, 1, 0}, -1,\n\t},\n\t{\n\t\t\"SparseMax Case 2\",\n\t\ttensor.Shape{3, 3}, []float32{-3.1437, -0.5651, 0.0000, -0.7925, 0.0000, -0.5319, -0.0313, -1.1569, 0.0000}, []float32{0, 0.21744996, 0.78255, 0, 0.76594996, 0.23404998, 0.48434997, 0, 0.51565}, []float32{1, 0, 0, 0.45735168, 0.5426483, 0, 0.64763314, 0, 0.3523669}, -1,\n\t},\n\t{\n\t\t\"SparseMax Case 3\",\n\t\ttensor.Shape{6, 2},\n\t\t[]float32{-1.0000, -1.0000, 1.0000, 1.0000, -0.9998, -0.9998, 0.9998, 0.9998, 0.9945, 0.9945, -0.9945, -0.9945},\n\t\t[]float32{0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5}, []float32{0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667},\n\t\t-1,\n\t},\n}\n\nfunc TestSparsemaxFull(t *testing.T) {\n\tc := require.New(t)\n\n\tfor i, testCase := range testCasesSparseMaxDo {\n\t\tvar err error\n\n\t\tdtype := tensor.Float64\n\n\t\tif _, ok := testCase.input.([]float32); ok {\n\t\t\tdtype = tensor.Float32\n\t\t}\n\n\t\ttt := tensor.New(tensor.Of(dtype), tensor.WithShape(testCase.size...), tensor.WithBacking(testCase.input))\n\n\t\tweightsT := tensor.New(tensor.Of(dtype), tensor.WithShape(testCase.size[1], testCase.size[0]), tensor.WithBacking(testCase.weights))\n\n\t\texpected := tensor.New(tensor.Of(dtype), tensor.WithShape(testCase.size[0], testCase.size[0]), tensor.WithBacking(testCase.expected))\n\n\t\tg := NewGraph()\n\t\tinp := NewTensor(g, dtype, testCase.size.Dims(), WithShape(testCase.size...), WithName(\"inp\"))\n\n\t\tweights := NewTensor(g, dtype, 2, WithValue(weightsT), WithName(\"weights\"))\n\n\t\tfc := Must(Mul(inp, weights))\n\t\tout := Must(Sparsemax(fc, testCase.axis))\n\t\tcost := Must(Mean(out))\n\n\t\t_, err = Grad(cost, weights, inp)\n\t\tc.NoError(err)\n\n\t\tvm := NewTapeMachine(g, BindDualValues(weights))\n\t\terr = Let(inp, tt)\n\t\tc.NoError(err, \"failed assigning input on case %d\", i)\n\n\t\tc.NoError(vm.RunAll())\n\t\tc.NoError(vm.Close())\n\n\t\tc.Equal(expected.Data(), out.Value().(*tensor.Dense).Data(), \"output is not equal to expected value for case %d\", i)\n\n\t\toutGrad, _ := out.Grad()\n\t\tt.Logf(\"%v output grad:\\n%v\", testCase.desc, outGrad)\n\n\t\tinpGrad, _ := inp.Grad()\n\t\tt.Logf(\"%v input grad: %v\", testCase.desc, inpGrad)\n\t}\n}\n"
        },
        {
          "name": "op_tensor.go",
          "type": "blob",
          "size": 30.775390625,
          "content": "package gorgonia\n\nimport (\n\t\"bytes\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"hash\"\n\t\"sort\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\n/* This file contains tensor related Ops */\n\n// atOp takes a Tensor and returns the value at the coordinates.\ntype atOp struct {\n\tcoordinates coordinates\n\td           int\n}\n\nfunc (op atOp) Arity() int { return 1 }\n\n// atOp has this type\n//\t\top :: Tensor a → a\nfunc (op atOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\ttt := makeTensorType(op.d, a)\n\n\treturn hm.NewFnType(tt, a)\n}\n\nfunc (op atOp) ReturnsPtr() bool                                        { return false }\nfunc (op atOp) OverwritesInput() int                                    { return -1 }\nfunc (op atOp) CallsExtern() bool                                       { return false }\nfunc (op atOp) InferShape(...DimSizer) (retVal tensor.Shape, err error) { return scalarShape, nil }\nfunc (op atOp) DiffWRT(i int) []bool                                    { return make([]bool, i) }\nfunc (op atOp) SymDiff(Nodes, *Node, *Node) (Nodes, error)              { return nil, nondiffErr(op) }\nfunc (op atOp) String() string                                          { return fmt.Sprintf(\"At(%v)\", op.coordinates) }\n\nfunc (op atOp) Do(inputs ...Value) (retVal Value, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tswitch tt := inputs[0].(type) {\n\tcase *tensor.Dense:\n\t\tvar r interface{}\n\t\tif r, err = tt.At(op.coordinates...); err != nil {\n\t\t\terr = errors.Wrap(err, opDoFail)\n\t\t\treturn\n\t\t}\n\n\t\tretVal, _, _, err = anyToValue(r)\n\tdefault:\n\t\terr = errors.Errorf(nyiTypeFail, \"atOp.Do()\", tt)\n\t}\n\treturn\n}\n\nfunc (op atOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, \"atOp%v%v\", op.d, op.coordinates)\n}\n\nfunc (op atOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op atOp) isStmt() bool { return true }\n\ntype sizeOp struct {\n\taxis, d int\n\tval     int // if we know ahead of time what the size is...\n}\n\nfunc (op sizeOp) Arity() int { return 1 }\n\n// sizeOp is a function with this type:\n//\t\tsizeOp :: Tensor d a → a\nfunc (op sizeOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\n\t// handle scalar cases\n\tif op.d == 0 {\n\t\treturn hm.NewFnType(a, a)\n\t}\n\n\ttt := makeTensorType(op.d, a)\n\treturn hm.NewFnType(tt, a)\n}\n\nfunc (op sizeOp) ReturnsPtr() bool                             { return false }\nfunc (op sizeOp) OverwritesInput() int                         { return -1 }\nfunc (op sizeOp) CallsExtern() bool                            { return false }\nfunc (op sizeOp) InferShape(...DimSizer) (tensor.Shape, error) { return scalarShape, nil } // TODO: return error\nfunc (op sizeOp) DiffWRT(i int) []bool                         { return []bool{false} }\nfunc (op sizeOp) String() string {\n\tif op.val != 0 {\n\t\treturn fmt.Sprintf(\"SizeOf=%d\", op.val)\n\t}\n\treturn fmt.Sprintf(\"SizeOf(%d)\", op.axis)\n}\n\nfunc (op sizeOp) SymDiff(inputs Nodes, output, gradNode *Node) (Nodes, error) {\n\treturn nil, nondiffErr(op)\n}\n\nfunc (op sizeOp) Do(inputs ...Value) (retVal Value, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tswitch t := inputs[0].(type) {\n\tcase Scalar:\n\t\tretVal = one(t.Dtype())\n\n\t\t// bools are special\n\t\tif _, ok := t.(*B); ok {\n\t\t\tretVal = NewI(1)\n\t\t}\n\tcase tensor.Tensor:\n\t\tsh := t.Shape()\n\t\tif op.axis >= len(sh) {\n\t\t\treturn nil, errors.Errorf(\"Shape is %v. Want size of %d\", sh, op.axis)\n\t\t}\n\t\tsize := sh[op.axis]\n\n\t\t// cast as ... types\n\t\tswitch t.Dtype() {\n\t\tcase tensor.Float64:\n\t\t\tretVal = NewF64(float64(size))\n\t\tcase tensor.Float32:\n\t\t\tretVal = NewF32(float32(size))\n\t\tcase tensor.Int:\n\t\t\tretVal = NewI(size)\n\t\tdefault:\n\t\t\treturn nil, errors.Errorf(nyiFail, \"sizeOf.Do()\", t.Dtype())\n\t\t}\n\t}\n\n\treturn\n}\n\nfunc (op sizeOp) WriteHash(h hash.Hash) {\n\th.Write([]byte(\"sizeOf\"))\n\tif err := binary.Write(h, binary.LittleEndian, byte(op.d)); err != nil {\n\t\tpanic(err)\n\t}\n\th.Write([]byte(\"on\"))\n\tif err := binary.Write(h, binary.LittleEndian, byte(op.axis)); err != nil {\n\t\tpanic(err)\n\t}\n}\n\nfunc (op sizeOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op sizeOp) DimSize(d int) (int, error) {\n\tif d != op.axis {\n\t\treturn -1, errors.Errorf(\"Dimension mismatch. Size Op is for axis %d. Want Dim Size of %d\", op.axis, d)\n\t}\n\treturn op.val, nil\n}\n\ntype repeatOp struct {\n\talong      int\n\tinputShape tensor.Shape\n}\n\nfunc newRepeatOp(along int, a *Node) *repeatOp {\n\treturn &repeatOp{\n\t\talong:      along,\n\t\tinputShape: a.Shape().Clone(),\n\t}\n}\n\nfunc repeatedApply(along []int, children Nodes) (retVal *Node, err error) {\n\tif len(children) != len(along)+1 {\n\t\treturn nil, errors.Errorf(\"Expected %v children. Got %v instead (hint: along axes and number of children must match)\", len(along)+1, len(children))\n\t}\n\n\tretVal = children[0]\n\tfor i := range along {\n\t\top := newRepeatOp(along[i], retVal)\n\t\tif retVal, err = ApplyOp(op, retVal, children[i+1]); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\treturn\n}\n\nfunc (op repeatOp) Arity() int { return 2 }\n\n// repeat is defined as one of the following:\n//\t\trepeat :: Tensor-n a → a → Tensor-n a\n//\t\trepeat :: a → Vector a\n// The end result must have the same dimensions as the input\nfunc (op repeatOp) Type() hm.Type {\n\n\ta := hm.TypeVariable('a')\n\n\td := op.inputShape.Dims()\n\n\tvar i0t hm.Type\n\tvar rt hm.Type\n\n\tif d == 0 {\n\t\ti0t = a\n\t\trt = makeTensorType(d+1, a)\n\t} else {\n\t\ti0t = makeTensorType(d, a)\n\t\trt = makeTensorType(d, a)\n\t}\n\n\treturn hm.NewFnType(i0t, a, rt)\n}\n\nfunc (op repeatOp) ReturnsPtr() bool     { return true }\nfunc (op repeatOp) OverwritesInput() int { return 0 }\nfunc (op repeatOp) CallsExtern() bool    { return true } // set to true because we want to force the VM to use PreallocDo\n\nfunc (op repeatOp) InferShape(inputs ...DimSizer) (retVal tensor.Shape, err error) {\n\tretVal = inputs[0].(tensor.Shape).Clone()\n\trep, err := inputs[1].DimSize(op.along)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// TODO: switch stmt\n\tif retVal.IsVector() && retVal.Dims() <= op.along {\n\t\t// extend\n\t\tretVal = append(retVal, make(tensor.Shape, op.along-retVal.Dims()+1)...)\n\t\tfor i := range retVal {\n\t\t\tif retVal[i] == 0 {\n\t\t\t\tretVal[i] = 1\n\t\t\t}\n\t\t}\n\t}\n\tif retVal.IsScalar() {\n\t\tretVal = tensor.Shape{1}\n\t}\n\tretVal[op.along] *= rep\n\n\treturn\n}\n\nfunc (op repeatOp) DiffWRT(i int) []bool {\n\tsymdiffLogf(\"DiffWRT: %d\", i)\n\tretVal := make([]bool, i)\n\tretVal[0] = true\n\treturn retVal\n}\n\nfunc (op repeatOp) SymDiff(inputs Nodes, output, gradNode *Node) (retVal Nodes, err error) {\n\tvar n *Node\n\tif n, err = Sum(gradNode, op.along); err == nil {\n\t\tn.setGroup(gradClust)\n\t}\n\tretVal = make(Nodes, len(inputs))\n\tretVal[0] = n\n\treturn\n}\n\nfunc (op repeatOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) (err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\txdv, ydv := getDV(inputs[0], output)\n\n\tvar reps []int\n\tvar repeats []Value\n\tfor _, r := range inputs[1:] {\n\t\trepeats = append(repeats, r.Value())\n\t}\n\n\tif reps, err = valuesToInts(repeats); err != nil {\n\t\treturn\n\t}\n\n\txshape := xdv.Shape()\n\tvar d Value\n\td = ydv.d\n\n\t// we make it a colVec\n\tif xshape.IsVector() && !xshape.IsColVec() && !xshape.IsRowVec() {\n\t\txshape = tensor.Shape{xshape[0], 1}\n\t}\n\n\tif xshape.IsScalar() {\n\t\tsum := newSumOp([]int{op.along}, output.shape, output.Dims())\n\t\tif d, err = sum.Do(d); err != nil {\n\t\t\terr = errors.Wrapf(err, doFail, sum)\n\t\t\treturn\n\t\t}\n\t} else {\n\t\taxis := op.along\n\t\tif xshape[axis] == 1 {\n\t\t\tsum := newSumOp([]int{op.along}, output.shape, output.Dims())\n\t\t\tif d, err = sum.Do(d); err != nil {\n\t\t\t\terr = errors.Wrapf(err, doFail, sum)\n\t\t\t\treturn\n\t\t\t}\n\t\t} else {\n\t\t\tnewShape := xshape.Clone()\n\t\t\tnewShape = newShape[0 : axis+1]\n\t\t\tnewShape = append(newShape, reps...)\n\t\t\tif axis+1 < xshape.Dims() {\n\t\t\t\tnewShape = append(newShape, xshape[axis+1:]...)\n\t\t\t}\n\n\t\t\talong := []int{axis + 1}\n\n\t\t\t// a scalar can never get to this path\n\t\t\tt := d.(tensor.Tensor)\n\t\t\tif err = t.Reshape(newShape...); err != nil {\n\t\t\t\terr = errors.Wrapf(err, reshapeFail, newShape, t.DataSize())\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tsum := newSumOp(along, newShape, len(newShape))\n\t\t\tif d, err = sum.Do(d); err != nil {\n\t\t\t\terr = errors.Wrapf(err, doFail, sum)\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// sum.Do leaves the dimension of size 1 behind, so reshape here.\n\t\t\tt = d.(tensor.Tensor)\n\t\t\tfinalShape := newShape[:axis+1]\n\t\t\tif axis+1 < newShape.Dims() {\n\t\t\t\tfinalShape = append(finalShape, newShape[axis+2:]...)\n\t\t\t}\n\t\t\tif err = t.Reshape(finalShape...); err != nil {\n\t\t\t\terr = errors.Wrapf(err, reshapeFail, newShape, t.DataSize())\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t}\n\n\tadd := newEBOByType(addOpType, TypeOf(xdv.d), TypeOf(d))\n\tif d, err = add.UnsafeDo(xdv.d, d); err != nil {\n\t\treturn\n\t}\n\n\tif !add.ReturnsPtr() || inputs[0].IsScalar() {\n\t\terr = xdv.SetDeriv(d)\n\t}\n\n\treturn\n\n}\n\nfunc (op repeatOp) String() string { return fmt.Sprintf(\"Repeat%v\", op.along) }\n\n// Do performs a repeat on the value.\n// TODO(anyone): implement for other types\nfunc (op repeatOp) Do(inputs ...Value) (retVal Value, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tvar rep int\n\tif rep, err = valueToInt(inputs[1]); err != nil {\n\t\treturn nil, errors.Wrapf(err, \"Cannot convert %v to an int\", inputs[1])\n\t}\n\n\t// process inputs[0]\n\tvar t tensor.Tensor\n\tswitch iv := inputs[0].(type) {\n\tcase Scalar:\n\t\ts := iv.Data()\n\t\tt = tensor.New(tensor.FromScalar(s))\n\tcase tensor.Tensor:\n\t\t// if iv.Shape().IsScalarEquiv() {\n\t\t// \tlog.Printf(\"SCALAR EQUIV %v\", iv.Data())\n\t\t// \tt = iv.Clone().(tensor.Tensor)\n\t\t// \tretVal = t\n\t\t// \treturn\n\t\t// }\n\t\tt = iv\n\tdefault:\n\t\terr = errors.Errorf(nyiTypeFail, \"repeatOp.Do()\", inputs[0])\n\t\treturn\n\t}\n\n\t// actually do repeat\n\tif rep == 1 {\n\t\tgoto fin\n\t}\n\tif t, err = tensor.Repeat(t, op.along, rep); err != nil {\n\t\terr = errors.Wrapf(err, repFail, op.along, rep)\n\t\treturn\n\t}\nfin:\n\tretVal = t\n\treturn\n}\n\nfunc (op repeatOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, \"repeat %v %v\", op.along, op.inputShape)\n\tvar arg0Dim int\n\tif !op.inputShape.Eq(tensor.ScalarShape()) {\n\t\targ0Dim = op.inputShape[0]\n\t}\n\tif arg0Dim == 0 {\n\t\th.Write([]byte{1})\n\t} else {\n\t\th.Write([]byte{0})\n\t}\n}\n\nfunc (op repeatOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op repeatOp) UsePreallocDo(prealloc Value, inputs ...Value) (retVal Value, err error) {\n\tpt, ok := prealloc.(tensor.Tensor)\n\tif !ok {\n\t\treturn nil, errors.Errorf(\"Expected Tensor as a preallocated value. Got %v of %T instead\", prealloc, prealloc)\n\t}\n\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tvar rep int\n\tif rep, err = valueToInt(inputs[1]); err != nil {\n\t\treturn nil, errors.Wrapf(err, \"Cannot convert %v to an int\", inputs[1])\n\t}\n\n\t// process inputs[0]\n\tvar t tensor.Tensor\n\tswitch iv := inputs[0].(type) {\n\tcase Scalar:\n\t\ts := iv.Data()\n\t\tpt.Memset(s)\n\t\tretVal = pt\n\t\treturn\n\t\tt = tensor.New(tensor.FromScalar(s))\n\tcase tensor.Tensor:\n\t\tif iv.Shape().IsScalarEquiv() {\n\t\t\tdata := iv.Data()\n\t\t\tswitch dt := data.(type) {\n\t\t\tcase float64:\n\t\t\t\tptd := pt.Data().([]float64)\n\t\t\t\tfor i := range ptd {\n\t\t\t\t\tptd[i] = dt\n\t\t\t\t}\n\t\t\tcase float32:\n\t\t\t\tptd := pt.Data().([]float32)\n\t\t\t\tfor i := range ptd {\n\t\t\t\t\tptd[i] = dt\n\t\t\t\t}\n\t\t\tcase []float64:\n\t\t\t\tptd := pt.Data().([]float64)\n\t\t\t\tfor i := range ptd {\n\t\t\t\t\tptd[i] = dt[0]\n\t\t\t\t}\n\t\t\tcase []float32:\n\t\t\t\tptd := pt.Data().([]float32)\n\t\t\t\tfor i := range ptd {\n\t\t\t\t\tptd[i] = dt[0]\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn pt, nil\n\t\t}\n\t\tt = iv\n\tdefault:\n\t\terr = errors.Errorf(nyiTypeFail, \"repeatOp.Do()\", inputs[0])\n\t\treturn\n\t}\n\tif rep == 1 {\n\t\treturn Copy(pt, t)\n\t}\n\n\treturn tensor.RepeatReuse(t, pt, op.along, rep)\n}\n\n// sliceOp represents a slicing operation. If end ⩽ start, it means \":\"\ntype sliceOp struct {\n\ttensor.Slice\n\n\talong int // along which axis to slice?\n\n\ta int // along which axis of the original tensor\n\td int // how many dimensions were the original tensor\n}\n\nfunc (op *sliceOp) IsSlice() tensor.Slice { return op.Slice }\n\nfunc newSliceOp(s tensor.Slice, along, d int) *sliceOp {\n\treturn &sliceOp{\n\t\tSlice: s,\n\t\talong: along,\n\t\td:     d,\n\t}\n}\n\nfunc (op *sliceOp) Arity() int { return 1 }\n\n// slicing a tensor value T[:] has type\n// \t\tslice :: Tensor a → Tensor a\n// \t\tslice :: Tensor a → a\n//\n// The latter is in the case where the resulting dimensions is 0, returning a scalar\nfunc (op *sliceOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\ttt := makeTensorType(op.d, a)\n\n\tvar selection int\n\n\tif op.Slice == nil {\n\t\tselection = -1\n\t} else {\n\t\tselection = op.End() - op.Start()\n\t}\n\n\tif selection == 1 {\n\t\tif op.d == 1 {\n\t\t\treturn hm.NewFnType(tt, a)\n\t\t}\n\n\t\ttt2 := makeTensorType(op.d-1, a)\n\t\treturn hm.NewFnType(tt, tt2)\n\t}\n\n\treturn hm.NewFnType(tt, tt)\n}\n\nfunc (op *sliceOp) InferShape(inputs ...DimSizer) (s tensor.Shape, err error) {\n\tinput := inputs[0].(tensor.Shape)\n\tslices := make([]tensor.Slice, op.along+1)\n\tslices[op.along] = op.Slice\n\n\treturn input.S(slices...)\n\n\t// return input.S(op.Slice)\n}\n\nfunc (op *sliceOp) DiffWRT(i int) []bool {\n\tif i > 1 {\n\t\t// error\n\t\terr := errors.Errorf(\"sliceOp should only have one or more inputs. Got %v instead\", i)\n\t\tpanic(err)\n\t}\n\n\treturn []bool{true}\n}\n\nfunc (op *sliceOp) SymDiff(inputs Nodes, outputNode, gradNode *Node) (retVal Nodes, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tt := inputs[0]\n\tincrOp := sliceIncrOp{op}\n\n\tretVal = make(Nodes, 1)\n\tretVal[0], err = ApplyOp(incrOp, t, gradNode)\n\treturn\n}\n\nfunc (op *sliceOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) (err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\txdv, ydv := getDV(inputs[0], output)\n\n\t// var d Value\n\tincrOp := sliceIncrOp{op}\n\tif _, err = incrOp.UsePreallocDo(xdv.d, xdv.d, ydv.d); err != nil {\n\t\treturn errors.Wrapf(err, doFail, incrOp)\n\t}\n\n\t// there is no need to handle scalars, because you can never slice a scalar\n\t// add := newElemBinOp(addOpType, inputs[0], output)\n\t// if _, err = add.UnsafeDo(xdv.d, d); err != nil {\n\t// \treturn errors.Wrapf(err, unsafeDoFail, add)\n\t// }\n\n\treturn\n}\n\nfunc (op *sliceOp) Do(inputs ...Value) (retVal Value, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tt := inputs[0]\n\t// prep the slices\n\tvar slices []tensor.Slice\n\tslices = make([]tensor.Slice, len(t.Shape()))\n\n\tif !op.all() {\n\t\tslices[op.along] = op\n\t}\n\tswitch T := t.(type) {\n\tcase tensor.Tensor:\n\t\tvar v tensor.Tensor\n\t\tif v, err = T.Slice(slices...); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, sliceFail, slices)\n\t\t}\n\t\tif v.IsScalar() {\n\t\t\tretVal, _ = anyToScalar(v.ScalarValue())\n\t\t} else {\n\t\t\tretVal = v.(tensor.View).Materialize()\n\t\t}\n\tcase Scalar:\n\t\treturn nil, errors.New(\"Cannot slice a scalar value\")\n\tdefault:\n\t\treturn nil, errors.Errorf(nyiFail, \"sliceOp.Do()\", t)\n\t}\n\treturn\n}\n\nfunc (op *sliceOp) ReturnsPtr() bool     { return true }\nfunc (op *sliceOp) CallsExtern() bool    { return true }\nfunc (op *sliceOp) OverwritesInput() int { return -1 }\nfunc (op sliceOp) WriteHash(h hash.Hash) {\n\th.Write([]byte(\"slice\"))\n\tif err := binary.Write(h, binary.LittleEndian, byte(op.d)); err != nil {\n\t\tpanic(err)\n\t}\n\tfmt.Fprintf(h, \"%v\", op.along)\n\tif op.Slice == nil {\n\t\tfmt.Fprintf(h, \":\")\n\t\treturn\n\t}\n\n\tif err := binary.Write(h, binary.LittleEndian, byte(op.Start())); err != nil {\n\t\tpanic(err)\n\t}\n\tif err := binary.Write(h, binary.LittleEndian, byte(op.End())); err != nil {\n\t\tpanic(err)\n\t}\n\tif err := binary.Write(h, binary.LittleEndian, byte(op.Step())); err != nil {\n\t\tpanic(err)\n\t}\n}\n\nfunc (op sliceOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op sliceOp) String() string {\n\tvar buf bytes.Buffer\n\tbuf.WriteString(\"T[\")\n\tfor i := 0; i < op.along; i++ {\n\t\tbuf.WriteString(\":, \")\n\t}\n\n\tif op.all() {\n\t\tbuf.WriteString(\":\")\n\t} else {\n\t\tfmt.Fprintf(&buf, \"%d:%d:%d\", op.Start(), op.End(), op.Step())\n\t}\n\n\tbuf.WriteString(\"...]\")\n\treturn buf.String()\n}\n\n// func (op sliceOp) CUDADo(extern External, dev Device, prealloc Value, inputs ...Value) (retVal Value, err error) {\n// \treturn op.Do(inputs...)\n// }\n\n// func (op sliceOp) CUDAFuncName() string { return \"\" }\n\nfunc (op sliceOp) all() bool { return op.Slice == nil || op.End() <= op.Start() }\n\n// T[:] +=incr\n// THIS IS AN UNSAFE OPERATION\ntype sliceIncrOp struct {\n\t*sliceOp\n}\n\n// slicing a tensor value T[:] has type\n// \t\tslice :: Tensor a → b → Tensor a\n//\n// b can be a or Vector a\nfunc (op sliceIncrOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\tb := hm.TypeVariable('c')\n\ttt := makeTensorType(op.d, a)\n\n\tretVal := hm.NewFnType(tt, b, tt)\n\treturn retVal\n}\n\nfunc (op sliceIncrOp) Arity() int { return 2 }\n\nfunc (op sliceIncrOp) InferShape(inputs ...DimSizer) (retVal tensor.Shape, err error) {\n\tretVal = inputs[0].(tensor.Shape)\n\treturn\n}\n\nfunc (op sliceIncrOp) DiffWRT(i int) []bool {\n\tif err := checkArity(op, i); err != nil {\n\t\tpanic(err)\n\t}\n\n\treturn []bool{true, false}\n}\n\nfunc (op sliceIncrOp) SymDiff(inputs Nodes, outputNode, gradNode *Node) (retVal Nodes, err error) {\n\tvar slicedRes *Node\n\tif slicedRes, err = ApplyOp(op.sliceOp, gradNode); err != nil {\n\t\treturn nil, errors.Wrap(err, operationError)\n\t}\n\tretVal = Nodes{gradNode, slicedRes}\n\n\treturn\n}\n\nfunc (op sliceIncrOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) (err error) {\n\txdv, ydv, zdv := getDV3(inputs[0], inputs[1], output)\n\n\t// dzdx\n\tadd := newElemBinOp(addOpType, inputs[0], output)\n\tif _, err = add.UnsafeDo(xdv.d, zdv.d); err != nil {\n\t\treturn errors.Wrapf(err, unsafeDoFail, add)\n\t}\n\n\t// dzdy\n\tvar d Value\n\tif d, err = op.sliceOp.Do(zdv.d); err != nil {\n\t\treturn errors.Wrapf(err, doFail, op)\n\t}\n\n\tadd = newElemBinOp(addOpType, inputs[1], output)\n\tif _, err = add.UnsafeDo(ydv.d, d); err != nil {\n\t\treturn errors.Wrapf(err, doFail, add)\n\t}\n\treturn\n}\n\nfunc (op sliceIncrOp) Do(inputs ...Value) (retVal Value, err error) {\n\tmachineLogf(\"Doing %v\", op)\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tt := inputs[0]\n\tincr := inputs[1]\n\n\t// prep the slices\n\tslices := make([]tensor.Slice, op.d)\n\tif !op.all() {\n\t\tslices[op.along] = op\n\t}\n\n\tswitch T := t.(type) {\n\tcase *tensor.Dense:\n\t\tgrad := tensor.NewDense(T.Dtype(), T.Shape().Clone())\n\t\tvar v tensor.Tensor\n\t\tif v, err = grad.Slice(slices...); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, sliceFail, slices)\n\t\t}\n\t\tswitch i := incr.(type) {\n\t\tcase *F64:\n\t\t\ttensor.Add(v, i.any(), tensor.UseUnsafe())\n\t\tcase *F32:\n\t\t\ttensor.Add(v, i.any(), tensor.UseUnsafe())\n\t\tcase *tensor.Dense:\n\t\t\ttensor.Add(v, i, tensor.UseUnsafe())\n\t\t}\n\t\tretVal = grad\n\tcase Scalar:\n\t\treturn nil, errors.New(\"Cannot slice a scalar value\")\n\tdefault:\n\t\treturn nil, errors.Errorf(nyiFail, \"sliceIncrOp()\", t)\n\t}\n\treturn\n}\n\nfunc (op sliceIncrOp) UsePreallocDo(prealloc Value, inputs ...Value) (retVal Value, err error) {\n\tmachineLogf(\"Doing %v\", op)\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\tincr := inputs[1]\n\n\t// prep the slices\n\tslices := make([]tensor.Slice, op.d)\n\tif !op.all() {\n\t\tslices[op.along] = op\n\t}\n\n\tswitch T := prealloc.(type) {\n\tcase *tensor.Dense:\n\t\tvar v tensor.Tensor\n\t\tif v, err = T.Slice(slices...); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, sliceFail, slices)\n\t\t}\n\t\tswitch i := incr.(type) {\n\t\tcase *F64:\n\t\t\ttensor.Add(v, i.any(), tensor.UseUnsafe())\n\t\tcase *F32:\n\t\t\ttensor.Add(v, i.any(), tensor.UseUnsafe())\n\t\tcase *tensor.Dense:\n\t\t\ttensor.Add(v, i, tensor.UseUnsafe())\n\t\t}\n\t\tretVal = T\n\tcase Scalar:\n\t\treturn nil, errors.New(\"Cannot slice a scalar value\")\n\tdefault:\n\t\treturn nil, errors.Errorf(nyiFail, \"sliceIncrOp()\", prealloc)\n\t}\n\treturn\n}\n\nfunc (op sliceIncrOp) OverwritesInput() int { return 0 }\n\nfunc (op sliceIncrOp) WriteHash(h hash.Hash) {\n\th.Write([]byte(\"sliceIncr\"))\n\tif err := binary.Write(h, binary.LittleEndian, byte(op.d)); err != nil {\n\t\tpanic(err)\n\t}\n\tif err := binary.Write(h, binary.LittleEndian, byte(op.along)); err != nil {\n\t\tpanic(err)\n\t}\n\n\tif op.Slice == nil {\n\t\tfmt.Fprintf(h, \":\")\n\t\treturn\n\t}\n\n\tif err := binary.Write(h, binary.LittleEndian, byte(op.Start())); err != nil {\n\t\tpanic(err)\n\t}\n\tif err := binary.Write(h, binary.LittleEndian, byte(op.End())); err != nil {\n\t\tpanic(err)\n\t}\n\tif err := binary.Write(h, binary.LittleEndian, byte(op.Step())); err != nil {\n\t\tpanic(err)\n\t}\n}\n\nfunc (op sliceIncrOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op sliceIncrOp) String() string {\n\tvar buf bytes.Buffer\n\tbuf.WriteString(\"T[\")\n\n\tfor i := 0; i < op.along; i++ {\n\t\tbuf.WriteString(\":, \")\n\t}\n\n\tif op.all() {\n\t\tbuf.WriteString(\":\")\n\t} else {\n\t\tfmt.Fprintf(&buf, \"%d:%d:%d\", op.Start(), op.End(), op.Step())\n\t}\n\n\tbuf.WriteString(\"...]+=...\")\n\treturn buf.String()\n}\n\n// func (op sliceIncrOp) UsePreallocDo(val Value, inputs ...Value) (Value, error) {\n\n// }\n\ntype transposeOp struct {\n\tpattern []int\n\td       int\n}\n\nfunc (op transposeOp) Arity() int { return 1 }\n\n// transposing a tensor has type\n// \t\ttranspose :: Tensor a → Tensor a\nfunc (op transposeOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\ttt := makeTensorType(op.d, a)\n\n\treturn hm.NewFnType(tt, tt)\n}\n\nfunc (op transposeOp) InferShape(inputs ...DimSizer) (retVal tensor.Shape, err error) {\n\tinput := inputs[0].(tensor.Shape)\n\tif input.IsScalar() {\n\t\treturn nil, errors.Errorf(undefinedOnShape, op, input)\n\t}\n\n\tretVal = make(tensor.Shape, len(input))\n\tcopy(retVal, input)\n\terr = tensor.UnsafePermute(op.pattern, retVal)\n\treturn\n}\n\nfunc (op transposeOp) DiffWRT(i int) []bool {\n\tif err := checkArity(op, i); err != nil {\n\t\tpanic(err)\n\t}\n\n\treturn []bool{true}\n}\n\nfunc (op transposeOp) SymDiff(inputs Nodes, outputNode, gradNode *Node) (retVal Nodes, err error) {\n\tnewPattern := make([]int, len(op.pattern))\n\tfor i, p := range op.pattern {\n\t\tnewPattern[p] = i\n\t}\n\top2 := transposeOp{pattern: newPattern, d: op.d}\n\n\tretVal = make(Nodes, 1)\n\tretVal[0], err = ApplyOp(op2, gradNode)\n\treturn\n}\n\nfunc (op transposeOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) (err error) {\n\txdv, zdv := getDV(inputs[0], output)\n\n\tnewPattern := make([]int, len(op.pattern))\n\tfor i, p := range op.pattern {\n\t\tnewPattern[p] = i\n\t}\n\n\tvar zdvdT tensor.Tensor\n\tvar ok bool\n\tif zdvdT, ok = zdv.d.(tensor.Tensor); !ok {\n\t\treturn errors.Errorf(\"Expected the gradient of the output node to be a Tensor. Got %v instead\", zdv.d)\n\t}\n\n\tif err = zdvdT.T(newPattern...); err != nil {\n\t\treturn errors.Wrap(err, \"Failed to T()\")\n\t}\n\n\td := tensor.Materialize(zdvdT)\n\tzdvdT.UT()\n\n\tadd := newEBOByType(addOpType, inputs[0].t, TypeOf(zdvdT))\n\tif _, err = add.UnsafeDo(xdv.d, d); err != nil {\n\t\terr = errors.Wrapf(err, doFail, add)\n\t}\n\treturn\n}\n\nfunc (op transposeOp) Do(inputs ...Value) (retVal Value, err error) {\n\tmachineLogf(\"Doing %v\", op)\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tt := inputs[0].(tensor.Tensor)\n\n\tthrowaway := tensor.BorrowInts(len(op.pattern))\n\tcopy(throwaway, op.pattern)\n\t// return tensor.T(t, throwaway...)\n\n\treturn tensor.Transpose(t, throwaway...)\n\n\t// DEPRECATED\n\t// the reason for this is because the .T() method of a Tensor\n\t// will use the axes in the .transposedWith field\n\t// Later when .UT() is called, the .transposedWith field is recycled into the pool\n\t// throwaway := tensor.BorrowInts(len(op.pattern))\n\t// copy(throwaway, op.pattern)\n\n\t// t.T(throwaway...)\n\t// ret := t.Materialize()\n\t// t.UT()\n}\n\nfunc (op transposeOp) ReturnsPtr() bool     { return true }\nfunc (op transposeOp) CallsExtern() bool    { return false }\nfunc (op transposeOp) OverwritesInput() int { return 0 }\n\nfunc (op transposeOp) WriteHash(h hash.Hash) {\n\th.Write([]byte(\"transposeOp\"))\n\tfmt.Fprintf(h, \"%v\", op.pattern)\n\tif err := binary.Write(h, binary.LittleEndian, byte(op.d)); err != nil {\n\t\tpanic(err)\n\t}\n}\n\nfunc (op transposeOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op transposeOp) String() string {\n\tvar buf bytes.Buffer\n\tbuf.WriteString(\"Aᵀ{\")\n\tfor i, ax := range op.pattern {\n\t\tfmt.Fprintf(&buf, \"%d\", ax)\n\t\tif i < len(op.pattern)-1 {\n\t\t\tbuf.WriteString(\", \")\n\t\t}\n\t}\n\n\tbuf.WriteString(\"}\")\n\treturn buf.String()\n}\n\ntype concatOp struct {\n\taxis     int\n\td        int\n\tchildren int\n}\n\nfunc (op concatOp) Arity() int { return -1 }\n\n// concat only works for Tensor types\n//\t\tconcat :: Tensor a → Tensor a → ... → Tensor a\nfunc (op concatOp) Type() hm.Type {\n\ttt := makeTensorType(op.d, hm.TypeVariable('a'))\n\tfnt := make([]hm.Type, op.children+1)\n\tfor i := range fnt {\n\t\tfnt[i] = tt\n\t}\n\n\treturn hm.NewFnType(fnt...)\n}\n\nfunc (op concatOp) InferShape(ds ...DimSizer) (tensor.Shape, error) {\n\tif len(ds) == 0 {\n\t\treturn nil, errors.Errorf(\"No shapes passed in!\")\n\t}\n\tshapes, err := DimSizersToShapes(ds)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn shapes[0].Concat(op.axis, shapes[1:]...)\n}\n\nfunc (op concatOp) Do(vals ...Value) (Value, error) {\n\tif len(vals) == 1 {\n\t\treturn vals[0], nil\n\t}\n\n\tts, err := valuesToTensors(vals)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn tensor.Concat(op.axis, ts[0], ts[1:]...)\n}\n\nfunc (op concatOp) ReturnsPtr() bool     { return true }\nfunc (op concatOp) CallsExtern() bool    { return false }\nfunc (op concatOp) OverwritesInput() int { return -1 }\n\nfunc (op concatOp) WriteHash(h hash.Hash) {\n\th.Write([]byte(\"concatOp\"))\n\tfmt.Fprintf(h, \"axis: %d, dims: %d\", op.axis, op.d)\n}\n\nfunc (op concatOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op concatOp) String() string {\n\treturn fmt.Sprintf(\"Concat(axis=%d)\", op.axis)\n}\n\nfunc (op concatOp) DiffWRT(inputs int) []bool {\n\tretVal := make([]bool, inputs)\n\tfor i := range retVal {\n\t\tretVal[i] = true\n\t}\n\treturn retVal\n}\n\nfunc (op concatOp) SymDiff(inputs Nodes, output *Node, grad *Node) (retVal Nodes, err error) {\n\tvar start int\n\n\tretVal = make(Nodes, len(inputs))\n\tfor i, in := range inputs {\n\t\tif op.axis >= len(in.shape) {\n\t\t\treturn nil, errors.Errorf(\"Wanted dimension %d is larger than the shape %v\", op.axis, in.shape)\n\t\t}\n\t\tend := in.shape[op.axis] + start\n\n\t\ts := newSliceOp(S(start, end), op.axis, op.d)\n\t\tif retVal[i], err = ApplyOp(s, grad); err != nil {\n\t\t\treturn\n\t\t}\n\n\t\t// keep dims\n\t\tif end-start == 1 {\n\t\t\tshp := retVal[i].Shape().Clone()\n\t\t\tshp = append(shp, -1)\n\t\t\tcopy(shp[op.axis+1:], shp[op.axis:])\n\t\t\tshp[op.axis] = 1\n\t\t\tif retVal[i], err = Reshape(retVal[i], shp); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t\tstart = end\n\n\t}\n\treturn\n}\n\nfunc (op concatOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) error {\n\todv := output.boundTo.(*dualValue)\n\todvd := odv.d.(tensor.Tensor)\n\n\tvar start int\n\tfor _, in := range inputs {\n\t\tif op.axis >= len(in.shape) {\n\t\t\treturn errors.Errorf(\"Wanted dimension %d is larger than the shape %v\", op.axis, in.shape)\n\t\t}\n\t\tend := in.shape[op.axis] + start\n\n\t\tidv := in.boundTo.(*dualValue)\n\t\tidvd := idv.d.(tensor.Tensor)\n\n\t\tsliced, err := odvd.Slice(S(start, end))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// TODO: fix VAdd hack\n\t\t// add to odvd\n\t\tswitch st := sliced.(type) {\n\t\tcase *tensor.Dense:\n\t\t\td := idvd.(*tensor.Dense)\n\t\t\td.Add(st, tensor.UseUnsafe())\n\t\tdefault:\n\t\t\treturn errors.Errorf(nyiTypeFail, \"DoDiff (hack) \", st)\n\t\t}\n\n\t\tstart = end\n\t}\n\treturn nil\n}\n\ntype reshapeOp struct {\n\tfrom, to tensor.Shape\n}\n\nfunc (op reshapeOp) Arity() int { return 1 }\nfunc (op reshapeOp) Type() hm.Type {\n\tif op.from.Dims() != op.to.Dims() {\n\t\tfr := op.from.Dims()\n\t\tvar frT hm.Type\n\t\tfrT = newTensorType(fr, hm.TypeVariable('a'))\n\t\tif fr == 0 {\n\t\t\tfrT = hm.TypeVariable('a')\n\t\t}\n\n\t\tto := op.to.Dims()\n\t\tvar toT hm.Type\n\t\ttoT = newTensorType(to, hm.TypeVariable('a'))\n\t\tif to == 0 {\n\t\t\ttoT = hm.TypeVariable('a')\n\t\t}\n\t\treturn hm.NewFnType(frT, toT)\n\t}\n\treturn hm.NewFnType(hm.TypeVariable('a'), hm.TypeVariable('a'))\n}\nfunc (op reshapeOp) InferShape(ds ...DimSizer) (tensor.Shape, error) { return op.to.Clone(), nil }\n\nfunc (op reshapeOp) Do(vals ...Value) (Value, error) {\n\tif err := checkArity(op, len(vals)); err != nil {\n\t\treturn nil, err\n\t}\n\tvar val Value\n\tvar err error\n\tswitch vals[0].(type) {\n\tcase tensor.Tensor:\n\t\tif v, ok := vals[0].(*tensor.Dense); ok {\n\t\t\tif v.IsView() {\n\t\t\t\tval = v.Materialize()\n\t\t\t} else {\n\t\t\t\tval = v.ShallowClone()\n\t\t\t}\n\t\t} else {\n\t\t\tif val, err = CloneValue(vals[0]); err != nil {\n\t\t\t\treturn nil, errors.Wrapf(err, cloneFail, vals[0])\n\t\t\t}\n\t\t}\n\t\tif val.Shape().TotalSize() != op.from.TotalSize() {\n\t\t\treturn nil, errors.Errorf(\"Shape mismatch. Input shape is %v. Expected %v\", val.Shape(), op.from)\n\t\t}\n\n\t\tif err := val.(tensor.Tensor).Reshape(op.to...); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn val, nil\n\tcase Scalar:\n\t\tv0 := ScalarAsTensor(vals[0], op.to.Dims(), nil)\n\t\tif err := v0.(tensor.Tensor).Reshape(op.to...); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn v0, nil\n\tdefault:\n\t\treturn nil, errors.Errorf(nyiTypeFail, \"reshape.Do\", vals[0])\n\t}\n}\n\nfunc (op reshapeOp) ReturnsPtr() bool     { return true }\nfunc (op reshapeOp) CallsExtern() bool    { return false }\nfunc (op reshapeOp) OverwritesInput() int { return 0 }\nfunc (op reshapeOp) WriteHash(h hash.Hash) {\n\th.Write([]byte(\"reshapeOp\"))\n\tfmt.Fprintf(h, \"from: %v, dims: %v\", op.from, op.to)\n}\n\nfunc (op reshapeOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op reshapeOp) String() string { return fmt.Sprintf(\"Reshape%v\", op.to) }\n\nfunc (op reshapeOp) UnsafeDo(vals ...Value) (Value, error) {\n\tif err := checkArity(op, len(vals)); err != nil {\n\t\treturn nil, err\n\t}\n\tvar val Value\n\tvar err error\n\tswitch vals[0].(type) {\n\tcase tensor.Tensor:\n\t\tval = vals[0]\n\t\terr = val.(tensor.Tensor).Reshape(op.to...)\n\n\t\treturn val, err\n\tcase Scalar:\n\t\tv0 := ScalarAsTensor(vals[0], op.to.Dims(), nil)\n\t\tif err := v0.(tensor.Tensor).Reshape(op.to...); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn v0, nil\n\tdefault:\n\t\treturn nil, errors.Errorf(nyiTypeFail, \"reshape.Do\", vals[0])\n\t}\n}\n\nfunc (op reshapeOp) CUDADo(extern External, dev Device, prealloc Value, vals ...Value) (retVal Value, err error) {\n\tif err := checkArity(op, len(vals)); err != nil {\n\t\treturn nil, err\n\t}\n\tval := vals[0]\n\tswitch v := val.(type) {\n\tcase tensor.Tensor:\n\t\tif err := v.Reshape(op.to...); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn v, nil\n\tcase Scalar:\n\t\tvT := ScalarAsTensor(v, op.to.Dims(), nil)\n\t\tif err := vT.(tensor.Tensor).Reshape(op.to...); err != nil {\n\n\t\t\treturn nil, errors.Errorf(nyiTypeFail, \"reshape.Do\", \"Scalar\")\n\t\t}\n\t\treturn vT, nil\n\t}\n\n\tpanic(\"Unreachable\")\n}\n\nfunc (op reshapeOp) DiffWRT(i int) []bool { return []bool{true} }\n\nfunc (op reshapeOp) SymDiff(inputs Nodes, output *Node, grad *Node) (retVal Nodes, err error) {\n\tvar ret *Node\n\tif ret, err = Reshape(grad, op.from); err != nil {\n\t\treturn\n\t}\n\tret.setGroup(gradClust)\n\treturn Nodes{ret}, nil\n}\n\nfunc (op reshapeOp) DoDiff(ctx ExecutionContext, inputs Nodes, output *Node) (err error) {\n\tvar grad Value\n\tif grad, err = output.Grad(); err != nil {\n\t\treturn\n\t}\n\tT := grad.(tensor.Tensor)\n\tif err = T.Reshape(op.from...); err != nil {\n\t\treturn\n\t}\n\tinput := inputs[0]\n\tdv := input.boundTo.(*dualValue)\n\treturn dv.SetDeriv(T)\n}\n\n/* PRIVATE FUNCTIONS */\n\n// if value is contained in slice, contains returns the corresp. index in slice, -1 otherwise\nfunc contains(slice []int, value int) int {\n\tif nil == slice {\n\t\treturn -1\n\t}\n\n\tfor sliceIndex, sliceValue := range slice {\n\t\tif value == sliceValue {\n\t\t\treturn sliceIndex\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// TODO: This function is an overkill for a small number of axes...\nfunc sortUniqueIntWithImitator(toBeSorted, imitator []int) {\n\ttoBeSortedBackup := make([]int, len(toBeSorted))\n\tfor index, value := range toBeSorted {\n\t\ttoBeSortedBackup[index] = value\n\t}\n\n\timitatorBackup := make([]int, len(imitator))\n\tfor index, value := range imitator {\n\t\timitatorBackup[index] = value\n\t}\n\n\tsort.Ints(toBeSorted)\n\n\t// Permutate the imitator accordingly\n\tfor originalIndex, originalValue := range toBeSortedBackup {\n\t\tsortedIndex := sort.SearchInts(toBeSorted, originalValue)\n\n\t\timitator[sortedIndex] = imitatorBackup[originalIndex]\n\t}\n\n\treturn\n}\n"
        },
        {
          "name": "op_tensor_test.go",
          "type": "blob",
          "size": 9.08984375,
          "content": "package gorgonia\n\nimport (\n\t\"crypto/sha256\"\n\t\"runtime\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"gorgonia.org/tensor\"\n)\n\nvar repeatOpTests = []struct {\n\tname string\n\trep  int\n\taxes int\n\tval  Value\n\n\tcorrect       Value\n\texpectedShape tensor.Shape\n\terr           bool\n}{\n\t{\n\t\t\"repeat matrix on axis 0\", 2, 0,\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4}), tensor.WithShape(2, 2)),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 1, 2, 3, 4, 3, 4}), tensor.WithShape(4, 2)),\n\t\ttensor.Shape{4, 2}, false,\n\t},\n\n\t{\n\t\t\"repeat matrix on axis 1\", 2, 1,\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 3, 4}), tensor.WithShape(2, 2)),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 1, 2, 2, 3, 3, 4, 4}), tensor.WithShape(2, 4)),\n\t\ttensor.Shape{2, 4}, false,\n\t},\n\n\t{\n\t\t\"repeat col vec on axis 0\", 2, 0,\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2}), tensor.WithShape(2, 1)),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 1, 2, 2}), tensor.WithShape(4, 1)),\n\t\ttensor.Shape{4, 1}, false,\n\t},\n\n\t{\n\t\t\"repeat col vec on axis 1\", 2, 1,\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2}), tensor.WithShape(2, 1)),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 1, 2, 2}), tensor.WithShape(2, 2)),\n\t\ttensor.Shape{2, 2}, false,\n\t},\n\n\t{\n\t\t\"repeat row vec on axis 0\", 2, 0,\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2}), tensor.WithShape(1, 2)),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2, 1, 2}), tensor.WithShape(2, 2)),\n\t\ttensor.Shape{2, 2}, false,\n\t},\n\n\t{\n\t\t\"repeat row vec on axis 1\", 2, 1,\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2}), tensor.WithShape(1, 2)),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 1, 2, 2}), tensor.WithShape(1, 4)),\n\t\ttensor.Shape{1, 4}, false,\n\t},\n\n\t{\n\t\t\"repeat vector on axis 0\", 2, 0,\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2}), tensor.WithShape(2)),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 1, 2, 2}), tensor.WithShape(4)),\n\t\ttensor.Shape{4}, false,\n\t},\n\n\t{\n\t\t\"repeat vector on axis 1\", 2, 1,\n\t\ttensor.New(tensor.WithBacking([]float64{1, 2}), tensor.WithShape(2)),\n\t\ttensor.New(tensor.WithBacking([]float64{1, 1, 2, 2}), tensor.WithShape(2, 2)),\n\t\ttensor.Shape{2, 2}, false,\n\t},\n\n\t{\n\t\t\"repeat scalar\", 2, 0,\n\t\tNewF64(3.14), tensor.New(tensor.WithBacking([]float64{3.14, 3.14}), tensor.WithShape(2)),\n\t\ttensor.Shape{2}, false,\n\t},\n}\n\nfunc TestRepeatOp(t *testing.T) {\n\t// assert := assert.New(t)\n\n\tfor _, rots := range repeatOpTests {\n\t\t// if rots.name != \"repeat matrix on axis 1\" {\n\t\t// \tcontinue\n\t\t// }\n\t\tg := NewGraph()\n\t\tvar res Value\n\t\tvar err error\n\t\tvar repeat *repeatOp\n\n\t\trep := NewI(rots.rep)\n\t\tn := NodeFromAny(g, rots.val)\n\n\t\trepeat = newRepeatOp(rots.axes, n)\n\n\t\tres, err = repeat.Do(rots.val, rep)\n\t\tswitch {\n\t\tcase rots.err:\n\t\t\tif err == nil {\n\t\t\t\tt.Errorf(\"Test %q: Expected an error\", rots.name)\n\t\t\t}\n\t\t\tgoto infershape\n\t\tcase !rots.err && err != nil:\n\t\t\tt.Errorf(\"%+v\", err)\n\t\t\tgoto infershape\n\t\t}\n\n\t\tif !ValueEq(res, rots.correct) {\n\t\t\tt.Errorf(\"Test %q: Expected %v. Got %v\", rots.name, rots.correct, res)\n\t\t}\n\n\tinfershape:\n\t\tvar s tensor.Shape\n\t\tsize := sizeOp{axis: rots.axes, val: rots.rep}\n\t\ts, err = repeat.InferShape(rots.val.Shape(), size)\n\t\tswitch {\n\t\tcase rots.err:\n\t\t\tif err == nil {\n\t\t\t\tt.Error(\"Expected an error\")\n\t\t\t}\n\t\t\tcontinue\n\t\tcase !rots.err && err != nil:\n\t\t\tt.Errorf(\"Test %q %+v\", rots.name, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif !rots.expectedShape.Eq(s) {\n\t\t\tt.Errorf(\"Test %q InferShape: Expected %v. Got %v instead\", rots.name, rots.expectedShape, s)\n\t\t}\n\t}\n}\n\nfunc repeatOpDiff(repeatOn int, shape tensor.Shape, xV, yV interface{}) (g *ExprGraph, x, y *Node, err error) {\n\tg = NewGraph()\n\tswitch shape.Dims() {\n\tcase 0:\n\t\tx = NewScalar(g, Float64, WithName(\"x\"))\n\tcase 1:\n\t\t// vanilla vector\n\t\tx = NewVector(g, Float64, WithName(\"x\"), WithShape(shape...))\n\tcase 2:\n\t\tx = NewMatrix(g, Float64, WithName(\"x\"), WithShape(shape...))\n\tdefault:\n\t\t//matrix and tensors\n\t\tx = NewTensor(g, Float64, shape.Dims(), WithName(\"x\"), WithShape(shape...))\n\t}\n\n\trepOp := sizeOp{axis: repeatOn, val: 2}\n\trepN := NewScalar(g, Float64, WithName(\"REPCONST\"), WithOp(repOp), WithValue(2.0))\n\trepeat := newRepeatOp(repeatOn, x)\n\n\tif y, err = ApplyOp(repeat, x, repN); err != nil {\n\t\treturn\n\t}\n\txVal, _, _, _ := anyToValue(xV)\n\tyVal, _, _, _ := anyToValue(yV)\n\tx.bind(dvUnit(xVal))\n\ty.bind(dvUnitVar(yVal))\n\tif err = repeat.DoDiff(ExecutionContext{}, Nodes{x, repN}, y); err != nil {\n\t\treturn\n\t}\n\treturn\n}\n\nfunc TestRepeatOpDoDiff(t *testing.T) {\n\t//t.SkipNow()\n\tassert := assert.New(t)\n\t// var g *ExprGraph\n\t// var x, y, repN *Node\n\t// var repeat *repeatOp\n\tvar x *Node\n\tvar err error\n\n\tvar xG Value\n\tvar xT, yT *tensor.Dense\n\n\tyT = tensor.New(tensor.WithShape(2), tensor.WithBacking([]float64{3.14, 3.14}))\n\n\t// scalar repeated into a vec/colvec\n\tif _, x, _, err = repeatOpDiff(0, scalarShape, 3.14, yT); err != nil {\n\t\tt.Fatal(err)\n\t}\n\txG, _ = x.Grad()\n\tassert.Equal(2.0, extractF64(xG))\n\n\t// scalar repeated into a rowvec\n\t// if _, x, _, err = repeatOpDiff(1, scalarShape, 3.14, yT); err != nil {\n\t// \tt.Fatal(err)\n\t// }\n\t// xG, _ = x.Grad()\n\t// assert.Equal(2.0, extractF64(xG))\n\n\t// vector repeated unto itself\n\txT = tensor.New(tensor.WithShape(2), tensor.WithBacking([]float64{3.14, 3.14}))\n\tyT = tensor.New(tensor.WithShape(4), tensor.WithBacking([]float64{3.14, 3.14, 3.14, 3.14}))\n\tif _, x, _, err = repeatOpDiff(0, tensor.Shape{2}, xT, yT); err != nil {\n\t\tt.Fatal(err)\n\t}\n\txG, _ = x.Grad()\n\tassert.Equal([]float64{2, 2}, extractF64s(xG))\n\n\t// colvec repeated unto itself\n\txT = tensor.New(tensor.WithShape(2, 1), tensor.WithBacking([]float64{3.14, 3.14}))\n\tyT = tensor.New(tensor.WithShape(4, 1), tensor.WithBacking([]float64{3.14, 3.14, 3.14, 3.14}))\n\tif _, x, _, err = repeatOpDiff(0, tensor.Shape{2}, xT, yT); err != nil {\n\t\tt.Fatal(err)\n\t}\n\txG, _ = x.Grad()\n\tassert.Equal([]float64{2, 2}, extractF64s(xG))\n\n\t// rowvec repeated unto itself\n\txT = tensor.New(tensor.WithShape(1, 2), tensor.WithBacking([]float64{3.14, 3.14}))\n\tyT = tensor.New(tensor.WithShape(1, 4), tensor.WithBacking([]float64{3.14, 3.14, 3.14, 3.14}))\n\tif _, x, _, err = repeatOpDiff(1, tensor.Shape{1, 2}, xT, yT); err != nil {\n\t\tt.Fatal(err)\n\t}\n\txG, _ = x.Grad()\n\tassert.Equal([]float64{2, 2}, extractF64s(xG))\n\n\t// matrix on axis 0\n\txT = tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{3.14, 2.718, 1.618, 1.414}))\n\tyT = tensor.New(tensor.WithShape(4, 2), tensor.WithBacking([]float64{3.14, 2.718, 3.14, 2.718, 1.618, 1.414, 1.618, 1.414}))\n\tif _, x, _, err = repeatOpDiff(0, tensor.Shape{1, 2}, xT, yT); err != nil {\n\t\tt.Fatal(err)\n\t}\n\txG, _ = x.Grad()\n\tassert.Equal([]float64{2, 2, 2, 2}, extractF64s(xG))\n\n\t// matrix on axis 1\n\txT = tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{3.14, 2.718, 1.618, 1.414}))\n\tyT = tensor.New(tensor.WithShape(4, 2), tensor.WithBacking([]float64{3.14, 2.718, 3.14, 2.718, 1.618, 1.414, 1.618, 1.414}))\n\tif _, x, _, err = repeatOpDiff(1, tensor.Shape{1, 2}, xT, yT); err != nil {\n\t\tt.Fatal(err)\n\t}\n\txG, _ = x.Grad()\n\tassert.Equal([]float64{2, 2, 2, 2}, extractF64s(xG))\n\n}\n\nfunc TestTransposeOp(t *testing.T) {\n\tassert := assert.New(t)\n\tg := NewGraph()\n\tA := NewMatrix(g, Float64, WithShape(2, 3), WithInit(RangedFrom(0)))\n\tAT := Must(Transpose(A))\n\tcost1 := Must(Sum(AT))\n\n\tvar m VM\n\tvar err error\n\n\tm = NewLispMachine(g)\n\tdefer m.Close()\n\tif err = m.RunAll(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tassert.Equal(tensor.Shape{3, 2}, AT.shape)\n\n\th := NewGraph()\n\tB := NewMatrix(h, Float64, WithShape(2, 3), WithInit(RangedFrom(0)))\n\tBT := Must(Transpose(B))\n\tcost2 := Must(Sum(BT))\n\tGrad(cost2, B)\n\n\tm = NewTapeMachine(h)\n\tdefer m.Close()\n\tif err = m.RunAll(); err != nil {\n\t\tt.Error(err)\n\t}\n\tassert.Equal(tensor.Shape{3, 2}, BT.shape)\n\n\tvar ag, bg Value\n\tif ag, err = A.Grad(); err != nil {\n\t\tt.Fatalf(\"Cannot get grad of A. Err: %v\", err)\n\t}\n\n\tif bg, err = B.Grad(); err != nil {\n\t\tt.Fatalf(\"Cannot get grad of B. Err: %v\", err)\n\t}\n\n\tvar costGrad1, costGrad2 Value\n\tif costGrad1, err = cost1.Grad(); err != nil {\n\t\tt.Fatalf(\"Cannot get grad of Cost1. Err %v\", err)\n\t}\n\n\tif costGrad2, err = cost2.Grad(); err != nil {\n\t\tt.Fatalf(\"Cannot get grad of Cost2. Err %v\", err)\n\t}\n\n\tt.Logf(\"%v %v\", cost1.Value(), cost2.Value())\n\tt.Logf(\"%v %v\", costGrad1, costGrad2)\n\n\tassert.True(ValueEq(ag, bg))\n}\n\nfunc TestConcatOp(t *testing.T) {\n\tdefer runtime.GC()\n\n\tassert := assert.New(t)\n\tg := NewGraph()\n\tx := NewVector(g, Float64, WithShape(2))\n\txx, err := Concat(0, x, x)\n\tif err != nil {\n\t\tt.Fatalf(\"%+v\", err)\n\t}\n\n\tcost := Must(Sum(xx))\n\tGrad(cost, x)\n\n\tg2 := NewGraph()\n\ta := NewVector(g2, Float64, WithShape(2))\n\taa, err := Concat(0, a, a)\n\tif err != nil {\n\t\tt.Fatalf(\"%+v\", err)\n\t}\n\tMust(Sum(aa)) // cost\n\n\taBack := []float64{1, 2}\n\taT := tensor.New(tensor.WithShape(2), tensor.WithBacking(aBack))\n\n\txBack := []float64{1, 2}\n\txT := tensor.New(tensor.WithShape(2), tensor.WithBacking(xBack))\n\n\tLet(a, aT)\n\tLet(x, xT)\n\tm1 := NewTapeMachine(g)\n\tm2 := NewLispMachine(g2)\n\tdefer m1.Close()\n\tdefer m2.Close()\n\n\tif err = m1.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err = m2.RunAll(); err != nil {\n\t\tt.Fatalf(\"%+v\", err)\n\t}\n\n\txG, _ := x.Grad()\n\taG, _ := a.Grad()\n\tassert.True(ValueEq(xG, aG))\n\tassert.True(ValueEq(xx.Value(), aa.Value()))\n}\n\nfunc Test_atOp_WriteHash(t *testing.T) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tt.Fail()\n\t\t}\n\t}()\n\th := sha256.New()\n\n\tat := &atOp{}\n\tat.WriteHash(h)\n}\n"
        },
        {
          "name": "op_test.go",
          "type": "blob",
          "size": 0.8486328125,
          "content": "package gorgonia\n\nimport \"testing\"\n\nfunc TestStupid(t *testing.T) {\n\tg := NewGraph()\n\tn := newNode(WithType(Float64), In(g))\n\top := newElemUnaryOp(negOpType, n)\n\n\tt.Logf(\"%v %d %s\", op, op.unaryOpType(), op.ʘUnaryOperator)\n\n\tv := NewF64(3.1415)\n\trv, err := op.Do(v)\n\tt.Logf(\"%v, %v\", rv, err)\n}\n\nfunc TestOpEquality(t *testing.T) {\n\tvar op1, op2 Op\n\tg := NewGraph()\n\ta := NewScalar(g, Float64, WithValue(3.14))\n\tb := NewScalar(g, Float64, WithValue(6.28))\n\top1 = newElemBinOp(addOpType, a, b)\n\top2 = newElemBinOp(addOpType, a, b)\n\n\tif op1.Hashcode() != op2.Hashcode() {\n\t\tt.Error(\"oops\")\n\t}\n\n\top1 = maxOp{\n\t\talong: axes{0, 1},\n\t\td:     2,\n\t}\n\n\top2 = maxOp{\n\t\talong: axes{0, 1},\n\t\td:     2,\n\t}\n\n\tif op1.Hashcode() != op2.Hashcode() {\n\t\tt.Error(\"oops\")\n\t}\n\n\top2 = sumOp{\n\t\talong: axes{0, 1},\n\t\td:     2,\n\t}\n\n\tif op1.Hashcode() == op2.Hashcode() {\n\t\tt.Error(\"oops\")\n\t}\n}\n"
        },
        {
          "name": "op_types.go",
          "type": "blob",
          "size": 4.876953125,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"hash\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\n// ConvType converts the type of the x Node from one type to other\nfunc ConvType(x *Node, from, to tensor.Dtype) (*Node, error) {\n\top := &dtConvOp{\n\t\tinshape: x.Shape(),\n\t\tfrom:    from,\n\t\tto:      to,\n\t}\n\n\treturn ApplyOp(op, x)\n}\n\ntype dtConvOp struct {\n\tinshape  tensor.Shape\n\tfrom, to tensor.Dtype\n}\n\n/* Graph Building Related Methods */\n\n// Arity returns the number of inputs the Op expects. -1 indicates that it's n-ary and will be determined at runtime\nfunc (op *dtConvOp) Arity() int { return 1 }\n\n// Informs the type of the Op (not the node). This will be used by the type system to infer the final type of the node\nfunc (op *dtConvOp) Type() hm.Type {\n\tif op.inshape.IsScalar() {\n\t\treturn hm.NewFnType(op.from, op.to)\n\t}\n\tt := makeTensorType(op.inshape.Dims(), op.from)\n\tu := makeTensorType(op.inshape.Dims(), op.to)\n\treturn hm.NewFnType(t, u)\n}\n\n// returns the output shape as a function of the inputs\nfunc (op *dtConvOp) InferShape(_ ...DimSizer) (tensor.Shape, error) {\n\treturn op.inshape.Clone(), nil\n}\n\n// Do executes the op\nfunc (op *dtConvOp) Do(vals ...Value) (Value, error) {\n\tretVal := tensor.New(tensor.Of(op.to), tensor.WithShape(op.inshape.Clone()...))\n\treturn op.UsePreallocDo(retVal, vals...)\n}\n\n/* Analysis Related Methods */ // indicates if the Op will return a pointer (allowing possible inplace edits) or by value\n// if it's false, the return value of the Op will be a copy of its input\nfunc (op *dtConvOp) ReturnsPtr() bool { return false }\n\n// Does this op potentially call external (cgo or cuda) functions (thereby requiring extra overhead for Go's trampolining thing)\nfunc (op *dtConvOp) CallsExtern() bool { return false }\n\n// overwriteInput() is a method which states which input the output will be overwriting.\n// This allows for some efficiency gains as the underlying arrays wouldn't have to be re-allocated.\n// The method returns an int instead of a bool because potentially different operations may be allowed\n// to overwrite certain inputs. For example, consider an operation to increment a value:\n// the IncrementOp would be a unary operator, and assuming we would like to overwrite the input,\n// the retVal of overwriteInput() will be 0 (inputs[0]).\n// -1 is returned if overwriting of input is disallowed\nfunc (op *dtConvOp) OverwritesInput() int { return -1 }\n\n/* Other methods */\nfunc (op *dtConvOp) WriteHash(h hash.Hash) { fmt.Fprintf(h, \"(%v)\", op.Type()) }\n\nfunc (op *dtConvOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op *dtConvOp) String() string { return fmt.Sprintf(\"%v\", op.Type()) }\n\n// DiffWRT indicates if the op is differentiable with regards to the given number of inputs\n// returns []bool to indicate which input it is differentiable to\nfunc (op *dtConvOp) DiffWRT(inputs int) []bool { return []bool{true} }\n\n// SymDiff symbolically differentiates the op\nfunc (op *dtConvOp) SymDiff(inputs Nodes, output *Node, grad *Node) (retVal Nodes, err error) {\n\tdiffOp := &dtConvOp{\n\t\tinshape: grad.Shape().Clone(),\n\t\tfrom:    op.to,\n\t\tto:      op.from,\n\t}\n\tretVal = make(Nodes, op.Arity())\n\tretVal[0], err = ApplyOp(diffOp, grad)\n\treturn retVal, err\n}\n\n// UsePreallocDo executes the Op with a preallocated value in the result.s\nfunc (op *dtConvOp) UsePreallocDo(prealloc Value, inputs ...Value) (Value, error) {\n\ta := inputs[0]\n\tretVal := prealloc\n\tswitch {\n\tcase op.from == tensor.Float64 && op.to == tensor.Int:\n\t\tswitch aData := a.Data().(type) {\n\t\tcase []float64:\n\t\t\tretData := retVal.Data().([]int)\n\t\t\tfor i := range aData {\n\t\t\t\tretData[i] = int(aData[i])\n\t\t\t}\n\t\tcase float64:\n\t\t\tretVal = tensor.New(\n\t\t\t\ttensor.Of(tensor.Int),\n\t\t\t\ttensor.WithShape(1),\n\t\t\t\ttensor.WithBacking([]int{int(aData)}),\n\t\t\t)\n\t\t}\n\tcase op.from == tensor.Float32 && op.to == tensor.Int:\n\t\tswitch aData := a.Data().(type) {\n\t\tcase []float32:\n\t\t\tretData := retVal.Data().([]int)\n\t\t\tfor i := range aData {\n\t\t\t\tretData[i] = int(aData[i])\n\t\t\t}\n\t\tcase float32:\n\t\t\tretVal = tensor.New(\n\t\t\t\ttensor.Of(tensor.Int),\n\t\t\t\ttensor.WithShape(1),\n\t\t\t\ttensor.WithBacking([]int{int(aData)}),\n\t\t\t)\n\t\t}\n\tcase op.from == tensor.Int && op.to == tensor.Float64:\n\t\tswitch aData := a.Data().(type) {\n\t\tcase []int:\n\t\t\tretData := retVal.Data().([]float64)\n\t\t\tfor i := range aData {\n\t\t\t\tretData[i] = float64(aData[i])\n\t\t\t}\n\t\tcase int:\n\t\t\tretVal = tensor.New(\n\t\t\t\ttensor.Of(tensor.Float64),\n\t\t\t\ttensor.WithShape(1),\n\t\t\t\ttensor.WithBacking([]float64{float64(aData)}),\n\t\t\t)\n\t\t}\n\tcase op.from == tensor.Int && op.to == tensor.Float32:\n\t\tswitch aData := a.Data().(type) {\n\t\tcase []int:\n\t\t\tretData := retVal.Data().([]float32)\n\t\t\tfor i := range aData {\n\t\t\t\tretData[i] = float32(aData[i])\n\t\t\t}\n\t\tcase int:\n\t\t\tretVal = tensor.New(\n\t\t\t\ttensor.Of(tensor.Float32),\n\t\t\t\ttensor.WithShape(1),\n\t\t\t\ttensor.WithBacking([]float32{float32(aData)}),\n\t\t\t)\n\t\t}\n\tdefault:\n\t\treturn nil, errors.Errorf(\"Cannot do conversion %v\", op.Type())\n\t\t// TODO: other types\n\t}\n\n\treturn retVal, nil\n}\n"
        },
        {
          "name": "op_upsample.go",
          "type": "blob",
          "size": 5.458984375,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"hash\"\n\t\"hash/fnv\"\n\n\t\"gorgonia.org/tensor\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n)\n\ntype upsampleOp struct {\n\tstride int\n}\n\nfunc newUpsampleOp(inputShape tensor.Shape, stride int) *upsampleOp {\n\tupsampleop := &upsampleOp{\n\t\tstride: stride,\n\t}\n\treturn upsampleop\n}\n\n//Upsample2D -  simply upscaling Tensor by scale factor.\n/*\n\t1, 2\n\t3, 4\n\tconverts to\n\t1,1,2,2\n\t1,1,2,2\n\t3,3,4,4,\n\t3,3,4,4,\n*/\nfunc Upsample2D(x *Node, scale int) (*Node, error) {\n\tif scale < 1 {\n\t\treturn nil, errors.Errorf(\"Upsample scale %v does not make sense\", scale)\n\t}\n\txShape := x.Shape()\n\top := newUpsampleOp(xShape, scale-1)\n\tretVal, err := ApplyOp(op, x)\n\treturn retVal, err\n}\n\nfunc (op *upsampleOp) Arity() int {\n\n\treturn 1\n}\nfunc (op *upsampleOp) ReturnsPtr() bool { return false }\n\nfunc (op *upsampleOp) CallsExtern() bool { return false }\n\nfunc (op *upsampleOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, \"Upsample{}(stride: (%d))\", op.stride)\n}\nfunc (op *upsampleOp) Hashcode() uint32 {\n\th := fnv.New32a()\n\top.WriteHash(h)\n\treturn h.Sum32()\n}\n\nfunc (op *upsampleOp) String() string {\n\treturn fmt.Sprintf(\"Upsample{}(stride: (%d))\", op.stride)\n}\nfunc (op *upsampleOp) InferShape(inputs ...DimSizer) (tensor.Shape, error) {\n\ts := inputs[0].(tensor.Shape).Clone()\n\ts[2] = s[2] * (1 + op.stride)\n\ts[3] = s[3] * (1 + op.stride)\n\treturn s, nil\n}\nfunc (op *upsampleOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\tt := TensorType{Dims: 4, Of: a}\n\treturn hm.NewFnType(t, t)\n}\nfunc (op *upsampleOp) OverwritesInput() int { return -1 }\n\nfunc (op *upsampleOp) checkInput(inputs ...Value) (tensor.Tensor, error) {\n\tif err := checkArity(op, len(inputs)); err != nil {\n\t\treturn nil, err\n\t}\n\tvar in tensor.Tensor\n\tvar ok bool\n\tif in, ok = inputs[0].(tensor.Tensor); !ok {\n\t\treturn nil, errors.Errorf(\"Expected input to be a tensor\")\n\t}\n\n\tif in.Shape().Dims() != 4 {\n\t\treturn nil, errors.Errorf(\"Expected input to have 4 dimensions\")\n\t}\n\treturn in, nil\n}\n\nfunc (op *upsampleOp) Do(inputs ...Value) (retVal Value, err error) {\n\tvar in tensor.Tensor\n\tif in, err = op.checkInput(inputs...); err != nil {\n\t\treturn nil, err\n\t}\n\tinShp := in.Shape()\n\tb, c, h, w := inShp[0], inShp[1], inShp[2], inShp[3]\n\n\tout := tensor.New(tensor.Of(in.Dtype()), tensor.WithShape(b, c, h*(1+op.stride), w*(1+op.stride)), tensor.WithEngine(in.Engine()))\n\tfor bi := 0; bi < b; bi++ {\n\t\tfor ci := 0; ci < c; ci++ {\n\t\t\tfor hi := 0; hi < h; hi++ {\n\t\t\t\tfor wi := 0; wi < w; wi++ {\n\t\t\t\t\tval, err := in.At(bi, ci, hi, wi)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn nil, errors.Errorf(\"Error accessing input data at [%v, %v, %v, %v]\", bi, ci, hi, wi)\n\t\t\t\t\t}\n\t\t\t\t\thout := hi * (op.stride + 1)\n\t\t\t\t\twout := wi * (op.stride + 1)\n\t\t\t\t\tfor shi := 0; shi <= op.stride; shi++ {\n\t\t\t\t\t\tfor swi := 0; swi <= op.stride; swi++ {\n\t\t\t\t\t\t\tout.SetAt(val, bi, ci, hout+shi, wout+swi)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn out, nil\n}\n\nfunc (op *upsampleOp) DiffWRT(inputs int) []bool { return []bool{true} }\n\nfunc (op *upsampleOp) SymDiff(inputs Nodes, output, grad *Node) (retVal Nodes, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\tinput := inputs[0]\n\n\tvar op2 upsampleOp\n\top2 = *op\n\tdiff := &upsampleDiffOp{op2}\n\n\tvar ret *Node\n\tif ret, err = ApplyOp(diff, input, output, grad); err != nil {\n\t\treturn nil, err\n\t}\n\treturn Nodes{ret}, nil\n}\n\ntype upsampleDiffOp struct {\n\tupsampleOp\n}\n\nfunc (op *upsampleDiffOp) Arity() int { return 3 }\n\nfunc (op *upsampleDiffOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\tt := TensorType{Dims: 4, Of: a}\n\treturn hm.NewFnType(t, t, t, t)\n}\n\nfunc (op *upsampleDiffOp) InferShape(inputs ...DimSizer) (tensor.Shape, error) {\n\treturn inputs[0].(tensor.Shape).Clone(), nil\n}\n\nfunc (op *upsampleDiffOp) checkInput(inputs ...Value) (in, pooled, pooledGrad tensor.Tensor, err error) {\n\tif err = checkArity(op, len(inputs)); err != nil {\n\t\treturn\n\t}\n\n\tvar ok bool\n\tif in, ok = inputs[0].(tensor.Tensor); !ok {\n\t\terr = errors.Errorf(\"Expected input to be a tensor\")\n\t\treturn\n\t}\n\tif in.Shape().Dims() != 4 {\n\t\terr = errors.Errorf(\"Expected input to have 4 dimensions\")\n\t\treturn\n\t}\n\n\tif pooled, ok = inputs[1].(tensor.Tensor); !ok {\n\t\terr = errors.Errorf(\"Expected pooled to be a tensor\")\n\t\treturn\n\t}\n\n\tif pooledGrad, ok = inputs[2].(tensor.Tensor); !ok {\n\t\terr = errors.Errorf(\"Expected pooledGrad to be a tensor\")\n\t\treturn\n\t}\n\treturn\n}\n\nfunc (op *upsampleDiffOp) Do(inputs ...Value) (retVal Value, err error) {\n\tvar gradIn tensor.Tensor\n\tin, pooled, pooledGrad, err := op.checkInput(inputs...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tinsh := in.Shape()\n\tgradIn = tensor.New(tensor.Of(in.Dtype()), tensor.WithShape(in.Shape().Clone()...), tensor.WithEngine(in.Engine()))\n\tb, c, h, w := insh[0], insh[1], insh[2], insh[3]\n\tfor bi := 0; bi < b; bi++ {\n\t\tfor ci := 0; ci < c; ci++ {\n\t\t\tfor hi := 0; hi < h; hi++ {\n\t\t\t\tfor wi := 0; wi < w; wi++ {\n\t\t\t\t\tsumm := 0.\n\t\t\t\t\tfor sh := 0; sh <= op.stride; sh++ {\n\t\t\t\t\t\tfor sw := 0; sw <= op.stride; sw++ {\n\t\t\t\t\t\t\tval, err := pooledGrad.At(bi, ci, hi*(op.stride+1)+sh, wi*(op.stride+1)+sw)\n\t\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\t\treturn nil, errors.Errorf(\"Error accessing input data at [%v, %v, %v, %v]\", bi, ci, hi, wi)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif pooled.Dtype() == tensor.Float32 {\n\t\t\t\t\t\t\t\tsumm += float64(val.(float32))\n\t\t\t\t\t\t\t} else if pooled.Dtype() == tensor.Float64 {\n\t\t\t\t\t\t\t\tsumm += val.(float64)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tif pooled.Dtype() == tensor.Float32 {\n\t\t\t\t\t\tgradIn.SetAt(float32(summ), bi, ci, hi, wi)\n\t\t\t\t\t}\n\t\t\t\t\tif pooled.Dtype() == tensor.Float64 {\n\t\t\t\t\t\tgradIn.SetAt(summ, bi, ci, hi, wi)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn gradIn, nil\n}\n"
        },
        {
          "name": "op_upsample_test.go",
          "type": "blob",
          "size": 20.09375,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"gorgonia.org/tensor\"\n)\n\ntype testData struct {\n\tinsh    []int\n\tindata  []float64\n\toutsh   []int\n\toutdata []float64\n}\n\nvar data testData = testData{\n\tinsh:    []int{1, 5, 5, 8},\n\tindata:  []float64{3.588184118270874, 6.765244483947754, 8.703499794006348, 7.832754611968994, 7.205557346343994, 2.2351908683776855, 9.140625, 1.8968262672424316, 7.579838752746582, 2.0422589778900146, 1.7213886976242065, 8.008468627929688, 1.8242313861846924, 0.15505588054656982, 7.218662738800049, 6.959044456481934, 5.268470287322998, 3.0470921993255615, 6.904945373535156, 0.048324376344680786, 5.862209796905518, 9.752281188964844, 9.729859352111816, 6.917296409606934, 5.983068466186523, 8.5852689743042, 8.877313613891602, 0.06957779079675674, 8.360713958740234, 1.0679903030395508, 1.2100963592529297, 9.838610649108887, 2.814711809158325, 6.38685941696167, 8.260359764099121, 2.532329559326172, 7.589834213256836, 1.294305443763733, 0.6550663113594055, 0.8506288528442383, 2.1071043014526367, 8.425798416137695, 8.408304214477539, 7.394162178039551, 2.590216875076294, 0.7849066257476807, 2.676325559616089, 8.072299003601074, 3.4864327907562256, 2.422727346420288, 9.429472923278809, 2.553463935852051, 5.96559476852417, 7.482579231262207, 2.3580377101898193, 1.5595018863677979, 0.034589122980833054, 3.0223019123077393, 4.7403154373168945, 0.2549790143966675, 6.765586853027344, 6.91905403137207, 1.7624300718307495, 1.8117458820343018, 5.739025592803955, 7.67076301574707, 8.772150039672852, 2.1648266315460205, 2.8233001232147217, 3.455390214920044, 1.954986333847046, 6.2339935302734375, 8.180193901062012, 3.4818568229675293, 0.5501848459243774, 7.886703014373779, 3.614240884780884, 7.023927211761475, 0.4005505442619324, 2.0974090099334717, 6.166986465454102, 4.413336753845215, 7.7545647621154785, 5.656180381774902, 8.241524696350098, 0.1758895069360733, 7.527948379516602, 2.4103832244873047, 2.5865209102630615, 8.264690399169922, 4.0821614265441895, 4.711780548095703, 8.985666275024414, 3.614298105239868, 2.9979336261749268, 1.0938977003097534, 0.6748584508895874, 8.776729583740234, 7.792627811431885, 4.120683670043945, 8.067517280578613, 6.481339931488037, 3.7537691593170166, 1.5674660205841064, 4.622176647186279, 6.572947025299072, 6.97075891494751, 0.5755885243415833, 9.231983184814453, 5.7552490234375, 6.862707138061523, 6.564248085021973, 4.892410755157471, 5.318929672241211, 6.568413257598877, 9.133810043334961, 7.237424373626709, 4.397247314453125, 7.976856708526611, 2.3416335582733154, 6.3428730964660645, 9.161764144897461, 9.220541000366211, 8.534843444824219, 4.4301581382751465, 3.7176058292388916, 4.751829624176025, 5.455737113952637, 0.894568920135498, 5.666994571685791, 1.3768253326416016, 9.591104507446289, 8.1528902053833, 7.843255519866943, 1.170852541923523, 9.404593467712402, 1.0466225147247314, 2.0946621894836426, 8.859201431274414, 7.388158321380615, 0.26116275787353516, 4.805743217468262, 7.201430320739746, 3.692185878753662, 4.12822151184082, 5.931211471557617, 6.904507637023926, 6.349696636199951, 4.302095413208008, 6.546483516693115, 8.069499969482422, 2.374738931655884, 9.065333366394043, 9.402444839477539, 6.593384265899658, 8.194018363952637, 3.910661458969116, 2.1357905864715576, 0.8497279286384583, 1.6840051412582397, 7.609548568725586, 1.4399187564849854, 1.0644301176071167, 8.14090347290039, 2.919234275817871, 2.675851583480835, 3.0641536712646484, 6.6956963539123535, 3.498112678527832, 0.69023597240448, 1.1889065504074097, 8.269831657409668, 8.937707901000977, 1.6059719324111938, 0.15695418417453766, 0.9913957118988037, 7.1435065269470215, 4.237936973571777, 2.295494556427002, 5.945472240447998, 3.4239118099212646, 4.425090312957764, 3.2222511768341064, 1.6377434730529785, 4.766551971435547, 6.3335795402526855, 3.467780351638794, 5.677145004272461, 1.9361212253570557, 2.3081438541412354, 5.466902256011963, 4.283600807189941, 2.0228371620178223, 5.984248638153076, 6.023360252380371, 8.454516410827637, 0.48479247093200684, 1.7577219009399414, 7.09525203704834, 1.0326943397521973},\n\toutsh:   []int{1, 5, 10, 16},\n\toutdata: []float64{3.588184118270874, 3.588184118270874, 6.765244483947754, 6.765244483947754, 8.703499794006348, 8.703499794006348, 7.832754611968994, 7.832754611968994, 7.205557346343994, 7.205557346343994, 2.2351908683776855, 2.2351908683776855, 9.140625, 9.140625, 1.8968262672424316, 1.8968262672424316, 3.588184118270874, 3.588184118270874, 6.765244483947754, 6.765244483947754, 8.703499794006348, 8.703499794006348, 7.832754611968994, 7.832754611968994, 7.205557346343994, 7.205557346343994, 2.2351908683776855, 2.2351908683776855, 9.140625, 9.140625, 1.8968262672424316, 1.8968262672424316, 7.579838752746582, 7.579838752746582, 2.0422589778900146, 2.0422589778900146, 1.7213886976242065, 1.7213886976242065, 8.008468627929688, 8.008468627929688, 1.8242313861846924, 1.8242313861846924, 0.15505588054656982, 0.15505588054656982, 7.218662738800049, 7.218662738800049, 6.959044456481934, 6.959044456481934, 7.579838752746582, 7.579838752746582, 2.0422589778900146, 2.0422589778900146, 1.7213886976242065, 1.7213886976242065, 8.008468627929688, 8.008468627929688, 1.8242313861846924, 1.8242313861846924, 0.15505588054656982, 0.15505588054656982, 7.218662738800049, 7.218662738800049, 6.959044456481934, 6.959044456481934, 5.268470287322998, 5.268470287322998, 3.0470921993255615, 3.0470921993255615, 6.904945373535156, 6.904945373535156, 0.048324376344680786, 0.048324376344680786, 5.862209796905518, 5.862209796905518, 9.752281188964844, 9.752281188964844, 9.729859352111816, 9.729859352111816, 6.917296409606934, 6.917296409606934, 5.268470287322998, 5.268470287322998, 3.0470921993255615, 3.0470921993255615, 6.904945373535156, 6.904945373535156, 0.048324376344680786, 0.048324376344680786, 5.862209796905518, 5.862209796905518, 9.752281188964844, 9.752281188964844, 9.729859352111816, 9.729859352111816, 6.917296409606934, 6.917296409606934, 5.983068466186523, 5.983068466186523, 8.5852689743042, 8.5852689743042, 8.877313613891602, 8.877313613891602, 0.06957779079675674, 0.06957779079675674, 8.360713958740234, 8.360713958740234, 1.0679903030395508, 1.0679903030395508, 1.2100963592529297, 1.2100963592529297, 9.838610649108887, 9.838610649108887, 5.983068466186523, 5.983068466186523, 8.5852689743042, 8.5852689743042, 8.877313613891602, 8.877313613891602, 0.06957779079675674, 0.06957779079675674, 8.360713958740234, 8.360713958740234, 1.0679903030395508, 1.0679903030395508, 1.2100963592529297, 1.2100963592529297, 9.838610649108887, 9.838610649108887, 2.814711809158325, 2.814711809158325, 6.38685941696167, 6.38685941696167, 8.260359764099121, 8.260359764099121, 2.532329559326172, 2.532329559326172, 7.589834213256836, 7.589834213256836, 1.294305443763733, 1.294305443763733, 0.6550663113594055, 0.6550663113594055, 0.8506288528442383, 0.8506288528442383, 2.814711809158325, 2.814711809158325, 6.38685941696167, 6.38685941696167, 8.260359764099121, 8.260359764099121, 2.532329559326172, 2.532329559326172, 7.589834213256836, 7.589834213256836, 1.294305443763733, 1.294305443763733, 0.6550663113594055, 0.6550663113594055, 0.8506288528442383, 0.8506288528442383, 2.1071043014526367, 2.1071043014526367, 8.425798416137695, 8.425798416137695, 8.408304214477539, 8.408304214477539, 7.394162178039551, 7.394162178039551, 2.590216875076294, 2.590216875076294, 0.7849066257476807, 0.7849066257476807, 2.676325559616089, 2.676325559616089, 8.072299003601074, 8.072299003601074, 2.1071043014526367, 2.1071043014526367, 8.425798416137695, 8.425798416137695, 8.408304214477539, 8.408304214477539, 7.394162178039551, 7.394162178039551, 2.590216875076294, 2.590216875076294, 0.7849066257476807, 0.7849066257476807, 2.676325559616089, 2.676325559616089, 8.072299003601074, 8.072299003601074, 3.4864327907562256, 3.4864327907562256, 2.422727346420288, 2.422727346420288, 9.429472923278809, 9.429472923278809, 2.553463935852051, 2.553463935852051, 5.96559476852417, 5.96559476852417, 7.482579231262207, 7.482579231262207, 2.3580377101898193, 2.3580377101898193, 1.5595018863677979, 1.5595018863677979, 3.4864327907562256, 3.4864327907562256, 2.422727346420288, 2.422727346420288, 9.429472923278809, 9.429472923278809, 2.553463935852051, 2.553463935852051, 5.96559476852417, 5.96559476852417, 7.482579231262207, 7.482579231262207, 2.3580377101898193, 2.3580377101898193, 1.5595018863677979, 1.5595018863677979, 0.034589122980833054, 0.034589122980833054, 3.0223019123077393, 3.0223019123077393, 4.7403154373168945, 4.7403154373168945, 0.2549790143966675, 0.2549790143966675, 6.765586853027344, 6.765586853027344, 6.91905403137207, 6.91905403137207, 1.7624300718307495, 1.7624300718307495, 1.8117458820343018, 1.8117458820343018, 0.034589122980833054, 0.034589122980833054, 3.0223019123077393, 3.0223019123077393, 4.7403154373168945, 4.7403154373168945, 0.2549790143966675, 0.2549790143966675, 6.765586853027344, 6.765586853027344, 6.91905403137207, 6.91905403137207, 1.7624300718307495, 1.7624300718307495, 1.8117458820343018, 1.8117458820343018, 5.739025592803955, 5.739025592803955, 7.67076301574707, 7.67076301574707, 8.772150039672852, 8.772150039672852, 2.1648266315460205, 2.1648266315460205, 2.8233001232147217, 2.8233001232147217, 3.455390214920044, 3.455390214920044, 1.954986333847046, 1.954986333847046, 6.2339935302734375, 6.2339935302734375, 5.739025592803955, 5.739025592803955, 7.67076301574707, 7.67076301574707, 8.772150039672852, 8.772150039672852, 2.1648266315460205, 2.1648266315460205, 2.8233001232147217, 2.8233001232147217, 3.455390214920044, 3.455390214920044, 1.954986333847046, 1.954986333847046, 6.2339935302734375, 6.2339935302734375, 8.180193901062012, 8.180193901062012, 3.4818568229675293, 3.4818568229675293, 0.5501848459243774, 0.5501848459243774, 7.886703014373779, 7.886703014373779, 3.614240884780884, 3.614240884780884, 7.023927211761475, 7.023927211761475, 0.4005505442619324, 0.4005505442619324, 2.0974090099334717, 2.0974090099334717, 8.180193901062012, 8.180193901062012, 3.4818568229675293, 3.4818568229675293, 0.5501848459243774, 0.5501848459243774, 7.886703014373779, 7.886703014373779, 3.614240884780884, 3.614240884780884, 7.023927211761475, 7.023927211761475, 0.4005505442619324, 0.4005505442619324, 2.0974090099334717, 2.0974090099334717, 6.166986465454102, 6.166986465454102, 4.413336753845215, 4.413336753845215, 7.7545647621154785, 7.7545647621154785, 5.656180381774902, 5.656180381774902, 8.241524696350098, 8.241524696350098, 0.1758895069360733, 0.1758895069360733, 7.527948379516602, 7.527948379516602, 2.4103832244873047, 2.4103832244873047, 6.166986465454102, 6.166986465454102, 4.413336753845215, 4.413336753845215, 7.7545647621154785, 7.7545647621154785, 5.656180381774902, 5.656180381774902, 8.241524696350098, 8.241524696350098, 0.1758895069360733, 0.1758895069360733, 7.527948379516602, 7.527948379516602, 2.4103832244873047, 2.4103832244873047, 2.5865209102630615, 2.5865209102630615, 8.264690399169922, 8.264690399169922, 4.0821614265441895, 4.0821614265441895, 4.711780548095703, 4.711780548095703, 8.985666275024414, 8.985666275024414, 3.614298105239868, 3.614298105239868, 2.9979336261749268, 2.9979336261749268, 1.0938977003097534, 1.0938977003097534, 2.5865209102630615, 2.5865209102630615, 8.264690399169922, 8.264690399169922, 4.0821614265441895, 4.0821614265441895, 4.711780548095703, 4.711780548095703, 8.985666275024414, 8.985666275024414, 3.614298105239868, 3.614298105239868, 2.9979336261749268, 2.9979336261749268, 1.0938977003097534, 1.0938977003097534, 0.6748584508895874, 0.6748584508895874, 8.776729583740234, 8.776729583740234, 7.792627811431885, 7.792627811431885, 4.120683670043945, 4.120683670043945, 8.067517280578613, 8.067517280578613, 6.481339931488037, 6.481339931488037, 3.7537691593170166, 3.7537691593170166, 1.5674660205841064, 1.5674660205841064, 0.6748584508895874, 0.6748584508895874, 8.776729583740234, 8.776729583740234, 7.792627811431885, 7.792627811431885, 4.120683670043945, 4.120683670043945, 8.067517280578613, 8.067517280578613, 6.481339931488037, 6.481339931488037, 3.7537691593170166, 3.7537691593170166, 1.5674660205841064, 1.5674660205841064, 4.622176647186279, 4.622176647186279, 6.572947025299072, 6.572947025299072, 6.97075891494751, 6.97075891494751, 0.5755885243415833, 0.5755885243415833, 9.231983184814453, 9.231983184814453, 5.7552490234375, 5.7552490234375, 6.862707138061523, 6.862707138061523, 6.564248085021973, 6.564248085021973, 4.622176647186279, 4.622176647186279, 6.572947025299072, 6.572947025299072, 6.97075891494751, 6.97075891494751, 0.5755885243415833, 0.5755885243415833, 9.231983184814453, 9.231983184814453, 5.7552490234375, 5.7552490234375, 6.862707138061523, 6.862707138061523, 6.564248085021973, 6.564248085021973, 4.892410755157471, 4.892410755157471, 5.318929672241211, 5.318929672241211, 6.568413257598877, 6.568413257598877, 9.133810043334961, 9.133810043334961, 7.237424373626709, 7.237424373626709, 4.397247314453125, 4.397247314453125, 7.976856708526611, 7.976856708526611, 2.3416335582733154, 2.3416335582733154, 4.892410755157471, 4.892410755157471, 5.318929672241211, 5.318929672241211, 6.568413257598877, 6.568413257598877, 9.133810043334961, 9.133810043334961, 7.237424373626709, 7.237424373626709, 4.397247314453125, 4.397247314453125, 7.976856708526611, 7.976856708526611, 2.3416335582733154, 2.3416335582733154, 6.3428730964660645, 6.3428730964660645, 9.161764144897461, 9.161764144897461, 9.220541000366211, 9.220541000366211, 8.534843444824219, 8.534843444824219, 4.4301581382751465, 4.4301581382751465, 3.7176058292388916, 3.7176058292388916, 4.751829624176025, 4.751829624176025, 5.455737113952637, 5.455737113952637, 6.3428730964660645, 6.3428730964660645, 9.161764144897461, 9.161764144897461, 9.220541000366211, 9.220541000366211, 8.534843444824219, 8.534843444824219, 4.4301581382751465, 4.4301581382751465, 3.7176058292388916, 3.7176058292388916, 4.751829624176025, 4.751829624176025, 5.455737113952637, 5.455737113952637, 0.894568920135498, 0.894568920135498, 5.666994571685791, 5.666994571685791, 1.3768253326416016, 1.3768253326416016, 9.591104507446289, 9.591104507446289, 8.1528902053833, 8.1528902053833, 7.843255519866943, 7.843255519866943, 1.170852541923523, 1.170852541923523, 9.404593467712402, 9.404593467712402, 0.894568920135498, 0.894568920135498, 5.666994571685791, 5.666994571685791, 1.3768253326416016, 1.3768253326416016, 9.591104507446289, 9.591104507446289, 8.1528902053833, 8.1528902053833, 7.843255519866943, 7.843255519866943, 1.170852541923523, 1.170852541923523, 9.404593467712402, 9.404593467712402, 1.0466225147247314, 1.0466225147247314, 2.0946621894836426, 2.0946621894836426, 8.859201431274414, 8.859201431274414, 7.388158321380615, 7.388158321380615, 0.26116275787353516, 0.26116275787353516, 4.805743217468262, 4.805743217468262, 7.201430320739746, 7.201430320739746, 3.692185878753662, 3.692185878753662, 1.0466225147247314, 1.0466225147247314, 2.0946621894836426, 2.0946621894836426, 8.859201431274414, 8.859201431274414, 7.388158321380615, 7.388158321380615, 0.26116275787353516, 0.26116275787353516, 4.805743217468262, 4.805743217468262, 7.201430320739746, 7.201430320739746, 3.692185878753662, 3.692185878753662, 4.12822151184082, 4.12822151184082, 5.931211471557617, 5.931211471557617, 6.904507637023926, 6.904507637023926, 6.349696636199951, 6.349696636199951, 4.302095413208008, 4.302095413208008, 6.546483516693115, 6.546483516693115, 8.069499969482422, 8.069499969482422, 2.374738931655884, 2.374738931655884, 4.12822151184082, 4.12822151184082, 5.931211471557617, 5.931211471557617, 6.904507637023926, 6.904507637023926, 6.349696636199951, 6.349696636199951, 4.302095413208008, 4.302095413208008, 6.546483516693115, 6.546483516693115, 8.069499969482422, 8.069499969482422, 2.374738931655884, 2.374738931655884, 9.065333366394043, 9.065333366394043, 9.402444839477539, 9.402444839477539, 6.593384265899658, 6.593384265899658, 8.194018363952637, 8.194018363952637, 3.910661458969116, 3.910661458969116, 2.1357905864715576, 2.1357905864715576, 0.8497279286384583, 0.8497279286384583, 1.6840051412582397, 1.6840051412582397, 9.065333366394043, 9.065333366394043, 9.402444839477539, 9.402444839477539, 6.593384265899658, 6.593384265899658, 8.194018363952637, 8.194018363952637, 3.910661458969116, 3.910661458969116, 2.1357905864715576, 2.1357905864715576, 0.8497279286384583, 0.8497279286384583, 1.6840051412582397, 1.6840051412582397, 7.609548568725586, 7.609548568725586, 1.4399187564849854, 1.4399187564849854, 1.0644301176071167, 1.0644301176071167, 8.14090347290039, 8.14090347290039, 2.919234275817871, 2.919234275817871, 2.675851583480835, 2.675851583480835, 3.0641536712646484, 3.0641536712646484, 6.6956963539123535, 6.6956963539123535, 7.609548568725586, 7.609548568725586, 1.4399187564849854, 1.4399187564849854, 1.0644301176071167, 1.0644301176071167, 8.14090347290039, 8.14090347290039, 2.919234275817871, 2.919234275817871, 2.675851583480835, 2.675851583480835, 3.0641536712646484, 3.0641536712646484, 6.6956963539123535, 6.6956963539123535, 3.498112678527832, 3.498112678527832, 0.69023597240448, 0.69023597240448, 1.1889065504074097, 1.1889065504074097, 8.269831657409668, 8.269831657409668, 8.937707901000977, 8.937707901000977, 1.6059719324111938, 1.6059719324111938, 0.15695418417453766, 0.15695418417453766, 0.9913957118988037, 0.9913957118988037, 3.498112678527832, 3.498112678527832, 0.69023597240448, 0.69023597240448, 1.1889065504074097, 1.1889065504074097, 8.269831657409668, 8.269831657409668, 8.937707901000977, 8.937707901000977, 1.6059719324111938, 1.6059719324111938, 0.15695418417453766, 0.15695418417453766, 0.9913957118988037, 0.9913957118988037, 7.1435065269470215, 7.1435065269470215, 4.237936973571777, 4.237936973571777, 2.295494556427002, 2.295494556427002, 5.945472240447998, 5.945472240447998, 3.4239118099212646, 3.4239118099212646, 4.425090312957764, 4.425090312957764, 3.2222511768341064, 3.2222511768341064, 1.6377434730529785, 1.6377434730529785, 7.1435065269470215, 7.1435065269470215, 4.237936973571777, 4.237936973571777, 2.295494556427002, 2.295494556427002, 5.945472240447998, 5.945472240447998, 3.4239118099212646, 3.4239118099212646, 4.425090312957764, 4.425090312957764, 3.2222511768341064, 3.2222511768341064, 1.6377434730529785, 1.6377434730529785, 4.766551971435547, 4.766551971435547, 6.3335795402526855, 6.3335795402526855, 3.467780351638794, 3.467780351638794, 5.677145004272461, 5.677145004272461, 1.9361212253570557, 1.9361212253570557, 2.3081438541412354, 2.3081438541412354, 5.466902256011963, 5.466902256011963, 4.283600807189941, 4.283600807189941, 4.766551971435547, 4.766551971435547, 6.3335795402526855, 6.3335795402526855, 3.467780351638794, 3.467780351638794, 5.677145004272461, 5.677145004272461, 1.9361212253570557, 1.9361212253570557, 2.3081438541412354, 2.3081438541412354, 5.466902256011963, 5.466902256011963, 4.283600807189941, 4.283600807189941, 2.0228371620178223, 2.0228371620178223, 5.984248638153076, 5.984248638153076, 6.023360252380371, 6.023360252380371, 8.454516410827637, 8.454516410827637, 0.48479247093200684, 0.48479247093200684, 1.7577219009399414, 1.7577219009399414, 7.09525203704834, 7.09525203704834, 1.0326943397521973, 1.0326943397521973, 2.0228371620178223, 2.0228371620178223, 5.984248638153076, 5.984248638153076, 6.023360252380371, 6.023360252380371, 8.454516410827637, 8.454516410827637, 0.48479247093200684, 0.48479247093200684, 1.7577219009399414, 1.7577219009399414, 7.09525203704834, 7.09525203704834, 1.0326943397521973, 1.0326943397521973},\n}\n\nfunc TestUpsampleSimple(t *testing.T) {\n\ttt := tensor.New(tensor.Of(tensor.Float64), tensor.WithShape(1, 1, 3, 3), tensor.WithBacking([]float64{1, 2, 3, 4, 5, 6, 7, 8, 9}))\n\tupop := newUpsampleOp(tt.Shape(), 2)\n\tfmt.Println(tt)\n\tout, err := upop.Do(tt)\n\tt.Log(out)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n}\n\nfunc TestUpsampleWithData(t *testing.T) {\n\ttt := tensor.New(tensor.Of(tensor.Float64), tensor.WithShape(data.insh...), tensor.WithBacking(data.indata))\n\texpected := tensor.New(tensor.Of(tensor.Float64), tensor.WithShape(data.outsh...), tensor.WithBacking(data.outdata))\n\n\tg := NewGraph()\n\tinp := NewTensor(g, tensor.Float64, 4, WithShape(data.insh...), WithName(\"inp\"))\n\tout := Must(Upsample2D(inp, 2))\n\n\tvm := NewTapeMachine(g)\n\tif err := Let(inp, tt); err != nil {\n\t\tpanic(err)\n\t}\n\tvm.RunAll()\n\t// fmt.Println(out.Value())\n\tvm.Close()\n\tassert.Equal(t, expected.Data(), out.Value().(*tensor.Dense).Data(), \"Output is not equal to expected value\")\n\n}\n"
        },
        {
          "name": "op_yolo.go",
          "type": "blob",
          "size": 23.6357421875,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"hash\"\n\t\"image\"\n\t\"math\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/chewxy/math32\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\ntype yoloOp struct {\n\tanchors     []float32\n\tmasks       []int\n\tignoreTresh float32\n\tdimensions  int\n\tnumClasses  int\n\ttrainMode   bool\n}\n\nfunc newYoloOp(anchors []float32, masks []int, netSize, numClasses int, ignoreTresh float32, trainMode bool) *yoloOp {\n\tyoloOp := &yoloOp{\n\t\tanchors:     anchors,\n\t\tdimensions:  netSize,\n\t\tnumClasses:  numClasses,\n\t\tignoreTresh: ignoreTresh,\n\t\tmasks:       masks,\n\t\ttrainMode:   trainMode,\n\t}\n\treturn yoloOp\n}\n\n// YOLOv3 https://arxiv.org/abs/1804.02767\nfunc YOLOv3(input *Node, anchors []float32, masks []int, netSize, numClasses int, ignoreTresh float32, targets ...*Node) (*Node, error) {\n\tif len(targets) > 0 {\n\t\tinputSlice, err := Slice(input, S(0), nil, nil, nil)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"Can't prepare YOLOv3 node for training mode due Slice() on input node error\")\n\t\t}\n\t\ttargetsSlice, err := Slice(targets[0], S(0), nil, nil, nil)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"Can't prepare YOLOv3 node for training mode due Slice() on first node in target nodes slice error\")\n\t\t}\n\t\tinputTargetConcat, err := Concat(0, inputSlice, targetsSlice)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"Can't prepare YOLOv3 node for training mode due Concat() error\")\n\t\t}\n\t\tconcatShp := inputTargetConcat.Shape()\n\t\tinputTargetConcat, err = Reshape(inputTargetConcat, []int{1, concatShp[0], concatShp[1], concatShp[2]})\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"Can't prepare YOLOv3 node for training mode due Reshape() error\")\n\t\t}\n\t\top := newYoloOp(anchors, masks, netSize, numClasses, ignoreTresh, true)\n\t\treturn ApplyOp(op, inputTargetConcat)\n\t}\n\top := newYoloOp(anchors, masks, netSize, numClasses, ignoreTresh, false)\n\treturn ApplyOp(op, input)\n}\n\nfunc (op *yoloOp) Arity() int {\n\treturn 1\n}\n\nfunc (op *yoloOp) ReturnsPtr() bool { return false }\n\nfunc (op *yoloOp) CallsExtern() bool { return false }\n\nfunc (op *yoloOp) WriteHash(h hash.Hash) {\n\tfmt.Fprintf(h, \"YOLO{}(anchors: (%v))\", op.anchors)\n}\nfunc (op *yoloOp) Hashcode() uint32 { return simpleHash(op) }\n\nfunc (op *yoloOp) String() string {\n\treturn fmt.Sprintf(\"YOLO{}(anchors: (%v))\", op.anchors)\n}\nfunc (op *yoloOp) InferShape(inputs ...DimSizer) (tensor.Shape, error) {\n\tshp := inputs[0].(tensor.Shape)\n\tif len(shp) < 4 {\n\t\treturn nil, fmt.Errorf(\"InferShape() for YOLO must contain 4 dimensions\")\n\t}\n\ts := shp.Clone()\n\tif op.trainMode {\n\t\treturn []int{s[0], s[2] * s[3] * len(op.masks), (s[1] - 1) / len(op.masks)}, nil\n\t}\n\treturn []int{s[0], s[2] * s[3] * len(op.masks), s[1] / len(op.masks)}, nil\n}\n\nfunc (op *yoloOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\tt := newTensorType(4, a)\n\to := newTensorType(3, a)\n\treturn hm.NewFnType(t, o)\n}\n\nfunc (op *yoloOp) OverwritesInput() int { return -1 }\n\nfunc (op *yoloOp) checkInput(inputs ...Value) (tensor.Tensor, error) {\n\tif err := checkArity(op, len(inputs)); err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't check arity for YOLO operation\")\n\t}\n\tvar in tensor.Tensor\n\tvar ok bool\n\tif in, ok = inputs[0].(tensor.Tensor); !ok {\n\t\treturn nil, errors.Errorf(\"Can't check YOLO input: expected input has to be a tensor\")\n\t}\n\tif in.Shape().Dims() != 4 {\n\t\treturn nil, errors.Errorf(\"Can't check YOLO input: expected input must have 4 dimensions\")\n\t}\n\treturn in, nil\n}\n\nfunc sigmoidSlice(v tensor.View) error {\n\tswitch v.Dtype() {\n\tcase Float32:\n\t\t_, err := v.Apply(_sigmoidf32, tensor.UseUnsafe())\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"Can't apply _sigmoidf32 as activation function to YOLO operation\")\n\t\t}\n\tcase Float64:\n\t\t_, err := v.Apply(_sigmoidf64, tensor.UseUnsafe())\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"Can't apply _sigmoidf64 as activation function to YOLO operation\")\n\t\t}\n\tdefault:\n\t\treturn fmt.Errorf(\"Unsupported numeric type for YOLO sigmoid function. Please use float64 or float32\")\n\t}\n\treturn nil\n}\n\nfunc expSlice(v tensor.View) error {\n\tswitch v.Dtype() {\n\tcase Float32:\n\t\t_, err := v.Apply(math32.Exp, tensor.UseUnsafe())\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"Can't apply exp32 to YOLO operation\")\n\t\t}\n\tcase Float64:\n\t\t_, err := v.Apply(math.Exp, tensor.UseUnsafe())\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"Can't apply exp64 to YOLO operation\")\n\t\t}\n\tdefault:\n\t\treturn fmt.Errorf(\"Unsupported numeric type for YOLO for exp function. Please use float64 or float32\")\n\t}\n\treturn nil\n}\n\nfunc (op *yoloOp) Do(inputs ...Value) (retVal Value, err error) {\n\tif !op.trainMode {\n\t\tinputTensor, err := op.checkInput(inputs...)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"Can't check YOLO input\")\n\t\t}\n\t\tbatchSize := inputTensor.Shape()[0]\n\t\tstride := op.dimensions / inputTensor.Shape()[2]\n\t\tgridSize := inputTensor.Shape()[2]\n\t\tbboxAttributes := 5 + op.numClasses\n\t\tnumAnchors := len(op.anchors) / 2\n\t\tcurrentAnchors := []float32{}\n\t\tfor i := range op.masks {\n\t\t\tif op.masks[i] >= numAnchors {\n\t\t\t\treturn nil, fmt.Errorf(\"Incorrect mask %v for anchors in YOLO layer\", op.masks)\n\t\t\t}\n\t\t\tcurrentAnchors = append(currentAnchors, op.anchors[i*2], op.anchors[i*2+1])\n\t\t}\n\t\treturn op.evaluateYOLO_f32(inputTensor, batchSize, stride, gridSize, bboxAttributes, len(op.masks), currentAnchors)\n\t}\n\n\t// Training mode\n\tinput, err := op.checkInput(inputs...)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't check YOLO input [Training mode]\")\n\t}\n\tinv, err := input.Slice(nil, S(0, input.Shape()[1]-1), nil, nil)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't prepare slice in YOLO (1) [Training mode]\")\n\t}\n\tnumTargets, err := input.At(0, input.Shape()[1]-1, 0, 0)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't select targets from YOLO input [Training mode]\")\n\t}\n\n\tbatchSize := input.Shape()[0]\n\tstride := op.dimensions / input.Shape()[2]\n\tgrid := input.Shape()[2]\n\tbboxAttributes := 5 + op.numClasses\n\tnumAnchors := len(op.masks)\n\tcurrentAnchors := []float32{}\n\tfor i := range op.masks {\n\t\tif op.masks[i] >= (len(op.anchors) / 2) {\n\t\t\treturn nil, fmt.Errorf(\"Incorrect mask %v for anchors in YOLO layer [Training mode]\", op.masks)\n\t\t}\n\t\tcurrentAnchors = append(currentAnchors, op.anchors[i*2], op.anchors[i*2+1])\n\t}\n\n\ttargets := []float32{}\n\tinputNumericType := input.Dtype()\n\n\tswitch inputNumericType {\n\tcase Float32:\n\t\tlt := int(numTargets.(float32))\n\t\ttargets = make([]float32, lt)\n\t\tfor i := 1; i <= lt; i++ {\n\t\t\tvalAt, err := input.At(0, input.Shape()[1]-1, i/grid, i%grid)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"Can't select float32 targets for YOLO [Training mode]\")\n\t\t\t}\n\t\t\ttargets[i-1] = valAt.(float32)\n\t\t}\n\t\tbreak\n\tcase Float64:\n\t\tlt := int(numTargets.(float64))\n\t\ttargets = make([]float32, lt)\n\t\tfor i := 1; i <= lt; i++ {\n\t\t\tvalAt, err := input.At(0, input.Shape()[1]-1, i/grid, i%grid)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"Can't select float64 targets for YOLO [Training mode]\")\n\t\t\t}\n\t\t\ttargets[i-1] = float32(valAt.(float64))\n\t\t}\n\t\tbreak\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"Unsupported numeric type while preparing targets for YOLO Please use float64 or float32 [Training mode]\")\n\t}\n\n\tinput = inv.Materialize()\n\n\terr = input.Reshape(batchSize, bboxAttributes*numAnchors, grid*grid)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't reshape in YOLO (1) [Training mode]\")\n\t}\n\terr = input.T(0, 2, 1)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't safely transponse in YOLO (1) [Training mode]\")\n\t}\n\terr = input.Transpose()\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't transponse in YOLO (1) [Training mode]\")\n\t}\n\terr = input.Reshape(batchSize, grid*grid*numAnchors, bboxAttributes)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't reshape in YOLO (2) [Training mode]\")\n\t}\n\n\tclonedInput := input.Clone().(tensor.Tensor)\n\toutyolo, err := op.evaluateYOLO_f32(input, batchSize, stride, grid, bboxAttributes, numAnchors, currentAnchors)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't evaluate YOLO operation [Training mode]\")\n\t}\n\n\tyoloNumericType := outyolo.Dtype()\n\tresult := &tensor.Dense{}\n\n\tswitch yoloNumericType {\n\tcase Float32:\n\t\tyoloBBoxesF32 := make([]float32, 0)\n\t\tinputF32 := make([]float32, 0)\n\t\terr = clonedInput.Reshape(input.Shape()[0] * input.Shape()[1] * input.Shape()[2])\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"Can't reshape in YOLO (3) [Training mode]\")\n\t\t}\n\t\terr = outyolo.Reshape(outyolo.Shape()[0] * outyolo.Shape()[1] * outyolo.Shape()[2])\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"Can't reshape in YOLO (3) [Training mode]\")\n\t\t}\n\t\tfor i := 0; i < outyolo.Shape()[0]; i++ {\n\t\t\tbuf, err := outyolo.At(i)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"Can't select value from YOLO output [Training mode]\")\n\t\t\t}\n\t\t\tyoloBBoxesF32 = append(yoloBBoxesF32, buf.(float32))\n\t\t\tbuf, err = clonedInput.At(i)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"Can't select value from YOLO bounding boxes [Training mode]\")\n\t\t\t}\n\t\t\tinputF32 = append(inputF32, buf.(float32))\n\t\t}\n\t\tpreparedOut := prepareOutputYOLO_f32(inputF32, yoloBBoxesF32, targets, op.anchors, op.masks, op.numClasses, op.dimensions, grid, op.ignoreTresh)\n\t\tresult = tensor.New(tensor.WithShape(1, grid*grid*len(op.masks), 5+op.numClasses), tensor.Of(tensor.Float32), tensor.WithBacking(preparedOut))\n\t\tbreak\n\tcase Float64:\n\t\t// @todo\n\t\treturn nil, fmt.Errorf(\"float64 numeric type is not implemented for preparing result for YOLO [Training mode]\")\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"Unsupported numeric type for preparing result for YOLO. Please use float64 or float32 [Training mode]\")\n\t}\n\n\treturn result, nil\n}\n\nfunc (op *yoloOp) evaluateYOLO_f32(input tensor.Tensor, batchSize, stride, grid, bboxAttrs, numAnchors int, currentAnchors []float32) (retVal tensor.Tensor, err error) {\n\n\tinputNumericType := input.Dtype()\n\tif inputNumericType != Float32 {\n\t\treturn nil, fmt.Errorf(\"evaluateYOLO_f32() called with input tensor of type %v. Float32 is required\", inputNumericType)\n\t}\n\n\terr = input.Reshape(batchSize, bboxAttrs*numAnchors, grid*grid)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't make reshape grid^2 for YOLO\")\n\t}\n\n\terr = input.T(0, 2, 1)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't safely transponse input for YOLO\")\n\t}\n\terr = input.Transpose()\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't transponse input for YOLO\")\n\t}\n\terr = input.Reshape(batchSize, grid*grid*numAnchors, bboxAttrs)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't reshape bbox for YOLO\")\n\t}\n\n\t// Activation of x, y, and objects via sigmoid function\n\tslXY, err := input.Slice(nil, nil, S(0, 2))\n\terr = sigmoidSlice(slXY)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't activate XY\")\n\t}\n\tslClasses, err := input.Slice(nil, nil, S(4, 5+op.numClasses))\n\terr = sigmoidSlice(slClasses)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't activate classes\")\n\t}\n\n\tstep := grid * numAnchors\n\tfor i := 0; i < grid; i++ {\n\n\t\tvy, err := input.Slice(nil, S(i*step, i*step+step), S(1))\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"Can't slice while doing steps for grid\")\n\t\t}\n\n\t\t_, err = tensor.Add(vy, float32(i), tensor.UseUnsafe())\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"Can't do tensor.Add(...) for float32; (1)\")\n\t\t}\n\n\t\tfor n := 0; n < numAnchors; n++ {\n\t\t\tanchorsSlice, err := input.Slice(nil, S(i*numAnchors+n, input.Shape()[1], step), S(0))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"Can't slice anchors while doing steps for grid\")\n\t\t\t}\n\t\t\t_, err = tensor.Add(anchorsSlice, float32(i), tensor.UseUnsafe())\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"Can't do tensor.Add(...) for float32; (1)\")\n\t\t\t}\n\t\t}\n\n\t}\n\n\tanchors := []float32{}\n\tfor i := 0; i < grid*grid; i++ {\n\t\tanchors = append(anchors, currentAnchors...)\n\t}\n\n\tanchorsTensor := tensor.New(tensor.Of(inputNumericType), tensor.WithShape(1, grid*grid*numAnchors, 2))\n\tfor i := range anchors {\n\t\tanchorsTensor.Set(i, anchors[i])\n\t}\n\n\t_, err = tensor.Div(anchorsTensor, float32(stride), tensor.UseUnsafe())\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't do tensor.Div(...) for float32\")\n\t}\n\n\tvhw, err := input.Slice(nil, nil, S(2, 4))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't do slice on input S(2,4)\")\n\t}\n\n\t_, err = vhw.Apply(math32.Exp, tensor.UseUnsafe())\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't apply exp32 to YOLO operation\")\n\t}\n\n\t_, err = tensor.Mul(vhw, anchorsTensor, tensor.UseUnsafe())\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't do tensor.Mul(...) for anchors\")\n\t}\n\n\tvv, err := input.Slice(nil, nil, S(0, 4))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't do slice on input S(0,4)\")\n\t}\n\n\t_, err = tensor.Mul(vv, float32(stride), tensor.UseUnsafe())\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't do tensor.Mul(...) for float32\")\n\t}\n\n\treturn input, nil\n}\n\nfunc iou_f32(r1, r2 image.Rectangle) float32 {\n\tintersection := r1.Intersect(r2)\n\tinterArea := intersection.Dx() * intersection.Dy()\n\tr1Area := r1.Dx() * r1.Dy()\n\tr2Area := r2.Dx() * r2.Dy()\n\treturn float32(interArea) / float32(r1Area+r2Area-interArea)\n}\n\nfunc getBestIOU_f32(input, target []float32, numClasses, dims int) [][]float32 {\n\tious := make([][]float32, 0)\n\timgsize := float32(dims)\n\tfor i := 0; i < len(input); i = i + numClasses + 5 {\n\t\tious = append(ious, []float32{0, -1})\n\t\tr1 := rectifyBox_f32(input[i], input[i+1], input[i+2], input[i+3], dims)\n\t\tfor j := 0; j < len(target); j = j + 5 {\n\t\t\tr2 := rectifyBox_f32(target[j+1]*imgsize, target[j+2]*imgsize, target[j+3]*imgsize, target[j+4]*imgsize, dims)\n\t\t\tcuriou := iou_f32(r1, r2)\n\t\t\tif curiou > ious[i/(5+numClasses)][0] {\n\t\t\t\tious[i/(5+numClasses)][0] = curiou\n\t\t\t\tious[i/(5+numClasses)][1] = float32(j / 5)\n\t\t\t}\n\t\t}\n\t}\n\treturn ious\n}\n\nfunc getBestAnchors_f32(target []float32, anchors []float32, masks []int, dims int, gridSize float32) [][]int {\n\tbestAnchors := make([][]int, len(target)/5)\n\timgsize := float32(dims)\n\tfor j := 0; j < len(target); j = j + 5 {\n\t\ttargetRect := rectifyBox_f32(0, 0, target[j+3]*imgsize, target[j+4]*imgsize, dims) //not absolutely confident in rectangle sizes\n\t\tbestIOU := float32(0.0)\n\t\tbestAnchors[j/5] = make([]int, 3)\n\t\tfor i := 0; i < len(anchors); i = i + 2 {\n\t\t\tanchorRect := rectifyBox_f32(0, 0, anchors[i], anchors[i+1], dims)\n\t\t\tcurrentIOU := iou_f32(anchorRect, targetRect)\n\t\t\tif currentIOU >= bestIOU {\n\t\t\t\tbestAnchors[j/5][0] = i\n\t\t\t\tbestIOU = currentIOU\n\t\t\t}\n\t\t}\n\t\tbestAnchors[j/5][0] = findIntElement(masks, bestAnchors[j/5][0]/2)\n\t\tif bestAnchors[j/5][0] != -1 {\n\t\t\tbestAnchors[j/5][1] = int(target[j+1] * gridSize)\n\t\t\tbestAnchors[j/5][2] = int(target[j+2] * gridSize)\n\t\t}\n\t}\n\treturn bestAnchors\n}\n\nfunc prepareOutputYOLO_f32(input, yoloBoxes, target, anchors []float32, masks []int, numClasses, dims, gridSize int, ignoreTresh float32) []float32 {\n\tyoloBBoxes := make([]float32, len(yoloBoxes))\n\tgridSizeF32 := float32(gridSize)\n\tbestAnchors := getBestAnchors_f32(target, anchors, masks, dims, gridSizeF32)\n\tbestIous := getBestIOU_f32(yoloBoxes, target, numClasses, dims)\n\tfor i := 0; i < len(yoloBoxes); i = i + (5 + numClasses) {\n\t\tif bestIous[i/(5+numClasses)][0] <= ignoreTresh {\n\t\t\tyoloBBoxes[i+4] = bceLoss32(0, yoloBoxes[i+4])\n\t\t}\n\t}\n\tfor i := 0; i < len(bestAnchors); i++ {\n\t\tif bestAnchors[i][0] != -1 {\n\t\t\tscale := (2 - target[i*5+3]*target[i*5+4])\n\t\t\tgiInt := bestAnchors[i][1]\n\t\t\tgjInt := bestAnchors[i][2]\n\t\t\tgx := invsigm32(target[i*5+1]*gridSizeF32 - float32(giInt))\n\t\t\tgy := invsigm32(target[i*5+2]*gridSizeF32 - float32(gjInt))\n\t\t\tgw := math32.Log(target[i*5+3]/anchors[bestAnchors[i][0]] + 1e-16)\n\t\t\tgh := math32.Log(target[i*5+4]/anchors[bestAnchors[i][0]+1] + 1e-16)\n\t\t\tbboxIdx := gjInt*gridSize*len(masks) + giInt*len(masks) + bestAnchors[i][0]\n\t\t\tyoloBBoxes[bboxIdx] = mseLoss32(gx, input[bboxIdx], scale)\n\t\t\tyoloBBoxes[bboxIdx+1] = mseLoss32(gy, input[bboxIdx+1], scale)\n\t\t\tyoloBBoxes[bboxIdx+2] = mseLoss32(gw, input[bboxIdx+2], scale)\n\t\t\tyoloBBoxes[bboxIdx+3] = mseLoss32(gh, input[bboxIdx+3], scale)\n\t\t\tyoloBBoxes[bboxIdx+4] = bceLoss32(1, yoloBoxes[bboxIdx+4])\n\t\t\tfor j := 0; j < numClasses; j++ {\n\t\t\t\tif j == int(target[i]) {\n\t\t\t\t\tyoloBBoxes[bboxIdx+5+j] = bceLoss32(1, yoloBoxes[bboxIdx+4])\n\t\t\t\t} else {\n\t\t\t\t\tyoloBBoxes[bboxIdx+5+j] = bceLoss32(0, yoloBoxes[bboxIdx+4])\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn yoloBBoxes\n}\n\nfunc findIntElement(arr []int, ele int) int {\n\tfor i := range arr {\n\t\tif arr[i] == ele {\n\t\t\treturn i\n\t\t}\n\t}\n\treturn -1\n}\n\nfunc rectifyBox_f32(x, y, h, w float32, imgSize int) image.Rectangle {\n\treturn image.Rect(maxInt(int(x-w/2), 0), maxInt(int(y-h/2), 0), minInt(int(x+w/2+1), imgSize), minInt(int(y+h/2+1), imgSize))\n}\n\nfunc bceLoss32(target, pred float32) float32 {\n\tif target == 1.0 {\n\t\treturn -(math32.Log(pred + 1e-16))\n\t}\n\treturn -(math32.Log((1.0 - pred) + 1e-16))\n}\n\nfunc mseLoss32(target, pred, scale float32) float32 {\n\treturn math32.Pow(scale*(target-pred), 2) / 2.0\n}\n\nfunc invsigm32(target float32) float32 {\n\treturn -math32.Log(1-target+1e-16) + math32.Log(target+1e-16)\n}\n\nfunc (op *yoloOp) evaluateYOLO_f64(input tensor.Tensor, batchSize, stride, grid, bboxAttrs, numAnchors int, currentAnchors []float64) (retVal tensor.Tensor, err error) {\n\tinputNumericType := input.Dtype()\n\tif inputNumericType != Float64 {\n\t\treturn nil, fmt.Errorf(\"evaluateYOLO_f64() called with input tensor of type %v. Float64 is required\", inputNumericType)\n\t}\n\terr = input.Reshape(batchSize, bboxAttrs*numAnchors, grid*grid)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't make reshape grid^2 for YOLO\")\n\t}\n\terr = input.T(0, 2, 1)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't safely transponse input for YOLO\")\n\t}\n\terr = input.Transpose()\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't transponse input for YOLO\")\n\t}\n\terr = input.Reshape(batchSize, grid*grid*numAnchors, bboxAttrs)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't reshape bbox for YOLO\")\n\t}\n\n\t// Activation of x, y, and objects via sigmoid function\n\tslXY, err := input.Slice(nil, nil, S(0, 2))\n\terr = sigmoidSlice(slXY)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't activate XY\")\n\t}\n\tslClasses, err := input.Slice(nil, nil, S(4, 5+op.numClasses))\n\terr = sigmoidSlice(slClasses)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't activate classes\")\n\t}\n\n\tstep := grid * numAnchors\n\tfor i := 0; i < grid; i++ {\n\t\tvy, err := input.Slice(nil, S(i*step, i*step+step), S(1))\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"Can't slice while doing steps for grid\")\n\t\t}\n\t\t_, err = tensor.Add(vy, float64(i), tensor.UseUnsafe())\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"Can't do tensor.Add(...) for float64; (1)\")\n\t\t}\n\t\tfor n := 0; n < numAnchors; n++ {\n\t\t\tanchorsSlice, err := input.Slice(nil, S(i*numAnchors+n, input.Shape()[1], step), S(0))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"Can't slice anchors while doing steps for grid\")\n\t\t\t}\n\t\t\t_, err = tensor.Add(anchorsSlice, float64(i), tensor.UseUnsafe())\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"Can't do tensor.Add(...) for float64; (2)\")\n\t\t\t}\n\t\t}\n\n\t}\n\n\tanchors := []float64{}\n\tfor i := 0; i < grid*grid; i++ {\n\t\tanchors = append(anchors, currentAnchors...)\n\t}\n\n\tanchorsTensor := tensor.New(tensor.Of(inputNumericType), tensor.WithShape(1, grid*grid*numAnchors, 2))\n\tfor i := range anchors {\n\t\tanchorsTensor.Set(i, anchors[i])\n\t}\n\n\t_, err = tensor.Div(anchorsTensor, float64(stride), tensor.UseUnsafe())\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't do tensor.Div(...) for float64\")\n\t}\n\n\tvhw, err := input.Slice(nil, nil, S(2, 4))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't do slice on input S(2,4)\")\n\t}\n\n\t_, err = vhw.Apply(math.Exp, tensor.UseUnsafe())\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't apply exp64 to YOLO operation\")\n\t}\n\n\t_, err = tensor.Mul(vhw, anchorsTensor, tensor.UseUnsafe())\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't do tensor.Mul(...) for anchors\")\n\t}\n\n\tvv, err := input.Slice(nil, nil, S(0, 4))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't do slice on input S(0,4)\")\n\t}\n\n\t_, err = tensor.Mul(vv, float64(stride), tensor.UseUnsafe())\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Can't do tensor.Mul(...) for float64\")\n\t}\n\n\treturn input, nil\n}\n\nfunc iou_f64(r1, r2 image.Rectangle) float64 {\n\tintersection := r1.Intersect(r2)\n\tinterArea := intersection.Dx() * intersection.Dy()\n\tr1Area := r1.Dx() * r1.Dy()\n\tr2Area := r2.Dx() * r2.Dy()\n\treturn float64(interArea) / float64(r1Area+r2Area-interArea)\n}\n\nfunc getBestIOU_f64(input, target []float64, numClasses, dims int) [][]float64 {\n\tious := make([][]float64, 0)\n\timgsize := float64(dims)\n\tfor i := 0; i < len(input); i = i + numClasses + 5 {\n\t\tious = append(ious, []float64{0, -1})\n\t\tr1 := rectifyBox_f64(input[i], input[i+1], input[i+2], input[i+3], dims)\n\t\tfor j := 0; j < len(target); j = j + 5 {\n\t\t\tr2 := rectifyBox_f64(target[j+1]*imgsize, target[j+2]*imgsize, target[j+3]*imgsize, target[j+4]*imgsize, dims)\n\t\t\tcuriou := iou_f64(r1, r2)\n\t\t\tif curiou > ious[i/(5+numClasses)][0] {\n\t\t\t\tious[i/(5+numClasses)][0] = curiou\n\t\t\t\tious[i/(5+numClasses)][1] = float64(j / 5)\n\t\t\t}\n\t\t}\n\t}\n\treturn ious\n}\n\nfunc getBestAnchors_f64(target []float64, anchors []float64, masks []int, dims int, gridSize float64) [][]int {\n\tbestAnchors := make([][]int, len(target)/5)\n\timgsize := float64(dims)\n\tfor j := 0; j < len(target); j = j + 5 {\n\t\ttargetRect := rectifyBox_f64(0, 0, target[j+3]*imgsize, target[j+4]*imgsize, dims) //not absolutely confident in rectangle sizes\n\t\tbestIOU := float64(0.0)\n\t\tbestAnchors[j/5] = make([]int, 3)\n\t\tfor i := 0; i < len(anchors); i = i + 2 {\n\t\t\tanchorRect := rectifyBox_f64(0, 0, anchors[i], anchors[i+1], dims)\n\t\t\tcurrentIOU := iou_f64(anchorRect, targetRect)\n\t\t\tif currentIOU >= bestIOU {\n\t\t\t\tbestAnchors[j/5][0] = i\n\t\t\t\tbestIOU = currentIOU\n\t\t\t}\n\t\t}\n\t\tbestAnchors[j/5][0] = findIntElement(masks, bestAnchors[j/5][0]/2)\n\t\tif bestAnchors[j/5][0] != -1 {\n\t\t\tbestAnchors[j/5][1] = int(target[j+1] * gridSize)\n\t\t\tbestAnchors[j/5][2] = int(target[j+2] * gridSize)\n\t\t}\n\t}\n\treturn bestAnchors\n}\n\nfunc prepareOutputYOLO_f64(input, yoloBoxes, target, anchors []float64, masks []int, numClasses, dims, gridSize int, ignoreTresh float64) []float64 {\n\tyoloBBoxes := make([]float64, len(yoloBoxes))\n\tgridSizeF64 := float64(gridSize)\n\tbestAnchors := getBestAnchors_f64(target, anchors, masks, dims, gridSizeF64)\n\tbestIous := getBestIOU_f64(yoloBoxes, target, numClasses, dims)\n\tfor i := 0; i < len(yoloBoxes); i = i + (5 + numClasses) {\n\t\tif bestIous[i/(5+numClasses)][0] <= ignoreTresh {\n\t\t\tyoloBBoxes[i+4] = bceLoss64(0, yoloBoxes[i+4])\n\t\t}\n\t}\n\tfor i := 0; i < len(bestAnchors); i++ {\n\t\tif bestAnchors[i][0] != -1 {\n\t\t\tscale := (2 - target[i*5+3]*target[i*5+4])\n\t\t\tgiInt := bestAnchors[i][1]\n\t\t\tgjInt := bestAnchors[i][2]\n\t\t\tgx := invsigm64(target[i*5+1]*gridSizeF64 - float64(giInt))\n\t\t\tgy := invsigm64(target[i*5+2]*gridSizeF64 - float64(gjInt))\n\t\t\tgw := math.Log(target[i*5+3]/anchors[bestAnchors[i][0]] + 1e-16)\n\t\t\tgh := math.Log(target[i*5+4]/anchors[bestAnchors[i][0]+1] + 1e-16)\n\t\t\tbboxIdx := gjInt*gridSize*len(masks) + giInt*len(masks) + bestAnchors[i][0]\n\t\t\tyoloBBoxes[bboxIdx] = mseLoss64(gx, input[bboxIdx], scale)\n\t\t\tyoloBBoxes[bboxIdx+1] = mseLoss64(gy, input[bboxIdx+1], scale)\n\t\t\tyoloBBoxes[bboxIdx+2] = mseLoss64(gw, input[bboxIdx+2], scale)\n\t\t\tyoloBBoxes[bboxIdx+3] = mseLoss64(gh, input[bboxIdx+3], scale)\n\t\t\tyoloBBoxes[bboxIdx+4] = bceLoss64(1, yoloBoxes[bboxIdx+4])\n\t\t\tfor j := 0; j < numClasses; j++ {\n\t\t\t\tif j == int(target[i]) {\n\t\t\t\t\tyoloBBoxes[bboxIdx+5+j] = bceLoss64(1, yoloBoxes[bboxIdx+4])\n\t\t\t\t} else {\n\t\t\t\t\tyoloBBoxes[bboxIdx+5+j] = bceLoss64(0, yoloBoxes[bboxIdx+4])\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn yoloBBoxes\n}\n\nfunc rectifyBox_f64(x, y, h, w float64, imgSize int) image.Rectangle {\n\treturn image.Rect(maxInt(int(x-w/2), 0), maxInt(int(y-h/2), 0), minInt(int(x+w/2+1), imgSize), minInt(int(y+h/2+1), imgSize))\n}\n\nfunc bceLoss64(target, pred float64) float64 {\n\tif target == 1.0 {\n\t\treturn -(math.Log(pred + 1e-16))\n\t}\n\treturn -(math.Log((1.0 - pred) + 1e-16))\n}\n\nfunc mseLoss64(target, pred, scale float64) float64 {\n\treturn math.Pow(scale*(target-pred), 2) / 2.0\n}\n\nfunc invsigm64(target float64) float64 {\n\treturn -math.Log(1-target+1e-16) + math.Log(target+1e-16)\n}\n"
        },
        {
          "name": "op_yolo_test.go",
          "type": "blob",
          "size": 2.1044921875,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"testing\"\n\n\t\"gorgonia.org/tensor\"\n)\n\nfunc TestYolo(t *testing.T) {\n\n\tinputSize := 416\n\tnumClasses := 80\n\ttestAnchors := [][]float32{\n\t\t[]float32{10, 13, 16, 30, 33, 23},\n\t\t[]float32{30, 61, 62, 45, 59, 119},\n\t\t[]float32{116, 90, 156, 198, 373, 326},\n\t}\n\n\tnumpyInputs := []string{\n\t\t\"./examples/tiny-yolo-v3-coco/data/test_yolo_op/1input.[(10, 13), (16, 30), (33, 23)].npy\",\n\t\t\"./examples/tiny-yolo-v3-coco/data/test_yolo_op/1input.[(30, 61), (62, 45), (59, 119)].npy\",\n\t\t\"./examples/tiny-yolo-v3-coco/data/test_yolo_op/1input.[(116, 90), (156, 198), (373, 326)].npy\",\n\t}\n\n\tnumpyExpectedOutputs := []string{\n\t\t\"./examples/tiny-yolo-v3-coco/data/test_yolo_op/1output.[(10, 13), (16, 30), (33, 23)].npy\",\n\t\t\"./examples/tiny-yolo-v3-coco/data/test_yolo_op/1output.[(30, 61), (62, 45), (59, 119)].npy\",\n\t\t\"./examples/tiny-yolo-v3-coco/data/test_yolo_op/1output.[(116, 90), (156, 198), (373, 326)].npy\",\n\t}\n\n\tfor i := range testAnchors {\n\t\t// Read input values from numpy format\n\t\tinput := tensor.New(tensor.Of(tensor.Float32))\n\t\tr, err := os.Open(numpyInputs[i])\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t\treturn\n\t\t}\n\t\terr = input.ReadNpy(r)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t\treturn\n\t\t}\n\n\t\t// Read expected values from numpy format\n\t\texpected := tensor.New(tensor.Of(tensor.Float32))\n\t\tr, err = os.Open(numpyExpectedOutputs[i])\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t\treturn\n\t\t}\n\t\terr = expected.ReadNpy(r)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t\treturn\n\t\t}\n\n\t\t// Load graph\n\t\tg := NewGraph()\n\t\tinputTensor := NewTensor(g, tensor.Float32, 4, WithShape(input.Shape()...), WithName(\"yolo\"))\n\t\t// Prepare YOLOv3 node\n\t\toutNode, err := YOLOv3(inputTensor, testAnchors[i], []int{0, 1, 2}, inputSize, numClasses, 0.7)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t\treturn\n\t\t}\n\t\t// Run operation\n\t\tvm := NewTapeMachine(g)\n\t\tif err := Let(inputTensor, input); err != nil {\n\t\t\tt.Error(err)\n\t\t\treturn\n\t\t}\n\t\tvm.RunAll()\n\t\tvm.Close()\n\n\t\tif !floatsEqual32(outNode.Value().Data().([]float32), expected.Data().([]float32)) {\n\t\t\tt.Error(fmt.Sprintf(\"Test Anchor %d: %v\\nGot: \\n%v\\nExpected: \\n%v\", i, testAnchors[i], outNode.Value(), expected))\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "operations.go",
          "type": "blob",
          "size": 16.9404296875,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\n// contains all public operations that can be performed on nodes\n// all the functions here have the signature:\n// \t\tfunc (...) (*Node, error)\n\n/* BINARY FUNCTIONS */\nfunc binOpNode(op BinaryOp, a, b *Node) (retVal *Node, err error) {\n\tstabLogf(\"Creating node for %v, a: %p, b: %p\", op, a, b)\n\tenterLogScope()\n\tdefer leaveLogScope()\n\t// maybe make stabilization a build tag?\n\tif stabilization {\n\t\tenterLogScope()\n\t\tif ebo, ok := op.(elemBinOp); ok {\n\t\t\tot := ebo.binOpType()\n\n\t\t\tenterLogScope()\n\t\t\tfor _, fn := range binOpStabilizationFns[ot] {\n\t\t\t\tif retVal, err = fn(a, b); err == nil {\n\t\t\t\t\tleaveLogScope()\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tif _, ok := err.(errNoStabilization); !ok {\n\t\t\t\t\tleaveLogScope()\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\tleaveLogScope()\n\t\t}\n\t\tleaveLogScope()\n\t}\n\tstabLogf(\"No bin op stabilization\")\n\treturn ApplyOp(op, a, b)\n}\n\n// Mul is the general handler for multiplication of nodes. It is extremely overloaded. Only use if you know what you're doing\n//\n// If any of the nodes are ScalarType, then it'll be redirected to HadamardProd() instead\n// If the nodes are both vectors (that is, have a shape of (x, 1) or (1, x)), then the operator used will be a vectorDot\n// If only one of the nodes is a vector, then the operator used will be a matrix-vector multiplication will be used, and most importantly,\n// a transpose will be used (when necessary)\n// If both nodes are matrices, then well, matrix multiplication will be done\nfunc Mul(a, b *Node) (retVal *Node, err error) {\n\tif a.IsScalar() || b.IsScalar() {\n\t\treturn HadamardProd(a, b)\n\t}\n\n\tvar op BinaryOp\n\tswitch {\n\tcase a.IsVector() && b.IsVector():\n\t\top = linAlgBinOp{āBinaryOperator: vecDotOperator}\n\t\treturn binOpNode(op, a, b)\n\tcase a.IsVector() && b.IsMatrix():\n\t\top = linAlgBinOp{āBinaryOperator: matVecMulOperator, transA: true}\n\t\treturn binOpNode(op, b, a)\n\tcase a.IsMatrix() && b.IsVector():\n\t\top = linAlgBinOp{āBinaryOperator: matVecMulOperator}\n\t\treturn binOpNode(op, a, b)\n\tcase a.IsMatrix() && b.IsMatrix():\n\t\top = linAlgBinOp{āBinaryOperator: matMulOperator}\n\t\treturn binOpNode(op, a, b)\n\tdefault:\n\t\treturn nil, errors.Errorf(nyiFail, \"Mul\", fmt.Sprintf(\"a %v b %v\", a.shape, b.shape))\n\t}\n}\n\n// BatchedMatMul returns a node representing the batched mat mul operation.\n//\n// A list of transpose options are allowed. The\nfunc BatchedMatMul(a, b *Node, transes ...bool) (retVal *Node, err error) {\n\top := linAlgBinOp{āBinaryOperator: batchedMatMulOperator}\n\tswitch len(transes) {\n\tcase 0:\n\t\t// noop\n\tcase 1:\n\t\t// transA\n\t\top.transA = transes[0]\n\tcase 2:\n\t\t// transA and transB\n\t\top.transA = transes[0]\n\t\top.transB = transes[1]\n\tdefault:\n\t\t// unsupported\n\t\top.transA = transes[0]\n\t\top.transB = transes[1]\n\t}\n\n\treturn binOpNode(op, a, b)\n}\n\n// OuterProd returns a Node representing the outer product of two vectors. This function will return an error if both input nodes are not vectors\nfunc OuterProd(a, b *Node) (retVal *Node, err error) {\n\tif !a.IsVector() || !b.IsVector() {\n\t\treturn nil, errors.Errorf(\"Expected only vectors to be able to do OuterProd. %v is %v. %v is %v\", a, a.Shape(), b, b.Shape()) //for now\n\t}\n\n\t// TODO: maybe align shapes?\n\top := linAlgBinOp{āBinaryOperator: outerProdOperator}\n\treturn binOpNode(op, a, b)\n}\n\n// Div is a shortcut function for HadamardDiv for scalar values. For matrix/tensor values, the matrix division operation is not yet handled, and will panic.\nfunc Div(a, b *Node) (retVal *Node, err error) {\n\tif a.IsScalar() || b.IsScalar() || a.Shape().Eq(b.Shape()) {\n\t\treturn HadamardDiv(a, b)\n\t}\n\n\t// otherwise, matrix division\n\tpanic(\"Unhandled\")\n}\n\n// Auto automatically calculates the padding for the given operations, for example:\n// \t\tgorgonia.Auto(gorgonia.BroadcastHadamardProd, a, b)\nfunc Auto(op func(a, b *Node, leftPattern, rightPattern []byte) (*Node, error), a, b *Node) (*Node, error) {\n\tleftPattern, rightPattern, err := autoBroadcastPattern(a.Shape(), b.Shape())\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Auto failed to find broadcastable pattern\")\n\t}\n\n\treturn op(a, b, rightPattern, leftPattern)\n}\n\n/* UNARY STUFF */\n\nfunc unaryOpNode(op Op, a *Node) (retVal *Node, err error) {\n\tstabLogf(\"Creating node for %v, a: %p %v\", op, a, a)\n\tenterLogScope()\n\tdefer leaveLogScope()\n\tif stabilization {\n\n\t\t// do optimization/stabilization\n\t\t// TODO: maybe recursively stabilize?\n\t\tenterLogScope()\n\t\tot := op.(elemUnaryOp).unaryOpType()\n\t\tfor _, fn := range unaryOpStabilizationFns[ot] {\n\t\t\tif retVal, err = fn(a); err == nil {\n\t\t\t\tstabLogf(\"stabilized\")\n\t\t\t\tleaveLogScope()\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif _, ok := err.(errNoStabilization); !ok {\n\t\t\t\tstabLogf(\"Actual error\")\n\t\t\t\tleaveLogScope()\n\t\t\t\treturn\n\t\t\t}\n\t\t\tstabLogf(\"No stabilization found\")\n\t\t}\n\t\tleaveLogScope()\n\t\tstabLogf(\"No stabilizations - retVal: %v\", retVal)\n\t}\n\n\treturn ApplyOp(op, a)\n}\n\n// more complex unaries\n\n// LogSumExp performs addition in the log domain\nfunc LogSumExp(a *Node, axis int) (retVal *Node, err error) {\n\tvar max, exp, sum, logSum *Node\n\tif max, err = Max(a, axis); err != nil {\n\t\treturn nil, errors.Wrap(err, operationError)\n\t}\n\tif retVal, err = Sub(a, max); err == nil {\n\t\tif exp, err = Exp(retVal); err == nil {\n\t\t\tif sum, err = Sum(exp, axis); err == nil {\n\t\t\t\tif sum, err = Add(sum, max); err == nil {\n\t\t\t\t\tif logSum, err = Log(sum); err == nil {\n\t\t\t\t\t\treturn Sum(logSum, axis)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn nil, errors.Wrap(err, operationError)\n}\n\n/* Aggregate Functions */\n\n// At is a symbolic operation for getting a value at the provided coordinates.\n// If the input is a scalar, all the coordinates MUST be 0, or else an error will be returned.\nfunc At(a *Node, coords ...int) (retVal *Node, err error) {\n\tif a.IsScalar() {\n\t\tfor _, c := range coords {\n\t\t\tif c != 0 {\n\t\t\t\treturn nil, errors.Errorf(\"At() only works with scalars when the coordinates are (0...0). Got %v instead\", coords)\n\t\t\t}\n\t\t}\n\t\treturn a, nil\n\t}\n\n\tdims := a.Dims()\n\top := atOp{\n\t\tcoordinates: coords,\n\t\td:           dims,\n\t}\n\n\treturn ApplyOp(op, a)\n}\n\n// Max performs a max() on the input and the provided axes.\nfunc Max(a *Node, along ...int) (retVal *Node, err error) {\n\tif a.IsScalar() {\n\t\t// can't max a scalar. Should return error\n\t\treturn a, nil\n\t}\n\n\tdims := a.Dims()\n\tif len(along) == 0 {\n\t\talong = intRange(0, dims)\n\t}\n\n\top := newMaxOp(along, dims)\n\n\treturn ApplyOp(op, a)\n}\n\n// Mean performs a mean() on the input and the provided axes.\nfunc Mean(a *Node, along ...int) (retVal *Node, err error) {\n\tif a.IsScalar() {\n\t\t// can't mean a scalar... return error\n\t\treturn a, nil\n\t}\n\n\tdims := a.Dims()\n\n\tif len(along) == 0 {\n\t\talong = intRange(0, dims)\n\t}\n\n\tvar s *Node\n\tif s, err = Sum(a, along...); err != nil {\n\t\treturn nil, errors.Wrap(err, operationError)\n\t}\n\n\tsizes := make(Nodes, len(along))\n\tfor i, axis := range along {\n\t\tif sizes[i], err = SizeOf(axis, a); err != nil {\n\t\t\treturn nil, errors.Wrap(err, operationError)\n\t\t}\n\t}\n\n\tvar counts *Node\n\tif counts, err = ReduceMul(sizes); err == nil {\n\t\treturn HadamardDiv(s, counts)\n\t}\n\treturn nil, errors.Wrap(err, operationError)\n}\n\n// Sum performs a sum() on the input and the provided axes.\nfunc Sum(a *Node, along ...int) (retVal *Node, err error) {\n\tif a.IsScalar() {\n\t\tretVal = a // or error?\n\t\treturn\n\t}\n\n\tdims := a.Dims()\n\tif len(along) == 0 {\n\t\talong = intRange(0, dims)\n\t}\n\n\top := newSumOp(along, a.shape, dims)\n\treturn ApplyOp(op, a)\n}\n\n// Norm returns the p-norm of a Value. Use p=2 if you want to use unordered norms.\n//\n// This is a simpler version of the norms found in the Tensor package, which specializes and optimizes even more\n// (well, given it's adapted from Numpy, it is clearly way more optimized)\nfunc Norm(a *Node, axis, p int) (retVal *Node, err error) {\n\tif p == 2 {\n\t\tif retVal, err = Square(a); err == nil {\n\t\t\tif retVal, err = Sum(retVal, axis); err == nil {\n\t\t\t\tif retVal, err = Sqrt(retVal); err != nil {\n\t\t\t\t\treturn nil, errors.Wrap(err, operationError)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\treturn nil, errors.Wrap(err, operationError)\n\t\t\t}\n\t\t} else {\n\t\t\treturn nil, errors.Wrap(err, operationError)\n\t\t}\n\t\treturn\n\t}\n\n\tvar dt tensor.Dtype\n\tif dt, err = dtypeOf(a.t); err != nil {\n\t\treturn nil, errors.Wrapf(err, \"Failed to determine the dtype of %T\", a.t)\n\t}\n\n\tvar b, inv *Node\n\tswitch dt {\n\tcase Float32:\n\t\tb = NewConstant(float32(p))\n\t\tinv = NewConstant(float32(1) / float32(p))\n\tcase Float64:\n\t\tb = NewConstant(float64(p))\n\t\tinv = NewConstant(float64(1) / float64(p))\n\tdefault:\n\t\treturn nil, errors.New(\"Cannot norm a non-floating point type\")\n\t}\n\n\tif retVal, err = Pow(a, b); err == nil {\n\t\tif retVal, err = Sum(retVal, axis); err == nil {\n\t\t\tif retVal, err = Pow(retVal, inv); err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, operationError)\n\t\t\t}\n\t\t} else {\n\t\t\treturn nil, errors.Wrap(err, operationError)\n\t\t}\n\t} else {\n\t\treturn nil, errors.Wrap(err, operationError)\n\t}\n\treturn\n}\n\n// Reduction\n\n// ReduceAdd takes a slice of *Nodes, and folds them into one by adding\nfunc ReduceAdd(nodes Nodes, opts ...NodeConsOpt) (retVal *Node, err error) {\n\tswitch len(nodes) {\n\tcase 0:\n\t\treturn nil, nil // or error?\n\tcase 1:\n\t\treturn nodes[0], nil\n\tcase 2:\n\t\tif retVal, err = Add(nodes[0], nodes[1]); err == nil {\n\t\t\tfor _, opt := range opts {\n\t\t\t\topt(retVal)\n\t\t\t}\n\t\t} else {\n\t\t\treturn nil, errors.Wrap(err, operationError)\n\t\t}\n\t\treturn\n\t}\n\n\tretVal = nodes[0]\n\tfor i, n := range nodes {\n\t\tif i == 0 {\n\t\t\tcontinue\n\t\t}\n\n\t\tif retVal, err = Add(retVal, n); err != nil {\n\t\t\terr = errors.Wrap(err, operationError)\n\t\t\treturn\n\t\t}\n\t\tfor _, opt := range opts {\n\t\t\topt(retVal)\n\t\t}\n\t}\n\treturn\n}\n\n// ReduceMul is like foldl(*, nodes)\nfunc ReduceMul(nodes Nodes, opts ...NodeConsOpt) (retVal *Node, err error) {\n\tswitch len(nodes) {\n\tcase 0:\n\t\treturn nil, nil // or error?\n\tcase 1:\n\t\treturn nodes[0], nil\n\tcase 2:\n\t\tif retVal, err = Mul(nodes[0], nodes[1]); err == nil {\n\t\t\tfor _, opt := range opts {\n\t\t\t\topt(retVal)\n\t\t\t}\n\t\t} else {\n\t\t\treturn nil, errors.Wrap(err, operationError)\n\t\t}\n\t\treturn\n\t}\n\n\tretVal = nodes[0]\n\tfor i, n := range nodes {\n\t\tif i == 0 {\n\t\t\tcontinue\n\t\t}\n\n\t\tif retVal, err = Mul(retVal, n); err != nil {\n\t\t\treturn nil, errors.Wrap(err, operationError)\n\t\t}\n\t\tfor _, opt := range opts {\n\t\t\topt(retVal)\n\t\t}\n\t}\n\treturn\n}\n\n/* Shape related operations */\n\n// SizeOf returns the size of a value along an axis\nfunc SizeOf(axis int, x *Node) (retVal *Node, err error) {\n\top := sizeOp{\n\t\taxis: axis,\n\t\td:    x.Dims(),\n\t}\n\n\t// if the shape is known\n\tif x.shape != nil {\n\t\top.val = x.shape[axis]\n\t}\n\n\treturn ApplyOp(op, x)\n}\n\n// Slice slices a *Node. For T[:] slices, pass in nil. Will error out if node's type is not a Tensor\nfunc Slice(n *Node, slices ...tensor.Slice) (retVal *Node, err error) {\n\tif _, ok := n.t.(TensorType); !ok {\n\t\treturn nil, errors.Errorf(\"Cannot slice on non Tensor tensor. Got %T\", n.t)\n\t}\n\n\tif len(slices) > n.shape.Dims() {\n\t\treturn nil, errors.Errorf(\"Cannot slice %v. Shape: %v. Slices: %d\", n, n.shape, len(slices))\n\t}\n\n\tretVal = n\n\tvar dimsChanged int\n\tfor i, s := range slices {\n\t\tvar along int\n\t\tif i > 0 {\n\t\t\tif prev := slices[i-1]; prev != nil {\n\t\t\t\tif prev.End()-prev.Start() == 1 {\n\t\t\t\t\tdimsChanged++\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\talong = i - dimsChanged\n\n\t\top := newSliceOp(s, along, retVal.Dims())\n\t\tif retVal, err = ApplyOp(op, retVal); err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\treturn\n}\n\n// Transpose performs a transpose on the input and provided permutation axes.\nfunc Transpose(n *Node, axes ...int) (retVal *Node, err error) {\n\t// prep axes\n\tif len(axes) > 0 && len(axes) != n.Dims() {\n\t\treturn nil, errors.Errorf(\"n has %d dims, while requested transposes is %d\", n.Dims(), len(axes))\n\t}\n\tdims := len(n.shape)\n\tif len(axes) == 0 || axes == nil {\n\t\taxes = make([]int, dims)\n\t\tfor i := 0; i < dims; i++ {\n\t\t\taxes[i] = dims - 1 - i\n\t\t}\n\t}\n\n\t// if axes is 0, 1, 2, 3... then no op\n\tif monotonic, incr1 := tensor.IsMonotonicInts(axes); monotonic && incr1 && axes[0] == 0 {\n\t\tretVal = n\n\t\treturn\n\t}\n\top := transposeOp{\n\t\tpattern: axes,\n\t\td:       len(axes),\n\t}\n\n\treturn ApplyOp(op, n)\n}\n\n// Concat performs a concatenate on the provided axis and inputs.\nfunc Concat(axis int, ns ...*Node) (retVal *Node, err error) {\n\t// check that all the nodes have the same number of dimensions\n\tvar d int\n\tfor i, n := range ns {\n\t\tif i == 0 {\n\t\t\td = n.shape.Dims()\n\t\t\tcontinue\n\t\t}\n\n\t\tif n.shape.Dims() != d {\n\t\t\terr = errors.Errorf(\"Dimension mismatch. Expected all the nodes to be concatenated to have %d dimensions. Got %d instead\", d, n.shape.Dims())\n\t\t\treturn\n\t\t}\n\t}\n\n\tif d == 0 {\n\t\terr = errors.Errorf(\"Concat only works on Tensor nodes\")\n\t\treturn\n\t}\n\n\tif axis >= d {\n\t\terr = errors.Errorf(\"Invalid axis. Nodes have %d dimensions. Axis is %d\", d, axis)\n\t\treturn\n\t}\n\n\top := concatOp{axis: axis, d: d, children: len(ns)}\n\treturn ApplyOp(op, ns...)\n}\n\n// Unconcat is the opposite of the built in concat function\n// TODO: port this back to Gorgonia and use Gorgonia's sli instead\nfunc Unconcat(a *Node, along int, n int) (Nodes, error) {\n\taShape := a.Shape()\n\tif along < 0 || along > aShape.Dims() {\n\t\treturn nil, errors.Errorf(\"Unable to Unconcat a of shape %v along axis %d\", aShape, along)\n\t}\n\n\tif aShape[along]%n != 0 {\n\t\treturn nil, errors.Errorf(\"Axis %d of %v cannot be nicely split into %d parts\", along, aShape, n)\n\t}\n\n\tnewShapeAlong := aShape[along] / n\n\tbatches := aShape[along] / newShapeAlong\n\n\tvar start int\n\tvar retVal Nodes\n\tfor i := 0; i < batches; i++ {\n\t\tss := make([]tensor.Slice, len(aShape))\n\t\tfor i := range ss {\n\t\t\tif i == along {\n\t\t\t\tss[i] = S(start, start+newShapeAlong)\n\t\t\t} else {\n\t\t\t\tss[i] = S(0, aShape[i])\n\t\t\t}\n\t\t}\n\n\t\ta2, err := Slice(a, ss...)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrapf(err, \"Unable to slice a of shape %v along %d on batch %d. Slices were: %v\", aShape, along, i, ss)\n\t\t}\n\t\tretVal = append(retVal, a2)\n\t\tstart += newShapeAlong\n\t}\n\treturn retVal, nil\n}\n\n// Reshape reshapes a node and returns a new node with the new shape\nfunc Reshape(n *Node, to tensor.Shape) (retVal *Node, err error) {\n\t// check shape\n\tvar negs int\n\tvar infer int\n\tfor i, s := range to {\n\t\tif s < 0 {\n\t\t\tnegs++\n\t\t\tinfer = i\n\t\t}\n\t}\n\tif negs > 1 {\n\t\treturn nil, errors.Errorf(\"Unfortunately, inference of reshape parameters only allow for one variable (a negative number). Got %v instead\", to)\n\t}\n\n\tif negs == 1 {\n\t\tprod := 1\n\t\tfor i, s := range to {\n\t\t\tif i == infer {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tprod *= s\n\t\t}\n\t\tinferred, rem := divmod(n.Shape().TotalSize(), prod)\n\t\tif rem != 0 {\n\t\t\treturn nil, errors.Errorf(\"Cannot reshape %v to %v\", n.Shape(), to)\n\t\t}\n\t\tto[infer] = inferred\n\t}\n\n\t// the Node n might not have shape at this point, in that case we skip the check\n\tif n.Shape().Dims() > 0 && n.Shape().TotalSize() != to.TotalSize() {\n\t\treturn nil, errors.Errorf(\"shape size doesn't not match. Expected %v, got %v\", n.Shape().TotalSize(), to.TotalSize())\n\t}\n\n\top := reshapeOp{\n\t\tfrom: n.Shape(),\n\t\tto:   to,\n\t}\n\treturn ApplyOp(op, n)\n}\n\n// Ravel flattens the given node and returns the new node\nfunc Ravel(n *Node) (retVal *Node, err error) {\n\treturn Reshape(n, tensor.Shape{n.shape.TotalSize()})\n}\n\n/* Contraction related operations */\n\n// Tensordot performs a tensor contraction of a and b along specified axes.\nfunc Tensordot(aAxes []int, bAxes []int, a, b *Node) (retVal *Node, err error) {\n\n\t// Check if input tensors actually have dim ⩾ 1\n\tif (len(a.Shape()) < 1) || (len(b.Shape()) < 1) || (a.Dims() < 1) || (b.Dims() < 1) {\n\t\treturn nil, errors.New(\"Input Node's shape should have length at least 1\")\n\t}\n\n\t// Check if number of specified axes for a and b matches\n\tif len(aAxes) != len(bAxes) {\n\t\treturn nil, errors.New(\"Number of Axes supplied along which to contract tensors does not match\")\n\t}\n\n\t// Check for duplicate indices\n\tif containsDuplicate(aAxes) || containsDuplicate(bAxes) {\n\t\treturn nil, errors.New(\"Supplied axes to contract along contain duplicates\")\n\t}\n\n\t// Check for more compatibility\n\n\taShape := a.Shape()\n\tbShape := b.Shape()\n\n\tfor _, aAxis := range aAxes {\n\t\tif aAxis >= len(aShape) {\n\t\t\treturn nil, errors.New(\"Supplied higher higher axes number to contract along than Tensor's actual number of axes\")\n\t\t}\n\t}\n\n\tfor _, bAxis := range bAxes {\n\t\tif bAxis >= len(bShape) {\n\t\t\treturn nil, errors.New(\"Supplied higher higher axes number to contract along than Tensor's actual number of axes\")\n\t\t}\n\t}\n\n\tfor aAxis, aDim := range aAxes {\n\t\tif aShape[aDim] != bShape[bAxes[aAxis]] {\n\t\t\treturn nil, errors.New(\"Dimension mismatch: Can't contract tensors along supplied axes\")\n\t\t}\n\t}\n\n\t// Otherwise, apply contraction\n\top := makeTensordotOp(a, b, aAxes, bAxes)\n\n\treturn ApplyOp(op, a, b)\n}\n\n// Mish is a novel activation function that is self regularizing.\n//\n// https://arxiv.org/abs/1908.08681\nfunc Mish(a *Node) (retVal *Node, err error) {\n\tvar sp, tsp *Node\n\tif sp, err = Softplus(a); err != nil {\n\t\treturn nil, errors.Wrap(err, \"Mish() - SoftPlus failed\")\n\t}\n\tif tsp, err = Tanh(sp); err != nil {\n\t\treturn nil, errors.Wrap(err, \"Mish() - Tanh failed\")\n\t}\n\treturn HadamardProd(a, tsp)\n}\n\nfunc MinBetween(a, b *Node) (retVal *Node, err error) {\n\top := minBetween{}\n\treturn ApplyOp(op, a, b)\n}\nfunc MaxBetween(a, b *Node) (retVal *Node, err error) {\n\top := maxBetween{}\n\treturn ApplyOp(op, a, b)\n}\n\n// Private functions\n\nfunc containsDuplicate(slice []int) bool {\n\tif nil == slice {\n\t\treturn false\n\t}\n\n\tfor index1, value1 := range slice {\n\t\tfor index2, value2 := range slice {\n\t\t\tif (value1 == value2) && (index1 != index2) {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t}\n\n\treturn false\n}\n"
        },
        {
          "name": "operations_broadcast_test.go",
          "type": "blob",
          "size": 10.966796875,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"gorgonia.org/tensor\"\n)\n\ntype broadcastOpTest struct {\n\tname string\n\ta    Value\n\tb    Value\n\n\t// broadcast axes\n\tleft, right []byte\n\n\t// results\n\tab  Value\n\terr bool\n}\n\nvar broadcastAddTests = []broadcastOpTest{\n\t{name: \"vec-mat\",\n\t\ta:     tensor.New(tensor.WithShape(2), tensor.WithBacking([]float64{100, 200})),\n\t\tb:     tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\tleft:  []byte{1},\n\t\tright: nil,\n\t\tab:    tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{101, 102, 203, 204})),\n\t\terr:   false,\n\t},\n\n\t{name: \"mat-vec\",\n\t\ta:     tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\tb:     tensor.New(tensor.WithShape(2), tensor.WithBacking([]float64{100, 200})),\n\t\tleft:  nil,\n\t\tright: []byte{1},\n\t\tab:    tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{101, 102, 203, 204})),\n\t\terr:   false,\n\t},\n\t{name: \"rowvec-mat\",\n\t\ta:     tensor.New(tensor.WithShape(2, 1), tensor.WithBacking([]float64{100, 200})),\n\t\tb:     tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\tleft:  []byte{1},\n\t\tright: nil,\n\t\tab:    tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{101, 102, 203, 204})),\n\t\terr:   false,\n\t},\n\t{name: \"mat-rowvec\",\n\t\ta:     tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\tb:     tensor.New(tensor.WithShape(2, 1), tensor.WithBacking([]float64{100, 200})),\n\t\tleft:  nil,\n\t\tright: []byte{1},\n\t\tab:    tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{101, 102, 203, 204})),\n\t\terr:   false,\n\t},\n\t{name: \"colvec-mat\",\n\t\ta:     tensor.New(tensor.WithShape(1, 2), tensor.WithBacking([]float64{100, 200})),\n\t\tb:     tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\tleft:  []byte{0},\n\t\tright: nil,\n\t\tab:    tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{101, 202, 103, 204})),\n\t\terr:   false,\n\t},\n\t{name: \"mat-colvec\",\n\t\ta:     tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\tb:     tensor.New(tensor.WithShape(1, 2), tensor.WithBacking([]float64{100, 200})),\n\t\tleft:  nil,\n\t\tright: []byte{0},\n\t\tab:    tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{101, 202, 103, 204})),\n\t\terr:   false,\n\t},\n\t/* // SKIPPED UNTIL WE CAN FIX BROADCAST SEMANTICS\n\t{name: \"3col-3tensor\",\n\t\ta:     tensor.New(tensor.WithShape(1, 1, 2), tensor.WithBacking([]float64{100, 200})),\n\t\tb:     tensor.New(tensor.WithShape(2, 2, 2), tensor.WithBacking([]float64{1, 2, 3, 4, 5, 6, 7, 8})),\n\t\tleft:  []byte{0, 1},\n\t\tright: nil,\n\t\tab:    tensor.New(tensor.WithShape(2, 2, 2), tensor.WithBacking([]float64{101, 202, 103, 204, 105, 206, 107, 208})),\n\t\terr:   false,\n\t},\n\t{name: \"3vec-3tensor\",\n\t\ta:     tensor.New(tensor.WithShape(2, 1, 1), tensor.WithBacking([]float64{100, 200})),\n\t\tb:     tensor.New(tensor.WithShape(2, 2, 2), tensor.WithBacking([]float64{1, 2, 3, 4, 5, 6, 7, 8})),\n\t\tleft:  []byte{1, 2},\n\t\tright: nil,\n\t\tab:    tensor.New(tensor.WithShape(2, 2, 2), tensor.WithBacking([]float64{101, 102, 103, 104, 205, 206, 207, 208})),\n\t\terr:   false,\n\t},\n\t{name: \"colmat-3tensor\",\n\t\ta:     tensor.New(tensor.WithShape(1, 2, 2), tensor.WithBacking([]float64{100, 200, 300, 400})),\n\t\tb:     tensor.New(tensor.WithShape(2, 2, 2), tensor.WithBacking([]float64{1, 2, 3, 4, 5, 6, 7, 8})),\n\t\tleft:  []byte{0},\n\t\tright: nil,\n\t\tab:    tensor.New(tensor.WithShape(2, 2, 2), tensor.WithBacking([]float64{101, 202, 303, 404, 105, 206, 307, 408})),\n\t\terr:   false,\n\t},\n\t{name: \"3tensor-colmat\",\n\t\ta:     tensor.New(tensor.WithShape(2, 2, 2), tensor.WithBacking([]float64{1, 2, 3, 4, 5, 6, 7, 8})),\n\t\tb:     tensor.New(tensor.WithShape(1, 2, 2), tensor.WithBacking([]float64{100, 200, 300, 400})),\n\t\tleft:  nil,\n\t\tright: []byte{0},\n\t\tab:    tensor.New(tensor.WithShape(2, 2, 2), tensor.WithBacking([]float64{101, 202, 303, 404, 105, 206, 307, 408})),\n\t\terr:   false,\n\t},\n\t{name: \"rowmat-3tensor\",\n\t\ta:     tensor.New(tensor.WithShape(2, 2, 1), tensor.WithBacking([]float64{100, 200, 300, 400})),\n\t\tb:     tensor.New(tensor.WithShape(2, 2, 2), tensor.WithBacking([]float64{1, 2, 3, 4, 5, 6, 7, 8})),\n\t\tleft:  []byte{2},\n\t\tright: nil,\n\t\tab:    tensor.New(tensor.WithShape(2, 2, 2), tensor.WithBacking([]float64{101, 102, 203, 204, 305, 306, 407, 408})),\n\t\terr:   false,\n\t},\n\t{name: \"3tensor-rowmat\",\n\t\ta:     tensor.New(tensor.WithShape(2, 2, 2), tensor.WithBacking([]float64{1, 2, 3, 4, 5, 6, 7, 8})),\n\t\tb:     tensor.New(tensor.WithShape(2, 2, 1), tensor.WithBacking([]float64{100, 200, 300, 400})),\n\t\tleft:  nil,\n\t\tright: []byte{2},\n\t\tab:    tensor.New(tensor.WithShape(2, 2, 2), tensor.WithBacking([]float64{101, 102, 203, 204, 305, 306, 407, 408})),\n\t\terr:   false,\n\t},\n\t{name: \"vec-3tensor\",\n\t\ta:     tensor.New(tensor.WithShape(2), tensor.WithBacking([]float64{100, 200})),\n\t\tb:     tensor.New(tensor.WithShape(2, 2, 2), tensor.WithBacking([]float64{1, 2, 3, 4, 5, 6, 7, 8})),\n\t\tleft:  []byte{1, 2},\n\t\tright: nil,\n\t\tab:    tensor.New(tensor.WithShape(2, 2, 2), tensor.WithBacking([]float64{101, 202, 103, 204, 105, 206, 107, 208})),\n\t\terr:   false,\n\t},\n\t*/\n\t// TODO (these would give coverage to all broadcast applications)\n\t// \tvec-3tensor\n\t// \t3tensor-vec\n\t// \tmat-3tensor\n\t// \t3-tensor-mat\n\t// and their corresponding errors\n\n\t// WILL ERR\n\t// {name: \"vec-mat- wrong left pattern axis\",\n\t// \ta:     tensor.New(tensor.WithShape(2), tensor.WithBacking([]float64{100, 200})),\n\t// \tb:     tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t// \tleft:  []byte{0},\n\t// \tright: nil,\n\t// \tab:    tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{101, 102, 203, 204})),\n\t// \terr:   true,\n\t// },\n\t{name: \"rowvec-mat: wrong axis\",\n\t\ta:     tensor.New(tensor.WithShape(2, 1), tensor.WithBacking([]float64{100, 200})),\n\t\tb:     tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\tleft:  []byte{2},\n\t\tright: nil,\n\t\tab:    tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{101, 102, 203, 204})),\n\t\terr:   true,\n\t},\n\n\t{name: \"impossible mat-mat\",\n\t\ta:     tensor.New(tensor.WithShape(2, 4), tensor.WithBacking([]float64{1, 2, 3, 4, 5, 6, 7, 8})),\n\t\tb:     tensor.New(tensor.WithShape(1, 2), tensor.WithBacking([]float64{100, 200})),\n\t\tleft:  nil,\n\t\tright: []byte{0, 1},\n\t\tab:    tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{101, 102, 203, 204})),\n\t\terr:   true,\n\t},\n}\n\nfunc TestBroadcastAdd(t *testing.T) {\n\tassert := assert.New(t)\n\tfor i, bat := range broadcastAddTests {\n\t\t//if bat.name != \"impossible mat-mat\" {\n\t\t//\t\tcontinue\n\t\t//\t}\n\t\tg := NewGraph()\n\t\ta := NodeFromAny(g, bat.a, WithName(\"a\"))\n\t\tb := NodeFromAny(g, bat.b, WithName(\"b\"))\n\t\tc, err := BroadcastAdd(a, b, bat.left, bat.right)\n\t\tif checkErr(t, bat.err, err, bat.name, i) {\n\t\t\tcontinue\n\t\t}\n\t\tmachine := NewTapeMachine(g)\n\n\t\tif err = machine.RunAll(); err != nil {\n\t\t\tt.Errorf(\"Test %v(%d): %v\", bat.name, i, err)\n\t\t}\n\t\tassert.Equal(bat.ab.Data(), c.Value().Data(), \"Test %v(%v)\", bat.name, i)\n\t\tmachine.Close()\n\t}\n}\n\nvar broadcastMulTests = []broadcastOpTest{\n\t{name: \"vec-mat\",\n\t\ta:     tensor.New(tensor.WithShape(2), tensor.WithBacking([]float64{10, 20})),\n\t\tb:     tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\tleft:  []byte{1},\n\t\tright: nil,\n\t\tab:    tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{10, 20, 60, 80})),\n\t\terr:   false,\n\t},\n\n\t{name: \"mat-vec\",\n\t\ta:     tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\tb:     tensor.New(tensor.WithShape(2), tensor.WithBacking([]float64{10, 20})),\n\t\tleft:  nil,\n\t\tright: []byte{1},\n\t\tab:    tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{10, 20, 60, 80})),\n\t\terr:   false,\n\t},\n\t{name: \"rowvec-mat\",\n\t\ta:     tensor.New(tensor.WithShape(2, 1), tensor.WithBacking([]float64{10, 20})),\n\t\tb:     tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\tleft:  []byte{1},\n\t\tright: nil,\n\t\tab:    tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{10, 20, 60, 80})),\n\t\terr:   false,\n\t},\n\t{name: \"mat-rowvec\",\n\t\ta:     tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\tb:     tensor.New(tensor.WithShape(2, 1), tensor.WithBacking([]float64{10, 20})),\n\t\tleft:  nil,\n\t\tright: []byte{1},\n\t\tab:    tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{10, 20, 60, 80})),\n\t\terr:   false,\n\t},\n\t{name: \"colvec-mat\",\n\t\ta:     tensor.New(tensor.WithShape(1, 2), tensor.WithBacking([]float64{10, 20})),\n\t\tb:     tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\tleft:  []byte{0},\n\t\tright: nil,\n\t\tab:    tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{10, 40, 30, 80})),\n\t\terr:   false,\n\t},\n\t{name: \"mat-colvec\",\n\t\ta:     tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\tb:     tensor.New(tensor.WithShape(1, 2), tensor.WithBacking([]float64{10, 20})),\n\t\tleft:  nil,\n\t\tright: []byte{0},\n\t\tab:    tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{10, 40, 30, 80})),\n\t\terr:   false,\n\t},\n\n\t// TODO (these would give coverage to all broadcast applications)\n\t// \tvec-3tensor\n\t// \t3tensor-vec\n\t// \tmat-3tensor\n\t// \t3-tensor-mat\n\t// and their corresponding errors\n\n\t// WILL ERR\n\t// {name: \"vec-mat- wrong left pattern axis\",\n\t// \ta:     tensor.New(tensor.WithShape(2), tensor.WithBacking([]float64{10, 20})),\n\t// \tb:     tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t// \tleft:  []byte{0},\n\t// \tright: nil,\n\t// \terr:   true,\n\t// },\n\t{name: \"rowvec-mat: wrong axis\",\n\t\ta:     tensor.New(tensor.WithShape(2, 1), tensor.WithBacking([]float64{10, 20})),\n\t\tb:     tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{1, 2, 3, 4})),\n\t\tleft:  []byte{2},\n\t\tright: nil,\n\t\terr:   true,\n\t},\n\n\t{name: \"impossible mat-mat\",\n\t\ta:     tensor.New(tensor.WithShape(2, 4), tensor.WithBacking([]float64{1, 2, 3, 4, 5, 6, 7, 8})),\n\t\tb:     tensor.New(tensor.WithShape(1, 2), tensor.WithBacking([]float64{10, 20})),\n\t\tleft:  nil,\n\t\tright: []byte{0, 1},\n\t\terr:   true,\n\t},\n}\n\nfunc TestBroadcastHadamardProd(t *testing.T) {\n\tassert := assert.New(t)\n\tfor i, bat := range broadcastMulTests {\n\t\tg := NewGraph()\n\t\ta := NodeFromAny(g, bat.a, WithName(\"a\"))\n\t\tb := NodeFromAny(g, bat.b, WithName(\"b\"))\n\t\tc, err := BroadcastHadamardProd(a, b, bat.left, bat.right)\n\t\tif checkErr(t, bat.err, err, bat.name, i) {\n\t\t\tcontinue\n\t\t}\n\t\tmachine := NewTapeMachine(g)\n\n\t\tif err = machine.RunAll(); err != nil {\n\t\t\tt.Errorf(\"Test %v(%d): %v\", bat.name, i, err)\n\t\t}\n\t\tassert.Equal(bat.ab.Data(), c.Value().Data(), \"Test %v(%v)\", bat.name, i)\n\t\tmachine.Close()\n\t}\n}\n\n// Broadcasts with nils in both left and right patterns will yield the original inputs.\nfunc ExampleBroadcast_nils() {\n\tg := NewGraph()\n\tx := NewMatrix(g, Float64, WithShape(2, 3), WithName(\"x\"))\n\ty := NewMatrix(g, Float64, WithShape(2, 3), WithName(\"y\"))\n\ta, b, err := Broadcast(x, y, NewBroadcastPattern(nil, nil))\n\tif err != nil {\n\t\tfmt.Printf(\"Error: %v\\n\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"a == x %t; b == y %t\", a == x, b == y)\n\t//  Output:\n\t// a == x true; b == y true\n}\n"
        },
        {
          "name": "operations_nondiff.go",
          "type": "blob",
          "size": 0.34375,
          "content": "package gorgonia\n\nimport \"github.com/pkg/errors\"\n\n// DiagFlat takes the flattened value and creates a diagonal matrix from it.\n//\n// It is non-differentiable.\nfunc DiagFlat(a *Node) (*Node, error) {\n\tif a.Shape().IsScalarEquiv() {\n\t\treturn nil, errors.Errorf(\"Cannot perform DiagFlat on a scalar equivalent node\")\n\t}\n\treturn ApplyOp(diagFlatOp{}, a)\n}\n"
        },
        {
          "name": "operations_test.go",
          "type": "blob",
          "size": 33.9248046875,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"runtime\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc TestApplyOp(t *testing.T) {\n\tassert := assert.New(t)\n\tg := NewGraph()\n\n\tvar cpi *Node\n\tvar ct *Node\n\tvar op Op\n\n\tt.Log(\"Simple Constant Scalar test\")\n\tcpi = NewConstant(3.1415, WithName(\"constantPi\"))\n\tcpi = g.AddNode(cpi)\n\n\tt.Logf(\"g: %v\", cpi.g)\n\n\top = newElemBinOp(addOpType, cpi, cpi)\n\tadded, err := ApplyOpWithName(op, \"+ pi pi\", cpi, cpi)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tassert.Equal(g, added.g)\n\tassert.Equal(Float64, added.t)\n\n\tct = NewConstant(tensor.Ones(tensor.Float64, 3, 3)) // no graph set for ct\n\top = newElemBinOp(addOpType, cpi, ct)\n\tif added, err = ApplyOpWithName(op, \"+ pi constTensor(3,3)_ones\", cpi, ct); err != nil {\n\t\tt.Error(err)\n\t}\n}\n\nvar mulTests = []struct {\n\tname   string\n\txshape tensor.Shape\n\twshape tensor.Shape\n\n\tgradX []float64\n\tgradW []float64\n}{\n\t{\"x vector\", tensor.Shape{2}, tensor.Shape{2, 3}, []float64{3, 12}, []float64{0, 0, 0, 1, 1, 1}},\n\t{\"x mat\", tensor.Shape{3, 2}, tensor.Shape{2, 3}, []float64{3, 12, 3, 12, 3, 12}, []float64{6, 6, 6, 9, 9, 9}},\n\t{\"x_vec_w_vec\", tensor.Shape{6}, tensor.Shape{6}, []float64{0, 1, 2, 3, 4, 5}, []float64{0, 1, 2, 3, 4, 5}},\n}\n\nfunc TestMul(t *testing.T) {\n\tdefer runtime.GC()\n\tassert := assert.New(t)\n\tfor _, mts := range mulTests {\n\t\tg := NewGraph()\n\t\tx := NewTensor(g, Float64, mts.xshape.Dims(), WithName(mts.name), WithShape(mts.xshape...), WithInit(RangedFrom(0)))\n\t\tw := NewTensor(g, Float64, mts.wshape.Dims(), WithName(\"w\"), WithShape(mts.wshape...), WithInit(RangedFrom(0)))\n\n\t\txw, err := Mul(x, w)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Error when testing %q. Err: %v\", mts.name, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif mts.xshape.IsVector() && mts.wshape.IsVector() {\n\t\t\tif _, err = Grad(xw, x, w); err != nil {\n\t\t\t\tt.Errorf(\"Error while differentiating %q, Err: %v\", mts.name, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t} else {\n\t\t\tcost, err := Sum(xw)\n\t\t\tif err != nil {\n\t\t\t\tt.Errorf(\"Error when summing %q. Err: %v\", mts.name, err)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif _, err = Grad(cost, x, w); err != nil {\n\t\t\t\tt.Errorf(\"Error while differentiating %q, Err: %v\", mts.name, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tm := NewTapeMachine(g)\n\t\tif err = m.RunAll(); err != nil {\n\t\t\tt.Errorf(\"Error while executing %q. Err: %v\", mts.name, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tgradX, err := x.Grad()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Error while getting gradient of x %q. Err: %v\", mts.name, err)\n\t\t}\n\n\t\tgradW, err := w.Grad()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Error while getting gradient of w %q. Err: %v\", mts.name, err)\n\t\t}\n\n\t\tassert.Equal(mts.gradX, gradX.Data().([]float64))\n\t\tassert.Equal(mts.gradW, gradW.Data().([]float64))\n\t\tassert.True(mts.xshape.Eq(gradX.Shape()))\n\t\tassert.True(mts.wshape.Eq(gradW.Shape()))\n\t\tm.Close()\n\t}\n\n\tt.Logf(\"Testing Mul with LispMachine\")\n\tfor _, mts := range mulTests {\n\t\tg := NewGraph()\n\t\tx := NewTensor(g, Float64, mts.xshape.Dims(), WithName(mts.name), WithShape(mts.xshape...), WithInit(RangedFrom(0)))\n\t\tw := NewTensor(g, Float64, mts.wshape.Dims(), WithName(\"w\"), WithShape(mts.wshape...), WithInit(RangedFrom(0)))\n\n\t\txw, err := Mul(x, w)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Error when testing %q. Err: %v\", mts.name, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif mts.xshape.IsVector() && mts.wshape.IsVector() {\n\n\t\t} else {\n\t\t\tif _, err = Sum(xw); err != nil {\n\t\t\t\tt.Errorf(\"Error when summing %q. Err: %v\", mts.name, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tm := NewLispMachine(g)\n\n\t\tif err = m.RunAll(); err != nil {\n\t\t\t// ioutil.WriteFile(fmt.Sprintf(\"fullGraph_%v.dot\", mts.name), []byte(g.ToDot()), 0644)\n\t\t\tt.Errorf(\"Error while executing %q. Err: %v\", mts.name, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tgradX, err := x.Grad()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Error while getting gradient of x %q. Err: %v\", mts.name, err)\n\t\t}\n\n\t\tgradW, err := w.Grad()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Error while getting gradient of w %q. Err: %v\", mts.name, err)\n\t\t}\n\n\t\tassert.Equal(mts.gradX, gradX.Data().([]float64))\n\t\tassert.Equal(mts.gradW, gradW.Data().([]float64))\n\t\tassert.True(mts.xshape.Eq(gradX.Shape()))\n\t\tassert.True(mts.wshape.Eq(gradW.Shape()))\n\t\tm.Close()\n\t}\n}\n\nvar gtTests = []struct {\n\ta, b    Value\n\tretSame bool\n\n\texpected Value\n\terr      bool\n}{\n\t// s-s\n\t{NewF64(float64(1)), NewF64(float64(0)), true, NewF64(1.0), false},\n\t{NewF64(float64(0)), NewF64(float64(1)), true, NewF64(0.0), false},\n\t{NewF64(float64(1)), NewF64(float64(0)), false, NewB(true), false},\n\t{NewF32(float32(0)), NewF32(float32(1)), false, NewB(false), false},\n\n\t// s-t\n\t{\n\t\tNewF64(float64(1)), tensor.New(tensor.WithShape(2), tensor.WithBacking([]float64{0, 2})),\n\t\ttrue,\n\t\ttensor.New(tensor.WithShape(2), tensor.WithBacking([]float64{1, 0})),\n\t\tfalse,\n\t},\n\n\t{\n\t\tNewF32(float32(1)), tensor.New(tensor.WithShape(2), tensor.WithBacking([]float32{0, 2})),\n\t\tfalse,\n\t\ttensor.New(tensor.WithShape(2), tensor.WithBacking([]bool{true, false})),\n\t\tfalse,\n\t},\n\n\t// t-s\n\t{\n\t\ttensor.New(tensor.WithShape(2), tensor.WithBacking([]float64{0, 2})), NewF64(float64(1)),\n\t\ttrue,\n\t\ttensor.New(tensor.WithShape(2), tensor.WithBacking([]float64{0, 1})),\n\t\tfalse,\n\t},\n\n\t{\n\t\ttensor.New(tensor.WithShape(2), tensor.WithBacking([]float32{0, 2})), NewF32(float32(1)),\n\t\tfalse,\n\t\ttensor.New(tensor.WithShape(2), tensor.WithBacking([]bool{false, true})),\n\t\tfalse,\n\t},\n\n\t// t-t\n\t{\n\t\ttensor.New(tensor.WithShape(2, 3), tensor.WithBacking([]float64{0, 1, 2, 3, 4, 5})),\n\t\ttensor.New(tensor.WithShape(2, 3), tensor.WithBacking([]float64{5, 4, 3, 2, 1, 0})),\n\t\ttrue,\n\n\t\ttensor.New(tensor.WithShape(2, 3), tensor.WithBacking([]float64{0, 0, 0, 1, 1, 1})),\n\t\tfalse,\n\t},\n\n\t{\n\t\ttensor.New(tensor.WithShape(2, 3), tensor.WithBacking([]float64{0, 1, 2, 3, 4, 5})),\n\t\ttensor.New(tensor.WithShape(2, 3), tensor.WithBacking([]float64{5, 4, 3, 2, 1, 0})),\n\t\tfalse,\n\n\t\ttensor.New(tensor.WithShape(2, 3), tensor.WithBacking([]bool{false, false, false, true, true, true})),\n\t\tfalse,\n\t},\n\n\t// stupids\n\n\t// different shapes\n\t{\n\t\ttensor.New(tensor.Of(tensor.Float32), tensor.WithShape(2)), tensor.New(tensor.Of(tensor.Float32), tensor.WithShape(4)),\n\t\ttrue, nil, true,\n\t},\n\n\t// different dtypes\n\t{\n\t\ttensor.New(tensor.Of(tensor.Float64), tensor.WithShape(2)), tensor.New(tensor.Of(tensor.Float32), tensor.WithShape(2)),\n\t\ttrue, nil, true,\n\t},\n}\n\nfunc TestGt(t *testing.T) {\n\tdefer runtime.GC()\n\tfor i, gtts := range gtTests {\n\t\t// if i != 11 {\n\t\t// \tcontinue\n\t\t// }\n\t\tg := NewGraph()\n\t\ta := NodeFromAny(g, gtts.a, WithName(\"a\"))\n\t\tb := NodeFromAny(g, gtts.b, WithName(\"b\"))\n\n\t\tvar ret *Node\n\t\tvar err error\n\t\tret, err = Gt(a, b, gtts.retSame)\n\n\t\tswitch {\n\t\tcase gtts.err:\n\t\t\tif err == nil {\n\t\t\t\tt.Errorf(\"Expected an error in Test %d\", i)\n\t\t\t}\n\t\t\tcontinue\n\t\tcase !gtts.err && err != nil:\n\t\t\tt.Errorf(\"Test %d: %+v\", i, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif gtts.retSame {\n\t\t\tcost := Must(Sum(ret))\n\t\t\tGrad(cost, a, b)\n\t\t}\n\n\t\tm1 := NewTapeMachine(g)\n\t\tif err = m1.RunAll(); err != nil {\n\t\t\tioutil.WriteFile(\"fail.dot\", []byte(g.ToDot()), 0644)\n\t\t\tt.Errorf(\"%v\", m1.Prog())\n\t\t\tt.Errorf(\"Test %d: %+v\", i, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif !ValueEq(gtts.expected, ret.Value()) {\n\t\t\tt.Errorf(\"Test %d Expected %v. Got %v\", i, gtts.expected, ret.Value())\n\t\t}\n\n\t\t// Test LispMachine implementation\n\t\th := NewGraph()\n\t\tx := NodeFromAny(h, gtts.a, WithName(\"x\"))\n\t\ty := NodeFromAny(h, gtts.b, WithName(\"y\"))\n\t\tret2, _ := Gt(x, y, gtts.retSame)\n\n\t\tvar m2 VM\n\t\tif gtts.retSame {\n\t\t\tMust(Sum(ret2))\n\t\t\tm2 = NewLispMachine(h)\n\t\t} else {\n\t\t\tm2 = NewLispMachine(h, ExecuteFwdOnly())\n\t\t}\n\t\tif err = m2.RunAll(); err != nil {\n\t\t\tt.Errorf(\"Test %d LispMachine: %+v\", i, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif !ValueEq(ret.Value(), ret2.Value()) {\n\t\t\tt.Errorf(\"Test %d. Expected %v. Got  %v\", i, ret.Value(), ret2.Value())\n\t\t}\n\t\tm1.Close()\n\t\tm2.Close()\n\t\truntime.GC()\n\t}\n\n\t// other special cases\n\tg := NewGraph()\n\tc := NewConstant(F64(1))\n\t// T := NewTensor(g, Float64, 1, WithShape(2), WithInit(RangedFrom(0)))\n\tT := UniformRandomNode(g, Float64, 0, 1, 2)\n\n\tvar gt *Node\n\tvar err error\n\tif gt, err = Gt(c, T, true); err != nil {\n\t\tt.Error(err)\n\t}\n\tcost := Must(Sum(gt))\n\tGrad(cost, T)\n\n\tm1 := NewTapeMachine(g)\n\tdefer m1.Close()\n\tif err = m1.RunAll(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tif (TensorType{Dims: 1, Of: Float64}) != TypeOf(gt.Value()) {\n\t\tt.Error(\"Expected a tensor type of float64\")\n\t}\n\n\t// Same test as above, but using *lispMachine\n\n\th := NewGraph()\n\td := NewConstant(F64(1))\n\tU := UniformRandomNode(h, Float64, 0, 1, 2)\n\tvar gt2 *Node\n\tif gt2, err = Gt(d, U, true); err != nil {\n\t\tt.Error(err)\n\t}\n\tMust(Sum(gt2))\n\n\tm2 := NewLispMachine(h)\n\tdefer m2.Close()\n\tif err = m2.RunAll(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tif (TensorType{Dims: 1, Of: Float64}) != TypeOf(gt2.Value()) {\n\t\tt.Error(\"Expected a tensor type of float64\")\n\t}\n\n\tt.Logf(\"%v\", gt2.Value())\n\truntime.GC()\n\n}\n\nfunc TestMisha(t *testing.T) {\n\tdefer runtime.GC()\n\tassert := assert.New(t)\n\tg := NewGraph()\n\tvar err error\n\tvar x0, x1, x2, f0, f1, f2 *Node\n\tvar grad0, grad1, grad2 Nodes\n\n\tx0 = NewScalar(g, Float64, WithName(\"x0\"))\n\tx1 = NewScalar(g, Float64, WithName(\"x1\"))\n\tx2 = NewScalar(g, Float64, WithName(\"x2\"))\n\n\tLet(x0, -2.5)\n\tLet(x1, -2.2)\n\tLet(x2, 1.0)\n\n\tf0 = Must(Mish(x0))\n\tf1 = Must(Mish(x1))\n\tf2 = Must(Mish(x2))\n\n\tif grad0, err = Grad(f0, x0); err != nil {\n\t\tt.Error(err)\n\t}\n\tif grad1, err = Grad(f1, x1); err != nil {\n\t\tt.Error(err)\n\t}\n\tif grad2, err = Grad(f2, x2); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tmachine := NewTapeMachine(g)\n\tdefer machine.Close()\n\tif err = machine.RunAll(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\t// assert non-monotonicity of Mish\n\t// x0 < x1 < x2 && f0 > f1 < f2\n\tassert.Less(extractF64(x0.Value()), extractF64(x1.Value()))\n\tassert.Less(extractF64(x1.Value()), extractF64(x2.Value()))\n\tassert.Greater(extractF64(f0.Value()), extractF64(f1.Value()))\n\tassert.Less(extractF64(f1.Value()), extractF64(f2.Value()))\n\n\t// assert non-monotonocity of Mish'\n\tassert.Greater(extractF64(grad0[0].Value()), extractF64(grad1[0].Value()))\n\tassert.Less(extractF64(grad1[0].Value()), extractF64(grad2[0].Value()))\n}\n\nfunc TestSoftMax(t *testing.T) {\n\tdefer runtime.GC()\n\tg := NewGraph()\n\txT := tensor.New(tensor.WithBacking([]float64{0.1, 0.2, -0.3, 0.4, 0.5}))\n\tx := NewVector(g, Float64, WithShape(5), WithValue(xT))\n\tsm := Must(SoftMax(x))\n\tlogsm := Must(Neg(Must(Log(sm))))\n\tcost := Must(Slice(logsm, S(2)))\n\n\tif _, err := Grad(cost, x); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tm := NewTapeMachine(g, TraceExec())\n\tdefer m.Close()\n\tif err := m.RunAll(); err != nil {\n\t\tt.Error(err)\n\t}\n\tioutil.WriteFile(\"fullGraph.dot\", []byte(g.ToDot()), 0644)\n\tvar xG Value\n\tvar err error\n\tif xG, err = x.Grad(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\t// machine 2, graph 2\n\th := NewGraph()\n\txT2 := tensor.New(tensor.WithBacking([]float64{0.1, 0.2, -0.3, 0.4, 0.5}))\n\tx2 := NewVector(h, Float64, WithShape(5), WithValue(xT2))\n\tsm2 := Must(SoftMax(x2))\n\tlogsm2 := Must(Neg(Must(Log(sm2))))\n\tMust(Slice(logsm2, S(2)))\n\n\tm2 := NewLispMachine(h)\n\tdefer m2.Close()\n\tif err = m2.RunAll(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tvar x2G Value\n\tif x2G, err = x2.Grad(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tif !floatsEqual64(xG.Data().([]float64), x2G.Data().([]float64)) {\n\t\tt.Errorf(\"Expected both gradients of X to be the same.\")\n\t}\n\tt.Logf(\"\\n%v\\n%v\\n%v\", sm.Value(), logsm.Value(), cost.Value())\n\tcorrectXGrad := []float64{\n\t\t0.178025447751409, 0.1967485475322529, -0.8806659736677602, 0.24030921861990098, 0.2655827597641975,\n\t}\n\n\tif !floatsEqual64(correctXGrad, x2G.Data().([]float64)) {\n\t\tt.Errorf(\"Expected results to be %v. Got %v.\", correctXGrad, x2G.Data())\n\t}\n\tif !floatsEqual64(correctXGrad, xG.Data().([]float64)) {\n\t\tt.Errorf(\"Expected results to be %v. Got %v.\", correctXGrad, xG.Data())\n\t}\n}\n\nvar sliceTests = []struct {\n\tname   string\n\tshape  tensor.Shape\n\tslices []tensor.Slice\n\n\texpected tensor.Shape\n\tdata     interface{}\n\terr      bool\n}{\n\t{\"vec[0]\", tensor.Shape{2}, []tensor.Slice{S(0)}, scalarShape, float64(0), false},\n\t{\"vec[0:2]\", tensor.Shape{2}, []tensor.Slice{S(0, 2)}, tensor.Shape{2}, []float64{0, 1}, false},\n\t{\"Mat[0]\", tensor.Shape{2, 3}, []tensor.Slice{S(0)}, tensor.Shape{3}, []float64{0, 1, 2}, false},\n\t{\"Mat[:, 0]\", tensor.Shape{2, 3}, []tensor.Slice{nil, S(0)}, tensor.Shape{2}, []float64{0, 3}, false},\n\t{\"3Tensor[0]\", tensor.Shape{2, 3, 4}, []tensor.Slice{S(0)}, tensor.Shape{3, 4}, tensor.Range(tensor.Float64, 0, 12), false},\n\t{\"3Tensor[0:2]\", tensor.Shape{2, 3, 4}, []tensor.Slice{S(0, 2)}, tensor.Shape{2, 3, 4}, tensor.Range(tensor.Float64, 0, 24), false},\n\t{\"3Tensor[:, 0]\", tensor.Shape{2, 3, 4}, []tensor.Slice{nil, S(0)}, tensor.Shape{2, 4}, []float64{0, 1, 2, 3, 12, 13, 14, 15}, false},\n\t{\"3Tensor[0, :, 0]\", tensor.Shape{2, 3, 4}, []tensor.Slice{S(0), nil, S(0)}, tensor.Shape{3}, []float64{0, 4, 8}, false},\n\n\t{\"vec[:, 0]\", tensor.Shape{2}, []tensor.Slice{nil, S(0)}, nil, nil, true},\n}\n\nfunc TestSlice(t *testing.T) {\n\tdefer runtime.GC()\n\tfor _, sts := range sliceTests {\n\t\tg := NewGraph()\n\t\tx := NewTensor(g, Float64, len(sts.shape), WithShape(sts.shape...), WithInit(RangedFrom(0)))\n\t\tsliced, err := Slice(x, sts.slices...)\n\t\tswitch {\n\t\tcase sts.err:\n\t\t\tif err == nil {\n\t\t\t\tt.Errorf(\"Expected an error while running test %q\", sts.name)\n\t\t\t}\n\t\t\tcontinue\n\t\tcase !sts.err && err != nil:\n\t\t\tt.Errorf(\"Error in %q: %+v\", sts.name, err)\n\t\t\tcontinue\n\t\t}\n\n\t\t// test expected shapes:\n\t\tif !sts.expected.Eq(sliced.shape) {\n\t\t\tt.Errorf(\"Test %q - Expected %v. Got %v instead\", sts.name, sts.expected, sliced.shape)\n\t\t\tcontinue\n\t\t}\n\n\t\t// test forwards and backwards prop\n\t\tcost := Must(Sum(sliced))\n\t\tif _, err := Grad(cost, x); err != nil {\n\t\t\tt.Errorf(\"Test %q failed to backprop: %+v\", sts.name, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tm1 := NewTapeMachine(g)\n\t\tif err = m1.RunAll(); err != nil {\n\t\t\tt.Errorf(\"Test %q Runtime error %+v \", sts.name, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tsV := sliced.Value()\n\t\tif !sts.expected.Eq(sV.Shape()) {\n\t\t\tt.Errorf(\"Test %q For TapeMachine. Expected sliced value to have the shape %v. Got %v instead\", sts.name, sts.expected, sV.Shape())\n\t\t}\n\n\t\tassert.Equal(t, sts.data, sV.Data(), \"Test %q For TapeMachine data expected %v, Got %v instead. Formatted:\\n %+v\", sts.name, sts.data, sV.Data(), sV)\n\n\t\t// Test Lisp Machine for equivalence of gradients\n\n\t\th := NewGraph()\n\t\ta := NewTensor(g, Float64, len(sts.shape), WithShape(sts.shape...), WithInit(RangedFrom(0)))\n\t\tsliced2 := Must(Slice(a, sts.slices...))\n\t\tMust(Sum(sliced2))\n\n\t\tm2 := NewLispMachine(h)\n\t\tif err = m2.RunAll(); err != nil {\n\t\t\tt.Errorf(\"Test %q Lispmachine Runtime error: %+v\", sts.name, err)\n\t\t\tcontinue\n\t\t}\n\n\t\ts2V := sliced2.Value()\n\t\tif !sts.expected.Eq(s2V.Shape()) {\n\t\t\tt.Errorf(\"Test %q For LispMachine. Expected sliced value to have the shape %v. Got %v instead\", sts.name, sts.expected, s2V.Shape())\n\t\t}\n\n\t\tassert.Equal(t, sts.data, s2V.Data(), \"Test %q For TapeMachine data expected %v, Got %v instead. Formatted:\\n %+v\", sts.name, sts.data, s2V.Data(), s2V)\n\n\t\tsG, err := sliced.Grad()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Test %q sliced has no grad: %+v\", sts.name, err)\n\t\t\tcontinue\n\t\t}\n\n\t\ts2G, err := sliced2.Grad()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Test %q sliced2 has no grad: %+v\", sts.name, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif !ValueEq(sG, s2G) {\n\t\t\tt.Errorf(\"Test %q - Expected sG and s2G to have the same value\", sts.name)\n\t\t}\n\n\t\tm1.Close()\n\t\tm2.Close()\n\n\t\t// For visual checks\n\t\t// xG, err := x.Grad()\n\t\t// t.Logf(\"Test  %q x: \\n%+v,\\n%+v\", sts.name, x.Value(), xG)\n\t}\n\n\t// special cases with UnsafeLet\n\tg := NewGraph()\n\tx := NewTensor(g, Float64, 2, WithShape(2, 3), WithInit(RangedFrom(0)))\n\tsliced, _ := Slice(x, S(0))\n\tcost := Must(Slice(sliced, S(0)))\n\tGrad(cost, x)\n\n\tm := NewTapeMachine(g)\n\tdefer m.Close()\n\t// mutate the graph before running\n\tUnsafeLet(sliced, S(1))\n\tUnsafeLet(cost, S(2))\n\tif err := m.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\txG, err := x.Grad()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// ioutil.WriteFile(\"blah.dot\", []byte(g.ToDot()), 0644)\n\tassert.Equal(t, []float64{0, 0, 0, 0, 0, 1}, xG.Data())\n\t// visual inspection\n\t// t.Logf(\"x: \\n%+v,\\n%+v\", x.Value(), xG)\n\n}\n\nvar sumTests = []struct {\n\tname  string\n\tshape tensor.Shape\n\talong []int\n\n\texpectedShape tensor.Shape\n\texpectedVal   Value\n\texpectedGrad  Value\n\terr           bool\n}{\n\t{\"Sum(vec)\", tensor.Shape{2}, nil, scalarShape, NewF64(1.0), NewF64(1.0), false},\n\t{\"Sum(vec, 0)\", tensor.Shape{2}, []int{0}, scalarShape, NewF64(1), NewF64(1.0), false},\n\t{\"Sum(Mat)\", tensor.Shape{2, 3}, nil, scalarShape, NewF64(15.0), tensor.New(tensor.WithShape(2, 3), tensor.WithBacking([]float64{1, 1, 1, 1, 1, 1})), false},\n\t{\"Sum(Mat, 0)\", tensor.Shape{2, 3}, []int{0}, tensor.Shape{3},\n\t\ttensor.New(tensor.WithShape(3), tensor.WithBacking([]float64{3, 5, 7})),\n\t\ttensor.New(tensor.WithShape(2, 3), tensor.WithBacking([]float64{1, 1, 1, 1, 1, 1})), false,\n\t},\n\t{\"Sum(Mat, 1)\", tensor.Shape{2, 3}, []int{1}, tensor.Shape{2},\n\t\ttensor.New(tensor.WithShape(2), tensor.WithBacking([]float64{3, 12})),\n\t\ttensor.New(tensor.WithShape(2, 3), tensor.WithBacking([]float64{1, 1, 1, 1, 1, 1})), false,\n\t},\n\n\t// TODO: tests for 3-Tensors\n\t// TODO: negative and stupids cases.\n}\n\nfunc TestSum(t *testing.T) {\n\tdefer runtime.GC()\n\tfor _, sts := range sumTests {\n\t\tg := NewGraph()\n\t\tx := NewTensor(g, Float64, len(sts.shape), WithShape(sts.shape...), WithInit(RangedFrom(0)))\n\t\tvar s *Node\n\t\tvar err error\n\n\t\tif len(sts.along) == 0 {\n\t\t\ts, err = Sum(x)\n\t\t} else {\n\t\t\ts, err = Sum(x, sts.along...)\n\t\t}\n\n\t\tswitch {\n\t\tcase sts.err:\n\t\t\tif err == nil {\n\t\t\t\tt.Errorf(\"Expected an error in %q\", sts.name)\n\t\t\t}\n\t\t\tcontinue\n\t\tcase !sts.err && err != nil:\n\t\t\tt.Errorf(\"Test %q errored while Sum() %+v\", sts.name, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif !sts.expectedShape.Eq(s.shape) {\n\t\t\tt.Errorf(\"Test %q has wrong shape. Want %v, got %v instead\", sts.name, sts.expectedShape, s.shape)\n\t\t\tcontinue\n\t\t}\n\n\t\tcost := s\n\t\tif len(sts.along) < len(sts.shape) && len(sts.along) > 0 {\n\t\t\tcost = Must(Sum(s))\n\t\t}\n\n\t\tif _, err = Grad(cost, x); err != nil {\n\t\t\tt.Errorf(\"Test %q - Unable to back prop. Err : %+v\", sts.name, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tm := NewTapeMachine(g)\n\t\tif err = m.RunAll(); err != nil {\n\t\t\tt.Errorf(\"Test %q - Runtime error: %v\", sts.name, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif !ValueEq(sts.expectedVal, s.Value()) {\n\t\t\tt.Errorf(\"Test %q Expected %v. Got %v\", sts.name, sts.expectedVal, s.Value())\n\t\t}\n\n\t\tsG, err := s.Grad()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Test %q Grad() error: %+v\", sts.name, err)\n\t\t\tcontinue\n\t\t}\n\n\t\t// LISP MACHINE TO TEST GRAD EQUIVALENCE\n\t\th := NewGraph()\n\t\ta := NewTensor(h, Float64, len(sts.shape), WithShape(sts.shape...), WithInit(RangedFrom(0)))\n\t\tvar b *Node\n\t\tif len(sts.along) == 0 {\n\t\t\tb = Must(Sum(a))\n\t\t} else {\n\t\t\tb = Must(Sum(a, sts.along...))\n\t\t}\n\n\t\tif len(sts.along) < len(sts.shape) && len(sts.along) > 0 {\n\t\t\tMust(Sum(b))\n\t\t}\n\n\t\tm2 := NewLispMachine(h)\n\t\tif err = m2.RunAll(); err != nil {\n\t\t\tt.Errorf(\"Test %q Lisp machine runtime error %+v\", sts.name, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif !ValueEq(sts.expectedVal, b.Value()) {\n\t\t\tt.Errorf(\"Test %q LispMachine Run. Expected %v. Got %v instead\", sts.name, sts.expectedVal, b.Value())\n\t\t}\n\n\t\tbG, err := b.Grad()\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Test %q Grad() err in lispmachine run %+v\", sts.name, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif !ValueEq(sG, bG) {\n\t\t\tt.Errorf(\"Expected the values of the partial derivatives of both machines to be the same\")\n\t\t}\n\n\t\tm.Close()\n\t\tm2.Close()\n\t}\n}\n\nfunc TestNorm(t *testing.T) {\n\tassert := assert.New(t)\n\tg := NewGraph()\n\tx := NewMatrix(g, Float64, WithShape(3, 3))\n\tnorm, err := Norm(x, 0, 2)\n\tif err != nil {\n\t\tt.Error(err)\n\t\treturn\n\t}\n\tm := NewLispMachine(g, ExecuteFwdOnly())\n\tdefer m.Close()\n\n\txT := tensor.New(tensor.WithShape(3, 3), tensor.WithBacking(tensor.Range(tensor.Float64, 0, 9)))\n\tLet(x, xT)\n\tm.RunAll()\n\n\tcorrect := []float64{6.708203932499369, 8.12403840463596, 9.643650760992955}\n\tassert.Equal(correct, extractF64s(norm.Value()))\n\n}\n\nfunc TestMean(t *testing.T) {\n\tg := NewGraph()\n\tx := NewMatrix(g, Float64, WithShape(3, 3))\n\tm, err := Mean(x)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif !m.IsScalar() {\n\t\tt.Error(\"Expected result to be scalar\")\n\t}\n}\n\nfunc TestTensordot(t *testing.T) {\n\tassert := assert.New(t)\n\n\t// Scalars\n\tg := NewGraph()\n\n\ta := NewTensor(g, Float64, 0, WithName(\"a\"), WithShape(1), WithInit(RangedFrom(2)))\n\tb := NewTensor(g, Float64, 0, WithName(\"b\"), WithShape(1), WithInit(RangedFrom(21)))\n\tc := NewTensor(g, Float64, 0, WithName(\"c\"), WithShape(1), WithInit(ValuesOf(1.0)))\n\n\ttensordot, err := Tensordot([]int{0}, []int{0}, a, b)\n\tif err == nil {\n\t\tt.Fatal(\"Expected scalars to fail\")\n\t}\n\n\t// Scalar-like\n\tg = NewGraph()\n\ta = NewTensor(g, Float64, 1, WithName(\"a\"), WithShape(1), WithInit(RangedFrom(2)))\n\tb = NewTensor(g, Float64, 1, WithName(\"b\"), WithShape(1), WithInit(RangedFrom(21)))\n\tc = NewTensor(g, Float64, 1, WithName(\"c\"), WithShape(1), WithInit(ValuesOf(1.0)))\n\n\ttensordot, err = Tensordot([]int{0}, []int{0}, a, b)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tt.Logf(\"SHAPE a %v b %v c %v tensordot %v\", a.Shape(), b.Shape(), c.Shape(), tensordot.Shape())\n\n\tdtensordot, err := Backpropagate(Nodes{tensordot}, Nodes{c}, Nodes{a, b})\n\n\tif err != nil {\n\t\tt.Fatalf(\"%+v\", err)\n\t}\n\n\tm := NewTapeMachine(g)\n\tdefer m.Close()\n\tif err = m.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tcorrectScalarlike := []float64{42.0}\n\tvalue := tensordot.Value().Data()\n\tassert.Equal(correctScalarlike, value)\n\n\tdtensordotCorrectScalarlike0 := []float64{21}\n\tdtensordotCorrectScalarlike1 := []float64{2}\n\n\tassert.Equal(dtensordotCorrectScalarlike0, dtensordot[0].Value().Data())\n\tassert.Equal(dtensordotCorrectScalarlike1, dtensordot[1].Value().Data())\n\n\t// Vectors\n\n\tg = NewGraph()\n\ta = NewTensor(g, Float64, 1, WithName(\"a\"), WithShape(2), WithInit(RangedFrom(1)))\n\tb = NewTensor(g, Float64, 1, WithName(\"b\"), WithShape(2), WithInit(RangedFrom(3)))\n\tc = NewTensor(g, Float64, 0, WithName(\"c\"), WithShape(), WithInit(ValuesOf(1.0)))\n\n\tif tensordot, err = Tensordot([]int{0}, []int{0}, a, b); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif dtensordot, err = Backpropagate(Nodes{tensordot}, Nodes{c}, Nodes{a, b}); err != nil {\n\t\tt.Fatalf(\"%+v\", err)\n\t}\n\n\t// Need to multiply dtensordot with identiy matrix, otherwise the transpose action in symdiff is not performed\n\tid := NewConstant(tensor.I(Float64, 2, 2, 0))\n\n\tdtensordot0 := Must(Mul(id, dtensordot[0]))\n\tdtensordot1 := Must(Mul(id, dtensordot[1]))\n\n\tm = NewTapeMachine(g)\n\tdefer m.Close()\n\tif err = m.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tt.Logf(\"TensorDot %v | %v\", tensordot.Value().Shape(), tensordot.Type())\n\tcorrectScalarlike = []float64{11}\n\tassert.Equal(correctScalarlike, tensordot.Value().Data())\n\n\tdcorrect0 := []float64{3, 4}\n\tdcorrect1 := []float64{1, 2}\n\n\tassert.Equal(dcorrect0, extractF64s(dtensordot[0].Value()))\n\tassert.Equal(dcorrect1, extractF64s(dtensordot[1].Value()))\n\n\t// Vector and Matrix\n\tg = NewGraph()\n\ta = NewTensor(g, Float64, 2, WithName(\"a\"), WithShape(2, 2), WithInit(RangedFrom(0)))\n\tb = NewTensor(g, Float64, 1, WithName(\"b\"), WithShape(2), WithInit(RangedFrom(0)))\n\n\tc = NewTensor(g, Float64, 1, WithName(\"c\"), WithShape(2), WithInit(ValuesOf(1.0)))\n\n\tif tensordot, err = Tensordot([]int{1}, []int{0}, a, b); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif dtensordot, err = Backpropagate(Nodes{tensordot}, Nodes{c}, Nodes{a, b}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Need to multiply dtensordot with identiy matrix, otherwise the transpose action in symdiff is not performed\n\tid = NewConstant(tensor.I(Float64, 2, 2, 0))\n\n\tif dtensordot0, err = Mul(id, dtensordot[0]); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif dtensordot1, err = Mul(id, dtensordot[1]); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tm = NewTapeMachine(g)\n\tdefer m.Close()\n\tif err = m.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tcorrect := []float64{1, 3}\n\tassert.Equal(correct, extractF64s(tensordot.Value()))\n\n\tdcorrect0 = []float64{0, 1, 0, 1}\n\tdcorrect1 = []float64{2, 4}\n\n\tassert.Equal(dcorrect0, extractF64s(dtensordot0.Value()))\n\tassert.Equal(dcorrect1, extractF64s(dtensordot1.Value()))\n\n\t// Matrices\n\tg = NewGraph()\n\n\ta = NewTensor(g, Float64, 2, WithName(\"a\"), WithShape(2, 2), WithInit(RangedFrom(0)))\n\tb = NewTensor(g, Float64, 2, WithName(\"b\"), WithShape(2, 2), WithInit(RangedFrom(0)))\n\n\tc = NewTensor(g, Float64, 2, WithName(\"c\"), WithShape(2, 2), WithInit(ValuesOf(1.0)))\n\n\tif tensordot, err = Tensordot([]int{1}, []int{1}, a, b); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif dtensordot, err = Backpropagate(Nodes{tensordot}, Nodes{c}, Nodes{a, b}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Need to multiply dtensordot with identiy matrix, otherwise the transpose action in symdiff is not performed\n\tid = NewConstant(tensor.I(Float64, 2, 2, 0))\n\n\tif dtensordot0, err = Mul(id, dtensordot[0]); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif dtensordot1, err = Mul(id, dtensordot[1]); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tm = NewTapeMachine(g)\n\tif err = m.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tcorrect = []float64{1, 3, 3, 13}\n\tassert.Equal(correct, extractF64s(tensordot.Value()))\n\n\tdcorrect := []float64{2, 4, 2, 4}\n\tassert.Equal(dcorrect, extractF64s(dtensordot0.Value()))\n\tassert.Equal(dcorrect, extractF64s(dtensordot1.Value()))\n\n\t// Total matrix contraction\n\tg = NewGraph()\n\n\ta = NewTensor(g, Float64, 2, WithName(\"a\"), WithShape(2, 2), WithInit(RangedFrom(0)))\n\tb = NewTensor(g, Float64, 2, WithName(\"b\"), WithShape(2, 2), WithInit(RangedFrom(0)))\n\n\tc = NewTensor(g, Float64, 0, WithName(\"c\"), WithShape(), WithInit(ValuesOf(1.0)))\n\n\tif tensordot, err = Tensordot([]int{0, 1}, []int{0, 1}, a, b); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif dtensordot, err = Backpropagate(Nodes{tensordot}, Nodes{c}, Nodes{a, b}); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Need to multiply dtensordot with identiy matrix, otherwise the transpose action in symdiff is not performed\n\tid = NewConstant(tensor.I(Float64, 2, 2, 0))\n\n\tif dtensordot0, err = Mul(id, dtensordot[0]); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif dtensordot1, err = Mul(id, dtensordot[1]); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tm = NewTapeMachine(g)\n\tdefer m.Close()\n\tif err = m.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tcorrectScalarlike = []float64{14}\n\tassert.Equal(correctScalarlike, tensordot.Value().Data())\n\n\tdcorrect = []float64{0, 1, 2, 3}\n\tassert.Equal(dcorrect, extractF64s(dtensordot0.Value()))\n\tassert.Equal(dcorrect, extractF64s(dtensordot1.Value()))\n\n}\n\nvar reshapeTests = []struct {\n\ttestName string\n\tinput    tensor.Shape\n\tto       tensor.Shape\n\toutput   tensor.Shape\n\terr      bool\n}{\n\t{\"simple\", tensor.Shape{2, 2}, tensor.Shape{4}, tensor.Shape{4}, false},\n\t{\"simple big tensor\", tensor.Shape{200, 200}, tensor.Shape{200 * 200}, tensor.Shape{200 * 200}, false},\n\t{\"negative dim1 1\", tensor.Shape{3, 2}, tensor.Shape{6, -1}, tensor.Shape{6, 1}, false},\n\t{\"negative dim1 2\", tensor.Shape{3, 2}, tensor.Shape{2, -1}, tensor.Shape{2, 3}, false},\n\t{\"negative dim0 1\", tensor.Shape{3, 2}, tensor.Shape{-1, 3}, tensor.Shape{2, 3}, false},\n\t{\"negative dims0.1 with error\", tensor.Shape{3, 2}, tensor.Shape{-1, -1}, nil, true},\n\t{\"devative dim0 with error\", tensor.Shape{3, 2}, tensor.Shape{4, -1}, nil, true},\n}\n\nfunc TestReshape(t *testing.T) {\n\tfor _, rst := range reshapeTests {\n\t\tg := NewGraph()\n\t\tT := NewTensor(g, Float64, len(rst.input), WithShape(rst.input.Clone()...))\n\t\tT2, err := Reshape(T, rst.to.Clone())\n\t\tt.Log(T2)\n\t\tswitch {\n\t\tcase rst.err && err == nil:\n\t\t\tt.Fatalf(\"Expected Error when testing %v\", rst)\n\t\tcase rst.err:\n\t\t\tcontinue\n\t\tcase err != nil:\n\t\t\tt.Fatal(err)\n\t\tdefault:\n\t\t\tassert.True(t, rst.output.Eq(T2.Shape()), \"expected both to be the same\")\n\t\t}\n\n\t}\n}\nfunc TestReshape_Dense(t *testing.T) {\n\tfor _, rst := range reshapeTests {\n\t\tg := NewGraph()\n\t\ttT := tensor.New(tensor.Of(tensor.Float64), tensor.WithShape(rst.input.Clone()...))\n\t\tT := NodeFromAny(g, tT)\n\t\tT2, err := Reshape(T, rst.to.Clone())\n\t\tswitch {\n\t\tcase rst.err && err == nil:\n\t\t\tt.Fatalf(\"Expected Error when testing %v\", rst)\n\t\tcase rst.err:\n\t\t\tcontinue\n\t\tcase err != nil:\n\t\t\tt.Fatal(err)\n\t\tdefault:\n\t\t\tassert.True(t, rst.output.Eq(T2.Shape()), \"expected both to be the same\")\n\t\t}\n\t\tm := NewTapeMachine(g)\n\t\tif err := m.RunAll(); err != nil {\n\t\t\tt.Errorf(\"Error while executing %q. Err: %v\", rst.testName, err)\n\t\t\tcontinue\n\t\t}\n\n\t}\n}\n\nfunc TestReshapeRuntime(t *testing.T) {\n\tg := NewGraph()\n\tx := NewMatrix(g, tensor.Float64, WithName(\"x\"), WithShape(28, 28), WithInit(GlorotU(1)))\n\tw := NewMatrix(g, tensor.Float64, WithName(\"W\"), WithShape(50, 784), WithInit(GlorotU(1)))\n\tx2 := Must(Reshape(x, tensor.Shape{784}))\n\twx := Must(Mul(w, x2))\n\twx2 := Must(Reshape(wx, tensor.Shape{5, 10}))\n\n\tcost := Must(Sum(wx2))\n\tif _, err := Grad(cost, w); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tm := NewTapeMachine(g)\n\tif err := m.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif !x.Value().Shape().Eq(tensor.Shape{28, 28}) {\n\t\tt.Errorf(\"A mutation of shape has occurred\")\n\t}\n}\n\nvar ravelTests = []struct {\n\tinput  tensor.Shape\n\toutput tensor.Shape\n}{\n\t{\n\t\ttensor.Shape{3, 3},\n\t\ttensor.Shape{9},\n\t},\n\t{\n\t\ttensor.Shape{2, 3},\n\t\ttensor.Shape{6},\n\t},\n\t{\n\t\ttensor.Shape{2, 1, 3},\n\t\ttensor.Shape{6},\n\t},\n\t{\n\t\ttensor.Shape{1, 1, 1},\n\t\ttensor.Shape{1},\n\t},\n}\n\nfunc TestRavel(t *testing.T) {\n\tc := require.New(t)\n\n\tfor i, rst := range ravelTests {\n\t\tg := NewGraph()\n\t\tt := NewTensor(g, Float64, len(rst.input), WithShape(rst.input...))\n\t\tt2, err := Ravel(t)\n\n\t\tc.NoError(err)\n\t\tc.Equal(rst.output, t2.Shape(), \"expected to be flatten in test case: %d\", i)\n\t}\n}\n\nfunc TestAuto(t *testing.T) {\n\ttestCases := []struct {\n\t\tdesc          string\n\t\tshapeA        tensor.Shape\n\t\tshapeB        tensor.Shape\n\t\texpectedShape tensor.Shape\n\t\texpectedErr   string\n\t}{\n\t\t{\n\t\t\tdesc:        \"Example 0\",\n\t\t\tshapeA:      tensor.Shape{12},\n\t\t\tshapeB:      tensor.Shape{1, 11},\n\t\t\texpectedErr: \"Auto failed to find broadcastable pattern: shapes (12) and (1, 11) should have the same dimensions\",\n\t\t},\n\t\t{\n\t\t\tdesc:          \"Example 1\",\n\t\t\tshapeA:        tensor.Shape{12, 1},\n\t\t\tshapeB:        tensor.Shape{12, 11},\n\t\t\texpectedShape: tensor.Shape{12, 11},\n\t\t\texpectedErr:   \"\",\n\t\t},\n\t\t{\n\t\t\tdesc:          \"Example 2\",\n\t\t\tshapeA:        tensor.Shape{1, 12},\n\t\t\tshapeB:        tensor.Shape{11, 12},\n\t\t\texpectedShape: tensor.Shape{11, 12},\n\t\t\texpectedErr:   \"\",\n\t\t},\n\t\t{\n\t\t\tdesc:          \"Example 3\",\n\t\t\tshapeA:        tensor.Shape{2, 3, 5},\n\t\t\tshapeB:        tensor.Shape{2, 3, 1},\n\t\t\texpectedShape: tensor.Shape{2, 3, 5},\n\t\t\texpectedErr:   \"\",\n\t\t},\n\t\t{\n\t\t\tdesc:          \"Example 4\",\n\t\t\tshapeA:        tensor.Shape{2, 1, 5},\n\t\t\tshapeB:        tensor.Shape{2, 3, 5},\n\t\t\texpectedShape: tensor.Shape{2, 3, 5},\n\t\t\texpectedErr:   \"\",\n\t\t},\n\t\t{\n\t\t\tdesc:          \"Example 5\",\n\t\t\tshapeA:        tensor.Shape{2, 1, 1},\n\t\t\tshapeB:        tensor.Shape{2, 5, 3},\n\t\t\texpectedShape: tensor.Shape{2, 5, 3},\n\t\t\texpectedErr:   \"\",\n\t\t},\n\t}\n\tfor _, tC := range testCases {\n\t\tt.Run(tC.desc, func(t *testing.T) {\n\t\t\tc := require.New(t)\n\n\t\t\tg := NewGraph()\n\t\t\ta := NewTensor(g, Float64, tC.shapeA.Dims(), WithShape(tC.shapeA...), WithInit(RangedFrom(0)))\n\t\t\tb := NewTensor(g, Float64, tC.shapeB.Dims(), WithShape(tC.shapeB...), WithInit(RangedFrom(0)))\n\n\t\t\tout, err := Auto(BroadcastHadamardProd, a, b)\n\n\t\t\tif tC.expectedErr != \"\" {\n\t\t\t\tc.Error(err)\n\t\t\t\tc.Equal(tC.expectedErr, err.Error())\n\t\t\t\treturn\n\t\t\t} else {\n\t\t\t\tc.NoError(err)\n\t\t\t}\n\n\t\t\tc.Equal(tC.expectedShape, out.Shape())\n\n\t\t\tout, err = Auto(BroadcastHadamardProd, b, a)\n\t\t\tc.NoError(err)\n\t\t\tc.Equal(tC.expectedShape, out.Shape())\n\t\t})\n\t}\n}\n\nfunc TestSliceBNConcat(t *testing.T) {\n\ttestCases := []struct {\n\t\tXInit             InitWFn\n\t\tXShape            tensor.Shape\n\t\tWeightsInit       InitWFn\n\t\tExpectedScale     []float64\n\t\tExpectedScaleGrad []float64\n\t\tExpectedOutput    []float64\n\t\tExpectedInputGrad []float64\n\t}{\n\t\t{\n\t\t\tXInit:             RangedFromWithStep(0.1, 2),\n\t\t\tXShape:            tensor.Shape{4, 2, 2, 2},\n\t\t\tWeightsInit:       RangedFromWithStep(-0.05, 3),\n\t\t\tExpectedScale:     []float64{-0.05, 2.95},\n\t\t\tExpectedScaleGrad: []float64{3.6115753308647314, 3.611575330864615},\n\t\t\tExpectedOutput:    []float64{17.605945678258838, 27.882593114861223, 389.7811912407563, 936.3045438041536, 17.605945678259026, 27.882593114861688, 389.78119124075647, 936.3045438041542},\n\t\t\tExpectedInputGrad: []float64{0.004972459049631833, 0.0007851252852762911, -0.003402208479079247, -0.007589542243434792, -0.2933750839282767, -0.04632239183130019, 0.20073030026567673, 0.4477829923626533, 0.007589542243434788, 0.0034022084790792466, -0.0007851252852762914, -0.004972459049631836, -0.4477829923626529, -0.20073030026567634, 0.046322391831300574, 0.2933750839282771, 0.004972459049631834, 0.0007851252852762919, -0.003402208479079246, -0.007589542243434792, -0.2933750839282794, -0.04632239183130238, 0.20073030026567504, 0.44778299236265207, 0.007589542243434788, 0.003402208479079246, -0.0007851252852762919, -0.004972459049631836, -0.4477829923626515, -0.20073030026567448, 0.04632239183130295, 0.29337508392828},\n\t\t},\n\t}\n\n\tfor i, tC := range testCases {\n\t\tt.Run(fmt.Sprintf(\"#%d %v\", i+1, tC.XShape), func(t *testing.T) {\n\t\t\tc := require.New(t)\n\n\t\t\tg := NewGraph()\n\n\t\t\tinput := NewTensor(g, Float64, tC.XShape.Dims(), WithShape(tC.XShape...), WithInit(tC.XInit), WithName(\"x\"))\n\n\t\t\tscale := NewTensor(g, Float64, 4, WithShape(1, 2, 1, 1), WithInit(tC.WeightsInit), WithName(\"scale\"))\n\t\t\tbias := NewTensor(g, Float64, 4, WithShape(1, 2, 1, 1), WithInit(tC.WeightsInit), WithName(\"bias\"))\n\n\t\t\tsl1 := Must(Slice(input, S(2, 4)))\n\t\t\tw1 := NewTensor(g, Float64, 2, WithShape(2, 8), WithInit(tC.WeightsInit), WithName(\"w1\"))\n\n\t\t\tsl2 := Must(Slice(input, S(0, 2)))\n\t\t\tw2 := NewTensor(g, Float64, 2, WithShape(2, 8), WithInit(tC.WeightsInit), WithName(\"w2\"))\n\n\t\t\tslShape := tensor.Shape{sl1.Shape()[0], tensor.Shape(sl1.Shape()[1:]).TotalSize()}\n\n\t\t\tbn1, _, _, _, err := BatchNorm(sl1, scale, bias, 0.1, 1e-5)\n\t\t\tc.NoError(err)\n\n\t\t\tbn1 = Must(Reshape(bn1, slShape))\n\n\t\t\ty1 := Must(Mul(bn1, Must(Transpose(w1, 1, 0))))\n\n\t\t\tbn2, _, _, _, err := BatchNorm(sl2, scale, bias, 0.1, 1e-5)\n\t\t\tc.NoError(err)\n\t\t\tbn2 = Must(Reshape(bn2, slShape))\n\n\t\t\ty2 := Must(Mul(bn2, Must(Transpose(w2, 1, 0))))\n\n\t\t\ty := Must(Concat(0, y1, y2))\n\n\t\t\tcost := Must(Mean(y))\n\n\t\t\t_, err = Grad(cost, input, scale)\n\t\t\tc.NoError(err)\n\n\t\t\tvm := NewTapeMachine(g) //, TraceExec())\n\t\t\tc.NoError(vm.RunAll())\n\n\t\t\tt.Logf(\"y: %v\", y.Value())\n\t\t\tt.Logf(\"dx: %v\", input.Deriv().Value())\n\t\t\tt.Logf(\"scale: %v\", scale.Value())\n\t\t\tt.Logf(\"dScale: %v\", scale.Deriv().Value())\n\n\t\t\tc.InDeltaSlice(tC.ExpectedScale, scale.Value().Data(), 1e-5, \"expected: %v\\ngot: %#v\", tC.ExpectedScale, scale.Value().Data())\n\t\t\tc.InDeltaSlice(tC.ExpectedScaleGrad, scale.Deriv().Value().Data(), 1e-5, \"expected: %v\\ngot: %#v\", tC.ExpectedScaleGrad, scale.Deriv().Value().Data())\n\n\t\t\tc.InDeltaSlice(tC.ExpectedOutput, y.Value().Data(), 1e-5, \"expected: %v\\ngot: %#v\", tC.ExpectedOutput, y.Value().Data())\n\t\t\tc.InDeltaSlice(tC.ExpectedInputGrad, input.Deriv().Value().Data(), 1e-5, \"expected: %#v\\ngot: %#v\", tC.ExpectedInputGrad, input.Deriv().Value().Data())\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "operatorLinAlg.go",
          "type": "blob",
          "size": 12.890625,
          "content": "package gorgonia\n\nimport (\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\n// ā and Ā are used to denote that it's a matrix/vector type.\n// if you want to type it, it's Latin Letter A with Macron (lowercase and capital)\n// Codepoints : U+101 for the small one, and U+100 for the capital one\n\ntype āBinaryOperator byte\n\nconst (\n\tmatMulOperator        āBinaryOperator = iota // emits S/DGEMM BLAS calls\n\tmatVecMulOperator                            // emits S/DGEMV BLAS calls\n\tvecDotOperator                               // emits S/DDOT BLAS calls\n\touterProdOperator                            // emits S/DGER BLAS calls\n\tbatchedMatMulOperator                        // just S/GEMM BLAS calls in a loop\n\n\tmaxĀBinaryOperator // delimits all possible linalg operators. Add above this line\n)\n\nfunc (op āBinaryOperator) String() string {\n\tif op >= maxĀBinaryOperator {\n\t\treturn \"UNSUPPORTED LINEAR ALGEBRA OPERATOR\"\n\t}\n\treturn āBinOpStrs[op]\n}\n\nfunc (op āBinaryOperator) Type() hm.Type {\n\tif op >= maxĀBinaryOperator {\n\t\tpanic(\"UNSUPPORTED LINEAR ALGEBRA OPERATOR\")\n\t}\n\treturn āBinOpTypes[op]()\n}\n\nfunc (op āBinaryOperator) DiffWRT(inputs int) []bool {\n\tif inputs != 2 {\n\t\tpanic(\"binary linear algebra operator only supports two and only two inputs\")\n\t}\n\n\tif op >= maxĀBinaryOperator {\n\t\tpanic(\"Unsupported unary operator is not differentiable\")\n\t}\n\treturn []bool{true, true}\n}\n\n// todo: write explanation.\nfunc matMulDiffExpr(transA, transB bool, x, y, z, gradZ *Node) (retVal Nodes, err error) {\n\tvar dzdx, dzdy *Node\n\top := linAlgBinOp{\n\t\tāBinaryOperator: matMulOperator,\n\t}\n\n\tswitch {\n\tcase transA && transB:\n\t\top.transA = transA\n\t\top.transB = transB\n\t\tif dzdx, err = binOpNode(op, y, gradZ); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, binOpNodeFail, op)\n\t\t}\n\t\tif dzdy, err = binOpNode(op, gradZ, x); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, binOpNodeFail, op)\n\t\t}\n\tcase !transA && transB:\n\t\tif dzdx, err = binOpNode(op, gradZ, y); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, binOpNodeFail, op)\n\t\t}\n\n\t\top.transA = true\n\t\tif dzdy, err = binOpNode(op, gradZ, x); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, binOpNodeFail, op)\n\t\t}\n\tcase transA && !transB:\n\t\top.transB = true\n\t\tif dzdx, err = binOpNode(op, y, gradZ); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, binOpNodeFail, op)\n\t\t}\n\n\t\top.transB = false\n\t\tif dzdy, err = binOpNode(op, x, gradZ); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, binOpNodeFail, op)\n\t\t}\n\tcase !transA && !transB:\n\t\t// dzdy\n\t\top.transA = false\n\t\top.transB = true\n\t\tif dzdx, err = binOpNode(op, gradZ, y); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, binOpNodeFail, op)\n\t\t}\n\t\t// do dzdx\n\t\top.transA = true\n\t\top.transB = false\n\t\tif dzdy, err = binOpNode(op, x, gradZ); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, binOpNodeFail, op)\n\t\t}\n\t}\n\tretVal = Nodes{dzdx, dzdy}\n\treturn\n}\n\nfunc matMulDiff(ctx ExecutionContext, transA, transB bool, x, y, z *Node) (err error) {\n\txdv, ydv, zdv := getDV3(x, y, z)\n\n\top := linAlgBinOp{\n\t\tāBinaryOperator: matMulOperator,\n\t}\n\n\tswitch {\n\tcase transA && transB:\n\t\top.transA = transA\n\t\top.transB = transB\n\n\t\t// dzdx\n\t\terr = op.IncrDo(xdv.d, ydv.Value, zdv.d)\n\t\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t\t}\n\n\t\t// dzdy\n\t\terr = op.IncrDo(ydv.d, zdv.d, xdv.Value)\n\t\tif err = checkErrSetDeriv(err, ydv); err != nil {\n\t\t\treturn errors.Wrapf(err, autodiffFail, y)\n\t\t}\n\n\t\treturn\n\n\tcase !transA && transB:\n\t\t// dzdx\n\t\terr = op.IncrDo(xdv.d, zdv.d, ydv.Value)\n\t\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t\t}\n\n\t\t// dzdy\n\t\top.transA = true\n\t\terr = op.IncrDo(ydv.d, zdv.d, xdv.Value)\n\t\tif err = checkErrSetDeriv(err, ydv); err != nil {\n\t\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t\t}\n\n\t\treturn\n\n\tcase transA && !transB:\n\t\t// dzdx\n\t\top.transB = true\n\t\terr = op.IncrDo(xdv.d, ydv.Value, zdv.d)\n\t\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t\t}\n\n\t\t// dzdy\n\t\top.transA = false\n\t\top.transB = false\n\t\terr = op.IncrDo(ydv.d, xdv.Value, zdv.d)\n\t\tif err = checkErrSetDeriv(err, ydv); err != nil {\n\t\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t\t}\n\t\treturn\n\tcase !transA && !transB:\n\t\top.transB = true\n\t\terr = op.IncrDo(xdv.d, zdv.d, ydv.Value)\n\t\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t\t}\n\n\t\top.transA = true\n\t\top.transB = false\n\t\terr = op.IncrDo(ydv.d, xdv.Value, zdv.d)\n\t\tif err = checkErrSetDeriv(err, ydv); err != nil {\n\t\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t\t}\n\t\treturn\n\t}\n\n\tpanic(\"unreachable\")\n}\n\nfunc matVecMulDiffExpr(transA, transB bool, x, y, z, gradZ *Node) (retVal Nodes, err error) {\n\tvar dzdx, dzdy *Node\n\tif transA {\n\t\tdzdx, err = OuterProd(y, gradZ)\n\t} else {\n\t\tdzdx, err = OuterProd(gradZ, y)\n\t}\n\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Failed to carry outper product\")\n\t}\n\n\top := linAlgBinOp{\n\t\tāBinaryOperator: matVecMulOperator,\n\t\ttransA:          !transA,\n\t}\n\n\tif dzdy, err = binOpNode(op, x, gradZ); err != nil {\n\t\treturn nil, errors.Wrapf(err, binOpNodeFail, op)\n\t}\n\treturn Nodes{dzdx, dzdy}, nil\n}\n\nfunc matVecMulDiff(ctx ExecutionContext, transA, transB bool, x, y, z *Node) (err error) {\n\txdv, ydv, zdv := getDV3(x, y, z)\n\n\top := linAlgBinOp{\n\t\tāBinaryOperator: outerProdOperator,\n\t}\n\n\tif transA {\n\t\terr = op.IncrDo(xdv.d, ydv.Value, zdv.d)\n\t} else {\n\t\terr = op.IncrDo(xdv.d, zdv.d, ydv.Value)\n\t}\n\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t}\n\n\top = linAlgBinOp{\n\t\tāBinaryOperator: matVecMulOperator,\n\t\ttransA:          !transA,\n\t}\n\n\terr = op.IncrDo(ydv.d, xdv.Value, zdv.d)\n\tif err = checkErrSetDeriv(err, ydv); err != nil {\n\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t}\n\treturn\n}\n\nfunc vecDotDiffExpr(transA, transB bool, x, y, z, gradZ *Node) (retVal Nodes, err error) {\n\tvar dzdx, dzdy *Node\n\tif dzdx, err = HadamardProd(y, gradZ); err == nil {\n\t\tif dzdy, err = HadamardProd(x, gradZ); err == nil {\n\t\t\tretVal = Nodes{dzdx, dzdy}\n\t\t} else {\n\t\t\treturn nil, errors.Wrap(err, \"Failed to carry HadamardProd()\")\n\t\t}\n\t} else {\n\t\treturn nil, errors.Wrap(err, \"Failed to carry HadamardProd()\")\n\t}\n\treturn\n}\n\nfunc vecDotDiff(ctx ExecutionContext, transA, transB bool, x, y, z *Node) (err error) {\n\txdv, ydv, zdv := getDV3(x, y, z)\n\n\tmul := newElemBinOp(mulOpType, x, z)\n\terr = mul.IncrDo(xdv.d, ydv.Value, zdv.d)\n\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t}\n\n\terr = mul.IncrDo(ydv.d, xdv.Value, zdv.d)\n\tif err = checkErrSetDeriv(err, ydv); err != nil {\n\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t}\n\treturn\n}\n\nfunc outerProdDiffExpr(transA, transB bool, x, y, z, gradZ *Node) (retVal Nodes, err error) {\n\tvar dzdx, dzdy *Node\n\tif dzdx, err = Mul(x, gradZ); err == nil {\n\t\tif dzdy, err = Mul(y, gradZ); err == nil {\n\t\t\tretVal = Nodes{dzdx, dzdy}\n\t\t} else {\n\t\t\treturn nil, errors.Wrap(err, \"Failed to carry Mul()\")\n\t\t}\n\t} else {\n\t\treturn nil, errors.Wrap(err, \"Failed to carry Mul()\")\n\t}\n\treturn\n}\n\nfunc outerProdDiff(ctx ExecutionContext, transA, transB bool, x, y, z *Node) (err error) {\n\txdv, ydv, zdv := getDV3(x, y, z)\n\n\tmul := newElemBinOp(mulOpType, x, z)\n\terr = mul.IncrDo(xdv.d, xdv.Value, zdv.d)\n\terr = mul.IncrDo(xdv.d, ydv.Value, zdv.d)\n\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t}\n\n\terr = mul.IncrDo(ydv.d, ydv.Value, zdv.d)\n\tif err = checkErrSetDeriv(err, ydv); err != nil {\n\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t}\n\treturn\n}\n\nfunc batchedMatMulDiffExpr(transA, transB bool, x, y, z, gradZ *Node) (retVal Nodes, err error) {\n\tvar dzdx, dzdy *Node\n\top := linAlgBinOp{\n\t\tāBinaryOperator: batchedMatMulOperator,\n\t}\n\n\tswitch {\n\tcase transA && transB:\n\t\top.transA = transA\n\t\top.transB = transB\n\t\tif dzdx, err = binOpNode(op, y, gradZ); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, binOpNodeFail, op)\n\t\t}\n\t\tif dzdy, err = binOpNode(op, gradZ, x); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, binOpNodeFail, op)\n\t\t}\n\tcase !transA && transB:\n\t\tif dzdx, err = binOpNode(op, gradZ, y); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, binOpNodeFail, op)\n\t\t}\n\n\t\top.transA = true\n\t\tif dzdy, err = binOpNode(op, gradZ, x); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, binOpNodeFail, op)\n\t\t}\n\tcase transA && !transB:\n\t\top.transB = true\n\t\tif dzdx, err = binOpNode(op, y, gradZ); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, binOpNodeFail, op)\n\t\t}\n\n\t\top.transB = false\n\t\tif dzdy, err = binOpNode(op, x, gradZ); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, binOpNodeFail, op)\n\t\t}\n\tcase !transA && !transB:\n\t\t// dzdy\n\t\top.transA = false\n\t\top.transB = true\n\t\tif dzdx, err = binOpNode(op, gradZ, y); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, binOpNodeFail, op)\n\t\t}\n\t\t// do dzdx\n\t\top.transA = true\n\t\top.transB = false\n\t\tif dzdy, err = binOpNode(op, x, gradZ); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, binOpNodeFail, op)\n\t\t}\n\t}\n\tretVal = Nodes{dzdx, dzdy}\n\treturn\n}\n\nfunc batchedMatMulDiff(ctx ExecutionContext, transA, transB bool, x, y, z *Node) (err error) {\n\txdv, ydv, zdv := getDV3(x, y, z)\n\n\top := linAlgBinOp{\n\t\tāBinaryOperator: batchedMatMulOperator,\n\t}\n\n\tswitch {\n\tcase transA && transB:\n\t\top.transA = transA\n\t\top.transB = transB\n\n\t\t// dzdx\n\t\terr = op.IncrDo(xdv.d, ydv.Value, zdv.d)\n\t\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t\t}\n\n\t\t// dzdy\n\t\terr = op.IncrDo(ydv.d, zdv.d, xdv.Value)\n\t\tif err = checkErrSetDeriv(err, ydv); err != nil {\n\t\t\treturn errors.Wrapf(err, autodiffFail, y)\n\t\t}\n\n\t\treturn\n\n\tcase !transA && transB:\n\t\t// dzdx\n\t\terr = op.IncrDo(xdv.d, zdv.d, ydv.Value)\n\t\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t\t}\n\n\t\t// dzdy\n\t\top.transA = true\n\t\terr = op.IncrDo(ydv.d, zdv.d, xdv.Value)\n\t\tif err = checkErrSetDeriv(err, ydv); err != nil {\n\t\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t\t}\n\n\t\treturn\n\n\tcase transA && !transB:\n\t\t// dzdx\n\t\top.transB = true\n\t\terr = op.IncrDo(xdv.d, ydv.Value, zdv.d)\n\t\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t\t}\n\n\t\t// dzdy\n\t\top.transA = false\n\t\top.transB = false\n\t\terr = op.IncrDo(ydv.d, xdv.Value, zdv.d)\n\t\tif err = checkErrSetDeriv(err, ydv); err != nil {\n\t\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t\t}\n\t\treturn\n\tcase !transA && !transB:\n\t\top.transB = true\n\t\terr = op.IncrDo(xdv.d, zdv.d, ydv.Value)\n\t\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t\t}\n\n\t\top.transA = true\n\t\top.transB = false\n\t\terr = op.IncrDo(ydv.d, xdv.Value, zdv.d)\n\t\tif err = checkErrSetDeriv(err, ydv); err != nil {\n\t\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t\t}\n\t\treturn\n\t}\n\n\tpanic(\"unreachable\")\n}\n\nfunc reshape(name string, t tensor.Tensor, shape ...int) error {\n\tif t.Shape().Eq(shape) {\n\t\treturn nil\n\t}\n\tif t.DataOrder().IsContiguous() {\n\t\tif err := t.Reshape(shape...); err != nil {\n\t\t\treturn errors.Wrapf(err, \"Reshaping slice for %s failed\", name)\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc batchedMatMul(a, b, c tensor.Tensor, transA, transB, incr bool) (retVal tensor.Tensor, err error) {\n\tshapeA := a.Shape().Clone()\n\tshapeB := b.Shape().Clone()\n\touter := shapeA[:len(shapeA)-2]\n\tinnerA := shapeA[len(shapeA)-2:]\n\tinnerB := shapeB[len(shapeB)-2:]\n\tif transA {\n\t\tinnerA[0], innerA[1] = innerA[1], innerA[0]\n\t}\n\tif transB {\n\t\tinnerB[0], innerB[1] = innerB[1], innerB[0]\n\t}\n\n\tif c == nil {\n\t\tnewShape := append(outer.Clone(), innerA[0], innerB[1])\n\t\tc = tensor.New(tensor.Of(a.Dtype()), tensor.WithShape(newShape...), tensor.WithEngine(a.Engine()))\n\t}\n\n\tslices := make([]sli, len(outer))\n\tss := make([]tensor.Slice, len(slices))\n\tfor i := range slices {\n\t\tslices[i].end = slices[i].start + 1\n\t\tss[i] = &slices[i]\n\t}\n\n\tvar as, bs, cs tensor.Tensor\n\tfor halt := false; !halt; halt = incrSlices(slices, outer) {\n\t\tif as, err = a.Slice(ss...); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, \"Slicing %v from a failed\", ss)\n\t\t}\n\t\tif bs, err = b.Slice(ss...); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, \"Slicing %v from b failed\", ss)\n\t\t}\n\t\tif cs, err = c.Slice(ss...); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, \"Slicing %v from c failed\", ss)\n\t\t}\n\n\t\tif transA {\n\t\t\tas.T()\n\t\t}\n\t\tif transB {\n\t\t\tbs.T()\n\t\t}\n\n\t\t// Reshape the result matrix slice in case matrices like 1x1 will be converted to scalar which results in\n\t\t// not satisfying matrix multiplication dimension requirements.\n\t\tif err := reshape(\"a\", as, innerA...); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif err := reshape(\"b\", bs, innerB...); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tvar fo tensor.FuncOpt\n\t\tif incr {\n\t\t\tfo = tensor.WithIncr(cs)\n\t\t} else {\n\t\t\tfo = tensor.WithReuse(cs)\n\t\t}\n\n\t\tif _, err = tensor.MatMul(as, bs, fo); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, \"MatMul on batch %v failed.\", ss)\n\t\t}\n\n\t}\n\n\treturn c, nil\n}\n\n// incrSlices increments the slices. If everything has matched then return true\nfunc incrSlices(a []sli, shp tensor.Shape) (halt bool) {\n\tfor i := len(a) - 1; i >= 0; i-- {\n\t\tif shp[i]-a[i].start == 1 {\n\t\t\ta[i].start = 0\n\t\t\ta[i].end = 1\n\t\t\tif i == 0 {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\ta[i].start++\n\t\ta[i].end = a[i].start + 1\n\t\treturn false\n\t}\n\treturn true\n}\n"
        },
        {
          "name": "operatorLinAlg_const.go",
          "type": "blob",
          "size": 2.0771484375,
          "content": "package gorgonia\n\nimport \"github.com/chewxy/hm\"\n\n// āBinOpStrs is the string representation for binLAOperator\n// It should be held constant\nvar āBinOpStrs = [maxĀBinaryOperator]string{\n\t\"×\",   // matMulOperator\n\t\"×\",   // matVecMulOperator\n\t\"⋅\",   // vecDotOperator\n\t\"⊗\",   // outerProdOperator\n\t\"×××\", // batchedMatMulOperator\n}\n\nvar āBinOpDiffExprs = [maxĀBinaryOperator]func(tA, tB bool, x, y, z, grad *Node) (Nodes, error){\n\tmatMulDiffExpr,\n\tmatVecMulDiffExpr,\n\tvecDotDiffExpr,\n\touterProdDiffExpr,\n\tbatchedMatMulDiffExpr,\n}\n\nvar āBinOpDiffs = [maxĀBinaryOperator]func(ctx ExecutionContext, tA, tB bool, x, y, z *Node) error{\n\tmatMulDiff,\n\tmatVecMulDiff,\n\tvecDotDiff,\n\touterProdDiff,\n\tbatchedMatMulDiff,\n}\n\nvar āBinOpTypes = [maxĀBinaryOperator]func() hm.Type{\n\tmatMulType,\n\tmatVecMulType,\n\tvecDotType,\n\touterProdType,\n\tbatchedMatMulType,\n}\n\n/* TYPES FOR LINALG BINARY OP*/\n\n// matVecMulOp is a function with this type:\n//\t\tmatVecMulOp :: (Float a) ⇒ Vector a → Matrix a → Vector a\n//\n// For the moment only floats are allowed\nfunc matVecMulType() hm.Type {\n\ta := hm.TypeVariable('a')\n\tv := makeTensorType(1, a)\n\tm := makeTensorType(2, a)\n\n\treturn hm.NewFnType(m, v, v)\n}\n\n// matMulOp is a function with this type:\n//\t\tmatMulOp :: (Float a) ⇒ Matrix a → Matrix a → Matrix a\n//\n// For the moment only floats are allowed\nfunc matMulType() hm.Type {\n\ta := hm.TypeVariable('a')\n\tm := makeTensorType(2, a)\n\n\treturn hm.NewFnType(m, m, m)\n}\n\n// vecDotOp is a function with this type:\n//\t\tvecDotOp :: (Float a) ⇒ Vector a → Vector a → a\n//\n// For the moment only floats are allowed\nfunc vecDotType() hm.Type {\n\ta := hm.TypeVariable('a')\n\tv := makeTensorType(1, a)\n\n\treturn hm.NewFnType(v, v, a)\n}\n\n// outerProdOp is a function with this type:\n//\t\touterProdOp :: (Float a) ⇒ Vector a → Vector a → Matrix a\n//\n// For the moment only floats are allowed\nfunc outerProdType() hm.Type {\n\ta := hm.TypeVariable('a')\n\tv := makeTensorType(1, a)\n\tm := makeTensorType(2, a)\n\n\treturn hm.NewFnType(v, v, m)\n}\n\nfunc batchedMatMulType() hm.Type {\n\ta := hm.TypeVariable('a')\n\treturn hm.NewFnType(a, a, a)\n}\n"
        },
        {
          "name": "operatorPointwise_binary.go",
          "type": "blob",
          "size": 23.375,
          "content": "package gorgonia\n\nimport (\n\t\"math\"\n\n\t\"github.com/chewxy/math32\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\ntype incrDoerBinOp interface {\n\tIncrDo(v Value, retSame bool, inputs ...Value) error\n}\ntype usePreallocDoerBinOp interface {\n\tUsePreallocDo(v Value, retSame bool, inputs ...Value) (retVal Value, err error)\n}\ntype unsafeDoerBinOp interface {\n\tUnsafeDo(retSame bool, inputs ...Value) (Value, error)\n}\n\n/* BINARY OPERATOR */\n\ntype ʘBinaryOperator interface {\n\tisArith() bool\n\tbinOpType() ʘBinaryOperatorType\n\tDo(bool, ...Value) (Value, error)\n\tString() string\n}\n\ntype scalarBinOp struct {\n\tʘBinaryOperatorType\n\tt tensor.Dtype\n}\n\nfunc (o scalarBinOp) Arity() int                     { return 2 }\nfunc (o scalarBinOp) binOpType() ʘBinaryOperatorType { return o.ʘBinaryOperatorType }\nfunc (o scalarBinOp) isArith() bool                  { return o.ʘBinaryOperatorType.isArith() }\nfunc (o scalarBinOp) String() string                 { return o.ʘBinaryOperatorType.String() }\n\nfunc (o scalarBinOp) Do(same bool, vals ...Value) (retVal Value, err error) {\n\tif err = checkArity(o, len(vals)); err != nil {\n\t\treturn\n\t}\n\n\tat := TypeOf(vals[0])\n\tbt := TypeOf(vals[1])\n\tif !at.Eq(bt) {\n\t\terr = errors.Errorf(\"Type Mismatch: %v != %v\", at, bt)\n\t\treturn\n\t}\n\n\tvar r interface{} // float or bool only plz\n\tswitch a := vals[0].(type) {\n\tcase *F64:\n\t\tb := vals[1].(*F64)\n\t\tswitch o.ʘBinaryOperatorType {\n\t\tcase addOpType:\n\t\t\tr = NewF64(a.any() + b.any())\n\t\tcase subOpType:\n\t\t\tr = NewF64(a.any() - b.any())\n\t\tcase mulOpType:\n\t\t\tr = NewF64(a.any() * b.any())\n\t\tcase divOpType:\n\t\t\tr = NewF64(a.any() / b.any())\n\t\tcase powOpType:\n\t\t\tr = NewF64(math.Pow(a.any(), b.any()))\n\t\tcase ltOpType:\n\t\t\tr = NewB(a.any() < b.any())\n\t\tcase gtOpType:\n\t\t\tr = NewB(a.any() > b.any())\n\t\tcase lteOpType:\n\t\t\tr = NewB(a.any() <= b.any())\n\t\tcase gteOpType:\n\t\t\tr = NewB(a.any() >= b.any())\n\t\tcase eqOpType:\n\t\t\tr = NewB(a.any() == b.any())\n\t\tcase neOpType:\n\t\t\tr = NewB(a.any() != b.any())\n\t\tdefault:\n\t\t\terr = errors.Errorf(nyiFail, \"scalarBinOp.Do() - Float64\", o.ʘBinaryOperatorType)\n\t\t}\n\n\t\tif same && !o.isArith() {\n\t\t\tif *(r.(*B)) {\n\t\t\t\tr = NewF64(1.0)\n\t\t\t} else {\n\t\t\t\tr = NewF64(0.0)\n\t\t\t}\n\t\t}\n\n\tcase *F32:\n\t\tb := vals[1].(*F32)\n\t\tswitch o.ʘBinaryOperatorType {\n\t\tcase addOpType:\n\t\t\tr = NewF32(a.any() + b.any())\n\t\tcase subOpType:\n\t\t\tr = NewF32(a.any() - b.any())\n\t\tcase mulOpType:\n\t\t\tr = NewF32(a.any() * b.any())\n\t\tcase divOpType:\n\t\t\tr = NewF32(a.any() / b.any())\n\t\tcase powOpType:\n\t\t\tr = NewF32(math32.Pow(float32(a.any()), float32(b.any())))\n\t\tcase ltOpType:\n\t\t\tr = NewB(a.any() < b.any())\n\t\tcase gtOpType:\n\t\t\tr = NewB(a.any() > b.any())\n\t\tcase lteOpType:\n\t\t\tr = NewB(a.any() <= b.any())\n\t\tcase gteOpType:\n\t\t\tr = NewB(a.any() >= b.any())\n\t\tcase eqOpType:\n\t\t\tr = NewB(a.any() == b.any())\n\t\tcase neOpType:\n\t\t\tr = NewB(a.any() != b.any())\n\t\tdefault:\n\t\t\terr = errors.Errorf(nyiFail, \"scalarBinOp.Do() - Float32\", o.ʘBinaryOperatorType)\n\t\t}\n\n\t\tif same && !o.isArith() {\n\t\t\tif *(r.(*B)) {\n\t\t\t\tr = NewF32(1)\n\t\t\t} else {\n\t\t\t\tr = NewF32(0)\n\t\t\t}\n\t\t}\n\n\tcase *I:\n\t\tb := vals[1].(*I)\n\t\tswitch o.ʘBinaryOperatorType {\n\t\tcase addOpType:\n\t\t\tr = NewI(a.any() + b.any())\n\t\tcase subOpType:\n\t\t\tr = NewI(a.any() - b.any())\n\t\tcase mulOpType:\n\t\t\tr = NewI(a.any() * b.any())\n\t\tcase divOpType:\n\t\t\tr = NewI(a.any() / b.any())\n\t\t// case powOpType:\n\t\t// \tr = math.Pow(a, b)\n\t\tcase ltOpType:\n\t\t\tr = NewB(a.any() < b.any())\n\t\tcase gtOpType:\n\t\t\tr = NewB(a.any() > b.any())\n\t\tcase lteOpType:\n\t\t\tr = NewB(a.any() <= b.any())\n\t\tcase gteOpType:\n\t\t\tr = NewB(a.any() >= b.any())\n\t\tcase eqOpType:\n\t\t\tr = NewB(a.any() == b.any())\n\t\tcase neOpType:\n\t\t\tr = NewB(a.any() != b.any())\n\t\tdefault:\n\t\t\terr = errors.Errorf(nyiFail, \"scalarBinOp.Do() - Int\", o.ʘBinaryOperatorType)\n\t\t}\n\n\t\tif same && !o.isArith() {\n\t\t\tif *(r.(*B)) {\n\t\t\t\tr = NewI(1)\n\t\t\t} else {\n\t\t\t\tr = NewI(0)\n\t\t\t}\n\t\t}\n\tcase *I32:\n\t\tb := vals[1].(*I32)\n\t\tswitch o.ʘBinaryOperatorType {\n\t\tcase addOpType:\n\t\t\tr = NewI32(a.any() + b.any())\n\t\tcase subOpType:\n\t\t\tr = NewI32(a.any() - b.any())\n\t\tcase mulOpType:\n\t\t\tr = NewI32(a.any() * b.any())\n\t\tcase divOpType:\n\t\t\tr = NewI32(a.any() / b.any())\n\t\t// case powOpType:\n\t\t// \tr = math.Pow(a, b)\n\t\tcase ltOpType:\n\t\t\tr = NewB(a.any() < b.any())\n\t\tcase gtOpType:\n\t\t\tr = NewB(a.any() > b.any())\n\t\tcase lteOpType:\n\t\t\tr = NewB(a.any() <= b.any())\n\t\tcase gteOpType:\n\t\t\tr = NewB(a.any() >= b.any())\n\t\tcase eqOpType:\n\t\t\tr = NewB(a.any() == b.any())\n\t\tcase neOpType:\n\t\t\tr = NewB(a.any() != b.any())\n\t\tdefault:\n\t\t\terr = errors.Errorf(nyiFail, \"scalarBinOp.Do() - Int32\", o.ʘBinaryOperatorType)\n\t\t}\n\n\t\tif same && !o.isArith() {\n\t\t\tif *(r.(*B)) {\n\t\t\t\tr = NewI32(1)\n\t\t\t} else {\n\t\t\t\tr = NewI32(0)\n\t\t\t}\n\t\t}\n\tcase *I64:\n\t\tb := vals[1].(*I64)\n\t\tswitch o.ʘBinaryOperatorType {\n\t\tcase addOpType:\n\t\t\tr = NewI64(a.any() + b.any())\n\t\tcase subOpType:\n\t\t\tr = NewI64(a.any() - b.any())\n\t\tcase mulOpType:\n\t\t\tr = NewI64(a.any() * b.any())\n\t\tcase divOpType:\n\t\t\tr = NewI64(a.any() / b.any())\n\t\t// case powOpType:\n\t\t// \tr = math.Pow(a, b)\n\t\tcase ltOpType:\n\t\t\tr = NewB(a.any() < b.any())\n\t\tcase gtOpType:\n\t\t\tr = NewB(a.any() > b.any())\n\t\tcase lteOpType:\n\t\t\tr = NewB(a.any() <= b.any())\n\t\tcase gteOpType:\n\t\t\tr = NewB(a.any() >= b.any())\n\t\tcase eqOpType:\n\t\t\tr = NewB(a.any() == b.any())\n\t\tcase neOpType:\n\t\t\tr = NewB(a.any() != b.any())\n\t\tdefault:\n\t\t\terr = errors.Errorf(nyiFail, \"scalarBinOp.Do() - Int64\", o.ʘBinaryOperatorType)\n\t\t}\n\n\t\tif same && !o.isArith() {\n\t\t\tif *(r.(*B)) {\n\t\t\t\tr = NewI64(1)\n\t\t\t} else {\n\t\t\t\tr = NewI64(0)\n\t\t\t}\n\t\t}\n\tcase *U8:\n\t\tb := vals[1].(*U8)\n\t\tswitch o.ʘBinaryOperatorType {\n\t\tcase addOpType:\n\t\t\tr = NewU8(a.any() + b.any())\n\t\tcase subOpType:\n\t\t\tr = NewU8(a.any() - b.any())\n\t\tcase mulOpType:\n\t\t\tr = NewU8(a.any() * b.any())\n\t\tcase divOpType:\n\t\t\tr = NewU8(a.any() / b.any())\n\t\t// case powOpType:\n\t\t// \tr = math.Pow(a, b)\n\t\tcase ltOpType:\n\t\t\tr = NewB(a.any() < b.any())\n\t\tcase gtOpType:\n\t\t\tr = NewB(a.any() > b.any())\n\t\tcase lteOpType:\n\t\t\tr = NewB(a.any() <= b.any())\n\t\tcase gteOpType:\n\t\t\tr = NewB(a.any() >= b.any())\n\t\tcase eqOpType:\n\t\t\tr = NewB(a.any() == b.any())\n\t\tcase neOpType:\n\t\t\tr = NewB(a.any() != b.any())\n\t\tdefault:\n\t\t\terr = errors.Errorf(nyiFail, \"scalarBinOp.Do() - Byte\", o.ʘBinaryOperatorType)\n\t\t}\n\n\t\tif same && !o.isArith() {\n\t\t\tif *(r.(*B)) {\n\t\t\t\tr = NewU8(1)\n\t\t\t} else {\n\t\t\t\tr = NewU8(0)\n\t\t\t}\n\t\t}\n\tcase *B:\n\t\tb := vals[1].(*B)\n\t\tswitch o.ʘBinaryOperatorType {\n\t\tcase eqOpType:\n\t\t\tr = NewB(a.any() == b.any())\n\t\tcase neOpType:\n\t\t\tr = NewB(a.any() != b.any())\n\t\tdefault:\n\t\t\terr = errors.Errorf(nyiFail, \"scalarBinOp.Do() - Bool\", o.ʘBinaryOperatorType)\n\t\t}\n\n\tdefault:\n\t\terr = errors.Errorf(nyiFail, \"scalarBinOp.Do() - Unhandled Scalar Type\", o.t)\n\t}\n\n\tif err != nil {\n\t\treturn\n\t}\n\n\tretVal, _ = anyToScalar(r)\n\treturn\n}\n\ntype tBinOp struct {\n\tʘBinaryOperatorType\n\ttensorLeft bool\n}\n\nfunc (o tBinOp) Arity() int                     { return 2 }\nfunc (o tBinOp) binOpType() ʘBinaryOperatorType { return o.ʘBinaryOperatorType }\nfunc (o tBinOp) String() string                 { return o.ʘBinaryOperatorType.String() }\nfunc (o tBinOp) isArith() bool                  { return o.ʘBinaryOperatorType.isArith() }\n\nfunc (o tBinOp) Do(same bool, inputs ...Value) (Value, error) {\n\tif same {\n\t\treturn o.do(inputs, tensor.AsSameType())\n\t}\n\treturn o.do(inputs)\n}\n\nfunc (o tBinOp) UnsafeDo(retSame bool, inputs ...Value) (Value, error) {\n\tif retSame {\n\t\treturn o.do(inputs, tensor.AsSameType(), tensor.UseUnsafe())\n\t}\n\treturn o.do(inputs, tensor.UseUnsafe())\n}\nfunc (o tBinOp) UsePreallocDo(v Value, retSame bool, inputs ...Value) (retVal Value, err error) {\n\tt, ok := v.(tensor.Tensor)\n\tif !ok {\n\t\treturn nil, errors.Errorf(\"Expected Tensor as preallocated value. Got %v of %T instead\", v, v)\n\t}\n\n\treuse := t\n\tif retSame {\n\t\treturn o.do(inputs, tensor.WithReuse(reuse), tensor.AsSameType())\n\t}\n\treturn o.do(inputs, tensor.WithReuse(reuse))\n}\n\nfunc (o tBinOp) IncrDo(incr Value, retSame bool, inputs ...Value) (err error) {\n\treuse, ok := incr.(tensor.Tensor)\n\tif ok {\n\t\t_, err = o.do(inputs, tensor.WithIncr(reuse))\n\t\treturn\n\t}\n\n\tvar retVal Value\n\tif retSame {\n\t\tif retVal, err = o.do(inputs, tensor.AsSameType()); err != nil {\n\t\t\treturn errors.Wrapf(err, doFail, o)\n\t\t}\n\t} else {\n\t\tif retVal, err = o.do(inputs); err != nil {\n\t\t\treturn errors.Wrapf(err, doFail, o)\n\t\t}\n\n\t}\n\n\tadd := newEBOByType(addOpType, TypeOf(incr), TypeOf(retVal))\n\tif retVal, err = add.UnsafeDo(incr, retVal); err != nil {\n\t\treturn errors.Wrapf(err, unsafeDoFail, add)\n\t}\n\n\terr = noIncrErr{retVal}\n\treturn\n}\n\nfunc (o tBinOp) do(vals []Value, opts ...tensor.FuncOpt) (retVal Value, err error) {\n\tif err = checkArity(o, len(vals)); err != nil {\n\t\treturn\n\t}\n\n\t// typecheck the operands\n\td0 := vals[0].Dtype()\n\td1 := vals[1].Dtype()\n\n\tif d0 != d1 {\n\t\treturn nil, errors.Errorf(\"Dtype mismatch for bin op: %v and %v\", d0, d1)\n\t}\n\n\t// extract the goddamn values\n\tvar a, b interface{}\n\tif o.tensorLeft {\n\t\tt, ok := vals[0].(tensor.Tensor)\n\t\tif !ok {\n\t\t\treturn nil, errors.Errorf(\"Expected left value to be Tensor. Got %v of %T instead\", vals[0], vals[0])\n\t\t}\n\t\ta = tensor.Materialize(t)\n\t\t// a = t\n\n\t\tswitch other := vals[1].(type) {\n\t\tcase *F64:\n\t\t\tb = other.any()\n\t\tcase *F32:\n\t\t\tb = other.any()\n\t\tcase tensor.Tensor:\n\t\t\tb = tensor.Materialize(other)\n\t\tdefault:\n\t\t\treturn nil, errors.Errorf(nyiFail, \"tBinOp.do()\", vals[1])\n\t\t}\n\t} else {\n\t\tt, ok := vals[1].(tensor.Tensor)\n\t\tif !ok {\n\t\t\treturn nil, errors.Errorf(\"Expected right value to be Tensor. Got %v of %T instead\", vals[1], vals[1])\n\t\t}\n\t\tb = tensor.Materialize(t)\n\n\t\tswitch other := vals[0].(type) {\n\t\tcase *F64:\n\t\t\ta = other.any()\n\t\tcase *F32:\n\t\t\ta = other.any()\n\t\tcase tensor.Tensor:\n\t\t\ta = tensor.Materialize(other)\n\t\tdefault:\n\t\t\treturn nil, errors.Errorf(nyiFail, \"tBinOp.do()\", vals[1])\n\t\t}\n\t}\n\n\tif o.isArith() {\n\t\tfn := binOps[o.ʘBinaryOperatorType]\n\t\tif fn == nil {\n\t\t\treturn nil, errors.Errorf(\"nil function returned for %v\", o.ʘBinaryOperatorType)\n\t\t}\n\t\tretVal, err = (*fn)(a, b, opts...)\n\t} else {\n\t\tfn := cmpOps[o.ʘBinaryOperatorType]\n\t\tif fn == nil {\n\t\t\treturn nil, errors.Errorf(\"nil function returned for %v\", o.ʘBinaryOperatorType)\n\t\t}\n\t\tretVal, err = (*fn)(a, b, opts...)\n\n\t}\n\treturn\n}\n\n// type binDiffFn func(x, y, z, gradZ *Node) (Nodes, err error)\n\nfunc addDiffExpr(x, y, z, gradZ *Node) (retVal Nodes, err error) {\n\treturn Nodes{gradZ, gradZ}, nil\n}\n\nfunc addDiff(ctx ExecutionContext, x, y, z *Node) (err error) {\n\txdv, ydv := getDV(x, y)\n\n\t// set up the op to be executed\n\top := NewAddOp(x, z, ctx)\n\top.Device = x.Device()\n\top.UseUnsafe = true\n\n\t// we'll use the same device as the device the data from the node resides in\n\tdev := op.Device\n\n\tvar d, xd, yd, zd Value\n\tvar extra bool\n\n\t// allocate if necessary\n\tif xd, extra, err = x.GradOnDevice(dev, ctx.External); err != nil {\n\t\treturn errors.Wrapf(err, gradOnDeviceFail, x, dev)\n\t}\n\tif extra {\n\t\tdefer ctx.PutValue(dev, xd)\n\t}\n\n\tif zd, extra, err = z.GradOnDevice(dev, ctx.External); err != nil {\n\t\treturn errors.Wrapf(err, gradOnDeviceFail, z, dev)\n\t}\n\tif extra {\n\t\tdefer ctx.PutValue(dev, xd)\n\t}\n\n\t// if x is scalar, an additional vector needs to be acquired\n\tif x.IsScalar() && dev != CPU {\n\t\tvar mem tensor.Memory\n\t\tvar xd2 Value\n\t\tmemsize := calcMemSize(zd.Dtype(), zd.Shape())\n\t\tif mem, err = ctx.Get(dev, memsize); err != nil {\n\t\t\treturn\n\t\t}\n\n\t\tif xd2, err = makeValueFromMem(z.t, zd.Shape(), mem); err != nil {\n\t\t\treturn\n\t\t}\n\n\t\top.Prealloc = xd2\n\t\tdefer ctx.Signal()\n\t}\n\n\t// xd += zd\n\tif d, err = op.Do(xd, zd); err != nil {\n\t\treturn errors.Wrapf(err, doFail, op)\n\t}\n\txdv.SetDeriv(d)\n\n\t// set up the op to be executed for y\n\top = NewAddOp(y, z, ctx)\n\top.Device = y.Device()\n\top.UseUnsafe = true\n\n\tdev = op.Device\n\n\tif yd, extra, err = y.GradOnDevice(dev, ctx.External); err != nil {\n\t\treturn errors.Wrapf(err, gradOnDeviceFail, y, dev)\n\t}\n\tif extra {\n\t\tdefer ctx.PutValue(dev, yd)\n\t}\n\n\tif zd, extra, err = z.GradOnDevice(dev, ctx.External); err != nil {\n\t\treturn errors.Wrapf(err, gradOnDeviceFail, z, dev)\n\t}\n\tif extra {\n\t\tdefer ctx.PutValue(dev, zd)\n\t}\n\n\t// if y is scalar, an additional vector needs to be acquired\n\tif y.IsScalar() && dev != CPU {\n\t\tvar mem tensor.Memory\n\t\tvar yd2 Value\n\t\tmemsize := calcMemSize(zd.Dtype(), zd.Shape())\n\t\tif mem, err = ctx.Get(dev, memsize); err != nil {\n\t\t\treturn\n\t\t}\n\t\tif yd2, err = makeValueFromMem(z.t, zd.Shape(), mem); err != nil {\n\t\t\treturn\n\t\t}\n\n\t\top.Prealloc = yd2\n\t\tdefer ctx.Signal()\n\t}\n\n\t// yd += zd\n\tif d, err = op.Do(yd, zd); err != nil {\n\t\treturn errors.Wrapf(err, doFail, op)\n\t}\n\tydv.SetDeriv(d) // ignore errors on purpose\n\n\treturn nil\n}\n\nfunc subDiffExpr(x, y, z, gradZ *Node) (retVal Nodes, err error) {\n\tvar dzdy *Node\n\tif dzdy, err = Neg(gradZ); err == nil {\n\t\tWithGroupName(gradClust)(dzdy)\n\t\tWithGroupName(gradClust)(gradZ)\n\t\tretVal = Nodes{gradZ, dzdy}\n\t} else {\n\t\treturn nil, errors.Wrap(err, \"Failed to carry Neg()\")\n\t}\n\treturn\n}\n\nfunc subDiff(ctx ExecutionContext, x, y, z *Node) (err error) {\n\txdv, ydv := getDV(x, y)\n\n\tadd := NewAddOp(x, z, ctx)\n\tsub := NewSubOp(y, z, ctx)\n\tadd.Device = x.Device()\n\tsub.Device = y.Device()\n\tsub.UseUnsafe = true\n\tadd.UseUnsafe = true\n\t// sub := newEBOByType(subOpType, y.t, z.t)\n\t// add := newEBOByType(addOpType, x.t, z.t)\n\n\tdev := sub.Device\n\tvar xd, yd, zd, d Value\n\tvar extra bool\n\n\tif zd, extra, err = z.GradOnDevice(dev, ctx.External); err != nil {\n\t\treturn errors.Wrapf(err, gradOnDeviceFail, z, dev)\n\t}\n\tif extra {\n\t\tdefer ctx.PutValue(dev, zd)\n\t}\n\n\tif yd, extra, err = y.GradOnDevice(dev, ctx.External); err != nil {\n\t\treturn errors.Wrapf(err, gradOnDeviceFail, y, dev)\n\t}\n\tif extra {\n\t\tdefer ctx.PutValue(dev, yd)\n\t}\n\n\t// if y is scalar an additional vector needs to be allocated for the prelloc\n\tswitch {\n\tcase y.IsScalar() && dev != CPU:\n\t\tvar mem tensor.Memory\n\t\tvar yd2 Value\n\t\tmemsize := calcMemSize(zd.Dtype(), zd.Shape())\n\t\tif mem, err = ctx.Get(dev, memsize); err != nil {\n\t\t\treturn errors.Wrapf(err, allocFail, memsize, dev)\n\t\t}\n\t\tif yd2, err = makeValueFromMem(z.t, zd.Shape(), mem); err != nil {\n\t\t\treturn errors.Wrapf(err, makeValueFail, z.t, zd.Shape())\n\t\t}\n\n\t\tsub.Prealloc = yd2\n\t\tdefer ctx.Signal()\n\tcase y.IsScalar() && dev == CPU:\n\t\tif sub.Prealloc, err = makeValue(z.t, zd.Shape()); err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\n\t// dz/dy\n\tif d, err = sub.Do(yd, zd); err != nil {\n\t\treturn errors.Wrapf(err, doFail, sub)\n\t}\n\tydv.SetDeriv(d) // errors are ignored on purpose\n\n\t//\thandle x\n\n\tdev = add.Device\n\tif zd, extra, err = z.GradOnDevice(dev, ctx.External); err != nil {\n\t\treturn errors.Wrapf(err, gradOnDeviceFail, z, dev)\n\t}\n\tif extra {\n\t\tdefer ctx.PutValue(dev, zd)\n\t}\n\n\tif xd, extra, err = x.GradOnDevice(dev, ctx.External); err != nil {\n\t\treturn errors.Wrapf(err, gradOnDeviceFail, x, dev)\n\t}\n\tif extra {\n\t\tdefer ctx.PutValue(dev, xd)\n\t}\n\n\tswitch {\n\tcase x.IsScalar() && dev != CPU:\n\t\tvar mem tensor.Memory\n\t\tvar xd2 Value\n\t\tmemsize := calcMemSize(zd.Dtype(), zd.Shape())\n\t\tif mem, err = ctx.Get(dev, memsize); err != nil {\n\t\t\treturn\n\t\t}\n\n\t\tif xd2, err = makeValueFromMem(z.t, zd.Shape(), mem); err != nil {\n\t\t\treturn\n\t\t}\n\t\tadd.Prealloc = xd2\n\t\tdefer ctx.Signal()\n\tcase x.IsScalar() && dev == CPU:\n\t\tif sub.Prealloc, err = makeValue(z.t, zd.Shape()); err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\n\t// dz/dx\n\tif d, err = add.Do(xd, zd); err != nil {\n\t\treturn errors.Wrapf(err, doFail, add)\n\t}\n\txdv.SetDeriv(d) // ignore errors on purpose\n\n\treturn nil\n}\n\nfunc hadamardProdDiffExpr(x, y, z, gradZ *Node) (retVal Nodes, err error) {\n\tvar dzdx, dzdy *Node\n\tif dzdx, err = HadamardProd(y, gradZ); err == nil {\n\t\tdzdy, err = HadamardProd(x, gradZ)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"Failed to carry HadamardProd()\")\n\t\t}\n\t\tWithGroupName(gradClust)(dzdx)\n\t\tWithGroupName(gradClust)(dzdy)\n\t\tretVal = Nodes{dzdx, dzdy}\n\t\treturn\n\t}\n\treturn nil, errors.Wrap(err, \"Failed to carry HadamardProd()\")\n}\n\nfunc hadamardProdDiff(ctx ExecutionContext, x, y, z *Node) (err error) {\n\txdv, ydv := getDV(x, y)\n\n\tvar mul *ExternalOp\n\tvar dev Device\n\tvar xd, yd, zd, d Value\n\tvar extra bool\n\n\tif x.isConstant() {\n\t\tgoto dzdy\n\t}\n\n\t//dzdx\n\tmul = NewHadamardProdOp(y, z, ctx)\n\tmul.Device = x.Device()\n\tdev = mul.Device\n\n\tif xd, extra, err = x.GradOnDevice(dev, ctx.External); err != nil {\n\t\treturn errors.Wrapf(err, gradOnDeviceFail, x, dev)\n\t}\n\tif extra {\n\t\tdefer ctx.PutValue(dev, xd)\n\t}\n\n\tif yd, extra, err = y.ValueOnDevice(dev, ctx.External); err != nil {\n\t\treturn errors.Wrapf(err, gradOnDeviceFail, y, dev)\n\t}\n\tif extra {\n\t\tdefer ctx.PutValue(dev, yd)\n\t}\n\n\tif zd, extra, err = z.GradOnDevice(dev, ctx.External); err != nil {\n\t\treturn errors.Wrapf(err, gradOnDeviceFail, z, dev)\n\t}\n\tif extra {\n\t\tdefer ctx.PutValue(dev, zd)\n\t}\n\n\tmul.Incr = xd\n\n\t// if y is Scalar, then it needs to be broadcasted across to the\n\tif x.IsScalar() && dev != CPU && !zd.Shape().IsScalar() {\n\t\tvar memIncr, mem2 tensor.Memory\n\t\tvar xdIncr, xd2 Value\n\t\tmemsize := calcMemSize(zd.Dtype(), zd.Shape())\n\t\tif mem2, err = ctx.Get(dev, memsize); err != nil {\n\t\t\treturn errors.Wrapf(err, allocFail, memsize, dev)\n\t\t}\n\n\t\tif xd2, err = makeValueFromMem(z.t, zd.Shape(), mem2); err != nil {\n\t\t\treturn errors.Wrapf(err, makeValueFail, z.t, zd.Shape())\n\t\t}\n\n\t\t// \"broadcast\" x (in a very sloppy way)\n\t\tif memIncr, err = ctx.Get(dev, memsize); err != nil {\n\t\t\treturn errors.Wrapf(err, allocFail, memsize, dev)\n\t\t}\n\n\t\tif xdIncr, err = makeValueFromMem(z.t, zd.Shape(), memIncr); err != nil {\n\t\t\treturn errors.Wrapf(err, makeValueFail, z.t, zd.Shape())\n\t\t}\n\t\txdIncr.(tensor.Tensor).Memset(xdv.d.Data())\n\n\t\tmul.Prealloc = xd2\n\t\tmul.Incr = xdIncr\n\n\t\tdefer ctx.PutValue(dev, xd2) // xd2 is temporary, we need to dealloc it\n\t\tdefer ctx.Signal()           // work needs to be done\n\t}\n\n\tif d, err = mul.Do(yd, zd); err != nil {\n\t\treturn errors.Wrapf(err, \"IncrDo xd faile\")\n\t}\n\n\txdv.SetDeriv(d)\n\ndzdy:\n\tif y.isConstant() {\n\t\tgoto end\n\t}\n\n\tmul = NewHadamardProdOp(x, z, ctx)\n\tmul.Device = y.Device()\n\tdev = mul.Device\n\n\tif xd, extra, err = x.ValueOnDevice(dev, ctx.External); err != nil {\n\t\treturn errors.Wrapf(err, gradOnDeviceFail, x, dev)\n\t}\n\tif extra {\n\t\tdefer ctx.PutValue(dev, xd)\n\t}\n\n\tif yd, extra, err = y.GradOnDevice(dev, ctx.External); err != nil {\n\t\treturn errors.Wrapf(err, gradOnDeviceFail, y, dev)\n\t}\n\tif extra {\n\t\tdefer ctx.PutValue(dev, yd)\n\t}\n\n\tif zd, extra, err = z.GradOnDevice(dev, ctx.External); err != nil {\n\t\treturn errors.Wrapf(err, gradOnDeviceFail, z, dev)\n\t}\n\tif extra {\n\t\tdefer ctx.PutValue(dev, zd)\n\t}\n\n\tmul.Incr = yd\n\n\t// if y is Scalar, then it needs to be broadcasted across to the\n\tif y.IsScalar() && dev != CPU && !zd.Shape().IsScalar() {\n\t\tvar memIncr, mem2 tensor.Memory\n\t\tvar ydIncr, yd2 Value\n\t\tmemsize := calcMemSize(zd.Dtype(), zd.Shape())\n\t\tif mem2, err = ctx.Get(dev, memsize); err != nil {\n\t\t\treturn errors.Wrapf(err, allocFail, memsize, dev)\n\t\t}\n\n\t\tif yd2, err = makeValueFromMem(z.t, zd.Shape(), mem2); err != nil {\n\t\t\treturn errors.Wrapf(err, makeValueFail, z.t, zd.Shape())\n\t\t}\n\n\t\t// \"broadcast\" y (in a very sloppy way)\n\t\tif memIncr, err = ctx.Get(dev, memsize); err != nil {\n\t\t\treturn errors.Wrapf(err, allocFail, memsize, dev)\n\t\t}\n\n\t\tif ydIncr, err = makeValueFromMem(z.t, zd.Shape(), memIncr); err != nil {\n\t\t\treturn errors.Wrapf(err, makeValueFail, z.t, zd.Shape())\n\t\t}\n\t\tydIncr.(tensor.Tensor).Memset(ydv.d.Data())\n\n\t\tmul.Prealloc = yd2\n\t\tmul.Incr = ydIncr\n\n\t\tdefer ctx.PutValue(dev, yd2) // yd2 is temporary, we need to dealloc it\n\t\tdefer ctx.Signal()           // work needs to be done\n\t}\n\n\tif d, err = mul.Do(xd, zd); err != nil {\n\t\treturn errors.Wrapf(err, \"IncrDo yd failed\")\n\t}\n\tydv.SetDeriv(d)\n\nend:\n\treturn nil\n}\n\nfunc hadamardDivDiffExpr(x, y, z, gradZ *Node) (retVal Nodes, err error) {\n\tvar dzdx, dzdy *Node\n\tif dzdx, err = HadamardDiv(gradZ, y); err == nil {\n\t\tWithGroupName(gradClust)(dzdx)\n\t\tif dzdy, err = HadamardDiv(z, y); err == nil {\n\t\t\tWithGroupName(gradClust)(dzdy)\n\t\t\tif dzdy, err = Neg(dzdy); err == nil {\n\t\t\t\tWithGroupName(gradClust)(dzdy)\n\t\t\t\tif dzdy, err = HadamardProd(dzdy, gradZ); err == nil {\n\t\t\t\t\tWithGroupName(gradClust)(dzdy)\n\t\t\t\t\tretVal = Nodes{dzdx, dzdy}\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\treturn nil, errors.Wrap(err, \"Failed to carry HadamardProd()\")\n\t\t\t}\n\t\t\treturn nil, errors.Wrap(err, \"Failed to carry Neg()\")\n\t\t}\n\t\treturn nil, errors.Wrap(err, \"Failed to carry HadamardProd()\")\n\t}\n\treturn nil, errors.Wrap(err, \"Failed to carry HadamardProd()\")\n}\n\nfunc hadamardDivDiff(ctx ExecutionContext, x, y, z *Node) (err error) {\n\txdv, ydv, zdv := getDV3(x, y, z)\n\n\t// dzdx = 1/y * dz\n\tdiv := newEBOByType(divOpType, TypeOf(zdv.d), TypeOf(ydv.Value))\n\terr = div.IncrDo(xdv.d, zdv.d, ydv.Value)\n\tif err != nil {\n\t\tvar ver Valuer\n\t\tvar ok bool\n\t\tif ver, ok = err.(Valuer); !ok {\n\t\t\treturn\n\t\t}\n\n\t\txdv.SetDeriv(ver.Value()) // ignore errors on purpose\n\t}\n\n\t//dzdy = -x/y²\n\t// TODO: investigate if this can be done (if no other node uses z):\n\t//\t\tunsafe do : neg zdv.d\n\t// \t\tunsafe do : mul zdv.d, zdv.Value\n\t//\t\tincr do   : <incr: ydv.d> div zdv.d, ydv.Value\n\tvar d Value\n\tif d, err = div.Do(zdv.Value, ydv.Value); err != nil {\n\t\treturn errors.Wrapf(err, doFail, div)\n\t}\n\n\tneg := newElemUnaryOp(negOpType, y)\n\tif d, err = neg.Do(d); err != nil {\n\t\treturn errors.Wrapf(err, doFail, neg)\n\t}\n\n\tmul := newElemBinOp(mulOpType, z, y)\n\terr = mul.IncrDo(ydv.d, zdv.d, d)\n\tif err != nil {\n\t\tvar ver Valuer\n\t\tvar ok bool\n\t\tif ver, ok = err.(Valuer); !ok {\n\t\t\treturn\n\t\t}\n\n\t\tydv.SetDeriv(ver.Value()) // ignore errors on purpose\n\t}\n\n\treturn nil\n}\n\n// TODO: go back in time, pay more attention to calculus class in high school and learn how to differentiate x^y\nfunc hadamardPowDiffExpr(x, y, z, grad *Node) (retVal Nodes, err error) {\n\tvar one *Node\n\tvar dt tensor.Dtype\n\n\tif dt, err = dtypeOf(y.t); err != nil {\n\t\treturn nil, errors.Wrapf(err, dtypeExtractionFail, y.t)\n\t}\n\n\tswitch dt {\n\tcase Float32:\n\t\tone = onef32\n\tcase Float64:\n\t\tone = onef64\n\tdefault:\n\t\terr = errors.Errorf(nyiTypeFail, \"Hadamard Power Diff\", y.t)\n\t\treturn\n\t}\n\n\tvar ym1, pow *Node\n\tif ym1, err = Sub(y, one); err != nil {\n\t\treturn\n\t}\n\n\tif pow, err = Pow(x, ym1); err != nil {\n\t\treturn\n\t}\n\n\tvar dzdx *Node\n\tif dzdx, err = HadamardProd(grad, y); err != nil {\n\t\treturn\n\t}\n\tif dzdx, err = HadamardProd(dzdx, pow); err != nil {\n\t\treturn\n\t}\n\n\tvar logx *Node\n\tif logx, err = Log(x); err != nil {\n\t\treturn\n\t}\n\n\tvar dzdy *Node\n\tif dzdy, err = HadamardProd(grad, z); err != nil {\n\t\treturn\n\t}\n\tif dzdy, err = HadamardProd(dzdy, logx); err != nil {\n\t\treturn\n\t}\n\n\tretVal = Nodes{dzdx, dzdy}\n\treturn\n\t// return nil, errors.New(\"hadamardPowDiffExpr not yet implemented\")\n}\n\nfunc hadamardPowDiff(ctx ExecutionContext, x, y, z *Node) (err error) {\n\txdv, ydv, zdv := getDV3(x, y, z)\n\n\tvar ym1 Value\n\tswitch ydvt := ydv.Value.(type) {\n\tcase *F64:\n\t\tym1 = NewF64(ydvt.any() - float64(1))\n\tcase *F32:\n\t\tym1 = NewF32(ydvt.any() - float32(1))\n\tcase *tensor.Dense:\n\t\tvar one interface{}\n\t\tswitch ydvt.Dtype() {\n\t\tcase tensor.Float64:\n\t\t\tone = float64(1)\n\t\tcase tensor.Float32:\n\t\t\tone = float32(1)\n\t\t}\n\t\tif ym1, err = tensor.Sub(ydvt, one); err != nil {\n\t\t\treturn\n\t\t}\n\tdefault:\n\t\terr = errors.Errorf(nyiTypeFail, \"hadamardPowDiff\", ydv.Value)\n\t\treturn\n\t}\n\n\t// dzdx\n\tvar pow Value\n\tpowOp := newEBOByType(powOpType, TypeOf(xdv.Value), TypeOf(ym1))\n\tif pow, err = powOp.Do(xdv.Value, ym1); err != nil {\n\t\treturn\n\t}\n\n\tmul := newEBOByType(mulOpType, TypeOf(ydv.Value), TypeOf(xdv.Value))\n\tif pow, err = mul.UnsafeDo(pow, ydv.Value); err != nil {\n\t\treturn\n\t}\n\n\tif err = mul.IncrDo(xdv.d, pow, zdv.d); err != nil {\n\t\tvar ver Valuer\n\t\tvar ok bool\n\t\tif ver, ok = err.(Valuer); !ok {\n\t\t\treturn\n\t\t}\n\n\t\txdv.SetDeriv(ver.Value())\n\t}\n\n\t// dzdy\n\tvar logx Value\n\tlogOp := newElemUnaryOp(lnOpType, x)\n\tif logx, err = logOp.Do(xdv.Value); err != nil {\n\t\treturn\n\t}\n\tif logx, err = mul.Do(zdv.Value, logx); err != nil {\n\t\treturn\n\t}\n\tif err = mul.IncrDo(ydv.d, logx, zdv.d); err != nil {\n\t\tvar ver Valuer\n\t\tvar ok bool\n\t\tif ver, ok = err.(Valuer); !ok {\n\t\t\treturn\n\t\t}\n\n\t\tydv.SetDeriv(ver.Value())\n\t}\n\treturn nil\n}\n\nfunc nondiffBinOpExpr(x, y, z, grad *Node) (retVal Nodes, err error) {\n\treturn nil, errors.New(\"Nondifferentiable\")\n}\n\nfunc nondiffBinOp(ctx ExecutionContext, x, y, z *Node) (err error) {\n\treturn AutoDiffError{}\n}\n"
        },
        {
          "name": "operatorPointwise_binary_const.go",
          "type": "blob",
          "size": 3.423828125,
          "content": "package gorgonia\n\nimport \"gorgonia.org/tensor\"\n\nvar (\n\t/* scalar-tensor float64 and vice versa */\n\n\t// arith\n\ttadd = denseBinOp(tensor.Add)\n\ttsub = denseBinOp(tensor.Sub)\n\ttmul = denseBinOp(tensor.Mul)\n\ttdiv = denseBinOp(tensor.Div)\n\ttpow = denseBinOp(tensor.Pow)\n\n\t// cmp\n\ttlt  = denseCmpOp(tensor.Lt)\n\ttgt  = denseCmpOp(tensor.Gt)\n\ttlte = denseCmpOp(tensor.Lte)\n\ttgte = denseCmpOp(tensor.Gte)\n\tteq  = denseCmpOp(tensor.ElEq)\n\ttne  = denseCmpOp(tensor.ElNe)\n)\n\ntype denseBinOp func(a, b interface{}, opts ...tensor.FuncOpt) (tensor.Tensor, error)\ntype denseCmpOp func(a, b interface{}, opts ...tensor.FuncOpt) (tensor.Tensor, error)\n\ntype ʘBinaryOperatorType byte\n\nconst (\n\t// arith\n\taddOpType ʘBinaryOperatorType = iota\n\tsubOpType\n\tmulOpType\n\tdivOpType\n\tpowOpType\n\n\t// cmp\n\tltOpType\n\tgtOpType\n\tlteOpType\n\tgteOpType\n\teqOpType\n\tneOpType\n\n\tmaxʘBinaryOpType // delimits the end of all possible binOpType\n)\n\nfunc (op ʘBinaryOperatorType) String() string {\n\treturn ʘBinOpStrs[op]\n}\n\n// ʘBinOpStrs is the string representation for a binOpType\n// It should be held constant.\nvar ʘBinOpStrs = [maxʘBinaryOpType]string{\n\t// arith ops\n\t\"+\",\n\t\"-\",\n\t\"⊙\",\n\t\"÷\",\n\t\"^\",\n\n\t// cmp ops\n\t\"<\",\n\t\">\",\n\t\"<=\",\n\t\">=\",\n\t\"==\",\n\t\"!=\",\n}\n\n// ʘBinOpNames is the string representation for a binOpType\n// It should be held constant.\nvar ʘBinOpNames = [maxʘBinaryOpType]string{\n\t// arith ops\n\t\"add\",\n\t\"sub\",\n\t\"mul\",\n\t\"div\",\n\t\"pow\",\n\n\t// cmp ops\n\t\"lt\",\n\t\"gt\",\n\t\"lte\",\n\t\"gte\",\n\t\"eq\",\n\t\"ne\",\n}\n\n// ʘBinOpCommutative is the array that stores whether a binary operator is commutative\n// It should be held constant.\nvar ʘBinOpCommutative = [maxʘBinaryOpType]bool{\n\ttrue, false, true, false, false,\n\tfalse, false, false, false, true, true,\n}\n\nvar ʘBinOpDiffExprs = [maxʘBinaryOpType]func(x, y, z, gradZ *Node) (Nodes, error){\n\taddDiffExpr, subDiffExpr, hadamardProdDiffExpr, hadamardDivDiffExpr, hadamardPowDiffExpr,\n\tnondiffBinOpExpr, nondiffBinOpExpr, nondiffBinOpExpr, nondiffBinOpExpr, nondiffBinOpExpr, nondiffBinOpExpr,\n}\n\nvar ʘBinOpDiffFns = [maxʘBinaryOpType]func(ctx ExecutionContext, x, y, z *Node) error{\n\taddDiff, subDiff, hadamardProdDiff, hadamardDivDiff, hadamardPowDiff,\n\tnondiffBinOp, nondiffBinOp, nondiffBinOp, nondiffBinOp, nondiffBinOp, nondiffBinOp,\n}\n\n// isCommutative gives info about whether the operator is commutative\n// For example:\n//\t\ta + b == b + a\n// will ALWAYS evaluate to true. The same cannot be said about subtraction:\n// \t\ta - b != b - a\n// While a-b *may* be equal to b-a, it is not guaranteed. Therefore subtraction\n// is not commutative\nfunc (op ʘBinaryOperatorType) isCommutative() bool {\n\tif op >= maxʘBinaryOpType {\n\t\tpanic(\"isCommutative() for unsupported BinOp undefined\")\n\t}\n\treturn ʘBinOpCommutative[op]\n}\n\nfunc (op ʘBinaryOperatorType) diffWRT(inputs int) []bool {\n\tif inputs != 2 {\n\t\tpanic(\"binary operator only supports 2 inputs\")\n\t}\n\n\tif op.isArith() {\n\t\treturn []bool{true, true}\n\t}\n\treturn []bool{false, false}\n}\n\n// isArith indicates if the binary operator is an arithmetic type\nfunc (op ʘBinaryOperatorType) isArith() bool {\n\tswitch op {\n\tcase addOpType, subOpType, mulOpType, divOpType, powOpType:\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\nvar binOps = [maxʘBinaryOpType]*denseBinOp{\n\t&tadd,\n\t&tsub,\n\t&tmul,\n\t&tdiv,\n\t&tpow,\n\tnil, // lt\n\tnil, // gt\n\tnil, // lte\n\tnil, // gte\n\tnil, // eq\n\tnil, // ne\n}\n\nvar cmpOps = [maxʘBinaryOpType]*denseCmpOp{\n\tnil, // add\n\tnil, // sub\n\tnil, // mul\n\tnil, // div\n\tnil, // pow\n\t&tlt,\n\t&tgt,\n\t&tlte,\n\t&tgte,\n\t&teq,\n\t&tne,\n}\n"
        },
        {
          "name": "operatorPointwise_binary_test.go",
          "type": "blob",
          "size": 6.3896484375,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"math/rand\"\n\t\"runtime\"\n\t\"testing\"\n\n\t\"github.com/pkg/errors\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc ssBinOpTest(t *testing.T, op ʘBinaryOperatorType, dt tensor.Dtype) (err error) {\n\tdefer runtime.GC()\n\tassert := assert.New(t)\n\tvar randX, randY interface{}\n\tswitch dt {\n\tcase Float64:\n\t\trandX = rand.ExpFloat64()\n\t\trandY = rand.ExpFloat64()\n\tcase Float32:\n\t\trandX = float32(rand.ExpFloat64())\n\t\trandY = float32(rand.ExpFloat64())\n\tdefault:\n\t\treturn errors.Errorf(\"op %v Test not yet implemented for %v \", op, dt)\n\t}\n\n\tbinOp := newEBOByType(op, dt, dt)\n\tt.Logf(\"ssBinOp %v %v %v\", randX, op, randY)\n\n\tvar g, g2 *ExprGraph\n\tvar x, y, z *Node\n\tvar a, b, c *Node\n\tvar i, j, k *Node\n\tg = NewGraph()\n\tx = NewScalar(g, dt, WithName(\"x\"))\n\ty = NewScalar(g, dt, WithName(\"y\"))\n\tif z, err = ApplyOp(binOp, x, y); err != nil {\n\t\treturn err\n\t}\n\n\tg2 = NewGraph()\n\ta = NewScalar(g2, dt, WithName(\"a\"))\n\tb = NewScalar(g2, dt, WithName(\"b\"))\n\tif c, err = ApplyOp(binOp, a, b); err != nil {\n\t\treturn err\n\t}\n\n\ti = NewScalar(g, dt, WithName(\"i\"))\n\tj = NewScalar(g, dt, WithName(\"j\"))\n\tbinOp.retSame = true\n\tif k, err = ApplyOp(binOp, i, j); err != nil {\n\t\treturn err\n\t}\n\n\t// var grads Nodes\n\tvar m1 VM\n\tif op.isArith() {\n\t\tif _, err = Grad(c, a, b); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tm1 = NewLispMachine(g)\n\t} else {\n\t\tm1 = NewLispMachine(g, ExecuteFwdOnly())\n\t}\n\n\tm2 := NewTapeMachine(g2, TraceExec(), BindDualValues())\n\tdefer m2.Close()\n\tdefer m1.Close()\n\n\tLet(x, randX)\n\tLet(y, randY)\n\tLet(i, randX)\n\tLet(j, randY)\n\tif err = m1.RunAll(); err != nil {\n\t\treturn\n\t}\n\n\tLet(a, randX)\n\tLet(b, randY)\n\tif err = m2.RunAll(); err != nil {\n\t\treturn\n\t}\n\n\tvar xG, aG, yG, bG, zG, cG Value\n\tif op.isArith() {\n\t\tif xG, err = x.Grad(); err != nil {\n\t\t\treturn\n\t\t}\n\t\tif yG, err = y.Grad(); err != nil {\n\t\t\treturn\n\t\t}\n\t\tif aG, err = a.Grad(); err != nil {\n\t\t\treturn\n\t\t}\n\t\tif bG, err = b.Grad(); err != nil {\n\t\t\treturn\n\t\t}\n\n\t\tif zG, err = z.Grad(); err != nil {\n\t\t\treturn\n\t\t}\n\t\tif cG, err = c.Grad(); err != nil {\n\t\t\treturn\n\t\t}\n\n\t\tif _, err = i.Grad(); err != nil {\n\t\t\treturn\n\t\t}\n\n\t\tif _, err = j.Grad(); err != nil {\n\t\t\treturn\n\t\t}\n\t\tif _, err = k.Grad(); err != nil {\n\t\t\treturn\n\t\t}\n\n\t\tassert.True(ValueClose(xG, aG), \"Test ssDiff of %v. xG != aG. Got %v and %v\", op, xG, aG)\n\t\tassert.True(ValueClose(yG, bG), \"Test ssDiff of %v. yG != bG. Got %v and %v\", op, yG, bG)\n\t\tassert.True(ValueClose(zG, cG), \"Test ssDiff of %v. zG != cG. Got %v and %v\", op, zG, cG)\n\t}\n\n\tassert.True(ValueClose(x.Value(), a.Value()), \"Test ss op %v. Values are different: x: %v, a %v\", op, x.Value(), a.Value())\n\tassert.True(ValueClose(y.Value(), b.Value()), \"Test ss op %v. Values are different: y: %v, b %v\", op, y.Value(), b.Value())\n\tassert.True(ValueClose(z.Value(), c.Value()), \"Test ss op %v. Values are different: z: %v, c %v\", op, z.Value(), c.Value())\n\n\treturn nil\n}\n\nfunc ttBinOpTest(t *testing.T, op ʘBinaryOperatorType, dt tensor.Dtype) (err error) {\n\tdefer runtime.GC()\n\tassert := assert.New(t)\n\tvar x, y, z, a, b, c, cost *Node\n\tvar g, g2 *ExprGraph\n\n\tvar randX, randY interface{}\n\tswitch dt {\n\tcase Float32:\n\t\trandX = []float32{1, 2, 3, 4}\n\t\trandY = []float32{2, 2, 2, 2}\n\tcase Float64:\n\t\trandX = []float64{1, 2, 3, 4}\n\t\trandY = []float64{2, 2, 2, 2}\n\t}\n\n\tt.Logf(\"ttBinOp: %v %v %v\", randX, op, randY)\n\t// randX := Gaussian(0, 1)(dt, 2, 2)\n\t// randY := Gaussian(0, 1)(dt, 2, 2)\n\n\txV := tensor.New(tensor.WithShape(2, 2), tensor.WithBacking(randX))\n\tyV := tensor.New(tensor.WithShape(2, 2), tensor.WithBacking(randY))\n\n\tg = NewGraph()\n\tg2 = NewGraph()\n\tx = NewMatrix(g, dt, WithName(\"x\"), WithShape(2, 2))\n\ty = NewMatrix(g, dt, WithName(\"y\"), WithShape(2, 2))\n\ta = NewMatrix(g2, dt, WithName(\"a\"), WithShape(2, 2))\n\tb = NewMatrix(g2, dt, WithName(\"b\"), WithShape(2, 2))\n\n\tbinOp := newEBOByType(op, x.t, y.t)\n\tif z, err = ApplyOp(binOp, x, y); err != nil {\n\t\treturn err\n\t}\n\tif c, err = ApplyOp(binOp, a, b); err != nil {\n\t\treturn err\n\t}\n\n\tvar m1 VM\n\tif op.isArith() {\n\t\tif _, err = Sum(z); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif cost, err = Sum(c); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif _, err = Grad(cost, a, b); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tm1 = NewLispMachine(g)\n\t} else {\n\t\tm1 = NewLispMachine(g, ExecuteFwdOnly())\n\t}\n\n\t// lg := log.New(os.Stderr, \"\", 0)\n\tm2 := NewTapeMachine(g2, TraceExec())\n\tdefer m2.Close()\n\tdefer m1.Close()\n\n\t// m2 := NewTapeMachine(prog, locMap, TraceExec(), WithLogger(logger), WithWatchlist())\n\n\tLet(x, xV)\n\tLet(y, yV)\n\tif err = m1.RunAll(); err != nil {\n\t\treturn\n\t}\n\n\tLet(a, xV)\n\tLet(b, yV)\n\tif err = m2.RunAll(); err != nil {\n\t\treturn\n\t}\n\n\tvar xG, aG, yG, bG, zG, cG Value\n\tif op.isArith() {\n\t\tif xG, err = x.Grad(); err != nil {\n\t\t\treturn\n\t\t}\n\t\tif yG, err = y.Grad(); err != nil {\n\t\t\treturn\n\t\t}\n\t\tif aG, err = a.Grad(); err != nil {\n\t\t\treturn\n\t\t}\n\t\tif bG, err = b.Grad(); err != nil {\n\t\t\treturn\n\t\t}\n\n\t\tif zG, err = z.Grad(); err != nil {\n\t\t\treturn\n\t\t}\n\t\tif cG, err = c.Grad(); err != nil {\n\t\t\treturn\n\t\t}\n\t\tassert.True(ValueClose(xG, aG), \"Test ttDiff of %v. xG != aG. Got %+v \\nand %+v\", op, xG, aG)\n\t\tassert.True(ValueClose(yG, bG), \"Test ttDiff of %v. yG != bG. Got %+v \\nand %+v\", op, yG, bG)\n\t\tassert.True(ValueClose(zG, cG), \"Test ttDiff of %v. zG != cG. Got %+v \\nand %+v\", op, zG, cG)\n\t}\n\n\tassert.True(ValueClose(x.Value(), a.Value()), \"Test tt op %v. Values are different: x: %+v\\n a %+v\", op, x.Value(), a.Value())\n\tassert.True(ValueClose(y.Value(), b.Value()), \"Test tt op %v. Values are different: y: %+v\\n b %+v\", op, y.Value(), b.Value())\n\tassert.True(ValueClose(z.Value(), c.Value()), \"Test tt op %v. Values are different: z: %+v\\n c %+v\", op, z.Value(), c.Value())\n\n\tif t.Failed() {\n\t\tioutil.WriteFile(fmt.Sprintf(\"Test_%v_tt.dot\", op), []byte(g2.ToDot()), 0644)\n\t}\n\n\treturn nil\n}\n\nfunc TestBinOps(t *testing.T) {\n\tfor op := addOpType; op < maxʘBinaryOpType; op++ {\n\t\tt.Logf(\"OP: %v\", op)\n\n\t\t// if op != addOpType {\n\t\t// \tcontinue\n\t\t// }\n\n\t\t// for op := subOpType; op < mulOpType; op++ {\n\t\tvar err error\n\t\terr = ssBinOpTest(t, op, Float64)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Float64 version err: %v\", err)\n\t\t}\n\n\t\terr = ssBinOpTest(t, op, Float32)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Float32 version err: %v\", err)\n\t\t}\n\n\t\tt.Logf(\"Float64 T-T test for %v\", op)\n\t\terr = ttBinOpTest(t, op, Float64)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"ttBinOp Float64 version err %v\", err)\n\t\t}\n\n\t\tt.Logf(\"Float32 T-T test\")\n\t\terr = ttBinOpTest(t, op, Float32)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"ttBinOp Float64 version err %v\", err)\n\t\t}\n\t}\n\n\t// single tests\n\n\t// ttBinOpTest(t, subOpType, Float64)\n}\n"
        },
        {
          "name": "operatorPointwise_unary.go",
          "type": "blob",
          "size": 18.53515625,
          "content": "package gorgonia\n\nimport (\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\n// a ʘUnaryOperator is essentially a function that takes a float32 or float64 and returns the same\n// pros : no overloading = clear understanding\n// cons : no overloading = a lot of extra code\n//\n// There are TWO ʘUnaryOperator types so far:\n//\t\tsf32UnaryOperator - scalar float32 unary operator\n//\t\tsf64UnaryOperator - scalar float64 unary operator\n//\n// Because TensorTypes are parameterized by a scalar type, it isn't necessary to create operators\n// that will work on TensorTypes. A simple type switch will do.\n//\n// n.b.: ʘ is used to denote pointwiseness of the operator.\n// if you want to type it, it's U+0298 - Latin Letter Bilabial Click\ntype ʘUnaryOperator interface {\n\tunaryOpType() ʘUnaryOperatorType\n\tString() string\n}\n\ntype sf32UnaryOperator func(float32) float32\ntype sf64UnaryOperator func(float64) float64\n\n// unaryCheckApply checks in a interface is fulfilled. If it is, that engine is used instead\nfunc unaryCheckApply(op ʘUnaryOperator, t tensor.Tensor, opts ...tensor.FuncOpt) (retVal tensor.Tensor, err error) {\n\te := t.Engine()\n\tswitch op.unaryOpType() {\n\tcase absOpType:\n\t\tif oe, ok := e.(tensor.Abser); ok {\n\t\t\treturn oe.Abs(t, opts...)\n\t\t}\n\tcase signOpType:\n\t\tif oe, ok := e.(tensor.Signer); ok {\n\t\t\treturn oe.Sign(t, opts...)\n\t\t}\n\tcase ceilOpType:\n\tcase floorOpType:\n\tcase sinOpType:\n\tcase cosOpType:\n\tcase expOpType:\n\t\tif oe, ok := e.(tensor.Exper); ok {\n\t\t\treturn oe.Exp(t, opts...)\n\t\t}\n\tcase lnOpType:\n\t\tif oe, ok := e.(tensor.Loger); ok {\n\t\t\treturn oe.Log(t, opts...)\n\t\t}\n\tcase log2OpType:\n\t\tif oe, ok := e.(tensor.Log2er); ok {\n\t\t\treturn oe.Log2(t, opts...)\n\t\t}\n\tcase negOpType:\n\t\tif oe, ok := e.(tensor.Neger); ok {\n\t\t\treturn oe.Neg(t, opts...)\n\t\t}\n\tcase squareOpType:\n\t\tif oe, ok := e.(tensor.Squarer); ok {\n\t\t\treturn oe.Square(t, opts...)\n\t\t}\n\tcase sqrtOpType:\n\t\tif oe, ok := e.(tensor.Sqrter); ok {\n\t\t\treturn oe.Sqrt(t, opts...)\n\t\t}\n\tcase inverseOpType:\n\t\tif oe, ok := e.(tensor.Inver); ok {\n\t\t\treturn oe.Inv(t, opts...)\n\t\t}\n\tcase inverseSqrtOpType:\n\t\tif oe, ok := e.(tensor.InvSqrter); ok {\n\t\t\treturn oe.InvSqrt(t, opts...)\n\t\t}\n\tcase cubeOpType:\n\t\tif oe, ok := e.(tensor.Cuber); ok {\n\t\t\treturn oe.Cube(t, opts...)\n\t\t}\n\tcase tanhOpType:\n\t\tif oe, ok := e.(tensor.Tanher); ok {\n\t\t\treturn oe.Tanh(t, opts...)\n\t\t}\n\tcase sigmoidOpType:\n\tcase log1pOpType:\n\tcase expm1OpType:\n\tcase softplusOpType:\n\t}\n\n\t//default case:\n\tvar fn interface{}\n\tswitch opFn := op.(type) {\n\tcase *sf64UnaryOperator:\n\t\tfn = (func(float64) float64)(*opFn)\n\tcase *sf32UnaryOperator:\n\t\tfn = (func(float32) float32)(*opFn)\n\t}\n\n\treturn t.Apply(fn, opts...)\n}\n\n/*\nDIFFERENTIATION EXPRESSIONS\n\nAll the functions here are expressed in terms of *Node and/or Nodes\n\n*/\n\nfunc nondiffUnaryOpExpr(x, y, gradY *Node) (*Node, error) {\n\treturn nil, errors.Errorf(\"Nondifferentiable Function\")\n}\nfunc nondiffUnaryOp(x, y *Node) error {\n\treturn AutoDiffError{}\n}\n\n// apparently abs is differentiable\nfunc absDiffExpr(x, y, gradY *Node) (retVal *Node, err error) {\n\tif retVal, err = Sign(x); err != nil {\n\t\treturn nil, errors.Wrap(err, \"Failed to call Sign()\")\n\t}\n\tWithGroupName(gradClust)(retVal)\n\n\tif retVal, err = HadamardProd(gradY, retVal); err != nil {\n\t\treturn nil, errors.Wrap(err, hadamardProdFail)\n\t}\n\treturn\n}\n\nfunc absDiff(x, y *Node) (err error) {\n\txdv, ydv := getDV(x, y)\n\n\tvar d Value\n\tsign := newElemUnaryOp(signOpType, x)\n\tif d, err = sign.Do(xdv.Value); err == nil {\n\t\tif dT, ok := d.(tensor.Tensor); ok {\n\t\t\tdefer returnTensor(dT)\n\t\t}\n\n\t\tmul := newElemBinOp(mulOpType, y, x)\n\t\terr = mul.IncrDo(xdv.d, d, ydv.d)\n\t\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t\t}\n\t}\n\treturn\n}\n\n// Solution here\n// https://www.symbolab.com/solver/step-by-step/%5Cfrac%7Bd%7D%7Bdx%7D%5Cleft(sin%5Cleft(x%5Cright)%5Cright)\nfunc sinDiffExpr(x, y, gradY *Node) (retVal *Node, err error) {\n\tif retVal, err = Cos(x); err == nil {\n\t\tWithGroupName(gradClust)(retVal)\n\t\tretVal, err = HadamardProd(retVal, gradY)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, hadamardProdFail)\n\t\t}\n\t} else {\n\t\treturn nil, errors.Wrap(err, \"Failed to carry Cos()\")\n\t}\n\treturn\n}\n\nfunc sinDiff(x, y *Node) (err error) {\n\txdv, ydv := getDV(x, y)\n\n\tcos := newElemUnaryOp(cosOpType, x)\n\n\tvar d Value\n\tif d, err = cos.Do(xdv.Value); err == nil {\n\t\tif dT, ok := d.(tensor.Tensor); ok {\n\t\t\tdefer returnTensor(dT)\n\t\t}\n\n\t\tmul := newElemBinOp(mulOpType, x, y)\n\t\terr = mul.IncrDo(xdv.d, d, ydv.d)\n\t\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t\t}\n\t}\n\treturn\n}\n\n// Solution here (then apply chain rule to result by multiplying gradY):\n// https://www.symbolab.com/solver/step-by-step/%5Cfrac%7Bd%7D%7Bdx%7D%5Cleft(cos%5Cleft(x%5Cright)%5Cright)\nfunc cosDiffExpr(x, y, gradY *Node) (retVal *Node, err error) {\n\tif retVal, err = Sin(x); err == nil {\n\t\tWithGroupName(gradClust)(retVal)\n\t\tif retVal, err = Neg(retVal); err == nil {\n\t\t\tWithGroupName(gradClust)(retVal)\n\t\t\tretVal, err = HadamardProd(retVal, gradY)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, hadamardProdFail)\n\t\t\t}\n\t\t} else {\n\t\t\treturn nil, errors.Wrap(err, negFail)\n\t\t}\n\t} else {\n\t\treturn nil, errors.Wrap(err, \"Failed to call Sin()\")\n\t}\n\treturn\n}\n\nfunc cosDiff(x, y *Node) (err error) {\n\txdv, ydv := getDV(x, y)\n\n\tsin := newElemUnaryOp(sinOpType, x)\n\n\tvar d Value\n\tif d, err = sin.Do(xdv.Value); err == nil {\n\t\tif dT, ok := d.(tensor.Tensor); ok {\n\t\t\tdefer returnTensor(dT)\n\t\t}\n\n\t\tneg := newElemUnaryOp(negOpType, x)\n\t\tif d, err = neg.UnsafeDo(d); err == nil {\n\t\t\tmul := newElemBinOp(mulOpType, x, y)\n\t\t\terr = mul.IncrDo(xdv.d, d, ydv.d)\n\t\t\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\t\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t\t\t}\n\n\t\t}\n\t}\n\treturn\n}\n\nfunc expDiffExpr(x, y, gradY *Node) (retVal *Node, err error) {\n\treturn HadamardProd(y, gradY)\n}\n\nfunc expDiff(x, y *Node) (err error) {\n\txdv, ydv := getDV(x, y)\n\n\tmul := newElemBinOp(mulOpType, x, y)\n\terr = mul.IncrDo(xdv.d, ydv.Value, ydv.d)\n\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t}\n\treturn\n}\n\n// solution is 1/x.\n// Upon multiplying with gradY for chain rule, it simply becomes gradY/x\nfunc lnDiffExpr(x, y, gradY *Node) (retVal *Node, err error) {\n\treturn HadamardDiv(gradY, x)\n}\n\nfunc lnDiff(x, y *Node) (err error) {\n\txdv, ydv := getDV(x, y)\n\n\tdiv := newElemBinOp(divOpType, y, x)\n\n\terr = div.IncrDo(xdv.d, ydv.d, xdv.Value)\n\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t}\n\n\treturn\n}\n\n// 1/(x*ln(2))\nfunc log2DiffExpr(x, y, gradY *Node) (retVal *Node, err error) {\n\tvar log2 *Node\n\tif log2, err = getConst(x, \"log2\"); err != nil {\n\t\treturn nil, errors.Wrap(err, \"getConst failed\")\n\t}\n\n\tif retVal, err = HadamardDiv(x, log2); err != nil {\n\t\treturn nil, errors.Wrap(err, hadamardProdFail)\n\t}\n\tWithGroupName(gradClust)(retVal)\n\tif retVal, err = HadamardDiv(gradY, retVal); err != nil {\n\t\treturn nil, errors.Wrap(err, hadamardDivFail)\n\t}\n\treturn\n}\n\nfunc log2Diff(x, y *Node) (err error) {\n\txdv, ydv := getDV(x, y)\n\n\tvar log2 *Node\n\tif log2, err = getConst(x, \"log2\"); err != nil {\n\t\treturn errors.Wrap(err, \"getConst failed\")\n\t}\n\n\tmul := newElemBinOp(mulOpType, x, log2)\n\tvar d Value\n\tif d, err = mul.Do(xdv.Value, log2.boundTo); err != nil {\n\t\treturn errors.Wrapf(err, doFail, mul)\n\t}\n\n\tif dT, ok := d.(tensor.Tensor); ok {\n\t\tdefer returnTensor(dT)\n\t}\n\n\tdiv := newElemBinOp(divOpType, y, x)\n\terr = div.IncrDo(xdv.d, ydv.d, d)\n\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t}\n\n\treturn\n}\n\nfunc negDiffExpr(x, y, gradY *Node) (retVal *Node, err error) {\n\treturn Neg(gradY)\n}\n\nfunc negDiff(x, y *Node) (err error) {\n\txdv, ydv := getDV(x, y)\n\n\tsub := newElemBinOp(subOpType, x, y)\n\tvar d Value\n\td, err = sub.UnsafeDo(xdv.d, ydv.d)\n\n\t// first we check if what essentially is a noIncrError is called\n\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t}\n\n\t// then we set derivs, if d is a scalar\n\tif _, ok := xdv.Value.(Scalar); ok {\n\t\tif err = xdv.SetDeriv(d); err != nil {\n\t\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t\t}\n\t}\n\n\treturn\n}\n\nfunc squareDiffExpr(x, y, gradY *Node) (retVal *Node, err error) {\n\tvar two *Node\n\tif two, err = getConst(x, \"two\"); err != nil {\n\t\treturn nil, errors.Wrap(err, \"getConst failed\")\n\t}\n\n\t// symdiffLogf(\"X %v and TWO %v\", x.Shape(), two.Shape())\n\tif retVal, err = HadamardProd(x, two); err == nil {\n\t\tsymdiffLogf(\"Spawned: %d\", retVal.ID())\n\t\tWithGroupName(gradClust)(retVal)\n\t\tretVal, err = HadamardProd(retVal, gradY)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, hadamardProdFail)\n\t\t}\n\t\tsymdiffLogf(\"Spawned: %d\", retVal.ID())\n\t} else {\n\t\treturn nil, errors.Wrap(err, hadamardProdFail)\n\t}\n\treturn\n}\n\nfunc squareDiff(x, y *Node) (err error) {\n\txdv, ydv := getDV(x, y)\n\n\tvar two *Node\n\tif two, err = getConst(x, \"two\"); err != nil {\n\t\treturn errors.Wrap(err, \"getConst failed\")\n\t}\n\n\tvar d Value\n\tmul := newElemBinOp(mulOpType, x, y)\n\tif d, err = mul.Do(xdv.Value, two.boundTo); err == nil {\n\t\tif dT, ok := d.(tensor.Tensor); ok {\n\t\t\tdefer returnTensor(dT)\n\t\t}\n\n\t\terr = mul.IncrDo(xdv.d, d, ydv.d)\n\t\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t\t}\n\t}\n\treturn\n}\n\nfunc sqrtDiffExpr(x, y, gradY *Node) (retVal *Node, err error) {\n\tvar two *Node\n\tif two, err = getConst(x, \"two\"); err != nil {\n\t\treturn nil, errors.Wrap(err, \"getConst failed\")\n\t}\n\n\tif retVal, err = HadamardProd(two, y); err == nil {\n\t\tWithGroupName(gradClust)(retVal)\n\t\tretVal, err = HadamardDiv(gradY, retVal)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, hadamardDivFail)\n\t\t}\n\t} else {\n\t\treturn nil, errors.Wrap(err, hadamardProdFail)\n\t}\n\treturn\n}\n\nfunc sqrtDiff(x, y *Node) (err error) {\n\txdv, ydv := getDV(x, y)\n\n\tvar two *Node\n\tif two, err = getConst(x, \"two\"); err != nil {\n\t\treturn errors.Wrap(err, \"getConst failed\")\n\t}\n\n\tmul := newElemBinOp(mulOpType, x, y)\n\n\tvar d Value\n\tif d, err = mul.Do(ydv.Value, two.boundTo); err == nil {\n\t\tif dT, ok := d.(tensor.Tensor); ok {\n\t\t\tdefer returnTensor(dT)\n\t\t}\n\n\t\tdiv := newElemBinOp(divOpType, y, x)\n\t\terr = div.IncrDo(xdv.d, ydv.d, d)\n\t\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t\t}\n\t}\n\treturn\n}\n\nfunc inverseDiffExpr(x, y, gradY *Node) (retVal *Node, err error) {\n\tif retVal, err = HadamardProd(y, y); err == nil {\n\t\tWithGroupName(gradClust)(retVal)\n\t\tif retVal, err = Neg(retVal); err == nil {\n\t\t\tWithGroupName(gradClust)(retVal)\n\t\t\tretVal, err = HadamardProd(retVal, gradY)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, hadamardProdFail)\n\t\t\t}\n\t\t} else {\n\t\t\treturn nil, errors.Wrap(err, negFail)\n\t\t}\n\t} else {\n\t\treturn nil, errors.Wrap(err, hadamardProdFail)\n\t}\n\treturn\n}\n\nfunc inverseDiff(x, y *Node) (err error) {\n\txdv, ydv := getDV(x, y)\n\n\tsq := newElemUnaryOp(squareOpType, y)\n\n\tvar d Value\n\tif d, err = sq.Do(ydv.Value); err != nil {\n\t\treturn errors.Wrapf(err, doFail, sq)\n\t}\n\n\tneg := newElemUnaryOp(negOpType, y)\n\tif d, err = neg.Do(d); err != nil {\n\t\treturn errors.Wrapf(err, doFail, neg)\n\t}\n\tif dT, ok := d.(tensor.Tensor); ok {\n\t\tdefer returnTensor(dT)\n\t}\n\n\tmul := newElemBinOp(mulOpType, y, y)\n\terr = mul.IncrDo(xdv.d, d, ydv.d)\n\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t}\n\treturn\n}\n\nfunc inverseSqrtDiffExpr(x, y, gradY *Node) (retVal *Node, err error) {\n\tvar two *Node\n\tif two, err = getConst(x, \"two\"); err != nil {\n\t\treturn nil, errors.Wrap(err, \"getConst failed\")\n\t}\n\tif retVal, err = Cube(y); err != nil {\n\t\treturn nil, errors.Wrapf(err, cubeFail)\n\t}\n\tif retVal, err = HadamardProd(two, retVal); err != nil {\n\t\treturn nil, errors.Wrapf(err, hadamardProdFail)\n\t}\n\tif retVal, err = HadamardDiv(gradY, retVal); err != nil {\n\t\treturn nil, errors.Wrapf(err, hadamardDivFail)\n\t}\n\treturn Neg(retVal)\n}\n\nfunc inverseSqrtDiff(x, y *Node) (err error) {\n\txdv, ydv := getDV(x, y)\n\tvar two *Node\n\tif two, err = getConst(x, \"two\"); err != nil {\n\t\treturn errors.Wrap(err, \"getConst failed\")\n\t}\n\n\tcb := newElemUnaryOp(cubeOpType, y)\n\tvar d Value\n\tif d, err = cb.Do(ydv.Value); err != nil {\n\t\treturn errors.Wrapf(err, doFail, cb)\n\t}\n\n\tmul := newElemBinOp(mulOpType, x, y)\n\tif d, err = mul.Do(two.boundTo, d); err != nil {\n\t\treturn errors.Wrapf(err, doFail, mul)\n\t}\n\n\tdiv := newElemBinOp(divOpType, y, x)\n\tif d, err = div.Do(ydv.d, d); err != nil {\n\t\treturn errors.Wrapf(err, doFail, div)\n\t}\n\n\tsub := newElemBinOp(subOpType, x, y)\n\tif _, err = sub.Do(xdv.d, d); err != nil {\n\t\treturn errors.Wrapf(err, doFail, sub)\n\t}\n\treturn nil\n}\n\nfunc cubeDiffExpr(x, y, gradY *Node) (retVal *Node, err error) {\n\tvar three *Node\n\tif three, err = getConst(x, \"three\"); err != nil {\n\t\treturn nil, errors.Wrap(err, \"getConst failed\")\n\t}\n\n\tif retVal, err = HadamardProd(x, x); err == nil {\n\t\tWithGroupName(gradClust)(retVal)\n\t\tif retVal, err = HadamardProd(retVal, three); err == nil {\n\t\t\tWithGroupName(gradClust)(retVal)\n\t\t\tretVal, err = HadamardProd(retVal, gradY)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, hadamardProdFail)\n\t\t\t}\n\t\t} else {\n\t\t\treturn nil, errors.Wrap(err, hadamardProdFail)\n\t\t}\n\t} else {\n\t\treturn nil, errors.Wrap(err, hadamardProdFail)\n\t}\n\treturn\n}\n\nfunc cubeDiff(x, y *Node) (err error) {\n\txdv, ydv := getDV(x, y)\n\n\tvar three *Node\n\tif three, err = getConst(x, \"three\"); err != nil {\n\t\treturn errors.Wrap(err, \"getConst failed\")\n\t}\n\n\tmul := newElemBinOp(mulOpType, x, y)\n\tvar d Value\n\tif d, err = mul.Do(xdv.Value, xdv.Value); err != nil {\n\t\treturn errors.Wrapf(err, doFail, mul)\n\t}\n\n\tif dT, ok := d.(tensor.Tensor); ok {\n\t\tdefer returnTensor(dT)\n\t}\n\n\tif d, err = mul.UnsafeDo(d, three.boundTo); err != nil {\n\t\treturn errors.Wrapf(err, unsafeDoFail, mul)\n\t}\n\n\terr = mul.IncrDo(xdv.d, d, ydv.d)\n\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t}\n\treturn\n}\n\nfunc tanhDiffExpr(x, y, gradY *Node) (retVal *Node, err error) {\n\tvar one *Node\n\tif one, err = getConst(x, \"one\"); err != nil {\n\t\treturn nil, errors.Wrap(err, \"getConst failed\")\n\t}\n\n\tif retVal, err = HadamardProd(y, y); err == nil {\n\t\tWithGroupName(gradClust)(retVal)\n\t\tif retVal, err = Sub(one, retVal); err == nil {\n\t\t\tWithGroupName(gradClust)(retVal)\n\t\t\tretVal, err = HadamardProd(retVal, gradY)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, hadamardProdFail)\n\t\t\t}\n\t\t} else {\n\t\t\treturn nil, errors.Wrap(err, subFail)\n\t\t}\n\t} else {\n\t\treturn nil, errors.Wrap(err, hadamardProdFail)\n\t}\n\treturn\n}\n\nfunc tanhDiff(x, y *Node) (err error) {\n\txdv, ydv := getDV(x, y)\n\n\tvar one *Node\n\tif one, err = getConst(x, \"one\"); err != nil {\n\t\treturn errors.Wrap(err, \"getConst failed\")\n\t}\n\n\tsq := newElemUnaryOp(squareOpType, y)\n\n\tvar d Value\n\tif d, err = sq.Do(ydv.Value); err != nil {\n\t\treturn errors.Wrapf(err, doFail, sq)\n\t}\n\n\tif dT, ok := d.(tensor.Tensor); ok {\n\t\tdefer returnTensor(dT)\n\t}\n\n\tsub := newElemBinOp(subOpType, one, y)\n\tif d, err = sub.UnsafeDo(one.boundTo, d); err != nil {\n\t\treturn errors.Wrapf(err, unsafeDoFail, sub)\n\t}\n\n\tmul := newElemBinOp(mulOpType, x, y)\n\terr = mul.IncrDo(xdv.d, d, ydv.d)\n\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t}\n\treturn\n}\n\nfunc sigmoidDiffExpr(x, y, gradY *Node) (retVal *Node, err error) {\n\tvar one *Node\n\tif one, err = getConst(x, \"one\"); err != nil {\n\t\treturn nil, errors.Wrap(err, \"getConst failed\")\n\t}\n\n\tif retVal, err = Sub(one, y); err == nil {\n\t\tWithGroupName(gradClust)(retVal)\n\t\tif retVal, err = HadamardProd(y, retVal); err == nil {\n\t\t\tWithGroupName(gradClust)(retVal)\n\t\t\tretVal, err = HadamardProd(retVal, gradY)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, hadamardProdFail)\n\t\t\t}\n\t\t} else {\n\t\t\treturn nil, errors.Wrap(err, hadamardProdFail)\n\t\t}\n\t} else {\n\t\treturn nil, errors.Wrap(err, subFail)\n\t}\n\treturn\n}\n\nfunc sigmoidDiff(x, y *Node) (err error) {\n\txdv, ydv := getDV(x, y)\n\n\tvar one *Node\n\tif one, err = getConst(x, \"one\"); err != nil {\n\t\treturn errors.Wrap(err, \"getConst failed\")\n\t}\n\n\tsub := newElemBinOp(subOpType, one, y)\n\n\tvar d Value\n\tif d, err = sub.Do(one.boundTo, ydv.Value); err != nil {\n\t\treturn errors.Wrapf(err, doFail, sub)\n\t}\n\n\tif dT, ok := d.(tensor.Tensor); ok {\n\t\tdefer returnTensor(dT)\n\t}\n\n\tmul := newElemBinOp(mulOpType, x, y)\n\tif d, err = mul.UnsafeDo(d, ydv.Value); err != nil {\n\t\treturn errors.Wrapf(err, unsafeDoFail, mul)\n\t}\n\n\terr = mul.IncrDo(xdv.d, d, ydv.d)\n\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t}\n\treturn\n}\n\n// 1/(x+1)\nfunc log1pDiffExpr(x, y, gradY *Node) (retVal *Node, err error) {\n\tvar one *Node\n\tif one, err = getConst(x, \"one\"); err != nil {\n\t\treturn nil, errors.Wrap(err, \"getConst failed\")\n\t}\n\n\tif retVal, err = Add(x, one); err == nil {\n\t\tWithGroupName(gradClust)(retVal)\n\t\tretVal, err = HadamardDiv(gradY, retVal)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, hadamardProdFail)\n\t\t}\n\t} else {\n\t\treturn nil, errors.Wrap(err, \"Failed to carry Add()\")\n\t}\n\treturn\n}\n\nfunc log1pDiff(x, y *Node) (err error) {\n\txdv, ydv := getDV(x, y)\n\n\tvar one *Node\n\tif one, err = getConst(x, \"one\"); err != nil {\n\t\treturn errors.Wrap(err, \"getConst failed\")\n\t}\n\n\tadd := newElemBinOp(addOpType, x, one)\n\n\tvar d Value\n\tif d, err = add.Do(xdv.Value, one.boundTo); err != nil {\n\t\treturn errors.Wrapf(err, doFail, add)\n\t}\n\n\tif dT, ok := d.(tensor.Tensor); ok {\n\t\tdefer returnTensor(dT)\n\t}\n\n\tdiv := newElemBinOp(divOpType, y, x)\n\terr = div.IncrDo(xdv.d, ydv.d, d)\n\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t}\n\treturn\n}\n\nfunc expm1DiffExpr(x, y, gradY *Node) (retVal *Node, err error) {\n\tif retVal, err = Exp(x); err == nil {\n\t\tWithGroupName(gradClust)(retVal)\n\t\treturn HadamardProd(gradY, retVal)\n\t}\n\treturn nil, errors.Wrap(err, \"Failled to carry Exp()\")\n}\n\nfunc expm1Diff(x, y *Node) (err error) {\n\txdv, ydv := getDV(x, y)\n\n\texp := newElemUnaryOp(expOpType, x)\n\n\tvar d Value\n\tif d, err = exp.Do(xdv.Value); err != nil {\n\t\treturn errors.Wrapf(err, doFail, exp)\n\t}\n\n\tif dT, ok := d.(tensor.Tensor); ok {\n\t\tdefer returnTensor(dT)\n\t}\n\n\tmul := newElemBinOp(mulOpType, x, y)\n\terr = mul.IncrDo(xdv.d, d, ydv.d)\n\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t}\n\treturn\n}\n\nfunc softplusDiffExpr(x, y, gradY *Node) (retVal *Node, err error) {\n\tif retVal, err = Sigmoid(x); err == nil {\n\t\tWithGroupName(gradClust)(retVal)\n\t\treturn HadamardProd(retVal, gradY)\n\t}\n\treturn nil, errors.Wrap(err, \"Failed to carry Sigmoid()\")\n}\n\nfunc softplusDiff(x, y *Node) (err error) {\n\txdv, ydv := getDV(x, y)\n\n\tsigmoid := newElemUnaryOp(sigmoidOpType, x)\n\n\tvar d Value\n\tif d, err = sigmoid.Do(xdv.Value); err != nil {\n\t\treturn errors.Wrapf(err, doFail, sigmoid)\n\t}\n\n\tif dT, ok := d.(tensor.Tensor); ok {\n\t\tdefer returnTensor(dT)\n\t}\n\n\tmul := newElemBinOp(mulOpType, x, y)\n\terr = mul.IncrDo(xdv.d, d, ydv.d)\n\tif err = checkErrSetDeriv(err, xdv); err != nil {\n\t\treturn errors.Wrapf(err, autodiffFail, x)\n\t}\n\treturn\n}\n"
        },
        {
          "name": "operatorPointwise_unary_const.go",
          "type": "blob",
          "size": 4.8837890625,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\n\t\"github.com/chewxy/math32\"\n)\n\nvar (\n\t/* float64 */\n\n\t// non differentiable\n\tabsf64   = sf64UnaryOperator(math.Abs)\n\tsignf64  = sf64UnaryOperator(_signf64)\n\tceilf64  = sf64UnaryOperator(math.Ceil)\n\tfloorf64 = sf64UnaryOperator(math.Floor)\n\n\t// differentiable\n\tsinf64         = sf64UnaryOperator(math.Sin)\n\tcosf64         = sf64UnaryOperator(math.Cos)\n\texpf64         = sf64UnaryOperator(math.Exp)\n\tlnf64          = sf64UnaryOperator(math.Log)\n\tlog2f64        = sf64UnaryOperator(math.Log2)\n\tnegf64         = sf64UnaryOperator(_negf64)\n\tsquaref64      = sf64UnaryOperator(_squaref64)\n\tsqrtf64        = sf64UnaryOperator(math.Sqrt)\n\tinversef64     = sf64UnaryOperator(_inversef64)\n\tinverseSqrtf64 = sf64UnaryOperator(_inverseSqrtf64)\n\n\t// activation functions\n\tcubef64    = sf64UnaryOperator(_cubef64)\n\ttanhf64    = sf64UnaryOperator(_tanhf64)\n\tsigmoidf64 = sf64UnaryOperator(_sigmoidf64)\n\n\t// numerical stabilization optimization\n\tlog1pf64    = sf64UnaryOperator(math.Log1p)\n\texpm1f64    = sf64UnaryOperator(math.Expm1)\n\tsoftplusf64 = sf64UnaryOperator(_softplusf64)\n\t// softplus isn't necessarily only a numerical stabilization op\n\t// (you can use it elsewhere), but I included it under numerical optimization\n\n\t/* Float32 */\n\n\t// non differentiable\n\tabsf32   = sf32UnaryOperator(math32.Abs)\n\tsignf32  = sf32UnaryOperator(_signf32)\n\tceilf32  = sf32UnaryOperator(math32.Ceil)\n\tfloorf32 = sf32UnaryOperator(math32.Floor)\n\n\t// start differentiable\n\tsinf32         = sf32UnaryOperator(math32.Sin)\n\tcosf32         = sf32UnaryOperator(math32.Cos)\n\texpf32         = sf32UnaryOperator(math32.Exp)\n\tlnf32          = sf32UnaryOperator(math32.Log)\n\tlog2f32        = sf32UnaryOperator(math32.Log2)\n\tnegf32         = sf32UnaryOperator(_negf32)\n\tsquaref32      = sf32UnaryOperator(_squaref32)\n\tsqrtf32        = sf32UnaryOperator(math32.Sqrt)\n\tinversef32     = sf32UnaryOperator(_inversef32)\n\tinverseSqrtf32 = sf32UnaryOperator(_inverseSqrtf32)\n\n\t// typically used in activation functions\n\tcubef32    = sf32UnaryOperator(_cubef32)\n\ttanhf32    = sf32UnaryOperator(_tanhf32)\n\tsigmoidf32 = sf32UnaryOperator(_sigmoidf32)\n\n\t// numerical stabilization optimization\n\tlog1pf32    = sf32UnaryOperator(math32.Log1p)\n\texpm1f32    = sf32UnaryOperator(math32.Expm1)\n\tsoftplusf32 = sf32UnaryOperator(_softplusf32)\n)\n\ntype ʘUnaryOperatorType byte\n\nconst (\n\tabsOpType ʘUnaryOperatorType = iota\n\tsignOpType\n\tceilOpType\n\tfloorOpType\n\n\t// start differentiable\n\tsinOpType\n\tcosOpType\n\texpOpType\n\tlnOpType\n\tlog2OpType\n\tnegOpType\n\tsquareOpType\n\tsqrtOpType\n\tinverseOpType     // multiplicative inverse\n\tinverseSqrtOpType // 1/sqrt(x)\n\n\t// typically used in activation functions\n\tcubeOpType\n\ttanhOpType\n\tsigmoidOpType\n\n\t// optimization related\n\tlog1pOpType\n\texpm1OpType\n\tsoftplusOpType\n\n\tmaxʘUnaryOperator // delimits end of all possible unary ops\n)\n\nfunc (u ʘUnaryOperatorType) String() string {\n\tif u >= maxʘUnaryOperator {\n\t\treturn fmt.Sprintf(\"UNSUPPORTED UNARY OPERATOR (%d); max: %d\", u, maxʘUnaryOperator)\n\t}\n\n\treturn ʘUnaryOpStrs[u]\n}\n\n// ʘUnaryOpStrs is the string representation for a unaryOpType\n// It should be held constant.\nvar ʘUnaryOpStrs = [maxʘUnaryOperator]string{\n\t\"abs\", \"sign\", \"ceil\", \"floor\",\n\t\"sin\", \"cos\", \"exp\",\n\t\"ln\", \"log2\", \"neg\", \"square\", \"sqrt\",\n\t\"inv\", \"invSqrt\",\n\t\"cube\", \"tanh\", \"sigmoid\",\n\n\t\"log1p\", \"expm1\", \"softplus\",\n}\n\n// ʘUnaryOpDifferentiable is the array of whether a unary operator is differentiable\n// It should be held constant\nvar ʘUnaryOpDifferentiable = [maxʘUnaryOperator]bool{\n\ttrue, false, false, false,\n\ttrue, true, true,\n\ttrue, true, true, true, true,\n\ttrue, true,\n\ttrue, true, true,\n\n\ttrue, true, true,\n}\n\nvar ʘUnaryOpDiffExprs = [maxʘUnaryOperator]func(x, y, gradY *Node) (*Node, error){\n\tabsDiffExpr, nondiffUnaryOpExpr, nondiffUnaryOpExpr, nondiffUnaryOpExpr,\n\tsinDiffExpr, cosDiffExpr, expDiffExpr,\n\tlnDiffExpr, log2DiffExpr, negDiffExpr, squareDiffExpr, sqrtDiffExpr,\n\tinverseDiffExpr, inverseSqrtDiffExpr, cubeDiffExpr, tanhDiffExpr, sigmoidDiffExpr,\n\n\tlog1pDiffExpr, expm1DiffExpr, softplusDiffExpr,\n}\n\nvar ʘUnaryOpDiffFns = [maxʘUnaryOperator]func(x, y *Node) error{\n\tabsDiff, nondiffUnaryOp, nondiffUnaryOp, nondiffUnaryOp,\n\tsinDiff, cosDiff, expDiff,\n\tlnDiff, log2Diff, negDiff, squareDiff, sqrtDiff,\n\tinverseDiff, inverseSqrtDiff, cubeDiff, tanhDiff, sigmoidDiff,\n\n\tlog1pDiff, expm1Diff, softplusDiff,\n}\n\nvar sf64UnaryOperators = [maxʘUnaryOperator]*sf64UnaryOperator{\n\t&absf64,\n\t&signf64,\n\t&ceilf64,\n\t&floorf64,\n\t&sinf64,\n\t&cosf64,\n\t&expf64,\n\t&lnf64,\n\t&log2f64,\n\t&negf64,\n\t&squaref64,\n\t&sqrtf64,\n\t&inversef64,\n\t&inverseSqrtf64,\n\t&cubef64,\n\t&tanhf64,\n\t&sigmoidf64,\n\n\t&log1pf64,\n\t&expm1f64,\n\t&softplusf64,\n}\n\nvar sf32UnaryOperators = [maxʘUnaryOperator]*sf32UnaryOperator{\n\t&absf32,\n\t&signf32,\n\t&ceilf32,\n\t&floorf32,\n\t&sinf32,\n\t&cosf32,\n\t&expf32,\n\t&lnf32,\n\t&log2f32,\n\t&negf32,\n\t&squaref32,\n\t&sqrtf32,\n\t&inversef32,\n\t&inverseSqrtf32,\n\t&cubef32,\n\t&tanhf32,\n\t&sigmoidf32,\n\n\t&log1pf32,\n\t&expm1f32,\n\t&softplusf32,\n}\n"
        },
        {
          "name": "operatorPointwise_unary_gen.go",
          "type": "blob",
          "size": 1.9384765625,
          "content": "package gorgonia\n\n// Code generated by genapi, which is a API generation tool for Gorgonia. DO NOT EDIT.\n\nfunc (f *sf32UnaryOperator) unaryOpType() ʘUnaryOperatorType {\n\n\tswitch f {\n\tcase &absf32:\n\t\treturn absOpType\n\tcase &signf32:\n\t\treturn signOpType\n\tcase &ceilf32:\n\t\treturn ceilOpType\n\tcase &floorf32:\n\t\treturn floorOpType\n\tcase &sinf32:\n\t\treturn sinOpType\n\tcase &cosf32:\n\t\treturn cosOpType\n\tcase &expf32:\n\t\treturn expOpType\n\tcase &lnf32:\n\t\treturn lnOpType\n\tcase &log2f32:\n\t\treturn log2OpType\n\tcase &negf32:\n\t\treturn negOpType\n\tcase &squaref32:\n\t\treturn squareOpType\n\tcase &sqrtf32:\n\t\treturn sqrtOpType\n\tcase &inversef32:\n\t\treturn inverseOpType\n\tcase &inverseSqrtf32:\n\t\treturn inverseSqrtOpType\n\tcase &cubef32:\n\t\treturn cubeOpType\n\tcase &tanhf32:\n\t\treturn tanhOpType\n\tcase &sigmoidf32:\n\t\treturn sigmoidOpType\n\tcase &log1pf32:\n\t\treturn log1pOpType\n\tcase &expm1f32:\n\t\treturn expm1OpType\n\tcase &softplusf32:\n\t\treturn softplusOpType\n\t}\n\treturn maxʘUnaryOperator\n}\nfunc (f *sf32UnaryOperator) String() string { return f.unaryOpType().String() }\n\nfunc (f *sf64UnaryOperator) unaryOpType() ʘUnaryOperatorType {\n\n\tswitch f {\n\tcase &absf64:\n\t\treturn absOpType\n\tcase &signf64:\n\t\treturn signOpType\n\tcase &ceilf64:\n\t\treturn ceilOpType\n\tcase &floorf64:\n\t\treturn floorOpType\n\tcase &sinf64:\n\t\treturn sinOpType\n\tcase &cosf64:\n\t\treturn cosOpType\n\tcase &expf64:\n\t\treturn expOpType\n\tcase &lnf64:\n\t\treturn lnOpType\n\tcase &log2f64:\n\t\treturn log2OpType\n\tcase &negf64:\n\t\treturn negOpType\n\tcase &squaref64:\n\t\treturn squareOpType\n\tcase &sqrtf64:\n\t\treturn sqrtOpType\n\tcase &inversef64:\n\t\treturn inverseOpType\n\tcase &inverseSqrtf64:\n\t\treturn inverseSqrtOpType\n\tcase &cubef64:\n\t\treturn cubeOpType\n\tcase &tanhf64:\n\t\treturn tanhOpType\n\tcase &sigmoidf64:\n\t\treturn sigmoidOpType\n\tcase &log1pf64:\n\t\treturn log1pOpType\n\tcase &expm1f64:\n\t\treturn expm1OpType\n\tcase &softplusf64:\n\t\treturn softplusOpType\n\t}\n\treturn maxʘUnaryOperator\n}\nfunc (f *sf64UnaryOperator) String() string { return f.unaryOpType().String() }\n"
        },
        {
          "name": "operatorPointwise_unary_test.go",
          "type": "blob",
          "size": 14.3720703125,
          "content": "package gorgonia\n\nimport (\n\t\"math\"\n\t\"math/rand\"\n\t\"testing\"\n\n\t\"github.com/chewxy/math32\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"gorgonia.org/dawson\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc unaryOpTest(t *testing.T, dt tensor.Dtype, shape tensor.Shape, fn func(*Node) (*Node, error)) (x, y, a, b *Node, v Value, err error) {\n\tvar xV, aV Value\n\tvar any interface{}\n\tif shape.IsScalar() {\n\t\tif dt == tensor.Float64 {\n\t\t\tany = rand.ExpFloat64()\n\t\t} else {\n\t\t\tany = float32(rand.ExpFloat64())\n\t\t}\n\t} else {\n\t\tany = tensor.New(tensor.WithBacking(tensor.Random(dt, shape.TotalSize())))\n\t}\n\tif v, _, _, err = anyToValue(any); err != nil {\n\t\tt.Errorf(\"anyToValue failed %v\", err)\n\t\treturn\n\t}\n\tif xV, err = CloneValue(v); err != nil {\n\t\tt.Errorf(\"Clone to xV failed %v\", err)\n\t\treturn\n\t}\n\n\tg := NewGraph()\n\tx = NodeFromAny(g, xV, WithName(\"x\"))\n\ty = Must(fn(x))\n\tMust(Sum(y))\n\n\tvar grads Nodes\n\th := NewGraph()\n\ta = NodeFromAny(h, xV, WithName(\"x\"))\n\tb = Must(fn(a))\n\tcost := Must(Sum(b))\n\tif grads, err = Grad(cost, a); err != nil {\n\t\tt.Errorf(\"Unable to get gradient %v\", err)\n\t\treturn\n\t}\n\n\tif aV, err = CloneValue(v); err != nil {\n\t\tt.Errorf(\"Clone to aV failed: %v\", err)\n\t\treturn\n\t}\n\n\tm0 := NewLispMachine(g)\n\tm1 := NewTapeMachine(h)\n\tdefer m1.Close()\n\tdefer m0.Close()\n\n\tLet(x, xV)\n\tif err = m0.RunAll(); err != nil {\n\t\tt.Errorf(\"m0 failed: %v\", err)\n\t\treturn\n\t}\n\n\tLet(a, aV)\n\tif err = m1.RunAll(); err != nil {\n\t\tt.Errorf(\"m1 failed: %v\", err)\n\t\treturn\n\t}\n\n\tvar yV, xG, bV, aG Value\n\tyV = y.Value()\n\tif xG, err = x.Grad(); err != nil {\n\t\tt.Errorf(\"x has no grad: %v\", err)\n\t\treturn\n\t}\n\n\tbV = b.Value()\n\tif aG, err = a.Grad(); err != nil {\n\t\tt.Errorf(\"a has no grad: %v\", err)\n\t\tt.Logf(\"a.deriv %p | %p\", a.deriv, grads[0])\n\t\treturn\n\t}\n\n\tif !ValueClose(yV, bV) {\n\t\tt.Errorf(\"Expected yV and bV to be close. yV: %v, bV: %v\", yV, bV)\n\t}\n\n\tif !ValueClose(aG, xG) {\n\t\tt.Errorf(\"Expected aG and xG to be close. aG: %v, xG %v\", aG, xG)\n\t}\n\n\treturn\n}\n\nfunc unaryOpDiffTest(op ʘUnaryOperatorType) (xRandVal float64, x, y, xT, yT *Node, err error) {\n\t_, x, y = simpleUnaryEqn()\n\n\txRandVal = rand.ExpFloat64()\n\tfn := *(sf64UnaryOperators[op])\n\tdiff := ʘUnaryOpDiffFns[op]\n\n\t// let the first stone be cast!\n\tLet(x, xRandVal)\n\tv, _, _, _ := anyToValue(fn(xRandVal)) // as if the graph has been executed upon\n\tydv := variableDV(v)\n\n\tif err = y.bind(ydv); err != nil {\n\t\treturn\n\t}\n\n\tif err = x.bind(dvUnit(x.boundTo)); err != nil {\n\t\treturn\n\t}\n\n\tif err = diff(x, y); err != nil {\n\t\treturn\n\t}\n\n\t// Tensor edition\n\t_, xT, yT = simpleUnaryVecEqn()\n\n\txBack := []float64{-xRandVal, xRandVal}\n\tyBack := []float64{fn(-xRandVal), fn(xRandVal)}\n\tLet(xT, tensor.New(tensor.WithShape(2, 1), tensor.WithBacking(xBack)))\n\tvT, _, _, _ := anyToValue(tensor.New(tensor.WithShape(2, 1), tensor.WithBacking(yBack)))\n\tyTdv := variableDV(vT)\n\n\tif err = yT.bind(yTdv); err != nil {\n\t\treturn\n\t}\n\n\tif err = xT.bind(dvUnit(xT.boundTo)); err != nil {\n\t\treturn\n\t}\n\n\tif err = diff(xT, yT); err != nil {\n\t\treturn\n\t}\n\treturn\n}\n\nfunc TestAbs(t *testing.T) {\n\tassert := assert.New(t)\n\n\tvar x, y, a, b *Node\n\tvar v Value\n\tvar yV, xG, bV, aG Value\n\tvar err error\n\n\t/* FLOAT 64 Scalar */\n\n\tx, y, a, b, v, err = unaryOpTest(t, Float64, tensor.Shape{}, Abs)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tyV = y.Value()\n\tif xG, err = x.Grad(); err != nil {\n\t\tt.Errorf(\"x has no grad: %v\", err)\n\t\treturn\n\t}\n\n\tbV = b.Value()\n\tif aG, err = a.Grad(); err != nil {\n\t\tt.Errorf(\"a has no grad: %v\", err)\n\t}\n\n\tcorrectF64 := math.Abs(v.Data().(float64))\n\tassert.True(ValueClose(NewF64(correctF64), yV))\n\tassert.True(ValueClose(NewF64(correctF64), bV))\n\tassert.True(ValueClose(NewF64(1.0), xG))\n\tassert.True(ValueClose(NewF64(1.0), aG))\n\n\t/* FLOAT 32 Scalar */\n\n\tx, y, a, b, v, err = unaryOpTest(t, Float32, tensor.Shape{}, Abs)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tyV = y.Value()\n\tif xG, err = x.Grad(); err != nil {\n\t\tt.Errorf(\"x has no grad: %v\", err)\n\t\treturn\n\t}\n\n\tbV = b.Value()\n\tif aG, err = a.Grad(); err != nil {\n\t\tt.Errorf(\"a has no grad: %v\", err)\n\t}\n\n\tcorrectF32 := math32.Abs(v.Data().(float32))\n\tassert.True(ValueClose(NewF32(correctF32), yV))\n\tassert.True(ValueClose(NewF32(correctF32), bV))\n\tassert.True(ValueClose(NewF32(1.0), xG))\n\tassert.True(ValueClose(NewF32(1.0), aG))\n\n\t/* FLOAT64 Vector */\n\n\tx, y, a, b, v, err = unaryOpTest(t, Float64, tensor.Shape{10}, Abs)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tyV = y.Value()\n\tif xG, err = x.Grad(); err != nil {\n\t\tt.Errorf(\"x has no grad: %v\", err)\n\t\treturn\n\t}\n\n\tbV = b.Value()\n\tif aG, err = a.Grad(); err != nil {\n\t\tt.Errorf(\"a has no grad: %v\", err)\n\t}\n\n\tabsF64s := v.Data().([]float64)\n\tbackingGrad64 := make([]float64, len(absF64s))\n\tfor i, v := range absF64s {\n\t\tabsF64s[i] = math.Abs(v)\n\t\tif v > 0 {\n\t\t\tbackingGrad64[i] = 1\n\t\t} else {\n\t\t\tbackingGrad64[i] = -1\n\t\t}\n\t}\n\tcorrectVecF64 := tensor.New(tensor.WithBacking(absF64s))\n\tgradF64s := tensor.New(tensor.WithBacking(backingGrad64))\n\n\tassert.True(ValueClose(correctVecF64, yV))\n\tassert.True(ValueClose(correctVecF64, bV))\n\tassert.True(ValueClose(gradF64s, xG), \"xG %v\", xG)\n\tassert.True(ValueClose(gradF64s, aG), \"aG %v\", aG)\n\n\t/* FLOAT32 Vector */\n\n\tx, y, a, b, v, err = unaryOpTest(t, Float32, tensor.Shape{10}, Abs)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tyV = y.Value()\n\tif xG, err = x.Grad(); err != nil {\n\t\tt.Errorf(\"x has no grad: %v\", err)\n\t\treturn\n\t}\n\n\tbV = b.Value()\n\tif aG, err = a.Grad(); err != nil {\n\t\tt.Errorf(\"a has no grad: %v\", err)\n\t}\n\n\tabsF32s := v.Data().([]float32)\n\tbackingGrad32 := make([]float32, len(absF32s))\n\tfor i, v := range absF32s {\n\t\tabsF32s[i] = math32.Abs(v)\n\t\tif v > 0 {\n\t\t\tbackingGrad32[i] = 1\n\t\t} else {\n\t\t\tbackingGrad32[i] = -1\n\t\t}\n\t}\n\tcorrectVecF32 := tensor.New(tensor.WithBacking(absF32s))\n\tgradF32s := tensor.New(tensor.WithBacking(backingGrad32))\n\n\tassert.True(ValueClose(correctVecF32, yV))\n\tassert.True(ValueClose(correctVecF32, bV))\n\tassert.True(ValueClose(gradF32s, xG), \"xG %v\", xG)\n\tassert.True(ValueClose(gradF32s, aG), \"aG %v\", aG)\n\n}\n\nfunc TestSinDiff(t *testing.T) {\n\tassert := assert.New(t)\n\tv, x, _, xT, _, err := unaryOpDiffTest(sinOpType)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tcorrect := math.Cos(v)\n\tassert.Equal(correct, x.boundTo.(*dualValue).d.Data())\n\n\t// Tensor edition\n\txdvd := xT.boundTo.(*dualValue).d.(*tensor.Dense)\n\tcorrectT := []float64{math.Cos(-v), math.Cos(v)}\n\tassert.Equal(correctT, xdvd.Data())\n}\n\nfunc TestCosDiff(t *testing.T) {\n\tassert := assert.New(t)\n\n\tv, x, _, xT, _, err := unaryOpDiffTest(cosOpType)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tassert.Equal(-math.Sin(v), x.boundTo.(*dualValue).d.Data())\n\n\t// Tensor edition\n\txdvd := xT.boundTo.(*dualValue).d.(*tensor.Dense)\n\tcorrect := []float64{-math.Sin(-v), -math.Sin(v)}\n\tassert.Equal(correct, xdvd.Data())\n}\n\nfunc TestExpDiff(t *testing.T) {\n\tassert := assert.New(t)\n\t_, x, y, xT, yT, err := unaryOpDiffTest(expOpType)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tassert.Equal(y.boundTo.(*dualValue).Value, x.boundTo.(*dualValue).d)\n\n\t// Tensor edition\n\txdvd := xT.boundTo.(*dualValue).d.(*tensor.Dense)\n\tydvd := yT.boundTo.(*dualValue).Value.(*tensor.Dense)\n\tassert.Equal(ydvd.Data(), xdvd.Data())\n}\n\nfunc TestLnDiff(t *testing.T) {\n\tassert := assert.New(t)\n\tvar err error\n\tv, x, _, xT, _, err := unaryOpDiffTest(lnOpType)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tcorrect := 1.0 / v\n\tassert.Equal(correct, x.boundTo.(*dualValue).d.Data(), \"v was %v\", v)\n\n\t// Tensor edition\n\txdvd := xT.boundTo.(*dualValue).d.(*tensor.Dense)\n\tcorrectT := []float64{1.0 / -v, 1.0 / v}\n\tassert.Equal(correctT, xdvd.Data())\n}\n\nfunc TestLog2Diff(t *testing.T) {\n\tassert := assert.New(t)\n\tv, x, _, xT, _, err := unaryOpDiffTest(log2OpType)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tcorrect := 1.0 / (v * math.Ln2)\n\tassert.Equal(correct, x.boundTo.(*dualValue).d.Data())\n\n\t// Tensor edition\n\txdvd := xT.boundTo.(*dualValue).d.(*tensor.Dense)\n\tcorrectT := []float64{1.0 / (-v * math.Ln2), 1.0 / (v * math.Ln2)}\n\tassert.Equal(correctT, xdvd.Data())\n}\n\nfunc TestSquareDiff(t *testing.T) {\n\tassert := assert.New(t)\n\tvar err error\n\tv, x, _, xT, _, err := unaryOpDiffTest(squareOpType)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tassert.Equal(2*v, x.boundTo.(*dualValue).d.Data())\n\n\t// Tensor edition\n\txdvd := xT.boundTo.(*dualValue).d.(*tensor.Dense)\n\tcorrect := []float64{2 * -v, 2 * v}\n\tassert.Equal(correct, xdvd.Data())\n}\n\nfunc TestSqrtDiff(t *testing.T) {\n\tassert := assert.New(t)\n\tv, x, _, xT, _, err := unaryOpDiffTest(sqrtOpType)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tassert.Equal(1.0/(2*math.Sqrt(v)), x.boundTo.(*dualValue).d.Data())\n\n\t// Tensor edition\n\txdvd := xT.boundTo.(*dualValue).d.(*tensor.Dense)\n\tcorrect := []float64{1.0 / (2 * math.Sqrt(-v)), 1.0 / (2 * math.Sqrt(v))}\n\tgot := xdvd.Data().([]float64)\n\tif !math.IsNaN(got[0]) && math.IsNaN(correct[0]) {\n\t\tt.Error(\"Expected NaN for the first value\")\n\t}\n\tif got[1] != correct[1] {\n\t\tt.Error(\"Different second values\")\n\t}\n}\n\nfunc TestInverseDiff(t *testing.T) {\n\tassert := assert.New(t)\n\tv, x, _, xT, _, err := unaryOpDiffTest(inverseOpType)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tcorrect := -((1 / v) * (1 / v))\n\tassert.Equal(correct, x.boundTo.(*dualValue).d.Data())\n\n\t// Tensor edition\n\txdvd := xT.boundTo.(*dualValue).d.(*tensor.Dense)\n\tcorrectT := []float64{correct, correct}\n\tassert.Equal(correctT, xdvd.Data())\n}\n\nfunc TestCubeDiff(t *testing.T) {\n\tassert := assert.New(t)\n\tv, x, _, xT, _, err := unaryOpDiffTest(cubeOpType)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tcorrect := 3 * v * v\n\txG, err := x.Grad()\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tassert.True(dawson.CloseF64(correct, extractF64(xG)), \"%v != %v\", xG, correct)\n\n\t// Tensor edition\n\txdvd := xT.boundTo.(*dualValue).d\n\tcorrectT := []float64{correct, correct}\n\tassert.True(floatsEqual64(correctT, extractF64s(xdvd)))\n}\n\nfunc TestTanhDiff(t *testing.T) {\n\tassert := assert.New(t)\n\tv, x, _, xT, _, err := unaryOpDiffTest(tanhOpType)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\t// NOTE: there are not guarantees of identical behaviours across architectures,\n\t// in this case arm64 gives different results than amd64 for Tanh.\n\t// See https://github.com/golang/go/issues/18354#issuecomment-267705645\n\tcorrect := 1.0 - (float64(math.Tanh(v)) * float64(math.Tanh(v))) // I'm surprised Golang doesn't have a secant function!\n\tassert.InDeltaf(correct, x.boundTo.(*dualValue).d.Data(), 1e-14, \"\")\n\n\t// Tensor edition\n\txdvd := xT.boundTo.(*dualValue).d.(*tensor.Dense)\n\tassert.InDeltaSlicef([]float64{correct, correct}, xdvd.Data(), 1e-14, \"\")\n}\n\nfunc TestSigmoidDiff(t *testing.T) {\n\tassert := assert.New(t)\n\tv, x, _, xT, _, err := unaryOpDiffTest(sigmoidOpType)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tcorrect := math.Exp(-v) / ((1 + math.Exp(-v)) * (1 + math.Exp(-v)))\n\txG := x.boundTo.(*dualValue).d\n\tassert.True(dawson.CloseF64(correct, extractF64(xG)))\n\n\t// Tensor edition\n\txdvd := xT.boundTo.(*dualValue).d\n\tnegCorrect := math.Exp(v) / ((1 + math.Exp(v)) * (1 + math.Exp(v)))\n\tcorrects := []float64{negCorrect, correct}\n\tassert.True(floatsEqual64(corrects, extractF64s(xdvd)))\n}\n\nfunc TestLog1pDiff(t *testing.T) {\n\tassert := assert.New(t)\n\tv, x, _, xT, _, err := unaryOpDiffTest(log1pOpType)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tcorrect := 1 / (1.0 + v)\n\tassert.Equal(correct, x.boundTo.(*dualValue).d.Data())\n\n\t// Tensor edition\n\txdvd := xT.boundTo.(*dualValue).d.(*tensor.Dense)\n\tcorrect0 := 1 / (1.0 - v)\n\tassert.Equal([]float64{correct0, correct}, xdvd.Data())\n}\n\nfunc TestExpm1Diff(t *testing.T) {\n\tassert := assert.New(t)\n\tv, x, _, xT, _, err := unaryOpDiffTest(expm1OpType)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\n\tcorrect := math.Exp(v)\n\tassert.Equal(correct, x.boundTo.(*dualValue).d.Data())\n\n\t// Tensor edition\n\txdvd := xT.boundTo.(*dualValue).d.(*tensor.Dense)\n\tcorrect0 := math.Exp(-v)\n\tassert.Equal([]float64{correct0, correct}, xdvd.Data())\n}\n\nfunc TestSoftplus(t *testing.T) {\n\tassert := assert.New(t)\n\n\tvar x, y, a, b *Node\n\tvar v Value\n\tvar xV, yV, xG, bV, aG Value\n\tvar err error\n\n\t/* FLOAT64 SCALAR */\n\n\tif x, y, a, b, v, err = unaryOpTest(t, Float64, tensor.Shape{}, Softplus); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\txV = x.Value()\n\tyV = y.Value()\n\tif xG, err = x.Grad(); err != nil {\n\t\tt.Errorf(\"x has no grad: %v\", err)\n\t\treturn\n\t}\n\n\tbV = b.Value()\n\tif aG, err = a.Grad(); err != nil {\n\t\tt.Errorf(\"a has no grad: %v\", err)\n\t}\n\n\tcorrectVF64 := softplusf64(v.Data().(float64))\n\tcorrectDF64 := sigmoidf64(xV.Data().(float64))\n\tassert.True(ValueClose(NewF64(correctVF64), yV))\n\tassert.True(ValueClose(NewF64(correctVF64), bV))\n\tassert.True(ValueClose(NewF64(correctDF64), xG))\n\tassert.True(ValueClose(NewF64(correctDF64), aG))\n\n\t/* FLOAT32 SCALAR */\n\n\tif x, y, a, b, v, err = unaryOpTest(t, Float32, tensor.Shape{}, Softplus); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\txV = x.Value()\n\tyV = y.Value()\n\tif xG, err = x.Grad(); err != nil {\n\t\tt.Errorf(\"x has no grad: %v\", err)\n\t\treturn\n\t}\n\n\tbV = b.Value()\n\tif aG, err = a.Grad(); err != nil {\n\t\tt.Errorf(\"a has no grad: %v\", err)\n\t}\n\n\tcorrectVF32 := softplusf32(v.Data().(float32))\n\tcorrectDF32 := sigmoidf32(xV.Data().(float32))\n\tassert.True(ValueClose(NewF32(correctVF32), yV))\n\tassert.True(ValueClose(NewF32(correctVF32), bV))\n\tassert.True(ValueClose(NewF32(correctDF32), xG))\n\tassert.True(ValueClose(NewF32(correctDF32), aG))\n\n\t/* FLOAT64 Vector */\n\n\tif x, y, a, b, v, err = unaryOpTest(t, Float64, tensor.Shape{10}, Softplus); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\txV = x.Value()\n\tyV = y.Value()\n\tif xG, err = x.Grad(); err != nil {\n\t\tt.Errorf(\"x has no grad: %v\", err)\n\t\treturn\n\t}\n\n\tbV = b.Value()\n\tif aG, err = a.Grad(); err != nil {\n\t\tt.Errorf(\"a has no grad: %v\", err)\n\t}\n\n\tcorrectVF64s := v.Data().([]float64)\n\tcorrectDF64s := xV.Data().([]float64)\n\n\tfor i, v := range correctVF64s {\n\t\tcorrectVF64s[i] = softplusf64(v)\n\t\tcorrectDF64s[i] = sigmoidf64(correctDF64s[i])\n\t}\n\tassert.True(floatsEqual64(correctVF64s, yV.Data().([]float64)))\n\tassert.True(floatsEqual64(correctVF64s, bV.Data().([]float64)))\n\tassert.True(floatsEqual64(correctDF64s, xG.Data().([]float64)))\n\tassert.True(floatsEqual64(correctDF64s, aG.Data().([]float64)))\n\n\t/* FLOAT32 Vector */\n\n\tif x, y, a, b, v, err = unaryOpTest(t, Float32, tensor.Shape{10}, Softplus); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\txV = x.Value()\n\tyV = y.Value()\n\tif xG, err = x.Grad(); err != nil {\n\t\tt.Errorf(\"x has no grad: %v\", err)\n\t\treturn\n\t}\n\n\tbV = b.Value()\n\tif aG, err = a.Grad(); err != nil {\n\t\tt.Errorf(\"a has no grad: %v\", err)\n\t}\n\n\tcorrectVF32s := v.Data().([]float32)\n\tcorrectDF32s := xV.Data().([]float32)\n\n\tfor i, v := range correctVF32s {\n\t\tcorrectVF32s[i] = softplusf32(v)\n\t\tcorrectDF32s[i] = sigmoidf32(correctDF32s[i])\n\t}\n\tassert.True(floatsEqual32(correctVF32s, yV.Data().([]float32)))\n\tassert.True(floatsEqual32(correctVF32s, bV.Data().([]float32)))\n\tassert.True(floatsEqual32(correctDF32s, xG.Data().([]float32)))\n\tassert.True(floatsEqual32(correctDF32s, aG.Data().([]float32)))\n}\n"
        },
        {
          "name": "ops",
          "type": "tree",
          "content": null
        },
        {
          "name": "opt.go",
          "type": "blob",
          "size": 0.689453125,
          "content": "package gorgonia\n\n// global options\n\nvar debugDerives = true\nvar stabilization = true\nvar optimizationLevel = 0\n\n// UseStabilization sets the global option to invoke stabilization functions when building the graph.\n// Numerical stabilization is on by default\nfunc UseStabilization() {\n\tstabilization = true\n}\n\n// UseNonStable turns off the stabilization functions when building graphs.\nfunc UseNonStable() {\n\tstabilization = false\n}\n\n// DebugDerives turns on the derivation debug option when printing a graph\nfunc DebugDerives() {\n\tdebugDerives = true\n}\n\n// DontDebugDerives turns off derivation debug option when printing a graph.\n// It is off by default\nfunc DontDebugDerives() {\n\tdebugDerives = false\n}\n"
        },
        {
          "name": "perf.go",
          "type": "blob",
          "size": 2.6396484375,
          "content": "package gorgonia\n\nimport (\n\t\"sync\"\n\n\t\"github.com/chewxy/hm\"\n\t\"gorgonia.org/tensor\"\n)\n\nvar nodePool = &sync.Pool{\n\tNew: func() interface{} { return new(Node) },\n}\n\nfunc borrowNode() *Node { return nodePool.Get().(*Node) }\n\nfunc returnNode(n *Node) {\n\t// if the node is being returned to the pool then it should be removed from the graph that it is linked too as well\n\tif n.g != nil {\n\t\tn.g.RemoveNode(n)\n\t}\n\n\t// zero out any data in the node\n\tReturnType(n.t)\n\ttensor.ReturnInts(n.shape)\n\n\tn.t = nil\n\tn.shape = nil\n\tn.op = nil\n\tn.children = nil\n\tn.name = \"\"\n\tn.group = \"\"\n\tn.groups = nil\n\tn.g = nil\n\tn.boundTo = nil\n\tn.derivOf = nil\n\tn.deriv = nil\n\tn.hash = 0\n\tn.hashed = false\n\tn.inferredShape = false\n\tn.unchanged = false\n\tn.isStmt = false\n\tn.ofInterest = false\n\n\tnodePool.Put(n)\n}\n\n// ReturnNode returns a node to the pool. It does not check that the *Node has been removed from the graph. USE WITH CAUTION.\nfunc ReturnNode(n *Node) {\n\tn.g = nil\n\treturnNode(n)\n}\n\n// handles Returning of Values\n\nvar dvpool = &sync.Pool{\n\tNew: func() interface{} { return new(dualValue) },\n}\n\nfunc borrowDV() *dualValue { return dvpool.Get().(*dualValue) }\n\nfunc returnDV(dv *dualValue) {\n\treturnValue(dv.d)\n\treturnValue(dv.Value)\n\t// if dvdT, ok := dv.d.(tensor.Tensor); ok {\n\t// \treturnTensor(dvdT)\n\t// }\n\t// if dvvT, ok := dv.Value.(tensor.Tensor); ok {\n\t// \treturnTensor(dvvT)\n\t// }\n\n\tdv.d = nil\n\tdv.Value = nil\n\tdvpool.Put(dv)\n}\n\nfunc returnTensor(t tensor.Tensor) {\n\ttensor.ReturnTensor(t)\n}\n\nfunc returnValue(v Value) {\n\tif t, ok := v.(tensor.Tensor); ok {\n\t\treturnTensor(t)\n\t}\n}\n\nvar dimSizerPool = new(sync.Map)\n\nfunc borrowDimSizers(size int) []DimSizer {\n\tvar pool *sync.Pool\n\tp, ok := dimSizerPool.Load(size)\n\n\tif !ok {\n\t\ts := size\n\t\tpool = &sync.Pool{\n\t\t\tNew: func() interface{} { return make([]DimSizer, s, s) },\n\t\t}\n\t\tdimSizerPool.Store(size, pool)\n\t} else {\n\t\tpool = p.(*sync.Pool)\n\t}\n\treturn pool.Get().([]DimSizer)\n}\n\nfunc returnDimSizers(ds []DimSizer) {\n\tp, ok := dimSizerPool.Load(cap(ds))\n\tif !ok {\n\t\treturn\n\t}\n\tpool := p.(*sync.Pool)\n\tfor i := range ds {\n\t\tds[i] = nil\n\t}\n\tpool.Put(ds)\n}\n\nvar tensorTypePool = &sync.Pool{\n\tNew: func() interface{} { return new(TensorType) },\n}\n\nfunc borrowTensorType() *TensorType {\n\treturn tensorTypePool.Get().(*TensorType)\n}\n\nfunc returnTensorType(t *TensorType) {\n\tswitch t {\n\tcase vecF64, vecF32:\n\t\treturn\n\tcase matF64, matF32:\n\t\treturn\n\tcase ten3F64, ten3F32:\n\t\treturn\n\t}\n\tt.Of = nil\n\tt.Dims = 0\n\ttensorTypePool.Put(t)\n}\n\n// ReturnType ...\nfunc ReturnType(t hm.Type) {\n\tswitch tt := t.(type) {\n\tcase *TensorType:\n\t\treturnTensorType(tt)\n\tcase TensorType:\n\t\t// do nothing\n\tcase tensor.Dtype:\n\t\t// do nothing\n\tcase *hm.FunctionType:\n\t\thm.ReturnFnType(tt)\n\t}\n}\n"
        },
        {
          "name": "perf_test.go",
          "type": "blob",
          "size": 0.173828125,
          "content": "package gorgonia\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestNodePool(t *testing.T) {\n\tassert := assert.New(t)\n\tn := borrowNode()\n\tassert.NotNil(n)\n}\n"
        },
        {
          "name": "regalloc.go",
          "type": "blob",
          "size": 7.271484375,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/xtgo/set\"\n)\n\n// this file holds all the code that relates to register allocation\n// a lot of the code is shamelessly copied from my previous HIL work, the thirteenthfloor\n// TODO: cleanup\n\ntype interval struct {\n\tstart, end int\n\n\tresult       register\n\treads        []register\n\tranges       []intervalRange\n\tusePositions []int\n}\n\nfunc newInterval() *interval {\n\tretVal := &interval{\n\t\tstart: -1,\n\t\tend:   -1,\n\t}\n\treturn retVal\n}\n\nfunc (i *interval) String() string {\n\treturn fmt.Sprintf(\"%s | %d - %d | %v\", i.result, i.start, i.end, i.usePositions)\n}\n\nfunc (i *interval) setFrom(from int) {\n\tif i.start == -1 || (from < i.start && from >= 0) {\n\t\ti.start = from\n\t}\n}\n\nfunc (i *interval) fix() {\n\tif len(i.usePositions) == 0 {\n\t\treturn\n\t}\n\ti.usePositions = set.Ints(i.usePositions)\n\ti.end = i.usePositions[len(i.usePositions)-1]\n\n\tfor _, r := range i.ranges {\n\t\tif r.to > i.end {\n\t\t\ti.end = r.to\n\t\t}\n\t}\n}\n\nfunc (i *interval) addRange(from, to int) {\n\tif to < from {\n\t\tpanic(\"to < from\") // note: to == from is a valid interval range\n\t}\n\n\tr := intervalRange{from, to}\n\n\t// because I'm lazy to create a intervalRangeSet type, we'll just iterate and check\n\tfor _, ra := range i.ranges {\n\t\tif r == ra {\n\t\t\treturn\n\t\t}\n\t}\n\n\ti.ranges = append(i.ranges, r)\n\n\t// set the end property\n\tif to > i.end {\n\t\ti.end = to\n\t}\n\n\ti.setFrom(from)\n}\n\n// added so only unique usePositions are added\nfunc (i *interval) addUsePositions(up int) {\n\ti.usePositions = append(i.usePositions, up)\n}\n\nfunc (i *interval) noUsePositions() bool {\n\tif len(i.usePositions) == 0 || i.usePositions == nil {\n\t\treturn true\n\t}\n\treturn false\n}\n\n// inclusive of start, but exclusive of end\nfunc (i *interval) liveAt(id int) bool {\n\t// compileLogf(\"%v live at %d\", i, id)\n\tif i.start <= id && id < i.end {\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (i *interval) lastUse() int {\n\tif len(i.usePositions) == 0 {\n\t\treturn -1\n\t}\n\n\t// if !sort.IntsAreSorted(i.usePositions) {\n\t// \tsort.Ints(i.usePositions)\n\t// }\n\treturn i.usePositions[len(i.usePositions)-1]\n}\n\nfunc (i *interval) merge(other *interval) {\n\tif other.start < i.start && other.start >= 0 {\n\t\ti.start = other.start\n\t}\n\n\tif other.end > i.end {\n\t\ti.end = other.end\n\t}\n\n\tfor _, r := range other.ranges {\n\t\ti.addRange(r.from, r.to)\n\t}\n\n\ti.usePositions = append(i.usePositions, other.usePositions...)\n\ti.usePositions = set.Ints(i.usePositions)\n\n}\n\ntype intervalRange struct {\n\tfrom, to int\n}\n\ntype regalloc struct {\n\tcpucount      int\n\tgpucount      int\n\tinstructionID int\n\tdf            *dataflow\n}\n\nfunc newRegalloc(df *dataflow) *regalloc {\n\treturn &regalloc{\n\t\tdf: df,\n\t}\n}\n\nfunc (ra *regalloc) newReg(device Device) register {\n\tvar out register\n\tswitch device {\n\tcase CPU:\n\t\tout = register{ra.cpucount, device}\n\t\tra.cpucount++\n\tdefault:\n\t\tout = register{ra.gpucount, device}\n\t\tra.gpucount++\n\n\t}\n\treturn out\n}\n\nfunc (ra *regalloc) allocArg(nInterv *interval) {\n\tnInterv.result = ra.newReg(CPU)\n}\n\nfunc (ra *regalloc) allocMutableOp(node *Node, nInterv *interval) {\n\t// create new write to if overwriteInput and the used register is stil live\n\tcompileLogf(\"Allocating MutableOp NodeID: %x returns pointer\", node.ID())\n\tcompileLogf(\"Op: %v\", node.op)\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tvar writeTo register\n\tvar reads []*interval\n\n\tvar children Nodes\n\tvar ok bool\n\tif children, ok = ra.df.devTransChildren[node]; !ok {\n\t\tcompileLogf(\"replacement children not found\")\n\t\tchildren = node.children\n\t}\n\tfor _, child := range children {\n\t\tcReplace := ra.df.replacements[child]\n\t\trepInterv := ra.df.intervals[cReplace]\n\t\treads = append(reads, repInterv)\n\t}\n\tcompileLogf(\"Read %v\", reads)\n\n\tvar letStmts Nodes\n\tit := node.g.To(node.ID())\n\tfor it.Next() {\n\t\tparent := it.Node()\n\n\t\tn := parent.(*Node)\n\t\tcompileLogf(\"Parent: %v | %T\", n, n.op)\n\t\tif n.isStmt {\n\t\t\t// compileLogf(\"isStmt\")\n\t\t\tif _, ok := n.op.(letOp); ok {\n\t\t\t\tletStmts = append(letStmts, n)\n\t\t\t}\n\t\t}\n\t}\n\n\toverwrites := node.op.OverwritesInput()\n\tvar onDev bool\n\tswitch node.op.(type) {\n\tcase CUDADoer:\n\t\tonDev = true\n\tcase CLDoer:\n\t\tonDev = true\n\tdefault:\n\t}\n\n\tif overwrites >= 0 {\n\t\toverwriteReg := reads[overwrites].result\n\t\toverwriteDev := overwriteReg.device\n\t\toverwrittenIsLive := reads[overwrites].liveAt(ra.instructionID)\n\t\tcompileLogf(\"Overwrites : %v \", overwrites)\n\t\tcompileLogf(\"Overwritten (%v) is live at %d? %t\", reads[overwrites], ra.instructionID, overwrittenIsLive)\n\t\tcompileLogf(\"Let Statements: %d | %v\", len(letStmts), reads[overwrites])\n\n\t\t// If the overwritten is not live, and the node does not call external processes (obiviating the need to prealloc)\n\t\t// then we can directly overwrite the register.\n\t\tif len(letStmts) == 1 || !overwrittenIsLive {\n\n\t\t\tswitch {\n\t\t\tcase onDev && overwriteDev != CPU:\n\t\t\t\t// if overwritten reg is on external device and op will execute on external device\n\t\t\t\t// then safe to overwrite\n\t\t\t\twriteTo = overwriteReg\n\t\t\tcase !node.op.CallsExtern() && overwriteDev == CPU:\n\t\t\t\t// original case:\n\t\t\t\t// if the op doesn't call an extern, and is executed on CPU\n\t\t\t\t// safe to overwrite\n\t\t\t\twriteTo = overwriteReg\n\t\t\tcase onDev:\n\t\t\t\t// new register otherwise\n\t\t\t\twriteTo = ra.newReg(Device(0))\n\t\t\tcase !onDev:\n\t\t\t\t// new register otherwise\n\t\t\t\twriteTo = ra.newReg(CPU)\n\t\t\t}\n\n\t\t} else {\n\t\t\tif onDev {\n\t\t\t\twriteTo = ra.newReg(Device(0))\n\t\t\t} else {\n\t\t\t\twriteTo = ra.newReg(CPU)\n\t\t\t}\n\t\t}\n\t} else {\n\t\tcompileLogf(\"New register\")\n\t\tif onDev {\n\t\t\twriteTo = ra.newReg(Device(0))\n\t\t} else {\n\t\t\twriteTo = ra.newReg(CPU)\n\t\t}\n\t}\n\n\tfor _, r := range reads {\n\t\tnInterv.reads = append(nInterv.reads, r.result)\n\t}\n\tnInterv.result = writeTo\n\tcompileLogf(\"%v: %v\", node.op, nInterv)\n}\n\nfunc (ra *regalloc) allocImmutableOp(node *Node, nInterv *interval) {\n\tcompileLogf(\"Allocating Immutable Op\")\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tvar writeTo register\n\tvar reads []*interval\n\n\tvar children Nodes\n\tvar ok bool\n\tif children, ok = ra.df.devTransChildren[node]; !ok {\n\t\tchildren = node.children\n\t}\n\tfor _, child := range children {\n\t\tcReplace := ra.df.replacements[child]\n\t\trepInterv := ra.df.intervals[cReplace]\n\t\treads = append(reads, repInterv)\n\t}\n\n\tcompileLogf(\"NodeID: %x does not returns pointer\", node.ID())\n\tif _, ok := node.op.(CUDADoer); ok {\n\t\twriteTo = ra.newReg(Device(0))\n\t} else {\n\t\twriteTo = ra.newReg(CPU)\n\t}\n\n\tfor _, r := range reads {\n\t\tnInterv.reads = append(nInterv.reads, r.result)\n\t}\n\tnInterv.result = writeTo\n}\n\nfunc (ra *regalloc) allocStatement(node *Node, nInterv *interval) {\n\tvar writeTo register\n\tswitch op := node.op.(type) {\n\tcase devTrans:\n\t\twriteTo = ra.newReg(op.to)\n\t}\n\tnInterv.result = writeTo\n}\n\nfunc (ra *regalloc) alloc(sorted Nodes) {\n\tcompileLogf(\"Allocating registers\")\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tfor i, node := range sorted {\n\t\tra.instructionID = i\n\n\t\treplacement := ra.df.replacements[node]\n\t\tnInterv := ra.df.intervals[replacement]\n\n\t\tcompileLogf(\"replacement %v, interval %v\", replacement, nInterv)\n\n\t\tif node != replacement {\n\t\t\tcompileLogf(\"Merging\")\n\t\t\tra.df.intervals[node].merge(nInterv)\n\t\t}\n\t\tcompileLogf(\"Working on %v(%x). InstructionID: %d\", node, node.ID(), ra.instructionID)\n\n\t\tswitch {\n\t\tcase node.isArg():\n\t\t\tra.allocArg(nInterv)\n\t\tcase node.isStmt:\n\t\t\tra.allocStatement(node, nInterv)\n\t\tcase node.op.ReturnsPtr():\n\t\t\tra.allocMutableOp(node, nInterv)\n\t\tdefault:\n\t\t\tra.allocImmutableOp(node, nInterv)\n\t\t}\n\t\tcompileLogf(\"n: %x; result: %v; reads: %v\", node.ID(), nInterv.result, nInterv.reads)\n\t\t// ra.instructionID++\n\t}\n}\n"
        },
        {
          "name": "regalloc_test.go",
          "type": "blob",
          "size": 1.763671875,
          "content": "package gorgonia\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestIntervalMethods(t *testing.T) {\n\tassert := assert.New(t)\n\tiv := newInterval()\n\n\tiv.setFrom(2)\n\n\tif !iv.noUsePositions() {\n\t\tt.Error(\"interval has no useposition. How did a usePos get in?\")\n\t}\n\n\t//simulate an register being defined at instruction 2, and its last use was 7\n\tiv.addRange(2, 8)\n\tiv.addUsePositions(6)\n\tassert.Equal([]int{6}, iv.usePositions)\n\tassert.Equal([]intervalRange{{2, 8}}, iv.ranges)\n\n\t// now comes a new player... it essentially uses the same data in the same register as iv\n\t// but was defined a few instructions down the road.\n\tiv2 := newInterval()\n\tiv2.addRange(20, 25)\n\tiv2.addUsePositions(22)\n\n\tiv.merge(iv2)\n\tassert.Equal([]int{6, 22}, iv.usePositions)\n\tassert.Equal(iv2.end, iv.end)\n\tassert.Equal(2, iv.start)\n\tassert.Equal([]intervalRange{iv.ranges[0], iv2.ranges[0]}, iv.ranges)\n}\n\nfunc TestRegAlloc(t *testing.T) {\n\tvar sorted Nodes\n\tvar err error\n\n\tg, x, y, z := simpleVecEqn()\n\tz2 := Must(Square(z))\n\tif sorted, err = Sort(g); err != nil {\n\t\tt.Fatal(err)\n\t}\n\treverseNodes(sorted)\n\n\tdf := analyze(g, sorted)\n\tdf.buildIntervals(sorted)\n\tis := df.intervals\n\n\tra := newRegalloc(df)\n\tra.alloc(sorted)\n\n\tif is[x].result.id >= len(is) {\n\t\tt.Error(\"x is an input, and would have a lifetime of the entire program\")\n\t}\n\n\tif is[y].result.id >= len(is) {\n\t\tt.Error(\"y is an input, and would have a lifetime of the entire program\")\n\t}\n\n\tvar onDev bool\n\tswitch z2.op.(type) {\n\tcase CUDADoer:\n\t\tonDev = true\n\tcase CLDoer:\n\t\tonDev = true\n\t}\n\n\tswitch {\n\tcase z2.op.CallsExtern() && !onDev:\n\t\tif is[z].result.id == is[z2].result.id {\n\t\t\tt.Error(\"z2 should NOT reuse the register of z\")\n\t\t}\n\tdefault:\n\t\tif is[z].result.id != is[z2].result.id {\n\t\t\tt.Error(\"z2 should reuse the register of z\")\n\t\t}\n\t}\n\n}\n"
        },
        {
          "name": "release.go",
          "type": "blob",
          "size": 1.6494140625,
          "content": "// +build !debug\n\npackage gorgonia\n\n// DEBUG indicates if this build is in debug mode. It is not.\nconst DEBUG = false\n\nconst (\n\tcompileDev        = false\n\tshapeInferenceDev = false\n\ttypeSystemDev     = false\n\tsymdiffDev        = false\n\tautodiffDev       = false\n\tmachineDev        = false\n\tstabilizationDev  = false\n\tcudaDev           = false\n\tallocatorDev      = false\n)\n\nfunc tabcount() int { return 0 }\n\nfunc enterLogScope()                                   {}\nfunc leaveLogScope()                                   {}\nfunc logf(format string, others ...interface{})        {}\nfunc compileLogf(format string, attrs ...interface{})  {}\nfunc shapeLogf(format string, attrs ...interface{})    {}\nfunc typeSysLogf(format string, attrs ...interface{})  {}\nfunc symdiffLogf(format string, attrs ...interface{})  {}\nfunc autodiffLogf(format string, attrs ...interface{}) {}\nfunc machineLogf(format string, attrs ...interface{})  {}\nfunc stabLogf(format string, attrs ...interface{})     {}\nfunc solverLogf(format string, attrs ...interface{})   {}\nfunc cudaLogf(format string, attrs ...interface{})     {}\nfunc allocatorLogf(format string, attr ...interface{}) {}\nfunc recoverFrom(format string, attrs ...interface{})  {}\n\n// GraphCollisionStats returns the collisions in the graph only when built with the debug tag, otherwise it's a noop that returns 0\nfunc GraphCollisionStats() (int, int, int) { return 0, 0, 0 }\n\nfunc incrCC() {}\nfunc incrEC() {}\nfunc incrNN() {}\n\n/* Compilation related debug utility functions/methods*/\nfunc logCompileState(name string, g *ExprGraph, df *dataflow) {}\n\n/* Analysis Debug Utility Functions/Methods */\nfunc (df *dataflow) debugIntervals(sorted Nodes) {}\n"
        },
        {
          "name": "shape.go",
          "type": "blob",
          "size": 2.689453125,
          "content": "package gorgonia\n\nimport (\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\nvar scalarShape = tensor.ScalarShape()\n\ntype axes []int\ntype coordinates []int\n\n// only works for 2D\nfunc transpose2D(shape tensor.Shape) tensor.Shape {\n\tif len(shape) != 2 {\n\t\treturn shape\n\t}\n\tretVal := tensor.BorrowInts(2)\n\tretVal[0] = shape[1]\n\tretVal[1] = shape[0]\n\treturn retVal\n}\n\n// for batched matmul\nfunc transposeBatch2D(shape tensor.Shape) tensor.Shape {\n\tif len(shape) != 3 {\n\t\treturn shape\n\t}\n\tretVal := tensor.BorrowInts(3)\n\tretVal[0] = shape[0]\n\tretVal[1] = shape[2]\n\tretVal[2] = shape[1]\n\treturn retVal\n}\n\n// calcBroadcastShape calculates the new shape of a given Node and broadcast axes.\n// Note that `a` will be the *Node reshaped to the newShape.\nfunc calcBroadcastShape(a *Node, expectedDims int, broadcastAlong []int) (newShape tensor.Shape) {\n\tshp := a.Shape()\n\tif shp.Dims() == expectedDims {\n\t\tnewShape = shp.Clone()\n\t} else {\n\t\tnewShape = make(tensor.Shape, expectedDims)\n\t\tfor _, i := range broadcastAlong {\n\t\t\tnewShape[i] = 1\n\t\t}\n\t}\n\n\tswitch {\n\tcase a.Shape().Eq(tensor.ScalarShape()):\n\t\tfor i := range newShape {\n\t\t\tnewShape[i] = 1\n\t\t}\n\tcase shp.Dims() == expectedDims:\n\tdefault:\n\t\tfor _, s := range a.Shape() {\n\t\t\t// search for first non 0\n\t\t\tfor j := range newShape {\n\t\t\t\tif newShape[j] == 0 {\n\t\t\t\t\tnewShape[j] = s\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn\n}\n\n// KeepDims is a function that ensures that input and output dimensions are the same though the shape may change.\n//\n// The expandLeft flag in the function indicates if any shape expansion should be done leftwards or rightwards.\n// For example, if fn() returns a tensor with a shape (3) and the desired dimension is 2,\n// then if `expandLeft` is true the result will be `(1, 3)`. Otherwise the result will be `(3, 1)`.\n//\n// At the moment, results that turn into scalars cannot have their dimensions kept - the semantics isn't well established yet and is a work in progress.\nfunc KeepDims(a *Node, expandLeft bool, fn func(a *Node) (*Node, error)) (*Node, error) {\n\toshape := a.Shape()\n\tadims := oshape.Dims()\n\tb, err := fn(a)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// happy path = quick exit\n\tnewShape := b.Shape()\n\tif newShape.Eq(oshape) {\n\t\treturn b, nil\n\t}\n\n\tbdims := newShape.Dims()\n\tdiff := adims - bdims\n\tif diff < 0 {\n\t\treturn b, errors.Errorf(\"Unable to KeepDims for a result with shape %v. It has more dimensions than input %v\", newShape, oshape)\n\t}\n\tvar retShape tensor.Shape\n\tif expandLeft {\n\t\tretShape = tensor.BorrowInts(diff + newShape.Dims())\n\t\tfor i := 0; i < diff; i++ {\n\t\t\tretShape[i] = 1\n\t\t}\n\t\tcopy(retShape[diff:], newShape)\n\t} else {\n\t\tretShape = newShape.Clone()\n\t\tfor i := 0; i < diff; i++ {\n\t\t\tretShape = append(retShape, 1)\n\t\t}\n\n\t}\n\treturn Reshape(b, retShape)\n}\n"
        },
        {
          "name": "shape_test.go",
          "type": "blob",
          "size": 1.8935546875,
          "content": "package gorgonia_test\n\nimport (\n\t\"fmt\"\n\n\t. \"gorgonia.org/gorgonia\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc Example_keepDims() {\n\tg := NewGraph()\n\ta := NodeFromAny(g, tensor.New(tensor.WithShape(2, 3), tensor.WithBacking([]float64{1, 2, 3, 4, 5, 6})))\n\tm1, _ := Mean(a, 1)\n\tm2, _ := KeepDims(a, false, func(a *Node) (*Node, error) { return Mean(a, 1) })\n\tm3, _ := Mean(a, 0)\n\tm4, _ := KeepDims(a, true, func(a *Node) (*Node, error) { return Mean(a, 0) })\n\tm5, _ := KeepDims(a, true, func(a *Node) (*Node, error) { return Mean(a) })\n\n\t// these reads are necessary as the VM may feel free to clobber the underlying data.\n\t// e.g. if m1.Value() is used in the print statement below, the answer will be wrong.\n\t// This is because before the VM executes the operations, a check is done to see if unsafe\n\t// operations may be done. Unsafe operations are useful in saving memory.\n\t// In this example, Reshape can be unsafely done if no other node is \"using\" m1,\n\t// so m1.Value() will have its shape clobbered. Thus if m1.Value() is read after the VM has run,\n\t// there is no guarantee that the data is correct. The only way around this is to \"use\" m1, by the Read() function.\n\tvar m1v, m2v, m3v, m4v Value\n\tRead(m1, &m1v)\n\tRead(m2, &m2v)\n\tRead(m3, &m3v)\n\tRead(m4, &m4v)\n\n\tvm := NewTapeMachine(g)\n\tif err := vm.RunAll(); err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Printf(\"a:\\n%v\\n\", a.Value())\n\tfmt.Printf(\"m1 (shape: %v):\\n%v\\n\", m1.Value().Shape(), m1v)\n\tfmt.Printf(\"m2 (shape: %v):\\n%v\\n\", m2.Value().Shape(), m2v)\n\tfmt.Printf(\"m3 (shape: %v):\\n%v\\n\", m3.Value().Shape(), m3v)\n\tfmt.Printf(\"m4 (shape: %v):\\n%v\\n\", m4.Value().Shape(), m4v)\n\tfmt.Printf(\"m5 (shape: %v):\\n%v\\n\", m5.Value().Shape(), m5.Value())\n\n\t// Output:\n\t// a:\n\t// ⎡1  2  3⎤\n\t// ⎣4  5  6⎦\n\t//\n\t// m1 (shape: (2)):\n\t// [2  5]\n\t// m2 (shape: (2, 1)):\n\t// C[2  5]\n\t// m3 (shape: (3)):\n\t// [2.5  3.5  4.5]\n\t// m4 (shape: (1, 3)):\n\t// R[2.5  3.5  4.5]\n\t// m5 (shape: (1, 1)):\n\t// [[3.5]]\n\n}\n"
        },
        {
          "name": "slice.go",
          "type": "blob",
          "size": 0.8349609375,
          "content": "package gorgonia\n\nimport \"gorgonia.org/tensor\"\n\n// sli is slice. It's named sli to prevent confusion over naming\ntype sli struct {\n\tstart, end, step int\n}\n\n// S creates a tensor.Slice.\n// end is optional. It should be passed in as the first param of the optionals.\n// step is optional. It should be passed in as the second param of the optionals.\n//\n// Default end is start+1. Default step is 1, unless end == step+1, then it defaults to 0\nfunc S(start int, opt ...int) tensor.Slice {\n\tvar end, step int\n\tif len(opt) > 0 {\n\t\tend = opt[0]\n\t} else {\n\t\tend = start + 1\n\t}\n\n\tstep = 1\n\tif len(opt) > 1 {\n\t\tstep = opt[1]\n\t} else if end == start+1 {\n\t\tstep = 0\n\t}\n\n\treturn &sli{\n\t\tstart: start,\n\t\tend:   end,\n\t\tstep:  step,\n\t}\n}\n\nfunc (s *sli) Start() int { return s.start }\nfunc (s *sli) End() int   { return s.end }\nfunc (s *sli) Step() int  { return s.step }\n"
        },
        {
          "name": "solvers.go",
          "type": "blob",
          "size": 40.8525390625,
          "content": "package gorgonia\n\nimport (\n\t\"math\"\n\n\t\"github.com/chewxy/math32\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\n// Solver is anything that does gradient updates.\n// The name solvers is stolen from Caffe. A much shorter name than GradientUpdaters\ntype Solver interface {\n\tStep([]ValueGrad) error\n}\n\n// ValueGrad is any type that has a value and a grad. This is used for Solvers\ntype ValueGrad interface {\n\tValuer\n\tGrad() (Value, error)\n}\n\n// Namer is anything that has a name\ntype Namer interface {\n\tName() string\n}\n\nfunc newCachedDV(n ValueGrad, weights, grad Value, zero bool) (cached *dualValue, err error) {\n\tcached = new(dualValue)\n\tif cached.Value, err = CloneValue(weights); err != nil {\n\t\tif nm, ok := n.(Namer); ok {\n\t\t\treturn nil, errors.Wrapf(err, \"Failed to clone weights of %v\", nm.Name())\n\t\t}\n\t\treturn nil, errors.Wrap(err, \"Failed to clone weights\")\n\t}\n\tif cached.d, err = CloneValue(grad); err != nil {\n\t\tif nm, ok := n.(Namer); ok {\n\t\t\treturn nil, errors.Wrapf(err, \"Failed to clone grad of %v\", nm.Name())\n\t\t}\n\t\treturn nil, errors.Wrap(err, \"Failed to clone grad\")\n\t}\n\tif zero {\n\t\tcached.Value = ZeroValue(cached.Value)\n\t\tcached.d = ZeroValue(cached.d)\n\t}\n\treturn\n}\n\nfunc extractWeightGrad(n ValueGrad) (weights, grad Value, err error) {\n\tweights = n.Value()\n\tif grad, err = n.Grad(); err != nil {\n\t\tif nm, ok := n.(Namer); ok {\n\t\t\treturn weights, nil, errors.Wrapf(err, \"No Grad found for %v\", nm.Name())\n\t\t}\n\t\treturn weights, nil, errors.Wrap(err, \"No Grad found\")\n\t}\n\treturn\n}\n\n// SolverOpt is a function that provides construction options for a Solver\ntype SolverOpt func(s Solver)\n\n// WithL2Reg adds a L2 regularization parameter to the solver. By default, the solvers do not use any regularization param\nfunc WithL2Reg(l2reg float64) SolverOpt {\n\tf := func(s Solver) {\n\t\tswitch st := s.(type) {\n\t\tcase *RMSPropSolver:\n\t\t\tst.l2reg = l2reg\n\t\t\tst.useL2Reg = true\n\t\tcase *AdamSolver:\n\t\t\tst.l2reg = l2reg\n\t\t\tst.useL2Reg = true\n\t\tcase *VanillaSolver:\n\t\t\tst.l2reg = l2reg\n\t\t\tst.useL2Reg = true\n\t\tcase *Momentum:\n\t\t\tst.l2reg = l2reg\n\t\t\tst.useL2Reg = true\n\t\tcase *AdamW:\n\t\t\tst.l2reg = l2reg\n\t\t\tst.useL2Reg = true\n\t\t}\n\t}\n\treturn f\n}\n\n// WithL1Reg adds a L1 regularization parameter to the solver. By default, the solvers do not use any regularization param\nfunc WithL1Reg(l1reg float64) SolverOpt {\n\tf := func(s Solver) {\n\t\tswitch st := s.(type) {\n\t\tcase *AdamSolver:\n\t\t\tst.l1reg = l1reg\n\t\t\tst.useL1Reg = true\n\t\tcase *VanillaSolver:\n\t\t\tst.l1reg = l1reg\n\t\t\tst.useL1Reg = true\n\t\tcase *Momentum:\n\t\t\tst.l1reg = l1reg\n\t\t\tst.useL1Reg = true\n\t\tcase *AdamW:\n\t\t\tst.l1reg = l1reg\n\t\t\tst.useL1Reg = true\n\t\t}\n\t}\n\treturn f\n}\n\n// WithBatchSize sets the batch size for the solver. Currently only Adam and Vanilla (basic SGD) has batch size support\nfunc WithBatchSize(batch float64) SolverOpt {\n\tf := func(s Solver) {\n\t\tswitch st := s.(type) {\n\t\tcase *AdamSolver:\n\t\t\tst.batch = batch\n\t\tcase *VanillaSolver:\n\t\t\tst.batch = batch\n\t\tcase *Momentum:\n\t\t\tst.batch = batch\n\t\tcase *AdamW:\n\t\t\tst.batch = batch\n\t\t}\n\t}\n\treturn f\n}\n\n// WithEps sets the smoothing factor for the solver.\nfunc WithEps(eps float64) SolverOpt {\n\tf := func(s Solver) {\n\t\tswitch st := s.(type) {\n\t\tcase *RMSPropSolver:\n\t\t\tst.eps = eps\n\t\tcase *AdamSolver:\n\t\t\tst.eps = eps\n\t\tcase *AdamW:\n\t\t\tst.ɛ = eps\n\t\t}\n\t}\n\treturn f\n}\n\n// WithClip clips the gradient if it gets too crazy. By default all solvers do not have any clips attached\nfunc WithClip(clip float64) SolverOpt {\n\tf := func(s Solver) {\n\t\tswitch st := s.(type) {\n\t\tcase *RMSPropSolver:\n\t\t\tst.clip = clip\n\t\t\tst.useClip = true\n\t\tcase *AdamSolver:\n\t\t\tst.clip = clip\n\t\t\tst.useClip = true\n\t\tcase *VanillaSolver:\n\t\t\tst.clip = clip\n\t\t\tst.useClip = true\n\t\tcase *BarzilaiBorweinSolver:\n\t\t\tst.clip = clip\n\t\t\tst.useClip = true\n\t\tcase *Momentum:\n\t\t\tst.clip = clip\n\t\t\tst.useClip = true\n\t\tcase *AdamW:\n\t\t\tst.clip = clip\n\t\t\tst.useClip = true\n\t\t}\n\t}\n\treturn f\n}\n\n// WithLearnRate sets the learn rate or step size for the solver.\nfunc WithLearnRate(eta float64) SolverOpt {\n\tf := func(s Solver) {\n\t\tswitch st := s.(type) {\n\t\tcase *RMSPropSolver:\n\t\t\tst.eta = eta\n\t\tcase *AdamSolver:\n\t\t\tst.eta = eta\n\t\tcase *VanillaSolver:\n\t\t\tst.eta = eta\n\t\tcase *BarzilaiBorweinSolver:\n\t\t\tst.eta = eta\n\t\tcase *Momentum:\n\t\t\tst.eta = eta\n\t\tcase *AdamW:\n\t\t\tst.η = eta\n\t\t}\n\t}\n\treturn f\n}\n\n// WithBeta1 sets the beta1 param of the solver. Only works with Adam\nfunc WithBeta1(beta1 float64) SolverOpt {\n\tf := func(s Solver) {\n\t\tswitch st := s.(type) {\n\t\tcase *AdamSolver:\n\t\t\tst.beta1 = beta1\n\t\tcase *AdamW:\n\t\t\tst.β1 = beta1\n\t\t}\n\t}\n\treturn f\n}\n\n// WithBeta2 sets the beta1 param of the solver. Only works with Adam\nfunc WithBeta2(beta2 float64) SolverOpt {\n\tf := func(s Solver) {\n\t\tswitch st := s.(type) {\n\t\tcase *AdamSolver:\n\t\t\tst.beta2 = beta2\n\t\tcase *AdamW:\n\t\t\tst.β2 = beta2\n\t\t}\n\t}\n\treturn f\n}\n\n// WithRho sets the decay parameter of the RMSProp solver\nfunc WithRho(rho float64) SolverOpt {\n\tf := func(s Solver) {\n\t\tswitch st := s.(type) {\n\t\tcase *RMSPropSolver:\n\t\t\tst.decay = rho\n\t\tcase *AdamW:\n\t\t\tst.λ = rho\n\t\t}\n\t}\n\treturn f\n}\n\n// WithMomentum sets the momentum of the solver. It is a no-op is the solver's type is not Momentum\nfunc WithMomentum(momentum float64) SolverOpt {\n\tf := func(s Solver) {\n\t\tswitch st := s.(type) {\n\t\tcase *Momentum:\n\t\t\tst.momentum = momentum\n\t\t}\n\t}\n\treturn f\n}\n\n// RMSPropSolver is a solver that implements Geoffrey Hinton's RMSProp gradient descent optimization algorithm.\n// http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\ntype RMSPropSolver struct {\n\tdecay float64 // decay rate/rho\n\teps   float64 // smoothing factor\n\tl2reg float64 // l2 regularization\n\tclip  float64 // clip value\n\teta   float64 // learn rate\n\n\tuseClip, useL2Reg bool\n\n\t// unsettable\n\tcache []*dualValue\n}\n\n// NewRMSPropSolver creates an RMSProp solver with these default values:\n//\t\teta (learn rate)\t  : 0.001\n//\t\teps (smoothing factor): 1e-8\n//\t\trho (decay factor)    : 0.999\nfunc NewRMSPropSolver(opts ...SolverOpt) *RMSPropSolver {\n\ts := &RMSPropSolver{\n\t\tdecay: 0.999,\n\t\teps:   1e-8,\n\t\teta:   0.001,\n\t}\n\n\tfor _, opt := range opts {\n\t\topt(s)\n\t}\n\treturn s\n}\n\n// Step steps through each node in the model and applies the RMSProp gradient descent algorithm on the value.\n//\n// This function will error out if the nodes do not have an associated Grad value.\nfunc (s *RMSPropSolver) Step(model []ValueGrad) (err error) {\n\tif s.cache == nil {\n\t\ts.cache = make([]*dualValue, len(model))\n\t}\n\n\tfor i, n := range model {\n\t\tvar weights, grad Value\n\t\tif weights, grad, err = extractWeightGrad(n); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tvar cached *dualValue\n\t\tif cached = s.cache[i]; cached == nil {\n\t\t\tif cached, err = newCachedDV(n, weights, grad, true); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\ts.cache[i] = cached\n\t\t}\n\n\t\tcv := cached.Value\n\t\t// cw = cw*decay + (1-decay) * grad²\n\t\tswitch cw := cv.(type) {\n\t\tcase *tensor.Dense:\n\t\t\tvar gt, gt2, w, regularized tensor.Tensor\n\t\t\tvar decay, omdecay, stepSize, eps, l2reg, clip, negClip interface{}\n\t\t\tswitch cw.Dtype() {\n\t\t\tcase tensor.Float64:\n\t\t\t\tdecay = s.decay\n\t\t\t\tomdecay = 1.0 - s.decay\n\t\t\t\tstepSize = -s.eta\n\t\t\t\teps = s.eps\n\t\t\t\tl2reg = s.l2reg\n\t\t\t\tclip = s.clip\n\t\t\t\tnegClip = -s.clip\n\t\t\tcase tensor.Float32:\n\t\t\t\tdecay = float32(s.decay)\n\t\t\t\tomdecay = float32(1.0 - s.decay)\n\t\t\t\tstepSize = float32(-s.eta)\n\t\t\t\teps = float32(s.eps)\n\t\t\t\tl2reg = float32(s.l2reg)\n\t\t\t\tclip = float32(s.clip)\n\t\t\t\tnegClip = float32(-s.clip)\n\t\t\t}\n\n\t\t\tgt = grad.(tensor.Tensor)\n\t\t\tif gt2, err = tensor.Square(gt); err != nil {\n\t\t\t\treturn errors.Wrap(err, pointWiseSquareFail)\n\t\t\t}\n\t\t\ttensor.Mul(cw, decay, tensor.UseUnsafe())\n\t\t\ttensor.Mul(gt2, omdecay, tensor.UseUnsafe())\n\t\t\ttensor.Add(cw, gt2, tensor.UseUnsafe())\n\t\t\tdefer returnTensor(gt2)\n\n\t\t\tif s.useClip {\n\t\t\t\tif _, err = tensor.Clamp(gt, negClip, clip, tensor.UseUnsafe()); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, clampFail)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// regularize\n\t\t\tvar upd tensor.Tensor\n\t\t\tif upd, err = tensor.Add(cw, eps); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"Failed to carry Add()\")\n\t\t\t}\n\n\t\t\tif _, err = tensor.InvSqrt(upd, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, invSqrtFail)\n\t\t\t}\n\t\t\tif _, err = tensor.Mul(gt, stepSize, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t}\n\t\t\tif _, err = tensor.Mul(upd, gt, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t}\n\n\t\t\t// update\n\t\t\tw = weights.(*tensor.Dense)\n\t\t\tif s.useL2Reg {\n\t\t\t\tif regularized, err = tensor.Mul(w, l2reg); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t\t}\n\t\t\t\tif _, err = tensor.Sub(upd, regularized, tensor.UseUnsafe()); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, subFail)\n\t\t\t\t}\n\t\t\t\tdefer returnTensor(regularized)\n\t\t\t}\n\n\t\t\tif _, err = tensor.Add(w, upd, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, addFail)\n\t\t\t}\n\t\t\tdefer returnTensor(upd)\n\n\t\t\t// zero all\n\t\t\tgt.Zero()\n\n\t\tcase *F32:\n\t\t\tdecay := float32(s.decay)\n\t\t\tomdecay := float32(1.0 - s.decay)\n\t\t\tstepSize := float32(s.eta)\n\t\t\teps := float32(s.eps)\n\t\t\tl2reg := float32(s.l2reg)\n\n\t\t\tgs := grad.(*F32).any()\n\t\t\tc := cw.any()\n\t\t\tc = c*decay + omdecay*gs*gs\n\n\t\t\tcached.Value, _ = anyToScalar(c)\n\n\t\t\tw := weights.(*F32).any()\n\t\t\tupd := -stepSize*gs/math32.Sqrt(c+eps) - l2reg*w\n\t\t\tw += upd\n\n\t\t\t// because scalar values are copies, and not pointers, we have to actually re-update the dualValu in model[i]\n\t\t\t*(weights.(*F32)) = F32(w)\n\t\t\t*(grad.(*F32)) = F32(0.0)\n\t\tcase *F64:\n\t\t\tdecay := s.decay\n\t\t\tomdecay := 1.0 - s.decay\n\t\t\tstepSize := s.eta\n\t\t\teps := s.eps\n\t\t\tl2reg := s.l2reg\n\n\t\t\tgs := grad.(*F64).any()\n\t\t\tc := cw.any()\n\t\t\tc = c*decay + omdecay*gs*gs\n\n\t\t\tcached.Value, _ = anyToScalar(c)\n\n\t\t\tw := weights.(*F64).any()\n\t\t\tupd := -stepSize*gs/math.Sqrt(c+eps) - l2reg*w\n\t\t\tw += upd\n\n\t\t\t// because scalar values are copies, and not pointers, we have to actually re-update the dualValu in model[i]\n\t\t\t*(weights.(*F64)) = F64(w)\n\t\t\t*(grad.(*F64)) = F64(0.0)\n\t\tdefault:\n\t\t}\n\t\tsolverLogf(\"AFTER %1.1s\", n)\n\t}\n\treturn nil\n}\n\n// AdamSolver is the Adaptive Moment Estimation solver (basically RMSProp on steroids).\n// Paper: http://arxiv.org/abs/1412.6980\n//\n// We overload the purpose of existing data structure of a *dualValue. However, instead of just holding a value and its derivative,\n// the cache's *dualValues hold the Means of gradients (in .Value) and the variances of the gradients (in .d)\ntype AdamSolver struct {\n\teta   float64 // learn rate\n\teps   float64 // smoothing\n\tbeta1 float64 // modifier for means\n\tbeta2 float64 // modifier for variances\n\tclip  float64 // clip gradients\n\tl1reg float64 // l1 regularization parameter\n\tl2reg float64 // l2 regularization parameter\n\tbatch float64 // batch size\n\n\tuseClip, useL1Reg, useL2Reg bool\n\n\t// unsettable\n\titer  int\n\tcache []*dualValue\n}\n\n// NewAdamSolver creates an Adam solver with these default values:\n//\t\teta (learn rate)\t  \t: 0.001\n//\t\teps (smoothing factor)\t\t: 1e-8\n//\t\tbeta1\t\t\t\t: 0.9\n//\t\tbeta2 \t\t\t\t: 0.999\n//\t\tbatch\t\t\t\t: 1\nfunc NewAdamSolver(opts ...SolverOpt) *AdamSolver {\n\ts := &AdamSolver{\n\t\teta:   0.001,\n\t\teps:   1e-8,\n\t\tbeta1: 0.9,\n\t\tbeta2: 0.999,\n\t\tbatch: 1,\n\t}\n\n\tfor _, opt := range opts {\n\t\topt(s)\n\t}\n\treturn s\n}\n\n// Step steps through each node in the model and applies the Adaptive Moment Estimation gradient descent algorithm on the value.\n//\n// This function will error out if the nodes do not have an associated Grad value.\nfunc (s *AdamSolver) Step(model []ValueGrad) (err error) {\n\tif s.cache == nil {\n\t\ts.cache = make([]*dualValue, len(model))\n\t}\n\n\ts.iter++\n\tcorrection1 := (1 - math.Pow(s.beta1, float64(s.iter)))\n\tcorrection2 := (1 - math.Pow(s.beta2, float64(s.iter)))\n\n\tfor i, n := range model {\n\t\tvar weights, grad Value\n\t\tif weights, grad, err = extractWeightGrad(n); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tvar cached *dualValue\n\t\tif cached = s.cache[i]; cached == nil {\n\t\t\tif cached, err = newCachedDV(n, weights, grad, true); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\ts.cache[i] = cached\n\t\t}\n\n\t\tcvm := cached.Value // means of gradients\n\t\tcvv := cached.d     // variances of gradients\n\n\t\tswitch m := cvm.(type) {\n\t\tcase *tensor.Dense:\n\t\t\tg := grad.(*tensor.Dense)\n\t\t\tw := weights.(*tensor.Dense)\n\t\t\tv := cvv.(*tensor.Dense)\n\n\t\t\tvar l1reg, l2reg, clip, negClip, beta1, beta2, omβ1, omβ2, eps, negEta, onePerBatch interface{}\n\t\t\tvar correctionV1, correctionV2 interface{}\n\t\t\tswitch m.Dtype() {\n\t\t\tcase tensor.Float64:\n\t\t\t\tl1reg = s.l1reg\n\t\t\t\tl2reg = s.l2reg\n\t\t\t\tclip = s.clip\n\t\t\t\tnegClip = -s.clip\n\t\t\t\tbeta1 = s.beta1\n\t\t\t\tbeta2 = s.beta2\n\t\t\t\tomβ1 = float64(1) - s.beta1\n\t\t\t\tomβ2 = float64(1) - s.beta2\n\t\t\t\teps = s.eps\n\t\t\t\tnegEta = -s.eta\n\t\t\t\tonePerBatch = float64(1) / s.batch\n\t\t\t\tcorrectionV1 = float64(1) / float64(correction1)\n\t\t\t\tcorrectionV2 = float64(1) / float64(correction2)\n\t\t\tcase tensor.Float32:\n\t\t\t\tl1reg = float32(s.l1reg)\n\t\t\t\tl2reg = float32(s.l2reg)\n\t\t\t\tclip = float32(s.clip)\n\t\t\t\tnegClip = -float32(s.clip)\n\t\t\t\tbeta1 = float32(s.beta1)\n\t\t\t\tbeta2 = float32(s.beta2)\n\t\t\t\tomβ1 = float32(1) - float32(s.beta1)\n\t\t\t\tomβ2 = float32(1) - float32(s.beta2)\n\t\t\t\teps = float32(s.eps)\n\t\t\t\tnegEta = -float32(s.eta)\n\t\t\t\tonePerBatch = float32(1) / float32(s.batch)\n\t\t\t\tcorrectionV1 = float32(1) / float32(correction1)\n\t\t\t\tcorrectionV2 = float32(1) / float32(correction2)\n\t\t\t}\n\n\t\t\t// prep the regularization of gradients\n\t\t\tif s.useL1Reg {\n\t\t\t\tvar l1regs tensor.Tensor\n\t\t\t\tif l1regs, err = tensor.Sign(w); err != nil {\n\t\t\t\t\terrors.Wrap(err, signFail)\n\t\t\t\t}\n\t\t\t\tif l1regs, err = tensor.Mul(l1reg, l1regs, tensor.UseUnsafe()); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t\t}\n\t\t\t\tif _, err = tensor.Add(g, l1regs, tensor.UseUnsafe()); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, addFail)\n\t\t\t\t}\n\t\t\t\tdefer returnTensor(l1regs)\n\t\t\t}\n\n\t\t\tif s.useL2Reg {\n\t\t\t\tvar l2regs tensor.Tensor\n\t\t\t\tif l2regs, err = tensor.Mul(w, l2reg); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t\t}\n\n\t\t\t\tif _, err = tensor.Add(g, l2regs, tensor.UseUnsafe()); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, addFail)\n\t\t\t\t}\n\n\t\t\t\tdefer returnTensor(l2regs)\n\t\t\t}\n\n\t\t\tif s.batch > 1 {\n\t\t\t\tif _, err = tensor.Mul(g, onePerBatch, tensor.UseUnsafe()); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif s.useClip && s.clip > 0 {\n\t\t\t\tif _, err = tensor.Clamp(g, negClip, clip, tensor.UseUnsafe()); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, clampFail)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// prep done. Now let's apply the formula:\n\t\t\t// the formula is\n\t\t\t//\t\t(β_1 * m_t-1) + (1 - β_1)g_t ..................\t1\n\t\t\t//\t\t(β_2 * v_t-1) + (1 - β_2)*(g_t)² .............\t2\n\n\t\t\t// equation(1): t1 = grad * (1 - β_1)\n\t\t\tt1 := g.Clone().(*tensor.Dense)\n\t\t\tif _, err = tensor.Mul(t1, omβ1, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t}\n\n\t\t\t// equation(2): g = grad**2 * (1 - β_2)\n\t\t\tif _, err = tensor.Mul(g, g, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t}\n\t\t\tif _, err = tensor.Mul(g, omβ2, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t}\n\n\t\t\t// equation (1): cached = cached * beta1 + t1\n\t\t\tif _, err = tensor.Mul(m, beta1, tensor.WithIncr(t1), tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t}\n\n\t\t\t// equation (2): v = v * beta2 + g\n\t\t\tif _, err = tensor.Mul(v, beta2, tensor.WithIncr(g), tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t}\n\n\t\t\tdefer returnTensor(m)\n\t\t\tdefer returnTensor(v)\n\t\t\tcached.SetValue(t1)\n\t\t\tcached.SetDeriv(g.Clone().(*tensor.Dense))\n\n\t\t\t// now deal with the hats\n\t\t\tmHats := t1.Clone().(*tensor.Dense)\n\t\t\tvHats := g.Clone().(*tensor.Dense)\n\n\t\t\tif _, err = tensor.Mul(mHats, correctionV1, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t}\n\n\t\t\tif _, err = tensor.Mul(vHats, correctionV2, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t}\n\n\t\t\t// update := -eta * mHat / (sqrt(vHat) + epsilon)\n\t\t\tif _, err = tensor.Sqrt(vHats, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn // TODO: rewrite this to use InvSqrt\n\t\t\t}\n\n\t\t\tif _, err = tensor.Add(vHats, eps, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif _, err = tensor.Mul(mHats, negEta, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t}\n\n\t\t\tif _, err = tensor.Div(mHats, vHats, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tdefer returnTensor(vHats)\n\t\t\tdefer returnTensor(mHats)\n\n\t\t\tif _, err = tensor.Add(w, mHats, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, addFail)\n\t\t\t}\n\n\t\t\tg.Zero()\n\n\t\tcase *F32:\n\t\t\tg := grad.(*F32).any()\n\t\t\tw := weights.(*F32).any()\n\t\t\tv := cvv.(*F32).any()\n\t\t\tmm := m.any()\n\n\t\t\tl1reg := float32(s.l1reg)\n\t\t\tl2reg := float32(s.l2reg)\n\t\t\tbatch := float32(s.batch)\n\t\t\tclip := float32(s.clip)\n\t\t\tbeta1 := float32(s.beta1)\n\t\t\tbeta2 := float32(s.beta2)\n\t\t\teps := float32(s.eps)\n\t\t\teta := float32(s.eta)\n\n\t\t\tif s.useL1Reg {\n\t\t\t\tif w < 0 {\n\t\t\t\t\tl1reg = -l1reg\n\t\t\t\t}\n\t\t\t\tg += l1reg\n\t\t\t}\n\n\t\t\tif s.useL2Reg {\n\t\t\t\tl2reg *= w\n\t\t\t\tg += l2reg\n\t\t\t}\n\n\t\t\tif batch > 1 {\n\t\t\t\tg *= (1 / batch)\n\t\t\t}\n\n\t\t\tif s.useClip {\n\t\t\t\tif g > clip {\n\t\t\t\t\tg = clip\n\t\t\t\t} else if g < -clip {\n\t\t\t\t\tg = -clip\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tnewM := (beta1 * mm) + (1-beta1)*g\n\t\t\tnewV := (beta2 * v) + (1-beta2)*g*g\n\n\t\t\tcached.Value, _ = anyToScalar(newM)\n\t\t\tcached.d, _ = anyToScalar(newV)\n\n\t\t\tmHat := (1 / float32(correction1)) * newM\n\t\t\tvHat := (1 / float32(correction2)) * newV\n\n\t\t\tupd := -eta * mHat / (float32(math.Sqrt(float64(vHat))) + eps)\n\t\t\tw += upd\n\n\t\t\t*(weights.(*F32)) = F32(w)\n\t\t\t*(grad.(*F32)) = F32(0.0)\n\t\tcase *F64:\n\t\t\tg := grad.(*F64).any()\n\t\t\tw := weights.(*F64).any()\n\t\t\tv := cvv.(*F64).any()\n\t\t\tmm := m.any()\n\n\t\t\tl1reg := s.l1reg\n\t\t\tl2reg := s.l2reg\n\t\t\tbatch := s.batch\n\t\t\tclip := s.clip\n\t\t\tbeta1 := s.beta1\n\t\t\tbeta2 := s.beta2\n\t\t\teps := s.eps\n\t\t\teta := s.eta\n\n\t\t\tif s.useL1Reg {\n\t\t\t\tif w < 0 {\n\t\t\t\t\tl1reg = -l1reg\n\t\t\t\t}\n\t\t\t\tg += l1reg\n\t\t\t}\n\n\t\t\tif s.useL2Reg {\n\t\t\t\tl2reg *= w\n\t\t\t\tg += l2reg\n\t\t\t}\n\n\t\t\tif batch > 1 {\n\t\t\t\tg *= (1 / batch)\n\t\t\t}\n\n\t\t\tif s.useClip {\n\t\t\t\tif g > clip {\n\t\t\t\t\tg = clip\n\t\t\t\t} else if g < -clip {\n\t\t\t\t\tg = -clip\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tnewM := (beta1 * mm) + (1-beta1)*g\n\t\t\tnewV := (beta2 * v) + (1-beta2)*g*g\n\n\t\t\tcached.Value, _ = anyToScalar(newM)\n\t\t\tcached.d, _ = anyToScalar(newV)\n\n\t\t\tmHat := (1 / correction1) * newM\n\t\t\tvHat := (1 / correction2) * newV\n\n\t\t\tupd := -eta * mHat / (math.Sqrt(vHat) + eps)\n\t\t\tw += upd\n\n\t\t\t*(weights.(*F64)) = F64(w)\n\t\t\t*(grad.(*F64)) = F64(0.0)\n\n\t\tdefault:\n\t\t\terr = errors.Errorf(nyiTypeFail, \"AdamSolver\", cvm)\n\t\t\treturn\n\t\t}\n\n\t}\n\treturn\n}\n\n// VanillaSolver is your bog standard stochastic gradient descent optimizer. There are no fancy features to this\ntype VanillaSolver struct {\n\teta   float64 // learn rate\n\tclip  float64 // clip gradients\n\tl1reg float64 // l1 regularization parameter\n\tl2reg float64 // l2 regularization parameter\n\tbatch float64 // batch size\n\n\tuseClip, useL1Reg, useL2Reg bool\n}\n\n// NewVanillaSolver creates a new VanillaSolver with sane-ish default values\nfunc NewVanillaSolver(opts ...SolverOpt) *VanillaSolver {\n\ts := &VanillaSolver{\n\t\tbatch: 1,\n\t\teta:   0.001,\n\t}\n\tfor _, opt := range opts {\n\t\topt(s)\n\t}\n\treturn s\n}\n\n// Step steps through each node in the model and applies the most basic gradient descent algorithm on the value.\n//\n// This function will error out if the nodes do not have an associated Grad value.\nfunc (s *VanillaSolver) Step(model []ValueGrad) (err error) {\n\tfor _, n := range model {\n\t\tvar weights, grad Value\n\t\tif weights, grad, err = extractWeightGrad(n); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tswitch w := weights.(type) {\n\t\tcase *tensor.Dense:\n\t\t\tg := grad.(*tensor.Dense)\n\n\t\t\tvar l1reg, l2reg, clip, negClip, eta interface{}\n\t\t\tvar onePerBatch interface{}\n\t\t\tswitch w.Dtype() {\n\t\t\tcase tensor.Float64:\n\t\t\t\tl1reg = s.l1reg\n\t\t\t\tl2reg = s.l2reg\n\t\t\t\tclip = s.clip\n\t\t\t\tnegClip = -s.clip\n\t\t\t\teta = -s.eta\n\t\t\t\tonePerBatch = float64(1) / s.batch\n\t\t\tcase tensor.Float32:\n\t\t\t\tl1reg = float32(s.l1reg)\n\t\t\t\tl2reg = float32(s.l2reg)\n\t\t\t\tclip = float32(s.clip)\n\t\t\t\tnegClip = float32(-s.clip)\n\t\t\t\teta = float32(-s.eta)\n\t\t\t\tonePerBatch = float32(1) / float32(s.batch)\n\t\t\t}\n\t\t\t// prep the regularization of gradients\n\t\t\tvar l1regs, l2regs tensor.Tensor\n\t\t\tif s.useL1Reg {\n\t\t\t\tif l1regs, err = tensor.Sign(w); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, signFail)\n\t\t\t\t}\n\n\t\t\t\tif l1regs, err = tensor.Mul(l1reg, l1regs, tensor.UseUnsafe()); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t\t}\n\n\t\t\t\tif _, err = tensor.Add(g, l1regs, tensor.UseUnsafe()); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, addFail)\n\t\t\t\t}\n\n\t\t\t\tdefer returnTensor(l1regs)\n\t\t\t}\n\n\t\t\tif s.useL2Reg {\n\t\t\t\tif l2regs, err = tensor.Mul(w, l2reg); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t\t}\n\n\t\t\t\tif _, err = tensor.Add(g, l2regs, tensor.UseUnsafe()); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, addFail)\n\t\t\t\t}\n\n\t\t\t\tdefer returnTensor(l2regs)\n\t\t\t}\n\n\t\t\tif s.batch > 1 {\n\t\t\t\tif _, err = tensor.Mul(g, onePerBatch, tensor.UseUnsafe()); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif s.useClip && s.clip > 0 {\n\t\t\t\tif _, err = tensor.Clamp(g, negClip, clip, tensor.UseUnsafe()); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, clampFail)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif _, err = tensor.Mul(g, eta, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t}\n\n\t\t\tif _, err = tensor.Add(w, g, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, addFail)\n\t\t\t}\n\n\t\t\tg.Zero()\n\n\t\tcase *F32:\n\t\t\tg := grad.(*F32).any()\n\t\t\twv := w.any()\n\n\t\t\tl1reg := float32(s.l1reg)\n\t\t\tl2reg := float32(s.l2reg)\n\t\t\tbatch := float32(s.batch)\n\t\t\tclip := float32(s.clip)\n\t\t\teta := float32(s.eta)\n\n\t\t\tif s.useL1Reg {\n\t\t\t\tif wv < 0 {\n\t\t\t\t\tl1reg = -l1reg\n\t\t\t\t}\n\t\t\t\tg += l1reg\n\t\t\t}\n\n\t\t\tif s.useL2Reg {\n\t\t\t\tl2reg *= wv\n\t\t\t\tg += l2reg\n\t\t\t}\n\n\t\t\tif batch > 1 {\n\t\t\t\tg *= (1 / batch)\n\t\t\t}\n\n\t\t\tif s.useClip {\n\t\t\t\tif g > clip {\n\t\t\t\t\tg = clip\n\t\t\t\t} else if g < -clip {\n\t\t\t\t\tg = -clip\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tupd := -eta * g\n\t\t\twv += upd\n\n\t\t\t*(weights.(*F32)) = F32(wv)\n\t\t\t*(grad.(*F32)) = F32(0.0)\n\t\tcase *F64:\n\t\t\tg := grad.(*F64).any()\n\t\t\twv := w.any()\n\n\t\t\tl1reg := s.l1reg\n\t\t\tl2reg := s.l2reg\n\t\t\tbatch := s.batch\n\t\t\tclip := s.clip\n\t\t\teta := s.eta\n\n\t\t\tif s.useL1Reg {\n\t\t\t\tif wv < 0 {\n\t\t\t\t\tl1reg = -l1reg\n\t\t\t\t}\n\t\t\t\tg += l1reg\n\t\t\t}\n\n\t\t\tif s.useL2Reg {\n\t\t\t\tl2reg *= wv\n\t\t\t\tg += l2reg\n\t\t\t}\n\n\t\t\tif batch > 1 {\n\t\t\t\tg *= (1 / batch)\n\t\t\t}\n\n\t\t\tif s.useClip {\n\t\t\t\tif g > clip {\n\t\t\t\t\tg = clip\n\t\t\t\t} else if g < -clip {\n\t\t\t\t\tg = -clip\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tupd := -eta * g\n\t\t\twv += upd\n\n\t\t\t*(weights.(*F64)) = F64(wv)\n\t\t\t*(grad.(*F64)) = F64(0.0)\n\t\tdefault:\n\t\t\treturn errors.Errorf(nyiFail, \"VanillaSolver.step\", w)\n\t\t}\n\t}\n\treturn\n}\n\n// Momentum is the stochastic gradient descent optimizer with momentum item.\ntype Momentum struct {\n\teta      float64 // learn rate\n\tmomentum float64 // momentum\n\tclip     float64 // clip gradients\n\tl1reg    float64 // l1 regularization parameter\n\tl2reg    float64 // l2 regularization parameter\n\tbatch    float64 // batch size\n\n\tuseClip, useL1Reg, useL2Reg bool\n\n\tcache []*dualValue\n}\n\n// NewMomentum creates a new Momentum with sane-ish default values\nfunc NewMomentum(opts ...SolverOpt) *Momentum {\n\ts := &Momentum{\n\t\tbatch:    1,\n\t\teta:      0.001,\n\t\tmomentum: 0.9,\n\t}\n\tfor _, opt := range opts {\n\t\topt(s)\n\t}\n\treturn s\n}\n\n// Step steps through each node in the model and applies the Momentum stochastic gradient descent algorithm on the value.\n//\n// This function will error out if the nodes do not have an associated Grad value.\nfunc (s *Momentum) Step(model []ValueGrad) (err error) {\n\tif s.cache == nil {\n\t\ts.cache = make([]*dualValue, len(model))\n\t}\n\n\tfor i, n := range model {\n\t\tvar weights, grad Value\n\t\tif weights, grad, err = extractWeightGrad(n); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tvar cached *dualValue\n\t\tif cached = s.cache[i]; cached == nil {\n\t\t\tif cached, err = newCachedDV(n, weights, grad, true); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\ts.cache[i] = cached\n\t\t}\n\n\t\tcv := cached.Value\n\t\t// cw = cw * momentum - eta * grad\n\t\t// w = w + cw\n\t\tswitch cw := cv.(type) {\n\t\tcase *tensor.Dense:\n\t\t\tw := weights.(*tensor.Dense)\n\t\t\tg := grad.(*tensor.Dense)\n\n\t\t\tvar l1reg, l2reg, clip, negClip, eta, momentum, onePerBatch interface{}\n\t\t\tswitch cw.Dtype() {\n\t\t\tcase tensor.Float64:\n\t\t\t\tl1reg = s.l1reg\n\t\t\t\tl2reg = s.l2reg\n\t\t\t\tclip = s.clip\n\t\t\t\tnegClip = -s.clip\n\t\t\t\teta = -s.eta\n\t\t\t\tmomentum = s.momentum\n\t\t\t\tonePerBatch = float64(1) / s.batch\n\t\t\tcase tensor.Float32:\n\t\t\t\tl1reg = float32(s.l1reg)\n\t\t\t\tl2reg = float32(s.l2reg)\n\t\t\t\tclip = float32(s.clip)\n\t\t\t\tnegClip = float32(-s.clip)\n\t\t\t\teta = float32(-s.eta)\n\t\t\t\tmomentum = float32(s.momentum)\n\t\t\t\tonePerBatch = float32(1) / float32(s.batch)\n\t\t\t}\n\n\t\t\t// prep the regularization of gradients\n\t\t\tvar l1regs, l2regs tensor.Tensor\n\t\t\tif s.useL1Reg {\n\t\t\t\tif l1regs, err = tensor.Sign(cw); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, signFail)\n\t\t\t\t}\n\n\t\t\t\tif l1regs, err = tensor.Mul(l1reg, l1regs, tensor.UseUnsafe()); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t\t}\n\n\t\t\t\tif _, err = tensor.Add(g, l1regs, tensor.UseUnsafe()); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, addFail)\n\t\t\t\t}\n\n\t\t\t\tdefer returnTensor(l1regs)\n\t\t\t}\n\n\t\t\tif s.useL2Reg {\n\t\t\t\tif l2regs, err = tensor.Mul(cw, l2reg); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t\t}\n\n\t\t\t\tif _, err = tensor.Add(g, l2regs, tensor.UseUnsafe()); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, addFail)\n\t\t\t\t}\n\n\t\t\t\tdefer returnTensor(l2regs)\n\t\t\t}\n\n\t\t\tif s.batch > 1 {\n\t\t\t\tif _, err = tensor.Mul(g, onePerBatch, tensor.UseUnsafe()); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif s.useClip && s.clip > 0 {\n\t\t\t\tif _, err = tensor.Clamp(g, negClip, clip, tensor.UseUnsafe()); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, clampFail)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// momentum\n\t\t\tif _, err = tensor.Mul(g, eta, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t}\n\n\t\t\t// cw * momentum\n\t\t\tif _, err = tensor.Mul(cw, momentum, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t}\n\n\t\t\t//  cw * momentum - eta * grad\n\t\t\tif _, err = tensor.Add(cw, g, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t}\n\n\t\t\tif _, err = tensor.Add(w, cw, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, addFail)\n\t\t\t}\n\n\t\t\tg.Zero()\n\n\t\tcase *F32:\n\t\t\tl1reg := float32(s.l1reg)\n\t\t\tl2reg := float32(s.l2reg)\n\t\t\tbatch := float32(s.batch)\n\t\t\tclip := float32(s.clip)\n\t\t\teta := float32(s.eta)\n\t\t\tmomentum := float32(s.momentum)\n\n\t\t\tg := grad.(*F32).any()\n\t\t\tw := weights.(*F32).any()\n\t\t\tc := cw.any()\n\n\t\t\tif s.useL1Reg {\n\t\t\t\tif w < 0 {\n\t\t\t\t\tl1reg = -l1reg\n\t\t\t\t}\n\t\t\t\tg += l1reg\n\t\t\t}\n\n\t\t\tif s.useL2Reg {\n\t\t\t\tl2reg *= w\n\t\t\t\tg += l2reg\n\t\t\t}\n\n\t\t\tif batch > 1 {\n\t\t\t\tg *= (1 / batch)\n\t\t\t}\n\n\t\t\tif s.useClip {\n\t\t\t\tif g > clip {\n\t\t\t\t\tg = clip\n\t\t\t\t} else if g < -clip {\n\t\t\t\t\tg = -clip\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tc = c*momentum - eta*g\n\t\t\tw += c\n\n\t\t\t*(weights.(*F32)) = F32(w)\n\t\t\t*(grad.(*F32)) = F32(0.0)\n\t\tcase *F64:\n\t\t\tl1reg := s.l1reg\n\t\t\tl2reg := s.l2reg\n\t\t\tbatch := s.batch\n\t\t\tclip := s.clip\n\t\t\teta := s.eta\n\t\t\tmomentum := s.momentum\n\n\t\t\tg := grad.(*F64).any()\n\t\t\tw := weights.(*F64).any()\n\t\t\tc := cw.any()\n\n\t\t\tif s.useL1Reg {\n\t\t\t\tif w < 0 {\n\t\t\t\t\tl1reg = -l1reg\n\t\t\t\t}\n\t\t\t\tg += l1reg\n\t\t\t}\n\n\t\t\tif s.useL2Reg {\n\t\t\t\tl2reg *= w\n\t\t\t\tg += l2reg\n\t\t\t}\n\n\t\t\tif batch > 1 {\n\t\t\t\tg *= (1 / batch)\n\t\t\t}\n\n\t\t\tif s.useClip {\n\t\t\t\tif g > clip {\n\t\t\t\t\tg = clip\n\t\t\t\t} else if g < -clip {\n\t\t\t\t\tg = -clip\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tc = c*momentum - eta*g\n\t\t\tw += c\n\n\t\t\t*(weights.(*F64)) = F64(w)\n\t\t\t*(grad.(*F64)) = F64(0.0)\n\t\tdefault:\n\t\t\treturn errors.Errorf(nyiFail, \"Momentum.step\", cv)\n\t\t}\n\t}\n\treturn\n}\n\n// AdaGradSolver is the solver that does adaptive gradient descent. Read the paper: http://jmlr.org/papers/v12/duchi11a.html\ntype AdaGradSolver struct {\n\teta   float64 // learn rate\n\teps   float64 // smoothing factor\n\tl1Reg float64 // l1reg param\n\tl2reg float64 // l2reg param\n\tclip  float64 // clip at\n\n\tuseL2Reg, useClip bool\n\n\tcache []*dualValue\n}\n\n// NewAdaGradSolver creates a new AdaGradSolver with sane-ish default values\nfunc NewAdaGradSolver(opts ...SolverOpt) *AdaGradSolver {\n\ts := &AdaGradSolver{\n\t\teta: 0.001,\n\t\teps: 1e-8,\n\t}\n\n\tfor _, opt := range opts {\n\t\topt(s)\n\t}\n\treturn s\n}\n\n// Step steps through each node in the model and applies the Adaptive Gradient gradient descent algorithm on the value.\n//\n// This function will error out if the nodes do not have an associated Grad value.\nfunc (s *AdaGradSolver) Step(model []ValueGrad) (err error) {\n\tif s.cache == nil {\n\t\ts.cache = make([]*dualValue, len(model))\n\t}\n\n\tfor i, n := range model {\n\t\tvar weights, grad Value\n\t\tif weights, grad, err = extractWeightGrad(n); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tvar cached *dualValue\n\t\tif cached = s.cache[i]; cached == nil {\n\t\t\tif cached, err = newCachedDV(n, weights, grad, true); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\ts.cache[i] = cached\n\t\t}\n\n\t\tcv := cached.Value\n\n\t\tswitch cw := cv.(type) {\n\t\tcase *tensor.Dense:\n\t\t\tvar w, g, c, g2, regularized tensor.Tensor\n\n\t\t\tvar l2reg, clip, negClip, eps, eta interface{}\n\t\t\tswitch cw.Dtype() {\n\t\t\tcase tensor.Float64:\n\t\t\t\tl2reg = s.l2reg\n\t\t\t\tclip = s.clip\n\t\t\t\tnegClip = -s.clip\n\t\t\t\teps = s.eps\n\t\t\t\teta = -s.eta\n\t\t\tcase tensor.Float32:\n\t\t\t\tl2reg = float32(s.l2reg)\n\t\t\t\tclip = float32(s.clip)\n\t\t\t\tnegClip = float32(-s.clip)\n\t\t\t\teps = float32(s.eps)\n\t\t\t\teta = float32(-s.eta)\n\t\t\t}\n\n\t\t\tg = grad.(*tensor.Dense)\n\t\t\tif g2, err = tensor.Square(g); err != nil {\n\t\t\t\treturn errors.Wrap(err, pointWiseSquareFail)\n\t\t\t}\n\n\t\t\tc = cw\n\t\t\ttensor.Add(c, g2, tensor.UseUnsafe())\n\t\t\tdefer returnTensor(g2)\n\n\t\t\tif s.useClip {\n\t\t\t\tif _, err = tensor.Clamp(g, negClip, clip, tensor.UseUnsafe()); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, clampFail)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// update\n\t\t\tvar upd tensor.Tensor\n\t\t\tif upd, err = tensor.Add(c, eps); err != nil {\n\t\t\t\treturn errors.Wrap(err, addFail)\n\t\t\t}\n\n\t\t\tif _, err = tensor.InvSqrt(upd, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, invSqrtFail)\n\t\t\t}\n\t\t\tif _, err = tensor.Mul(g, eta, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t}\n\n\t\t\tif _, err = tensor.Mul(upd, g, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t}\n\n\t\t\t// regularize\n\t\t\tw = weights.(*tensor.Dense)\n\n\t\t\tif s.useL2Reg {\n\t\t\t\tif regularized, err = tensor.Mul(w, l2reg); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t\t}\n\n\t\t\t\tif _, err = tensor.Sub(upd, regularized, tensor.UseUnsafe()); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, subFail)\n\t\t\t\t}\n\n\t\t\t\tdefer returnTensor(regularized)\n\t\t\t}\n\n\t\t\tif _, err = tensor.Add(w, upd, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, addFail)\n\t\t\t}\n\t\t\tdefer returnTensor(upd)\n\n\t\t\t// zero all\n\t\t\tg.Zero()\n\n\t\tcase *F32:\n\t\t\tvar w, g, c float32\n\n\t\t\tl2reg := float32(s.l2reg)\n\t\t\tclip := float32(s.clip)\n\t\t\teps := float32(s.eps)\n\t\t\teta := float32(s.eta)\n\n\t\t\tc = cw.any()\n\t\t\tg = grad.(*F32).any()\n\n\t\t\tc += g * g\n\n\t\t\tif s.useClip {\n\t\t\t\tif g > clip {\n\t\t\t\t\tg = clip\n\t\t\t\t} else if g < -clip {\n\t\t\t\t\tg = -clip\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tw = weights.(*F32).any()\n\n\t\t\tupd := -eta * g / math32.Sqrt(c+eps)\n\n\t\t\tif s.useL2Reg {\n\t\t\t\tupd -= w * l2reg\n\t\t\t}\n\n\t\t\tw += upd\n\n\t\t\t// because scalar values are copies, and not pointers, we have to actually re-update the dualValu in model[i]\n\t\t\t*(weights.(*F32)) = F32(w)\n\t\t\t*(grad.(*F32)) = F32(0.0)\n\t\tcase *F64:\n\t\t\tvar w, g, c float64\n\n\t\t\tl2reg := s.l2reg\n\t\t\tclip := s.clip\n\t\t\teps := s.eps\n\t\t\teta := s.eta\n\n\t\t\tc = cw.any()\n\t\t\tg = grad.(*F64).any()\n\n\t\t\tc += g * g\n\n\t\t\tif s.useClip {\n\t\t\t\tif g > clip {\n\t\t\t\t\tg = clip\n\t\t\t\t} else if g < -clip {\n\t\t\t\t\tg = -clip\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tw = weights.(*F64).any()\n\t\t\tupd := -eta * g / math.Sqrt(c+eps)\n\t\t\tif s.useL2Reg {\n\t\t\t\tupd -= w * l2reg\n\t\t\t}\n\n\t\t\tw += upd\n\n\t\t\t// because scalar values are copies, and not pointers, we have to actually re-update the dualValu in model[i]\n\t\t\t*(weights.(*F64)) = F64(w)\n\t\t\t*(grad.(*F64)) = F64(0.0)\n\n\t\tdefault:\n\t\t\treturn errors.Errorf(nyiFail, \"Adagrad step\", cv)\n\t\t}\n\n\t}\n\n\treturn\n}\n\n// BarzilaiBorweinSolver / Barzilai-Borwein performs Gradient Descent in steepest descend direction\n// Solves 0 = F(x), by\n//  xᵢ₊₁ = xᵢ - eta * Grad(F)(xᵢ)\n// Where the learn rate eta is calculated by the Barzilai-Borwein method:\n//  eta(xᵢ) = <(xᵢ - xᵢ₋₁), (Grad(F)(xᵢ) - Grad(F)(xᵢ₋₁))> /\n//                  ∥(Grad(F)(xᵢ) - Grad(F)(xᵢ₋₁))∥²\n// The input learn rate is used for the first iteration.\n//\n// TODO: Check out stochastic implementations, e.g. \"Barzilai-Borwein Step Size for Stochastic Gradient Descent\" https://arxiv.org/abs/1605.04131\ntype BarzilaiBorweinSolver struct {\n\teta     float64 // initial learn rate\n\tclip    float64 // clip value\n\tuseClip bool\n\tprevDV  []*dualValue // dual value for xᵢ₋₁ step\n}\n\n// NewBarzilaiBorweinSolver creates a new Barzilai-Borwein solver withs some default values:\n// the learn rate is set to 0.001 and the solver does not use clipping.\nfunc NewBarzilaiBorweinSolver(opts ...SolverOpt) *BarzilaiBorweinSolver {\n\ts := &BarzilaiBorweinSolver{\n\t\teta:     0.001,\n\t\tuseClip: false,\n\t}\n\n\tfor _, opt := range opts {\n\t\topt(s)\n\t}\n\treturn s\n}\n\n// Step steps through each node in the model and applies the Barzilai-Borwein gradient descent algorithm on the value.\n//\n// This function will error out if the nodes do not have an associated Grad value.\nfunc (s *BarzilaiBorweinSolver) Step(model []ValueGrad) (err error) {\n\n\tfirstRun := false\n\tif s.prevDV == nil {\n\t\tfirstRun = true\n\t\ts.prevDV = make([]*dualValue, len(model))\n\t}\n\n\t// Update the learning rate\n\tif false == firstRun {\n\t\tnominator := float64(0.0)\n\t\tdenominator := float64(0.0)\n\n\t\tfor nodeNr, node := range model {\n\t\t\tvar weights, grad Value\n\t\t\tif weights, grad, err = extractWeightGrad(node); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tswitch w := weights.(type) {\n\t\t\tcase *tensor.Dense:\n\t\t\t\tg, ok := grad.(*tensor.Dense)\n\t\t\t\tif !ok {\n\t\t\t\t\treturn errors.Errorf(\"Expected a *tensor.Dense in %v. Got %T instead\", node, grad)\n\t\t\t\t}\n\n\t\t\t\twOld, ok := s.prevDV[nodeNr].Value.(*tensor.Dense)\n\t\t\t\tif !ok {\n\t\t\t\t\treturn errors.Errorf(\"Expected a *tensor.Dense in %v. Got %T instead\", node, s.prevDV[nodeNr].Value)\n\t\t\t\t}\n\n\t\t\t\tgOld, ok := s.prevDV[nodeNr].d.(*tensor.Dense)\n\t\t\t\tif !ok {\n\t\t\t\t\treturn errors.Errorf(\"Expected a *tensor.Dense in %v. Got %T instead\", node, s.prevDV[nodeNr].d)\n\t\t\t\t}\n\n\t\t\t\tvalueDiff, err := tensor.Sub(w, wOld)\n\t\t\t\tdefer returnTensor(valueDiff)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errors.Wrap(err, subFail)\n\t\t\t\t}\n\n\t\t\t\tgradDiff, err := tensor.Sub(g, gOld)\n\t\t\t\tdefer returnTensor(gradDiff)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errors.Wrap(err, subFail)\n\t\t\t\t}\n\n\t\t\t\t// <(xᵢ - xᵢ₋₁), (Grad(F)(xᵢ) - Grad(F)(xᵢ₋₁))>\n\n\t\t\t\t// Scalar Product == Total tensor contraction\n\t\t\t\tdims := valueDiff.Dims()\n\t\t\t\tcontractionAxes := make([]int, dims, dims)\n\t\t\t\tfor axis := 0; axis < len(contractionAxes); axis++ {\n\t\t\t\t\tcontractionAxes[axis] = axis\n\t\t\t\t}\n\n\t\t\t\tvalGradDiffscalarProd, err := tensor.Contract(valueDiff, gradDiff, contractionAxes, contractionAxes)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errors.New(\"operationError, Contracting value / gradient difference\")\n\t\t\t\t}\n\t\t\t\tdefer returnTensor(valGradDiffscalarProd)\n\n\t\t\t\tnominator += valGradDiffscalarProd.Data().([]float64)[0]\n\n\t\t\t\t// ∥(Grad(F)(xᵢ) - Grad(F)(xᵢ₋₁))∥²\n\t\t\t\tgradDiffscalarProd, err := tensor.Contract(gradDiff, gradDiff, contractionAxes, contractionAxes)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errors.New(\"operationError, Contracting value / gradient difference\")\n\t\t\t\t}\n\t\t\t\tdefer returnTensor(gradDiffscalarProd)\n\n\t\t\t\tdenominator += gradDiffscalarProd.Data().([]float64)[0]\n\n\t\t\tdefault:\n\t\t\t\treturn errors.Errorf(nyiFail, \"Barizai-Borwein step\", w)\n\t\t\t}\n\t\t}\n\n\t\ts.eta = nominator / denominator\n\n\t\tif s.useClip && (math.Abs(s.eta) > s.clip) {\n\t\t\tif math.Signbit(s.eta) {\n\t\t\t\ts.eta = -s.clip\n\t\t\t} else {\n\t\t\t\ts.eta = s.clip\n\t\t\t}\n\t\t}\n\t}\n\n\t// Save this iteration's values for the next run\n\tfor nodeNr, node := range model {\n\t\tvar weights, grad Value\n\t\tif weights, grad, err = extractWeightGrad(node); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif false == firstRun {\n\t\t\t// return memory for the old dual value used in this iteration\n\t\t\treturnDV(s.prevDV[nodeNr])\n\t\t}\n\t\tvar oldDV *dualValue\n\t\tif oldDV, err = newCachedDV(node, weights, grad, false); err != nil {\n\t\t\treturn err\n\t\t}\n\t\ts.prevDV[nodeNr] = oldDV\n\t}\n\n\t// Update the weights\n\tfor _, node := range model {\n\t\tvar weights, grad Value\n\t\tif weights, grad, err = extractWeightGrad(node); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tswitch w := weights.(type) {\n\t\tcase *tensor.Dense:\n\t\t\tg, ok := grad.(*tensor.Dense)\n\t\t\tif !ok {\n\t\t\t\treturn errors.Errorf(\"Expected a *tensor.Dense in %v. Got %T instead\", node, grad)\n\t\t\t}\n\n\t\t\tupd, err := tensor.Mul(g, s.eta)\n\t\t\tdefer returnTensor(upd)\n\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t\t\t}\n\n\t\t\tif _, err = tensor.Sub(w, upd, tensor.UseUnsafe()); err != nil {\n\t\t\t\treturn errors.Wrap(err, subFail)\n\t\t\t}\n\n\t\t\tg.Zero()\n\n\t\tdefault:\n\t\t\treturn errors.Errorf(nyiFail, \"Barizai-Borwein step\", w)\n\t\t}\n\t}\n\n\treturn nil\n}\n\ntype adamwState struct {\n\texpMA   tensor.Tensor // exponential moving average\n\texpMASq tensor.Tensor\n\tdenom   tensor.Tensor // a temporary tensor used for computation\n}\n\n// AdamW is a Adam-like solver where the weight decay regularization is decoupled.\n// See also: https://arxiv.org/abs/1711.05101\ntype AdamW struct {\n\tη     float64 // learn rate\n\tε     float64 // smoothing\n\tλ     float64 // weight decay\n\tɛ     float64 // epsilon, a fudge factor\n\tβ1    float64\n\tβ2    float64\n\tclip  float64 // clip gradients to between -clip and +clip\n\tl1reg float64 // l1 regularization parameter\n\tl2reg float64 // l2 regularization parameter\n\tbatch float64 // batch size\n\n\tuseL1Reg, useL2Reg, useClip bool\n\n\t// unsettable\n\titer   float64\n\tstates map[*Node]*adamwState\n}\n\nfunc NewAdamW(opts ...SolverOpt) *AdamW {\n\ts := &AdamW{\n\t\tη:      0.001,\n\t\tɛ:      1e-8,\n\t\tλ:      0.01,\n\t\tβ1:     0.9,\n\t\tβ2:     0.999,\n\t\tstates: make(map[*Node]*adamwState),\n\t}\n\tfor _, opt := range opts {\n\t\topt(s)\n\t}\n\treturn s\n}\n\nfunc (a *AdamW) Step(model []ValueGrad) (err error) {\n\t/*\n\t In the rest of the algorithm, we will use the following symbols for explainersx:\n\t \tθ  - the parameter to be updated.\n\t        _t - at time step t\n\t        m  - the moving average (first moment)\n\t        v  - the second momennt (MA squared)\n\t*/\n\ta.iter++\n\tfor _, n := range model {\n\t\tn := n.(*Node)\n\t\tvar weights, grad Value\n\t\tif weights, grad, err = extractWeightGrad(n); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tw := weights.(tensor.Tensor)\n\t\tg := grad.(tensor.Tensor)\n\n\t\tst, ok := a.states[n]\n\t\tif !ok {\n\t\t\tst = new(adamwState)\n\t\t\tst.expMA = tensor.New(tensor.WithShape(grad.Shape().Clone()...), tensor.Of(grad.Dtype()))\n\t\t\tst.expMASq = tensor.New(tensor.WithShape(grad.Shape().Clone()...), tensor.Of(grad.Dtype()))\n\t\t\tst.denom = tensor.New(tensor.WithShape(grad.Shape().Clone()...), tensor.Of(grad.Dtype()))\n\t\t\ta.states[n] = st\n\t\t}\n\n\t\tvar decay, a1, a2, b1, b2, b2sqrt, ss, eps interface{}\n\t\tvar l1reg, l2reg, clip, negClip interface{}\n\t\tswitch weights.Dtype() {\n\t\tcase tensor.Float64:\n\t\t\tlr := a.η\n\t\t\twd := a.λ\n\t\t\tβ1 := a.β1\n\t\t\tβ2 := a.β2\n\t\t\tit := a.iter\n\t\t\teps = a.ɛ\n\t\t\tdecay = 1.0 - lr*wd\n\t\t\tb1f := 1.0 - math.Pow(β1, it) // correction for beta\n\t\t\tb2f := 1.0 - math.Pow(β2, it)\n\t\t\ta1 = 1.0 - b1f\n\t\t\ta2 = 1.0 - b2f // note here b2f is not sqrt'd\n\t\t\tb1 = b1f\n\t\t\tb2 = b2f\n\t\t\tb2sqrt = math.Sqrt(b2f)\n\t\t\tss = -(lr / b1f)\n\n\t\t\tl1reg = a.l1reg\n\t\t\tl2reg = a.l2reg\n\t\t\tclip = a.clip\n\t\t\tnegClip = -a.clip\n\t\tcase tensor.Float32:\n\t\t\tlr := float32(a.η)\n\t\t\twd := float32(a.λ)\n\t\t\tβ1 := float32(a.β1)\n\t\t\tβ2 := float32(a.β2)\n\t\t\tit := float32(a.iter)\n\t\t\teps = float32(a.ɛ)\n\n\t\t\tdecay = float32(1.0) - lr*wd\n\t\t\tb1f := float32(1.0) - math32.Pow(β1, it) // correction for beta\n\t\t\tb2f := float32(1.0) - math32.Pow(β2, it)\n\t\t\ta1 = float32(1.0) - b1f\n\t\t\ta2 = float32(1.0) - b2f // note here b2f is not sqrt'd\n\t\t\tb1 = b1f\n\t\t\tb2 = b2f\n\t\t\tb2sqrt = math32.Sqrt(b2f)\n\t\t\tss = -(lr / b1f)\n\n\t\t\tl1reg = a.l1reg\n\t\t\tl2reg = a.l2reg\n\t\t\tclip = a.clip\n\t\t\tnegClip = -a.clip\n\n\t\t}\n\t\t// regularization of gradients\n\n\t\tif a.useL1Reg {\n\t\t\tif err = doL1Reg(w, g, l1reg); err != nil {\n\t\t\t\treturn errors.Wrapf(err, \"Failed to perform L1 regularization on the gradients of %v\", n)\n\t\t\t}\n\t\t}\n\t\tif a.useL2Reg {\n\t\t\tif err = doL2Reg(w, g, l2reg); err != nil {\n\t\t\t\treturn errors.Wrapf(err, \"Failed to perform L2 regularization on the gradients of %v\", n)\n\t\t\t}\n\t\t}\n\t\tif a.batch > 1 {\n\t\t\tif err = divBatch(g, a.batch); err != nil {\n\t\t\t\treturn errors.Wrapf(err, \"Failed to divide gradients by batch count of %v\", n)\n\t\t\t}\n\t\t}\n\t\tif a.useClip && a.clip > 0 {\n\t\t\tif err = clipGrad(g, clip, negClip); err != nil {\n\t\t\t\treturn errors.Wrapf(err, \"Failed to clip gradients of %v to between %v and %v\", n, clip, negClip)\n\t\t\t}\n\t\t}\n\n\t\tvar gSq tensor.Tensor\n\n\t\t// θ_t =  (1 - ηλ)θ_t-1\n\t\tif w, err = tensor.Mul(w, decay); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// m_t = = β_1*m_t-1 + (1-β_1)g\n\t\tif st.expMA, err = tensor.Mul(st.expMA, b1, tensor.UseUnsafe()); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif _, err = tensor.Mul(g, a1, tensor.WithIncr(st.expMA)); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// v_t = β_2*v_t-1 + (1 - β_2)g²\n\t\tif st.expMASq, err = tensor.Mul(st.expMASq, b2, tensor.UseUnsafe()); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif gSq, err = tensor.Mul(g, g, tensor.UseUnsafe()); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif _, err = tensor.Mul(gSq, a2, tensor.WithIncr(st.expMASq)); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif st.denom, err = tensor.Sqrt(st.expMASq, tensor.WithReuse(st.denom)); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif st.denom, err = tensor.Div(st.denom, b2sqrt, tensor.UseUnsafe()); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif st.denom, err = tensor.Add(st.denom, eps, tensor.UseUnsafe()); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif st.denom, err = tensor.Div(st.expMA, st.denom, tensor.WithReuse(st.denom)); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif w, err = tensor.Mul(st.denom, ss, tensor.WithIncr(w)); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tg.(tensor.Tensor).Zero()\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "solvers_test.go",
          "type": "blob",
          "size": 11.8837890625,
          "content": "package gorgonia\n\nimport (\n\t\"math\"\n\t\"runtime\"\n\t\"testing\"\n\n\t\"github.com/chewxy/math32\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n\t\"gorgonia.org/dawson\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc clampFloat64(v, min, max float64) float64 {\n\tif v < min {\n\t\treturn min\n\t}\n\tif v > max {\n\t\treturn max\n\t}\n\treturn v\n}\n\nfunc clampFloat32(v, min, max float32) float32 {\n\tif v < min {\n\t\treturn min\n\t}\n\tif v > max {\n\t\treturn max\n\t}\n\treturn v\n}\n\nfunc tf64Node() []ValueGrad {\n\tbackingV := []float64{1, 2, 3, 4}\n\tbackingD := []float64{0.5, -10, 10, 0.5}\n\tv := tensor.New(tensor.WithBacking(backingV), tensor.WithShape(2, 2))\n\td := tensor.New(tensor.WithBacking(backingD), tensor.WithShape(2, 2))\n\n\tdv := dvUnit0(v)\n\tdv.d = d\n\n\tn := new(Node)\n\tn.boundTo = dv\n\n\tmodel := []ValueGrad{n}\n\treturn model\n}\n\nfunc tf32Node() []ValueGrad {\n\tbackingV := []float32{1, 2, 3, 4}\n\tbackingD := []float32{0.5, -10, 10, 0.5}\n\n\tv := tensor.New(tensor.WithBacking(backingV), tensor.WithShape(2, 2))\n\td := tensor.New(tensor.WithBacking(backingD), tensor.WithShape(2, 2))\n\n\tdv := dvUnit0(v)\n\tdv.d = d\n\n\tn := new(Node)\n\tn.boundTo = dv\n\n\tmodel := []ValueGrad{n}\n\treturn model\n}\n\nfunc manualRMSProp64(t *testing.T, s *RMSPropSolver, model []ValueGrad) {\n\tassert := assert.New(t)\n\tcorrect := make([]float64, 4)\n\tcached := make([]float64, 4)\n\n\tgrad0, _ := model[0].Grad()\n\tbackingV := model[0].Value().Data().([]float64)\n\tbackingD := grad0.Data().([]float64)\n\n\tfor i := 0; i < 5; i++ {\n\t\tfor j, v := range backingV {\n\t\t\tgrad := backingD[j]\n\t\t\tcw := cached[j]\n\n\t\t\tdecayed := cw*s.decay + (1.0-s.decay)*grad*grad\n\t\t\tcached[j] = decayed\n\n\t\t\tgrad = clampFloat64(grad, -s.clip, s.clip)\n\t\t\tupd := -s.eta*grad/math.Sqrt(decayed+s.eps) - s.l2reg*v\n\t\t\tcorrect[j] = v + upd\n\t\t}\n\n\t\terr := s.Step(model)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\n\t\tsCache := s.cache[0].Value.(tensor.Tensor)\n\t\tassert.Equal(correct, backingV, \"Iteration: %d\", i)\n\t\tassert.Equal(cached, sCache.Data(), \"Iteration: %d\", i)\n\n\t}\n}\n\nfunc manualRMSProp32(t *testing.T, s *RMSPropSolver, model []ValueGrad) {\n\tassert := assert.New(t)\n\tcorrect := make([]float32, 4)\n\tcached := make([]float32, 4)\n\n\tgrad0, _ := model[0].Grad()\n\tbackingV := model[0].Value().Data().([]float32)\n\tbackingD := grad0.Data().([]float32)\n\n\tdecay := float32(s.decay)\n\tl2reg := float32(s.l2reg)\n\teta := float32(s.eta)\n\teps := float32(s.eps)\n\tclip := float32(s.clip)\n\n\t// NOTE: THIS IS NAUGHTY. A proper comparison using 1e-5  should be used but that causes errors.\n\tclosef32 := func(a, b float32) bool {\n\t\treturn dawson.ToleranceF32(a, b, 1e-4)\n\t}\n\n\tfor i := 0; i < 5; i++ {\n\t\tfor j, v := range backingV {\n\t\t\tgrad := backingD[j]\n\t\t\tcw := cached[j]\n\n\t\t\tdecayed := cw*decay + (1.0-decay)*grad*grad\n\t\t\tcached[j] = decayed\n\n\t\t\tgrad = clampFloat32(grad, -clip, clip)\n\t\t\tupd := -eta*grad/math32.Sqrt(decayed+eps) - l2reg*v\n\t\t\tcorrect[j] = v + upd\n\t\t}\n\n\t\terr := s.Step(model)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\n\t\tsCache := s.cache[0].Value.(tensor.Tensor)\n\t\tassert.True(dawson.AllClose(correct, backingV, closef32))\n\t\tassert.True(dawson.AllClose(cached, sCache.Data().([]float32), closef32))\n\t}\n}\n\nfunc TestRMSPropSolverManual(t *testing.T) {\n\n\tstepSize := 0.01\n\tl2Reg := 0.000001\n\tclip := 5.0\n\n\tvar s *RMSPropSolver\n\tvar model []ValueGrad\n\n\ts = NewRMSPropSolver(WithLearnRate(stepSize), WithL2Reg(l2Reg), WithClip(clip))\n\tmodel = tf64Node()\n\tmanualRMSProp64(t, s, model)\n\n\ts = NewRMSPropSolver(WithLearnRate(stepSize), WithL2Reg(l2Reg), WithClip(clip))\n\tmodel = tf32Node()\n\tmanualRMSProp32(t, s, model)\n\n}\n\nfunc TestRMSPropSolver(t *testing.T) {\n\tassert := assert.New(t)\n\n\tz, cost, m, err := model2dRosenbrock(1, 100, -0.5, 0.5)\n\tdefer m.Close()\n\tconst costThreshold = 0.68\n\tif nil != err {\n\t\tt.Fatal(err)\n\t}\n\n\tsolver := NewRMSPropSolver()\n\n\tmaxIterations := 1000\n\n\tcostFloat := 42.0\n\tfor 0 != maxIterations {\n\t\tm.Reset()\n\t\terr = m.RunAll()\n\t\tif nil != err {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tcostFloat = cost.Value().Data().(float64)\n\t\tif costThreshold > math.Abs(costFloat) {\n\t\t\tbreak\n\t\t}\n\n\t\terr = solver.Step([]ValueGrad{z})\n\t\tif nil != err {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tmaxIterations--\n\t}\n\n\tassert.InDelta(0, costFloat, costThreshold)\n}\n\nfunc TestAdaGradSolver(t *testing.T) {\n\tassert := assert.New(t)\n\n\tz, cost, m, err := model2dSquare(-0.5, 0.5)\n\tdefer m.Close()\n\tconst costThreshold = 0.39\n\tif nil != err {\n\t\tt.Fatal(err)\n\t}\n\n\tsolver := NewAdaGradSolver()\n\n\tmaxIterations := 1000\n\n\tcostFloat := 42.0\n\tfor 0 != maxIterations {\n\t\tm.Reset()\n\t\terr = m.RunAll()\n\t\tif nil != err {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tcostFloat = cost.Value().Data().(float64)\n\t\tif costThreshold > math.Abs(costFloat) {\n\t\t\tbreak\n\t\t}\n\n\t\terr = solver.Step([]ValueGrad{z})\n\t\tif nil != err {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tmaxIterations--\n\t}\n\n\tassert.InDelta(0, costFloat, costThreshold)\n}\n\nfunc TestVanillaSolver(t *testing.T) {\n\tassert := assert.New(t)\n\n\tz, cost, m, err := model2dRosenbrock(1, 100, -0.5, 0.5)\n\tdefer m.Close()\n\tconst costThreshold = 0.185\n\tif nil != err {\n\t\tt.Fatal(err)\n\t}\n\n\tsolver := NewVanillaSolver()\n\n\tmaxIterations := 1000\n\n\tcostFloat := 42.0\n\tfor 0 != maxIterations {\n\t\tm.Reset()\n\t\terr = m.RunAll()\n\t\tif nil != err {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tcostFloat = cost.Value().Data().(float64)\n\t\tif costThreshold > math.Abs(costFloat) {\n\t\t\tbreak\n\t\t}\n\n\t\terr = solver.Step([]ValueGrad{z})\n\t\tif nil != err {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tmaxIterations--\n\t}\n\n\tassert.InDelta(0, costFloat, costThreshold)\n}\n\nfunc TestMomentum(t *testing.T) {\n\tassert := assert.New(t)\n\n\tz, cost, m, err := model2dRosenbrock(1, 100, -0.5, 0.5)\n\tdefer m.Close()\n\tconst costThreshold = 0.39\n\tif nil != err {\n\t\tt.Fatal(err)\n\t}\n\n\tsolver := NewMomentum()\n\n\tmaxIterations := 1000\n\n\tcostFloat := 42.0\n\tfor 0 != maxIterations {\n\t\tm.Reset()\n\t\terr = m.RunAll()\n\t\tif nil != err {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tcostFloat = cost.Value().Data().(float64)\n\t\tif costThreshold > math.Abs(costFloat) {\n\t\t\tbreak\n\t\t}\n\n\t\terr = solver.Step([]ValueGrad{z})\n\t\tif nil != err {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tmaxIterations--\n\t}\n\n\tassert.InDelta(0, costFloat, costThreshold)\n}\n\nfunc TestAdamSolver(t *testing.T) {\n\tassert := assert.New(t)\n\n\tz, cost, m, err := model2dRosenbrock(1, 100, -0.5, 0.5)\n\tdefer m.Close()\n\tconst costThreshold = 0.113\n\tif nil != err {\n\t\tt.Fatal(err)\n\t}\n\n\tsolver := NewAdamSolver(WithLearnRate(0.1))\n\n\tmaxIterations := 1000\n\n\tcostFloat := 42.0\n\tfor 0 != maxIterations {\n\t\tm.Reset()\n\t\terr = m.RunAll()\n\t\tif nil != err {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tcostFloat = cost.Value().Data().(float64)\n\t\tif costThreshold > math.Abs(costFloat) {\n\t\t\tbreak\n\t\t}\n\n\t\terr = solver.Step([]ValueGrad{z})\n\t\tif nil != err {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tmaxIterations--\n\t}\n\n\tassert.InDelta(0, costFloat, costThreshold)\n}\n\nfunc TestAdamSolverPrecision(t *testing.T) {\n\ttestCases := []struct {\n\t\tdesc           string\n\t\tlearnRate      float64\n\t\tinputStart     float32\n\t\tinputEnd       float32\n\t\tinputIncrement float32\n\t\tsize           int\n\t\tdtype          tensor.Dtype\n\t\texpectedOutput interface{}\n\t}{\n\t\t{\n\t\t\tdesc:           \"Example-float32-1\",\n\t\t\tlearnRate:      0.1,\n\t\t\tinputStart:     0.0,\n\t\t\tinputEnd:       1.0,\n\t\t\tinputIncrement: 0.1,\n\t\t\tsize:           4,\n\t\t\tdtype:          tensor.Float32,\n\t\t\texpectedOutput: []float32{0.18293014, 0.18293014, 0.18293014, 0.18293014},\n\t\t},\n\t\t{\n\t\t\tdesc:           \"Example-float64-1\",\n\t\t\tlearnRate:      0.1,\n\t\t\tinputStart:     0.0,\n\t\t\tinputEnd:       1.0,\n\t\t\tinputIncrement: 0.1,\n\t\t\tsize:           8,\n\t\t\tdtype:          tensor.Float64,\n\t\t\texpectedOutput: []float64{0.18293561851374684, 0.18293561851374684, 0.18293561851374684, 0.18293561851374684, 0.18293561851374684, 0.18293561851374684, 0.18293561851374684, 0.18293561851374684},\n\t\t},\n\t}\n\tfor _, tC := range testCases {\n\t\tt.Run(tC.desc, func(t *testing.T) {\n\t\t\tc := require.New(t)\n\t\t\tg := NewGraph()\n\n\t\t\tweights := NewTensor(g, tC.dtype, 2, WithShape(tC.size, 1), WithInit(Ones()), WithName(\"weights\"))\n\t\t\tinput := NewTensor(g, tC.dtype, 2, WithShape(1, tC.size), WithName(\"x\"))\n\n\t\t\tfc := Must(Mul(input, weights))\n\t\t\tloss := Must(Mean(fc))\n\n\t\t\t_, err := Grad(loss, weights)\n\t\t\tc.NoError(err)\n\n\t\t\tsolver := NewAdamSolver(WithLearnRate(tC.learnRate))\n\t\t\tvm := NewTapeMachine(g, BindDualValues(weights))\n\n\t\t\tfor d := tC.inputStart; d < tC.inputEnd; d += tC.inputIncrement {\n\t\t\t\tvar backing interface{}\n\n\t\t\t\tif tC.dtype == tensor.Float32 {\n\t\t\t\t\tarr := make([]float32, tC.size)\n\t\t\t\t\tfor i := range arr {\n\t\t\t\t\t\tarr[i] = float32(d)\n\t\t\t\t\t}\n\n\t\t\t\t\tbacking = arr\n\t\t\t\t} else {\n\t\t\t\t\tarr := make([]float64, tC.size)\n\t\t\t\t\tfor i := range arr {\n\t\t\t\t\t\tarr[i] = float64(d)\n\t\t\t\t\t}\n\n\t\t\t\t\tbacking = arr\n\t\t\t\t}\n\n\t\t\t\tLet(input, tensor.New(\n\t\t\t\t\ttensor.WithShape(1, tC.size),\n\t\t\t\t\ttensor.WithBacking(backing),\n\t\t\t\t))\n\t\t\t\tc.NoError(vm.RunAll())\n\n\t\t\t\tc.NoError(solver.Step([]ValueGrad{weights}))\n\n\t\t\t\tvm.Reset()\n\t\t\t}\n\n\t\t\tc.True(dawson.AllClose(tC.expectedOutput, weights.Value().Data()))\n\t\t})\n\t}\n}\n\nfunc TestBarzilaiBorweinSolver(t *testing.T) {\n\tassert := assert.New(t)\n\n\tz, cost, m, err := model2dRosenbrock(1, 100, -0.5, 0.5)\n\tdefer m.Close()\n\tconst costThreshold = 0.00002\n\tif nil != err {\n\t\tt.Fatal(err)\n\t}\n\n\tsolver := NewBarzilaiBorweinSolver(WithLearnRate(0.0001))\n\titerations := 0\n\tcostFloat := 42.0\n\n\t// NOTE: due to precision issues with floating-point arithmetic,\n\t// amd64 reaches the minimum expected cost at iteration #198\n\t// arm64 reaches the minimum expected cost at iteration #210\n\t// In some other cases arm converges faster than amd\n\t// See https://github.com/golang/go/issues/18354#issuecomment-267705645\n\n\tfor iterations < 250 {\n\t\tm.Reset()\n\t\terr = m.RunAll()\n\t\tif nil != err {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tcostFloat = cost.Value().Data().(float64)\n\t\tif costThreshold > math.Abs(costFloat) {\n\t\t\tbreak\n\t\t}\n\n\t\terr = solver.Step([]ValueGrad{z})\n\t\tif nil != err {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\titerations++\n\t}\n\n\tt.Logf(\"Found minimum cost at iteration %d. arch=%s\", iterations, runtime.GOARCH)\n\n\tassert.InDelta(0, costFloat, costThreshold)\n}\n\n// The Rosenbrock function is a non-convex function,\n// which is used as a performance test problem for optimization algorithms.\n// https://en.wikipedia.org/wiki/Rosenbrock_function\n//\n// f(x,y) = (a-x)² + b(y-x²)²\n// It has a global minimum at (x, y) = (a, a²), where f(x,y) = 0.\n// Usually a = 1, b = 100, then the minimum is at x = y = 1\n// TODO: There is also an n-dimensional version...see wiki\nfunc model2dRosenbrock(a, b, xInit, yInit float64) (z, cost *Node, machine *tapeMachine, err error) {\n\tg := NewGraph()\n\n\tz = NewTensor(g, Float64, 1, WithShape(2), WithName(\"z\"))\n\n\taN := NewConstant(a, WithName(\"a\"))\n\tbN := NewConstant(b, WithName(\"b\"))\n\n\txProjFloat := []float64{1, 0}\n\txProj := NewConstant(tensor.New(tensor.WithBacking(xProjFloat), tensor.WithShape(2)))\n\n\tyProjFloat := []float64{0, 1}\n\tyProj := NewConstant(tensor.New(tensor.WithBacking(yProjFloat), tensor.WithShape(2)))\n\n\tx := Must(Mul(z, xProj))\n\ty := Must(Mul(z, yProj))\n\n\t// First term\n\n\tsqrt1stTerm := Must(Sub(aN, x))\n\n\tfirstTerm := Must(Square(sqrt1stTerm))\n\n\t// Second term\n\n\txSquared := Must(Square(x))\n\n\tyMinusxSquared := Must(Sub(y, xSquared))\n\n\tyMinusxSquaredSqu := Must(Square(yMinusxSquared))\n\n\tsecondTerm := Must(Mul(bN, yMinusxSquaredSqu))\n\n\t// cost\n\tcost = Must(Add(firstTerm, secondTerm))\n\n\tdcost, err := Grad(cost, z)\n\tif nil != err {\n\t\treturn nil, nil, nil, err\n\t}\n\n\tprog, locMap, err := CompileFunction(g, Nodes{z}, Nodes{cost, dcost[0]})\n\tif nil != err {\n\t\treturn nil, nil, nil, err\n\t}\n\n\tmachine = NewTapeMachine(g, WithPrecompiled(prog, locMap), BindDualValues(z))\n\n\terr = machine.Let(z, tensor.New(tensor.WithBacking([]float64{xInit, yInit}), tensor.WithShape(2)))\n\tif nil != err {\n\t\treturn nil, nil, nil, err\n\t}\n\n\treturn\n}\n\nfunc model2dSquare(xInit, yInit float64) (z, cost *Node, machine *tapeMachine, err error) {\n\tg := NewGraph()\n\n\tz = NewTensor(g, Float64, 1, WithShape(2), WithName(\"z\"))\n\n\t// cost\n\tcost = Must(Mul(z, z))\n\n\tdcost, err := Grad(cost, z)\n\tif nil != err {\n\t\treturn nil, nil, nil, err\n\t}\n\n\tprog, locMap, err := CompileFunction(g, Nodes{z}, Nodes{cost, dcost[0]})\n\tif nil != err {\n\t\treturn nil, nil, nil, err\n\t}\n\n\tmachine = NewTapeMachine(g, WithPrecompiled(prog, locMap), BindDualValues(z))\n\n\terr = machine.Let(z, tensor.New(tensor.WithBacking([]float64{xInit, yInit}), tensor.WithShape(2)))\n\tif nil != err {\n\t\treturn nil, nil, nil, err\n\t}\n\n\treturn\n}\n"
        },
        {
          "name": "solvers_utils.go",
          "type": "blob",
          "size": 1.4462890625,
          "content": "package gorgonia\n\nimport (\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\n// this file provides utility functions for solvers\n\nfunc doL1Reg(w, g tensor.Tensor, l1reg interface{}) (err error) {\n\tvar l1regs tensor.Tensor\n\tif l1regs, err = tensor.Sign(w); err != nil {\n\t\terrors.Wrap(err, signFail)\n\t}\n\tif _, err = tensor.Mul(l1reg, l1regs, tensor.WithIncr(g)); err != nil {\n\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t}\n\tdefer returnTensor(l1regs)\n\treturn nil\n}\n\nfunc doL2Reg(w, g tensor.Tensor, l2reg interface{}) (err error) {\n\tif _, err = tensor.Mul(w, l2reg, tensor.WithIncr(g)); err != nil {\n\t\treturn errors.Wrap(err, pointWiseMulFail)\n\t}\n\treturn nil\n}\n\nfunc computeRecip(x float64, as tensor.Dtype) (retVal interface{}, err error) {\n\tswitch as {\n\tcase tensor.Float64:\n\t\treturn 1.0 / x, nil\n\tcase tensor.Float32:\n\t\treturn float32(1.0) / float32(x), nil\n\tdefault:\n\t\treturn 0.0, errors.Errorf(\"Unhandled Dtype %v for computeRecip\", as)\n\t}\n}\n\nfunc divBatch(g tensor.Tensor, batch float64) (err error) {\n\trecip, err := computeRecip(batch, g.Dtype())\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"In divBatch()\")\n\t}\n\n\t_, err = tensor.Mul(g, recip, tensor.UseUnsafe())\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"Cannot multiply by reciprocal of batch count\")\n\t}\n\treturn nil\n}\n\nfunc clipGrad(g tensor.Tensor, clip, negClip interface{}) (err error) {\n\tif _, err = tensor.Clamp(g, negClip, clip, tensor.UseUnsafe()); err != nil {\n\t\treturn errors.Wrap(err, clampFail)\n\t}\n\treturn nil\n}\n"
        },
        {
          "name": "stabilization.go",
          "type": "blob",
          "size": 7.3837890625,
          "content": "package gorgonia\n\nimport \"github.com/pkg/errors\"\n\nvar unaryOpStabilizationFns = make(map[ʘUnaryOperatorType][]func(*Node) (*Node, error))\nvar binOpStabilizationFns = make(map[ʘBinaryOperatorType][]func(*Node, *Node) (*Node, error))\n\nfunc init() {\n\tunaryOpStabilizationFns[lnOpType] = []func(*Node) (*Node, error){\n\t\tlogSigmoidStabilization,\n\t\tlogStabilization,\n\t\tlogSoftmaxStabilization,\n\t}\n\tbinOpStabilizationFns[subOpType] = []func(*Node, *Node) (*Node, error){\n\t\texp1mStabilization,\n\t\toneMinusSigmoidStabilization,\n\t}\n\tunaryOpStabilizationFns[log1pOpType] = []func(*Node) (*Node, error){\n\t\tlog1pExpStabilization,\n\t\tlog1pNegSigmoidStabilization,\n\t}\n\tunaryOpStabilizationFns[negOpType] = []func(*Node) (*Node, error){negNegOptimization}\n}\n\n// logStabilization converts\n// \tlog(1+a) or log(a+1) to log1p(a)\n//\tlog(1-a) to log1p(-a)\n// place before log; a should be positive.\nfunc logStabilization(a *Node) (retVal *Node, err error) {\n\tstabLogf(\"Stabilizing log(1+a) of %v\", a)\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tvar x *Node\n\tvar aop elemBinOp\n\tvar ok bool\n\n\tif aop, ok = a.op.(elemBinOp); !ok {\n\t\treturn a, noStabilizationErr{}\n\t}\n\tinput0 := a.children[0]\n\tinput1 := a.children[1]\n\n\tstabLogf(\"input0: %v\", input0.Name())\n\tstabLogf(\"input1: %v\", input1.Name())\n\tbot := aop.ʘBinaryOperator.binOpType()\n\tswitch bot {\n\tcase addOpType:\n\t\tif cnst, ok := input0.op.(constant); ok {\n\t\t\tif constEq(cnst, onef32ConstOp) || constEq(cnst, onef64ConstOp) {\n\t\t\t\tx = input1\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\tif cnst, ok := input1.op.(constant); ok {\n\t\t\tif constEq(cnst, onef32ConstOp) || constEq(cnst, onef64ConstOp) {\n\t\t\t\tx = input0\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\treturn a, noStabilizationErr{}\n\tcase subOpType:\n\t\tif cnst, ok := input0.op.(constant); !ok || (ok && !constEq(cnst, onef32ConstOp) && !constEq(cnst, onef64ConstOp)) {\n\t\t\treturn a, noStabilizationErr{}\n\t\t}\n\t\tx = input1\n\tdefault:\n\t\treturn a, noStabilizationErr{}\n\t}\n\n\tg := a.g\n\tg.removeAllEdgesFrom(a) // remove all references\n\tg.RemoveNode(a)\n\tdefer returnNode(a) // send it back to the pool, since it is literally useless now\n\n\tif bot == subOpType {\n\t\tif retVal, err = Neg(x); err == nil {\n\t\t\treturn Log1p(retVal)\n\t\t}\n\t\treturn nil, errors.Wrap(err, negFail)\n\t}\n\treturn Log1p(x)\n}\n\n// expStabilization converts exp(x)-1 to expm1(x)\n// place before sub; i0 should be exp(x); i1 should be 1\nfunc exp1mStabilization(a, b *Node) (retVal *Node, err error) {\n\tstabLogf(\"Stabilizing exp(x)-1 to expm1(x) of %v and %v\", a, b)\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tif cnst, ok := b.op.(constant); !ok || (ok && !constEq(cnst, onef32ConstOp) && !constEq(cnst, onef64ConstOp)) {\n\t\treturn nil, noStabilizationErr{}\n\t}\n\n\tif euo, ok := a.op.(elemUnaryOp); !ok || euo.unaryOpType() != expOpType {\n\t\treturn nil, noStabilizationErr{}\n\t}\n\n\top := newElemUnaryOp(expm1OpType, a.children[0])\n\treturn ApplyOp(op, a.children[0])\n}\n\n// oneMinusSigmoidStabilization stabilizes 1-sigmoid(x) by replacing it with sigmoid(-x)\n// place before sub\nfunc oneMinusSigmoidStabilization(a, b *Node) (retVal *Node, err error) {\n\tstabLogf(\"Stabilizing 1-sigmoid(x) to sigmoid(-x) of %v and %v\", a, b)\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tif cnst, ok := a.op.(constant); !ok || (ok && !constEq(cnst, onef32ConstOp) && !constEq(cnst, onef64ConstOp)) {\n\t\treturn nil, noStabilizationErr{}\n\t}\n\n\tif euo, ok := b.op.(elemUnaryOp); !ok || euo.unaryOpType() != sigmoidOpType {\n\t\treturn nil, noStabilizationErr{}\n\t}\n\n\tx := b.children[0]\n\tif retVal, err = Neg(x); err == nil {\n\t\treturn Sigmoid(retVal)\n\t}\n\treturn nil, errors.Wrap(err, negFail)\n}\n\n// logSigmoidStabilization stabilizes log(sigmoid(x)) by replacing it with -softplus(-x)\n// place before log; a should be sigmoid(x)\nfunc logSigmoidStabilization(a *Node) (retVal *Node, err error) {\n\tstabLogf(\"Stabilizing log sigmoid of %v\", a)\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tif euo, ok := a.op.(elemUnaryOp); !ok || (ok && euo.unaryOpType() != sigmoidOpType) {\n\t\treturn a, noStabilizationErr{}\n\t}\n\n\tx := a.children[0]\n\tstabLogf(\"x : %v\", x.Name())\n\n\tif retVal, err = Neg(x); err == nil {\n\t\tif retVal, err = Softplus(retVal); err == nil {\n\t\t\tretVal, err = Neg(retVal)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, negFail)\n\t\t\t}\n\t\t\treturn retVal, nil\n\t\t}\n\t\treturn nil, errors.Wrap(err, softplusFail)\n\t}\n\treturn nil, errors.Wrap(err, negFail)\n}\n\n// log1pExpStabilization stabilizes log1p(exp(x)) by substituting it with softplus(x)\n// place before log1p; a should be exp(x)\nfunc log1pExpStabilization(a *Node) (retVal *Node, err error) {\n\tstabLogf(\"Stabilizing log1p(exp(x)) of %v\", a)\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tif euo, ok := a.op.(elemUnaryOp); !ok || (ok && euo.unaryOpType() != expOpType) {\n\t\tstabLogf(\"op: %v; %v\", a.op, a.children)\n\t\treturn a, noStabilizationErr{}\n\t}\n\n\tx := a.children[0]\n\tstabLogf(\"OKKKKK\")\n\treturn Softplus(x)\n}\n\n// log1pNegSigmoidStabilization stabilizes log1p(-sigmoid(x)) by substituting it with -softplus(x)\n// place before log1p;  a should be -sigmoid(x)\nfunc log1pNegSigmoidStabilization(a *Node) (retVal *Node, err error) {\n\tstabLogf(\"Stabilizing log1p(-sigmoid(x)) : %v\", a)\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tif euo, ok := a.op.(elemUnaryOp); !ok || (ok && euo.unaryOpType() != negOpType) {\n\t\treturn a, noStabilizationErr{}\n\t}\n\n\tif euo, ok := a.children[0].op.(elemUnaryOp); !ok || (ok && euo.unaryOpType() != sigmoidOpType) {\n\t\treturn a, noStabilizationErr{}\n\t}\n\n\tx := a.children[0].children[0]\n\n\tstabLogf(\"x : %v\", x.Name())\n\n\tif retVal, err = Softplus(x); err == nil {\n\t\tretVal, err = Neg(retVal)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, negFail)\n\t\t}\n\t\treturn retVal, nil\n\t}\n\treturn nil, errors.Wrap(err, softplusFail)\n}\n\n// logSoftmaxStabilization converts\n// \tlog(softmax(a)) to softmax{isLog: true}(a)\n//\tlog(a * softmax(b)) to log(a) + softmax{isLog: true}(b)\nfunc logSoftmaxStabilization(a *Node) (retVal *Node, err error) {\n\tstabLogf(\"Stabilizing log(softmax) of %v\", a)\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tswitch op := a.op.(type) {\n\tcase *softmaxOp:\n\t\top.isLog = true\n\t\treturn a, nil\n\tcase elemBinOp:\n\t\tif op.ʘBinaryOperator.binOpType() == mulOpType {\n\t\t\tfst := a.children[0]\n\t\t\tsnd := a.children[1]\n\n\t\t\tvar hasSm bool\n\t\t\tvar smop1, smop2 *softmaxOp\n\t\t\tif smop, ok := fst.op.(*softmaxOp); ok {\n\t\t\t\thasSm = true\n\t\t\t\tsmop1 = smop\n\t\t\t}\n\t\t\tif smop, ok := snd.op.(*softmaxOp); ok {\n\t\t\t\thasSm = true\n\t\t\t\tsmop2 = smop\n\t\t\t}\n\n\t\t\tif hasSm {\n\t\t\t\tvar newFst, newSnd *Node\n\t\t\t\tswitch {\n\t\t\t\tcase smop1 != nil && smop2 == nil:\n\t\t\t\t\tsmop1.isLog = true\n\t\t\t\t\tnewFst = fst\n\t\t\t\t\tif newSnd, err = Log(snd); err != nil {\n\t\t\t\t\t\treturn nil, err\n\t\t\t\t\t}\n\t\t\t\tcase smop1 == nil && smop2 != nil:\n\t\t\t\t\tsmop2.isLog = true\n\t\t\t\t\tnewSnd = snd\n\t\t\t\t\tif newFst, err = Log(fst); err != nil {\n\t\t\t\t\t\treturn nil, err\n\t\t\t\t\t}\n\t\t\t\tcase smop1 != nil && smop2 != nil:\n\t\t\t\t\tsmop1.isLog = true\n\t\t\t\t\tsmop2.isLog = true\n\t\t\t\t\tnewFst = fst\n\t\t\t\t\tnewSnd = snd\n\t\t\t\tdefault:\n\t\t\t\t\treturn a, noStabilizationErr{}\n\t\t\t\t}\n\n\t\t\t\t// g := a.g\n\t\t\t\t// g.removeAllEdgesFrom(a) // remove all references\n\t\t\t\t// g.RemoveNode(a)\n\t\t\t\t// returnNode(a) // send it back to the pool, since it is literally useless now\n\t\t\t\treturn Add(newFst, newSnd)\n\t\t\t}\n\n\t\t}\n\t}\n\treturn a, noStabilizationErr{}\n\n}\n\n/* Graph Optimizations */\n\n// negNegOptimization optimizes away -(-x) to just return x\n// place before neg\nfunc negNegOptimization(a *Node) (retVal *Node, err error) {\n\tstabLogf(\"Optimizing -(-x)\")\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tif euo, ok := a.op.(elemUnaryOp); !ok || (ok && euo.unaryOpType() != negOpType) {\n\t\treturn a, noStabilizationErr{}\n\t}\n\n\tx := a.children[0]\n\treturn x, nil\n}\n"
        },
        {
          "name": "stabilization_test.go",
          "type": "blob",
          "size": 2.48828125,
          "content": "package gorgonia\n\nimport (\n\t\"io/ioutil\"\n\t\"testing\"\n)\n\nfunc TestLogStabilization(t *testing.T) {\n\tg := NewGraph()\n\n\t// log(a+1)\n\tx := NewVector(g, Float64, WithName(\"x\"), WithShape(2))\n\tp := Must(Add(x, onef64))\n\tlp := Must(Log(p))\n\tif lp.children[0] != x {\n\t\tt.Error(\"Oops.\")\n\t\tioutil.WriteFile(\"log(a+1).dot\", []byte(lp.ToDot()), 0644)\n\t}\n\n\t// log(1+a)\n\tp = Must(Add(onef64, x))\n\tlp = Must(Log(p))\n\tif lp.children[0] != x {\n\t\tt.Error(\"Oops.\")\n\t\tioutil.WriteFile(\"log(1+a).dot\", []byte(lp.ToDot()), 0644)\n\t}\n\n\t//log(1-a)\n\tm := Must(Sub(onef64, x))\n\tlp = Must(Log(m))\n\tif euo, ok := lp.children[0].op.(elemUnaryOp); !ok {\n\t\tt.Error(\"Oops.\")\n\t} else {\n\t\tif euo.unaryOpType() != negOpType {\n\t\t\tt.Error(\"Expected Neg Op\")\n\t\t}\n\n\t\tif lp.children[0].children[0] != x {\n\t\t\tt.Error(\"Oops.\")\n\t\t}\n\t}\n\n\tif t.Failed() {\n\t\tioutil.WriteFile(\"log(1-a).dot\", []byte(lp.ToDot()), 0644)\n\t}\n\n\t//log(a-1)\n\tm = Must(Sub(x, onef64))\n\tlp = Must(Log(m))\n\t//TODO: surely there is a better way to test?\n\tif lp.children[0] == x {\n\t\tt.Error(\"Oops.\")\n\t}\n\n\t// log(a+2)\n\t// We expect to keep the same operation tree, without stabilization\n\tp = Must(Add(x, twof64))\n\tlp = Must(Log(p))\n\tif lp.children[0] != p {\n\t\tt.Error(\"Oops.\")\n\t\tioutil.WriteFile(\"log(a+2).dot\", []byte(lp.ToDot()), 0644)\n\t}\n}\n\nfunc TestExpStabilization(t *testing.T) {\n\tg := NewGraph()\n\n\tx := NewVector(g, Float64, WithName(\"x\"), WithShape(2))\n\te := Must(Exp(x))\n\ts := Must(Sub(e, onef64))\n\n\tif s.children[0] != x {\n\t\tt.Error(\"oops\")\n\t}\n\n\tif euo, ok := s.op.(elemUnaryOp); !ok || (ok && euo.unaryOpType() != expm1OpType) {\n\t\tt.Error(\"oops\")\n\t}\n\n\tif t.Failed() {\n\t\tioutil.WriteFile(\"exp(a)-1.dot\", []byte(s.ToDot()), 0644)\n\t}\n}\n\nfunc TestLogSigmoidStabilization(t *testing.T) {\n\tg := NewGraph()\n\n\tstabilization = true\n\tx := NewVector(g, Float64, WithName(\"x\"), WithShape(2))\n\ty := Must(Sigmoid(x))\n\tWithName(\"y\")(y)\n\tlogY := Must(Log(y))\n\tWithName(\"log(sigmoid(x))\")(logY)\n\n\tif euo, ok := logY.op.(elemUnaryOp); !ok || (ok && euo.unaryOpType() != negOpType) {\n\t\tt.Error(\"Oops\")\n\t}\n\n\tif euo, ok := logY.children[0].op.(elemUnaryOp); !ok || (ok && euo.unaryOpType() != softplusOpType) {\n\t\tt.Error(\"Oops2\")\n\t}\n\n\tif euo, ok := logY.children[0].children[0].op.(elemUnaryOp); !ok || (ok && euo.unaryOpType() != negOpType) {\n\t\tt.Error(\"Oops3\")\n\t}\n\n\tif logY.children[0].children[0].children[0] != x {\n\t\tt.Errorf(\"Oops4: %v\", logY.children[0].children[0].children[0].Name())\n\t}\n\n\tif t.Failed() {\n\t\tioutil.WriteFile(\"fullGraph.dot\", []byte(g.ToDot()), 0644)\n\t\tioutil.WriteFile(\"logY.dot\", []byte(logY.ToDot()), 0644)\n\t}\n}\n"
        },
        {
          "name": "templates.go",
          "type": "blob",
          "size": 3.2421875,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n\t\"text/template\"\n)\n\nconst exprNodeTemplText = `<\n<TABLE BORDER=\"0\" CELLBORDER=\"1\" CELLSPACING=\"0\" PORT=\"anchor\" {{if isLeaf .}} COLOR=\"#00FF00;\"{{else if isRoot . }} COLOR=\"#FF0000;\" {{else if isMarked .}} COLOR=\"#0000FF;\" {{end}}{{if isInput .}} BGCOLOR=\"lightyellow\"{{else if isStmt .}} BGCOLOR=\"lightblue\"{{end}}>\n\n<TR><TD>{{printf \"%x\" .ID}}</TD><TD>{{printf \"%v\" .Name | html | dotEscape}} :: {{nodeType . | html | dotEscape }}</TD></TR>\n{{if printOp . }}<TR><TD>Op</TD><TD>{{ opStr . | html | dotEscape }} :: {{ opType . | html | dotEscape }}</TD></TR>{{end}}\n{{if hasShape .}}<TR><TD>Shape</TD><TD>{{ getShape .}}</TD></TR>{{end}}\n<TR><TD>Overwrites Input {{overwritesInput . }}</TD><TD>Data On: {{.Device}}</TD></TR>\n{{if hasGrad .}}<TR><TD>Value</TD><TD>Grad</TD></TR>\n<TR><TD>{{printf \"%+3.3s\" .Value | dotEscape}}</TD><TD>{{getGrad . | dotEscape }} </TD></TR>\n<TR><TD>Ptr: {{getValPtr . | dotEscape}} </TD><TD>Ptr: {{getGradPtr . | dotEscape}} </TD></TR>\n{{else}}\n<TR><TD>Value</TD><TD>{{printf \"%+3.3s\" .Value | dotEscape}}</TD></TR>\n{{end}}\n\n</TABLE>\n>`\n\nfunc dotEscape(s string) string {\n\ts = strings.Replace(s, \"\\n\", \"<BR />\", -1)\n\ts = strings.Replace(s, \"<nil>\", \"NIL\", -1)\n\treturn s\n}\n\nfunc printOp(n *Node) bool  { return n.op != nil && !n.isStmt }\nfunc isLeaf(n *Node) bool   { return len(n.children) == 0 }\nfunc isInput(n *Node) bool  { return n.isInput() }\nfunc isMarked(n *Node) bool { return n.ofInterest }\nfunc isRoot(n *Node) bool   { return n.isRoot() }\nfunc isStmt(n *Node) bool   { return n.isStmt }\nfunc hasShape(n *Node) bool { return n.shape != nil }\nfunc hasGrad(n *Node) bool  { _, err := n.Grad(); return err == nil }\nfunc opStr(n *Node) string  { return n.op.String() }\nfunc opType(n *Node) string { return n.op.Type().String() }\n\nfunc nodeType(n *Node) string {\n\tif n.t == nil {\n\t\treturn \"NIL\"\n\t}\n\treturn n.t.String()\n}\n\nfunc overwritesInput(n *Node) int {\n\tif n.op == nil {\n\t\treturn -1\n\t}\n\treturn n.op.OverwritesInput()\n}\n\nfunc getShape(n *Node) string {\n\tif !n.inferredShape {\n\t\treturn fmt.Sprintf(\"%v\", n.shape)\n\t}\n\treturn fmt.Sprintf(\"<U>%v</U>\", n.shape) // graphviz 2.38+ only supports <O>\n}\n\nfunc getGrad(n *Node) string {\n\tgrad, err := n.Grad()\n\tif err == nil {\n\t\treturn fmt.Sprintf(\"%+3.3s\", grad)\n\t}\n\treturn \"\"\n}\n\nfunc getGradPtr(n *Node) string {\n\tgrad, err := n.Grad()\n\tif err == nil && grad != nil {\n\t\treturn fmt.Sprintf(\"0x%x\", grad.Uintptr())\n\t}\n\treturn \"\"\n}\n\nfunc getValPtr(n *Node) string {\n\tif n.Value() == nil {\n\t\treturn \"<nil>\"\n\t}\n\treturn fmt.Sprintf(\"0x%dx\", n.Value().Uintptr())\n}\n\nvar funcMap = template.FuncMap{\n\t\"dotEscape\":       dotEscape,\n\t\"printOp\":         printOp,\n\t\"isRoot\":          isRoot,\n\t\"isLeaf\":          isLeaf,\n\t\"isInput\":         isInput,\n\t\"isStmt\":          isStmt,\n\t\"isMarked\":        isMarked,\n\t\"hasShape\":        hasShape,\n\t\"hasGrad\":         hasGrad,\n\t\"getShape\":        getShape,\n\t\"getValPtr\":       getValPtr,\n\t\"getGrad\":         getGrad,\n\t\"getGradPtr\":      getGradPtr,\n\t\"overwritesInput\": overwritesInput,\n\t\"opStr\":           opStr,\n\t\"opType\":          opType,\n\t\"nodeType\":        nodeType,\n}\n\nvar (\n\texprNodeTempl     *template.Template\n\texprNodeJSONTempl *template.Template\n)\n\nfunc init() {\n\texprNodeTempl = template.Must(template.New(\"node\").Funcs(funcMap).Parse(exprNodeTemplText))\n}\n"
        },
        {
          "name": "testsetup_test.go",
          "type": "blob",
          "size": 6.705078125,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"math/rand\"\n\t\"reflect\"\n\t\"runtime\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"gorgonia.org/dawson\"\n\t\"gorgonia.org/tensor\"\n\n\t\"testing\"\n)\n\ntype errorStacker interface {\n\tErrorStack() string\n}\n\nfunc floatsEqual64(a, b []float64) bool {\n\tif len(a) != len(b) {\n\t\treturn false\n\t}\n\n\tfor i, v := range a {\n\t\tif !dawson.CloseF64(v, b[i]) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc floatsEqual32(a, b []float32) bool {\n\tif len(a) != len(b) {\n\t\treturn false\n\t}\n\n\tfor i, v := range a {\n\t\tif !dawson.CloseF32(v, b[i]) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc extractF64s(v Value) []float64 {\n\treturn v.Data().([]float64)\n}\n\nfunc extractF64(v Value) float64 {\n\tswitch vt := v.(type) {\n\tcase *F64:\n\t\treturn float64(*vt)\n\tcase tensor.Tensor:\n\t\tif !vt.IsScalar() {\n\t\t\tpanic(\"Got a non scalar result!\")\n\t\t}\n\t\tpc, _, _, _ := runtime.Caller(1)\n\t\tlog.Printf(\"Better watch it: %v called with a Scalar tensor\", runtime.FuncForPC(pc).Name())\n\t\treturn vt.ScalarValue().(float64)\n\t}\n\tpanic(fmt.Sprintf(\"Unhandled types! Got %v of %T instead\", v, v))\n}\n\nfunc extractF32s(v Value) []float32 {\n\treturn v.Data().([]float32)\n}\n\nfunc extractF32(v Value) float32 {\n\tswitch vt := v.(type) {\n\tcase *F32:\n\t\treturn float32(*vt)\n\tcase tensor.Tensor:\n\t\tif !vt.IsScalar() {\n\t\t\tpanic(\"Got a non scalar result!\")\n\t\t}\n\t\tpc, _, _, _ := runtime.Caller(1)\n\t\tlog.Printf(\"Better watch it: %v called with a Scalar tensor\", runtime.FuncForPC(pc).Name())\n\t\treturn vt.ScalarValue().(float32)\n\t}\n\tpanic(fmt.Sprintf(\"Unhandled types! Got %v of %T instead\", v, v))\n}\n\nfunc f64sTof32s(f []float64) []float32 {\n\tretVal := make([]float32, len(f))\n\tfor i, v := range f {\n\t\tretVal[i] = float32(v)\n\t}\n\treturn retVal\n}\n\nfunc simpleMatEqn() (g *ExprGraph, x, y, z *Node) {\n\tg = NewGraph()\n\tx = NewMatrix(g, Float64, WithName(\"x\"), WithShape(2, 2))\n\ty = NewMatrix(g, Float64, WithName(\"y\"), WithShape(2, 2))\n\tz = Must(Add(x, y))\n\treturn\n}\n\nfunc simpleVecEqn() (g *ExprGraph, x, y, z *Node) {\n\tg = NewGraph()\n\tx = NewVector(g, Float64, WithName(\"x\"), WithShape(2))\n\ty = NewVector(g, Float64, WithName(\"y\"), WithShape(2))\n\tz = Must(Add(x, y))\n\treturn\n}\n\nfunc simpleEqn() (g *ExprGraph, x, y, z *Node) {\n\tg = NewGraph()\n\tx = NewScalar(g, Float64, WithName(\"x\"))\n\ty = NewScalar(g, Float64, WithName(\"y\"))\n\tz = Must(Add(x, y))\n\treturn\n}\n\nfunc simpleUnaryEqn() (g *ExprGraph, x, y *Node) {\n\tg = NewGraph()\n\tx = NewScalar(g, Float64, WithName(\"x\"))\n\ty = Must(Square(x))\n\treturn\n}\n\nfunc simpleUnaryVecEqn() (g *ExprGraph, x, y *Node) {\n\tg = NewGraph()\n\tx = NewVector(g, Float64, WithName(\"x\"), WithShape(2))\n\ty = Must(Square(x))\n\treturn\n}\n\ntype malformed struct{}\n\nfunc (t malformed) Name() string                   { return \"malformed\" }\nfunc (t malformed) Format(state fmt.State, c rune) { fmt.Fprintf(state, \"malformed\") }\nfunc (t malformed) String() string                 { return \"malformed\" }\nfunc (t malformed) Apply(hm.Subs) hm.Substitutable { return t }\nfunc (t malformed) FreeTypeVar() hm.TypeVarSet     { return nil }\nfunc (t malformed) Eq(hm.Type) bool                { return false }\nfunc (t malformed) Types() hm.Types                { return nil }\nfunc (t malformed) Normalize(a, b hm.TypeVarSet) (hm.Type, error) {\n\treturn nil, errors.Errorf(\"cannot normalize malformed\")\n}\n\ntype assertState struct {\n\t*assert.Assertions\n\tcont bool\n}\n\nfunc newAssertState(a *assert.Assertions) *assertState { return &assertState{a, true} }\n\nfunc (a *assertState) Equal(expected interface{}, actual interface{}, msgAndArgs ...interface{}) {\n\tif !a.cont {\n\t\treturn\n\t}\n\ta.cont = a.Assertions.Equal(expected, actual, msgAndArgs...)\n}\n\nfunc (a *assertState) True(value bool, msgAndArgs ...interface{}) {\n\tif !a.cont {\n\t\treturn\n\t}\n\ta.cont = a.Assertions.True(value, msgAndArgs...)\n}\n\nfunc checkErr(t *testing.T, expected bool, err error, name string, id interface{}) (cont bool) {\n\tswitch {\n\tcase expected:\n\t\tif err == nil {\n\t\t\tt.Errorf(\"Expected error in test %v (%v)\", name, id)\n\t\t}\n\t\treturn true\n\tcase !expected && err != nil:\n\t\tt.Errorf(\"Test %v (%v) errored: %+v\", name, id, err)\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc deepNodeEq(a, b *Node) bool {\n\tif a == b {\n\t\treturn true\n\t}\n\n\tif a.isInput() {\n\t\tif !b.isInput() {\n\t\t\treturn false\n\t\t}\n\n\t\tif a.name != b.name {\n\t\t\treturn false\n\t\t}\n\t\tif !ValueEq(a.boundTo, b.boundTo) {\n\t\t\treturn false\n\t\t}\n\t\treturn true\n\t}\n\n\tif b.isInput() {\n\t\treturn false\n\t}\n\n\tif a.name != b.name {\n\t\treturn false\n\t}\n\n\tif a.group != b.group {\n\t\treturn false\n\t}\n\n\tif a.id != b.id {\n\t\treturn false\n\t}\n\n\tif a.hash != b.hash {\n\t\treturn false\n\t}\n\n\tif a.hashed != b.hashed {\n\t\treturn false\n\t}\n\n\tif a.inferredShape != b.inferredShape {\n\t\treturn false\n\t}\n\n\tif a.unchanged != b.unchanged {\n\t\treturn false\n\t}\n\n\tif a.isStmt != b.isStmt {\n\t\treturn false\n\t}\n\n\tif a.ofInterest != b.ofInterest {\n\t\treturn false\n\t}\n\n\tif a.dataOn != b.dataOn {\n\t\treturn false\n\t}\n\n\tif !a.t.Eq(b.t) {\n\t\treturn false\n\t}\n\tif !a.shape.Eq(b.shape) {\n\t\treturn false\n\t}\n\n\tif a.op.Hashcode() != b.op.Hashcode() {\n\t\treturn false\n\t}\n\n\tif !ValueEq(a.boundTo, b.boundTo) {\n\t\treturn false\n\t}\n\n\tif len(a.children) != len(b.children) {\n\t\treturn false\n\t}\n\n\tif len(a.derivOf) != len(b.derivOf) {\n\t\treturn false\n\t}\n\n\tif a.deriv != nil {\n\t\tif b.deriv == nil {\n\t\t\treturn false\n\t\t}\n\t\tif a.deriv.Hashcode() != b.deriv.Hashcode() {\n\t\t\treturn false\n\t\t}\n\t}\n\n\tfor i, c := range a.children {\n\t\tif c.Hashcode() != b.children[i].Hashcode() {\n\t\t\treturn false\n\t\t}\n\t}\n\n\tfor i, c := range a.derivOf {\n\t\tif c.Hashcode() != b.derivOf[i].Hashcode() {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// TensorGenerator only generates Dense tensors for now\ntype TensorGenerator struct {\n\tShapeConstraint tensor.Shape // [0, 6, 0] implies that the second dimension is the constraint. 0 is any.\n\tDtypeConstraint tensor.Dtype\n}\n\nfunc (g TensorGenerator) Generate(r *rand.Rand, size int) reflect.Value {\n\t// shape := g.ShapeConstraint\n\t// of := g.DtypeConstraint\n\n\t// if g.ShapeConstraint == nil {\n\t// \t// generate\n\t// } else {\n\t// \t// generate for 0s in constraints\n\t// }\n\n\t// if g.DtypeConstraint == (tensor.Dtype{}) {\n\t// \tof = g.DtypeConstraint\n\t// }\n\tvar retVal Value\n\n\treturn reflect.ValueOf(retVal)\n}\n\ntype ValueGenerator struct {\n\tShapeConstraint tensor.Shape // [0, 6, 0] implies that the second dimension is the constraint. 0 is any.\n\tDtypeConstraint tensor.Dtype\n}\n\nfunc (g ValueGenerator) Generate(r *rand.Rand, size int) reflect.Value {\n\t// generate scalar or tensor\n\tri := r.Intn(2)\n\tif ri == 0 {\n\t\tgen := TensorGenerator{\n\t\t\tShapeConstraint: g.ShapeConstraint,\n\t\t\tDtypeConstraint: g.DtypeConstraint,\n\t\t}\n\t\treturn gen.Generate(r, size)\n\n\t}\n\tvar retVal Value\n\t// of := acceptableDtypes[r.Intn(len(acceptableDtypes))]\n\n\treturn reflect.ValueOf(retVal)\n}\n\ntype NodeGenerator struct{}\n\nfunc (g NodeGenerator) Generate(r *rand.Rand, size int) reflect.Value {\n\tvar n *Node\n\treturn reflect.ValueOf(n)\n}\n"
        },
        {
          "name": "type.go",
          "type": "blob",
          "size": 4.01171875,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/chewxy/hm\"\n\t\"gorgonia.org/tensor\"\n)\n\nvar (\n\t// Represents the types that Nodes can take in Gorgonia\n\n\t// Float64 ...\n\tFloat64 = tensor.Float64\n\t// Float32 ...\n\tFloat32 = tensor.Float32\n\t// Int ...\n\tInt = tensor.Int\n\t// Int64 ...\n\tInt64 = tensor.Int64\n\t// Int32 ...\n\tInt32 = tensor.Int32\n\t// Byte ...\n\tByte = tensor.Uint8\n\t// Bool ...\n\tBool = tensor.Bool\n\n\t// Ptr is equivalent to interface{}. Ugh Ugh Ugh\n\tPtr = tensor.UnsafePointer\n\n\tvecF64  = &TensorType{Dims: 1, Of: tensor.Float64}\n\tvecF32  = &TensorType{Dims: 1, Of: tensor.Float32}\n\tmatF64  = &TensorType{Dims: 2, Of: tensor.Float64}\n\tmatF32  = &TensorType{Dims: 2, Of: tensor.Float32}\n\tten3F64 = &TensorType{Dims: 3, Of: tensor.Float64}\n\tten3F32 = &TensorType{Dims: 3, Of: tensor.Float32}\n\n\t// removes the need for type checking\n\tf64T = tensor.Float64 // hm.Type\n\tf32T = tensor.Float32 // hm.Type\n)\n\nvar acceptableDtypes = [...]tensor.Dtype{tensor.Float64, tensor.Float32, tensor.Int, tensor.Int64, tensor.Int32, tensor.Byte, tensor.Bool}\n\n/*Tensor Type*/\n\n// TensorType is a type constructor for tensors.\n//\n// Think of it as  something like this:\n//\t\tdata Tensor a = Tensor d a\n//\n// The shape of the Tensor is not part of TensorType.\n// Shape checking is relegated to the dynamic part of the program run\ntype TensorType struct {\n\tDims int // dims\n\n\tOf hm.Type\n}\n\nfunc makeFromTensorType(t TensorType, tv hm.TypeVariable) TensorType {\n\treturn makeTensorType(t.Dims, tv)\n}\n\nfunc makeTensorType(dims int, typ hm.Type) TensorType {\n\treturn TensorType{\n\t\tDims: dims,\n\t\tOf:   typ,\n\t}\n}\n\nfunc newTensorType(dims int, typ hm.Type) *TensorType {\n\tswitch {\n\tcase dims == 1 && typ == f64T:\n\t\treturn vecF64\n\tcase dims == 1 && typ == f32T:\n\t\treturn vecF32\n\tcase dims == 2 && typ == f64T:\n\t\treturn matF64\n\tcase dims == 2 && typ == f32T:\n\t\treturn matF32\n\tcase dims == 3 && typ == f64T:\n\t\treturn ten3F64\n\tcase dims == 3 && typ == f32T:\n\t\treturn ten3F32\n\t}\n\tt := borrowTensorType()\n\tt.Dims = dims\n\tt.Of = typ\n\treturn t\n}\n\n// Name returns the name of the type, which will always be \"Tensor\". Satisfies the hm.Type interface.\nfunc (t TensorType) Name() string { return \"Tensor\" }\n\n// Format implements fmt.Formatter. It is also required for the satisfication the hm.Type interface.\nfunc (t TensorType) Format(state fmt.State, c rune) {\n\tif state.Flag('#') {\n\t\tfmt.Fprintf(state, \"Tensor-%d %#v\", t.Dims, t.Of)\n\t} else {\n\t\tswitch t.Dims {\n\t\tcase 1:\n\t\t\tfmt.Fprintf(state, \"Vector %v\", t.Of)\n\t\tcase 2:\n\t\t\tfmt.Fprintf(state, \"Matrix %v\", t.Of)\n\t\tdefault:\n\t\t\tfmt.Fprintf(state, \"Tensor-%d %v\", t.Dims, t.Of)\n\t\t}\n\t}\n}\n\n// String implements fmt.Stringer and runtime.Stringer. Satisfies the hm.Type interface.\nfunc (t TensorType) String() string { return fmt.Sprintf(\"%v\", t) }\n\n// Types returns a list of types that TensorType contains - in this case, the type of Tensor (float64, float32, etc). Satisfies the hm.Type interface.\nfunc (t TensorType) Types() hm.Types { ts := hm.BorrowTypes(1); ts[0] = t.Of; return ts }\n\n// Normalize normalizes the type variable names (if any) in the TensorType. Satisfies the hm.Type interface.\nfunc (t TensorType) Normalize(k, v hm.TypeVarSet) (hm.Type, error) {\n\tvar err error\n\tif t.Of, err = t.Of.Normalize(k, v); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn t, nil\n}\n\n// Apply applies the substitutions on the types. Satisfies the hm.Type interface.\nfunc (t TensorType) Apply(sub hm.Subs) hm.Substitutable {\n\tt.Of = t.Of.Apply(sub).(hm.Type)\n\treturn t\n}\n\n// FreeTypeVar returns any free (unbound) type variables in this type. Satisfies the hm.Type interface.\nfunc (t TensorType) FreeTypeVar() hm.TypeVarSet {\n\treturn t.Of.FreeTypeVar()\n}\n\n// Eq is the equality function of this type. The type of Tensor has to be the same, and for now, only the dimensions are compared.\n// Shape may be compared in the future for tighter type inference. Satisfies the hm.Type interface.\nfunc (t TensorType) Eq(other hm.Type) bool {\n\tswitch ot := other.(type) {\n\tcase TensorType:\n\t\treturn t.Of.Eq(ot.Of) && t.Dims == ot.Dims\n\tcase *TensorType:\n\t\treturn t.Of.Eq(ot.Of) && t.Dims == ot.Dims\n\t}\n\treturn false\n}\n"
        },
        {
          "name": "typeSystem.go",
          "type": "blob",
          "size": 3.0625,
          "content": "package gorgonia\n\nimport (\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\n// inferType infers the type of the expression\nfunc inferType(expr interface{}) (retVal hm.Type, err error) {\n\tswitch e := expr.(type) {\n\tcase *Node:\n\t\tif e.isInput() || e.isConstant() {\n\t\t\t// Var (and Let const)\n\t\t\treturn e.t, nil\n\t\t}\n\n\t\t// stop the recursive inference early - if the node already has a type, return it\n\t\tif e.t != nil {\n\t\t\treturn e.t, nil\n\t\t}\n\n\t\treturn inferNodeType(e.op, e.children...)\n\tcase Op:\n\t\treturn e.Type(), nil\n\tcase float32:\n\t\treturn Float32, nil\n\tcase float64:\n\t\treturn Float64, nil\n\tcase int:\n\t\treturn Int, nil\n\tcase int64:\n\t\treturn Int64, nil\n\tcase int32:\n\t\treturn Int32, nil\n\tcase bool:\n\t\treturn Bool, nil\n\tdefault:\n\t\terr = errors.Errorf(nyiTypeFail, \"inferType\", expr)\n\t\treturn\n\t}\n}\n\n// Instead of using hm's Infer function, since all the nodes are pretty much hm.Apply, we write our own.\nfunc inferNodeType(op Op, children ...*Node) (retVal hm.Type, err error) {\n\tfnType := op.Type()\n\tif fnt, ok := fnType.(*hm.FunctionType); ok {\n\t\tdefer hm.ReturnFnType(fnt)\n\t}\n\n\targTypes := hm.BorrowTypes(len(children) + 1)\n\tdefer hm.ReturnTypes(argTypes)\n\tfor i, child := range children {\n\t\tif argTypes[i], err = inferType(child); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, \"Failed to infer type of %v\", child)\n\t\t}\n\t}\n\n\tb := hm.TypeVariable('b')\n\targTypes[len(argTypes)-1] = b\n\n\tfn := hm.NewFnType(argTypes...)\n\tdefer hm.ReturnFnType(fn)\n\n\t// var t0 hm.Type\n\tvar sub hm.Subs\n\tif sub, err = hm.Unify(fn, fnType); err != nil {\n\t\treturn nil, errors.Wrapf(err, \"Unable to unify while inferring type of %v\", op)\n\t}\n\n\tvar ok bool\n\tif retVal, ok = sub.Get(b); !ok {\n\t\treturn nil, errors.Errorf(\"Expected a replacement for %v\", b)\n\t}\n\n\t// return pruneReturn(t0.(*hm.FunctionType).ReturnType()), nil\n\treturn retVal, nil\n}\n\nfunc isScalarType(t hm.Type) bool {\n\tswitch tt := t.(type) {\n\tcase tensor.Dtype:\n\t\treturn true\n\tcase TensorType:\n\t\tif tt.Dims == 0 {\n\t\t\treturn true\n\t\t}\n\t\treturn false\n\tcase hm.TypeVariable:\n\t\tpanic(\"Type Variable is a type that is not yet known.\")\n\tdefault:\n\t\tpanic(\"Unhandled type\")\n\t}\n}\n\nfunc dtypeOf(t hm.Type) (retVal tensor.Dtype, err error) {\n\tswitch p := t.(type) {\n\tcase tensor.Dtype:\n\t\tretVal = p\n\tcase TensorType:\n\t\treturn dtypeOf(p.Of)\n\tcase hm.TypeVariable:\n\t\terr = errors.Errorf(\"instance %v does not have a dtype\", p)\n\tdefault:\n\t\terr = errors.Errorf(nyiFail, \"dtypeOf\", p)\n\t\treturn\n\t}\n\n\treturn\n}\n\n// DEPRECATED\n\n/*\nfunc runtimeTypeCheck(expected, got hm.Types) (of Dtype, err error) {\n\tif len(expected) != len(got) {\n\t\terr = NewError(RuntimeError, \"Input length mismatch\")\n\t\treturn\n\t}\n\n\tif of, err = dtypeOf(expected[0]); err != nil {\n\t\treturn\n\t}\n\n\tfor i, e := range expected {\n\t\tg := got[i]\n\t\tif !e.Eq(g) {\n\t\t\terr = NewError(RuntimeError, \"Expected input[%d] to be %v. Got %v instead\", i, e, got[i])\n\t\t\treturn\n\t\t}\n\n\t\tif i > 0 {\n\t\t\tvar gdt Dtype\n\t\t\tif gdt, err = dtypeOf(g); err == nil {\n\t\t\t\tif gdt != of {\n\t\t\t\t\terr = NewError(RuntimeError, \"Different dtypes encountered... Expected %v. Got %v instead\", of, gdt)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\treturn\n}\n*/\n"
        },
        {
          "name": "typeSystem_test.go",
          "type": "blob",
          "size": 4.146484375,
          "content": "package gorgonia\n\nimport (\n\t\"testing\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"gorgonia.org/tensor\"\n)\n\n// TODO: gather edge cases\nfunc TestInferNodeType(t *testing.T) {\n\tassert := assert.New(t)\n\tg := NewGraph()\n\tvar inferNodeTests = []struct {\n\t\tname     string\n\t\top       Op\n\t\tchildren Nodes\n\n\t\tcorrect hm.Type\n\t\terr     bool\n\t}{\n\t\t// simple case Float+Float\n\t\t{\"+(1, 2)\",\n\t\t\tnewEBOByType(addOpType, Float64, Float64),\n\t\t\tNodes{\n\t\t\t\tnewNode(In(g), WithType(Float64), WithName(\"a\")),\n\t\t\t\tnewNode(In(g), WithType(Float64), WithName(\"b\"))},\n\t\t\tFloat64,\n\t\t\tfalse},\n\n\t\t// complicated case: will error out due to mis match\n\t\t{\"+(1, 2)\",\n\t\t\tnewEBOByType(addOpType, Float64, Float32),\n\t\t\tNodes{\n\t\t\t\tnewNode(In(g), WithType(Float64), WithName(\"a\")),\n\t\t\t\tnewNode(In(g), WithType(Float32), WithName(\"b\"))},\n\t\t\tFloat64,\n\t\t\ttrue},\n\t}\n\n\tfor _, ints := range inferNodeTests {\n\t\tt0, err := inferNodeType(ints.op, ints.children...)\n\t\tswitch {\n\t\tcase ints.err && err == nil:\n\t\t\tt.Errorf(\"Expected an error in test %q\", ints.name)\n\t\tcase !ints.err && err != nil:\n\t\t\tt.Errorf(\"Error in test %q: %v\", ints.name, err)\n\t\t}\n\n\t\tif ints.err {\n\t\t\tcontinue\n\t\t}\n\n\t\tassert.True(ints.correct.Eq(t0))\n\t}\n}\n\nvar inferTypeTests = []struct {\n\texpr interface{}\n\n\tcorrect hm.Type\n\terr     bool\n}{\n\t{newEBOByType(addOpType, Float64, Float64), hm.NewFnType(hm.TypeVariable('a'), hm.TypeVariable('a'), hm.TypeVariable('a')), false},\n\t{float32(0), Float32, false},\n\t{float64(0), Float64, false},\n\t{0, Int, false},\n\t{int64(0), Int64, false},\n\t{int32(0), Int32, false},\n\t{true, Bool, false},\n\t{newNode(In(NewGraph()), WithType(Float64), WithOp(newEBOByType(addOpType, Float64, Float64))), Float64, false},\n\n\t{[]int{0}, nil, true},\n}\n\nfunc TestInferType(t *testing.T) {\n\tfor i, itts := range inferTypeTests {\n\t\tt0, err := inferType(itts.expr)\n\t\tswitch {\n\t\tcase itts.err && err == nil:\n\t\t\tt.Errorf(\"Expected an error in infering type of %T\", itts.expr)\n\t\tcase !itts.err && err != nil:\n\t\t\tt.Errorf(\"Error while inferring type of %T: %v\", itts.expr, err)\n\t\t}\n\n\t\tif itts.err {\n\t\t\tcontinue\n\t\t}\n\t\tassert.True(t, itts.correct.Eq(t0), \"Test %d: %v != %v\", i, t0, itts.correct)\n\t}\n\n\t// way out there stuff\n\tg := NewGraph()\n\tn := newNode(In(g), WithOp(newEBOByType(addOpType, Float64, Float64)), WithChildren(Nodes{newNode(In(g), WithName(\"a\"), WithType(Float64)), newNode(In(g), WithName(\"b\"), WithType(Float64))}))\n\tt0, err := inferType(n)\n\tif err != nil {\n\t\tt.Errorf(\"Special Case #1: %v\", err)\n\t}\n\tt.Logf(\"t0: %v\", t0)\n}\n\nvar scalarTypeTests []struct {\n\tname string\n\ta    hm.Type\n\n\tisScalar bool\n\tpanics   bool\n}\n\nfunc TestIsScalarType(t *testing.T) {\n\tfor _, stts := range scalarTypeTests {\n\t\tif stts.panics {\n\t\t\tf := func() {\n\t\t\t\tisScalarType(stts.a)\n\t\t\t}\n\t\t\tassert.Panics(t, f)\n\t\t\tcontinue\n\t\t}\n\n\t\tif isScalarType(stts.a) != stts.isScalar {\n\t\t\tt.Errorf(\"Expected isScalarType(%v) to be scalar: %v\", stts.a, stts.isScalar)\n\t\t}\n\t}\n}\n\nvar dtypeOfTests []struct {\n\ta hm.Type\n\n\tcorrect tensor.Dtype\n\terr     bool\n}\n\nfunc TestDtypeOf(t *testing.T) {\n\tfor _, dots := range dtypeOfTests {\n\t\tdt, err := dtypeOf(dots.a)\n\n\t\tswitch {\n\t\tcase err != nil && !dots.err:\n\t\t\tt.Errorf(\"Error when performing dtypeOf(%v): %v\", dots.a, err)\n\t\tcase err == nil && dots.err:\n\t\t\tt.Errorf(\"Expected an error when performing dtypeOf(%v)\", dots.a)\n\t\t}\n\n\t\tif dots.err {\n\t\t\tcontinue\n\t\t}\n\n\t\tif !dots.correct.Eq(dt) {\n\t\t\tt.Errorf(\"Incorrect dtypeOf when performing dtypeOf(%v). Expected %v. Got %v\", dots.a, dots.correct, dt)\n\t\t}\n\t}\n}\n\nfunc init() {\n\tscalarTypeTests = []struct {\n\t\tname string\n\t\ta    hm.Type\n\n\t\tisScalar bool\n\t\tpanics   bool\n\t}{\n\t\t{\"Float64\", Float64, true, false},\n\t\t{\"Tensor Float64\", makeTensorType(1, Float64), false, false},\n\t\t{\"Tensor Float64 (special)\", makeTensorType(0, Float64), true, false},\n\n\t\t// this is bad\n\t\t{\"a\", hm.TypeVariable('a'), false, true},\n\t\t{\"malformed\", malformed{}, false, true},\n\t}\n\n\tdtypeOfTests = []struct {\n\t\ta hm.Type\n\n\t\tcorrect tensor.Dtype\n\t\terr     bool\n\t}{\n\t\t{Float64, Float64, false},\n\t\t{makeTensorType(1, Float64), Float64, false},\n\n\t\t// this is bad\n\t\t// {hm.TypeVariable('a'), MAXDTYPE, true},\n\t\t// {hm.TypeVariable('a'), MAXDTYPE, true},\n\t\t// {makeTensorType(1, hm.TypeVariable('a')), MAXDTYPE, true},\n\t\t// {malformed{}, MAXDTYPE, true},\n\t}\n}\n"
        },
        {
          "name": "type_test.go",
          "type": "blob",
          "size": 3.9169921875,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc TestDtypeBasics(t *testing.T) {\n\tassert := assert.New(t)\n\n\tvar t0 tensor.Dtype\n\tvar a hm.TypeVariable\n\n\tt0 = Float64\n\ta = hm.TypeVariable('a')\n\n\tassert.True(t0.Eq(Float64))\n\tassert.False(t0.Eq(Float32))\n\tassert.False(t0.Eq(a))\n\tassert.Nil(t0.Types())\n\n\tk := hm.TypeVarSet{'x', 'y'}\n\tv := hm.TypeVarSet{'a', 'b'}\n\tt1, err := t0.Normalize(k, v)\n\tassert.Nil(err)\n\tassert.Equal(t0, t1)\n\n\t// for completeness sake\n\tassert.Equal(\"float64\", t0.Name())\n\tassert.Equal(\"float64\", t0.String())\n\tassert.Equal(\"float64\", fmt.Sprintf(\"%v\", t0))\n\n}\n\nfunc TestDtypeOps(t *testing.T) {\n\tvar sub hm.Subs\n\tvar a hm.TypeVariable\n\tvar err error\n\n\ta = hm.TypeVariable('a')\n\n\tif sub, err = hm.Unify(a, Float64); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif repl, ok := sub.Get(a); !ok {\n\t\tt.Errorf(\"Expected a substitution for %v\", a)\n\t} else if repl != Float64 {\n\t\tt.Errorf(\"Expecetd substitution for %v to be %v. Got %v instead\", a, Float64, repl)\n\t}\n\n\tif sub, err = hm.Unify(Float64, a); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif repl, ok := sub.Get(a); !ok {\n\t\tt.Errorf(\"Expected a substitution for %v\", a)\n\t} else if repl != Float64 {\n\t\tt.Errorf(\"Expecetd substitution for %v to be %v. Got %v instead\", a, Float64, repl)\n\t}\n}\n\nvar tensorTypeTests []struct {\n\ta, b TensorType\n\n\teq     bool\n\ttypes  hm.Types\n\tformat string\n}\n\nfunc TestTensorTypeBasics(t *testing.T) {\n\tassert := assert.New(t)\n\n\tfor _, ttts := range tensorTypeTests {\n\t\t// Equality\n\t\tif ttts.eq {\n\t\t\tassert.True(ttts.a.Eq(ttts.b), \"TensorType Equality failed: %#v != %#v\", ttts.a, ttts.b)\n\t\t} else {\n\t\t\tassert.False(ttts.a.Eq(ttts.b), \"TensorType Equality: %v == %v should be false\", ttts.a, ttts.b)\n\t\t}\n\n\t\t// Types\n\t\tassert.Equal(ttts.types, ttts.a.Types())\n\n\t\t// string and format for completeness sake\n\t\tassert.Equal(\"Tensor\", ttts.a.Name())\n\t\tassert.Equal(ttts.format, fmt.Sprintf(\"%v\", ttts.a))\n\t\tassert.Equal(fmt.Sprintf(\"Tensor-%d %v\", ttts.a.Dims, ttts.a.Of), fmt.Sprintf(\"%#v\", ttts.a))\n\t}\n\n\ttt := makeTensorType(1, hm.TypeVariable('x'))\n\tk := hm.TypeVarSet{'x', 'y'}\n\tv := hm.TypeVarSet{'a', 'b'}\n\ttt2, err := tt.Normalize(k, v)\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tassert.True(tt2.Eq(makeTensorType(1, hm.TypeVariable('a'))))\n\n}\n\nvar tensorOpsTest []struct {\n\tname string\n\n\ta hm.Type\n\tb hm.Type\n\n\taSub hm.Type\n}\n\nfunc TestTensorTypeOps(t *testing.T) {\n\tfor _, tots := range tensorOpsTest {\n\t\tsub, err := hm.Unify(tots.a, tots.b)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif subst, ok := sub.Get(hm.TypeVariable('a')); !ok {\n\t\t\tt.Errorf(\"Expected a substitution for a\")\n\t\t} else if !subst.Eq(tots.aSub) {\n\t\t\tt.Errorf(\"Expected substitution to be %v. Got %v instead\", tots.aSub, subst)\n\t\t}\n\t}\n}\n\nfunc init() {\n\ttensorTypeTests = []struct {\n\t\ta, b TensorType\n\n\t\teq     bool\n\t\ttypes  hm.Types\n\t\tformat string\n\t}{\n\n\t\t{makeTensorType(1, Float64), makeTensorType(1, Float64), true, hm.Types{Float64}, \"Vector float64\"},\n\t\t{makeTensorType(1, Float64), makeTensorType(1, Float32), false, hm.Types{Float64}, \"Vector float64\"},\n\t\t{makeTensorType(1, Float64), makeTensorType(2, Float64), false, hm.Types{Float64}, \"Vector float64\"},\n\t\t{makeTensorType(1, hm.TypeVariable('a')), makeTensorType(1, hm.TypeVariable('a')), true, hm.Types{hm.TypeVariable('a')}, \"Vector a\"},\n\t\t{makeTensorType(1, hm.TypeVariable('a')), makeTensorType(1, hm.TypeVariable('b')), false, hm.Types{hm.TypeVariable('a')}, \"Vector a\"},\n\t}\n\n\ttensorOpsTest = []struct {\n\t\tname string\n\n\t\ta hm.Type\n\t\tb hm.Type\n\n\t\taSub hm.Type\n\t}{\n\t\t{\"a ~ Tensor Float64\", hm.TypeVariable('a'), makeTensorType(1, Float64), makeTensorType(1, Float64)},\n\t\t{\"Tensor Float64 ~ a\", makeTensorType(1, Float64), hm.TypeVariable('a'), makeTensorType(1, Float64)},\n\t\t{\"Tensor a ~ Tensor Float64\", makeTensorType(1, hm.TypeVariable('a')), makeTensorType(1, Float64), Float64},\n\t\t{\"Tensor a ~ Tensor Float64\", makeTensorType(1, Float64), makeTensorType(1, hm.TypeVariable('a')), Float64},\n\t}\n}\n"
        },
        {
          "name": "utils.go",
          "type": "blob",
          "size": 6.228515625,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"hash/fnv\"\n\t\"math\"\n\t\"sync\"\n\n\t\"github.com/chewxy/math32\"\n\t\"github.com/pkg/errors\"\n\t\"gonum.org/v1/gonum/graph\"\n\t\"gonum.org/v1/gonum/graph/iterator\"\n\t\"gorgonia.org/tensor\"\n)\n\nconst (\n\tmaxFloat32 = math32.MaxFloat32\n\tmaxFloat64 = math.MaxFloat64\n)\n\n// NodesToValueGrads is a utility function that converts a Nodes to a slice of ValueGrad for the solvers\nfunc NodesToValueGrads(in Nodes) (out []ValueGrad) {\n\tout = make([]ValueGrad, len(in))\n\tfor i := range in {\n\t\tout[i] = in[i]\n\t}\n\treturn out\n}\n\nfunc graphNodeToNode(in graph.Nodes) (out Nodes) {\n\tout = make(Nodes, in.Len())\n\tfor i := 0; in.Next(); i++ {\n\t\tout[i] = in.Node().(*Node)\n\t}\n\n\treturn\n}\n\nfunc sliceNodesToNodes(in []graph.Node) (out Nodes) {\n\tout = make(Nodes, len(in))\n\tfor i := range in {\n\t\tout[i] = in[i].(*Node)\n\t}\n\treturn\n}\n\nfunc nodeToGraphNode(in []*Node) graph.Nodes {\n\tnodes := make([]graph.Node, len(in))\n\tfor i, n := range in {\n\t\tnodes[i] = n\n\t}\n\treturn iterator.NewOrderedNodes(nodes)\n}\n\nfunc tensorInfo(t tensor.Tensor) (dt tensor.Dtype, dim int) {\n\tdt = t.Dtype()\n\tdim = t.Dims()\n\treturn\n}\n\nfunc valueToInt(v Value) (int, error) {\n\tvar intV int\n\tswitch sv := v.(type) {\n\tcase *F64:\n\t\tintV = int(float64(*sv))\n\tcase *F32:\n\t\tintV = int(float32(*sv))\n\tcase *I:\n\t\tintV = int(*sv)\n\tcase *I32:\n\t\tintV = int(int32(*sv))\n\tcase *I64:\n\t\tintV = int(int64(*sv))\n\tcase *U8:\n\t\tintV = int(byte(*sv))\n\tdefault:\n\t\treturn -1, errors.Errorf(\"Expected values to be all Scalar Value. Got %v of %T instead\", v, v)\n\t}\n\treturn intV, nil\n}\n\n// valuesToInts will FORCIBLY cast floats to ints.\nfunc valuesToInts(values []Value) (retVal []int, err error) {\n\tretVal = tensor.BorrowInts(len(values))\n\tfor i, v := range values {\n\t\tvar intV int\n\t\tswitch sv := v.(type) {\n\t\tcase *F64:\n\t\t\tintV = int(float64(*sv))\n\t\tcase *F32:\n\t\t\tintV = int(float32(*sv))\n\t\tcase *I:\n\t\t\tintV = int(*sv)\n\t\tcase *I32:\n\t\t\tintV = int(int32(*sv))\n\t\tcase *I64:\n\t\t\tintV = int(int64(*sv))\n\t\tcase *U8:\n\t\t\tintV = int(byte(*sv))\n\t\tcase Scalar:\n\t\t\treturn nil, errors.Errorf(nyiTypeFail, \"valueToInts\", v)\n\t\tdefault:\n\t\t\treturn nil, errors.Errorf(\"Expected values to be all Scalar Value. Got %v of %T instead\", v, v)\n\n\t\t}\n\t\tretVal[i] = intV\n\t}\n\treturn\n}\n\nfunc valuesToTensors(values []Value) (retVal []tensor.Tensor, err error) {\n\tretVal = make([]tensor.Tensor, len(values))\n\tfor i, v := range values {\n\t\tif vt, ok := v.(tensor.Tensor); ok {\n\t\t\tretVal[i] = vt\n\t\t\tcontinue\n\t\t}\n\t\treturn nil, errors.Errorf(\"Expected values to all be tensor.Tensor. Got %v of %T in %dth index of the slice\", v, v, i)\n\t}\n\treturn\n}\n\nfunc intRange(start, end int) []int {\n\tsize := end - start\n\tincr := true\n\tif start > end {\n\t\tincr = false\n\t\tsize = start - end\n\t}\n\n\tif size < 0 {\n\t\tpanic(\"Cannot create an int range that is somehow negative in size\")\n\t}\n\n\tretVal := make([]int, size)\n\n\tfor i, v := 0, start; i < size; i++ {\n\t\tretVal[i] = v\n\t\tif incr {\n\t\t\tv++\n\t\t} else {\n\t\t\tv--\n\t\t}\n\t}\n\treturn retVal\n}\n\nfunc ones(dt tensor.Dtype, sizes ...int) (retVal Value) {\n\tif len(sizes) == 0 {\n\t\treturn one(dt)\n\t}\n\treturn tensor.Ones(dt, sizes...)\n}\n\nfunc hasInf(v Value, dev Device) bool {\n\tswitch vt := v.(type) {\n\tcase *F64:\n\t\treturn math.IsInf(float64(*vt), 0)\n\tcase *F32:\n\t\treturn math32.IsInf(float32(*vt), 0)\n\tcase tensor.Tensor:\n\t\tif e, ok := vt.Engine().(tensor.InfChecker); ok {\n\t\t\tok, _ := e.HasInf(vt) // BUG: errors not checked\n\t\t\treturn ok\n\t\t}\n\n\t\tdt := vt.Dtype()\n\t\tif dt != tensor.Float64 && dt != tensor.Float32 {\n\t\t\treturn false\n\t\t}\n\t\tswitch dt {\n\t\tcase tensor.Float32:\n\t\t\tdata := vt.Data().([]float32)\n\t\t\tfor _, datum := range data {\n\t\t\t\tif math32.IsInf(datum, 0) {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t}\n\t\tcase tensor.Float64:\n\t\t\tdata := vt.Data().([]float64)\n\t\t\tfor _, datum := range data {\n\t\t\t\tif math.IsInf(datum, 0) {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn false\n\tcase *dualValue:\n\t\treturn hasInf(vt.Value, dev) || hasInf(vt.d, dev)\n\tdefault:\n\t\terr := nyi(\"hasInf\", v)\n\t\tpanic(err)\n\t}\n}\n\nfunc hasNaN(v Value, dev Device) bool {\n\tswitch vt := v.(type) {\n\tcase *F64:\n\t\treturn math.IsNaN(float64(*vt))\n\tcase *F32:\n\t\treturn math32.IsNaN(float32(*vt))\n\tcase tensor.Tensor:\n\t\tif e, ok := vt.Engine().(tensor.NaNChecker); ok {\n\t\t\tok, _ := e.HasNaN(vt) // BUG: errors not checked\n\t\t\treturn ok\n\t\t}\n\n\t\tdt := vt.Dtype()\n\t\tif dt != tensor.Float64 && dt != tensor.Float32 {\n\t\t\treturn false\n\t\t}\n\n\t\tswitch dt {\n\t\tcase tensor.Float32:\n\t\t\tdata := vt.Data().([]float32)\n\t\t\tfor _, datum := range data {\n\t\t\t\tif math32.IsNaN(datum) {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t}\n\t\tcase tensor.Float64:\n\t\t\tdata := vt.Data().([]float64)\n\t\t\tfor _, datum := range data {\n\t\t\t\tif math.IsNaN(datum) {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn false\n\tcase *dualValue:\n\t\treturn hasNaN(vt.Value, dev) || hasNaN(vt.d, dev)\n\tdefault:\n\t\terr := nyi(\"hasNaN\", vt)\n\t\tpanic(err)\n\t}\n}\n\nfunc setZero(val Value) (retVal Value) {\n\tswitch v := val.(type) {\n\tcase Zeroer:\n\t\tv.Zero()\n\t\treturn v\n\tcase Scalar:\n\t\treturn zero(v.Dtype())\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"setZero not implemented yet for %T\", v))\n\t}\n}\n\nfunc checkArity(op arityer, inputs int) error {\n\tif inputs != op.Arity() && op.Arity() >= 0 {\n\t\treturn errors.Errorf(\"%v has an arity of %d. Got %d instead\", op, op.Arity(), inputs)\n\t}\n\treturn nil\n}\n\nfunc maxInt(a, b int) int {\n\tif a > b {\n\t\treturn a\n\t}\n\treturn b\n}\n\nfunc minInt(a, b int) int {\n\tif a < b {\n\t\treturn a\n\t}\n\treturn b\n}\n\nfunc ceilDivInt(a, b int) int {\n\treturn (a + b - 1) / b\n}\n\nfunc simpleHash(op hashWriter) uint32 {\n\th := fnv.New32a()\n\top.WriteHash(h)\n\treturn h.Sum32()\n}\n\nfunc getDV(x, y *Node) (xdv, ydv *dualValue) {\n\treturn x.boundTo.(*dualValue), y.boundTo.(*dualValue)\n}\n\nfunc getDV3(x, y, z *Node) (xdv, ydv, zdv *dualValue) {\n\treturn x.boundTo.(*dualValue), y.boundTo.(*dualValue), z.boundTo.(*dualValue)\n}\n\nfunc getConst(x *Node, constant string) (retVal *Node, err error) {\n\tvar dt tensor.Dtype\n\tif dt, err = dtypeOf(x.t); err != nil {\n\t\treturn nil, errors.Wrap(err, dtypeOfFail)\n\t}\n\n\tif m, ok := constmap[constant]; ok {\n\t\tif n, ok := m[dt]; ok {\n\t\t\treturn n, nil\n\t\t}\n\t}\n\treturn nil, errors.Errorf(\"constant %v not provided for %v\", constant, dt)\n}\n\nfunc scalarEquiv(s tensor.Shape) bool {\n\tif len(s) == 0 {\n\t\treturn true\n\t}\n\tprod := 1\n\tfor _, v := range s {\n\t\tprod *= v\n\t}\n\n\treturn prod == 1\n}\n\nfunc runInParallel(from, to int, cb func(i int)) {\n\twg := sync.WaitGroup{}\n\n\tfor i := from; i < to; i++ {\n\t\twg.Add(1)\n\n\t\tgo func(i int) {\n\t\t\tcb(i)\n\t\t\twg.Done()\n\t\t}(i)\n\t}\n\n\twg.Wait()\n}\n"
        },
        {
          "name": "values.go",
          "type": "blob",
          "size": 5.72265625,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"unsafe\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\n// Value represents a value that Gorgonia accepts. At this point it is implemented by:\n//\t\t- all scalar value types (F64, F32... etc)\n// \t\t- *tensor.Dense\n// \t\t- *dualValue\n//\n// A Value is essentially any thing that knows its own type and shape.\n// Most importantly though, a Value is a pointer - and can be converted into a tensor.Memory.\n// This is done for the sake of interoperability with external devices like cgo or CUDA or OpenCL.\n// This also means for the most part most Values will be allocated on the heap.\n// There are some performance tradeoffs made in this decision, but ultimately this is better than having to manually manage blocks of memory\ntype Value interface {\n\tShape() tensor.Shape // Shape  returns the shape of the Value. Scalar values return ScalarShape()\n\tSize() int           // Size represents the number of elements in the Value. Note that in cases such as a *tensor.Dense, the underlying slice MAY have more elements than the Size() reports. This is correct.\n\tData() interface{}   // Data returns the original representation of the Value\n\tDtype() tensor.Dtype // Dtype returns the Dtype of the value\n\n\ttensor.Memory\n\tfmt.Formatter\n}\n\n// Valuer is any type that can return a Value\ntype Valuer interface {\n\tValue() Value\n}\n\n// Zeroer is a Value that can zero itself\ntype Zeroer interface {\n\tValue\n\tZero()\n}\n\n// ZeroValuer is a a Value that can provide the zero-value of its type\ntype ZeroValuer interface {\n\tValue\n\tZeroValue() Value\n}\n\n// Dtyper represents any type (typically a Value) that knows its own Dtype\ntype Dtyper interface {\n\tDtype() tensor.Dtype\n}\n\n// Typer represents any type (typically a Op) that knows its own Type\ntype Typer interface {\n\tType() hm.Type\n}\n\n// ValueEqualer represents any type that can perform a equal value check\ntype ValueEqualer interface {\n\tValueEq(Value) bool\n}\n\n// ValueCloser represents any type that can perform a close-value check\ntype ValueCloser interface {\n\tValueClose(interface{}) bool\n}\n\n// Cloner represents any type that can clone itself.\ntype Cloner interface {\n\tClone() interface{}\n}\n\n// CloneErrorer represents any type that can clone itself and return an error if necessary\ntype CloneErrorer interface {\n\tClone() (interface{}, error)\n}\n\n// CopierTo represents any type that can copy data to the destination.\ntype CopierTo interface {\n\tCopyTo(dest interface{}) error\n}\n\n// CopierFrom represents any type that can copy data from the source provided.\ntype CopierFrom interface {\n\tCopyFrom(src interface{}) error\n}\n\n// Setter is a any value that can Memset itself to the provided value\n// type Setter interface {\n// \tSetAll(interface{}) error\n// }\n\n// makeValue creates a value given a type and shape. The default value is the zero value of the type.\nfunc makeValue(t hm.Type, s tensor.Shape) (retVal Value, err error) {\n\tvar dt tensor.Dtype\n\tif dt, err = dtypeOf(t); err != nil {\n\t\treturn\n\t}\n\n\tif s.IsScalar() {\n\t\tswitch dt {\n\t\tcase tensor.Float64:\n\t\t\treturn NewF64(0), nil\n\t\tcase tensor.Float32:\n\t\t\treturn NewF32(0), nil\n\t\tcase tensor.Int:\n\t\t\treturn NewI(0), nil\n\t\tcase tensor.Int64:\n\t\t\treturn NewI64(0), nil\n\t\tcase tensor.Int32:\n\t\t\treturn NewI32(0), nil\n\t\tcase tensor.Byte:\n\t\t\treturn NewU8(0), nil\n\t\tcase tensor.Bool:\n\t\t\treturn NewB(false), nil\n\t\t}\n\t}\n\n\tswitch tt := t.(type) {\n\tcase TensorType:\n\t\treturn tensor.New(tensor.Of(dt), tensor.WithShape(s...)), nil\n\tdefault:\n\t\terr = errors.Errorf(nyiTypeFail, \"MakeValue\", tt)\n\t\treturn\n\t}\n}\n\nfunc makeValueFromMem(t hm.Type, s tensor.Shape, mem tensor.Memory) (retVal Value, err error) {\n\tvar dt tensor.Dtype\n\tif dt, err = dtypeOf(t); err != nil {\n\t\treturn\n\t}\n\tif s.IsScalar() {\n\t\treturn makeScalarFromMem(dt, mem)\n\t}\n\n\tswitch tt := t.(type) {\n\tcase TensorType:\n\t\tmemsize := calcMemSize(dt, s)\n\t\treturn tensor.New(tensor.Of(dt), tensor.WithShape(s...), tensor.FromMemory(mem.Uintptr(), uintptr(memsize))), nil\n\tcase tensor.Dtype:\n\t\treturn makeScalarFromMem(tt, mem)\n\tdefault:\n\t\terr = errors.Errorf(nyiTypeFail, \"MakeValue\", tt)\n\t\treturn\n\t}\n}\n\nfunc makeScalarFromMem(dt tensor.Dtype, mem tensor.Memory) (retVal Value, err error) {\n\tswitch dt {\n\tcase tensor.Float64:\n\t\tretVal = (*F64)(unsafe.Pointer(mem.Uintptr()))\n\tcase tensor.Float32:\n\t\tretVal = (*F32)(unsafe.Pointer(mem.Uintptr()))\n\tcase tensor.Int:\n\t\tretVal = (*I)(unsafe.Pointer(mem.Uintptr()))\n\tcase tensor.Int64:\n\t\tretVal = (*I64)(unsafe.Pointer(mem.Uintptr()))\n\tcase tensor.Int32:\n\t\tretVal = (*I32)(unsafe.Pointer(mem.Uintptr()))\n\tcase tensor.Byte:\n\t\tretVal = (*U8)(unsafe.Pointer(mem.Uintptr()))\n\tcase tensor.Bool:\n\t\tretVal = (*B)(unsafe.Pointer(mem.Uintptr()))\n\tdefault:\n\t\terr = errors.Errorf(nyiTypeFail, \"makeScalarFromMem\", dt)\n\t}\n\treturn\n}\n\nfunc logicalSize(s tensor.Shape) int {\n\tif s.IsScalar() {\n\t\treturn 1\n\t}\n\treturn s.TotalSize()\n}\n\nfunc calcMemSize(dt tensor.Dtype, s tensor.Shape) int64 {\n\tvar elemSize int64\n\tif s.IsScalar() {\n\t\telemSize = 1\n\t} else {\n\t\telemSize = int64(s.TotalSize())\n\t}\n\tdtSize := int64(dt.Size())\n\treturn elemSize * dtSize\n}\n\n// ScalarAsTensor returns the tensor representation of a scalar. It is particularly useful as a \"reshape\" of tensors of sorts\n//\n// The Value passed in are either Scalar, tensor.Tensor, or *dualValue. Anything else will panic.\nfunc ScalarAsTensor(v Value, dims int, e tensor.Engine) Value {\n\tswitch a := v.(type) {\n\tcase Scalar:\n\t\tsh := make(tensor.Shape, dims)\n\t\tfor i := range sh {\n\t\t\tsh[i] = 1\n\t\t}\n\t\treturn tensor.New(tensor.WithShape(sh...), tensor.Of(a.Dtype()), tensor.FromMemory(a.Uintptr(), a.MemSize()), tensor.WithEngine(e))\n\tcase tensor.Tensor:\n\t\treturn a\n\tcase *dualValue:\n\t\tb := new(dualValue)\n\t\tb.Value = ScalarAsTensor(a.Value, dims, e)\n\t\tb.d = ScalarAsTensor(a.d, dims, e)\n\t\treturn b\n\tcase nil:\n\t\treturn nil\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"Unable to convert %v to Tensor\", v))\n\t}\n}\n"
        },
        {
          "name": "values_primitives.go",
          "type": "blob",
          "size": 10.84375,
          "content": "package gorgonia\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"unsafe\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\n// Scalar represents a scalar(non-array-based) value. Do note that it's the pointers of the scalar types (F64, F32, etc) that implement\n// the Scalar interface. The main reason is primarily due to optimizations with regards to memory allocation and copying for device interoperability.\ntype Scalar interface {\n\tValue\n\tisScalar() bool\n}\n\n// F64 represents a float64 value.\ntype F64 float64\n\n// F32 represents a float32 value.\ntype F32 float32\n\n// I represents a int value.\ntype I int\n\n// I64 represents a int64 value.\ntype I64 int64\n\n// I32 represents a int32 value.\ntype I32 int32\n\n// U8 represents a byte value.\ntype U8 byte\n\n// B represents a bool value.\ntype B bool\n\nfunc NewF64(v float64) *F64 { r := F64(v); return &r }\nfunc NewF32(v float32) *F32 { r := F32(v); return &r }\nfunc NewI(v int) *I         { r := I(v); return &r }\nfunc NewI64(v int64) *I64   { r := I64(v); return &r }\nfunc NewI32(v int32) *I32   { r := I32(v); return &r }\nfunc NewU8(v byte) *U8      { r := U8(v); return &r }\nfunc NewB(v bool) *B        { r := B(v); return &r }\n\n/* Shape() */\n\n// Shape returns a scalar shape for all scalar values\nfunc (v *F64) Shape() tensor.Shape { return scalarShape }\n\n// Shape returns a scalar shape for all scalar values\nfunc (v *F32) Shape() tensor.Shape { return scalarShape }\n\n// Shape returns a scalar shape for all scalar values\nfunc (v *I) Shape() tensor.Shape { return scalarShape }\n\n// Shape returns a scalar shape for all scalar values\nfunc (v *I64) Shape() tensor.Shape { return scalarShape }\n\n// Shape returns a scalar shape for all scalar values\nfunc (v *I32) Shape() tensor.Shape { return scalarShape }\n\n// Shape returns a scalar shape for all scalar values\nfunc (v *U8) Shape() tensor.Shape { return scalarShape }\n\n// Shape returns a scalar shape for all scalar values\nfunc (v *B) Shape() tensor.Shape { return scalarShape }\n\n// Size returns 0 for all scalar Values\nfunc (v *F64) Size() int { return 0 }\n\n// Size returns 0 for all scalar Values\nfunc (v *F32) Size() int { return 0 }\n\n// Size returns 0 for all scalar Values\nfunc (v *I) Size() int { return 0 }\n\n// Size returns 0 for all scalar Values\nfunc (v *I64) Size() int { return 0 }\n\n// Size returns 0 for all scalar Values\nfunc (v *I32) Size() int { return 0 }\n\n// Size returns 0 for all scalar Values\nfunc (v *U8) Size() int { return 0 }\n\n// Size returns 0 for all scalar Values\nfunc (v *B) Size() int { return 0 }\n\n/* Data() */\n\n// Data returns the original representation of the Value\nfunc (v *F64) Data() interface{} { return v.any() }\n\n// Data returns the original representation of the Value\nfunc (v *F32) Data() interface{} { return v.any() }\n\n// Data returns the original representation of the Value\nfunc (v *I) Data() interface{} { return v.any() }\n\n// Data returns the original representation of the Value\nfunc (v *I64) Data() interface{} { return v.any() }\n\n// Data returns the original representation of the Value\nfunc (v *I32) Data() interface{} { return v.any() }\n\n// Data returns the original representation of the Value\nfunc (v *U8) Data() interface{} { return v.any() }\n\n// Data returns the original representation of the Value\nfunc (v *B) Data() interface{} { return v.any() }\n\nfunc (v *F64) any() float64 { return float64(*v) }\nfunc (v *F32) any() float32 { return float32(*v) }\nfunc (v *I) any() int       { return int(*v) }\nfunc (v *I64) any() int64   { return int64(*v) }\nfunc (v *I32) any() int32   { return int32(*v) }\nfunc (v *U8) any() byte     { return byte(*v) }\nfunc (v *B) any() bool      { return bool(*v) }\n\n/* implements fmt.Formatter */\n\n// Format implements fmt.Formatter\nfunc (v *F64) Format(s fmt.State, c rune) { formatScalar(v, s, c) }\n\n// Format implements fmt.Formatter\nfunc (v *F32) Format(s fmt.State, c rune) { formatScalar(v, s, c) }\n\n// Format implements fmt.Formatter\nfunc (v *I) Format(s fmt.State, c rune) { formatScalar(v, s, c) }\n\n// Format implements fmt.Formatter\nfunc (v *I64) Format(s fmt.State, c rune) { formatScalar(v, s, c) }\n\n// Format implements fmt.Formatter\nfunc (v *I32) Format(s fmt.State, c rune) { formatScalar(v, s, c) }\n\n// Format implements fmt.Formatter\nfunc (v *U8) Format(s fmt.State, c rune) { formatScalar(v, s, c) }\n\n// Format implements fmt.Formatter\nfunc (v *B) Format(s fmt.State, c rune) { formatScalar(v, s, c) }\n\n/* Dtype() */\n\n// Dtype  returns the Dtype of the value\nfunc (v *F64) Dtype() tensor.Dtype { return tensor.Float64 }\n\n// Dtype  returns the Dtype of the value\nfunc (v *F32) Dtype() tensor.Dtype { return tensor.Float32 }\n\n// Dtype  returns the Dtype of the value\nfunc (v *I) Dtype() tensor.Dtype { return tensor.Int }\n\n// Dtype  returns the Dtype of the value\nfunc (v *I64) Dtype() tensor.Dtype { return tensor.Int64 }\n\n// Dtype  returns the Dtype of the value\nfunc (v *I32) Dtype() tensor.Dtype { return tensor.Int32 }\n\n// Dtype  returns the Dtype of the value\nfunc (v *U8) Dtype() tensor.Dtype { return tensor.Byte }\n\n// Dtype  returns the Dtype of the value\nfunc (v *B) Dtype() tensor.Dtype { return tensor.Bool }\n\n/* isScalar */\n\nfunc (v *F64) isScalar() bool { return true }\nfunc (v *F32) isScalar() bool { return true }\nfunc (v *I) isScalar() bool   { return true }\nfunc (v *I64) isScalar() bool { return true }\nfunc (v *I32) isScalar() bool { return true }\nfunc (v *U8) isScalar() bool  { return true }\nfunc (v *B) isScalar() bool   { return true }\n\n/* Uintptr */\n\n// Uintptr satisfies the tensor.Memory interface\nfunc (v *F64) Uintptr() uintptr { return uintptr(unsafe.Pointer(v)) }\n\n// Uintptr satisfies the tensor.Memory interface\nfunc (v *F32) Uintptr() uintptr { return uintptr(unsafe.Pointer(v)) }\n\n// Uintptr satisfies the tensor.Memory interface\nfunc (v *I) Uintptr() uintptr { return uintptr(unsafe.Pointer(v)) }\n\n// Uintptr satisfies the tensor.Memory interface\nfunc (v *I64) Uintptr() uintptr { return uintptr(unsafe.Pointer(v)) }\n\n// Uintptr satisfies the tensor.Memory interface\nfunc (v *I32) Uintptr() uintptr { return uintptr(unsafe.Pointer(v)) }\n\n// Uintptr satisfies the tensor.Memory interface\nfunc (v *U8) Uintptr() uintptr { return uintptr(unsafe.Pointer(v)) }\n\n// Uintptr satisfies the tensor.Memory interface\nfunc (v *B) Uintptr() uintptr { return uintptr(unsafe.Pointer(v)) }\n\n/* MemSize */\n\n// MemSize satisfies the tensor.Memory interface\nfunc (v *F64) MemSize() uintptr { return 8 }\n\n// MemSize satisfies the tensor.Memory interface\nfunc (v *F32) MemSize() uintptr { return 4 }\n\n// MemSize satisfies the tensor.Memory interface\nfunc (v *I) MemSize() uintptr { return reflect.TypeOf(*v).Size() }\n\n// MemSize satisfies the tensor.Memory interface\nfunc (v *I64) MemSize() uintptr { return 8 }\n\n// MemSize satisfies the tensor.Memory interface\nfunc (v *I32) MemSize() uintptr { return 4 }\n\n// MemSize satisfies the tensor.Memory interface\nfunc (v *U8) MemSize() uintptr { return 1 }\n\n// MemSize satisfies the tensor.Memory interface\nfunc (v *B) MemSize() uintptr { return reflect.TypeOf(*v).Size() }\n\n/* Pointer */\n\n// Pointer returns the pointer as an unsafe.Pointer. Satisfies the tensor.Memory interface\nfunc (v *F64) Pointer() unsafe.Pointer { return unsafe.Pointer(v) }\n\n// Pointer returns the pointer as an unsafe.Pointer. Satisfies the tensor.Memory interface\nfunc (v *F32) Pointer() unsafe.Pointer { return unsafe.Pointer(v) }\n\n// Pointer returns the pointer as an unsafe.Pointer. Satisfies the tensor.Memory interface\nfunc (v *I) Pointer() unsafe.Pointer { return unsafe.Pointer(v) }\n\n// Pointer returns the pointer as an unsafe.Pointer. Satisfies the tensor.Memory interface\nfunc (v *I64) Pointer() unsafe.Pointer { return unsafe.Pointer(v) }\n\n// Pointer returns the pointer as an unsafe.Pointer. Satisfies the tensor.Memory interface\nfunc (v *I32) Pointer() unsafe.Pointer { return unsafe.Pointer(v) }\n\n// Pointer returns the pointer as an unsafe.Pointer. Satisfies the tensor.Memory interface\nfunc (v *U8) Pointer() unsafe.Pointer { return unsafe.Pointer(v) }\n\n// Pointer returns the pointer as an unsafe.Pointer. Satisfies the tensor.Memory interface\nfunc (v *B) Pointer() unsafe.Pointer { return unsafe.Pointer(v) }\n\nfunc formatScalar(v Scalar, s fmt.State, c rune) {\n\tvar buf bytes.Buffer\n\tvar ok bool\n\n\tbuf.WriteRune('%')\n\n\tvar width int\n\tif width, ok = s.Width(); ok {\n\t\tfmt.Fprintf(&buf, \"%d\", width)\n\t}\n\n\tvar prec int\n\tif prec, ok = s.Precision(); ok {\n\t\tfmt.Fprintf(&buf, \".%d\", prec)\n\t}\n\n\tswitch c {\n\tcase 's':\n\t\tbuf.WriteRune('v')\n\tcase 'd':\n\t\tswitch v.(type) {\n\t\tcase *F64, *F32, *U8, *B:\n\t\t\tbuf.WriteRune('v')\n\t\tdefault:\n\t\t\tbuf.WriteRune(c)\n\t\t}\n\tcase 'f', 'g':\n\t\tswitch v.(type) {\n\t\tcase *I, *I64, *I32, *U8, *B:\n\t\t\tbuf.WriteRune('v')\n\t\tdefault:\n\t\t\tbuf.WriteRune(c)\n\t\t}\n\tdefault:\n\t\tbuf.WriteRune(c)\n\t}\n\n\tif s.Flag('+') {\n\t\ts.Write([]byte(v.Dtype().String()))\n\t\ts.Write([]byte{' '})\n\t}\n\n\tfmt.Fprintf(s, buf.String(), v.Data())\n}\n\nfunc anyToScalar(any interface{}) (Scalar, tensor.Dtype) {\n\tswitch at := any.(type) {\n\tcase Scalar:\n\t\treturn at, at.Dtype()\n\tcase float64:\n\t\treturn NewF64(at), Float64\n\tcase float32:\n\t\treturn NewF32(at), Float32\n\tcase int:\n\t\treturn NewI(at), Int\n\tcase int32:\n\t\treturn NewI32(at), Int32\n\tcase int64:\n\t\treturn NewI64(at), Int64\n\tcase byte:\n\t\treturn NewU8(at), Byte\n\tcase bool:\n\t\treturn NewB(at), Bool\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"%v(%T) not scalar/not handled\", any, any))\n\t}\n}\n\nfunc anyToValue(any interface{}) (val Value, t hm.Type, dt tensor.Dtype, err error) {\n\tswitch a := any.(type) {\n\tcase Value:\n\t\tval = a\n\t\tt = TypeOf(a)\n\t\tdt = a.Dtype()\n\t\treturn\n\tcase float64, float32, int, int64, int32, byte, bool:\n\t\tval, dt = anyToScalar(any)\n\t\tt = dt\n\t\treturn\n\tcase F64:\n\t\treturn NewF64(float64(a)), tensor.Float64, tensor.Float64, nil\n\tcase F32:\n\t\treturn NewF32(float32(a)), tensor.Float32, tensor.Float32, nil\n\tcase I:\n\t\treturn NewI(int(a)), tensor.Int, tensor.Int, nil\n\tcase I64:\n\t\treturn NewI64(int64(a)), tensor.Int64, tensor.Int64, nil\n\tcase I32:\n\t\treturn NewI32(int32(a)), tensor.Int32, tensor.Int32, nil\n\tcase U8:\n\t\treturn NewU8(byte(a)), tensor.Uint8, tensor.Uint8, nil\n\tcase B:\n\t\treturn NewB(bool(a)), tensor.Bool, tensor.Bool, nil\n\tcase tensor.Tensor:\n\t\tval = a\n\t\tt = TypeOf(a)\n\t\tdt = a.Dtype()\n\t\treturn\n\tdefault:\n\t\terr = errors.Errorf(\"value %v of %T not yet handled\", any, any)\n\t\treturn\n\t}\n}\n\nfunc one(dt tensor.Dtype) Scalar {\n\tswitch dt {\n\tcase tensor.Float64:\n\t\treturn NewF64(float64(1))\n\tcase tensor.Float32:\n\t\treturn NewF32(float32(1))\n\tcase tensor.Int:\n\t\treturn NewI(1)\n\tcase tensor.Int32:\n\t\treturn NewI32(int32(1))\n\tcase tensor.Int64:\n\t\treturn NewI64(int64(1))\n\tcase tensor.Byte:\n\t\treturn NewU8(byte(1))\n\tcase tensor.Bool:\n\t\treturn NewB(true)\n\tdefault:\n\t\tpanic(\"Unhandled dtype\")\n\t}\n}\n\nfunc zero(dt tensor.Dtype) Scalar {\n\tswitch dt {\n\tcase tensor.Float64:\n\t\treturn NewF64(float64(0))\n\tcase tensor.Float32:\n\t\treturn NewF32(float32(0))\n\tcase tensor.Int:\n\t\treturn NewI(0)\n\tcase tensor.Int32:\n\t\treturn NewI32(int32(0))\n\tcase tensor.Int64:\n\t\treturn NewI64(int64(0))\n\tcase tensor.Byte:\n\t\treturn NewU8(byte(0))\n\tcase tensor.Bool:\n\t\treturn NewB(false)\n\tdefault:\n\t\tpanic(\"Unhandled dtype\")\n\t}\n}\n"
        },
        {
          "name": "values_utils.go",
          "type": "blob",
          "size": 5.48828125,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n\t\"unsafe\"\n)\n\n// TypeOf returns the Type of the value\nfunc TypeOf(v Value) hm.Type {\n\tswitch t := v.(type) {\n\tcase tensor.Tensor:\n\t\tdt, dim := tensorInfo(t)\n\t\treturn makeTensorType(dim, dt)\n\tcase Scalar:\n\t\treturn t.Dtype()\n\tcase Typer:\n\t\treturn t.Type()\n\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"TypeOf Not yet implemented for %v %T\", v, v))\n\t}\n}\n\nfunc typeCheckTypeOf(v Value) hm.Type {\n\tswitch t := v.(type) {\n\tcase tensor.Tensor:\n\t\tdt, dim := tensorInfo(t)\n\t\treturn newTensorType(dim, dt)\n\tcase Scalar:\n\t\treturn t.Dtype()\n\tcase Typer:\n\t\treturn t.Type()\n\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"TypeOf Not yet implemented for %v %T\", v, v))\n\t}\n}\n\n// ValueEq is the equality function for values\nfunc ValueEq(a, b Value) bool {\n\tif a == nil && b == nil {\n\t\treturn true\n\t}\n\tswitch at := a.(type) {\n\tcase Scalar:\n\t\tif bt, ok := b.(Scalar); ok {\n\t\t\treturn scalarEq(at, bt)\n\t\t}\n\t\treturn false\n\tcase tensor.Tensor:\n\t\tif bt, ok := b.(tensor.Tensor); ok {\n\t\t\treturn at.Eq(bt)\n\t\t\t//log.Printf(\"at.info %#v, bt.info %#v\", a.(*tensor.Dense).Info(), b.(*tensor.Dense).Info())\n\t\t}\n\t\treturn false\n\tcase ValueEqualer:\n\t\treturn at.ValueEq(b)\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"Not implemented yet, %T\", a))\n\t}\n}\n\n// ValueClose checks whether two values are close to one another. It's predominantly used as an alternative equality test for floats\nfunc ValueClose(a, b Value) bool {\n\tif a == nil && b == nil {\n\t\treturn true\n\t}\n\n\tswitch at := a.(type) {\n\tcase Scalar:\n\t\tif bt, ok := b.(Scalar); ok {\n\t\t\treturn scalarClose(at, bt)\n\t\t}\n\t\treturn false\n\tcase tensor.Tensor:\n\t\tif bt, ok := b.(tensor.Tensor); ok {\n\t\t\treturn tensorClose(at, bt)\n\t\t}\n\t\treturn false\n\tcase ValueCloser:\n\t\treturn at.ValueClose(b)\n\tdefault:\n\t\tpanic(\"Not implemented yet\")\n\t}\n}\n\n// CloneValue clones a value. For scalars, since Go copies scalars, it returns itself\nfunc CloneValue(v Value) (Value, error) {\n\tswitch vt := v.(type) {\n\tcase *F64:\n\t\tretVal := *vt\n\t\treturn &retVal, nil\n\tcase *F32:\n\t\tretVal := *vt\n\t\treturn &retVal, nil\n\tcase *I:\n\t\tretVal := *vt\n\t\treturn &retVal, nil\n\tcase *I32:\n\t\tretVal := *vt\n\t\treturn &retVal, nil\n\tcase *I64:\n\t\tretVal := *vt\n\t\treturn &retVal, nil\n\tcase *U8:\n\t\tretVal := *vt\n\t\treturn &retVal, nil\n\tcase *B:\n\t\tretVal := *vt\n\t\treturn &retVal, nil\n\tcase tensor.Tensor:\n\t\treturn vt.Clone().(*tensor.Dense), nil\n\tcase CloneErrorer:\n\t\tret, err := vt.Clone()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tretVal, ok := ret.(Value)\n\t\tif !ok {\n\t\t\treturn nil, errors.Errorf(\"Cloner is not a value: %v %T\", v, v)\n\t\t}\n\t\treturn retVal, nil\n\tcase Cloner:\n\t\treturn vt.Clone().(Value), nil\n\tdefault:\n\t\treturn nil, errors.Errorf(\"Unable to clone value of type %T\", v)\n\t}\n}\n\n// ZeroValue returns the zero value of a type\nfunc ZeroValue(v Value) Value {\n\tswitch vt := v.(type) {\n\tcase *F64:\n\t\t*vt = 0\n\t\treturn vt\n\tcase *F32:\n\t\t*vt = 0\n\t\treturn vt\n\tcase *I:\n\t\t*vt = 0\n\t\treturn vt\n\tcase *I32:\n\t\t*vt = 0\n\t\treturn vt\n\tcase *I64:\n\t\t*vt = 0\n\t\treturn vt\n\tcase *U8:\n\t\t*vt = 0\n\t\treturn vt\n\tcase *B:\n\t\t*vt = false\n\t\treturn vt\n\tcase tensor.Tensor:\n\t\tvt.Zero()\n\t\treturn vt\n\tcase ZeroValuer:\n\t\treturn vt.ZeroValue()\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"Cannot return zero value of %T\", v))\n\t}\n}\n\n// Copy copies the src values into dest values. For scalars, it just returns itself\nfunc Copy(dest, src Value) (Value, error) {\n\tvar ok bool\n\tswitch srcT := src.(type) {\n\tcase *F64:\n\t\tvar destS *F64\n\t\tif destS, ok = dest.(*F64); !ok {\n\t\t\treturn nil, errors.Errorf(\"Expected dest to be *F64. Got %T instead\", dest)\n\t\t}\n\t\t*destS = *srcT\n\t\treturn destS, nil\n\tcase *F32:\n\t\tvar destS *F32\n\t\tif destS, ok = dest.(*F32); !ok {\n\t\t\treturn nil, errors.Errorf(\"Expected dest to be *F32. Got %T instead\", dest)\n\t\t}\n\t\t*destS = *srcT\n\t\treturn destS, nil\n\tcase *I:\n\t\tvar destS *I\n\t\tif destS, ok = dest.(*I); !ok {\n\t\t\treturn nil, errors.Errorf(\"Expected dest to be *I) . Got %T instead\", dest)\n\t\t}\n\t\t*destS = *srcT\n\t\treturn destS, nil\n\tcase *I64:\n\t\tvar destS *I64\n\t\tif destS, ok = dest.(*I64); !ok {\n\t\t\treturn nil, errors.Errorf(\"Expected dest to be *I64. Got %T instead\", dest)\n\t\t}\n\t\t*destS = *srcT\n\t\treturn destS, nil\n\tcase *I32:\n\t\tvar destS *I32\n\t\tif destS, ok = dest.(*I32); !ok {\n\t\t\treturn nil, errors.Errorf(\"Expected dest to be *I32. Got %T instead\", dest)\n\t\t}\n\t\t*destS = *srcT\n\t\treturn destS, nil\n\tcase *U8:\n\t\tvar destS *U8\n\t\tif destS, ok = dest.(*U8); !ok {\n\t\t\treturn nil, errors.Errorf(\"Expected dest to be *U8). Got %T instead\", dest)\n\t\t}\n\t\t*destS = *srcT\n\t\treturn destS, nil\n\tcase *B:\n\t\tvar destS *B\n\t\tif destS, ok = dest.(*B); !ok {\n\t\t\treturn nil, errors.Errorf(\"Expected dest to be *B) . Got %T instead\", dest)\n\t\t}\n\t\t*destS = *srcT\n\t\treturn destS, nil\n\tcase tensor.Tensor:\n\t\tvar destT tensor.Tensor\n\t\tif destT, ok = dest.(tensor.Tensor); !ok {\n\t\t\treturn nil, errors.Errorf(\"Expected dest to be a tensor.Tensor. Got %T instead\", dest)\n\t\t}\n\t\terr := tensor.Copy(destT, srcT)\n\t\treturn dest, err\n\tcase CopierTo:\n\t\terr := srcT.CopyTo(dest)\n\t\treturn dest, err\n\tdefault:\n\t\tvar copyFrom CopierFrom\n\t\tif copyFrom, ok = dest.(CopierFrom); ok {\n\t\t\terr := copyFrom.CopyFrom(src)\n\t\t\treturn dest, err\n\t\t}\n\t\treturn nil, errors.Errorf(\"Unable to copy value of type %T into value of type %T\", src, dest)\n\t}\n}\n\nfunc setEngine(v Value, e tensor.Engine) {\n\tswitch vv := v.(type) {\n\tcase *dualValue:\n\t\tsetEngine(vv.Value, e)\n\t\tsetEngine(vv.d, e)\n\tcase tensor.Tensor:\n\t\ttensor.WithEngine(e)(vv)\n\t}\n}\n\n// I think that all values are supposed to have the pointer method but they don't seem to so to just fix this for now this appears to work\nfunc valueToPointer(v Value) unsafe.Pointer{\n\treturn unsafe.Pointer(v.Uintptr())\n}"
        },
        {
          "name": "values_utils_test.go",
          "type": "blob",
          "size": 0.6279296875,
          "content": "package gorgonia\n\nimport (\n\t\"testing\"\n\n\t\"gorgonia.org/tensor\"\n)\n\nvar cloneValTests = []Value{\n\t// prims\n\tNewF64(10.0),\n\tNewF32(10.0),\n\tNewI(10),\n\tNewI64(10),\n\tNewI32(10),\n\tNewU8(10),\n\tNewB(true),\n\n\ttensor.New(tensor.Of(tensor.Float64), tensor.WithShape(2, 4, 6)),\n\ttensor.New(tensor.Of(tensor.Float32), tensor.WithShape(2, 4, 6)),\n}\n\nfunc TestCloneValue(t *testing.T) {\n\tfor _, cvts := range cloneValTests {\n\t\tv, err := CloneValue(cvts)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\n\t\tif v == cvts {\n\t\t\tt.Errorf(\"Expected values to have different pointers. Got %p == %p\", v, cvts)\n\t\t}\n\n\t\tif !ValueEq(cvts, v) {\n\t\t\tt.Errorf(\"Cloning failed\")\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "vm.go",
          "type": "blob",
          "size": 7.9716796875,
          "content": "package gorgonia\n\nimport (\n\t\"bytes\"\n\t\"log\"\n\n\t\"gorgonia.org/tensor\"\n)\n\n// VM represents a structure that can execute a graph or program. There are two VMs (both unexported):\n//\t\t- *tapeMachine\n//\t\t- *lispMachine\n//\n// The *tapeMachine pre-compiles a graph into a list of instructions, then executes the instructions linearly and sequentially.\n// The main tradeoff is dynamism. Graphs cannot be dynamically created on the fly as a re-compilation process is required\n// (and compilation is relatively expensive). However, graphs executed with the *tapeMachine run much faster as plenty of optimizations\n// has been done in the code generation stage.\n//\n// The *lispMachine allows for graphs to be dynamically built and executed upon. The tradeoff is that executing a graph on *lispMachine\n// is generally slower than on *tapeMachine, given the same static \"image\" of a graph.\ntype VM interface {\n\tRunAll() error\n\tReset()\n\n\t// Close closes all the machine resources (CUDA, if any, loggers if any)\n\tClose() error\n}\n\nconst (\n\tfwdOnly byte = iota\n\tbwdOnly\n\twatchNaN\n\twatchInf\n\twatchPointer\n\tallocVals\n\tspare2 // spare2 = trace in tapeVM,\n\tspare3 // spare3 = bindDV in tapeVM, manualRootGrad in LispVM\n\twatchAll\n)\n\n// VMOpt is a VM creation option\ntype VMOpt func(m VM)\n\n// EvalMode enables the eval mode for the VM and graph\nfunc EvalMode() VMOpt {\n\treturn func(m VM) {\n\t\tswitch v := m.(type) {\n\t\tcase *lispMachine:\n\t\t\tv.evalMode = true\n\t\tcase *tapeMachine:\n\t\t\tv.evalMode = true\n\t\tdefault:\n\t\t\tpanic(nyi(\"EvalMode\", v))\n\t\t}\n\t}\n}\n\n// WithLogger creates a VM with the supplied logger.\nfunc WithLogger(logger *log.Logger) VMOpt {\n\tf := func(m VM) {\n\t\tif logger == nil {\n\t\t\treturn\n\t\t}\n\t\tswitch v := m.(type) {\n\t\tcase *lispMachine:\n\t\t\tv.logger = logger\n\t\t\tv.buf = new(bytes.Buffer)\n\t\tcase *tapeMachine:\n\t\t\tv.logger = logger\n\t\t\tv.buf = new(bytes.Buffer)\n\t\tdefault:\n\t\t\tpanic(nyi(\"WithLogger\", v))\n\t\t}\n\t}\n\treturn f\n}\n\n// WithValueFmt defines how the logger will output the values. It defaults to \"%3.3f\"\nfunc WithValueFmt(format string) VMOpt {\n\tf := func(m VM) {\n\t\tswitch v := m.(type) {\n\t\tcase *lispMachine:\n\t\t\tv.valueFmt = format\n\t\tcase *tapeMachine:\n\t\t\tv.valueFmt = format\n\t\tdefault:\n\t\t\tpanic(nyi(\"WithValueFmt\", v))\n\t\t}\n\t}\n\treturn f\n}\n\n// WithWatchlist creates a VM with a watchlist. When the execution touches the things in the watchlist, the VM's logger will the log it.\n// This allows for watching and finetuning of the algorithm. When nothing is passed in, then the VM will default to watching and logging every single\n// execution object.\n//\n// The watchlist allows for different things to be watched, depending on VM type:\n//\t\t*lispMachine will ONLY take *Node\n//\t\t*tapeMachine will take int (for register IDs) or *Node.\nfunc WithWatchlist(list ...interface{}) VMOpt {\n\tf := func(m VM) {\n\t\tswitch v := m.(type) {\n\t\tcase *lispMachine:\n\t\t\tif len(list) == 0 {\n\t\t\t\tv.doWatchAll()\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tfor _, item := range list {\n\t\t\t\tn := item.(*Node) // will panic if node is not passed in. This is expected behaviour.\n\t\t\t\tv.watchlist = append(v.watchlist, n)\n\t\t\t}\n\t\tcase *tapeMachine:\n\t\t\tif len(list) == 0 {\n\t\t\t\tv.doWatchAll()\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tfor _, item := range list {\n\t\t\t\tswitch i := item.(type) {\n\t\t\t\tcase int:\n\t\t\t\t\tv.watchRegs = append(v.watchRegs, register{id: i})\n\t\t\t\tcase NodeID:\n\t\t\t\t\tv.watchNodeIDs = append(v.watchNodeIDs, i)\n\t\t\t\tcase *Node:\n\t\t\t\t\tv.watchNodes = append(v.watchNodes, i)\n\t\t\t\tdefault:\n\t\t\t\t\tpanic(\"WithWatchlist only works with register ids or nodes\")\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\tpanic(nyi(\"WithWatchlist\", v))\n\t\t}\n\t}\n\treturn f\n}\n\n// WithNaNWatch creates a VM that will watch for NaNs when executing. This slows the execution down.\nfunc WithNaNWatch() VMOpt {\n\tf := func(m VM) {\n\t\tswitch v := m.(type) {\n\t\tcase *lispMachine:\n\t\t\tv.doWatchNaN()\n\t\tcase *tapeMachine:\n\t\t\tv.doWatchNaN()\n\t\tdefault:\n\t\t\tpanic(nyi(\"withNaNWatch\", v))\n\t\t}\n\t}\n\treturn f\n}\n\n// WithInfWatch creates a VM that will watch for Infs when executing. It watches for +Inf, -Inf and Inf. No choice there. This slows the execution down.\nfunc WithInfWatch() VMOpt {\n\tf := func(m VM) {\n\t\tswitch v := m.(type) {\n\t\tcase *lispMachine:\n\t\t\tv.doWatchInf()\n\t\tcase *tapeMachine:\n\t\t\tv.doWatchInf()\n\t\tdefault:\n\t\t\tpanic(nyi(\"withInfWatch\", v))\n\t\t}\n\t}\n\treturn f\n}\n\n// WithPointerWatch creates a VM that will watch for pointer clashes when executing. This slows the execution down and it's only recommended for gorgonia development.\nfunc WithPointerWatch() VMOpt {\n\tf := func(m VM) {\n\t\tswitch v := m.(type) {\n\t\tcase *lispMachine:\n\t\t\tpanic(\"pointer watch not supported by the Lisp Machine yet\")\n\t\tcase *tapeMachine:\n\t\t\tv.doWatchPointer()\n\t\tdefault:\n\t\t\tpanic(nyi(\"withInfWatch\", v))\n\t\t}\n\t}\n\treturn f\n}\n\n// ExecuteFwdOnly creates a VM that will execute a graph forwards only - it will not do back propagation.\n// This option is only for *lispMachine. Try it on any other VMs and it will panic.\nfunc ExecuteFwdOnly() VMOpt {\n\tf := func(m VM) {\n\t\tswitch v := m.(type) {\n\t\tcase *lispMachine:\n\t\t\tv.doExecFwd()\n\t\t\tv.dontExecBwd()\n\t\tdefault:\n\t\t\tpanic(nyi(\"ExecuteFwdOnly\", v))\n\t\t}\n\t}\n\treturn f\n}\n\n// ExecuteBwdOnly creates a VM that will execute a graph by doing back propagation only.\n// The assumption is of course, that the forward graph has already been executed, and there\n// are already values associated with the nodes.\n// This option is only for *lispMachine. Try it on any other VMs and it will panic.\nfunc ExecuteBwdOnly() VMOpt {\n\tf := func(m VM) {\n\t\tswitch v := m.(type) {\n\t\tcase *lispMachine:\n\t\t\tv.doExecBwd()\n\t\t\tv.dontExecFwd()\n\t\tdefault:\n\t\t\tpanic(nyi(\"ExecuteBwdOnly\", v))\n\t\t}\n\t}\n\treturn f\n}\n\n// LogFwd logs the forward execution of a graph.\n// This option is only for *lispMachine. Try it on any other VMs and it will panic.\nfunc LogFwd() VMOpt {\n\tf := func(m VM) {\n\t\tswitch v := m.(type) {\n\t\tcase *lispMachine:\n\t\t\tv.doLogFwd()\n\t\tdefault:\n\t\t\tpanic(nyi(\"LogFwdOnly\", v))\n\t\t}\n\t}\n\treturn f\n}\n\n// LogBwd logs the backwards execution of a graph.\n// This option is only for *lispMachine. Try it on any other VMs and it will panic.\nfunc LogBwd() VMOpt {\n\tf := func(m VM) {\n\t\tswitch v := m.(type) {\n\t\tcase *lispMachine:\n\t\t\tv.doLogBwd()\n\t\tdefault:\n\t\t\tpanic(nyi(\"LogBwdOnly\", v))\n\t\t}\n\t}\n\treturn f\n}\n\n// LogBothDir logs both directions of the execution of the graph.\n// This option is only available for *lispMachine.\nfunc LogBothDir() VMOpt {\n\tf := func(m VM) {\n\t\tswitch v := m.(type) {\n\t\tcase *lispMachine:\n\t\t\tv.doLogFwd()\n\t\t\tv.doLogBwd()\n\t\tdefault:\n\t\t\tpanic(nyi(\"LogBothDir\", v))\n\t\t}\n\t}\n\treturn f\n}\n\n// TraceExec is an option for *tapeMachine only.\n// It stores an immutable copy of the executed value into the node, instead of a mutable value, which may be clobbered\nfunc TraceExec() VMOpt {\n\tf := func(m VM) {\n\t\tswitch v := m.(type) {\n\t\tcase *tapeMachine:\n\t\t\tv.doTrace()\n\t\tdefault:\n\t\t\tpanic(nyi(\"TraceExec\", v))\n\t\t}\n\t}\n\treturn f\n}\n\n// BindDualValues is an option for *tapeMachine only.\n// This is useful to set when using a Solver\nfunc BindDualValues(nodes ...*Node) VMOpt {\n\tf := func(m VM) {\n\t\tswitch v := m.(type) {\n\t\tcase *tapeMachine:\n\t\t\tv.doBindDV()\n\t\t\tv.bindNodesDV = append(v.bindNodesDV, nodes...)\n\t\t\tv.bindNodesDV = v.bindNodesDV.Set()\n\t\tdefault:\n\t\t\t// on by default for LispMachine\n\t\t}\n\t}\n\treturn f\n}\n\n// WithPrecompiled is an option to pass in compiled programs.\n// This is useful for users who use the CompileFunction function\nfunc WithPrecompiled(prog *program, locMap map[*Node]register) VMOpt {\n\tf := func(m VM) {\n\t\tswitch v := m.(type) {\n\t\tcase *tapeMachine:\n\t\t\tv.p = prog\n\t\t\tv.locMap = locMap\n\t\t\tv.cpumem = make([]Value, prog.cpulocs)\n\t\t\tv.gpumem = make([]Value, prog.gpulocs)\n\t\tdefault:\n\t\t\t// no op\n\t\t}\n\t}\n\treturn f\n}\n\n// WithManualGradient allows the user to set the gradient of the root, before backprop. The root gradients should be set using the SetDeriv method\nfunc WithManualGradient() VMOpt {\n\tf := func(m VM) {\n\t\tswitch v := m.(type) {\n\t\tcase *lispMachine:\n\t\t\tv.allowSetRootGrad()\n\t\tdefault:\n\t\t\t// noop\n\t\t}\n\t}\n\treturn f\n}\n\n// WithEngine sets the tensor engine for computation inside the VM.\nfunc WithEngine(e tensor.Engine) VMOpt {\n\tf := func(m VM) {\n\t\tswitch v := m.(type) {\n\t\tcase *lispMachine:\n\t\t\tv.setEngine(e)\n\t\tcase *tapeMachine:\n\t\t\tv.setEngine(e)\n\t\t}\n\t}\n\treturn f\n}\n"
        },
        {
          "name": "vm_genera.go",
          "type": "blob",
          "size": 15.8896484375,
          "content": "package gorgonia\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"log\"\n\t\"runtime\"\n\t\"strings\"\n\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\ntype lispMachine struct {\n\tExternMetadata\n\tg *ExprGraph\n\tq []adInstr // a to-do list of differentiation instructions\n\n\t// device stuff\n\tcpumem int64\n\tgpumem []int64 // gpumem is indexed by gpuid\n\n\t// state stuff, to allow continuation\n\tsorted Nodes\n\tdf     *dataflow\n\tfwd    int\n\tbwd    int\n\n\t// logging stuff\n\twatchlist Nodes\n\tlogger    *log.Logger\n\tbuf       *bytes.Buffer\n\tvalueFmt  string\n\ttabcount  int\n\tlogFlags  uint16\n\n\trunFlags     uint16 // supposed to go into state stuff.  Placed here for better compacting of struct\n\tcheckedRoots bool   // supposed to go into state stuff.\n\tevalMode     bool\n}\n\n// NewLispMachine creates a VM that executes the graph as it is traversed. Depending on the VMOpts passed in\n// this VM is also capable of performing automatic differentiation.\nfunc NewLispMachine(g *ExprGraph, opts ...VMOpt) *lispMachine {\n\trunFlags := (uint16(0) | (uint16(1) << fwdOnly)) | (1 << bwdOnly) // run fwd and backwards\n\tm := &lispMachine{\n\t\tg:        g,\n\t\tfwd:      -1,\n\t\tbwd:      -1,\n\t\tvalueFmt: \"%3.3f\",\n\t\tlogFlags: 0x0,      // log nothing\n\t\trunFlags: runFlags, // run only fwd and bwd\n\t}\n\tm.Engine = StandardEngine{}\n\n\tfor _, opt := range opts {\n\t\topt(m)\n\t}\n\tif err := m.init(); err != nil {\n\t\tpanic(err)\n\t}\n\n\tfor _, n := range g.AllNodes() {\n\t\tsetEngine(n.boundTo, m.Engine)\n\n\t\tif op, ok := n.op.(TrainModeOp); ok {\n\t\t\top.SetTraining(!m.evalMode)\n\t\t}\n\t}\n\n\truntime.SetFinalizer(m, finalizeLispMachine)\n\treturn m\n}\n\nfunc (m *lispMachine) logBwd() bool { return (m.logFlags>>bwdOnly)&uint16(1) == 1 }\nfunc (m *lispMachine) doLogBwd()    { m.logFlags |= uint16(1) << bwdOnly }\nfunc (m *lispMachine) dontLogBwd()  { m.logFlags &= (^(uint16(1) << bwdOnly)) }\nfunc (m *lispMachine) runBwd() bool { return m.runFlags>>bwdOnly&uint16(1) == 1 }\nfunc (m *lispMachine) doExecBwd()   { m.runFlags |= uint16(1) << bwdOnly }\nfunc (m *lispMachine) dontExecBwd() { m.runFlags &= (^(uint16(1) << bwdOnly)) }\n\nfunc (m *lispMachine) logFwd() bool { return (m.logFlags>>fwdOnly)&uint16(1) == 1 }\nfunc (m *lispMachine) doLogFwd()    { m.logFlags |= uint16(1) << fwdOnly }\nfunc (m *lispMachine) dontLogFwd()  { m.logFlags &= (^(uint16(1) << fwdOnly)) }\nfunc (m *lispMachine) runFwd() bool { return m.runFlags>>fwdOnly&uint16(1) == 1 }\nfunc (m *lispMachine) doExecFwd()   { m.runFlags |= uint16(1) << fwdOnly }\nfunc (m *lispMachine) dontExecFwd() { m.runFlags &= (^(uint16(1) << fwdOnly)) }\n\nfunc (m *lispMachine) watchNaN() bool { return (m.runFlags>>watchNaN)&uint16(1) == 1 }\nfunc (m *lispMachine) doWatchNaN()    { m.runFlags |= uint16(1) << watchNaN }\nfunc (m *lispMachine) dontWatchNaN()  { m.runFlags &= (^(uint16(1) << watchNaN)) }\n\nfunc (m *lispMachine) watchInf() bool { return (m.runFlags>>watchInf)&uint16(1) == 1 }\nfunc (m *lispMachine) doWatchInf()    { m.runFlags |= uint16(1) << watchInf }\nfunc (m *lispMachine) dontWatchInf()  { m.runFlags &= (^(uint16(1) << watchInf)) }\n\nfunc (m *lispMachine) watchAll() bool { return (m.logFlags>>watchAll)&uint16(1) == 1 }\nfunc (m *lispMachine) doWatchAll()    { m.logFlags |= (uint16(1) << watchAll) }\nfunc (m *lispMachine) dontWatchAll()  { m.logFlags &= (^(uint16(1) << watchAll)) }\n\nfunc (m *lispMachine) dealloc() bool { return (m.runFlags>>allocVals)&uint16(1) == 1 }\nfunc (m *lispMachine) doDealloc()    { m.runFlags |= uint16(1) << allocVals }\nfunc (m *lispMachine) dontDealloc()  { m.runFlags &= (^(uint16(1) << allocVals)) }\n\nfunc (m *lispMachine) setRootGrad() bool    { return (m.runFlags>>spare3)&uint16(1) == 1 }\nfunc (m *lispMachine) allowSetRootGrad()    { m.runFlags |= uint16(1) << spare3 }\nfunc (m *lispMachine) disallowSetRootGrad() { m.runFlags &= (^(uint16(1) << spare3)) }\n\nfunc (m *lispMachine) Reset() {\n\tm.fwd = len(m.sorted) - 1\n\tm.bwd = len(m.q) - 1\n}\n\nfunc (m *lispMachine) Close() error {\n\tfinalizeLispMachine(m)\n\treturn nil\n}\n\n// RunAll traverses a graph and executes every node. Backpropagation is done if necessary\nfunc (m *lispMachine) RunAll() (err error) {\n\truntime.LockOSThread()\n\tdefer runtime.UnlockOSThread()\n\n\tif err = m.checkRoots(); err != nil {\n\t\treturn errors.Wrap(err, \"Could not checkRoots()\")\n\t}\n\n\tif m.runBwd() {\n\t\tdefer func() {\n\t\t\tm.q = nil // this needs to be nil'd or else there would still be references to m. Then there won't be any garbage being collected\n\t\t}()\n\t}\n\n\tworkAvailable := m.WorkAvailable()\n\tsyncChan := m.ExternMetadata.Sync()\n\terrChan := make(chan error)\n\tdoneChan := make(chan struct{})\n\n\tgo m.runall(errChan, doneChan)\n\tfor {\n\t\tselect {\n\t\tcase synchronous := <-workAvailable:\n\t\t\terr := m.ExternMetadata.DoWork()\n\t\t\tif err != nil {\n\t\t\t\tvar node *Node\n\t\t\t\tswitch {\n\t\t\t\tcase synchronous:\n\t\t\t\t\tif m.fwd < len(m.sorted) {\n\t\t\t\t\t\tnode = m.sorted[m.fwd]\n\t\t\t\t\t} else {\n\t\t\t\t\t\tnode = m.sorted[m.fwd-1]\n\t\t\t\t\t}\n\t\t\t\tdefault:\n\t\t\t\t\tif m.fwd-1 > 0 && m.fwd <= len(m.sorted) {\n\t\t\t\t\t\tnode = m.sorted[m.fwd-1]\n\t\t\t\t\t} else {\n\t\t\t\t\t\tnode = m.sorted[0]\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\terr = vmContextualError{\n\t\t\t\t\terror: errors.Wrapf(err, \"DoWork failed\"),\n\t\t\t\t\tnode:  node,\n\t\t\t\t\tinstr: m.fwd,\n\t\t\t\t}\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif synchronous {\n\t\t\t\tsyncChan <- struct{}{}\n\t\t\t}\n\t\tcase err = <-errChan:\n\t\t\tif m.fwd < len(m.sorted) {\n\t\t\t\terr = vmContextualError{\n\t\t\t\t\terror: errors.Wrapf(err, \"Running Node: %v\", m.sorted[m.fwd]),\n\t\t\t\t\tnode:  m.sorted[m.fwd],\n\t\t\t\t\tinstr: m.fwd,\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\treturn errors.Wrap(err, \"RunAll\")\n\t\tcase <-doneChan:\n\t\t\terr := m.ExternMetadata.DoWork()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\n// UnbindAll detaches the values from the node, allowing for them to be cleaned up the next GC cycle.\nfunc (m *lispMachine) UnbindAll() {\n\t// if m.dealloc() {\n\tfor _, n := range m.sorted {\n\t\tm.logf(\"dealloc n; %v %x %p\", n, n.Hashcode(), n)\n\t\tif !n.isInput() {\n\t\t\tn.unbind()\n\t\t}\n\t}\n\t// }\n}\n\n// LastRun returns the nodes and results from the last run. Additionally it returns whether backprop was done.\nfunc (m *lispMachine) LastRun() (n *Node, backprop bool) {\n\tif m.fwd < 0 && m.runBwd() {\n\t\tgoto backward\n\t} else if !m.runBwd() {\n\t\tn = m.sorted[0] // last to run\n\t\treturn\n\t} else {\n\t\tn = m.sorted[m.fwd]\n\t\treturn\n\t}\n\nbackward:\n\tbackprop = true\n\tif m.bwd < 0 {\n\t\tn = m.q[0].output\n\t\treturn\n\t}\n\tn = m.q[m.bwd].output\n\treturn\n}\n\n// check roots only applies if you want to run a backprop as well\nfunc (m *lispMachine) checkRoots() (err error) {\n\tif !m.checkedRoots && m.runBwd() {\n\t\tmachineLogf(\"Checking if provided graph is sensible\")\n\t\tm.logf(\"roots: %v\", m.g.Roots())\n\t\tfor _, root := range m.g.Roots() {\n\t\t\tswitch {\n\t\t\tcase m.setRootGrad() && !root.isStmt:\n\t\t\t\t// check root's value\n\t\t\t\t// if _, ok := root.boundTo.(*dualValue); !ok {\n\t\t\t\t// \terr = errors.Errorf(\"Expected root %v to have a boundTo of a dualValue\", root)\n\t\t\t\t// \treturn\n\t\t\t\t// }\n\t\t\tcase !m.setRootGrad() && !root.IsScalar() && !root.isStmt:\n\t\t\t\terr = errors.Errorf(\"Expected cost to be a scalar. Got %v with shape %v instead\", root, root.Shape())\n\t\t\t\tioutil.WriteFile(\"err.dot\", []byte(root.RestrictedToDot(2, 10)), 0644)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\treturn\n}\n\nfunc (m *lispMachine) prepGraph() (err error) {\n\tif m.sorted == nil {\n\t\tif m.sorted, err = Sort(m.g); err != nil {\n\t\t\treturn errors.Wrap(err, sortFail)\n\t\t}\n\t\treverseNodes(m.sorted)\n\t\tm.fwd = 0\n\t}\n\treturn\n}\n\nfunc (m *lispMachine) runall(errChan chan error, doneChan chan struct{}) {\n\tvar err error\n\tif !m.runFwd() {\n\t\tgoto backward\n\t}\n\n\tfor err = nil; err == nil && m.fwd < len(m.sorted); m.fwd++ {\n\t\terr = m.forward()\n\t}\n\n\tif err != nil {\n\t\terrChan <- err\n\t}\n\n\t// send a synchronous signal, do all (if any) CUDA work before continuing with backprop\n\tm.Signal()\n\nbackward:\n\tif !m.runBwd() {\n\t\tdoneChan <- struct{}{}\n\t\treturn\n\t}\n\n\tif m.bwd < 0 {\n\t\tm.bwd = len(m.q) - 1\n\t}\n\n\tfor err = nil; err == nil && m.bwd >= 0; m.bwd-- {\n\t\terr = m.backward()\n\t}\n\tif err != nil {\n\t\terrChan <- err\n\t}\n\tdoneChan <- struct{}{}\n}\n\nfunc (m *lispMachine) forward() (err error) {\n\tif m.fwd < 0 {\n\t\treturn nil // or err?\n\t}\n\tn := m.sorted[m.fwd]\n\n\tm.watchedLogf(\"n: %v | (%x) | %p\", n, n.id, n)\n\tm.enterLogScope()\n\tdefer m.leaveLogScope()\n\n\tdefer setEngine(n.boundTo, m.Engine)\n\n\tif !n.isStmt {\n\t\tswitch {\n\t\tcase n.isArg():\n\t\t\tmachineLogf(\"Unit() on input node\")\n\t\t\tif err = n.bind(dvUnit(n.boundTo)); err != nil {\n\t\t\t\treturn errors.Wrap(err, bindFail)\n\t\t\t}\n\t\t\treturn\n\t\tcase n.isRandom():\n\t\t\tmachineLogf(\"binding value of random node\")\n\t\t\tvar v Value\n\t\t\tif v, err = n.op.Do(); err != nil {\n\t\t\t\treturn errors.Wrapf(err, execFail, n.op, n)\n\t\t\t}\n\n\t\t\t// we wrap it in a dualValue, but we make it a constant\n\t\t\tif err = n.bind(dvUnit(v)); err != nil {\n\t\t\t\treturn errors.Wrap(err, bindFail)\n\t\t\t}\n\n\t\t\treturn\n\t\tdefault:\n\t\t\t// do nothihng\n\t\t}\n\t\tm.watchedLogf(m.valueFmt, n.boundTo)\n\t}\n\n\t// other wise it's time to execute the op\n\tm.logf(\"execute Op\")\n\tdev := n.dataOn\n\top := NewExternalOp(n.op, ExecutionContext{m, dev}, nil)\n\n\t// m.watchedLogf(\"Result of execution of this node would reside in %v\", dev)\n\tvar output *dualValue\n\n\tinputs := make([]*dualValue, len(n.children))\n\tchildren := n.children\n\n\tm.enterLogScope()\n\tfor i, child := range children {\n\t\tm.logf(\"child %d: %v %v\", i, child, child.Shape())\n\t\tif child.Device() == n.Device() {\n\t\t\tinputs[i] = child.boundTo.(*dualValue)\n\t\t\t// continue\n\t\t}\n\n\t\tvar allocV, allocD bool\n\t\tvar v, d Value\n\t\tif v, allocV, err = child.ValueOnDevice(dev, m); err != nil {\n\t\t\treturn errors.Wrapf(err, \"Unable to get Value on Device %v\", dev)\n\t\t}\n\t\tif d, allocD, err = child.GradOnDevice(dev, m); err != nil {\n\t\t\tif !child.isRandom() {\n\t\t\t\treturn errors.Wrapf(err, \"Unable to get Grad on Device %v\", dev)\n\t\t\t}\n\t\t\terr = nil\n\t\t}\n\n\t\tdv := borrowDV()\n\n\t\tdv.Value = v\n\t\tdv.d = d\n\t\tinputs[i] = dv\n\n\t\tdefer func() {\n\t\t\tif allocV {\n\t\t\t\tm.logf(\"Putting 0x%x |%T\", v.Uintptr(), v)\n\t\t\t\tm.PutValue(dev, v)\n\t\t\t}\n\t\t\tif allocD {\n\t\t\t\tm.PutValue(dev, d)\n\t\t\t}\n\t\t\tif allocV && allocD {\n\t\t\t\treturnDV(dv)\n\t\t\t}\n\t\t}()\n\t}\n\tm.leaveLogScope()\n\tm.watchedLogf(\"Before:\")\n\tm.watchedLogf(m.valueFmt, n.boundTo)\n\n\tswitch {\n\tcase (m.g.roots.Contains(n) || n.isRoot()) && !n.isStmt:\n\t\tmachineLogf(\"Applying op %v to root\", op)\n\t\tif n.boundTo == nil {\n\t\t\tmachineLogf(\"dvBindVar\")\n\t\t\tm.logf(\"dvBindVar\")\n\t\t\tif output, err = dvBindVar(op, inputs); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"Failed to bindVar\")\n\t\t\t}\n\t\t\tif err = n.bind(output); err != nil {\n\t\t\t\treturn errors.Wrap(err, bindFail)\n\t\t\t}\n\t\t} else {\n\t\t\tmachineLogf(\"dvBindVar0\")\n\t\t\tm.logf(\"dvBindVar0\")\n\t\t\tdv, ok := n.boundTo.(*dualValue)\n\t\t\tif !ok {\n\t\t\t\tdv = dvUnitVar(n.boundTo)\n\t\t\t\tn.boundTo = dv\n\t\t\t\t// panic(fmt.Sprintf(\"n not dual value %v\", n))\n\t\t\t}\n\t\t\tif err = dvBindVar0(op, dv, inputs); err != nil {\n\t\t\t\treturn errors.Wrapf(err, execFail, op, n)\n\t\t\t}\n\t\t}\n\n\tcase n.isStmt:\n\t\tswitch ot := n.op.(type) {\n\t\tcase readOp:\n\t\t\tmachineLogf(\"ReadOp: %v \", op)\n\t\t\tchild := children[0]\n\t\t\tchildVal := child.boundTo\n\t\t\tif child.Device() != CPU {\n\t\t\t\tm.Signal() // get work to be done first\n\n\t\t\t\tif dv, ok := n.children[0].boundTo.(*dualValue); ok {\n\t\t\t\t\t*ot.into = dv.Value\n\t\t\t\t} else {\n\t\t\t\t\t*ot.into = childVal\n\t\t\t\t}\n\n\t\t\t} else {\n\t\t\t\tif dv, ok := childVal.(*dualValue); ok {\n\t\t\t\t\t*ot.into = dv.Value\n\t\t\t\t} else {\n\t\t\t\t\t*ot.into = childVal\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\tcase n.boundTo == nil:\n\t\tm.watchedLogf(\"Fresh, unencountered node, so dvBind(%v)\", op)\n\t\tif dev != CPU {\n\t\t\tvar dt tensor.Dtype\n\t\t\tif dt, err = dtypeOf(n.t); err != nil {\n\t\t\t\treturn errors.Wrapf(err, dtypeExtractionFail, n.t)\n\t\t\t}\n\n\t\t\tvar mem tensor.Memory\n\t\t\tmemsize := calcMemSize(dt, n.shape)\n\t\t\tif mem, err = m.Get(dev, memsize); err != nil {\n\t\t\t\treturn errors.Wrapf(err, allocFail, memsize, dev)\n\t\t\t}\n\n\t\t\tvar reuse Value\n\t\t\tif reuse, err = makeValueFromMem(n.t, n.shape, mem); err != nil {\n\t\t\t\treturn errors.Wrapf(err, makeValueFail, n.t, n.shape)\n\t\t\t}\n\n\t\t\top.Prealloc = reuse\n\t\t}\n\n\t\tif output, err = dvBind(op, inputs); err != nil {\n\t\t\treturn errors.Wrapf(err, execFail, op, n)\n\t\t}\n\n\t\tif err = n.bind(output); err != nil {\n\t\t\treturn errors.Wrap(err, bindFail)\n\t\t}\n\n\tdefault:\n\t\tm.logf(\"bind(%v) with as much reuse as possible\", op)\n\t\t// reuse as much as possible\n\t\toutput := dvUnit(n.boundTo)\n\t\tif err = n.bind(output); err != nil {\n\t\t\treturn errors.Wrap(err, bindFail)\n\t\t}\n\n\t\tif dev != CPU {\n\t\t\top.Prealloc = output.Value\n\t\t}\n\n\t\terr = dvBind0(op, output, inputs)\n\t\tif _, ok := errors.Cause(err).(AutoDiffError); ok {\n\t\t\terr = nil\n\t\t} else if err != nil {\n\t\t\treturn errors.Wrapf(err, execFail, op, n)\n\t\t}\n\t}\n\tm.watchedLogf(\"After:\")\n\tm.watchedLogf(m.valueFmt, n.boundTo)\n\n\tif aop, ok := op.Op.(ADOp); ok && m.runBwd() {\n\t\tinstr := adInstr{\n\t\t\tADOp: aop,\n\t\t\tctx:  op.ExecutionContext,\n\n\t\t\tinputs: n.children, // this is correct.\n\t\t\toutput: n,\n\t\t}\n\t\tm.q = append(m.q, instr)\n\t}\n\tm.watchedLogf(\"Added to Queue\")\n\n\tif m.watchNaN() && !n.isStmt {\n\t\tif hasNaN(n.boundTo, dev) {\n\t\t\treturn errors.New(\"NaN found in value\")\n\t\t}\n\t}\n\n\treturn\n}\n\nfunc (m *lispMachine) backward() (err error) {\n\tif m.bwd < 0 {\n\t\treturn errors.New(\"no backprop queue\")\n\t}\n\tif m.bwd >= len(m.q) {\n\t\treturn errors.New(\"Nothing to backprop\")\n\t}\n\n\tinstr := m.q[m.bwd]\n\tm.watchedLogf(\"Differentiating op %v. Output: %v (%x)\", instr, instr.output, instr.output.Hashcode())\n\tm.enterLogScope()\n\tdefer m.leaveLogScope()\n\n\tm.watchedLogf(\"Inputs: %v\", instr.inputs)\n\tm.enterLogScope()\n\tfor _, in := range instr.inputs {\n\t\tm.watchedLogf(m.valueFmt, in.boundTo.(*dualValue).d)\n\t}\n\tm.leaveLogScope()\n\n\t// actual differentiation\n\tif err = instr.do(); err != nil {\n\t\treturn errors.Wrapf(err, autodiffFail, instr.ADOp)\n\t}\n\n\t// Make sure that all the engines of all the values are set to use the correct engine\n\tfor _, in := range instr.inputs {\n\t\tsetEngine(in.boundTo, m.Engine)\n\t}\n\tsetEngine(instr.output.boundTo, m.Engine)\n\n\tm.watchedLogf(\"After:\")\n\tm.enterLogScope()\n\tfor _, in := range instr.inputs {\n\t\tm.watchedLogf(m.valueFmt, in.boundTo.(*dualValue).d)\n\t}\n\n\tm.leaveLogScope()\n\n\tif m.watchNaN() {\n\t\tif hasNaN(instr.output.boundTo, instr.ctx.Device) {\n\t\t\treturn errors.New(\"NaN found in value\")\n\t\t}\n\n\t\tfor _, in := range instr.inputs {\n\t\t\tif hasNaN(in.boundTo, instr.ctx.Device) {\n\t\t\t\treturn errors.New(\"NaN found in value\")\n\t\t\t}\n\t\t}\n\t}\n\treturn\n}\n\nfunc (m *lispMachine) watchedLogf(format string, attrs ...interface{}) {\n\tif !m.logFwd() && !DEBUG {\n\t\tgoto backwards\n\t}\n\n\tif m.fwd >= 0 && m.fwd < len(m.sorted) {\n\t\tn := m.sorted[m.fwd]\n\t\tif m.watchlist.Contains(n) || m.watchAll() {\n\t\t\tm.logf(format, attrs...)\n\t\t}\n\t\treturn\n\t}\n\nbackwards:\n\tif !m.logBwd() && !DEBUG {\n\t\treturn\n\t}\n\n\tif m.bwd >= 0 {\n\t\tinstr := m.q[m.bwd]\n\t\twrite := m.watchlist.Contains(instr.output)\n\t\tif !write {\n\t\t\tfor _, in := range instr.inputs {\n\t\t\t\tif m.watchlist.Contains(in) {\n\t\t\t\t\twrite = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif write || m.watchAll() || DEBUG {\n\t\t\tm.logf(format, attrs...)\n\t\t}\n\t}\n}\n\nfunc (m *lispMachine) logf(format string, attrs ...interface{}) {\n\tswitch {\n\tcase machineDev, autodiffDev:\n\t\tif machineDev {\n\n\t\t\tmachineLogf(format, attrs...)\n\t\t} else {\n\t\t\tautodiffLogf(format, attrs...)\n\t\t}\n\n\t\tif m.logger != nil {\n\t\t\tgoto loggercase\n\t\t}\n\n\t\tbreak\n\n\tloggercase:\n\t\tfallthrough\n\tcase m.logger != nil:\n\t\ts := fmt.Sprintf(format, attrs...)\n\t\ts = strings.Replace(s, \"\\n\", m.buf.String(), -1)\n\t\tm.logger.Println(s)\n\t}\n}\n\nfunc (m *lispMachine) enterLogScope() {\n\tif DEBUG && machineDev {\n\t\tenterLogScope()\n\t}\n\tm.tabcount++\n\tif m.logger != nil {\n\t\treps := strings.Repeat(\"\\t\", m.tabcount)\n\t\tm.logger.SetPrefix(reps)\n\t\tm.buf.Reset()\n\t\tm.buf.WriteString(\"\\n\")\n\t\tm.buf.WriteString(reps)\n\t}\n}\n\nfunc (m *lispMachine) leaveLogScope() {\n\tif DEBUG && machineDev {\n\t\tleaveLogScope()\n\t}\n\tm.tabcount--\n\tif m.tabcount < 0 {\n\t\tm.tabcount = 0\n\t}\n\tif m.logger != nil {\n\t\treps := strings.Repeat(\"\\t\", m.tabcount)\n\t\tm.logger.SetPrefix(reps)\n\t\tm.buf.Reset()\n\t\tm.buf.WriteString(\"\\n\")\n\t\tm.buf.WriteString(reps)\n\t}\n}\n\n// adInstr is an autodifferentiation instruction\ntype adInstr struct {\n\tADOp\n\tctx ExecutionContext\n\n\tinputs Nodes\n\toutput *Node\n}\n\nfunc (instr adInstr) do() error {\n\tif instr.output.dataOn != CPU {\n\t\tfor _, in := range instr.inputs {\n\t\t\tif in.dataOn == CPU {\n\t\t\t\t// ensure everything gets executed in the GPU first\n\t\t\t\tinstr.ctx.Signal()\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\terr := instr.ADOp.DoDiff(instr.ctx, instr.inputs, instr.output)\n\t// logf(\"INPUTS:\")\n\t// for _, in := range instr.inputs {\n\t// \tlogf(\"%v\\n\", in.boundTo.(*dualValue).d)\n\t// }\n\treturn err\n}\n"
        },
        {
          "name": "vm_genera_cuda.go",
          "type": "blob",
          "size": 3.771484375,
          "content": "// +build cuda\n\npackage gorgonia\n\nimport (\n\t\"log\"\n\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc (m *lispMachine) init() error {\n\tif err := m.prepGraph(); err != nil {\n\t\treturn err\n\t}\n\n\t// VERY simple data analysis (even simpler than the one used in Compile)\n\t// using replaceWithSelf reduces the need for hashing, hence less work is required\n\t// However this also means that CSE won't be performed\n\tdf := newdataflow()\n\tdf.replaceWithSelf(m.sorted)\n\tdf.buildIntervals(m.sorted)\n\tdf.fixIntervalDevices(m.sorted)\n\tm.df = df\n\n\tif err := m.calcMemSize(); err != nil {\n\t\tlog.Printf(\"err1\")\n\t\treturn err\n\t}\n\n\tif len(m.gpumem) == 0 {\n\t\tm.ForceCPU()\n\t\treturn nil\n\t}\n\n\tif err := m.ExternMetadata.init(m.gpumem); err != nil {\n\t\tm.ExternMetadata.initFail()\n\t\treturn err\n\t}\n\tm.loadStdLib()\n\n\tif len(m.engines) == 0 {\n\t\tm.ForceCPU()\n\t}\n\treturn nil\n}\n\nfunc finalizeLispMachine(m *lispMachine) {\n\tm.ExternMetadata.cleanup()\n\tm.ExternMetadata.initFail()\n}\n\nfunc (m *lispMachine) WorkAvailable() <-chan bool {\n\tif m.ExternMetadata.WorkAvailable() == nil {\n\t\treturn nil\n\t}\n\treturn m.ExternMetadata.WorkAvailable()\n}\n\nfunc (m *lispMachine) calcMemSize() (err error) {\n\tcompileLogf(\"calcmemsize\")\n\tenterLogScope()\n\tdefer leaveLogScope()\n\tvar cpumem int64\n\tvar gpumem []int64\n\tfor _, n := range m.sorted {\n\t\tinterv := m.df.intervals[n]\n\t\tdev := interv.result.device\n\t\tcompileLogf(\"n: %v | %v\", n, interv)\n\n\t\tvar dt tensor.Dtype\n\t\tif dt, err = dtypeOf(n.t); err != nil {\n\t\t\tif n.isStmt {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn errors.Wrapf(err, \"Cannot calculate memsize of n(%v)\", n)\n\t\t}\n\t\tswitch {\n\t\tcase n.isArg():\n\t\t\tcpumem += calcMemSize(dt, n.Shape())\n\t\tcase n.isStmt:\n\t\tdefault:\n\t\t\t// if !n.op.ReturnsPtr() {\n\t\t\tif dev != CPU {\n\t\t\t\tif len(gpumem) < int(dev)+1 {\n\t\t\t\t\tdiff := int(dev) + 1 - len(gpumem)\n\t\t\t\t\tgpumem = append(gpumem, make([]int64, diff)...)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tswitch dev {\n\t\t\tcase CPU:\n\t\t\t\tcpumem += calcMemSize(dt, n.Shape())\n\t\t\tdefault:\n\t\t\t\tcompileLogf(\"n: %v. AddedDEF\", n)\n\t\t\t\tgpumem[int(dev)] += 4 * calcMemSize(dt, n.Shape())\n\t\t\t}\n\t\t\t// }\n\t\t}\n\t}\n\n\tm.cpumem = cpumem\n\tm.gpumem = gpumem\n\treturn nil\n}\n\nfunc (m *lispMachine) execDevTrans(op devTrans, n *Node, children Nodes) (err error) {\n\tchild := children[0]\n\tm.logf(\"DevTrans: %v | %v | %v\", op, n.boundTo, child.boundTo)\n\n\tvar dv *dualValue\n\tvar cv, cd, v, d Value\n\tif child.boundTo != nil {\n\t\tvar ok bool\n\t\tif dv, ok = child.boundTo.(*dualValue); ok {\n\t\t\tcv = dv.Value\n\t\t\tcd = dv.d\n\t\t} else {\n\t\t\tcv = child.boundTo\n\t\t}\n\t} else {\n\t\terr = errors.Errorf(\"Cannot execute transfer when there is no value in child\")\n\t\treturn\n\t}\n\n\tvar synchronous bool\n\tif op.to == CPU && op.from != CPU {\n\t\tsynchronous = true\n\t}\n\n\tif v, err = m.Transfer(op.to, op.from, cv, false); err != nil {\n\t\treturn\n\t}\n\n\tif cd != nil {\n\t\tif d, err = m.Transfer(op.to, op.from, cd, false); err != nil {\n\t\t\treturn\n\t\t}\n\t} else {\n\t\tvar mem tensor.Memory\n\t\tif mem, err = m.Get(op.to, calcMemSize(cv.Dtype(), child.shape)); err != nil {\n\t\t\treturn\n\t\t}\n\t\tif _, err = makeValueFromMem(child.t, child.shape, mem); err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif synchronous {\n\t\tm.Signal()\n\t}\n\n\tdv = new(dualValue)\n\tdv.Value = v\n\tdv.d = d\n\tn.boundTo = dv\n\n\treturn nil\n}\n\n// loads the standardlib\nfunc (m *lispMachine) loadStdLib() {\n\tif cudaStdLib == nil {\n\t\treturn\n\t}\n\n\tfor _, lib := range cudaStdLib {\n\t\tfor i := range m.engines {\n\t\t\te := &m.engines[i]\n\t\t\tif err := e.LoadCUDAFunc(lib.name, lib.data, lib.funcs); err != nil {\n\t\t\t\tpanic(err)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// ForceCPU forces the lispMachine to have the nodes run on the CPU\nfunc (m *lispMachine) ForceCPU() {\n\tm.cleanup()\n\tm.initFail()\n\tm.df = nil\n\n\tfor _, n := range m.sorted {\n\t\tn.dataOn = CPU\n\t}\n\n\t// remove devTrans if any\n\tfor i := 0; i < len(m.sorted); i++ {\n\t\tn := m.sorted[i]\n\t\tif _, ok := n.op.(devTrans); ok {\n\t\t\tcopy(m.sorted[i:], m.sorted[i+1:])\n\t\t\tm.sorted = m.sorted[:len(m.sorted)-1]\n\t\t\ti--\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "vm_genera_cuda_test.go",
          "type": "blob",
          "size": 2.38671875,
          "content": "// +build cuda\n\npackage gorgonia\n\nimport (\n\t\"log\"\n\t\"testing\"\n\n\t\"gorgonia.org/tensor\"\n)\n\nfunc TestGeneraCUDA_init(t *testing.T) {\n\tg, x, y, z := simpleMatEqn()\n\tzs := Must(Slice(z, S(0))) // not a CUDA op (for now)\n\tex := NewVector(g, Float64, WithName(\"extra\"), WithShape(2))\n\txs := Must(Slice(x, S(1)))\n\tzpe := Must(Add(zs, ex))\n\tzpepxpe := Must(Add(xs, zpe))\n\tMust(Sum(zpepxpe))\n\n\txV := tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{0, 1, 2, 3}))\n\tyV := tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{5, 4, 3, 2}))\n\teV := tensor.New(tensor.WithShape(2), tensor.WithBacking([]float64{1000, 50000}))\n\n\tLet(x, xV)\n\tLet(y, yV)\n\tLet(ex, eV)\n\n\t// logger := log.New(os.Stderr, \"\", 0)\n\t// m := NewLispMachine(g, WithLogger(logger), WithWatchlist(), LogBothDir())\n\tm := NewLispMachine(g)\n\tdefer m.Close()\n\n\tt.Logf(\"%v\", m.sorted)\n\tt.Logf(\"%v %v\", m.cpumem, m.gpumem)\n\tt.Logf(\"%v\", m.df.devTransChildren)\n\tt.Logf(\"%v\", m.df.devTransRepl[m.sorted[0]])\n\tif err := m.RunAll(); err != nil {\n\t\tt.Errorf(\"Error %v\", err)\n\t}\n\n\tfor _, n := range m.sorted {\n\t\tif n.boundTo != nil {\n\t\t\tif dv, ok := n.boundTo.(*dualValue); ok {\n\t\t\t\tlog.Printf(\"\\tEncountered %v 0x%x | 0x%x\", n, dv.Value.Uintptr(), dv.d.Uintptr())\n\t\t\t} else {\n\t\t\t\tlog.Printf(\"\\tEncountered %v 0x%x\", n, n.boundTo.Uintptr())\n\t\t\t}\n\t\t}\n\t}\n\n\tvar xG, yG Value\n\tvar err error\n\tif xG, err = x.Grad(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif yG, err = y.Grad(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tt.Logf(\"xG:\\n%v\", xG)\n\tt.Logf(\"yG:\\n%v\", yG)\n\n\t// Compile(g)\n\n}\n\nfunc TestGenera_ForceCPU(t *testing.T) {\n\tg, x, y, z := simpleMatEqn()\n\n\txV := tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{0, 1, 2, 3}))\n\tyV := tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{5, 4, 3, 2}))\n\n\tLet(x, xV)\n\tLet(y, yV)\n\tm := NewLispMachine(g, WithManualGradient())\n\tdefer m.Close()\n\tm.ForceCPU()\n\n\tif err := m.RunAll(); err != nil {\n\t\tt.Errorf(\"%v\", err)\n\t}\n\n\tt.Logf(\"%v\", z.Value())\n}\n\nfunc TestGenera_Backprop(t *testing.T) {\n\tg, x, y, z := simpleMatEqn()\n\tMust(Sum(z))\n\n\txV := tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{0, 1, 2, 3}))\n\tyV := tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float64{5, 4, 3, 2}))\n\n\tLet(x, xV)\n\tLet(y, yV)\n\tm := NewLispMachine(g)\n\n\tif err := m.RunAll(); err != nil {\n\t\tt.Errorf(\"%v\", err)\n\t}\n\tt.Logf(\"x.Value: 0x%x | d: 0x%x\", x.boundTo.(*dualValue).Value.Uintptr(), x.boundTo.(*dualValue).d.Uintptr())\n}\n"
        },
        {
          "name": "vm_genera_nocuda.go",
          "type": "blob",
          "size": 0.31640625,
          "content": "// +build !cuda\n\npackage gorgonia\n\nfunc (m *lispMachine) init() error {\n\tif err := m.prepGraph(); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc (m *lispMachine) execDevTrans(op devTrans, n *Node, children Nodes) (err error) { return nil }\n\nfunc finalizeLispMachine(m *lispMachine) {}\n\nfunc (m *lispMachine) ForceCPU() {}\n"
        },
        {
          "name": "vm_genera_test.go",
          "type": "blob",
          "size": 5.9921875,
          "content": "package gorgonia\n\nimport (\n\t\"bytes\"\n\t\"log\"\n\t\"runtime\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc TestLispMachineBasics(t *testing.T) {\n\tassert := assert.New(t)\n\tvar m *lispMachine\n\t// var err error\n\tvar buf bytes.Buffer\n\n\t// test various flags first\n\tg := NewGraph()\n\tm = NewLispMachine(g)\n\tdefer m.Close()\n\tassert.Equal(uint16(0x3), m.runFlags)\n\tassert.True(m.runFwd())\n\tassert.True(m.runBwd())\n\n\tlogger := log.New(&buf, \"\", 0)\n\tm = NewLispMachine(g, WithLogger(logger))\n\tdefer m.Close()\n\tassert.Equal(logger, m.logger)\n\tassert.Equal(uint16(0x0), m.logFlags) // if you pass in a logger without telling which direction to log... nothing gets logged\n\n\tm = NewLispMachine(g, WithLogger(nil))\n\tdefer m.Close()\n\tassert.True(m.logger == nil, \"logger: %v\", m.logger)\n\n\tm = NewLispMachine(g, WithValueFmt(\"%v\"))\n\tdefer m.Close()\n\tassert.Equal(\"%v\", m.valueFmt)\n\n\tm = NewLispMachine(g, WithNaNWatch())\n\tdefer m.Close()\n\tassert.Equal(uint16(0x7), m.runFlags)\n\tassert.True(m.watchNaN())\n\n\tm = NewLispMachine(g, WithInfWatch())\n\tdefer m.Close()\n\tassert.Equal(uint16(0xb), m.runFlags)\n\tassert.True(m.watchInf())\n\n\tm = NewLispMachine(g, ExecuteFwdOnly())\n\tdefer m.Close()\n\tassert.Equal(uint16(0x1), m.runFlags)\n\tassert.True(m.runFwd())\n\tassert.False(m.runBwd())\n\n\tm = NewLispMachine(g, ExecuteBwdOnly())\n\tdefer m.Close()\n\tassert.Equal(uint16(0x2), m.runFlags)\n\tassert.True(m.runBwd())\n\tassert.False(m.runFwd())\n\n\tm = NewLispMachine(g, LogFwd())\n\tdefer m.Close()\n\tassert.Equal(uint16(0x1), m.logFlags)\n\tassert.Equal(uint16(0x3), m.runFlags)\n\tassert.True(m.logFwd())\n\tassert.False(m.logBwd())\n\n\tm = NewLispMachine(g, LogBwd())\n\tdefer m.Close()\n\tassert.Equal(uint16(0x2), m.logFlags)\n\tassert.Equal(uint16(0x3), m.runFlags)\n\tassert.True(m.logBwd())\n\tassert.False(m.logFwd())\n\n\t// if you pass in a watchlist, but don't have any logger, well, it's not gonna log anything\n\tm = NewLispMachine(g, WithWatchlist())\n\tdefer m.Close()\n\n\tassert.Equal(uint16(0x100), m.logFlags)\n\tassert.Equal(uint16(0x3), m.runFlags)\n\tassert.True(m.watchAll())\n\n}\n\nfunc TestLispMachineMechanics(t *testing.T) {\n\tassert := assert.New(t)\n\tvar err error\n\tg, x, y, z := simpleVecEqn()\n\n\tsz := Must(Sum(z))\n\n\txBack := []float64{1, 5}\n\tyBack := []float64{2, 4}\n\tLet(x, tensor.New(tensor.WithShape(x.shape...), tensor.WithBacking(xBack)))\n\tLet(y, tensor.New(tensor.WithShape(y.shape...), tensor.WithBacking(yBack)))\n\n\tmachine := NewLispMachine(g)\n\tdefer machine.Close()\n\tif err = machine.RunAll(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tgBack := []float64{1, 1}\n\tgrad := tensor.New(tensor.WithShape(x.shape...), tensor.WithBacking(gBack))\n\txG, _ := x.Grad()\n\tyG, _ := y.Grad()\n\n\tassert.True(ValueEq(grad, xG))\n\tassert.True(ValueEq(grad, yG))\n\n\t// tack more items onto the graph, and execute it again\n\tszp2 := Must(Add(sz, twof64))\n\tszp3 := Must(Add(sz, threef64))\n\n\tvar szp2Val Value\n\treadSzp2 := Read(szp2, &szp2Val)\n\n\tsg := g.SubgraphRoots(readSzp2, szp2)\n\tmachine = NewLispMachine(sg)\n\tdefer machine.Close()\n\tif err = machine.RunAll(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tassert.NotNil(szp2Val)\n\tassert.Equal(szp2.Value(), szp2Val)\n\tassert.Nil(szp3.boundTo) // node that was not executed on should not have any values bound to it\n\n\t// play it again, sam!\n\t// this is to test that if given the same root that had previously been executed on, it will not reallocate a new *dv\n\tsg = g.SubgraphRoots(szp3)\n\tmachine = NewLispMachine(sg)\n\tdefer machine.Close()\n\n\tif err = machine.RunAll(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\t// save szp3's value\n\tszp3dv := szp3.boundTo.(*dualValue)\n\tszp3dvv := szp3dv.Value\n\n\tif err = machine.RunAll(); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tif dv := szp3.boundTo.(*dualValue); dv != szp3dv {\n\t\tt.Error(\"A new *dualValue had been allocated for szp3dv. That's not supposed to happen\")\n\t} else if dv.Value != szp3dvv {\n\t\tt.Error(\"A new value for szp3dv.Value has been allocated. That ain't supposed to happen\")\n\t}\n\n\t// idiotsville\n\n\t// non scalar costs\n\tcost := Must(Add(sz, x))\n\tsg = g.Subgraph(cost)\n\tmachine = NewLispMachine(sg)\n\tdefer machine.Close()\n\tif err = machine.RunAll(); err == nil {\n\t\tt.Error(\"Expected a AutoDiff error\")\n\t}\n}\n\nfunc TestLispMachineRepeatedRuns(t *testing.T) {\n\tassert := assert.New(t)\n\tvar err error\n\tg := NewGraph()\n\tx := NewVector(g, Float64, WithShape(2), WithName(\"x\"), WithInit(RangedFrom(0)))\n\ty := NewMatrix(g, Float64, WithShape(2, 3), WithName(\"y\"), WithInit(RangedFrom(0)))\n\tz := Must(Mul(x, y))\n\tcost := Must(Slice(z, S(1))) // this simulates the more complex cost functions\n\n\treps := 10\n\n\tfor i := 0; i < reps; i++ {\n\t\tm := NewLispMachine(g)\n\t\tif err := m.RunAll(); err != nil {\n\t\t\tt.Errorf(\"Repetition %d error: %+v\", i, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tvar gradX, gradY, gradZ, gradC Value\n\t\tif gradX, err = x.Grad(); err != nil {\n\t\t\tt.Errorf(\"No gradient for x in repetition %d. Error: %v\", i, err)\n\t\t\tcontinue\n\t\t}\n\t\tif gradY, err = y.Grad(); err != nil {\n\t\t\tt.Errorf(\"No gradient for y in repetition %d. Error: %v\", i, err)\n\t\t\tcontinue\n\t\t}\n\t\tif gradZ, err = z.Grad(); err != nil {\n\t\t\tt.Errorf(\"No gradient for z in repetition %d. Error: %v\", i, err)\n\t\t\tcontinue\n\t\t}\n\t\tif gradC, err = cost.Grad(); err != nil {\n\t\t\tt.Errorf(\"No gradient for cost in repetition %d. Error: %v\", i, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tassert.Equal([]float64{1, 4}, gradX.Data(), \"run %d\", i)\n\t\tassert.Equal([]float64{0, 0, 0, 0, 1, 0}, gradY.Data(), \"run %d\", i)\n\t\tassert.Equal([]float64{0, 1, 0}, gradZ.Data(), \"run %d\", i)\n\t\tassert.Equal(1.0, gradC.Data(), \"run %d\", i)\n\n\t\t// assert that the data has been unchanged\n\t\tassert.Equal([]float64{0, 1}, x.Value().Data())\n\t\tassert.Equal([]float64{0, 1, 2, 3, 4, 5}, y.Value().Data())\n\t\tassert.Equal([]float64{3, 4, 5}, z.Value().Data())\n\t\tassert.Equal(float64(4), cost.Value().Data())\n\n\t\t// This simulates the cloberring of of the gradients of the nodes. The next iteration should STILL reveal the same results\n\t\tmodel := Nodes{x, y, z, cost}\n\t\tfor _, n := range model {\n\t\t\tdv := n.boundTo.(*dualValue)\n\t\t\tif err = dv.SetDeriv(ZeroValue(dv.d)); err != nil {\n\t\t\t\tt.Errorf(\"Unable to set the gradient to 0 for %v. Error : %v\", n, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t\tm.Close()\n\t\truntime.GC()\n\t}\n\n}\n"
        },
        {
          "name": "vm_tape.go",
          "type": "blob",
          "size": 19.2392578125,
          "content": "package gorgonia\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"log\"\n\t\"runtime\"\n\t\"strings\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\ntype tapeMachine struct {\n\tExternMetadata\n\n\tp      *program\n\tlocMap map[*Node]register\n\n\t// \"register\" banks\n\tcpumem []Value // Value - knows its own type and shape\n\tgpumem []Value // Value of which the memories are stored in GPU memory\n\n\t// state stuff, to allow continuation\n\tpc int\n\n\t// operational stuff\n\tbindNodesDV  Nodes // nodes that require binding of DV\n\twatchNodes   Nodes\n\twatchRegs    []register\n\twatchNodeIDs []NodeID\n\tlogger       *log.Logger\n\tbuf          *bytes.Buffer\n\tvalueFmt     string\n\ttabcount     int\n\tlogFlags     uint16\n\tclosureQueue []func() error\n\n\trunFlags uint16 //  spare2: trace(copy values and put into nodes)\n\tevalMode bool\n}\n\n// NewTapeMachine creates a VM that compiles a graph into a prog.\nfunc NewTapeMachine(g *ExprGraph, opts ...VMOpt) *tapeMachine {\n\tm := &tapeMachine{\n\t\tvalueFmt: \"%3.3g\",\n\t}\n\tm.Engine = StandardEngine{}\n\n\tif b, ok := whichblas.(batchedBLAS); ok {\n\t\tm.b = b\n\t}\n\n\tfor _, opt := range opts {\n\t\topt(m)\n\t}\n\n\tm.doAlloc()\n\n\tif m.p == nil || m.locMap == nil {\n\t\tprog, locMap, err := Compile(g)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\n\t\tm.p = prog\n\t\tm.locMap = locMap\n\t}\n\tm.cpumem = make([]Value, m.p.cpulocs)\n\tm.gpumem = make([]Value, m.p.gpulocs)\n\tm.init()\n\tfor _, n := range m.p.g.AllNodes() {\n\t\tsetEngine(n.boundTo, m.Engine)\n\n\t\tif op, ok := n.op.(TrainModeOp); ok {\n\t\t\top.SetTraining(!m.evalMode)\n\t\t}\n\t}\n\n\truntime.SetFinalizer(m, finalizeTapeMachine) // a \"defer\" to deinitialize CUDA stuff (if using CUDA build)\n\treturn m\n}\n\nfunc (m *tapeMachine) logBwd() bool { return (m.logFlags>>bwdOnly)&uint16(1) == 1 }\nfunc (m *tapeMachine) doLogBwd()    { m.logFlags |= uint16(1) << bwdOnly }\nfunc (m *tapeMachine) dontLogBwd()  { m.logFlags &= (^(uint16(1) << bwdOnly)) }\n\nfunc (m *tapeMachine) logFwd() bool { return (m.logFlags>>fwdOnly)&uint16(1) == 1 }\nfunc (m *tapeMachine) doLogFwd()    { m.logFlags |= uint16(1) << fwdOnly }\nfunc (m *tapeMachine) dontLogFwd()  { m.logFlags &= (^(uint16(1) << fwdOnly)) }\n\nfunc (m *tapeMachine) watchNaN() bool { return (m.runFlags>>watchNaN)&uint16(1) == 1 }\nfunc (m *tapeMachine) doWatchNaN()    { m.runFlags |= uint16(1) << watchNaN }\nfunc (m *tapeMachine) dontWatchNaN()  { m.runFlags &= (^(uint16(1) << watchNaN)) }\n\nfunc (m *tapeMachine) watchInf() bool { return (m.runFlags>>watchInf)&uint16(1) == 1 }\nfunc (m *tapeMachine) doWatchInf()    { m.runFlags |= uint16(1) << watchInf }\nfunc (m *tapeMachine) dontWatchInf()  { m.runFlags &= (^(uint16(1) << watchInf)) }\n\nfunc (m *tapeMachine) watchPointer() bool { return (m.logFlags>>watchPointer)&uint16(1) == 1 }\nfunc (m *tapeMachine) doWatchPointer()    { m.logFlags |= uint16(1) << watchPointer }\nfunc (m *tapeMachine) dontWatchPointer()  { m.logFlags &= (^(uint16(1) << watchPointer)) }\n\nfunc (m *tapeMachine) watchAll() bool { return (m.logFlags>>watchAll)&uint16(1) == 1 }\nfunc (m *tapeMachine) doWatchAll()    { m.logFlags |= (uint16(1) << watchAll) }\nfunc (m *tapeMachine) dontWatchAll()  { m.logFlags &= (^(uint16(1) << watchAll)) }\n\nfunc (m *tapeMachine) alloc() bool { return (m.runFlags>>allocVals)&uint16(1) == 1 }\nfunc (m *tapeMachine) doAlloc()    { m.runFlags |= uint16(1) << allocVals }\nfunc (m *tapeMachine) dontAlloc()  { m.runFlags &= (^(uint16(1) << allocVals)) }\n\nfunc (m *tapeMachine) trace() bool { return (m.runFlags>>spare2)&uint16(1) == 1 }\nfunc (m *tapeMachine) doTrace()    { m.runFlags |= uint16(1) << spare2 }\nfunc (m *tapeMachine) dontTrace()  { m.runFlags &= (^(uint16(1) << spare2)) }\n\nfunc (m *tapeMachine) bindDV() bool { return m.runFlags>>spare3&uint16(1) == 1 }\nfunc (m *tapeMachine) doBindDV()    { m.runFlags |= uint16(1) << spare3 }\nfunc (m *tapeMachine) dontBindDV()  { m.runFlags &= (^(uint16(1) << spare3)) }\n\n// Reset resets the run state of the machine by changing the instruction pointer back to 0\n// and reseting the registry\nfunc (m *tapeMachine) Reset() {\n\tm.pc = 0\n\tm.ExternMetadata.Reset()\n\n\tfor i := range m.gpumem {\n\t\treturnValue(m.gpumem[i])\n\t\tm.gpumem[i] = nil //\n\t}\n\tfor i := range m.cpumem {\n\t\tm.cpumem[i] = nil\n\t}\n}\n\nfunc (m *tapeMachine) Close() error {\n\tfinalizeTapeMachine(m)\n\treturn nil\n}\n\n// Prog returns the compiled program. This would mainly be used in debugging functions\nfunc (m *tapeMachine) Prog() *program { return m.p }\n\n// LocMap returns the location where the Node's execution results are stored. This would mainly be used in debugging functions.\nfunc (m *tapeMachine) LocMap() map[*Node]register { return m.locMap }\n\n// Let wraps the Let() function of the package, with additional checks that n is in the machine\nfunc (m *tapeMachine) Let(n *Node, be interface{}) (err error) {\n\tif !m.p.g.Has(n.ID()) {\n\t\treturn errors.Errorf(\"Node %v does not exist in this graph\", n)\n\t}\n\n\treturn Let(n, be)\n}\n\n// Set wraps the Set() function of this package, with additional checks that both a and b are in the machine\nfunc (m *tapeMachine) Set(a, b *Node) (err error) {\n\tif !m.p.g.Has(a.ID()) {\n\t\treturn errors.Errorf(\"Node %v does not exist in this graph\", a)\n\t}\n\tif !m.p.g.Has(b.ID()) {\n\t\treturn errors.Errorf(\"Node %v does not exist in this graph\", b)\n\t}\n\n\tif b.Value() != nil {\n\t\treturn a.bind(b.Value())\n\t}\n\n\t// get the registry location\n\tbreg := m.locMap[b]\n\tv := m.getValue(breg)\n\tif v == nil {\n\t\treturn nyi(\"handling of tensor.Memory -> Value\", \"tapeMachine.Set\")\n\t}\n\n\tmachineLogf(\"Setting %v to %v. Read from %v Value is %v\", b, a, breg, v)\n\treturn a.bind(v)\n}\n\n// Run runs a fragment (a subset of a program).\nfunc (m *tapeMachine) Run(frag fragment) (err error) {\n\tdefer func() {\n\t\tif err == nil {\n\t\t\tm.dontAlloc()\n\t\t}\n\t}()\n\n\tfor _, instr := range frag {\n\t\tif err = instr.exec(m); err != nil {\n\t\t\treturn errors.Wrap(err, \"Failed to carry exec()\")\n\t\t}\n\t}\n\tmachineLogf(\"Binding values based on final output\")\n\tenterLogScope()\n\tfor n, r := range m.locMap {\n\t\tif n.isInput() {\n\t\t\tcontinue\n\t\t}\n\n\t\tv := m.getValue(r)\n\t\tif v == nil {\n\t\t\treturn nyi(\"converting tensor.Memory to Value\", \"TapeMachine.Run\")\n\t\t}\n\n\t\tif err = n.bind(m.cpumem[r.id]); err != nil {\n\t\t\treturn errors.Wrap(err, bindFail)\n\t\t}\n\t}\n\tleaveLogScope()\n\treturn\n}\n\nfunc (m *tapeMachine) RunAll() (err error) {\n\truntime.LockOSThread()\n\tdefer runtime.UnlockOSThread()\n\tdefer m.DoWork()\n\n\tworkAvailable := m.ExternMetadata.WorkAvailable()\n\tsyncChan := m.ExternMetadata.Sync()\n\terrChan := make(chan error)\n\tdoneChan := make(chan struct{})\n\n\tgo m.runall(errChan, doneChan)\n\tfor {\n\t\tselect {\n\t\tcase sychronous := <-workAvailable:\n\t\t\terr := m.ExternMetadata.DoWork()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif sychronous {\n\t\t\t\tsyncChan <- struct{}{}\n\t\t\t}\n\t\tcase err := <-errChan:\n\t\t\treturn err\n\t\tcase <-doneChan:\n\t\t\terr := m.ExternMetadata.DoWork()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tfor _, closure := range m.closureQueue {\n\t\t\t\tif err := closure(); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t}\n}\n\nfunc (m *tapeMachine) runall(errChan chan error, doneChan chan struct{}) {\n\tvar pointers map[int64]string\n\n\tif m.watchPointer() {\n\t\tpointers = make(map[int64]string, len(m.p.instructions))\n\t}\n\n\tfor ; m.pc < len(m.p.instructions); m.pc++ {\n\t\tinstr := m.p.instructions[m.pc]\n\t\tm.logf(\"PC %d\", m.pc)\n\n\t\tif err := instr.exec(m); err != nil {\n\t\t\terrNode := m.nodeFromInstr(instr)\n\t\t\terr = vmContextualError{\n\t\t\t\terror: errors.Wrapf(err, \"PC %d. Failed to execute instruction %v\", m.pc, instr),\n\t\t\t\tinstr: m.pc,\n\t\t\t\tnode:  errNode,\n\t\t\t}\n\t\t\terrChan <- err\n\t\t\treturn\n\t\t}\n\t\t// only proceed to check NaNs and Infs for execOp\n\t\tif _, ok := instr.(*execOp); !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tif m.watchNaN() {\n\t\t\twriteTo := instr.writes().id\n\t\t\tid := instr.ID()\n\t\t\tif writeTo > 0 && id > 0 {\n\t\t\t\tv := m.getValue(instr.writes())\n\t\t\t\tif v == nil {\n\t\t\t\t\terr := errors.Errorf(nyiFail, \"converting tensor.Memory to Value\", \"watchNaN\")\n\t\t\t\t\terrChan <- err\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tif hasNaN(v, CPU) {\n\t\t\t\t\tn := m.p.g.Node(id).(*Node)\n\t\t\t\t\terr := errors.Errorf(\"NaN found in value. Node: %v(%x)\", n, n.ID())\n\t\t\t\t\terrChan <- err\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif m.watchInf() {\n\t\t\twriteTo := instr.writes().id\n\t\t\tid := instr.ID()\n\t\t\tif writeTo > 0 && id > 0 {\n\t\t\t\tv := m.getValue(instr.writes())\n\t\t\t\tif v == nil {\n\t\t\t\t\terr := errors.Errorf(nyiFail, \"converting tensor.Memory to Value\", \"watchInf\")\n\t\t\t\t\terrChan <- err\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tif hasInf(v, CPU) {\n\t\t\t\t\tn := m.p.g.Node(id).(*Node)\n\t\t\t\t\terr := errors.Errorf(\"Inf found in value. Node: %v(%x)\", n, n.ID())\n\t\t\t\t\terrChan <- err\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif m.watchPointer() {\n\t\t\twriteTo := instr.writes().id\n\t\t\tid := instr.ID()\n\t\t\tif writeTo > 0 && id > 0 {\n\t\t\t\tv := m.getValue(instr.writes())\n\t\t\t\tif v == nil {\n\t\t\t\t\terr := errors.Errorf(nyiFail, \"converting tensor.Memory to Value\", \"watchPointer\")\n\t\t\t\t\terrChan <- err\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tpointerID := fmt.Sprintf(\"%p\", v)\n\n\t\t\t\t// this checks all previous node pointers. The new one should not be included.\n\t\t\t\tfor cID, pointer := range pointers {\n\t\t\t\t\tif pointer == pointerID {\n\t\t\t\t\t\tn := m.p.g.Node(id).(*Node)\n\t\t\t\t\t\tc := m.p.g.Node(cID).(*Node)\n\n\t\t\t\t\t\terr := errors.Errorf(\"Pointer clash found in value. Node: %v(%x) %s clashed with %v(%x) %s\", n, n.ID(), pointerID, c, c.ID(), pointer)\n\t\t\t\t\t\terrChan <- err\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tpointers[id] = pointerID\n\t\t\t}\n\t\t}\n\t}\n\tdoneChan <- struct{}{}\n}\n\nfunc (m *tapeMachine) nodeFromInstr(instr tapeInstr) *Node { return m.p.r[instr.ID()] }\n\nfunc (m *tapeMachine) getValue(r register) Value {\n\tswitch r.device {\n\tcase CPU:\n\t\treturn m.cpumem[r.id]\n\tdefault:\n\t\treturn m.gpumem[r.id]\n\t}\n}\n\nfunc (m *tapeMachine) writeValue(r register, v Value) {\n\tswitch r.device {\n\tcase CPU:\n\t\tm.cpumem[r.id] = v\n\tdefault:\n\t\tm.gpumem[r.id] = v\n\t}\n}\n\nfunc (m *tapeMachine) watchedLogf(format string, attrs ...interface{}) {\n\tinstr := m.p.instructions[m.pc]\n\treads := instr.reads()\n\twrites := instr.writes()\n\n\twatched := m.watchAll()\n\n\tif !watched {\n\t\tfor _, reg := range reads {\n\t\t\tfor _, watch := range m.watchRegs {\n\t\t\t\tif reg.id == watch.id {\n\t\t\t\t\twatched = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif watched {\n\t\t\tgoto end\n\t\t}\n\n\t\tfor _, watch := range m.watchRegs {\n\t\t\tif watch.id == writes.id {\n\t\t\t\twatched = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\tif watched {\n\t\t\tgoto end\n\t\t}\n\n\t\tn := m.nodeFromInstr(instr)\n\t\tfor _, watch := range m.watchNodes {\n\t\t\tif watch == n {\n\t\t\t\twatched = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif watched {\n\t\t\tgoto end\n\t\t}\n\n\t\tfor _, watch := range m.watchNodeIDs {\n\t\t\tif int64(watch) == n.ID() {\n\t\t\t\twatched = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\tif !watched {\n\t\treturn\n\t}\n\nend:\n\tm.logf(format, attrs...)\n\n}\n\nfunc (m *tapeMachine) logf(format string, attrs ...interface{}) {\n\tswitch {\n\tcase machineDev:\n\t\tif m.logger != nil {\n\t\t\tgoto loggercase\n\t\t}\n\n\t\tmachineLogf(format, attrs...)\n\t\tbreak\n\n\tloggercase:\n\t\tfallthrough\n\tcase m.logger != nil:\n\t\ts := fmt.Sprintf(format, attrs...)\n\t\ts = strings.Replace(s, \"\\n\", m.buf.String(), -1)\n\t\tm.logger.Println(s)\n\t}\n}\n\nfunc (m *tapeMachine) enterLogScope() {\n\tif DEBUG && machineDev {\n\t\tenterLogScope()\n\t}\n\tm.tabcount++\n\tif m.logger != nil {\n\t\treps := strings.Repeat(\"\\t\", m.tabcount)\n\t\tm.logger.SetPrefix(reps)\n\t\tm.buf.Reset()\n\t\tm.buf.WriteString(\"\\n\")\n\t\tm.buf.WriteString(reps)\n\t}\n}\n\nfunc (m *tapeMachine) leaveLogScope() {\n\tif DEBUG && machineDev {\n\t\tleaveLogScope()\n\t}\n\tm.tabcount--\n\tif m.tabcount < 0 {\n\t\tm.tabcount = 0\n\t}\n\tif m.logger != nil {\n\t\treps := strings.Repeat(\"\\t\", m.tabcount)\n\t\tm.logger.SetPrefix(reps)\n\t\tm.buf.Reset()\n\t\tm.buf.WriteString(\"\\n\")\n\t\tm.buf.WriteString(reps)\n\t}\n}\n\n/* PROGRAM */\n\ntype program struct {\n\tinstructions fragment\n\targs         int\n\tcpulocs      int\n\tgpulocs      int\n\tcpumem       int64\n\tgpumem       []int64\n\tg            *ExprGraph         // original dag\n\tdf           *dataflow          // dataflow analysis\n\tm            map[*Node]fragment // store which nodes create which instructions\n\tr            map[int64]*Node    // reverse of m, storing the instruction ID\n\tsorted       Nodes\n}\n\nfunc (p *program) String() string {\n\tvar buf bytes.Buffer\n\tfmt.Fprintf(&buf, \"Instructions:\\n%s\\nArgs: %d | CPU Memories: %d | GPU Memories: %d\\nCPU Mem: %v | GPU Mem %v\\n\\nNode:instructions map:\\n\", p.instructions, p.args, p.cpulocs, p.gpulocs, p.cpumem, p.gpumem)\n\n\tfor i, n := range p.sorted {\n\t\tfmt.Fprintf(&buf, \"\\t%d\\t%x:\", i, n.ID())\n\t\tfrag := p.m[n]\n\t\tfor j, instr := range frag {\n\t\t\tif j == 0 {\n\t\t\t\tfmt.Fprintf(&buf, \"\\t%v\\n\", instr)\n\t\t\t} else {\n\t\t\t\tfmt.Fprintf(&buf, \"\\t\\t%v\\n\", instr)\n\t\t\t}\n\t\t}\n\n\t}\n\n\treturn buf.String()\n}\n\n// Graph enables the end user to inspect the graph (typically useful for debugging)\nfunc (p *program) Graph() *ExprGraph { return p.g }\n\nfunc (p *program) CPUMemReq() int64 { return p.cpumem }\n\nfunc (p *program) GPUMemReq() []int64 {\n\tretVal := make([]int64, len(p.gpumem))\n\tcopy(retVal, p.gpumem)\n\treturn retVal\n}\n\n/* REGISTER */\n\ntype register struct {\n\tid     int\n\tdevice Device\n}\n\nfunc (r register) String() string { return fmt.Sprintf(\"%s%d\", r.device, r.id) }\n\n/* INSTRUCTIONS */\n\ntype tapeInstr interface {\n\tID() int64 // ID is the node ID\n\treads() []register\n\twrites() register\n\texec(*tapeMachine) error\n\tfmt.Stringer\n}\n\ntype fragment []tapeInstr\n\nfunc (f fragment) String() string {\n\tvar buf bytes.Buffer\n\tfor i, instr := range f {\n\t\tfmt.Fprintf(&buf, \"\\t%d\\t%s\\n\", i, instr)\n\t}\n\treturn buf.String()\n}\n\nfunc (f fragment) has(want tapeInstr) bool {\n\tfor _, instr := range f {\n\t\tif instr == want {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\ntype alloc struct {\n\tid int64 // node ID\n\tt  hm.Type\n\ts  tensor.Shape\n\n\treadFrom []register\n\twriteTo  register\n}\n\nfunc newAlloc(n *Node, writeTo register) alloc {\n\treturn alloc{\n\t\tid:      n.ID(),\n\t\tt:       n.t,\n\t\ts:       n.shape,\n\t\twriteTo: writeTo,\n\t}\n}\n\nfunc (instr alloc) ID() int64         { return instr.id }\nfunc (instr alloc) reads() []register { return instr.readFrom }\nfunc (instr alloc) writes() register  { return instr.writeTo }\n\nfunc (instr alloc) exec(m *tapeMachine) (err error) {\n\tm.logf(\"Executing %v\", instr)\n\tm.enterLogScope()\n\tdefer m.leaveLogScope()\n\n\tvar dt tensor.Dtype\n\tif dt, err = dtypeOf(instr.t); err != nil {\n\t\treturn errors.Wrapf(err, dtypeExtractionFail, instr.t)\n\t}\n\n\treg := m.getValue(instr.writeTo)\n\tif reg != nil && reg.Dtype() == dt && reg.Shape().Eq(instr.s) {\n\t\treturn nil\n\t}\n\n\tdev := instr.writeTo.device\n\tvar v Value\n\tswitch dev {\n\tcase CPU:\n\n\t\tv, err = makeValue(instr.t, instr.s)\n\n\tdefault:\n\t\tvar mem tensor.Memory\n\t\tmemsize := calcMemSize(dt, instr.s)\n\t\tif mem, err = m.ExternMetadata.Get(dev, memsize); err != nil {\n\t\t\treturn errors.Wrapf(err, \"Unable to allocate %v bytes from %v | %T\", memsize, dev, err)\n\t\t}\n\t\tv, err = makeValueFromMem(instr.t, instr.s, mem)\n\t}\n\tif err != nil {\n\t\treturn\n\t}\n\tsetEngine(v, m.getEngine(dev))\n\tif vt, ok := v.(tensor.Tensor); ok {\n\t\tm.watchedLogf(\"%x | %T\", v.Uintptr(), vt.Engine())\n\t} else {\n\t\tm.watchedLogf(\"%x\", v.Uintptr())\n\t}\n\n\tm.writeValue(instr.writeTo, v)\n\treturn nil\n}\n\nfunc (instr alloc) String() string {\n\treturn fmt.Sprintf(\"Alloc %v%v\\t\\t%v\", instr.t, instr.s, instr.writeTo)\n}\n\ntype free struct {\n\treadsFrom register\n}\n\nfunc (instr free) ID() int64         { return -1 }\nfunc (instr free) reads() []register { return []register{instr.readsFrom} }\nfunc (instr free) writes() register  { return register{-1, CPU} }\nfunc (instr free) exec(m *tapeMachine) error {\n\tm.logf(\"Executing Free %v\", instr.readsFrom)\n\tswitch instr.readsFrom.device {\n\tcase CPU:\n\t\treturn nil\n\tdefault:\n\t\tm.logf(\"instr.read from not CPU - %v %v %d\", instr.readsFrom, instr.readsFrom.device == CPU, instr.readsFrom.device)\n\t\tmem := m.gpumem[instr.readsFrom.id]\n\t\tsize := int64(mem.MemSize())\n\n\t\tm.Put(instr.readsFrom.device, mem, size)\n\t\tm.gpumem[instr.readsFrom.id] = nil\n\t\treturn nil\n\t}\n}\nfunc (instr free) String() string { return fmt.Sprintf(\"Free %v\", instr.readsFrom) }\n\ntype loadArg struct {\n\tindex   int64\n\twriteTo register\n\tname    string\n}\n\nfunc (instr loadArg) ID() int64         { return instr.index }\nfunc (instr loadArg) reads() []register { return nil }\nfunc (instr loadArg) writes() register  { return instr.writeTo }\n\nfunc (instr loadArg) exec(m *tapeMachine) error {\n\tm.logf(\"Executing %v\", instr)\n\tm.enterLogScope()\n\tdefer m.leaveLogScope()\n\n\tnode := m.p.g.Node(instr.index).(*Node)\n\tm.logf(\"node %v\", node)\n\n\tif node.boundTo == nil {\n\t\treturn errors.Errorf(\"No value bound to node %v (%x)\", node, node.ID())\n\t}\n\n\tvar v Value\n\tif dv, ok := node.boundTo.(*dualValue); ok {\n\t\tv = dv.Value\n\t} else {\n\t\tv = node.boundTo\n\t}\n\n\tm.writeValue(instr.writeTo, v)\n\t// m.watchedLogf(\"Write To: %v\", instr.writeTo)\n\t// m.watchedLogf(m.valueFmt, m.cpumem[instr.writeTo.id])\n\treturn nil\n}\n\nfunc (instr loadArg) String() string {\n\treturn fmt.Sprintf(\"loadArg %x (%v) to %v\", instr.index, instr.name, instr.writeTo)\n}\n\ntype execOp struct {\n\top Op\n\n\tid int64\n\n\treadFrom []register\n\twriteTo  register\n\tsize     int64 // size represents the outputsize\n\n\tpreAllocated bool\n\tuseUnsafe    bool\n\tuseGPU       bool\n}\n\nfunc (instr *execOp) ID() int64         { return instr.id }\nfunc (instr *execOp) reads() []register { return instr.readFrom }\nfunc (instr *execOp) writes() register  { return instr.writeTo }\n\nfunc newExecOp(n *Node) *execOp {\n\t_, useGPU := n.op.(CUDADoer)\n\tcompileLogf(\"op %v uses GPU %v\", n.op, useGPU)\n\tdt, err := dtypeOf(n.t)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tsize := calcMemSize(dt, n.Shape())\n\n\treturn &execOp{\n\t\top:     n.op,\n\t\tid:     n.ID(),\n\t\tuseGPU: useGPU,\n\t\tsize:   size,\n\t}\n}\n\nfunc (instr *execOp) String() string {\n\treturn fmt.Sprintf(\"%v\\t%v\\t%v\\t%t\\t%t\\t%t\", instr.op, instr.readFrom, instr.writeTo, instr.op.CallsExtern(), instr.useUnsafe, instr.preAllocated)\n}\n\n// flushInstr is for blastoise and cubone\ntype flushInstr struct{}\n\nfunc (instr flushInstr) exec(m *tapeMachine) error {\n\tm.logf(\"Executing DoWork\")\n\treturn m.ExternMetadata.DoWork()\n}\n\nfunc (instr flushInstr) ID() int64         { return -1 }\nfunc (instr flushInstr) reads() []register { return nil }\nfunc (instr flushInstr) writes() register  { return register{-1, CPU} }\nfunc (instr flushInstr) String() string    { return \"DoWork\" }\n\ntype letInstr struct {\n\treadFrom register\n\twriteTo  register\n}\n\nfunc (instr letInstr) ID() int64               { return -1 }\nfunc (instr letInstr) reads() []register       { return []register{instr.readFrom} }\nfunc (instr letInstr) writes() register        { return instr.writeTo }\nfunc (instr letInstr) exec(*tapeMachine) error { return nil }\n\nfunc (instr letInstr) String() string {\n\treturn fmt.Sprintf(\"LET %v = %v\", instr.writeTo, instr.readFrom)\n}\n\ntype readInstr struct {\n\treadFrom register\n\tinto     *Value\n\n\t// required to convert tensor.Memory to Value\n\tt hm.Type\n\ts tensor.Shape\n}\n\nfunc (instr *readInstr) ID() int64         { return -1 }\nfunc (instr *readInstr) reads() []register { return []register{instr.readFrom} }\nfunc (instr *readInstr) writes() register  { return register{-1, CPU} }\nfunc (instr *readInstr) exec(m *tapeMachine) (err error) {\n\tm.logf(\"Executing READ - read from %v into %v\", instr.readFrom, instr.into)\n\tv := m.getValue(instr.readFrom)\n\tif v == nil {\n\t\treturn nyi(\"value of nil\", \"readInstr.exec\")\n\t}\n\n\tif *instr.into != nil {\n\t\tdest := *instr.into\n\t\t_, err = Copy(dest, v)\n\t\treturn err\n\t}\n\n\tv2, err := CloneValue(v)\n\tif err != nil {\n\t\treturn errors.Wrap(err, cloneFail)\n\t}\n\n\t*instr.into = v2\n\treturn nil\n}\n\nfunc (instr *readInstr) String() string {\n\treturn fmt.Sprintf(\"Read %v into %p\", instr.readFrom, instr.into)\n}\n\ntype deviceTransport struct {\n\tfrom, to register\n}\n\nfunc (instr deviceTransport) ID() int64 { return -1 }\nfunc (instr deviceTransport) reads() []register {\n\treturn []register{instr.from}\n}\nfunc (instr deviceTransport) writes() register { return instr.to }\n\nfunc (instr deviceTransport) String() string {\n\treturn fmt.Sprintf(\"memcpy(%v, %v)\", instr.to, instr.from)\n}\n"
        },
        {
          "name": "vm_tape_cuda.go",
          "type": "blob",
          "size": 6.1201171875,
          "content": "// +build cuda\n\npackage gorgonia\n\nimport (\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/cu\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc finalizeTapeMachine(m *tapeMachine) {\n\tcudaLogf(\"Finalizing tape machine %p\", m)\n\tm.cleanup()\n\tm.initFail() // not really a failure. Just call to detroy all the contexts and shit\n}\n\nfunc (m *tapeMachine) init() {\n\tvar initCUDA bool\n\tcudaLogf(\"instructions %v\", len(m.p.instructions))\n\tfor _, instr := range m.p.instructions {\n\t\tif eo, ok := instr.(*execOp); ok {\n\t\t\tif _, ok := eo.op.(CUDADoer); ok {\n\t\t\t\tinitCUDA = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\t// don't bother initializing contexts if no instructions were CUDA based\n\tif !initCUDA {\n\t\tcudaLogf(\"No CUDA ops\")\n\t\treturn\n\t}\n\n\tif err := m.ExternMetadata.init(m.p.gpumem); err != nil {\n\t\tm.ExternMetadata.initFail()\n\t\tpanic(err)\n\t}\n\tm.loadStdLib()\n\n}\n\n// loads the standardlib\nfunc (m *tapeMachine) loadStdLib() {\n\tif cudaStdLib == nil {\n\t\treturn\n\t}\n\n\tfor _, lib := range cudaStdLib {\n\t\tfor i := range m.engines {\n\t\t\te := &m.engines[i]\n\t\t\tif err := e.LoadCUDAFunc(lib.name, lib.data, lib.funcs); err != nil {\n\t\t\t\tpanic(err)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (m *tapeMachine) getEngine(dev Device) tensor.Engine {\n\tif dev == CPU {\n\t\treturn m.Engine\n\t}\n\treturn &m.Engines()[int(dev)]\n}\n\nfunc (instr *execOp) exec(m *tapeMachine) (err error) {\n\tm.logf(\"Executing %v. Node is: %x\", instr, instr.id)\n\tm.enterLogScope()\n\tdefer m.leaveLogScope()\n\n\tenterLogScope()\n\tdefer leaveLogScope()\n\n\tm.watchedLogf(\"Inputs:\")\n\tm.enterLogScope()\n\tvar inputs []Value\n\tfor _, reg := range instr.readFrom {\n\t\tv := m.getValue(reg)\n\t\tinputs = append(inputs, v)\n\t\tm.watchedLogf(m.valueFmt, v.Uintptr())\n\t}\n\tm.leaveLogScope()\n\n\ttoDev := instr.writeTo.device\n\tvar v Value\n\tswitch op := instr.op.(type) {\n\tcase CUDADoer:\n\t\tprealloc := m.getValue(instr.writeTo)\n\t\tif v, err = op.CUDADo(m, toDev, prealloc, inputs...); err != nil {\n\t\t\treturn errors.Wrapf(err, \"Happened while attempting to use CUDA to execute %v. Node is %x. Register was %v\", instr, instr.id, instr.writeTo.id)\n\t\t}\n\t\te := &m.Engines()[int(toDev)]\n\t\tsetEngine(v, e)\n\tcase CLDoer:\n\tdefault:\n\t\tswitch {\n\t\tcase instr.preAllocated:\n\t\t\tif pd, ok := instr.op.(UsePreallocDoer); ok {\n\t\t\t\tp := m.cpumem[instr.writeTo.id]\n\t\t\t\tif v, err = pd.UsePreallocDo(p, inputs...); err != nil {\n\t\t\t\t\treturn errors.Wrapf(err, \"Happened while attempting to execute %v. Node is %x. Register was: %v \", instr, instr.id, instr.writeTo.id)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// TODO: maybe warn?\n\t\t\t\tif v, err = instr.op.Do(inputs...); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, opDoFail)\n\t\t\t\t}\n\t\t\t}\n\t\tcase instr.useUnsafe:\n\t\t\tif ud, ok := instr.op.(UnsafeDoer); ok {\n\t\t\t\tif v, err = ud.UnsafeDo(inputs...); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, \"Failed to carry UnsafeDo()\")\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// TODO: warn?\n\t\t\t\tif v, err = instr.op.Do(inputs...); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, opDoFail)\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\tif v, err = instr.op.Do(inputs...); err != nil {\n\t\t\t\treturn errors.Wrap(err, opDoFail)\n\t\t\t}\n\t\t}\n\t\tsetEngine(v, m.Engine)\n\n\t}\n\tm.watchedLogf(\"Result E:\")\n\tm.enterLogScope()\n\tif vt, ok := v.(tensor.Tensor); ok {\n\t\tm.watchedLogf(\"%x | %T\", v.Uintptr(), vt.Engine())\n\t} else {\n\t\tm.watchedLogf(\"%x\", v.Uintptr())\n\t}\n\tm.leaveLogScope()\n\t// TODO: type and shape checks\n\n\t// Write\n\tm.writeValue(instr.writeTo, v)\n\tnode := m.p.g.Node(instr.id).(*Node)\n\n\tif m.trace() && (len(m.watchNodes) == 0 || m.watchNodes.Contains(node)) {\n\t\tm.Signal()\n\t\tif err = node.bindCopy(v); err != nil {\n\t\t\treturn errors.Wrapf(err, \"TraceExec failed to bind copy\")\n\t\t}\n\t\t// TODO: Iop{} is not supported yet\n\t} else {\n\t\tnode.bind(v)\n\t}\n\n\t// this is a gradient node then, we should also bind the value to the node's dualValue\n\tif m.bindDV() && node.derivOf != nil {\n\t\tfor _, src := range node.derivOf {\n\t\t\tif len(m.bindNodesDV) > 0 && !m.bindNodesDV.Contains(src) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// TODO: Iop{} not supported by CUDA yet\n\n\t\t\tif src.boundTo != nil {\n\t\t\t\tdv := dvUnit(src.boundTo)\n\t\t\t\tcudaLogf(\"dv.d 0x%x v 0x%x | writeTo: %v\", dv.d.Uintptr(), v.Uintptr(), instr.writeTo)\n\t\t\t\tdev := instr.writeTo.device\n\t\t\t\tadd := newEBOByType(addOpType, TypeOf(dv.d), TypeOf(v))\n\t\t\t\tswitch dev {\n\t\t\t\tcase CPU:\n\t\t\t\t\tif d, err := add.UnsafeDo(dv.d, v); err == nil {\n\t\t\t\t\t\tdv.SetDeriv(d)\n\t\t\t\t\t\tsrc.bind(dv)\n\t\t\t\t\t} else {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\tdefault:\n\t\t\t\t\t// temporarily allocate a valu\n\t\t\t\t\tctx := m.Contexts()[int(dev)]\n\n\t\t\t\t\tdt := dv.d.Dtype()\n\t\t\t\t\tshp := dv.d.Shape()\n\t\t\t\t\tmemsize := calcMemSize(dt, shp)\n\n\t\t\t\t\tvar mem tensor.Memory\n\t\t\t\t\tif mem, err = m.Get(dev, memsize); err != nil {\n\t\t\t\t\t\treturn errors.Wrapf(err, \"Unable to allocate %v bytes from %v\", memsize, dev)\n\t\t\t\t\t}\n\n\t\t\t\t\tvar d Value\n\t\t\t\t\tif d, err = makeValueFromMem(dt, shp, mem); err != nil {\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\n\t\t\t\t\t// copy dv.d to d\n\t\t\t\t\tctx.MemcpyHtoD(mem.(cu.DevicePtr), valueToPointer(dv.d), memsize)\n\n\t\t\t\t\t// perform  the op\n\t\t\t\t\tif _, err = add.CUDADo(m, dev, d, d, v); err != nil {\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\t// copy the value back into dv.d\n\t\t\t\t\tctx.MemcpyDtoH(valueToPointer(dv.d), mem.(cu.DevicePtr), memsize)\n\t\t\t\t\tm.Put(dev, mem, memsize) // then free it\n\n\t\t\t\t\tsrc.bind(dv)\n\t\t\t\t\t// the CPU method is correct. This method is correct for MOST cases, but will not be correct under some other circumstances\n\t\t\t\t\t// ctx.MemcpyDtoH(dv.d.Pointer(), cu.DevicePtr(v.Uintptr()), instr.size)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t}\n\n\tm.watchedLogf(\"Written To: %v\", instr.writeTo)\n\tm.enterLogScope()\n\tm.watchedLogf(m.valueFmt, v.Uintptr())\n\tm.leaveLogScope()\n\n\treturn nil\n}\n\nfunc (instr deviceTransport) exec(m *tapeMachine) (err error) {\n\tm.logf(\"Executing %v\", instr)\n\tfrom := m.getValue(instr.from)\n\tto := m.getValue(instr.to)\n\n\tvar ctx *cu.BatchedContext\n\tswitch {\n\tcase instr.from.device == CPU && instr.to.device != CPU:\n\t\tmemsize := int64(from.MemSize())\n\t\tctx = m.Contexts()[int(instr.to.device)]\n\t\tctx.MemcpyHtoD(cu.DevicePtr(to.Uintptr()), valueToPointer(from), memsize)\n\tcase instr.from.device != CPU && instr.to.device == CPU:\n\t\tdt := from.Dtype()\n\t\tmemsize := calcMemSize(dt, from.Shape())\n\t\tctx = m.Contexts()[int(instr.from.device)]\n\t\tctx.MemcpyDtoH(valueToPointer(to), cu.DevicePtr(from.Uintptr()), memsize)\n\n\t\t// when copying from device to host, it's assumed that the host will want to immediately use\n\t\t// so signal the DoWork\n\t\tm.Signal()\n\t}\n\n\treturn nil\n}\n"
        },
        {
          "name": "vm_tape_nocuda.go",
          "type": "blob",
          "size": 4.1025390625,
          "content": "// +build !cuda\n\npackage gorgonia\n\nimport (\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc finalizeTapeMachine(m *tapeMachine) {}\n\n// UseCudaFor is an option for *tapeMachine. This function is NO-OP unless the program is built with the `cuda` tag.\nfunc UseCudaFor(ops ...string) VMOpt {\n\treturn func(m VM) {}\n}\n\nfunc (m *tapeMachine) getEngine(dev Device) tensor.Engine { return m.Engine }\n\nfunc (instr *execOp) exec(m *tapeMachine) (err error) {\n\tm.logf(\"Executing %v. Node is: %x\", instr, instr.id)\n\tm.enterLogScope()\n\tdefer m.leaveLogScope()\n\n\t// Read\n\tm.watchedLogf(\"Inputs:\")\n\tm.enterLogScope()\n\tvar inputs []Value\n\tfor _, reg := range instr.readFrom {\n\t\tv := m.cpumem[reg.id]\n\t\tinputs = append(inputs, v)\n\t\tm.watchedLogf(m.valueFmt, v)\n\t}\n\tm.leaveLogScope()\n\n\t// check if the destination has already been allocated\n\tvar usePrealloc bool\n\tdest := instr.writeTo.id\n\tif m.cpumem[dest] != nil {\n\t\tusePrealloc = true\n\t}\n\n\t// Execute\n\tvar v Value\n\tswitch {\n\tcase instr.preAllocated:\n\t\tif pd, ok := instr.op.(UsePreallocDoer); ok {\n\t\t\tp := m.cpumem[instr.writeTo.id]\n\t\t\tif v, err = pd.UsePreallocDo(p, inputs...); err != nil {\n\t\t\t\treturn errors.Wrapf(err, \"Happened while attempting to execute %v. Node is %x. Register was: %v \", instr, instr.id, instr.writeTo.id)\n\t\t\t}\n\t\t} else {\n\t\t\t// TODO: maybe warn?\n\t\t\tif v, err = instr.op.Do(inputs...); err != nil {\n\t\t\t\treturn errors.Wrap(err, opDoFail)\n\t\t\t}\n\t\t}\n\tcase usePrealloc:\n\t\tif pd, ok := instr.op.(UsePreallocDoer); ok {\n\t\t\tp := m.cpumem[instr.writeTo.id]\n\t\t\tif v, err = pd.UsePreallocDo(p, inputs...); err != nil {\n\t\t\t\tif v, err = instr.op.Do(inputs...); err != nil {\n\t\t\t\t\treturn errors.Wrap(err, opDoFail)\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tif v, err = instr.op.Do(inputs...); err != nil {\n\t\t\t\treturn errors.Wrap(err, opDoFail)\n\t\t\t}\n\t\t}\n\tcase instr.useUnsafe:\n\t\tif ud, ok := instr.op.(UnsafeDoer); ok {\n\t\t\tif v, err = ud.UnsafeDo(inputs...); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"Failed to carry UnsafeDo()\")\n\t\t\t}\n\t\t} else {\n\t\t\t// TODO: warn?\n\t\t\tif v, err = instr.op.Do(inputs...); err != nil {\n\t\t\t\treturn errors.Wrap(err, opDoFail)\n\t\t\t}\n\t\t}\n\tdefault:\n\t\tif v, err = instr.op.Do(inputs...); err != nil {\n\t\t\treturn errors.Wrap(err, opDoFail)\n\t\t}\n\t}\n\n\tm.watchedLogf(\"Result:\")\n\tm.enterLogScope()\n\tm.watchedLogf(m.valueFmt, v)\n\tm.leaveLogScope()\n\t// TODO: type and sohape checks\n\n\t// Write\n\tsetEngine(v, m.Engine)\n\n\tm.cpumem[dest] = v\n\tnode := m.p.g.Node(instr.id).(*Node)\n\n\tif m.trace() && (len(m.watchNodes) == 0 || m.watchNodes.Contains(node)) {\n\t\tif err = node.bindCopy(v); err != nil {\n\t\t\treturn errors.Wrapf(err, \"TraceExec failed to bind copy\")\n\t\t}\n\t\t// Iop is special\n\t\tif node.op == (Iop{}) {\n\t\t\tv = node.Value()\n\t\t\tm.cpumem[dest] = v\n\t\t}\n\n\t} else {\n\t\tnode.bind(v)\n\t}\n\n\t// this is a gradient node then, we should also bind the value to the node's dualValue\n\tif m.bindDV() && node.derivOf != nil {\n\t\tfor _, src := range node.derivOf {\n\t\t\tif len(m.bindNodesDV) > 0 && !m.bindNodesDV.Contains(src) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tswitch {\n\t\t\tcase node.op == (Iop{}):\n\t\t\t\t// we'll need to put a closure into the closure queue\n\t\t\t\tclosure := func() error {\n\t\t\t\t\tdv := dvUnit(src.boundTo)\n\t\t\t\t\tadd := newEBOByType(addOpType, TypeOf(dv.d), TypeOf(v))\n\t\t\t\t\tif _, err := add.UnsafeDo(dv.d, v); err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\t\t\t\tm.closureQueue = append(m.closureQueue, closure)\n\t\t\tdefault:\n\t\t\t\t// TODO HERE\n\t\t\t\t/*\n\t\t\t\t   e.g. problem\n\t\t\t\t   z = y * (x + 1)\n\n\t\t\t\t   Here, 1 is a constant. But 1 comes early in the expression graph.\n\t\t\t\t   The final gradient is also 1, so 1 will also be the derivOf `z`\n\t\t\t\t   But because the graph is sorted, the 1 node will be walked before\n\t\t\t\t   the `z` node, and this part will cause a panic, as `z` will have no `Value`\n\t\t\t\t   associated with it yet.\n\t\t\t\t*/\n\t\t\t\tdv := dvUnit(src.boundTo)\n\t\t\t\tadd := newEBOByType(addOpType, TypeOf(dv.d), TypeOf(v))\n\n\t\t\t\tif d, err := add.UnsafeDo(dv.d, v); err == nil {\n\t\t\t\t\tdv.SetDeriv(d)\n\t\t\t\t\tsrc.bind(dv)\n\t\t\t\t} else {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\n\t\t}\n\n\t}\n\n\tm.watchedLogf(\"Written To: %v\", instr.writeTo)\n\tm.enterLogScope()\n\tm.watchedLogf(m.valueFmt, v)\n\tm.leaveLogScope()\n\treturn nil\n}\n\nfunc (instr deviceTransport) exec(m *tapeMachine) error {\n\treturn nil\n}\n"
        },
        {
          "name": "vm_tape_test.go",
          "type": "blob",
          "size": 3.92578125,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"hash\"\n\t\"hash/fnv\"\n\t\"reflect\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/chewxy/hm\"\n\t\"github.com/stretchr/testify/require\"\n\t\"gorgonia.org/tensor\"\n)\n\nfunc Test_tapeMachine_Reset(t *testing.T) {\n\tg := NewGraph()\n\n\tvar x, y, z *Node\n\tvar err error\n\n\t// define the expression\n\tx = NewScalar(g, Float64, WithName(\"x\"))\n\ty = NewScalar(g, Float64, WithName(\"y\"))\n\tif z, err = Add(x, y); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// create a VM to run the program on\n\tm1 := NewTapeMachine(g)\n\tm2 := NewTapeMachine(g)\n\tdefer m1.Close()\n\tdefer m2.Close()\n\n\t// set initial values then run\n\tLet(x, 2.0)\n\tLet(y, 2.5)\n\tif err = m1.RunAll(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif z.Value().Data().(float64) != 4.5 {\n\t\tt.Fatalf(\"Expected %v, got %v\", 4.5, z.Value())\n\t}\n\tm1.Reset()\n\tif !reflect.DeepEqual(m1.locMap, m2.locMap) {\n\t\tt.Fatalf(\"expected locmap\\n\\n%#v, got\\n\\n%#v\", m1, m2)\n\t}\n\tif !reflect.DeepEqual(m1.p, m2.p) {\n\t\tt.Fatalf(\"expected program\\n\\n%#v, got\\n\\n%#v\", m1, m2)\n\t}\n\tif !reflect.DeepEqual(m1.cpumem, m2.cpumem) {\n\t\tt.Fatalf(\"expected cpumem\\n\\n%#v, got\\n\\n%#v\", m1, m2)\n\t}\n\tif !reflect.DeepEqual(m1.gpumem, m2.gpumem) {\n\t\tt.Fatalf(\"expected gpumem\\n\\n%#v, got\\n\\n%#v\", m1, m2)\n\t}\n\tif !reflect.DeepEqual(m1.pc, m2.pc) {\n\t\tt.Fatalf(\"expected pc\\n\\n%#v, got\\n\\n%#v\", m1, m2)\n\t}\n}\n\nfunc Test_tapeMachineEvalMode(t *testing.T) {\n\tc := require.New(t)\n\n\tg := NewGraph()\n\n\tx := NewTensor(g, Float32, 2, WithShape(3, 2), WithInit(GlorotN(1)), WithName(\"x\"))\n\tscale := NewTensor(g, Float32, 2, WithShape(1, 2), WithInit(GlorotN(1)), WithName(\"scale\"))\n\tbias := NewTensor(g, Float32, 2, WithShape(1, 2), WithInit(GlorotN(1)), WithName(\"bias\"))\n\n\ty, _, _, op, err := BatchNorm(x, scale, bias, 0.9, 1e-5)\n\tc.NoError(err)\n\n\top.SetTraining(true)\n\n\tvar yVal, scaleVal Value\n\tRead(y, &yVal)\n\tRead(scale, &scaleVal)\n\n\tcost, _ := Mean(y)\n\n\tif _, err := Grad(cost, x, scale, bias); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttrainVM := NewTapeMachine(g, BindDualValues(x, scale, bias), TraceExec())\n\n\terr = trainVM.RunAll()\n\tc.NoError(err)\n\n\tevalVM := NewTapeMachine(g, EvalMode())\n\terr = evalVM.RunAll()\n\tc.NoError(err)\n}\n\ntype faultyOp struct{}\n\n// Arity returns 1\nfunc (i faultyOp) Arity() int { return 2 }\n\nfunc (i faultyOp) Type() hm.Type {\n\ta := hm.TypeVariable('a')\n\n\treturn hm.NewFnType(a, a, a)\n}\n\nfunc (i faultyOp) InferShape(ds ...DimSizer) (tensor.Shape, error) { return ds[0].(tensor.Shape), nil }\n\nfunc (i faultyOp) Do(vs ...Value) (Value, error) {\n\treturn vs[0], nil\n}\n\nfunc (i faultyOp) ReturnsPtr() bool      { return true }\nfunc (i faultyOp) CallsExtern() bool     { return false }\nfunc (i faultyOp) OverwritesInput() int  { return -1 }\nfunc (i faultyOp) WriteHash(h hash.Hash) { fmt.Fprintf(h, \"I\") }\n\nfunc (i faultyOp) Hashcode() uint32 {\n\th := fnv.New32a()\n\ti.WriteHash(h)\n\treturn h.Sum32()\n}\n\nfunc (i faultyOp) String() string { return \"FaultyOp\" }\n\nfunc Test_tapeMachinePointerWatchOk(t *testing.T) {\n\tvar err error\n\n\tc := require.New(t)\n\n\tg := NewGraph()\n\n\ta := NewTensor(g, Float32, 2, WithShape(2, 2), WithInit(GlorotN(1)), WithName(\"a\"))\n\n\tout := Must(Mul(a, a))\n\tout = Must(Mul(a, out))\n\tout = Must(Mul(a, out))\n\n\ttrainVM := NewTapeMachine(g, WithPointerWatch())\n\n\tfor i := 0; i < 3; i++ {\n\t\terr = trainVM.RunAll()\n\t\tc.NoError(err)\n\t}\n}\n\nfunc Test_tapeMachinePointerWatchFail(t *testing.T) {\n\tvar err error\n\n\tc := require.New(t)\n\n\tg := NewGraph()\n\n\tts := tensor.New(tensor.WithShape(2, 2), tensor.WithBacking([]float32{1, 2, 3, 4}))\n\n\ta := NewTensor(g, Float32, 2, WithShape(2, 2), WithValue(ts), WithName(\"a\"))\n\tb := NewTensor(g, Float32, 2, WithShape(2, 2), WithValue(ts), WithName(\"b\"))\n\n\tprod := Must(HadamardProd(a, b))\n\tout := Must(ApplyOp(&faultyOp{}, prod, b))\n\n\ttrainVM := NewTapeMachine(g, WithPointerWatch())\n\terr = trainVM.RunAll()\n\tc.Error(err)\n\tt.Logf(\"error: %v\", err)\n\n\t// in this test, prod and out both have the same value which is not right\n\tt.Logf(\"prod: %v\", prod.Value())\n\tt.Logf(\"out: %v\", out.Value())\n\n\tc.True(strings.Contains(err.Error(), \"Pointer clash found in value.\"))\n}\n"
        },
        {
          "name": "walker.go",
          "type": "blob",
          "size": 2.3115234375,
          "content": "package gorgonia\n\nimport (\n\t\"sort\"\n\n\t\"github.com/pkg/errors\"\n\t\"gonum.org/v1/gonum/graph\"\n\t\"gonum.org/v1/gonum/graph/iterator\"\n\t\"gonum.org/v1/gonum/graph/topo\"\n)\n\n// WalkGraph walks a graph. It returns a channel of *Nodes, so be sure to consume the channel or there may be a deadlock\nfunc WalkGraph(start *Node) <-chan *Node {\n\tch := make(chan *Node)\n\twalked := NewNodeSet()\n\n\tgo func() {\n\t\twalkGraph(start, ch, walked)\n\t\tclose(ch)\n\t}()\n\treturn ch\n}\n\nfunc walkGraph(start *Node, ch chan *Node, walked NodeSet) {\n\tdefer func() {\n\t\twalked.Add(start)\n\t}()\n\tif _, ok := walked[start]; ok {\n\t\treturn // walked before\n\t}\n\n\tch <- start\n\n\tfor _, child := range start.children {\n\t\twalkGraph(child, ch, walked)\n\t}\n\n}\n\n// Sort topologically sorts a ExprGraph: root of graph will be first\n// nodes are sorted using gonum's SortStabilized function.\n//\n// see https://godoc.org/gonum.org/v1/gonum/graph/topo#SortStabilized for more info\nfunc Sort(g *ExprGraph) (sorted Nodes, err error) {\n\tvar sortedNodes []graph.Node\n\t// if sortedNodes, err = topo.Sort(g); err != nil {\n\tif sortedNodes, err = topo.SortStabilized(g, reverseLexical); err != nil {\n\t\treturn nil, errors.Wrap(err, sortFail)\n\t}\n\n\tsorted = graphNodeToNode(iterator.NewOrderedNodes(sortedNodes))\n\treturn\n}\n\n// UnstableSort performs a topological sort of the directed graph g returning the 'from' to 'to'\n// sort order. If a topological ordering is not possible, an Unorderable error is returned\n// listing cyclic components in g with each cyclic component's members sorted by ID. When\n// an Unorderable error is returned, each cyclic component's topological position within\n// the sorted nodes is marked with a nil graph.Node.\nfunc UnstableSort(g *ExprGraph) (sorted Nodes, err error) {\n\tvar sortedNodes []graph.Node\n\tif sortedNodes, err = topo.Sort(g); err != nil {\n\t\treturn nil, errors.Wrap(err, sortFail)\n\t}\n\n\tsorted = graphNodeToNode(iterator.NewOrderedNodes(sortedNodes))\n\treturn\n}\n\nfunc reverseNodes(sorted Nodes) {\n\tfor i := len(sorted)/2 - 1; i >= 0; i-- {\n\t\tj := len(sorted) - i - 1\n\t\tsorted[i], sorted[j] = sorted[j], sorted[i]\n\t}\n}\n\ntype byID []graph.Node\n\nfunc (ns byID) Len() int           { return len(ns) }\nfunc (ns byID) Less(i, j int) bool { return ns[i].ID() > ns[j].ID() }\nfunc (ns byID) Swap(i, j int)      { ns[i], ns[j] = ns[j], ns[i] }\n\nfunc reverseLexical(a []graph.Node) {\n\tsort.Sort(byID(a))\n}\n"
        },
        {
          "name": "weights.go",
          "type": "blob",
          "size": 14.302734375,
          "content": "package gorgonia\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"reflect\"\n\t\"time\"\n\n\trng \"github.com/leesper/go_rng\"\n\t\"github.com/pkg/errors\"\n\t\"gorgonia.org/tensor\"\n)\n\n// This file provides several weight initialization utility functions.\n// It uses the rng package by leesper\n\n// InitWFn is a type of helper function to help initialize weights vector/matrices.\n// It generates the backing required for the tensors.\n//\n// It's typically used in closures\ntype InitWFn func(dt tensor.Dtype, s ...int) interface{}\n\n// Zeroes creates an InitWfn that populates a Value with... zeroes. I don't know what you expected.\nfunc Zeroes() InitWFn {\n\tf := func(dt tensor.Dtype, s ...int) interface{} {\n\t\tsize := tensor.Shape(s).TotalSize()\n\t\tswitch dt {\n\t\tcase tensor.Float64:\n\t\t\treturn make([]float64, size)\n\t\tcase tensor.Float32:\n\t\t\treturn make([]float32, size)\n\t\tcase tensor.Int:\n\t\t\treturn make([]int, size)\n\t\tdefault:\n\t\t\treturn reflect.MakeSlice(reflect.SliceOf(dt.Type), size, size).Interface()\n\t\t}\n\t}\n\treturn f\n}\n\n// Ones creates an InitWfn that populates a Value with ones. See Zeroes() for more explanation.\nfunc Ones() InitWFn {\n\treturn func(dt tensor.Dtype, s ...int) interface{} { return ones(dt, s...).Data() }\n}\n\n// RangedFrom creates an InitWFn that populates a Value starting with the provided start, increamenting the number for each element in the value by 1\nfunc RangedFrom(start int) InitWFn {\n\tf := func(dt tensor.Dtype, s ...int) interface{} {\n\t\tsize := tensor.Shape(s).TotalSize()\n\t\treturn tensor.Range(dt, start, start+size)\n\t}\n\treturn f\n}\n\n// RangedFromWithStep creates an InitWFn that populates a value starting with the provided start, and incrementing the number for each element by the provided increment.\nfunc RangedFromWithStep(start, increment interface{}) InitWFn {\n\tf := func(dt tensor.Dtype, s ...int) interface{} {\n\t\ttotalSize := tensor.Shape(s).TotalSize()\n\n\t\tswitch dt {\n\t\tcase tensor.Float64:\n\t\t\t// for convenience\n\t\t\tvar st, incr float64\n\t\t\tswitch s := start.(type) {\n\t\t\tcase float64:\n\t\t\t\tst = s\n\t\t\tcase int:\n\t\t\t\tst = float64(s)\n\t\t\tdefault:\n\t\t\t\tpanic(fmt.Sprintf(\"Cannot use `start`(%v of %T) in RangedFromWithStep in a Float64 tensor\", start, start))\n\t\t\t}\n\t\t\tswitch i := increment.(type) {\n\t\t\tcase float64:\n\t\t\t\tincr = i\n\t\t\tcase int:\n\t\t\t\tincr = float64(i)\n\t\t\tdefault:\n\t\t\t\tpanic(fmt.Sprintf(\"Cannot use `increment`(%v of %T) in RangedFromWithStep in a Float64 tensor\", increment, increment))\n\t\t\t}\n\n\t\t\tresult := make([]float64, totalSize)\n\t\t\tfor i := 0; i < totalSize; i++ {\n\t\t\t\tresult[i] = st\n\t\t\t\tst += incr\n\t\t\t}\n\t\t\treturn result\n\t\tcase tensor.Float32:\n\t\t\t// for convenience, because when you write literals with decimal point in Go it will be converted to float64\n\t\t\tvar st, incr float32\n\t\t\tswitch s := start.(type) {\n\t\t\tcase float32:\n\t\t\t\tst = s\n\t\t\tcase float64:\n\t\t\t\tst = float32(s)\n\t\t\tcase int:\n\t\t\t\tst = float32(s)\n\t\t\tdefault:\n\t\t\t\tpanic(fmt.Sprintf(\"Cannot use `start`(%v of %T) in RangedFromWithStep in a Float32 tensor\", start, start))\n\t\t\t}\n\n\t\t\tswitch i := increment.(type) {\n\t\t\tcase float32:\n\t\t\t\tincr = i\n\t\t\tcase float64:\n\t\t\t\tincr = float32(i)\n\t\t\tcase int:\n\t\t\t\tincr = float32(i)\n\t\t\tdefault:\n\t\t\t\tpanic(fmt.Sprintf(\"Cannot use `increment`(%v of %T) in RangedFromWithStep in a Float32 tensor\", increment, increment))\n\t\t\t}\n\n\t\t\tresult := make([]float32, totalSize)\n\t\t\tfor i := 0; i < totalSize; i++ {\n\t\t\t\tresult[i] = st\n\t\t\t\tst += incr\n\t\t\t}\n\t\t\treturn result\n\t\tcase tensor.Int:\n\t\t\tst := start.(int)\n\t\t\tincr := increment.(int)\n\t\t\tresult := make([]int, totalSize)\n\t\t\tfor i := 0; i < totalSize; i++ {\n\t\t\t\tresult[i] = st\n\t\t\t\tst += incr\n\t\t\t}\n\t\t\treturn result\n\t\tdefault:\n\t\t\tpanic(fmt.Sprintf(\"Dtype %v not yet supported for RangedFromWithStep. Please put a pull request in to support this function\", dt))\n\t\t}\n\n\t}\n\treturn f\n}\n\n// ValuesOf creates an InitWrn that populates a value with val. This function will cause a panic if val's type is incompatible with the values type.\nfunc ValuesOf(val interface{}) InitWFn {\n\tf := func(dt tensor.Dtype, s ...int) interface{} {\n\t\tsize := tensor.Shape(s).TotalSize()\n\n\t\tswitch dt {\n\t\tcase tensor.Float64:\n\t\t\tv := val.(float64)\n\t\t\tretVal := make([]float64, size)\n\t\t\tfor i := range retVal {\n\t\t\t\tretVal[i] = v\n\t\t\t}\n\t\t\treturn retVal\n\t\tcase tensor.Float32:\n\t\t\tv := val.(float32)\n\t\t\tretVal := make([]float32, size)\n\t\t\tfor i := range retVal {\n\t\t\t\tretVal[i] = v\n\t\t\t}\n\t\t\treturn retVal\n\t\tcase tensor.Int:\n\t\t\tv := val.(int)\n\t\t\tretVal := make([]int, size)\n\t\t\tfor i := range retVal {\n\t\t\t\tretVal[i] = v\n\t\t\t}\n\t\t\treturn retVal\n\t\tdefault:\n\t\t\terr := errors.Errorf(nyiTypeFail, \"Zeroes\", dt)\n\t\t\tpanic(err)\n\t\t}\n\t}\n\treturn f\n}\n\n// Gaussian creates a InitWFn with the specified parameters.\n// Example Usage:\n//\t\tw := NewMatrix(g, Float64, WithName(\"w\"), WithShape(2,2), WithInit(Gaussian(0, 1)))\n// This will create a backing slice of []float64, with the length of 4, and its values are drawn from a gaussian distro\nfunc Gaussian(mean, stdev float64) InitWFn {\n\tf := func(dt tensor.Dtype, s ...int) interface{} {\n\t\tswitch dt {\n\t\tcase tensor.Float64:\n\t\t\treturn Gaussian64(mean, stdev, s...)\n\t\tcase tensor.Float32:\n\t\t\treturn Gaussian32(mean, stdev, s...)\n\t\tdefault:\n\t\t\terr := errors.Errorf(nyiTypeFail, \"Gaussian init\", dt)\n\t\t\tpanic(err)\n\t\t}\n\t}\n\treturn f\n}\n\n// Uniform creates a InitWFn with the specified parameters.\n// Example Usage:\n//\t\tw := NewMatrix(g, Float64, WithName(\"w\"), WithShape(2,2), WithInit(Uniform(-1, 1)))\n// This will create a backing slice of []float64, with the length of 4, and its values are drawn from a uniform distro\nfunc Uniform(low, high float64) InitWFn {\n\tf := func(dt tensor.Dtype, s ...int) interface{} {\n\t\tswitch dt {\n\t\tcase tensor.Float64:\n\t\t\treturn Uniform64(low, high, s...)\n\t\tcase tensor.Float32:\n\t\t\treturn Uniform32(low, high, s...)\n\t\tdefault:\n\t\t\terr := errors.Errorf(nyiTypeFail, \"Uniform init\", dt)\n\t\t\tpanic(err)\n\t\t}\n\t}\n\treturn f\n}\n\n// GlorotN creates a InitWFn that populates a Value with weights normally sampled using Glorot et al.'s algorithm\nfunc GlorotN(gain float64) InitWFn {\n\tf := func(dt tensor.Dtype, s ...int) interface{} {\n\t\tswitch dt {\n\t\tcase tensor.Float64:\n\t\t\treturn GlorotEtAlN64(gain, s...)\n\t\tcase tensor.Float32:\n\t\t\treturn GlorotEtAlN32(gain, s...)\n\t\tdefault:\n\t\t\terr := errors.Errorf(nyiTypeFail, \"GlorotN\", dt)\n\t\t\tpanic(err)\n\t\t}\n\t}\n\treturn f\n}\n\n// GlorotU creates a InitWFn that populates a Value with weights uniformly sampled using Glorot et al.'s algorithm\nfunc GlorotU(gain float64) InitWFn {\n\tf := func(dt tensor.Dtype, s ...int) interface{} {\n\t\tswitch dt {\n\t\tcase tensor.Float64:\n\t\t\treturn GlorotEtAlU64(gain, s...)\n\t\tcase tensor.Float32:\n\t\t\treturn GlorotEtAlU32(gain, s...)\n\t\tdefault:\n\t\t\terr := errors.Errorf(nyiTypeFail, \"GlorotU\", dt)\n\t\t\tpanic(err)\n\t\t}\n\t}\n\treturn f\n}\n\nfunc HeN(gain float64) InitWFn {\n\tf := func(dt tensor.Dtype, s ...int) interface{} {\n\t\tswitch dt {\n\t\tcase tensor.Float64:\n\t\t\treturn HeEtAlN64(gain, s...)\n\t\tdefault:\n\t\t\terr := errors.Errorf(nyiTypeFail, \"HeNormal\", dt)\n\t\t\tpanic(err)\n\t\t}\n\t}\n\treturn f\n}\n\nfunc HeU(gain float64) InitWFn {\n\tf := func(dt tensor.Dtype, s ...int) interface{} {\n\t\tswitch dt {\n\t\tcase tensor.Float64:\n\t\t\treturn HeEtAlU64(gain, s...)\n\t\tdefault:\n\t\t\terr := errors.Errorf(nyiTypeFail, \"HeUniform\", dt)\n\t\t\tpanic(err)\n\t\t}\n\t}\n\treturn f\n}\n\n// Gaussian64 returns a []float64 drawn from a gaussian distribution as defined by the mean and stdev\nfunc Gaussian64(mean, stdev float64, s ...int) []float64 {\n\tsize := tensor.Shape(s).TotalSize()\n\n\trand := rng.NewGaussianGenerator(time.Now().UnixNano())\n\tretVal := make([]float64, size)\n\tfor i := range retVal {\n\t\tretVal[i] = rand.Gaussian(mean, stdev)\n\t}\n\treturn retVal\n}\n\n// Gaussian32 returns a []float32 drawn from a gaussian distribution as defined by the mean and stdev\nfunc Gaussian32(mean, stdev float64, s ...int) []float32 {\n\tsize := tensor.Shape(s).TotalSize()\n\n\trand := rng.NewGaussianGenerator(time.Now().UnixNano())\n\tretVal := make([]float32, size)\n\tfor i := range retVal {\n\t\tretVal[i] = float32(rand.Gaussian(mean, stdev))\n\t}\n\treturn retVal\n}\n\n// Uniform64 returns a []float64 drawn from a uniform distribution between [low, high) that is provided\nfunc Uniform64(low, high float64, s ...int) []float64 {\n\tsize := tensor.Shape(s).TotalSize()\n\n\trand := rng.NewUniformGenerator(time.Now().UnixNano())\n\tretVal := make([]float64, size)\n\tfor i := range retVal {\n\t\tretVal[i] = rand.Float64Range(low, high)\n\t}\n\treturn retVal\n}\n\n// Uniform32 returns a []float64 drawn from a uniform distribution between [low, high) that is provided\nfunc Uniform32(low, high float64, s ...int) []float32 {\n\tsize := tensor.Shape(s).TotalSize()\n\tl := float32(low)\n\th := float32(high)\n\n\trand := rng.NewUniformGenerator(time.Now().UnixNano())\n\tretVal := make([]float32, size)\n\tfor i := range retVal {\n\t\tretVal[i] = rand.Float32Range(l, h)\n\t}\n\treturn retVal\n}\n\n// Binomial64 returns a []float64 drawn from a binomial distribution given the trial and probability parameters.\nfunc Binomial64(trials, prob float64, s ...int) []float64 {\n\tsize := tensor.Shape(s).TotalSize()\n\tt := int64(trials)\n\n\trand := rng.NewBinomialGenerator(time.Now().UnixNano())\n\tretVal := make([]float64, size)\n\tfor i := range retVal {\n\t\tretVal[i] = float64(rand.Binomial(t, prob))\n\t}\n\treturn retVal\n}\n\n// Binomial32 returns a []float32 drawn from a binomial distribution given the trial and probability parameters.\nfunc Binomial32(trials, prob float64, s ...int) []float32 {\n\tsize := tensor.Shape(s).TotalSize()\n\tt := int64(trials)\n\n\trand := rng.NewBinomialGenerator(time.Now().UnixNano())\n\tretVal := make([]float32, size)\n\tfor i := range retVal {\n\t\tretVal[i] = float32(rand.Binomial(t, prob))\n\t}\n\treturn retVal\n}\n\n/* SOPHISTICATED INITIALIZATION STRATEGIES */\n\n// GlorotEtAlN64 returns float64 weights sampled from a normal distribution\n// using the methods specified in Glorot et. al (2010).\n// See also: http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\nfunc GlorotEtAlN64(gain float64, s ...int) []float64 {\n\tvar n1, n2 int\n\tfieldSize := 1\n\tswitch len(s) {\n\tcase 0:\n\t\tpanic(\"Glorot Normal only works with Tensors of dimensions >= 1\")\n\tcase 1:\n\t\t// treat it as a col vec\n\t\tn1 = 1\n\t\tn2 = s[0]\n\tdefault:\n\t\tn1, n2 = s[0], s[1]\n\t\tfor _, v := range s[2:] {\n\t\t\tfieldSize *= v\n\t\t}\n\t}\n\n\tsize := tensor.Shape(s).TotalSize()\n\tfanIn := float64((n1 + n2) * fieldSize)\n\n\tstdev := gain * math.Sqrt(2.0/fanIn)\n\n\trand := rng.NewGaussianGenerator(time.Now().UnixNano())\n\tretVal := make([]float64, size)\n\tfor i := range retVal {\n\t\tretVal[i] = rand.Gaussian(0.0, stdev)\n\t}\n\treturn retVal\n}\n\n// GlorotEtAlN32 returns float32 weights sampled from a normal distribution\n// using the methods specified in Glorot et. al (2010).\n// See also: http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\nfunc GlorotEtAlN32(gain float64, s ...int) []float32 {\n\tf64 := GlorotEtAlN64(gain, s...)\n\tretVal := make([]float32, len(f64))\n\tfor i, v := range f64 {\n\t\tretVal[i] = float32(v)\n\t}\n\treturn retVal\n}\n\n// GlorotEtAlU64 returns float64 weights sampled from a uniform distribution\n// using the methods specified in Glorot et. al (2010).\n// See also: http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n//\n// For best results, use:\n// \t\t1.0 for gain for weights that will be used in linear and/or sigmoid units\n//\t\tmath.Sqrt(2.0) for gain for weights that will be used in ReLU units\n//\t\tmath.Sqrt(2.0 / (1+alpha*alpha)) for ReLU that are leaky with alpha\nfunc GlorotEtAlU64(gain float64, s ...int) []float64 {\n\tvar n1, n2 int\n\tfieldSize := 1\n\tswitch len(s) {\n\tcase 0:\n\t\tpanic(\"Glorot Uniform only works with Tensors of dimensions >= 1\")\n\tcase 1:\n\t\t// treat it as a col vec\n\t\tn1 = 1\n\t\tn2 = s[0]\n\tdefault:\n\t\tn1, n2 = s[0], s[1]\n\t\tfor _, v := range s[2:] {\n\t\t\tfieldSize *= v\n\t\t}\n\t}\n\n\tsize := tensor.Shape(s).TotalSize()\n\tfanIn := float64((n1 + n2) * fieldSize)\n\n\tstdev := gain * math.Sqrt(2.0/fanIn)\n\tlo := 0.0 - math.Sqrt(3.0)*stdev\n\thi := 0.0 + math.Sqrt(3.0)*stdev\n\n\trand := rng.NewUniformGenerator(time.Now().UnixNano())\n\tretVal := make([]float64, size)\n\tfor i := range retVal {\n\t\tretVal[i] = rand.Float64Range(lo, hi)\n\t}\n\treturn retVal\n}\n\n// GlorotEtAlU32 returns float32 weights sampled from a uniform distribution\n// using the methods specified in Glorot et. al (2010).\n// See also: http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n//\n// For best results, use:\n// \t\t1.0 for gain for weights that will be used in linear and/or sigmoid units\n//\t\tmath.Sqrt(2.0) for gain for weights that will be used in ReLU units\n//\t\tmath.Sqrt(2.0 / (1+alpha*alpha)) for ReLU that are leaky with alpha\nfunc GlorotEtAlU32(gain float64, s ...int) []float32 {\n\tf64 := GlorotEtAlN64(gain, s...)\n\tretVal := make([]float32, len(f64))\n\tfor i, v := range f64 {\n\t\tretVal[i] = float32(v)\n\t}\n\treturn retVal\n}\n\n// HeEtAlN64 returns float64 weights sampled from a normal distro, using the methods\n// described in He et al (2015). The formula is:\n//\t\trandn(n) * sqrt(2/n)\n// See also https://arxiv.org/abs/1502.01852\n//\n// For best results, use:\n// \t\t1.0 for gain for weights that will be used in linear and/or sigmoid units\n//\t\tmath.Sqrt(2.0) for gain for weights that will be used in ReLU units\n//\t\tmath.Sqrt(2.0 / (1+alpha*alpha)) for ReLU that are leaky with alpha\nfunc HeEtAlN64(gain float64, s ...int) []float64 {\n\tvar fanIn float64\n\n\tswitch len(s) {\n\tcase 0, 1:\n\t\tpanic(\"He et al only works with Tensors of dimensions >= 2\")\n\tcase 2:\n\t\tfanIn = float64(s[0])\n\tdefault:\n\t\tfanIn = 1.0\n\t\tfor _, v := range s[1:] {\n\t\t\tfanIn *= float64(v)\n\t\t}\n\t}\n\n\tsize := tensor.Shape(s).TotalSize()\n\tstdev := gain * math.Sqrt(1.0/fanIn)\n\n\trand := rng.NewGaussianGenerator(time.Now().UnixNano())\n\tretVal := make([]float64, size)\n\tfor i := range retVal {\n\t\tretVal[i] = rand.Gaussian(0.0, stdev)\n\t}\n\treturn retVal\n}\n\n// HeEtAlU64 returns float64 weights sampled from a uniform distro, using the methods\n// described in He et al (2015). The formula is:\n//\t\trandn(n) * sqrt(2/n)\n// See also https://arxiv.org/abs/1502.01852\n//\n// For best results, use:\n// \t\t1.0 for gain for weights that will be used in linear and/or sigmoid units\n//\t\tmath.Sqrt(2.0) for gain for weights that will be used in ReLU units\n//\t\tmath.Sqrt(2.0 / (1+alpha*alpha)) for ReLU that are leaky with alpha\nfunc HeEtAlU64(gain float64, s ...int) []float64 {\n\tvar fanIn float64\n\n\tswitch len(s) {\n\tcase 0, 1:\n\t\tpanic(\"He et al only works with Tensors of dimensions >= 2\")\n\tcase 2:\n\t\tfanIn = float64(s[0])\n\tdefault:\n\t\tfanIn = 1.0\n\t\tfor _, v := range s[1:] {\n\t\t\tfanIn *= float64(v)\n\t\t}\n\t}\n\n\tsize := tensor.Shape(s).TotalSize()\n\tstdev := gain * math.Sqrt(1.0/fanIn)\n\n\tlo := 0.0 - math.Sqrt(3.0)*stdev\n\thi := 0.0 + math.Sqrt(3.0)*stdev\n\n\trand := rng.NewUniformGenerator(time.Now().UnixNano())\n\tretVal := make([]float64, size)\n\tfor i := range retVal {\n\t\tretVal[i] = rand.Float64Range(lo, hi)\n\t}\n\treturn retVal\n}\n"
        },
        {
          "name": "x",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}